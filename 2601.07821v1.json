{
    "id": "2601.07821v1",
    "title": "Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation",
    "authors": [
        "Huanyu Li",
        "Kun Lei",
        "Sheng Zang",
        "Kaizhe Hu",
        "Yongyuan Liang",
        "Bo An",
        "Xiaoli Li",
        "Huazhe Xu"
    ],
    "abstract": "åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒç®—æ³•èƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šç›®æ ‡ï¼ˆå¦‚æ³›åŒ–æ€§ã€å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼‰çªç ´æœºå™¨äººæ¨¡å‹çš„æ€§èƒ½æé™ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œæ¢ç´¢è¿‡ç¨‹ä¸­ï¼Œéœ€è¦äººå·¥å¹²é¢„çš„æ•…éšœï¼ˆä¾‹å¦‚æœºå™¨äººæ‰“ç¿»æ°´æ¯æˆ–æ‰“ç¢ç»ç’ƒï¼‰éš¾ä»¥é¿å…ï¼Œé˜»ç¢äº†æ­¤ç±»èŒƒå¼çš„å®é™…éƒ¨ç½²ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ•…éšœæ„ŸçŸ¥ç¦»çº¿è‡³åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆFARLï¼‰è¿™ä¸€æ–°èŒƒå¼ï¼Œæ—¨åœ¨æœ€å°åŒ–ç°å®ä¸–ç•Œå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ•…éšœå‘ç”Ÿã€‚æˆ‘ä»¬æ„å»ºäº†FailureBenchåŸºå‡†æµ‹è¯•é›†ï¼Œå…¶ä¸­æ•´åˆäº†å¸¸è§ä¸”éœ€è¦äººå·¥å¹²é¢„çš„æ•…éšœåœºæ™¯ï¼Œå¹¶æå‡ºä¸€ç§èåˆåŸºäºä¸–ç•Œæ¨¡å‹çš„å®‰å…¨è¯„åˆ¤å™¨ä¸ç¦»çº¿è®­ç»ƒæ¢å¤ç­–ç•¥çš„ç®—æ³•ï¼Œä»¥é¢„é˜²åœ¨çº¿æ¢ç´¢è¿‡ç¨‹ä¸­çš„æ•…éšœã€‚å¤§é‡ä»¿çœŸä¸çœŸå®ä¸–ç•Œå®éªŒè¡¨æ˜ï¼ŒFARLèƒ½æ˜¾è‘—å‡å°‘éœ€è¦å¹²é¢„çš„æ•…éšœï¼ŒåŒæ—¶åœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ åè®­ç»ƒé˜¶æ®µæå‡æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å®é™…æœºå™¨äººå¼ºåŒ–å­¦ä¹ åè®­ç»ƒä¸­ï¼ŒFARLå¹³å‡å‡å°‘73.1%çš„å¹²é¢„å‹æ•…éšœï¼ŒåŒæ—¶æå‡11.3%çš„æ€§èƒ½è¡¨ç°ã€‚æ¼”ç¤ºè§†é¢‘ä¸ä»£ç è¯¦è§https://failure-aware-rl.github.ioã€‚",
    "url": "https://arxiv.org/abs/2601.07821v1",
    "html_url": "https://arxiv.org/html/2601.07821v1",
    "html_content": "Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation\nHuanyu Li\n1,2âˆ—\n,\nKun Lei\n1,2âˆ—\n,\nSheng Zang\n4\n,\nKaizhe Hu\n1,3\n,\nYongyuan Liang\n6\n,\nBo An\n4\n,\nXiaoli Li\n5\n,\nHuazhe Xu\n1,3\nâˆ—\nEqual contribution.\n1\nShanghai Qi Zhi Institute,\n2\nShanghai Jiao Tong University,\n3\nIIIS, Tsinghua University,\n4\nNanyang Technological University,\n5\nA*STAR Institute for Infocomm Research,\n6\nUniversity of Maryland.\nAbstract\nPost-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness.\nHowever, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm.\nTo tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration.\nExtensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at\nfailure-aware-rl.github.io\n.\nI\nINTRODUCTION\nLearning efficient and accurate policies has been longed for years by roboticists and AI researchers.\nHowever, pure imitation learning\n[\n43\n,\n36\n,\n6\n]\noften overfits the training distribution, while reinforcement learning is notoriously data-inefficient.\nConsequently, reinforcement learning (RL) based post-training becomes crucial for continuously refining specific objectives and adapting policies to the dynamic conditions of the real world. Specifically, offline-to-online RL\n[\n21\n,\n20\n,\n22\n]\n, where the agent is first trained offline on demonstrations and then fine-tuned through online RL, leverages the advantages of both demonstration-based pre-training and RL-based post-training.\nHowever, a significant challenge in deploying RL in such settings is the occurrence of Intervention-requiring Failures (IR Failures) during the learning process. These failures stem from the intrinsic necessity for exploration in RL, which introduces randomness into the agentâ€™s actions. While exploration is crucial for learning optimal policies, it can result in actions that lead to irreversible damage or unsafe situations, such as breaking fragile objects, knocking items out of reach, or damaging the robot arm by hitting objects. Thus, IR Failures often necessitate human intervention to resolve, as they cannot be easily addressed through heuristic methods or reset-free RL techniques\n[\n12\n,\n13\n,\n35\n]\n.\nTo this end, we introduce the concept of\nF\nailure-\nA\nware Offline-to-Online\nR\neinforcement\nL\nearning (FARL), where the agent refines its policy while minimizing IR Failures that would otherwise require human intervention.\nOur method builds on an offline-to-online RL algorithm\n[\n22\n]\n, which employs a policy gradient objective to unify online and offline RL in an on-policy manner.\nIn this work, we focus on failure-aware post-training within real-world manipulation settings.\nTo study various failure scenarios in simulation environments, we introduce a new benchmark,\nFailureBench\n.\nFailureBench\nbuilds upon existing simulation environments\n[\n46\n]\nby incorporating common IR Failure scenarios that frequently occur in real-world manipulation tasks.\nWe simulate situations where failures necessitate human intervention, such as objects being pushed out of the workspace or the robot entering unsafe states. This benchmark enables us to evaluate how effectively the RL algorithms balance performance improvement and generalization while minimizing IR Failures during exploration.\nOur research reveals that existing online and offline-to-online RL algorithms\n[\n22\n,\n31\n,\n24\n]\nfrequently encounter IR Failures in this setting due to the inherent exploration-exploitation trade-off in RL. To mitigate this problem, we introduce a safety critic based on a latent world model for failure prediction, along with a recovery policy designed to prevent failures foreseen by the safety critic.\nBoth components are trained offline using carefully curated demonstrations and later deployed to prevent failures during the online task policy post-training process.\nOur framework demonstrates significant reductions in IR Failures across various simulated settings within FailureBench, while simultaneously enhancing task performance and generalizability. We validate the effectiveness of FARL through real-world experiments conducted on a Franka Emika Panda robot, showing that our methods substantially reduce the need for human intervention during training.\nOur contributions are summarized as follows\n:\nâ€¢\nWe identify a major barrier to deploying RL in real-world scenarios: IR Failures caused by exploration-induced randomness. To study such potential risks in existing methods, we introduce\nFailureBench\n, a benchmark that repurposes existing RL environments to study IR Failure cases requiring human intervention, enabling evaluation of RL algorithms for both performance and failure minimization.\nâ€¢\nWe propose a failure-aware offline-to-online framework, including a specifically designed world model and recovery policy to minimize IR Failures while facilitating learning and adaptation with RL, theoretically justified by an â€œadvantage correctionâ€ analysis to simultaneously enhance learning and safety.\nâ€¢\nWe conduct extensive experiments in both simulated environments and three challenging real robotic tasks susceptible to IR Failures to validate the effectiveness of our approach.\nII\nRelated Work\nPrevious research in safe reinforcement learning has extensively examined safety through various approaches, focusing mainly on learning-from-scratch scenarios\n[\n33\n,\n49\n,\n1\n]\n. However, with advances in modern robotic models that learn from demonstrations\n[\n43\n,\n36\n,\n6\n,\n47\n,\n15\n,\n19\n]\n, there is an increasing recognition of the importance of online post-training strategies. Consequently, this work shifts its focus to a failure-aware offline-to-online reinforcement learning setting.\nSafe RL.\nConstrained Markov Decision Processes (CMDPs)\n[\n2\n,\n17\n,\n45\n]\nhave gained significant attention within the RL community, particularly in the context of constrained and safe real-world learning. Safe and recovery RL seeks to address two primary challenges\n[\n28\n]\n: the development of effective safety constraints\n[\n41\n]\nand the formulation of safe actions. To tackle these challenges, a substantial body of work utilizes the established online RL workflow to balance task rewards and constraints, their focus lies on optimization techniques, such as Lagrangian relaxation\n[\n25\n,\n37\n,\n38\n]\n, Lyapunov functions\n[\n8\n,\n9\n]\nand robustness guarantees\n[\n26\n,\n30\n,\n27\n,\n29\n]\n. However, a common issue is that these constraints are often enforced prematurely, which limits exploration and can reduce overall performance\n[\n14\n]\n.\nThe term â€œrecoveryâ€ in safe RL can be misleading. True recovery methods like damage adaptation\n[\n10\n]\naddress post-failure adaptation, while many â€œrecovery RLâ€ methods actually prevent failures. Recovery RL\n[\n39\n]\nand ABS\n[\n14\n]\npredict constraint violations and employ recovery policies to avoid unsafe states before failures occur. Similarly, recent prevention methods include Control Barrier Functions\n[\n3\n]\nthat ensure forward invariance of safe sets, and predictive safety filters\n[\n40\n]\nthat use MPC to modify unsafe control inputs. SafeDreamer\n[\n16\n]\nintegrates Lagrangian methods into the DreamerV3 planning process for model-based safe RL. While SafeDreamer focuses on traditional safe RL tasks in simulation, our FARL addresses offline-to-online post-training of pre-trained policies in the real world. Alternative approaches include hierarchical safe RL\n[\n11\n,\n44\n]\n, which utilizes structural dynamics, and methods incorporating safety certificates from control theory\n[\n5\n,\n32\n]\n.\nOffline-to-online RL.\nOffline RL aims to address distributional shift issues that arise when a policy encounters out-of-distribution (OOD) state-action pairs. Prior methods mitigate this challenge by incorporating conservatism\n[\n21\n]\nor constraint-based regularization\n[\n20\n,\n4\n,\n50\n]\n, thereby either discouraging the policy from exploring OOD regions or providing conservative value estimates.\nOnce pre-trained with offline data, policies can be further improved through online fine-tuning. However, directly applying standard online off-policy RL algorithms often leads to severe performance degradation due to distributional shift during online exploration\n[\n48\n]\n.\nUni-O4\n[\n22\n]\ndirectly applies the PPO\n[\n34\n]\nobjective to unify offline and online learning, eliminating the need for extra regularization. RL-100\n[\n23\n]\ncombines iterative offline RL with online RL to train diffusion-based policies for deployable robot learning.\nHowever, deploying such offline-to-online methods in real-world robotic systems remains unsafe and expensive due to the high risk of IR Failures during online exploration.\nIn this work, we build upon Uni-O4 and extend it to address safety concerns in real-world manipulation tasks, enabling safer and more efficient policy refinement in real-world environments.\nFigure 2\n:\nThe entire training pipeline consists of two main phases: 1) the offline phase, which involves pre-training the task policy, recovery policy, and world model, and 2) the online phase, during which the task policy is fine-tuned within safe exploration settings.\nIII\nProblem Statement\nWe consider RL-based post-training under Constrained Markov Decision Processes (CMDPs)\n[\n2\n]\n. Following\n[\n39\n]\n, we limit constraint costs to binary indicator functions that identify constraint-violating states. This can be described by tuple\nâ„³\n(\nğ’®\n,\nğ’œ\n,\nP\n(\nâ‹…\nâˆ£\nâ‹…\n,\nâ‹…\n)\n,\nR\n(\nâ‹…\n,\nâ‹…\n)\n,\nÎ³\n,\nC\n(\nâ‹…\n)\n,\nÎ³\nrisk\n,\nÎ¼\n)\n\\mathcal{M}\\left(\\mathcal{S},\\mathcal{A},P(\\cdot\\mid\\cdot,\\cdot),R(\\cdot,\\cdot),\\gamma,C(\\cdot),\\gamma_{\\text{risk }},\\mu\\right)\n. Here,\nğ’®\n\\mathcal{S}\nand\nğ’œ\n\\mathcal{A}\nare the state and action spaces.\nP\n:\nğ’®\nÃ—\nğ’œ\nÃ—\nğ’®\nâ†’\n[\n0\n,\n1\n]\nP:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow{}[0,1]\ndenotes the stochastic dynamics model which maps the given state and action to a probability distribution over the next state.\nR\n:\nğ’®\nÃ—\nğ’œ\nâ†’\nâ„\nR:\\mathcal{S}\\times\\mathcal{A}\\rightarrow{}\\mathbb{R}\nis the reward function, and\nÎ³\n\\gamma\ndenotes the discount factor.\nC\n:\nğ’®\nÃ—\nğ’œ\nâ†’\n{\n0\n,\n1\n}\nC:\\mathcal{S}\\times\\mathcal{A}\\rightarrow{}\\{0,1\\}\nis a constraint cost function, which denotes whether a state and action pair violate the designed constraint and it is associated with a threshold\nÎ³\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\n\\gamma_{risk}\n.\nGiven a set of behavior policies\nÏ€\nâˆˆ\nÎ \n\\pi\\in\\Pi\n, the objective of the expected return can be defined as\nR\n=\nğ”¼\nÏ€\n,\nÎ¼\n,\nP\nâ€‹\n[\nâˆ‘\nt\nÎ³\nt\nâ€‹\n(\ns\nt\n,\na\nt\n)\n]\nR=\\mathbb{E}_{\\pi,\\mu,P}\\left[\\sum_{t}\\gamma^{t}\\left(s_{t},a_{t}\\right)\\right]\n.\nWe consider the\nH\nH\n-steps discounted probability of constraint violation, which can be defined as\nC\nH\nÏ€\n\\displaystyle C_{H}^{\\pi}\n=\nğ”¼\nğ…\n,\nğ\n,\nğ‘·\nâ€‹\n[\nâˆ‘\ni\n=\nt\nt\n+\nH\nğœ¸\nrisk\ni\nâˆ’\nt\nâ€‹\nC\nâ€‹\n(\ns\nt\n,\na\nt\n)\n]\n\\displaystyle=\\mathbb{E}_{\\bm{\\pi},\\bm{\\mu},\\bm{P}}\\left[\\sum_{i=t}^{t+H}\\bm{\\gamma}_{\\mathrm{risk}}^{i-t}C(s_{t},a_{t})\\right]\n(1)\n=\nâˆ‘\ni\n=\nt\nt\n+\nH\nğœ¸\nrisk\ni\nâˆ’\nt\nâ€‹\nâ„™\nâ€‹\n(\nğ‘ª\nâ€‹\n(\ns\nt\n,\na\nt\n)\n=\n1\n)\n\\displaystyle=\\sum_{i=t}^{t+H}\\bm{\\gamma}_{\\mathrm{risk}}^{i-t}\\mathbb{P}\\left(\\bm{C}(s_{t},a_{t})=1\\right)\nin which the constraint costs are defined by the binary indicator functions.\nThus, the objective of RL under CMDPs is:\nÏ€\nâˆ—\n=\nargmax\nÏ€\nâˆˆ\nÎ \nâ€‹\nR\nÏ€\nâ€‹\ns.t.\nâ€‹\nC\nH\nÏ€\nâ‰¤\nğœº\nsafe\n\\displaystyle\\pi^{*}=\\underset{\\pi\\in\\Pi}{\\operatorname{argmax}}\\,R^{\\pi}\\;\\text{s.t.}\\;C_{H}^{\\pi}\\leq\\bm{\\varepsilon}_{\\mathrm{safe}}\n(2)\nThe constraints define a set of feasible policies, i.e., {\nÏ€\nâˆˆ\nÎ \n:\nC\nH\nÏ€\nâ‰¤\nğœº\nsafe\n\\pi\\in\\Pi:\\quad C_{H}^{\\pi}\\leq\\bm{\\varepsilon}_{\\mathrm{safe}}\n}. We optimize this objective over the set of feasible policies.\nIn this work, we propose a failure-aware offline-to-online RL algorithm designed to optimize safe policy exploration and refinement. The core concept is inspired by recovery RL.\nHowever, our focus is on post-training, which enhances robotic models learned from demonstrations by refining specific capabilities and providing a better policy initialization for safety, rather than relying on unnecessary random exploration from scratch.\nIV\nMethod\nWe formally introduce FARL, our offline-to-online failure-aware RL framework. Our overall algorithm pipeline comprises both offline and online phases, as summarized in Figure\n2\n:\n1) We first pre-train a task policy, a recovery policy, and a world model. The task policy is trained using task demonstrations that show successful task completion. The recovery policy is trained with recovery demonstrations that illustrate how to avoid or escape from near-failure states. The world model leverages both task demonstrations and failure demonstrations that capture state-action sequences leading to IR Failures.\n2) Next, we fine-tune the task policy, which is optimized according to Eq. (\n2\n), and guided by the recovery policy. This policy directs the agent back to the state-action pair\n(\ns\n,\na\n)\n(s,a)\nwhere\nC\nH\nÏ€\nâ‰¤\nğœº\nsafe\nC_{H}^{\\pi}\\leq\\bm{\\varepsilon}_{\\mathrm{safe}}\n. During the online phase, the recovery policy and world model remain fixed to minimize IR Failures during exploration.\nIV-A\nOffline Pre-training\nWe pre-train a task policy to serve as the initial framework for online fine-tuning. In addition, we also pre-train a recovery policy and a world model, both designed to guide the task policyâ€™s exploration and help prevent IR Failures during the online phase. For the task policy pre-training, we follow the training pipeline established in Uni-O4\n[\n22\n]\n. Initially, the policy undergoes behavior cloning, followed by fine-tuning using the objective:\nJ\nk\nâ€‹\n(\nÏ€\n)\n=\n\\displaystyle J_{k}\\left(\\pi\\right)=\nğ”¼\ns\nâˆ¼\nÏ\nÏ€\n(\nâ‹…\n)\n,\na\nâˆ¼\nÏ€\nk\n(\nâ‹…\n|\ns\n)\n[\nmin\n(\nr\n(\nÏ€\n)\nA\n(\ns\n,\na\n)\n,\n\\displaystyle\\mathbb{E}_{s\\sim\\rho_{\\pi}\\left(\\cdot\\right),a\\sim\\pi_{k}\\left(\\cdot|s\\right)}\\bigg[\\min\\Big(r(\\pi)A(s,a),\n(3)\nclip\n(\nr\n(\nÏ€\n)\n,\n1\nâˆ’\nÏµ\n,\n1\n+\nÏµ\n)\nA\n(\ns\n,\na\n)\n)\n]\n\\displaystyle\\text{clip}\\left(r(\\pi),1-\\epsilon,1+\\epsilon\\right)A(s,a)\\Big)\\bigg]\nwhere\nÏ\nÏ€\n\\rho_{\\pi}\nis the stationary distribution of states under policy\nÏ€\n\\pi\n,\nr\nâ€‹\n(\nÏ€\n)\n=\nÏ€\nâ€‹\n(\na\n|\ns\n)\nÏ€\nk\nâ€‹\n(\na\n|\ns\n)\nr(\\pi)=\\frac{\\pi\\left(a|s\\right)}{\\pi_{k}\\left(a|s\\right)}\ndenotes the importance sampling ratio between the target policy\nÏ€\n\\pi\nand behavior policy\nÏ€\nk\n\\pi_{k}\n, clip\n(\nâ‹…\n)\n(\\cdot)\nis a conservatism operation that constrains the ratio, hyper-parameter\nÏµ\n\\epsilon\nis used to adjust the degree of conservatism, and\nA\nâ€‹\n(\ns\nt\n,\na\nt\n)\nA(s_{t},a_{t})\nis the advantage function.\nSubsequently, we continue fine-tuning within the environment to optimize the objective in Equation\n3\nusing GAE advantage estimation.\nNext, we train the recovery policy through behavior cloning, utilizing recovery demonstrations. The training process during the offline phase mirrors that of the task policy, beginning with behavior cloning and then fine-tuning via Uni-O4 in an offline context. However, we avoid fine-tuning the recovery policy during the online phase due to the limited availability of failure data. We observed that this enhances the safe exploration of the task policy in real-world environments.\nWe also pre-train a world model with both task and failure demonstrations, specifically for predicting future failures. The world model focuses on a limited number of near-future steps rather than the entire episode in recovery reinforcement learning\n[\n39\n]\n. Our findings indicate that planning a short distance into the future can effectively minimize IR Failures in scenarios of real-world manipulation. To achieve this, we augment the world model for failure prediction by introducing a constraint prediction head. The training objective of our world model can be defined as:\nğ’¥\nâ€‹\n(\nÎ¸\n;\nÎ“\n)\n=\nâˆ‘\ni\n=\nt\nt\n+\nH\nÎ»\ni\nâˆ’\nt\nâ‹…\nâ„’\nâ€‹\n(\nÎ¸\n;\nÎ“\ni\n)\n,\n\\displaystyle\\mathcal{J}(\\theta;\\Gamma)=\\sum_{i=t}^{t+H}\\lambda^{i-t}\\cdot\\mathcal{L}(\\theta;\\Gamma_{i}),\n(4)\nwhich can be computed step-wise as follows:\nâ„’\n(\nÎ¸\n;\n\\displaystyle\\mathcal{L}(\\theta;\nÎ“\ni\n)\n=\nc\n1\nâ€–\nR\nÎ¸\nâ€‹\n(\nğ³\ni\n,\nğš\ni\n)\nâˆ’\nr\ni\nâ€–\n2\n2\nâŸ\nreward\n\\displaystyle\\Gamma_{i})=\\ c_{1}\\underbrace{\\|R_{\\theta}(\\mathbf{z}_{i},\\mathbf{a}_{i})-r_{i}\\|_{2}^{2}}_{\\mathrm{reward}}\n(5)\n+\nc\n2\nâ‹…\nCE\nâ€‹\n(\nQ\nÎ¸\nâ€‹\n(\nğ³\ni\n,\nğš\ni\n)\nâˆ’\n(\nr\ni\n+\nÎ³\nâ€‹\nQ\nÎ¸\nâˆ’\nâ€‹\n(\nğ³\ni\n+\n1\n,\nÏ€\nÎ¸\nâ€‹\n(\nğ³\ni\n+\n1\n)\n)\n)\n)\nâŸ\nvalue\n\\displaystyle+c_{2}\\cdot\\underbrace{\\text{CE}(Q_{\\theta}(\\mathbf{z}_{i},\\mathbf{a}_{i})-\\left(r_{i}+\\gamma Q_{\\theta^{-}}(\\mathbf{z}_{i+1},\\pi_{\\theta}(\\mathbf{z}_{i+1}))\\right))}_{\\mathrm{value}}\n(6)\n+\nc\n3\nâ‹…\nCE\nâ€‹\n(\nd\nÎ¸\nâ€‹\n(\nğ³\ni\n,\nğš\ni\n)\nâˆ’\nh\nÎ¸\nâˆ’\nâ€‹\n(\nğ¬\ni\n+\n1\n)\n)\nâŸ\nlatent state consistency\n\\displaystyle+c_{3}\\cdot\\underbrace{\\text{CE}(d_{\\theta}(\\mathbf{z}_{i},\\mathbf{a}_{i})-h_{\\theta^{-}}(\\mathbf{s}_{i+1}))}_{\\text{latent state consistency}}\n(7)\n+\nc\n4\nâ€‹\nâ€–\nC\nÎ¸\nâ€‹\n(\nğ³\ni\n,\nğš\ni\n)\nâˆ’\nc\ni\nâ€–\n2\n2\nâŸ\nconstraint\n\\displaystyle+c_{4}\\underbrace{\\|C_{\\theta}(\\mathbf{z}_{i},\\mathbf{a}_{i})-c_{i}\\|_{2}^{2}}_{\\mathrm{constraint}}\n(8)\n+\nc\n5\nâ€‹\nâ€–\nS\nÎ¸\nâ€‹\n(\nğ³\ni\n)\nâˆ’\ns\ni\nâ€–\n2\n2\nâŸ\ndecoder\n,\n\\displaystyle+c_{5}\\underbrace{\\|S_{\\theta}(\\mathbf{z}_{i})-s_{i}\\|_{2}^{2}}_{\\mathrm{decoder}},\n(9)\nwhere the model consists of seven components:\nRepresentation:\nğ³\nt\n=\nh\nÎ¸\nâ€‹\n(\nğ¬\nt\n)\nLatent dynamics:\nğ³\nt\n+\n1\n=\nd\nÎ¸\nâ€‹\n(\nğ³\nt\n,\nğš\nt\n)\nReward:\nr\n^\nt\n=\nR\nÎ¸\nâ€‹\n(\nğ³\nt\n,\nğš\nt\n)\nValue:\nq\n^\nt\n=\nQ\nÎ¸\nâ€‹\n(\nğ³\nt\n,\nğš\nt\n)\nPolicy:\nğš\n^\nt\nâˆ¼\nÏ€\nÎ¸\nâ€‹\n(\nğ³\nt\n)\nDecoder:\nğ¬\n^\nt\nâˆ¼\nS\nÎ¸\nâ€‹\n(\nğ³\nt\n)\nConstraint:\nğœ\n^\nt\nâˆ¼\nC\nÎ¸\nâ€‹\n(\nğ³\nt\n,\nğš\nt\n)\n\\vskip-5.0pt\\footnotesize\\begin{aligned} \\text{Representation:}&\\qquad\\mathbf{z}_{t}=h_{\\theta}\\left(\\mathbf{s}_{t}\\right)\\\\\n\\text{Latent dynamics:}&\\qquad\\mathbf{z}_{t+1}=d_{\\theta}\\left(\\mathbf{z}_{t},\\mathbf{a}_{t}\\right)\\\\\n\\text{Reward:}&\\qquad\\hat{r}_{t}=R_{\\theta}\\left(\\mathbf{z}_{t},\\mathbf{a}_{t}\\right)\\\\\n\\text{Value:}&\\qquad\\hat{q}_{t}=Q_{\\theta}\\left(\\mathbf{z}_{t},\\mathbf{a}_{t}\\right)\\\\\n\\end{aligned}\\qquad\\begin{aligned} \\text{Policy:}&\\qquad\\hat{\\mathbf{a}}_{t}\\sim\\pi_{\\theta}\\left(\\mathbf{z}_{t}\\right)\\\\\n\\text{Decoder:}&\\qquad\\hat{\\mathbf{s}}_{t}\\sim S_{\\theta}\\left(\\mathbf{z}_{t}\\right)\\\\\n\\text{Constraint:}&\\qquad\\hat{\\mathbf{c}}_{t}\\sim C_{\\theta}\\left(\\mathbf{z}_{t},\\mathbf{a}_{t}\\right)\\\\\n\\end{aligned}\n(10)\nwhere\ns\nt\ns_{t}\nis the observation at time-step\nt\nt\n, which is encoded to a representation\nğ³\n\\mathbf{z}\nby an encoder\nh\nÎ¸\nh_{\\theta}\n. Conditioned on the representation, each head of the dynamics model predicts the representation of the next state, the reconstructed state, the constraint, a single-step reward, a state-action\nQ\nQ\n-value, and an action that maximizes the\nQ\nQ\n-function.\nWe incorporate value and reward signals during training to enrich latent representations while providing gradient-based regularization that prevents overfitting to constraint patterns.\nAfter pre-training, we infer the discounted near-future constraint of\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\n\\pi_{task}\nmentioned in Eq. (\n1\n) as:\nC\nH\nÏ€\n=\nğ”¼\nğ…\n[\n\\displaystyle C_{H}^{\\pi}=\\mathbb{E}_{\\bm{\\pi}}\\bigg[\nâˆ‘\ni\n=\nt\nt\n+\nH\nğœ¸\nrisk\ni\nâˆ’\nt\n[\nC\nÎ¸\n(\nz\ni\n,\na\ni\n)\n|\nz\ni\n=\nh\nÎ¸\n(\ns\ni\n)\n;\n\\displaystyle\\sum_{i=t}^{t+H}\\bm{\\gamma}_{\\mathrm{risk}}^{i-t}\\big[C_{\\theta}(z_{i},a_{i})\\,|\\,z_{i}=h_{\\theta}(s_{i});\n(11)\na\ni\n=\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\n(\nS\nÎ¸\n(\nz\ni\n)\n)\n;\nz\ni\n+\n1\n=\nd\nÎ¸\n(\nz\ni\n,\na\ni\n)\n]\n]\n\\displaystyle a_{i}=\\pi_{task}(S_{\\theta}(z_{i}));z_{i+1}=d_{\\theta}(z_{i},a_{i})\\big]\\bigg]\nIV-B\nOnline fine-tuning with recovery\nThe task policy is fine-tuned by online PPO with the policy and value network initialization from offline pre-training, following Uni-O4. However, we consider the safe exploration setting, in which we require each state-action tuple\n(\ns\n,\na\n)\n(s,a)\nto be safe. Let task policy\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\n\\pi_{task}\ninteract with the environments, each state action tuple will be checked by the world model to consider safety in the near-future steps through planning. We define the task transitions as\nğ’¯\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nÏ€\n=\n(\ns\nt\n,\na\nt\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\n,\ns\nt\n+\n1\n,\nr\nt\n)\n\\mathcal{T}_{task}^{\\pi}=(s_{t},a_{t}^{\\pi_{task}},s_{t+1},r_{t})\n, where\n(\ns\n,\na\n)\nâˆˆ\nğ’®\nÃ—\nğ’œ\n:\nC\nH\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ‰¤\nğœº\nsafe\n(s,a)\\in\\mathcal{S}\\times\\mathcal{A}:C_{H}^{\\pi_{task}}\\leq\\bm{\\varepsilon}_{\\mathrm{safe}}\nand the recovery transitions\nğ’¯\nr\nâ€‹\ne\nâ€‹\nc\nÏ€\n=\n(\ns\nt\n,\na\nt\nÏ€\nr\nâ€‹\ne\nâ€‹\nc\n,\ns\nt\n+\n1\n,\nr\nt\n)\n\\mathcal{T}_{rec}^{\\pi}=(s_{t},a_{t}^{\\pi_{rec}},s_{t+1},r_{t})\n, where\n(\ns\n,\na\n)\nâˆˆ\nğ’®\nÃ—\nğ’œ\n:\nC\nH\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ‰¥\nğœº\nsafe\n(s,a)\\in\\mathcal{S}\\times\\mathcal{A}:C_{H}^{\\pi_{task}}\\geq\\bm{\\varepsilon}_{\\mathrm{safe}}\n. In other words, if the action sampled from the task policy\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\n\\pi_{task}\nunder state\ns\ns\ndoes not satisfy\nC\nH\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ‰¤\nğœº\nsafe\nC_{H}^{\\pi_{task}}\\leq\\bm{\\varepsilon}_{\\mathrm{safe}}\n, we would revise the task transition to the recovery transition via the action\na\nr\nâ€‹\ne\nâ€‹\nc\na_{rec}\nsampled from\nÏ€\nr\nâ€‹\ne\nâ€‹\nc\n\\pi_{rec}\n. The workflow is also described in the\nsafe exploration\narea of the online phase in Figure\n2\n. The safe transitions could be defined as:\nğ’¯\nsafe\nÏ€\n=\n{\nğ’¯\ntask\nÏ€\n,\nC\nH\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ‰¤\nğœº\nsafe\nğ’¯\nrec\nÏ€\n,\nC\nH\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\n>\nğœº\nsafe\n\\mathcal{T}_{\\mathrm{safe}}^{\\pi}=\\begin{cases}\\mathcal{T}_{\\mathrm{task}}^{\\pi},&C_{H}^{\\pi_{task}}\\leq\\bm{\\varepsilon}_{\\mathrm{safe}}\\\\\n\\mathcal{T}_{\\mathrm{rec}}^{\\pi},&C_{H}^{\\pi_{task}}>\\bm{\\varepsilon}_{\\mathrm{safe}}\\end{cases}\n(12)\nBased on the safe transitions, we fine-tune the task policy using Objective\n3\nwith GAE advantage estimation.\nV\nTheoretical Analysis\nWe provide theoretical justification for FARLâ€™s superior performance through an \"action correction\" mechanism.\nV-A\nPreliminaries\nLet\nA\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n,\na\n)\nA^{\\pi_{task}}(s,a)\ndenote the GAE-based advantage function under policy\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\n\\pi_{task}\n. We classify states based on H-step constraint violation:\nğ’®\nr\nâ€‹\ne\nâ€‹\nc\n=\n{\ns\nâˆˆ\nğ’®\nâˆ£\nâˆƒ\na\nâˆ¼\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\n(\nâ‹…\n|\ns\n)\n:\nC\nH\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\n(\ns\n,\na\n)\n>\nÎµ\ns\nâ€‹\na\nâ€‹\nf\nâ€‹\ne\n}\n\\mathcal{S}_{rec}=\\{s\\in\\mathcal{S}\\mid\\exists a\\sim\\pi_{task}(\\cdot|s):C_{H}^{\\pi_{task}}(s,a)>\\varepsilon_{safe}\\}\nWe classify actions based on H-step constraint violation at the action level:\nğ’œ\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\n=\n{\na\nâˆˆ\nğ’œ\nâˆ£\nC\nH\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n,\na\n)\n>\nÎµ\ns\nâ€‹\na\nâ€‹\nf\nâ€‹\ne\n}\n\\mathcal{A}_{risk}(s)=\\{a\\in\\mathcal{A}\\mid C_{H}^{\\pi_{task}}(s,a)>\\varepsilon_{safe}\\}\nğ’œ\ns\nâ€‹\na\nâ€‹\nf\nâ€‹\ne\nâ€‹\n(\ns\n)\n=\n{\na\nâˆˆ\nğ’œ\nâˆ£\nC\nH\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n,\na\n)\nâ‰¤\nÎµ\ns\nâ€‹\na\nâ€‹\nf\nâ€‹\ne\n}\n\\mathcal{A}_{safe}(s)=\\{a\\in\\mathcal{A}\\mid C_{H}^{\\pi_{task}}(s,a)\\leq\\varepsilon_{safe}\\}\nThe risk probability at state\ns\ns\nis defined as:\np\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\n=\nâ„™\na\nâˆ¼\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\n(\nâ‹…\n|\ns\n)\nâ€‹\n[\na\nâˆˆ\nğ’œ\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\n]\np_{risk}(s)=\\mathbb{P}_{a\\sim\\pi_{task}(\\cdot|s)}[a\\in\\mathcal{A}_{risk}(s)]\nV-B\nAssumptions\nAssumption 1\n(Non-trivial Risk Distribution)\nThere exists a non-negligible fraction of states where the task policy samples risky actions:\nğ”¼\ns\nâˆ¼\nÏ\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n[\np\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\n]\n>\n0\n\\mathbb{E}_{s\\sim\\rho_{\\pi_{task}}}[p_{risk}(s)]>0\nwhere\nÏ\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\n\\rho_{\\pi_{task}}\nis the state visitation distribution under policy\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\n\\pi_{task}\n.\nAssumption 2\n(Probabilistic Safe Recovery)\nFor any state\ns\ns\n, the recovery policy provides safe actions with high probability:\nâ„™\na\nâˆ¼\nÏ€\nr\nâ€‹\ne\nâ€‹\nc\n(\nâ‹…\n|\ns\n)\nâ€‹\n[\na\nâˆˆ\nğ’œ\ns\nâ€‹\na\nâ€‹\nf\nâ€‹\ne\nâ€‹\n(\ns\n)\n]\nâ‰¥\n1\nâˆ’\nÏµ\nr\nâ€‹\ne\nâ€‹\nc\n\\mathbb{P}_{a\\sim\\pi_{rec}(\\cdot|s)}[a\\in\\mathcal{A}_{safe}(s)]\\geq 1-\\epsilon_{rec}\nwhere\nÏµ\nr\nâ€‹\ne\nâ€‹\nc\n>\n0\n\\epsilon_{rec}>0\nis the recovery failure rate.\nAssumption 3\n(Safe Action Advantage)\nFor states where both safe and risky actions exist, safe actions provide better expected advantage:\nğ”¼\na\nâˆˆ\nğ’œ\ns\nâ€‹\na\nâ€‹\nf\nâ€‹\ne\nâ€‹\n(\ns\n)\nâ€‹\n[\nA\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n,\na\n)\n]\nâ‰¥\nğ”¼\na\nâˆˆ\nğ’œ\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\nâ€‹\n[\nA\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n,\na\n)\n]\n+\nÎ´\n\\mathbb{E}_{a\\in\\mathcal{A}_{safe}(s)}[A^{\\pi_{task}}(s,a)]\\geq\\mathbb{E}_{a\\in\\mathcal{A}_{risk}(s)}[A^{\\pi_{task}}(s,a)]+\\delta\nwhere\nÎ´\n>\n0\n\\delta>0\nrepresents the advantage gap between safe and risky actions.\nV-C\nMain Result\nTheorem 1\n(Action Correction Benefit)\nUnder Assumptions\n1\n-\n3\n, the policy improvement from FARLâ€™s corrected transitions satisfies:\nÎ”\nâ€‹\nJ\nF\nâ€‹\nA\nâ€‹\nR\nâ€‹\nL\nâ‰¥\nÎ”\nâ€‹\nJ\nb\nâ€‹\na\nâ€‹\ns\nâ€‹\ne\nâ€‹\nl\nâ€‹\ni\nâ€‹\nn\nâ€‹\ne\n+\nğ”¼\ns\nâˆ¼\nÏ\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n[\np\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\n]\nâ‹…\nÎ´\nâ‹…\n(\n1\nâˆ’\nÏµ\nr\nâ€‹\ne\nâ€‹\nc\n)\nâˆ’\nO\nâ€‹\n(\nÏµ\nr\nâ€‹\ne\nâ€‹\nc\n)\n\\begin{split}\\Delta J_{FARL}\\geq&\\Delta J_{baseline}\\\\\n&+\\mathbb{E}_{s\\sim\\rho_{\\pi_{task}}}[p_{risk}(s)]\\cdot\\delta\\cdot(1-\\epsilon_{rec})-O(\\epsilon_{rec})\\end{split}\n(13)\nwhere\np\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\n=\nâ„™\na\nâˆ¼\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n[\na\nâˆˆ\nğ’œ\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\n]\np_{risk}(s)=\\mathbb{P}_{a\\sim\\pi_{task}}[a\\in\\mathcal{A}_{risk}(s)]\nis the probability of sampling risky actions at state\ns\ns\n.\nProof:\nThe baseline improvement is\nÎ”\nâ€‹\nJ\nb\nâ€‹\na\nâ€‹\ns\nâ€‹\ne\nâ€‹\nl\nâ€‹\ni\nâ€‹\nn\nâ€‹\ne\n=\nğ”¼\ns\n,\na\nâˆ¼\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n[\nA\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n,\na\n)\n]\n\\Delta J_{baseline}=\\mathbb{E}_{s,a\\sim\\pi_{task}}[A^{\\pi_{task}}(s,a)]\n.\nFARLâ€™s action correction yields expected advantage:\nğ”¼\nF\nâ€‹\nA\nâ€‹\nR\nâ€‹\nL\nâ€‹\n[\nA\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n,\na\n)\n]\n\\displaystyle\\mathbb{E}_{FARL}[A^{\\pi_{task}}(s,a)]\n=\n(\n1\nâˆ’\np\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\n)\nâ€‹\nğ”¼\na\nâˆˆ\nğ’œ\ns\nâ€‹\na\nâ€‹\nf\nâ€‹\ne\nâ€‹\n(\ns\n)\nâ€‹\n[\nA\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n,\na\n)\n]\n\\displaystyle=(1-p_{risk}(s))\\mathbb{E}_{a\\in\\mathcal{A}_{safe}(s)}[A^{\\pi_{task}}(s,a)]\n+\np\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\nâ€‹\nğ”¼\na\nâˆ¼\nÏ€\nr\nâ€‹\ne\nâ€‹\nc\nâ€‹\n[\nA\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n,\na\n)\n]\n\\displaystyle\\quad+p_{risk}(s)\\mathbb{E}_{a\\sim\\pi_{rec}}[A^{\\pi_{task}}(s,a)]\n(14)\nThe improvement per state is:\nğ”¼\nF\nâ€‹\nA\nâ€‹\nR\nâ€‹\nL\nâ€‹\n[\nA\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n,\na\n)\n]\nâˆ’\nğ”¼\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n[\nA\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n,\na\n)\n]\n\\displaystyle\\mathbb{E}_{FARL}[A^{\\pi_{task}}(s,a)]-\\mathbb{E}_{\\pi_{task}}[A^{\\pi_{task}}(s,a)]\n=\np\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\nâ€‹\n(\nğ”¼\na\nâˆ¼\nÏ€\nr\nâ€‹\ne\nâ€‹\nc\nâ€‹\n[\nA\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n,\na\n)\n]\nâˆ’\nğ”¼\na\nâˆˆ\nğ’œ\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\nâ€‹\n[\nA\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n,\na\n)\n]\n)\n\\displaystyle=p_{risk}(s)\\left(\\mathbb{E}_{a\\sim\\pi_{rec}}[A^{\\pi_{task}}(s,a)]-\\mathbb{E}_{a\\in\\mathcal{A}_{risk}(s)}[A^{\\pi_{task}}(s,a)]\\right)\n(15)\nUnder Assumptions 2-3, this difference is at least\np\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\nâ‹…\nÎ´\nâ‹…\n(\n1\nâˆ’\nÏµ\nr\nâ€‹\ne\nâ€‹\nc\n)\nâˆ’\nO\nâ€‹\n(\nÏµ\nr\nâ€‹\ne\nâ€‹\nc\n)\np_{risk}(s)\\cdot\\delta\\cdot(1-\\epsilon_{rec})-O(\\epsilon_{rec})\n. Averaging over states yields the result.\nâˆ\nV-D\nDiscussion\nThe improvement bound\nğ”¼\ns\nâˆ¼\nÏ\nÏ€\nt\nâ€‹\na\nâ€‹\ns\nâ€‹\nk\nâ€‹\n[\np\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\n]\nâ‹…\nÎ´\nâ‹…\n(\n1\nâˆ’\nÏµ\nr\nâ€‹\ne\nâ€‹\nc\n)\nâˆ’\nO\nâ€‹\n(\nÏµ\nr\nâ€‹\ne\nâ€‹\nc\n)\n\\mathbb{E}_{s\\sim\\rho_{\\pi_{task}}}[p_{risk}(s)]\\cdot\\delta\\cdot(1-\\epsilon_{rec})-O(\\epsilon_{rec})\nshows FARL gains most when: (1) risky states are frequent (\nğ”¼\ns\nâ€‹\n[\np\nr\nâ€‹\ni\nâ€‹\ns\nâ€‹\nk\nâ€‹\n(\ns\n)\n]\n\\mathbb{E}_{s}[p_{risk}(s)]\nlarge), (2) safe actions significantly outperform risky ones (\nÎ´\n\\delta\nlarge), and (3) recovery demonstrations are high-quality (\nÏµ\nr\nâ€‹\ne\nâ€‹\nc\n\\epsilon_{rec}\nsmall).\nThis explains FARLâ€™s dual benefit: improved safety and enhanced performance by selectively replacing risky actions with recovery alternatives when necessary.\nVI\nExperiments\nVI-A\nFailureBench: Simulation Benchmark for Evaluating Failure-Aware RL\nTo evaluate failure-aware RL algorithms, we introduce FailureBench, a benchmark suite comprising modified versions of the MetaWorld environment\n[\n46\n]\n. FailureBench is designed to incorporate realistic failure scenarios that typically require human intervention in real-world settings. We categorize these failure scenarios into four representative tasks:\nâ€¢\nSawyer Bounded Push\n. The robot must push a puck to the target while keeping it within a bounded workspace. If it pushes the object beyond the boundary, it is considered an IR Failure, simulating scenarios where objects become unreachable or fall to the ground, necessitating human intervention.\nâ€¢\nSawyer Bounded Soccer\n. The robot must hit a ball into a goal while keeping it within a boundary. The dynamic nature of the rolling ball makes it more prone to IR Failures.\nâ€¢\nSawyer Fragile Push Wall\n. The robot must push an fragile object to a target position behind a wall. If the object collides with the wall, it is considered an IR Failure, simulating real-world scenarios where fragile objects would be damaged upon collision and require replacement.\nâ€¢\nSawyer Obstructed Push\n. The robot must navigate around a fragile vase while pushing the object to its goal location. Any collision with the vase is treated as an IR Failure.\nFigure 3\n:\nIllustration of the four tasks in our FailureBench. From left to right: Bounded Push, Bounded Soccer, Fragile Push Wall, and Obstructed Push.\nVI-B\nSimulation Experiments\nWe conduct extensive experiments using our proposed FailureBench to evaluate the effectiveness of FARL. We compare FARL against the baseline Uni-O4 algorithm\n[\n22\n]\n, which represents the state-of-the-art offline-to-online RL approach but without explicit failure avoidance mechanisms.\nAdditionally, we compare our method with three state-of-the-art online safe RL algorithms, i.e., PPO-Lagrangian\n[\n33\n]\n, P3O\n[\n49\n]\n, and CPO\n[\n1\n]\n, where each is used to fine-tune the same offline pre-trained policy.\nVI-B\n1\nExperimental Setup\nFor each environment in FailureBench, we collect three types of demonstration data for offline pre-training:\nâ€¢\nTask demonstrations\n: 20 medium expert trajectories for each task collected using BAC\n[\n18\n]\n, representing sub-optimal demonstration for manipulation tasks.\nâ€¢\nRecovery demonstrations\n: 120 trajectories demonstrating recovery behaviors from near-failure states collected with script policy.\nâ€¢\nFailure demonstrations\n:\n20kâ€“200k transition sequences containing failures for world model training. Specifically, we simulate online exploration by adding controlled stochastic noise during the deployment of the offline pre-trained policy. We then collect the failing trajectories, which match the distribution of failures likely to occur during online fine-tuning.\nWe evaluate performance using two key metrics:\nâ€¢\nAverage Return\n: The mean episodic reward improvement after\n10\n6\n10^{6}\nsteps of online finetuning.\nâ€¢\nFailure Episodes\n: The number of episodes containing IR Failures during\n10\n6\n10^{6}\nsteps of online finetuning.\nVI-B\n2\nResults and Analysis\nFigure\n4\ncompares failure episodes between our method FARL and the baseline Uni-O4 across all four environments. Our approach consistently encounters significantly fewer failure episodes with an average reduction of\n43.6%\nacross all tasks and up to\n65.8%\nin the most challenging environments. The larger improvements in highly dynamic environments (Bounded Soccer) and complex constraint scenarios (Obstructed Push) highlight our methodâ€™s strength in handling challenging interaction dynamics.\nFigure 4\n:\nComparison of average failure episodes during fine-tuning for Uni-O4 (blue) and our method (red) in FailureBench.\nFigure 5\n:\nPerformance comparison between Uni-O4 (blue) and our method (red) in FailureBench. The bars show average return before and after fine-tuning. All returns are normalized relative to an expert script policyâ€™s performance (100).\nAnalysis of average returns (Figure\n5\n) reveals that despite the additional constraints imposed by our failure-avoiding mechanisms, our approach achieves competitive or higher performance in terms of task rewards. This demonstrates that our method successfully balances exploration and safety, achieving effective learning without sacrificing performance.\nTraditional safe RL methods significantly underperform when applied to offline-to-online scenarios, despite careful hyperparameter tuning. Table\nI\nshows that FARL outperforms these approaches by an average of over 800% across tasks, suggesting a fundamental incompatibility between standard safe RL algorithms and pre-trained policy initialization. This performance gap likely stems from their optimization objectives creating distributional shifts that prevent effective utilization of pre-trained knowledge.\nTable I\n:\nPerformance comparison after fine-tuning from the same offline pre-trained policy. Values represent average returns across three seeds.\nMethod\nBounded Push\nBounded Soccer\nFragile Push Wall\nObstructed Push\nFARL (Ours)\n4593.96\nÂ±\n\\pm\n50.69\n2276.53\nÂ±\n\\pm\n554.32\n3220.12\nÂ±\n\\pm\n66.67\n1227.18\nÂ±\n\\pm\n12.40\nPPO-Lag\n420.79\nÂ±\n\\pm\n841.07\n404.80\nÂ±\n\\pm\n334.67\n268.83\nÂ±\n\\pm\n695.50\n543.95\nÂ±\n\\pm\n387.59\nP3O\n95.33\nÂ±\n\\pm\n61.51\n598.55\nÂ±\n\\pm\n653.84\n-278.62\nÂ±\n\\pm\n530.71\n455.86\nÂ±\n\\pm\n348.82\nCPO\n156.28\nÂ±\n\\pm\n212.07\n692.90\nÂ±\n\\pm\n267.52\n-994.55\nÂ±\n\\pm\n1139.89\n47.40\nÂ±\n\\pm\n37.26\nVI-B\n3\nAblation Studies\nTo better understand the contribution of each component in FARL, we conduct two key ablation studies focused on our failure prediction and avoidance mechanisms:\nStudy 1. World Model vs. Recovery-RL Safety Critic\nFirst, we investigate whether our world-model-based safety critic with future rollout prediction is necessary by replacing it with a Q-function safety critic from Recovery-RL\n[\n39\n]\n, which is based on a simple MLP.\nThis approach estimates the discounted future probability of constraint violation under the current policy:\nQ\nsafe\nâ€‹\n(\ns\nt\n,\na\nt\n)\n=\nğ”¼\nÏ€\nâ€‹\n[\nâˆ‘\nt\nâ€²\n=\nt\nâˆ\nÎ³\nsafe\nt\nâ€²\nâˆ’\nt\nâ€‹\nc\nt\nâ€²\n|\ns\nt\n,\na\nt\n]\n\\displaystyle Q_{\\text{safe}}(s_{t},a_{t})=\\mathbb{E}_{\\pi}\\left[\\sum_{t^{\\prime}=t}^{\\infty}\\gamma_{\\text{safe}}^{t^{\\prime}-t}c_{t^{\\prime}}|s_{t},a_{t}\\right]\n(16)\n=\nc\nt\n+\n(\n1\nâˆ’\nc\nt\n)\nâ€‹\nÎ³\nsafe\nâ€‹\nğ”¼\nÏ€\nâ€‹\n[\nQ\nsafe\nâ€‹\n(\ns\nt\n+\n1\n,\na\nt\n+\n1\n)\n|\ns\nt\n,\na\nt\n]\n.\n\\displaystyle=c_{t}+(1-c_{t})\\gamma_{\\text{safe}}\\mathbb{E}_{\\pi}\\left[Q_{\\text{safe}}(s_{t+1},a_{t+1})|s_{t},a_{t}\\right].\n(17)\nStudy 2. Recovery Policy vs. MPPI Planning\nSecond, we evaluate the necessity of a separate pre-trained recovery policy by replacing it with Model Predictive Path Integral (MPPI)\n[\n42\n]\n, a sampling-based model predictive control method. Our implementation plans actions by:\nâ€¢\nSampling multiple action trajectories from a Gaussian distribution.\nâ€¢\nComputing rewards and constraints for each trajectory using the world model.\nâ€¢\nFiltering trajectories based on a safety threshold (\nÎµ\nsafe\n\\varepsilon_{\\text{safe}}\n).\nâ€¢\nSelecting the best trajectory among safe options according to expected returns.\nResult Analysis\nTable\nII\npresents the failure episode results of our ablation studies. The performance across different variants reveals several interesting insights:\nReplacing our world-model-based safety critic with the simple critic from Recovery-RL\n[\n39\n]\nleads to a substantial increase in failure episodes in all environments, which is the most pronounced in Bounded Soccer with 92% more failures, where complex dynamics make accurate long-term risk assessment crucial. This confirms that our approachâ€™s explicit modeling of future state-action sequences provides superior prediction of potential failures compared to a simple Q-function estimator.\nMPPI planning underperforms our learned recovery policy, particularly in environments with complex dynamics. This suggests that planning-based methods that implicitly infer recovery action from the world model encounter difficulties in highly dynamic environments. Our recovery policy leverages expert demonstrations of recovery behaviors that would be difficult to discover through planning alone, providing a significant advantage in handling complex failure scenarios.\nThese results validate our design choices and demonstrate that our combined approach of a world-model-based safety critic and a pre-trained recovery policy provides the most robust performance across diverse failure scenarios.\nThe return comparison in Figure\n6\nfurther illustrates that our FARL consistently achieves superior final performance after fine-tuning.\nTable II\n:\nComparison of Average Failure Episodes across FailureBench environments.\nFailure Episodes\nâ†“\n\\downarrow\nFARL (Ours)\nRec-RL\nMPPI\nBounded Push\n951.00\nÂ±\n52.37\n\\mathbf{951.00\\pm 52.37}\n1250.67\nÂ±\n293.33\n1250.67\\pm 293.33\n1112.00\nÂ±\n180.01\n1112.00\\pm 180.01\nBounded Soccer\n878.67\nÂ±\n120.65\n\\mathbf{878.67\\pm 120.65}\n1687.00\nÂ±\n130.85\n1687.00\\pm 130.85\n2022.67\nÂ±\n279.74\n2022.67\\pm 279.74\nFragile Push Wall\n3393.00\nÂ±\n42.51\n\\mathbf{3393.00\\pm 42.51}\n4234.00\nÂ±\n56.93\n4234.00\\pm 56.93\n3753.00\nÂ±\n13.23\n3753.00\\pm 13.23\nObstructed Push\n1914.67\nÂ±\n711.46\n\\mathbf{1914.67\\pm 711.46}\n2175.00\nÂ±\n128.42\n2175.00\\pm 128.42\n2229.00\nÂ±\n727.12\n2229.00\\pm 727.12\nFigure 6\n:\nComparison of average returns before and after fine-tuning for our method and the two ablation variants. All returns are normalized relative to an expert script policyâ€™s performance (100).\nTable III\n:\nAverage return comparison for three Franka tasks.\nTask\nMethod\nInitial\nFinetuned\nFranka Fragile\nPush Wall\nUni-O4\n343.28\nÂ±\n\\pm\n121.23\n369.15\nÂ±\n\\pm\n8.93\nFARL (Ours)\n349.95\nÂ±\n\\pm\n123.12\n363.65\nÂ±\n\\pm\n14.38\nFranka Disturbed\nPush\nUni-O4\n262.62\nÂ±\n\\pm\n129.59\n308.29\nÂ±\n\\pm\n79.83\nFARL (Ours)\n321.49\nÂ±\n\\pm\n149.57\n384.52\nÂ±\n\\pm\n87.45\nFranka Bounded\nSoccer\nUni-O4\n512.79\nÂ±\n\\pm\n143.34\n615.97\nÂ±\n\\pm\n103.20\nFARL (Ours)\n578.77\nÂ±\n\\pm\n133.80\n638.96\nÂ±\n\\pm\n91.59\nVI-C\nReal-World Experiments\nTo validate the practical effectiveness of FARL, we deploy FARL on a Franka Emika Panda robot to evaluate its performance in challenging real-world scenarios. We implement three representative tasks and assess both failure reduction and performance improvement (Figure\nLABEL:fig:mainteaser\n).\nVI-C\n1\nExperimental Setup\nâ€¢\nFranka Fragile Push Wall.\nThe robot must push an object, which is assumed to be fragile, to a target position behind a wall. If the object collides with the wall during manipulation, it is considered damaged and requires replacement, indicating an IR Failure that necessitates human intervention.\nâ€¢\nFranka Disturbed Push.\nThe robot must push an object to a target while avoiding a dynamic obstacle (a decorative flower) that is randomly moved by a human operator, simulating unpredictable environmental changes.\nâ€¢\nFranka Bounded Soccer.\nThe robot must use a UMI gripper\n[\n7\n]\nto \"kick\" a ball toward a target on an uneven turf surface. If the ball rolls beyond the defined boundaries due to the irregular surface dynamics, it is considered an IR Failure.\nWe use an Intel RealSense D435 camera for real-time image acquisition. State estimation is performed using YOLOv8 object detection and color-based filtering to track the positions of objects in the workspace. The system operates at approximately 5Hz for both perception and control.\nFor each task, we conduct training sessions consisting of 50 episodes and compared FARL against the baseline algorithm. We collect 40-80 trajectories for three types of demonstration via teleoperation. Each demonstration type requires approximately 10-20 minutes of teleoperation, resulting in a total human effort of 30-60 minutes per task for collecting all three demonstration types. For the failure demonstrations, specifically, we guide the robot via teleoperation into various failure states that the robot would likely encounter during autonomous execution, providing representative data for safety critic pre-training.\nVI-C\n2\nResults and Analysis\nFigure 7\n:\nComparison of total failure state-action pairs during 50 training episodes in real-world experiments.\nFigure\n7\npresents a comparison of total failures during the 50-episode training phase for all environments. The results demonstrate a dramatic reduction in failures when using FARL compared to the baseline.\nTable\nIII\nshows the average return achieved by each method before and after fine-tuning, which further highlights the necessity and superiority of online RL finetuning. Online RL finetuning not only boosts task performance but also significantly decreases the standard deviation, indicating more robust policies. In the Disturbed Push and Bounded Soccer tasks, the finetuned policy achieves noticeably higher returns. This result signifies that FARL successfully leverages the privilege of online RL finetuning while minimizing failures requiring extensive human intervention in the real world.\nVII\nconclusion and future work\nWe introduce a failure-aware offline-to-online reinforcement learning framework that integrates a world-model-based safety critic and a recovery policy to reduce IR Failures during exploration significantly.\nLimitations and future work.\nOur current work does not incorporate additional modalities such as 2D/3D vision, tactile sensing, and others, which could further enrich the perceptual capabilities of the system. In future work, we plan to extend FARL to address mobile manipulation and dual-arm robotic platforms and to integrate these other modalities.\nAnother promising direction is to pre-train large-scale failure world models across diverse tasks and embodiments, potentially enabling better generalization to unseen failure modes in new domains.\nAcknowledgements\nWe would like to thank Zhecheng Yuan, Tianming Wei, Guangqi Jiang, and Xiyao Wang for their valuable discussions.\nThis work was supported by the Tsinghua University Dushi Program.\nReferences\n[1]\nJ. Achiam, D. Held, A. Tamar, and P. Abbeel\n(2017)\nConstrained policy optimization\n.\nIn\nICML\n,\nProceedings of Machine Learning Research\n, Vol.\n70\n,\npp.Â 22â€“31\n.\nCited by:\nÂ§II\n,\nÂ§\nVI-B\n.\n[2]\nE. Altman\n(2021)\nConstrained markov decision processes\n.\nCited by:\nÂ§II\n,\nÂ§III\n.\n[3]\nA. D. Ames, S. Coogan, M. Egerstedt, G. Notomista, K. Sreenath, and P. Tabuada\n(2019)\nControl barrier functions: theory and applications\n.\nIn\n2019 18th European Control Conference (ECC)\n,\nVol.\n,\npp.Â 3420â€“3431\n.\nExternal Links:\nDocument\nCited by:\nÂ§II\n.\n[4]\nD. Brandfonbrener, W. Whitney, R. Ranganath, and J. Bruna\n(2021)\nOffline RL without off-policy evaluation\n.\nIn\nNeurIPS\n,\npp.Â 4933â€“4946\n.\nCited by:\nÂ§II\n.\n[5]\nR. Cheng, G. Orosz, R. M. Murray, and J. W. Burdick\n(2019)\nEnd-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks\n.\nIn\nAAAI\n,\npp.Â 3387â€“3395\n.\nExternal Links:\nDocument\nCited by:\nÂ§II\n.\n[6]\nC. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song\n(2023)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nIn\nRSS\n,\nExternal Links:\nDocument\nCited by:\nÂ§I\n,\nÂ§II\n.\n[7]\nC. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake, and S. Song\n(2024)\nUniversal manipulation interface: in-the-wild robot teaching without in-the-wild robots\n.\nIn\nRSS\n,\nExternal Links:\nDocument\nCited by:\n3rd item\n.\n[8]\nY. Chow, O. Nachum, E. A. DuÃ©Ã±ez-GuzmÃ¡n, and M. Ghavamzadeh\n(2018)\nA lyapunov-based approach to safe reinforcement learning\n.\nIn\nNeurIPS\n,\npp.Â 8103â€“8112\n.\nCited by:\nÂ§II\n.\n[9]\nY. Chow, O. Nachum, A. Faust, E. Duenez-Guzman, and M. Ghavamzadeh\n(2019)\nLyapunov-based safe policy optimization for continuous control\n.\nArXiv preprint\nabs/1901.10031\n.\nCited by:\nÂ§II\n.\n[10]\nA. Cully, J. Clune, D. Tarapore, and J. Mouret\n(2015)\nRobots that can adapt like animals\n.\nNature\n521\n(\n7553\n),\npp.Â 503â€“507\n.\nCited by:\nÂ§II\n.\n[11]\nG. Dalal, K. Dvijotham, M. Vecerik, T. Hester, C. Paduraru, and Y. Tassa\n(2018)\nSafe exploration in continuous action spaces\n.\nArXiv preprint\nabs/1801.08757\n.\nCited by:\nÂ§II\n.\n[12]\nB. Eysenbach, S. Gu, J. Ibarz, and S. Levine\n(2018)\nLeave no trace: learning to reset for safe and autonomous reinforcement learning\n.\nIn\nICLR\n,\nCited by:\nÂ§I\n.\n[13]\nA. Gupta, J. Yu, T. Z. Zhao, V. Kumar, A. Rovinsky, K. Xu, T. Devlin, and S. Levine\n(2021)\nReset-free reinforcement learning via multi-task learning: learning dexterous manipulation behaviors without human intervention\n.\nIn\nICRA\n,\npp.Â 6664â€“6671\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[14]\nT. He, C. Zhang, W. Xiao, G. He, C. Liu, and G. Shi\n(2024)\nAgile but safe: learning collision-free high-speed legged locomotion\n.\nIn\nRSS\n,\nExternal Links:\nDocument\nCited by:\nÂ§II\n.\n[15]\nZ. He, K. Lei, Y. Ze, K. Sreenath, Z. Li, and H. Xu\n(2024)\nLearning visual quadrupedal loco-manipulation from demonstrations\n.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 9102â€“9109\n.\nCited by:\nÂ§II\n.\n[16]\nW. Huang, J. Ji, C. Xia, B. Zhang, and Y. Yang\n(2024)\nSafeDreamer: safe reinforcement learning with world models\n.\nIn\nThe Twelfth International Conference on Learning Representations\n,\nExternal Links:\nLink\nCited by:\nÂ§II\n.\n[17]\nT. Ji, Y. Liang, Y. Zeng, Y. Luo, G. Xu, J. Guo, R. Zheng, F. Huang, F. Sun, and H. Xu\n(2024)\nAce: off-policy actor-critic with causality-aware entropy regularization\n.\narXiv preprint arXiv:2402.14528\n.\nCited by:\nÂ§II\n.\n[18]\nT. Ji, Y. Luo, F. Sun, X. Zhan, J. Zhang, and H. Xu\n(2024)\nSeizing serendipity: exploiting the value of past success in off-policy actor-critic\n.\nIn\nICML\n,\nCited by:\n1st item\n.\n[19]\nG. Jiang, Y. Sun, T. Huang, H. Li, Y. Liang, and H. Xu\n(2024)\nRobots pre-train robots: manipulation-centric robotic representation from large-scale robot datasets\n.\narXiv preprint arXiv:2410.22325\n.\nCited by:\nÂ§II\n.\n[20]\nI. Kostrikov, A. Nair, and S. Levine\n(2022)\nOffline reinforcement learning with implicit q-learning\n.\nIn\nICLR\n,\nCited by:\nÂ§I\n,\nÂ§II\n.\n[21]\nA. Kumar, A. Zhou, G. Tucker, and S. Levine\n(2020)\nConservative q-learning for offline reinforcement learning\n.\nIn\nNeurIPS\n,\nCited by:\nÂ§I\n,\nÂ§II\n.\n[22]\nK. Lei, Z. He, C. Lu, K. Hu, Y. Gao, and H. Xu\n(2024)\nUni-o4: unifying online and offline deep reinforcement learning with multi-step on-policy optimization\n.\nIn\nICLR\n,\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§I\n,\nÂ§II\n,\nÂ§\nIV-A\n,\nÂ§\nVI-B\n.\n[23]\nK. Lei, H. Li, D. Yu, Z. Wei, L. Guo, Z. Jiang, Z. Wang, S. Liang, and H. Xu\n(2025)\nRl-100: performant robotic manipulation with real-world reinforcement learning\n.\narXiv preprint arXiv:2510.14830\n.\nCited by:\nÂ§II\n.\n[24]\nJ. Li, X. Hu, H. Xu, J. Liu, X. Zhan, and Y. Zhang\n(2023)\nPROTO: iterative policy regularized offline-to-online reinforcement learning\n.\nArXiv preprint\nabs/2305.15669\n.\nCited by:\nÂ§I\n.\n[25]\nQ. Liang, F. Que, and E. Modiano\n(2018)\nAccelerated primal-dual policy optimization for safe reinforcement learning\n.\nArXiv preprint\nabs/1802.06480\n.\nCited by:\nÂ§II\n.\n[26]\nY. Liang, Y. Sun, R. Zheng, and F. Huang\n(2022)\nEfficient adversarial training without attacking: worst-case-aware robust reinforcement learning\n.\nIn\nNeurIPS\n,\nCited by:\nÂ§II\n.\n[27]\nY. Liang, Y. Sun, R. Zheng, X. Liu, B. Eysenbach, T. Sandholm, F. Huang, and S. M. McAleer\n(2024)\nGame-theoretic robust reinforcement learning handles temporally-coupled perturbations\n.\nIn\nICLR\n,\nCited by:\nÂ§II\n.\n[28]\nP. Liu, H. Bou-Ammar, J. Peters, and D. Tateo\n(2024)\nSafe reinforcement learning on the constraint manifold: theory and applications\n.\nArXiv preprint\nabs/2404.09080\n.\nCited by:\nÂ§II\n.\n[29]\nX. Liu, C. Deng, Y. Sun, Y. Liang, and F. Huang\n(2024)\nBeyond worst-case attacks: robust rl with adaptive defense via non-dominated policies\n.\narXiv preprint arXiv:2402.12673\n.\nCited by:\nÂ§II\n.\n[30]\nZ. Liu, Z. Guo, Z. Cen, H. Zhang, J. Tan, B. Li, and D. Zhao\n(2023)\nOn the robustness of safe reinforcement learning under observational perturbations\n.\nIn\nICLR\n,\nCited by:\nÂ§II\n.\n[31]\nM. Nakamoto, S. Zhai, A. Singh, M. S. Mark, Y. Ma, C. Finn, A. Kumar, and S. Levine\n(2023)\nCal-ql: calibrated offline RL pre-training for efficient online fine-tuning\n.\nIn\nNeurIPS\n,\nCited by:\nÂ§I\n.\n[32]\nY. K. Nakka, A. Liu, G. Shi, A. Anandkumar, Y. Yue, and S. Chung\n(2020)\nChance-Constrained Trajectory Optimization for Safe Exploration and Learning of Nonlinear Systems\n.\nArXiv preprint\nabs/2005.04374\n.\nCited by:\nÂ§II\n.\n[33]\nA. Ray, J. Achiam, and D. Amodei\n(2019)\nBenchmarking safe exploration in deep reinforcement learning\n.\nArXiv preprint\nabs/1910.01708\n.\nCited by:\nÂ§II\n,\nÂ§\nVI-B\n.\n[34]\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov\n(2017)\nProximal Policy Optimization Algorithms\n.\nArXiv preprint\nabs/1707.06347\n.\nCited by:\nÂ§II\n.\n[35]\nA. Sharma, K. Xu, N. Sardana, A. Gupta, K. Hausman, S. Levine, and C. Finn\n(2022)\nAutonomous reinforcement learning: formalism and benchmarking\n.\nIn\nICLR\n,\nCited by:\nÂ§I\n.\n[36]\nM. Shridhar, L. Manuelli, and D. Fox\n(2023)\nPerceiver-actor: a multi-task transformer for robotic manipulation\n.\nIn\nCoRL\n,\npp.Â 785â€“799\n.\nCited by:\nÂ§I\n,\nÂ§II\n.\n[37]\nK. Srinivasan, B. Eysenbach, S. Ha, J. Tan, and C. Finn\n(2020)\nLearning to be Safe: Deep RL with a Safety Critic\n.\nArXiv preprint\nabs/2010.14603\n.\nCited by:\nÂ§II\n.\n[38]\nC. Tessler, D. J. Mankowitz, and S. Mannor\n(2019)\nReward constrained policy optimization\n.\nIn\nICLR\n,\nCited by:\nÂ§II\n.\n[39]\nB. Thananjeyan, A. Balakrishna, S. Nair, M. Luo, K. Srinivasan, M. Hwang, J. E. Gonzalez, J. Ibarz, C. Finn, and K. Goldberg\n(2021)\nRecovery rl: safe reinforcement learning with learned recovery zones\n.\nRAL\n6\n(\n3\n),\npp.Â 4915â€“4922\n.\nCited by:\nÂ§II\n,\nÂ§III\n,\nÂ§\nIV-A\n,\nÂ§\nVI-B\n3\n,\nÂ§\nVI-B\n3\n.\n[40]\nK. P. Wabersich and M. N. Zeilinger\n(2021)\nA predictive safety filter for learning-based control of constrained nonlinear dynamical systems\n.\nAutomatica\n129\n,\npp.Â 109597\n.\nExternal Links:\nISSN 0005-1098\n,\nDocument\n,\nLink\nCited by:\nÂ§II\n.\n[41]\nA. Wachi, X. Shen, and Y. Sui\n(2024)\nA survey of constraint formulations in safe reinforcement learning\n.\nIn\nProceedings of the Thirty-Third International Joint Conference on\nArtificial Intelligence, IJCAI 2024, Jeju, South Korea, August 3-9,\n2024\n,\npp.Â 8262â€“8271\n.\nCited by:\nÂ§II\n.\n[42]\nG. Williams, A. Aldrich, and E. Theodorou\n(2015)\nModel predictive path integral control using covariance variable importance sampling\n.\nVol.\nabs/1509.01149\n.\nCited by:\nÂ§\nVI-B\n3\n.\n[43]\nY. Wu, G. Tucker, and O. Nachum\n(2019)\nBehavior regularized offline reinforcement learning\n.\nArXiv preprint\nabs/1911.11361\n.\nCited by:\nÂ§I\n,\nÂ§II\n.\n[44]\nW. Xiao, T. He, J. M. Dolan, and G. Shi\n(2024)\nSafe deep policy adaptation\n.\nIn\nICRA\n,\npp.Â 17286â€“17292\n.\nExternal Links:\nDocument\nCited by:\nÂ§II\n.\n[45]\nG. Xu, R. Zheng, Y. Liang, X. Wang, Z. Yuan, T. Ji, Y. Luo, X. Liu, J. Yuan, P. Hua,\net al.\n(2023)\nDrm: mastering visual reinforcement learning through dormant ratio minimization\n.\narXiv preprint arXiv:2310.19668\n.\nCited by:\nÂ§II\n.\n[46]\nT. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine\n(2019)\nMeta-world: a benchmark and evaluation for multi-task and meta reinforcement learning\n.\nVol.\nabs/1910.10897\n.\nCited by:\nÂ§I\n,\nÂ§\nVI-A\n.\n[47]\nZ. Yuan, T. Wei, L. Gu, P. Hua, T. Liang, Y. Chen, and H. Xu\n(2025)\nHermes: human-to-robot embodied learning from multi-source motion data for mobile dexterous manipulation\n.\narXiv preprint arXiv:2508.20085\n.\nCited by:\nÂ§II\n.\n[48]\nZ. Yuan, S. Yang, P. Hua, C. Chang, K. Hu, and H. Xu\n(2023)\nRL-vigen: a reinforcement learning benchmark for visual generalization\n.\nExternal Links:\n2307.10224\nCited by:\nÂ§II\n.\n[49]\nL. Zhang, L. Shen, L. Yang, S. Chen, X. Wang, B. Yuan, and D. Tao\n(2022)\nPenalized proximal policy optimization for safe reinforcement learning\n.\nIn\nProceedings of the Thirty-First International Joint Conference on\nArtificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July\n2022\n,\npp.Â 3744â€“3750\n.\nExternal Links:\nDocument\nCited by:\nÂ§II\n,\nÂ§\nVI-B\n.\n[50]\nZ. Zhuang, K. Lei, J. Liu, D. Wang, and Y. Guo\n(2023)\nBehavior proximal policy optimization\n.\nIn\nICLR\n,\nCited by:\nÂ§II\n.\nHP\nhigh-pass\nLP\nlow-pass",
    "preview_text": "Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.\n\nFailure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation\nHuanyu Li\n1,2âˆ—\n,\nKun Lei\n1,2âˆ—\n,\nSheng Zang\n4\n,\nKaizhe Hu\n1,3\n,\nYongyuan Liang\n6\n,\nBo An\n4\n,\nXiaoli Li\n5\n,\nHuazhe Xu\n1,3\nâˆ—\nEqual contribution.\n1\nShanghai Qi Zhi Institute,\n2\nShanghai Jiao Tong University,\n3\nIIIS, Tsinghua University,\n4\nNanyang Technological University,\n5\nA*STAR Institute for Infocomm Research,\n6\nUniversity of Maryland.\nAbstract\nPost-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness.\nHowever, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during r",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¤±è´¥æ„ŸçŸ¥çš„ç¦»çº¿åˆ°åœ¨çº¿å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œä¸“æ³¨äºå‡å°‘æœºå™¨äººæ“ä½œä¸­çš„å¹²é¢„æ€§å¤±è´¥ï¼Œä»¥æé«˜å®é™…éƒ¨ç½²çš„å¯é æ€§ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T18:53:11Z",
    "created_at": "2026-01-21T12:09:12.926706",
    "updated_at": "2026-01-21T12:09:12.926714",
    "recommend": 0
}