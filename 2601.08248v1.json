{
  "id": "2601.08248v1",
  "title": "Spiking Neural-Invariant Kalman Fusion for Accurate Localization Using Low-Cost IMUs",
  "authors": [
    "Yaohua Liu",
    "Qiao Xu",
    "Yemin Wang",
    "Hui Yi Leong",
    "Binkai Ou"
  ],
  "abstract": "Low-cost inertial measurement units (IMUs) are widely utilized in mobile robot localization due to their affordability and ease of integration. However, their complex, nonlinear, and time-varying noise characteristics often lead to significant degradation in localization accuracy when applied directly for dead reckoning. To overcome this limitation, we propose a novel brain-inspired state estimation framework that combines a spiking neural network (SNN) with an invariant extended Kalman filter (InEKF). The SNN is designed to extract motion-related features from long sequences of IMU data affected by substantial random noise and is trained via a surrogate gradient descent algorithm to enable dynamic adaptation of the covariance noise parameter within the InEKF. By fusing the SNN output with raw IMU measurements, the proposed method enhances the robustness and accuracy of pose estimation. Extensive experiments conducted on the KITTI dataset and real-world data collected using a mobile robot equipped with a low-cost IMU demonstrate that the proposed approach outperforms state-of-the-art methods in localization accuracy and exhibits strong robustness to sensor noise, highlighting its potential for real-world mobile robot applications.",
  "url": "https://arxiv.org/abs/2601.08248v1",
  "html_url": "https://arxiv.org/html/2601.08248v1",
  "html_content": "Spiking Neural-Invariant Kalman Fusion for Accurate Localization Using Low-Cost IMUs\nYaohuaÂ Liu, QiaoÂ Xu, YeminÂ Wang, Hui YiÂ Leong, BinkaiÂ Ou\nY. Liu is with Guangdong Institute of Intelligence Science and Technology,\nHengqin, Zhuhai, Guangdong, China, 519031, e-mail: liuyaohua@gdiist.cn.Q. Xu is with East China Normal University, Shanghai, 200062, ChinaY. Wang is with Xiamen University, Xiamen, Fujian, 360000, ChinaH. Leng is with University of Chicago, AmericaB. Ou is with Innovation and Research and Development Department, BoardWare Information System Co.Ltd, Macau, 999078, China.\nAbstract\nLow-cost inertial measurement units (IMUs) are widely utilized in mobile robot localization due to their affordability and ease of integration. However, their complex, nonlinear, and time-varying noise characteristics often lead to significant degradation in localization accuracy when applied directly for dead reckoning. To overcome this limitation, we propose a novel brain-inspired state estimation framework that combines a spiking neural network (SNN) with an invariant extended Kalman filter (InEKF). The SNN is designed to extract motion-related features from long sequences of IMU data affected by substantial random noise and is trained via a surrogate gradient descent algorithm to enable dynamic adaptation of the covariance noise parameter within the InEKF. By fusing the SNN output with raw IMU measurements, the proposed method enhances the robustness and accuracy of pose estimation. Extensive experiments conducted on the KITTI dataset and real-world data collected using a mobile robot equipped with a low-cost IMU demonstrate that the proposed approach outperforms state-of-the-art methods in localization accuracy and exhibits strong robustness to sensor noise, highlighting its potential for real-world mobile robot applications.\nIndex Terms:\nRobot localization, spiking neural network, inertial measurement unit(IMU), invariant\nextended Kalman filter.\nI\nIntroduction\nAccurate localization is a fundamental requirement for intelligent vehicles, enabling them to determine their precise position within a given environment. The Global Navigation Satellite System (GNSS) is widely employed for this purpose, offering continuous, centimeter-level positioning. However, GNSS is inherently limited by a low update rate and is highly susceptible to multipath effects and signal loss in dense urban areas or enclosed environments such as tunnels. Vision-based localization systems, which rely on cameras or LiDAR, also face considerable challenges in scenarios with poor lighting or insufficient visual features. In contrast, inertial measurement units (IMUs) provide attitude and position estimates by integrating measurements from three-axis gyroscopes and accelerometers, making them an essential component in mobile robotic systems. When combined with other sensors, IMUs can deliver high-frequency inertial data and maintain pose estimation during temporary failures of external sensors. While high-precision IMUs offer excellent performance, their cost renders them impractical for commercial autonomous robots. As an alternative, low-cost IMUs based on micro-electromechanical systems (MEMS) technology have become increasingly popular due to their compact size and energy efficiency. Nonetheless, these sensors exhibit complex, nonlinear, and time-varying noise characteristics, which significantly degrade localization accuracy when directly used for dead reckoning.\nBayesian filters, particularly various forms of the Kalman filter, are commonly used to integrate IMU data for improved pose estimation during dead reckoning. This process relies on real-time estimation of a vehicleâ€™s position, velocity, and orientation by integrating accelerometer and gyroscope measurements through kinematic models. To mitigate inevitable error accumulation from IMU integration, Kalman filters dynamically fuse predictive and observational data, with uncertainty modeled through a covariance noise matrix. However, specifying this parameter poses two major challenges: it is susceptible to system observability, making manual tuning difficult, and it requires real-time adaptation to accommodate nonlinear motion dynamics, such as amplified lateral slip variance during curved trajectories compared to straight-line motion. To address these issues, AI-enhanced methods have been proposed to learn and adapt the noise parameters for low-cost IMUs automatically\n[\n1\n,\n2\n,\n3\n]\n. Nevertheless, most existing approaches rely on convolutional or recurrent neural networks, which are often inadequate in extracting robust motion features from long IMU sequences corrupted by substantial random noise.\nSpiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are inspired by the spatiotemporal dynamics of biological neurons and offer a distinctive framework for processing temporal information. Recent studies have demonstrated that SNNs can achieve performance comparable to traditional deep learning models in various time-series tasks\n[\n4\n,\n5\n,\n6\n]\n. In the context of dead reckoning, which inherently involves sequential IMU measurements over time windows for state estimation, SNNs are well-suited to capture temporal patterns and extract spatial features from noisy IMU data. This enables the real-time adaptation of noise parameters, a capability particularly beneficial for low-cost MEMS IMUs that are prone to significant noise and bias\n[\n7\n]\n. By leveraging the temporal coding and event-driven characteristics of SNNs, the robustness and accuracy of state estimation in dead reckoning can be significantly improved, especially in challenging environments where conventional methods often fail.\nIn this paper, we propose a hybrid state estimation framework that integrates an SNN with an invariant extended Kalman filter (InEKF) to enable accurate dead reckoning using low-cost MEMS IMUs. The SNN is designed to extract spatial features from IMU data and dynamically adjust the noise covariance parameters of the InEKF in real-time, all with minimal computational overhead. The InEKF maintains invariance to certain group transformations, such as rotations and translations, thereby improving robustness under noisy and uncertain conditions. The main contributions of this paper are\nsummarized as follows:\nâ€¢\nWe propose a brain-inspired fusion method that integrates an SNN with an InEKF to achieve accurate dead reckoning for low-cost MEMS IMUs. To the best of our knowledge, this is the first work that combines SNNs with InEKF for IMU dead reckoning. In comparison to traditional deep learning methods, the proposed method is more efficient in\nprocessing temporal information and can extract spatial features from low-cost\nMEMS IMU data with minimal computational cost.\nâ€¢\nA hybrid state estimation strategy is developed in which the SNN not only performs feature extraction but also enables adaptive noise modeling, while the InEKF provides consistent and transformation-invariant state estimation\nâ€¢\nExtensive experiments on the KITTI odometry benchmark and real field tests demonstrate that the proposed method significantly improves localization accuracy and robustness, particularly in degraded sensor scenarios such as IMU drift or GPS signal loss, validating the effectiveness of the approach for practical low-cost inertial navigation applications.\nThe remainder of this paper is organized as follows. Section II reviews related work on IMU-based dead reckoning and recent AI-enhanced IMU methods. Section III introduces the IMU error model, the underlying kinematic model, and the theoretical foundations of the InEKF and SNNs. Section IV details the proposed hybrid dead reckoning framework, including SNN-based feature extraction and noise parameter adaptation, as well as InEKF-based state estimation. Section V presents experimental results and analysis on the KITTI odometry dataset and real-world field tests, validating the performance and robustness of the proposed method. Finally, Section VI concludes the paper and outlines directions for future research.\nII\nRelated Works\nErrors in low-cost MEMS IMUs are typically substantial and arise from various sources, including bias instability, scale factor variations, and sensor misalignment. To enhance the accuracy of such IMUs, calibration is generally necessary. Classical calibration methods in inertial navigation are broadly categorized into discrete and system-level approaches\n[\n8\n]\n. Discrete calibration methods, such as the Allan variance technique\n[\n9\n]\n, estimate IMU errors by analyzing the statistical characteristics of sensor outputs. The Kalibr library\n[\n10\n]\nis a widely adopted toolkit for offline calibration, capable of estimating IMU intrinsic parameters and camera-IMU extrinsic parameters. Lu et al.\n[\n11\n]\nproposed a comprehensive calibration method for six-axis skewed IMUs using a two-step procedure with complexity comparable to conventional techniques. In another study\n[\n12\n]\n, a multi-IMU array system with weighted fusion significantly reduced measurement noise and improved accuracy. However, discrete calibration methods typically require specialized equipment such as precision turntables and involve high operational costs, making them impractical for the mass deployment of low-cost MEMS IMUs.\nSystem-level calibration approaches primarily leverage Kalman filter-based techniques\n[\n13\n]\n, which estimate IMU errors by fusing IMU measurements with data from complementary sensors such as GPS or LiDAR. For example, Jung et al.\n[\n14\n]\nproposed a self-calibrating visual-inertial odometry (VIO) system that employs an extended Kalman filter (EKF) to estimate IMU scale factor and misalignment errors. To improve robustness during external sensor outages, the Zero-Velocity Update (ZUPT) technique has been introduced to enhance EKF performance by incorporating instantaneous velocity constraints as pseudo-observations\n[\n15\n]\n. Building on Lie group theory, Barrau and Bonnabel\n[\n16\n]\ndeveloped the InEKF, which incorporates geometric invariance principles to maintain consistent and robust state estimation, particularly for wheeled robots operating under nonholonomic constraints. While system-level calibration methods reduce the reliance on specialized equipment like turntables, they typically assume a simplified IMU error model and struggle to capture its inherent nonlinearities. Consequently, achieving high-precision localization using such model-based methods often requires either highly accurate system modeling or meticulous parameter tuningâ€”both of which are challenging and costly in the context of low-cost MEMS IMUs.\nIn recent years, AI-enhanced IMU methods have been proposed to automatically learn IMU error characteristics and adapt noise parameters dynamically for low-cost MEMS IMUs. These approaches leverage deep learning models to extract features from raw IMU data and adjust the noise covariance of Kalman filters in real-time. For instance, Brossard et al.\n[\n1\n]\nintroduced a convolutional neural network (CNN)-based framework to adaptively tune filter parameters from IMU inputs, while Zhou et al.\n[\n3\n]\nemployed a recurrent neural network (RNN) to model temporal dependencies in IMU errors. Guo et al.\n[\n2\n]\nproposed a hybrid approach that combines model-based constraints with learning-based adaptation to enhance dead reckoning accuracy in wheeled robots. Although these methods demonstrate promising results in improving localization performance using low-cost IMUs, their reliance on CNNs and RNNs limits their ability to capture long-term dependencies and extract motion-relevant features from IMU sequences heavily corrupted by random noise. To address this, Guyard et al.\n[\n17\n]\nproposed a Transformer-based bidirectional encoder for IMU-GPS fusion during GPS outages, showing improved modeling of long-term temporal patterns. However, such deep learning methods typically require large-scale, high-quality datasets, which constrain their generalization in low-data regimes. In contrast, SNNs offer greater efficiency and robustness in temporal feature extraction and can process low-cost MEMS IMU data with minimal computational cost\n[\n6\n]\n. Compared to the aforementioned work, our proposed hybrid state estimation method that integrates a spike neural network with an invariant extended Kalman filter can achieve more accurate dead reckoning with a single low-cost MEMS IMU.\nIII\nLow-Cost MEMS IMU and Dead Reckoning Modeling\nIII-A\nLow-Cost MEMS IMU Error Model\nThe IMU is a critical sensor in mobile robot navigation, providing measurements of angular velocity and linear acceleration. However, IMU data are affected by various error sources, including bias, scale factor distortion, sensor misalignment, and stochastic noise. These inaccuracies can significantly degrade the reliability and accuracy of state estimation in mobile robotic systems. To account for these effects, we model the IMU measurement errors by considering the following contributing factors:\nâ€¢\nBias\n: Bias refers to a constant or slow time-varying error that systematically affects IMU measurements. It can arise from various sources, including temperature fluctuations, mechanical stress, and imperfections introduced during manufacturing. In practice, IMU bias is commonly modeled as a random walk process, wherein the bias at time t is assumed to be equal to the bias at time t-1 perturbed by a zero-mean Gaussian noise term. This formulation captures the gradual and stochastic nature of bias drift over time.\nâ€¢\nScale factor\n: The scale factor represents a multiplicative error that affects the amplitude of IMU measurements. Similar to bias, it can result from temperature variations, mechanical stress, or manufacturing imperfections. In most cases, the scale factor is modeled as a constant or slowly time-varying parameter, reflecting its relatively stable yet imperfect calibration over time. This error leads to proportional distortion in the measured angular velocity and linear acceleration, thereby impacting the accuracy of state estimation if uncorrected.\nâ€¢\nMisalignment\n: Misalignment refers to angular deviations between the actual sensor axes and the ideal reference axes, resulting in cross-axis sensitivity and distortion in IMU measurements. This error is typically introduced by mechanical stress, structural deformation, or manufacturing imperfections. Misalignment is commonly modeled as a constant or slowly time-varying parameter, as its variation tends to be gradual over time. If not properly compensated, it can lead to significant degradation in the accuracy of attitude and position estimation.\nâ€¢\nNoise\n: Random noise is an inherent stochastic error that affects IMU measurements and arises from various sources, including thermal fluctuations, quantization effects, and electromagnetic interference. This type of noise is typically modeled as zero-mean Gaussian white noise, characterized by a predefined covariance matrix that encapsulates the uncertainty of the sensor. Accurate modeling of this noise is essential for reliable state estimation, as it directly influences the performance of filtering and sensor fusion algorithms.\nBased on the above factors, the IMU measurements and the true values can be\ndescribed as, see\n[\n18\n]\n,\n[\n19\n]\n,\nğ\n~\nt\nI\nâ€‹\nM\nâ€‹\nU\n=\nğ‚\n1\nT\nâ€‹\nğ\nt\n+\nğ›\nt\nÏ‰\n+\nğœ¼\nt\nÏ‰\n{\\bm{\\tilde{\\omega}}}_{t}^{{IMU}}={\\bf{C}}_{{}_{1}}^{T}{{\\bm{\\omega}}_{t}}+{\\bf{b}}_{t}^{\\omega}+{\\bm{\\eta}}_{t}^{\\omega}\n(1)\nğš\n~\nt\nI\nâ€‹\nM\nâ€‹\nU\n=\nğ‚\n2\nT\nâ€‹\nğš\nğ­\n+\nğ›\nt\na\n+\nğœ¼\nt\na\n{\\bf{\\tilde{a}}}_{t}^{{IMU}}={\\bf{C}}_{{}_{2}}^{T}{\\bf{{a}}_{t}}+{\\bf{b}}_{t}^{a}+{\\bm{\\eta}}_{t}^{a}\n(2)\nwhere\nğ\n~\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bm{\\tilde{\\omega}}}_{t}^{{IMU}}\nand\nğš\n~\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bf{\\tilde{a}}}_{t}^{{IMU}}\nare\nthe angular velocity and linear acceleration measurements of the IMU at time\nt\nt\n,\nrespectively;\nğ\nt\n{{\\bm{\\omega}}_{t}}\nand\nğš\nt\n{{\\bf{a}}_{t}}\ndenote the true angular\nvelocity and linear acceleration, respectively;\nğ›\nt\nÏ‰\n{\\bf{b}}_{t}^{\\omega}\nand\nğ›\nt\na\n{\\bf{b}}_{t}^{a}\nrepresent the gyroscope and accelerometer biases, respectively;\nğœ¼\nt\nÏ‰\n{\\bm{\\eta}}_{t}^{\\omega}\nand\nğœ¼\nt\na\n{\\bm{\\eta}}_{t}^{a}\nare the gyroscope and accelerometer noise,\nand they can be regarded as zero-mean Gaussian noise;\nğ‚\n1\nT\n{\\bf{C}}_{{}_{1}}^{T}\nand\nğ‚\n2\nT\n{\\bf{C}}_{{}_{2}}^{T}\nare the calibration matrix of gyroscope and accelerometer,\nrespectively. Therefore, the calibration matrix\nC\ncan be further\nrepresented as,\nğ‚\n=\n[\nğ‚\n1\nT\nğ‚\n2\nT\n]\n=\n[\nğ’\nÏ‰\nâ€‹\nğŒ\nÏ‰\nğ€\nğŸ\n3\nÃ—\n3\nğ’\na\nâ€‹\nğŒ\na\n]\n{\\bf{C}}=\\left[{\\begin{array}[]{*{20}{c}}{{\\bf{C}}_{1}^{T}}\\\\\n{{\\bf{C}}_{2}^{T}}\\end{array}}\\right]=\\left[{\\begin{array}[]{*{20}{c}}{{{\\bf{S}}_{\\omega}}{{\\bf{M}}_{\\bf{\\omega}}}}&{\\bf{A}}\\\\\n{{{\\bf{0}}_{{3\\times 3}}}}&{{{\\bf{S}}_{a}}{{\\bf{M}}_{a}}}\\end{array}}\\right]\n(3)\nwhere\nğ’\nÏ‰\n{{{\\bf{S}}_{\\omega}}}\nand\nğ’\na\n{{{\\bf{S}}_{a}}}\nare the scale factor of gyroscope\nand accelerometer, respectively;\nğŒ\nÏ‰\n{{{\\bf{M}}_{\\omega}}}\nand\nğŒ\na\n{{{\\bf{M}}_{a}}}\ndenote the axis misalignments, and\nğ€\n{\\bf{A}}\nis the coefficient matrix\nrepresenting the cross-coupling effects between the gyroscope and\naccelerometer, a.k.a. g-sensitivity.\nIII-B\nKinematic Model of the Mobile Robot\nThe kinematic model of a mobile robot characterizes the relationship between its motion and state variables, including attitude, velocity, and position. In this study, we focus on a wheeled mobile robot subject to nonholonomic constraints, which restrict its motion to the forward or backward direction along its instantaneous heading. Under this assumption, the attitude increment of the robot can be described as follows\n[\n8\n]\n,\nğ‘\nt\nI\nâ€‹\nM\nâ€‹\nU\n=\nğ‘\nt\nâˆ’\n1\nI\nâ€‹\nM\nâ€‹\nU\nâ€‹\nexp\nâ¡\n(\nğœ½\nt\nâˆ’\n1\n)\n{\\bf{R}}_{t}^{{IMU}}={\\bf{R}}_{{t-1}}^{{IMU}}\\exp({{\\bm{\\theta}}_{{t-1}}})\n(4)\nwhere\nğ‘\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bf{R}}_{t}^{{IMU}}\nis the rotation matrix of the mobile robot at\ntime\nt\nt\n, i.e., that maps the IMU frame to the world frame;\nğ‘\nt\nâˆ’\n1\nI\nâ€‹\nM\nâ€‹\nU\n{\\bf{R}}_{{t-1}}^{{IMU}}\nis the rotation matrix of the mobile robot at\ntime\nt\nâˆ’\n1\nt-1\n; and\nğœ½\nt\nâˆ’\n1\n{\\bm{\\theta}}_{{t-1}}\nis the angular velocity vector of the\nmobile robot at time\nt\nâˆ’\n1\nt-1\n, which can be obtained from the gyroscope\nmeasurements as follows,\nğœ½\nt\nâˆ’\n1\n=\nğ\n~\nt\nâˆ’\n1\nI\nâ€‹\nM\nâ€‹\nU\nâ€‹\nd\nâ€‹\nt\n{{\\bm{\\theta}}_{{t-1}}}={\\bm{\\tilde{\\omega}}}_{{t-1}}^{{IMU}}dt\n(5)\nThe\nexp\nâ¡\n(\nğœ½\nt\nâˆ’\n1\n)\n\\exp({{\\bm{\\theta}}_{{t-1}}})\ncan be further expressed as,\nexp\n(\nğœ½\nt\nâˆ’\n1\n)\n=\nğˆ\n+\nsin\nâ¡\nÎ¸\nt\nâˆ’\n1\nÎ¸\nt\nâˆ’\n1\n[\nÎ¸\nt\nâˆ’\n1\nÃ—\n]\n+\n1\nâˆ’\ncos\nâ¡\nÎ¸\nt\nâˆ’\n1\nÎ¸\nt\nâˆ’\n1\n2\n[\nÎ¸\nt\nâˆ’\n1\nÃ—\n]\n2\n\\exp({{\\bm{\\theta}}_{{t-1}}})={\\bf{I}}+\\frac{{\\sin{\\theta_{t-1}}}}{{{\\theta_{t-1}}}}[{\\theta_{t-1}}\\times]+\\frac{{1-\\cos{\\theta_{t-1}}}}{{\\theta_{t-1}^{2}}}{[{\\theta_{t-1}}\\times]^{2}}\n(6)\nwhere\nğˆ\n{\\bf{I}}\nis the identity matrix, and\n[\nÎ¸\nt\nâˆ’\n1\nÃ—\n]\n[{\\theta_{t-1}}\\times]\nis the\nskew-symmetric matrix of the angular vector\nÎ¸\nt\nâˆ’\n1\n{\\bf{\\theta}}_{{t-1}}\n. The\nvelocity increment of the mobile robot can be described by the following,\nğ¯\nt\n=\nğ¯\nt\nâˆ’\n1\n+\n(\nğ‘\nt\nâˆ’\n1\nâ€‹\nğš\n~\nt\nâˆ’\n1\nâˆ’\nğ \n)\nâ€‹\nd\nâ€‹\nt\n{{\\bf{v}}_{t}}={{\\bf{v}}_{{t-1}}}+({{\\bf{R}}_{{t-1}}}{{{\\bf{\\tilde{a}}}}_{t-1}}-{\\bf{g}})dt\n(7)\nwhere\nğ¯\nğ­\n{\\bf{v}}_{\\bf{t}}\nis the velocity vector of the mobile robot at time\nt\nt\n,\nand\ng\ng\nis gravity.\nThe position is updated by integrating the velocity over time,\nğ©\nt\n=\nğ©\nt\nâˆ’\n1\n+\nğ¯\nt\nâˆ’\n1\nâ€‹\nd\nâ€‹\nt\n{{\\bf{p}}_{t}}={{\\bf{p}}_{{t-1}}}+{{\\bf{v}}_{{t-1}}}dt\n(8)\nwhere\nğ©\nt\n{\\bf{p}}_{t}\nis the position vector of the mobile robot at time\nt\nt\n.\nThe errors described above propagate through the motion equations Eq.\n4\n-Eq.\n8\n, ultimately affecting the estimated rotation, velocity, and position of the mobile robot, with their impact accumulating over time. Consequently, effective denoising of IMU measurements and accurate state estimation are essential for reliable dead reckoning. To address this, we propose a method that integrates the IMU error model with a spiking Transformer architecture, which enables dynamic adjustment of model parameters and facilitates the extraction of cleaner, denoised IMU signals for improved state estimation.\nIII-C\nInvariant Extended Kalman Filter\nThe InEKF is a variant of the classical EKF that leverages geometric invariance principles to enhance the robustness of state estimation in systems subject to nonholonomic constraints\n[\n16\n]\n. Unlike the standard EKF, the InEKF is formulated on Lie groups, ensuring consistency and invariance under transformations such as rotations and translations. This property is particularly beneficial for mobile robotic systems. The system state in the InEKF framework can be formulated as follows,\nğ±\nt\n=\nf\nâ€‹\n(\nğ±\nt\nâˆ’\n1\n,\nğ®\nt\nâˆ’\n1\n)\n+\nğ°\nt\nâˆ’\n1\n{\\bf{x}}_{t}={f}({\\bf{x}}_{{t-1}},{\\bf{u}}_{t-1})+{\\bf{w}}_{t-1}\n(9)\nwhere\nğ±\nt\n{\\bf{x}}_{t}\nis the state vector of the system at time\nt\nt\n,\nğŸ\n{\\bf{f}}\nis the state transition function that describes the dynamics of the system,\nğ®\nt\nâˆ’\n1\n{\\bf{u}}_{t-1}\nis the control input at time\nt\nâˆ’\n1\nt-1\n, and\nğ°\nt\nâˆ’\n1\n{\\bf{w}}_{t-1}\nis the\nprocess noise, which is assumed to be Gaussian with zero mean and a certain covariance\nmatrix\nğ\nt\nâˆ’\n1\n{\\bf{Q}}_{t-1}\n.\nThe measurement model of the InEKF can be described as follows,\nğ³\nt\n=\nh\nâ€‹\n(\nğ±\nt\n)\n+\nğ¯\nt\n{\\bf{z}}_{t}={h}({\\bf{x}}_{t})+{\\bf{v}}_{t}\n(10)\nwhere\nğ³\nğ­\n{\\bf{z}}_{\\bf{t}}\nis the measurement vector at time\nt\nt\n,\nğ¡\n{\\bf{h}}\nis\nthe measurement function that maps the state vector to the measurement space,\nand\nğ¯\nt\n{\\bf{v}}_{t}\nis the measurement noise, which is also assumed to be Gaussian\nwith zero mean and a certain covariance matrix\nğ‘\nt\n{\\bf{R}}_{t}\n.\nThe InEKF initializes with a Gaussian prior distribution over the state vector,\ndenoted as\nğ’©\nâ€‹\n(\nğ±\n^\n0\n,\nğ\n0\n)\n\\mathcal{N}({{{\\bf{\\hat{x}}}}_{0}},{{\\bf{P}}_{0}})\n, where\nğ±\n^\n0\n{{{\\bf{\\hat{x}}}}_{0}}\nis the initial state estimate and\nğ\n0\n{{\\bf{P}}_{0}}\nis the initial covariance matrix.\nThe InEKF then iteratively updates the state estimate and covariance matrix\nbased on the incoming measurements and the system dynamics. The process of InEKF\ncan be divided into two main steps: prediction and correction. The prediction step\nof the InEKF can be described as follows:\nğ\nt\n=\nğ…\nt\nâˆ’\n1\nâ€‹\nğ\nt\nâˆ’\n1\nâ€‹\nğ…\nt\nâˆ’\n1\nT\n+\nğ†\nt\nâˆ’\n1\nâ€‹\nğ\nt\nâˆ’\n1\nâ€‹\nğ†\nt\nâˆ’\n1\nT\n{{\\bf{P}}_{t}}={{\\bf{F}}_{t-1}}{{\\bf{P}}_{t-1}}{\\bf{F}}_{t-1}^{T}+{{\\bf{G}}_{t-1}}{{\\bf{Q}}_{t-1}}{\\bf{G}}_{t-1}^{T}\n(11)\nwhere\nğ…\nt\nâˆ’\n1\n{{\\bf{F}}_{t-1}}\nand\nğ†\nt\nâˆ’\n1\n{{\\bf{G}}_{t-1}}\nis the Jacobian matrix of\nthe state transition function\nf\n{f}\nwith respect to the state vector and the\nprocess noise. During the measurement update phase, the filter incorporates\npseudo-observations through the Kalman gain mechanism.\nIn practical implementations of InEKF, precise specification of the system dynamics\nf\nf\nand observation model\nh\nh\n, along with their associated process noise\ncovariance\nğ\nt\n{{\\bf{Q}}_{t}}\nand measurement noise covariance\nğ\nt\n{{\\bf{N}}_{t}}\n, is essential\nfor optimal performance. Conventionally, these noise parameters require manual\ntuning, which often presents significant challenges in achieving proper filter\nconvergence. To address this limitation, our proposed framework employs a\nneural network to autonomously estimate the complete set of noise parameters\n{\nğ\nt\n,\nğ\nt\n}\n\\{{{\\bf{Q}}_{t}},{{\\bf{N}}_{t}}\\}\nin real-time, thereby eliminating the need for\nmanual calibration while maintaining rigorous estimation-theoretic guarantees.\nIII-D\nSpiking Neurons and Surrogate Gradient\nUnlike traditional deep neural networks (DNNs), the fundamental computational unit in the SNN used in our proposed method is the leaky integrate-and-fire (LIF) neuron\n[\n20\n]\n, which offers a more biologically plausible representation of neuronal dynamics. As illustrated in Fig.\n1\n, an LIF neuron integrates incoming spikes over time, and emits an output spike when its membrane potential surpasses a predefined threshold. The evolution of the membrane potential in the LIF model can be mathematically described as follows,\nU\nâ€‹\n(\nt\n)\n=\nH\nâ€‹\n(\nt\nâˆ’\nÎ”\nâ€‹\nt\n)\n+\nI\nâ€‹\n(\nt\n)\n,\nI\nâ€‹\n(\nt\n)\n=\nf\nâ€‹\n(\nğ±\n;\nÎ¸\n)\n,\nH\nâ€‹\n(\nt\n)\n=\nV\nreset\nâ€‹\nS\nâ€‹\n(\nt\n)\n+\n(\n1\nâˆ’\nS\nâ€‹\n(\nt\n)\n)\nâ€‹\nÎ²\nâ€‹\nU\nâ€‹\n(\nt\n)\n,\nS\nâ€‹\n(\nt\n)\n=\n{\n1\n,\nif\nâ€‹\nU\nâ€‹\n(\nt\n)\nâ‰¥\nU\nthr\n,\n0\n,\nif\nâ€‹\nU\nâ€‹\n(\nt\n)\n<\nU\nthr\n\\begin{array}[]{l}U(t)=H(t-\\Delta t)+I(t),\\quad I(t)=f(\\mathbf{x};\\theta),\\\\\nH(t)=V_{\\text{reset }}S(t)+(1-S(t))\\beta U(t),\\\\\nS(t)=\\left\\{\\begin{array}[]{ll}1,&\\text{ if }U(t)\\geq U_{\\text{thr }},\\\\\n0,&\\text{ if }U(t)<U_{\\text{thr }}\\end{array}\\right.\\end{array}\n(12)\nFigure 1:\nThe structure illustration of the LIF neuron.\nIn the context of the LIF neuron model, the spatial input\nI\nâ€‹\n(\nt\n)\nI(t)\nat time step\nt\nt\nis computed by applying a learnable function\nf\nf\nto the input\nğ±\n\\bf{x}\n, parameterized by\nÎ¸\n\\theta\n. The temporal output\nH\nâ€‹\n(\nt\n)\nH(t)\nis governed by a discretization constant\nÎ”\nâ€‹\nt\n\\Delta t\n, which controls the resolution of the LIF dynamics. The spike response\nS\nâ€‹\n(\nt\n)\nS(t)\nis defined using a Heaviside step function, triggered when the membrane potential\nU\nâ€‹\n(\nt\n)\nU(t)\nexceeds a predefined threshold\nU\nthr\nU_{\\text{thr }}\n. Upon firing, the neuron emits a spike and the membrane potential is reset to\nV\nr\nâ€‹\ne\nâ€‹\ns\nâ€‹\ne\nâ€‹\nt\nV_{reset}\n. If the threshold is not reached, no spike is generated, and the membrane potential decays toward\nH\nâ€‹\n(\nt\n)\nH(t)\nat a rate determined by the decay rate\nÎ²\n\\beta\n. This formulation enables the neuron to capture temporal dependencies and sparse activation patterns, which are crucial for efficient spatiotemporal feature extraction.\nThrough a spiking neuron layer\nS\nâ€‹\nN\nâ€‹\n(\nâ‹…\n)\nSN(\\cdot)\n, the spike trains\nğ’\n\\bf{S}\nare generated by iterating over\nT\nâ€²\n{T^{\\prime}}\ndiscrete time steps, where each of the\nN\nN\ninput currents\nğˆ\n\\bf{I}\nis processed by a corresponding LIF neuron. Formally, this can be expressed as,\nğ’\n=\nS\nâ€‹\nN\nâ€‹\n(\nğˆ\n)\n{\\bf{S}}=SN({\\bf{I}})\n(13)\nwhere the arctangent-like surrogate gradients\n[\n21\n]\nare used\nto approximate the gradients of the spiking neurons during the backpropagation\nprocess. The surrogate gradient is defined as follows,\nS\nâ€‹\n(\nt\n)\nâ‰ˆ\n1\nÏ€\nâ€‹\narctan\nâ¡\n(\nÏ€\n2\nâ€‹\nÎ±\nâ€‹\nU\nâ€‹\n(\nt\n)\n)\n+\n1\n2\nS(t)\\approx\\frac{1}{\\pi}\\arctan(\\frac{\\pi}{2}\\alpha U(t))+\\frac{1}{2}\n(14)\nwhere\nÎ±\n\\alpha\nis a hyperparameter that controls the steepness of the surrogate\ngradient. Therefore, the comprehensive model can be trained using an end-to-end\napproach, facilitated by backpropagation through time (BPTT), which enables efficient\nlearning and optimization.\nIV\nProposed Method\nIV-A\nThe Hybrid State Estimation Strategy for IMU Dead Reckoning\nThe proposed hybrid state estimation framework integrates an SNN with an InEKF to enable accurate dead reckoning using low-cost MEMS IMUs. In general, IMU dead reckoning errors can be categorized into two primary sources: (1) measurement errors arising from sensor noise and bias, and (2) state estimation errors resulting from uncertainties in system dynamics and measurement models. The objective of the proposed method is to mitigate both error types by leveraging the complementary strengths of data-driven SNNs and model-based InEKF. Notably, dead reckoning with low-cost MEMS IMUs involves nonlinear dynamics and is subject to significant stochastic noise, making it difficult to establish precise analytical models. Traditional filter-based methods rely heavily on the accuracy of system modeling and sensor quality; as a result, their performance often degrades when confronted with corrupted IMU data, such as those affected by mechanical vibration or environmental disturbances. Moreover, such methods typically require manual tuning of filter parameters, including noise covariance matrices, which are labor-intensive and lack adaptability in dynamic environments. In contrast, the proposed framework adopts a data-driven paradigm wherein the SNN learns temporal patterns directly from raw IMU data and adaptively adjusts the noise parameters of the InEKF in real-time. Inspired by the brainâ€™s biological mechanisms for processing temporal information, SNNs exhibit strong capabilities in extracting spatial features from noisy inputs while maintaining high robustness and estimation accuracy, outperforming conventional deep learning models in this context.\nAs shown in Fig.\n2\n, the raw IMU measurements are first calibrated\nusing the IMU error model to reduce the bias and noise of the low-cost MEMS IMU.\nSpecifically, according to the analysis of the IMU error model in Eq.\n1\nand Eq.\n2\n, the IMU measurements can be calibrated by\nestimating the calibration matrix\nğ‚\n{\\bf{C}}\n, the biases\nğ›\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bf{b}}_{t}^{IMU}\n,\nand the noise\nğœ¼\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bm{\\eta}}_{t}^{IMU}\n. Thus, the IMU measurements after\ncalibration can be further expressed as,\nğ®\nt\nI\nâ€‹\nM\nâ€‹\nU\n=\nC\nâˆ’\n1\nâ€‹\n(\nğ®\n~\nt\nI\nâ€‹\nM\nâ€‹\nU\nâˆ’\nğ›\nt\nI\nâ€‹\nM\nâ€‹\nU\nâˆ’\nğœ¼\nt\nI\nâ€‹\nM\nâ€‹\nU\n)\n=\nC\nâˆ’\n1\nâ€‹\nğ®\n~\nt\nI\nâ€‹\nM\nâ€‹\nU\n+\nğœ¹\nt\n{\\bf{u}}_{t}^{IMU}={C^{-1}}({\\bf{\\tilde{u}}}_{t}^{IMU}-{\\bf{b}}_{t}^{IMU}-{\\bm{\\eta}}_{t}^{IMU})={C^{-1}}{\\bf{\\tilde{u}}}_{t}^{IMU}+{\\bm{\\delta}_{t}}\n(15)\nwhere\nğ®\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bf{u}}_{t}^{IMU}\nis the calibrated IMU measurements at time\nt\nt\n;\nğ®\n~\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bf{\\tilde{u}}}_{t}^{IMU}\nis the raw IMU measurements at time\nt\nt\n, which can\nbe either angular velocity or linear acceleration;\nğ›\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bf{b}}_{t}^{IMU}\nand\nğœ¼\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bm{\\eta}}_{t}^{IMU}\nare the bias and noise of the IMU respectively;\nÎ´\n=\nC\nâˆ’\n1\nâ€‹\n(\nğ›\nt\nI\nâ€‹\nM\nâ€‹\nU\n+\nÎ·\nt\nI\nâ€‹\nM\nâ€‹\nU\n)\n{\\bf{\\delta}}={C^{-1}}({\\bf{b}}_{t}^{IMU}+{\\bf{\\eta}}_{t}^{IMU})\nis defined as the IMU correction\nterm, which is a time-varying error.\nFigure 2:\nAn overview of the proposed hybrid state estimation method for IMU\ndead reckoning.\nTo compute the IMU correction term\nÎ´\nt\n{\\bf{\\delta}}_{t}\n, the SNN introduced in\nSection IV.C is employed to make predictions based on a local temporal window of size\nN\nN\n, i.e.,\nğ®\nt\nâˆ’\nN\nI\nâ€‹\nM\nâ€‹\nU\n,\nâ€¦\n,\nğ®\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bf{u}}_{t-N}^{IMU},\\ldots,{\\bf{u}}_{t}^{IMU}\n.\nA learnable calibration matrix\nğ‚\n{\\bf{C}}\n, initialized as the identity matrix\nğˆ\n3\n{{\\bf{I}}_{3}}\n, is optimized during training to correct measurement distortions. This allows the SNN to refine the IMU measurements from the initial training epochs, effectively denoising and debiasing the inputs before feeding them into the InEKF. Once calibrated, the corrected IMU data are integrated into the InEKF framework, which updates the system state\nğ±\nğ­\n\\bf{x_{t}}\nby incorporating ZUPT constraints based on the robotâ€™s kinematic model, i.e., Eq.\n4\n-Eq.\n8\n.\nHowever, the performance of the InEKF still depends on accurate modeling and predefined noise parameters, which are often difficult to obtain in practice. To address this limitation, the SNN is further utilized to dynamically infer the observation noise covariance matrix\nğ\nğ­\n\\bf{N}_{t}\nin real-time from the calibrated IMU data. By providing both denoised inputs and adaptive noise estimates, the proposed SNN-InEKF framework enhances the robustness and accuracy of pose estimation, including rotation\nğ‘\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bf{R}}_{t}^{IMU}\n, velocity\nğ¯\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bf{v}}_{t}^{IMU}\nand postion\nğ©\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bf{p}}_{t}^{IMU}\n.\nIV-B\nInEKF with Pseudo-measurements\nThe IMU dead reckoning can be formulated as a time-series problem, wherein a sequence of IMU measurements collected over a temporal window is used to recursively estimate the systemâ€™s state. Given an initial state comprising the rotation matrix, velocity, and position, denoted as\n(\nğ‘\n0\nI\nâ€‹\nM\nâ€‹\nU\n,\nğ¯\n0\nI\nâ€‹\nM\nâ€‹\nU\n,\nğ©\n0\nI\nâ€‹\nM\nâ€‹\nU\n)\n({\\bf{R}}_{0}^{IMU},{\\bf{v}}_{0}^{IMU},{\\bf{p}}_{0}^{IMU})\n, the subsequent IMU readings, i.e.,\n(\nğ\nt\nI\nâ€‹\nM\nâ€‹\nU\n,\nğš\nt\nI\nâ€‹\nM\nâ€‹\nU\n)\n({\\bm{\\omega}}_{t}^{IMU},{\\bf{a}}_{t}^{IMU})\n, can be integrated through the systemâ€™s kinematic model to predict the state at the next time step.\nğ±\nt\n=\n[\nğ‘\nt\nI\nâ€‹\nM\nâ€‹\nU\nğ¯\nt\nI\nâ€‹\nM\nâ€‹\nU\nğ©\nt\nI\nâ€‹\nM\nâ€‹\nU\nğ›\nt\nÏ‰\nğ›\nt\na\nğ‘\nt\nc\nğ©\nt\nc\n]\nT\n{{\\bf{x}}_{t}}={[\\begin{array}[]{*{20}{c}}{{\\bf{R}}_{t}^{IMU}}&{{\\bf{v}}_{t}^{IMU}}&{{\\bf{p}}_{t}^{IMU}}&{{\\bf{b}}_{t}^{\\omega}}&{{\\bf{b}}_{t}^{a}}&{{\\bf{R}}_{t}^{c}}&{{\\bf{p}}_{t}^{c}}\\end{array}]^{T}}\n(16)\nwhere\nğ‘\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bf{R}}_{t}^{IMU}\nis the rotation matrix from the world frame to the\nIMU frame at time\nt\nt\n;\nğ¯\nt\nI\nâ€‹\nM\nâ€‹\nU\n{{\\bf{v}}_{t}^{IMU}}\nand\nğ©\nt\nI\nâ€‹\nM\nâ€‹\nU\n{{\\bf{p}}_{t}^{IMU}}\nare the velocity\nand position of the mobile robot in the world frame at time\nt\nt\n;\nğ›\nt\nÏ‰\n{\\bf{b}}_{t}^{\\omega}\nand\nğ›\nt\na\n{\\bf{b}}_{t}^{a}\nare the gyroscope and\naccelerometer biases at time\nt\nt\n;\nğ‘\nt\nc\n{\\bf{R}}_{t}^{c}\ndescribes the rotation matrix\nfrom the mobile robot frame to the IMU frame;\nğ©\nt\nc\n{\\bf{p}}_{t}^{c}\ndenotes the\ndisplacement vector from the mobile robot frame to the IMU frame. Thus,\nğ±\nt\n{{\\bf{x}}_{t}}\nis also the state vector of the InEKF in our proposed method\nand the state transition function\nğŸ\n{\\bf{f}}\ncan be derivated from the\nkinematic model of the mobile robot, i.e., Eq.\n4\n-Eq.\n8\n.\nConsider the different frames, the velocity of the origin point of the mobile\nrobot frame can be expressed as,\nğ¯\nt\nc\n=\n[\nv\nt\nf\nâ€‹\nw\nâ€‹\nd\nv\nt\nl\nâ€‹\na\nâ€‹\nt\nv\nt\nu\nâ€‹\np\n]\n=\n(\nğ‘\nt\nc\n)\nT\nâ€‹\n(\n(\nğ‘\nt\nI\nâ€‹\nM\nâ€‹\nU\n)\nT\nâ€‹\nğ¯\nt\nI\nâ€‹\nM\nâ€‹\nU\n+\n(\nÏ‰\nt\nâˆ’\nğ›\nt\nÏ‰\n)\nÃ—\nâ€‹\nğ©\nt\nc\n)\n{\\bf{v}}_{t}^{c}=\\left[{\\begin{array}[]{*{20}{c}}{v_{t}^{fwd}}\\\\\n{v_{t}^{lat}}\\\\\n{v_{t}^{up}}\\end{array}}\\right]={({\\bf{R}}_{t}^{c})^{T}}({({\\bf{R}}_{t}^{IMU})^{T}}{\\bf{v}}_{t}^{IMU}+{({{\\bf{\\omega}}_{t}}-{\\bf{b}}_{t}^{\\omega})_{\\times}}{\\bf{p}}_{t}^{c})\n(17)\nwhere\nv\nt\nf\nâ€‹\nw\nâ€‹\nd\n{v_{t}^{fwd}}\n,\nv\nt\nl\nâ€‹\na\nâ€‹\nt\n{v_{t}^{lat}}\n, and\nv\nt\nu\nâ€‹\np\n{v_{t}^{up}}\nare the forward, lateral,\nand upward velocity of the mobile robot frame, respectively.\nAccording to the kinematic constraints of the wheeled mobile robot operating\nunder nonholonomic conditions, the lateral velocity and vertical velocity should\ntheoretically be zero, i.e.,\nv\nt\nl\nâ€‹\na\nâ€‹\nt\n=\n0\nv_{t}^{lat}=0\nand\nv\nt\nu\nâ€‹\np\n=\n0\nv_{t}^{up}=0\n. Therefore, we\ncan use the lateral and upward velocity as pseudo-measurements to update the\nstate of the InEKF. The pseudo-measurements can be expressed as,\nğ²\nt\n=\nh\nâ€‹\n(\nğ±\nt\n)\n=\n[\ny\nt\nl\nâ€‹\na\nâ€‹\nt\ny\nt\nu\nâ€‹\np\n]\n=\n[\nh\nl\nâ€‹\na\nâ€‹\nt\nâ€‹\n(\nğ±\nt\n)\n+\nn\nt\nl\nâ€‹\na\nâ€‹\nt\nh\nu\nâ€‹\np\nâ€‹\n(\nğ±\nt\n)\n+\nn\nt\nu\nâ€‹\np\n]\n=\n[\nv\nt\nl\nâ€‹\na\nâ€‹\nt\nv\nt\nu\nâ€‹\np\n]\n+\nn\nt\n{{\\bf{y}}_{t}}=h({{\\bf{x}}_{t}})=\\left[{\\begin{array}[]{*{20}{c}}{y_{t}^{lat}}\\\\\n{y_{t}^{up}}\\end{array}}\\right]=\\left[{\\begin{array}[]{*{20}{c}}{{h^{lat}}({{\\bf{x}}_{t}})+n_{t}^{lat}}\\\\\n{{h^{up}}({{\\bf{x}}_{t}})+n_{t}^{up}}\\end{array}}\\right]=\\left[{\\begin{array}[]{*{20}{c}}{v_{t}^{lat}}\\\\\n{v_{t}^{up}}\\end{array}}\\right]+{n_{t}}\n(18)\nwhere\nğ¡\nâ€‹\n(\nğ±\nt\n)\n{\\bf{h}}({\\bf{x}}_{t})\nis the measurement function that maps the state\nvector to the measurement space, and\nn\nt\n{n_{t}}\nis the measurement noise, which\nis assumed to be Gaussian with zero mean and a certain covariance matrix\nğ\nt\n{\\bf{N}}_{t}\n. The InEKF is then fed with the pseudo-measurements\nğ²\nt\n{\\bf{y}}_{t}\nand the state vector\nğ±\nt\n{\\bf{x}}_{t}\nis updated based on the incoming measurements\nand the system dynamics. Afterwards, the noise covariance matrices\nğ\nt\n{\\bf{N}}_{t}\nis used to update the InEKF gain, i.e.,\nğŠ\nt\n=\nğ\nt\nâ€‹\nğ‡\nt\nT\nâ€‹\n(\nğ‡\nt\nâ€‹\nğ\nt\nâ€‹\nğ‡\nt\nT\n+\nğ\nt\n)\nâˆ’\n1\n{{\\bf{K}}_{t}}={{\\bf{P}}_{t}}{{\\bf{H}}_{t}}^{T}({{\\bf{H}}_{t}}{{\\bf{P}}_{t}}{{\\bf{H}}_{t}}^{T}+{{\\bf{N}}_{t}})^{-1}\n(19)\nwhere\nğŠ\nt\n{\\bf{K}}_{t}\nis the Kalman gain, and\nğ‡\nt\n{\\bf{H}}_{t}\nis the Jacobian matrix\nof the measurement function\nğ¡\n{\\bf{h}}\nwith respect to the state vector\nğ±\nt\n{\\bf{x}}_{t}\n.\nIV-C\nProposed SNN Architecture\nSNNs are brain-inspired models that encode and process information through discrete spike events occurring at specific time points. Unlike traditional DNNs, SNNs more closely emulate the information transmission mechanisms observed in biological neural systems, rendering them particularly suitable for neuroscience-inspired modeling and highly compatible with energy-efficient neuromorphic hardware platforms\n[\n22\n]\n. In recent years, SNNs have demonstrated strong potential in handling temporal dynamics and extracting spatial features from time-series data, offering promising capabilities for applications requiring real-time and low-power computation.\nIn the proposed method, an SNN is designed to predict both the IMU correction term and the observation noise parameters of the InEKF. Given a temporal window of\nN\nN\ninertial measurements as input, the network processes the data to compute the necessary corrections and dynamically adjust the noise model parameters,\nğ²\nn\nâ€‹\ne\nâ€‹\nt\n=\nS\nâ€‹\nN\nâ€‹\nN\nâ€‹\n(\n{\nğ\nt\nI\nâ€‹\nM\nâ€‹\nU\n,\nğš\nt\nI\nâ€‹\nM\nâ€‹\nU\n}\ni\n=\nt\nâˆ’\nN\nt\n)\n{{\\bf{y}}_{net}}=SNN(\\{{\\bm{\\omega}}_{t}^{IMU},{\\bf{a}}_{t}^{IMU}\\}_{i=t-N}^{t})\n(20)\nwhere\nğ²\nn\nâ€‹\ne\nâ€‹\nt\n{\\bf{y}}_{net}\nis the output of the SNN, which is a 14-dimensional\nvector containing the IMU correction term\nğœ¹\nt\n{\\bm{\\delta}}_{t}\nand the noise\nparameters\nğ\nt\n{\\bf{N}}_{t}\nof the InEKF, i.e.,\nğ²\nn\nâ€‹\ne\nâ€‹\nt\n=\n[\ny\nc\n1\ny\nc\n2\ny\nc\n3\ny\nc\n4\ny\nc\n5\ny\nc\n6\ny\nb\n1\ny\nb\n2\ny\nb\n3\ny\nb\n4\ny\nb\n5\ny\nb\n6\ny\nr\n1\ny\nr\n2\n]\nT\n\\begin{array}[]{*{20}{c}}{{\\bf{y}}_{net}}=[{y^{{c_{1}}}}&{y^{{c_{2}}}}&{y^{{c_{3}}}}&{y^{{c_{4}}}}&{y^{{c_{5}}}}&{y^{{c_{6}}}}&{y^{{b_{1}}}}&{y^{{b_{2}}}}\\\\\n&{y^{{b_{3}}}}&{y^{{b_{4}}}}&{y^{{b_{5}}}}&{y^{{b_{6}}}}&{y^{{r_{1}}}}&{y^{{r_{2}}}}]^{T}\\end{array}\n(21)\nSpecifically, through the SNN predict value\nğ²\nn\nâ€‹\ne\nâ€‹\nt\n{\\bf{y}}_{net}\n, the inverse of calibration\nmatrix\nC\nâˆ’\n1\n{C^{-1}}\nof the IMU error simplified model can be obtained as\n[\n2\n]\n,\nC\nâˆ’\n1\n=\nd\nâ€‹\ni\nâ€‹\na\nâ€‹\ng\nâ€‹\n(\n10\nÎ²\nâ€‹\ny\nc\n1\n,\n10\nÎ²\nâ€‹\ny\nc\n2\n,\n10\nÎ²\nâ€‹\ny\nc\n3\n,\n10\nÎ²\nâ€‹\ny\nc\n4\n,\n10\nÎ²\nâ€‹\ny\nc\n5\n,\n10\nÎ²\nâ€‹\ny\nc\n6\n)\n{C^{-1}}=diag({10^{\\beta{y^{{c_{1}}}}}},{10^{\\beta{y^{{c_{2}}}}}},{10^{\\beta{y^{{c_{3}}}}}},{10^{\\beta{y^{c_{4}}}}},{10^{\\beta{y^{{c_{5}}}}}},{10^{\\beta{y^{{c_{6}}}}}})\n(22)\nwhere\nÎ²\n\\beta\nis a scaling factor to adjust the range of the calibration\nmatrix elements and it is set to 0.1 in this paper according to engineering practice. The bias vector\nğ›\nt\nI\nâ€‹\nM\nâ€‹\nU\n{\\bf{b}}_{t}^{IMU}\ncan be dictated as,\nğ›\nt\nI\nâ€‹\nM\nâ€‹\nU\n=\n[\ny\nb\n1\n,\ny\nb\n2\n,\ny\nb\n3\n,\ny\nb\n4\n,\ny\nb\n5\n,\ny\nb\n6\n]\nT\n{\\bf{b}}_{t}^{IMU}={[{y^{{b_{1}}}},{y^{{b_{2}}}},{y^{{b_{3}}}},{y^{{b_{4}}}},{y^{{b_{5}}}},{y^{{b_{6}}}}]^{T}}\n(23)\n.\nFor InEKF, the parameters that need to be adjusted including the initial covariance matrix\nğ\n0\n{{\\bf{P}}_{0}}\n, the process noise covariance matrix\nğ\nt\n{\\bf{Q}}_{t}\nand the\nmeasurement noise covariance matrix\nğ\nt\n{\\bf{N}}_{t}\n. For\nğ\n0\n{{\\bf{P}}_{0}}\nand\nğ\nt\n{\\bf{Q}}_{t}\nare initialized as a diagonal matrix with small values, which can be\ndynamically adapted for future iterations. Regarding\nğ\nt\n{\\bf{N}}_{t}\n, it can be\nestimated as follows,\nğ\nt\n=\nd\nâ€‹\ni\nâ€‹\na\nâ€‹\ng\nâ€‹\n(\nÏƒ\nl\nâ€‹\na\nâ€‹\nt\n2\nâ€‹\n10\ny\nr\nâ€‹\n1\n,\nÏƒ\nu\nâ€‹\np\n2\nâ€‹\n10\ny\nr\nâ€‹\n2\n)\n{{\\bf{N}}_{t}}=diag(\\sigma_{lat}^{2}{10^{{y^{r1}}}},\\sigma_{up}^{2}{10^{{y^{r2}}}})\n(24)\nwhere\nÏƒ\nl\nâ€‹\na\nâ€‹\nt\n2\n\\sigma_{lat}^{2}\nand\nÏƒ\nu\nâ€‹\np\n2\n\\sigma_{up}^{2}\nare the initial covariance values\nof the lateral and upward velocity, respectively.\nğ\nt\n{{\\bf{N}}_{t}}\nmay inflate covariance\nup to a factor\n10\ny\nr\nâ€‹\n1\n10^{{y^{r1}}}\nand squeeze it up to a factor\n10\ny\nr\nâ€‹\n2\n10^{{y^{r2}}}\n.\nGiven that wheeled mobile robots are susceptible to lateral slip and other\nconditions that can compromise the accuracy of the ZUPT method, real-time adjustments\nto\nğ\nt\n{\\bf{N}}_{t}\nare essential. To address this, our approach dynamically adapts\nğ\nt\n{\\bf{N}}_{t}\nbased on the SNN output. This adaptive strategy mitigates the\ninterference from pseudo-observations with lower reliability, thereby enhancing\nthe positioning accuracy of the InEKF algorithm.\nFigure 3:\nThe pipeline of SNN for IMU dead reckoning.\nTo fully leverage the inherent characteristics of SNNs, it is essential to\nalign the temporal dimension between IMU time-series data and SNNs. As shown in Fig.\n3\n,\nthe core strategy in the paper is that revolves around integrating the relevant finer details of\nspike activity within the time-series data at each time step. To achieve this,\na time step\nÎ”\nâ€‹\nT\n\\Delta T\nof the time series is divided into Ts segments. Each of\nthese segments enables a firing event for neurons whose membrane potentials\nexceed the threshold, thereby ensuring\nÎ”\nâ€‹\nT\n\\Delta T\nis equivalent to\nÎ”\nâ€‹\nT\n=\nT\ns\nâ€‹\nÎ”\nâ€‹\nt\n\\Delta T={T_{s}}\\Delta t\n. This equation serves as a bridge between the time step\nÎ”\nâ€‹\nT\n\\Delta T\nin a time series and the time step\nÎ”\nâ€‹\nt\n\\Delta t\nin the SNN. Consequently, the\nindependent variable\nt\nt\n, as employed in both the time-series context (denoted\nas\nX\nâ€‹\n(\nt\n)\nX(t)\n) and within the SNN framework\n(\nU\nâ€‹\n(\nt\n)\n,\nI\nâ€‹\n(\nt\n)\n,\nH\nâ€‹\n(\nt\n)\n,\nS\nâ€‹\n(\nt\n)\n)\n(U(t),I(t),H(t),S(t))\n, now holds\na shared significance. In a recent study, Qu et al.\n[\n23\n]\ncompellingly\ndemonstrated that this specific type of morphological information can be effectively\nmodeled employing a distinct variety of CNN kernels. Building on this\ninsightful finding, the innovative utilization of a convolutional layer is selected\nas an optimal temporal encoder. Given the\ni\nt\nâ€‹\nh\ni_{th}\nhistorical raw IMU data\nğ”\nğ¢\n=\n(\nğ®\nğ­\nâˆ’\nğ\nğˆğŒğ”\n,\nâ€¦\n,\nğ®\nğ­\nğˆğŒğ”\n)\n\\bf{U}_{i}=({\\bf{u}}_{t-N}^{IMU},\\ldots,{\\bf{u}}_{t}^{IMU})\n, it can be input\nto the convolutional layer followed by batch normalization and generate the spike\nas,\nğ’\n=\nS\nâ€‹\nN\nâ€‹\n(\nB\nâ€‹\nN\nâ€‹\n(\nC\nâ€‹\no\nâ€‹\nn\nâ€‹\nv\nâ€‹\n(\nğ”\ni\n)\n)\n)\n{\\bf{S}}=SN(BN(Conv({{\\bf{U}}_{i}})))\n(25)\nBy passing through the spike encoder based on the convolutional layer, the\ndimension of\nğ”\ni\n{{\\bf{U}}_{i}}\ncan be expanded to\nT\ns\nÃ—\nT\nÃ—\nC\n{T_{s}}\\times T\\times C\n, where\nC\nC\nis the number of channels, i.e. 6, the 3-axis angular velocity, and the 3-axis\nlinear acceleration. The convolutional spike encoder effectively captures the internal\ntemporal information inherent in the input data, namely, the temporal changes and\nshapes, which significantly contributes to the robust representation of the\ndynamic nature of this information over time. This encoding process adeptly caters\nto the subsequent spiking layers, thereby facilitating event-driven modeling.\nConsidering the high sampling frequency of the IMU, usually 100Hz, and the\npresence of substantial nonlinear noise components in IMU data, the SNN needs to\nlearn the temporal features of the IMU measurements over a long time window.\nTraditional neural networks, such as RNN or CNN, often struggle to capture long-term\ndependencies due to the vanishing gradient problem. To address this issue, we employ\na spiking Transformer as the backbone of the SNN, which is capable of\ncapturing long-term dependencies and temporal features in the IMU measurements.\nIn the proposed SNN architecture, the spiking version of Transformer is built\nupon the iTransformer\n[\n24\n]\n, which is the state-of-the-art\ntime-series forecasting model on several benchmark datasets.\nInspired by Spikformer v2\n[\n25\n]\n, the spiking self-attention\n(SSA) mechanism is incorporated into the iTransformer architecture to build\nthe spiking Transformer blocks. The SSA mechanism is designed to capture the temporal\ndependencies and long-term relationships in the input data, which is essential\nfor accurately modeling the IMU measurements. Specifically, a channel-wise\nspiking embedding layer is used to convert the input spike trains into a\ncontinuous representation, which is then fed into the spiking Transformer blocks\nand can be expressed as follows,\nğ’\ne\nâ€‹\nm\nâ€‹\nb\n=\nS\nâ€‹\nN\nâ€‹\n(\nL\nâ€‹\ni\nâ€‹\nn\nâ€‹\ne\nâ€‹\na\nâ€‹\nr\nâ€‹\n(\nğ’\n)\n)\n{{\\bf{S}}_{emb}}=SN(Linear({\\bf{S}}))\n(26)\nwhere\nğ’\ne\nâ€‹\nm\nâ€‹\nb\n{\\bf{S}}_{emb}\nis the spiking embedding of the input spike trains\nğ’\n{\\bf{S}}\n, and the linear layer is used to project the spike trains into a higher-dimensional\nspace.\nThe spiking Transformer blocks consist of multiple layers of spiking self-attention\nand feed-forward networks, which are designed to capture the temporal features\nand long-term dependencies in the input data. The output of the spiking Transformer\nblocks are then fed into a projection layer, such as a fully connected layer, to\ngenerate the final output of the SNN, i.e., the IMU correction term\nğœ¹\nt\n{\\bm{\\delta}}_{t}\nand the noise parameters\nğ\nt\n{\\bf{N}}_{t}\nof the InEKF.\nThe proposed SNN architecture is trained using backpropagation through time\n(BPTT)\n[\n26\n]\nalgorithm, which is a common training method\nfor SNNs. In the proposed SNN optimization framework, the relative\ndisplacement\nÎ”\nâ€‹\nğ©\n\\Delta{\\bf{p}}\n, the alteration in relative velocity\nÎ”\nâ€‹\nğ¯\n\\Delta{\\bf{v}}\n,\nand the relative rotation\nÎ”\nâ€‹\nğ‘\n\\Delta{\\bf{R}}\nat each instant are designated as\nthe optimization variables. The comprehensive training loss is subsequently derived\nfrom the Huber loss function\n[\n27\n]\n, which helps in\nbalancing robustness and sensitivity to outliers during the training process. The\nloss function can be expressed as,\nL\nHuber\nâ€‹\n(\ny\n,\ny\n^\n)\n=\n{\n1\n2\nâ€‹\n(\ny\nâˆ’\ny\n^\n)\n2\n,\n|\ny\nâˆ’\ny\n^\n|\nâ‰¤\nÎ´\nÎ´\nâ€‹\n|\ny\nâˆ’\ny\n^\n|\nâˆ’\n1\n2\nâ€‹\nÎ´\n2\n,\n|\ny\nâˆ’\ny\n^\n|\n>\nÎ´\nL_{\\text{Huber }}(y,\\hat{y})=\\left\\{\\begin{array}[]{ll}\\frac{1}{2}(y-\\hat{y})^{2},&|y-\\hat{y}|\\leq\\delta\\\\\n\\delta|y-\\hat{y}|-\\frac{1}{2}\\delta^{2},&|y-\\hat{y}|>\\delta\\end{array}\\right.\n(27)\nwhere the Huber parameter\nÎ´\n\\delta\nis set\n4\nâ€‹\ne\nâˆ’\n4\n4e-4\n. The loss function is constructed to be fully differentiable with respect to the parameters of the SNN, thereby enabling efficient optimization via gradient-based learning algorithms. During training, the SNN parameters are iteratively updated to minimize the loss, which in turn enhances the accuracy of the IMU dead reckoning estimates by improving the quality of the predicted correction terms and noise parameters.\nV\nExperimental Results and Analysis\nV-A\nDataset and Evaluation Metrics\nTo evaluate the performance of the proposed method, the KITTI odometry dataset\n[\n28\n]\nis used, which serves as a widely adopted benchmark for\nevaluating visual-inertial odometry and SLAM algorithms. The KITTI dataset contains\na series of sequences captured by a stereo camera and an IMU mounted on a vehicle,\nwhich provides ground truth poses for evaluation. The dataset consists of 22 sequences,\neach with a different length and varying environmental conditions. We download\nthe raw data with the IMU measurements sampled at 100 Hz. In this article, two\nwidely recognized evaluation metrics are employed to assess the accuracy of the\nproposed method, which are also used in the KITTI dataset, i.e., KITTI odometry\nevaluation metrics,\n1) Relative Translational Error (RTE): It computes across all possible trajectory\nsubsequences with lengths ranging from 100 to 800 meters, expressed as a\npercentage of the total traversed distance, and can be defined as follows,\nR\nâ€‹\nT\nâ€‹\nE\n=\n1\n|\nâ„±\n|\nâ€‹\nâˆ‘\n(\ni\n,\nj\n)\nâˆˆ\nâ„±\nTrans\nâ¡\n(\n(\np\n^\nj\nâŠ–\np\n^\ni\n)\nâŠ–\n(\np\nj\nâŠ–\np\ni\n)\n)\nlength\nâ¡\n(\ni\n,\nj\n)\nRTE=\\frac{1}{|\\mathcal{F}|}\\sum_{(i,j)\\in\\mathcal{F}}\\frac{\\operatorname{Trans}\\left(\\left(\\hat{p}_{j}\\ominus\\hat{p}_{i}\\right)\\ominus\\left(p_{j}\\ominus p_{i}\\right)\\right)}{\\operatorname{length}(i,j)}\n(28)\nwhere\np\n^\ni\n\\hat{p}_{i}\nand\np\ni\np_{i}\nare the estimated and ground truth poses at\ntime\ni\ni\n, respectively;\nâŠ–\n\\ominus\ndenotes the operation of transforming the\npose from the world frame to the IMU frame;\nTrans\n\\operatorname{Trans}\nrepresents that\ncomputes the translation value of the pose;\nlength\nâ¡\n(\ni\n,\nj\n)\n\\operatorname{length}(i,j)\nis\nthe length of the subsequence from time\ni\ni\nto time\nj\nj\n, i.e., ranging from 100\nto 800 meters.\n2) Relative Rotation Error (RRE): It represents the relative rotational increment\nerror for all potential subsequences ranging in length from 100 to 800 meters,\nexpressed in degrees per kilometer, and can be defined as follows,\nR\nâ€‹\nR\nâ€‹\nE\n=\n1\n|\nâ„±\n|\nâ€‹\nâˆ‘\n(\ni\n,\nj\n)\nâˆˆ\nâ„±\nRot\nâ¡\n(\n(\np\n^\nj\nâŠ–\np\n^\ni\n)\nâŠ–\n(\np\nj\nâŠ–\np\ni\n)\n)\nlength\nâ¡\n(\ni\n,\nj\n)\nRRE=\\frac{1}{|\\mathcal{F}|}\\sum_{(i,j)\\in\\mathcal{F}}\\frac{\\operatorname{Rot}\\left(\\left(\\hat{p}_{j}\\ominus\\hat{p}_{i}\\right)\\ominus\\left(p_{j}\\ominus p_{i}\\right)\\right)}{\\operatorname{length}(i,j)}\n(29)\nwhere\nRot\n\\operatorname{Rot}\nrepresents that computes the rotation value of the\npose.\nV-B\nExperimental Setup\nThe KITTI raw dataset is acquired utilizing the RT 3000v3 IMU produced by OxTS,\nrenowned for its superior accuracy but at a high cost\n[\n28\n]\n. To\ngenerate low-cost IMU data, a predefined quantity of noise and bias is intentionally\nintroduced into the raw KITTI IMU data. Specifically, gyroscope measurements are\ncorrupted with Gaussian noise\nN\nâ€‹\n(\n0\n,\n10\nâˆ’\n3\n)\nN(0,{10^{-3}})\nand a random bias\nU\nâ€‹\n(\n0.015\n,\n0.025\n)\nU(0.015,0.025)\n, whereas accelerometer data is augmented with Gaussian noise\nN\nâ€‹\n(\n0\n,\n10\nâˆ’\n2\n)\nN(0,{10^{-2}})\nand a bias\nU\nâ€‹\n(\n0.45\n,\n0.55\n)\nU(0.45,0.55)\n.\nA total of 14 sequences (Seq. 1â€“14), each exceeding 80 seconds in duration, are selected for evaluation. Among them, the first 40 seconds of Seq. 6-14 are used as the training set, while the remaining portions of these sequences are reserved for validation. Seq. 1-5 are designated as the test set, where GNSS data serve as ground-truth references for quantitative assessment. To address the mismatch in sampling frequencies between the GNSS and IMU data, linear interpolation is employed to resample the ground-truth trajectories to match the IMUâ€™s 100 Hz rate. All experiments are initialized from the current robot state, and the proposed method is benchmarked against three representative baseline approaches.\nâ€¢\nORB-SLAM3-Stereo\n[\n29\n]\n: An advanced visual SLAM system\nthat utilizes stereo cameras for accurate pose estimation without the\nloopclosing.\nâ€¢\nAI-IMU\n[\n1\n]\n: A deep learning-based method that\nemploys a neural network to estimate the pose of the mobile robot using\nIMU data.\nâ€¢\nMD-IMU\n[\n2\n]\n: A model-driven method that combines a\nKalman filter with a neural network to estimate the pose of the mobile\nrobot using IMU data.\nThe proposed method is implemented using PyTorch and SpikingJelly\n[\n21\n]\n. To reduce the risk of overfitting and accelerate\nthe training process, the SNN is trained using an Adam optimizer\n[\n30\n]\nwith a learning rate initialized\n1\nâ€‹\ne\nâˆ’\n4\n1e-4\nwith cosines\nwarming restart scheduler\n[\n31\n]\n. The weight decay is set to\n5\nâ€‹\ne\nâˆ’\n2\n5e-2\n, and the dropout rate is set to\n0.1\n0.1\n. In addition, training is\nconducted for 1000 epochs. To capture more temporal information, the SNN is\ndesigned with a window size of\nN\n=\n500\nN=500\n, which means that the SNN takes the last\n500 IMU measurements as input to predict the IMU correction term and the noise\nparameters of the InEKF. The dimension of the embedding layer and feed-forward\nlayer in the Spike-Transformer is set to 256, and the number of heads in the self-attention\nmechanism is set to 4. The number of spiking Transformer blocks is set to 2,\nand the number of neurons in the output layer is set to 14, which corresponds to\nthe IMU correction term and the noise parameters of the InEKF. The proposed\nSNN is trained on a single NVIDIA RTX A6000 GPU with a batch size of 8.\nV-C\nExperimental Results on the KITTI Dataset\nThe experimental results of the proposed method and baseline approaches on the KITTI dataset are shown in Fig.\n4\nand Tab.\nI\n, evaluated in terms of RTE, RRE, and per-sequence processing time. As illustrated in Fig.\n4\n, the trajectory of Seq. 3 estimated by MD-IMU, AI-IMU, ORB-SLAM3, and the proposed method is compared against the ground truth, represented by blue, black, orange, and green curves, respectively. The proposed method demonstrates superior accuracy over MD-IMU and AI-IMU across both RTE and RRE metrics. This improvement stems from the limited ability of MD-IMU and AI-IMU to suppress the substantial noise present in low-cost IMU data, with MD-IMU further constrained by its reliance on a short time window for InEKF noise parameter estimation. In contrast, the proposed method utilizes a spike encoder to transform raw IMU measurements into spike trains, which are then processed by spiking Transformer blocks to extract temporal features and long-range dependencies. This mechanism enables the denoised IMU signals to more accurately reflect the true motion dynamics of the robot, thereby enhancing the performance of InEKF and improving localization accuracy. On average, the proposed method achieves lower RTE than ORB-SLAM3 on Seq. 12 and 13, though it underperforms slightly in RRE. It is worth noting that the localization performance of ORB-SLAM3 Stereo could be further improved with loop-closure enabled, which was not activated in this evaluation.\nThe position and attitude estimations are further illustrated in Fig.\n5\nand Fig.\n6\n, respectively. The AI-IMU method exhibits significant drift in both translational and rotational estimates, while the proposed method shows high fidelity to the ground truth, demonstrating its efficacy in mitigating the effects of noise and bias in low-cost MEMS IMUs. These results confirm that the proposed approach is competitive with, and in several cases outperforms, state-of-the-art methods such as ORB-SLAM3 and MD-IMU.\nTABLE I:\nExperimental results of different methods on the KITTI Dataset\nSeq.\nSequence Name\nLength(m)\nORB-SLAM3\nAI-IMU\nMD-IMU\nProposed\nRTE\nRRE\nT(s)\nRTE\nRRE\nT(s)\nRTE\nRRE\nT(s)\nRTE\nRRE\nT(s)\n1\n2011_09_26_drive_0036\n743.9\n4.8\n19.1\n41.5\n115.4\n188.2\n15.5\n2.9\n3.8\n31.9\n1.4\n2.5\n40.2\n2\n2011_09_26_drive_0101\n1312.6\n3.3\n4.9\n51.8\n1273.1\n35.2\n19.9\n3.6\n3.3\n30.4\n3.5\n0.8\n42.3\n3\n2011_09_30_drive_0028\n4243.4\n5.9\n24.3\n81.0\n102.4\n152.4\n33.8\n9.1\n5.2\n49.8\n2.6\n2.5\n60.7\n4\n2011_09_30_drive_0034\n922.3\n3.2\n27.9\n63.7\n46.3\n47.2\n26.0\n2.8\n5.5\n40.3\n2.3\n2.5\n52.4\n5\n2011_10_03_drive_0042\n2600.4\n18.1\n19.2\n79.4\n828.1\n14.1\n27.8\n6.1\n2.5\n35.7\n5.3\n1.4\n48.5\n6\n2011_09_26_drive_0022\n522.9\n4.7\n42.1\n38.3\n62.9\n85.2\n16.6\n1.9\n6.3\n22.9\n1.6\n4.0\n32.3\n7\n2011_09_29_drive_0071\n245.2\n4.5\n11.2\n48.3\n126.9\n767.2\n22.1\n4.9\n6.5\n31.4\n4.0\n2.6\n42.8\n8\n2011_09_30_drive_0018\n2212.6\n3.9\n23.2\n149.2\n85.6\n184.1\n73.7\n19.2\n25.5\n76.9\n3.2\n8.2\n90.5\n9\n2011_09_30_drive_0020\n1242.8\n4.8\n32.5\n57.8\n147.1\n114.5\n22.6\n3.7\n4.0\n29.0\n2.2\n2.1\n42.4\n10\n2011_09_30_drive_0027\n692.9\n4.5\n37.2\n51.9\n115.8\n286.9\n21.3\n1.8\n5.9\n28.5\n1.4\n3.2\n40.7\n11\n2011_09_30_drive_0033\n1714.5\n8.1\n27.3\n276.2\n135.3\n178.2\n185.0\n2.6\n5.2\n134.7\n1.8\n2.4\n149.4\n12\n2011_10_03_drive_0027\n3731.8\n3.1\n33.2\n244.6\n105.2\n77.7\n160.2\n63.8\n51.4\n114.7\n7.1\n23.6\n128.1\n13\n2011_10_03_drive_0034\n5087.4\n3.8\n26.1\n264.0\n119.3\n63.9\n170.3\n7.1\n8.2\n125.0\n4.2\n3.9\n139.8\n14\n2011_10_03_drive_0047\n714.2\n10.7\n16.2\n44.6\n419.1\n35.3\n19.0\n2.5\n2.3\n21.3\n1.1\n1.7\n36.3\nAverage scores\n6.0\n24.6\n/\n263.0\n159.3\n/\n9.4\n9.7\n/\n3.0\n4.4\n/\nFigure 4:\nTest results on Seq.3 in the KITTI dataset.\nFigure 5:\nThe position results on Seq.3 in the KITTI dataset.\nFigure 6:\nThe attitude results on Seq.3 in the KITTI dataset.\nSeq. 8, 12, and 13 in the KITTI dataset exhibit missing IMU data spanning several seconds, as illustrated in Fig.\n7\n. Specifically, a 2-second data gap occurs around sample index\nn\n=\n33750\nn=33750\n, resulting in notably increased RTE and RRE values for the MD-IMU methods. In contrast, the proposed method effectively manages this data loss by leveraging available IMU measurements to estimate both the IMU correction term and the noise parameters of the InEKF, as evidenced in Fig.\n8\n. For Seq. 13, the proposed approach reduces the RTE and RRE by 40.8\n%\n\\%\nand 52.4\n%\n\\%\n, respectively, compared to the MD-IMU method. These results underscore the robustness of the proposed method in handling incomplete IMU data while maintaining accurate localization performance. On average, the proposed method achieves RTE and RRE values of 3.0\n%\n\\%\nand 4.4\ndeg\n/\nk\nâ€‹\nm\n\\deg/km\n, significantly outperforming the MD-IMU method, which records 9.7\n%\n\\%\nand 9.4\ndeg\n/\nk\nâ€‹\nm\n\\deg/km\n, respectively.\nTab.\nI\npresents a comparative evaluation of computational efficiency across the tested methods, with runtime (seconds) as the performance metric. The ORB-SLAM3 method exhibits the longest runtime due to its reliance on visual data processing. In contrast, the AI-IMU method achieves the shortest runtime, owing to its lightweight network architecture, though it struggles to maintain robust performance when processing data from low-cost IMUs with substantial sensor noise. Notably, the proposed method significantly improves dead-reckoning accuracy compared to the MD-IMU baseline while incurring only a marginal increase in computational time. Furthermore, when deployed on a 45 nm neuromorphic hardware platform\n[\n32\n]\n, the proposed spiking Transformer architecture achieves a theoretical energy consumption reduction of 66.3\n%\n\\%\nrelative to conventional Transformer-based neural networks\n[\n6\n]\n. This substantial improvement highlights the energy efficiency of the bio-inspired architecture, making it particularly suitable for edge computing applications with stringent power constraints.\nFigure 7:\nIMU data loss of the Seq.13.\nFigure 8:\nThe results of the proposed method on Seq.13 with IMU data loss.\nV-D\nField Tests\nTo further validate the effectiveness of the proposed method, real-world field tests are conducted using a self-developed mobile robot, as illustrated in Fig.\n9\n. The robot is equipped with a CUAV Pixhawk V5 Nano autopilot, a high-performance autopilot module suitable for fixed-wing aircraft, multi-rotors, ground vehicles, and other robotic platforms. Ground-truth trajectory data are provided by a Ublox NEO-M9N GPS receiver, offering a horizontal positioning accuracy of approximately 0.7 meters. The proposed method utilizes raw inertial data collected from an ICM-20602 IMU at a sampling rate of 100 Hz. It is worth noting that the ICM-20602 is a low-cost MEMS IMU with substantially lower accuracy compared to the high-precision OxTS RT3000v3 IMU used in the KITTI dataset, highlighting the practical applicability of the proposed approach in resource-constrained scenarios.\nFigure 9:\nThe self-build mobile robot platform.\nTABLE II:\nExperimental results in the real filed tests\nSeq.\nLength(m)\nProposed\nRTE\nRRE\n1\n182.1\n4.3\n4.1\n2\n110.6\n6.7\n6.4\n3\n99.7\n6.1\n8.5\n4\n121.3\n7.6\n10.3\n5\n91.5\n5.8\n7.2\nAverage scores\n6.1\n7.3\nFigure 10:\nThe results on Seq.1 in the real field tests.\nThe experimental results of the proposed method on real-world field tests are presented in Tab.\nII\nand Fig.\n10\n. As shown, the proposed method achieves average RTE and RRE values of 6.1\n%\n\\%\nand 7.3\ndeg\n/\nk\nâ€‹\nm\n\\deg/km\n, respectively. It is important to note that the low-cost ICM-20602 IMU used in these tests is highly susceptible to road surface irregularities and chassis vibrations due to its limited design precision. Compared to the high-grade sensor used in the KITTI dataset, this IMU produces significantly noisier measurements under dynamic conditions. In challenging sequences (e.g., Seqs. 3, 4, and 5), where conventional neural networks struggle to optimize parameters due to excessive noise, the proposed method exhibits strong adaptability through cross-sequence feature learning. Specifically, the spiking Transformer architecture effectively captures motion dynamics from higher-quality sequences while modeling noise characteristics via event-driven sparse representations. Although the localization accuracy is slightly reduced compared to results on the KITTI dataset, the proposed method consistently mitigates the effects of noise and bias. As shown in Fig.\n10\n, the estimated trajectory closely aligns with the ground truth, demonstrating the methodâ€™s robustness and effectiveness in real-world applications.\nVI\nConclusion\nIn this paper, we propose a novel SNN-based IMU dead reckoning method for low-cost\nIMU. The proposed method employs a spiking Transformer to extract the temporal\nfeatures and long-term dependencies in the IMU measurements, which enables the\nSNN to learn the complex dynamics of the IMU measurements and accurately\nestimate the IMU correction term and the noise parameters of the InEKF. The proposed\nmethod is evaluated on the KITTI dataset, and the experimental results\ndemonstrate that the proposed method outperforms the state-of-the-art methods\nin terms of localization accuracy and robustness to data loss. The proposed method\ncan be applied to various applications, such as autonomous driving, robotics,\nand augmented reality, where accurate and robust localization is essential. In\nthe future, we will explore the integration of the proposed method with other sensors,\nsuch as cameras and LiDAR, to further improve the localization accuracy and robustness.\nAdditionally, we will investigate the application of the proposed method to other\ndatasets and scenarios, such as indoor environments and dynamic scenes, to evaluate\nits generalization capability and robustness.\nReferences\n[1]\nM.Â Brossard, A.Â Barrau, and S.Â Bonnabel, â€œAi-imu dead-reckoning,â€\nIEEE Transactions on Intelligent Vehicles\n, vol.Â 5, no.Â 4, pp.Â 585â€“595, 2020.\n[2]\nF.Â Guo, H.Â Yang, X.Â Wu, H.Â Dong, Q.Â Wu, and Z.Â Li, â€œModel-based deep learning for low-cost imu dead reckoning of wheeled mobile robot,â€\nIEEE Transactions on Industrial Electronics\n, vol.Â 71, no.Â 7, pp.Â 7531â€“7541, 2023.\n[3]\nH.Â Zhou, Y.Â Zhao, X.Â Xiong, Y.Â Lou, and S.Â Kamal, â€œImu dead-reckoning localization with rnn-iekf algorithm,â€ in\n2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, pp.Â 11382â€“11387, IEEE, 2022.\n[4]\nM.Â Yao, G.Â Zhao, H.Â Zhang, Y.Â Hu, L.Â Deng, Y.Â Tian, B.Â Xu, and G.Â Li, â€œAttention spiking neural networks,â€\nIEEE transactions on pattern analysis and machine intelligence\n, vol.Â 45, no.Â 8, pp.Â 9393â€“9410, 2023.\n[5]\nH.Â Fang, A.Â Shrestha, and Q.Â Qiu, â€œMultivariate time series classification using spiking neural networks,â€ in\n2020 International Joint Conference on Neural Networks (IJCNN)\n, pp.Â 1â€“7, IEEE, 2020.\n[6]\nC.Â Lv, Y.Â Wang, D.Â Han, X.Â Zheng, X.Â Huang, and D.Â Li, â€œEfficient and effective time-series forecasting with spiking neural networks,â€\narXiv preprint arXiv:2402.01533\n, 2024.\n[7]\nR.Â Stagsted, A.Â Vitale, J.Â Binz, L.Â BondeÂ Larsen, Y.Â Sandamirskaya,\netÂ al.\n, â€œTowards neuromorphic control: A spiking neural network based pid controller for uav,â€ RSS, 2020.\n[8]\nF.Â Huang, Z.Â Wang, L.Â Xing, and C.Â Gao, â€œA mems imu gyroscope calibration method based on deep learning,â€\nIEEE Transactions on Instrumentation and Measurement\n, vol.Â 71, pp.Â 1â€“9, 2022.\n[9]\nD.Â W. Allan, â€œStatistics of atomic frequency standards,â€\nProceedings of the IEEE\n, vol.Â 54, no.Â 2, pp.Â 221â€“230, 2005.\n[10]\nP.Â Furgale, T.Â D. Barfoot, and G.Â Sibley, â€œContinuous-time batch estimation using temporal basis functions,â€ in\n2012 IEEE International Conference on Robotics and Automation\n, pp.Â 2088â€“2095, IEEE, 2012.\n[11]\nJ.Â Lu, L.Â Ye, Z.Â Niu, J.Â Dong, and A.Â Su, â€œAn all-parameter calibration for 6-axis skewed imu,â€\nIEEE Transactions on Industrial Electronics\n, vol.Â 70, no.Â 3, pp.Â 3126â€“3135, 2022.\n[12]\nF.Â Lin, Q.Â Cai, Y.Â Liu, Y.Â Chen, J.Â Huang, and H.Â Peng, â€œPedestrian dead reckoning method based on array imu,â€\nIEEE Sensors Journal\n, 2024.\n[13]\nY.Â Yang, P.Â Geneva, X.Â Zuo, and G.Â Huang, â€œOnline self-calibration for visual-inertial navigation: Models, analysis, and degeneracy,â€\nIEEE Transactions on Robotics\n, vol.Â 39, no.Â 5, pp.Â 3479â€“3498, 2023.\n[14]\nJ.Â H. Jung, S.Â Heo, and C.Â G. Park, â€œObservability analysis of imu intrinsic parameters in stereo visualâ€“inertial odometry,â€\nIEEE Transactions on Instrumentation and Measurement\n, vol.Â 69, no.Â 10, pp.Â 7530â€“7541, 2020.\n[15]\nG.Â Dissanayake, S.Â Sukkarieh, E.Â Nebot, and H.Â Durrant-Whyte, â€œThe aiding of a low-cost strapdown inertial measurement unit using vehicle model constraints for land vehicle applications,â€\nIEEE transactions on robotics and automation\n, vol.Â 17, no.Â 5, pp.Â 731â€“747, 2002.\n[16]\nA.Â Barrau and S.Â Bonnabel, â€œThe invariant extended kalman filter as a stable observer,â€\nIEEE Transactions on Automatic Control\n, vol.Â 62, no.Â 4, pp.Â 1797â€“1812, 2016.\n[17]\nK.Â C. Guyard, J.Â Bertolaccini, S.Â Montavon, and M.Â Deriaz, â€œA transformer encoder approach for localization reconstruction during gps outages from an imu and gps-based sensor,â€\nSensors\n, vol.Â 25, no.Â 2, p.Â 522, 2025.\n[18]\nJ.Â Rehder, J.Â Nikolic, T.Â Schneider, T.Â Hinzmann, and R.Â Siegwart, â€œExtending kalibr: Calibrating the extrinsics of multiple imus and of individual axes,â€ in\n2016 IEEE International Conference on Robotics and Automation (ICRA)\n, pp.Â 4304â€“4311, IEEE, 2016.\n[19]\nJ.Â Rohac, M.Â Sipos, and J.Â Simanek, â€œCalibration of low-cost triaxial inertial sensors,â€\nIEEE Instrumentation & Measurement Magazine\n, vol.Â 18, no.Â 6, pp.Â 32â€“38, 2015.\n[20]\nW.Â Maass, â€œNetworks of spiking neurons: the third generation of neural network models,â€\nNeural networks\n, vol.Â 10, no.Â 9, pp.Â 1659â€“1671, 1997.\n[21]\nW.Â Fang, Y.Â Chen, J.Â Ding, Z.Â Yu, T.Â Masquelier, D.Â Chen, L.Â Huang, H.Â Zhou, G.Â Li, and Y.Â Tian, â€œSpikingjelly: An open-source machine learning infrastructure platform for spike-based intelligence,â€\nScience Advances\n, vol.Â 9, no.Â 40, p.Â eadi1480, 2023.\n[22]\nK.Â Roy, A.Â Jaiswal, and P.Â Panda, â€œTowards spike-based machine intelligence with neuromorphic computing,â€\nNature\n, vol.Â 575, no.Â 7784, pp.Â 607â€“617, 2019.\n[23]\nE.Â Qu, Y.Â Wang, X.Â Luo, W.Â He, K.Â Ren, and D.Â Li, â€œCnn kernels can be the best shapelets,â€ in\nThe Twelfth International Conference on Learning Representations\n, 2024.\n[24]\nY.Â Liu, T.Â Hu, H.Â Zhang, H.Â Wu, S.Â Wang, L.Â Ma, and M.Â Long, â€œitransformer: Inverted transformers are effective for time series forecasting,â€\narXiv preprint arXiv:2310.06625\n, 2023.\n[25]\nZ.Â Zhou, Y.Â Zhu, C.Â He, Y.Â Wang, S.Â Yan, Y.Â Tian, and L.Â Yuan, â€œSpikformer: When spiking neural network meets transformer,â€\narXiv preprint arXiv:2209.15425\n, 2022.\n[26]\nA.Â Gruslys, R.Â Munos, I.Â Danihelka, M.Â Lanctot, and A.Â Graves, â€œMemory-efficient backpropagation through time,â€\nAdvances in neural information processing systems\n, vol.Â 29, 2016.\n[27]\nG.Â P. Meyer, â€œAn alternative probabilistic interpretation of the huber loss,â€ in\nProceedings of the ieee/cvf conference on computer vision and pattern recognition\n, pp.Â 5261â€“5269, 2021.\n[28]\nA.Â Geiger, P.Â Lenz, and R.Â Urtasun, â€œAre we ready for autonomous driving? the kitti vision benchmark suite,â€ in\n2012 IEEE conference on computer vision and pattern recognition\n, pp.Â 3354â€“3361, IEEE, 2012.\n[29]\nC.Â Campos, R.Â Elvira, J.Â J.Â G. RodrÃ­guez, J.Â M. Montiel, and J.Â D. TardÃ³s, â€œOrb-slam3: An accurate open-source library for visual, visualâ€“inertial, and multimap slam,â€\nIEEE transactions on robotics\n, vol.Â 37, no.Â 6, pp.Â 1874â€“1890, 2021.\n[30]\nD.Â P. Kingma and J.Â Ba, â€œAdam: A method for stochastic optimization,â€\narXiv preprint arXiv:1412.6980\n, 2014.\n[31]\nI.Â Loshchilov and F.Â Hutter, â€œSgdr: Stochastic gradient descent with warm restarts,â€\narXiv preprint arXiv:1608.03983\n, 2016.\n[32]\nM.Â Horowitz, â€œ1.1 computingâ€™s energy problem (and what we can do about it),â€ in\n2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC)\n, pp.Â 10â€“14, IEEE, 2014.",
  "preview_text": "Low-cost inertial measurement units (IMUs) are widely utilized in mobile robot localization due to their affordability and ease of integration. However, their complex, nonlinear, and time-varying noise characteristics often lead to significant degradation in localization accuracy when applied directly for dead reckoning. To overcome this limitation, we propose a novel brain-inspired state estimation framework that combines a spiking neural network (SNN) with an invariant extended Kalman filter (InEKF). The SNN is designed to extract motion-related features from long sequences of IMU data affected by substantial random noise and is trained via a surrogate gradient descent algorithm to enable dynamic adaptation of the covariance noise parameter within the InEKF. By fusing the SNN output with raw IMU measurements, the proposed method enhances the robustness and accuracy of pose estimation. Extensive experiments conducted on the KITTI dataset and real-world data collected using a mobile robot equipped with a low-cost IMU demonstrate that the proposed approach outperforms state-of-the-art methods in localization accuracy and exhibits strong robustness to sensor noise, highlighting its potential for real-world mobile robot applications.\n\nSpiking Neural-Invariant Kalman Fusion for Accurate Localization Using Low-Cost IMUs\nYaohuaÂ Liu, QiaoÂ Xu, YeminÂ Wang, Hui YiÂ Leong, BinkaiÂ Ou\nY. Liu is with Guangdong Institute of Intelligence Science and Technology,\nHengqin, Zhuhai, Guangdong, China, 519031, e-mail: liuyaohua@gdiist.cn.Q. Xu is with East China Normal University, Shanghai, 200062, ChinaY. Wang is with Xiamen University, Xiamen, Fujian, 360000, ChinaH. Leng is with University of Chicago, AmericaB. Ou is with Innovation and Research and Development Department, BoardWare Information System Co.Ltd, Macau, 999078, China.\nAbstract\nLow-cost inertial measurement units (IMUs) are widely utilized in mobile robot localization due to their affordability and ease of integration. Howev",
  "is_relevant": false,
  "relevance_score": 1.0,
  "extracted_keywords": [
    "IMU",
    "localization",
    "spiking neural network",
    "Kalman filter",
    "mobile robot"
  ],
  "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆè„‰å†²ç¥ç»ç½‘ç»œå’Œä¸å˜æ‰©å±•å¡å°”æ›¼æ»¤æ³¢çš„æ–°æ¡†æ¶ï¼Œç”¨äºæå‡ä½æˆæœ¬IMUåœ¨ç§»åŠ¨æœºå™¨äººå®šä½ä¸­çš„ç²¾åº¦å’Œé²æ£’æ€§ã€‚",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-13T06:12:12Z",
  "created_at": "2026-01-20T17:49:41.576464",
  "updated_at": "2026-01-20T17:49:41.576472"
}