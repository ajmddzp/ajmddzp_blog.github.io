{
    "id": "2601.07454v1",
    "title": "WaveMan: mmWave-Based Room-Scale Human Interaction Perception for Humanoid Robots",
    "authors": [
        "Yuxuan Hu",
        "Kuangji Zuo",
        "Boyu Ma",
        "Shihao Li",
        "Zhaoyang Xia",
        "Feng Xu",
        "Jianfei Yang"
    ],
    "abstract": "åœ¨å®¶åº­çŽ¯å¢ƒä¸­å®žçŽ°å¯é çš„äººå½¢æœºå™¨äººäº¤äº’é¢ä¸´ä¸¤å¤§åŸºç¡€æ€§çº¦æŸï¼šå¯¹ç”¨æˆ·ä»»æ„ä½ç½®çš„é²æ£’æ€§ä»¥åŠç”¨æˆ·éšç§ä¿æŠ¤ã€‚æ¯«ç±³æ³¢ä¼ æ„ŸæŠ€æœ¯å› å…¶å›ºæœ‰çš„éšç§ä¿æŠ¤ç‰¹æ€§ï¼Œæˆä¸ºå®žçŽ°æˆ¿é—´å°ºåº¦äººå½¢æœºå™¨äººäº¤äº’çš„ç†æƒ³æ¨¡æ€ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰åŸºäºŽæ¯«ç±³æ³¢çš„äº¤äº’æ„ŸçŸ¥ç³»ç»Ÿåœ¨æœªè®­ç»ƒè¿‡çš„è·ç¦»æˆ–è§†è§’ä¸‹è¡¨çŽ°å‡ºè¾ƒå·®çš„ç©ºé—´æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†WaveManâ€”â€”ä¸€ç§ç©ºé—´è‡ªé€‚åº”çš„æˆ¿é—´å°ºåº¦æ„ŸçŸ¥ç³»ç»Ÿï¼Œå¯åœ¨ä»»æ„ç”¨æˆ·ä½ç½®å®žçŽ°å¯é çš„äººç±»äº¤äº’æ„ŸçŸ¥ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è§†è§’å¯¹é½ä¸Žé¢‘è°±å›¾å¢žå¼ºæŠ€æœ¯ä¿éšœç©ºé—´ä¸€è‡´æ€§ï¼Œå¹¶é‡‡ç”¨åŒé€šé“æ³¨æ„åŠ›æœºåˆ¶å®žçŽ°é²æ£’ç‰¹å¾æå–ã€‚åœ¨äº”ä½å‚ä¸Žè€…çš„å®žéªŒè¡¨æ˜Žï¼šåœ¨å›ºå®šä½ç½®è¯„ä¼°ä¸­ï¼ŒWaveManä»…éœ€åŸºçº¿æ–¹æ³•äº”åˆ†ä¹‹ä¸€çš„è®­ç»ƒä½ç½®å³å¯è¾¾åˆ°åŒç­‰è·¨ä½ç½®å‡†ç¡®çŽ‡ï¼›åœ¨éšæœºè‡ªç”±ä½ç½®æµ‹è¯•ä¸­ï¼Œå‡†ç¡®çŽ‡ä»Ž33.00%æå‡è‡³94.33%ã€‚è¿™äº›ç»“æžœéªŒè¯äº†åœ¨ä¸å—é™ç”¨æˆ·ä½ç½®æ¡ä»¶ä¸‹ï¼Œå®žçŽ°å¯é ä¸”ä¿æŠ¤éšç§çš„å®¶åº­äººå½¢æœºå™¨äººäº¤äº’çš„å¯è¡Œæ€§ã€‚",
    "url": "https://arxiv.org/abs/2601.07454v1",
    "html_url": "https://arxiv.org/html/2601.07454v1",
    "html_content": "WaveMan: mmWave-Based Room-Scale Human Interaction Perception for Humanoid Robots\nYuxuan Hu, Kuangji Zuo, Boyu Ma, Shihao Li, Zhaoyang Xia,\nFeng Xu, , and Jianfei Yang\nThis work is supported by MOE Singapore Tier 1 Grant RG83/25, RS36/24 and a Start-up Grant from Nanyang Technological University. (Corresponding authors: Feng Xu, Jianfei Yang).Yuxuan Hu, Zhaoyang Xia, and Feng Xu are with the Key Laboratory for Information Science of Electromagnetic Waves, Ministry of Education, School of Information Science and Technology, Fudan University, Shanghai 200433, China (e-mail: huyx23@m.fudan.edu.cn; xiazy@fudan.edu.cn; fengxu@fudan.edu.cn).Yuxuan Hu is also with the School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore 639798, as a visiting Ph.D. student. Kuangji Zuo, Boyu Ma, Shihao Li, and Jianfei Yang are with the School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore 639798 (e-mail: kuangji001@e.ntu.edu.sg; boyu.ma@ntu.edu.sg; n2409375d@e.ntu.edu.sg; jianfei.yang@ntu.edu.sg).\nAbstract\nReliable humanoid-robot interaction (HRI) in household environments is constrained by two fundamental requirements, namely robustness to unconstrained user positions and preservation of user privacy. Millimeter-wave (mmWave) sensing inherently supports privacy-preserving interaction, making it a promising modality for room-scale HRI. However, existing mmWave-based interaction-sensing systems exhibit poor spatial generalization at unseen distances or viewpoints. To address this challenge, we introduce WaveMan, a spatially adaptive room-scale perception system that restores reliable human interaction sensing across arbitrary user positions. WaveMan integrates viewpoint alignment and spectrogram enhancement for spatial consistency, with dual-channel attention for robust feature extraction. Experiments across five participants show that, under fixed-position evaluation, WaveMan achieves the same cross-position accuracy as the baseline with five times fewer training positions. In random free-position testing, accuracy increases from 33.00% to 94.33%, enabled by the proposed method. These results demonstrate the feasibility of reliable, privacy-preserving interaction for household humanoid robots across unconstrained user positions.\nI\nIntroduction\nHumanoid robots have achieved substantial progress in perception, manipulation, and control in recent years\n[\n1\n]\n. However, in human-centered interaction scenarios, reliably understanding human intent while preserving user privacy remains a key bottleneck hindering their deployment in everyday environments. In real indoor settings, users may initiate interaction from arbitrary locations within a room, invalidating traditional assumptions of front-facing, short-range, or user-aligned interaction. Consequently, intent understanding must remain reliable under significant variations in distance, viewpoint, and spatial relationships\n[\n13\n]\n.\nGestures constitute an intuitive humanâ€“robot interaction modality that conveys intent without speech or wearable devices, making them suitable for natural interaction in unconstrained spatial settings\n[\n6\n]\n. However, most existing gesture recognition systems are designed for short-range scenarios with fixed viewpoints, limiting their applicability in real environments\n[\n11\n]\n. In unconstrained humanâ€“robot interaction scenarios, the system cannot assume control over user pose or sensing viewpoint, and thus must rely on passively acquired observations from a fixed sensor. Accordingly, we consider an ambient sensing setting in which an environment-installed mmWave radar (e.g., a smart-home device) provides privacy-preserving observations to the robot for interaction. Under this setting, the relative distance, azimuth, and elevation between the user and the sensing node can vary substantially, imposing stricter requirements on the stability of room-scale gesture recognition across positions and viewpoints, as illustrated in Fig.\n1\n.\nFigure 1:\nSpatially adaptive room-scale interaction scenario.\nWaveMan aligns observations from different user spatial positions into a unified perception space to mitigate spatial inconsistencies.\nMillimeter-wave (mmWave) radar has recently emerged as a promising sensing modality for room-scale gesture recognition\n[\n4\n]\n. Its non-intrusive sensing capability, robustness to lighting conditions and occlusion, and inherent privacy-preserving properties make it particularly suitable for long-term deployment in domestic environments to support humanoid interaction, offering clear advantages over vision-based\n[\n10\n]\n, audio-based\n[\n29\n]\n, and contact-based\n[\n19\n]\nsystems. Prior studies have demonstrated that mmWave radar can capture fine-grained hand motion dynamics with high temporal resolution\n[\n24\n]\n. However, most existing mmWave-based gesture recognition methods remain limited to short-range or frontal configurations\n[\n22\n]\n, resulting in poor robustness under room-scale interaction conditions. Although recent efforts have explored long-range mmWave gesture recognition\n[\n17\n]\n, many of these approaches rely on raw ADC data, incurring high data throughput and bandwidth requirements that hinder practical deployment on resource-constrained in-home embedded sensing nodes or edge hubs.\nPoint-cloud-based mmWave methods leverage chip-level preprocessing to extract more compact spatial representations, partially alleviating system-level data throughput and bandwidth constraints\n[\n21\n,\n25\n]\n. Existing approaches typically follow two paradigms: (i) end-to-end learning directly on raw or lightly processed point cloud sequences\n[\n21\n]\n, and (ii) transforming point cloud sequences into denser representations, such as timeâ€“frequency spectrograms, for deep learning\n[\n25\n]\n. Despite notable progress\n[\n27\n]\n, room-scale gesture recognition remains challenging. Substantial variations in user spatial position cause the same gesture to exhibit highly inconsistent observations across different distances and viewpoints, leading to pronounced feature distribution shifts and degraded generalization performance under unseen positions or viewpoints\n[\n15\n,\n14\n]\n. Moreover, gestures performed at long distances or off-axis directions typically produce weaker and sparser echoes with lower signal-to-noise ratios, further impairing the reliability of spectrogram-based representations\n[\n2\n,\n16\n]\n. Together, these factors induce geometry-, spectrum-, and direction-domain distribution shifts driven by spatial position changes, making existing methods ineffective under the random spatial configurations in domestic environments where the radar viewpoint is fixed. As a result, room-scale mmWave-based gesture recognition remains a largely unresolved challenge.\nTo address these challenges, this paper proposes a spatially adaptive gesture recognition framework tailored for room-scale humanâ€“robot interaction. The main contributions of this work are summarized as follows:\n1.\nA spatially adaptive room-scale perception framework for humanoid robot interaction.\nWe develop a room-scale gesture interaction system for humanoid robots that integrates mmWave sensing with robot behavior execution, enabling stable interaction performance across varying user positions.\n2.\nGeometry-, spectrum-, and direction-aware representation learning.\nWe propose a spatially adaptive processing pipeline to address key spatial variations induced by user position changes in room-scale interaction scenarios. The pipeline consists of: (i) geometric alignment to compensate for position and viewpoint variations; (ii) unsupervised spectrogram enhancement to mitigate long-range sparsity and low signal-to-noise ratios; and (iii) an attention-based recognition network that fuses multi-channel information to improve robustness across positions.\n3.\nRoom-scale deployment and real-world humanoid robot evaluation.\nThe proposed framework is deployed on a humanoid robot platform and evaluated under fixed-position, unseen-position, and random-position settings. Experimental results demonstrate reliable whole-room intent understanding, stable robot behavior execution, and significant performance improvements over existing methods.\nAn overview of the entire framework is presented in Fig.\n2\n.\nThe remainder of this paper is organized as follows:\nSectionÂ II reviews related work;\nSectionÂ III introduces the mmWave radar signal model and spectrogram derivation;\nSectionÂ IV presents the proposed spatially adaptive perception framework;\nSectionÂ V describes experimental setups and results;\nand SectionÂ VI concludes the paper.\nFigure 2:\nOverview of the proposed spatially adaptive interaction framework.\n(a) Radar point-cloud data captured under diverse positional configurations are spatially aligned and transformed into spectrogram representations.\n(b) Sparse spectrograms are enhanced and fused with dense spectra to obtain robust gesture representations.\n(c) The recognized gestures are mapped to corresponding humanoid robot behaviors, enabling reliable humanâ€“robot interaction across unconstrained user positions.\nII\nRelated Work\nII-A\nGesture-based Humanâ€“Robot Interaction\nGesture-based interaction has become an important modality for natural and contact-free human-robot communication\n[\n5\n]\n. Millimeter-wave (mmWave) radar is particularly appealing for HRI due to its robustness to lighting variations, capability of capturing fine-grained motion, and strong privacy preservation\n[\n20\n]\n. Prior systems based on FMCW and impulse radars have demonstrated reliable gesture-driven control of robots in structured environments\n[\n24\n]\n. However, most radar-based gesture recognition frameworks are developed for short-range and fixed-view setups, where the relative user-radar geometry remains stable\n[\n22\n]\n. These constraints limit their applicability to room-scale scenarios, in which users can move freely and perform gestures from diverse spatial positions and orientations. Such settings introduce two fundamental challenges: spatial variation causing position-dependent feature inconsistency, and signal degradation resulting in sparse or incomplete spectrograms under long-range or off-axis conditions. These challenges motivate the need for radar-based HRI systems that can operate robustly across wide spatial configurations, as elaborated in the following subsections.\nII-B\nSpatial Robustness in Gesture Recognition\nThe spatial relationship between the user and radar varies substantially in room-scale environments, leading to geometric distortions and inconsistent feature representations that impair recognition under unseen viewpoints. Several learning-based strategies have been proposed to mitigate this issue. Zhang et al.\n[\n26\n]\nemployed a convolutional autoencoder to extract position-invariant gesture features, while Li et al.\n[\n15\n]\nutilized signal-aware data augmentation to improve generalization across unseen spatial configurations. Xia et al.\n[\n25\n]\nintroduced a viewpoint alignment scheme to normalize multi-view gesture data, and Li et al.\n[\n14\n]\napplied domain adaptation to reduce distribution gaps caused by varying user positions. In addition, a ShuffleNet-based system has been shown to achieve high accuracy on 43 alphanumeric and symbolic gestures using a single radar unit\n[\n12\n]\n. Although these approaches improve model-level robustness, most operate at the feature level and do not explicitly standardize the geometric structure of the radar input. Consequently, variations in user-radar distance, azimuth, and elevation can still lead to inconsistent spatial patterns, limiting robustness in unseen room-scale conditions.\nII-C\nSparse Gesture Representations\nLong-range or off-axis gestures often produce sparse and low-SNR spectrograms due to weak reflections and incomplete signal returns, posing another major challenge for room-scale recognition. Efforts to address this problem include both network-level and data-level enhancement strategies. Dong et al.\n[\n2\n]\nproposed spatiotemporal deformable convolution and context-aware modules to improve robustness against degraded inputs, while Towakel et al.\n[\n23\n]\ndesigned a multimodal attention mechanism to enhance temporal fusion. Data-centered methods such as spectrogram inpainting, physically interpretable augmentation\n[\n7\n]\n, and deep-learning-based reconstruction\n[\n16\n]\nhave also shown promise in restoring missing or attenuated signal components. However, these methods often target specific degradation patterns or operate under fixed spatial configurations, making them less effective when degradation is tightly coupled with spatial variation. This limitation highlights the need for adaptive enhancement mechanisms that can account for user-radar geometry and provide consistent representations across room-scale environments.\nIII\nRadar Signal Model and Spectrogram Derivation\nThis section summarizes the signal processing principles of the MIMO FMCW radar employed in this work, which serves as the primary sensing modality for humanoid robot interaction in indoor environments.\nIII-A\nFMCW Signal Model\nThe transmitted waveform of a linear frequency-modulated continuous-wave (LFMCW) radar during a single chirp is\nS\ntx\nâ€‹\n(\nt\n)\n=\nA\nT\nâ€‹\nexp\nâ¡\n[\nâˆ’\nj\nâ€‹\n2\nâ€‹\nÏ€\nâ€‹\n(\nf\n0\nâ€‹\nt\n+\nB\n2\nâ€‹\nT\ns\nâ€‹\nt\n2\n)\n]\n,\n0\nâ‰¤\nt\nâ‰¤\nT\ns\n,\nS_{\\mathrm{tx}}(t)=A_{\\mathrm{T}}\\exp\\!\\left[-j2\\pi\\!\\left(f_{0}t+\\frac{B}{2T_{\\mathrm{s}}}t^{2}\\right)\\right],\\quad 0\\leq t\\leq T_{\\mathrm{s}},\n(1)\nwhere\nA\nT\nA_{\\mathrm{T}}\nis the amplitude,\nf\n0\nf_{0}\nis the carrier frequency,\nB\nB\nis the bandwidth, and\nT\ns\nT_{\\mathrm{s}}\nis the chirp duration.\nA target at range\nR\nR\nand radial velocity\nv\nv\nreturns a delayed and Doppler-shifted copy of the transmitted chirp. Mixing the received and transmitted signals followed by low-pass filtering produces the intermediate-frequency (IF) signal\nS\nif\nâ€‹\n(\nt\n)\n=\nA\nT\nâ€‹\nA\nR\nâ€‹\nexp\nâ¡\n[\nj\nâ€‹\n(\nÏ•\ntx\nâ€‹\n(\nt\n)\nâˆ’\nÏ•\nrx\nâ€‹\n(\nt\n)\n)\n]\n,\nS_{\\mathrm{if}}(t)=A_{\\mathrm{T}}A_{\\mathrm{R}}\\exp\\!\\left[j(\\phi_{\\mathrm{tx}}(t)-\\phi_{\\mathrm{rx}}(t))\\right],\n(2)\nwhose instantaneous phase encodes both range and velocity.\nA fast-time FFT across samples within each chirp yields the range spectrum, while a slow-time FFT across consecutive chirps extracts Doppler information. Combining the two produces a two-dimensional range-Doppler (RD) map. The range and velocity resolutions follow standard FMCW relationships\n[\n18\n]\n, i.e.,\nÎ”\nâ€‹\nr\n=\nc\n/\n(\n2\nâ€‹\nB\n)\n\\Delta r=c/(2B)\nand\nÎ”\nâ€‹\nv\n=\nÎ»\n/\n(\n2\nâ€‹\nN\nchirp\nâ€‹\nT\ns\n)\n\\Delta v=\\lambda/(2N_{\\mathrm{chirp}}T_{\\mathrm{s}})\n.\nIII-B\nAngle Estimation and 5D Point-Cloud Representation\nSpatial directionality is obtained via digital beamforming (DBF) on the MIMO virtual array. One-dimensional DBF estimates azimuth, while two-dimensional DBF jointly estimates azimuth and elevation, forming a three-dimensional angleâ€“elevation (AE) spectrum.\nEach radar frame produces a set of scattering points represented as\nP\n=\n{\n(\nA\n~\ni\n,\nr\ni\n,\nv\ni\n,\nh\ni\n,\ne\ni\n)\nâˆ£\ni\n=\n1\n,\nâ€¦\n,\nN\nP\n}\n,\nP=\\left\\{\\big(\\tilde{A}_{i},r_{i},v_{i},h_{i},e_{i}\\big)\\mid i=1,\\ldots,N_{\\mathrm{P}}\\right\\},\n(3)\nwhere\nA\n~\ni\n\\tilde{A}_{i}\ndenotes the complex amplitude, and\nr\ni\nr_{i}\n,\nv\ni\nv_{i}\n,\nh\ni\nh_{i}\n, and\ne\ni\ne_{i}\nare the range, radial velocity, azimuth, and elevation of the\ni\ni\n-th reflection, respectively. The angular components are converted to Cartesian coordinates\n(\nx\ni\n,\ny\ni\n,\nz\ni\n)\n(x_{i},y_{i},z_{i})\n, yielding a 3D radar point cloud that serves as the geometric input for subsequent spatial alignment.\nIII-C\nMulti-Domain Time-Spectrogram Construction\nTo characterize gesture dynamics over time, multiple time-spectrograms are constructed across different physical domains, including rangeâ€“time (RT), Dopplerâ€“time (DT), azimuthâ€“time (HT), elevationâ€“time (ET), and positionâ€“time (XT, YT, ZT) representations derived from Cartesian coordinates.\nThese multi-domain spectrograms provide complementary descriptions of gesture evolution in range, velocity, direction, and spatial displacement, and are used as structured inputs to the proposed spatially adaptive perception modules.\nIV\nSpatially Adaptive Perception Framework\nIV-A\nSystem Overview and Robot Integration\nThis section introduces the proposed spatially adaptive perception framework for room-scale humanâ€“robot interaction in humanoid robot systems, which explicitly accounts for unconstrained user positions and viewpoints (Fig.\n2\n). The framework processes streaming mmWave radar measurements through a unified pipeline for room-scale humanoid interaction, including spatially adaptive point cloud alignment, spectrogram-based feature construction and enhancement, and viewpoint-adaptive recognition with dual-channel attention to infer interaction gestures.\nThe perception framework runs on an external workstation, while interaction control is executed on the humanoid robotâ€™s onboard controller, with inferred gesture labels and confidence transmitted via UDP to form a closed perceptionâ€“action loop. During operation, the system runs online in a streaming manner, producing interaction predictions over temporal windows and enabling consistent robot responses across room-scale distances and viewpoints. All experimental evaluations in SectionÂ V follow this setup.\nFigure 3:\nSpatially adaptive point cloud alignment.\n(a) shows the geometric relationship between the real radar position\nO\na\n\\mathrm{O_{a}}\n, the virtual canonical radar position\nO\nb\n\\mathrm{O_{b}}\n, and the observed point\nP\n\\mathrm{P}\n, where azimuth and elevation offsets are compensated through sequential rotations and translation.\n(b) presents an example point cloud before and after alignment, illustrating reduced angular distortion and a more compact spatial distribution for subsequent spectrogram generation.\nFigure 4:\nArchitecture of the proposed spectrogram enhancement and recognition network.\n(a) Unpaired spectrogram enhancement pipeline, where sparse spectral inputs are translated into dense representations using an Enhancerâ€“Reducer pair supervised by two discriminators.\n(b) Internal network components, including the Enhancer/Reducer architectures, PatchGAN-based discriminators, and a compact CNN equipped with the proposed Dual-Branch Channel Attention (DBCA) module for generalization-oriented gesture recognition.\nIV-B\nSpatially Adaptive Point Cloud Alignment\nNon-frontal radar perspectives introduce significant geometric distortions in raw point clouds, including spatial compression, angular skew, and amplitude attenuation due to oblique reflections. Moreover, the nonlinear mapping from spherical measurements to Cartesian coordinates further amplifies viewpoint-dependent inconsistencies. To unify the appearance of gestures observed at different distances and orientations, we propose a spatially adaptive point cloud alignment algorithm that reprojects each aggregated gesture instance into a canonical front-facing configuration, as shown in Fig.\n3\n.\nIV-B\n1\nMotivation and Geometric Deformation\nWhen a user performs gestures at nonzero elevation or azimuth relative to the antenna boresight, the radar observes a geometrically distorted point cloud. This distortion arises from oblique-angle surface projection and nonlinear spherical-to-Cartesian mapping in the radar measurement model, causing identical gestures to exhibit inconsistent spatial distribution, density, and energy across different userâ€“radar configurations. The goal of spatial alignment is therefore to suppress such viewpoint-induced variations by transforming each point cloud into a canonical coordinate system corresponding to a frontal user orientation.\nIV-B\n2\nCanonical Reprojection Model\nLet the superscript\n(j)\ndenote the\nj\nj\n-th aggregated point cloud of a gesture instance. Subscripts\na\n\\mathrm{a}\nand\nb\n\\mathrm{b}\ndenote the original and aligned coordinate systems, respectively. The transformation from\na\n\\mathrm{a}\nto\nb\n\\mathrm{b}\njointly compensates elevation and azimuth offsets, performs distance normalization, and corrects amplitude attenuation. The reprojection model is defined as\nr\nb\n(\nj\n)\n\\displaystyle r_{\\mathrm{b}}^{(j)}\n=\nr\na\n(\nj\n)\nâ€‹\ncos\nâ¡\n(\nÏ†\n(\nj\n)\n)\nâ€‹\ncos\nâ¡\n(\nÎ¸\n(\nj\n)\n)\n,\n\\displaystyle=r_{\\mathrm{a}}^{(j)}\\cos\\!\\big(\\varphi^{(j)}\\big)\\cos\\!\\big(\\theta^{(j)}\\big),\n(4)\nÏ†\nb\n(\nj\n)\n\\displaystyle\\varphi_{\\mathrm{b}}^{(j)}\n=\narctan\nâ¡\n(\ny\n0\nâ€‹\ntan\nâ¡\n(\nÏ†\na\n(\nj\n)\nâˆ’\nÏ†\n(\nj\n)\n)\n)\n,\n\\displaystyle=\\arctan\\!\\big(y_{0}\\tan(\\varphi_{\\mathrm{a}}^{(j)}-\\varphi^{(j)})\\big),\nÎ¸\nb\n(\nj\n)\n\\displaystyle\\theta_{\\mathrm{b}}^{(j)}\n=\narctan\nâ¡\n(\ny\n0\nâ€‹\ntan\nâ¡\n(\nÎ¸\na\n(\nj\n)\n+\nÎ¸\n(\nj\n)\n)\n)\n,\n\\displaystyle=\\arctan\\!\\big(y_{0}\\tan(\\theta_{\\mathrm{a}}^{(j)}+\\theta^{(j)})\\big),\nA\nb\n(\nj\n)\n\\displaystyle A_{\\mathrm{b}}^{(j)}\n=\nA\na\n(\nj\n)\ncos\nâ¡\n(\nÏ†\n(\nj\n)\n)\nâ€‹\ncos\nâ¡\n(\nÎ¸\n(\nj\n)\n)\n.\n\\displaystyle=\\dfrac{A_{\\mathrm{a}}^{(j)}}{\\cos\\!\\big(\\varphi^{(j)}\\big)\\cos\\!\\big(\\theta^{(j)}\\big)}.\nHere,\nÏ†\n(\nj\n)\n\\varphi^{(j)}\nand\nÎ¸\n(\nj\n)\n\\theta^{(j)}\nare the estimated elevation and azimuth deviations, and\ny\n0\ny_{0}\nis a reference distance constant used to stabilize the virtual reprojection. This formulation approximates a mapping to a virtual frontal viewpoint through empirical calibration.\nThe reprojection standardizes the geometry and amplitude distribution of the point cloud, ensuring a unified spatial structure for gestures observed from different viewpoints.\nIV-B\n3\nReal-Time Parameter Estimation\nTo enable adaptive alignment in real time, the elevation offset\nÏ†\n(\nj\n)\n\\varphi^{(j)}\n, azimuth offset\nÎ¸\n(\nj\n)\n\\theta^{(j)}\n, and mean vertical distance\ny\nÂ¯\n(\nj\n)\n\\bar{y}^{(j)}\nare estimated directly from radar spectrograms. Specifically, range-time (RT), elevation-time (ZT), and azimuth-time (XT) spectrograms are used to extract radial, vertical, and horizontal motion cues, respectively. Let\nO\na\nâ€‹\nP\n(\nj\n)\n\\mathrm{O_{a}P}^{(j)}\ndenote the radial distance between the radar origin\nO\na\n\\mathrm{O_{a}}\nand the representative scattering point\nP\nP\n, and let\nO\na\nâ€‹\nP\nv\n(\nj\n)\n\\mathrm{O_{a}P_{v}}^{(j)}\nand\nO\na\nâ€‹\nP\nh\n(\nj\n)\n\\mathrm{O_{a}P_{h}}^{(j)}\ndenote its vertical and horizontal displacements. The geometric relationships yield\nÏ†\n(\nj\n)\n\\displaystyle\\varphi^{(j)}\n=\narcsin\nâ¡\n(\nO\na\nâ€‹\nP\nv\n(\nj\n)\nO\na\nâ€‹\nP\n(\nj\n)\n)\n,\n\\displaystyle=\\arcsin\\!\\left(\\frac{\\mathrm{O_{a}P_{v}}^{(j)}}{\\mathrm{O_{a}P}^{(j)}}\\right),\n(5)\nÎ¸\n(\nj\n)\n\\displaystyle\\theta^{(j)}\n=\narcsin\nâ¡\n(\nO\na\nâ€‹\nP\nh\n(\nj\n)\nO\na\nâ€‹\nP\n(\nj\n)\n)\n.\n\\displaystyle=\\arcsin\\!\\left(\\frac{\\mathrm{O_{a}P_{h}}^{(j)}}{\\mathrm{O_{a}P}^{(j)}}\\right).\nThe radial distance is estimated from the RT spectrogram using an amplitude-weighted centroid:\nO\na\nâ€‹\nP\n(\nj\n)\n=\n1\nN\nthre-r\nâ€‹\nâˆ‘\nt\n=\n1\nN\nF\nâˆ‘\ni\n=\n1\nN\nadc\n/\n2\ni\nâ€‹\nr\nres\nâ€‹\nf\nâ€‹\n(\nR\nt\nâ€‹\n(\ni\n)\n)\n,\n\\mathrm{O_{a}P}^{(j)}=\\frac{1}{N_{\\text{thre-r}}}\\sum_{t=1}^{N_{F}}\\sum_{i=1}^{N_{\\text{adc}}/2}i\\,r_{\\text{res}}\\,f(R_{t}(i)),\n(6)\nwhere\nr\nres\nr_{\\text{res}}\nis the range resolution and\nf\nâ€‹\n(\nx\n)\n=\n{\n1\n,\nx\n>\nA\nthre-r\n,\n0\n,\notherwise\n.\nf(x)=\\begin{cases}1,&x>A_{\\text{thre-r}},\\\\\n0,&\\text{otherwise}.\\end{cases}\n(7)\nVertical and horizontal displacements are obtained analogously from the ZT and XT spectrogram centroids.\nThe estimated mean vertical distance\ny\nÂ¯\n(\nj\n)\n=\n1\nN\nâ€‹\nâˆ‘\nn\n=\n1\nN\nr\na\n(\nj\n)\nâ€‹\n(\nn\n)\nâ€‹\ncos\nâ¡\n(\nÏ†\na\n(\nj\n)\nâ€‹\n(\nn\n)\n)\nâ€‹\ncos\nâ¡\n(\nÎ¸\na\n(\nj\n)\nâ€‹\n(\nn\n)\n)\n\\bar{y}^{(j)}=\\frac{1}{N}\\sum_{n=1}^{N}r_{a}^{(j)}(n)\\cos\\!\\big(\\varphi_{a}^{(j)}(n)\\big)\\cos\\!\\big(\\theta_{a}^{(j)}(n)\\big)\n(8)\nis used to determine whether projection normalization is activated. When\ny\nÂ¯\n(\nj\n)\n>\nÏ„\ny\n\\bar{y}^{(j)}>\\tau_{y}\n, the transformation in (\n4\n) is applied using the stabilized reference distance\ny\n0\ny_{0}\n.\nIV-B\n4\nNoise Suppression and Center Normalization\nAfter spatial alignment, density-based noise suppression and center normalization are applied to refine the spectrogram representation\n[\n3\n]\n. The denoising step removes low-density reflections unrelated to the main gesture region, while center normalization shifts the dominant cluster to the geometric center, improving spatial consistency across frames. These refinements yield a compact and well-centered spectrogram representation, facilitating more consistent gesture feature extraction.\nFigure 5:\nSampled examples of spectrograms and quantitative metrics before/after alignment (top) and enhancement (bottom).\nIV-C\nSpectrogram Construction and Enhancement\nFollowing spatial alignment, radar frames captured during humanoid robot interaction are converted into the multi-domain temporalâ€“spectral representations introduced in SectionÂ III-C.\nThese spectrograms are subsequently enhanced using an unpaired spectral translation module to mitigate long-range and oblique-view degradation and to recover more complete spectral patterns for recognition.\nThe overall enhancementâ€“recognition framework is illustrated in Fig.\n4\n, where sparse spectrograms are first enhanced and then fed into a recognition network. Long-range and off-axis humanoid interaction scenarios often yield sparse or fragmented spectrograms due to signal attenuation and weak backscattering. To reconstruct dense and continuous spectral patterns without requiring labels, we adopt an unpaired spectral domain translation framework inspired by CycleGAN\n[\n28\n]\n.\nAs shown in Fig.\n4\n(a), the enhancement module uses an Enhancer and a Reducer to reconstruct dense spectral patterns from sparse observations, supervised by two discriminators that operate on the sparse and dense domains. Let the sparse and dense spectrogram domains be denoted by\nð’®\n1\n\\mathcal{S}_{1}\nand\nð’®\n2\n\\mathcal{S}_{2}\n, with samples\nð’\n1\nâˆ¼\np\nð’®\n1\n\\mathbf{S}_{1}\\sim p_{\\mathcal{S}_{1}}\nand\nð’\n2\nâˆ¼\np\nð’®\n2\n\\mathbf{S}_{2}\\sim p_{\\mathcal{S}_{2}}\n.\nTwo generatorsâ€”the forward translator\nE\n:\nð’®\n1\nâ†’\nð’®\n2\nE:\\mathcal{S}_{1}\\rightarrow\\mathcal{S}_{2}\nand backward translator\nR\n:\nð’®\n2\nâ†’\nð’®\n1\nR:\\mathcal{S}_{2}\\rightarrow\\mathcal{S}_{1}\n-learn bidirectional mappings regulated by adversarial and cycle-consistency losses:\nL\nâ€‹\n(\nE\n,\nR\n,\nA\n,\nB\n)\n\\displaystyle L(E,R,A,B)\n=\nL\nGAN\nâ€‹\n(\nE\n,\nB\n,\nð’®\n1\n,\nð’®\n2\n)\n+\nL\nGAN\nâ€‹\n(\nR\n,\nA\n,\nð’®\n2\n,\nð’®\n1\n)\n\\displaystyle=L_{\\text{GAN}}(E,B,\\mathcal{S}_{1},\\mathcal{S}_{2})+L_{\\text{GAN}}(R,A,\\mathcal{S}_{2},\\mathcal{S}_{1})\n(9)\n+\nÎ»\nâ€‹\nL\ncyc\nâ€‹\n(\nE\n,\nR\n)\n,\n\\displaystyle\\quad+\\lambda\\,L_{\\text{cyc}}(E,R),\nL\ncyc\nâ€‹\n(\nE\n,\nR\n)\n=\n\\displaystyle L_{\\text{cyc}}(E,R)=\nð”¼\nð’\n1\nâ€‹\n[\nâ€–\nR\nâ€‹\n(\nE\nâ€‹\n(\nð’\n1\n)\n)\nâˆ’\nð’\n1\nâ€–\n1\n]\n\\displaystyle\\mathbb{E}_{\\mathbf{S}_{1}}\\!\\left[\\|R(E(\\mathbf{S}_{1}))-\\mathbf{S}_{1}\\|_{1}\\right]\n(10)\n+\nð”¼\nð’\n2\nâ€‹\n[\nâ€–\nE\nâ€‹\n(\nR\nâ€‹\n(\nð’\n2\n)\n)\nâˆ’\nð’\n2\nâ€–\n1\n]\n.\n\\displaystyle\\quad+\\mathbb{E}_{\\mathbf{S}_{2}}\\!\\left[\\|E(R(\\mathbf{S}_{2}))-\\mathbf{S}_{2}\\|_{1}\\right].\nBoth generators adopt a residual encoder-decoder architecture\n[\n8\n]\n, while each discriminator uses a\n34\nÃ—\n34\n34\\times 34\nPatchGAN to enforce local spectral realism. Training stability is improved using a replay buffer, which stores previously generated samples for discriminator updates. The resulting enhanced spectrograms exhibit improved continuity, stronger structural cues, and reduced sparsity, providing more reliable inputs for humanoid robot interaction.\nIV-D\nGeneralization-Oriented Recognition Network\nThe enhanced spectrograms are fed into an attention-based recognition network that extracts discriminative spectral features robust to spatial variations. The network employs a compact CNN backbone with a Dual-Branch Channel Attention (DBCA) mechanism to reweight spectral features that remain stable across user positions in room-scale humanoid interaction.\nIV-D\n1\nDual-Branch Channel Attention Architecture\nStandard CNNs treat feature channels uniformly, which can attenuate informative spectral cues under spatially varying observation conditions. To address this issue, we introduce a Dual-Branch Channel Attention (DBCA) module inspired by squeeze-and-excitation\n[\n9\n]\n, as illustrated in Fig.\n4\n(b).\nDBCA constructs two complementary descriptor branches to capture local texture patterns and global motion signatures. The descriptors are fused and transformed to generate channel-wise attention weights, which adaptively rescale intermediate feature maps to enhance informative responses and suppress irrelevant activations.\nIV-D\n2\nGesture Classification Module\nThe backbone comprises three convolutionâ€“batch-normalizationâ€“ReLUâ€“max-pooling blocks, followed by two fully connected layers and a softmax classifier. Dropout is applied after each block. The network is trained using Adam with an initial learning rate of 0.001, a batch size of 64, and 70 epochs.\nThe complete enhancementâ€“recognition pipeline, summarized in Fig.\n4\n, robustly handles long-range degradation and spatial variation in room-scale interaction scenarios. The pipeline occupies 124.6Â MB and achieves an average inference time of 5.45 ms per sample, demonstrating suitability for real-time humanoidâ€“robot interaction.\nTABLE I:\nCross-position recognition performance under six training configurations.\nAll values are accuracies (%).\n|\nð’«\ntrain\n|\n|\\mathcal{P}_{\\mathrm{train}}|\ndenotes the number of training positions (1â€“6).\nThe corresponding training sets are:\n|\nð’«\ntrain\n|\n=\n1\n|\\mathcal{P}_{\\mathrm{train}}|=1\n:\n{\nP1\n}\n\\{\\mathrm{P1}\\}\n;\n|\nð’«\ntrain\n|\n=\n2\n|\\mathcal{P}_{\\mathrm{train}}|=2\n:\n{\nP1\n,\nP3\n}\n\\{\\mathrm{P1},\\mathrm{P3}\\}\n;\n|\nð’«\ntrain\n|\n=\n3\n|\\mathcal{P}_{\\mathrm{train}}|=3\n:\n{\nP1\n,\nP3\n,\nP5\n}\n\\{\\mathrm{P1},\\mathrm{P3},\\mathrm{P5}\\}\n;\n|\nð’«\ntrain\n|\n=\n4\n|\\mathcal{P}_{\\mathrm{train}}|=4\n:\n{\nP1\n,\nP3\n,\nP5\n,\nP6\n}\n\\{\\mathrm{P1},\\mathrm{P3},\\mathrm{P5},\\mathrm{P6}\\}\n;\n|\nð’«\ntrain\n|\n=\n5\n|\\mathcal{P}_{\\mathrm{train}}|=5\n:\n{\nP1\n,\nP3\n,\nP4\n,\nP5\n,\nP6\n}\n\\{\\mathrm{P1},\\mathrm{P3},\\mathrm{P4},\\mathrm{P5},\\mathrm{P6}\\}\n;\n|\nð’«\ntrain\n|\n=\n6\n|\\mathcal{P}_{\\mathrm{train}}|=6\n:\n{\nP1\n,\nâ€¦\n,\nP6\n}\n\\{\\mathrm{P1},\\dots,\\mathrm{P6}\\}\n.\nâ€œB/Oâ€ denotes Baseline vs. Ours.\n|\nð’«\nt\nâ€‹\nr\nâ€‹\na\nâ€‹\ni\nâ€‹\nn\n|\n|\\mathcal{P}_{train}|\nPer-position Accuracy (B/O)\nSeen-Mean\nUnseen-Mean\nâ†‘\n\\uparrow\nGap\nâ†“\n\\downarrow\nP1\nB\nP1\nO\nP2\nB\nP2\nO\nP3\nB\nP3\nO\nP4\nB\nP4\nO\nP5\nB\nP5\nO\nP6\nB\nP6\nO\nB\nO\nB\nO\nB\nO\n1\n97.66\n98.03\n58.94\n96.02\n50.74\n68.90\n68.54\n78.23\n68.93\n79.80\n55.69\n78.80\n97.66\n98.03\n60.57\n80.35\n37.09\n17.68\n2\n97.66\n98.28\n67.53\n93.83\n98.45\n99.37\n58.52\n91.50\n80.13\n78.01\n65.72\n85.44\n98.05\n98.82\n67.97\n87.20\n30.08\n11.62\n3\n97.40\n97.78\n97.76\n99.05\n99.06\n98.60\n60.47\n97.26\n63.44\n96.09\n78.44\n94.46\n98.40\n98.48\n67.45\n95.94\n30.95\n2.54\n4\n97.40\n98.52\n80.12\n98.08\n96.90\n98.73\n68.72\n96.42\n98.43\n98.88\n98.13\n99.21\n97.71\n98.84\n74.42\n97.25\n23.29\n1.59\n5\n97.92\n98.77\n79.18\n97.67\n96.90\n98.10\n99.12\n99.57\n99.69\n97.77\n96.63\n99.21\n98.05\n98.68\n79.18\n97.67\n18.87\n1.01\n6\n98.18\n98.77\n100.00\n98.97\n98.14\n97.78\n97.80\n99.57\n97.17\n98.60\n98.50\n95.63\n98.30\n98.22\nâ€“\nâ€“\nâ€“\nâ€“\nTABLE II:\nComparison between Ours and Baseline across different numbers of training positions.\n|\nð’«\nt\nâ€‹\nr\nâ€‹\na\nâ€‹\ni\nâ€‹\nn\n|\n|\\mathcal{P}_{train}|\n1\n2\n3\n4\n5\nOurs Unseen (%)\n80.35\n87.20\n95.94\n97.25\n97.67\nBaseline Unseen (%)\n60.57\n67.97\n67.45\n74.42\n79.18\nÎ”\n\\Delta\n% (O-B)\n+19.78\n+19.23\n+28.49\n+22.83\n+18.49\nBaseline Pos Needed\n5\nâ€”\nâ€”\nâ€”\nâ€”\nData Reduction\n5\nÃ—\n\\times\nâ€”\nâ€”\nâ€”\nâ€”\nV\nExperiments\nV-A\nExperimental Setup\nFigure 6:\nIllustration of room-scale humanoidâ€“robot interaction scenarios under diverse user positions.\nFigure 7:\nIllustration of the experimental environment and the six defined spatial positions (\nP1\n\\mathrm{P1}\n-\nP6\n\\mathrm{P6}\n) within the radar field of view.\nV-A\n1\nHardware Platform\nAll experiments are conducted on a robotic interaction platform that integrates a commercial FMCW mmWave radar and a Unitree G1 humanoid robot.\nThe mmWave radar serves as the primary sensing module for room-scale humanâ€“robot interaction, while the G1 robot provides a human-like embodiment for executing interactive behaviors.\nWe employ a commercial FMCW MIMO radar (Calterah RDP60S244-IEM MERCURY) as the sensing front-end.\nThe radar adopts a\n4\nÃ—\n4\n4\\times 4\nTX/RX MIMO configuration, forming 16 virtual channels for high-resolution angular perception, as illustrated in Fig.\n2\n.\nIt operates at a start frequency of 60.5Â GHz with a 3.5Â GHz bandwidth and a 10Â MHz ADC sampling rate.\nEach frame contains 256 chirps, and each chirp includes 256 ADC samples.\nThe FMCW slope is 118.24Â MHz/\nÎ¼\n\\mu\ns, and the radar frame period is set to 50Â ms.\nUnder these settings, the radar achieves a maximum detection range of 6.4Â m, a range resolution of 0.05Â m, a maximum radial velocity of 6.4Â m/s, and a velocity resolution of 0.05Â m/s.\nThe perception outputs from the mmWave radar are transmitted to a Unitree G1 humanoid robot, which acts as the interactive agent throughout all experiments.\nDuring experiments, the robot receives real-time perception results for interpreting human interaction cues and executing responsive behaviors.\nV-A\n2\nData Collection and Participants\nGesture data are collected in an indoor room-scale environment containing six pre-defined spatial positions within the radar field of view, denoted as\nP1\n\\mathrm{P1}\nâ€“\nP6\n\\mathrm{P6}\n(Fig.\n7\n).\nThese positions correspond to different combinations of azimuth offsets, elevation offsets, and user-radar distances, enabling systematic evaluation of cross-position generalization.\nFive participants perform five gesture classes at each position.\nEach gesture is repeated 80 times, producing a dataset of 12,000 samples\n(6 positions\nÃ—\n\\times\n5 participants\nÃ—\n\\times\n5 gestures\nÃ—\n\\times\n80 repetitions).\nV-A\n3\nTraining and Testing Protocol\nUnless otherwise specified,\nP1\n\\mathrm{P1}\n(or\nP1\n\\mathrm{P1}\nâ€“\nP3\n\\mathrm{P3}\nfor multi-position training) is used as the seen training position and the remaining positions are treated as unseen during inference.\nAll compared methods adopt identical training and testing splits to ensure fair comparison.\nEach model is trained for 100 epochs using only the training-position samples and evaluated on all six positions.\nCross-position accuracy, unseen-position mean accuracy, and overall mean accuracy are reported in the following sections.\nV-A\n4\nImplementation Details\nSpectrogram enhancement and recognition networks are implemented in PyTorch and trained on a workstation equipped with an Intel Core i5-9500 CPU, 16Â GB RAM, and an NVIDIA RTXÂ 2070 GPU with 8Â GB memory.\nThe complete inference pipeline, including enhancement, and classification, requires less than 6Â ms per sample on this hardware configuration.\nAll experiments are conducted under the same computational environment for consistent comparison.\nFigure 8:\nSeen-mean and unseen-mean accuracy trends for the baseline and the proposed method across different numbers of training positions.\nFigure 9:\n3D scatter distribution of each sample (a) and accuracy comparison under random-position testing (b).\nTABLE III:\nAblation study under\n|\nð’«\ntrain\n|\n=\n3\n|\\mathcal{P}_{\\mathrm{train}}|=3\n(training at\nP1\n,\nP3\n,\nP5\n\\mathrm{P1},\\mathrm{P3},\\mathrm{P5}\n).\nLeft: accuracy on training and unseen test positions.\nRight: unseen-position accuracy for\nP2\n,\nP4\n,\nP6\n\\mathrm{P2},\\mathrm{P4},\\mathrm{P6}\n.\nMethod\nTraining Pos.\nTest Pos.\nTrain-Mean\nâ†‘\n\\uparrow\nTest-Mean\nâ†‘\n\\uparrow\nGap\nâ†“\n\\downarrow\nP1\nP3\nP5\nP2\nP4\nP6\nBaseline (Raw Only)\n97.40\n97.76\n99.06\n60.47\n63.44\n78.44\n98.40\n67.45\n30.95\nw/ Alignment\n98.28\n96.84\n98.60\n97.12\n95.75\n89.87\n97.90\n94.25\n3.65\nw/ Alignment+Enh.\n97.29\n98.42\n97.77\n95.20\n96.26\n92.56\n97.82\n94.67\n3.15\nw/ Attention Only\n97.66\n97.83\n99.37\n72.12\n68.54\n75.75\n98.29\n72.14\n26.15\nFull Ours (A+E+Attn)\n97.78\n99.05\n98.60\n97.26\n96.09\n94.46\n98.48\n95.94\n2.54\nV-B\nCross-Position Generalization Results\nV-B\n1\nFixed Unseen-Position Results\nTable\nI\nand Table\nII\nsummarize the cross-position recognition accuracy when the model is trained on one to six spatial positions and evaluated on all six. Across all configurations, the proposed method significantly improves unseen-position accuracy compared with the baseline. When trained on a single position (\n|\nð’«\nt\nâ€‹\nr\nâ€‹\na\nâ€‹\ni\nâ€‹\nn\n|\n=\n1\n|\\mathcal{P}_{train}|=1\n), our method achieves 80.35% unseen accuracy, outperforming the baseline by 19.78%. As the number of training positions increases, unseen accuracy continues to improve and reaches 97.67% with five training positions, while the baseline still remains below 80%. This trend demonstrates that our spatial alignment and spectral enhancement modules effectively mitigate viewpoint- and distance-induced distortions and provide robust representations even under limited training diversity.\nA position-by-position analysis reveals that the most challenging locations are typically those with large azimuth or elevation offsets (e.g.,\nP2\n\\mathrm{P2}\n,\nP4\n\\mathrm{P4}\n) or long-range conditions (e.g.,\nP5\n\\mathrm{P5}\n,\nP6\n\\mathrm{P6}\n). These positions cause severe geometric deformation or spectral sparsity for the baseline, with accuracy dropping to 50â€“70%. In contrast, our method maintains consistently high accuracy across all unseen positions. For instance, at configuration\n|\nð’«\ntrain\n|\n=\n3\n|\\mathcal{P}_{\\mathrm{train}}|=3\n, accuracy at\nP4\n\\mathrm{P4}\nimproves from 60.47% to 97.26%, and at\nP5\n\\mathrm{P5}\nfrom 63.44% to 96.09%. These results demonstrate that the proposed framework not only improves overall generalization but also stabilizes performance under challenging positional variations encountered in room-scale environments.\nThe overall trend is further illustrated in Fig.\n8\n, which plots the mean seen and unseen accuracy as a function of the number of training positions. The baseline exhibits a clear dependence on training diversity: unseen accuracy increases slowly and only approaches 80% when five training positions are provided. In contrast, the proposed method achieves strong generalization even under limited supervision, reaching above 80% unseen accuracy with only a single training position and surpassing 95% when three positions are used. The gain at\n|\nð’«\nt\nâ€‹\nr\nâ€‹\na\nâ€‹\ni\nâ€‹\nn\n|\n=\n3\n|\\mathcal{P}_{train}|=3\nreaches 28.49%, highlighting that geometric alignment and spectral enhancement effectively remove position-induced variations that otherwise require extensive multi-position training. This behavior indicates that our method learns position-invariant representations and drastically reduces the amount of data required for robust cross-position recognition.\nV-B\n2\nRandom Free-Position Evaluation\nTo further evaluate room-scale generalization beyond the predefined positions\nP1\n\\mathrm{P1}\n-\nP6\n\\mathrm{P6}\n, we conduct a random-position test in which users freely move within the radar coverage and perform gestures from arbitrary viewpoints. This scenario produces highly diverse combinations of distance, azimuth, and elevation, and therefore represents a more realistic and challenging setting. As shown in Fig.\n9\n(b), the baseline achieves only 33.51% accuracy due to severe geometric deformation and spectral sparsity. In contrast, the proposed method reaches 94.33%, representing a substantial improvement of 60.82 percentage points. The 3D scatter comparison in Fig.\n9\n(a) shows that the aligned point clouds form a compact and stable structure, enabling reliable recognition under unconstrained spatial configurations.\nThese results confirm that the proposed alignment and enhancement pipeline generalizes effectively to free-form, previously unseen positions and supports robust gesture recognition in room-scale scenarios where the user location cannot be predetermined. This ability is essential for humanoid-robot interaction, where users naturally move throughout the environment rather than standing at fixed viewpoints.\nTABLE IV:\nCross-position gesture recognition accuracy (%) for the proposed method and reproduced baseline approaches.\nTraining positions:\nP1\n\\mathrm{P1}\n,\nP3\n\\mathrm{P3}\n,\nP5\n\\mathrm{P5}\n; testing positions:\nP2\n\\mathrm{P2}\n,\nP4\n\\mathrm{P4}\n,\nP6\n\\mathrm{P6}\n.\nMethod\nInput\nTraining Pos.\nTest Pos.\nTrain-Mean\nâ†‘\n\\uparrow\nTest-Mean\nâ†‘\n\\uparrow\nGap\nâ†“\n\\downarrow\nRandom-Mean\nâ†‘\n\\uparrow\nP1\n\\mathrm{P1}\nP3\n\\mathrm{P3}\nP5\n\\mathrm{P5}\nP2\n\\mathrm{P2}\nP4\n\\mathrm{P4}\nP6\n\\mathrm{P6}\nTS-DRSPA\n[\n25\n]\n3-channel spectrum\n96.91\n96.53\n96.97\n83.70\n80.78\n75.69\n96.80\n80.06\n16.74\n63.43\nDI-Gesture\n[\n15\n]\nrange-angle image\n94.33\n96.18\n95.15\n77.53\n75.68\n58.56\n95.22\n70.59\n24.63\n76.12\nShuffleNet-Traj\n[\n12\n]\n3-channel spectrum\n96.65\n96.53\n95.45\n72.33\n69.39\n69.27\n96.22\n70.39\n25.83\n61.34\n5D-DCN\n[\n2\n]\n4-channel spectrum\n94.85\n94.44\n93.33\n86.99\n71.94\n65.60\n94.21\n74.84\n19.37\n73.72\nOurs-Basic\n3-channel spectrum\n97.40\n97.76\n99.06\n60.47\n63.44\n78.44\n98.40\n67.45\n30.95\n33.51\nOurs\n3-channel spectrum\n97.78\n99.05\n98.60\n97.26\n96.09\n94.46\n98.48\n95.94\n2.54\n94.33\nV-C\nVisual Results\nFig.\n4\nprovides both qualitative and quantitative evidence that the proposed alignment and enhancement modules substantially improve the spatial coherence of mmWave spectrograms. These improvements extend beyond classification accuracy, as they directly strengthen the geometric and spectral consistency of radar measurements, which is critical for downstream spatially varying sensing tasks.\nIn the top row, raw spectrograms exhibit pronounced cross-position inconsistencies, where samples of the same gesture are dispersed across positions in the manifold visualization. After alignment, spectral shapes become more consistent and the manifold contracts into tighter clusters. All three quantitative metrics (IGD, CentroidSpread, and CSI) decrease accordingly, confirming that alignment better preserves the underlying spatial structure.\nThe bottom row illustrates the effect of spectral enhancement under long-range sparse conditions. Raw sparse spectrograms suffer from fragmented reflections and incomplete motion trajectories. After enhancement, energy continuity is restored and motion patterns become more discernible. The manifold visualization shows increased compactness, and the quantitative metrics further decrease, indicating reduced spectral inconsistency.\nSpecifically, in the cross-position setting, IGD decreases from 0.45 to 0.39, CentroidSpread from 0.38 to 0.19, and CSI from 0.23 to 0.14. In the long-range sparse case, IGD decreases from 0.47 to 0.37, CentroidSpread from 0.40 to 0.17, and CSI from 0.24 to 0.14, corresponding to reductions of approximately 20%, 58%, and 62%, respectively. These results indicate that alignment primarily improves geometric compactness across viewpoints, while enhancement reinforces spectral continuity.\nTogether, these modules yield position-invariant and spectrally stable representations, suggesting their applicability beyond gesture recognition to broader spatial perception tasks that require viewpoint-robust sensing.\nV-D\nAblation Study\nTo assess the contribution of each component, we conduct an ablation study with\n|\nð’«\ntrain\n|\n=\n3\n|\\mathcal{P}_{\\mathrm{train}}|=3\n(training at\nP1\n\\mathrm{P1}\n,\nP3\n\\mathrm{P3}\n, and\nP5\n\\mathrm{P5}\n). Results are reported in Table\nIII\n, with the right-hand bar plot illustrating unseen-position accuracy at\nP2\n\\mathrm{P2}\n,\nP4\n\\mathrm{P4}\n, and\nP6\n\\mathrm{P6}\n.\nAlignment yields the largest single-component gain, improving unseen accuracy from 67.45% to 94.25% and reducing the generalization gap from 30.95% to 3.65%, confirming its critical role in correcting viewpoint-induced geometric distortion. Incorporating spectral enhancement further strengthens long-range spectral density, increasing the test-mean accuracy to 94.67%. By contrast, attention alone provides marginal improvement, as geometric and spectral inconsistencies persist, leaving the gap (26.15%) close to the baseline.\nThe full model integrating alignment, enhancement, and attention achieves the best performance, with\nFull Ours\nreaching a test-mean accuracy of 95.94% and a minimal gap of 2.54%. The bar plot shows consistent gains across all unseen positions, particularly at\nP2\n\\mathrm{P2}\nand\nP4\n\\mathrm{P4}\nwhere raw spectrograms suffer from severe geometric distortion and energy sparsity. These results demonstrate that the three modules play complementary roles in enabling stable viewpoint-invariant gesture recognition.\nV-E\nComparison with Existing Methods\nTo ensure a fair and representative comparison, we selected several recent gesture recognition methods that explicitly account for spatial variation and reproduced their architectures according to the original papers. All methods were trained and evaluated under the same cross-position protocol, with\nP1\n\\mathrm{P1}\n,\nP3\n\\mathrm{P3}\n, and\nP5\n\\mathrm{P5}\nused for training and\nP2\n\\mathrm{P2}\n,\nP4\n\\mathrm{P4}\n, and\nP6\n\\mathrm{P6}\nfor unseen-position testing. This protocol isolates spatial generalization from dataset bias. Detailed per-position results and aggregated metrics (Train-Mean, Test-Mean, generalization gap, and random-position accuracy) are reported in Table\nIV\n.\nOverall, our method demonstrates the strongest generalization among all approaches. While all baselines achieve comparable performance on training positions (Train-Mean\n>\n>\n94%), their accuracies degrade substantially on unseen positions. For instance, ShuffleNet-Traj and DI-Gesture reach Test-Mean values of only 70.39% and 70.59%, with large generalization gaps of 25.83 and 24.63 percentage points, respectively. Even the 5D-DCN model, despite leveraging four-channel spectrum inputs, still exhibits a notable gap of 19.37.\nBy contrast, our approach maintains consistently high performance across both training and unseen positions, achieving a Test-Mean of 95.94% with an exceptionally small gap of 2.54. Moreover, the random-position mean accuracy further confirms the robustness of our model: whereas existing methods range from 61% to 76%, our framework attains 94.33%, indicating strong stability under arbitrary spatial perturbations. These results verify that the proposed alignment, enhanced spectral representation, and attention modeling effectively improve spatial generalization in room-scale gesture recognition.\nVI\nConclusion\nThis work presented a spatially adaptive gesture recognition framework for room-scale humanâ€“humanoid interaction. By jointly addressing geometric distortion and spectral sparsity, the proposed alignment and enhancement modules produce spatially consistent representations that generalize across user positions. As a result, unseen-position accuracy improves from 67.45% to 95.94%, and random free-position accuracy increases from 33.00% to 94.33%, accompanied by substantial reductions in CentroidSpread and CSI of up to 58% and 62%. These results demonstrate robust cross-position sensing beyond fixed-view configurations and indicate applicability to broader mmWave-based perception tasks. Building on this capability, future work will explore multimodal humanoid robot perception by integrating mmWave sensing with complementary modalities, leveraging public datasets such as the mm-Fi dataset.\nReferences\n[1]\nD. Bruckner, H. Zeilinger, and D. Dietrich\n(2012)\nCognitive automationâ€”survey of novel artificial general intelligence methods for the automation of human technical environments\n.\nIEEE Trans Ind. Informat.\n8\n(\n2\n),\npp.Â 206â€“215\n.\nCited by:\nÂ§I\n.\n[2]\nX. Dong, Z. Zhao, Y. Wang, T. Zeng, J. Wang, and Y. Sui\n(2022)\nFMCW radar-based hand gesture recognition using spatiotemporal deformable and context-aware convolutional 5-d feature representation\n.\nIEEE Trans. Geosci. Remote Sens.\n60\n(\n),\npp.Â 1â€“11\n.\nCited by:\nÂ§I\n,\nÂ§\nII-C\n,\nTABLE IV\n.\n[3]\nM. Ester, H. Kriegel, J. Sander, and X. Xu\n(1996)\nA density-based algorithm for discovering clusters in large spatial databases with noise\n.\nIn\nProc. 2nd Int. Conf. Knowledge Discovery and Data Mining (KDD)\n,\nKDDâ€™96\n,\npp.Â 226â€“231\n.\nCited by:\nÂ§\nIV-B\n4\n.\n[4]\nJ. Fan, H. Rao, J. Zhang, J. Yang, and L. Xie\n(2025)\nMmPred: radar-based human motion prediction in the dark\n.\nNote:\narXiv:2512.00345\nCited by:\nÂ§I\n.\n[5]\nW. Fang, G. Lai, Y. Yu, C. Shi, X. Meng, J. Sun, and Z. Cui\n(2025)\nReal-time humanâ€“drone interaction via active multimodal gesture recognition under limited field of view in indoor environments\n.\nIEEE Rob. Autom. Lett.\n10\n(\n11\n),\npp.Â 11705â€“11712\n.\nCited by:\nÂ§\nII-A\n.\n[6]\nX. Guo, Y. Hu, Y. Wang, Q. Zhu, and Y. Mo\n(2025)\nA lightweight spatio-temporal skeleton attention transformer for long-distance gesture recognition in uav control\n.\nIEEE Trans. Ind. Electron.\n.\nCited by:\nÂ§I\n.\n[7]\nY. Hassab, T. Stadelmayer, and F. Lurz\n(2023)\nPhysically-interpretable data augmentation for multi-range hand gesture recognition using fmcw radar time series\n.\nIEEE Trans. Radar Syst.\n1\n(\n),\npp.Â 571â€“582\n.\nCited by:\nÂ§\nII-C\n.\n[8]\nK. He, X. Zhang, S. Ren, and J. Sun\n(2016-06)\nDeep residual learning for image recognition\n.\nIn\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)\n,\nCited by:\nÂ§\nIV-C\n.\n[9]\nJ. Hu, L. Shen, and G. Sun\n(2018)\nSqueeze-and-excitation networks\n.\nIn\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)\n,\nVol.\n,\npp.Â 7132â€“7141\n.\nCited by:\nÂ§\nIV-D\n1\n.\n[10]\nC. Jiang, Z. Liu, and J. She\n(2024)\nHierarchical co-consistency quantization and information refining binary network for facial expression recognition in humanâ€“robot interaction\n.\nIEEE Trans Ind. Informat.\n20\n(\n10\n),\npp.Â 12178â€“12188\n.\nCited by:\nÂ§I\n.\n[11]\nB. Jin, H. Wu, Z. Zhang, Z. Lian, X. Zhang, and G. Du\n(2025)\nSRDST: effective dynamic gesture recognition with sparse representation and dual-stream transformers in mmwave radar\n.\nIEEE Trans Ind. Informat.\n21\n(\n1\n),\npp.Â 604â€“612\n.\nCited by:\nÂ§I\n.\n[12]\nW. Kim, J. Byung Park, S. Ahmed, and S. Ho Cho\n(2025)\nFMCW radar-based in-air alphanumeric gesture recognition with machine learning\n.\nIEEE Trans. Instrum. Meas.\n74\n(\n),\npp.Â 1â€“12\n.\nCited by:\nÂ§\nII-B\n,\nTABLE IV\n.\n[13]\nO. Kobzarev, A. Lykov, and D. Tsetserukou\n(2025)\nGestLLM: advanced hand gesture interpretation via large language models for human-robot interaction\n.\nIn\nProc. 20th ACM/IEEE Int. Conf. Humanâ€“Robot Interaction (HRI)\n,\nVol.\n,\npp.Â 1413â€“1417\n.\nCited by:\nÂ§I\n.\n[14]\nB. Li, Y. Yang, L. Yang, and C. Fan\n(2023)\nSign language/gesture recognition on ood target domains using uwb radar\n.\nIEEE Trans. Instrum. Meas.\n72\n(\n),\npp.Â 1â€“11\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[15]\nY. Li, D. Zhang, J. Chen, J. Wan, D. Zhang, Y. Hu, Q. Sun, and Y. Chen\n(2022)\nDI-gesture: domain-independent and real-time gesture recognition with millimeter-wave signals\n.\nIn\nProc. IEEE Global Commun. Conf. (GLOBECOM)\n,\nVol.\n,\npp.Â 5007â€“5012\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nTABLE IV\n.\n[16]\nJ. Lin, Y. Tseng, W. Chen, N. Wu, T. Zhang, and P. Tseng\n(2025)\nMmWave mimo radar-based handwriting recognition with lightweight fingertip tracking\n.\nIEEE Sensors J\n25\n(\n2\n),\npp.Â 3794â€“3805\n.\nCited by:\nÂ§I\n,\nÂ§\nII-C\n.\n[17]\nH. Liu and Z. Liu\n(2023)\nA multimodal dynamic hand gesture recognition based on radarâ€“vision fusion\n.\nIEEE Trans. Instrum. Meas.\n72\n(\n),\npp.Â 1â€“15\n.\nCited by:\nÂ§I\n.\n[18]\nJ. Mohinder\nFMCW radar design\n.\nArtech\n.\nCited by:\nÂ§\nIII-A\n.\n[19]\nP. PÅ‚awiak, T. SoÅ›nicki, M. NiedÅºwiecki, Z. Tabor, and K. Rzecki\n(2016)\nHand body language gesture recognition based on signals from specialized glove and machine learning algorithms\n.\nIEEE Trans Ind. Informat.\n12\n(\n3\n),\npp.Â 1104â€“1113\n.\nCited by:\nÂ§I\n.\n[20]\nN. Roshandel, C. Scholz, H. Cao, H. Cao, M. Amighi, H. Firouzipouyaei, A. Burkiewicz, S. Menet, F. Ballen-Moreno, D. W. Sisavath, E. Imrith, A. Paolillo, J. Genoe, and B. Vanderborght\n(2025)\nMmPrivPose3D: a radar-based approach to privacy-compliant pose estimation and gesture command recognition in humanâ€“robot collaboration\n.\nIEEE Sensors J\n25\n(\n15\n),\npp.Â 29437â€“29445\n.\nCited by:\nÂ§\nII-A\n.\n[21]\nD. Salami, R. Hasibi, S. Palipana, P. Popovski, T. Michoel, and S. Sigg\n(2023)\nTesla-rapture: a lightweight gesture recognition system from mmwave radar sparse point clouds\n.\nIEEE Trans. Mobile Comput.\n22\n(\n8\n),\npp.Â 4946â€“4960\n.\nCited by:\nÂ§I\n.\n[22]\nJ. Tian, Y. Zou, J. Lai, and F. Liu\n(2025)\nMmDigit: a real-time digit recognition framework in air-writing using fmcw radar\n.\nIEEE Internet Things J.\n12\n(\n7\n),\npp.Â 9238â€“9251\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[23]\nP. Towakel, D. Windridge, and H. X. Nguyen\n(2023)\nDeep combination of radar with optical data for gesture recognition: role of attention in fusion architectures\n.\nIEEE Trans. Instrum. Meas.\n72\n(\n),\npp.Â 1â€“15\n.\nCited by:\nÂ§\nII-C\n.\n[24]\nZ. Xia, Y. Luomei, C. Zhou, and F. Xu\n(2021)\nMultidimensional feature representation and learning for robust hand-gesture recognition on commercial millimeter-wave radar\n.\nIEEE Trans. Geosci. Remote Sens.\n59\n(\n6\n),\npp.Â 4749â€“4764\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[25]\nZ. Xia and F. Xu\n(2022)\nTime-space dimension reduction of millimeter-wave radar point-clouds for smart-home hand-gesture recognition\n.\nIEEE Sensors J\n22\n(\n5\n),\npp.Â 4425â€“4437\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nTABLE IV\n.\n[26]\nZ. Zhang, Z. Tian, Y. Zhang, M. Zhou, and B. Wang\n(2019)\nU-deephand: fmcw radar-based unsupervised hand gesture feature learning using deep convolutional auto-encoder network\n.\nIEEE Sensors J\n19\n(\n16\n),\npp.Â 6811â€“6821\n.\nCited by:\nÂ§\nII-B\n.\n[27]\nH. Zhao, Y. Ma, Y. Lu, and K. Liu\n(2023)\nDGSCR: double-target gesture separation and classification recognition based on deep learning and millimeter-wave radar\n.\nIEEE Sensors J\n23\n(\n21\n),\npp.Â 26701â€“26711\n.\nCited by:\nÂ§I\n.\n[28]\nJ. Zhu, T. Park, P. Isola, and A. A. Efros\n(2020)\nUnpaired image-to-image translation using cycle-consistent adversarial networks\n.\nNote:\narXiv:1703.10593\nExternal Links:\n1703.10593\nCited by:\nÂ§\nIV-C\n.\n[29]\nY. Zou, J. Weng, W. Kuang, Y. Jiao, V. C. Leung, and K. Wu\n(2024)\nImg2Acoustic: a cross-modal gesture recognition method based on few-shot learning\n.\nIEEE Trans. Mobile Comput.\n.\nCited by:\nÂ§I\n.",
    "preview_text": "Reliable humanoid-robot interaction (HRI) in household environments is constrained by two fundamental requirements, namely robustness to unconstrained user positions and preservation of user privacy. Millimeter-wave (mmWave) sensing inherently supports privacy-preserving interaction, making it a promising modality for room-scale HRI. However, existing mmWave-based interaction-sensing systems exhibit poor spatial generalization at unseen distances or viewpoints. To address this challenge, we introduce WaveMan, a spatially adaptive room-scale perception system that restores reliable human interaction sensing across arbitrary user positions. WaveMan integrates viewpoint alignment and spectrogram enhancement for spatial consistency, with dual-channel attention for robust feature extraction. Experiments across five participants show that, under fixed-position evaluation, WaveMan achieves the same cross-position accuracy as the baseline with five times fewer training positions. In random free-position testing, accuracy increases from 33.00% to 94.33%, enabled by the proposed method. These results demonstrate the feasibility of reliable, privacy-preserving interaction for household humanoid robots across unconstrained user positions.\n\nWaveMan: mmWave-Based Room-Scale Human Interaction Perception for Humanoid Robots\nYuxuan Hu, Kuangji Zuo, Boyu Ma, Shihao Li, Zhaoyang Xia,\nFeng Xu, , and Jianfei Yang\nThis work is supported by MOE Singapore Tier 1 Grant RG83/25, RS36/24 and a Start-up Grant from Nanyang Technological University. (Corresponding authors: Feng Xu, Jianfei Yang).Yuxuan Hu, Zhaoyang Xia, and Feng Xu are with the Key Laboratory for Information Science of Electromagnetic Waves, Ministry of Education, School of Information Science and Technology, Fudan University, Shanghai 200433, China (e-mail: huyx23@m.fudan.edu.cn; xiazy@fudan.edu.cn; fengxu@fudan.edu.cn).Yuxuan Hu is also with the School of Mechanical and Aerospace Engineering, Nanyang Technological University, ",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "mmWave",
        "humanoid robots",
        "human-robot interaction",
        "privacy-preserving",
        "spatial generalization",
        "viewpoint alignment",
        "spectrogram enhancement",
        "attention mechanism"
    ],
    "one_line_summary": "WaveMan æ˜¯ä¸€ç§åŸºäºŽæ¯«ç±³æ³¢æ„ŸçŸ¥çš„ã€ç”¨äºŽäººå½¢æœºå™¨äººåœ¨å®¶åº­çŽ¯å¢ƒä¸­è¿›è¡Œéšç§ä¿æŠ¤ã€æˆ¿é—´å°ºåº¦äººæœºäº¤äº’çš„ç©ºé—´è‡ªé€‚åº”æ„ŸçŸ¥ç³»ç»Ÿã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T11:53:32Z",
    "created_at": "2026-01-21T12:09:10.003521",
    "updated_at": "2026-01-21T12:09:10.003528"
}