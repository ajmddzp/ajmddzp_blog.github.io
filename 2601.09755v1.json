{
  "id": "2601.09755v1",
  "title": "Heterogeneous computing platform for real-time robotics",
  "authors": [
    "Jakub Fil",
    "Yulia Sandamirskaya",
    "Hector Gonzalez",
    "Loïc Azzalin",
    "Stefan Glüge",
    "Lukas Friedenstab",
    "Friedrich Wolf",
    "Tim Rosmeisl",
    "Matthias Lohrmann",
    "Mahmoud Akl",
    "Khaleel Khan",
    "Leonie Wolf",
    "Kristin Richter",
    "Holm Puder",
    "Mazhar Ali Bari",
    "Xuan Choo",
    "Noha Alharthi",
    "Michael Hopkins",
    "Mansoor Hanif Christian Mayr",
    "Jens Struckmeier",
    "Steve Furber"
  ],
  "abstract": "After Industry 4.0 has embraced tight integration between machinery (OT), software (IT), and the Internet, creating a web of sensors, data, and algorithms in service of efficient and reliable production, a new concept of Society 5.0 is emerging, in which infrastructure of a city will be instrumented to increase reliability, efficiency, and safety. Robotics will play a pivotal role in enabling this vision that is pioneered by the NEOM initiative - a smart city, co-inhabited by humans and robots. In this paper we explore the computing platform that will be required to enable this vision. We show how we can combine neuromorphic computing hardware, exemplified by the Loihi2 processor used in conjunction with event-based cameras, for sensing and real-time perception and interaction with a local AI compute cluster (GPUs) for high-level language processing, cognition, and task planning. We demonstrate the use of this hybrid computing architecture in an interactive task, in which a humanoid robot plays a musical instrument with a human. Central to our design is the efficient and seamless integration of disparate components, ensuring that the synergy between software and hardware maximizes overall performance and responsiveness. Our proposed system architecture underscores the potential of heterogeneous computing architectures in advancing robotic autonomy and interactive intelligence, pointing toward a future where such integrated systems become the norm in complex, real-time applications.",
  "url": "https://arxiv.org/abs/2601.09755v1",
  "html_url": "https://arxiv.org/html/2601.09755v1",
  "html_content": "Heterogeneous computing platform for real-time robotics.\nJakub Fil\n1\n1\n1\n1\nCorresponding author–jakub.fil@waiys.com,\n1\nWAIYS GmbH,\n2\nZurich University of Applied Sciences,\n3\nSpiNNcloud Systems GmbH,\n4\nTU Dresden,\n5\nApplied Brain Research,\n6\nNEOM,\n7\nThe University of Manchester.\nYulia Sandamirskaya\n2\nHector Gonzalez\n3\nLoïc Azzalin\n2\nStefan Glüge\n2\nLukas Friedenstab\n1\nFriedrich Wolf\n1\nTim Rosmeisl\n3\nMatthias Lohrmann\n3\nMahmoud Akl\n3\nKhaleel Khan\n4\nLeonie Wolf\n1\nKristin Richter\n1\nHolm Puder\n1\nMazhar Ali Bari\n1\nXuan Choo\n5\nNoha Alharthi\n6\nMichael Hopkins\n1\nMansoor Hanif\nChristian Mayr\n4\nJens Struckmeier\n1\nSteve Furber\n7\nAbstract\nAfter Industry 4.0 has embraced tight integration between machinery (OT), software (IT), and the Internet, creating a web of sensors, data, and algorithms in service of efficient and reliable production, a new concept of Society 5.0 is emerging, in which infrastructure of a city will be instrumented to increase reliability, efficiency, and safety. Robotics will play a pivotal role in enabling this vision that is pioneered by the NEOM initiative - a smart city, co-inhabited by humans and robots. In this paper we explore the computing platform that will be required to enable this vision. We show how we can combine neuromorphic computing hardware, exemplified by the Loihi2 processor used in conjunction with event-based cameras, for sensing and real-time perception and interaction with a local AI compute cluster (GPUs) for high-level language processing, cognition, and task planning. We demonstrate the use of this hybrid computing architecture in an interactive task, in which a humanoid robot plays a musical instrument with a human. Central to our design is the efficient and seamless integration of disparate components, ensuring that the synergy between software and hardware maximizes overall performance and responsiveness. Our proposed system architecture underscores the potential of heterogeneous computing architectures in advancing robotic autonomy and interactive intelligence, pointing toward a future where such integrated systems become the norm in complex, real-time applications.\n1\nIntroduction\nIn the near future cognitive cities will become a reality. They will be meticulously designed for sustainability, powered by renewable energy, and characterized by a digital landscape interwoven with robots coexisting alongside humans\n[Golubchikov and Thornbush,\n2020\n]\n. This vision of seamless human-robot interaction will require a one-of-a-kind solution to analyse vast amounts of data powered by energy-efficient and low-latency computing power.\nThis project aims to carry out an investigation into heterogeneous combination of brain-inspired hardware with high-density GPU (Graphics processing units) solutions for real-time social robotics. In this paper we propose joining the two brain-inspired approaches: SpiNNaker2\n[Gonzalez\net al.\n,\n2024\n]\nfor large-scale brain modeling and Intel’s research chip Loihi2\n[Orchard\net al.\n,\n2021\n]\nfor real-time perception and actuation at the edge.\nMoreover, we combined a Dynamic Vision Sensor (DVS) camera\n[Lichtsteiner\net al.\n,\n2008\n]\nwith Intel’s Loihi2 chip and trained an algorithm to track human hands—marking the first such real-time robotic vision workload deployed on this system.\nWe propose to further extend the capabilities of these neuromorphic approaches, by including NVIDIA DGX GPU systems that are used to facilitate showcase orchestration, running large language models and other deep learning algorithms, as well as to execution of a specialized version of the model Spaun\n[Eliasmith,\n2013\n, Choo,\n2018\n]\n, which uses brain-inspired computation mechanisms to implement a cognitive architecture.\nThe combination of those elements, humanoid robot, and a functional brain model Spaun, enables performing real-time processing of sensor streams to achieve complex memory tasks.\nThis multifaceted approach facilitates real-time speech and high-level action prediction, which are critical for the control of a humanoid robot tasked with playing a musical instrument, in an effort to create an interactive demonstration towards an intelligent General Purpose Robot (GPR).\nThe project involves the implementation of an interactive demonstrator focusing on natural human-robot communication as the first functional capability of an AI enabled humanoid GPR.\nTo achieve the outlined goals, we have also designed a highly specialised,\nsustainable supercomputing Micro Data Center with an innovative direct hot liquid cooled cooling system, enabling brain-like capabilities for real-time autonomous systems within a customized environment.\nImplementing a real-time AI cognitive platform is crucial for managing vehicles, drones, robots, and other cognitive solutions within future cognitive cities.\nIn addition to processing vast amounts of data, these platforms must support seamless communication among diverse systems and devices.\nThe operation of such a cognitive infrastructure demands advanced algorithms capable of adapting to dynamic urban environments, enabling predictive maintenance, swift anomaly detection, and efficient resource allocation\n[Shenavarmasouleh\net al.\n,\n2022\n]\n.\nIt has long been argued that such solution requires a hybrid architecture that combines large-scale deep learning with symbolic reasoning and structured knowledge, supporting the claim that diverse and heterogeneous systems are needed for effective AI\n[Garcez and Lamb,\n2023\n, Marcus,\n2020\n]\n. In this paper, we present a collaborative approach that integrates Neuromorphic, Deep Learning, and Symbolic AI, creating a real-time, low-latency, and energy-efficient cognitive AI platform.\nIn an effort to exemplify how these elements can be put into practice, we present an innovative use case demonstrating real-time interaction between the robot and a human performing music sequences on a theremin.\nWe believe that the future of cognitive cities and robotics relies on the integration of heterogeneous computing systems to meet diverse processing needs. High-density GPUs handle compute-intensive tasks over longer timescales, while energy-efficient neuromorphic hardware at the edge excels in real-time, low-latency decision-making. This combination enables optimized performance while reducing energy consumption through the use of brain-inspired algorithms.\nMoreover, we believe that it is necessary to also consider other infrastructure elements such as cooling and heat reuse, to enable sustainable operation of a smart city.\nOur innovative direct hot liquid cooling technology complements these systems, enhancing thermal management and further reducing power usage by allowing for efficient heat recycling\n[Struckmeier,\n2018\n]\n.\nTogether, these technologies form a sustainable, energy-efficient platform for the advanced capabilities required in tomorrow’s connected cities and intelligent robotics.\n2\nResults\n2.1\nShowcase overview\nFigure 1:\nOverview of the key components of the interactive musical showcase.\nRobots are increasingly integrated into our daily environments, and their ability to communicate effectively becomes a crucial consideration. Music, undeniably one of the most potent means of communication, is frequently hailed as a universal language, fostering connections among humans\n[Mehr\net al.\n,\n2019\n]\n. Playing music together requires many skills similar to the ones used in real-world, embodied communication: combination of long-term memory and planning with in-the-moment coordination, turn taking, and multimodal signal exchange. In this context we use playing a musical instrument as an exemplary task. We teach the humanoid robot Ameca to play music pieces using the theremin, an electronic instrument that can be played without physical contact. Through our showcase, we explore capabilities of the Spaun 2.0 brain model in a DGX-based cluster setup involving multiple GPUs. Moreover, we demonstrate real-time neuromorphic computing capabilities on Intel’s research chip Loihi2. Importantly, this work constitutes the first demonstration of integration of a real-time workload on neuromorphic platform with cognitive model, running on a computing cluster, thereby laying the groundwork for advanced real-time vision systems that enable robots to continuously perceive and interact with their environment, while also using a large world model or a knowledge base.\nThe core of this use case centers around Ameca, a humanoid robot created by Engineered Arts.\nThis robot represents a state-of-the-art platform for research in human–robot interaction. Its design incorporates a sophisticated facial mechanism, featuring 32 motors dedicated to enabling nuanced and expressive facial mimicry\n[Singh\net al.\n,\n2022\n]\n. This capability is integral to the robot’s ability to convey a wide range of emotions.\nThis consideration is crucial to make interactions with humans feel more natural by providing nonverbal cues similar to those in human communication\n[Paulus\net al.\n,\n2013\n]\n.\nFor example,\nBreazeal [\n2003\n]\nnotes, that equipping robots with dynamic and expressive facial features is essential not only for conveying clear affective signals but also for mitigating the uncanny valley effect\n[Mori\net al.\n,\n2012\n]\n, thereby fostering more natural human–robot interactions.\nMoreover, Ameca is equipped with flexible and dexterous arms that support complex manipulation tasks.\nIn this study, the robot was set up to replicate musical notes produced on a theremin by a human reference. Conventional note pre-processing techniques, combined with advanced Deep Neural Network (DNN) models, were employed to ensure precise sound identification. Additionally, the integration of the Spaun 2.0 system facilitated cognitive memory tasks influencing the robot’s decision-making processes and the state transitions of the system. Spaun 2.0 is the world’s largest functional brain model spanning up to 6.5 million neurons, which we deployed using an NVIDIA DGX A100 system. A neuromorphic chip Loihi2 was used for fast on-board processing of visual information to enabling moment-to-moment alignment of the robot with the simultaneously playing human.\nThe general objective of this project is to propose a brain-inspired hierarchy of processing combining the cognitive and memory capabilities of large-scale neural networks models with the energy-efficiency and real-time capabilities of neuromorphic hardware, such as Loihi2, at the edge. Such an unprecedented combination of brain-like systems in the cloud and at the edge has the potential to enable many of the sophisticated use cases required in a cognitive city, while also contributing to the overall goal of reducing the energy consumption of robotics use-cases at scale.\n2.2\nMusical automata and music-playing robots\nThe AI community and society at large have been interested in music playing robots for a long time.\nSome very early examples include Ismail al-Jazari - a XII century Muslim pioneer of engineering and cybernetics\n[Dirik,\n2020\n]\n.\nHe constructed a musical automaton, in the form of a boat with four automatic musicians, which performed pre-programmed musical sequences.\nMore recently, there have been attempts to create robots capable of playing instruments, such as piano\n[Roads,\n1986\n]\n, violin\n[Park\net al.\n,\n2015\n]\n, or percussion\n[Weinberg and Driscoll,\n2006\n]\n. In contrast to these automata, real music playing not only includes faithful reproduction of notes and melodies, but also dynamics that aligns with other players or with individual swings of mood of the player or sound production processes of the instrument in solo performances. It is a dynamic, interactive process.\nFigure 2:\nThe musical instrument used in our study - the Theremin.\nThe theremin, deemed to be one of the most difficult instruments to play, is an electronic musical instrument that requires no physical contact to play music (see Fig.\n2\n).\nThe theremin produces sound by detecting an interference created by the player’s hands placed between two antennas.\nThe antenna on the right influences the pitch of the sound, while the one on the left controls the volume. Playing a theremin requires constant adjustment of the hands’ positions depending on the produced sound.\nThere has previously been some research effort devoted to adapting robots to perform music using the theremin, for example iCub\n[Mizumoto\net al.\n,\n2009\n]\nand HRP-2 Promet\n[Wu\net al.\n,\n2011\n]\n.\nThese researchers developed methods for feedforward and feedback arm control based on the theremin’s pitch, which enabled the robots to interact with the theremin in a solo act. The lack of moving fingers allowed only very simple melodies to be played and the robots didn’t follow a professionally accepted human-like technique, such as the one coined by Carolina Eyck.\nIn contrast to the previously described approaches, where robots focused only on pitch fluctuations and ignored the extended dynamic range controlled by the volume antenna, we trained the Ameca robot to control the theremin using its own hands and make decisions using the brain simulator, which we will describe in more detail in the next parts of this paper (see sec.\n4.2.1\n).\nThese previous approaches also ignored human-like movements such as vibrato, in which subtle frequency changes induce harmonically coupled sounds on top of the base notes. The previous approaches didn’t include automatic and advanced calibration, done by the robot itself, which is a must in autonomous systems, especially with an instrument that is inclined to be affected by external actors such as humidity and room geometry.\nLast but not least, none of these robots were equipped with realistic facial expressions to accompany the playing process and increase audience engagement.\nIn fact, none of these approaches was designed to be standalone nor to interact with the public.\nMoreover, the approach described in this paper involves cooperative performance between the robot and the user, unlike previous approaches.\nOur system translates the note and volume into movements for both hands.\nThe backend software provides the robot with the notes to play, as well as their duration, to control the right hand.\nInformation about the volume is used to control the left hand.\nA sequence of notes and a sequence of volume values are provided to the robot in appropriate states, thus allowing the robot to perform long and complex pieces of music.\n2\n2\n2\nTraining process: https://youtube.com/shorts/bj6NaeokJbA\n3\n3\n3\nFinal video demonstration: https://youtu.be/OYW9qQl93aU\nThe robot was trained on a dataset that includes musical pieces played by the master theremin player Carolina Eyck. For each musical piece, the dataset contains video recorded with a ZED2 Stereo camera, data from a Dynamic Vision Sensor (DVS) from iniVation, and the audio. The purpose of this dataset was to generate data that could be used as a reference during the process of translating the melodies into robot movements.\nImages captured via the ZED2 camera were used to perform body tracking.\nFor this purpose, we used the dynamic pose estimation algorithm from Body Tracking API provided by Stereolabs, creators of the ZED2 camera.\nThis system allows for the detection of 18 keypoints on the human body, following COCO18 standard. It allows for accurate prediction of position and movements of human’s limbs. Additionally, we used the Object Detection API from Stereolabs to detect the object of interest, and thus further enhance the quality of the recordings.\nSubsequently, we used the resulting body posture recordings to train the robot to perform the same movements, and thus replicate the theremin player. For this purpose, we used a proprietary operating system for Ameca - Tritium, which allows one to program sequences of movements for the robot based on reference recordings.\nIn addition to the musical performance, the robot can converse with a person on the stage and with the audience, thanks to speech and language models deployed on the GPU cluster.\nAmeca can engage in conversation about playing the theremin, but also about any other topic within the limitations of its language model.\nBy interacting with the robot, the user can ask for a solo performance, as well as activating teaching or duet mode.\nIn these modes, the user is presented with the GUI displayed on a tablet in front of the secondary theremin.\nThe interface shows eight notes ranging from C4 to C5, and the user can engage in an interactive game to accompany the song performed by Ameca.\n3\nDiscussion\n3.1\nHeterogeneous computing for real-time robotics\nThe evolution of social robotics necessitates a computing paradigm that can reconcile the conflicting demands of high-speed signal processing and control and slower long-term cognitive reasoning. In this context, heterogeneous computing platforms—integrating neuromorphic processors with high-density GPU architectures—can be a promising solution. Such systems are essential for enabling seamless human–robot interaction by addressing the diverse computational requirements that span from immediate sensory feedback to complex memory tasks.\nA key insight from our work is the importance of aligning software algorithms with hardware architectures. The integration of brain-inspired hardware, such Loihi2, with conventional high-performance GPU clusters demonstrates that a well-matched software–hardware interface is not merely beneficial but essential for efficiency. This alignment minimizes latency in the feedback loop, which is a critical factor when a robot must process sensor inputs, execute decision-making algorithms, and react in real time. As the robot operates over multiple timescales, from rapid perception-action cycles to longer-term planning and learning, the interaction between processing units must ensure that no single component becomes a bottleneck, as demonstrated for example by\nZhao and Sentis [\n2019\n]\n.\nThis heterogeneous approach holds significant relevance for both the industry, as well as for the broader neuromorphic and robotics communities. These groups are increasingly attracted to solutions that promise not only technical sophistication but also scalability and energy efficiency. A robust, heterogeneous computing platform addresses market demands by reducing power consumption and operational costs, while enhancing performance in dynamic, real-world environments. Moreover, community-driven efforts in standardizing hardware interfaces and software protocols are laying the groundwork for a future in which disparate systems can communicate effectively, further reducing development risks and fostering innovation.\nIntegral to this vision is the development of universal communication protocols that can harmonize the interactions between diverse devices and computing nodes. The Transport-Independent Protocol for Universal Communications - Address Event Representation (AER), as proposed by\nRast\net al.\n[\n2015\n]\n, may serve as inspiration by leveraging event-based data encoding to envisage a low-overhead, energy-efficient standard for future systems, a feature that is particularly significant for urban robotics where fleet management demands real-time coordinated decision making alongside strict energy efficiency. Despite several attempts to connect event-based sensors and computational units, for example also by\nPedersen\net al.\n[\n2024\n]\n, no protocol has become a standard yet, even as efforts are made to transition them from academia into engineering. With SNNs and neuromorphic hardware increasingly pervading the commercial space, there is a strong argument for an easily usable standard that bridges event-based devices and more conventional networks and computational resources. AER is likely to be the underlying high-level protocol, given its flexibility and past success, and the speed and reliability of its software and hardware implementation will be central to any heterogeneous system. A robust protocol must fulfill at least two requirements: 1) a raw, asynchronous transport of spikes without timestamps and minimal overhead for very local, high-bandwidth sensor-to-compute connections; 2) a safer, more conservative transport with reference headers, timestamps, and potentially payloads and error-correction for less local connections subject to corruption, delays, and other uncertainties.\nLooking ahead, establishing a common representation and universal communication protocol will be vital for the evolution of social robotics, ensuring that as robots become integral components of cognitive cities, diverse systems can “speak the same language” to facilitate seamless collaboration between high-density cloud-based models and edge-based neuromorphic processors, thereby enabling improved system integration and a more resilient, adaptive robotic infrastructure capable of evolving alongside the increasingly complex demands of urban environments.\nTherefore, the anticipated standardization of communication protocols represents a critical step towards realizing a future where integrated cognitive systems form the backbone of smart, sustainable cities.\n3.2\nHeterogeneous computing as the future of smart cities\nThe urban landscapes of tomorrow will be shaped by integrated, heterogeneous computing systems that bridge the gap between high-performance data analysis and real-time, energy-efficient decision-making. As smart cities evolve, they will increasingly rely on hybrid architectures that combine the computational heft of high-density GPU clusters with the rapid, low-latency responsiveness of neuromorphic processors. This dual strategy enables the simultaneous processing of large models and historical datasets for strategic planning and instantaneous sensor data critical for dynamic urban management.\nMany scholars have explored the foundational elements of smart cities. For instance,\n[Caragliu\net al.\n,\n2011\n]\nand\n[Albino\net al.\n,\n2015\n]\nprovide comprehensive frameworks that emphasize scalability, resilience, and the transformative potential of integrating advanced technologies into urban infrastructures. Their insights highlight the necessity for systems that not only manage everyday urban functions but also anticipate and adapt to future challenges.\nBuilding on these foundational insights, it is evident that the multifaceted challenges of future urban environments will demand heterogeneous computing architectures capable of spanning a wide range of operational timescales and decision-making processes. For instance, the instantaneous processing of edge sensory data, crucial for real-time traffic management, environmental monitoring, and emergency response, must be integrated with long-term analytics that underpin urban planning. Furthermore, effective autonomous fleet management, including service robots, delivery drones, and self-driving vehicles, relies on systems that can respond quickly to dynamic changes while continuously learning to optimize performance. Nevertheless, conventional deep learning models often struggle with continuous learning and often suffer catastrophic forgetting, as highlighted by\nLiu [\n2017\n]\n. One promising alternative is to employ brain-inspired approaches, such as BitBrain\n[Hopkins\net al.\n,\n2023\n]\n, which have a natural ability to learn quickly from small amounts of data and then retain, adapt, or forget this knowledge over time as required.\nIn parallel, fostering effective human-robot collaboration in public safety, healthcare, and municipal services calls for computational frameworks that merge context-aware responses with predictive decision-making capabilities. This diversity of use cases underscores the importance of adopting heterogeneous computing solutions that are agile and capable of adapting to the evolving complexities of smart cities\n[Zanella\net al.\n,\n2014\n, Batty,\n2012\n]\n.\nFurthermore, innovations in thermal management—such as direct hot liquid cooling—are not only optimizing the thermal performance of centralized and distributed computing resources but also paving the way for innovative urban energy recycling. For instance, the excess heat generated by state-of-the-art data centers can be redirected into district heating networks to warm residential buildings, public facilities, or even urban greenhouses. In some pioneering projects, waste heat has been used to preheat domestic water or power efficient cooling for robotics fleets and autonomous vehicles, thereby reducing both energy consumption and operational costs. This synergy between high-performance computing and sustainable energy reuse is a cornerstone in designing smart cities of the future (see sec.\n4.1.4\n).\nIn summary, we believe that the convergence of heterogeneous computing architectures, standardized communication protocols, and advanced energy management solutions offers a robust blueprint for the smart cities of the future. This integrated approach will not only empower urban planners to optimize resource allocation and infrastructure resilience but will also enable intelligent robotics to operate more efficiently, ensuring that tomorrow’s cities are both smart and sustainable.\n3.3\nBrain-inspired approaches in combination with machine learning\nFor the purpose of enhancing decision-making sub-system of the robot we employed a brain-inspired approach - Spaun 2.0 (Semantic Pointer Architecture: Unified Network). It is a large-scale cognitive neural model consisting of 6.5 million neurons\n[Choo,\n2018\n]\n.\nThis model was used for cognitive memory tasks and the robot’s control process, therefore allowing Ameca to interact with the users using a set of predefined scenarios, i.e. for playing solo, teaching how to play the instrument, conversing etc.\nFor the purpose of this project we deployed the Spaun model on a GPU system using Nengo, which is an open-source neural simulation platform that enables researchers to build, simulate, and deploy large-scale cognitive models on both conventional computers and neuromorphic hardware\n[Bekolay\net al.\n,\n2014\n]\n.\nIn the future, we envision that a server with 5 SpiNNaker2 modules could replace the\nexecution of the Spaun model and further reduce the energy footprint.\nSee Section\n4.2.1\nfor more details about our usage and GPU-based implementation of this model.\nAlthough Nengo and Spaun are made up of a complex and ingenious interaction between different modules and computational paradigms inspired by brain connectivity and population coding, much of the representational power comes from a choice of one of the Vector Symbolic Architecture (VSA) types which underlies the operation of Spaun. The breadth of possibilities allowed by VSAs is indeed significantly greater than this one choice and it is perhaps not widely appreciated how flexible these ideas are\n[Kleyko\net al.\n,\n2022\n,\n2023\n, Schlegel\net al.\n,\n2021\n]\nor how well they can be applied to actual machine learning and AI problems relevant to social robotics such as visual scene recognition by\nRenner\net al.\n[\n2024\n]\nand classification, as described by\nGe and Parhi [\n2020\n]\n, amongst many other cognitive architectures such as: ACT-R\n[Ritter\net al.\n,\n2019\n]\n, SOAR\n[Laird,\n2019\n]\n, Adaptive Resonance Theory\n[Grossberg,\n2013\n]\n, or DAC\n[Moulin-Frier\net al.\n,\n2017\n]\n.\nAs well as possessing desirable levels of robustness and viable solutions to the ’binding problem’–which are two of the key issues for AI systems if they are going to compete with biological intelligence–they are also arguably a very natural fit to event-based sensors and computational resources and so should allow the full latency and energy benefits of neuromorphic computation to be realised. Although it has not yet been proven by demonstration, a direct, sparse, binary, event-driven implementation of a well-chosen set of VSA mechanisms should be significantly faster and more energy efficient than the Spaun implementation using Nengo with its large populations of rate-coded neurons and associated encoding/decoding computations.\nThis is because very simple and massively parallel local homeostatic and structural plasticity mechanisms inspired directly by Dendritic Computation principles\n[Ahmad and Hawkins,\n2016\n, Branco and Häusser,\n2010\n, Kastellakis\net al.\n,\n2015\n, Larkum and Nevian,\n2008\n, London and Häusser,\n2005\n]\nhave been shown to autonomously generate sparse, binary patterns which are ideally suited to appropriate VSA mechanisms with little or no intervening computation or data movement\n[Hopkins\net al.\n,\n2023\n]\n. The savings are even more obvious in the context of a more conventional ANN where hundreds or thousands of epochs of backpropagation and the necessary per-epoch floating point calculations of derivatives are extremely time- and energy-intensive\n3.4\nImportance of neuromorphic computing\nMost computing systems today are based on the classical von Neumann computer architecture in which a processor, or a central computing unit (CPU), performs elementary computations sequentially fetching variables from memory, one at a time (or at least in dense blocks of contiguous memory). Billions of operations per second can be performed this way, controlled by a central computer clock ticking at multiple Gigahertz. Neural network based algorithms that power today’s AI require massively parallel computation, as their computing substrate comprises millions of neurons and billions of connections. When such algorithms are implemented on a CPU, a lot of time and energy are wasted on moving variables out and back to the memory, for every single computation. Modern GPUs alleviate part of this problem by introducing a parallel system of many computing units that still fetch the data sequentially from a separate memory unit. This allows neural network algorithms with appropriate network architecture to run much faster than on a CPU. However, a lot of energy is still wasted on transferring variables to and from the memory, especially if the structure of the neural network can not be mapped easily on the system of local memory (caches) on the GPU.\nNeuromorphic technology is the next fundamental step in computing architecture. Its structure is inspired by the neural networks in biological brains.\nBiological brains have evolved to efficiently combine real-time perception and control with long-term memory, formed and conserved on different time scales. Human brains require only 20-30 Watts to function, to “compute\" over 86 billion of neurons and trillions of synaptic connections.\nThese networks form massively parallel systems of elementary computing units and circuits, each of which “ticks” independently of the others i.e., there is no central clock triggering synchronous computation and the whole system supports a more efficient, asynchronous computing mode that can be replicated in hardware. Moreover, as in the brain where computing units are inseparable from the memory – both are realised in synaptic connection between neurons as well as their internal states – neuromorphic hardware systems feature local memory, co-located with the processing unit (as in digital neuromorphic devices, such as SpiNNaker or Loihi) or embedded in the computing VLSI circuits of mixed-signal devices or in-memory computing systems. Thus, neuromorphic processors compute with a fine-grained parallelism that does not rely on a particular neuronal network topology (as GPUs that are well-suited for batched, dense matrix- and tensor-based computation). This property allows for low-latency power-efficient processing\n[Sandamirskaya\net al.\n,\n2022\n]\n.\nRenner\net al.\n[\n2024\n]\nshows that for large network sizes Loihi can be up to 171 times more efficient in terms of energy-delay-product (EDP).\nOther researches showed that a EGRU-based language model can be simulated with SpiNNaker2 using only 0.39W, while an NVIDIA A100 GPU would need 60W for an equivalent task\n[Nazeer\net al.\n,\n2024\n]\n.\nAt the same time, it was shown that the first generation of SpiNNaker can be used for low-latency simulation of different areas of the brain.\nSpiNNaker allowed for the real-time simulation of the sensory cortex, surpassing best-published efforts on HPC by the factor of 3, and on GPUs by a factor of 2\n[Rhodes\net al.\n,\n2019\n]\n.\nThe importance of considering alternative computing paradigms, such as neuromorphic, is especially apparent when we consider energy efficiency. In our project, we chose to use Loihi2, rather than basing the hand detection subsystem on a GPU-based system such as Nvidia Jason Nano.\nJason Nano has a maximum power consumption of 5-10W, depending on the setup, while Loihi2 only requires 4mW to drive the computation.\nThe individual SpiNNaker2 chip achieves an energy consumption of the order of 1 to 2.5W and in a range of 48-120W per 48-node board\n[Rhodes\net al.\n,\n2019\n]\n. For the purpose of this paper, we have estimated that SPAUN implementation on SpiNNaker2 would take ten 48-node boards, as compared to a full DGX with 8 A100 GPUs. The DGX A100 unit consumes about 6.5KW, therefore a SpiNNaker2 implementation would consume between 5.4 to 13.5 times less power.\nAnother advantage is that on the SpiNNaker platform, the model could be run in real-time, whereas using the DGX box, the model had to be deployed at 45% of real-time.\nAlthough that was sufficient for our use case, this could prove to be problematic for real-time applications.\nNeuromorphic hardware opens a new algorithmic space, in which event-based, or spiking neural networks with various topologies can solve perception, memory formation (i.e. learning), planning, and control tasks orders of magnitude faster and more power efficiently than conventional algorithms. New ways of computation and information representation – such as temporal computing with spike-events and spike-timing dependent plasticity, or spiking realisation of vector-symbolic architectures and hyperdimensional computing, or attractor dynamics based computation with neural fields – mark a new era of computing theory and technology. These methods allow powerful computations to be carried out even with small models, enabling rapid recognition and tracking of patterns, dynamic memory formation, and real-time control.\nThere are many other ways in which biologically-inspired learning and inference mechanisms will offer new opportunities, such as fast and continuous learning from small amounts of data which because of how current AI models are constructed and learned will always be difficult for them to achieve. The ’binding problem’ has also proven to be very difficult to solve in conventional artificial neural networks (ANNs).\nOther less obvious benefits that may be of equal importance are the ability to work robustly in the presence of corrupted data or sensor degradation\n[Hopkins\net al.\n,\n2023\n]\n. These capabilities are particularly useful to enable future ubiquitous robotic systems that will require a lot of intelligent computation, performed in real time in a sustainable manner.\nDespite the many computational and inferential opportunities offered by biologically-inspired, SNN-like and neuromorphic algorithms, the tiny amount of R&D investment so far spent on them compared to more conventional ANNs and LLMs means that they continue to lag behind in terms of absolute performance in most cases. A particular problem is that of toolchains and frameworks which allow experimentation and sharing of new ideas and which are very well developed in the more conventional AI space. As these new ideas draw in more interest and investment we will undoubtedly see a leveling up of the playing field.\nThe promising results already achieved will almost certainly be consolidated with new ones that compete much more closely whilst retaining their engineering advantages, as long as those with vested interests in heavily memory-reliant and energy-hungry ANNs do not attempt to stymie this competition.\n4\nMethods\n4.1\nHardware overview\n4.1.1\nAmeca - General purpose social robot\nFigure 3:\nAmeca - the humanoid social robot produced by Engineered Arts.\nAmeca is a social humanoid robot which can exhibit life-like human facial expressions\n[Singh\net al.\n,\n2022\n]\n.\nIt has been described as the world’s most advanced social robot.\nThe robot was developed by the robotics company Engineered Arts, the leading designer and manufacturer of humanoid entertainment robots.\nAmeca uses a proprietary operating system, Tritium, which allows for extending its capabilities via Python scripts. In order to make decisions on its movement and reactions, it is endowed with a buffering system and connected to the Spaun model responsible for state transitions.\nAmeca collects environmental data through hardware components, such as motors, sensors, cameras, and microphones. The robot can communicate via a speaker built into its chest and an external microphone added for interactions between the user and the robot. The microphone is directly connected to the robot’s internal computer - an Intel NUC - to reduce background noise and improve the quality of the transferred audio.\n4.1.2\nMicro Data Center\nA Micro Data Center (MDC) provides the backend of the showcase.\nTo minimize the acoustic noise in the stage area that is emitted from the server ventilation the MDC was built as a soundproofing housing.\nThe MDC consists of two racks including the following components:\n•\n3 NVIDIA DGX A100 servers,\n•\nData server with Dual CPU AMD Epyc 7313,\n•\nVideo server for displaying visual content,\n•\nNetwork switches for management access to the backend and network integration of frontend devices and Intelligent Platform Management Interface (IPMI) for the server.\n•\nPeripheral infrastructure including: electrical and control infrastructure, fire suppression and ventilation systems, amplifier and mixer of the audio system.\n4.1.3\nGPUs\nThree NVIDIA DGX A100 systems, each equipped with 8 A100 GPUs were used in this project. Each system was dedicated to a specific functionality:\n•\nDGX 1: Large language model and speech recognition.\nThe first DGX system hosts the\nlarge language model (LLM) with 13 billion parameters\n. It is used for low-latency general knowledge conversation with the robot. The input for the LLM is provided by the speech recognition engine - Whisper by OpenAI.\n•\nDGX 2: Spaun 2.0 model and audio processing.\nThe DGX 2 is used to host the Spaun 2.0 model.\nwhich was adapted to run on the A100 GPU architecture using 7 GPUs (see section\n4.2.1\n).\nThe audio input from the two theremins is provided to DGX-2 using USB interface.\n•\nDGX 3: Processing sensory input for stage monitoring and lights management.\nThe third DGX handles the detection\nof people and their position relative to the stage.\nThe sensor input is provided by 3 cameras (2 RGB cameras, 1 depth camera). Each camera is used for a designated task (see sec.\n4.2.5\n).\n4.1.4\nNeuromorphic\nSpiNNaker\nThe SpiNNaker system\n[\n51\n]\nis a digital neuromorphic platform developed to simulate large-scale spiking neural networks. The installation built at the University of Manchester hosts over 1 million ARM968 cores in a single cluster, thus allowing for an unmatched parallelism, vastly improving the speed of computation compared to standard processor architectures. This made the SpiNNaker neuromorphic cluster in Manchester the world’s largest real-time brain simulator, until it was recently overtaken by the Hala Point system based on Loihi2.\nHowever, the new SpiNNaker2 cluster being built by the Technical University of Dresden exceeds the capacity of those two systems, setting up a new record for the world’s largest brain-like supercomputer.\nSpiNNaker2\nTo unleash the cutting-edge real-time autonomous capabilities, in the next iterations of the project we will harness the power of SpiNNaker2, the most flexible neural architecture in the realm of neuromorphic substrates. SpiNNaker2 serves as an accelerator tailored for extensive event-based and asynchronous processing\n[Mayr\net al.\n,\n2019\n]\n.\nThus, it allows for unprecedented energy-consumption reduction, reaching 1/10th of the power used in a comparable GPU setup.\nThe SpiNNaker2 microchip comprises 152 ARM-based processing elements interconnected via a network-on-chip (NoC).\nThe chip’s collective 19 MB on-chip SRAM is complemented by 2 GB of LPDDR4 memory.\nWhat sets it apart from the first generation SpiNNaker chip is specialized accelerators for exponential functions, random number generation, and multiply-accumulate (MAC) operations.\nThese features allow it to support the efficient implementation of sparsity-aware artificial neural networks, symbolic AI, and spiking neural networks\n[Gonzalez\net al.\n,\n2024\n]\n.\nSpiNNaker2‘s biological inspiration is evident in the chip’s architecture. Distributed processing elements work asynchronously, enabling massively-parallel event-based and sparse computation with on-demand power consumption.\nThe chip is designed to minimise power consumption during idle times compared to peak performance. Each processing element is running autonomously, allowing power control with Dynamic Voltage and Frequency Scaling (DVFS) per element, which ensures fine-grained power optimisation on-demand.\nA light-weight Network-on-Chip distributes information across the chip and supports input and output streaming of data with a minimum latency.\nBrain-like accelerators enhance computational performance for biologically inspired networks. Hence, the system provides native support for hybrids of traditional AI and spiking networks.\nSustainable Supercomputing capabilities and hot liquid-cooling design\nThe expected exponential growth in AI application and hardware in the next few years makes it very meaningful to investigate energy efficient solutions to save\nCO\n2\n\\textrm{CO}_{2}\nemissions and operational costs.\nOne pillar of this approach is the use of neuromorphic hardware. Another is the implementation of hot liquid cooling that enables a maximum efficient operation of the cooling infrastructure and an optional waste heat reuse.\nA reference example is the design of the 100% liquid-cooled infrastructure for SpiNNaker2 in cooperation with TU Dresden and SpiNNcloud systems.\nThe patented liquid cooling technology designed by Cloud&Heat and WAIYS not only operates much more quietly than conventional air cooling but also leverages high liquid temperatures to enable effective waste heat reuse for facility cooling or even industrial process heat applications\n[Struckmeier,\n2018\n]\n.\nOur previous datacenter development has shown that the cooling circuit for a neurmorphic server consumes approximately 1% of the total energy needed for the whole cluster, which significantly outperforms any air-cooled solutions.\nWhen this technology is combined with other complementary cooling methods, the integrated approach can reduce energy consumption by up to 70% compared to traditional air cooling, underscoring the importance of employing a diverse cooling strategy for energy efficient use cases\n[Cloud&Heat Technologies GmbH,\n2019\n]\n.\nIntel Loihi2 - Real-time Processing of Sensory Streams\nThe Loihi chip was originally released by Intel in 2018, along with the launch of Intel’s Neuromorphic Research Community program\n[\nDavies\net al.\n,\n]\n. Loihi2 was released in 2021 alongside Lava - an open-source software framework for developing neuro-inspired applications\n[Orchard\net al.\n,\n2021\n]\n. Loihi2 is a neuromorphic chip that generalizes the spiking neural network computation to asynchronous graphs with almost arbitrary topology at scale and adds the capacity to send graded information as spikes. This greatly expands the solution space available to algorithm designs. Lava opens a new way to think of neuromorphic algorithms as a system of interacting processes, each unfolding in time and exchanging spike-messages with other processes running on neuromorphic or conventional hardware, or with the interface to the physical world (sensors, robot controllers, or screens).\nThe focus of our use of Intel technology in this paper is to demonstrate how this emerging computing hardware can benefit robotic applications. Owing to its fundamentally different way of performing computations in an event-based, asynchronous manner, its fine-grained parallelism, and on-chip learning, neuromorphic hardware enables a new kind of robotic functionality: namely to integrate sophisticated “intelligent\" processing with artificial neural networks – as required for object detection, tracking, and localization, as well as motion planning – in fast real-time control loops. This fast perception and cognition capability will make robots safer and more effective in dynamic, human-centered environments. In the showcase, we focus on the sensing part of this processing, demonstrating fast, real-time visual processing.\nA crucial component of enabling real-time vision in our show case is the neuromorphic, event-based camera - Dynamic Vision Sensor (DVS). A DVS pairs particularly well with neuromorphic computing hardware: both events of the DVS and spikes in neuromorphic chips are asynchronous signals, making visual computation energy efficient and fast. We have integrated a DVS sensor with Intel’s Loihi2 chip and have trained a neural-network algorithm to track human hands based on asynchronous change events, output by the DVS. This is the first time that such real-time workload is demonstrated on Loihi, paving the way towards real-time vision, which is required for robots to be aware of their environment on a moment-to-moment basis.\nFigure 4:\nOverview of the hardware layer for the real-time neuromorphic hand tracking.\nThe hand tracking system was constructed using off-the-shelf components, in combination with the most recent Intel’s development board, Kapoho Point, and neuromorphic research chip Loihi2, see fig.\n4\n. The delivered hardware solution consists of a number of key components:\n•\nThe Kapoho Point system - Intel’s neuromorphic development board, consisting of 8 Loihi2 chips, an Intel FPGA system, and a baseboard to handle peripherals. One of the peripherals is an ethernet interface allowing communication with the NUC. The ethernet interface is used to send the commands to the neuromorphic chips from the NUC as well as to relay any data generated by the on-board application.\n•\nProphesee Evaluation Kit 4 HD (EVK4-HD) is an event-based camera which outputs spatiotemporal streams of events with 1280x720 spatial resolution and with a latency of 220\nμ\n\\mu\ns with a high dynamic range greater than 110dB.\n•\nDepth information is captured by the Intel RealSense Depth Camera D435 at a resolution of 1280x720 and up to a range of 3m.\n4.2\nSoftware overview\nIn this section, we will dive deeper into the software and hardware architecture which enables the unique interactive musical experience, as well as discuss some key components which allow Ameca to communicate in a seamless way.\nThe robot is provided with a variety of environmental cues, coming from the sensors placed directly in the robot, and additional sensors placed around the stage.\nSome of these additional sensory inputs come from the cameras placed at different positions around the stage, others from additional microphones on the stage and next to it.\nAdditionally, the robot receives the digital information from the theremins, in order to control its sound and aid the user in the interactive duet mode.\nFigure\n5\ndepicts all of the hardware and software components built for the purpose of this project, and how are they integrated.\nWe employed a unique integration of state-of-the-art systems to process the incoming streams of data. This combination involves neuromorphic solutions, such as Loihi2 for low-latency hand detection, high-performance GPUs in the form of NVIDIA DGX A100, which enables local simulation of the large language and speech recognition models, as well as cloud solutions.\nIn addition to that, the architecture includes the stage management system, which allows for the control of the stage lighting and video wall display.\nMoreover, we have integrated a number of audio processing elements, including speakers, microphones, and interfaces.\nFigure 5:\nOverview of the hardware and software integration architecture for the interactive musical showcase.\n4.2.1\nSpaun 2.0 integration for cognitive memory tasks and role in decision-making\nTo emulate the state transitions of the system a Spaun 2.0 model was used.\nSpaun (Semantic Pointer Architecture: Unified Network) is a large-scale cognitive neural model consisting of 2.5 million neurons\n[Stewart\net al.\n,\n2012\n]\n, implemented using the Semantic Pointer Architecture\n[Eliasmith,\n2013\n]\n.\nThis architecture was later expanded in Spaun 2.0, the World’s Largest Functional Brain Model\n[Choo,\n2018\n]\n.\nThis unique model has up to 6.5 million neurons used for cognitive memory tasks and to further enhance the robot’s control process, thus allowing Ameca to interact with the users using a finite-state machine (FSM).\nThe model can perform 12 cognitive tasks and has been demonstrated to reproduce behavioural and neurological data observed in natural cognitive agents.\nThe functional modules of the model corresponded to specific regions of the brain.\nThe authors demonstrated the feasibility of establishing models to conduct various tasks by following the organisational principles of the human brain.\nThe Spaun model is comprised of 8 modules: visual processing, information encoding, transformation, reward evaluation, information decoding, motor control, working memory, and action selection.\nSpaun’s action selection module is responsible for keeping track of Spaun’s progress and deciding what actions to perform as it executes each task. Additionally, the action selection system is responsible for managing the flow of information throughout the different networks that make up Spaun.\nFigure 6:\nSchematic diagram of the three-layer action selection hierarchy (Diagram from\nChoo [\n2018\n]\n).\nIn order to reduce the number and complexity of the condition-consequence pairs, Spaun’s action selection module is implemented as a three-layer hierarchy, with each layer of the hierarchy responsible for different functions of the action selection system (see fig.\n6\n). The top level of the hierarchy receives input from the vision module and is only responsible for high-level tracking, planning, and execution of Spaun’s task progress. This layer is implemented using the reasoning system. The second layer of the hierarchy combines the state representation of the top layer with the visual semantic pointers to generate the module-specific control signals used to control the flow of information within each of Spaun’s modules. The last layer of the action selection hierarchy includes the physical networks necessary to accomplish the information flows dictated by the outputs of the second layer of the action selection hierarchy.\nThe behaviour of the state machine, and thus the decision-making of the robot, is controlled by the Spaun model.\nThis allows the robot to modify its behaviour according to a system of state-based tasks related to music playing (see fig.\n7\n).\nFigure 7:\nOverview of the robot’s state transitions relevant for the theremin interactions.\nThe Spaun 2.0 model was deployed on the NVIDIA GPU system. To achieve a real-time multi-GPU implementation, the original Spaun 2.0 architecture was adapted to partition each of Spaun’s modules into its own “sub-network”. Each of these sub-networks can then be run in its own NengoOCL simulator\n[Bekolay\net al.\n,\n2014\n]\n. Figure\n8\nillustrates how the Spaun 2.0 system architecture has been distributed across 7 GPUs.\nIn the future, this system could be deployed on liquid-cooled neuromorphic hardware, such as SpiNNaker2, in order to make a full use of its energy-efficiency and further improve the latency.\nWhile the original Spaun 2.0 architecture contains 8 modules (which could intuitively be distributed across 8 GPUs), the reward evaluation and action selection modules have been combined due to the lightweight structure of the reward evaluation system and its extensive connectivity with the action selection system.\nFigure 8:\nThe segmentation of the Spaun 2.0 model architecture across 7 GPUs. Each red box indicates a part of the model running on its own GPU (Diagram from\nChoo [\n2018\n]\n).\n4.2.2\nConversational engine\nSeamless low-latency conversations with the social robot necessitate a high-performance large language model.\nTo this end, a recent model from the LLaMA line introduced by Meta was used\n[Touvron\net al.\n,\n2023\n]\n.\nFor the purpose of this showcase we used the variant of the LLaMA model with 13B parameters, which allows for quick computation of the responses without sacrificing their quality. This model is deployed locally on a GPU system.\nThe robot can have a multi-turn conversation, answer general domain questions, and questions related to the showcase within the context information provided to the model.\nIn order to enable the robot to communicate with users from different regions we use translation software. Verbal communication with the robot is translated using Google Cloud Translation API.\n4.2.3\nSpeech generation\nIn order to allow the text produced by the language models to be converted into lifelike speech, and thus enable the robot to communicate with its users, we decided to use Amazon Polly API\n[Werchniak\net al.\n,\n2021\n]\n.\nAmazon Polly employs advanced deep learning technologies to generate human-like speech, enabling the conversion of written text into natural-sounding audio.\n4.2.4\nSpeech recognition\nThe OpenAI Whisper model\n[Radford\net al.\n,\n2022\n]\nis used to enable the robot to listen to commands spoken by the users.\nWhisper is an automatic speech recognition system which shows human level robustness and accuracy on English speech recognition.\nThe Whisper model has 1550M parameters and runs locally on one of the NVIDIA DGX A100 machines.\n4.2.5\nPlayer detection and depth analysis\nTo enhance the robot’s awareness of its environment we use three strategically placed cameras around the stage. Each camera employs a distinct method to interpret the scene:\n•\nCamera 1 focuses on the audience in front of the stage, utilizing real-time face detection to count the number of people looking at the stage.\n•\nCamera 2 is tasked with detecting the presence of people on the stage. This can be used to understand stage occupancy and ensure safety measures are followed during Ameca’s performance.\n•\nCamera 3 incorporates a depth sensor to identify if someone is located behind the theremin. This gives the robot an indication to interact with the theremin player. To this end, we used a state-of-the-art depth estimation model, Depth Anything, introduced by\nYang\net al.\n[\n2024\n]\n.\n4.2.6\nIntention detection\nIntention detection to drive the robot’s FSM and enable interactive musical experience was carried out using the GPT3.5 API provided by OpenAI\n[Brown\net al.\n,\n2020\n]\n.\nIntention detection works by specifying a context and one or more intentions derived from it. The possible intentions consist of a name and a description combined as key-value pairs in a dictionary. These records are passed to an LLM together with the context and the message from which to detect the intention. A callback function handles the output which is the name of the detected intention or “None” if no intention could be detected.\n4.2.7\nHand tracking\nTo visualise the user’s input during the interactive theremin duet with the Ameca robot we decided to develop energy-efficient, low-latency hand tracking.\nTracking human hands is key for many robotic applications like imitation learning, human-robot collaboration, and it plays a key role in the showcase of playing an instrument with a human.\nTo this end, we collaborated with Intel and employed their latest neuromorphic processor - Loihi2\n[\nDavies\net al.\n,\n]\nin conjunction with a DVS camera.\nGiven that an event camera only outputs information when a change in scene brightness is detected at a particular pixel, a fixed event camera can readily serve as an elementary motion detector. As a result of motion between the scene and the camera, the pixels independently report perceived changes in brightness, resulting in a sparse and asynchronous stream of spatiotemporal data. See Section\n4.1.4\nfor more details regarding the hardware setup.\nFor the purpose of this project, three parallel tracks were developed to enable reliable hand tracking.\nFigure 9:\nOverview of the software layer of the neuromorphic hand tracking.\nThe first track deploys a deep spiking neural network (DSNN) trained to track hands using the DHP19 event-based dataset which features automatically generated labels for joints of a moving person that were obtained with a motion tracking system\n[Calabrese\net al.\n,\n2019\n]\n. We used the dataset to train a spiking neural network compatible with Loihi2 to track the hands.\nBy using spikes as the basis for computation, neuromorphic hardware achieves the highest efficiencies when running appropriate, spike-based algorithms. Given that the state-of-the-art neural-network-based motion tracking has been optimized for a different type of hardware (e.g., GPUs), novel neural network architectures are needed to take full advantage of neuromorphic computing platforms. The sigma-delta quantization is an example of an effective approach to run convolutional neural network more efficiently on conventional, as well as neuromorphic, hardware. In sigma-delta neural networks, spiking is achieved by means of a delta encoding process and a sigma decoding process. The encoder computes the difference between the latest nonlinear activations of the ANN neurons. Once the difference exceeds a predefined threshold, the encoder sends a spike to the next layer. The decoder accumulates the spikes until the original value is recovered, providing a discrete approximation of the ANN activation process.\nFigure 10:\nTracking example with both hands moving side-to-side (waving): (a) (240x180) event-frame capturing the active hands; (b) (86x65) event-frame downsampled to run on chip; (c) output of the multi-peak DNF overlayed the downsampled event-frame; (d) upscaled DNF output overlayed the original event-frame.\nThe second computation that is deployed relies on the theory of dynamic neural fields\n[Schöner\net al.\n,\n2015\n]\nto improve deep-learning based tracking. This neuromorphic algorithm can also be used to filter the coherent motion in the event-stream, constituting an additional mechanism to generate hand-motion hypotheses. Dynamic field theory (DFT) is a mathematical and conceptual framework for modelling human cognition. This approach has been used to develop cognitive architectures for robots (spatial language, action planning, learning), and is used as one of computing paradigms for high-level programming of neuromorphic chips.\nIn this framework, neural populations (layers) form attractor dynamics with stabilized spatial patterns that filter out noise and provide temporal filtering for tracking moving objects in the presence of distractors.\nIn this application, we decided to use these methods in a sequential configuration, in which the DNF-filter stabilizes the output of the deep neural network. Additional filtering of the visual scene (figure-background segmentation) is performed based on the depth information from a conventional RGB-D camera, in this case Intel’s RealSense (see fig.\n10\n).\nAuthor contributions\nAuthors are grouped based on contributions:\nT.R., X.C., C.M., H.G., M.A., K.K., C.E., M.A. - Initial programming, theremin research, and Spaun integration.\nM.H., S.F. - Help with writing the paper.\nY.S., L.A., S.G. - Development of the Loihi2 subsystem.\nJ.F., L.F., F.W. - Programming and further development of the showcase.\nL.W., H.P., J.S., K.R., N.A., M.H. - Supervision and original vision for the showcase.\nAcknowledgements\nWe would like to also thank the NEOM smart city initiative for initiating and funding the creation of the Theremin playing robotic usecase. We thank Intel Neuromorphic Research Community and Intel’s Neuromorphic Computing Lab for their support with Loihi2 implementation. Some of the ideas behind this work were originally discussed by the authors at the Capo Caccia Workshop towards Neuromorphic Intelligence.\nMoreover, we would like to thank and acknowledge the role of our colleagues and collaborators:\nProf. Dr. Chris Eliasmith,\nCarolina Eyck,\nNourchene Ferchichi,\nMatthieu Amiguet,\nTravis Dewolf,\nEric Schulz,\nEric Wallin,\nMike Davies,\nand Morgan Roe.\nReferences\nS. Ahmad and J. Hawkins (2016)\nHow do neurons operate on sparse distributed representations? a mathematical theory of sparsity, neurons and active dendrites\n.\npp.\n.\nCited by:\n§3.3\n.\nV. Albino, U. Berardi, and R. Dangelico (2015)\nSmart Cities: Definitions, Dimensions, Performance, and Initiatives\n.\nJournal of Urban Technology\n22\n,\npp. 2015\n.\nExternal Links:\nDocument\nCited by:\n§3.2\n.\nM. Batty (2012)\nSmart Cities, Big Data\n.\nEnvironment and Planning B: Planning and Design\n39\n(\n2\n),\npp. 191–193\n.\nExternal Links:\nDocument\n,\nLink\nCited by:\n§3.2\n.\nT. Bekolay, J. Bergstra, E. Hunsberger, T. DeWolf, T. C. Stewart, D. Rasmussen, X. Choo, A. Voelker, and C. Eliasmith (2014)\nNengo: a python tool for building large-scale functional brain models\n.\nFrontiers in Neuroinformatics\n7\n.\nExternal Links:\nLink\n,\nDocument\n,\nISSN 1662-5196\nCited by:\n§3.3\n,\n§4.2.1\n.\nT. Branco and M. Häusser (2010)\nThe single dendritic branch as a fundamental functional unit in the nervous system\n.\nCurrent Opinion in Neurobiology\n20\n(\n4\n),\npp. 494–502\n.\nNote:\nSignalling mechanisms\nExternal Links:\nISSN 0959-4388\n,\nDocument\n,\nLink\nCited by:\n§3.3\n.\nC. Breazeal (2003)\nToward sociable robots\n.\nRobotics and Autonomous Systems\n42\n(\n3\n),\npp. 167–175\n.\nNote:\nSocially Interactive Robots\nExternal Links:\nISSN 0921-8890\n,\nDocument\n,\nLink\nCited by:\n§2.1\n.\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei (2020)\nLanguage Models are Few-Shot Learners\n.\nExternal Links:\n2005.14165\nCited by:\n§4.2.6\n.\nE. Calabrese, G. Taverni, C. A. Easthope, S. Skriabine, F. Corradi, L. Longinotti, K. Eng, and T. Delbruck (2019)\nDHP19: Dynamic Vision Sensor 3D Human Pose Dataset\n.\nIn\n2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\n,\nVol.\n,\npp. 1695–1704\n.\nExternal Links:\nDocument\nCited by:\n§4.2.7\n.\nA. Caragliu, C. Del Bo, and P. Nijkamp (2011)\nSmart Cities in Europe\n.\nJournal of Urban Technology\n18\n(\n2\n),\npp. 65–82\n.\nExternal Links:\nDocument\n,\nLink\nCited by:\n§3.2\n.\nF. Choo (2018)\nSpaun 2.0: Extending the World’s Largest Functional Brain Model\n.\nPh.D. Thesis\n,\nUWSpace\n.\nCited by:\n§1\n,\n§3.3\n,\nFigure 6\n,\nFigure 8\n,\n§4.2.1\n.\nCloud&Heat Technologies GmbH (2019)\nCO2 - and Cost-saving Potential in Data Centers Using the Cloud&Heat Cooling System with Waste Heat Utilization\n.\nNote:\nWhitepaperAccessed: 2025-02-20\nExternal Links:\nLink\nCited by:\n§4.1.4\n.\n[12]\nM. Davies, A. Wild, G. Orchard, Y. Sandamirskaya, G. A. F. Guerra, P. Joshi, P. Plank, and S. R. Risbud\nAdvancing neuromorphic computing with loihi: a survey of results and outlook\n.\nProceedings of the IEEE\n.\nCited by:\n§4.1.4\n,\n§4.2.7\n.\nM. Dirik (2020)\nAl-Jazari: The Ingenious Inventor of Cybernetics and Robotics\n.\nJournal of Soft Computing and Artificial Intelligence\n1\n(\n1\n),\npp. 47–58\n.\nCited by:\n§2.2\n.\nC. Eliasmith (2013)\nHow to Build a Brain: A Neural Architecture for Biological Cognition\n.\nOxford Series on Cognitive Models and Architectures\n,\nOxford University Press\n.\nExternal Links:\nISBN 9780199794690\n,\nLink\nCited by:\n§1\n,\n§4.2.1\n.\nA. d. Garcez and L. C. Lamb (2023)\nNeurosymbolic ai: the 3rd wave\n.\nArtificial Intelligence Review\n56\n(\n11\n),\npp. 12387–12406\n.\nExternal Links:\nDocument\n,\nISBN 1573-7462\n,\nLink\nCited by:\n§1\n.\nL. Ge and K. K. Parhi (2020)\nClassification using hyperdimensional computing: a review\n.\nIEEE Circuits and Systems Magazine\n20\n,\npp. 30–47\n.\nExternal Links:\nLink\nCited by:\n§3.3\n.\nO. Golubchikov and M. Thornbush (2020)\nArtificial intelligence and robotics in smart city strategies and planned smart development\n.\nSmart Cities\n3\n(\n4\n),\npp. 1133–1144\n.\nExternal Links:\nLink\n,\nISSN 2624-6511\n,\nDocument\nCited by:\n§1\n.\nH. A. Gonzalez, J. Huang, F. Kelber, K. K. Nazeer, T. Langer, C. Liu, M. Lohrmann, A. Rostami, M. Schöne, B. Vogginger, T. C. Wunderlich, Y. Yan, M. Akl, and C. Mayr (2024)\nSpiNNaker2: A Large-Scale Neuromorphic System for Event-Based and Asynchronous Machine Learning\n.\nExternal Links:\n2401.04491\nCited by:\n§1\n,\n§4.1.4\n.\nS. Grossberg (2013)\nAdaptive resonance theory: how a brain learns to consciously attend, learn, and recognize a changing world\n.\nNeural networks\n37\n,\npp. 1–47\n.\nCited by:\n§3.3\n.\nM. Hopkins, J. Fil, E. G. Jones, and S. Furber (2023)\nBitBrain and Sparse Binary Coincidence (SBC) memories: Fast, robust learning and inference for neuromorphic architectures\n.\nFrontiers in Neuroinformatics\n17\n.\nExternal Links:\nLink\n,\nDocument\n,\nISSN 1662-5196\nCited by:\n§3.2\n,\n§3.3\n,\n§3.4\n.\nG. Kastellakis, D. J. Cai, S. C. Mednick, A. J. Silva, and P. Poirazi (2015)\nSynaptic clustering within dendrites: an emerging theory of memory formation\n.\nProgress in Neurobiology\n126\n,\npp. 19–35\n.\nExternal Links:\nISSN 0301-0082\n,\nDocument\n,\nLink\nCited by:\n§3.3\n.\nD. Kleyko, D. A. Rachkovskij, E. Osipov, and A. Rahimi (2022)\nA survey on hyperdimensional computing aka vector symbolic architectures, part i: models and data transformations\n.\nACM Computing Surveys\n55\n(\n6\n),\npp. 1–40\n.\nExternal Links:\nISSN 1557-7341\n,\nLink\n,\nDocument\nCited by:\n§3.3\n.\nD. Kleyko, D. Rachkovskij, E. Osipov, and A. Rahimi (2023)\nA survey on hyperdimensional computing aka vector symbolic architectures, part ii: applications, cognitive models, and challenges\n.\nACM Computing Surveys\n55\n(\n9\n),\npp. 1–52\n.\nExternal Links:\nISSN 1557-7341\n,\nLink\n,\nDocument\nCited by:\n§3.3\n.\nJ. E. Laird (2019)\nThe soar cognitive architecture\n.\nMIT press\n.\nCited by:\n§3.3\n.\nM. E. Larkum and T. Nevian (2008)\nSynaptic clustering by dendritic signalling mechanisms\n.\nCurrent Opinion in Neurobiology\n18\n(\n3\n),\npp. 321–331\n.\nNote:\nSignalling mechanisms\nExternal Links:\nISSN 0959-4388\n,\nDocument\n,\nLink\nCited by:\n§3.3\n.\nP. Lichtsteiner, C. Posch, and T. Delbruck (2008)\nA 128\n×\n\\times\n128 120 db 15\nμ\n\\mu\ns latency asynchronous temporal contrast vision sensor\n.\nIEEE Journal of Solid-State Circuits\n43\n(\n2\n),\npp. 566–576\n.\nExternal Links:\nDocument\nCited by:\n§1\n.\nB. Liu (2017)\nLifelong machine learning: a paradigm for continuous learning\n.\nFrontiers of Computer Science\n11\n(\n3\n),\npp. 359–361\n.\nExternal Links:\nDocument\n,\nISBN 2095-2236\n,\nLink\nCited by:\n§3.2\n.\nM. London and M. Häusser (2005)\nDendritic computation.\n.\nAnnual review of neuroscience\n28\n,\npp. 503–32\n.\nExternal Links:\nLink\nCited by:\n§3.3\n.\nG. Marcus (2020)\nThe Next Decade in AI: Four Steps Towards Robust Artificial Intelligence\n.\narXiv e-prints\n,\npp. arXiv:2002.06177\n.\nExternal Links:\nDocument\n,\n2002.06177\nCited by:\n§1\n.\nC. Mayr, S. Hoeppner, and S. Furber (2019)\nSpiNNaker 2: A 10 Million Core Processor System for Brain Simulation and Machine Learning\n.\nExternal Links:\n1911.02385\nCited by:\n§4.1.4\n.\nS. A. Mehr, M. Singh, D. Knox, D. M. Ketter, D. Pickens-Jones, S. Atwood, C. Lucas, N. Jacoby, A. A. Egner, E. J. Hopkins, R. M. Howard, J. K. Hartshorne, M. V. Jennings, J. Simson, C. M. Bainbridge, S. Pinker, T. J. O’Donnell, M. M. Krasnow, and L. Glowacki (2019)\nUniversality and diversity in human song\n.\nScience\n366\n(\n6468\n),\npp. eaax0868\n.\nExternal Links:\nDocument\n,\nLink\n,\nhttps://www.science.org/doi/pdf/10.1126/science.aax0868\nCited by:\n§2.1\n.\nT. Mizumoto, H. Tsujino, T. Takahashi, T. Ogata, and H. G. Okuno (2009)\nThereminist robot: Development of a robot theremin player with feedforward and feedback arm control based on a Theremin’s pitch model\n.\nIn\n2009 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp. 2297–2302\n.\nExternal Links:\nDocument\nCited by:\n§2.2\n.\nM. Mori, K. F. MacDorman, and N. Kageki (2012)\nThe uncanny valley [from the field]\n.\nIEEE Robotics & Automation Magazine\n19\n(\n2\n),\npp. 98–100\n.\nExternal Links:\nDocument\nCited by:\n§2.1\n.\nC. Moulin-Frier, T. Fischer, M. Petit, G. Pointeau, J. Puigbo, U. Pattacini, S. C. Low, D. Camilleri, P. Nguyen, M. Hoffmann,\net al.\n(2017)\nDAC-h3: a proactive robot cognitive architecture to acquire and express knowledge about the world and the self\n.\nIEEE Transactions on Cognitive and Developmental Systems\n10\n(\n4\n),\npp. 1005–1022\n.\nCited by:\n§3.3\n.\nK. K. Nazeer, M. Schöne, R. Mukherji, B. Vogginger, C. Mayr, D. Kappel, and A. Subramoney (2024)\nLanguage Modeling on a SpiNNaker 2 Neuromorphic Chip\n.\nExternal Links:\n2312.09084\nCited by:\n§3.4\n.\nG. Orchard, E. P. Frady, D. B. D. Rubin, S. Sanborn, S. B. Shrestha, F. T. Sommer, and M. Davies (2021)\nEfficient Neuromorphic Signal Processing with Loihi 2\n.\nExternal Links:\n2111.03746\nCited by:\n§1\n,\n§4.1.4\n.\nH. Park, W. Jo, K. Choi, H. Jung, Y. jargalbaatar, B. Lee, and D. Kim (2015)\nA study about sound quality for violin playing robot\n.\nProcedia Computer Science\n56\n,\npp. 496–501\n.\nNote:\nThe 10th International Conference on Future Networks and Communications (FNC 2015) / The 12th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2015) Affiliated Workshops\nExternal Links:\nISSN 1877-0509\n,\nDocument\n,\nLink\nCited by:\n§2.2\n.\nD. Paulus, V. Seib, J. Giesen, and D. Grüntjens (2013)\nEnhancing human-robot interaction by a robot face with facial expressions and synchronized lip movements\n.\npp.\n.\nCited by:\n§2.1\n.\nJ. E. Pedersen, S. Abreu, M. Jobst, G. Lenz, V. Fra, F. C. Bauer, D. R. Muir, P. Zhou, B. Vogginger, K. Heckel, G. Urgese, S. Shankar, T. C. Stewart, S. Sheik, and J. K. Eshraghian (2024)\nNeuromorphic intermediate representation: a unified instruction set for interoperable brain-inspired computing\n.\nNature Communications\n15\n(\n1\n),\npp. 8122\n.\nExternal Links:\nDocument\n,\nISBN 2041-1723\n,\nLink\nCited by:\n§3.1\n.\nA. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever (2022)\nRobust Speech Recognition via Large-Scale Weak Supervision\n.\nExternal Links:\n2212.04356\nCited by:\n§4.2.4\n.\nA. Rast, A. B. Stokes, S. Davies, S. V. Adams, H. Akolkar, D. R. Lester, C. Bartolozzi, A. Cangelosi, and S. Furber (2015)\nTransport-independent protocols for universal aer communications\n.\nIn\nNeural Information Processing\n,\nLecture Notes in Computer Science\n, Vol.\n9492\n,\nUnited States\n,\npp. 675–684\n(\nEnglish\n).\nExternal Links:\nDocument\n,\nISBN 978-3-319-26560-5\nCited by:\n§3.1\n.\nA. Renner, L. Supic, A. Danielescu, G. Indiveri, B. A. Olshausen, Y. Sandamirskaya, F. T. Sommer, and E. P. Frady (2024)\nNeuromorphic visual scene understanding with resonator networks\n.\nNature Machine Intelligence\n6\n(\n6\n),\npp. 641–652\n.\nExternal Links:\nISSN 2522-5839\n,\nLink\n,\nDocument\nCited by:\n§3.3\n,\n§3.4\n.\nO. Rhodes, L. Peres, A. G. D. Rowley, A. Gait, L. A. Plana, C. Brenninkmeijer, and S. B. Furber (2019)\nReal-time cortical simulation on neuromorphic hardware\n.\nPhilosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences\n378\n(\n2164\n),\npp. 20190160\n.\nExternal Links:\nISSN 1471-2962\n,\nLink\n,\nDocument\nCited by:\n§3.4\n,\n§3.4\n.\nF. E. Ritter, F. Tehranchi, and J. D. Oury (2019)\nACT-r: a cognitive architecture for modeling cognition\n.\nWiley Interdisciplinary Reviews: Cognitive Science\n10\n(\n3\n),\npp. e1488\n.\nCited by:\n§3.3\n.\nC. Roads (1986)\nThe tsukuba musical robot\n.\nComputer Music Journal\n10\n(\n2\n),\npp. 39–43\n.\nExternal Links:\nISSN 01489267, 15315169\n,\nLink\nCited by:\n§2.2\n.\nY. Sandamirskaya, M. Kaboli, J. Conradt, and T. Celikel (2022)\nNeuromorphic computing hardware and neural architectures for robotics\n.\nScience Robotics\n7\n(\n67\n),\npp. eabl8419\n.\nCited by:\n§3.4\n.\nK. Schlegel, P. Neubert, and P. Protzel (2021)\nA comparison of vector symbolic architectures\n.\nArtificial Intelligence Review\n55\n(\n6\n),\npp. 4523–4555\n.\nExternal Links:\nISSN 1573-7462\n,\nLink\n,\nDocument\nCited by:\n§3.3\n.\nG. Schöner, J. Spencer, and D. Research Group (2015)\nDynamic Thinking: A Primer on Dynamic Field Theory\n.\nOxford University Press\n.\nExternal Links:\nISBN 9780199300563\n,\nDocument\n,\nLink\nCited by:\n§4.2.7\n.\nF. Shenavarmasouleh, F. G. Mohammadi, M. H. Amini, and H. Reza Arabnia (2022)\nEmbodied ai-driven operation of smart cities: a concise review\n.\nIn\nCyberphysical Smart Cities Infrastructures\n,\npp. 29–45\n.\nExternal Links:\nISBN 9781119748342\n,\nDocument\n,\nLink\n,\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119748342.ch3\nCited by:\n§1\n.\nS. Singh, D. Chaudhary, A. D. Gupta, B. Prakash Lohani, P. K. Kushwaha, and V. Bibhu (2022)\nArtificial intelligence, cognitive robotics and nature of consciousness\n.\nIn\n2022 3rd International Conference on Intelligent Engineering and Management (ICIEM)\n,\nVol.\n,\npp. 447–454\n.\nExternal Links:\nDocument\nCited by:\n§2.1\n,\n§4.1.1\n.\n[51]\n(2020)\nSpiNNaker: A Spiking Neural Network Architecture\n.\nS. Furber and P. Bogdan (Eds.)\n,\nExternal Links:\nDocument\n,\nISBN 978-1-68083-652-3\n,\nLink\nCited by:\n§4.1.4\n.\nT. C. Stewart, X. Choo, and C. Eliasmith (2012)\nSpaun: A Perception-Cognition-Action Model Using Spiking Neurons\n.\nIn\nCognitive Science Society\n,\npp. 1018–1023\n.\nCited by:\n§4.2.1\n.\nJ. Struckmeier (2018)\nCited by:\n§1\n,\n§4.1.4\n.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom (2023)\nLlama 2: Open Foundation and Fine-Tuned Chat Models\n.\nExternal Links:\n2307.09288\nCited by:\n§4.2.2\n.\nG. Weinberg and S. Driscoll (2006)\nRobot-human interaction with an anthropomorphic percussionist\n.\npp. 1229–1232\n.\nExternal Links:\nDocument\nCited by:\n§2.2\n.\nA. Werchniak, R. B. Chicote, Y. Mishchenko, J. Droppo, J. Condal, P. Liu, and A. Shah (2021)\nExploring the application of synthetic audio in training keyword spotters\n.\nIn\nICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\n,\nVol.\n,\npp. 7993–7996\n.\nExternal Links:\nDocument\nCited by:\n§4.2.3\n.\nY. Wu, P. Kuvinichkul, P. Cheung, and Y. Demiris (2011)\nTowards anthropomorphic robot thereminist\n.\npp. 235 – 240\n.\nExternal Links:\nDocument\nCited by:\n§2.2\n.\nL. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao (2024)\nDepth Anything: Unleashing the Power of Large-Scale Unlabeled Data\n.\narXiv:2401.10891\n.\nCited by:\n3rd item\n.\nA. Zanella, N. Bui, A. Castellani, L. Vangelista, and M. Zorzi (2014)\nInternet of Things for Smart Cities\n.\nIEEE Internet of Things Journal\n1\n(\n1\n),\npp. 22–32\n.\nExternal Links:\nDocument\nCited by:\n§3.2\n.\nY. Zhao and L. Sentis (2019)\nChapter 2 - distributed impedance control of latency-prone robotic systems with series elastic actuation\n.\nIn\nStability, Control and Application of Time-delay Systems\n,\nQ. Gao and H. R. Karimi (Eds.)\n,\npp. 23–51\n.\nExternal Links:\nISBN 978-0-12-814928-7\n,\nDocument\n,\nLink\nCited by:\n§3.1\n.",
  "preview_text": "After Industry 4.0 has embraced tight integration between machinery (OT), software (IT), and the Internet, creating a web of sensors, data, and algorithms in service of efficient and reliable production, a new concept of Society 5.0 is emerging, in which infrastructure of a city will be instrumented to increase reliability, efficiency, and safety. Robotics will play a pivotal role in enabling this vision that is pioneered by the NEOM initiative - a smart city, co-inhabited by humans and robots. In this paper we explore the computing platform that will be required to enable this vision. We show how we can combine neuromorphic computing hardware, exemplified by the Loihi2 processor used in conjunction with event-based cameras, for sensing and real-time perception and interaction with a local AI compute cluster (GPUs) for high-level language processing, cognition, and task planning. We demonstrate the use of this hybrid computing architecture in an interactive task, in which a humanoid robot plays a musical instrument with a human. Central to our design is the efficient and seamless integration of disparate components, ensuring that the synergy between software and hardware maximizes overall performance and responsiveness. Our proposed system architecture underscores the potential of heterogeneous computing architectures in advancing robotic autonomy and interactive intelligence, pointing toward a future where such integrated systems become the norm in complex, real-time applications.\n\nHeterogeneous computing platform for real-time robotics.\nJakub Fil\n1\n1\n1\n1\nCorresponding author–jakub.fil@waiys.com,\n1\nWAIYS GmbH,\n2\nZurich University of Applied Sciences,\n3\nSpiNNcloud Systems GmbH,\n4\nTU Dresden,\n5\nApplied Brain Research,\n6\nNEOM,\n7\nThe University of Manchester.\nYulia Sandamirskaya\n2\nHector Gonzalez\n3\nLoïc Azzalin\n2\nStefan Glüge\n2\nLukas Friedenstab\n1\nFriedrich Wolf\n1\nTim Rosmeisl\n3\nMatthias Lohrmann\n3\nMahmoud Akl\n3\nKhaleel Khan\n4\nLeonie Wolf\n1\nKristin Richter\n1\nHolm Puder",
  "is_relevant": true,
  "relevance_score": 3.0,
  "extracted_keywords": [
    "locomotion",
    "whole body control"
  ],
  "one_line_summary": "这篇论文探讨了用于实时机器人应用的异构计算平台，结合神经形态计算和GPU集群，以支持人形机器人的交互任务，但未直接涉及强化学习、VLA、扩散模型、Flow Matching、VLM等特定技术。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-13T11:48:25Z",
  "created_at": "2026-01-20T17:49:43.383047",
  "updated_at": "2026-01-20T17:49:43.383058"
}