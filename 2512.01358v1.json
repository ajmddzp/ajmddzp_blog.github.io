{
    "id": "2512.01358v1",
    "title": "Modality-Augmented Fine-Tuning of Foundation Robot Policies for Cross-Embodiment Manipulation on GR1 and G1",
    "authors": [
        "Junsung Park",
        "Hogun Kee",
        "Songhwai Oh"
    ],
    "abstract": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨¡æ€å¢å¼ºçš„å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨å°†åŸºç¡€æœºå™¨äººç­–ç•¥é€‚é…åˆ°å¤šç§ç±»äººå½¢æ€ä¸Šã€‚æˆ‘ä»¬åœ¨ä¸¤ç§ä¸åŒè®¾ç½®ä¸‹éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼šï¼ˆiï¼‰GR1å½¢æ€ï¼Œåˆ©ç”¨å…¬å¼€æ•°æ®é›†å¹¶å¼•å…¥åå¤„ç†æ¨¡æ€ï¼ŒåŒ…æ‹¬äºŒå€¼æ¥è§¦ä¿¡å·å’Œç”±ZoeDepthç”Ÿæˆçš„åº¦é‡æ·±åº¦ï¼›ï¼ˆiiï¼‰Unitree G1å½¢æ€ï¼Œä¸ºæ­¤æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå…¨æ–°çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œèåˆäº†cuRoboè¿åŠ¨è§„åˆ’ã€é€†è¿åŠ¨å­¦è§£ç®—ä»¥åŠçœŸå®çš„æ¥è§¦åŠ›æµ‹é‡æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡æ€å¢å¼ºåœ¨ä¸åŒå½¢æ€ä¸Šå‡èƒ½æŒç»­æå‡ç­–ç•¥æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨GR1ä¸Šï¼Œå¼•å…¥æ¥è§¦çŠ¶æ€çº¿ç´¢ä¸RGB-Dèåˆä½¿åœ¨çº¿ä»»åŠ¡æˆåŠŸç‡ä»51%æå‡è‡³63%ã€‚è€Œåœ¨G1çš„â€œæ‘˜è‹¹æœæ”¾å…¥ç¢—ä¸­â€ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¥è§¦å¢å¼ºæ¨¡å‹å®ç°äº†94%çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºæ ‡å‡†å¾®è°ƒçš„48%ä»¥åŠé›¶æ ·æœ¬è¿ç§»çš„0%åŸºçº¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè½»é‡çº§çš„åå¤„ç†å¯æœ‰æ•ˆå¢å¼ºGR1ä¸Šçš„ç­–ç•¥è¡¨ç°ï¼Œè€Œé«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®å¯¹äºå‘Unitree G1å®ç°å¯é è¿ç§»è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶å»ºç«‹äº†ä¸€æ¡ç»Ÿä¸€çš„ã€ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è·¯å¾„ï¼Œé€šè¿‡æœ‰é’ˆå¯¹æ€§çš„æ¨¡æ€è®¾è®¡å’Œå¤šæ¨¡æ€å¾®è°ƒæ¥æ‹“å±•åŸºç¡€æœºå™¨äººç­–ç•¥ã€‚",
    "url": "https://arxiv.org/abs/2512.01358v1",
    "html_url": "https://arxiv.org/html/2512.01358v1",
    "html_content": "Modality-Augmented Fine-Tuning of Foundation Robot Policies for Cross-Embodiment Manipulation on GR1 and G1\nJunsung Park\n1\n, Hogun Kee\n1\n, and Songhwai Oh\n1\n1\nDepartment of Electrical and Computer Engineering, Seoul National University\nAbstract\nWe present a modality-augmented fine-tuning framework for adapting foundation robot policies to diverse embodiments. Our study evaluates two settings: (i) the GR1 embodiment using the public HuggingFace dataset, where we introduce post-processed modalities including binary contact signals and ZoeDepth-generated metric depth; and (ii) the Unitree G1 embodiment, for which we build a new multi-modal dataset using cuRobo motion planning, inverse kinematics, and ground-truth contact-force measurements (\nHuggingFace repository\n).\nModality augmentation consistently improves policy performance across embodiments. On GR1, adding contact-state cues and RGB-D fusion increases online success from 51% to 63%. On the G1\nPick Apple to Bowl\ntask, zero-shot GR00T achieves 0% success, standard fine-tuning reaches 48%, and our contact-enhanced model attains up to 94%.\nOverall, our results show that lightweight post-processing strengthens GR1 policies, while high-quality multi-modal data enables reliable Unitree G1 embodiment transfer. This work demonstrates a unified, data-centric pathway for extending foundation robot policies through targeted modality design and multi-modal fine-tuning.\nI\nIntroduction\nRecent advances in foundation robot policies have demonstrated that large-scale, vision-language-action\n(VLA) models such as Isaac GR00T N1.5\n[\n9\n]\nand PiZero can serve as strong generalist\ncontrollers capable of performing diverse manipulation tasks. These models leverage massive demonstration\ndatasets and diffusion-based action generation\n[\n1\n,\n2\n,\n3\n]\n, enabling\nrobust trajectory synthesis and long-horizon control. However, despite these successes, current foundation\npolicies still exhibit notable limitations when deployed in fine-grained manipulation settings that require\naccurate perception of contact events, reliable depth reasoning, and precise interaction-aware control.\nAs highlighted in the GR00T N1 report\n[\n9\n]\n, existing robotic datasets form an archipelago of\nâ€œdata islandsâ€ arising from heterogeneous embodiments, sensing configurations, and control modes. This\nheterogeneity prevents foundation models from leveraging Internet-scale data in the same way as language or\nvision models. Moreover, cross-embodiment generalization remains challenging because robot morphology\nand kinematic structure fundamentally shape the distribution of feasible motions and interaction strategies.\nWhile GR00T addresses part of this challenge through embodiment-specific encoders and a dual-system\narchitecture, its generalization ability is ultimately bounded by the modalities present in its training data.\nA concrete example of these modality limitations appears in the publicly available GR1 dataset on\nHuggingFace, which provides RGB observations and proprioceptive states but lacks key interaction-centric\nmodalities such as depth, binary contact indicators, or force measurements. Without these modalities, VLA\nmodels must infer contact and object interaction boundaries purely from color imageryâ€”an ill-posed problem\nin scenarios involving occlusion, object overlap, or visually ambiguous contact transitions. As a result,\nGR00T-style policies often struggle to detect stable grasps, reason about objectâ€“robot contact timing, or\nmaintain closed-loop stability during contact-rich manipulation. These limitations become even more\npronounced when transferring a foundation model to a new embodiment such as the Unitree G1, where\ndifferences in kinematics, actuation, workspace geometry, and camera placement further degrade zero-shot\nperformance.\nTo address these limitations, we adopt a data-centric and modality-augmented fine-tuning strategy. For the\nGR1 embodiment, we enrich the original dataset by post-processing demonstrations to estimate metric depth\nvia ZoeDepth and by injecting binary contact signals derived from collision reasoning within the simulator.\nThis yields a multi-modal variant of the GR1 dataset suitable for training policies that better understand\nobject interaction boundaries. For the G1 embodimentâ€”where no public dataset existsâ€”we generate a\nfully customized multi-modal demonstration set using cuRobo-based motion planning and inverse\nkinematics\n[\n17\n]\n. This pipeline provides precise trajectories paired with ground-truth contact force\nmeasurements, enabling us to analyze how modality composition and embodiment-specific data influence\npolicy adaptation.\nOur contributions are threefold.\n(1) We construct multi-modal GR1 and G1 datasets that incorporate depth and contact information, addressing\nthe modality bottlenecks that limit current VLA foundation policies.\n(2) We propose lightweight architectural augmentations for incorporating contact and depth signals into the\nGR00T diffusion policy without modifying the core model structure.\n(3) We demonstrate that modality-augmented fine-tuning substantially improves manipulation success rates\nand enables strong embodiment transfer from GR1 to G1, reducing the performance gap introduced by\nmorphological differences.\nOverall, this work presents a practical pathway for adapting foundation robot policies to new embodiments and\ninteraction-rich tasks by explicitly engineering the sensory modalities that underpin robust manipulation.\nII\nPreliminaries\nII-A\nDiffusion Policy\nDiffusion policy is a generative action model that formulates robot control as a conditional denoising process.\nInstead of predicting actions directly, the policy learns a diffusion model that iteratively transforms a noisy\nlatent action representation into a feasible trajectory conditioned on observations. This formulation is grounded\nin denoising diffusion probabilistic models\n[\n1\n]\nand their implicit variants\n[\n2\n]\n,\nand has recently been extended to Transformer-based architectures such as DiT\n[\n3\n]\n.\nFormally, let\nğ¨\nt\n\\mathbf{o}_{t}\ndenote the observation at time\nt\nt\n, including proprioceptive states or visual features,\nand let\nğš\nt\n:\nt\n+\nH\n\\mathbf{a}_{t:t+H}\nrepresent a future action sequence over a horizon\nH\nH\n. The diffusion policy samples\na noisy latent\nğ±\nT\n\\mathbf{x}_{T}\nfrom a Gaussian prior and gradually denoises it over\nT\nT\nsteps using a learned\ndenoising network\nÏµ\nÎ¸\n\\epsilon_{\\theta}\n:\nğ±\nt\nâˆ’\n1\n=\n1\nÎ±\nt\nâ€‹\n(\nğ±\nt\nâˆ’\n1\nâˆ’\nÎ±\nt\n1\nâˆ’\nÎ±\nÂ¯\nt\nâ€‹\nÏµ\nÎ¸\nâ€‹\n(\nğ±\nt\n,\nğ¨\nt\n)\n)\n+\nÏƒ\nt\nâ€‹\nğ³\n.\n\\mathbf{x}_{t-1}=\\frac{1}{\\sqrt{\\alpha_{t}}}\\left(\\mathbf{x}_{t}-\\frac{1-\\alpha_{t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{o}_{t})\\right)+\\sigma_{t}\\mathbf{z}.\nHere,\nÎ±\nt\n\\alpha_{t}\nand\nÏƒ\nt\n\\sigma_{t}\nfollow a predefined noise schedule, and\nğ³\n\\mathbf{z}\nis Gaussian noise.\nAfter the final denoising step, the clean latent\nğ±\n0\n\\mathbf{x}_{0}\nis decoded into an action sequence via\nğš\nt\n:\nt\n+\nH\n=\nf\nÎ¸\nâ€‹\n(\nğ±\n0\n)\n.\n\\mathbf{a}_{t:t+H}=f_{\\theta}(\\mathbf{x}_{0}).\nReplacing the traditional UNet with a Transformer backboneâ€”as done in DiT and subsequent robotic diffusion\npoliciesâ€”enables scalability and improves multimodal conditioning. This approach provides two key advantages\nfor robot manipulation:\n(1)\nMultimodality\n: the policy can generate diverse yet valid action trajectories under ambiguous\nperception, and\n(2)\nTemporal coherence\n: predicting full action sequences improves stability for long-horizon tasks.\nII-B\nIsaac GR00T\nIsaac GR00T is an open Vision-Language-Action (VLA) foundation model designed for general-purpose\nhumanoid robotics\n[\n9\n]\n. As illustrated in Fig.\n1\n, GR00T adopts a\ndual-system cognitive architecture inspired by human SystemÂ 1 and SystemÂ 2 processing. SystemÂ 2 is a\nVision-Language Model (VLM) that interprets instructions and visual observations, while SystemÂ 1 is a\nDiffusion Transformer (DiT) that performs low-level motor action generation.\nThe VLM encodes RGB images into visual tokens and tokenizes language instructions into text tokens, providing\nsemantic grounding of the task objective and object relationships. In parallel, the robotâ€™s proprioceptive state is\nembedded into state tokens through embodiment-specific encoders. All tokens are then passed to the DiT module,\nwhich applies the diffusion-based action generation described in Sec.Â II-A using transformer-based denoising\nrather than a UNet. This enables improved scalability, conditioning, and data efficiency.\nA central design goal of GR00T is embodiment generality. State and action projection layers embed each robotâ€™s\nobservation and action space into a unified token dimension, allowing the same diffusion policy to be fine-tuned\nacross heterogeneous embodiments such as the Fourier GR1 or the Unitree G1. This provides a strong foundation\nfor building manipulation policies, while still allowing additional modalities such as depth or contact to be\nincorporated during fine-tuning.\nIn this work, GR00T serves as the base policy for modality augmentation. By enriching the GR1 dataset with\npost-processed depth and contact estimates, and by constructing a fully multi-modal G1 dataset that includes\ncontact force measurements, we investigate how GR00Tâ€™s diffusion-based architecture adapts to richer sensory\ninputs and transfers across embodiments.\nFigure 1:\nOverview of the GR00T dual-system architecture. SystemÂ 2 (VLM) processes image observations\nand language instructions into semantic tokens, while SystemÂ 1 (Diffusion Transformer)\ngenerates motor actions via iterative denoising conditioned on multimodal tokens.\nIII\nProblem Formulation\nIII-A\nTask Definition\nWe consider two robot embodiments: the Fourier GR1 and the Unitree G1.\nFor the GR1 platform, we use three manipulation tasks from the publicly available GR1 dataset on HuggingFace:\n(1)\nPick-and-Place Can to Drawer Close\n,\n(2)\nPick-and-Place Potato to Microwave Close\n, and\n(3)\nPick-and-Place Wine to Cabinet Close\n.\nEach task requires long-horizon manipulation with object interaction, container insertion, and precise end-effector control.\nFor the G1 embodiment, no public dataset exists; therefore, we focus on a single custom-defined task,\nPick Apple to Bowl\n, which requires stable grasp acquisition, object transport, and placement.\nThis task allows us to evaluate embodiment adaptation under newly collected multi-modal data.\nIII-B\nMulti-Embodiment Policy Adaptation\nAdapting a single foundation policy to both GR1 and G1 introduces two primary challenges.\nObservation-space mismatch.\nThe GR1 dataset provides RGB images and proprioceptive states, but lacks\ndepth, contact indicators, or force feedback. In contrast, the G1 dataset we\nconstruct includes RGB-D observations, proprioception, and ground-truth\ncontact forces. As a result, the policy must operate under heterogeneous and\nembodiment-specific sensory modalities across the two platforms.\nAction-space mismatch.\nBeyond sensory differences, the two robots differ significantly in embodiment.\nThe GR1 platform uses a Fourier hand with 6 DoF, paired with a 7-DoF arm\n(13 DoF per arm, 26 DoF for both upper limbs), whereas the G1 platform employs\na 7-DoF dexterous hand combined with a 7-DoF arm (14 DoF per arm, 28 DoF total).\nThese differences in hand kinematics, finger actuation, and joint coordination\nintroduce a non-trivial embodiment gap, raising the question of whether a\nsingle diffusion-based policy can transfer effectively across robots whose\ngripper morphology and manipulability differ substantially.\nIII-C\nObjective\nOur goal is to enable a single diffusion-based policy to operate robustly across\ntwo robot embodiments with heterogeneous sensing and action modalities.\nTo this end, we consider two complementary datasets:\n(i) an RGB-only GR1 dataset that we augment with estimated depth maps and\nbinary contact indicators, and\n(ii) a custom multi-modal G1 dataset that includes RGB-D observations,\nproprioception, cuRobo-generated reference trajectories, and ground-truth\ncontact-force measurements.\nThe objective is to fine-tune a unified policy that can:\nâ€¢\nperform stable and contact-aware manipulation by leveraging richer sensory modalities, and\nâ€¢\ntransfer effectively across robots with different embodiments, hand structures, and action spaces (GR1\nâ†’\n\\rightarrow\nG1).\nMore formally, we aim to learn a policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nthat generates action sequences\nvia a diffusion denoising process while maintaining consistency and performance under\nvariations in embodiment, sensing modality, and task specifications.\nThe policy should generalize across both datasets and exploit modality augmentation\nto improve robustness in contact-rich manipulation scenarios.\nIV\nProposed Method\nIV-A\nG1 Dataset Creation\nFigure 2:\nVisualization of the G1 data collection environment.\nLeft: front view of the scene setup.\nRight: ego-centric view from the robotâ€™s onboard camera during demonstration execution.\nUnlike the GR1 platform, no public dataset exists for the Unitree G1. Moreover, the G1 embodiment\ndiffers substantially in hand morphology (7-DoF dexterous hand) and upper-limb kinematics (7-DoF arm),\nmaking direct transfer from GR1 challenging. To enable modality-augmented fine-tuning and embodiment\nevaluation, we construct a high-quality, multi-modal demonstration dataset for the\nPick Apple to Bowl\ntask using a motion-planningâ€“driven pipeline.\nTrajectory generation via cuRobo.\nWe leverage cuRoboâ€™s GPU-accelerated motion planner to generate smooth and collision-free\nend-effector trajectories. For each demonstration, cuRobo computes dynamically feasible Cartesian\npaths that satisfy grasping and placement constraints. These trajectories are converted to G1\njoint-space commands using an analytical inverse kinematics (IK) solver, producing reference\ntrajectories with high temporal consistency.\nProprioception and joint-action recording.\nDuring trajectory rollout, we record the full proprioceptive state, including joint positions, joint\nvelocities, and end-effector poses. We also log the executed joint actions generated by the\ncontroller at each timestep. Fig.\n3\nshows an example of the recorded\nstateâ€“action alignment across the 20 joints of the G1 upper-limb chain, demonstrating that the\ngenerated trajectories exhibit smooth and physically valid motion profiles.\nFigure 3:\nState vs.Â Action per G1 joint.\nFor each joint, we visualize the proprioceptive state\n(blue) and the executed action command (red) over time. The trajectories generated through\ncuRobo-based planning exhibit smooth, consistent evolution across the 20 DoF upper-limb chain,\nensuring high-quality supervision for diffusion policy fine-tuning.\nGround-truth contact force measurement.\nA key feature of the G1 dataset is the availability of accurate contact-force feedback\nfrom the dexterous hand. For each frame, we extract fingertip and palm contact forces\nusing the G1 physics engine. These signals allow the policy to learn fine-grained\ninteraction cues such as grasp stability, slip, and object contact boundaries.\nFig.\n4\nillustrates the per-finger contact forces during a\nrepresentative demonstration. The left thumb, left middle finger, and palm exhibit\nstrong, temporally structured contact patterns during grasping, while other fingers\nremain inactiveâ€”highlighting the asymmetric force distribution characteristic of\ndexterous manipulation.\nFigure 4:\nContact forces per finger over time.\nThe G1 dexterous hand provides\nhigh-fidelity fingertip and palm force measurements. During grasp execution,\ncontact is concentrated on specific fingers (left thumb, middle, and palm),\nwhile others remain inactive. These rich interaction signals are used to train\ncontact-aware diffusion policies.\nMulti-modal observation structure.\nEach demonstration in the G1 dataset includes:\nâ€¢\nRGB-D observations\nfrom the onboard camera,\nâ€¢\nfull proprioception\n(joint positions, velocities, end-effector pose),\nâ€¢\ncuRobo-generated joint-space reference trajectories\n,\nâ€¢\nfingertip and palm contact-force vectors\n.\nThis multi-modal dataset provides the dense physical and perceptual cues necessary\nfor training policies capable of contact-rich manipulation and for evaluating\ncross-embodiment transfer from GR1 to G1.\nIV-B\nContact Augmentation for GR1 Dataset\nThe GR1 dataset does not contain any contact information, preventing the policy from identifying\ngrasp events, slip boundaries, or objectâ€“robot interaction timing. To compensate for this modality\ngap, we introduce two complementary forms of contact augmentation:\n(1) fusing collision-derived contact signals directly into the robotâ€™s proprioceptive state, and\n(2) treating contact as a distinct modality through a dedicated contact encoder.\nIV-B1\nFusing Contact into Proprioceptive State\nWe first adopt a lightweight strategy that augments the GR1 proprioceptive state vector with a\nbinary contact indicator. For each timestep, we detect collisions between the end-effector and the\nmanipulated object in simulation and assign a 1-bit contact value\nc\nt\nâˆˆ\n{\n0\n,\n1\n}\nc_{t}\\in\\{0,1\\}\n, which is\nconcatenated to the original state vector:\ns\n~\nt\n=\n[\ns\nt\njoints\n,\nc\nt\n]\n.\n\\tilde{s}_{t}=[\\,s^{\\text{joints}}_{t},\\;c_{t}\\,].\nThis minimally increases the input dimension of the state encoder while allowing the diffusion\npolicy to reason about object interaction boundaries. The augmented state is passed through the\nembodiment-specific state encoder to obtain a contact-aware embedding:\nz\nt\n=\nStateEnc\nâ€‹\n(\ns\n~\nt\n)\n.\nz_{t}=\\text{StateEnc}(\\tilde{s}_{t}).\nDuring denoising, this embedding conditions the DiT backbone via cross-attention, directly\ninfluencing noise prediction:\nx\nt\nâˆ’\n1\n=\n1\nÎ±\nt\nâ€‹\n(\nx\nt\nâˆ’\nÎ²\nt\nâ€‹\nÏµ\nÎ¸\nâ€‹\n(\nx\nt\n,\nz\nt\n,\nt\n)\n)\n+\nÏƒ\nt\nâ€‹\nz\n,\nx_{t-1}=\\frac{1}{\\sqrt{\\alpha_{t}}}\\Big(x_{t}-\\beta_{t}\\,\\epsilon_{\\theta}(x_{t},z_{t},t)\\Big)+\\sigma_{t}z,\nwhere\nx\nt\nx_{t}\nis the noisy action,\nÏµ\nÎ¸\n\\epsilon_{\\theta}\nis the DiT denoiser, and\nt\nt\nis the diffusion step.\nThis simple augmentation provides a strong supervisory signal:\ncontact-aware embeddings help the policy stabilize grasping actions, prevent premature release,\nand improve the consistency of object transport behaviors.\nIV-B2\nDedicated Contact Encoder Module\nTo treat contact as an independent sensory modality rather than a scalar feature embedded into\nthe proprioceptive state, we introduce a dedicated contact encoder module (Fig. 5).\nHere, the contact signal\nc\nt\nc_{t}\nis encoded through a learnable MLP before entering the diffusion transformer,\nallowing the model to represent interaction dynamics at a higher level of abstraction.\nFormally, the contact embedding is obtained as:\nz\nt\ncontact\n=\nContactEnc\nâ€‹\n(\nc\nt\n)\n,\nz^{\\text{contact}}_{t}=\\text{ContactEnc}(c_{t}),\nwhich captures temporal and semantic structure in the contact sequence.\nThe full conditioning set for each DiT block becomes:\nZ\nt\n=\n[\nz\nt\nvision\n,\nz\nt\ntext\n,\nz\nt\nstate\n,\nz\nt\ncontact\n]\n,\nZ_{t}=\\left[z^{\\text{vision}}_{t},\\;z^{\\text{text}}_{t},\\;z^{\\text{state}}_{t},\\;z^{\\text{contact}}_{t}\\right],\nensuring that contact is treated as a first-class modality, equal in importance to vision and proprioception.\nBy injecting contact as an independent token stream, the denoiser\nÏµ\nÎ¸\n\\epsilon_{\\theta}\ngains explicit access\nto tactile cues during each diffusion step. This enables the model to differentiate between weak vs.Â strong\ngrasp phases, detect pre-contact vs.Â post-contact transitions, and learn more expressive representations\nfor contact-rich manipulation.\nFigure 5:\nDedicated Contact Encoder Module.\nInstead of concatenating contact into the proprioceptive state,\nthe binary (or continuous) contact signal\nc\nt\nc_{t}\nis processed by a learnable\nContact Encoder. The resulting embedding is input to the DiT blocks as a separate\nmodality, alongside vision, language, and state tokens. This design treats contact\nas an independent modality and allows the policy to learn richer interaction-aware\nrepresentations.\nIV-C\nDepth Augmentation (RGB-D Fusion)\nThe GR1 dataset contains only RGB observations, which restricts the policyâ€™s ability to reason\nabout 3D geometry, object boundaries, and occlusions. To overcome this limitation, we generate\nmetric depth maps for all GR1 demonstrations using ZoeDepth, a state-of-the-art monocular depth\nestimator capable of producing scale-consistent depth predictions. The resulting RGB-D frames\nequip the policy with explicit geometric cues that are essential for contact-rich manipulation.\nFigure 6:\nRGB and Estimated Depth for GR1 Demonstrations.\nLeft: original RGB observation from the GR1 simulator.\nRight: corresponding metric depth map computed by ZoeDepth.\nDepth enhances geometric reasoning by capturing spatial structure such as object shape,\ntable height, and cabinet geometry.\nExtending the Vision Encoder for RGB-D Input.\nGR00Tâ€™s Eagle-2 ViT processes\n16\nÃ—\n16\n16\\times 16\nRGB patches using a linear projection layer.\nTo incorporate depth, we expand the patch embedding from 3 to 4 channels. For each\n16\nÃ—\n16\n16\\times 16\npatch\nP\nt\nP_{t}\nfrom the RGB-D input\nI\nt\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n4\nI_{t}\\in\\mathbb{R}^{H\\times W\\times 4}\n:\ne\nt\n=\nW\npatch\nâ‹…\nFlatten\nâ€‹\n(\nP\nt\n)\n+\nb\npatch\n,\ne_{t}=W_{\\text{patch}}\\cdot\\text{Flatten}(P_{t})+b_{\\text{patch}},\nwith\nW\npatch\nâˆˆ\nâ„\nd\nÃ—\n(\n16\nÃ—\n16\nÃ—\n4\n)\n.\nW_{\\text{patch}}\\in\\mathbb{R}^{d\\times(16\\times 16\\times 4)}.\nTo stabilize early training, the depth-channel weights are initialized using RGB kernel averaging:\nW\npatch\n(\nD\n)\n=\n1\n3\nâ€‹\n(\nW\npatch\n(\nR\n)\n+\nW\npatch\n(\nG\n)\n+\nW\npatch\n(\nB\n)\n)\n.\nW_{\\text{patch}}^{(D)}=\\frac{1}{3}\\left(W_{\\text{patch}}^{(R)}+W_{\\text{patch}}^{(G)}+W_{\\text{patch}}^{(B)}\\right).\nRGB-D Token Fusion.\nThe depth map is concatenated with the RGB channels prior to the Vision Encoder,\nforming an RGB-D input that undergoes early fusion through the expanded patch\nembedding layer. Once embedded, the RGB-D patches pass through the standard ViT encoder without architectural\nmodification. The resulting tokens\n{\nz\nt\nimg\n}\n\\{z^{\\text{img}}_{t}\\}\nencode both appearance and geometric\nstructure, enabling the diffusion policy to reason more effectively about depth discontinuities,\ngrasp approach angles, and occluded object surfaces.\nFigure 7:\nExample simulation environments used for online evaluation.\nEach rollout randomizes object placement, textures, lighting, and robot\nconfiguration to assess robustness.\nV\nExperiments\nV-A\nExperimental Settings\nHardware and Environment.\nAll experiments are conducted on a single NVIDIA A6000 GPU\nusing the official GR00T fine-tuning framework.\nOnline evaluation is performed in a robosuite and robocasa based simulator\nvia a serverâ€“client inference pipeline, while offline evaluation\nuses trajectory reconstruction MSE computed from the dataset.\nTraining Configuration.\nWe fine-tune the GR00T-N1.5-3B base model with the vision tower\nand VLM backbone frozen. Only the projector and embodiment-specific\nstate/action encoders are updated.\nTraining uses the AdamW optimizer with a learning rate of\n1\nâ€‹\ne\nâˆ’\n4\n1\\mathrm{e}{-4}\n,\nweight decay of\n1\nâ€‹\ne\nâˆ’\n5\n1\\mathrm{e}{-5}\n, cosine LR schedule with 5% warmup,\nand batch size 32. Models are trained for 20k steps using\nbf16\nprecision.\nCheckpoints are saved every 5,000 steps.\nDiffusion and Policy Head.\nAction generation uses GR00Tâ€™s diffusion transformer with\n12 layers, 8-head cross-attention, hidden size 1024, and GELU activation.\nFlow-matching training employs a Beta noise distribution\n(\nÎ±\n=\n1.5\n\\alpha{=}1.5\n,\nÎ²\n=\n1.0\n\\beta{=}1.0\n) with 1,000 timestep discretization.\nV-B\nEvaluation Protocol\nWe evaluate all models using two complementary procedures:\n(1) offline trajectory reconstruction and\n(2) online policy rollout in simulation.\nThis dual protocol allows us to measure both the accuracy of the\ndiffusion modelâ€™s action prediction and its real-time manipulation capability.\nOffline Evaluation (Trajectory MSE).\nGiven an observation sequence\nğ¨\nt\n\\mathbf{o}_{t}\nfrom the dataset,\nthe diffusion policy predicts a 16-step action sequence\nğš\n^\nt\n:\nt\n+\nH\n\\hat{\\mathbf{a}}_{t:t+H}\nthrough iterative denoising.\nWe compute the mean-squared error (MSE) between the predicted\nand ground-truth actions:\nMSE\n=\n1\nH\nâ€‹\nâˆ‘\nk\n=\n0\nH\nâˆ’\n1\nâ€–\nğš\n^\nt\n+\nk\nâˆ’\nğš\nt\n+\nk\nâ€–\n2\n2\n.\n\\text{MSE}=\\frac{1}{H}\\sum_{k=0}^{H-1}\\left\\|\\hat{\\mathbf{a}}_{t+k}-\\mathbf{a}_{t+k}\\right\\|_{2}^{2}.\nThis metric quantifies the policyâ€™s ability to reconstruct expert trajectories\nand reflects the quality of multimodal conditioning (contact, depth, proprioception).\nOffline evaluation is performed on held-out validation data from both GR1 and G1 datasets.\nOnline Evaluation (Success Rate).\nTo evaluate real-time manipulation performance, we deploy the policy in a simulator\nstack composed of\nrobosuite\nand\nrobocasa\n.\nrobosuite\nprovides the manipulation task environments, while\nrobocasa\nsupplies object assets, scene setups, and interaction\nconfigurations used for household and pick-and-place scenarios.We adopt a serverâ€“client inference loop in which, at each timestep, the client\ncaptures the current RGB (or RGB-D) observations and proprioceptive states,\npackages them into a policy query, and sends the request to the inference server.\nThe server processes this input through the GR00T policy and returns the predicted\naction, which is immediately applied to the simulator.\nThe success rate is computed as:\nSuccess Rate\n=\n# successful rollouts\n# total rollouts\n.\n\\text{Success Rate}=\\frac{\\text{\\# successful rollouts}}{\\text{\\# total rollouts}}.\nFor each task, we perform 20 randomized rollouts with varying initial\nobject positions and robot configurations to evaluate robustness.\nComplementary Nature of the Metrics.\nOffline MSE captures the modelâ€™s ability to match expert demonstrations\nbut does not fully reflect closed-loop stability.\nOnline evaluation measures actual manipulation success under\nembodiment-specific dynamics, noisy perceptual conditions,\nand contact-rich interactions.\nTogether, these metrics provide a comprehensive assessment of\npolicy quality across GR1 and G1 embodiments.\nV-C\nGR1 Experiments: Modality Augmentation Analysis\nWe conduct a modality-wise ablation study on the publicly released GR1 dataset\nto evaluate the effect of contact and depth augmentation on both offline trajectory\nreconstruction and online manipulation performance.\nV-C1\nOnline Evaluation (Success Rate)\nTable 1 summarizes the online success rates\nmeasured through simulation rollouts across three GR1 tasks.\nContact-state augmentation provides modest gains, while depth augmentation\nsignificantly improves success under visually ambiguous scenarios.\nThe full model incorporating both depth and contact achieves the highest\nsuccess rate across all evaluated tasks.\nFigure 8:\nComparison between ground-truth actions, predicted actions, and executed inference points\nacross the first three action dimensions. The model closely tracks the demonstration trajectory,\nindicating stable diffusion-based denoising and accurate action reconstruction.\nV-C2\nOffline Evaluation (MSE)\nTable 2 reports the mean-squared error (MSE) between the predicted and ground-truth action trajectories for the\nPnP Can to Drawer Close\ntask. Among all configurations, depth augmentation provides the most substantial improvement in offline reconstruction quality, indicating that geometric cues significantly reduce ambiguity in action prediction. Furthermore, combining both depth and contact information yields the lowest overall error, demonstrating the complementary benefit of integrating multi-modal signals into the diffusion denoising process.\nV-D\nG1 Experiments: Embodiment Transfer with Custom Multi-Modal Data\nTo assess embodiment transfer and quantify the impact of contact-rich demonstrations on the Unitree G1,\nwe perform a modality ablation study on a customized\nPick Apple to Bowl\ndataset generated using\ncuRobo-based motion planning and ground-truth force measurements.\nCompared to the GR1 platform, the G1 provides explicit 6-axis force sensing and a 7-DoF dexterous hand,\noffering a more informative testbed for analyzing how contact and geometric modalities contribute to\nfine-grained manipulation.\nV-D1\nOnline Evaluation (Success Rate)\nModel Configuration\nSuccess Rate (%)\nMSE\nGR00T N1.5 (Zero-shot)\n0.0%\n0.35719\n0.35719\nGR00T N1.5 (Finetuning)\n48.0%\n0.031716\n0.031716\nOurs (+ Contact Encoder)\n74.0%\n0.024407\n0.024407\nOurs (+ Contact State)\n94.0%\n0.018623\n0.018623\nOurs (+ Depth)\n82.0%\n0.022596\n0.022596\nTABLE I:\nAblation study on multi-modal augmentation for embodiment transfer on Unitree G1.\nTable 1. reports the online evaluation results across all\nmodality configurations. Zero-shot GR00T N1.5 fails entirely on the G1, underscoring the\nsubstantial embodiment gap between GR1 and G1 in hand morphology, sensor topology, and\ncontact dynamics. Finetuning with 5K embodiment-aligned demonstrations improves performance\nto 48%, demonstrating that platform-specific data is essential for stable closed-loop control.\nIncorporating contact-force measurements yields significant additional gains. Modeling force\nsignals as an auxiliary modality increases success to 74%, indicating that contact cues\nprovide valuable information for regulating grasp pressure and mitigating slip during the\ntransport phase. The best performance (94%) arises when force signals are fused directly\ninto the proprioceptive state representation, suggesting that early fusion produces richer\nlatent dynamics and stronger conditioning for the diffusion-based denoising process.\nDepth augmentation also contributes substantial improvements. Integrating depth features\nachieves an 82% success rate, showing that 3D geometry aids in resolving self-occlusion,\ndisambiguating the appleâ€“bowl interaction region, and stabilizing approach trajectories.\nHowever, unlike the GR1â€”where depth was the dominant performance factor due to limited\ncontact sensingâ€”the G1 benefits more from force-aware modalities. This indicates that\ndepth alone cannot capture the fine-scale local interactions required for dexterous\ncontact-rich manipulation.\nCross-Embodiment Insight.\nOverall, the results highlight that optimal modality design is inherently embodiment-specific.\nGR1 derives most of its improvement from geometric augmentation, whereas G1 relies primarily on\ncontact-rich supervision to compensate for its higher manipulation dexterity and increased\ninteraction complexity. These findings underscore that successful transfer of foundation\npolicies to new platforms requires aligning sensory modalities with the target robotâ€™s\nkinematic structure and physical interaction characteristics.\nVI\nConclusion and Future Work\nThis work presented a modality-augmented fine-tuning framework for adapting\nfoundation robot policies to heterogeneous embodiments. Using the GR1 dataset,\nwe showed that depth augmentation substantially improves both offline trajectory\nreconstruction and online manipulation success, while contact-state integration\nprovides complementary gains for contact-rich tasks. To address the limitations of\nRGB-only public datasets, we introduced a customized multi-modal dataset for the\nUnitreeÂ G1, generated using cuRobo-based motion planning and ground-truth force\nmeasurements. Experiments demonstrate that contact-force supervision is essential\nfor reliable control on the G1 platform: while zero-shot GR00T fails due to embodiment\nmismatch, incorporating contact information yields a dramatic performance increase,\nachieving up to 94% success in the\nPick Apple to Bowl\ntask.\nOur cross-embodiment analysis highlights that optimal modality selection depends\nstrongly on robot morphology and sensing capabilities: depth is most beneficial for\nthe GR1â€™s visually ambiguous manipulation scenes, whereas explicit contact-force\nfeedback is crucial for the G1â€™s dexterous hand and grasp-centric behaviors. These\nfindings underscore the importance of tailoring modality design and dataset construction\nto the target embodiment when deploying foundation policies across platforms.\nFuture Work.\nThere are several promising directions for extending this study.\nFirst, integrating real-world sensory streamsâ€”such as tactile arrays, joint torque\nmeasurements, and stereo depthâ€”would enable evaluation beyond simulation and support\nrobust sim-to-real deployment. Second, jointly training the vision, language, and diffusion\nmodules, rather than relying on frozen VLM backbones, may lead to stronger multi-modal\nfusion and improved generalization. Third, expanding the G1 dataset to additional task\nfamilies and more diverse environments would facilitate large-scale multi-embodiment\nbenchmarking. Fourth, exploring cross-robot policy distillation or embodiment-conditioned\nlatent representations may further improve transfer between morphologically distinct robots.\nAnother promising direction is the development of a\ncontact-adaptive denoising schedule\nfor diffusion policies. Prior to establishing contact, the policy could employ a larger number\nof denoising iterations with higher noise levelsÂ (\nÏƒ\nt\n\\sigma_{t}\n) to encourage broader action\nexploration, improve wrist alignment, and mitigate approach-phase miscalibration. After\ncontact is detected, the algorithm could transition to fewer iterations with lower\nÏƒ\nt\n\\sigma_{t}\n,\nyielding a contact-conditioned\nÎ²\nt\n/\nÏƒ\nt\n\\beta_{t}/\\sigma_{t}\nschedule that stabilizes grasp execution\nand reduces action variance during transport. Such adaptive schedules may significantly improve\nclosed-loop robustness in contact-rich manipulation.\nOverall, our results demonstrate that modality-aware fine-tuning, combined with carefully\nconstructed multi-modal datasets, provides an effective pathway for adapting foundation\nrobot policies to new embodiments and unlocking robust manipulation capabilities across\nplatforms.\nReferences\n[1]\nJ. Ho, A. Jain, and P. Abbeel,\nâ€œDenoising diffusion probabilistic models,â€\nin\nAdvances in Neural Information Processing Systems\n, 2020.\n[2]\nJ. Song, C. Meng, and S. Ermon,\nâ€œDenoising diffusion implicit models,â€\nin\nInternational Conference on Learning Representations\n, 2021.\n[3]\nW. Peebles and S. Xie,\nâ€œScalable diffusion models with transformers,â€\nin\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, 2023.\n[4]\nY. Lipman et al.,\nâ€œFlow matching for generative modeling,â€\nin\nInternational Conference on Learning Representations\n, 2023.\n[5]\nJ.-B. Alayrac et al.,\nâ€œFlamingo: A visual language model for few-shot learning,â€\nin\nAdvances in Neural Information Processing Systems\n, 2022.\n[6]\nA. Brohan et al.,\nâ€œRT-2: Vision-language-action models transfer web knowledge to robotic control,â€\nGoogle DeepMind technical report, 2023.\n[7]\nP. Jain et al.,\nâ€œOpenVLA: An open vision-language-action model for generalist robots,â€\nTechnical report, 2024.\n[8]\nT. Kollar et al.,\nâ€œOcto: An embodied foundation model for robotic manipulation,â€\nTechnical report, 2024.\n[9]\nNVIDIA,\nâ€œGR00T N1: An open foundation model for generalist humanoid robots,â€\nNVIDIA Research report, 2025.\n[10]\nY. Xia et al.,\nâ€œPerAct: Perception-and-action transformers for robotic manipulation,â€\nin\nConference on Robot Learning\n, 2022.\n[11]\nT. Chi et al.,\nâ€œDiffusion policy: Visuomotor policy learning via action diffusion,â€\nin\nRobotics: Science and Systems\n, 2023.\n[12]\nH. Jang et al.,\nâ€œDepth-aware policy learning for robot manipulation,â€\nin\nIEEE International Conference on Robotics and Automation\n, 2024.\n[13]\nR. Calandra et al.,\nâ€œDeep tactile representation learning,â€\nInternational Journal of Robotics Research\n, 2021.\n[14]\nA. Church et al.,\nâ€œLearning contact-rich manipulation with differentiable physics,â€\nin\nRobotics: Science and Systems\n, 2022.\n[15]\nA. Z. Wang et al.,\nâ€œrobosuite: A modular simulation framework for robot learning,â€\nin\nIEEE International Conference on Robotics and Automation\n, 2020.\n[16]\nM. Li et al.,\nâ€œRoboCasa: Large-scale household simulation for robotic manipulation,â€\nMeta AI technical report, 2023.\n[17]\nP. Sundaram et al.,\nâ€œcuRobo: Parallelized motion generation for complex robotic systems,â€\nNVIDIA Research report, 2023.\n[18]\nD. Berenson et al.,\nâ€œLearning neural inverse kinematics solvers for articulated robots,â€\nin\nIEEE International Conference on Robotics and Automation\n, 2022.\n[19]\nS. Bhat et al.,\nâ€œZoeDepth: Zero-shot metric monocular depth estimation,â€\nin\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2023.\n[20]\nJ. T. Wu et al.,\nâ€œEgoSchema: Scaling robot learning with egocentric multimodal data,â€\nTechnical report, 2024.",
    "preview_text": "This paper presents a modality-augmented fine-tuning framework designed to adapt foundation robot policies to diverse humanoid embodiments. We validate our approach across two distinct settings: (i) the GR1 embodiment, utilizing public datasets where we introduce post-processed modalities, including binary contact signals and ZoeDepth-generated metric depth; and (ii) the Unitree G1 embodiment, for which we contribute a novel multi-modal dataset incorporating cuRobo motion planning, inverse kinematics, and ground-truth contact-force measurements. Our experiments demonstrate that modality augmentation consistently enhances policy performance across different embodiments. Specifically, for the GR1, integrating contact-state cues and RGB-D fusion improves online success rates from 51% to 63%. Furthermore, in the G1 \"Pick Apple to Bowl\" task, our contact-augmented model achieves a success rate of 94%, significantly outperforming the 48% achieved by standard fine-tuning and the 0% baseline of zero-shot transfer. These results highlight that lightweight post-processing effectively strengthens policies for GR1, while high-quality multi-modal data is crucial for reliable transfer to the Unitree G1. Consequently, this work establishes a unified, data-centric pathway for extending foundation robot policies through targeted modality design and multi-modal fine-tuning.\n\nModality-Augmented Fine-Tuning of Foundation Robot Policies for Cross-Embodiment Manipulation on GR1 and G1\nJunsung Park\n1\n, Hogun Kee\n1\n, and Songhwai Oh\n1\n1\nDepartment of Electrical and Computer Engineering, Seoul National University\nAbstract\nWe present a modality-augmented fine-tuning framework for adapting foundation robot policies to diverse embodiments. Our study evaluates two settings: (i) the GR1 embodiment using the public HuggingFace dataset, where we introduce post-processed modalities including binary contact signals and ZoeDepth-generated metric depth; and (ii) the Unitree G1 embodiment, for which we",
    "is_relevant": null,
    "relevance_score": 0.0,
    "extracted_keywords": [],
    "one_line_summary": "",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T07:13:38Z",
    "created_at": "2026-01-08T10:08:07.851198",
    "updated_at": "2026-01-08T10:08:07.851208",
    "flag": true
}