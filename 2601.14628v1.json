{
    "id": "2601.14628v1",
    "title": "A Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control",
    "authors": [
        "Weiyu Guo",
        "He Zhang",
        "Pengteng Li",
        "Tiefu Cai",
        "Ziyang Chen",
        "Yandong Guo",
        "Xiao He",
        "Yongkui Yang",
        "Ying Sun",
        "Hui Xiong"
    ],
    "abstract": "å…·èº«æ™ºèƒ½é¢†åŸŸçš„æœ€æ–°è¿›å±•é€šè¿‡å¤§è§„æ¨¡æ•°æ®å’Œæ¨¡å‹å‚æ•°çš„æ‰©å±•ï¼Œå·²èƒ½å®ç°è‡ªç„¶è¯­è¨€æŒ‡ä»¤è·Ÿéšä¸å¤šä»»åŠ¡æ§åˆ¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”Ÿç‰©ç³»ç»Ÿå±•ç°å‡ºä»ç¨€ç–ç»éªŒä¸­å¿«é€Ÿä¹ å¾—æŠ€èƒ½çš„å…ˆå¤©èƒ½åŠ›ã€‚å…³é”®åœ¨äºï¼Œå½“å‰æœºå™¨äººç­–ç•¥éš¾ä»¥å¤ç°ç”Ÿç‰©è¿åŠ¨å›ºæœ‰çš„åŠ¨æ€ç¨³å®šæ€§ã€åå°„æ€§å“åº”èƒ½åŠ›å’Œæ—¶åºè®°å¿†ç‰¹æ€§ã€‚æœ¬æ–‡æå‡ºç¥ç»å½¢æ€è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ¨¡æ‹Ÿäº†ç”Ÿç‰©ç¥ç»ç³»ç»Ÿä¸­çš®å±‚ã€å°è„‘ä¸è„Šé«“é—´çš„ç»“æ„ç»„ç»‡ã€‚æˆ‘ä»¬é‡‡ç”¨ç³»ç»Ÿçº§ä»¿ç”Ÿè®¾è®¡ï¼šé«˜å±‚æ¨¡å‹è´Ÿè´£ç›®æ ‡è§„åˆ’ï¼Œè‡ªé€‚åº”å°è„‘æ¨¡å—é€šè¿‡é«˜é¢‘ä¼ æ„Ÿå™¨åé¦ˆå®ç°è¿åŠ¨ç¨³å®šï¼Œä»¿ç”Ÿè„Šé«“å±‚æ‰§è¡Œé—ªç”µå¼åŠ¨ä½œç”Ÿæˆã€‚è¯¥æ¡†æ¶é¦–æ¬¡å®ç°äº†ç¥ç»å½¢æ€è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å®ä½“æœºå™¨äººä¸Šçš„éƒ¨ç½²ï¼Œå¹¶è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°è¯¥ç³»ç»Ÿæ— éœ€é¢å¤–æ•°æ®æˆ–ç‰¹æ®ŠæŒ‡å¯¼å³å¯æ¶Œç°ç”Ÿç‰©è¿åŠ¨ç‰¹æ€§ï¼šæ¶ˆé™¤æœºæ¢°è‡‚æŠ–åŠ¨ç°è±¡ï¼Œæ˜¾è‘—é™ä½èƒ½è€—ï¼ˆç¥ç»å½¢æ€å¤„ç†å™¨ä»…éœ€0.4ç“¦ï¼‰ï¼Œå±•ç°æ—¶åºè®°å¿†èƒ½åŠ›ï¼Œå¹¶åœ¨20æ¯«ç§’å†…è§¦å‘å®‰å…¨åå°„æœºåˆ¶ã€‚",
    "url": "https://arxiv.org/abs/2601.14628v1",
    "html_url": "https://arxiv.org/html/2601.14628v1",
    "html_content": "A Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control\nWeiyu Guo\nThe Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\nAI\n2\nRobotics, Shenzhen, China\nHe Zhang\nThe Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\nAI\n2\nRobotics, Shenzhen, China\nPengteng Li\nThe Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\nAI\n2\nRobotics, Shenzhen, China\nTiefu Cai\nThe Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\nAI\n2\nRobotics, Shenzhen, China\nZiyang Chen\nThe Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\nAI\n2\nRobotics, Shenzhen, China\nYandong Guo\nThe Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\nAI\n2\nRobotics, Shenzhen, China\nHe Xiao\nAI\n2\nRobotics, Shenzhen, China\nYongkui Yang\nYing Sun\nThe Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\nHui Xiong\nThe Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\nAbstract\nRecent advances in embodied intelligence have leveraged massive scaling of data and model parameters to master natural-language command following and multi-task control. In contrast, biological systems demonstrate an innate ability to acquire skills rapidly from sparse experience. Crucially, current robotic policies struggle to replicate the dynamic stability, reflexive responsiveness, and temporal memory inherent in biological motion. Here we present Neuromorphic Vision-Language-Action (NeuroVLA), a framework that mimics the structural organization of the bio-nervous system between the cortex, cerebellum, and spinal cord. We adopt a system-level bio-inspired design: a high-level model plans goals, an adaptive cerebellum module stabilizes motion using high-frequency sensors feedback, and a bio-inspired spinal layer executes lightning-fast actions generation. NeuroVLA represents the first deployment of a neuromorphic VLA on physical robotics, achieving state-of-the-art performance. We observe the emergence of biological motor characteristics without additional data or special guidance: it stops the shaking in robotic arms, saves significant energy(only 0.4w on Neuromorphic Processor), shows temporal memory ability and triggers safety reflexes in less than 20 milliseconds.\n1\nIntroduction\nThe pursuit of general-purpose embodied intelligence has been accelerated by Vision-Language-Action (VLA) models, which unify perception, reasoning, and actuation to master complex tasks\n[\n1\n,\n2\n]\n. However, current architectures remain far from matching the adaptive motor intelligence inherent in humans and animals. First, restricted to current-step visual observations, these architectures suffer from temporal blindness, failing to perceive execution progress in repetitive tasks. Second, their inability to process high-frequency signals and lack of proprioceptive feedback result in severe action jitter and failure to reflex instantaneously in dynamic scenarios. Ultimately, invoking a large foundation model for every fine-grained motor adjustment incurs prohibitive power consumption and extreme computational inefficiency.\nTo resolve this bottleneck, we look to biological motor intelligence, which achieves robustness not through a monolithic processor, but via a hierarchical division of labor. Contemporary theories of optimal feedback control posit that biological agility relies on a distinct hierarchical architecture of sensorimotor loops to overcome the intrinsic latencies of neural transmission\n[\n3\n,\n4\n]\n. Rather than centralized micromanagement, the nervous system enforces a strict functional partitioning: (1) Cortical regions integrate multisensory streams to generate semantic goals and high-level plans; (2) the cerebellum, receiving dense sensorimotor inputs, acts as a high-frequency adaptive module that predicts sensory consequences, refines motor commands, and contributes critically to temporal memory and error correction\n[\n5\n,\n6\n]\n; and (3) the spinal cord comprises interneuronal networks and reflex pathways that realize fast, decentralized sensorimotor loops\n[\n7\n]\n. This architectural separation decouples semantic planning from high-frequency proprioceptive modulation and execution and underpins smooth trajectories, rapid protective responses, and energy-efficient behavior, supporting the lifelong plasticity of low-level motor circuits\n[\n8\n]\n.\nWe validate this architecture across simulated benchmarks and physical robotic hardware, demonstrating capabilities that are difficult to replicate via existing tokenized sequence-modeling paradigms (e.g., VLAs). Experimental results indicated that functions akin to biological motor intelligence emerged in the model, even in the absence of pre-training data or specific supervision. First, the cerebellar module functions as a critical adaptive damper: by acting as an effective filter, it suppresses high-frequency intention tremor to reduce kinematic jerk by over 75% and anchors execution in proprioceptive states. This ensures smooth execution essential for fine-grained manipulation and maintains temporal rhythmicityâ€”anticipating task phases rather than purely reactingâ€”even in the presence of noisy visual feedback. Moreover, the neuromorphic spinal layer exhibits functional self-organization characteristic of biological circuits. We observe two complementary forms of sparsity that were not explicitly supervised: temporal sparsity, where neurons spontaneously revert to quiescence during static posturing to minimize metabolic cost; and spatial disentanglement, where the network naturally segregates high-dimensional control signals into distinct, somatotopic behavioral modes. Crucially, this event-driven substrate enables survival capabilities inaccessible to conventional planners. Under unexpected physical collisions, the cerebellar module leverages force feedback to trigger rapid (\n<\n20\n<20\nms) withdrawal reflexes via cerebellar-spinal loops, bypassing the prohibitive latency (\n>\n200\n>200\nms) of the cortical loop\n[\n9\n]\n. To test the capability for rapid few-shot learning, we eschewed massive expert datasets. Instead, we utilized only the pretrained weights of a Vision-Language Model (VLM) and fine-tuned on a limited set of a few hundred downstream samples, remarkably achieving task success rates that even outperformed the pretrained baselines.\nOur findings demonstrate that mimicking the hierarchical architecture of the biological motor nervous system endows robots with highly adaptive and energy-efficient embodied intelligence. Rather than introducing brittle complexity\n[\n10\n]\n, this bio-inspired hierarchy reduces control overhead by decoupling functionalities across circuits commensurate with their intrinsic timescales and physiological roles. Consequently, the proposed architecture offers a scalable computational scaffold for future embodied agents, ensuring that high-level semantics and low-level reflexes operate in parallel to achieve robust and fluid performance.\nFigure 1:\nHierarchical decoupling of semantic planning and neuromorphic motor control.\na,\nThe\nBio-inspired Computing Paradigm\nbridges the timescale gap between cognition and actuation. The architecture allocates high-latency, high-dimensional visual-language processing to a\nCUDA Computing Tier (Cortical Module)\n, while offloading high-frequency proprioceptive modulation and reflexes to an energy-efficient\nNeuromorphic Chip Tier (Cerebellar/Spinal Modules)\n. This separation enables a 10\nÃ—\n\\times\nspeedup in local sensorimotor loops compared to cortical planning.\nb,\nData flow in the\nTri-Level Neuromorphic VLA\n. (1) The\nCortical Module\nsynthesizes abstract, low-frequency motor goals from visual instructions. (2) The\nCerebellar Module\nfunctions as a state-adaptive filter, utilizing dense proprioception to perform real-time gain modulation (inset graph), compensating for dynamic discrepancies. (3) The\nSpiking Spinal Module\ntranslates these commands into precise actuation via Spiking Neural Networks (SNNs). Crucially, it incorporates a\nFast Safety Reflex\npathway (red loop) that processes tactile/force signals locally to trigger withdrawal responses, bypassing the slower cortical loop entirely, while enabling on-device plasticity for continuous adaptation.\n2\nResults\n2.1\nA Bio-inspired VLA with Event-driven Sparsity\nTo reconcile the conflicting timescales of semantic cognition and high-frequency actuation, we instantiated the biological motor hierarchy within a distributed computational framework: Neuromorphic Vision-Language-Action (NeuroVLA) architecture (Fig.\n1\n). This system fundamentally decouples the CUDA Computing Tier, responsible for latency-tolerant semantic reasoning, from the Neuromorphic Chip Tier, dedicated to millisecond-level sensorimotor loops.\nAt the cortical level, the CUDA tier processes computationally heavy visual and linguistic modalities. To bridge the dimensionality gap between high-level reasoning and low-level control, we employ a Querying Transformer (Q-Former) structure\n[\n11\n]\n. Functionally analogous to biological descending motor pathways (e.g., the corticospinal tract) which distill complex cortical dynamics into streamlined motor commands, the Q-Former extracts compact, task-relevant intention signals from the dense VLM representations. These distilled low-frequency signals are transmitted to the Neuromorphic tier, where the Cerebellar Module acts as an adaptive filter. By ingesting high-frequency proprioceptive and force signalsâ€”which significantly outpace visual frame ratesâ€”this module serves a dual role: it smooths kinematic trajectories via gain control and provides robust resistance to visual sensory noise, enabling rapid trajectory re-planning without waiting for cortical updates\n[\n12\n]\n.\nThe hierarchy is grounded in the Spiking Spinal Module, deployed on the neuromorphic hardware to enforce extreme metabolic efficiency. Deviating from the dense computations of standard deep learning, this module exploits the event-driven sparsity of Spiking Neural Networks (SNNs). Neurons are selectively recruited only during active kinematic changes, with firing rates dropping to near-quiescence during static holding phases, thereby minimizing power consumption while maintaining vigilance. Moreover, unlike stateless Multi-Layer Perceptron (MLP) heads that treat control as independent regression steps, the intrinsic membrane dynamics of the spinal SNN naturally encode temporal dependencies. This preserves motion continuity and enables fast, monosynaptic-like safety reflexes, strictly mirroring the physiological division of labor.\nFurther details of the methods are provided in Method\n4\n.\nFigure 2:\nSemantic distillation of descending motor intent via attentional gating.\nAnalogous to the\ncorticospinal tract\n, which filters high-dimensional cortical processing into streamlined execution commands, the Q-Former interface extracts task-specific geometric features while suppressing task-irrelevant information.\na, b,\nMulti-stage intent extraction for the task â€œPut the wine bottle on the cabinet.â€ The mechanism initially isolates the manipulation target (wine bottle) in the wrist view to guide grasping\n(a)\n, before shifting attention to the destination surface to orchestrate placement\n(b)\n, mirroring the sequential focus of motor planning.\nc, d,\nSemantic selectivity under the instruction â€œOpen the middle drawer.â€ Despite the visual salience of the wine bottle (a potential distractor), the descending queries actively inhibit this feature\n(c)\n, instead exclusively grounding the drawer handle\n(d)\n. This confirms that the module does not merely encode visual saliency but performs top-down attentional modulation, ensuring that only task-relevant spatial primitives are transmitted to downstream cerebellar and spinal circuits.\n2.2\nTask intent drives top-down attentional gating of spatial primitives\nTo investigate how abstract semantic goals are translated into actionable motor commands, we visualized the cross-modal attention maps generated by the Intention Extraction Module (Fig.\n2\n). We observed a distinct dynamic reconfiguration of spatial attention conditioned solely on linguistic instructions, overriding bottom-up visual saliency. In scenes populated with multiple potential interactors (e.g., a wine bottle and a cabinet drawer), the system exhibited precise attentional selectivity that dynamically evolves/shifts in alignment with task progression. Under the instruction Put the wine bottle on top of the cabinetâ€ (Fig.\n2\na-b), the descending queries formed a preferential attachment to the bottleâ€™s geometry, effectively filtering out the adjacent drawer despite its visual prominence.Subsequently, as the task advanced to the placement phase, the focus seamlessly shifted to the target destination. Crucially, when the instruction was switched to Open the middle drawerâ€ (Fig.\n2\nc-d) within the identical visual scene, the attentional hotspot shifted instantaneously to ground the drawer handle, effectively filtering out visually prominent but irrelevant distractors.\nThese observations indicate that the module functions as an active semantic filter. Rather than passively encoding the entire scene, the system implements a top-down attentional spotlight that isolates task-relevant geometric primitives while suppressing perceptual entropy. This mechanism creates a semantic information bottleneck functionally isomorphic to the biological\ncorticospinal tract\n, ensuring that downstream cerebellar and spinal circuits receive only the streamlined, task-essential execution commands necessary for low-latency control.\nFigure 3:\nProprioceptive temporal dynamics and force-aware adaptive control.\na,\nTemporal sensorimotor dynamics as a compact form of motor memory. In rhythmic manipulation tasks (e.g., â€œShake the cupâ€), the system encodes the motion primitive not through redundant visual frames, but via high-frequency trajectories of\nRight Wrench\n(force/torque) and\nRight Arm Joint\nstates. This proprioceptive encoding allows the agent to maintain phase consistency and temporal rhythm independent of visual occlusion.\nb,\nCollision-induced wrench perturbations and cerebellum-inspired trajectory adjustment. Real-time monitoring of 6D wrench signals detects a physical contact event (sharp spike in force profiles). Upon detection, the cerebellar feedback loop triggers an immediate spatial trajectory reformulation (blue solid line in 3D plot), allowing the end-effector to autonomously navigate around the obstacle, whereas the open-loop baseline (red dashed line) fails to adapt.\n2.3\nThe cerebellar module recapitulates the three canonical functional loops of the biological cerebellum\nFigure 4:\nCerebellar-mediated attenuation of high-frequency kinematic motor noise.\na,\nQualitative comparison of commanded acceleration traces over time. The baseline cortical policy (red) exhibits significant high-frequency stochastic jitter (analogous to â€œintention tremorâ€) across translation (\nd\nâ€‹\nx\n,\nd\nâ€‹\ny\n,\nd\nâ€‹\nz\ndx,dy,dz\n) and rotation (\nd\nâ€‹\nÏ•\n,\nd\nâ€‹\nÎ¸\n,\nd\nâ€‹\nÏˆ\nd\\phi,d\\theta,d\\psi\n) dimensions. The inclusion of the cerebellar module (blue) acts as a physiological damper, producing markedly smoother control signals.\nb, c,\nQuantitative assessment of kinematic smoothing. Bar charts show the\nb,\nMean Absolute Commanded Jerk and\nc,\nMean Absolute Commanded Acceleration. The cerebellar module achieves substantial noise attenuation, reducing average jerk by over 75% and average acceleration by over 40% compared to the monolithic baseline, confirming its critical role in stabilizing stochastic cortical outputs prior to spinal execution.\nFigure 5:\nEvent-driven sparsity and temporal robustness of the neuromorphic spinal module.\na, b,\nSelective neural recruitment and functional modularity.\na,\nTraces of membrane potentials and spike trains demonstrate the event-driven nature of the SNN. During the specific phase of â€œStatic Pose & Dynamic Gripperâ€ (shaded region), the\nGripper Control Neurons (GCN, red)\nexhibit high-frequency spiking activity driven by the actuation demand, while the\nEnd-Effector Pose Control Neurons (ECN, blue)\nremain relatively quiescent.\nb,\nQuantitative comparison of firing rates confirms this decoupling: neurons are selectively recruited only when their corresponding motor primitives undergo state changes, minimizing redundant computation.\nc,\nMetabolic efficiency via temporal sparsity. Mean activation rates across network layers drop significantly during static holding phases compared to dynamic action phases. This â€œactivity-on-demandâ€ mechanism ensures low power consumption, crucial for edge-side deployment on battery-constrained robots.\nd,\nAblation study on the LIBERO benchmark. The\nMulti-step SNN\n(blue), which integrates temporal context, consistently outperforms the\nSingle-step SNN\n(red) and the\nNo-Cerebellum baseline\n(green). The performance gap is particularly pronounced in long-horizon tasks (e.g., â€œBowl on stoveâ€), validating that the spinal moduleâ€™s intrinsic temporal dynamicsâ€”analogous to the cerebellumâ€™s role in sequencingâ€”are essential for robust complex manipulation.\nFigure 6:\nEmergent functional specialization and latent disentanglement in the neuromorphic spinal substrate.\nNeural Representation of Action\n: Spatially rearranged firing rate maps reveal a spontaneous functional organization where distinct neural subpopulations are selectively recruited to encode specific kinematic dimensions (e.g.,\n|\nÎ”\nâ€‹\nR\nâ€‹\no\nâ€‹\nl\nâ€‹\nl\n|\n|\\Delta Roll|\nvs.\n|\nGripper\n|\n|\\text{Gripper}|\n), mirroring the energy-efficient modularity of the biological motor cortex.\nAction Latent Cluster\n: the low-dimensional t-SNE projection of spinal hidden states demonstrates the networkâ€™s intrinsic capacity to disentangle high-dimensional control signals into discretely clustered behavioral modes (motor primitives) without explicit supervision, validating the emergence of structural representation learning.\nTo investigate the mechanistic basis of the systemâ€™s robustness, we analyzed the agentâ€™s behavior under diverse physical conditions. Our observations suggest that the Cerebellar Module does not merely act as a generic filter, but functionally instantiates the three distinct phylogenetic loops of the biological cerebellum: the\nSpinocerebellum\n,\nVestibulocerebellum\n, and\nCerebrocerebellum\n.\nSpinocerebellar loop: Damping intention tremor via proprioceptive gain control\nWe first examined the kinematic stability of the robotic arm during free-space motion. As visualized in the acceleration traces (Fig.\n4\na), the baseline cortical policy (the red line) exhibited significant high-frequency stochastic oscillations across all spatial dimensions, a phenomenon mechanistically analogous to â€œclinical intention tremor.â€ However, upon activating the cerebellar loop, we observed a suppression of these oscillations (the blue line).\nQuantitative analysis confirms that the module leverages real-time proprioceptive feedback (joint angles and velocities) to continuously compute the discrepancy between predicted and observed states, attenuating motion jerk by an average of 75.6% (peaking at 80.2% in\nÎ”\nâ€‹\ny\nâ€‹\na\nâ€‹\nw\n\\Delta yaw\nand 80.0% in\nÎ”\nâ€‹\nZ\n\\Delta Z\n) relative to the baseline (Fig.\n4\nb). Consequently, this smoothing effect leads to a significant reduction in the Mean Absolute Commanded Acceleration (MACA) by 32.8% to 58.0% across all axes (Fig.\n4\nc).\nThese results indicate that the module effectively recapitulates the\nSpinocerebellum\n(Paleocerebellum) function: it regulates digital â€œmuscle toneâ€ to dampen execution noise and smooth trajectories, independent of the high-level semantic planner.\nVestibulocerebellar loop: Restoring equilibrium via fast force reflexes\nNext, we assessed the systemâ€™s capacity to maintain physical equilibrium under perturbation. In the collision experiments (Fig.\n3\nb), we observed distinct signatures in the high-frequency sensor data: physical contact manifested as sharp, immediate spikes in the 6-DoF wrench profile (middle panel).\nSpecifically, the impact induced high-magnitude fluctuations, with force readings surging to mean values of\nÎ¼\nâ‰ˆ\nâˆ’\n37.02\n\\mu\\approx-37.02\nin\nF\nx\nF_{x}\nand\nÎ¼\nâ‰ˆ\n8.30\n\\mu\\approx 8.30\nin\nF\ny\nF_{y}\n, accompanied by significant variances (e.g.,\nÏƒ\n2\nâ‰ˆ\n3.27\n\\sigma^{2}\\approx 3.27\nin\nF\ny\nF_{y}\n) indicative of sudden instability.\nWhile the vision-centric baselineâ€”constrained by high inference latencyâ€”persisted in executing the blocked plan (red dashed line), our system exploited these sparse, high-frequency (\n>\n200\n>200\nHz) force signals to detect the anomaly within milliseconds. Consequently, the agent triggered an instantaneous spatial trajectory reformulation (blue solid line), successfully navigating around the obstacle. This emergent behavior confirms that the 6-DoF force sensor acts as a functional vestibular organ. By bypassing the slow cortical loop to trigger local corrective actions, the system instantiates a\nVestibulocerebellum\n-like (Archicerebellum) reflex, preserving operational â€œpostural stabilityâ€ and equipment safety through rapid, local adaptation.\nCerebrocerebellar loop: Encoding temporal rhythm as motor memory\nFinally, we investigated the representation of complex, rhythmic manipulation tasks. In the â€œShake the cupâ€ protocol (Fig.\n3\na), we observed that the motion primitive was encoded not as a sequence of redundant visual frames, but as a structured periodic trajectory.\nAs detailed in the sensorimotor traces, the semantic instruction was directly translated into three distinct, high-fidelity oscillatory cycles (red shaded regions). The 6-DoF wrench profile (middle panel) and joint angles (right panel) exhibited precise temporal synchronization, with the sinusoidal joint modulations perfectly aligning with the force feedback dynamics across all three repetitions.\nThis proprioceptive encoding allowed the system to maintain phase consistency and rhythmicity even when visual feedback was static or occluded. In the meantime, as shown in Fig.\n5\ne, comparative analysis under sensory degradation revealed that while cortical baselines faltered when lighting or textures were altered, the cerebellar-enhanced agent maintained high success rates by leveraging the invariance of physical dynamics. This observation suggests that the module performs cross-modal sensory re-weighting to serve as a robust form of motor memory. Mirroring the\nCerebrocerebellum\n(Neocerebellum) and the dentate nucleus, the system decouples temporal sequencing from visual perception, effectively â€œimmunizingâ€ the agent against environmental entropy and visual sensory failure.\n2.4\nEvent-driven sparsity and temporal integration in the neuromorphic spinal substrate\nHaving established the role of the cerebellar loop in modulation, we next validated the computational efficiency of the Spiking Spinal Module, which functions as the systemâ€™s execution interface. Biological spinal circuits are characterized by extreme metabolic efficiency, recruiting motor neurons only when actuation is demanded. Our neuromorphic implementation strictly adheres to this principle of event-driven sparsity. As visualized in the membrane potential traces (Fig.\n5\na), the network exhibits a profound functional decoupling: during phases of â€œStatic Pose & Dynamic Gripper,â€ neurons controlling the end-effector pose remain in a sub-threshold, quiescent state, while gripper-specific neurons burst into high-frequency spiking activity. This selective recruitment is quantified in Fig.\n5\nb, confirming that the architecture avoids the wasteful, continuous computation characteristic of standard Artificial Neural Networks (ANNs). Consequently, the mean activation rate drops significantly during static holding phases (Fig.\n5\nc), effectively implementing an â€œactivity-on-demandâ€ energy profile that is critical for prolonged deployment on battery-constrained Neuromorphic hardware.\nBeyond metabolic benefits, the spinal SNN plays a decisive role in temporal information processing. Unlike standard stateless policies that treat sequential control as independent Markovian steps, spiking neurons possess intrinsic memory via membrane potential leakage and accumulation\n[\n14\n]\n. This creates a natural temporal receptive field. We assessed this capability on the LIBERO\n[\n18\n,\n31\n]\nbenchmark (Fig.\n5\nd), comparing our Multi-step SNN against a Single-step variant (ablation of temporal integration) and a No-Cerebellum baseline. The Multi-step SNN achieved superior success rates, particularly in long-horizon tasks requiring phase transitions (e.g., â€œBowl on stoveâ€). The performance degradation observed in the Single-step variant suggests that the temporal integration inherent to the spiking dynamics serves as a short-term working memory, essential for preserving motion continuity and logic across extended manipulation sequences. Thus, the spinal module is not merely a passive actuator, but an active, spatiotemporal computer that harmonizes energy efficiency with control robustness.\nThe visualization (Fig.\n6\nNeural Representation of Action) shows the mean firing rates of neurons in the first hidden spiking layer, spatially rearranged to highlight topological structure. Each map represents the neural population response conditioned on a dominant kinematic dimension (e.g.,\n|\nÎ”\nâ€‹\nZ\n|\n|\\Delta Z|\n,\n|\nGripper\n|\n|\\text{Gripper}|\n), with red indicating high metabolic activity. The distinct, non-overlapping activation patterns across dimensions reveal a spontaneous somatotopic organization, where specific neural subpopulations are selectively recruited to control specific Degrees of Freedom (DoF), mirroring the functional segregation found in the biological motor cortex.\n(Fig.\n6\nAction Latent Cluster) depicts the low-dimensional manifold structure of the spinal hidden states projected via t-SNE. The clear separation of color-coded motor primitives (e.g., green for vertical\nÎ”\nâ€‹\nZ\n\\Delta Z\nmotion, pink for Gripper actuation) demonstrates that the spiking network disentangles high-dimensional control signals into distinct behavioral modes without explicit supervision, validating the networkâ€™s internal capacity for structural representation learning.\n2.5\nNeuromorphic processor validation confirms the spinal moduleâ€™s low latency and energy efficiency.\nThe Spiking Spinal Module is deployed on a customized neuromorphic processor which is implemented on an FPGA platform (Fig.\n7\na). To minimize inference latency, the design adopts a systolic-array architecture (Fig.\n7\nb) that offers high spatio-temporal parallelism while reducing data movement. Spatial parallelism is achieved by updating multiple LIF\n[\n24\n]\nneurons concurrently across array columns with spike reuse, whereas temporal parallelism arises from parallel weight accumulation along array rows with weight reuse. This organization markedly accelerates computation compared with conventional neuromorphic processorsâ€”such as IBMâ€™s TrueNorth\n[\n25\n]\nand Intelâ€™s Loihi\n[\n26\n]\nâ€”whose neuron updates proceed in a time-sequential fashion. To further improve energy efficiency, a spike-detection module enables spike-sparsity-aware computation by suppressing inactive events: spikes that remain non-active throughout the entire time window are prevented from entering the systolic array, thereby eliminating unnecessary neuron updates\n[\n35\n,\n36\n]\n. The FPGA hardware resource utilization and the performance summary are presented (Fig.\n7\nc-d), respectively. At an operating frequency of 20 MHz, the design achieves an inference latency of 2.19 ms and an energy cost of 0.87 mJ per inference.\nFigure 7:\nNeuromorphic processor implemented on an FPGA platform for deploying the Spiking Spinal Module.\na,\nFPGA board used for implementation.\nb,\nNeuromorphic processor incorporating an LIF systolic-array architecture and spike-sparsity-aware computation to reduce inference latency and energy consumption.\nc,\nFPGA resource utilization, with 51,953 LUTs, 27,880 FFs and 169 BRAMs.\nd,\nPerformance summary, showing an inference latency of 2.19 ms at 20 MHz and an energy cost of 0.87 mJ per inference.\nFigure 8:\nVersatile manipulation dexterity and safety-critical adaptation in physical laboratory environments.\naâ€“e,\nPerformance on standard manipulation tasks. The proposed NeuroVLA (blue bars) consistently outperforms state-of-the-art VLA baselines (OpenVLA, OpenVLA-OFT, UniVLA, WorldVLA) across tasks demanding spatial precision (\na,\nrelocating test tubes;\nc,\norganizing items;\ne,\ndiscarding waste), dynamic trajectory smoothness (\nb,\npouring liquid), and temporal rhythmicity (\nd,\nshaking flasks). The results highlight the architectureâ€™s capability to handle complex contact dynamics and long-horizon sequencing.\nf,\nSafety-critical collision recovery. In this stress test, the robot encounters an unexpected physical obstruction while holding a fragile test tube. Baseline models, lacking low-level reflex circuits, suffer catastrophic failure (0.0% success rate). In contrast, the NeuroVLA detects the contact force via the spinal module, triggers a rapid withdrawal reflex, and autonomously re-plans to navigate around the obstacle and complete the shaking task, demonstrating emergent survival behaviors in unstructured environments.\n2.6\nReal-world Observations of Emergent Motor Intelligence\nTo validate the ecological validity of the NeuroVLA beyond simulation, we deployed the architecture on a physical bimanual humanoid platform performing a suite of biochemical protocols (Fig.\n8\na). These tasks were selected to probe distinct motor primitives: spatial precision (relocating test tubes), dynamic fluid control (pouring), and rhythmic sequencing (flask shaking). As detailed in (Fig.\n8\nbâ€“f), our system consistently outperformed state-of-the-art monolithic baselines, including OpenVLA\n[\n2\n]\n, OpenVLA-OFT\n[\n15\n]\n, UniVLA\n[\n17\n]\nand WorldVLA\n[\n16\n]\n. The performance gap was particularly pronounced in tasks involving fine manipulation and fluid dynamics. For instance, during liquid pouring (Fig.\n8\nc), monolithic models frequently suffered from open-loop drift; once the pouring vessel tilted, the visual occlusion of the liquid level and the non-linear weight shifts caused these models to overshoot or spill. In contrast, the NeuroVLA leveraged the Cerebellar Module to integrate high-frequency proprioceptive feedback (200 Hz), allowing it to sense the shifting center of mass and modulate wrist gain in real-time, thereby maintaining a smooth, spill-free trajectory despite visual uncertainty. Similarly, in rhythmic tasks like flask shaking (Fig.\n8\ne), the Spinal Moduleâ€™s intrinsic temporal memory maintained phase consistency, whereas baseline models often exhibited jerky, aperiodic motions due to their inability to encode high-frequency temporal dependencies.\nHowever, the most profound evidence of emergent intelligence was observed under safety-critical perturbations.\nWe introduced a â€œRecover to Safe Areaâ€ stress test (Fig.\n8\ng), where the robot encounters an unmodeled physical obstruction while handling fragile glasswareâ€”a scenario representative of unstructured environments. Here, standard cortical-centric VLAs suffered catastrophic failure (0.0% success rate). The failure mode was consistent: constrained not merely by the latency of the vision-language inference loop (\n>\n200\n>200\nms), but fundamentally by the structural absence of specialized modules for high-frequency proprioceptive processing, these models failed to register the instantaneous collision force until deep into the contact event, typically persisting in executing the blocked plan until mechanical stall or object breakage.\nIn stark contrast, our system achieved a 54.8% recovery rate, demonstrating emergent survival behaviors. This capability is mechanically attributable to the decoupling of perception and survival. Upon detecting a sudden wrench spike via 6-DoF sensors, the neuromorphic spinal substrate triggers a monosynaptic-like withdrawal reflex (\n<\n<\n50 ms latency). This fast-path response bypasses the slower cortical planner entirely, retracting the end-effector to prevent damage. Crucially, this is not a hard-coded stop; following the reflex, the Cerebellar Module utilizes the tactile feedback to orchestrate a local trajectory reformulation, guiding the arm around the obstacle to resume the task. This result provides empirical confirmation that robust embodied intelligence requires a division of labor: a cortical â€œbrainâ€ for semantic intent, supported by a spinal â€œbodyâ€ capable of handling the immediacy and unpredictability of the physical world.\n3\nDiscussion\nReinstating the Biological Hierarchy in Embodied AI\nCurrent approaches to embodied intelligence have largely focused on scaling monolithic Vision-Language-Action models, implicitly assuming that motor control will emerge as a byproduct of semantic pre-training. Our results challenge this assumption. We demonstrate that while cortical-scale models excel at high-level planning, they suffer from inherent latency and open-loop drift when directly tasked with high-frequency actuation. By instantiating a bio-inspired Neuromorphic Architecture, we show that the path to robust embodied agents lies not merely in data volume, but in reinstating the architectural inductive biases selected by evolution: a distinct separation of semantic planning, state modulation, and action execution.\nThe Neuromorphic Advantage\nThe integration of the\nSpinal Module\nmarks a paradigm shift from continuous-time computing to event-driven neuromorphic control. Our analysis of the spinal SNN reveals two critical advantages.\nMetabolic Parsimony\n: by adhering to the principle of activity-on-demand, the system drastically reduces computational redundancy during static holding phases (Fig.\n5\na), offering a viable path for deploying complex policies on battery-constrained edge hardware.\nTemporal Dependence\n: Unlike standard stateless MLPs that generate discrete actions or action chunks, the stateful membrane dynamics of our LIF neurons explicitly capture temporal dependencies. This intrinsic integration converts stochastic neural spikes into continuous action commands (Fig.\n4\n), effectively bridging the gap between the discrete SNN or MLP and the continuous nature of physics.\nReflexes: Survival Precedes Understanding\nWe achieve rapid reflexes through the simulation of cerebellum and spinal cord structuresâ€”essential for dynamic physical interaction. Traditional VLA systems treat collision avoidance as a planning problem, constrained by the latency of visual perception. Our Safety Reflex experiments (Fig.\n8\ng) demonstrate that survival behaviors should be decoupled from semantic reasoning. The success of our cerebellum module in triggering monosynaptic-like withdrawal reflexes (\n<\n20\n<20\nms) proves that\nsurvival precedes understanding\n. By bypassing the cortical bottleneck, we endow the robot with a physiological intuition for physical pain, ensuring safety even when the high-level planner is confused or occluded.\nLimitations and Future Outlook\nWhile we observe the emergence of some biological intelligence capabilities in our work, current limitations point to future research directions. First, our SNN training relies on GPU acceleration with surrogate gradients; Realizing the full energy-efficiency potential will require training deployment on specialized neuromorphic chips (e.g., Loihi or Tianjic\n[\n29\n]\n). Second, the current learning rule relies on offline behavior cloning; incorporating online Spike-Timing-Dependent Plasticity (STDP) could enable true lifelong learning, allowing the spinal module to adapt to muscle fatigue or wear over time.\nUltimately, NeuroVLA exhibits emergent biological motor functions by simulating the hierarchical organization of biological motor systems. This provides a novel perspective for VLA design, suggesting that future research should look beyond data to uncover the latent potential within biological neural structures.\n4\nMethods\n4.1\nArchitecture Overview\nWe formalize the embodied control task as a Partially Observable Markov Decision Process (POMDP)\n[\n30\n]\n. At each time step\nt\nt\n, the agent receives a multimodal observation tuple\no\nt\nâˆˆ\nğ’ª\no_{t}\\in\\mathcal{O}\n, comprising high-dimensional visual sensory data\nI\nt\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n3\nI_{t}\\in\\mathbb{R}^{H\\times W\\times 3}\n, a natural language instruction\nL\nL\n, and a history of low-dimensional proprioceptive states\nğ¬\nt\nâˆ’\nH\n:\nt\nâˆˆ\nâ„\nH\nÃ—\nD\ns\n\\mathbf{s}_{t-H:t}\\in\\mathbb{R}^{H\\times D_{s}}\n, where\nH\nH\ndenotes the size of history window and\nD\ns\nD_{s}\nrepresents the dimensions of joint positions, velocities, and end-effector forces (6-DoF wrench). The objective is to generate a sequence of continuous motor actions\nğš\nt\nâˆˆ\nğ’œ\n\\mathbf{a}_{t}\\in\\mathcal{A}\nto maximize the cumulative reward.\nTo reconcile the conflicting computational requirements of semantic planning (high-latency, abstract) and motor execution (low-latency, precise), we propose the\nTri-Level Neuromorphic Vision-Language-Action ( NeuroVLA)\narchitecture. As shown in Fig. 1, the system control policy\nÏ€\nâ€‹\n(\na\nt\n|\no\nt\n)\n\\pi(a_{t}|o_{t})\nis decomposed into a hierarchical composition of three specialized mapping functions:\nğš\nt\n=\nÎ¦\nspine\nâ€‹\n(\nÎ¦\ncerebellum\nâ€‹\n(\nÎ¦\ncortex\nâ€‹\n(\nI\nt\n,\nL\n)\n,\nğ¡\nt\n)\n)\n,\n\\mathbf{a}_{t}=\\Phi_{\\text{spine}}\\left(\\Phi_{\\text{cerebellum}}\\left(\\Phi_{\\text{cortex}}(I_{t},L),\\mathbf{h}_{t}\\right)\\right),\nwhere\nÎ¦\ncortex\n\\Phi_{\\text{cortex}}\n,\nÎ¦\ncerebellum\n\\Phi_{\\text{cerebellum}}\n, and\nÎ¦\nspine\n\\Phi_{\\text{spine}}\ncorrespond to the cortical, cerebellar, and spinal modules, respectively.\nğ¡\nt\n\\mathbf{h}_{t}\ndenotes the compact dynamic context vector from the neurological state estimator. This decomposition enforces a strict division of labor across timescales and hardware substrates:\n1.\nCortical Semantic Projection (\nÎ¦\ncortex\n\\Phi_{\\text{cortex}}\n):\nOperates on the CUDA Computing Tier. It maps the visual-linguistic inputs\n(\nI\nt\n,\nL\n)\n(I_{t},L)\ninto a semantic latent intention space\nğ³\nsem\nâˆˆ\nâ„\nD\nmodel\n\\mathbf{z}_{\\text{sem}}\\in\\mathbb{R}^{D_{\\text{model}}}\n, where\nD\nmodel\nD_{\\text{model}}\ndenotes the dimension of model internal feature space. This module utilizes a Vision-Language Model (VLM) backbone to extract open-vocabulary affordances, unaffected by high-frequency physical perturbations.\n2.\nCerebellar State Modulation (\nÎ¦\ncerebellum\n\\Phi_{\\text{cerebellum}}\n):\nOperates on the CUDA Computing Tier. It functions as a state-dependent affine transformation. A recurrent estimator aggregates the proprioceptive history\nğ¬\nt\nâˆ’\nH\n:\nt\n\\mathbf{s}_{t-H:t}\ninto a dynamic context vector\nğ¡\nt\n\\mathbf{h}_{t}\n. This context is then used to modulate the semantic latent\nğ³\nsem\n\\mathbf{z}_{\\text{sem}}\nvia a Gated Feature-wise Linear Modulation (FiLM) mechanism, yielding a refined, physics-aware latent code\nğ³\nmod\n\\mathbf{z}_{\\text{mod}}\n. This process mimics the biological cerebellumâ€™s role in gain control and error correction based on the efference copy and sensory feedback.\n3.\nSpinal Neuromorphic Decoding (\nÎ¦\nspine\n\\Phi_{\\text{spine}}\n):\nOperates on Neuromorphic Chip Tier. It maps the modulated latent\nğ³\nmod\n\\mathbf{z}_{\\text{mod}}\nto continuous actions\nğš\nt\n\\mathbf{a}_{t}\nvia a Spiking Neural Network (SNN) governed by Leaky Integrate-and-Fire (LIF) dynamics. This module leverages event-driven sparsity and intrinsic temporal memory to ensure energy-efficient, smooth trajectory generation and rapid reflex responses.\nThe entire system is trained end-to-end using a hybrid objective that combines behavior cloning losses with surrogate gradients\n[\n22\n]\nfor the non-differentiable spiking components, ensuring functional coherence across the hierarchy.\n4.2\nCortical Module: Semantic Latent Generation\nThe Cortical Module occupies the highest tier of the control hierarchy, tasked with cross-modal reasoning and the synthesis of abstract motor intentions from high-dimensional observations. Unlike lower-level circuits that must react to instantaneous physical dynamics, the cortical layer operates on a semantic timescale, prioritizing long-horizon planning and generalization over high-frequency control.\n4.2.1\nVision-Language Reasoning Backbone\nWe leverage a pre-trained Large Vision-Language Model (VLM), specifically Qwen-VL\n[\n27\n]\n, as the general-purpose reasoning engine. Formally, given the current RGB observation\nI\nt\nI_{t}\nand the natural language instruction\nL\nL\n, the VLM functions as a dense feature extractor:\nâ„‹\nt\n=\nF\nVLM\nâ€‹\n(\nI\nt\n,\nL\n;\nÎ¸\nvlm\n)\n,\n\\mathcal{H}_{t}=F_{\\text{VLM}}(I_{t},L;\\theta_{\\text{vlm}}),\nwhere\nâ„‹\nt\n=\n{\nh\nt\n(\n1\n)\n,\nâ€¦\n,\nh\nt\n(\nN\n)\n}\n\\mathcal{H}_{t}=\\{h_{t}^{(1)},\\dots,h_{t}^{(N)}\\}\nrepresents the stack of hidden states across all\nN\nN\ntransformer layers and\nÎ¸\nvlm\n\\theta_{\\text{vlm}}\ndenotes the parameters of VLM. This rich representation encapsulates scene geometry, object semantics, and linguistic understanding, providing a â€œworld modelâ€ necessary for open-vocabulary reasoning and manipulation.\n4.2.2\nLayer-wise Semantic Distillation\nDirectly feeding the high-dimensional, variable-length VLM hidden states into a motor policy is computationally prohibitive and prone to overfitting. To bridge the dimensionality gap between the VLMâ€™s linguistic-visual space and the robotâ€™s control manifold, we employ a\nLayer-wise Querying Transformer (Q-Former)\n.\nThe Q-Former acts as a learnable information bottleneck. It utilizes a set of learnable query tokens\nğ\nâˆˆ\nâ„\nK\nÃ—\nD\n\\mathbf{Q}\\in\\mathbb{R}^{K\\times D}\nto attend to specific subsets of the VLMâ€™s hidden representations, where\nK\nK\ndenotes the number of tokens and\nD\nD\ndenotes the token dimension. Crucially, rather than utilizing only the final layer, we extract features from a designated range of intermediate layer index\n[\nl\nstart\n,\nl\nend\n]\n[l_{\\text{start}},l_{\\text{end}}]\nto capture both low-level spatial details and high-level semantic abstractions. The extraction process is formalized as:\nğ³\nsem\n=\nQ-Former\n(\nConcat\n(\nâ„‹\nt\n[\nl\nstart\n:\nl\nend\n]\n)\n,\nğ\n;\nÎ¸\nQ-Former\n)\n,\n\\mathbf{z}_{\\text{sem}}=\\text{Q-Former}(\\text{Concat}(\\mathcal{H}_{t}[l_{\\text{start}}:l_{\\text{end}}]),\\mathbf{Q};\\theta_{\\text{Q-Former}}),\nwhere\nğ³\nsem\nâˆˆ\nâ„\nK\nÃ—\nD\naction\n\\mathbf{z}_{\\text{sem}}\\in\\mathbb{R}^{K\\times D_{\\text{action}}}\ndenotes the\nSemantic Latent Intention\nand\nÎ¸\nQ-Former\n\\theta_{\\text{Q-Former}}\ndenotes the parameters of Q-Former.\nBiological Insight:\nThis mechanism mirrors the biological principle of\ncortical abstraction\n. Just as the brainâ€™s motor cortex sends compact, population-level intent signals (rather than raw pixel data) to downstream circuits, our Q-Former distills the VLMâ€™s vast knowledge into a compact, action-centric representation. This semantic latent\nğ³\nsem\n\\mathbf{z}_{\\text{sem}}\nencodes\nâ€œwhat to doâ€\n(e.g., grasp the cup) while remaining agnostic to the specific physics of\nâ€œhow to do itâ€\n(e.g., compensate for friction), effectively decoupling semantic planning from physical modulation.\n4.3\nCerebellar Module: Gated Recurrent Neuromodulation\nWhile the cortical module generates semantic intentions, it operates significantly slower than the dynamics of physical interaction and lacks access to high-frequency proprioceptive feedback. To bridge this gap, the Cerebellar Module functions as an adaptive filter, performing\ndynamic state estimation\nand\ntrajectory refinement\nbefore the signals reach the spinal execution layer.\n4.3.1\nProprioceptive State Estimation via Recurrent Dynamics\nBiological motor control relies on the spinocerebellar tract to convey real-time information about limb position, velocity, and external forces. To model this, we employ a Gated Recurrent Unit (GRU)\n[\n28\n]\nas a neurological state estimator.\nLet\nğ¬\nt\nâˆ’\nH\n:\nt\nâˆˆ\nâ„\nH\nÃ—\nD\ns\n\\mathbf{s}_{t-H:t}\\in\\mathbb{R}^{H\\times D_{s}}\ndenote the history of robot states over a horizon\nH\nH\n(aligned with the sensor frequency, e.g., 50 Hz), where each state vector includes joint angles, velocities, and 6-DoF wrench measurements. The estimator integrates this temporal sequence to produce a compact dynamic context vector\nğ¡\nt\n\\mathbf{h}_{t}\n:\nğ¡\nt\n=\nGRU\nâ€‹\n(\nğ¬\nt\nâˆ’\nH\n:\nt\n;\nÎ¸\ngru\n)\n,\n\\mathbf{h}_{t}=\\text{GRU}(\\mathbf{s}_{t-H:t};\\theta_{\\text{gru}}),\nwhere\nÎ¸\ngru\n\\theta_{\\text{gru}}\ndenotes the parameter of the designed GRU. Unlike static encoders (e.g., MLPs), the GRU captures the\nrate of change\nand\ncontact transients\n(e.g., collision impulses), which are critical for stabilizing motion under perturbation.\n4.3.2\nGated Feature-wise Linear Modulation (FiLM)\nThe core computational role of the cerebellum is to modulate cortical commands based on current physical constraints. We formalize this interaction using a Gated FiLM mechanism\n[\n23\n]\n.\nFirst, to prevent proprioceptive noise from overwhelming semantic intent during stable phases, we employ a learnable gating factor\nğ \nt\n\\mathbf{g}_{t}\n:\nğ \nt\n=\nÏƒ\nâ€‹\n(\nW\ng\nâ‹…\nProj\nâ€‹\n(\nğ¡\nt\n)\n)\n,\n\\mathbf{g}_{t}=\\sigma(W_{g}\\cdot\\text{Proj}(\\mathbf{h}_{t})),\nwhere\nÏƒ\n\\sigma\n,\nW\ng\nW_{g}\nand\nProj\nâ€‹\n(\nâ‹…\n)\n\\text{Proj}(\\cdot)\ndenote the sigmoid function, the learnable affine transformation matrix and the projection function. This gate selectively regulates how much physical context is allowed to influence the cortical plan.\nSubsequently, the dynamic context\nğ¡\nt\n\\mathbf{h}_{t}\nis projected into affine transformation parametersâ€”scale\nğœ¸\nt\n\\boldsymbol{\\gamma}_{t}\nand shift\nğœ·\nt\n\\boldsymbol{\\beta}_{t}\nâ€”which are applied to the cortical semantic latent\nğ³\nsem\n\\mathbf{z}_{\\text{sem}}\n:\nğœ¸\nt\n=\nf\nÎ³\nâ€‹\n(\nğ¡\nt\n)\n,\nğœ·\nt\n=\nf\nÎ²\nâ€‹\n(\nğ¡\nt\n)\n\\boldsymbol{\\gamma}_{t}=f_{\\gamma}(\\mathbf{h}_{t}),\\quad\\boldsymbol{\\beta}_{t}=f_{\\beta}(\\mathbf{h}_{t})\nğ³\nmod\n=\n(\n1\n+\nğœ¸\nt\n)\nâŠ™\n(\nğ³\nsem\nâ‹…\nğ \nt\n)\n+\nğœ·\nt\n\\mathbf{z}_{\\text{mod}}=(1+\\boldsymbol{\\gamma}_{t})\\odot(\\mathbf{z}_{\\text{sem}}\\cdot\\mathbf{g}_{t})+\\boldsymbol{\\beta}_{t}\n, where\nâŠ™\n\\odot\ndenotes the element-wise Hadamard product. This operation allows the cerebellar module to strictly enforce gain control: for instance, upon detecting a collision (spike in\nğ¡\nt\n\\mathbf{h}_{t}\n), the network can suppress the forward velocity encoded in\nğ³\nsem\n\\mathbf{z}_{\\text{sem}}\n(via\nğœ¸\nt\nâ‰ˆ\nâˆ’\n1\n\\boldsymbol{\\gamma}_{t}\\approx-1\n) and inject a retraction bias (via\nğœ·\nt\n\\boldsymbol{\\beta}_{t}\n), effectively rewriting the motor plan in real-time.\n4.3.3\nIterative Refinement as a Forward Internal Model\nA defining characteristic of cerebellar function is its ability to continuously revise motor intentions based on anticipated sensory feedback. In our model, this predictive correction is operationalized through an\nIterative Refinement Loop\n. Upon receiving the updated anticipated state, the module\nRefine\nâ€‹\n(\nâ‹…\n)\n\\text{Refine}(\\cdot)\nre-applies the two cerebellar computations introduced in Sections\n4.3.1\nand\n4.3.2\nto incrementally adjust the motor latent. We instantiate this via an\nIterative Refinement Loop\n. Rather than a single-pass feedforward execution, the module performs\nK\nK\ncycles of internal recurrence (set to\nK\n=\n2\nK=2\nin our experiments).\nIn each iteration\nk\nâˆˆ\n{\n0\n,\nâ€¦\n,\nK\nâˆ’\n1\n}\nk\\in\\{0,...,K-1\\}\n, the module predicts a tentative action latent, updates the anticipated state evolution, and re-modulates the input. Formally:\nğ³\nmod\n(\nk\n+\n1\n)\nâ†\nRefine\nâ€‹\n(\nğ³\nmod\n(\nk\n)\n,\nğ¬\nt\n+\n1\n)\n\\mathbf{z}_{\\text{mod}}^{(k+1)}\\leftarrow\\text{Refine}(\\mathbf{z}_{\\text{mod}}^{(k)},\\mathbf{s}_{t+1})\nThis recursive process acts as a computational â€œmental simulation,â€ allowing the agent to minimize the Sim-to-Real gap by pre-correcting for expected dynamical errors (e.g., gravity or friction compensation) prior to actual spinal actuation.\nBiological Insight:\nThis architecture instantiates the\nEfference Copy\nprinciple. The semantic latent\nğ³\nsem\n\\mathbf{z}_{\\text{sem}}\nrepresents the â€œintendedâ€ movement (efference copy), while\nğ¡\nt\n\\mathbf{h}_{t}\nrepresents the â€œactualâ€ sensory feedback (re-afference). The FiLM layer computes the discrepancy (sensory prediction error) and applies the necessary correction. This explains why our model exhibits â€œdigital muscle memoryâ€â€”the ability to adaptively smooth trajectories and reject disturbances without needing to re-invoke the slow, computationally expensive cortical planner.\n4.4\nSpinal Module: Spiking Residual Dynamics\nThe Spinal Module serves as the final execution interface, translating the modulated latent intentions\nğ³\nmod\n\\mathbf{z}_{\\text{mod}}\ninto continuous motor commands\nğš\nt\n\\mathbf{a}_{t}\n. Implemented on neuromorphic principles, this module is designed to maximize\nmetabolic efficiency\nthrough event-driven processing and to ensure\ntrajectory smoothness\nvia intrinsic neuronal dynamics.\n4.4.1\nStateful Leaky Integrate-and-Fire Dynamics\nWe model the spinal interneurons using the iterative Leaky Integrate-and-Fire (LIF) formalism. Crucially, the network is instantiated with\nstateful membrane dynamics\n, meaning the membrane potential\nu\ni\n(\nl\n)\nu_{i}^{(l)}\nis strictly preserved across successive time steps rather than being re-initialized at each forward pass. This endows the spinal substrate with an\nimplicit temporal working memory\n, allowing it to encode history-dependent features without requiring explicit recurrent gating units (e.g., LSTMs). For a hidden neuron\ni\ni\nin layer\nl\nl\nat time step\nÏ„\n\\tau\n, the dynamics evolve according to:\nu\ni\n(\nl\n)\nâ€‹\n[\nÏ„\n]\n=\nÎ²\nâ€‹\nu\ni\n(\nl\n)\nâ€‹\n[\nÏ„\nâˆ’\n1\n]\n+\nâˆ‘\nj\nw\ni\nâ€‹\nj\nâ€‹\ns\nj\n(\nl\nâˆ’\n1\n)\nâ€‹\n[\nÏ„\n]\nâˆ’\ns\ni\n(\nl\n)\nâ€‹\n[\nÏ„\nâˆ’\n1\n]\nâ‹…\nÏ‘\n,\nu_{i}^{(l)}[\\tau]=\\beta u_{i}^{(l)}[\\tau-1]+\\sum_{j}w_{ij}s_{j}^{(l-1)}[\\tau]-s_{i}^{(l)}[\\tau-1]\\cdot\\vartheta,\nwhere\nÎ²\nâˆˆ\n(\n0\n,\n1\n)\n\\beta\\in(0,1)\nis the membrane decay factor,\nw\ni\nâ€‹\nj\nw_{ij}\nis the synaptic weight,\ns\nj\n(\nl\nâˆ’\n1\n)\nâ€‹\n[\nÏ„\n]\nâˆˆ\n{\n0\n,\n1\n}\ns_{j}^{(l-1)}[\\tau]\\in\\{0,1\\}\nrepresents the incoming spike train and\nÏ‘\n\\vartheta\ndenotes the voltage. The term\nu\ni\n(\nl\n)\nâ€‹\n[\nÏ„\nâˆ’\n1\n]\nu_{i}^{(l)}[\\tau-1]\nrepresents the\nresidual potential\ncarried over from the previous moment, establishing a continuous temporal context.\n4.4.2\nDeep Spiking Residual Architecture\nTo enable the learning of complex sensorimotor transformations without signal degradation in deep spiking layers, we structure the spinal module as a Spiking ResNet\n[\n32\n,\n33\n]\n. By incorporating residual skip connections:\nğ±\n(\nl\n+\n1\n)\n=\nğ±\n(\nl\n)\n+\nLIF\nâ€‹\n(\nLinear\nâ€‹\n(\nğ±\n(\nl\n)\n)\n)\n.\n\\mathbf{x}^{(l+1)}=\\mathbf{x}^{(l)}+\\text{LIF}(\\text{Linear}(\\mathbf{x}^{(l)})).\nWe facilitate the direct propagation of gradients and spike rates during training. This architecture mimics the robust signal transmission found in the propriospinal tracts, ensuring that high-level modulation from the cerebellar tier effectively reaches the distal actuation layers.\n4.4.3\nContinuous Integration for Motor Decoding\nA critical challenge in applying SNNs to robotics is bridging the gap between discrete, stochastic spikes and the continuous domain of joint actuation. We resolve this by implementing a\nContinuous Integration Protocol\nat the output layer\n[\n34\n]\n. Unlike hidden interneurons which undergo rapid repolarization (reset) upon spiking, the output motor neurons are configured as\nnon-resetting integrators\n. They continuously accumulate synaptic influx over the simulation window without discharging their potential:\nğš\nt\nâ€‹\n[\nÏ„\n]\n=\nğ’²\nout\nâ‹…\nğ®\nout\nâ€‹\n[\nÏ„\n]\n,\n\\mathbf{a}_{t}[\\tau]=\\mathcal{W}_{\\text{out}}\\cdot\\mathbf{u}_{\\text{out}}[\\tau],\nwhere\nğ®\nout\n\\mathbf{u}_{\\text{out}}\nrepresents the accumulated membrane voltage. This mechanism serves two physiological functions:\n1.\nTemporal Smoothing:\nThe integration process acts as a natural low-pass filter, converting high-frequency spike trains into smooth, continuous motor trajectories. This mirrors the biological process of\ntwitch summation\n, where muscles integrate neural drive into continuous force, filtering out neural noise.\n2.\nKinematic Consistency:\nBy maintaining the voltage state, the network naturally enforces temporal consistency in the output action sequence, reducing kinematic jerk and ensuring fluid transitions between control steps.\n4.4.4\nSurrogate Gradient Learning\nTo overcome the non-differentiability of the spike generation function\ns\n=\nÎ˜\nâ€‹\n(\nu\nâˆ’\nÏ‘\n)\ns=\\Theta(u-\\vartheta)\n, we employ a surrogate gradient method for end-to-end training. During the backward pass, the Heaviside step function is approximated by a fast sigmoid function\nÏƒ\nâ€‹\n(\nx\n)\n=\nx\n1\n+\n|\nx\n|\n\\sigma(x)=\\frac{x}{1+|x|}\n, allowing gradients to flow from the continuous action space back through the discrete spinal layers to the cerebellar and cortical modules.\nData Availability\nThe simulation datasets used in this study are derived from the open-source\nLIBERO\nand\nLIBERO-Plus\nbenchmarks. The real-world experimental data collected during the current study are available from the corresponding author upon reasonable request.\nCode Availability\nThe code of this paper is available at\nhttps://github.com/guoweiyu/NeuroVLA\n.\nAcknowledgements\nThis work was supported in part by the National Key R & D Program of China (Grant No.2023YFF0725001), in part by the National Natural Science Foundation of China (Grant No.92370204), in part by the Guangdong Basic and Applied Basic Research Foundation (Grant No.2023B1515120057), in part by the Education Bureau of Guangzhou.\nAuthor Contributions\nW.Y.G. proposed the idea of NeuroVLA, implemented the core codebase, and analyzed the results.\nP.T.L designed the real-world experiments, test the performance and discuss the idea.\nZ.H,T.F.C and Z.Y.C designed the explainable experiments,analyzed the results and discuss the idea.\nY.S, Y.K.Y and H.X supervised the literature review, data processing, methodology, analysis, and discussion.\nY.D.G, X.H and H.X managed this project.\nFigure Legends/Captions\nCompeting Interests\nThe authors declare no competing interests.\nInclusion & Ethics Statement\nWe confirm that this study adheres to all applicable ethical regulations.\nReferences\n[1]\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A. and others. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In\nConference on Robot Learning\n, 2165â€“2183 (2023).\n[2]\nKim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E. P., Sanketi, P. R., Vuong, Q., Kollar, T., Burchfiel, B., Tedrake, R., Sadigh, D., Levine, S., Liang, P., and Finn, C. OpenVLA: An Open-Source Vision-Language-Action Model. In\n8th Annual Conference on Robot Learning\n(2024).\n[3]\nScott, S. H. Optimal feedback control and the neural basis of volitional movement.\nNature Reviews Neuroscience\n5\n, 532â€“546 (2004).\n[4]\nTodorov, E. Optimality principles in sensorimotor control.\nNature Neuroscience\n7\n, 907â€“915 (2004).\n[5]\nK. Doya,\nâ€œWhat are the computations of the cerebellum, the basal ganglia and the cerebral cortex?â€\nin\nElsevier Science Ltd.\n, 1999.\n[6]\nM. Ito,\nâ€œControl of mental activities by internal models in the cerebellum,â€\nNature Reviews Neuroscience\n, vol. 9, no. 4, pp. 304â€“313, 2008.\ndoi:10.1006/jsvi.1999.2518.\n[7]\nS. Vahdat, O. Lungu, J. Cohen-Adad, V. Marchand-Pauvert, H. Benali, and J. Doyon,\nâ€œSimultaneous Brainâ€“Cervical Cord fMRI Reveals Intrinsic Spinal Cord Plasticity during Motor Sequence Learning,â€\nPLoS Biology\n, vol. 13, 2015.\ndoi:10.1371/journal.pbio.1002186.\n[8]\nVahdat, S., Lungu, O., Cohen-Adad, J., Marchand-Pauvert, V., Benali, H., Doyon, J. Simultaneous brainâ€“cervical cord fMRI reveals intrinsic spinal cord plasticity during motor sequence learning.\nPLoS Biology\n13\n, e1002186 (2015).\n[9]\nK. Roy, A. Jaiswal, and P. Panda,\nâ€œTowards spike-based machine intelligence with neuromorphic computing,â€\nNature\n, vol. 575, no. 7784, pp. 607â€“617, 2019.\ndoi:10.1038/s41586-019-1677-2.\n[10]\nBartolozzi, C., Indiveri, G., Donati, E. Embodied neuromorphic intelligence.\nNature Communications\n13\n, 1024 (2022).\n[11]\nLi, J., Li, D., Savarese, S., Hoi, S. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\nInternational Conference on Machine Learning\n, 19730â€“19742 (2023).\n[12]\nGarrido, J., Luque, N., Dâ€™Angelo, E., Ros, E. Distributed cerebellar plasticity implements adaptable gain control in a manipulation task: A closed-loop robotic simulation.\nFrontiers in Neural Circuits\n7\n, 159 (2013).\n[13]\nBlack, K.\net al.\nÏ€\n0.5\n\\pi_{0.5}\n: A vision-language-action model with open-world generalization.\n9th Annual Conference on Robot Learning\n(2025).\n[14]\nZhao, Q., Zhang, L., Zhang, H., Jiang, H., Cui, K., Wu, Z., Liu, J., Zhao, M., Tian, F., Hu, B. LSNN model: A lightweight spiking neural network-based depression classification model for wearable EEG sensors.\nIEEE Transactions on Mobile Computing\n(2025).\n[15]\nKim, M. J., Finn, C., and Liang, P. Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success.\nArXiv\nabs/2502.19645\n(2025).\n[16]\nCen, J., Yu, C., Yuan, H., Jiang, Y., Huang, S., Guo, J., Li, X., Song, Y., Luo, H., Wang, F., Zhao, D., and Chen, H. WorldVLA: Towards Autoregressive Action World Model.\nArXiv\nabs/2506.21539\n(2025).\n[17]\nWang, Y.-Q., Li, X., Wang, W., Zhang, J., Li, Y., Chen, Y., Wang, X., and Zhang, Z. Unified Vision-Language-Action Model.\nArXiv\nabs/2506.19850\n(2025).\n[18]\nLiu, B., Zhu, Y., Gao, C., Feng, Y., Liu, Q., Zhu, Y., and Stone, P. Libero: Benchmarking knowledge transfer for lifelong robot learning.\nAdvances in Neural Information Processing Systems\n36\n, 44776â€“44791 (2023).\n[19]\nLi, J., Li, D., Savarese, S., and Hoi, S. C. H. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In\nInternational Conference on Machine Learning\n(2023).\n[20]\nFeldman, D. E. The spike-timing dependence of plasticity.\nNeuron\n75\n, 556â€“571 (2012).\n[21]\nCaporale, N. and Dan, Y. Spike timingâ€“dependent plasticity: a Hebbian learning rule.\nAnnu. Rev. Neurosci.\n31\n, 25â€“46 (2008).\n[22]\nNeftci, E. O., Mostafa, H., and Zenke, F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks.\nIEEE Signal Processing Magazine\n36\n, 51â€“63 (2019).\n[23]\nPerez, E., Strub, F., De Vries, H., Dumoulin, V., and Courville, A. FiLM: Visual reasoning with a general conditioning layer. In\nProceedings of the AAAI conference on artificial intelligence\n32\n, (2018).\n[24]\nAbbott, L. F. Lapicqueâ€™s introduction of the integrate-and-fire model neuron (1907).\nBrain research bulletin\n50\n, 303â€“304 (1999).\n[25]\nAkopyan, F., Sawada, J., Cassidy, A., Alvarez-Icaza, R., Arthur, J., Merolla, P., Imam, N., Nakamura, Y., Datta, P., Nam, G.-J., et al. Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip.\nIEEE transactions on computer-aided design of integrated circuits and systems\n34\n, 1537â€“1557 (2015).\n[26]\nDavies, M., Srinivasa, N., Lin, T.-H., Chinya, G., Cao, Y., Choday, S. H., Dimou, G., Joshi, P., Imam, N., Jain, S., et al. Loihi: A neuromorphic manycore processor with on-chip learning.\nIeee Micro\n38\n, 82â€“99 (2018).\n[27]\nQiu, Z., Wang, Z., Zheng, B., Huang, Z., Wen, K., Yang, S., Men, R., Yu, L., Huang, F., Huang, S., Liu, D., Zhou, J., and Lin, J. Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free. In\nThe Thirty-ninth Annual Conference on Neural Information Processing Systems\n(2025).\n[28]\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling.\narXiv preprint arXiv:1412.3555\n(2014).\n[29]\nPei, J., Deng, L., Song, S., Zhao, M., Zhang, Y., Wu, S., Wang, G., Zou, Z., Wu, Z., He, W., et al. Towards artificial general intelligence with hybrid Tianjic chip architecture.\nNature\n572\n, 106â€“111 (2019).\n[30]\nLauri, M., Hsu, D., and Pajarinen, J. Partially observable markov decision processes in robotics: A survey.\nIEEE Transactions on Robotics\n39\n, 21â€“40 (2022).\n[31]\nFei, S., Wang, S., Shi, J., Dai, Z., Cai, J., Qian, P., Ji, L., He, X., Zhang, S., Fei, Z. and others. Libero-plus: In-depth robustness analysis of vision-language-action models.\narXiv preprint arXiv:2510.13626\n(2025).\n[32]\nFang, W., Yu, Z., Chen, Y., Huang, T., Masquelier, T., and Tian, Y. Deep residual learning in spiking neural networks.\nAdvances in Neural Information Processing Systems\n34\n, 21056â€“21069 (2021).\n[33]\nHu, Y., Tang, H., and Pan, G. Spiking deep residual networks.\nIEEE Transactions on Neural Networks and Learning Systems\n34\n, 5200â€“5205 (2021).\n[34]\nGuo, W., Sun, Y., Xu, Y., Qiao, Z., Yang, Y., and Xiong, H. SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition with Jaccard Attentive Spiking Neural Network.\nAdvances in Neural Information Processing Systems\n37\n, 36717â€“36747 (2024).\n[35]\nRao, A., Plank, P., Wild, A., and Maass, W. A long short-term memory for AI applications in spike-based neuromorphic hardware.\nNature Machine Intelligence\n4\n, 467â€“479 (2022).\n[36]\nRathi, N., Chakraborty, I., Kosta, A., Sengupta, A., Ankit, A., Panda, P., and Roy, K. Exploring Neuromorphic Computing Based on Spiking Neural Networks: Algorithms to Hardware.\nACM Computing Surveys\n55\n, 1â€“49 (2022).\n[37]\nLiu, Y., Chen, W., Bai, Y., Liang, X., Li, G., Gao, W., Lin, L. Aligning cyber space with physical world: A comprehensive survey on embodied AI.\nIEEE/ASME Transactions on Mechatronics\n, 1â€“22 (2025).\n[38]\nBruel, A., AbadÃ­a, I., Collin, T., Sakr, I., Lorach, H., Luque, N. R., Ros, E., Ijspeert, A. The spinal cord facilitates cerebellar upper limb motor learning and control; inputs from neuromusculoskeletal simulation.\nPLOS Computational Biology\n20\n, e1011008 (2024).\n[39]\nH. Lorach, A. GÃ¡lvez, V. Spagnolo, F. Martel, S. Karakas, N. Intering, M. Vat,\nO. Faivre, C. Harte, S. Komi, J. Ravier, T. Collin, L. Coquoz, I. Sakr,\nE. Baaklini, S. D. Hernandez-Charpak, G. Dumont, R. Buschman, N. Buse,\nT. Denison, I. V. van Nes, L. Asboth, A. Watrin, L. Struber, F. Sauter-Starace,\nL. Langar, V. Auboiroux, S. Carda, S. ChabardÃ¨s, T. Aksenova,\nR. Demesmaeker, G. Charvet, J. Bloch, and G. Courtine,\nâ€œWalking naturally after spinal cord injury using a brainâ€“spine interface,â€\nNature\n, vol. 618, pp. 126â€“133, 2023.\n[40]\nNiu, Z., Zhong, G., Yu, H. A review on the attention mechanism of deep learning.\nNeurocomputing\n452\n, 48â€“62 (2021).\n[41]\nK. Kawaharazuka\net al.\n,\nâ€œVision-Language-Action Models for Robotics: A Review Towards Real-World Applications,â€\nIEEE Access\n, 2025.\n[42]\nYang, S., Wang, J., Zhang, N., Deng, B., Pang, Y., Azghadi, M. CerebelluMorphic: Large-scale neuromorphic model and architecture for supervised motor learning.\nIEEE Transactions on Neural Networks and Learning Systems\n33\n, 4398â€“4412 (2022).\n[43]\nBlack, K., Galliker, M. Y., Levine, S. Real-time execution of action chunking flow policies.\nThe Thirty-ninth Annual Conference on Neural Information Processing Systems\n(2025).\n[44]\nSuÃ¡rez, L. E., Richards, B. A., Lajoie, G., Misic, B. Learning function from structure in neuromorphic networks.\nNature Machine Intelligence\n3\n, 771â€“786 (2021).",
    "preview_text": "Recent advances in embodied intelligence have leveraged massive scaling of data and model parameters to master natural-language command following and multi-task control. In contrast, biological systems demonstrate an innate ability to acquire skills rapidly from sparse experience. Crucially, current robotic policies struggle to replicate the dynamic stability, reflexive responsiveness, and temporal memory inherent in biological motion. Here we present Neuromorphic Vision-Language-Action (NeuroVLA), a framework that mimics the structural organization of the bio-nervous system between the cortex, cerebellum, and spinal cord. We adopt a system-level bio-inspired design: a high-level model plans goals, an adaptive cerebellum module stabilizes motion using high-frequency sensors feedback, and a bio-inspired spinal layer executes lightning-fast actions generation. NeuroVLA represents the first deployment of a neuromorphic VLA on physical robotics, achieving state-of-the-art performance. We observe the emergence of biological motor characteristics without additional data or special guidance: it stops the shaking in robotic arms, saves significant energy(only 0.4w on Neuromorphic Processor), shows temporal memory ability and triggers safety reflexes in less than 20 milliseconds.\n\nA Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control\nWeiyu Guo\nThe Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\nAI\n2\nRobotics, Shenzhen, China\nHe Zhang\nThe Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\nAI\n2\nRobotics, Shenzhen, China\nPengteng Li\nThe Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\nAI\n2\nRobotics, Shenzhen, China\nTiefu Cai\nThe Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\nAI\n2",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "VLA",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å—ç”Ÿç‰©ç¥ç»ç³»ç»Ÿå¯å‘çš„ç¥ç»å½¢æ€è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶ï¼Œç”¨äºå®ç°æµç•…ã€å¿«é€Ÿåå°„çš„æœºå™¨äººæ§åˆ¶ï¼Œåœ¨ç‰©ç†æœºå™¨äººä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šã€Šä¸€ç§ç”¨äºæµç•…å¿«é€Ÿåå°„å¼æœºå™¨äººæ§åˆ¶çš„è„‘å¯å‘å…·èº«æ™ºèƒ½ã€‹\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nå½“å‰ï¼ŒåŸºäºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„å…·èº«æ™ºèƒ½åœ¨å¤æ‚ä»»åŠ¡è§„åˆ’æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å…¶æ¶æ„åœ¨æ¨¡ä»¿ç”Ÿç‰©è¿åŠ¨æ™ºèƒ½æ–¹é¢å­˜åœ¨æ ¹æœ¬æ€§ç¼ºé™·ã€‚ç°æœ‰æ¨¡å‹æ™®éå­˜åœ¨**æ—¶é—´ç›²åŒº**ã€æ— æ³•å¤„ç†é«˜é¢‘ä¿¡å·ã€åŠ¨ä½œæŠ–åŠ¨ä¸¥é‡ã€åŠŸè€—å·¨å¤§ï¼Œä¸”ç¼ºä¹å¿«é€Ÿåå°„èƒ½åŠ›ï¼Œéš¾ä»¥åº”å¯¹åŠ¨æ€ç‰©ç†ç¯å¢ƒä¸­çš„çªå‘æ‰°åŠ¨ã€‚ä¸ºè§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæœ¬ç ”ç©¶è½¬å‘**ç”Ÿç‰©è¿åŠ¨ç¥ç»ç³»ç»Ÿ**å¯»æ±‚çµæ„Ÿï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿå…¶å±‚æ¬¡åŒ–åˆ†å·¥æ¶æ„ï¼Œèµ‹äºˆæœºå™¨äººç”Ÿç‰©èˆ¬çš„åŠ¨æ€ç¨³å®šæ€§ã€åå°„å“åº”èƒ½åŠ›å’Œèƒ½é‡æ•ˆç‡ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡ºäº† **Neuromorphic Vision-Language-Action (NeuroVLA)** æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº**ç³»ç»Ÿçº§çš„ç”Ÿç‰©å¯å‘å¼åˆ†å±‚è®¾è®¡**ï¼š\n- **çš®è´¨æ¨¡å—**ï¼šåŸºäºé¢„è®­ç»ƒå¤§æ¨¡å‹ï¼ˆå¦‚Qwen-VLï¼‰è¿›è¡Œè¯­ä¹‰ç†è§£å’Œé«˜çº§ç›®æ ‡è§„åˆ’ï¼Œç”Ÿæˆä½é¢‘è¿åŠ¨æ„å›¾ã€‚\n- **å°è„‘æ¨¡å—**ï¼šä½œä¸º**è‡ªé€‚åº”æ»¤æ³¢å™¨**ï¼Œæ¥æ”¶é«˜é¢‘æœ¬ä½“æ„Ÿè§‰ï¼ˆå…³èŠ‚çŠ¶æ€ã€å…­ç»´åŠ›/åŠ›çŸ©ï¼‰åé¦ˆï¼Œé€šè¿‡é—¨æ§ç‰¹å¾çº¿æ€§è°ƒåˆ¶ï¼ˆFiLMï¼‰æœºåˆ¶å®æ—¶è°ƒæ•´å’Œç¨³å®šè¿åŠ¨æŒ‡ä»¤ï¼ŒæŠ‘åˆ¶æŠ–åŠ¨ï¼Œå¹¶å…·å¤‡æ—¶åºè®°å¿†èƒ½åŠ›ã€‚\n- **è„Šé«“æ¨¡å—**ï¼šåœ¨**ç¥ç»å½¢æ€èŠ¯ç‰‡**ä¸Šå®ç°ï¼Œé‡‡ç”¨**è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰** ä½œä¸ºæ‰§è¡Œæ¥å£ã€‚å…¶äº‹ä»¶é©±åŠ¨çš„ç¨€ç–æ€§å’Œæ³„æ¼ç§¯åˆ†å‘æ”¾ï¼ˆLIFï¼‰ç¥ç»å…ƒçš„è†œç”µä½åŠ¨åŠ›å­¦ï¼Œå®ç°äº†â€œæŒ‰éœ€æ¿€æ´»â€çš„æé«˜èƒ½æ•ˆå’Œå†…åœ¨çš„æ—¶åºè¿ç»­æ€§ï¼Œå¹¶èƒ½è§¦å‘æ¯«ç§’çº§çš„å¿«é€Ÿå®‰å…¨åå°„ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\n- **è¿åŠ¨å¹³æ»‘æ€§**ï¼šå°è„‘æ¨¡å—å°†è¿åŠ¨åŠ åŠ é€Ÿåº¦ï¼ˆjerkï¼‰é™ä½äº†è¶…è¿‡75%ï¼Œæ˜¾è‘—æ¶ˆé™¤äº†æœºæ¢°è‡‚çš„â€œæ„å‘æ€§éœ‡é¢¤â€ã€‚\n- **å¿«é€Ÿåå°„**ï¼šè„Šé«“æ¨¡å—èƒ½åˆ©ç”¨åŠ›ä¼ æ„Ÿå™¨ä¿¡å·ï¼Œåœ¨**20æ¯«ç§’å†…**è§¦å‘é¿ç¢°åå°„ï¼Œè¿œè¶…ä¼ ç»ŸVLAæ¨¡å‹ï¼ˆ>200æ¯«ç§’ï¼‰çš„å“åº”é€Ÿåº¦ã€‚\n- **èƒ½é‡æ•ˆç‡**ï¼šSNNçš„ç¨€ç–ç‰¹æ€§ä½¿å…¶åœ¨é™æ€ä¿æŒé˜¶æ®µèƒ½è€—æä½ï¼Œåœ¨ç¥ç»å½¢æ€å¤„ç†å™¨ä¸Šå•æ¬¡æ¨ç†ä»…æ¶ˆè€—**0.87æ¯«ç„¦è€³**ã€‚\n- **ä»»åŠ¡æ€§èƒ½**ï¼šåœ¨LIBEROåŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œç”ŸåŒ–æ“ä½œä»»åŠ¡ä¸­ï¼ŒNeuroVLAåœ¨é•¿æ—¶åºä»»åŠ¡ã€ç²¾ç»†æ“ä½œå’ŒåŠ¨æ€æ‰°åŠ¨æ¢å¤æ–¹é¢å‡æ˜¾è‘—ä¼˜äºOpenVLAç­‰å…ˆè¿›åŸºçº¿æ¨¡å‹ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\næœ¬ç ”ç©¶è¯æ˜äº†**å°†ç”Ÿç‰©è¿åŠ¨æ§åˆ¶çš„è®¡ç®—åŸç†å¼•å…¥æœºå™¨äººå­¦çš„å·¨å¤§æ½œåŠ›**ã€‚NeuroVLAé€šè¿‡åŠŸèƒ½è§£è€¦çš„å±‚æ¬¡åŒ–æ¶æ„ï¼Œè€Œéå•çº¯æ‰©å¤§æ•°æ®ä¸æ¨¡å‹è§„æ¨¡ï¼ŒæˆåŠŸèµ‹äºˆäº†æœºå™¨äºº**ç±»ç”Ÿç‰©çš„æµç•…æ€§ã€é€‚åº”æ€§å’Œç”Ÿå­˜æœ¬èƒ½**ã€‚å®ƒä¸ºè§£å†³å½“å‰å…·èº«æ™ºèƒ½åœ¨**å»¶è¿Ÿã€èƒ½è€—å’Œç‰©ç†å®‰å…¨æ€§**æ–¹é¢çš„æ ¸å¿ƒæŒ‘æˆ˜æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œä¸ºæœªæ¥å¼€å‘çœŸæ­£ç¨³å¥ã€é«˜æ•ˆçš„è‡ªä¸»æœºå™¨äººç³»ç»Ÿå¥ å®šäº†å¯æ‰©å±•çš„è®¡ç®—æ¡†æ¶ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **æ ¸å¿ƒé—®é¢˜**\nå½“å‰ä¸»æµçš„**è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹**åœ¨æœºå™¨äººæ§åˆ¶ä¸­å­˜åœ¨ä¸‰ä¸ªå…³é”®ç“¶é¢ˆï¼š\n1.  **æ—¶é—´ç›²åŒº**ï¼šä¾èµ–å•æ­¥è§†è§‰è§‚æµ‹ï¼Œç¼ºä¹å¯¹ä»»åŠ¡æ‰§è¡Œè¿›åº¦å’Œæ—¶åºèŠ‚å¥çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚\n2.  **åŠ¨ä½œæŠ–åŠ¨ä¸åå°„è¿Ÿé’**ï¼šæ— æ³•å¤„ç†é«˜é¢‘æœ¬ä½“æ„Ÿè§‰ä¿¡å·ï¼Œå¯¼è‡´åŠ¨ä½œä¸å¹³æ»‘ï¼Œä¸”æ— æ³•åœ¨åŠ¨æ€åœºæ™¯ä¸­å¿«é€Ÿåå°„ã€‚\n3.  **èƒ½æ•ˆä½ä¸‹**ï¼šä¸ºæ¯ä¸ªç»†å¾®çš„åŠ¨ä½œè°ƒæ•´éƒ½è°ƒç”¨åºå¤§çš„åŸºç¡€æ¨¡å‹ï¼Œè®¡ç®—å¼€é”€å’ŒåŠŸè€—å·¨å¤§ã€‚\n\n### **æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº† **NeuroVLA** æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº**å€Ÿé‰´ç”Ÿç‰©ç¥ç»ç³»ç»Ÿçš„å±‚æ¬¡åŒ–æ¶æ„ï¼Œå°†è¯­ä¹‰è§„åˆ’ä¸é«˜é¢‘è¿åŠ¨æ§åˆ¶è¿›è¡Œè§£è€¦**ï¼Œå¹¶é¦–æ¬¡åœ¨ç‰©ç†æœºå™¨äººä¸Šéƒ¨ç½²äº†**ç¥ç»å½¢æ€çš„VLAç³»ç»Ÿ**ã€‚\n\n#### **1. ç”Ÿç‰©å¯å‘çš„ä¸‰çº§æ¶æ„**\næ¨¡ä»¿å¤§è„‘çš®å±‚ã€å°è„‘å’Œè„Šé«“çš„åŠŸèƒ½åˆ’åˆ†ï¼Œæ„å»ºäº†ä¸€ä¸ªå¼‚æ„è®¡ç®—ç³»ç»Ÿï¼š\n- **çš®å±‚æ¨¡å—**ï¼šè¿è¡Œåœ¨CUDAè®¡ç®—å±‚ã€‚ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œé«˜å±‚è¯­ä¹‰ç†è§£å’Œç›®æ ‡è§„åˆ’ï¼Œç”Ÿæˆä½é¢‘çš„â€œæ„å›¾ä¿¡å·â€ã€‚\n- **å°è„‘æ¨¡å—**ï¼šè¿è¡Œåœ¨CUDAè®¡ç®—å±‚ã€‚ä½œä¸ºä¸€ä¸ª**è‡ªé€‚åº”æ»¤æ³¢å™¨**ï¼Œæ¥æ”¶é«˜é¢‘ï¼ˆ>200 Hzï¼‰çš„æœ¬ä½“æ„Ÿè§‰å’ŒåŠ›åé¦ˆä¿¡å·ï¼Œå¯¹çš®å±‚æŒ‡ä»¤è¿›è¡Œå®æ—¶å¢ç›Šè°ƒåˆ¶å’Œè½¨è¿¹å¹³æ»‘ï¼ŒæŠ‘åˆ¶æŠ–åŠ¨ï¼Œå¹¶å…·å¤‡æ—¶åºè®°å¿†èƒ½åŠ›ã€‚\n- **è„Šé«“æ¨¡å—**ï¼šè¿è¡Œåœ¨**ç¥ç»å½¢æ€èŠ¯ç‰‡å±‚**ã€‚é‡‡ç”¨**è„‰å†²ç¥ç»ç½‘ç»œ**ï¼Œå°†è°ƒåˆ¶åçš„æŒ‡ä»¤è½¬æ¢ä¸ºç²¾ç¡®çš„åŠ¨ä½œã€‚å…¶**äº‹ä»¶é©±åŠ¨**çš„ç‰¹æ€§å®ç°äº†æä½çš„åŠŸè€—ï¼Œå¹¶æ”¯æŒè¶…å¿«é€Ÿï¼ˆ<20 msï¼‰çš„å®‰å…¨åå°„ã€‚\n\n#### **2. å…³é”®æŠ€æœ¯æœºåˆ¶**\n- **è¯­ä¹‰è’¸é¦**ï¼šé€šè¿‡**æŸ¥è¯¢å˜æ¢å™¨**ï¼Œå°†é«˜ç»´çš„VLMè¡¨å¾å‹ç¼©ä¸ºç´§å‡‘çš„ã€ä»»åŠ¡ç›¸å…³çš„è¿åŠ¨æ„å›¾ï¼Œç±»ä¼¼äºç”Ÿç‰©ä¸­çš„**çš®è´¨è„Šé«“æŸ**ã€‚\n- **å°è„‘çš„ä¸‰é‡åŠŸèƒ½ç¯è·¯æ¨¡æ‹Ÿ**ï¼š\n    - **è„Šé«“å°è„‘ç¯**ï¼šåˆ©ç”¨æœ¬ä½“æ„Ÿè§‰è¿›è¡Œå¢ç›Šæ§åˆ¶ï¼Œå°†è¿åŠ¨æŠ–åŠ¨é™ä½è¶…è¿‡75%ã€‚\n    - **å‰åº­å°è„‘ç¯**ï¼šå¤„ç†åŠ›è§‰åé¦ˆï¼Œå®ç°å¿«é€Ÿå§¿æ€è°ƒæ•´å’Œé¿éšœåå°„ã€‚\n    - **å¤§è„‘å°è„‘ç¯**ï¼šç¼–ç æ—¶åºèŠ‚å¥ï¼Œä½œä¸ºè¿åŠ¨è®°å¿†ï¼Œä½¿ä»»åŠ¡æ‰§è¡Œä¸å—è§†è§‰å¹²æ‰°ã€‚\n- **ç¥ç»å½¢æ€è„Šé«“çš„æ‰§è¡Œä¼˜åŠ¿**ï¼š\n    - **äº‹ä»¶é©±åŠ¨ç¨€ç–æ€§**ï¼šä»…åœ¨éœ€è¦æ—¶æ¿€æ´»ç¥ç»å…ƒï¼Œé™æ€ä¿æŒé˜¶æ®µåŠŸè€—æä½ï¼ˆä»…0.4Wï¼‰ã€‚\n    - **å†…åœ¨æ—¶åºè®°å¿†**ï¼šSNNçš„è†œç”µä½åŠ¨åŠ›å­¦è‡ªç„¶ç¼–ç æ—¶é—´ä¾èµ–å…³ç³»ï¼Œä¿éšœäº†é•¿æ—¶ç¨‹ä»»åŠ¡çš„è¿åŠ¨è¿ç»­æ€§ã€‚\n    - **åŠŸèƒ½è§£è€¦**ï¼šç½‘ç»œè‡ªå‘åœ°å°†é«˜ç»´æ§åˆ¶ä¿¡å·åˆ†ç¦»åˆ°ä¸åŒçš„ç¥ç»å…ƒç¾¤ï¼Œæ¨¡ä»¿äº†ç”Ÿç‰©è¿åŠ¨çš®å±‚çš„æ‹“æ‰‘ç»„ç»‡ã€‚\n\n#### **3. ç¡¬ä»¶å®ç°ä¸èƒ½æ•ˆ**\n- å°†è„Šé«“SNNéƒ¨ç½²åœ¨**å®šåˆ¶åŒ–çš„FPGAç¥ç»å½¢æ€å¤„ç†å™¨**ä¸Šï¼Œé‡‡ç”¨è„‰åŠ¨é˜µåˆ—æ¶æ„ã€‚\n- å®ç°äº†**2.19 msçš„è¶…ä½æ¨ç†å»¶è¿Ÿ**å’Œ**æ¯æ¬¡æ¨ç†0.87 mJçš„æä½èƒ½è€—**ã€‚\n\n### **è§£å†³æ–¹æ¡ˆæ€»ç»“**\nè®ºæ–‡é€šè¿‡**æ¶æ„åˆ›æ–°**è€Œéå•çº¯çš„æ•°æ®æˆ–æ¨¡å‹ç¼©æ”¾æ¥è§£å†³æœºå™¨äººæ§åˆ¶é—®é¢˜ï¼š\n1.  **è§£è€¦**ï¼šå°†é«˜å»¶è¿Ÿçš„è¯­ä¹‰è§„åˆ’ä¸ä½å»¶è¿Ÿçš„é«˜é¢‘è¿åŠ¨æ§åˆ¶åˆ†ç¦»åˆ°ä¸åŒçš„è®¡ç®—ç¡¬ä»¶ä¸Šã€‚\n2.  **æ¨¡ä»¿**ï¼šä¸¥æ ¼æ¨¡ä»¿ç”Ÿç‰©è¿åŠ¨ç¥ç»ç³»ç»Ÿçš„å±‚æ¬¡åŒ–åˆ†å·¥å’Œä¿¡å·å¤„ç†æœºåˆ¶ã€‚\n3.  **é›†æˆ**ï¼šé¦–æ¬¡å°†ç¥ç»å½¢æ€è®¡ç®—ä¸VLAæ¨¡å‹æ·±åº¦ç»“åˆï¼Œå¹¶éƒ¨ç½²åˆ°å®ä½“æœºå™¨äººï¼Œå®ç°äº†**ç±»ç”Ÿç‰©çš„è¿åŠ¨ç‰¹æ€§**ï¼ˆå¹³æ»‘ã€èŠ‚èƒ½ã€å…·æ—¶åºè®°å¿†ã€å¿«é€Ÿåå°„ï¼‰ã€‚\n\n**å®é™…ä»·å€¼**ï¼šä¸ºæ„å»ºèƒ½åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­å®‰å…¨ã€æµç•…ã€é«˜æ•ˆè¿è¡Œçš„ä¸‹ä¸€ä»£å…·èº«æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„è®¡ç®—æ¡†æ¶ï¼Œè¯æ˜äº†**ç”Ÿå­˜åå°„åº”ä¼˜å…ˆäºè¯­ä¹‰ç†è§£**çš„è®¾è®¡åŸåˆ™çš„é‡è¦æ€§ã€‚",
            "timestamp": "2026-01-27T15:54:28.549210",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡åˆ†æï¼šNeuroVLA åŠå…¶ç ”ç©¶è„‰ç»œæ¢³ç†\n\nåŸºäºè®ºæ–‡ã€ŠA Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Controlã€‹çš„å†…å®¹ï¼Œæˆ‘ä»¬å¯ä»¥æ¢³ç†å‡ºè¯¥ç ”ç©¶æ–¹å‘ï¼ˆå³â€œå…·èº«æ™ºèƒ½â€ä¸â€œæœºå™¨äººæ§åˆ¶â€ï¼‰çš„å‘å±•è„‰ç»œã€‚è¿™ç¯‡è®ºæ–‡ä»£è¡¨äº†å½“å‰ä¸€ä¸ªé‡è¦çš„æ¼”è¿›æ–¹å‘ï¼š**ä»å•ä¸€ã€é€šç”¨çš„â€œè§†è§‰-è¯­è¨€-åŠ¨ä½œâ€æ¨¡å‹ï¼Œè½¬å‘å—ç”Ÿç‰©ç¥ç»ç³»ç»Ÿå¯å‘çš„ã€å±‚æ¬¡åŒ–ã€å¼‚æ„åŒ–çš„è®¡ç®—æ¶æ„**ã€‚\n\nä»¥ä¸‹å°†æŒ‰ç…§å‡ ä¸ªå…³é”®çš„å‘å±•æ–¹å‘ï¼Œæ¢³ç†å…¶æ¼”è¿›æ­¥éª¤å’Œæ”¹è¿›ä¹‹å¤„ï¼š\n\n### å‘å±•æ–¹å‘ä¸€ï¼šä»â€œæ„ŸçŸ¥-è§„åˆ’-æ‰§è¡Œâ€ä¸²è¡Œæ¶æ„åˆ°â€œç”Ÿç‰©å±‚æ¬¡åŒ–â€å¹¶è¡Œæ¶æ„\n\n1.  **æ—©æœŸå·¥ä½œï¼ˆä¼ ç»ŸVLAæ¨¡å‹ï¼‰ï¼š**\n    *   **ä»£è¡¨æ¨¡å‹ï¼š** RT-2, ä»¥åŠè®ºæ–‡ä¸­æåˆ°çš„ OpenVLAã€UniVLAã€WorldVLA ç­‰åŸºçº¿æ¨¡å‹ã€‚\n    *   **æ ¸å¿ƒæ€æƒ³ï¼š** æ„å»ºä¸€ä¸ªåºå¤§çš„ã€ç«¯åˆ°ç«¯çš„â€œè§†è§‰-è¯­è¨€-åŠ¨ä½œâ€æ¨¡å‹ã€‚é€šå¸¸ä½¿ç”¨ä¸€ä¸ªå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºä¸»å¹²ï¼Œé€šè¿‡å¾®è°ƒæˆ–é€‚é…å™¨ï¼Œç›´æ¥è¾“å‡ºæœºå™¨äººåŠ¨ä½œã€‚**è¿™æ˜¯ä¸€ä¸ªâ€œå•ä¸€è®¡ç®—æµâ€çš„èŒƒå¼**ï¼šè§†è§‰å’Œè¯­è¨€è¾“å…¥ç»è¿‡åŒä¸€ä¸ªæ·±åº¦ç½‘ç»œï¼Œç›´æ¥ç”ŸæˆåŠ¨ä½œã€‚\n    *   **ä¸»è¦é—®é¢˜ï¼ˆæœ¬æ–‡æŒ‡å‡ºçš„ï¼‰ï¼š**\n        *   **æ—¶é—´ç›²åŒºï¼š** æ¨¡å‹åŸºäºå½“å‰æ—¶åˆ»çš„è§†è§‰è§‚å¯Ÿï¼Œç¼ºä¹å¯¹ä»»åŠ¡æ‰§è¡Œè¿›åº¦ï¼ˆå¦‚å‘¨æœŸæ€§åŠ¨ä½œï¼‰çš„**æ—¶é—´è®°å¿†**ã€‚\n        *   **åŠ¨ä½œæŠ–åŠ¨ä¸å»¶è¿Ÿï¼š** æ— æ³•å¤„ç†é«˜é¢‘æœ¬ä½“æ„Ÿè§‰ä¿¡å·ï¼Œå¯¼è‡´åŠ¨ä½œä¸å¹³æ»‘ï¼ˆâ€œæ„å‘æ€§éœ‡é¢¤â€ï¼‰ï¼Œä¸”æ— æ³•å¯¹åŠ¨æ€æ‰°åŠ¨ï¼ˆå¦‚ç¢°æ’ï¼‰åšå‡º**å¿«é€Ÿåå°„**ï¼ˆååº”å»¶è¿Ÿ >200msï¼‰ã€‚\n        *   **èƒ½æ•ˆä½ä¸‹ï¼š** ä¸ºæ¯ä¸€ä¸ªç»†å¾®çš„åŠ¨ä½œè°ƒæ•´éƒ½è°ƒç”¨å¤§å‹åŸºç¡€æ¨¡å‹ï¼Œè®¡ç®—å¼€é”€å’ŒåŠŸè€—å·¨å¤§ã€‚\n\n2.  **æœ¬æ–‡å·¥ä½œï¼ˆNeuroVLAï¼‰ï¼š**\n    *   **æ ¸å¿ƒæ”¹è¿›ï¼š** **å¼•å…¥ä¸¥æ ¼çš„ç”Ÿç‰©å±‚æ¬¡åŒ–åˆ†å·¥**ï¼Œå°†å•ä¸€æ¨¡å‹åˆ†è§£ä¸ºä¸‰ä¸ªåŠŸèƒ½è§£è€¦çš„æ¨¡å—ï¼Œåˆ†åˆ«å¯¹åº”å¤§è„‘çš®å±‚ã€å°è„‘å’Œè„Šé«“ã€‚\n    *   **å…·ä½“åˆ›æ–°ï¼š**\n        *   **çš®å±‚æ¨¡å—ï¼š** ä¸“æ³¨äº**è¯­ä¹‰è§„åˆ’å’Œç›®æ ‡ç”Ÿæˆ**ï¼Œå¤„ç†é«˜å»¶è¿Ÿã€é«˜ç»´åº¦çš„è§†è§‰-è¯­è¨€ä¿¡æ¯ã€‚ä½¿ç”¨Q-Formerä½œä¸ºâ€œä¿¡æ¯ç“¶é¢ˆâ€ï¼Œå°†å¤æ‚çš„è¯­ä¹‰æ„å›¾æç‚¼ä¸ºç´§å‡‘çš„è¿åŠ¨ç›®æ ‡ä¿¡å·ã€‚\n        *   **å°è„‘æ¨¡å—ï¼š** ä½œä¸º**é«˜é¢‘è‡ªé€‚åº”æ»¤æ³¢å™¨**ï¼Œæ¥æ”¶å¯†é›†çš„æœ¬ä½“æ„Ÿè§‰ï¼ˆå…³èŠ‚è§’åº¦ã€é€Ÿåº¦ï¼‰å’ŒåŠ›è§‰åé¦ˆã€‚å®ƒå¹³æ»‘è¿åŠ¨è½¨è¿¹ã€æ ¡æ­£è¯¯å·®ï¼Œå¹¶å…·å¤‡**æ—¶é—´è®°å¿†**èƒ½åŠ›ï¼Œèƒ½é¢„æµ‹ä»»åŠ¡é˜¶æ®µã€‚\n        *   **è„Šé«“æ¨¡å—ï¼š** ä½œä¸º**è„‰å†²ç¥ç»ç½‘ç»œçš„æ‰§è¡Œå±‚**ï¼Œéƒ¨ç½²åœ¨ç¥ç»å½¢æ€èŠ¯ç‰‡ä¸Šã€‚å®ƒå°†è°ƒåˆ¶åçš„å‘½ä»¤è½¬åŒ–ä¸ºå¹³æ»‘åŠ¨ä½œï¼Œå¹¶å®ç°**è¶…å¿«é€Ÿï¼ˆ<20msï¼‰çš„å®‰å…¨åå°„**ï¼Œå®Œå…¨ç»•è¿‡æ…¢é€Ÿçš„çš®å±‚å›è·¯ã€‚\n    *   **å¸¦æ¥çš„ä¼˜åŠ¿ï¼š**\n        *   **æ€§èƒ½æå‡ï¼š** æ¶ˆé™¤äº†75%ä»¥ä¸Šçš„åŠ¨ä½œæŠ–åŠ¨ï¼Œåœ¨éœ€è¦æ—¶ç©ºç²¾åº¦çš„ä»»åŠ¡ï¼ˆå¦‚å€’æ°´ã€æ‘‡æ¯ï¼‰ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»ŸVLAæ¨¡å‹ã€‚\n        *   **å¿«é€Ÿåå°„ï¼š** å®ç°äº†ç”Ÿå­˜ä¼˜å…ˆçš„å¿«é€Ÿåå°„èƒ½åŠ›ï¼Œåœ¨ç¢°æ’ç­‰å®‰å…¨å…³é”®åœºæ™¯ä¸­æˆåŠŸç‡ä»0%æå‡è‡³54.8%ã€‚\n        *   **èƒ½æ•ˆé©å‘½ï¼š** è„Šé«“æ¨¡å—çš„**äº‹ä»¶é©±åŠ¨ç¨€ç–æ€§**ä½¿å¾—åœ¨é™æ€å§¿åŠ¿ä¿æŒæ—¶èƒ½è€—æä½ï¼ˆä»…0.4Wï¼‰ï¼Œä¸ºè¾¹ç¼˜éƒ¨ç½²æä¾›äº†å¯èƒ½ã€‚\n\n### å‘å±•æ–¹å‘äºŒï¼šä»â€œè¿ç»­ç¨ å¯†è®¡ç®—â€åˆ°â€œäº‹ä»¶é©±åŠ¨ç¨€ç–è®¡ç®—â€\n\n1.  **å…ˆå‰å·¥ä½œï¼ˆåŸºäºANNçš„è¿ç»­è®¡ç®—ï¼‰ï¼š**\n    *   å‡ ä¹æ‰€æœ‰ä¼ ç»Ÿçš„VLAå’Œæœºå™¨äººç­–ç•¥æ¨¡å‹éƒ½åŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼Œåœ¨**æ¯ä¸ªæ—¶é—´æ­¥éƒ½è¿›è¡Œç¨ å¯†çš„çŸ©é˜µä¹˜æ³•è¿ç®—**ï¼Œæ— è®ºæœºå™¨äººæ˜¯å¦åœ¨è¿åŠ¨ã€‚\n    *   **é—®é¢˜ï¼š** è¿™å¯¼è‡´äº†å·¨å¤§çš„**è®¡ç®—å†—ä½™å’Œèƒ½é‡æµªè´¹**ï¼Œä¸ç¬¦åˆç”Ÿç‰©ç¥ç»ç³»ç»Ÿâ€œæŒ‰éœ€æ¿€æ´»â€çš„èŠ‚èƒ½åŸåˆ™ï¼Œé™åˆ¶äº†åœ¨ç”µæ± ä¾›ç”µæœºå™¨äººä¸Šçš„é•¿æœŸéƒ¨ç½²ã€‚\n\n2.  **æœ¬æ–‡å·¥ä½œï¼ˆåŸºäºSNNçš„ç¥ç»å½¢æ€è®¡ç®—ï¼‰ï¼š**\n    *   **æ ¸å¿ƒæ”¹è¿›ï¼š** åœ¨**è„Šé«“æ¨¡å—**ä¸­å…¨é¢é‡‡ç”¨**è„‰å†²ç¥ç»ç½‘ç»œ**ï¼Œå¹¶éƒ¨ç½²åœ¨å®šåˆ¶åŒ–çš„ç¥ç»å½¢æ€å¤„ç†å™¨ï¼ˆFPGAå®ç°ï¼‰ä¸Šã€‚\n    *   **å…·ä½“åˆ›æ–°ï¼š**\n        *   **äº‹ä»¶é©±åŠ¨ç¨€ç–æ€§ï¼š** SNNç¥ç»å…ƒåªåœ¨è¾“å…¥åˆºæ¿€è¶³å¤Ÿå¼ºæ—¶æ‰å‘æ”¾è„‰å†²ã€‚è®ºæ–‡æ˜¾ç¤ºï¼Œåœ¨â€œé™æ€å§¿åŠ¿ã€åŠ¨æ€å¤¹çˆªâ€é˜¶æ®µï¼Œæ§åˆ¶æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€çš„ç¥ç»å…ƒä¿æŒé™é»˜ï¼Œè€Œæ§åˆ¶å¤¹çˆªçš„ç¥ç»å…ƒæ´»è·ƒã€‚**å®ç°äº†â€œæŒ‰éœ€è®¡ç®—â€**ã€‚\n        *   **å†…åœ¨æ—¶é—´åŠ¨åŠ›å­¦ï¼š** LIFç¥ç»å…ƒçš„è†œç”µä½å…·æœ‰æ³„æ¼å’Œç´¯ç§¯ç‰¹æ€§ï¼Œ**å¤©ç„¶åœ°ç¼–ç äº†æ—¶é—´ä¾èµ–å…³ç³»**ï¼Œèµ·åˆ°äº†çŸ­æœŸå·¥ä½œè®°å¿†çš„ä½œç”¨ã€‚è¿™ä½¿å¾—Multi-step SNNåœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸Šä¼˜äºå•æ­¥æ¨¡å‹ã€‚\n        *   **ä¸“ç”¨ç¡¬ä»¶åŠ é€Ÿï¼š** è®¾è®¡äº†é‡‡ç”¨è„‰åŠ¨é˜µåˆ—æ¶æ„çš„ç¥ç»å½¢æ€å¤„ç†å™¨ï¼Œåˆ©ç”¨æ—¶ç©ºå¹¶è¡Œæ€§å’Œè„‰å†²ç¨€ç–æ€§æ„ŸçŸ¥è®¡ç®—ï¼Œå®ç°äº†**2.19msçš„è¶…ä½æ¨ç†å»¶è¿Ÿå’Œ0.87mJ/æ¬¡çš„æä½èƒ½è€—**ã€‚\n    *   **å¸¦æ¥çš„ä¼˜åŠ¿ï¼š**\n        *   **è¶…é«˜èƒ½æ•ˆï¼š** ä¸ºçœŸæ­£çš„ä½åŠŸè€—ã€å®æ—¶æœºå™¨äººæ§åˆ¶å¥ å®šäº†åŸºç¡€ã€‚\n        *   **æ¶Œç°ç‰¹æ€§ï¼š** ç½‘ç»œè‡ªå‘å‡ºç°äº†**åŠŸèƒ½è§£è€¦**ï¼ˆä¸åŒç¥ç»å…ƒç¾¤æ§åˆ¶ä¸åŒè‡ªç”±åº¦ï¼‰å’Œ**æ—¶é—´é²æ£’æ€§**ï¼Œæ›´æ¥è¿‘ç”Ÿç‰©ç³»ç»Ÿçš„ç»„ç»‡æ–¹å¼ã€‚\n\n### å‘å±•æ–¹å‘ä¸‰ï¼šä»â€œå¼€ç¯è§†è§‰é©±åŠ¨â€åˆ°â€œå¤šæ¨¡æ€é—­ç¯æ„ŸçŸ¥é©±åŠ¨â€\n\n1.  **å…ˆå‰å·¥ä½œï¼ˆè§†è§‰ä¸»å¯¼çš„å¼€ç¯æ§åˆ¶ï¼‰ï¼š**\n    *   è®¸å¤šVLAæ¨¡å‹ä¸¥é‡ä¾èµ–è§†è§‰å¸§ä½œä¸ºä¸»è¦ç”šè‡³å”¯ä¸€çš„çŠ¶æ€è¾“å…¥ã€‚åŠ¨ä½œç”Ÿæˆåï¼Œåœ¨ä¸‹ä¸€ä¸ªè§†è§‰å¸§åˆ°æ¥å‰æ˜¯å¼€ç¯æ‰§è¡Œçš„ã€‚\n    *   **é—®é¢˜ï¼š** å¯¹è§†è§‰é®æŒ¡ã€å…‰ç…§å˜åŒ–ã€çº¹ç†æ”¹å˜éå¸¸æ•æ„Ÿã€‚æ— æ³•æ„ŸçŸ¥ç‰©ç†äº¤äº’ä¸­çš„ç¬æ—¶åŠ›å˜åŒ–ï¼ˆå¦‚ç¢°æ’ï¼‰ï¼Œå¯¼è‡´åœ¨åŠ¨æ€ç¯å¢ƒä¸­é€‚åº”æ€§å·®ã€‚\n\n2.  **æœ¬æ–‡å·¥ä½œï¼ˆæœ¬ä½“æ„Ÿè§‰ä¸åŠ›è§‰é—­ç¯èåˆï¼‰ï¼š**\n    *   **æ ¸å¿ƒæ”¹è¿›ï¼š** **å°†é«˜é¢‘æœ¬ä½“æ„Ÿè§‰ï¼ˆ200Hzï¼‰å’Œ6ç»´åŠ›/åŠ›çŸ©ä¿¡å·ä½œä¸ºæ ¸å¿ƒåé¦ˆæ¨¡æ€**ï¼Œä¸å°è„‘æ¨¡å—å½¢æˆå¿«é€Ÿé—­ç¯ã€‚\n    *   **å…·ä½“åˆ›æ–°ï¼š**\n        *   **å°è„‘ä½œä¸ºå¤šæ¨¡æ€èåˆå™¨ï¼š** å°è„‘æ¨¡å—é€šè¿‡GRUç½‘ç»œæ•´åˆå†å²æœ¬ä½“æ„Ÿè§‰çŠ¶æ€ï¼Œå¹¶åˆ©ç”¨FiLMæœºåˆ¶åŠ¨æ€è°ƒåˆ¶çš®å±‚ä¸‹å‘çš„è¯­ä¹‰æ„å›¾ã€‚**å®ç°äº†è·¨æ¨¡æ€æ„Ÿå®˜é‡åŠ æƒ**ã€‚\n        *   **åŠ›è§‰è§¦å‘å¿«é€Ÿåå°„ï¼š** å°†6ç»´åŠ›ä¼ æ„Ÿå™¨è§†ä¸ºâ€œåŠŸèƒ½æ€§å‰åº­å™¨å®˜â€ï¼Œå…¶ä¿¡å·ç›´æ¥è¾“å…¥è„Šé«“æ¨¡å—çš„å¿«é€Ÿå®‰å…¨åå°„é€šè·¯ï¼Œè§¦å‘æ¯«ç§’çº§çš„æ’¤å›åŠ¨ä½œã€‚\n        *   **ç¼–ç ç‰©ç†åŠ¨åŠ›å­¦ï¼š** åœ¨â€œæ‘‡æ¯â€ä»»åŠ¡ä¸­ï¼Œç³»ç»Ÿä¸æ˜¯è®°å¿†è§†è§‰å¸§åºåˆ—ï¼Œè€Œæ˜¯å°†è¿åŠ¨ç¼–ç ä¸º**æœ¬ä½“æ„Ÿè§‰å’ŒåŠ›ä¿¡å·çš„å‘¨æœŸæ€§è½¨è¿¹**ï¼Œä»è€Œåœ¨è§†è§‰ä¿¡æ¯å¤±æ•ˆæ—¶ä»èƒ½ä¿æŒèŠ‚å¥ã€‚\n    *   **å¸¦æ¥çš„ä¼˜åŠ¿ï¼š**\n        *   **åŠ¨æ€ç¨³å®šæ€§ï¼š** æ˜¾è‘—å¹³æ»‘äº†è¿åŠ¨è½¨è¿¹ï¼ŒæŠµæŠ—è§†è§‰å™ªå£°ã€‚\n        *   **ç‰©ç†äº¤äº’æ™ºèƒ½ï¼š** èƒ½å¤Ÿå®æ—¶å“åº”ç¢°æ’ã€é‡é‡å˜åŒ–ç­‰ç‰©ç†æ‰°åŠ¨ï¼Œå®ç°åœ¨çº¿è½¨è¿¹é‡è§„åˆ’ã€‚\n        *   **æ„Ÿå®˜é€€åŒ–é²æ£’æ€§ï¼š** åœ¨è§†è§‰æ„ŸçŸ¥é™çº§æ—¶ï¼Œä¾é ä¸å˜çš„ç‰©ç†åŠ¨åŠ›å­¦ä»èƒ½ä¿æŒé«˜æ€§èƒ½ã€‚\n\n### æ€»ç»“ä¸å‘å±•è„‰ç»œå›¾\n\n```mermaid\ngraph TD\n    A[ä¼ ç»ŸVLAæ¨¡å‹] --> B[æ ¸å¿ƒé—®é¢˜ï¼š æ—¶é—´ç›²åŒº/åŠ¨ä½œæŠ–åŠ¨/èƒ½æ•ˆä½/åå°„æ…¢];\n    B --> C{æ¼”è¿›æ–¹å‘};\n\n    C --> D1[æ¶æ„å±‚æ¬¡åŒ–];\n    C --> D2[è®¡ç®—ç¨€ç–åŒ–];\n    C --> D3[æ„ŸçŸ¥é—­ç¯åŒ–];\n\n    D1 --> E1[NeuroVLA: çš®å±‚-å°è„‘-è„Šé«“ä¸‰çº§æ¶æ„];\n    D2 --> E2[NeuroVLA: SNNè„Šé«“æ¨¡å— + ç¥ç»å½¢æ€ç¡¬ä»¶];\n    D3 --> E3[NeuroVLA: é«˜é¢‘æœ¬ä½“/åŠ›è§‰é—­ç¯ + å¿«é€Ÿåå°„];\n\n    E1 --> F[æ”¹è¿›ï¼š åŠŸèƒ½è§£è€¦/å¹¶è¡Œå¤„ç†/å„å¸å…¶èŒ];\n    E2 --> F[æ”¹è¿›ï¼š äº‹ä»¶é©±åŠ¨/è¶…ä½åŠŸè€—/å†…åœ¨æ—¶åº];\n    E3 --> F[æ”¹è¿›ï¼š åŠ¨æ€ç¨³å®š/ç‰©ç†äº¤äº’/ç”Ÿå­˜åå°„];\n\n    F --> G[æœ€ç»ˆç›®æ ‡ï¼š å®ç°**æµä½“ã€å¿«é€Ÿã€èŠ‚èƒ½ã€é²æ£’**çš„ç”Ÿç‰©å¯å‘å¼å…·èº«æ™ºèƒ½];\n```\n\n**ç»“è®ºï¼š** æœ¬æ–‡çš„NeuroVLAå¹¶éå­¤ç«‹åˆ›æ–°ï¼Œè€Œæ˜¯ä»£è¡¨äº†å…·èº«æ™ºèƒ½é¢†åŸŸä¸€ä¸ªæ¸…æ™°çš„èŒƒå¼è½¬å˜è¶‹åŠ¿ï¼š**ä»è¿½æ±‚å•ä¸€æ¨¡å‹çš„â€œè§„æ¨¡ç¼©æ”¾â€ï¼Œè½¬å‘å€Ÿé‰´äº¿ä¸‡å¹´è¿›åŒ–æ™ºæ…§çš„â€œç»“æ„åˆ›æ–°â€**ã€‚å®ƒé€šè¿‡æ¨¡æ‹Ÿç¥ç»ç³»ç»Ÿçš„å±‚æ¬¡åŒ–ã€ç¨€ç–åŒ–å’Œå¤šæ¨¡æ€é—­ç¯ç‰¹æ€§ï¼Œåœ¨**è¿åŠ¨è´¨é‡ã€å“åº”é€Ÿåº¦ã€èƒ½é‡æ•ˆç‡å’Œç¯å¢ƒé€‚åº”æ€§**ç­‰å¤šä¸ªå…³é”®ç»´åº¦ä¸Šï¼Œå®ç°äº†å¯¹å‰ä»£å·¥ä½œçš„ç³»ç»Ÿæ€§è¶…è¶Šï¼Œä¸ºä¸‹ä¸€ä»£æœºå™¨äººæ§åˆ¶æ¶æ„æä¾›äº†ä¸€ä¸ªå¼ºæœ‰åŠ›çš„è“å›¾ã€‚æœªæ¥çš„å·¥ä½œï¼ˆå¦‚å…¨ç¥ç»å½¢æ€èŠ¯ç‰‡ä¸Šçš„åœ¨çº¿STDPå­¦ä¹ ï¼‰å°†æ²¿ç€è¿™æ¡ç”Ÿç‰©å¯å‘çš„é“è·¯ç»§ç»­æ·±åŒ–ã€‚",
            "timestamp": "2026-01-27T15:55:37.760691",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œä½œè€…å›¢é˜Ÿï¼ˆå°¤å…¶æ˜¯ç¬¬ä¸€ä½œè€… Weiyu Guoï¼‰åœ¨æœ¬æ–‡ä¸­å¼•ç”¨äº†ä¸€é¡¹ç›´æ¥ç›¸å…³çš„å‰ä½œã€‚ä»¥ä¸‹æ˜¯åŸºäºè®ºæ–‡å‚è€ƒæ–‡çŒ®å’Œå†…å®¹çš„è¯¦ç»†åˆ†æã€‚\n\n### å‰ä½œè¯†åˆ«ä¸å¯¹æ¯”\n\né€šè¿‡åˆ†æè®ºæ–‡çš„ **â€œMethodsâ€** éƒ¨åˆ†ï¼ˆ4.4.4èŠ‚ï¼‰å’Œ **â€œReferencesâ€**ï¼Œå¯ä»¥æ˜ç¡®è¯†åˆ«å‡ºä¸€ç¯‡å…³é”®çš„å‰ä½œï¼š\n\n| å‰ä½œæ ‡é¢˜ | å‘è¡¨ä¼šè®®/å¹´ä»½ | æ ¸å¿ƒè´¡çŒ® | ä¸æœ¬æ–‡ï¼ˆNeuroVLAï¼‰çš„ä¸»è¦åŒºåˆ« | NeuroVLA çš„ä¸»è¦æ”¹è¿›ä¸åˆ›æ–° |\n| :--- | :--- | :--- | :--- | :--- |\n| **SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition with Jaccard Attentive Spiking Neural Network** <br>ï¼ˆå‚è€ƒæ–‡çŒ® [34]ï¼‰ | Advances in Neural Information Processing Systems (NeurIPS), 2024 | æå‡ºäº†ä¸€ç§åŸºäº**å°–å³°ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰**å’Œ**æ°å¡å¾·æ³¨æ„åŠ›æœºåˆ¶**çš„æ¨¡å‹ï¼Œç”¨äº**è¡¨é¢è‚Œç”µä¿¡å·ï¼ˆsEMGï¼‰æ‰‹åŠ¿è¯†åˆ«**ï¼Œå¹¶ä¸“æ³¨äº**æºè‡ªç”±åŸŸè‡ªé€‚åº”**é—®é¢˜ã€‚ | 1. **åº”ç”¨é¢†åŸŸ**ï¼šå‰ä½œä¸“æ³¨äº**ç”Ÿç‰©ä¿¡å·å¤„ç†ï¼ˆsEMGï¼‰** å’Œ**æ‰‹åŠ¿è¯†åˆ«**ï¼Œæ˜¯ä¸€ä¸ªç›¸å¯¹å‚ç›´çš„æ„ŸçŸ¥ä»»åŠ¡ã€‚<br>2. **ç³»ç»Ÿæ¶æ„**ï¼šå‰ä½œæ˜¯ä¸€ä¸ª**å•ä¸€çš„SNNæ¨¡å‹**ï¼Œä¸»è¦è§£å†³è·¨åŸŸåˆ†ç±»é—®é¢˜ï¼Œæœªæ¶‰åŠæœºå™¨äººæ§åˆ¶é—­ç¯ã€‚<br>3. **æ§åˆ¶å±‚çº§**ï¼šå‰ä½œæ²¡æœ‰ä½“ç°**å¤§è„‘-inspiredçš„å±‚æ¬¡åŒ–æ¶æ„**ï¼Œæ²¡æœ‰åŒºåˆ†çš®å±‚ã€å°è„‘ã€è„Šé«“çš„åŠŸèƒ½ã€‚<br>4. **ç¡¬ä»¶éƒ¨ç½²**ï¼šå‰ä½œå¯èƒ½ä¸»è¦åœ¨ç®—æ³•å±‚é¢è¿›è¡Œä»¿çœŸå’ŒéªŒè¯ï¼Œæœªå¼ºè°ƒåœ¨**ç‰©ç†æœºå™¨äºº**å’Œ**ä¸“ç”¨ç¥ç»å½¢æ€å¤„ç†å™¨**ä¸Šçš„ç«¯åˆ°ç«¯éƒ¨ç½²ã€‚ | 1. **é¢†åŸŸæ‹“å±•ä¸æ¶æ„é©æ–°**ï¼šå°†SNNçš„åº”ç”¨ä»**ç”Ÿç‰©ä¿¡å·æ„ŸçŸ¥**é£è·ƒåˆ°**å®Œæ•´çš„æœºå™¨äººè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ§åˆ¶**ã€‚æå‡ºäº†å…¨æ–°çš„ **â€œä¸‰å±‚ç¥ç»å½¢æ€VLAâ€** æ¶æ„ï¼Œæ˜ç¡®æ¨¡æ‹Ÿäº†**çš®å±‚-å°è„‘-è„Šé«“**çš„ç”Ÿç‰©åˆ†å·¥ã€‚<br>2. **åŠŸèƒ½æ¨¡å—åŒ–ä¸ç”Ÿç‰©æ¨¡æ‹Ÿ**ï¼š<br>   - **æ–°å¢çš®å±‚æ¨¡å—**ï¼šå¼•å…¥åŸºäºVLMï¼ˆå¦‚Qwen-VLï¼‰å’ŒQ-Formerçš„**è¯­ä¹‰æ„å›¾æå–**ï¼Œè§£å†³äº†å‰ä½œæ²¡æœ‰çš„**é«˜å±‚æ¬¡ä»»åŠ¡ç†è§£å’Œè§„åˆ’**é—®é¢˜ã€‚<br>   - **æ–°å¢å°è„‘æ¨¡å—**ï¼šè®¾è®¡äº†åŸºäºGRUå’ŒFiLMçš„**çŠ¶æ€è°ƒåˆ¶å™¨**ï¼Œå®ç°äº†**å®æ—¶ proprioceptive åé¦ˆèåˆã€è½¨è¿¹å¹³æ»‘å’ŒæŠ—æ‰°åŠ¨**ï¼Œè¿™æ˜¯å‰ä½œå®Œå…¨ä¸å…·å¤‡çš„**åŠ¨æ€è¿åŠ¨æ§åˆ¶æ ¸å¿ƒ**ã€‚<br>   - **å¼ºåŒ–è„Šé«“æ¨¡å—**ï¼šæœ¬æ–‡çš„è„Šé«“SNNä¸å†æ˜¯ç‹¬ç«‹çš„åˆ†ç±»å™¨ï¼Œè€Œæ˜¯**æ§åˆ¶å‘½ä»¤çš„ç”Ÿæˆå™¨**ã€‚å®ƒæ¥æ”¶æ¥è‡ªå°è„‘çš„è°ƒåˆ¶åæ„å›¾ï¼Œå¹¶åˆ©ç”¨LIFç¥ç»å…ƒçš„**è†œç”µä½è®°å¿†ç‰¹æ€§**ç”Ÿæˆå¹³æ»‘è¿ç»­çš„åŠ¨ä½œã€‚æœ¬æ–‡è¿˜ç‰¹åˆ«è®¾è®¡äº†**éé‡ç½®çš„è¾“å‡ºç§¯åˆ†ç¥ç»å…ƒ**ä»¥å®ç°è¿ç»­æ§åˆ¶ï¼Œè¿™æ˜¯å¯¹å‰ä½œSNNè¾“å‡ºçš„é‡å¤§æ”¹é€ ã€‚<br>3. **å®ç°é—­ç¯ä¸ç‰©ç†éªŒè¯**ï¼šé¦–æ¬¡å®ç°äº†**ç¥ç»å½¢æ€VLAåœ¨ç‰©ç†æœºå™¨äººä¸Šçš„éƒ¨ç½²**ï¼Œå¹¶å®šåˆ¶äº†**FPGAç¥ç»å½¢æ€å¤„ç†å™¨**æ¥è¿è¡Œè„Šé«“æ¨¡å—ï¼Œå®æµ‹äº†**ä½å»¶è¿Ÿï¼ˆ2.19msï¼‰å’Œä½åŠŸè€—ï¼ˆ0.87mJ/æ¨ç†ï¼‰**ã€‚è¿™æ˜¯ä»å‰ä½œçš„â€œç®—æ³•ä»¿çœŸâ€åˆ°â€œ**å…·èº«æ™ºèƒ½ç³»ç»Ÿ**â€çš„å…³é”®è·¨è¶Šã€‚<br>4. **æ¶Œç°ç‰¹æ€§çš„å®ç°**ï¼šæœ¬æ–‡ç³»ç»Ÿå±•ç°äº†å‰ä½œæ— æ³•å®ç°çš„ç”Ÿç‰©è¿åŠ¨ç‰¹æ€§ï¼š**æ¶ˆé™¤æœºæ¢°è‡‚æŠ–åŠ¨ã€å°äº20msçš„å®‰å…¨åå°„ã€æ—¶é—´è®°å¿†èƒ½åŠ›ã€æä½çš„é™æ€åŠŸè€—ï¼ˆ0.4Wï¼‰**ã€‚è¿™äº›ç‰¹æ€§ç›´æ¥æºäºæ–°çš„å±‚æ¬¡åŒ–æ¶æ„å’Œç¡¬ä»¶ååŒè®¾è®¡ã€‚ |\n\n### æ€»ç»“ï¼šä» SpGesture åˆ° NeuroVLA çš„æ¼”è¿›è·¯å¾„\n\n1.  **æŠ€æœ¯åŸºç¡€ç»§æ‰¿**ï¼šä¸¤ç¯‡å·¥ä½œéƒ½æ·±åº¦åˆ©ç”¨äº†**å°–å³°ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰** å’Œ**æ³¨æ„åŠ›æœºåˆ¶**ã€‚å¯ä»¥æ¨æ–­ï¼Œä½œè€…å›¢é˜Ÿåœ¨SpGestureä¸­ç§¯ç´¯çš„SNNå»ºæ¨¡ã€è®­ç»ƒï¼ˆå¦‚æ›¿ä»£æ¢¯åº¦æ³•ï¼‰å’Œä¼˜åŒ–ç»éªŒï¼Œä¸ºNeuroVLAä¸­å¤æ‚çš„è„Šé«“æ¨¡å—è®¾è®¡å¥ å®šäº†åŸºç¡€ã€‚\n2.  **ç ”ç©¶èŒƒå¼è·ƒè¿**ï¼š\n    *   **SpGesture** ä»£è¡¨äº†åœ¨**ä¸“ç”¨æ„ŸçŸ¥ä»»åŠ¡**ä¸Šåº”ç”¨å’Œä¼˜åŒ–SNNï¼Œè¿½æ±‚çš„æ˜¯**ç®—æ³•çš„å‡†ç¡®æ€§å’Œé€‚åº”æ€§**ã€‚\n    *   **NeuroVLA** åˆ™ä»£è¡¨äº†ä¸€ä¸ª**ç³»ç»Ÿçº§çš„èŒƒå¼åˆ›æ–°**ã€‚å®ƒä¸å†æ»¡è¶³äºä¼˜åŒ–å•ä¸ªæ¨¡å‹ï¼Œè€Œæ˜¯**ä»ç”Ÿç‰©ç¥ç»ç³»ç»Ÿä¸­æ±²å–æ¶æ„çµæ„Ÿ**ï¼Œæ„å»ºä¸€ä¸ªå¼‚æ„çš„ã€åˆ†å±‚çš„ã€è½¯ç¡¬ååŒçš„è®¡ç®—ç³»ç»Ÿï¼Œä»¥è§£å†³å½“å‰å…·èº«æ™ºèƒ½ï¼ˆå°¤å…¶æ˜¯VLAæ¨¡å‹ï¼‰åœ¨**åŠ¨æ€ç¨³å®šæ€§ã€åå°„é€Ÿåº¦å’Œèƒ½æ•ˆ**æ–¹é¢çš„æ ¹æœ¬ç“¶é¢ˆã€‚\n3.  **æ ¸å¿ƒæ”¹å–„çš„å‡ç»ƒ**ï¼š\n    *   **ä»å‰ä½œçš„â€œæ„ŸçŸ¥SNNâ€åˆ°æœ¬æ–‡çš„â€œæ§åˆ¶SNNâ€**ï¼šSNNçš„è§’è‰²ä»æ¨¡å¼è¯†åˆ«è½¬å˜ä¸º**å…·èº«ä»£ç†çš„è¿åŠ¨ä¸­æ¢**ã€‚\n    *   **å¼•å…¥äº†â€œç”Ÿç‰©å±‚çº§â€è¿™ä¸€æ ¸å¿ƒè®¾è®¡åŸåˆ™**ï¼šé€šè¿‡åŠŸèƒ½è§£è€¦ï¼ˆè¯­ä¹‰è§„åˆ’ vs. é«˜é¢‘è°ƒåˆ¶ vs. å¿«é€Ÿæ‰§è¡Œï¼‰æ¥å…‹æœå•ä¸€æ¨¡å‹åœ¨æ—¶åºå’Œç®—åŠ›ä¸Šçš„çŸ›ç›¾ã€‚\n    *   **å®ç°äº†â€œç®—æ³•-ç¡¬ä»¶â€ååŒè®¾è®¡**ï¼šä¸“é—¨ä¸ºè„Šé«“SNNå®šåˆ¶å¤„ç†å™¨ï¼Œä½¿å¾—ç†è®ºä¸Šçš„äº‹ä»¶é©±åŠ¨ç¨€ç–æ€§è½¬åŒ–ä¸ºå®é™…ä¸Šçš„**è¶…ä½åŠŸè€—å’Œå»¶è¿Ÿ**ï¼Œè¿™æ˜¯å·¥ç¨‹ä¸Šçš„é‡å¤§è¿›æ­¥ã€‚\n\n**ç»“è®º**ï¼šå¦‚æœè¯´å‰ä½œã€ŠSpGestureã€‹æ˜¯ä½œè€…åœ¨**ç¥ç»å½¢æ€è®¡ç®—åº”ç”¨äºç”Ÿç‰©ä¿¡å·å¤„ç†**çš„ä¸€æ¬¡æ·±åº¦æ¢ç´¢ï¼Œé‚£ä¹ˆæœ¬æ–‡ã€ŠNeuroVLAã€‹åˆ™æ˜¯ä»¥æ­¤ä¸ºåŸºç¡€ï¼Œè¿›è¡Œçš„ä¸€æ¬¡**é›„å¿ƒå‹ƒå‹ƒçš„ã€ç³»ç»Ÿçº§çš„æ¶æ„åˆ›æ–°**ã€‚å®ƒå°†ç¥ç»å½¢æ€è®¡ç®—ä»è¾¹ç¼˜çš„æ„ŸçŸ¥ä»»åŠ¡ï¼Œæå‡ä¸º**æ„å»ºæ–°ä¸€ä»£é«˜æ•ˆã€é²æ£’å…·èº«æ™ºèƒ½çš„æ ¸å¿ƒåŸºçŸ³**ï¼Œå…¶æ”¹è¿›æ˜¯æ ¹æœ¬æ€§å’Œè·¨è¶Šå¼çš„ã€‚",
            "timestamp": "2026-01-27T15:56:23.931386",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†å¯¹è¿™ç¯‡é¢˜ä¸ºã€ŠA Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Controlã€‹çš„è®ºæ–‡è¿›è¡Œæ·±å…¥åˆ†æï¼Œå¹¶è¯¦ç»†é˜è¿°å…¶æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åˆ†æ\n\næœ¬æ–‡çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºå¹¶éªŒè¯äº† **â€œç¥ç»å½¢æ€è§†è§‰-è¯­è¨€-åŠ¨ä½œâ€** æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»ç³»ç»Ÿä¸­**çš®å±‚-å°è„‘-è„Šé«“**çš„ä¸‰çº§åŠŸèƒ½ä¸ç»“æ„åˆ†ç¦»ï¼Œè§£å†³äº†å½“å‰VLAæ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶ä¸­å­˜åœ¨çš„**æ—¶é—´ç›²åŒºã€åŠ¨ä½œæŠ–åŠ¨ã€é«˜èƒ½è€—å’Œç¼ºä¹å¿«é€Ÿåå°„**ç­‰å…³é”®ç“¶é¢ˆã€‚\n\nä»¥ä¸‹æ˜¯å¯¹è®ºæ–‡ä¸­æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•çš„è¯¦ç»†è¯´æ˜ï¼Œä»¥è¡¨æ ¼å½¢å¼å‘ˆç°ï¼Œå¹¶åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†å’Œå‚æ•°ç»†èŠ‚ã€‚\n\n### å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è¿°\n\n| æ¨¡å—å±‚çº§ | æŠ€æœ¯æ–¹æ³•åç§° | æ ¸å¿ƒåŠŸèƒ½ä¸ç”Ÿç‰©ç±»æ¯” | å…·ä½“æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…³é”®å‚æ•°ä¸è®¾è®¡é€‰æ‹© |\n| :--- | :--- | :--- | :--- | :--- |\n| **1. æ•´ä½“æ¶æ„** | **ä¸‰çº§ç¥ç»å½¢æ€VLA** | **ç»“æ„åˆ†ç¦»**ï¼šå°†é«˜å»¶è¿Ÿçš„è¯­ä¹‰è§„åˆ’ä¸æ¯«ç§’çº§çš„ä¼ æ„Ÿ-è¿åŠ¨æ§åˆ¶è§£è€¦ï¼Œåˆ†åˆ«éƒ¨ç½²åœ¨CUDAè®¡ç®—å±‚å’Œç¥ç»å½¢æ€èŠ¯ç‰‡å±‚ã€‚ | å°†æ§åˆ¶ç­–ç•¥åˆ†è§£ä¸ºä¸‰ä¸ªå‡½æ•°çš„çº§è”ï¼š<br> `a_t = Î¦_spine( Î¦_cerebellum( Î¦_cortex(I_t, L), h_t ) )` <br> - **Î¦_cortex**: å¤„ç†è§†è§‰(I_t)å’Œè¯­è¨€(L)è¾“å…¥ï¼Œç”Ÿæˆè¯­ä¹‰æ½œåœ¨æ„å›¾ `z_sem`ã€‚<br> - **Î¦_cerebellum**: æ¥æ”¶ `z_sem` å’Œé«˜é¢‘æœ¬ä½“æ„Ÿè§‰å†å² `s_{t-H:t}`ï¼Œè¾“å‡ºç»ç‰©ç†è°ƒåˆ¶çš„æ½œåœ¨ç¼–ç  `z_mod`ã€‚<br> - **Î¦_spine**: å°† `z_mod` é€šè¿‡SNNè§£ç ä¸ºè¿ç»­åŠ¨ä½œ `a_t`ã€‚ | - **ç¡¬ä»¶åˆ†ç¦»**ï¼šçš®å±‚/å°è„‘æ¨¡å—åœ¨GPU(CUDAå±‚)è¿è¡Œï¼Œè„Šé«“æ¨¡å—åœ¨FPGAç¥ç»å½¢æ€å¤„ç†å™¨ä¸Šè¿è¡Œã€‚<br> - **æ•°æ®æµ**ï¼šè¯­ä¹‰æ„å›¾ä¸ºä½é¢‘ä¿¡å·ï¼Œæœ¬ä½“æ„Ÿè§‰ï¼ˆå…³èŠ‚è§’ã€é€Ÿåº¦ã€6ç»´åŠ›/åŠ›çŸ©ï¼‰ä¸ºé«˜é¢‘ä¿¡å·ï¼ˆ>200 Hzï¼‰ã€‚ |\n| **2. çš®å±‚æ¨¡å—** | **åˆ†å±‚è¯­ä¹‰è’¸é¦** | **åŠŸèƒ½æŠ½è±¡**ï¼šæ¨¡æ‹Ÿ**çš®è´¨è„Šé«“æŸ**ï¼Œå°†é«˜ç»´ã€æŠ½è±¡çš„VLMè¡¨å¾â€œè’¸é¦â€ä¸ºç´§å‡‘çš„ã€ä»»åŠ¡ç›¸å…³çš„è¿åŠ¨æ„å›¾ã€‚ | 1. **VLMéª¨å¹²**ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„Qwen-VLæ¨¡å‹æå–è·¨æ¨¡æ€ç‰¹å¾ `H_t = F_VLM(I_t, L; Î¸_vlm)`ã€‚<br>2. **æŸ¥è¯¢å˜æ¢å™¨**ï¼šä½¿ç”¨ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢ä»¤ç‰Œ `Q âˆˆ R^{KÃ—D}`ï¼Œä»VLMçš„**æŒ‡å®šä¸­é—´å±‚èŒƒå›´** `[l_start, l_end]` æå–ç‰¹å¾ï¼Œè€Œéä»…æœ€åä¸€å±‚ï¼š<br> `z_sem = Q-Former( Concat(H_t[l_start:l_end]), Q; Î¸_Q-Former )` <br> `z_sem âˆˆ R^{KÃ—D_action}` å³ä¸ºè¯­ä¹‰æ½œåœ¨æ„å›¾ã€‚ | - **VLMæ¨¡å‹**ï¼šQwen-VLã€‚<br> - **æŸ¥è¯¢ä»¤ç‰Œæ•° K**ï¼šæœªæ˜ç¡®ç»™å‡ºï¼Œé€šå¸¸ä¸º32æˆ–64ç­‰ã€‚<br> - **ç‰¹å¾å±‚èŒƒå›´**ï¼š`[l_start, l_end]` æœªç»™å‡ºå…·ä½“å€¼ï¼Œæ—¨åœ¨èåˆä½å±‚ç©ºé—´ä¿¡æ¯å’Œé«˜å±‚è¯­ä¹‰ä¿¡æ¯ã€‚ |\n| **3. å°è„‘æ¨¡å—** | **é—¨æ§å¾ªç¯ç¥ç»è°ƒåˆ¶** | **è‡ªé€‚åº”æ»¤æ³¢ä¸å¢ç›Šæ§åˆ¶**ï¼šæ¨¡æ‹Ÿç”Ÿç‰©å°è„‘ï¼Œåˆ©ç”¨é«˜é¢‘æœ¬ä½“æ„Ÿè§‰åé¦ˆå¯¹çš®å±‚æŒ‡ä»¤è¿›è¡Œå®æ—¶è°ƒåˆ¶ã€å¹³æ»‘è½¨è¿¹ã€çº æ­£è¯¯å·®ã€‚ | 1. **çŠ¶æ€ä¼°è®¡**ï¼šä½¿ç”¨GRUç¼–ç æœ¬ä½“æ„Ÿè§‰å†å² `s_{t-H:t} âˆˆ R^{HÃ—D_s}`ï¼Œå¾—åˆ°åŠ¨æ€ä¸Šä¸‹æ–‡å‘é‡ `h_t = GRU(s_{t-H:t}; Î¸_gru)`ã€‚<br>2. **é—¨æ§FiLMè°ƒåˆ¶**ï¼š<br>   - **é—¨æ§**ï¼š`g_t = Ïƒ(W_g Â· Proj(h_t))`ï¼Œé€‰æ‹©æ€§æ§åˆ¶ç‰©ç†ä¸Šä¸‹æ–‡çš„å½±å“ã€‚<br>   - **ä»¿å°„å˜æ¢**ï¼šä» `h_t` ç”Ÿæˆç¼©æ”¾ `Î³_t` å’Œåç§» `Î²_t` å‚æ•°ã€‚<br>   - **è°ƒåˆ¶**ï¼š`z_mod = (1 + Î³_t) âŠ™ (z_sem Â· g_t) + Î²_t` <br>   å…¶ä¸­ `âŠ™` ä¸ºå“ˆè¾¾ç›ç§¯ã€‚æ­¤æ“ä½œå¯å®ç°å®æ—¶å¢ç›Šè°ƒæ•´ï¼ˆå¦‚ç¢°æ’æ—¶æŠ‘åˆ¶é€Ÿåº¦ï¼‰ã€‚<br>3. **è¿­ä»£ç²¾ç‚¼å¾ªç¯**ï¼šè¿›è¡Œ `K=2` æ¬¡å†…éƒ¨å¾ªç¯ `z_mod^{(k+1)} â† Refine(z_mod^{(k)}, s_{t+1})`ï¼Œæ¨¡æ‹Ÿå°è„‘çš„å‰å‘å†…éƒ¨æ¨¡å‹è¿›è¡Œé¢„æµ‹æ€§æ ¡æ­£ã€‚ | - **å†å²çª—å£ H**ï¼šä¸ä¼ æ„Ÿå™¨é¢‘ç‡å¯¹é½ï¼ˆå¦‚50Hzï¼‰ï¼Œå…·ä½“æ•°å€¼æœªç»™å‡ºã€‚<br> - **çŠ¶æ€ç»´åº¦ D_s**ï¼šåŒ…æ‹¬å…³èŠ‚ä½ç½®ã€é€Ÿåº¦ã€6ç»´åŠ›/åŠ›çŸ©ã€‚<br> - **è¿­ä»£æ¬¡æ•° K**ï¼šè®¾ç½®ä¸º2ã€‚<br> - **é—¨æ§å‡½æ•°**ï¼šSigmoidã€‚ |\n| **4. è„Šé«“æ¨¡å—** | **è„‰å†²æ®‹å·®åŠ¨åŠ›å­¦** | **é«˜æ•ˆæ‰§è¡Œä¸å¿«é€Ÿåå°„**ï¼šæ¨¡æ‹Ÿè„Šé«“ä¸­é—´ç¥ç»å…ƒç½‘ç»œï¼Œåˆ©ç”¨SNNçš„äº‹ä»¶é©±åŠ¨ç¨€ç–æ€§å’Œå›ºæœ‰æ—¶é—´åŠ¨åŠ›å­¦å®ç°èŠ‚èƒ½ã€å¹³æ»‘çš„åŠ¨ä½œç”Ÿæˆã€‚ | 1. **æœ‰çŠ¶æ€LIFç¥ç»å…ƒ**ï¼šè†œç”µä½ `u` åœ¨æ—¶é—´æ­¥é—´æŒç»­å­˜åœ¨ï¼Œæä¾›éšå¼å·¥ä½œè®°å¿†ã€‚<br>   å¯¹äºå±‚ `l` ä¸­ç¥ç»å…ƒ `i` åœ¨æ—¶é—´æ­¥ `Ï„`ï¼š<br>   `u_i^{(l)}[Ï„] = Î² u_i^{(l)}[Ï„-1] + Î£_j w_ij s_j^{(l-1)}[Ï„] - s_i^{(l)}[Ï„-1] Â· Ï‘` <br>   `Î² âˆˆ (0,1)` ä¸ºè†œç”µä½è¡°å‡å¸¸æ•°ï¼Œ`Ï‘` ä¸ºå‘æ”¾é˜ˆå€¼ã€‚<br>2. **æ·±åº¦è„‰å†²æ®‹å·®ç½‘ç»œ**ï¼šé‡‡ç”¨ResNetç»“æ„é¿å…æ¢¯åº¦æ¶ˆå¤±ï¼š<br>   `x^{(l+1)} = x^{(l)} + LIF( Linear(x^{(l)}) )`ã€‚<br>3. **è¿ç»­ç§¯åˆ†è¿åŠ¨è§£ç **ï¼šè¾“å‡ºå±‚ç¥ç»å…ƒé…ç½®ä¸º**éé‡ç½®ç§¯åˆ†å™¨**ï¼ŒæŒç»­ç´¯ç§¯çªè§¦è¾“å…¥è€Œä¸å‘æ”¾è„‰å†²ï¼Œèµ·åˆ°å¹³æ»‘æ»¤æ³¢ä½œç”¨ï¼š<br>   `a_t[Ï„] = W_out Â· u_out[Ï„]`ã€‚<br>4. **ä»£ç†æ¢¯åº¦å­¦ä¹ **ï¼šä½¿ç”¨å¿«é€ŸSigmoidå‡½æ•° `Ïƒ(x) = x/(1+|x|)` è¿‘ä¼¼ä¸å¯å¾®çš„è„‰å†²ç”Ÿæˆé˜¶è·ƒå‡½æ•°ï¼Œå®ç°ç«¯åˆ°ç«¯è®­ç»ƒã€‚ | - **è†œè¡°å‡å¸¸æ•° Î²**ï¼šæœªç»™å‡ºå…·ä½“å€¼ï¼Œå…¸å‹èŒƒå›´0.9-0.99ã€‚<br> - **å‘æ”¾é˜ˆå€¼ Ï‘**ï¼šé€šå¸¸è®¾ä¸º1ã€‚<br> - **ä»¿çœŸæ—¶é—´çª—**ï¼šæœªæ˜ç¡®ï¼Œä½†è¾“å‡ºæ˜¯æ—¶é—´çª—å†…è†œç”µä½çš„ç§¯åˆ†ã€‚<br> - **ç½‘ç»œç»“æ„**ï¼šæœªç»™å‡ºå…·ä½“å±‚æ•°å’Œç¥ç»å…ƒæ•°é‡ã€‚ |\n| **5. ç¥ç»å½¢æ€ç¡¬ä»¶** | **å®šåˆ¶åŒ–FPGAå¤„ç†å™¨** | **ä½å»¶è¿Ÿä¸é«˜èƒ½æ•ˆéƒ¨ç½²**ï¼šä¸ºè„Šé«“SNNè®¾è®¡ä¸“ç”¨ç¡¬ä»¶ï¼Œæœ€å¤§åŒ–æ—¶ç©ºå¹¶è¡Œåº¦å’Œäº‹ä»¶é©±åŠ¨ç¨€ç–æ€§ä¼˜åŠ¿ã€‚ | 1. **è„‰åŠ¨é˜µåˆ—æ¶æ„**ï¼šå®ç°ç©ºé—´ï¼ˆè·¨ç¥ç»å…ƒï¼‰å’Œæ—¶é—´ï¼ˆæ²¿é˜µåˆ—è¡Œï¼‰å¹¶è¡Œï¼ŒåŠ é€Ÿè®¡ç®—ã€‚<br>2. **è„‰å†²ç¨€ç–æ„ŸçŸ¥è®¡ç®—**ï¼šæŠ‘åˆ¶æ•´ä¸ªæ—¶é—´çª—å†…éæ´»è·ƒçš„è„‰å†²äº‹ä»¶ï¼Œé¿å…ä¸å¿…è¦çš„ç¥ç»å…ƒæ›´æ–°ã€‚<br>3. **æ€§èƒ½æŒ‡æ ‡**ï¼šåœ¨20 MHzå·¥ä½œé¢‘ç‡ä¸‹ï¼Œå•æ¬¡æ¨ç†å»¶è¿Ÿä¸º **2.19 ms**ï¼Œèƒ½è€—ä¸º **0.87 mJ**ã€‚ | - **å·¥ä½œé¢‘ç‡**ï¼š20 MHzã€‚<br> - **èµ„æºå ç”¨**ï¼š51,953 LUTs, 27,880 FFs, 169 BRAMsã€‚<br> - **å…³é”®æ€§èƒ½**ï¼šå»¶è¿Ÿ 2.19 msï¼Œèƒ½è€— 0.87 mJ/æ¨ç†ã€‚ |\n\n### æŠ€æœ¯åˆ›æ–°çš„å®é™…ä»·å€¼æ€»ç»“\n\n1.  **æ€§èƒ½æå‡**ï¼š\n    - **å¹³æ»‘æ€§**ï¼šå°è„‘æ¨¡å—å°†è¿åŠ¨æ€¥åŠ¨åº¦é™ä½è¶…è¿‡ **75%**ï¼ŒåŠ é€Ÿåº¦é™ä½ **40-58%**ï¼Œæ¶ˆé™¤äº†æœºæ¢°è‡‚çš„â€œæ„å‘æ€§éœ‡é¢¤â€ã€‚\n    - **å¿«é€Ÿåå°„**ï¼šè„Šé«“å®‰å…¨åå°„é€šè·¯å¯åœ¨ **<20 ms** å†…è§¦å‘é¿éšœååº”ï¼Œè¿œè¶…ä¼ ç»ŸåŸºäºè§†è§‰çš„VLAæ¨¡å‹ï¼ˆ>200 msï¼‰ã€‚\n    - **èƒ½æ•ˆ**ï¼šè„Šé«“SNNçš„äº‹ä»¶é©±åŠ¨ç¨€ç–æ€§ï¼Œç»“åˆä¸“ç”¨å¤„ç†å™¨ï¼Œå®ç°äº†æä½çš„åŠŸè€—ï¼ˆå•æ¬¡æ¨ç† **0.87 mJ**ï¼‰ã€‚\n\n2.  **æ¶Œç°çš„ç”Ÿç‰©æ™ºèƒ½ç‰¹æ€§**ï¼š\n    - **æ—¶é—´è®°å¿†**ï¼šSNNçš„è†œåŠ¨åŠ›å­¦å’ŒGRUçŠ¶æ€ä¼°è®¡ä½¿ç³»ç»Ÿå…·å¤‡å¯¹ä»»åŠ¡é˜¶æ®µå’ŒèŠ‚å¥çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚\n    - **åŠŸèƒ½è§£è€¦**ï¼šç½‘ç»œè‡ªå‘å½¢æˆäº†å¯¹ç‰¹å®šè¿åŠ¨ç»´åº¦ï¼ˆå¦‚æŠ“å– vs. æœ«ç«¯ä½å§¿ï¼‰çš„ç¥ç»å­ç¾¤ç¼–ç ï¼Œç±»ä¼¼äºç”Ÿç‰©è¿åŠ¨çš®å±‚çš„èº¯ä½“æ‹“æ‰‘ç»„ç»‡ã€‚\n    - **è·¨æ¨¡æ€é²æ£’æ€§**ï¼šå°è„‘æ¨¡å—èƒ½å¯¹è§†è§‰å™ªå£°å’Œæœ¬ä½“æ„Ÿè§‰è¿›è¡Œé‡æ–°åŠ æƒï¼Œåœ¨æ„Ÿå®˜é€€åŒ–æ—¶ä¿æŒä»»åŠ¡æˆåŠŸç‡ã€‚\n\n3.  **æ–¹æ³•è®ºæ„ä¹‰**ï¼š\n    - è®ºæ–‡è¯æ˜äº†**é€šè¿‡æ¨¡ä»¿ç¥ç»ç³»ç»Ÿçš„ç»“æ„å…ˆéªŒï¼Œè€Œéä»…ä»…æ‰©å¤§æ•°æ®ä¸æ¨¡å‹è§„æ¨¡**ï¼Œå¯ä»¥æ›´é«˜æ•ˆåœ°èµ‹äºˆæœºå™¨äººä»¥è‡ªé€‚åº”ã€èŠ‚èƒ½çš„å…·èº«æ™ºèƒ½ã€‚\n    - ä¸ºæœªæ¥å…·èº«",
            "timestamp": "2026-01-27T15:57:34.752567",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡\n\nè¯¥è®ºæ–‡é‡‡ç”¨äº†**å¤šç»´åº¦ã€å¤šå±‚æ¬¡**çš„è¯„ä»·ä½“ç³»ï¼Œä»¥å…¨é¢éªŒè¯å…¶æå‡ºçš„ **NeuroVLA** æ¡†æ¶åœ¨æ€§èƒ½ã€æ•ˆç‡ã€é²æ£’æ€§å’Œç”Ÿç‰©ç›¸ä¼¼æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚å…·ä½“æŒ‡æ ‡å¦‚ä¸‹ï¼š\n\n| æŒ‡æ ‡ç±»åˆ« | å…·ä½“æŒ‡æ ‡ | ç»†èŠ‚ä¸ç›®çš„ |\n| :--- | :--- | :--- |\n| **è¿åŠ¨è´¨é‡ä¸ç¨³å®šæ€§** | **1. è¿åŠ¨å¹³æ»‘åº¦** | - **å¹³å‡æŒ‡ä»¤åŠ åŠ é€Ÿåº¦ (Mean Absolute Commanded Jerk)**ï¼šè¡¡é‡æ§åˆ¶æŒ‡ä»¤åœ¨åŠ é€Ÿåº¦å±‚é¢çš„å˜åŒ–ç‡ï¼ˆå³ä¸‰é˜¶å¯¼æ•°ï¼‰ã€‚**å…³é”®ç»“æœ**ï¼šä¸åŸºçº¿ç›¸æ¯”ï¼Œ**é™ä½è¶…è¿‡75%**ï¼Œè¯æ˜å°è„‘æ¨¡å—æœ‰æ•ˆæŠ‘åˆ¶äº†â€œæ„å‘æ€§éœ‡é¢¤â€ã€‚<br>- **å¹³å‡æŒ‡ä»¤åŠ é€Ÿåº¦ (Mean Absolute Commanded Acceleration)**ï¼šè¡¡é‡æ§åˆ¶æŒ‡ä»¤çš„åŠ é€Ÿåº¦å¤§å°ã€‚**å…³é”®ç»“æœ**ï¼šä¸åŸºçº¿ç›¸æ¯”ï¼Œ**é™ä½32.8%è‡³58.0%**ï¼Œè¡¨æ˜è¿åŠ¨è½¨è¿¹æ›´å¹³æ»‘ã€‚ |\n| | **2. æ—¶åºè®°å¿†ä¸ä¸€è‡´æ€§** | - åœ¨â€œæ‘‡åŠ¨æ¯å­â€ç­‰**èŠ‚å¾‹æ€§ä»»åŠ¡**ä¸­ï¼Œé€šè¿‡åˆ†æ**6ç»´åŠ›/åŠ›çŸ©**å’Œ**å…³èŠ‚è§’åº¦**çš„ä¼ æ„Ÿå™¨è½¨è¿¹ï¼Œè¯„ä¼°ç³»ç»Ÿæ˜¯å¦èƒ½ç”Ÿæˆ**é«˜ä¿çœŸã€åŒæ­¥çš„å‘¨æœŸæ€§æŒ¯è¡**ï¼Œä»¥è¯æ˜å…¶å¯¹è¿åŠ¨ç›¸ä½å’ŒèŠ‚å¥çš„ç¼–ç èƒ½åŠ›ã€‚ |\n| **è®¡ç®—ä¸èƒ½æºæ•ˆç‡** | **1. ç¥ç»æ´»åŠ¨ç¨€ç–æ€§** | - **å¹³å‡æ¿€æ´»ç‡ (Mean Activation Rate)**ï¼šæµ‹é‡SNNç¥ç»å…ƒåœ¨**é™æ€ä¿æŒé˜¶æ®µ**ä¸**åŠ¨æ€è¿åŠ¨é˜¶æ®µ**çš„æ”¾ç”µé¢‘ç‡å·®å¼‚ã€‚**å…³é”®ç»“æœ**ï¼šé™æ€æ—¶æ¿€æ´»ç‡æ˜¾è‘—ä¸‹é™ï¼Œå®ç°â€œæŒ‰éœ€æ¿€æ´»â€ï¼Œé™ä½åŠŸè€—ã€‚ |\n| | **2. ç¡¬ä»¶æ€§èƒ½** | - **æ¨ç†å»¶è¿Ÿ (Inference Latency)**ï¼šåœ¨FPGAä¸Šéƒ¨ç½²çš„è„‰å†²è„Šé«“æ¨¡å—ï¼Œ**å»¶è¿Ÿä¸º2.19 ms**ã€‚<br>- **èƒ½è€— (Energy Cost)**ï¼šæ¯æ¬¡æ¨ç†çš„èƒ½è€—ä¸º**0.87 mJ**ã€‚æ•´ä¸ªç³»ç»Ÿåœ¨ç¥ç»å½¢æ€å¤„ç†å™¨ä¸Šè¿è¡ŒåŠŸè€—**ä»…0.4W**ã€‚ |\n| **å®‰å…¨ä¸åå°„èƒ½åŠ›** | **1. åå°„å»¶è¿Ÿ** | - **å®‰å…¨åå°„è§¦å‘æ—¶é—´**ï¼šä»æ£€æµ‹åˆ°ç¢°æ’ï¼ˆåŠ›ä¼ æ„Ÿå™¨ä¿¡å·çªå˜ï¼‰åˆ°è§¦å‘æ’¤å›åå°„çš„å»¶è¿Ÿã€‚**å…³é”®ç»“æœ**ï¼š**å°äº20æ¯«ç§’**ï¼Œè¿œå¿«äºåŸºäºè§†è§‰çš„çš®å±‚ç¯è·¯ï¼ˆ>200 msï¼‰ã€‚ |\n| | **2. å®‰å…¨æ¢å¤æˆåŠŸç‡** | - åœ¨â€œé‡åˆ°éšœç¢ç‰©åæ¢å¤è‡³å®‰å…¨åŒºåŸŸâ€çš„å‹åŠ›æµ‹è¯•ä¸­ï¼Œæµ‹é‡ç³»ç»ŸæˆåŠŸç»•è¿‡éšœç¢å¹¶å®Œæˆä»»åŠ¡çš„æ¯”ç‡ã€‚**å…³é”®ç»“æœ**ï¼šNeuroVLAè¾¾åˆ° **54.8%** çš„æ¢å¤ç‡ï¼Œè€ŒåŸºçº¿æ¨¡å‹ä¸º **0.0%**ã€‚ |\n| **ä»»åŠ¡æ€§èƒ½** | **1. ä»»åŠ¡æˆåŠŸç‡** | - åœ¨**LIBERO**ç­‰æ¨¡æ‹ŸåŸºå‡†å’Œ**çœŸå®ä¸–ç•Œç”ŸåŒ–æ“ä½œä»»åŠ¡**ä¸Šï¼Œè¯„ä¼°å®ŒæˆæŒ‡å®šæŒ‡ä»¤çš„æˆåŠŸç‡ã€‚ç”¨äºä¸SOTA VLAæ¨¡å‹ï¼ˆå¦‚OpenVLA, UniVLAç­‰ï¼‰è¿›è¡Œæ¨ªå‘æ¯”è¾ƒã€‚ |\n| | **2. é•¿æ—¶ç¨‹ä»»åŠ¡æ€§èƒ½** | - åœ¨LIBEROåŸºå‡†ä¸­ï¼Œç‰¹åˆ«å…³æ³¨**éœ€è¦å¤šé˜¶æ®µè½¬æ¢çš„é•¿æ—¶ç¨‹ä»»åŠ¡**ï¼ˆå¦‚â€œå°†ç¢—æ”¾åœ¨ç‚‰å­ä¸Šâ€ï¼‰ã€‚é€šè¿‡å¯¹æ¯”**å¤šæ­¥SNN**ã€**å•æ­¥SNN**å’Œ**æ— å°è„‘åŸºçº¿**çš„æ€§èƒ½ï¼ŒéªŒè¯è„Šé«“æ¨¡å—çš„**æ—¶åºæ•´åˆèƒ½åŠ›**å¯¹å¤æ‚æ“ä½œè‡³å…³é‡è¦ã€‚ |\n| **æ¶Œç°çš„ç”Ÿç‰©æ™ºèƒ½ç‰¹æ€§** | **1. åŠŸèƒ½è§£è€¦ä¸æ‹“æ‰‘ç»„ç»‡** | - **ç¥ç»è¡¨å¾åˆ†æ**ï¼šé€šè¿‡å¯è§†åŒ–SNNéšè—å±‚ç¥ç»å…ƒçš„å¹³å‡æ”¾ç”µç‡å›¾ï¼Œè§‚å¯Ÿæ˜¯å¦è‡ªå‘å½¢æˆäº†**èº¯ä½“æ‹“æ‰‘ç»„ç»‡**ï¼Œå³ç‰¹å®šç¥ç»å…ƒå­ç¾¤é€‰æ‹©æ€§åœ°æ§åˆ¶ç‰¹å®šçš„è‡ªç”±åº¦ï¼ˆå¦‚Zè½´ç§»åŠ¨ vs. å¤¹çˆªæ§åˆ¶ï¼‰ã€‚<br>- **æ½œåœ¨è¡Œä¸ºæ¨¡å¼èšç±»**ï¼šä½¿ç”¨t-SNEå¯¹è„Šé«“éšè—çŠ¶æ€è¿›è¡Œé™ç»´å¯è§†åŒ–ï¼Œè§‚å¯Ÿé«˜ç»´æ§åˆ¶ä¿¡å·æ˜¯å¦è¢«**æ— ç›‘ç£åœ°è§£è€¦ä¸ºç¦»æ•£çš„è¡Œä¸ºæ¨¡å¼ç°‡**ï¼ˆè¿åŠ¨åŸºå…ƒï¼‰ã€‚ |\n\n### äºŒã€ ä½¿ç”¨çš„æ•°æ®é›†\n\nè®ºæ–‡ä½¿ç”¨äº†**æ¨¡æ‹Ÿ**ä¸**çœŸå®ä¸–ç•Œ**ä¸¤ç±»æ•°æ®ç¯å¢ƒè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚\n\n| æ•°æ®é›†ç±»å‹ | åç§° | ç»†èŠ‚ä¸ç”¨é€” |\n| :--- | :--- | :--- |\n| **æ¨¡æ‹ŸåŸºå‡†æ•°æ®é›†** | **LIBERO** | - **æ¥æº**ï¼šå¼€æºçš„ç»ˆèº«æœºå™¨äººå­¦ä¹ åŸºå‡†ã€‚<br>- **ç‰¹ç‚¹**ï¼šåŒ…å«ä¸€ç³»åˆ—**éœ€è¦çŸ¥è¯†è¿ç§»çš„å¤æ‚é•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡**ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ—¶åºæ¨ç†èƒ½åŠ›ã€‚<br>- **è®ºæ–‡ä¸­çš„ç”¨é€”**ï¼šä¸»è¦ç”¨äº**æ¶ˆèå®éªŒ**ï¼ŒéªŒè¯**å¤šæ­¥SNN**ï¼ˆå…·æœ‰æ—¶åºæ•´åˆï¼‰ç›¸æ¯”**å•æ­¥SNN**å’Œ**æ— å°è„‘åŸºçº¿**åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ï¼ˆå¦‚å›¾5dï¼‰ã€‚ |\n| | **LIBERO-Plus** | - **æ¥æº**ï¼šLIBEROçš„æ‰©å±•ç‰ˆæœ¬ï¼Œä¸“æ³¨äºå¯¹VLAæ¨¡å‹çš„**æ·±åº¦é²æ£’æ€§åˆ†æ**ã€‚<br>- **ç‰¹ç‚¹**ï¼šå¯èƒ½åŒ…å«æ›´å¤šè§†è§‰å¹²æ‰°ã€åœºæ™¯å˜åŒ–æˆ–ç‰©ç†æ‰°åŠ¨ã€‚<br>- **è®ºæ–‡ä¸­çš„ç”¨é€”**ï¼šåœ¨æ„Ÿå®˜é€€åŒ–ï¼ˆå¦‚å…‰ç…§ã€çº¹ç†æ”¹å˜ï¼‰å®éªŒä¸­ï¼Œç”¨äºè¯æ˜å°è„‘æ¨¡å—é€šè¿‡è·¨æ¨¡æ€æ„Ÿå®˜é‡åŠ æƒï¼ˆä¾èµ–ä¸å˜çš„èº«ä½“åŠ¨åŠ›å­¦ï¼‰ä¿æŒäº†é«˜æˆåŠŸç‡ã€‚ |\n| **çœŸå®ä¸–ç•Œæ•°æ®é›†** | **è‡ªå®šä¹‰ç”ŸåŒ–å®éªŒåè®®** | - **æ¥æº**ï¼šä½œè€…åœ¨ç‰©ç†åŒæ‰‹æœºå™¨äººå¹³å°ä¸Šè‡ªè¡Œé‡‡é›†ã€‚<br>- **ä»»åŠ¡è®¾è®¡**ï¼šç²¾å¿ƒè®¾è®¡äº†ä¸€ç³»åˆ—ä»»åŠ¡ï¼Œä»¥æ¢ç©¶ä¸åŒçš„è¿åŠ¨åŸºå…ƒï¼š<br>  - **ç©ºé—´ç²¾åº¦**ï¼šé‡æ–°å®šä½è¯•ç®¡ã€‚<br>  - **åŠ¨æ€æµä½“æ§åˆ¶**ï¼šå€¾å€’æ¶²ä½“ã€‚<br>  - **èŠ‚å¾‹åºåˆ—**ï¼šæ‘‡åŠ¨çƒ§ç“¶ã€‚<br>  - **å®‰å…¨ä¸´ç•Œé€‚åº”**ï¼šâ€œé‡åˆ°éšœç¢ç‰©åæ¢å¤è‡³å®‰å…¨åŒºåŸŸâ€çš„å‹åŠ›æµ‹è¯•ã€‚<br>- **æ•°æ®è§„æ¨¡**ï¼šè®ºæ–‡å¼ºè°ƒ**ä»…ä½¿ç”¨æ•°ç™¾ä¸ªä¸‹æ¸¸æ ·æœ¬è¿›è¡Œå¾®è°ƒ**ï¼Œå³â€œå°æ ·æœ¬å­¦ä¹ â€ï¼Œè¯æ˜äº†å…¶æ¶æ„çš„æ•°æ®æ•ˆç‡ã€‚é¢„è®­ç»ƒæƒé‡æ¥è‡ªé€šç”¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆQwen-VLï¼‰ã€‚<br>- **è¯„ä¼°å¯¹è±¡**ï¼šåœ¨æ­¤æ•°æ®é›†ä¸Šï¼Œå°†NeuroVLAä¸å¤šä¸ªSOTA VLAåŸºçº¿æ¨¡å‹ï¼ˆOpenVLA, OpenVLA-OFT, UniVLA, WorldVLAï¼‰è¿›è¡Œæ€§èƒ½å¯¹æ¯”ï¼ˆå¦‚å›¾8ï¼‰ã€‚ |\n\n### æ€»ç»“\nè¯¥è®ºæ–‡çš„è¯„ä»·ä½“ç³»ç´§å¯†ç»“åˆå…¶**è„‘å¯å‘åˆ†å±‚æ¶æ„**çš„åˆ›æ–°ç‚¹ï¼Œä¸ä»…å…³æ³¨ä¼ ç»Ÿçš„ä»»åŠ¡æˆåŠŸç‡ï¼Œæ›´æ·±å…¥é‡åŒ–äº†**è¿åŠ¨å¹³æ»‘æ€§ã€åå°„é€Ÿåº¦ã€èƒ½è€—ã€æ—¶åºè®°å¿†ä»¥åŠæ¶Œç°çš„ç¥ç»ç»“æ„ç‰¹æ€§**ã€‚æ‰€ä½¿ç”¨çš„æ•°æ®é›†ä»**æ ‡å‡†åŒ–çš„æ¨¡æ‹ŸåŸºå‡†**åˆ°**ç²¾å¿ƒè®¾è®¡çš„çœŸå®ä¸–ç•Œå¤æ‚ä»»åŠ¡**ï¼Œå…¨é¢éªŒè¯äº†NeuroVLAåœ¨**æ¨¡æ‹Ÿä¸å®ç‰©**ä¸Šçš„ä¼˜è¶Šæ€§èƒ½å’Œç”Ÿç‰©åˆç†æ€§ã€‚è¿™å¥—è¯„ä»·æ–¹æ³•ä¸ºå…¶å®£ç§°çš„â€œå®ç°ç±»ç”Ÿç‰©è¿åŠ¨æ™ºèƒ½â€æä¾›äº†æ‰å®ã€å¤šç»´åº¦çš„è¯æ®ã€‚",
            "timestamp": "2026-01-27T15:58:25.931586",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨**ä»¿çœŸ**å’Œ**çœŸå®ä¸–ç•Œ**ä¸¤ä¸ªå±‚é¢è¿›è¡Œäº†å…¨é¢çš„å®éªŒéªŒè¯ã€‚\n\n#### 1. ä»¿çœŸåŸºå‡†æµ‹è¯•\n- **ä¸»è¦æ•°æ®é›†**ï¼š**LIBERO** å’Œ **LIBERO-Plus** åŸºå‡†æµ‹è¯•é›†ã€‚\n    - **æ€§è´¨**ï¼šå¼€æºçš„ç»ˆèº«æœºå™¨äººå­¦ä¹ åŸºå‡†ï¼Œç”¨äºè¯„ä¼°çŸ¥è¯†è¿ç§»å’Œé•¿è§†é‡ã€å¤šé˜¶æ®µå¤æ‚æ“ä½œä»»åŠ¡ã€‚\n    - **ä»£è¡¨æ€§ä»»åŠ¡**ï¼šä¾‹å¦‚ â€œBowl on stoveâ€ ç­‰éœ€è¦é˜¶æ®µè½¬æ¢çš„é•¿è§†é‡ä»»åŠ¡ã€‚\n\n#### 2. çœŸå®ä¸–ç•Œå®éªŒ\n- **å®éªŒå¹³å°**ï¼šéƒ¨ç½²åœ¨**ç‰©ç†åŒæ‰‹æœºå™¨äººå¹³å°**ä¸Šã€‚\n- **ä»»åŠ¡åœºæ™¯**ï¼šæ‰§è¡Œä¸€å¥—**ç”ŸåŒ–å®éªŒåè®®**ä»»åŠ¡ï¼Œæ—¨åœ¨æµ‹è¯•ä¸åŒçš„è¿åŠ¨åŸºå…ƒï¼š\n    - **ç©ºé—´ç²¾åº¦**ï¼šé‡æ–°å®šä½è¯•ç®¡ã€‚\n    - **åŠ¨æ€æµä½“æ§åˆ¶**ï¼šå€¾å€’æ¶²ä½“ã€‚\n    - **èŠ‚å¾‹æ€§åºåˆ—**ï¼šæ‘‡æ™ƒçƒ§ç“¶ã€‚\n- **å‹åŠ›æµ‹è¯•**ï¼šè®¾è®¡äº†â€œ**æ¢å¤è‡³å®‰å…¨åŒºåŸŸ**â€åœºæ™¯ï¼Œæ¨¡æ‹Ÿéç»“æ„åŒ–ç¯å¢ƒä¸­é‡åˆ°æ„å¤–ç‰©ç†éšœç¢ï¼ˆå¦‚ç¢°æ’ï¼‰çš„æƒ…å†µã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡ä»å¤šä¸ªç»´åº¦è¯„ä¼° NeuroVLA çš„æ€§èƒ½ï¼ŒæŒ‡æ ‡å¯åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š\n\n#### 1. **è¿åŠ¨å¹³æ»‘æ€§ä¸ç¨³å®šæ€§**\n- **å¹³å‡æŒ‡ä»¤åŠ åŠ é€Ÿåº¦**ï¼šè¡¡é‡è¿åŠ¨è½¨è¿¹çš„çªå˜ï¼ˆæŠ–åŠ¨ï¼‰ç¨‹åº¦ã€‚\n- **å¹³å‡æŒ‡ä»¤åŠ é€Ÿåº¦**ï¼šè¡¡é‡æ•´ä½“è¿åŠ¨çš„å‰§çƒˆç¨‹åº¦ã€‚\n- **è¿åŠ¨æŠ–åŠ¨è¡°å‡ç™¾åˆ†æ¯”**ï¼šé‡åŒ–å°è„‘æ¨¡å—å¯¹â€œæ„å‘æ€§éœ‡é¢¤â€çš„æŠ‘åˆ¶æ•ˆæœã€‚\n\n#### 2. **ä»»åŠ¡æˆåŠŸç‡**\n- åœ¨ LIBERO ç­‰åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­ï¼ŒæˆåŠŸå®ŒæˆæŒ‡å®šæŒ‡ä»¤çš„æ¯”ä¾‹ã€‚\n- ä¸å½“å‰æœ€å…ˆè¿›çš„ VLA åŸºçº¿æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚\n\n#### 3. **åå°„ä¸å®‰å…¨æ€§èƒ½**\n- **åå°„è§¦å‘å»¶è¿Ÿ**ï¼šä»æ£€æµ‹åˆ°ç¢°æ’ï¼ˆåŠ›ä¼ æ„Ÿå™¨ä¿¡å·çªå˜ï¼‰åˆ°è§¦å‘æ’¤å›åå°„çš„æ—¶é—´ã€‚\n- **å®‰å…¨æ¢å¤æˆåŠŸç‡**ï¼šåœ¨æ„å¤–ç¢°æ’åï¼ŒæˆåŠŸæ’¤å›å¹¶é‡æ–°è§„åˆ’è·¯å¾„å®Œæˆä»»åŠ¡çš„æ¯”ç‡ã€‚\n\n#### 4. **èƒ½æ•ˆä¸è®¡ç®—æ€§èƒ½**\n- **åŠŸè€—**ï¼šç¥ç»å½¢æ€è„Šé«“æ¨¡å—åœ¨ä¸“ç”¨å¤„ç†å™¨ä¸Šçš„è¿è¡ŒåŠŸè€—ã€‚\n- **æ¨ç†å»¶è¿Ÿ**ï¼šè„Šé«“æ¨¡å—ç”ŸæˆåŠ¨ä½œå‘½ä»¤çš„è€—æ—¶ã€‚\n- **ç¥ç»å…ƒæ¿€æ´»ç‡**ï¼šåœ¨é™æ€ä¿æŒé˜¶æ®µä¸åŠ¨æ€è¿åŠ¨é˜¶æ®µçš„å¹³å‡æ¿€æ´»ç‡å¯¹æ¯”ï¼Œä½“ç°â€œæŒ‰éœ€æ¿€æ´»â€çš„ç¨€ç–æ€§ã€‚\n\n#### 5. **æ¶Œç°ç‰¹æ€§ä¸é²æ£’æ€§**\n- **æ—¶é—´è®°å¿†èƒ½åŠ›**ï¼šåœ¨è§†è§‰åé¦ˆè¢«é®æŒ¡æˆ–é™æ€æ—¶ï¼Œç»´æŒèŠ‚å¾‹æ€§ä»»åŠ¡ï¼ˆå¦‚â€œæ‘‡æ™ƒæ¯å­â€ï¼‰ç›¸ä½ä¸€è‡´æ€§çš„èƒ½åŠ›ã€‚\n- **æ„Ÿè§‰é‡åŠ æƒ**ï¼šåœ¨è§†è§‰æ„ŸçŸ¥é€€åŒ–ï¼ˆå…‰ç…§ã€çº¹ç†å˜åŒ–ï¼‰æ—¶ï¼Œåˆ©ç”¨æœ¬ä½“æ„Ÿè§‰ç»´æŒä»»åŠ¡æˆåŠŸç‡çš„èƒ½åŠ›ã€‚\n\n### ä¸‰ã€ å…³é”®æ€§èƒ½æå‡ä¸ç»“æœ\nNeuroVLA ç›¸æ¯”ç°æœ‰çš„å•ä½“å¼ VLA åŸºçº¿æ¨¡å‹ï¼ˆå¦‚ OpenVLA, UniVLA, WorldVLA ç­‰ï¼‰å–å¾—äº†æ˜¾è‘—æå‡ï¼š\n\n#### 1. **è¿åŠ¨å¹³æ»‘æ€§å¤§å¹…æ”¹å–„**\n- **åŠ åŠ é€Ÿåº¦é™ä½è¶…è¿‡ 75%**ï¼šå°è„‘æ¨¡å—ä½œä¸ºè‡ªé€‚åº”æ»¤æ³¢å™¨ï¼Œæœ‰æ•ˆæŠ‘åˆ¶äº†é«˜å±‚ç­–ç•¥äº§ç”Ÿçš„é«˜é¢‘æŠ–åŠ¨ã€‚\n- **åŠ é€Ÿåº¦é™ä½ 32.8% è‡³ 58.0%**ï¼š across all axesï¼Œä½¿å¾—æœºæ¢°è‡‚è¿åŠ¨è½¨è¿¹æ˜¾è‘—æ›´å¹³æ»‘ã€‚\n\n#### 2. **è¶…å¿«é€Ÿå®‰å…¨åå°„**\n- **åå°„å»¶è¿Ÿ < 20 ms**ï¼šé€šè¿‡ç¥ç»å½¢æ€è„Šé«“å±‚çš„å¿«é€Ÿé€šè·¯ï¼Œå®ç°äº†ç±»ä¼¼å•çªè§¦çš„æ’¤å›åå°„ã€‚\n- **å®‰å…¨æ¢å¤æˆåŠŸç‡ 54.8%**ï¼šåœ¨åŸºçº¿æ¨¡å‹æˆåŠŸç‡å‡ä¸º **0.0%** çš„ç¢°æ’å‹åŠ›æµ‹è¯•ä¸­ï¼ŒNeuroVLA èƒ½æœ‰æ•ˆè§¦å‘åå°„å¹¶é‡æ–°è§„åˆ’ï¼Œå±•ç°äº†â€œç”Ÿå­˜ä¼˜å…ˆâ€çš„æ¶Œç°æ™ºèƒ½ã€‚\n\n#### 3. **å“è¶Šçš„èƒ½æ•ˆ**\n- **æä½åŠŸè€—**ï¼šç¥ç»å½¢æ€è„Šé«“æ¨¡å—åœ¨ FPGA ä¸Šéƒ¨ç½²ï¼Œæ¯æ¬¡æ¨ç†èƒ½è€—ä»… **0.87 mJ**ï¼Œè¿è¡Œæ—¶åŠŸè€—ä½è‡³ **0.4W**ã€‚\n- **äº‹ä»¶é©±åŠ¨ç¨€ç–æ€§**ï¼šåœ¨é™æ€ä¿æŒé˜¶æ®µï¼Œç¥ç»å…ƒæ¿€æ´»ç‡æ˜¾è‘—ä¸‹é™ï¼Œå®ç°äº†â€œæŒ‰éœ€è®¡ç®—â€ï¼Œé€‚åˆç”µæ± å—é™çš„è¾¹ç¼˜éƒ¨ç½²ã€‚\n\n#### 4. **ä»»åŠ¡æ€§èƒ½å…¨é¢é¢†å…ˆ**\n- åœ¨çœŸå®ä¸–ç•Œçš„**ç²¾ç»†æ“ä½œ**ï¼ˆè¯•ç®¡é‡å®šä½ï¼‰ã€**æµä½“æ§åˆ¶**ï¼ˆå€¾å€’ï¼‰å’Œ**èŠ‚å¾‹ä»»åŠ¡**ï¼ˆæ‘‡æ™ƒï¼‰ä¸­ï¼Œ**æ€§èƒ½å‡è¶…è¶Šæ‰€æœ‰å¯¹æ¯”çš„ SOTA VLA åŸºçº¿**ã€‚\n- åœ¨é•¿è§†é‡ã€å¤šé˜¶æ®µçš„ LIBERO ä»¿çœŸä»»åŠ¡ä¸­ï¼Œ**å¤šæ­¥ SNNï¼ˆå…·æœ‰æ—¶é—´ä¸Šä¸‹æ–‡ï¼‰æ˜¾è‘—ä¼˜äºå•æ­¥ SNN å’Œæ— å°è„‘åŸºçº¿**ï¼ŒéªŒè¯äº†è„Šé«“æ¨¡å—å†…åœ¨æ—¶é—´è®°å¿†çš„é‡è¦æ€§ã€‚\n\n#### 5. **å°æ ·æœ¬å­¦ä¹ èƒ½åŠ›**\n- ä»…ä½¿ç”¨é¢„è®­ç»ƒçš„ VLMï¼ˆQwen-VLï¼‰æƒé‡ï¼Œå¹¶åœ¨**å‡ ç™¾ä¸ªä¸‹æ¸¸æ ·æœ¬**ä¸Šå¾®è°ƒï¼Œå³å¯è¾¾åˆ°ç”šè‡³**è¶…è¿‡é¢„è®­ç»ƒåŸºçº¿çš„ä»»åŠ¡æˆåŠŸç‡**ï¼Œå±•ç¤ºäº†æ¡†æ¶é«˜æ•ˆçš„çŸ¥è¯†è¿ç§»å’Œå¿«é€ŸæŠ€èƒ½è·å–èƒ½åŠ›ã€‚\n\n### æ€»ç»“\nNeuroVLA é€šè¿‡åœ¨**LIBEROï¼ˆä»¿çœŸï¼‰å’ŒçœŸå®æœºå™¨äººç”ŸåŒ–åè®®ä»»åŠ¡**ä¸Šçš„å®éªŒï¼Œç³»ç»ŸéªŒè¯äº†å…¶ç”Ÿç‰©å¯å‘åˆ†å±‚æ¶æ„çš„ä»·å€¼ã€‚å…¶æ ¸å¿ƒæå‡åœ¨äºï¼š**å°†è¿åŠ¨æ§åˆ¶ä»å•ä½“å¼æ¨¡å‹çš„â€œæŠ–åŠ¨ã€é«˜å»¶è¿Ÿã€é«˜èƒ½è€—â€å›°å¢ƒä¸­è§£æ”¾å‡ºæ¥ï¼Œå®ç°äº†â€œå¹³æ»‘ã€è¶…å¿«åå°„ã€é«˜èƒ½æ•ˆâ€çš„ç±»ç”Ÿç‰©è¿åŠ¨æ™ºèƒ½**ã€‚å…³é”®æ•°æ®å¦‚ **75% çš„æŠ–åŠ¨è¡°å‡ã€<20ms çš„åå°„ã€0.4W çš„åŠŸè€—**å’Œ**54.8% çš„ç¢°æ’æ¢å¤ç‡**ï¼Œæœ‰åŠ›åœ°è¯æ˜äº†è¯¥æ¡†æ¶åœ¨æ¨åŠ¨æœºå™¨äººå…·èº«æ™ºèƒ½èµ°å‘å®ç”¨åŒ–ã€é²æ£’åŒ–æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
            "timestamp": "2026-01-27T15:59:08.617524",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡â€œè®¨è®ºâ€éƒ¨åˆ†ï¼ˆSection 3: Discussionï¼‰åŠæ–¹æ³•ç»†èŠ‚ï¼ŒNeuroVLAæ¡†æ¶å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n### 1. **è®­ç»ƒä¸ç¡¬ä»¶å®ç°çš„è„±èŠ‚**\n- **å½“å‰æ–¹æ³•**ï¼šSNNï¼ˆè„‰å†²ç¥ç»ç½‘ç»œï¼‰çš„è®­ç»ƒä¾èµ–äºGPUåŠ é€Ÿå’Œ**æ›¿ä»£æ¢¯åº¦æ³•**è¿›è¡Œç«¯åˆ°ç«¯å­¦ä¹ ã€‚è¿™æœ¬è´¨ä¸Šæ˜¯åœ¨æ¨¡æ‹Ÿç¥ç»å½¢æ€è®¡ç®—ï¼Œè€Œéåœ¨çœŸæ­£çš„ç¥ç»å½¢æ€èŠ¯ç‰‡ï¼ˆå¦‚Loihiã€Tianjicï¼‰ä¸Šå®Œæˆè®­ç»ƒã€‚\n- **åæœ**ï¼šæ— æ³•å®Œå…¨å®ç°ç¥ç»å½¢æ€ç¡¬ä»¶æ‰¿è¯ºçš„**æè‡´èƒ½æ•ˆä¼˜åŠ¿**ã€‚è®­ç»ƒè¿‡ç¨‹æœ¬èº«ä»æ¶ˆè€—å¤§é‡ä¼ ç»Ÿè®¡ç®—èµ„æºã€‚\n\n### 2. **å­¦ä¹ æœºåˆ¶çš„å±€é™æ€§**\n- **ç¦»çº¿æ¨¡ä»¿å­¦ä¹ **ï¼šå½“å‰ç­–ç•¥å­¦ä¹ ä¸»è¦åŸºäº**è¡Œä¸ºå…‹éš†**ï¼Œè¿™æ˜¯ä¸€ç§ç¦»çº¿ã€ç›‘ç£å¼çš„å­¦ä¹ æ–¹æ³•ã€‚\n- **åæœ**ï¼šç¼ºä¹ç”Ÿç‰©ç³»ç»Ÿä¸­å…³é”®çš„**åœ¨çº¿ã€ç»ˆèº«å­¦ä¹ èƒ½åŠ›**ã€‚ç³»ç»Ÿæ— æ³•åœ¨éƒ¨ç½²åå®æ—¶é€‚åº”æœºæ¢°ç£¨æŸã€è´Ÿè½½å˜åŒ–æˆ–ç¯å¢ƒåŠ¨æ€æ¼”å˜ã€‚\n\n### 3. **æ¨¡å—åŒ–è®¾è®¡çš„æ·±åº¦ä¸çµæ´»æ€§**\n- **å›ºå®šå±‚æ¬¡ç»“æ„**ï¼šè™½ç„¶æ¨¡ä»¿äº†çš®å±‚-å°è„‘-è„Šé«“çš„ä¸‰çº§ç»“æ„ï¼Œä½†å„æ¨¡å—é—´çš„äº¤äº’å’Œæƒè´£åˆ’åˆ†æ˜¯ç›¸å¯¹å›ºå®šçš„ã€‚\n- **åæœ**ï¼šå¯èƒ½é™åˆ¶äº†ç³»ç»Ÿå¤„ç†è¶…å‡ºç°æœ‰è®¾è®¡èŒƒå›´çš„ã€å¼‚å¸¸å¤æ‚çš„ä¼ æ„Ÿå™¨èåˆä»»åŠ¡æˆ–éœ€è¦æ›´åŠ¨æ€å±‚æ¬¡é‡ç»„ä»»åŠ¡çš„èƒ½åŠ›ã€‚\n\n### 4. **åå°„è¡Œä¸ºçš„æ³›åŒ–æ€§**\n- **å®‰å…¨åå°„çš„é’ˆå¯¹æ€§**ï¼šè®ºæ–‡å±•ç¤ºçš„å¿«é€Ÿåå°„ï¼ˆå¦‚ç¢°æ’æ’¤å›ï¼‰æ˜¯é’ˆå¯¹ç‰¹å®šä¼ æ„Ÿå™¨ï¼ˆ6ç»´åŠ›ä¼ æ„Ÿå™¨ï¼‰å’Œé¢„è®¾åœºæ™¯è§¦å‘çš„ã€‚\n- **åæœ**ï¼šå¯¹äºæ›´å¤æ‚ã€å¤šæ¨¡æ€çš„å¨èƒï¼ˆå¦‚è§†è§‰è¯†åˆ«çš„ç¼“æ…¢æ¥è¿‘ç‰©ä½“ï¼‰ï¼Œè¿™ç§ä½å±‚åå°„æœºåˆ¶å¯èƒ½æ— æ³•ç›´æ¥é€‚ç”¨ï¼Œä»éœ€é«˜å±‚è¯­ä¹‰ä»‹å…¥ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nåŸºäºä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡åœ¨â€œè®¨è®ºâ€éƒ¨åˆ†ï¼ˆSection 3: Discussionï¼‰åŠè¡Œæ–‡é€»è¾‘ä¸­ï¼ŒæŒ‡å‡ºäº†æ˜ç¡®çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼š\n\n### 1. **å®ç°çœŸæ­£çš„â€œç¥ç»å½¢æ€å…¨æ ˆâ€**\n- **æ–¹å‘**ï¼šå°†**è®­ç»ƒè¿‡ç¨‹**ä¹Ÿè¿ç§»åˆ°ç¥ç»å½¢æ€èŠ¯ç‰‡ä¸Šã€‚\n- **å…·ä½“æ–¹æ³•**ï¼šæ¢ç´¢åˆ©ç”¨**è„‰å†²æ—¶åºä¾èµ–å¯å¡‘æ€§ï¼ˆSTDPï¼‰** ç­‰åŸºäºèŠ¯ç‰‡çš„æœ¬åœ°å­¦ä¹ è§„åˆ™ï¼Œå®ç°**åœ¨çº¿ã€æ— ç›‘ç£æˆ–å¼±ç›‘ç£çš„ç»ˆèº«å­¦ä¹ **ã€‚è¿™å°†ä½¿è„Šé«“æ¨¡å—èƒ½è‡ªé€‚åº”å…³èŠ‚æ‘©æ“¦å˜åŒ–ã€æ‰§è¡Œå™¨ç–²åŠ³ç­‰ã€‚\n\n### 2. **å¢å¼ºå­¦ä¹ ä¸è‡ªé€‚åº”èƒ½åŠ›**\n- **æ–¹å‘**ï¼šè¶…è¶Šè¡Œä¸ºå…‹éš†ï¼Œå¼•å…¥**å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰** æ¡†æ¶ã€‚\n- **å…·ä½“æ–¹æ³•**ï¼šè®©å°è„‘æ¨¡å—ä¸ä»…èƒ½å¹³æ»‘å’Œè°ƒåˆ¶ä¿¡å·ï¼Œè¿˜èƒ½åŸºäºä»»åŠ¡å¥–åŠ±ä¿¡å·**åœ¨çº¿ä¼˜åŒ–å…¶å†…éƒ¨å‰å‘æ¨¡å‹å’Œå¢ç›Šæ§åˆ¶ç­–ç•¥**ï¼Œå®ç°æ›´ä¼˜çš„åŠ¨æ€æ€§èƒ½ã€‚\n\n### 3. **æ·±åŒ–ç”Ÿç‰©æ‹ŸçœŸä¸æ¨¡å—äº¤äº’**\n- **æ–¹å‘**ï¼šæ„å»ºæ›´ç²¾ç»†ã€æ›´çµæ´»çš„ç”Ÿç‰©å¯å‘æ¶æ„ã€‚\n- **å…·ä½“æ–¹æ³•**ï¼š\n    - å¼•å…¥ç±»ä¼¼**åŸºåº•æ ¸**çš„æ¨¡å—æ¥å¤„ç†åŠ¨ä½œé€‰æ‹©ä¸ä¹ æƒ¯å½¢æˆã€‚\n    - å®ç°çš®å±‚ä¸å°è„‘ä¹‹é—´æ›´åŠ¨æ€çš„**åŒå‘é€šä¿¡**ï¼Œä½¿é«˜å±‚è®¡åˆ’èƒ½æ ¹æ®ä½å±‚æ‰§è¡Œçš„å®é™…çº¦æŸè¿›è¡Œæ›´çµæ´»çš„è°ƒæ•´ã€‚\n\n### 4. **æ‰©å±•æ„ŸçŸ¥-è¡ŒåŠ¨é—­ç¯**\n- **æ–¹å‘**ï¼šå°†ç¥ç»å½¢æ€å¤„ç†æ‰©å±•åˆ°æ›´å¹¿æ³›çš„æ„ŸçŸ¥æ¨¡æ€ã€‚\n- **å…·ä½“æ–¹æ³•**ï¼šä½¿ç”¨**äº‹ä»¶ç›¸æœº**æ›¿ä»£æˆ–è¡¥å……ä¼ ç»ŸRGBç›¸æœºï¼Œä¸ºå°è„‘å’Œè„Šé«“æ¨¡å—æä¾›åŸç”Ÿçš„äº‹ä»¶æµè§†è§‰è¾“å…¥ï¼Œå®ç°ä»æ„ŸçŸ¥åˆ°è¡ŒåŠ¨çš„**å…¨äº‹ä»¶é©±åŠ¨ã€è¶…ä½å»¶è¿Ÿé—­ç¯**ã€‚\n\n### 5. **æå‡æ³›åŒ–ä¸å¯æ‰©å±•æ€§**\n- **æ–¹å‘**ï¼šç ”ç©¶å¦‚ä½•å°†ç³»ç»Ÿä¸­æ¶Œç°çš„â€œè¿åŠ¨åŸºå…ƒâ€å’Œâ€œåŠŸèƒ½è§£è€¦â€ç‰¹æ€§ç³»ç»ŸåŒ–ã€å¯è§£é‡ŠåŒ–ã€‚\n- **å…·ä½“æ–¹æ³•**ï¼šå¼€å‘ç†è®ºæ¡†æ¶ï¼Œç†è§£è„Šé«“SNNä¸­**æ½œåœ¨è¡Œä¸ºæ¨¡å¼**æ˜¯å¦‚ä½•å½¢æˆå’Œç»„ç»‡çš„ï¼Œå¹¶åˆ©ç”¨è¿™äº›çŸ¥è¯†å®ç°è·¨ä»»åŠ¡ã€è·¨æœºå™¨äººå¹³å°çš„**æŠ€èƒ½å¿«é€Ÿè¿ç§»**ã€‚\n\n**æ€»ç»“è€Œè¨€**ï¼Œè®ºæ–‡çš„æ ¸å¿ƒå±€é™æ€§åœ¨äºå…¶**ç”Ÿç‰©å¯å‘çš„æ¶æ„åœ¨å®ç°ä¸Šå°šæœªå®Œå…¨â€œç¥ç»å½¢æ€åŒ–â€**ï¼Œä¸”å­¦ä¹ æœºåˆ¶ä»è¾ƒåŸå§‹ã€‚æœªæ¥çš„æ”¹è¿›å°†å›´ç»•**å®ç°çœŸæ­£çš„è½¯ç¡¬ä»¶ååŒç¥ç»å½¢æ€è®¡ç®—**ã€**å¼•å…¥æ›´å¼ºå¤§çš„åœ¨çº¿å­¦ä¹ èƒ½åŠ›**ä»¥åŠ**æ„å»ºæ›´å¤æ‚ã€çµæ´»çš„å±‚æ¬¡åŒ–æ™ºèƒ½ä½“**è¿™ä¸‰ä¸ªæ ¸å¿ƒæ–¹å‘å±•å¼€ã€‚",
            "timestamp": "2026-01-27T15:59:42.612205",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T04:04:44Z",
    "created_at": "2026-01-27T15:53:16.843732",
    "updated_at": "2026-01-27T15:53:16.843739"
}