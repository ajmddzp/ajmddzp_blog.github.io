{
    "id": "2601.09444v1",
    "title": "Data Scaling for Navigation in Unknown Environments",
    "authors": [
        "Lauri Suomela",
        "Naoki Takahata",
        "Sasanka Kuruppu Arachchige",
        "Harry Edelman",
        "Joni-Kristian KÃ¤mÃ¤rÃ¤inen"
    ],
    "abstract": "å°†æ¨¡ä»¿å­¦ä¹ è·å¾—çš„å¯¼èˆªç­–ç•¥æ³›åŒ–è‡³è®­ç»ƒæœªè§ç¯å¢ƒä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–æ¬¡é€šè¿‡å¤§è§„æ¨¡å®éªŒæ¢ç©¶æ•°æ®è§„æ¨¡ä¸æ•°æ®å¤šæ ·æ€§å¦‚ä½•å½±å“ç«¯åˆ°ç«¯ã€æ— åœ°å›¾è§†è§‰å¯¼èˆªåœ¨ç°å®ä¸–ç•Œä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚åŸºäºä»å…¨çƒ35ä¸ªå›½å®¶161ä¸ªåœ°ç‚¹é‡‡é›†çš„4,565å°æ—¶ç²¾é€‰ä¼—åŒ…æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ç‚¹ç›®æ ‡å¯¼èˆªç­–ç•¥ï¼Œå¹¶åœ¨å››ä¸ªå›½å®¶çš„ sidewalk æœºå™¨äººä¸Šé€šè¿‡125å…¬é‡Œè‡ªåŠ¨é©¾é©¶é‡Œç¨‹è¿›è¡Œé—­ç¯æ§åˆ¶æ€§èƒ½è¯„ä¼°ã€‚\n\nç ”ç©¶ç»“æœè¡¨æ˜ï¼šå¤§è§„æ¨¡è®­ç»ƒæ•°æ®å¯å®ç°æœªçŸ¥ç¯å¢ƒä¸­çš„é›¶æ ·æœ¬å¯¼èˆªï¼Œå…¶æ€§èƒ½æ¥è¿‘é€šè¿‡ç¯å¢ƒç‰¹å®šæ¼”ç¤ºæ•°æ®è®­ç»ƒçš„ç­–ç•¥ã€‚å…³é”®å‘ç°æ˜¯ï¼Œæ•°æ®å¤šæ ·æ€§è¿œæ¯”æ•°æ®è§„æ¨¡æ›´é‡è¦â€”â€”è®­ç»ƒé›†è¦†ç›–çš„åœ°ç†ä½ç½®æ•°é‡ç¿»å€å¯ä½¿å¯¼èˆªè¯¯å·®é™ä½çº¦15%ï¼Œè€Œåœ¨ç°æœ‰åœ°ç‚¹è¡¥å……æ•°æ®å¸¦æ¥çš„æ€§èƒ½æå‡åœ¨æå°‘é‡æ•°æ®æ—¶å³è¶‹äºé¥±å’Œã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œå¯¹äºå­˜åœ¨å™ªå£°çš„ä¼—åŒ…æ•°æ®ï¼ŒåŸºäºç®€å•å›å½’çš„æ¨¡å‹è¡¨ç°ä¼˜äºç”Ÿæˆå¼æ¶æ„å’Œåºåˆ—æ¨¡å‹ã€‚ç›¸å…³ç­–ç•¥ã€è¯„ä¼°æ¡†æ¶åŠç¤ºä¾‹è§†é¢‘å·²åœ¨é¡¹ç›®é¡µé¢å¼€æºã€‚",
    "url": "https://arxiv.org/abs/2601.09444v1",
    "html_url": "https://arxiv.org/html/2601.09444v1",
    "html_content": "Data scaling for navigation in unknown environments\nLauri Suomela\n1\n,\nNaoki Takahata\n2\n,\nSasanka Kuruppu Arachchige\n1\n,\nHarry Edelman\n3\n,\nJoni-Kristian KÃ¤mÃ¤rÃ¤inen\n1\n1\nTampere University,\n2\nTohoku University,\n3\nTurku University of Applied Sciences.\nâ€ \nlasuomela.github.io/navigation_scaling\nAbstract\nGeneralization of imitation-learned navigation policies to environments unseen in training remains a major challenge. We address this by conducting the first large-scale study of how data quantity and data diversity affect real-world generalization in end-to-end, map-free visual navigation. Using a curated 4,565-hour crowd-sourced dataset collected across 161 locations in 35 countries, we train policies for point goal navigation and evaluate their closed-loop control performance on sidewalk robots operating in four countries, covering 125 km of autonomous driving.\nOur results show that large-scale training data enables zero-shot navigation in unknown environments, approaching the performance of policies trained with environment-specific demonstrations. Critically, we find that data diversity is far more important than data quantity. Doubling the number of geographical locations in a training set decreases navigation errors by\nâˆ¼\n15\n%\n\\sim 15\\%\n, while performance benefit from adding data from existing locations saturates with very little data. We also observe that, with noisy crowd-sourced data, simple regression-based models outperform generative and sequence-based architectures. We release our policies, evaluation setup and example videos on the project page\nâ€ \n.\nI\nINTRODUCTION\nIn recent years, learning-based approaches to robot navigation have received increasing interest. They enable communicating with humans in natural language, utilizing semantic cues in the environment, and performing tasks whose solutions are difficult to engineer by hand.\nA key challenge in learning-based methods is generalization beyond locations seen in the training data. To be useful, a navigation algorithm should be able to function in a novel, unseen environment without additional finetuning.\nData scaling\n[\n28\n]\nis a promising approach to addressing generalization in robot learning, yet its different dimensions remain under-explored.\nRecent work in robotic manipulation\n[\n21\n,\n2\n,\n8\n]\nshows that imitation learning policiesâ€™ out-of-domain performance is strongly influenced by training data\ndiversity\nrather than quantity alone. Whether similar scaling behavior holds for navigation is unclear, as navigation involves long-horizon, closed-loop decision making where errors compound over time.\nWe hypothesize that the geographic variability encountered during training is a major driver of diversity that benefits navigation performance. Different locations naturally vary in visual appearance, material composition, and route structures.\nMost prior work on learning-based navigation has not explicitly examined how training data diversity impacts generalization, in part due to the lack of sufficiently large and geographically diverse real-world datasets. Collecting expert navigation demonstrations across many locations is labor-intensive.\nIn this work, we achieve sufficient scale by leveraging non-expert, crowd-sourced demonstrations collected by a globally distributed fleet of low-cost sidewalk robots teleoperated over the internet.\nWe present an analysis that disentangles how training data quantity and geographic diversity individually contribute to navigation performance in unknown environments. Specifically, we focus on vision-based navigation methods that formulate navigation as an end-to-end imitation learning task,\nand use the number of distinct locations in the training set as a proxy measure for diversity.\nWe train map-free point-goal\n[\n1\n]\nnavigation policies using different subsets of a crowd-sourced dataset comprising\n4600\nh\n4600\\text{\\,}\\mathrm{h}\nof demonstrations collected across 35 countries. These subsets jointly vary the number of training locations and the amount of data sampled from each location.\nThe resulting policiesâ€™ generalization to environments not seen during training is evaluated on robots situated in four different countries.\nIn total, our experiments comprise 125 kilometers of real-world autonomous driving, which took 120 hours to complete.\nWe find that a policy trained with diverse large-scale data can control a robot in an unseen environment better than an in-domain policy trained specifically for that environment, and almost as well as a policy incorporating both large-scale and in-domain data.\nAnalysis of the individual data scale factors shows that increasing the amount of training data from any given set of locations saturates with very little data per location. In contrast, the number of navigation failures consistently decreases with a power-law relationship to the number of distinct training locations.\nDoubling the number of training locations leads to a\nâˆ¼\n15\n%\n\\sim 15\\%\ndrop in navigation failures.\nAs a summary of our contributions,\nâ€¢\nWe perform the first in-depth, real-world investigation of data scaling for learning-based autonomous navigation.\nâ€¢\nWe show that generalization to unseen environments scales well with increased geographical diversity of the training data, while benefit from additional data from existing locations saturates surprisingly quickly.\nâ€¢\nTo support reproducible research, we open-source the trained policies, all code and the deployment setup. The dataset is available upon request.\nII\nRELATED WORK\nVision-based Navigation in Unknown Environments.\nNavigation in unknown environments is commonly implemented as finding minimum-cost paths through cost maps with geometric\n[\n7\n,\n9\n,\n24\n]\nor learned\n[\n11\n,\n46\n,\n36\n]\ncosts.\nThey often rely on depth measurements and accurate synchronization between different sensors.\nThis makes the methods challenging to implement on low-cost robot platforms with limited sensing.\nIn this work we focus on end-to-end imitation learning approaches that can be deployed with a minimal set of sensors.\nEvaluating such policies in environments unseen during training is standard practice.\nHowever, many of the works with real-world deployment\n[\n30\n,\n32\n,\n40\n,\n34\n]\nutilize image-goals, and often require prior maps of the environment or only perform short-horizon navigation.\nPoint goal navigation\n[\n1\n]\nis an alternative form of goal specification that is better suited for long-horizon tasks.\nStill, most works on the point goal navigation are limited to short-horizon goal reaching\n[\n23\n]\nor simulation\n[\n42\n]\n.\nIn this work we focus on the topic of long-horizon point goal navigation.\nWhile the experiments in the existing work\n[\n18\n,\n14\n,\n17\n,\n15\n]\ntend to be centered around a single geographic location, we deploy policies in multiple different countries without any prior knowledge of the environment.\nData Scaling in Robotics.\nMost work on scaling robotics datasets has been conducted in the context of robotic manipulation\n[\n29\n]\n. Recent studies\n[\n2\n,\n8\n,\n21\n,\n41\n]\nsuggest that for generalization to unseen environments, dataset\ndiversity\nfrom scaling the number of locations from which training data is sampled is more important than absolute data quantity.\nThe phenomenon, however, has not been thoroughly investigated in the context of navigation.\nSeveral works present some data scaling experiments, but the resultsâ€™ significance is limited by use of open-loop metrics\n[\n48\n,\n23\n]\nor experimentation in simulation\n[\n48\n,\n43\n]\n. Furthermore, they do not explicitly address generalization to novel environments or examine dataset composition.\nIn this work, we specifically investigate how policy performance in unseen environments depends on both the quantity and the geographic diversity of training data. Our analysis is based on the results of closed-loop experiments conducted on real robots.\nNavigation Datasets.\nSimilar to most areas of robotics, data for training vision-based navigation models is scarce. Commonly utilized datasets contain around\n100\nh\n100\\text{\\,}\\mathrm{h}\nof demonstrations\n[\n31\n,\n16\n]\nfrom a limited number of locations.\nIncreasing the amount of training data has been explored using several different approaches.\nSimulation is an attractive option for generating large amounts of demonstrations\n[\n42\n,\n6\n,\n39\n]\n, but models trained with synthetic data suffer from the simulation-to-real gap.\nAutonomous driving datasets are larger, in the order of thousands of hours\n[\n44\n,\n45\n]\n, but closed-loop evaluation on an autonomous vehicle is risky and deployment on smaller platforms incurs an embodiment gap.\nYet another avenue is to train with generic internet videos\n[\n22\n,\n23\n]\n, but the resulting policies need to be fine-tuned with embodiment-specific data before deployment.\nFinally, embodiment-specific data collection can be scaled by crowd-sourcing the process\n[\n26\n,\n13\n]\n.\nImitating these potentially sub-optimal demonstrations using noisy, low-cost sensors can be challenging\n[\n17\n]\n, but we find that rigorous data cleaning enables us to train navigation policies even from such data.\nIn this work, we utilize a dataset extracted from\n8.000\nh\n8.000\\text{\\,}\\mathrm{h}\nof raw crowd-sourced demonstrations.\nIII\nMETHODS\nProblem statement.\nWe consider PointGoal\n[\n1\n]\nnavigation, where an agent is tasked with navigating to goals specified as 2D coordinates without access to a map. At each time step\nt\nt\n, the agent receives an RGB image observation\nO\nt\nO_{t}\nand the (noisy) distance and direction\ng\nt\n=\n[\nd\nÎ¸\n]\ng_{t}=\\big[\\begin{smallmatrix}d\\\\\n\\theta\\end{smallmatrix}\\big]\nto the goal coordinate\nG\nG\n.\nThe navigation agent learns a policy\nÏ€\nâ€‹\n(\nğš\nt\n|\nğ\nt\n,\nğ \nt\n)\n\\pi(\\mathbf{a}_{t}|\\mathbf{O}_{t},\\mathbf{g}_{t})\n(1)\nthat maps the sequence of\nP\nP\nmost recent observations\nğ\nt\n=\n{\nO\nt\nâˆ’\nP\n+\n1\n,\nâ€¦\n,\nO\nt\n}\n\\mathbf{O}_{t}=\\left\\{O_{t-P+1},\\ldots,O_{t}\\right\\}\n,\nğ \nt\n=\n{\ng\nt\nâˆ’\nP\n+\n1\n,\nâ€¦\n,\ng\nt\n}\n\\mathbf{g}_{t}=\\left\\{g_{t-P+1},\\ldots,g_{t}\\right\\}\nto an action chunk\nğš\nt\n=\n{\na\nt\n,\nâ€¦\n,\na\nt\n+\nH\nâˆ’\n1\n}\n\\mathbf{a}_{t}=\\left\\{a_{t},\\ldots,a_{t+H-1}\\right\\}\nof\nH\nH\nfuture actions towards the goal.\nFrodoBots8k Dataset.\nWe train the policies on navigation data collected using the FrodoBots AI crowd-sourcing platform\n[\n12\n]\n.\nTo incentivize the general public to contribute data, the platform casts the task as a game where players teleoperate actual robots over the internet.\nThe robots are located across different countries around the world. Since each robot only produces data from the immediate surroundings of its â€™home baseâ€™, the resulting dataset is geographically clustered into distinct locations. For each location, there are multiple repetitions of roughly the same route, performed at distinct times and dates by different robot operators.\nWe curate our training dataset from a larger batch of raw teleoperation data that totals around\n8000\nh\n8000\\text{\\,}\\mathrm{h}\nof navigation.\nTo cluster the data geographically, we first find pairs of navigation episodes whose GPS locations are less than\n100\nm\n100\\text{\\,}\\mathrm{m}\napart. Then, the pair-wise connectivity is propagated to find connected sets that we refer to as distinct training data locations.\nTo improve data quality, we remove navigation episodes and episode segments where the robot is stationary or any sensor measurements are missing. After filtering, we are left with\n4565\nh\n4565\\text{\\,}\\mathrm{h}\nof navigation from 161 distinct locations across 35 different countries on all continents except Antarctica.\nSince the robotsâ€™ low-quality sensors are prone to noise, we improve pose estimates via sensor fusion. We combine GPS measurements, magnetometer readings, and wheel RPMs using non-linear factor graph optimization with GTSAM\n[\n10\n]\n. However, faulty magnetometers on some robots limit the accuracy of the resulting orientation estimates.\nThe recorded navigation episodes do not contain information about which goal coordinate the player was trying to reach at any given time, and the routes are often circular. To sample plausible demonstrations of reaching some goal, we divide the navigation episodes into segments\nby finding peaks in the relationship between traveled path length and the Euclidean distance from segment start location. An example is shown in Fig.\n1\n.\nFinally, all sensors are subsampled and aligned to\n4\nHz\n4\\text{\\,}\\mathrm{Hz}\n. The player-issued robot velocity commands are resampled so that each sensor time step\nt\nt\nreceives an associated\n10\nHz\n10\\text{\\,}\\mathrm{Hz}\nchunk\na\nt\ng\nâ€‹\nt\na_{t}^{gt}\nof actions over the next\n1\ns\n1\\text{\\,}\\mathrm{s}\n.\nAny demonstrations from locations closer than\n95\nkm\n95\\text{\\,}\\mathrm{km}\nto our test sites are removed from the training set.\nFigure 1\n:\nA crowd-sourced navigation episode segmented into 5 demonstrations for policy training.\nPolicy design.\nWe experiment with multiple alternative policy architectures that all share the same basic structure.\nImage observations\nO\nt\nO_{t}\nare encoded by a vision transformer into patch tokens that are compressed into a single feature vector with a 2D convolutional layer, similar to\n[\n25\n]\n.\nThe distances and directions to goal\ng\nt\ng_{t}\nare encoded into a\n64\nâ€‹\nD\n64D\nvector with a 3-layer MLP followed by LayerNorm.\nFrom here on, the different architectures diverge (see Sec.\nIV-D\n), but ultimately all of them produce an action chunk\nğš\nt\n\\mathbf{a}_{t}\nof robot control commands.\nBased on the model comparison study in Sec.\nIV-D\n, we chose to perform our main experiments with the\nMLP-BC\npolicy using the Theia-Base\n[\n33\n]\nvision encoder. The MLP-BC policy simply concatenates the latest (\nP\n=\n1\nP=1\n) image embedding and goal embedding, and an MLP head maps the concatenated vector into an action chunk.\nTraining details.\nThe policies are trained to imitate the player-issued velocity-space robot control commands given the associated observations.\nA training sample consists of\nP\nP\nfront camera observations\nğ\nt\n\\mathbf{O}_{t}\n, and the distances and directions\nğ \nt\n\\mathbf{g}_{t}\nto a goal location\nG\nG\nfor each observation.\nğ\nt\n\\mathbf{O}_{t}\nis picked from a demonstration trajectory at a random time\nt\nt\n, and\nG\nG\nis sampled from the robotâ€™s future trajectory within\n150\nm\n150\\text{\\,}\\mathrm{m}\ntraveled distance from the robot location at observation time\nt\nt\n.\nThe images are resized to\n224\nÃ—\n224\n224\\times 224\n.\nThe goal inputs\ng\nt\ng_{t}\nare obtained as the Euclidean distance and direction to\nG\nG\nfrom the robotâ€™s pose at time\nt\nt\n. The goal direction\nÎ¸\nâˆˆ\n[\nâˆ’\nÏ€\n,\nÏ€\n]\n\\theta\\in[-\\pi,\\pi]\nis normalized to\n[\nâˆ’\n1\n,\n1\n]\n[-1,1]\n, and the distance\nd\nd\nis clipped to\n[\n0\n,\n1\n]\n[0,1]\nin kilometers.\nThe 2D action-space\nğ’œ\nâŠ‚\nâ„\n2\n,\na\n=\n(\nv\n,\nÏ‰\n)\n\\mathcal{A}\\subset\\mathbb{R}^{2},~a=(v,\\omega)\nof forward linear and yaw angular velocities is normalized to\n[\nâˆ’\n1\n,\n1\n]\n[-1,1]\n. The chunks\nğš\nt\ng\nâ€‹\nt\n\\mathbf{a}_{t}^{gt}\nof\nH\n=\n10\nH=10\nactions issued by a player over\n1\ns\n1\\text{\\,}\\mathrm{s}\nwindow are used as prediction targets.\nWe train all policies with batch size of 2048, an AdamW optimizer and a learning rate of\n1\nâ€‹\ne\nâˆ’\n4\n1e-4\ndecayed with cosine schedule. Training is distributed across 16 AMD MI250x GPUs. The policies with a generative objective are trained with the L2 loss.\nPolicies trained for a regression objective with the regular L2 loss, however, learn to just drive straight forward unless about to collide with a obstacle.\nGiven that a major part of the dataset demonstrations consists of driving straight ahead, we observe better goal-following behavior when we increase the weight of the L2 loss\nâ„’\n2\n\\mathcal{L}_{2}\nfor samples with non-zero target angular velocity as\nâ„’\ns\nâ€‹\nc\nâ€‹\na\nâ€‹\nl\nâ€‹\ne\nâ€‹\nd\n=\nâ„’\n2\nâ‹…\n(\ns\nm\nâ€‹\ni\nâ€‹\nn\n+\n(\ns\nm\nâ€‹\na\nâ€‹\nx\nâˆ’\ns\nm\nâ€‹\ni\nâ€‹\nn\n)\nâ‹…\n|\nÏ‰\n|\n)\n,\n\\mathcal{L}_{scaled}=\\mathcal{L}_{2}\\cdot(s_{min}+(s_{max}-s_{min})\\cdot|\\omega|),\n(2)\nwhere\ns\nm\nâ€‹\ni\nâ€‹\nn\ns_{min}\nand\ns\nm\nâ€‹\na\nâ€‹\nx\ns_{max}\nare the minimum and maximum scaling weights, and\n|\nÏ‰\n|\nâˆˆ\n[\n0\n,\n1\n]\n|\\omega|\\in[0,1]\nis the largest angular velocity magnitude in the associated\nğš\nt\ng\nâ€‹\nt\n\\mathbf{a}_{t}^{gt}\n. We set\ns\nm\nâ€‹\ni\nâ€‹\nn\ns_{min}\nto 1 and\ns\nm\nâ€‹\na\nâ€‹\nx\ns_{max}\nto 10.\nWe apply standard color jitter augmentation to the images. The\nO\nt\nO_{t}\n,\ng\nt\ng_{t}\nand\na\nt\ng\nâ€‹\nt\na_{t}^{gt}\nare mirrored horizontally with 50% probability. We train all policies for two epochs, the exact number of optimizer steps depending on dataset size.\nIV\nEXPERIMENTS\n(a)\nWuhan, China.\nWGS84 (30.48244, 114.30264)\n(b)\nKisumu, Kenya.\n(-0.11052, 34.75131)\n(c)\nPort Louis, Mauritius.\n(-20.17148, 57.49782)\n(d)\nSelebi Phikwe, Botswana.\n(-21.98324, 27.83091)\nFigure 2\n:\nThe test route segment checkpoints and robot front camera views in the different environments.\nWe performed a series of experiments to answer the following research questions:\nâ€¢\nQ1\n:\nDoes training on large-scale data enable a policy to generalize and successfully navigate in unseen environments across diverse geographic regions?\nâ€¢\nQ2\n: How does the generalization performance depend on the\nquantity\nand\ndiversity\nof training data?\nâ€¢\nQ3\n: How well do different policy architectures perform in a setting with crowd-sourced training data and remote inference?\nIV-A\nExperiment setup.\nPolicy Deployment.\nWe perform all experiments on Earth Rover Zero robots, the same robot type used for collecting the training dataset. It is a differential drive robot designed for remote teleoperation over the internet. The robots, located in different countries around the world, stream sensor observations to a remote machine that runs policy inference and sends control commands back to the robot. We utilize a desktop computer with an Nvidia RTX3090 GPU. With good network conditions, the round-trip latency including policy inference is\n<\n<\n1\ns\n1\\text{\\,}\\mathrm{s}\n. The policies receive monocular RGB image observations from the robotâ€™s\n110\nÂ°\n110\\text{\\,}\\mathrm{\\SIUnitSymbolDegree}\nFOV\nfrontal camera, distance to goal calculated from GPS, and direction to goal computed from magnetometer readings fused with optical flow by a Kalman filter. All policies run at\n4\nHz\n4\\text{\\,}\\mathrm{Hz}\n.\nTest environments.\nPolicy performance is evaluated in 4 different locations situated in China, Kenya, Mauritius, and Botswana, demonstrated in Figure\n2\n. The test routes represent typical navigation tasks in urban and semi-urban environments, with lengths from\n250\nm\n250\\text{\\,}\\mathrm{m}\nto\n400\nm\n400\\text{\\,}\\mathrm{m}\n. Each test route is divided into multiple shorter segments\n30\nm\n30\\text{\\,}\\mathrm{m}\n-\n90\nm\n90\\text{\\,}\\mathrm{m}\nin length. To complete a full test route, the robot sequentially navigates the series of segment checkpoints. In total, our test environments contain 30 individual navigation segments.\nFigure 3\n:\nPolicies trained with large-scale data achieve higher success rates compared to environment-specific policies. The zero-shot policy almost matches the policy that incorporates both large-scale data and data from the test environments.\nEvaluation and metrics.\nWe use\nnavigation success rate\n[\n1\n]\n, the proportion of successful navigation attempts, as our main metric.\nFor increased granularity, we report success rates over\nsegments\nrather than evaluate route-level performance over the full routes.\nIf a policy fails a segment, we teleoperate the robot to the start location of the next segment and continue evaluation from there.\nWe consider a segment failed if the robot collides with an object and cannot independently get unstuck, deviates from the test route in such a way that there is no chance of recovering, or the robot operator has to intervene in order to prevent harm to the robot or bystanders. We do not count interventions needed because of external circumstances,\ne.g\n.\nintentional harassment by humans or yielding to vehicles as safety precaution.\nA segment is considered successful if the robot arrives within\n10\nm\n10\\text{\\,}\\mathrm{m}\nof the segment checkpoint. The relatively loose success margin was chosen to accommodate GPS noise.\nTo capture variation in weather conditions, network latency and GPS drift, we test each policy multiple times in each environment on different dates and times of day.\nHowever, as a single test run can take up to\n30\nmin\n30\\text{\\,}\\mathrm{min}\nand we test a large number of different policies, we needed to balance the number with practical feasibility. As compromise, we perform 3 repetitions of each route with each policy in all experiments.\nWe report the average success rate and its 95% confidence interval from the continuity-corrected Wilson method\n[\n27\n]\nover segments from the different test environments.\nTo support additional analysis, we provide auxiliary metrics for some of the experiments. We calculate\n1) Normalized Intervention Rate (NIR)\n- the number of interventions needed to make\n100\nm\n100\\text{\\,}\\mathrm{m}\nof route progress\n2) Normalized Progress Speed (NPS)\n- duration (seconds) to make\n100\nm\n100\\text{\\,}\\mathrm{m}\nof route progress, calculated from successful segments\n3) Distance (Dist.)\n- a route-level metric measuring progress along the test route until the first failure.\nIV-B\nLarge-scale data enables navigation in unseen environments.\nTo study\nQ1.\n, we perform an experiment to evaluate policy zero-shot navigation performance in unseen environments. To contextualize the result, we compare the zero-shot policyâ€™s performance with in-domain policies trained with data from the test environments.\nWe train three versions of the MLP-BC policy, and evaluate them in the environments described in Section\nIV-A\n. The policies are denoted as\nZero-shot:\nTrained with the full train split of the cleaned FrodoBots8K dataset, comprising\n3600\nh\n3600\\text{\\,}\\mathrm{h}\nfrom across 153 locations. Deployed zero-shot to unseen environments.\nScale + in-domain:\nA policy trained with the full train split\nplus\n25\nh\n25\\text{\\,}\\mathrm{h}\nof demonstrations from each test location.\nFor all environments except Selebi Phikwe, the in-domain data includes demonstrations of the route used in the experiments.\nIn-domain only:\nA small-data baseline - separate policy for each test location, trained exclusively on\n25\nh\n25\\text{\\,}\\mathrm{h}\nof demonstrations from the corresponding environment.\nResults.\nFigure\n3\nshows the navigation success rates, and Figure\n4\ndemonstrates the policy paths over one repetition in the Wuhan environment.\nThe environment-specific policies overfit to the training routes. They tend to ignore the goal vector, and instead try to repeat the route seen during training.\nThis is beneficial on routes like Kisumu, where the robot never visits the same place twice. In Wuhan, where the robot visits the environmentâ€™s central location multiple times with different goals, the policy often starts driving down a wrong part of the route.\nFor a reason we could not determine, the policy specific to Selebi Phikwe only drives straight, resulting in zero success.\nThe policies trained with data from a large number of different locations perform better.\nThe zero-shot policy is capable of complex behaviors like avoiding obstacles, stopping for humans, and finding intersections.\nGenerally, success rates of the zero-shot and in-domain versions of the large-scale policy are very close.\nFigure 4\n:\nPolicy paths until the first failure in Wuhan. See Fig.\n2(a)\nfor the mission checkpoints.\nFigure 5\n:\nFailure reasons for the zero-shot policy.\nAs shown by Figure\n5\n, the most common failure mode for the zero-shot policy is deviating from the route\ne.g\n.\nbecause of taking a wrong turn at an intersection or failing to perform a\n180\nÂ°\n180\\text{\\,}\\mathrm{\\SIUnitSymbolDegree}\nturn after goal change.\nWe hypothesize two potential causes for this behavior, the noisy robot orientation in the training data, and the absence of longer-term planning in the point goal formulation of the navigation task.\nThe metrics in Table\nI\nprovide additional insight into policy performance. Incorporating large-scale data brings benefits across all metrics except progress speed, which mostly depends on the magnitude of linear velocities that a policy commands.\nOverall, these results show that large-scale training data enables robust, non-trivial navigation across diverse environments. Scale gives rise to emergent behaviors, such as obstacle avoidance and stopping for pedestrians, that are absent in small-scale in-domain policies. Moreover, the small performance gap between the large-scale zero-shot and in-domain policies suggests that sufficiently diverse training data can largely remove the need for collecting demonstrations in the deployment environment.\nNext, we turn to study how the individual factors of large-scale data contribute to zero-shot navigation performance.\nPolicy\nNIR\n(\nâ†“\n)\n(\\downarrow)\nNPS\n(\nâ†“\n)\n(\\downarrow)\nDist.\n(\nâ†‘\n)\n(\\uparrow)\nZero-shot\n0.86\n226\ns\n226\\text{\\,}\\mathrm{s}\n64\nm\n64\\text{\\,}\\mathrm{m}\nScale + in-domain\n0.67\n220\ns\n220\\text{\\,}\\mathrm{s}\n123\nm\n123\\text{\\,}\\mathrm{m}\nIn-domain only\n1.86\n210\ns\n210\\text{\\,}\\mathrm{s}\n39\nm\n39\\text{\\,}\\mathrm{m}\nTABLE I\n:\nAdditional metrics for the first experiment. Averages over all test environments.\nIV-C\nData diversity beats absolute scale.\nTo examine\nQ2.\n, we designed an experiment to disentangle how training data\nsize\nand\ngeographic diversity\nindependently contribute to zero-shot navigation performance in unseen environments.\nWe use the number of distinct locations in the training set as a proxy for data diversity, as different locations naturally induce variation in visual appearance, scene layout, navigation maneuvers, and route structure.\nWe train different versions of the MLP-BC policy with subsets of the full\n3600\nh\n3600\\text{\\,}\\mathrm{h}\ndataset, varying\n1)\nthe number of geographic locations from which training data is sampled, and\n2)\nthe amount of data sampled from each location.\nIn both cases, we ensure the subsets of increasing size are supersets of the smaller ones.\nThe feasible combinations of location count and per-location data are restricted by the composition of the full dataset.\nIn total, we train 21 different policies, and evaluate them on each test route 3 times.\nFigure 6\n:\nSuccess rate\n(\nâ†‘\n\\uparrow\n) as function of number of train locations, for\nfixed amounts of total training data\n.\nFigure 7\n:\nSuccess rate\n(\nâ†‘\n\\uparrow\n) as function of number of train locations, for\nfixed amounts of training data per location\n.\nFigure 8\n:\nSuccess rate\n(\nâ†‘\n\\uparrow\n) as function of training data per location, for\nfixed numbers of locations\n.\nResults.\nIn general, the policy success rates increase with additional training locations. This holds both in the case where the total dataset size is held constant (Fig.\n6\n), and the case where the total dataset size increases with new locations (Fig.\n7\n). As seen from Fig.\n8\n, increasing the quantity of training data from any fixed set of environments has negligible effect on policy performance.\nWe observe an anomaly in the performance of policies trained with data from 8 locations, suggesting that the data quality across the locations is not equal. This specific mix of locations causes the policies to get stuck in infinite spinning loops in the Kisumu environment, leading to a dip in the overall success rates. The effect, however, is remedied when the number of training locations is further increased.\nThe results demonstrate that for navigation performance in unknown environments, it is more beneficial to increase training data size by gathering data from as many different environments as possible, rather than collect large amounts of data from few environments. Fig.\n8\nsuggests that the performance increase is already saturated at\n2\nh\n2\\text{\\,}\\mathrm{h}\nper location, below the range of quantities we considered in the experiment. At 64 environments, the trend of performance increase from new locations does not show signs of saturation.\nFigure 9\n:\nPower law fits and correlation coefficients\nr\nr\nfor\nTotal Dataset Size =\n128\nh\n128\\text{\\,}\\mathrm{h}\n(Fig.\n6\n) and\nHours per Location =\n16\nh\n16\\text{\\,}\\mathrm{h}\n(Fig.\n7\n) on a log-log scale.\nTo further analyze the connection between data diversity and navigation performance, we examine if they exhibit the same power-law relationship\nY\n=\nÎ²\nâ‹…\nX\nÎ±\nY=\\beta\\cdot X^{\\alpha}\nobserved in data scaling studies in other fields\n[\n19\n,\n21\n]\n.\nWe fit a linear model on the log-transformed number of locations and the navigation\nF\nâ€‹\na\nâ€‹\ni\nâ€‹\nl\nâ€‹\nu\nâ€‹\nr\nâ€‹\ne\nâ€‹\nr\nâ€‹\na\nâ€‹\nt\nâ€‹\ne\n=\n(\n1\nâˆ’\nS\nâ€‹\nu\nâ€‹\nc\nâ€‹\nc\nâ€‹\ne\nâ€‹\ns\nâ€‹\nr\nâ€‹\na\nâ€‹\nt\nâ€‹\ne\n)\nFailure~rate=(1-Succes~rate)\n. The results shown in Fig.\n9\ndo demonstrate the existence of a power-law relationship, indicated by high values of Pearsonâ€™s\nr\nr\n. While variation in the data quality between different environments causes the fit not to be perfect, the overall trend of navigation failures decreasing proportional to increase in number of train locations is clear.\nSubstituting the coefficients from the left panel in Fig.\n9\n, doubling the number of locations decreases failures by\n1\nâˆ’\nY\nâ€‹\n(\n2\nâ€‹\nX\n)\nY\nâ€‹\n(\nX\n)\n=\n1\nâˆ’\n2\nâˆ’\n0.229\nâ‰ˆ\n15\n%\n1-\\frac{Y(2X)}{Y(X)}=1-2^{-0.229}\\approx 15\\%\n.\nIV-D\nWith noisy data, simple models can be the most effective.\nIn order to answer\nQ3.\n, we compared different policy architectures trained with regression and generative objectives.\nWe emphasize that we are\nnot\nproposing a new navigation method - we seek to determine if utilization of crowd-sourced demonstrations or remote deployment on low-cost robots impact the policy choice for the main experiments in Sections\nIV-B\nand\nIV-C\n.\nSome are navigation-specific architectures from earlier work\n[\n32\n,\n17\n,\n38\n]\n, and others are general robot policies\n[\n5\n,\n3\n]\n. We compare both single-observation models (\nP\n=\n1\nP=1\n) and ones that utilize sequences of historical observations (\nP\n>\n1\nP>1\n). We train all policies with a total of\n1024\nh\n1024\\text{\\,}\\mathrm{h}\nof data from 32 different locations.\nAll the policies utilize the Theia-Base\n[\n33\n]\nvision encoder unless stated otherwise.\nMLP-BC:\nThe latest image and goal embeddings are simply concatenated and passed to an MLP head to produce a chunk of\nH\nH\nfuture actions\nğš\nt\n\\mathbf{a}_{t}\n. Trained with a regression objective.\nDiffusion Policy (DP)\n[\n5\n]\n:\nThe concatenated observation embedding conditions a diffusion denoising U-Net that maps Gaussian noise into an action chunk\nğš\nt\n\\mathbf{a}_{t}\n. Deployed with temporal ensembling\n[\n47\n]\nto reduce mode-switching.\nFlow-matching Policy (FM):\nSame architecture as DP, but trained with a flow-matching objective\n[\n3\n]\n. Instead of temporal ensembling, the policy is deployed with real-time chunking\n[\n4\n]\nusing pseudo-inverse guidance\n[\n37\n]\n.\nViNT\nâˆ—\n[\n32\n]\n/ LogoNav\n[\n17\n]\n:\nThe latest goal embedding and 6 latest image embeddings are processed by a non-causal transformer, and an MLP head maps an output token into an action chunk. Trained with a regression objective.\nNoMAD\nâˆ—\n[\n38\n]\n:\nSame architecture as ViNT\nâˆ—\n, but instead of an MLP head trained for regression, a denoising U-Net\n[\n5\n]\nconditioned with the observation sequence produces the action chunk.\nResults.\nFigure 10\n:\nComparison of policy architecture success rates, averaged over the segments for 3 repetitions in all test environments.\nFigure\n10\nshows the comparison results.\nOverall, even though different methods perform worse in some test environments and better in others, there is surprisingly little difference in the average success rates.\nInterestingly, the MLP-BC policies perform the best. As also observed in\n[\n20\n]\n, we hypothesize that the noisy sensors and crowd-sourced demonstrations cause the generative policies (DP, FM, NoMAD) capable of modeling multi-modal action distributions to also fit the noise in the data, decreasing performance during deployment. The regression objective causes the models fit a single mode of actions, reducing the effect of noisy data.\nViNT\nâˆ—\n, which is very similar to the MLP-BC policies but utilizes a sequence of latest observations, performs worse despite being trained with a regression objective. From qualitative observations it seems like the policy learns to exploit the temporal correlation between subsequent observations too much, decreasing the policyâ€™s reactivity to\ne.g\n.\nobstacles.\nIn addition, the variable FPS and control loop latency caused by running the policies remotely can be an issue for the sequence models.\nFinally, we observe very little difference between the MLP-BC policies utilizing Theia\n[\n33\n]\nor DINOv3\n[\n35\n]\n, suggesting that the visual encoder is not the primary performance bottleneck.\nV\nDISCUSSION & FUTURE WORK\nWe show that training end-to-end visual navigation policies on large-scale data enables generalization to unseen environments across different geographic regions. These policies significantly outperform environment-specific, in-domain policies, even when evaluated in a zero-shot manner. Our results highlight geographic diversity as a main driver of generalization. While performance gains from adding more data from a fixed set of locations saturate quickly, increasing the number of distinct training locations reduces navigation errors following a power-law relationship.\nThese results suggest several promising directions for future research. Extending the analysis to other domains, such as off-road navigation or aerial robotics, would help assess the generality of our findings. We also observed variation in the utility of data across training environments, as reflected in downstream policy performance. A more fine-grained understanding of the underlying causes of this variation is an important direction for future work. Finally, while we focus on map-free navigation, many real-world systems can leverage coarse maps (e.g., OpenStreetMap) to provide additional context. Whether such information can further improve generalization of learning-based systems remains an open question. Overall, we hope these findings motivate continued investigation into dataset composition and data scaling strategies for navigation and robot learning.\nACKNOWLEDGMENT\nWe wish to thank the FrodoBotsAI team for operations support and generous access to their robot fleet.\nWe acknowledge CSC, IT Center for Science, Finland, for awarding this project access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium.\nReferences\n[1]\nP. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, and A. R. Zamir\n(2018-07)\nOn Evaluation of Embodied Navigation Agents\n.\narXiv:1807.06757 [cs]\n.\nExternal Links:\nLink\nCited by:\nÂ§I\n,\nÂ§II\n,\nÂ§III\n,\nÂ§\nIV-A\n.\n[2]\nK. Black, N. Brown, J. Darpinian, K. Dhabalia, D. Driess, A. Esmail, M. R. Equi, C. Finn, N. Fusai, M. Y. Galliker, D. Ghosh, L. Groom, K. Hausman, B. Ichter, S. Jakubczak, T. Jones, L. Ke, D. LeBlanc, S. Levine, A. Li-Bell, M. Mothukuri, S. Nair, K. Pertsch, A. Z. Ren, L. X. Shi, L. Smith, J. T. Springenberg, K. Stachowicz, J. Tanner, Q. Vuong, H. Walke, A. Walling, H. Wang, L. Yu, and U. Zhilinsky\n(2025-09)\n$\\pi_{0.5}$: a vision-language-action model with open-world generalization\n.\nIn\n9th Annual Conference on Robot Learning\n,\n(\nen\n).\nExternal Links:\nLink\nCited by:\nÂ§I\n,\nÂ§II\n.\n[3]\nK. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter, S. Jakubczak, T. Jones, L. Ke, S. Levine, A. Li-Bell, M. Mothukuri, S. Nair, K. Pertsch, L. X. Shi, J. Tanner, Q. Vuong, A. Walling, H. Wang, and U. Zhilinsky\n(2024-11)\n$pi_0$: A Vision-Language-Action Flow Model for General Robot Control\n.\narXiv\n.\nNote:\narXiv:2410.24164 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§\nIV-D\n,\nÂ§\nIV-D\n.\n[4]\nK. Black, M. Y. Galliker, and S. Levine\n(2025-06)\nReal-Time Execution of Action Chunking Flow Policies\n.\narXiv\n.\nNote:\narXiv:2506.07339 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§\nIV-D\n.\n[5]\nC. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. C. Burchfiel, and S. Song\n(2023-07)\nDiffusion Policy: Visuomotor Policy Learning via Action Diffusion\n.\nIn\nRobotics: Science and Systems XIX\n,\nExternal Links:\nISBN 978-0-9923747-9-2\n,\nLink\nCited by:\nÂ§\nIV-D\n,\nÂ§\nIV-D\n,\nÂ§\nIV-D\n.\n[6]\nK. Ehsani, T. Gupta, R. Hendrix, J. Salvador, L. Weihs, K. Zeng, K. P. Singh, Y. Kim, W. Han, A. Herrasti, R. Krishna, D. Schwenk, E. VanderBilt, and A. Kembhavi\n(2024)\nSPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World\n.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 16238â€“16250\n(\nen\n).\nExternal Links:\nLink\nCited by:\nÂ§II\n.\n[7]\nA. Elfes\n(1989-06)\nUsing occupancy grids for mobile robot perception and navigation\n.\nComputer\n22\n(\n6\n),\npp.Â 46â€“57\n.\nExternal Links:\nISSN 1558-0814\n,\nLink\n,\nDocument\nCited by:\nÂ§II\n.\n[8]\nH. Etukuru, N. Naka, Z. Hu, S. Lee, J. Mehu, A. Edsinger, C. Paxton, S. Chintala, L. Pinto, and N. M. Mahi Shafiullah\n(2025-05)\nRobot Utility Models: General Policies for Zero-Shot Deployment in New Environments\n.\nIn\nIEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 8275â€“8283\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§I\n,\nÂ§II\n.\n[9]\nD. Fox, W. Burgard, and S. Thrun\n(1997-03)\nThe dynamic window approach to collision avoidance\n.\nIEEE Robotics & Automation Magazine\n4\n(\n1\n),\npp.Â 23â€“33\n.\nExternal Links:\nISSN 1558-223X\n,\nLink\n,\nDocument\nCited by:\nÂ§II\n.\n[10]\nFrank Dellaert and GTSAM Contributors\n(2022-05)\nBorglab/gtsam\n.\nGeorgia Tech Borg Lab\n.\nNote:\nhttps://github.com/borglab/gtsam\nCited by:\nÂ§III\n.\n[11]\nJ. Frey, M. Mattamala, N. Chebrolu, C. Cadena, M. Fallon, and M. Hutter\n(2023-07)\nFast Traversability Estimation for Wild Visual Navigation\n.\nIn\nRobotics: Science and Systems XIX\n,\nExternal Links:\nISBN 978-0-9923747-9-2\n,\nLink\nCited by:\nÂ§II\n.\n[12]\nFrodoBots AI\n.\nNote:\nhttps://www.frodobots.ai/\nCited by:\nÂ§III\n.\n[13]\n(2025)\nFrodoBots-2K dataset\n.\nNote:\nhttps://huggingface.co/datasets/frodobots/FrodoBots-2K\nCited by:\nÂ§II\n.\n[14]\nH. He, Y. Ma, W. Wu, and B. Zhou\n(2025-07)\nFrom Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning\n.\narXiv\n.\nNote:\narXiv:2507.22028 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§II\n.\n[15]\nN. Hirose, C. Glossop, D. Shah, and S. Levine\n(2025-09)\nOmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation\n.\narXiv\n.\nNote:\narXiv:2509.19480 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§II\n.\n[16]\nN. Hirose, C. Glossop, A. Sridhar, O. Mees, and S. Levine\n(2024-09)\nLeLaN: Learning A Language-Conditioned Navigation Policy from In-the-Wild Video\n.\nIn\n8th Annual Conference on Robot Learning\n,\n(\nen\n).\nExternal Links:\nLink\nCited by:\nÂ§II\n.\n[17]\nN. Hirose, L. Ignatova, K. Stachowicz, C. Glossop, S. Levine, and D. Shah\n(2025-05)\nLearning to Drive Anywhere with Model-Based Reannotation\n.\nIn\nICRA Workshops\n,\n(\nen\n).\nExternal Links:\nLink\nCited by:\nÂ§II\n,\nÂ§II\n,\nÂ§\nIV-D\n,\nÂ§\nIV-D\n.\n[18]\nG. Kahn, P. Abbeel, and S. Levine\n(2021-04)\nBADGR: An Autonomous Self-Supervised Learning-Based Navigation System\n.\nIEEE Robotics and Automation Letters\n6\n(\n2\n),\npp.Â 1312â€“1319\n.\nExternal Links:\nISSN 2377-3766\n,\nLink\n,\nDocument\nCited by:\nÂ§II\n.\n[19]\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei\n(2020-01)\nScaling Laws for Neural Language Models\n.\narXiv\n.\nNote:\narXiv:2001.08361 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§\nIV-C\n.\n[20]\nM. J. Kim, C. Finn, and P. Liang\n(2025-04)\nFine-Tuning Vision-Language-Action Models: Optimizing Speed and Success\n.\narXiv\n.\nNote:\narXiv:2502.19645 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§\nIV-D\n.\n[21]\nF. Lin, Y. Hu, P. Sheng, C. Wen, J. You, and Y. Gao\n(2025-05)\nData Scaling Laws in Imitation Learning for Robotic Manipulation\n.\nInternational Conference on Representation Learning\n,\npp.Â 54877â€“54910\n(\nen\n).\nExternal Links:\nLink\nCited by:\nÂ§I\n,\nÂ§II\n,\nÂ§\nIV-C\n.\n[22]\nK. Lin, P. Chen, D. Huang, T. H. Li, M. Tan, and C. Gan\n(2023)\nLearning Vision-and-Language Navigation from YouTube Videos\n.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 8317â€“8326\n(\nen\n).\nExternal Links:\nLink\nCited by:\nÂ§II\n.\n[23]\nX. Liu, J. Li, Y. Jiang, N. Sujay, Z. Yang, J. Zhang, J. Abanes, J. Zhang, and C. Feng\n(2025)\nCityWalker: Learning Embodied Urban Navigation from Web-Scale Videos\n.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 6875â€“6885\n(\nen\n).\nExternal Links:\nLink\nCited by:\nÂ§II\n,\nÂ§II\n,\nÂ§II\n.\n[24]\nD. V. Lu, D. Hershberger, and W. D. Smart\n(2014-09)\nLayered costmaps for context-sensitive navigation\n.\nIn\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp.Â 709â€“715\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§II\n.\n[25]\nA. Majumdar, K. Yadav, S. Arnaud, Y. J. Ma, C. Chen, S. Silwal, A. Jain, V. Berges, T. Wu, J. Vakil, P. Abbeel, J. Malik, D. Batra, Y. Lin, O. Maksymets, A. Rajeswaran, and F. Meier\n(2024-05)\nWhere are we in the search for an artificial visual cortex for embodied intelligence?\n.\nIn\nAdvances in Neural Information Processing Systems\n,\nVol.\n37\n,\npp.Â 655â€“677\n.\nCited by:\nÂ§III\n.\n[26]\nM. MÃ¼ller, S. Brahmbhatt, A. Deka, Q. Leboutet, D. Hafner, and V. Koltun\n(2024-05)\nOpenBot-Fleet: A System for Collective Learning with Real Robots\n.\nIn\nIEEE International Conference on Robotics and Automation\n,\npp.Â 4758â€“4765\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§II\n.\n[27]\nR. G. Newcombe\n(1998-04)\nTwo-sided confidence intervals for the single proportion: comparison of seven methods\n.\nStatistics in Medicine\n17\n(\n8\n),\npp.Â 857â€“872\n(\neng\n).\nExternal Links:\nISSN 0277-6715\n,\nDocument\nCited by:\nÂ§\nIV-A\n.\n[28]\nJ. S. Rosenfeld\n(2021-08)\nScaling Laws for Deep Learning\n.\narXiv\n(\nen\n).\nNote:\narXiv:2108.07686 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§I\n.\n[29]\nS. Sartor and N. Thompson\n(2025-01)\nNeural Scaling Laws in Robotics\n.\narXiv\n.\nNote:\narXiv:2405.14005 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§II\n.\n[30]\nD. Shah and S. Levine\n(2022-06)\nViKiNG: Vision-Based Kilometer-Scale Navigation with Geographic Hints\n.\nIn\nRobotics: Science and Systems XVIII\n,\nExternal Links:\nISBN 978-0-9923747-8-5\n,\nLink\nCited by:\nÂ§II\n.\n[31]\nD. Shah, A. Sridhar, A. Bhorkar, N. Hirose, and S. Levine\n(2023-05)\nGNM: A General Navigation Model to Drive Any Robot\n.\nIn\nIEEE International Conference on Robotics and Automation\n,\npp.Â 7226â€“7233\n.\nExternal Links:\nDocument\nCited by:\nÂ§II\n.\n[32]\nD. Shah, A. Sridhar, N. Dashora, K. Stachowicz, K. Black, N. Hirose, and S. Levine\n(2023-08)\nViNT: A Large-Scale, Multi-Task Visual Navigation Backbone with Cross-Robot Generalization\n.\nIn\n7th Annual Conference on Robot Learning\n,\n(\nen\n).\nExternal Links:\nLink\nCited by:\nÂ§II\n,\nÂ§\nIV-D\n,\nÂ§\nIV-D\n.\n[33]\nJ. Shang, K. Schmeckpeper, B. B. May, M. V. Minniti, T. Kelestemur, D. Watkins, and L. Herlant\n(2024-09)\nTheia: Distilling Diverse Vision Foundation Models for Robot Learning\n.\nIn\n8th Annual Conference on Robot Learning\n,\n(\nen\n).\nExternal Links:\nLink\nCited by:\nÂ§III\n,\nÂ§\nIV-D\n,\nÂ§\nIV-D\n.\n[34]\nW. Shen, P. Gu, H. Qin, and Z. Meng\n(2025-07)\nEffoNAV: An Effective Foundation-Model-Based Visual Navigation Approach in Challenging Environment\n.\nIEEE Robotics and Automation Letters\n10\n(\n7\n),\npp.Â 6824â€“6831\n.\nExternal Links:\nISSN 2377-3766\n,\nLink\n,\nDocument\nCited by:\nÂ§II\n.\n[35]\nO. SimÃ©oni, H. V. Vo, M. Seitzer, F. Baldassarre, M. Oquab, C. Jose, V. Khalidov, M. Szafraniec, S. Yi, M. Ramamonjisoa, F. Massa, D. Haziza, L. Wehrstedt, J. Wang, T. Darcet, T. Moutakanni, L. Sentana, C. Roberts, A. Vedaldi, J. Tolan, J. Brandt, C. Couprie, J. Mairal, H. JÃ©gou, P. Labatut, and P. Bojanowski\n(2025-08)\nDINOv3\n.\narXiv\n.\nNote:\narXiv:2508.10104 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§\nIV-D\n.\n[36]\nM. Sivaprakasam, S. Triest, C. Ho, S. Aich, J. Lew, I. Adu, W. Wang, and S. Scherer\n(2025-05)\nSALON: Self-supervised Adaptive Learning for Off-road Navigation\n.\nIn\nIEEE International Conference on Robotics and Automation\n,\npp.Â 16999â€“17006\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§II\n.\n[37]\nJ. Song, A. Vahdat, M. Mardani, and J. Kautz\n(2023)\nPseudoinverse-Guided Diffusion Models for Inverse Problems\n.\nIn\nInternational Conference on Learning Representations\n,\n(\nen\n).\nExternal Links:\nLink\nCited by:\nÂ§\nIV-D\n.\n[38]\nA. Sridhar, D. Shah, C. Glossop, and S. Levine\n(2024-05)\nNoMaD: Goal Masked Diffusion Policies for Navigation and Exploration\n.\nIn\nIEEE International Conference on Robotics and Automation\n,\npp.Â 63â€“70\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§\nIV-D\n,\nÂ§\nIV-D\n.\n[39]\nL. Suomela, S. K. Arachchige, G. F. Torres, H. Edelman, and J. KÃ¤mÃ¤rÃ¤inen\n(2025-09)\nSynthetic vs. Real Training Data for Visual Navigation\n.\nNote:\narXiv:2509.11791 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§II\n.\n[40]\nL. Suomela, J. Kalliola, H. Edelman, and J. KÃ¤mÃ¤rÃ¤inen\n(2024-05)\nPlaceNav: Topological Navigation through Place Recognition\n.\nIn\nIEEE International Conference on Robotics and Automation\n,\npp.Â 5205â€“5213\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§II\n.\n[41]\nM. T. Villasevil, A. Jain, J. Yuan, V. Macha, L. L. Ankile, A. Simeonov, P. Agrawal, and A. Gupta\n(2025-06)\nRobot Learning with Super-Linear Scaling\n.\nIn\nRobotics: Science and Systems XXI\n,\nExternal Links:\nISBN 979-8-9902848-1-4\n,\nLink\nCited by:\nÂ§II\n.\n[42]\nE. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra\n(2020)\nDD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames\n.\nIn\nInternational Conference on Learning Representations\n,\nExternal Links:\nLink\nCited by:\nÂ§II\n,\nÂ§II\n.\n[43]\nZ. Xie, Z. Liu, Z. Peng, W. Wu, and B. Zhou\n(2025)\nVid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation\n.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 1581â€“1591\n(\nen\n).\nExternal Links:\nLink\nCited by:\nÂ§II\n.\n[44]\nH. Xu, Y. Gao, F. Yu, and T. Darrell\n(2017-07)\nEnd-to-End Learning of Driving Models from Large-Scale Video Datasets\n.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 3530â€“3538\n(\nEnglish\n).\nExternal Links:\nISBN 978-1-5386-0457-1\n,\nLink\n,\nDocument\nCited by:\nÂ§II\n.\n[45]\nYaak & LeRobot team\n(2025)\nLeRobot goes to driving school: Worldâ€™s largest open-source self-driving dataset\n.\nNote:\nhttps://www.huggingface.com/blog/lerobot-goes-to-driving-school\nExternal Links:\nLink\nCited by:\nÂ§II\n.\n[46]\nA. Zhang, H. Sikchi, J. Biswas, and A. Zhang\n(2025-06)\nCREStE: Scalable Mapless Navigation with Internet Scale Priors and Counterfactual Guidance\n.\nIn\nRobotics: Science and Systems XXI\n,\nExternal Links:\nISBN 979-8-9902848-1-4\n,\nLink\nCited by:\nÂ§II\n.\n[47]\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn\n(2023-07)\nLearning Fine-Grained Bimanual Manipulation with Low-Cost Hardware\n.\nIn\nRobotics: Science and Systems XIX\n,\nExternal Links:\nISBN 978-0-9923747-9-2\n,\nLink\nCited by:\nÂ§\nIV-D\n.\n[48]\nY. Zheng, Z. Xia, Q. Zhang, T. Zhang, B. Lu, X. Huo, C. Han, Y. Li, M. Yu, B. Jin, P. Yang, Y. Zheng, H. Yuan, K. Jiang, P. Jia, X. Lang, and D. Zhao\n(2024-12)\nPreliminary Investigation into Data Scaling Laws for Imitation Learning-Based End-to-End Autonomous Driving\n.\narXiv\n(\nen\n).\nNote:\narXiv:2412.02689 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§II\n.",
    "preview_text": "Generalization of imitation-learned navigation policies to environments unseen in training remains a major challenge. We address this by conducting the first large-scale study of how data quantity and data diversity affect real-world generalization in end-to-end, map-free visual navigation. Using a curated 4,565-hour crowd-sourced dataset collected across 161 locations in 35 countries, we train policies for point goal navigation and evaluate their closed-loop control performance on sidewalk robots operating in four countries, covering 125 km of autonomous driving.\n  Our results show that large-scale training data enables zero-shot navigation in unknown environments, approaching the performance of policies trained with environment-specific demonstrations. Critically, we find that data diversity is far more important than data quantity. Doubling the number of geographical locations in a training set decreases navigation errors by ~15%, while performance benefit from adding data from existing locations saturates with very little data. We also observe that, with noisy crowd-sourced data, simple regression-based models outperform generative and sequence-based architectures. We release our policies, evaluation setup and example videos on the project page.\n\nData scaling for navigation in unknown environments\nLauri Suomela\n1\n,\nNaoki Takahata\n2\n,\nSasanka Kuruppu Arachchige\n1\n,\nHarry Edelman\n3\n,\nJoni-Kristian KÃ¤mÃ¤rÃ¤inen\n1\n1\nTampere University,\n2\nTohoku University,\n3\nTurku University of Applied Sciences.\nâ€ \nlasuomela.github.io/navigation_scaling\nAbstract\nGeneralization of imitation-learned navigation policies to environments unseen in training remains a major challenge. We address this by conducting the first large-scale study of how data quantity and data diversity affect real-world generalization in end-to-end, map-free visual navigation. Using a curated 4,565-hour crowd-sourced dataset collected across 161 locations in 35 countries, we train policies for point goal navigatio",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "navigation",
        "imitation learning",
        "data scaling",
        "generalization",
        "real-world evaluation"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡é€šè¿‡å¤§è§„æ¨¡æ•°æ®ç ”ç©¶ï¼Œæ¢è®¨æ•°æ®é‡å’Œå¤šæ ·æ€§å¯¹ç«¯åˆ°ç«¯è§†è§‰å¯¼èˆªç­–ç•¥åœ¨æœªçŸ¥ç¯å¢ƒä¸­æ³›åŒ–èƒ½åŠ›çš„å½±å“ï¼Œå‘ç°æ•°æ®å¤šæ ·æ€§æ¯”æ•°é‡æ›´é‡è¦ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-14T12:44:39Z",
    "created_at": "2026-01-20T17:49:49.991826",
    "updated_at": "2026-01-20T17:49:49.991834"
}