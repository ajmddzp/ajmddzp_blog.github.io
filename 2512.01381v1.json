{
  "id": "2512.01381v1",
  "title": "Accelerating Probabilistic Response-Time Analysis: Revised Critical Instant and Optimized Convolution",
  "authors": [
    "Hiroto Takahashi",
    "Atsushi Yano",
    "Takuya Azumi"
  ],
  "abstract": "Accurate estimation of the Worst-Case Deadline Failure Probability (WCDFP) has attracted growing attention as a means to provide safety assurances in complex systems such as robotic platforms and autonomous vehicles. WCDFP quantifies the likelihood of deadline misses under the most pessimistic operating conditions, and safe estimation is essential for dependable real-time applications. However, achieving high accuracy in WCDFP estimation often incurs significant computational cost. Recent studies have revealed that the classical assumption of the critical instant, the activation pattern traditionally considered to trigger the worst-case behavior, can lead to underestimation of WCDFP in probabilistic settings. This observation motivates the use of a revised critical instant formulation that more faithfully captures the true worst-case scenario. This paper investigates convolution-based methods for WCDFP estimation under this revised setting and proposes an optimization technique that accelerates convolution by improving the merge order. Extensive experiments with diverse execution-time distributions demonstrate that the proposed optimized Aggregate Convolution reduces computation time by up to an order of magnitude compared to Sequential Convolution, while retaining accurate and safe-sided WCDFP estimates. These results highlight the potential of the approach to provide both efficiency and reliability in probabilistic timing analysis for safety-critical real-time applications.",
  "url": "https://arxiv.org/abs/2512.01381v1",
  "html_url": "https://arxiv.org/html/2512.01381v1",
  "html_content": "\\affiliate\nSaitamaGraduate School of Science and Engineering, Saitama University, Japan\n\\affiliate\nAcademicAcademic Association (Graduate School of Science and Engineering), Saitama University\n\\affiliate\nTIERIVTIER IV Inc., Japan\nSaitama[takahashi.h.477@ms.saitama-u.ac.jp]\nSaitama,TIERIV[]\nAcademic,TIERIV[]\nAccelerating Probabilistic Response-Time Analysis: Revised Critical Instant and Optimized Convolution\nHiroto Takahashi\nAtsushi Yano\nTakuya Azumi\nAbstract\nAccurate estimation of the Worst-Case Deadline Failure Probability (WCDFP) has attracted growing attention as a means to provide safety assurances in complex systems such as robotic platforms and autonomous vehicles. WCDFP quantifies the likelihood of deadline misses under the most pessimistic operating conditions, and safe estimation is essential for dependable real-time applications. However, achieving high accuracy in WCDFP estimation often incurs significant computational cost. Recent studies have revealed that the classical assumption of the critical instant, the activation pattern traditionally considered to trigger the worst-case behavior, can lead to underestimation of WCDFP in probabilistic settings. This observation motivates the use of a revised critical instant formulation that more faithfully captures the true worst-case scenario. This paper investigates convolution-based methods for WCDFP estimation under this revised setting and proposes an optimization technique that accelerates convolution by improving the merge order. Extensive experiments with diverse execution-time distributions demonstrate that the proposed optimized Aggregate Convolution reduces computation time by up to an order of magnitude compared to Sequential Convolution, while retaining accurate and safe-sided WCDFP estimates. These results highlight the potential of the approach to provide both efficiency and reliability in probabilistic timing analysis for safety-critical real-time applications.\nkeywords:\nReal-Time Systems, Autonomous Driving Systems, Automobiles, WCDFP Estimation, Critical Instant, Convolution Methods\n1\nIntroduction\nDependability in real-time systems is critical for safety-oriented applications such as autonomous vehicles and robotics\n[\nKato2018Autoware\n,\nMalik2021Industrial\n]\n. In such systems, response-time analysis is fundamental; missing a deadline in hard real-time systems can cause catastrophic failures, whereas soft real-time systems are evaluated through Quality of Service (QoS) metrics\n[\nbuttazzo2010Soft\n]\n.\nTraditional analysis relies on the\nworst-case execution time\n(WCET), which represents the maximum task execution time\n[\nHansen2009Statistical-Based\n,\nPuschner2000Guest\n,\nSantinelli2014On\n]\n. However, WCET-based analysis is often pessimistic for dynamic workloads, such as those in autonomous driving, leading to resource over-provisioning\n[\nAbella2015WCET\n,\nWilhelm2008The\n]\n. To address this limitation, probabilistic methods modeling execution times as random variables (\nprobabilistic worst-case execution time\n, or pWCET) have gained attention. These methods estimate the\nworst-case deadline failure probability\n(WCDFP), allowing for the configuration of acceptable deadline miss rates and providing more realistic evaluations, with seminal contributions and rigorous definitions presented in recent literature\n[\nroux2021montecarlo\n,\nVonDerBruggen2021Efficiently\n,\nmarkovic2022analytical\n,\nmarkovic2021convolution\n,\nBozhko2023What\n]\n.\nA central challenge in probabilistic response-time analysis is managing the trade-off between estimation accuracy and computational effort. For safety-critical domains, underestimation of the deadline failure probability must be avoided to ensure trustworthy guarantees. Excessive overestimation, however, reduces the practical value of such guarantees. Thus, approximating the true WCDFP with a tight upper bound, computable in a reasonable time, is highly desirable. The classical critical instant, which posits simultaneous task releases to maximize system load, can underestimate the WCDFP in probabilistic analysis, as recent studies show scenarios with higher workloads\n[\nliu1989Scheduling\n,\nchen2022critical\n]\n. Consequently, the redefinition of the critical instant requires a reassessment of analysis methods that relied on the classical assumption.\nThis paper focuses on circular convolution-based methods for WCDFP estimation, adapting these approaches to the revised critical instant. The main objective is to improve computational efficiency while addressing the accuracy-computation trade-off. For comparison, Monte Carlo-based methods\n[\nroux2021montecarlo\n]\nand Berry-Esseen inequality-based methods\n[\nmarkovic2022analytical\n]\nare also evaluated under the revised critical instant. These evaluations highlight the trade-offs among WCDFP estimation methods in modern probabilistic real-time systems, particularly in robotics and autonomous driving where reliability is paramount.\nThe main contributions of this paper are:\n‚Ä¢\nIntroduction of an optimization technique for convolution merge order, enabling efficient WCDFP estimation under the revised critical instant.\n‚Ä¢\nA comparison of the optimized convolution method against Sequential Convolution, Monte Carlo, and Berry-Esseen methods, all adapted to the revised critical instant.\n‚Ä¢\nEmpirical insights into trade-offs between accuracy, execution time, and applicability of WCDFP estimation methods in probabilistic real-time systems.\nThe remainder of this paper is structured as follows.\nSection\n2\noutlines the system model.\nSection\n3\nspecifies the definition and upper bound of WCDFP.\nSection\n4\nintroduces the mathematical and algorithmic foundations of efficient convolution.\nSection\n5\npresents the proposed accelerated aggregate convolution algorithm incorporating the revised critical instant.\nSection\n6\ndetails the evaluation results.\nSection\n7\nreviews related work.\nSection\n8\nconcludes the paper and discusses future work.\n2\nSystem Model\nIn this section, the necessary probabilistic concepts and task set assumptions are introduced, serving as the foundation for the subsequent analysis. The variables and symbols introduced in this section are summarized in\n\\tabref\ntable:notation, which provides a comprehensive overview of the notation used throughout the paper.\n2.1\nProbabilistic Notation and Symbols\nTable 1:\nSummary of notation\nSymbol\nExplanation\nœÑ\n\\tau\nA task set.\nœÑ\ni\n\\tau_{i}\nA task from\nœÑ\n\\tau\nwith index\ni\ni\n.\nŒ≥\n\\gamma\nMinimum indivisible unit of time (e.g., processor cycle).\nùïã\n\\mathbb{T}\nTime domain,\nùïã\n=\n{\nŒ≥\n‚ãÖ\nk\n‚à£\nk\n‚àà\n‚Ñï\n}\n\\mathbb{T}=\\{\\gamma\\cdot k\\mid k\\in\\mathbb{N}\\}\n.\nT\ni\nT_{i}\nThe minimum inter-arrival time of\nœÑ\ni\n\\tau_{i}\n.\nD\ni\nD_{i}\nRelative deadline of\nœÑ\ni\n\\tau_{i}\n.\nœÄ\ni\n\\pi_{i}\nPriority of task\nœÑ\ni\n\\tau_{i}\n.\nJ\ni\n,\nj\nJ_{i,j}\nThe\nj\nj\n-th job of\nœÑ\ni\n\\tau_{i}\n.\nC\ni\nC_{i}\nProbabilistic execution time distribution of\nœÑ\ni\n\\tau_{i}\n.\nC\ni\n,\nj\nC_{i,j}\nExecution time (a specific value) of the\nj\nj\n-th job\nJ\ni\n,\nj\nJ_{i,j}\nof task\nœÑ\ni\n\\tau_{i}\n.\na\ni\n,\nj\na_{i,j}\nArrival time of\nJ\ni\n,\nj\nJ_{i,j}\n.\nd\ni\n,\nj\nd_{i,j}\nAbsolute deadline of\nJ\ni\n,\nj\nJ_{i,j}\n.\nŒ©\n\\Omega\nSample space of system evolutions.\nœâ\n‚àà\nŒ©\n\\omega\\in\\Omega\nA sample (particular system evolution) from\nŒ©\n\\Omega\n.\nŒæ\n‚äÜ\nŒ©\n\\xi\\subseteq\\Omega\nEvent encompassing all evolutions exhibiting an identical arrival sequence.\nThe probability space\n(\nŒ©\n,\n‚Ñ±\n,\n‚Ñô\n)\n(\\Omega,\\mathcal{F},\\mathbb{P})\nis defined as follows:\nŒ©\n\\Omega\ndenotes the set of all possible outcomes (the sample space),\n‚Ñ±\n‚äÜ\n2\nŒ©\n\\mathcal{F}\\subseteq 2^{\\Omega}\nrepresents the event space, consisting of subsets of\nŒ©\n\\Omega\n, and\n‚Ñô\n:\n‚Ñ±\n‚Üí\n[\n0\n,\n1\n]\n\\mathbb{P}:\\mathcal{F}\\to[0,1]\nis the probability measure. The set of real numbers is denoted by\n‚Ñù\n\\mathbb{R}\n.\nDefinition 1\n(Random Variable)\n.\nA random variable\nX\nX\non a probability space\n(\nŒ©\n,\n‚Ñ±\n,\n‚Ñô\n)\n(\\Omega,\\mathcal{F},\\mathbb{P})\nis defined as a function\nX\n:\nŒ©\n‚Üí\n‚Ñù\nX:\\Omega\\to\\mathbb{R}\nsuch that for any real number\nx\n‚àà\n‚Ñù\nx\\in\\mathbb{R}\n, the set\n{\nœâ\n‚àà\nŒ©\n‚à£\nX\n‚Äã\n(\nœâ\n)\n=\nx\n}\n‚àà\n‚Ñ±\n\\{\\omega\\in\\Omega\\mid X(\\omega)=x\\}\\in\\mathcal{F}\nis measurable.\nThe probability of a random variable\nX\nX\ntaking a value\nx\nx\nis expressed as\n‚Ñô\n‚Äã\n[\nœâ\n‚àà\nŒ©\n‚à£\nX\n‚Äã\n(\nœâ\n)\n=\nx\n]\n\\mathbb{P}[\\omega\\in\\Omega\\mid X(\\omega)=x]\n, or in short form,\n‚Ñô\n‚Äã\n[\nX\n=\nx\n]\n\\mathbb{P}[X=x]\n. From this point onward, similar notation is consistently used to represent probabilities and distributions of random variables.\nA discrete random variable\nX\nX\nand the corresponding probability mass function (PMF) can be explicitly represented by listing the possible values\nx\nk\nx_{k}\nand their associated probabilities\n‚Ñô\n‚Äã\n[\nX\n=\nx\nk\n]\n\\mathbb{P}[X=x_{k}]\n. Throughout this paper, a two-row matrix notation is adopted for clarity, similar to representations in related literature\n[\nmarkovic2021convolution\n]\n:\nX\n‚àº\n[\nx\n1\nx\n2\n‚Ä¶\nx\nn\n‚Ñô\n‚Äã\n[\nX\n=\nx\n1\n]\n‚Ñô\n‚Äã\n[\nX\n=\nx\n2\n]\n‚Ä¶\n‚Ñô\n‚Äã\n[\nX\n=\nx\nn\n]\n]\n,\nX\\sim\\begin{bmatrix}x_{1}&x_{2}&\\dots&x_{n}\\\\\n\\mathbb{P}[X=x_{1}]&\\mathbb{P}[X=x_{2}]&\\dots&\\mathbb{P}[X=x_{n}]\\end{bmatrix},\n(1)\nwhere\nx\n1\n<\nx\n2\n<\n‚ãØ\n<\nx\nn\nx_{1}<x_{2}<\\dots<x_{n}\nare the distinct values\nX\nX\ncan take, sorted in increasing order, and\nn\nn\nis the cardinality of the support of\nX\nX\n(the set of values taken by X with positive probability, denoted Im\nX\nX\n).\nDefinition 2\n(Cumulative Distribution Function (CDF))\n.\nThe cumulative distribution function (CDF) of a random variable\nX\nX\n, denoted by\nF\nX\n:\n‚Ñù\n‚Üí\n[\n0\n,\n1\n]\nF_{X}:\\mathbb{R}\\rightarrow[0,1]\n, is defined as follows:\nF\nX\n‚Äã\n(\nx\n)\n‚âú\n‚Ñô\n‚Äã\n[\nX\n‚â§\nx\n]\n,\n‚àÄ\nx\n‚àà\n‚Ñù\n.\nF_{X}(x)\\triangleq\\mathbb{P}[X\\leq x],\\quad\\forall x\\in\\mathbb{R}.\n(2)\nThe CDF describes the probability that the random variable\nX\nX\ntakes on a value less than or equal to\nx\nx\n. For a discrete random variable, this function represents the sum of probabilities of all values in the support up to\nx\nx\n.\nDefinition 3\n(Partial Order of Random Variables)\n.\nFor two random variables\nX\nX\nand\nY\nY\n,\nX\nX\nis said to be less than or equal to\nY\nY\nin distribution, denoted\nX\n‚™Ø\nY\nX\\preceq Y\n, if\nF\nX\n‚Äã\n(\nx\n)\n‚â•\nF\nY\n‚Äã\n(\nx\n)\n,\n‚àÄ\nx\n‚àà\n‚Ñù\nF_{X}(x)\\geq F_{Y}(x),\\forall x\\in\\mathbb{R}\n.\n2.2\nTask Set Assumptions\nA system is considered in which time is discretized into multiples of a constant minimum time interval\nŒ≥\n>\n0\n\\gamma>0\n, representing the smallest indivisible unit of time, such as a processor cycle. The corresponding time domain is given by\nùïã\n=\n{\nŒ≥\n‚ãÖ\nk\n‚à£\nk\n‚àà\n‚Ñï\n}\n‚äÇ\n‚Ñù\n\\mathbb{T}=\\{\\gamma\\cdot k\\mid k\\in\\mathbb{N}\\}\\subset\\mathbb{R}\n. The system follows a fully preemptive fixed-priority scheduling policy, ensuring that the highest-priority ready job is executed at any given time. Within this framework, the system consists of\nn\nn\nsporadic real-time tasks\nœÑ\n‚âú\n{\nœÑ\n1\n,\nœÑ\n2\n,\n‚Ä¶\n,\nœÑ\nn\n}\n\\tau\\triangleq\\{\\tau_{1},\\tau_{2},\\dots,\\tau_{n}\\}\n, where each task\nœÑ\ni\n\\tau_{i}\nis assigned a unique priority.\nEach task\nœÑ\ni\n\\tau_{i}\nis defined by a tuple\n(\nC\ni\n,\nT\ni\n,\nD\ni\n,\nœÄ\ni\n)\n\\left(C_{i},T_{i},D_{i},\\pi_{i}\\right)\n. The term\nC\ni\nC_{i}\n, referred to in\n\\tabref\ntable:notation as the execution time distribution of task\nœÑ\ni\n\\tau_{i}\n, represents the random variable for the execution time of task\nœÑ\ni\n\\tau_{i}\nin this paper. This\nC\ni\nC_{i}\nembodies the probabilistic Worst-Case Execution Time (pWCET) characterization. The concept of pWCET provides an upper probabilistic bound on the inherent, underlying probabilistic execution time (pET) of any job generated by task\nœÑ\ni\n\\tau_{i}\n. Let\nùíû\ni\n‚àó\n\\mathcal{C}_{i}^{*}\ndenote a random variable representing this underlying pET for a job of task\nœÑ\ni\n\\tau_{i}\n. The distribution of\nC\ni\nC_{i}\nis defined such that the distribution stochastically upper-bounds\nùíû\ni\n‚àó\n\\mathcal{C}_{i}^{*}\n, meaning\nùíû\ni\n‚àó\n‚™Ø\nC\ni\n\\mathcal{C}_{i}^{*}\\preceq C_{i}\naccording to\nDefinition\n3\n. This signifies that\nC\ni\nC_{i}\nrepresents a more pessimistic or equally pessimistic execution time profile compared to the underlying pET\nùíû\ni\n‚àó\n\\mathcal{C}_{i}^{*}\n. The PMF of\nC\ni\nC_{i}\n, as represented by the notation in\nEquation\n1\n, is assumed to satisfy this pWCET characteristic. A primary motivation for employing such a pWCET abstraction is to facilitate analytical techniques that require random variables to be independent and identically distributed (IID), which is often not true for underlying pETs due to systemic interactions. The pWCET characterization is therefore constructed as a safe, often pessimistic, over-approximation of pETs, specifically to enable this IID assumption for analytical tractability\n[\nBozhko2023What\n]\n. Consequently, for many probabilistic analyses, including those in this paper, the pWCET characterizations,\nC\ni\nC_{i}\n, for different tasks\nœÑ\ni\n\\tau_{i}\nand\nœÑ\nk\n\\tau_{k}\n(where\ni\n‚â†\nk\ni\\neq k\n) are assumed to be independent random variables. Furthermore, all jobs\nJ\ni\n,\nj\nJ_{i,j}\ngenerated from the same task\nœÑ\ni\n\\tau_{i}\nare assumed to have execution times that are IID according to the pWCET distribution\nC\ni\nC_{i}\n.\nFollowing the characterization of task execution times, other task parameters are defined as follows.\nT\ni\nT_{i}\nis the minimum inter-arrival time, and constrained deadlines are assumed, meaning\nD\ni\n‚â§\nT\ni\nD_{i}\\leq T_{i}\nfor all tasks\nœÑ\ni\n‚àà\nœÑ\n\\tau_{i}\\in\\tau\n.\nD\ni\nD_{i}\nis the relative deadline, and\nœÄ\ni\n\\pi_{i}\nrepresents the priority of the task\nœÑ\ni\n\\tau_{i}\n, with\nœÄ\ni\n>\nœÄ\nj\n\\pi_{i}>\\pi_{j}\nindicating that\nœÑ\ni\n\\tau_{i}\nhas a higher priority than\nœÑ\nj\n\\tau_{j}\n. When a task\nœÑ\ni\n\\tau_{i}\nis activated, a job\nJ\ni\n,\nj\nJ_{i,j}\nis generated, where\nj\nj\nis the job index. Each job\nJ\ni\n,\nj\nJ_{i,j}\nhas a release time\na\ni\n,\nj\na_{i,j}\nand must complete execution before the absolute deadline\nd\ni\n,\nj\n=\na\ni\n,\nj\n+\nD\ni\nd_{i,j}=a_{i,j}+D_{i}\n. If a job misses the deadline, the job is immediately discarded and does not continue executing. The execution time of an individual job\nJ\ni\n,\nj\nJ_{i,j}\nof task\nœÑ\ni\n\\tau_{i}\n, denoted as\nC\ni\n,\nj\nC_{i,j}\n, is a specific value sampled from the pWCET distribution\nC\ni\nC_{i}\n. For simplicity, the scenario\nœâ\n\\omega\nis omitted in this notation.\nFigure 1:\nVisualization of the job arrival sequence, showing jobs released at various time instants for different tasks, along with their respective priorities\nThe arrival sequence\nŒæ\n\\xi\ndefines the overall pattern of job arrivals for all tasks over time. Specifically, within a given arrival scenario\nŒæ\n\\xi\n, the release times, jobs, and absolute deadlines are\na\ni\n,\nj\nŒæ\na_{i,j}^{\\xi}\n,\nJ\ni\n,\nj\nŒæ\nJ_{i,j}^{\\xi}\n, and\nd\ni\n,\nj\nŒæ\nd_{i,j}^{\\xi}\n, respectively, indicating that these values are defined within the context of this particular scenario. As shown in\nFig.\n1\n, this visualization provides an example of job release patterns over time. This foundational framework helps analyze system behavior.\n3\nWCDFP and a Revised Upper Bound\nThe primary focus of this section is to establish a rigorous foundation for the understanding and estimation of WCDFP. The necessary foundational concepts and notation are defined for the accurate formulation of WCDFP. Following this, the computational challenge of directly calculating WCDFP is addressed, and an upper bound is introduced as a practical alternative.\n3.1\nDefinitions and Notations\nTo define WCDFP rigorously, this subsection introduces foundational concepts and notation. These include workload, carry-in, aborted execution time, and processor demand, essential for accurately modeling response times and deadline failure probabilities. The definitions presented in this subsection follow those established in prior work\n[\nmarkovic2023cta\n]\n, ensuring consistency with research.\nDefinition 4\n(Workload)\n.\nThe workload of a task\nœÑ\ni\n\\tau_{i}\nover a time interval\n[\nt\n1\n,\nt\n2\n)\n[t_{1},t_{2})\nin arrival sequence\nŒæ\n\\xi\n, denoted by\nùí≤\ni\nŒæ\n‚Äã\n[\nt\n1\n,\nt\n2\n)\n\\mathcal{W}_{i}^{\\xi}[t_{1},t_{2})\n, is defined as the total processing time required by all jobs of\nœÑ\ni\n\\tau_{i}\nreleased in that interval:\nùí≤\ni\nŒæ\n‚Äã\n[\nt\n1\n,\nt\n2\n)\n‚âú\n‚àë\nj\n‚àà\n{\nj\n‚Ä≤\n‚à£\na\ni\n,\nj\n‚Ä≤\nŒæ\n‚àà\n[\nt\n1\n,\nt\n2\n)\n}\nC\ni\n,\nj\n.\n\\mathcal{W}_{i}^{\\xi}[t_{1},t_{2})\\triangleq\\sum_{j\\in\\{j^{\\prime}\\mid a_{i,j^{\\prime}}^{\\xi}\\in[t_{1},t_{2})\\}}C_{i,j}.\n(3)\nDefinition 5\n(Total Workload)\n.\nThe total workload of task\nœÑ\ni\n\\tau_{i}\nand all higher-priority tasks, including\nœÑ\ni\n\\tau_{i}\n, over a time interval\n[\nt\n1\n,\nt\n2\n)\n[t_{1},t_{2})\nin arrival sequence\nŒæ\n\\xi\n, denoted by\nùíØ\n‚Äã\nùí≤\ni\nŒæ\n‚Äã\n[\nt\n1\n,\nt\n2\n)\n\\mathcal{TW}_{i}^{\\xi}[t_{1},t_{2})\n, is defined as:\nùíØ\n‚Äã\nùí≤\ni\nŒæ\n‚Äã\n[\nt\n1\n,\nt\n2\n)\n‚âú\n‚àë\nœÄ\ni\n‚â§\nœÄ\nk\nùí≤\nk\nŒæ\n‚Äã\n[\nt\n1\n,\nt\n2\n)\n.\n\\mathcal{TW}_{i}^{\\xi}[t_{1},t_{2})\\triangleq\\sum_{\\pi_{i}\\leq\\pi_{k}}\\mathcal{W}_{k}^{\\xi}[t_{1},t_{2}).\n(4)\nDefinition 6\n(Service Time)\n.\nLet\nœÉ\n‚Äã\n(\nt\n)\n\\sigma(t)\ndenote the scheduling function, which returns the job being executed by the processor at each discrete time\nt\nt\n. The service time of a job\nJ\ni\n,\nj\nŒæ\nJ_{i,j}^{\\xi}\nof task\nœÑ\ni\n\\tau_{i}\nup to time\nt\nt\nin arrival sequence\nŒæ\n\\xi\n, denoted by\nùíÆ\ni\n,\nj\nŒæ\n‚Äã\n(\nt\n)\n\\mathcal{S}_{i,j}^{\\xi}(t)\n, is defined as the total number of discrete time units in the interval\n[\na\ni\n,\nj\nŒæ\n,\nt\n)\n[a_{i,j}^{\\xi},t)\nduring which\nJ\ni\n,\nj\nŒæ\nJ_{i,j}^{\\xi}\nis included in\nœÉ\n‚Äã\n(\nt\n‚Ä≤\n)\n\\sigma(t^{\\prime})\n:\nùíÆ\ni\n,\nj\nŒæ\n‚Äã\n(\nt\n)\n‚âú\n|\n{\nt\n‚Ä≤\n‚àà\n[\na\ni\n,\nj\nŒæ\n,\nt\n)\n‚à©\nùïã\n‚à£\nœÉ\n‚Äã\n(\nt\n‚Ä≤\n)\n=\nJ\ni\n,\nj\nŒæ\n}\n|\n.\n\\mathcal{S}_{i,j}^{\\xi}(t)\\triangleq\\left|\\left\\{t^{\\prime}\\in[a_{i,j}^{\\xi},t)\\cap\\mathbb{T}\\mid\\sigma(t^{\\prime})=J_{i,j}^{\\xi}\\right\\}\\right|.\n(5)\nDefinition 7\n(Carry-in)\n.\nThe carry-in for task\nœÑ\ni\n\\tau_{i}\nat time\nt\nt\nin arrival sequence\nŒæ\n\\xi\n, denoted by\nùíû\n‚Äã\n‚Ñê\ni\nŒæ\n‚Äã\n(\nt\n)\n\\mathcal{CI}_{i}^{\\xi}(t)\n, is defined as:\nùíû\n‚Äã\n‚Ñê\ni\nŒæ\n‚Äã\n(\nt\n)\n‚âú\n{\nC\ni\n,\nj\n‚àí\nùíÆ\ni\n,\nj\nŒæ\n‚Äã\n(\nt\n)\nif\n‚Äã\n‚àÉ\nj\n‚àà\n‚Ñï\n:\na\ni\n,\nj\nŒæ\n‚â§\nt\n<\nd\ni\n,\nj\nŒæ\n,\n0\notherwise\n.\n\\mathcal{CI}_{i}^{\\xi}(t)\\triangleq\\begin{cases}C_{i,j}-\\mathcal{S}_{i,j}^{\\xi}(t)&\\text{if }\\exists j\\in\\mathbb{N}:a_{i,j}^{\\xi}\\leq t<d_{i,j}^{\\xi},\\\\\n0&\\text{otherwise}.\\end{cases}\n(6)\nDefinition 8\n(Total Carry-in of Task\nœÑ\ni\n\\tau_{i}\n)\n.\nThe total carry-in for task\nœÑ\ni\n\\tau_{i}\nat time\nt\nt\nin arrival sequence\nŒæ\n\\xi\n, denoted by\nùíØ\n‚Äã\nùíû\n‚Äã\n‚Ñê\ni\nŒæ\n‚Äã\n(\nt\n)\n\\mathcal{TCI}_{i}^{\\xi}(t)\n, is defined as the sum of the carry-in contributions from all higher-priority tasks:\nùíØ\n‚Äã\nùíû\n‚Äã\n‚Ñê\ni\nŒæ\n‚Äã\n(\nt\n)\n‚âú\n‚àë\nœÄ\ni\n<\nœÄ\nk\nùíû\n‚Äã\n‚Ñê\nk\nŒæ\n‚Äã\n(\nt\n)\n.\n\\mathcal{TCI}_{i}^{\\xi}(t)\\triangleq\\sum_{\\pi_{i}<\\pi_{k}}\\mathcal{CI}_{k}^{\\xi}(t).\n(7)\nDefinition 9\n(Aborted Execution Time)\n.\nThe aborted execution time for a task\nœÑ\ni\n\\tau_{i}\nat time\nt\nt\nin arrival sequence\nŒæ\n\\xi\n, denoted by\nùí¶\n‚Äã\nùí≤\ni\nŒæ\n‚Äã\n(\nt\n)\n\\mathcal{KW}_{i}^{\\xi}(t)\n, represents the remaining workload of a job\nJ\ni\n,\nj\nŒæ\nJ_{i,j}^{\\xi}\nof\nœÑ\ni\n\\tau_{i}\nthat has been preempted and does not complete by the absolute deadline of the job. This metric is defined as:\nùí¶\n‚Äã\nùí≤\ni\nŒæ\n‚Äã\n(\nt\n)\n‚âú\n{\nC\ni\n,\nj\n‚àí\nùíÆ\ni\n,\nj\nŒæ\n‚Äã\n(\nt\n)\nif\n‚Äã\n‚àÉ\nj\n‚àà\n‚Ñï\n:\nd\ni\n,\nj\nŒæ\n=\nt\n,\n0\notherwise\n.\n\\mathcal{KW}_{i}^{\\xi}(t)\\triangleq\\begin{cases}C_{i,j}-\\mathcal{S}_{i,j}^{\\xi}(t)&\\text{if }\\exists j\\in\\mathbb{N}:d_{i,j}^{\\xi}=t,\\\\\n0&\\text{otherwise}.\\end{cases}\n(8)\nDefinition 10\n(Total Aborted Execution Time of Task\nœÑ\ni\n\\tau_{i}\n)\n.\nThe total aborted execution time for task\nœÑ\ni\n\\tau_{i}\nand all higher-priority tasks, including\nœÑ\ni\n\\tau_{i}\n, over a time interval\n[\nt\n1\n,\nt\n2\n)\n[t_{1},t_{2})\nin arrival sequence\nŒæ\n\\xi\n, denoted by\nùíØ\n‚Äã\nùí¶\n‚Äã\nùí≤\ni\nŒæ\n‚Äã\n[\nt\n1\n,\nt\n2\n)\n\\mathcal{TKW}_{i}^{\\xi}[t_{1},t_{2})\n, is defined as the sum of aborted execution times for all higher-priority tasks, including\nœÑ\ni\n\\tau_{i}\n:\nùíØ\n‚Äã\nùí¶\n‚Äã\nùí≤\ni\nŒæ\n‚Äã\n[\nt\n1\n,\nt\n2\n)\n‚âú\n‚àë\nt\n‚àà\n[\nt\n1\n,\nt\n2\n)\n‚àë\nœÄ\ni\n‚â§\nœÄ\nk\nùí¶\n‚Äã\nùí≤\nk\nŒæ\n‚Äã\n(\nt\n)\n.\n\\mathcal{TKW}_{i}^{\\xi}[t_{1},t_{2})\\triangleq\\sum_{t\\in[t_{1},t_{2})}\\sum_{\\pi_{i}\\leq\\pi_{k}}\\mathcal{KW}_{k}^{\\xi}(t).\n(9)\nDefinition 11\n(Processor Demand)\n.\nThe processor demand for task\nœÑ\ni\n\\tau_{i}\nover an interval\n[\nt\n1\n,\nt\n2\n)\n[t_{1},t_{2})\nin arrival sequence\nŒæ\n\\xi\n, denoted by\n‚Ñ∞\ni\nŒæ\n‚Äã\n[\nt\n1\n,\nt\n2\n)\n\\mathcal{E}_{i}^{\\xi}[t_{1},t_{2})\n, is defined as:\n‚Ñ∞\ni\nŒæ\n‚Äã\n[\nt\n1\n,\nt\n2\n)\n‚âú\nùíØ\n‚Äã\nùíû\n‚Äã\n‚Ñê\ni\nŒæ\n‚Äã\n(\nt\n1\n)\n+\nùíØ\n‚Äã\nùí≤\ni\nŒæ\n‚Äã\n[\nt\n1\n,\nt\n2\n)\n‚àí\nùíØ\n‚Äã\nùí¶\n‚Äã\nùí≤\ni\nŒæ\n‚Äã\n[\nt\n1\n,\nt\n2\n)\n.\n\\mathcal{E}_{i}^{\\xi}[t_{1},t_{2})\\triangleq\\mathcal{TCI}_{i}^{\\xi}(t_{1})+\\mathcal{TW}_{i}^{\\xi}[t_{1},t_{2})-\\mathcal{TKW}_{i}^{\\xi}[t_{1},t_{2}).\n(10)\nDefinition 12\n(Probabilistic Response Time)\n.\nThe probabilistic response time\n‚Ñõ\n‚Äã\nùíØ\ni\n,\nj\nŒæ\n\\mathcal{RT}_{i,j}^{\\xi}\nof a job\nJ\ni\n,\nj\nŒæ\nJ_{i,j}^{\\xi}\nof task\nœÑ\ni\n\\tau_{i}\nin arrival sequence\nŒæ\n\\xi\nis defined as the earliest time at which the processor demand for\nJ\ni\n,\nj\nŒæ\nJ_{i,j}^{\\xi}\ncan be fully met, measured from the release time of the job. Formally, this quantity is given by:\n‚Ñõ\n‚Äã\nùíØ\ni\n,\nj\nŒæ\n‚âú\ninf\n{\nŒî\n‚à£\nŒî\n>\n0\n‚àß\n‚Ñ∞\ni\nŒæ\n‚Äã\n[\na\ni\n,\nj\nŒæ\n,\na\ni\n,\nj\nŒæ\n+\nŒî\n)\n‚â§\nŒî\n}\n.\n\\mathcal{RT}_{i,j}^{\\xi}\\triangleq\\inf\\{\\Delta\\mid\\Delta>0\\land\\mathcal{E}_{i}^{\\xi}[a_{i,j}^{\\xi},a_{i,j}^{\\xi}+\\Delta)\\leq\\Delta\\}.\n(11)\nDefinition 13\n(Worst Case Deadline Failure Probability (WCDFP))\n.\nThe Worst Case Deadline Failure Probability (WCDFP) for a task\nœÑ\ni\n\\tau_{i}\nis defined as the maximum of the Deadline Failure Probability (DFP) across all possible arrival scenarios\nŒæ\n\\xi\nand all jobs\nJ\ni\n,\nj\nJ_{i,j}\nof the task, under the worst-case execution scenario\nœâ\n\\omega\n. Formally, WCDFP is expressed as:\nWCDFP\ni\n‚âú\nmax\nŒæ\n‚Å°\nmax\nj\n‚Å°\n‚Ñô\n‚Äã\n[\n‚Ñõ\n‚Äã\nùíØ\ni\n,\nj\nŒæ\n>\nD\ni\n]\n.\n\\text{WCDFP}_{i}\\triangleq\\max_{\\xi}\\max_{j}\\mathbb{P}[\\mathcal{RT}_{i,j}^{\\xi}>D_{i}].\n(12)\n3.2\nUpper Bound Considering the Critical Instant\nWCDFP is formally defined as the maximum deadline failure probability across all arrival scenarios. Traditionally, the value was approximated by the critical instant, assuming simultaneous task releases, but probabilistic interpretations reveal this assumption can be insufficient\n[\nchen2022critical\n]\n. To address this limitation, an indicator\nS\nk\n,\nt\nS_{k,t}\nis introduced to capture both maximum interference from higher-priority tasks and the execution time of\nœÑ\nk\n\\tau_{k}\n, enabling feasible WCDFP approximation.\nFigure 2:\nProcessor load indicator\nS\nk\n,\nt\nS_{k,t}\nfor task\nœÑ\nk\n\\tau_{k}\n, including carry-in and preemption effects from higher-priority tasks\nCorollary 1\n(Corollary 12 in Ref.\n[\nchen2022critical\n]\n)\n.\nGiven a fully preemptive fixed-priority scheduler, a set of constrained-deadline sporadic tasks, and under the assumption that incomplete jobs are aborted at their deadline, the following inequality holds:\nWCDFP\nk\n‚â§\ninf\n0\n<\nt\n‚â§\nD\nk\n‚Ñô\n‚Äã\n(\nS\nk\n,\nt\n>\nt\n)\n.\n\\text{WCDFP}_{k}\\leq\\inf_{0<t\\leq D_{k}}\\mathbb{P}(S_{k,t}>t).\n(13)\nHere,\nS\nk\n,\nt\nS_{k,t}\nis defined as:\nS\nk\n,\nt\n‚âú\nC\nk\n,\n1\n+\n‚àë\nœÄ\nk\n<\nœÄ\ni\n‚àë\nj\n=\n1\n‚åà\nt\n+\nD\ni\nT\ni\n‚åâ\nC\ni\n,\nj\n,\nS_{k,t}\\triangleq C_{k,1}+\\sum_{\\pi_{k}<\\pi_{i}}\\sum_{j=1}^{\\left\\lceil\\frac{t+D_{i}}{T_{i}}\\right\\rceil}C_{i,j},\n(14)\nwhere\n‚Ñô\n‚Äã\n(\nS\nk\n,\nt\n>\nt\n)\n\\mathbb{P}(S_{k,t}>t)\nrepresents the probability that the processor demand of task\nœÑ\nk\n\\tau_{k}\nand higher-priority tasks exceeds the time\nt\nt\n.\nThe processor load indicator\nS\nk\n,\nt\nS_{k,t}\nis best understood from\nFig.\n2\n. The indicator represents the upper bound of processor demand when higher-priority jobs are released with maximum density within\n[\n‚àí\nD\ni\n,\nt\n)\n[-D_{i},t)\n. Under an abort policy where jobs missing their deadlines are terminated, and assuming the analyzed job is released at time 0, the carry-in workload for higher-priority jobs is limited to\n[\n‚àí\nD\ni\n,\n0\n)\n[-D_{i},0)\n. Unlike the classical assumption of simultaneous releases, the revised indicator allows higher-priority jobs to be released up to\nD\ni\nD_{i}\nbefore the analyzed job. This adjustment provides a realistic representation of worst-case interference and enables WCDFP approximation by evaluating\n‚Ñô\n‚Äã\n(\nS\nk\n,\nt\n>\nt\n)\n\\mathbb{P}(S_{k,t}>t)\nover\n(\n0\n,\nD\nk\n]\n(0,D_{k}]\n, without exhaustively considering all arrival scenarios.\n4\nFoundations for Efficient Convolution\nThis section overviews convolution-based methods for summing random variables, which form the basis of WCDFP estimation under the revised critical instant. Definitions of independence, convolution, and element-wise product are given, followed by efficient single and repeated convolution techniques.\n4.1\nMathematical Preliminaries for Convolution\nThis subsection introduces essential mathematical definitions that underpin efficient convolution techniques.\nDefinition 14\n(Independence of Random Variables)\n.\nTwo discrete random variables\nX\nX\nand\nY\nY\nare independent if\n‚Ñô\n‚Äã\n[\nX\n=\nx\n,\nY\n=\ny\n]\n=\n‚Ñô\n‚Äã\n[\nX\n=\nx\n]\n‚Äã\n‚Ñô\n‚Äã\n[\nY\n=\ny\n]\n\\mathbb{P}[X=x,Y=y]=\\mathbb{P}[X=x]\\mathbb{P}[Y=y]\nfor all\nx\n‚àà\nIm\n‚Äã\nX\nx\\in\\text{Im}X\nand all\ny\n‚àà\nIm\n‚Äã\nY\ny\\in\\text{Im}Y\n.\nThe independence assumption (\nDefinition\n14\n) is crucial for simplifying the analysis of combined probabilistic behaviors and aligns with pWCET characterization assumptions in\nSection\n2.2\n.\nDefinition 15\n(Convolution (Sum of Independent Random Variables))\n.\nIf\nX\nX\nand\nY\nY\nare independent discrete random variables (\nDefinition\n14\n), their sum\nZ\n=\nX\n+\nY\nZ=X+Y\nis also a discrete random variable. The probability mass function (PMF) of\nZ\nZ\nis obtained by the linear convolution of the PMFs of\nX\nX\nand\nY\nY\n:\n‚Ñô\n‚Äã\n[\nZ\n=\nz\n]\n=\n‚àë\nk\n‚àà\nIm\n‚Äã\nX\n‚Ñô\n‚Äã\n[\nX\n=\nk\n]\n‚Äã\n‚Ñô\n‚Äã\n[\nY\n=\nz\n‚àí\nk\n]\n,\n‚àÄ\nz\n‚àà\nIm\n‚Äã\nZ\n.\n\\mathbb{P}[Z=z]=\\sum_{k\\in\\text{Im}X}\\mathbb{P}[X=k]\\mathbb{P}[Y=z-k],\\quad\\forall z\\in\\text{Im}Z.\n(15)\nThe support Im\nZ\nZ\nconsists of all possible sums of elements from Im\nX\nX\nand Im\nY\nY\n. Direct computation has\nùí™\n‚Äã\n(\nN\n‚Äã\nM\n)\n\\mathcal{O}(NM)\ncomplexity, with support size up to\nN\n+\nM\n‚àí\n1\nN+M-1\n. Repeated convolution causes quadratic growth and explosion, hindering large-scale analysis.\nDefinition 16\n(Element-wise Product)\n.\nThe element-wise product of two vectors\nA\n,\nB\nA,B\nof length\nn\nn\n, denoted\nA\n‚äô\nB\nA\\odot B\n, results in vector\nC\n=\nA\n‚äô\nB\nC=A\\odot B\nof length\nn\nn\n, where each element\nc\ni\n=\na\ni\n‚ãÖ\nb\ni\nc_{i}=a_{i}\\cdot b_{i}\n.\nThe element-wise product (\nDefinition\n16\n) is central to efficient frequency-domain convolution (\nSection\n4.2\n).\n4.2\nEfficient Convolution Techniques\nThe\nùí™\n‚Äã\n(\nN\n‚Äã\nM\n)\n\\mathcal{O}(NM)\ncomplexity of direct linear convolution (\nDefinition\n15\n) and associated state-space expansion are major bottlenecks. This subsection details two techniques to mitigate this: Fast Fourier Transform (FFT) for single convolutions and exponentiation by squaring for repeated convolutions.\n4.2.1\nSingle Convolution using Fast Fourier Transform\nThe Fast Fourier Transform (FFT)\n[\nCooley1965An\n]\nsubstantially reduces convolution complexity, based on the Convolution Theorem\n[\nPapoulis1962\n]\n.\nTheorem 1\n(Convolution Theorem\n[\nPapoulis1962\n]\n)\n.\nFor discrete sequences\nV\nX\n,\nV\nY\nV_{X},V_{Y}\n, the Discrete Fourier Transform (DFT) of their linear convolution (\nV\nX\n‚àó\nV\nY\nV_{X}*V_{Y}\n) equals the element-wise product (\nDefinition\n16\n) of their DFTs, assuming appropriate zero-padding of\nV\nX\n,\nV\nY\nV_{X},V_{Y}\n:\n‚Ñ±\n‚Äã\n{\nV\nX\n‚àó\nV\nY\n}\n=\n‚Ñ±\n‚Äã\n{\nV\nX\n‚Ä≤\n}\n‚äô\n‚Ñ±\n‚Äã\n{\nV\nY\n‚Ä≤\n}\n,\n\\mathcal{F}\\{V_{X}*V_{Y}\\}=\\mathcal{F}\\{V_{X}^{\\prime}\\}\\odot\\mathcal{F}\\{V_{Y}^{\\prime}\\},\n(16)\nwhere\n‚Ñ±\n\\mathcal{F}\nis the DFT operation.\nAs described in\nTheorem\n1\n, time-domain convolution becomes a frequency-domain element-wise product. To compute\nZ\n=\nX\n+\nY\nZ=X+Y\n, PMFs\nV\nX\n,\nV\nY\nV_{X},V_{Y}\nare zero-padded to\nL\nL\n, transformed via FFT, multiplied element-wise, and converted back with an inverse FFT. With FFT/IFFT complexity of\nùí™\n‚Äã\n(\nL\n‚Äã\nlog\n‚Å°\nL\n)\n\\mathcal{O}(L\\log L)\n, overall complexity is\nùí™\n‚Äã\n(\nL\n‚Äã\nlog\n‚Å°\nL\n)\n\\mathcal{O}(L\\log L)\n. This enables larger supports within practical time limits\n[\nmarkovic2021convolution\n]\n. However, FFT-based convolution requires memory proportional to\nL\nL\nand can suffer from numerical errors due to floating-point arithmetic, especially for distributions with very small probabilities or a wide range of probability values.\n4.2.2\nExponentiation by Squaring in Convolutions\nFor\nk\nk\nIID variables, exponentiation by squaring exploits associativity to reduce the number of convolutions to\nùí™\n‚Äã\n(\nlog\n‚Å°\nk\n)\n\\mathcal{O}(\\log k)\n[\nGordon1998A\n]\n. The idea is to compute PMFs for sums corresponding to powers of two (\nS\n1\n,\nS\n2\n=\nS\n1\n‚àó\nS\n1\n,\nS\n4\n=\nS\n2\n‚àó\nS\n2\n,\n‚Ä¶\nS_{1},S_{2}=S_{1}*S_{1},S_{4}=S_{2}*S_{2},\\dots\n) using FFT-based convolution (\nSection\n4.2.1\n). For\nk\nk\ninstances, the binary representation of\nk\nk\ndetermines which\nS\n2\nj\nS_{2^{j}}\nPMFs are convolved. For example,\n13\n=\n8\n+\n4\n+\n1\n13=8+4+1\nrequires\nS\n8\n,\nS\n4\n,\nS\n1\nS_{8},S_{4},S_{1}\n. Each of these\nùí™\n‚Äã\n(\nlog\n‚Å°\nk\n)\n\\mathcal{O}(\\log k)\nconvolutions uses the FFT-based method. This efficiency is beneficial for analyzing execution times of multiple jobs from a single task, assuming IID execution times.\n5\nAccelerated Aggregate Convolution with Revised Critical Instant\nThis section details the proposed\nAccelerated Aggregate Convolution\nfor efficient WCDFP estimation, integrating the revised critical instant detailed in\nSection\n3.2\n. The theoretical basis for optimizing the convolution merge order is explored, contrasting naive sequential merging with a Huffman-based strategy, before presenting the pseudocode of the algorithm.\n5.1\nOptimizing Convolution Merge Order\nWhen computing the probabilistic distribution resulting from convolving multiple independent random variables, such as the execution times of numerous jobs, the order in which these convolutions are performed significantly impacts the overall computational cost. The Aggregate Convolution approach, central to this work, combines these distributions. While the efficiency of individual convolution operations is addressed by techniques like the FFT, detailed in\nSection\n4\n, this subsection focuses on a complementary optimization: minimizing the sum of intermediate distribution sizes encountered during the merging process.\nTable 2:\nHelper Functions for Accelerated Aggregate Convolution\nFunction Signature\nDescription\nCircularConvolution(\na\n,\nb\na,b\n)\nComputes the discrete convolution of two PMF vectors\na\na\nand\nb\nb\nusing FFT for efficient computation.\nTruncateAndSum(\na\n,\n[\nb\n,\nc\n)\na,[b,c)\n)\nTruncates PMF vector\na\na\nto range\n[\nb\n,\nc\n)\n[b,c)\nby removing elements outside this interval. Returns the sum of probabilities of the removed (out-of-range) elements.\n5.1.1\nWorst-Case Cost of Sequential Merging\nConsider\nN\nN\ninitial PMFs with sizes\nw\n1\n,\n‚Ä¶\n,\nw\nN\nw_{1},\\ldots,w_{N}\n(total size\nS\n=\n‚àë\nw\ni\nS=\\sum w_{i}\n). Sequential convolution (e.g.,\n(\nV\n1\n‚àó\nV\n2\n)\n‚àó\nV\n3\n‚Äã\n‚Ä¶\n(V_{1}*V_{2})*V_{3}\\ldots\n) causes progressive growth of intermediate distribution sizes. The cost, often measured by the sum of all input distribution sizes during merging, can be significant. For a sequential merge, the sum of operand sizes over\nN\n‚àí\n1\nN-1\nconvolutions can reach\nùí™\n‚Äã\n(\nS\n‚Äã\nN\n)\n\\mathcal{O}(SN)\nin the worst case. This occurs if large distributions form early and are repeatedly convolved, posing challenges for large\nN\nN\nor\nS\nS\n.\n5.1.2\nHuffman-Based Optimal Merging Strategy\nA more efficient strategy for merging\nN\nN\ndistributions aims to minimize the total computational effort by carefully selecting the pair of distributions to convolve at each step. This problem is analogous to constructing an optimal prefix code, a task solved by Huffman‚Äôs algorithm\n[\nhuffman1952method\n]\n. In this analogy, the initial PMF vectors correspond to symbols, and the size of each vector (\nw\ni\nw_{i}\n) corresponds to the frequency of a symbol. The merging process is equivalent to building a Huffman tree, where each internal node represents the convolution of the two child nodes.\nHuffman‚Äôs algorithm constructs this tree by iteratively merging the two nodes with the smallest current weights (sizes). This process is known to minimize the total weighted external path length,\n‚àë\ni\n=\n1\nN\nw\ni\n‚Äã\nd\ni\n\\sum_{i=1}^{N}w_{i}d_{i}\n, where\nd\ni\nd_{i}\nis the depth of a leaf. In the context of convolution, minimizing this sum effectively minimizes the total sum of operand sizes across all steps. By always merging the two smallest available distributions, the Huffman-based strategy ensures that larger distributions are, on average, involved in fewer merging steps. While\nN\n‚àí\n1\nN-1\nconvolutions are always needed, the Huffman strategy minimizes the sum of operand sizes for these operations to a bound of\nùí™\n‚Äã\n(\nS\n‚Äã\nlog\n‚Å°\nN\n)\n\\mathcal{O}(S\\log N)\n[\nhuffman1952method\n]\n, significantly improving upon the naive sequential\nùí™\n‚Äã\n(\nS\n‚Äã\nN\n)\n\\mathcal{O}(SN)\nworst-case bound. This merge sequence optimization complements other techniques such as FFT-based convolution and exponentiation by squaring.\n5.2\nImplementation of Accelerated Aggregate Convolution\nInput:\nœÑ\n\\tau\n,\nP\nC\nP_{C}\n(map of pWCET PMFs\nP\nC\ni\nP_{C_{i}}\n),\nJ\nx\n,\ny\nJ_{x,y}\n(Target Job)\nOutput:\nWCDFP for\nJ\nx\n,\ny\nJ_{x,y}\n1\n2\npdf_array\n‚Üê\n‚àÖ\n\\textnormal{pdf\\_array}\\leftarrow\\emptyset\n;\n3\nWCDFP\n‚Üê\n0.0\n\\textnormal{WCDFP}\\leftarrow 0.0\n;\npriority_queue\n‚Üê\n‚àÖ\n\\textnormal{priority\\_queue}\\leftarrow\\emptyset\n//\nMin-queue for (size, index)\n4\n5\nfor\nœÑ\ni\n‚àà\n{\nœÑ\nj\n‚à£\nœÄ\nj\n‚â•\nœÄ\nx\n}\n\\tau_{i}\\in\\{\\tau_{j}\\mid\\pi_{j}\\geq\\pi_{x}\\}\ndo\n6\nrelease_count\n‚Üê\n‚åà\nd\nx\n,\ny\n+\nD\ni\nT\ni\n‚åâ\n\\textnormal{release\\_count}\\leftarrow\\left\\lceil\\dfrac{d_{x,y}+D_{i}}{T_{i}}\\right\\rceil\n;\n7\nif\ni\n=\nx\ni=x\nthen\n8\nrelease_count\n‚Üê\n1\n\\textnormal{release\\_count}\\leftarrow 1\n;\n9\n10\n11\npdf\n‚Üê\nP\nC\ni\n\\textnormal{pdf}\\leftarrow P_{C_{i}}\n;\n12\nwhile\nrelease_count\n>\n0\n\\textnormal{release\\_count}>0\ndo\n13\nif\nrelease_count\nmod\n2\n=\n1\n\\textnormal{release\\_count}\\bmod 2=1\nthen\n14\nidx\n‚Üê\nlength\n‚Äã\n(\npdf_array\n)\n\\textnormal{idx}\\leftarrow\\textnormal{length}(\\textnormal{pdf\\_array})\n;\n15\npdf_array\n‚Üê\npdf_array\n‚à™\n{\npdf\n}\n\\textnormal{pdf\\_array}\\leftarrow\\textnormal{pdf\\_array}\\cup\\{\\textnormal{pdf}\\}\n;\n16\npriority_queue.push\n‚Äã\n(\n(\nlength\n‚Äã\n(\npdf\n)\n,\nidx\n)\n)\n\\textnormal{priority\\_queue.push}\\bigl(\\,(\\textnormal{length}(\\textnormal{pdf}),\\,\\textnormal{idx})\\bigr)\n;\n17\n18\npdf\n‚Üê\nCircularConvolution\n‚Äã\n(\npdf\n,\npdf\n)\n\\textnormal{pdf}\\leftarrow\\textnormal{CircularConvolution}(\\textnormal{pdf},\\textnormal{pdf})\n;\n19\nrelease_count\n‚Üê\n‚åä\nrelease_count\n2\n‚åã\n\\textnormal{release\\_count}\\leftarrow\\left\\lfloor\\dfrac{\\textnormal{release\\_count}}{2}\\right\\rfloor\n;\n20\n21\n22\n23\nwhile\nlength\n‚Äã\n(\npriority\n‚Äã\n_\n‚Äã\nqueue\n)\n>\n1\n\\mathrm{length(priority\\_queue)}>1\ndo\n24\n(\nsize\n‚Äã\n1\n,\nidx\n‚Äã\n1\n)\n‚Üê\npriority_queue.pop\n‚Äã\n(\n)\n(\\textnormal{size}1,\\textnormal{idx}1)\\leftarrow\\textnormal{priority\\_queue.pop}()\n;\n25\n(\nsize\n‚Äã\n2\n,\nidx\n‚Äã\n2\n)\n‚Üê\npriority_queue.pop\n‚Äã\n(\n)\n(\\textnormal{size}2,\\textnormal{idx}2)\\leftarrow\\textnormal{priority\\_queue.pop}()\n;\n26\nmerged_pdf\n‚Üê\nCircularConvolution\n‚Äã\n(\npdf_array\n‚Äã\n[\nidx\n‚Äã\n1\n]\n,\npdf_array\n‚Äã\n[\nidx\n‚Äã\n2\n]\n)\n\\textnormal{merged\\_pdf}\\leftarrow\\textnormal{CircularConvolution}\\bigl(\\textnormal{pdf\\_array}[\\,\\textnormal{idx}1\\,],\\textnormal{pdf\\_array}[\\,\\textnormal{idx}2\\,]\\bigr)\n;\n27\n28\nWCDFP\n‚Üê\nWCDFP\n+\nTruncateAndSum\n‚Äã\n(\nmerged_pdf\n,\n[\n0\n,\nd\nx\n,\ny\n)\n)\n\\mathrm{WCDFP}\\leftarrow\\mathrm{WCDFP}+\\textnormal{TruncateAndSum}\\bigl(\\textnormal{merged\\_pdf},[\\,0,d_{x,y}\\,)\\bigr)\n;\n29\n30\npdf_array\n‚Äã\n[\nidx\n‚Äã\n1\n]\n‚Üê\nmerged_pdf\n\\textnormal{pdf\\_array}[\\,\\textnormal{idx}1\\,]\\leftarrow\\textnormal{merged\\_pdf}\n;\n31\npriority_queue.push\n‚Äã\n(\nlength\n‚Äã\n(\nmerged_pdf\n)\n,\nidx\n‚Äã\n1\n)\n\\textnormal{priority\\_queue.push}\\bigl(\\textnormal{length}(\\textnormal{merged\\_pdf}),\\,\\textnormal{idx}1\\bigr)\n;\n32\n33\n34\nreturn\nWCDFP\n;\nAlgorithm¬†1\nAccelerated Aggregate Convolution for WCDFP\nThe\nAccelerated Aggregate Convolution\nmethod for WCDFP estimation incorporates the revised critical instant to identify relevant job releases and employs the Huffman-based merging strategy for efficient convolution of the resulting PMFs. The key helper functions in this algorithm are summarized in\n\\tabref\ntable:helper_functions_acc_agg_conv.\nAs shown in Algorithm\n1\n, the core logic first involves generating per-task workload PMFs. For each interfering task\nœÑ\ni\n\\tau_{i}\n, the algorithm determines the number of relevant job releases (\nrelease_count\n) under the revised critical instant (Lines 4-6). This count considers potential carry-in effects and concurrent execution; for the task\nœÑ\nx\n\\tau_{x}\nunder analysis, the count is one. Then, using the pWCET distribution\nP\nC\ni\nP_{C_{i}}\n, the aggregate PMF for these jobs is computed efficiently via exponentiation by squaring (Lines 9-15). The resulting PMFs are collected in\npdf_array\nwith references (size and index) pushed to a\npriority_queue\n.\nWith all PMF components in the\npriority_queue\n, a Huffman-like merge occurs (Lines 16-25). The two smallest PMFs are repeatedly extracted and convolved via CircularConvolution. The sum of probabilities outside the deadline, obtained from the TruncateAndSum function, is added to the WCDFP. The resulting PMF is reinserted into the\npriority_queue\n. This procedure continues until one PMF remains, returning the accumulated WCDFP.\n6\nEvaluation\nThis section presents the evaluation of the methods, focusing on their effectiveness and computational efficiency under various task set configurations.\nAll evaluated methods incorporate the revised critical instant, a key aspect of this work, addressing recent findings that classical assumptions can lead to WCDFP underestimation\n[\nchen2022critical\n]\n. Throughout this evaluation, four approaches are compared: the Monte Carlo method (\nMC\n); Sequential Convolution (\nSC\n), which performs convolutions iteratively following the arrival sequence\nŒæ\n\\xi\n, processing the pWCET of one job at a time; Aggregate Convolution (\nAC\n), with two variants:\nAC (Orig.)\n, an unoptimized version that applies exponentiation by squaring to convolve multiple jobs from the same task and then merges distributions from different tasks without considering specific arrival order, and\nAC (Imp.)\n, the optimized version that incorporates Huffman-based convolution merge order optimization\n[\nhuffman1952method\n]\nfor enhanced speed when combining multiple distributions; and the Berry-Esseen theorem-based approximation (\nBE\n). SC, AC (Orig.), and AC (Imp.) are convolution-based methods, with SC serving as a baseline for high accuracy and AC (Imp.) being the primary contribution regarding performance optimization.\n6.1\nExperiment Setup\nThis evaluation considers 1,500 sporadic task sets, evenly distributed across 10 task cardinalities {10, 20, 30, 40, 50, 60, 70, 80, 90, 100} and three utilization levels {0.60, 0.65, 0.70}. For each combination of cardinality and utilization, 50 task sets were generated. The inter-arrival time\nT\ni\nT_{i}\nof task\nœÑ\ni\n\\tau_{i}\nwas sampled logarithmically within the range [10 ms, 1000 ms]. Each task\nœÑ\ni\n\\tau_{i}\nhas a utilization\nU\ni\nU_{i}\nand a WCET denoted by\nWCET\ni\n\\mathrm{WCET}_{i}\n. The utilization\nU\ni\nU_{i}\nwas assigned using the Dirichlet-Rescale algorithm to balance workload distribution across tasks\n[\nGriffin2020Generating\n]\n. Execution times follow a truncated mixture distribution. Under normal operation, the execution time distribution is modeled as\nùí©\n‚Äã\n(\nŒº\ni\n,\nœÉ\ni\n)\n\\mathcal{N}(\\mu_{i},\\sigma_{i})\n, where\nŒº\ni\n=\nWCET\ni\n/\n3.0\n\\mu_{i}=\\mathrm{WCET}_{i}/3.0\nand\nœÉ\ni\n=\nWCET\ni\n/\n6.0\n\\sigma_{i}=\\mathrm{WCET}_{i}/6.0\n. For abnormal operation, the execution time is modeled as\nùí©\n‚Äã\n(\nŒº\ni\n‚Ä≤\n,\nœÉ\ni\n‚Ä≤\n)\n\\mathcal{N}(\\mu_{i}^{\\prime},\\sigma_{i}^{\\prime})\n, where\nŒº\ni\n‚Ä≤\n=\nWCET\ni\n/\n1.2\n\\mu_{i}^{\\prime}=\\mathrm{WCET}_{i}/1.2\nand\nœÉ\ni\n‚Ä≤\n=\nWCET\ni\n/\n30.0\n\\sigma_{i}^{\\prime}=\\mathrm{WCET}_{i}/30.0\n. The final execution time is defined as a weighted mixture of these two distributions:\nC\n~\ni\n‚àº\n0.95\n‚ãÖ\nùí©\n‚Äã\n(\nŒº\ni\n,\nœÉ\ni\n)\n+\n0.05\n‚ãÖ\nùí©\n‚Äã\n(\nŒº\ni\n‚Ä≤\n,\nœÉ\ni\n‚Ä≤\n)\n\\tilde{C}_{i}\\sim 0.95\\cdot\\mathcal{N}(\\mu_{i},\\sigma_{i})+0.05\\cdot\\mathcal{N}(\\mu_{i}^{\\prime},\\sigma_{i}^{\\prime})\ntruncated to the range [0,\nWCET\ni\n\\mathrm{WCET}_{i}\n], then discretized to the nearest unit\nŒ≥\n\\gamma\nand normalized, yielding the final distribution\nC\ni\nC_{i}\n. These distributions were generated based on empirical observations of Autoware runtimes measured by CARET\n[\nKuboichi2022CARET\n,\nToba2024Deadline\n]\n. The WCET values were uniquely derived based on the assigned utilization\nU\ni\nU_{i}\n, the task periods\nT\ni\nT_{i}\n, and the execution time distribution\nC\ni\nC_{i}\n, ensuring consistency across all task sets. Tasks were prioritized using rate-monotonic scheduling, and the minimum time unit\nŒ≥\n\\gamma\nwas set to 1 ¬µs to balance computational efficiency with precision. The task model assumes constrained deadlines, defined as\nD\ni\n‚â§\nT\ni\nD_{i}\\leq T_{i}\nin\n\\figref\nsec: model. For simplicity, this evaluation assumes\nD\ni\n=\nT\ni\nD_{i}=T_{i}\n.\nMonte Carlo (\nMC\n) simulations were performed with a fixed sample size of 100,000. For\nSC\nand Aggregate Convolution (\nAC\n), Python implementations used the\nfftconvolve\nfunction from the\nscipy\nlibrary to compute circular convolutions efficiently. Berry-Esseen (\nBE\n) was evaluated using the same parameters as specified in previous studies, ensuring consistency with existing approaches (e.g.,\n[\nmarkovic2022analytical\n]\n). The simulations were executed on an AMD Ryzen Threadripper 7960X processor (24 cores, 48 threads) with 128 MiB of L3 cache, running Ubuntu 22.04 LTS. Most methods were implemented in Python and executed in a single-threaded mode to ensure fair performance comparisons. In contrast,\nMC\nutilized 24 threads for parallel sample generation, taking advantage of the independence of these samples to reduce runtime without sacrificing accuracy. Results were aggregated at the end of computation, and this parallel execution must be taken into account when interpreting runtime comparisons.\n6.2\nEfficiency and Accuracy in WCDFP Estimation\nFigure 3:\nComparison of WCDFP estimation accuracy and execution time across\nMC\n,\nSC\n,\nAC (Imp.)\n, and\nBE\n. Points are positioned relative to baseline values at\nx\n=\n1.0\nx=1.0\nand\ny\n=\n1.0\ny=1.0\n, with quadrant counts displayed in the corners\nThis subsection examines the relationship between execution time and WCDFP estimation accuracy across the four evaluated methods, with a particular focus on the performance of the convolution-based approaches. Comparisons illustrated in\n\\figref\nfig:ratio_comparison show the relative differences between methods. Each point represents the ratios of accuracy and execution time, measured relative to baseline values at\nx\n=\n1.0\nx=1.0\nand\ny\n=\n1.0\ny=1.0\n.\nThe\nBE\nmethod, while consistently achieving the fastest execution times (10 to 100 times faster than other methods), generally yielded less accurate WCDFP estimates, especially against\nSC\nand\nAC (Imp.)\n.\nThe\nMC\nmethod was more accurate than\nBE\n. However, when compared against the convolution methods,\nMC\nshowed lower accuracy in many cases (e.g., 930 cases against\nAC (Imp.)\n) and did not outperform\nSC\nin any evaluated scenario for the given sample size. In terms of execution time,\nMC\nrequired approximately 10 times more computation than\nAC (Imp.)\n.\nThe primary comparison between the convolution methods,\nSC\nand\nAC (Imp.)\n, revealed that while\nAC (Imp.)\nwas consistently faster,\nSC\nwas generally more accurate. This behavior is detailed in\n\\figref\nfig:ratio_comparison. Interestingly, in 207 cases,\nAC (Imp.)\nshowed better accuracy than\nSC\n. This phenomenon, often observed when the true WCDFP is extremely low, is attributed to numerical errors in circular convolution; such errors are less likely to propagate extensively in\nAC (Imp.)\ndue to the reduced number of convolution operations of\nAC (Imp.)\n. Additional details regarding absolute accuracy comparisons are presented in\n\\figref\nfig:wcdfp_comparison.\nFigure 4:\nComparison of WCDFP values computed using\nMC\n,\nSC\n,\nAC (Imp.)\n, and\nBE\n. Each point represents a taskset, with counts of task sets above and below the\ny\n=\nx\ny=x\nline displayed in the top-left and bottom-right corners, respectively\nFigure 5:\nComparison of execution times for WCDFP estimation across\nMC\n,\nSC\n,\nAC (Orig.)\n,\nAC (Imp.)\n, and\nBE\n. The boxplots summarize execution time distributions, with each method corresponding to a box\n6.3\nDetailed Accuracy and Performance Analysis\nThis subsection further examines the accuracy of WCDFP estimates and the execution times of\nSC\nand\nAC (Imp.)\n, emphasizing the impact of the proposed optimization in\nAC (Imp.)\n. Data for MC and BE from Figs.\n4\nand\n5\nserve as context.\nFor WCDFP values exceeding\n10\n‚àí\n12\n10^{-12}\n,\nSC\nachieved up to four orders of magnitude higher precision than\nAC (Imp.)\n, underscoring the suitability of\nSC\nfor scenarios demanding the highest precision, albeit at the cost of significantly longer execution times (tens of times slower than\nAC (Imp.)\n, as shown in\n\\figref\nfig:time_comparison). Conversely, for WCDFP values below\n10\n‚àí\n12\n10^{-12}\n,\nAC (Imp.)\noccasionally outperformed\nSC\nin accuracy. This behavior is attributed to\nAC (Imp.)\nperforming fewer convolution operations, which reduces the accumulation of numerical errors, a critical factor when dealing with extremely small probabilities.\nThe core contribution related to computational efficiency, the optimization implemented in\nAC (Imp.)\n, yielded substantial performance improvements. As shown in\n\\figref\nfig:time_comparison,\nAC (Imp.)\ndemonstrated an average speedup of about 5 times over\nAC (Orig.)\n. This speedup enabled\nAC (Imp.)\nto complete WCDFP analyses within 1 second for many configurations, making advanced convolution-based WCDFP estimation under the revised critical instant significantly more practical. While this optimization primarily targets the order of convolution operations, and thus theoretical precision differences between\nAC (Imp.)\nand\nAC (Orig.)\nare not the central concern, the reduction in the size of intermediate distributions handled during the merging process in\nAC (Imp.)\ncan potentially contribute to a reduction in absolute numerical error.\nFor context,\nMC\nestimates, with a sample size of 100,000, converged near\n10\n‚àí\n4\n10^{-4}\nand did not match the precision of\nSC\n(see\n\\figref\nfig:wcdfp_comparison).\nThe\nBE\nmethod, while extremely fast (average\n10\n‚àí\n1\n10^{-1}\nseconds, as shown in\n\\figref\nfig:time_comparison), exhibited clear precision limitations, especially for WCDFP values below\n10\n‚àí\n3\n10^{-3}\n(see\n\\figref\nfig:wcdfp_comparison).\n7\nRelated Work\nThis section presents an overview of existing studies and their differences from this paper, with a summary shown in\n\\tabref\ntable:comparison.\nRecent research has redefined the critical instant for probabilistic response-time analysis, showing that simultaneous task releases do not always yield the worst-case scenario\n[\nchen2022critical\n]\n. This finding underscores the need for methods integrating such insights, as classical assumptions may lead to WCDFP underestimation.\nConvolution-based methods\n[\nmarkovic2021convolution\n]\napproximate response-time distributions by convolving task execution time distributions. While capable of precise results, scalability can be limited by computational costs, and many existing approaches rely on the classical critical instant assumption. To improve efficiency, advancements like circular convolution and down-sampling have been introduced. In contrast to prior work using coarser discretizations (e.g., 50 ¬µs\n[\nmarkovic2021convolution\n]\n), this paper employs a finer 1¬µs discretization for higher resolution. This work builds upon these foundational techniques by applying the revised critical instant and introducing further optimizations for aggregate convolution.\nMonte Carlo-based analysis\n[\nroux2021montecarlo\n]\nestimates WCDFP via random sampling, offering scalability, particularly for systems with many tasks. However, this method often relies on the classical critical instant and can require substantial samples to achieve high precision for very low WCDFP values. Notably, while some evaluations use simplified execution time models (e.g., bimodal distributions\n[\nroux2021montecarlo\n]\n), this paper applies MC analysis to the same high-resolution and realistic mixture model distributions used for all evaluated methods, ensuring a consistent comparison.\nThe Berry-Esseen inequality-based analysis\n[\nmarkovic2022analytical\n]\noffers an analytical approximation using statistical properties via the Lyapunov central limit theorem. This approach is computationally efficient and avoids the combinatorial explosion of convolutions, though the bound tightness can vary as the method abstracts away fine-grained distribution details.\nCantelli inequality-based methods address WCDFP estimation for dependent tasks. Correlation Tolerant Analysis (CTA)\n[\nmarkovic2023cta\n]\nprovides bounds from mean and standard deviation but can be pessimistic due to not explicitly considering task dependencies. Correlation Aware Analysis (CAA)\n[\nmarkovic2024caa\n]\nimproves upon CTA by incorporating covariance bounds for more accurate estimates.\nTable 3:\nComparison of related methods and features\nRCI\nBE\nMC\nCC\nACC\nHRED\nRTSS 2023\n[\nmarkovic2023cta\n]\n‚úì\n‚úì\nRTSS 2024\n[\nmarkovic2024caa\n]\n‚úì\n‚úì\nRTSS 2021\n[\nroux2021montecarlo\n]\n‚úì\nECRTS 2021\n[\nmarkovic2021convolution\n]\n‚úì\nRTSS 2022\n[\nmarkovic2022analytical\n]\n‚úì\n‚úì\n‚úì\nThis paper\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\nRCI:\nConsideration of Revised Critical Instant\nBE:\nBerry-Esseen Correction (included as a baseline method)\nMC:\nMonte Carlo Simulation (included as a baseline method)\nCC:\nCircular Convolution (included as a baseline method)\nACC:\nAccelerated Circular Convolution with Order Optimization\nHRED:\nHigh-Resolution and Realistic Execution Distributions\n8\nConclusions\nThis paper enhanced WCDFP estimation by incorporating the revised critical instant into convolution-based analysis and introducing an optimized Aggregate Convolution technique. The Huffman-based merge ordering strategy significantly improved efficiency while preserving the safe-sided accuracy required for dependable real-time systems. Evaluations with realistic execution-time distributions confirmed that the optimized method outperforms Sequential Convolution in computation time, while still providing reliable WCDFP estimates. The study emphasized that method selection requires balancing accuracy, cost, and target WCDFP characteristics, with the revised critical instant as a key assumption for trustworthy analysis.\nFuture work includes extending the method to systems with task dependencies, exploring sustained high-utilization scenarios that challenge current interference assumptions, and investigating parallel implementations to improve scalability. Ultimately, advancing robustness and broader adoption of accurate WCDFP estimation techniques remains a vital pursuit in real-time systems.\n{acknowledgment}\nThis work was supported by JST AIP Acceleration Research JPMJCR25U1 and JST CREST Grant Number JPMJCR23M1, Japan.",
  "preview_text": "Accurate estimation of the Worst-Case Deadline Failure Probability (WCDFP) has attracted growing attention as a means to provide safety assurances in complex systems such as robotic platforms and autonomous vehicles. WCDFP quantifies the likelihood of deadline misses under the most pessimistic operating conditions, and safe estimation is essential for dependable real-time applications. However, achieving high accuracy in WCDFP estimation often incurs significant computational cost. Recent studies have revealed that the classical assumption of the critical instant, the activation pattern traditionally considered to trigger the worst-case behavior, can lead to underestimation of WCDFP in probabilistic settings. This observation motivates the use of a revised critical instant formulation that more faithfully captures the true worst-case scenario. This paper investigates convolution-based methods for WCDFP estimation under this revised setting and proposes an optimization technique that accelerates convolution by improving the merge order. Extensive experiments with diverse execution-time distributions demonstrate that the proposed optimized Aggregate Convolution reduces computation time by up to an order of magnitude compared to Sequential Convolution, while retaining accurate and safe-sided WCDFP estimates. These results highlight the potential of the approach to provide both efficiency and reliability in probabilistic timing analysis for safety-critical real-time applications.\n\n\\affiliate\nSaitamaGraduate School of Science and Engineering, Saitama University, Japan\n\\affiliate\nAcademicAcademic Association (Graduate School of Science and Engineering), Saitama University\n\\affiliate\nTIERIVTIER IV Inc., Japan\nSaitama[takahashi.h.477@ms.saitama-u.ac.jp]\nSaitama,TIERIV[]\nAcademic,TIERIV[]\nAccelerating Probabilistic Response-Time Analysis: Revised Critical Instant and Optimized Convolution\nHiroto Takahashi\nAtsushi Yano\nTakuya Azumi\nAbstract\nAccurate estimation of the Worst-Ca",
  "is_relevant": null,
  "relevance_score": 0.0,
  "extracted_keywords": [],
  "one_line_summary": "",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "2025-12-01T07:54:48Z",
  "created_at": "2026-01-08T10:08:08.050903",
  "updated_at": "2026-01-08T10:08:08.050913"
}