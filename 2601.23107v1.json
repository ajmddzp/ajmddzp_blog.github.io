{
    "id": "2601.23107v1",
    "title": "FlowCalib: LiDAR-to-Vehicle Miscalibration Detection using Scene Flows",
    "authors": [
        "Ilir Tahiraj",
        "Peter Wittal",
        "Markus Lienkamp"
    ],
    "abstract": "ç²¾ç¡®çš„ä¼ æ„Ÿå™¨ä¸è½¦è¾†æ ‡å®šå¯¹äºè‡ªåŠ¨é©¾é©¶å®‰å…¨è‡³å…³é‡è¦ã€‚æ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨çš„è§’åº¦å¤±å‡†å¯èƒ½å¯¼è‡´è‡ªåŠ¨é©¾é©¶è¿‡ç¨‹ä¸­çš„å…³é”®å®‰å…¨é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ ¡æ­£ä¼ æ„Ÿå™¨é—´è¯¯å·®ï¼Œå´æœªä»æ ¹æºä¸Šè€ƒè™‘å¼•å‘è¿™äº›è¯¯å·®çš„å•ä¸ªä¼ æ„Ÿå™¨æ ‡å®šå¤±å‡†é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºFlowCalibæ¡†æ¶â€”â€”é¦–ä¸ªåˆ©ç”¨é™æ€ç‰©ä½“åœºæ™¯æµè¿åŠ¨ç‰¹å¾æ£€æµ‹æ¿€å…‰é›·è¾¾ä¸è½¦è¾†æ ‡å®šå¤±å‡†çš„ç³»ç»Ÿã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æè¿ç»­ä¸‰ç»´ç‚¹äº‘ç”Ÿæˆæµåœºä¸­ç”±æ—‹è½¬å¤±å‡†å¼•èµ·çš„ç³»ç»Ÿæ€§åå·®å®ç°æ£€æµ‹ï¼Œæ— éœ€é¢å¤–ä¼ æ„Ÿå™¨æ”¯æŒã€‚è¯¥æ¶æ„èåˆç¥ç»åœºæ™¯æµå…ˆéªŒè¿›è¡Œæµåœºä¼°è®¡ï¼Œå¹¶é‡‡ç”¨åŒåˆ†æ”¯æ£€æµ‹ç½‘ç»œï¼Œå°†å­¦ä¹ åˆ°çš„å…¨å±€æµç‰¹å¾ä¸äººå·¥è®¾è®¡çš„å‡ ä½•æè¿°ç¬¦ç›¸ç»“åˆã€‚è¿™ç§å¤åˆè¡¨å¾ä½¿ç³»ç»Ÿèƒ½å¤Ÿæ‰§è¡Œä¸¤é¡¹äº’è¡¥çš„äºŒåˆ†ç±»ä»»åŠ¡ï¼šå…¨å±€äºŒåˆ†ç±»åˆ¤æ–­æ˜¯å¦å­˜åœ¨å¤±å‡†ï¼Œä»¥åŠé’ˆå¯¹å„æ—‹è½¬è½´çš„ç‹¬ç«‹äºŒåˆ†ç±»åˆ¤æ–­å„è½´å‘æ˜¯å¦å¤±å‡†ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒFlowCalibèƒ½ç¨³å¥æ£€æµ‹æ ‡å®šå¤±å‡†ï¼Œä¸ºä¼ æ„Ÿå™¨ä¸è½¦è¾†æ ‡å®šå¤±å‡†æ£€æµ‹å»ºç«‹äº†æ–°åŸºå‡†ã€‚",
    "url": "https://arxiv.org/abs/2601.23107v1",
    "html_url": "https://arxiv.org/html/2601.23107v1",
    "html_content": "FlowCalib: LiDAR-to-Vehicle Miscalibration Detection using Scene Flows\nIlir Tahiraj\n1,âˆ—\n, Peter Wittal\n2\n, Markus Lienkamp\n1\nâˆ—\nCorresponding author\nilir.tahiraj@tum.de\n1\nAuthors are with the TUM School of Engineering and Design, Chair of Automotive Technology, Technical University of Munich.\n2\nis with the TUM School of Computation, Information and Technology, Technical University of Munich.\nAbstract\nAccurate sensor-to-vehicle calibration is essential for safe autonomous driving. Angular misalignments of LiDAR sensors can lead to safety-critical issues during autonomous operation. However, current methods primarily focus on correcting sensor-to-sensor errors without considering the miscalibration of individual sensors that cause these errors in the first place. We introduce FlowCalib, the first framework that detects LiDAR-to-vehicle miscalibration using motion cues from the scene flow of static objects. Our approach leverages the systematic bias induced by rotational misalignment in the flow field generated from sequential 3D point clouds, eliminating the need for additional sensors. The architecture integrates a neural scene flow prior for flow estimation and incorporates a dual-branch detection network that fuses learned global flow features with handcrafted geometric descriptors. These combined representations allow the system to perform two complementary binary classification tasks: a global binary decision indicating whether misalignment is present and separate, axis-specific binary decisions indicating whether each rotational axis is misaligned. Experiments on the nuScenes dataset demonstrate FlowCalibâ€™s ability to robustly detect miscalibration, establishing a benchmark for sensor-to-vehicle miscalibration detection.\nI\nINTRODUCTION\nExtrinsic sensor calibration is key to enabling safe object detection. It ensures an accurate and reliable spatial representation of fused data. There are two methods of extrinsic calibration for multi-sensor setups: sensor-to-sensor (S2S) and sensor-to-vehicle (S2V). S2S calibration primarily contributes to object detection and sensor fusion performance, while S2V calibration contributes to overall safety and driving tasks. S2V calibration ensures accurate object localization and environment modeling, directly influencing downstream tasks such as autonomous vehicle (AV) planning.\nTo illustrate this, consider the following practical example of S2V yaw errors: An autonomous vehicle is driving at high speed on a highway and approaching another vehicle around 100 meters away. There are three cases of calibration error: positive yaw angle error, no yaw angle error, and negative yaw angle error. These are depicted in Fig.\n1\n. These errors can result in objects being detected in the lane next to the vehicle, raising safety concerns. Similarly, a vehicle driving next to the ego vehicle could be detected in the ego vehicleâ€™s lane. This can lead to comfort issues, such as unnecessary braking, as well as safety issues for vehicles driving behind the ego vehicle.\nThis makes S2V errors safety-critical during autonomous vehicle operation. Note that these problems can occur even with perfect S2S calibration. Although many online calibration algorithms exist that align LiDAR-camera data and meet such accuracy requirements, there is an inherent problem with online S2S calibration that must be considered for autonomous driving in the real world: detecting individual sensor miscalibration.\nFigure 1\n:\nThis is a qualitative illustration of objects in the environment model in the presence of sensor-to-vehicle yaw angle errors. Objects appearing in neighboring lanes can pose safety issues during autonomous vehicle operation.\n{\nV\n}\n\\{V\\}\nrepresents the vehicle frame and\n{\nL\n}\n\\{L\\}\nthe LiDAR frame.\nOnline recalibration algorithms across the field train their frameworks to regress the transformation matrix between the LiDAR and camera sensors. Training data is generated by injecting errors into the matrix that transforms LiDAR point clouds onto the image plane, thereby learning a unidirectional correction from LiDAR to the camera sensor\n[\n1\n]\n. This approach is valid when the misalignment between the two sensors originates from a miscalibrated LiDAR sensor. However, always correcting the LiDAR points, even when the camera is miscalibrated, can worsen the situation in the real world. This problem can be solved by detecting miscalibration in one of the sensors to enable bidirectional correction.\nTherefore, we propose FlowCalib, a framework for detecting LiDAR-to-vehicle miscalibration that uses object motion patterns in the presence of angular calibration errors. Specifically, we use 3D point cloud flow fields of static objects to detect extrinsic LiDAR miscalibration. Our contributions are as follows:\nâ€¢\nFlowCalib is the first LiDAR-to-vehicle miscalibration detection framework.\nâ€¢\nWe demonstrate that our framework can reliably detect angular calibration errors and establish a baseline for miscalibration detection on the nuScenes dataset.\nâ€¢\nThe detection works without the use of additional sensors, such as IMU, GPS, or camera.\nâ€¢\nFlowCalib provides both global miscalibration detection and axis-specific detection, providing information on roll, pitch, and yaw angle miscalibration.\nâ€¢\nThe FlowCalib code will be made available as open source.\nII\nRELATED WORK\nThis section summarizes the work on S2V calibration and miscalibration detection. First, a sensitivity analysis of calibration errors on state-of-the-art object detectors is presented to motivate the detection of LiDAR miscalibration. Next, we provide an overview of current miscalibration detection and sensor-to-vehicle calibration algorithms in general.\nUsing LiDAR as the reference sensor and enhancing detection with camera images has become the de facto standard in autonomous driving research. Fig.\n2\nillustrates the performance of four detection models\n[\n22\n,\n2\n,\n12\n,\n24\n]\nfrom the nuScenes\n[\n3\n]\nleaderboard in the presence of various S2S calibration errors. The left image shows the modelsâ€™ performance with S2S errors due to LiDAR miscalibration. The left image below shows the LiDAR-to-camera alignment in the presence of a LiDAR-to-vehicle yaw error. The upper right plot shows the modelsâ€™ performance with S2S errors due to camera miscalibration. Accordingly, the lower right image shows the LiDAR-to-camera alignment in the presence of a Camera-to-vehicle yaw error. Note that both images correspond to the same yaw error of\n+\n3\n+3\ndegree for the LiDAR-to-vehicle and\nâˆ’\n3\n-3\ndegree for the Camera-to-vehicle and therefore to the exact same relative S2S error.\nEven though the S2S for both configurations are the same, the primary object detection performance decline occurs in the presence of miscalibrated LiDAR, while minimal performance drops are observed in cases of camera miscalibration. This suggests that LiDAR-to-vehicle calibration plays a predominant role in overall detection performance. This finding is not unexpected, as LiDAR provides the primary features necessary for localizing objects in the environment and plays an instrumental role in evaluating object detection algorithms. This makes it clear that detecting LiDAR miscalibration is the first and foremost priority to ensure safety at all times during autonomous operation.\nII-\n1\nMiscalibration Detection\nThere are many robust and efficient online calibration algorithms\n[\n16\n,\n7\n,\n25\n,\n20\n]\navailable in the field of calibration research that regress the calibration parameters directly. An early consideration of calibration as a classification problem is presented by Levinson\net al.\n[\n10\n]\n. The proposed method first generates an edge image and the discontinuity of the point cloud. The depth image is created by applying an edge filter,\nresulting in its â€edginessâ€. The point cloudsâ€™ discontinuities are calculated by determining the depth difference between each point and its two neighbors. In\n[\n21\n]\n, a deep-learning-based method that can detect and calibrate misaligned sensors is introduced. Tahiraj\net al.\n[\n19\n]\nalso shift the focus from regressing the parameters to providing solely a classification of the miscalibration state. They propose a lightweight network based on contrastive learning to detect miscalibration in real time.\nFigure 2\n:\nThe plots on the left show the performance and sensor alignment in the presence of a yaw angle LiDAR-to-vehicle error. The right plots show the performance and LiDAR-camera alignment in the presence of an inverse yaw angle camera-to-vehicle error. From the LiDAR-to-camera perspective, the two sensors have identical errors. However, significant performance drops are observed only for LiDAR-to-vehicle errors.\nAll of these approaches focus on sensor-to-sensor miscalibration detection. Yet, as previously highlighted, it is essential to move beyond detecting misalignment between sensors and toward identifying which individual sensor is miscalibrated. This work focuses on detecting angular miscalibration in LiDAR sensors.\nII-\n2\nSensor-to-Vehicle Calibration\nSeveral methods for calibrating individual LiDAR systems are described in the literature. Most of these approaches perform roll and pitch calibration using the ground plane, such as Multi-LiCa\n[\n8\n]\nand SensorX2Car\n[\n23\n]\n. Yan\n[\n23\n]\nand Seok\n[\n17\n]\nadditionally provide yaw angle calibration. An alternative approach for LiDAR-to-vehicle calibration is presented in\n[\n18\n]\n. CaLiV performs LiDAR-to-vehicle calibration by leveraging calibration targets and a simple maneuver.\nOur approach to detecting miscalibration between the LiDAR and the vehicle frame differs in that we analyze the motion patterns of static objects using scene flows and train a classifier to identify miscalibration. As previously mentioned, miscalibration causes time-dependent patterns in the observed motion of objects when the vehicle is moving. Our method does not depend on known ground planes, calibration targets, or specific vehicle maneuvers.\nIII\nMETHODOLOGY\nThe underlying idea of our proposed framework is that the position of any point on a static object follows the motion pattern of the ego vehicle. If an object is directly in front of the vehicle and moving in a straight line, the trajectory of the perceived points is also expected to be straight. In the case of angular misalignment, the pointâ€™s movement includes a bias in the respective dimensions and changes with the distance between the vehicle and the object. Fig.\n3\nillustrates the variation in the perception of point positions in the case of a straight trajectory. For illustrative purposes, a straight trajectory is shown. However, it should be noted that this phenomenon applies to arbitrary trajectories.\nFlow vectors incorporate information about sensor alignment status and represent the movement perceived by each sensor. The perceived flow of each point encodes ego-motion patterns that change systematically if misalignment is present. Thus, flow provides an alternative to using hard geometric constraints\n[\n14\n]\nor vehicle movement with SLAM\n[\n23\n]\nor ICP\n[\n17\n]\n.\nFigure 3\n:\nThe idea behind using flow fields to detect LiDAR miscalibration is that a calibration error appears as a bias in the point flow. This causes distance-dependent errors that decrease as an object approaches the vehicle. The vehicleâ€™s trajectory is shown above. The resulting motion pattern of the objects, represented here as points, is shown below.\nFigure 4\n:\nFlowCalib uses a two-stage learning process. First, flow fields are generated to construct features. These features are used to learn feature embeddings and train the global and axis detection heads. The heads are trained to detect miscalibration in the presence of random calibration errors and to identify the miscalibrated axis.\nWith this premise of point motion, this section describes the FlowCalib framework (see Fig.\n4\n) and is structured as follows: First, fault injection and the preprocessing steps that each point cloud undergoes are presented. We also explains how scene flow is generated and how the LiDAR miscalibration detection is designed. Finally, the implementation details of FlowCalib are presented.\nIII-A\nFault Injection\nThe nuScenes\n[\n3\n]\ndataset is used as the base set, providing a pre-calibrated dataset. Miscalibrations are applied by injecting an additional angular error to the point cloud. The resulting transformation is given in Eq.\n1\n. The rotation matrix\nğ‘\nÎ±\ne\nâ€‹\nr\nâ€‹\nr\n\\mathbf{R}_{\\alpha_{err}}\nrepresents a rotation of\nÎ±\ne\nâ€‹\nr\nâ€‹\nr\n\\alpha_{err}\napplied to the original point cloud\nX\no\nâ€‹\nr\nâ€‹\ng\n\\textbf{X}_{org}\n. The original point cloud is transformed into the distorted point cloud\nğ—\nd\nâ€‹\ni\nâ€‹\ns\nâ€‹\nt\n\\mathbf{X}_{dist}\nğ—\nd\nâ€‹\ni\nâ€‹\ns\nâ€‹\nt\n=\nX\no\nâ€‹\nr\nâ€‹\ng\nâ€‹\nğ‘\nÎ±\ne\nâ€‹\nr\nâ€‹\nr\n.\n\\mathbf{X}_{dist}=\\textbf{X}_{org}\\mathbf{R}_{\\alpha_{err}}.\\qquad\n(1)\nRotation is applied randomly to each axis. The resulting samples are evenly distributed across different levels of severity.\nÎ±\ne\nâ€‹\nr\nâ€‹\nr\nâˆ¼\nğ’°\nâ€‹\n(\n[\nâˆ’\n5.0\n,\nâˆ’\n0.5\n]\nâˆ˜\nâˆª\n[\n0.5\n,\n5.0\n]\nâˆ˜\n)\n\\alpha_{err}\\sim\\mathcal{U}\\big([-5.0,-0.5]^{\\circ}\\cup[0.5,5.0]^{\\circ}\\big)\nis sampled from a uniform distribution resulting in equal rotation exposure following\n[\n5\n]\n.\nIII-B\nData Preprocessing\nThe raw nuScenes point clouds require pre-processing to enable scene flow generation. The input and output of this step is illustrated in Fig.\n5\n. The preprocessing ensures data relevance, reduces noise, and aligns the perspective with the vehicleâ€™s reference frame. There are four key operations involved in pre-processing:\n(a)\nBefore preprocessing\n(b)\nAfter preprocessing\nFigure 5\n:\nThe point cloud is shown before (left) and after (right) the pre-processing steps. The blue and green point clouds after preprocessing are shown at timesteps\nt\nt\nand\nt\n+\n1\nt+1\n, respectively. These serve as input for scene flow generation. Note that the point cloud are transformed into vehicle coordinate systems after the preprocessing.\nIII-B\n1\nGround removal\nPoint clouds include ground points as part of the sensing process. These points represent a flat area with few distinguishing characteristics, which makes it difficult to provide robust information for scene flow prediction\n[\n11\n]\n. The absence of features makes determining the movement pattern of the points difficult, as the flat surface represents a homogeneous area. Scene flow predictors have difficulty finding the correct correspondences and calculating movement over time. To overcome these challenges and prevent false flows, ground points are removed prior to scene flow generation. Ground point removal follows the implementation of UNION\n[\n9\n]\nemploying RANSAC\n[\n6\n]\n.\nIII-B\n2\nCoordinate Systems\nThe raw point clouds provided by the nuScenes dataset are initially represented in the LiDAR sensor frame. Howevery, the bias introduced by misalignment is anchored in the S2V transformation. To facilitate analysis and interpretation relative to the vehicleâ€™s coordinate system, the point clouds are transformed into the vehicle frame. This transformation is achieved by applying the expected extrinsic rotation,\nğ‘\nS\nV\n{}^{V}_{S}\\mathbf{R}\n, which aligns the sensor data with the vehicleâ€™s coordinate system. This alignment enables a consistent evaluation of point positions and motions with respect to the vehicleâ€™s pose.\nIII-B\n3\nPoint Cloud Distillation\nnuScenes provides keyframes captured at 2 Hz, and additional sweeps captured at 20 Hz\n[\n3\n]\n. FlowCalib focuses on the rotation detection per keyframe. Scene flow is generated between\nn\nt\nn_{t}\ntime steps to incorporate temporal information described by the motion of points to detect LiDAR miscalibration. Directly estimating scene flow over large time intervals increases the risk of incorrect point correspondences, which is inherent in the scene flow generation process\n[\n11\n]\n. To address this issue, scene flow is estimated over smaller steps. Therefore, it is necessary to select frames prior to generating the scene flow. The frames chosen represent the farthest possible time steps. Key frames include ten sweeps and intermediate, non-labeled frames. Selecting\nn\nt\nn_{t}\nframes results in a frequency of the chosen frames:\nf\ns\nâ€‹\ne\nâ€‹\nl\nâ€‹\ne\nâ€‹\nc\nâ€‹\nt\n=\nf\ns\nâ€‹\na\nâ€‹\nm\nâ€‹\np\nâ€‹\nl\nâ€‹\ne\nâ€‹\nd\nn\nf\nâ€‹\nr\nâ€‹\na\nâ€‹\nm\nâ€‹\ne\nâ€‹\ns\nn\nt\nf_{select}=\\frac{f_{sampled}}{\\frac{n_{frames}}{n_{t}}}\nThis approach enables us to generate meaningful and robust scene flow by balancing the need for temporal information with the practical challenges of point cloud matching.\nIII-B\n4\nDynamic Object Removal\nMoving objects can disturb the perceived movement represented in the flow field. Since they have independent velocities, point movements do not represent the sensorâ€™s perceived point flow. Therefore, to increase the robustness of miscalibration detection, it is necessary to focus on static objects. This can be achieved by removing labeled object points. In this work, these points are selected using the ground truth bounding boxes of the dataset.\nIII-C\nScene Flow Generation\nWe use scene flow detection to generate a vector field that describes the movement of each point between two time steps. This information provides insight into the perceived movement of the LiDAR sensor. The micalibration detector. described in the next subsection, uses the vector field to check for sensor rotation because the pattern of point movement differentiates between an aligned and misaligned sensor. The scene flow generation is based on the Neural Scene Flow Prior (NSFP)\n[\n11\n]\n.\nGiven two consecutive 3D point clouds\nğ—\nt\nâˆ’\n1\n\\mathbf{X}_{t-1}\nand\nğ—\nt\n\\mathbf{X}_{t}\n, NSFP aims to estimate the scene flow\nF\n=\n{\nğ®\ni\n}\ni\n=\n1\nğ—\nt\nâˆ’\n1\nF=\\{\\mathbf{u}_{i}\\}_{i=1}^{\\mathbf{X}_{t-1}}\n, where each point\nğ±\nt\nâˆ’\n1\nâˆˆ\nğ—\nt\nâˆ’\n1\n\\mathbf{x}_{t-1}\\in\\mathbf{X}_{t-1}\nmoves according to a flow vector\nğ®\nâˆˆ\nâ„\n3\n\\mathbf{u}\\in\\mathbb{R}^{3}\nsuch that\nğ±\nt\nâˆ’\n1\nâ€²\n=\nğ—\nt\nâˆ’\n1\n+\nğ®\n\\mathbf{x}^{\\prime}_{t-1}=\\mathbf{X}_{t-1}+\\mathbf{u}\n. The problem is formalized as minimizing the distance between the transformed points from\nğ—\nt\nâˆ’\n1\n\\mathbf{X}_{t-1}\nand the points in\nğ—\nt\n\\mathbf{X}_{t}\n, with an added regularization term:\nâ„±\nâˆ—\n=\narg\nâ¡\nmin\nâ„±\nâ€‹\nâˆ‘\nğ±\nt\nâˆ’\n1\nâˆˆ\nğ—\nt\nâˆ’\n1\nD\nâ€‹\n(\nğ±\nt\nâˆ’\n1\n+\nğ®\n,\nğ—\nt\n)\n+\nÎ»\nâ€‹\nC\n,\n\\mathcal{F}^{*}=\\arg\\min_{\\mathcal{F}}\\sum_{\\mathbf{x}_{t-1}\\in\\mathbf{X}_{t-1}}D(\\mathbf{x}_{t-1}+\\mathbf{u},\\mathbf{X}_{t})+\\lambda C,\n(2)\nwhere:\nâ€¢\nD\nâ€‹\n(\nğ±\nt\nâˆ’\n1\n+\nğ®\n,\nğ—\nt\n)\nD(\\mathbf{x}_{t-1}+\\mathbf{u},\\mathbf{X}_{t})\ncomputes the distance from the shifted point\nğ±\nt\nâˆ’\n1\n+\nğ®\n\\mathbf{x}_{t-1}+\\mathbf{u}\nto its nearest neighbor in\nğ—\nt\n\\mathbf{X}_{t}\n,\nâ€¢\nC\nC\nis a regularization term,\nâ€¢\nÎ»\n\\lambda\nis a weighting factor for the regularizer.\nFor the distance function, the squared Euclidean distance to the nearest neighbor is used. NSFP additionally incorporates a neural network as an implicit regularizer that optimizes the scene flow. This allows scene-specific flows at runtime and does not require pre-training or labeled datasets\n[\n11\n]\n.\nIII-D\nMiscalibration Detection\nA prediction model using a mixture of hand-crafted and learned feature embeddings is proposed to check for miscalibration of the LiDAR. First, the flow field\nâ„±\n\\mathcal{F}\nis generated by the scene flow distillation process using the point cloud at time step\nt\nn\nt_{n}\n, which is used to anchor the flow vectors in the correct spatial context. Each flow vector\nğ®\nt\nâˆ’\n1\nâ†’\nt\n,\ni\nâˆˆ\nâ„±\nt\nâˆ’\n1\nâ†’\nt\n{\\mathbf{u}_{t-1\\to t,i}}\\in\\mathcal{F}_{t-1\\to t}\ncorresponds to a point\nğ±\ni\n\\mathbf{x}_{i}\nin the point cloud\nğ—\nt\nâˆ’\ni\n\\mathbf{X}_{t-i}\n, representing the motion of\nğ±\ni\n\\mathbf{x}_{i}\nfrom time\nt\nâˆ’\n1\nt-1\nto\nt\nt\n.\nThe detector consists of two encoders that learn global flow feature embeddings and handcrafted geometric flow features, as well as two separate decoders that detect the LiDARâ€™s alignment. The detectorâ€™s output is an alignment indicator in the range of\n[\n0\n,\n1\n]\n[0,1]\n, representing the degree of trust in the angular alignment of each axis.\nIII-D\n1\nGlobal Flow Features\nWe employ a PointNet architecture\n[\n4\n]\nto process unordered flow vectors (see Fig.\n6\n) and extract global flow features. The feature dimension is expanded by processing each flow vector through a series of\n1\nÃ—\n1\n1\\times 1\nconvolutions. Each convolutional layer is followed by batch normalization and a ReLU activation to help stabilize and accelerate training. The output represents a global signature of the flow field illustrated as the Learned features box in Fig.\n4\n. To aggregate these features, maximum and mean pooling are applied. Maximum pooling captures dominant patterns, but it is sensitive to outliers. Mean pooling, on the other hand, provides a smoother, more robust representation. Combining the two mitigates the impact of local inaccuracies in the flow field, particularly in regions with dense objects where backward flows can introduce ambiguities. Unlike the original PointNet, we omit concatenating local and global features. This design choice reduces the number of model parameters and computational complexity because the model focuses on the global characteristics of the flow field for misalignment detection.\nIII-D\n2\nGeometric Features\nIn addition to learned global features, specific geometric features that are directly related to the sensorâ€™s misalignment state are extracted. Computing the global statistics of these features captures the overall effect of misalignment on the point cloud while mitigating the impact of outliers. Each feature is produced for every axis individually and then concatenated.\nFigure 6\n:\nAbove\n: This illustrates the flow fields of aligned and misaligned sensors.\nBelow\n: The resulting distribution of the angle\nÏˆ\n\\psi\nand cross value\nc\nc\nfor aligned and misaligned flow fields.\nMagnitude.\nThe flow magnitude for each point is computed as the Euclidean norm of its flow vector\nm\ni\n=\nâ€–\nğ®\ni\nâ€–\nâˆˆ\nâ„\nm_{i}=\\left\\|\\mathbf{u}_{i}\\right\\|\\in\\mathbb{R}\n. The resulting vector\nğ¦\nâˆˆ\nâ„\nN\n\\mathbf{m}\\in\\mathbb{R}^{N}\ncaptures the overall motion length of each point in\nğ—\ni\n\\mathbf{X}_{i}\nand implicitly reflects the expected impact of misalignment.\nSensor misalignment introduces an additional lateral offset in the flow field, which is proportional to the forward ego-motion. Therefore, the greater the ego-vehicleâ€™s forward movement, the more pronounced the effect of misalignment on the flow magnitude. To capture the global effect, the mean\nÎ¼\nğ¦\n\\mu_{\\mathbf{m}}\nand standard deviation\nÏƒ\nğ¦\n\\sigma_{\\mathbf{m}}\nof\nğ¦\n\\mathbf{m}\nare taken into consideration.\nAngle.\nThe angle between each flow vectorâ€™s\ny\ny\nz\nz\n,\nx\nx\nz\nz\n, and\nx\nx\ny\ny\naxes provides a direct measure of the sensorâ€™s offset relative to the vehicleâ€™s forward movement. For each flow vector the angle\nÏˆ\ni\n=\narctan\nâ¡\nu\nd\nn\n,\ni\nu\nd\nm\n,\ni\n\\psi_{i}=\\arctan{\\frac{u_{d_{n},i}}{u_{d_{m},i}}}\nis calculated. With\nd\nd\nrepresenting the respective axis for pitch, roll, and yaw.\nThe angles are aggregated into a histogram\nğ¡\nn\nb\nâ€‹\ni\nâ€‹\nn\nâ€‹\ns\nâˆˆ\nâ„\nn\nb\nâ€‹\ni\nâ€‹\nn\nâ€‹\ns\n\\mathbf{h}_{n_{bins}}\\in\\mathbb{R}^{{n_{bins}}}\n, providing a comprehensive representation of the angle distribution (Fig.\n6\n). Therefore, histogram-based angular features are used. This enables the model to learn the characteristic bias introduced by the misalignment angle\nÎ±\ne\nâ€‹\nr\nâ€‹\nr\n\\alpha_{err}\n. The number of bins\nn\nb\nâ€‹\ni\nâ€‹\nn\nâ€‹\ns\nn_{bins}\nis a hyperparameter that controls the histogramâ€™s resolution.\nRotation.\nTo capture rotational effects, the cross product between each pointâ€™s radial position relative to the ego-vehicle\nP\nV\ni\nâ€‹\n(\nt\n0\n)\nd\nn\nâ€‹\nd\nm\n,\ni\n{}^{V_{i}}{P(t_{0})_{d_{n}d_{m},i}}\nand its flow vector\nğ®\nt\nâ†’\nt\n+\n1\n,\ni\nV\ni\n{}^{V_{i}}{\\mathbf{u}_{t\\to t+1,i}}\nis computed as defined in Eq.\n3\n. The distribution of\nğœ\nğ\nğ§\nâ€‹\nğ\nğ¦\nâˆˆ\nâ„\nN\n\\mathbf{c_{d_{n}d_{m}}}\\in\\mathbb{R}^{N}\nvalues provides a global descriptor of the detected movement. Since misalignment introduces a systematic offset in the flow field, the model uses the mean and standard deviation to learn the effects of the induced bias. This is important as the sensor misalignment introduces a global offset to the flow field, which is captured by the distribution of\nğœ\nd\nn\nâ€‹\nd\nm\n{\\mathbf{c}_{d_{n}d_{m}}}\n.\nc\nd\nn\nâ€‹\nd\nm\n,\ni\n=\nğ©\nV\ni\nâ€‹\n(\nt\n0\n)\nd\nn\nâ€‹\nd\nm\n,\ni\nÃ—\nğ®\nt\n0\nâ†’\nt\n1\n,\ni\nV\ni\n=\n|\nğ©\nV\ni\nâ€‹\n(\nt\n0\n)\nd\nn\nâ€‹\nd\nm\n,\ni\n|\nâ€‹\n|\nğ®\nt\n0\nâ†’\nt\n1\n,\ni\nV\ni\n|\nâ€‹\nsin\nâ¡\n(\nÏ•\n)\n\\begin{split}{c_{d_{n}d_{m},i}}&={}^{V_{i}}{\\mathbf{p}(t_{0})_{d_{n}d_{m},i}}\\times{}^{V_{i}}{\\mathbf{u}_{t_{0}\\to t_{1},i}}\\\\\n&=|{}^{V_{i}}{\\mathbf{p}(t_{0})_{d_{n}d_{m},i}}|\\,|{}^{V_{i}}{\\mathbf{u}_{t_{0}\\to t_{1},i}}|\\sin(\\phi)\\end{split}\n(3)\nThe distribution is illustrated in Fig\n6\n. The resulting geometrical features are concatenated, representing a per-sample\n21\n+\n3\nâˆ—\nn\nb\nâ€‹\ni\nâ€‹\nn\nâ€‹\ns\n21+3*n_{bins}\ndimensional feature vector\nğŸ\ng\nâ€‹\ne\nâ€‹\no\nâ€‹\nm\nâˆˆ\nâ„\n21\n+\n3\nâˆ—\nn\nb\nâ€‹\ni\nâ€‹\nn\nâ€‹\ns\n\\mathbf{f}_{geom}\\in\\mathbb{R}^{21+3*n_{bins}}\n. This vector is fed through a three-block MLP, each block consisting of a linear layer, batch normalization, and ReLU activation. The dimensions of the linear layers are 256, 128, and 128. The result is the embedded feature vector\nğŸ\ne\nâ€‹\nm\nâ€‹\nb\n,\ng\nâ€‹\ne\nâ€‹\no\nâ€‹\nm\nâˆˆ\nâ„\n128\n\\mathbf{f}_{emb,geom}\\in\\mathbb{R}^{128}\n.\nThe global flow and geometric features are concatenated and passed through additional MLPs. Two detection heads are implemented, each consisting of a linear layer, an activation function, and an additional linear layer. The head responsible for miscalibration detection has an output dimension of three, which allows it to relate the prediction to pitch, roll, and yaw. A value close to 1 indicates high confidence in misalignment, and a value close to 0 indicates alignment.\nIII-E\nImplementation Details\nFlowCalib is implemented in pytorch\n[\n15\n]\n. ReLU is used as the activation function. The loss is represented by a Binary Cross Entropy (BCE) with logits. This implementation allows for more stable and integrated usage of ReLU and BCE than separate usage.\nAdamW\n[\n13\n]\nis used as the optimizer with a learning rate of\n8\nâ€‹\ne\nâˆ’\n3\n8e-3\n. The optimizer uses a weight decay of\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\n. To maintain a low dimension of the geometric encoders while providing a detailed picture,\nn\nb\nâ€‹\ni\nâ€‹\nn\nâ€‹\ns\nn_{bins}\nis set to 72, leading to a resolution of\n5\nâˆ˜\n5^{\\circ}\n.\nGlobal features are encoded by a two-block encoder that consisting of a\n1\nÃ—\n1\n1\\times 1\nconvolution, a 1D batch normalization, and an activation function. The channel sizes used are 64 and 128. The geometric feature encoder uses a linear layer instead of convolutions. The layers have depths of 256, 128, and 128. The decoder is a block consisting of a linear layer, an activation function, and an additional linear layer that reduces the output to 1 or 3, respectively.\nIV\nRESULTS & DISCUSSION\nThis section presents the performance of global misalignment detection, as well as the modelâ€™s ability to verify proper sensor alignment. It also presents the detection performance on the individual axes.\nIV-A\nGlobal Alignment Status\nTable\nI\nreports the detection capabilities, categorized based on the severity of distortion. The models achieve an overall accuracy of 81.16 %. Angular miscalibrations in the range of\n[\nÂ±\n5\n,\nÂ±\n2\n]\nâˆ˜\n[\\pm 5,\\pm 2]^{\\circ}\ncan be detected with high accuracy of 90.27%. The classification performance drops to 73.81% for angular miscalibrations in the range of\n[\nÂ±\n2\n,\nÂ±\n0.5\n]\nâˆ˜\n[\\pm 2,\\pm 0.5]^{\\circ}\n, showing comparable accuracy in the Medium and Hard distortion severity levels.\nCategory\nRot. Error\nCorrect\nIncorrect\nCorrect in %\nAligned\n[\nâˆ’\n0.4\n,\n0.4\n]\nâˆ˜\n[-0.4,0.4]^{\\circ}\n527\n187\n73.81\nHard\n(\nÂ±\n1\n,\nÂ±\n0.5\n]\nâˆ˜\n(\\pm 1,\\pm 0.5]^{\\circ}\n173\n44\n79.72\nMedium\n(\nÂ±\n2\n,\nÂ±\n1\n]\nâˆ˜\n(\\pm 2,\\pm 1]^{\\circ}\n87\n22\n79.82\nEasy\n[\nÂ±\n5\n,\nÂ±\n2\n]\nâˆ˜\n[\\pm 5,\\pm 2]^{\\circ}\n566\n61\n90.27\nTotal\n1353\n314\n81.16\nTABLE I\n:\nAnalysis of the global classification performance on different levels of severity. Note that hard refers to errors with only slight angular velocities that are harder to detect.\nTo better understand the performance drop, Table\nII\nreports the modelâ€™s true-positive detection performance for global misalignment at the specific rotation axes where the error is introduced.\nIn other words, the table shows for which samples the model struggles to detect a miscalibration.\nTable\nII\nshows the number of samples in the evaluation dataset for specific combinations of angular miscalibration. The analysis concludes that the model has difficulty classifying misalignments arising from pitch angle rotations. Only 24.73% of samples are classified correctly when miscalibration occurs in that dimension. The share of correctly classified samples for combinations in which another angular miscalibration is present becomes much higher. When all three angles are disturbed, a high detection rate of 94.70% can be achieved. One possible reason is the strong overall distortion caused by misalignment in three dimensions.\nCategory\nCorrect\nSamples\nIncorrect\nSamples\nShare correct\nin %\nÏ•\n\\phi\n104\n8\n92.86\nÎ¸\n\\theta\n23\n70\n24.73\nÏˆ\n\\psi\n112\n5\n95.73\nÏ•\n\\phi\n&\nÎ¸\n\\theta\n92\n8\n92.00\nÏ•\n\\phi\n&\nÏˆ\n\\psi\n102\n7\n93.58\nÎ¸\n\\theta\n&\nÏˆ\n\\psi\n89\n12\n88.12\nÏ•\n\\phi\n&\nÎ¸\n\\theta\n&\nÏˆ\n\\psi\n304\n17\n94.70\nTABLE II\n:\nClassification results in the presence of different\nÏ•\n\\phi\n(Roll),\nÎ¸\n\\theta\n(Pitch),\nÏˆ\n\\psi\n(Yaw) error combinations.\nAlthough the model does not perform well in every dimension, its ability to detect misalignment is evident. The model has difficulty predicting deviations in the pitch angle in particular. There are several possible reasons for this behavior. Distortions in the pitch angle lead to uniform up- or down-movements. Yaw angles vary more distinctively relative to the vehicle depending on the position of the point. For example, a point on the front of the vehicle is shifted left or right, while a point on the side of the vehicle moves forward or backward. Similarly, the roll error effect depends on the position of the point. These variations create distinctive patterns that the model can detect. However, as this is not the case for pitch perturbations, it becomes more difficult for the model to detect them.\nIV-B\nRotated Axis detection\nIn the following, the modelâ€™s ability to detect the distorted axis, as represented by the output of the axis head, is analyzed.\nAs shown in Table\nIII\n, across all data samples, the model correctly detects present rotations around the pitch axis in 60.81% of the cases. Pitch misalignment is more difficult to detect because it produces a very uniform motion pattern across the entire scene.\nIn contrast, the model performs better at identifying present rotations around the roll and yaw axes, achieving accuracies of 87.04 % and 76.06 %, respectively.\nAxis\nAccuracy\nPrecision\nRecall\nÏ•\n\\phi\n87.04\n86.60\n78.50\nÎ¸\n\\theta\n60.81\n55.87\n73.50\nÏˆ\n\\psi\n76.06\n63.82\n88.73\nTABLE III\n:\nAxis-specific detection results in the presence of different\nÏ•\n\\phi\n(Roll),\nÎ¸\n\\theta\n(Pitch),\nÏˆ\n\\psi\n(Yaw) error combinations.\nThe model reliably detects misalignment and performs particularly well when distortions span multiple angles. In such instances, larger deviations across several dimensions generate clearer misalignment patterns in the flow field, which the model effectively leverages. The model also shows promise in identifying the axis of decalibration.\nV\nCONCLUSION\nIn this work, we introduced FlowCalib, the first framework for detecting LiDAR-to-vehicle miscalibration by leveraging scene flow patterns of static objects. FlowCalib provides a novel and lightweight alternative to existing calibration or miscalibration-detection approaches. We exploit point-wise motion cues without relying on additional sensors or strict geometric priors. Our results on the nuScenes dataset demonstrate that flow-based cues contain meaningful signatures of angular misalignment and that these can be learned to reliably identify global and axis-specific calibration errors. In particular, FlowCalib achieves strong performance in detecting yaw and roll miscalibration, with consistently high detection rates when multiple axes are jointly perturbed. These findings confirm that motion-induced distortions in the flow field provide a robust and interpretable basis for identifying extrinsic sensor calibration errors during autonomous vehicle operation.\nReferences\n[1]\nP. An, J. Ding, S. Quan, J. Yang, Y. Yang, Q. Liu, and J. Ma\n(2024-07)\nSurvey of extrinsic calibration on lidar-camera system for intelligent vehicle: challenges, approaches, and trends\n.\nIEEE Transactions on Intelligent Transportation Systems\n,\npp.Â 1â€“25\n.\nExternal Links:\nDocument\n,\nISSN 1524-9050\nCited by:\nÂ§I\n.\n[2]\nX. Bai\net al.\n(2022)\nTransFusion: robust lidar-camera fusion for 3d object detection with transformers\n.\nIn\n2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\nÂ§II\n.\n[3]\nH. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom\n(2020)\nNuScenes: a multimodal dataset for autonomous driving\n.\nIn\nCVPR\n,\nCited by:\nÂ§II\n,\nÂ§\nIII-A\n,\nÂ§\nIII-B\n3\n.\n[4]\nR. Q. Charles, H. Su, M. Kaichun, and L. J. Guibas\n(2017)\nPointNet: deep learning on point sets for 3d classification and segmentation\n.\nIn\n2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nVol.\n,\npp.Â 77â€“85\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nIII-D\n1\n.\n[5]\nY. Dong, C. Kang, J. Zhang, Z. Zhu, Y. Wang, X. Yang, H. Su, X. Wei, and J. Zhu\n(2023)\nBenchmarking robustness of 3d object detection to common corruptions in autonomous driving\n.\nIn\n2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nVol.\n,\npp.Â 1022â€“1032\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nIII-A\n.\n[6]\nM. A. Fischler and R. C. Bolles\n(1987)\nRandom sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography\n.\nIn\nReadings in Computer Vision\n,\nM. A. Fischler and O. Firschein (Eds.)\n,\npp.Â 726â€“740\n.\nExternal Links:\nISBN 978-0-08-051581-6\n,\nDocument\nCited by:\nÂ§\nIII-B\n1\n.\n[7]\nI. Ganesh, R. K. Ram, K. Murthy, and K. M. Kirshna\n(2018)\nCalibNet: geometrically supervised extrinsic calibration using 3d spatial transformer networks\n.\nIn\n2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nExternal Links:\nISBN 9781538680940\nCited by:\nÂ§\nII-\n1\n.\n[8]\nD. Kulmer, I. Tahiraj, A. Chumak, and M. Lienkamp\n(2024)\nMulti-lica: a motion- and targetless multi - lidar-to-lidar calibration framework\n.\nIn\nIEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems\n,\nExternal Links:\nDocument\n,\nISBN 9798350368031\n,\nISSN 27679357\nCited by:\nÂ§\nII-\n2\n.\n[9]\nT. Lentsch, H. Caesar, and D. M. Gavrila\n(2024)\nUNION: unsupervised 3D object detection using object appearance-based pseudo-classes\n.\nVol.\n37\n.\nCited by:\nÂ§\nIII-B\n1\n.\n[10]\nJ. Levinson and S. Thrun\n(2013)\nAutomatic online calibration of cameras and lasers\n.\nIn\nRobotics: Science and Systems\n,\nCited by:\nÂ§\nII-\n1\n.\n[11]\nX. Li, J. K. Pontes, and S. Lucey\n(2021)\nNeural scene flow prior\n.\nIn\nProceedings of the 35th International Conference on Neural Information Processing Systems\n,\nNIPS â€™21\n,\nRed Hook, NY, USA\n.\nExternal Links:\nISBN 9781713845393\nCited by:\nÂ§\nIII-B\n1\n,\nÂ§\nIII-B\n3\n,\nÂ§\nIII-C\n,\nÂ§\nIII-C\n.\n[12]\nZ. Liu\net al.\n(2023)\nBEVFusion: multi-task multi-sensor fusion with unified birdâ€™s-eye view representation\n.\nIn\n2023 IEEE International Conference on Robotics and Automation (ICRA)\n,\nCited by:\nÂ§II\n.\n[13]\nI. Loshchilov and F. Hutter\n(2019)\nDecoupled weight decay regularization\n.\nExternal Links:\n1711.05101\nCited by:\nÂ§\nIII-E\n.\n[14]\nS. W. Meyer, H. Chen, and D. M. Bevly\n(2021)\nAutomatic extrinsic rotational calibration of lidar sensors and vehicle orientation estimation\n.\nIFAC-PapersOnLine\n54\n(\n20\n),\npp.Â 424â€“429\n.\nNote:\nModeling, Estimation and Control Conference MECC 2021\nExternal Links:\nISSN 2405-8963\n,\nDocument\nCited by:\nÂ§III\n.\n[15]\nA. Paszke\net al.\n(2019)\nPyTorch: an imperative style, high-performance deep learning library\n.\nIn\nAdvances in Neural Information Processing Systems 32\n,\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett (Eds.)\n,\npp.Â 8024â€“8035\n.\nCited by:\nÂ§\nIII-E\n.\n[16]\nN. Schneider, F. Piewak, C. Stiller, and U. Franke\n(2017)\nRegNet: multimodal sensor registration using deep neural networks\n.\nIn\n2017 IEEE Intelligent Vehicles Symposium (IV)\n,\npp.Â 1927\n.\nExternal Links:\nISBN 9781509048045\nCited by:\nÂ§\nII-\n1\n.\n[17]\nJ. Seok, C. Kim, P. Resende, B. Bradai, and K. Jo\n(2024)\nFault detection and exclusion for robust online calibration of vehicle to lidar rotation parameter\n.\nIEEE Transactions on Intelligent Vehicles\n(\n),\npp.Â 1â€“10\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n2\n,\nÂ§III\n.\n[18]\nI. Tahiraj, M. Edinger, D. Kulmer, and M. Lienkamp\n(2025)\nCaLiV: lidar-to-vehicle calibration of arbitrary sensor setups\n.\nCited by:\nÂ§\nII-\n2\n.\n[19]\nI. Tahiraj\net al.\n(2025)\nCal or No Cal? â€“ Real-Time Miscalibration Detection of LiDAR and Camera Sensors\n.\nIn\nIEEE International Conference on Intelligent Robots and Systems (IROS)\n,\nCited by:\nÂ§\nII-\n1\n.\n[20]\nG. Wang, J. Qiu, and Y. Guo\n(2022)\nFusionNet: coarse-to-fine extrinsic calibration network of lidar and camera with hierarchical point-pixel fusion\n.\nIn\nIEEE International Conference on Robotics and Automation (ICRA)\n,\nExternal Links:\nISBN 9781728196800\nCited by:\nÂ§\nII-\n1\n.\n[21]\nP. Wei\net al.\n(2024-10)\nOnline lidar-camera extrinsic parameters self-checking and recalibration\n.\nMeasurement Science and Technology\n35\n.\nExternal Links:\nDocument\n,\nISSN 13616501\nCited by:\nÂ§\nII-\n1\n.\n[22]\nY. Xie\net al.\n(2023)\nSparseFusion: fusing multi-modal sparse representations for multi-sensor 3d object detection\n.\nIn\nICCV\n,\nCited by:\nÂ§II\n.\n[23]\nG. Yan, Z. Luo, Z. Liu, Y. Li, B. Shi, and K. Zhang\n(2024)\nSensorX2Vehicle: online sensors-to-vehicle rotation calibration methods in road scenarios\n.\nIEEE Robotics and Automation Letters\n9\n(\n4\n),\npp.Â 3775â€“3782\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n2\n,\nÂ§III\n.\n[24]\nZ. Yang\net al.\n(2022)\nDeepInteraction: 3d object detection via modality interaction\n.\nIn\n2022 Conference on Neural Information Processing Systems (NeurIPS)\n,\nCited by:\nÂ§II\n.\n[25]\nK. Yuan, Z. Guo, and Z. J. Wang\n(2020-10)\nRGGNet: tolerance aware lidar-camera online calibration with geometric deep learning and generative model\n.\nIEEE Robotics and Automation Letters\n5\n,\npp.Â 6956â€“6963\n.\nExternal Links:\nDocument\n,\nISSN 23773766\nCited by:\nÂ§\nII-\n1\n.",
    "preview_text": "Accurate sensor-to-vehicle calibration is essential for safe autonomous driving. Angular misalignments of LiDAR sensors can lead to safety-critical issues during autonomous operation. However, current methods primarily focus on correcting sensor-to-sensor errors without considering the miscalibration of individual sensors that cause these errors in the first place. We introduce FlowCalib, the first framework that detects LiDAR-to-vehicle miscalibration using motion cues from the scene flow of static objects. Our approach leverages the systematic bias induced by rotational misalignment in the flow field generated from sequential 3D point clouds, eliminating the need for additional sensors. The architecture integrates a neural scene flow prior for flow estimation and incorporates a dual-branch detection network that fuses learned global flow features with handcrafted geometric descriptors. These combined representations allow the system to perform two complementary binary classification tasks: a global binary decision indicating whether misalignment is present and separate, axis-specific binary decisions indicating whether each rotational axis is misaligned. Experiments on the nuScenes dataset demonstrate FlowCalib's ability to robustly detect miscalibration, establishing a benchmark for sensor-to-vehicle miscalibration detection.\n\nFlowCalib: LiDAR-to-Vehicle Miscalibration Detection using Scene Flows\nIlir Tahiraj\n1,âˆ—\n, Peter Wittal\n2\n, Markus Lienkamp\n1\nâˆ—\nCorresponding author\nilir.tahiraj@tum.de\n1\nAuthors are with the TUM School of Engineering and Design, Chair of Automotive Technology, Technical University of Munich.\n2\nis with the TUM School of Computation, Information and Technology, Technical University of Munich.\nAbstract\nAccurate sensor-to-vehicle calibration is essential for safe autonomous driving. Angular misalignments of LiDAR sensors can lead to safety-critical issues during autonomous operation. However, current methods primarily focus on correcting sensor",
    "is_relevant": false,
    "relevance_score": 0.0,
    "extracted_keywords": [],
    "one_line_summary": "FlowCalibæ˜¯ä¸€ç§åŸºäºåœºæ™¯æµæ£€æµ‹LiDARä¸è½¦è¾†é—´æ ¡å‡†è¯¯å·®çš„æ–¹æ³•ï¼Œä¸“æ³¨äºè‡ªåŠ¨é©¾é©¶ä¸­çš„ä¼ æ„Ÿå™¨æ ¡å‡†é—®é¢˜ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T15:53:16Z",
    "created_at": "2026-02-03T15:53:10.839592",
    "updated_at": "2026-02-03T15:53:10.839598"
}