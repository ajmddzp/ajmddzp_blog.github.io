{
    "id": "2601.14133v1",
    "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
    "authors": [
        "Bin Yu",
        "Shijie Lian",
        "Xiaopeng Lin",
        "Yuliang Wei",
        "Zhaolong Shen",
        "Changti Wu",
        "Yuzhuo Miao",
        "Xinming Wang",
        "Bailing Wang",
        "Cong Huang",
        "Kai Chen"
    ],
    "abstract": "æ ‡å‡†è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹é€šå¸¸é€šè¿‡æ˜¾å¼å¾®è°ƒå•ä¸€è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸»å¹²ç½‘ç»œæ¥å®ç°æœºå™¨äººæ§åˆ¶ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åœ¨ä¿æŒé«˜å±‚é€šç”¨è¯­ä¹‰ç†è§£ä¸å­¦ä¹ ä½å±‚ç»†ç²’åº¦æ„ŸçŸ¥è¿åŠ¨æŠ€èƒ½ä¹‹é—´å½¢æˆäº†å…³é”®çŸ›ç›¾ï¼Œå¾€å¾€å¯¼è‡´æ¨¡å‹å¯¹å¼€æ”¾ä¸–ç•Œèƒ½åŠ›çš„\"ç¾éš¾æ€§é—å¿˜\"ã€‚ä¸ºè§£å†³è¿™ä¸€å†²çªï¼Œæˆ‘ä»¬æå‡ºTwinBrainVLAâ€”â€”ä¸€ç§åè°ƒé€šç”¨å‹VLMï¼ˆä¿æŒæ™®é€‚è¯­ä¹‰ç†è§£ï¼‰ä¸ä¸“ç”¨å‹VLMï¼ˆä¸“æ³¨å…·èº«æœ¬ä½“æ„ŸçŸ¥ï¼‰è¿›è¡Œè”åˆæœºå™¨äººæ§åˆ¶çš„æ–°å‹æ¶æ„ã€‚è¯¥æ¶æ„é€šè¿‡æ–°é¢–çš„éå¯¹ç§°æ··åˆå˜æ¢å™¨ï¼ˆAsyMoTï¼‰æœºåˆ¶ï¼Œå°†ä¿æŒå¼ºå¤§é€šç”¨è§†è§‰æ¨ç†èƒ½åŠ›çš„å†»ç»“\"å·¦è„‘\"ä¸ä¸“ç²¾å…·èº«æ„ŸçŸ¥çš„å¯è®­ç»ƒ\"å³è„‘\"ç›¸ååŒã€‚è¿™ç§è®¾è®¡ä½¿å³è„‘èƒ½å¤ŸåŠ¨æ€æŸ¥è¯¢å†»ç»“å·¦è„‘çš„è¯­ä¹‰çŸ¥è¯†ï¼Œå¹¶å°†å…¶ä¸æœ¬ä½“æ„ŸçŸ¥çŠ¶æ€èåˆï¼Œä¸ºæµåŒ¹é…åŠ¨ä½œä¸“å®¶ç”Ÿæˆç²¾ç¡®è¿ç»­æ§åˆ¶æä¾›ä¸°å¯Œæ¡ä»¶ã€‚åœ¨SimplerEnvå’ŒRoboCasaåŸºå‡†æµ‹è¯•ä¸­çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTwinBrainVLAåœ¨å®ç°å“è¶Šæ“ä½œæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜ç¡®ä¿ç•™äº†é¢„è®­ç»ƒVLMçš„å…¨é¢è§†è§‰ç†è§£èƒ½åŠ›ï¼Œä¸ºæ„å»ºåŒæ—¶å…·å¤‡é«˜å±‚è¯­ä¹‰ç†è§£å’Œä½å±‚ç‰©ç†æ“æ§èƒ½åŠ›çš„é€šç”¨æœºå™¨äººæä¾›äº†å‰æ™¯å¹¿é˜”çš„ç ”ç©¶æ–¹å‘ã€‚",
    "url": "https://arxiv.org/abs/2601.14133v1",
    "html_url": "https://arxiv.org/html/2601.14133v1",
    "html_content": "TwinBrainVLA\n: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers\nBin Yu\n1,2,\nShijie Lian\n2,4,\n1\n1\nfootnotemark:\n1\nXiaopeng Lin\n2,5,\n1\n1\nfootnotemark:\n1\nYuliang Wei\n1,\nZhaolong Shen\n2,6\nChangti Wu\n2,7\nYuzhuo Miao\n1,2\nXinming Wang\n2,8\nBailing Wang\n1\nCong Huang\n2,3\nKai Chen\n2,3,9,\n2\n2\nfootnotemark:\n2\n1\nHIT\n2\nZGCA\n3\nZGCI\n4\nHUST\n5\nHKUST(GZ)\n6\nBUAA\n7\nECNU\n8\nCASIA\n9\nDeepCybo\nEqual contributionCorresponding author\nAbstract\nStandard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the modelâ€™s open-world capabilities. To resolve this conflict, we introduce\nTwinBrainVLA\n, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control.\nTwinBrainVLA\nsynergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (\nAsyMoT\n) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that\nTwinBrainVLA\nachieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.\nTwinBrainVLA\n: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers\nBin Yu\n1,2,\nâ€ \nâ€ \nthanks:\nEqual contribution\nShijie Lian\n2,4,\n1\n1\nfootnotemark:\n1\nXiaopeng Lin\n2,5,\n1\n1\nfootnotemark:\n1\nYuliang Wei\n1,\nâ€ \nâ€ \nthanks:\nCorresponding author\nZhaolong Shen\n2,6\nChangti Wu\n2,7\nYuzhuo Miao\n1,2\nXinming Wang\n2,8\nBailing Wang\n1\nCong Huang\n2,3\nKai Chen\n2,3,9,\n2\n2\nfootnotemark:\n2\n1\nHIT\n2\nZGCA\n3\nZGCI\n4\nHUST\n5\nHKUST(GZ)\n6\nBUAA\n7\nECNU\n8\nCASIA\n9\nDeepCybo\nhttps://github.com/ZGC-EmbodyAI/TwinBrainVLA\nâ€ \nâ€ \nfootnotetext:\nWork done at Zhongguancun Academy (Beijing).\n1\nIntroduction\nThe pursuit of embodied artificial intelligence has recently converged on the paradigm of Vision-Language-Action (VLA) models\n(Kim\net al.\n,\n2024\n; Intelligence\net al.\n,\n2025\n; NVIDIA\net al.\n,\n2025\n)\n. By grafting robotic control heads onto pre-trained Vision-Language Models (VLMs), these systems inherit the rich semantic reasoning and open-world generalization capabilities of large-scale internet data. This unified approach promises a robot that can not only recognize a \"coffee mug\" but also understand the instruction \"place the mug on the coaster\" and execute the corresponding motor commands.\nHowever, a fundamental tension lies at the heart of current VLA architectures: the\nmisalignment between the optimization objectives of semantic pre-training and robotic fine-tuning.\nPre-trained VLMs are originally optimized for general semantic understanding and dialogue. In contrast, during VLA fine-tuning, the backbone is repurposed to provide precise, spatially-grounded conditioning for the Action Expert to enable precise robotic control. Forcing a single VLM backbone to satisfy these divergent objectives simultaneously inevitably precipitates \"\ncatastrophic forgetting\n\"\n(Hancock\net al.\n,\n2025b\n; Zhang\net al.\n,\n2026\n; Zhou\net al.\n,\n2025\n; Fang\net al.\n,\n2025\n)\n, where the model sacrifices its general linguistic capabilities to adapt to the specialized demands of robotic actuation. More critically, this catastrophic forgetting undermines the fundamental premise of the VLA paradigm, which aims to leverage the general world understanding acquired during VLM pre-training to enable generalized robotic control. However, current fine-tuning methodologies paradoxically strip the VLM of this very capability.\nTo resolve this conflict, we draw inspiration from the biological principle of hemispheric lateralization, where the human brain allocates distinct cognitive functions to specialized hemispheresâ€”logic and language to the left, spatial attention and motor coordination to the right. We propose that VLA models should mimic this decoupling. By separating general semantic understanding from embodied action estimation, we cY-an achieve a system that is both cognitively capable and physically skillful.\nFigure 1:\nArchitectural comparison between Vanilla VLA and\nTwinBrainVLA\n.\nIn this paper, we introduce\nTwinBrainVLA\n, a novel VLA framework that orchestrates two isomorphic VLM pathways via an asymmetric dual-stream joint training strategy. Structurally, our model consists of a \"Left Brain\" and a \"Right Brain\". The Left Brain is a frozen, pre-trained VLM that retains robust open-world knowledge and instruction following capabilities. The Right Brain, initialized with the same architecture, is fully trainable and specialized for embodied perception. These two streams are seamlessly fused through a Asymmetric Mixture-of-Transformers (\nAsyMoT\n) mechanism, which allows for joint attention over hidden states without sharing parameters. Crucially, the specialized representations from the Right Brain serve as the primary condition for our action expert utilizing flow-matching for precise action generation. This design ensures that the action expert receives spatially rich, task-aligned embeddings from the Right Brain, while the Left Brain explicitly preserves the modelâ€™s general semantic understanding capabilities.\nOur contributions are threefold:\nâ€¢\nWe propose\nTwinBrainVLA\n, the first VLA architecture to explicitly decouple general semantic understanding and embodied perception via an asymmetric dual-stream design, resolving the training conflict inherent in single-backbone VLAs.\nâ€¢\nWe introduce the\nAsymmetric Mixture-of-Transformers (AsyMoT)\nmechanism for the information interaction of two isomorphic VLM pathways to build the VLA backbone, and employ an asymmetric parameter freezing strategy to achieve the joint training of the dual models.\nâ€¢\nExtensive comparative experiments and evaluations on the SimplerEnv and RoboCasa benchmarks, demonstrates the effectiveness of the\nTwinBrainVLA\narchitecture, the AsyMoT mechanism, and the proposed training strategy.\n2\nRelated Work\nMultimodal understanding.\nThe landscape of computer vision and natural language processing has been revolutionized by Vision Language Models (VLMs)\n(Liu\net al.\n,\n2023\n; Bai\net al.\n,\n2025b\n,\na\n)\n. By seamlessly integrating visual encoders with Large Language Models (LLMs) through sophisticated projection layers or adapter modules\n(Jiang\net al.\n,\n2025a\n)\n, these models exhibit emergent abilities in holistic scene understanding, visual question answering (VQA), and complex math reasoning. Given the inherent limitations of general-purpose VLMs in spatial perception\n(Xiao\net al.\n,\n2026\n; Dang\net al.\n,\n2025\n; Zhang\net al.\n,\n2025b\n; Chen\net al.\n,\n2025\n; Yang\net al.\n,\n2025c\n)\n, recent studies have increasingly focused on employing post-training strategies to enhance spatial intelligence\n(Yang\net al.\n,\n2025a\n; Tan\net al.\n,\n2025\n; Zhou\net al.\n,\n2026\n; Zhu\net al.\n,\n2025\n; Li\net al.\n,\n2025\n)\nand construct specialized embodied foundation models tailored for embodied scenarios\n(Team\net al.\n,\n2025a\n; Yang\net al.\n,\n2025a\n; Hao\net al.\n,\n2025\n; Lin\net al.\n,\n2025c\n)\n.\nVision-Language-Action (VLA) Models.\nBuilding upon the zero-shot generalization and rich semantic priors of VLMs, VLA models have emerged as a scalable paradigm for embodied intelligence\nPertsch\net al.\n(\n2025\n); Zheng\net al.\n(\n2025a\n); Wen\net al.\n(\n2025\n); Shukor\net al.\n(\n2025\n); Zheng\net al.\n(\n2025b\n); Lin\net al.\n(\n2025b\n); Cai\net al.\n(\n2026\n)\n. By fine-tuning pre-trained VLMs on large-scale robotic manipulation datasets, these models learn to map multimodal instructions and observations to low-level control signals\n(Jiang\net al.\n,\n2025b\n; Fan\net al.\n,\n2025\n; Zhai\net al.\n,\n2025\n)\n. To mitigate the degradation of general conversational skills during VLA training, recent approaches have integrated general visual-dialogue datasets\n(Mazzaglia\net al.\n,\n2025\n; Yang\net al.\n,\n2025b\n; Hancock\net al.\n,\n2025a\n; Zhou\net al.\n,\n2025\n)\nand reasoning mechanisms into the VLA framework\n(Zawalski\net al.\n,\n2025\n; Yin\net al.\n,\n2025\n; Lin\net al.\n,\n2025a\n; Qu\net al.\n,\n2025a\n; Ye\net al.\n,\n2025\n; Huang\net al.\n,\n2025\n; Guo\net al.\n,\n2025\n; Lee\net al.\n,\n2025\n; Huang\net al.\n,\n2026\n)\n.\n3\nMotivation: Catastrophic Forgetting in VLA Models\nStandard Vision-Language-Action (VLA) models typically adopt a transfer learning paradigm. Formally, let a pre-trained Vision-Language Model (VLM) be parameterized by\nÎ¸\npre\n\\theta_{\\text{pre}}\n, which has been aligned on massive internet-scale image-text pairs to maximize the likelihood of generating semantic text responses\ny\ny\ngiven an image\nv\nv\nand instruction\nx\nx\n. To adapt this backbone for robotic manipulation, the model is fine-tuned on a robotic demonstration dataset\nğ’Ÿ\nrobot\n=\n{\n(\nv\n,\nx\n,\na\n)\ni\n}\ni\n=\n1\nN\n\\mathcal{D}_{\\text{robot}}=\\{(v,x,a)_{i}\\}_{i=1}^{N}\n, where\na\na\nrepresents the robotâ€™s action (e.g., end-effector pose or joint angles). The training objective shifts to minimizing the action prediction loss\nâ„’\naction\n\\mathcal{L}_{\\text{action}}\n:\nÎ¸\nâˆ—\n=\narg\nâ¡\nmin\nÎ¸\nâ€‹\nâˆ‘\n(\nv\n,\nx\n,\na\n)\nâˆˆ\nğ’Ÿ\nrobot\nâˆ’\nlog\nâ¡\nP\nâ€‹\n(\na\n|\nv\n,\nx\n;\nÎ¸\n)\n,\n\\theta^{*}=\\arg\\min_{\\theta}\\sum_{(v,x,a)\\in\\mathcal{D}_{\\text{robot}}}-\\log P(a|v,x;\\theta),\n(1)\ninitialized with\nÎ¸\n=\nÎ¸\npre\n\\theta=\\theta_{\\text{pre}}\n.\nWhile this process effectively imparts manipulation skillsâ€”enabling the model to control servos and execute tasksâ€”it introduces a critical side effect known as\ncatastrophic forgetting\n. The distribution of\nğ’Ÿ\nrobot\n\\mathcal{D}_{\\text{robot}}\ndiffers fundamentally from the diverse visual-linguistic data seen during pre-training\n(Kachaev\net al.\n,\n2025\n; Hancock\net al.\n,\n2025a\n; Zhang\net al.\n,\n2025a\n)\n. As the parameters\nÎ¸\n\\theta\nare updated to minimize the high-frequency spatial error in action space, the delicate semantic alignment established in\nÎ¸\npre\n\\theta_{\\text{pre}}\nis often overwritten. Consequently, the resulting VLA model suffers from a severe degradation in its original capabilities: it loses the ability to follow complex open-ended instructions, fails to describe visual scenes accurately, and exhibits weakened general semantic reasoning. This phenomenon turns the VLM backbone from a \"generalist\" into a \"specialist\", sacrificing its linguistic brain to gain a robotic body.\n4\nMethod:\nTwinBrainVLA\nIn this section, we present\nTwinBrainVLA\n, a unified VLA framework designed to disentangle high-level semantic reasoning from fine-grained sensorimotor control. As illustrated in Figure\n2\n, our architecture operates on an asymmetric dual-stream paradigm to resolve the \"catastrophic forgetting\" dilemma. The framework comprises three core components: (1) a frozen\n\"Left Brain\"\nthat preserves open-world visual-linguistic knowledge; (2) a trainable\n\"Right Brain\"\nthat specializes in embodied motor control; and (3) a\nFlow-Matching Action Expert\nthat generates precise continuous actions. The two brains interact via a novel\nAsymmetric Mixture-of-Transformers (AsyMoT)\nmechanism, enabling the specialist to leverage generalist semantics without corrupting them. We detail the Dual-VLM architecture and AsyMoT fusion strategy in Sec.\n4.1\n, describe the flow-matching policy formulation in Sec.\n4.2\n, and finally present our joint-training objective in Sec.\n4.3\n.\nFigure 2:\nThe framework of\nTwinBrainVLA\n.\n(a) Overall Architecture.\nThe model features an Asymmetric Mixture-of-Transformers design composed of two distinct pathways: a frozen \"Left Brain\" (Generalist) for semantic reasoning and a trainable \"Right Brain\" (Specialist) for embodied motor control. The Right Brain fuses visual, textual, and proprioceptive state inputs to provide conditioning for the Action Expert, which utilizes a Flow-Matching algorithm to denoise continuous robotic actions.\n(b) Asymmetric MoT Mechanism (AsyMoT).\nThrough causal self-attention, the trainable Right Brain attends to the frozen Key-Value (KV) pairs of the Left Brain, enabling the transfer of general semantic knowledge to the robotic control policy without catastrophic forgetting.\n4.1\nAsymmetric Dual-VLM as Backbone\nThe backbone of\nTwinBrainVLA\nconsists of two parallel Vision-Language Models (VLMs), denoted as the \"Left Brain\" (\nâ„³\nL\n\\mathcal{M}_{L}\n) and the \"Right Brain\" (\nâ„³\nR\n\\mathcal{M}_{R}\n). Both streams are initialized with identical pre-trained weights (e.g., Qwen3-VL series\n(Bai\net al.\n,\n2025a\n)\n) but serve distinct roles during training.\nDual-Stream Inputs and State Encoding.\nThe input to the system comprises a visual observation\nğˆ\n\\mathbf{I}\n, a natural language instruction\nğ“\n\\mathbf{T}\n, and the robotâ€™s proprioceptive state\nğ¬\nâˆˆ\nâ„\nd\ns\n\\mathbf{s}\\in\\mathbb{R}^{d_{s}}\n(e.g., joint angles and end-effector pose).\nThe\nLeft Brain\nfunctions as a pure semantic generalist. It receives only the vision and language tokens:\nğ‡\nL\n0\n=\n[\nğ’±\nâ€‹\n(\nğˆ\n)\n;\nğ’¯\nâ€‹\n(\nğ“\n)\n]\n,\n\\mathbf{H}_{L}^{0}=[\\mathcal{V}(\\mathbf{I});\\mathcal{T}(\\mathbf{T})],\n(2)\nwhere\nğ’±\n\\mathcal{V}\nand\nğ’¯\n\\mathcal{T}\nrepresent the vision encoder and text tokenizer, respectively.\nIn contrast, the\nRight Brain\nis responsible for embodied control and must be grounded in the robotâ€™s physical state. To achieve this, we introduce a lightweight\nState Encoder\nÏ•\n\\phi\n, modeled as a Multi-Layer Perceptron (MLP), which projects the low-level proprioceptive state\nğ¬\n\\mathbf{s}\ninto the VLMâ€™s embedding space. These state tokens are appended to the multimodal input sequence:\nğ‡\nR\n0\n=\n[\nğ’±\nâ€‹\n(\nğˆ\n)\n;\nğ’¯\nâ€‹\n(\nğ“\n)\n;\nÏ•\nâ€‹\n(\nğ¬\n)\n]\n.\n\\mathbf{H}_{R}^{0}=[\\mathcal{V}(\\mathbf{I});\\mathcal{T}(\\mathbf{T});\\phi(\\mathbf{s})].\n(3)\nThis design ensures that the Right Brain explicitly attends to the robotâ€™s current configuration, a critical requirement for closed-loop control.\nAsymmetric Mixture-of-Transformers (AsyMoT).\nThe core innovation of our backbone lies in how these two streams interact. We propose the AsyMoT mechanism to fuse general semantic knowledge into the robotic control stream.\nLet\nğ‡\nL\nl\n\\mathbf{H}_{L}^{l}\nand\nğ‡\nR\nl\n\\mathbf{H}_{R}^{l}\ndenote the hidden states of the Left and Right Brains at layer\nl\nl\n, respectively.\nThe Left Brain remains\nfrozen\nduring training. Its self-attention mechanism operates independently to preserve pre-trained general capabilities:\nğ‡\nL\nl\n+\n1\n=\nAttn\nâ€‹\n(\nQ\nL\nl\n,\nK\nL\nl\n,\nV\nL\nl\n)\n+\nFFN\nâ€‹\n(\nğ‡\nL\nl\n)\n,\n\\mathbf{H}_{L}^{l+1}=\\text{Attn}(Q_{L}^{l},K_{L}^{l},V_{L}^{l})+\\text{FFN}(\\mathbf{H}_{L}^{l}),\n(4)\nwhere\nQ\n,\nK\n,\nV\nQ,K,V\nare derived from\nğ‡\nL\nl\n\\mathbf{H}_{L}^{l}\nusing frozen projection weights.\nThe Right Brain is\ntrainable\nand employs an\nAsymmetric Joint Attention\nmechanism. It queries not only its own context but also the semantic features from the Left Brain. Specifically, at each attention head, the Right Brain computes its Query (\nQ\nR\nQ_{R}\n), while the Key (\nK\nK\n) and Value (\nV\nV\n) are constructed by concatenating the representations from both brains:\nK\njoint\n\\displaystyle K_{\\text{joint}}\n=\n[\nsg\nâ€‹\n(\nK\nL\nl\n)\n;\nK\nR\nl\n]\n,\n\\displaystyle=[\\text{sg}(K_{L}^{l})\\ ;\\ K_{R}^{l}],\n(5)\nV\njoint\n\\displaystyle V_{\\text{joint}}\n=\n[\nsg\nâ€‹\n(\nV\nL\nl\n)\n;\nV\nR\nl\n]\n,\n\\displaystyle=[\\text{sg}(V_{L}^{l})\\ ;\\ V_{R}^{l}],\n(6)\nğ‡\nR\nl\n+\n1\n\\displaystyle\\mathbf{H}_{R}^{l+1}\n=\nSoftmax\nâ€‹\n(\nQ\nR\nl\nâ€‹\n(\nK\njoint\n)\nT\nd\nk\n)\nâ€‹\nV\njoint\n+\nFFN\nâ€‹\n(\nğ‡\nR\nl\n)\n,\n\\displaystyle=\\text{Softmax}\\left(\\frac{Q_{R}^{l}(K_{\\text{joint}})^{T}}{\\sqrt{d_{k}}}\\right)V_{\\text{joint}}+\\text{FFN}(\\mathbf{H}_{R}^{l}),\n(7)\nwhere\n[\n;\n]\n[\\ ;\\ ]\ndenotes concatenation along the sequence length dimension, and\nsg\nâ€‹\n(\nâ‹…\n)\n\\text{sg}(\\cdot)\nindicates the stop-gradient operation.\nThis asymmetric flow ensures a strict hierarchy: the Left Brain acts as a stable \"semantic anchor\" providing high-level reasoning features, while the Right Brain dynamically fuses these semantics with fine-grained proprioceptive cues to reason about spatial actions. The final hidden states of the Right Brain,\nğ‡\nR\nf\nâ€‹\ni\nâ€‹\nn\nâ€‹\na\nâ€‹\nl\n\\mathbf{H}_{R}^{final}\n, are then passed to the Action Expert.\n4.2\nFlow-Matching Action Expert\nTo enable high-precision, continuous control, we move beyond the discrete tokenization paradigm\n(Kim\net al.\n,\n2024\n)\nand adopt the state-of-the-art generative policy architecture utilized in recent foundation models like Isaac-GR00T\n(NVIDIA\net al.\n,\n2025\n)\nand\nÏ€\n0\n\\pi_{0}\n(Intelligence\net al.\n,\n2025\n)\n. Specifically, our Action Expert employs the Diffusion Transformer (DiT) architecture\n(Peebles and Xie,\n2023\n)\nand is trained via flow matching.\nArchitecture and Conditioning.\nFollowing the design in Isaac-GR00T, the Action Expert operates as a conditional decoder that denoises action trajectories. It takes a noisy action sequence\nğš\nt\n\\mathbf{a}_{t}\nand the current timestep\nt\nt\nas input.\nWhile the policy architecture aligns with established baselines, a key distinction in our framework lies in the\nsource\nof the condition. We inject the spatially-rich representations from our trainable Right Brain,\nğ‡\nR\n\\mathbf{H}_{R}\n, into the DiT via cross-attention layers. This ensures that the proven control capabilities of the DiT are guided explicitly by the embodied perception features extracted by our asymmetric backbone.\nFlow-Matching Formulation.\nWe utilize the flow-matching objective to train the policy, as demonstrated effectively in prior work\n(NVIDIA\net al.\n,\n2025\n)\n. The process models a conditional probability path flowing from a standard Gaussian prior\nğš\n0\nâˆ¼\nğ’©\nâ€‹\n(\nğŸ\n,\nğˆ\n)\n\\mathbf{a}_{0}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\nto the ground-truth action distribution\nğš\n1\n\\mathbf{a}_{1}\n.\nThe vector field regresson loss is defined as:\nâ„’\nFM\nâ€‹\n(\nÏˆ\n)\n=\nğ”¼\nt\n,\nğš\n0\n,\nğš\n1\nâ€‹\n[\nâ€–\nv\nÏˆ\nâ€‹\n(\nğš\nt\n,\nt\n,\nğ‡\nR\n)\nâˆ’\n(\nğš\n1\nâˆ’\nğš\n0\n)\nâ€–\n2\n]\n,\n\\mathcal{L}_{\\text{FM}}(\\psi)=\\mathbb{E}_{t,\\mathbf{a}_{0},\\mathbf{a}_{1}}\\left[||v_{\\psi}(\\mathbf{a}_{t},t,\\mathbf{H}_{R})-(\\mathbf{a}_{1}-\\mathbf{a}_{0})||^{2}\\right],\n(8)\nwhere\nv\nÏˆ\nv_{\\psi}\nis the DiT network,\nt\nâˆ¼\nğ’°\nâ€‹\n[\n0\n,\n1\n]\nt\\sim\\mathcal{U}[0,1]\n, and the target vector field is the straight line\nğš\n1\nâˆ’\nğš\n0\n\\mathbf{a}_{1}-\\mathbf{a}_{0}\n.\nDuring inference, actions are synthesized by solving the Ordinary Differential Equation (ODE) using an Euler solver, translating the Right Brainâ€™s understanding into smooth robotic movements.\n4.3\nTraining Strategy\nOptimization Objective.\nConsistent with standard VLA fine-tuning paradigms\n(NVIDIA\net al.\n,\n2025\n)\n, we train\nTwinBrainVLA\nusing exclusively the robotic action objective. We do not incorporate auxiliary Next-Token Prediction (NTP) losses or mix in general vision-language datasets during this stage. The training objective is to minimize the Flow-Matching loss:\nâ„’\ntotal\n=\nâ„’\nFM\nâ€‹\n(\nÎ¸\nR\n,\nÏˆ\n,\nÏ•\n;\nğ’Ÿ\nrobot\n)\n,\n\\mathcal{L}_{\\text{total}}=\\mathcal{L}_{\\text{FM}}(\\theta_{R},\\psi,\\phi;\\mathcal{D}_{\\text{robot}}),\n(9)\nwhere\nğ’Ÿ\nrobot\n\\mathcal{D}_{\\text{robot}}\nrepresents the robotic demonstration dataset, and\n{\nÎ¸\nR\n,\nÏˆ\n,\nÏ•\n}\n\\{\\theta_{R},\\psi,\\phi\\}\ndenote the trainable parameters of the Right Brain, Action Expert, and State Encoder, respectively.\nIt is worth noting that in monolithic VLA architectures, relying solely on action loss typically leads to severe catastrophic forgetting of general semantic capabilities. However, our dual-stream design structurally immunizes the model against this degradation: the \"Right Brain\" is free to specialize entirely in control dynamics, while the frozen \"Left Brain\" implicitly safeguards the linguistic and semantic priors.\nAsymmetric Update Rule.\nWe enforce a strict parameter update policy. Let\nÎ¸\nL\n\\theta_{L}\nbe the parameters of the Left Brain. During backpropagation, we set\nâˆ‡\nÎ¸\nL\n=\n0\n\\nabla\\theta_{L}=0\n. Gradients originating from the Action Expert flow backwards through the DiT (\nÏˆ\n\\psi\n), permeate the Right Brain (\nÎ¸\nR\n\\theta_{R}\n) via the conditioning signals, and update the State Encoder (\nÏ•\n\\phi\n). Crucially, at the AsyMoT fusion layer, the gradient flow is explicitly blocked (stop-gradient) at the Key-Value interface from the Left Brain. This ensures that the Left Brain acts as a stable semantic anchor, providing consistent features\nK\nL\n,\nV\nL\nK_{L},V_{L}\nwithout its weights being perturbed by the high-variance gradients typical of robotic control tasks.\n5\nExperiment\nTo comprehensively evaluate the efficacy of\nTwinBrainVLA\n, we conduct extensive experiments on two simulation benchmarks:\nSimplerEnv\n(Li\net al.\n,\n2024c\n)\nand\nRoboCasa\n(Nasiriany\net al.\n,\n2024\n)\n. Our training pipeline is built upon the starVLA framework\n(starVLA,\n2025\n)\n, distributed across 16\nÃ—\n\\times\nNVIDIA H100 GPUs. We strictly follow its default training protocols to ensure a fair comparison. More extensive simulation benchmarks and real-world robotic experiments are in progress.\n5.1\nExperiments on SimplerEnv\nImplementation and Training Setup.\nTo demonstrate the scalability and effectiveness of our architecture, we instantiate\nTwinBrainVLA\nwith two state-of-the-art VLM backbones: Qwen2.5-VL-3B-Instruct and Qwen3-VL-4B-Instruct. Consistent with our asymmetric design, both the frozen Left Brain and the trainable Right Brain are initialized from these pre-trained checkpoints to ensure aligned feature spaces. For the training data, we utilize two large-scale subsets from the Open X-Embodiment (OXE) dataset: the\nBridge-V2\nand the\nFractal\ndataset. We are actively working on scaling our training to the complete OXE dataset, and future experimental results will be reported in subsequent updates. Comprehensive implementation details are provided in Appendix\nA\n.\nEvaluation.\nThe benchmark consists of four manipulation tasks: \"put spoon on towel\", \"put carrot on plate\", \"stack green block on yellow block\", \"put eggplant in the yellow basket\". For each task, we evaluate our VLA policy using the evaluation script provided by the SimplerEnv repository\n(Li\net al.\n,\n2024c\n)\n. To mitigate randomness, we run 480 independent trials and report the mean performance (Avg@480).\nBaselines.\nWe benchmark\nTwinBrainVLA\nagainst a comprehensive suite of state-of-the-art VLA policies, including RT-1-X\n(Oâ€™Neill\net al.\n,\n2024\n)\n, Octo\n(Team\net al.\n,\n2024\n)\n, OpenVLA\n(Kim\net al.\n,\n2024\n)\n, RoboVLM\n(Li\net al.\n,\n2024b\n)\n, TraceVLA\n(Zheng\net al.\n,\n2025c\n)\n, SpatialVLA\n(Qu\net al.\n,\n2025b\n)\n, CogACT\n(Li\net al.\n,\n2024a\n)\n, VideoVLA\n(Shen\net al.\n,\n2025\n)\n,\nÏ€\n0\n\\pi_{0}\n(Black\net al.\n,\n2024\n)\n, and\nÏ€\n0.5\n\\pi_{0.5}\n(Intelligence\net al.\n,\n2025\n)\n. All baseline results are obtained from their original papers or official reproductions to ensure a fair and reliable comparison.\nTable 1:\nResults of evaluating the VLA models with the WidowX robot in the SimplerEnv simulation environment\n. We highlight the best results in\nbold\nand the second-best results with\nunderline\n.\nMethod\nPut Spoon\non Towel\nPut Carrot\non Plate\nStack Green Block\non Yellow Block\nPut Eggplant\nin Yellow Basket\nAverage\nRT-1-X\n(Oâ€™Neill\net al.\n,\n2024\n)\n0.0\n4.2\n0.0\n0.0\n1.1\nOcto-Base\n(Team\net al.\n,\n2024\n)\n15.8\n12.5\n0.0\n41.7\n17.5\nOcto-Small\n(Team\net al.\n,\n2024\n)\n41.7\n8.2\n0.0\n56.7\n26.7\nOpenVLA\n(Kim\net al.\n,\n2024\n)\n4.2\n0.0\n0.0\n12.5\n4.2\nOpenVLA-OFT\n(Kim\net al.\n,\n2025\n)\n12.5\n4.2\n4.2\n72.5\n23.4\nRoboVLM\n(Li\net al.\n,\n2024b\n)\n50.0\n37.5\n0.0\n83.3\n42.7\nTraceVLA\n(Zheng\net al.\n,\n2025c\n)\n12.5\n16.6\n16.6\n65.0\n27.7\nSpatialVLA\n(Qu\net al.\n,\n2025b\n)\n20.8\n20.8\n25.0\n70.8\n34.4\nCogACT\n(Li\net al.\n,\n2024a\n)\n71.7\n50.8\n15.0\n67.5\n51.3\nVideoVLA\n(Shen\net al.\n,\n2025\n)\n75.0\n20.8\n45.8\n70.8\n53.1\nÏ€\n0\n\\pi_{0}\n(Black\net al.\n,\n2024\n)\n29.1\n0.0\n16.6\n62.5\n27.1\nÏ€\n0.5\n\\pi_{0.5}\n(Intelligence\net al.\n,\n2025\n)\n49.3\n64.7\n44.7\n69.7\n57.1\nQwenGR00T + Qwen3-VL-3B\n(starVLA,\n2025\n)\n87.5\n50.0\n29.2\n54.2\n55.2\nQwenGR00T + Qwen3-VL-8B\n(starVLA,\n2025\n)\n70.8\n38.9\n22.2\n65.3\n49.3\nQwenGR00T + Qwen2.5-VL-7B\n(starVLA,\n2025\n)\n59.2\n30.8\n3.3\n44.2\n34.4\nIsaac-GR00T-N1.6-Bridge\n(Team\net al.\n,\n2025b\n)\n64.5\n65.5\n5.5\n93.0\n57.1\nTwinBrainVLA\n+ Qwen2.5-VL-3B-Instruct\n83.3\n41.7\n31.5\n77.1\n58.4\nTwinBrainVLA\n+ Qwen3-VL-4B-Instruct\n73.0\n58.3\n33.3\n83.3\n62.0\nResults.\nThe quantitative results on the SimplerEnv benchmark are presented in Table\n1\n. Notably, despite not undergoing large-scale pre-training for robotic action prediction,\nTwinBrainVLA\nachieves state-of-the-art performance among all listed methods. Our framework demonstrates strong generalizability across different VLM families, attaining competitive success rates of 58.4% with Qwen2.5-VL-3B-Instruct and\n62.0%\nwith Qwen3-VL-4B-Instruct. The latter configuration surpasses the strongest baseline, Isaac-GR00T-N1.6 (57.1%), by a notable margin of +4.9%, validating the effectiveness of our asymmetric dual-brain architecture in bridging high-level semantic understanding and low-level robotic control.\n5.2\nExperiments on RoboCasa\nImplementation.\nWe train\nTwinBrainVLA\non the\nHumanoid Robot Tabletop Manipulation\nsubset from the\nPhysicalAI-Robotics-GR00T-X-Embodiment-Sim\ndataset\n(NVIDIA\net al.\n,\n2025\n)\n. All other experimental settings, including model initialization, distributed training infrastructure, and hyperparameters, remain identical to those described in Sec.\n5.1\n.\nEvaluation.\nWe evaluate our policy on the RoboCasa GR1 Tabletop Benchmark, which comprises a diverse suite of 24 tabletop manipulation tasks. These tasks involve complex interactions with articulated objects and varying geometries, such as\nPnPBottleToCabinetClose\n(placing a bottle into a cabinet and closing the door),\nPnPCanToDrawerClose\n, and various microwave/toaster interaction scenarios.\nTo ensure statistical reliability while maintaining consistency with our previous experiments, we report the success rate averaged over multiple independent trials (Avg@50) for each of the 24 tasks.\nTable 2:\nResults of evaluating the VLA models with the GR1 robot in the RoboCasa Tabletop simulation environment\n. We highlight the best results in\nbold\nand the second-best results with\nunderline\n.\n\\rowcolor\nwhite\nTask\nIsaac-GR00T\nN1.6\nQwenGR00T\n+ Qwen3VL\nQwenPI\n+ Qwen3VL\nTwinBrainVLA\n+ Qwen2.5VL\nTwinBrainVLA\n+ Qwen3VL\nPnP Bottle To Cabinet Close\n51.5\n46.0\n26.0\n62.0\n74.0\nPnP Can To Drawer Close\n13.0\n80.0\n62.0\n66.0\n72.0\nPnP Cup To Drawer Close\n8.5\n54.0\n42.0\n46.0\n52.0\nPnP Milk To Microwave Close\n14.0\n48.0\n50.0\n52.0\n60.0\nPnP Potato To Microwave Close\n41.5\n28.0\n42.0\n56.0\n36.0\nPnP Wine To Cabinet Close\n16.5\n46.0\n32.0\n58.0\n46.0\nPnP Novel From Cuttingboard To Basket\n58.0\n48.0\n40.0\n50.0\n62.0\nPnP Novel From Cuttingboard To Cardboardbox\n46.5\n40.0\n46.0\n44.0\n46.0\nPnP Novel From Cuttingboard To Pan\n68.5\n68.0\n60.0\n64.0\n70.0\nPnP Novel From Cuttingboard To Pot\n65.0\n52.0\n40.0\n60.0\n66.0\nPnP Novel From Cuttingboard To Tieredbasket\n46.5\n56.0\n44.0\n52.0\n52.0\nPnP Novel From Placemat To Basket\n58.5\n42.0\n44.0\n56.0\n30.0\nPnP Novel From Placemat To Bowl\n57.5\n44.0\n52.0\n59.0\n54.0\nPnP Novel From Placemat To Plate\n63.0\n48.0\n50.0\n62.0\n64.0\nPnP Novel From Placemat To Tieredshelf\n28.5\n18.0\n28.0\n28.0\n38.0\nPnP Novel From Plate To Bowl\n57.0\n60.0\n52.0\n62.0\n60.0\nPnP Novel From Plate To Cardboardbox\n43.5\n50.0\n40.0\n52.0\n58.0\nPnP Novel From Plate To Pan\n51.0\n54.0\n36.0\n56.0\n56.0\nPnP Novel From Plate To Plate\n78.7\n70.0\n48.0\n68.0\n66.0\nPnP Novel From Tray To Cardboardbox\n51.5\n38.0\n34.0\n40.0\n46.0\nPnP Novel From Tray To Plate\n71.0\n56.0\n64.0\n56.0\n72.0\nPnP Novel From Tray To Pot\n64.5\n50.0\n44.0\n60.0\n56.0\nPnP Novel From Tray To Tieredbasket\n57.0\n36.0\n50.0\n42.0\n46.0\nPnP Novel From Tray To Tieredshelf\n31.5\n16.0\n28.0\n34.0\n28.0\n\\rowcolor\ngray!30\nAverage\n47.6\n47.8\n43.9\n53.5\n54.6\nBaselines.\nWe compare\nTwinBrainVLA\nagainst several competitive baselines trained on the same dataset. Specifically, we include Isaac-GR00T-N1.6\n(Team\net al.\n,\n2025b\n)\n, a large-scale VLA model that demonstrates strong performance on humanoid manipulation tasks. Additionally, to directly assess the advantages of our dual-brain architecture, we benchmark against two representative models from the starVLA framework\n(starVLA,\n2025\n)\n: QwenGR00T and QwenPI, both instantiated with Qwen3-VL-4B-Instruct backbones. These baselines follow similar training protocols and utilize the same dataset, enabling a fair comparison and highlighting the specific contributions of our asymmetric dual-VLM design.\nResults.\nThe quantitative results on the RoboCasa GR1 Tabletop Benchmark are presented in Table\n2\n.\nTwinBrainVLA\nachieves the best performance across all 24 manipulation tasks, with our Qwen3-VL-4B-Instruct variant attaining an average success rate of\n54.6%\n, followed closely by the Qwen2.5-VL-3B-Instruct variant at 53.5%. These results substantially outperform all baseline methods: our best model surpasses Isaac-GR00T-N1.6 (47.6%) by +7.0%, QwenGR00T (47.8%) by +6.8%, and QwenPI (43.9%) by +10.7%. These results reinforce our hypothesis that decoupling semantic understanding from embodied perception enables more effective learning of fine-grained manipulation skills in complex tabletop scenarios.\n6\nConclusion\nIn this paper, we introduced\nTwinBrainVLA\n, a novel VLA framework that addresses the fundamental tension between semantic generalization and embodied specialization in robotic learning. By drawing inspiration from hemispheric lateralization, we proposed an asymmetric dual-stream architecture that structurally decouples the visual cognitive process. Our\nAsyMoT\nmechanism effectively synergizes a frozen \"Left Brain\" for robust open-world reasoning with a trainable \"Right Brain\" for precise sensorimotor control, enabling the system to master complex manipulation skills without \"catastrophic forgetting\" of its general visual capabilities.\nLimitation and Future Work\nWhile\nTwinBrainVLA\nprovides a powerful framework for decoupling semantic and embodied processing, several limitations remain that outline our future research directions.\nA more decoupled model architecture.\nFirst, the current implementation of the Asymmetric Mixture-of-Transformers (AsyMoT) mechanism requires the Left and Right Brains to share an identical model architecture. This structural constraint ensures compatible hidden state dimensions for seamless joint attention but limits the flexibility to pair heterogeneous modelsâ€”for instance, synergizing a massive, reasoning-heavy VLM with a lightweight, high-frequency control model. We are actively exploring generalized fusion mechanisms, such as learnable projection layers or cross-attention adapters, to support backbones of varying sizes and architectures.\nMore diverse model checkpoints.\nSecond, both streams in our current setup are initialized from standard general-purpose VLM checkpoints (e.g., Qwen-VL). However, the field has arguably seen a surge in specialized Embodied VLMs trained specifically for robotics. Integrating these specialized models into our dual-brain paradigmâ€”potentially initializing the \"Right Brain\" with an embodied checkpoint while retaining a generalist \"Left Brain\"â€”presents a promising avenue. We are currently investigating strategies to effectively adapt our framework to leverage these domain-specific post-trained weights.\nTraining on larger-scale data.\nThird, regarding data scale, our current training utilizes subsets of the Open X-Embodiment dataset. We acknowledge that the capacity of our dual-stream architecture may not be fully realized with limited data. We are in the process of scaling our pre-training to the complete dataset to further enhance the modelâ€™s robustness and downstream performance.\nMore benchmarks and real-robot experiment.\nFinally, future work will also focus on extending our evaluation to a broader range of benchmarks, including real-robot scenarios, to comprehensively assess the versatility of\nTwinBrainVLA\n.\nReferences\nS. Bai, Y. Cai, R. Chen, K. Chen, X. Chen, Z. Cheng, L. Deng, W. Ding, C. Gao, C. Ge, W. Ge, Z. Guo, Q. Huang, J. Huang, F. Huang, B. Hui, S. Jiang, Z. Li, M. Li, M. Li, K. Li, Z. Lin, J. Lin, X. Liu, J. Liu, C. Liu, Y. Liu, D. Liu, S. Liu, D. Lu, R. Luo, C. Lv, R. Men, L. Meng, X. Ren, X. Ren, S. Song, Y. Sun, J. Tang, J. Tu, J. Wan, P. Wang, P. Wang, Q. Wang, Y. Wang, T. Xie, Y. Xu, H. Xu, J. Xu, Z. Yang, M. Yang, J. Yang, A. Yang, B. Yu, F. Zhang, H. Zhang, X. Zhang, B. Zheng, H. Zhong, J. Zhou, F. Zhou, J. Zhou, Y. Zhu, and K. Zhu (2025a)\nQwen3-vl technical report\n.\nCited by:\nÂ§2\n,\nÂ§4.1\n.\nS. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin (2025b)\nQwen2.5-vl technical report\n.\nCited by:\nÂ§2\n.\nK. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter,\net al.\n(2024)\nÏ€\n0\n\\pi_{0}\n: a vision-language-action flow model for general robot control\n.\narXiv preprint arXiv:2410.24164\n.\nCited by:\nÂ§5.1\n,\nTable 1\n.\nJ. Cai, Z. Cai, J. Cao, Y. Chen, Z. He, L. Jiang, H. Li, H. Li, Y. Li, Y. Liu, Y. Lu, Q. Lv, H. Ma, J. Pang, Y. Qiao, Z. Qiu, Y. Shen, X. Shi, Y. Tian, B. Wang, H. Wang, J. Wang, T. Wang, X. Wei, C. Wu, Y. Xie, B. Xing, Y. Yang, Y. Yang, Q. Yu, F. Yuan, J. Zeng, J. Zhang, S. Zhang, S. Zhang, Z. Zhaxi, B. Zhou, Y. Zhou, Y. Zhou, H. Zhu, Y. Zhu, and Y. Zhu (2026)\nInternVLA-a1: unifying understanding, generation and action for robotic manipulation\n.\nExternal Links:\n2601.02456\nCited by:\nÂ§2\n.\nS. Chen, T. Zhu, R. Zhou, J. Zhang, S. Gao, J. C. Niebles, M. Geva, J. He, J. Wu, and M. Li (2025)\nWhy is spatial reasoning hard for VLMs? an attention mechanism perspective on focus areas\n.\nIn\nInternational conference on machine learning (ICML)\n,\nExternal Links:\nLink\nCited by:\nÂ§2\n.\nR. Dang, Y. Yuan, Y. Mao, K. Li, J. Liu, Z. Wang, X. Li, F. Wang, and D. Zhao (2025)\nRynnEC: bringing mllms into embodied world\n.\nExternal Links:\n2508.14160\n,\nLink\nCited by:\nÂ§2\n.\nS. Fan, K. Wu, Z. Che, X. Wang, D. Wu, F. Liao, N. Liu, Y. Zhang, Z. Zhao, Z. Xu, M. Li, Q. Liu, S. Zhang, M. Wan, and J. Tang (2025)\nXR-1: towards versatile vision-language-action models via learning unified vision-motion representations\n.\nExternal Links:\n2511.02776\nCited by:\nÂ§2\n.\nZ. Fang, Z. Liu, J. Liu, H. Chen, Y. Zeng, S. Huang, Z. Chen, L. Chen, S. Zhang, and F. Zhao (2025)\nDualVLA: building a generalizable embodied agent via partial decoupling of reasoning and action\n.\nExternal Links:\n2511.22134\nCited by:\nÂ§1\n.\nW. Guo, G. Lu, H. Deng, Z. Wu, Y. Tang, and Z. Wang (2025)\nVLA-reasoner: empowering vision-language-action models with reasoning via online monte carlo tree search\n.\nExternal Links:\n2509.22643\nCited by:\nÂ§2\n.\nA. J. Hancock, X. Wu, L. Zha, O. Russakovsky, and A. Majumdar (2025a)\nActions as language: fine-tuning vlms into vlas without catastrophic forgetting\n.\nExternal Links:\n2509.22195\nCited by:\nÂ§2\n,\nÂ§3\n.\nA. J. Hancock, X. Wu, L. Zha, O. Russakovsky, and A. Majumdar (2025b)\nActions as language: fine-tuning vlms into vlas without catastrophic forgetting\n.\nExternal Links:\n2509.22195\nCited by:\nÂ§1\n.\nX. Hao, L. Zhou, Z. Huang, Z. Hou, Y. Tang, L. Zhang, G. Li, Z. Lu, S. Ren, X. Meng, Y. Zhang, J. Wu, J. Lu, C. Dang, J. Guan, J. Wu, Z. Hou, H. Li, S. Xia, M. Zhou, Y. Zheng, Z. Yue, S. Gu, H. Tian, Y. Shen, J. Cui, W. Zhang, S. Xu, B. Wang, H. Sun, Z. Zhu, Y. Jiang, Z. Guo, C. Gong, C. Zhang, W. Ding, K. Ma, G. Chen, R. Cai, D. Xiang, H. Qu, F. Luo, H. Ye, and L. Chen (2025)\nMiMo-embodied: x-embodied foundation model technical report\n.\nExternal Links:\n2511.16518\nCited by:\nÂ§2\n.\nC. Huang, Y. Man, Z. Yu, M. Chen, J. Kautz, Y. F. Wang, and F. Yang (2026)\nFast-thinkact: efficient vision-language-action reasoning via verbalizable latent planning\n.\nExternal Links:\n2601.09708\nCited by:\nÂ§2\n.\nC. Huang, Y. Wu, M. Chen, Y. F. Wang, and F. Yang (2025)\nThinkAct: vision-language-action reasoning via reinforced visual latent planning\n.\nExternal Links:\n2507.16815\nCited by:\nÂ§2\n.\nP. Intelligence, K. Black, N. Brown, J. Darpinian, K. Dhabalia, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, M. Y. Galliker, D. Ghosh, L. Groom, K. Hausman, B. Ichter, S. Jakubczak, T. Jones, L. Ke, D. LeBlanc, S. Levine, A. Li-Bell, M. Mothukuri, S. Nair, K. Pertsch, A. Z. Ren, L. X. Shi, L. Smith, J. T. Springenberg, K. Stachowicz, J. Tanner, Q. Vuong, H. Walke, A. Walling, H. Wang, L. Yu, and U. Zhilinsky (2025)\nÏ€\n0.5\n\\pi_{0.5}\n: A vision-language-action model with open-world generalization\n.\nExternal Links:\n2504.16054\nCited by:\nÂ§1\n,\nÂ§4.2\n,\nÂ§5.1\n,\nTable 1\n.\nJ. Jiang, C. Ma, X. Song, H. Zhang, and J. Luo (2025a)\nCorvid: improving multimodal large language models towards chain-of-thought reasoning\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)\n,\npp.Â 3034â€“3046\n.\nCited by:\nÂ§2\n.\nT. Jiang, T. Yuan, Y. Liu, C. Lu, J. Cui, X. Liu, S. Cheng, J. Gao, H. Xu, and H. Zhao (2025b)\nGalaxea open-world dataset and g0 dual-system vla model\n.\nExternal Links:\n2509.00576\nCited by:\nÂ§2\n.\nN. Kachaev, M. Kolosov, D. Zelezetsky, A. K. Kovalev, and A. I. Panov (2025)\nDonâ€™t blind your vla: aligning visual representations for ood generalization\n.\nExternal Links:\n2510.25616\nCited by:\nÂ§3\n.\nM. J. Kim, C. Finn, and P. Liang (2025)\nFine-tuning vision-language-action models: optimizing speed and success\n.\nExternal Links:\n2502.19645\nCited by:\nTable 1\n.\nM. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. P. Foster, P. R. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang, and C. Finn (2024)\nOpenVLA: an open-source vision-language-action model\n.\nIn\nAnnual Conference on Robot Learning (CoRL)\n,\nCited by:\nÂ§1\n,\nÂ§4.2\n,\nÂ§5.1\n,\nTable 1\n.\nJ. Lee, J. Duan, H. Fang, Y. Deng, S. Liu, B. Li, B. Fang, J. Zhang, Y. R. Wang, S. Lee, W. Han, W. Pumacay, A. Wu, R. Hendrix, K. Farley, E. VanderBilt, A. Farhadi, D. Fox, and R. Krishna (2025)\nMolmoAct: action reasoning models that can reason in space\n.\nExternal Links:\n2508.07917\nCited by:\nÂ§2\n.\nF. Li, W. Song, H. Zhao, J. Wang, P. Ding, D. Wang, L. Zeng, and H. Li (2025)\nSpatial forcing: implicit spatial representation alignment for vision-language-action model\n.\nExternal Links:\n2510.12276\nCited by:\nÂ§2\n.\nQ. Li, Y. Liang, Z. Wang, L. Luo, X. Chen, M. Liao, F. Wei, Y. Deng, S. Xu, Y. Zhang, X. Wang, B. Liu, J. Fu, J. Bao, D. Chen, Y. Shi, J. Yang, and B. Guo (2024a)\nCogACT: a foundational vision-language-action model for synergizing cognition and action in robotic manipulation\n.\nExternal Links:\n2411.19650\nCited by:\nÂ§5.1\n,\nTable 1\n.\nX. Li, P. Li, M. Liu, D. Wang, J. Liu, B. Kang, X. Ma, T. Kong, H. Zhang, and H. Liu (2024b)\nTowards generalist robot policies: what matters in building vision-language-action models\n.\nExternal Links:\n2412.14058\nCited by:\nÂ§5.1\n,\nTable 1\n.\nX. Li, K. Hsu, J. Gu, O. Mees, K. Pertsch, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani, S. Levine, J. Wu, C. Finn, H. Su, Q. Vuong, and T. Xiao (2024c)\nEvaluating real-world robot manipulation policies in simulation\n.\nIn\nAnnual Conference on Robot Learning (CoRL)\n,\nCited by:\nÂ§5.1\n,\nÂ§5\n.\nF. Lin, R. Nai, Y. Hu, J. You, J. Zhao, and Y. Gao (2025a)\nOneTwoVLA: a unified vision-language-action model with adaptive reasoning\n.\nExternal Links:\n2505.11917\nCited by:\nÂ§2\n.\nT. Lin, Y. Zhong, Y. Du, J. Zhang, J. Liu, Y. Chen, E. Gu, Z. Liu, H. Cai, Y. Zou, L. Zou, Z. Zhou, G. Li, and B. Zhao (2025b)\nEvo-1: lightweight vision-language-action model with preserved semantic alignment\n.\nExternal Links:\n2511.04555\nCited by:\nÂ§2\n.\nX. Lin, S. Lian, B. Yu, R. Yang, C. Wu, Y. Miao, Y. Jin, Y. Shi, C. Huang, B. Cheng, and K. Chen (2025c)\nPhysBrain: human egocentric data as a bridge from vision language models to physical intelligence\n.\nExternal Links:\n2512.16793\nCited by:\nÂ§2\n.\nH. Liu, C. Li, Q. Wu, and Y. J. Lee (2023)\nVisual instruction tuning\n.\nIn\nAdvances in neural information processing systems (NeurIPS)\n,\nCited by:\nÂ§2\n.\nI. Loshchilov and F. Hutter (2019)\nDecoupled weight decay regularization\n.\nExternal Links:\n1711.05101\nCited by:\nAppendix A\n.\nP. Mazzaglia, C. Sancaktar, M. Peschl, and D. Dijkman (2025)\nHybrid training for vision-language-action models\n.\nExternal Links:\n2510.00600\nCited by:\nÂ§2\n.\nS. Nasiriany, A. Maddukuri, L. Zhang, A. Parikh, A. Lo, A. Joshi, A. Mandlekar, and Y. Zhu (2024)\nRoboCasa: large-scale simulation of everyday tasks for generalist robots\n.\nIn\nRobotics: Science and Systems\n,\nCited by:\nÂ§5\n.\nNVIDIA, :, J. Bjorck, F. CastaÃ±eda, N. Cherniadev, X. Da, R. Ding, L. \". Fan, Y. Fang, D. Fox, F. Hu, S. Huang, J. Jang, Z. Jiang, J. Kautz, K. Kundalia, L. Lao, Z. Li, Z. Lin, K. Lin, G. Liu, E. Llontop, L. Magne, A. Mandlekar, A. Narayan, S. Nasiriany, S. Reed, Y. L. Tan, G. Wang, Z. Wang, J. Wang, Q. Wang, J. Xiang, Y. Xie, Y. Xu, Z. Xu, S. Ye, Z. Yu, A. Zhang, H. Zhang, Y. Zhao, R. Zheng, and Y. Zhu (2025)\nGR00T n1: an open foundation model for generalist humanoid robots\n.\nExternal Links:\n2503.14734\nCited by:\nÂ§1\n,\nÂ§4.2\n,\nÂ§4.2\n,\nÂ§4.3\n,\nÂ§5.2\n.\nA. Oâ€™Neill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain,\net al.\n(2024)\nOpen x-embodiment: robotic learning datasets and rt-x models: open x-embodiment collaboration\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 6892â€“6903\n.\nCited by:\nÂ§5.1\n,\nTable 1\n.\nW. Peebles and S. Xie (2023)\nScalable diffusion models with transformers\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)\n,\npp.Â 4195â€“4205\n.\nCited by:\nÂ§4.2\n.\nK. Pertsch, K. Stachowicz, B. Ichter, D. Driess, S. Nair, Q. Vuong, O. Mees, C. Finn, and S. Levine (2025)\nFAST: efficient action tokenization for vision-language-action models\n.\nExternal Links:\n2501.09747\nCited by:\nÂ§2\n.\nD. Qu, H. Song, Q. Chen, Z. Chen, X. Gao, X. Ye, Q. Lv, M. Shi, G. Ren, C. Ruan, M. Yao, H. Yang, J. Bao, B. Zhao, and D. Wang (2025a)\nEO-1: interleaved vision-text-action pretraining for general robot control\n.\nExternal Links:\n2508.21112\nCited by:\nÂ§2\n.\nD. Qu, H. Song, Q. Chen, Y. Yao, X. Ye, Y. Ding, Z. Wang, J. Gu, B. Zhao, D. Wang, and X. Li (2025b)\nSpatialVLA: exploring spatial representations for visual-language-action model\n.\nExternal Links:\n2501.15830\nCited by:\nÂ§5.1\n,\nTable 1\n.\nY. Shen, F. Wei, Z. Du, Y. Liang, Y. Lu, J. Yang, N. Zheng, and B. Guo (2025)\nVideoVLA: video generators can be generalizable robot manipulators\n.\nExternal Links:\n2512.06963\nCited by:\nÂ§5.1\n,\nTable 1\n.\nM. Shukor, D. Aubakirova, F. Capuano, P. Kooijmans, S. Palma, A. Zouitine, M. Aractingi, C. Pascal, M. Russi, A. Marafioti, S. Alibert, M. Cord, T. Wolf, and R. Cadene (2025)\nSmolVLA: a vision-language-action model for affordable and efficient robotics\n.\nExternal Links:\n2506.01844\nCited by:\nÂ§2\n.\nstarVLA (2025)\nStarVLA: a lego-like codebase for vision-language-action model developing\n.\nGitHub\n.\nNote:\nGitHub repository\nExternal Links:\nLink\n,\nDocument\nCited by:\nAppendix A\n,\nÂ§5.2\n,\nTable 1\n,\nTable 1\n,\nTable 1\n,\nÂ§5\n.\nH. Tan, Y. Ji, X. Hao, X. Chen, P. Wang, Z. Wang, and S. Zhang (2025)\nReason-rft: reinforcement fine-tuning for visual reasoning of vision language models\n.\nExternal Links:\n2503.20752\nCited by:\nÂ§2\n.\nB. R. Team, M. Cao, H. Tan, Y. Ji, X. Chen, M. Lin, Z. Li, Z. Cao, P. Wang, E. Zhou, Y. Han, Y. Tang, X. Xu, W. Guo, Y. Lyu, Y. Xu, J. Shi, M. Du, C. Chi, M. Zhao, X. Hao, J. Zhao, X. Zhang, S. Rong, H. Lyu, Z. Cai, Y. Fu, N. Chen, B. Zhang, L. Zhang, S. Zhang, D. Liu, X. Feng, S. Wang, X. Liu, Y. Jiao, M. Lyu, Z. Chen, C. He, Y. Ao, X. Sun, Z. He, J. Zheng, X. Yang, D. Shi, K. Xie, B. Zhang, S. Nie, C. Men, Y. Lin, Z. Wang, T. Huang, and S. Zhang (2025a)\nRoboBrain 2.0 technical report\n.\nExternal Links:\n2507.02029\nCited by:\nÂ§2\n.\nG. Team, A. Azzolini, J. Bjorck, V. Blukis, F. CastaÃ±eda, R. Chand,\net al.\n(2025b)\nGR00T n1.6: an improved open foundation model for generalist humanoid robots\n.\nNote:\nhttps://research.nvidia.com/labs/gear/gr00t-n1_6/\nCited by:\nÂ§5.2\n,\nTable 1\n.\nO. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu, J. Luo, Y. L. Tan, L. Y. Chen, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh, C. Finn, and S. Levine (2024)\nOcto: an open-source generalist robot policy\n.\nExternal Links:\n2405.12213\nCited by:\nÂ§5.1\n,\nTable 1\n,\nTable 1\n.\nJ. Wen, Y. Zhu, J. Li, M. Zhu, K. Wu, Z. Xu, N. Liu, R. Cheng, C. Shen, Y. Peng, F. Feng, and J. Tang (2025)\nTinyVLA: towards fast, data-efficient vision-language-action models for robotic manipulation\n.\nExternal Links:\n2409.12514\nCited by:\nÂ§2\n.\nY. Xiao, L. Li, S. Yan, X. Liu, S. Peng, Y. Wei, X. Zhou, and B. Kang (2026)\nSpatialTree: how spatial abilities branch out in mllms\n.\nExternal Links:\n2512.20617\nCited by:\nÂ§2\n.\nR. Yang, Z. Zhu, Y. Li, J. Huang, S. Yan, S. Zhou, Z. Liu, X. Li, S. Li, W. Wang, Y. Lin, and H. Zhao (2025a)\nVisual spatial tuning\n.\nExternal Links:\n2511.05491\nCited by:\nÂ§2\n.\nS. Yang, H. Li, Y. Chen, B. Wang, Y. Tian, T. Wang, H. Wang, F. Zhao, Y. Liao, and J. Pang (2025b)\nInstructVLA: vision-language-action instruction tuning from understanding to manipulation\n.\nExternal Links:\n2507.17520\nCited by:\nÂ§2\n.\nS. Yang, J. Yang, P. Huang, E. Brown, Z. Yang, Y. Yu, S. Tong, Z. Zheng, Y. Xu, M. Wang, D. Lu, R. Fergus, Y. LeCun, L. Fei-Fei, and S. Xie (2025c)\nCambrian-s: towards spatial supersensing in video\n.\nExternal Links:\n2511.04670\nCited by:\nÂ§2\n.\nA. Ye, Z. Zhang, B. Wang, X. Wang, D. Zhang, and Z. Zhu (2025)\nVLA-r1: enhancing reasoning in vision-language-action models\n.\nExternal Links:\n2510.01623\nCited by:\nÂ§2\n.\nC. Yin, Y. Lin, W. Xu, S. Tam, X. Zeng, Z. Liu, and Z. Yin (2025)\nDeepThinkVLA: enhancing reasoning capability of vision-language-action models\n.\nExternal Links:\n2511.15669\nCited by:\nÂ§2\n.\nM. Zawalski, W. Chen, K. Pertsch, O. Mees, C. Finn, and S. Levine (2025)\nRobotic control via embodied chain-of-thought reasoning\n.\nExternal Links:\n2407.08693\nCited by:\nÂ§2\n.\nA. Zhai, B. Liu, B. Fang, C. Cai, E. Ma, E. Yin, H. Wang, H. Zhou, J. Wang, L. Shi, L. Liang, M. Wang, Q. Wang, R. Gan, R. Yu, S. Li, S. Liu, S. Chen, V. Chen, and Z. Xu (2025)\nIgniting vlms toward the embodied space\n.\nExternal Links:\n2509.11766\n,\nLink\nCited by:\nÂ§2\n.\nC. Zhang, R. Yang, X. Chen, K. Wang, L. Zhao, Y. Chen, and J. Bian (2025a)\nHow do vlas effectively inherit from vlms?\n.\nExternal Links:\n2511.06619\nCited by:\nÂ§3\n.\nJ. Zhang, X. Chen, Q. Wang, M. Li, Y. Guo, Y. Hu, J. Zhang, S. Bai, J. Lin, and J. Chen (2026)\nVLM4VLA: revisiting vision-language-models in vision-language-action models\n.\nExternal Links:\n2601.03309\nCited by:\nÂ§1\n.\nZ. Zhang, Z. Wang, G. Zhang, W. Dai, Y. Xia, Z. Yan, M. Hong, and Z. Zhao (2025b)\nDSI-bench: a benchmark for dynamic spatial intelligence\n.\nExternal Links:\n2510.18873\nCited by:\nÂ§2\n.\nJ. Zheng, J. Li, D. Liu, Y. Zheng, Z. Wang, Z. Ou, Y. Liu, J. Liu, Y. Zhang, and X. Zhan (2025a)\nUniversal actions for enhanced embodied foundation models\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 22508â€“22519\n.\nCited by:\nÂ§2\n.\nJ. Zheng, J. Li, Z. Wang, D. Liu, X. Kang, Y. Feng, Y. Zheng, J. Zou, Y. Chen, J. Zeng, Y. Zhang, J. Pang, J. Liu, T. Wang, and X. Zhan (2025b)\nX-vla: soft-prompted transformer as scalable cross-embodiment vision-language-action model\n.\nExternal Links:\n2510.10274\nCited by:\nÂ§2\n.\nR. Zheng, Y. Liang, S. Huang, J. Gao, H. D. III, A. Kolobov, F. Huang, and J. Yang (2025c)\nTraceVLA: visual trace prompting enhances spatial-temporal awareness for generalist robotic policies\n.\nExternal Links:\n2412.10345\nCited by:\nÂ§5.1\n,\nTable 1\n.\nE. Zhou, J. An, C. Chi, Y. Han, S. Rong, C. Zhang, P. Wang, Z. Wang, T. Huang, L. Sheng, and S. Zhang (2026)\nRoboRefer: towards spatial referring with reasoning in vision-language models for robotics\n.\nExternal Links:\n2506.04308\n,\nLink\nCited by:\nÂ§2\n.\nZ. Zhou, Y. Zhu, M. Zhu, J. Wen, N. Liu, Z. Xu, W. Meng, R. Cheng, Y. Peng, C. Shen, and F. Feng (2025)\nChatVLA: unified multimodal understanding and robot control with vision-language-action model\n.\nIn\nProceedings of the Empirical Methods in Natural Language Processing (EMNLP)\n,\nCited by:\nÂ§1\n,\nÂ§2\n.\nC. Zhu, T. Wang, W. Zhang, J. Pang, and X. Liu (2025)\nLLaVA-3d: a simple yet effective pathway to empowering lmms with 3d-awareness\n.\nExternal Links:\n2409.18125\nCited by:\nÂ§2\n.\nAppendix A\nTraining Hyperparameters\nWe instantiate the VLM backbones by initializing the weights from Qwen2.5-VL-3B-Instruct and Qwen3-VL-4B-Instruct. The model is fine-tuned for 40k steps on a cluster of 16 GPUs (batch size 16 per device). We employ the AdamW\n(Loshchilov and Hutter,\n2019\n)\noptimizer initialized with a learning rate of 1e-5 and a cosine annealing schedule. System-level optimizations include DeepSpeed ZeRO-2\n(Loshchilov and Hutter,\n2019\n)\n, gradient clipping at a norm of 1.0, and no gradient accumulation. Our training pipeline is built upon the starVLA\n(starVLA,\n2025\n)\nframework.\nAppendix B\nVLM Prompt Template\nThis section presents the prompt templates used for VLM input during the VLA training process.\nSystem Prompt Template for Left Brain\nYou are a helpful robot brain that can understand images and texts.\nSystem Prompt Template for Right Brain\nYou are a helpful robot brain that can understand images, texts, and robot states.\nYou will be provided with observation, an instruction, and the robot state. Take action to execute the instruction.\nUser Prompt Template for Left Brain\n<image>\nInstruction: {instruction}\nUser Prompt Template for Right Brain\n<image>\nInstruction: {instruction}\nRobot Type: {robot_type}\nPredict the next action for the robot based on the observation, instruction and robot type.",
    "preview_text": "Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.\n\nTwinBrainVLA\n: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers\nBin Yu\n1,2,\nShijie Lian\n2,4,\n1\n1\nfootnotemark:\n1\nXiaopeng Lin\n2,5,\n1\n1\nfootnotemark:\n1\nYuliang Wei\n1,\nZhaolong Shen\n2,6\nChangti Wu\n2,7\nYuzhuo Miao\n1,2\nXinming Wang\n2,8\nBailing Wang\n1\nCong Huang\n2,3\nKai Chen\n2,3,9,\n2\n2\nfootnotemark:\n2\n1\nHIT\n2\nZGCA\n3\nZGCI\n4\nHUST\n5\nHKUST(GZ)\n6\nBUAA\n7\nECNU\n8\nCASIA\n9\nDeepCybo\nEqual contributionCorrespo",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "VLA",
        "VLM",
        "Flow Matching",
        "whole body control"
    ],
    "one_line_summary": "TwinBrainVLAé€šè¿‡éå¯¹ç§°æ··åˆTransformeræ¶æ„ï¼Œç»“åˆé€šç”¨VLMå’Œä¸“ç”¨VLMï¼Œåˆ©ç”¨Flow-Matchingç”Ÿæˆç²¾ç¡®è¿ç»­æ§åˆ¶ï¼Œè§£å†³å…·èº«ä»»åŠ¡ä¸­è¯­ä¹‰ç†è§£ä¸è¿åŠ¨æŠ€èƒ½å†²çªçš„é—®é¢˜ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šTwinBrainVLA\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nå½“å‰ï¼Œå°†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¾®è°ƒä¸ºè§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹æ˜¯æ„å»ºå…·èº«æ™ºèƒ½çš„ä¸»æµèŒƒå¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨ä¸€ä¸ªæ ¹æœ¬æ€§çŸ›ç›¾ï¼š**è¯­ä¹‰ç†è§£**ï¼ˆVLMçš„åŸå§‹ç›®æ ‡ï¼‰ä¸**ç²¾ç»†è¿åŠ¨æ§åˆ¶**ï¼ˆæœºå™¨äººä»»åŠ¡çš„ç›®æ ‡ï¼‰ä¹‹é—´çš„ä¼˜åŒ–ç›®æ ‡ä¸åŒ¹é…ã€‚å¯¹å•ä¸€VLMä¸»å¹²è¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒï¼Œå¾€å¾€å¯¼è‡´æ¨¡å‹åœ¨è·å¾—æ“æ§æŠ€èƒ½çš„åŒæ—¶ï¼Œå‘ç”Ÿ**ç¾éš¾æ€§é—å¿˜**ï¼Œå³ä¸§å¤±äº†åŸæœ‰çš„å¼€æ”¾ä¸–ç•Œè¯­ä¹‰ç†è§£å’ŒæŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œè¿™ä»æ ¹æœ¬ä¸Šå‰Šå¼±äº†VLAæ¨¡å‹åˆ©ç”¨é€šç”¨ä¸–ç•ŒçŸ¥è¯†å®ç°æ³›åŒ–æ§åˆ¶çš„åˆè¡·ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\nä¸ºè§£å†³ä¸Šè¿°çŸ›ç›¾ï¼Œæœ¬æ–‡å—**å¤§è„‘åŠçƒåŠŸèƒ½åä¾§åŒ–**çš„å¯å‘ï¼Œæå‡ºäº† **TwinBrainVLA** æ¡†æ¶ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š\n- **éå¯¹ç§°åŒæµæ¶æ„**ï¼šæ¨¡å‹åŒ…å«ä¸€ä¸ª**å†»ç»“çš„â€œå·¦è„‘â€**ï¼ˆé€šç”¨VLMï¼‰å’Œä¸€ä¸ª**å¯è®­ç»ƒçš„â€œå³è„‘â€**ï¼ˆä¸“ç”¨VLMï¼‰ã€‚å·¦è„‘ä¿æŒå¼ºå¤§çš„é€šç”¨è¯­ä¹‰ç†è§£ï¼Œå³è„‘åˆ™ä¸“æ³¨äºèåˆè§†è§‰ã€è¯­è¨€å’Œæœºå™¨äººæœ¬ä½“æ„ŸçŸ¥çŠ¶æ€ï¼Œä¸ºåŠ¨ä½œç”Ÿæˆæä¾›æ¡ä»¶ã€‚\n- **éå¯¹ç§°æ··åˆTransformeræœºåˆ¶**ï¼šé€šè¿‡ **AsyMoT** æœºåˆ¶ï¼Œå¯è®­ç»ƒçš„å³è„‘å¯ä»¥åŠ¨æ€åœ°æŸ¥è¯¢å¹¶èåˆæ¥è‡ªå†»ç»“å·¦è„‘çš„è¯­ä¹‰çŸ¥è¯†ï¼ˆKey-Valueå¯¹ï¼‰ï¼Œè€Œå·¦è„‘çš„å‚æ•°å®Œå…¨ä¸å—æ¢¯åº¦æ›´æ–°å½±å“ã€‚è¿™å®ç°äº†è¯­ä¹‰çŸ¥è¯†ä¸å…·èº«æ„ŸçŸ¥çš„å®‰å…¨ã€é«˜æ•ˆååŒã€‚\n- **æµåŒ¹é…åŠ¨ä½œä¸“å®¶**ï¼šé‡‡ç”¨åŸºäºæ‰©æ•£Transformerçš„åŠ¨ä½œä¸“å®¶ï¼Œä»¥å³è„‘è¾“å‡ºçš„å…·èº«è¡¨å¾ä¸ºæ¡ä»¶ï¼Œé€šè¿‡æµåŒ¹é…ç›®æ ‡ç”Ÿæˆç²¾ç¡®çš„è¿ç»­æ§åˆ¶åŠ¨ä½œã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nåœ¨ **SimplerEnv** å’Œ **RoboCasa** ä¸¤ä¸ªä»¿çœŸåŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼š\n- åœ¨SimplerEnvçš„4ä¸ªæ“ä½œä»»åŠ¡ä¸Šï¼ŒTwinBrainVLAï¼ˆåŸºäºQwen3-VL-4Bï¼‰å–å¾—äº†**62.0%**çš„å¹³å‡æˆåŠŸç‡ï¼Œè¶…è¶Šäº†åŒ…æ‹¬Isaac-GR00Tåœ¨å†…çš„å¤šä¸ªå…ˆè¿›åŸºçº¿ã€‚\n- åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„RoboCasa GR1æ¡Œé¢æ“ä½œåŸºå‡†ï¼ˆ24ä¸ªä»»åŠ¡ï¼‰ä¸Šï¼ŒTwinBrainVLAå–å¾—äº†**54.6%**çš„å¹³å‡æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰å¯¹æ¯”åŸºçº¿ï¼ˆå¦‚Isaac-GR00Tçš„47.6%ï¼‰ã€‚\n- å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨è·å¾—å“è¶Šæ“æ§æ€§èƒ½çš„åŒæ—¶ï¼Œ**æ˜¾å¼åœ°ä¿ç•™äº†**é¢„è®­ç»ƒVLMçš„é€šç”¨è§†è§‰ç†è§£èƒ½åŠ›ï¼ŒæˆåŠŸè§£å†³äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\nTwinBrainVLAä¸ºæ„å»ºé€šç”¨æœºå™¨äººæŒ‡æ˜äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼š**åŒæ—¶å®ç°é«˜å±‚è¯­ä¹‰ç†è§£å’Œä½å±‚ç‰©ç†çµå·§æ€§**ã€‚å…¶éå¯¹ç§°åŒè„‘è®¾è®¡ä»ç»“æ„ä¸Šè§£è€¦äº†è®¤çŸ¥ä¸æ“æ§ï¼Œä½¿å¾—æ¨¡å‹æ—¢èƒ½åƒâ€œä¸“å®¶â€ä¸€æ ·æ‰§è¡Œç²¾ç»†æ“ä½œï¼Œåˆèƒ½åƒâ€œé€šæ‰â€ä¸€æ ·ç†è§£å¼€æ”¾ä¸–ç•Œã€‚è¿™é¡¹å·¥ä½œä¸ä»…æå‡äº†VLAæ¨¡å‹çš„æ€§èƒ½ï¼Œæ›´é‡è¦çš„æ˜¯ä¸ºå…¶**ä¿æŒå’Œåˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒè·å¾—çš„ä¸–ç•ŒçŸ¥è¯†**æä¾›äº†å¯æ‰©å±•çš„æ¶æ„è§£å†³æ–¹æ¡ˆï¼Œå¯¹æ¨åŠ¨çœŸæ­£é€šç”¨ã€å¯äº¤äº’çš„å…·èº«æ™ºèƒ½ä½“å‘å±•å…·æœ‰é‡è¦ä»·å€¼ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šTwinBrainVLA\n\n### **ä¸€ã€æ ¸å¿ƒè¦è§£å†³çš„é—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³å½“å‰**è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹**åœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­é¢ä¸´çš„ä¸€ä¸ªæ ¹æœ¬æ€§çŸ›ç›¾ï¼š\n- **çŸ›ç›¾ç‚¹**ï¼šæ ‡å‡†çš„VLAæ¨¡å‹é€šå¸¸é€šè¿‡å¾®è°ƒä¸€ä¸ªå•ä¸€çš„ã€é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹æ¥å­¦ä¹ æœºå™¨äººæ§åˆ¶ã€‚è¿™å¯¼è‡´äº†**ç¾éš¾æ€§é—å¿˜**â€”â€”æ¨¡å‹åœ¨é€‚åº”ä½å±‚æ¬¡ã€ç²¾ç»†çš„è¿åŠ¨æ§åˆ¶æŠ€èƒ½æ—¶ï¼Œç‰ºç‰²äº†å…¶é¢„è®­ç»ƒè·å¾—çš„é«˜å±‚æ¬¡ã€é€šç”¨çš„è¯­ä¹‰ç†è§£å’Œå¼€æ”¾ä¸–ç•Œæ³›åŒ–èƒ½åŠ›ã€‚\n- **åæœ**ï¼šè¿™è¿èƒŒäº†VLAèŒƒå¼çš„åˆè¡·ï¼ˆå³åˆ©ç”¨VLMçš„é€šç”¨ä¸–ç•ŒçŸ¥è¯†æ¥å®ç°æ³›åŒ–çš„æœºå™¨äººæ§åˆ¶ï¼‰ï¼Œä½¿å¾—æ¨¡å‹ä»ä¸€ä¸ªâ€œé€šæ‰â€é€€åŒ–ä¸ºä¸€ä¸ªä»…æ“…é•¿åŠ¨ä½œçš„â€œä¸“æ‰â€ã€‚\n\n### **äºŒã€æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº† **TwinBrainVLA** æ¶æ„ï¼Œå…¶åˆ›æ–°æ€§ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹ä¸‰ä¸ªå±‚é¢ï¼š\n\n1. **æ¶æ„åˆ›æ–°ï¼šéå¯¹ç§°åŒè„‘è®¾è®¡**\n    - **æ ¸å¿ƒç†å¿µ**ï¼šå—å¤§è„‘**åŠçƒåŠŸèƒ½ä¾§åŒ–**å¯å‘ï¼Œå°†é€šç”¨è¯­ä¹‰ç†è§£ä¸å…·èº«æ„ŸçŸ¥æ§åˆ¶**è§£è€¦**ã€‚\n    - **â€œå·¦è„‘â€**ï¼šä¸€ä¸ª**å†»ç»“çš„ã€é¢„è®­ç»ƒçš„VLM**ã€‚è´Ÿè´£ä¿æŒå¼ºå¤§çš„å¼€æ”¾ä¸–ç•Œè¯­ä¹‰ç†è§£å’ŒæŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œä½œä¸ºç¨³å®šçš„â€œè¯­ä¹‰é”šâ€ã€‚\n    - **â€œå³è„‘â€**ï¼šä¸€ä¸ª**å¯è®­ç»ƒçš„ã€ç»“æ„ç›¸åŒçš„VLM**ã€‚ä¸“é—¨è´Ÿè´£å¤„ç†å…·èº«æ„ŸçŸ¥ï¼Œæ¥æ”¶è§†è§‰ã€æ–‡æœ¬**ä»¥åŠæœºå™¨äººæœ¬ä½“æ„ŸçŸ¥çŠ¶æ€**ï¼Œå­¦ä¹ ä¸è¿åŠ¨æ§åˆ¶ç›¸å…³çš„ç©ºé—´å’Œç‰©ç†æ¨ç†ã€‚\n\n2. **æœºåˆ¶åˆ›æ–°ï¼šéå¯¹ç§°æ··åˆTransformer**\n    - **Asymmetric Mixture-of-Transformers**ï¼šè¿™æ˜¯è¿æ¥åŒè„‘çš„æ ¸å¿ƒæŠ€æœ¯ã€‚\n    - **å·¥ä½œæ–¹å¼**ï¼šåœ¨æ¯ä¸€å±‚ï¼Œå¯è®­ç»ƒçš„â€œå³è„‘â€ä¼šè®¡ç®—è‡ªèº«çš„æŸ¥è¯¢å‘é‡ï¼Œä½†åŒæ—¶å°†å†»ç»“â€œå·¦è„‘â€çš„é”®å€¼å¯¹ä¸è‡ªèº«çš„é”®å€¼å¯¹**æ‹¼æ¥**èµ·æ¥ï¼Œå…±åŒå‚ä¸æ³¨æ„åŠ›è®¡ç®—ã€‚\n    - **å…³é”®ç‰¹æ€§**ï¼šé€šè¿‡**åœæ­¢æ¢¯åº¦**æ“ä½œï¼Œç¡®ä¿â€œå·¦è„‘â€çš„å‚æ•°åœ¨è®­ç»ƒä¸­å®Œå…¨ä¸è¢«æ›´æ–°ï¼Œå…¶é€šç”¨çŸ¥è¯†å¯ä»¥å®‰å…¨ã€åŠ¨æ€åœ°è¢«â€œå³è„‘â€æŸ¥è¯¢å’Œåˆ©ç”¨ï¼Œè€Œä¸ä¼šå› æœºå™¨äººæ§åˆ¶ä»»åŠ¡çš„é«˜æ–¹å·®æ¢¯åº¦è€Œå—æŸã€‚\n\n3. **è®­ç»ƒç­–ç•¥åˆ›æ–°ï¼šä¸¥æ ¼çš„éå¯¹ç§°æ›´æ–°**\n    - ä»…å¯¹â€œå³è„‘â€ã€çŠ¶æ€ç¼–ç å™¨å’ŒåŠ¨ä½œä¸“å®¶è¿›è¡Œä¼˜åŒ–ã€‚\n    - â€œå·¦è„‘â€å§‹ç»ˆä¿æŒå†»ç»“ï¼Œå…¶æä¾›çš„è¯­ä¹‰ç‰¹å¾ä½œä¸ºç¨³å®šçš„æ¡ä»¶è¾“å…¥ï¼Œä»ç»“æ„ä¸Šé¿å…äº†ç¾éš¾æ€§é—å¿˜ã€‚\n\n### **ä¸‰ã€è§£å†³æ–¹æ¡ˆçš„å®Œæ•´æµç¨‹**\n1. **è¾“å…¥å¤„ç†**ï¼š\n    - **å·¦è„‘**ï¼šæ¥æ”¶å›¾åƒå’Œæ–‡æœ¬æŒ‡ä»¤ã€‚\n    - **å³è„‘**ï¼šé¢å¤–æ¥æ”¶é€šè¿‡è½»é‡çº§MLPç¼–ç çš„æœºå™¨äººæœ¬ä½“çŠ¶æ€ï¼ˆå¦‚å…³èŠ‚è§’åº¦ã€æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿ï¼‰ã€‚\n\n2. **çŸ¥è¯†èåˆä¸è¡¨å¾ç”Ÿæˆ**ï¼š\n    - åŒè„‘é€šè¿‡ **AsyMoT æœºåˆ¶**è¿›è¡Œäº¤äº’ã€‚â€œå³è„‘â€åœ¨è‡ªèº«æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒæŒç»­ä»â€œå·¦è„‘â€æ±²å–é€šç”¨è¯­ä¹‰çŸ¥è¯†ï¼Œå¹¶å°†å…¶ä¸å…·ä½“çš„æœ¬ä½“æ„ŸçŸ¥ä¿¡æ¯èåˆã€‚\n    - æœ€ç»ˆï¼Œâ€œå³è„‘â€è¾“å‡ºçš„ã€å¯Œå«ç©ºé—´å’Œä»»åŠ¡ä¿¡æ¯çš„è¡¨å¾ï¼Œä½œä¸ºç”ŸæˆåŠ¨ä½œçš„**ä¸»è¦æ¡ä»¶**ã€‚\n\n3. **åŠ¨ä½œç”Ÿæˆ**ï¼š\n    - é‡‡ç”¨åŸºäº**æµåŒ¹é…**çš„æ‰©æ•£Transformerä½œä¸ºåŠ¨ä½œä¸“å®¶ã€‚\n    - è¯¥ä¸“å®¶ä»¥â€œå³è„‘â€çš„è¡¨å¾ä¸ºæ¡ä»¶ï¼Œå»å™ªå¹¶ç”Ÿæˆç²¾ç¡®çš„è¿ç»­æœºå™¨äººæ§åˆ¶åŠ¨ä½œã€‚\n\n4. **è®­ç»ƒç›®æ ‡**ï¼š\n    - **ä»…ä½¿ç”¨æœºå™¨äººåŠ¨ä½œæŸå¤±**è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€æ··åˆé€šç”¨è§†è§‰-è¯­è¨€æ•°æ®æˆ–è¾…åŠ©æŸå¤±å‡½æ•°ã€‚\n    - ç”±äºæ¶æ„çš„è§£è€¦è®¾è®¡ï¼Œå³ä½¿åªä¼˜åŒ–åŠ¨ä½œæŸå¤±ï¼Œâ€œå·¦è„‘â€çš„é€šç”¨èƒ½åŠ›ä¹Ÿèƒ½å¾—åˆ°å®Œå¥½ä¿å­˜ã€‚\n\n### **å››ã€å®é™…ä»·å€¼ä¸éªŒè¯**\n- **æ€§èƒ½æå‡**ï¼šåœ¨ SimplerEnv å’Œ RoboCasa ä¸¤ä¸ªä¸»æµä»¿çœŸåŸºå‡†æµ‹è¯•ä¸­ï¼ŒTwinBrainVLA å‡å–å¾—äº†**æœ€å…ˆè¿›çš„æ“æ§æ€§èƒ½**ï¼Œè¶…è¶Šäº†åŒ…æ‹¬ Isaac-GR00Tã€Ï€â‚€.â‚… åœ¨å†…çš„å¤šä¸ªå¼ºå¤§åŸºçº¿ã€‚\n- **èƒ½åŠ›ä¿ç•™**ï¼š**åŒæ—¶å®ç°äº†é«˜æ€§èƒ½çš„æœºå™¨äººæ§åˆ¶ä¸å®Œæ•´çš„VLMè¯­ä¹‰èƒ½åŠ›**ï¼ŒçœŸæ­£å…‘ç°äº†â€œé€šæ‰VLMç”¨äºå…·èº«ä»»åŠ¡â€çš„æ½œåŠ›ã€‚\n- **æ–¹å‘æ€§æ„ä¹‰**ï¼šä¸ºæ„å»º**å…¼å…·é«˜çº§è®¤çŸ¥å’Œä½çº§çµå·§æ€§**çš„é€šç”¨æœºå™¨äººæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–°æ¶æ„èŒƒå¼ï¼Œå³é€šè¿‡â€œåˆ†å·¥åä½œâ€è€Œéâ€œå•ä¸€æ¨¡å‹å…¨èƒ½â€çš„æ–¹å¼æ¥åŒ–è§£æ ¹æœ¬æ€§çŸ›ç›¾ã€‚\n\n**æ€»ç»“**ï¼šTwinBrainVLA çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºï¼Œå®ƒé€šè¿‡ä¸€ä¸ª**ç»“æ„ä¸Šè§£è€¦ã€æœºåˆ¶ä¸Šèåˆ**çš„éå¯¹ç§°åŒè„‘æ¶æ„ï¼Œç³»ç»Ÿæ€§åœ°è§£å†³äº†VLAæ¨¡å‹ä¸­çš„â€œç¾éš¾æ€§é—å¿˜â€éš¾é¢˜ï¼Œä½¿å¾—æœºå™¨äººæ—¢èƒ½â€œæ€è€ƒâ€åˆèƒ½â€œåŠ¨æ‰‹â€ï¼Œä¸ºè¿ˆå‘é€šç”¨å…·èº«æ™ºèƒ½æä¾›äº†å…³é”®çš„æŠ€æœ¯è·¯å¾„ã€‚",
            "timestamp": "2026-01-27T15:54:35.559470",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡ã€ŠTwinBrainVLAã€‹åˆ†æä¸å‘å±•è„‰ç»œæ¢³ç†\n\n### ä¸€ã€æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ä¸ä»·å€¼\n\n**TwinBrainVLA** çš„æ ¸å¿ƒè´¡çŒ®åœ¨äº**è§£å†³äº†ä¸€ä¸ªæ ¹æœ¬æ€§çŸ›ç›¾**ï¼šä¼ ç»Ÿè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å¾®è°ƒç”¨äºæœºå™¨äººæ§åˆ¶æ—¶ï¼Œä¼šå› ç›®æ ‡å†²çªå¯¼è‡´**ç¾éš¾æ€§é—å¿˜**â€”â€”æ¨¡å‹ä¸ºäº†å­¦ä¹ ç²¾ç»†çš„è¿åŠ¨æ§åˆ¶æŠ€èƒ½ï¼Œç‰ºç‰²äº†å…¶é¢„è®­ç»ƒè·å¾—çš„é€šç”¨è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚\n\n**å…³é”®æŠ€æœ¯çªç ´**ï¼š\n1.  **æ¶æ„åˆ›æ–°ï¼ˆä¸å¯¹ç§°åŒè„‘è®¾è®¡ï¼‰**ï¼š\n    - **å·¦è„‘**ï¼šå†»ç»“çš„é€šç”¨VLMï¼Œä¿ç•™å¼€æ”¾ä¸–ç•Œçš„è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚\n    - **å³è„‘**ï¼šå¯è®­ç»ƒçš„ä¸“ç”¨VLMï¼Œä¸“æ³¨äºå…·èº«æ„ŸçŸ¥ï¼ˆèåˆè§†è§‰ã€è¯­è¨€å’Œæœ¬ä½“æ„Ÿè§‰çŠ¶æ€ï¼‰ã€‚\n    - **ä»·å€¼**ï¼šç»“æ„ä¸Šè§£è€¦äº†â€œè®¤çŸ¥â€ä¸â€œæ§åˆ¶â€ï¼Œä½¿æ¨¡å‹èƒ½åŒæ—¶ä¿æŒé«˜å±‚æ¬¡çš„è¯­ä¹‰èƒ½åŠ›å’Œä½å±‚æ¬¡çš„è¿åŠ¨æŠ€èƒ½ã€‚\n\n2.  **æœºåˆ¶åˆ›æ–°ï¼ˆéå¯¹ç§°æ··åˆTransformer - AsyMoTï¼‰**ï¼š\n    - å…è®¸å¯è®­ç»ƒçš„å³è„‘é€šè¿‡**è”åˆæ³¨æ„åŠ›**ï¼ŒåŠ¨æ€æŸ¥è¯¢å¹¶èåˆæ¥è‡ªå†»ç»“å·¦è„‘çš„è¯­ä¹‰çŸ¥è¯†ï¼ˆKey-Valueå¯¹ï¼‰ã€‚\n    - é‡‡ç”¨**åœæ­¢æ¢¯åº¦**æ“ä½œï¼Œç¡®ä¿å·¦è„‘çš„å‚æ•°å’Œè¯­ä¹‰è¡¨å¾åœ¨è®­ç»ƒä¸­ä¿æŒç¨³å®šï¼Œä¸è¢«æœºå™¨äººæ§åˆ¶ä»»åŠ¡çš„é«˜æ–¹å·®æ¢¯åº¦ç ´åã€‚\n    - **ä»·å€¼**ï¼šå®ç°äº†ä»é€šç”¨çŸ¥è¯†åˆ°ä¸“ç”¨æŠ€èƒ½çš„å®‰å…¨ã€é«˜æ•ˆçŸ¥è¯†è¿ç§»ï¼Œæ˜¯è§£å†³ç¾éš¾æ€§é—å¿˜çš„å…³é”®ã€‚\n\n3.  **è®­ç»ƒç­–ç•¥åˆ›æ–°**ï¼š\n    - **éå¯¹ç§°æ›´æ–°è§„åˆ™**ï¼šä»…æ›´æ–°å³è„‘ã€çŠ¶æ€ç¼–ç å™¨å’ŒåŠ¨ä½œä¸“å®¶ï¼Œå·¦è„‘å®Œå…¨å†»ç»“ã€‚\n    - **å•ä¸€åŠ¨ä½œç›®æ ‡**ï¼šä»…ä½¿ç”¨æµåŒ¹é…åŠ¨ä½œæŸå¤±è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€æ··åˆé€šç”¨VLMæ•°æ®æˆ–è¾…åŠ©æŸå¤±ï¼Œç®€åŒ–äº†è®­ç»ƒæµç¨‹ã€‚\n    - **ä»·å€¼**ï¼šè¯æ˜äº†é€šè¿‡å·§å¦™çš„æ¶æ„è®¾è®¡ï¼Œå¯ä»¥ä»…ç”¨æœºå™¨äººæ•°æ®è®­ç»ƒï¼Œå°±èƒ½åŒæ—¶è·å¾—å“è¶Šçš„æ§åˆ¶æ€§èƒ½å’Œä¿ç•™çš„è¯­ä¹‰èƒ½åŠ›ã€‚\n\n**å®é™…ä»·å€¼**ï¼š\n- **ä¸ºé€šç”¨æœºå™¨äººæä¾›æ–°èŒƒå¼**ï¼šä¸ºå®ç°åŒæ—¶å…·å¤‡â€œå¤§è„‘â€ï¼ˆç†è§£ï¼‰å’Œâ€œå°è„‘â€ï¼ˆæ§åˆ¶ï¼‰çš„æœºå™¨äººæŒ‡æ˜äº†æ–¹å‘ã€‚\n- **æå‡æ•°æ®æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›**ï¼šé€šè¿‡åˆ©ç”¨å†»ç»“çš„é€šç”¨VLMå…ˆéªŒï¼Œå¯èƒ½å‡å°‘å¯¹å¤§è§„æ¨¡æœºå™¨äººæ¼”ç¤ºæ•°æ®çš„ä¾èµ–ï¼Œå¹¶å¢å¼ºåœ¨å¼€æ”¾åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚\n- **å·¥ç¨‹å‹å¥½**ï¼šåŸºäºç°æœ‰å¼ºå¤§VLMï¼ˆå¦‚Qwen-VLï¼‰æ„å»ºï¼Œåˆ©ç”¨äº†æˆç†Ÿç”Ÿæ€ã€‚\n\n### äºŒã€VLAæ¨¡å‹å‘å±•è„‰ç»œä¸æ¼”è¿›æ–¹å‘\n\nè¯¥ç ”ç©¶å»ºç«‹åœ¨è¿‘å¹´æ¥**è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹**çš„å¿«é€Ÿå‘å±•ä¹‹ä¸Šã€‚å…¶å‘å±•è„‰ç»œå¯æ¢³ç†ä¸ºä»¥ä¸‹å‡ ä¸ªé˜¶æ®µå’Œæ–¹å‘ï¼š\n\n#### **æ–¹å‘ä¸€ï¼šæ¶æ„æ¼”è¿›â€”â€”ä»â€œå•ä½“å¾®è°ƒâ€åˆ°â€œè§£è€¦è®¾è®¡â€**\n- **æ—©æœŸå·¥ä½œï¼ˆå¦‚OpenVLAï¼‰**ï¼š**ç›´æ¥å¾®è°ƒ**ä¸€ä¸ªå•ä¸€çš„ã€é¢„è®­ç»ƒçš„VLMä¸»å¹²ç½‘ç»œï¼Œä¸ºå…¶æ·»åŠ åŠ¨ä½œè¾“å‡ºå¤´ã€‚è¿™å¯¼è‡´äº†è®ºæ–‡ä¸­æŒ‡å‡ºçš„**ç¾éš¾æ€§é—å¿˜**é—®é¢˜ã€‚\n- **æ”¹è¿›å·¥ä½œ**ï¼šå¼€å§‹å°è¯•**éƒ¨åˆ†è§£è€¦**ã€‚ä¾‹å¦‚ï¼Œä¸€äº›å·¥ä½œä»…å¾®è°ƒVLMä¸­çš„éƒ¨åˆ†æ¨¡å—ï¼ˆå¦‚é€‚é…å™¨ï¼‰ï¼Œæˆ–å¼•å…¥é¢å¤–çš„ç½‘ç»œåˆ†æ”¯æ¥å¤„ç†ç©ºé—´ä¿¡æ¯ï¼Œä½†ä¸»å¹²ç½‘ç»œä»å¯èƒ½è¢«æ›´æ–°ã€‚\n- **TwinBrainVLAçš„è´¡çŒ®**ï¼šæå‡ºäº†**å½»åº•çš„ç»“æ„åŒ–è§£è€¦**ã€‚é€šè¿‡â€œåŒè„‘â€éå¯¹ç§°æ¶æ„ï¼Œä»æ ¹æœ¬ä¸Šéš”ç¦»äº†è¯­ä¹‰ç†è§£å’Œè¿åŠ¨æ§åˆ¶çš„å­¦ä¹ è¿‡ç¨‹ã€‚è¿™æ˜¯å¯¹â€œç¾éš¾æ€§é—å¿˜â€é—®é¢˜çš„**æ ¹æœ¬æ€§è§£å†³æ–¹æ¡ˆ**ï¼Œè€Œéç¼“è§£æªæ–½ã€‚\n\n#### **æ–¹å‘äºŒï¼šè®­ç»ƒç­–ç•¥æ¼”è¿›â€”â€”ä»â€œçº¯æ§åˆ¶â€åˆ°â€œå¤šä»»åŠ¡ä¿ç•™â€**\n- **åˆæœŸç­–ç•¥**ï¼šä»…åœ¨æœºå™¨äººæ•°æ®é›†ä¸Šå¾®è°ƒï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–åŠ¨ä½œé¢„æµ‹è¯¯å·®ã€‚è¿™ç›´æ¥å¯¼è‡´è¯­ä¹‰èƒ½åŠ›é€€åŒ–ã€‚\n- **æ··åˆè®­ç»ƒç­–ç•¥**ï¼šåç»­å·¥ä½œï¼ˆå¦‚ `Actions as Language`ï¼‰å°è¯•åœ¨è®­ç»ƒä¸­æ··åˆé€šç”¨è§†è§‰-è¯­è¨€å¯¹è¯æ•°æ®å’Œæœºå™¨äººæ•°æ®ï¼Œä½¿ç”¨**å¤šä»»åŠ¡æŸå¤±**ï¼ˆå¦‚ä¸‹ä¸€ä¸ªè¯é¢„æµ‹æŸå¤± + åŠ¨ä½œæŸå¤±ï¼‰ï¼Œæ—¨åœ¨ä¿ç•™VLMçš„å¯¹è¯èƒ½åŠ›ã€‚\n- **TwinBrainVLAçš„ç­–ç•¥**ï¼š**å›å½’å•ä¸€ç›®æ ‡ï¼Œä½†é€šè¿‡æ¶æ„ä¿è¯ä¿ç•™**ã€‚å®ƒä»…ä½¿ç”¨åŠ¨ä½œæŸå¤±ï¼Œä½†ä¾é å†»ç»“çš„å·¦è„‘æ¥**ç»“æ„æ€§åœ°ã€éšå¼åœ°**ä¿ç•™æ‰€æœ‰è¯­ä¹‰èƒ½åŠ›ã€‚è¿™ç®€åŒ–äº†è®­ç»ƒç›®æ ‡ï¼Œé¿å…äº†å¤šä»»åŠ¡å­¦ä¹ çš„æƒè¡¡å’Œå¤æ‚æ€§ã€‚\n\n#### **æ–¹å‘ä¸‰ï¼šç©ºé—´ä¸å…·èº«æ„ŸçŸ¥å¢å¼ºâ€”â€”ä»â€œè¯­ä¹‰VLMâ€åˆ°â€œå…·èº«VLMâ€**\n- **é—®é¢˜æ„è¯†**ï¼šé€šç”¨VLMåœ¨**ç©ºé—´æ¨ç†**ï¼ˆå¦‚ç›¸å¯¹ä½ç½®ã€è·ç¦»ã€å‡ ä½•ï¼‰æ–¹é¢å­˜åœ¨å›ºæœ‰å¼±ç‚¹ï¼Œè€Œè¿™å¯¹äºæœºå™¨äººæ§åˆ¶è‡³å…³é‡è¦ã€‚\n- **ä¸“ç”¨æ¨¡å‹æ¶Œç°**ï¼šå‡ºç°äº†ä¸“é—¨ä¸ºå…·èº«åœºæ™¯è®¾è®¡çš„**å…·èº«åŸºç¡€æ¨¡å‹**ã€‚è¿™äº›æ¨¡å‹é€šè¿‡åœ¨åè®­ç»ƒé˜¶æ®µæ³¨å…¥ç©ºé—´æ„ŸçŸ¥æ•°æ®æˆ–è®¾è®¡ç©ºé—´æ„ŸçŸ¥æ¨¡å—ï¼Œå¢å¼ºVLMçš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚\n- **TwinBrainVLAçš„å®šä½**ï¼šå®ƒæ²¡æœ‰é€‰æ‹©åˆ›å»ºä¸€ä¸ªå…¨æ–°çš„â€œå…·èº«VLMâ€ï¼Œè€Œæ˜¯**é€šè¿‡æ¶æ„æ•´åˆæ¥å¼¥è¡¥**ã€‚å…¶â€œå³è„‘â€é€šè¿‡æ¥æ”¶**æœ¬ä½“æ„Ÿè§‰çŠ¶æ€**ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¸“æ³¨äºä»è§†è§‰è§‚å¯Ÿä¸­æå–ä¸è¿åŠ¨æ§åˆ¶ç›¸å…³çš„ç‰¹å¾ï¼Œä»è€Œå…·å¤‡äº†â€œå…·èº«æ„ŸçŸ¥â€ specializationã€‚åŒæ—¶ï¼Œå®ƒé€šè¿‡AsyMoTä»â€œå·¦è„‘â€è·å–é€šç”¨è¯­ä¹‰ä½œä¸ºè¡¥å……ã€‚\n\n#### **æ–¹å‘å››ï¼šåŠ¨ä½œè¡¨ç¤ºä¸ç”Ÿæˆæ¼”è¿›â€”â€”ä»â€œç¦»æ•£åŒ–â€åˆ°â€œè¿ç»­æµåŒ¹é…â€**\n- **æ—©æœŸåŠ¨ä½œè¡¨ç¤º**ï¼šå°†è¿ç»­åŠ¨ä½œç¦»æ•£åŒ–ä¸ºä»¤ç‰Œï¼Œä½¿ç”¨VLMçš„è¯­è¨€æ¨¡å‹å¤´è¿›è¡Œè‡ªå›å½’é¢„æµ‹ã€‚\n- **å…ˆè¿›åŠ¨ä½œç”Ÿæˆ**ï¼šé‡‡ç”¨**æ‰©æ•£Transformer**å’Œ**æµåŒ¹é…**ç­‰ç”Ÿæˆå¼æ¨¡å‹æ¥ç›´æ¥å»ºæ¨¡è¿ç»­åŠ¨ä½œåˆ†å¸ƒã€‚è¿™å·²æˆä¸ºå½“å‰SOTA VLAæ¨¡å‹ï¼ˆå¦‚Isaac-GR00T, Ï€â‚€ï¼‰çš„æ ‡å‡†é…ç½®ã€‚\n- **TwinBrainVLAçš„é‡‡ç”¨**ï¼šè®ºæ–‡é‡‡ç”¨äº†åŸºäºDiTçš„æµåŒ¹é…åŠ¨ä½œä¸“å®¶ï¼Œè¿™è¡¨æ˜å®ƒèšç„¦äº**æ¶æ„åˆ›æ–°**ï¼Œè€Œåœ¨åŠ¨ä½œç”Ÿæˆå±‚é¢åˆ™é›†æˆäº†å½“å‰é¢†åŸŸå…¬è®¤çš„æœ€ä½³å®è·µã€‚å…¶åˆ›æ–°ç‚¹åœ¨äº**åŠ¨ä½œä¸“å®¶çš„æ¡ä»¶è¾“å…¥æº**â€”â€”æ¥è‡ªå…¶ä¸“æœ‰çš„ã€ç»è¿‡å…·èº«æ„ŸçŸ¥è®­ç»ƒçš„â€œå³è„‘â€è¡¨å¾ã€‚\n\n### ä¸‰ã€æ€»ç»“ï¼šTwinBrainVLAåœ¨è„‰ç»œä¸­çš„ä½ç½®\n\n`TwinBrainVLA` ä»£è¡¨äº†VLAæ¨¡å‹å‘å±•çš„ä¸€ä¸ª**èåˆä¸çªç ´ç‚¹**ï¼š\n1.  **å®ƒå¸æ”¶äº†æ¶æ„è§£è€¦çš„æ€æƒ³**ï¼Œå¹¶å°†å…¶æ¨å‘æè‡´ï¼Œå½¢æˆäº†æ¸…æ™°çš„ä¸å¯¹ç§°åŒè„‘èŒƒå¼ã€‚\n2.  **å®ƒæ‘’å¼ƒäº†å¤æ‚çš„å¤šä»»åŠ¡æ··åˆè®­ç»ƒ**ï¼Œç”¨ç®€æ´çš„æ¶æ„è®¾è®¡å®ç°äº†æ›´æ ¹æœ¬çš„èƒ½åŠ›ä¿ç•™ã€‚\n3.  **å®ƒæ²¡æœ‰ä»å¤´æ„å»ºå…·èº«VLM**ï¼Œè€Œæ˜¯é€šè¿‡åŒè„‘åˆ†å·¥ä¸èåˆï¼Œè®©é€šç”¨VLMå’Œä¸“ç”¨æ§åˆ¶ç½‘ç»œå„å¸å…¶èŒã€ååŒå·¥ä½œã€‚\n4.  **å®ƒé›†æˆäº†å…ˆè¿›çš„è¿ç»­åŠ¨ä½œç”ŸæˆæŠ€æœ¯**ï¼Œç¡®ä¿åœ¨æ§åˆ¶æ€§èƒ½ä¸Šä¸è½åã€‚\n\n**æœªæ¥å‘å±•æ–¹å‘ï¼ˆåŸºäºè®ºæ–‡å±•æœ›ï¼‰**ï¼š\n- **æ›´çµæ´»çš„æ¶æ„é…å¯¹**ï¼šæ”¯æŒä¸åŒæ¶æ„ã€ä¸åŒå¤§å°çš„â€œå·¦è„‘â€å’Œâ€œå³è„‘â€ç»„åˆã€‚\n- **èå…¥é¢†åŸŸä¸“ç”¨æ¨¡å‹**ï¼šç”¨åè®­ç»ƒçš„â€œå…·èº«VLMâ€åˆå§‹åŒ–â€œå³è„‘â€ï¼Œè¿›ä¸€æ­¥æå‡èµ·ç‚¹ã€‚\n- **æ›´å¤§è§„æ¨¡æ•°æ®è®­ç»ƒ**ï¼šåœ¨å®Œæ•´OXEç­‰æ•°æ®é›†ä¸ŠéªŒè¯æ¶æ„çš„æ‰©å±•æ€§ã€‚\n- **ç°å®ä¸–ç•ŒéªŒè¯**ï¼šå°†ä»¿çœŸä¸­çš„æˆåŠŸè¿ç§»åˆ°çœŸå®æœºå™¨äººå¤æ‚ç¯å¢ƒä¸­ã€‚\n\næ€»ä¹‹ï¼Œ`TwinBrainVLA` é€šè¿‡ä¸€ä¸ªä¼˜é›…çš„ç¥ç»æ¶æ„è®¾è®¡ï¼Œç³»ç»Ÿæ€§åœ°è§£å†³äº†VLAé¢†åŸŸçš„æ ¸å¿ƒçŸ›ç›¾ï¼Œä¸ºæ„å»ºçœŸæ­£â€œçŸ¥è¡Œåˆä¸€â€çš„é€šç”¨æœºå™¨äººæ™ºèƒ½ä½“æä¾›äº†å¼ºæœ‰åŠ›çš„æ–°æ¡†æ¶ã€‚",
            "timestamp": "2026-01-27T15:55:32.634897",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œ**TwinBrainVLA** æ˜¯ä½œè€…å›¢é˜Ÿåœ¨ Vision-Language-Action æ¨¡å‹é¢†åŸŸçš„æœ€æ–°å·¥ä½œã€‚è®ºæ–‡ä¸­æ˜ç¡®å¼•ç”¨äº†è¯¥å›¢é˜Ÿè¿‘æœŸå‘è¡¨çš„ä¸€ç¯‡ç›¸å…³å‰ä½œ **PhysBrain**ã€‚ä»¥ä¸‹æ˜¯å¯¹å…¶å‰ä½œçš„è¯¦ç»†åˆ†æã€‚\n\n### å‰ä½œæ¦‚è§ˆè¡¨æ ¼\n\n| è®ºæ–‡æ ‡é¢˜ | å‘è¡¨å¹´ä»½/çŠ¶æ€ | æ ¸å¿ƒåˆ›æ–°ç‚¹ | ä¸ TwinBrainVLA çš„å…³ç³» |\n| :--- | :--- | :--- | :--- |\n| **PhysBrain: human egocentric data as a bridge from vision language models to physical intelligence** | 2025 (é¢„å°æœ¬) | æå‡ºåˆ©ç”¨**äººç±»ç¬¬ä¸€è§†è§’ï¼ˆEgocentricï¼‰æ•°æ®**ä½œä¸ºæ¡¥æ¢ï¼Œæ¥å¢å¼º VLM çš„ç‰©ç†æ™ºèƒ½ï¼ˆPhysical Intelligenceï¼‰ï¼Œä»¥æ›´å¥½åœ°æœåŠ¡äºå…·èº«ä»»åŠ¡ã€‚ | **ç›´æ¥å‰ä½œ**ã€‚TwinBrainVLA åœ¨æ–¹æ³•è®ºä¸Šè¿›è¡Œäº†æ ¹æœ¬æ€§çš„é©æ–°ï¼Œä» **â€œæ•°æ®æ¡¥æ¢â€** æ€è·¯è½¬å‘äº† **â€œæ¶æ„è§£è€¦â€** æ€è·¯ã€‚ |\n\n---\n\n### å‰ä½œ **PhysBrain** è¯¦è§£\n\n1.  **æ ¸å¿ƒé—®é¢˜**ï¼š\n    *   ä¸ TwinBrainVLA å…³æ³¨ç›¸åŒçš„é—®é¢˜ï¼šå¦‚ä½•è®©é€šç”¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ›´å¥½åœ°é€‚åº”éœ€è¦ä½å±‚çº§ã€ç²¾ç»†åŒ–è¿åŠ¨æ§åˆ¶çš„å…·èº«ä»»åŠ¡ã€‚\n    *   **PhysBrain çš„åˆ‡å…¥ç‚¹**ï¼šè®¤ä¸ºé€šç”¨ VLM åœ¨é¢„è®­ç»ƒæ—¶ç¼ºä¹å¯¹ç‰©ç†ä¸–ç•Œäº¤äº’å’Œç©ºé—´å…³ç³»çš„ç†è§£ï¼ˆå³â€œç‰©ç†æ™ºèƒ½â€ä¸è¶³ï¼‰ï¼Œè¿™æ˜¯å…¶ç›´æ¥ç”¨äºæœºå™¨äººæ§åˆ¶çš„ç“¶é¢ˆã€‚\n\n2.  **è§£å†³æ–¹æ¡ˆï¼ˆä¸»è¦è´¡çŒ®ï¼‰**ï¼š\n    *   **æ•°æ®é©±åŠ¨**ï¼šæå‡ºæ”¶é›†å’Œåˆ©ç”¨å¤§è§„æ¨¡**äººç±»ç¬¬ä¸€è§†è§’è§†é¢‘æ•°æ®**ï¼ˆå¦‚ Ego4D, Epic-Kitchensï¼‰ä½œä¸ºâ€œæ¡¥æ¢â€æ•°æ®é›†ã€‚\n    *   **ä¸¤é˜¶æ®µè®­ç»ƒ**ï¼š\n        1.  **ç‰©ç†æ™ºèƒ½æ³¨å…¥**ï¼šåœ¨é€šç”¨ VLMï¼ˆå¦‚ Qwen-VLï¼‰ä¸Šï¼Œä½¿ç”¨äººç±»ç¬¬ä¸€è§†è§’æ•°æ®åŠå…¶å¯¹åº”çš„æ–‡æœ¬æè¿°ï¼ˆæ¶‰åŠç‰©ä½“ã€åŠ¨ä½œã€ç©ºé—´å…³ç³»ï¼‰è¿›è¡Œ**ç»§ç»­é¢„è®­ç»ƒ**ã€‚ç›®æ ‡æ˜¯è®© VLM å­¦ä¼šä»ç¬¬ä¸€è§†è§’ç†è§£ç‰©ç†äº¤äº’ã€ç‰©ä½“åŠŸèƒ½ã€æ“ä½œæ­¥éª¤ç­‰ã€‚\n        2.  **æœºå™¨äººæ§åˆ¶å¾®è°ƒ**ï¼šå°†ç»è¿‡â€œç‰©ç†æ™ºèƒ½â€å¢å¼ºçš„ VLM ä½œä¸ºä¸»å¹²ï¼Œåœ¨æœºå™¨äººæ¼”ç¤ºæ•°æ®é›†ï¼ˆå¦‚ OXEï¼‰ä¸Šè¿›è¡Œæ ‡å‡†çš„ VLA å¾®è°ƒï¼Œå­¦ä¹ è¾“å‡ºåŠ¨ä½œã€‚\n    *   **æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡**ä¸­é—´æ•°æ®ï¼ˆç¬¬ä¸€è§†è§’è§†é¢‘ï¼‰** æ¥å¼¥åˆäº’è”ç½‘å°ºåº¦è¯­ä¹‰æ•°æ®ä¸æœºå™¨äººæ§åˆ¶æ•°æ®ä¹‹é—´çš„åˆ†å¸ƒå·®è·ï¼Œè®©æ¨¡å‹å…ˆå­¦ä¼šâ€œçœ‹äººæ€ä¹ˆåšâ€ï¼Œå†å­¦ä¼šâ€œè®©æœºå™¨äººåšâ€ã€‚\n\n3.  **ä¸ TwinBrainVLA çš„åŒºåˆ«ä¸ä¸»è¦æ”¹å–„**ï¼š\n\n    TwinBrainVLA å¹¶éå¯¹ PhysBrain çš„ç®€å•æ”¹è¿›ï¼Œè€Œæ˜¯æå‡ºäº†ä¸€ç§**å…¨æ–°çš„ã€æ¶æ„å±‚é¢çš„è§£å†³æ–¹æ¡ˆ**ï¼Œä¸»è¦åŒºåˆ«å’Œæ”¹å–„å¦‚ä¸‹ï¼š\n\n| å¯¹æ¯”ç»´åº¦ | **å‰ä½œï¼šPhysBrain** | **æœ¬æ–‡ï¼šTwinBrainVLA** | **TwinBrainVLA çš„ä¸»è¦æ”¹å–„ä¸åˆ›æ–°** |\n| :--- | :--- | :--- | :--- |\n| **æ ¸å¿ƒæ€è·¯** | **æ•°æ®æ¡¥æ¢**ï¼šç”¨æ–°æ•°æ®ï¼ˆäººç±»è§†é¢‘ï¼‰å¡«è¡¥VLMä¸æœºå™¨äººä»»åŠ¡ä¹‹é—´çš„è¯­ä¹‰-æ§åˆ¶é¸¿æ²Ÿã€‚ | **æ¶æ„è§£è€¦**ï¼šè®¾è®¡æ–°æ¨¡å‹ç»“æ„ï¼Œå°†è¯­ä¹‰ç†è§£ä¸è¿åŠ¨æ§åˆ¶åŠŸèƒ½åˆ†ç¦»åˆ°ä¸¤ä¸ªå¹¶è¡Œçš„â€œå¤§è„‘â€ä¸­ã€‚ | **ä»â€œè®­æ•°æ®â€è½¬å‘â€œæ”¹ç»“æ„â€**ã€‚æä¾›äº†ä¸€ç§æ›´æ ¹æœ¬ã€å¯èƒ½æ›´é«˜æ•ˆçš„è§£å†³è·¯å¾„ï¼Œä¸ä¾èµ–äºç‰¹å®šç±»å‹çš„å¤§è§„æ¨¡ä¸­é—´æ•°æ®ã€‚ |\n| **è§£å†³â€œç¾éš¾æ€§é—å¿˜â€çš„æ–¹å¼** | **é—´æ¥ç¼“è§£**ï¼šé€šè¿‡æ³¨å…¥ç‰©ç†æ™ºèƒ½ï¼Œå¸Œæœ›æ¨¡å‹åœ¨å¾®è°ƒæ§åˆ¶æ—¶èƒ½ä¿ç•™æ›´å¤šç›¸å…³ç©ºé—´çŸ¥è¯†ï¼Œä½†ä¸»å¹²å‚æ•°ä»éœ€å…¨éƒ¨æ›´æ–°ï¼Œé—å¿˜é£é™©ä¾ç„¶å­˜åœ¨ã€‚ | **ç»“æ„æ€§å…ç–«**ï¼š**å·¦è„‘ï¼ˆé€šç”¨VLMï¼‰å®Œå…¨å†»ç»“**ï¼Œå‚æ•°æ°¸ä¸æ›´æ–°ï¼Œä»æ ¹æœ¬ä¸Šæœç»äº†é—å¿˜ã€‚å³è„‘ä¸“é—¨å­¦ä¹ æ§åˆ¶ï¼Œå¯ä»¥å¤§èƒ†æ›´æ–°è€Œæ— éœ€æ‹…å¿ƒæŸå®³è¯­ä¹‰èƒ½åŠ›ã€‚ | **å½»åº•è§£å†³äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜**ã€‚è¿™æ˜¯æœ€æ ¸å¿ƒçš„æ”¹è¿›ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨è·å¾—é«˜è¶…æ“æ§æŠ€èƒ½çš„åŒæ—¶ï¼Œ100%ä¿ç•™å…¶åŸæœ‰çš„å¼€æ”¾ä¸–ç•Œè¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚ |\n| **æ¨¡å‹æ¶æ„** | **å•ä¸»å¹²åºåˆ—å¼**ï¼šä¸€ä¸ªVLMä¸»å¹²ï¼Œä¾æ¬¡è¿›è¡Œç‰©ç†æ™ºèƒ½é¢„è®­ç»ƒå’Œæœºå™¨äººæ§åˆ¶å¾®è°ƒã€‚ | **åŒä¸»å¹²å¹¶è¡Œå¼**ï¼šä¸¤ä¸ªåŒæ„VLMï¼ˆå·¦è„‘å’Œå³è„‘ï¼‰å¹¶è¡Œå¤„ç†ï¼Œé€šè¿‡ **AsyMoT æœºåˆ¶**è¿›è¡Œä¸å¯¹ç§°äº¤äº’ã€‚ | **å¼•å…¥äº†éå¯¹ç§°æ··åˆä¸“å®¶æ¶æ„**ã€‚å³è„‘å¯ä»¥åŠ¨æ€æŸ¥è¯¢å·¦è„‘çš„è¯­ä¹‰çŸ¥è¯†ï¼Œå®ç°äº†çŸ¥è¯†æµçš„å¯æ§èåˆï¼Œè¿™æ˜¯æ¨¡å‹èƒ½ååŒå·¥ä½œçš„å…³é”®ã€‚ |\n| **è®­ç»ƒç­–ç•¥** | **å¤šé˜¶æ®µè®­ç»ƒ**ï¼šéœ€è¦é¢å¤–çš„ã€å¤§è§„æ¨¡çš„äººç±»ç¬¬ä¸€è§†è§’æ•°æ®æ”¶é›†ä¸æ ‡æ³¨ï¼Œè®­ç»ƒæµç¨‹æ›´é•¿ã€‚ | **ï¼ˆç›¸å¯¹ï¼‰å•é˜¶æ®µè®­ç»ƒ**ï¼šä»…ä½¿ç”¨æœºå™¨äººæ¼”ç¤ºæ•°æ®è¿›è¡Œè®­ç»ƒã€‚å·¦è„‘å†»ç»“ï¼Œå³è„‘å’ŒåŠ¨ä½œä¸“å®¶ä¸€èµ·è®­ç»ƒã€‚ç®€åŒ–äº†è®­ç»ƒæµç¨‹ï¼Œé™ä½äº†å¯¹ç‰¹å®šä¸­é—´æ•°æ®çš„ä¾èµ–ã€‚ | **è®­ç»ƒæ›´é«˜æ•ˆã€æ›´ä¸“æ³¨**ã€‚æ‰€æœ‰è®­ç»ƒä¿¡å·éƒ½ç›´æ¥ç”¨äºä¼˜åŒ–æ§åˆ¶æ€§èƒ½ï¼ŒåŒæ—¶é€šè¿‡æ¶æ„ä¿è¯è¯­ä¹‰ä¸ä¸¢å¤±ã€‚ |\n| **ä¿¡æ¯æµ** | **å•å‘ã€èåˆ**ï¼šç‰©ç†æ™ºèƒ½è¢«èåˆè¿›åŒä¸€ä¸ªæ¨¡å‹å‚æ•°ä¸­ï¼Œæœ€ç»ˆè¾“å‡ºæ˜¯è¯­ä¹‰ä¸æ§åˆ¶æ··åˆçš„è¡¨å¾ã€‚ | **åŒå‘ã€æŸ¥è¯¢å¼**ï¼šæ§åˆ¶æµï¼ˆå³è„‘ï¼‰ä¸»åŠ¨å‘è¯­ä¹‰æµï¼ˆå·¦è„‘ï¼‰å‘èµ·æŸ¥è¯¢ã€‚**å³è„‘çš„æœ€ç»ˆè¡¨å¾æ˜¯ä¸“é—¨ä¸ºæ§åˆ¶ä¼˜åŒ–çš„**ï¼Œå¹¶ä½œä¸ºåŠ¨ä½œä¸“å®¶çš„å”¯ä¸€æ¡ä»¶ã€‚ | **å®ç°äº†æ›´æ¸…æ™°çš„åŠŸèƒ½åˆ†ç¦»**ã€‚åŠ¨ä½œç”Ÿæˆåªä¾èµ–äºä¸“é—¨å¤„ç†å…·èº«æ„ŸçŸ¥çš„å³è„‘è¾“å‡ºï¼Œä½¿å¾—æ§åˆ¶ç­–ç•¥æ›´å¹²å‡€ã€æ›´å¯è§£é‡Šã€‚ |\n| **å®é™…ä»·å€¼ä¾§é‡** | å¼ºè°ƒåˆ©ç”¨ä¸°å¯Œæ˜“å¾—çš„äººç±»è§†é¢‘æ•°æ®æ¥æå‡æ¨¡å‹çš„ç‰©ç†å¸¸è¯†ï¼Œæ˜¯ä¸€ç§**æ•°æ®å¢å¼º**æ–¹æ¡ˆã€‚ | å¼ºè°ƒé€šè¿‡åˆ›æ–°æ¶æ„æ¥åŒæ—¶è·å¾—é¡¶å°–çš„æ“æ§æ€§èƒ½å’Œå®Œæ•´çš„è¯­ä¹‰èƒ½åŠ›ï¼Œä¸ºæ„å»º**é€šç”¨æœºå™¨äºº**æä¾›äº†æ–°çš„è“å›¾ï¼Œæ˜¯ä¸€ç§**æ¨¡å‹æ¶æ„**æ–¹æ¡ˆã€‚ | **æŒ‡æ˜äº†æ„å»ºé€šç”¨æ™ºèƒ½ä½“çš„æ–°æ–¹å‘**ï¼šå³é€šè¿‡æ¨¡å—åŒ–ã€ä¸“ä¸šåŒ–çš„å­ç³»ç»ŸååŒï¼Œè€Œéæ‰“é€ ä¸€ä¸ªâ€œå…¨èƒ½â€ä½†å¯èƒ½â€œå¹³åº¸â€çš„å•ä¸€æ¨¡å‹ã€‚ |\n\n### æ€»ç»“\n\n**TwinBrainVLA ç›¸å¯¹äºå‰ä½œ PhysBrain æ˜¯ä¸€æ¬¡èŒƒå¼ä¸Šçš„è·ƒè¿**ï¼š\n*   **PhysBrain** æ˜¯åœ¨ç°æœ‰ **VLA å¾®è°ƒèŒƒå¼å†…**è¿›è¡Œä¼˜åŒ–ï¼Œè¯•å›¾ç”¨æ›´å¥½çš„**æ•°æ®å‡†å¤‡**æ¥å‡è½»å¾®è°ƒå¸¦æ¥çš„å‰¯ä½œç”¨ã€‚\n*   **TwinBrainVLA** åˆ™æ˜¯**é‡æ–°è®¾è®¡äº† VLA èŒƒå¼æœ¬èº«**ï¼Œé€šè¿‡å—è„‘ç§‘å­¦å¯å‘çš„**åŒè„‘éå¯¹ç§°æ¶æ„**ï¼Œä»æ ¹æºä¸Šæ‹†è§£äº†è¯­ä¹‰ä¸æ§åˆ¶çš„çŸ›ç›¾ï¼ŒåŒæ—¶è¾¾æˆäº† **SOTA çš„æ“æ§æ€§èƒ½**å’Œ**å®Œç¾çš„è¯­ä¹‰ä¿ç•™**ï¼Œå…¶åˆ›æ–°æ€§å’Œæ½œåŠ›æ›´ä¸ºæ˜¾è‘—ã€‚",
            "timestamp": "2026-01-27T15:56:26.478583",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## è®ºæ–‡å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\næœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º **TwinBrainVLA** çš„æ–°å‹è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹æ¶æ„ï¼Œå…¶æ ¸å¿ƒç›®æ ‡æ˜¯è§£å†³é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¾®è°ƒç”¨äºæœºå™¨äººæ§åˆ¶ä»»åŠ¡æ—¶å‡ºç°çš„ **â€œç¾éš¾æ€§é—å¿˜â€** é—®é¢˜ã€‚ä»¥ä¸‹æ˜¯å…¶å…³é”®æŠ€æœ¯æ–¹æ³•çš„è¯¦ç»†è¯´æ˜ã€‚\n\n### å…³é”®æŠ€æœ¯æ–¹æ³•åˆ—è¡¨\n\n| æ–¹æ³•åç§° | æ ¸å¿ƒç›®æ ‡ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°/å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. éå¯¹ç§°åŒVLMéª¨å¹²ç½‘ç»œ** | å°†é€šç”¨è¯­ä¹‰ç†è§£ä¸å…·èº«æ„ŸçŸ¥æ§åˆ¶è§£è€¦ï¼Œé¿å…ç¾éš¾æ€§é—å¿˜ã€‚ | **ç»“æ„**ï¼šä½¿ç”¨ä¸¤ä¸ªåŒæ„çš„VLMï¼Œåˆ†åˆ«ä½œä¸º **â€œå·¦è„‘â€** å’Œ **â€œå³è„‘â€**ã€‚<br>**å·¦è„‘ (â„³â‚—)**ï¼š**å®Œå…¨å†»ç»“**çš„é¢„è®­ç»ƒVLMï¼Œä»…æ¥æ”¶è§†è§‰ **I** å’Œæ–‡æœ¬æŒ‡ä»¤ **T** ä½œä¸ºè¾“å…¥ï¼š**Hâ‚—â° = [ğ’±(I); ğ’¯(T)]**ã€‚å…¶å‚æ•° Î¸â‚— åœ¨è®­ç»ƒä¸­ä¸æ›´æ–°ï¼ˆâˆ‡Î¸â‚— = 0ï¼‰ï¼Œä»¥ä¿ç•™é€šç”¨è¯­ä¹‰çŸ¥è¯†ã€‚<br>**å³è„‘ (â„³áµ£)**ï¼š**å®Œå…¨å¯è®­ç»ƒ**çš„VLMï¼Œä¸“ç”¨äºå…·èº«æ§åˆ¶ã€‚é™¤äº†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œè¿˜é€šè¿‡ä¸€ä¸ªè½»é‡çº§çŠ¶æ€ç¼–ç å™¨ **Ï•** (ä¸€ä¸ªMLP) å°†æœºå™¨äººæœ¬ä½“æ„ŸçŸ¥çŠ¶æ€ **s âˆˆ â„^{dâ‚›}** æŠ•å½±åˆ°VLMåµŒå…¥ç©ºé—´ï¼Œå¹¶æ‹¼æ¥ä¸ºè¾“å…¥ï¼š**Háµ£â° = [ğ’±(I); ğ’¯(T); Ï•(s)]**ã€‚ | **VLMéª¨å¹²**ï¼šä½¿ç”¨ Qwen2.5-VL-3B-Instruct æˆ– Qwen3-VL-4B-Instruct åˆå§‹åŒ–å·¦å³è„‘ã€‚<br>**çŠ¶æ€ç¼–ç å™¨ Ï•**ï¼šä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œå°†ä½ç»´çŠ¶æ€ **s** æŠ•å½±ä¸ºä¸VLMè¯åµŒå…¥åŒç»´çš„ä»¤ç‰Œã€‚å…·ä½“ç»´åº¦å–å†³äºæœºå™¨äººçŠ¶æ€ï¼ˆå¦‚å…³èŠ‚è§’ã€æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿ï¼‰ã€‚ |\n| **2. éå¯¹ç§°æ··åˆTransformeræœºåˆ¶** | ä½¿å¯è®­ç»ƒçš„å³è„‘èƒ½å¤ŸåŠ¨æ€æŸ¥è¯¢å¹¶èåˆå†»ç»“å·¦è„‘çš„è¯­ä¹‰çŸ¥è¯†ï¼ŒåŒæ—¶ä¸ç ´åå·¦è„‘çš„å‚æ•°ã€‚ | **åŸç†**ï¼šåœ¨Transformerçš„æ¯ä¸€å±‚ **l**ï¼Œå³è„‘é€šè¿‡ä¸€ç§**éå¯¹ç§°è”åˆæ³¨æ„åŠ›**æœºåˆ¶ï¼Œå°†å…¶è‡ªèº«çš„æŸ¥è¯¢ï¼ˆ**Qáµ£Ë¡**ï¼‰ä¸æ¥è‡ªå·¦å³è„‘çš„é”®å€¼å¯¹è¿›è¡Œäº¤äº’ã€‚<br>**é”®å€¼æ‹¼æ¥**ï¼šå°†å†»ç»“å·¦è„‘çš„é”®å€¼å¯¹ï¼ˆç»è¿‡åœæ­¢æ¢¯åº¦æ“ä½œ **sg(Â·)**ï¼‰ä¸å³è„‘è‡ªèº«çš„é”®å€¼å¯¹æ²¿åºåˆ—ç»´åº¦æ‹¼æ¥ï¼š<br>â€ƒ**K_joint = [sg(Kâ‚—Ë¡); Káµ£Ë¡]**<br>â€ƒ**V_joint = [sg(Vâ‚—Ë¡); Váµ£Ë¡]**<br>**æ³¨æ„åŠ›è®¡ç®—**ï¼šå³è„‘çš„ä¸‹ä¸€å±‚éšè—çŠ¶æ€é€šè¿‡ä»¥ä¸‹å…¬å¼è®¡ç®—ï¼š<br>â€ƒ**Háµ£Ë¡âºÂ¹ = Softmax( Qáµ£Ë¡ (K_joint)^T / âˆšdâ‚– ) V_joint + FFN(Háµ£Ë¡)**<br>å…¶ä¸­ **dâ‚–** æ˜¯é”®å‘é‡çš„ç»´åº¦ã€‚**åœæ­¢æ¢¯åº¦æ“ä½œ sg(Â·)** ç¡®ä¿æ¥è‡ªåŠ¨ä½œä¸“å®¶çš„æ¢¯åº¦ä¸ä¼šæµå‘å·¦è„‘çš„å‚æ•°ï¼Œä»è€Œä¿æŒå…¶è¯­ä¹‰çŸ¥è¯†çš„å®Œæ•´æ€§ã€‚ | **å®ç°äº**ï¼šVLMçš„æ¯ä¸€ä¸ªTransformerå±‚ã€‚**åœæ­¢æ¢¯åº¦**åœ¨è®¡ç®—å›¾ä¸­æ˜ç¡®é˜»æ–­ä»å·¦è„‘KVå¯¹å›ä¼ çš„æ¢¯åº¦ã€‚è¿™ä½¿å¾—å·¦è„‘æˆä¸ºä¸€ä¸ªç¨³å®šçš„â€œè¯­ä¹‰é”šç‚¹â€ã€‚ |\n| **3. æµåŒ¹é…åŠ¨ä½œä¸“å®¶** | ç”Ÿæˆé«˜ç²¾åº¦ã€è¿ç»­çš„æœºå™¨äººæ§åˆ¶åŠ¨ä½œã€‚ | **æ¶æ„**ï¼šé‡‡ç”¨**æ‰©æ•£Transformer** ä½œä¸ºåŠ¨ä½œä¸“å®¶ï¼Œå…¶ä»¥å³è„‘çš„æœ€ç»ˆéšè—çŠ¶æ€ **Háµ£^{final}** ä¸ºæ¡ä»¶ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å±‚æ³¨å…¥ã€‚<br>**æµåŒ¹é…å…¬å¼**ï¼šé‡‡ç”¨**æ¡ä»¶æ¦‚ç‡è·¯å¾„æµåŒ¹é…**ç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªå‘é‡åœº **v_Ïˆ**ï¼Œå°†å™ªå£°åŠ¨ä½œ **aâ‚œ** æ˜ å°„åˆ°çœŸå®åŠ¨ä½œ **aâ‚** çš„æ–¹å‘ã€‚æŸå¤±å‡½æ•°å®šä¹‰ä¸ºï¼š<br>â€ƒ**â„’_FM(Ïˆ) = ğ”¼_{t, aâ‚€, aâ‚} [ \\|\\| v_Ïˆ(aâ‚œ, t, Háµ£) - (aâ‚ - aâ‚€) \\|\\|Â² ]**<br>å…¶ä¸­ï¼š<br>â€ƒ- **t ~ ğ’°[0,1]** æ˜¯éšæœºæ—¶é—´æ­¥ã€‚<br>â€ƒ- **aâ‚€ ~ ğ’©(0, I)** æ˜¯æ ‡å‡†é«˜æ–¯å…ˆéªŒå™ªå£°ã€‚<br>â€ƒ- **aâ‚** æ˜¯åœ°é¢çœŸå®åŠ¨ä½œã€‚<br>â€ƒ- ç›®æ ‡å‘é‡åœºæ˜¯è¿æ¥ **aâ‚€** å’Œ **aâ‚** çš„ç›´çº¿ **(aâ‚ - aâ‚€)**ã€‚<br>**æ¨ç†**ï¼šé€šè¿‡æ±‚è§£å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼Œå¦‚æ¬§æ‹‰æ±‚è§£å™¨ï¼‰ï¼Œä»å™ªå£°ä¸­é‡‡æ ·ç”Ÿæˆå¹³æ»‘çš„åŠ¨ä½œè½¨è¿¹ã€‚ | **ç­–ç•¥æ¶æ„**ï¼šéµå¾ªIsaac-GR00Tå’ŒÏ€â‚€ä¸­ä½¿ç”¨çš„DiTæ¶æ„ã€‚<br>**æ¡ä»¶æ³¨å…¥**ï¼šå³è„‘çš„ **Háµ£** é€šè¿‡äº¤å‰æ³¨æ„åŠ›å±‚æä¾›ç»™DiTã€‚<br>**è®­ç»ƒç›®æ ‡**ï¼šä»…ä½¿ç”¨æœºå™¨äººåŠ¨ä½œçš„æµåŒ¹é…æŸå¤±ï¼Œä¸ä½¿ç”¨è¾…åŠ©çš„ä¸‹ä¸€ä»¤ç‰Œé¢„æµ‹æŸå¤±ã€‚ |\n| **4. éå¯¹ç§°è”åˆè®­ç»ƒç­–ç•¥** | åœ¨ä¼˜åŒ–å³è„‘å’ŒåŠ¨ä½œä¸“å®¶æ—¶ï¼Œç¡®ä¿å·¦è„‘çš„çŸ¥è¯†ä¸è¢«ç ´åã€‚ | **æ€»æŸå¤±å‡½æ•°**ï¼šè®­ç»ƒä»…æœ€å°åŒ–æµåŒ¹é…æŸå¤±ï¼š<br>â€ƒ**â„’_total = â„’_FM(Î¸áµ£, Ïˆ, Ï•; ğ’Ÿ_robot)**<br>å…¶ä¸­ **Î¸áµ£**, **Ïˆ**, **Ï•** åˆ†åˆ«æ˜¯å³è„‘ã€åŠ¨ä½œä¸“å®¶å’ŒçŠ¶æ€ç¼–ç å™¨çš„å¯è®­ç»ƒå‚æ•°ã€‚<br>**æ¢¯åº¦æµ**ï¼šæ¢¯åº¦ä»åŠ¨ä½œä¸“å®¶ï¼ˆÏˆï¼‰åå‘ä¼ æ’­ï¼Œæ›´æ–°å³è„‘ï¼ˆÎ¸áµ£ï¼‰å’ŒçŠ¶æ€ç¼–ç å™¨ï¼ˆÏ•ï¼‰ã€‚åœ¨AsyMoTå±‚ï¼Œæ¢¯åº¦è¢«æ˜ç¡®é˜»æ­¢æµå‘å†»ç»“å·¦è„‘çš„é”®å€¼å¯¹ï¼ˆ**sg(Kâ‚—), sg(Vâ‚—)**ï¼‰ã€‚ | **ä¼˜åŒ–å™¨**ï¼šAdamWï¼Œåˆå§‹å­¦ä¹ ç‡ 1e-5ï¼Œä½™å¼¦é€€ç«è°ƒåº¦ã€‚<br>**è®­ç»ƒé…ç½®**ï¼šåœ¨16ä¸ªH100 GPUä¸Šè®­ç»ƒ40kæ­¥ï¼Œæ¯è®¾å¤‡æ‰¹é‡å¤§å°16ã€‚ä½¿ç”¨DeepSpeed ZeRO-2å’Œæ¢¯åº¦è£å‰ªï¼ˆèŒƒæ•°1.0ï¼‰ã€‚<br>**æ•°æ®**ï¼šä½¿ç”¨Open X-Embodimentæ•°æ®é›†çš„å­é›†ï¼ˆBridge-V2, Fractalï¼‰ã€‚ |\n\n### æ€»ç»“ä¸åˆ›æ–°ä»·å€¼\n\n**TwinBrainVLA** çš„æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åœ¨äºå…¶ **â€œè§£è€¦â€** çš„è®¾è®¡å“²å­¦ï¼š\n1.  **ç»“æ„è§£è€¦**ï¼šé€šè¿‡éå¯¹ç§°åŒè„‘æ¶æ„ï¼Œå°†â€œè®¤çŸ¥â€ï¼ˆé€šç”¨è¯­ä¹‰ï¼‰ä¸â€œæ‰§è¡Œâ€ï¼ˆå…·èº«æ§åˆ¶ï¼‰åˆ†ç¦»ï¼Œä»æ ¹æºä¸Šé¿å…äº†å•ä¸€éª¨å¹²ç½‘ç»œåœ¨ä¼˜åŒ–ç›®æ ‡å†²çªä¸‹çš„æ€§èƒ½é€€åŒ–ã€‚\n2.  **ä¿¡æ¯èåˆè§£è€¦**ï¼šé€šè¿‡ **AsyMoT** æœºåˆ¶ï¼Œå®ç°äº†ä»å†»ç»“é€šç”¨æ¨¡å‹åˆ°å¯è®­ç»ƒä¸“ç”¨æ¨¡å‹çš„**å•å‘çŸ¥è¯†æµåŠ¨**ï¼Œæ—¢åˆ©ç”¨äº†å…ˆéªŒçŸ¥è¯†ï¼Œåˆä¿æŠ¤äº†å…¶ä¸è¢«è¦†ç›–ã€‚\n3.  **è®­ç»ƒç›®æ ‡è§£è€¦**ï¼šæ•´ä¸ªæ¨¡å‹ä»…ä½¿ç”¨æœºå™¨äººåŠ¨ä½œæ•°æ®è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œæ— éœ€æ··åˆé€šç”¨VQAæ•°æ®æˆ–æ·»åŠ å¤æ‚çš„æ­£åˆ™åŒ–æŸå¤±ï¼Œç®€åŒ–äº†è®­ç»ƒæµç¨‹ï¼Œè€Œé€šç”¨èƒ½åŠ›é€šè¿‡ç»“æ„è®¾è®¡å¾—ä»¥ä¿å…¨ã€‚\n\nå…¶å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨SimperEnvå’ŒRoboCasaåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†é¢†å…ˆçš„æ€§èƒ½ï¼ŒåŒæ—¶**æ˜¾å¼åœ°ä¿ç•™äº†**é¢„è®­ç»ƒVLMçš„è§†è§‰ç†è§£èƒ½åŠ›ï¼Œä¸ºæ„å»ºå…¼å…·é«˜çº§è¯­ä¹‰ç†è§£å’Œä½çº§ç‰©ç†çµå·§æ€§çš„é€šç”¨æœºå™¨äººæŒ‡æ˜äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ã€‚",
            "timestamp": "2026-01-27T15:57:26.817159",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡ã€ŠTwinBrainVLAã€‹çš„è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\næœ¬æ–‡é€šè¿‡**ä»¿çœŸåŸºå‡†æµ‹è¯•**æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œä¸»è¦ä½¿ç”¨äº†ä¸¤ä¸ªæ•°æ®é›†å’Œå¯¹åº”çš„æˆåŠŸç‡æŒ‡æ ‡ã€‚\n\n### ä¸€ã€ ä½¿ç”¨çš„è¯„ä»·æŒ‡æ ‡\n\n**æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼šä»»åŠ¡æˆåŠŸç‡ (Success Rate)**\n\n- **å®šä¹‰**ï¼šåœ¨ç‰¹å®šä»»åŠ¡ä¸­ï¼Œæœºå™¨äººæˆåŠŸå®ŒæˆæŒ‡ä»¤çš„è¯•éªŒæ¬¡æ•°å æ€»è¯•éªŒæ¬¡æ•°çš„ç™¾åˆ†æ¯”ã€‚\n- **è®¡ç®—æ–¹å¼**ï¼š`æˆåŠŸç‡ = (æˆåŠŸæ¬¡æ•° / æ€»è¯•éªŒæ¬¡æ•°) * 100%`\n- **æŠ¥å‘Šå½¢å¼**ï¼š\n    - **å•ä»»åŠ¡æˆåŠŸç‡**ï¼šé’ˆå¯¹æ¯ä¸ªå…·ä½“ä»»åŠ¡ï¼ˆå¦‚â€œå°†å‹ºå­æ”¾åœ¨æ¯›å·¾ä¸Šâ€ï¼‰æŠ¥å‘Šã€‚\n    - **å¹³å‡æˆåŠŸç‡ (Average)**ï¼šåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè®¡ç®—æˆåŠŸç‡çš„å¹³å‡å€¼ï¼Œç”¨äºè¡¡é‡æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚\n- **è¯•éªŒè®¾ç½®**ï¼šä¸ºå‡å°‘éšæœºæ€§ï¼Œæ¯ä¸ªä»»åŠ¡è¿›è¡Œå¤šæ¬¡ç‹¬ç«‹è¯•éªŒï¼ˆå¦‚480æ¬¡æˆ–50æ¬¡ï¼‰ï¼ŒæŠ¥å‘Šå¹³å‡æˆåŠŸç‡ã€‚\n- **æŒ‡æ ‡ç»†èŠ‚**ï¼š\n    - **SimperEnv**ï¼šæŠ¥å‘Š **Avg@480**ï¼Œå³åœ¨480æ¬¡è¯•éªŒä¸­çš„å¹³å‡æˆåŠŸç‡ã€‚\n    - **RoboCasa**ï¼šæŠ¥å‘Š **Avg@50**ï¼Œå³åœ¨50æ¬¡è¯•éªŒä¸­çš„å¹³å‡æˆåŠŸç‡ã€‚\n    - è¯¥æŒ‡æ ‡ç›´æ¥ã€å®¢è§‚ï¼Œæ˜¯æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­æœ€æ ¸å¿ƒçš„è¯„ä¼°æ ‡å‡†ï¼Œåæ˜ äº†æ¨¡å‹ç”Ÿæˆæœ‰æ•ˆæ§åˆ¶æŒ‡ä»¤çš„èƒ½åŠ›ã€‚\n\n### äºŒã€ ä½¿ç”¨çš„æ•°æ®é›†\n\nè®ºæ–‡åœ¨ä¸¤ä¸ªä¸»æµçš„æœºå™¨äººæ“ä½œä»¿çœŸåŸºå‡†ä¸Šè¿›è¡Œäº†è®­ç»ƒå’Œè¯„ä¼°ã€‚\n\n#### 1. è®­ç»ƒæ•°æ®é›†\n\n| æ•°æ®é›†åç§° | æ¥æº | ç”¨é€” | å…³é”®ç»†èŠ‚ |\n| :--- | :--- | :--- | :--- |\n| **Bridge-V2** | Open X-Embodiment (OXE) æ•°æ®é›†å­é›† | **è®­ç»ƒ** | å¤§è§„æ¨¡æœºå™¨äººæ¼”ç¤ºæ•°æ®é›†ï¼ŒåŒ…å«å¤šæ ·çš„æ“ä½œä»»åŠ¡ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹çš„è¡ŒåŠ¨é¢„æµ‹èƒ½åŠ›ã€‚ |\n| **Fractal** | Open X-Embodiment (OXE) æ•°æ®é›†å­é›† | **è®­ç»ƒ** | å¦ä¸€ä¸ªå¤§è§„æ¨¡æ“ä½œæ•°æ®é›†ï¼Œä¸Bridge-V2ç»“åˆä½¿ç”¨ï¼Œæä¾›ä¸°å¯Œçš„è®­ç»ƒæ ·æœ¬ã€‚ |\n| **Humanoid Robot Tabletop Manipulation** | PhysicalAI-Robotics-GR00T-X-Embodiment-Sim æ•°æ®é›†å­é›† | **è®­ç»ƒ** | ä¸“é—¨ç”¨äºäººå½¢æœºå™¨äººæ¡Œé¢æ“ä½œä»»åŠ¡çš„æ•°æ®é›†ï¼ŒåŒ…å«ä¸ articulated objectsï¼ˆå¸¦å…³èŠ‚ç‰©ä½“ï¼Œå¦‚æŸœé—¨ã€å¾®æ³¢ç‚‰ï¼‰çš„äº¤äº’ã€‚ |\n\n#### 2. è¯„ä¼°åŸºå‡†/æ•°æ®é›† (Benchmarks)\n\n| åŸºå‡†åç§° | ç¯å¢ƒ | è¯„ä¼°ä»»åŠ¡ä¸¾ä¾‹ | å…³é”®ç»†èŠ‚ |\n| :--- | :--- | :--- | :--- |\n| **SimplerEnv** | ä»¿çœŸç¯å¢ƒ (WidowX æœºå™¨äºº) | 1. Put spoon on towel <br> 2. Put carrot on plate <br> 3. Stack green block on yellow block <br> 4. Put eggplant in the yellow basket | - åŒ…å«4ä¸ªåŸºç¡€çš„ç‰©ä½“æ“ä½œå’Œæ”¾ç½®ä»»åŠ¡ã€‚<br>- ä»»åŠ¡æŒ‡ä»¤æ˜ç¡®ï¼Œä¾§é‡äºè¯„ä¼°æ¨¡å‹å¯¹ç©ºé—´å…³ç³»å’ŒåŸºæœ¬æ“ä½œçš„ç†è§£ä¸æ‰§è¡Œèƒ½åŠ›ã€‚<br>- ä½¿ç”¨å®˜æ–¹æä¾›çš„è¯„ä¼°è„šæœ¬ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”ã€‚ |\n| **RoboCasa (GR1 Tabletop Benchmark)** | ä»¿çœŸç¯å¢ƒ (GR1 äººå½¢æœºå™¨äºº) | åŒ…å« **24ä¸ª** å¤æ‚çš„æ¡Œé¢ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼š<br> - `PnPBottleToCabinetClose` (å°†ç“¶å­æ”¾å…¥æ©±æŸœå¹¶å…³é—¨)<br> - `PnPCanToDrawerClose` (å°†ç½å¤´æ”¾å…¥æŠ½å±‰å¹¶å…³é—­)<br> - å„ç§ä¸å¾®æ³¢ç‚‰ã€çƒ¤é¢åŒ…æœºçš„äº¤äº’ä»»åŠ¡ | - ä»»åŠ¡**å¤šæ ·æ€§é«˜**ï¼Œæ¶‰åŠå¸¦å…³èŠ‚ç‰©ä½“ã€å¤æ‚å‡ ä½•å½¢çŠ¶å’Œå¤šæ­¥éª¤æ“ä½œã€‚<br>- å¯¹æ¨¡å‹çš„**ç©ºé—´æ„ŸçŸ¥ã€è§„åˆ’èƒ½åŠ›å’Œç²¾ç»†æ§åˆ¶**æå‡ºäº†æ›´é«˜è¦æ±‚ã€‚<br>- æ˜¯è¯„ä¼°æ¨¡å‹åœ¨æ›´æ¥è¿‘çœŸå®åœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›çš„é‡è¦åŸºå‡†ã€‚ |\n\n### æ€»ç»“\n\n**TwinBrainVLA** é€šè¿‡åœ¨ä¸Šè¿°ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»¿çœŸåŸºå‡†ä¸Šè¿›è¡Œå®šé‡è¯„ä¼°ï¼Œè¯æ˜äº†å…¶æ¶æ„çš„æœ‰æ•ˆæ€§ã€‚å…¶è¯„ä¼°ä½“ç³»å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n\n1.  **æŒ‡æ ‡èšç„¦**ï¼šå®Œå…¨å›´ç»•**ä»»åŠ¡æˆåŠŸç‡**è¿™ä¸€æ ¸å¿ƒæœºå™¨äººæ€§èƒ½æŒ‡æ ‡å±•å¼€ï¼Œç›´æ¥ã€æœ‰åŠ›ã€‚\n2.  **åŸºå‡†å…¨é¢**ï¼š\n    - **SimplerEnv** æµ‹è¯•åŸºç¡€æ“ä½œèƒ½åŠ›ã€‚\n    - **RoboCasa** æµ‹è¯•å¤æ‚åœºæ™¯ä¸‹çš„æ³›åŒ–ä¸ç²¾ç»†æ“ä½œèƒ½åŠ›ã€‚\n3.  **å¯¹æ¯”å…¬å¹³**ï¼šä¸ä¼—å¤šå‰æ²¿æ¨¡å‹ï¼ˆå¦‚RT-1-X, Octo, OpenVLA, Isaac-GR00Tç­‰ï¼‰åœ¨ç›¸åŒä»»åŠ¡å’Œè¯„ä¼°åè®®ä¸‹è¿›è¡Œå¯¹æ¯”ï¼Œç»“æœå…·æœ‰è¯´æœåŠ›ã€‚\n4.  **å®é™…ä»·å€¼ä½“ç°**ï¼šåœ¨RoboCasaå¤æ‚ä»»åŠ¡ä¸Šçš„æ˜¾è‘—æ€§èƒ½æå‡ï¼ˆå¹³å‡æˆåŠŸç‡é¢†å…ˆåŸºçº¿7-10ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œç›´æ¥éªŒè¯äº†å…¶â€œ**å…¼é¡¾é«˜å±‚è¯­ä¹‰ç†è§£ä¸ä½å±‚è¿åŠ¨æ§åˆ¶**â€çš„è®¾è®¡ç›®æ ‡ï¼Œå¯¹äºæ„å»ºé€šç”¨æœºå™¨äººå…·æœ‰é‡è¦ä»·å€¼ã€‚",
            "timestamp": "2026-01-27T15:58:03.627216",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ä¸¤ä¸ªä¸»æµçš„æœºå™¨äººä»¿çœŸåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼š\n\n1.  **SimplerEnv** (Li et al., 2024c)\n    *   **è®­ç»ƒæ•°æ®**ï¼šä½¿ç”¨äº†Open X-Embodiment (OXE) æ•°æ®é›†çš„ä¸¤ä¸ªå¤§è§„æ¨¡å­é›†ï¼š\n        *   **Bridge-V2** æ•°æ®é›†\n        *   **Fractal** æ•°æ®é›†\n    *   **è¯„ä¼°ä»»åŠ¡**ï¼šåŒ…å«4ä¸ªå…·ä½“çš„æ¡Œé¢æ“ä½œä»»åŠ¡ï¼š\n        *   `put spoon on towel`\n        *   `put carrot on plate`\n        *   `stack green block on yellow block`\n        *   `put eggplant in the yellow basket`\n\n2.  **RoboCasa** (Nasiriany et al., 2024)\n    *   **è®­ç»ƒæ•°æ®**ï¼šä½¿ç”¨äº† **PhysicalAI-Robotics-GR00T-X-Embodiment-Sim** æ•°æ®é›†ä¸­çš„ **Humanoid Robot Tabletop Manipulation** å­é›†ã€‚\n    *   **è¯„ä¼°ä»»åŠ¡**ï¼šåœ¨ **RoboCasa GR1 Tabletop Benchmark** ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒåŒ…å« **24ä¸ª** å¤æ‚çš„æ¡Œé¢æ“ä½œä»»åŠ¡ï¼Œæ¶‰åŠä¸é“°æ¥ç‰©ä½“å’Œä¸åŒå‡ ä½•å½¢çŠ¶çš„äº¤äº’ï¼ˆä¾‹å¦‚ï¼Œå°†ç“¶å­æ”¾å…¥æ©±æŸœå¹¶å…³é—¨ã€ä¸å¾®æ³¢ç‚‰/çƒ¤é¢åŒ…æœºäº¤äº’ç­‰ï¼‰ã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨çš„æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡æ˜¯ **ä»»åŠ¡æˆåŠŸç‡**ã€‚\n\n*   **SimplerEnv**ï¼šå¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œ **480æ¬¡** ç‹¬ç«‹è¯•éªŒï¼ŒæŠ¥å‘Šå¹³å‡æˆåŠŸç‡ (`Avg@480`)ã€‚\n*   **RoboCasa**ï¼šå¯¹24ä¸ªä»»åŠ¡ä¸­çš„æ¯ä¸€ä¸ªè¿›è¡Œå¤šæ¬¡ç‹¬ç«‹è¯•éªŒï¼ŒæŠ¥å‘Šå¹³å‡æˆåŠŸç‡ (`Avg@50`)ã€‚\n\n### ä¸‰ã€æ€§èƒ½æå‡ä¸å¯¹æ¯”ç»“æœ\n\n#### 1. SimplerEnv åŸºå‡†æµ‹è¯•\n*   **æœ€ä½³æ¨¡å‹**ï¼š`TwinBrainVLA` + Qwen3-VL-4B-Instruct\n*   **å¹³å‡æˆåŠŸç‡**ï¼š**62.0%**\n*   **å…³é”®å¯¹æ¯”ä¸æå‡**ï¼š\n    *   è¶…è¶Šäº†å½“æ—¶æœ€å¼ºçš„åŸºçº¿æ¨¡å‹ **Isaac-GR00T-N1.6-Bridge** (57.1%)ï¼Œ**ç›¸å¯¹æå‡ +4.9%**ã€‚\n    *   åœ¨å››ä¸ªå…·ä½“ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨ `Put Eggplant in Yellow Basket` ä»»åŠ¡ä¸Šè¾¾åˆ°83.3%çš„æˆåŠŸç‡ï¼Œå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚\n    *   å³ä½¿ä½¿ç”¨æ›´å°çš„éª¨å¹²ç½‘ç»œ (Qwen2.5-VL-3B-Instruct)ï¼Œä¹Ÿå–å¾—äº†58.4%çš„å¹³å‡æˆåŠŸç‡ï¼Œè¯æ˜äº†æ¶æ„çš„æœ‰æ•ˆæ€§ã€‚\n\n#### 2. RoboCasa åŸºå‡†æµ‹è¯•\n*   **æœ€ä½³æ¨¡å‹**ï¼š`TwinBrainVLA` + Qwen3-VL-4B-Instruct\n*   **å¹³å‡æˆåŠŸç‡**ï¼š**54.6%**\n*   **å…³é”®å¯¹æ¯”ä¸æå‡**ï¼š\n    *   æ˜¾è‘—è¶…è¶Šäº†æ‰€æœ‰å¯¹æ¯”åŸºçº¿ï¼š\n        *   ç›¸æ¯” **Isaac-GR00T-N1.6** (47.6%)ï¼Œ**ç›¸å¯¹æå‡ +7.0%**ã€‚\n        *   ç›¸æ¯” **QwenGR00T + Qwen3VL** (47.8%)ï¼Œ**ç›¸å¯¹æå‡ +6.8%**ã€‚\n        *   ç›¸æ¯” **QwenPI + Qwen3VL** (43.9%)ï¼Œ**ç›¸å¯¹æå‡ +10.7%**ã€‚\n    *   åœ¨24ä¸ªä»»åŠ¡ä¸­çš„å¤§å¤šæ•°ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æˆ–æ¥è¿‘æœ€ä½³çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ `PnP Bottle To Cabinet Close` (74.0%) ç­‰æ¶‰åŠå¤æ‚ç‰©ä½“äº¤äº’çš„ä»»åŠ¡ä¸Šä¼˜åŠ¿æ˜æ˜¾ã€‚\n\n### æ€»ç»“\n**TwinBrainVLA** é€šè¿‡åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»¿çœŸåŸºå‡†ä¸Šè¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†å…¶**éå¯¹ç§°åŒè„‘æ¶æ„**çš„æœ‰æ•ˆæ€§ã€‚å…¶ä¸»è¦ä»·å€¼ä½“ç°åœ¨ï¼š\n1.  **æ€§èƒ½é¢†å…ˆ**ï¼šåœ¨ä¿æŒé€šç”¨VLMè¯­ä¹‰ç†è§£èƒ½åŠ›ä¸é€€åŒ–ï¼ˆè§£å†³ç¾éš¾æ€§é—å¿˜ï¼‰çš„åŒæ—¶ï¼Œåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šå–å¾—äº†**æœ€å…ˆè¿›çš„æˆåŠŸç‡**ã€‚\n2.  **æ³›åŒ–æ€§å¼º**ï¼šæ— è®ºæ˜¯åœ¨ç›¸å¯¹ç®€å•çš„ `SimplerEnv` ä»»åŠ¡ï¼Œè¿˜æ˜¯åœ¨å¤æ‚å¤šæ ·çš„ `RoboCasa` 24ä»»åŠ¡å¥—ä»¶ä¸Šï¼Œéƒ½è¡¨ç°å‡ºä¸€è‡´ä¸”ä¼˜å¼‚çš„æ€§èƒ½ã€‚\n3.  **æ¶æ„ä¼˜åŠ¿å¾—åˆ°å®è¯**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œå°†é€šç”¨è¯­ä¹‰ç†è§£ï¼ˆå·¦è„‘ï¼Œå†»ç»“ï¼‰ä¸å…·èº«æ„ŸçŸ¥æ§åˆ¶ï¼ˆå³è„‘ï¼Œå¯è®­ç»ƒï¼‰è§£è€¦çš„è®¾è®¡ï¼Œæ˜¯åŒæ—¶å®ç°**é«˜çº§è®¤çŸ¥**å’Œ**ä½çº§çµå·§æ“ä½œ**çš„æœ‰æ•ˆé€”å¾„ã€‚",
            "timestamp": "2026-01-27T15:58:36.894945",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡ç¬¬6èŠ‚â€œLimitation and Future Workâ€ï¼Œ**TwinBrainVLA** ä¸»è¦å­˜åœ¨ä»¥ä¸‹å››ä¸ªæ–¹é¢çš„å±€é™æ€§ï¼š\n\n- **æ¨¡å‹æ¶æ„è€¦åˆæ€§è¾ƒå¼º**ï¼šå½“å‰çš„**éå¯¹ç§°æ··åˆTransformerï¼ˆAsyMoTï¼‰** æœºåˆ¶è¦æ±‚â€œå·¦è„‘â€ï¼ˆé€šç”¨VLMï¼‰å’Œâ€œå³è„‘â€ï¼ˆä¸“ç”¨VLMï¼‰å¿…é¡»å…·æœ‰**å®Œå…¨ç›¸åŒçš„æ¨¡å‹æ¶æ„**ã€‚è¿™ç¡®ä¿äº†éšè—çŠ¶æ€ç»´åº¦å…¼å®¹ï¼Œä¾¿äºè¿›è¡Œè”åˆæ³¨æ„åŠ›è®¡ç®—ï¼Œä½†ä¹Ÿé™åˆ¶äº†æ¶æ„è®¾è®¡çš„çµæ´»æ€§ã€‚ä¾‹å¦‚ï¼Œæ— æ³•è½»æ¾åœ°å°†ä¸€ä¸ªåºå¤§çš„ã€ä¾§é‡æ¨ç†çš„VLMä¸ä¸€ä¸ªè½»é‡çº§çš„ã€ä¾§é‡é«˜é¢‘æ§åˆ¶çš„æ¨¡å‹è¿›è¡Œç»„åˆã€‚\n\n- **æ¨¡å‹æ£€æŸ¥ç‚¹æ¥æºå•ä¸€**ï¼šç›®å‰ï¼ŒåŒè„‘çš„åˆå§‹åŒ–å‡ä¾èµ–äºæ ‡å‡†çš„**é€šç”¨VLMæ£€æŸ¥ç‚¹**ï¼ˆå¦‚Qwen-VLç³»åˆ—ï¼‰ã€‚ç„¶è€Œï¼Œé¢†åŸŸå†…å·²ç»å‡ºç°äº†å¤§é‡ä¸“é—¨ä¸ºæœºå™¨äººä»»åŠ¡è®­ç»ƒçš„**å…·èº«VLM**ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œå°šæœªæ¢ç´¢å°†è¿™ç±»ç»è¿‡é¢†åŸŸç‰¹å®šåè®­ç»ƒçš„æ¨¡å‹é›†æˆåˆ°åŒè„‘èŒƒå¼ä¸­çš„æ½œåŠ›ï¼Œä¾‹å¦‚ç”¨å…·èº«VLMåˆå§‹åŒ–â€œå³è„‘â€ï¼ŒåŒæ—¶ä¿ç•™é€šç”¨VLMä½œä¸ºâ€œå·¦è„‘â€ã€‚\n\n- **è®­ç»ƒæ•°æ®è§„æ¨¡æœ‰é™**ï¼šå½“å‰çš„å®éªŒä»…åœ¨**Open X-Embodimentæ•°æ®é›†çš„ä¸¤ä¸ªå­é›†**ï¼ˆBridge-V2å’ŒFractalï¼‰ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä½œè€…æ‰¿è®¤ï¼ŒåŒæµæ¶æ„çš„æ½œåŠ›å¯èƒ½å°šæœªåœ¨æ›´å¤§è§„æ¨¡çš„æ•°æ®ä¸Šå¾—åˆ°å……åˆ†é‡Šæ”¾ã€‚\n\n- **è¯„ä¼°èŒƒå›´æœ‰å¾…æ‰©å±•**ï¼šç›®å‰çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨**ä»¿çœŸç¯å¢ƒ**ï¼ˆSimplerEnvå’ŒRoboCasaï¼‰ä¸­ã€‚è®ºæ–‡å°šæœªåŒ…å«**çœŸå®æœºå™¨äººåœºæ™¯**çš„æµ‹è¯•ï¼Œè¿™é™åˆ¶äº†å¯¹å…¶åœ¨ç°å®ä¸–ç•Œå¤æ‚æ€§å’Œä¸ç¡®å®šæ€§ä¸‹æ³›åŒ–èƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡æå‡ºäº†æ˜ç¡®çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼š\n\n- **å¼€å‘æ›´è§£è€¦çš„æ¨¡å‹æ¶æ„**ï¼šæ¢ç´¢æ›´é€šç”¨çš„èåˆæœºåˆ¶ï¼Œä¾‹å¦‚**å¯å­¦ä¹ çš„æŠ•å½±å±‚**æˆ–**äº¤å‰æ³¨æ„åŠ›é€‚é…å™¨**ã€‚è¿™å°†æ”¯æŒä¸åŒå¤§å°ã€ä¸åŒæ¶æ„çš„éª¨å¹²æ¨¡å‹è¿›è¡Œç»„åˆï¼Œå®ç°æ›´çµæ´»çš„â€œå¼‚æ„åŒè„‘â€ååŒï¼ˆå¦‚â€œå·¨å‹æ¨ç†è„‘â€+â€œè½»é‡æ§åˆ¶è„‘â€ï¼‰ã€‚\n\n- **é›†æˆæ›´å¤šæ ·åŒ–çš„æ¨¡å‹æ£€æŸ¥ç‚¹**ï¼šç ”ç©¶å¦‚ä½•æœ‰æ•ˆåœ°å°†**é¢†åŸŸä¸“ç”¨çš„å…·èº«VLM**æ•´åˆåˆ°åŒè„‘èŒƒå¼ä¸­ã€‚ä¸€ä¸ªå…³é”®æ–¹å‘æ˜¯æ¢ç´¢ç”¨ç»è¿‡æœºå™¨äººæ•°æ®åè®­ç»ƒçš„VLMæ¥åˆå§‹åŒ–â€œå³è„‘â€ï¼ŒåŒæ—¶ä¿æŒâ€œå·¦è„‘â€çš„é€šç”¨æ€§ï¼Œä»¥æœŸè·å¾—æ›´å¼ºçš„å³æ’å³ç”¨çš„æ§åˆ¶èƒ½åŠ›ã€‚\n\n- **åœ¨æ›´å¤§è§„æ¨¡æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒ**ï¼šè®¡åˆ’å°†é¢„è®­ç»ƒæ‰©å±•åˆ°**å®Œæ•´çš„Open X-Embodimentæ•°æ®é›†**ï¼Œä»¥å……åˆ†æŒ–æ˜æ¨¡å‹æ½œåŠ›ï¼Œè¿›ä¸€æ­¥æå‡å…¶é²æ£’æ€§å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚\n\n- **æ‰©å±•è¯„æµ‹åŸºå‡†ä¸è¿›è¡ŒçœŸæœºå®éªŒ**ï¼šæœªæ¥çš„å·¥ä½œå°†è‡´åŠ›äºåœ¨**æ›´å¹¿æ³›çš„è¯„æµ‹åŸºå‡†**ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œå¹¶æœ€ç»ˆå¼€å±•**çœŸå®æœºå™¨äººå®éªŒ**ï¼Œä»¥å…¨é¢éªŒè¯ **TwinBrainVLA** åœ¨ç‰©ç†ä¸–ç•Œä¸­çš„é€šç”¨æ€§å’Œå®ç”¨æ€§ã€‚\n\n**æ€»ç»“**ï¼š**TwinBrainVLA** çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡æ¶æ„è®¾è®¡è§£å†³äº†â€œç¾éš¾æ€§é—å¿˜â€é—®é¢˜ï¼Œä½†å…¶å½“å‰å®ç°ä»å—é™äºæ¶æ„çš„å¯¹ç§°æ€§ã€æ¨¡å‹æ¥æºçš„å•ä¸€æ€§ä»¥åŠæ•°æ®å’Œè¯„ä¼°çš„è§„æ¨¡ã€‚æœªæ¥çš„æ”¹è¿›å°†å›´ç»•**æ¶æ„è§£è€¦**ã€**æ¨¡å‹å¼‚æ„åŒ–**ã€**æ•°æ®è§„æ¨¡åŒ–**å’Œ**è¯„æµ‹ç°å®åŒ–**å››ä¸ªç»´åº¦å±•å¼€ï¼Œæ—¨åœ¨æ„å»ºæ›´çµæ´»ã€æ›´å¼ºå¤§ã€æ›´å®ç”¨çš„é€šç”¨æœºå™¨äººæ™ºèƒ½ä½“ã€‚",
            "timestamp": "2026-01-27T15:59:02.859938",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T16:30:07Z",
    "created_at": "2026-01-27T15:53:13.835259",
    "updated_at": "2026-01-27T15:53:13.835266"
}