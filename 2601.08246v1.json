{
    "id": "2601.08246v1",
    "title": "FSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models",
    "authors": [
        "Yifan Han",
        "Pengfei Yi",
        "Junyan Li",
        "Hanqing Wang",
        "Gaojing Zhang",
        "Qi Peng Liu",
        "Wenzhao Lian"
    ],
    "abstract": "çµå·§æŠ“å–åˆæˆä»æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šå¤šæŒ‡æ‰‹çš„é«˜ç»´åº¦å’Œè¿åŠ¨å¤šæ ·æ€§é˜»ç¢äº†ä¸ºå¹³è¡Œå¤¹çˆªå¼€å‘çš„ç®—æ³•ç›´æŽ¥è¿ç§»ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºŽåœ¨ä»¿çœŸä¸­æˆ–é€šè¿‡æ˜‚è´µçš„çœŸå®žä¸–ç•Œè¯•éªŒæ”¶é›†çš„å¤§åž‹ã€ç¡¬ä»¶ç‰¹å®šçš„æŠ“å–æ•°æ®é›†ï¼Œè¿™åœ¨æ–°çµå·§æ‰‹è®¾è®¡å‡ºçŽ°æ—¶é˜»ç¢äº†å¯æ‰©å±•æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®é«˜æ•ˆçš„æ¡†æž¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒç”Ÿæˆæ‰©æ•£æ¨¡åž‹ä¸­ä¸°å¯Œçš„ã€ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„è¯­ä¹‰å…ˆéªŒï¼Œç»•è¿‡æœºå™¨äººæŠ“å–æ•°æ®æ”¶é›†ã€‚æˆ‘ä»¬ä»ŽåŽŸå§‹äººç±»è§†é¢‘æ¼”ç¤ºä¸­æå–æ—¶é—´å¯¹é½ä¸”ç»†ç²’åº¦çš„æŠ“å–å¯ä¾›æ€§ï¼Œå¹¶å°†å…¶ä¸Žæ·±åº¦å›¾åƒä¸­çš„ä¸‰ç»´åœºæ™¯å‡ ä½•èžåˆï¼Œä»¥æŽ¨æ–­åŸºäºŽè¯­ä¹‰çš„æŽ¥è§¦ç›®æ ‡ã€‚éšåŽï¼Œä¸€ä¸ªè¿åŠ¨å­¦æ„ŸçŸ¥çš„é‡å®šå‘æ¨¡å—å°†è¿™äº›å¯ä¾›æ€§è¡¨ç¤ºæ˜ å°„åˆ°ä¸åŒçš„çµå·§æ‰‹ä¸Šï¼Œè€Œæ— éœ€é’ˆå¯¹æ¯åªæ‰‹è¿›è¡Œé‡æ–°è®­ç»ƒã€‚æœ€ç»ˆç³»ç»Ÿç”Ÿæˆç¨³å®šã€åŠŸèƒ½é€‚å½“çš„å¤šæŽ¥è§¦æŠ“å–ï¼Œåœ¨å¸¸è§ç‰©ä½“å’Œå·¥å…·ä¸Šä¿æŒå¯é æˆåŠŸï¼ŒåŒæ—¶åœ¨ç±»åˆ«å†…æœªè§è¿‡çš„ç‰©ä½“å®žä¾‹ã€å§¿æ€å˜åŒ–ä»¥åŠå¤šç§æ‰‹éƒ¨å½¢æ€ä¸Šå±•çŽ°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬å·¥ä½œï¼ˆiï¼‰å¼•å…¥äº†ä¸€ç§åˆ©ç”¨è§†è§‰è¯­è¨€ç”Ÿæˆå…ˆéªŒè¿›è¡Œçµå·§æŠ“å–çš„è¯­ä¹‰å¯ä¾›æ€§æå–æµç¨‹ï¼Œï¼ˆiiï¼‰å±•ç¤ºäº†æ— éœ€æž„å»ºç¡¬ä»¶ç‰¹å®šæŠ“å–æ•°æ®é›†çš„è·¨æ‰‹æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŠï¼ˆiiiï¼‰è¯æ˜Žäº†å½“ç»“åˆåŸºç¡€æ¨¡åž‹çš„è¯­ä¹‰æ—¶ï¼Œå•ä¸€æ·±åº¦æ¨¡æ€è¶³ä»¥å®žçŽ°é«˜æ€§èƒ½æŠ“å–åˆæˆã€‚æˆ‘ä»¬çš„æˆæžœä¸ºé€šè¿‡äººç±»æ¼”ç¤ºå’Œé¢„è®­ç»ƒç”Ÿæˆæ¨¡åž‹å®žçŽ°å¯æ‰©å±•ã€ç¡¬ä»¶æ— å…³çš„çµå·§æ“ä½œæŒ‡æ˜Žäº†ä¸€æ¡è·¯å¾„ã€‚",
    "url": "https://arxiv.org/abs/2601.08246v1",
    "html_url": "https://arxiv.org/html/2601.08246v1",
    "html_content": "FSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models\nYifan Han\n1,2\n,\nPengfei Yi\n1,2\n,\nJunyan Li\n1,2\n,\nHanqing Wang\n3\n,\nGaojing Zhang\n4\n,\nQiÂ PengÂ Liu\n5\n,\nWenzhaoÂ Lian\n5â€ \n1\nInstitute of Automation, Chinese Academy of Sciences.;\n2\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences.;\n3\nThe Hong Kong University of Science and Technology (Guangzhou).;\n4\nUniversity of Sussex.;\n5\nSchool of Artificial Intelligence, Shanghai Jiao Tong University.\nlianwenzhao@sjtu.edu.cn\n.;\nâ€ \nCorresponding Author.\nAbstract\nDexterous grasp synthesis remains a central challenge: the high dimensionality and kinematic diversity of multi-fingered hands prevent direct transfer of algorithms developed for parallel-jaw grippers. Existing approaches typically depend on large, hardware-specific grasp datasets collected in simulation or through costly real-world trials, hindering scalability as new dexterous hand designs emerge. To this end, we propose a data-efficient framework, which is designed to bypass robot grasp data collection by exploiting the rich, object-centric semantic priors latent in pretrained generative diffusion models. Temporally aligned and fine-grained grasp affordances are extracted from raw human video demonstrations and fused with 3D scene geometry from depth images to infer semantically grounded contact targets. A kinematics-aware retargeting module then maps these affordance representations to diverse dexterous hands without per-hand retraining. The resulting system produces stable, functionally appropriate multi-contact grasps that remain reliably successful across common objects and tools, while exhibiting strong generalization across previously unseen object instances within a category, pose variations, and multiple hand embodiments. This work (i) introduces a semantic affordance extraction pipeline leveraging visionâ€“language generative priors for dexterous grasping, (ii) demonstrates cross-hand generalization without constructing hardware-specific grasp datasets, and (iii) establishes that a single depth modality suffices for high-performance grasp synthesis when coupled with foundation-model semantics. Our results highlight a path toward scalable, hardware-agnostic dexterous manipulation driven by human demonstrations and pretrained generative models.\nI\nIntroduction\nReliable grasp synthesis remains a central, yet unresolved, challenge in dexterous robot manipulation. Compared with parallel-jaw grippers, multi-fingered dexterous hands offer substantially higher kinematic expressivity and potential contact richness, but this increase in dimensionality and handâ€“objectâ€“environment interaction complexity dramatically amplifies the difficulty of learning and deploying robust grasp strategies. The classical problem can be informally factorized into (i) â€œ\nhow to grasp\nâ€: the structured, role-aware coordination of fingers, contacts, approach direction, and hand synergies, and (ii) â€œ\nwhere to grasp\nâ€: the spatial localization of functionally exploitable object regions. Despite rapid progress, existing methods typically address these two issues in isolation, leading to brittle policies, embodiment entanglement, and limited cross-object generalization.\nOn â€œ\nhow\nto graspâ€, recent works leverage reinforcement learning or large-scale supervised training in simulation to explore expansive action spaces\n[\n36\n,\n34\n]\n. However, simulation-trained policies customarily rely on privileged, complete, and noise-free geometric states, which are rarely attainable in real-world deployments. This induces significant sim-to-real gaps when facing realistic sensing artifacts such as partial views, self-occlusions, depth discontinuities, and modality noise.\nAn alternative lineâ€”direct large-scale real-world data collection (e.g., AnyDexGrasp\n[\n9\n]\n)â€”suffers from prohibitive operational cost and still struggles to generalize to unseen dexterous hand embodiments. A core culprit is representational overfitting: action/policy parameterizations are tightly coupled to a specific hand modelâ€™s joint layout, contact affordances, and actuation limits, precluding systematic transfer.\nOn â€œ\nwhere\nto graspâ€, prevailing affordance research yields coarse region-of-interest predictions\n[\n23\n,\n22\n]\n. While such representations offer a spatial prior for potential contact regions, they stop short of encoding the per-finger engagement strategy and the geometry-conditioned sequencing necessary for stable multi-contact closure.\nThus, although they identify candidate regions, they fail to specify the fine-grained per-finger instructions required for dexterous manipulation (e.g., contact assignments, approach vectors, and local-geometry adaptations). Recasting contact reasoning as keypoint detection is likewise insufficient: reliable contact loci often lack discriminative RGB cues and hinge on object-part semantics and inter-finger relations that appearance-only backbones (e.g., CNNs/ViTs\n[\n8\n]\n) do not capture.\nOur core idea to tackle the above challenges is that Internet-scale text-to-image diffusion models internalize multi-level knowledge about objects, parts, materials, and functional geometry\n[\n29\n]\n. Prior work\n[\n38\n]\nshows that intermediate denoising features encode exactly the semantic priors missing in standard discriminative backbones, and that these features can be repurposedâ€”not to generate data, but to ground grasp semantics.\nBuilding on this insight, we propose to infer a\nFinger-Specific Affordance Field (FSAF)\nthat unifies the â€œhowâ€ and â€œwhereâ€: a dense, object-conditioned mapping that assigns, for each prospective surface location, structured likelihoods and role descriptors for individual fingers, jointly capturing feasible contact placement, approach direction hypotheses, and inter-finger relational compatibility.\nSpecifically, we repurpose a frozen Stable Diffusion\n[\n29\n]\nU\n-\nNet as a semantic backbone to infer a\nFSAF and couple it with a kinematics\n-\naware planner for on\n-\ntable execution. Once FSAF is inferred, we compose novel grasps without additional teleoperation and transfer across heterogeneous dexterous hands via object\n-\ncentric planning.\nThis affordance-based representation enables more precise manipulation from only a small number of human demonstrations and supports deploying a single policy across multiple dexterous hands.\nExperiments demonstrate state-of-the-art performance on seen and unseen objectsâ€”superior affordance grounding and a 90% grasp success rateâ€”with cross-embodiment transfer to different dexterous hands.\nIn summary, we make the following contributions:\nâ€¢\nFinger-Specific Affordance Field (FSAF).\nWe introduce a fine-grained, per-finger affordance representation that leverages visionâ€“language generative priors to jointly encode object affordance functions and contact-level manipulation semantics, decomposing monolithic grasp poses into semantically grounded finger role assignments.\nâ€¢\nDemonstration-light, affordance-conditioned grasp synthesis.\nOur grasp generation consumes the inferred FSAF to synthesize stable, role-constrained multi-finger grasp configurations without requiring large-scale teleoperation demonstration corpora, substantially reducing data collection burden.\nâ€¢\nCross-embodiment generalization.\nWe achieve direct transfer of grasp strategies across heterogeneous dexterous hands, evidencing that the proposed affordance abstraction disentangles grasp semantics from specific embodiment kinematics.\nII\nRELATED WORK\nII-A\nLearning from Human Demonstration Videos\nLearning from human demonstration videos is central to robot learning because such data are easier to collect and encode strong task priors\n[\n22\n,\n28\n]\n; however, a persistent embodiment gap separates human hands from robotic dexterous end-effectors. The prevailing remedy is retargetingâ€”mapping human motions to robot kinematics\n[\n25\n,\n43\n,\n18\n]\nâ€”which is effective for teleoperation and data gathering but typically still requires reinforcement learning in simulation to attain dexterous proficiency, thereby introducing a sim-to-real gap and limiting cross-embodiment generalization. Recently, a more fundamental route is to learn manipulation structure from human hand data that robots can directly consume. For parallel-jaw grippers, PointPolicy\n[\n13\n]\nuses video-derived keypoints to supervise gripper control; however, such position-only cues under-specify dexterous-hand interaction. In this work, we therefore learn finger-specific affordances from human demonstrationsâ€”object-conditioned descriptors that specify, per finger, contactable regions and approach directions over timeâ€”to represent how dexterous hands should interact with objects.\nFigure 2\n:\nPipeline overview.\n(1)\nHyperfeature extraction:\nA frozen text-to-image diffusion U-Net encodes the object image with text conditioning. Multi-timestep, multi-scale activations are aggregated into hyperfeatures\nA\ng\nA_{g}\n.\n(2)\nFinger-specific affordance grounding:\nAn FPN-style decoder maps\nA\ng\nA_{g}\nto five per-finger likelihood maps\nH\n^\n\\hat{H}\nsupervised by fingertip labels from human demonstrations.\n(3)\nManipulation:\nBack-projecting GroundingDINOâ€“SAM2 masks with FoundationStereo depth yields a partial object point cloud; peaks of\nH\n^\n\\hat{H}\nare lifted to 3D and local normals define phase-labeled waypoints (\napproach\n,\nclosure\n,\nhold\n).A damped least-squares QP tracks these waypoints under joint and collision constraints to execute dexterous grasps.\nII-B\nObject-centric Representation Learning\nMuch of object-centric grasping is tailored to parallel-jaw grippers, representing grasps by 2D/3D locations. Such position-only cues under-specify dexterous-hand interactions\n[\n31\n,\n7\n]\n. In addition, a subset of dexterous hand approaches relies on template matching, built around exemplars specific to the object or the task, thus having limited generalizability to novel objects/tasks\n[\n17\n,\n10\n]\n. Contact-centric representations improve task relevance but require complex pipelines and costly data collection to obtain dexterous-hand contact regions\n[\n39\n,\n4\n]\n. More recently, interaction-driven approaches such as CMKA\n[\n37\n]\nlearn interaction regions from web imagery, yet map object keypoints to wrist, index, and little-finger anchors, effectively producing a single grasp parameterized by three points and limiting performance to on-shelf settings. These limitations motivate per-finger contact representations rather than region- or point-only cues.\nWe learn per-finger contact cues and, via depth-and-segmentation projection, map them onto object surfaces, to further derive precise tabletop grasps.\nII-C\nAffordance Learning\nThe concept of affordance was popularized by psychologist James Gibson\n[\n12\n]\n,\nwhich aims to highlight the actionable areas on objects, is crucial for robotic grasping and manipulation in dynamic, complex, and physical environments. Previous research has achieved remarkable progress in learning affordance knowledge from Human-Object-Interaction (HOI) images\n[\n40\n,\n30\n]\nand videos\n[\n1\n,\n23\n]\n, language instructions\n[\n19\n]\n, and 3D point clouds\n[\n6\n,\n5\n]\n. More recently, researchers have expanded the reasoning ability of multimodal large language models into affordance learning\n[\n33\n,\n24\n]\n, achieving advanced affordance reasoning and generalization abilities. Moreover, the text-to-image diffusion model\n[\n29\n]\nshows promising affordance-aware understanding ability by generating precise hand-object images\n[\n38\n]\n.\nHowever, most approaches localize coarse object regions (e.g., boxes or masks), which is insufficient for fine-grained, finger-specific guidance required by dexterous handsâ€”motivating representations at the per-finger contact level.\nIII\nMETHODS\nWe present a perception-to-optimization framework that learns\nfinger-specific affordances\nfrom human demonstrations and converts them into executable dexterous grasps, as shown in Fig.\n2\n. Semantically grounded hyperfeatures from a frozen text-to-image diffusion model\n[\n29\n]\nare decoded into five-channel per-finger likelihood maps, which guide contact selection on a reconstructed object point cloud. Local surface geometry defines waypointed approach/closure trajectories that are tracked via a damped least-squares QP under joint and collision constraints. As grasp execution is optimization-based, the pipeline transfers across dexterous hands with little or no retraining.\nIII-A\nData Collection\nWe construct a finger-wise contact affordance dataset from human demonstration videos using a custom data acquisition pipeline. For each recorded grasp sequence, we first automatically detect 2D hand keypoints with the RTMPose\n[\n15\n]\nhand pose estimator. We then identify (i) an\nobject-only\nkeyframe in which the target object is fully visible and no hand pixels are present, and (ii) the earliest\nstable grasp\nframe in which the human hand establishes clear contact with the object. This pair of frames form one training sample: the object-only frame provides a clean visual context of the objectâ€™s geometry and texture, while the grasp frame provides the supervisory signal for finger-specific contact localization.\nOn the image lattice\nÎ©\n=\n{\n0\n,\nâ€¦\n,\nh\nâˆ’\n1\n}\nÃ—\n{\n0\n,\nâ€¦\n,\nw\nâˆ’\n1\n}\n\\Omega=\\{0,\\ldots,h-1\\}\\times\\{0,\\ldots,w-1\\}\n, each fingertip\nk\nk\nwith center\nÎ¼\nk\n\\mu_{k}\ninduces a Gaussian channel\nH\nk\nâ€‹\n(\nu\n)\n=\nexp\nâ¡\n(\nâˆ’\nâ€–\nu\nâˆ’\nÎ¼\nk\nâ€–\n2\n2\n/\n(\n2\nâ€‹\nÏƒ\n2\n)\n)\nH_{k}(u)=\\exp(-\\|u-\\mu_{k}\\|_{2}^{2}/(2\\sigma^{2}))\nfor\nu\nâˆˆ\nÎ©\nu\\in\\Omega\n; stacking yields a tensor\nH\nH\nof shape\n5\nÃ—\nh\nÃ—\nw\n5\\times h\\times w\n.\nWe set\nÏƒ\n=\nmin\nâ¡\n(\nh\n,\nw\n)\n/\n64\n\\sigma=\\min(h,w)/64\nand train the predictor\nH\n^\n\\hat{H}\nto regress\nH\nH\nusing MSE.\nIII-B\nFinger-Specific Affordance Representation from Diffusion Models\nIII-B\n1\nFeature synthesis and hyperfeature aggregation\nEmpirical evidence that diffusion models generate precise handâ€“object grasp\n[\n38\n]\nscenarios indicates that Stable Diffusion\n[\n29\n]\nhas already acquired the grasp-critical feature space, integrating holistic affordance semantics with nuanced image details via its generative learning process. Inspired by prior work\n[\n38\n,\n21\n]\n, we leverage the rich semantic grounding capability of large pretrained text-to-image diffusion models to localize finger-specific contact affordances on object images. Distinct from previous approaches, we jointly harvest both visual and textual affordance semantics from Stable Diffusion. Roughly speaking, we employ a U-Net backbone comprising convolutional blocks, downsampling / upsampling paths, skip connections, and cross-attention modules. At each denoising timestep, cross-attention injects textual embeddings into the visual feature stream, encouraging intermediate activations to align with interpretable semantic concepts. Specifically,\ngiven an imageâ€“text pair\n(\nx\n0\n,\ns\n)\n(x_{0},s)\n, we encode a latent\nz\n0\n=\nE\nâ€‹\n(\nx\n0\n)\nz_{0}=E(x_{0})\nwith the variational autoencoder (VAE)\nand follow the standard\nlatent\nforward noising process\nz\nt\n=\nÎ±\nÂ¯\nt\nâ€‹\nz\n0\n+\n1\nâˆ’\nÎ±\nÂ¯\nt\nâ€‹\nÏµ\n,\nÏµ\nâˆ¼\nð’©\nâ€‹\n(\n0\n,\nI\n)\n,\nz_{t}\\;=\\;\\sqrt{\\bar{\\alpha}_{t}}\\,z_{0}\\;+\\;\\sqrt{1-\\bar{\\alpha}_{t}}\\,\\epsilon,\\qquad\\epsilon\\sim\\mathcal{N}(0,I),\n(1)\nwhere\nÎ±\nÂ¯\nt\n=\nâˆ\nÏ„\n=\n1\nt\nÎ±\nÏ„\n\\bar{\\alpha}_{t}=\\prod_{\\tau=1}^{t}\\alpha_{\\tau}\nas defined in\n[\n29\n]\n.\nWe keep the text encoder\nT\nâ€‹\n(\nâ‹…\n)\nT(\\cdot)\nfrozen and obtain token embeddings\nc\n=\nT\nâ€‹\n(\ns\n)\nc=T(s)\n.\nFor computational reasons, we run the diffusion network for\nT\nT\ntimesteps but only select\na subset\nð’®\n\\mathcal{S}\nwith\n|\nð’®\n|\n=\nS\n|\\mathcal{S}|=S\nfor feature aggregation.\nFeeding\n(\nz\nt\n,\nt\n,\nc\n)\n(z_{t},t,c)\ninto a frozen diffusion U\n-\nNet yields multi-scale activations:\n{\nA\nv\n,\n1\n(\nt\n)\n,\nA\nv\n,\n2\n(\nt\n)\n,\nâ€¦\n,\nA\nv\n,\nL\n(\nt\n)\n}\n,\nt\nâˆˆ\nð’®\n,\n\\big\\{A^{(t)}_{v,1},\\,A^{(t)}_{v,2},\\,\\ldots,\\,A^{(t)}_{v,L}\\big\\},\\quad t\\in\\mathcal{S},\nHere,\nA\nv\n,\nâ„“\n(\nt\n)\nâˆˆ\nâ„\nB\nÃ—\nC\nâ„“\nÃ—\nH\nâ„“\nÃ—\nW\nâ„“\nA^{(t)}_{v,\\ell}\\in\\mathbb{R}^{B\\times C_{\\ell}\\times H_{\\ell}\\times W_{\\ell}}\ndenotes the feature map from the\nâ„“\n\\ell\n-th U\n-\nNet block at timestep\nt\nt\n.\nTo summarize complementary semantics across scales and timesteps, we attach lightweight\nbottlenecks\n{\nb\nâ„“\n}\nâ„“\n=\n1\nL\n\\{b_{\\ell}\\}_{\\ell=1}^{L}\n(shared across timesteps) and learn mixing coefficients\n{\nw\nâ„“\n,\nt\n}\nâ„“\n=\n1\n,\nt\nâˆˆ\nð’®\nL\n\\{w_{\\ell,t}\\}_{\\ell=1,t\\in\\mathcal{S}}^{L}\nto produce a global affordance descriptor:\nA\ng\n=\nâˆ‘\nt\nâˆˆ\nð’®\nâˆ‘\nâ„“\n=\n1\nL\nw\nâ„“\n,\nt\nâ€‹\nb\nâ„“\nâ€‹\n(\nA\nv\n,\nâ„“\n(\nt\n)\n)\n.\nA_{g}\\;=\\;\\sum_{t\\in\\mathcal{S}}\\;\\sum_{\\ell=1}^{L}w_{\\ell,t}\\;\\,b_{\\ell}\\!\\left(A^{(t)}_{v,\\ell}\\right).\n(2)\nIn practice, each\nb\nâ„“\nb_{\\ell}\nis a\n1\nÃ—\n1\n1{\\times}1\nconvolution followed by global average pooling\nto a fixed\nd\nd\n-dimensional vector.\nWe parameterize\nw\n~\nâ„“\n,\nt\n\\tilde{w}_{\\ell,t}\nfreely and apply a softmax over all\n(\nâ„“\n,\nt\n)\n(\\ell,t)\n.\nIII-B\n2\nFinger-Specific Affordance Heatmap Prediction\nFigure 3\n:\nRight:\n(1) finger-specific affordance likelihoods predicted from diffusion-derived hyperfeatures;\n(2) partial object point cloud reconstructed by back-projecting GroundingDINOâ€“SAM2 segmentation with FoundationStereo depth;\n(3) approach vectors obtained from local surface normals around the selected contact candidates.\nLeft:\na damped least-squares QP tracks phase-labeled waypoints to execute a dexterous grasp.\nTo enhance the fine-grained finger-specific affordance grounding ability while preserving high-level semantic discrimination and recovering spatial details, we proposed a feature pyramid network (FPN)â€“style decoder, which progressively upsamples\nA\ng\nA_{g}\nwhile injecting lateral features from shallower resolutions, producing a dense fused feature map suitable for contact prediction.\nGiven the dense hyperfeature\nA\ng\nâˆˆ\nâ„\nB\nÃ—\nC\nÃ—\nh\nÃ—\nw\nA_{g}\\!\\in\\!\\mathbb{R}^{B\\times C\\times h\\times w}\n, we decode it with\nthree lateral\n1\nÃ—\n1\n1{\\times}1\nprojections\n{\nÏ•\nr\n}\nr\n=\n3\n1\n\\{\\phi_{r}\\}_{r=3}^{1}\n(to channels\nc\nr\nâˆˆ\n{\n256\n,\n128\n,\n64\n}\nc_{r}\\!\\in\\!\\{256,128,64\\}\n)\nand a topâ€“down pathway. Let target sizes\ns\n3\n=\n(\nh\n,\nw\n)\ns_{3}{=}(h,w)\n,\ns\n2\n=\n(\n2\nâ€‹\nh\n,\n2\nâ€‹\nw\n)\ns_{2}{=}(2h,2w)\n,\ns\n1\n=\n(\nH\no\nâ€‹\nu\nâ€‹\nt\n,\nW\no\nâ€‹\nu\nâ€‹\nt\n)\ns_{1}{=}(H_{\\!out},W_{\\!out})\n,\nU\ns\nU_{s}\nbe bilinear upsampling to size\ns\ns\n,\nÏ„\nr\n\\tau_{r}\na\n1\nÃ—\n1\n1{\\times}1\nadapter, and\nÏˆ\nr\n\\psi_{r}\na\n3\nÃ—\n3\n3{\\times}3\nsmoothing conv with nonlinearity. With\nF\n4\nâ‰¡\n0\nF_{4}\\!\\equiv\\!0\n, we compute\nF\nr\n=\nÏˆ\nr\nâ€‹\n(\nU\ns\nr\nâ€‹\n(\nÏ„\nr\nâ€‹\n(\nF\nr\n+\n1\n)\n)\n+\nU\ns\nr\nâ€‹\n(\nÏ•\nr\nâ€‹\n(\nA\ng\n)\n)\n)\n,\nr\n=\n3\n,\n2\n,\n1\n,\nF_{r}\\;=\\;\\psi_{r}\\!\\Big(U_{s_{r}}\\big(\\tau_{r}(F_{r+1})\\big)\\;+\\;U_{s_{r}}\\big(\\phi_{r}(A_{g})\\big)\\Big),\\quad r=3,2,1,\n(3)\nand obtain the per-finger heatmaps by a final\n3\nÃ—\n3\n3{\\times}3\nprojection\nH\n^\n=\nÎº\nâ€‹\n(\nF\n1\n)\nâˆˆ\nâ„\nB\nÃ—\nK\nÃ—\nH\no\nâ€‹\nu\nâ€‹\nt\nÃ—\nW\no\nâ€‹\nu\nâ€‹\nt\n,\nK\n=\n5\n.\n\\hat{H}\\;=\\;\\kappa(F_{1})\\in\\mathbb{R}^{B\\times K\\times H_{\\!out}\\times W_{\\!out}},\\quad K=5.\n(4)\nWe optimize a mean squared error (MSE) loss over all fingers and pixels:\nâ„’\nMSE\n=\n1\n5\nâ€‹\n|\nÎ©\n|\nâ€‹\nâˆ‘\nk\n=\n1\n5\nâˆ‘\nu\nâˆˆ\nÎ©\n(\nH\n^\nk\nâ€‹\n(\nu\n)\nâˆ’\nH\nk\nâ€‹\n(\nu\n)\n)\n2\n.\n\\mathcal{L}_{\\mathrm{MSE}}=\\frac{1}{5|\\Omega|}\\sum_{k=1}^{5}\\sum_{u\\in\\Omega}\\bigl(\\hat{H}_{k}(u)-H_{k}(u)\\bigr)^{2}.\n(5)\nIII-C\nFinger-Specific Affordance Matching for Grasp Planning\nUpon obtaining the finger-specific affordances, we employ a perception-to-optimization pipeline (Fig.\n3\n) to synthesize executable dexterous grasp motions. We deliberately eschew any learned policy so that the method can be ported across distinct dexterous hand embodiments without model modification or retraining; adopting a pure motion-planning and optimization formulation preserves cross-embodiment generality. Our perception pipeline reconstructs a partial object point cloud by first generating a dense depth map from stereo infrared (IR) using the FoundationStereo\n[\n35\n]\n. The target object mask, segmented by GroundingDINO-SAM2\n[\n20\n,\n27\n]\n, is then back-projected using camera intrinsics to yield the final 3D point cloud. Using the finger keypoints obtained in Section\nIII-B\n, we first reâ€‘project the entire 3D point cloud onto the image plane. Then, for each finger keypoint, we use the previously described method to select the N pointâ€‘cloud projections that are closest to it in the image, forming robust fingertip contact candidates.\nApproach Vector Construction:\nGiven the predicted per-finger contact locations\nc\nk\nc_{k}\n, we estimate a consistent surface normal\nn\n^\nk\n\\hat{n}_{k}\nby fitting a local tangent plane to the object point cloud and resolving its orientation with an object-centric rule (normal oriented toward the contact patch). We then parameterize a short, geometry-aligned 1D approach vector for each finger:\nÎ³\nk\nâ€‹\n(\ns\n)\n=\nc\nk\n+\ns\nâ€‹\nn\n^\nk\n,\ns\nâˆˆ\nâ„\n,\n\\gamma_{k}(s)=c_{k}+s\\,\\hat{n}_{k},\\qquad s\\in\\mathbb{R},\n(6)\nand instantiate three phase anchors along this curve:\napproach\n(\ns\n>\n0\ns>0\n),\nclosure\n(\ns\nâ†’\n0\n+\ns\\to 0^{+}\n), and\nhold\n(\ns\n<\n0\ns<0\n, introducing a small preload). For each phase\nw\nâˆˆ\n{\napp\n,\nclo\n,\nhold\n}\nw\\in\\{\\text{app},\\text{clo},\\text{hold}\\}\n, the Cartesian target is defined as\np\nk\nâ‹†\nâ€‹\n(\nw\n)\n=\nÎ³\nk\nâ€‹\n(\ns\nw\n)\np_{k}^{\\star}(w)=\\gamma_{k}(s_{w})\n. A hand-level waypoint is obtained by taking the barycentric average over all finger targets.\nTask Formulation:\nLet\nq\nâˆˆ\nâ„\nn\nq\nq\\in\\mathbb{R}^{n_{q}}\nbe the full armâ€“hand joint vector. For fingertip pad\nk\nâˆˆ\n{\n1\n,\nâ€¦\n,\nK\n}\nk\\in\\{1,\\dots,K\\}\n,\nlet\np\nk\nâ€‹\n(\nq\n)\nâˆˆ\nâ„\n3\np_{k}(q)\\in\\mathbb{R}^{3}\nbe its forward kinematics and\nJ\nk\nâ€‹\n(\nq\n)\n=\nâˆ‚\np\nk\n/\nâˆ‚\nq\nâˆˆ\nâ„\n3\nÃ—\nn\nq\nJ_{k}(q)=\\partial p_{k}/\\partial q\\in\\mathbb{R}^{3\\times n_{q}}\nits Jacobian.\nThe goal is to drive all fingertip pads to the waypoint positions\n{\np\nk\nâ‹†\nâ€‹\n(\nw\n)\n}\n\\{p_{k}^{\\star}(w)\\}\n.\nAt control step\nt\nt\nwith active waypoint\nw\nt\nw_{t}\n, define the per-fingertip tracking residuals\nr\nk\nâ€‹\n(\nq\nt\n)\n=\np\nk\nâ€‹\n(\nq\nt\n)\nâˆ’\np\nk\nâ‹†\nâ€‹\n(\nw\nt\n)\nr_{k}(q_{t})=p_{k}(q_{t})-p_{k}^{\\star}(w_{t})\n.\nAdditionally, we model finger underactuation via a\ncoupling residual\ne\nc\nâ€‹\n(\nq\nt\n)\ne_{c}(q_{t})\nwith Jacobian\nJ\nc\nâ€‹\n(\nq\nt\n)\nJ_{c}(q_{t})\n,\nwhich is penalized in the objective (soft constraint):\nr\nt\n=\n[\nr\n1\nâ€‹\n(\nq\nt\n)\nâ‹®\nr\nK\nâ€‹\n(\nq\nt\n)\nr\ncoup\nâ€‹\n(\nq\nt\n)\n]\n=\n[\np\n1\nâ€‹\n(\nq\nt\n)\nâˆ’\np\n1\nâ‹†\nâ€‹\n(\nw\nt\n)\nâ‹®\np\nK\nâ€‹\n(\nq\nt\n)\nâˆ’\np\nK\nâ‹†\nâ€‹\n(\nw\nt\n)\ne\nc\nâ€‹\n(\nq\nt\n)\n]\n,\nJ\nt\n=\n[\nJ\n1\nâ€‹\n(\nq\nt\n)\nâ‹®\nJ\nK\nâ€‹\n(\nq\nt\n)\nJ\nc\nâ€‹\n(\nq\nt\n)\n]\n,\nr_{t}=\\begin{bmatrix}r_{1}(q_{t})\\\\[2.0pt]\n\\vdots\\\\[2.0pt]\nr_{K}(q_{t})\\\\[2.0pt]\nr_{\\mathrm{coup}}(q_{t})\\end{bmatrix}=\\begin{bmatrix}p_{1}(q_{t})-p_{1}^{\\star}(w_{t})\\\\[2.0pt]\n\\vdots\\\\[2.0pt]\np_{K}(q_{t})-p_{K}^{\\star}(w_{t})\\\\[2.0pt]\ne_{c}(q_{t})\\end{bmatrix},\\qquad J_{t}=\\begin{bmatrix}J_{1}(q_{t})\\\\[2.0pt]\n\\vdots\\\\[2.0pt]\nJ_{K}(q_{t})\\\\[2.0pt]\nJ_{c}(q_{t})\\end{bmatrix},\nwith weight matrix\nW\n=\ndiag\nâ€‹\n(\nÏ‰\n1\nâ€‹\nI\n3\n,\nâ€¦\n,\nÏ‰\nK\nâ€‹\nI\n3\n,\nÎ»\ncoup\nâ€‹\nI\nm\nc\n)\nW=\\mathrm{diag}(\\omega_{1}I_{3},\\dots,\\omega_{K}I_{3},\\lambda_{\\mathrm{coup}}I_{m_{c}})\n.\nA Gaussâ€“Newton step solves for\nÎ”\nâ€‹\nq\nt\n\\Delta q_{t}\nvia the convex quadratic program:\nmin\nÎ”\nâ€‹\nq\nt\n\\displaystyle\\min_{\\Delta q_{t}}\n1\n2\nâ€‹\nâ€–\nW\n1\n/\n2\nâ€‹\n(\nJ\nt\nâ€‹\nÎ”\nâ€‹\nq\nt\n+\nr\nt\n)\nâ€–\n2\n2\n+\nÎ»\ndamp\n2\nâ€‹\nâ€–\nÎ”\nâ€‹\nq\nt\nâ€–\n2\n2\n\\displaystyle\\tfrac{1}{2}\\big\\|W^{1/2}\\big(J_{t}\\Delta q_{t}+r_{t}\\big)\\big\\|_{2}^{2}+\\tfrac{\\lambda_{\\text{damp}}}{2}\\|\\Delta q_{t}\\|_{2}^{2}\n(7)\ns.t.\nq\nmin\nâˆ’\nq\nt\nâ‰¤\nÎ”\nâ€‹\nq\nt\nâ‰¤\nq\nmax\nâˆ’\nq\nt\n\\displaystyle q_{\\min}-q_{t}\\leq\\Delta q_{t}\\leq q_{\\max}-q_{t}\nG\ncol\nâ€‹\nÎ”\nâ€‹\nq\nt\nâ‰¤\nh\ncol\n\\displaystyle G_{\\text{col}}\\Delta q_{t}\\leq h_{\\text{col}}\nwhere\nÎ»\ndamp\n\\lambda_{\\text{damp}}\nis a Levenbergâ€“Marquardt damping term; the first constraint enforces joint bounds and the second specifies linearized collision avoidance constraints\n(including table contact and self-collisions). We warm-start OSQP\n[\n32\n]\nat\nq\nt\nq_{t}\n, set\nq\nt\n+\n1\n=\nq\nt\n+\nÎ”\nâ€‹\nq\nt\nâ‹†\nq_{t+1}=q_{t}+\\Delta q_{t}^{\\star}\n,\nand recompute forward kinematics for the next step, ensuring motion continuity and avoiding commands that would place the robot in unreachable configurations.\nExecution.\nThe controller is implemented in the Mink framework\n[\n41\n]\n;\nsolutions are computed in simulation and synchronized to the physical robot in real time.\nIV\nEXPERIMENTS\nIn this section, we conduct comprehensive experiments to validate our method from three complementary perspectives:\nRQ1:\nDoes the proposed finger-specific grasp representation outperform prior keypoint/affordance grounding, and does leveraging visionâ€“language priors from Stable Diffusion model yield gains over discriminative backbones?\nRQ2:\nDoes the method improve the functional correctness and stability of dexterous grasps on seen and unseen objects, even for objects with weak part cues?\nRQ3:\nDoes the learned finger-specific affordance representation transfer across dexterous hands with minimal adaptation?\nIV-A\nExperimental Setup\nFigure 4\n:\nRobotic arm and dexterous-hand experimental platform used in our study.\nDataset:\nWe collect 130\nfinger-specific\nhuman grasp demonstrations over 13 everyday objects (10 per object; e.g., banana, screwdriver, bottle). For evaluation, we test on seven\nunseen\nobjects that include both (i) new instances of previously seen categories and (ii) entirely novel categories (e.g., wrench, hammer).\nFigure 5\n:\nQualitative comparison of finger-specific affordance grounding.\nLeft:\nSeen\nobjects; Right:\nUnseen\nobjects. Rows show ReKep, CMKA, Ours (DINO features), and Ours (Stable Diffusion). Overlays visualize five per-finger likelihood maps; colored dots indicate annotated fingertip contacts. The diffusion-based variant produces sharper, finger-disentangled hotspots aligned with functional parts and preserves localization quality on unseen tools. ReKep and CMKA each produce keypoints directly and a single-channel heatmap is generated via post-processing following CMKA, thus their overlays are rendered in a single purple color, whereas our method outputs five-channel heatmaps rendered with five distinct colors.\nImplementation Details:\nWe train for\n4\n,\n000\n4{,}000\nsteps with batch size\n2\n2\non a single NVIDIA H100 GPU using AdamW (initial learning rate\n10\nâˆ’\n3\n10^{-3}\nwith cosine decay). Raw RGB frames are resized to\n640\nÃ—\n320\n640\\times 320\nwith letterboxing when necessary to preserve aspect ratio, and pixel intensities are normalized to\n[\n0\n,\n1\n]\n[0,1]\n. Predicted and ground-truth finger heatmaps are uniformly rescaled to\n448\nÃ—\n448\n448\\times 448\nfollowing saliency-grounding conventions, and each per-finger heatmap is re-normalized to integrate to\n1\n1\n.\nAll results are averaged over 3 random seeds.\nThe robotic armâ€“dexterous-hand platform is shown in Fig.\n4\n; the real-world setup comprises two dexterous hands (DexHand021 and Linker Hand L20), a Franka arm, and a RealSenseÂ 435 camera.\nEvaluation Metrics:\nFor finger-specific affordance grounding, We report KLD, SIM, and NSS\n[\n2\n]\n:\nâ€¢\nKLD\n(lower is better): Kullbackâ€“Leibler divergence between predicted and ground-truth heatmaps.\nâ€¢\nSIM\n(higher is better): Normalized histogram intersection (similarity).\nâ€¢\nNSS\n(higher is better): Normalized Scanpath Saliency; per finger we sample the annotated keypoint location as a fixation.\nFor Dexterous functional grasping, we follow the definitions of metrics provided\nin [4] and [5]:\nGrasping Success Rate (Suc. R.)\n: A grasp is considered\nsuccessful if the object can be lifted above 0.1 m and\nheld stable longer than 3 seconds. Here, grasping specifically refers to grasping based on manipulation affordances.\nIV-B\nResults of Finger-Specific Affordance Grounding\nBecause there is no prior work that directly couples vision-language models with fine-grained, finger-specific affordance keypoints, we select two closest approaches as baselines. (1) Rekep\n[\n14\n]\nis a robotic manipulation keypoint prediction framework that extracts image features using DINO and obtains keypoints via unsupervised clustering. (2) CMKA\n[\n37\n]\nleverages exocentric (third-person) interaction images as input, applies SAM\n[\n16\n]\nfor multi-scale object segmentation, and then performs clustering over the segmentation-driven feature hierarchy to learn object affordance keypoints.\nTo ensure fairness, we train CMKA\n[\n37\n]\nusing the exocentric interaction images from our dataset, keeping all other hyperparameters unchanged. For metric computation, we follow a procedure consistent with CMKA: the predicted keypoints are first rescaled to the original image resolution, then converted into heatmaps by placing Gaussian kernels (with\nÏƒ\n\\sigma\nset to shorter side of image size / 64) centered at each keypoint. To validate the necessity of large-scale generative models in our framework, we conduct ablations that replace the diffusion\n-\nderived hyperfeatures with discriminative backbones CLIP\n[\n26\n]\nand DINO\n[\n3\n]\nused purely as feature extractors.\nTABLE I\n:\nComparison and ablation on the finger-specific affordance grounding benchmark. Best results in\nbold\n. (\nâ†‘\nâ£\n/\nâ£\nâ†“\n\\uparrow/\\downarrow\ndenote higher/lower is better.)\nModel / Variant\nKLD(\nâ†“\n\\downarrow\n)\nSIM(\nâ†‘\n\\uparrow\n)\nNSS(\nâ†‘\n\\uparrow\n)\nReKep\n[\n14\n]\n10.127\n0.173\n1.801\nCMKA\n[\n37\n]\n11.184\n0.177\n1.861\nOurs (CLIP)\n6.690\n0.355\n3.815\nOurs (DINO)\n3.301\n0.473\n5.016\nOurs (SD)\n2.491\n0.551\n5.518\nTable\nI\nshows that both ReKep and CMKA fall short on finger-specific affordance grounding: ReKepâ€™s unsupervised keypoints, learned purely from image features, lack the semantic selectivity needed to localize functionally meaningful, finger-wise contacts, while CMKAâ€™s segmentation-driven hierarchy struggles when objects do not present distinctive, part-aligned regions. Fig.\n5\n(left,\nseen\n) and (right,\nunseen\n) qualitatively reveal these failure cases where on objects with weak or uniform appearance cue, e.g., the smooth, texture-poor\ntomato\nand the monochrome\nmouse\n, both baselines produce diffuse, centroid-biased blobs and scattered contact dots that either bleed across boundaries or miss functional parts. Our model simultaneously improves all three metricsâ€”lower KLD and higher SIM/NSSâ€”indicating more precise localization of affordance-relevant regions rather than generic centers or silhouette edges.\nHolding data, labels, losses, and training schedule fixed while varying only the feature extractor, we isolate the contribution of diffusion-derived hyperfeatures.\nKLD\nincreases by\n1.33\nÃ—\n1.33\\times\ncompared with DINO and\n2.69\nÃ—\n2.69\\times\nwith CLIP, while\nSIM\ndrops by\n14.2\n%\n14.2\\%\n/\n35.6\n%\n35.6\\%\nand\nNSS\nby\n9.1\n%\n9.1\\%\n/\n30.9\n%\n30.9\\%\n.\nOurs (DINO) tends to merge adjacent fingers into single peaks and produces broader activations with boundary spill-over. Besides, on\nunseen\nelongated tools (e.g.,\nhammer\n), it allocates excess activation to the visually salient head rather than the grasp-functional handle. In contrast, our diffusion-based model maintains sharp, part-aligned peaks along the handle and places stabilizing secondary contacts on supporting faces. Across both seen and unseen items (e.g.,\nscrewdriver\n,\nwrench\n,\nbottle\n), diffusion hyperfeatures consistently suppress three dominant errors, i.e., centroid collapse, boundary spill-over, and fingertip swapping, supporting\nRQ1\nthat diffusion-derived hyperfeatures are critical for precise per-finger affordance localization.\nIV-C\nFunctional Affordance Grasping with High-DOF Hands\nOur evaluation targets\non-table\n, affordance-driven dexterous grasping with finger-specific contacts and human-consistent grasp sites.\nExisting simulation-trained policies\n[\n36\n]\noptimize for success in simplified settings and cannot select grasp sites following human conventions (e.g., grasping a hammer by its head).\nTo assess alignment with human grasp choices within our setting, we include two point-cloud-based imitation learning policies (Diffusion Policy 3D\n[\n42\n]\nand ACT-3D\n[\n11\n]\n) trained from our teleoperated demonstrations(30 trajectories). We also compare to CMKA representing a recent affordance-based dexterous grasping approach. We evaluate CMKA strictly under its original, unmodified pipeline: we directly use the three keypoints predicted by CMKA, lift them to 3D, assign them to the functional finger, little finger, and wrist, and then execute the methodâ€™s fixed grasp execution strategy.\nAs shown in Tab.\nII\n, the conventional imitation-learning baselines tend to reproduce trajectories close to demonstrations: as object morphology varies, they do not proactively adapt their strategy, leading to significantly degraded performance both on seen objects with poses absent from the training data and on unseen objects. CMKAâ€™s performance is limited by the layered segmentation output of SAM; as discussed in Section\nII-B\n, for objects that lack salient part delineation such as tomatoes and bananas, it often fails to predict correct grasp locations. Moreover, because it executes a preprogrammed grasp approaching motion, the fingers collide with the table surface when attempting to grasp low-profile objects such as bananas and screwdrivers, thus preventing successful task completion.\nBy contrast, our method achieves higher success on common objects and remains robust on previously unseen instances within the same categories. In Fig.\n6\n, per-finger affordance inference places contacts on grasp-functional geometry, while robust stereo depth estimation provides stable surface normals for accurate on-table execution. The geometry-aligned\nApproach Vector\nco-optimizes wrist pose and finger trajectories relative to the tabletop object, producing table-skimming entries that reliably avoid fingerâ€“table collisions, particularly beneficial for low-profile objects.\nFigure 6\n:\nAffordance-driven dexterous grasping in the real world.\nRepresentative rollouts on everyday objects and tools; columns denote tasks (banana, bottle, tomato, screwdriver, drill).\nTABLE II\n:\nPerformance comparison across different methods on common objects and tools manipulation tasks. Each cell reports\nN\n=\n20\nN{=}20\ntrials per object (S = seen, U = unseen).\nCommon Objects (%)\nTools (%)\nModel\nBottle (S/U)\nBanana\nTomato\nScrewdriver\nDrill\nACT-3D\n[\n11\n]\n30/25\n30\n50\n30\n40\nDiffusion Policy 3D\n[\n42\n]\n40/40\n50\n45\n0\n30\nCMKA\n[\n37\n]\n30/20\n20\n0\n0\n60\nOurs\n100/85\n85\n90\n60\n70\nIV-D\nCross-Embodiment Evaluation\nFigure 7\n:\nZero-shot generalization on unseen objects\nAs shown in the right column of Fig.Â 1, we maintain the same perception-to-optimization pipeline across dexterous-hand embodiments, performing finger-specific affordance inference fused with depth, followed by an approachâ€“closureâ€“hold planner. We transfer the pipeline from DexHand021 (\n12\n+\n5\n12{+}5\nDOFs) to Linker Hand L20 (\n16\n+\n5\n16{+}5\nDOFs) via\nkinematic retargeting of fingertip targets; perception and affordance inference remain unchanged.\nAs shown in Tab.\nIII\n, grasp success remains statistically unchanged across embodiments.\nThis invariance indicates that the proposed finger-specific affordance representation and geometry-aligned waypoint construction decouple grasp semantics from hand morphology, enabling deployment without hand-specific retraining.\nTABLE III\n:\nGrasp success rates across different hand embodiments.\nCommon Objects (%)\nTools (%)\nModel\nBottle (S/U)\nBanana\nTomato\nScrewdriver\nDrill\nDexHand021\n100/85\n85\n90\n60\n70\nLinker Hand L20\n100/80\n90\n100\n70\n90\nV\nCONCLUSION AND FUTURE WORK\nWe leverage pretrained generative modelsâ€™ understanding of handâ€“object interactions to extract both high-level object semantics and fine-grained visual cues, and fuse these with a small set of human demonstrations providing fingertip annotations. This enables learning\nfinger-specific affordances\nthat unify perception and control: our representation captures not only\nwhere\nto act but also\nhow\nto act. Built on this, our FSAG-based motion planning maps dexterous-hand postures onto finger-specific contacts, yielding stable functional grasps and generalization across multiple dexterous-hand embodiments. Empirically, our approach substantially outperforms contemporary methods on both affordance localization accuracy and functional grasping success rate.\nAs illustrated in Fig.\n7\n, occasional failures on slender objects arise primarily from the execution: a simple, fixed-offset open-loop closure can induce rotation or slip even when contact targets are valid.\nNext, we will close the loop around FSAF contacts via learned residual-impedance/operational-space control with contact-force estimation and slip detection, yielding more robust and dynamic manipulation.\nReferences\n[1]\nS. Bahl, R. Mendonca, L. Chen, U. Jain, and D. Pathak\n(2023)\nAffordances from human videos as a versatile representation for robotics\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 13778â€“13790\n.\nCited by:\nÂ§\nII-C\n.\n[2]\nZ. Bylinskii, T. Judd, A. Oliva, A. Torralba, and F. Durand\n(2018)\nWhat do different evaluation metrics tell us about saliency models?\n.\nIEEE transactions on pattern analysis and machine intelligence\n41\n(\n3\n),\npp.Â 740â€“757\n.\nCited by:\nÂ§\nIV-A\n.\n[3]\nM. Caron, H. Touvron, I. Misra, H. JÃ©gou, J. Mairal, P. Bojanowski, and A. Joulin\n(2021)\nEmerging properties in self-supervised vision transformers\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp.Â 9650â€“9660\n.\nCited by:\nÂ§\nIV-B\n.\n[4]\nY. Chao, W. Yang, Y. Xiang, P. Molchanov, A. Handa, J. Tremblay, Y. S. Narang, K. Van Wyk, U. Iqbal, S. Birchfield,\net al.\n(2021)\nDexYCB: a benchmark for capturing hand grasping of objects\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 9044â€“9053\n.\nCited by:\nÂ§\nII-B\n.\n[5]\nA. Delitzas, A. Takmaz, F. Tombari, R. Sumner, M. Pollefeys, and F. Engelmann\n(2024)\nSceneFun3D: fine-grained functionality and affordance understanding in 3d scenes\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 14531â€“14542\n.\nCited by:\nÂ§\nII-C\n.\n[6]\nS. Deng, X. Xu, C. Wu, K. Chen, and K. Jia\n(2021)\n3d affordancenet: a benchmark for visual object affordance understanding\n.\nIn\nproceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 1778â€“1787\n.\nCited by:\nÂ§\nII-C\n.\n[7]\nW. Dong, D. Huang, J. Liu, C. Tang, and H. Zhang\n(2025)\nRtagrasp: learning task-oriented grasping from human videos via retrieval, transfer, and alignment\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 1â€“7\n.\nCited by:\nÂ§\nII-B\n.\n[8]\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby\n(2021)\nAn image is worth 16x16 words: transformers for image recognition at scale\n.\nIn\nInternational Conference on Learning Representations\n,\nExternal Links:\nLink\nCited by:\nÂ§I\n.\n[9]\nH. Fang, H. Yan, Z. Tang, H. Fang, C. Wang, and C. Lu\n(2025)\nAnyDexGrasp: general dexterous grasping for different hands with human-level learning efficiency\n.\narXiv preprint arXiv:2502.16420\n.\nCited by:\nÂ§I\n.\n[10]\nC. Gabellieri, F. Angelini, V. Arapi, A. Palleschi, M. G. Catalano, G. Grioli, L. Pallottino, A. Bicchi, M. Bianchi, and M. Garabini\n(2020)\nGrasp it like a pro: grasp of unknown objects with robotic hands based on skilled human expertise\n.\nIEEE Robotics and Automation Letters\n5\n(\n2\n),\npp.Â 2808â€“2815\n.\nCited by:\nÂ§\nII-B\n.\n[11]\nT. Gervet, Z. Xian, N. Gkanatsios, and K. Fragkiadaki\n(2023)\nAct3D: 3d feature field transformers for multi-task robotic manipulation\n.\nIn\n7th Annual Conference on Robot Learning\n,\nExternal Links:\nLink\nCited by:\nÂ§\nIV-C\n,\nTABLE II\n.\n[12]\nJ. J. Gibson\n(1977)\nThe theory of affordances\n.\nHilldale, USA\n1\n(\n2\n),\npp.Â 67â€“82\n.\nCited by:\nÂ§\nII-C\n.\n[13]\nS. Haldar and L. Pinto\n(2025)\nPoint policy: unifying observations and actions with key points for robot manipulation\n.\narXiv preprint arXiv:2502.20391\n.\nCited by:\nÂ§\nII-A\n.\n[14]\nW. Huang, C. Wang, Y. Li, R. Zhang, and L. Fei-Fei\n(2025)\nReKep: spatio-temporal reasoning of relational keypoint constraints for robotic manipulation\n.\nIn\nConference on Robot Learning\n,\npp.Â 4573â€“4602\n.\nCited by:\nÂ§\nIV-B\n,\nTABLE I\n.\n[15]\nT. Jiang, P. Lu, L. Zhang, N. Ma, R. Han, C. Lyu, Y. Li, and K. Chen\n(2023)\nRTMPose: real-time multi-person pose estimation based on mmpose\n.\nCoRR\n.\nCited by:\nÂ§\nIII-A\n.\n[16]\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W. Lo,\net al.\n(2023)\nSegment anything\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp.Â 4015â€“4026\n.\nCited by:\nÂ§\nIV-B\n.\n[17]\nM. Kokic, D. Kragic, and J. Bohg\n(2020)\nLearning task-oriented grasping from human activity datasets\n.\nIEEE Robotics and Automation Letters\n5\n(\n2\n),\npp.Â 3352â€“3359\n.\nCited by:\nÂ§\nII-B\n.\n[18]\nK. Li, P. Li, T. Liu, Y. Li, and S. Huang\n(2025)\nManiptrans: efficient dexterous bimanual manipulation transfer via residual learning\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 6991â€“7003\n.\nCited by:\nÂ§\nII-A\n.\n[19]\nY. Li, N. Zhao, J. Xiao, C. Feng, X. Wang, and T. Chua\n(2024)\nLASO: language-guided affordance segmentation on 3d object\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 14251â€“14260\n.\nCited by:\nÂ§\nII-C\n.\n[20]\nS. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su,\net al.\n(2024)\nGrounding dino: marrying dino with grounded pre-training for open-set object detection\n.\nIn\nEuropean conference on computer vision\n,\npp.Â 38â€“55\n.\nCited by:\nÂ§\nIII-C\n.\n[21]\nG. Luo, L. Dunlap, D. H. Park, A. Holynski, and T. Darrell\n(2023)\nDiffusion hyperfeatures: searching through time and space for semantic correspondence\n.\nAdvances in Neural Information Processing Systems\n36\n,\npp.Â 47500â€“47510\n.\nCited by:\nÂ§\nIII-B\n1\n.\n[22]\nH. Luo, W. Zhai, J. Zhang, Y. Cao, and D. Tao\n(2022)\nLearning affordance grounding from exocentric images\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 2252â€“2261\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[23]\nH. Luo, W. Zhai, J. Zhang, Y. Cao, and D. Tao\n(2023)\nLearning visual affordance grounding from demonstration videos\n.\nIEEE Transactions on Neural Networks and Learning Systems\n.\nCited by:\nÂ§I\n,\nÂ§\nII-C\n.\n[24]\nS. Qian, W. Chen, M. Bai, X. Zhou, Z. Tu, and L. E. Li\n(2024)\nAffordancellm: grounding affordance from vision language models\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 7587â€“7597\n.\nCited by:\nÂ§\nII-C\n.\n[25]\nY. Qin, Y. Wu, S. Liu, H. Jiang, R. Yang, Y. Fu, and X. Wang\n(2022)\nDexmv: imitation learning for dexterous manipulation from human videos\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 570â€“587\n.\nCited by:\nÂ§\nII-A\n.\n[26]\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,\net al.\n(2021)\nLearning transferable visual models from natural language supervision\n.\nIn\nInternational conference on machine learning\n,\npp.Â 8748â€“8763\n.\nCited by:\nÂ§\nIV-B\n.\n[27]\nN. Ravi, V. Gabeur, Y. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. RÃ¤dle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, K. V. Alwala, N. Carion, C. Wu, R. Girshick, P. DollÃ¡r, and C. Feichtenhofer\n(2024)\nSAM 2: segment anything in images and videos\n.\nExternal Links:\n2408.00714\n,\nLink\nCited by:\nÂ§\nIII-C\n.\n[28]\nJ. Ren, P. Sundaresan, D. Sadigh, S. Choudhury, and J. Bohg\n(2025)\nMotion tracks: a unified representation for human-robot transfer in few-shot imitation learning\n.\narXiv preprint arXiv:2501.06994\n.\nCited by:\nÂ§\nII-A\n.\n[29]\nR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer\n(2022)\nHigh-resolution image synthesis with latent diffusion models\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 10684â€“10695\n.\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-C\n,\nÂ§\nIII-B\n1\n,\nÂ§\nIII-B\n1\n,\nÂ§III\n.\n[30]\nY. Shao, W. Zhai, Y. Yang, H. Luo, Y. Cao, and Z. Zha\n(2024)\nGREAT: geometry-intention collaborative inference for open-vocabulary 3d object affordance grounding\n.\narXiv preprint arXiv:2411.19626\n.\nCited by:\nÂ§\nII-C\n.\n[31]\nM. K. Srirama, S. Dasari, S. Bahl, and A. Gupta\n(2024)\nHrp: human affordances for robotic pre-training\n.\narXiv preprint arXiv:2407.18911\n.\nCited by:\nÂ§\nII-B\n.\n[32]\nB. Stellato, G. Banjac, P. Goulart, A. Bemporad, and S. Boyd\n(2020)\nOSQP: an operator splitting solver for quadratic programs\n.\nMathematical Programming Computation\n12\n(\n4\n),\npp.Â 637â€“672\n.\nCited by:\nÂ§\nIII-C\n.\n[33]\nH. Wang, S. Wang, Y. Zhong, Z. Yang, J. Wang, Z. Cui, J. Yuan, Y. Han, M. Liu, and Y. Ma\n(2025)\nAffordance-r1: reinforcement learning for generalizable affordance reasoning in multimodal large language model\n.\narXiv preprint arXiv:2508.06206\n.\nCited by:\nÂ§\nII-C\n.\n[34]\nR. Wang, J. Zhang, J. Chen, Y. Xu, P. Li, T. Liu, and H. Wang\n(2022)\nDexgraspnet: a large-scale robotic dexterous grasp dataset for general objects based on simulation\n.\narXiv preprint arXiv:2210.02697\n.\nCited by:\nÂ§I\n.\n[35]\nB. Wen, M. Trepte, J. Aribido, J. Kautz, O. Gallo, and S. Birchfield\n(2025)\nFoundationstereo: zero-shot stereo matching\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 5249â€“5260\n.\nCited by:\nÂ§\nIII-C\n.\n[36]\nY. Xu, W. Wan, J. Zhang, H. Liu, Z. Shan, H. Shen, R. Wang, H. Geng, Y. Weng, J. Chen,\net al.\n(2023)\nUnidexgrasp: universal robotic dexterous grasping via learning diverse proposal generation and goal-conditioned policy\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 4737â€“4746\n.\nCited by:\nÂ§I\n,\nÂ§\nIV-C\n.\n[37]\nF. Yang, D. Luo, W. Chen, J. Lin, J. Cai, K. Yang, Z. Li, and Y. Wang\n(2025)\nMulti-keypoint affordance representation for functional dexterous grasping\n.\narXiv preprint arXiv:2502.20018\n.\nCited by:\nÂ§\nII-B\n,\nÂ§\nIV-B\n,\nTABLE I\n,\nTABLE II\n.\n[38]\nJ. Yang, B. Li, F. Yang, A. Zeng, L. Zhang, and R. Zhang\n(2023)\nBoosting human-object interaction detection with text-to-image diffusion model\n.\narXiv preprint arXiv:2305.12252\n.\nCited by:\nÂ§I\n,\nÂ§\nII-C\n,\nÂ§\nIII-B\n1\n.\n[39]\nL. Yang, K. Li, X. Zhan, F. Wu, A. Xu, L. Liu, and C. Lu\n(2022)\nOakink: a large-scale knowledge repository for understanding hand-object interaction\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 20953â€“20962\n.\nCited by:\nÂ§\nII-B\n.\n[40]\nY. Yang, W. Zhai, H. Luo, Y. Cao, J. Luo, and Z. Zha\n(2023)\nGrounding 3d object affordance from 2d interactions in images\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.Â 10905â€“10915\n.\nCited by:\nÂ§\nII-C\n.\n[41]\nMink: Python inverse kinematics based on MuJoCo\nExternal Links:\nLink\nCited by:\nÂ§\nIII-C\n.\n[42]\nY. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, and H. Xu\n3D diffusion policy: generalizable visuomotor policy learning via simple 3d representations\n.\nIn\nICRA 2024 Workshop on 3D Visual Representations for Robot Manipulation\n,\nCited by:\nÂ§\nIV-C\n,\nTABLE II\n.\n[43]\nS. Zhao, X. Zhu, Y. Chen, C. Li, X. Zhang, M. Ding, and M. Tomizuka\n(2024)\nDexh2r: task-oriented dexterous manipulation from human to robots\n.\narXiv preprint arXiv:2411.04428\n.\nCited by:\nÂ§\nII-A\n.",
    "preview_text": "Dexterous grasp synthesis remains a central challenge: the high dimensionality and kinematic diversity of multi-fingered hands prevent direct transfer of algorithms developed for parallel-jaw grippers. Existing approaches typically depend on large, hardware-specific grasp datasets collected in simulation or through costly real-world trials, hindering scalability as new dexterous hand designs emerge. To this end, we propose a data-efficient framework, which is designed to bypass robot grasp data collection by exploiting the rich, object-centric semantic priors latent in pretrained generative diffusion models. Temporally aligned and fine-grained grasp affordances are extracted from raw human video demonstrations and fused with 3D scene geometry from depth images to infer semantically grounded contact targets. A kinematics-aware retargeting module then maps these affordance representations to diverse dexterous hands without per-hand retraining. The resulting system produces stable, functionally appropriate multi-contact grasps that remain reliably successful across common objects and tools, while exhibiting strong generalization across previously unseen object instances within a category, pose variations, and multiple hand embodiments. This work (i) introduces a semantic affordance extraction pipeline leveraging vision-language generative priors for dexterous grasping, (ii) demonstrates cross-hand generalization without constructing hardware-specific grasp datasets, and (iii) establishes that a single depth modality suffices for high-performance grasp synthesis when coupled with foundation-model semantics. Our results highlight a path toward scalable, hardware-agnostic dexterous manipulation driven by human demonstrations and pretrained generative models.\n\nFSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models\nYifan Han\n1,2\n,\nPengfei Yi\n1,2\n,\nJunyan Li\n1,2\n,\nHanqing Wang\n3\n,\nGaojing Zhang\n4\n,\nQiÂ PengÂ Liu\n5\n,\nWenzhaoÂ Lian\n5â€ \n1\n",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "diffusion",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„æ•°æ®é«˜æ•ˆæ¡†æž¶ï¼Œé€šè¿‡äººç±»è§†é¢‘æ¼”ç¤ºå’Œé¢„è®­ç»ƒç”Ÿæˆæ¨¡åž‹ï¼Œå®žçŽ°çµå·§æ‰‹æŠ“å–çš„è¯­ä¹‰åŸºç¡€åŒ–å’Œè·¨ç¡¬ä»¶æ³›åŒ–ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šFSAG: é€šè¿‡æ‰©æ•£æ¨¡åž‹å¢žå¼ºäºº-çµå·§æ‰‹çš„æ‰‹æŒ‡ç‰¹å®šå¯ä¾›æ€§æŽ¥åœ°\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nçµå·§æŠ“å–åˆæˆæ˜¯æœºå™¨äººæ“ä½œçš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸Žå¹³è¡Œå¤¹çˆªç›¸æ¯”ï¼Œå¤šæŒ‡çµå·§æ‰‹å…·æœ‰æ›´é«˜çš„è¿åŠ¨è‡ªç”±åº¦å’ŒæŽ¥è§¦å¤æ‚æ€§ï¼Œè¿™ä½¿å¾—ä»Žå¹³è¡Œå¤¹çˆªç®—æ³•ç›´æŽ¥è¿ç§»å˜å¾—å›°éš¾ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºŽåœ¨ä»¿çœŸä¸­æˆ–é€šè¿‡æ˜‚è´µçš„çœŸå®žä¸–ç•Œè¯•éªŒæ”¶é›†çš„å¤§è§„æ¨¡ã€ç¡¬ä»¶ç‰¹å®šçš„æŠ“å–æ•°æ®é›†ï¼Œè¿™åœ¨æ–°çµå·§æ‰‹è®¾è®¡å‡ºçŽ°æ—¶é˜»ç¢äº†å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼ŒçŽ°æœ‰æ–¹æ³•å¾€å¾€å°†â€œå¦‚ä½•æŠ“â€ï¼ˆæ‰‹æŒ‡åè°ƒï¼‰å’Œâ€œåœ¨å“ªé‡ŒæŠ“â€ï¼ˆåŠŸèƒ½åŒºåŸŸå®šä½ï¼‰åˆ†å¼€å¤„ç†ï¼Œå¯¼è‡´ç­–ç•¥è„†å¼±ã€æ³›åŒ–èƒ½åŠ›å·®ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æå‡ºä¸€ç§**æ•°æ®é«˜æ•ˆ**çš„æ¡†æž¶ï¼Œç»•è¿‡æœºå™¨äººæŠ“å–æ•°æ®æ”¶é›†ï¼Œåˆ©ç”¨é¢„è®­ç»ƒç”Ÿæˆæ¨¡åž‹ä¸­çš„ä¸°å¯Œè¯­ä¹‰å…ˆéªŒã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡ºäº† **FSAG** æ¡†æž¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºŽï¼š\n- **æ‰‹æŒ‡ç‰¹å®šå¯ä¾›æ€§åœº (FSAF)**ï¼šä¸€ç§ç»†ç²’åº¦çš„ã€åŸºäºŽå¯¹è±¡çš„è¡¨ç¤ºï¼Œä¸ºæ¯ä¸ªæ½œåœ¨è¡¨é¢ä½ç½®åˆ†é…æ¯ä¸ªæ‰‹æŒ‡çš„æŽ¥è§¦ä¼¼ç„¶å’Œè§’è‰²æè¿°ç¬¦ï¼Œç»Ÿä¸€äº†â€œå¦‚ä½•â€å’Œâ€œå“ªé‡Œâ€çš„é—®é¢˜ã€‚\n- **åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„è¯­ä¹‰æå–**ï¼šåˆ©ç”¨**å†»ç»“çš„ Stable Diffusion U-Net** ä½œä¸ºè¯­ä¹‰ä¸»å¹²ï¼Œä»Žå…¶å¤šæ—¶é—´æ­¥ã€å¤šå°ºåº¦çš„åŽ»å™ªç‰¹å¾ä¸­èšåˆå‡ºâ€œè¶…ç‰¹å¾â€ï¼Œè¿™äº›ç‰¹å¾ç¼–ç äº†å¯¹è±¡åŠŸèƒ½ã€éƒ¨ä»¶å’Œäº¤äº’çš„å…³é”®è¯­ä¹‰ã€‚\n- **æ¼”ç¤ºè½»é‡åŒ–çš„æŠ“å–åˆæˆ**ï¼šä»…éœ€å°‘é‡å¸¦æŒ‡å°–æ ‡æ³¨çš„äººç±»æ¼”ç¤ºè§†é¢‘æ¥ç›‘ç£ FSAF çš„å­¦ä¹ ï¼Œå¤§å¹…å‡å°‘äº†æ•°æ®æ”¶é›†è´Ÿæ‹…ã€‚\n- **è·¨å…·èº«æ³›åŒ–**ï¼šé€šè¿‡ä¸€ä¸ª**è¿åŠ¨å­¦æ„ŸçŸ¥çš„ä¼˜åŒ–è§„åˆ’å™¨**ï¼ˆåŸºäºŽé˜»å°¼æœ€å°äºŒä¹˜äºŒæ¬¡è§„åˆ’ï¼‰ï¼Œå°† FSAF æŽ¨æ–­å‡ºçš„æŽ¥è§¦ç›®æ ‡æ˜ å°„åˆ°ä¸åŒçš„çµå·§æ‰‹ä¸Šï¼Œæ— éœ€ä¸ºæ¯ç§æ‰‹é‡æ–°è®­ç»ƒæ¨¡åž‹ï¼Œå®žçŽ°äº†ç¡¬ä»¶æ— å…³çš„æŠ“å–ç­–ç•¥è¿ç§»ã€‚\n\n### 3. ä¸»è¦å®žéªŒç»“æžœ\nåœ¨çœŸå®žæœºå™¨äººå¹³å°ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼š\n- **å¯ä¾›æ€§æŽ¥åœ°ç²¾åº¦**ï¼šåœ¨ KLDã€SIMã€NSS æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºŽ ReKepã€CMKA ç­‰åŸºçº¿ã€‚æ¶ˆèžå®žéªŒè¯æ˜Žï¼Œ**æ‰©æ•£æ¨¡åž‹è¶…ç‰¹å¾**æ¯” CLIPã€DINO ç­‰åˆ¤åˆ«å¼éª¨å¹²ç½‘ç»œèƒ½äº§ç”Ÿæ›´å°–é”ã€æ‰‹æŒ‡è§£è€¦çš„æ¿€æ´»å›¾ã€‚\n- **åŠŸèƒ½æ€§æŠ“å–æˆåŠŸçŽ‡**ï¼šåœ¨å¸¸è§ç‰©ä½“ï¼ˆå¦‚ç“¶å­ã€é¦™è•‰ï¼‰å’Œå·¥å…·ï¼ˆå¦‚èžºä¸åˆ€ã€ç”µé’»ï¼‰ä¸Šå®žçŽ°äº†é«˜è¾¾ **90%** çš„æŠ“å–æˆåŠŸçŽ‡ï¼Œä¼˜äºŽ ACT-3Dã€Diffusion Policy 3D ç­‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•å’Œ CMKAã€‚\n- **è·¨å…·èº«æ³›åŒ–**ï¼šå°†è®­ç»ƒå¥½çš„ç³»ç»Ÿ**é›¶æ ·æœ¬**è¿ç§»åˆ°å¦ä¸€æ¬¾çµå·§æ‰‹ï¼ˆLinker Hand L20ï¼‰ä¸Šï¼ŒæŠ“å–æˆåŠŸçŽ‡ä¿æŒç¨³å®šï¼Œè¯æ˜Žäº†æ–¹æ³•åœ¨ç¡¬ä»¶é—´çš„å¯ç§»æ¤æ€§ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\næœ¬å·¥ä½œçš„ä»·å€¼åœ¨äºŽï¼š\n- **æ–¹æ³•è®ºåˆ›æ–°**ï¼šé¦–æ¬¡å°†å¤§è§„æ¨¡è§†è§‰-è¯­è¨€ç”Ÿæˆæ¨¡åž‹çš„è¯­ä¹‰å…ˆéªŒç³»ç»Ÿåœ°ç”¨äºŽç»†ç²’åº¦ã€æ‰‹æŒ‡ç‰¹å®šçš„æœºå™¨äººå¯ä¾›æ€§å­¦ä¹ ï¼Œä¸ºæ•°æ®é«˜æ•ˆçš„çµå·§æ“ä½œå¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚\n- **å®žç”¨ä»·å€¼**ï¼šè¯æ˜Žäº†**å•ä¸€æ·±åº¦æ¨¡æ€**ç»“åˆåŸºç¡€æ¨¡åž‹è¯­ä¹‰å³å¯å®žçŽ°é«˜æ€§èƒ½æŠ“å–åˆæˆï¼Œé™ä½Žäº†å¯¹å¤šæ¨¡æ€æˆ–å®Œç¾Žå‡ ä½•ä¿¡æ¯çš„ä¾èµ–ã€‚æå‡ºçš„ç¡¬ä»¶æ— å…³æ¡†æž¶æœ‰åŠ©äºŽé™ä½Žæ–°çµå·§æ‰‹çš„éƒ¨ç½²æˆæœ¬ï¼ŒæŽ¨åŠ¨**å¯æ‰©å±•çš„çµå·§æ“ä½œ**å‘å±•ã€‚\n- **æœªæ¥æ–¹å‘**ï¼šå½“å‰å¼€çŽ¯æ‰§è¡Œåœ¨ç»†é•¿ç‰©ä½“ä¸Šå¯èƒ½å¤±è´¥ï¼Œæœªæ¥å°†é€šè¿‡ç»“åˆåŠ›æ„ŸçŸ¥å’Œæ»‘ç§»æ£€æµ‹çš„é—­çŽ¯æŽ§åˆ¶æ¥å¢žå¼ºé²æ£’æ€§ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€Žä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æžï¼šFSAG\n\n### **ä¸€ã€ æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**çµå·§æ‰‹æŠ“å–åˆæˆ**ä¸­çš„ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼š\n1.  **â€œå¦‚ä½•æŠ“â€ä¸Žâ€œä½•å¤„æŠ“â€çš„å‰²è£‚**ï¼šçŽ°æœ‰æ–¹æ³•é€šå¸¸å°†æŠ“å–ç­–ç•¥ï¼ˆæ‰‹æŒ‡åè°ƒã€æŽ¥è§¦ç‚¹ã€æŽ¥è¿‘æ–¹å‘ï¼‰ä¸ŽæŠ“å–ä½ç½®ï¼ˆåŠŸèƒ½åŒ–å¯æ“ä½œåŒºåŸŸï¼‰åˆ†å¼€å¤„ç†ï¼Œå¯¼è‡´ç­–ç•¥è„†å¼±ã€æ³›åŒ–èƒ½åŠ›å·®ã€‚\n2.  **æ•°æ®ä¾èµ–ä¸Žç¡¬ä»¶è€¦åˆ**ï¼šä¼ ç»Ÿæ–¹æ³•ä¸¥é‡ä¾èµ–åœ¨ä»¿çœŸæˆ–çœŸå®žä¸–ç•Œä¸­æ”¶é›†çš„ã€é’ˆå¯¹ç‰¹å®šçµå·§æ‰‹ç¡¬ä»¶çš„å¤§è§„æ¨¡æŠ“å–æ•°æ®é›†ã€‚è¿™å¯¼è‡´æˆæœ¬é«˜æ˜‚ï¼Œä¸”éš¾ä»¥æŽ¨å¹¿åˆ°æ–°è®¾è®¡çš„çµå·§æ‰‹ä¸Šï¼Œ**å¯æ‰©å±•æ€§å·®**ã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸º **FSAG** çš„æ•°æ®é«˜æ•ˆæ¡†æž¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºŽï¼š\n\n1.  **æå‡ºâ€œæ‰‹æŒ‡ç‰¹å®šå¯ä¾›æ€§åœºâ€ï¼ˆFinger-Specific Affordance Field, FSAFï¼‰**\n    -   **æ˜¯ä»€ä¹ˆ**ï¼šä¸€ç§ç»†ç²’åº¦çš„ã€**ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒ**çš„å¯†é›†æ˜ å°„è¡¨ç¤ºã€‚å®ƒä¸ºç‰©ä½“è¡¨é¢çš„æ¯ä¸ªæ½œåœ¨ä½ç½®ï¼Œä¸ä»…é¢„æµ‹å…¶æ˜¯å¦â€œå¯æŠ“å–â€ï¼Œæ›´å…³é”®çš„æ˜¯**ä¸ºæ¯ä¸ªæ‰‹æŒ‡ï¼ˆäº”æŒ‡ï¼‰åˆ†é…ç»“æž„åŒ–çš„ä¼¼ç„¶å€¼å’Œè§’è‰²æè¿°ç¬¦**ã€‚\n    -   **è§£å†³äº†ä»€ä¹ˆ**ï¼šç»Ÿä¸€äº†â€œå¦‚ä½•æŠ“â€ä¸Žâ€œä½•å¤„æŠ“â€ã€‚FSAF åŒæ—¶ç¼–ç äº†å¯è¡Œçš„æŽ¥è§¦ç‚¹æ”¾ç½®ã€æŽ¥è¿‘æ–¹å‘å‡è®¾ä»¥åŠæ‰‹æŒ‡é—´çš„ååŒå…³ç³»ï¼Œå°†æ•´ä½“çš„æŠ“å–å§¿æ€åˆ†è§£ä¸º**åŸºäºŽè¯­ä¹‰çš„æ‰‹æŒ‡è§’è‰²åˆ†é…**ã€‚\n\n2.  **åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡åž‹çš„è¯­ä¹‰å…ˆéªŒï¼Œå®žçŽ°æ•°æ®é«˜æ•ˆå­¦ä¹ **\n    -   **å…³é”®æŠ€æœ¯**ï¼š**é‡ç”¨å†»ç»“çš„ Stable Diffusion U-Net ä½œä¸ºè¯­ä¹‰éª¨å¹²ç½‘ç»œ**ï¼Œæå–å…¶åŽ»å™ªè¿‡ç¨‹ä¸­çš„å¤šå°ºåº¦ã€å¤šæ—¶é—´æ­¥ç‰¹å¾ï¼ˆç§°ä¸ºâ€œè¶…ç‰¹å¾â€ï¼‰ï¼Œè¿™äº›ç‰¹å¾ç¼–ç äº†å…³äºŽç‰©ä½“ã€éƒ¨ä»¶ã€ææ–™å’ŒåŠŸèƒ½å‡ ä½•çš„ä¸°å¯ŒçŸ¥è¯†ã€‚\n    -   **è§£å†³äº†ä»€ä¹ˆ**ï¼š**ç»•è¿‡æ˜‚è´µçš„æœºå™¨äººæŠ“å–æ•°æ®æ”¶é›†**ã€‚ä»…éœ€å°‘é‡ï¼ˆè®ºæ–‡ä¸­ä¸º130ä¸ªï¼‰å¸¦æŒ‡å°–æ ‡æ³¨çš„äººç±»æŠ“å–æ¼”ç¤ºè§†é¢‘ï¼Œå³å¯å­¦ä¹ åˆ°é«˜è´¨é‡çš„ FSAFã€‚è¿™å¤§å¹…é™ä½Žäº†æ•°æ®è´Ÿæ‹…ã€‚\n\n3.  **å®žçŽ°è·¨å…·èº«ï¼ˆCross-Embodimentï¼‰æ³›åŒ–**\n    -   **æ–¹æ³•**ï¼šé‡‡ç”¨**åŸºäºŽä¼˜åŒ–çš„è¿åŠ¨è§„åˆ’**ï¼ˆé˜»å°¼æœ€å°äºŒä¹˜äºŒæ¬¡è§„åˆ’ï¼ŒQPï¼‰æ¥æ‰§è¡ŒæŠ“å–ã€‚æ„ŸçŸ¥æ¨¡å—ï¼ˆFSAFé¢„æµ‹ï¼‰ä¸Žè§„åˆ’æ¨¡å—è§£è€¦ã€‚è§„åˆ’å™¨ä»…æŽ¥æ”¶ä»ŽFSAFå’Œæ·±åº¦å›¾åƒä¸­æŽ¨å¯¼å‡ºçš„**å‡ ä½•å¯¹é½çš„æŽ¥è§¦ç‚¹ç›®æ ‡ä¸ŽæŽ¥è¿‘è·¯å¾„**ï¼Œè€Œä¸ä¾èµ–äºŽç‰¹å®šæ‰‹çš„è¿åŠ¨å­¦æ¨¡åž‹ã€‚\n    -   **è§£å†³äº†ä»€ä¹ˆ**ï¼š**å°†æŠ“å–è¯­ä¹‰ä¸Žç‰¹å®šç¡¬ä»¶è¿åŠ¨å­¦è§£è€¦**ã€‚ä½¿å¾—å­¦ä¹ åˆ°çš„æŠ“å–ç­–ç•¥èƒ½å¤Ÿ**é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰** è¿ç§»åˆ°ä¸åŒçš„çµå·§æ‰‹ï¼ˆå¦‚DexHand021å’ŒLinker Hand L20ï¼‰ä¸Šï¼Œæ— éœ€é’ˆå¯¹æ¯æ¬¾æ‰‹é‡æ–°è®­ç»ƒã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆï¼ˆæŠ€æœ¯æµç¨‹ï¼‰**\næ•´ä¸ªç³»ç»Ÿæ˜¯ä¸€ä¸ªä»Žæ„ŸçŸ¥åˆ°ä¼˜åŒ–çš„ç«¯åˆ°ç«¯ç®¡é“ï¼š\n\n1.  **æ•°æ®æ”¶é›†ä¸Žæ ‡æ³¨**ï¼šä»Žäººç±»æŠ“å–è§†é¢‘ä¸­ï¼Œè‡ªåŠ¨æ£€æµ‹æ‰‹éƒ¨å…³é”®ç‚¹ï¼Œç”Ÿæˆäº”æŒ‡æŒ‡å°–çš„2Dé«˜æ–¯çƒ­å›¾ä½œä¸ºç›‘ç£ä¿¡å·ã€‚\n2.  **FSAFé¢„æµ‹ï¼ˆæ ¸å¿ƒæ„ŸçŸ¥æ¨¡å—ï¼‰**ï¼š\n    -   **ç‰¹å¾æå–**ï¼šè¾“å…¥ç‰©ä½“RGBå›¾åƒå’Œæ–‡æœ¬æè¿°ï¼Œé€šè¿‡å†»ç»“çš„æ‰©æ•£U-Netæå–å¤šå°ºåº¦â€œè¶…ç‰¹å¾â€ `Ag`ã€‚\n    -   **çƒ­å›¾è§£ç **ï¼šä½¿ç”¨ä¸€ä¸ªFPNé£Žæ ¼çš„è§£ç å™¨ï¼Œå°† `Ag` ä¸Šé‡‡æ ·å¹¶è§£ç ä¸º **5é€šé“çš„ã€æ¯æ‰‹æŒ‡çš„ä¼¼ç„¶çƒ­å›¾** `H_hat`ã€‚\n3.  **3Dåœºæ™¯é‡å»º**ï¼šç»“åˆæ·±åº¦å›¾åƒï¼ˆFoundationStereoï¼‰å’Œç‰©ä½“åˆ†å‰²ï¼ˆGroundingDINO-SAM2ï¼‰ï¼Œç”Ÿæˆç‰©ä½“çš„**éƒ¨åˆ†ç‚¹äº‘**ã€‚\n4.  **æŠ“å–è§„åˆ’ä¸Žæ‰§è¡Œ**ï¼š\n    -   **æŽ¥è§¦ç‚¹é€‰æ‹©**ï¼šå°†é¢„æµ‹çš„2DæŒ‡å°–çƒ­å›¾å³°å€¼åæŠ•å½±åˆ°3Dç‚¹äº‘ä¸Šï¼Œä½œä¸ºå€™é€‰æŽ¥è§¦ç‚¹ã€‚\n    -   **è·¯å¾„ç”Ÿæˆ**ï¼šæ ¹æ®æŽ¥è§¦ç‚¹å¤„çš„å±€éƒ¨è¡¨é¢æ³•çº¿ï¼Œä¸ºæ¯ä¸ªæ‰‹æŒ‡æž„å»ºä¸€æ¡åˆ†ä¸º **â€œæŽ¥è¿‘-é—­åˆ-ä¿æŒâ€ä¸‰é˜¶æ®µ**çš„å‡ ä½•å¯¹é½è·¯å¾„ã€‚\n    -   **ä¼˜åŒ–æŽ§åˆ¶**ï¼šä½¿ç”¨ä¸€ä¸ª**é˜»å°¼æœ€å°äºŒä¹˜QPæ±‚è§£å™¨**ï¼Œåœ¨æ»¡è¶³å…³èŠ‚é™ä½ã€ç¢°æ’žé¿å…ç­‰çº¦æŸçš„æ¡ä»¶ä¸‹ï¼Œè·Ÿè¸ªæ‰€æœ‰æ‰‹æŒ‡çš„é˜¶æ®µè·¯å¾„ç‚¹ï¼Œç”Ÿæˆå¯æ‰§è¡Œçš„å…³èŠ‚æŽ§åˆ¶æŒ‡ä»¤ã€‚\n\n### **å››ã€ å®žé™…ä»·å€¼ä¸Žæ„ä¹‰**\n-   **æ•°æ®é«˜æ•ˆä¸Žä½Žæˆæœ¬**ï¼šæ˜¾è‘—å‡å°‘å¯¹å¤§è§„æ¨¡ã€ç¡¬ä»¶ç‰¹å®šæœºå™¨äººæ•°æ®é›†çš„ä¾èµ–ï¼Œåˆ©ç”¨æ˜“äºŽèŽ·å–çš„äººç±»è§†é¢‘å’Œå¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡åž‹å…ˆéªŒã€‚\n-   **ç¡¬ä»¶æ— å…³ä¸Žå¯æ‰©å±•**ï¼šä¸ºå®žçŽ°**å¯æ‰©å±•çš„ã€ç¡¬ä»¶æ— å…³çš„çµå·§æ“ä½œ**æŒ‡æ˜Žäº†ä¸€æ¡è·¯å¾„ï¼Œæ–°è®¾è®¡çš„çµå·§æ‰‹å¯ä»¥å¿«é€Ÿéƒ¨ç½²ï¼Œæ— éœ€ä»Žå¤´å¼€å§‹æ”¶é›†æ•°æ®ã€‚\n-   **åŠŸèƒ½åŒ–æŠ“å–**ï¼šç”Ÿæˆçš„æŠ“å–ä¸ä»…ç¨³å®šï¼Œè€Œä¸”ç¬¦åˆäººç±»æŠ“å–çš„åŠŸèƒ½æ€§æ„å›¾ï¼ˆä¾‹å¦‚ï¼Œæ¡é”¤å­çš„æ‰‹æŸ„è€Œéžé”¤å¤´ï¼‰ï¼Œæå‡äº†æŠ“å–çš„å®žç”¨æ€§å’Œä»»åŠ¡ç›¸å…³æ€§ã€‚\n-   **æ€§èƒ½å“è¶Š**ï¼šå®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨**æ‰‹æŒ‡ç‰¹å®šå¯ä¾›æ€§å®šä½ç²¾åº¦**å’Œ**çœŸå®žä¸–ç•Œçµå·§æŠ“å–æˆåŠŸçŽ‡**ï¼ˆæœ€é«˜è¾¾90%ï¼‰ä¸Šå‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„ç‰©ä½“ç±»åˆ«å’Œä¸åŒçµå·§æ‰‹ä¸Šã€‚\n\n**æ€»ç»“**ï¼šFSAG çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºŽåˆ›é€ æ€§åœ°åˆ©ç”¨**ç”Ÿæˆå¼æ‰©æ•£æ¨¡åž‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›**ï¼Œæž„å»ºäº†ä¸€ä¸ª**ç»†ç²’åº¦ã€æ‰‹æŒ‡çº§ã€è¯­ä¹‰ grounded çš„å¯ä¾›æ€§è¡¨ç¤ºï¼ˆFSAFï¼‰**ï¼Œå¹¶ä»¥æ­¤ä¸ºåŸºç¡€ï¼Œé€šè¿‡**ä¸Žç¡¬ä»¶è§£è€¦çš„ä¼˜åŒ–è§„åˆ’**ï¼Œå®žçŽ°äº†æ•°æ®é«˜æ•ˆã€åŠŸèƒ½æ­£ç¡®ä¸”å¯è·¨ç¡¬ä»¶æ³›åŒ–çš„çµå·§æŠ“å–åˆæˆã€‚",
            "timestamp": "2026-01-20T17:51:16.086110",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºŽä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºŽä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºŽå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡å‘å±•è„‰ç»œæ¢³ç†ï¼šä»Žç²—ç²’åº¦åˆ°ç»†ç²’åº¦ã€ä»Žæ•°æ®é©±åŠ¨åˆ°å…ˆéªŒé©±åŠ¨çš„çµå·§æ‰‹æŠ“å–\n\næœ¬æ–‡ï¼ˆFSAGï¼‰æ˜¯**åŸºäºŽé¢„è®­ç»ƒæ‰©æ•£æ¨¡åž‹çš„è¯­ä¹‰å…ˆéªŒ**å’Œ**äººç±»è§†é¢‘æ¼”ç¤º**ï¼Œå®žçŽ°**æ•°æ®é«˜æ•ˆã€è·¨å…·èº«ã€æŒ‡çº§ç»†ç²’åº¦**çš„æŠ“å–è§„åˆ’ã€‚å…¶å‘å±•è„‰ç»œå¯ä»¥æ¢³ç†ä¸ºä»¥ä¸‹å‡ ä¸ªå…³é”®æ–¹å‘ï¼Œæ¯ä¸€æ­¥éƒ½é’ˆå¯¹å‰ä½œçš„å±€é™æ€§è¿›è¡Œäº†é’ˆå¯¹æ€§æ”¹è¿›ï¼š\n\n### æ–¹å‘ä¸€ï¼šæŠ“å–è¡¨ç¤ºä»Žâ€œç²—ç²’åº¦åŒºåŸŸâ€åˆ°â€œç»†ç²’åº¦æŒ‡çº§æŽ¥è§¦â€\n\n1.  **æ—©æœŸå·¥ä½œï¼šå¹¶è¡Œå¤¹çˆªä¸ŽåŒºåŸŸçº§å¯æ“ä½œæ€§**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šä¼ ç»Ÿçš„å¹¶è¡Œå¤¹çˆªæŠ“å–ç ”ç©¶ï¼Œä»¥åŠæ—©æœŸåŸºäºŽæ·±åº¦å­¦ä¹ çš„å¯æ“ä½œæ€§é¢„æµ‹å·¥ä½œï¼ˆå¦‚ [22, 23]ï¼‰ã€‚\n    *   **æ ¸å¿ƒè¡¨ç¤º**ï¼šé¢„æµ‹ç‰©ä½“ä¸Šâ€œå¯æŠ“å–â€çš„**äºŒç»´è¾¹ç•Œæ¡†æˆ–æŽ©ç åŒºåŸŸ**ã€‚è¿™ç§è¡¨ç¤ºå›žç­”äº†â€œ**åœ¨å“ªé‡Œ**â€æŠ“å–ï¼Œä½†éžå¸¸ç²—ç³™ã€‚\n    *   **å±€é™æ€§**ï¼šå¯¹äºŽå…·æœ‰å¤šä¸ªæ‰‹æŒ‡ã€éœ€è¦å¤æ‚æŽ¥è§¦æ¨¡å¼çš„çµå·§æ‰‹è€Œè¨€ï¼Œä»…çŸ¥é“ä¸€ä¸ªå¤§è‡´åŒºåŸŸ**å®Œå…¨ä¸è¶³ä»¥æŒ‡å¯¼å…·ä½“çš„æŠ“å–åŠ¨ä½œ**ã€‚å®ƒæ— æ³•æŒ‡å®šæ¯ä¸ªæ‰‹æŒ‡åº”è¯¥æ”¾åœ¨å“ªé‡Œã€ä»¥ä»€ä¹ˆè§’åº¦æŽ¥è¿‘ã€‚\n\n2.  **ä¸­æœŸæ”¹è¿›ï¼šå…³é”®ç‚¹ä¸ŽæŽ¥è§¦ç‚¹è¡¨ç¤º**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šå°†æŠ“å–è¡¨ç¤ºä¸ºå‡ ä¸ª**3Då…³é”®ç‚¹**ï¼ˆå¦‚æ‰‹è…•ã€æŒ‡å°–ä½ç½®ï¼‰æˆ–**æŽ¥è§¦ç‚¹é›†åˆ**ï¼ˆå¦‚ [37, 39]ï¼‰ã€‚\n    *   **æ ¸å¿ƒè¡¨ç¤º**ï¼šä»ŽåŒºåŸŸç»†åŒ–åˆ°ç¦»æ•£çš„**ç‚¹**ã€‚è¿™ä¸ºæŽ§åˆ¶æä¾›äº†æ›´ç²¾ç¡®çš„ç›®æ ‡ã€‚\n    *   **å±€é™æ€§**ï¼š\n        *   **è¯­ä¹‰ç¼ºå¤±**ï¼šå…³é”®ç‚¹é€šå¸¸é€šè¿‡å‡ ä½•æˆ–å¤–è§‚ç‰¹å¾å­¦ä¹ ï¼Œç¼ºä¹å¯¹ç‰©ä½“**åŠŸèƒ½éƒ¨ä»¶**ï¼ˆå¦‚é”¤æŸ„ã€ç“¶ç›–ï¼‰çš„è¯­ä¹‰ç†è§£ï¼Œå¯¼è‡´æŠ“å–å¯èƒ½åŠŸèƒ½ä¸å½“ï¼ˆå¦‚æŠ“é”¤å¤´ï¼‰ã€‚\n        *   **æŒ‡çº§å…³ç³»æ¨¡ç³Š**ï¼šå¦‚CMKA [37] ä»…é¢„æµ‹ä¸‰ä¸ªå…³é”®ç‚¹ï¼ˆå¯¹åº”åŠŸèƒ½æŒ‡ã€å°æŒ‡ã€æ‰‹è…•ï¼‰ï¼Œ**å›ºå®šäº†æ‰‹æŒ‡çš„è§’è‰²åˆ†é…**ï¼Œé™åˆ¶äº†æŠ“å–ç­–ç•¥çš„å¤šæ ·æ€§ï¼Œä¸”éš¾ä»¥æ³›åŒ–åˆ°ä¸åŒå½¢æ€çš„ç‰©ä½“ã€‚\n        *   **æ•°æ®ä¾èµ–**ï¼šèŽ·å–ç²¾ç¡®çš„3Dæ‰‹-ç‰©ä½“æŽ¥è§¦æ•°æ®ï¼ˆå¦‚DexYCB [4]ï¼‰æˆæœ¬æžé«˜ã€‚\n\n3.  **æœ¬æ–‡FSAGçš„çªç ´ï¼šæŒ‡çº§å¯æ“ä½œæ€§åœº**\n    *   **æ ¸å¿ƒåˆ›æ–°**ï¼šæå‡º **Finger-Specific Affordance Field**ã€‚\n    *   **æ”¹è¿›ä¹‹å¤„**ï¼š\n        *   **ç»†ç²’åº¦ä¸Žè¯­ä¹‰ç»“åˆ**ï¼šä¸ºç‰©ä½“å›¾åƒä¸Šçš„æ¯ä¸ªåƒç´ é¢„æµ‹**äº”ä¸ªç‹¬ç«‹é€šé“çš„çƒ­åŠ›å›¾**ï¼Œåˆ†åˆ«å¯¹åº”æ‹‡æŒ‡ã€é£ŸæŒ‡ã€ä¸­æŒ‡ã€æ— åæŒ‡ã€å°æŒ‡çš„æŽ¥è§¦å¯èƒ½æ€§ã€‚è¿™ç›´æŽ¥å°†â€œåœ¨å“ªé‡ŒæŠ“â€åˆ†è§£ä¸ºâ€œæ¯ä¸ªæ‰‹æŒ‡åº”è¯¥æŽ¥è§¦å“ªé‡Œâ€ã€‚\n        *   **æ³¨å…¥è¯­ä¹‰å…ˆéªŒ**ï¼šåˆ©ç”¨**Stable Diffusion**åœ¨å¤§é‡äº’è”ç½‘å›¾åƒä¸­å­¦åˆ°çš„**ç‰©ä½“éƒ¨ä»¶ã€åŠŸèƒ½å’Œæ‰‹-ç‰©ä½“äº¤äº’çŸ¥è¯†**ï¼Œä½¿é¢„æµ‹çš„æŽ¥è§¦ç‚¹è‡ªç„¶è½åœ¨åŠŸèƒ½éƒ¨ä»¶ä¸Šï¼ˆå¦‚æŠ“ç“¶èº«ã€æ¡é”¤æŸ„ï¼‰ã€‚\n        *   **æ•°æ®é«˜æ•ˆ**ï¼šä»…éœ€å°‘é‡äººç±»è§†é¢‘æ¼”ç¤ºï¼ˆ130ä¸ªï¼‰æä¾›æŒ‡å°–æ ‡æ³¨ï¼Œå³å¯å­¦ä¹ åˆ°å¼ºå¤§çš„æŒ‡çº§å¯æ“ä½œæ€§é¢„æµ‹æ¨¡åž‹ï¼Œé¿å…äº†å¤§è§„æ¨¡æœºå™¨äººæŠ“å–æ•°æ®æ”¶é›†ã€‚\n\n### æ–¹å‘äºŒï¼šå­¦ä¹ æ–¹æ³•ä»Žâ€œå¤§è§„æ¨¡ä»¿çœŸ/çŽ°å®žæ•°æ®â€åˆ°â€œå°æ ·æœ¬æ¼”ç¤º+åŸºç¡€æ¨¡åž‹å…ˆéªŒâ€\n\n1.  **æ•°æ®é©±åŠ¨èŒƒå¼ï¼šä»¿çœŸRLä¸Žå¤§è§„æ¨¡çœŸå®žæ•°æ®**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šåœ¨ä»¿çœŸä¸­ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æŽ¢ç´¢é«˜ç»´åŠ¨ä½œç©ºé—´ï¼ˆå¦‚ [36]ï¼‰ï¼Œæˆ–æ”¶é›†å¤§è§„æ¨¡çœŸå®žä¸–ç•ŒæŠ“å–æ•°æ®ï¼ˆå¦‚ AnyDexGrasp [9]ï¼‰ã€‚\n    *   **ä¼˜åŠ¿**ï¼šèƒ½åœ¨ç‰¹å®šè®¾ç½®ä¸‹å®žçŽ°é«˜æ€§èƒ½ã€‚\n    *   **å±€é™æ€§**ï¼š\n        *   **ä»¿çœŸåˆ°çŽ°å®žçš„é¸¿æ²Ÿ**ï¼šä»¿çœŸç­–ç•¥ä¾èµ–å®Œç¾ŽçŠ¶æ€ä¿¡æ¯ï¼Œå¯¹çœŸå®žä¼ æ„Ÿå™ªå£°ï¼ˆå¦‚æ·±åº¦ç¼ºå¤±ã€é®æŒ¡ï¼‰é²æ£’æ€§å·®ã€‚\n        *   **æˆæœ¬ä¸Žæ³›åŒ–**ï¼šå¤§è§„æ¨¡çœŸå®žæ•°æ®æ”¶é›†**æˆæœ¬æžå…¶é«˜æ˜‚**ï¼Œä¸”å­¦åˆ°çš„ç­–ç•¥ä¸Ž**ç‰¹å®šçµå·§æ‰‹çš„åŠ¨åŠ›å­¦ç´§å¯†è€¦åˆ**ï¼Œéš¾ä»¥è¿ç§»åˆ°æ–°æ‰‹ä¸Šï¼ˆâ€œå…·èº«è¿‡æ‹Ÿåˆâ€ï¼‰ã€‚\n\n2.  **æ¨¡ä»¿å­¦ä¹ èŒƒå¼ï¼šä»Žäººç±»æ¼”ç¤ºç›´æŽ¥å­¦ä¹ **\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šé€šè¿‡è¿åŠ¨é‡å®šå‘å°†äººç±»è§†é¢‘è½¬æ¢ä¸ºæœºå™¨äººè½¨è¿¹è¿›è¡Œæ¨¡ä»¿å­¦ä¹ ï¼ˆå¦‚ [25]ï¼‰ã€‚\n    *   **ä¼˜åŠ¿**ï¼šåˆ©ç”¨äººç±»è‡ªç„¶ã€é«˜æ•ˆçš„æŠ€èƒ½ã€‚\n    *   **å±€é™æ€§**ï¼šé€šå¸¸ä»éœ€åœ¨ä»¿çœŸä¸­è¿›è¡Œå¤§é‡å¾®è°ƒï¼ˆå†æ¬¡å¼•å…¥sim-to-real gapï¼‰ï¼Œä¸”å­¦åˆ°çš„ç­–ç•¥æ˜¯**è½¨è¿¹çº§åˆ«çš„é»‘ç®±**ï¼Œå¯¹ç‰©ä½“å§¿æ€ã€å½¢çŠ¶å˜åŒ–çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚\n\n3.  **æœ¬æ–‡FSAGçš„çªç ´ï¼šå…ˆéªŒé©±åŠ¨çš„å°æ ·æœ¬å­¦ä¹ **\n    *   **æ ¸å¿ƒåˆ›æ–°**ï¼š**è§£è€¦æ„ŸçŸ¥ä¸ŽæŽ§åˆ¶**ï¼Œåˆ©ç”¨**å†»ç»“çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡åž‹**ä½œä¸ºè¯­ä¹‰æ„ŸçŸ¥éª¨å¹²ã€‚\n    *   **æ”¹è¿›ä¹‹å¤„**ï¼š\n        *   **ç»•è¿‡æœºå™¨äººæ•°æ®æ”¶é›†**ï¼šä¸å†éœ€è¦ä»»ä½•æœºå™¨äººæŠ“å–æ•°æ®ã€‚æ ¸å¿ƒçš„æŒ‡çº§å¯æ“ä½œæ€§é¢„æµ‹æ¨¡å—ä»…ä»Ž**äººç±»è§†é¢‘å›¾åƒ+æ‰©æ•£æ¨¡åž‹å…ˆéªŒ**ä¸­å­¦ä¹ ã€‚\n        *   **æ³›åŒ–èƒ½åŠ›æ¥æº**ï¼šæ³›åŒ–èƒ½åŠ›ä¸»è¦æ¥è‡ª**æ‰©æ•£æ¨¡åž‹å¼ºå¤§çš„è·¨ç±»åˆ«è¯­ä¹‰ç†è§£èƒ½åŠ›**ï¼Œè€Œéžå¤§é‡ä»»åŠ¡æ•°æ®ã€‚è¿™ä½¿å¾—æ¨¡åž‹èƒ½å¯¹æœªè§è¿‡çš„ç‰©ä½“å®žä¾‹è¿›è¡Œåˆç†çš„æŒ‡çº§æŽ¥è§¦ç‚¹é¢„æµ‹ã€‚\n        *   **è¡¨ç¤ºå¯è¿ç§»**ï¼šå­¦ä¹ åˆ°çš„æ˜¯ç‰©ä½“ä¸­å¿ƒçš„ã€æŒ‡çº§çš„**å¯æ“ä½œæ€§è¡¨ç¤º**ï¼Œè¯¥è¡¨ç¤ºä¸Žå…·ä½“æ‰‹åž‹æ— å…³ï¼Œä¸ºåŽç»­çš„è·¨å…·èº«è¿ç§»å¥ å®šäº†åŸºç¡€ã€‚\n\n### æ–¹å‘ä¸‰ï¼šç³»ç»Ÿæ³›åŒ–ä»Žâ€œå…·èº«ç‰¹å®šâ€åˆ°â€œç¡¬ä»¶æ— å…³â€\n\n1.  **ä¼ ç»Ÿå±€é™ï¼šä¸€æœºä¸€ç­–**\n    *   **å¤§å¤šæ•°çŽ°æœ‰æ–¹æ³•**ï¼šæ— è®ºæ˜¯ä»¿çœŸRLè¿˜æ˜¯æ¨¡ä»¿å­¦ä¹ ï¼Œå­¦åˆ°çš„ç­–ç•¥ï¼ˆç¥žç»ç½‘ç»œï¼‰å…¶å‚æ•°éƒ½éšå¼ç¼–ç äº†ç‰¹å®šçµå·§æ‰‹çš„å…³èŠ‚èŒƒå›´ã€è¿žæ†é•¿åº¦ã€æŒ‡å°–å¤§å°ç­‰ä¿¡æ¯ã€‚\n    *   **åŽæžœ**ï¼šä¸ºæ–°çš„çµå·§æ‰‹éƒ¨ç½²éœ€è¦**ä»Žå¤´å¼€å§‹æ”¶é›†æ•°æ®å’Œè®­ç»ƒ**ï¼Œæ— æ³•è¿ç§»ã€‚\n\n2.  **æœ¬æ–‡FSAGçš„çªç ´ï¼šåŸºäºŽä¼˜åŒ–çš„è·¨å…·èº«è§„åˆ’**\n    *   **æ ¸å¿ƒåˆ›æ–°**ï¼šé‡‡ç”¨**åŸºäºŽä¼˜åŒ–çš„è¿åŠ¨è§„åˆ’**ï¼ˆé˜»å°¼æœ€å°äºŒä¹˜QPï¼‰æ¥æ‰§è¡ŒæŠ“å–ï¼Œè€Œéžå­¦ä¹ ä¸€ä¸ªç­–ç•¥ç½‘ç»œã€‚\n    *   **æ”¹è¿›ä¹‹å¤„**ï¼š\n        *   **è¡¨ç¤ºä¸Žæ‰§è¡Œè§£è€¦**ï¼šFSAGæ¨¡å—è¾“å‡ºçš„æ˜¯**åœ¨ç‰©ä½“åæ ‡ç³»ä¸‹çš„3DæŒ‡å°–æŽ¥è§¦ç‚¹ç›®æ ‡åŠå…¶æ³•çº¿æ–¹å‘**ã€‚è¿™æ˜¯ä¸€ä¸ª**ä¸Žæ‰‹åž‹æ— å…³çš„ä»»åŠ¡æè¿°**ã€‚\n        *   **é€šç”¨æ‰§è¡Œå™¨**ï¼šä¼˜åŒ–å™¨æŽ¥æ”¶è¿™äº›ç›®æ ‡ï¼Œç»“åˆ**å½“å‰ä½¿ç”¨çš„çµå·§æ‰‹çš„è¿åŠ¨å­¦ï¼ˆæ­£å‘è¿åŠ¨å­¦ã€é›…å¯æ¯”çŸ©é˜µï¼‰å’ŒåŠ¨åŠ›å­¦çº¦æŸ**ï¼ˆå…³èŠ‚é™ä½ã€ç¢°æ’žï¼‰ï¼Œå®žæ—¶æ±‚è§£å…³èŠ‚æŒ‡ä»¤ã€‚**åªéœ€æ›´æ¢è¿åŠ¨å­¦æ¨¡åž‹ï¼ŒåŒä¸€å¥—è§„åˆ’å™¨å³å¯é€‚é…æ–°æ‰‹**ã€‚\n        *   **å®žéªŒéªŒè¯**ï¼šè®ºæ–‡å±•ç¤ºäº†åœ¨DexHand021å’ŒLinker Hand L20ä¸¤ç§æˆªç„¶ä¸åŒçš„çµå·§æ‰‹ä¹‹é—´**é›¶æ ·æœ¬è¿ç§»**ï¼ŒæˆåŠŸçŽ‡ä¿æŒç¨³å®šï¼Œè¯æ˜Žäº†å…¶ç¡¬ä»¶æ— å…³æ€§ã€‚\n\n### æ€»ç»“ï¼šFSAGçš„æ ¸å¿ƒæŽ¨è¿›\nFSAGå¹¶éžå­¤ç«‹å·¥ä½œï¼Œå®ƒç«™åœ¨äº†ä¸‰ä¸ªå‘å±•æ–¹å‘çš„å‰æ²¿äº¤æ±‡ç‚¹ï¼š\n1.  **åœ¨è¡¨ç¤ºä¸Š**ï¼Œå°†**è¯­ä¹‰åŸºç¡€æ¨¡åž‹**ä¸Ž**æŒ‡çº§ç»†ç²’åº¦å»ºæ¨¡**ç»“åˆï¼Œå®žçŽ°äº†åŠŸèƒ½æ„ŸçŸ¥çš„æŠ“å–ã€‚\n2.  **åœ¨æ•°æ®ä¸Š**ï¼Œç”¨**åŸºç¡€æ¨¡åž‹å…ˆéªŒ**æ›¿ä»£**æµ·é‡æœºå™¨äººæ•°æ®**ï¼Œå®žçŽ°äº†å°æ ·æœ¬é«˜æ•ˆå­¦ä¹ ã€‚\n3.  **åœ¨ç³»ç»Ÿä¸Š**ï¼Œç”¨**ä¸Žæ‰‹åž‹æ— å…³çš„å¯æ“ä½œæ€§è¡¨ç¤º+æ¨¡åž‹é¢„æµ‹æŽ§åˆ¶**æ›¿ä»£**å…·èº«ç‰¹å®šçš„ç­–ç•¥ç½‘ç»œ**ï¼Œå®žçŽ°äº†çœŸæ­£çš„è·¨å…·èº«æ³›åŒ–ã€‚\n\nå®ƒæŒ‡æ˜Žäº†ä¸€ä¸ªæ–°èŒƒå¼ï¼š**åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡åž‹è•´å«çš„ç‰©ç†å’Œè¯­ä¹‰å¸¸è¯†ï¼Œå°†äººç±»æ¼”ç¤ºæç‚¼ä¸ºå¯è¿ç§»çš„ã€ç»†ç²’åº¦çš„ä»»åŠ¡è¡¨ç¤ºï¼Œå†é€šè¿‡é€šç”¨è§„åˆ’å™¨é€‚é…åˆ°ä¸åŒçš„æœºå™¨äººèº«ä½“ä¸Š**ï¼Œä»Žè€Œè¿ˆå‘å¯æ‰©å±•çš„ã€é€šç”¨çš„çµå·§æ“ä½œã€‚",
            "timestamp": "2026-01-20T17:52:13.127404",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºŽå…·ä½“ç›¸æ¯”äºŽä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## FSAG è®ºæ–‡çš„å‰ä½œåˆ†æžä¸Žå¯¹æ¯”\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼ŒFSAG çš„ç ”ç©¶å»ºç«‹åœ¨ä½œè€…å›¢é˜ŸåŠé¢†åŸŸå†…ä¸€ç³»åˆ—å‰æœŸå·¥ä½œçš„åŸºç¡€ä¸Šã€‚è®ºæ–‡åœ¨â€œç›¸å…³å·¥ä½œâ€éƒ¨åˆ†å¼•ç”¨äº†å¤šç¯‡æ–‡çŒ®ï¼Œä½†å¹¶æœªæ˜Žç¡®æŒ‡å‡ºå“ªäº›æ˜¯æœ¬æ–‡ä½œè€…çš„â€œå‰ä½œâ€ã€‚å› æ­¤ï¼Œä»¥ä¸‹åˆ†æžå°†åŸºäºŽè®ºæ–‡ä¸­å¼•ç”¨çš„ã€ä¸Žæœ¬æ–‡æ ¸å¿ƒæ–¹æ³•ï¼ˆ**åˆ©ç”¨æ‰©æ•£æ¨¡åž‹è¿›è¡Œç»†ç²’åº¦ã€æ‰‹æŒ‡ç‰¹å®šçš„å¯ä¾›æ€§å­¦ä¹ **ï¼‰æœ€ç›´æŽ¥ç›¸å…³ä¸”å¯èƒ½æž„æˆæŠ€æœ¯æ¼”è¿›è·¯å¾„çš„å‡ ç¯‡å…³é”®è®ºæ–‡è¿›è¡Œæ¢³ç†å’Œå¯¹æ¯”ã€‚\n\n### å…³é”®å‰ä½œæŠ€æœ¯è„‰ç»œå¯¹æ¯”è¡¨\n\n| è®ºæ–‡ç®€ç§° / æ ¸å¿ƒä½œè€… | å‘è¡¨å¹´ä»½ / ä¼šè®® | æ ¸å¿ƒæ€æƒ³ä¸Žæ–¹æ³• | ä¸Ž FSAG çš„ä¸»è¦åŒºåˆ« | FSAG çš„ä¸»è¦æ”¹è¿›ä¸Žåˆ›æ–° |\n| :--- | :--- | :--- | :--- | :--- |\n| **DexMV** <br> (Qin et al.) | 2022 / ECCV | ä»Žäººç±»è§†é¢‘ä¸­æ¨¡ä»¿å­¦ä¹ çµå·§æ“ä½œã€‚é€šè¿‡**è¿åŠ¨é‡å®šå‘**å°†äººæ‰‹åŠ¨ä½œæ˜ å°„åˆ°æœºå™¨äººæ‰‹ä¸Šï¼Œå¹¶é€šå¸¸åœ¨ä»¿çœŸä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä»¥æŽŒæ¡æŠ€èƒ½ã€‚ | 1. **ä¾èµ–é‡å®šå‘ä¸Žä»¿çœŸå­¦ä¹ **ï¼šéœ€è¦å°†äººæ‰‹è¿åŠ¨é‡å®šå‘åˆ°ç‰¹å®šæœºå™¨äººæ¨¡åž‹ï¼Œå¹¶ä¾èµ–ä»¿çœŸè®­ç»ƒï¼Œå­˜åœ¨ sim-to-real å·®è·ã€‚<br>2. **åŠ¨ä½œå±‚é¢æ¨¡ä»¿**ï¼šä¾§é‡äºŽæ¨¡ä»¿æ•´ä½“çš„æ‰‹éƒ¨è¿åŠ¨è½¨è¿¹ï¼Œè€Œéžè§£è€¦çš„ã€åŸºäºŽè¯­ä¹‰çš„æŽ¥è§¦ç‚¹æŽ¨ç†ã€‚<br>3. ** embodiment ç»‘å®š**ï¼šç­–ç•¥ä¸Žç‰¹å®šçš„çµå·§æ‰‹æ¨¡åž‹ç´§å¯†è€¦åˆã€‚ | 1. **ç»•è¿‡é‡å®šå‘ä¸Žä»¿çœŸ**ï¼šç›´æŽ¥ä»Žäººç±»è§†é¢‘ä¸­æå–**ç‰©ä½“ä¸­å¿ƒ**çš„ã€**æ‰‹æŒ‡ç‰¹å®š**çš„å¯ä¾›æ€§è¯­ä¹‰ï¼Œæ— éœ€å°†å®Œæ•´äººæ‰‹è¿åŠ¨é‡å®šå‘åˆ°æœºå™¨äººã€‚<br>2. **è¯­ä¹‰å¯ä¾›æ€§è¡¨ç¤º**ï¼šæå‡º **FSAF**ï¼Œå°†â€œå¦‚ä½•æŠ“â€å’Œâ€œåœ¨å“ªé‡ŒæŠ“â€ç»Ÿä¸€ä¸ºä¸€ä¸ªåŸºäºŽè¯­ä¹‰çš„ã€æ¯æ‰‹æŒ‡çš„æŽ¥è§¦ä¼¼ç„¶åœºï¼Œè§£è€¦äº†æŠ“å–è¯­ä¹‰ä¸Žå…·ä½“è¿åŠ¨è½¨è¿¹ã€‚<br>3. **è·¨æœ¬ä½“æ³›åŒ–**ï¼šåŸºäºŽ FSAF çš„è§„åˆ’å™¨æ˜¯ä¼˜åŒ–é©±åŠ¨çš„ï¼Œå¯**é›¶æ ·æœ¬**é€‚é…åˆ°ä¸åŒçš„çµå·§æ‰‹ä¸Šï¼Œæ— éœ€é’ˆå¯¹æ¯ä¸ªæ‰‹é‡æ–°è®­ç»ƒã€‚ |\n| **CMKA** <br> (Yang et al.) | 2025 / arXiv | ä»Žç½‘ç»œå›¾åƒä¸­å­¦ä¹ äº¤äº’åŒºåŸŸï¼Œå°†ç‰©ä½“å…³é”®ç‚¹æ˜ å°„åˆ°æ‰‹è…•ã€é£ŸæŒ‡å’Œå°æ‹‡æŒ‡ä¸‰ä¸ªé”šç‚¹ï¼Œç”¨è¿™ä¸‰ä¸ªç‚¹å‚æ•°åŒ–ä¸€ä¸ªæŠ“å–å§¿åŠ¿ã€‚ | 1. **å…³é”®ç‚¹æ•°é‡ä¸Žè§’è‰²å›ºå®š**ï¼šä»…ä½¿ç”¨ä¸‰ä¸ªå…³é”®ç‚¹ï¼ˆæ‰‹è…•ã€é£ŸæŒ‡ã€å°æ‹‡æŒ‡ï¼‰æ¥å®šä¹‰æ•´ä¸ªæŠ“å–ï¼Œ**ä¸è¶³ä»¥æŒ‡å¯¼æ‰€æœ‰äº”ä¸ªæ‰‹æŒ‡çš„ç²¾ç»†åè°ƒ**ã€‚<br>2. **è¡¨ç¤ºç²’åº¦ç²—**ï¼šè¾“å‡ºæ˜¯ç¨€ç–çš„å…³é”®ç‚¹ï¼Œè€Œéžå¯†é›†çš„ã€æ¯æ‰‹æŒ‡çš„å¯ä¾›æ€§çƒ­å›¾ï¼Œç¼ºä¹å¯¹æŽ¥è§¦åŒºåŸŸå’Œæ‰‹æŒ‡é—´å…³ç³»çš„ç»†ç²’åº¦å»ºæ¨¡ã€‚<br>3. **ä¾èµ–åˆ†å‰²å±‚æ¬¡**ï¼šæ€§èƒ½å—é™äºŽ SAM çš„åˆ†å‰²è´¨é‡ï¼Œåœ¨ç¼ºä¹æ˜¾è‘—éƒ¨ä»¶è¾¹ç•Œçš„ç‰©ä½“ä¸Šï¼ˆå¦‚è¥¿çº¢æŸ¿ï¼‰å®¹æ˜“å¤±è´¥ã€‚ | 1. **äº”æŒ‡ç‰¹å®šè¡¨ç¤º**ï¼šä¸º**æ‰€æœ‰äº”ä¸ªæ‰‹æŒ‡**ç‹¬ç«‹é¢„æµ‹å¯†é›†çš„æŽ¥è§¦ä¼¼ç„¶çƒ­å›¾ï¼ˆFSAFï¼‰ï¼Œæä¾›äº†å®Œæ•´çš„ã€ç»†ç²’åº¦çš„æŠ“å–æŒ‡ä»¤ã€‚<br>2. **åˆ©ç”¨æ‰©æ•£æ¨¡åž‹å…ˆéªŒ**ï¼šæ ¸å¿ƒåˆ›æ–°åœ¨äºŽä½¿ç”¨**é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡åž‹**ä½œä¸ºè¯­ä¹‰éª¨å¹²ï¼Œæå–èžåˆäº†é«˜å±‚æ¬¡è¯­ä¹‰å’Œç»†ç²’åº¦è§†è§‰çº¿ç´¢çš„â€œè¶…ç‰¹å¾â€ï¼Œæ˜¾è‘—æå‡äº†åœ¨å¼±çº¹ç†æˆ–æ–°ç‰©ä½“ä¸Šçš„å¯ä¾›æ€§å®šä½ç²¾åº¦ã€‚<br>3. **å‡ ä½•å¯¹é½çš„è¿åŠ¨è§„åˆ’**ï¼šåŸºäºŽé¢„æµ‹çš„æŽ¥è§¦ç‚¹å’Œå±€éƒ¨è¡¨é¢æ³•çº¿ï¼Œæž„å»º**å‡ ä½•å¯¹é½çš„æŽ¥è¿‘å‘é‡**ï¼Œå¹¶åˆ†é˜¶æ®µï¼ˆæŽ¥è¿‘ã€é—­åˆã€ä¿æŒï¼‰è¿›è¡Œä¼˜åŒ–æŽ§åˆ¶ï¼Œæé«˜äº†åœ¨æ¡Œé¢åœºæ™¯ä¸‹çš„æˆåŠŸçŽ‡å’Œé²æ£’æ€§ã€‚ |\n| **Boosting HOI Detection...** <br> (Yang et al.) | 2023 / arXiv | æ­ç¤ºäº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡åž‹ï¼ˆå¦‚ Stable Diffusionï¼‰çš„ä¸­é—´åŽ»å™ªç‰¹å¾ç¼–ç äº†ä¸°å¯Œçš„**äºº-ç‰©äº¤äº’è¯­ä¹‰å…ˆéªŒ**ï¼Œå¯ç”¨äºŽæå‡ discriminative ä»»åŠ¡ï¼ˆå¦‚ HOI æ£€æµ‹ï¼‰çš„æ€§èƒ½ã€‚ | 1. **ä»»åŠ¡ä¸åŒ**ï¼šè¯¥å·¥ä½œå…³æ³¨**æ£€æµ‹**ä»»åŠ¡ï¼ˆè¯†åˆ«å›¾åƒä¸­äººä¸Žç‰©ä½“çš„äº¤äº’ï¼‰ï¼Œè€Œéž**ç”Ÿæˆ**æˆ–**è§„åˆ’**æœºå™¨äººçš„æŠ“å–åŠ¨ä½œã€‚<br>2. **æœªç”¨äºŽæœºå™¨äººæŽ§åˆ¶**ï¼šæ²¡æœ‰å°†æ‰©æ•£æ¨¡åž‹çš„å…ˆéªŒçŸ¥è¯†ç”¨äºŽæœºå™¨äººæ„ŸçŸ¥-æŽ§åˆ¶é—­çŽ¯ã€‚ | 1. **é¦–æ¬¡å°†æ‰©æ•£å…ˆéªŒç”¨äºŽçµå·§æŠ“å–**ï¼šFSAG æ˜¯**é¦–ä¸ª**ç³»ç»Ÿæ€§åœ°å°†é¢„è®­ç»ƒæ‰©æ•£æ¨¡åž‹çš„ç”Ÿæˆå…ˆéªŒé‡æ–°ç”¨äºŽ**çµå·§æ‰‹æŠ“å–åˆæˆ**çš„å·¥ä½œã€‚<br>2. **æž„å»º FSAF æ–°è¡¨ç¤º**ï¼šåŸºäºŽæ‰©æ•£ç‰¹å¾ï¼Œåˆ›æ–°æ€§åœ°æå‡ºäº† **Finger-Specific Affordance Field** è¿™ä¸€æ–°çš„ä¸­é—´è¡¨ç¤ºï¼Œä½œä¸ºè¿žæŽ¥è¯­ä¹‰æ„ŸçŸ¥ä¸Žè¿åŠ¨è§„åˆ’çš„æ¡¥æ¢ã€‚<br>3. **ç«¯åˆ°ç«¯ç³»ç»ŸéªŒè¯**ï¼šä¸ä»…éªŒè¯äº†è¡¨ç¤ºçš„ä¼˜è¶Šæ€§ï¼Œè¿˜æž„å»ºäº†å®Œæ•´çš„ä»Žæ„ŸçŸ¥ï¼ˆRGB-Dï¼‰åˆ°ä¼˜åŒ–æŽ§åˆ¶ï¼ˆQPï¼‰çš„æµæ°´çº¿ï¼Œå¹¶åœ¨çœŸå®žæœºå™¨äººå¹³å°ä¸ŠéªŒè¯äº†åŠŸèƒ½æ€§æŠ“å–çš„æˆåŠŸçŽ‡ã€‚ |\n| **Learning Affordance Grounding...** <br> (Luo et al.) | 2022 / CVPR | ä»Žå¤–ä¸­å¿ƒè§†è§’å›¾åƒä¸­å­¦ä¹ ç‰©ä½“çš„å¯ä¾›æ€§åŒºåŸŸï¼ˆé€šå¸¸è¾“å‡ºä¸€ä¸ªç²—ç³™çš„è¾¹ç•Œæ¡†æˆ–æŽ©ç ï¼‰ï¼Œä¸ºæŠ“å–æä¾›ç©ºé—´å…ˆéªŒã€‚ | 1. **è¡¨ç¤ºç²’åº¦éžå¸¸ç²—**ï¼šä»…é¢„æµ‹â€œ**å“ªé‡Œ**â€å¯ä»¥æŠ“å–çš„å¤§è‡´åŒºåŸŸï¼Œæ— æ³•æä¾›â€œ**å¦‚ä½•**â€æŠ“å–çš„ä»»ä½•æŒ‡å¯¼ï¼Œä¾‹å¦‚å“ªä¸ªæ‰‹æŒ‡åº”è¯¥æ”¾åœ¨å“ªé‡Œã€ä»¥ä»€ä¹ˆè§’åº¦æŽ¥è¿‘ã€‚<br>2. **é’ˆå¯¹å¹³è¡Œå¤¹çˆª**ï¼šè¿™ç±»æ–¹æ³•å¤§å¤šä¸ºç®€å•çš„å¹³è¡Œå¤¹çˆªè®¾è®¡ï¼Œå…¶è¡¨ç¤ºä¸è¶³ä»¥æŒ‡å¯¼å¤šæ‰‹æŒ‡çµå·§æ‰‹çš„å¤æ‚åè°ƒã€‚ | 1. **ä»Žâ€œåŒºåŸŸâ€åˆ°â€œæ‰‹æŒ‡-æŽ¥è§¦ç‚¹â€**ï¼šFSAG å°†å¯ä¾›æ€§é¢„æµ‹çš„ç²’åº¦ä»Žç‰©ä½“**åŒºåŸŸ**æå‡åˆ°æ¯ä¸ªæ‰‹æŒ‡çš„**æŽ¥è§¦ç‚¹**çº§åˆ«ï¼Œå®žçŽ°äº†çœŸæ­£çš„â€œæ‰‹æŒ‡ç‰¹å®šâ€ã€‚<br>2. **ç»Ÿä¸€â€œå¦‚ä½•â€ä¸Žâ€œå“ªé‡Œâ€**ï¼šFSAF åŒæ—¶ç¼–ç äº†æŽ¥è§¦ä½ç½®ã€æŽ¥è¿‘æ–¹å‘å‡è®¾å’Œæ‰‹æŒ‡é—´å…³ç³»å…¼å®¹æ€§ï¼Œç»Ÿä¸€äº†æŠ“å–è§„åˆ’çš„ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ã€‚<br>3. **æ•°æ®æ•ˆçŽ‡é«˜**ï¼šå¾—ç›ŠäºŽå¼ºå¤§çš„æ‰©æ•£æ¨¡åž‹å…ˆéªŒï¼ŒFSAG ä»…éœ€å°‘é‡ï¼ˆ130ä¸ªï¼‰å¸¦æŒ‡å°–æ ‡æ³¨çš„äººç±»æ¼”ç¤ºè§†é¢‘å³å¯å­¦ä¹ ï¼Œé¿å…äº†å¤§è§„æ¨¡æœºå™¨äººæ•°æ®æ”¶é›†ã€‚ |\n\n### æ€»ç»“ï¼šFSAG çš„æ ¸å¿ƒè¿›æ­¥\n\nFSAG ç›¸å¯¹äºŽå…¶æŠ€æœ¯å‰ä½œï¼Œå®žçŽ°äº†ä¸€æ¬¡**èŒƒå¼è½¬æ¢**ï¼š\n\n1.  **è¡¨ç¤ºå±‚é¢çš„é©æ–°**ï¼šä»Ž**æ¨¡ä»¿è¿åŠ¨è½¨è¿¹**ã€**é¢„æµ‹ç¨€ç–å…³é”®ç‚¹**æˆ–**ç²—ç³™å¯ä¾›æ€§åŒºåŸŸ**ï¼Œè·ƒè¿åˆ°å­¦ä¹ ä¸€ä¸ª**å¯†é›†çš„ã€è¯­ä¹‰ grounded çš„ã€äº”æŒ‡ç‰¹å®šçš„å¯ä¾›æ€§åœº**ã€‚è¿™æ˜¯å…¶æœ€æ ¸å¿ƒçš„è´¡çŒ®ã€‚\n2.  **æŠ€æœ¯è·¯å¾„çš„é©æ–°**ï¼š**é¦–åˆ›æ€§åœ°åˆ©ç”¨é¢„è®­ç»ƒç”Ÿæˆå¼æ‰©æ•£æ¨¡åž‹**ä½œä¸ºè¯­ä¹‰éª¨å¹²ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„åˆ¤åˆ«å¼éª¨å¹²ç½‘ç»œï¼ˆå¦‚ CNN, ViT, DINOï¼‰ã€‚è¿™ä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿåˆ©ç”¨äº’è”ç½‘å°ºåº¦æ•°æ®ä¸­è•´å«çš„å…³äºŽç‰©ä½“åŠŸèƒ½ã€éƒ¨ä»¶å’Œäº¤äº’çš„å¸¸è¯†ï¼Œæ˜¾è‘—æå‡äº†å¯¹æ–°ç‰©ä½“å’Œå¼±çº¹ç†ç‰©ä½“çš„æ³›åŒ–èƒ½åŠ›ã€‚\n3.  **ç³»ç»Ÿè®¾è®¡çš„é©æ–°**ï¼šé‡‡ç”¨â€œ**æ„ŸçŸ¥ï¼ˆæ‰©æ•£æ¨¡åž‹+FSAFï¼‰â†’ è§„åˆ’ï¼ˆä¼˜åŒ–æŽ§åˆ¶å™¨ï¼‰**â€çš„æ¾è€¦åˆæž¶æž„ã€‚æ„ŸçŸ¥éƒ¨åˆ†è¾“å‡ºä¸Žæœ¬ä½“æ— å…³çš„è¯­ä¹‰å¯ä¾›æ€§ï¼Œè§„åˆ’éƒ¨åˆ†åŸºäºŽé€šç”¨è¿åŠ¨å­¦è¿›è¡Œä¼˜åŒ–ã€‚è¿™ç§è®¾è®¡ä»Žæ ¹æœ¬ä¸Šå®žçŽ°äº†**è·¨çµå·§æ‰‹æœ¬ä½“çš„é›¶æ ·æœ¬æ³›åŒ–**ï¼Œè§£å†³äº†çŽ°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–ç‰¹å®šç¡¬ä»¶æ•°æ®é›†çš„ç“¶é¢ˆã€‚\n\n**ç®€è€Œè¨€ä¹‹ï¼ŒFSAG çš„æ ¸å¿ƒæ”¹å–„åœ¨äºŽï¼šé€šè¿‡å¼•å…¥æ‰©æ•£æ¨¡åž‹çš„å¼ºå¤§è¯­ä¹‰å…ˆéªŒï¼Œå®šä¹‰äº†ä¸€ä¸ªæ›´ç²¾ç»†ã€æ›´è§£è€¦çš„å¯ä¾›æ€§è¡¨ç¤ºï¼Œä»Žè€Œç”¨æžå°‘çš„äººç±»æ¼”ç¤ºæ•°æ®ï¼Œå®žçŽ°äº†é«˜æ€§èƒ½ã€å¯è·¨æœ¬ä½“æ³›åŒ–çš„çµå·§æŠ“å–åˆæˆï¼Œä¸º scalable çš„çµå·§æ“ä½œæŒ‡æ˜Žäº†ä¸€æ¡æ–°è·¯å¾„ã€‚**",
            "timestamp": "2026-01-20T17:53:08.951179",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜ŽæŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŽŸç†æŽ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## FSAG å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è§£\n\næœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®é«˜æ•ˆã€ç¡¬ä»¶æ— å…³çš„çµå·§æ‰‹æŠ“å–æ¡†æž¶ï¼Œå…¶æ ¸å¿ƒåœ¨äºŽåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡åž‹çš„è¯­ä¹‰å…ˆéªŒï¼Œä»Žå°‘é‡äººç±»æ¼”ç¤ºè§†é¢‘ä¸­å­¦ä¹ **æ‰‹æŒ‡ç‰¹å¼‚æ€§å¯ä¾›æ€§**ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„æŠ“å–åŠ¨ä½œã€‚å…³é”®æŠ€æœ¯æ–¹æ³•å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æ–¹æ³•æ¨¡å— | æ ¸å¿ƒç›®æ ‡ | æŠ€æœ¯ç»†èŠ‚ä¸Žæ•°å­¦åŽŸç† | å…·ä½“å‚æ•°ä¸Žå®žçŽ° |\n| :--- | :--- | :--- | :--- |\n| **1. æ‰‹æŒ‡ç‰¹å¼‚æ€§å¯ä¾›æ€§åœº (FSAF)** | ä»Žå•å¼ ç‰©ä½“å›¾åƒä¸­ï¼Œé¢„æµ‹æ¯ä¸ªåƒç´ ç‚¹å¯¹äº”æ ¹æ‰‹æŒ‡çš„æŽ¥è§¦å¯èƒ½æ€§åˆ†å¸ƒï¼Œç»Ÿä¸€â€œæŠ“å“ªé‡Œâ€å’Œâ€œå¦‚ä½•æŠ“â€ã€‚ | **1.1 åŸºäºŽæ‰©æ•£æ¨¡åž‹çš„è¶…ç‰¹å¾æå–ï¼š** <br> - **è¾“å…¥ï¼š** å¹²å‡€ç‰©ä½“å›¾åƒ \\(x_0\\) å’Œæ–‡æœ¬æç¤º \\(s\\)ï¼ˆå¦‚â€œa bottleâ€ï¼‰ã€‚<br> - **æ½œå˜é‡ç¼–ç ä¸ŽåŠ å™ªï¼š** ä½¿ç”¨VAEç¼–ç å™¨å¾—åˆ°æ½œå˜é‡ \\(z_0 = E(x_0)\\)ã€‚éµå¾ªDDPMå‰å‘è¿‡ç¨‹æ·»åŠ å™ªå£°ï¼š<br> \\(z_t = \\sqrt{\\bar{\\alpha}_t} z_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\\) <br> å…¶ä¸­ \\(\\bar{\\alpha}_t = \\prod_{\\tau=1}^{t} \\alpha_{\\tau}\\)ï¼Œ\\(\\alpha_{\\tau}\\) æ˜¯é¢„å®šä¹‰çš„å™ªå£°è°ƒåº¦å‚æ•°ã€‚<br> - **ç‰¹å¾èšåˆï¼š** å°†åŠ å™ªæ½œå˜é‡ \\(z_t\\)ã€æ—¶é—´æ­¥ \\(t\\) å’Œæ–‡æœ¬åµŒå…¥ \\(c = T(s)\\) è¾“å…¥**å†»ç»“çš„Stable Diffusion U-Net**ï¼Œæå–å¤šæ—¶é—´æ­¥ \\(t \\in \\mathcal{S}\\)ã€å¤šå°ºåº¦ \\(\\ell = 1,...,L\\) çš„ä¸­é—´ç‰¹å¾å›¾ \\(A_{v,\\ell}^{(t)} \\in \\mathbb{R}^{B \\times C_{\\ell} \\times H_{\\ell} \\times W_{\\ell}}\\)ã€‚<br> - **è¶…ç‰¹å¾ç”Ÿæˆï¼š** é€šè¿‡å¯å­¦ä¹ çš„æƒé‡ \\(w_{\\ell,t}\\) å’Œè½»é‡çº§ç“¶é¢ˆå±‚ \\(b_{\\ell}\\)ï¼ˆ1x1å·ç§¯+å…¨å±€å¹³å‡æ± åŒ–ï¼‰èšåˆè¿™äº›ç‰¹å¾ï¼š<br> \\(A_g = \\sum_{t \\in \\mathcal{S}} \\sum_{\\ell=1}^{L} w_{\\ell,t} \\cdot b_{\\ell}(A_{v,\\ell}^{(t)})\\) <br> \\(w_{\\ell,t}\\) é€šè¿‡åœ¨æ‰€æœ‰ \\((\\ell, t)\\) å¯¹ä¸Šåº”ç”¨softmaxå¾—åˆ°ï¼Œç¡®ä¿æƒé‡å½’ä¸€åŒ–ã€‚ | - **æ¨¡åž‹ï¼š** å†»ç»“çš„ Stable Diffusion v1.5 U-Netã€‚<br> - **ç‰¹å¾é€‰æ‹©ï¼š** ä»Žæ€»é‡‡æ ·æ­¥æ•° \\(T\\) ä¸­é€‰å–ä¸€ä¸ªå­é›† \\(\\mathcal{S}\\)ï¼Œ\\(|\\mathcal{S}| = S\\)ï¼ˆæ–‡ä¸­æœªç»™å‡ºå…·ä½“Så€¼ï¼Œé€šå¸¸S<<Tï¼‰ã€‚<br> - **ç“¶é¢ˆå±‚ï¼š** æ¯ä¸ª \\(b_{\\ell}\\) å°†ç‰¹å¾æŠ•å½±åˆ°å›ºå®šç»´åº¦ \\(d\\)ã€‚<br> - **è¾“å‡ºï¼š** å…¨å±€å¯ä¾›æ€§æè¿°ç¬¦ \\(A_g \\in \\mathbb{R}^{B \\times C \\times h \\times w}\\)ã€‚ |\n| | | **1.2 FPNå¼è§£ç å™¨ä¸Žçƒ­å›¾é¢„æµ‹ï¼š** <br> - **ç»“æž„ï¼š** é‡‡ç”¨è‡ªä¸Šè€Œä¸‹ã€æ¨ªå‘è¿žæŽ¥çš„ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ(FPN)è§£ç  \\(A_g\\)ï¼Œé€æ­¥ä¸Šé‡‡æ ·å¹¶èžåˆæµ…å±‚ç»†èŠ‚ç‰¹å¾ã€‚<br> - **è®¡ç®—è¿‡ç¨‹ï¼š** è®¾ \\(F_4 = 0\\)ï¼Œå¯¹äºŽå±‚çº§ \\(r = 3, 2, 1\\)ï¼š<br> \\(F_r = \\psi_r \\left( U_{s_r}(\\tau_r(F_{r+1})) + U_{s_r}(\\phi_r(A_g)) \\right)\\) <br> å…¶ä¸­ï¼š<br> &nbsp;&nbsp;â€¢ \\(\\phi_r\\)ï¼š1x1å·ç§¯ï¼Œå°† \\(A_g\\) æŠ•å½±åˆ°é€šé“æ•° \\(c_r\\)ï¼ˆåˆ†åˆ«ä¸º256, 128, 64ï¼‰ã€‚<br> &nbsp;&nbsp;â€¢ \\(\\tau_r\\)ï¼š1x1å·ç§¯é€‚é…å™¨ã€‚<br> &nbsp;&nbsp;â€¢ \\(U_{s_r}\\)ï¼šåŒçº¿æ€§ä¸Šé‡‡æ ·åˆ°ç›®æ ‡å°ºå¯¸ \\(s_r\\)ã€‚<br> &nbsp;&nbsp;â€¢ \\(\\psi_r\\)ï¼š3x3å·ç§¯ + éžçº¿æ€§æ¿€æ´»ï¼Œç”¨äºŽç‰¹å¾å¹³æ»‘ä¸Žèžåˆã€‚<br> - **æœ€ç»ˆé¢„æµ‹ï¼š** é€šè¿‡æœ€ç»ˆçš„3x3å·ç§¯å±‚ \\(\\kappa\\) å°† \\(F_1\\) æ˜ å°„ä¸ºäº”æŒ‡çƒ­å›¾ï¼š<br> \\(\\hat{H} = \\kappa(F_1) \\in \\mathbb{R}^{B \\times 5 \\times H_{out} \\times W_{out}}\\) | - **é€šé“æ•°ï¼š** \\(c_3=256, c_2=128, c_1=64\\)ã€‚<br> - **è¾“å‡ºå°ºå¯¸ï¼š** \\(H_{out} = W_{out} = 448\\)ã€‚<br> - **æŸå¤±å‡½æ•°ï¼š** å‡æ–¹è¯¯å·®(MSE)æŸå¤±ï¼š<br> \\(\\mathcal{L}_{MSE} = \\frac{1}{5|\\Omega|} \\sum_{k=1}^{5} \\sum_{u \\in \\Omega} (\\hat{H}_k(u) - H_k(u))^2\\) <br> å…¶ä¸­ \\(H_k(u) = \\exp(-\\|u - \\mu_k\\|_2^2 / (2\\sigma^2))\\) æ˜¯ä»¥æ ‡æ³¨æŒ‡å°–ä½ç½® \\(\\mu_k\\) ä¸ºä¸­å¿ƒçš„é«˜æ–¯çƒ­å›¾ï¼Œ\\(\\sigma = \\min(h,w)/64\\)ã€‚ |\n| **2. åŸºäºŽå¯ä¾›æ€§çš„è¿åŠ¨è§„åˆ’ä¸Žæ‰§è¡Œ** | å°†2Dæ‰‹æŒ‡ç‰¹å¼‚æ€§çƒ­å›¾è½¬åŒ–ä¸º3DæŽ¥è§¦ç‚¹ï¼Œå¹¶ç”Ÿæˆæ»¡è¶³è¿åŠ¨å­¦ä¸Žç¢°æ’žçº¦æŸçš„çµå·§æ‰‹è½¨è¿¹ã€‚ | **2.1 3Dåœºæ™¯é‡å»ºä¸ŽæŽ¥è§¦ç‚¹é€‰æ‹©ï¼š** <br> - **æ·±åº¦ä¼°è®¡ï¼š** ä½¿ç”¨FoundationStereoä»Žç«‹ä½“çº¢å¤–å›¾åƒç”Ÿæˆå¯†é›†æ·±åº¦å›¾ã€‚<br> - **å®žä¾‹åˆ†å‰²ï¼š** ä½¿ç”¨GroundingDINO-SAM2èŽ·å–ç›®æ ‡ç‰©ä½“æŽ©ç ã€‚<br> - **ç‚¹äº‘ç”Ÿæˆï¼š** ç»“åˆç›¸æœºå†…å‚ï¼Œå°†æŽ©ç å†…çš„åƒç´ åå‘æŠ•å½±ï¼Œå¾—åˆ°éƒ¨åˆ†ç‰©ä½“ç‚¹äº‘ \\(P \\subset \\mathbb{R}^3\\)ã€‚<br> - **æŽ¥è§¦ç‚¹é€‰å–ï¼š** å°†ç‚¹äº‘æŠ•å½±å›žå›¾åƒå¹³é¢ï¼Œå¯¹äºŽæ¯ä¸ªé¢„æµ‹çš„æŒ‡å°–çƒ­å›¾å³°å€¼ï¼Œé€‰æ‹©å…¶åœ¨å›¾åƒå¹³é¢ä¸Šæœ€è¿‘çš„Nä¸ªæŠ•å½±ç‚¹ä½œä¸º3DæŽ¥è§¦å€™é€‰ç‚¹ \\(c_k \\in \\mathbb{R}^3\\)ã€‚ | - **æ·±åº¦æ¨¡åž‹ï¼š** FoundationStereoã€‚<br> - **åˆ†å‰²æ¨¡åž‹ï¼š** GroundingDINO + SAM2ã€‚<br> - **æŽ¥è§¦å€™é€‰æ•°ï¼š** æ–‡ä¸­æåŠä¸ºæ¯ä¸ªæŒ‡å°–é€‰æ‹©Nä¸ªæœ€è¿‘ç‚¹ï¼Œå…·ä½“Nå€¼æœªæ˜Žç¡®ç»™å‡ºã€‚ |\n| | | **2.2 å‡ ä½•å¯¹é½çš„æŽ¥è¿‘å‘é‡æž„å»ºï¼š** <br> - **æ³•å‘é‡ä¼°è®¡ï¼š** åœ¨3DæŽ¥è§¦ç‚¹ \\(c_k\\) çš„é‚»åŸŸå†…æ‹Ÿåˆå±€éƒ¨åˆ‡å¹³é¢ï¼Œè®¡ç®—è¡¨é¢æ³•å‘é‡ \\(\\hat{n}_k\\)ï¼Œå¹¶æ ¹æ®ç‰©ä½“ä¸­å¿ƒæ–¹å‘è°ƒæ•´ç¬¦å·ï¼Œç¡®ä¿æŒ‡å‘ç‰©ä½“å¤–éƒ¨ã€‚<br> - **è½¨è¿¹å‚æ•°åŒ–ï¼š** ä¸ºæ¯æ ¹æ‰‹æŒ‡å®šä¹‰ä¸€æ¡æ²¿æ³•çº¿æ–¹å‘çš„1Dè½¨è¿¹ï¼š<br> \\(\\gamma_k(s) = c_k + s \\cdot \\hat{n}_k, \\quad s \\in \\mathbb{R}\\) <br> æ²¿æ­¤è½¨è¿¹å®šä¹‰ä¸‰ä¸ª**ç›¸ä½é”šç‚¹**ï¼š<br> &nbsp;&nbsp;â€¢ **æŽ¥è¿‘ (approach)ï¼š** \\(s > 0\\)ï¼Œæ‰‹æŒ‡ä»Žè¿œå¤„æŽ¥è¿‘ç‰©ä½“ã€‚<br> &nbsp;&nbsp;â€¢ **é—­åˆ (closure)ï¼š** \\(s \\to 0^+\\)ï¼Œæ‰‹æŒ‡å³å°†æŽ¥è§¦ç‰©ä½“è¡¨é¢ã€‚<br> &nbsp;&nbsp;â€¢ **ä¿æŒ (hold)ï¼š** \\(s < 0\\)ï¼Œæ‰‹æŒ‡è½»å¾®åµŒå…¥ç‰©ä½“å†…éƒ¨ï¼Œäº§ç”Ÿé¢„åŠ è½½åŠ›ã€‚ | - **ç›¸ä½ä½ç§»ï¼š** æ–‡ä¸­æœªç»™å‡ºå…·ä½“çš„ \\(s_{app}, s_{clo}, s_{hold}\\) å€¼ï¼Œè¿™äº›æ˜¯é¢„è®¾æˆ–è°ƒå‚çš„è¶…å‚æ•°ã€‚ |\n| | | **2.3 é˜»å°¼æœ€å°äºŒä¹˜QPä¼˜åŒ–æŽ§åˆ¶å™¨ï¼š** <br> - **ç›®æ ‡ï¼š** æ±‚è§£å…³èŠ‚ç©ºé—´å¢žé‡ \\(\\Delta q_t\\)ï¼Œé©±åŠ¨æ‰€æœ‰æŒ‡å°– \\(p_k(q)\\) è·Ÿè¸ªå…¶ç›¸ä½ç›®æ ‡ç‚¹ \\(p_k^\\star(w_t)\\)ã€‚<br> - **ä¼˜åŒ–é—®é¢˜ï¼š** åœ¨æ¯ä¸ªæŽ§åˆ¶æ­¥ \\(t\\)ï¼Œæ±‚è§£ä»¥ä¸‹å¸¦çº¦æŸçš„äºŒæ¬¡è§„åˆ’(QP)ï¼š<br> \\(\\min_{\\Delta q_t} \\frac{1}{2} \\| W^{1/2} (J_t \\Delta q_t + r_t ) \\|_2^2 + \\frac{\\lambda_{\\text{damp}}}{2} \\|\\Delta q_t\\|_2^2\\) <br> \\(\\text{s.t.} \\quad q_{\\min} - q_t \\leq \\Delta q_t \\leq q_{\\max} - q_t\\) <br> \\(\\quad \\quad G_{\\text{col}} \\Delta q_t \\leq h_{\\text{col}}\\) <br> - **å˜é‡å®šä¹‰ï¼š** <br> &nbsp;&nbsp;â€¢ \\(r_t = [r_1(q_t); ...; r_K(q_t); r_{\\text{coup}}(q_t)]\\)ï¼Œå…¶ä¸­ \\(r_k(q_t) = p_k(q_t) - p_k^\\star(w_t)\\)ã€‚<br> &nbsp;&nbsp;â€¢ \\(J_t = [J",
            "timestamp": "2026-01-20T17:54:12.309765",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸Žæ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸Žæ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è¯„ä»·æŒ‡æ ‡ä¸Žæ•°æ®é›†åˆ†æž\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä½¿ç”¨äº†ä¸¤ä¸ªå±‚é¢çš„è¯„ä»·æŒ‡æ ‡ï¼š**æ‰‹æŒ‡ç‰¹å®šå¯ä¾›æ€§å®šä½** å’Œ **çµå·§æŠ“å–åŠŸèƒ½æˆåŠŸçŽ‡**ã€‚å…·ä½“ç»†èŠ‚å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| è¯„ä»·å±‚é¢ | æŒ‡æ ‡åç§° | ç¼©å†™ | è®¡ç®—æ–¹å¼ä¸Žç»†èŠ‚ | æ„ä¹‰ï¼ˆè¶Šé«˜/ä½Žè¶Šå¥½ï¼‰ |\n| :--- | :--- | :--- | :--- | :--- |\n| **æ‰‹æŒ‡ç‰¹å®šå¯ä¾›æ€§å®šä½** | åº“å°”è´å…‹-èŽ±å¸ƒå‹’æ•£åº¦ | KLD | è®¡ç®—é¢„æµ‹çš„çƒ­åŠ›å›¾ä¸ŽçœŸå®žæ ‡æ³¨çƒ­åŠ›å›¾ï¼ˆä»¥æ ‡æ³¨çš„æŒ‡å°–ä½ç½®ä¸ºä¸­å¿ƒçš„é«˜æ–¯åˆ†å¸ƒï¼‰ä¹‹é—´çš„KLæ•£åº¦ã€‚è¡¡é‡é¢„æµ‹åˆ†å¸ƒä¸ŽçœŸå®žåˆ†å¸ƒçš„å·®å¼‚ã€‚ | **è¶Šä½Žè¶Šå¥½** |\n| | ç›¸ä¼¼åº¦ | SIM | è®¡ç®—é¢„æµ‹çƒ­åŠ›å›¾ä¸ŽçœŸå®žçƒ­åŠ›å›¾çš„å½’ä¸€åŒ–ç›´æ–¹å›¾äº¤é›†ã€‚å€¼åŸŸä¸º[0,1]ï¼Œè¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒçš„ç›¸ä¼¼ç¨‹åº¦ã€‚ | **è¶Šé«˜è¶Šå¥½** |\n| | å½’ä¸€åŒ–æ‰«æè·¯å¾„æ˜¾è‘—æ€§ | NSS | å°†é¢„æµ‹çƒ­åŠ›å›¾å½’ä¸€åŒ–ä¸ºé›¶å‡å€¼ã€å•ä½æ–¹å·®ã€‚åœ¨**æ¯ä¸ªæ‰‹æŒ‡çš„çœŸå®žæ ‡æ³¨å…³é”®ç‚¹ä½ç½®**é‡‡æ ·è¯¥å½’ä¸€åŒ–åŽçš„å€¼ï¼Œå¹¶å¯¹æ‰€æœ‰æ‰‹æŒ‡å–å¹³å‡ã€‚è¡¡é‡æ¨¡åž‹åœ¨çœŸå®žæŽ¥è§¦ç‚¹ä½ç½®çš„é¢„æµ‹ç½®ä¿¡åº¦ã€‚ | **è¶Šé«˜è¶Šå¥½** |\n| **çµå·§æŠ“å–åŠŸèƒ½æˆåŠŸçŽ‡** | æŠ“å–æˆåŠŸçŽ‡ | Suc. R. | ä¸€æ¬¡æŠ“å–è¢«åˆ¤å®šä¸ºæˆåŠŸçš„æ¡ä»¶æ˜¯ï¼š1) æœºå™¨äººèƒ½å°†ç‰©ä½“æå‡è¶…è¿‡0.1ç±³ï¼›2) èƒ½ç¨³å®šæŒæ¡è¶…è¿‡3ç§’é’Ÿã€‚æˆåŠŸçŽ‡ = (æˆåŠŸæ¬¡æ•° / æ€»å°è¯•æ¬¡æ•°) * 100%ã€‚ | **è¶Šé«˜è¶Šå¥½** |\n\n**æŒ‡æ ‡ç»†èŠ‚è¡¥å……**ï¼š\n1.  **å¯ä¾›æ€§å®šä½æŒ‡æ ‡**ï¼šè¿™ä¸‰ä¸ªæŒ‡æ ‡ï¼ˆKLD, SIM, NSSï¼‰æ˜¯è®¡ç®—æœºè§†è§‰ä¸­è¯„ä¼°æ˜¾è‘—æ€§é¢„æµ‹æˆ–çƒ­åŠ›å›¾å›žå½’è´¨é‡çš„ç»å…¸æŒ‡æ ‡ã€‚è®ºæ–‡ç‰¹åˆ«å¼ºè°ƒåœ¨è®¡ç®—NSSæ—¶ï¼Œ**ä»¥æ¯ä¸ªæ‰‹æŒ‡çš„çœŸå®žæ ‡æ³¨ç‚¹ä¸ºâ€œæ³¨è§†ç‚¹â€**ï¼Œè¿™ç›´æŽ¥è¯„ä¼°äº†æ¨¡åž‹åœ¨çœŸå®žæŽ¥è§¦ä½ç½®çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œè€Œéžæ•´ä½“åˆ†å¸ƒã€‚\n2.  **æŠ“å–æˆåŠŸçŽ‡æŒ‡æ ‡**ï¼šè¿™æ˜¯ä¸€ä¸ª**ç«¯åˆ°ç«¯çš„åŠŸèƒ½æ€§æŒ‡æ ‡**ï¼Œä¸ä»…è¦æ±‚æŠ“å–åŠ¨ä½œæˆåŠŸï¼Œè¿˜è¦æ±‚æŠ“å–åŽèƒ½ç¨³å®šæŒæ¡ï¼Œåæ˜ äº†æŠ“å–çš„ç‰©ç†ç¨³å®šæ€§å’Œå®žç”¨æ€§ã€‚\n\n### äºŒã€ æ•°æ®é›†\n\nè®ºæ–‡æž„å»ºå¹¶ä½¿ç”¨äº†ä¸“é—¨çš„æ•°æ®é›†æ¥è®­ç»ƒå’Œè¯„ä¼°å…¶æ‰‹æŒ‡ç‰¹å®šå¯ä¾›æ€§æ¨¡åž‹ã€‚\n\n| æ•°æ®é›†ç”¨é€” | æ•°æ®å†…å®¹ä¸Žè§„æ¨¡ | æž„å»ºæµç¨‹ä¸Žç»†èŠ‚ |\n| :--- | :--- | :--- |\n| **è®­ç»ƒä¸ŽéªŒè¯** | **130ä¸ªæ‰‹æŒ‡ç‰¹å®šçš„æŠ“å–æ¼”ç¤º**ï¼Œè¦†ç›–**13ç§æ—¥å¸¸ç‰©ä½“**ï¼ˆå¦‚é¦™è•‰ã€èžºä¸åˆ€ã€ç“¶å­ï¼‰ã€‚æ¯ä¸ªç‰©ä½“çº¦10ä¸ªæ¼”ç¤ºã€‚ | 1. **æ•°æ®é‡‡é›†**ï¼šå½•åˆ¶äººæ‰‹æŠ“å–ç‰©ä½“çš„è§†é¢‘ã€‚<br>2. **å…³é”®å¸§æå–**ï¼šä¸ºæ¯ä¸ªæŠ“å–åºåˆ—è‡ªåŠ¨é€‰æ‹©ä¸¤å¸§ï¼š<br>   - **ç‰©ä½“å¸§**ï¼šç‰©ä½“å®Œå…¨å¯è§ä¸”æ²¡æœ‰æ‰‹éƒ¨åƒç´ çš„å¸§ï¼Œæä¾›å¹²å‡€çš„ç‰©ä½“ä¸Šä¸‹æ–‡ã€‚<br>   - **ç¨³å®šæŠ“å–å¸§**ï¼šäººæ‰‹ä¸Žç‰©ä½“å»ºç«‹æ¸…æ™°æŽ¥è§¦çš„æœ€æ—©å¸§ï¼Œæä¾›ç›‘ç£ä¿¡å·ã€‚<br>3. **æ ‡æ³¨ç”Ÿæˆ**ï¼šä½¿ç”¨RTMPoseæ‰‹éƒ¨å§¿æ€ä¼°è®¡å™¨æ£€æµ‹2Dæ‰‹éƒ¨å…³é”®ç‚¹ã€‚ä»¥æ¯ä¸ª**æŒ‡å°–**ä½ç½® `Î¼_k` ä¸ºä¸­å¿ƒï¼Œç”Ÿæˆæ ‡å‡†å·®ä¸º `Ïƒ = min(h, w) / 64` çš„äºŒç»´é«˜æ–¯çƒ­åŠ›å›¾ `H_k(u)`ã€‚å°†äº”ä¸ªæ‰‹æŒ‡çš„çƒ­åŠ›å›¾å †å ï¼Œå¾—åˆ°å½¢çŠ¶ä¸º `5 Ã— h Ã— w` çš„çœŸå®žæ ‡ç­¾å¼ é‡ `H`ã€‚ |\n| **æµ‹è¯•** | **7ä¸ªæœªè§è¿‡çš„ç‰©ä½“**ï¼ŒåŒ…æ‹¬ï¼š<br>1. å·²è§ç±»åˆ«çš„æ–°å®žä¾‹ã€‚<br>2. å…¨æ–°ç±»åˆ«çš„ç‰©ä½“ï¼ˆå¦‚æ‰³æ‰‹ã€é”¤å­ï¼‰ã€‚ | æµ‹è¯•é›†ç”¨äºŽè¯„ä¼°æ¨¡åž‹çš„**æ³›åŒ–èƒ½åŠ›**ï¼ŒåŒ…æ‹¬è·¨å®žä¾‹æ³›åŒ–å’Œè·¨ç±»åˆ«æ³›åŒ–ã€‚ |\n\n**æ•°æ®é›†æ ¸å¿ƒç‰¹ç‚¹**ï¼š\n1.  **å°è§„æ¨¡ä¸Žé«˜æ•ˆçŽ‡**ï¼šä»…éœ€çº¦130ä¸ªäººæ‰‹æ¼”ç¤ºè§†é¢‘ï¼Œè¿œå°‘äºŽéœ€è¦å¤§è§„æ¨¡æœºå™¨äººæŠ“å–æ•°æ®æˆ–æ¨¡æ‹Ÿæ•°æ®çš„æ–¹æ³•ï¼Œä½“çŽ°äº†å…¶â€œæ•°æ®é«˜æ•ˆâ€çš„æ ¸å¿ƒä¼˜åŠ¿ã€‚\n2.  **ç²¾ç»†åŒ–çš„ç›‘ç£ä¿¡å·**ï¼šæ ‡ç­¾ä¸æ˜¯ç²—ç³™çš„æŠ“å–åŒºåŸŸæˆ–è¾¹ç•Œæ¡†ï¼Œè€Œæ˜¯**ä¸ºæ¯ä¸ªæ‰‹æŒ‡ç‹¬ç«‹ç”Ÿæˆçš„ã€åŸºäºŽé«˜æ–¯åˆ†å¸ƒçš„2Dçƒ­åŠ›å›¾**ï¼Œæä¾›äº†åƒç´ çº§çš„ã€æ‰‹æŒ‡ç‰¹å¼‚æ€§çš„æŽ¥è§¦ä½ç½®ç›‘ç£ã€‚\n3.  **è·¨æ¨¡æ€å¯¹é½**ï¼šé€šè¿‡é…å¯¹â€œç‰©ä½“å¸§â€å’Œâ€œæŠ“å–å¸§â€ï¼Œæ¨¡åž‹å­¦ä¹ ä»Žé™æ€ç‰©ä½“å¤–è§‚åˆ°åŠ¨æ€æŠ“å–æŽ¥è§¦ç‚¹çš„æ˜ å°„ï¼Œä¿ƒè¿›äº†è¯­ä¹‰ç†è§£ã€‚\n\n### æ€»ç»“\nè®ºæ–‡çš„è¯„ä»·ä½“ç³»è®¾è®¡ç²¾è‰¯ï¼š**å¯ä¾›æ€§å®šä½æŒ‡æ ‡**ï¼ˆKLD, SIM, NSSï¼‰ä»Žæ„ŸçŸ¥å±‚é¢å®šé‡è¯„ä¼°æ¨¡åž‹é¢„æµ‹æŒ‡å°–æŽ¥è§¦ä½ç½®çš„å‡†ç¡®æ€§ï¼›**æŠ“å–æˆåŠŸçŽ‡**ï¼ˆSuc. R.ï¼‰åˆ™ä»Žæ‰§è¡Œå±‚é¢éªŒè¯æ•´ä¸ªç³»ç»Ÿï¼ˆæ„ŸçŸ¥+è§„åˆ’ï¼‰çš„æœ€ç»ˆæ•ˆæžœã€‚è‡ªå»ºçš„æ•°æ®é›†è™½å°ï¼Œä½†æ ‡æ³¨ç²¾ç»†ã€é’ˆå¯¹æ€§å¼ºï¼Œæ˜¯æˆåŠŸè®­ç»ƒæ‰‹æŒ‡ç‰¹å®šå¯ä¾›æ€§æ¨¡åž‹çš„å…³é”®ã€‚è¿™ç§â€œå°æ•°æ®+å¤§æ¨¡åž‹å…ˆéªŒ+ç²¾ç»†åŒ–æ ‡æ³¨â€çš„ç­–ç•¥ï¼Œæ˜¯å…¶å®žçŽ°é«˜æ€§èƒ½å’Œå¼ºæ³›åŒ–èƒ½åŠ›çš„åŸºç¡€ã€‚",
            "timestamp": "2026-01-20T17:54:48.755492",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®žéªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## å®žéªŒæ•°æ®é›†ä¸Žè¯„ä¼°æŒ‡æ ‡åˆ†æž\n\n### ä¸€ã€å®žéªŒæ•°æ®é›†\n\nè®ºæ–‡ä½¿ç”¨äº†**ä¸¤ä¸ªå±‚æ¬¡**çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼š\n\n1.  **è®­ç»ƒæ•°æ®é›†**ï¼š\n    - **æ¥æº**ï¼šä½œè€…è‡ªè¡Œé‡‡é›†çš„**äººç±»æŠ“æ¡æ¼”ç¤ºè§†é¢‘**ã€‚\n    - **è§„æ¨¡**ï¼šåŒ…å« **130ä¸ª** æ‰‹æŒ‡çº§åˆ«çš„æŠ“æ¡æ¼”ç¤ºã€‚\n    - **å†…å®¹**ï¼šè¦†ç›– **13ç§** æ—¥å¸¸ç‰©ä½“ï¼ˆå¦‚é¦™è•‰ã€èžºä¸åˆ€ã€ç“¶å­ï¼‰ï¼Œæ¯ç§ç‰©ä½“çº¦10ä¸ªæ¼”ç¤ºæ ·æœ¬ã€‚\n    - **æ ‡æ³¨**ï¼šä»Žè§†é¢‘ä¸­æå–äº†**æŒ‡å°–æŽ¥è§¦ä½ç½®çš„2Dé«˜æ–¯çƒ­å›¾**ä½œä¸ºç›‘ç£ä¿¡å·ã€‚\n\n2.  **è¯„ä¼°æ•°æ®é›†**ï¼š\n    - **æž„æˆ**ï¼šåŒ…å« **7ç§** æœªè§è¿‡çš„ç‰©ä½“ï¼Œåˆ†ä¸ºä¸¤ç±»ï¼š\n        - **ç±»åˆ«å†…æ–°å®žä¾‹**ï¼šå·²è§è¿‡ç±»åˆ«çš„æ–°ç‰©ä½“ã€‚\n        - **å…¨æ–°ç±»åˆ«**ï¼šå®Œå…¨æœªåœ¨è®­ç»ƒä¸­å‡ºçŽ°çš„ç‰©ä½“ï¼ˆå¦‚æ‰³æ‰‹ã€é”¤å­ï¼‰ã€‚\n    - **ç›®çš„**ï¼šç”¨äºŽæµ‹è¯•æ¨¡åž‹çš„**è·¨å®žä¾‹**å’Œ**è·¨ç±»åˆ«**æ³›åŒ–èƒ½åŠ›ã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\n\nè¯„ä¼°åˆ†ä¸ºä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼Œå¯¹åº”ä¸åŒçš„æŒ‡æ ‡ï¼š\n\n#### 1. æ‰‹æŒ‡ç‰¹å®šå¯ä¾›æ€§å®šä½ï¼ˆFinger-Specific Affordance Groundingï¼‰\n- **ç›®çš„**ï¼šè¯„ä¼°æ¨¡åž‹é¢„æµ‹çš„æŒ‡å°–æŽ¥è§¦çƒ­å›¾ä¸ŽçœŸå®žæ ‡æ³¨çš„åŒ¹é…ç²¾åº¦ã€‚\n- **æŒ‡æ ‡**ï¼š\n    - **KLDï¼ˆKullback-Leibler Divergenceï¼‰**ï¼šè¡¡é‡é¢„æµ‹çƒ­å›¾ä¸ŽçœŸå®žçƒ­å›¾åˆ†å¸ƒçš„å·®å¼‚ã€‚**å€¼è¶Šä½Žè¶Šå¥½**ã€‚\n    - **SIMï¼ˆSimilarityï¼‰**ï¼šè®¡ç®—é¢„æµ‹ä¸ŽçœŸå®žçƒ­å›¾çš„å½’ä¸€åŒ–ç›´æ–¹å›¾äº¤é›†ã€‚**å€¼è¶Šé«˜è¶Šå¥½**ã€‚\n    - **NSSï¼ˆNormalized Scanpath Saliencyï¼‰**ï¼šåœ¨æ ‡æ³¨çš„æŒ‡å°–å…³é”®ç‚¹ä½ç½®é‡‡æ ·ï¼Œè¯„ä¼°é¢„æµ‹å€¼çš„æ˜¾è‘—æ€§ã€‚**å€¼è¶Šé«˜è¶Šå¥½**ã€‚\n\n#### 2. çµå·§æ‰‹åŠŸèƒ½æ€§æŠ“æ¡ï¼ˆFunctional Dexterous Graspingï¼‰\n- **ç›®çš„**ï¼šè¯„ä¼°åœ¨çœŸå®žæœºå™¨äººå¹³å°ä¸Šæ‰§è¡ŒæŠ“æ¡çš„æˆåŠŸçŽ‡ã€‚\n- **æŒ‡æ ‡**ï¼š\n    - **æŠ“æ¡æˆåŠŸçŽ‡ï¼ˆGrasping Success Rate, Suc. R.ï¼‰**ï¼šæˆåŠŸå®šä¹‰ä¸ºå°†ç‰©ä½“æå‡è¶…è¿‡0.1ç±³å¹¶ç¨³å®šä¿æŒ3ç§’ä»¥ä¸Šã€‚æŠ¥å‘Šç™¾åˆ†æ¯”ã€‚\n\n### ä¸‰ã€æ€§èƒ½æå‡ä¸Žç»“æžœ\n\n#### 1. å¯ä¾›æ€§å®šä½æ€§èƒ½ï¼ˆå¯¹æ¯”å®žéªŒï¼‰\nåœ¨è¡¨Iä¸­ï¼Œè®ºæ–‡æå‡ºçš„åŸºäºŽStable Diffusionï¼ˆSDï¼‰çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºŽåŸºçº¿ï¼š\n\n| æ¨¡åž‹ / å˜ä½“ | KLD (â†“) | SIM (â†‘) | NSS (â†‘) | å…³é”®ä¼˜åŠ¿ |\n| :--- | :--- | :--- | :--- | :--- |\n| **ReKep** | 10.127 | 0.173 | 1.801 | åŸºäºŽæ— ç›‘ç£å…³é”®ç‚¹ï¼Œç¼ºä¹è¯­ä¹‰é€‰æ‹©æ€§ |\n| **CMKA** | 11.184 | 0.177 | 1.861 | åˆ†å‰²é©±åŠ¨ï¼Œå¯¹å¼±çº¹ç†ç‰©ä½“å¤±æ•ˆ |\n| **Ours (CLIP)** | 6.690 | 0.355 | 3.815 | å¼•å…¥è¯­è¨€å…ˆéªŒï¼Œæ€§èƒ½æå‡ |\n| **Ours (DINO)** | 3.301 | 0.473 | 5.016 | è§†è§‰ç‰¹å¾ä¼˜ç§€ï¼Œä½†å­˜åœ¨æ‰‹æŒ‡åˆå¹¶çŽ°è±¡ |\n| **Ours (SD)** | **2.491** | **0.551** | **5.518** | **æœ€ä½³æ€§èƒ½ï¼Œå®žçŽ°ç²¾ç»†ã€è§£è€¦çš„æŒ‡å°–å®šä½** |\n\n- **æ€§èƒ½æå‡**ï¼šç›¸æ¯”æœ€ä½³åˆ¤åˆ«å¼éª¨å¹²ç½‘ç»œï¼ˆDINOï¼‰ï¼ŒåŸºäºŽæ‰©æ•£æ¨¡åž‹çš„æ–¹æ³•åœ¨KLDä¸Šæå‡äº† **24.5%**ï¼Œåœ¨SIMä¸Šæå‡äº† **16.5%**ï¼Œåœ¨NSSä¸Šæå‡äº† **10%**ã€‚è¿™è¯æ˜Žäº†**é¢„è®­ç»ƒç”Ÿæˆæ¨¡åž‹åœ¨æå–ç»†ç²’åº¦ã€è¯­ä¹‰åŒ–æŽ¥è§¦å…ˆéªŒæ–¹é¢çš„æ ¸å¿ƒä¼˜åŠ¿**ã€‚\n\n#### 2. åŠŸèƒ½æ€§æŠ“æ¡æ€§èƒ½ï¼ˆå¯¹æ¯”å®žéªŒï¼‰\nåœ¨è¡¨IIä¸­ï¼Œè®ºæ–‡æ–¹æ³•åœ¨çœŸå®žæœºå™¨äººæŠ“æ¡ä»»åŠ¡ä¸Šå±•çŽ°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›å’ŒæˆåŠŸçŽ‡ï¼š\n\n| æ¨¡åž‹ | ç“¶å­ (S/U) | é¦™è•‰ | ç•ªèŒ„ | èžºä¸åˆ€ | ç”µé’» | ç»¼åˆè¡¨çŽ° |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **ACT-3D** | 30%/25% | 30% | 50% | 30% | 40% | æ¨¡ä»¿æ¼”ç¤ºï¼Œæ³›åŒ–å·® |\n| **Diffusion Policy 3D** | 40%/40% | 50% | 45% | 0% | 30% | å¯¹æœªè§å§¿æ€é€‚åº”å·® |\n| **CMKA** | 30%/20% | 20% | 0% | 0% | 60% | åˆ†å‰²å’Œå›ºå®šç­–ç•¥å¯¼è‡´å¤±è´¥ |\n| **Ours** | **100%/85%** | **85%** | **90%** | **60%** | **70%** | **ç¨³å®šã€åŠŸèƒ½æ­£ç¡®ã€æ³›åŒ–å¼º** |\n\n- **æ€§èƒ½æå‡**ï¼šåœ¨å¸¸è§ç‰©ä½“ï¼ˆå¦‚ç“¶å­ã€é¦™è•‰ã€ç•ªèŒ„ï¼‰ä¸Šï¼ŒæˆåŠŸçŽ‡æ™®éè¾¾åˆ° **85%-100%**ï¼Œè¿œé«˜äºŽåŸºçº¿æ¨¡åž‹ã€‚ç‰¹åˆ«æ˜¯åœ¨**ä½ŽçŸ®ç‰©ä½“ï¼ˆå¦‚é¦™è•‰ã€èžºä¸åˆ€ï¼‰** ä¸Šï¼ŒåŸºäºŽå‡ ä½•å¯¹é½çš„è½¨è¿¹è§„åˆ’æœ‰æ•ˆé¿å…äº†æ‰‹æŒ‡ä¸Žæ¡Œé¢çš„ç¢°æ’žï¼Œè¿™æ˜¯åŸºçº¿æ–¹æ³•çš„ä¸»è¦å¤±è´¥åŽŸå› ã€‚\n\n#### 3. è·¨å…·èº«æ³›åŒ–æ€§èƒ½\nåœ¨è¡¨IIIä¸­ï¼Œè®ºæ–‡æ–¹æ³•å±•ç¤ºäº†å¼ºå¤§çš„**é›¶æ ·æœ¬è·¨çµå·§æ‰‹è¿ç§»èƒ½åŠ›**ï¼š\n- å°†ç­–ç•¥ä»Ž **DexHand021** è¿ç§»åˆ° **Linker Hand L20**ï¼Œ**æ— éœ€é‡æ–°è®­ç»ƒ**ã€‚\n- æŠ“æ¡æˆåŠŸçŽ‡ä¿æŒç»Ÿè®¡ä¸Šä¸å˜ï¼ˆç”šè‡³éƒ¨åˆ†ä»»åŠ¡æœ‰æå‡ï¼‰ï¼Œä¾‹å¦‚ç•ªèŒ„æŠ“æ¡æˆåŠŸçŽ‡ä»Ž90%æå‡åˆ°100%ã€‚\n- **æ ¸å¿ƒä»·å€¼**ï¼šè¿™è¯æ˜Žäº†**æ‰‹æŒ‡ç‰¹å®šå¯ä¾›æ€§åœºï¼ˆFSAFï¼‰æˆåŠŸè§£è€¦äº†æŠ“æ¡è¯­ä¹‰ä¸Žå…·ä½“æ‰‹éƒ¨å½¢æ€å­¦**ï¼Œä¸ºå®žçŽ°ç¡¬ä»¶æ— å…³çš„çµå·§æ“ä½œæä¾›äº†å¯è¡Œè·¯å¾„ã€‚\n\n### æ€»ç»“\nè®ºæ–‡é€šè¿‡**å°è§„æ¨¡äººç±»æ¼”ç¤ºæ•°æ®**ä¸Ž**é¢„è®­ç»ƒæ‰©æ•£æ¨¡åž‹çš„è¯­ä¹‰å…ˆéªŒ**ç›¸ç»“åˆï¼Œåœ¨**æ‰‹æŒ‡çº§å¯ä¾›æ€§å®šä½ç²¾åº¦**å’Œ**çœŸå®žä¸–ç•Œçµå·§æŠ“æ¡æˆåŠŸçŽ‡**ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ä¸Šå‡å®žçŽ°äº†**æ˜¾è‘—æ€§èƒ½æå‡**ã€‚å…¶æœ€å¤§çš„å®žé™…ä»·å€¼åœ¨äºŽï¼š**ä»¥æ•°æ®é«˜æ•ˆçš„æ–¹å¼ï¼Œå®žçŽ°äº†å¯¹æœªè§ç‰©ä½“ã€å§¿æ€å˜åŒ–ä»¥åŠä¸åŒçµå·§æ‰‹ç¡¬ä»¶çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›**ï¼Œä¸º scalable çš„çµå·§æ“ä½œç³»ç»Ÿæä¾›äº†æ–°èŒƒå¼ã€‚",
            "timestamp": "2026-01-20T17:55:32.532418",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æž\n\nåŸºäºŽå¯¹è®ºæ–‡å†…å®¹çš„åˆ†æžï¼ŒFSAGæ–¹æ³•è™½ç„¶å–å¾—äº†æ˜¾è‘—æˆæžœï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n- **æ‰§è¡ŒçŽ¯èŠ‚çš„é—­çŽ¯æŽ§åˆ¶ç¼ºå¤±**ï¼šè®ºæ–‡æ˜Žç¡®æŒ‡å‡ºï¼Œåœ¨æŠ“å–**ç»†é•¿ç‰©ä½“**ï¼ˆå¦‚èžºä¸åˆ€ã€ç¬”ï¼‰æ—¶ï¼Œå³ä½¿æŽ¥è§¦ç‚¹é¢„æµ‹å‡†ç¡®ï¼Œ**ç®€å•çš„å¼€çŽ¯ã€å›ºå®šåç§»é‡çš„é—­åˆåŠ¨ä½œ**ä¹Ÿå¯èƒ½å¯¼è‡´ç‰©ä½“æ—‹è½¬æˆ–æ»‘è½ã€‚è¿™æš´éœ²å‡ºçŽ°æœ‰è¿åŠ¨è§„åˆ’æ¨¡å—åœ¨**æŽ¥è§¦åŠ›æŽ§åˆ¶ã€æ»‘åŠ¨æ£€æµ‹å’Œåœ¨çº¿é€‚åº”æ€§è°ƒæ•´**æ–¹é¢çš„ä¸è¶³ã€‚\n\n- **å¯¹æ„ŸçŸ¥æ¨¡å—çš„ä¾èµ–ä¸Žæ½œåœ¨è¯¯å·®**ï¼šæ•´ä¸ªæµç¨‹ä¸¥é‡ä¾èµ–å‰ç«¯æ„ŸçŸ¥çš„å‡†ç¡®æ€§ï¼š\n    - **æ·±åº¦ä¼°è®¡**ï¼šä¾èµ–FoundationStereoè¿›è¡Œæ·±åº¦æ¢å¤ï¼Œåœ¨é€æ˜Žã€åå…‰æˆ–çº¹ç†ç¼ºå¤±çš„ç‰©ä½“ä¸Šå¯èƒ½å¤±æ•ˆã€‚\n    - **å®žä¾‹åˆ†å‰²**ï¼šä¾èµ–GroundingDINO-SAM2èŽ·å–ç‰©ä½“æŽ©ç ï¼Œåœ¨ç‰©ä½“å †å æˆ–ä¸¥é‡é®æŒ¡åœºæ™¯ä¸‹åˆ†å‰²å¯èƒ½ä¸å‡†ç¡®ã€‚\n    - **è¿™äº›æ„ŸçŸ¥è¯¯å·®ä¼šç›´æŽ¥ä¼ å¯¼è‡³åŽç»­çš„æŽ¥è§¦ç‚¹æ˜ å°„å’Œè¿åŠ¨è§„åˆ’**ï¼Œå¯¼è‡´å¤±è´¥ã€‚\n\n- **æ•°æ®æ ‡æ³¨ä¸Žæ³›åŒ–èŒƒå›´çš„é™åˆ¶**ï¼š\n    - æ–¹æ³•éœ€è¦**å°‘é‡å¸¦æŒ‡å°–æ ‡æ³¨çš„äººç±»æ¼”ç¤ºè§†é¢‘**è¿›è¡Œç›‘ç£è®­ç»ƒã€‚è™½ç„¶æ•°æ®é‡è¿œå°äºŽä»¿çœŸæˆ–å¤§è§„æ¨¡çœŸæœºé‡‡é›†ï¼Œä½†**æ ‡æ³¨è¿‡ç¨‹ä»éœ€æˆæœ¬**ã€‚\n    - å®žéªŒéªŒè¯çš„å¯¹è±¡ç±»åˆ«ï¼ˆ13ç§ï¼‰å’Œåœºæ™¯ï¼ˆæ¡Œé¢å•ä¸€ç‰©ä½“æŠ“å–ï¼‰ç›¸å¯¹æœ‰é™ã€‚åœ¨**æ›´å¤æ‚ã€éžç»“æž„åŒ–çš„çœŸå®žçŽ¯å¢ƒ**ï¼ˆå¦‚æ‚ä¹±æ¡Œé¢ã€åŠ¨æ€åœºæ™¯ï¼‰ä¸­çš„æ³›åŒ–èƒ½åŠ›æœ‰å¾…éªŒè¯ã€‚\n\n- **æ–¹æ³•å‡è®¾ä¸Žåœºæ™¯çº¦æŸ**ï¼š\n    - å‡è®¾ç‰©ä½“åœ¨**â€œæ— æ‰‹â€å…³é”®å¸§**ä¸­å®Œå…¨å¯è§ï¼Œè¿™åœ¨åŠ¨æ€æˆ–æŒç»­äº¤äº’åœºæ™¯ä¸­å¯èƒ½ä¸æˆç«‹ã€‚\n    - å½“å‰çš„**è¿åŠ¨è§„åˆ’åŸºäºŽä¼˜åŒ–æ±‚è§£**ï¼Œè®¡ç®—æ•ˆçŽ‡åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ï¼ˆå¦‚å¸¦éšœç¢ç‰©çš„çµå·§æ“ä½œï¼‰ä¸­å¯èƒ½æˆä¸ºç“¶é¢ˆã€‚\n    - **æœªå……åˆ†åˆ©ç”¨å¤šè§†è§’ä¿¡æ¯**ï¼Œä»…ä¾èµ–å•è§†è§’çš„RGB-Dè¾“å…¥ï¼Œå¯¹è‡ªé®æŒ¡ä¸¥é‡çš„ç‰©ä½“å¯èƒ½æ— æ³•é‡å»ºå®Œæ•´å‡ ä½•ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡åœ¨ç»“è®ºéƒ¨åˆ†åŠåˆ†æžä¸­æš—ç¤ºäº†ä»¥ä¸‹æœªæ¥å·¥ä½œæ–¹å‘ï¼š\n\n- **å¢žå¼ºé—­çŽ¯ä¸Žè§¦è§‰åé¦ˆ**ï¼š\n    ```python\n    # æœªæ¥æ–¹å‘ç¤ºä¾‹ï¼šç»“åˆåŠ›/è§¦è§‰çš„é—­çŽ¯æŽ§åˆ¶\n    æœªæ¥ç³»ç»Ÿ = FSAG(æŽ¥è§¦ç‚¹é¢„æµ‹) + å­¦ä¹ åž‹é˜»æŠ—æŽ§åˆ¶ + å…­ç»´åŠ›/è§¦è§‰ä¼ æ„Ÿå™¨åé¦ˆ\n    ```\n    - é›†æˆ**å…­ç»´åŠ›/åŠ›çŸ©ä¼ æ„Ÿå™¨**æˆ–**è§¦è§‰ä¼ æ„Ÿå™¨**ï¼Œå®žçŽ°åŸºäºŽæŽ¥è§¦åŠ›ä¼°è®¡å’Œæ»‘åŠ¨æ£€æµ‹çš„**åœ¨çº¿è°ƒæ•´ç­–ç•¥**ã€‚\n    - å¼€å‘**å­¦ä¹ åž‹æ®‹å·®é˜»æŠ—æŽ§åˆ¶**æˆ–æ“ä½œç©ºé—´æŽ§åˆ¶ï¼Œä½¿æœºå™¨äººåœ¨æŽ¥è§¦åŽèƒ½åŠ¨æ€é€‚åº”ç‰©ä½“è¿åŠ¨å’Œå—åŠ›å˜åŒ–ã€‚\n\n- **æå‡æ„ŸçŸ¥é²æ£’æ€§ä¸Žå¤šæ¨¡æ€èžåˆ**ï¼š\n    - æŽ¢ç´¢æ›´é²æ£’çš„**å¤šè§†è§’èžåˆ**æˆ–**ä¸»åŠ¨æ„ŸçŸ¥**ç­–ç•¥ï¼Œä»¥åº”å¯¹å•è§†è§’é®æŒ¡é—®é¢˜ã€‚\n    - ç ”ç©¶å¦‚ä½•å°†æ‰©æ•£æ¨¡åž‹æä¾›çš„è¯­ä¹‰å…ˆéªŒä¸Ž**3Då‡ ä½•å­¦ä¹ ï¼ˆå¦‚3Dæ‰©æ•£æ¨¡åž‹ï¼‰** æ›´æ·±åº¦åœ°ç»“åˆï¼Œå‡å°‘å¯¹ç‹¬ç«‹æ·±åº¦ä¼°è®¡å’Œåˆ†å‰²æ¨¡å—çš„ä¾èµ–ã€‚\n\n- **æ‰©å±•ä»»åŠ¡èŒƒå›´ä¸Žæ•°æ®æ•ˆçŽ‡**ï¼š\n    - å°†æ¡†æž¶æ‰©å±•åˆ°**æ›´å¤æ‚çš„çµå·§æ“ä½œä»»åŠ¡**ï¼Œå¦‚ä½¿ç”¨å·¥å…·ã€é‡æ–°æ‘†æ”¾ã€åŒæ‰‹åè°ƒç­‰ã€‚\n    - æŽ¢ç´¢**æ›´é«˜æ•ˆæˆ–æ— ç›‘ç£çš„æ ‡æ³¨æ–¹å¼**ï¼Œä¾‹å¦‚åˆ©ç”¨å¤§è§„æ¨¡äººç±»è§†é¢‘ï¼ˆæ— ç²¾ç¡®æŒ‡å°–æ ‡æ³¨ï¼‰è¿›è¡Œå¼±ç›‘ç£æˆ–è‡ªç›‘ç£å­¦ä¹ ï¼Œè¿›ä¸€æ­¥é™ä½Žæ•°æ®ä¾èµ–ã€‚\n\n- **ç®—æ³•ä¸Žç³»ç»Ÿçš„ä¼˜åŒ–**ï¼š\n    - ä¼˜åŒ–è¿åŠ¨è§„åˆ’å™¨ï¼Œç»“åˆ**å­¦ä¹ åž‹é‡‡æ ·**æˆ–**åˆ†å±‚è§„åˆ’**ï¼Œä»¥å¤„ç†æ›´å¤æ‚çš„éšœç¢ç‰©çŽ¯å¢ƒå’ŒåŠ¨æ€åœºæ™¯ã€‚\n    - ç ”ç©¶å¦‚ä½•å°†**FSAFè¡¨ç¤º**ä¸Ž**å¼ºåŒ–å­¦ä¹ **ç»“åˆï¼Œåœ¨ä¿æŒè·¨æœ¬ä½“æ³›åŒ–ä¼˜åŠ¿çš„åŒæ—¶ï¼Œå®žçŽ°æ›´åŠ¨æ€å’Œé€‚åº”æ€§çš„ç­–ç•¥å­¦ä¹ ã€‚\n\n**æ€»ç»“**ï¼šFSAGçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºŽåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡åž‹çš„è¯­ä¹‰å…ˆéªŒï¼Œå®žçŽ°äº†æ•°æ®é«˜æ•ˆå’Œè·¨æœ¬ä½“çš„çµå·§æŠ“å–ã€‚å…¶å½“å‰å±€é™ä¸»è¦é›†ä¸­äºŽ**æ‰§è¡Œå±‚çš„å¼€çŽ¯æŽ§åˆ¶ã€æ„ŸçŸ¥é“¾çš„è¯¯å·®ç´¯ç§¯ä»¥åŠåœºæ™¯å¤æ‚æ€§**ã€‚æœªæ¥çš„æ”¹è¿›å°†å›´ç»•**å¼•å…¥è§¦è§‰é—­çŽ¯ã€å¢žå¼ºæ„ŸçŸ¥é²æ£’æ€§ã€æ‹“å±•ä»»åŠ¡è¾¹ç•Œ**å±•å¼€ï¼Œæ—¨åœ¨æž„å»ºæ›´é€šç”¨ã€æ›´é²æ£’çš„çµå·§æ“ä½œç³»ç»Ÿã€‚",
            "timestamp": "2026-01-20T17:56:03.722012",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-13T06:08:56Z",
    "created_at": "2026-01-20T17:49:41.282281",
    "updated_at": "2026-01-20T17:49:41.282293"
}