{
    "id": "2601.07823v1",
    "title": "Video Generation Models in Robotics -- Applications, Research Challenges, Future Directions",
    "authors": [
        "Zhiting Mei",
        "Tenny Yin",
        "Ola Shorinwa",
        "Apurva Badithela",
        "Zhonghe Zheng",
        "Joseph Bruno",
        "Madison Bland",
        "Lihan Zha",
        "Asher Hancock",
        "Jaime FernÃ¡ndez Fisac",
        "Philip Dames",
        "Anirudha Majumdar"
    ],
    "abstract": "è§†é¢‘ç”Ÿæˆæ¨¡å‹å·²æˆä¸ºç‰©ç†ä¸–ç•Œçš„é«˜ä¿çœŸæ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®å¤šæ¨¡æ€ç”¨æˆ·è¾“å…¥åˆæˆæ•æ‰æ™ºèƒ½ä½“ä¸ç¯å¢ƒé—´ç»†ç²’åº¦äº¤äº’çš„é«˜è´¨é‡è§†é¢‘ã€‚å…¶å“è¶Šèƒ½åŠ›è§£å†³äº†åŸºäºç‰©ç†çš„ä»¿çœŸå™¨é•¿æœŸé¢ä¸´çš„è¯¸å¤šæŒ‘æˆ˜ï¼Œæ¨åŠ¨äº†è®¸å¤šé—®é¢˜é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œä¾‹å¦‚æœºå™¨äººå­¦ã€‚ä¾‹å¦‚ï¼Œè§†é¢‘æ¨¡å‹èƒ½å¤Ÿå®ç°æ— éœ€è¿›è¡Œä¸¥è‹›ç®€åŒ–å‡è®¾çš„ç…§ç‰‡çº§çœŸå®æ„Ÿã€ç‰©ç†ä¸€è‡´çš„å¯å˜å½¢ä½“ä»¿çœŸï¼Œè€Œè¿™æ­£æ˜¯åŸºäºç‰©ç†ä»¿çœŸçš„ä¸»è¦ç“¶é¢ˆã€‚æ­¤å¤–ï¼Œè§†é¢‘æ¨¡å‹å¯ä½œä¸ºåŸºç¡€ä¸–ç•Œæ¨¡å‹ï¼Œä»¥ç»†ç²’åº¦ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„æ–¹å¼æ•æ‰ä¸–ç•ŒåŠ¨æ€ï¼Œä»è€Œå…‹æœäº†çº¯è¯­è¨€æŠ½è±¡åœ¨æè¿°å¤æ‚ç‰©ç†äº¤äº’æ—¶è¡¨ç°åŠ›æœ‰é™çš„ç¼ºé™·ã€‚æœ¬ç»¼è¿°ç³»ç»Ÿæ¢³ç†äº†è§†é¢‘æ¨¡å‹åŠå…¶ä½œä¸ºå…·èº«ä¸–ç•Œæ¨¡å‹åœ¨æœºå™¨äººå­¦ä¸­çš„åº”ç”¨ï¼Œæ¶µç›–æ¨¡ä»¿å­¦ä¹ ä¸­ä½æˆæœ¬æ•°æ®ç”Ÿæˆä¸åŠ¨ä½œé¢„æµ‹ã€å¼ºåŒ–å­¦ä¹ ä¸­çš„åŠ¨æ€ä¸å¥–åŠ±å»ºæ¨¡ã€è§†è§‰è§„åˆ’åŠç­–ç•¥è¯„ä¼°ç­‰æ–¹é¢ã€‚è¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬é‡ç‚¹æ¢è®¨äº†é˜»ç¢è§†é¢‘æ¨¡å‹åœ¨æœºå™¨äººé¢†åŸŸå¯ä¿¡æ•´åˆçš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸è¶³ã€è¿åç‰©ç†è§„å¾‹çš„å¹»è§‰ç°è±¡ã€ä¸å®‰å…¨å†…å®¹ç”Ÿæˆç­‰é—®é¢˜ï¼Œä»¥åŠæ•°æ®æ ‡æ³¨ã€è®­ç»ƒä¸æ¨ç†æˆæœ¬é«˜æ˜‚ç­‰æ ¹æœ¬æ€§å±€é™ã€‚æœ¬æ–‡æå‡ºäº†åº”å¯¹è¿™äº›å¼€æ”¾æ€§ç ”ç©¶æŒ‘æˆ˜çš„æ½œåœ¨æœªæ¥æ–¹å‘ï¼Œæ—¨åœ¨æ¿€åŠ±ç›¸å…³ç ”ç©¶å¹¶æœ€ç»ˆä¿ƒè¿›æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­ã€‚",
    "url": "https://arxiv.org/abs/2601.07823v1",
    "html_url": "https://arxiv.org/html/2601.07823v1",
    "html_content": "1]Princeton University\n2]Temple University\n\\contribution\n[*]Equal contribution.\nVideo Generation Models in Robotics:\nApplications, Research Challenges, Future Directions\nZhiting Mei\nTenny Yin\nOla Shorinwa\nApurva Badithela\nZhonghe Zheng\nJoseph Bruno\nMadison Bland\nLihan Zha\nAsher Hancock\nJaime FernÃ¡ndez Fisac\nPhilip Dames\nAnirudha Majumdar\n[\n[\nAbstract\nVideo generation models have emerged as high-fidelity models of the physical world, capable of synthesizing high-quality videos capturing fine-grained interactions between agents and their environments conditioned on multi-modal user inputs. Their impressive capabilities address many of the long-standing challenges faced by physics-based simulators, driving broad adoption in many problem domains, e.g., robotics.\nFor example, video models enable photorealistic, physically consistent deformable-body simulation without making prohibitive simplifying assumptions, which is a major bottleneck in physics-based simulation.\nMoreover, video models can serve as foundation world models that capture the dynamics of the world in a fine-grained and expressive way. They thus overcome the limited expressiveness of language-only abstractions in describing intricate physical interactions.\nIn this survey, we provide a review of video models and their applications as embodied world models in robotics, encompassing cost-effective data generation and action prediction in imitation learning, dynamics and rewards modeling in reinforcement learning, visual planning, and policy evaluation. Further, we highlight important challenges hindering the trustworthy integration of video models in robotics, which include poor instruction following, hallucinations such as violations of physics, and unsafe content generation, in addition to fundamental limitations such as significant data curation, training, and inference costs. We present potential future directions to address these open research challenges to motivate research and ultimately facilitate broader applications, especially in safety-critical settings.\nkeywords:\nVideo Generation Models, World Modeling, Visual Planning, Policy Learning and Evaluation\nFigure 1:\nOverview.\nAs embodied world models, video models generate high-fidelity predictions of the spatiotemporal evolution of real-world environments, capturing fine-grained robot-environment interactions that have been traditionally challenging for classical physics-based simulators. Their remarkable capabilities enable generalist robot policy learning, policy evaluation, and visual planning that is well-aligned with commonsense knowledge.\n1\nIntroduction\nRecent breakthroughs in generative modeling, such as diffusion and flow-matching\n[\n1\n,\n2\n]\n, have enabled high-fidelity controllable video synthesis conditioned on user inputs such as text prompts, robot actions, and video frames. By training on internet-scale data, state-of-the-art (SOTA) video models learn to create rich videos that capture aesthetic and dynamic (motion) effects (e.g., with cinematic lighting, camera motion, and physical interaction between agents), driving widespread applications in video editing and content creation\n[\n3\n,\n4\n,\n5\n,\n6\n,\n7\n,\n8\n,\n9\n,\n10\n,\n11\n]\n.\nAs a result, these models have been increasingly integrated into robotics, e.g., in robot data generation, visual planning, policy learning, and policy evaluation\n[\n12\n,\n13\n,\n14\n,\n15\n,\n16\n,\n17\n]\n, drawing upon their impressive zero-shot generalization capabilities\n[\n18\n]\n.\nIn this survey, we provide a review of video models, highlighting their capabilities, applications in robotics, and limitations.\nThe advent of large vision-language models (VLMs/LLMs)\n[\n19\n,\n20\n,\n21\n]\nhas significantly transformed the state of the art in many problem domains (e.g., natural language processing, computer vision, robotics, etc.) due to their remarkable language generation and commonsense reasoning capabilities. Through pretraining on internet-scale data, LLMs learn broad foundational knowledge required to solve a wide range of tasks, which has resulted in their emergence as impressive zero-shot AI models. For example, VLMs function as the backbones of SOTA vision-language-action robot policies\n[\n22\n,\n23\n,\n24\n]\n, which enable robots to perform diverse tasks with a single (unified) policy in place of multiple task-specific policies.\nDespite these impressive capabilities, language models face some important limitations. First, language-only abstractions lack the expressiveness required to efficiently capture the intricate interaction processes inherent in the physical world. For example, consider the task of creating a concise fine-grained description of the contact interactions between a robotâ€™s gripper and a target deformable object, e.g., a cloth. Such a task is immensely challenging due to the high density of information and the limited capacity of language.\nSecond, language-centric modeling fails to accurately model the spatial and temporal dependencies between real-world phenomena (events) that are crucial to comprehensively understanding the physical world.\nAlthough physics-based simulators offer visual world modeling capabilities, their practical effectiveness is limited by a number of fundamental challenges. Particularly, physics-based simulation generally requires restrictive simplifying assumptions that hinder the visual and physical fidelity of their predictions. For example, physics engines require expensive asset curation procedures to minimize the sim-to-real gap and struggle to accurately simulate deformable bodies with complex morphology and dynamics.\nVideo generation models address these aforementioned limitations by providing a photorealistic, physically consistent spatiotemporal model of the world. These capabilities have driven growing applications of video models in robotics.\nIn robotics, video models have been increasingly adopted as embodied world models\n[\n25\n,\n26\n,\n27\n,\n28\n]\n.\nHigh-fidelity\nworld modeling establishes a trustworthy foundation for efficient evaluation of robot policies\n[\n17\n,\n29\n,\n16\n,\n30\n]\n.\nPolicy evaluation\nhas traditionally necessitated setting up real-world robot stations for online policy roll-outs, which is notably expensive given the pertinent hardware and labor cost. Video models circumvent this bottleneck without compromising the reliability of evaluation results.\nAs high-fidelity world models, video models also provide accurate dynamics and rewards predictions, which is essential in training robot policies using reinforcement learning\n[\n31\n,\n32\n,\n33\n,\n34\n]\n.\nAdditionally, video models enable\ncost-effective robot data generation\n, which is particularly important in imitation learning. While data scaling has proven to be a critical recipe for advancing the performance of SOTA robot policies\n[\n35\n,\n23\n]\n, collecting expert demonstration data is incredibly expensive, posing a significant limitation. Video models facilitate scalable data generation without relying on human supervision, addressing this pressing challenge.\nThe expert demonstrations from video models can also be\ndirectly applied to a robot through motion retargeting\n[\n36\n,\n37\n,\n38\n]\n.\nBeyond generating successful task demonstrations, video models can also synthesize failure video trajectories that endow robot policies with corrective behaviors for more robustness.\nFurthermore, the generated data can be optimized through\nvisual planning\nto compute more optimal robot trajectories\n[\n39\n,\n15\n,\n40\n,\n14\n,\n41\n,\n42\n]\n.\nAlthough video models provide valuable capabilities, their integration into robotics faces notable limitations. Like LLMs, video models tend to hallucinate, generating videos that are physically unrealistic, e.g., with objects appearing/disappearing or deforming in ways that violate physics\n[\n43\n,\n44\n,\n45\n,\n46\n]\n. Video models also struggle to follow user instructions\n[\n47\n,\n48\n]\n, especially in long-duration video generation tasks\n[\n49\n,\n50\n,\n51\n]\n. Further, significant data curation, training, and inference costs and the lack of adequate content safeguards in video generation remain critical challenges\n[\n52\n,\n53\n]\n, hindering broader adoption in robotics.\nThis survey provides an extensive overview of video models, identifying prevailing model architectures, key applications in robotics, and major challenges. We highlight four prominent applications of video models as embodied world models in robotics, including: (i) robot data generation and action prediction in imitation learning, (ii) dynamics and rewards modeling in reinforcement learning, (iii) policy evaluation, and (iv) visual planning, as illustrated in\nFigure\n1\n. Furthermore, we outline important open research challenges impeding trustworthy applications of video models in robotics and present important directions for future research to address these problems.\nComparison to Existing Surveys.\nPrior survey papers have covered video generation models broadly, particularly video diffusion models, identifying the prevailing model architectures\n[\n54\n,\n55\n]\n, techniques for controllable video generation\n[\n56\n]\n, core applications in video generation, video editing, and video understanding\n[\n57\n]\n. Other surveys\n[\n58\n,\n59\n,\n60\n,\n61\n]\nprovide a broad discussion of world models, presenting a valuable review of research across a wide range of model architectures, such as recurrent neural networks, transformers, language models, and scene reconstruction models, e.g., Gaussian Splatting.\nPrior work also highlights the unified framework for information representation and task formulation provided by video models\n[\n62\n]\nand discuss the progression in the capabilities of video models, through a primarily historical lens, along with potential future capabilities\n[\n63\n]\n.\nAlthough relevant for broader understanding, these existing surveys lack a comprehensive review of generative video models as world models in robotics\n[\n58\n,\n59\n,\n60\n,\n61\n]\nor an extensive discussion of specific applications, associated challenges, and future directions of generative video models in robotics\n[\n54\n,\n55\n,\n56\n,\n57\n,\n63\n]\n.\nIn contrast, our survey provides an exhaustive discussion of video generation models in robotics, especially in robot manipulation, which has not been considered by existing surveys unlike areas such as autonomous driving\n[\n64\n,\n65\n,\n66\n,\n67\n]\n.\nOrganization.\nWe discuss the organization of this survey, as summarized in\nFigure\n2\n.\nIn\nSection\n2\n, we provide an overview of important concepts that are crucial to understanding generative video modeling. We begin with a discussion of learned world models, highlighting the evolution of these models from (latent) state-based representations to high-fidelity video-based representations. Further, we review diffusion/flow-matching-based modeling highlighting the core principles behind their success. In\nSection\n3\n, we present major applications of video models in robotics, encompassing robot data generation and action prediction in imitation learning, dynamics and rewards modeling in reinforcement learning, policy evaluation, and visual planning. In\nSection\n4\n, we highlight useful metrics and benchmarks for evaluating video models. Subsequently, we identify open research challenges in\nSection\n5\nand suggest directions for future research. We conclude the survey in\nSection\n6\n.\n2\nBackground\nIn this section, we provide relevant background concepts that are essential to understanding the material presented in this survey. We begin with a brief introduction of world models, followed by an overview of video models. Specifically, we categorize video models broadly into non-diffusion-based video models and diffusion/flow-matching-based video models. State-of-the-art video models generally utilize diffusion or flow-matching for high-fidelity video synthesis. As a result, we provide a more comprehensive discussion of these models.\nVideo Generation Models in Robotics\nBackground\nApplications in Robotics\nEvaluating Video Models\nOpen Challenges and Future Directions\nMarkovian\nState-Based\nWorld Models\nVideo JEPA Models\nDiffusion/Flow-Matching Models\nData Generation/\nBackbones\nin Imitation Learning\nDynamics/Rewards Models in Reinforcement Learning\nPolicy Evaluation\nVisual Planning\nMetrics\nBenchmarks\nHallucinations and Violation of Physics\nUncertainty Quantification\nInstruction Following\nEvaluating Video Models\nSafe Content Generation\nSafe Robot Interactions\nAction Estimation\nLong Video Generation\nData Curation Costs\nTraining and Inference Costs\n[\n68\n,\n69\n,\n70\n,\n71\n,\n72\n,\n73\n,\n74\n,\n75\n]\n[\n76\n,\n77\n,\n78\n,\n79\n]\n[\n1\n,\n2\n,\n5\n,\n7\n,\n8\n,\n9\n,\n10\n,\n11\n,\n80\n,\n81\n]\n[\n12\n,\n82\n,\n83\n,\n84\n,\n85\n,\n86\n,\n87\n,\n88\n]\n[\n31\n,\n32\n,\n33\n,\n34\n,\n89\n,\n90\n]\n[\n16\n,\n17\n,\n29\n,\n30\n,\n91\n,\n92\n,\n93\n]\n[\n13\n,\n14\n,\n15\n,\n39\n,\n40\n,\n94\n,\n95\n,\n96\n]\n[\n97\n,\n98\n,\n99\n,\n100\n,\n101\n,\n102\n]\n[\n44\n,\n45\n,\n103\n,\n104\n,\n105\n,\n106\n]\n[\n43\n,\n48\n,\n107\n,\n108\n,\n109\n,\n110\n]\n[\n46\n,\n111\n]\n[\n112\n,\n113\n,\n114\n,\n115\n,\n116\n,\n117\n]\n[\n17\n,\n47\n,\n48\n,\n103\n,\n118\n]\n[\n52\n,\n53\n,\n119\n]\n[\n30\n,\n120\n,\n121\n]\n[\n8\n,\n122\n,\n123\n,\n124\n,\n125\n]\n[\n49\n,\n50\n,\n51\n,\n126\n,\n127\n,\n128\n]\n[\n11\n,\n129\n,\n130\n,\n131\n,\n132\n,\n133\n]\n[\n31\n,\n134\n,\n135\n,\n136\n,\n137\n,\n138\n]\nFigure 2:\nOrganization.\nThe organization of this survey, including background material, taxonomy of robotics applications, evaluation metrics and benchmarks, and open challenges and directions for future research.\n2.1\nLearned World Models\nMany robotics algorithms require a model of the robotâ€™s environment to efficiently learn policies that are effective in the real-world, especially when real-world interactions are prohibitively costly or unsafe. For example, reinforcement learning (RL) methods\n[\n139\n,\n140\n]\ngenerally require a large number of interactions between an agent and its environment to learn useful behaviors, which is typically expensive in the real-world. World models enable scalable data collection for training these policies with little to no real-world interaction.\nAt their core, world models predict the evolution of the environment of an agent due to interactions.\nTraditionally, physics-based simulators\n[\n141\n,\n142\n]\nhave been utilized as world models for predicting the dynamical effects of a robotâ€™s actions.\nHowever, physics-based simulators typically utilize simplified physics engines that approximate physical laws for computational feasibility, introducing inductive biases that limit their realism, e.g., in simulating non-rigid objects. Importantly, these approximations often contribute to the sim-to-real gap in robotics, impeding successful transfer of simulation-trained robot policies to the real-world. Moreover, robot manipulation tasks have become increasingly complex, exacerbating these challenges.\nLearned world models\n[\n143\n,\n144\n]\nhave emerged as an effective solution to these challenges.\nWe can broadly classify learned world models into two categories:\nMarkovian state-based\nworld models and\nvideo\nworld models.\nMarkovian State-Based World Models.\nA state refers to a description of the environment of an agent at a given time step, which could include its RGB observations, proprioception, or latent embeddings of these observations at past and current time steps. Markovian state-based world models assume that the future evolution of the agentâ€™s environment only depends on the agentâ€™s state\ns\nt\ns_{t}\nat the current time step\nt\nt\nand its action\na\nt\na_{t}\n, which represents either a physical action or a latent action.\nMarkovian state-based world models are trained to predict the future state\ns\nt\n+\n1\ns_{t+1}\n, given the current state\ns\nt\ns_{t}\nand action\na\nt\na_{t}\n:\ns\nt\n+\n1\nâˆ¼\np\nÎ·\nâ€‹\n(\ns\nt\n+\n1\n|\ns\nt\n,\na\nt\n)\n,\ns_{t+1}\\sim p_{\\eta}(s_{t+1}|s_{t},a_{t}),\n(1)\nwhere\np\nÎ·\np_{\\eta}\nrepresents the dynamics predictor with parameters\nÎ·\n\\eta\n.\nThese world models generally operate in latent space and are composed of an encoder for embedding observations into a latent space, dynamics predictor, and rewards predictor, given by:\nEncoder:\ns\nt\nâˆ¼\nâ„°\nÎ³\nâ€‹\n(\ns\nt\n|\no\nt\n)\n,\n\\displaystyle s_{t}\\sim\\mathcal{E}_{\\gamma}(s_{t}|o_{t}),\n(2)\nDynamics Predictor:\ns\n^\nt\n+\n1\nâˆ¼\np\nÎ·\nâ€‹\n(\ns\n^\nt\n+\n1\n|\ns\nt\n,\na\nt\n)\n,\n\\displaystyle\\hat{s}_{t+1}\\sim p_{\\eta}(\\hat{s}_{t+1}|s_{t},a_{t}),\n(3)\nRewards Predictor:\nr\n^\nt\n+\n1\nâˆ¼\np\nÎ¶\nâ€‹\n(\nr\n^\nt\n+\n1\n|\ns\n^\nt\n+\n1\n)\n,\n\\displaystyle\\hat{r}_{t+1}\\sim p_{\\zeta}(\\hat{r}_{t+1}|\\hat{s}_{t+1}),\n(4)\nwith parameters\nÎ³\n\\gamma\nand\nÎ¶\n\\zeta\nfor the encoder and rewards predictor, respectively.\nPrior work\n[\n145\n,\n146\n,\n68\n,\n69\n,\n147\n,\n148\n]\nhas largely parameterized the dynamics predictor\np\nÎ·\np_{\\eta}\nusing recurrent neural networks (RNNs) or recurrent state-space models (RSSMs).\nHowever, more recent works employ transformers\n[\n70\n]\nand diffusion in pixel space\n[\n149\n,\n150\n,\n151\n]\nor latent space\n[\n152\n,\n153\n]\nfor more expressive dynamics prediction.\nIn general, world models\n[\n74\n,\n75\n,\n154\n,\n155\n,\n70\n,\n156\n,\n88\n]\nare trained to minimize the error between the predicted and ground-truth next state, either in latent space or reconstructed pixel space using the mean-squared error (MSE) loss function\n[\n70\n,\n149\n]\nor the Kullback-Leibler (KL) divergence loss\n[\n68\n]\n.\nWe refer interested readers to\n[\n58\n]\nfor a detailed review of Markovian state-based world models.\nVideo World Models.\nTo model the world, video models learn spatiotemporal mappings that capture the evolution of an environment over space and time, without explicitly modeling a Markovian state, in contrast to Markovian state-based world models.\nIn general, video models apply non-linear transformations to the patches or pixels of a video frame to propagate the environment dynamics.\nEarly research in video prediction applied spatial transformations to pixels to generate future frames\n[\n157\n,\n158\n]\n. However, these transformations capture only local perturbations of the frames, limiting their expressiveness in more complex video generation tasks, e.g., with dynamic backgrounds.\nOther methods\n[\n71\n,\n159\n]\nextend generative adversarial networks (GANs)\n[\n160\n]\nto the video prediction setting, demonstrating higher-fidelity video generation. However, the susceptibility of GANs to mode collapse limits their ability to model diverse potential video evolution paths.\nSubsequent work\n[\n72\n,\n161\n,\n162\n,\n163\n,\n73\n]\nuses variational inference via variational autoencoders\n[\n164\n]\nto learn a latent distribution over pixels, explicitly encoding the stochasticity in video prediction within the latent distribution. Some of these methods\n[\n72\n]\nutilize a convolutional decoder to map back to the RGB space, while others\n[\n161\n,\n163\n]\napply GANs and long-short term memory networks\n[\n165\n]\nfor decoding the sampled latent variables.\nGiven the success of transformers in language modeling, the work in\n[\n166\n]\nintroduces the video transformer, achieving higher-quality video generation through autoregressive predictions.\nHowever, powerful autoregressive decoders introduce the challenge of latent collapse, where the learned model fails to efficiently use the latent space. Vector-quantized variational autoencoders (VQ-VAEs)\n[\n167\n]\naddress this issue by quantizing the latent representations into discrete codes in a codebook.\nVQ-VAEs are typically pre-trained on large-scale image and video datasets with a reconstruction objective and typically utilize spatial and temporal compression for compact latent embeddings across frames. To enforce temporal causality, VQ-VAEs replace GroupNorm operations with RMSNorm operations for temporal feature caching\n[\n10\n,\n11\n]\n.\nBuilding on these advances, later research efforts use VQ-VAEs to learn the latent distribution with autoregressive convolutional decoders\n[\n162\n]\nor autoregressive transformers\n[\n73\n,\n168\n]\nfor higher-fidelity video generation. Further, Genie\n[\n28\n]\ntrains a controllable video model using a VQ-VAE and a latent action model for frame-by-frame control.\nDespite the resulting performance gains, these video models lack the ability to capture complex interactions within an environment, limiting their applications.\nThese limitations have been addressed by diffusion and flow-based matching, enabling video models to learn highly expressive representations of the physical world.\nWe provide a detailed discussion of these video models in the subsequent subsections, along with a brief overview of joint-embedding predictive architectures, which seek to learn world models from video data without predicting pixel-level visual details.\n2.2\nDiffusion/Flow-Matching Video Models\nFigure 3:\nDiffusion Video Model Architectures.\nDiffusion/Flow-matching has emerged as the dominant model architecture for training photorealistic controllable video models that can be steered using text, image, and other conditioning inputs. These models broadly utilize diffusion transformers (DiTs) or U-Nets to learn important interpendencies across space and time within a compact latent space.\nThe emergence of diffusion modeling\n[\n1\n]\nand flow-matching\n[\n2\n]\nhas transformed the SOTA in image and video generation\n[\n169\n,\n170\n]\n, achieving high-fidelity video generation that capture fine-grained realistic and cinematic effects.\nGiven the similarity between diffusion modeling and flow-matching, we limit our discussion to diffusion models for simplicity.\nWe refer readers to\n[\n2\n]\nfor a more detailed presentation of flow-matching.\nDiffusion models\n[\n1\n]\nhave gained prominence as a powerful formulation for generative models that synthesize data by modeling an iterative denoising process. We review the mathematical formulation of diffusion models, common architectural designs, and their supported input/output modalities, which is summarized in\nFigure\n3\n.\n2.2.1\nMathematical Formulation\nDiffusion models\n[\n1\n,\n171\n]\nlearn a generative process by reversing a gradual noising process that transforms data into pure noise. Instead of directly modeling the likelihood function, diffusion models learn the gradient of the log-probability density, enabling them to effectively guide any sample\nğ±\n\\mathbf{x}\nto the desired data manifold, without explicitly parameterizing the data distribution.\nFormally, given data\nğ±\n0\nâˆ¼\nq\nâ€‹\n(\nğ±\n0\n)\n\\mathbf{x}_{0}\\sim q(\\mathbf{x}_{0})\n, the\nforward process\n(also called the diffusion process) progressively corrupts clean data\nğ±\n0\n\\mathbf{x}_{0}\nwith Gaussian noise over\nT\nT\ntime steps under some noise schedule\n{\nÎ²\nt\n}\nt\n=\n1\nT\n\\{\\beta_{t}\\}_{t=1}^{T}\n, given by:\nq\nâ€‹\n(\nğ±\nt\n|\nğ±\nt\nâˆ’\n1\n)\n=\nğ’©\nâ€‹\n(\nğ±\nt\n;\n1\nâˆ’\nÎ²\nt\nâ€‹\nğ±\nt\nâˆ’\n1\n,\nÎ²\nt\nâ€‹\nğˆ\n)\n.\nq(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})=\\mathcal{N}\\left(\\mathbf{x}_{t};\\sqrt{1-\\beta_{t}}\\,\\mathbf{x}_{t-1},\\,\\beta_{t}\\mathbf{I}\\right).\n(5)\nThe forward diffusion process can be expressed in closed form as the marginal distribution over noisy samples at any time step\nt\nt\n[\n172\n]\n:\nq\nâ€‹\n(\nğ±\nt\n|\nğ±\n0\n)\n=\nğ’©\nâ€‹\n(\nğ±\nt\n;\nÎ±\nÂ¯\nt\nâ€‹\nğ±\n0\n,\n(\n1\nâˆ’\nÎ±\nÂ¯\nt\n)\nâ€‹\nğˆ\n)\n,\nq(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}\\left(\\mathbf{x}_{t};\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0},\\,(1-\\bar{\\alpha}_{t})\\mathbf{I}\\right),\n(6)\nwhere\nÎ±\nt\n=\n1\nâˆ’\nÎ²\nt\n{\\alpha_{t}=1-\\beta_{t}}\nand\nÎ±\nt\nÂ¯\n=\nÎ \ni\n=\n0\nÎ±\nt\n{\\bar{\\alpha_{t}}=\\Pi_{i=0}^{\\alpha_{t}}}\n.\nIn contrast, the\nreverse process\niteratively removes the noise added to the images in the forward process.\nMore specifically, each denoising step is parameterized by a Gaussian transition\np\nÎ¸\nâ€‹\n(\nğ±\nt\nâˆ’\n1\n|\nğ±\nt\n)\np_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})\nwith learnable weights\nÎ¸\n\\theta\nthat approximates the true posterior\nq\nâ€‹\n(\nğ±\nt\nâˆ’\n1\n|\nğ±\nt\n,\nğ±\n0\n)\nq(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0})\n:\np\nÎ¸\nâ€‹\n(\nğ±\nt\nâˆ’\n1\n|\nğ±\nt\n)\n=\nğ’©\nâ€‹\n(\nğ±\nt\nâˆ’\n1\n;\nÎ¼\nÎ¸\nâ€‹\n(\nğ±\nt\n,\nt\n)\n,\nğšº\nÎ¸\nâ€‹\n(\nğ±\nt\n,\nt\n)\n)\n.\np_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})=\\mathcal{N}\\left(\\mathbf{x}_{t-1};\\mathbf{\\mu}_{\\theta}\\left(\\mathbf{x}_{t},t\\right),\\mathbf{\\Sigma}_{\\theta}\\left(\\mathbf{x}_{t},t\\right)\\right).\n(7)\nDiffusion models are trained to minimize the divergence between the distribution of the forward and backward transitions by predicting the ground-truth noise\nÏµ\n\\bm{\\epsilon}\nwith the (simplified) loss function\n[\n1\n]\n:\nL\nÏµ\n=\nğ”¼\nğ±\n0\n,\nt\n,\nÏµ\nâ€‹\n[\nâ€–\nÏµ\nâˆ’\nÏµ\nÎ¸\nâ€‹\n(\nğ±\nt\n,\nt\n)\nâ€–\n2\n]\n.\nL_{\\bm{\\epsilon}}=\\mathbb{E}_{\\mathbf{x}_{0},t,\\bm{\\epsilon}}\\left[\\left\\|\\bm{\\epsilon}-\\bm{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)\\right\\|^{2}\\right].\n(8)\nAlternatively, the loss function can be expressed using a velocity-based parameterization\n[\n173\n]\nfor better numerical stability and faster convergence, where the model is trained to predict the velocity\nğ’—\n=\nÎ±\nÂ¯\nt\nâ€‹\nÏµ\nâˆ’\n1\nâˆ’\nÎ±\nÂ¯\nt\nâ€‹\nğ±\n0\n{\\bm{v}=\\sqrt{\\bar{\\alpha}_{t}}\\bm{\\epsilon}-\\sqrt{1-\\bar{\\alpha}_{t}}\\mathbf{x}_{0}}\n, with the loss function:\nL\nğ’—\n=\nğ”¼\nğ±\n0\n,\nt\n,\nğ’—\nt\nâ€‹\n[\nâ€–\nğ’—\nt\nâˆ’\nğ’—\nÎ¸\nâ€‹\n(\nğ±\nt\n,\nt\n)\nâ€–\n2\n]\n.\nL_{\\bm{v}}=\\mathbb{E}_{\\mathbf{x}_{0},t,\\bm{v}_{t}}\\left[\\left\\|\\bm{v}_{t}-\\bm{v}_{\\theta}(\\mathbf{x}_{t},t)\\right\\|^{2}\\right].\n(9)\nTo enable controllability of diffusion models, the loss function is modified for conditioning using input prompts or actions, which we discuss next.\n2.2.2\nClassifier-Based and Classifier-Free Guidance\nConditioning for diffusion models is used to steer the outputs of diffusion models towards desired attributes specified by an input prompt through classifier-based and classifier-free guidance. Classifier-based guidance\n[\n169\n]\ntrains an external classifier that learns to predict an attribute (label)\ny\ny\nfrom a noisy sample\nx\nt\nx_{t}\n, denoted as\np\nÏ•\nâ€‹\n(\ny\nâˆ£\nx\nt\n)\np_{\\phi}(y\\mid x_{t})\n. At inference time, the model can be steered towards a desired attribute by biasing the sampled noise towards the direction that increases the likelihood of that attribute via:\nÏµ\n^\nÎ¸\nâ€‹\n(\nx\nt\n,\nt\n,\ny\n)\n=\nÏµ\nÎ¸\nâ€‹\n(\nx\nt\n,\nt\n)\nâˆ’\ns\nâ‹…\nÏƒ\nt\nâ€‹\nâˆ‡\nx\nt\nlog\nâ¡\np\nÏ•\nâ€‹\n(\ny\nâˆ£\nx\nt\n)\n,\n\\hat{\\epsilon}_{\\theta}(x_{t},t,y)=\\epsilon_{\\theta}(x_{t},t)-s\\cdot\\sigma_{t}\\nabla_{x_{t}}\\log p_{\\phi}(y\\mid x_{t}),\n(10)\nwhere\nÏµ\n^\nÎ¸\n\\hat{\\epsilon}_{\\theta}\ndenotes the biased noise sample.\nClassifier-based guidance is limited by high computation costs and training instability since it requires training a classifier on noisy data. Classifier-free guidance\n[\n174\n]\naddresses these challenges by training a joint conditional and unconditional model.\nAt inference time, the strength of the conditioning can be modulated by adjusting the guidance scale\nÏ‰\n\\omega\n:\nÏµ\n~\nÎ¸\nâ€‹\n(\nx\nt\n,\nt\n,\ny\n)\n=\n(\n1\n+\nw\n)\nâ€‹\nÏµ\nÎ¸\nâ€‹\n(\nx\nt\n,\nt\n,\ny\n)\nâˆ’\nw\nâ€‹\nÏµ\nÎ¸\nâ€‹\n(\nx\nt\n,\nt\n)\n.\n\\tilde{\\epsilon}_{\\theta}(x_{t},t,y)=(1+w)\\,\\epsilon_{\\theta}(x_{t},t,y)-w\\,\\epsilon_{\\theta}(x_{t},t).\n(11)\nwhere\nÏµ\n~\nÎ¸\n\\tilde{\\epsilon}_{\\theta}\ndenotes the biased noise sample.\nClassifier-free guidance has become the dominant conditioning strategy due to its simplicity, versatility, and robustness\n[\n175\n,\n176\n,\n177\n,\n178\n]\n.\n2.2.3\nModel Architecture\nEarly diffusion models\n[\n80\n]\noperate directly in the pixel space, which is prohibitively expensive at high resolutions. To overcome this limitation, recent approaches adopt a latent diffusion paradigm, where observations are first compressed into a lower-dimensional latent space using a variational autoencoder (VAE)\n[\n179\n,\n81\n,\n180\n,\n10\n,\n11\n]\nprior to the diffusion process.\nOther alternative latent representations can also be used, e.g., DINO features\n[\n181\n]\n.\nDiffusion models are broadly parameterized using U-Nets\n[\n182\n,\n183\n]\nor transformers\n[\n184\n,\n185\n]\n, which we discuss in the rest of this section.\nU-Net Architectures.\nThe U-Net architecture\n[\n182\n,\n183\n]\nhas become a foundational backbone for a wide range of vision tasks, including diffusion modeling. U-Nets consist of a downsampling path that progressively reduces the spatial resolution of an input image (video frame) with hierarchical convolution layers to capture high-level semantic features, followed by an upsampling path that recovers the original resolution of the input data. U-Nets rely on skip connections between corresponding layers in the encoder and decoder to preserve fine-grained spatial information and to stabilize training.\nBuilding on its success in image generation, early works on video diffusion\n[\n80\n,\n186\n,\n135\n]\nextend 2D U-Nets to the spatiotemporal domain by lifting 2D convolutions to 3D or by introducing temporal attention modules to jointly model spatial and temporal dependencies.\nTo create a unified strategy for training (U-Net) video diffusion models, Stable Video Diffusion\n[\n81\n]\nexplored effective strategies for data curation, text-to-image pretraining, video pretraining, and video model finetuning, demonstrating the superior performance of the resulting video models.\nThe resulting training recipe has been widely adopted by open-source video model implementations.\nTransformers.\nRecent work has utilized transformers in video diffusion modeling, given their effectiveness in language and computer vision tasks, e.g., language generation and object segmentation.\nTransformer-based diffusion models\n[\n184\n,\n185\n]\nreplace the traditional U-Net backbone with a diffusion transformers (DiT), leveraging the\nexpressivity of self-attention mechanisms.\nIn DiTs, visual inputs are first partitioned into patches and then embedded as tokens with positional encodings to preserve spatial relationships. Unlike U-Nets, which rely on an encoderâ€“decoder hierarchy with skip connections to fuse multi-scale features, DiTs employ a uniform transformer architecture that jointly processes all tokens. This design trades the U-Netâ€™s inductive biases toward locality and translation equivariance for a more flexible capacity to model long-range dependencies and higher-order semantic relationships.\nFor video generation tasks, these key characteristics enable DiTs to better capture temporal coherence and overall scene consistency, making them particularly effective for large-scale video diffusion models.\nConsequently, SOTA video models\n[\n185\n,\n10\n,\n5\n,\n7\n,\n9\n]\nutilize DiT-based architectures.\nMany methods employ specialized video encoders to perform temporal compression, substantially reducing token counts and thereby improving overall generation efficiency.\n2.2.4\nConditioning Modalities\nModern video diffusion models support a wide range of generation modalities for different levels of granularity in controllable video generation.\nBefore discussing specific conditioning modalities in video diffusion models, we first clarify the primary mechanisms by which conditioning signals are applied to the diffusion process.\nVideo diffusion models generally introduce conditioning inputs into the video generation process through channel concatenation, cross-attention, or adaptive normalization.\nChannel concatenation augments the inputs or latent representations along the channel dimension, which is most effective when the conditioning signal is spatially aligned with the target output, such as in image-to-video generation, depth guidance, or pose conditioning. This approach enforces a strong, pixel-wise correspondence and imposes a hard structural constraint on the generation process. In contrast, cross-attention captures interactions between intermediate video features (queries) and the conditioning signal (keys and values)\n[\n187\n]\n, making it the dominant approach for semantic, non-spatial conditions like text prompts. Finally, adaptive normalization methods such as AdaLN or FiLM\n[\n188\n,\n189\n]\nmodulate the scale and shift parameters of normalization layers using a global conditioning vector and are typically employed in (scalar) global attribute control, such as target frame rate or motion intensity, that affect overall generation statistics rather than localized structure.\nThe primary conditioning modalities supported by video diffusion models include text prompts, input images, and motion or trajectory primitives, described in the subsequent discussion.\nText-to-Video (T2V) Generation.\nText-to-Video (T2V) generation requires translating high-level semantic descriptions into temporally coherent visual sequences. To achieve this, conditioning is most commonly injected through cross-attention layers, enabling the model to selectively attend to individual words or phrases while synthesizing specific spatial regions and temporal segments of the video\n[\n190\n,\n191\n]\n. This mechanism allows linguistic concepts to exert fine-grained, context-dependent influence, while preserving temporal consistency across frames.\nImage-to-Video (I2V) Generation.\nImage-to-Video (I2V) generation pipelines synthesize a video from a reference image (usually the first frame) and optionally a text prompt. The initial frame provides contextual information that seeds the generation process\n[\n81\n,\n175\n]\n.\nExisting approaches generally incorporate image conditioning through either frame-level input concatenation or cross-layer attention. For low-level, frame-aligned conditioning, the conditioning frames are concatenated with the video input along the temporal or channel dimension. For higher-level semantic guidance, the image features (extracted using pretrained encoders such as CLIP\n[\n100\n]\n) are incorporated via cross-attention layers, either independently or jointly with text tokens.\nMotion/Trajectory-Guided Generation..\nPrior work\n[\n192\n]\nencodes motion trajectories as coordinate maps, optical flow fields, keypoint heatmaps, or applied forces, which are injected via channel concatenation, dense cross-attention, or specialized conditioning adapters such as ControlNet\n[\n193\n]\n. More recent efforts focus on embodiment-specific video models tailored to robots, directly conditioning on low-level robot states or actions (e.g., joint positions or torques)\n[\n11\n,\n17\n,\n16\n,\n194\n,\n30\n]\n.\nThe fine-grained conditioning inputs enable impressive controllability and physical grounding of the generated videos.\n2.3\nVideo Joint-Embedding Predictive Architecture Models\nHere, we discuss methods based on joint-embedding predictive architectures (JEPAs)\n[\n76\n,\n77\n,\n79\n]\nthat learn world models from internet-scale videos for future prediction, video understanding, and planning in latent space.\nIn contrast to diffusion/flow-matching video models, which prioritize photorealistic video generation, video JEPA models\n[\n77\n,\n79\n]\nseek to learn effective latent representations through self-supervised training on video data. These latent representations form a foundational component for robust video understanding in downstream tasks, such as video question answering, motion understanding, and action recognition, using task-specific models trained on latent features extracted by JEPA models. Importantly, JEPA models\n[\n79\n]\ncan be directly applied to robot planning by conditioning the JEPA backbone on robot actions during the training process.\nJEPAs build on contrastive learning approaches for effective self-supervised learning. Next, we review contrastive methods before discussing JEPA methods.\nContrastive learning.\nContrastive-based methods learn latent space representations that identify salient characteristic features of (image) inputs to distinguish between similar and dissimilar (image) pairs. Under the assumption of separability of the inputs, contrastive models map inputs to a latent space such that similar data are encoded close together in the latent space while differing inputs are further apart.\nContrastive learning\n[\n195\n]\ntypically applies random transformations, such as cropping, resizing, color distortions, or noise injection, to generate augmentations of an input data point. Subsequently, the augmented inputs are mapped to latent embeddings using a learned model, which is trained using the InfoNCE loss function:\nâ„’\ni\n,\nj\n=\nâˆ’\nlog\nâ¡\nexp\nâ¡\n(\nsim\nâ€‹\n(\nz\ni\n,\nz\nj\n)\n/\nÏ„\n)\nâˆ‘\nk\n=\n1\n2\nâ€‹\nN\nğŸ™\nk\nâ‰ \ni\nâ€‹\nexp\nâ¡\n(\nsim\nâ€‹\n(\nz\ni\n,\nz\nk\n)\n/\nÏ„\n)\n,\n\\displaystyle\\mathcal{L}_{i,j}=-\\log\\frac{\\exp(\\text{sim}(z_{i},z_{j})/\\tau)}{\\sum_{k=1}^{2N}\\mathbbm{1}_{k\\neq i}\\exp(\\text{sim}(z_{i},z_{k})/\\tau)},\n(12)\nwhere\nsim\nâ€‹\n(\nâ‹…\n)\n\\text{sim}(\\cdot)\ncorresponds to the normalized dot product of the two vectors, given a positive image pair\n(\nx\ni\n,\nx\nj\n)\n{(x_{i},x_{j})}\nfrom a dataset of\nN\nN\nimage pairs.\nContrastive learning suffers from two core issues. First, since positive pairs come from different augmentations of the same image, other similar images are treated as negative examples, forcing the model to push apart representations that should be close. Second, these methods are prone to representation collapse, where the encoder learns to produce a trivial solution for all inputs.\nTo address these challenges, Bootstrap Your Own Latent (BYOL)\n[\n196\n]\nintroduces a slow-moving teacher network that guides a student network. During training, the teacher networkâ€™s weights are updated as an exponential moving average of the studentâ€™s weights. In place of a contrastive loss, BYOL minimizes a normalized mean squared error between the studentâ€™s and teacherâ€™s predictions, stabilizing training and enabling effective representation learning using only positive pairs. DINO encoders\n[\n197\n,\n198\n,\n199\n]\nbuild upon BYOL by recasting the teacher-student framework as a knowledge distillation process, where the student learns to match the teacher networkâ€™s probability distribution over\nK\nK\nclasses by minimizing the cross-entropy loss between the predicted probabilities.\nJoint-Embedding Predictive Architecture.\nJoint-embedding predictive architectures\n[\n76\n]\nextend self-supervised learning (e.g., BYOL) to the predictive domain through the introduction of a predictor and a conditioning variable to steer the outputs. To avoid representation collapse, JEPAs utilize an asymmetric architecture for embedding input images, composed of a student encoder, target encoder, and predictor.\nVideo joint-embedding predictive architecture (V-JEPA) models\n[\n77\n,\n79\n]\nbuild upon this foundation for video generation, predicting spatially and temporally masked features within the latent space. By making predictions within the latent space as opposed to pixel space, the model is encouraged to predict higher-level representations rather than pixel-level information\n[\n78\n]\n. This abstract understanding allows the model to build an internal world model, enabling it to also make predictions about future states and plan without task-specific training or reward.\nTo train a V-JEPA video model, masked video frames are passed to an encoder\nE\nÎ¸\nE_{\\theta}\n, while the unmasked frames are fed into an exponential moving average of\nE\nÎ¸\nE_{\\theta}\n, denoted by\nE\nÎ¸\nÂ¯\nE_{\\bar{\\theta}}\n. Given these embeddings, a predictor network\nP\nÏ•\nP_{\\phi}\ncomputes the latent representations of the masked frames conditioned on a learnable mask token\nÎ”\ny\n\\Delta_{y}\n. The model is trained with the loss function:\nmin\nÎ¸\n,\nÏ•\n,\nÎ”\ny\nâ€‹\nâ€–\nP\nÏ•\nâ€‹\n(\nÎ”\ny\n,\nE\nÎ¸\nâ€‹\n(\nx\n)\n)\nâˆ’\nsg\nâ€‹\n(\nE\nÎ¸\nÂ¯\nâ€‹\n(\ny\n)\n)\nâ€–\n1\n,\n\\displaystyle\\min_{\\theta,\\phi,\\Delta_{y}}||P_{\\phi}(\\Delta_{y},E_{\\theta}(x))-\\text{sg}(E_{\\bar{\\theta}}(y))||_{1},\nwhere\nsg\nâ€‹\n(\nâ‹…\n)\n\\text{sg}(\\cdot)\nstands for the stop-gradient operator.\nAfter\nE\nÎ¸\nE_{\\theta}\nhas been trained, it can be frozen and used to train a transition model with additional action-labeled trajectory data\n[\n79\n]\n. JEPA models are still prone to representation collapse\n[\n200\n]\n, although they are more robust compared to prior approaches. Regularizers\n[\n201\n,\n202\n]\nhave been shown to help prevent posterior collapse, but are difficult to tune and interpret\n[\n200\n]\n.\n3\nApplications of Video Models in Robotics\nIn this section, we explore applications of video models as high-fidelity world models in robotics.\nWorld models capture the evolution of an environment under the effects of actions applied by an agent. However, modeling physical interactions is extremely challenging, given the stochastic nature of dynamical interactions, which is often not fully described by simplified physical laws.\nAlthough early world models\n[\n69\n,\n154\n]\neffectively learn high-level scene dynamics, they struggle with modeling intricate dynamical changes, especially with high-fidelity. Notably, in fine-grained interaction tasks in robotics, such as dexterous manipulation, these models often fail to predict the effect of subtle changes in robot actions that have significant impacts on the success of such tasks.\nFor high-fidelity world modeling, these models generally require lots of training data centered around such dynamical events for effective supervision, which is often difficult to collect in many practical applications.\nMore traditional physics-based simulators function as world models; however, their capabilities are often limited by restrictive assumptions built on simplified object models and dynamics models. For example, physics-based simulators typically struggle with deformable-body simulations, which require higher-fidelity dynamics model. Importantly, these assumptions contribute to the sim-to-real gap faced by physics-based simulators, e.g., through the use of primitive object shapes, appearance and visual conditions.\nRecent work in robotics seeks to address these limitations by leveraging video models as\nembodied world models\n.\nFigure 4:\nVideo Models for Embodied World Modeling.\nVideo models provide high-quality representations of the physical world, which could be implicit (e.g., latent and video representations) or explicit (e.g., point clouds and Gaussian Splatting models).\nVideo models can generate high-quality predictions of the future state of the physical world without a prohibitive demand for large-scale action-labeled datasets. In general, recent approaches\n[\n203\n,\n26\n,\n204\n,\n25\n,\n8\n]\nfine-tune pre-trained video models to synthesize high-accuracy 4D scene representations autoregressively, repeatedly generating future frames conditioned on the previously generated video frames. Video world models can be classified into two categories, based on the maintained scene representation, namely:\nimplicit\nand\nexplicit\nvideo world models, as illustrated in\nFigure\n4\n.\nImplicit video world models.\nIn implicit video world models, the 3D scene representation is encoded exclusively within the video model, without any external representation.\nAs such, visualization of the evolving scene can only be achieved by generating videos from the video model.\nWhile some methods\n[\n203\n]\nonly support text-conditioned video generation, others\n[\n26\n,\n205\n,\n25\n,\n157\n,\n206\n,\n207\n]\nfine-tune these models on robot datasets, e.g., Bridge\n[\n208\n]\n, DROID\n[\n209\n]\n, etc., for action-conditioned generation using robot actions. Pandora\n[\n203\n]\nenables finer control over longer-duration videos by fine-tuning the DynamicCrafter video model\n[\n210\n]\nfor text-conditioned video generation with language instructions for any video frame. Similarly, FreeAction\n[\n194\n]\nmodulates classifier-free guidance with the magnitude of the input actions to improve action-conditioned video generation.\nLikewise, Vid2World\n[\n25\n]\nextends the DynamicCrafter video model to autoregressive action-conditioned video generation using casual attention and diffusion forcing\n[\n126\n]\n.\nOther approaches fine-tune video models to predict a target objectâ€™s dynamics for object-centric video generation\n[\n211\n]\nand also explore wrist-camera video generation\n[\n205\n]\ngiven only scene-camera input images.\nExplicit video world models.\nIn contrast to implicit video world models, explicit video world models create a concrete 3D scene representation using video models, e.g., with multi-view videos or depth maps.\nFor example, Enerverse\n[\n26\n]\ntrains an autoregressive video generation model on multi-view video data conditioned on text and image inputs using simulation data since many open-source large-scale robot datasets lack multi-view inputs. The resulting multi-view video frames provide a sparse representation of the 3D world, which might not be detailed enough in fine robotics tasks. Consequently, Enerverse trains a 4D Gaussian Splatting scene representation\n[\n212\n]\nfor high-fidelity novel-view synthesis. In contrast, Aether\n[\n204\n]\nfine-tunes a video model (CogVideoX\n[\n129\n]\n) to generate depth and camera raymap videos, from which a 3D scene representation is extracted via back-projection.\nOther methods\n[\n213\n]\nconstruct a voxel-grid scene representation of the world from generated videos, which also serves as a memory bank for the video model.\nGenie Envisioner\n[\n27\n]\ntrains a video generation model alongside an action decoder to create a world model amenable to action prediction, policy evaluation, and data generation. With its action decoder, Genie Envisioner transforms latent video states to predicted actions given a language-conditioned robot task, and uses the video model for policy rollouts.\nThe high-fidelity world modeling capabilities of video models enable a broad range of downstram robotics applications, including: (i) efficient data generation and action prediction in imitation learning, (ii) expressive dynamics and rewards modeling in reinforcement learning, (iii) scalable policy evaluation, and (iv) visual planning.\n3.1\nCost-Effective Data Generation and Action Prediction in Imitation Learning\nIn recent years, impressive research advances in robotics have been driven by imitation learning on large-scale expert demonstrations, circumventing the well-documented challenges associated with explicitly modeling dynamical interactions between robots and their environments. For example, SOTA vision-language-action (VLA) models\n[\n23\n,\n24\n,\n214\n,\n215\n]\nhave demonstrated remarkable capabilities in generalist language-conditioned robot manipulation, exhibiting strong task and environment generalization and robust recovery behaviors in the presence of disturbances. Scaling foundation models in terms of model and training data size has proven to be essential to realizing these significant leaps in performance, a fact that has been further underscored by the success of LLMs trained on internet-scale data. Although this path to advancing robot learning holds significant promise, the costs associated with collecting large-scale expert demonstrations, such as time and labor costs, pose notable challenges.\nTo address this challenge, recent work\n[\n216\n,\n83\n,\n12\n,\n118\n,\n217\n,\n218\n]\nemploys video generation models as cost-effective data generators of expert demonstrations, eliminating the overhead associated with human-supervised data collection.\nConcurrently, recent research has explored the use of video models as policy backbones in imitation learning, seeking to harness the synergies between dynamics prediction and policy learning to ground proposed robot actions.\nWe present these application areas in the subsequent discussion.\nFigure 5:\nVideo Models for Data Generation.\nVideo models enable high-fidelity data generation for cost-effective policy learning. Robot actions can be extracted from videos through modular approaches using end-effector pose tracking or end-to-end approaches, such as inverse-dynamics methods.\nVideo Models as Data Generators.\nFor scalable data generation, existing methods fine-tune pre-trained video models, such as Cosmos Predict\n[\n219\n]\nand Wan\n[\n10\n]\n, on robot datasets for adaptation to robot embodiments and environments.\nHowever, the expressiveness of text-conditioned and image-conditioned video models is often too limited to capture the diversity of the training data required to train generalizable robot policies. Subsequent research on controllable video models\n[\n220\n,\n221\n,\n222\n,\n118\n]\nseek to overcome this limitation through finer control of camera and object motion in generated videos, given an input text, image, keypoints, or trajectory vectors, describing spatial changes in the scene temporally.\nSome methods\n[\n218\n]\nfurther decompose the task into a set of subtasks for generating keyframes, which serve as conditioning inputs to generate video data for longer-horizon tasks.\nOther methods train video models for effective cross-embodiment transfer.\nFor example, Human2Robot\n[\n216\n]\ntrains a video model to generate robot videos given human videos, from which robot actions are estimated.\nThe resulting action-annotated data is used for policy learning, e.g., with VLA models\n[\n12\n,\n219\n]\n, or directly applied to a robot\n[\n36\n,\n37\n,\n223\n]\n.\nIn general, robot data generation methods recover robot actions from generated videos using an end-to-end approach or a modular approach, which is illustrated in\nFigure\n5\n.\nEnd-to-end methods typically utilize latent action models or inverse-dynamics models to estimate actions from videos. Latent action models\n[\n224\n,\n122\n,\n123\n]\ninfer actions that explain the transition between a pair of video frames in latent space without the need for ground-truth actions, which often resides in a different action space. To learn latent actions without supervision, existing methods train an encoder-decoder model with a reconstruction-based loss function, generally implemented in latent space. The encoder takes in a pair of frames and outputs a latent action, which is passed into the decoder to recover the future frame. SOTA latent action models use VQ-VAEs as the model architecture for the encoder and decoder, given the superior tradeoff offered by VQ-VAEs in terms of expressiveness and training efficiency. Given the non-equivalence between the action space of the ground-truth and latent actions, latent action models often require a subsequent fine-tuning stage at deployment time to align the ground-truth and latent action spaces using a small set of action-labeled data.\nUnlike latent action models, inverse-dynamics models (IDMs)\n[\n225\n,\n124\n,\n125\n]\nlearn to predict actions from videos in a supervised fashion from action-labeled video data. Recent methods parameterize the video-to-action mapping using an encoder-decoder model trained with a diffusion objective. Although the need for ground-truth action labels poses a challenge to IDMs, IDMs do not require a fine-tuning stage, enabling zero-shot deployment.\nFor example, DreamGen\n[\n12\n]\ntrains an IDM using action-annotated robot datasets for action estimation.\nLikewise, Video Prediction Policy (VPP)\n[\n226\n]\ntrains an IDM based on a diffusion model architecture to regress robot actions from latent features generated by a text-conditioned video model.\nARDuP\n[\n227\n]\nand Vidar\n[\n217\n]\nemploy a similar approach. Whereas Vidar supervises the IDM by predicting action-relevant regions of the videos, ARDuP directly feeds active region masks to the video model as conditioning inputs.\nBroadly, modular approaches estimate the target objectâ€™s pose in each video frame. Some of these methods\n[\n82\n,\n83\n,\n84\n]\nutilize learned pose trackers\n[\n228\n,\n229\n]\nand monocular depth estimators\n[\n230\n]\nfor 3D object pose estimation from 2D image keypoints.\nFurther, methods such as AVDC\n[\n38\n]\nand VideoAgent\n[\n85\n]\nuse an off-the-shelf optical flow predictor to extract pixel movement, from which the motion of the target object is estimated.\nOther methods\n[\n86\n]\nestimate the target objectâ€™s pose using computer-aided design (CAD) models. The resulting object trajectory is subsequently retargeted (i.e., applied) to the robot, under the assumption that a fixed rigid transformation exists between the objectâ€™s reference point and the robotâ€™s end-effector.\nBy taking advantage of low-level control routines, these methods can be deployed zero-shot without the need for a separate fine-tuning stage for aligning the action spaces between training and deployment.\nVideo Models as Policy Backbones.\nUsing video models as policy backbones, unified video-action methods train robot policies to jointly predict videos and actions conditioned on a language instruction and initial observation. While most unified video-action methods\n[\n231\n,\n232\n,\n87\n,\n233\n]\nutilize a VLA architecture for these policies, more recent work\n[\n234\n,\n41\n,\n235\n]\ndirectly adapts pre-trained video models for joint video and action generation.\nGR1\n[\n236\n,\n232\n]\ntrains an autoregressive transformer-based model to jointly predict future images and actions, given the language instruction, sequence of observation images, and a sequence of robot states.\nRPT\n[\n231\n]\ntakes a masking-based training approach, by masking the modelâ€™s inputs, e.g. actions, images, or robot states, before action prediction.\nIn contrast, UVA\n[\n87\n]\ntrains an encoder to learn a joint latent representation over video frames and actions. The resulting latent embeddings are used in training an autoregressive transformer for action prediction, video generation, and inverse-dynamics modeling.\nUnlike UVA, PAD\n[\n237\n]\nand Video Policy\n[\n234\n]\nuse a pre-trained Stable Diffusion encoder\n[\n179\n]\nwith diffusion transformers or U-Nets for video and action generation, reducing training overhead.\nLike UVA, UWM\n[\n233\n]\nuses independent diffusion processes for action and video generation.\nMeanwhile, DreamVLA\n[\n88\n]\nfurther supervises the VLA with depth maps, dynamic regions, and semantic feature prediction to more strongly guide the model in learning useful features.\nLikewise, UniVLA\n[\n238\n]\nintroduces stronger supervisory signals into the training process by predicting language tokens (e.g., text descriptions) in addition to video frames and actions.\n3.2\nDynamics and Rewards Modeling in Reinforcement Learning\nFigure 6:\nDynamics and Rewards Modeling.\nVideo models provide high-accuracy dynamics modeling and rich reward signals, which are essential in reinforcement learning, circumventing long-standing challenges in system identification and reward engineering.\nAs discussed in the preceding subsection, SOTA robot policies are generally trained using imitation learning, which provides more efficient training compared to reinforcement learning (RL) but does not generalize well outside of the training data distribution. Although RL circumvents this challenge, RL requires the specification of dynamics and rewards models, which are often non-trivial in many practical problems. Further, RL suffers from low sample efficiency, requiring large amounts of training data to achieve the same level of performance as imitation-learned policies. Recent research seeks to address these limitations by using generative video models as expressive dynamics and rewards models in RL, shown in\nFigure\n6\n.\nWhereas earlier work utilized recurrent state-space models\n[\n239\n]\nor image diffusion models\n[\n240\n]\nfor dynamics prediction, more\nrecent work trains video diffusion models, which provide higher-fidelity predictions that are more physically consistent.\nDreamer 4\n[\n31\n]\ntrains an action-conditioned video model from scratch by first training a text-conditioned video model and subsequently fine-tuning the model on action-labeled data.\nThe resulting action-conditioned video model serves as a dynamics predictor to fine-tune imitation-learned policy and reward heads using RL, e.g., in Minecraft.\nIn contrast, World-Env\n[\n32\n]\nuses a pre-trained action-conditioned video model\n[\n89\n]\nfor video generation and a VLM (instance reflector) for dense reward signals\nto fine-tune a VLA using RL.\nAnother subclass of methods directly use video models as reward models in RL. While VIPER\n[\n33\n]\nuses the video prediction likelihood as the reward signal, Diffusion Reward\n[\n34\n]\nuses the conditional entropy of the video generation distribution as the reward signal, inspired by the idea that video trajectories closer to the expert trajectories seen in the training data have lower entropy.\nFurther, the work in\n[\n90\n]\nuses a video model to generate videos that guide exploration during robot policy learning. The resulting experience is saved to a buffer for fine-tuning the policy. Although this work uses a goal-conditioned policy, where the video frames serve as image subgoals, the approach can be applied to RL.\n3.3\nScalable Policy Evaluation\nBeyond policy learning, video models enable reliable policy evaluation that is reproducible across different environments and task settings. Real-world evaluation of robot manipulation policies is notably challenging due to the significant inherent hardware and labor costs, especially for generalist robot policies, which typically require evaluation across a combinatorial number of operating environments. For example, each hardware trial requires human monitoring to reset the environment, observe the policy rollout, and record success scores, which is expensive.\nWhile hardware evaluation is the gold-standard, policy evaluation in simulation is the best available alternative, especially with recent improvements in visual fidelity, system identification, and careful tuning of material properties. However, physics-based simulation requires significant setup time, as each real environment needs to be manually recreated, including the careful tuning of several parameters such as lighting and material properties to minimize the sim-to-real gap.\nIn contrast, video models\n[\n241\n,\n242\n,\n243\n]\noffer a higher-fidelity, more scalable framework for policy evaluation, modeling intricate robot-environment interactions that are challenging for physics-based simulators, e.g., in deformable-body simulation.\nHere, we survey a growing body of research on the application of video models to policy evaluation, including policy comparison and estimation of real-world success rates\n[\n16\n,\n17\n,\n26\n,\n29\n,\n91\n,\n92\n,\n93\n,\n30\n]\n. These studies demonstrate that video models can offer useful sanity checks prior to real-world policy deployment.\nReal-world success rates can be estimated by rolling out the policy in closed-loop with a video model on sampled initial observations (e.g., camera images) and task instructions.\nTo improve the quality and consistency of video generation, some architectures\n[\n91\n]\ncondition the video model on the latent action representation in the policy, instead of conditioning on the robotâ€™s physical actions\n[\n17\n,\n16\n]\n. As robot manipulation setups often involve multiple camera views, several architectures are adapted for consistent multi-view generation\n[\n17\n,\n30\n,\n26\n,\n93\n]\n, which has been empirically shown to reduce hallucinations\n[\n17\n]\n. To further mitigate prediction errors that accumulate over long rollouts, existing methods incorporate history during video generation, conditioning future video frames not only on the current observation but also on past observations. The history may consist of all frames within a fixed window length\n[\n92\n,\n16\n,\n29\n]\nor a sparsely sampled subset of past frames\n[\n17\n,\n26\n]\n. Moreover, augmenting observations with corresponding robot joint poses has been shown to improve frame-level action controllability\n[\n17\n]\n.\nEach rollout is scored according to a rubric to quantify success (e.g., Bernoulli score, partial credit) on task completion and instruction following, and the scores of individual rollouts are aggregated into empirical success rates.\nTo assess the accuracy of video models in policy evaluation, existing approaches\n[\n30\n,\n17\n,\n93\n]\nuse metrics such as the Pearson correlation coefficient and the Mean Maximum Rank Violation (MMRV). The Pearson correlation coefficient measures the strength of the linear relationship between predicted and real-world success rates â€” a high correlation is achieved when policies with high real-world success rates also exhibit high success rates in video model evaluations, and policies with low real success rates similarly have low video model success rates. In contrast, rank violation captures ranking inconsistencies between policy pairs, weighting each violation by the absolute difference in real-world success rates. The MMRV score is computed as the average across policies of their maximum rank violations.\nAlthough the predicted and real-world success rates do not always match, the evaluation results provide valuable insights for ranking the relative performance of policies, which is essential in improving robot policies in subsequent design and training stages.\nFurthermore, video models can also be used to assess the robustness and safety of robot policies, particularly in out-of-distribution conditions\n[\n30\n,\n16\n]\n. In the Veo World Simulator\n[\n30\n]\n, such out-of-distribution scenarios can be rapidly constructed via image editing, e.g., to alter the background or add novel objects and distractors. This pipeline enables the use of video models to consistently predict performance degradation along different axes of generalization. Additionally, the pipeline can be readily interfaced with a VLM to generate safety-critical scenes and tasks for testing whether the policy executes semantically or physically unsafe behavior\n[\n30\n]\n.\nFor accurate predictions across successful and failure scenarios, existing methods\n[\n29\n,\n93\n,\n92\n]\nincorporate failure data when training video models to avoid a bias towards always predicting success, which we discuss further in\nSection\n5.9\n.\n3.4\nVisual Planning\nRobot planning offers a compelling alternative to imitation learning, enabling impressive generalization beyond the distribution of the training dataset.\nHowever, the design of high-fidelity models that closely predict the scene dynamics often proves prohibitively challenging. To address these drawbacks, recent work has explored utilizing video models in\nvisual planning\n, defined as the problem of synthesizing a sequence of images or video frames that show the steps necessary to complete a task specified by a language instruction and an initial observation. By leveraging the extensive diversity of the training data encoded by video models, these methods circumvent the need for large-scale expert demonstrations or explicit dynamics models to solve a broad range of robot tasks.\nVisual planning methods\n[\n244\n]\ngenerally optimize the generated video plans using sampling-based trajectory optimization methods, such as the gradient-free cross-entropy method, or gradient-based methods, e.g., gradient descent or Levenberg-Marquardt optimizers. The optimization routine is often embedded within a model predictive control (MPC) framework, facilitating the incorporation of new observations during planning via sensor feedback.\nVideo-model-based visual planning methods can utilize\naction-guided\nor\naction-free\napproaches to create feasible robot trajectories.\nAction-Guided Visual Planning.\nAction-guided visual planning methods utilize a three-step approach, which consists of: generating action proposals, synthesizing video trajectories using video models as dynamics prediction modules given these action proposals, and subsequently evaluating the resulting video trajectories based on an objective function. Broadly, the action proposals are typically generated by sampling-based approaches\n[\n39\n]\n, learned approaches\n[\n15\n]\n, or VLMs\n[\n94\n,\n14\n]\n. The work in\n[\n39\n]\nuses the cross-entropy method to generate action proposals, optimizing the photometric/Euclidean error between a user-provided goal image or keypoints and the video frames generated by the video model conditioned on the proposed actions. The action samples are drawn by fitting a Gaussian distribution to the actions associated with the minimum objective value and sampling from the resulting Gaussian. Similarly, FLIP\n[\n15\n]\nuses a conditional variational autoencoder to generate action proposals and fine-tunes a language-image value network\n[\n245\n]\nto learn a value function over the generated video trajectories. The action that maximizes the discounted return is selected for execution.\nIn contrast to these approaches, MindJourney\n[\n94\n]\nuses a VLM to propose candidate camera trajectories and to evaluate the generated video trajectories from a video model given these action proposals.\nLikewise, the work in\n[\n14\n]\nuses a VLM for task decomposition, breaking down the language-specified task into subtasks in natural-language, which serve as text inputs for the video model. The VLM further analyzes the generated videos to expand and refine the subtasks using a tree-search-based approach. Subsequently, a goal-conditioned policy predicts robot actions from the best video plan. VLP\n[\n95\n]\ntakes a similar approach, integrating a video model with an LLM, which analyzes the generated videos and selects a final plan for execution.\nAction-Free Visual Planning.\nAction-free visual planning methods do not utilize action proposals for planning. Rather, these methods generate video plans directly from text-conditioned video models and use the video frames as image subgoals for planning.\nPrior work\n[\n96\n]\nuses goal-conditioned behavior cloning for extracting robot actions from the image subgoals. In contrast, CLOVER\n[\n40\n]\ntrains an IDM that maps the current image observation and image subgoals to robot actions. The IDM is further conditioned on the error between the image embeddings of the current observation and image subgoals for stronger feedback.\nSimilarly, UniPi\n[\n246\n]\nuses an IDM for action estimation but also introduces a super-resolution video generation phase that refines coarse video trajectories into finer video plans.\nIn contrast, NovaFlow\n[\n13\n]\nextracts the object pose (represented by particles in the case of deformable objects) from generated videos, which serve as coarse robot actions. The resulting action plan serves as a reference trajectory that is refined using non-linear least-squares optimization with a pre-trained particle-based dynamics model.\n4\nEvaluating Video Models\nIn this section, we present standard metrics and benchmarks for evaluating video models across different dimensions, including perceptual quality, physical consistency, and semantic alignment with the input prompts.\n4.1\nMetrics for Evaluating Video Models\nTo assess video generation quality in accordance with human judgment, various metrics are needed to evaluate video models on visual quality, temporal coherence, diversity of generation, and physical commonsense.\nAlthough many traditional image quality metrics\n[\n97\n,\n98\n,\n247\n]\nexist, these metrics do not assess temporal consistency of generated videos. To address this limitation, recent work\n[\n101\n,\n101\n]\nhas introduced video-specific metrics that measure video quality both spatially and temporally.\nAdditionally, application-focused metrics are equally important. For example, in policy evaluation, the usefulness of an action-conditioned video generation model depends on how well it can predict policy success rate, and in policy learning, whether the generated demonstration data results in more performant policies\n[\n12\n]\n.\nHere, we discuss frame-level metrics and spatiotemporal consistency metrics.\nFrame-level Metrics.\nTraditional frame-level metrics from computer vision can be used to assess video generation quality, such as the peak signal-to-noise ratio (PSNR)\n[\n97\n]\nand structural similarity index (SSIM)\n[\n98\n]\n.\nUnlike PSNR, which is inversely proportional to the mean squared error of the pixel values, SSIM assesses image quality on luminance, contrast, and structure, which is more indicative of perceived similarity. Pixel-based metrics often fail to capture higher-order image structures that align with human perceptual judgments\n[\n98\n]\n, which might be better represented in the learned feature spaces of deep models. For instance, the CLIP similarity score uses the cosine similarity between images in the embedding space\n[\n100\n]\nto assess semantic alignment between a pair of images. For generative models, the inception score\n[\n248\n]\nassesses if the model confidently generates diverse, yet semantically meaningful images by computing the expected KL-divergence between the predicted class distribution and the marginal distribution of all generated images. However, the inception score does not use real samples, depends on a fixed label space, and is not suited for image generation without a target object. The FrÃ©chet Inception Distance (FID)\n[\n249\n]\naddresses these challenges by computing the Wasserstein-2 distance between multi-dimensional Gaussian distributions over the real and generated feature embeddings. Likewise, the Learned Perceptual Image Patch Similarity (LPIPS)\n[\n99\n]\nis a similarity score on feature embeddings from several layers of a deep image recognition network, which has been shown to be comparable to human judgments. Furthermore, recent work\n[\n247\n]\nhas demonstrated that these perceptual metrics can be learned to further align with human judgments.\nSpatiotemporal Metrics.\nWhile frame-level metrics focus on visual representation at any given video frame, assessing the temporal coherence of videos is also important for evaluating video generation quality. The FrÃ©chet Video Distance (FVD)\n[\n101\n]\naccomplishes this by extending the FrÃ©chet Inception Distance temporally to video representations that capture motion features across frames as well as the quality of images per frame.\nInstead of assuming a Gaussian representation for the features, the Kernel Video Distance (KVD)\n[\n101\n]\nuses a kernel-based metric (maximum mean discrepancy), to capture higher-order variations. The FrÃ©chet Video Motion Distance (FVMD) computes the FrÃ©chet distance on motion features by extracting keypoints that can be tracked across multiple frames and computing their velocities and accelerations.\nPrior work has also employed optical flow\n[\n250\n,\n10\n,\n102\n]\nto assess temporal coherence and VLMs\n[\n44\n,\n12\n]\nto evaluate physical consistency of generated videos.\n4.2\nBenchmarks for Evaluating Video Models\nDespite their impressive capabilities, video models tend to generate videos that violate specific desired qualities, such as physical consistency, even when these videos are of high visual quality. Prior work\n[\n251\n,\n48\n,\n252\n]\nhas introduced benchmarks to assess the performance of video models across different criteria, spanning visual quality, dynamic consistency, and instruction following.\nBy evaluating video models across many dimensions, these benchmarks highlight critical areas for future research, in addition to identifying SOTA video models.\nGenerally, existing benchmarks demonstrate that video models fail to follow physical laws, even after scaling these models, although their aesthetic quality and temporal consistency tend to improve with scale.\nWhile many existing benchmarks evaluate the overall quality or physical consistency of generated videos from video models, only a few benchmarks assess the safety of generated videos. These safety benchmarks\n[\n53\n,\n52\n]\nprovide an extensive evaluation of the safety of video models, demonstrating their tendency to generate illegal or unethical videos that violate their safety guidelines.\nIn the subsequent discussion, we present benchmarks that examine the overall quality of video models broadly and those that focus on evaluating physical commonsense.\nBroad Benchmarks.\nWorldModelBench\n[\n48\n]\nintroduces a benchmark to evaluate the instruction following and physics adherence capabilities of video models, identifying physically inconsistent changes in the sizes of objects, which violates the law of mass conservation.\nFurther, EvalCrafter\n[\n103\n]\nevaluates the quality of generated videos in terms of their aesthetic and motion quality as well as temporal consistency and text-to-video alignment. While visual quality is assessed using DOVER\n[\n253\n]\n, motion quality is evaluated using action recognition methods, which are based on the activity classes in the Kinetics 400 dataset\n[\n254\n]\n. Temporal consistency and text-to-video alignment are measured using optical flow and CLIP, respectively.\nEvalCrafter shows that its evaluation criteria are well-aligned with human preferences.\nEWMBench\n[\n255\n]\nalso evaluates video models along similar axes but uses the cosine-similarity metric with DINOv2\n[\n198\n]\nembeddings to measure visual consistency.\nFurthermore, VBench\n[\n104\n]\nexamines the performance of video models across sixteen fine-grained criteria, such as background consistency, subject consistency, and temporal flickering, in addition to aesthetic and semantic quality and instruction following.\nLike other benchmarks, VBench demonstrates that the results of each video model on each criterion is highly correlated with human preferences.\nLikewise, PAI-Bench\n[\n252\n]\ncompares video models based on the temporal consistency, motion smoothness, and aesthetic quality of their generated videos.\nBeyond measuring overall video consistency, T2V-CompBench\n[\n105\n]\nevaluates object attribute consistency, such as color, shape, and texture, along with action consistency in generated videos. Similarly, WorldSimBench\n[\n47\n]\nassesses the physical consistency of motion, particularly the perception of a sense of depth, and changes in the velocity of objects in different media such as air and water.\nPhysical Commonsense.\nOther benchmarks focus primarily on evaluating the physical commonsense of video models\n[\n108\n,\n43\n]\n.\nFor example, Physics-IQ\n[\n43\n]\nassesses video modelsâ€™ understanding of the laws of physics, such as those on optics, thermodynamics, magnetism and fluid dynamics, demonstrating that video models lack a solid understanding of physical laws.\nLikewise, PhyGenBench\n[\n45\n]\nmeasures the commensense knowledge of video models on\n27\n27\nphysical laws, including gravity, sublimation, solubility, and friction, across\n160\n160\nprompts. VideoPhy\n[\n44\n]\nevaluates video models along similar axes, focusing more on object interactions.\nVP\n2\n\\text{VP}^{2}\n[\n106\n]\nexamines the alignment of video models with physical laws in the task of visual planning, demonstrating that strong performance on perceptual metrics is generally not indicative of alignment with physical consistency. Further,\nVP\n2\n\\text{VP}^{2}\n[\n106\n]\nshows that although scaling the model size and training dataset improves performance, the resulting gains plateau relatively quickly.\n5\nOpen Challenges and Future Directions\nWe identify open research challenges in robotics applications of video models and highlight directions for future research to address these challenges. We note that solutions to these challenges apply broadly beyond robotics, which could motivate novel applications of video models.\n5.1\nHallucinations and Violations of Physics\nDespite their impressive capabilities, video models often hallucinate, generating implausible video frames that are temporally inconsistent or misaligned with physical reality.\nIn T2V generation, recent works\n[\n256\n,\n257\n,\n46\n]\nhave explored different types of hallucinations, including vanishing subject, omission error, numeric variability, visual incongruity, and subject dysmorphia\n[\n256\n]\n, and have proposed hallucination detectors.\nIn robotics, the challenge of hallucination is especially important, given the critical role of accurate future prediction in policy evaluation and visual planning, among other application areas.\nRecent works have found that using multi-view frame inputs and particularly including a wrist camera view reduces hallucinations\n[\n17\n]\n.\nHowever, augmented conditioning inputs have limited effect on mitigating the violation of physics.\nSpecifically, video models typically fail to follow the laws of physics governing object motion and interactions\n[\n107\n,\n111\n]\n. Prior work\n[\n44\n,\n43\n,\n48\n]\nhas demonstrated that generated videos from these models violate fundamental principles, such as Newtonâ€™s law of motion, conservation of energy and mass, and gravitational effects, revealing a lack of understanding of these laws. Further, these models tend to mimic the closest training example when presented with a task at inference, limiting their generalization to new (unseen) tasks. In this setting, video models exhibit a propensity to prioritize transferring the color, size, velocity, and shape of objects in the training dataset to the objects in the new task, in that particular order\n[\n108\n,\n109\n]\n.\nFurther, existing work\n[\n44\n,\n45\n]\nshows that video models struggle with generating physically-realistic solid-solid interactions, demonstrating that video models do not understand the material properties of objects, the law of conservation of momentum, and the impenetrability of objects. Video models also show a fundamental lack of understanding of fluid mechanics and conservation of mass\n[\n43\n]\n, generating unrealistic videos of liquid flows, e.g., pouring a drink into a cup without a corresponding change in the volume of liquid in the cup.\nHowever, generating physically realistic data is crucial in robotics, such as in robot learning where the generated data serves as expert demonstrations for training a policy, highlighting the need to impart physical understanding to video models.\nMoreover, prompt engineering and scaling techniques do not adequately resolve this challenge, suggesting that novel architectures and training techniques are required to address this problem.\nNext, we discuss potential directions for future research to improve the physical consistency of videos generated by video models.\nFuture Directions.\nPhysical realism of generated videos from video models is essential in applications requiring trustworthy video generation, such as policy evaluation, planning, and policy learning, underscoring the importance of research on hallucination mitigation. To improve the physical consistency of generated videos, prior work\n[\n258\n,\n259\n,\n260\n,\n261\n]\nhas explored integrating physics-based priors and physics-based simulation with video generation.\nHamiltonian and Lagrangian approaches\n[\n258\n,\n259\n]\ntrain models to predict the dynamics of system based on Hamiltonian or Lagrangian mechanics. By encoding the laws of physics as priors in the training process, the resulting models tend to adhere better to the physical laws. Other methods employ simulation engines to enforce physical laws.\nFor example, PhysGen\n[\n260\n]\nextracts object segmentation masks and physical properties using VLMs and computes feasible trajectories for the annotated objects using rigid-body dynamics equations. Subsequently, the trajectories are applied to corresponding pixels for video generation. However, the resulting video often contains artifacts, which degrades its fidelity. PhysGen uses a video diffusion model to edit these artifacts to generate higher-quality videos\n[\n262\n]\n.\nLikewise, WonderPlay\n[\n261\n]\nconstructs a 3D scene representation from the conditioning inputs and estimates the physical properties of objects in the scene using a VLM. The scene and objectsâ€™ physical properties are passed into a physics-based simulator, which computes a coarse trajectory conditioned on applied forces.\nThe coarse trajectory from the simulator serve as conditioning inputs for a video model to generate future frames.\nOther methods\n[\n110\n]\nhave utilized LLMs to refine video modelsâ€™ input prompts to provide comprehensive descriptions of physical attributes and interactions to improve the physical accuracy of generated videos.\nAlthough these methods improve physical alignment, they do not eliminate violations of the laws of physics. Specifically, LLMs/VLMs are prone to hallucinations\n[\n263\n]\n, which limits the effectiveness of the aforementioned methods.\nMoreover, these methods rely on ad-hoc solutions, negatively impacting their generality and ease of implementation.\nWe believe a promising future direction will be to explore efficient techniques for natively endowing video models with a fundamental understanding of physical laws, which might necessitate the design of novel training techniques and model architectures\n[\n264\n]\nthat enforce physical laws.\nAdditionally, improving the ability of video models to understand feasible interaction modes could be critical to mitigating hallucinations. Prior work has explored identifying and localizing functional interaction cues from videos, e.g., how humans interact with objects and environments, which is referred to as affordance-based video understanding\n[\n265\n,\n266\n]\n.\nExisting methods\n[\n267\n,\n268\n,\n269\n,\n270\n]\nlocalize interaction regions in videos and embed these cues into a latent space to predict the evolution of contact regions (hotspots) across future video frames.\nThe resulting affordance maps\n[\n271\n,\n272\n,\n273\n,\n274\n]\ncan serve as conditioning (guidance) signals in video synthesis to improve the physical consistency of generated videos. Developing effective strategies for incorporating these signals during video generation is an important area for future work.\n5.2\nUncertainty Quantification\nUncertainty quantification (UQ) techniques are widely utilized with traditional deep neural networks to examine the trustworthiness of these difficult-to-interpret models. While the rapid development of LLMs has led to increased research in UQ of large generative models to address severe hallucinations (see\n[\n275\n]\nfor a review of LLM UQ methods), UQ methods for image and video generation models remain largely underexplored.\nMeanwhile, extending existing UQ methods to video generation models faces significant challenges due to the complexity of spatial and temporal relationships. For example, generative video modeling fails to satisfy the central assumptions of standard Bayesian UQ methods, such as the assumption of independent, identically distributed samples, given the correlation between frames across multiple timesteps. Additionally, the significant computational cost associated with video generation impedes the application of ensemble-based UQ methods. Importantly, existing video models lack the ability to express or verbalize their confidence. As a result, inference-time UQ methods, e.g., black-box LLM UQ methods, cannot be directly applied to video models.\nFuture Directions.\nA few recent works have explored UQ techniques for controllable video generation, including text-conditioned and action-conditioned models.\nS-QUBED\n[\n46\n]\nquantifies uncertainty of T2V generation in the semantic space. However, this method only estimates task-level confidence on video generation.\nOn the other hand,\nC\n3\nC^{3}\n[\n111\n]\ntrains video models for simultaneous video generation and uncertainty quantification in latent space. Specifically,\nC\n3\nC^{3}\nenables video models to predict the uncertainty associated with each subpatch of the generated video, providing dense confidence estimates spatially and temporally.\nIn general, these methods are only guaranteed to provide calibrated uncertainty estimates within the distribution of the training dataset, limiting their effectiveness in out-of-distribution use-cases.\nA promising direction for future work will be to explore more cost-effective methods for uncertainty quantification with provable guarantees both within and beyond the training distribution.\n5.3\nInstruction Following\nLike early language models\n[\n276\n]\n, text-conditioned video models struggle with following user instructions specified in the input prompts\n[\n44\n,\n47\n]\n, which poses a significant limitation. Although these models are generally trained with guidance for alignment with the conditioning inputs, existing guidance mechanisms do not provide sufficient supervision to produce videos consistent with the input prompt. In general, SOTA video models often fail to complete the specified task due to their inability to extract and transfer the intended action from the conditioning input to the specified actor (agent). Prior work\n[\n48\n]\nhas shown that video models are generally able to correctly generate videos containing the agents stated in the input prompt but only partially follow the specified action and in some cases, completely fail to incorporate the specified action into the generated video.\nFurthermore, video models typically fail to generate high-quality text (annotations) in videos\n[\n103\n]\n, even in videos with otherwise high-fidelity components. This limitation becomes especially conspicuous when the input prompt specifically requests text annotations. Further, camera motion control via input prompts remains a major challenge for video models\n[\n255\n,\n103\n]\n. Even when asked to generate videos with a static camera viewpoint without panning, video models tend to mimic their training videos, which often contain camera motion, resulting in non-adherence to the input prompt.\nThese challenges limit robotics applications in data generation and policy learning, among others. For example, many robot data generation methods operate under the assumption of a static camera position for accurate\n3\n3\nD pose estimation of the robotâ€™s end-effector pose through back-projection. Violation of this assumption results in inaccurate goal poses for tracking, degrading the robotâ€™s task performance. Likewise, failure to generate videos that correctly follow the actions specified by the input prompt leads to training data corruption, limiting the effectiveness of imitation learning, which relies on high-quality (expert) demonstrations.\nFuture Directions.\nRecent work\n[\n112\n,\n113\n,\n114\n]\nhas explored improving the instruction-following capability of video models through multimodal conditioning. ViMi\n[\n112\n]\ninterleaves language and images into a single instruction prompt, which is passed into a VLM to extract a conditional embedding for video generation. Similarly, Aid\n[\n113\n]\nuses a VLM to predict the states of future video frames, which are fused with the text instruction to generate a conditional embedding for video synthesis. Other approaches\n[\n115\n,\n116\n]\nenable fine-grained multimodal user inputs to more strongly guide video models in the generation process. With InteractiveVideo\n[\n115\n]\n, users can control the content of generated videos through image, text, and trajectory prompts, describing the desired motion of different elements of the video scene. Similarly, ATI\n[\n116\n]\nenables localized control of deformations using keypoints and motion paths. Some other methods\n[\n117\n]\nhave examined instruction fine-tuning using preference-based reward models to train video models that are better at following user instructions.\nThe aforementioned approaches generally rely on VLMs or other learned models that are susceptible to hallucinations, limiting their practical effectiveness.\nTo address this challenge, a promising research direction will be to explore intrinsic methods for improving task understanding and instruction following, e.g., through inference-time â€œreasoningâ€ over the generated video patches and frames, similar to the approach used in reasoning language models\n[\n19\n,\n277\n]\n.\n5.4\nEvaluating Video Models\nA unified framework for evaluating video models remains lacking, especially in the context of robotics applications.\nExisting metrics for evaluating video models generally assess either the perceptual quality\n[\n97\n,\n98\n,\n249\n]\nof their generated videos or their semantic consistency\n[\n100\n,\n198\n]\n. While visual quality may be prioritized in content generation, physical consistency and predictive accuracy are more critical in robotics\n[\n106\n]\n.\nGiven the lack of suitable metrics, researchers often resort to surrogate measures based on the downstream application, e.g., correlation between real and predicted policy success rates by video models, to approximate the performance of the video models\n[\n17\n]\n.\nAdditionally, many works rely on human judgment to evaluate video model performance\n[\n118\n]\n, which is qualitative, costly, and subject to bias.\nDesigning effective evaluation metrics is critical in assessing the performance of video models, which could further motivate directions for future development.\nFuture Directions.\nPrior works have developed benchmarks for evaluating video generation in terms of alignment with\ndownstream task performance\n[\n106\n,\n47\n]\n. However, the environment complexity and visual quality of these benchmarks are fundamentally limited by the simulators. Recent works have explored benchmarking and evaluating T2V generation without ground-truth videos along many axes, such as overall video quality, text-to-video alignment, motion quality, and temporal consistency\n[\n103\n]\n. To evaluate the quality of video models as embodied world models, WorldModelBench\n[\n48\n]\ntrains a VLM-based judge to assess video models in terms of instruction following, physics adherence, and commonsense.\nIn general, these benchmarks still lack the ability to assess video generation quality in fine-grained robot manipulation tasks.\nA promising direction for future research will be to explore robotics-centric benchmarks and multi-dimensional metrics for quantitative, efficient, and task-relevant video model evaluation. Further, future research on evaluation pipelines that compare the physical consistency of generated videos based on the corresponding motion in the associated\n3\n3\nD scene reconstructions will be important.\n5.5\nSafe Content Generation\nMany video models lack adequate safety guardrails, impeding their integration into many real-world applications.\nPrior work\n[\n53\n,\n52\n]\nhas demonstrated the propensity of video models to generate unsafe content, containing crime, offensive activities, violence, or misinformation, which could hinder adoption in sensitive applications.\nDespite the importance of safety, only a few papers\n[\n119\n]\nhave explored methods for improving the safety of video models.\nAddressing this challenge is essential to driving broader applications in robotics.\nFuture Directions.\nEnforcing the safety of video models is particularly challenging, given the broad diversity of unsafe video content. For tractability, prior work has utilized model guardrails to prevent the generation of potentially harmful content. While there is extensive literature on safeguarding LLMs\n[\n278\n]\n, analogous techniques for video generation models are far less developed. Recent work (e.g., SAFEWatch\n[\n52\n]\n) introduces a mechanism for enforcing user-specified safety policies during video generation. However, these methods are task-specific which limits their amenability to more general applications, underscoring the need for more versatile safety guardrails.\nAdditionally, there is a growing need for safety benchmarks for video models.\nExisting benchmarks\n[\n52\n,\n53\n]\nevaluate video models on a limited range of criteria, e.g., crimes, hate content, privacy violations, and abusive content; however, a broader evaluation of safety is required for real-world use-cases in robotics. Developing more effective safety guardrails and introducing more comprehensive safety benchmarks will be critical to improving the overall safety of video models in video synthesis.\n5.6\nSafe Robot Interaction\nBeyond safe video synthesis, robots must interact safely with other objects and agents in their environments. Safety in robotics can be broadly classified into two categories: (i) physical safety, which involves avoiding all forms of collisions; and (ii) semantic safety, which involves avoiding situations deemed potentially harmful by common-sense knowledge, such as tossing sharp objects at other agents. These forms of safety have been underexplored in robotics applications of video models. However, the success of many robotics tasks depends significantly on satisfying these safety constraints, e.g., in visual planning.\nFuture Directions.\nAs embodied world models, video models enable robots to assess the safety of proposed actions without executing these actions in the real-world, which is critical to preserving the safety of robots, other agents, and their environments.\nTo detect safety violations, some existing methods\n[\n30\n]\ngenerate video predictions from robot action proposals using video models.\nOther prior work\n[\n279\n,\n120\n]\nperform robot safety filtering directly in the latent space of world models to predict failures and preempt their occurrence. Likewise, existing methods\n[\n121\n,\n280\n]\nquantify uncertainty in the latent space to guard against unseen out-of-distribution (OOD) hazards and other user-specified constraint violations. While these works make notable steps toward safe robot control, they have primarily been limited to Markovian state-based world models. Extending these ideas to the spatiotemporal latent spaces of video world models remains a challenge, constituting a important direction for future research.\nMoreover, generalization beyond the training distribution remains a core challenge in long-tail or safety-critical scenarios because world models are inherently limited by the distribution of their training data.\n5.7\nAction Estimation\nSOTA imitation-learned robot policies require high-quality data\n[\n281\n,\n209\n,\n282\n]\n, which is often expensive to collect in the real-world. Although video generation models can address this challenge, the video generated by these models typically do not contain action-labeled frames, which is essential in learning robot policies.\nSome methods have explored estimating robot actions from videos; however, these methods\n[\n122\n]\ntypically fail to achieve the high level of accuracy required in imitation learning in fine-grained tasks, hindering the effectiveness of current solutions that integrate video models in policy learning frameworks. We discuss a few limitations of latent action models and inverse-dynamics models, the two main approaches for action estimation from robot videos.\nLatent action models\n[\n122\n,\n123\n]\nestimate robot actions between a pair of video frames in a (discrete) latent space, defined by a fixed set of action codes (primitives). The expressiveness of the learned latent actions is strongly influenced by the size of the latent codebook. Scaling the size of the latent codebook is challenging in practice due to the significant training instabilities and higher compute cost associated with training latent action models with larger codebooks. Further, latent actions are generally difficult to interpret, which makes data curation and analysis more challenging. Additional real-world data is also required to fine-tune latent action models to predict actions that are compatible with physical robots.\nInverse-dynamics models\n[\n124\n,\n125\n]\nsuffer from similar challenges and require lots of training data to sufficiently cover the space of robot actions. Like other imitation-learned models, inverse-dynamics models exhibit limited generalization outside of the training distribution, hindering real-world applications.\nFuture Directions.\nFor better interpretability, recent work\n[\n8\n]\nutilizes small latent codebooks, which facilitate mapping each latent action to interpretable conditioning inputs. However, this approach fails to scale to more complex robotics tasks which require larger codebooks to effectively learn latent actions.\nExploring model architectures that induce interpretable latent actions, without compromising the expressiveness of the latent action space, will be critical to advancing the adoption of video models in robot policy learning. Likewise, the development of robust training procedures for latent action models will be essential to their generalization in robotics.\nAdditionally, semi-supervised training techniques could enable efficient training of generalizable inverse-dynamics models with only a few human annotations, constituting a promising area for future work.\n5.8\nLong Video Generation\nTo serve as effective world models in robotics tasks, video models must predict sufficiently long future horizons that match the duration of robotic tasks, which are often minutes long.\nHowever, SOTA video models are limited to generating videos of only a few seconds in duration. For example, Veo 3.1\n[\n9\n]\ngenerates 8-second long videos while Wan 2.5\n[\n10\n]\ngenerates 10-second long videos. These durations are not long enough for informed decision-making in many robotics problems, such as visual planning. Current video generation pipelines require extending multiple short clips to create longer-duration videos; however, this approach often introduces artifacts that degrade the temporal coherence and physical consistency of the resulting videos.\nWhile SOTA video models excel in short-duration video generation tasks, scaling these models to longer horizons for robotics tasks remains an open challenge.\nFuture Directions.\nSeveral architectures attempt to address video consistency over long horizons. Broadly, these methods use frame compression, sampling schedule optimizations, or hierarchical frameworks to enable long video generation. For example, MALT\n[\n49\n]\nencodes past segments into a compact latent memory vector to facilitate autoregressive generation; however, this approach remains susceptible to error accumulation\n[\n283\n]\n. FramePack\n[\n50\n]\nmitigates drift by compressing frame contexts based on importance and establishing early endpoints to anchor the generation process.\nTTTVideo\n[\n127\n]\nand LaCT\n[\n128\n]\nutilize test-time training (TTT) to dynamically encode history into model weights or neural hidden states during inference. However, the quality of the generated videos degrades as video length increases.\nTo improve video fidelity, Long-Context Tuning (LCT)\n[\n51\n]\nexpands the context window to maintain dense attention across multi-shot scenes but is limited by the quadratic cost of self-attention, which imposes a computational ceiling on the maximum generation length. In contrast to these approaches, Mixture of Contexts (MoC)\n[\n284\n]\ncircumvents compressing history entirely by recasting generation as an information retrieval task via a sparse attention routing mechanism.\nDiffusion Forcing\n[\n126\n]\noffers an alternative solution by training models to denoise tokens with independent noise levels, enabling variable-horizon generation that empirically improves stability in policy rollouts. On the other hand, NUWA-XL\n[\n285\n]\nutilizes a hierarchical â€œdiffusion-over-diffusionâ€ structure, where a global model generates sparse keyframes and local models recursively fill the intermediate gaps.\nDespite these improvements, existing methods still struggle to generate minutes-long videos, impeding the their effectiveness in real-world applications. A promising direction for future work will be to design efficient techniques for extending the memory (context window) of video models without a prohibitive increase in the computation cost to enable high-fidelity long-video generation.\n5.9\nData Curation Costs\nHigh-quality data is essential to training video models that are capable of synthesizing high-fidelity, physically-consistent videos across diverse tasks. Beyond video quality, diversity of the training data strongly influences the fidelity of generated videos\n[\n136\n,\n129\n]\n, especially for text-conditioned or action-conditioned video models, which require extensive data coverage for desirable results.\nAlthough large volumes of videos exist on the internet, many of these videos lack good visual quality and descriptive text annotations, posing a challenge.\nSpecifically, many existing datasets, such as WebVideo-10M\n[\n286\n]\nand Panda-70M\n[\n130\n]\n, have focused on scale rather than quality, aggregating tens of millions of videos along with their captions. As a result, these datasets suffer from inaccurate, non-descriptive video captions, blurry videos, and rapid shot changes between temporally-inconsistent clips, which negatively impact training.\nSOTA methods\n[\n136\n,\n11\n,\n129\n,\n10\n]\ntypically rely on expensive data preprocessing pipelines to identify good-quality video data for training. This pipeline consists of three broad stages: (i) video splitting, (ii) video filtering, and (iii) video annotation. At the video splitting stage, candidate video data is temporally segmented into short continuous clips using classical shot detection tools. Videos that are too short are removed at this stage. The resulting clips are then processed by video filters using metrics that assess the visual quality, text quality, motion smoothness and jitter, among others, often with learned models.\nSubsequently, many methods\n[\n136\n,\n11\n]\nutilize VLMs\n[\n287\n,\n288\n,\n130\n]\nfor annotating the processed videos, pairing text descriptions with the video data for supervising video models. The VLMs are typically fine-tuned for higher-quality video captioning.\nThese processes often require human supervision for evaluation, increasing the cost of data curation.\nFuture Directions.\nRecent video datasets, e.g., VidGen-1M\n[\n131\n]\nand OpenVid-1M\n[\n132\n]\n, have explored different pre-processing techniques to filter videos using well-defined quality scores before training. These quality scores evaluate the aesthetics, temporal consistency, motion fidelity, and caption descriptiveness, identifying video clips that are more likely to be natural without unrealistic motion. Although these datasets generally provide higher-quality training data, their relatively small size impedes strong zero-shot generalization.\nConsequently, many SOTA video models\n[\n10\n,\n11\n,\n9\n]\nutilize proprietary curated datasets with dense annotations by VLMs and humans to ensure high quality control.\nHowever, VLMs are prone to hallucinations, and human-annotated data collection is expensive, posing significant challenges in high-fidelity video data collection. Exploring strategies for grounding VLMs to minimize the risk of hallucinations is a promising area for future work. Likewise, developing novel-view synthesis techniques to efficiently scale data collection to new scenes from a small set of high-quality videos will be essential to reducing the costs of data curation. Further, more accurate methods for splitting and filtering videos will be critical to curating highly diverse datasets with good temporal and spatial consistency, e.g.,\n[\n133\n]\n.\nIn robotics, high-fidelity future prediction in both successful and unsuccessful task rollout or demonstrations is essential for video models to serve as good proxies for real-world environments. Notably, failure demonstrations are important to train video models that faithfully execute actions output by the policy. Recent studies suggest that incorporating failure data from autonomous policy rollouts is critical for training world models that exhibit action controllability\n[\n29\n,\n93\n,\n92\n]\n. In the absence of such data, generated videos can exhibit an optimistic bias toward success, with hallucinations that reposition objects for easier grasping, over-estimate the feasibility of a grasp, and insufficiently model obstructions, all of which would cause real executions to fail\n[\n29\n]\n.\n5.10\nTraining and Inference Costs\nSOTA video models require large amounts of compute resources for training and inference.\nAlthough the true cost of training video models is often confidential especially for closed-source models, the most cost-effective SOTA open-source video models require hundreds of thousands of dollars to trainâ€”e.g., $200k for Open-Sora 2.0\n[\n136\n]\n.\nVideo models typically have billions of parameters, which is a major contributing factor to their significant training costs.\nAlthough latent diffusion\n[\n135\n]\nreduces the number of parameters required by these models, the corresponding reduction in training costs is often not sufficient. As a result, research on training video models has broadly remained limited to large research groups with deep budgets. Moreover, recent developments such as classifier-free guidance\n[\n174\n,\n169\n]\nfor fine-grained input-conditioning introduce additional computational overhead. Reducing training and inference costs is critical to advancing broader applications of video models in robotics.\nAdditionally, high-fidelity video models\n[\n10\n,\n11\n,\n7\n,\n9\n]\nare often limited by significantly slow inference speeds.\nFor example, Veo 3\n[\n9\n]\ngenerates about\n12\n12\nvideo frames per second on an NVIDIA A100 GPU. Although sufficient in some use cases, the slow inference time presents an important challenge in many robotics applications, such as visual planning, where closed-loop execution requires real-time feedback to the planner for robustness.\nExisting video model planners often take a few seconds to generate feasible action trajectories for a single episode\n[\n134\n]\n, which is not fast enough for real-time operation.\nFuture Directions.\nFor faster training and inference, recent work\n[\n136\n,\n31\n]\nhas explored spatial and temporal compression techniques to reduce the number of costly attention operations during video generation.\nDreamer 4\n[\n31\n]\napplies temporal attention sparsely to every fourth video frame by decoupling spatial and temporal attention. Similarly, OpenSora uses deep compression autoencoders\n[\n289\n]\nto downsample input tokens at greater ratios to speed up inference by an order of magnitude. Likewise, Wan\n[\n10\n]\nemploys a feature cache mechanism to enable chunk-based video synthesis while preserving temporal continuity.\nSome other methods utilize shortcut models\n[\n137\n]\nfor finer control over the number of sampling steps required for video generation without any significant degradation in video quality.\nLike shortcut models, consistency models\n[\n138\n,\n290\n]\nenable efficient video diffusion using a single-step denoising process.\nOther video models utilize more classical speed optimization techniques for faster inference, e.g., quantization\n[\n291\n]\nand model distillation\n[\n292\n]\n.\nFurther research on these topics is required to improve inference speeds for real-time applications.\n6\nConclusion\nThis survey reviews video models and their applications as embodied world models in robotics, identifying prevailing model architectures, conditioning modalities, and key capabilities of video models. Specifically, we categorize existing robotics applications into four broad classes: robot data generation and action prediction in imitation learning, dynamics and rewards modeling in reinforcement learning, policy evaluation, and visual planning.\nWe emphasize that these applications are underpinned by the remarkable ability of video models to learn fine-grained spatiotemporal relationships that govern the evolution of the state of real-world environments, which is essential for physically consistent future predictions.\nFurthermore, we identify critical open research challenges and propose directions for future research, seeking to motivate broader applications of video models.\nAcknowledgments\nAsher J. Hancock was supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-2146755. Apurva Badithela is supported by the Presidential Postdoctoral Research Fellowship at Princeton University. The authors were partially supported by the NSF CAREER Award #2044149, #2107048, the Office of Naval Research (N00014-23-1-2148), the Sloan Fellowship, and Apple Inc. Any views, opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and should not be interpreted as reflecting the views, policies or position, either expressed or implied, of Apple Inc.\nReferences\nHo et al. [2020]\nJonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising diffusion probabilistic models.\nAdvances in neural information processing systems\n, 33:6840â€“6851, 2020.\nLipman et al. [2023]\nYaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.\nFlow matching for generative modeling, 2023.\nURL\nhttps://arxiv.org/abs/2210.02747\n.\nLi et al. [2025a]\nXiaowen Li, Haolan Xue, Peiran Ren, and Liefeng Bo.\nDiffueraser: A diffusion model for video inpainting.\narXiv preprint arXiv:2501.10018\n, 2025a.\nKuaishou Technology [2024]\nKuaishou Technology.\nKling AI: Next-generation AI creative studio.\nhttps://ir.kuaishou.com/news-releases/news-release-details/kling-ai-launches-25-turbo-video-model-industry-leading\n, June 2024.\nRunway [2024]\nRunway.\nIntroducing Gen-3 Alpha: A New Frontier for Video Generation.\nhttps://runwayml.com/research/introducing-gen-3-alpha\n, jun 2024.\nLuma Labs [2024]\nLuma Labs.\nDream Machine: New Freedoms of Imagination.\nhttps://lumalabs.ai/dream-machine\n, June 2024.\nBrooks et al. [2024]\nTim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al.\nVideo generation models as world simulators.\nOpenAI Blog\n, 1(8):1, 2024.\nGoogle DeepMind [2025]\nGoogle DeepMind.\nGenie 3: A New Frontier for World Models.\nGoogle DeepMind Blog, aug 2025.\nURL\nhttps://deepmind.google/blog/genie-3-a-new-frontier-for-world-models/\n.\nDeepMind [2025]\nDeepMind.\nVeo-3: A text-to-video generation system with audio.\nTechnical Report Tech Report, DeepMind / Google, 2025.\nAccessed: YYYY-MM-DD.\nWan et al. [2025]\nTeam Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al.\nWan: Open and advanced large-scale video generative models.\narXiv preprint arXiv:2503.20314\n, 2025.\nAgarwal et al. [2025]\nNiket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al.\nCosmos world foundation model platform for physical ai.\narXiv preprint arXiv:2501.03575\n, 2025.\nJang et al. [2025]\nJoel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, et al.\nDreamgen: Unlocking generalization in robot learning through video world models.\narXiv preprint arXiv:2505.12705\n, 2025.\nLi et al. [2025b]\nHongyu Li, Lingfeng Sun, Yafei Hu, Duy Ta, Jennifer Barry, George Konidaris, and Jiahui Fu.\nNovaflow: Zero-shot manipulation via actionable flow from generated videos.\narXiv preprint arXiv:2510.08568\n, 2025b.\nDu et al. [2023a]\nYilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum, Leslie Kaelbling, Andy Zeng, and Jonathan Tompson.\nVideo language planning, 2023a.\nURL\nhttps://arxiv.org/abs/2310.10625\n.\nGao et al. [2024]\nChongkai Gao, Haozhuo Zhang, Zhixuan Xu, Zhehao Cai, and Lin Shao.\nFlip: Flow-centric generative planning as general-purpose manipulation world model.\narXiv preprint arXiv:2412.08261\n, 2024.\nQuevedo et al. [2025]\nJulian Quevedo, Ansh Kumar Sharma, Yixiang Sun, Varad Suryavanshi, Percy Liang, and Sherry Yang.\nWorldgym: World model as an environment for policy evaluation, 2025.\nURL\nhttps://arxiv.org/abs/2506.00613\n.\nGuo et al. [2025a]\nYanjiang Guo, Lucy Xiaoyang Shi, Jianyu Chen, and Chelsea Finn.\nCtrl-world: A controllable generative world model for robot manipulation.\narXiv preprint arXiv:2510.10125\n, 2025a.\nWiedemer et al. [2025]\nThaddÃ¤us Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos.\nVideo models are zero-shot learners and reasoners, 2025.\nURL\nhttps://arxiv.org/abs/2509.20328\n.\nDeepSeek-AI [2025]\nDeepSeek-AI.\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, 2025.\nURL\nhttps://arxiv.org/abs/2501.12948\n.\nOpenAI [2025]\nOpenAI.\nGPT-5 and the new era of work.\nhttps://openai.com/index/gpt-5-new-era-of-work/\n, August 2025.\nHassabis et al. [2025]\nDemis Hassabis, Koray Kavukcuoglu, and on behalf of the Gemini team.\nA new era of intelligence with Gemini 3.\nhttps://blog.google/products/gemini/gemini-3/\n, nov 2025.\nBlog Post.\nZitkovich et al. [2023]\nBrianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al.\nRt-2: Vision-language-action models transfer web knowledge to robotic control.\nIn\nConference on Robot Learning\n, pages 2165â€“2183. PMLR, 2023.\nIntelligence et al. [2025]\nPhysical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al.\nÏ€\n0.5\n\\pi_{0.5}\n: a vision-language-action model with open-world generalization.\narXiv preprint arXiv:2504.16054\n, 2025.\nTeam et al. [2025a]\nGemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al.\nGemini robotics: Bringing ai into the physical world.\narXiv preprint arXiv:2503.20020\n, 2025a.\nHuang et al. [2025a]\nSiqiao Huang, Jialong Wu, Qixing Zhou, Shangchen Miao, and Mingsheng Long.\nVid2world: Crafting video diffusion models to interactive world models.\narXiv preprint arXiv:2505.14357\n, 2025a.\nHuang et al. [2025b]\nSiyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Yue Liao, Peng Gao, Hongsheng Li, Maoqing Yao, et al.\nEnerverse: Envisioning embodied future space for robotics manipulation.\narXiv preprint arXiv:2501.01895\n, 2025b.\nLiao et al. [2025]\nYue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, et al.\nGenie envisioner: A unified world foundation platform for robotic manipulation.\narXiv preprint arXiv:2508.05635\n, 2025.\nBruce et al. [2024]\nJake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al.\nGenie: Generative interactive environments.\nIn\nForty-first International Conference on Machine Learning\n, 2024.\nTeam [2025]\n1X World Model Team.\n1x world model: Evaluating bits, not atoms.\nTechnical report, 1X, 2025.\nTeam et al. [2025b]\nGemini Robotics Team, Coline Devin, Yilun Du, Debidatta Dwibedi, Ruiqi Gao, Abhishek Jindal, Thomas Kipf, Sean Kirmani, Fangchen Liu, Anirudha Majumdar, Andrew Marmon, Carolina Parada, Yulia Rubanova, Dhruv Shah, Vikas Sindhwani, Jie Tan, Fei Xia, Ted Xiao, Sherry Yang, Wenhao Yu, and Allan Zhou.\nEvaluating Gemini robotics policies in a veo world simulator, 2025b.\nURL\nhttps://arxiv.org/abs/2512.10675\n.\nHafner et al. [2025]\nDanijar Hafner, Wilson Yan, and Timothy Lillicrap.\nTraining agents inside of scalable world models.\narXiv preprint arXiv:2509.24527\n, 2025.\nXiao et al. [2025]\nJunjin Xiao, Yandan Yang, Xinyuan Chang, Ronghan Chen, Feng Xiong, Mu Xu, Wei-Shi Zheng, and Qing Zhang.\nWorld-env: Leveraging world model as a virtual environment for vla post-training.\narXiv preprint arXiv:2509.24948\n, 2025.\nEscontrela et al. [2023]\nAlejandro Escontrela, Ademi Adeniji, Wilson Yan, Ajay Jain, Xue Bin Peng, Ken Goldberg, Youngwoon Lee, Danijar Hafner, and Pieter Abbeel.\nVideo prediction models as rewards for reinforcement learning.\nAdvances in Neural Information Processing Systems\n, 36:68760â€“68783, 2023.\nHuang et al. [2024a]\nTao Huang, Guangqi Jiang, Yanjie Ze, and Huazhe Xu.\nDiffusion reward: Learning rewards via conditional video diffusion.\nIn\nEuropean Conference on Computer Vision\n, pages 478â€“495. Springer, 2024a.\nGeneralist AI [2025]\nGeneralist AI.\nGEN-0 / Embodied Foundation Models That Scale with Physical Interaction.\nhttps://generalistai.com/blog/nov-04-2025-GEN-0\n, November 2025.\nAjay et al. [2023]\nAnurag Ajay, Seungwook Han, Yilun Du, Shuang Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum, Leslie Kaelbling, Akash Srivastava, and Pulkit Agrawal.\nCompositional foundation models for hierarchical planning.\nAdvances in Neural Information Processing Systems\n, 36:22304â€“22325, 2023.\nXu et al. [2025]\nZhengtong Xu, Qiang Qiu, and Yu She.\nVilp: Imitation learning with latent video planning.\nIEEE Robotics and Automation Letters\n, 2025.\nKo et al. [2023]\nPo-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, and Joshua B Tenenbaum.\nLearning to act from actionless videos through dense correspondences.\narXiv preprint arXiv:2310.08576\n, 2023.\nEbert et al. [2018]\nFrederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine.\nVisual foresight: Model-based deep reinforcement learning for vision-based robotic control.\narXiv preprint arXiv:1812.00568\n, 2018.\nBu et al. [2024]\nQingwen Bu, Jia Zeng, Li Chen, Yanchao Yang, Guyue Zhou, Junchi Yan, Ping Luo, Heming Cui, Yi Ma, and Hongyang Li.\nClosed-loop visuomotor control with generative expectation for robotic manipulation.\nAdvances in Neural Information Processing Systems\n, 37:139002â€“139029, 2024.\nChen et al. [2025a]\nBoyuan Chen, Tianyuan Zhang, Haoran Geng, Kiwhan Song, Caiyi Zhang, Peihao Li, William T. Freeman, Jitendra Malik, Pieter Abbeel, Russ Tedrake, Vincent Sitzmann, and Yilun Du.\nLarge video planner enables generalizable robot control, 2025a.\nURL\nhttps://arxiv.org/abs/2512.15840\n.\nPai et al. [2025a]\nJonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, and Elvis Nava.\nmimic-video: Video-action models for generalizable robot control beyond vlas.\narXiv preprint arXiv:2512.15692\n, 2025a.\nMotamed et al. [2025a]\nSaman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos.\nDo generative video models understand physical principles?\narXiv preprint arXiv:2501.09038\n, 2025a.\nBansal et al. [2024]\nHritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover.\nVideophy: Evaluating physical commonsense for video generation.\narXiv preprint arXiv:2406.03520\n, 2024.\nMeng et al. [2024]\nFanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo.\nTowards world simulator: Crafting physical commonsense-based benchmark for video generation.\narXiv preprint arXiv:2410.05363\n, 2024.\nMei et al. [2025a]\nZhiting Mei, Ola Shorinwa, and Anirudha Majumdar.\nHow confident are video models? Empowering video models to express their uncertainty.\narXiv preprint arXiv:2510.02571\n, 2025a.\nQin et al. [2024]\nYiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen Zhou, Lijun Li, Zhenfei Yin, Xihui Liu, Lu Sheng, Jing Shao, et al.\nWorldsimbench: Towards video generation models as world simulators.\narXiv preprint arXiv:2410.18072\n, 2024.\nLi et al. [2025c]\nDacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu Yin, Joseph E Gonzalez, et al.\nWorldmodelbench: Judging video generation models as world models.\narXiv preprint arXiv:2502.20694\n, 2025c.\nYu et al. [2025a]\nSihyun Yu, Meera Hahn, Dan Kondratyuk, Jinwoo Shin, Agrim Gupta, JosÃ© Lezama, Irfan Essa, David Ross, and Jonathan Huang.\nMalt diffusion: Memory-augmented latent transformers for any-length video generation.\narXiv preprint arXiv:2502.12632\n, 2025a.\nZhang and Agrawala [2025]\nLvmin Zhang and Maneesh Agrawala.\nPacking input frame context in next-frame prediction models for video generation.\narXiv preprint arXiv:2504.12626\n, 2025.\nGuo et al. [2025b]\nYuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang.\nLong context tuning for video generation.\narXiv preprint arXiv:2503.10589\n, 2025b.\nChen et al. [2024a]\nZhaorun Chen, Francesco Pinto, Minzhou Pan, and Bo Li.\nSafewatch: An efficient safety-policy following video guardrail model with transparent explanations, 2024a.\nURL\nhttps://arxiv.org/abs/2412.06878\n.\nMiao et al. [2024]\nYibo Miao, Yifan Zhu, Yinpeng Dong, Lijia Yu, Jun Zhu, and Xiao-Shan Gao.\nT2vsafetybench: Evaluating the safety of text-to-video generative models, 2024.\nURL\nhttps://arxiv.org/abs/2407.05965\n.\nMelnik et al. [2024]\nAndrew Melnik, Michal Ljubljanac, Cong Lu, Qi Yan, Weiming Ren, and Helge Ritter.\nVideo diffusion models: A survey.\narXiv preprint arXiv:2405.03150\n, 2024.\nYu et al. [2025b]\nJiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Hao Chen, and Xihui Liu.\nA survey of interactive generative video.\narXiv preprint arXiv:2504.21853\n, 2025b.\nMa et al. [2025]\nYue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, et al.\nControllable video generation: A survey.\narXiv preprint arXiv:2507.16869\n, 2025.\nXing et al. [2024a]\nZhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang.\nA survey on video diffusion models.\nACM Computing Surveys\n, 57(2):1â€“42, 2024a.\nLi et al. [2025d]\nXinqing Li, Xin He, Le Zhang, and Yun Liu.\nA comprehensive survey on world models for embodied ai.\narXiv preprint arXiv:2510.16732\n, 2025d.\nDing et al. [2025]\nJingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Zefang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li, Nicholas Sukiennik, et al.\nUnderstanding world or predicting future? a comprehensive survey of world models.\nACM Computing Surveys\n, 58(3):1â€“38, 2025.\nKong et al. [2025]\nLingdong Kong, Wesley Yang, Jianbiao Mei, Youquan Liu, Ao Liang, Dekai Zhu, Dongyue Lu, Wei Yin, Xiaotao Hu, Mingkai Jia, et al.\n3d and 4d world modeling: A survey.\narXiv preprint arXiv:2509.07996\n, 2025.\nZhang et al. [2025a]\nKun Zhang, Peng Yun, Jun Cen, Junhao Cai, Didi Zhu, Hangjie Yuan, Chao Zhao, Tao Feng, Michael Yu Wang, Qifeng Chen, et al.\nGenerative artificial intelligence in robotic manipulation: A survey.\narXiv preprint arXiv:2503.03464\n, 2025a.\nYang et al. [2024a]\nSherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans.\nVideo as the new language for real-world decision making, 2024a.\nURL\nhttps://arxiv.org/abs/2402.17139\n.\nYue et al. [2025a]\nJingtong Yue, Ziqi Huang, Zhaoxi Chen, Xintao Wang, Pengfei Wan, and Ziwei Liu.\nSimulating the visual world with artificial intelligence: A roadmap, 2025a.\nURL\nhttps://arxiv.org/abs/2511.08585\n.\nZhu et al. [2024a]\nZheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, et al.\nIs sora a world simulator? a comprehensive survey on general world models and beyond.\narXiv preprint arXiv:2405.03520\n, 2024a.\nGuan et al. [2024]\nYanchen Guan, Haicheng Liao, Zhenning Li, Jia Hu, Runze Yuan, Guohui Zhang, and Chengzhong Xu.\nWorld models for autonomous driving: An initial survey.\nIEEE Transactions on Intelligent Vehicles\n, 2024.\nFeng et al. [2025a]\nTuo Feng, Wenguan Wang, and Yi Yang.\nA survey of world models for autonomous driving.\narXiv preprint arXiv:2501.11260\n, 2025a.\nLi et al. [2025e]\nYingyan Li, Shuyao Shang, Weisong Liu, Bing Zhan, Haochen Wang, Yuqi Wang, Yuntao Chen, Xiaoman Wang, Yasong An, Chufeng Tang, et al.\nDrivevla-w0: World models amplify data scaling law in autonomous driving.\narXiv preprint arXiv:2510.12796\n, 2025e.\nHafner et al. [2023]\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap.\nMastering diverse domains through world models.\narXiv preprint arXiv:2301.04104\n, 2023.\nHansen et al. [2023]\nNicklas Hansen, Hao Su, and Xiaolong Wang.\nTd-mpc2: Scalable, robust world models for continuous control.\narXiv preprint arXiv:2310.16828\n, 2023.\nZhou et al. [2024]\nGaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto.\nDino-wm: World models on pre-trained visual features enable zero-shot planning.\narXiv preprint arXiv:2411.04983\n, 2024.\nVondrick et al. [2016]\nCarl Vondrick, Hamed Pirsiavash, and Antonio Torralba.\nGenerating videos with scene dynamics.\nAdvances in neural information processing systems\n, 29, 2016.\nBabaeizadeh et al. [2017]\nMohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine.\nStochastic variational video prediction.\narXiv preprint arXiv:1710.11252\n, 2017.\nWu et al. [2024]\nJialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long.\nivideogpt: Interactive videogpts are scalable world models.\nAdvances in Neural Information Processing Systems\n, 37:68082â€“68119, 2024.\nNakamura et al. [2025a]\nKensuke Nakamura, Lasse Peters, and Andrea Bajcsy.\nGeneralizing safety beyond collision-avoidance via latent-space reachability analysis.\narXiv preprint arXiv:2502.00935\n, 2025a.\nYin et al. [2025a]\nTenny Yin, Zhiting Mei, Tao Sun, Lihan Zha, Emily Zhou, Jeremy Bao, Miyu Yamane, Ola Sho, and Anirudha Majumdar.\nWomap: World models for embodied open-vocabulary object localization.\nIn\nRSS 2025 Workshop: Mobile Manipulation: Emerging Opportunities\n{\n\\{\n\\\n\\backslash\n&\n}\n\\}\nContemporary Challenges\n, 2025a.\nAssran et al. [2023]\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas.\nSelf-supervised learning from images with a joint-embedding predictive architecture.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 15619â€“15629, 2023.\nBardes et al. [2024]\nAdrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas.\nRevisiting feature prediction for learning visual representations from video.\narXiv preprint arXiv:2404.08471\n, 2024.\nGarrido et al. [2025]\nQuentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael Rabbat, Emmanuel Dupoux, and Yann LeCun.\nIntuitive physics understanding emerges from self-supervised pretraining on natural videos, 2025.\nURL\nhttps://arxiv.org/abs/2502.11831\n.\nAssran et al. [2025]\nMido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al.\nV-jepa 2: Self-supervised video models enable understanding, prediction and planning.\narXiv preprint arXiv:2506.09985\n, 2025.\nHo et al. [2022a]\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.\nVideo diffusion models.\nAdvances in neural information processing systems\n, 35:8633â€“8646, 2022a.\nBlattmann et al. [2023a]\nAndreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach.\nStable video diffusion: Scaling latent video diffusion models to large datasets, 2023a.\nURL\nhttps://arxiv.org/abs/2311.15127\n.\nPatel et al. [2025]\nShivansh Patel, Shraddhaa Mohan, Hanlin Mai, Unnat Jain, Svetlana Lazebnik, and Yunzhu Li.\nRobotic manipulation by imitating generated videos without physical demonstrations.\narXiv preprint arXiv:2507.00990\n, 2025.\nQiu et al. [2025]\nXiaowen Qiu, Yian Wang, Jiting Cai, Zhehuan Chen, Chunru Lin, Tsun-Hsuan Wang, and Chuang Gan.\nLucibot: Automated robot policy learning from generated videos.\narXiv preprint arXiv:2503.09871\n, 2025.\nBharadhwaj et al. [2024]\nHomanga Bharadhwaj, Debidatta Dwibedi, Abhinav Gupta, Shubham Tulsiani, Carl Doersch, Ted Xiao, Dhruv Shah, Fei Xia, Dorsa Sadigh, and Sean Kirmani.\nGen2act: Human video generation in novel scenarios enables generalizable robot manipulation.\narXiv preprint arXiv:2409.16283\n, 2024.\nSoni et al. [2024]\nAchint Soni, Sreyas Venkataraman, Abhranil Chandra, Sebastian Fischmeister, Percy Liang, Bo Dai, and Sherry Yang.\nVideoagent: Self-improving video generation.\narXiv preprint arXiv:2410.10076\n, 2024.\nLiang et al. [2024]\nJunbang Liang, Ruoshi Liu, Ege Ozguroglu, Sruthi Sudhakar, Achal Dave, Pavel Tokmakov, Shuran Song, and Carl Vondrick.\nDreamitate: Real-world visuomotor policy learning via video generation.\narXiv preprint arXiv:2406.16862\n, 2024.\nLi et al. [2025f]\nShuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran Song.\nUnified video action model.\narXiv preprint arXiv:2503.00200\n, 2025f.\nZhang et al. [2025b]\nWenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, Fan Lu, He Wang, et al.\nDreamvla: a vision-language-action model dreamed with comprehensive world knowledge.\narXiv preprint arXiv:2507.04447\n, 2025b.\nJiang et al. [2025a]\nYuxin Jiang, Shengcong Chen, Siyuan Huang, Liliang Chen, Pengfei Zhou, Yue Liao, Xindong He, Chiming Liu, Hongsheng Li, Maoqing Yao, et al.\nEnerverse-ac: Envisioning embodied environments with action condition.\narXiv preprint arXiv:2505.09723\n, 2025a.\nLuo and Du [2024]\nYunhao Luo and Yilun Du.\nGrounding video models to actions through goal conditioned exploration.\narXiv preprint arXiv:2411.07223\n, 2024.\nLi et al. [2025g]\nYaxuan Li, Yichen Zhu, Junjie Wen, Chaomin Shen, and Yi Xu.\nWorldeval: World model as real-world robot policies evaluator.\narXiv preprint arXiv:2505.19017\n, 2025g.\nZhu et al. [2024b]\nFangqi Zhu, Hongtao Wu, Song Guo, Yuxiao Liu, Chilam Cheang, and Tao Kong.\nIrasim: Learning interactive real-robot action simulators.\narXiv preprint arXiv:2406.14540\n, 2024b.\nTseng et al. [2025]\nWei-Cheng Tseng, Jinwei Gu, Qinsheng Zhang, Hanzi Mao, Ming-Yu Liu, Florian Shkurti, and Lin Yen-Chen.\nScalable policy evaluation with video world models.\narXiv preprint arXiv:2511.11520\n, 2025.\nYang et al. [2025a]\nYuncong Yang, Jiageng Liu, Zheyuan Zhang, Siyuan Zhou, Reuben Tan, Jianwei Yang, Yilun Du, and Chuang Gan.\nMindjourney: Test-time scaling with world models for spatial reasoning.\narXiv preprint arXiv:2507.12508\n, 2025a.\nCen et al. [2024]\nJun Cen, Chenfei Wu, Xiao Liu, Shengming Yin, Yixuan Pei, Jinglong Yang, Qifeng Chen, Nan Duan, and Jianguo Zhang.\nUsing left and right brains together: Towards vision and language planning.\narXiv preprint arXiv:2402.10534\n, 2024.\nBlack et al. [2023]\nKevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine.\nZero-shot robotic manipulation with pretrained image-editing diffusion models.\narXiv preprint arXiv:2310.10639\n, 2023.\nHorÃ© and Ziou [2010]\nAlain HorÃ© and Djemel Ziou.\nImage quality metrics: Psnr vs. ssim.\nIn\n2010 20th International Conference on Pattern Recognition\n, pages 2366â€“2369, 2010.\n10.1109/ICPR.2010.579\n.\nWang et al. [2004]\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli.\nImage quality assessment: from error visibility to structural similarity.\nIEEE transactions on image processing\n, 13(4):600â€“612, 2004.\nZhang et al. [2018]\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.\nThe unreasonable effectiveness of deep features as a perceptual metric.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n, pages 586â€“595, 2018.\nRadford et al. [2021]\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\nLearning transferable visual models from natural language supervision.\nIn\nInternational conference on machine learning\n, pages 8748â€“8763. PmLR, 2021.\nUnterthiner et al. [2018]\nThomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly.\nTowards accurate generative models of video: A new metric & challenges.\narXiv preprint arXiv:1812.01717\n, 2018.\nGibson [1951]\nJames J Gibson.\nThe perception of the visual world.\nThe American Journal of Psychology\n, 64(3):440, 1951.\nLiu et al. [2024a]\nYaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan.\nEvalcrafter: Benchmarking and evaluating large video generation models.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 22139â€“22149, 2024a.\nHuang et al. [2024b]\nZiqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al.\nVbench: Comprehensive benchmark suite for video generative models.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 21807â€“21818, 2024b.\nSun et al. [2025]\nKaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu.\nT2v-compbench: A comprehensive benchmark for compositional text-to-video generation.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 8406â€“8416, 2025.\nTian et al. [2023]\nStephen Tian, Chelsea Finn, and Jiajun Wu.\nA control-centric benchmark for video prediction.\narXiv preprint arXiv:2304.13723\n, 2023.\nLin et al. [2025]\nMinghui Lin, Xiang Wang, Yishan Wang, Shu Wang, Fengqi Dai, Pengxiang Ding, Cunxiang Wang, Zhengrong Zuo, Nong Sang, Siteng Huang, et al.\nExploring the evolution of physics cognition in video generation: A survey.\narXiv preprint arXiv:2503.21765\n, 2025.\nKang et al. [2024]\nBingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng.\nHow far is video generation from world model: A physical law perspective.\narXiv preprint arXiv:2411.02385\n, 2024.\nMotamed et al. [2025b]\nSaman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos.\nDo generative video models understand physical principles?, 2025b.\nURL\nhttps://arxiv.org/abs/2501.09038\n.\nZhang et al. [2025c]\nKe Zhang, Cihan Xiao, Yiqun Mei, Jiacong Xu, and Vishal M. Patel.\nThink before you diffuse: Llms-guided physics-aware video generation, 2025c.\nURL\nhttps://arxiv.org/abs/2505.21653\n.\nMei et al. [2025b]\nZhiting Mei, Tenny Yin, Micah Baker, Ola Shorinwa, and Anirudha Majumdar.\nWorld models that know when they donâ€™t know: Controllable video generation with calibrated uncertainty.\narXiv preprint arXiv:2512.05927\n, 2025b.\nFang et al. [2024]\nYuwei Fang, Willi Menapace, Aliaksandr Siarohin, Tsai-Shien Chen, Kuan-Chieh Wang, Ivan Skorokhodov, Graham Neubig, and Sergey Tulyakov.\nVimi: Grounding video generation through multi-modal instruction.\nIn\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\n, pages 4444â€“4456, 2024.\nXing et al. [2025a]\nZhen Xing, Qi Dai, Zejia Weng, Zuxuan Wu, and Yu-Gang Jiang.\nAid: Adapting image2video diffusion models for instruction-guided video prediction.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, pages 21243â€“21253, 2025a.\nLi et al. [2025h]\nJialu Li, Shoubin Yu, Han Lin, Jaemin Cho, Jaehong Yoon, and Mohit Bansal.\nTraining-free guidance in text-to-video generation via multimodal planning and structured noise initialization, 2025h.\nURL\nhttps://arxiv.org/abs/2504.08641\n.\nZhang et al. [2024a]\nYiyuan Zhang, Yuhao Kang, Zhixin Zhang, Xiaohan Ding, Sanyuan Zhao, and Xiangyu Yue.\nInteractivevideo: User-centric controllable video generation with synergistic multimodal instructions, 2024a.\nURL\nhttps://arxiv.org/abs/2402.03040\n.\nWang et al. [2025a]\nAngtian Wang, Haibin Huang, Jacob Zhiyuan Fang, Yiding Yang, and Chongyang Ma.\nAti: Any trajectory instruction for controllable video generation, 2025a.\nURL\nhttps://arxiv.org/abs/2505.22944\n.\nYuan et al. [2024]\nHangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, and Dong Ni.\nInstructvideo: Instructing video diffusion models with human feedback.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 6463â€“6474, 2024.\nWang et al. [2025b]\nBoyang Wang, Nikhil Sridhar, Chao Feng, Mark Van der Merwe, Adam Fishman, Nima Fazeli, and Jeong Joon Park.\nThis&that: Language-gesture controlled video generation for robot planning.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 12842â€“12849. IEEE, 2025b.\nYoon et al. [2025]\nJaehong Yoon, Shoubin Yu, Vaidehi Patil, Huaxiu Yao, and Mohit Bansal.\nSafree: Training-free and adaptive guard for safe text-to-image and video generation, 2025.\nURL\nhttps://arxiv.org/abs/2410.12761\n.\nNakamura et al. [2025b]\nKensuke Nakamura, Lasse Peters, and Andrea Bajcsy.\nGeneralizing safety beyond collision-avoidance via latent-space reachability analysis, 2025b.\nURL\nhttps://arxiv.org/abs/2502.00935\n.\nSeo et al. [2025]\nJunwon Seo, Kensuke Nakamura, and Andrea Bajcsy.\nUncertainty-aware latent safety filters for avoiding out-of-distribution failures, 2025.\nURL\nhttps://arxiv.org/abs/2505.00779\n.\nYe et al. [2024]\nSeonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al.\nLatent action pretraining from videos.\nIn\nThe Thirteenth International Conference on Learning Representations\n, 2024.\nChen et al. [2025b]\nXiaoyu Chen, Hangxing Wei, Pushi Zhang, Chuheng Zhang, Kaixin Wang, Yanjiang Guo, Rushuai Yang, Yucen Wang, Xinquan Xiao, Li Zhao, et al.\nVilla-x: enhancing latent action modeling in vision-language-action models.\narXiv preprint arXiv:2507.23682\n, 2025b.\nPathak et al. [2018]\nDeepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell.\nZero-shot visual imitation.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition workshops\n, pages 2050â€“2053, 2018.\nTan et al. [2025]\nHengkai Tan, Yao Feng, Xinyi Mao, Shuhe Huang, Guodong Liu, Zhongkai Hao, Hang Su, and Jun Zhu.\nAnypos: Automated task-agnostic actions for bimanual manipulation.\narXiv preprint arXiv:2507.12768\n, 2025.\nChen et al. [2024b]\nBoyuan Chen, Diego MartÃ­ MonsÃ³, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann.\nDiffusion forcing: Next-token prediction meets full-sequence diffusion.\nIn A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors,\nAdvances in Neural Information Processing Systems\n, volume 37, pages 24081â€“24125. Curran Associates, Inc., 2024b.\nURL\nhttps://proceedings.neurips.cc/paper_files/paper/2024/file/2aee1c4159e48407d68fe16ae8e6e49e-Paper-Conference.pdf\n.\nDalal et al. [2025]\nKaran Dalal, Daniel Koceja, Jiarui Xu, Yue Zhao, Shihao Han, Ka Chun Cheung, Jan Kautz, Yejin Choi, Yu Sun, and Xiaolong Wang.\nOne-minute video generation with test-time training.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 17702â€“17711, 2025.\nZhang et al. [2025d]\nTianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T Freeman, and Hao Tan.\nTest-time training done right.\narXiv preprint arXiv:2505.23884\n, 2025d.\nYang et al. [2024b]\nZhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al.\nCogvideox: Text-to-video diffusion models with an expert transformer.\narXiv preprint arXiv:2408.06072\n, 2024b.\nChen et al. [2024c]\nTsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al.\nPanda-70m: Captioning 70m videos with multiple cross-modality teachers.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 13320â€“13331, 2024c.\nTan et al. [2024]\nZhiyu Tan, Xiaomeng Yang, Luozheng Qin, and Hao Li.\nVidgen-1m: A large-scale dataset for text-to-video generation.\narXiv preprint arXiv:2408.02629\n, 2024.\nNan et al. [2024]\nKepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai.\nOpenvid-1m: A large-scale high-quality dataset for text-to-video generation.\narXiv preprint arXiv:2407.02371\n, 2024.\nWang et al. [2025c]\nQiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al.\nKoala-36m: A large-scale video dataset improving consistency between fine-grained conditions and video content.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 8428â€“8437, 2025c.\nKo et al. [2025]\nPo-Chen Ko, Jiayuan Mao, Yu-Hsiang Fu, Hsien-Jeng Yeh, Chu-Rong Chen, Wei-Chiu Ma, Yilun Du, and Shao-Hua Sun.\nImplicit state estimation via video replanning, 2025.\nURL\nhttps://arxiv.org/abs/2510.17315\n.\nBlattmann et al. [2023b]\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with latent diffusion models.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pages 22563â€“22575, 2023b.\nPeng et al. [2025]\nXiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, et al.\nOpen-sora 2.0: Training a commercial-level video generation model in 200 k.\narXiv preprint arXiv:2503.09642\n, 2025.\nFrans et al. [2024]\nKevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel.\nOne step diffusion via shortcut models.\narXiv preprint arXiv:2410.12557\n, 2024.\nSong et al. [2023]\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.\nConsistency models.\nIn\nInternational Conference on Machine Learning\n, pages 32211â€“32252. PMLR, 2023.\nRajeswaran et al. [2017]\nAravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine.\nLearning complex dexterous manipulation with deep reinforcement learning and demonstrations.\narXiv preprint arXiv:1709.10087\n, 2017.\nMatas et al. [2018]\nJan Matas, Stephen James, and Andrew J Davison.\nSim-to-real reinforcement learning for deformable object manipulation.\nIn\nConference on Robot Learning\n, pages 734â€“743. PMLR, 2018.\nTodorov et al. [2012]\nEmanuel Todorov, Tom Erez, and Yuval Tassa.\nMujoco: A physics engine for model-based control.\nIn\n2012 IEEE/RSJ international conference on intelligent robots and systems\n, pages 5026â€“5033. IEEE, 2012.\nCoumans and Bai [2016]\nErwin Coumans and Yunfei Bai.\nPybullet, a python module for physics simulation for games, robotics and machine learning, 2016.\nHa and Schmidhuber [2018a]\nDavid Ha and JÃ¼rgen Schmidhuber.\nRecurrent world models facilitate policy evolution.\nAdvances in neural information processing systems\n, 31, 2018a.\nHa and Schmidhuber [2018b]\nDavid Ha and JÃ¼rgen Schmidhuber.\nWorld models.\narXiv preprint arXiv:1803.10122\n, 2(3), 2018b.\nHafner et al. [2020a]\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi.\nDream to control: Learning behaviors by latent imagination, 2020a.\nURL\nhttps://arxiv.org/abs/1912.01603\n.\nHafner et al. [2020b]\nDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba.\nMastering atari with discrete world models.\narXiv preprint arXiv:2010.02193\n, 2020b.\nMendonca et al. [2023]\nRussell Mendonca, Shikhar Bahl, and Deepak Pathak.\nStructured world models from human videos.\narXiv preprint arXiv:2308.10901\n, 2023.\nWu et al. [2023a]\nJialong Wu, Haoyu Ma, Chaoyi Deng, and Mingsheng Long.\nPre-training contextualized world models with in-the-wild videos for reinforcement learning.\nAdvances in Neural Information Processing Systems\n, 36:39719â€“39743, 2023a.\nAlonso et al. [2024]\nEloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos J Storkey, Tim Pearce, and FranÃ§ois Fleuret.\nDiffusion for world modeling: Visual details matter in atari.\nAdvances in Neural Information Processing Systems\n, 37:58757â€“58791, 2024.\nYang et al. [2023]\nMengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel.\nLearning interactive real-world simulators.\narXiv preprint arXiv:2310.06114\n, 1(2):6, 2023.\nDing et al. [2024]\nZihan Ding, Amy Zhang, Yuandong Tian, and Qinqing Zheng.\nDiffusion world model: Future modeling beyond step-by-step rollout for offline reinforcement learning.\narXiv preprint arXiv:2402.03570\n, 2024.\nHuang et al. [2025c]\nYuhang Huang, Jiazhao Zhang, Shilong Zou, XInwang Liu, Ruizhen Hu, and Kai Xu.\nLadi-wm: A latent diffusion-based world model for predictive manipulation.\narXiv preprint arXiv:2505.11528\n, 2025c.\nBar et al. [2025]\nAmir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun.\nNavigation world models.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 15791â€“15801, 2025.\nHafner et al. [2019]\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi.\nDream to control: Learning behaviors by latent imagination.\narXiv preprint arXiv:1912.01603\n, 2019.\nWu et al. [2023b]\nPhilipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg.\nDaydreamer: World models for physical robot learning.\nIn\nConference on robot learning\n, pages 2226â€“2240. PMLR, 2023b.\nChandra et al. [2025]\nAkshay L Chandra, Iman Nematollahi, Chenguang Huang, Tim Welschehold, Wolfram Burgard, and Abhinav Valada.\nDiwa: Diffusion policy adaptation with world models.\narXiv preprint arXiv:2508.03645\n, 2025.\nFinn et al. [2016]\nChelsea Finn, Ian Goodfellow, and Sergey Levine.\nUnsupervised learning for physical interaction through video prediction.\nAdvances in neural information processing systems\n, 29, 2016.\nJia et al. [2016]\nXu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool.\nDynamic filter networks.\nAdvances in neural information processing systems\n, 29, 2016.\nClark et al. [2019]\nAidan Clark, Jeff Donahue, and Karen Simonyan.\nAdversarial video generation on complex datasets.\narXiv preprint arXiv:1907.06571\n, 2019.\nGoodfellow et al. [2014]\nIan J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.\nGenerative adversarial nets.\nAdvances in neural information processing systems\n, 27, 2014.\nLee et al. [2018]\nAlex X Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn, and Sergey Levine.\nStochastic adversarial video prediction.\narXiv preprint arXiv:1804.01523\n, 2018.\nWalker et al. [2021]\nJacob Walker, Ali Razavi, and AÃ¤ron van den Oord.\nPredicting video with vqvae.\narXiv preprint arXiv:2103.01950\n, 2021.\nBabaeizadeh et al. [2021]\nMohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn, and Dumitru Erhan.\nFitvid: Overfitting in pixel-level video prediction.\narXiv preprint arXiv:2106.13195\n, 2021.\nKingma and Welling [2013]\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint arXiv:1312.6114\n, 2013.\nHochreiter and Schmidhuber [1997]\nSepp Hochreiter and JÃ¼rgen Schmidhuber.\nLong short-term memory.\nNeural computation\n, 9(8):1735â€“1780, 1997.\nWeissenborn et al. [2019]\nDirk Weissenborn, Oscar TÃ¤ckstrÃ¶m, and Jakob Uszkoreit.\nScaling autoregressive video models.\narXiv preprint arXiv:1906.02634\n, 2019.\nVan Den Oord et al. [2017]\nAaron Van Den Oord, Oriol Vinyals, et al.\nNeural discrete representation learning.\nAdvances in neural information processing systems\n, 30, 2017.\nYan et al. [2021]\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas.\nVideogpt: Video generation using vq-vae and transformers, 2021.\nURL\nhttps://arxiv.org/abs/2104.10157\n.\nDhariwal and Nichol [2021]\nPrafulla Dhariwal and Alexander Nichol.\nDiffusion models beat gans on image synthesis.\nAdvances in neural information processing systems\n, 34:8780â€“8794, 2021.\nKong et al. [2020]\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro.\nDiffwave: A versatile diffusion model for audio synthesis.\narXiv preprint arXiv:2009.09761\n, 2020.\nSong et al. [2022]\nJiaming Song, Chenlin Meng, and Stefano Ermon.\nDenoising diffusion implicit models, 2022.\nURL\nhttps://arxiv.org/abs/2010.02502\n.\nSohl-Dickstein et al. [2015]\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.\nDeep unsupervised learning using nonequilibrium thermodynamics.\nIn\nInternational conference on machine learning\n, pages 2256â€“2265. pmlr, 2015.\nSalimans and Ho [2022]\nTim Salimans and Jonathan Ho.\nProgressive distillation for fast sampling of diffusion models, 2022.\nURL\nhttps://arxiv.org/abs/2202.00512\n.\nHo and Salimans [2022]\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598\n, 2022.\nXing et al. [2023]\nJinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin Wong, and Ying Shan.\nDynamicrafter: Animating open-domain images with video diffusion priors, 2023.\nURL\nhttps://arxiv.org/abs/2310.12190\n.\nYin et al. [2023a]\nShengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan.\nDragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory, 2023a.\nURL\nhttps://arxiv.org/abs/2308.08089\n.\nWang et al. [2024a]\nZhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan.\nMotionctrl: A unified and flexible motion controller for video generation, 2024a.\nURL\nhttps://arxiv.org/abs/2312.03641\n.\nGuo et al. [2024a]\nYuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai.\nAnimatediff: Animate your personalized text-to-image diffusion models without specific tuning, 2024a.\nURL\nhttps://arxiv.org/abs/2307.04725\n.\nRombach et al. [2022]\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer.\nHigh-resolution image synthesis with latent diffusion models.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pages 10684â€“10695, 2022.\nKong et al. [2024]\nWeijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al.\nHunyuanvideo: A systematic framework for large video generative models.\narXiv preprint arXiv:2412.03603\n, 2024.\nBaldassarre et al. [2025]\nFederico Baldassarre, Marc Szafraniec, Basile Terver, Vasil Khalidov, Francisco Massa, Yann LeCun, Patrick Labatut, Maximilian Seitzer, and Piotr Bojanowski.\nBack to the features: Dino as a foundation for video world models.\narXiv preprint arXiv:2507.19468\n, 2025.\nRonneberger et al. [2015]\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-net: Convolutional networks for biomedical image segmentation.\nIn\nInternational Conference on Medical image computing and computer-assisted intervention\n, pages 234â€“241. Springer, 2015.\nÃ‡iÃ§ek et al. [2016]\nÃ–zgÃ¼n Ã‡iÃ§ek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger.\n3d u-net: learning dense volumetric segmentation from sparse annotation.\nIn\nInternational conference on medical image computing and computer-assisted intervention\n, pages 424â€“432. Springer, 2016.\nPeebles and Xie [2023]\nWilliam Peebles and Saining Xie.\nScalable diffusion models with transformers.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n, pages 4195â€“4205, 2023.\nChen et al. [2024d]\nShoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua.\nGentron: Diffusion transformers for image and video generation.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 6441â€“6451, 2024d.\nWu et al. [2023c]\nJay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou.\nTune-a-video: One-shot tuning of image diffusion models for text-to-video generation.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n, pages 7623â€“7633, 2023c.\nLiu et al. [2024b]\nBingyan Liu, Chengyu Wang, Tingfeng Cao, Kui Jia, and Jun Huang.\nTowards understanding cross and self-attention in stable diffusion for text-guided image editing, 2024b.\nURL\nhttps://arxiv.org/abs/2403.03431\n.\nPerez et al. [2017]\nEthan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville.\nFilm: Visual reasoning with a general conditioning layer, 2017.\nURL\nhttps://arxiv.org/abs/1709.07871\n.\nXu et al. [2019]\nJingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin.\nUnderstanding and improving layer normalization, 2019.\nURL\nhttps://arxiv.org/abs/1911.07013\n.\nHo et al. [2022b]\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al.\nImagen video: High definition video generation with diffusion models.\narXiv preprint arXiv:2210.02303\n, 2022b.\nSinger et al. [2022]\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al.\nMake-a-video: Text-to-video generation without text-video data.\narXiv preprint arXiv:2209.14792\n, 2022.\nGillman et al. [2025]\nNate Gillman, Charles Herrmann, Michael Freeman, Daksh Aggarwal, Evan Luo, Deqing Sun, and Chen Sun.\nForce prompting: Video generation models can learn and generalize physics-based control signals, 2025.\nURL\nhttps://arxiv.org/abs/2505.19386\n.\nZhang et al. [2023]\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala.\nAdding conditional control to text-to-image diffusion models, 2023.\nURL\nhttps://arxiv.org/abs/2302.05543\n.\nKim et al. [2025]\nSeungwook Kim, Seunghyeon Lee, and Minsu Cho.\nFreeaction: Training-free techniques for enhanced fidelity of trajectory-to-video generation.\narXiv preprint arXiv:2509.24241\n, 2025.\nChen et al. [2020]\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.\nA simple framework for contrastive learning of visual representations, 2020.\nURL\nhttps://arxiv.org/abs/2002.05709\n.\nGrill et al. [2020]\nJean-Bastien Grill, Florian Strub, Florent AltchÃ©, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, RÃ©mi Munos, and Michal Valko.\nBootstrap your own latent: A new approach to self-supervised learning, 2020.\nURL\nhttps://arxiv.org/abs/2006.07733\n.\nCaron et al. [2021]\nMathilde Caron, Hugo Touvron, Ishan Misra, HervÃ© JÃ©gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.\nEmerging properties in self-supervised vision transformers.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n, pages 9650â€“9660, 2021.\nOquab et al. [2024]\nMaxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, HervÃ© Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.\nDinov2: Learning robust visual features without supervision, 2024.\nURL\nhttps://arxiv.org/abs/2304.07193\n.\nEgbe et al. [2025]\nThankGod Egbe, Peng Wang, Zhihao Guo, and Zidong Chen.\nDinov3-diffusion policy: Self-supervised large visual model for visuomotor diffusion policy learning, 2025.\nURL\nhttps://arxiv.org/abs/2509.17684\n.\nXing et al. [2025b]\nEric Xing, Mingkai Deng, Jinyu Hou, and Zhiting Hu.\nCritiques of world models, 2025b.\nURL\nhttps://arxiv.org/abs/2507.05169\n.\nBalestriero and LeCun [2025]\nRandall Balestriero and Yann LeCun.\nLejepa: Provable and scalable self-supervised learning without the heuristics, 2025.\nURL\nhttps://arxiv.org/abs/2511.08544\n.\nBardes et al. [2022]\nAdrien Bardes, Jean Ponce, and Yann LeCun.\nVicreg: Variance-invariance-covariance regularization for self-supervised learning, 2022.\nURL\nhttps://arxiv.org/abs/2105.04906\n.\nXiang et al. [2024]\nJiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo Hao, Yemin Shi, et al.\nPandora: Towards general world model with natural language actions and video states.\narXiv preprint arXiv:2406.09455\n, 2024.\nTeam et al. [2025c]\nAether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, et al.\nAether: Geometric-aware unified world modeling.\narXiv preprint arXiv:2503.18945\n, 2025c.\nQian et al. [2025]\nZezhong Qian, Xiaowei Chi, Yuming Li, Shizun Wang, Zhiyuan Qin, Xiaozhu Ju, Sirui Han, and Shanghang Zhang.\nWristworld: Generating wrist-views via 4d world models for robotic manipulation.\narXiv preprint arXiv:2510.07313\n, 2025.\nLuo et al. [2025]\nCalvin Luo, Zilai Zeng, Yilun Du, and Chen Sun.\nSolving new tasks by adapting internet video knowledge.\narXiv preprint arXiv:2504.15369\n, 2025.\nZhao et al. [2025a]\nHongxiang Zhao, Xingchen Liu, Mutian Xu, Yiming Hao, Weikai Chen, and Xiaoguang Han.\nTaste-rob: Advancing video generation of task-oriented hand-object interaction for generalizable robotic manipulation.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 27683â€“27693, 2025a.\nEbert et al. [2021]\nFrederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine.\nBridge data: Boosting generalization of robotic skills with cross-domain datasets.\narXiv preprint arXiv:2109.13396\n, 2021.\nKhazatsky et al. [2024]\nAlexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al.\nDroid: A large-scale in-the-wild robot manipulation dataset.\narXiv preprint arXiv:2403.12945\n, 2024.\nXing et al. [2024b]\nJinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong.\nDynamicrafter: Animating open-domain images with video diffusion priors.\nIn\nEuropean Conference on Computer Vision\n, pages 399â€“417. Springer, 2024b.\nVillar-Corrales and Behnke [2025]\nAngel Villar-Corrales and Sven Behnke.\nPlayslot: Learning inverse latent dynamics for controllable object-centric video prediction and planning.\narXiv preprint arXiv:2502.07600\n, 2025.\nKerbl et al. [2023]\nBernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis.\n3d gaussian splatting for real-time radiance field rendering.\nACM Trans. Graph.\n, 42(4):139â€“1, 2023.\nZhou et al. [2025a]\nSiyuan Zhou, Yilun Du, Yuncong Yang, Lei Han, Peihao Chen, Dit-Yan Yeung, and Chuang Gan.\nLearning 3d persistent embodied world models.\narXiv preprint arXiv:2505.05495\n, 2025a.\nLee et al. [2025]\nJason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, et al.\nMolmoact: Action reasoning models that can reason in space.\narXiv preprint arXiv:2508.07917\n, 2025.\nHancock et al. [2025]\nAsher J Hancock, Xindi Wu, Lihan Zha, Olga Russakovsky, and Anirudha Majumdar.\nActions as language: Fine-tuning vlms into vlas without catastrophic forgetting.\narXiv preprint arXiv:2509.22195\n, 2025.\nXie et al. [2025]\nSicheng Xie, Haidong Cao, Zejia Weng, Zhen Xing, Haoran Chen, Shiwei Shen, Jiaqi Leng, Zuxuan Wu, and Yu-Gang Jiang.\nHuman2robot: Learning robot actions from paired human-robot videos.\narXiv preprint arXiv:2502.16587\n, 2025.\nFeng et al. [2025b]\nYao Feng, Hengkai Tan, Xinyi Mao, Chendong Xiang, Guodong Liu, Shuhe Huang, Hang Su, and Jun Zhu.\nVidar: Embodied video diffusion model for generalist manipulation.\narXiv preprint arXiv:2507.12898\n, 2025b.\nYang et al. [2025b]\nLiudi Yang, Yang Bai, George Eskandar, Fengyi Shen, Mohammad Altillawi, Dong Chen, Soumajit Majumder, Ziyuan Liu, Gitta Kutyniok, and Abhinav Valada.\nRoboenvision: A long-horizon video generation model for multi-task robot manipulation.\narXiv preprint arXiv:2506.22007\n, 2025b.\nNVIDIA Research [2025]\nNVIDIA Research.\nWorld Simulation With Video Foundation Models for Physical AI.\nNVIDIA Research Publication\n, September 2025.\nWang et al. [2024b]\nZhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan.\nMotionctrl: A unified and flexible motion controller for video generation.\nIn\nACM SIGGRAPH 2024 Conference Papers\n, pages 1â€“11, 2024b.\nZhang et al. [2025e]\nZhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang.\nTora: Trajectory-oriented diffusion transformer for video generation.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 2063â€“2073, 2025e.\nFu et al. [2025]\nXiao Fu, Xintao Wang, Xian Liu, Jianhong Bai, Runsen Xu, Pengfei Wan, Di Zhang, and Dahua Lin.\nLearning video generation for robotic manipulation with collaborative trajectory control.\narXiv preprint arXiv:2506.01943\n, 2025.\nZhang et al. [2025f]\nHongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, and Donglin Wang.\nGevrm: Goal-expressive video generation model for robust visual manipulation.\narXiv preprint arXiv:2502.09268\n, 2025f.\nSchmidt and Jiang [2024]\nDominik Schmidt and Minqi Jiang.\nLearning to act without actions.\nIn\nThe Twelfth International Conference on Learning Representations (ICLR)\n, 2024.\nAgrawal et al. [2016]\nPulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine.\nLearning to poke by poking: Experiential learning of intuitive physics.\nAdvances in neural information processing systems\n, 29, 2016.\nHu et al. [2024]\nYucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen.\nVideo prediction policy: A generalist robot policy with predictive visual representations.\narXiv preprint arXiv:2412.14803\n, 2024.\nHuang et al. [2024c]\nShuaiyi Huang, Mara Levy, Zhenyu Jiang, Anima Anandkumar, Yuke Zhu, Linxi Fan, De-An Huang, and Abhinav Shrivastava.\nArdup: Active region video diffusion for universal policies.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, pages 8465â€“8472. IEEE, 2024c.\nWen et al. [2024]\nBowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield.\nFoundationpose: Unified 6d pose estimation and tracking of novel objects.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 17868â€“17879, 2024.\nLabbÃ© et al. [2022]\nYann LabbÃ©, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier, Mathieu Aubry, Dieter Fox, and Josef Sivic.\nMegapose: 6d pose estimation of novel objects via render & compare.\narXiv preprint arXiv:2212.06870\n, 2022.\nKe et al. [2025]\nBingxin Ke, Dominik Narnhofer, Shengyu Huang, Lei Ke, Torben Peters, Katerina Fragkiadaki, Anton Obukhov, and Konrad Schindler.\nVideo depth without video models.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 7233â€“7243, 2025.\nRadosavovic et al. [2023]\nIlija Radosavovic, Baifeng Shi, Letian Fu, Ken Goldberg, Trevor Darrell, and Jitendra Malik.\nRobot learning with sensorimotor pre-training.\nIn\nConference on Robot Learning\n, pages 683â€“693. PMLR, 2023.\nCheang et al. [2024]\nChi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al.\nGr-2: A generative video-language-action model with web-scale knowledge for robot manipulation.\narXiv preprint arXiv:2410.06158\n, 2024.\nZhu et al. [2025]\nChuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, and Abhishek Gupta.\nUnified world models: Coupling video and action diffusion for pretraining on large robotic datasets.\narXiv preprint arXiv:2504.02792\n, 2025.\nLiang et al. [2025]\nJunbang Liang, Pavel Tokmakov, Ruoshi Liu, Sruthi Sudhakar, Paarth Shah, Rares Ambrus, and Carl Vondrick.\nVideo generators are robot policies, 2025.\nURL\nhttps://arxiv.org/abs/2508.00795\n.\nPai et al. [2025b]\nJonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, and Elvis Nava.\nmimic-video: Video-action models for generalizable robot control beyond vlas, 2025b.\nURL\nhttps://arxiv.org/abs/2512.15692\n.\nWu et al. [2023d]\nHongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong.\nUnleashing large-scale video generative pre-training for visual robot manipulation.\narXiv preprint arXiv:2312.13139\n, 2023d.\nGuo et al. [2024b]\nYanjiang Guo, Yucheng Hu, Jianke Zhang, Yen-Jen Wang, Xiaoyu Chen, Chaochao Lu, and Jianyu Chen.\nPrediction with action: Visual policy learning via joint denoising process.\nAdvances in Neural Information Processing Systems\n, 37:112386â€“112410, 2024b.\nWang et al. [2025d]\nYuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, and Zhaoxiang Zhang.\nUnified vision-language-action model.\narXiv preprint arXiv:2506.19850\n, 2025d.\nSeo et al. [2022]\nYounggyo Seo, Kimin Lee, Stephen L James, and Pieter Abbeel.\nReinforcement learning with action-free pre-training from videos.\nIn\nInternational Conference on Machine Learning\n, pages 19561â€“19579. PMLR, 2022.\nJiang et al. [2025b]\nZhennan Jiang, Kai Liu, Yuxin Qin, Shuai Tian, Yupeng Zheng, Mingcai Zhou, Chao Yu, Haoran Li, and Dongbin Zhao.\nWorld4rl: Diffusion world models for policy refinement with reinforcement learning for robotic manipulation.\narXiv preprint arXiv:2509.19080\n, 2025b.\nLi et al. [2024]\nXuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, et al.\nEvaluating real-world robot manipulation policies in simulation.\narXiv preprint arXiv:2405.05941\n, 2024.\nZhang et al. [2025g]\nKaifeng Zhang, Shuo Sha, Hanxiao Jiang, Matthew Loper, Hyunjong Song, Guangyan Cai, Zhuo Xu, Xiaochen Hu, Changxi Zheng, and Yunzhu Li.\nReal-to-sim robot policy evaluation with gaussian splatting simulation of soft-body interactions.\narXiv preprint arXiv:2511.04665\n, 2025g.\nJain et al. [2025]\nArhan Jain, Mingtong Zhang, Kanav Arora, William Chen, Marcel Torne, Muhammad Zubair Irshad, Sergey Zakharov, Yue Wang, Sergey Levine, Chelsea Finn, et al.\nPolaris: Scalable real-to-sim evaluations for generalist robot policies.\narXiv preprint arXiv:2512.16881\n, 2025.\nGao et al. [2025]\nShenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, and Chuang Gan.\nAdaworld: Learning adaptable world models with latent actions.\narXiv preprint arXiv:2503.18938\n, 2025.\nMa et al. [2023]\nYecheng Jason Ma, Vikash Kumar, Amy Zhang, Osbert Bastani, and Dinesh Jayaraman.\nLiv: Language-image representations and rewards for robotic control.\nIn\nInternational Conference on Machine Learning\n, pages 23301â€“23320. PMLR, 2023.\nDu et al. [2023b]\nYilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel.\nLearning universal policies via text-guided video generation.\nAdvances in neural information processing systems\n, 36:9156â€“9172, 2023b.\nFu et al. [2023]\nStephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola.\nDreamsim: Learning new dimensions of human visual similarity using synthetic data, 2023.\nURL\nhttps://arxiv.org/abs/2306.09344\n.\nSalimans et al. [2016]\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans.\nAdvances in neural information processing systems\n, 29, 2016.\nHeusel et al. [2018]\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium, 2018.\nURL\nhttps://arxiv.org/abs/1706.08500\n.\nManasa and Channappayya [2016]\nK Manasa and Sumohana S Channappayya.\nAn optical flow-based full reference video quality assessment algorithm.\nIEEE Transactions on Image Processing\n, 25(6):2480â€“2492, 2016.\nChi et al. [2024]\nXiaowei Chi, Chun-Kai Fan, Hengyuan Zhang, Xingqun Qi, Rongyu Zhang, Anthony Chen, Chi-min Chan, Wei Xue, Qifeng Liu, Shanghang Zhang, et al.\nEva: An embodied world model for future video anticipation.\narXiv preprint arXiv:2410.15461\n, 2024.\nZhou et al. [2025b]\nFengzhe Zhou, Jiannan Huang, Jialuo Li, Deva Ramanan, and Humphrey Shi.\nPhysical ai bench: A comprehensive benchmark for physical ai generation and understanding, 2025b.\nURL\nhttps://github.com/SHI-Labs/physical-ai-bench\n.\nWu et al. [2023e]\nHaoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin.\nExploring video quality assessment on user generated contents from aesthetic and technical perspectives.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, pages 20144â€“20154, 2023e.\nKay et al. [2017]\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman.\nThe kinetics human action video dataset, 2017.\nURL\nhttps://arxiv.org/abs/1705.06950\n.\nYue et al. [2025b]\nHu Yue, Siyuan Huang, Yue Liao, Shengcong Chen, Pengfei Zhou, Liliang Chen, Maoqing Yao, and Guanghui Ren.\nEwmbench: Evaluating scene, motion, and semantic quality in embodied world models.\narXiv preprint arXiv:2505.09694\n, 2025b.\nRawte et al. [2025]\nVipula Rawte, Sarthak Jain, Aarush Sinha, Garv Kaushik, Aman Bansal, Prathiksha Rumale Vishwanath, Samyak Rajesh Jain, Aishwarya Naresh Reganti, Vinija Jain, Aman Chadha, et al.\nVibe: A text-to-video benchmark for evaluating hallucination in large multimodal models.\nIn\nProceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025)\n, pages 232â€“246, 2025.\nChu et al. [2024]\nZhixuan Chu, Lei Zhang, Yichen Sun, Siqiao Xue, Zhibo Wang, Zhan Qin, and Kui Ren.\nSora detector: A unified hallucination detection for large text-to-video models.\narXiv preprint arXiv:2405.04180\n, 2024.\nGreydanus et al. [2019]\nSamuel Greydanus, Misko Dzamba, and Jason Yosinski.\nHamiltonian neural networks.\nAdvances in neural information processing systems\n, 32, 2019.\nAllen-Blanchette et al. [2020]\nChristine Allen-Blanchette, Sushant Veer, Anirudha Majumdar, and Naomi Ehrich Leonard.\nLagnetvip: A lagrangian neural network for video prediction, 2020.\nURL\nhttps://arxiv.org/abs/2010.12932\n.\nLiu et al. [2024c]\nShaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang.\nPhysgen: Rigid-body physics-grounded image-to-video generation.\nIn\nEuropean Conference on Computer Vision (ECCV)\n, 2024c.\nLi et al. [2025i]\nZizhang Li, Hong-Xing Yu, Wei Liu, Yin Yang, Charles Herrmann, Gordon Wetzstein, and Jiajun Wu.\nWonderplay: Dynamic 3d scene generation from a single image and actions, 2025i.\nURL\nhttps://arxiv.org/abs/2505.18151\n.\nMeng et al. [2021]\nChenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSdedit: Guided image synthesis and editing with stochastic differential equations.\narXiv preprint arXiv:2108.01073\n, 2021.\nLiu et al. [2024d]\nHanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng.\nA survey on hallucination in large vision-language models.\narXiv preprint arXiv:2402.00253\n, 2024d.\nKarniadakis et al. [2021]\nGeorge Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang.\nPhysics-informed machine learning.\nNature Reviews Physics\n, 3(6):422â€“440, 2021.\nYu et al. [2023]\nZecheng Yu, Yifei Huang, Ryosuke Furuta, Takuma Yagi, Yusuke Goutsu, and Yoichi Sato.\nFine-grained affordance annotation for egocentric hand-object interaction videos.\nIn\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision\n, pages 2155â€“2163, 2023.\nLi et al. [2022]\nYong-Lu Li, Hongwei Fan, Zuoyu Qiu, Yiming Dou, Liang Xu, Hao-Shu Fang, Peiyang Guo, Haisheng Su, Dongliang Wang, Wei Wu, et al.\nDiscovering a variety of objects in spatio-temporal human-object interactions.\narXiv preprint arXiv:2211.07501\n, 2022.\nFang et al. [2018]\nKuan Fang, Te-Lin Wu, Daniel Yang, Silvio Savarese, and Joseph J. Lim.\nDemo2vec: Reasoning object affordances from online videos.\nIn\n2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 2139â€“2147, 2018.\n10.1109/CVPR.2018.00228\n.\nNagarajan et al. [2019]\nTushar Nagarajan, Christoph Feichtenhofer, and Kristen Grauman.\nGrounded human-object interaction hotspots from video.\nIn\n2019 IEEE/CVF International Conference on Computer Vision (ICCV)\n, pages 8687â€“8696, 2019.\n10.1109/ICCV.2019.00878\n.\nLiu et al. [2022]\nShaowei Liu, Subarna Tripathi, Somdeb Majumdar, and Xiaolong Wang.\nJoint hand motion and interaction hotspots prediction from egocentric videos.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 3282â€“3292, 2022.\nMur-Labadia et al. [2023]\nLorenzo Mur-Labadia, Jose J Guerrero, and Ruben Martinez-Cantin.\nMulti-label affordance mapping from egocentric vision.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, pages 5238â€“5249, 2023.\nBahl et al. [2023]\nShikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak.\nAffordances from human videos as a versatile representation for robotics.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 13778â€“13790, 2023.\nJu et al. [2024]\nYuanchen Ju, Kaizhe Hu, Guowei Zhang, Gu Zhang, Mingrun Jiang, and Huazhe Xu.\nRobo-abc: Affordance generalization beyond categories via semantic correspondence for robot manipulation.\nIn\nEuropean Conference on Computer Vision\n, pages 222â€“239. Springer, 2024.\nLi et al. [2025j]\nGen Li, Nikolaos Tsagkas, Jifei Song, Ruaridh Mon-Williams, Sethu Vijayakumar, Kun Shao, and Laura Sevilla-Lara.\nLearning precise affordances from egocentric videos for robotic manipulation.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, pages 10581â€“10591, 2025j.\nLiu et al. [2020]\nMiao Liu, Siyu Tang, Yin Li, and James M Rehg.\nForecasting human-object interaction: joint prediction of motor attention and actions in first person video.\nIn\nEuropean conference on computer vision\n, pages 704â€“721. Springer, 2020.\nShorinwa et al. [2025]\nOla Shorinwa, Zhiting Mei, Justin Lidard, Allen Z Ren, and Anirudha Majumdar.\nA survey on uncertainty quantification of large language models: Taxonomy, open research challenges, and future directions.\nACM Computing Surveys\n, 2025.\nOuyang et al. [2022]\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\nTraining language models to follow instructions with human feedback.\nAdvances in neural information processing systems\n, 35:27730â€“27744, 2022.\nMei et al. [2025c]\nZhiting Mei, Christina Zhang, Tenny Yin, Justin Lidard, Ola Shorinwa, and Anirudha Majumdar.\nReasoning about uncertainty: Do reasoning models know when they donâ€™t know?, 2025c.\nURL\nhttps://arxiv.org/abs/2506.18183\n.\nBai et al. [2022]\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.\nTraining a helpful and harmless assistant with reinforcement learning from human feedback, 2022.\nURL\nhttps://arxiv.org/abs/2204.05862\n.\nHsu et al. [2024]\nKai-Chieh Hsu, Haimin Hu, and Jaime F. Fisac.\nThe safety filter: A unified view of safety-critical control in autonomous systems.\nAnnual Review of Control, Robotics, and Autonomous Systems\n, 7(Volume 7, 2024):47â€“72, 2024.\nISSN 2573-5144.\nhttps://doi.org/10.1146/annurev-control-071723-102940\n.\nURL\nhttps://www.annualreviews.org/content/journals/10.1146/annurev-control-071723-102940\n.\nAgrawal et al. [2025]\nSankalp Agrawal, Junwon Seo, Kensuke Nakamura, Ran Tian, and Andrea Bajcsy.\nAnysafe: Adapting latent safety filters at runtime via safety constraint parameterization in the latent space, 2025.\nURL\nhttps://arxiv.org/abs/2509.19555\n.\nWalke et al. [2023]\nHomer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al.\nBridgedata v2: A dataset for robot learning at scale.\nIn\nConference on Robot Learning\n, pages 1723â€“1736. PMLR, 2023.\nOâ€™Neill et al. [2024]\nAbby Oâ€™Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al.\nOpen x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 6892â€“6903. IEEE, 2024.\nWang et al. [2025e]\nJing Wang, Fengzhuo Zhang, Xiaoli Li, Vincent YF Tan, Tianyu Pang, Chao Du, Aixin Sun, and Zhuoran Yang.\nError analyses of auto-regressive video diffusion models: A unified framework.\narXiv preprint arXiv:2503.10704\n, 2025e.\nCai et al. [2025]\nShengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, et al.\nMixture of contexts for long video generation.\narXiv preprint arXiv:2508.21058\n, 2025.\nYin et al. [2023b]\nShengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al.\nNuwa-xl: Diffusion over diffusion for extremely long video generation.\nIn\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\n, pages 1309â€“1320, 2023b.\nBain et al. [2021]\nMax Bain, Arsha Nagrani, GÃ¼l Varol, and Andrew Zisserman.\nFrozen in time: A joint video and image encoder for end-to-end retrieval.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n, pages 1728â€“1738, 2021.\nZhang et al. [2024b]\nYuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li.\nVideo instruction tuning with synthetic data.\narXiv preprint arXiv:2410.02713\n, 2024b.\nLin et al. [2024]\nJi Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han.\nVila: On pre-training for visual language models.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pages 26689â€“26699, 2024.\nChen et al. [2025c]\nJunyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han.\nDeep compression autoencoder for efficient high-resolution diffusion models, 2025c.\nURL\nhttps://arxiv.org/abs/2410.10733\n.\nFeng et al. [2024]\nRuili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, and Hongyang Zhang.\nThe matrix: Infinite-horizon world generation with real-time moving control.\narXiv preprint arXiv:2412.03568\n, 2024.\nZhao et al. [2025b]\nTianchen Zhao, Tongcheng Fang, Haofeng Huang, Enshu Liu, Rui Wan, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, and Yu Wang.\nVidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation, 2025b.\nURL\nhttps://arxiv.org/abs/2406.02540\n.\nYin et al. [2025b]\nTianwei Yin, Qiang Zhang, Richard Zhang, William T Freeman, Fredo Durand, Eli Shechtman, and Xun Huang.\nFrom slow bidirectional to fast autoregressive video diffusion models.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 22963â€“22974, 2025b.",
    "preview_text": "Video generation models have emerged as high-fidelity models of the physical world, capable of synthesizing high-quality videos capturing fine-grained interactions between agents and their environments conditioned on multi-modal user inputs. Their impressive capabilities address many of the long-standing challenges faced by physics-based simulators, driving broad adoption in many problem domains, e.g., robotics. For example, video models enable photorealistic, physically consistent deformable-body simulation without making prohibitive simplifying assumptions, which is a major bottleneck in physics-based simulation. Moreover, video models can serve as foundation world models that capture the dynamics of the world in a fine-grained and expressive way. They thus overcome the limited expressiveness of language-only abstractions in describing intricate physical interactions. In this survey, we provide a review of video models and their applications as embodied world models in robotics, encompassing cost-effective data generation and action prediction in imitation learning, dynamics and rewards modeling in reinforcement learning, visual planning, and policy evaluation. Further, we highlight important challenges hindering the trustworthy integration of video models in robotics, which include poor instruction following, hallucinations such as violations of physics, and unsafe content generation, in addition to fundamental limitations such as significant data curation, training, and inference costs. We present potential future directions to address these open research challenges to motivate research and ultimately facilitate broader applications, especially in safety-critical settings.\n\n1]Princeton University\n2]Temple University\n\\contribution\n[*]Equal contribution.\nVideo Generation Models in Robotics:\nApplications, Research Challenges, Future Directions\nZhiting Mei\nTenny Yin\nOla Shorinwa\nApurva Badithela\nZhonghe Zheng\nJoseph Bruno\nMadison Bland\nLihan Zha\nAsher Hancock\nJaime ",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¿™ç¯‡è®ºæ–‡ç»¼è¿°äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æœºå™¨äººå­¦ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬å¼ºåŒ–å­¦ä¹ ä¸­çš„åŠ¨æ€å’Œå¥–åŠ±å»ºæ¨¡ï¼Œä»¥åŠä½œä¸ºå…·èº«ä¸–ç•Œæ¨¡å‹æ”¯æŒå…¨èº«æ§åˆ¶ç­‰ä»»åŠ¡ï¼Œä½†æœªç›´æ¥æ¶‰åŠVLAã€diffusionã€Flow Matchingå’ŒVLMã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šã€Šæœºå™¨äººå­¦ä¸­çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹â€”â€”åº”ç”¨ã€ç ”ç©¶æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘ã€‹\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nä¼ ç»Ÿçš„æœºå™¨äººå­¦ä¹ ä¾èµ–äºç‰©ç†ä»¿çœŸå™¨æˆ–åŸºäºè¯­è¨€çš„ä¸–ç•Œæ¨¡å‹ï¼Œä½†å‰è€…å¸¸å› ç®€åŒ–å‡è®¾å¯¼è‡´è§†è§‰ä¸ç‰©ç†ä¿çœŸåº¦ä¸è¶³ï¼Œåè€…åˆ™éš¾ä»¥ç²¾ç»†æè¿°å¤æ‚çš„ç‰©ç†äº¤äº’è¿‡ç¨‹ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…çš„é«˜ä¿çœŸã€å¯æ§è§†é¢‘ç”Ÿæˆæ¨¡å‹å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚è¿™äº›æ¨¡å‹é€šè¿‡æµ·é‡äº’è”ç½‘æ•°æ®è®­ç»ƒï¼Œèƒ½å¤Ÿåˆæˆæ•æ‰ç»†ç²’åº¦ç‰©ç†äº¤äº’çš„é«˜è´¨é‡è§†é¢‘ï¼Œä¸ºè§£å†³æœºå™¨äººå­¦ä¸­é•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ï¼ˆå¦‚æ•°æ®ç¨€ç¼ºã€ä»¿çœŸåˆ°ç°å®çš„é¸¿æ²Ÿï¼‰æä¾›äº†æ–°é€”å¾„ã€‚æœ¬ç»¼è¿°æ—¨åœ¨ç³»ç»Ÿæ¢³ç†è§†é¢‘æ¨¡å‹ä½œä¸º**å…·èº«ä¸–ç•Œæ¨¡å‹**åœ¨æœºå™¨äººå­¦ä¸­çš„åº”ç”¨ã€å½“å‰é¢ä¸´çš„æŒ‘æˆ˜åŠæœªæ¥ç ”ç©¶æ–¹å‘ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\nè®ºæ–‡å°†è§†é¢‘ä¸–ç•Œæ¨¡å‹åˆ†ä¸º**éšå¼**ï¼ˆæ¨¡å‹å†…éƒ¨ç¼–ç åœºæ™¯ï¼‰å’Œ**æ˜¾å¼**ï¼ˆç”Ÿæˆå¤šè§†è§’è§†é¢‘æˆ–æ·±åº¦å›¾ä»¥æ„å»º3Dåœºæ™¯è¡¨ç¤ºï¼‰ä¸¤ç±»ã€‚æ ¸å¿ƒæŠ€æœ¯æ¶æ„ä¸»è¦åŒ…æ‹¬ï¼š\n- **æ‰©æ•£/æµåŒ¹é…æ¨¡å‹**ï¼šæˆä¸ºå½“å‰ä¸»æµï¼Œé€šè¿‡U-Netæˆ–æ‰©æ•£Transformeræ¶æ„ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè¿­ä»£å»å™ªï¼Œç”Ÿæˆé«˜ä¿çœŸè§†é¢‘ã€‚æ”¯æŒé€šè¿‡æ–‡æœ¬ã€å›¾åƒã€åŠ¨ä½œè½¨è¿¹ç­‰å¤šç§æ¨¡æ€è¿›è¡Œæ¡ä»¶æ§åˆ¶ã€‚\n- **è§†é¢‘è”åˆåµŒå…¥é¢„æµ‹æ¶æ„**ï¼šä¾§é‡äºä»è§†é¢‘æ•°æ®ä¸­å­¦ä¹ æœ‰æ•ˆçš„æ½œåœ¨è¡¨ç¤ºï¼Œç”¨äºæœªæ¥é¢„æµ‹å’Œç†è§£ï¼Œè€Œéåƒç´ çº§ç”Ÿæˆã€‚\n\n**3. ä¸»è¦åº”ç”¨é¢†åŸŸï¼ˆå®éªŒç»“æœï¼‰**\nè®ºæ–‡é‡ç‚¹é˜è¿°äº†è§†é¢‘æ¨¡å‹åœ¨æœºå™¨äººå­¦ä¸­çš„å››å¤§æ ¸å¿ƒåº”ç”¨ï¼š\n- **æ¨¡ä»¿å­¦ä¹ ä¸­çš„æ•°æ®ç”Ÿæˆä¸åŠ¨ä½œé¢„æµ‹**ï¼šä½æˆæœ¬ç”Ÿæˆä¸“å®¶æ¼”ç¤ºè§†é¢‘ï¼Œå¹¶é€šè¿‡æ½œåœ¨åŠ¨ä½œæ¨¡å‹æˆ–é€†åŠ¨åŠ›å­¦æ¨¡å‹ä»ä¸­æå–æœºå™¨äººåŠ¨ä½œï¼Œç”¨äºç­–ç•¥è®­ç»ƒã€‚\n- **å¼ºåŒ–å­¦ä¹ ä¸­çš„åŠ¨åŠ›å­¦ä¸å¥–åŠ±å»ºæ¨¡**ï¼šä½œä¸ºé«˜ä¿çœŸçš„åŠ¨åŠ›å­¦é¢„æµ‹å™¨å’Œä¸°å¯Œçš„å†…åœ¨å¥–åŠ±ä¿¡å·æ¥æºï¼Œæå‡æ ·æœ¬æ•ˆç‡ã€‚\n- **å¯æ‰©å±•çš„ç­–ç•¥è¯„ä¼°**ï¼šä½œä¸ºä¸–ç•Œæ¨¡å‹è¿›è¡Œé—­ç¯ç­–ç•¥æ¨æ¼”ï¼Œé¢„æµ‹çœŸå®ä¸–ç•ŒæˆåŠŸç‡ï¼Œæˆæœ¬è¿œä½äºå®ç‰©å®éªŒï¼Œä¸”ä¸çœŸå®ç»“æœç›¸å…³æ€§è¾ƒé«˜ã€‚\n- **è§†è§‰è§„åˆ’**ï¼šæ ¹æ®ä»»åŠ¡æŒ‡ä»¤å’Œåˆå§‹è§‚å¯Ÿï¼Œç›´æ¥ç”Ÿæˆè§†é¢‘è®¡åˆ’ï¼ˆåŠ¨ä½œå¼•å¯¼æˆ–åŠ¨ä½œæ— å…³ï¼‰ï¼Œå¹¶å°†å…¶ä½œä¸ºå­ç›®æ ‡æˆ–å‚è€ƒè½¨è¿¹æ¥æ¨å¯¼æœºå™¨äººåŠ¨ä½œã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\næœ¬ç»¼è¿°é¦–æ¬¡ç³»ç»Ÿæ€§åœ°å°†è§†é¢‘ç”Ÿæˆæ¨¡å‹å®šä½ä¸ºæœºå™¨äººå­¦çš„**å…·èº«ä¸–ç•Œæ¨¡å‹**ï¼Œæ˜ç¡®äº†å…¶è¶…è¶Šä¼ ç»Ÿä»¿çœŸå™¨å’Œè¯­è¨€æ¨¡å‹çš„ç‹¬ç‰¹ä»·å€¼â€”â€”å³æä¾›**é«˜ä¿çœŸã€ç‰©ç†ä¸€è‡´**çš„æ—¶ç©ºç¯å¢ƒæ¼”åŒ–é¢„æµ‹ã€‚é€šè¿‡è¯¦ç»†æ¢³ç†åº”ç”¨æ¡†æ¶å¹¶æŒ‡å‡ºåå¤§å…³é”®æŒ‘æˆ˜ï¼ˆå¦‚**ç‰©ç†è¿åã€æŒ‡ä»¤è·Ÿéšå·®ã€é•¿è§†é¢‘ç”Ÿæˆã€æ•°æ®ä¸è®¡ç®—æˆæœ¬é«˜æ˜‚**ç­‰ï¼‰ï¼Œæœ¬æ–‡ä¸ºé¢†åŸŸç ”ç©¶è€…æä¾›äº†æ¸…æ™°çš„æŠ€æœ¯è·¯çº¿å›¾ã€‚æ–‡ä¸­æ‰€æ¢è®¨çš„è§£å†³æ–¹æ¡ˆï¼ˆå¦‚èåˆç‰©ç†å…ˆéªŒã€ä¸ç¡®å®šæ€§é‡åŒ–ã€å®‰å…¨æŠ¤æ è®¾è®¡ï¼‰ä¸ä»…å¯¹æœºå™¨äººå­¦è‡³å…³é‡è¦ï¼Œä¹Ÿå¯¹æ¨åŠ¨å¯ä¿¡ã€å¯é è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„é€šç”¨å‘å±•å…·æœ‰æ·±è¿œæ„ä¹‰ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè¿™ç¯‡ç»¼è¿°è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äº**ç³»ç»Ÿæ€§åœ°å°†è§†é¢‘ç”Ÿæˆæ¨¡å‹å®šä½ä¸ºâ€œå…·èº«ä¸–ç•Œæ¨¡å‹â€**ï¼Œå¹¶å…¨é¢é˜è¿°äº†å…¶åœ¨æœºå™¨äººå­¦ä¸­çš„å››å¤§åº”ç”¨èŒƒå¼ã€é¢ä¸´çš„åå¤§æŒ‘æˆ˜ä»¥åŠæœªæ¥ç ”ç©¶æ–¹å‘ã€‚å®ƒå¹¶éæå‡ºå•ä¸€çš„æ–°ç®—æ³•ï¼Œè€Œæ˜¯æ„å»ºäº†ä¸€ä¸ª**ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶**ï¼Œå°†è§†é¢‘ç”Ÿæˆæ¨¡å‹ä»å†…å®¹åˆ›ä½œå·¥å…·æå‡ä¸ºç†è§£å’Œé¢„æµ‹ç‰©ç†ä¸–ç•ŒåŠ¨æ€çš„â€œæ¨¡æ‹Ÿå™¨â€ã€‚\n\n### **è¯•å›¾è§£å†³çš„æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººå­¦ä¸­é•¿æœŸå­˜åœ¨çš„å‡ ä¸ªæ ¹æœ¬æ€§ç“¶é¢ˆï¼š\n\n1.  **é«˜ä¿çœŸä¸–ç•Œå»ºæ¨¡çš„éš¾é¢˜**ï¼šä¼ ç»ŸåŸºäºç‰©ç†çš„ä»¿çœŸå™¨ï¼ˆå¦‚MuJoCo, PyBulletï¼‰ä¾èµ–äºç®€åŒ–çš„ç‰©ç†å‡è®¾ï¼Œéš¾ä»¥æ¨¡æ‹Ÿéåˆšæ€§ä½“ã€å¤æ‚æ¥è§¦äº¤äº’å’ŒçœŸå®è§†è§‰å¤–è§‚ï¼Œå¯¼è‡´â€œä»¿çœŸåˆ°ç°å®â€çš„é¸¿æ²Ÿã€‚\n2.  **æœºå™¨äººæ•°æ®ä¸ç­–ç•¥è¯„ä¼°çš„æˆæœ¬é—®é¢˜**ï¼šåœ¨çœŸå®ä¸–ç•Œä¸­æ”¶é›†ä¸“å®¶æ¼”ç¤ºæ•°æ®ã€è¿›è¡Œç­–ç•¥è®­ç»ƒå’Œè¯„ä¼°æå…¶æ˜‚è´µã€è€—æ—¶ä¸”ä¸å®‰å…¨ã€‚\n3.  **è¯­è¨€æ¨¡å‹åœ¨ç‰©ç†äº¤äº’è¡¨è¾¾ä¸Šçš„å±€é™æ€§**ï¼šçº¯è¯­è¨€æŠ½è±¡ç¼ºä¹å¯¹å¤æ‚æ—¶ç©ºåŠ¨æ€å’Œç»†ç²’åº¦ç‰©ç†äº¤äº’ï¼ˆå¦‚å¸ƒæ–™æŠ“å–ï¼‰çš„ç²¾ç¡®æè¿°èƒ½åŠ›ã€‚\n4.  **å¯»æ±‚å¯æ‰©å±•ä¸”é€šç”¨çš„æœºå™¨äººå­¦ä¹ æ–¹æ³•**ï¼šè¶…è¶Šé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç­–ç•¥ï¼Œå‘èƒ½å¤Ÿè¿›è¡Œå¸¸è¯†æ¨ç†ã€è§†è§‰è§„åˆ’å’Œé›¶æ ·æœ¬æ³›åŒ–çš„é€šç”¨æœºå™¨äººç­–ç•¥è¿ˆè¿›ã€‚\n\n### **è§£å†³æ–¹æ¡ˆï¼šè§†é¢‘æ¨¡å‹ä½œä¸ºå…·èº«ä¸–ç•Œæ¨¡å‹**\nè®ºæ–‡æå‡ºï¼Œåˆ©ç”¨åœ¨å¤§è§„æ¨¡äº’è”ç½‘æ•°æ®ä¸Šè®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯åŸºäº**æ‰©æ•£/æµåŒ¹é…**çš„æ¨¡å‹ï¼‰ä½œä¸ºè§£å†³æ–¹æ¡ˆã€‚å…¶è§£å†³è·¯å¾„å¦‚ä¸‹ï¼š\n\n#### **1. æŠ€æœ¯åŸºç¡€ï¼šé«˜ä¿çœŸã€å¯æ§çš„è§†é¢‘ç”Ÿæˆ**\n- **æ¨¡å‹æ¶æ„**ï¼šé‡‡ç”¨**æ½œåœ¨æ‰©æ•£æ¨¡å‹**ï¼Œä½¿ç”¨U-Netæˆ–**æ‰©æ•£Transformer** ä½œä¸ºä¸»å¹²ï¼Œåœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå»å™ªç”Ÿæˆï¼Œå¹³è¡¡äº†è´¨é‡ä¸æ•ˆç‡ã€‚\n- **å¯æ§æ€§**ï¼šé€šè¿‡**åˆ†ç±»å™¨æ— å…³å¼•å¯¼**ã€äº¤å‰æ³¨æ„åŠ›ã€é€šé“æ‹¼æ¥ç­‰æ–¹å¼ï¼Œæ”¯æŒå¤šæ¨¡æ€æ¡ä»¶è¾“å…¥ï¼ˆæ–‡æœ¬ã€å›¾åƒã€æœºå™¨äººåŠ¨ä½œã€è¿åŠ¨è½¨è¿¹ï¼‰ï¼Œå®ç°å¯¹ç”Ÿæˆå†…å®¹çš„ç²¾ç»†æ§åˆ¶ã€‚\n- **ç‰©ç†ä¸€è‡´æ€§å­¦ä¹ **ï¼šæ¨¡å‹ä»æµ·é‡è§†é¢‘æ•°æ®ä¸­éšå¼å­¦ä¹ ç‰©ç†ä¸–ç•Œçš„æ—¶ç©ºæ¼”åŒ–è§„å¾‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰é€¼çœŸåŠ¨æ€å’Œäº¤äº’çš„è§†é¢‘ã€‚\n\n#### **2. åœ¨æœºå™¨äººå­¦ä¸­çš„å››å¤§åº”ç”¨ï¼ˆè§£å†³æ–¹æ¡ˆçš„å…·ä½“ä½“ç°ï¼‰**\n- **æ¨¡ä»¿å­¦ä¹ ä¸­çš„æ•°æ®ç”Ÿæˆä¸åŠ¨ä½œé¢„æµ‹**ï¼š\n    - **æ•°æ®ç”Ÿæˆ**ï¼šå¾®è°ƒé¢„è®­ç»ƒè§†é¢‘æ¨¡å‹ï¼Œæ ¹æ®ä»»åŠ¡æè¿°ç”Ÿæˆé«˜è´¨é‡çš„ä¸“å®¶æ¼”ç¤ºè§†é¢‘ï¼Œæ›¿ä»£æ˜‚è´µçš„äººå·¥æ•°æ®é‡‡é›†ã€‚\n    - **åŠ¨ä½œé¢„æµ‹**ï¼šé€šè¿‡**æ½œåœ¨åŠ¨ä½œæ¨¡å‹**æˆ–**é€†åŠ¨åŠ›å­¦æ¨¡å‹**ä»ç”Ÿæˆçš„è§†é¢‘ä¸­æå–æœºå™¨äººåŠ¨ä½œï¼Œæˆ–ç›´æ¥è®­ç»ƒ**ç»Ÿä¸€çš„è§†é¢‘-åŠ¨ä½œæ¨¡å‹**ä½œä¸ºç­–ç•¥ä¸»å¹²ã€‚\n- **å¼ºåŒ–å­¦ä¹ ä¸­çš„åŠ¨åŠ›å­¦ä¸å¥–åŠ±å»ºæ¨¡**ï¼š\n    - å°†åŠ¨ä½œæ¡ä»¶è§†é¢‘æ¨¡å‹ä½œä¸º**åŠ¨åŠ›å­¦é¢„æµ‹å™¨**ï¼Œåœ¨ä»¿çœŸä¸–ç•Œä¸­è¿›è¡Œå®‰å…¨çš„ç­–ç•¥è¯•é”™ã€‚\n    - åˆ©ç”¨è§†é¢‘ç”Ÿæˆçš„å¯èƒ½æ€§æˆ–ç†µä½œä¸º**å†…åœ¨å¥–åŠ±ä¿¡å·**ï¼Œå¼•å¯¼æ¢ç´¢ã€‚\n- **å¯æ‰©å±•çš„ç­–ç•¥è¯„ä¼°**ï¼š\n    - åœ¨è§†é¢‘ä¸–ç•Œæ¨¡å‹ä¸­é—­ç¯æ‰§è¡Œç­–ç•¥ï¼Œç”Ÿæˆé¢„æµ‹è½¨è¿¹ï¼Œå¹¶ä½¿ç”¨VLMæˆ–äººå·¥è§„åˆ™è¯„ä¼°æˆåŠŸç‡ã€‚\n    - èƒ½å¿«é€Ÿæ„å»ºOODåœºæ™¯ï¼ˆå¦‚æ·»åŠ å¹²æ‰°ç‰©ï¼‰ï¼Œè¯„ä¼°ç­–ç•¥çš„é²æ£’æ€§å’Œå®‰å…¨æ€§ï¼Œæˆæœ¬è¿œä½äºçœŸå®ç¡¬ä»¶æµ‹è¯•ã€‚\n- **è§†è§‰è§„åˆ’**ï¼š\n    - **åŠ¨ä½œå¼•å¯¼**ï¼šé‡‡æ ·åŠ¨ä½œåºåˆ—ï¼Œç”¨è§†é¢‘æ¨¡å‹é¢„æµ‹ç»“æœï¼Œä¼˜åŒ–ä¸ç›®æ ‡å›¾åƒçš„åŒ¹é…åº¦ã€‚\n    - **åŠ¨ä½œæ— å…³**ï¼šç›´æ¥ç”Ÿæˆè¾¾æˆä»»åŠ¡çš„è§†é¢‘è®¡åˆ’ï¼Œå°†å…¶åˆ†è§£ä¸ºå›¾åƒå­ç›®æ ‡ï¼Œå†ç”±ç­–ç•¥æ‰§è¡Œã€‚\n    - å®ç°äº†åŸºäºè§†è§‰å¸¸è¯†çš„**é•¿ç¨‹ä»»åŠ¡è§„åˆ’**ã€‚\n\n#### **3. åº”å¯¹æŒ‘æˆ˜çš„åˆæ­¥æ–¹å‘**\nè®ºæ–‡ä¸ä»…å±•ç¤ºåº”ç”¨ï¼Œä¹Ÿå¦è¯šæŒ‡å‡ºå…³é”®æŒ‘æˆ˜ï¼Œå¹¶å‹¾å‹’è§£å†³æ€è·¯ï¼š\n- **å¹»è§‰ä¸ç‰©ç†è¿å**ï¼šæè®®æ•´åˆ**ç‰©ç†å…ˆéªŒ**ï¼ˆå¦‚å“ˆå¯†é¡¿åŠ›å­¦ï¼‰ã€ä¸ç‰©ç†ä»¿çœŸå™¨ç»“åˆã€åˆ©ç”¨**åŠŸèƒ½å¯ä¾›æ€§**åœ°å›¾è¿›è¡Œå¼•å¯¼ã€‚\n- **æŒ‡ä»¤è·Ÿéš**ï¼šæ¢ç´¢å¤šæ¨¡æ€æ¡ä»¶ç»†åŒ–ã€åŸºäºVLMçš„æ¨ç†ã€æŒ‡ä»¤å¾®è°ƒã€‚\n- **ä¸ç¡®å®šæ€§é‡åŒ–**ï¼šå¼€å‘é€‚ç”¨äºè§†é¢‘ç”Ÿæˆç©ºé—´çš„**æ ¡å‡†ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•**ï¼Œè®©æ¨¡å‹â€œçŸ¥å…¶æ‰€ä¸çŸ¥â€ã€‚\n- **é•¿è§†é¢‘ç”Ÿæˆä¸æˆæœ¬**ï¼šç ”ç©¶**è®°å¿†å¢å¼ºæ¶æ„**ã€**æµ‹è¯•æ—¶è®­ç»ƒ**ã€**æ—¶ç©ºå‹ç¼©**å’Œ**ä¸€è‡´æ€§æ¨¡å‹**ä»¥æå‡ç”Ÿæˆé•¿åº¦å’Œæ•ˆç‡ã€‚\n\n### **æ€»ç»“**\nè¿™ç¯‡è®ºæ–‡çš„**æ ¸å¿ƒä»·å€¼**åœ¨äºå®Œæˆäº†ä¸€æ¬¡é‡è¦çš„**èŒƒå¼è½¬æ¢è®ºè¯**ï¼šå®ƒè¯æ˜ï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹å‡­å€Ÿå…¶ä»æ•°æ®ä¸­å­¦ä¹ åˆ°çš„ã€é«˜ä¿çœŸçš„ç‰©ç†ä¸–ç•ŒåŠ¨æ€å…ˆéªŒï¼Œèƒ½å¤Ÿä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„ã€å¯è®¡ç®—çš„â€œ**ä¸–ç•Œæ¨¡æ‹Ÿå™¨**â€ï¼Œç³»ç»Ÿæ€§è§£å†³æœºå™¨äººå­¦åœ¨æ•°æ®ã€ä»¿çœŸã€è§„åˆ’å’Œè¯„ä¼°æ–¹é¢çš„ä¼ ç»Ÿéš¾é¢˜ã€‚å®ƒä¸ºè§£å†³â€œå¦‚ä½•è®©æœºå™¨äººåœ¨å¤æ‚çœŸå®ä¸–ç•Œä¸­é«˜æ•ˆã€å®‰å…¨ã€é€šç”¨åœ°å­¦ä¹ ä¸è¡ŒåŠ¨â€è¿™ä¸€æ ¹æœ¬é—®é¢˜ï¼ŒæŒ‡æ˜äº†ä¸€æ¡åŸºäºç”Ÿæˆå¼ä¸–ç•Œæ¨¡å‹çš„æ–°è·¯å¾„ã€‚è®ºæ–‡çš„è´¡çŒ®åœ¨äºæ¢³ç†ã€æ•´åˆå¹¶å‰ç»äº†è¿™ä¸€æ–°å…´äº¤å‰é¢†åŸŸçš„å…¨è²Œï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æ¸…æ™°çš„è·¯æ ‡ã€‚",
            "timestamp": "2026-01-21T12:10:22.691624",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æœºå™¨äººå­¦ä¸­çš„å‘å±•è„‰ç»œä¸å…³é”®æ”¹è¿›\n\næœ¬æ–‡æ˜¯ã€Šæœºå™¨äººå­¦ä¸­çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹â€”â€”åº”ç”¨ã€ç ”ç©¶æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘ã€‹çš„ç»¼è¿°ã€‚å®ƒç³»ç»Ÿæ€§åœ°æ¢³ç†äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹ä½œä¸º**å…·èº«ä¸–ç•Œæ¨¡å‹**åœ¨æœºå™¨äººå­¦ä¸­çš„æ¼”è¿›ã€åº”ç”¨ä¸æŒ‘æˆ˜ã€‚å…¶å‘å±•è„‰ç»œå¹¶éå•ä¸€çº¿æ€§ï¼Œè€Œæ˜¯å›´ç»•**æ¨¡å‹æ¶æ„çš„æ¼”è¿›**å’Œ**åœ¨æœºå™¨äººå­¦ä¸­åº”ç”¨èŒƒå¼çš„æ·±åŒ–**ä¸¤ä¸ªæ ¸å¿ƒè½´çº¿å±•å¼€ã€‚\n\nä»¥ä¸‹å°†åŸºäºæ–‡ä¸­çº¿ç´¢ï¼Œæ¢³ç†è¯¥æ–¹å‘çš„å‘å±•è„‰ç»œï¼Œå¹¶ç€é‡åˆ†æå‡ ä¸ªå…³é”®å‘å±•æ–¹å‘ä¸Šçš„æ ¸å¿ƒæ”¹è¿›ã€‚\n\n### å‘å±•è„‰ç»œæ€»è§ˆ\n\nè¯¥é¢†åŸŸçš„å‘å±•å¯ä»¥æ¦‚æ‹¬ä¸ºä»**ç®€åŒ–çš„ã€åŸºäºçŠ¶æ€çš„åŠ¨åŠ›å­¦æ¨¡å‹**ï¼Œæ¼”è¿›åˆ°**é«˜ä¿çœŸã€åƒç´ çº§çš„è§†é¢‘ä¸–ç•Œæ¨¡å‹**ï¼Œå¹¶æœ€ç»ˆå°†å…¶æ·±åº¦æ•´åˆåˆ°æœºå™¨äººå­¦ä¹ çš„å…¨æµç¨‹ä¸­ã€‚æ¯ä¸€æ­¥çš„æ”¹è¿›éƒ½æ—¨åœ¨å…‹æœå‰ä»£æ–¹æ³•åœ¨**è¡¨è¾¾åŠ›ã€çœŸå®æ€§å’Œå¯æ‰©å±•æ€§**ä¸Šçš„å±€é™ã€‚\n\n```mermaid\ntimeline\n    title è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æœºå™¨äººå­¦ä¸­çš„å‘å±•è„‰ç»œ\n    section æ—©æœŸä¸–ç•Œæ¨¡å‹ (~2018å‰)\n        åŸºäºç‰©ç†çš„ä»¿çœŸå™¨ : ç®€åŒ–å‡è®¾å¯¼è‡´ä»¿çœŸä¸ç°å®å·®è·å¤§\n        é©¬å°”å¯å¤«çŠ¶æ€æ¨¡å‹ : åœ¨æ½œåœ¨ç©ºé—´ä¸­é¢„æµ‹ï¼Œ<br>ç¼ºä¹è§†è§‰ç»†èŠ‚å’Œç‰©ç†ä¸€è‡´æ€§\n    section è§†é¢‘é¢„æµ‹æ¨¡å‹å…´èµ· (2018-2021)\n        GAN/VAEè§†é¢‘é¢„æµ‹ : ç”Ÿæˆè´¨é‡æå‡ï¼Œä½†å­˜åœ¨æ¨¡å¼å´©æºƒã€<br>éš¾ä»¥å»ºæ¨¡å¤æ‚äº¤äº’\n        VQ-VAEä¸è‡ªå›å½’æ¨¡å‹ : åˆ©ç”¨ç¦»æ•£åŒ–æ½œåœ¨ç©ºé—´å’ŒTransformerï¼Œ<br>ç”Ÿæˆé•¿åº¦ã€è´¨é‡æå‡\n    section æ‰©æ•£/æµåŒ¹é…æ¨¡å‹ä¸»å¯¼ (2022è‡³ä»Š)\n        æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ : å®ç°é«˜ä¿çœŸã€å¯æ§è§†é¢‘åˆæˆï¼Œ<br>æˆä¸ºä¸»æµä¸–ç•Œæ¨¡å‹æ¶æ„\n        ä½œä¸ºæœºå™¨äººå…·èº«ä¸–ç•Œæ¨¡å‹ : åº”ç”¨äºæ•°æ®ç”Ÿæˆã€ç­–ç•¥è¯„ä¼°ã€<br>è§†è§‰è§„åˆ’ç­‰å…¨æµç¨‹\n    section å½“å‰æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘\n        è§£å†³ç‰©ç†ä¸€è‡´æ€§ã€æŒ‡ä»¤è·Ÿéšç­‰ : æ¢ç´¢ç‰©ç†å…ˆéªŒé›†æˆã€<br>ä¸ç¡®å®šæ€§é‡åŒ–ã€é•¿è§†é¢‘ç”Ÿæˆç­‰\n```\n\n### å…³é”®å‘å±•æ–¹å‘ä¸æ ¸å¿ƒæ”¹è¿›\n\n#### æ–¹å‘ä¸€ï¼šä¸–ç•Œæ¨¡å‹æ¶æ„çš„æ¼”è¿› â€”â€” ä»æ½œåœ¨çŠ¶æ€åˆ°é«˜ä¿çœŸè§†é¢‘\n\nè¿™ä¸€æ­¥çš„æ ¸å¿ƒæ”¹è¿›æ˜¯**è¡¨è¾¾åª’ä»‹**çš„å‡çº§ï¼Œä»æŠ½è±¡ã€ä¿¡æ¯å‹ç¼©çš„â€œçŠ¶æ€â€å˜ä¸ºä¸°å¯Œã€ç›´è§‚çš„â€œè§†é¢‘â€ï¼Œä»è€Œæ›´è‡ªç„¶åœ°å»ºæ¨¡ç‰©ç†ä¸–ç•Œçš„å¤æ‚æ—¶ç©ºåŠ¨æ€ã€‚\n\n1.  **é©¬å°”å¯å¤«çŠ¶æ€ä¸–ç•Œæ¨¡å‹**\n    *   **å‰èº«ä¸åŸºç¡€**ï¼šæ—©æœŸå·¥ä½œï¼ˆå¦‚Dreamer, TD-MPCï¼‰ä½¿ç”¨RNNã€RSSMæˆ–Transformeråœ¨**æ½œåœ¨çŠ¶æ€ç©ºé—´**ä¸­é¢„æµ‹æœªæ¥ã€‚å®ƒä»¬å°†è§‚æµ‹ï¼ˆå¦‚å›¾åƒï¼‰ç¼–ç ä¸ºç´§å‡‘çš„æ½œåœ¨å‘é‡ `s_t`ï¼Œå¹¶é¢„æµ‹ `s_{t+1}`ã€‚\n    *   **æ”¹è¿›ä¸å±€é™**ï¼šç›¸æ¯”åŸºäºç‰©ç†æ–¹ç¨‹çš„ä»¿çœŸå™¨ï¼Œè¿™äº›**å­¦ä¹ å‹æ¨¡å‹**æ•°æ®é©±åŠ¨ï¼Œèƒ½é€‚åº”æ›´å¤æ‚çš„åŠ¨æ€ã€‚ä½†**çŠ¶æ€è¡¨ç¤ºçš„ä¿¡æ¯å¯†åº¦æœ‰é™**ï¼Œéš¾ä»¥æ•æ‰ç»†ç²’åº¦çš„ç‰©ç†äº¤äº’ï¼ˆå¦‚å¯å˜å½¢ä½“æ¥è§¦ã€çº¹ç†å˜åŒ–ï¼‰ï¼Œä¸”ä¾èµ–äºç¼–ç å™¨çš„è´¨é‡ï¼Œå­˜åœ¨ä¿¡æ¯æŸå¤±ã€‚\n\n2.  **æ—©æœŸè§†é¢‘é¢„æµ‹æ¨¡å‹**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šåŸºäºGANã€VAEåŠå…¶å˜ä½“ï¼ˆå¦‚VQ-VAEï¼‰çš„è§†é¢‘é¢„æµ‹æ¨¡å‹ã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼šç›´æ¥åœ¨**åƒç´ ç©ºé—´**æˆ–**é‡åŒ–æ½œåœ¨ç©ºé—´**è¿›è¡Œé¢„æµ‹ï¼Œä¿ç•™äº†è§†è§‰ç»†èŠ‚ã€‚VQ-VAEç»“åˆè‡ªå›å½’Transformerï¼ˆå¦‚VideoGPTï¼‰èƒ½ç”Ÿæˆæ›´é•¿çš„åºåˆ—ã€‚\n    *   **é—ç•™é—®é¢˜**ï¼šç”Ÿæˆè´¨é‡ã€æ—¶é—´ä¸€è‡´æ€§å’Œç‰©ç†çœŸå®æ€§ä»ä¸è¶³ã€‚GANå­˜åœ¨æ¨¡å¼å´©æºƒï¼ŒVAEç±»æ¨¡å‹ç”Ÿæˆç»“æœå¯èƒ½æ¨¡ç³Šã€‚å®ƒä»¬å¯¹å¤æ‚ç‰©ç†äº¤äº’çš„å»ºæ¨¡èƒ½åŠ›æœ‰é™ã€‚\n\n3.  **æ‰©æ•£/æµåŒ¹é…è§†é¢‘æ¨¡å‹ï¼ˆå½“å‰ä¸»æµï¼‰**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šStable Video Diffusion, Sora, Gen-3 Alpha, Veo, Wanã€‚\n    *   **é©å‘½æ€§æ”¹è¿›**ï¼š\n        *   **é«˜ä¿çœŸä¸ç‰©ç†ä¸€è‡´æ€§**ï¼šé€šè¿‡åœ¨å¤§è§„æ¨¡äº’è”ç½‘è§†é¢‘æ•°æ®ä¸Šè®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿåˆæˆå…·æœ‰é€¼çœŸå…‰å½±ã€è¿åŠ¨æ•ˆæœå’Œ**å¤æ‚ç‰©ç†äº¤äº’**çš„è§†é¢‘ï¼Œè§£å†³äº†ä¼ ç»Ÿç‰©ç†ä»¿çœŸå™¨åœ¨å¯å˜å½¢ä½“æ¨¡æ‹Ÿç­‰æ–¹é¢çš„ç“¶é¢ˆã€‚\n        *   **å¼ºå¤§å¯æ§æ€§**ï¼šé€šè¿‡**åˆ†ç±»å™¨æ— å…³å¼•å¯¼**ç­‰æŠ€æœ¯ï¼Œæ¨¡å‹èƒ½æ¥å—å¤šæ¨¡æ€æ¡ä»¶è¾“å…¥ï¼ˆæ–‡æœ¬ã€å›¾åƒã€åŠ¨ä½œã€è½¨è¿¹ï¼‰ï¼Œå®ç°å¯¹ç”Ÿæˆå†…å®¹çš„ç²¾ç»†æ§åˆ¶ï¼Œä½¿å…¶èƒ½ä½œä¸ºå“åº”æœºå™¨äººåŠ¨ä½œçš„â€œæ¨¡æ‹Ÿå™¨â€ã€‚\n        *   **æˆä¸ºé€šç”¨ä¸–ç•Œæ¨¡å‹**ï¼šå…¶ç”Ÿæˆèƒ½åŠ›ä½¿å…¶ä¸å†ä»…ä»…æ˜¯â€œè§†é¢‘åˆæˆå·¥å…·â€ï¼Œè€Œæ˜¯èƒ½å¤Ÿ**é¢„æµ‹ç‰©ç†ä¸–ç•Œæ—¶ç©ºæ¼”å˜**çš„â€œå…·èº«ä¸–ç•Œæ¨¡å‹â€ï¼Œä¸ºæœºå™¨äººæä¾›äº†å‰æ‰€æœªæœ‰çš„é«˜ä¿çœŸã€å¯ç¼–ç¨‹çš„è™šæ‹Ÿç¯å¢ƒã€‚\n\n#### æ–¹å‘äºŒï¼šåœ¨æ¨¡ä»¿å­¦ä¹ ä¸­çš„åº”ç”¨ â€”â€” ä»æ•°æ®ç”Ÿæˆåˆ°ç­–ç•¥éª¨å¹²\n\nè¿™ä¸€æ­¥çš„æ”¹è¿›ä½“ç°åœ¨åˆ©ç”¨è§†é¢‘æ¨¡å‹**è§£å†³æœºå™¨äººæ•°æ®ç¨€ç¼ºçš„æ ¸å¿ƒç—›ç‚¹**ï¼Œå¹¶å°è¯•ç»Ÿä¸€æ„ŸçŸ¥ã€é¢„æµ‹ä¸å†³ç­–ã€‚\n\n1.  **ä½œä¸ºä½æˆæœ¬æ•°æ®ç”Ÿæˆå™¨**\n    *   **å‰èº«**ï¼šæ”¶é›†çœŸå®ä¸–ç•Œä¸“å®¶ç¤ºæ•™æ•°æ®æˆæœ¬æé«˜ï¼Œæ˜¯æ¨¡ä»¿å­¦ä¹ çš„ä¸»è¦ç“¶é¢ˆã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼šå¾®è°ƒé¢„è®­ç»ƒè§†é¢‘æ¨¡å‹ï¼ˆå¦‚Cosmos, Wanï¼‰ï¼Œä½¿å…¶èƒ½æ ¹æ®ä»»åŠ¡æè¿°ï¼ˆæ–‡æœ¬ï¼‰æˆ–åŠ¨ä½œæ¡ä»¶ï¼Œåˆæˆé«˜è´¨é‡çš„æœºå™¨äººä»»åŠ¡æ¼”ç¤ºè§†é¢‘ã€‚\n    *   **å…³é”®æŠ€æœ¯çªç ´**ï¼š\n        *   **åŠ¨ä½œä¼°è®¡**ï¼šä»ç”Ÿæˆè§†é¢‘ä¸­åæ¨æœºå™¨äººåŠ¨ä½œã€‚åˆ†ä¸º**æ¨¡å—åŒ–æ–¹æ³•**ï¼ˆä½¿ç”¨å§¿æ€è·Ÿè¸ªå™¨ã€å…‰æµä¼°è®¡ç‰©ä½“è½¨è¿¹åé‡å®šå‘ï¼‰å’Œ**ç«¯åˆ°ç«¯æ–¹æ³•**ï¼ˆè®­ç»ƒ**é€†åŠ¨åŠ›å­¦æ¨¡å‹**æˆ–**æ½œåœ¨åŠ¨ä½œæ¨¡å‹**ç›´æ¥ä»è§†é¢‘å¸§é¢„æµ‹åŠ¨ä½œï¼‰ã€‚\n        *   **è·¨å…·èº«è¿ç§»**ï¼šå¦‚Human2Robotå·¥ä½œï¼Œå°†äººç±»è§†é¢‘è½¬åŒ–ä¸ºæœºå™¨äººè§†é¢‘ï¼Œæå¤§æ‰©å±•äº†æ•°æ®æ¥æºã€‚\n\n2.  **ä½œä¸ºç­–ç•¥éª¨å¹²ç½‘ç»œ**\n    *   **å‰èº«**ï¼šä¼ ç»Ÿçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹é€šå¸¸ä½¿ç”¨ç‹¬ç«‹çš„è§†è§‰ç¼–ç å™¨å’Œç­–ç•¥å¤´ã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼šæå‡º**ç»Ÿä¸€è§†é¢‘-åŠ¨ä½œæ¨¡å‹**ï¼Œå°†è§†é¢‘é¢„æµ‹å’ŒåŠ¨ä½œç”Ÿæˆåœ¨ä¸€ä¸ªæ¨¡å‹ä¸­è¿›è¡Œè”åˆè®­ç»ƒã€‚\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šGR1, RPT, UVA, DreamVLAã€‚è¿™äº›æ¨¡å‹ä»¥è¯­è¨€æŒ‡ä»¤å’Œå½“å‰è§‚æµ‹ä¸ºæ¡ä»¶ï¼Œ**åŒæ—¶é¢„æµ‹æœªæ¥è§†é¢‘å¸§å’Œæœºå™¨äººåŠ¨ä½œ**ã€‚\n    *   **ä¼˜åŠ¿**ï¼šé€šè¿‡è§†é¢‘é¢„æµ‹çš„è¾…åŠ©ä»»åŠ¡ï¼Œç­–ç•¥èƒ½å­¦ä¹ åˆ°æ›´ä¸°å¯Œçš„ä¸–ç•ŒåŠ¨åŠ›å­¦è¡¨å¾ï¼Œæœ‰åŠ©äºæå‡ç­–ç•¥çš„æ³›åŒ–æ€§å’Œå¯è§£é‡Šæ€§ã€‚**PADã€Video Policy** ç­‰å·¥ä½œç›´æ¥é€‚é…é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼ˆU-Net/DiTï¼‰ä½œä¸ºéª¨å¹²ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚\n\n#### æ–¹å‘ä¸‰ï¼šåœ¨å¼ºåŒ–å­¦ä¹ ä¸è§„åˆ’ä¸­çš„åº”ç”¨ â€”â€” ä»å¥–åŠ±è®¾è®¡åˆ°è§†è§‰è§„åˆ’\n\nè¿™ä¸€æ­¥çš„æ”¹è¿›åœ¨äºåˆ©ç”¨è§†é¢‘æ¨¡å‹æä¾›æ›´ä¸°å¯Œã€æ›´å‡†ç¡®çš„**åŠ¨åŠ›å­¦æ¨¡å‹å’Œå¥–åŠ±ä¿¡å·**ï¼Œå¹¶å¼€å¯æ— éœ€é¢„å®šä¹‰åŠ¨ä½œç©ºé—´çš„**è§†è§‰è§„åˆ’**æ–°èŒƒå¼ã€‚\n\n1.  **ä½œä¸ºåŠ¨åŠ›å­¦ä¸å¥–åŠ±æ¨¡å‹**\n    *   **å‰èº«**ï¼šå¼ºåŒ–å­¦ä¹ éœ€è¦å®šä¹‰å¥–åŠ±å‡½æ•°ï¼Œä¸”æ ·æœ¬æ•ˆç‡ä½ã€‚åŸºäºæ¨¡å‹çš„RLä¾èµ–çš„åŠ¨åŠ›å­¦æ¨¡å‹å¾€å¾€ä¸å‡†ç¡®ã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼š\n        *   **åŠ¨åŠ›å­¦æ¨¡å‹**ï¼šå¦‚Dreamer 4ï¼Œè®­ç»ƒåŠ¨ä½œæ¡ä»¶è§†é¢‘æ¨¡å‹ä½œä¸ºé«˜ä¿çœŸåŠ¨åŠ›å­¦æ¨¡å‹ï¼Œåœ¨æ¨¡å‹å†…è¿›è¡Œç­–ç•¥å¾®è°ƒï¼Œå®ç°**â€œåœ¨å¯æ‰©å±•ä¸–ç•Œæ¨¡å‹ä¸­è®­ç»ƒæ™ºèƒ½ä½“â€**ã€‚\n        *   **å¥–åŠ±æ¨¡å‹**ï¼šåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹æœ¬èº«æä¾›å¥–åŠ±ä¿¡å·ã€‚ä¾‹å¦‚ï¼Œ**VIPER** ä½¿ç”¨è§†é¢‘é¢„æµ‹ä¼¼ç„¶ä½œä¸ºå¥–åŠ±ï¼Œ**Diffusion Reward** ä½¿ç”¨ç”Ÿæˆåˆ†å¸ƒçš„æ¡ä»¶ç†µä½œä¸ºå¥–åŠ±ï¼Œè¡¡é‡å½“å‰çŠ¶æ€ä¸ä¸“å®¶æ•°æ®çš„æ¥è¿‘ç¨‹åº¦ã€‚\n\n2.  **ä½œä¸ºè§†è§‰è§„åˆ’çš„æ ¸å¿ƒ**\n    *   **å‰èº«**ï¼šä¼ ç»Ÿè§„åˆ’éœ€è¦å®šä¹‰çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ï¼Œå¹¶åœ¨å…¶ä¸­è¿›è¡Œæœç´¢ã€‚\n    *   **èŒƒå¼é©æ–°**ï¼š**è§†è§‰è§„åˆ’**ç›´æ¥åˆæˆå®Œæˆä»»åŠ¡çš„è§†é¢‘åºåˆ—ï¼ˆè®¡åˆ’ï¼‰ï¼Œå†ä»ä¸­æå–è¡ŒåŠ¨æŒ‡å—ã€‚\n    *   **ä¸¤ç§è·¯å¾„**ï¼š\n        *   **åŠ¨ä½œå¼•å¯¼çš„è§„åˆ’**ï¼šå¦‚VLPã€MindJourneyï¼Œä½¿ç”¨LLM/VLMæå‡ºå€™é€‰åŠ¨ä½œæˆ–å­ä»»åŠ¡ï¼Œç”¨è§†é¢‘æ¨¡å‹æ¨æ¼”ç»“æœï¼Œè¯„ä¼°åé€‰æ‹©æœ€ä¼˜è®¡åˆ’ã€‚\n        *   **åŠ¨ä½œè‡ªç”±çš„è§„åˆ’**ï¼šå¦‚UniPiã€CLOVERï¼Œç›´æ¥ç”¨æ–‡æœ¬æ¡ä»¶è§†é¢‘æ¨¡å‹ç”Ÿæˆç›®æ ‡è§†é¢‘ï¼Œå°†å…¶å¸§ä½œä¸ºå›¾åƒå­ç›®æ ‡ï¼Œç”±å¦ä¸€ä¸ªæ¨¡å‹ï¼ˆå¦‚é€†åŠ¨åŠ›å­¦æ¨¡å‹ï¼‰ç”Ÿæˆå…·ä½“åŠ¨ä½œã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼š**è§„é¿äº†æ˜¾å¼çš„ã€æ‰‹å·¥å®šä¹‰çš„çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´**ï¼Œåˆ©ç”¨è§†é¢‘æ¨¡å‹å†…è•´çš„å¸¸è¯†å’Œç‰©ç†çŸ¥è¯†è¿›è¡Œè§„åˆ’ï¼Œåœ¨å¤æ‚ã€é•¿è§†é‡ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§æ½œåŠ›ã€‚\n\n#### æ–¹å‘å››ï¼šç­–ç•¥è¯„ä¼°ä¸å®‰å…¨æµ‹è¯• â€”â€” ä»çœŸå®éƒ¨ç½²åˆ°è™šæ‹Ÿè¯„ä¼°\n\nè¿™ä¸€æ­¥çš„æ”¹è¿›æ˜¯åˆ›å»ºäº†ä¸€ä¸ª**ä½æˆæœ¬ã€é«˜ä¿çœŸã€å¯é‡å¤çš„ç­–ç•¥è¯„ä¼°ä¸å®‰å…¨æµ‹è¯•å¹³å°**ã€‚\n\n*   **å‰èº«**ï¼šåœ¨çœŸå®æœºå™¨äººä¸Šè¯„ä¼°ç­–ç•¥è€—æ—¶ã€è€—åŠ›ã€æ˜‚è´µï¼›ç‰©ç†ä»¿çœŸå™¨è®¾ç½®å¤æ‚ï¼Œä¸”å­˜åœ¨ä»¿çœŸåˆ°ç°å®çš„å·®è·ã€‚\n*   **æ ¸å¿ƒæ”¹è¿›**ï¼šå°†è§†é¢‘æ¨¡å‹ä½œä¸º**ç­–ç•¥è¯„ä¼°æ¨¡æ‹Ÿå™¨**ã€‚\n*   **å…³é”®æŠ€æœ¯**ï¼š\n    *   **é—­ç¯æ»šé™**ï¼šå°†å¾…è¯„ä¼°ç­–ç•¥ä¸åŠ¨ä½œæ¡ä»¶è§†é¢‘æ¨¡å‹è¿æ¥ï¼Œä»åˆå§‹è§‚æµ‹å¼€å§‹ï¼Œäº¤æ›¿ç”ŸæˆåŠ¨ä½œå’Œé¢„æµ‹ä¸‹ä¸€å¸§è§‚æµ‹ï¼Œå½¢æˆå®Œæ•´è½¨è¿¹ã€‚\n    *   **å¤šè§†å›¾ä¸å†å² conditioning**ï¼šä¸ºäº†å‡å°‘å¹»è§‰å’Œæé«˜ä¸€è‡´æ€§ï¼Œé‡‡ç”¨å¤šç›¸æœºè§†å›¾å’Œè¿‡å»å¸§å†å²ä½œä¸ºæ¡ä»¶ã€‚\n    *   **æˆåŠŸç‡é¢„æµ‹ä¸æ’å**ï¼šä½¿ç”¨VLMsæˆ–äººå·¥å®šä¹‰è§„åˆ™å¯¹ç”Ÿæˆçš„è½¨è¿¹è¿›è¡Œè¯„åˆ†ï¼Œè®¡ç®—ä»»åŠ¡æˆåŠŸç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§†é¢‘æ¨¡å‹é¢„æµ‹çš„ç­–ç•¥æˆåŠŸç‡ä¸çœŸå®ä¸–ç•ŒæˆåŠŸç‡ä¹‹é—´å­˜åœ¨**é«˜ç›¸å…³æ€§**ï¼Œèƒ½æœ‰æ•ˆè¿›è¡Œç­–ç•¥æ’åã€‚\n    *   **å®‰å…¨ä¸æ³›åŒ–æµ‹è¯•**ï¼šå¯ä»¥å¿«é€Ÿç¼–è¾‘åˆå§‹åœºæ™¯ï¼ˆ",
            "timestamp": "2026-01-21T12:11:30.859866",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œå¯¹æ¯”åˆ†æ\n\næ ¹æ®æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œè¯¥ç»¼è¿°æ–‡ç« ï¼ˆ*Video Generation Models in Robotics: Applications, Research Challenges, Future Directions*ï¼‰æ˜ç¡®æŒ‡å‡ºäº†å…¶ä¸ç°æœ‰ç›¸å…³ç»¼è¿°çš„åŒºåˆ«ã€‚å…¶â€œå‰ä½œâ€ä¸»è¦æŒ‡æ­¤å‰å‘è¡¨çš„ã€ä¸»é¢˜ç›¸è¿‘çš„ç»¼è¿°æ€§æ–‡ç« ã€‚ä»¥ä¸‹è¡¨æ ¼è¯¦ç»†å¯¹æ¯”äº†è¯¥æ–‡ä¸å„ç¯‡å‰ä½œçš„æ ¸å¿ƒåŒºåˆ«ä¸ä¸»è¦æ”¹è¿›ã€‚\n\n| å‰ä½œ (Prior Surveys) | æ ¸å¿ƒå…³æ³¨ç‚¹ / è¦†ç›–èŒƒå›´ | ä¸æœ¬ç»¼è¿°çš„åŒºåˆ« | æœ¬ç»¼è¿°çš„ä¸»è¦æ”¹å–„ä¸è´¡çŒ® |\n| :--- | :--- | :--- | :--- |\n| Melnik et al. [54], Yu et al. [55] | **è§†é¢‘æ‰©æ•£æ¨¡å‹ç»¼è¿°**ï¼šèšç„¦äºä¸»æµæ¨¡å‹æ¶æ„ï¼ˆå¦‚U-Net, DiTï¼‰ã€‚ | èŒƒå›´è¾ƒçª„ï¼Œä¸»è¦è®¨è®º**è§†é¢‘ç”Ÿæˆæ¨¡å‹æœ¬èº«çš„æŠ€æœ¯æ¶æ„**ï¼Œæœªæ·±å…¥æ¢è®¨å…¶åœ¨**æœºå™¨äººå­¦**ä¸­çš„å…·ä½“åº”ç”¨ã€æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚ | **å°†ç„¦ç‚¹ä»é€šç”¨è§†é¢‘ç”Ÿæˆè½¬ç§»åˆ°æœºå™¨äººé¢†åŸŸ**ã€‚ä¸ä»…ä»‹ç»æ¨¡å‹ï¼Œæ›´ç³»ç»Ÿæ€§åœ°é˜è¿°äº†è§†é¢‘æ¨¡å‹ä½œä¸º**å…·èº«ä¸–ç•Œæ¨¡å‹**åœ¨æœºå™¨äººæ•°æ®ç”Ÿæˆã€ç­–ç•¥å­¦ä¹ ã€è¯„ä¼°ä¸è§„åˆ’ç­‰å››å¤§æ ¸å¿ƒåº”ç”¨ï¼Œå¹¶è¯¦ç»†åˆ†æäº†é¢†åŸŸç‰¹æœ‰çš„æŒ‘æˆ˜ï¼ˆå¦‚ç‰©ç†ä¸€è‡´æ€§ã€å®‰å…¨æ€§ï¼‰ã€‚ |\n| Ma et al. [56] | **å¯æ§è§†é¢‘ç”ŸæˆæŠ€æœ¯ç»¼è¿°**ï¼šæ€»ç»“åŸºäºæ–‡æœ¬ã€å›¾åƒã€è½¨è¿¹ç­‰æ¡ä»¶çš„ç”Ÿæˆæ–¹æ³•ã€‚ | ä¾§é‡äºè§†é¢‘ç”Ÿæˆçš„**æ§åˆ¶æŠ€æœ¯**ï¼Œå±äºæ–¹æ³•å­¦èŒƒç•´ï¼Œç¼ºä¹å¯¹**ä¸‹æ¸¸æœºå™¨äººä»»åŠ¡**å¦‚ä½•é›†æˆå’Œåˆ©ç”¨è¿™äº›æŠ€æœ¯çš„æ·±å…¥è®¨è®ºã€‚ | **å¼ºè°ƒåº”ç”¨é©±åŠ¨**ã€‚è¯¦ç»†åˆ†æäº†ä¸åŒæ¡ä»¶æ¨¡æ€ï¼ˆå¦‚æœºå™¨äººåŠ¨ä½œã€å¤šè§†è§’å›¾åƒï¼‰å¦‚ä½•æœåŠ¡äºå…·ä½“çš„æœºå™¨äººä»»åŠ¡ï¼ˆå¦‚æ¨¡ä»¿å­¦ä¹ ä¸­çš„åŠ¨ä½œé¢„æµ‹ã€å¼ºåŒ–å­¦ä¹ ä¸­çš„åŠ¨åŠ›å­¦å»ºæ¨¡ï¼‰ï¼Œå»ºç«‹äº†ä»ç”ŸæˆæŠ€æœ¯åˆ°æœºå™¨äººåº”ç”¨çš„æ¡¥æ¢ã€‚ |\n| Xing et al. [57] | **è§†é¢‘ç”Ÿæˆã€ç¼–è¾‘ä¸ç†è§£çš„æ ¸å¿ƒåº”ç”¨ç»¼è¿°**ï¼šè¦†ç›–è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å¤šä¸ªé€šç”¨åº”ç”¨é¢†åŸŸã€‚ | è¦†ç›–é¢†åŸŸ**å®½æ³›ä½†æµ…å±‚**ï¼Œæ¶‰åŠè§†é¢‘ç¼–è¾‘ã€å†…å®¹åˆ›ä½œç­‰ï¼Œå¯¹**æœºå™¨äºº**è¿™ä¸€å‚ç›´é¢†åŸŸçš„è®¨è®ºä¸å¤Ÿæ·±å…¥å’Œç³»ç»Ÿã€‚ | **æ·±åº¦èšç„¦äºæœºå™¨äººå­¦**ã€‚æä¾›äº†æœºå™¨äººæ“çºµï¼ˆManipulationï¼‰è¿™ä¸€æ ¸å¿ƒåœºæ™¯ä¸‹çš„è¯¦å°½åº”ç”¨åˆ†æï¼Œè¿™æ˜¯ç°æœ‰ç»¼è¿°æ‰€æœªæ·±å…¥æ¶‰åŠçš„ï¼ˆä¸åŒäºè‡ªåŠ¨é©¾é©¶[64-67]ï¼‰ã€‚ |\n| Li et al. [58], Ding et al. [59], Kong et al. [60], Zhang et al. [61] | **ä¸–ç•Œæ¨¡å‹ç»¼è¿°**ï¼šå¹¿æ³›è®¨è®ºåŒ…æ‹¬RNNã€Transformerã€è¯­è¨€æ¨¡å‹ã€é«˜æ–¯æº…å°„ç­‰å¤šç§æ¶æ„çš„ä¸–ç•Œæ¨¡å‹ã€‚ | è¦†ç›–çš„**æ¨¡å‹ç±»å‹éå¸¸å¹¿æ³›**ï¼Œä½†**æœªå°†ç”Ÿæˆå¼è§†é¢‘æ¨¡å‹ä½œä¸ºä¸€ç±»é‡è¦çš„ä¸–ç•Œæ¨¡å‹è¿›è¡Œé‡ç‚¹å’Œå…¨é¢çš„å®¡è§†**ã€‚è®¨è®ºç›¸å¯¹åˆ†æ•£ï¼Œæœªçªå‡ºè§†é¢‘æ¨¡å‹åœ¨**é«˜ä¿çœŸã€ç‰©ç†ä¸€è‡´æ¨¡æ‹Ÿ**æ–¹é¢çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚ | **æ˜ç¡®æå‡ºå¹¶æ·±å…¥è®ºè¯â€œè§†é¢‘æ¨¡å‹ä½œä¸ºå…·èº«ä¸–ç•Œæ¨¡å‹â€çš„æ ¸å¿ƒè®ºç‚¹**ã€‚ç³»ç»Ÿæ¢³ç†äº†ä»é©¬å°”å¯å¤«çŠ¶æ€æ¨¡å‹åˆ°è§†é¢‘è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼ˆV-JEPAï¼‰ï¼Œå†åˆ°æ‰©æ•£/æµåŒ¹é…è§†é¢‘æ¨¡å‹çš„æ¼”è¿›ï¼Œçªå‡ºäº†è§†é¢‘æ¨¡å‹åœ¨å…‹æœä¼ ç»Ÿç‰©ç†æ¨¡æ‹Ÿå™¨å’Œè¯­è¨€æ¨¡å‹å±€é™æ€§æ–¹é¢çš„**æŠ€æœ¯é£è·ƒ**ã€‚ |\n| Yang et al. [62] | **è§†é¢‘æ¨¡å‹æä¾›ç»Ÿä¸€ä¿¡æ¯è¡¨ç¤ºä¸ä»»åŠ¡æ¡†æ¶**ï¼šè®¨è®ºè§†é¢‘ä½œä¸ºä¸€ç§é€šç”¨è¡¨å¾çš„æ½œåŠ›ã€‚ | è§†è§’æ›´å**ç†è®ºä¸æ¡†æ¶**ï¼Œç¼ºä¹å¯¹**å…·ä½“æœºå™¨äººåº”ç”¨åœºæ™¯ã€å®é™…æŒ‘æˆ˜ï¼ˆå¦‚å¹»è§‰ã€æˆæœ¬ï¼‰å’Œè¯¦ç»†æŠ€æœ¯è·¯å¾„**çš„å±•å¼€ã€‚ | **æä¾›äº†ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´å›¾è°±**ã€‚ä¸ä»…è®¤å¯å…¶ç»Ÿä¸€æ¡†æ¶çš„æ½œåŠ›ï¼Œæ›´ç”¨å¤§é‡ç¯‡å¹…ï¼ˆç¬¬3ã€5èŠ‚ï¼‰è¯¦ç»†æ‹†è§£äº†æ¯ä¸ªåº”ç”¨å¦‚ä½•å®ç°ã€é¢ä¸´å“ªäº›å…·ä½“é—®é¢˜ï¼ˆå¦‚é•¿è§†é¢‘ç”Ÿæˆã€åŠ¨ä½œä¼°è®¡ä¸å‡†ï¼‰ï¼Œå¹¶æå‡ºäº†æœ‰é’ˆå¯¹æ€§çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚ |\n| Yue et al. [63] | **è§†é¢‘æ¨¡å‹èƒ½åŠ›æ¼”è¿›ä¸æœªæ¥æ½œåŠ›**ï¼šä¸»è¦é€šè¿‡å†å²è§†è§’å›é¡¾è§†é¢‘æ¨¡å‹çš„å‘å±•ã€‚ | ä¾§é‡äº**å†å²å›é¡¾ä¸å®è§‚èƒ½åŠ›å±•æœ›**ï¼Œç¼ºä¹å¯¹**å½“å‰åœ¨æœºå™¨äººé¢†åŸŸå·²è½åœ°çš„åº”ç”¨ã€å­˜åœ¨çš„ç“¶é¢ˆä»¥åŠè§£å†³è·¯å¾„**çš„ç»†è‡´åˆ†æã€‚ | **å¼ºè°ƒç°çŠ¶åˆ†æä¸é—®é¢˜è§£å†³**ã€‚æœ¬æ–‡æ˜¯ä¸€ç¯‡å…¸å‹çš„â€œç»¼è¿°â€ï¼Œç»“æ„ä¸ŠåŒ…å«è¯¦å°½çš„èƒŒæ™¯ã€åº”ç”¨åˆ†ç±»ã€è¯„ä¼°æ–¹æ³•ã€**å¼€æ”¾æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘**ï¼Œå…·æœ‰å¾ˆå¼ºçš„**é—®é¢˜å¯¼å‘æ€§å’ŒæŒ‡å¯¼æ€§**ï¼Œæ—¨åœ¨æ¨åŠ¨è¯¥é¢†åŸŸç ”ç©¶è§£å†³å®é™…é›†æˆéšœç¢ã€‚ |\n\n### **æ€»ç»“ï¼šæœ¬ç»¼è¿°çš„æ ¸å¿ƒåˆ›æ–°ä¸ä»·å€¼**\n\n1.  **å®šä½åˆ›æ–°**ï¼šé¦–æ¬¡å¯¹**ç”Ÿæˆå¼è§†é¢‘æ¨¡å‹åœ¨æœºå™¨äººå­¦ï¼ˆç‰¹åˆ«æ˜¯æœºå™¨äººæ“çºµï¼‰ä¸­çš„åº”ç”¨**è¿›è¡Œäº†å…¨é¢ã€ç³»ç»Ÿçš„ç»¼è¿°ï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡çŒ®çš„ç©ºç™½ã€‚\n2.  **è§†è§’åˆ›æ–°**ï¼šæ˜ç¡®æå‡ºå¹¶è´¯ç©¿å…¨æ–‡çš„â€œ**è§†é¢‘æ¨¡å‹ä½œä¸ºé«˜ä¿çœŸå…·èº«ä¸–ç•Œæ¨¡å‹**â€è¿™ä¸€æ ¸å¿ƒè§†è§’ï¼Œæ·±åˆ»é˜è¿°äº†å…¶ç›¸å¯¹äºä¼ ç»Ÿç‰©ç†æ¨¡æ‹Ÿå™¨å’Œè¯­è¨€æŠ½è±¡æ¨¡å‹çš„**æŠ€æœ¯ä¼˜åŠ¿**ï¼ˆå¦‚å¯å˜å½¢ä½“æ¨¡æ‹Ÿã€ç»†ç²’åº¦æ—¶ç©ºåŠ¨æ€å»ºæ¨¡ï¼‰ã€‚\n3.  **ç»“æ„åˆ›æ–°**ï¼šæ„å»ºäº†æ¸…æ™°çš„åº”ç”¨åˆ†ç±»æ¡†æ¶ï¼ˆæ•°æ®ç”Ÿæˆã€å¼ºåŒ–å­¦ä¹ å»ºæ¨¡ã€ç­–ç•¥è¯„ä¼°ã€è§†è§‰è§„åˆ’ï¼‰ï¼Œå¹¶é…å¥—æå‡ºäº†å¯¹åº”çš„**è¯„ä¼°æŒ‡æ ‡ã€åŸºå‡†æµ‹è¯•ä»¥åŠåå¤§æ ¸å¿ƒæŒ‘æˆ˜**ï¼Œä¸ºé¢†åŸŸç ”ç©¶æä¾›äº†æ¸…æ™°çš„è·¯çº¿å›¾ã€‚\n4.  **å†…å®¹æ·±åº¦**ï¼šä¸ä»…åˆ—ä¸¾åº”ç”¨ï¼Œæ›´æ·±å…¥åˆ†æäº†æ¯ä¸ªåº”ç”¨èƒŒåçš„æŠ€æœ¯åŸç†ï¼ˆå¦‚éšå¼vsæ˜¾å¼ä¸–ç•Œæ¨¡å‹ã€ç«¯åˆ°ç«¯vsæ¨¡å—åŒ–åŠ¨ä½œä¼°è®¡ï¼‰ã€å­˜åœ¨çš„å±€é™æ€§ï¼ˆå¦‚å¹»è§‰ã€æŒ‡ä»¤è·Ÿéšå·®ï¼‰ä»¥åŠå‰æ²¿çš„è§£å†³æ–¹æ¡ˆå°è¯•ï¼Œä¿¡æ¯å¯†åº¦å’Œå®ç”¨æ€§æé«˜ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼Œæœ¬ç»¼è¿°ä¸å‰ä½œçš„æœ€å¤§åŒºåˆ«åœ¨äºå…¶é«˜åº¦çš„é¢†åŸŸèšç„¦æ€§ã€æŠ€æœ¯æ·±åº¦å’Œé—®é¢˜å¯¼å‘æ€§ã€‚å®ƒä¸ä»…ä»…æ˜¯å¯¹è§†é¢‘æ¨¡å‹æŠ€æœ¯çš„åˆä¸€æ¬¡æ€»ç»“ï¼Œæ›´æ˜¯æ¨åŠ¨è¯¥æŠ€æœ¯å‘æœºå™¨äººè¿™ä¸€å®‰å…¨å…³é”®é¢†åŸŸå¯é ã€å¯ä¿¡é›†æˆçš„å­¦æœ¯æŒ‡å—ã€‚**",
            "timestamp": "2026-01-21T12:12:11.670903",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†æ ¹æ®æ‚¨æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œå¯¹å…¶ä¸­æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•è¿›è¡Œè¯¦ç»†åˆ†æã€‚\n\n## è®ºæ–‡å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\nè¿™ç¯‡ç»¼è¿°è®ºæ–‡ã€ŠVideo Generation Models in Roboticsã€‹ç³»ç»Ÿæ€§åœ°å›é¡¾äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æœºå™¨äººå­¦ä¸­çš„åº”ç”¨ã€æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘ã€‚è®ºæ–‡æœ¬èº«å¹¶æœªæå‡ºå…¨æ–°çš„æŠ€æœ¯æ–¹æ³•ï¼Œè€Œæ˜¯å¯¹ç°æœ‰æŠ€æœ¯è¿›è¡Œäº†å…¨é¢çš„æ¢³ç†ã€åˆ†ç±»å’Œæ€»ç»“ã€‚å…¶æ ¸å¿ƒä»·å€¼åœ¨äºæ„å»ºäº†ä¸€ä¸ªæ¸…æ™°çš„æŠ€æœ¯æ¡†æ¶ï¼Œå¹¶æŒ‡å‡ºäº†å…³é”®çš„æŠ€æœ¯ç“¶é¢ˆä¸æœªæ¥ç ”ç©¶æ–¹å‘ã€‚\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†æ¢³ç†äº†è®ºæ–‡ä¸­é‡ç‚¹è®¨è®ºçš„**æ ¸å¿ƒæŠ€æœ¯æ–¹æ³•ç±»åˆ«**ï¼Œå¹¶ä¾æ®è®ºæ–‡å†…å®¹è¡¥å……äº†å…¶æ•°å­¦åŸç†ã€æŠ€æœ¯ç»†èŠ‚å’Œå…³é”®å‚æ•°ã€‚\n\n### æ ¸å¿ƒæŠ€æœ¯æ–¹æ³•åˆ†ç±»ä¸ç»†èŠ‚\n\n| æ–¹æ³•ç±»åˆ« | æ ¸å¿ƒå­ç±»/æ¶æ„ | æ•°å­¦åŸç†ä¸æŠ€æœ¯ç»†èŠ‚ | å…³é”®å‚æ•°/ç»„ä»¶ |\n| :--- | :--- | :--- | :--- |\n| **1. è§†é¢‘ä¸–ç•Œæ¨¡å‹æ¶æ„** | **A. åŸºäºæ‰©æ•£/æµåŒ¹é…çš„æ¨¡å‹** | **1. å‰å‘è¿‡ç¨‹ï¼ˆåŠ å™ªï¼‰**ï¼š<br>  ç»™å®šå¹²å‡€æ•°æ® $\\mathbf{x}_0$ï¼Œåœ¨ $T$ æ­¥å†…é€æ­¥æ·»åŠ é«˜æ–¯å™ªå£°ï¼š<br>  $q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I})$<br>  å…¶ä¸­ $\\{\\beta_t\\}_{t=1}^T$ æ˜¯é¢„å®šä¹‰çš„å™ªå£°è°ƒåº¦ã€‚<br><br>  **2. åå‘è¿‡ç¨‹ï¼ˆå»å™ªï¼‰**ï¼š<br>  å­¦ä¹ ä¸€ä¸ªå‚æ•°åŒ–çš„å»å™ªç½‘ç»œ $p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)$ æ¥è¿‘ä¼¼çœŸå®åéªŒï¼š<br>  $p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_\\theta(\\mathbf{x}_t, t), \\Sigma_\\theta(\\mathbf{x}_t, t))$<br><br>  **3. è®­ç»ƒç›®æ ‡ï¼ˆç®€åŒ–ï¼‰**ï¼š<br>  é¢„æµ‹æ·»åŠ åˆ°æ•°æ®ä¸­çš„å™ªå£° $\\epsilon$ï¼š<br>  $L_\\epsilon = \\mathbb{E}_{\\mathbf{x}_0, t, \\epsilon}[\\| \\epsilon - \\epsilon_\\theta(\\mathbf{x}_t, t) \\|^2]$<br>  æˆ–é¢„æµ‹é€Ÿåº¦ $\\bm{v}$ï¼š<br>  $L_{\\bm{v}} = \\mathbb{E}_{\\mathbf{x}_0, t, \\bm{v}_t}[\\| \\bm{v}_t - \\bm{v}_\\theta(\\mathbf{x}_t, t) \\|^2]$ï¼Œå…¶ä¸­ $\\bm{v} = \\sqrt{\\bar{\\alpha}_t}\\epsilon - \\sqrt{1-\\bar{\\alpha}_t}\\mathbf{x}_0$ã€‚ | - **å™ªå£°è°ƒåº¦** $\\{\\beta_t\\}$ æˆ– $\\{\\alpha_t=1-\\beta_t\\}$<br>- **æ€»æ­¥æ•° $T$** (é€šå¸¸æ•°ç™¾è‡³æ•°åƒæ­¥)<br>- **æ¡ä»¶å¼•å¯¼å°ºåº¦ $\\omega$** (åœ¨åˆ†ç±»å™¨æ— å…³å¼•å¯¼ä¸­) |\n| | **B. è§†é¢‘è”åˆåµŒå…¥é¢„æµ‹æ¶æ„** | **1. æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œé¢„æµ‹ï¼Œè€Œéåƒç´ ç©ºé—´ï¼Œä»¥å­¦ä¹ é«˜çº§è¯­ä¹‰è¡¨ç¤ºã€‚<br><br>  **2. è®­ç»ƒæµç¨‹**ï¼š<br>  - **ç¼–ç **ï¼šä½¿ç”¨å­¦ç”Ÿç¼–ç å™¨ $E_\\theta$ å¤„ç†æ©ç è§†é¢‘ $x$ï¼Œä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡çš„æ•™å¸ˆç¼–ç å™¨ $E_{\\bar{\\theta}}$ å¤„ç†æœªæ©ç è§†é¢‘ $y$ã€‚<br>  - **é¢„æµ‹**ï¼šé¢„æµ‹å™¨ç½‘ç»œ $P_\\phi$ åŸºäºæ©ç ä»¤ç‰Œ $\\Delta_y$ å’Œ $E_\\theta(x)$ é¢„æµ‹ $y$ çš„æ½œåœ¨è¡¨ç¤ºã€‚<br>  - **æŸå¤±å‡½æ•°**ï¼šæœ€å°åŒ– $L1$ æŸå¤±ï¼š<br>    $\\min_{\\theta,\\phi,\\Delta_y} \\| P_\\phi(\\Delta_y, E_\\theta(x)) - \\text{sg}(E_{\\bar{\\theta}}(y)) \\|_1$<br>    å…¶ä¸­ $\\text{sg}(\\cdot)$ è¡¨ç¤ºåœæ­¢æ¢¯åº¦æ“ä½œã€‚ | - **æ©ç ç­–ç•¥** (æ—¶ç©ºæ©ç æ¯”ä¾‹ã€æ¨¡å¼)<br>- **æ•™å¸ˆç½‘ç»œåŠ¨é‡ç³»æ•°**<br>- **æ½œåœ¨ç©ºé—´ç»´åº¦** |\n| **2. æ¡ä»¶æ§åˆ¶æœºåˆ¶** | **A. åˆ†ç±»å™¨æ— å…³å¼•å¯¼** | **æ ¸å¿ƒå…¬å¼**ï¼šé€šè¿‡æ’å€¼æ¡ä»¶æ¨¡å‹å’Œæ— æ¡ä»¶æ¨¡å‹çš„è¾“å‡ºæ¥å¢å¼ºæ¡ä»¶æ§åˆ¶ï¼š<br> $\\tilde{\\epsilon}_\\theta(x_t, t, y) = (1+w) \\epsilon_\\theta(x_t, t, y) - w \\epsilon_\\theta(x_t, t)$<br> å…¶ä¸­ $y$ æ˜¯æ¡ä»¶ï¼ˆå¦‚æ–‡æœ¬ï¼‰ï¼Œ$w$ æ˜¯å¼•å¯¼å°ºåº¦ã€‚$w$ è¶Šå¤§ï¼Œç”Ÿæˆç»“æœä¸æ¡ä»¶ $y$ çš„å¯¹é½è¶Šå¼ºï¼Œä½†å¯èƒ½ç‰ºç‰²å¤šæ ·æ€§ã€‚ | **å¼•å¯¼å°ºåº¦ $w$** (é€šå¸¸ $w \\in [1.0, 7.5]$) |\n| | **B. å¤šæ¨¡æ€æ¡ä»¶æ³¨å…¥** | **1. é€šé“æ‹¼æ¥**ï¼šå°†æ¡ä»¶ä¿¡å·ï¼ˆå¦‚å›¾åƒã€æ·±åº¦å›¾ã€è½¨è¿¹åæ ‡å›¾ï¼‰ä¸å™ªå£°æ½œåœ¨è¡¨ç¤ºåœ¨é€šé“ç»´åº¦æ‹¼æ¥ã€‚é€‚ç”¨äºç©ºé—´å¯¹é½çš„æ¡ä»¶ã€‚<br><br>  **2. äº¤å‰æ³¨æ„åŠ›**ï¼šå°†è§†é¢‘ç‰¹å¾ä½œä¸ºæŸ¥è¯¢(Q)ï¼Œæ¡ä»¶åµŒå…¥ï¼ˆå¦‚æ–‡æœ¬CLIPç‰¹å¾ï¼‰ä½œä¸ºé”®(K)å’Œå€¼(V)ã€‚<br>  $\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$<br>  é€‚ç”¨äºè¯­ä¹‰ã€éç©ºé—´æ¡ä»¶ã€‚<br><br>  **3. è‡ªé€‚åº”å½’ä¸€åŒ–**ï¼šä½¿ç”¨æ¡ä»¶å‘é‡ï¼ˆå¦‚ç›®æ ‡å¸§ç‡ã€è¿åŠ¨å¼ºåº¦ï¼‰æ¥è°ƒåˆ¶å½’ä¸€åŒ–å±‚ï¼ˆå¦‚GroupNormï¼‰çš„ç¼©æ”¾å’Œåç½®å‚æ•° $\\gamma, \\beta$ã€‚ | - **æ¡ä»¶ç¼–ç å™¨** (å¦‚CLIPæ–‡æœ¬ç¼–ç å™¨ã€VAEå›¾åƒç¼–ç å™¨)<br>- **æ³¨æ„åŠ›å¤´æ•°ã€ç»´åº¦**<br>- **ControlNetç­‰é€‚é…å™¨ç½‘ç»œ** |\n| **3. æœºå™¨äººåº”ç”¨ä¸­çš„å…³é”®æŠ€æœ¯** | **A. ä»è§†é¢‘ä¸­ä¼°è®¡åŠ¨ä½œ** | **1. æ½œåœ¨åŠ¨ä½œæ¨¡å‹**ï¼š<br>  å­¦ä¹ ä¸€ä¸ªç¦»æ•£çš„æ½œåœ¨åŠ¨ä½œç©ºé—´ï¼ˆå¦‚ä½¿ç”¨VQ-VAEï¼‰ã€‚ç¼–ç å™¨å°†å¸§å¯¹ $(o_t, o_{t+1})$ æ˜ å°„ä¸ºç¦»æ•£ç  $z_t$ï¼Œè§£ç å™¨ä» $(o_t, z_t)$ é‡å»º $o_{t+1}$ã€‚æŸå¤±ä¸ºé‡å»ºæŸå¤± + VQ æ‰¿è¯ºæŸå¤±ã€‚<br><br>  **2. é€†åŠ¨åŠ›å­¦æ¨¡å‹**ï¼š<br>  ç›‘ç£å­¦ä¹ ï¼Œç›´æ¥å­¦ä¹ æ˜ å°„ $f: (o_t, o_{t+1}) \\rightarrow a_t$ã€‚å¯ä½¿ç”¨æ‰©æ•£æ¨¡å‹å‚æ•°åŒ–ï¼š<br>  $p_\\phi(a_t | o_t, o_{t+1})$ï¼Œé€šè¿‡å»å™ªè¿‡ç¨‹å›å½’åŠ¨ä½œã€‚ | - **VQ-VAEçš„ç æœ¬å¤§å° $K$**<br>- **æ½œåœ¨åŠ¨ä½œç»´åº¦**<br>- **é€†åŠ¨åŠ›å­¦æ¨¡å‹æ¶æ„** (MLP, Diffusion) |\n| | **B. è§†è§‰è§„åˆ’** | **1. åŸºäºé‡‡æ ·çš„ä¼˜åŒ–ï¼ˆå¦‚CEMï¼‰**ï¼š<br>  a. ä»åŠ¨ä½œå…ˆéªŒåˆ†å¸ƒï¼ˆå¦‚é«˜æ–¯åˆ†å¸ƒï¼‰é‡‡æ · $N$ ä¸ªåŠ¨ä½œåºåˆ—ææ¡ˆ $\\{\\tau_i\\}$ã€‚<br>  b. ç”¨è§†é¢‘ä¸–ç•Œæ¨¡å‹ $p(o_{1:H} \\vert o_0, \\tau_i)$ ç”Ÿæˆå¯¹åº”è§†é¢‘è½¨è¿¹ã€‚<br>  c. ç”¨ä»·å€¼å‡½æ•° $V(o_{1:H})$ (å¦‚ä¸ç›®æ ‡å›¾åƒçš„ç›¸ä¼¼åº¦) è¯„ä¼°è½¨è¿¹ã€‚<br>  d. é€‰å– top-$K$ è½¨è¿¹ï¼Œç”¨å…¶åŠ¨ä½œæ›´æ–°å…ˆéªŒåˆ†å¸ƒï¼ˆå‡å€¼å’Œæ–¹å·®ï¼‰ï¼Œè¿­ä»£ä¼˜åŒ–ã€‚<br><br>  **2. åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–**ï¼š<br>  é€šè¿‡è§†é¢‘æ¨¡å‹å¯¹åŠ¨ä½œåºåˆ— $\\tau$ å¯å¾®ï¼Œå¯ç›´æ¥é€šè¿‡æ¢¯åº¦ä¸‹é™æœ€å°åŒ–æŸå¤±å‡½æ•° $L(\\tau) = \\text{Distance}(G(o_0, \\tau), o_{\\text{goal}})$ï¼Œå…¶ä¸­ $G$ æ˜¯è§†é¢‘ç”Ÿæˆå™¨ã€‚ | - **é‡‡æ ·æ•° $N$**<br>- **ç²¾è‹±é›†å¤§å° $K$**<br>- **è§„åˆ’æ­¥é•¿ $H$**<br>- **ä»·å€¼å‡½æ•° $V$** (CLIPç›¸ä¼¼åº¦ã€VLMsè¯„åˆ†) |\n| **4. åº”å¯¹æŒ‘æˆ˜çš„æ½œåœ¨æŠ€æœ¯æ–¹å‘** | **A. æé«˜ç‰©ç†ä¸€è‡´æ€§** | **ç‰©ç†å…ˆéªŒæ³¨å…¥**ï¼š<br>  1. **åŸºäºç‰©ç†çš„æ¸²æŸ“**ï¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥åˆšä½“/æµä½“åŠ¨åŠ›å­¦æ¨¡æ‹Ÿå™¨ï¼Œæä¾›ä¸­é—´ç‰©ç†çŠ¶æ€ä½œä¸ºæ¡ä»¶ã€‚<br>  2. **ç‰©ç†å¼•å¯¼æŸå¤±**ï¼šåœ¨è®­ç»ƒæˆ–æ¨ç†æ—¶ï¼Œå¢åŠ åŸºäºç‰©ç†çº¦æŸçš„æŸå¤±é¡¹ï¼Œå¦‚è´¨é‡å®ˆæ’ã€èƒ½é‡å®ˆæ’ã€ä¸å¯ç©¿é€æ€§çº¦æŸã€‚<br>  3. **ç¬¦å·ç‰©ç†ä¸LLM/VLMç»“åˆ**ï¼šåˆ©ç”¨LLM/VLMè§£æåœºæ™¯çš„ç‰©ç†å±æ€§ï¼ˆæè´¨ã€è´¨é‡ï¼‰ï¼Œå¹¶ç”Ÿæˆæ›´ç¬¦åˆç‰©ç†çš„æ–‡æœ¬æè¿°æ¥å¼•å¯¼è§†é¢‘ç”Ÿæˆã€‚ | - **ç‰©ç†å¼•æ“å‚æ•°** (æ‘©æ“¦ç³»æ•°ã€å¼¹æ€§ã€å¯†åº¦)<br>- **ç‰©ç†çº¦æŸæŸå¤±æƒé‡**<br>- **LLMæç¤ºå·¥ç¨‹æ¨¡æ¿** |\n| | **B. é•¿è§†é¢‘ç”Ÿæˆ** | **1. è‡ªå›å½’",
            "timestamp": "2026-01-21T12:13:17.795510",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†æ ¹æ®æ‚¨æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œå¯¹å…¶ä¸­æ¶‰åŠçš„è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†è¿›è¡Œè¯¦ç»†åˆ†æã€‚\n\n## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\nè¿™ç¯‡é¢˜ä¸ºã€ŠVideo Generation Models in Robotics: Applications, Research Challenges, Future Directionsã€‹çš„ç»¼è¿°æ€§è®ºæ–‡ï¼Œç³»ç»Ÿæ€§åœ°å›é¡¾äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æœºå™¨äººé¢†åŸŸçš„åº”ç”¨ã€æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘ã€‚è®ºæ–‡æœ¬èº«å¹¶æœªæå‡ºæ–°çš„æŒ‡æ ‡æˆ–æ•°æ®é›†ï¼Œè€Œæ˜¯å¯¹ç°æœ‰ç ”ç©¶ä¸­çš„**è¯„ä»·æŒ‡æ ‡**å’Œ**åŸºå‡†æµ‹è¯•ï¼ˆBenchmarksï¼‰** è¿›è¡Œäº†æ¢³ç†å’Œæ€»ç»“ã€‚å…¶æ ¸å¿ƒä»·å€¼åœ¨äºæä¾›äº†ä¸€ä¸ªå…¨é¢çš„åˆ†ç±»æ¡†æ¶å’Œæ‰¹åˆ¤æ€§åˆ†æã€‚\n\nä»¥ä¸‹æ˜¯æ ¹æ®è®ºæ–‡å†…å®¹æ•´ç†çš„æŒ‡æ ‡ä¸åŸºå‡†æµ‹è¯•è¯¦æƒ…ï¼š\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡ï¼ˆMetricsï¼‰\n\nè®ºæ–‡å°†è¯„ä»·æŒ‡æ ‡åˆ†ä¸ºä¸¤å¤§ç±»ï¼š**å¸§çº§æŒ‡æ ‡**å’Œ**æ—¶ç©ºä¸€è‡´æ€§æŒ‡æ ‡**ã€‚æ­¤å¤–ï¼Œè¿˜å¼ºè°ƒäº†**é¢å‘åº”ç”¨çš„æŒ‡æ ‡**çš„é‡è¦æ€§ã€‚\n\n#### 1. å¸§çº§æŒ‡æ ‡ï¼ˆFrame-level Metricsï¼‰\nè¿™äº›æŒ‡æ ‡è¯„ä¼°å•ä¸ªè§†é¢‘å¸§çš„è§†è§‰è´¨é‡ã€‚\n\n| æŒ‡æ ‡åç§° | ç¼©å†™ | æ ¸å¿ƒåŸç† | ä¼˜ç‚¹ | ç¼ºç‚¹/å±€é™æ€§ | åœ¨æœºå™¨äººé¢†åŸŸçš„é€‚ç”¨æ€§ |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **å³°å€¼ä¿¡å™ªæ¯”** | PSNR | è®¡ç®—ç”Ÿæˆå¸§ä¸çœŸå®å¸§åƒç´ å€¼ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼Œå†è½¬åŒ–ä¸ºå¯¹æ•°å°ºåº¦ã€‚å€¼è¶Šé«˜ï¼Œå¤±çœŸè¶Šå°ã€‚ | è®¡ç®—ç®€å•ï¼Œæœ‰æ˜ç¡®çš„ç‰©ç†æ„ä¹‰ï¼ˆä¿¡å·ä¸å™ªå£°çš„åŠŸç‡æ¯”ï¼‰ã€‚ | **ä¸äººç±»æ„ŸçŸ¥ç›¸å…³æ€§å¼±**ã€‚å¯¹åƒç´ çš„å¾®å°åç§»éå¸¸æ•æ„Ÿï¼Œå³ä½¿è§†è§‰ä¸Šæ— å·®å¼‚ï¼ŒPSNRä¹Ÿå¯èƒ½å¾ˆä½ã€‚æ— æ³•è¯„ä¼°é«˜çº§è¯­ä¹‰ã€‚ | **è¾ƒä½**ã€‚æœºå™¨äººä»»åŠ¡æ›´å…³æ³¨é«˜çº§è¯­ä¹‰å’Œç‰©ç†ä¸€è‡´æ€§ï¼Œè€Œéåƒç´ çº§ç²¾ç¡®åŒ¹é…ã€‚ |\n| **ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°** | SSIM | ä»**äº®åº¦**ã€**å¯¹æ¯”åº¦**ã€**ç»“æ„**ä¸‰ä¸ªç»´åº¦æ¯”è¾ƒä¸¤å¹…å›¾åƒï¼Œæ›´ç¬¦åˆäººçœ¼è§†è§‰ç³»ç»Ÿã€‚ | æ¯”PSNRæ›´è´´è¿‘äººç±»ä¸»è§‚æ„Ÿå—ï¼Œèƒ½æ•æ‰å›¾åƒçš„ç»“æ„ä¿¡æ¯ã€‚ | ä»ç„¶æ˜¯**å…¨å‚è€ƒ**æŒ‡æ ‡ï¼Œéœ€è¦çœŸå®å¸§ä½œä¸ºå¯¹æ¯”ã€‚å¯¹æŸäº›ç±»å‹çš„å¤±çœŸï¼ˆå¦‚æ¨¡ç³Šï¼‰å¯èƒ½ä¸æ•æ„Ÿã€‚ | **ä¸­ç­‰**ã€‚å¯ç”¨äºè¯„ä¼°ç”Ÿæˆç”»é¢çš„é€¼çœŸåº¦ï¼Œä½†ä¸è¶³ä»¥è¡¡é‡ç‰©ç†æ­£ç¡®æ€§æˆ–ä»»åŠ¡å®Œæˆåº¦ã€‚ |\n| **CLIPç›¸ä¼¼åº¦å¾—åˆ†** | CLIP Score | ä½¿ç”¨CLIPæ¨¡å‹å°†å›¾åƒç¼–ç ä¸ºç‰¹å¾å‘é‡ï¼Œè®¡ç®—ç”Ÿæˆå¸§ä¸ç›®æ ‡æè¿°ï¼ˆæ–‡æœ¬ï¼‰æˆ–å‚è€ƒå¸§ï¼ˆå›¾åƒï¼‰åœ¨ç‰¹å¾ç©ºé—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ | **æ— éœ€æˆå¯¹çš„çœŸå®å¸§**ï¼Œå¯ä»¥è¯„ä¼°è¯­ä¹‰å¯¹é½ã€‚ä¸äººç±»å¯¹è¯­ä¹‰ä¸€è‡´æ€§çš„åˆ¤æ–­ç›¸å…³æ€§é«˜ã€‚ | ä¾èµ–äºCLIPæ¨¡å‹æœ¬èº«çš„èƒ½åŠ›å’Œåè§ã€‚å¯èƒ½æ— æ³•æ•æ‰ç»†ç²’åº¦çš„ç‰©ç†ç»†èŠ‚ã€‚ | **é«˜**ã€‚éå¸¸é€‚åˆè¯„ä¼°ç”Ÿæˆå†…å®¹æ˜¯å¦ä¸è¯­è¨€æŒ‡ä»¤ï¼ˆå¦‚â€œå°†è“è‰²ç§¯æœ¨æ”¾åœ¨çº¢è‰²ç§¯æœ¨ä¸Šâ€ï¼‰åœ¨è¯­ä¹‰ä¸Šå¯¹é½ã€‚ |\n| **FrÃ©chet Inceptionè·ç¦»** | FID | è®¡ç®—çœŸå®å›¾åƒé›†å’Œç”Ÿæˆå›¾åƒé›†åœ¨Inception-v3ç½‘ç»œä¸­é—´å±‚ç‰¹å¾ç©ºé—´ä¸­çš„ä¸¤ä¸ªå¤šå…ƒé«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„FrÃ©chetè·ç¦»ï¼ˆWasserstein-2è·ç¦»ï¼‰ã€‚ | **æ— å‚è€ƒ**ï¼Œè¯„ä¼°æ•´ä½“åˆ†å¸ƒè´¨é‡ã€‚åŒæ—¶è¡¡é‡**ç”Ÿæˆè´¨é‡**ï¼ˆé€¼çœŸåº¦ï¼‰å’Œ**å¤šæ ·æ€§**ã€‚ | éœ€è¦è¶³å¤Ÿå¤šçš„æ ·æœ¬ä»¥ç¨³å®šä¼°è®¡åˆ†å¸ƒã€‚å¯¹è®¡ç®—èµ„æºè¦æ±‚è¾ƒé«˜ã€‚Inception-v3çš„ç‰¹å¾ç©ºé—´å¯èƒ½ä¸é€‚ç”¨äºæ‰€æœ‰é¢†åŸŸã€‚ | **ä¸­ç­‰åé«˜**ã€‚å¯ç”¨äºè¯„ä¼°ç”Ÿæˆæœºå™¨äººæ¼”ç¤ºæ•°æ®çš„æ•´ä½“è´¨é‡å’Œå¤šæ ·æ€§ï¼Œä½†æ— æ³•è¯„ä¼°åŠ¨æ€è¿‡ç¨‹ã€‚ |\n| **å­¦ä¹ æ„ŸçŸ¥å›¾åƒå—ç›¸ä¼¼åº¦** | LPIPS | ä½¿ç”¨é¢„è®­ç»ƒçš„æ·±åº¦ç½‘ç»œï¼ˆå¦‚AlexNetã€VGGï¼‰æå–å¤šå±‚çº§ç‰¹å¾ï¼Œè®¡ç®—ç‰¹å¾ç©ºé—´ä¸­çš„è·ç¦»ã€‚è·ç¦»è¶Šå°ï¼Œæ„ŸçŸ¥è¶Šç›¸ä¼¼ã€‚ | è¢«è¯æ˜ä¸äººç±»æ„ŸçŸ¥åˆ¤æ–­é«˜åº¦ç›¸å…³ï¼Œèƒ½æœ‰æ•ˆè¯„ä¼°çº¹ç†ã€é£æ ¼ç­‰é«˜çº§ç‰¹å¾å·®å¼‚ã€‚ | æ˜¯**å…¨å‚è€ƒ**æŒ‡æ ‡ã€‚ä¾èµ–äºæ‰€é€‰ç½‘ç»œæ¶æ„ã€‚ | **ä¸­ç­‰**ã€‚å¯ç”¨äºè¯„ä¼°ç”Ÿæˆç”»é¢çš„æ„ŸçŸ¥è´¨é‡ï¼Œä½†éæœºå™¨äººä»»åŠ¡çš„æ ¸å¿ƒã€‚ |\n\n#### 2. æ—¶ç©ºä¸€è‡´æ€§æŒ‡æ ‡ï¼ˆSpatiotemporal Metricsï¼‰\nè¿™äº›æŒ‡æ ‡è¯„ä¼°è§†é¢‘åœ¨æ—¶é—´ç»´åº¦ä¸Šçš„è¿è´¯æ€§å’ŒåŠ¨æ€è´¨é‡ã€‚\n\n| æŒ‡æ ‡åç§° | ç¼©å†™ | æ ¸å¿ƒåŸç† | ä¼˜ç‚¹ | ç¼ºç‚¹/å±€é™æ€§ | åœ¨æœºå™¨äººé¢†åŸŸçš„é€‚ç”¨æ€§ |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **FrÃ©chet è§†é¢‘è·ç¦»** | FVD | **FIDåœ¨è§†é¢‘åŸŸçš„æ‰©å±•**ã€‚ä½¿ç”¨ä¸“ä¸ºè§†é¢‘åˆ†ç±»è®¾è®¡çš„I3Dç½‘ç»œæå–**æ—¶ç©ºç‰¹å¾**ï¼Œç„¶åè®¡ç®—çœŸå®è§†é¢‘é›†ä¸ç”Ÿæˆè§†é¢‘é›†ç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„FrÃ©chetè·ç¦»ã€‚ | ä¸“é—¨ä¸ºè§†é¢‘è®¾è®¡ï¼Œèƒ½åŒæ—¶è¯„ä¼°**è§†è§‰è´¨é‡**å’Œ**æ—¶é—´åŠ¨æ€**ã€‚æ˜¯å½“å‰è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„**é»„é‡‘æ ‡å‡†**ä¹‹ä¸€ã€‚ | è®¡ç®—æˆæœ¬é«˜ã€‚éœ€è¦å¤§é‡è§†é¢‘æ ·æœ¬ã€‚I3Dç½‘ç»œåœ¨éåŠ¨ä½œè¯†åˆ«ä»»åŠ¡ä¸Šçš„ç‰¹å¾ä»£è¡¨æ€§å¯èƒ½æœ‰é™ã€‚ | **é«˜**ã€‚æ˜¯è¯„ä¼°ä½œä¸ºä¸–ç•Œæ¨¡å‹çš„è§†é¢‘ç”Ÿæˆè´¨é‡çš„å…³é”®æŒ‡æ ‡ï¼Œèƒ½åæ˜ åŠ¨æ€é¢„æµ‹çš„é€¼çœŸåº¦ã€‚ |\n| **æ ¸è§†é¢‘è·ç¦»** | KVD | ä¸FVDç›®æ ‡ç›¸åŒï¼Œä½†ä½¿ç”¨**æœ€å¤§å‡å€¼å·®å¼‚**ï¼ˆMMDï¼‰è€ŒéFrÃ©chetè·ç¦»æ¥æ¯”è¾ƒç‰¹å¾åˆ†å¸ƒã€‚MMDä¸å‡è®¾ç‰¹å¾æœä»é«˜æ–¯åˆ†å¸ƒã€‚ | å¯¹ç‰¹å¾åˆ†å¸ƒçš„å‡è®¾æ›´å¼±ï¼Œå¯èƒ½æ›´é²æ£’ã€‚ | è®¡ç®—å¤æ‚åº¦å¯èƒ½æ›´é«˜ï¼Œä¸”åœ¨å®è·µä¸­FVDæ›´ä¸»æµã€‚ | **ä¸­ç­‰**ã€‚æ˜¯FVDçš„ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åº”ç”¨å’ŒéªŒè¯ç›¸å¯¹è¾ƒå°‘ã€‚ |\n| **FrÃ©chet è§†é¢‘è¿åŠ¨è·ç¦»** | FVMD | ä¸“æ³¨äºè¯„ä¼°**è¿åŠ¨è´¨é‡**ã€‚é¦–å…ˆä»è§†é¢‘ä¸­æå–å¯è·Ÿè¸ªçš„å…³é”®ç‚¹ï¼Œè®¡ç®—å…¶é€Ÿåº¦å’ŒåŠ é€Ÿåº¦ç­‰è¿åŠ¨ç‰¹å¾ï¼Œç„¶åè®¡ç®—è¿™äº›è¿åŠ¨ç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„FrÃ©chetè·ç¦»ã€‚ | ç›´æ¥é‡åŒ–**è¿åŠ¨**çš„é€¼çœŸåº¦å’Œè‡ªç„¶åº¦ï¼Œä¸è§†è§‰å†…å®¹è§£è€¦ã€‚ | ä¾èµ–å…³é”®ç‚¹æ£€æµ‹ä¸è·Ÿè¸ªçš„å‡†ç¡®æ€§ï¼Œåœ¨å¤æ‚åœºæ™¯æˆ–é®æŒ¡ä¸‹å¯èƒ½å¤±æ•ˆã€‚ | **é«˜**ã€‚å¯¹äºæœºå™¨äººä»»åŠ¡è‡³å…³é‡è¦ï¼Œå› ä¸ºç‰©ç†ä¸Šåˆç†çš„è¿åŠ¨æ˜¯è§„åˆ’å’Œæ§åˆ¶çš„åŸºç¡€ã€‚ |\n| **å…‰æµä¸€è‡´æ€§** | Optical Flow Consistency | æ¯”è¾ƒè¿ç»­ç”Ÿæˆå¸§ä¹‹é—´é¢„æµ‹çš„å…‰æµåœºä¸ä»ç”Ÿæˆå¸§ä¸­å®é™…è®¡ç®—å‡ºçš„å…‰æµåœºä¹‹é—´çš„å·®å¼‚ã€‚ | ç›´æ¥æµ‹é‡ç”Ÿæˆè§†é¢‘çš„**æ—¶é—´å¹³æ»‘æ€§**å’Œ**è¿åŠ¨ä¸€è‡´æ€§**ã€‚ | åªæ˜¯ä¸€ä¸ªä¸­é—´æŒ‡æ ‡ï¼Œé«˜å…‰æµä¸€è‡´æ€§ä¸ä¸€å®šä»£è¡¨ç‰©ç†æ­£ç¡®ã€‚ | **ä¸­ç­‰**ã€‚æ˜¯ä¿è¯è§†é¢‘ä¸å‡ºç°é—ªçƒã€æŠ–åŠ¨ç­‰ä½çº§ç‘•ç–µçš„å¿…è¦æ¡ä»¶ï¼Œä½†éå……åˆ†æ¡ä»¶ã€‚ |\n\n#### 3. é¢å‘åº”ç”¨çš„æŒ‡æ ‡ï¼ˆApplication-focused Metricsï¼‰\nè®ºæ–‡ç‰¹åˆ«å¼ºè°ƒï¼Œåœ¨æœºå™¨äººå­¦ä¸­ï¼Œä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°æ‰æ˜¯æœ€ç»ˆæ£€éªŒæ ‡å‡†ã€‚\n\n| æŒ‡æ ‡ç±»å‹ | å…·ä½“ç¤ºä¾‹ | è¯´æ˜ |\n| :--- | :--- | :--- |\n| **ç­–ç•¥è¯„ä¼°æŒ‡æ ‡** | **çš®å°”é€Šç›¸å…³ç³»æ•°**ã€**å¹³å‡æœ€å¤§ç§©è¿ä¾‹** | æ¯”è¾ƒè§†é¢‘æ¨¡å‹é¢„æµ‹çš„ç­–ç•¥æˆåŠŸç‡ä¸çœŸå®ä¸–ç•Œå®éªŒæˆåŠŸç‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼ˆçš®å°”é€Šç³»æ•°ï¼‰ã€‚MMRVåˆ™é‡åŒ–ç­–ç•¥æ’åé¢„æµ‹é”™è¯¯çš„ä¸¥é‡ç¨‹åº¦ã€‚**è¿™æ˜¯è¡¡é‡è§†é¢‘æ¨¡å‹èƒ½å¦ä½œä¸ºå¯é ä»¿çœŸå™¨çš„æ ¸å¿ƒæŒ‡æ ‡ã€‚** |\n| **ç­–ç•¥å­¦ä¹ æŒ‡æ ‡** | **æœ€ç»ˆä»»åŠ¡æˆåŠŸç‡**ã€**æ•°æ®ç”Ÿæˆæœ‰æ•ˆæ€§** | ä½¿ç”¨è§†é¢‘æ¨¡å‹ç”Ÿæˆçš„æ•°æ®è®­ç»ƒå‡ºçš„æœºå™¨äººç­–ç•¥ï¼Œåœ¨çœŸå®ä¸–ç•Œæˆ–æ ‡å‡†æµ‹è¯•ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚ä¾‹å¦‚ï¼ŒDreamGenç­‰å·¥ä½œé€šè¿‡å¯¹æ¯”ä½¿ç”¨ç”Ÿæˆæ•°æ®ä¸çœŸå®æ•°æ®è®­ç»ƒçš„ç­–ç•¥æ€§èƒ½æ¥è¯„ä¼°è§†é¢‘æ¨¡å‹ã€‚ |\n| **ç‰©ç†å¸¸è¯†æŒ‡æ ‡** | **VLMåˆ¤åˆ†**ã€**ä¸“ç”¨ç‰©ç†é—®ç­”** | ä½¿ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4Vï¼‰ä½œä¸ºâ€œè£åˆ¤â€ï¼Œåˆ¤æ–­ç”Ÿæˆè§†é¢‘æ˜¯å¦è¿åç‰©ç†å®šå¾‹ï¼ˆå¦‚ç‰©ä½“ç©¿é€ã€è¿åé‡åŠ›ï¼‰ã€‚æˆ–æ„å»ºä¸“é—¨çš„ç‰©ç†å¸¸è¯†é—®ç­”å¯¹è¿›è¡Œè¯„ä¼°ã€‚ |\n\n### äºŒã€ åŸºå‡†æµ‹è¯•ä¸æ•°æ®é›†ï¼ˆBenchmarks & Datasetsï¼‰\n\nè®ºæ–‡åŒºåˆ†äº†ç”¨äº**è®­ç»ƒ**è§†é¢‘æ¨¡å‹çš„å¤§è§„æ¨¡äº’è”ç½‘æ•°æ®é›†å’Œç”¨äº**è¯„ä¼°**å…¶å„æ–¹é¢èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚\n\n#### 1. è®­ç»ƒæ•°æ®é›†ï¼ˆTraining Datasetsï¼‰\nè¿™äº›æ•°æ®é›†è§„æ¨¡å·¨å¤§ï¼Œä½†è´¨é‡å‚å·®ä¸é½ï¼Œéœ€è¦å¤æ‚çš„æ¸…æ´—æµç¨‹ã€‚\n\n| æ•°æ®é›†åç§° | è§„æ¨¡ | ç‰¹ç‚¹ä¸ç”¨é€” | å±€é™æ€§ï¼ˆè®ºæ–‡æŒ‡å‡ºï¼‰ |\n| :--- | :--- | :--- | :--- |\n| **WebVideo-10M** | 1000ä¸‡è§†é¢‘-æ–‡æœ¬å¯¹ | æ—©æœŸå¤§è§„æ¨¡ç½‘ç»œè§†é¢‘æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒæ–‡æœ¬-è§†é¢‘æ£€ç´¢å’Œç”Ÿæˆæ¨¡å‹ã€‚ | ä¾§é‡**è§„æ¨¡**è€Œé**è´¨é‡**ã€‚å­—å¹•ä¸å‡†ç¡®ã€è§†é¢‘æ¨¡ç³Šã€é•œå¤´åˆ‡æ¢é¢‘ç¹ï¼Œå¯¼è‡´è®­ç»ƒä¿¡å·å˜ˆæ‚ã€‚ |\n| **Panda-70M** | 7000ä¸‡è§†é¢‘-æ–‡æœ¬å¯¹ | æ›´å¤§è§„æ¨¡çš„ç½‘ç»œè§†é¢‘æ•°æ®é›†ï¼Œä½¿ç”¨å¤šæ¨¡æ€æ•™å¸ˆæ¨¡å‹ï¼ˆVLMsï¼‰è‡ªåŠ¨ç”Ÿæˆæè¿°ã€‚ | å°½ç®¡ä½¿ç”¨äº†VLMæ ‡æ³¨ï¼Œä½†**VLMæœ¬èº«ä¼šäº§ç”Ÿå¹»è§‰**ï¼Œå¯¼è‡´æ ‡æ³¨å™ªå£°ã€‚è§†é¢‘è´¨é‡è¿‡æ»¤ä»æ˜¯æŒ‘æˆ˜ã€‚ |\n| **VidGen-1M / OpenVid-1M** | çº¦100ä¸‡è§†é¢‘ | è¾ƒæ–°çš„æ•°æ®é›†ï¼Œå¼ºè°ƒé€šè¿‡ä¸¥æ ¼çš„é¢„å¤„ç†ï¼ˆç¾å­¦è¯„åˆ†ã€è¿åŠ¨å¹³æ»‘åº¦ã€å­—å¹•æè¿°æ€§ï¼‰æ¥ä¿è¯**é«˜è´¨é‡**ã€‚ | **è§„æ¨¡ç›¸å¯¹è¾ƒå°**ï¼Œå¯èƒ½é™åˆ¶æ¨¡å‹çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚é«˜è´¨é‡æ ‡æ³¨ä¾èµ–äººå·¥æˆ–æ›´å¯é çš„VLMï¼Œæˆæœ¬é«˜ã€‚ |\n| **ä¸“æœ‰æ•°æ®é›†** (å¦‚ç”¨äºè®­ç»ƒVeo, Wan, Cosmos) | æœªå…¬å¼€ï¼Œæ¨æµ‹æ•°äº¿è‡³ä¸Šäº¿ | SOTAé—­æºæ¨¡å‹ï¼ˆå¦‚OpenAIçš„Veoã€Googleçš„Wanï¼‰ä½¿ç”¨çš„æ•°æ®é›†ã€‚é€šå¸¸ç»“åˆäº†**è‡ªåŠ¨åŒ–è¿‡æ»¤**å’Œ**äººå·¥æ ‡æ³¨**ï¼Œå…·æœ‰æé«˜çš„è´¨é‡å’Œå¯†é›†çš„æ³¨é‡Šã€‚ | **ä¸å…¬å¼€**ï¼Œé˜»ç¢äº†å¼€æºç ”ç©¶ã€‚æ„å»ºæˆæœ¬æå…¶é«˜æ˜‚ï¼ˆæ•°æ®æ¸…æ´—ã€VLMæ ‡æ³¨ã€äººå·¥å®¡æ ¸ï¼‰ã€‚ |\n\n**è®ºæ–‡å¼ºè°ƒçš„æ ¸å¿ƒæŒ‘æˆ˜**ï¼šé«˜è´¨é‡æ•°æ®é›†çš„æ„å»ºéœ€è¦æ˜‚è´µçš„â€œ**è§†é¢‘åˆ†å‰² -> è§†é¢‘è¿‡æ»¤ ->",
            "timestamp": "2026-01-21T12:14:25.490062",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½åˆ†æ\n\næ ¹æ®æ‚¨æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œè¿™ç¯‡ç»¼è¿°æ€§æ–‡ç« ã€ŠVideo Generation Models in Robotics -- Applications, Research Challenges, Future Directionsã€‹**å¹¶æœªæŠ¥å‘Šå…·ä½“çš„å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡æ•°å€¼æˆ–æ€§èƒ½æå‡**ã€‚è¿™æ˜¯å› ä¸ºè¯¥è®ºæ–‡æ˜¯ä¸€ç¯‡**ç»¼è¿°ï¼ˆSurveyï¼‰**ï¼Œå…¶æ ¸å¿ƒç›®æ ‡æ˜¯ç³»ç»Ÿæ€§åœ°å›é¡¾ã€åˆ†ç±»å’Œè®¨è®ºè¯¥é¢†åŸŸçš„ç ”ç©¶ç°çŠ¶ã€åº”ç”¨ã€æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘ï¼Œè€Œéå‘ˆç°ä½œè€…å›¢é˜Ÿè‡ªå·±çš„æ–°å®éªŒå’Œç»“æœã€‚\n\nè®ºæ–‡ä¸­å¼•ç”¨äº†å¤§é‡å…¶ä»–ç ”ç©¶å·¥ä½œï¼ˆé€šè¿‡ `[æ•°å­—]` æ ‡æ³¨ï¼‰ï¼Œè¿™äº›è¢«å¼•ç”¨çš„åŸå§‹è®ºæ–‡æ‰åŒ…å«äº†å…·ä½“çš„å®éªŒç»†èŠ‚ã€‚æœ¬ç»¼è¿°çš„ä½œç”¨æ˜¯æ•´åˆè¿™äº›ä¿¡æ¯ï¼Œæä¾›ä¸€ä¸ªå®è§‚çš„å›¾æ™¯ã€‚\n\nå°½ç®¡å¦‚æ­¤ï¼Œè®ºæ–‡åœ¨ç›¸å…³ç« èŠ‚æ˜ç¡®æ¢³ç†äº†è¯¥é¢†åŸŸ**å¸¸ç”¨çš„è¯„ä¼°æ–¹æ³•**å’Œ**ä»£è¡¨æ€§çš„åŸºå‡†æµ‹è¯•**ï¼Œè¿™å¯¹äºç†è§£è§†é¢‘æ¨¡å‹åœ¨æœºå™¨äººå­¦ä¸­çš„æ€§èƒ½è¡¡é‡è‡³å…³é‡è¦ã€‚\n\n### 1. ä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆç¬¬4.1èŠ‚ï¼‰\n\nè®ºæ–‡å°†è¯„ä¼°æŒ‡æ ‡åˆ†ä¸ºä¸¤å¤§ç±»ï¼š\n\n#### **A. å¸§çº§æŒ‡æ ‡ï¼ˆFrame-level Metricsï¼‰**\n- **PSNRï¼ˆå³°å€¼ä¿¡å™ªæ¯”ï¼‰**ï¼šè¡¡é‡ç”Ÿæˆå¸§ä¸çœŸå®å¸§åœ¨åƒç´ å±‚é¢çš„è¯¯å·®ã€‚\n- **SSIMï¼ˆç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼‰**ï¼šè¯„ä¼°å›¾åƒåœ¨äº®åº¦ã€å¯¹æ¯”åº¦å’Œç»“æ„æ–¹é¢çš„ç›¸ä¼¼æ€§ï¼Œæ›´ç¬¦åˆäººç±»æ„ŸçŸ¥ã€‚\n- **CLIPç›¸ä¼¼åº¦**ï¼šè®¡ç®—ç”Ÿæˆå›¾åƒä¸å‚è€ƒå›¾åƒï¼ˆæˆ–æ–‡æœ¬æç¤ºï¼‰åœ¨CLIPæ¨¡å‹åµŒå…¥ç©ºé—´ä¸­çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œç”¨äºè¯„ä¼°**è¯­ä¹‰å¯¹é½**ã€‚\n- **FIDï¼ˆFrÃ©chet Inception Distanceï¼‰**ï¼šè®¡ç®—çœŸå®å›¾åƒå’Œç”Ÿæˆå›¾åƒåœ¨ç‰¹å¾ç©ºé—´åˆ†å¸ƒä¹‹é—´çš„Wasserstein-2è·ç¦»ï¼Œè¡¡é‡ç”Ÿæˆå¤šæ ·æ€§å’ŒçœŸå®æ€§ã€‚\n- **LPIPSï¼ˆå­¦ä¹ æ„ŸçŸ¥å›¾åƒå—ç›¸ä¼¼åº¦ï¼‰**ï¼šåŸºäºæ·±åº¦ç½‘ç»œç‰¹å¾è®¡ç®—ç›¸ä¼¼åº¦ï¼Œä¸äººç±»åˆ¤æ–­é«˜åº¦ç›¸å…³ã€‚\n\n#### **B. æ—¶ç©ºä¸€è‡´æ€§æŒ‡æ ‡ï¼ˆSpatiotemporal Metricsï¼‰**\n- **FVDï¼ˆFrÃ©chet Video Distanceï¼‰**ï¼šFIDåœ¨è§†é¢‘é¢†åŸŸçš„æ‰©å±•ï¼Œè¯„ä¼°è§†é¢‘åºåˆ—çš„è§†è§‰è´¨é‡å’Œæ—¶é—´è¿è´¯æ€§ã€‚\n- **KVDï¼ˆKernel Video Distanceï¼‰**ï¼šä½¿ç”¨æ ¸æ–¹æ³•ï¼ˆæœ€å¤§å‡å€¼å·®å¼‚ï¼‰æ¥æ•æ‰è§†é¢‘ç‰¹å¾çš„é«˜é˜¶ç»Ÿè®¡å·®å¼‚ã€‚\n- **FVMDï¼ˆFrÃ©chet Video Motion Distanceï¼‰**ï¼šä¸“æ³¨äºè¯„ä¼°ç”Ÿæˆè§†é¢‘çš„**è¿åŠ¨è´¨é‡**ï¼Œé€šè¿‡æå–å¹¶æ¯”è¾ƒå…³é”®ç‚¹çš„é€Ÿåº¦ã€åŠ é€Ÿåº¦ç­‰è¿åŠ¨ç‰¹å¾ã€‚\n- **å…‰å­¦æµï¼ˆOptical Flowï¼‰**ï¼šç”¨äºè¯„ä¼°å¸§ä¸å¸§ä¹‹é—´è¿åŠ¨çš„è¿è´¯æ€§å’Œåˆç†æ€§ã€‚\n- **VLMè¯„ä¼°**ï¼šä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ¥è¯„ä¼°ç”Ÿæˆè§†é¢‘çš„**ç‰©ç†å¸¸è¯†ä¸€è‡´æ€§**ã€‚\n\n#### **C. æœºå™¨äººåº”ç”¨ç‰¹å®šæŒ‡æ ‡**\n- **ç­–ç•¥è¯„ä¼°ç›¸å…³æ€§**ï¼šä¾‹å¦‚ï¼Œé¢„æµ‹çš„æˆåŠŸç‡ä¸çœŸå®ä¸–ç•ŒæˆåŠŸç‡ä¹‹é—´çš„**çš®å°”é€Šç›¸å…³ç³»æ•°**ã€‚\n- **æ’åä¸€è‡´æ€§**ï¼šå¦‚**å¹³å‡æœ€å¤§æ’åè¿åï¼ˆMMRVï¼‰**ï¼Œè¡¡é‡è§†é¢‘æ¨¡å‹å¯¹ä¸åŒæœºå™¨äººç­–ç•¥çš„æ’åæ˜¯å¦ä¸çœŸå®ä¸–ç•Œè¯„ä¼°ä¸€è‡´ã€‚\n\n### 2. ä¸»è¦çš„åŸºå‡†æµ‹è¯•ï¼ˆç¬¬4.2èŠ‚ï¼‰\n\nè®ºæ–‡ç»¼è¿°äº†å¤šä¸ªç”¨äºå…¨é¢è¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„åŸºå‡†æµ‹è¯•ï¼š\n\n| åŸºå‡†æµ‹è¯•åç§° | æ ¸å¿ƒè¯„ä¼°ç»´åº¦ |\n| :--- | :--- |\n| **WorldModelBench** | æŒ‡ä»¤è·Ÿéšã€ç‰©ç†è§„å¾‹éµå®ˆï¼ˆå¦‚è´¨é‡å®ˆæ’ï¼‰ |\n| **EvalCrafter** | ç¾å­¦è´¨é‡ã€è¿åŠ¨è´¨é‡ã€æ—¶é—´ä¸€è‡´æ€§ã€æ–‡æœ¬-è§†é¢‘å¯¹é½ |\n| **VBench** | 16ä¸ªç»†ç²’åº¦æ ‡å‡†ï¼Œå¦‚ä¸»ä½“ä¸€è‡´æ€§ã€èƒŒæ™¯ä¸€è‡´æ€§ã€æ—¶é—´é—ªçƒç­‰ |\n| **PAI-Bench** | æ—¶é—´ä¸€è‡´æ€§ã€è¿åŠ¨å¹³æ»‘åº¦ã€ç¾å­¦è´¨é‡ |\n| **T2V-CompBench** | å¯¹è±¡å±æ€§ï¼ˆé¢œè‰²ã€å½¢çŠ¶ï¼‰ä¸€è‡´æ€§ã€åŠ¨ä½œä¸€è‡´æ€§ |\n| **WorldSimBench** | ç‰©ç†è¿åŠ¨ä¸€è‡´æ€§ï¼Œå¦‚åœ¨ä¸åŒä»‹è´¨ï¼ˆç©ºæ°”ã€æ°´ï¼‰ä¸­çš„é€Ÿåº¦æ„ŸçŸ¥ |\n| **Physics-IQ / VideoPhy / PhyGenBench** | **ç‰©ç†å¸¸è¯†**ï¼Œè¯„ä¼°å¯¹å…‰å­¦ã€çƒ­åŠ›å­¦ã€é‡åŠ›ã€åŠ¨é‡å®ˆæ’ç­‰åŸºæœ¬ç‰©ç†å®šå¾‹çš„ç†è§£ |\n| **VPÂ² (Visual Planning)** | åœ¨è§†è§‰è§„åˆ’ä»»åŠ¡ä¸­ï¼Œç”Ÿæˆè½¨è¿¹ä¸ç‰©ç†å®šå¾‹çš„**å¯¹é½ç¨‹åº¦** |\n| **SAFEWatch / T2VSafetyBench** | **ç”Ÿæˆå†…å®¹çš„å®‰å…¨æ€§**ï¼Œè¯„ä¼°æ˜¯å¦ä¼šäº§ç”Ÿéæ³•ã€æš´åŠ›æˆ–ä¸é“å¾·çš„å†…å®¹ |\n\n### 3. å…³é”®ç»“è®ºä¸æ€§èƒ½è¶‹åŠ¿ï¼ˆç»¼åˆè‡ªè®ºæ–‡åˆ†æï¼‰\n\nè™½ç„¶æ— å…·ä½“æ•°å€¼ï¼Œä½†è®ºæ–‡é€šè¿‡å¯¹ç°æœ‰ç ”ç©¶çš„ç»¼è¿°ï¼ŒæŒ‡å‡ºäº†é‡è¦çš„**æ€§èƒ½è¶‹åŠ¿å’Œç»“è®º**ï¼š\n\n1.  **è§„æ¨¡ä¸è´¨é‡çš„æƒè¡¡**ï¼šæ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒæ•°æ®é€šå¸¸èƒ½æå‡ç”Ÿæˆè§†é¢‘çš„**è§†è§‰è´¨é‡ï¼ˆç¾å­¦ã€æ¸…æ™°åº¦ï¼‰å’Œæ—¶é—´ä¸€è‡´æ€§**ï¼Œä½†å¯¹æå‡**ç‰©ç†å¸¸è¯†ç†è§£**å’Œ**æŒ‡ä»¤è·Ÿéšèƒ½åŠ›**çš„æ•ˆæœæœ‰é™ï¼Œä¸”å¾ˆå¿«ä¼šè¾¾åˆ°å¹³å°æœŸã€‚\n2.  **ç‰©ç†ä¸€è‡´æ€§æ˜¯ä¸»è¦çŸ­æ¿**ï¼šå½“å‰æœ€å…ˆè¿›çš„è§†é¢‘æ¨¡å‹åœ¨ç”Ÿæˆæ¶‰åŠå¤æ‚ç‰©ä½“äº¤äº’ï¼ˆå°¤å…¶æ˜¯å¯å˜å½¢ä½“ã€æµä½“ï¼‰ã€éµå®ˆåŠ¨é‡/èƒ½é‡å®ˆæ’å®šå¾‹çš„è§†é¢‘æ—¶ï¼Œä»ç„¶å­˜åœ¨ä¸¥é‡çš„**å¹»è§‰ï¼ˆHallucinationï¼‰**é—®é¢˜ã€‚è¿™æ˜¯é˜»ç¢å…¶åœ¨æœºå™¨äººç­‰é«˜ä¿çœŸè¦æ±‚åœºæ™¯ä¸­å¯é åº”ç”¨çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚\n3.  **è¯„ä¼°ä¸åº”ç”¨çš„é¸¿æ²Ÿ**ï¼šåœ¨æ ‡å‡†è§†è§‰è´¨é‡æŒ‡æ ‡ï¼ˆå¦‚FVDï¼‰ä¸Šè¡¨ç°ä¼˜å¼‚çš„æ¨¡å‹ï¼Œåœ¨æœºå™¨äººç›¸å…³çš„**ç‰©ç†ä¸€è‡´æ€§**æˆ–**ä¸‹æ¸¸ä»»åŠ¡æˆåŠŸç‡**ä¸Šå¯èƒ½è¡¨ç°ä¸ä½³ã€‚å› æ­¤ï¼Œéœ€è¦å‘å±•æ›´å¤šé¢å‘æœºå™¨äººåº”ç”¨çš„ã€ä»»åŠ¡ç›¸å…³çš„è¯„ä¼°åŸºå‡†ã€‚\n4.  **ç­–ç•¥è¯„ä¼°çš„æ½œåŠ›**ï¼šç ”ç©¶è¡¨æ˜ï¼Œè§†é¢‘æ¨¡å‹åœ¨**é¢„æµ‹æœºå™¨äººç­–ç•¥çš„æˆåŠŸç‡**å’Œ**å¯¹ç­–ç•¥è¿›è¡Œæ’å**æ–¹é¢ï¼Œä¸çœŸå®ä¸–ç•Œç»“æœæ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç›¸å…³æ€§ï¼Œè¿™ä½¿å…¶æˆä¸ºä¸€ç§å…·æœ‰æ½œåŠ›çš„ã€ä½æˆæœ¬çš„é¢„éƒ¨ç½²è¯„ä¼°å·¥å…·ã€‚\n\n**æ€»ç»“**ï¼šæœ¬è®ºæ–‡ä½œä¸ºä¸€ç¯‡é¢†åŸŸç»¼è¿°ï¼Œå…¶ä»·å€¼åœ¨äºç³»ç»Ÿæ€§åœ°æ„å»ºäº†è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æœºå™¨äººå­¦ä¸­çš„åº”ç”¨æ¡†æ¶ï¼Œæ¸…æ™°åœ°æŒ‡å‡ºäº†å½“å‰æŠ€æœ¯çš„**èƒ½åŠ›è¾¹ç•Œ**ï¼ˆé«˜ä¿çœŸæ¨¡æ‹Ÿã€æ•°æ®ç”Ÿæˆï¼‰å’Œ**æ ¸å¿ƒç“¶é¢ˆ**ï¼ˆç‰©ç†å¹»è§‰ã€æŒ‡ä»¤è·Ÿéšã€é•¿è§†é¢‘ç”Ÿæˆã€å®‰å…¨ä¸æˆæœ¬ï¼‰ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†éœ€è¦æ”»å…‹çš„å…³é”®æŒ‘æˆ˜å’Œæ½œåœ¨æ–¹å‘ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®éœ€è¦æŸ¥é˜…è®ºæ–‡ä¸­å¼•ç”¨çš„ç›¸åº”åŸå§‹ç ”ç©¶æ–‡çŒ®ã€‚",
            "timestamp": "2026-01-21T12:15:07.725102",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nè¯¥ç»¼è¿°ç³»ç»Ÿæ€§åœ°æŒ‡å‡ºäº†å½“å‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æœºå™¨äººå­¦åº”ç”¨ä¸­é¢ä¸´çš„**æ ¸å¿ƒå±€é™æ€§**ï¼Œè¿™äº›å±€é™æ€§é˜»ç¢äº†å…¶åœ¨é«˜é£é™©ã€å®‰å…¨å…³é”®åœºæ™¯ä¸­çš„å¯ä¿¡é›†æˆä¸åº”ç”¨ã€‚\n\n### ä¸»è¦å±€é™æ€§\n\n1.  **å¹»è§‰ä¸ç‰©ç†å®šå¾‹è¿å**\n    - **é—®é¢˜**ï¼šæ¨¡å‹å€¾å‘äºç”Ÿæˆç‰©ç†ä¸Šä¸ç°å®çš„è§†é¢‘ï¼Œä¾‹å¦‚ç‰©ä½“è¿åè´¨é‡å®ˆæ’å®šå¾‹ã€å‡ºç°/æ¶ˆå¤±ã€å‘ç”Ÿéç‰©ç†å½¢å˜ã€‚\n    - **å½±å“**ï¼šä¸¥é‡å‰Šå¼±äº†æ¨¡å‹ä½œä¸ºä¸–ç•Œæ¨¡å‹è¿›è¡Œå¯é é¢„æµ‹ï¼ˆå¦‚ç­–ç•¥è¯„ä¼°ã€è§†è§‰è§„åˆ’ï¼‰çš„èƒ½åŠ›ã€‚\n\n2.  **æŒ‡ä»¤è·Ÿéšèƒ½åŠ›å·®**\n    - **é—®é¢˜**ï¼šæ¨¡å‹éš¾ä»¥ç²¾ç¡®éµå¾ªç”¨æˆ·è¾“å…¥çš„å¤æ‚æŒ‡ä»¤ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ—¶ç¨‹è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ã€‚ä¾‹å¦‚ï¼Œæ— æ³•å‡†ç¡®æ‰§è¡ŒæŒ‡å®šçš„åŠ¨ä½œæˆ–ä¿æŒé™æ€ç›¸æœºè§†è§’ã€‚\n    - **å½±å“**ï¼šå¯¼è‡´ç”Ÿæˆçš„æ•°æ®ï¼ˆå¦‚ä¸“å®¶æ¼”ç¤ºï¼‰ä¸ä»»åŠ¡ç›®æ ‡ä¸ç¬¦ï¼Œæ±¡æŸ“æ¨¡ä»¿å­¦ä¹ çš„æ•°æ®é›†ã€‚\n\n3.  **ä¸ç¡®å®šæ€§é‡åŒ–ç¼ºå¤±**\n    - **é—®é¢˜**ï¼šç°æœ‰è§†é¢‘æ¨¡å‹ç¼ºä¹è¡¨è¾¾æˆ–é‡åŒ–å…¶é¢„æµ‹ç½®ä¿¡åº¦çš„èƒ½åŠ›ã€‚æ— æ³•åˆ¤æ–­æ¨¡å‹ä½•æ—¶â€œçŸ¥é“â€æˆ–â€œä¸çŸ¥é“â€ã€‚\n    - **å½±å“**ï¼šåœ¨å®‰å…¨å…³é”®å†³ç­–ä¸­ï¼Œæ— æ³•è¯„ä¼°é¢„æµ‹çš„å¯é æ€§ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨éœ€è¦é£é™©æ„ŸçŸ¥çš„åœºæ™¯ä¸­çš„åº”ç”¨ã€‚\n\n4.  **å®‰å…¨å†…å®¹ç”Ÿæˆä¸äº¤äº’ä¿éšœä¸è¶³**\n    - **å†…å®¹å®‰å…¨**ï¼šæ¨¡å‹å¯èƒ½ç”ŸæˆåŒ…å«æš´åŠ›ã€çŠ¯ç½ªç­‰æœ‰å®³å†…å®¹çš„è§†é¢‘ï¼Œç¼ºä¹æœ‰æ•ˆçš„å®‰å…¨æŠ¤æ ã€‚\n    - **äº¤äº’å®‰å…¨**ï¼šåœ¨æœºå™¨äººæ§åˆ¶å›è·¯ä¸­ï¼Œå¦‚ä½•åˆ©ç”¨è§†é¢‘ä¸–ç•Œæ¨¡å‹æ¥é¢„æµ‹å¹¶é¿å…ç‰©ç†ç¢°æ’æˆ–è¯­ä¹‰ä¸Šå±é™©çš„è¡Œä¸ºï¼Œä»æ˜¯ä¸€ä¸ªå¼€æ”¾æŒ‘æˆ˜ã€‚\n\n5.  **åŠ¨ä½œä¼°è®¡ç²¾åº¦ä¸è¶³**\n    - **é—®é¢˜**ï¼šä»ç”Ÿæˆè§†é¢‘ä¸­åæ¨æœºå™¨äººåŠ¨ä½œï¼ˆé€šè¿‡æ½œåœ¨åŠ¨ä½œæ¨¡å‹æˆ–é€†åŠ¨åŠ›å­¦æ¨¡å‹ï¼‰çš„ç²¾åº¦æœ‰é™ï¼Œä¸”æ³›åŒ–èƒ½åŠ›å·®ã€‚\n    - **å½±å“**ï¼šé™åˆ¶äº†åˆ©ç”¨ç”Ÿæˆè§†é¢‘è¿›è¡Œä½æˆæœ¬ç­–ç•¥å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚\n\n6.  **ç”Ÿæˆé•¿è§†é¢‘èƒ½åŠ›æœ‰é™**\n    - **é—®é¢˜**ï¼šå½“å‰SOTAæ¨¡å‹é€šå¸¸åªèƒ½ç”Ÿæˆæ•°ç§’çš„è§†é¢‘ï¼Œè€Œæœºå™¨äººä»»åŠ¡å¾€å¾€éœ€è¦åˆ†é’Ÿçº§çš„æ—¶åºé¢„æµ‹ã€‚\n    - **å½±å“**ï¼šé€šè¿‡æ‹¼æ¥çŸ­ç‰‡æ®µç”Ÿæˆé•¿è§†é¢‘ä¼šå¼•å…¥ä¼ªå½±å’Œæ—¶é—´ä¸ä¸€è‡´æ€§ï¼Œæ— æ³•æ»¡è¶³é•¿æ—¶ç¨‹è§„åˆ’çš„éœ€æ±‚ã€‚\n\n7.  **æ•°æ®ã€è®­ç»ƒä¸æ¨ç†æˆæœ¬é«˜æ˜‚**\n    - **æ•°æ®æˆæœ¬**ï¼šéœ€è¦å¤§è§„æ¨¡ã€é«˜è´¨é‡ã€æ ‡æ³¨è‰¯å¥½çš„è§†é¢‘æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ•°æ®ç­›é€‰å’Œæ ‡æ³¨è¿‡ç¨‹æ˜‚è´µä¸”ä¾èµ–äººå·¥æˆ–æ˜“äº§ç”Ÿå¹»è§‰çš„VLMã€‚\n    - **è®¡ç®—æˆæœ¬**ï¼šæ¨¡å‹å‚æ•°é‡å¤§ï¼Œè®­ç»ƒéœ€æ•°åä¸‡ç¾é‡‘è®¡ç®—èµ„æºï¼›æ¨ç†é€Ÿåº¦æ…¢ï¼Œéš¾ä»¥æ»¡è¶³æœºå™¨äººå®æ—¶é—­ç¯åº”ç”¨ï¼ˆå¦‚åœ¨çº¿è§„åˆ’ï¼‰çš„éœ€æ±‚ã€‚\n\n8.  **è¯„ä¼°ä½“ç³»ä¸å®Œå–„**\n    - **é—®é¢˜**ï¼šç¼ºä¹ç»Ÿä¸€çš„ã€é¢å‘æœºå™¨äººä»»åŠ¡çš„è¯„ä¼°æ¡†æ¶ã€‚ç°æœ‰æŒ‡æ ‡å¤šå…³æ³¨æ„ŸçŸ¥è´¨é‡ï¼Œè€Œéå¯¹æœºå™¨äººåº”ç”¨è‡³å…³é‡è¦çš„ç‰©ç†ä¸€è‡´æ€§å’Œé¢„æµ‹å‡†ç¡®æ€§ã€‚\n    - **å½±å“**ï¼šéš¾ä»¥å®¢è§‚æ¯”è¾ƒä¸åŒæ¨¡å‹çš„å®ç”¨ä»·å€¼ï¼Œé˜»ç¢äº†æŠ€æœ¯å‘å±•ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡æå‡ºäº†å¤šä¸ªå…·æœ‰æ·±åº¦çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚\n\n### 1. æå‡ç‰©ç†ä¸€è‡´æ€§ä¸å‡å°‘å¹»è§‰\n- **æ–¹å‘**ï¼šå°†ç‰©ç†å…ˆéªŒ**åŸç”Ÿåœ°**æ•´åˆåˆ°æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç›®æ ‡ä¸­ï¼Œè€Œéä¾èµ–äº‹åçš„ã€ä¸´æ—¶æ€§çš„è§£å†³æ–¹æ¡ˆã€‚\n- **å…·ä½“è·¯å¾„**ï¼š\n    - æ¢ç´¢ç»“åˆ**å“ˆå¯†é¡¿æˆ–æ‹‰æ ¼æœ—æ—¥åŠ›å­¦**çš„åŠ¨åŠ›å­¦å»ºæ¨¡æ–¹æ³•ã€‚\n    - ç ”ç©¶**ç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ **æ¡†æ¶ï¼Œå°†ç‰©ç†å®šå¾‹ä½œä¸ºè½¯çº¦æŸæˆ–æ­£åˆ™åŒ–é¡¹ã€‚\n    - åˆ©ç”¨**å¯å¾®ç‰©ç†æ¨¡æ‹Ÿå™¨**ä¸è§†é¢‘ç”Ÿæˆæ¨¡å‹ååŒï¼Œå…ˆç”Ÿæˆç‰©ç†åˆç†çš„ç²—è½¨è¿¹ï¼Œå†ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜ä¿çœŸæ¸²æŸ“ã€‚\n    - å¼€å‘åŸºäº**åŠŸèƒ½å¯ä¾›æ€§**çš„å¼•å¯¼æœºåˆ¶ï¼Œè®©æ¨¡å‹ç†è§£ç‰©ä½“é—´åˆç†çš„äº¤äº’æ–¹å¼ã€‚\n\n### 2. å¢å¼ºæŒ‡ä»¤è·Ÿéšä¸å¯æ§æ€§\n- **æ–¹å‘**ï¼šå¼€å‘**å†…ç¦€çš„**å¤šæ¨¡æ€æ¨ç†ä¸è§„åˆ’èƒ½åŠ›ã€‚\n- **å…·ä½“è·¯å¾„**ï¼š\n    - å€Ÿé‰´å¤§è¯­è¨€æ¨¡å‹çš„**æ€ç»´é“¾**æŠ€æœ¯ï¼Œè®©è§†é¢‘æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œâ€œæ¨ç†â€ã€‚\n    - è®¾è®¡æ›´å¼ºå¤§çš„**å¤šæ¨¡æ€æ¡ä»¶èåˆæœºåˆ¶**ï¼ˆå›¾åƒã€æ–‡æœ¬ã€è½¨è¿¹ã€å…³é”®ç‚¹ï¼‰ï¼Œå®ç°ç»†ç²’åº¦ã€å±€éƒ¨åŒ–çš„æ§åˆ¶ã€‚\n    - åˆ©ç”¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œ**æŒ‡ä»¤å¾®è°ƒ**ã€‚\n\n### 3. å‘å±•é«˜æ•ˆçš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•\n- **æ–¹å‘**ï¼šä¸ºè§†é¢‘ç”Ÿæˆå¼€å‘**å¯è¯æ˜çš„ã€è®¡ç®—é«˜æ•ˆ**çš„UQæ–¹æ³•ã€‚\n- **å…·ä½“è·¯å¾„**ï¼š\n    - æ‰©å±•å¦‚ `CÂ³` ç­‰æ–¹æ³•ï¼Œæä¾›**æ—¶ç©ºå¯†é›†**çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚\n    - ç ”ç©¶åœ¨**åˆ†å¸ƒå¤–**åœºæ™¯ä¸‹ä»èƒ½ä¿æŒæ ¡å‡†çš„UQæŠ€æœ¯ã€‚\n    - æ¢ç´¢å°†UQä¸**å®‰å…¨æ»¤æ³¢å™¨**ç»“åˆï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­é¢„é˜²å±é™©åŠ¨ä½œã€‚\n\n### 4. æ„å»ºå®‰å…¨æŠ¤æ ä¸åŸºå‡†\n- **æ–¹å‘**ï¼šå¼€å‘é€šç”¨ã€å¯é…ç½®çš„**å®‰å…¨ç­–ç•¥æ‰§è¡Œæœºåˆ¶**å’Œå…¨é¢çš„**å®‰å…¨è¯„æµ‹åŸºå‡†**ã€‚\n- **å…·ä½“è·¯å¾„**ï¼š\n    - è®¾è®¡ç±»ä¼¼LLMå®‰å…¨æŠ¤æ çš„æ¨¡å—ï¼Œåœ¨æ¨ç†æ—¶å®æ—¶è¿‡æ»¤ä¸å®‰å…¨å†…å®¹ã€‚\n    - åˆ›å»ºæ¶µç›–æ›´å¹¿æ³›å®‰å…¨ç»´åº¦ï¼ˆç‰©ç†å®‰å…¨ã€è¯­ä¹‰å®‰å…¨ã€éšç§ï¼‰çš„æœºå™¨äººä¸“ç”¨å®‰å…¨è¯„æµ‹æ•°æ®é›†ã€‚\n\n### 5. æ”¹è¿›åŠ¨ä½œä¼°è®¡ä¸æ•°æ®ç”Ÿæˆ\n- **æ–¹å‘**ï¼šæå‡åŠ¨ä½œä¼°è®¡çš„**å¯è§£é‡Šæ€§**å’Œ**æ³›åŒ–æ€§**ã€‚\n- **å…·ä½“è·¯å¾„**ï¼š\n    - è®¾è®¡èƒ½äº§ç”Ÿ**å¯è§£é‡Šæ½œåœ¨åŠ¨ä½œ**çš„æ¨¡å‹æ¶æ„ã€‚\n    - æ¢ç´¢**åŠç›‘ç£**æˆ–**è‡ªç›‘ç£**çš„é€†åŠ¨åŠ›å­¦æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œå‡å°‘å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚\n    - åˆ©ç”¨**æ–°è§†è§’åˆæˆ**æŠ€æœ¯ï¼Œä»å°‘é‡é«˜è´¨é‡è§†é¢‘ä¸­é«˜æ•ˆæ‰©å±•æ•°æ®ã€‚\n\n### 6. å®ç°é«˜æ•ˆçš„é•¿è§†é¢‘ç”Ÿæˆ\n- **æ–¹å‘**ï¼šçªç ´**æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—ç“¶é¢ˆ**å’Œ**è¯¯å·®ç´¯ç§¯é—®é¢˜**ã€‚\n- **å…·ä½“è·¯å¾„**ï¼š\n    - ç ”ç©¶æ›´é«˜æ•ˆçš„**é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡æ¶æ„**ï¼Œå¦‚æ··åˆä¸“å®¶ã€ç¨€ç–æ³¨æ„åŠ›ã€åˆ†å±‚ç”Ÿæˆï¼ˆå…ˆå…³é”®å¸§åæ’å€¼ï¼‰ã€‚\n    - æ¢ç´¢**æµ‹è¯•æ—¶è®­ç»ƒ**ç­‰æŠ€æœ¯ï¼ŒåŠ¨æ€é€‚åº”ç”Ÿæˆé•¿è§†é¢‘çš„éœ€æ±‚ã€‚\n    - ä¼˜åŒ–**è‡ªå›å½’ç”Ÿæˆ**ä¸­çš„å†å²ä¿¡æ¯å‹ç¼©ä¸ç¼“å­˜æœºåˆ¶ã€‚\n\n### 7. é™ä½æ€»ä½“æˆæœ¬\n- **æ–¹å‘**ï¼šä»ç®—æ³•å’Œç³»ç»Ÿå±‚é¢è¿›è¡Œ**è”åˆä¼˜åŒ–**ã€‚\n- **å…·ä½“è·¯å¾„**ï¼š\n    - **ç®—æ³•**ï¼šé‡‡ç”¨æ›´é«˜æ•ˆçš„**æ—¶ç©ºå‹ç¼©**ï¼ˆå¦‚æ·±åº¦å‹ç¼©è‡ªç¼–ç å™¨ï¼‰ã€**ä¸€è‡´æ€§æ¨¡å‹**ï¼ˆå•æ­¥ç”Ÿæˆï¼‰ã€**æ¨¡å‹è’¸é¦**ã€‚\n    - **ç³»ç»Ÿ**ï¼šåº”ç”¨**é‡åŒ–**ã€**ç¼–è¯‘ä¼˜åŒ–**ç­‰æŠ€æœ¯åŠ é€Ÿæ¨ç†ã€‚\n    - **æ•°æ®**ï¼šå¼€å‘æ›´æ™ºèƒ½çš„**è‡ªåŠ¨æ•°æ®è¿‡æ»¤ä¸æ ‡æ³¨ç®¡é“**ï¼Œå‡å°‘å¯¹äººå·¥å’Œå¤§å‹VLMçš„ä¾èµ–ã€‚\n\n### 8. å»ºç«‹æœºå™¨äººä¸­å¿ƒçš„è¯„ä¼°ä½“ç³»\n- **æ–¹å‘**ï¼šåˆ›å»ºä»¥**ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½**ä¸ºæ ¸å¿ƒçš„è¯„ä¼°æ ‡å‡†ã€‚\n- **å…·ä½“è·¯å¾„**ï¼š\n    - å¼€å‘ä¸“æ³¨äº**ç²¾ç»†æœºå™¨äººæ“ä½œä»»åŠ¡**çš„åŸºå‡†æµ‹è¯•ã€‚\n    - è®¾è®¡ç»“åˆ**3Dåœºæ™¯é‡å»º**ä¸**ç‰©ç†ä¸€è‡´æ€§éªŒè¯**çš„è¯„ä¼°æµç¨‹ã€‚\n    - æ¨å¹¿ä½¿ç”¨**é¢„æµ‹ç­–ç•¥æˆåŠŸç‡ä¸çœŸå®æˆåŠŸç‡çš„ç›¸å…³æ€§**ç­‰å®ç”¨æŒ‡æ ‡ã€‚\n\n## æ€»ç»“\nè¯¥è®ºæ–‡æŒ‡å‡ºï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æœºå™¨äººå­¦ä¸­å±•ç°å‡º**é¢ è¦†æ€§æ½œåŠ›**ï¼Œä½†å…¶èµ°å‘å¤§è§„æ¨¡ã€å®‰å…¨å…³é”®çš„åº”ç”¨ï¼Œå¿…é¡»è·¨è¶Š**ç‰©ç†çœŸå®æ€§ã€å®‰å…¨æ€§ã€å¯æ§æ€§ã€é«˜æ•ˆæ€§**ç­‰å¤šé‡éšœç¢ã€‚æœªæ¥çš„çªç ´å°†ä¾èµ–äº**è·¨å­¦ç§‘åˆ›æ–°**ï¼Œä¸ä»…éœ€è¦æ”¹è¿›ç”Ÿæˆæ¨¡å‹æœ¬èº«ï¼Œè¿˜éœ€æ·±åº¦èåˆç‰©ç†å»ºæ¨¡ã€å®‰å…¨éªŒè¯ã€æœºå™¨äººæ§åˆ¶ç­‰é¢†åŸŸçš„çŸ¥è¯†ï¼Œæœ€ç»ˆæ„å»ºå‡ºçœŸæ­£å¯ä¿¡ã€å¯ç”¨çš„â€œå…·èº«ä¸–ç•Œæ¨¡å‹â€ã€‚",
            "timestamp": "2026-01-21T12:16:00.343029",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T18:57:34Z",
    "created_at": "2026-01-21T12:09:13.185373",
    "updated_at": "2026-01-21T12:09:13.185380"
}