{
    "id": "2601.07476v1",
    "title": "NanoCockpit: Performance-optimized Application Framework for AI-based Autonomous Nanorobotics",
    "authors": [
        "Elia Cereda",
        "Alessandro Giusti",
        "Daniele Palossi"
    ],
    "abstract": "åŸºäºŽè§†è§‰å¾®åž‹æœºå™¨å­¦ä¹ æ¨¡åž‹é©±åŠ¨çš„è‡ªä¸»çº³ç±³æ— äººæœºï¼Œä½œä¸ºä¸€ç§æ–°å…´æŠ€æœ¯ï¼Œå‡­å€Ÿå…¶å¹¿æ³›é€‚ç”¨æ€§åŠå¯¹èµ„æºå—é™åµŒå…¥å¼ç³»ç»Ÿçš„ç§‘å­¦æŽ¨åŠ¨ä½œç”¨æ­£å¿«é€Ÿå‘å±•ã€‚å…¶è½»é‡åŒ–æœºèº«ï¼ˆä»…æ•°åå…‹ï¼‰å°†æœºè½½è®¡ç®—èµ„æºä¸¥æ ¼é™åˆ¶åœ¨äºš100æ¯«ç“¦å¾®æŽ§åˆ¶å™¨å•å…ƒå†…ã€‚Bitcraze Crazyflieçº³ç±³æ— äººæœºä½œä¸ºäº‹å®žæ ‡å‡†å¹³å°ï¼Œæä¾›äº†ä¸°å¯Œçš„å¯ç¼–ç¨‹MCUèµ„æºï¼Œæ”¯æŒåº•å±‚æŽ§åˆ¶ã€å¤šæ ¸å¤„ç†ä¸Žæ— çº¿ç”µä¼ è¾“ã€‚ç„¶è€Œï¼Œç”±äºŽç¼ºä¹èƒ½å¤Ÿå¯¹å¤šç¼“å†²å›¾åƒé‡‡é›†ã€å¤šæ ¸è®¡ç®—ã€MCUé—´æ•°æ®äº¤æ¢å’ŒWi-Fiæµä¼ è¾“è¿›è¡Œæ—¶é—´æœ€ä¼˜æµæ°´çº¿å¤„ç†çš„è½»é‡é«˜æ•ˆè½¯ä»¶å±‚ï¼Œæœºå™¨äººç ”ç©¶è€…å¾€å¾€æœªèƒ½å……åˆ†åˆ©ç”¨è¿™äº›å®è´µçš„æœºè½½èµ„æºï¼Œå¯¼è‡´æŽ§åˆ¶æ€§èƒ½æœªè¾¾æœ€ä¼˜ã€‚æˆ‘ä»¬æå‡ºçš„NanoCockpitæ¡†æž¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œé€šè¿‡åŸºäºŽåç¨‹çš„å¤šä»»åŠ¡å¤„ç†æœºåˆ¶ï¼Œåœ¨æå‡ç³»ç»Ÿåžåé‡ä¸Žæœ€å°åŒ–å»¶è¿Ÿçš„åŒæ—¶ç®€åŒ–å¼€å‘ä½“éªŒã€‚åœ¨ä¸‰ä¸ªçœŸå®žåœºæ™¯çš„å¾®åž‹æœºå™¨å­¦ä¹ çº³ç±³æœºå™¨äººåº”ç”¨ä¸­è¿›è¡Œå®žåœ°å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ¡†æž¶å®žçŽ°äº†ç†æƒ³çš„ç«¯åˆ°ç«¯å»¶è¿Ÿï¼ˆå³ä¸²è¡ŒåŒ–ä»»åŠ¡é›¶å¼€é”€ï¼‰ï¼Œé—­çŽ¯æŽ§åˆ¶æ€§èƒ½èŽ·å¾—å¯é‡åŒ–æå‡ï¼ˆå¹³å‡ä½ç½®è¯¯å·®é™ä½Ž30%ï¼Œä»»åŠ¡æˆåŠŸçŽ‡ä»Ž40%æå‡è‡³100%ï¼‰ã€‚",
    "url": "https://arxiv.org/abs/2601.07476v1",
    "html_url": "https://arxiv.org/html/2601.07476v1",
    "html_content": "NanoCockpit: Performance-optimized Application Framework for AI-based Autonomous Nanorobotics\nEliaÂ Cereda,Â \nAlessandroÂ Giusti,Â \nandÂ DanieleÂ Palossi\nE. Cereda, A. Giusti, and D. Palossi are with the Dalle Molle Institute for Artificial IntelligenceÂ (IDSIA), USI-SUPSI, 6962 Lugano, Switzerland. Corresponding author:\nelia.cereda@idsia.ch\n.D. Palossi is also with the Integrated Systems Laboratory (IIS), ETH ZÃ¼rich, 8092 ZÃ¼rich, Switzerland.GitHub repository:\nhttps://github.com/idsia-robotics/crazyflie-nanocockpit\nAbstract\nAutonomous nano-drones, powered by vision-based tiny machine learning (TinyML) models, are a novel technology gaining momentum thanks to their broad applicability and pushing scientific advancement on resource-limited embedded systems.\nTheir small form factor, i.e., a few 10s grams, severely limits their onboard computational resources to sub-\n100\nmW\n100\\text{\\,}\\mathrm{mW}\nmicrocontroller units (MCUs).\nThe Bitcraze Crazyflie nano-drone is the\nde facto\nstandard, offering a rich set of programmable MCUs for low-level control, multi-core processing, and radio transmission.\nHowever, roboticists very often underutilize these onboard precious resources due to the absence of a simple yet efficient software layer capable of time-optimal pipelining of multi-buffer image acquisition, multi-core computation, intra-MCUs data exchange, and Wi-Fi streaming, leading to sub-optimal control performances.\nOur\nNanoCockpit\nframework aims to fill this gap, increasing the throughput and minimizing the systemâ€™s latency, while simplifying the developer experience through coroutine-based multi-tasking.\nIn-field experiments on three real-world TinyML nanorobotics applications show our framework achieves ideal end-to-end latency, i.e. zero overhead due to serialized tasks, delivering quantifiable improvements in closed-loop control performance (\nâˆ’\n-\n30% mean position error, mission success rate increased from 40% to 100%).\n01-introduction\nI\nHardware-software architecture\nI-A\nRobotic platform\nHardware.\nFigure\n1\n-A shows the hardware design of the Crazyflie extended with the AI-deck board and a radio-connected remote computer.\nOn the nano-drone, an STM32 MCU manages flight-control tasks, while a Nordic NRF51 handles the\n2.4\nGHz\n2.4\\text{\\,}\\mathrm{GHz}\nradio.\nThe AI-deck features a QVGA Himax camera (HM01B0), a GWT GAP8 octa-core RISC-V-based SoC, off-chip HyperDRAM/Flash of 8 and\n64\nMB\n64\\text{\\,}\\mathrm{MB}\n, respectively, and an Espressif ESP32 SoC for Wi-Fi connectivity.\nFigure\n1\n-A also depicts the internal architecture of the GAP8 and ESP32.\nGAP8 is composed of two general-purpose power domains, a single-core\nfabric controller\n(FC) for data-handling tasks and an octa-core\ncluster\n(CL) for computational-intensive workloads, such as vision-based TinyML algorithms\n[\n15\n,\n11\n]\n.\nGAP8 also features two on-chip memories, i.e. a\n512\nkB\n512\\text{\\,}\\mathrm{kB}\nL2 memory and a\n64\nkB\n64\\text{\\,}\\mathrm{kB}\nL1 low-latency scratchpad, a micro direct memory access (\nÎ¼\n\\mu\nDMA) unit to interact with off-chip peripherals, e.g., camera and memories, and a cluster DMA for data movement between on-chip memories.\nThe ESP32 features a 32-bit dual-core Xtensa LX6 CPU, a Wi-Fi radio,\n520\nkB\n520\\text{\\,}\\mathrm{kB}\non-chip SRAM and\n2\nMB\n2\\text{\\,}\\mathrm{MB}\noff-chip Flash memory.\nRemote communication is possible over two wireless channels: a low-latency and low-bandwidth Crazy Real-Time Protocol (CRTP) driven by the NRF51 and a high-bandwidth Crazyflie Packet eXchange (CPX)\n[\n3\n]\nprotocol, through the ESP32.\nThe former typically transfers control setpoints and logging, while the latter is meant for high-volume data exchange, such as image streaming.\nThe STM32 communicates over two universal asynchronous receiver-transmitter (UART) interfaces, with the NRF51 and with the GAP8.\nInstead, the GAP8 exchanges data with the ESP32 over serial peripheral interface (SPI), with the camera over camera parallel interface (CPI), and with off-chip memories over HyperBus.\nSoftware.\nFigure\n1\n-B illustrates the software tasks that typically run on the various MCUs aboard a Crazyflie and on a remotely connected computer, highlighting which tasks have been contributed by our framework.\nThe typical execution flow employs the STM32 for all the droneâ€™s basic functionalities, such as stabilization and control, and state estimation.\nThe GAP8 SoC typically handles computationally intensive tasks: image acquisition, processing, e.g., vision-based TinyML workloads, and optionally streaming to the remote computer of images and/or onboard processing results.\nThe ESP32 performs CPX routing between the SPI and Wi-Fi interfaces to feed the remote computer that can collect data and perform off-board computation, optionally leveraging the Robot Operating System (ROS)\n[\n13\n]\n.\nAs low-level software stacks, the STM32 and ESP32 use the FreeRTOS real-time operating system (RTOS)\n[\n1\n]\nwith full pre-emptive multitasking, while the GAP8 runs a lightweight runtime, called PMSIS.\nOur framework extends the PMSIS functionalities, offers optimized camera drivers, and improves the Wi-Fi stack.\nFigure 1:\nHardware (A) and software (B) overviews of the Crazyflie nano-drone with the AI-deck companion board and a remote computer.\nFigure 2:\n(A) Serialized execution without our NanoCockpit framework vs. (B) pipelined (double-buffered) execution with our framework.\nI-B\nFramework architecture\nCoroutine-based multi-tasking.\nThe ideal closed-loop throughputs from Table\nLABEL:tab:related_work\nare achieved when all computational units are always loaded with useful work, i.e., zero idle time.\nIn our context, camera acquisition and communication must overlap with GAP8â€™s computation, e.g., TinyML inference, by employing DMA-based data transfers and multi-tasking execution.\nHowever, GAP8â€™s PMSIS runtime is a single-thread hardware abstraction layer and only supports asynchronous execution via event-based callbacks.\nThis programming model requires significant effort to implement cooperative behavior and forces programmers to explicitly handle all events and their synchronization\n[\n2\n]\n.\nThe control flow of each task ends up spread among many callback functions.\nSo far, this limitation prevented fine-grained task overlapping and forced several SotA works\n[\n14\n,\n5\n]\nto resort to suboptimal serialized execution, where camera capture, inference, and SPI transmission (TX) tasks execute one after the other, as Figure\n2\n-A.\nOur NanoCockpit framework extends PMSIS with a custom asynchronous and cooperative programming layer based on stackless co-routines\n[\n2\n]\n, allowing the programmer to easily suspend a task and resume it when a desired condition is met.\nIn Figure\n2\n-B, we show an example of the fully pipelined execution enabled by our framework.\nEach concurrent task is implemented as a distinct co-routine that can use synchronous control flow constructs (e.g.,\nif\n, loops), suspend itself, and yield control to another task (details in Appendix\nV\n).\nThe proposed mechanism is extremely fast, with sub-\n10\nÂµ\nâ€‹\ns\n10\\text{\\,}\\mathrm{\\SIUnitSymbolMicro s}\ncontext switches (on the order of a function call) and, compared to a full RTOS such as FreeRTOS on STM32, it has 8\nÃ—\n\\times\nlower memory overhead (i.e.,\n18\nB\n18\\text{\\,}\\mathrm{B}\nper task vs.\nâ‰¥\n\\geq\n150\nB\n150\\text{\\,}\\mathrm{B}\nrequired by each taskâ€™s stack in FreeRTOS).\nMulti-buffered camera drivers.\nOur Himax camera has two operative modes: a\ntrigger\nmode, where the GAP8 acts as master, requesting single images (up to\nâˆ¼\n\\sim\n30\nframe\n/\ns\n30\\text{\\,}\\mathrm{frame}\\mathrm{/}\\mathrm{s}\n), and a\nstreaming\nmode, where the camera is programmed once and then produces a continuous stream of images.\nThe streaming mode allows for the highest frame rate (up to\n150\nframe\n/\ns\n150\\text{\\,}\\mathrm{frame}\\mathrm{/}\\mathrm{s}\nwith our framework), but the GAP8 must work synchronously with the camera and be ready to store a new image every time it is produced.\nHowever, the cameraâ€™s stock drivers and GAP8 runtime do not support this optimized execution flow.\nOur framework provides optimized camera drivers (fine-grained timing control), allowing the camera to operate at the ideal frame rate for the application and a software layer for pipelined multi-buffered acquisition, preventing frame dropping and jittering.\nIn Figure\n2\n-B, we show an example of double-buffered image acquisition, where the\nÎ¼\n\\mu\nDMA acquires new images in parallel with the CLâ€™s inference and SPI TX.\nZero-copy Wi-Fi stack.\nThe original Wi-Fi stack based on the CPX protocol suffers from high latency, serial-only execution, and lack of congestion control\n[\n3\n]\n.\nWe provide a novel communication stack that fulfills the CPX specifications while enabling high-throughput and low-latency bi-directional Wi-Fi streaming between the drone and a remote computer.\nOur stack, which spans the STM32, GAP8, and ESP32 MCUs, also offers hardware-based timestamping for precise synchronization of data acquired across different onboard MCUs, which is fundamental for dataset collection.\nCompared to the original CPX stack, our version implements zero-copy packet transmission on GAP8 and a multi-tasking and multi-buffer router implementation on ESP32, enabling complete overlapping of SPI and Wi-Fi transfers, as shown in Figure\n2\n-B.\nOur stack can stream\n160\nÃ—\n160\npx\n160\\times$160\\text{\\,}\\mathrm{px}$\nimages at\n72\nHz\n72\\text{\\,}\\mathrm{Hz}\n, i.e.,\n2.4\nÃ—\n2.4\\times\nhigher than the original CPX.\nII\nResults\nIn this section, we explore three different real-world applications, and to assess their quantitative figures, we employ a motion capture system (additional details in Appendix\nVI\n).\nII-A\nHuman pose estimation\nA convolutional neural network (CNN) takes as input a\n160\nÃ—\n96\npx\n160\\times$96\\text{\\,}\\mathrm{px}$\nimage and outputs the subjectâ€™s pose relative to the droneâ€™s horizontal frame.\nThe poses are filtered by a Kalman filter and used by a closed-loop velocity controller to keep the drone at a fixed distance in front of the subject, i.e.,\n1.5\nm\n1.5\\text{\\,}\\mathrm{m}\n.\nTo enrich our analysis, we evaluate two CNN models: PULP-Frontnet\n[\n8\n]\nrunning onboard and a MobileNetV2-based CNN\n[\n9\n]\n, running on a remote computer.\nPULP-Frontnet has\n304\nk\n304\\text{\\,}\\mathrm{k}\nparameters and requires\n14.3\nMMAC\n14.3\\text{\\,}\\mathrm{MMAC}\nper inference, while the MobileNetV2 has\n7\nÃ—\n7\\times\nmore parameters and requires\n90\nMMAC\n90\\text{\\,}\\mathrm{MMAC}\noperations per inference.\nTABLE I:\nHuman pose estimation control performance.\nModel\nThroughput\n[\nHz\n\\mathrm{Hz}\n]\nEnd-to-end\nlatency\n[\nms\n\\mathrm{ms}\n]\nControl error\ne\nx\nâ€‹\ny\ne_{xy}\n[\nm\n\\mathrm{m}\n]\ne\nÎ¸\ne_{\\theta}\n[\nrad\n\\mathrm{r}\\mathrm{a}\\mathrm{d}\n]\nOnboard\nPULP-Frontnet\n[\n8\n]\n12\n30.3\n0.96\n0.63\n24\n30.3\n1.01\n0.69\n48\n30.3\n0.80\n0.50\nRemote\nCereda\net al.\n[\n9\n]\n10\n136.6\n0.83\n0.55\n20\n146.2\n0.74\n0.46\n40\n168.6\n0.65\n0.41\nCereda\net al.\n[\n9\n]\nw/\n500\nms\n500\\text{\\,}\\mathrm{ms}\ndelay\n10\n636.6\n0.87\n0.55\n20\n646.2\n0.91\n0.50\n40\n668.6\n0.81\n0.46\nFigure 3:\nClosed-loop control performance in the A) drone-to-drone localization and B) nano-drone racing obstacle avoidance experiments.\nA human subject, not included in training data, walks on a predefined path while the nano-drone predicts its pose and follows it.\nFor each CNN, we test three different throughput configurations, and for each, we run three tests.\nThe throughput represents how often a new camera acquisition and inference is triggered, while the end-to-end latency measures the time from the image acquisition to the CNNâ€™s output reaching the low-level controller.\nTo assess the impact of increased throughput on the final control errors, ideally, we would like to keep constant end-to-end latency for all configurations.\nHowever, due to Wi-Fi congestion, the MobileNetV2 can be affected by unpredictable end-to-end latencies.\nTo decouple the effect of throughput from end-to-end latency, we introduce a third experiment with the MobileNetV2-based model, where we inject an additional latency of\n500\nms\n500\\text{\\,}\\mathrm{ms}\nwhile keeping the same throughputs of the previous one.\nTable\nI\nreports the mean horizontal position (\ne\nx\nâ€‹\ny\ne_{xy}\n) and mean angular (\ne\nÎ¸\ne_{\\theta}\n) errors.\nFor each model, the highest throughput consistently scores the lowest errors, while, among all models, the peak performance comes with the biggest CNN, the higher throughput, and the lower latency.\nIn fact, the off-board MobileNetV2 running at\n40\nHz\n40\\text{\\,}\\mathrm{Hz}\nachieves a 19% lower error despite a more than\n100\nms\n100\\text{\\,}\\mathrm{ms}\nlatency compared to the PULP-Frontnet and scores almost on-par when affected by a high latency, i.e.,\n>\n>\n600\nms\n600\\text{\\,}\\mathrm{ms}\n.\nThis trend suggests that high-throughput inference, with more predictions per second for the filtering process, reduces the impact of noise on predictions and leads to better final performance.\nII-B\nDrone-to-drone localization\nOur Crazyflie (called\nobserver\n) is tasked to estimate the relative pose of a target peer nano-drone in its field of view.\nA fully convolutional neural network\n[\n10\n]\n(FCNN) takes in input a\n160\nÃ—\n160\npx\n160\\times$160\\text{\\,}\\mathrm{px}$\nimage from the onboard camera and produces three\n20\nÃ—\n20\npx\n20\\times$20\\text{\\,}\\mathrm{px}$\nprobability maps, that are processed to extract the targetâ€™s image-plane coordinates\n(\nu\n,\nv\n)\n(u,v)\n, and distance\nd\nd\n.\nThe target drone performs a predefined 10-meter spiral trajectory in front of the observer drone, which uses the FCNN and the controller onboard to track and follow it.\nWe reproduce the trajectory over either 48 or\n24\ns\n24\\text{\\,}\\mathrm{s}\n(average target drone speeds of 0.21 and\n0.34\nm\n/\ns\n0.34\\text{\\,}\\mathrm{m}\\mathrm{/}\\mathrm{s}\n, respectively).\nAt each speed, we test three end-to-end throughputs (10, 20, and\n39\nHz\n39\\text{\\,}\\mathrm{Hz}\n), i.e., accounting from the image acquisition to the final pose forwarded to the low-level controller.\nEach test is repeated three times.\nFigure\n3\n-A shows that higher throughputs reduce both the mean position error\ne\nð‘¥ð‘¦ð‘§\ne_{\\mathit{xyz}}\n, i.e. Euclidean distance in 3D space, and the instances of target tracking loss (\nÃ—\n\\times\nin the plot).\nThe benefits of an increased throughput are clear for both target speed configurations: at\n0.21\nm\n/\ns\n0.21\\text{\\,}\\mathrm{m}\\mathrm{/}\\mathrm{s}\n, we lose track of the target only once at\n10\nHz\n10\\text{\\,}\\mathrm{Hz}\n, while at\n0.34\nm\n/\ns\n0.34\\text{\\,}\\mathrm{m}\\mathrm{/}\\mathrm{s}\n, we lose track of the target in 2 out of 3 runs at\n10\nHz\n10\\text{\\,}\\mathrm{Hz}\nand only one time at\n20\nHz\n20\\text{\\,}\\mathrm{Hz}\n.\nOnly at the highest throughput, the observer drone never loses track of the target, achieving the best\ne\nð‘¥ð‘¦ð‘§\ne_{\\mathit{xyz}}\nerror of\n0.18\nm\n0.18\\text{\\,}\\mathrm{m}\n.\nII-C\nNano-drone racing\nOur obstacle avoidance task is based on a CNN for a drone racing scenario\n[\n12\n]\n, which is fed with onboard\n162\nÃ—\n162\npx\n162\\times$162\\text{\\,}\\mathrm{px}$\nimages and predicts three probabilities of collision\n[\n0\n,\n1\n]\n[0,1]\n: left, center, and right vertical splits of the image.\nThe model has\n331\nk\n331\\text{\\,}\\mathrm{k}\nparameters and requires\n25\nMMAC\n25\\text{\\,}\\mathrm{MMAC}\noperations per frame, achieving up to\n30\nHz\n30\\text{\\,}\\mathrm{Hz}\nthroughput on the GAP8 with our framework.\nThe collision probabilities are filtered by a low-pass filter and used to drive the droneâ€™s forward speed.\nThe drone takes off at\n4\nm\n4\\text{\\,}\\mathrm{m}\nfrom the obstacle and flies toward it until the collision probability reaches a threshold, breaking until stopping.\nIn the first\n2\nm\n2\\text{\\,}\\mathrm{m}\n, the predictions are always at 0, allowing the controller to reach the desired max speed.\nIf the drone stops between 0.15 and\n2\nm\n2\\text{\\,}\\mathrm{m}\nfrom the obstacle, we consider the test successful (ideal) or failed otherwise (too early/late).\nIn Figure\n3\n-B, we explore three max speeds (1, 2, and\n3\nm\n/\ns\n3\\text{\\,}\\mathrm{m}\\mathrm{/}\\mathrm{s}\n) and two CNN throughputs either at\n5\nHz\n5\\text{\\,}\\mathrm{Hz}\nor\n30\nHz\n30\\text{\\,}\\mathrm{Hz}\n, coupled with 0.05 and 0.30 breaking threshold, respectively.\nFor each configuration, we repeat five runs.\nAs the velocity grows, the advantage of higher throughputs becomes evident, with a 60% success rate at the highest speed.\nIII\nConclusion\nThis work presents the NanoCockpit framework for increasing the throughput and minimizing the systemâ€™s latency while simplifying the programmability of the Crazyflie nano-drone.\nReal-world experiments demonstrate how increased throughput and reduced perception-to-control latency are fundamental sources of improved behavior of the nano-drone in all use cases.\nFinally, we open-source our framework for the benefit of the wider research community.\nTABLE II:\nDescription of supplementary materials\nSupplementary material\nDescription\nGitHub repository\nhttps://github.com/idsia-robotics/crazyflie-nanocockpit\nSource code of our NanoCockpit framework:\nâ€¢\nFramework code for the GAP8, ESP32, and STM32 MCUs and the remote computer (ROS and Python).\nâ€¢\nStandalone examples of our co-routine and synchronization primitives, CPX communication stack, camera acquisition and streaming (Section\nI-B\n).\nâ€¢\nFull application deployed on top of our framework: PULP-Frontnet CNN\n[\n8\n]\nand closed-loop controller for the human pose estimation experiment (Section\nII-A\n).\nâ€¢\nProfiling and debugging tools: GAP8 and ESP32 event trace collection (over GPIO or UART), Wireshark protocol dissectors to inspect CPX Wi-Fi traffic.\nâ€¢\nManually filtered and annotated dataset from the literature review in Figure\nLABEL:fig:scopus\n.\nsupplementary-materials.pdf\nAdditional methodology and implementation details:\nâ€¢\nMethodology, Scopus queries, and in-depth analysis of our literature review.\nâ€¢\nImplementation details of our co-routine primitive.\nâ€¢\nDetailed descriptions of our experiment tasks, neural network deployment, experimental setup, and the evaluation metrics.\nReferences\n[1]\nR. Barry and the FreeRTOS Team\n(2025)\nFreeRTOS kernel\n.\nReal Time Engineers Ltd.\n.\nExternal Links:\nLink\nCited by:\nÂ§\nI-A\n.\n[2]\nB. Belson, J. Holdsworth, W. Xiang, and B. Philippa\n(2019-06)\nA survey of asynchronous programming using coroutines in the internet of things and embedded systems\n.\nACM Trans. Embed. Comput. Syst.\n18\n(\n3\n).\nExternal Links:\nISSN 1539-9087\n,\nDocument\nCited by:\nÂ§\nI-B\n,\nÂ§\nI-B\n,\nÂ§V\n.\n[3]\nBitcraze\n(2025)\nCPX - Crazyflie Packet eXchange\n(Website)\nExternal Links:\nLink\nCited by:\nÂ§\nI-A\n,\nÂ§\nI-B\n.\n[4]\nS. Bonato, S. C. Lambertenghi, E. Cereda, A. Giusti, and D. Palossi\n(2023)\nUltra-low power deep learning-based monocular relative localization onboard nano-quadrotors\n.\nIn\n2023 IEEE International Conference on Robotics and Automation (ICRA)\n,\nVol.\n,\npp.Â 3411â€“3417\n.\nExternal Links:\nDocument\nCited by:\nFigure 6\n.\n[5]\nR. J. Bouwmeester, F. Paredes-VallÃ©s, and G. C. De Croon\n(2023)\nNanoFlowNet: real-time dense optical flow on a nano quadcopter\n.\nIn\n2023 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 1996â€“2003\n.\nCited by:\nÂ§\nI-B\n.\n[6]\nN. Bruschi, A. Garofalo, F. Conti, G. Tagliavini, and D. Rossi\n(2020)\nEnabling mixed-precision quantized neural networks in extreme-edge devices\n.\nIn\nProceedings of the 17th ACM International Conference on Computing Frontiers\n,\nCF â€™20\n,\nNew York, NY, USA\n,\npp.Â 217â€“220\n.\nExternal Links:\nISBN 9781450379564\n,\nLink\n,\nDocument\nCited by:\nÂ§\nVI-D\n.\n[7]\nA. Burrello, A. Garofalo, N. Bruschi, G. Tagliavini, D. Rossi, and F. Conti\n(2021)\nDORY: automatic end-to-end deployment of real-world DNNs on low-cost IoT MCUs\n.\nIEEE Transactions on Computers\n70\n(\n8\n),\npp.Â 1253â€“1268\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nVI-D\n.\n[8]\nE. Cereda, M. Ferri, D. Mantegazza, N. Zimmerman, L. M. Gambardella, J. Guzzi, A. Giusti, and D. Palossi\n(2021)\nImproving the generalization capability of DNNs for ultra-low power autonomous nano-UAVs\n.\nIn\n2021 17th International Conference on Distributed Computing in Sensor Systems (DCOSS)\n,\npp.Â 327â€“334\n.\nCited by:\nÂ§\nII-A\n,\nTABLE I\n,\n3rd item\n,\nÂ§\nVI-A\n.\n[9]\nE. Cereda, A. Giusti, and D. Palossi\n(2023)\nSecure deep learning-based distributed intelligence on pocket-sized drones\n.\nIn\nProceedings of the 2023 International Conference on Embedded Wireless Systems and Networks\n,\nEWSN â€™23\n,\nNew York, NY, USA\n,\npp.Â 409â€“414\n.\nCited by:\nÂ§\nII-A\n,\nTABLE I\n,\nTABLE I\n,\nÂ§\nVI-A\n.\n[10]\nL. Crupi, A. Giusti, and D. Palossi\n(2024)\nHigh-throughput visual nano-drone to nano-drone relative localization using onboard fully convolutional networks\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\nVol.\n,\npp.Â 5345â€“5351\n.\nCited by:\nÂ§\nII-B\n,\nÂ§\nVI-B\n,\nÂ§\nVI-B\n.\n[11]\nL. Lamberti, L. Bompani, V. J. Kartsch, M. Rusci, D. Palossi, and L. Benini\n(2023)\nBio-inspired autonomous exploration policies with CNN-based object detection on nano-drones\n.\nIn\n2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)\n,\nVol.\n,\npp.Â 1â€“6\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nI-A\n.\n[12]\nL. Lamberti, E. Cereda, G. Abbate, L. Bellone, V. J. K. Morinigo, M. BarciÅ›, A. BarciÅ›, A. Giusti, F. Conti, and D. Palossi\n(2024)\nA sim-to-real deep learning-based framework for autonomous nano-drone racing\n.\nIEEE Robotics and Automation Letters\n9\n(\n2\n),\npp.Â 1899â€“1906\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-C\n,\nÂ§\nVI-C\n.\n[13]\nS. Macenski, T. Foote, B. Gerkey, C. Lalancette, and W. Woodall\n(2022)\nRobot Operating System 2: design, architecture, and uses in the wild\n.\nScience Robotics\n7\n(\n66\n),\npp.Â eabm6074\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nI-A\n.\n[14]\nV. Niculescu, L. Lamberti, F. Conti, L. Benini, and D. Palossi\n(2021)\nImproving autonomous nano-drones performance via automated end-to-end optimization and deployment of DNNs\n.\nIEEE Journal on Emerging and Select Topics in Circuits and Systems\n11\n(\n4\n),\npp.Â 548â€“562\n.\nCited by:\nÂ§\nI-B\n.\n[15]\nD. Palossi, N. Zimmerman, A. Burrello, F. Conti, H. MÃ¼ller, L. M. Gambardella, L. Benini, A. Giusti, and J. Guzzi\n(2021)\nFully onboard AI-powered human-drone pose estimation on ultra-low power autonomous flying nano-UAVs\n.\nIEEE Internet of Things Journal\n(\n),\npp.Â 1â€“1\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nI-A\n,\nÂ§\nVI-A\n.\n[16]\nM. Spallanzani, G. Rutishauser, M. Scherer, P. Wiese, and F. Conti\n(2025)\nQuantLib\n.\nExternal Links:\nLink\nCited by:\nÂ§\nVI-D\n.\nIV\nNanorobotics survey\nTABLE III:\nSurvey on nano-quadrotors appearing in peer-reviewed scientific publications in the last five years.\nNano-drone\nDimensions\nL\nÃ—\nW\nÃ—\nH\nL\\times W\\times H\n[\nmm\n\\mathrm{mm}\n]\nWeight\n[\ng\n\\mathrm{g}\n]\nBattery\n[\nmA\nh\n\\mathrm{mA}\\text{\\,}\\mathrm{h}\n]\nFlight time\n[\nmin\n\\mathrm{min}\n]\nSoC\nExpansion\ninterface\nOpen\nfirmware\nOpen\nhardware\nPublications\n2020\n2021\n2022\n2023\n2024\nTotal\nBitcraze Crazyflie\n92\nÃ—\n92\nÃ—\n29\n92\\times 92\\times 29\n27\n250\n7\nSTM32F405\nâœ“\nâœ“\nâœ“\n22\n55\n99\n107\n98\n381\nDJI Tello\n98\nÃ—\n93\nÃ—\n41\n98\\times 93\\times 41\n80\n1100\n13\nIntel Movidius Myriad 2\nâœ—\nâœ—\nâœ—\n2\n17\n29\n38\n42\n128\nParrot Mambo\n160\nÃ—\n78\nÃ—\n9.8\n160\\times 78\\times 9.8\n62\n660\n10\nParrot P6 (ARM9)\nâœ—\nâœ—\nâœ—\n4\n2\n12\n17\n15\n50\nParrot Rolling Spider\n140\nÃ—\n140\nÃ—\n36\n140\\times 140\\times 36\n57\n550\n7\nParrot P6 (ARM9)\nâœ—\nâœ—\nâœ—\n2\n1\n1\n1\n0\n5\nESPcopter\n90\nÃ—\n90\nÃ—\n35\n90\\times 90\\times 35\n35\n240\n7\nESP8266\nâœ“\nâœ“\nâœ“\n0\n0\n2\n1\n1\n4\nSyma x27 Ladybug\n103\nÃ—\n103\nÃ—\n26\n103\\times 103\\times 26\nâ€“\n200\n7\nâ€“\nâœ—\nâœ—\nâœ—\n0\n1\n0\n1\n0\n2\nArcade PICO\n90\nÃ—\n90\nÃ—\n30\n90\\times 90\\times 30\n30\n150\n5.5\nâ€“\nâœ—\nâœ—\nâœ—\n1\n1\n0\n0\n1\n3\nCheerson CX-10W\n32\nÃ—\n32\nÃ—\n22\n32\\times 32\\times 22\n17\n150\n4\nSTM32F050K\nâœ—\nâœ—\nâœ—\n1\n0\n0\n0\n0\n1\nPluto X\n160\nÃ—\n160\nÃ—\n45\n160\\times 160\\times 45\n60\n600\n9\nSTM32F303\nâœ“\nâœ“\nâœ“\n1\n0\n0\n0\n0\n1\nRadioShack DIY Drone\nâ€“\nâ€“\n250\n5\nâ€“\nâœ—\nâœ—\nâœ—\n1\n0\n0\n0\n0\n1\nCustom\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâœ—\nâœ—\nâœ—\n0\n1\n1\n1\n4\n7\nTable\nIII\nsurveys the most popular nano-quadrotors used in research, and provides the raw information used to build Figure\nLABEL:fig:scopus\n, in Section\nLABEL:sec:intro\n.\nThe most widely used nano-drone is the Bitcraze Crazyflie\n1\n1\n1\nhttps://www.bitcraze.io/products/crazyflie-2-1-plus/\n, referenced in 381 publications over the last five years (65%), for which we provide a detailed description of its architecture in Section\nI\n-A.\nThe second most used is the DJI Tello\n2\n2\n2\nhttps://store.dji.com/en/product/tello\n(and TelloEDU) nano-quadrotor, which accounts for 21% of publications.\nThe Parrot Mambo\n3\n3\n3\nhttps://www.parrot.com/en/support/documentation/mambo-range\nand its variant, the Parrot Rolling Spider, account together for 9% of publications in the same five years.\nCompared to the Crazyflie, these are all slightly larger drones and come with closed-source firmware and hardware.\nNonetheless, they allow for the development of off-board algorithms that can be tested via their Python SDKs.\nAll remaining nano-drones rarely appear in research, with just 1 to 5 publications each, over five years.\nAmong these, the ESPcopter and Pluto X nano-quadrotors are the most interesting as they are based on the ESP8266 and the STM32F303 microcontroller unit (MCU), respectively.\nBoth nano-drones share Crazyflieâ€™s modular design and offer fully open-source firmware, making them attractive alternatives for education and research.\nHowever, to date, they do not have onboard cameras or additional computational resources, limiting their applicability either to simple algorithms or requiring off-board processing.\nThe Syma X27 Ladybug, Arcade PICO, Cheerson CX-10W, and the RadioShack DIY Drone Kit are all consumer-oriented toy drones with limited adoption in research, despite their attractive ultra-small form factor, e.g., the Cheerson is a\n3\nÃ—\n3\ncm\n3\\times$3\\text{\\,}\\mathrm{cm}$\n(\nlength\nÃ—\nwidth\n\\mathrm{length}\\times\\mathrm{width}\n) tiny nano-drone.\nFinally, a handful of publications propose their own custom-built solutions, showing limited research interest in developing novel nano-quadrotor platforms and preferring to leverage existing ones.\nâ¬‡\nTITLE\n-\nABS\n-\nKEY\n(\n(\n(\nnano\n-\nuav\nOR\nnano\n-\nuavs\nOR\nnano\n-\ndrone\nOR\nnano\n-\ndrones\nOR\nnanocopter\nOR\nnano\n-\nquadrotor\n)\nOR\n(\npalm\n-\nsize\nAND\nuav\n)\nOR\n(\npalm\n-\nsize\nAND\ndrone\n)\nOR\n(\npocket\n-\nsize\nAND\ndrone\n)\nOR\n(\nmini\n-\ndrone\n)\nOR\n(\nmini\n-\nuav\n)\nOR\n(\npocket\n-\nsized\nAND\nuav\n)\nOR\n(\npalm\n-\nsized\nAND\nuav\n)\nOR\n(\npocket\n-\nsized\nAND\ndrone\n)\nOR\n(\npalm\n-\nsized\nAND\ndrone\n)\nOR\n(\nmini\n-\nuav\n)\n)\nOR\n(\n(\nCrazyflie\n)\nOR\n(\nTello\nAND\n(\ndrone\nOR\nuav\nOR\nquadrotor\n))\nOR\n(\nladybug\nOR\n(\nsyma\nAND\n(\nladybug\nOR\nx27\n))\nAND\n(\ndrone\nOR\nuav\nOR\nquadrotor\n))\nOR\n(\nMambo\nAND\n(\ndrone\nOR\nuav\nOR\nquadrotor\n))\nOR\n(\n\"Rolling\nSpider\"\nAND\n(\ndrone\nOR\nuav\nOR\nquadrotor\n))\nOR\n(\nCrazePony2\nAND\n(\ndrone\nOR\nuav\nOR\nquadrotor\n))\nOR\n(\nArduBee\nAND\n(\ndrone\nOR\nuav\nOR\nquadrotor\n))\nOR\n(\nESPcopter\n)\nOR\n(\n\"CX-10W\"\nAND\n(\ndrone\nOR\nuav\nOR\nquadrotor\n))\nOR\n(\nPlutoX\nAND\n(\ndrone\nOR\nuav\nOR\nquadrotor\n))\nOR\n(\n\"Arcade\nPico\"\nAND\n(\ndrone\nOR\nuav\nOR\nquadrotor\n))\nOR\n(\nRadioShack\nAND\n(\ndrone\nOR\nuav\nOR\nquadrotor\n))\n)\n)\nAND\nPUBYEAR\n>\n2019\nAND\nPUBYEAR\n<\n2025\nAND\nNOT\nDOCTYPE\n(\ncr\n)\nAND\nNOT\nDOCTYPE\n(\ner\n)\nListingÂ 1:\nElsevier Scopus query used for our nanorobotics survey.\nWith these aerial nanorobotics platforms in mind, we query the Elsevier Scopus citation database (update date: 9 Jan 2026) to collect reliable metrics about their employment in scientific publications.\nWe retrieve the Scopus data for all nano-drone publications over the last five years, starting with the query in Listing\n1\nand focusing on the years 2020â€“2024 to account for the\nâˆ¼\n\\sim\nsix month delay of the Scopus indexing process.\nThis first query results in 710 entries, which we complement with 325 other publications acknowledged by Bitcraze in their website\n4\n4\n4\nhttps://www.bitcraze.io/portals/research/\n.\nAfter removing duplicates and manually checking the fit w.r.t. our scope, i.e., surveying all and only publications employing nano-quadrotors, we obtain the final 586 entries reported in Table\nIII\n.\nThe final CSV file with our survey is provided on the NanoCockpit repository (available online).\nV\nCo-routine implementation\nFigure 4:\nCo-routine-based cooperative multi-tasking. A) C API example of our co-routine implementation. B) Execution flow with interactions among tasks and event loop. C) Co-routine state machine with start, running, and end states and the three suspension points. D) Internal implementation of co-routine structures.\nStackless co-routines\n[\n2\n]\nare structured asynchronous programming primitives that enable cooperative concurrency with extremely low memory and time overheads.\nThey represent a common programming pattern with a clear control flow that eases the development, improves readability, and fosters the reusability of code.\nWe provide a co-routine example in Figure\n4\n: a\nmain_task\nstarts two sub-tasks, i.e.,\ntask1\nand\ntask2\n, in a fork-join scenario, which reflects common situations in an embedded application.\nThe two tasks execute concurrently in background while the\nmain_task\nsuspends and waits for their completion.\nThis scenario matches, for example, the camera acquisition loop, which concurrently starts the\ninference\nand streaming\nSPI TX\ntasks (Figure\n2\n-A) for each frame and waits until both tasks complete before proceeding to the next frame.\nFigure\n4\n-A demonstrates the syntax of our co-routine API.\nA co-routine is delimited by the\nCO_FN_BEGIN\nand\n_END\nmacros, which define the co-routineâ€™s name (e.g.,\nmain_task\n), a pointer-sized parameter (\nargs\n, 4 bytes), and its body.\nEach\nco-routine instance\n(i.e.,Â an invocation of a co-routine running at a given time) stores its state in an\nexecution context\nobject,\nco_fn_ctx_t\n.\nAs the main task starts two co-routine instances,\ntask1\nand\ntask2\n, in Figure\n4\n-A, it needs to reserve two execution contexts to store their respective states.\nEvent\nobjects,\nco_event_t\n, represent external events that a co-routine can wait upon.\nAn event is initialized with\nco_event_init\nand waited upon using the\nCO_WAIT\nmacro.\nCO_WAIT\ndefines a\nsuspension point\nin the co-routine: when reached, it registers the co-routine in a linked list of\nwaiters\nstored by the event, then suspends the co-routine, storing the co-routineâ€™s state in its execution context.\nThe execution flow of our example is detailed in the temporal diagram of Figure\n4\n-B.\nAn event loop, provided by the PMSIS hardware abstraction layer, keeps track of events that have completed.\nWhen the\ntask1\nand\ntask2\nco-routine are started, theyâ€™re pushed on the event loopâ€™s ready list.\nWhen the\nmain_task\nreaches a\nCO_WAIT\nwait call, it yields control to the event loop, which then processes the next ready events in order.\nThis implements a cooperative multi-tasking system in which tasks must regularly yield to the event loop to ensure all events are processed on time.\nThe\ntask#_start_async\nfunctions are examples of\nasynchronous functions\n, functions which which start a background task that continues beyond the their own execution.\nIn PMSIS API conventions, these functions are identified by the\n_async\nsuffix appended to their name and take an\nevent\nobject as their last parameter, to notify the caller of the background taskâ€™s completion.\nOur\nco_event_t\nevent object can be passed to all PMSIS asynchronous functions, to allow our co-routines to wait on any background task from PMSIS drivers.\nFigure\n4\n-D shows the internal details of the two key C structures,\nco_event_t\nand\nco_fn_ctx_t\n.\nUpon suspension, a co-routine stores the suspension point from which the co-routine will resume (\nresume_point\n,\n2\nbytes\n2\\text{\\,}\\mathrm{b}\\mathrm{y}\\mathrm{t}\\mathrm{e}\\mathrm{s}\n) when the waited-upon event is completed (\nresume_task\n,\n4\nbytes\n4\\text{\\,}\\mathrm{b}\\mathrm{y}\\mathrm{t}\\mathrm{e}\\mathrm{s}\npointer).\nThe\nresume_point\nidentifies the current state of the co-routine, in the state machine from Figure\n4\n-C.\nWhen a\nco_event_t\nis completed, its list of waiters is walked through by the event loop and all waiting co-routines are resumed, restarting their execution from their respective\nresume_point\n.\nNotably, our implementation does not keep a private stack for each co-routine, saving memory, which is a critical resource in any MCU-class device.\nA single stack, owned by the main task, is shared across all co-routines, with each co-routine accessing it while actively running.\nHowever, the lack of a private stack limits the use of local variables across suspension points.\nA co-routine has access to the shared stack only while running.\nAs such, local data must be stored in static variables (for co-routines that do not need to be re-entrant, i.e.,Â for which only a single instance will be running at any given time) or in memory allocated by the programmer for this purpose (e.g.,Â passed through co-routineâ€™s\nargs\nparameter).\nIn practice, explicitly managing memory forces the programmer to reason about the lifetime of variables, ultimately encouraging clearer and more efficient programming.\nComparison to GAP SDK.\nThe difference between co-routines and the traditional multi-tasking model provided by the PMSIS GAP SDK is that\nco_event_t\ndecouples the moment an event is initialized from the moment a co-routine waits for it.\nMultiple waiters can all listen to the same event.\nThis allows a task to spawn sub-tasks, perform its work, and suspend itself when it finally needs the results, waiting on the sub-task completion events.\nVI\nExperimental setup\nFigure 5:\nAutonomous tasks considered in our three in-field experiments: A) human pose estimation, B) drone-to-drone localization, and C) nano-drone racing.\nFigure 6:\nThree-dimensional trajectory flown by the target nano-drone during our drone-to-drone localization experiment\n[\n4\n]\n.\nVI-A\nHuman pose estimation\nA convolutional neural network (CNN)\n[\n8\n,\n9\n]\ntakes a single gray-scale\n160\nÃ—\n96\npx\n160\\times$96\\text{\\,}\\mathrm{px}$\ncamera frame as input, and outputs the subjectâ€™s pose\n(\nx\n,\ny\n,\nz\n,\nÏ•\n)\n(x,y,z,\\phi)\nrelative to droneâ€™s horizontal frame, depicted in Figure\n5\n-A.\nThe estimated poses are filtered by a Kalman filter, and used to drive a â€œfollow-meâ€ velocity controller that keeps the drone in front of the subject, in the center of the image.\nWe evaluate our system with the experimental setup from the literature\n[\n15\n]\n.\nWe track the subjectâ€™s and droneâ€™s absolute poses along a pre-determined test trajectory in a mocap-equipped laboratory.\nWe report two metrics of control performance used in the literature.\nThe\nmean horizontal position error\ne\nð‘¥ð‘¦\ne_{\\mathit{xy}}\nrepresents the average distance in the XY plane of the drone from its desired position\n1.5\nm\n1.5\\text{\\,}\\mathrm{m}\nin front of the subject.\nThe\nmean angular error\ne\nÎ¸\ne_{\\theta}\nmeasures the average angle between the droneâ€™s orientation and its desired orientation (i.e.,Â looking directly at the subject).\nVI-B\nDrone-to-drone localization\nA fully convolutional neural network (FCNN)\n[\n10\n]\ntakes a single gray-scale\n160\nÃ—\n160\npx\n160\\times$160\\text{\\,}\\mathrm{px}$\ncamera frame as input, and outputs three\n20\nÃ—\n20\npx\n20\\times$20\\text{\\,}\\mathrm{px}$\nmaps.\nFor each pixel, the first two maps represent the targetâ€™s presence (position map) and distance from the observer (depth map), respectively.\nThe third map estimates the state of the targetâ€™s LEDs, which are employed for different use cases and not used in this work.\nWe compute the targetâ€™s image-plane 2D coordinates\n(\nu\n,\nv\n)\n(u,v)\nfrom the barycenter of the position map activations and the targetâ€™s 3D distance\nd\nd\nfrom the average of the depth map weighed by the position map.\nFinally, we convert the\n(\nu\n,\nv\n)\n(u,v)\nand\nd\nd\ncoordinates into the targetâ€™s pose\n(\nx\n,\ny\n,\nz\n)\n(x,y,z)\nrelative to the observerâ€™s horizontal frame, depicted in Figure\n5\n-B.\nA closed-loop velocity controller then maintains the observer at a fixed distance\nÎ”\n=\n0.8\nm\n\\Delta=$0.8\\text{\\,}\\mathrm{m}$\nfrom the target drone, trying to keep it in the center of the image.\nWe follow the experimental setup of Crupi\net al.\n[\n10\n]\n, in which the target performs the pre-recorded spiral trajectory shown in Figure\n6\n.\nWe evaluate the control performance through the\nmean position error\ne\nð‘¥ð‘¦ð‘§\ne_{\\mathit{xyz}}\n, which measures the Euclidean distance between the observerâ€™s desired and actual position in 3D space w.r.t. the target drone.\nThis metric measures the observerâ€™s ability to track the target and reproduce its trajectory.\nVI-C\nNano-drone racing\nWe consider a drone autonomously exploring an environment with a CNN\n[\n12\n]\nthat predicts, for each input frame, the presence of obstacles within\n2\nm\n2\\text{\\,}\\mathrm{m}\nof the drone.\nThe CNN takes as input a single gray-scale\n162\nÃ—\n162\npx\n162\\times$162\\text{\\,}\\mathrm{px}$\ncamera frame and produces three\ncollision probability\nscalar outputs, which represent respectively the presence of obstacles in the left, center, and right regions forward of the drone.\nThe collision probabilities are then filtered by a low-pass filter and used to bring the drone to a stop, when it comes in an obstacleâ€™s proximity.\nWe focus our evaluation on the systemâ€™s obstacle detection performance.\nThe drone takes off facing an obstacle at a pre-determined\n4\nm\n4\\text{\\,}\\mathrm{m}\ndistance, and the controller drives it at a target forward speed toward the obstacle.\nIn our experiment, we track the droneâ€™s absolute pose with a mocap system, recording the minimum distance from the obstacle over the course of the run.\nWe evaluate the control performance in terms of\nsuccess rate\n, i.e.,Â the percentage of runs in which the drone successfully halts inside the ideal zone depicted in Figure\n5\n-C.\nWhile the drone is flying towards the obstacle, the controller breaks the drone as soon as the CNNâ€™s predicted collision probability rises above a threshold.\nWe experimentally select a separate breaking threshold (0.05 and 0.30) for each of the two throughput configurations (5 and\n30\nHz\n30\\text{\\,}\\mathrm{Hz}\n, respectively) to achieve the maximum success rate.\nDue to the reduced number of collision probability samples at\n5\nHz\n5\\text{\\,}\\mathrm{Hz}\na lower threshold is necessary compared to\n30\nHz\n30\\text{\\,}\\mathrm{Hz}\nto ensure the drone breaks in time (the ideal zone in Figure\n5\n-C).\nToo low thresholds, on the other hand, cause the drone to break prematurely (the early zone).\nTo identify the best thresholds, we fly the drone in open loop toward the obstacle and record the CNN outputs without the predictions impacting the control.\nWe perform 18 experiment runs at max speeds 1.0 through\n3.5\nm\n/\ns\n3.5\\text{\\,}\\mathrm{m}\\mathrm{/}\\mathrm{s}\nand analyze the data offline to determine each runâ€™s outcome at each possible breaking threshold.\nFigure 7:\nMemory breakdown of the deployed PULP-Frontnet CNN for the human pose estimation task.\nVI-D\nNeural network deployment\nAcross all three experiments, we deploy the onboard CNNs on the GAP8 SoC through an automated pipeline.\nWe train a full-precision model in PyTorch with\nfloat32\narithmetic.\nWe then quantize it to\nint8\narithmetic with the open-source QuantLib\n[\n16\n]\nquantization library through a quantization-aware fine-tuning post-training process.\nThis results in a\n4\nÃ—\n4\\times\nreduction in memory footprint, with a marginal loss in performance metrics (i.e.,Â mean horizontal position and angular error for the human pose estimation task, mean position error for drone-to-drone localization, and success rate for nano-drone racing).\nThe DORY\n[\n7\n]\nframework then automatically generates the C code for onboard model inference, i.e.,Â given a single input image in L2 memory, it computes and writes the model outputs back to L2 memory.\nDORY relies on computation kernels from the PULP-NN-mixed\n[\n6\n]\nlibrary for individual model layers (e.g.,Â convolution, max pooling, etc.).\nThese hand-written routines are tuned to exploit GAP8â€™s parallel processing, low-latency L1 memory, and single-instruction multiple-data (SIMD) vector instructions.\nDORYâ€™s generated code orchestrates the computation kernelsâ€™ execution, automatically subdivides each layer in subunits that fit the L1 memory (memory tiling), and schedules DMA memory transfers in parallel to computation.\nOur NanoCockpit framework, on the other hand, implements all support software tasks required to run an autonomous application on a nano-drone, including camera acquisition and wireless streaming tasks.\nFor each application, a minimal user-provided\nmain.c\nfile is used to integrate the automatically generated inference code on top of the NanoCockpit framework, resulting in a full closed-loop application.\nWe analyze the whole-system memory usage resulting from the PULP-Frontnet CNN from the human pose estimation task from Section\nII\n-A.",
    "preview_text": "Autonomous nano-drones, powered by vision-based tiny machine learning (TinyML) models, are a novel technology gaining momentum thanks to their broad applicability and pushing scientific advancement on resource-limited embedded systems. Their small form factor, i.e., a few 10s grams, severely limits their onboard computational resources to sub-\\SI{100}{\\milli\\watt} microcontroller units (MCUs). The Bitcraze Crazyflie nano-drone is the \\textit{de facto} standard, offering a rich set of programmable MCUs for low-level control, multi-core processing, and radio transmission. However, roboticists very often underutilize these onboard precious resources due to the absence of a simple yet efficient software layer capable of time-optimal pipelining of multi-buffer image acquisition, multi-core computation, intra-MCUs data exchange, and Wi-Fi streaming, leading to sub-optimal control performances. Our \\textit{NanoCockpit} framework aims to fill this gap, increasing the throughput and minimizing the system's latency, while simplifying the developer experience through coroutine-based multi-tasking. In-field experiments on three real-world TinyML nanorobotics applications show our framework achieves ideal end-to-end latency, i.e. zero overhead due to serialized tasks, delivering quantifiable improvements in closed-loop control performance ($-$30\\% mean position error, mission success rate increased from 40\\% to 100\\%).\n\nNanoCockpit: Performance-optimized Application Framework for AI-based Autonomous Nanorobotics\nEliaÂ Cereda,Â \nAlessandroÂ Giusti,Â \nandÂ DanieleÂ Palossi\nE. Cereda, A. Giusti, and D. Palossi are with the Dalle Molle Institute for Artificial IntelligenceÂ (IDSIA), USI-SUPSI, 6962 Lugano, Switzerland. Corresponding author:\nelia.cereda@idsia.ch\n.D. Palossi is also with the Integrated Systems Laboratory (IIS), ETH ZÃ¼rich, 8092 ZÃ¼rich, Switzerland.GitHub repository:\nhttps://github.com/idsia-robotics/crazyflie-nanocockpit\nAbstract\nAutonomous nano-drones, powered by vision-bas",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "TinyML",
        "nano-drones",
        "embedded systems",
        "latency optimization",
        "multi-core processing"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºNanoCockpitæ¡†æž¶ï¼Œç”¨äºŽä¼˜åŒ–åŸºäºŽAIçš„è‡ªä¸»çº³ç±³æœºå™¨äººï¼ˆå¦‚Crazyflieçº³ç±³æ— äººæœºï¼‰åœ¨èµ„æºå—é™åµŒå…¥å¼ç³»ç»Ÿä¸Šçš„æ€§èƒ½ï¼Œé€šè¿‡æ—¶é—´æœ€ä¼˜æµæ°´çº¿æŠ€æœ¯å‡å°‘å»¶è¿Ÿå¹¶æå‡æŽ§åˆ¶æ€§èƒ½ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T12:29:38Z",
    "created_at": "2026-01-21T12:09:10.213995",
    "updated_at": "2026-01-21T12:09:10.214003",
    "recommend": 0
}