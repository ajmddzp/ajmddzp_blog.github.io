{
    "id": "2601.23080v1",
    "title": "Robust and Generalized Humanoid Motion Tracking",
    "authors": [
        "Yubiao Ma",
        "Han Yu",
        "Jiayin Xie",
        "Changtai Lv",
        "Qiang Luo",
        "Chi Zhang",
        "Yunpeng Yin",
        "Boyang Xing",
        "Xuemei Ren",
        "Dongdong Zheng"
    ],
    "abstract": "å­¦ä¹ é€šç”¨äººå½¢æœºå™¨äººå…¨èº«æ§åˆ¶å™¨å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®é™…å‚è€ƒåŠ¨ä½œåœ¨è¿ç§»è‡³æœºå™¨äººé¢†åŸŸåå¯èƒ½å‡ºç°å™ªå£°ä¸ä¸ä¸€è‡´æ€§ï¼Œé—­ç¯æ‰§è¡Œè¿‡ç¨‹ä¸­å±€éƒ¨ç¼ºé™·å¯èƒ½è¢«æ”¾å¤§ï¼Œå¯¼è‡´é«˜åŠ¨æ€ã€å¤šæ¥è§¦è¡Œä¸ºä¸­çš„æ¼‚ç§»æˆ–å¤±æ•ˆã€‚æˆ‘ä»¬æå‡ºä¸€ç§åŠ¨æ€æ¡ä»¶æŒ‡ä»¤èšåˆæ¡†æ¶ï¼šé€šè¿‡å› æœæ—¶åºç¼–ç å™¨æ±‡æ€»è¿‘æœŸæœ¬ä½“æ„ŸçŸ¥ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨å¤šå¤´äº¤å‰æ³¨æ„åŠ›æŒ‡ä»¤ç¼–ç å™¨åŸºäºå½“å‰åŠ¨æ€é€‰æ‹©æ€§åœ°èšåˆä¸Šä¸‹æ–‡çª—å£ã€‚è¿›ä¸€æ­¥æ•´åˆäº†åŒ…å«éšæœºä¸ç¨³å®šåˆå§‹åŒ–å’Œæ¸è¿›å¼å‘ä¸Šè¾…åŠ©åŠ›çš„è·Œå€’æ¢å¤è¯¾ç¨‹ï¼Œä»¥æå‡é²æ£’æ€§ä¸æŠ—å¹²æ‰°èƒ½åŠ›ã€‚è¯¥ç­–ç•¥ä»…éœ€çº¦3.5å°æ—¶è¿åŠ¨æ•°æ®ï¼Œæ”¯æŒæ— éœ€è’¸é¦çš„å•é˜¶æ®µç«¯åˆ°ç«¯è®­ç»ƒã€‚æ‰€ææ–¹æ³•åœ¨å¤šæ ·åŒ–å‚è€ƒè¾“å…¥ä¸æŒ‘æˆ˜æ€§è¿åŠ¨åœºæ™¯ä¸‹è¿›è¡Œè¯„ä¼°ï¼Œä¸ä»…å®ç°äº†å¯¹æœªè§åŠ¨ä½œçš„é›¶æ ·æœ¬è¿ç§»ï¼Œè¿˜åœ¨å®ä½“äººå½¢æœºå™¨äººä¸Šå±•ç¤ºäº†é²æ£’çš„ä»¿çœŸåˆ°ç°å®è¿ç§»èƒ½åŠ›ã€‚",
    "url": "https://arxiv.org/abs/2601.23080v1",
    "html_url": "https://arxiv.org/html/2601.23080v1",
    "html_content": "Robust and Generalized Humanoid Motion Tracking\nYubiaoÂ Ma\nâ€ \n1\n{}^{1}\\dagger\n,\nHanÂ Yu\nâ€ \n2\n{}^{2}\\dagger\n,\nJiayinÂ Xie\n2\n,\nChangtaiÂ Lv\n2\n,\nQiangÂ Luo\n2\n,\nChiÂ Zhang\n2\n,\nYunpengÂ Yin\n2\n,\nBoyangÂ Xing\n2\n,\nXuemeiÂ Ren\n1\n,\nandÂ DongdongÂ Zheng\nâˆ—\n1\n,\n2\n{}^{1,2}\\ast\n1\nBeijing Institute of Technology, Beijing, China.\n2\nHumanoid Robotics (Shanghai) Co., Ltd., Shanghai 201203, China.\nâ€ \n\\dagger\nThese authors contributed equally.\nâˆ—\n\\ast\nCorresponding author: Dongdong Zheng (\nddzheng@bit.edu.cn\n).\nAbstract\nLearning a general humanoid whole-body controller is challenging because practical reference motions can exhibit noise and inconsistencies\nafter being transferred to the robot domain, and local defects may be amplified by closed-loop execution,\ncausing drift or failure in highly dynamic and contact-rich behaviors.\nWe propose a dynamics-conditioned command aggregation framework that uses a causal temporal encoder to summarize recent proprioception\nand a multi-head cross-attention command encoder to selectively aggregate a context window based on the current dynamics.\nWe further integrate a fall recovery curriculum with random unstable initialization\nand an annealed upward assistance force to improve robustness and disturbance rejection.\nThe resulting policy requires only about\n3.5\n3.5\nhours of motion data and supports single-stage end-to-end training without distillation.\nThe proposed method is evaluated under diverse reference inputs and challenging motion regimes,\ndemonstrating zero-shot transfer to unseen motions as well as robust sim-to-real transfer on a physical humanoid robot.\nProject page:\nhttps://zeonsunlightyu.github.io/RGMT.github.io/\nI\nIntroduction\nHumanoid robots are compelling largely because of their generality.\nTheir morphology is naturally compatible with human environments, allowing them to operate within existing infrastructure and manipulate tools,\nworkspaces, and interfaces designed for people.\nMoreover, their high-dimensional actuation and multi-contact capabilities support a broad spectrum of behaviors spanning locomotion, manipulation, and interaction\n[\n19\n,\n18\n,\n25\n,\n14\n,\n32\n,\n11\n,\n9\n,\n4\n,\n2\n,\n30\n]\n.\nTo translate this vision into reliable operation,\nit is necessary to develop a general whole-body controller that can coordinate the full body under changing contacts and task demands,\nwhile maintaining stable behavior over long horizons.\nHowever, this objective remains difficult to achieve in existing whole-body control research.\nHigh-fidelity imitation in practice still often relies on training for a single motion or a small set of motions\n[\n29\n,\n26\n,\n7\n,\n22\n]\n,\nwhich tightly couples policy capability to a specific reference distribution and limits unified modeling and generalization across skills.\nTo learn diverse humanoid motions within a single policy,\nrecent work has moved toward unified whole-body control that combines large-scale motion tracking objectives with broader motion coverage and more practical data acquisition.\nSeveral systems\n[\n8\n,\n13\n,\n28\n,\n12\n]\nfocus on scalable human to humanoid data collection and teleoperation,\nproviding richer and more diverse demonstrations for training.\nIn parallel, a number of motion tracking based controllers\n[\n3\n,\n31\n,\n27\n,\n5\n]\naim to directly train a universal tracker that can follow many motions under different disturbances.\nThese approaches demonstrate promising progress toward general whole-body controllers on humanoid robots,\nbut their tracking accuracy and closed loop stability remain suboptimal,\nespecially during highly dynamic maneuvers and rapid contact transitions.\nSONIC\n[\n15\n]\nfurther pushes motion coverage and naturalness,\nbut it relies on very large-scale data and training resources,\nusing more than 700 hours of motion data and substantial compute,\nwhich raises the barrier for iterative research and deployment.\nMoreover, long-horizon operation in the real world requires not only stable tracking but also recovery after disturbances or falls, yet fall recovery is often not integrated into the main control policy\n[\n11\n,\n9\n]\n, limiting robustness and safety.\nTherefore, we need a general whole-body control framework that can learn effectively under limited data, maintain stable closed-loop tracking under imperfect references, and jointly integrate robustness improvements for highly dynamic and multi-contact scenarios with fall recovery within a single training and policy pipeline, enabling safe and continuous real-world task execution.\nThis paper presents a learning framework for general humanoid whole-body control.\nThe key idea is to condition the policy on the current dynamics, enabling it to interpret and aggregate contextual commands selectively rather than treating all reference signals as equally reliable supervision.\nWe obtain a compact dynamics representation from recent proprioceptive history using a causal temporal encoder and use it to guide command aggregation via multi-head attention\n[\n24\n]\n.\nThis design allows the policy to adaptively select and adjust reference segments under feasibility constraints imposed by the current dynamics, reducing the impact of noise and artifacts, particularly for highly dynamic motions and frequent contact transitions.\nTo further support safe and continuous real-world operation, we incorporate fall recovery into the same training framework\n[\n11\n,\n9\n]\n.\nThis integration broadens the experienced state distribution, improves robustness to disturbances, and strengthens tracking performance for contact-rich motion segments.\nOur contributions are summarized as follows:\nâ€¢\nWe propose a dynamics-conditioned command aggregation framework that combines causal temporal dynamics encoding with multi-head cross-attention.\nThis design enables selective use of contextual commands under imperfect references and improves tracking accuracy and closed-loop stability in highly dynamic and contact-rich scenarios.\nThe resulting general policy is trained end-to-end using a compact dataset of about\n3.5\n3.5\nhours, without distillation or multi-stage training.\nâ€¢\nWe integrate fall recovery into a unified training framework. With randomized unstable initialization and an annealed external assistance force, a single policy learns stable control and self-recovery over a broader state distribution and contact conditions, leading to significantly improved robustness and disturbance rejection.\nâ€¢\nWe demonstrate strong generalization across diverse reference sources, including mocap, video-derived motions, and real-time full-body teleoperation.\nThe learned policy transfers zero-shot to unseen motions and deploys robustly on the Unitree G1, enabling stable long-horizon tracking with integrated recovery and downstream applications such as joystick-driven locomotion.\nFigure 2\n:\nOverview of the proposed whole-body control pipeline.\nA history encoder extracts a dynamics embedding from recent proprioception,\nwhich conditions a command encoder to aggregate the contextual command window.\nThe resulting representation is fused with the current observation and fed to an actor-critic policy trained with PPO,\nand the learned actor is deployed for real-world whole-body motion tracking and teleoperation.\nII\nMETHOD\nII-A\nHumanoid Motion Dataset\nOur motion corpus is constructed from LAFAN1\n[\n6\n]\nand a selected subset of AMASS\n[\n16\n]\n, and all sequences are retargeted to our humanoid using General Motion Retargeting\n[\n1\n]\n.\nIn practice, large-scale mocap sources\n[\n16\n]\nand their retargeted counterparts often exhibit substantial redundancy and may include segments with low motion quality and inconsistent contacts.\nWe therefore perform quality control to remove infeasible motions and low-quality sequences and to reduce redundancy, resulting in a relatively compact dataset of approximately\n3.5\n3.5\nhours.\nThis quality-driven construction is crucial for general humanoid motion tracking.\nEven when the raw corpus is large, effective supervision can be limited by duplicated motions and low-quality clips, which can distract optimization and reduce training efficiency.\nIn contrast, a smaller but diverse and higher-quality reference set provides cleaner and more informative supervision, improving generalization and closed-loop tracking accuracy.\nImportantly, enabled by our dynamics-conditioned command aggregation, this compact dataset is sufficient to train a strong general whole-body policy that is robust to noisy references and generalizes effectively to unseen motions.\nII-B\nMotion Tracking Formulation\nII-B\n1\nObservation Space\nOur policy receives an observation\nğ¨\nt\n\\mathbf{o}_{t}\nthat consists of a proprioceptive component and a command component.\nThe proprioceptive observation is\nğ¨\nt\n=\n[\nğ \nt\nproj\n,\nğ\nt\n,\nğª\nt\nâˆ’\nğª\n0\n,\nğª\nË™\nt\n,\nğš\nt\nâˆ’\n1\n]\n,\n\\mathbf{o}_{t}=\\Big[\\mathbf{g}^{\\mathrm{proj}}_{t},\\;\\boldsymbol{\\omega}_{t},\\;\\mathbf{q}_{t}-\\mathbf{q}_{0},\\;\\dot{\\mathbf{q}}_{t},\\;\\mathbf{a}_{t-1}\\Big],\n(1)\nwhere\nğ \nt\nproj\nâˆˆ\nâ„\n3\n\\mathbf{g}^{\\mathrm{proj}}_{t}\\in\\mathbb{R}^{3}\ndenotes the gravity direction projected into the body frame,\nğ\nt\nâˆˆ\nâ„\n3\n\\boldsymbol{\\omega}_{t}\\in\\mathbb{R}^{3}\nis the base angular velocity,\nğª\nt\nâˆˆ\nâ„\n29\n\\mathbf{q}_{t}\\in\\mathbb{R}^{29}\nand\nğª\nË™\nt\nâˆˆ\nâ„\n29\n\\dot{\\mathbf{q}}_{t}\\in\\mathbb{R}^{29}\nare joint positions and velocities,\nğª\n0\n\\mathbf{q}_{0}\nis the default joint configuration, and\nğš\nt\nâˆ’\n1\nâˆˆ\nâ„\n29\n\\mathbf{a}_{t-1}\\in\\mathbb{R}^{29}\nis the previous action.\nThe command observation provides per step reference targets extracted from the reference motion,\nğ \nt\n=\n[\nğ¯\nt\nref\n,\nğ\nt\nref\n,\nğ \nt\nref\n,\nğª\nt\nref\n]\n,\n\\mathbf{g}_{t}=\\Big[\\mathbf{v}^{\\mathrm{ref}}_{t},\\;\\boldsymbol{\\omega}^{\\mathrm{ref}}_{t},\\;\\mathbf{g}^{\\mathrm{ref}}_{t},\\;\\mathbf{q}^{\\mathrm{ref}}_{t}\\Big],\n(2)\nwhere\nğ¯\nt\nref\nâˆˆ\nâ„\n3\n\\mathbf{v}^{\\mathrm{ref}}_{t}\\in\\mathbb{R}^{3}\nand\nğ\nt\nref\nâˆˆ\nâ„\n3\n\\boldsymbol{\\omega}^{\\mathrm{ref}}_{t}\\in\\mathbb{R}^{3}\nare reference base linear and angular velocities expressed in the body frame,\nğ \nt\nref\nâˆˆ\nâ„\n3\n\\mathbf{g}^{\\mathrm{ref}}_{t}\\in\\mathbb{R}^{3}\nis the reference gravity direction in the body frame, and\nğª\nt\nref\nâˆˆ\nâ„\n29\n\\mathbf{q}^{\\mathrm{ref}}_{t}\\in\\mathbb{R}^{29}\nis the reference joint position at time\nt\nt\n.\nFor value learning, we adopt an asymmetric actor-critic training scheme.\nThe critic additionally takes a privileged observation that facilitates more accurate value estimation,\nğ¨\nt\npriv\n=\n[\nh\nt\nref\n,\nğ±\nt\nlink\n,\nğ¯\nt\n]\n,\n\\mathbf{o}^{\\mathrm{priv}}_{t}=\\Big[h^{\\mathrm{ref}}_{t},\\;\\mathbf{x}^{\\mathrm{link}}_{t},\\;\\mathbf{v}_{t}\\Big],\n(3)\nwhere\nh\nt\nref\nh^{\\mathrm{ref}}_{t}\ndenotes the reference base height,\nğ±\nt\nlink\n\\mathbf{x}^{\\mathrm{link}}_{t}\ndenotes body link poses, and\nğ¯\nt\n\\mathbf{v}_{t}\nis the base linear velocity.\nAccordingly, the critic input is\nğ¬\nt\n=\n[\nğ¨\nt\n,\nğ \nt\n,\nğ¨\nt\npriv\n]\n.\n\\mathbf{s}_{t}=\\Big[\\mathbf{o}_{t},\\;\\mathbf{g}_{t},\\;\\mathbf{o}^{\\mathrm{priv}}_{t}\\Big].\n(4)\nAll components of\nğ¬\nt\n\\mathbf{s}_{t}\nused by the critic are noise-free,\nwhile the actor receives noisy observations\nğ¨\nt\n\\mathbf{o}_{t}\n.\nII-B\n2\nAction Space and Low-level Control\nThe policy outputs a residual joint position command\nğš\nt\nâˆˆ\nâ„\n29\n\\mathbf{a}_{t}\\in\\mathbb{R}^{29}\n.\nWe interpret\nğš\nt\n\\mathbf{a}_{t}\nas a corrective offset to the reference joint configuration and form the PD setpoint as\nğª\nt\ntar\n=\nğª\nt\nref\n+\nğš\nt\n.\n\\mathbf{q}^{\\mathrm{tar}}_{t}=\\mathbf{q}^{\\mathrm{ref}}_{t}+\\mathbf{a}_{t}.\n(5)\nJoint torques are computed by a joint-level PD controller,\nğ‰\nt\n=\nğŠ\np\nâ€‹\n(\nğª\nt\ntar\nâˆ’\nğª\nt\n)\nâˆ’\nğŠ\nd\nâ€‹\nğª\nË™\nt\n,\n\\boldsymbol{\\tau}_{t}=\\mathbf{K}_{p}\\Big(\\mathbf{q}^{\\mathrm{tar}}_{t}-\\mathbf{q}_{t}\\Big)-\\mathbf{K}_{d}\\,\\dot{\\mathbf{q}}_{t},\n(6)\nwhere\nğŠ\np\n\\mathbf{K}_{p}\nand\nğŠ\nd\n\\mathbf{K}_{d}\nare diagonal gain matrices.\nThis residual formulation improves tracking accuracy by anchoring the PD setpoint to the reference motion while allowing corrective adjustments.\nIt also makes exploration more efficient, since the policy searches around the reference pose rather than over the full joint configuration space.\nAs a result, training becomes more sample efficient and converges faster in practice.\nII-B\n3\nReward Function\nFollowing\n[\n22\n]\n,\nwe adopt a dense reward that combines reference tracking with safety and smoothness regularization.\nWe define keypoints as a fixed set of links used in\n[\n22\n]\n,\nand formulate the tracking reward on these keypoints.\nSpecifically, the tracking part measures keypoint alignment, relative pose consistency,\nand keypoint velocity consistency with exponential kernels.\nThe regularization part penalizes rapid action changes, joint limit violations,\nand contacts on non-target body parts, which stabilizes learning and reduces physically implausible behaviors.\nII-C\nPolicy Learning Framework\nMulti-head attention (MHA)\n[\n24\n]\nprovides an effective mechanism for query-conditioned information fusion.\nIt computes content-based similarities between a query and a set of candidate features to produce adaptive aggregation weights,\nyielding a compact representation that emphasizes relevant elements while attenuating irrelevant or noisy ones.\nThe multi-head formulation performs this matching in multiple learned subspaces,\nallowing the model to capture diverse relevance cues in parallel and increasing expressiveness beyond a single attention map.\nThis capability is well suited to whole-body control, where command inputs are high dimensional and heterogeneous and may contain unreliable segments.\nIn practice, command sequences obtained from diverse sources often contain artifacts such as body penetration,\ninconsistent contacts, and high frequency noise. If the policy treats the entire command sequence as equally trustworthy supervision,\nabnormal segments can enter the representation with the same weight and be amplified in the action output,\nwhich can ultimately harm closed loop stability. Motivated by this observation,\nwe introduce dynamics-conditioned cross-attention in the policy architecture, as illustrated in Fig.\n2\n.\nA dynamics representation is first extracted from recent proprioceptive history using causal temporal encoding and used as the query signal,\nand the contextual command window is then aggregated with adaptive attention weights.\nThis design enables the policy to interpret and filter command information under physical feasibility constraints,\nreducing the influence of inconsistent reference signals on the control representation.\nII-C\n1\nHistory Encoder\nWe encode the most recent\n10\n10\n-step proprioceptive observations into a compact dynamics embedding.\nThe input sequence is\nğ¨\nt\nâˆ’\nK\n:\nt\n=\n[\nğ¨\nt\nâˆ’\nK\n,\nâ€¦\n,\nğ¨\nt\n]\n,\n\\mathbf{o}_{t-K:t}=\\big[\\mathbf{o}_{t-K},\\ldots,\\mathbf{o}_{t}\\big],\n(7)\nwhere each\nğ¨\nt\nâˆˆ\nâ„\n93\n\\mathbf{o}_{t}\\in\\mathbb{R}^{93}\nand the sequence length is\nK\n+\n1\nK+1\n.\nEach observation is mapped to a token in an embedding space of dimension\nn\nembd\n=\n128\nn_{\\mathrm{embd}}=128\nby a two-layer multi-layer perceptron (MLP),\nğ„\nt\nâˆ’\nK\n:\nt\n=\nMLP\nâ€‹\n(\nğ¨\nt\nâˆ’\nK\n:\nt\n)\nâˆˆ\nâ„\n(\nK\n+\n1\n)\nÃ—\nn\nembd\n,\n\\mathbf{E}_{t-K:t}=\\mathrm{MLP}\\!\\left(\\mathbf{o}_{t-K:t}\\right)\\in\\mathbb{R}^{(K+1)\\times n_{\\mathrm{embd}}},\n(8)\nand a sinusoidal positional encoding is added to preserve temporal order,\nğ„\n~\nt\nâˆ’\nK\n:\nt\n=\nğ„\nt\nâˆ’\nK\n:\nt\n+\nğ\n,\nğ\nâˆˆ\nâ„\n(\nK\n+\n1\n)\nÃ—\nn\nembd\n.\n\\tilde{\\mathbf{E}}_{t-K:t}=\\mathbf{E}_{t-K:t}+\\mathbf{P},\\qquad\\mathbf{P}\\in\\mathbb{R}^{(K+1)\\times n_{\\mathrm{embd}}}.\n(9)\nThe resulting token sequence is processed by a lightweight causal Transformer with multi-head self-attention.\nCausality is enforced by a causal mask so that the token at time\nÏ„\n\\tau\ncan attend only to tokens from times\nâ‰¤\nÏ„\n\\leq\\tau\nwithin the window.\nLetting\nğ‡\n(\n0\n)\n=\nğ„\n~\nt\nâˆ’\nK\n:\nt\n\\mathbf{H}^{(0)}=\\tilde{\\mathbf{E}}_{t-K:t}\n, the causal self-attention block yields\nğ‡\n(\n1\n)\n\\displaystyle\\mathbf{H}^{(1)}\n=\nğ‡\n(\n0\n)\n+\nMHA\nâ€‹\n(\nLN\nâ€‹\n(\nğ‡\n(\n0\n)\n)\n)\n,\n\\displaystyle=\\mathbf{H}^{(0)}+\\mathrm{MHA}\\!\\left(\\mathrm{LN}\\!\\left(\\mathbf{H}^{(0)}\\right)\\right),\n(10)\nğ‡\n(\n2\n)\n\\displaystyle\\mathbf{H}^{(2)}\n=\nğ‡\n(\n1\n)\n+\nMLP\nâ€‹\n(\nLN\nâ€‹\n(\nğ‡\n(\n1\n)\n)\n)\n,\n\\displaystyle=\\mathbf{H}^{(1)}+\\mathrm{MLP}\\!\\left(\\mathrm{LN}\\!\\left(\\mathbf{H}^{(1)}\\right)\\right),\nğ‡\nÂ¯\n\\displaystyle\\bar{\\mathbf{H}}\n=\nLN\nâ€‹\n(\nğ‡\n(\n2\n)\n)\n,\n\\displaystyle=\\mathrm{LN}\\!\\left(\\mathbf{H}^{(2)}\\right),\nwhere\nLN\nâ€‹\n(\nâ‹…\n)\n\\mathrm{LN}(\\cdot)\ndenotes layer normalization.\nFinally, we aggregate token features over time via element-wise max pooling to obtain the dynamics embedding\nğ¡\nt\nâˆˆ\nâ„\nn\nembd\n\\mathbf{h}_{t}\\in\\mathbb{R}^{n_{\\mathrm{embd}}}\n:\nğ¡\nt\nâ€‹\n[\nj\n]\n=\nmax\nÏ„\nâˆˆ\n{\nt\nâˆ’\nK\n,\nâ€¦\n,\nt\n}\nâ¡\nğ‡\nÂ¯\nÏ„\nâ€‹\n[\nj\n]\n,\nj\n=\n1\n,\nâ€¦\n,\nn\nembd\n.\n\\mathbf{h}_{t}[j]=\\max_{\\tau\\in\\{t-K,\\ldots,t\\}}\\bar{\\mathbf{H}}_{\\tau}[j],\\qquad j=1,\\ldots,n_{\\mathrm{embd}}.\n(11)\nThis embedding extracts the robot dynamics from recent proprioceptive history and is used as the query signal for the subsequent dynamics-conditioned command encoder.\nII-C\n2\nCommand Encoder\nThe command encoder compresses a contextual command window into a compact latent representation while conditioning the aggregation on the current dynamics.\nIts inputs are the dynamics embedding\nğ¡\nt\nâˆˆ\nâ„\nn\nembd\n\\mathbf{h}_{t}\\in\\mathbb{R}^{n_{\\mathrm{embd}}}\nand the command sequence\nğ \nt\nâˆ’\nL\n:\nt\n+\nL\n=\n[\nğ \nt\nâˆ’\nL\n,\nâ€¦\n,\nğ \nt\n+\nL\n]\n,\n\\mathbf{g}_{t-L:t+L}=\\big[\\mathbf{g}_{t-L},\\ldots,\\mathbf{g}_{t+L}\\big],\n(12)\nwhere each\nğ \nt\nâˆˆ\nâ„\n38\n\\mathbf{g}_{t}\\in\\mathbb{R}^{38}\nand the window length is\n2\nâ€‹\nL\n+\n1\n2L+1\n.\nThe dynamics embedding is projected to the Transformer dimension through a two-layer multi-layer perceptron, yielding the query vector\nğª\nt\n=\nMLP\ndyn\nâ€‹\n(\nğ¡\nt\n)\nâˆˆ\nâ„\nn\nembd\n.\n\\mathbf{q}_{t}=\\mathrm{MLP}_{\\mathrm{dyn}}\\!\\left(\\mathbf{h}_{t}\\right)\\in\\mathbb{R}^{n_{\\mathrm{embd}}}.\n(13)\nIn parallel, the command window is mapped to a token sequence in the same feature space using another two-layer multi-layer perceptron, and a sinusoidal positional encoding is added to preserve temporal order\nğ™\n~\n=\nMLP\ncmd\nâ€‹\n(\nğ \nt\nâˆ’\nL\n:\nt\n+\nL\n)\n+\nğ\ncmd\n,\nğ™\n~\nâˆˆ\nâ„\n(\n2\nâ€‹\nL\n+\n1\n)\nÃ—\nn\nembd\n.\n\\tilde{\\mathbf{Z}}=\\mathrm{MLP}_{\\mathrm{cmd}}\\!\\left(\\mathbf{g}_{t-L:t+L}\\right)+\\mathbf{P}^{\\mathrm{cmd}},\\qquad\\tilde{\\mathbf{Z}}\\in\\mathbb{R}^{(2L+1)\\times n_{\\mathrm{embd}}}.\n(14)\nThe encoder then applies a single dynamics-conditioned cross-attention block to aggregate the command tokens into a compact latent representation\nğ¬\n(\n1\n)\n\\displaystyle\\mathbf{s}^{(1)}\n=\nğª\nt\n+\nMHA\nâ€‹\n(\nLN\nâ€‹\n(\nğª\nt\n)\n,\nğ™\n~\n)\n,\n\\displaystyle=\\mathbf{q}_{t}+\\mathrm{MHA}\\!\\left(\\mathrm{LN}\\!\\left(\\mathbf{q}_{t}\\right),\\,\\tilde{\\mathbf{Z}}\\right),\n(15)\nğ¬\n(\n2\n)\n\\displaystyle\\mathbf{s}^{(2)}\n=\nğ¬\n(\n1\n)\n+\nMLP\nâ€‹\n(\nLN\nâ€‹\n(\nğ¬\n(\n1\n)\n)\n)\n,\n\\displaystyle=\\mathbf{s}^{(1)}+\\mathrm{MLP}\\!\\left(\\mathrm{LN}\\!\\left(\\mathbf{s}^{(1)}\\right)\\right),\nğ®\nt\n\\displaystyle\\mathbf{u}_{t}\n=\nLN\nâ€‹\n(\nğ¬\n(\n2\n)\n)\nâˆˆ\nâ„\nn\nembd\n.\n\\displaystyle=\\mathrm{LN}\\!\\left(\\mathbf{s}^{(2)}\\right)\\in\\mathbb{R}^{n_{\\mathrm{embd}}}.\nThe resulting vector\nğ®\nt\n\\mathbf{u}_{t}\nserves as a compact, dynamics-conditioned command embedding at time\nt\nt\n.\nBecause the cross-attention weights are conditioned on\nğ¡\nt\n\\mathbf{h}_{t}\nthrough the query\nğª\nt\n\\mathbf{q}_{t}\n, the encoder adaptively emphasizes command elements that are more consistent with the current dynamics and down-weights unreliable segments, thereby reducing the influence of abnormal command artifacts on the control representation.\nTABLE I\n:\nPerformance under different motion data sources.\nWe report mean\nÂ±\n\\pm\nstandard deviation.\nHigher is better for success rate, and lower is better for\nE\nMPJPE\nE_{\\mathrm{MPJPE}}\n.\nMethod\nMoCap Data\nVideo-derived Motion\nGround-interaction Motion\nSucc.\nâ†‘\n\\uparrow\nğ„\nMPJPE\n\\mathbf{E_{\\mathrm{MPJPE}}}\nâ†“\n\\downarrow\nSucc.\nâ†‘\n\\uparrow\nğ„\nMPJPE\n\\mathbf{E_{\\mathrm{MPJPE}}}\nâ†“\n\\downarrow\nSucc.\nâ†‘\n\\uparrow\nğ„\nMPJPE\n\\mathbf{E_{\\mathrm{MPJPE}}}\nâ†“\n\\downarrow\n(a) Baseline\nGMT\n84.6%\n65.15\nÂ±\n(\n112\n)\n65.15\\,\\pm\\,(112)\n72.4%\n96.47\nÂ±\n(\n198\n)\n96.47\\,\\pm\\,(198)\n48.9%\n146.95\nÂ±\n(\n512\n)\n146.95\\,\\pm\\,(512)\nAny2Track\n89.2%\n56.96\nÂ±\n(\n91\n)\n56.96\\,\\pm\\,(91)\n54.3%\n112.16\nÂ±\n(\n396\n)\n112.16\\,\\pm\\,(396)\n41.2%\n209.57\nÂ±\n(\n422\n)\n209.57\\,\\pm\\,(422)\nOurs\n98.3%\n41.12\nÂ±\n(\n12\n)\n41.12\\,\\pm\\,(12)\n94.6%\n46.56\nÂ±\n(\n28\n)\n46.56\\,\\pm\\,(28)\n90.1%\n54.92\nÂ±\n(\n93\n)\n54.92\\,\\pm\\,(93)\n(b) Ablations on Policy Architecture\nOurs SelfAttn CmdEnc\n89.8%\n51.96\nÂ±\n(\n72\n)\n51.96\\,\\pm\\,(72)\n76.7%\n67.19\nÂ±\n(\n131\n)\n67.19\\,\\pm\\,(131)\n73.2%\n92.65\nÂ±\n(\n231\n)\n92.65\\,\\pm\\,(231)\nOurs CNN HistEnc\n94.3%\n48.61\nÂ±\n(\n32\n)\n48.61\\,\\pm\\,(32)\n91.9%\n53.13\nÂ±\n(\n78\n)\n53.13\\,\\pm\\,(78)\n81.5%\n68.92\nÂ±\n(\n184\n)\n68.92\\,\\pm\\,(184)\nOurs\n98.3%\n41.12\nÂ±\n(\n12\n)\n41.12\\,\\pm\\,(12)\n94.6%\n46.56\nÂ±\n(\n28\n)\n46.56\\,\\pm\\,(28)\n90.1%\n54.92\nÂ±\n(\n93\n)\n54.92\\,\\pm\\,(93)\n(c) Ablations on Fall Recovery\nOurs w/o Fall Recovery\n98.4%\n40.98\nÂ±\n(\n9\n)\n40.98\\,\\pm\\,(9)\n94.9%\n46.16\nÂ±\n(\n31\n)\n46.16\\,\\pm\\,(31)\n70.5%\n96.75\nÂ±\n(\n277\n)\n96.75\\,\\pm\\,(277)\nOurs\n98.3%\n41.12\nÂ±\n(\n12\n)\n41.12\\,\\pm\\,(12)\n94.6%\n46.56\nÂ±\n(\n28\n)\n46.56\\,\\pm\\,(28)\n90.1%\n54.92\nÂ±\n(\n93\n)\n54.92\\,\\pm\\,(93)\nII-D\nFall Recovery Integration\nAutomatic fall recovery is a key prerequisite for reliable humanoid deployment.\nWithout an effective self-recovery mechanism,\nthe system not only faces significant safety risks but also requires frequent human intervention,\nbreaking task continuity. This issue is particularly pronounced for whole-body motion tracking,\nwhere rapid dynamics and frequent contact transitions can amplify closed-loop errors and trigger instability.\nTherefore, we integrate a simple yet reliable automatic fall recovery mechanism into our whole-body control framework.\nII-D\n1\nRandomized Recovery Initialization\nWe designate a subset of the parallel environments as recovery environments with probability\n0.15\n0.15\nand reset the robot in these environments to randomized poses,\nexposing the policy to a broad range of unstable configurations and contact initial conditions.\nThis process also enriches contact experience during training,\nsince repeated falls and stand-ups naturally induce diverse ground-contact patterns,\nwhich improves tracking accuracy and generalization for motions with frequent or complex contact transitions.\nFor these recovery environments, we apply an upward pulling force with magnitude uniformly sampled from\n[\n0\n,\n200\n]\n[0,200]\nto assist exploration\nat early training stages by increasing the frequency of recoverable states.\nThe assistance is linearly annealed over training iterations and reduced to a negligible level,\nensuring that the final policy performs fall recovery using only its own control.\nII-D\n2\nTermination Conditions\nWe employ state-based episode termination and environment resets to maintain stable training and high-quality rollouts.\nFor all environments, an episode is reset either when it reaches a predefined time limit or when an instability event is detected.\nInstability is identified by three conditions: excessive base orientation deviation, insufficient base height, and abnormally low height of key body links.\nTo learn automatic fall recovery, we introduce an additional termination strategy for the recovery environments.\nWithin a predetermined recovery window of\n3\n3\nseconds, recovery environments are not terminated early by the instability criteria,\nallowing the policy to complete stand-up and re-stabilization within the same episode.\nIf the robot fails to recover within this window, the episode is terminated and the environment is reset,\navoiding prolonged rollouts in unrecoverable states and maintaining training efficiency.\nII-E\nTraining Setup\nWe train our policy in the Isaac Gym simulator\n[\n17\n]\n.\nAll training runs are conducted on a single NVIDIA RTXÂ 4090 GPU with 5,680 parallel environments.\nWe train on approximately 3.5 hours of motion data curated from subsets of LAFAN1\n[\n6\n]\nand AMASS\n[\n16\n]\n,\nand observe strong generalization to previously unseen motions.\nThe proprioceptive history length is set to\nK\n=\n9\nK=9\n, and the command window half-length is set to\nL\n=\n10\nL=10\n.\nIII\nExperiments\nIn this section, we evaluate our approach on the 29 degrees-of-freedom (DOF) Unitree G1\n[\n23\n]\nhumanoid robot\nand demonstrate strong generalization and robustness in both simulation and the real world.\nWe conduct quantitative comparisons against representative baselines and targeted ablation studies to validate improved whole-body motion tracking performance\nand robustness to noise in command inputs.\nFinally, we deploy the learned policy on the physical robot to showcase reliable tracking and generalization across diverse motions,\nand demonstrate its applicability to downstream tasks such as real-time teleoperation and online motion generation.\nTABLE II\n:\nNoise specifications for command.\nCommand\nNoise Specification\nBase linear velocity jitter\nÎ”\nâ€‹\nğ¯\nt\nref\nâˆ¼\nğ’°\nâ€‹\n(\n[\nâˆ’\n0.5\n,\n0.5\n]\n3\n)\n\\Delta\\mathbf{v}_{t}^{\\mathrm{ref}}\\sim\\mathcal{U}([-0.5,0.5]^{3})\n(z:\nÂ±\n\\pm\n0.2)\nBase angular velocity jitter\nÎ”\nâ€‹\nÏ‰\nt\nref\nâˆ¼\nğ’°\nâ€‹\n(\n[\nâˆ’\n0.52\n,\n0.52\n]\n3\n)\n\\Delta\\omega^{\\mathrm{ref}}_{t}\\sim\\mathcal{U}([-0.52,0.52]^{3})\nBase gravity direction jitter\nÎ”\nâ€‹\nğ \nt\nref\nâˆ¼\nğ’°\nâ€‹\n(\n[\nâˆ’\n0.05\n,\n0.05\n]\n3\n)\n\\Delta\\mathbf{g}_{t}^{\\mathrm{ref}}\\sim\\mathcal{U}([-0.05,0.05]^{3})\nBase joint position jitter\nÎ”\nâ€‹\nğª\nt\nref\nâˆ¼\nğ’°\nâ€‹\n(\n[\nâˆ’\n0.1\n,\n0.1\n]\n)\n\\Delta\\mathbf{q}_{t}^{\\mathrm{ref}}\\sim\\mathcal{U}([-0.1,0.1])\nFigure 3\n:\nRobustness under reference command noise.\nIII-A\nPerformance Evaluation\nIII-A\n1\nComparison with Baselines\nWe validate that the proposed approach learns a general motion tracker\nthat achieves stable and accurate tracking over a wide range of previously unseen motions.\nWe compare our method against two representative recent trackers, Any2Track\n[\n31\n]\nand GMT\n[\n3\n]\n,\nboth of which provide officially released models.\nTo ensure consistency and fairness, we directly use these models and evaluate all methods on the same datasets.\nAll methods are tested in MuJoCo\n[\n21\n]\n, which is supported by the above baseline implementations and provides a unified evaluation platform.\nA comprehensive set of metrics is adopted to capture both pose accuracy and physical feasibility.\nSpecifically, the success rate (Succ) is defined as the fraction of rollouts without falls,\nwhere a fall is identified when the root height deviates from the reference by more than\n0.2\n0.2\nm.\nIn addition, we report the mean per-joint position error (MPJPE)\nE\nMPJPE\nE_{\\mathrm{MPJPE}}\n(in mm),\nwhich measures the 3D position error of joints and quantifies joint-level tracking accuracy.\nTable\nI\n(a) compares baseline performance across three motion sources:\nMoCap data curated from subsets of LAFAN1 and AMASS, video-derived motions, and ground-interaction motions.\nThe MoCap subset comes from the same MoCap sources used to train all methods and thus measures in-distribution\ntracking performance. The video-derived motions are estimated from publicly available videos\nand cover common behaviors such as walking, dance, and martial arts.\nSince this subset is not used for training, it directly evaluates cross-source generalization.\nAcross all three subsets, our method achieves the highest success rate and the lowest\nE\nMPJPE\nE_{\\mathrm{MPJPE}}\n.\nThe improvement is most pronounced on the video-derived subset,\nwhere we substantially outperform GMT and Any2Track,\nindicating stronger robustness to distribution shifts in the reference motions.\nIII-A\n2\nAblations on Policy Architecture\nTable\nI\n(b) studies the effect of key design choices in our tracker.\nReplacing the causal history encoder with a CNN-based variant (\nOurs CNN HistEnc\n) consistently degrades performance across all three data sources,\nindicating that the causal history encoder contributes to stable and accurate tracking.\nReplacing the cross-attention command encoder with a self-attention variant (\nOurs SelfAttn CmdEnc\n) leads to a much larger performance drop,\nparticularly on the video-derived and ground-interaction subsets. This highlights the key role of dynamics-conditioned cross-attention in robust command aggregation under distribution shifts and command artifacts.\nIII-A\n3\nAblations on Fall Recovery\nTable\nI\n(c) studies the effect of integrating fall recovery during training.\nIncorporating fall recovery does not noticeably compromise tracking accuracy or generalization to unseen motions, as reflected by comparable performance on the MoCap and video-derived subsets.\nIn contrast, removing fall recovery training (\nOurs w/o Fall Recovery\n) substantially degrades performance on the ground-interaction subset, which includes contact-intensive behaviors such as crawling, kneeling, sitting, and breakdance-style motions.\nIn this setting, the success rate (Succ.) drops markedly and\nE\nMPJPE\nE_{\\mathrm{MPJPE}}\nincreases.\nThese results suggest that fall recovery not only provides self-recovery capability for safer long-horizon execution but also broadens contact experience during training, thereby improving robustness under complex ground interactions.\nIII-B\nRobustness Evaluation\nRobustness to command noise is essential for real-time teleoperation and generalization to unseen motions.\nIn practical applications, commands from human operators, motion estimation pipelines, or high-level planners are often noisy and subject to uncertainty.\nTo evaluate robustness under such conditions, we evaluate on the Charleston dance motion,\nwhich is included in the training set of all methods and has been showcased by both GMT and Any2Track.\nWe inject varying levels of noise into the reference commands to systematically assess the robustness of each method.\nThe noise specifications (base noise patterns) are summarized in Table\nII\n.\nWe define the noise level (%) by uniformly scaling the ranges in Table\nII\n.\nIn Fig.\n3\n, we report additional metrics beyond\nE\nMPJPE\nE_{\\mathrm{MPJPE}}\n.\nE\nMPKPE\nE_{\\mathrm{MPKPE}}\nmeasures the mean keypoint position error (in mm).\nTo assess physical fidelity,\nE\nvel\nE_{\\mathrm{vel}}\nand\nE\nacc\nE_{\\mathrm{acc}}\nmeasure the differences in keypoint velocities and accelerations relative to the reference motion,\nreported in mm/frame and mm/frame\n2\n, respectively.\nAs shown in Fig.\n3\n, baseline methods such as GMT and Any2Track degrade rapidly as the noise level increases.\nE\nMPJPE\nE_{\\mathrm{MPJPE}}\n,\nE\nvel\nE_{\\mathrm{vel}}\n, and\nE\nacc\nE_{\\mathrm{acc}}\nrise substantially, and the baselines struggle to maintain stable tracking when the noise level exceeds 200%.\nThese results suggest that, without an explicit mechanism to interpret and filter corrupted commands, baseline methods are more likely to amplify command noise in closed-loop execution, leading to instability at high noise levels.\nIn contrast, our method demonstrates significantly stronger robustness across all four metrics.\nEven at high noise levels, the errors remain relatively low and degrade much more gradually.\nAblation experiments further confirm this trend.\nReplacing the causal history encoder with a CNN-based variant consistently degrades robustness, confirming the importance of the causal history encoder.\nReplacing the cross-attention command encoder with a self-attention variant leads to a much larger degradation, especially in the high-noise regime, highlighting the cross-attention module as the key mechanism for filtering noisy commands.\nNotably, our full model maintains stable motion tracking even under noise levels up to 1500%, demonstrating strong tolerance to severe command corruption.\nOverall, these results highlight the central role of dynamics-conditioned cross-attention in filtering noisy commands and improving robustness, while the causal history encoder provides complementary benefits by stabilizing the dynamics representation used for command interpretation.\nIII-C\nReal-World Experiments\nFigure 4\n:\nReal-world dance tracking with fall recovery.\nIII-C\n1\nRobust Whole-Body Motion Tracking\nFig.\n4\nqualitatively illustrates two challenging behaviors enabled by our policy.\nIn the top row, the robot tracks a breakdance-style motion with frequent ground contacts and rapid contact transitions, demonstrating our policyâ€™s ability to robustly coordinate whole-body motion under complex contact patterns.\nIn the bottom row, we apply an external push that causes the robot to fall. The policy autonomously executes a recovery maneuver and then resumes the motion-tracking task without manual resets, which improves robustness for long-horizon deployment and reduces the need for human intervention.\nIII-C\n2\nVideo-Derived Motion Tracking\nFig.\nLABEL:result\n(b) presents qualitative results on video-derived motions to evaluate generalization to unseen motions.\nFor these examples, the reference commands are produced by a video-based human motion estimation pipeline\n[\n20\n]\n, which introduces noise and distribution shift.\nDespite these challenges, our policy tracks the commanded motions on the real robot with high fidelity, reproducing the overall timing and whole-body coordination.\nThese results demonstrate effective transfer to previously unseen motion styles.\nIII-C\n3\nReal-Time Teleoperation\nWe develop a real-time full-body teleoperation pipeline with two different motion-tracking front ends: a consumer-grade PICO VR whole-body tracking interface and a wearable motion-capture suit.\nFor the PICO setup, the operator wears a VR headset, two ankle trackers, and handheld controllers, and the system outputs full-body pose estimates at runtime.\nFor the motion-capture suit setup, the suit streams full-body pose measurements online through its vendor SDK, which are converted to the same kinematic representation to ensure a unified downstream interface.\nIn both cases, the estimated human motion is streamed in real-time and transformed into our reference command representation\nğ \nt\n\\mathbf{g}_{t}\n, which is then consumed by the deployed actor together with proprioceptive observations.\nThis teleoperation setting is substantially noisier than offline MoCap due to sensing drift, latency, and operator inconsistency, and thus serves as a practical stress test of generalization and robustness.\nAs shown in Fig.\nLABEL:result\n(c) andÂ (d), under both teleoperation sources, our policy remains stable and precisely tracks the incoming commands without manual resets, enabling challenging whole-body behaviors such as crawling, high kicks, and deep squats.\nIII-C\n4\nJoystick-driven locomotion\nFig.\n5\nillustrates a downstream integration of our tracker into a representative computer graphics motion synthesis method\n[\n10\n]\n.\nA handheld game joystick provides high-level locomotion commands, which are mapped to a sequence of reference motion targets and streamed to our policy as contextual commands.\nAlthough the synthesized reference sequence can exhibit abrupt transitions due to discrete clip-switching and matching artifacts, our policy remains stable and tracks the commanded base velocity in a stylized manner, producing coherent whole-body locomotion on the real robot.\nThis result suggests that our tracker can serve as a robust low-level controller for upstream motion generation modules, tolerating non-smooth reference trajectories while preserving responsive velocity tracking and motion style.\nFigure 5\n:\nJoystick-driven stylized locomotion.\nIV\nConclusion\nWe propose a whole-body motion tracking framework that achieves robust, generalized humanoid control with a single policy.\nBy conditioning command aggregation on a dynamics representation extracted from recent proprioception, the policy can down-weight inconsistent reference segments and remain stable under substantial command corruption.\nResults in simulation and on a 29-DoF Unitree G1 demonstrate accurate tracking across diverse motion sources, strong generalization to previously unseen motions, and reliable execution under complex contact patterns and external disturbances, enabling practical applications such as real-time teleoperation and joystick-driven locomotion.\nFuture work will incorporate global localization to enable long-horizon, world-frame consistent tracking, and expand integration with upstream motion generation and planning modules to support richer downstream tasks.\nReferences\n[1]\nJ. P. Araujo, Y. Ze, P. Xu, J. Wu, and C. K. Liu\n(2025)\nRetargeting matters: general motion retargeting for humanoid motion tracking\n.\nExternal Links:\n2510.02252\nCited by:\nÂ§\nII-A\n.\n[2]\nQ. Ben, F. Jia, J. Zeng, J. Dong, D. Lin, and J. Pang\n(2025-Jun.)\nHOMIE: humanoid loco-manipulation with isomorphic exoskeleton cockpit\n.\nIn\nProc. Robotics Sci. Syst. (RSS)\n,\nLos Angeles, CA, USA\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[3]\nZ. Chen, M. Ji, X. Cheng, X. Peng, X. B. Peng, and X. Wang\n(2025)\nGMT: general motion tracking for humanoid whole-body control\n.\nExternal Links:\nDocument\n,\n2506.14770\nCited by:\nÂ§I\n,\nÂ§\nIII-A\n1\n.\n[4]\nX. Cheng, Y. Ji, J. Chen, R. Yang, G. Yang, and X. Wang\n(2024-Jul.)\nExpressive whole-body control for humanoid robots\n.\nIn\nProc. Robotics Sci. Syst. (RSS)\n,\nDelft, Netherlands\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[5]\nJ. Han, W. Xie, J. Zheng, J. Shi, W. Zhang, T. Xiao, and C. Bai\n(2025)\nKungfuBot2: learning versatile motion skills for humanoid whole-body control\n.\nExternal Links:\nDocument\n,\n2509.16638\nCited by:\nÂ§I\n.\n[6]\nF. G. Harvey, M. Yurick, D. Nowrouzezahrai, and C. J. Pal\n(2020)\nRobust motion in-betweening\n.\nACM Trans. Graph.\n39\n(\n4\n),\npp.Â 60:1â€“60:12\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-A\n,\nÂ§\nII-E\n.\n[7]\nT. He, J. Gao, W. Xiao, Y. Zhang, Z. Wang, J. Wang, Z. Luo, G. He, N. Sobanbabu, C. Pan, Z. Yi, G. Qu, K. Kitani, J. K. Hodgins, L. Fan, Y. Zhu, C. Liu, and G. Shi\n(2025-Jun.)\nASAP: aligning simulation and real-world physics for learning agile humanoid whole-body skills\n.\nIn\nProc. Robotics Sci. Syst. (RSS)\n,\nLos Angeles, CA, USA\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[8]\nT. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. M. Kitani, C. Liu, and G. Shi\n(2025-Nov.)\nOmniH2O: universal and dexterous human-to-humanoid whole-body teleoperation and learning\n.\nIn\nProc. Conf. Robot Learn. (CoRL)\n,\nProceedings of Machine Learning Research\n, Vol.\n270\n,\npp.Â 1516â€“1540\n.\nCited by:\nÂ§I\n.\n[9]\nX. He, R. Dong, Z. Chen, and S. Gupta\n(2025-Jun.)\nLearning getting-up policies for real-world humanoid robots\n.\nIn\nProc. Robotics Sci. Syst. (RSS)\n,\nLos Angeles, CA, USA\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§I\n.\n[10]\nD. Holden, O. Kanoun, M. Perepichka, and T. Popa\n(2020)\nLearned motion matching\n.\nACM Trans. Graph.\n39\n(\n4\n),\npp.Â 53:1â€“53:12\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nIII-C\n4\n.\n[11]\nT. Huang, J. Ren, H. Wang, Z. Wang, Q. Ben, M. Wen, X. Chen, J. Li, and J. Pang\n(2025-Jun.)\nLearning humanoid standing-up control across diverse postures\n.\nIn\nProc. Robotics Sci. Syst. (RSS)\n,\nLos Angeles, CA, USA\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§I\n.\n[12]\nM. Ji, X. Peng, F. Liu, J. Li, G. Yang, X. Cheng, and X. Wang\n(2025-Jun.)\nExBody2: advanced expressive humanoid whole-body control\n.\nIn\nRSS 2025 Workshop Whole-Body Control and Bimanual Manipulation\n,\nCited by:\nÂ§I\n.\n[13]\nY. Li, Y. Lin, J. Cui, T. Liu, W. Liang, Y. Zhu, and S. Huang\n(2025-Sep.)\nCLONE: closed-loop whole-body humanoid teleoperation for long-horizon tasks\n.\nIn\nProc. Conf. Robot Learn. (CoRL)\n,\nProceedings of Machine Learning Research\n, Vol.\n305\n,\npp.Â 4493â€“4505\n.\nCited by:\nÂ§I\n.\n[14]\nZ. Li, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, and K. Sreenath\n(2023-Jul.)\nRobust and versatile bipedal jumping control through reinforcement learning\n.\nIn\nProc. Robotics Sci. Syst. (RSS)\n,\nDaegu, Republic of Korea\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[15]\nZ. Luo, Y. Yuan, T. Wang, C. Li, S. Chen, F. Castaneda, Z. Cao, J. Li, D. Minor, Q. Ben, X. Da, R. Ding, C. Hogg, L. Song, E. Lim, E. Jeong, T. He, H. Xue, W. Xiao, Z. Wang, S. Yuen, J. Kautz, Y. Chang, U. Iqbal, L. Fan, and Y. Zhu\n(2025)\nSONIC: supersizing motion tracking for natural humanoid whole-body control\n.\nExternal Links:\nDocument\n,\n2511.07820\nCited by:\nÂ§I\n.\n[16]\nN. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black\n(2019)\nAMASS: archive of motion capture as surface shapes\n.\nIn\nProc. IEEE CVF Int. Conf. Comput. Vis. (ICCV)\n,\npp.Â 5441â€“5450\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-A\n,\nÂ§\nII-E\n.\n[17]\nV. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin, A. Allshire, A. Handa, and G. State\n(2021)\nIsaac gym: high performance gpu based physics simulation for robot learning\n.\nIn\nProc. NeurIPS Track Datasets and Benchmarks\n,\nJ. Vanschoren and S. Yeung (Eds.)\n,\nCited by:\nÂ§\nII-E\n.\n[18]\nI. Radosavovic, S. Kamat, T. Darrell, and J. Malik\n(2024)\nLearning humanoid locomotion over challenging terrain\n.\nExternal Links:\nDocument\n,\n2410.03654\nCited by:\nÂ§I\n.\n[19]\nI. Radosavovic, B. Zhang, B. Shi, J. Rajasegaran, S. Kamat, T. Darrell, K. Sreenath, and J. Malik\n(2024)\nHumanoid locomotion as next token prediction\n.\nIn\nProc. Adv. Neural Inf. Process. Syst. (NeurIPS)\n,\nVol.\n37\n,\npp.Â 79307â€“79324\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[20]\nZ. Shen, H. Pi, Y. Xia, Z. Cen, S. Peng, Z. Hu, H. Bao, R. Hu, and X. Zhou\n(2024-Dec.)\nWorld-grounded human motion recovery via gravity-view coordinates\n.\nIn\nSIGGRAPH Asia 2024 Conference Papers (SA â€™24)\n,\nTokyo, Japan\n,\npp.Â 144:1â€“144:11\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nIII-C\n2\n.\n[21]\nE. Todorov, T. Erez, and Y. Tassa\n(2012-Oct.)\nMuJoCo: a physics engine for model-based control\n.\nIn\nProc. IEEE RSJ Int. Conf. Intell. Robots Syst. (IROS)\n,\nVilamoura-Algarve, Portugal\n,\npp.Â 5026â€“5033\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nIII-A\n1\n.\n[22]\nT. E. Truong, Q. Liao, X. Huang, G. Tevet, C. K. Liu, and K. Sreenath\n(2025)\nBeyondMimic: from motion tracking to versatile humanoid control via guided diffusion\n.\nExternal Links:\nDocument\n,\n2508.08241\nCited by:\nÂ§I\n,\nÂ§\nII-B\n3\n.\n[23]\nUnitree Robotics\n(2024)\nUnitree g1: humanoid robot functions and price\n.\nNote:\nAccessed 2025-10-31\nCited by:\nÂ§III\n.\n[24]\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin\n(2017)\nAttention is all you need\n.\nIn\nProc. Adv. Neural Inf. Process. Syst. (NeurIPS)\n,\nVol.\n30\n,\npp.Â 5998â€“6008\n.\nCited by:\nÂ§I\n,\nÂ§\nII-C\n.\n[25]\nH. Wang, Z. Wang, J. Ren, Q. Ben, T. Huang, W. Zhang, and J. Pang\n(2025-Jun.)\nBeamDojo: learning agile humanoid locomotion on sparse footholds\n.\nIn\nProc. Robotics Sci. Syst. (RSS)\n,\nLos Angeles, CA, USA\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[26]\nW. Xie, J. Han, J. Zheng, H. Li, X. Liu, J. Shi, W. Zhang, C. Bai, and X. Li\n(2025)\nKungfuBot: physics-based humanoid whole-body control for learning highly-dynamic skills\n.\nIn\nProc. Adv. Neural Inf. Process. Syst. (NeurIPS)\n,\nCited by:\nÂ§I\n.\n[27]\nK. Yin, W. Zeng, K. Fan, M. Dai, Z. Wang, Q. Zhang, Z. Tian, J. Wang, J. Pang, and W. Zhang\n(2025)\nUniTracker: learning universal whole-body motion tracker for humanoid robots\n.\nExternal Links:\nDocument\n,\n2507.07356\nCited by:\nÂ§I\n.\n[28]\nY. Ze, S. Zhao, W. Wang, A. Kanazawa, R. Duan, P. Abbeel, G. Shi, J. Wu, and C. K. Liu\n(2025)\nTWIST2: scalable, portable, and holistic humanoid data collection system\n.\nExternal Links:\nDocument\n,\n2511.02832\nCited by:\nÂ§I\n.\n[29]\nT. Zhang, B. Zheng, R. Nai, Y. Hu, Y. J. Wang, G. Chen, F. Lin, J. Li, C. Hong, K. Sreenath, and Y. Gao\n(2025-Sep.)\nHuB: learning extreme humanoid balance\n.\nIn\nProc. Conf. Robot Learn. (CoRL)\n,\nProceedings of Machine Learning Research\n, Vol.\n305\n,\npp.Â 686â€“704\n.\nCited by:\nÂ§I\n.\n[30]\nY. Zhang, Y. Yuan, P. Gurunath, T. He, S. Omidshafiei, A. Agha-mohammadi, M. Vazquez-Chanlatte, L. Pedersen, and G. Shi\n(2025)\nFALCON: learning force-adaptive humanoid loco-manipulation\n.\nExternal Links:\nDocument\n,\n2505.06776\nCited by:\nÂ§I\n.\n[31]\nZ. Zhang, J. Guo, C. Chen, J. Wang, C. Lin, Y. Lian, H. Xue, Z. Wang, M. Liu, J. Lyu, H. Liu, H. Wang, and L. Yi\n(2025)\nTrack any motions under any disturbances\n.\nExternal Links:\nDocument\n,\n2509.13833\nCited by:\nÂ§I\n,\nÂ§\nIII-A\n1\n.\n[32]\nZ. Zhuang, S. Yao, and H. Zhao\n(2025-Nov.)\nHumanoid parkour learning\n.\nIn\nProc. Conf. Robot Learn. (CoRL)\n,\nProceedings of Machine Learning Research\n, Vol.\n270\n,\npp.Â 1975â€“1991\n.\nCited by:\nÂ§I\n.",
    "preview_text": "Learning a general humanoid whole-body controller is challenging because practical reference motions can exhibit noise and inconsistencies after being transferred to the robot domain, and local defects may be amplified by closed-loop execution, causing drift or failure in highly dynamic and contact-rich behaviors. We propose a dynamics-conditioned command aggregation framework that uses a causal temporal encoder to summarize recent proprioception and a multi-head cross-attention command encoder to selectively aggregate a context window based on the current dynamics. We further integrate a fall recovery curriculum with random unstable initialization and an annealed upward assistance force to improve robustness and disturbance rejection. The resulting policy requires only about 3.5 hours of motion data and supports single-stage end-to-end training without distillation. The proposed method is evaluated under diverse reference inputs and challenging motion regimes, demonstrating zero-shot transfer to unseen motions as well as robust sim-to-real transfer on a physical humanoid robot.\n\nRobust and Generalized Humanoid Motion Tracking\nYubiaoÂ Ma\nâ€ \n1\n{}^{1}\\dagger\n,\nHanÂ Yu\nâ€ \n2\n{}^{2}\\dagger\n,\nJiayinÂ Xie\n2\n,\nChangtaiÂ Lv\n2\n,\nQiangÂ Luo\n2\n,\nChiÂ Zhang\n2\n,\nYunpengÂ Yin\n2\n,\nBoyangÂ Xing\n2\n,\nXuemeiÂ Ren\n1\n,\nandÂ DongdongÂ Zheng\nâˆ—\n1\n,\n2\n{}^{1,2}\\ast\n1\nBeijing Institute of Technology, Beijing, China.\n2\nHumanoid Robotics (Shanghai) Co., Ltd., Shanghai 201203, China.\nâ€ \n\\dagger\nThese authors contributed equally.\nâˆ—\n\\ast\nCorresponding author: Dongdong Zheng (\nddzheng@bit.edu.cn\n).\nAbstract\nLearning a general humanoid whole-body controller is challenging because practical reference motions can exhibit noise and inconsistencies\nafter being transferred to the robot domain, and local defects may be amplified by closed-loop execution,\ncausing drift or failure in highly dynamic and contact-rich behaviors.\nWe propose a dynamics-conditioned command aggregation framework that uses a causal temporal encod",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "whole body control",
        "locomotion",
        "fine tune",
        "world model"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€æ¡ä»¶å‘½ä»¤èšåˆæ¡†æ¶çš„äººå½¢æœºå™¨äººå…¨èº«æ§åˆ¶å™¨ï¼Œé€šè¿‡å› æœæ—¶é—´ç¼–ç å™¨å’Œå¤šå¤´éƒ¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆè·Œå€’æ¢å¤è¯¾ç¨‹ï¼Œå®ç°äº†å¯¹å™ªå£°å‚è€ƒè¿åŠ¨çš„é²æ£’è·Ÿè¸ªå’Œé›¶æ ·æœ¬æ³›åŒ–ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šã€Šé²æ£’ä¸”é€šç”¨çš„äººå½¢æœºå™¨äººè¿åŠ¨è·Ÿè¸ªã€‹\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\näººå½¢æœºå™¨äººå› å…¶å½¢æ€ä¸äººç±»ç¯å¢ƒå…¼å®¹è€Œå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿåè°ƒå…¨èº«ã€é€‚åº”å¤šå˜æ¥è§¦ä¸ä»»åŠ¡çš„é€šç”¨å…¨èº«æ§åˆ¶å™¨ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼šå¯¹ç‰¹å®šè¿åŠ¨æ•°æ®ä¾èµ–æ€§å¼ºã€é—­ç¯ç¨³å®šæ€§å·®ï¼ˆå°¤å…¶åœ¨åŠ¨æ€è¿åŠ¨å’Œé¢‘ç¹æ¥è§¦è½¬æ¢æ—¶ï¼‰ã€éœ€è¦æµ·é‡æ•°æ®è®­ç»ƒï¼Œä¸”ç¼ºä¹æœ‰æ•ˆçš„è·Œå€’æ¢å¤æœºåˆ¶ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„é²æ£’æ€§å’ŒæŒç»­è¿è¡Œèƒ½åŠ›ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡ºäº†ä¸€ä¸ª**åŠ¨æ€æ¡ä»¶å‘½ä»¤èšåˆæ¡†æ¶**ï¼Œæ—¨åœ¨åˆ©ç”¨æœ‰é™æ•°æ®è®­ç»ƒå‡ºé²æ£’ã€é€šç”¨çš„å…¨èº«æ§åˆ¶ç­–ç•¥ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š\n- **åŠ¨æ€æ¡ä»¶å‘½ä»¤èšåˆ**ï¼šä½¿ç”¨ä¸€ä¸ª**å› æœæ—¶åºç¼–ç å™¨**ä»è¿‘æœŸæœ¬ä½“æ„ŸçŸ¥å†å²ä¸­æå–åŠ¨æ€è¡¨å¾ï¼Œå¹¶ä»¥æ­¤ä½œä¸ºæŸ¥è¯¢ä¿¡å·ï¼Œé€šè¿‡**å¤šå¤´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶**é€‰æ‹©æ€§åœ°èšåˆä¸€ä¸ªä¸Šä¸‹æ–‡å‘½ä»¤çª—å£ã€‚è¿™ä½¿å¾—ç­–ç•¥èƒ½æ ¹æ®å½“å‰ç‰©ç†å¯è¡Œæ€§åŠ¨æ€è°ƒæ•´å¯¹å‚è€ƒå‘½ä»¤çš„ä¿¡ä»»åº¦ï¼Œæœ‰æ•ˆè¿‡æ»¤å™ªå£°å’Œä¸ä¸€è‡´ç‰‡æ®µã€‚\n- **é›†æˆå¼è·Œå€’æ¢å¤è¯¾ç¨‹**ï¼šåœ¨è®­ç»ƒä¸­é›†æˆè·Œå€’æ¢å¤ï¼Œé€šè¿‡éšæœºä¸ç¨³å®šåˆå§‹åŒ–å’Œé€€ç«è¾…åŠ©åŠ›ï¼Œä½¿å•ä¸€ç­–ç•¥åŒæ—¶å­¦ä¼šè¿åŠ¨è·Ÿè¸ªå’Œè‡ªä¸»ç«™èµ·ï¼Œæ˜¾è‘—æå‡äº†æŠ—å¹²æ‰°èƒ½åŠ›å’Œæ¥è§¦ä¸°å¯Œåœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚\n- **é«˜æ•ˆè®­ç»ƒæµç¨‹**ï¼šæ•´ä¸ªç­–ç•¥é‡‡ç”¨**ç«¯åˆ°ç«¯å•é˜¶æ®µè®­ç»ƒ**ï¼Œæ— éœ€è’¸é¦æˆ–å¤šé˜¶æ®µè®­ç»ƒï¼Œä»…éœ€çº¦**3.5å°æ—¶**ç»è´¨é‡ç­›é€‰çš„è¿åŠ¨æ•°æ®ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\nåœ¨ä»¿çœŸå’ŒUnitree G1å®ä½“æœºå™¨äººä¸Šçš„å®éªŒè¡¨æ˜ï¼š\n- **å“è¶Šçš„è·Ÿè¸ªæ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨MoCapã€è§†é¢‘è¡ç”Ÿè¿åŠ¨ç­‰å¤šç§å‚è€ƒæºä¸Šï¼Œå…¶æˆåŠŸç‡å’Œå…³èŠ‚ä½ç½®è¯¯å·®å‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ˆå¦‚GMTã€Any2Trackï¼‰ï¼Œå¹¶èƒ½é›¶æ ·æœ¬è¿ç§»åˆ°æœªè§è¿‡çš„è¿åŠ¨ã€‚\n- **å¼ºå¤§çš„æŠ—å™ªå£°é²æ£’æ€§**ï¼šåœ¨å‚è€ƒå‘½ä»¤æ³¨å…¥ä¸¥é‡å™ªå£°ï¼ˆæœ€é«˜è¾¾1500%ï¼‰çš„æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¸‹é™å¹³ç¼“ï¼Œè€ŒåŸºçº¿æ–¹æ³•è¿…é€Ÿå¤±æ•ˆï¼ŒéªŒè¯äº†åŠ¨æ€æ¡ä»¶èšåˆæœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚\n- **æˆåŠŸçš„å®ç‰©éƒ¨ç½²**ï¼šç­–ç•¥èƒ½ç¨³å®šå®Œæˆå¤æ‚åŠ¨æ€è¿åŠ¨ï¼ˆå¦‚éœ¹é›³èˆï¼‰ã€ä»è·Œå€’ä¸­è‡ªä¸»æ¢å¤ï¼Œå¹¶æˆåŠŸåº”ç”¨äºå®æ—¶å…¨èº«é¥æ“ä½œå’Œæ‘‡æ†é©±åŠ¨ locomotion ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\næœ¬ç ”ç©¶ä¸ºå®ç°**æ•°æ®é«˜æ•ˆã€é²æ£’ä¸”é€šç”¨**çš„äººå½¢æœºå™¨äººå…¨èº«æ§åˆ¶æä¾›äº†åˆ‡å®å¯è¡Œçš„æ¡†æ¶ã€‚å…¶æŠ€æœ¯åˆ›æ–°â€”â€”åŠ¨æ€æ„ŸçŸ¥çš„å‘½ä»¤ç­›é€‰ä¸å†…ç½®æ¢å¤èƒ½åŠ›â€”â€”æœ‰æ•ˆè§£å†³äº†å®é™…éƒ¨ç½²ä¸­å‚è€ƒä¿¡å·ä¸å®Œç¾å’Œçªå‘å¹²æ‰°çš„æ ¸å¿ƒéš¾é¢˜ã€‚è¯¥å·¥ä½œæ¨åŠ¨äº†äººå½¢æœºå™¨äººå‘åœ¨å¤æ‚ã€éç»“æ„åŒ–ç¯å¢ƒä¸­è¿›è¡Œ**å®‰å…¨ã€è¿ç»­ã€é•¿æœŸä½œä¸š**è¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ï¼Œä¸ºåç»­ä¸é«˜å±‚è¿åŠ¨ç”Ÿæˆã€è§„åˆ’æ¨¡å—çš„é›†æˆå¥ å®šäº†åšå®åŸºç¡€ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **ä¸€ã€ ç ”ç©¶é—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**é€šç”¨äººå½¢æœºå™¨äººå…¨èº«è¿åŠ¨è·Ÿè¸ª**ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼š\n1.  **å‚è€ƒè¿åŠ¨ä¸å®Œç¾**ï¼šä»äººç±»åŠ¨ä½œæ•æ‰ï¼ˆMoCapï¼‰ã€è§†é¢‘ä¼°è®¡æˆ–é¥æ“ä½œè·å¾—çš„å‚è€ƒè¿åŠ¨ï¼Œåœ¨è¿ç§»åˆ°æœºå™¨äººé¢†åŸŸåå¸¸åŒ…å«å™ªå£°ã€ç‰©ç†ä¸ä¸€è‡´ï¼ˆå¦‚ç©¿é€ã€æ¥è§¦é”™è¯¯ï¼‰å’Œå±€éƒ¨ç¼ºé™·ã€‚\n2.  **é—­ç¯ç¨³å®šæ€§å·®**ï¼šä¸Šè¿°ç¼ºé™·åœ¨é—­ç¯æ‰§è¡Œä¸­ä¼šè¢«æ”¾å¤§ï¼Œå¯¼è‡´åœ¨**é«˜åŠ¨æ€**ï¼ˆå¦‚å¿«é€Ÿèˆè¹ˆï¼‰å’Œ**æ¥è§¦ä¸°å¯Œ**ï¼ˆå¦‚çˆ¬è¡Œã€æ‘”å€’æ¢å¤ï¼‰çš„è¡Œä¸ºä¸­å‡ºç°æ¼‚ç§»æˆ–å¤±è´¥ã€‚\n3.  **æ•°æ®ä¸è®­ç»ƒæ•ˆç‡ä½**ï¼šç°æœ‰æ–¹æ³•ï¼ˆå¦‚SONICï¼‰éœ€è¦æµ·é‡æ•°æ®ï¼ˆ>700å°æ—¶ï¼‰å’Œè®¡ç®—èµ„æºï¼Œä¸”å¸¸éœ€å¤šé˜¶æ®µè®­ç»ƒæˆ–è’¸é¦ï¼Œé˜»ç¢äº†è¿­ä»£ç ”ç©¶å’Œéƒ¨ç½²ã€‚\n4.  **é²æ£’æ€§ä¸è¶³**ï¼šè®¸å¤šç­–ç•¥ç¼ºä¹**é›†æˆåŒ–çš„æ‘”å€’æ¢å¤**èƒ½åŠ›ï¼Œæ— æ³•åœ¨æ‰°åŠ¨åè‡ªä¸»æ¢å¤ï¼Œé™åˆ¶äº†é•¿æœŸã€å®‰å…¨çš„çœŸå®ä¸–ç•Œéƒ¨ç½²ã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸º **â€œRGMTâ€** çš„é›†æˆå­¦ä¹ æ¡†æ¶ï¼Œå…¶åˆ›æ–°ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹ä¸‰ä¸ªæ–¹é¢ï¼š\n\n#### **1. åŠ¨æ€æ¡ä»¶åŒ–çš„å‘½ä»¤èšåˆæ¡†æ¶**\n- **æ ¸å¿ƒæ€æƒ³**ï¼šæ”¿ç­–ä¸åº”å¹³ç­‰å¯¹å¾…æ‰€æœ‰å‚è€ƒå‘½ä»¤ï¼Œè€Œåº”æ ¹æ®**å½“å‰æœºå™¨äººåŠ¨æ€çŠ¶æ€**ï¼Œæœ‰é€‰æ‹©åœ°èšåˆå’Œè§£é‡Šä¸Šä¸‹æ–‡å‘½ä»¤ã€‚\n- **å…³é”®æŠ€æœ¯**ï¼š\n    - **å› æœæ—¶åºå†å²ç¼–ç å™¨**ï¼šä½¿ç”¨ä¸€ä¸ªè½»é‡çº§å› æœTransformerï¼Œå¯¹æœ€è¿‘10æ­¥çš„æœ¬ä½“æ„ŸçŸ¥å†å²ï¼ˆå…³èŠ‚çŠ¶æ€ã€é€Ÿåº¦ç­‰ï¼‰è¿›è¡Œç¼–ç ï¼Œæå–å‡ºç´§å‡‘çš„**åŠ¨æ€åµŒå…¥**ï¼Œè¡¨å¾æœºå™¨äººå½“å‰çš„ç‰©ç†çŠ¶æ€å’Œè¶‹åŠ¿ã€‚\n    - **å¤šå¤´äº¤å‰æ³¨æ„åŠ›å‘½ä»¤ç¼–ç å™¨**ï¼šä»¥ä¸Šè¿°åŠ¨æ€åµŒå…¥ä½œä¸º**æŸ¥è¯¢**ï¼Œå¯¹ä¸€ä¸ªæ—¶é—´çª—å£å†…çš„å‚è€ƒå‘½ä»¤è¿›è¡Œ**äº¤å‰æ³¨æ„åŠ›**èšåˆã€‚è¿™ä½¿å¾—æ”¿ç­–èƒ½å¤Ÿâ€œå…³æ³¨â€ä¸å½“å‰åŠ¨æ€ç›¸å®¹çš„å‘½ä»¤ç‰‡æ®µï¼Œå¹¶è‡ªåŠ¨å¼±åŒ–ä¸ä¸€è‡´æˆ–ä¸å¯è¡Œçš„å™ªå£°å‘½ä»¤ã€‚\n- **ä»·å€¼**ï¼šæ˜¾è‘—æå‡äº†åœ¨å™ªå£°å‚è€ƒè¾“å…¥ä¸‹çš„**è·Ÿè¸ªç²¾åº¦**å’Œ**é—­ç¯ç¨³å®šæ€§**ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åŠ¨æ€å’Œæ¥è§¦é¢‘ç¹çš„åœºæ™¯ä¸­ã€‚\n\n#### **2. é›†æˆåŒ–æ‘”å€’æ¢å¤è¯¾ç¨‹**\n- **æ ¸å¿ƒæ€æƒ³**ï¼šå°†æ‘”å€’æ¢å¤ä½œä¸º**åŒä¸€æ”¿ç­–è®­ç»ƒæµç¨‹çš„ä¸€éƒ¨åˆ†**ï¼Œè€Œéç‹¬ç«‹æ¨¡å—ï¼Œä»¥æ‰©å¤§çŠ¶æ€åˆ†å¸ƒç»éªŒå¹¶æå‡é²æ£’æ€§ã€‚\n- **å…³é”®æŠ€æœ¯**ï¼š\n    - **éšæœºä¸ç¨³å®šåˆå§‹åŒ–**ï¼šåœ¨è®­ç»ƒä¸­ï¼Œä»¥ä¸€å®šæ¦‚ç‡å°†ç¯å¢ƒé‡ç½®ä¸ºéšæœºçš„ä¸ç¨³å®šå§¿æ€ï¼ˆå¦‚èººå€’ï¼‰ï¼Œè¿«ä½¿æ”¿ç­–å­¦ä¹ ä»å„ç§æ‘”å€’çŠ¶æ€ä¸­ç«™èµ·ã€‚\n    - **é€€ç«è¾…åŠ©åŠ›**ï¼šåœ¨è®­ç»ƒæ—©æœŸæ–½åŠ å‘ä¸Šçš„è¾…åŠ©åŠ›ï¼Œå¸®åŠ©æ¢ç´¢å¯æ¢å¤çŠ¶æ€ï¼Œéšåçº¿æ€§é€€ç«è‡³é›¶ï¼Œç¡®ä¿æœ€ç»ˆæ”¿ç­–ä»…ä¾é è‡ªèº«æ§åˆ¶å®Œæˆæ¢å¤ã€‚\n    - **æ¢å¤çª—å£**ï¼šåœ¨æ¢å¤ç¯å¢ƒä¸­è®¾ç½®çŸ­æš‚çš„ä¸æå‰ç»ˆæ­¢çª—å£ï¼Œè®©æ”¿ç­–æœ‰å®Œæ•´çš„æœºä¼šå­¦ä¹ ç«™èµ·åºåˆ—ã€‚\n- **ä»·å€¼**ï¼š\n    - ä½¿å•ä¸€æ”¿ç­–åŒæ—¶å…·å¤‡**è¿åŠ¨è·Ÿè¸ª**å’Œ**è‡ªä¸»æ¢å¤**èƒ½åŠ›ã€‚\n    - é€šè¿‡æ¥è§¦ä¸°å¯Œçš„æ¢å¤è¿‡ç¨‹ï¼Œé—´æ¥æå‡äº†æ”¿ç­–åœ¨å¤„ç†å¤æ‚åœ°é¢äº¤äº’è¿åŠ¨æ—¶çš„**é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›**ã€‚\n\n#### **3. é«˜æ•ˆçš„æ•°æ®åˆ©ç”¨ä¸ç«¯åˆ°ç«¯è®­ç»ƒ**\n- **æ•°æ®**ï¼šä»…ä½¿ç”¨çº¦**3.5å°æ—¶**ç»è¿‡è´¨é‡ç­›é€‰çš„å¤šæ ·åŒ–è¿åŠ¨æ•°æ®ï¼ˆæ¥è‡ªLAFAN1å’ŒAMASSï¼‰ï¼Œè¿œå°‘äºåŒç±»å·¥ä½œã€‚\n- **è®­ç»ƒ**ï¼šæ”¯æŒ**å•é˜¶æ®µç«¯åˆ°ç«¯è®­ç»ƒ**ï¼Œæ— éœ€å¤æ‚çš„è’¸é¦æˆ–å¤šé˜¶æ®µ pipelineï¼Œé™ä½äº†è®­ç»ƒå¤æ‚åº¦å’Œèµ„æºéœ€æ±‚ã€‚\n- **ä»·å€¼**ï¼šè¯æ˜äº†é€šè¿‡**ç²¾å·§çš„æ¶æ„è®¾è®¡**å’Œ**è¯¾ç¨‹å­¦ä¹ **ï¼Œå¯ä»¥ç”¨å°æ•°æ®é›†è®­ç»ƒå‡ºå¼ºå¤§ã€é€šç”¨çš„å…¨èº«æ§åˆ¶å™¨ï¼Œé™ä½äº†ç ”ç©¶ä¸åº”ç”¨é—¨æ§›ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆæ€»ç»“**\nè®ºæ–‡é€šè¿‡ä¸€ä¸ª**ç»Ÿä¸€çš„ã€ä»¥åŠ¨æ€æ„ŸçŸ¥ä¸ºæ ¸å¿ƒçš„ç­–ç•¥æ¶æ„**ï¼Œç»“åˆ**é›†æˆåŒ–çš„é²æ£’æ€§è®­ç»ƒè¯¾ç¨‹**ï¼Œç³»ç»Ÿæ€§åœ°è§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼š\n\n1.  **é’ˆå¯¹ä¸å®Œç¾å‚è€ƒ**ï¼šåˆ©ç”¨**åŠ¨æ€æ¡ä»¶åŒ–çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶**ä½œä¸ºâ€œè¿‡æ»¤å™¨â€ï¼Œä½¿æ”¿ç­–èƒ½è‡ªé€‚åº”åœ°ä¾èµ–å¯é çš„å‘½ä»¤ï¼ŒæŠ‘åˆ¶å™ªå£°å’Œä¼ªå½±ã€‚\n2.  **é’ˆå¯¹é—­ç¯ä¸ç¨³å®š**ï¼šä¸Šè¿°æœºåˆ¶ä»æºå¤´å‡å°‘äº†é”™è¯¯å‘½ä»¤çš„æ”¾å¤§ï¼›åŒæ—¶ï¼Œ**æ‘”å€’æ¢å¤è¯¾ç¨‹**è®©æ”¿ç­–ç»å†äº†æ›´å¹¿çš„ï¼ˆåŒ…æ‹¬ä¸ç¨³å®šçš„ï¼‰çŠ¶æ€åˆ†å¸ƒï¼Œå¢å¼ºäº†å…¶æŠ—æ‰°åŠ¨å’Œç»´æŒç¨³å®šçš„èƒ½åŠ›ã€‚\n3.  **é’ˆå¯¹æ•ˆç‡ä¸æ³›åŒ–**ï¼šç´§å‡‘ä½†é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œé…åˆç«¯åˆ°ç«¯è®­ç»ƒï¼Œå®ç°äº†é«˜æ•ˆå­¦ä¹ ã€‚åœ¨å®éªŒä¸­å±•ç°äº†å¼ºå¤§çš„**é›¶æ ·æœ¬æ³›åŒ–**èƒ½åŠ›ï¼ˆå¯¹æœªè§è¿‡çš„è§†é¢‘ä¼°è®¡è¿åŠ¨ï¼‰å’Œ**ä»¿çœŸåˆ°å®ç‰©çš„è½¬ç§»**èƒ½åŠ›ã€‚\n\n**å®é™…ä»·å€¼**ï¼šè¯¥æ¡†æ¶æˆåŠŸåœ¨Unitree G1äººå½¢æœºå™¨äººä¸Šå®ç°äº†ï¼š\n- å¯¹å¤šæ ·æ¥æºï¼ˆMoCapã€è§†é¢‘ã€å®æ—¶é¥æ“ä½œï¼‰è¿åŠ¨çš„é«˜ä¿çœŸã€é²æ£’è·Ÿè¸ªã€‚\n- åœ¨å¤–éƒ¨æ¨åŠ¨ä¸‹çš„è‡ªä¸»æ‘”å€’æ¢å¤ä¸ä»»åŠ¡ç»­è¡Œã€‚\n- ä¸‹æ¸¸åº”ç”¨é›†æˆï¼Œå¦‚æ‘‡æ†é©±åŠ¨çš„é£æ ¼åŒ– locomotionã€‚\nè¿™ä¸ºäººå½¢æœºå™¨äººåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­è¿›è¡Œé•¿æœŸã€å®‰å…¨ã€è¿ç»­çš„ä»»åŠ¡æ‰§è¡Œæä¾›äº†å¯è¡Œçš„é€šç”¨åº•å±‚æ§åˆ¶å™¨æ–¹æ¡ˆã€‚",
            "timestamp": "2026-02-03T15:54:29.278226",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡åˆ†æï¼šç¨³å¥ä¸”é€šç”¨çš„äººå½¢æœºå™¨äººè¿åŠ¨è·Ÿè¸ª\n\n### ä¸€ã€ æ ¸å¿ƒé—®é¢˜ä¸æ€»ä½“è´¡çŒ®\n\næœ¬æ–‡æ—¨åœ¨è§£å†³**é€šç”¨äººå½¢æœºå™¨äººå…¨èº«æ§åˆ¶**ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼šåœ¨å‚è€ƒè¿åŠ¨å­˜åœ¨å™ªå£°ã€ä¸ä¸€è‡´æ€§ï¼Œä¸”é—­ç¯æ‰§è¡Œå¯èƒ½æ”¾å¤§å±€éƒ¨ç¼ºé™·çš„æƒ…å†µä¸‹ï¼Œå®ç°**ç¨³å®šã€é²æ£’ä¸”å¯æ³›åŒ–**çš„è¿åŠ¨è·Ÿè¸ªã€‚\n\n**ä¸»è¦è´¡çŒ®**ï¼š\n1.  **æå‡ºåŠ¨æ€æ¡ä»¶åŒ–å‘½ä»¤èšåˆæ¡†æ¶**ï¼šåˆ©ç”¨å› æœæ—¶åºç¼–ç å™¨æ€»ç»“è¿‘æœŸæœ¬ä½“æ„ŸçŸ¥ï¼Œå¹¶é€šè¿‡å¤šå¤´äº¤å‰æ³¨æ„åŠ›å‘½ä»¤ç¼–ç å™¨ï¼Œæ ¹æ®å½“å‰åŠ¨æ€æœ‰é€‰æ‹©åœ°èšåˆä¸Šä¸‹æ–‡çª—å£ä¸­çš„å‘½ä»¤ã€‚è¿™ä½¿å¾—ç­–ç•¥èƒ½å¤Ÿâ€œæ™ºèƒ½â€åœ°è¿‡æ»¤ä¸å¯é çš„å‚è€ƒä¿¡å·ã€‚\n2.  **é›†æˆè·Œå€’æ¢å¤è¯¾ç¨‹**ï¼šé€šè¿‡éšæœºä¸ç¨³å®šåˆå§‹åŒ–å’Œé€€ç«å¼å‘ä¸Šè¾…åŠ©åŠ›ï¼Œå°†è·Œå€’æ¢å¤è®­ç»ƒç»Ÿä¸€åˆ°ä¸»ç­–ç•¥å­¦ä¹ ä¸­ï¼Œæ˜¾è‘—æå‡äº†é²æ£’æ€§å’ŒæŠ—å¹²æ‰°èƒ½åŠ›ã€‚\n3.  **å®ç°é«˜æ•ˆè®­ç»ƒä¸å¼ºå¤§æ³›åŒ–**ï¼šä»…éœ€çº¦ **3.5å°æ—¶** çš„é«˜è´¨é‡è¿åŠ¨æ•°æ®ï¼Œæ”¯æŒ**å•é˜¶æ®µç«¯åˆ°ç«¯è®­ç»ƒ**ï¼ˆæ— éœ€è’¸é¦ï¼‰ã€‚ç­–ç•¥åœ¨å¤šç§å™ªå£°å‚è€ƒè¾“å…¥å’ŒæŒ‘æˆ˜æ€§è¿åŠ¨æ¨¡å¼ä¸‹å‡è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†å¯¹æœªè§è¿åŠ¨çš„**é›¶æ ·æœ¬æ³›åŒ–**ä»¥åŠä»ä»¿çœŸåˆ°å®ç‰©çš„**é²æ£’è¿ç§»**ã€‚\n\n### äºŒã€ æŠ€æœ¯è„‰ç»œä¸å‘å±•æ–¹å‘æ¢³ç†\n\nåŸºäºæœ¬æ–‡åŠå…¶å¼•ç”¨çš„ç›¸å…³å·¥ä½œï¼Œè¯¥é¢†åŸŸï¼ˆæ•°æ®é©±åŠ¨çš„é€šç”¨äººå½¢å…¨èº«è¿åŠ¨è·Ÿè¸ªï¼‰çš„å‘å±•è„‰ç»œå¯æ¢³ç†ä¸ºä»¥ä¸‹å‡ ä¸ªç›¸äº’äº¤ç»‡çš„æ–¹å‘ï¼š\n\n#### **æ–¹å‘ä¸€ï¼šä»â€œä¸“ç²¾â€åˆ°â€œé€šç”¨â€â€”â€”ç­–ç•¥å­¦ä¹ èŒƒå¼çš„æ¼”è¿›**\n\n*   **æ—©æœŸå·¥ä½œï¼ˆä¸“ç²¾åŒ–æ¨¡ä»¿ï¼‰**ï¼šå¦‚ `HuB`ã€`KungfuBot`ã€`ASAP` ç­‰ï¼Œé€šå¸¸é’ˆå¯¹**å•ä¸€æˆ–å°‘é‡ç‰¹å®šæŠ€èƒ½**ï¼ˆå¦‚å¹³è¡¡ã€é«˜åŠ¨æ€æŠ€èƒ½ï¼‰è¿›è¡Œè®­ç»ƒã€‚å…¶ç­–ç•¥èƒ½åŠ›ä¸ç‰¹å®šçš„å‚è€ƒè¿åŠ¨åˆ†å¸ƒç´§å¯†è€¦åˆï¼Œ**æ³›åŒ–èƒ½åŠ›æœ‰é™**ï¼Œéš¾ä»¥ç»Ÿä¸€å»ºæ¨¡å¤šç§æŠ€èƒ½ã€‚\n*   **è¿‘æœŸè¿›å±•ï¼ˆé€šç”¨è·Ÿè¸ªå™¨ï¼‰**ï¼šå¦‚ `GMT`ã€`Any2Track`ã€`UniTracker` åŠæœ¬æ–‡å·¥ä½œï¼Œç›®æ ‡è½¬å‘è®­ç»ƒä¸€ä¸ª**é€šç”¨è·Ÿè¸ªå™¨**ï¼Œä½¿å…¶èƒ½å¤Ÿè·Ÿéšå¤šç§å¤šæ ·çš„å‚è€ƒè¿åŠ¨ã€‚è¿™éœ€è¦æ›´å¤§è§„æ¨¡ã€æ›´å¤šæ ·åŒ–çš„è¿åŠ¨æ•°æ®ä½œä¸ºç›‘ç£ä¿¡å·ã€‚\n    *   **æ”¹è¿›**ï¼šä»â€œä¸€æŠ€èƒ½ä¸€æ¨¡å‹â€èµ°å‘â€œå¤šæŠ€èƒ½ä¸€æ¨¡å‹â€ï¼Œæé«˜äº†ç­–ç•¥çš„é€šç”¨æ€§å’Œå®ç”¨æ€§ï¼Œä¸ºæ„å»ºç»Ÿä¸€çš„å…¨èº«æ§åˆ¶å™¨å¥ å®šäº†åŸºç¡€ã€‚\n\n#### **æ–¹å‘äºŒï¼šæ•°æ®è§„æ¨¡ä¸è´¨é‡çš„æƒè¡¡â€”â€”ä»â€œæµ·é‡â€åˆ°â€œç²¾é€‰â€**\n\n*   **æ•°æ®é©±åŠ¨è¶‹åŠ¿**ï¼šä¸ºäº†å­¦ä¹ é€šç”¨ç­–ç•¥ï¼Œæ”¶é›†å¤§è§„æ¨¡äººå½¢æœºå™¨äººè¿åŠ¨æ•°æ®æˆä¸ºå…³é”®ã€‚`OmniH2O`ã€`CLONE`ã€`TWIST2`ã€`ExBody2` ç­‰å·¥ä½œèšç„¦äº**å¯æ‰©å±•çš„äºº-äººå½¢æ•°æ®é‡‡é›†ä¸é¥æ“ä½œ**ç³»ç»Ÿï¼Œæä¾›äº†æ›´ä¸°å¯Œã€æ›´å¤šæ ·åŒ–çš„æ¼”ç¤ºæ•°æ®ã€‚\n*   **æ•°æ®æ•ˆç‡æŒ‘æˆ˜**ï¼š`SONIC` ç­‰å·¥ä½œå±•ç¤ºäº†ä½¿ç”¨è¶…å¤§è§„æ¨¡æ•°æ®ï¼ˆ>700å°æ—¶ï¼‰å’Œç®—åŠ›æ‰€èƒ½è¾¾åˆ°çš„æè‡´è‡ªç„¶è¿åŠ¨æ•ˆæœï¼Œä½†**é—¨æ§›æé«˜**ï¼Œä¸åˆ©äºè¿­ä»£ç ”ç©¶å’Œéƒ¨ç½²ã€‚\n*   **æœ¬æ–‡çš„æ”¹è¿›**ï¼šæœ¬æ–‡åå…¶é“è€Œè¡Œä¹‹ï¼Œå¼ºè°ƒ**æ•°æ®è´¨é‡ä¼˜äºå•çº¯æ•°é‡**ã€‚é€šè¿‡å¯¹åŸå§‹åŠ¨ä½œæ•æ‰æ•°æ®è¿›è¡Œä¸¥æ ¼çš„**è´¨é‡æ§åˆ¶ã€å»å†—ä½™å’Œç­›é€‰**ï¼Œæ„å»ºäº†ä¸€ä¸ªä»… **3.5å°æ—¶** ä½†é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„ç´§å‡‘æ•°æ®é›†ã€‚ç»“åˆå…¶åˆ›æ–°çš„ç½‘ç»œæ¶æ„ï¼Œè¯æ˜äº†åœ¨æœ‰é™æ•°æ®ä¸‹ä¹Ÿèƒ½è®­ç»ƒå‡ºå¼ºå¤§çš„é€šç”¨ç­–ç•¥ï¼Œ**æ˜¾è‘—é™ä½äº†æ•°æ®ä¸è®¡ç®—æˆæœ¬**ã€‚\n\n#### **æ–¹å‘ä¸‰ï¼šæ¶æ„åˆ›æ–°ä¸é²æ£’æ€§æå‡â€”â€”ä»â€œç›²ä»â€åˆ°â€œæ¡ä»¶åŒ–ç†è§£â€**\n\n*   **åŸºçº¿æ–¹æ³•å±€é™**ï¼š`GMT`ã€`Any2Track` ç­‰é€šç”¨è·Ÿè¸ªå™¨ï¼Œé€šå¸¸å°†å‚è€ƒå‘½ä»¤åºåˆ—ä½œä¸ºå›ºå®šè¾“å…¥ï¼Œç­–ç•¥å¹³ç­‰åœ°å¯¹å¾…æ‰€æœ‰æ—¶é—´æ­¥çš„å‚è€ƒä¿¡å·ã€‚å½“å‚è€ƒå‘½ä»¤å­˜åœ¨å™ªå£°ã€ä¸ä¸€è‡´æ¥è§¦æˆ–ä¼ªå½±æ—¶ï¼Œè¿™ç§â€œç›²ä»â€å®¹æ˜“åœ¨é—­ç¯ä¸­æ”¾å¤§é”™è¯¯ï¼Œå¯¼è‡´**è·Ÿè¸ªç²¾åº¦ä¸‹é™å’Œå¤±ç¨³**ã€‚\n*   **æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼ˆåŠ¨æ€æ¡ä»¶åŒ–å‘½ä»¤èšåˆï¼‰**ï¼š\n    1.  **å› æœå†å²ç¼–ç å™¨**ï¼šæå–è¿‘æœŸæœ¬ä½“æ„ŸçŸ¥çš„åŠ¨æ€åµŒå…¥ï¼Œä½œä¸ºå¯¹å½“å‰æœºå™¨äººç‰©ç†çŠ¶æ€çš„ç´§å‡‘è¡¨ç¤ºã€‚\n    2.  **å¤šå¤´äº¤å‰æ³¨æ„åŠ›å‘½ä»¤ç¼–ç å™¨**ï¼šä»¥åŠ¨æ€åµŒå…¥ä¸º **Query**ï¼Œå¯¹ä¸Šä¸‹æ–‡å‘½ä»¤çª—å£è¿›è¡Œ **Key-Value** æ£€ç´¢ä¸èšåˆã€‚è¿™ä½¿å¾—ç­–ç•¥èƒ½å¤Ÿæ ¹æ®**å½“å‰çš„ç‰©ç†å¯è¡Œæ€§**ï¼Œè‡ªé€‚åº”åœ°**å…³æ³¨æˆ–å¿½ç•¥**å‚è€ƒå‘½ä»¤çš„ä¸åŒéƒ¨åˆ†ã€‚\n    *   **æ”¹è¿›**ï¼šå¼•å…¥äº†å¯¹å‚è€ƒå‘½ä»¤çš„â€œç†è§£â€å’Œâ€œè¿‡æ»¤â€èƒ½åŠ›ã€‚ç­–ç•¥ä¸å†æ˜¯ç®€å•åœ°è·Ÿè¸ªå‚è€ƒï¼Œè€Œæ˜¯åœ¨ç‰©ç†çº¦æŸä¸‹**æœ‰é€‰æ‹©åœ°éµä»**å‚è€ƒï¼Œä»è€Œå¯¹å™ªå£°å’Œä¼ªå½±è¡¨ç°å‡ºæå¼ºçš„**é²æ£’æ€§**ï¼ˆå®éªŒæ˜¾ç¤ºå¯å®¹å¿é«˜è¾¾1500%çš„å™ªå£°ï¼‰ã€‚\n\n#### **æ–¹å‘å››ï¼šå®‰å…¨ä¸æŒç»­è¿è¡Œâ€”â€”ä»â€œå­¤ç«‹æ§åˆ¶â€åˆ°â€œé›†æˆæ¢å¤â€**\n\n*   **ä¼ ç»Ÿå±€é™**ï¼šè®¸å¤šå…¨èº«æ§åˆ¶ç­–ç•¥ä¸“æ³¨äºè¿åŠ¨è·Ÿè¸ªæœ¬èº«ï¼Œ**è·Œå€’æ¢å¤é€šå¸¸ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å—æˆ–äº‹åè¡¥æ•‘æªæ–½**ï¼ˆå¦‚ `Learning getting-up policies`ã€`Learning humanoid standing-up control`ï¼‰ã€‚è¿™é™åˆ¶äº†ç³»ç»Ÿçš„æ•´ä½“é²æ£’æ€§å’Œä»»åŠ¡è¿ç»­æ€§ï¼Œä¸€æ—¦è·Œå€’éœ€è¦äººå·¥å¹²é¢„ã€‚\n*   **æœ¬æ–‡çš„é›†æˆæ–¹æ¡ˆ**ï¼š\n    *   **è®­ç»ƒé›†æˆ**ï¼šåœ¨è®­ç»ƒä¸­ï¼Œä»¥ä¸€å®šæ¦‚ç‡å°†ç¯å¢ƒåˆå§‹åŒ–ä¸ºéšæœºä¸ç¨³å®šå§¿æ€ï¼Œå¹¶æ–½åŠ é€€ç«çš„å‘ä¸Šè¾…åŠ©åŠ›ï¼Œè®©**å•ä¸€ç­–ç•¥åŒæ—¶å­¦ä¹ è¿åŠ¨è·Ÿè¸ªå’Œè‡ªä¸»ç«™èµ·**ã€‚\n    *   **æ”¹è¿›**ï¼š\n        1.  **æå‡å®‰å…¨æ€§**ï¼šç­–ç•¥å…·å¤‡å†…åœ¨çš„è·Œå€’æ¢å¤èƒ½åŠ›ï¼Œæ”¯æŒ**é•¿æ—¶ç¨‹ã€æ— éœ€äººå·¥é‡ç½®**çš„è¿ç»­è¿è¡Œã€‚\n        2.  **é—´æ¥æå‡è·Ÿè¸ªæ€§èƒ½**ï¼šæ¢å¤è¿‡ç¨‹è®©ç­–ç•¥ç»å†äº†æ›´å¹¿æ³›çš„æ¥è§¦çŠ¶æ€å’ŒåŠ¨åŠ›å­¦åˆ†å¸ƒï¼Œåè€Œ**å¢å¼ºäº†å…¶åœ¨å¤æ‚æ¥è§¦è¿åŠ¨ï¼ˆå¦‚åœ°é¢äº¤äº’åŠ¨ä½œï¼‰ä¸­çš„è·Ÿè¸ªé²æ£’æ€§**ï¼ˆæ¶ˆèå®éªŒè¯å®ï¼‰ã€‚\n\n#### **æ–¹å‘äº”ï¼šåº”ç”¨åœºæ™¯æ‹“å±•â€”â€”ä»â€œç¦»çº¿è·Ÿè¸ªâ€åˆ°â€œåœ¨çº¿äº¤äº’â€**\n\n*   **æ—©æœŸéªŒè¯**ï¼šå¤šåœ¨ä»¿çœŸæˆ–å—æ§ç¯å¢ƒä¸­è·Ÿè¸ªå¹²å‡€çš„é¢„å½•åŠ¨ä½œæ•æ‰æ•°æ®ã€‚\n*   **æœ¬æ–‡å±•ç¤ºçš„æ³›åŒ–èƒ½åŠ›**ï¼š\n    *   **è¾“å…¥æºæ³›åŒ–**ï¼šæˆåŠŸè·Ÿè¸ªæ¥è‡ª**è§†é¢‘ä¼°è®¡**ï¼ˆå™ªå£°å¤§ã€æœ‰åˆ†å¸ƒåç§»ï¼‰å’Œ**å®æ—¶å…¨èº«é¥æ“ä½œ**ï¼ˆå­˜åœ¨å»¶è¿Ÿã€æŠ–åŠ¨ï¼‰çš„å‚è€ƒå‘½ä»¤ã€‚\n    *   **ä»»åŠ¡é›†æˆ**ï¼šä½œä¸ºåº•å±‚æ§åˆ¶å™¨ï¼Œä¸ä¸Šæ¸¸æ¨¡å—ï¼ˆå¦‚åŸºäºæ¸¸æˆæ‰‹æŸ„çš„**è¿åŠ¨åˆæˆæ–¹æ³•**ï¼‰é›†æˆï¼Œå®ç°**é¥æ“ä½œé©±åŠ¨**å’Œ**äº¤äº’å¼é£æ ¼åŒ–è¿åŠ¨**ï¼Œå±•ç¤ºäº†å…¶åœ¨**å®é™…ä¸‹æ¸¸ä»»åŠ¡**ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚\n\n### ä¸‰ã€ æ€»ç»“\n\næœ¬æ–‡ä»£è¡¨äº†ä¸€ä¸ªæ¸…æ™°çš„æ¼”è¿›æ–¹å‘ï¼šåœ¨è¿½æ±‚é€šç”¨äººå½¢æ§åˆ¶çš„è¿‡ç¨‹ä¸­ï¼Œä»ä¾èµ–**æµ·é‡æ•°æ®**å’Œ**å­¤ç«‹æ¨¡å—**ï¼Œè½¬å‘ä¾é **ç²¾å·§çš„ç®—æ³•æ¶æ„**ï¼ˆåŠ¨æ€æ¡ä»¶åŒ–æ³¨æ„åŠ›ï¼‰å’Œ**ä¸€ä½“åŒ–çš„è®­ç»ƒèŒƒå¼**ï¼ˆé›†æˆæ¢å¤è¯¾ç¨‹ï¼‰ï¼Œåœ¨**ä¿è¯æ•°æ®æ•ˆç‡çš„åŒæ—¶ï¼Œæå¤§åœ°æå‡äº†ç­–ç•¥çš„é²æ£’æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œå®é™…éƒ¨ç½²çš„å®ç”¨æ€§**ã€‚å®ƒä¸ºè§£å†³â€œå¦‚ä½•è®©æœºå™¨äººåƒäººä¸€æ ·çµæ´»ã€ç¨³å®šåœ°è¿åŠ¨â€è¿™ä¸€æ ¸å¿ƒé—®é¢˜ï¼Œæä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”å¼ºå¤§çš„æ¡†æ¶ã€‚æœªæ¥çš„å·¥ä½œé¢„è®¡å°†åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥ç»“åˆå…¨å±€å®šä½ã€æ›´é«˜çº§åˆ«çš„è¿åŠ¨ç”Ÿæˆä¸è§„åˆ’ï¼Œä»¥æ”¯æŒæ›´å¤æ‚çš„é•¿æœŸä»»åŠ¡ã€‚",
            "timestamp": "2026-02-03T15:55:25.491885",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä»”ç»†åˆ†æè¿™ç¯‡é¢˜ä¸ºã€ŠRobust and Generalized Humanoid Motion Trackingã€‹çš„è®ºæ–‡ï¼Œå¹¶é‡ç‚¹æ¢³ç†å…¶ä¸å‰ä½œçš„åŒºåˆ«ä¸æ”¹è¿›ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒè¦ç‚¹åˆ†æ\n\næœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äº**äººå½¢æœºå™¨äººå…¨èº«è¿åŠ¨è·Ÿè¸ª**çš„é²æ£’ä¸”é€šç”¨çš„å­¦ä¹ æ¡†æ¶ã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯è§£å†³ç°æœ‰æ–¹æ³•åœ¨**æ•°æ®æ•ˆç‡ã€é—­ç¯ç¨³å®šæ€§ã€å¯¹å™ªå£°å‚è€ƒçš„é²æ£’æ€§ä»¥åŠè·Œå€’æ¢å¤èƒ½åŠ›**æ–¹é¢çš„ä¸è¶³ã€‚\n\n**ä¸»è¦æŠ€æœ¯åˆ›æ–°**ï¼š\n1.  **åŠ¨æ€æ¡ä»¶åŒ–æŒ‡ä»¤èšåˆæ¡†æ¶**ï¼šåˆ©ç”¨å› æœæ—¶åºç¼–ç å™¨æ€»ç»“è¿‘æœŸæœ¬ä½“æ„ŸçŸ¥ä¿¡æ¯ï¼Œå¹¶é€šè¿‡å¤šå¤´äº¤å‰æ³¨æ„åŠ›ç¼–ç å™¨ï¼Œæ ¹æ®å½“å‰åŠ¨æ€æœ‰é€‰æ‹©åœ°èšåˆä¸Šä¸‹æ–‡æŒ‡ä»¤çª—å£ã€‚è¿™ä½¿å¾—ç­–ç•¥èƒ½å¤Ÿâ€œè¿‡æ»¤â€å‚è€ƒè¿åŠ¨ä¸­çš„å™ªå£°å’Œä¸ä¸€è‡´éƒ¨åˆ†ã€‚\n2.  **é›†æˆå¼è·Œå€’æ¢å¤è¯¾ç¨‹**ï¼šåœ¨è®­ç»ƒä¸­å¼•å…¥éšæœºä¸ç¨³å®šåˆå§‹åŒ–å’Œé€€ç«å¼å‘ä¸Šè¾…åŠ©åŠ›ï¼Œä½¿å•ä¸€ç­–ç•¥åŒæ—¶å­¦ä¼šè¿åŠ¨è·Ÿè¸ªå’Œè‡ªä¸»ç«™èµ·ï¼Œæ˜¾è‘—æå‡äº†é²æ£’æ€§å’ŒæŠ—å¹²æ‰°èƒ½åŠ›ã€‚\n3.  **é«˜æ•ˆçš„æ•°æ®ä¸è®­ç»ƒ**ï¼šä»…éœ€çº¦**3.5å°æ—¶**çš„é«˜è´¨é‡è¿åŠ¨æ•°æ®ï¼Œæ”¯æŒ**ç«¯åˆ°ç«¯çš„å•é˜¶æ®µè®­ç»ƒ**ï¼ˆæ— éœ€è’¸é¦æˆ–å¤šé˜¶æ®µè®­ç»ƒï¼‰ï¼Œé™ä½äº†ç ”ç©¶å’Œéƒ¨ç½²çš„é—¨æ§›ã€‚\n\n**å®é™…ä»·å€¼**ï¼š\n- **å¼ºæ³›åŒ–æ€§**ï¼šåœ¨æœªè§è¿‡çš„è¿åŠ¨ï¼ˆå¦‚è§†é¢‘è¡ç”ŸåŠ¨ä½œï¼‰ã€å¸¦å™ªå£°çš„å®æ—¶å…¨èº«é¥æ“ä½œæŒ‡ä»¤ä¸Šå®ç°é›¶æ ·æœ¬è¿ç§»ã€‚\n- **é«˜é²æ£’æ€§**ï¼šåœ¨ä»¿çœŸå’ŒçœŸå®æœºå™¨äººï¼ˆUnitree G1ï¼‰ä¸Šå‡èƒ½ç¨³å®šæ‰§è¡Œé«˜åº¦åŠ¨æ€ã€æ¥è§¦ä¸°å¯Œçš„å¤æ‚åŠ¨ä½œï¼Œå¹¶èƒ½ä»è·Œå€’ä¸­è‡ªä¸»æ¢å¤ã€‚\n- **åº”ç”¨æ½œåŠ›**ï¼šä¸ºå®æ—¶é¥æ“ä½œã€æ¸¸æˆæ‰‹æŸ„é©±åŠ¨æ­¥æ€ç­‰ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†å¯é çš„åº•å±‚å…¨èº«æ§åˆ¶å™¨ã€‚\n\n---\n\n## ä¸å‰ä½œçš„å¯¹æ¯”åˆ†æ\n\næœ¬æ–‡å±äºâ€œ**åŸºäºå­¦ä¹ çš„é€šç”¨äººå½¢å…¨èº«è¿åŠ¨è·Ÿè¸ª**â€è¿™ä¸€ç ”ç©¶æ–¹å‘ã€‚ä»¥ä¸‹æ˜¯å…¶ä¸»è¦å‰ä½œåŠæœ¬æ–‡ä¸ä¹‹çš„åŒºåˆ«å’Œæ”¹è¿›ï¼š\n\n| å‰ä½œåç§° | æ ¸å¿ƒæ–¹æ³• / è´¡çŒ® | æœ¬æ–‡ä¸ä¹‹çš„ä¸»è¦åŒºåˆ«ä¸æ”¹è¿› |\n| :--- | :--- | :--- |\n| **GMT (General Motion Tracking)** [3] | æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„è¿åŠ¨è·Ÿè¸ªæ¡†æ¶ï¼Œä½¿ç”¨å¤§è§„æ¨¡æ•°æ®è®­ç»ƒï¼Œæ—¨åœ¨è·Ÿè¸ªå¤šæ ·åŒ–çš„è¿åŠ¨ã€‚ | **1. æ¶æ„åˆ›æ–°**ï¼šGMTæœªæ˜ç¡®é‡‡ç”¨**åŠ¨æ€æ¡ä»¶åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶**æ¥èšåˆæŒ‡ä»¤ã€‚æœ¬æ–‡çš„**åŠ¨æ€æ¡ä»¶åŒ–äº¤å‰æ³¨æ„åŠ›æŒ‡ä»¤ç¼–ç å™¨**æ˜¯å…³é”®åŒºåˆ«ï¼Œå®ƒä½¿ç­–ç•¥èƒ½æ ¹æ®å½“å‰æœºå™¨äººçŠ¶æ€è‡ªé€‚åº”åœ°æƒè¡¡å‚è€ƒæŒ‡ä»¤ï¼Œä»è€Œåœ¨**æŒ‡ä»¤å«å™ªå£°æ—¶æ›´é²æ£’**ï¼ˆå¦‚è¡¨Iã€å›¾3æ‰€ç¤ºï¼‰ã€‚<br>**2. é›†æˆæ¢å¤**ï¼šGMTä¸»è¦å…³æ³¨è·Ÿè¸ªï¼Œæœªå°†**è·Œå€’æ¢å¤**ä½œä¸ºæ ¸å¿ƒè®­ç»ƒç›®æ ‡é›†æˆåˆ°ç­–ç•¥ä¸­ã€‚æœ¬æ–‡é€šè¿‡æ¢å¤è¯¾ç¨‹æ˜¾è‘—æå‡äº†åœ¨æ¥è§¦å¤æ‚åŠ¨ä½œï¼ˆå¦‚åœ°é¢äº¤äº’ï¼‰ä¸Šçš„æˆåŠŸç‡å’Œç¨³å®šæ€§ï¼ˆè¡¨I-cï¼‰ã€‚<br>**3. æ•°æ®æ•ˆç‡**ï¼šæœ¬æ–‡å¼ºè°ƒä½¿ç”¨**å°è€Œç²¾çš„é«˜è´¨é‡æ•°æ®é›†**ï¼ˆ3.5å°æ—¶ï¼‰ï¼Œå¹¶é€šè¿‡æ¶æ„æ”¹è¿›å®ç°å¼ºæ€§èƒ½ï¼Œä¸GMTç­‰ä¾èµ–æ›´å¤§è§„æ¨¡æ•°æ®çš„æ–¹æ³•å½¢æˆå¯¹æ¯”ã€‚ |\n| **Any2Track** [31] | æ—¨åœ¨â€œåœ¨ä»»ä½•å¹²æ‰°ä¸‹è·Ÿè¸ªä»»ä½•è¿åŠ¨â€ï¼Œä¾§é‡äºå¯¹æ‰°åŠ¨å’Œä¸åŒè¿åŠ¨é£æ ¼çš„é²æ£’æ€§ã€‚ | **1. å™ªå£°å¤„ç†æœºåˆ¶**ï¼šAny2Trackçš„é²æ£’æ€§å¯èƒ½æ›´å¤šä¾èµ–äºæ•°æ®å¢å¼ºæˆ–ç­–ç•¥å®¹é‡ã€‚æœ¬æ–‡æ˜ç¡®è®¾è®¡äº†**ä»¥åŠ¨æ€ä¸ºæ¡ä»¶çš„æŒ‡ä»¤è¿‡æ»¤æœºåˆ¶**ï¼Œåœ¨**ç³»ç»Ÿæ€§è¯„ä¼°æŒ‡ä»¤å™ªå£°é²æ£’æ€§**çš„å®éªŒä¸­ï¼ˆå›¾3ï¼‰ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨é«˜å™ªå£°æ°´å¹³ä¸‹çš„æ€§èƒ½ä¸‹é™è¿œå°äºAny2Trackï¼Œè¯æ˜äº†è¯¥æ¶æ„æ”¹è¿›çš„æœ‰æ•ˆæ€§ã€‚<br>**2. æ¢å¤èƒ½åŠ›**ï¼šåŒGMTï¼ŒAny2Trackä¹Ÿæœªå¼ºè°ƒé›†æˆè·Œå€’æ¢å¤ã€‚æœ¬æ–‡çš„æ¢å¤è¯¾ç¨‹æ˜¯æå‡é•¿æœŸéƒ¨ç½²å®‰å…¨æ€§å’Œè¿ç»­æ€§çš„å…³é”®è¡¥å……ã€‚ |\n| **SONIC** [15] | â€œè¶…å¤§åŒ–â€è¿åŠ¨è·Ÿè¸ªï¼Œä½¿ç”¨äº†è¶…è¿‡**700å°æ—¶**çš„è¿åŠ¨æ•°æ®å’Œå¤§é‡ç®—åŠ›ï¼Œè¿½æ±‚æè‡´çš„è¿åŠ¨è¦†ç›–èŒƒå›´å’Œè‡ªç„¶åº¦ã€‚ | **1. æ•°æ®ä¸è®¡ç®—æ•ˆç‡**ï¼šæœ¬æ–‡æœ€ç›´æ¥çš„å¯¹æ¯”å°±æ˜¯**æ•°æ®æ•ˆç‡**ã€‚SONICä»£è¡¨äº†â€œå¤§åŠ›å‡ºå¥‡è¿¹â€çš„è·¯å¾„ï¼Œè€Œæœ¬æ–‡è¯æ˜äº†é€šè¿‡**æ›´å¥½çš„ç®—æ³•è®¾è®¡ï¼ˆåŠ¨æ€æ¡ä»¶åŒ–æ³¨æ„åŠ›ï¼‰å’Œé«˜è´¨é‡æ•°æ®ç­›é€‰**ï¼Œä»…ç”¨**çº¦0.5%çš„æ•°æ®é‡**ï¼ˆ3.5 vs 700+å°æ—¶ï¼‰ä¹Ÿèƒ½å®ç°å¼ºå¤§çš„æ³›åŒ–å’Œé²æ£’è·Ÿè¸ªæ€§èƒ½ï¼Œ**å¤§å¹…é™ä½äº†è¿­ä»£ç ”ç©¶å’Œå®é™…éƒ¨ç½²çš„é—¨æ§›**ã€‚<br>**2. è®­ç»ƒæµç¨‹**ï¼šSONICå¯èƒ½æ¶‰åŠå¤æ‚çš„å¤šé˜¶æ®µè®­ç»ƒæˆ–è’¸é¦ã€‚æœ¬æ–‡åšæŒ**å•é˜¶æ®µç«¯åˆ°ç«¯è®­ç»ƒ**ï¼Œæµç¨‹æ›´ç®€æ´ã€‚ |\n| **Learning getting-up policies** [9] / **Learning humanoid standing-up** [11] | ä¸“é—¨ç ”ç©¶äººå½¢æœºå™¨äººçš„**ç«™èµ·/æ¢å¤ç­–ç•¥**ï¼Œé€šå¸¸æ˜¯ç‹¬ç«‹äºä¸»è¿åŠ¨è·Ÿè¸ªç­–ç•¥çš„æ¨¡å—ã€‚ | **1. ç»Ÿä¸€ç­–ç•¥**ï¼šå‰ä½œå°†â€œè·Ÿè¸ªâ€å’Œâ€œæ¢å¤â€ä½œä¸ºä¸¤ä¸ªç‹¬ç«‹çš„é—®é¢˜æˆ–æ¨¡å—ã€‚æœ¬æ–‡çš„æ ¸å¿ƒæ”¹è¿›ä¹‹ä¸€æ˜¯**å°†è·Œå€’æ¢å¤è¯¾ç¨‹é›†æˆåˆ°ä¸»è¿åŠ¨è·Ÿè¸ªç­–ç•¥çš„åŒä¸€è®­ç»ƒæ¡†æ¶ä¸­**ï¼Œå­¦å¾—ä¸€ä¸ª**ç»Ÿä¸€çš„ç­–ç•¥**ã€‚è¿™é¿å…äº†æ¨¡å—é—´åˆ‡æ¢çš„å¤æ‚æ€§ï¼Œå¹¶è®©æ¢å¤ç»éªŒåå“ºæå‡äº†è·Ÿè¸ªç­–ç•¥åœ¨æ¥è§¦ä¸°å¯Œåœºæ™¯ä¸‹çš„é²æ£’æ€§ï¼ˆè¡¨I-cï¼‰ã€‚<br>**2. è¯¾ç¨‹è®¾è®¡**ï¼šæœ¬æ–‡æå‡ºäº†å…·ä½“çš„æ¢å¤è¯¾ç¨‹æŠ€æœ¯ï¼š**éšæœºä¸ç¨³å®šåˆå§‹åŒ–**å’Œ**é€€ç«å¼å‘ä¸Šè¾…åŠ©åŠ›**ï¼Œåœ¨è®­ç»ƒä¸­é€æ­¥å¼•å¯¼ç­–ç•¥å­¦ä¼šè‡ªä¸»ç«™èµ·ã€‚ |\n\n## æ€»ç»“ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ”¹è¿›\n\næœ¬æ–‡å¹¶éåœ¨å•ä¸€ç»´åº¦ä¸Šè¶…è¶Šå‰ä½œï¼Œè€Œæ˜¯è¿›è¡Œäº†ä¸€æ¬¡**ç³»ç»Ÿæ€§çš„æ•´åˆä¸åˆ›æ–°**ï¼š\n\n1.  **ç®—æ³•å±‚é¢**ï¼šå¼•å…¥äº†**åŠ¨æ€æ¡ä»¶åŒ–äº¤å‰æ³¨æ„åŠ›**è¿™ä¸€æ–°é¢–çš„æ¶æ„è®¾è®¡ï¼Œä½¿ç­–ç•¥å…·å¤‡äº†å¯¹å‚è€ƒæŒ‡ä»¤è¿›è¡Œâ€œæ™ºèƒ½æ»¤æ³¢â€çš„èƒ½åŠ›ï¼Œä»æ ¹æœ¬ä¸Šæå‡äº†å¯¹ä¸å®Œç¾ã€å«å™ªå£°å‚è€ƒçš„**é²æ£’æ€§**ã€‚\n2.  **è®­ç»ƒèŒƒå¼å±‚é¢**ï¼šåˆ›é€ æ€§åœ°å°†**è·Œå€’æ¢å¤è¯¾ç¨‹**ä¸**ä¸»è¿åŠ¨è·Ÿè¸ªä»»åŠ¡**åœ¨**å•é˜¶æ®µã€ç«¯åˆ°ç«¯**çš„è®­ç»ƒä¸­ç›¸ç»“åˆã€‚è¿™ä¸ä»…èµ‹äºˆäº†ç­–ç•¥å…³é”®çš„â€œè‡ªæ„ˆâ€èƒ½åŠ›ï¼Œè¿˜é€šè¿‡æš´éœ²æ›´å¤šä¸ç¨³å®šå’Œæ¥è§¦ä¸°å¯Œçš„çŠ¶æ€ï¼Œ**é—´æ¥æå‡äº†ä¸»ä»»åŠ¡åœ¨åŠ¨æ€å’Œæ¥è§¦åœºæ™¯ä¸‹çš„æ€§èƒ½**ã€‚\n3.  **æ•ˆç‡å±‚é¢**ï¼šé€šè¿‡ä¸Šè¿°ç®—æ³•æ”¹è¿›ï¼Œç»“åˆå¯¹è®­ç»ƒæ•°æ®è´¨é‡çš„é‡è§†ï¼ˆå»å†—ä½™ã€æè´¨é‡ï¼‰ï¼Œå®ç°äº†**ä»¥æå°çš„æ•°æ®é‡ï¼ˆ3.5å°æ—¶ï¼‰è®­ç»ƒå‡ºé«˜æ€§èƒ½é€šç”¨è·Ÿè¸ªå™¨**çš„ç›®æ ‡ï¼Œåœ¨æ•°æ®ä¸è®¡ç®—æ•ˆç‡ä¸Šç›¸å¯¹äºSONICç­‰å¤§å‹æ–¹æ³•æœ‰æ•°é‡çº§ä¼˜åŠ¿ã€‚\n\nå› æ­¤ï¼Œæœ¬æ–‡çš„è´¡çŒ®åœ¨äºæä¾›äº†ä¸€ç§**æ›´é²æ£’ã€æ›´é€šç”¨ã€æ›´é«˜æ•ˆä¸”æ›´å®‰å…¨ï¼ˆé›†æˆæ¢å¤ï¼‰** çš„äººå½¢å…¨èº«è¿åŠ¨è·Ÿè¸ªè§£å†³æ–¹æ¡ˆï¼Œåœ¨ç®—æ³•è®¾è®¡ã€è®­ç»ƒæ¡†æ¶å’Œå·¥ç¨‹å®è·µä¸Šéƒ½åšå‡ºäº†æ˜ç¡®çš„æ¨è¿›ã€‚",
            "timestamp": "2026-02-03T15:56:16.877331",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†å¯¹è¿™ç¯‡é¢˜ä¸ºã€ŠRobust and Generalized Humanoid Motion Trackingã€‹çš„è®ºæ–‡è¿›è¡Œæ·±å…¥åˆ†æï¼Œå¹¶è¯¦ç»†é˜è¿°å…¶æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒè¦ç‚¹åˆ†æ\n\næœ¬æ–‡æ—¨åœ¨è§£å†³**äººå½¢æœºå™¨äººå…¨èº«è¿åŠ¨è·Ÿè¸ª**ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼šåœ¨å‚è€ƒè¿åŠ¨æ•°æ®å­˜åœ¨å™ªå£°ã€ä¸ä¸€è‡´æ€§ï¼Œä¸”é—­ç¯æ‰§è¡Œä¼šæ”¾å¤§å±€éƒ¨ç¼ºé™·çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•å®ç°ä¸€ä¸ª**é²æ£’ã€é€šç”¨ã€æ•°æ®é«˜æ•ˆ**çš„æ§åˆ¶å™¨ã€‚è®ºæ–‡çš„ä¸»è¦è´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ä¸ª**åŠ¨æ€æ¡ä»¶åŒ–çš„æŒ‡ä»¤èšåˆæ¡†æ¶**ï¼Œå¹¶**å°†è·Œå€’æ¢å¤è®­ç»ƒé›†æˆåˆ°ç»Ÿä¸€çš„ç­–ç•¥å­¦ä¹ æµç¨‹ä¸­**ã€‚\n\n## å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è§£\n\nä¸‹è¡¨è¯¦ç»†è¯´æ˜äº†è®ºæ–‡æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€å…¶æ•°å­¦åŸç†ã€å…·ä½“å‚æ•°åŠè®¾è®¡åŠ¨æœºã€‚\n\n| æ–¹æ³•æ¨¡å— | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°ä¸è®¾è®¡è¯´æ˜ |\n| :--- | :--- | :--- |\n| **1. åŠ¨æ€æ¡ä»¶åŒ–æŒ‡ä»¤èšåˆæ¡†æ¶** | **æ ¸å¿ƒæ€æƒ³**ï¼šåˆ©ç”¨å½“å‰æœºå™¨äººæœ¬ä½“æ„ŸçŸ¥çš„åŠ¨æ€ä¿¡æ¯ä½œä¸ºâ€œæŸ¥è¯¢â€ï¼Œå¯¹ä¸Šä¸‹æ–‡æŒ‡ä»¤çª—å£è¿›è¡Œ**é€‰æ‹©æ€§èšåˆ**ï¼Œè€Œéå¹³ç­‰å¯¹å¾…æ‰€æœ‰å‚è€ƒä¿¡å·ã€‚è¿™é€šè¿‡ä¸€ä¸ª**å› æœæ—¶åºç¼–ç å™¨**å’Œä¸€ä¸ª**å¤šå¤´äº¤å‰æ³¨æ„åŠ›æŒ‡ä»¤ç¼–ç å™¨**å®ç°ã€‚ | **æ•´ä½“æ¶æ„**ï¼šè§å›¾2ã€‚å†å²ç¼–ç å™¨è¾“å‡ºåŠ¨æ€åµŒå…¥ $\\mathbf{h}_t$ï¼Œä½œä¸ºæŒ‡ä»¤ç¼–ç å™¨çš„æŸ¥è¯¢å‘é‡ $\\mathbf{q}_t$ï¼ŒæŒ‡å¯¼å¯¹æŒ‡ä»¤çª—å£ $\\mathbf{g}_{t-L:t+L}$ çš„èšåˆã€‚ |\n| **1.1 å†å²ç¼–ç å™¨** | **ç›®æ ‡**ï¼šä»æœ€è¿‘çš„æœ¬ä½“æ„ŸçŸ¥å†å²ä¸­æå–ç´§å‡‘çš„åŠ¨æ€è¡¨ç¤º $\\mathbf{h}_t$ã€‚<br>**æµç¨‹**ï¼š<br>1. **åµŒå…¥**ï¼šå°† $K+1$ æ­¥å†å²è§‚æµ‹ $\\mathbf{o}_{t-K:t}$ é€šè¿‡ä¸€ä¸ª2å±‚MLPæ˜ å°„ä¸ºåµŒå…¥åºåˆ— $\\mathbf{E}_{t-K:t} \\in \\mathbb{R}^{(K+1) \\times n_{\\text{embd}}}$ã€‚<br>2. **ä½ç½®ç¼–ç **ï¼šåŠ å…¥æ­£å¼¦ä½ç½®ç¼–ç  $\\mathbf{P}$: $\\tilde{\\mathbf{E}} = \\mathbf{E} + \\mathbf{P}$ã€‚<br>3. **å› æœTransformer**ï¼šä½¿ç”¨ä¸€ä¸ªè½»é‡çº§ã€å¸¦å› æœæ©ç çš„å•å±‚Transformerå—å¤„ç†åºåˆ—ã€‚å› æœæ©ç ç¡®ä¿æ—¶é—´ $\\tau$ çš„æ ‡è®°åªèƒ½å…³æ³¨ $\\le \\tau$ çš„æ ‡è®°ã€‚<br>   $\\mathbf{H}^{(1)} = \\mathbf{H}^{(0)} + \\text{MHA}(\\text{LN}(\\mathbf{H}^{(0)}))$<br>   $\\mathbf{H}^{(2)} = \\mathbf{H}^{(1)} + \\text{MLP}(\\text{LN}(\\mathbf{H}^{(1)}))$<br>   $\\bar{\\mathbf{H}} = \\text{LN}(\\mathbf{H}^{(2)})$<br>4. **æ± åŒ–**ï¼šå¯¹ $\\bar{\\mathbf{H}}$ æ²¿æ—¶é—´ç»´åº¦è¿›è¡Œ**é€å…ƒç´ æœ€å¤§æ± åŒ–**ï¼Œå¾—åˆ°æœ€ç»ˆåŠ¨æ€åµŒå…¥ $\\mathbf{h}_t$:<br>   $\\mathbf{h}_t[j] = \\max_{\\tau \\in \\{t-K, ..., t\\}} \\bar{\\mathbf{H}}_\\tau[j], \\quad j=1,...,n_{\\text{embd}}$ | - **å†å²é•¿åº¦** $K = 9$ï¼ˆå³10æ­¥è§‚æµ‹ï¼Œæ¯æ­¥33msï¼Œå…±~0.33ç§’ï¼‰ã€‚<br>- **åµŒå…¥ç»´åº¦** $n_{\\text{embd}} = 128$ã€‚<br>- **è§‚æµ‹ç»´åº¦** $\\mathbf{o}_t \\in \\mathbb{R}^{93}$ï¼ˆåŒ…å«é‡åŠ›æŠ•å½±ã€è§’é€Ÿåº¦ã€å…³èŠ‚ä½ç½®/é€Ÿåº¦ã€ä¸Šä¸€åŠ¨ä½œç­‰ï¼‰ã€‚<br>- **Transformer**ï¼šå•å±‚ï¼Œå¤šå¤´æ³¨æ„åŠ›å¤´æ•°æœªæ˜ç¡®ç»™å‡ºï¼Œä½†ä¸ºæ ‡å‡†é…ç½®ã€‚<br>- **æ± åŒ–é€‰æ‹©**ï¼šæœ€å¤§æ± åŒ–èƒ½æ•æ‰åŠ¨æ€å†å²ä¸­çš„æ˜¾è‘—ç‰¹å¾ã€‚ |\n| **1.2 æŒ‡ä»¤ç¼–ç å™¨** | **ç›®æ ‡**ï¼šä»¥åŠ¨æ€åµŒå…¥ $\\mathbf{h}_t$ ä¸ºæ¡ä»¶ï¼Œèšåˆä¸€ä¸ªæœªæ¥-è¿‡å»æŒ‡ä»¤çª—å£ï¼Œè¾“å‡ºæ¡ä»¶åŒ–æŒ‡ä»¤åµŒå…¥ $\\mathbf{u}_t$ã€‚<br>**æµç¨‹**ï¼š<br>1. **æŸ¥è¯¢ç”Ÿæˆ**ï¼šåŠ¨æ€åµŒå…¥ $\\mathbf{h}_t$ é€šè¿‡ä¸€ä¸ª2å±‚MLPç”ŸæˆæŸ¥è¯¢å‘é‡ $\\mathbf{q}_t$: $\\mathbf{q}_t = \\text{MLP}_{\\text{dyn}}(\\mathbf{h}_t)$ã€‚<br>2. **æŒ‡ä»¤åµŒå…¥**ï¼šæŒ‡ä»¤çª—å£ $\\mathbf{g}_{t-L:t+L}$ é€šè¿‡å¦ä¸€ä¸ª2å±‚MLPæ˜ å°„ä¸ºæ ‡è®°åºåˆ—ï¼Œå¹¶åŠ å…¥ä½ç½®ç¼–ç  $\\mathbf{P}^{\\text{cmd}}$: $\\tilde{\\mathbf{Z}} = \\text{MLP}_{\\text{cmd}}(\\mathbf{g}_{t-L:t+L}) + \\mathbf{P}^{\\text{cmd}}$ã€‚<br>3. **åŠ¨æ€æ¡ä»¶åŒ–äº¤å‰æ³¨æ„åŠ›**ï¼šä½¿ç”¨ä¸€ä¸ªå•Transformerå—ï¼Œä»¥ $\\mathbf{q}_t$ ä¸ºæŸ¥è¯¢ï¼Œä»¥ $\\tilde{\\mathbf{Z}}$ ä¸ºé”®å’Œå€¼ï¼Œè¿›è¡Œäº¤å‰æ³¨æ„åŠ›èšåˆã€‚<br>   $\\mathbf{s}^{(1)} = \\mathbf{q}_t + \\text{MHA}(\\text{LN}(\\mathbf{q}_t), \\tilde{\\mathbf{Z}})$<br>   $\\mathbf{s}^{(2)} = \\mathbf{s}^{(1)} + \\text{MLP}(\\text{LN}(\\mathbf{s}^{(1)}))$<br>   $\\mathbf{u}_t = \\text{LN}(\\mathbf{s}^{(2)})$<br>**æ•°å­¦åŸç†**ï¼šäº¤å‰æ³¨æ„åŠ›çš„æƒé‡ç”± $\\mathbf{q}_t$ï¼ˆå½“å‰åŠ¨æ€ï¼‰å’Œ $\\tilde{\\mathbf{Z}}$ï¼ˆæŒ‡ä»¤ï¼‰ä¹‹é—´çš„ç›¸ä¼¼åº¦å†³å®šã€‚è¿™ä½¿å¾—ç­–ç•¥èƒ½å¤Ÿ**å…³æ³¨ä¸å½“å‰ç‰©ç†çŠ¶æ€æ›´ç›¸å®¹çš„æŒ‡ä»¤ç‰‡æ®µ**ï¼Œå¹¶æŠ‘åˆ¶å™ªå£°æˆ–ä¸ä¸€è‡´çš„æŒ‡ä»¤ã€‚ | - **æŒ‡ä»¤çª—å£åŠé•¿** $L = 10$ï¼ˆå³æ€»å…±21æ­¥æŒ‡ä»¤ï¼Œçº¦0.7ç§’ä¸Šä¸‹æ–‡ï¼‰ã€‚<br>- **æŒ‡ä»¤ç»´åº¦** $\\mathbf{g}_t \\in \\mathbb{R}^{38}$ï¼ˆåŒ…å«å‚è€ƒåŸºåº§çº¿/è§’é€Ÿåº¦ã€é‡åŠ›æ–¹å‘ã€å…³èŠ‚ä½ç½®ï¼‰ã€‚<br>- **åµŒå…¥ç»´åº¦** $n_{\\text{embd}} = 128$ï¼ˆä¸å†å²ç¼–ç å™¨ä¸€è‡´ï¼‰ã€‚<br>- **äº¤å‰æ³¨æ„åŠ›**ï¼šè¿™æ˜¯å®ç°**é€‰æ‹©æ€§èšåˆ**çš„æ ¸å¿ƒã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œå°†å…¶æ”¹ä¸ºè‡ªæ³¨æ„åŠ›ï¼ˆ`Ours SelfAttn CmdEnc`ï¼‰ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ |\n| **2. åŠ¨ä½œç©ºé—´ä¸åº•å±‚æ§åˆ¶** | **ç›®æ ‡**ï¼šå®šä¹‰ç­–ç•¥è¾“å‡ºå¦‚ä½•è½¬æ¢ä¸ºå…³èŠ‚æ‰­çŸ©ï¼Œå®ç°é«˜æ•ˆæ¢ç´¢å’Œç²¾ç¡®è·Ÿè¸ªã€‚<br>**åŸç†**ï¼šç­–ç•¥è¾“å‡ºæ®‹å·®å…³èŠ‚ä½ç½®æŒ‡ä»¤ $\\mathbf{a}_t \\in \\mathbb{R}^{29}$ã€‚ç›®æ ‡å…³èŠ‚ä½ç½®æ˜¯å‚è€ƒä½ç½®ä¸æ®‹å·®ä¹‹å’Œï¼š<br>$\\mathbf{q}^{\\text{tar}}_t = \\mathbf{q}^{\\text{ref}}_t + \\mathbf{a}_t$<br>å…³èŠ‚æ‰­çŸ©ç”±PDæ§åˆ¶å™¨è®¡ç®—ï¼š<br>$\\boldsymbol{\\tau}_t = \\mathbf{K}_p (\\mathbf{q}^{\\text{tar}}_t - \\mathbf{q}_t) - \\mathbf{K}_d \\dot{\\mathbf{q}}_t$<br>å…¶ä¸­ $\\mathbf{K}_p, \\mathbf{K}_d$ ä¸ºå¯¹è§’å¢ç›ŠçŸ©é˜µã€‚ | - **åŠ¨ä½œç»´åº¦**ï¼š29ç»´ï¼Œå¯¹åº”æœºå™¨äºº29ä¸ªè‡ªç”±åº¦ã€‚<br>- **è®¾è®¡ä¼˜åŠ¿**ï¼š<br>  1. **æå‡è·Ÿè¸ªç²¾åº¦**ï¼šPDè®¾å®šç‚¹é”šå®šåœ¨å‚è€ƒè¿åŠ¨ä¸Šï¼Œç­–ç•¥åªéœ€å­¦ä¹ ä¿®æ­£é‡ã€‚<br>  2. **æå‡æ ·æœ¬æ•ˆç‡**ï¼šæ¢ç´¢ç©ºé—´è¢«é™åˆ¶åœ¨å‚è€ƒå§¿æ€é™„è¿‘ï¼Œè€Œéæ•´ä¸ªå…³èŠ‚é…ç½®ç©ºé—´ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚ |\n| **3. è·Œå€’æ¢å¤è¯¾ç¨‹é›†æˆ** | **ç›®æ ‡**ï¼šåœ¨ç»Ÿä¸€è®­ç»ƒæ¡†æ¶å†…æ•™ä¼šç­–ç•¥è‡ªä¸»è·Œå€’æ¢å¤ï¼Œä»è€Œæ‹“å®½çŠ¶æ€åˆ†å¸ƒç»éªŒï¼Œæå‡å¯¹æ¥è§¦å’Œæ‰°åŠ¨çš„é²æ£’æ€§ã€‚<br>**æ–¹æ³•**ï¼š<br>1. **éšæœºåŒ–æ¢å¤åˆå§‹åŒ–**ï¼šä»¥0.15çš„æ¦‚ç‡å°†å¹¶è¡Œç¯å¢ƒè®¾ä¸ºâ€œæ¢å¤ç¯å¢ƒâ€ï¼Œå¹¶å°†æœºå™¨äººé‡ç½®ä¸ºéšæœºçš„ä¸ç¨³å®šå§¿æ€ã€‚<br>2. **é€€ç«è¾…åŠ©åŠ›**ï¼šåœ¨æ¢å¤ç¯å¢ƒä¸­ï¼Œæ–½åŠ ä¸€ä¸ªå‘ä¸Šçš„è¾…åŠ©åŠ›ï¼Œå¤§å°ä» $[0, 200]$ Nå‡åŒ€é‡‡æ ·ï¼Œä»¥å¢åŠ æ—©æœŸè®­ç»ƒä¸­å¯æ¢å¤çŠ¶æ€çš„å‡ºç°é¢‘ç‡ã€‚è¯¥åŠ›éšè®­ç»ƒè¿­ä»£**çº¿æ€§é€€ç«è‡³å¯å¿½ç•¥æ°´å¹³**ï¼Œç¡®ä¿æœ€ç»ˆç­–ç•¥ä»…ä¾é è‡ªèº«æ§åˆ¶å®Œæˆæ¢å¤ã€‚<br>3. **æ¢å¤çª—å£ç»ˆæ­¢æ¡ä»¶**ï¼šåœ¨æ¢å¤ç¯å¢ƒä¸­ï¼Œè®¾ç½®ä¸€ä¸ª3ç§’çš„â€œæ¢å¤çª—å£â€ã€‚åœ¨æ­¤çª—å£å†…ï¼Œå³ä½¿è§¦å‘è·Œå€’åˆ¤æ®ï¼ˆå¦‚å§¿æ€è¿‡åº¦åç¦»ï¼‰ï¼Œä¹Ÿä¸æå‰ç»ˆæ­¢å›åˆï¼Œå…è®¸ç­–ç•¥å®Œæˆç«™èµ·å’Œé‡æ–°ç¨³å®šã€‚è‹¥è¶…æ—¶æœªæ¢å¤ï¼Œåˆ™ç»ˆæ­¢ã€‚ | - **æ¢å¤ç¯å¢ƒæ¦‚ç‡**ï¼š15%ã€‚<br>- **è¾…åŠ©åŠ›èŒƒå›´**ï¼š[0, 200] Nï¼Œçº¿æ€§é€€ç«ã€‚<br>- **æ¢å¤çª—å£**ï¼š3ç§’ã€‚<br>- **å®é™…ä»·å€¼**ï¼šæ¶ˆèå®éªŒï¼ˆ`Ours w/o Fall Recovery`ï¼‰æ˜¾ç¤ºï¼Œæ­¤è®­ç»ƒæ˜¾è‘—æå‡äº†ç­–ç•¥åœ¨**åœ°é¢äº¤äº’å¯†é›†**åŠ¨ä½œï¼ˆå¦‚çˆ¬è¡Œã€è·ªåï¼‰ä¸Šçš„æˆåŠŸç‡ï¼Œè¯æ˜äº†å…¶ä¸ä»…æä¾›äº†æ¢å¤èƒ½åŠ›ï¼Œæ›´é€šè¿‡æ¥è§¦ç»éªŒå¢å¼ºäº†æ•´ä½“é²æ£’æ€§ã€‚ |\n| **4. è®­ç»ƒä¸æ•°æ®ç­–ç•¥** | **ç›®æ ‡**ï¼šå®ç°æ•°æ®é«˜æ•ˆå’Œè®­ç»ƒç¨³å®šçš„ç«¯åˆ°ç«¯å­¦ä¹ ã€‚<br>**æ–¹æ³•**ï¼š<br>1. **ç´§å‡‘é«˜è´¨é‡æ•°æ®é›†**ï¼šä»LAFAN1å’ŒAMASSæ•°æ®é›†ä¸­ï¼Œç»è¿‡è´¨é‡æ§åˆ¶å’Œå»å†—ä½™ï¼Œæ„å»ºçº¦**3.5å°æ—¶**çš„å¤šæ ·åŒ–é«˜è´¨é‡è¿åŠ¨æ•°æ®ã€‚<br>2. **éå¯¹ç§°æ¼”å‘˜-è¯„è®ºå®¶è®­ç»ƒ**ï¼šæ¼”å‘˜ï¼ˆç­–ç•¥ï¼‰æ¥æ”¶å¸¦å™ªå£°çš„è§‚æµ‹ $\\mathbf{o}_t$ å’Œæ¡ä»¶åŒ–",
            "timestamp": "2026-02-03T15:57:27.015174",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä½¿ç”¨äº†**å®šé‡**å’Œ**å®šæ€§**ä¸¤ç±»æŒ‡æ ‡æ¥è¯„ä¼°å…¶æå‡ºçš„å…¨èº«è¿åŠ¨è·Ÿè¸ªæ¡†æ¶çš„æ€§èƒ½ã€‚æ ¸å¿ƒå®šé‡æŒ‡æ ‡å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æŒ‡æ ‡åç§° | ç¼©å†™ | è®¡ç®—æ–¹å¼ä¸ç»†èŠ‚ | è¯„ä¼°ç›®çš„ |\n| :--- | :--- | :--- | :--- |\n| **æˆåŠŸç‡** | Succ. | å®šä¹‰ä¸º**æœªå‘ç”Ÿæ‘”å€’çš„ rollout æ¯”ä¾‹**ã€‚æ‘”å€’çš„åˆ¤æ–­æ ‡å‡†æ˜¯ï¼šæœºå™¨äººæ ¹å…³èŠ‚ï¼ˆèº¯å¹²ï¼‰é«˜åº¦ä¸å‚è€ƒè¿åŠ¨çš„é«˜åº¦åå·®è¶…è¿‡ **0.2 ç±³**ã€‚ | è¯„ä¼°ç­–ç•¥çš„**é—­ç¯ç¨³å®šæ€§**å’Œ**é²æ£’æ€§**ã€‚é«˜æˆåŠŸç‡æ„å‘³ç€ç­–ç•¥èƒ½åœ¨é•¿æ—¶é—´æ‰§è¡Œä¸­ä¿æŒå¹³è¡¡ï¼Œé¿å…å¤±è´¥ã€‚ |\n| **å¹³å‡æ¯å…³èŠ‚ä½ç½®è¯¯å·®** | `E_MPJPE` | è®¡ç®—æ‰€æœ‰å…³èŠ‚çš„ **3D ä½ç½®è¯¯å·®ï¼ˆå•ä½ï¼šæ¯«ç±³ï¼Œmmï¼‰** çš„å¹³å‡å€¼ã€‚å…¬å¼ä¸ºå„å…³èŠ‚åœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„ä½ç½®ä¸å‚è€ƒä½ç½®ä¹‹å·®çš„L2èŒƒæ•°çš„å‡å€¼ã€‚ | è¯„ä¼°**è¿åŠ¨è·Ÿè¸ªçš„ç²¾ç¡®åº¦**ã€‚è¯¥æŒ‡æ ‡ç›´æ¥åæ˜ äº†ç­–ç•¥å¤ç°å‚è€ƒè¿åŠ¨å§¿æ€çš„ä¿çœŸåº¦ï¼Œæ•°å€¼è¶Šä½è¶Šå¥½ã€‚ |\n| **å¹³å‡å…³é”®ç‚¹ä½ç½®è¯¯å·®** | `E_MPKPE` | è®¡ç®—ä¸€ç»„é¢„å®šä¹‰çš„**èº«ä½“å…³é”®ç‚¹**ï¼ˆå¦‚æ‰‹ã€è„šã€è‚˜ã€è†ç­‰ï¼‰çš„3Dä½ç½®è¯¯å·®ï¼ˆmmï¼‰çš„å¹³å‡å€¼ã€‚è¿™äº›å…³é”®ç‚¹é€šå¸¸æ¯”å…³èŠ‚æ›´èƒ½ä»£è¡¨æ•´ä½“å§¿æ€ã€‚ | ä¸MPJPEç±»ä¼¼ï¼Œä½†æ›´ä¾§é‡äºè¯„ä¼°**æ•´ä½“èº«ä½“å½¢æ€å’Œæœ«ç«¯æ‰§è¡Œå™¨**çš„è·Ÿè¸ªç²¾åº¦ã€‚ |\n| **å…³é”®ç‚¹é€Ÿåº¦è¯¯å·®** | `E_vel` | è®¡ç®—å…³é”®ç‚¹çš„**é€Ÿåº¦**ä¸å‚è€ƒé€Ÿåº¦ä¹‹é—´çš„å·®å¼‚ï¼ˆå•ä½ï¼šæ¯«ç±³/å¸§ï¼Œmm/frameï¼‰ã€‚ | è¯„ä¼°**è¿åŠ¨åŠ¨æ€ç‰¹æ€§**çš„è·Ÿè¸ªè´¨é‡ã€‚ä½è¯¯å·®æ„å‘³ç€ç­–ç•¥ä¸ä»…å¤ç°äº†å§¿æ€ï¼Œä¹Ÿå¤ç°äº†è¿åŠ¨çš„é€Ÿåº¦å’Œæµç•…æ€§ã€‚ |\n| **å…³é”®ç‚¹åŠ é€Ÿåº¦è¯¯å·®** | `E_acc` | è®¡ç®—å…³é”®ç‚¹çš„**åŠ é€Ÿåº¦**ä¸å‚è€ƒåŠ é€Ÿåº¦ä¹‹é—´çš„å·®å¼‚ï¼ˆå•ä½ï¼šæ¯«ç±³/å¸§Â²ï¼Œmm/frameÂ²ï¼‰ã€‚ | è¯„ä¼°**è¿åŠ¨åŠ¨åŠ›å­¦ç‰¹æ€§**çš„è·Ÿè¸ªè´¨é‡ã€‚è¿™å¯¹äºé«˜åº¦åŠ¨æ€çš„è¿åŠ¨ï¼ˆå¦‚è·³è·ƒã€å¿«é€Ÿè½¬èº«ï¼‰å°¤ä¸ºé‡è¦ï¼Œèƒ½åæ˜ ç­–ç•¥å¯¹è¿åŠ¨åŠ›å­¦çš„ç†è§£ã€‚ |\n\n**æŒ‡æ ‡ä½¿ç”¨åœºæ™¯ï¼š**\n- **è¡¨ I** ä¸­ä¸»è¦ä½¿ç”¨ **Succ.** å’Œ **`E_MPJPE`** æ¥å¯¹æ¯”ä¸åŒæ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„ç»¼åˆæ€§èƒ½ã€‚\n- **å›¾ 3** çš„é²æ£’æ€§å®éªŒä¸­ï¼ŒåŒæ—¶ä½¿ç”¨äº† **`E_MPJPE`ã€`E_MPKPE`ã€`E_vel`ã€`E_acc`** å››ä¸ªæŒ‡æ ‡ï¼Œå…¨é¢è¡¡é‡åœ¨å‘½ä»¤å™ªå£°ä¸‹è·Ÿè¸ªç²¾åº¦å’Œç‰©ç†åˆç†æ€§çš„é€€åŒ–æƒ…å†µã€‚\n\n### äºŒã€ æ•°æ®é›†\n\nè®ºæ–‡æ„å»ºäº†ä¸€ä¸ª**ç´§å‡‘è€Œé«˜è´¨é‡**çš„è¿åŠ¨æ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°ã€‚\n\n| æ•°æ®é›†æ–¹é¢ | ç»†èŠ‚æè¿° |\n| :--- | :--- |\n| **æ•°æ®æ¥æº** | 1. **LAFAN1**ï¼šä¸€ä¸ªç”¨äºè¿åŠ¨è¡¥é—´çš„å…¬å¼€æ•°æ®é›†ã€‚<br>2. **AMASS**ï¼šä¸€ä¸ªå¤§è§„æ¨¡ã€èšåˆäº†å¤šä¸ªå…‰å­¦åŠ¨æ•æ•°æ®é›†çš„åŠ¨ä½œæ¡£æ¡ˆã€‚è®ºæ–‡ä»ä¸­**é€‰å–äº†ä¸€ä¸ªå­é›†**ã€‚ |\n| **æ•°æ®å¤„ç†** | 1. **è¿åŠ¨é‡å®šå‘**ï¼šä½¿ç”¨ **General Motion Retargeting** æ–¹æ³•ï¼Œå°†æ‰€æœ‰äººä½“è¿åŠ¨æ•°æ®é‡å®šå‘åˆ°ç›®æ ‡äººå½¢æœºå™¨äººï¼ˆUnitree G1ï¼‰çš„ kinematics æ¨¡å‹ä¸Šã€‚<br>2. **è´¨é‡æ§åˆ¶**ï¼šå¯¹é‡å®šå‘åçš„è¿åŠ¨è¿›è¡Œç­›é€‰ï¼Œ**ç§»é™¤ä¸å¯è¡Œçš„è¿åŠ¨ã€ä½è´¨é‡åºåˆ—ï¼Œå¹¶å‡å°‘å†—ä½™**ã€‚è¿™æ˜¯è¯¥å·¥ä½œçš„ä¸€ä¸ªå…³é”®æ­¥éª¤ï¼Œæ—¨åœ¨æå‡æ•°æ®é›†çš„â€œä¿¡æ¯å¯†åº¦â€ã€‚ |\n| **æœ€ç»ˆè§„æ¨¡** | ç»è¿‡ä¸Šè¿°å¤„ç†åï¼Œå¾—åˆ°ä¸€ä¸ªçº¦ **3.5 å°æ—¶** çš„è¿åŠ¨æ•°æ®åˆé›†ã€‚è®ºæ–‡å¼ºè°ƒï¼Œç›¸è¾ƒäºå…¶ä»–éœ€è¦æ•°ç™¾å°æ—¶æ•°æ®çš„å·¥ä½œï¼ˆå¦‚SONICï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸**ç´§å‡‘**çš„æ•°æ®é›†ã€‚ |\n| **æ•°æ®é›†åˆ’åˆ†ï¼ˆç”¨äºè¯„ä¼°ï¼‰** | ä¸ºäº†å…¨é¢è¯„ä¼°æ³›åŒ–èƒ½åŠ›ï¼Œè®ºæ–‡åœ¨æµ‹è¯•æ—¶ä½¿ç”¨äº†ä¸‰ç±»ä¸åŒçš„è¿åŠ¨æºï¼š<br>1. **åŠ¨æ•æ•°æ®**ï¼šæ¥è‡ªè®­ç»ƒé›†åŒæºï¼ˆLAFAN1/AMASSå­é›†ï¼‰ä½†æœªåœ¨è®­ç»ƒä¸­è§è¿‡çš„åºåˆ—ã€‚è¯„ä¼°**åˆ†å¸ƒå†…**æ€§èƒ½ã€‚<br>2. **è§†é¢‘è¡ç”Ÿè¿åŠ¨**ï¼šä»å…¬å¼€è§†é¢‘ä¸­é€šè¿‡**åŸºäºè§†é¢‘çš„äººä½“è¿åŠ¨ä¼°è®¡ç®¡çº¿**æå–çš„è¿åŠ¨ã€‚åŒ…å«è¡Œèµ°ã€èˆè¹ˆã€æ­¦æœ¯ç­‰å¸¸è§è¡Œä¸ºã€‚è¯„ä¼°**è·¨æ•°æ®æº**å’Œ**å¯¹å™ªå£°çš„æ³›åŒ–**èƒ½åŠ›ã€‚<br>3. **åœ°é¢äº¤äº’è¿åŠ¨**ï¼šåŒ…å«å¤§é‡æ¥è§¦å’Œå¤æ‚åœ°é¢äº¤äº’çš„è¡Œä¸ºï¼Œå¦‚çˆ¬è¡Œã€è·ªå§¿ã€åå§¿ã€åœ°æ¿èˆç­‰ã€‚è¯„ä¼°**åœ¨å¤æ‚æ¥è§¦æ¨¡å¼ä¸‹çš„é²æ£’æ€§**ã€‚ |\n\n### æ€»ç»“ä¸åˆ›æ–°ç‚¹\n\n1.  **è¯„ä»·æŒ‡æ ‡çš„ç»¼åˆæ€§**ï¼šè®ºæ–‡ä¸ä»…å…³æ³¨é™æ€å§¿æ€è¯¯å·®ï¼ˆ`E_MPJPE`ï¼‰ï¼Œè¿˜å¼•å…¥äº†åŠ¨æ€è¯¯å·®ï¼ˆ`E_vel`, `E_acc`ï¼‰å’Œç¨³å®šæ€§æŒ‡æ ‡ï¼ˆSucc.ï¼‰ï¼Œå½¢æˆäº†å¯¹è¿åŠ¨è·Ÿè¸ªæ€§èƒ½**ä»ç²¾åº¦ã€åŠ¨æ€æ€§åˆ°é²æ£’æ€§**çš„å¤šç»´åº¦è¯„ä¼°ä½“ç³»ã€‚\n2.  **æ•°æ®é›†çš„æ„å»ºå“²å­¦**ï¼šå…¶æ ¸å¿ƒåˆ›æ–°ä¸åœ¨äºæ•°æ®é‡çš„â€œå¤§â€ï¼Œè€Œåœ¨äºæ•°æ®çš„â€œç²¾â€ã€‚é€šè¿‡ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶ï¼Œæ„å»ºäº†ä¸€ä¸ª**å°è€Œç²¾**çš„é«˜è´¨é‡æ•°æ®é›†ã€‚è¿™è¯æ˜äº†å…¶æå‡ºçš„ **â€œåŠ¨æ€æ¡ä»¶åŒ–å‘½ä»¤èšåˆâ€æ¡†æ¶** èƒ½å¤Ÿä»æœ‰é™ä½†é«˜è´¨é‡çš„æ•°æ®ä¸­é«˜æ•ˆå­¦ä¹ ï¼Œ**é™ä½äº†æ•°æ®æ”¶é›†å’Œè®¡ç®—çš„é—¨æ§›**ï¼Œå…·æœ‰å¾ˆé«˜çš„å®é™…åº”ç”¨ä»·å€¼ã€‚\n3.  **è¯„ä¼°åœºæ™¯çš„é’ˆå¯¹æ€§**ï¼šé€šè¿‡è®¾è®¡**åŠ¨æ•å†…åˆ†å¸ƒã€è§†é¢‘è·¨åˆ†å¸ƒã€å¤æ‚æ¥è§¦**ä¸‰ç±»æµ‹è¯•é›†ï¼Œç³»ç»Ÿæ€§åœ°éªŒè¯äº†æ–¹æ³•åœ¨**ç²¾åº¦ã€æ³›åŒ–æ€§å’Œé²æ£’æ€§**ä¸‰ä¸ªæ–¹é¢çš„ä¼˜åŠ¿ï¼Œå®éªŒè®¾è®¡éå¸¸ä¸¥è°¨å’Œå…¨é¢ã€‚",
            "timestamp": "2026-02-03T15:58:10.541740",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ä»¥ä¸‹ä¸‰ç±»æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒè¯„ä¼°ï¼š\n\n1.  **MoCap æ•°æ®**ï¼š\n    - **æ¥æº**ï¼š ä» **LAFAN1** å’Œ **AMASS** æ•°æ®é›†ä¸­é€‰å–çš„å­é›†ã€‚\n    - **ç‰¹ç‚¹**ï¼š ç»è¿‡è´¨é‡æ§åˆ¶å’Œå»å†—ä½™å¤„ç†ï¼Œæ„æˆäº†çº¦ **3.5å°æ—¶** çš„ç´§å‡‘è®­ç»ƒæ•°æ®é›†ã€‚è¿™éƒ¨åˆ†æ•°æ®ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹åœ¨â€œåˆ†å¸ƒå†…â€è¿åŠ¨ä¸Šçš„æ€§èƒ½ã€‚\n\n2.  **è§†é¢‘è¡ç”Ÿè¿åŠ¨**ï¼š\n    - **æ¥æº**ï¼š ä»å…¬å¼€è§†é¢‘ä¸­é€šè¿‡äººä½“è¿åŠ¨ä¼°è®¡ç®—æ³•ï¼ˆå¦‚ `[20]`ï¼‰ä¼°è®¡å¾—åˆ°çš„è¿åŠ¨åºåˆ—ã€‚\n    - **ç‰¹ç‚¹**ï¼š åŒ…å«è¡Œèµ°ã€èˆè¹ˆã€æ­¦æœ¯ç­‰å¸¸è§è¡Œä¸ºã€‚**æœªç”¨äºè®­ç»ƒ**ï¼Œä¸“é—¨ç”¨äºæµ‹è¯•æ¨¡å‹å¯¹**è·¨æ•°æ®æº**å’Œ**å™ªå£°å‚è€ƒ**çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n3.  **åœ°é¢äº¤äº’è¿åŠ¨**ï¼š\n    - **æ¥æº**ï¼š åŒ…å«å¤§é‡å¤æ‚æ¥è§¦è¡Œä¸ºçš„è¿åŠ¨ï¼Œå¦‚çˆ¬è¡Œã€è·ªå§¿ã€åå§¿ã€åœ°æ¿èˆç­‰ã€‚\n    - **ç‰¹ç‚¹**ï¼š ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨**æ¥è§¦å¯†é›†ã€åŠ¨æ€å¿«é€Ÿ**åœºæ™¯ä¸‹çš„é²æ£’æ€§å’Œè·Ÿè¸ªç²¾åº¦ã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨äº†ä»¥ä¸‹é‡åŒ–æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼š\n\n| æŒ‡æ ‡ | å…¨ç§° | å«ä¹‰ | è¯„ä¼°æ–¹å‘ |\n| :--- | :--- | :--- | :--- |\n| **Succ. (æˆåŠŸç‡)** | Success Rate | åœ¨ rollout ä¸­æœªå‘ç”Ÿæ‘”å€’çš„ç‰‡æ®µæ¯”ä¾‹ã€‚æ‘”å€’å®šä¹‰ä¸ºæ ¹å…³èŠ‚é«˜åº¦ä¸å‚è€ƒå€¼åå·®è¶…è¿‡ 0.2 ç±³ã€‚ | **è¶Šé«˜è¶Šå¥½** |\n| **`E_MPJPE`** | Mean Per-Joint Position Error | å…³èŠ‚ç‚¹çš„å¹³å‡ä¸‰ç»´ä½ç½®è¯¯å·®ï¼ˆå•ä½ï¼šæ¯«ç±³ï¼‰ã€‚è¡¡é‡å…³èŠ‚çº§åˆ«çš„è·Ÿè¸ªç²¾åº¦ã€‚ | **è¶Šä½è¶Šå¥½** |\n| **`E_MPKPE`** | Mean Keypoint Position Error | å…³é”®ç‚¹çš„å¹³å‡ä¸‰ç»´ä½ç½®è¯¯å·®ï¼ˆå•ä½ï¼šæ¯«ç±³ï¼‰ã€‚ | **è¶Šä½è¶Šå¥½** |\n| **`E_vel`** | Keypoint Velocity Error | å…³é”®ç‚¹é€Ÿåº¦ä¸å‚è€ƒè¿åŠ¨ä¹‹é—´çš„å·®å¼‚ï¼ˆå•ä½ï¼šæ¯«ç±³/å¸§ï¼‰ã€‚ | **è¶Šä½è¶Šå¥½** |\n| **`E_acc`** | Keypoint Acceleration Error | å…³é”®ç‚¹åŠ é€Ÿåº¦ä¸å‚è€ƒè¿åŠ¨ä¹‹é—´çš„å·®å¼‚ï¼ˆå•ä½ï¼šæ¯«ç±³/å¸§Â²ï¼‰ã€‚ | **è¶Šä½è¶Šå¥½** |\n\n### ä¸‰ã€ æ€§èƒ½æå‡ä¸å…³é”®ç»“æœ\nè®ºæ–‡æå‡ºçš„æ–¹æ³•ï¼ˆ**Ours**ï¼‰åœ¨å¤šä¸ªæ–¹é¢æ˜¾è‘—è¶…è¶Šäº†åŸºçº¿æ–¹æ³•ï¼ˆGMT `[3]` å’Œ Any2Track `[31]`ï¼‰ã€‚\n\n#### 1. æ•´ä½“è·Ÿè¸ªæ€§èƒ½ï¼ˆè¡¨ I (a)ï¼‰\n- **MoCapæ•°æ®**ï¼š æˆåŠŸç‡ä»åŸºçº¿æœ€ä½³ï¼ˆAny2Trackï¼‰çš„ **89.2%** æå‡è‡³ **98.3%**ï¼›`E_MPJPE` ä» **56.96 mm** é™ä½è‡³ **41.12 mm**ã€‚\n- **è§†é¢‘è¡ç”Ÿè¿åŠ¨**ï¼š æå‡æœ€ä¸ºæ˜¾è‘—ã€‚æˆåŠŸç‡ä»åŸºçº¿æœ€ä½³ï¼ˆGMTï¼‰çš„ **72.4%** å¤§å¹…æå‡è‡³ **94.6%**ï¼›`E_MPJPE` ä» **96.47 mm** å¤§å¹…é™ä½è‡³ **46.56 mm**ã€‚è¿™è¯æ˜äº†æ¨¡å‹å¯¹**å™ªå£°å‚è€ƒå’Œåˆ†å¸ƒåç§»**çš„å¼ºé²æ£’æ€§ã€‚\n- **åœ°é¢äº¤äº’è¿åŠ¨**ï¼š æˆåŠŸç‡ä»åŸºçº¿æœ€ä½³ï¼ˆGMTï¼‰çš„ **48.9%** å¤§å¹…æå‡è‡³ **90.1%**ï¼›`E_MPJPE` ä» **146.95 mm** å¤§å¹…é™ä½è‡³ **54.92 mm**ã€‚è¯æ˜äº†æ¨¡å‹åœ¨**å¤æ‚æ¥è§¦åœºæ™¯**ä¸‹çš„å“è¶Šèƒ½åŠ›ã€‚\n\n#### 2. å¯¹å‚è€ƒå™ªå£°çš„é²æ£’æ€§ï¼ˆå›¾ 3ï¼‰\n- åœ¨ Charleston èˆè¹ˆåŠ¨ä½œä¸Šæ³¨å…¥**ä¸åŒç¨‹åº¦**çš„å‚è€ƒå‘½ä»¤å™ªå£°è¿›è¡Œæµ‹è¯•ã€‚\n- **åŸºçº¿æ–¹æ³•**ï¼ˆGMT, Any2Trackï¼‰ï¼š åœ¨å™ªå£°æ°´å¹³è¶…è¿‡ 200% æ—¶ï¼Œå„é¡¹è¯¯å·® (`E_MPJPE`, `E_vel`, `E_acc`) æ€¥å‰§ä¸Šå‡ï¼Œè·Ÿè¸ªè¿…é€Ÿå¤±æ•ˆã€‚\n- **æœ¬æ–‡æ–¹æ³•**ï¼š å³ä½¿åœ¨é«˜å™ªå£°æ°´å¹³ï¼ˆå¦‚ 1500%ï¼‰ä¸‹ï¼Œè¯¯å·®å¢é•¿å¹³ç¼“ï¼Œä»èƒ½ä¿æŒç¨³å®šçš„è¿åŠ¨è·Ÿè¸ªï¼Œå±•ç°äº†**æå¼ºçš„æŠ—å‘½ä»¤è…èš€èƒ½åŠ›**ã€‚\n\n#### 3. æ¶ˆèå®éªŒéªŒè¯ï¼ˆè¡¨ I (b)(c)ï¼‰\n- **æ ¸å¿ƒæ¶æ„è´¡çŒ®**ï¼š\n    - **åŠ¨æ€æ¡ä»¶åŒ–äº¤å‰æ³¨æ„åŠ›å‘½ä»¤ç¼–ç å™¨** (`Ours SelfAttn CmdEnc`)ï¼š å°†å…¶æ›¿æ¢ä¸ºè‡ªæ³¨æ„åŠ›ç¼–ç å™¨åï¼Œæ€§èƒ½ï¼ˆå°¤å…¶æ˜¯å¯¹è§†é¢‘å’Œåœ°é¢äº¤äº’è¿åŠ¨ï¼‰**å¤§å¹…ä¸‹é™**ï¼Œè¯æ˜äº†**åˆ©ç”¨å½“å‰åŠ¨æ€çŠ¶æ€æœ‰é€‰æ‹©åœ°èšåˆå‚è€ƒå‘½ä»¤**æ˜¯æå‡æ³›åŒ–æ€§å’Œé²æ£’æ€§çš„**å…³é”®æœºåˆ¶**ã€‚\n    - **å› æœå†å²ç¼–ç å™¨** (`Ours CNN HistEnc`)ï¼š å°†å…¶æ›¿æ¢ä¸º CNN ç¼–ç å™¨åï¼Œæ€§èƒ½å‡ºç°**ä¸€è‡´ä½†ç›¸å¯¹è¾ƒå°çš„ä¸‹é™**ï¼Œè¯æ˜äº†**å› æœæ—¶åºå»ºæ¨¡**å¯¹ç¨³å®šåŠ¨æ€è¡¨å¾çš„é‡è¦æ€§ã€‚\n- **æ‘”å€’æ¢å¤é›†æˆè´¡çŒ®** (`Ours w/o Fall Recovery`)ï¼š\n    - ç§»é™¤æ‘”å€’æ¢å¤è®­ç»ƒåï¼Œå¯¹ MoCap å’Œè§†é¢‘æ•°æ®çš„æ€§èƒ½å½±å“å¾ˆå°ã€‚\n    - ä½†å¯¹**åœ°é¢äº¤äº’è¿åŠ¨**çš„æ€§èƒ½å½±å“**å·¨å¤§**ï¼šæˆåŠŸç‡ä» 90.1% éª¤é™è‡³ 70.5%ï¼Œ`E_MPJPE` ä» 54.92 mm æ¶åŒ–è‡³ 96.75 mmã€‚è¿™è¡¨æ˜**æ‘”å€’æ¢å¤è®­ç»ƒä¸ä»…èµ‹äºˆäº†è‡ªæ¢å¤èƒ½åŠ›ï¼Œæ›´é‡è¦çš„æ˜¯é€šè¿‡æ¥è§¦ç»éªŒçš„æ³›åŒ–ï¼Œæ˜¾è‘—æå‡äº†åœ¨å¤æ‚æ¥è§¦è¡Œä¸ºä¸‹çš„è·Ÿè¸ªé²æ£’æ€§**ã€‚\n\n### æ€»ç»“\næœ¬æ–‡æ–¹æ³•çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºï¼š\n- **æ•°æ®é«˜æ•ˆ**ï¼šä»…ç”¨çº¦ **3.5å°æ—¶** é«˜è´¨é‡æ•°æ®ï¼Œå®ç°äº†ç«¯åˆ°ç«¯è®­ç»ƒã€‚\n- **æ€§èƒ½å“è¶Š**ï¼šåœ¨**æˆåŠŸç‡**å’Œ**è·Ÿè¸ªç²¾åº¦** (`E_MPJPE`) ä¸Šå…¨é¢è¶…è¶Šç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚\n- **æ³›åŒ–æ€§å¼º**ï¼šå¯¹**æœªè§è¿‡çš„ã€å™ªå£°å¤§çš„**è§†é¢‘è¡ç”Ÿè¿åŠ¨è¡¨ç°å‡ºä¼˜å¼‚çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚\n- **é²æ£’æ€§é«˜**ï¼šé€šè¿‡**åŠ¨æ€æ¡ä»¶åŒ–å‘½ä»¤èšåˆ**æœºåˆ¶ï¼Œå¯¹å‚è€ƒå‘½ä»¤å™ªå£°å…·æœ‰**æé«˜çš„å®¹å¿åº¦**ï¼›é€šè¿‡**é›†æˆæ‘”å€’æ¢å¤è®­ç»ƒ**ï¼Œæ˜¾è‘—æå‡äº†åœ¨**æ¥è§¦å¯†é›†ã€åŠ¨æ€å¤æ‚**åœºæ™¯ä¸‹çš„ç¨³å®šæ€§å’Œå®‰å…¨æ€§ã€‚",
            "timestamp": "2026-02-03T15:58:56.413732",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œè¯¥ç ”ç©¶åœ¨å–å¾—æ˜¾è‘—æˆæœçš„åŒæ—¶ï¼Œä¹Ÿå­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¸»è¦é›†ä¸­åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š\n\n- **ç¼ºä¹å…¨å±€å®šä½ä¸é•¿æœŸä¸€è‡´æ€§**ï¼šè®ºæ–‡åœ¨ç»“è®ºéƒ¨åˆ†æ˜ç¡®æŒ‡å‡ºï¼Œå½“å‰çš„æ¡†æ¶**ç¼ºä¹å…¨å±€å®šä½èƒ½åŠ›**ã€‚è¿™æ„å‘³ç€æœºå™¨äººçš„è¿åŠ¨è·Ÿè¸ªæ˜¯åœ¨æœ¬ä½“åæ ‡ç³»ä¸‹è¿›è¡Œçš„ï¼Œæ— æ³•åœ¨é•¿æ—¶é—´ã€å¤§èŒƒå›´çš„æ‰§è¡Œä¸­ä¿æŒä¸ä¸–ç•Œåæ ‡ç³»çš„ä¸€è‡´æ€§ã€‚è¿™é™åˆ¶äº†å…¶åœ¨éœ€è¦ç²¾ç¡®å…¨å±€å¯¼èˆªæˆ–é•¿æœŸä»»åŠ¡è§„åˆ’åœºæ™¯ä¸­çš„åº”ç”¨ã€‚\n\n- **å¯¹ä¸Šæ¸¸è¿åŠ¨ç”Ÿæˆæ¨¡å—çš„ä¾èµ–ä¸å®¹å¿åº¦**ï¼šè™½ç„¶è®ºæ–‡å±•ç¤ºäº†ä¸è¿åŠ¨åˆæˆæ–¹æ³•ï¼ˆå¦‚åŸºäºæ¸¸æˆæ‰‹æŸ„çš„ locomotionï¼‰çš„é›†æˆï¼Œå¹¶è¡¨ç°å‡ºå¯¹éå¹³æ»‘å‚è€ƒè½¨è¿¹çš„é²æ£’æ€§ï¼Œä½†è¿™ç§é›†æˆä»ç„¶æ˜¯åˆæ­¥çš„ã€‚æ¡†æ¶**å¯¹ä¸Šæ¸¸æ¨¡å—äº§ç”Ÿçš„è¿åŠ¨è´¨é‡ï¼ˆå¦‚é€»è¾‘ä¸€è‡´æ€§ã€ç‰©ç†å¯è¡Œæ€§ï¼‰çš„ä¸»åŠ¨ç†è§£å’Œä¿®æ­£èƒ½åŠ›æœ‰é™**ã€‚å½“å‰æ–¹æ³•ä¸»è¦é€šè¿‡â€œè¿‡æ»¤â€å™ªå£°æ¥åº”å¯¹ï¼Œä½†æ— æ³•ä»æ ¹æœ¬ä¸Šçº æ­£åŸºäºé”™è¯¯å…ˆéªŒç”Ÿæˆçš„ã€æ•´ä½“ä¸åˆç†çš„è¿åŠ¨æŒ‡ä»¤ã€‚\n\n- **æ•°æ®ä¸è®­ç»ƒçš„ç‰¹å®šæ€§**ï¼š\n    - **æ•°æ®è´¨é‡ä¾èµ–**ï¼šå°½ç®¡ä»…ä½¿ç”¨äº†çº¦3.5å°æ—¶æ•°æ®ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†å¯¹æ•°æ®è¿›è¡Œä¸¥æ ¼**è´¨é‡æ§åˆ¶å’Œå»å†—ä½™**çš„é‡è¦æ€§ã€‚è¿™è¡¨æ˜æ–¹æ³•çš„æ€§èƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¾èµ–äºç²¾å¿ƒç­›é€‰çš„ã€é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ã€‚å¯¹äºæ›´å˜ˆæ‚ã€è´¨é‡æ›´ä½çš„åŸå§‹æ•°æ®æºï¼Œå…¶æœ‰æ•ˆæ€§å¯èƒ½ä¸‹é™ã€‚\n    - **ä»¿çœŸåˆ°ç°å®çš„å·®è·**ï¼šè™½ç„¶å®ç°äº†Sim-to-Realè½¬ç§»ï¼Œä½†è®­ç»ƒå®Œå…¨åœ¨ä»¿çœŸä¸­è¿›è¡Œã€‚ä»¿çœŸç¯å¢ƒï¼ˆIsaac Gymï¼‰çš„ç‰©ç†æ¨¡å‹ä¸çœŸå®ä¸–ç•Œå¿…ç„¶å­˜åœ¨å·®å¼‚ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶ç­–ç•¥åœ¨æç«¯æœªè§è¿‡åŠ¨æ€æˆ–å¤æ‚æ¥è§¦åœºæ™¯ï¼ˆå¦‚æŸ”è½¯ã€ä¸å¹³æ•´åœ°é¢ï¼‰ä¸‹çš„è¡¨ç°ã€‚\n\n- **è®¡ç®—æ¶æ„ä¸å®æ—¶æ€§**ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†å› æœTransformerå’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œè™½ç„¶æœ‰æ•ˆï¼Œä½†ç›¸æ¯”æ›´ç®€å•çš„ç½‘ç»œç»“æ„ï¼Œ**å¸¦æ¥äº†é¢å¤–çš„è®¡ç®—å¼€é”€**ã€‚åœ¨éœ€è¦æä½å»¶è¿Ÿçš„å®æ—¶åº”ç”¨ï¼ˆå¦‚é«˜é€ŸåŠ¨æ€å¹³è¡¡ï¼‰ä¸­ï¼Œè¿™å¯èƒ½æˆä¸ºä¸€ä¸ªç“¶é¢ˆã€‚è®ºæ–‡æœªè¯¦ç»†è®¨è®ºç­–ç•¥åœ¨è¾¹ç¼˜è®¡ç®—è®¾å¤‡ä¸Šçš„éƒ¨ç½²æ•ˆç‡ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nåŸºäºä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡åœ¨ç»“è®ºå’Œå®éªŒåˆ†æä¸­ä¹Ÿæš—ç¤ºæˆ–æ˜ç¡®æå‡ºäº†æœªæ¥çš„å·¥ä½œæ–¹å‘ï¼š\n\n1.  **é›†æˆå…¨å±€çŠ¶æ€ä¼°è®¡ä¸è§„åˆ’**ï¼š\n    ```python\n    # æœªæ¥æ¡†æ¶å¯èƒ½æ‰©å±•çš„è¾“å…¥\n    future_observation = [proprioception, command_window, global_pose, map_info]\n    ```\n    è¿™æ˜¯è®ºæ–‡æ˜ç¡®æŒ‡å‡ºçš„æ–¹å‘ï¼š**å¼•å…¥å…¨å±€å®šä½ï¼ˆå¦‚SLAMã€VIOï¼‰**ï¼Œä½¿ç­–ç•¥èƒ½æ„ŸçŸ¥è‡ªèº«åœ¨ä¸–ç•Œä¸­çš„ä½ç½®å’Œæœå‘ã€‚è¿™å°†æ”¯æŒé•¿è§†é‡ã€è·¨æˆ¿é—´çš„ä»»åŠ¡æ‰§è¡Œï¼Œå¹¶å®ç°ä¸ä¸–ç•Œç‰©ä½“çš„äº¤äº’ï¼ˆå¦‚èµ°å‘å¹¶æ“ä½œç‰¹å®šç‰©ä½“ï¼‰ã€‚\n\n2.  **ä¸ä¸Šæ¸¸æ¨¡å—çš„æ·±åº¦è€¦åˆä¸è”åˆä¼˜åŒ–**ï¼š\n    - **åŒå‘é€šä¿¡**ï¼šä¸ä»…ä¸‹æ¸¸æ§åˆ¶å™¨å®¹å¿ä¸Šæ¸¸å™ªå£°ï¼Œè¿˜å¯ä»¥å‘ä¸Šæ¸¸æ¨¡å—**åé¦ˆå½“å‰åŠ¨æ€å¯è¡Œæ€§ä¿¡æ¯**ï¼ŒæŒ‡å¯¼å…¶ç”Ÿæˆæ›´æ˜“æ‰§è¡Œçš„è¿åŠ¨ã€‚ä¾‹å¦‚ï¼Œæ„å»ºä¸€ä¸ªâ€œè¿åŠ¨å¯è¡Œæ€§è¯„ä¼°â€æ¨¡å—ã€‚\n    - **ç«¯åˆ°ç«¯å­¦ä¹ **ï¼šå°†è¿åŠ¨ç”Ÿæˆï¼ˆå¦‚åŸºäºè¯­è¨€æˆ–è§†é¢‘çš„æŒ‡ä»¤è§£æï¼‰ä¸åº•å±‚è·Ÿè¸ªæ§åˆ¶**åœ¨ä¸€ä¸ªè”åˆæ¡†æ¶ä¸­è¿›è¡Œè®­ç»ƒ**ï¼Œä½¿ç”Ÿæˆçš„è¿åŠ¨åœ¨æºå¤´å°±è€ƒè™‘æœºå™¨äººçš„ç‰©ç†çº¦æŸã€‚\n\n3.  **å¢å¼ºæ•°æ®å¤šæ ·æ€§ä¸ä»¿çœŸçœŸå®æ€§**ï¼š\n    - **æ‰©å……æ•°æ®é›†**ï¼šçº³å…¥æ›´å¤šæç«¯ã€æŒ‘æˆ˜æ€§çš„è¿åŠ¨ï¼ˆå¦‚æ‘”å€’ã€å‰§çƒˆå¯¹æŠ—å¹²æ‰°ï¼‰ï¼Œä»¥åŠåœ¨ä¸åŒåœ°é¢æè´¨ï¼ˆæ²™åœ°ã€ç¢çŸ³ï¼‰ä¸Šçš„è¿åŠ¨æ•°æ®ï¼Œè¿›ä¸€æ­¥æå‡æ³›åŒ–èƒ½åŠ›ã€‚\n    - **é«˜ä¿çœŸä»¿çœŸ**ï¼šåˆ©ç”¨**åŸŸéšæœºåŒ–**å’Œ**ç³»ç»Ÿè¾¨è¯†**æŠ€æœ¯ï¼Œç¼©å°ä»¿çœŸä¸ç°å®çš„å·®è·ã€‚ä¹Ÿå¯ä»¥æ¢ç´¢**åœ¨çº¿è‡ªé€‚åº”**æŠ€æœ¯ï¼Œè®©æœºå™¨äººåœ¨çœŸå®ä¸–ç•Œä¸­æŒç»­å¾®è°ƒç­–ç•¥ã€‚\n\n4.  **æ¶æ„ä¸æ•ˆç‡ä¼˜åŒ–**ï¼š\n    - **æ¨¡å‹è½»é‡åŒ–**ï¼šç ”ç©¶æ›´é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¦‚çº¿æ€§æ³¨æ„åŠ›ï¼‰ã€çŸ¥è¯†è’¸é¦æˆ–ç½‘ç»œå‰ªææŠ€æœ¯ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶é™ä½è®¡ç®—è´Ÿè½½ï¼Œä¾¿äºåœ¨æœºå™¨äººæœ¬ä½“è®¡ç®—å•å…ƒä¸Šéƒ¨ç½²ã€‚\n    - **åˆ†å±‚ä¸æ¨¡å—åŒ–è®¾è®¡**ï¼šå°†â€œåŠ¨æ€æ„ŸçŸ¥â€ã€â€œå‘½ä»¤è¿‡æ»¤â€ã€â€œå…³èŠ‚æ§åˆ¶â€ç­‰åŠŸèƒ½æ¨¡å—åŒ–ï¼Œæé«˜ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå¯ç»´æŠ¤æ€§ï¼Œå¹¶å…è®¸å•ç‹¬æ”¹è¿›æŸä¸ªæ¨¡å—ã€‚\n\n5.  **æ‹“å±•ä»»åŠ¡èŒƒå›´**ï¼š\n    - ä»çº¯è¿åŠ¨è·Ÿè¸ªæ‰©å±•åˆ°**åŒ…å«ç‰©ä½“æ“ä½œçš„å…¨èº«åè°ƒä»»åŠ¡**ï¼ˆLoco-Manipulationï¼‰ã€‚è¿™éœ€è¦å¼•å…¥å¯¹ç‰©ä½“çŠ¶æ€çš„è§‚æµ‹å’Œæ“ä½œè¯­ä¹‰ã€‚\n    - æ¢ç´¢åœ¨**éç»“æ„åŒ–åŠ¨æ€ç¯å¢ƒ**ï¼ˆå¦‚äººç¾¤ä¸­è¡Œèµ°ï¼‰ä¸­çš„é²æ£’æ€§ï¼Œè¿™éœ€è¦å¤„ç†æ›´å¤æ‚çš„å¤–éƒ¨å¹²æ‰°å’Œä¸ç¡®å®šæ€§ã€‚\n\n**æ€»ç»“**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒå±€é™æ€§åœ¨äºå…¶â€œè·Ÿè¸ªå™¨â€çš„å®šä½â€”â€”å®ƒå‡ºè‰²åœ°å®Œæˆäº†â€œå¦‚ä½•æ‰§è¡Œâ€ç»™å®šè¿åŠ¨æŒ‡ä»¤çš„ä»»åŠ¡ï¼Œä½†åœ¨â€œæ‰§è¡Œä»€ä¹ˆâ€å’Œâ€œåœ¨ä½•å¤„æ‰§è¡Œâ€çš„æ›´é«˜å±‚å†³ç­–ä¸è§„åˆ’æ–¹é¢èƒ½åŠ›æœ‰é™ã€‚æœªæ¥çš„æ”¹è¿›å°†ä¸»è¦å›´ç»•**èµ‹äºˆæœºå™¨äººç¯å¢ƒæ„ŸçŸ¥ã€ä»»åŠ¡ç†è§£å’Œé•¿æœŸè‡ªä¸»å†³ç­–èƒ½åŠ›**å±•å¼€ï¼ŒåŒæ—¶æŒç»­ä¼˜åŒ–åŸºç¡€è·Ÿè¸ªçš„é²æ£’æ€§ã€æ•ˆç‡ä¸æ³›åŒ–è¾¹ç•Œã€‚",
            "timestamp": "2026-02-03T15:59:35.278708",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T15:27:43Z",
    "created_at": "2026-02-03T15:53:10.409041",
    "updated_at": "2026-02-03T15:53:10.409048"
}