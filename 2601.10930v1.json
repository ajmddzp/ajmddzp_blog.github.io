{
    "id": "2601.10930v1",
    "title": "Where to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation",
    "authors": [
        "Zhixian Xie",
        "Yu Xiang",
        "Michael Posa",
        "Wanxin Jin"
    ],
    "abstract": "æ¥è§¦ä¸°å¯Œçš„çµå·§æ“ä½œé¢ä¸´çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜åœ¨äºï¼Œéœ€è¦åŒæ—¶å¯¹å‡ ä½•ç»“æ„ã€è¿åŠ¨å­¦çº¦æŸä»¥åŠå¤æ‚éå…‰æ»‘çš„æ¥è§¦åŠ¨åŠ›å­¦è¿›è¡Œè”åˆæ¨ç†ã€‚ç«¯åˆ°ç«¯çš„è§†è§‰è¿åŠ¨ç­–ç•¥ç»•è¿‡äº†è¿™ç§ç»“æ„åŒ–æ¨ç†ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡æ•°æ®ï¼Œä»ä»¿çœŸåˆ°ç°å®çš„è¿ç§»æ•ˆæœè¾ƒå·®ï¼Œä¸”åœ¨ä¸åŒä»»åŠ¡/å®ä½“é—´çš„æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„æ´è§æ¥åº”å¯¹è¿™äº›å±€é™ï¼šçµå·§æ“ä½œæœ¬è´¨ä¸Šæ˜¯åˆ†å±‚çš„â€”â€”åœ¨é«˜å±‚çº§ï¼Œæœºå™¨äººå†³å®šæ¥è§¦ä½ç½®ï¼ˆå‡ ä½•ï¼‰å’Œç‰©ä½“ç§»åŠ¨æ–¹å¼ï¼ˆè¿åŠ¨å­¦ï¼‰ï¼›åœ¨ä½å±‚çº§ï¼Œå®ƒé€šè¿‡æ¥è§¦åŠ¨åŠ›å­¦ç¡®å®šå¦‚ä½•å®ç°è¯¥è®¡åˆ’ã€‚åŸºäºè¿™ä¸€æ´è§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚å¼ºåŒ–å­¦ä¹ -æ¨¡å‹é¢„æµ‹æ§åˆ¶æ¡†æ¶ï¼Œå…¶ä¸­é«˜å±‚å¼ºåŒ–å­¦ä¹ ç­–ç•¥é¢„æµ‹æ¥è§¦æ„å›¾ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„æ¥å£ï¼Œç”¨äºæŒ‡å®šï¼ˆiï¼‰ç‰©ä½“è¡¨é¢æ¥è§¦ä½ç½®å’Œï¼ˆiiï¼‰æ¥è§¦åç‰©ä½“å±‚çº§å­ç›®æ ‡ä½å§¿ã€‚åœ¨æ­¤æ¥è§¦æ„å›¾æ¡ä»¶ä¸‹ï¼Œä½å±‚çº§çš„éšå¼æ¥è§¦æ¨¡å‹é¢„æµ‹æ§åˆ¶ä¼˜åŒ–å±€éƒ¨æ¥è§¦æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨æ¥è§¦åŠ¨åŠ›å­¦è¿›è¡Œé‡æ–°è§„åˆ’ï¼Œç”Ÿæˆèƒ½å¤Ÿç¨³å¥é©±åŠ¨ç‰©ä½“æœå‘å„å­ç›®æ ‡çš„æœºå™¨äººåŠ¨ä½œã€‚æˆ‘ä»¬åœ¨éæŠ“å–ä»»åŠ¡ä¸Šè¯„ä¼°è¯¥æ¡†æ¶ï¼ŒåŒ…æ‹¬å‡ ä½•æ³›åŒ–æ¨åŠ¨å’Œç‰©ä½“ä¸‰ç»´é‡å®šå‘ã€‚è¯¥æ¡†æ¶ä»¥æ˜¾è‘—å‡å°‘çš„æ•°æ®é‡ï¼ˆæ¯”ç«¯åˆ°ç«¯åŸºçº¿å°‘10å€ï¼‰å®ç°äº†æ¥è¿‘100%çš„æˆåŠŸç‡ï¼Œå…·æœ‰é«˜åº¦ç¨³å¥çš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†é›¶æ ·æœ¬ä»¿çœŸåˆ°ç°å®çš„è¿ç§»ã€‚",
    "url": "https://arxiv.org/abs/2601.10930v1",
    "html_url": "https://arxiv.org/html/2601.10930v1",
    "html_content": "Where to Touch, How to Contact: Hierarchical RLâ€“MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation\nZhixian Xie\nYu Xiang\nMichael Posa\nWanxin Jin\nAbstract\nA key challenge in contact-rich dexterous manipulation is the need to jointly reason over geometry, kinematic constraints, and intricate, nonsmooth contact dynamics. End-to-end visuomotor policies bypass this structure, but often require large amounts of data, transfer poorly from simulation to reality, and generalize weakly across tasks/embodiments. We address those limitations by leveraging a simple insight: dexterous manipulation is inherently hierarchicalâ€”at a high level, a robot decides where to touch (geometry) and move the object (kinematics); at a low level it determines how to realize that plan through contact dynamics. Building on this insight, we propose a hierarchical RLâ€“MPC framework in which a high-level reinforcement learning (RL) policy predicts a contact intention, a novel object-centric interface that specifies (i) an object-surface contact location and (ii) a post-contact object-level subgoal pose. Conditioned on this contact intention, a low-level contact-implicit model predictive control (MPC) optimizes local contact modes and replans with contact dynamics to generate robot actions that robustly drive the object toward each subgoal. We evaluate the framework on non-prehensile tasks, including geometry-generalized pushing and object 3D reorientation. It achieves near-100% success with substantially reduced data (10\nÃ—\n\\times\nless than end-to-end baselines), highly robust performance, and zero-shot sim-to-real transfer.\nI\nIntroduction\nDexterous manipulation is a hallmark skill of intelligent robots. Recent research has made significant progress by training robot policies either from demonstrations, e.g., imitation learning\n[\n65\n,\n8\n]\nand vision language action model\n[\n4\n,\n26\n,\n15\n]\n, or from robot experience, e.g., reinforcement learning\n[\n42\n,\n63\n]\n. These approaches typically learn a end-to-end policy from robot observations to low-level motor commands. While\nend-to-end methods can achieve impressive performance, such as emergent generalization\n[\n51\n]\nand robustness\n[\n30\n]\n, they often face practical challenges, such as data scarcity, sim-to-real gap, and lack of task or embodiment generalizability.\nYet, human manipulation does not necessarily use a monolithic end-to-end motor policy\n[\n37\n,\n17\n]\n. For instance, when picking up a hammer to strike a nail,we do not directly infer low-level fingerâ€“joint commands from raw perception. Instead, we reason hierarchically: at a higher level, we decide\nwhere\nto contact (e.g., grasping the handle) and\nwhat\nobject-level outcome to achieve (e.g., aligning the head with the nail); at lower level, we determine\nhow\nto realize this plan guided by our intuitive physics, leveraging interaction strategies such as rolling, sliding, sticking, and disengagement. This hierarchical decomposition enables humans to combine semantic and geometric understanding and physics intuition, yielding efficient and robust manipulation control.\nIn this work, we propose an analogous hierarchical reasoning framework for robotic contact-rich manipulation. The hierarchical framework includes\nâ€¢\nHigher level: geometric-kinematic reasoning.\nGiven a scene observation, the higher-level learning-based policy determines where on the object surface the robot should establish contact (geometry) and where the object should move as a result (kinematics). To formalize this decision, we introduce a novel representation, termed\nContact Intention\n, which specifies (i) contact locations on the object surface and (ii) a post-contact object subgoal for the desired object-level motion progress. We train this higher-level policy using reinforcement learning\nâ€¢\nLower level: contact dynamics reasoning.\nConditioned on the generated contact intention, the dynamic\ncontact planner generates joint-level commands by optimizing with contact dynamics. Leveraging recent advances\nin real-time contact-implicit model predictive control,\nthis level synthesizes local contact modes, including\nrolling, sticking, sliding, and purposeful disengagement, at\nthe suggested contact location, robustly moving\nthe object to the planned subgoal.\nThe two levels forms a hierarchical RLâ€“MPC framework that offers the following benefits:\n(I) Significant data efficiency for geometry-aware long-horizon manipulation.\nBy delegating contact dynamics reasoning to MPC optimization, the framework avoids inefficient learning over non-smooth contact dynamics. This allows the learning policy to focus on geometry and kinematics, enabling more efficient learning of long-horizon contact behaviors.\n(II) Robust performance enabled by contact implicit MPC.\nThe low-level MPC replans at high frequency (\nâˆ¼\n\\sim\n100â€‰Hz) with contact dynamics, continuously correcting for disturbances and modeling errors. This reduces sensitivity to simulation imperfections and improves robustness in sim-to-real deployment.\n(III) Contact intention interface facilitating sim-to-real transfer.\nThe contact-intention interface decouples the geometric-kinematic reasoning from low-level contact dynamics. This allows training a policy in simulator where geometry/kinematics transfer more reliably than contact dynamics. It makes the learned higher-level RL policy inherently more robust for sim-to-real deployment than an end-to-end policy that must account for hard-to-model contact dynamics.\nIn sum, we highlight our following contributions:\n(1) We introduce\ncontact intention\n, a hierarchical policy interface that decomposes dexterous manipulation into geometricâ€“kinematic reasoning (where to touch) and contact dynamics reasoning (how to contact). It enables data-efficient, geometry-aware manipulation with robust sim-to-real transfer.\n(2) We develop a hierarchical RLâ€“MPC framework built on the contact intention interface. The higher-level RL policy predicts the contact intention, which then parameterizes a lower-level MPC for contact dynamics reasoning. This framework demonstrated state-of-the-art performance in data efficiency, success rate, robustness for geometry-aware long-horizon non-prehensile manipulation.\n(3) We introduce multiple design novelties for higher-level RL policy training, including (i) a\ntri-component\nobject-centric representation space that jointly encodes object geometry, task progress, and collision feasibility, and (ii) a\ndual-branch\npolicy network architecture that captures both local geometry and global kinematic context. These designs enable a\nunified\npolicy to learn non-prehensile manipulation across diverse objects and to generalize to unseen geometries.\nII\nRelated Works\nII-A\nRL-MPC Hierarchy in Robot Manipulation\nThe use of RLâ€“MPC architectures is extensive in robotic locomotion\n[\n61\n,\n64\n,\n6\n,\n23\n,\n19\n]\n, where the primary objective is to achieve agile, adaptive, and diverse motor behaviors by combining learned high-level decision making with model-based control. This paradigm has more recently been transferred to robotic manipulation, where adaptability and generalization are crucial. For instance, in\n[\n47\n,\n33\n]\n, reinforcement learning is employed to infer the latent dynamics of deformable objects, which are subsequently incorporated into MPC or adaptive control for accurate manipulation. Other works\n[\n40\n,\n34\n]\nleverage RL to assist with learning contact-rich dynamics models and to provide a policy warm-start for sampling-based MPC solvers. In another line of research\n[\n3\n,\n68\n]\n, RL predicts intermediate end-effector goals that are tracked by a low-level MPC controller, thereby ensuring feasibility and safety during contact-rich tasks. Unlike prior uses of RL as auxiliary guidance or trajectory prediction, we introduce a tightly coupled RLâ€“MPC hierarchy enabled by a novel\ncontact intention\ninterface for contact-rich manipulation. This design allows RL to reason over object geometry and kinematics while MPC handles high-frequency dynamic contact strategies, together enabling robust and dexterous manipulation.\nII-B\nNon-Prehensile Manipulation\nSince the proposed method is primarily applied to non-prehensile manipulation, we review the relevant prior work below.\nNon-prehensile manipulation\n[\n36\n]\nis defined as manipulating objects without explicitly grasping them. Classic work in non-prehensile manipulation studied the mechanics and control of the manipulator-object motion primitives\n[\n35\n,\n1\n]\n. However, motion primitives\nface scalability issue in the open-ended world. Recent effort attacks the problem through contact-implicit planning\n[\n38\n,\n39\n,\n62\n,\n55\n,\n41\n,\n7\n,\n27\n,\n2\n,\n60\n,\n5\n]\n, which allows a robot to automatically discover contact modes through optimization with contact dynamics. Learning methods have also been used in non-prehensile manipulation, such as the work based on reinforcement learning\n[\n66\n,\n9\n,\n10\n,\n29\n,\n32\n]\n, imitation learning\n[\n8\n,\n45\n,\n67\n,\n44\n,\n53\n,\n54\n]\nor both\n[\n49\n]\n. From method structure perspective, our work is most relevant to the recent work of PushAnything\n[\n5\n]\n, HiDex\n[\n7\n]\n, HACMan\n[\n66\n]\nand HDP\n[\n54\n]\n. PushAnything samples contact locations and evaluates them via MPC; HiDex uses MCTS for contact selection followed by path planning; HACMan jointly learns contact selection and motion in a single RL policy; and HDP employs a diffusion-based hierarchy for contact prediction and trajectory generation. In contrast, our method introduces a tightly coupled RLâ€“MPC hierarchy in which the learned decision goes beyond contact point selection to include an object-level subgoal, and the predicted contact intention directly parameterizes the low-level MPC controller.\nII-C\nRobot Manipulation Based on Affordance\nOur work is also related to affordance-based manipulation, where affordance describes which object-surface regions are suitable for interaction with the robot. Substantial research has focused on predicting and leveraging affordance maps for grasping tasks\n[\n18\n,\n16\n,\n57\n,\n11\n,\n12\n,\n24\n,\n28\n]\n. Recently, foundation models with strong semantic reasoning have further been used to infer affordance for robotic tasks\n[\n21\n,\n22\n,\n50\n,\n31\n,\n48\n,\n52\n]\n.\nAffordance prediction alone only indicates\nwhere\nto make contact and lacks guidance on\nhow\nto act after contact, while remaining static during manipulation. Our method addresses this by dynamically predicting contact intentions online and coupling them with a physics-based planner to generate adaptive, physically grounded non-prehensile motions.\nFigure 2\n:\nIllustration of the RL-MPC hierarchical framework\nIII\nContact Intention and Hierarchical RL-MPC Framework\nIII-A\nContact Intention for Contact-Rich Manipulation\nConsider\nn\nn\nactive\ncontact points (general end effectors) on a robot that can be intentionally actuated to interact with an object. For example,\nn\n=\n1\nn=1\nfor a robot arm with a stick-like end-effector, and\nn\n=\n4\nn=4\nfor the fingertips of a four-fingered robotic hand. We distinguish these active contact points from passive ones that many arise without deliberate interaction, e.g., objectâ€“environment (or other robot links) contacts.\nThe goal of manipulation is for the robot to purposefully engage its\nn\nn\nactive contact points to interact with an object through contact and drive the object to a target pose\nğ’’\nÂ¯\ntarget\n\\boldsymbol{\\bar{q}}^{\\text{target}}\n. For this,\nwe define\nContact Intention\nas a higher-level action:\nğˆ\n=\n(\nğ’„\nÂ¯\n1\n,\nğ’„\nÂ¯\n2\n,\nâ€¦\n,\nğ’„\nÂ¯\nn\n,\nğ’’\nÂ¯\nobj\n)\nâˆˆ\nğ’¢\nn\nÃ—\nSE\nâ€‹\n(\n3\n)\n,\n\\mathbf{I}=(\\boldsymbol{\\bar{c}}^{1},\\boldsymbol{\\bar{c}}^{2},\\ldots,\\boldsymbol{\\bar{c}}^{n},\\,\\boldsymbol{\\bar{q}}^{\\text{obj}})\\in\\mathcal{G}^{n}\\times\\mathrm{SE}(3),\n(1)\nHere,\n(\nğ’„\nÂ¯\n1\n,\nğ’„\nÂ¯\n2\n,\nâ€¦\n,\nğ’„\nÂ¯\nn\n)\n(\\boldsymbol{\\bar{c}}^{1},\\boldsymbol{\\bar{c}}^{2},\\ldots,\\boldsymbol{\\bar{c}}^{n})\nare the corresponding contact locations on the object surface. Each location\nğ’„\nÂ¯\ni\nâˆˆ\nğ’¢\n\\boldsymbol{\\bar{c}}^{i}\\in\\mathcal{G}\n,\ni\n=\n1\n,\n.\n.\n,\nn\ni=1,..,n\n, specifies an\ninteraction anchor\non the object surface\nğ’¢\n\\mathcal{G}\nfor an active contact point\ni\ni\nto engage with.\nğ’’\nÂ¯\nobj\nâˆˆ\nSE\nâ€‹\n(\n3\n)\n\\boldsymbol{\\bar{q}}^{\\text{obj}}\\in\\mathrm{SE}(3)\nis the post-contact subgoal pose of the object. It specifies the desired object-level outcome for the robot-object interaction at the suggested contact locations. The subgoal may not coincide with the final target\nğ’’\nÂ¯\ntarget\n\\boldsymbol{\\bar{q}}^{\\text{target}}\n; it may be an intermediate object pose that provides meaningful progress toward\nğ’’\nÂ¯\ntarget\n\\boldsymbol{\\bar{q}}^{\\text{target}}\n. In this paper, we only consider non-prehensile manipulation using a stick-like end-effector attached to a robot arm. Thus,\nn\n=\n1\nn=1\n.\nIII-B\nOverview of RL-MPC Hierarchical Framework\nOur proposed RL-MPC hierarchical framework is shown in Fig.\n2\n. It consists of a higher-level policy for object-centric geometry and kinematics reasoning and a lower-level policy for contact dynamics reasoning.\nThe higher-level policy, trained using RL, predicts contact intention, which parameterizes the lower-level policy. The lower-level policy online optimize the robot action through local contact-implicit MPC.\nHigher-level RL policy for object-centric geometric-kinematic reasoning\nWe formulate contact-intention prediction as a POMDP, where the action space\nğ’œ\n\\mathcal{A}\nis the proposed contact intention space\nğ’œ\n=\nğ’¢\nn\nÃ—\nSE\nâ€‹\n(\n3\n)\n\\mathcal{A}=\\mathcal{G}^{n}\\times\\mathrm{SE}(3)\n. A higher-level policy\nÏ€\n\\pi\nmaps observations to contact intentions, which are executed by the lower-level MPC. At each decision step\nk\nk\n, the policy gets an observation\nğ’\nk\n\\boldsymbol{o}_{k}\nand predicts contact intention\nğˆ\nk\n=\nÏ€\nâ€‹\n(\nğ’\nk\n)\nâˆˆ\nğ’œ\n\\mathbf{I}_{k}=\\pi(\\boldsymbol{o}_{k})\\in\\mathcal{A}\n. The predicted\nğˆ\nk\n\\mathbf{I}_{k}\nwill parameterize the lower-level MPC for local contact dynamics reasoning. We train\nÏ€\n\\pi\nin simulation, which will be detailed in Section\nVI\n.\nLower-level contact-implicit MPC for contact dynamics reasoning\nFor contact dynamics reasoning, we use the complementarity-free model predictive control (ComFree-MPC) for its robustness and high speed\n[\n25\n]\n. Given a contact intention\nğˆ\nk\n=\n(\nğ’„\nÂ¯\nk\n1\n,\nğ’„\nÂ¯\nk\n2\n,\nâ€¦\n,\nğ’„\nÂ¯\nk\nn\n,\nğ’’\nÂ¯\nk\nobj\n)\n\\mathbf{I}_{k}=(\\boldsymbol{\\bar{c}}^{1}_{k},\\boldsymbol{\\bar{c}}^{2}_{k},...,\\boldsymbol{\\bar{c}}^{n}_{k},\\boldsymbol{\\bar{q}}^{\\text{obj}}_{k})\nfrom the higher level, ComFree-MPC reasons about the rich local contact strategies at the suggested contact location\n(\nğ’„\nÂ¯\nk\n1\n,\nğ’„\nÂ¯\nk\n2\n,\nâ€¦\n,\nğ’„\nÂ¯\nk\nn\n)\n(\\boldsymbol{\\bar{c}}^{1}_{k},\\boldsymbol{\\bar{c}}^{2}_{k},...,\\boldsymbol{\\bar{c}}^{n}_{k})\nto move the object towards the subgoal\nğ’’\nÂ¯\nk\nobj\n\\boldsymbol{\\bar{q}}^{\\text{obj}}_{k}\n. Specifically, at each MPC step\nt\nt\n, ComFree-MPC predicts\nH\nH\nsteps ahead through complementarity-free contact dynamics and optimizes the best robot action sequence for moving the object to the subgoal\nğ’’\nÂ¯\nk\nobj\n\\boldsymbol{\\bar{q}}^{\\text{obj}}_{k}\n.\nThe optimization is written as\nmin\nğ’–\n0\n:\nH\nâˆ’\n1\n\\displaystyle\\min_{\\boldsymbol{u}_{0:H{-}1}}\nâˆ‘\nt\n=\n0\nH\nâˆ’\n1\nâˆ‘\ni\nL\nâ€‹\n(\nğ©\nt\nee\n,\ni\n,\nğ’„\nÂ¯\nk\ni\n)\n+\nV\nâ€‹\n(\nğ’’\nH\nobj\n,\nğ’’\nÂ¯\nk\nobj\n)\n\\displaystyle\\sum\\nolimits_{t=0}^{H-1}\\sum\\nolimits_{i}L(\\mathbf{p}_{t}^{{\\mathrm{ee}},i},\\,\\boldsymbol{\\bar{c}}^{\\,i}_{k})+V(\\boldsymbol{q}^{\\text{obj}}_{H},\\boldsymbol{\\bar{q}}^{\\text{obj}}_{k})\n(2)\ns.t.\nğ’™\nt\n+\n1\n=\nComFree\nâ€‹\n(\nğ’™\nt\n,\nğ’–\nt\n)\n,\nt\n=\n0\n,\n1\n,\nâ€¦\n,\nH\nâˆ’\n1\n\\displaystyle\\boldsymbol{x}_{t+1}=\\texttt{ComFree}(\\boldsymbol{x}_{t},\\boldsymbol{u}_{t}),t=0,1,.,H{-}1\nGiven\nğ’™\n0\n=\nğ’™\nenv\n\\displaystyle\\text{Given}\\quad\\boldsymbol{x}_{0}{=}\\boldsymbol{x}^{\\text{env}}\nHere,\nğ’™\n0\n:\nH\n\\boldsymbol{x}_{0:H}\nis the state of the manipulation system (robot and object).\nğ’–\n0\n:\nH\nâˆ’\n1\n\\boldsymbol{u}_{0:H-1}\nis the robot action input sequence, which is the desired displacement of the active contact points in Cartesian space. The MPC objective includes: (1) a running cost\nL\nL\nwhich encourages the robotâ€™s active contact points\n(\nğ©\nt\nee\n,\ni\n)\ni\n=\n1\nn\n(\\mathbf{p}_{t}^{{\\mathrm{ee}},i})_{i=1}^{n}\nto get close to the suggested locations\n(\nğ’„\nÂ¯\nk\ni\n)\ni\n=\n1\nn\n(\\boldsymbol{\\bar{c}}^{i}_{k})_{i=1}^{n}\n. (2) The terminal cost\nV\nV\nencourage the robot to move the object\nğ’’\nH\nobj\n\\boldsymbol{q}^{\\text{obj}}_{H}\n(at the prediction step\nH\nH\n) to the subgoal\nğ’’\nÂ¯\nobj\n\\boldsymbol{\\bar{q}}^{\\text{obj}}\n.\nComFree\nis the complementarity-free contact physics model, enabling fast receding horizon optimization.\nğ’™\n0\n\\boldsymbol{x}_{0}\nis set as the systemâ€™s current physical state at each MPC call. After solving the optimization, only the first control input is sent as the action command to the robot. More details will be given in Section\nV\n.\nMulti-Rate RL-MPC Coupling\nThe ComFree-MPC executes for\nT\nT\nenvironment steps while the contact intention from the higher-level policy remains fixed. Below, we use\nk\nk\nto denote the higher-level decision step for updating contact intentions and\nt\nt\nto denote the lower-level MPC control step.\nIV\nRL for Geometric-Kinematic Reasoning\nWe use RL to train the higher-level policy for geometric-kinematic reasoning. At each decision step, the policy receives an observation and outputs a contact intention. To enable effective policy learning, we design the observation and action spaces, reward, and policy architecture in this section.\nIV-A\nRL Observation Space Design\nFigure 3\n:\nIllustration of the geometric, target and collision components in RL observation space. The green mesh stands for current object pose, and the red mesh stands for the manipulation target.\nAs in Fig.\n3\n, we design the higher-level observation space as a\ntri-component, object-centric\nrepresentation that jointly encodes object geometry, task progress, and collision. This design enables the RL policy to reason about\nobject shape\n,\ntask progress\n, and\ncontact feasibility\nin a unified manner.\nGeometry component\nWe downsample a set of\nN\nN\nkeypoints\nğ’«\n=\n{\nğ’‘\ni\n}\ni\n=\n0\nN\nâˆ’\n1\n\\mathcal{P}=\\{\\boldsymbol{p}_{i}\\}_{i=0}^{N-1}\nfrom object point cloud, defined in the\nobject frame\n. These keypoints serve dual purposes: (i) as an approximation for the object geometry\nğ’¢\n\\mathcal{G}\n, and (ii) as discrete candidates for selecting contact location in contact intention, i.e.,\n(\nğ’„\nÂ¯\nk\n1\n,\nğ’„\nÂ¯\nk\n2\n,\nâ€¦\n,\nğ’„\nÂ¯\nk\nn\n)\nâŠ†\nğ’«\n(\\boldsymbol{\\bar{c}}^{1}_{k},\\boldsymbol{\\bar{c}}^{2}_{k},...,\\boldsymbol{\\bar{c}}^{n}_{k})\\subseteq\\mathcal{P}\n.\nTarget component\nTo encode the target-aware observation, we define the\nkeypoint goal flow\nin the object frame,\nÎ”\nâ€‹\nğ’«\nk\n=\n{\nğœ¹\ni\n,\nk\n|\nğœ¹\ni\n,\nk\n=\nğ’‘\ni\n,\nk\ntarget\nâˆ’\nğ’‘\ni\n}\ni\n=\n0\nN\nâˆ’\n1\n\\Delta\\mathcal{P}_{k}=\\{\\boldsymbol{\\delta}_{i,k}|\\boldsymbol{\\delta}_{i,k}=\\boldsymbol{p}_{i,k}^{\\text{target}}-\\boldsymbol{p}_{i}\\}_{i=0}^{N-1}\n,\nwhere\nğ’‘\ni\n,\nk\ntarget\n\\boldsymbol{p}_{i,k}^{\\text{target}}\ndenotes the position of the same keypoint\nğ’‘\ni\n\\boldsymbol{p}_{i}\nwhen the object is at the target, expressed in the current object frame at decision step\nk\nk\n. The keypoint goal flow\nÎ”\nâ€‹\nğ’«\nk\n\\Delta\\mathcal{P}_{k}\nrepresents the object-frame displacement of all keypoints from the current object pose to the target. Encoding keypoint goals in the object frame is crucial for stable and geometry-consistent policy learning. Otherwise, the discrepency between body and world frames would force the policy to implicitly learn object translations and rotations, significantly increasing the learning burden.\nCollision component\nTo inform contact feasibility, we include\nper-keypoint clearance distances\nas part of the observation.\nFor each keypoint\nğ’‘\ni\n\\boldsymbol{p}_{i}\nat decision step\nk\nk\n, we compute its minimum Euclidean distance\nd\ni\n,\nk\nd_{i,k}\nto the environment (e.g., ground and walls). We collect these distances as\nğ’Ÿ\nk\n=\n{\nd\ni\n,\nk\n}\ni\n=\n0\nN\nâˆ’\n1\n\\mathcal{D}_{k}=\\{d_{i,k}\\}_{i=0}^{N-1}\n.\nğ’Ÿ\nk\n\\mathcal{D}_{k}\nencode the\ncontact feasibility\nof each keypoint: keypoints with small clearance (\nd\ni\n,\nk\nâ‰ˆ\n0\nd_{i,k}\\approx 0\n) indicate it is in contact with their surrounding environment, thus are treated as invalid candidates to be a contact location. Including\nğ’Ÿ\nk\n\\mathcal{D}_{k}\nenables RL policy to avoid selecting contact locations that are physically blocked by the environment. For example, for a cube resting on a table, keypoints on the bottom face are infeasible contact locations for the end-effectors.\nIn sum, the tri-component, object-centric observation at the higher-level decision step\nk\nk\nis the tuple\nğ’\nk\n=\n(\nğ’«\n,\nÎ”\nâ€‹\nğ’«\nk\n,\nğ’Ÿ\nk\n)\n.\n\\boldsymbol{o}_{k}=\\big(\\mathcal{P},\\,\\Delta\\mathcal{P}_{k},\\,\\mathcal{D}_{k}\\big).\n(3)\nWe provide the ablation and comparison study for our above design in Section\nVII\n.\nFigure 4\n:\nThe dual-branch architecture design for the higher-level RL policy that predicts contact intention.\nIV-B\nRL Action Space Design\nThe higher-level RL policy receives the observation\nğ’\n\\boldsymbol{o}\nand selects a contact intention\nğˆ\n=\n(\nğ’„\nÂ¯\n1\n,\nğ’„\nÂ¯\n2\n,\nâ€¦\n,\nğ’„\nÂ¯\nn\n,\nğ’’\nÂ¯\nobj\n)\n\\mathbf{I}=(\\boldsymbol{\\bar{c}}^{1},\\boldsymbol{\\bar{c}}^{2},...,\\boldsymbol{\\bar{c}}^{n},\\boldsymbol{\\bar{q}}^{\\text{obj}})\n. Here,\neach contact location is selected from the keypoint set\nğ’«\n\\mathcal{P}\n, i.e.,\nğ’„\nÂ¯\ni\nâˆˆ\nğ’«\n\\boldsymbol{\\bar{c}}^{i}\\in\\mathcal{P}\n,\ni\n=\n1\n,\n2\n,\nâ€¦\n,\nn\ni=1,2,...,n\n.\nInstead of predicting object subgoals in\nSE\nâ€‹\n(\n3\n)\n\\mathrm{SE}(3)\n, which is often inefficient for\nRL exploration to make meaningful progress toward the\nfinal goal, the higher-level RL policy selects MPC weights\n(\nw\npos\n,\nw\nori\n)\n(w_{\\text{pos}},w_{\\text{ori}})\nto implicitly define the subgoal:\nV\nâ€‹\n(\nğ’’\nobj\n,\nğ’’\nÂ¯\nobj\n)\n=\nw\npos\nâ€‹\ne\npos\nâ€‹\n(\nğ©\nobj\n,\nğ©\nÂ¯\ntarget\n)\n+\nw\nori\nâ€‹\ne\nori\nâ€‹\n(\nğ’“\nobj\n,\nğ’“\nÂ¯\ntarget\n)\n,\nV(\\boldsymbol{q}^{\\text{obj}},\\boldsymbol{\\bar{q}}^{\\text{obj}}){=}w_{\\text{pos}}\\,e_{\\text{pos}}(\\mathbf{p}^{\\text{obj}},\\mathbf{\\bar{p}}^{\\text{target}}){+}w_{\\text{ori}}\\,e_{\\text{ori}}(\\boldsymbol{r}^{\\text{obj}},\\boldsymbol{\\bar{r}}^{\\text{target}}),\n(4)\nwhere\ne\npos\ne_{\\text{pos}}\nand\ne\nori\ne_{\\text{ori}}\nmeasure the objectâ€™s position and orientation errors to the final target. In the simplest case,\n(\nw\npos\n,\nw\nori\n)\nâˆˆ\n{\n(\n1\n,\n0\n)\n,\n(\n0\n,\n1\n)\n,\n(\n1\n,\n1\n)\n}\n(w_{\\text{pos}},w_{\\text{ori}})\\in\\{(1,0),(0,1),(1,1)\\}\ndetermines the subgoal to be target position-only, orientation-only, or exactly target itself, respectively. We use discrete sets\nğ’²\nâ€‹\npos\nÃ—\nğ’²\nâ€‹\nori\n\\mathcal{W}{\\text{pos}}\\times\\mathcal{W}{\\text{ori}}\nto express richer trade-offs. In sum, the higher-level RL policy predicts\n(\nğ’„\nÂ¯\n1\n,\nğ’„\nÂ¯\n2\n,\nâ€¦\n,\nğ’„\nÂ¯\nn\n,\nw\npos\n,\nw\nori\n)\n=\nÏ€\nâ€‹\n(\nğ’\n)\n.\n(\\boldsymbol{\\bar{c}}^{1},\\boldsymbol{\\bar{c}}^{2},...,\\boldsymbol{\\bar{c}}^{n},w_{\\text{pos}},w_{\\text{ori}})=\\pi(\\boldsymbol{o}).\n(5)\nIV-C\nRL Reward\nWe train the higher-level RL policy using a reward\nr\nk\n=\n(\nw\n1\nâ€‹\nr\ndense\n+\nw\n2\nâ€‹\nr\ntarget\n)\nâ€‹\n(\n1\n+\nr\nfeasible\n)\n+\nw\n3\nâ€‹\nr\nfeasible\n,\n\\displaystyle r_{k}=\\big(w_{1}r_{\\text{dense}}+w_{2}r_{\\text{target}}\\big)(1+r_{\\text{feasible}})+w_{3}r_{\\text{feasible}},\n(6)\nwhere\nw\n1\n,\nw\n2\n,\nw\n3\nw_{1},w_{2},w_{3}\nare the weights for the negative dense keypoint flow reward (\nr\ndense\n=\nâˆ’\n1\nN\nâ€‹\nâˆ‘\nğœ¹\ni\n,\nk\nâˆˆ\nÎ”\nâ€‹\nğ’«\nk\nâˆ¥\nğœ¹\ni\n,\nk\nâˆ¥\nr_{\\text{dense}}{=}-\\frac{1}{N}\\sum_{\\boldsymbol{\\delta}_{i,k}\\in\\Delta\\mathcal{P}_{k}}\\lVert\\boldsymbol{\\delta}_{i,k}\\rVert\n), sparse target-reaching reward (\nr\ntarget\n=\n1\nr_{\\text{target}}{=}1\nat final target), and negative contact feasibility reward\nr\nfeasible\n=\nâˆ’\n1\nr_{\\text{feasible}}{=}-1\nwhen the contact location is invalid (e.g., contact location on the bottom surface); otherwise\nr\nfeasible\n=\n0\nr_{\\text{feasible}}=0\n.\nIV-D\nArchitecture of RL Policy Network\nOur higher-level RL policy is trained using an actorâ€“critic framework. The policy network architecture is shown in Fig.\n4\n, which adopts a\ndual-branch design\n. At decision step\nk\nk\n, the policy network takes as input per-keypoint observations\nğ’\nk\n=\n{\nğ’‘\ni\n,\nğœ¹\ni\n,\nk\n,\nd\ni\n,\nk\n}\ni\n=\n0\nN\nâˆ’\n1\n\\boldsymbol{o}_{k}=\\{\\boldsymbol{p}_{i},\\boldsymbol{\\delta}_{i,k},d_{i,k}\\}_{i=0}^{N-1}\n,\nwhich encode each keypointâ€™s body-frame location, goal displacement, and contact feasibility.\nThe input first goes through\ngeometric branch\n: it processes per-keypoint features using a segmentation-style PointNet++ encoder to produce latent features\nğ’›\ni\n\\boldsymbol{z}_{i}\n, which are mapped to logits for contact location selection. Meantime, the input also goes through\nglobal kinematic branch\n, which aggregates all keypoint inputs into a single object-centric embedding\nğ’ˆ\n\\boldsymbol{g}\nvia a classification-style PointNet++ encoder. Linear heads then predict categorical logits over MPC cost weights\n(\nğ’²\npos\n,\nğ’²\nori\n)\n(\\mathcal{W}_{\\text{pos}},\\mathcal{W}_{\\text{ori}})\nand a scalar value estimate for critic learning. This architecture efficiently combines local geometric reasoning with global object-level context, enabling the policy to select feasible contact locations and implicitly specify object subgoals.\nV\nMPC for Local Dynamic Contact Planning\nWe use ComFree-MPC for lower-level contact dynamics reasoning, conditioned on RL-predicted contact intentions. By leveraging an efficient closed-form contact physics model, ComFree-MPC performs high-speed receding-horizon optimization, enabling the system to reason over contact strategies (rolling, sliding, sticking, and making/breaking) and drive the object toward the subgoal. We detail the formulation below.\nV-A\nComplementarity-Free Contact Dynamics\nWe adopt the quasi-dynamic complementarity-free contact model from\n[\n25\n]\nfor our relatively slow non-prehensile manipulation task, given by\nM\nâ€‹\nğ’—\n+\n=\nh\nâ€‹\nğ‰\nâ€‹\n(\nğ’–\n)\n+\nh\nâ€‹\nJ\n~\nâŠ¤\nâ€‹\nğ€\n,\n\\displaystyle M\\boldsymbol{v}^{+}=h\\boldsymbol{\\tau}(\\boldsymbol{u})\\;+\\;h\\tilde{J}^{\\top}\\boldsymbol{\\lambda},\n(7)\nğ€\n:=\nmax\nâ€‹\n(\nâˆ’\nK\nâ€‹\n(\nh\n2\nâ€‹\nJ\n~\nâ€‹\nM\nâˆ’\n1\nâ€‹\nğ‰\nâ€‹\n(\nğ’–\n)\n+\nÏ•\n~\n)\n,\n0\n)\n,\n\\displaystyle\\boldsymbol{\\lambda}=\\mathrm{max}\\Big(-K\\bigl(h^{2}\\tilde{J}M^{-1}\\boldsymbol{\\tau}(\\boldsymbol{u})+\\tilde{\\boldsymbol{\\phi}}\\bigr),0\\Big),\nwhere\nh\nh\nis the time step and\nğ’—\n+\n\\boldsymbol{v}^{+}\nis the next-step system velocity.\nM\n=\ndiag\nâ€‹\n(\nM\no\n,\nM\nr\n)\nM=\\text{diag}(M_{o},M_{r})\nis the mass matrix, with object inertia\nM\no\nM_{o}\nand robot stiffness\nM\nr\n=\nh\n2\nâ€‹\ndiag\nâ€‹\n(\nK\nr\n,\nâ€¦\n,\nK\nr\n)\nM_{r}=h^{2}\\text{diag}(K_{r},\\dots,K_{r})\nfor\nn\nn\nactive contact points. The force vector\nğ‰\nâ€‹\n(\nğ’–\n)\n=\n[\nğ‰\no\n,\nğ‰\nr\nâ€‹\n(\nğ’–\n)\n]\n\\boldsymbol{\\tau}(\\boldsymbol{u})=[\\boldsymbol{\\tau}_{o},\\boldsymbol{\\tau}_{r}(\\boldsymbol{u})]\nincludes robot gravitational force\nğ‰\no\n\\boldsymbol{\\tau}_{o}\nand robot actuation force\nğ‰\nr\nâ€‹\n(\nğ’–\n)\n\\boldsymbol{\\tau}_{r}(\\boldsymbol{u})\n. We assume robot is in operational space impedance control and moves slowly. Thus, its damping and inertial effects are neglected, and end-effector force can be approximated as a spring model with stiffness\nK\nr\nK_{r}\n:\nğ‰\nr\nâ€‹\n(\nğ’–\n)\n=\nK\nr\nâ€‹\nğ’–\n\\boldsymbol{\\tau}_{r}(\\boldsymbol{u})=K_{r}\\boldsymbol{u}\n. Here, the input\nğ’–\n\\boldsymbol{u}\nis desired end-effector displacement. The contact-force vector\nğ€\n\\boldsymbol{\\lambda}\nis produced by a springâ€“damper model on the dual cones of the Coulomb friction cones, with\nJ\n~\n\\tilde{J}\nand\nÏ•\n~\n\\tilde{\\boldsymbol{\\phi}}\nthe Jacobian and signed distances for all candidate dual cones, and\nK\nK\nthe dual-cone stiffness. Different\ncontact modes, e.g., sticking, sliding, and separation, is captured by\ndifferent penetration regimes across facets of the dual cones.\nWith\nğ’—\n+\n\\boldsymbol{v}^{+}\n, the state after one-step forwarding of the contact dynamics is\nğ’™\n+\n=\nğ’™\nâŠ•\nh\nâ€‹\nğ’—\n+\n,\n\\boldsymbol{x}^{+}{=}\\boldsymbol{x}\\oplus h\\boldsymbol{v}^{+},\nwhere\nâŠ•\n\\oplus\nstands for the the integration of system positions with velocities. The details of this closed-form contact model can be found in\n[\n25\n]\n.\nV-B\nMPC Cost Function\nIn the ComFree-MPC (\n2\n), the\nrunning cost\nis defined as\nâˆ‘\ni\n=\n1\nn\nL\nâ€‹\n(\nğ©\nt\nee\n,\ni\n,\nğ’„\nÂ¯\nk\ni\n)\n=\nâˆ‘\ni\n=\n1\nn\nw\nc\nâ€‹\nâˆ¥\nğ©\nt\nee\n,\ni\nâˆ’\nT\nobj\nw\nâ€‹\nğ’„\nÂ¯\ni\nâˆ¥\n2\n,\n\\sum\\nolimits_{i=1}^{n}L(\\mathbf{p}_{t}^{{\\mathrm{ee}},i},\\boldsymbol{\\bar{c}}^{i}_{k})=\\sum\\nolimits_{i=1}^{n}\\,w_{c}\\big\\lVert\\mathbf{p}_{t}^{{\\mathrm{ee}},i}-T_{\\text{obj}}^{w}\\boldsymbol{\\bar{c}}^{i}\\big\\rVert^{2},\n(8)\nwhere\nT\no\nâ€‹\nb\nâ€‹\nj\nw\nT_{obj}^{w}\ntransforms\nğ’„\nÂ¯\ni\n\\boldsymbol{\\bar{c}}^{i}\nto the world frame.\nw\nc\nw_{c}\nis the hyperparameter controlling the strength of this cost. This is to encourage the robot end-effectors to stick to the contact locations from the contact intention.\nThe terminal cost follows the reparameterization form (\n4\n), with\nV\n=\nw\npos\nâ€‹\nâˆ¥\nğ©\nH\nobj\nâˆ’\nğ©\nÂ¯\ntarget\nâˆ¥\n2\n+\nw\nori\nâ€‹\n(\n1\nâˆ’\nâŸ¨\nğ’“\nH\nobj\n,\nğ’“\nÂ¯\ntarget\nâŸ©\n2\n)\n,\nV=w_{\\mathrm{pos}}\\,\\big\\lVert\\mathbf{p}_{H}^{\\text{obj}}-\\mathbf{\\bar{p}}^{\\text{target}}\\big\\rVert^{2}+w_{\\mathrm{ori}}\\,\\big(1{-}\\langle\\boldsymbol{r}_{H}^{\\text{obj}},\\,\\boldsymbol{\\bar{r}}^{\\text{target}}\\rangle^{2}\\big),\n(9)\nwhere\nğ’‘\nH\nobj\n,\nğ’“\nH\nobj\n\\boldsymbol{p}_{H}^{\\text{obj}},\\boldsymbol{r}_{H}^{\\text{obj}}\nare respectively the position and quaternion in predicted object pose\nğ’’\nH\nobj\n\\boldsymbol{q}^{\\text{obj}}_{H}\n.\nğ’‘\nÂ¯\ntarget\n,\nğ’“\nÂ¯\ntarget\n\\boldsymbol{\\bar{p}}^{\\text{target}},\\boldsymbol{\\bar{r}}^{\\text{target}}\nare the position and rotation component in\nğ’’\nÂ¯\ntarget\n\\boldsymbol{\\bar{q}}^{\\text{target}}\n.\nVI\nApplication to Non-Prehensile Manipulation\nWe apply our framework to solve two representative non-prehensile manipulation tasks:\ngeometry-generalized pushing\nand\nobject 3D reorientation\n. Both tasks are performed with a 7-DOF Franka robot arm with a stick end-effector (Fig.\n5\n).\nFigure 5\n:\nTwo non-prehensile manipulation tasks. Left: Geometry-generalized pushing. Right: object 3D reorientation.\nVI-A\nTask Setup\nGeometry-generalized pushing\nThe goal is to learn a\nunified policy\nthat pushes diverse letter-shaped objects from arbitrary initial poses to planar targets. We use 12 English letters (Fig.\nLABEL:fig:intro\n): (E, H, L, N, T, X) for training and (F, I, K, V, Y, Z) held out for generalization test. The task requires long-horizon contact reasoning: sequentially switching contact facets and selecting appropriate contact strategies. Letters vary geometrically, from elongated (I) to branched shapes (K, Y), providing a challenging benchmark for geometry-aware contact-rich manipulation. For each letter, 256 keypoints are uniformly sampled on vertical side surfaces (Fig.\nLABEL:fig:intro\n).\nObject 3D reorientation\nThe goal is to reorient a cube from a random initial pose to a random 3D target. We uniformly sample 256 keypoints on its six faces. This task evaluates the methodâ€™s ability to generate richer contact strategies for precise 3D reorientation beyond planar pushing.\nThe detailed task settings for both the simulation and real world system can be found in Appendix\nIX-A\n.\nVI-B\nTraining Higher-Level Robot-Abstract Policy in Simulation\nFigure 6\n:\nIn the higher-level RL policy training, an abstract end-effector (red dot) replaces full robot arm. The target is half-transparent.\nThe higher-level contact-intention policy predicts object-centric contact locations and kinematic subgoals, so we train it in a simulation environment where the robot end-effector is abstracted a massâ€“springâ€“damper point (Fig.\n6\n). This reduces lower-level MPC complexity, speeding up learning. Despite the notable model gaps, we will later show that our decoupled hierarchical framework enables the trained policy to direct transfer to full robot embodiments in simulation (Section\nVII\n) and real experiments (Section\nVIII\n) with lower-level MPC.\nWe train the higher-level policy using PPO\n[\n46\n]\n. At each episode, object and target poses are randomized (Appendix\nIX-A\n), with one training object sampled per episode for the geometry-generalized pushing task. Episodes end on success or after 64 steps. Details on rewards, hyperparameters, and implementation are in Appendix\nIX-B\n.\nVI-C\nZero-Shot Transfer to Full Robotic Systems\nDeploying the trained higher-level contact-intention policy on a full robot requires adding the robot-specific ComFree-MPC. For one decision step of the RL policy, the lower-level ComFree-MPC moves the robot end effector near the desired contact location and executes dynamic contact replanning and control for multiple control steps. Since the MPC objective is parameterized by the policy, no additional training is needed. Three MPC parameter sets, tuned for simulation, real-world pushing, and real-world 3D reorientation, account for different dynamics; details are in Appendix\nIX-C\n.\nVII\nSimulation Results\nVII-A\nLearning Curves\nVII-A\n1\nTraining in robot-abstract setting\nWe perform five independent training runs per task, evaluating success rates at saved checkpoints with 64 trials each. In geometry-generalized pushing, we report separate curves for seen and unseen letters, randomly sampling objects and start/target poses per trial. For object 3D reorientation, trials are generated similarly. A trial succeeds if the object reaches sufficiently close to the target within 64 RL steps. Pose distributions and success criteria are in Appendix\nIX-A\n, and learning curves with the abstract end-effector are shown in Fig.\n7\n.\n(a)\nGeometry-generalized pushing\n(b)\nObject 3D reorientation\nFigure 7\n:\nLearning curves for both tasks.\nThe results show that the proposed method achieves high success rates with relativily small amount of training data:\n100\n%\nÂ±\n0\n%\n100\\%\\pm 0\\%\nfor seen and\n99.06\n%\nÂ±\n1.25\n%\n99.06\\%\\pm 1.25\\%\nfor unseen letters in pushing, and\n98.75\n%\nÂ±\n1.17\n%\n98.75\\%\\pm 1.17\\%\nfor 3D reorientation, after\nâˆ¼\n\\sim\n40k and\nâˆ¼\n\\sim\n100k RL steps, respectively. The near-overlap of pushing learning curves for seen and unseen objects also indicates strong generalization to novel geometries in the pushing task.\nVII-A\n2\nEvaluation in Full Robotic System\nWe further evaluate the trained policies in a full Franka-arm simulation environment. For each trained checkpoint, we run 5 evaluation trials per letter in the geometry-generalized pushing task and 20 evaluation trials in the object 3D reorientation task.\nThe learned policies achieve consistently high success rates (near\n100\n%\n100\\%\n) while requiring only a small number of high-level RL decision steps to complete each task (approximately 10 on average). Table\nI\nsummarizes the success rates of our method in both the abstract and full-system settings, along with the number of RL decision steps required for task completion in full-system setting. We denote the pushing and 3D reorientation tasks by â€œPâ€ and â€œRâ€, respectively.\nTABLE I\n:\nResult in Simulated Abstract and Full robot Setting\nTask\nSuccess (Abstract)\nSuccess (Full)\nSteps (Full)\nP (seen)\n100\n%\nÂ±\n0\n%\n100\\%\\pm 0\\%\n100\n%\nÂ±\n0\n%\n100\\%\\pm 0\\%\n9.0\nÂ±\n5.4\n9.0\\pm 5.4\nP (unseen)\n99.06\n%\nÂ±\n1.25\n%\n99.06\\%\\pm 1.25\\%\n97.34\n%\nÂ±\n2.5\n%\n97.34\\%\\pm 2.5\\%\n11.2\nÂ±\n8.0\n11.2\\pm 8.0\nR\n98.75\n%\nÂ±\n1.17\n%\n98.75\\%\\pm 1.17\\%\n100\n%\nÂ±\n0\n%\n100\\%\\pm 0\\%\n14.0\nÂ±\n9.5\n14.0\\pm 9.5\nVII-B\nBaseline Comparison\nWe compare our hierarchical RL-MPC method with two baselines: (i) a end-to-end RL policy and (ii) a HACMan-style hierarchical policy without a physics-based MPC layer. In each comparison, five independent training runs are performed, and performance is evaluated using 64 trials per run.\nComparison with End-to-end RL learning\nThe end-to-end policy directly predicts end-effector actions, using an observation that includes object keypoints, goal flow, per-point clearance, and end-effector flow (distance vectors between the end-effector and all keypoints), all in world frame. End effector flow provides spatial context about the robotâ€“object geometry. The policy reuses the global kinematic branch (Section\nIV-D\n) with separate linear heads for action and value prediction.\nWe found that directly using the reward in Section\nIV-C\nleads to the struggling of the end-to-end policy to effectively explore the effect contact with the object. To facilitate object interaction, we add an additional reward term\n0.5\nâ€‹\nexp\nâ¡\n(\nâˆ’\nâ€–\nğ’…\nobj,ee\nâ€–\n2\n/\n0.02\n)\n,\n0.5\\exp\\!\\left(-{\\|\\boldsymbol{d}_{\\text{obj,ee}}\\|^{2}}/{0.02}\\right),\nto encourage the end effector to approach the object, where\nğ’…\nobj,ee\n\\boldsymbol{d}_{\\text{obj,ee}}\ndenotes the distance between the end effector and the object center. Episodes are also terminated early when\nâ€–\nğ’…\nobj,ee\nâ€–\nâ‰¥\n0.25\n\\|\\boldsymbol{d}_{\\text{obj,ee}}\\|\\geq 0.25\n, with a penalty of\nâˆ’\n5\n-5\n, to discourage drifting away from the object.\nWe evaluate the end-to-end and our hierarchical policies on (1) sample efficiency and (2) robustness to environment variations (i.e., sim-to-sim transfer), including external forces, friction, and actuator variations (Appendix\nIX-D\n). Ours use the same lower-level ComFree-MPC in geometry-generalized pushing task without additional tuning.\nFig.\n8\nshows checkpoint success versus high-level RL steps and total end-effector commands. The end-to-end policy converges slowly, reaching\n92.5\n%\nÂ±\n7.55\n%\n92.5\\%\\pm 7.55\\%\nafter\nâˆ¼\n\\sim\n600K RL steps (corresponding to 600K control steps), while our proposed method achieves\n100\n%\nÂ±\n0\n%\n100\\%\\pm 0\\%\nin 15K steps (approximately 300K control steps), giving\nâˆ¼\n\\sim\n40\nÃ—\n\\times\nimprovement in terms of RL decision steps and\nâˆ¼\n\\sim\n2\nÃ—\n\\times\nwith respect to control steps.\nFigure 8\n:\nThe result of baseline comparison with the end-to-end RL. Left: the training curve with respect to RL decision steps. Right: the training curve with respect to lower-level control steps.\nThe evaluation results of robustness to environment variation is shown in Table\nII\n. While the end-to-end policy performs well under external force and friction variations, its success drops from\n92.5\n%\n92.5\\%\nto\n44.0\n%\n44.0\\%\nunder robot actuator variations (including actuatorâ€™s stiffness and damping) due to overfitting to nominal dynamics in training. In contrast, the hierarchical policy maintains\n100\n%\nÂ±\n0\n%\n100\\%\\pm 0\\%\nsuccess rate. We attribute this robustness to two factors: the higher-level contact-intention policy abstracts away low-level dynamics variations, and the lower-level MPC performs high-frequency replanning with an explicit physics model, continuously correcting execution errors induced by model errors. Overall, these results indicate that the proposed hierarchy simplifies learning and substantially improves robustness in environment variations.\nTABLE II\n:\nEvaluation results for robustness to environment variations\nVaritions\nSuccess Rate (End-to-End)\nSuccess Rate (Ours)\nNo Perturbation\n92.5\n%\nÂ±\n7.55\n%\n92.5\\%\\pm 7.55\\%\n100\n%\nÂ±\n0\n%\n100\\%\\pm 0\\%\nExternal Force\n89.6\n%\nÂ±\n8.6\n%\n89.6\\%\\pm 8.6\\%\n100\n%\nÂ±\n0\n%\n100\\%\\pm 0\\%\nFriction\n92.8\n%\nÂ±\n6.8\n%\n92.8\\%\\pm 6.8\\%\n100\n%\nÂ±\n0\n%\n100\\%\\pm 0\\%\nActuation\n44.0\n%\nÂ±\n24.2\n%\n44.0\\%\\pm 24.2\\%\n100\n%\nÂ±\n0\n%\n100\\%\\pm 0\\%\nComparison with HACMan-Style Hierarchy\nWe compare our RL-MPC framework with a HACMan-style hierarchical RL policy\n[\n66\n]\n, which directly predicts contact locations and corresponding end-effector 3D displacements. The RL observation space is unchanged, while the action space is\nğ’«\nÃ—\n[\nâˆ’\n0.1\n,\n0.1\n]\n3\n\\mathcal{P}\\times[-0.1,0.1]^{3}\n, where\nğ’«\n\\mathcal{P}\ndenotes the set of candidate contact keypoints and\n[\nâˆ’\n0.1\n,\n0.1\n]\n3\n[-0.1,0.1]^{3}\nrepresents the end-effector 3D-displacement. Both our policy network are reused, but the MPC weight head is replaced with a displacement head. This comparison quantifies how the MPC lower-level aids structured exploration and improves manipulation success.\nFigure 9\n:\nResult of comparison with HACMan-style hierarchical policy.\nThe training result for solving the 3D object reorientation task is shown in Fig.\n9\n.\nHACMan-style hierarchical policy requires roughly 300K RL steps to reach about\n77.19\n%\nÂ±\n29.80\n%\n77.19\\%\\pm 29.80\\%\nsuccess rate, while our method converges to\n98.75\n%\nÂ±\n1.17\n%\n98.75\\%\\pm 1.17\\%\nsuccess rate within 200K steps. Furthermore, the variance in performance increases considerably without the MPC layer. These results highlight that the MPC plays a critical role in ensuring consistency and stability during task execution.\nVII-C\nAblation Studies\nVII-C\n1\nAblation 1: Removing Subgoal Prediction\n(a)\nAblation 1 & 2.\n(b)\nAblation 3.\nFigure 10\n:\nResults of ablation studies.\nTo evaluate the role of\nsubgoal prediction\n, we consider a variant where the higher-level policy predicts only contact locations, with fixed MPC terminal-cost weights (e.g.\nw\npos\n=\n100\nw_{\\text{pos}}{=}100\n,\nw\nori\n=\n4\nw_{\\text{ori}}{=}4\n) and all other settings unchanged. As shown by the green curve in Fig.\n10\n(a), removing subgoal prediction drops the success rate from\n100\n%\nÂ±\n0.0\n%\n100\\%\\pm 0.0\\%\nto\n26.56\n%\nÂ±\n20.44\n%\n26.56\\%\\pm 20.44\\%\n. We find that in the contact-only case, the MPC must simultaneously minimize both positional and rotational errors, leading to inefficient pushing that often exceeds the 64-step episode limit.. The ablation result highlights the critical role of subgoal selection for efficient manipulation.\nVII-C\n2\nAblation 2: Frame Mismatch in Observation\nThis ablation is designed to highlight the importance of expressing the geometry component\nğ’«\n\\mathcal{P}\nand the target component\nÎ”\nâ€‹\nğ’«\n\\Delta\\mathcal{P}\nin a consistent reference frame. We train a policy under the same settings as the main experiment in the geometry-generalized pushing task, with the only difference being that\nÎ”\nâ€‹\nğ’«\n\\Delta\\mathcal{P}\nis expressed in the world while\nğ’«\n\\mathcal{P}\nremains in the object frame. As shown with the blue curve in Fig.\n10\n(a), learning fails to bootstrap under this mismatch. This result demonstrates that a consistent object-centric representation is crucial for stable learning and for enabling the policy to reason effectively about geometry and goal relationships.\nVII-C\n3\nAblation 3: Removing Collision Component in Observation\nTo evaluate the role of the\ncollision component\nğ’Ÿ\nk\n\\mathcal{D}_{k}\n, we remove it from the observation for\nobject 3D reorientation\ntask, keeping all other settings unchanged. As shown in Fig.\n10\n(b), both learning speed and success rate drop (the success rate drops from\n98.75\n%\nÂ±\n1.17\n%\n98.75\\%\\pm 1.17\\%\nto\n85.94\n%\nÂ±\n8.15\n%\n85.94\\%\\pm 8.15\\%\n). A possible explanation is that without collision information, the value network overfits the feasibility penalty\nr\nfeasible\nr_{\\text{feasible}}\n, implicitly associating it with particular object configurations or poses, rather than with true contact feasibility. Thus it introduces noise and degrades policy performance. This result highlights the importance of\nğ’Ÿ\nk\n\\mathcal{D}_{k}\nfor effective manipulation.\nVIII\nReal-World Experiment\nVIII-A\nExperiment Setup\nFigure 11\n:\nThe Diagram of real-world system.\nWe deploy the higher-level policy trained in simulation in Section\nVII\non real-world system. Fig.\n11\nshows the hardware setup. The system composed of four modules. (1) The higher-level RL policy module. We use the stochastic version of the trained RL policy for contact-point selection in 3D reorientation task. This helps prevent the system from getting stuck due to noisy pose estimation. (2) Lower-level ComFree-MPC module (\nâˆ¼\n\\sim\n100Hz). (3) Robot operational space control (OSC) stack (\nâˆ¼\n\\sim\n500Hz)\n[\n13\n]\n. It receives the end-effector displacement command from ComFree-MPC. (4) The vision perception module (\nâˆ¼\n\\sim\n40Hz), which uses the TwinTrack\n[\n59\n]\nand FoundationPose++\n[\n56\n,\n58\n]\n.\nThese components interact through LCM communication middleware\n[\n20\n]\n. All objects are 3D-printed from the exact STL meshes used during training.\nVIII-B\nResults\nWe evaluate the proposed method on real-world geometry-generalized pushing and object 3D reorientation. For each letter in pushing task, we perform 10 independent trials. For the reorientation, we perform 25 independent trials. For each trial, the object is randomly placed on the table, and the target position is randomly generated. We analyze the result both qualitatively and quantitatively.\nQuantitative evaluation\nTable\nIII\nshows the hardware results for each letter in the pushing task and cube 3D reorientation task. It reports using three metrics: success rate over a fixed number of trials, the wall-clock time for completing the task (exec_time), and the number of RL decision steps needed to complete the task (num_steps).\nTABLE III\n:\nResult in real-world non-prehensile manipulation\nobject\nsuccess_rate\nexec_time (s)\nnum_steps\nE\n100.0%\n58.89 Â± 28.70\n5.60 Â± 2.72\nF\n100.0%\n89.64 Â± 67.31\n8.70 Â± 6.53\nH\n100.0%\n54.54 Â± 21.12\n5.20 Â± 2.04\nI\n70.0%\n72.69 Â± 31.53\n7.14 Â± 2.91\nK\n100.0%\n64.01 Â± 31.27\n6.20 Â± 3.08\nL\n100.0%\n100.39 Â± 42.01\n9.60 Â± 3.89\nN\n100.0%\n66.56 Â± 42.20\n6.20 Â± 3.85\nT\n100.0%\n48.13 Â± 27.50\n4.70 Â± 2.71\nV\n100.0%\n66.41 Â± 44.49\n6.40 Â± 4.33\nX\n100.0%\n54.62 Â± 31.23\n5.30 Â± 3.06\nY\n100.0%\n93.64 Â± 41.15\n9.00 Â± 3.92\nZ\n100.0%\n68.25 Â± 66.42\n6.60 Â± 6.24\nCube\n100.0%\n237.74 Â± 147.55\n24.96 Â± 15.5\nIn the pushing tasks, most objects, including unseen ones, achieve 100% success with few steps, showing reliable sim-to-real transfer; only letter I occasionally fails due to tracking failure. In the 3D reorientation task, our method maintains strong success, though requiring more steps.\nQualitative: diverse contact interaction strategies\nIn the object 3D reorientation task, multiple contact interaction strategies emerge, shown in Fig.\n12\n. In the first row, the robotâ€™s end effector reorientates the cube purely by frictional sliding on the top face. In the second row, the end effector pivots the cube at an edge to rotate the object. In the third row, the robot reorientates the cube by pivoting it at one corner. The diverse interaction demonstrate the policyâ€™s ability to adapt its contact strategy in different task situations, rather than relying on a single manipulation pattern. Overall, the results demonstrate that the hierarchical RL-MPC policy enables efficient, reliable, and generalizable real-world manipulation.\n(a)\nReorientation by sliding on the top.\n(b)\nEdge pivoting.\n(c)\nCorner pivoting.\nFigure 12\n:\nDifferent contact strategies in the 3D reorientation experiment.\nIX\nConclusion and Limitations\nWe introduced a hierarchical RL-MPC framework for contact-rich manipulation that explicitly separates geometric-kinematic planning from contact dynamics reasoning. The high-level RL policy predicts a\ncontact intention\n, including an object-surface contact location and a post-contact object-level subgoal, while a low-level contact-implicit MPC replans contact modes online with contact dynamics to robustly execute each intention. Across non-prehensile manipulation tasks, our approach achieves near-100% success with substantially improved data efficiency less than baselines, strong robustness, and zero-shot sim-to-real transfer.\nThe proposed approach has two main limitations. First, it relies on accurate object pose estimation for computing goal flow and MPC terms. Second, using a discrete keypoint set\nğ’«\n\\mathcal{P}\nsimplifies learning but scales poorly with multiple end-effectors due to the combinatorial action space. Future work will explore perception-native planning and extend the interface to multi-finger manipulation for scalable multi-fingered dexterous manipulation.\nReferences\n[1]\nS. Akella and M. T. Mason\n(1998)\nPosing polygonal objects in the plane by pushing\n.\nThe International Journal of Robotics Research\n17\n(\n1\n),\npp.Â 70â€“88\n.\nCited by:\nÂ§\nII-B\n.\n[2]\nA. Aydinoglu, A. Wei, W. Huang, and M. Posa\n(2024)\nConsensus complementarity control for multi-contact mpc\n.\nIEEE Transactions on Robotics\n.\nCited by:\nÂ§\nII-B\n.\n[3]\nZ. Bing, A. Mavrichev, S. Shen, X. Yao, K. Chen, K. Huang, and A. Knoll\n(2023)\nSafety guaranteed manipulation based on reinforcement learning planner and model predictive control actor\n.\narXiv preprint arXiv:2304.09119\n.\nCited by:\nÂ§\nII-A\n.\n[4]\nK. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter,\net al.\nÏ€\n\\pi\n0: A vision-language-action flow model for general robot control. corr, abs/2410.24164, 2024. doi: 10.48550\n.\narXiv preprint ARXIV.2410.24164\n.\nCited by:\nÂ§I\n.\n[5]\nH. Bui, Y. Gao, H. Yang, E. Cui, S. Mody, B. Acosta, T. S. Felix, B. Bianchini, and M. Posa\n(2025)\nPush anything: single-and multi-object pushing from first sight with contact-implicit mpc\n.\narXiv preprint arXiv:2510.19974\n.\nCited by:\nÂ§\nII-B\n.\n[6]\nY. Chen and Q. Nguyen\n(2024)\nLearning agile locomotion and adaptive behaviors via rl-augmented mpc\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 11436â€“11442\n.\nCited by:\nÂ§\nII-A\n.\n[7]\nX. Cheng, S. Patil, Z. Temel, O. Kroemer, and M. T. Mason\n(2023)\nEnhancing dexterity in robotic manipulation via hierarchical contact exploration\n.\nIEEE Robotics and Automation Letters\n9\n(\n1\n),\npp.Â 390â€“397\n.\nCited by:\nÂ§\nII-B\n.\n[8]\nC. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song\n(2025)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nThe International Journal of Robotics Research\n44\n(\n10-11\n),\npp.Â 1684â€“1704\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[9]\nY. Cho, J. Han, Y. Cho, and B. Kim\n(2024)\nCorn: contact-based object representation for nonprehensile manipulation of general unseen objects\n.\narXiv preprint arXiv:2403.10760\n.\nCited by:\nÂ§\nII-B\n.\n[10]\nY. Cho, J. Han, J. Han, and B. Kim\n(2025)\nHierarchical and modular network on non-prehensile manipulation in general environments\n.\narXiv preprint arXiv:2502.20843\n.\nCited by:\nÂ§\nII-B\n.\n[11]\nF. Chu, R. Xu, L. Seguin, and P. A. Vela\n(2019)\nToward affordance detection and ranking on novel objects for real-world robotic manipulation\n.\nIEEE Robotics and Automation Letters\n4\n(\n4\n),\npp.Â 4070â€“4077\n.\nCited by:\nÂ§\nII-C\n.\n[12]\nF. Chu, R. Xu, and P. A. Vela\n(2019)\nLearning affordance segmentation for real-world robotic manipulation via synthetic images\n.\nIEEE Robotics and Automation Letters\n4\n(\n2\n),\npp.Â 1140â€“1147\n.\nCited by:\nÂ§\nII-C\n.\n[13]\nJ. Elsner\n(2023)\nTaming the panda with python: a powerful duo for seamless robotics programming and integration\n.\nSoftwareX\n24\n,\npp.Â 101532\n.\nExternal Links:\nISSN 2352-7110\n,\nDocument\n,\nLink\nCited by:\nÂ§\nVIII-A\n,\nÂ§\nIX-A\n1\n.\n[14]\nM. Fey, J. Sunil, A. Nitta, R. Puri, M. Shah, B. StojanoviÄ, R. Bendias, B. Alexandria, V. Kocijan, Z. Zhang, X. He, J. E. Lenssen, and J. Leskovec\n(2025)\nPyG 2.0: scalable learning on real world graphs\n.\nIn\nTemporal Graph Learning Workshop @ KDD\n,\nCited by:\nÂ§\nIX-B\n1\n.\n[15]\nA. Figure\n(2024)\nHelix: a vision-language-action model for generalist humanoid control\n.\nFigure AI News\n.\nCited by:\nÂ§I\n.\n[16]\nY. Geng, B. An, H. Geng, Y. Chen, Y. Yang, and H. Dong\n(2022)\nEnd-to-end affordance learning for robotic manipulation\n.\narXiv preprint arXiv:2209.12941\n.\nCited by:\nÂ§\nII-C\n.\n[17]\nS. T. Grafton\n(2010)\nThe cognitive neuroscience of prehension: recent developments\n.\nExperimental brain research\n204\n(\n4\n),\npp.Â 475â€“491\n.\nCited by:\nÂ§I\n.\n[18]\nT. Hermans, J. M. Rehg, and A. Bobick\n(2011)\nAffordance prediction via learned object attributes\n.\nIn\nIEEE international conference on robotics and automation (ICRA): Workshop on semantic perception, mapping, and exploration\n,\npp.Â 181â€“184\n.\nCited by:\nÂ§\nII-C\n.\n[19]\nL. Hu, L. Ding, H. Yang, T. Liu, A. Zhang, S. Chen, H. Gao, P. Xu, and Z. Deng\n(2025)\nLNO-driven deep rl-mpc: hierarchical adaptive control architecture for dynamic legged locomotion\n.\nIEEE Transactions on Industrial Informatics\n.\nCited by:\nÂ§\nII-A\n.\n[20]\nA. S. Huang, E. Olson, and D. C. Moore\n(2010)\nLCM: lightweight communications and marshalling\n.\nIn\n2010 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp.Â 4057â€“4062\n.\nCited by:\nÂ§\nVIII-A\n.\n[21]\nW. Huang, F. Xia, D. Shah, D. Driess, A. Zeng, Y. Lu, P. Florence, I. Mordatch, S. Levine, K. Hausman,\net al.\n(2023)\nGrounded decoding: guiding text generation with grounded models for embodied agents\n.\nAdvances in Neural Information Processing Systems\n36\n,\npp.Â 59636â€“59661\n.\nCited by:\nÂ§\nII-C\n.\n[22]\nY. Ji, H. Tan, J. Shi, X. Hao, Y. Zhang, H. Zhang, P. Wang, M. Zhao, Y. Mu, P. An,\net al.\n(2025)\nRobobrain: a unified brain model for robotic manipulation from abstract to concrete\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 1724â€“1734\n.\nCited by:\nÂ§\nII-C\n.\n[23]\nM. Jia, M. Tao, M. Xu, P. Zhang, J. Qiu, G. Bergsieker, and J. Chen\n(2024)\nRl-mpc: reinforcement learning aided model predictive controller for autonomous vehicle lateral control\n.\nTechnical report\nSAE Technical Paper\n.\nCited by:\nÂ§\nII-A\n.\n[24]\nZ. Jiang, Y. Zhu, M. Svetlik, K. Fang, and Y. Zhu\n(2021)\nSynergies between affordance and geometry: 6-dof grasp detection via implicit representations\n.\narXiv preprint arXiv:2104.01542\n.\nCited by:\nÂ§\nII-C\n.\n[25]\nW. Jin\n(2024)\nComplementarity-free multi-contact modeling and optimization for dexterous manipulation\n.\narXiv preprint arXiv:2408.07855\n.\nCited by:\nÂ§\nIII-B\n,\nÂ§\nV-A\n,\nÂ§\nV-A\n,\nÂ§\nIX-C\n.\n[26]\nM. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi,\net al.\n(2024)\nOpenvla: an open-source vision-language-action model\n.\narXiv preprint arXiv:2406.09246\n.\nCited by:\nÂ§I\n.\n[27]\nV. Kurtz, A. Castro, A. Ã–. Ã–nol, and H. Lin\n(2023)\nInverse dynamics trajectory optimization for contact-implicit model predictive control\n.\nThe International Journal of Robotics Research\n,\npp.Â 02783649251344635\n.\nCited by:\nÂ§\nII-B\n.\n[28]\nG. Li, N. Tsagkas, J. Song, R. Mon-Williams, S. Vijayakumar, K. Shao, and L. Sevilla-Lara\n(2025)\nLearning precise affordances from egocentric videos for robotic manipulation\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.Â 10581â€“10591\n.\nCited by:\nÂ§\nII-C\n.\n[29]\nW. Li, H. Zhao, Z. Yu, Y. Du, Q. Zou, R. Hu, and K. Xu\n(2025)\nPin-wm: learning physics-informed world models for non-prehensile manipulation\n.\narXiv preprint arXiv:2504.16693\n.\nCited by:\nÂ§\nII-B\n.\n[30]\nX. Li, L. Heng, J. Liu, Y. Shen, C. Gu, Z. Liu, H. Chen, N. Han, R. Zhang, H. Tang,\net al.\n(2025)\n3ds-vla: a 3d spatial-aware vision language action model for robust multi-task manipulation\n.\nIn\n9th Annual Conference on Robot Learning\n,\nCited by:\nÂ§I\n.\n[31]\nX. Li, M. Zhang, Y. Geng, H. Geng, Y. Long, Y. Shen, R. Zhang, J. Liu, and H. Dong\n(2024)\nManipllm: embodied multimodal large language model for object-centric robotic manipulation\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 18061â€“18070\n.\nCited by:\nÂ§\nII-C\n.\n[32]\nT. Lin, K. Sachdev, L. Fan, J. Malik, and Y. Zhu\n(2025)\nSim-to-real reinforcement learning for vision-based dexterous manipulation on humanoids\n.\narXiv preprint arXiv:2502.20396\n.\nCited by:\nÂ§\nII-B\n.\n[33]\nJ. Liu, S. Yang, Y. Wang, H. Zhao, and H. Ding\n(2025)\nAutonomous bimanual manipulation of deformable objects using deep reinforcement learning guided adaptive control\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 6904â€“6910\n.\nCited by:\nÂ§\nII-A\n.\n[34]\nA. Lopes, C. Barata, and P. Moreno\n(2025)\nModel-based lookahead reinforcement learning for in-hand manipulation\n.\narXiv preprint arXiv:2510.08884\n.\nCited by:\nÂ§\nII-A\n.\n[35]\nK. M. Lynch and M. T. Mason\n(1996)\nStable pushing: mechanics, controllability, and planning\n.\nThe international journal of robotics research\n15\n(\n6\n),\npp.Â 533â€“556\n.\nCited by:\nÂ§\nII-B\n.\n[36]\nM. T. Mason\n(1986)\nMechanics and planning of manipulator pushing operations\n.\nThe International Journal of Robotics Research\n5\n(\n3\n),\npp.Â 53â€“71\n.\nCited by:\nÂ§\nII-B\n.\n[37]\nJ. Merel, M. Botvinick, and G. Wayne\n(2019)\nHierarchical motor control in mammals and machines\n.\nNature communications\n10\n(\n1\n),\npp.Â 5489\n.\nCited by:\nÂ§I\n.\n[38]\nI. Mordatch, Z. PopoviÄ‡, and E. Todorov\n(2012)\nContact-invariant optimization for hand manipulation\n.\nIn\nProceedings of the ACM SIGGRAPH/Eurographics symposium on computer animation\n,\npp.Â 137â€“144\n.\nCited by:\nÂ§\nII-B\n.\n[39]\nJ. Moura, T. Stouraitis, and S. Vijayakumar\n(2022)\nNon-prehensile planar manipulation via trajectory optimization with complementarity constraints\n.\nIn\n2022 International Conference on Robotics and Automation (ICRA)\n,\npp.Â 970â€“976\n.\nCited by:\nÂ§\nII-B\n.\n[40]\nM. Omer, R. Ahmed, B. Rosman, and S. F. Babikir\n(2021)\nModel predictive-actor critic reinforcement learning for dexterous manipulation\n.\nIn\n2020 International Conference on Computer, Control, Electrical, and Electronics Engineering (ICCCEEE)\n,\npp.Â 1â€“6\n.\nCited by:\nÂ§\nII-A\n.\n[41]\nM. Posa, C. Cantu, and R. Tedrake\n(2014)\nA direct method for trajectory optimization of rigid bodies through contact\n.\nThe International Journal of Robotics Research\n33\n(\n1\n),\npp.Â 69â€“81\n.\nCited by:\nÂ§\nII-B\n.\n[42]\nH. Qi, A. Kumar, R. Calandra, Y. Ma, and J. Malik\n(2023)\nIn-hand object rotation via rapid motor adaptation\n.\nIn\nConference on Robot Learning\n,\npp.Â 1722â€“1732\n.\nCited by:\nÂ§I\n.\n[43]\nA. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann\n(2021)\nStable-baselines3: reliable reinforcement learning implementations\n.\nJournal of Machine Learning Research\n22\n(\n268\n),\npp.Â 1â€“8\n.\nExternal Links:\nLink\nCited by:\nÂ§\nIX-B\n4\n.\n[44]\nQ. Rouxel, A. Ferrari, S. Ivaldi, and J. Mouret\n(2024)\nFlow matching imitation learning for multi-support manipulation\n.\nIn\n2024 IEEE-RAS 23rd International Conference on Humanoid Robots (Humanoids)\n,\npp.Â 528â€“535\n.\nCited by:\nÂ§\nII-B\n.\n[45]\nY. Saigusa, S. Sakaino, and T. Tsuji\n(2022)\nImitation learning for nonprehensile manipulation through self-supervised learning considering motion speed\n.\nIEEE Access\n10\n,\npp.Â 68291â€“68306\n.\nCited by:\nÂ§\nII-B\n.\n[46]\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov\n(2017)\nProximal policy optimization algorithms\n.\narXiv preprint arXiv:1707.06347\n.\nCited by:\nÂ§\nVI-B\n.\n[47]\nC. Shin, P. W. Ferguson, S. A. Pedram, J. Ma, E. P. Dutson, and J. Rosen\n(2019)\nAutonomous tissue manipulation via surgical robot using learning based model predictive control\n.\nIn\n2019 International conference on robotics and automation (ICRA)\n,\npp.Â 3875â€“3881\n.\nCited by:\nÂ§\nII-A\n.\n[48]\nZ. Song, G. Ouyang, M. Li, Y. Ji, C. Wang, Z. Xu, Z. Zhang, X. Zhang, Q. Jiang, Z. Chen,\net al.\n(2025)\nManiplvm-r1: reinforcement learning for reasoning in embodied manipulation with large vision-language models\n.\narXiv preprint arXiv:2505.16517\n.\nCited by:\nÂ§\nII-C\n.\n[49]\nX. Sun, J. Li, A. V. Kovalenko, W. Feng, and Y. Ou\n(2022)\nIntegrating reinforcement learning and learning from demonstrations to learn nonprehensile manipulation\n.\nIEEE Transactions on Automation Science and Engineering\n20\n(\n3\n),\npp.Â 1735â€“1744\n.\nCited by:\nÂ§\nII-B\n.\n[50]\nY. Tang, L. Zhang, S. Zhang, Y. Zhao, and X. Hao\n(2025)\nRoboafford: a dataset and benchmark for enhancing object and spatial affordance learning in robot manipulation\n.\nIn\nProceedings of the 33rd ACM International Conference on Multimedia\n,\npp.Â 12706â€“12713\n.\nCited by:\nÂ§\nII-C\n.\n[51]\nG. A. Team\net al.\nGen-0: embodied foundation models that scale with physical interaction. generalist ai blog, 2025\n.\nCited by:\nÂ§I\n.\n[52]\nE. Tong, A. Opipari, S. Lewis, Z. Zeng, and O. C. Jenkins\n(2024)\nOval-prompt: open-vocabulary affordance localization for robot manipulation through llm affordance-grounding\n.\narXiv preprint arXiv:2404.11000\n.\nCited by:\nÂ§\nII-C\n.\n[53]\nT. Tosun, E. Mitchell, B. Eisner, J. Huh, B. Lee, D. Lee, V. Isler, H. S. Seung, and D. Lee\n(2019)\nPixels to plans: learning non-prehensile manipulation by imitating a planner\n.\nIn\n2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 7431â€“7438\n.\nCited by:\nÂ§\nII-B\n.\n[54]\nD. Wang, C. Liu, F. Chang, and Y. Xu\n(2025)\nHierarchical diffusion policy: manipulation trajectory generation via contact guidance\n.\nIEEE Transactions on Robotics\n.\nCited by:\nÂ§\nII-B\n.\n[55]\nM. Wang, A. Ã–. Ã–nol, P. Long, and T. PadÄ±r\n(2022)\nContact-implicit planning and control for non-prehensile manipulation using state-triggered constraints\n.\nIn\nThe International Symposium of Robotics Research\n,\npp.Â 189â€“204\n.\nCited by:\nÂ§\nII-B\n.\n[56]\nB. Wen, W. Yang, J. Kautz, and S. Birchfield\n(2024)\nFoundationpose: unified 6d pose estimation and tracking of novel objects\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 17868â€“17879\n.\nCited by:\nÂ§\nVIII-A\n.\n[57]\nR. Xu, F. Chu, C. Tang, W. Liu, and P. A. Vela\n(2021)\nAn affordance keypoint detection network for robot manipulation\n.\nIEEE Robotics and Automation Letters\n6\n(\n2\n),\npp.Â 2870â€“2877\n.\nCited by:\nÂ§\nII-C\n.\n[58]\nFoundationPose++\nExternal Links:\nLink\nCited by:\nÂ§\nVIII-A\n.\n[59]\nW. Yang, Z. Xie, X. Zhang, H. B. Amor, S. Lin, and W. Jin\n(2025)\nTwinTrack: bridging vision and contact physics for real-time tracking of unknown dynamic objects\n.\narXiv preprint arXiv:2505.22882\n.\nCited by:\nÂ§\nVIII-A\n.\n[60]\nW. Yang and M. Posa\n(2024)\nDynamic on-palm manipulation via controlled sliding\n.\nCited by:\nÂ§\nII-B\n.\n[61]\nY. Yang, T. Zhang, E. Coumans, J. Tan, and B. Boots\n(2022)\nFast and efficient locomotion via learned gait transitions\n.\nIn\nConference on robot learning\n,\npp.Â 773â€“783\n.\nCited by:\nÂ§\nII-A\n.\n[62]\nX. Yi and N. Fazeli\n(2023)\nPrecise object sliding with top contact via asymmetric dual limit surfaces\n.\narXiv preprint arXiv:2305.14289\n.\nCited by:\nÂ§\nII-B\n.\n[63]\nZ. Yin, B. Huang, Y. Qin, Q. Chen, and X. Wang\n(2023)\nRotating without seeing: towards in-hand dexterity through touch\n.\narXiv preprint arXiv:2303.10880\n.\nCited by:\nÂ§I\n.\n[64]\nZ. Zhang, X. Chang, H. Ma, H. An, and L. Lang\n(2022)\nModel predictive control of quadruped robot based on reinforcement learning\n.\nApplied Sciences\n13\n(\n1\n),\npp.Â 154\n.\nCited by:\nÂ§\nII-A\n.\n[65]\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn\n(2023)\nLearning fine-grained bimanual manipulation with low-cost hardware\n.\narXiv preprint arXiv:2304.13705\n.\nCited by:\nÂ§I\n.\n[66]\nW. Zhou, B. Jiang, F. Yang, C. Paxton, and D. Held\n(2023)\nHacman: learning hybrid actor-critic maps for 6d non-prehensile manipulation\n.\narXiv preprint arXiv:2305.03942\n.\nCited by:\nÂ§\nII-B\n,\nÂ§\nVII-B\n.\n[67]\nY. Zhu, A. Joshi, P. Stone, and Y. Zhu\n(2023)\nViola: imitation learning for vision-based manipulation with object proposal priors\n.\nIn\nConference on Robot Learning\n,\npp.Â 1199â€“1210\n.\nCited by:\nÂ§\nII-B\n.\n[68]\nZ. Zhuang, L. Zheng, W. Li, R. Liu, P. Lu, and H. Cheng\n(2025)\nRM-planner: integrating reinforcement learning with whole-body model predictive control for mobile manipulation\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 7263â€“7269\n.\nCited by:\nÂ§\nII-A\n.\nAPPENDIX\nIX-A\nDetailed Task Setting in Sim and Real\nIX-A\n1\nRobot Operational Space Control\nIn the simulated experiments with full robot system, a Operational Space Controller (OSC) is implemented to transform the desired end-effector Cartesian space displacement output of Comfree-MPC to torque command in joint space. In the real-world testing with Franka robot arm, the robot is controlled by an OSC implemented in the panda-py library\n[\n13\n]\n. The controller executes at 500Â Hz, enabling fast reaction to the end-effector displacement command during manipulation.\nIX-A\n2\nRobot Motion Planning for Contact Locations\nBetween contacts, the end-effector is lifted to disengage from the object and repositioned using OSC in free space. The OSC then guides the end-effector to the next intended contact location, where control switches back to the MPC for contact-rich interaction.\nIX-A\n3\nDistribution of Initial and Target Pose\nFor each task, the initial and target translation of\nx\nâ€‹\ny\nxy\ncoordinates are shown in Table\nIV\n. Note that we use (P) to stand for Geometry-generalized pushing and (R) to stand for 3D object reorientation, and use Abstract, Full-Sim, Full-Real to stand for abstract end-effector setting, full robot setting in simulation, and real-robot setting. Additionally, each pushing target orientation is a planar rotation with an random\n[\nâˆ’\n180\nâˆ˜\n,\n180\nâˆ˜\n]\n[-180^{\\circ},180^{\\circ}]\nangle about the\nz\nz\n-axis, and each flipping target is defined a fixed\n180\nâˆ˜\n180^{\\circ}\nrotation about the\nx\nx\n-axis, and an additional rotation with a random\n[\nâˆ’\n180\nâˆ˜\n,\n180\nâˆ˜\n]\n[-180^{\\circ},180^{\\circ}]\nangle about the z-axis.\nTABLE IV\n:\nInitialization and target distributions for all tasks.\nTask\nInit Translation (x Ã— y)\nTarget Translation\nAbstract (P)\n[\nâˆ’\n0.25\n,\n0.25\n]\n2\n[-0.25,0.25]^{2}\nSame as init\nFull-Sim (P)\n[\n0.4\n,\n0.6\n]\nÃ—\n[\nâˆ’\n0.1\n,\n0.1\n]\n[0.4,0.6]\\times[-0.1,0.1]\nSame as init\nFull-Real (P)\n[\n0.4\n,\n0.6\n]\nÃ—\n[\nâˆ’\n0.3\n,\n0.3\n]\n[0.4,0.6]\\times[-0.3,0.3]\n[\n0.4\n,\n0.5\n]\nÃ—\n[\nâˆ’\n0.1\n,\n0.1\n]\n[0.4,0.5]\\times[-0.1,0.1]\nAbstract (R)\n[\nâˆ’\n0.1\n,\n0.1\n]\n2\n[-0.1,0.1]^{2}\nSame as init\nFull-Sim (R)\n[\n0.4\n,\n0.6\n]\nÃ—\n[\nâˆ’\n0.1\n,\n0.1\n]\n[0.4,0.6]\\times[-0.1,0.1]\nSame as init\nFull-Real (R)\n[\n0.45\n,\n0.5\n]\nÃ—\n[\nâˆ’\n0.05\n,\n0.05\n]\n[0.45,0.5]\\times[-0.05,0.05]\nSame as init\nSuccess thresholds of all tasks are shown in Table\nV\n. The translational and orientational distance are shown in (\n9\n). The rotation distance is defined as\n1\nâˆ’\nâŸ¨\nğ’“\nH\nobj\n,\nğ’“\nÂ¯\ntarget\nâŸ©\n2\n1{-}\\langle\\boldsymbol{r}_{H}^{\\text{obj}},\\,\\boldsymbol{\\bar{r}}^{\\text{target}}\\rangle^{2}\n.\nA trial is considered successful if the object satisfies both thresholds.\nTABLE V\n:\nSuccess Threshold for all tasks.\nTask\nTrans Threshold (m)\nRot Threshold\nAbstract (P)\n0.05\n0.05\nFull-Sim (P)\n0.03\n0.05\nFull-Real (P)\n0.03\n0.02\nAbstract (R)\n0.08\n0.05\nFull-Sim (R)\n0.05\n0.05\nFull-Real (R)\n0.075\n0.03\nIX-B\nSettings of RL policy, reward and training\nIX-B\n1\nRL Policy Setting\nPointnet++ Setting in RL policy\nWe employ two PointNet++ feature extractorsâ€”a\nGeometric Processing Branch\nfor dense per-point reasoning and a\nGlobal Kinematic Processing Branch\nfor object-level aggregation. The Geometric Processing Branch follows a segmentation-style PointNet++ architecture, the Global Kinematic Processing Branch follows a classification-style PointNet++ architecture. Those two branches shares the same Pointnet++ setting, as shown in Table\nVI\n. We use the implementation of Pointnet++ networks in pytorch-geometric library\n[\n14\n]\n. Specifically, although the nominal input channel number is 4 (per-point goal flow and contact-distance features), the 3D keypoint coordinates themselves are normalized and concatenated to this input inside the module.\nTABLE VI\n:\nPointNet++ hyperparameters.\nParameter\nValue\nInput channels\n4\nOutput feature dimension\n128\nDropout\n0.0\nFarthest point sampling random start\nTrue\nInclude point coordinates in Input\nTrue\nPosition normalization\nTrue\nActivation\nLeakyReLU\nMPC weights action space\nWe choose\nğ’²\npos\n=\n{\n0\n,\n50\n,\n100\n,\n150\n,\n200\n}\n\\mathcal{W}_{\\text{pos}}=\\{0,50,100,150,200\\}\nand\nğ’²\nori\n=\n{\n0\n,\n2\n,\n4\n,\n6\n,\n8\n}\n\\mathcal{W}_{\\text{ori}}=\\{0,2,4,6,8\\}\n.\nIX-B\n2\nScaling of Observation\nTo improve training stability and generalization across objects of different sizes, all observation components are normalized (divided) by the characteristic size of the object. We set this size to be 0.15 for all letters, and 0.10 for the cube.\nIX-B\n3\nRL Reward Setting\nThe reward hyperparameter is set to be\nw\n1\n=\n0.1\n,\nw\n2\n=\n5.0\nw_{1}=0.1,w_{2}=5.0\nfor both tasks, and\nw\n3\n=\n0\nw_{3}=0\nfor Geometry-generalized pushing,\nw\n3\n=\n1\nw_{3}=1\nfor 3D object reorientation. Specifically, to calculate the term\n1\nN\nâ€‹\nâˆ‘\nğœ¹\ni\n,\nk\nâˆˆ\nÎ”\nâ€‹\nğ’«\nk\nâˆ¥\nğœ¹\ni\n,\nk\nâˆ¥\n\\frac{1}{N}\\sum_{\\boldsymbol{\\delta}_{i,k}\\in\\Delta\\mathcal{P}_{k}}\\lVert\\boldsymbol{\\delta}_{i,k}\\rVert\n, we use the target component after scaling.\nIX-B\n4\nRL Training Setting\nAll reinforcement-learning experiments use Proximal Policy Optimization (PPO) in Stable-Baselines3\n[\n43\n]\nwith the hyperparameters summarized in Table\nVII\n. The only exception is\nn\nsteps\nn_{\\text{steps}}\nin end-to-end model baseline comparison, which is set to be 128 to match its higher decision frequency.\nAll hyperparameters are carefully tuned to ensure stable optimization and strong empirical performance across both pushing and flipping tasks. The proposed policy outputs a structured action\nğ’‚\nk\n\\boldsymbol{a}_{k}\ncomposed of multiple discrete components, corresponding to different aspects of contact intention and MPC parameterization. During training, the log-probability and entropy terms used in the PPO objective are computed as sums over individual components, enabling balanced policy update. During training, two tricks of temperature scaling and learning rate scheduling are used:\nTABLE VII\n:\nPPO hyperparameters used across all experiments.\nParameter\nValue\nNumber of environments (\nn\nenvs\nn_{\\text{envs}}\n)\n32\nRollout length (\nn\nsteps\nn_{\\text{steps}}\n)\n32\nOptimization epochs (\nn\nepochs\nn_{\\text{epochs}}\n)\n5\nBatch size\n512\nClipping range (\nÏµ\n\\epsilon\n)\n0.3\nDiscount factor (\nÎ³\n\\gamma\n)\n0.99\nGAE parameter (\nÎ»\n\\lambda\n)\n0.95\nEntropy coefficient (\nc\nent\nc_{\\text{ent}}\n)\n0.0\nValue loss coefficient (\nc\nvf\nc_{\\text{vf}}\n)\n0.5\nTempertaure Scaling of Logits\nDuring training, a temperature scaling of\n0.1\n0.1\n(i.e., scaling the logits by a factor of\n10\n10\n) is applied to the contact-selection logits to sharpen the resulting categorical distribution and improve learning efficiency, while the logits corresponding to the MPC cost weights are used without additional scaling. After the scaling process, a softmax function is applied on logits to form the discrete probability distribution among contact point/ MPC weights selection during training.\nLearning-Rate Scheduling.\nWe employ a monotonically decaying learning-rate schedule of the form\nLR\nâ€‹\n(\np\n)\n=\n10\nâˆ’\n4\nâ‹…\nÎ²\n(\np\nâˆ’\n1\n)\n,\np\nâˆˆ\n[\n1\n,\n0\n]\n,\n\\text{LR}(p)=10^{-4}\\cdot\\beta^{(p-1)},\\qquad p\\in[1,0],\nwhere\np\np\ndenotes normalized remaining training progress from start (\np\n=\n1\np{=}1\n) to end (\np\n=\n0\np{=}0\n).\nFor the main experiments, we set\nÎ²\n=\n100\n\\beta=100\n. In the baseline comparisons, we use a smaller decay factor\nÎ²\n=\n10\n\\beta=10\non end-to-end and HACMan style models to enable stronger updates. In ablation studies, we still set\nÎ²\n=\n100\n\\beta=100\n.\nIX-C\nMPC Setting in Sim and Real\nWhile the core MPC formulation is shared across all experiments, physical parameters are adjusted differently for sim/real task to reflect their distinct physical properties.\nTable\nVIII\nsummarizes these task-dependent settings. Among those parameters,\nM\nM\nin (\n7\n) is formed by:\nM\n=\ndiag\nâ€‹\n(\nm\n,\nm\n,\nm\n,\nğ¢\n,\nğ¢\n,\nğ¢\n,\nh\n2\nâ€‹\nK\nr\n,\nh\n2\nâ€‹\nK\nr\n,\nh\n2\nâ€‹\nK\nr\n)\nM=\\textbf{diag}(m,m,m,\\mathbf{i},\\mathbf{i},\\mathbf{i},h^{2}K_{r},h^{2}K_{r},h^{2}K_{r})\n(10)\nto match the 9 DOFs of the system with the abstract end-effector (6 for object, 3 for robot). In the equation above,\nm\nm\nstands for object mass,\nğ¢\n\\mathbf{i}\nstands for the rotation inertia,\nK\nr\nK_{r}\nis the stiffness of the robot end-effector. Note that these parameters do not necessarily correspond to the true physical values of the object or the robot in the environment. Rather than aiming for exact physical fidelity, these parameters are tuned to provide a stable and effective predictive model for real-time planning.\nK\n,\nh\nK,h\nalong with\nM\nM\nare then used in (\n7\n).\nH\n,\nw\nc\nH,w_{c}\nand the control range scale is used to parameterize the optimization problem (\n2\n).\nÎ¼\n\\mu\nis used in the calculation of the Jacobian of the dual cones\nJ\n~\n\\tilde{J}\nin (\n7\n).\nIn simulated abstract and full robot system test, all experiments shares the same MPC parameter. In real-world Geometry-generalized pushing (Real-P) and real-world 3D object reorientation (Real-R), MPC parameter is adjusted to fit the physical properties of the system. Importantly, these adjustments are lightweight, physically intuitive, and do\nnot\nrequire retraining the RL policy, as the policy reasons only on contact intention. For details of how to form the dynamics (\n7\n) and MPC optimization problems (\n2\n) using those parameters, please refer to\n[\n25\n]\n.\nTABLE VIII\n:\nTask-specific MPC parameter settings.\nParam\nSimulation\nReal-P\nReal-R\nDynamics time step\nh\nh\n0.01\n0.01\n0.01\nRobot stiffness\nK\nr\nK_{r}\n0.01\n0.01\n0.01\nMPC horizon\nH\nH\n3\n3\n3\nContact weight\nw\nc\nw_{c}\n0.2\n0.05\n0.2\nObject inertia\nğ¢\n\\mathbf{i}\n0.0005\n0.0001\n0.001\nObject mass\nm\nm\n0.2\n0.05\n0.2\nControl range scale\n0.03\n0.015\n0.02\nStiffness coefficient\nK\nK\n5.0\n5.0\n0.75\nFriction coefficient\nÎ¼\n\\mu\n0.5\n0.5\n1.0\nEnvironment Steps\nT\nT\n20\n20\n20\nIX-D\nDetailed Settings in End-to-End Comparison\nIX-D\n1\nTask and Action Space\nFor simplicity, this comparison is conducted for the T-pushing task. The task setting is the same as the geometry-generalized pushing task, while the only object used in training and evaluation is â€\nT\nâ€. For the action space, the end effector is constrained to move in the\nx\nâ€‹\ny\nxy\n-plane with a fixed\nz\nz\nposition, with a range\n[\nâˆ’\n0.05\n,\n0.05\n]\n2\n[-0.05,0.05]^{2}\nrepresenting delta\nx\nâ€‹\ny\nxy\nmotions.\nIX-D\n2\nRandom Perturbations for Robustness Evaluation\nThree types of random perturbation is performed:\nExternal Force\nrandom forces and torques of\nğ’©\nâ€‹\n(\n0\n,\n0.03\n)\n\\mathcal{N}(0,0.03)\nis applied on all axis of the object at each simulation step.\nFriction\nthe friction parameter of the object is randomized by a random multiplier, with uniform distribution on\n[\n0.1\n,\n3.0\n]\n[0.1,3.0]\nat each simulation step.\nActuation\nActuator proportional parameter (100 in training environment) is randomized uniformly between\n[\n50\n,\n150\n]\n[50,150]\nat each simulation step. Actuator differential parameter (0 in training environment) is randomized uniformly between\n[\n0\n,\n15\n]\n[0,15]\nat each simulation step.",
    "preview_text": "A key challenge in contact-rich dexterous manipulation is the need to jointly reason over geometry, kinematic constraints, and intricate, nonsmooth contact dynamics. End-to-end visuomotor policies bypass this structure, but often require large amounts of data, transfer poorly from simulation to reality, and generalize weakly across tasks/embodiments. We address those limitations by leveraging a simple insight: dexterous manipulation is inherently hierarchical - at a high level, a robot decides where to touch (geometry) and move the object (kinematics); at a low level it determines how to realize that plan through contact dynamics. Building on this insight, we propose a hierarchical RL--MPC framework in which a high-level reinforcement learning (RL) policy predicts a contact intention, a novel object-centric interface that specifies (i) an object-surface contact location and (ii) a post-contact object-level subgoal pose. Conditioned on this contact intention, a low-level contact-implicit model predictive control (MPC) optimizes local contact modes and replans with contact dynamics to generate robot actions that robustly drive the object toward each subgoal. We evaluate the framework on non-prehensile tasks, including geometry-generalized pushing and object 3D reorientation. It achieves near-100% success with substantially reduced data (10x less than end-to-end baselines), highly robust performance, and zero-shot sim-to-real transfer.\n\nWhere to Touch, How to Contact: Hierarchical RLâ€“MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation\nZhixian Xie\nYu Xiang\nMichael Posa\nWanxin Jin\nAbstract\nA key challenge in contact-rich dexterous manipulation is the need to jointly reason over geometry, kinematic constraints, and intricate, nonsmooth contact dynamics. End-to-end visuomotor policies bypass this structure, but often require large amounts of data, transfer poorly from simulation to reality, and generalize weakly across tasks/embodiments. We address those l",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ†å±‚RL-MPCæ¡†æ¶ï¼Œç”¨äºå‡ ä½•æ„ŸçŸ¥çš„é•¿æ—¶ç¨‹çµå·§æ“ä½œï¼Œé€šè¿‡é«˜å±‚RLç­–ç•¥é¢„æµ‹æ¥è§¦æ„å›¾å’Œä½å±‚MPCä¼˜åŒ–æ¥è§¦åŠ¨æ€ï¼Œä»¥å‡å°‘æ•°æ®éœ€æ±‚å¹¶æå‡é²æ£’æ€§å’Œé›¶æ ·æœ¬ä»¿çœŸåˆ°ç°å®çš„è¿ç§»èƒ½åŠ›ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nçµå·§æ“ä½œæ˜¯æœºå™¨äººæ™ºèƒ½çš„æ ¸å¿ƒä½“ç°ã€‚å½“å‰ä¸»æµçš„ç«¯åˆ°ç«¯è§†è§‰è¿åŠ¨ç­–ç•¥è™½ç„¶æ€§èƒ½å¼ºå¤§ï¼Œä½†é€šå¸¸é¢ä¸´æ•°æ®éœ€æ±‚é‡å¤§ã€ä»¿çœŸåˆ°ç°å®ï¼ˆSim-to-Realï¼‰è¿ç§»å›°éš¾ã€ä»¥åŠå¯¹ä»»åŠ¡å’Œæœºå™¨äººå½¢æ€æ³›åŒ–èƒ½åŠ›å¼±ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°ï¼Œäººç±»çš„çµå·§æ“ä½œæœ¬è´¨ä¸Šæ˜¯**åˆ†å±‚**çš„ï¼šé«˜å±‚å†³å®š**åœ¨å“ªé‡Œæ¥è§¦**ï¼ˆå‡ ä½•ï¼‰å’Œ**ç§»åŠ¨ç‰©ä½“**ï¼ˆè¿åŠ¨å­¦ï¼‰ï¼Œä½å±‚å†³å®š**å¦‚ä½•é€šè¿‡æ¥è§¦åŠ¨åŠ›å­¦**å®ç°è¯¥è®¡åˆ’ã€‚å—æ­¤å¯å‘ï¼Œæœ¬æ–‡æ—¨åœ¨æ„å»ºä¸€ä¸ªç±»ä¼¼çš„åˆ†å±‚æ¡†æ¶ï¼Œä»¥å…‹æœç«¯åˆ°ç«¯æ–¹æ³•çš„å±€é™æ€§ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡ºäº†ä¸€ä¸ª**åˆ†å±‚å¼ºåŒ–å­¦ä¹ -æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆRL-MPCï¼‰æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº† **â€œæ¥è§¦æ„å›¾â€** è¿™ä¸€é«˜å±‚ç­–ç•¥æ¥å£ã€‚\n- **é«˜å±‚ï¼ˆRLç­–ç•¥ï¼‰**ï¼šè´Ÿè´£å‡ ä½•-è¿åŠ¨å­¦æ¨ç†ã€‚å®ƒæ¥æ”¶ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„ä¸‰ç»„ä»¶è§‚æµ‹ï¼ˆå‡ ä½•ã€ç›®æ ‡æµã€ç¢°æ’è·ç¦»ï¼‰ï¼Œé€šè¿‡ä¸€ä¸ª**åŒåˆ†æ”¯ç­–ç•¥ç½‘ç»œ**ï¼Œé¢„æµ‹**æ¥è§¦æ„å›¾**â€”â€”å³ç‰©ä½“è¡¨é¢çš„æ¥è§¦ç‚¹å’Œä¸€ä¸ªç‰©ä½“å±‚é¢çš„å­ç›®æ ‡ä½å§¿ã€‚\n- **ä½å±‚ï¼ˆæ¥è§¦éšå¼MPCï¼‰**ï¼šè´Ÿè´£æ¥è§¦åŠ¨åŠ›å­¦æ¨ç†ã€‚å®ƒåŸºäºé«˜å±‚ç»™å‡ºçš„â€œæ¥è§¦æ„å›¾â€ï¼Œåˆ©ç”¨**äº’è¡¥æ€§è‡ªç”±æ¥è§¦åŠ¨åŠ›å­¦æ¨¡å‹ï¼ˆComFree-MPCï¼‰** è¿›è¡Œé«˜é¢‘ï¼ˆ~100 Hzï¼‰é‡è§„åˆ’ï¼Œåœ¨çº¿ä¼˜åŒ–å±€éƒ¨æ¥è§¦æ¨¡å¼ï¼ˆæ»‘åŠ¨ã€ç²˜æ»ã€æ»šåŠ¨ç­‰ï¼‰ï¼Œç”Ÿæˆæœºå™¨äººåŠ¨ä½œä»¥é²æ£’åœ°å°†ç‰©ä½“é©±åŠ¨è‡³å­ç›®æ ‡ã€‚\n- **å…³é”®ä»·å€¼**ï¼šè¯¥æ¡†æ¶å°†å¤æ‚çš„æ¥è§¦åŠ¨åŠ›å­¦æ¨ç†å§”æ‰˜ç»™åŸºäºæ¨¡å‹çš„MPCï¼Œä½¿å¾—RLåªéœ€å­¦ä¹ æ›´æ˜“è¿ç§»çš„å‡ ä½•ä¸è¿åŠ¨å­¦çŸ¥è¯†ï¼Œä»è€Œå®ç°äº†**æ•°æ®é«˜æ•ˆã€é²æ£’æ€§å¼ºä¸”æ˜“äºSim-to-Realè¿ç§»**çš„çµå·§æ“ä½œã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\næ–¹æ³•åœ¨éæŠ“å–æ“ä½œä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬**å‡ ä½•æ³›åŒ–æ¨åŠ¨**å’Œ**ç‰©ä½“3Dé‡å®šå‘**ã€‚\n- **æ•°æ®æ•ˆç‡ä¸æ€§èƒ½**ï¼šåœ¨ä»¿çœŸä¸­ï¼Œä»…éœ€ç«¯åˆ°ç«¯åŸºçº¿æ–¹æ³•çº¦**1/10çš„RLç»éªŒæ•°æ®**ï¼Œå³å¯è¾¾åˆ°æ¥è¿‘100%çš„æˆåŠŸç‡ï¼Œå¹¶å±•ç°å‡ºå¯¹æœªè§ç‰©ä½“å‡ ä½•å½¢çŠ¶çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚\n- **é²æ£’æ€§**ï¼šåœ¨å­˜åœ¨å¤–åŠ›ã€æ‘©æ“¦å’Œé©±åŠ¨å™¨å‚æ•°æ‰°åŠ¨çš„æƒ…å†µä¸‹ï¼Œæ–¹æ³•ä¿æŒäº†100%çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºå¯¹åŠ¨åŠ›å­¦è¿‡æ‹Ÿåˆçš„ç«¯åˆ°ç«¯åŸºçº¿ã€‚\n- **é›¶æ ·æœ¬Sim-to-Realè¿ç§»**ï¼šå°†åœ¨æŠ½è±¡ä»¿çœŸç¯å¢ƒä¸­è®­ç»ƒçš„é«˜å±‚ç­–ç•¥ç›´æ¥éƒ¨ç½²åˆ°çœŸå®Frankaæœºæ¢°è‡‚ä¸Šï¼Œé…åˆè°ƒå‚åçš„ä½å±‚MPCï¼Œåœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­å–å¾—äº†**æé«˜çš„æˆåŠŸç‡**ï¼ˆå¤šæ•°å­—æ¯æ¨åŠ¨ä»»åŠ¡ä¸º100%ï¼Œç«‹æ–¹ä½“é‡å®šå‘ä¸º100%ï¼‰ï¼Œå¹¶æ¶Œç°å‡ºæ»‘åŠ¨ã€è¾¹ç¼˜æ¢è½¬ã€è§’ç‚¹æ¢è½¬ç­‰å¤šç§æ¥è§¦ç­–ç•¥ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\næœ¬å·¥ä½œé€šè¿‡æ˜ç¡®çš„å±‚æ¬¡åˆ†è§£ï¼ˆå‡ ä½•/è¿åŠ¨å­¦ vs. æ¥è§¦åŠ¨åŠ›å­¦ï¼‰ï¼Œä¸ºæ¥è§¦ä¸°å¯Œçš„çµå·§æ“ä½œæä¾›äº†ä¸€ä¸ªæ–°é¢–ä¸”é«˜æ•ˆçš„èŒƒå¼ã€‚å…¶æ„ä¹‰åœ¨äºï¼š\n- **æ–¹æ³•è®ºä¸Š**ï¼šè¯æ˜äº†ç»“åˆå­¦ä¹ ï¼ˆç”¨äºé«˜å±‚è¯­ä¹‰/å‡ ä½•æ¨ç†ï¼‰ä¸åŸºäºæ¨¡å‹çš„ä¼˜åŒ–ï¼ˆç”¨äºä½å±‚ç‰©ç†æ¨ç†ï¼‰çš„æ··åˆæ¡†æ¶çš„å·¨å¤§ä¼˜åŠ¿ã€‚\n- **å®è·µä¸Š**ï¼šæ˜¾è‘—é™ä½äº†æ•°æ®éœ€æ±‚ï¼Œæå‡äº†ç³»ç»Ÿçš„é²æ£’æ€§å’Œè·¨ä»¿çœŸ-ç°å®é¸¿æ²Ÿçš„è¿ç§»èƒ½åŠ›ï¼Œä¸ºåœ¨ç°å®ä¸–ç•Œä¸­å®ç°å¤æ‚ã€é•¿è§†é‡çš„çµå·§æ“ä½œæä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚\n- **å±€é™æ€§**ï¼šå½“å‰æ–¹æ³•ä¾èµ–äºç²¾ç¡®çš„ç‰©ä½“ä½å§¿ä¼°è®¡ï¼Œä¸”æ¥è§¦ç‚¹ç¦»æ•£é‡‡æ ·çš„æ–¹å¼éš¾ä»¥æ‰©å±•åˆ°å¤šæŒ‡æ“ä½œã€‚æœªæ¥å·¥ä½œå¯æ¢ç´¢ä¸æ„ŸçŸ¥æ›´ç´§å¯†çš„ç»“åˆä»¥åŠè¿ç»­æ¥è§¦æ„å›¾è¡¨ç¤ºã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **ä¸€ã€ ç ”ç©¶é—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**æ¥è§¦ä¸°å¯Œçš„çµå·§æ“ä½œ**ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼šå¦‚ä½•è®©æœºå™¨äºº**é«˜æ•ˆã€é²æ£’åœ°**å®Œæˆéœ€è¦é•¿æ—¶é—´è§„åˆ’ã€å¹¶æ·±åˆ»ç†è§£ç‰©ä½“å‡ ä½•å½¢çŠ¶çš„ä»»åŠ¡ï¼ˆå¦‚éæŠ“å–å¼æ¨åŠ¨ã€ç‰©ä½“ä¸‰ç»´é‡å®šå‘ï¼‰ã€‚ä¼ ç»Ÿæ–¹æ³•ï¼ˆç«¯åˆ°ç«¯è§†è§‰è¿åŠ¨ç­–ç•¥ï¼‰é¢ä¸´ä¸‰å¤§éš¾é¢˜ï¼š\n1.  **æ•°æ®æ•ˆç‡ä½ä¸‹**ï¼šéœ€è¦æµ·é‡æ•°æ®å­¦ä¹ éå…‰æ»‘çš„æ¥è§¦åŠ¨åŠ›å­¦ã€‚\n2.  **ä»¿çœŸåˆ°ç°å®è¿ç§»å›°éš¾**ï¼šéš¾ä»¥å»ºæ¨¡çš„æ¥è§¦åŠ¨åŠ›å­¦å¯¼è‡´å·¨å¤§çš„â€œä»¿çœŸåˆ°ç°å®â€å·®è·ã€‚\n3.  **æ³›åŒ–èƒ½åŠ›å¼±**ï¼šå¯¹æœªè§è¿‡çš„ç‰©ä½“å‡ ä½•å½¢çŠ¶æˆ–ä»»åŠ¡æ³›åŒ–èƒ½åŠ›å·®ã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ª**åˆ†å±‚å¼ºåŒ–å­¦ä¹ -æ¨¡å‹é¢„æµ‹æ§åˆ¶æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº**â€œæ¥è§¦æ„å›¾â€** è¿™ä¸€æŠ½è±¡æ¥å£ï¼Œå°†å¤æ‚æ“ä½œä»»åŠ¡è¿›è¡Œäº†**ç»“æ„åŒ–åˆ†è§£**ã€‚\n\n#### **1. æ ¸å¿ƒæ¦‚å¿µï¼šæ¥è§¦æ„å›¾**\nè¿™æ˜¯è®ºæ–‡æœ€å…³é”®çš„åˆ›æ–°ã€‚å®ƒå°†é«˜å±‚å†³ç­–å®šä¹‰ä¸ºï¼š\n```\næ¥è§¦æ„å›¾ = (ç‰©ä½“è¡¨é¢æ¥è§¦ç‚¹, æ¥è§¦åç‰©ä½“å­ç›®æ ‡ä½å§¿)\n```\nè¿™å¯¹åº”äº†äººç±»æ“ä½œçš„ç›´è§‚æ¨ç†ï¼š\n- **â€œåœ¨å“ªé‡Œæ¥è§¦â€**ï¼šå‡ ä½•æ¨ç†ï¼ˆé€‰æ‹©ç‰©ä½“è¡¨é¢çš„å“ªä¸ªç‚¹è¿›è¡Œäº¤äº’ï¼‰ã€‚\n- **â€œè¾¾åˆ°ä»€ä¹ˆæ•ˆæœâ€**ï¼šè¿åŠ¨å­¦æ¨ç†ï¼ˆè®¾å®šæœ¬æ¬¡æ¥è§¦å¸Œæœ›ç‰©ä½“è¾¾åˆ°çš„ä¸­é—´ä½å§¿ï¼‰ã€‚\n\n#### **2. åˆ†å±‚æ¡†æ¶è®¾è®¡**\nåŸºäºâ€œæ¥è§¦æ„å›¾â€ï¼Œæ„å»ºäº†ä¸€ä¸ªç´§å¯†è€¦åˆçš„ä¸¤å±‚æ¶æ„ï¼š\n\n- **é«˜å±‚ï¼šåŸºäºRLçš„å‡ ä½•-è¿åŠ¨å­¦æ¨ç†å™¨**\n    - **è¾“å…¥**ï¼šç²¾å¿ƒè®¾è®¡çš„**ä¸‰ç»„ä»¶ç‰©ä½“ä¸­å¿ƒåŒ–è§‚æµ‹**ï¼ˆå‡ ä½•ã€ç›®æ ‡æµã€ç¢°æ’è·ç¦»ï¼‰ã€‚\n    - **ç½‘ç»œ**ï¼šé‡‡ç”¨**åŒåˆ†æ”¯æ¶æ„**ï¼Œåˆ†åˆ«å¤„ç†å±€éƒ¨å‡ ä½•ç‰¹å¾å’Œå…¨å±€è¿åŠ¨å­¦ä¸Šä¸‹æ–‡ã€‚\n    - **è¾“å‡º**ï¼šé¢„æµ‹**æ¥è§¦æ„å›¾**ï¼ˆæ¥è§¦ç‚¹ + é€šè¿‡MPCæƒé‡éšå«å®šä¹‰çš„å­ç›®æ ‡ï¼‰ã€‚\n    - **ä¼˜åŠ¿**ï¼šåªéœ€åœ¨ç®€åŒ–çš„æŠ½è±¡ç¯å¢ƒä¸­è®­ç»ƒï¼Œé¿å¼€äº†å¤æ‚çš„çœŸå®åŠ¨åŠ›å­¦ï¼Œå®ç°äº†**æ•°æ®é«˜æ•ˆ**å’Œ**å‡ ä½•æ„ŸçŸ¥**çš„å­¦ä¹ ã€‚\n\n- **ä½å±‚ï¼šåŸºäºMPCçš„æ¥è§¦åŠ¨åŠ›å­¦æ¨ç†å™¨**\n    - **è¾“å…¥**ï¼šé«˜å±‚ä¼ æ¥çš„â€œæ¥è§¦æ„å›¾â€ã€‚\n    - **æ–¹æ³•**ï¼šé‡‡ç”¨**äº’è¡¥è‡ªç”±æ¨¡å‹é¢„æµ‹æ§åˆ¶**ï¼Œåœ¨ç»™å®šçš„æ¥è§¦ç‚¹é™„è¿‘ï¼Œåœ¨çº¿ä¼˜åŒ–æ¥è§¦æ¨¡å¼ï¼ˆæ»‘åŠ¨ã€ç²˜æ»ã€æ»šåŠ¨ã€åˆ†ç¦»ï¼‰ã€‚\n    - **åŠŸèƒ½**ï¼šä»¥~100Hzé«˜é¢‘é‡è§„åˆ’ï¼Œé²æ£’åœ°å°†ç‰©ä½“é©±åŠ¨è‡³å­ç›®æ ‡ã€‚\n    - **ä¼˜åŠ¿**ï¼šåˆ©ç”¨æ˜¾å¼çš„ç‰©ç†æ¨¡å‹è¿›è¡Œå®æ—¶æ ¡æ­£ï¼Œæä¾›äº†**æ‰§è¡Œå±‚é¢çš„é²æ£’æ€§**ï¼Œå¹¶éš”ç¦»äº†åŠ¨åŠ›å­¦ä¸ç¡®å®šæ€§ã€‚\n\n#### **3. æ–¹æ³•å¸¦æ¥çš„æ ¸å¿ƒä¼˜åŠ¿**\n- **æ•°æ®é«˜æ•ˆ**ï¼šRLå±‚åªéœ€å­¦ä¹ å‡ ä½•å’Œè¿åŠ¨å­¦è§„åˆ’ï¼Œæ— éœ€åœ¨å¤æ‚çš„æ¥è§¦åŠ¨åŠ›å­¦ä¸­æ¢ç´¢ã€‚å®éªŒæ˜¾ç¤ºï¼Œæ•°æ®éœ€æ±‚æ¯”ç«¯åˆ°ç«¯åŸºçº¿**å‡å°‘çº¦10å€**ã€‚\n- **é²æ£’æ€§å¼º**ï¼šä½å±‚MPCçš„é«˜é¢‘é‡è§„åˆ’èƒ½æŒç»­è¡¥å¿æ‰°åŠ¨å’Œæ¨¡å‹è¯¯å·®ï¼Œå¯¹æ‰§è¡Œå™¨å˜åŒ–ç­‰æ‰°åŠ¨è¡¨ç°å‡ºæå¼ºé²æ£’æ€§ã€‚\n- **é›¶æ ·æœ¬ä»¿çœŸåˆ°ç°å®è¿ç§»**ï¼šâ€œæ¥è§¦æ„å›¾â€æ¥å£å°†æ˜“è¿ç§»çš„å‡ ä½•/è¿åŠ¨å­¦æ¨ç†ä¸éš¾å»ºæ¨¡çš„æ¥è§¦åŠ¨åŠ›å­¦è§£è€¦ï¼Œä½¿å¾—åœ¨ä»¿çœŸä¸­è®­ç»ƒçš„é«˜å±‚ç­–ç•¥èƒ½ç›´æ¥ç”¨äºçœŸå®æœºå™¨äººã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆæ€»ç»“**\nè®ºæ–‡é€šè¿‡å¼•å…¥ **â€œæ¥è§¦æ„å›¾â€** è¿™ä¸€**åˆ†å±‚ç­–ç•¥æ¥å£**ï¼Œå°†çµå·§æ“ä½œåˆ†è§£ä¸ºä¸¤ä¸ªå¯ç‹¬ç«‹ä¼˜åŒ–çš„å­é—®é¢˜ï¼š\n1.  **é«˜å±‚â€œè§„åˆ’ä»€ä¹ˆâ€**ï¼šä½¿ç”¨æ•°æ®é«˜æ•ˆçš„RLå­¦ä¹ **å‡ ä½•æ„ŸçŸ¥çš„é•¿æœŸä»»åŠ¡è§„åˆ’**ã€‚\n2.  **ä½å±‚â€œå¦‚ä½•æ‰§è¡Œâ€**ï¼šä½¿ç”¨åŸºäºæ¨¡å‹çš„MPCå¤„ç†**éå…‰æ»‘çš„æ¥è§¦åŠ¨åŠ›å­¦**ï¼Œå®ç°é²æ£’æ‰§è¡Œã€‚\n\nè¿™ç§â€œ**RLè§„åˆ’æ„å›¾ + MPCå®ç°æ„å›¾**â€çš„èŒƒå¼ï¼Œç»“åˆå…¶åˆ›æ–°çš„è§‚æµ‹ç©ºé—´ä¸ç½‘ç»œè®¾è®¡ï¼Œä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿåœ¨**å°‘é‡æ•°æ®**ä¸‹ï¼Œå­¦ä¼šå¯¹**æœªè§è¿‡çš„å‡ ä½•å½¢çŠ¶**è¿›è¡Œ**é•¿æ—¶é—´ã€æ¥è§¦ä¸°å¯Œ**çš„æ“ä½œï¼Œå¹¶æˆåŠŸå®ç°**é›¶æ ·æœ¬çš„ä»¿çœŸåˆ°ç°å®è¿ç§»**ï¼Œåœ¨æ¨åŠ¨å’Œä¸‰ç»´é‡å®šå‘ä»»åŠ¡ä¸­è¾¾åˆ°æ¥è¿‘100%çš„æˆåŠŸç‡ã€‚",
            "timestamp": "2026-01-20T17:57:42.110716",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†åŸºäºæ‚¨æä¾›çš„è®ºæ–‡ã€ŠWhere to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulationã€‹åŠå…¶å¼•ç”¨çš„ç›¸å…³æ–‡çŒ®ï¼Œæ¢³ç†éæŠ“å–çµå·§æ“ä½œé¢†åŸŸçš„å‘å±•è„‰ç»œï¼Œå¹¶ç€é‡åˆ†æå‡ ä¸ªå…³é”®çš„å‘å±•æ–¹å‘ã€‚\n\n## éæŠ“å–çµå·§æ“ä½œå‘å±•è„‰ç»œæ¢³ç†\n\nè¯¥é¢†åŸŸçš„å‘å±•å¯ä»¥æ¦‚æ‹¬ä¸ºä»**åŸºäºæ¨¡å‹çš„ç»å…¸æ–¹æ³•**ï¼Œåˆ°**åŸºäºå­¦ä¹ çš„ç«¯åˆ°ç«¯æ–¹æ³•**ï¼Œå†åˆ°**ç»“åˆæ¨¡å‹ä¸å­¦ä¹ çš„å±‚æ¬¡åŒ–æ–¹æ³•**çš„æ¼”è¿›è¿‡ç¨‹ã€‚æ¯ä¸€æ­¥çš„æ”¹è¿›éƒ½æ—¨åœ¨è§£å†³å‰ä¸€ç±»æ–¹æ³•çš„æ ¹æœ¬æ€§ç“¶é¢ˆã€‚\n\n### 1. ç»å…¸æ–¹æ³•ï¼šåŸºäºæ¨¡å‹ä¸è¿åŠ¨åŸºå…ƒ\n*   **ä»£è¡¨å·¥ä½œ**ï¼š Lynch & Mason (1996) çš„ç¨³å®šæ¨åŠ¨åˆ†æï¼Œ Akella & Mason (1998) çš„å¤šè¾¹å½¢ç‰©ä½“æ‘†æ”¾ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼š åŸºäºåˆšä½“åŠ›å­¦å’Œæ¥è§¦åŠ›å­¦ï¼Œå¯¹ç‰¹å®šçš„æ“ä½œä»»åŠ¡ï¼ˆå¦‚æ¨åŠ¨ã€æ»‘åŠ¨ï¼‰è¿›è¡Œç²¾ç¡®çš„å»ºæ¨¡å’Œåˆ†æï¼Œè®¾è®¡å‡ºå¯è¯æ˜ç¨³å®šçš„æ§åˆ¶å™¨æˆ–è¿åŠ¨åŸºå…ƒã€‚\n*   **æ”¹è¿›ä¸è´¡çŒ®**ï¼š ä¸ºé¢†åŸŸå¥ å®šäº†åšå®çš„ç†è®ºåŸºç¡€ï¼Œæ˜ç¡®äº†æ¥è§¦æ¨¡å¼ï¼ˆç²˜æ»ã€æ»‘åŠ¨ã€åˆ†ç¦»ï¼‰å’Œæ‘©æ“¦é”¥ç­‰æ ¸å¿ƒæ¦‚å¿µã€‚\n*   **å±€é™æ€§**ï¼š\n    *   **å¯æ‰©å±•æ€§å·®**ï¼š æ¨¡å‹å’Œæ§åˆ¶å™¨ä¸¥é‡ä¾èµ–äºç‰©ä½“å‡ ä½•å’Œä»»åŠ¡å‡è®¾ï¼Œéš¾ä»¥æ³›åŒ–åˆ°å¤æ‚å½¢çŠ¶æˆ–æ–°ä»»åŠ¡ã€‚\n    *   **è§„åˆ’å›°éš¾**ï¼š æ¥è§¦åŠ¨åŠ›å­¦éå…‰æ»‘ã€æ··åˆï¼Œå¯¼è‡´é•¿æ—¶åŸŸè§„åˆ’ï¼ˆæ¶‰åŠå¤šæ¬¡æ¥è§¦å»ºç«‹ä¸åˆ†ç¦»ï¼‰çš„è®¡ç®—å¤æ‚åº¦æé«˜ã€‚\n\n### 2. åŸºäºå­¦ä¹ çš„ç«¯åˆ°ç«¯æ–¹æ³•\n*   **ä»£è¡¨å·¥ä½œ**ï¼š ä»¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆ–æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰ç›´æ¥å­¦ä¹ ä»æ„ŸçŸ¥åˆ°åŠ¨ä½œçš„æ˜ å°„ç­–ç•¥ï¼Œå¦‚ HACMan (Zhou et al., 2023)ï¼Œ Diffusion Policy (Chi et al., 2025)ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼š ç»•è¿‡å¤æ‚çš„ç‰©ç†å»ºæ¨¡ï¼Œåˆ©ç”¨æ•°æ®é©±åŠ¨çš„æ–¹å¼ç›´æ¥ä»ç»éªŒä¸­å­¦ä¹ æ“ä½œæŠ€èƒ½ã€‚å¸Œæœ›æ¨¡å‹èƒ½è‡ªåŠ¨å‘ç°æœ‰æ•ˆçš„æ¥è§¦ç­–ç•¥ã€‚\n*   **æ”¹è¿›ä¸è´¡çŒ®**ï¼š\n    *   **å‡å°‘äººå·¥å»ºæ¨¡**ï¼š å¯¹å¤æ‚ã€éš¾ä»¥è§£æå»ºæ¨¡çš„æ¥è§¦åŠ¨åŠ›å­¦æœ‰æ›´å¥½çš„é€‚åº”æ€§ã€‚\n    *   **çµæ´»æ€§é«˜**ï¼š ç†è®ºä¸Šå¯ä»¥å­¦ä¹ åˆ°éå¸¸å¤šæ ·å’Œå¤æ‚çš„æ“ä½œè¡Œä¸ºã€‚\n*   **å±€é™æ€§ï¼ˆæœ¬æ–‡é‡ç‚¹é’ˆå¯¹çš„é—®é¢˜ï¼‰**ï¼š\n    *   **æ•°æ®æ•ˆç‡ä½**ï¼š éå…‰æ»‘æ¥è§¦åŠ¨åŠ›å­¦ä½¿æ¢ç´¢å’Œä¿¡ç”¨åˆ†é…å›°éš¾ï¼Œéœ€è¦æµ·é‡äº¤äº’æ•°æ®ã€‚\n    *   **ä»¿çœŸåˆ°ç°å®è¿ç§»éš¾**ï¼š å­¦ä¹ çš„ç­–ç•¥è¿‡åº¦æ‹Ÿåˆä»¿çœŸå™¨çš„ç‰¹å®šç‰©ç†å‚æ•°ï¼ˆå¦‚æ‘©æ“¦ã€é˜»å°¼ï¼‰ï¼Œå¯¹å»ºæ¨¡è¯¯å·®æ•æ„Ÿã€‚\n    *   **å¯è§£é‡Šæ€§ä¸æ³›åŒ–æ€§å¼±**ï¼š ç­–ç•¥æ˜¯ä¸€ä¸ªâ€œé»‘ç®±â€ï¼Œå…¶å†³ç­–é€»è¾‘ä¸é€æ˜ï¼Œåœ¨ç‰©ä½“å‡ ä½•ã€ä»»åŠ¡ç›®æ ‡å˜åŒ–æ—¶æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚\n\n### 3. æ¨¡å‹ä¸å­¦ä¹ ç»“åˆçš„å±‚æ¬¡åŒ–æ–¹æ³•ï¼ˆå½“å‰ä¸»æµæ–¹å‘ï¼‰\nä¸ºäº†å…‹æœçº¯å­¦ä¹ æ–¹æ³•çš„ç¼ºé™·ï¼Œç ”ç©¶è€…å¼€å§‹æ¢ç´¢å°†æ¨¡å‹ï¼ˆå…ˆéªŒçŸ¥è¯†ï¼‰ä¸å­¦ä¹ ï¼ˆé€‚åº”æ€§ï¼‰ç›¸ç»“åˆçš„å±‚æ¬¡åŒ–æ¶æ„ã€‚æœ¬æ–‡æ­£æ˜¯è¿™ä¸€æ–¹å‘çš„æœ€æ–°è¿›å±•ã€‚è¯¥æ–¹å‘å†…éƒ¨åˆè¡ç”Ÿå‡ºå‡ ä¸ªä¸åŒçš„å‘å±•è·¯å¾„ï¼š\n\n#### å‘å±•æ–¹å‘ä¸€ï¼š**å­¦ä¹ è¾…åŠ©æ¨¡å‹é¢„æµ‹æ§åˆ¶**\n*   **æ ¸å¿ƒæ€æƒ³**ï¼š ä½¿ç”¨å­¦ä¹ æŠ€æœ¯æ¥**å¢å¼ºæˆ–ç®€åŒ–åŸºäºæ¨¡å‹çš„ä¼˜åŒ–å™¨ï¼ˆå¦‚MPCï¼‰**ï¼Œè€Œä¸æ˜¯å–ä»£å®ƒã€‚\n*   **æ¼”è¿›ä¸æ”¹è¿›**ï¼š\n    1.  **å­¦ä¹ åŠ¨åŠ›å­¦æ¨¡å‹**ï¼š æ—©æœŸå·¥ä½œï¼ˆå¦‚ Shin et al., 2019; Liu et al., 2025ï¼‰ç”¨RLå­¦ä¹ éš¾ä»¥å»ºæ¨¡çš„å¯¹è±¡ï¼ˆå¦‚å¯å˜å½¢ç‰©ä½“ï¼‰çš„ç®€åŒ–åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œç„¶åå°†å…¶åµŒå…¥MPCã€‚**æ”¹è¿›åœ¨äº**å°†å­¦ä¹ èŒƒå›´é™å®šåœ¨æ¨¡å‹ä¸ç¡®å®šéƒ¨åˆ†ï¼Œä¿ç•™äº†MPCçš„å¯é æ¡†æ¶ã€‚\n    2.  **ä¸ºä¼˜åŒ–å™¨æä¾›çƒ­å¯åŠ¨æˆ–é‡‡æ ·å¼•å¯¼**ï¼š å¦‚ Omer et al., 2021; Lopes et al., 2025 ç”¨å­¦ä¹ ç­–ç•¥ä¸ºåŸºäºé‡‡æ ·çš„MPCæä¾›é«˜è´¨é‡çš„åˆå§‹è§£ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚**æ”¹è¿›åœ¨äº**æå‡äº†å¤æ‚æ¥è§¦ä¼˜åŒ–é—®é¢˜çš„æ±‚è§£æ•ˆç‡ã€‚\n    3.  **å­¦ä¹ é«˜å±‚â€œæ„å›¾â€ä»¥å‚æ•°åŒ–åº•å±‚MPCï¼ˆæœ¬æ–‡çš„åˆ›æ–°ç‚¹ï¼‰**ï¼š æœ¬æ–‡æå‡ºçš„ **â€œæ¥è§¦æ„å›¾â€** æ¥å£æ˜¯è¿™ä¸€æ€è·¯çš„æ·±åŒ–ã€‚ä¸ä¹‹å‰RLä»…é¢„æµ‹è½¨è¿¹æˆ–è¾…åŠ©ä¼˜åŒ–ä¸åŒï¼Œæœ¬æ–‡è®©RLå­¦ä¹ **å‡ ä½•-è¿åŠ¨å­¦å±‚é¢**çš„æŠ½è±¡å†³ç­–ï¼ˆâ€œåœ¨å“ªé‡Œæ¥è§¦â€ã€â€œè¾¾åˆ°ä»€ä¹ˆç‰©ä½“ä½å§¿å­ç›®æ ‡â€ï¼‰ï¼Œè€Œå°†**æ¥è§¦åŠ¨åŠ›å­¦å±‚é¢**çš„å¤æ‚è®¡ç®—ï¼ˆâ€œå¦‚ä½•é€šè¿‡æ¥è§¦å®ç°å®ƒâ€ï¼‰å®Œå…¨äº¤ç»™ä¸€ä¸ªé€šç”¨çš„ã€åŸºäºç‰©ç†çš„æ¥è§¦éšå¼MPCã€‚\n        *   **å…³é”®æ”¹è¿›**ï¼š\n            *   **è§£è€¦ä¸æ³›åŒ–**ï¼š å°†æ˜“äºä»¿çœŸçš„å‡ ä½•æ¨ç†ä¸éš¾ä»¥ä»¿çœŸçš„æ¥è§¦åŠ¨åŠ›å­¦è§£è€¦ï¼Œä½¿é«˜å±‚ç­–ç•¥çš„ä»¿çœŸåˆ°ç°å®è¿ç§»æ›´é²æ£’ã€‚\n            *   **æ•°æ®æ•ˆç‡**ï¼š MPCæ‰¿æ‹…äº†éå…‰æ»‘åŠ¨åŠ›å­¦ä¼˜åŒ–çš„è´Ÿæ‹…ï¼ŒRLåªéœ€åœ¨æ›´å¹³æ»‘ã€æ›´æŠ½è±¡çš„æ„å›¾ç©ºé—´å­¦ä¹ ï¼Œæå¤§æå‡äº†æ•°æ®æ•ˆç‡ï¼ˆè®ºæ–‡ç§°æ¯”ç«¯åˆ°ç«¯åŸºçº¿é«˜10å€ï¼‰ã€‚\n            *   **å¯è§£é‡Šæ€§**ï¼š å†³ç­–è¿‡ç¨‹è¢«æ¸…æ™°åœ°åˆ†å±‚ï¼Œä¾¿äºåˆ†æå’Œè°ƒè¯•ã€‚\n\n#### å‘å±•æ–¹å‘äºŒï¼š**åŸºäºæœç´¢/é‡‡æ ·çš„å±‚æ¬¡åŒ–è§„åˆ’**\n*   **æ ¸å¿ƒæ€æƒ³**ï¼š åœ¨é«˜å±‚ä½¿ç”¨æœç´¢æˆ–é‡‡æ ·æ–¹æ³•ï¼ˆè€Œéå­¦ä¹ ï¼‰æ¥è§„åˆ’æ¥è§¦åºåˆ—ï¼Œåº•å±‚ä½¿ç”¨æ¨¡å‹è¿›è¡Œè½¨è¿¹ç”Ÿæˆæˆ–æ§åˆ¶ã€‚\n*   **ä»£è¡¨å·¥ä½œä¸æ”¹è¿›**ï¼š\n    *   **PushAnything (Bui et al., 2025)**ï¼š åœ¨é«˜å±‚**é‡‡æ ·**å¯èƒ½çš„æ¥è§¦ç‚¹ï¼Œç„¶åç”¨MPCè¯„ä¼°æ¯ä¸ªæ¥è§¦ç‚¹çš„æ•ˆæœå¹¶æ‰§è¡Œæœ€ä¼˜è€…ã€‚**æ”¹è¿›åœ¨äº**å®ç°äº†å¯¹æœªçŸ¥ç‰©ä½“çš„å³æ—¶æ“ä½œï¼Œä½†é‡‡æ ·æ–¹å¼å¯èƒ½ä¸å¤Ÿé«˜æ•ˆã€‚\n    *   **HiDex (Cheng et al., 2023)**ï¼š ä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œæ¥è§¦é€‰æ‹©ï¼Œç„¶åè¿›è¡Œè·¯å¾„è§„åˆ’ã€‚**æ”¹è¿›åœ¨äº**é€šè¿‡æœç´¢æ˜¾å¼åœ°æ¨ç†é•¿æ—¶åŸŸæ¥è§¦åºåˆ—ã€‚\n    *   **ä¸æœ¬æ–‡å¯¹æ¯”**ï¼š è¿™äº›æ–¹æ³•ä¾èµ–äºåœ¨çº¿æœç´¢/é‡‡æ ·ï¼Œè®¡ç®—æˆæœ¬å¯èƒ½éšé—®é¢˜å¤æ‚åº¦å¢åŠ è€Œå‰§å¢ã€‚**æœ¬æ–‡ç”¨å­¦ä¹ ç­–ç•¥æ›¿ä»£äº†åœ¨çº¿æœç´¢**ï¼Œé€šè¿‡ç¦»çº¿è®­ç»ƒè·å¾—ä¸€ä¸ªå¿«é€Ÿçš„é«˜å±‚å†³ç­–å™¨ï¼Œå®ç°äº†å®æ—¶æ€§èƒ½ã€‚\n\n#### å‘å±•æ–¹å‘ä¸‰ï¼š**å®Œå…¨åŸºäºå­¦ä¹ çš„å±‚æ¬¡åŒ–ç­–ç•¥**\n*   **æ ¸å¿ƒæ€æƒ³**ï¼š å±‚æ¬¡ç»“æ„çš„ä¸¤å±‚éƒ½ä½¿ç”¨å­¦ä¹ æ¨¡å‹ï¼Œä½†è¿›è¡Œä»»åŠ¡åˆ†è§£ã€‚\n*   **ä»£è¡¨å·¥ä½œä¸æ”¹è¿›**ï¼š\n    *   **HACMan (Zhou et al., 2023)**ï¼š é«˜å±‚ç½‘ç»œé¢„æµ‹æ¥è§¦ç‚¹ï¼Œåº•å±‚ç½‘ç»œé¢„æµ‹æœ«ç«¯æ‰§è¡Œå™¨çš„ä½ç§»ã€‚ä¸¤å±‚å‡ä¸ºç¥ç»ç½‘ç»œã€‚**æ”¹è¿›åœ¨äº**åœ¨çº¯å­¦ä¹ æ¡†æ¶å†…å¼•å…¥äº†æ¥è§¦é€‰æ‹©çš„æŠ½è±¡å±‚ã€‚\n    *   **HDP (Wang et al., 2025)**ï¼š ä½¿ç”¨æ‰©æ•£æ¨¡å‹åˆ†åˆ«ç”Ÿæˆæ¥è§¦ç‚¹å’Œè½¨è¿¹ã€‚\n    *   **ä¸æœ¬æ–‡å¯¹æ¯”**ï¼š è¿™äº›æ–¹æ³•ä¿æŒäº†â€œç«¯åˆ°ç«¯â€å­¦ä¹ çš„çµæ´»æ€§ï¼Œä½†**åº•å±‚ä»ç„¶éœ€è¦å­¦ä¹ æ¥è§¦åŠ¨åŠ›å­¦**ï¼Œå› æ­¤ä»ç„¶é¢ä¸´æ•°æ®æ•ˆç‡ã€ä»¿çœŸåˆ°ç°å®è¿ç§»çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡çš„**å…³é”®æ”¹è¿›**åœ¨äºç”¨**åŸºäºç‰©ç†æ¨¡å‹çš„MPC**å–ä»£äº†å­¦ä¹ çš„åº•å±‚ç­–ç•¥ï¼Œä»è€Œä»æ ¹æœ¬ä¸Šè·å¾—äº†MPCçš„é²æ£’æ€§ã€å¯¹æ‰°åŠ¨çš„å³æ—¶ååº”èƒ½åŠ›ä»¥åŠç²¾ç¡®çš„ç‰©ç†ä¸€è‡´æ€§ã€‚\n\n## æ€»ç»“\néæŠ“å–çµå·§æ“ä½œçš„å‘å±•è„‰ç»œæ¸…æ™°åœ°å±•ç¤ºäº†ä»**çº¯æ¨¡å‹**åˆ°**çº¯å­¦ä¹ **ï¼Œå†åˆ°**æ¨¡å‹ä¸å­¦ä¹ æ·±åº¦èåˆ**çš„è¶‹åŠ¿ã€‚æœ¬æ–‡æå‡ºçš„ **â€œRL-MPC + æ¥è§¦æ„å›¾â€** æ¡†æ¶ï¼Œä»£è¡¨äº†å½“å‰ä¸€ä¸ªéå¸¸æœ‰åŠ›çš„å‘å±•æ–¹å‘ï¼š\n1.  **å®ƒç»§æ‰¿äº†åŸºäºæ¨¡å‹æ–¹æ³•çš„ç‰©ç†ç²¾ç¡®æ€§å’Œé²æ£’æ€§**ï¼ˆé€šè¿‡åº•å±‚MPCï¼‰ã€‚\n2.  **å®ƒå¸æ”¶äº†å­¦ä¹ æ–¹æ³•çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›**ï¼ˆé€šè¿‡é«˜å±‚RLç­–ç•¥å¤„ç†å¤šæ ·å‡ ä½•å’Œé•¿æ—¶åŸŸè§„åˆ’ï¼‰ã€‚\n3.  **å®ƒé€šè¿‡åˆ›æ–°çš„â€œæ¥è§¦æ„å›¾â€æ¥å£**ï¼Œå®ç°äº†ä¸¤ä¸ªå±‚æ¬¡é—´æ¸…æ™°ã€é«˜æ•ˆçš„è§£è€¦ï¼Œä»è€Œåœ¨æ•°æ®æ•ˆç‡ã€ä»¿çœŸåˆ°ç°å®è¿ç§»å’Œä»»åŠ¡æ³›åŒ–æ–¹é¢å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œæ˜¯æœç€æ„å»º**æ•°æ®é«˜æ•ˆã€é²æ£’ä¸”é€šç”¨**çš„çµå·§æ“ä½œç³»ç»Ÿè¿ˆå‡ºçš„å…³é”®ä¸€æ­¥ã€‚å…¶å±€é™æ€§ï¼ˆå¦‚å¯¹ç²¾ç¡®æ„ŸçŸ¥çš„ä¾èµ–ã€å¤šæŒ‡æ“ä½œçš„ç»„åˆçˆ†ç‚¸é—®é¢˜ï¼‰ä¹Ÿä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚",
            "timestamp": "2026-01-20T17:58:39.831893",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œä½œè€…å›¢é˜Ÿï¼ˆç‰¹åˆ«æ˜¯Wanxin Jinå’ŒMichael Posaï¼‰åœ¨æ¥è§¦åŠ¨åŠ›å­¦å»ºæ¨¡ä¸éæŠ“å–æ“ä½œé¢†åŸŸæœ‰ä¸€ç³»åˆ—ç›¸å…³çš„å‰æœŸå·¥ä½œã€‚è¿™äº›å·¥ä½œä¸ºæœ¬æ–‡æå‡ºçš„åˆ†å±‚RL-MPCæ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚ä»¥ä¸‹æ˜¯ä¸»è¦ç›¸å…³å‰ä½œçš„æ¢³ç†ã€å¯¹æ¯”åŠæœ¬æ–‡çš„å…·ä½“æ”¹è¿›ã€‚\n\n### å‰ä½œæ¢³ç†ä¸å¯¹æ¯”\n\n| å‰ä½œæ–‡çŒ® | æ ¸å¿ƒè´¡çŒ® / æ–¹æ³• | ä¸æœ¬æ–‡ (`Where to Touch, How to Contact`) çš„ä¸»è¦åŒºåˆ« | æœ¬æ–‡çš„ä¸»è¦æ”¹è¿›ä¸åˆ›æ–°ç‚¹ |\n| :--- | :--- | :--- | :--- |\n| **PushAnything: single- and multi-object pushing from first sight with contact-implicit MPC** <br> (Bui et al., 2025) [5] | æå‡ºä¸€ä¸ª**åŸºäºé‡‡æ ·çš„è§„åˆ’æ¡†æ¶**ã€‚ç»™å®šç‰©ä½“ç‚¹äº‘ï¼Œ**éšæœºé‡‡æ ·**å€™é€‰æ¥è§¦ç‚¹ï¼Œç„¶åä½¿ç”¨**æ¥è§¦éšå¼MPC**è¯„ä¼°æ¯ä¸ªæ¥è§¦ç‚¹æ¨åŠ¨ç‰©ä½“çš„æ•ˆæœï¼Œé€‰æ‹©æœ€ä¼˜è€…æ‰§è¡Œã€‚ | 1. **å†³ç­–æœºåˆ¶ä¸åŒ**ï¼šå‰ä½œé‡‡ç”¨**æ— æ¨¡å‹é‡‡æ ·+è¯„ä¼°**ï¼Œæœ¬æ–‡é‡‡ç”¨**å­¦ä¹ å‹ç­–ç•¥**é¢„æµ‹æ¥è§¦æ„å›¾ã€‚<br>2. **å†³ç­–å†…å®¹ä¸åŒ**ï¼šå‰ä½œä»…é€‰æ‹©â€œåœ¨å“ªé‡Œæ¥è§¦â€(`where`)ï¼Œæœ¬æ–‡åŒæ—¶é¢„æµ‹â€œæ¥è§¦åç‰©ä½“çš„å­ç›®æ ‡â€(`what`)ï¼Œå½¢æˆå®Œæ•´çš„`Contact Intention`ã€‚<br>3. **æ•ˆç‡ä¸æ³›åŒ–æ€§**ï¼šå‰ä½œçš„éšæœºé‡‡æ ·åœ¨å¤æ‚å‡ ä½•å’Œé•¿æ—¶åŸŸä»»åŠ¡ä¸Šå¯èƒ½ä½æ•ˆï¼›æœ¬æ–‡çš„RLç­–ç•¥é€šè¿‡è®­ç»ƒå­¦ä¹ å‡ ä½•å’Œè¿åŠ¨å­¦è§„å¾‹ï¼Œèƒ½æ›´é«˜æ•ˆåœ°åšå‡ºåºåˆ—å†³ç­–ã€‚ | **å¼•å…¥å­¦ä¹ å‹é«˜å±‚ç­–ç•¥**ï¼Œå°†åŸºäºé‡‡æ ·çš„å¼€ç¯æœç´¢ï¼Œè½¬å˜ä¸ºåŸºäºå­¦ä¹ çš„é—­ç¯å†³ç­–ã€‚è¿™æ˜¾è‘—æå‡äº†é•¿æ—¶åŸŸã€å¤šæ¥è§¦åºåˆ—ä»»åŠ¡çš„æ•°æ®æ•ˆç‡å’ŒæˆåŠŸç‡ã€‚ |\n| **HiDex: Enhancing Dexterity in Robotic Manipulation via Hierarchical Contact Exploration** <br> (Cheng et al., 2023) [7] | æå‡º**åˆ†å±‚æ¥è§¦æ¢ç´¢**æ¡†æ¶ã€‚é«˜å±‚ä½¿ç”¨**è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)** æ¢ç´¢æ¥è§¦æ¨¡å¼åºåˆ—ï¼Œä½å±‚ä½¿ç”¨**è·¯å¾„è§„åˆ’**ç”Ÿæˆæ— ç¢°æ’è½¨è¿¹æ¥æ‰§è¡Œæ¯ä¸ªæ¥è§¦ã€‚ | 1. **é«˜å±‚å†³ç­–å™¨ä¸åŒ**ï¼šå‰ä½œä½¿ç”¨**MCTS**è¿›è¡Œè§„åˆ’ï¼Œæœ¬æ–‡ä½¿ç”¨**å¼ºåŒ–å­¦ä¹ (RL)** è®­ç»ƒç­–ç•¥ã€‚<br>2. **ä½å±‚æ‰§è¡Œå™¨ä¸åŒ**ï¼šå‰ä½œä½¿ç”¨**å‡ ä½•è·¯å¾„è§„åˆ’**ï¼Œæœ¬æ–‡ä½¿ç”¨**åŸºäºç‰©ç†æ¨¡å‹çš„æ¥è§¦éšå¼MPC**ã€‚<br>3. **åŠ¨æ€æ€§å¤„ç†**ï¼šå‰ä½œçš„è·¯å¾„è§„åˆ’å¯èƒ½æ— æ³•ç²¾ç»†å¤„ç†æ¥è§¦åŠ¨åŠ›å­¦ï¼ˆå¦‚æ»‘åŠ¨ã€æ»šåŠ¨ï¼‰ï¼›æœ¬æ–‡çš„MPCæ˜¾å¼ä¼˜åŒ–æ¥è§¦åŠ›ä¸æ¨¡å¼ï¼Œå¯¹æ‰°åŠ¨æ›´é²æ£’ã€‚ | **ç”¨RLæ›¿ä»£MCTS**ï¼Œå®ç°æ›´å¿«çš„åœ¨çº¿å†³ç­–ï¼›**ç”¨åŠ¨æ€MPCæ›¿ä»£å‡ ä½•è§„åˆ’**ï¼Œä½¿ä½å±‚æ§åˆ¶èƒ½ä¸»åŠ¨åˆ©ç”¨å’Œé€‚åº”å¤æ‚çš„æ¥è§¦ç‰©ç†ï¼Œå®ç°äº†â€œå¦‚ä½•æ¥è§¦â€(`how`)çš„ä¼˜åŒ–ï¼Œæå‡äº†æ‰§è¡Œçš„é²æ£’æ€§å’ŒåŠ¨æ€é€‚åº”æ€§ã€‚ |\n| **HACMan: Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation** <br> (Zhou et al., 2023) [66] | æå‡ºä¸€ä¸ª**ç«¯åˆ°ç«¯çš„åˆ†å±‚RLç­–ç•¥**ã€‚ç­–ç•¥ç›´æ¥è¾“å‡ºæ¥è§¦ç‚¹é€‰æ‹©åŠå¯¹åº”çš„æœ«ç«¯æ‰§è¡Œå™¨ä½ç§»åŠ¨ä½œã€‚æ•´ä¸ªç­–ç•¥ï¼ˆé«˜å±‚é€‰æ‹©+ä½å±‚åŠ¨ä½œï¼‰é€šè¿‡RLä¸€æ¬¡æ€§è®­ç»ƒå®Œæˆã€‚ | 1. **å±‚æ¬¡ç»“æ„è€¦åˆåº¦**ï¼šå‰ä½œæ˜¯**å­¦ä¹ å‹åˆ†å±‚**ï¼Œä½†é«˜ä½å±‚éƒ½æ˜¯ç¥ç»ç½‘ç»œï¼Œè€¦åˆç´§å¯†ã€‚æœ¬æ–‡æ˜¯**å­¦ä¹ -ä¼˜åŒ–æ··åˆåˆ†å±‚**ï¼Œé«˜å±‚æ˜¯RLï¼Œä½å±‚æ˜¯æ¨¡å‹ä¼˜åŒ–(MPC)ã€‚<br>2. **ç‰©ç†æ¨¡å‹åˆ©ç”¨**ï¼šå‰ä½œå®Œå…¨ä¾èµ–å­¦ä¹ æ¥æŒæ¡æ¥è§¦åŠ¨åŠ›å­¦ï¼Œæœ¬æ–‡å°†åŠ¨åŠ›å­¦æ¨ç†**å¸è½½ç»™åŸºäºç‰©ç†æ¨¡å‹çš„MPC**ã€‚<br>3. **æ•°æ®æ•ˆç‡ä¸é²æ£’æ€§**ï¼šå¦‚è®ºæ–‡å®éªŒæ‰€ç¤ºï¼Œçº¯å­¦ä¹ æ¡†æ¶ï¼ˆHACMané£æ ¼ï¼‰éœ€è¦æ›´å¤šæ•°æ®ï¼Œä¸”æˆåŠŸç‡å’Œç¨³å®šæ€§ä½äºæœ¬æ–‡çš„RL-MPCæ··åˆæ¡†æ¶ã€‚ | **è§£è€¦å­¦ä¹ ä¸ä¼˜åŒ–**ã€‚å°†éš¾ä»¥å­¦ä¹ çš„éå…‰æ»‘æ¥è§¦åŠ¨åŠ›å­¦äº¤ç»™åŸºäºæ¨¡å‹çš„MPCå¤„ç†ï¼ŒRLåªéœ€å­¦ä¹ ç›¸å¯¹ç®€å•çš„å‡ ä½•-è¿åŠ¨å­¦æ¨ç†ã€‚è¿™å¸¦æ¥äº†**æ•°æ®æ•ˆç‡çš„æ˜¾è‘—æå‡**ï¼ˆè®ºæ–‡ä¸­çº¦10å€äºç«¯åˆ°ç«¯åŸºçº¿ï¼‰å’Œ**æ›´å¼ºçš„æ‰§è¡Œé²æ£’æ€§**ï¼ˆMPCé«˜é¢‘é‡è§„åˆ’çº æ­£è¯¯å·®ï¼‰ã€‚ |\n| **Complementarity-free multi-contact modeling and optimization for dexterous manipulation** <br> (Jin, 2024) [25] | æå‡ºäº† **â€œäº’è¡¥æ€§è‡ªç”±â€(ComFree)** æ¥è§¦åŠ¨åŠ›å­¦æ¨¡å‹ã€‚è¯¥æ¨¡å‹é¿å…äº†ä¼ ç»Ÿçš„çº¿æ€§äº’è¡¥é—®é¢˜ï¼Œæä¾›äº†**é—­å¼è§£**ï¼Œä»è€Œå®ç°äº†**é«˜é€Ÿçš„æ¥è§¦éšå¼æ¨¡å‹é¢„æµ‹æ§åˆ¶(MPC)**ã€‚ | 1. **å·¥ä½œå±‚çº§**ï¼šå‰ä½œæ˜¯**åº•å±‚æ§åˆ¶ä¸ä¼˜åŒ–æ–¹æ³•**ï¼Œæœ¬æ–‡æ˜¯**å®Œæ•´çš„å±‚æ¬¡åŒ–æ“çºµæ¡†æ¶**ã€‚<br>2. **è´¡çŒ®æ€§è´¨**ï¼šå‰ä½œæ˜¯æœ¬æ–‡çš„**ä½¿èƒ½æŠ€æœ¯**ï¼ˆå³æœ¬æ–‡ä½å±‚MPCæ‰€ä½¿ç”¨çš„æ ¸å¿ƒåŠ¨åŠ›å­¦æ¨¡å‹ä¸æ±‚è§£å™¨ï¼‰ã€‚ | **å°†ComFree-MPCé›†æˆåˆ°ä¸€ä¸ªæ–°é¢–çš„åˆ†å±‚æ¶æ„ä¸­**ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®ä¸åœ¨äºMPCæœ¬èº«ï¼Œè€Œåœ¨äºè®¾è®¡äº†`Contact Intention`æ¥å£ï¼Œå°†é«˜å±‚RLç­–ç•¥ä¸è¿™ä¸ªé«˜æ•ˆçš„åº•å±‚MPCè¿æ¥èµ·æ¥ï¼Œæ„å»ºäº†ä¸€ä¸ªå®Œæ•´ä¸”é«˜æ•ˆçš„æ“çºµç³»ç»Ÿã€‚ |\n| **Consensus complementarity control for multi-contact MPC** <br> (Aydinoglu et al., 2024) [2] | ç ”ç©¶**å¤šæ¥è§¦MPC**çš„åˆ†å¸ƒå¼ä¼˜åŒ–ä¸ä¸€è‡´æ€§æ§åˆ¶é—®é¢˜ï¼Œä¸“æ³¨äºå¤šæœºå™¨äººæˆ–å¤šæŒ‡åè°ƒæ“çºµåœºæ™¯ä¸‹çš„æ§åˆ¶ç®—æ³•ã€‚ | 1. **é—®é¢˜è§„æ¨¡ä¸ç„¦ç‚¹**ï¼šå‰ä½œèšç„¦äº**å¤šæ¥è§¦ç‚¹ååŒ**çš„MPCç®—æ³•æœ¬èº«ã€‚æœ¬æ–‡èšç„¦äº**å•æ¥è§¦ç‚¹åœºæ™¯ä¸‹çš„åˆ†å±‚å†³ç­–æ¡†æ¶**ï¼Œä½†ä¸ºæœªæ¥æ‰©å±•åˆ°å¤šæŒ‡æ“ä½œå¥ å®šäº†åŸºç¡€ã€‚<br>2. **æ¡†æ¶è§’è‰²**ï¼šå‰ä½œå¯è¢«è§†ä¸ºæœ¬æ–‡ä½å±‚MPCåœ¨å¤šæ¥è§¦æƒ…å†µä¸‹çš„æ½œåœ¨æ‰©å±•ï¼Œä½†æœ¬æ–‡å½“å‰å·¥ä½œæœªæ¶‰åŠå¤šæŒ‡åè°ƒã€‚ | æœ¬æ–‡çš„**æ ¸å¿ƒåˆ›æ–°åœ¨é«˜å±‚æ¥å£å’Œåˆ†å±‚èŒƒå¼**ã€‚`Contact Intention`çš„è®¾è®¡æ˜¯é€šç”¨çš„ï¼Œç†è®ºä¸Šå¯ä»¥æ‰©å±•ä¸ºå¤šæ¥è§¦ç‚¹ï¼ˆè®ºæ–‡æŒ‡å‡ºè¿™æ˜¯æœªæ¥å·¥ä½œï¼‰ã€‚æœ¬æ–‡é¦–å…ˆåœ¨å•æ¥è§¦ç‚¹ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥èŒƒå¼çš„å·¨å¤§ä¼˜åŠ¿ã€‚ |\n\n### æ€»ç»“ï¼šæœ¬æ–‡ç›¸å¯¹äºå‰ä½œçš„æ ¸å¿ƒæ”¹è¿›\n\n1.  **æå‡ºäº†ç»Ÿä¸€çš„â€œæ¥è§¦æ„å›¾â€(Contact Intention)æ¥å£**ï¼šè¿™æ˜¯æœ€æ ¸å¿ƒçš„æ¦‚å¿µåˆ›æ–°ã€‚å®ƒæ˜ç¡®åœ°å°†æ“çºµä»»åŠ¡åˆ†è§£ä¸ºï¼š\n    *   **é«˜å±‚ (RL)**ï¼šå›ç­” **`Where to Touch` (å‡ ä½•)** å’Œ **`What to Achieve` (è¿åŠ¨å­¦å­ç›®æ ‡)**ã€‚\n    *   **ä½å±‚ (MPC)**ï¼šå›ç­” **`How to Contact` (æ¥è§¦åŠ¨åŠ›å­¦)**ã€‚\n    è¿™ä¸€æ¥å£**æ¸…æ™°è§£è€¦äº†è¯­ä¹‰/å‡ ä½•æ¨ç†ä¸ç‰©ç†æ‰§è¡Œ**ï¼Œæ˜¯ä¹‹å‰å·¥ä½œæ‰€ä¸å…·å¤‡çš„ã€‚\n\n2.  **æ„å»ºäº†ç´§å¯†è€¦åˆçš„â€œå­¦ä¹ -ä¼˜åŒ–â€æ··åˆåˆ†å±‚æ¡†æ¶**ï¼šä¸åŒäºçº¯é‡‡æ ·è§„åˆ’(`PushAnything`)ã€çº¯æœç´¢è§„åˆ’(`HiDex`)æˆ–çº¯å­¦ä¹ åˆ†å±‚(`HACMan`)ï¼Œæœ¬æ–‡åˆ›é€ æ€§åœ°å°†**æ•°æ®é©±åŠ¨çš„RL**ä¸**æ¨¡å‹é©±åŠ¨çš„MPC**æ·±åº¦èåˆã€‚RLè´Ÿè´£é«˜å±‚ç­–ç•¥çš„æ³›åŒ–æ€§ï¼ŒMPCè´Ÿè´£åº•å±‚æ§åˆ¶çš„ç²¾ç¡®æ€§ä¸é²æ£’æ€§ï¼Œä¸¤è€…é€šè¿‡`Contact Intention`å‚æ•°åŒ–è¿æ¥ã€‚\n\n3.  **å®ç°äº†å¤šé¡¹æ€§èƒ½çš„æ˜¾è‘—æå‡**ï¼š\n    *   **æ•°æ®æ•ˆç‡**ï¼šå¾—ç›Šäºè§£è€¦ï¼ŒRLæ— éœ€å­¦ä¹ å¤æ‚åŠ¨åŠ›å­¦ï¼Œè®­ç»ƒæ•°æ®é‡æ¯”ç«¯åˆ°ç«¯RLåŸºçº¿**å‡å°‘çº¦10å€**ã€‚\n    *   **é²æ£’æ€§ä¸sim-to-real**ï¼šä½å±‚MPCåŸºäºç‰©ç†æ¨¡å‹è¿›è¡Œé«˜é¢‘é‡è§„åˆ’ï¼Œèƒ½å®æ—¶è¡¥å¿æ¨¡å‹è¯¯å·®å’Œæ‰°åŠ¨ï¼Œå®ç°äº†**é›¶æ ·æœ¬çš„ä»¿çœŸåˆ°ç°å®è¿ç§»**ã€‚\n    *   **ä»»åŠ¡æ€§èƒ½**ï¼šåœ¨å‡ ä½•æ³›åŒ–æ¨åŠ¨å’Œç‰©ä½“3Dé‡å®šå‘ä»»åŠ¡ä¸Šè¾¾åˆ°æ¥è¿‘100%çš„æˆåŠŸç‡ï¼Œä¸”èƒ½æ³›åŒ–åˆ°æœªè§è¿‡çš„ç‰©ä½“å‡ ä½•å½¢çŠ¶ã€‚\n\n4.  **è®¾è®¡äº†é’ˆå¯¹æ€§çš„å­¦ä¹ ç»„ä»¶**ï¼šä¸ºäº†é«˜æ•ˆå­¦ä¹ `Contact Intention`ï¼Œæœ¬æ–‡æå‡ºäº†ï¼š\n    *   **ä¸‰å…ƒç»„ç‰©ä½“ä¸­å¿ƒè§‚æµ‹ç©ºé—´**ï¼šè”åˆç¼–ç å‡ ä½•ã€ä»»åŠ¡è¿›åº¦ã€ç¢°æ’å¯è¡Œæ€§ã€‚\n    *   **åŒåˆ†æ”¯ç­–ç•¥ç½‘ç»œ**ï¼šåŒæ—¶å¤„ç†å±€éƒ¨å‡ ä½•ç‰¹å¾å’Œå…¨å±€è¿åŠ¨å­¦ä¸Šä¸‹æ–‡ã€‚\n    è¿™äº›è®¾è®¡ç¡®ä¿äº†ç­–ç•¥èƒ½æœ‰æ•ˆè¿›è¡Œå‡ ä½•æ„ŸçŸ¥çš„æ¨ç†ï¼Œå¦‚æ¶ˆèå®éªŒæ‰€è¯æ˜ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡ä¸æ˜¯å¯¹å‰ä½œä¸­MPCæˆ–RLæŠ€æœ¯çš„ç®€å•åº”ç”¨ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ªæ–°é¢–çš„æ¥å£å’Œæ¡†æ¶è®¾è®¡ï¼Œå°†å®ƒä»¬ç³»ç»Ÿæ€§åœ°æ•´åˆèµ·æ¥ï¼Œè§£å†³äº†â€œå‡ ä½•-è¿åŠ¨å­¦-åŠ¨åŠ›å­¦â€è”åˆæ¨ç†çš„éš¾é¢˜ï¼Œä»è€Œåœ¨æ•°æ®æ•ˆç‡ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚**",
            "timestamp": "2026-01-20T17:59:40.898083",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨è¯¦ç»†è§£æè¿™ç¯‡è®ºæ–‡ã€ŠWhere to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulationã€‹æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯æ–¹æ³•åˆ†æ\n\næœ¬æ–‡çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ç§**åˆ†å±‚å¼ºåŒ–å­¦ä¹ -æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆRL-MPCï¼‰æ¡†æ¶**ï¼Œç”¨äºè§£å†³æ¥è§¦ä¸°å¯Œçš„çµå·§æ“ä½œä»»åŠ¡ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†å¤æ‚çš„æ“ä½œä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå±‚æ¬¡ï¼š**é«˜å±‚ï¼ˆå‡ ä½•-è¿åŠ¨å­¦æ¨ç†ï¼‰** å’Œ **ä½å±‚ï¼ˆæ¥è§¦åŠ¨åŠ›å­¦æ¨ç†ï¼‰**ã€‚å…³é”®æŠ€æœ¯æ–¹æ³•æ€»ç»“å¦‚ä¸‹è¡¨ï¼š\n\n| æŠ€æœ¯æ¨¡å— | æ ¸å¿ƒæ¦‚å¿µ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°/å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. åˆ†å±‚æ¥å£ï¼šæ¥è§¦æ„å›¾** | å®šä¹‰é«˜å±‚ç­–ç•¥çš„è¾“å‡ºï¼Œä½œä¸ºè¿æ¥RLå’ŒMPCçš„æ¡¥æ¢ã€‚ | **æ•°å­¦å®šä¹‰**ï¼š<br> `I = (cÌ„Â¹, cÌ„Â², ..., cÌ„â¿, qÌ„_obj) âˆˆ Gâ¿ Ã— SE(3)` <br> - **cÌ„â±**: ç¬¬iä¸ªä¸»åŠ¨æ¥è§¦ç‚¹åœ¨**ç‰©ä½“è¡¨é¢**çš„åæ ‡ï¼ˆå®šä¹‰äºç‰©ä½“åæ ‡ç³»ï¼‰ã€‚æŒ‡å®šâ€œåœ¨å“ªé‡Œæ¥è§¦â€ã€‚<br> - **qÌ„_obj**: æ¥è§¦åæœŸæœ›è¾¾åˆ°çš„**ç‰©ä½“ä½å§¿å­ç›®æ ‡**ï¼ˆSE(3)ï¼‰ã€‚æŒ‡å®šâ€œè¦è¾¾åˆ°ä»€ä¹ˆç‰©ä½“çŠ¶æ€â€ã€‚<br> **æ ¸å¿ƒåˆ›æ–°**ï¼šå°†åŠ¨ä½œç©ºé—´ä»ä½å±‚å…³èŠ‚/æœ«ç«¯ä½ç§»ï¼Œæå‡ä¸ºç‰©ä½“ä¸­å¿ƒçš„å‡ ä½•-è¿åŠ¨å­¦æè¿°ï¼Œå®ç°äº†ä¸å…·ä½“æœºå™¨äººå½¢æ€çš„è§£è€¦ã€‚ | - åœ¨æœ¬æ–‡å®éªŒä¸­ï¼Œä½¿ç”¨å•æ ¹æ£’çŠ¶æœ«ç«¯æ‰§è¡Œå™¨ï¼Œå› æ­¤ `n=1`ã€‚<br> - æ¥è§¦ç‚¹ `cÌ„` ä»ç‰©ä½“ç‚¹äº‘ä¸‹é‡‡æ ·çš„**å…³é”®ç‚¹é›†åˆ P** ä¸­ç¦»æ•£é€‰æ‹©ã€‚ |\n| **2. é«˜å±‚ç­–ç•¥ï¼šåŸºäºRLçš„å‡ ä½•-è¿åŠ¨å­¦æ¨ç†** | è®­ç»ƒä¸€ä¸ªRLç­–ç•¥ï¼Œæ ¹æ®è§‚å¯Ÿé¢„æµ‹æ¥è§¦æ„å›¾ã€‚ | **a. è§‚å¯Ÿç©ºé—´è®¾è®¡ï¼ˆä¸‰å…ƒç»„å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºï¼‰**ï¼š<br> `o_k = (P, Î”P_k, D_k)` <br> 1. **å‡ ä½•ç»„ä»¶ P**: ä»ç‰©ä½“ç‚¹äº‘ä¸‹é‡‡æ ·çš„Nä¸ªå…³é”®ç‚¹ `{p_i}`ï¼Œåœ¨**ç‰©ä½“åæ ‡ç³»**ä¸­å®šä¹‰ã€‚è¡¨å¾ç‰©ä½“å½¢çŠ¶ï¼Œå¹¶ä½œä¸ºæ¥è§¦ç‚¹å€™é€‰é›†ã€‚<br> 2. **ç›®æ ‡ç»„ä»¶ Î”P_k**: **å…³é”®ç‚¹ç›®æ ‡æµ**ã€‚`Î´_i,k = p_i,k_target - p_i`ï¼Œå…¶ä¸­ `p_i,k_target` æ˜¯å½“ç‰©ä½“å¤„äºç›®æ ‡ä½å§¿æ—¶ï¼ŒåŒä¸€ä¸ªå…³é”®ç‚¹ `p_i` åœ¨å½“å‰ç‰©ä½“åæ ‡ç³»ä¸­çš„ä½ç½®ã€‚è¯¥è®¾è®¡**å°†ç›®æ ‡ç¼–ç åœ¨ç‰©ä½“åæ ‡ç³»ä¸­**ï¼Œé¿å…äº†ä¸–ç•Œåæ ‡ç³»ä¸ç‰©ä½“åæ ‡ç³»ä¸ä¸€è‡´å¸¦æ¥çš„å­¦ä¹ è´Ÿæ‹…ï¼Œæ˜¯ç¨³å®šå­¦ä¹ çš„å…³é”®ã€‚<br> 3. **ç¢°æ’ç»„ä»¶ D_k**: **æ¯å…³é”®ç‚¹é—´éš™è·ç¦»** `{d_i,k}`ï¼Œå³æ¯ä¸ªå…³é”®ç‚¹åˆ°ç¯å¢ƒï¼ˆå¦‚åœ°é¢ã€å¢™å£ï¼‰çš„æœ€å°æ¬§æ°è·ç¦»ã€‚ç”¨äºåˆ¤æ–­æ¥è§¦å¯è¡Œæ€§ï¼ˆä¾‹å¦‚ï¼Œåº•é¢å…³é”®ç‚¹ `dâ‰ˆ0`ï¼Œä¸å¯æ¥è§¦ï¼‰ã€‚ | - å…³é”®ç‚¹æ•°é‡ `N = 256`ã€‚<br> - æ‰€æœ‰è§‚å¯Ÿç»„ä»¶å‡é™¤ä»¥ç‰©ä½“çš„ç‰¹å¾å°ºå¯¸ï¼ˆå­—æ¯ç‰©ä½“ä¸º0.15mï¼Œç«‹æ–¹ä½“ä¸º0.10mï¼‰è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥æé«˜æ³›åŒ–æ€§ã€‚ |\n| | | **b. åŠ¨ä½œç©ºé—´è®¾è®¡**ï¼š<br> `a = (cÌ„Â¹, cÌ„Â², ..., cÌ„â¿, w_pos, w_ori) = Ï€(o)` <br> - æ¥è§¦ç‚¹ä» `P` ä¸­ç¦»æ•£é€‰æ‹©ã€‚<br> - **å­ç›®æ ‡éšå¼å‚æ•°åŒ–**ï¼šç­–ç•¥ä¸ç›´æ¥é¢„æµ‹SE(3)ä½å§¿ï¼Œè€Œæ˜¯é¢„æµ‹MPCç»ˆç«¯ä»£ä»·å‡½æ•°çš„æƒé‡ `(w_pos, w_ori)`ï¼Œä»è€Œéšå¼å®šä¹‰å­ç›®æ ‡ï¼š<br> `V(q_obj, qÌ„_obj) = w_pos * e_pos(p_obj, p_target) + w_ori * e_ori(r_obj, r_target)` <br> é€šè¿‡è°ƒæ•´æƒé‡ï¼Œå¯ä»¥æŒ‡å®šå­ç›®æ ‡æ˜¯ä»…å…³æ³¨ä½ç½®ã€ä»…å…³æ³¨å§¿æ€ï¼Œæˆ–ä¸¤è€…å…¼é¡¾ã€‚è¿™æ¯”ç›´æ¥æ¢ç´¢SE(3)ç©ºé—´æ›´é«˜æ•ˆã€‚ | - `w_pos âˆˆ {0, 50, 100, 150, 200}` <br> - `w_ori âˆˆ {0, 2, 4, 6, 8}` <br> - ä¾‹å¦‚ `(100, 0)` è¡¨ç¤ºå­ç›®æ ‡ä¸ºåˆ°è¾¾ç›®æ ‡ä½ç½®ï¼Œå§¿æ€ä»»æ„ï¼›`(0, 4)` è¡¨ç¤ºå­ç›®æ ‡ä¸ºåˆ°è¾¾ç›®æ ‡å§¿æ€ï¼Œä½ç½®ä»»æ„ã€‚ |\n| | | **c. ç­–ç•¥ç½‘ç»œæ¶æ„ï¼ˆåŒåˆ†æ”¯è®¾è®¡ï¼‰**ï¼š<br> 1. **å‡ ä½•åˆ†æ”¯**ï¼šé‡‡ç”¨åˆ†å‰²é£æ ¼çš„PointNet++ç¼–ç å™¨ï¼Œå¤„ç†æ¯ä¸ªå…³é”®ç‚¹çš„ç‰¹å¾ `(p_i, Î´_i,k, d_i,k)`ï¼Œè¾“å‡ºæ¯ä¸ªå…³é”®ç‚¹çš„æ½œåœ¨ç‰¹å¾ `z_i`ï¼Œç”¨äºé¢„æµ‹æ¥è§¦ç‚¹é€‰æ‹©çš„logitsã€‚<br> 2. **å…¨å±€è¿åŠ¨å­¦åˆ†æ”¯**ï¼šé‡‡ç”¨åˆ†ç±»é£æ ¼çš„PointNet++ç¼–ç å™¨ï¼Œå°†æ‰€æœ‰å…³é”®ç‚¹ç‰¹å¾èšåˆä¸ºä¸€ä¸ªå…¨å±€å¯¹è±¡ä¸­å¿ƒåµŒå…¥ `g`ï¼Œç”¨äºé¢„æµ‹MPCæƒé‡ `(w_pos, w_ori)` çš„logitså’Œä»·å€¼å‡½æ•°ä¼°è®¡ã€‚ | - ä½¿ç”¨PyTorch Geometricå®ç°PointNet++ã€‚<br> - è¾“å…¥é€šé“ä¸º4ï¼ˆ3Dåæ ‡+é¢å¤–ç‰¹å¾ï¼‰ã€‚<br> - è¾“å‡ºç‰¹å¾ç»´åº¦ä¸º128ã€‚<br> - è®­ç»ƒæ—¶å¯¹æ¥è§¦ç‚¹é€‰æ‹©çš„logitsåº”ç”¨æ¸©åº¦ç¼©æ”¾ï¼ˆå› å­10ï¼‰ä»¥é”åŒ–åˆ†å¸ƒã€‚ |\n| | | **d. å¥–åŠ±å‡½æ•°**ï¼š<br> `r_k = (wâ‚*r_dense + wâ‚‚*r_target) * (1 + r_feasible) + wâ‚ƒ*r_feasible` <br> - `r_dense`: è´Ÿçš„å¯†é›†å…³é”®ç‚¹æµå¥–åŠ±ï¼Œé¼“åŠ±æ‰€æœ‰å…³é”®ç‚¹å‘ç›®æ ‡ç§»åŠ¨ã€‚`r_dense = -1/N * Î£ ||Î´_i,k||`ã€‚<br> - `r_target`: ç¨€ç–ç›®æ ‡åˆ°è¾¾å¥–åŠ±ï¼ˆæˆåŠŸæ—¶+1ï¼‰ã€‚<br> - `r_feasible`: å¯è¡Œæ€§æƒ©ç½šï¼Œå½“é€‰æ‹©çš„æ¥è§¦ç‚¹æ— æ•ˆï¼ˆå¦‚åœ¨åœ°é¢ä¸Šï¼‰æ—¶ä¸º-1ï¼Œå¦åˆ™ä¸º0ã€‚ | - æ¨ä»»åŠ¡ï¼š`wâ‚=0.1, wâ‚‚=5.0, wâ‚ƒ=0`<br> - é‡å®šå‘ä»»åŠ¡ï¼š`wâ‚=0.1, wâ‚‚=5.0, wâ‚ƒ=1`<br> - ä½¿ç”¨PPOç®—æ³•è®­ç»ƒï¼Œå­¦ä¹ ç‡é‡‡ç”¨è¡°å‡è®¡åˆ’ã€‚ |\n| **3. ä½å±‚æ§åˆ¶å™¨ï¼šæ¥è§¦éšå¼MPC** | åŸºäºé«˜å±‚ç»™å‡ºçš„æ¥è§¦æ„å›¾ï¼Œè¿›è¡Œé«˜é¢‘åœ¨çº¿é‡è§„åˆ’ï¼Œå¤„ç†æ¥è§¦åŠ¨åŠ›å­¦ã€‚ | **a. åŠ¨åŠ›å­¦æ¨¡å‹ï¼ˆäº’è¡¥æ€§è‡ªç”±æ¨¡å‹ï¼‰**ï¼š<br> é‡‡ç”¨è®ºæ–‡[25]çš„å‡†é™æ€äº’è¡¥æ€§è‡ªç”±æ¥è§¦æ¨¡å‹ï¼Œå…¶æ ¸å¿ƒæ–¹ç¨‹é¿å…äº†ä¼ ç»Ÿçš„äº’è¡¥çº¦æŸï¼Œè®¡ç®—é«˜æ•ˆï¼š<br> ```<br> Mvâº = hÏ„(u) + hJÌƒáµ€Î»<br> Î» := max( -K( hÂ²JÌƒMâ»Â¹Ï„(u) + Ï†Ìƒ ), 0 )<br> ``` <br> - `M`: ç³»ç»Ÿè´¨é‡çŸ©é˜µï¼Œ`diag(M_o, M_r)`ã€‚`M_r = hÂ² diag(K_r, ..., K_r)` å°†æœºå™¨äººå»ºæ¨¡ä¸ºåˆšåº¦çŸ©é˜µã€‚<br> - `Ï„(u) = [Ï„_o, Ï„_r(u)]`: åŠ›å‘é‡ã€‚`Ï„_r(u) = K_r u`ï¼Œå…¶ä¸­æ§åˆ¶è¾“å…¥ `u` æ˜¯æœŸæœ›çš„æœ«ç«¯æ‰§è¡Œå™¨**ä½ç§»**ï¼ˆåœ¨æ“ä½œç©ºé—´é˜»æŠ—æ§åˆ¶å‡è®¾ä¸‹ï¼‰ã€‚<br> - `Î»`: æ¥è§¦åŠ›ï¼Œé€šè¿‡ä¸€ä¸ªä½œç”¨äºå¯¹å¶æ‘©æ“¦é”¥çš„å¼¹ç°§-é˜»å°¼æ¨¡å‹äº§ç”Ÿã€‚`JÌƒ` å’Œ `Ï†Ìƒ` æ˜¯å¯¹å¶é”¥çš„é›…å¯æ¯”çŸ©é˜µå’Œç¬¦å·è·ç¦»ã€‚`max(Â·, 0)` æ“ä½œç¬¦è‡ªåŠ¨äº§ç”Ÿä¸åŒçš„æ¥è§¦æ¨¡å¼ï¼ˆç²˜æ»ã€æ»‘åŠ¨ã€åˆ†ç¦»ï¼‰ã€‚ | - æ—¶é—´æ­¥é•¿ `h = 0.01s`ã€‚<br> - æœºå™¨äººåˆšåº¦ `K_r = 0.01` (ä»¿çœŸ)ã€‚<br> - å¯¹å¶é”¥åˆšåº¦ `K = 5.0` (ä»¿çœŸ)ã€‚<br> - æ‘©æ“¦ç³»æ•° `Î¼ = 0.5` (ä»¿çœŸ)ã€‚ |\n| | | **b. MPCä¼˜åŒ–é—®é¢˜**ï¼š<br> åœ¨æ¯ä¸ªMPCæ­¥é•¿ `t`ï¼Œæ±‚è§£ä»¥ä¸‹æœ‰é™æ—¶åŸŸä¼˜åŒ–é—®é¢˜ï¼š<br> ```<br> min_{u_0:H-1} Î£_{t=0}^{H-1} Î£_i L(p_t^{ee,i}, cÌ„_kâ±) + V(q_H^{obj}, qÌ„_k^{obj})<br> s.t. x_{t+1} = ComFree(x_t, u_t), t=0,...,H-1<br> Given x_0 = x_env<br> ``` <br> - **è¿è¡Œä»£ä»· L**: é¼“åŠ±æœ«ç«¯æ‰§è¡Œå™¨æ¥è¿‘æŒ‡å®šçš„æ¥è§¦ç‚¹ã€‚`L = w_c * || p_t^{ee,i} - T_obj^w cÌ„â± ||Â²`ï¼Œå…¶ä¸­ `T_obj^w` å°†ç‰©ä½“åæ ‡å˜æ¢åˆ°ä¸–ç•Œåæ ‡ã€‚<br> - **ç»ˆç«¯ä»£ä»· V**: å³é«˜å±‚ç­–ç•¥å®šä¹‰çš„æƒé‡åŒ–ç›®æ ‡è¯¯å·®ï¼ˆå…¬å¼(9)ï¼‰ã€‚<br> - æ±‚è§£åï¼Œåªæ‰§è¡Œç¬¬ä¸€ä¸ªæ§åˆ¶è¾“å…¥ `u_0`ã€‚ | - MPCé¢„æµ‹æ­¥é•¿ `H = ",
            "timestamp": "2026-01-20T18:00:45.233125",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨æ·±å…¥åˆ†æè¿™ç¯‡è®ºæ–‡ã€ŠWhere to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulationã€‹æ‰€ä½¿ç”¨çš„è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ã€‚\n\n## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\nè¯¥è®ºæ–‡é€šè¿‡**ä»¿çœŸå®éªŒ**å’Œ**çœŸå®ä¸–ç•Œå®éªŒ**ä¸¤ä¸ªç»´åº¦ï¼Œä½¿ç”¨äº†ä¸€å¥—ç»¼åˆçš„è¯„ä»·æŒ‡æ ‡æ¥éªŒè¯å…¶æå‡ºçš„åˆ†å±‚RL-MPCæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚å…¶æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡å›´ç»•**æˆåŠŸç‡ã€æ•°æ®æ•ˆç‡ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›**å±•å¼€ã€‚\n\n### ä¸€ã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡\n\n| æŒ‡æ ‡ç±»åˆ« | å…·ä½“æŒ‡æ ‡ | å®šä¹‰ä¸ç»†èŠ‚ | è¯„ä¼°ç›®çš„ |\n| :--- | :--- | :--- | :--- |\n| **æ ¸å¿ƒæ€§èƒ½** | **æˆåŠŸç‡** | åœ¨å›ºå®šæ­¥æ•°ï¼ˆä»¿çœŸ64ä¸ªé«˜å±‚å†³ç­–æ­¥ï¼‰å†…ï¼Œç‰©ä½“å§¿æ€è¾¾åˆ°é¢„è®¾é˜ˆå€¼ï¼ˆä½ç½®è¯¯å·®+æ—‹è½¬è¯¯å·®ï¼‰çš„è¯•éªŒæ¯”ä¾‹ã€‚ | è¡¡é‡æ–¹æ³•å®Œæˆä»»åŠ¡çš„**åŸºæœ¬èƒ½åŠ›**å’Œ**å¯é æ€§**ã€‚ |\n| **æ•°æ®æ•ˆç‡** | **å­¦ä¹ æ›²çº¿** | æ¨ªè½´ä¸º**é«˜å±‚RLå†³ç­–æ­¥æ•°**æˆ–**åº•å±‚æ§åˆ¶æ­¥æ•°**ï¼Œçºµè½´ä¸ºæˆåŠŸç‡ã€‚ç”¨äºå¯¹æ¯”ä¸åŒæ–¹æ³•çš„æ”¶æ•›é€Ÿåº¦å’Œæ‰€éœ€æ•°æ®é‡ã€‚ | è¯æ˜åˆ†å±‚æ¡†æ¶ç›¸æ¯”ç«¯åˆ°ç«¯æ–¹æ³•èƒ½**å¤§å¹…å‡å°‘æ‰€éœ€äº¤äº’æ•°æ®**ï¼ˆè®ºæ–‡ç§°å‡å°‘10å€ï¼‰ã€‚ |\n| **æ³›åŒ–èƒ½åŠ›** | **è·¨å‡ ä½•æ³›åŒ–** | åœ¨â€œå‡ ä½•æ³›åŒ–æ¨åŠ¨â€ä»»åŠ¡ä¸­ï¼Œåˆ†åˆ«æŠ¥å‘Šåœ¨**è®­ç»ƒé›†å­—æ¯**ï¼ˆE, H, L, N, T, Xï¼‰å’Œ**æµ‹è¯•é›†å­—æ¯**ï¼ˆF, I, K, V, Y, Zï¼‰ä¸Šçš„æˆåŠŸç‡ã€‚ | éªŒè¯æ–¹æ³•å¯¹**æœªè§è¿‡çš„ç‰©ä½“å½¢çŠ¶**çš„é€‚åº”èƒ½åŠ›ï¼Œæ˜¯â€œå‡ ä½•æ„ŸçŸ¥â€çš„å…³é”®ä½“ç°ã€‚ |\n| **é²æ£’æ€§** | **ç¯å¢ƒæ‰°åŠ¨é²æ£’æ€§** | åœ¨ä»¿çœŸä¸­å¼•å…¥ä¸‰ç§æ‰°åŠ¨ï¼š<br>1. **å¤–åŠ›æ‰°åŠ¨**ï¼šå¯¹ç‰©ä½“æ–½åŠ éšæœºåŠ›/åŠ›çŸ©ã€‚<br>2. **æ‘©æ“¦æ‰°åŠ¨**ï¼šéšæœºåŒ–ç‰©ä½“æ‘©æ“¦ç³»æ•°ã€‚<br>3. **æ‰§è¡Œå™¨æ‰°åŠ¨**ï¼šéšæœºåŒ–æœºå™¨äººå…³èŠ‚åˆšåº¦/é˜»å°¼å‚æ•°ã€‚æ¯”è¾ƒæ‰°åŠ¨ä¸‹ä¸æ— æ‰°åŠ¨ä¸‹çš„æˆåŠŸç‡ã€‚ | éªŒè¯æ–¹æ³•å¯¹**æ¨¡å‹è¯¯å·®å’Œå¤–éƒ¨å¹²æ‰°**çš„å®¹å¿åº¦ï¼Œä½“ç°MPCåº•å±‚é‡è§„åˆ’çš„ä¼˜åŠ¿ã€‚ |\n| **æ‰§è¡Œæ•ˆç‡** | **å†³ç­–æ­¥æ•°/æ‰§è¡Œæ—¶é—´** | æˆåŠŸå®Œæˆä¸€æ¬¡è¯•éªŒæ‰€éœ€çš„**é«˜å±‚RLå†³ç­–æ­¥æ•°**ï¼ˆä»¿çœŸï¼‰å’Œ**å®é™…æ‰§è¡Œæ—¶é—´**ï¼ˆç§’ï¼ŒçœŸå®å®éªŒï¼‰ã€‚ | è¡¡é‡æ–¹æ³•çš„**è§„åˆ’æ•ˆç‡**å’Œåœ¨å®é™…ç³»ç»Ÿä¸­çš„**è¿è¡Œé€Ÿåº¦**ã€‚ |\n| **é›¶æ ·æœ¬è¿ç§»** | **ä»¿çœŸåˆ°ç°å®æˆåŠŸç‡** | å°†åœ¨ä»¿çœŸä¸­è®­ç»ƒçš„é«˜å±‚ç­–ç•¥**ç›´æ¥éƒ¨ç½²**åˆ°çœŸå®æœºå™¨äººä¸Šï¼Œä¸è¿›è¡Œä»»ä½•å¾®è°ƒï¼ˆé›¶æ ·æœ¬ï¼‰ï¼ŒæŠ¥å‘ŠçœŸå®å®éªŒçš„æˆåŠŸç‡ã€‚ | éªŒè¯â€œæ¥è§¦æ„å›¾â€æ¥å£å’Œåˆ†å±‚è®¾è®¡èƒ½æœ‰æ•ˆ**è·¨è¶Šä»¿çœŸä¸ç°å®é—´çš„å·®è·**ã€‚ |\n\n### äºŒã€ ä»»åŠ¡ä¸æ•°æ®é›†ç»†èŠ‚\n\nè®ºæ–‡åœ¨ä¸¤ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„éæŠ“å–æ“ä½œä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæ„æˆäº†å…¶â€œæ•°æ®é›†â€ã€‚\n\n| ä»»åŠ¡åç§° | ç›®æ ‡ | ç‰©ä½“ä¸å…³é”®ç‚¹ | åˆå§‹/ç›®æ ‡å§¿æ€åˆ†å¸ƒ | æˆåŠŸé˜ˆå€¼ | å¤‡æ³¨ |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **1. å‡ ä½•æ³›åŒ–æ¨åŠ¨** | å°†å¤šæ ·åŒ–çš„å­—æ¯å½¢çŠ¶ç‰©ä½“ä»ä»»æ„åˆå§‹ä½å§¿æ¨è‡³å¹³é¢ç›®æ ‡ä½å§¿ï¼ˆä½ç½®+ç»•Zè½´æ—‹è½¬ï¼‰ã€‚ | **è®­ç»ƒé›†**ï¼š6ä¸ªå­—æ¯ (E, H, L, N, T, X)ã€‚<br>**æµ‹è¯•é›†**ï¼š6ä¸ªæœªè§å­—æ¯ (F, I, K, V, Y, Z)ã€‚<br>**å…³é”®ç‚¹**ï¼šåœ¨æ¯ä¸ªå­—æ¯çš„**å‚ç›´ä¾§é¢**å‡åŒ€é‡‡æ ·256ä¸ªç‚¹ã€‚ | **åˆå§‹ä½ç½®**ï¼šXYå¹³é¢å†…éšæœºçŸ©å½¢åŒºåŸŸã€‚<br>**ç›®æ ‡ä½ç½®**ï¼šä¸åˆå§‹ä½ç½®ç›¸åŒï¼ˆè¦æ±‚åŸåœ°æ—‹è½¬ï¼‰æˆ–æŒ‡å®šåŒºåŸŸï¼ˆçœŸå®å®éªŒï¼‰ã€‚<br>**ç›®æ ‡æœå‘**ï¼šç»•Zè½´éšæœºè§’åº¦ï¼ˆ-180Â° åˆ° 180Â°ï¼‰ã€‚ | **ä½ç½®è¯¯å·®**ï¼š0.03m - 0.05mï¼ˆä¸åŒå®éªŒè®¾ç½®ä¸åŒï¼‰ã€‚<br>**æ—‹è½¬è¯¯å·®**ï¼š0.02 - 0.05ï¼ˆåŸºäºå››å…ƒæ•°ç‚¹ç§¯çš„åº¦é‡ï¼‰ã€‚ | æ ¸å¿ƒæŒ‘æˆ˜åœ¨äº**é•¿æ—¶ç¨‹æ¥è§¦æ¨ç†**ï¼ˆéœ€é¡ºåºåˆ‡æ¢æ¥è§¦é¢ï¼‰å’Œå¯¹**å¤æ‚å‡ ä½•å½¢çŠ¶**çš„é€‚åº”ã€‚ |\n| **2. ç‰©ä½“3Dé‡å®šå‘** | å°†ä¸€ä¸ªç«‹æ–¹ä½“ä»éšæœºåˆå§‹6Dä½å§¿é‡æ–°å®šå‘åˆ°éšæœº3Dç›®æ ‡ä½å§¿ã€‚ | **ç‰©ä½“**ï¼šç«‹æ–¹ä½“ã€‚<br>**å…³é”®ç‚¹**ï¼šåœ¨ç«‹æ–¹ä½“**å…­ä¸ªé¢**ä¸Šå‡åŒ€é‡‡æ ·256ä¸ªç‚¹ã€‚ | **åˆå§‹ä½å§¿**ï¼šä½ç½®åœ¨XYå¹³é¢å°èŒƒå›´å†…éšæœºï¼Œæœå‘å®Œå…¨éšæœºã€‚<br>**ç›®æ ‡ä½å§¿**ï¼šä½ç½®ä¸åˆå§‹ç›¸åŒæˆ–å›ºå®šï¼Œæœå‘ä¸ºç»•Xè½´å›ºå®š180Â°ç¿»è½¬åå†ç»•Zè½´éšæœºæ—‹è½¬ã€‚ | **ä½ç½®è¯¯å·®**ï¼š0.05m - 0.075mã€‚<br>**æ—‹è½¬è¯¯å·®**ï¼š0.03 - 0.05ã€‚ | è¯„ä¼°æ–¹æ³•ç”Ÿæˆ**ä¸°å¯Œ3Dæ¥è§¦ç­–ç•¥**ï¼ˆæ»‘åŠ¨ã€è¾¹ç¼˜æ¢è½´ã€è§’ç‚¹æ¢è½´ï¼‰çš„èƒ½åŠ›ï¼Œæ¯”å¹³é¢æ¨åŠ¨æ›´å¤æ‚ã€‚ |\n\n### ä¸‰ã€ å®éªŒè®¾ç½®æ€»ç»“\n\n1.  **è®­ç»ƒç¯å¢ƒ**ï¼šé«˜å±‚RLç­–ç•¥åœ¨ä¸€ä¸ª**ç®€åŒ–çš„â€œæŠ½è±¡æœ«ç«¯æ‰§è¡Œå™¨â€ä»¿çœŸç¯å¢ƒ**ä¸­è®­ç»ƒï¼Œæœ«ç«¯è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªè´¨-å¼¹-é˜»ç‚¹ï¼Œä»¥åŠ é€Ÿå­¦ä¹ ã€‚\n2.  **è¯„ä¼°ç¯å¢ƒ**ï¼š\n    *   **ä»¿çœŸå…¨ç³»ç»Ÿ**ï¼šå°†è®­ç»ƒå¥½çš„ç­–ç•¥ä¸åº•å±‚MPCç»“åˆï¼Œåœ¨åŒ…å«å®Œæ•´Frankaæœºå™¨äººæ¨¡å‹çš„ä»¿çœŸä¸­è¯„ä¼°ã€‚\n    *   **çœŸå®ä¸–ç•Œç³»ç»Ÿ**ï¼šé›¶æ ·æœ¬è¿ç§»åˆ°çœŸå®Frankaæœºå™¨äººï¼Œä½¿ç”¨è§†è§‰ç³»ç»Ÿï¼ˆTwinTrack, FoundationPose++ï¼‰è¿›è¡Œç‰©ä½“ä½å§¿ä¼°è®¡ã€‚\n3.  **å¯¹æ¯”åŸºçº¿**ï¼š\n    *   **ç«¯åˆ°ç«¯RL**ï¼šç›´æ¥ä»è§‚æµ‹æ˜ å°„åˆ°æœ«ç«¯æ‰§è¡Œå™¨åŠ¨ä½œçš„å•ä¸€ç­–ç•¥ã€‚\n    *   **ç±»HACManåˆ†å±‚RL**ï¼šé«˜å±‚é¢„æµ‹æ¥è§¦ç‚¹å’Œæœ«ç«¯ä½ç§»ï¼Œä½†**æ²¡æœ‰åŸºäºç‰©ç†æ¨¡å‹çš„MPCåº•å±‚**ï¼Œè€Œæ˜¯ç”±å¦ä¸€ä¸ªå­¦ä¹ æ¨¡å—æ‰§è¡Œã€‚\n4.  **æ¶ˆèå®éªŒ**ï¼šé€šè¿‡ç§»é™¤å­ç›®æ ‡é¢„æµ‹ã€æ”¹å˜è§‚æµ‹åæ ‡ç³»ã€ç§»é™¤ç¢°æ’ä¿¡æ¯ç»„ä»¶ï¼ŒéªŒè¯äº†æ‰€æ**æ¥è§¦æ„å›¾æ¥å£ã€ç‰©ä½“ä¸­å¿ƒåŒ–è¡¨å¾ã€ä¸‰ç»„ä»¶è§‚æµ‹ç©ºé—´**ç­‰è®¾è®¡çš„å¿…è¦æ€§ã€‚\n\n### ç»“è®º\n\nè¯¥è®ºæ–‡çš„è¯„ä»·ä½“ç³»**å…¨é¢ä¸”å…·æœ‰è¯´æœåŠ›**ï¼Œä¸ä»…å…³æ³¨æœ€ç»ˆæ€§èƒ½ï¼ˆé«˜æˆåŠŸç‡ï¼‰ï¼Œæ›´æ·±å…¥éªŒè¯äº†å…¶æ–¹æ³•çš„æ ¸å¿ƒä¸»å¼ ï¼š**æ•°æ®é«˜æ•ˆæ€§**ï¼ˆé€šè¿‡å¯¹æ¯”å­¦ä¹ æ›²çº¿ï¼‰ã€**å‡ ä½•æ³›åŒ–èƒ½åŠ›**ï¼ˆé€šè¿‡åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•ç‰©ä½“é›†ï¼‰ã€**é²æ£’æ€§ä¸é›¶æ ·æœ¬ä»¿çœŸåˆ°ç°å®è¿ç§»èƒ½åŠ›**ï¼ˆé€šè¿‡æ‰°åŠ¨æµ‹è¯•å’ŒçœŸå®å®éªŒï¼‰ã€‚è¿™äº›æŒ‡æ ‡å…±åŒæ”¯æ’‘äº†è®ºæ–‡å…³äºåˆ†å±‚RL-MPCæ¡†æ¶åœ¨æ¥è§¦çµå·§æ“ä½œä¸­ä¼˜åŠ¿çš„ç»“è®ºã€‚",
            "timestamp": "2026-01-20T18:01:33.081089",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†ä¸ä»»åŠ¡è®¾ç½®\nè®ºæ–‡åœ¨ä¸¤ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„**éæŠ“å–å¼æ“ä½œä»»åŠ¡**ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå‡ä½¿ç”¨é…å¤‡æ£’çŠ¶æœ«ç«¯æ‰§è¡Œå™¨çš„7è‡ªç”±åº¦Frankaæœºæ¢°è‡‚ã€‚\n\n| ä»»åŠ¡åç§° | æè¿° | è®­ç»ƒé›† | æµ‹è¯•é›†ï¼ˆæ³›åŒ–è¯„ä¼°ï¼‰ |\n| :--- | :--- | :--- | :--- |\n| **å‡ ä½•æ³›åŒ–æ¨åŠ¨** | å°†ä¸åŒå½¢çŠ¶çš„å­—æ¯ç‰©ä½“ä»ä»»æ„åˆå§‹ä½å§¿æ¨è‡³å¹³é¢ç›®æ ‡ä½å§¿ã€‚ | 6ä¸ªå­—æ¯ï¼š**E, H, L, N, T, X** | 6ä¸ªæœªè§å­—æ¯ï¼š**F, I, K, V, Y, Z** |\n| **ç‰©ä½“3Dé‡å®šå‘** | å°†ç«‹æ–¹ä½“ä»éšæœºåˆå§‹ä½å§¿é‡æ–°å®šå‘è‡³éšæœº3Dç›®æ ‡ä½å§¿ã€‚ | ç«‹æ–¹ä½“ï¼ˆå•ä¸€å‡ ä½•ä½“ï¼‰ | åŒè®­ç»ƒé›†ï¼ˆè¯„ä¼°ä»»åŠ¡å®Œæˆèƒ½åŠ›ï¼‰ |\n\n**å…³é”®ç‚¹é‡‡æ ·**ï¼šæ¯ä¸ªç‰©ä½“è¡¨é¢å‡åŒ€é‡‡æ ·256ä¸ªå…³é”®ç‚¹ï¼Œä½œä¸ºå‡ ä½•è¡¨ç¤ºå’Œæ¥è§¦ç‚¹å€™é€‰é›†ã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\n1.  **æˆåŠŸç‡**ï¼šåœ¨å›ºå®šæ­¥æ•°ï¼ˆ64ä¸ªé«˜å±‚å†³ç­–æ­¥ï¼‰å†…ï¼Œç‰©ä½“ä½å§¿è¾¾åˆ°ä¸ç›®æ ‡ä½å§¿çš„è¯¯å·®é˜ˆå€¼ä»¥å†…ã€‚\n    *   **å¹³ç§»é˜ˆå€¼**ï¼šæ¨¡æ‹Ÿä¸­0.03-0.05ç±³ï¼Œå®ç‰©ä¸­0.03-0.075ç±³ã€‚\n    *   **æ—‹è½¬é˜ˆå€¼**ï¼šåŸºäºå››å…ƒæ•°å†…ç§¯çš„è¯¯å·®åº¦é‡ï¼Œé˜ˆå€¼åœ¨0.02-0.05ä¹‹é—´ã€‚\n2.  **æ•°æ®æ•ˆç‡**ï¼šè¾¾åˆ°é«˜æˆåŠŸç‡æ‰€éœ€çš„**å¼ºåŒ–å­¦ä¹ å†³ç­–æ­¥æ•°**å’Œ**åº•å±‚æ§åˆ¶æ­¥æ•°**ã€‚\n3.  **å†³ç­–æ­¥æ•°**ï¼šå®Œæˆä»»åŠ¡å¹³å‡æ‰€éœ€çš„é«˜å±‚RLç­–ç•¥å†³ç­–æ­¥æ•°ã€‚\n4.  **æ‰§è¡Œæ—¶é—´**ï¼šå®ç‰©å®éªŒä¸­å®Œæˆä»»åŠ¡çš„å¢™ä¸Šæ—¶é’Ÿæ—¶é—´ã€‚\n5.  **é²æ£’æ€§**ï¼šåœ¨ç¯å¢ƒæ‰°åŠ¨ï¼ˆå¤–åŠ›ã€æ‘©æ“¦ã€æ‰§è¡Œå™¨å‚æ•°å˜åŒ–ï¼‰ä¸‹çš„æˆåŠŸç‡ä¿æŒèƒ½åŠ›ã€‚\n6.  **é›¶æ ·æœ¬æ¨¡æ‹Ÿåˆ°å®ç‰©è¿ç§»**ï¼šåœ¨æ¨¡æ‹Ÿä¸­è®­ç»ƒçš„ç­–ç•¥ï¼Œ**ä¸ç»è¿‡ä»»ä½•é¢å¤–è°ƒæ•´æˆ–è®­ç»ƒ**ï¼Œç›´æ¥éƒ¨ç½²åˆ°çœŸå®æœºå™¨äººä¸Šçš„æ€§èƒ½ã€‚\n\n### ä¸‰ã€ æ ¸å¿ƒæ€§èƒ½æå‡ä¸ç»“æœ\næ‰€æå‡ºçš„åˆ†å±‚RL-MPCæ¡†æ¶åœ¨å¤šä¸ªç»´åº¦ä¸Šå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼š\n\n| å¯¹æ¯”ç»´åº¦ | æœ¬æ–‡æ–¹æ³• (Hierarchical RL-MPC) | å¯¹æ¯”åŸºçº¿ | æ€§èƒ½æå‡/ç»“æœ |\n| :--- | :--- | :--- | :--- |\n| **æ•°æ®æ•ˆç‡** | **~40k / ~100k** RLæ­¥è¾¾åˆ°è¿‘100%æˆåŠŸç‡ï¼ˆæ¨åŠ¨/é‡å®šå‘ï¼‰ã€‚ | **ç«¯åˆ°ç«¯RL**ï¼šéœ€~600k RLæ­¥è¾¾åˆ°92.5%æˆåŠŸç‡ã€‚ | **RLæ­¥æ•°æ•ˆç‡æå‡çº¦15å€**ï¼Œæ§åˆ¶æ­¥æ•°æ•ˆç‡æå‡çº¦2å€ã€‚ |\n| **æœ€ç»ˆæˆåŠŸç‡ (æ¨¡æ‹Ÿ)** | **æ¥è¿‘100%** (æ¨åŠ¨ï¼š100% seen, 99.06% unseenï¼› é‡å®šå‘ï¼š98.75%)ã€‚ | 1. ç«¯åˆ°ç«¯RL: **92.5%** <br> 2. HACMané£æ ¼åˆ†å±‚RL: **~77.19%** (é«˜æ–¹å·®)ã€‚ | æˆåŠŸç‡æ˜¾è‘—æ›´é«˜ä¸”æ›´ç¨³å®šã€‚ |\n| **æ³›åŒ–èƒ½åŠ›** | åœ¨**æœªè§è¿‡çš„å­—æ¯å½¢çŠ¶**ä¸Šæ¨åŠ¨æˆåŠŸç‡é«˜è¾¾**97.34%**ã€‚ | æœªæ˜ç¡®ä¸å…¶ä»–æ–¹æ³•åœ¨ç›¸åŒæ³›åŒ–é›†ä¸Šæ¯”è¾ƒã€‚ | è¯æ˜äº†å…¶å¯¹æ–°é¢–å‡ ä½•å½¢çŠ¶çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚ |\n| **é²æ£’æ€§** | åœ¨å¤–éƒ¨åŠ›ã€æ‘©æ“¦ã€æ‰§è¡Œå™¨å‚æ•°æ‰°åŠ¨ä¸‹ï¼ŒæˆåŠŸç‡ä¿æŒ**100%**ã€‚ | ç«¯åˆ°ç«¯RLåœ¨æ‰§è¡Œå™¨å‚æ•°æ‰°åŠ¨ä¸‹æˆåŠŸç‡**éª¤é™è‡³44.0%**ã€‚ | å¯¹åŠ¨åŠ›å­¦æ¨¡å‹è¯¯å·®å’Œæ‰°åŠ¨è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚ |\n| **æ¨¡æ‹Ÿåˆ°å®ç‰©è¿ç§»** | **é›¶æ ·æœ¬è¿ç§»æˆåŠŸ**ã€‚å¤§å¤šæ•°å­—æ¯æ¨åŠ¨ä»»åŠ¡æˆåŠŸç‡**100%**ï¼Œç«‹æ–¹ä½“é‡å®šå‘æˆåŠŸç‡**100%**ã€‚ | ç«¯åˆ°ç«¯æ–¹æ³•é€šå¸¸å› å»ºæ¨¡è¯¯å·®å­˜åœ¨æ˜¾è‘—çš„sim-to-realå·®è·ã€‚ | å¾—ç›Šäºâ€œæ¥è§¦æ„å›¾â€æ¥å£è§£è€¦äº†å‡ ä½•æ¨ç†ä¸æ¥è§¦åŠ¨åŠ›å­¦ï¼Œå®ç°äº†æ— ç¼è¿ç§»ã€‚ |\n| **å†³ç­–æ•ˆç‡** | å¹³å‡ä»…éœ€**~10æ­¥**é«˜å±‚å†³ç­–å³å¯å®Œæˆæ¨åŠ¨ä»»åŠ¡ã€‚ | æœªæä¾›åŸºçº¿å¯¹æ¯”æ•°æ®ã€‚ | è¡¨æ˜é«˜å±‚ç­–ç•¥èƒ½æœ‰æ•ˆè§„åˆ’é•¿è§†é‡ä»»åŠ¡ã€‚ |\n\n### å››ã€ å…³é”®ç»“è®º\nè®ºæ–‡é€šè¿‡ç³»ç»Ÿçš„å®éªŒè¡¨æ˜ï¼Œå…¶æå‡ºçš„**åˆ†å±‚RL-MPCæ¡†æ¶**å’Œ**æ¥è§¦æ„å›¾**æ¥å£ï¼Œåœ¨**æ•°æ®æ•ˆç‡ã€æœ€ç»ˆæ€§èƒ½ã€é²æ£’æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œæ¨¡æ‹Ÿåˆ°å®ç‰©è¿ç§»**æ–¹é¢ï¼Œå…¨é¢ä¼˜äºä¼ ç»Ÿçš„ç«¯åˆ°ç«¯RLæ–¹æ³•å’Œä¸åŒ…å«ç‰©ç†MPCçš„å±‚æ¬¡åŒ–RLæ–¹æ³•ã€‚è¿™éªŒè¯äº†å°†â€œæ¥è§¦ä½•å¤„â€ï¼ˆå‡ ä½•-è¿åŠ¨å­¦ï¼‰ä¸â€œå¦‚ä½•æ¥è§¦â€ï¼ˆæ¥è§¦åŠ¨åŠ›å­¦ï¼‰è¿›è¡Œå±‚æ¬¡åŒ–åˆ†è§£çš„æœ‰æ•ˆæ€§ã€‚",
            "timestamp": "2026-01-20T18:02:06.916528",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡ç»“è®ºéƒ¨åˆ†ï¼ˆSection IXï¼‰åŠå…¨æ–‡å†…å®¹ï¼Œæœ¬æ–‡æå‡ºçš„åˆ†å±‚RL-MPCæ¡†æ¶å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n### 1. **å¯¹ç²¾ç¡®ç‰©ä½“ä½å§¿ä¼°è®¡çš„ä¾èµ–**\n- **é—®é¢˜æè¿°**ï¼šæ¡†æ¶çš„æ ¸å¿ƒç»„ä»¶ï¼ˆå¦‚è®¡ç®—ç›®æ ‡æµ `Î”ğ’«_k` å’ŒMPCä¼˜åŒ–é¡¹ï¼‰ä¸¥é‡ä¾èµ–äºå¯¹ç‰©ä½“å½“å‰ä½å§¿ `ğ’’^obj` å’Œæœ€ç»ˆç›®æ ‡ä½å§¿ `ğ’’^target` çš„ç²¾ç¡®ä¼°è®¡ã€‚\n- **å½±å“**ï¼šåœ¨çœŸå®ä¸–ç•Œä¸­ï¼Œæ„ŸçŸ¥å™ªå£°ã€é®æŒ¡æˆ–åŠ¨æ€ç‰©ä½“è·Ÿè¸ªå¤±è´¥ï¼ˆå¦‚å®éªŒä¸­å­—æ¯â€œIâ€çš„ä¸ªåˆ«å¤±è´¥æ¡ˆä¾‹ï¼‰ä¼šç›´æ¥å¯¼è‡´é«˜å±‚ç­–ç•¥å†³ç­–é”™è¯¯å’Œåº•å±‚MPCæ§åˆ¶å¤±æ•ˆï¼Œå½±å“ç³»ç»Ÿçš„é²æ£’æ€§ã€‚\n\n### 2. **ç¦»æ•£å…³é”®ç‚¹è¡¨ç¤ºçš„å¯æ‰©å±•æ€§é—®é¢˜**\n- **é—®é¢˜æè¿°**ï¼šé«˜å±‚RLç­–ç•¥çš„åŠ¨ä½œç©ºé—´å»ºç«‹åœ¨ä»ç‰©ä½“ç‚¹äº‘ä¸‹é‡‡æ ·çš„ç¦»æ•£å…³é”®ç‚¹é›† `ğ’«` ä¸Šã€‚æ¥è§¦ä½ç½® `ğ’„Ì„` åªèƒ½ä»è¿™äº›é¢„å®šä¹‰çš„å…³é”®ç‚¹ä¸­é€‰æ‹©ã€‚\n- **å½±å“**ï¼š\n    - **ç²¾åº¦é™åˆ¶**ï¼šæ¥è§¦ä½ç½®çš„å†³ç­–ç²¾åº¦å—é™äºå…³é”®ç‚¹çš„é‡‡æ ·å¯†åº¦å’Œåˆ†å¸ƒã€‚\n    - **ç»„åˆçˆ†ç‚¸**ï¼šå½“åº”ç”¨äºå¤šæŒ‡çµå·§æ‰‹ï¼ˆ`n > 1`ï¼‰æ—¶ï¼Œéœ€è¦ä¸ºå¤šä¸ªæœ«ç«¯æ‰§è¡Œå™¨åŒæ—¶é€‰æ‹©æ¥è§¦ç‚¹ã€‚åŠ¨ä½œç©ºé—´å¤§å°å°†éšå…³é”®ç‚¹æ•°é‡ `N` å’Œæ‰‹æŒ‡æ•°é‡ `n` å‘ˆç»„åˆå¢é•¿ï¼ˆ`C(N, n)`ï¼‰ï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨ç†å˜å¾—æå…¶å›°éš¾ï¼Œé™åˆ¶äº†æ¡†æ¶å‘æ›´å¤æ‚å¤šæ¥è§¦ä»»åŠ¡ï¼ˆå¦‚æŠ“å–ã€é‡æ’ï¼‰çš„æ‰©å±•ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡åœ¨ç»“è®ºä¸­æå‡ºäº†æ˜ç¡®çš„æœªæ¥å·¥ä½œæ–¹å‘ï¼Œå¹¶éšå«äº†å…¶ä»–æ½œåœ¨æ”¹è¿›ç‚¹ï¼š\n\n### 1. **å¼€å‘æ„ŸçŸ¥-è§„åˆ’ä¸€ä½“åŒ–æ–¹æ³•**\n- **æ–¹å‘**ï¼šæ¢ç´¢â€œæ„ŸçŸ¥åŸç”Ÿè§„åˆ’â€ï¼Œæ—¨åœ¨å‡è½»å¯¹ç‹¬ç«‹ã€é«˜ç²¾åº¦ä½å§¿ä¼°è®¡æ¨¡å—çš„ä¾èµ–ã€‚\n- **æ½œåœ¨æ–¹æ³•**ï¼š\n    - å°†åŸå§‹è§†è§‰è§‚æµ‹ï¼ˆå¦‚RGB-Då›¾åƒï¼‰æ›´ç›´æ¥åœ°èå…¥åˆ†å±‚å†³ç­–è¿‡ç¨‹ã€‚\n    - åˆ©ç”¨éšå¼è¡¨ç¤ºæˆ–ç¥ç»è¾å°„åœºç­‰æ–°å…´æŠ€æœ¯ï¼Œä½¿ç­–ç•¥èƒ½åŸºäºæ›´é²æ£’çš„å‡ ä½•ç‰¹å¾è¿›è¡Œæ¨ç†ï¼Œè€Œéä¾èµ–è„†å¼±çš„æ˜¾å¼ä½å§¿ä¼°è®¡ã€‚\n\n### 2. **æ‰©å±•è‡³å¤šæŒ‡çµå·§æ“ä½œ**\n- **æ–¹å‘**ï¼šå°†æ¥è§¦æ„å›¾æ¥å£å’Œåˆ†å±‚æ¡†æ¶æ¨å¹¿åˆ°å¤šæŒ‡æœºå™¨äººæ‰‹ï¼Œå®ç°çœŸæ­£çš„â€œçµå·§â€æ“ä½œã€‚\n- **æŒ‘æˆ˜ä¸æ€è·¯**ï¼š\n    - **åŠ¨ä½œç©ºé—´è®¾è®¡**ï¼šéœ€è¦é‡æ–°è®¾è®¡é«˜å±‚åŠ¨ä½œç©ºé—´ï¼Œä»¥é«˜æ•ˆå¤„ç†å¤šæ¥è§¦ç‚¹çš„ç»„åˆé€‰æ‹©é—®é¢˜ã€‚å¯èƒ½é‡‡ç”¨åºåˆ—å†³ç­–ã€ç»“æ„åŒ–é¢„æµ‹æˆ–å¼•å…¥è¿ç»­å‚æ•°åŒ–ï¼ˆå¦‚é¢„æµ‹æ¥è§¦åŒºåŸŸè€Œéç¦»æ•£ç‚¹ï¼‰æ¥ç¼“è§£ç»„åˆçˆ†ç‚¸ã€‚\n    - **åº•å±‚MPC**ï¼šéœ€æ‰©å±•ComFree-MPCæ¨¡å‹ï¼Œä»¥åŒæ—¶ä¼˜åŒ–å¤šä¸ªæ¥è§¦ç‚¹çš„åŠ›å’Œè¿åŠ¨ï¼Œåè°ƒæ‰‹æŒ‡é—´çš„äº¤äº’ã€‚\n\n### 3. **å…¶ä»–æ½œåœ¨æ”¹è¿›æ–¹å‘ï¼ˆåŸºäºè®ºæ–‡å†…å®¹æ¨æ–­ï¼‰**\n- **è¿ç»­æ¥è§¦è¡¨ç¤º**ï¼šç”¨è¿ç»­å‚æ•°ï¼ˆå¦‚ç‰©ä½“è¡¨é¢çš„UVåæ ‡ã€æ³•å‘é‡ï¼‰æ›¿ä»£ç¦»æ•£å…³é”®ç‚¹é›†ï¼Œæå‡æ¥è§¦é€‰æ‹©çš„ç²¾åº¦å’Œçµæ´»æ€§ã€‚\n- **åŠ¨æ€å…³é”®ç‚¹/å…´è¶£åŒºåŸŸé¢„æµ‹**ï¼šè®©é«˜å±‚ç­–ç•¥èƒ½å¤ŸåŠ¨æ€é¢„æµ‹æˆ–å…³æ³¨ä¸å½“å‰ä»»åŠ¡æœ€ç›¸å…³çš„ç‰©ä½“åŒºåŸŸï¼Œè€Œéå§‹ç»ˆå¤„ç†å…¨éƒ¨é¢„å®šä¹‰å…³é”®ç‚¹ï¼Œæå‡è®¡ç®—æ•ˆç‡å’Œå†³ç­–è´¨é‡ã€‚\n- **æ¨¡å‹ä¸å­¦ä¹ çš„è¿›ä¸€æ­¥èåˆ**ï¼šæ¢ç´¢åœ¨MPCå±‚å¼•å…¥å­¦ä¹ æˆåˆ†ï¼ˆå¦‚å­¦ä¹ æ®‹å·®åŠ¨åŠ›å­¦æ¨¡å‹ï¼‰ï¼Œä»¥æ›´å¥½åœ°å¤„ç†æœªå»ºæ¨¡çš„æ¥è§¦ç‰©ç†ï¼Œè¿›ä¸€æ­¥æå‡sim-to-realçš„è¿ç§»èƒ½åŠ›å’Œåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é²æ£’æ€§ã€‚\n\n**æ€»ç»“**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºé€šè¿‡**æ¥è§¦æ„å›¾**è¿™ä¸€æ¥å£æ¸…æ™°åˆ†ç¦»äº†å‡ ä½•è¿åŠ¨æ¨ç†ä¸æ¥è§¦åŠ¨åŠ›å­¦æ¨ç†ï¼Œä»è€Œè·å¾—äº†æ•°æ®é«˜æ•ˆã€é²æ£’ä¸”å¯è¿ç§»çš„æ€§èƒ½ã€‚å…¶å±€é™æ€§ä¸»è¦é›†ä¸­äº**æ„ŸçŸ¥ä¾èµ–**å’Œ**è¡¨ç¤ºå¯æ‰©å±•æ€§**ä¸¤æ–¹é¢ï¼Œæœªæ¥çš„æ”¹è¿›ä¹Ÿå°†å›´ç»•è§£å†³è¿™ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜å±•å¼€ï¼Œä»¥æ¨åŠ¨æ¡†æ¶èµ°å‘æ›´é€šç”¨ã€æ›´å¤æ‚çš„çµå·§æ“ä½œåœºæ™¯ã€‚",
            "timestamp": "2026-01-20T18:02:36.143912",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-16T01:20:15Z",
    "created_at": "2026-01-20T17:49:58.427450",
    "updated_at": "2026-01-20T17:49:58.427459"
}