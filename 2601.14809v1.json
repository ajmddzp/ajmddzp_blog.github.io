{
    "id": "2601.14809v1",
    "title": "Stochastic Decision-Making Framework for Human-Robot Collaboration in Industrial Applications",
    "authors": [
        "Muhammad Adel Yusuf",
        "Ali Nasir",
        "Zeeshan Hameed Khan"
    ],
    "abstract": "协作机器人正日益融入各类工业与服务场景，与人类高效安全地协同工作。然而，要实现有效的人机协作，机器人必须基于人类动机水平、情绪状态等因素进行推理决策。本文提出一种基于随机建模的人机协作环境决策方法，通过概率模型与控制策略的结合，旨在预测人类行为与情绪变化，使协作机器人能够据此调整自身行为。目前多数研究聚焦于识别人类合作者的意图，本文则从理论与应用层面探讨了双向协作框架的实现策略、仿真结果及其在提升协作机器人安全性与工作效率方面的潜在应用价值。",
    "url": "https://arxiv.org/abs/2601.14809v1",
    "html_url": "https://arxiv.org/html/2601.14809v1",
    "html_content": "Stochastic Decision-Making Framework for Human-Robot Collaboration in Industrial Applications\nMuhammad Adel Yusuf,\nAli Nasir,\nZeashan Hameed Khan\nMuhammad Adel Yusuf is with the Department of Control and Instrumentation Engineering, King Fahd University of Petroleum and Minerals (KFUPM), Dhahran 31261, Saudi Arabia (e-mail: Muhammad.adel@ejust.edu.eg).Ali Nasir is with the Department of Control and Instrumentation Engineering, and the Interdisciplinary Research Center for Intelligent Manufacturing & Robotics, King Fahd University of Petroleum and Minerals (KFUPM), Dhahran 31261, Saudi Arabia (e-mail: ali.nasir@kfupm.edu.sa).Zeashan Hameed Khan is with the Interdisciplinary Research Center for Intelligent Manufacturing & Robotics, King Fahd University of Petroleum and Minerals (KFUPM), Dhahran 31261, Saudi Arabia (e-mail: Zeashan.khan@kfupm.edu.sa).\nAbstract\nCollaborative robots, or cobots, are increasingly integrated into various industrial and service settings to work efficiently and safely alongside humans. However, for effective human-robot collaboration, robots must reason based on human factors such as motivation level and aggression level. This paper proposes an approach for decision-making in human-robot collaborative (HRC) environments utilizing stochastic modeling. By leveraging probabilistic models and control strategies, the proposed method aims to anticipate human actions and emotions, enabling cobots to adapt their behavior accordingly. So far, most of the research has been done to detect the intentions of human co-workers. This paper discusses the theoretical framework, implementation strategies, simulation results, and potential applications of the bilateral collaboration approach for safety and efficiency in collaborative robotics.\n†\n†\npublicationid:\npubid: 0000–0000/00$00.00 © 2021 IEEE\nI\nIntroduction\nThe pressing need of modern industries to cope with the technological standards set by Industry 4.0 has resulted in significant advancements in robotics. These developments have facilitated the transition of robots from solitary, safety-constrained environments to cooperative ones where they operate alongside humans. This transition towards collaboration has led to the emergence of collaborative robots (cobots), which are found to be very helpful in executing many complicated tasks even through direct physical interaction or contactless collaboration with human\n[\n14\n]\n. This collaboration is crucial for achieving the maximum advantages from the human and robot by leveraging the unique capabilities of each. For instance, humans have demonstrated their ability to deal with the dynamic and variable factors of the workplace, while robots excel at doing repetitive jobs with great precision and prolonged endurance. However, there are safety issues that need to be taken into consideration when both humans and the robot are closely integrated in a shared workspace. To ensure safety collaboration between the robot and human in the working environment, these cobots are designed with safety features/components such as force sensors and emergency stop mechanisms to prevent accidents\n[\n43\n]\n. However, in open and unconstrained environments, these robots, alongside the humans need to be integrated with additional sensors (such as visual and non-visual sensors) to monitor their surroundings and detect the presence and actions of human workers, hence guaranteeing smooth and safe collaboration\n[\n50\n]\n. The integration of these components formulates a collaborative ecosystem that, when combined with human interactions, has the potential to trigger hazardous events, thereby posing risks that can make HRC unsafe under certain conditions\n[\n31\n]\n.\nTherefore, the emergence of this type of collaborative environment raises the need for safety procedures to be thoroughly reevaluated, considering both the most recent technology developments and the human element—which is crucial in Industry 5.0\n[\n3\n]\n. In this context, it has been noticed that humans use subtle and non-verbal cues, such as eye movements and head gestures, to communicate their intentions while simultaneously interpreting the cues from others to anticipate their actions\n[\n15\n]\n. This ability allows a group of people working in the same environment to interpret each other, which fosters their collaboration in shared activities. For instance, when two individuals are working together on the same task, one of them may look at an object or make a head gesture to indicate that he is going to reach for it. Without using words, the other person recognizes this cue and modifies their behavior, possibly moving aside or getting ready to help. This type of intention prediction occurs constantly in daily life and is necessary for productive teamwork. Similarly, predicting human behavior is also crucial in HRC, where the human seeks to accomplish a predefined task with the assistance of the robot\n[\n4\n]\n. For example, in collaborative industrial assembly jobs, the robot must understand the human’s intents, predict his future actions, and anticipate his next action in order to enable smooth and collaboration between the robot and human.\nFigure 1:\nBilateral intent prediction and recognition in Collaboration.\nIn this regard, different sensors have been installed even over the human co-operator or the robot to guarantee effective communication with each other. Among the sensors that have been used in this field to allow the robot and human operator to “see” and communicate with each other are proximity sensors, visual sensors, laser sensors, Inertial sensors, ultrasonic sensors, radar, and acoustic sensors\n[\n39\n,\n25\n]\n. Moreover, novel sensors have been designed in some studies that can provide human health state and motion intent in real environments\n[\n40\n]\n. In similar studies, transparency is assumed to be an important factor for intent detection\n[\n46\n]\n. However, there may be uncertainty in the correct prediction -especially with human emotions- that can be regarded as a key performance indicator in robotics tasks such as collision avoidance\n[\n38\n]\n. Human comfort is an important factor in the collaborative environment that can ensure a lesser cognitive load while interacting with the machines\n[\n48\n]\n. Therefore, the need for developing effective intent prediction algorithms based on stochastic modeling and control principles, besides the physical sensors, became a crucial need for collaborative robots to anticipate human emotions and actions more accurately, leading to safer, more efficient, and more intuitive HRC across various domains. Hence, instead of assuming the existence of the human model, this work seeks to adopt a partially observable Markov decision process (POMDP) to learn the human models from the observed data. POMDP offers a broad modelling paradigm for sequential decision-making in which actions have stochastic results and states are hidden. In contrast to most of the existing work on safety-enabled HRC, which considers the musculoskeletal human body model to determine the available workspace for the robot, this work proposes a novel tri-circle approach for safety-related decision-making.\nFigure 2:\nMulti-modal sensing and classification of the human mood and well-being.\nThis work uses stochastic modelling and control approaches to introduce a method that leverages POMDP for HRC environments. The suggested approach seeks to associate the system safety awareness to certain groups of chosen actions, which continuously encourages the HRC system to do the shared task safely and smoothly in a short time and smooth collaboration. The theoretical framework is assessed on a simulated human-robot collaborative scenario and proved capable of identifying loss and success scenarios.\nThe rest of the paper is organized as follows: Section\nII\npresents the background and reviews previous research for detecting and estimating human Intention. Section\nIII\noutlines the theoretical framework of the MDP and POMDP. Section\nIV\nintroduces the intent prediction algorithm. Section\nV\ndetails the closed-loop implementation for online trajectory generation. A numerical case study of a human collaborating with a robot in an industry environment is provided in Section\nVI\nto validate our proposed approach. Finally, Section\nVII\nsummarizes the paper with conclusions and our future work.\nII\nBackground and Related Work\nIntention detection of either the robot or human co-worker represents an essential part, and a critical area of research in HRC, since it allows robots to understand, communicate, and react to human actions and intentions. In this regard, various research studies have been conducted and analyzed from different perspectives in the hope of enhancing the capability of the robot to perceive, interpret, and predict the intentions of its human counterparts, fostering smooth interaction and efficient task execution. One of the most popular techniques in this area is the Vision-Based intent prediction model, which relies on camera systems to capture and analyze human behavior. The camera could be anchored somewhere in the workspace or may be placed on a moving part of the robot. In\n[\n20\n]\n, Khatib et al. used a depth camera mounted on the worker’s head in order to address the motion control problem between a KUKA robot and a human in a ROS environment. Nevertheless, the detection in this work was not in real-time, and three markers were positioned around the robot for continuous camera localization. On the other hand, there are other works where the vision sensor is mounted on a fixed location such as proposed in\n[\n9\n]\nin which a real time collision avoidance approach has been proposed using a Microsoft Kinect depth sensor mounted on the top of the robot workspace. In the same way, Maric et.al in\n[\n27\n]\nhave proposed a computationally effective approach that uses RGB-D-based skeleton tracking and hand-crafted modeling to identify human intention in cooperative pick-and-place tasks. However, the popularity of the vision-based approach over their counterparts, it frequently needs a lot of training data and has trouble extrapolating to new situations.\nFrom the HRC researcher’s perspective, coexisting HRC systems have been created to assist human co-operators in controlling industrial robots using simple gestures, voice commands, or even eye movements. Therefore, workers can collaborate with robots to achieve a shared goal without the need for traditional control devices/methods\n[\n34\n]\n. In this context, in\n[\n24\n]\nLavit Nicora et al. investigated the adoption of gaze direction to initiate and enhance natural collaboration between the robot and the co-operator in an assembly task. In\n[\n16\n]\nIslam et al. have proposed a simple HRC framework based on hand gesture in order to allow the underwater robots to follow the drivers without the need to translate these gestures into instructions, making it easy for divers to use without needing special tools or memorizing complex rules. Although, these gestures create a seamless collaboration, it suffers from low prediction accuracy in dynamic environments due to noisy data or the existence of obstacles. Neuroscience researchers have also contributed to predict human intentions by establishing a theoretical foundation based on analysing the motion and behaviour of the human body depending on the human brain signals\n[\n30\n]\n. For instance, Horowitz et al. [21] have proposed a feedback mathematical filter to evaluate human arm-reaching intentions in real-time, taking the advantages of the force data collected from the arm to predict the worker’s intended reaching trajectory and position. In\n[\n13\n]\n, Electromyography (EMG) signals obtained from human hands have been used to establish a model for predicting the human motor intention. This technique allows the cobot to understand and react to complex hand movements without the need for detailed knowledge of the arm’s joint mechanics. This prediction allows the robot to understand and respond to complex hand movements without needing detailed knowledge of the arm’s joint mechanics. However, the noise sensitivity of these sensors degrades their efficiency in practical applications.\nFrom the perspective of AI engineers, Intelligent decision using deep learning is found helpful for the gesture recognition of the operator in an industrial environment\n[\n11\n]\n. For instance, in\n[\n19\n]\n, Kamali Mohammadzadeh et al. have developed a virtual platform that facilitates realistic and context-aware human-robot collaboration while ensuring safety. Therefore, an unsupervised machine learning approach that combines dynamic time warping and k-means clustering has been utilized in order to allow the cobots to understand human intentions without requiring labelled data. For the same reason, Lagamtzis et al. in\n[\n21\n]\n, have proposed a novel Graph Neural Network (GNN) architecture taking the advantages of the successful application of graph-based methods for recognizing human actions and predicting 3D motion. In\n[\n51\n]\n, an HRC algorithm has been built based on a reinforcement learning algorithm that has been proposed to optimize work sequence allocation in assembly operations. Other recent studies covered intention detection for social mobile robots that interact with humans very closely for human-aware navigation\n[\n2\n]\n. In the field of industrial robotic applications,\n[\n6\n]\nCai et al. describes a federated learning approach to safeguard workers in a factory automation system. While autonomous robots are commonly used in the framework of industry 4.0, it is imperative to apply the fundamental concepts of motion estimation-based intention prediction as found in\n[\n7\n]\n. Shared control experiments using neural feedback of the Brain Computer Interface (BCI) are one of the possible use-case for safety approaches leading to improved transparency, performance, and safety\n[\n10\n]\n. Wearable devices interfaced with automation system are found to help in safe interfacing with the collaborative environment\n[\n18\n]\n. In some cases, intuitive approaches such as power and force limiting are used as a risk mitigation approach as in\n[\n22\n]\n. To verify and compare approaches, various datasets are used in the simulated response tasks in the robotics domain to assess the validity of approaches\n[\n33\n,\n49\n]\n. As shown in Figure.\n2\n, a multi-modal sensing (such as facial expressions using camera and EEG using BCI headset) for the human state of emotion could provide a measure of human well-being to the Robot safety system in order to execute collaborative tasks. Indeed if the human expressions are stressful, it may compromise the safety of both the men and machine within the industrial environment and as a precautionary measure the safety system may halt the Robot from performing any subsequent task.\nDue to the limitations found for the previous HRC approaches and in order to achieve an efficient and safe collaboration between the robot and the operator, researchers have proposed the use of probabilistic models. Among various probabilistic sequential models, such as Markov chains and Hidden Markov Models (HMMs), POMDP model has proven its capability to offer a more generalized approach by taking the partial observability into consideration. This makes it possible for POMDPs to deal with observing uncertainties brought on by concealed human actions and the robot’s subpar sensory abilities\n[\n5\n,\n32\n]\n. The fundamental idea of the POMDP approach is revolved around converted the planning and low/high level control problem in HRC into a comprehensive model for sequential decision making under uncertainties\n[\n55\n,\n52\n]\n. This model has been found in many applications such as privacy and security\n[\n1\n]\nand robotics\n[\n8\n]\nand more recently in HRC\n[\n17\n]\n. Besides POMDP, other probabilistic and stochastic models have been previously presented for discussing the systematic level design problem in HRC. For instance, in\n[\n47\n]\n, an Intention-Driven Dynamics Model (IDDM) in combination with previous work employing Gaussian Mixture Models (GMMs) has been employed to generate robot control policies through the use of reinforcement learning. In\n[\n23\n]\n, human and robot behaviors are modeled using MDP, which incorporates uncertainties through the construction of control policies based on reward functions. Similarly, a Mixed-Observability MDP (MOMDP), in which states are a combination of partially observable and observable variables, is used in\n[\n32\n]\n. In order to represent human intentions and preferences, the reward function is learned by Inverse Reinforcement Learning (IRL), and the control strategy is determined by solving an optimization problem. All in all, since POMDP extends MDP and MOMDP by handling partial observability of all system states and IDDM and GPDM operate in continuous state spaces, POMDP has been adopted as the most suitable and comprehensive approach for representing uncertainties in HRC.\nTABLE I:\nControl Approaches for Various Robot Types and Applications\nType of Robot\nControl Approach\nApplication\nRef\nAssembly Line Robot\nDeep Learning\nGesture Recognition\n[\n11\n]\n,\n[\n12\n]\nRobotic Limb\nMulti-Modal\nCollaborating Sorting\n[\n12\n]\nVirtual Robot\nShared Control using BCI\nPick and Place\n[\n10\n]\nSocial Robot\nHuman-Aware Navigation\nPath Planning\n[\n2\n]\nIndustrial Robot\nEEG-Based\nObject Handover\n[\n37\n]\nIndustrial Robot\nWorkspace Optimization\nHRC\n[\n42\n]\nIndustrial Robot\nShared Autonomy (Gaze-Based)\nAction Primitive Recognition\n[\n45\n]\nIndustrial Robot\nDigital Twin-Based Control for Human-Robot Symbiosis\nCollaborative Assembly\n[\n54\n]\nTABLE II:\nAI Algorithms Used in Human-Robot Collaboration\nAI Method\nApplication\nRef\nUnsupervised Learning\nContext-Aware HRC\n[\n19\n]\nGraph Neural Networks\nAction Recognition + 3D Motion Forecast\n[\n21\n]\nConvolutional Neural Networks (CNN)\nHuman-Robot Collaboration\n[\n26\n]\nVisuo-Lingual Transformers\nHuman-Robot Interaction\n[\n28\n]\nGAN + LSTM Nets\nCooperative Tasks\n[\n29\n]\nConditional Prediction\nHuman-Robot Collaboration\n[\n35\n]\nMarkov Decision/Prediction\nAssembly Line\n[\n36\n,\n44\n]\nBehavior Modeling\nDisassembly\n[\n41\n]\nTransformer Network\nHuman-Robot Collaboration\n[\n53\n]\nReactive Temporal Logic Planning\nHuman-Robot Collaboration\n[\n56\n]\nIII\nKey Aspects of the Proposed Approach\nBefore we present our mathematical model for decision-making, it is important to highlight the major contribution of this paper by describing the key aspects that are included in the proposed model. Based on the extensive review of literature, we are confident that our proposed approach is unique in terms of combining safety, emotion-based reasoning, and task management in a single decision-making framework.\nIII-A\nSafety Circles\nMost of the existing work on safety-enabled HRC considers the musculoskeletal human body model to determine the available workspace for the robot. In our work, we propose a tri-circle approach for safety-related decision-making. Figure.\n3\nshows the three circles of safety, where the first circle encapsulates the working space of a human (while the human is engaged in a certain task). This circle is considered by multiple existing approaches. The second circle of safety represents a human with fully stretched arms. This circle represents an aggressive physical act by the human. The third circle represents a human falling over, e.g., in case of an accident. In order to be fully safe, the robot needs to consider the largest circle of safety, but that results in a diminished workspace for the robot. On the other hand, for maximum availability of the workspace, the robot should consider the smallest circle of safety, but then the safety is not guaranteed for the cases of a human stretching his arms unexpectedly (or falling over accidentally). Therefore, the robot must have a decision-making mechanism to decide when to choose which circle of safety. Using semantic information (human pose data) from the Robot’s visual sensors, it is possible to exactly predict the safety strategy on the robot’s end that could guarantee a safe distance a robot could maintain under emergency conditions.\nFigure 3:\nThree circles of safety.\nIII-B\nHuman Emotional States and Motivation Level\nAnother factor in HRC is the human mood (or emotional state) and the motivation level of the human. Every day is different for humans because of many things going on in their lives that are related to their personal and professional lives. It is not feasible to know about the root cause of a human emotional behavior or motivational level. However, it is possible to determine the human emotional state and infer the motivation level of a human. The determination of human emotional state and motivation is a significant factor in the human-robot collaboration. Therefore, we propose this feature to be part of our framework.\nFigure 4:\nAn equivalence between Maslow’s theory of motivation for humans vs. Robot’s motivation in Collaboration.\nIn Figure.\n4\n, an equivalence of the Maslow’s hierarchy of needs is elaborated and proposed for the Robot’s motivation. This theory proposes a step-by-step progress to reach the self-actualization to deploy full human potential and, in case of a robot, requires advanced autonomy based on multi-modal sensing and an AI decision support system to play an equal supportive role in decision making.\nIII-C\nTask Assignment and Priorities\nAnother significant factor in the determination of human behavior is the tasks assigned to the human and their corresponding priorities. It is a common observation that human behavior is influenced by the rewards and cost of action choices. The awareness about the cost and rewards available to humans corresponding to various activities is likely to improve the accuracy of human intent prediction. Therefore, this factor is also considered in our formulation.\nIV\nHuman-Robot Collaboration Model\nIn this section, we develop POMDP model for collaborative decision-making. As discussed in the preceding section, we must incorporate three factors in the decision-making model, i.e., safety, human emotion, and task assignment. Furthermore, the model needs to be practically solvable as POMDPs are computationally complex in general.\nIV-A\nState Space\nKeeping all the factors into consideration, we propose the following state space for the model:\nS\n\\displaystyle S\n=\n{\ns\n1\n,\ns\n2\n,\n…\n,\ns\nN\n}\n\\displaystyle=\\{s_{1},s_{2},\\dots,s_{N}\\}\n(1)\ns\ni\n\\displaystyle s_{i}\n=\n{\nτ\ni\n,\nd\ni\n,\ne\ni\n,\nh\ni\n,\nb\ni\n}\n,\ni\n∈\n{\n1\n,\n2\n,\n3\n,\n…\n,\nN\n}\n\\displaystyle=\\{\\tau_{i},d_{i},e_{i},h_{i},b_{i}\\},\\quad i\\in\\{1,2,3,\\dots,N\\}\n(2)\nτ\ni\n\\displaystyle\\tau_{i}\n=\n{\nτ\np\n​\ni\n,\nτ\n​\nc\n​\ni\n,\nτ\n​\nx\n​\ni\n,\nτ\n​\ny\n​\ni\n}\n\\displaystyle=\\{\\tau_{pi},\\tau{ci},\\tau{xi},\\tau{yi}\\}\n(3)\nτ\np\n​\ni\n\\displaystyle\\tau_{pi}\n∈\n{\n0\n,\n1\n,\n2\n,\n…\n,\nρ\n}\n,\n\\displaystyle\\in\\{0,1,2,\\dots,\\rho\\},\n(4)\nτ\n​\nc\n​\ni\n\\displaystyle\\tau{ci}\n∈\n{\n0\n,\n1\n,\n2\n,\n3\n,\n…\n,\nσ\n}\n\\displaystyle\\in\\{0,1,2,3,\\dots,\\sigma\\}\n(5)\nτ\n​\nx\n​\ni\n\\displaystyle\\tau{xi}\n∈\n{\n0\n,\n1\n,\n2\n,\n3\n}\n\\displaystyle\\in\\{0,1,2,3\\}\n(6)\nτ\n​\ny\n​\ni\n\\displaystyle\\tau{yi}\n∈\n{\n0\n,\n1\n,\n2\n,\n3\n}\n\\displaystyle\\in\\{0,1,2,3\\}\n(7)\nd\ni\n\\displaystyle d_{i}\n∈\n{\n0\n,\n1\n,\n2\n,\n3\n}\n\\displaystyle\\in\\{0,1,2,3\\}\n(8)\ne\ni\n\\displaystyle e_{i}\n=\n{\ne\nm\n​\ni\n,\ne\na\n​\ni\n}\n,\n\\displaystyle=\\{e_{mi},e_{ai}\\},\n(9)\ne\nm\n​\ni\n\\displaystyle e_{mi}\n∈\n{\n0\n,\n1\n,\n2\n}\n\\displaystyle\\in\\{0,1,2\\}\n(10)\ne\na\n​\ni\n\\displaystyle e_{ai}\n∈\n{\n0\n,\n1\n,\n2\n}\n\\displaystyle\\in\\{0,1,2\\}\n(11)\nh\ni\n\\displaystyle h_{i}\n=\n{\nh\np\n​\ni\n,\nh\nc\n​\ni\n}\n,\n\\displaystyle=\\{h_{pi},h_{ci}\\},\n(12)\nh\np\n​\ni\n\\displaystyle h_{pi}\n∈\n{\n0\n,\n1\n,\n2\n,\n…\n,\nρ\n}\n,\n\\displaystyle\\in\\{0,1,2,\\dots,\\rho\\},\n(13)\nh\nc\n​\ni\n\\displaystyle h_{ci}\n∈\n{\n0\n,\n1\n,\n2\n,\n3\n,\n…\n,\nσ\n}\n\\displaystyle\\in\\{0,1,2,3,\\dots,\\sigma\\}\n(14)\nb\ni\n\\displaystyle b_{i}\n=\n{\nb\np\n​\ni\n,\nb\nc\n​\ni\n}\n,\n\\displaystyle=\\{b_{pi},b_{ci}\\},\n(15)\nb\np\n​\ni\n\\displaystyle b_{pi}\n∈\n{\n0\n,\n1\n,\n2\n,\n…\n,\nρ\n}\n,\n\\displaystyle\\in\\{0,1,2,\\dots,\\rho\\},\n(16)\nb\nc\n​\ni\n\\displaystyle b_{ci}\n∈\n{\n0\n,\n1\n,\n2\n,\n3\n,\n…\n,\nσ\n}\n\\displaystyle\\in\\{0,1,2,3,\\dots,\\sigma\\}\n(17)\nN\n\\displaystyle N\n=\n(\nρ\n×\nσ\n)\n3\n×\n36\n\\displaystyle=(\\rho\\times\\sigma)^{3}\\times 36\n(18)\nWe have included five types of variables in the state space, namely, task assigned (\nτ\ni\n\\tau_{i}\n), distance between the robot and the human (\nd\ni\nd_{i}\n), emotion of the human (\ne\ni\ne_{i}\n), current human activity (\nh\ni\nh_{i}\n), and current robot activity (\nb\ni\nb_{i}\n).\nThe task variable includes information regarding task priority (\nτ\np\n​\ni\n\\tau_{pi}\n), task duration (\nτ\nc\n​\ni\n\\tau_{ci}\n), task nature (\nτ\nx\n​\ni\n\\tau_{xi}\n), and task commitment status (\nτ\ny\n​\ni\n\\tau_{yi}\n). Task priority ranges between zero (lowest priority) to\nρ\n\\rho\n(highest priority). The units of task duration are in terms of the decision epochs ranging between zero (meaning that the task has been completed) and\nσ\n\\sigma\n(meaning that the task will need\nσ\n\\sigma\ndecision epochs to complete). The nature of the task (\nτ\nx\n​\ni\n\\tau_{xi}\n) identifies whether the task can be accomplished using the robot only (\nτ\nx\n​\ni\n=\n0\n\\tau_{xi}=0\n), the human only (\nτ\nx\n​\ni\n=\n1\n\\tau_{xi}=1\n), either the robot or the human (\nτ\nx\n​\ni\n=\n2\n\\tau_{xi}=2\n) or needs both human and robot (\nτ\nx\n​\ni\n=\n3\n\\tau_{xi}=3\n). Finally, the task commitment variable (\nτ\ny\n​\ni\n\\tau_{yi}\n) indicates whether the assigned task is uncommitted (\nτ\ny\n​\ni\n=\n0\n\\tau_{yi}=0\n), committed by the human (\nτ\ny\n​\ni\n=\n1\n\\tau_{yi}=1\n), committed by the robot (\nτ\ny\n​\ni\n=\n2\n\\tau_{yi}=2\n), or committed by the human and the robot as a joint venture (\nτ\ny\n​\ni\n=\n3\n\\tau_{yi}=3\n).\nThe distance variable (\nd\ni\nd_{i}\n) is defined in terms of the safety circles defined earlier where the value zero corresponds to the collision, one corresponds to the smallest safety circle distance, and three corresponds to the largest safety circle distance. Any distance greater than the largest safety circle is considered as the largest safety circle distance.\nNext, the human emotion variable (\ne\ni\ne_{i}\n) includes the motivation level (\ne\nm\n​\ni\ne_{mi}\n) that ranges from low motivation (\ne\nm\n​\ni\n=\n0\ne_{mi}=0\n) to high motivation (\ne\nm\n​\ni\n=\n2\ne_{mi}=2\n). Another emotion-related variable is human aggression level (\ne\na\n​\ni\ne_{ai}\n) that ranges from low (\ne\na\n​\ni\n=\n0\ne_{ai}=0\n) to high (\ne\na\n​\ni\n=\n2\ne_{ai}=2\n). The purpose of the motivation variable is clear as per our discussion in the preceding section. The aggression variable has been included from the safety distance point of view, e.g., if the human is working aggressively, then it is more desirable for the robot to maintain a larger circle of safety.\nThe fourth type of variable in our state space is current human activity (\nh\ni\nh_{i}\n), which includes the priority of the activity (\nh\np\n​\ni\nh_{pi}\n) and the progress of the activity (\nh\nc\n​\ni\nh_{ci}\n). Note that the priority variable here is similar to the one associated with the assigned task (ranging between zero and\nρ\n\\rho\n). Similarly, the task progress variable (\nh\nc\n​\ni\nh_{ci}\n) is similar to the task deadline variable (\nτ\nc\n​\ni\n\\tau_{ci}\n), i.e., the value of\nh\nc\n​\ni\nh_{ci}\nindicates how many more decision epochs are required by the human to complete the current activity.\nThe fifth type of variable is the current activity of the robot (\nb\ni\nb_{i}\n), which includes the priority of the activity (\nb\np\n​\ni\nb_{pi}\n) and the progress of the activity (\nb\nc\n​\ni\nb_{ci}\n).\nFigure\n5\nshows the map of the state space as discussed above.\nFigure 5:\nState Variable Mapping.\nIV-B\nDecision Variables\nThe decision-making in our setup is done by the robot only. The decisions made by the human are treated as exogenous events for the robot. Based on the state space defined above, the decision variables available to the robot include the following:\nD\n=\n{\nnw\n,\nrh\n1\n,\nrh\n2\n,\nct\n,\ndp\n,\ndm\n,\nmh\n,\ndn\n}\nD=\\{\\text{nw},\\text{rh}_{1},\\text{rh}_{2},\\text{ct},\\text{dp},\\text{dm},\\text{mh},\\text{dn}\\}\n(19)\nNote that, unlike the elements in the set of states, the elements in the set of decisions are not variables with real values. Rather, each of these elements corresponds to some action taken by the robot. Specifically,\nnw\nrefers to “normal work mode,” in which case, the robot shall continue to be in the same state of working (or not working) for the next decision epoch except for\nb\nc\nb_{c}\nand\nh\nc\nh_{c}\n. Next is\nrh\n1\n, which refers to requesting the human only to take up the assigned task, and\nrh\n2\n, which refers to requesting the human to take up the assigned task alongside the robot.\nmh\nrefers to motivating the human. Decision\nct\nrefers to “commit to task,” where the robot itself commits to the assigned task. The decisions\ndp\nand\ndm\ncorrespond to increasing and decreasing the distance between the human and the robot, respectively. Finally, the decision\ndn\nrefers to the “Do nothing” mode, which is to be activated when there is no task to be pursued or in case of collision or a high chance of collision. These actions are summarized and described in Table\nIII\n.\nTABLE III:\nDescription of the Actions\nNotation\nDescription\nnw\nNormal working\nrh1\nRequest human to do the assigned task alone\nrh2\nRequest human to do the assigned task in collaboration with the robot\nct\nCommit the task\ndp\nIncrease the distance between the human and the robot for safety purposes.\ndm\nDecrease the distance between the human and the robot when collaboration requires close interaction.\nmh\nMotivate the human to improve task performance and maintain engagement.\ndn\nDo nothing, idle mode activated when there is no task to be done.\nIV-C\nObservation Variables\nThe observation variables included in the proposed model concern the position of the human body, joint angles, and angular velocities for the human joints. Upon observing these variables, the robot can assess the probability distribution for the relevant state variables. Mathematically, the set of observation variables is given by:\nO\n=\n{\nq\n0\n,\nq\nl\n​\ns\n,\nq\nr\n​\ns\n,\nq\nl\n​\ne\n,\nq\nr\n​\ne\n,\nq\nl\n​\nh\n,\nq\nr\n​\nh\n,\nq\n˙\n0\n,\nq\n˙\nl\n​\ns\n,\nq\n˙\nr\n​\ns\n,\nq\n˙\nl\n​\ne\n,\nq\n˙\nr\n​\ne\n,\nq\n˙\nl\n​\nh\n,\nq\n˙\nr\n​\nh\n}\nO=\\{q_{0},q_{ls},q_{rs},q_{le},q_{re},q_{lh},q_{rh},\\dot{q}_{0},\\dot{q}_{ls},\\dot{q}_{rs},\\dot{q}_{le},\\dot{q}_{re},\\dot{q}_{lh},\\dot{q}_{rh}\\}\n(20)\nq\n∗\n,\nq\n˙\n∗\n∈\n{\n0\n,\n1\n,\n2\n,\n…\n,\ny\n}\nq_{*},\\dot{q}_{*}\\in\\{0,1,2,\\dots,y\\}\n(21)\nEquation\n20\nincludes the position of the center of mass for the human body (\nq\n0\nq_{0}\n) along with the corresponding velocity (\nq\n˙\n0\n\\dot{q}_{0}\n). Furthermore, we have the joint angles and velocities for the left shoulder (\nq\nl\n​\ns\n,\nq\n˙\nl\n​\ns\nq_{ls},\\dot{q}_{ls}\n), right shoulder (\nq\nr\n​\ns\n,\nq\n˙\nr\n​\ns\nq_{rs},\\dot{q}_{rs}\n), left elbow (\nq\nl\n​\ne\n,\nq\n˙\nl\n​\ne\nq_{le},\\dot{q}_{le}\n), right elbow (\nq\nr\n​\ne\n,\nq\n˙\nr\n​\ne\nq_{re},\\dot{q}_{re}\n), left hand (\nq\nl\n​\nh\n,\nq\n˙\nl\n​\nh\nq_{lh},\\dot{q}_{lh}\n), and right hand (\nq\nr\n​\nh\n,\nq\n˙\nr\n​\nh\nq_{rh},\\dot{q}_{rh}\n). Note that we have discretized the values from zero to\ny\ny\nbecause our framework is designed for high-level decision-making, not for low-level control where precise values of these variables may be required. From the observation of these variables, the robot can infer the probability distribution for the stochastic variables in the state space, such as human motivation level (\ne\nm\ne_{m}\n) and human aggression (\ne\na\ne_{a}\n).\nIV-D\nState Transition Mapping\nThere are five types of state variables. One type is formulated as random and unknown (with a known conditional probability distribution), i.e., the human emotion, including the human motivation level (\ne\nm\ne_{m}\n) and the human aggression level (\ne\na\ne_{a}\n). Task assignment (\nτ\n\\tau\n) is assumed to be an exogenous variable (the variable is known at any instant, but its transition in the next decision epoch depends upon the unmodeled user). The distance between the human and the robot (\nd\nd\n), as well as the current activities of the human (\nh\ni\nh_{i}\n) and the robot (\nb\ni\nb_{i}\n), are known. However, the transition of the distance is assumed to be stochastic with a known conditional transition probability distribution. The state transition also depends on the decision taken by the robot.\nConsequently, the state transition mapping is a function of the form:\nT\n​\n(\ns\nj\n,\nD\n,\ns\ni\n)\n,\n∀\ni\n,\nj\n∈\n{\n1\n,\n2\n,\n…\n,\nN\n}\nT(s_{j},D,s_{i}),\\quad\\forall i,j\\in\\{1,2,\\dots,N\\}\n(22)\nwith\n6\n​\nN\n2\n6N^{2}\nentries (because the size of the decision space is six, and the size of the state space is\nN\nN\n). The formulation of the state transition probabilities is expressed as follows:\nP\n​\n(\ns\nj\n∣\ns\ni\n,\nO\n,\nD\n)\n=\nP\n​\n(\ne\nm\nj\n,\ne\na\nj\n∣\ne\nm\ni\n,\ne\na\ni\n,\nO\n,\nD\n)\nP(s_{j}\\mid s_{i},O,D)=P(e_{m_{j}},e_{a_{j}}\\mid e_{m_{i}},e_{a_{i}},O,D)\n(23)\nTo complete the transition model, we also need to know the observation probabilities, given by:\nP\n​\n(\nO\n∣\ne\nm\nj\n,\ne\na\nj\n,\nD\n)\n\\displaystyle P(O\\mid e_{m_{j}},e_{a_{j}},D)\n=\nP\n(\nq\n0\n,\nq\nl\n​\ns\n,\nq\nr\n​\ns\n,\nq\nl\n​\ne\n,\nq\nr\n​\ne\n,\nq\nl\n​\nh\n,\nq\nr\n​\nh\n,\n\\displaystyle=P(q_{0},q_{ls},q_{rs},q_{le},q_{re},q_{lh},q_{rh},\n(24)\nq\n˙\n0\n,\nq\n˙\nl\n​\ns\n,\nq\n˙\nr\n​\ns\n,\nq\n˙\nl\n​\ne\n,\nq\n˙\nr\n​\ne\n,\nq\n˙\nl\n​\nh\n,\nq\n˙\nr\n​\nh\n∣\ne\nm\nj\n,\ne\na\nj\n,\nD\n)\n\\displaystyle\\quad\\dot{q}_{0},\\dot{q}_{ls},\\dot{q}_{rs},\\dot{q}_{le},\\dot{q}_{re},\\dot{q}_{lh},\\dot{q}_{rh}\\mid e_{m_{j}},e_{a_{j}},D)\nWe would like to point out that the only action of the robot that is likely to impact human emotion is when the robot requests the human to commit to an assigned task, i.e.,\nD\n=\nr\n​\nh\nD=rh\n. Furthermore, the transition for the variable\nh\nc\nh_{c}\nfrom one decision epoch to the next follows the rule:\nh\nc\nj\n=\n{\nτ\nc\ni\n,\nif human commits\nmax\n⁡\n(\n0\n,\n(\nh\nc\ni\n−\n1\n)\n)\n,\notherwise\nh_{c_{j}}=\\begin{cases}\\tau_{c_{i}},&\\text{if human commits}\\\\\n\\max(0,(h_{c_{i}}-1)),&\\text{otherwise}\\end{cases}\n(25)\nNote that in the above rule, the commitment by the human is an exogenous event. The transition of the variable\nb\nc\nb_{c}\nfollows the rule:\nb\nc\nj\n=\n{\nτ\nc\ni\n,\nif\n​\nD\n=\nc\n​\nt\nmax\n⁡\n(\n0\n,\n(\nb\nc\ni\n−\n1\n)\n)\n,\notherwise\nb_{c_{j}}=\\begin{cases}\\tau_{c_{i}},&\\text{if }D=ct\\\\\n\\max(0,(b_{c_{i}}-1)),&\\text{otherwise}\\end{cases}\n(26)\nIn the above rule, the main difference from the previous one is that the commitment by the robot is done via the decision-making policy. The variable\nh\np\nh_{p}\nfollows the transition rule:\nh\np\nj\n=\n{\nτ\np\ni\n,\nif human commits\nh\np\ni\n,\nelse if\n​\nh\nc\ni\n≠\n0\n0\n,\notherwise\nh_{p_{j}}=\\begin{cases}\\tau_{p_{i}},&\\text{if human commits}\\\\\nh_{p_{i}},&\\text{else if }h_{c_{i}}\\neq 0\\\\\n0,&\\text{otherwise}\\end{cases}\n(27)\nHere again, the commitment by the human is an exogenous event. Similarly, the variable\nb\np\nb_{p}\nfollows the transition rule:\nb\np\nj\n=\n{\nτ\np\ni\n,\nif\n​\nD\n=\nc\n​\nt\nb\np\ni\n,\nelse if\n​\nb\nc\ni\n≠\n0\n0\n,\notherwise\nb_{p_{j}}=\\begin{cases}\\tau_{p_{i}},&\\text{if }D=ct\\\\\nb_{p_{i}},&\\text{else if }b_{c_{i}}\\neq 0\\\\\n0,&\\text{otherwise}\\end{cases}\n(28)\nHere again, the commitment by the robot is made through the decision-making policy. The transition in the task assignment variable is assumed to be exogenous and does not follow any rule or probability distribution in our model. However, once the task is assigned, it can be committed by the robot or the human, resulting in the following transition rule:\nτ\ny\nj\n=\n{\n3\n,\nif committed by human and\n​\nD\n=\nc\n​\nt\n2\n,\nelse if\n​\nD\n=\nc\n​\nt\n1\n,\nelse if committed by human\nτ\ny\ni\n,\notherwise\n\\tau_{y_{j}}=\\begin{cases}3,&\\text{if committed by human and }D=ct\\\\\n2,&\\text{else if }D=ct\\\\\n1,&\\text{else if committed by human}\\\\\n\\tau_{y_{i}},&\\text{otherwise}\\end{cases}\n(29)\nNote that, in the above transition rule, the commitment by the human is an exogenous event. Finally, the transition in the distance variable follows the rule:\nd\nj\n=\n{\nmin\n⁡\n(\n3\n,\nd\ni\n+\n1\n−\nΔ\n)\n,\nif\n​\nD\n=\nd\n​\np\nmax\n⁡\n(\n1\n,\nd\ni\n−\n1\n−\nΔ\n)\n,\nelse if\n​\nD\n=\nd\n​\nm\nd\ni\n−\nΔ\n,\notherwise\nd_{j}=\\begin{cases}\\min(3,d_{i}+1-\\Delta),&\\text{if }D=dp\\\\\n\\max(1,d_{i}-1-\\Delta),&\\text{else if }D=dm\\\\\nd_{i}-\\Delta,&\\text{otherwise}\\end{cases}\n(30)\nIn the above rule,\nΔ\n∈\n{\n0\n,\n−\n1\n,\n1\n}\n\\Delta\\in\\{0,-1,1\\}\nis a random variable that has been included to model the random motion of the human, causing a change in the distance between the robot and the human. The probability distribution of this variable is given by:\nP\n​\n(\nΔ\n)\n=\nP\n​\n(\nΔ\n∣\ne\na\ni\n)\n​\nP\n​\n(\ne\na\ni\n)\nP\n​\n(\ne\na\ni\n∣\nΔ\n)\nP(\\Delta)=\\frac{P(\\Delta\\mid e_{a_{i}})P(e_{a_{i}})}{P(e_{a_{i}}\\mid\\Delta)}\n(31)\nThis completes our description of the state transitions. Note that two types of variables have randomness involved. This implies that to use our proposed framework, the associated probabilities have to be either calculated, estimated, or learned. We believe that obtaining the required probability distributions must be feasible owing to the current state of the art in machine learning.\nIV-E\nFormulation of the Cost Function\nThe cost function formulation is based on the objectives of the proposed framework. In our case, the objectives are the following:\n1.\nAvoid collision with the human (keep a safe distance),\n2.\nComplete the assigned task as soon as possible, and\n3.\nExercise regard for the human emotional state.\nConsequently, we formulate the reward function as follows:\nJ\n​\n(\ns\n)\n=\nk\n1\n​\nf\n1\n​\n(\nτ\np\n,\nτ\nc\n,\nτ\ny\n,\nb\nc\n,\nb\np\n,\nh\nc\n,\nh\np\n,\nd\n)\n+\nk\n2\n​\nf\n2\n​\n(\nd\n,\ne\na\n,\ne\nm\n)\nJ(s)=k_{1}f_{1}(\\tau_{p},\\tau_{c},\\tau_{y},b_{c},b_{p},h_{c},h_{p},d)+k_{2}f_{2}(d,e_{a},e_{m})\n(32)\nThe above cost function is composed of two terms. The first term corresponds to the task completion objective. The function\nf\n1\nf_{1}\nmay have many possible realizations as long as the cost is proportional to the time required to complete the task and commitment by either the human or the robot (or both). The cost of having an uncommitted task is proportional to the task priority. Similarly, the cost of time required to complete the task is the sum of the time required to execute the task and the time required to finish any ongoing activity before the commitment to the task.\nThe second term in the cost function is related to keeping a safe distance from the human that depends upon the aggression level of the human. The function\nf\n2\nf_{2}\ncan be defined in multiple ways as long as the objectives of collision avoidance and having regard for human emotion are satisfied. For example, the recommended safe distance for a higher level of aggression should be more than that for a lower level of aggression. Sample realization (used later in the simulation-based case study) of the functions\nf\n1\nf_{1}\nand\nf\n2\nf_{2}\nis provided as follows in Equations (\n33\n) and (\n34\n):\nf\n1\n\\displaystyle f_{1}\n=\n[\nα\n1\n​\n(\nτ\ny\n=\n3\n)\n⋅\nd\n⋅\nh\nc\n⋅\nb\nc\n]\n\\displaystyle=[\\alpha_{1}(\\tau_{y}=3)\\cdot d\\cdot h_{c}\\cdot b_{c}]\n(33)\n+\n[\n(\nα\n2\n+\nα\n3\n​\n(\nb\np\n=\n0\n)\n+\nα\n4\n​\n(\nh\np\n=\n0\n)\n)\n​\n(\nτ\ny\n=\n0\n)\n​\n(\nτ\np\n≠\n0\n)\n]\n\\displaystyle+[(\\alpha_{2}+\\alpha_{3}(b_{p}=0)+\\alpha_{4}(h_{p}=0))(\\tau_{y}=0)(\\tau_{p}\\neq 0)]\n+\n[\nα\n5\n​\n(\n2\n−\ne\nm\n)\n​\n(\nτ\nx\n=\n2\n)\n​\n(\nτ\ny\n=\n0\n)\n]\n\\displaystyle+[\\alpha_{5}(2-e_{m})(\\tau_{x}=2)(\\tau_{y}=0)]\n+\n[\nα\n6\n​\n(\nτ\nx\n=\n3\n)\n​\n(\nτ\ny\n≠\n3\n)\n]\n\\displaystyle+[\\alpha_{6}(\\tau_{x}=3)(\\tau_{y}\\neq 3)]\n+\n[\nα\n7\n​\n(\nτ\np\n>\nb\np\n)\n​\n(\nτ\ny\n=\n0\n)\n]\n\\displaystyle+[\\alpha_{7}(\\tau_{p}>b_{p})(\\tau_{y}=0)]\n+\n[\nα\n8\n​\nb\nc\n]\n+\n[\nα\n9\n​\nh\nc\n]\n\\displaystyle+[\\alpha_{8}b_{c}]+[\\alpha_{9}h_{c}]\nf\n2\n\\displaystyle f_{2}\n=\n[\nβ\n1\n​\n(\n3\n−\nd\n)\n]\n\\displaystyle=[\\beta_{1}(3-d)]\n(34)\n+\n[\n(\nτ\ny\n≠\n3\n)\n(\nβ\n2\ne\n(\nβ\n3\n​\n(\ne\na\n−\n3\n)\n​\nd\n)\n\\displaystyle+[(\\tau_{y}\\neq 3)(\\beta_{2}e^{(\\beta_{3}(e_{a}-3)d)}\n+\nβ\n4\n​\ne\n(\n−\nβ\n5\n​\n(\ne\nm\n+\n1\n)\n​\nd\n)\n\\displaystyle\\quad+\\beta_{4}e^{(-\\beta_{5}(e_{m}+1)d)}\n+\ne\na\n⋅\nβ\n6\ne\n−\nd\n)\n]\n\\displaystyle\\quad+e_{a}\\cdot\\beta_{6}e^{-d})]\nNote that Equation (\n33\n) is composed of seven terms. The first term penalizes the distance between the human and the robot when they are working in collaboration. This term will compete with the safety cost to ensure efficient collaboration. The second term in Equation (\n34\n) penalizes unassigned tasks (using\nα\n2\n\\alpha_{2}\n) and robot or human staying idle despite the availability of a task to be completed. The next term penalizes a low motivation level of the human. The fourth term penalizes the wrong task assignment. The fifth term penalizes the robot for doing a low-priority task despite the availability of a high-priority task. The last two terms penalize the robot and human for an incomplete task.\nThe sample safety cost function in Equation (\n34\n) consists of two main terms. The first term penalizes the lack of distance between the human and the robot. The second term penalizes the lack of distance from the perspective of human aggression level (\ne\na\ne_{a}\n) and human motivation level (\ne\nm\ne_{m}\n).\nIV-F\nDecision-Making under Uncertainty and MDP\nOne of the most widely used frameworks for calculating an optimal policy under uncertainty is the MDP. The MDP, as shown in Figure.\n6\n, provides a structured approach to model complex decision-making problems, enabling the calculation of an optimal policy through stochastic dynamic programming techniques, such as policy iteration and value iteration. To mathematically formulate an MDP problem, five key elements must be predefined: a set of states, a set of actions, a reward (or cost) function, a state-action transition probability matrix, and a discount factor within the interval\n(\n0\n,\n1\n)\n(0,1)\n. An MDP is thus characterized as a five-tuple:\nM\n​\nD\n​\nP\n=\n{\nS\n,\nD\n,\nR\n,\nP\n,\nγ\n}\nMDP=\\{S,D,R,P,\\gamma\\}\n(35)\nwhere the set of states\nS\nS\nrepresents possible configurations or conditions of the system. The action set\nD\nD\ncontains all possible actions that can be taken to transition the system from one state toward a desired goal state. The reward function\nR\nR\nassigns a value to each state-action pair, distinguishing between favorable and unfavorable outcomes, and can represent the cost or reward of specific actions. The probability tensor\nP\nP\ncaptures the transition dynamics, defined as:\nP\n​\n(\ns\n′\n∣\ns\n,\nd\n)\nP(s^{\\prime}\\mid s,d)\n(36)\nwhich gives the probability of reaching state\ns\n′\n∈\nS\ns^{\\prime}\\in S\nfrom state\ns\n∈\nS\ns\\in S\nafter performing action\nd\n∈\nD\nd\\in D\n. Finally, the discount factor\nγ\n\\gamma\ndetermines how future rewards are valued relative to immediate ones. A discount factor close to\n1\n1\nimplies that future rewards are valued almost as highly as current ones, while a lower\nγ\n\\gamma\nplaces more emphasis on immediate rewards. In addition to the five core elements, an MDP also requires a specified decision-making horizon, which defines the time frame over which decisions are evaluated. This includes determining the decision epoch—the interval between consecutive decision points. In our application, the decision epoch corresponds to the time required for the Cobot to complete its task, which can be identified in seconds, minutes, hours, or even according to the processing unit of the robot. Once a decision-making problem is modeled as an MDP, it can be solved using various stochastic dynamic programming algorithms, such as policy iteration or value iteration. In this paper, we use value iteration, as it allows us to leverage the optimal values of local MDP states when calculating the global decision—a process that will be elaborated upon in the next section. The value iteration algorithm determines the optimal value for each state by applying Bellman’s equation, as shown in Eq. (\n37\n):\nV\n(\nt\n+\n1\n)\n​\n(\ns\n)\n=\nmax\nd\n∈\nD\n⁡\n{\n−\nJ\n​\n(\ns\n,\nd\n)\n+\n∑\ns\n′\n∈\nS\nγ\n​\nP\n​\n(\ns\n′\n∣\ns\n,\nd\n)\n​\nV\nt\n​\n(\ns\n′\n)\n}\nV^{(t+1)}(s)=\\max_{d\\in D}\\left\\{-J(s,d)+\\sum_{s^{\\prime}\\in S}\\gamma P(s^{\\prime}\\mid s,d)V^{t}(s^{\\prime})\\right\\}\n(37)\nTo begin the value iteration process, the values for each state are initially set to arbitrary values. After a certain number of iterations, the state values converge, meaning that:\nV\n(\nt\n+\n1\n)\n​\n(\ns\n)\n=\nV\nt\n​\n(\ns\n)\n,\n∀\ns\n∈\nS\nV^{(t+1)}(s)=V^{t}(s),\\quad\\forall s\\in S\n(38)\nregardless of the initial values. This iterative process continues until a specified stopping criterion is met, typically based on the difference between consecutive iterations, ensuring that the values have stabilized to a satisfactory level of precision:\n‖\nV\n(\nt\n+\n1\n)\n−\nV\nt\n‖\n∞\n<\nη\n\\|V^{(t+1)}-V^{t}\\|_{\\infty}<\\eta\n(39)\nHere,\nV\nt\nV^{t}\nrepresents the vector containing the values of all states at time\nt\nt\n. Once the convergence criterion is satisfied, the resulting state values are considered optimal and are represented as:\nV\n∗\n​\n(\ns\n)\n,\n∀\ns\n∈\nS\n.\nV^{*}(s),\\quad\\forall s\\in S.\n(40)\nThese optimal values are then used to derive the optimal policy, calculated according to the following equation:\nπ\n​\n(\ns\n)\n=\narg\n⁡\nmax\nd\n∈\nD\n⁡\n{\n−\nJ\n​\n(\ns\n,\nd\n)\n+\n∑\ns\n′\n∈\nS\nγ\n​\nP\n​\n(\ns\n′\n∣\ns\n,\nd\n)\n​\nV\n∗\n​\n(\ns\n′\n)\n}\n\\pi(s)=\\arg\\max_{d\\in D}\\left\\{-J(s,d)+\\sum_{s^{\\prime}\\in S}\\gamma P(s^{\\prime}\\mid s,d)V^{*}(s^{\\prime})\\right\\}\n(41)\nNote that\nπ\n​\n(\ns\n)\n∈\nD\n\\pi(s)\\in D\n, i.e.,\nπ\n:\nS\n→\nD\n\\pi:S\\to D\n.\nFigure 6:\nOffline policy implementation of MDP.\nDue to the existence of true states of the system, such as human emotions, which are not directly observable, the decision-making process becomes extremely uncertain, necessitating the use of POMDP. A POMDP is defined as a combination of an MDP to model system dynamics with a hidden Markov model (HMM) that connects unobservable system states to quantifiable observations\n[\n14\n]\n. To extend the previously implemented MDP framework to a POMDP, the present model is improved by adding observation and belief states to account for hidden/uncertain states that cannot be directly measured. By adding an observation space and an observation model, the POMDP expands on the MDP structure. The observation model describes the likelihood of obtaining a specific observation given the system’s present hidden state, whereas the observation space contains quantifiable signals that indirectly reflect the hidden states.\nAs shown in Figure.\n7\n, the implementation begins with an initial belief state, which represents a probability distribution over possible hidden states. From this belief, initial samples are created to approximate the system’s state space. Using the optimal MDP policy, obtained previously through MDP, an action is selected using a probabilistic or heuristic technique that considers the score calculation of each action based on the belief state.\nThe chosen action is then applied, and the samples are propagated through the system’s transition model to predict the new state distribution. After that, an observation is sent to the system, which indirectly reveals the actual state. This observation is incorporated into the belief state, improving the probability distribution over all potential states. By adjusting to the dynamics of the system and the unpredictability of its hidden states, the updated belief state then feeds back into the action selection process, forming an iterative loop that continuously enhances decision-making.\nOur proposed action selection heuristic is presented in Figure.\n8\nIn this heuristic,\nw\ni\n​\nj\nw_{ij}\nrepresents the weight of\nj\nt\n​\nh\nj^{th}\nsample (state) that gives\ni\nt\n​\nh\ni^{th}\ndecision as an optimal one as per the policy calculated from the MDP. To calculate the best decision at any instant, the weights are added for each corresponding decision, and whichever decision has the highest sum of weights is declared as the best decision, and the same is executed for all samples.\nFurthermore, in our proposed action selection heuristic, we exploit the structure of the optimal policy by gathering all the states that have the same optimal decision as per the pre-calculated MDP policy. On top of that, since the weights are proportional to the probabilities of the states being the actual state, we add the weights to determine the best action. Consider the example of a robot in a maze. Assuming that the states in which turning left is the optimal action are highly likely as per the current belief state, then our action selection heuristic would choose turning left as the best action. If, for some reason, two or more actions have an equal score, then multiple ideas can be used to break the ties between the actions. For example, the highest individual weight value can be used as a tie breaker, e.g., the decision that can an individual weight whose value is larger than all individual weights of the other decision shall win.\nBy integrating the POMDP framework into the existing MDP implementation, the system can make intelligent and flexible decisions in the face of ambiguity. This allows the system to account for the dynamic and hidden nature of true states, such as human emotions. This method improves the model’s resilience and suitability for real-world situations where it is impractical to directly observe crucial states.\nFigure 7:\nFramework of executing the POMDP.\nFigure 8:\nAction Selection Heuristic.\nV\nClosed loop implementation (Online trajectory)\nAfter determining the optimal policy through the value iteration technique, the robot’s online trajectory control begins, as illustrated in Figure.\n9\nThis process initiates by setting the initial states of the system. In the subsequent step, the optimal actions are identified by referencing the pre-computed optimal policy, which ensures that the chosen actions maximize the expected reward. Once the best actions are selected, the likely state transitions are computed using the transition probability tensor, which captures the probability of moving from one state to another based on the selected actions. This step is critical for accounting for uncertainty in the environment and for predicting the robot’s next state with high accuracy. Finally, the states are updated according to the identified transitions, and the loop is completed by returning to the second step, where the policy is re-evaluated in light of the new states. This closed-loop process enables the robot to adapt its actions dynamically, optimizing its trajectory in real time based on the evolving states and environmental conditions.\nFigure 9:\nonline execution of optimal policy.\nVI\nNumerical Case study\nIn this section, we perform a detailed analysis of the proposed framework with the help of a numerical simulation-based case study. In our case study, we aim to find the optimal policy that can ensure performing the required task in a very short time taking into consideration the safe collaboration between the robot and the human. The next subsection provides detailed parameter values, followed by an analysis of the closed-loop execution results and key trends observed in the optimal task assignment policy derived through the proposed framework.\nVI-A\nParameter values\nThere are three types of parameter values: cost function parameters, transition probability parameters, and problem size parameters. Table\nIV\ndetails all parameter values and the definitions of the abstract functions. These values were selected roughly to estimate the relation between the time needed for the robot to accomplish the task and ensuring safe collaboration with the human. However, they are intended solely for demonstration and should not be applied directly to real-life missions without further validation\nTABLE IV:\nParameter Values for the Case Study\nParameter\nValue\nRemark\nn\nn\n419904\nNumber of states.\nk\nk\n20\nNumber of iterations.\nD\nD\n8\nNumber of decision variables.\nτ\np\ni\n\\tau_{p_{i}}\n{0,1,2}\n“0” for no task, “1” for low-priority task, “2” for high-priority task.\nτ\nc\ni\n\\tau_{c_{i}}\n{0,1,2}\n“0” for task completion, “1” the task needs 1 epoch for completion, “2” the task needs 2 epochs for completion.\nh\np\ni\nh_{p_{i}}\n{0,1,2}\n“0” for no human priority, “1” for low human priority, “2” for high human priority.\nh\nc\ni\nh_{c_{i}}\n{0,1,2}\n“0” for human completed assigned task, “1” human needs 1 epoch, “2” human needs 2 epochs to complete the assigned task.\nb\np\ni\nb_{p_{i}}\n{0,1,2}\n“0” for no robot priority, “1” for low robot priority, “2” for high robot priority.\nb\nc\ni\nb_{c_{i}}\n{0,1,2}\n“0” for robot completed assigned task, “1” robot needs 1 epoch, “2” robot needs 2 epochs to complete the assigned task.\nα\n1\n\\alpha_{1}\nto\nα\n13\n\\alpha_{13}\n1\nCoefficients of cost function.\n(Values are all set to 1)\nβ\n1\n,\nβ\n2\n\\beta_{1},\\beta_{2}\n1, 1\nCoefficients of cost function.\nO\ni\nO_{i}\n{1,2,3}\nObservation of human velocity: “1” for low velocity, “2” for medium velocity, “3” for high velocity.\nVI-B\nSample trajectories\nIn this subsection, we examine the outcomes of applying the optimal task assignment policy under different initial conditions to demonstrate how a robot can collaborate with a human in shared environments to complete tasks while maintaining human safety. In our case study, three different tasks have been introduced to the POMDP framework with the following characteristics:\nS\n1\n=\n[\n0\n,\n0\n,\n1\n,\n1\n,\n3\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n1\n]\nS_{1}=\\begin{bmatrix}0,0,1,1,3,0,0,0,0,0,1\\end{bmatrix}\n(42)\nS\n2\n=\n[\n0\n,\n0\n,\n2\n,\n2\n,\n1\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n1\n]\nS_{2}=\\begin{bmatrix}0,0,2,2,1,0,0,0,0,0,1\\end{bmatrix}\n(43)\nS\n3\n=\n[\n0\n,\n0\n,\n2\n,\n2\n,\n2\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n1\n]\nS_{3}=\\begin{bmatrix}0,0,2,2,2,0,0,0,0,0,1\\end{bmatrix}\n(44)\nThese tasks have been submitted to our framework sequentially each with its own characteristics to evaluate its applicability to execute different tasks under different scenarios efficiently in a collaboration environment while ensuring human safety. The first task, indicated by\nS\n1\nS_{1}\nin\n42\n, is characterized by the following features: (1) it is a medium-priority task (\nS\n1\n​\n(\n3\n)\n=\n1\nS_{1}(3)=1\nor\nτ\np\n=\n1\n\\tau_{p}=1\n); (2) it requires one epoch to complete (\nS\n1\n​\n(\n4\n)\n=\n1\nS_{1}(4)=1\nor\nτ\nc\n=\n1\n\\tau_{c}=1\n); and (3) it must be executed collaboratively by both the robot and the human (\nS\n1\n​\n(\n5\n)\n=\n3\nS_{1}(5)=3\nor\nτ\nx\n=\n3\n\\tau_{x}=3\n). Then, after 7 epochs, another task, defined in Eq.\n43\n, is introduced. This task has the following characteristic:\nS\n1\n​\n(\n3\n)\n=\n2\nS_{1}(3)=2\n,\nS\n1\n​\n(\n4\n)\n=\n2\nS_{1}(4)=2\n,\nS\n1\n​\n(\n5\n)\n=\n1\nS_{1}(5)=1\nwhich indicates a high priority task (\nτ\np\n=\n2\n\\tau_{p}=2\n) with time duration of 2 epochs (\nτ\nc\n=\n2\n\\tau_{c}=2\n), it can be executed by human only (\nτ\nx\n=\n1\n\\tau_{x}=1\n). Finally, the third task is introduced at epoch 14 with the following features:\nS\n1\n​\n(\n3\n)\n=\n2\nS_{1}(3)=2\n,\nS\n1\n​\n(\n4\n)\n=\n2\nS_{1}(4)=2\n,\nS\n1\n​\n(\n5\n)\n=\n2\nS_{1}(5)=2\nwhich indicates a high priority task (\nτ\np\n=\n2\n\\tau_{p}=2\n) with time duration of 2 epochs (\nτ\nc\n=\n2\n\\tau_{c}=2\n), it can be executed by human only (\nτ\nx\n=\n2\n\\tau_{x}=2\n). The rest of the states are initially started with ”zero values” to represent the initial conditions of the human, the robot, and the task prior to execution. The only exception is the eleventh state, where\nS\n​\n(\n11\n)\n=\n1\nS(11)=1\n, indicating that the initial distance between the human and the robot is short. These initial states reflect that the task is initially unassigned (\nS\n​\n(\n6\n)\n=\n0\nS(6)=0\n), both the human and the robot begin with zero task progress and low priority activity levels (\nS\n​\n(\n7\n,\n8\n,\n9\n,\n10\n)\n=\n0\nS(7,8,9,10)=0\n), and human emotions are at the low level (\nS\n​\n(\n1\n,\n2\n)\n=\n0\nS(1,2)=0\n).\nSince human emotions (\ne\nm\ne_{m}\nand\ne\na\ne_{a}\n) cannot be directly measured, these states are inferred based on observable variables such as the human body position, joint angles, and angular velocities of the human joints in real-world scenarios. Upon observing these variables, the robot can assess the probability distribution of the relevant state variables. In our simulation experiments, the human velocity in the workspace serves as an observation. Three velocity levels (low, medium, and high) are used to infer the human emotional state. Given that there are two unobservable states, the belief state consists of nine possible states, as outlined in Table\nV\n. If the worker’s velocity, as determined by the sensors mounted on the worker joints, is small (\nO\n=\n1\nO=1\n), the system is most likely in State 1 (\ne\nm\n=\n0\ne_{m}=0\nand\ne\na\n=\n0\ne_{a}=0\n), with a probability of 80%. There is a smaller chance (10%) that the system is in State 2 (\ne\nm\n=\n0\ne_{m}=0\nand\ne\na\n=\n1\ne_{a}=1\n), or State 4 (\ne\nm\n=\n1\ne_{m}=1\nand\ne\na\n=\n0\ne_{a}=0\n), while the probabilities for State 3 and States 5 through 9 are zero, indicating that these states are not possible given the observation. When the velocity of the worker is observed to be medium (\nO\n=\n2\nO=2\n), the belief state is updated. This belief state indicates that the system is most likely in State 5 (\ne\nm\n=\n1\ne_{m}=1\nand\ne\na\n=\n1\ne_{a}=1\n), with a probability of 70%. There is a 10% chance that the system is in State 2 (\ne\nm\n=\n0\ne_{m}=0\nand\ne\na\n=\n1\ne_{a}=1\n), or State 4 (\ne\nm\n=\n1\ne_{m}=1\nand\ne\na\n=\n0\ne_{a}=0\n), and a 5% chance it is in State 6 (\ne\nm\n=\n1\ne_{m}=1\nand\ne\na\n=\n2\ne_{a}=2\n), or State 8 (\ne\nm\n=\n2\ne_{m}=2\nand\ne\na\n=\n1\ne_{a}=1\n). The probabilities for other states are zero, meaning these states are not likely given the observation\nO\n=\n2\nO=2\n. Finally, if the velocity of the worker is high (\nO\n=\n3\nO=3\n), the belief state is updated to reflect that the system is most likely in State 9 with a probability of 70%. There is a 10% chance that the system is in State 6 (\ne\nm\n=\n1\ne_{m}=1\nand\ne\na\n=\n2\ne_{a}=2\n) or State 8 (\ne\nm\n=\n2\ne_{m}=2\nand\ne\na\n=\n1\ne_{a}=1\n), and a 5% chance it is in State 3 (\ne\nm\n=\n0\ne_{m}=0\nand\ne\na\n=\n2\ne_{a}=2\n), or State 7 (\ne\nm\n=\n2\ne_{m}=2\nand\ne\na\n=\n0\ne_{a}=0\n). The probability of other states has remained zero.\nTABLE V:\nBelief States\nState Number\nState Value\nState 1\n[\n0\n0\n]\n\\begin{bmatrix}0&0\\end{bmatrix}\nState 2\n[\n0\n1\n]\n\\begin{bmatrix}0&1\\end{bmatrix}\nState 3\n[\n0\n2\n]\n\\begin{bmatrix}0&2\\end{bmatrix}\nState 4\n[\n1\n0\n]\n\\begin{bmatrix}1&0\\end{bmatrix}\nState 5\n[\n1\n1\n]\n\\begin{bmatrix}1&1\\end{bmatrix}\nState 6\n[\n1\n2\n]\n\\begin{bmatrix}1&2\\end{bmatrix}\nState 7\n[\n2\n0\n]\n\\begin{bmatrix}2&0\\end{bmatrix}\nState 8\n[\n2\n1\n]\n\\begin{bmatrix}2&1\\end{bmatrix}\nState 9\n[\n2\n2\n]\n\\begin{bmatrix}2&2\\end{bmatrix}\nThe results of our experiments are illustrated in the following figures (\n10\n-\n13\n). Figure.\n10\nillustrates the external inputs that define the characteristics of the active task, including task nature, task priority, task duration, and commitment status. As depicted in this figure, three active tasks have been assigned sequentially each with its own features as illustrated earlier. For instance, the first task begins with a medium priority. Upon completion, the task priority returns to zero before the initiation of the second task. Task 2 and task 3, characterized by a high priority, follows a similar pattern: the priority increases from zero to two during its execution and reverts to zero once completed. The same behavior is observed with the task priority; the figure begins with the expected time for each task completion which decreases at each epoch till the task is completed it goes back to zero. Task nature and task commitment figures are related to each other. The task nature tells how the task should be performed - by Robot only (\nτ\nx\n=\n0\n\\tau_{x}=0\n), human only (\nτ\nx\n=\n1\n\\tau_{x}=1\n), either one of them (\nτ\nx\n=\n2\n\\tau_{x}=2\n) or needs a collaboration between the two agent (\nτ\nx\n=\n3\n\\tau_{x}=3\n)- while the task commitment (\nτ\ny\n\\tau_{y}\n) tells about the current status of the task. Therefore, once the simulation starts, the task is committed directly according to its nature. This is explicitly illustrated in task nature and task commitment graphs. these graphs show that each of the three tasks start with the initial condition (uncommitted task (\nτ\ny\n=\n0\n\\tau_{y}=0\n)) and with time progression it reaches the task nature before going back again to the zero initial states after task completion.\nFigure.\n11\nillustrates the status of the human and robot activity during executing the tasks. Since the first task is assumed to be executed in a medium priority collaboration environment between the robot and the worker, both Human and robot activity priority has moved from the low-level (initial state) to the medium-level. then, once the task has been completed, their priorities went back to the low-level again. The same happens with the activity progress, which indicates that the progress of human and robot are aligned with each other since they are engaged in a collaborative task. Moreover, they are also aligned with the value of task duration. For task 2, since its task nature tells that it should be executed by human, only the human activity and human progress will be changed to align with the task priority and task duration. Since the third task could be assigned to either human or robot, the task here is only assigned to the robot. consequently, the priority and progress of only the robot activity is changed to align with the ones of the task while the the priority and progress of the human activity went back to the initial states.\nFigure.\n12\nshows the relationship between distance and collaboration. Initially, the distance between the human and robot is set at\nd\n=\n1\nd=1\n, indicating close proximity, which is maintained during the execution of the first task which requires a high collaboration between the robot and the worker. Upon task completion at epoch 4, the robot re-enters stop mode and the distance changes to\nd\n=\n3\nd=3\n. In this mode, since the robot is not operating, the distance is flexible, ranging from collision (\nd\n=\n0\nd=0\n) to maximum separation (\nd\n=\n3\nd=3\n) without impacting human safety. However, it is preferable to increase the distance at the end of each task to be used as an initial state for the next task to ensure safety. For task 2 and task 3, the task should be executed by either the robot or human, therefore, the distance between the two agents should be high to ensure the safety of the worker and smoothness of the process.\nFigure.\n13\nshows the sequence of the actions the robot achieved using the optimal policy to achieve its goal. The figure demonstrates how the robot assigned the task to both the robot and human at the same time, keeping the distance between them in the small circle to achieve high collaboration between the robot and the human. The robot starts by motivating the human since the task requires human involvement, and a human with low motivation can impact safety. When motivation reaches a good value (moderately motivated human), the algorithm requests the human to share the task with the robot. Upon completion of the first task, the distance is increased, where the robot and the robot is sent the stop mode. Once the second task starts, which requires the task to be done by human only, the robot starts by motivating the human again to ensure high value of human motivation level. once the robot is motivated, the task is assigned to him. Again, once the task is completed, the robot re-enters the stop mode. Finally, once the third task is assigned to the robot, the robot worked normally until the task is completed after 2 epochs. This process is designed to operate continuously and can theoretically run indefinitely as new tasks are introduced over time. Table\nVI\nsummarizes and describes the status of the state variables across the 20 epochs.\nTABLE VI:\nTrajectory Description\nEpoch\nState Variable Values\n1\n[\n0\n,\n0\n,\n1\n,\n1\n,\n3\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n1\n]\n\\begin{bmatrix}0,0,1,1,3,0,0,0,0,0,1\\end{bmatrix}\n2\n[\n1\n,\n1\n,\n1\n,\n1\n,\n3\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n1\n]\n\\begin{bmatrix}1,1,1,1,3,0,0,0,0,0,1\\end{bmatrix}\n3\n[\n1\n,\n1\n,\n1\n,\n1\n,\n3\n,\n3\n,\n1\n,\n1\n,\n1\n,\n1\n,\n1\n]\n\\begin{bmatrix}1,1,1,1,3,3,1,1,1,1,1\\end{bmatrix}\n4\n[\n1\n,\n1\n,\n1\n,\n1\n,\n3\n,\n3\n,\n1\n,\n0\n,\n1\n,\n0\n,\n2\n]\n\\begin{bmatrix}1,1,1,1,3,3,1,0,1,0,2\\end{bmatrix}\n5\n[\n1\n,\n1\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,0,0,0,0,0,0,0,0,3\\end{bmatrix}\n6\n[\n1\n,\n1\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,0,0,0,0,0,0,0,0,3\\end{bmatrix}\n7\n[\n1\n,\n1\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,0,0,0,0,0,0,0,0,3\\end{bmatrix}\n8\n[\n1\n,\n1\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,0,0,0,0,0,0,0,0,3\\end{bmatrix}\n9\n[\n1\n,\n1\n,\n2\n,\n2\n,\n1\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,2,2,1,0,0,0,0,0,3\\end{bmatrix}\n10\n[\n1\n,\n1\n,\n2\n,\n2\n,\n1\n,\n1\n,\n1\n,\n2\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,2,2,1,1,1,2,0,0,3\\end{bmatrix}\n11\n[\n1\n,\n1\n,\n2\n,\n2\n,\n1\n,\n1\n,\n1\n,\n0\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,2,2,1,1,1,0,0,0,3\\end{bmatrix}\n12\n[\n1\n,\n1\n,\n2\n,\n2\n,\n1\n,\n1\n,\n1\n,\n0\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,2,2,1,1,1,0,0,0,3\\end{bmatrix}\n13\n[\n1\n,\n1\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,0,0,0,0,0,0,0,0,3\\end{bmatrix}\n14\n[\n1\n,\n1\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,0,0,0,0,0,0,0,0,3\\end{bmatrix}\n15\n[\n1\n,\n1\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,0,0,0,0,0,0,0,0,3\\end{bmatrix}\n16\n[\n1\n,\n1\n,\n2\n,\n2\n,\n2\n,\n2\n,\n0\n,\n0\n,\n2\n,\n2\n,\n3\n]\n\\begin{bmatrix}1,1,2,2,2,2,0,0,2,2,3\\end{bmatrix}\n17\n[\n1\n,\n1\n,\n2\n,\n2\n,\n2\n,\n2\n,\n0\n,\n0\n,\n1\n,\n1\n,\n3\n]\n\\begin{bmatrix}1,1,2,2,2,2,0,0,1,1,3\\end{bmatrix}\n18\n[\n1\n,\n1\n,\n2\n,\n2\n,\n2\n,\n2\n,\n0\n,\n0\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,2,2,2,2,0,0,0,0,3\\end{bmatrix}\n19\n[\n1\n,\n1\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,0,0,0,0,0,0,0,0,3\\end{bmatrix}\n20\n[\n1\n,\n1\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n0\n,\n3\n]\n\\begin{bmatrix}1,1,0,0,0,0,0,0,0,0,3\\end{bmatrix}\nFigure 10:\nStatus of The Active Task.\nFigure 11:\nStatus of The Human and Robot Activity.\nFigure 12:\nStatus of the Distance between the Robot and Human.\nFigure 13:\nSequence of the actions.\nVII\nConclusion\nThis paper proposes a novel theoretical framework implementation for intent prediction in HRC environments utilizing stochastic modeling and control techniques. By leveraging probabilistic models and a bilateral intent prediction approach, the framework enhances the safety and transparency of collaborative robots, ensuring that they can anticipate human actions and adapt their behavior in real-time. Since most of the research is done either to detect the intention of the robot or the human co-worker, this model excels in considering the bilateral intent prediction approach for safety and transparency in collaborative robotics. Furthermore, the inclusion of both the robot’s state and the uncertain emotional states of the human in a unified hierarchical structure provides a comprehensive solution for dynamic human-robot interactions. Through a numerical simulation-based case study, the paper adeptly demonstrates the practical application of the proposed framework. It not only describes the execution of the framework, but also the consequences of parameter choices.\nReferences\n[1]\nM. Ahmadi, B. Wu, H. Lin, and U. Topcu\n(2018)\nPrivacy verification in pomdps via barrier certificates\n.\nIn\n2018 IEEE Conference on Decision and Control (CDC)\n,\npp. 5610–5615\n.\nCited by:\n§II\n.\n[2]\nS. L. Alaguero, A. Chirtoaca, D. Chrysostomou, and L. Nalpantidis\n(2023)\nCommunicating robot intentions: usability study of a socially-aware mobile robot\n.\nIn\n2023 IEEE International Conference on Imaging Systems and Techniques (IST)\n,\npp. 1–6\n.\nCited by:\nTABLE I\n,\n§II\n.\n[3]\nJ. Alves, T. M. Lima, and P. D. Gaspar\n(2023)\nIs industry 5.0 a human-centred approach? a systematic review\n.\nProcesses\n11\n(\n1\n),\npp. 193\n.\nCited by:\n§I\n.\n[4]\nA. Bauer, D. Wollherr, and M. Buss\n(2008)\nHuman–robot collaboration: a survey\n.\nInternational Journal of Humanoid Robotics\n5\n(\n01\n),\npp. 47–66\n.\nCited by:\n§I\n.\n[5]\nF. Broz, I. Nourbakhsh, and R. Simmons\n(2011)\nDesigning pomdp models of socially situated tasks\n.\nIn\n2011 RO-MAN\n,\npp. 39–46\n.\nCited by:\n§II\n.\n[6]\nJ. Cai, Z. Gao, Y. Guo, B. Wibranek, and S. Li\n(2024)\nFedhip: federated learning for privacy-preserving human intention prediction in human-robot collaborative assembly tasks\n.\nAdvanced Engineering Informatics\n60\n,\npp. 102411\n.\nCited by:\n§II\n.\n[7]\nG. Chandramowleeswaran, C. Choubey, S. Pendem, S. Pal, and A. Verma\n(2023)\nImplementation of human robot interaction with motion planning and control parameters with autonomous systems in industry 4.0\n.\nIn\n2023 Second International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)\n,\npp. 1805–1810\n.\nCited by:\n§II\n.\n[8]\nK. Chatterjee, M. Chmelik, R. Gupta, and A. Kanodia\n(2015)\nQualitative analysis of pomdps with temporal logic specifications for robotics applications\n.\nIn\n2015 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp. 325–330\n.\nCited by:\n§II\n.\n[9]\nA. De Luca and F. Flacco\n(2012)\nIntegrated control for phri: collision avoidance, detection, reaction and collaboration\n.\nIn\n2012 4th IEEE RAS & EMBS international conference on biomedical robotics and biomechatronics (BioRob)\n,\npp. 288–295\n.\nCited by:\n§II\n.\n[10]\nV. Dimova-Edeleva, O. S. Rivera, R. Laha, L. F. Figueredo, M. Zavaglia, and S. Haddadin\n(2023)\nError-related potentials in a virtual pick-and-place experiment: toward real-world shared-control\n.\nIn\n2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)\n,\npp. 1–7\n.\nCited by:\nTABLE I\n,\n§II\n.\n[11]\nJ. Ding and Y. Juang\n(2024)\nA smart assembly line design using human–robot collaborations with operator gesture recognition by decision fusion of deep learning channels of three image sensing modalities from rgb-d devices.\n.\nSensors & Materials\n36\n.\nCited by:\nTABLE I\n,\n§II\n.\n[12]\nY. Du, H. B. Amor, J. Jin, Q. Wang, and A. Ajoudani\n(2024)\nLearning-based multimodal control for a supernumerary robotic system in human-robot collaborative sorting\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\nTABLE I\n,\nTABLE I\n.\n[13]\nA. G. Feleke, L. Bi, and W. Fei\n(2021)\nEMG-based 3d hand motor intention prediction for information transfer from human to robot\n.\nSensors\n21\n(\n4\n),\npp. 1316\n.\nCited by:\n§II\n.\n[14]\nV. K. Guda, S. Mugisha, C. Chevallereau, and D. Chablat\n(2023)\nIntroduction of a cobot as intermittent haptic contact interfaces in virtual reality\n.\nIn\nInternational Conference on Human-Computer Interaction\n,\npp. 497–509\n.\nCited by:\n§I\n,\n§\nIV-F\n.\n[15]\nC. Huang, S. Andrist, A. Sauppé, and B. Mutlu\n(2015)\nUsing gaze patterns to predict task intent in collaboration\n.\nFrontiers in psychology\n6\n,\npp. 1049\n.\nCited by:\n§I\n.\n[16]\nM. J. Islam, M. Ho, and J. Sattar\n(2019)\nUnderstanding human motion and gestures for underwater human–robot collaboration\n.\nJournal of Field Robotics\n36\n(\n5\n),\npp. 851–873\n.\nCited by:\n§II\n.\n[17]\nE. M. Jean-Baptiste, P. Rotshtein, and M. Russell\n(2015)\nPOMDP based action planning and human error detection\n.\nIn\nArtificial Intelligence Applications and Innovations: 11th IFIP WG 12.5 International Conference, AIAI 2015, Bayonne, France, September 14-17, 2015, Proceedings 11\n,\npp. 250–265\n.\nCited by:\n§II\n.\n[18]\nN. D. Kahanowich and A. Sintov\n(2024)\nLearning human-arm reaching motion using a wearable device in human–robot collaboration\n.\nIEEE Access\n12\n,\npp. 24855–24865\n.\nCited by:\n§II\n.\n[19]\nA. Kamali Mohammadzadeh, C. L. Allen, and S. Masoud\n(2023)\nVR driven unsupervised classification for context aware human robot collaboration\n.\nIn\nInternational Conference on Flexible Automation and Intelligent Manufacturing\n,\npp. 3–11\n.\nCited by:\nTABLE II\n,\n§II\n.\n[20]\nM. Khatib, K. Al Khudir, and A. De Luca\n(2017)\nVisual coordination task for human-robot collaboration\n.\nIn\n2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)\n,\npp. 3762–3768\n.\nCited by:\n§II\n.\n[21]\nD. Lagamtzis, F. Schmidt, J. Seyler, T. Dang, and S. Schober\n(2023)\nExploiting spatio-temporal human-object relations using graph neural networks for human action recognition and 3d motion forecasting\n.\nIn\n2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp. 7832–7838\n.\nCited by:\nTABLE II\n,\n§II\n.\n[22]\nT. Landmann\n(2023)\nTowards an automated approach for safety risk mitigation in flexible use of collaborative robot applications with power and force limiting.\n.\nPh.D. Thesis\n,\nStellenbosch: Stellenbosch University\n.\nCited by:\n§II\n.\n[23]\nP. Lasota, S. Nikolaidis, and J. A. Shah\n(2013)\nDeveloping an adaptive robotic assistant for close proximity human-robot collaboration in space\n.\nIn\nAIAA Infotech@ Aerospace (I@ A) Conference\n,\npp. 4806\n.\nCited by:\n§II\n.\n[24]\nM. Lavit Nicora, P. Prajod, M. Mondellini, G. Tauro, R. Vertechy, E. André, and M. Malosio\n(2024)\nGaze detection as a social cue to initiate natural human-robot collaboration in an assembly task\n.\nFrontiers in Robotics and AI\n11\n,\npp. 1394379\n.\nCited by:\n§II\n.\n[25]\nR. Liu, R. Chen, A. Abuduweili, and C. Liu\n(2023)\nProactive human-robot co-assembly: leveraging human intention prediction and robust safe control\n.\nIn\n2023 IEEE Conference on Control Technology and Applications (CCTA)\n,\npp. 339–345\n.\nCited by:\n§I\n.\n[26]\nZ. Liu, X. Lu, W. Liu, W. Qi, and H. Su\n(2024)\nHuman-robot collaboration through a multi-scale graph convolution neural network with temporal attention\n.\nIEEE Robotics and Automation Letters\n9\n(\n3\n),\npp. 2248–2255\n.\nCited by:\nTABLE II\n.\n[27]\nJ. Marić, L. Petrović, and I. Marković\n(2023)\nHuman intention recognition in collaborative environments using rgb-d camera\n.\nIn\n2023 46th MIPRO ICT and Electronics Convention (MIPRO)\n,\npp. 350–355\n.\nCited by:\n§II\n.\n[28]\nP. Mathur\n(2023)\nProactive human-robot interaction using visuo-lingual transformers\n.\narXiv preprint\narXiv:2310.02506\n.\nExternal Links:\n2310.02506\nCited by:\nTABLE II\n.\n[29]\nM. Mavsar, J. Morimoto, and A. Ude\n(2023)\nGAN-based semi-supervised training of lstm nets for intention recognition in cooperative tasks\n.\nIEEE Robotics and Automation Letters\n9\n(\n1\n),\npp. 263–270\n.\nCited by:\nTABLE II\n.\n[30]\nG. Mirabella\n(2014)\nShould i stay or should i go? conceptual underpinnings of goal-directed actions\n.\nFrontiers in systems neuroscience\n8\n,\npp. 206\n.\nCited by:\n§II\n.\n[31]\nL. Munoz\n(2018)\nErgonomics in the industry 4.0: exoskeletons\n.\nJ Ergonomics\n8\n(\n1\n),\npp. e176\n.\nCited by:\n§I\n.\n[32]\nS. Nikolaidis, R. Ramakrishnan, K. Gu, and J. Shah\n(2015)\nEfficient model learning from joint-action demonstrations for human-robot collaborative tasks\n.\nIn\nProceedings of the tenth annual ACM/IEEE international conference on human-robot interaction\n,\npp. 189–196\n.\nCited by:\n§II\n.\n[33]\nL. Orlov Savko, Z. Qian, G. Gremillion, C. Neubauer, J. Canady, V. Unhelkar, and C. Neubauer\n(2024)\nRW4T dataset: data of human-robot behavior and cognitive states in simulated disaster response tasks\n.\nIn\nProceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction\n,\npp. 924–928\n.\nCited by:\n§II\n.\n[34]\nU. Othman and E. Yang\n(2023)\nHuman–robot collaborations in smart manufacturing environments: review and outlook\n.\nSensors\n23\n(\n12\n),\npp. 5663\n.\nCited by:\n§II\n.\n[35]\nR. Pandya, Z. Wang, Y. Nakahira, and C. Liu\n(2024)\nTowards proactive safe human-robot collaborations via data-efficient conditional behavior prediction\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp. 12956–12963\n.\nCited by:\nTABLE II\n.\n[36]\nS. Precup, S. Walunj, A. Gellert, C. Plociennik, J. Antony, C. Zamfirescu, and M. Ruskowski\n(2023)\nRecognising worker intentions by assembly step prediction\n.\nIn\n2023 IEEE 28th international conference on emerging technologies and factory automation (ETFA)\n,\npp. 1–8\n.\nCited by:\nTABLE II\n.\n[37]\nN. Rajabi, P. Khanna, S. U. D. Kanik, E. Yadollahi, M. Vasco, M. Björkman, C. Smith, and D. Kragic\n(2023)\nDetecting the intention of object handover in human-robot collaborations: an eeg study\n.\nIn\n2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)\n,\npp. 549–555\n.\nCited by:\nTABLE I\n.\n[38]\nH. Renz, M. Krämer, and T. Bertram\n(2023)\nUncertainty estimation for predictive collision avoidance in human-robot collaboration\n.\nIn\n2023 IEEE International Conference on Robotics and Biomimetics (ROBIO)\n,\npp. 1–6\n.\nCited by:\n§I\n.\n[39]\nZ. Saleem, F. Gustafsson, E. Furey, M. McAfee, and S. Huq\n(2024)\nA review of external sensors for human detection in a human robot collaborative environment\n.\nJournal of Intelligent Manufacturing\n,\npp. 1–25\n.\nCited by:\n§I\n.\n[40]\nA. R. Sawyer\n(2023)\nIncorporating novel sensors for reading human health state and motion intent into real-time computing systems\n.\nMissouri University of Science and Technology\n.\nCited by:\n§I\n.\n[41]\nS. Tian, X. Liang, and M. Zheng\n(2023)\nAn optimization-based human behavior modeling and prediction for human-robot collaborative disassembly\n.\nIn\n2023 American Control Conference (ACC)\n,\npp. 3356–3361\n.\nCited by:\nTABLE II\n.\n[42]\nY. Tung, M. B. Luebbers, A. Roncone, and B. Hayes\n(2024)\nWorkspace optimization techniques to improve prediction of human motion during human-robot collaboration\n.\nIn\nProceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction\n,\npp. 743–751\n.\nCited by:\nTABLE I\n.\n[43]\nA. Vysocky and P. Novak\n(2016)\nHuman-robot collaboration in industry\n.\nMM Science Journal\n9\n(\n2\n),\npp. 903–906\n.\nCited by:\n§I\n.\n[44]\nK. Wang, C. J. Lin, A. A. Tadesse, and B. H. Woldegiorgis\n(2023)\nModeling of human–robot collaboration for flexible assembly—a hidden semi-markov-based simulation approach\n.\nThe International Journal of Advanced Manufacturing Technology\n126\n(\n11\n),\npp. 5371–5389\n.\nCited by:\nTABLE II\n.\n[45]\nX. Wang and V. J. Santos\n(2023)\nGaze-based shared autonomy framework with real-time action primitive recognition for robot manipulators\n.\nIEEE Transactions on Neural Systems and Rehabilitation Engineering\n31\n,\npp. 4306–4317\n.\nCited by:\nTABLE I\n.\n[46]\nY. Wang, J. Qiu, H. Cheng, X. Hu, P. Xu, J. Hou, and H. Xie\n(2023)\nThe effect of transparency on human-exoskeleton interaction\n.\nIn\nInternational Conference on Human-Computer Interaction\n,\npp. 642–652\n.\nCited by:\n§I\n.\n[47]\nZ. Wang, K. Mülling, M. P. Deisenroth, H. Ben Amor, D. Vogt, B. Schölkopf, and J. Peters\n(2013)\nProbabilistic movement modeling for intention inference in human–robot interaction\n.\nThe International Journal of Robotics Research\n32\n(\n7\n),\npp. 841–858\n.\nCited by:\n§II\n.\n[48]\nY. Yan, H. Su, and Y. Jia\n(2023)\nModeling and analysis of human comfort in human–robot collaboration\n.\nBiomimetics\n8\n(\n6\n),\npp. 464\n.\nCited by:\n§I\n.\n[49]\nB. Yao, B. Yang, W. Xu, Z. Ji, Z. Zhou, and L. Wang\n(2024)\nVirtual data generation for human intention prediction based on digital modeling of human-robot collaboration\n.\nRobotics and Computer-Integrated Manufacturing\n87\n,\npp. 102714\n.\nCited by:\n§II\n.\n[50]\nA. Zacharaki, I. Kostavelis, and I. Dokas\n(2021)\nDecision making with stpa through markov decision process, a theoretic framework for safe human-robot collaboration\n.\nApplied Sciences\n11\n(\n11\n),\npp. 5212\n.\nCited by:\n§I\n.\n[51]\nR. Zhang, Q. Lv, J. Li, J. Bao, T. Liu, and S. Liu\n(2022)\nA reinforcement learning method for human-robot collaboration in assembly tasks\n.\nRobotics and Computer-Integrated Manufacturing\n73\n,\npp. 102227\n.\nCited by:\n§II\n.\n[52]\nX. Zhang and H. Lin\n(2019)\nPerformance guaranteed human-robot collaboration with pomdp supervisory control\n.\nRobotics and Computer-Integrated Manufacturing\n57\n,\npp. 59–72\n.\nCited by:\n§II\n.\n[53]\nX. Zhang, S. Tian, X. Liang, M. Zheng, and S. Behdad\n(2024)\nEarly prediction of human intention for human–robot collaboration using transformer network\n.\nJournal of Computing and Information Science in Engineering\n24\n(\n5\n).\nCited by:\nTABLE II\n.\n[54]\nZ. Zhang, Y. Ji, D. Tang, J. Chen, and C. Liu\n(2024)\nEnabling collaborative assembly between humans and robots using a digital twin system\n.\nRobotics and Computer-Integrated Manufacturing\n86\n,\npp. 102691\n.\nCited by:\nTABLE I\n.\n[55]\nW. Zheng, B. Wu, and H. Lin\n(2018)\nPomdp model learning for human robot collaboration\n.\nIn\n2018 IEEE Conference on Decision and Control (CDC)\n,\npp. 1156–1161\n.\nCited by:\n§II\n.\n[56]\nZ. Zhou, S. Wang, Z. Chen, M. Cai, H. Wang, Z. Li, and Z. Kan\n(2023)\nLocal observation based reactive temporal logic planning of human-robot systems\n.\nIEEE Transactions on Automation Science and Engineering\n.\nCited by:\nTABLE II\n.",
    "preview_text": "Collaborative robots, or cobots, are increasingly integrated into various industrial and service settings to work efficiently and safely alongside humans. However, for effective human-robot collaboration, robots must reason based on human factors such as motivation level and aggression level. This paper proposes an approach for decision-making in human-robot collaborative (HRC) environments utilizing stochastic modeling. By leveraging probabilistic models and control strategies, the proposed method aims to anticipate human actions and emotions, enabling cobots to adapt their behavior accordingly. So far, most of the research has been done to detect the intentions of human co-workers. This paper discusses the theoretical framework, implementation strategies, simulation results, and potential applications of the bilateral collaboration approach for safety and efficiency in collaborative robotics.\n\nStochastic Decision-Making Framework for Human-Robot Collaboration in Industrial Applications\nMuhammad Adel Yusuf,\nAli Nasir,\nZeashan Hameed Khan\nMuhammad Adel Yusuf is with the Department of Control and Instrumentation Engineering, King Fahd University of Petroleum and Minerals (KFUPM), Dhahran 31261, Saudi Arabia (e-mail: Muhammad.adel@ejust.edu.eg).Ali Nasir is with the Department of Control and Instrumentation Engineering, and the Interdisciplinary Research Center for Intelligent Manufacturing & Robotics, King Fahd University of Petroleum and Minerals (KFUPM), Dhahran 31261, Saudi Arabia (e-mail: ali.nasir@kfupm.edu.sa).Zeashan Hameed Khan is with the Interdisciplinary Research Center for Intelligent Manufacturing & Robotics, King Fahd University of Petroleum and Minerals (KFUPM), Dhahran 31261, Saudi Arabia (e-mail: Zeashan.khan@kfupm.edu.sa).\nAbstract\nCollaborative robots, or cobots, are increasingly integrated into various industrial and service settings to work efficiently and safely alongside humans. However, for effective human-robot collaboration, robots must reas",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "stochastic modeling",
        "human-robot collaboration",
        "industrial applications",
        "probabilistic models",
        "control strategies"
    ],
    "one_line_summary": "该论文提出了一种基于随机建模的人机协作决策框架，用于工业应用中的安全与效率提升。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T09:35:33Z",
    "created_at": "2026-01-27T15:53:18.749100",
    "updated_at": "2026-01-27T15:53:18.749107",
    "recommend": 0
}