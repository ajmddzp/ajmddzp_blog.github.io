{
  "id": "2601.11078v1",
  "title": "Visual Marker Search for Autonomous Drone Landing in Diverse Urban Environments",
  "authors": [
    "Jiaohong Yao",
    "Linfeng Liang",
    "Yao Deng",
    "Xi Zheng",
    "Richard Han",
    "Yuankai Qi"
  ],
  "abstract": "Marker-based landing is widely used in drone delivery and return-to-base systems for its simplicity and reliability. However, most approaches assume idealized landing site visibility and sensor performance, limiting robustness in complex urban settings. We present a simulation-based evaluation suite on the AirSim platform with systematically varied urban layouts, lighting, and weather to replicate realistic operational diversity. Using onboard camera sensors (RGB for marker detection and depth for obstacle avoidance), we benchmark two heuristic coverage patterns and a reinforcement learning-based agent, analyzing how exploration strategy and scene complexity affect success rate, path efficiency, and robustness. Results underscore the need to evaluate marker-based autonomous landing under diverse, sensor-relevant conditions to guide the development of reliable aerial navigation systems.",
  "url": "https://arxiv.org/abs/2601.11078v1",
  "html_url": "https://arxiv.org/html/2601.11078v1",
  "html_content": "Visual Marker Search for Autonomous Drone Landing in Diverse Urban Environments\nJiaohong Yao, Linfeng Liang, Yao Deng, Xi Zheng, Richard Han, Yuankai Qi\nAbstract\nMarker-based landing is widely used in drone delivery and return-to-base systems for its simplicity and reliability. However, most approaches assume idealized landing site visibility and sensor performance, limiting robustness in complex urban settings.\nWe present a simulation-based evaluation suite on the AirSim platform with systematically varied urban layouts, lighting, and weather to replicate realistic operational diversity.\nUsing onboard camera sensors—RGB for marker detection and depth for obstacle avoidance—we benchmark two heuristic coverage patterns and a reinforcement learning–based agent, analyzing how exploration strategy and scene complexity affect success rate, path efficiency, and robustness.\nResults underscore the need to evaluate marker-based autonomous landing under diverse, sensor-relevant conditions to guide the development of reliable aerial navigation systems.\nI\nIntroduction\nTask Overview\nForward Depth\nDownward Depth\nDownward Scene\nFigure 1\n:\nTask description:\nThe agent autonomously searches for the marker within a fixed range. The yellow line illustrates an example trajectory, while the three images show the agent’s observations: depth for obstacle avoidance and RGB for marker detection.\nVisual-marker-based landing offers a practical solution for autonomous drone return and delivery, combining operational simplicity with reliable pose estimation at low cost\n[\n1\n,\n2\n]\n. By detecting visual markers at predefined landing zones using onboard cameras, drones can execute precise autonomous landings without relying on external infrastructure\n[\n3\n]\n.\nMarker-based landing has demonstrated effectiveness in structured sites with reliable GPS, where global positioning guides drones to a predefined marker and visual localization finalizes descent\n[\n3\n]\n. Extending this capability to arbitrary urban locations—such as residential\nrooftops or courtyards— is far more challenging, as GPS signals often degrade due to occlusion, multipath reflection, and weather-related interference. While sensors such as IMUs and barometers can partially enhance pose estimation, they cannot resolve GPS drift, leading to reduced Extended Kalman Filter (EKF) accuracy, mapping inconsistencies, and unreliable navigation. Thus, robust landing in urban contexts requires active visual search and autonomous exploration guided by onboard sensors, substantially increasing task complexity.\nIn urban environments, marker-based landing must handle variability in lighting, textures, structural layouts, and marker placement\n[\n3\n,\n4\n]\n, which compromise perception and navigation given the sensing and computational limits of onboard hardware, as illustrated in Figure\n2\n. Lightweight platforms such as the Jetson Nano provide limited memory and processing, constraining real-time visual inference. While LiDAR, radar, and thermal sensors improve robustness under poor visibility, their cost, weight, and power demands hinder scalable deployment. In contrast, RGB and depth cameras offer a compact, low-power alternative aligned with practical constraints. Robust evaluation therefore requires datasets capturing diverse environments and realistic drone–environment interactions to systematically assess their influence on search and landing performance.\nFigure 2\n:\nDrone System\nTo address these challenges, this work emphasizes the importance of constructing diverse, interactive simulation datasets to improve system robustness and generalization. Rather than relying solely on traditional synthetic data augmentation\n[\n5\n]\nor generating images for detection ability improvement\n[\n6\n,\n7\n,\n8\n]\n, we leverage the Unreal Engine 4\n[\n9\n]\nand Microsoft AirSim platform\n[\n10\n]\n, a high-fidelity simulator that facilitates dynamic interaction between drones and simulated environments. We manually select a wide range of urban landing sites with varying geometry, texture, contextual features, and environmental conditions—including diverse time-of-day and weather scenarios. At each simulated location, we strategically place markers and define corresponding drone initial positions, enabling drones to actively explore and locate landing targets.\nUsing this curated dataset, we evaluate three navigation strategies for autonomous marker search and landing. Experiments reveal clear differences in robustness and generalization, underscoring the impact of scenario diversity and environmental dynamics on system reliability. In summary, the main contributions are as follows:\n•\nA simulation-based evaluation suite with systematically varied scene attributes, including urban layouts, lighting, and weather.\n•\nBenchmarks three navigation strategies across heterogeneous scenarios, providing insights into the relationship between exploration strategy, scene complexity, and task performance.\nII\nRelated Work\nVisual-marker-based landing approaches have become prevalent in drone delivery systems due to their cost-effectiveness and high localization precision\n[\n11\n]\n. Commercial systems, such as those used by Amazon Prime Air\n[\n12\n]\nand Zipline\n[\n13\n]\n, employ GPS to approach predefined marker-equipped sites, where visual algorithms guide the final descent\n[\n11\n]\n. This paradigm, however, restricts deployment to known locations with reliable GPS\n[\n3\n]\n. To address this limitation, we propose a framework in which drones autonomously search for visual markers in previously unseen environments, enhancing adaptability and operational reach.\nIn the academic context, considerable effort has enhanced marker detection accuracy and robustness under varying visual conditions. For instance, Lee\n[\n8\n]\nhas optimized marker designs (e.g., embedded ArUco patterns) to extend detection range and improve pose estimation accuracy. Deep learning has also been applied; Truong et al.\n[\n14\n]\ncombined motion-deblurring with object detection to improve performance under blur and low light. However, these studies typically assume static markers placed in controlled, simplified environments, largely overlooking complex and dynamic real-world conditions. Furthermore, they tend to approach marker search implicitly as a straightforward detection task, without addressing the broader, system-level challenge of autonomously locating markers. To bridge these gaps, we introduce an interactive, richly varied simulation dataset, explicitly modeling dynamic drone-environment interactions, aligning research with the practical complexities of real-world drone delivery tasks.\nIII\nTask\nAs shown in Figure\n1\n, this task considers the problem of visually locating a small landing marker within a predefined circular region, without relying on GPS or prior map information. The drone is initialized at a random 6-DoF pose\nP\n=\n[\nx\n,\ny\n,\nz\n,\nyaw\n,\npitch\n,\nroll\n]\nP=[x,y,z,\\text{yaw},\\text{pitch},\\text{roll}]\n, while the marker is placed at an unknown\nposition within the environment. At each timestep\nt\nt\n, the agent receives onboard observations\nV\nt\n=\n{\nI\nt\nD\n,\nD\nt\nD\n,\nD\nt\nF\n}\nV_{t}=\\{I_{t}^{D},\\;D_{t}^{D},\\;D_{t}^{F}\\}\n, where\nI\nt\nD\nI_{t}^{D}\nand\nD\nt\nD\nD_{t}^{D}\ndenote the downward-facing RGB and depth images used for marker detection, and\nD\nt\nF\nD_{t}^{F}\nis a forward-facing depth image used for obstacle avoidance. The agent follows a fixed flying pattern and altitude adjustment with a step size of 5 meters, or selects actions from a discrete set, including forward movement with a step size of 2 meters and rotation of 90 degrees. A navigation episode is regarded as successful if the estimated marker position lies within 2 meters of its ground-truth location. This formulation reflects realistic drone deployment scenarios and introduces challenges such as partial observability, limited visual context, environmental variability, and the small size of the marker for detection.\nIV\nDataset\nIV-A\nDataset Description\nPostSoviet sample 1\nPostSoviet sample 2\nModernCity sample 1\nModernCity sample 2\nUrbanDistrict sample 1\nUrbanDistrict sample 2\nFigure 3\n:\nExamples of Marker Placement Across Environments (Markers Highlighted in Red)\nTo evaluate the generalization capabilities of vision-based marker search and landing methods, we construct a simulation-based dataset using Microsoft AirSim\n[\n10\n]\n, a high-fidelity platform built on Unreal Engine 4\n[\n9\n]\nthat supports realistic urban environments, dynamic lighting, weather variation, and interactive drone control—factors critical for robust visual perception.\nThe dataset is built using three visually and structurally diverse urban maps\n[\n15\n]\n— ModernCity, PostSoviet, and UrbanDistrict — chosen to reflect diverse real-world delivery scenarios. ModernCity features modern residential buildings, open plazas, and landscaped areas. PostSoviet contains tall buildings interspersed with vegetation, simulating environments with dense vertical structures or natural obstructions. UrbanDistrict consists of low-rise, irregularly arranged buildings and narrow passages, capturing the visual and spatial complexity of compact city regions. Together, these environments provide diverse layouts, textures, and contextual variability for evaluating model generalization.\nWithin each environment, we manually construct marker–drone configurations to simulate realistic and non-trivial search tasks. Markers are placed at physically safe landing locations with diverse surrounding contexts, including variations in nearby objects, surface textures, and occlusions. Drone initializations are selected to avoid direct visibility of the marker, ensuring that successful detection requires active exploration rather than incidental exposure. To introduce perceptual diversity, we vary the time of day and weather conditions across scenarios, affecting global illumination, contrast, and visibility through changes in lighting, fog, and dust. In total, we generate 966 episodes across three environments: 102 in\nModernCity\n, 240 in\nPostSoviet\n, and 624 in\nUrbanDistrict\n. These are derived from 161 unique marker–drone combinations—comprising 38 distinct marker locations with up to five unique drone initializations each—augmented by variations in time of day, weather type, and severity to produce multiple scenario variants. This design captures diverse spatial and perceptual conditions representative of real-world urban navigation and landing challenges. Figure\n3\nshows examples of marker placements across diverse urban environments under varied lighting (daytime, nighttime, glare, shadow) and visibility (fog, dust, occlusion) conditions. Markers are situated in cluttered, object-rich scenes, highlighting the dataset’s realism and its suitability for evaluating robust marker-based navigation.\nUnlike prior datasets\n[\n16\n,\n17\n,\n18\n]\nthat focus on static, easily visible landing sites for detection, our scenarios incorporate varied scene geometry, diverse visual contexts, and dynamic drone–environment interactions, creating realistic search complexity. This diversity enables robust evaluation of navigation strategies and systematic analysis of environmental factors (Section\nVI\n).\nIV-B\nDataset Analysis\nDrone Height Distribution\nTime Distribution\nWeather Type Distribution\nWeather Severity Distribution\nFigure 4\n:\nParameter Distribution\nThe three simulated environments vary in spatial scale and layout—ModernCity (54 × 49 m), PostSoviet (59 m × 63 m), and UrbanDistrict (137 × 108 m), introducing differences in openness, openness, and structural density that shape the search dynamics. Figure\n4\nsummarizes the distribution of key scenario parameters contributing to environmental diversity. Drone heights range from 10 to over 30m, with most concentrated between 15–25m, reflecting structural differences across maps and dataset diversity. The time-of-day distribution is bimodal (peaks near 0.5 and 0.85), covering a range from midday to late afternoon. Weather types are evenly distributed among sunny, foggy, and dusty scenes (\n∼\n33\n%\n\\sim 33\\%\neach), while severity levels are moderate (0.2–0.4), introducing perceptual challenges without hindering flight or detection. These variations provide a controlled yet diverse evaluation space for sensor-based navigation.\nV\nMethod\nWe compare three navigation strategies: two heuristic baselines—Spiral\n[\n19\n]\nand Zigzag\n[\n20\n]\n—and a learning-based approach, E2E-RL\n[\n21\n]\n. All operate without external localization, relying solely on onboard sensors. Navigation terminates either upon successful marker detection in the downward-facing RGB image by a pretrained detector—used only at evaluation to keep policy learning independent of detector-specific biases—or when a method-specific stopping rule is met: for Spiral and Zigzag, completion of the predefined trajectory or a collision; for E2E-RL, exhausting the step budget, colliding, violating the search boundary, or exhibiting sustained non-progressive behavior.\nSpiral\n[\n19\n]\nfollows a predefined outward-expanding pattern centered at the drone’s initial position, incrementally sweeping a circular region in the horizontal plane. Both 2D and 3D variants share the same fixed step-size trajectory at constant altitude; the 3D version additionally ascends only when necessary to avoid obstacles and returns to the original height afterward.\nZigzag\n[\n20\n]\ntraverses the environment using a structured back-and-forth sweep along one axis, interleaved with periodic shifts along the perpendicular axis. Like Spiral, waypoints are computed in advance with fixed spatial intervals, and 3D variates can adjust altitude temporarily in response to obstacles.\nE2E-RL\n, which follows the design principles of ground object-goal navigation agents\n[\n21\n]\n, is adapted for aerial visual marker search in complex urban environments. The agent learns an adaptive exploration policy from real-time onboard observations, receiving each timestep a front-view depth image and relative positional input, and selecting from a discrete 2D action set (move forward, turn left/right). Depth observations encode geometric structure, obstacle size and shape, relative distances, and spatial relationships—enabling the policy to infer promising directions for maximizing coverage and avoiding obstacles. The policy is optimized via reinforcement learning with a reward function that encourages coverage efficiency and penalizes collisions and unnecessary rotations, supporting perception-driven navigation without predefined trajectories or maps.\nVI\nExperiment\nVI-A\nExperiment Setup\nWe evaluate these navigation strategies over 966 simulated episodes with varied drone initializations, marker placements, lighting, and weather. The search area is a circle with a radius of 30 meters,\ncentered at the drone’s start position. All methods use identical sensor configurations, with 640\n×\n\\times\n480 depth images for obstacle avoidance and downward-facing RGB images for marker detection. During evaluation, marker detection is performed by a YOLOv11\n[\n22\n]\nmodel fine-tuned on domain-specific aerial imagery under varied altitudes and conditions, using a confidence threshold of 0.8. The detector is excluded from training to prevent the E2E-RL agent from exploiting detector-specific triggers instead of learning genuine exploration strategies. The E2E-RL policy is trained with PPO\n[\n23\n]\nand curriculum learning\n[\n24\n]\nentirely in a lightweight surrogate environment that preserves key task properties—partial observability, occlusions, and 3D obstacle avoidance—using only 64\n×\n\\times\n64 front-view depth images to maximize coverage efficiency while avoiding collisions.\nVI-B\nEvalaution Metrics\nTo evaluate navigation performance, we adopt five metrics commonly used in embodied visual navigation tasks\n[\n25\n,\n26\n,\n27\n]\n. Success Rate (SR) measures the proportion of episodes in which the agent stops within 2 meters of the marker’s true 3D location. Navigation Error (NE) calculates the Euclidean distance between the agent’s final stopping position and the ground-truth marker position. Success weighted by Path Length (SPL) reflects trajectory efficiency by weighting SR with the ratio of the shortest possible path length to the actual path length taken by the drone. To evaluate additional aspects of robustness, we further report Collision Rate (CR), defined as the fraction of episodes where the agent collides with obstacles, and False Detection Rate (FD), which quantifies the fraction of episodes where the detector incorrectly signals a successful marker detection but the reported position lies more than 2 meters away from the actual marker location.\nWhile metric definitions follow standard conventions, we introduce practical adjustments. For SR and NE, the agent’s final position is defined as the 3D location of the first detected marker, assuming reliable delivery via position control once detection occurs\n[\n3\n]\n. SPL and CR are instead computed over the exploration trajectory up to detection, with the shortest-path length in SPL approximated by the straight-line distance in the\nx\n​\ny\nxy\n-plane to treat vertical movement consistently. This separation ensures fair comparison of navigation behavior without conflating it with post-detection execution.\nVI-C\nResults and Analysis\nVI-C\n1\nModel Performance Analysis\nTABLE I\n:\nPerformance across methods and maps\nMethod\nMap\nSR/%\n↑\n\\uparrow\nNE/m\n↓\n\\downarrow\nSPL/%\n↑\n\\uparrow\nCR/%\n↓\n\\downarrow\nFD/%\n↓\n\\downarrow\nSpiral-2D\n[\n19\n]\nModernCity\n61.76\n13.20\n5.02\n22.55\n3.93\nUrbanDistrict\n60.90\n17.41\n4.73\n0.00\n8.18\nPostSoviet\n36.67\n20.46\n2.82\n47.50\n0.00\nAvg\n54.97\n17.72\n4.29\n14.18\n5.70\nZigzag-2D\n[\n20\n]\nModernCity\n37.25\n22.34\n2.13\n47.06\n3.93\nUrbanDistrict\n62.82\n16.13\n3.51\n0.00\n3.05\nPostSoviet\n32.92\n21.74\n1.90\n50.00\n0.00\nAvg\n52.69\n18.18\n2.96\n17.39\n2.39\nSpiral-3D\nModernCity\n63.73\n14.51\n5.17\n14.70\n9.80\nUrbanDistrict\n60.90\n17.41\n4.73\n0.00\n8.18\nPostSoviet\n57.92\n17.16\n4.69\n0.00\n1.66\nAvg\n60.63\n17.03\n4.79\n2.37\n6.80\nZigzag-3D\nModernCity\n57.84\n16.76\n3.35\n5.88\n3.92\nUrbanDistrict\n62.82\n16.13\n3.51\n0.00\n3.05\nPostSoviet\n47.92\n20.82\n2.92\n0.00\n0.83\nAvg\n58.66\n17.38\n3.34\n1.23\n2.87\nE2E-RL\n[\n21\n]\nModernCity\n18.63\n30.51\n9.87\n0.00\n7.84\nUrbanDistrict\n30.13\n26.37\n19.22\n0.00\n5.61\nPostSoviet\n17.08\n31.66\n11.95\n0.00\n9.17\nAvg\n24.38\n28.27\n15.49\n0.00\n6.53\nTable\nI\ncompares the five navigation strategies across multiple metrics. The heuristic baselines, Spiral-2D and Zigzag-2D show an acceptable success rate (SR) but high collision rates (CR), where the improved versions Spiral-3D and Zigzag-3D achieve a higher SR and lower navigation errors (NE) overall, benefiting from their predefined coverage patterns that ensure systematic spatial visibility. Spiral slightly outperforms Zigzag in SR, while Zigzag attains marginally lower false detection rates (FD), likely due to denser local coverage. In contrast, E2E-RL shows notably lower CR and higher SPL, reflecting more direct and efficient trajectories, which is valuable in real-world settings, where energy, time, and airspace constraints limit exhaustive search. This advantage arises from its use of real-time depth observations, which encode geometric structure, obstacle size and shape, relative distances, and spatial relationships, allowing the policy to infer promising directions for coverage and obstacle avoidance without predefined paths.\nAlthough its SR and NE are suboptimal, partly due to less exhaustive coverage and constrained vertical mobility, the consistently higher SPL and lowest CR suggest that E2E-RL exploits the environment intelligently, highlighting its potential as an adaptive alternative to deterministic strategies.\nAs shown in Table\nI\n, environmental layout significantly influences the effectiveness of each navigation strategy. In UrbanDistrict, the flat terrain and dense arrangement of low-rise, irregular buildings result in minimal vertical occlusion but increased horizontal clutter. This configuration favors coverage-based methods—Spiral and Zigzag—whose sweeping aligns well with surface-level visibility, yielding high SR and low NE. PostSoviet, by contrast, introduces greater vertical complexity, with tall structures and vegetation creating obstacles, occlusions and shadowed regions. These features impair line-of-sight and contribute to reduced SR and high CR. ModernCity offers a semi-structured layout, with mid-rise buildings enclosing an open courtyard. This predictable geometry supports consistent performance across methods, though Spiral’s uniform scanning may over-trigger detections in visually homogeneous regions, explaining its higher FD rate. Overall, the results highlight how obstacle density, spatial organization, and visual structure affect marker search performance.\nVI-C\n2\nEnvironmental Factors Analysis\nTABLE II\n:\nEnvironmental Correlates of Navigation Outcomes. This table summarizes average scene attributes associated with different navigation outcomes (Success, Failure, or False Detection).\nHeight (m)\nrefers to the drone’s initial flight altitude.\nTime\ndenotes the normalized time-of-day parameter.\nSunny (%)\nrepresents the proportion of episodes conducted under clear weather conditions.\nSeverity (%)\ncaptures the intensity of visual degradation in non-sunny environments (i.e., fog or dust).\nMethod\nType\nHeight/m\nTime\nSunny/%\nSeverity/%\nSpiral-3D\nSuccess\n19.01\n0.67\n33.00\n18.53\nFail\n22.82\n0.69\n33.00\n18.44\nFD\n18.72\n0.67\n38.00\n16.65\nZigzag-3D\nSuccess\n18.76\n0.68\n34.00\n18.13\nFail\n22.60\n0.68\n33.00\n18.65\nFD\n19.24\n0.80\n28.00\n19.98\nE2E-RL\n[\n21\n]\nSuccess\n18.77\n0.67\n35.08\n17.83\nFail\n20.86\n0.69\n33.08\n18.50\nFD\n19.89\n0.65\n29.23\n19.25\nTable\nII\nhighlights how spatial and perceptual factors such as altitude, lighting, and visibility influence marker detection stability and overall search performance.\nDrone height\nstrongly correlates with navigation success. For all methods, failed episodes occur at significantly higher initial altitudes—exceeding 20.8m for E2E-RL and 22.6m for the heuristic baselines—likely due to diminished marker size and reduced marker visibility at elevated viewpoints. In contrast, both successful and false detection (FD) cases concentrate around lower altitudes (18.7–19.9m), suggesting that FD errors are not simply due to limited visibility, but rather arise even when the marker is within view.\nTime of day\n, approximated by the normalized time parameter, is largely consistent across across success, failure, and false detection episodes for most methods, indicating that general lighting conditions are not a primary factor affecting success or false detection. A notable exception is Zigzag’s FD, which show a higher average time value—potentially reflecting more challenging illumination such as dark or shadow.\nWeather conditions\n, measured by visibility severity and sunny ratio, show modest but method-dependent effects. For Zigzag and E2E-RL, FD are more prevalent under lower visibility (i.e., higher severity, reduced sunny ratio), suggesting increased perceptual ambiguity in degraded scenes. In contrast, Spiral exhibits more false positives under clearer conditions, potentially due to its uniform coverage and frequent angle changes, encountering sunlit textures that resemble the marker. This contrast highlights divergent failure modes: while learning-based methods are more vulnerable to visual degradation, coverage-based approaches may over-trigger under high-visibility settings due to less selective filtering.\nVII\nConclusion\nIn this work, we develop a new simulated platform for evaluating visual marker search, navigation, and landing strategies under realistic urban conditions. Through systematic variation of layout, lighting, and weather, we identify key factors affecting method performance and generalization. This platform offers a controlled testbed for advancing robust aerial autonomy, with future work targeting improved policy adaptability via 3D exploration and richer visual cues such as RGB appearance.\nReferences\n[1]\nSkyy Network. (2025) Skyy network official website. Accessed: Aug. 31, 2025. [Online]. Available:\nhttps://skyy.network/\n[2]\nMeituan, “Meituan drones released its fourth-generation new model and showcased a brand-new urban low-altitude logistics solution,” [Online]. Available:\nhttps://www.meituan.com/news/NN230706019014042\n, 2023, accessed: Aug. 31, 2025.\n[3]\nS. Schroder, Y. Deng, A. James, A. Seth, K. Morton, S. Mukhopadhyay, R. Han, and X. Zheng, “Towards robust autonomous landing systems: Iterative solutions and key lessons learned,” in\n2025 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks-Supplemental Volume (DSN-S)\n. IEEE, 2025, pp. 167–173.\n[4]\nL. Liang, Y. Deng, K. Morton, V. Kallinen, A. James, A. Seth, E. Kuantama, S. Mukhopadhyay, R. Han, and X. Zheng, “Garl: Genetic algorithm-augmented reinforcement learning to detect violations in marker-based autonomous landing systems,” in\n2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)\n. IEEE Computer Society, 2025, pp. 613–613.\n[5]\nF. Dadboud, H. Azad, V. Mehta, M. Bolic, and I. Mantegh, “Drift: Autonomous drone dataset with integrated real and synthetic data, flexible views, and transformed domains,” in\n2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)\n. IEEE, 2025, pp. 6900–6910.\n[6]\nS. Raxit, S. B. Singh, and A. A. R. Newaz, “Yolotag: Vision-based robust uav navigation with fiducial markers,” in\n2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)\n. IEEE, 2024, pp. 311–316.\n[7]\nL. B. Junior, G. S. Berger, A. O. Júnior, J. Braun, M. A. Wehrmeister, M. F. Pinto, and J. Lima, “A comparison of fiducial markers pose estimation for uavs indoor precision landing,” in\nInternational Conference on Optimization, Learning Algorithms and Applications\n. Springer, 2023, pp. 18–33.\n[8]\nJ. Lee, S. Y. Choi, and T. Bretl, “The use of multi-scale fiducial markers to aid takeoff and landing navigation by rotorcraft,” in\nAIAA SCITECH 2024 Forum\n, 2024, p. 0338.\n[9]\nEpic Games, “Unreal engine 4 documentation,” [Online]. Available:\nhttps://dev.epicgames.com/documentation/en-us/unreal-engine/get-started-with-ue4?application_version=4.27\n, 2025, accessed: Aug. 31, 2025.\n[10]\nS. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity simulation for autonomous vehicles,” [Online]. Available:\nhttps://microsoft.github.io/AirSim/\n, 2017, accessed: Aug. 31, 2025.\n[11]\nJ. Springer, G. Þ. Guðmundsson, and M. Kyas, “A precision drone landing system using visual and ir fiducial markers and a multi-payload camera,”\narXiv preprint arXiv:2403.03806\n, 2024.\n[12]\nL. Matsakis, “Amazon’s drone delivery dream is crashing,” [Online]. Available:\nhttps://www.wired.com/story/crashes-and-layoffs-plague-amazons-drone-delivery-pilot/\n, 2023, accessed: Aug. 31, 2025.\n[13]\nZipline International Inc., “Zipline delivery,” [Online]. Available:\nhttps://www.zipline.com/\n, 2025, accessed: Aug. 31, 2025.\n[14]\nN. Q. Truong, Y. W. Lee, M. Owais, D. T. Nguyen, G. Batchuluun, T. D. Pham, and K. R. Park, “Slimdeblurgan-based motion deblurring and marker detection for autonomous drone landing,”\nSensors\n, vol. 20, no. 14, p. 3918, 2020.\n[15]\nS. Liu, H. Zhang, Y. Qi, P. Wang, Y. Zhang, and Q. Wu, “Aerialvln: Vision-and-language navigation for uavs,” in\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, 2023, pp. 15 384–15 394.\n[16]\nN. Q. Truong, P. H. Nguyen, S. H. Nam, and K. R. Park, “Deep learning-based super-resolution reconstruction and marker detection for drone landing,”\nIEEE Access\n, vol. 7, pp. 61 639–61 655, 2019.\n[17]\nP. H. Nguyen, M. Arsalan, J. H. Koo, R. A. Naqvi, N. Q. Truong, and K. R. Park, “Lightdenseyolo: A fast and accurate marker tracker for autonomous uav landing by visible light camera sensor on drone,”\nSensors\n, vol. 18, no. 6, p. 1703, 2018.\n[18]\nS. Kyristsis, A. Antonopoulos, T. Chanialakis, E. Stefanakis, C. Linardos, A. Tripolitsiotis, and P. Partsinevelos, “Towards autonomous modular uav missions: The detection, geo-location and landing paradigm,”\nSensors\n, vol. 16, no. 11, p. 1844, 2016.\n[19]\nU. Cao-Ky-Long and N. K. Toan, “Emergency uav delivery framework: A hybrid approach to gps navigation and visual landmark detection,”\nInternational Journal of Mechanical Engineering and Robotics Research\n, vol. 14, no. 4, 2025.\n[20]\nJ. Liu, D. Zou, X. Nan, X. Xia, and Z. Zhao, “Path planning algorithm for multi-drone collaborative search based on points of interest,” in\nInternational Conference on Autonomous Unmanned Systems\n. Springer, 2023, pp. 504–513.\n[21]\nK. Yadav, J. Krantz, R. Ramrakhya, S. K. Ramakrishnan, J. Yang, A. Wang, J. Turner, A. Gokaslan, V.-P. Berges, R. Mootaghi, O. Maksymets, A. X. Chang, M. Savva, A. Clegg, D. S. Chaplot, and D. Batra, “abitat challenge 2023,”\nhttps://aihabitat.org/challenge/2023/\n, 2023.\n[22]\nR. Khanam and M. Hussain, “Yolov11: An overview of the key architectural enhancements,”\narXiv preprint arXiv:2410.17725\n, 2024.\n[23]\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,”\narXiv preprint arXiv:1707.06347\n, 2017. [Online]. Available:\nhttps://arxiv.org/abs/1707.06347\n[24]\nY. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,” in\nProceedings of the 26th annual international conference on machine learning\n, 2009, pp. 41–48.\n[25]\nJ. Zhang, L. Dai, F. Meng, Q. Fan, X. Chen, K. Xu, and H. Wang, “3d-aware object goal navigation via simultaneous exploration and identification,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2023, pp. 6672–6682.\n[26]\nN. Xu, W. Wang, R. Yang, M. Qin, Z. Lin, W. Song, C. Zhang, J. Gu, and C. Li, “Aligning knowledge graph with visual perception for object-goal navigation,” in\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n. IEEE, 2024, pp. 5214–5220.\n[27]\nR. Dang, L. Wang, Z. He, S. Su, J. Tang, C. Liu, and Q. Chen, “Search for or navigate to? dual adaptive thinking for object navigation,” in\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, 2023, pp. 8250–8259.",
  "preview_text": "Marker-based landing is widely used in drone delivery and return-to-base systems for its simplicity and reliability. However, most approaches assume idealized landing site visibility and sensor performance, limiting robustness in complex urban settings. We present a simulation-based evaluation suite on the AirSim platform with systematically varied urban layouts, lighting, and weather to replicate realistic operational diversity. Using onboard camera sensors (RGB for marker detection and depth for obstacle avoidance), we benchmark two heuristic coverage patterns and a reinforcement learning-based agent, analyzing how exploration strategy and scene complexity affect success rate, path efficiency, and robustness. Results underscore the need to evaluate marker-based autonomous landing under diverse, sensor-relevant conditions to guide the development of reliable aerial navigation systems.\n\nVisual Marker Search for Autonomous Drone Landing in Diverse Urban Environments\nJiaohong Yao, Linfeng Liang, Yao Deng, Xi Zheng, Richard Han, Yuankai Qi\nAbstract\nMarker-based landing is widely used in drone delivery and return-to-base systems for its simplicity and reliability. However, most approaches assume idealized landing site visibility and sensor performance, limiting robustness in complex urban settings.\nWe present a simulation-based evaluation suite on the AirSim platform with systematically varied urban layouts, lighting, and weather to replicate realistic operational diversity.\nUsing onboard camera sensors—RGB for marker detection and depth for obstacle avoidance—we benchmark two heuristic coverage patterns and a reinforcement learning–based agent, analyzing how exploration strategy and scene complexity affect success rate, path efficiency, and robustness.\nResults underscore the need to evaluate marker-based autonomous landing under diverse, sensor-relevant conditions to guide the development of reliable aerial navigation systems.\nI\nIntroduction\nTask Overview\nForward Depth",
  "is_relevant": true,
  "relevance_score": 4.0,
  "extracted_keywords": [
    "Reinforcement Learning"
  ],
  "one_line_summary": "这篇论文通过模拟评估，比较了启发式覆盖模式和基于强化学习的智能体在无人机自主着陆中的性能，强调在多样化城市环境中评估着陆系统的重要性。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-16T08:24:23Z",
  "created_at": "2026-01-20T17:49:59.286162",
  "updated_at": "2026-01-20T17:49:59.286171"
}