{
  "id": "2601.11421v1",
  "title": "The Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents",
  "authors": [
    "Ziyu Wang",
    "Chenyuan Liu",
    "Yushun Xiang",
    "Runhao Zhang",
    "Qingbo Hao",
    "Hongliang Lu",
    "Houyu Chen",
    "Zhizhong Feng",
    "Kaiyue Zheng",
    "Dehao Ye",
    "Xianchao Zeng",
    "Xinyu Zhou",
    "Boran Wen",
    "Jiaxin Li",
    "Mingyu Zhang",
    "Kecheng Zheng",
    "Qian Zhu",
    "Ran Cheng",
    "Yong-Lu Li"
  ],
  "abstract": "Recently, with the rapid development of robot learning and imitation learning, numerous datasets and methods have emerged. However, these datasets and their task designs often lack systematic consideration and principles. This raises important questions: Do the current datasets and task designs truly advance the capabilities of robotic agents? Do evaluations on a few common tasks accurately reflect the differentiated performance of various methods proposed by different teams and evaluated on different tasks? To address these issues, we introduce the Great March 100 (\\textbf{GM-100}) as the first step towards a robot learning Olympics. GM-100 consists of 100 carefully designed tasks that cover a wide range of interactions and long-tail behaviors, aiming to provide a diverse and challenging set of tasks to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs. These tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from human-object interaction primitives and object affordances. We collect a large amount of trajectory data on different robotic platforms and evaluate several baseline models. Experimental results demonstrate that the GM-100 tasks are 1) feasible to execute and 2) sufficiently challenging to effectively differentiate the performance of current VLA models. Our data and code are available at https://rhos.ai/research/gm-100.",
  "url": "https://arxiv.org/abs/2601.11421v1",
  "html_url": "https://arxiv.org/html/2601.11421v1",
  "html_content": "\\keepXColumns\nThe Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents\nZiyu Wang\n1\n, Chenyuan Liu\n1\n, Yushun Xiang\n1\n, Runhao Zhang\n2\n,\nYu Zhang\n1\n,\nQingbo Hao\n1\n,\nHongliang Lu\n1\nHouyu Chen\n1\n,\nZhizhong Feng\n1\n,\nKaiyue Zheng\n1\n,\nDehao Ye\n1\n,\nXianchao Zeng\n2\n,\nXinyu Zhou\n2\n,\nBoran Wen\n1,2\nJiaxin Li\n1,2\n,\nMingyu Zhang\n1,2\n,\nKecheng Zheng\n3\n,\nQian Zhu\n3\n,\nRan Cheng\n3\n,\nYong-Lu Li\n1,2\n1\nSJTU,\n2\nSII,\n3\nRobbyant\n{ziyu.wang, yonglu_li}@sjtu.edu.cn\n,\nzkechengzk@gmail.com, {yunzhong.zq, zhouzhan.cr}@antgroup.com\nRHOS.ai, xbench\nCorresponding author.\nAbstract\nRecently, with the rapid development of robot learning and imitation learning, numerous datasets and methods have emerged. However, these datasets and their task designs often lack systematic consideration and principles. This raises important questions:\nDo the current datasets and task designs truly advance the capabilities of robotic agents?\nDo evaluations on a few common tasks accurately reflect the differentiated performance of various methods proposed by different teams and evaluated on different tasks?\nTo address these issues, we introduce the Great March 100 (\nGM-100\n) as the first step towards a\nrobot learning Olympics\n. GM-100 consists of 100 carefully designed tasks that cover a wide range of interactions and long-tail behaviors, aiming to provide a diverse and challenging set of tasks to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs. These tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from\nhuman-object interaction primitives and object affordances\n. We collect a large amount of trajectory data on different robotic platforms and evaluate several baseline models. Experimental results demonstrate that the GM-100 tasks are 1) feasible to execute and 2) sufficiently challenging to effectively differentiate the performance of current VLA models.\nOur data and code are available at\nhttps://rhos.ai/research/gm-100\n.\n1\nIntroduction\nRecently, with the rapid development of robot learning, numerous datasets and task designs were proposed. For example, Open X-Embodiment\n[\n7\n]\nassembled a dataset from 22 different robots, containing 160,266 tasks. Agibot\n[\n2\n]\ncollects 200+ tasks with 1M+ trajectories; RoboCOIN\n[\n26\n]\ncollects over 180,000 demonstrations for 421 tasks. However, these datasets and tasks often focus on a few common tasks and behaviors. After removing duplicates and categorizing them based on their semantic meanings, most tasks concentrate on very common behaviors such as “pick and hold”, while lacking coverage of complex and long-tail tasks. This singular task design leads to significant biases in the trained models, limiting their applicability in real-world scenarios as\npre-trained models\n, except for a few common tasks.\nSimilarly, current evaluation tasks suffer from analogous issues. Most studies, when proposing new methods, tend to test only on a few common tasks, without a unified task design standard, making fair comparisons across different works difficult.\nTo address these issues, we introduce the Great March 100 (\nGM-100\n) as the first step towards a robot learning Olympics.\nGM-100 consists of 100 carefully designed tasks that cover a wide range of interactions and long-tail behaviors, aiming to provide a diverse and challenging set of tasks to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs. These tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from human action understanding. We collect a large amount of trajectory data on two different robotic platforms and evaluate several baseline models. Experimental results demonstrate that the GM-100 tasks are 1) feasible to execute and 2) sufficiently challenging to effectively differentiate the performance of various methods.\nBesides, in the task design process, we do not rely on the\nutility\nfor real-world tasks as the standard to avoid human bias, but follow the\nphysical common sense\nand\nlow-level manipulation knowledge (the how-level affordance)\nas the only standards to generate and select the final tasks.\nTo summarize, in this report, we make the following contributions:\n•\nWe identify the limitations of existing robot task designs and evaluations, highlighting the need for more diverse and complex tasks.\n•\nWe propose GM-100, a task list consisting of 100 detail-oriented tasks that cover a wide range of interactions and long-tail behaviors.\n•\nWe collect a medium-sized dataset on robotic platforms and evaluate several baseline models, demonstrating the challenge and effectiveness of GM-100.\nOur data and code are available at\nhttps://rhos.ai/research/gm-100\n.\n2\nRelated Work\n2.1\nImitation Learning\nImitation learning underpins embodied intelligence by teaching agents to map sensory inputs to actions via expert demonstrations. Early methods include Behavioural Cloning\n[\n20\n]\n, interactive aggregation as in DAgger\n[\n22\n]\n, adversarial approaches like GAIL\n[\n10\n]\n. More recently, diffusion-based policies such as ACT\n[\n31\n]\n, Diffusion Policy\n[\n6\n]\n, and 3D Diffusion Policy\n[\n29\n]\n. These techniques improve sample efficiency and multimodal trajectory modeling but still face challenges in distributional shift, real-time inference, and training stability.\n2.2\nVision-Language Action Model\nBuilding on these foundations, Vision–Language–Action (VLA) models merge perception, instruction understanding, and control into unified networks. Representative instances include RT-2\n[\n32\n]\n, OpenVLA\n[\n13\n]\n, Robotics Diffusion Transformer (RDT)\n[\n18\n]\n,\nπ\n0\n\\pi_{0}\n[\n1\n]\n, CogACT\n[\n14\n]\n, SpatialVLA\n[\n21\n]\n,\nπ\n0.5\n\\pi_{0.5}\n[\n11\n]\n, SmolVLA\n[\n23\n]\n, UniVLA\n[\n3\n]\n, WALL-OSS\n[\n30\n]\n, GR00T\n[\n19\n]\n, RynnVLA-002\n[\n5\n]\n. Despite their effectiveness, both traditional and VLA-based imitation learning methods often require large-scale expert demonstrations.\n2.3\nManipulation Datasets and Task Design\nTo advance the learning and training of robotic policies, numerous datasets have emerged in recent years, such as Open X-Embodiment\n[\n7\n]\n, Agibot\n[\n2\n]\n, BridgeData V2\n[\n24\n]\n, RH20T\n[\n8\n]\n, DROID\n[\n12\n]\n, RoboCOIN\n[\n27\n]\n, RoboMIND\n[\n25\n]\n.\nHowever, while there has been a surge in data collection, few have focused on task design and balancing task diversity and long-tail representation. This has led to significant overlap in datasets and task designs, concentrating on a few common tasks and behaviors like “pick and hold” while lacking coverage of complex and long-tail tasks. This limitation hinders the development of intelligent agents with truly human-like capabilities.\nSimilarly, evaluations of robotic agents often focus on these common tasks, neglecting complex and long-tail tasks, which limits our comprehensive assessment of agent capabilities.\n3\nTask Design of GM-100\nIn previous works, researchers have designed robot tasks based on several subjective criteria, including designer judgment, common daily activities, and application scenarios.\nHowever, these approaches often lack systematic consideration and design principles, leading to significant overlap in tasks across different works and a focus on very common activities and tasks. This results in insufficient coverage of long-tail tasks in robot datasets and evaluation tasks, with data accumulation concentrated on common tasks while neglecting rare ones.\nWe collect and analyze the task designs from prior works like Agibot\n[\n2\n]\nand Open X-Embodiment\n[\n7\n]\n, removing duplicates and categorizing them. The accompanying word cloud and verb frequency chart in Figure\n1\nreveal a clear bias towards the most common tasks, with many tasks requiring similar actions like “pick and hold”.\nThis analysis highlights the limitations of past task designs, which often overlook rare but important tasks in the long-tail distribution and exhibit significant overlap among different tasks. These issues stem from a longstanding ignorance of the long-tail nature of human activities and the coupling of multi-class actions, which we need robots to learn and perform.\n(a)\nWord cloud of task descriptions of existing works.\n(b)\nVerb frequency distribution in previous task descriptions.\nFigure 1:\nTask design analysis from prior works. (a) Word cloud of task descriptions. (b) Verb frequency distribution in task descriptions.\nIn light of these, in the work, we propose to design the robot tasks according to the human action propriety. We aim to create a diverse set of tasks that cover a wide range of interactions, considering the coupling of actions and ensuring representation of long-tail, rare but important actions. We also design tasks that may\nseem simple in imagination but are actually challenging in practice\n, based on insights from human-object interaction studies such as HAKE\n[\n15\n]\n, PaStaNet\n[\n16\n]\n, and OCL\n[\n17\n]\n.\nWe first choose the basic interactions based on the previous robot learning works, like the task list from Agibot\n[\n2\n]\nand evaluation tasks from\nπ\n0.5\n\\pi_{0.5}\n[\n11\n]\n. In detail, we collect all the tasks from these works, remove duplicates, and categorize them based on their semantic meanings.\nThen, based on these existing tasks, we further expand and supplement the task list by referring to human-object interaction primitives and object affordances from HAKE\n[\n15\n]\nand OCL\n[\n17\n]\n.\nUsing a large language model, we automatically generate a large number of tasks based on carefully designed prompts that incorporate these actions and objects and select a diverse set of activities.\nSpecifically, we carefully select a set of representative human–object interaction primitives spanning from high-frequency to low-frequency activities. Under a unified task-design prompt, we leverage the Qwen3 model\n[\n28\n]\nto automatically generate a large pool of candidate tasks.\nFirst, we perform word sense disambiguation on the selected action primitives to eliminate potential ambiguities and ensure semantic uniqueness and consistency. We then prompt the Qwen3 to enumerate objects that are semantically and physically relevant to each action primitive. Based on these action–object pairs, the model further synthesizes concrete task instances and refines the task descriptions to produce clear, human-readable textual specifications.\nDuring the task filtering stage, we additionally employ the Qwen3 to automatically score the robot executability of the generated tasks, followed by a final selection conducted by five human experts as the gold standard.\nThrough this process, we obtain a high-quality set of robot tasks that are feasible under current hardware constraints and friendly for teleoperation-based data collection.\nFinally, we combine evaluations from multiple large language models and human experts to score and select the tasks. We ensure the selected tasks are feasible with current hardware capabilities and are friendly for data collection. We prioritize the tasks based on their scores, and for high-priority tasks, we design specific interaction details and select objects, e.g., choose proper objects from the\nTaobao.com\n. We also establish clear criteria for task completion evaluation, laying the groundwork for future metrics beyond success rate (SR). Additionally, we record template videos of humans completing these tasks to guide data collection.\nBased on the tasks generated and selected, and considering the workload for the first version, we select 100 tasks to form the GM-100 benchmark as our 1st version of GM. This set will serve as the foundation for future GM-series research.\nFigure 2:\nThe construction pipeline of the GM-100 benchmark. The process begins with collecting existing robot tasks, followed by a semantic expansion using HAKE\n[\n15\n]\nand LLM-based generation to cover long-tail interactions. The candidates then undergo a rigorous hybrid filtration by LLMs and human experts to ensure hardware feasibility and data collection friendliness. Finally, 100 high-priority tasks are selected and instantiated with detailed interaction criteria and template videos.\n4\nGM-100\nWe collect a medium-sized dataset containing over 13K trajectories with teleoperation in the GM-100 tasks. We open-source the dataset and the task list in\nhttps://rhos.ai/research/gm-100\n.\n4.1\nHardware Platform\nWe use two robotic platforms, Agilex Cobot Magic and Dobot Xtrainer, to collect the dataset and evaluate the embodied AI agents. The two platforms have different kinematic structures, bi-arm designs, and main camera views, which can provide diverse data for evaluation.\nCobot Magic is a Mobile-Aloha\n[\n9\n]\nlike robot platform that has a forward-reaching arm structure and a head-mounted camera, while Xtrainer is an Aloha\n[\n31\n]\nlike platform with an Inward-folding arm structure and a top-down camera view.\nThe two platforms are shown in Figure\n3\n.\nThe trajectory data is collected by teleoperation, where human operators control the robot arms to perform various tasks. We collect all 100 tasks on Cobot Magic and 10 tasks on Xtrainer for the current version. More data collections are being conducted and will be open-sourced in the next version.\nFigure 3:\nGM-100 Dataset.\nTwo distinct robotic platforms are utilized for data collection and evaluation. For Tasks 1–10, we collect 130 trajectories per task on both platforms, whereas for Tasks 11–100, data is collected exclusively on the Cobot Magic platform. To ensure the 100 training trajectories and 30 testing trajectories share a similar data distribution, we strive to maintain consistency in the environments and object configurations used for both data collection and evaluation. (Note: The included data distribution figure is schematic and intended solely for illustration purposes; it does not depict the exact statistical distribution.)\n4.2\nData Distribution\nFor each task, we first collect 100 trajectories with different initial conditions and design perturbations to ensure diversity in position, orientation, and object placement.\nThen, we collect another 30 trajectories with similar distributions to the first 100 trajectories. These 30 trajectories are used to align the test cases during evaluation, which ensures that the test cases remain consistent across different checkpoints or different models.\n5\nExperimental Setup\n5.1\nBaseline Models\nTo validate the feasibility and challenge of the GM-100 tasks, we evaluate several baseline models across 100 tasks. The baseline models include DP,\nπ\n0\n\\pi_{0}\n,\nπ\n0.5\n\\pi_{0.5}\n, and GR00T.\nThese models are either trained from scratch (for DP) or fine-tuned (for VLA models) on the collected 100 trajectories for each task until convergence. Details of each baseline model and their training procedures are provided in Appendix\nB\n.\n5.2\nEvaluation Metrics\nTo evaluate the performance of different models on GM-100 tasks, we use the following metrics:\nSuccess Rate (SR).\nThe percentage of tasks successfully completed within a fixed number of attempts. This is the most commonly used and straightforward metric for evaluating robot task performance. To ensure fairness and reproducibility, we align the test cases across different models by using the same set of 30 test trajectories collected for each task.\nDue to the time-consuming nature of real-world robot testing, we will gradually release the results on all baselines over time.\nTable 1:\nReal-world Performance\non Xtrainer platform. Task details are provided in Appendix\nA\n. Due to the time-consuming nature of real-world robot testing, we will gradually release the results on more baselines and tasks over time. Full results can be found at\nhttps://rhos.ai/research/gm-100/results\n.\nTask ID\nPSR\nSR\nDP\nπ\n0\n\\pi_{0}\nπ\n0.5\n\\pi_{0.5}\nDP\nπ\n0\n\\pi_{0}\nπ\n0.5\n\\pi_{0.5}\n0001\n2.5%\n45.8%\n72.5%\n0.0%\n13.3%\n36.7%\n0002\n0.5%\n35.0%\n39.0%\n0.0%\n0.0%\n8.0%\n0003\n4.4%\n47.8%\n51.1%\n0.0%\n0.0%\n0.0%\n0004\n25.8%\n45.8%\n70.8%\n3.3%\n0.0%\n30.0%\n0005\n12.2%\n10.2%\n8.2%\n12.2%\n10.2%\n8.2%\n0006\n6.3%\n19.6%\n60.6%\n0.0%\n0.0%\n13.3%\n0007\n6.2%\n44.6%\n90.0%\n0.0%\n3.3%\n50.0%\n0008\n11.1%\n42.2%\n78.9%\n0.0%\n6.7%\n66.7%\n0009\n11.1%\n20.0%\n31.1%\n0.0%\n0.0%\n0.0%\n0010\n0%\n10.0%\n36.7%\n0.0%\n10.0%\n36.7%\nAverage\n7.0%\n32.1%\n53.9%\n1.6%\n4.4%\n24.9%\nPartial Success Rate (PSR).\nThe percentage of subtasks successfully completed within a task. For complex tasks that involve multiple steps or goals, SR alone may not fully capture the model’s performance.\nSuboptimal robotic arm configurations for specific tasks, the wide distribution of collected datasets, and insufficient training data collectively contribute to low overall success rates on GM-100 benchmarks.\nWhile this outcome underscores the inherent challenges of the tasks under such data constraints, it also impairs our capacity to conduct fine-grained assessments of model performance.\nThus, for most GM-100 tasks, we define multiple\nsubtasks\nand\nsubgoals\nthat need to be accomplished to complete the overall task.\nDetailed definitions of score calculation for each task are provided at\nGM-100 Task List\n.\nPSR measures how many of these subtasks are successfully achieved, providing a more fine-grained evaluation of model performance.\nAction Prediction Error.\nThe mean squared error (MSE) and L1 loss between the predicted actions and ground truth actions in the specific prediction window on the test trajectories. Although low action prediction error does not guarantee high task success, it reflects the model’s ability to understand and replicate unseen expert demonstrations. However, each baseline may predict action chunks of different lengths, so to ensure fair comparison, we compute the action prediction error only on a specific overlapping prediction window across all baselines.\n6\nResults and Analysis\n6.1\nReal-world Performance\nWe show the Real-world Performance of different baseline models on the Xtrainer platform in Table\n1\nand Partial Success Rate on the Cobot Magic platform in Figure\n4\n. Detailed results are provided at\nGM-100 Results\ndue to space limitations.\nFigure 4:\nPartial Success Rate\non Cobot Magic Platform.\nThe color intensity in the heatmap indicates the PSR. Task Details are provided at\nGM-100 Task List\ndue to the space limit. Results on more baselines will be gradually released over time due to the time-consuming nature of real-world robot testing. Detailed PSR can be found at\nGM-100 PSR\n.\n6.2\nPrediction Loss\nTable 2:\nAction prediction error analysis\n(MSE and L1 loss) on Xtrainer platform. Task Details are provided in Appendix\nA\n.\nTask ID\nMSE\nL1\nDP\nπ\n0\n\\pi_{0}\nπ\n0.5\n\\pi_{0.5}\nDP\nπ\n0\n\\pi_{0}\nπ\n0.5\n\\pi_{0.5}\n0001\n0.0041\n0.0030\n0.0025\n0.0352\n0.0246\n0.0213\n0002\n0.0030\n0.0017\n0.0015\n0.0288\n0.0215\n0.0193\n0003\n0.0036\n0.0033\n0.0027\n0.0317\n0.0259\n0.0232\n0004\n0.0062\n0.0051\n0.0045\n0.0320\n0.0272\n0.0250\n0005\n0.0038\n0.0021\n0.0022\n0.0365\n0.0215\n0.0223\n0006\n0.0028\n0.0021\n0.0019\n0.0253\n0.0216\n0.0204\n0007\n0.0101\n0.0074\n0.0064\n0.0480\n0.0398\n0.0375\n0008\n0.0010\n0.0009\n0.0007\n0.0124\n0.0113\n0.0102\n0009\n0.0091\n0.0056\n0.0042\n0.0488\n0.0350\n0.0331\n0010\n0.0031\n0.0022\n0.0020\n0.0294\n0.0233\n0.0218\nAverage\n0.0047\n0.0033\n0.0029\n0.0328\n0.0252\n0.0234\nTable\n2\npresents the action prediction errors measured by MSE and L1 loss. To further evaluate the relationship between offline supervised objectives and online deployment performance, we visualize the Normalized MSE alongside the Physical Success Rate (PSR) for each task in Figure\n5\n.\nHere, the Normalized MSE is defined by scaling the raw MSE of each model relative to the maximum MSE recorded among the three candidate models, effectively mapping the error values onto a\n[\n0\n,\n1\n]\n[0,1]\nrange to facilitate cross-model comparison.\nA clear inverse correlation is observed between action prediction error and physical success across the three models. Specifically,\nπ\n0.5\n\\pi_{0.5}\n(represented in red) consistently minimizes the Normalized MSE (leftward bars) while achieving the highest PSR (rightward bars) in nearly all tasks. In contrast, for the Diffusion Policy, a larger deviation in action prediction directly translates to a diminished PSR, which remains below\n25\n%\n25\\%\nfor the majority of tasks. These results suggest that in the Xtrainer environment, high-precision action modeling is a prerequisite for successful physical interaction. The superior performance of\nπ\n0.5\n\\pi_{0.5}\nindicates that the architectural refinements and training objectives of the\nπ\n\\pi\nseries not only minimize the training loss but also effectively capture the underlying distribution of successful trajectories, leading to more robust policy execution in real-world or high-fidelity simulated scenarios.\nFigure 5:\nTask-level diverging comparison of\nNormalized MSE\n(left) and\nPartial Success Rate\n(right) across models.\n7\nWebsite of GM-100\nWe have open-sourced our task designs, data, and testing procedures. Since real-world robot testing is highly costly, we will gradually release all results over time.\nWe do not aim to build an absolutely fair physical testing environment, as current robotic learning models remain significantly influenced by tester capability and environmental conditions. Achieving truly fair, dispute-free, and mutually trusted global testing may be impractical at this stage.\nInstead, we provide fine-tuning data and task definitions, and operate an open platform where any researchers can upload their own results and evidence videos.\nWe will conduct verification and author confirmation to the best of our ability for open-source models. We require weight submissions for review and assign them “checked” labels accordingly.\nHowever, everyone can also update their results without verification, and then the whole community will naturally arrive at a long-term evaluation, as we have done with our papers on arXiv.\nWe believe in the power of community to foster transparent and credible benchmarking.\nMoving forward, we will continue to expand and refine our task generation system, open-source efforts, and platform maintenance. Our goal is to provide a reliable reference for model evaluation and to build a collaborative open ecosystem around\nGM-X\n.\nWe welcome partnerships and collaborations with OEMs and model developers. Stay tuned for GM-X!\n8\nConclusion\nIn this report, we present the Great March 100 (GM-100), a systematic step towards a comprehensive “Robot Learning Olympics”. Unlike previous datasets that prioritize scale over structure, GM-100 comprises 100 carefully curated tasks derived from a rigorous analysis of human-object interaction primitives and object affordances. This design philosophy ensures coverage of diverse, long-tail, and rare behaviors that critically test the generalization limits of robotic agents. We don’t aim to create just another benchmark; instead, GM-100 serves as a foundational task list for evaluating embodied AI systems in real-world settings.\nTo support this initiative, we established a medium-sized dataset containing over 13,000 teleoperated trajectories across two distinct robotic platforms: Agilex Cobot Magic and Dobot Xtrainer. Extensive evaluations of baseline models, using metrics such as Success Rate (SR), Partial Success Rate (PSR), and Action Prediction Error, demonstrate that GM-100 tasks are physically feasible yet sufficiently challenging to effectively differentiate the performance of VLAs. Furthermore, acknowledging the challenges of maintaining a “fair” physical testing environment, we advocate for a transparent, community-driven evaluation paradigm that relies on collective oversight and open evidence sharing rather than rigid, centralized testing.\nLooking forward, GM-100 serves as the foundational layer for GM-X. We are committed to continuously expanding our task generation system and fostering a collaborative, open ecosystem, aiming to build a reliable reference that drives the progress of embodied AI.\nReferences\n[1]\nK. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter,\net al.\n(2024)\nπ\n0\n\\pi_{0}\n: A vision-language-action flow model for general robot control\n.\narXiv preprint arXiv:2410.24164\n.\nCited by:\nAppendix B\n,\n§2.2\n.\n[2]\nQ. Bu, J. Cai, L. Chen, X. Cui, Y. Ding, S. Feng, X. He, X. Huang,\net al.\n(2025)\nAgibot world colosseo: a large-scale manipulation platform for scalable and intelligent embodied systems\n.\nIn\n2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nCited by:\n§1\n,\n§2.3\n,\n§3\n,\n§3\n.\n[3]\nQ. Bu, Y. Yang, J. Cai, S. Gao, G. Ren, M. Yao, P. Luo, and H. Li\n(2025)\nUnivla: learning to act anywhere with task-centric latent actions\n.\narXiv preprint arXiv:2505.06111\n.\nCited by:\n§2.2\n.\n[4]\nR. Cadene, S. Alibert, A. Soare, Q. Gallouedec, A. Zouitine, S. Palma, P. Kooijmans, M. Aractingi, M. Shukor, D. Aubakirova, M. Russi, F. Capuano, C. Pascal, J. Choghari, J. Moss, and T. Wolf\n(2024)\nLeRobot: state-of-the-art machine learning for real-world robotics in pytorch\n.\nNote:\nhttps://github.com/huggingface/lerobot\nCited by:\nAppendix B\n.\n[5]\nJ. Cen, S. Huang, Y. Yuan, K. Li, H. Yuan, C. Yu, Y. Jiang, J. Guo, X. Li, H. Luo, F. Wang, F. Wang, and D. Zhao\n(2025)\nRynnVLA-002: a unified vision-language-action and world model\n.\narXiv preprint arXiv:2511.17502\n.\nCited by:\n§2.2\n.\n[6]\nC. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song\n(2023)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nThe International Journal of Robotics Research\n,\npp. 02783649241273668\n.\nCited by:\nAppendix B\n,\n§2.1\n.\n[7]\nE. Collaboration, A. O’Neill, A. Rehman, A. Gupta, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain, A. Tung, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, A. Rai, A. Gupta, A. Wang, A. Kolobov, A. Singh, A. Garg, A. Kembhavi, A. Xie, A. Brohan, A. Raffin, A. Sharma, A. Yavary, A. Jain, A. Balakrishna, A. Wahid, B. Burgess-Limerick, B. Kim, B. Schölkopf, B. Wulfe, B. Ichter, C. Lu, C. Xu, C. Le, C. Finn, C. Wang, C. Xu, C. Chi, C. Huang, C. Chan, C. Agia, C. Pan, C. Fu, C. Devin, D. Xu, D. Morton, D. Driess, D. Chen, D. Pathak, D. Shah, D. Büchler, D. Jayaraman, D. Kalashnikov, D. Sadigh, E. Johns, E. Foster, F. Liu, F. Ceola, F. Xia, F. Zhao, F. V. Frujeri, F. Stulp, G. Zhou, G. S. Sukhatme, G. Salhotra, G. Yan, G. Feng, G. Schiavi, G. Berseth, G. Kahn, G. Yang, G. Wang, H. Su, H. Fang, H. Shi, H. Bao, H. B. Amor, H. I. Christensen, H. Furuta, H. Bharadhwaj, H. Walke, H. Fang, H. Ha, I. Mordatch, I. Radosavovic, I. Leal, J. Liang, J. Abou-Chakra, J. Kim, J. Drake, J. Peters, J. Schneider, J. Hsu, J. Vakil, J. Bohg, J. Bingham, J. Wu, J. Gao, J. Hu, J. Wu, J. Wu, J. Sun, J. Luo, J. Gu, J. Tan, J. Oh, J. Wu, J. Lu, J. Yang, J. Malik, J. Silvério, J. Hejna, J. Booher, J. Tompson, J. Yang, J. Salvador, J. J. Lim, J. Han, K. Wang, K. Rao, K. Pertsch, K. Hausman, K. Go, K. Gopalakrishnan, K. Goldberg, K. Byrne, K. Oslund, K. Kawaharazuka, K. Black, K. Lin, K. Zhang, K. Ehsani, K. Lekkala, K. Ellis, K. Rana, K. Srinivasan, K. Fang, K. P. Singh, K. Zeng, K. Hatch, K. Hsu, L. Itti, L. Y. Chen, L. Pinto, L. Fei-Fei, L. Tan, L. \". Fan, L. Ott, L. Lee, L. Weihs, M. Chen, M. Lepert, M. Memmel, M. Tomizuka, M. Itkina, M. G. Castro, M. Spero, M. Du, M. Ahn, M. C. Yip, M. Zhang, M. Ding, M. Heo, M. K. Srirama, M. Sharma, M. J. Kim, M. Z. Irshad, N. Kanazawa, N. Hansen, N. Heess, N. J. Joshi, N. Suenderhauf, N. Liu, N. D. Palo, N. M. M. Shafiullah, O. Mees, O. Kroemer, O. Bastani, P. R. Sanketi, P. \". Miller, P. Yin, P. Wohlhart, P. Xu, P. D. Fagan, P. Mitrano, P. Sermanet, P. Abbeel, P. Sundaresan, Q. Chen, Q. Vuong, R. Rafailov, R. Tian, R. Doshi, R. Martín-Martín, R. Baijal, R. Scalise, R. Hendrix, R. Lin, R. Qian, R. Zhang, R. Mendonca, R. Shah, R. Hoque, R. Julian, S. Bustamante, S. Kirmani, S. Levine, S. Lin, S. Moore, S. Bahl, S. Dass, S. Sonawani, S. Tulsiani, S. Song, S. Xu, S. Haldar, S. Karamcheti, S. Adebola, S. Guist, S. Nasiriany, S. Schaal, S. Welker, S. Tian, S. Ramamoorthy, S. Dasari, S. Belkhale, S. Park, S. Nair, S. Mirchandani, T. Osa, T. Gupta, T. Harada, T. Matsushima, T. Xiao, T. Kollar, T. Yu, T. Ding, T. Davchev, T. Z. Zhao, T. Armstrong, T. Darrell, T. Chung, V. Jain, V. Kumar, V. Vanhoucke, V. Guizilini, W. Zhan, W. Zhou, W. Burgard, X. Chen, X. Chen, X. Wang, X. Zhu, X. Geng, X. Liu, X. Liangwei, X. Li, Y. Pang, Y. Lu, Y. J. Ma, Y. Kim, Y. Chebotar, Y. Zhou, Y. Zhu, Y. Wu, Y. Xu, Y. Wang, Y. Bisk, Y. Dou, Y. Cho, Y. Lee, Y. Cui, Y. Cao, Y. Wu, Y. Tang, Y. Zhu, Y. Zhang, Y. Jiang, Y. Li, Y. Li, Y. Iwasawa, Y. Matsuo, Z. Ma, Z. Xu, Z. J. Cui, Z. Zhang, Z. Fu, and Z. Lin\n(2025)\nOpen x-embodiment: robotic learning datasets and rt-x models\n.\nExternal Links:\n2310.08864\n,\nLink\nCited by:\n§1\n,\n§2.3\n,\n§3\n.\n[8]\nH. Fang, H. Fang, Z. Tang, J. Liu, C. Wang, J. Wang, H. Zhu, and C. Lu\n(2024)\nRh20t: a comprehensive robotic dataset for learning diverse skills in one-shot\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp. 653–660\n.\nCited by:\n§2.3\n.\n[9]\nZ. Fu, T. Z. Zhao, and C. Finn\n(2024)\nMobile aloha: learning bimanual mobile manipulation with low-cost whole-body teleoperation\n.\nIn\nConference on Robot Learning (CoRL)\n,\nCited by:\n§4.1\n.\n[10]\nJ. Ho and S. Ermon\n(2016)\nGenerative adversarial imitation learning\n.\nAdvances in neural information processing systems\n29\n.\nCited by:\n§2.1\n.\n[11]\nP. Intelligence, K. Black, N. Brown, J. Darpinian, K. Dhabalia, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai,\net al.\n(2025)\nπ\n0.5\n\\pi_{0.5}\n: A vision-language-action model with open-world generalization\n.\narXiv preprint arXiv:2504.16054\n.\nCited by:\n§2.2\n,\n§3\n.\n[12]\nA. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, P. D. Fagan, J. Hejna, M. Itkina, M. Lepert, Y. J. Ma, P. T. Miller, J. Wu, S. Belkhale, S. Dass, H. Ha, A. Jain, A. Lee, Y. Lee, M. Memmel, S. Park, I. Radosavovic, K. Wang, A. Zhan, K. Black, C. Chi, K. B. Hatch, S. Lin, J. Lu, J. Mercat, A. Rehman, P. R. Sanketi, A. Sharma, C. Simpson, Q. Vuong, H. R. Walke, B. Wulfe, T. Xiao, J. H. Yang, A. Yavary, T. Z. Zhao, C. Agia, R. Baijal, M. G. Castro, D. Chen, Q. Chen, T. Chung, J. Drake, E. P. Foster, J. Gao, V. Guizilini, D. A. Herrera, M. Heo, K. Hsu, J. Hu, M. Z. Irshad, D. Jackson, C. Le, Y. Li, K. Lin, R. Lin, Z. Ma, A. Maddukuri, S. Mirchandani, D. Morton, T. Nguyen, A. O’Neill, R. Scalise, D. Seale, V. Son, S. Tian, E. Tran, A. E. Wang, Y. Wu, A. Xie, J. Yang, P. Yin, Y. Zhang, O. Bastani, G. Berseth, J. Bohg, K. Goldberg, A. Gupta, A. Gupta, D. Jayaraman, J. J. Lim, J. Malik, R. Martín-Martín, S. Ramamoorthy, D. Sadigh, S. Song, J. Wu, M. C. Yip, Y. Zhu, T. Kollar, S. Levine, and C. Finn\n(2025)\nDROID: a large-scale in-the-wild robot manipulation dataset\n.\nExternal Links:\n2403.12945\n,\nLink\nCited by:\n§2.3\n.\n[13]\nM. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi,\net al.\n(2024)\nOpenvla: an open-source vision-language-action model\n.\narXiv preprint arXiv:2406.09246\n.\nCited by:\n§2.2\n.\n[14]\nQ. Li, Y. Liang, Z. Wang, L. Luo, X. Chen, M. Liao, F. Wei, Y. Deng, S. Xu, Y. Zhang,\net al.\n(2024)\nCogACT: a foundational vision-language-action model for synergizing cognition and action in robotic manipulation\n.\narXiv preprint arXiv:2411.19650\n.\nCited by:\n§2.2\n.\n[15]\nY. Li, L. Xu, X. Liu, X. Huang, Y. Xu, M. Chen, Z. Ma, S. Wang, H. Fang, and C. Lu\n(2019)\nHAKE: human activity knowledge engine\n.\nExternal Links:\n1904.06539\n,\nLink\nCited by:\nFigure 2\n,\n§3\n,\n§3\n.\n[16]\nY. Li, L. Xu, X. Liu, X. Huang, Y. Xu, S. Wang, H. Fang, Z. Ma, M. Chen, and C. Lu\n(2020)\nPaStaNet: toward human activity knowledge engine\n.\nExternal Links:\n2004.00945\n,\nLink\nCited by:\n§3\n.\n[17]\nY. Li, Y. Xu, X. Xu, X. Mao, Y. Yao, S. Liu, and C. Lu\n(2023)\nBeyond object recognition: a new benchmark towards object concept learning\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp. 20029–20040\n.\nCited by:\n§3\n,\n§3\n.\n[18]\nS. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su, and J. Zhu\n(2024)\nRdt-1b: a diffusion foundation model for bimanual manipulation\n.\narXiv preprint arXiv:2410.07864\n.\nCited by:\n§2.2\n.\n[19]\nNVIDIA, J. Bjorck, N. C. Fernando Castañeda, X. Da, R. Ding, L. \". Fan, Y. Fang, D. Fox, F. Hu, S. Huang, J. Jang, Z. Jiang, J. Kautz, K. Kundalia, L. Lao, Z. Li, Z. Lin, K. Lin, G. Liu, E. Llontop, L. Magne, A. Mandlekar, A. Narayan, S. Nasiriany, S. Reed, Y. L. Tan, G. Wang, Z. Wang, J. Wang, Q. Wang, J. Xiang, Y. Xie, Y. Xu, Z. Xu, S. Ye, Z. Yu, A. Zhang, H. Zhang, Y. Zhao, R. Zheng, and Y. Zhu\n(2025-03)\nGR00T N1: an open foundation model for generalist humanoid robots\n.\nIn\nArXiv Preprint\n,\nExternal Links:\n2503.14734\nCited by:\n§2.2\n.\n[20]\nD. Pomerleau\n(1989-12)\nALVINN: an autonomous land vehicle in a neural network\n.\nIn\nProceedings of (NeurIPS) Neural Information Processing Systems\n,\nD.S. Touretzky (Ed.)\n,\npp. 305 – 313\n.\nCited by:\n§2.1\n.\n[21]\nD. Qu, H. Song, Q. Chen, Y. Yao, X. Ye, Y. Ding, Z. Wang, J. Gu, B. Zhao, D. Wang,\net al.\n(2025)\nSpatialVLA: exploring spatial representations for visual-language-action model\n.\narXiv preprint arXiv:2501.15830\n.\nCited by:\n§2.2\n.\n[22]\nS. Ross, G. Gordon, and D. Bagnell\n(2011)\nA reduction of imitation learning and structured prediction to no-regret online learning\n.\nIn\nProceedings of the fourteenth international conference on artificial intelligence and statistics\n,\npp. 627–635\n.\nCited by:\n§2.1\n.\n[23]\nM. Shukor, D. Aubakirova, F. Capuano, P. Kooijmans, S. Palma, A. Zouitine, M. Aractingi, C. Pascal, M. Russi, A. Marafioti,\net al.\n(2025)\nSmolvla: a vision-language-action model for affordable and efficient robotics\n.\narXiv preprint arXiv:2506.01844\n.\nCited by:\n§2.2\n.\n[24]\nH. Walke, K. Black, A. Lee, M. J. Kim, M. Du, C. Zheng, T. Zhao, P. Hansen-Estruch, Q. Vuong, A. He, V. Myers, K. Fang, C. Finn, and S. Levine\n(2023)\nBridgeData v2: a dataset for robot learning at scale\n.\nIn\nConference on Robot Learning (CoRL)\n,\nCited by:\n§2.3\n.\n[25]\nK. Wu, C. Hou, J. Liu, Z. Che, X. Ju, Z. Yang, M. Li, Y. Zhao, Z. Xu, G. Yang,\net al.\n(2025)\nRobomind: benchmark on multi-embodiment intelligence normative data for robot manipulation\n.\nIn\nRobotics: Science and Systems (RSS) 2025\n,\nExternal Links:\nLink\nCited by:\n§2.3\n.\n[26]\nS. Wu, X. Liu, S. Xie, P. Wang, X. Li, B. Yang, Z. Li, K. Zhu, H. Wu, Y. Liu, Z. Long, Y. Wang, C. Liu, D. Wang, Z. Ni, X. Yang, Y. Liu, R. Feng, R. Xu, L. Zhang, D. Huang, C. Jin, A. Yin, X. Wang, Z. Sun, J. Zhao, M. Du, M. Cao, X. Chen, H. Cheng, X. Zhang, Y. Fu, N. Chen, C. Chi, S. Chen, H. Lyu, X. Hao, Y. Wang, B. Lei, D. Liu, X. Yang, Y. Jiao, T. Pan, Y. Zhang, S. Wang, Z. Zhang, X. Liu, J. Zhang, C. Meng, Z. Zhang, J. Gao, S. Wang, X. Leng, Z. Xie, Z. Zhou, P. Huang, W. Yang, Y. Guo, Y. Zhu, S. Zheng, H. Cheng, X. Ding, Y. Yue, H. Wang, C. Chen, J. Pang, Y. Qian, H. Geng, L. Gao, H. Li, B. Fang, G. Huang, Y. Yang, H. Dong, H. Wang, H. Zhao, Y. Mu, D. Hu, H. Zhao, T. Huang, S. Zhang, Y. Lin, Z. Wang, and G. Yao\n(2025)\nRoboCOIN: an open-sourced bimanual robotic data collection for integrated manipulation\n.\nExternal Links:\nLink\nCited by:\n§1\n.\n[27]\nS. Wu, X. Liu, S. Xie, P. Wang, X. Li, B. Yang, Z. Li, K. Zhu, H. Wu, Y. Liu, Z. Long, Y. Wang, C. Liu, D. Wang, Z. Ni, X. Yang, Y. Liu, R. Feng, R. Xu, L. Zhang, D. Huang, C. Jin, A. Yin, X. Wang, Z. Sun, J. Zhao, M. Du, M. Cao, X. Chen, H. Cheng, X. Zhang, Y. Fu, N. Chen, C. Chi, S. Chen, H. Lyu, X. Hao, Y. Wang, B. Lei, D. Liu, X. Yang, Y. Jiao, T. Pan, Y. Zhang, S. Wang, Z. Zhang, X. Liu, J. Zhang, C. Meng, Z. Zhang, J. Gao, S. Wang, X. Leng, Z. Xie, Z. Zhou, P. Huang, W. Yang, Y. Guo, Y. Zhu, S. Zheng, H. Cheng, X. Ding, Y. Yue, H. Wang, C. Chen, J. Pang, Y. Qian, H. Geng, L. Gao, H. Li, B. Fang, G. Huang, Y. Yang, H. Dong, H. Wang, H. Zhao, Y. Mu, D. Hu, H. Zhao, T. Huang, S. Zhang, Y. Lin, Z. Wang, and G. Yao\n(2025)\nRoboCOIN: an open-sourced bimanual robotic data collection for integrated manipulation\n.\nExternal Links:\n2511.17441\n,\nLink\nCited by:\n§2.3\n.\n[28]\nA. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, C. Zheng, D. Liu, F. Zhou, F. Huang, F. Hu, H. Ge, H. Wei, H. Lin, J. Tang, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Zhou, J. Lin, K. Dang, K. Bao, K. Yang, L. Yu, L. Deng, M. Li, M. Xue, M. Li, P. Zhang, P. Wang, Q. Zhu, R. Men, R. Gao, S. Liu, S. Luo, T. Li, T. Tang, W. Yin, X. Ren, X. Wang, X. Zhang, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Zhang, Y. Wan, Y. Liu, Z. Wang, Z. Cui, Z. Zhang, Z. Zhou, and Z. Qiu\n(2025)\nQwen3 technical report\n.\narXiv preprint arXiv:2505.09388\n.\nCited by:\n§3\n.\n[29]\nY. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, and H. Xu\n(2024)\n3d diffusion policy: generalizable visuomotor policy learning via simple 3d representations\n.\narXiv preprint arXiv:2403.03954\n.\nCited by:\n§2.1\n.\n[30]\nA. Zhai, B. Liu, B. Fang, C. Cai, E. Ma, E. Yin, H. Wang, H. Zhou, J. Wang, L. Shi, L. Liang, M. Wang, Q. Wang, R. Gan, R. Yu, S. Li, S. Liu, S. Chen, V. Chen, and Z. Xu\n(2025)\nIgniting vlms toward the embodied space\n.\narXiv preprint arXiv:2509.11766\n.\nCited by:\n§2.2\n.\n[31]\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn\n(2023)\nLearning fine-grained bimanual manipulation with low-cost hardware\n.\narXiv preprint arXiv:2304.13705\n.\nCited by:\n§2.1\n,\n§4.1\n.\n[32]\nB. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid,\net al.\n(2023)\nRt-2: vision-language-action models transfer web knowledge to robotic control\n.\nIn\nConference on Robot Learning\n,\npp. 2165–2183\n.\nCited by:\n§2.2\n.\nAppendix A\nAppendix A: Full Task List\nHere we provide the details of some GM-100 tasks, along with their descriptions, interaction details, and object purchase links.\nMore details aboutthe full 100 tasks, including object specifications and success criteria, can be found at\nhttps://rhos.ai/research/gm-100/tasks\n.\n\\keepXColumns\nRobot Manipulation Tasks Dataset\nTask ID\nImage\nDescription\nInteraction Details\nObject Links\n\\endfirsthead\nTask ID\nImage\nDescription\nInteraction Details\nObject Links\n\\endhead\nContinued on next page…\n\\endfoot\n\\endlastfoot\n0001\nUse the gripper to strike the small ball into the tabletop goal.\nExecute a crisp, instantaneous strike. Do not push.\ntabletop ball\n0002\nGrasp the knife from the knife block, slice through clay.\nPress blade spine. Use sawing motion if needed.\n3D knife\n0003\nGrasp the stamp, remove cap, stamp, and re-insert.\nPress down firmly and hold for 1-2 seconds.\nstamp\n0004\nFold cardboard template along crease lines.\nOnly perform a single fold due to constraints.\ncardboard\n0005\nClose the desktop drawer while stabilizing the rear.\nUse second gripper to brace against backward movement.\ndrawer\n0006\nThread three hawthorns onto the skewer.\nSlide first hawthorns to base to prevent bending.\nhawthorn\nskewer\n0007\nClear workspace by tossing trash and placing tools.\nFast and clean tossing motion. Sort trash vs tools.\ntrashbin\n0008\nTransfer test tube from right rack to left rack.\nUse compliant grippers to avoid tube failure.\ntube\nrack\n0009\nPress the button of the desk lamp to turn it on.\nUse one gripper to hold the lamp, other to press.\nlamp\n0010\nSort cubes according to sizes from left to right.\nGrasp and compare the sizes of the cubes, placing them from left to right in ascending order while ensuring stable contact with the tabletop, no overlap, and consistent spacing between cubes.\ncube\nAppendix B\nAppendix B: Baseline Model Details\nHere we provide detailed descriptions of each baseline model used in our experiments, including their architectures, training procedures, and hyperparameter settings.\nDP\nis finetuned using the LeRobot framework\n[\n4\n]\nwith a tatal batch size of 512 (8 GPUs\n×\n\\times\n64 samples per device) for 100k steps on each task’s collected trajectories. The model architecture follows the original design in\n[\n6\n]\n, with modifications to accommodate the specific input topic name and action spaces of our robotic platforms. During inference, we utilize an NVIDIA RTX 4090 GPU for real-time action prediction. The inference chunk size is set to 16, and the model executes 8 steps per action prediction cycle.\nπ\n0\n\\pi_{0}\n&\nπ\n0.5\n\\pi_{0.5}\nis finetuned using the OpenPi framework\n[\n1\n]\nwith a batch size of 32 for 50k steps on each task’s collected trajectories. The model architecture follows the original design in\n[\n1\n]\n, with modifications to accommodate the specific input topic name and action spaces of our robotic platforms. During inference, we utilize an NVIDIA RTX 4090 GPU for real-time action prediction. The inference chunk size is set to 50, and the model executes 10 steps per action prediction cycle.",
  "preview_text": "Recently, with the rapid development of robot learning and imitation learning, numerous datasets and methods have emerged. However, these datasets and their task designs often lack systematic consideration and principles. This raises important questions: Do the current datasets and task designs truly advance the capabilities of robotic agents? Do evaluations on a few common tasks accurately reflect the differentiated performance of various methods proposed by different teams and evaluated on different tasks? To address these issues, we introduce the Great March 100 (\\textbf{GM-100}) as the first step towards a robot learning Olympics. GM-100 consists of 100 carefully designed tasks that cover a wide range of interactions and long-tail behaviors, aiming to provide a diverse and challenging set of tasks to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs. These tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from human-object interaction primitives and object affordances. We collect a large amount of trajectory data on different robotic platforms and evaluate several baseline models. Experimental results demonstrate that the GM-100 tasks are 1) feasible to execute and 2) sufficiently challenging to effectively differentiate the performance of current VLA models. Our data and code are available at https://rhos.ai/research/gm-100.\n\n\\keepXColumns\nThe Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents\nZiyu Wang\n1\n, Chenyuan Liu\n1\n, Yushun Xiang\n1\n, Runhao Zhang\n2\n,\nYu Zhang\n1\n,\nQingbo Hao\n1\n,\nHongliang Lu\n1\nHouyu Chen\n1\n,\nZhizhong Feng\n1\n,\nKaiyue Zheng\n1\n,\nDehao Ye\n1\n,\nXianchao Zeng\n2\n,\nXinyu Zhou\n2\n,\nBoran Wen\n1,2\nJiaxin Li\n1,2\n,\nMingyu Zhang\n1,2\n,\nKecheng Zheng\n3\n,\nQian Zhu\n3\n,\nRan Cheng\n3\n,\nYong-Lu Li\n1,2\n1\nSJTU,\n2\nSII,\n3\nRobbyant\n{ziyu.wang, yonglu_li}@sjtu.edu.cn\n,\nzkechengzk@gmail.com, {yunzhong.zq, zh",
  "is_relevant": true,
  "relevance_score": 4.0,
  "extracted_keywords": [
    "VLA",
    "locomotion",
    "whole body control"
  ],
  "one_line_summary": "该论文提出了一个包含100个任务的评估基准GM-100，用于全面评估具身AI代理的能力，特别是针对VLA模型，并涉及运动控制和全身控制任务。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-16T16:42:05Z",
  "created_at": "2026-01-20T17:50:01.375705",
  "updated_at": "2026-01-20T17:50:01.375712"
}