{
    "id": "2512.01446v1",
    "title": "$\\mathbf{M^3A}$ Policy: Mutable Material Manipulation Augmentation Policy through Photometric Re-rendering",
    "authors": [
        "Jiayi Li",
        "Yuxuan Hu",
        "Haoran Geng",
        "Xiangyu Chen",
        "Chuhao Zhou",
        "Ziteng Cui",
        "Jianfei Yang"
    ],
    "abstract": "ææ–™æ³›åŒ–å¯¹äºç°å®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œè‡³å…³é‡è¦ï¼Œæœºå™¨äººå¿…é¡»ä¸å…·æœ‰å¤šæ ·åŒ–è§†è§‰å’Œç‰©ç†ç‰¹æ€§çš„ç‰©ä½“è¿›è¡Œäº¤äº’ã€‚è¿™ä¸€æŒ‘æˆ˜å¯¹äºç”±ç»ç’ƒã€é‡‘å±æˆ–å…¶ä»–ææ–™åˆ¶æˆçš„ç‰©ä½“å°¤ä¸ºçªå‡ºï¼Œè¿™äº›ææ–™çš„é€æ˜æˆ–åå…‰è¡¨é¢ä¼šå¼•å…¥ä¸¥é‡çš„åˆ†å¸ƒå¤–å˜åŒ–ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–æ¨¡æ‹Ÿå™¨ä¸­çš„ä»¿çœŸææ–™è¿›è¡Œä»¿çœŸåˆ°ç°å®çš„è¿ç§»ï¼ˆä½†å—åˆ°æ˜¾è‘—è§†è§‰åŸŸå·®å¼‚çš„é˜»ç¢ï¼‰ï¼Œè¦ä¹ˆä¾èµ–æ”¶é›†å¤§é‡ç°å®ä¸–ç•Œæ¼”ç¤ºï¼ˆæˆæœ¬é«˜æ˜‚ã€è€—æ—¶ä¸”ä»ä¸è¶³ä»¥è¦†ç›–å„ç§ææ–™ï¼‰ã€‚ä¸ºå…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å€ŸåŠ©è®¡ç®—æ‘„å½±æŠ€æœ¯ï¼Œæå‡ºäº†å¯å˜ææ–™æ“ä½œå¢å¼ºæ¡†æ¶ï¼ˆM$^3$Aï¼‰â€”â€”ä¸€ä¸ªåˆ©ç”¨å…‰ä¼ è¾“æ•æ‰çš„ææ–™ç‰©ç†ç‰¹æ€§è¿›è¡Œå…‰åº¦é‡æ¸²æŸ“çš„ç»Ÿä¸€æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€æƒ³ç®€æ´è€Œå¼ºå¤§ï¼šç»™å®šå•ä¸ªç°å®ä¸–ç•Œæ¼”ç¤ºï¼Œæˆ‘ä»¬é€šè¿‡å…‰åº¦é‡æ¸²æŸ“åœºæ™¯ï¼Œç”Ÿæˆå…·æœ‰ä¸åŒææ–™ç‰¹æ€§çš„å¤šæ ·åŒ–é«˜çœŸå®æ„Ÿæ¼”ç¤ºã€‚è¿™ç§å¢å¼ºæœ‰æ•ˆè§£è€¦äº†ä»»åŠ¡ç‰¹å®šæ“ä½œæŠ€èƒ½ä¸è¡¨é¢å¤–è§‚ï¼Œä½¿ç­–ç•¥èƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–æ•°æ®æ”¶é›†çš„æƒ…å†µä¸‹å®ç°è·¨ææ–™æ³›åŒ–ã€‚ä¸ºç³»ç»Ÿè¯„ä¼°è¿™ä¸€èƒ½åŠ›ï¼Œæˆ‘ä»¬æ„å»ºäº†é¦–ä¸ªæ¶µç›–ä»¿çœŸä¸ç°å®ç¯å¢ƒçš„ç»¼åˆæ€§å¤šææ–™æ“ä½œåŸºå‡†æµ‹è¯•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒM$^3$Aç­–ç•¥æ˜¾è‘—æå‡äº†è·¨ææ–™æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ä¸‰ä¸ªç°å®ä¸–ç•Œä»»åŠ¡ä¸­å°†å¹³å‡æˆåŠŸç‡æé«˜äº†58.03%ï¼Œå¹¶åœ¨å…ˆå‰æœªè§ææ–™ä¸Šå±•ç°å‡ºé²æ£’æ€§èƒ½ã€‚",
    "url": "https://arxiv.org/abs/2512.01446v1",
    "html_url": "https://arxiv.org/html/2512.01446v1",
    "html_content": "ğŒ\nğŸ‘\nâ€‹\nğ€\n\\mathbf{M^{3}A}\nPolicy: Mutable Material Manipulation Augmentation Policy through Photometric Re-rendering\nJiayi Li\n1,2,âˆ—\n, Yuxuan Hu\n2\n, Haoran Geng\n3\n, Xiangyu Chen\n2\n, Chuhao Zhou\n2\n,\nZiteng Cui\n4\nand Jianfei Yang\n2,â€ \n1\nTsinghua University\n2\nMARS Lab, Nanyang Technological University\n3\nUniversity of California, Berkeley\n4\nThe University of Tokyo\nâˆ—\nWork carried out during NTU Research Internship\nâ€ \nCorresponding author\njy-l21@mails.tsinghua.edu.cn, jianfei.yang@ntu.edu.sg\nAbstract\nMaterial generalization is essential for real-world robotic manipulation, where robots must interact with objects exhibiting\ndiverse visual and physical properties\n. This challenge is particularly pronounced for objects made of glass, metal, or other materials whose transparent or reflective surfaces introduce severe out-of-distribution variations. Existing approaches either rely on simulated materials in simulators and perform sim-to-real transfer, which is hindered by substantial visual domain gaps, or depend on collecting extensive real-world demonstrations, which is costly, time-consuming, and still insufficient to cover various materials.\nTo overcome these limitations, we resort to computational photography and introduce\nMutable Material Manipulation Augmentation (M\n3\nA)\n, a unified framework that leverages the physical characteristics of materials as captured by light transport for photometric re-rendering. The core idea is simple yet powerful: given a single real-world demonstration, we\nphotometrically re-render the scene to generate a diverse set of highly realistic demonstrations with different material properties\n. This augmentation effectively decouples task-specific manipulation skills from surface appearance, enabling policies to generalize across materials without additional data collection.\nTo systematically evaluate this capability, we\nconstruct the first comprehensive multi-material manipulation benchmark\nspanning both simulation and real-world environments. Extensive experiments show that the M\n3\nA policy significantly enhances cross-material generalization, improving the average success rate across three real-world tasks by\n58.03%\n, and demonstrating\nrobust performance on previously unseen materials\n.\nFigure 1\n:\nOverview of the proposed\nğŒ\nğŸ‘\nâ€‹\nğ€\n\\mathbf{M^{3}A}\nframework\n, highlighting its significant advantage in material generalization over imitation learning baselines. By synthesizing demonstrations across a wide spectrum of materials, it trains policies that robustly adapt to out-of-distribution (OOD) unseen materials and in both simulation and real-world deployment.\n1\nIntroduction\nRobotic manipulation has recently gained significant attention for enabling general embodied agents\n[\nzeng2018learning\n,\nqin2023dexpoint\n,\nteam2023human\n,\nheng2025vitacformerlearningcrossmodalrepresentation\n,\nopen6dor\n,\nblack2024pi0visionlanguageactionflowmodel\n,\nopen_x_embodiment_rt_x_2023\n,\ngeng2023sage\n,\nzhang2024dexgraspnet20learninggenerative\n]\n, such as household robots and intelligent appliances. Operating in both industrial and household environments, robot agents are required to manipulate objects made of diverse materials (e.g., metal or plastic mugs), performing tasks such as grasping, placing, or pouring under varying visual and physical conditions.\nCurrent learning-based manipulation policies\n[\nshridhar2023perceiver\n,\nzhao2023learning\n]\nmainly rely on visual perception to infer object states and guide control actions, making them highly sensitive to variations in object appearance.\nIn particular, the material properties of objects introduce significant appearance changes, including differences in color, surface roughness, and transparency, which lead to inconsistencies in visual perception\n[\nsajjan2020clear\n,\nli2020through\n,\nverbin2024ref\n]\n, thereby deteriorating manipulation accuracy and potentially causing physical damage.\nThus, developing embodied agents that generalize across diverse materials is essential for reliable real-world deployment.\nTo enhance generalization, existing methods either rely on collecting large-scale real-world demonstrations\n[\nlevine2016end\n,\nkalashnikov2018scalable\n,\npinto2016supersizing\n]\nor adopt sim-to-real transfer using simulated data and domain randomization\n[\ntobin2017domain\n,\nchebotar2019closing\n,\nandrychowicz2020learning\n,\nzhang2023efficient\n]\n.\nA central challenge in material generalization is that learning robust manipulation policies would require demonstrations spanning a wide range of object materials to avoid overfitting. This requirement imposes two major limitations. First, real-world data collection becomes impractical, as acquiring diverse physical objects (e.g., wood, metal, or concrete mugs) and recording large-scale demonstrations are both labor-intensive and time-consuming\n[\ntobin2019real\n]\n. Second, while sim-to-real pipelines can easily render objects with different materials in simulation, the resulting model still suffers from visual discrepancies when transferred to the physical world due to the sim-to-real gap\n[\nzhao2020sim\n,\nwong2025survey\n]\n. This issue is further amplified for material generalization because critical visual cues, e.g., reflectance, transparency, and surface texture, are difficult to simulate with sufficient realism.\nThese limitations motivate us to ask: Can we develop an efficient framework for material-generalized manipulation that reduces data collection requirements while avoiding the sim-to-real gap? To this end, we propose to decouple the sources of material variation from the sources of manipulation demonstrations. Specifically, we encode material properties into compact, transferable representations that can be injected into target objects within any demonstration to alter their material appearance. This enables a single real-world demonstration to be photometrically re-rendered into numerous material variants, as long as the corresponding material representations are available. As a result, we can efficiently generate large-scale real-world mutable-material demonstrations, supporting the training and deployment of material-generalized policies without reliance on laborious data collection or imperfect simulation.\nNevertheless, the key technical challenge lies in obtaining physically plausible representations of diverse materials. Computational photography offers a principled solution to this problem\n[\ndebevec2008rendering\n,\nmallick2005beyond\n,\nboivin2001image\n]\nby explicitly modeling how light interacts with surfaces. A materialâ€™s visual appearance is governed by intrinsic properties, e.g., reflectance, roughness, and translucency, that determine how incoming and outgoing light vary across illumination and viewing conditions. Traditional methods estimate these properties through photometric analysis, multi-view reflectance reconstruction, or high-dynamic-range (HDR) imaging\n[\nlin2019site\n,\nsengupta2019neural\n,\nono2019practical\n,\nlensch2001image\n]\n, yielding spatially varying bidirectional reflectance distribution functions (BRDFs) that describe surface reflectance behavior. More recently, learning-based techniques\n[\ncheng2024zest\n,\ncheng2025marble\n]\nhave enabled material editing in a physically consistent feature space, guided by depth, shading, and surface cues to generate realistic variations in color, glossiness, and transparency. These advancements provide the foundation for producing photorealistic material augmentations, thereby enabling manipulation policies to generalize robustly to previously unseen materials and bridging the visualâ€“physical gap critical for real-world deployment.\nIn this paper, we propose Mutable Material Manipulation Augmentation (M\n3\nA), a highly efficient framework for material-generalized manipulation policies. As shown in Fig.\n1\n, we extract target objects using Grounded-SAM2\n[\nren2024grounded\n]\nguided by the corresponding manipulation task descriptions. Given the target objects and visual appearance of certain materials, M\n3\nA performs physically plausible material transformations on both real-world and simulated demonstrations. This enables a small number of collected demonstrations per task to be expanded into a large-scale, multi-material dataset without additional data collection effort.\nTo systematically assess the material generalization capability of state-of-the-art policies, we construct the standard Mutable Material Manipulation (M\n3\n) benchmark built on the high-fidelity Roboverse simulation platform\n[\ngeng2025roboverse\n]\n. By evaluating policies in both simulation and real-world experiments, the M\n3\nbenchmark ensures that methods performing well in simulation maintain consistent performance in physical environments, providing an efficient and reliable evaluation protocol.\nLeveraging the diverse data generated by the M\n3\nA pipeline, our learned policy exhibits strong material generalization and achieves superior zero-shot performance on unseen materials across several manipulation tasks. In summary, our contributions are threefold:\nâ€¢\nWe introduce M\n3\nA, a simple yet effective framework that enables physically plausible material transformations in both simulation and real-world demonstrations, supporting cross-material generalization for manipulation policies.\nâ€¢\nWe establish the M\n3\nbenchmark, a comprehensive evaluation suite built on high-fidelity simulation and real-world validation, ensuring that policies performing well on the benchmark exhibit consistent capability in physical environments.\nâ€¢\nExtensive experiments in both simulation and the real world show that policies trained with M\n3\nA achieve strong material generalization. Our approach attains zero-shot performance on unseen materials and improves success rates by 58.03% on average across three real-world tasks.\n2\nRelated works\n2.1\nData Augmentation for Robot Learning\nData augmentation is widely used in robotic imitation learning\n[\nhussein2017imitation\n,\nradford2021learning\n,\nwei2024droma\n,\nkuang2024ramretrievalbasedaffordancetransfer\n]\nto enhance robustness without increasing real-world data collection.\nImage-space augmentation methods (e.g., cropping, color jittering, random blur, and viewpoint perturbation) have been shown to improve visual robustness against lighting and camera variations, as demonstrated across several visuomotor learning methods\n[\nzhan2022learning\n,\ngraf2023learning\n,\nnair2022r3m\n]\n.\nBeyond pixel-level transformations,\ngeometry and physics-aware augmentation techniques exploit SE(3) pose perturbations, geometry-aware trajectory modifications, or local physics-informed transformations to increase spatial diversity while preserving action consistency\n[\nmitrano2022data\n,\nzhou2024gears\n,\nhsu2025spot\n]\n. Recently, scene-level counterfactual augmentation strategies modify distractors, backgrounds, object placements, and non-essential texture attributes to improve generalization to novel configurations and cluttered environments\n[\nameperosa2025rocoda\n,\ndasari2019robonet\n]\n.\nThese approaches collectively target variability from illumination, viewpoint, object pose, and scene composition.\nHowever, these methods do not explicitly address material generalization. To address this gap, recent works construct large-scale datasets with diverse material properties, including Robo360\n[\nliang2023robo360\n]\n, GAPartManip\n[\ncui2025gapartmanip\n]\n, and few-shot granular manipulation benchmarks\n[\nzhu2023few\n]\n. These datasets introduce material-level variability in simulation and real-world settings, thus providing richer training distributions for material-aware robotic manipulation. However, they remain limited in generalization to unseen tasks or novel object categories.\n2.2\nMaterial Acquisition and Editing\nMaterial editing in computational photography seeks to modify surface appearance while preserving geometry, enabling visually consistent rendering under realistic illumination.\nExisting methods for inverse rendering can be broadly categorized into single-image approaches that disentangle material properties from a limited observation\n[\nsengupta2019neural\n,\nchen2021invertible\n,\nidema2024neural\n]\nand those leveraging multi-view reconstruction\n[\nboss2021nerd\n,\nliu2023nero\n,\nwu2025neural\n]\n. These physically motivated pipelines have achieved high realism but were computationally demanding and sensitive to geometry and illumination accuracy, limiting their scalability for large-scale data generation.\nSubsequent diffusion-based studies shifted toward semantic and generative paradigms that emphasize controllable, data-driven editing.\nSingle-image exemplar-based approaches\n[\ncheng2024zest\n,\nwang2024diffusion\n]\nleverage diffusion models to transfer material appearance or perform 3D editing from a single image and depth cues.\nMask-preserving methods\n[\nyin2024benchmarking\n,\njiang2025pixelman\n]\nfocus on local attribute editing while maintaining object masks or structural consistency.\nParametric and attribute-controlled frameworks\n[\ncheng2025marble\n,\nvecchio2024matfuse\n,\nzhu2024mcmat\n]\nexploit latent spaces, such as CLIP\n[\nradford2021learning\n]\nor multi-encoder representations, to manipulate fine-grained material properties including roughness, metallicity, and transparency.\nFigure 2\n:\nThe framework of M\n3\nA policy.\nThe framework consists of three stages: (1) demonstration collection, where visuomotor trajectories (videos and action sequences) are collected from simulation or real-world environments; (2) M\n3\nA, which re-composes or replaces the material appearance of manipulated objects to introduce realistic visual diversity; and (3) imitation learning, where policies are trained on the augmented demonstrations to achieve improved generalization across materials and environments.\n3\nMethod\n3.1\nOverview\nAs illustrated inÂ Fig.\n2\n, M\n3\nA provides an efficient framework for training material-generalized policies by generating physically plausible material representations and injecting them into the original demonstrations.\nSpecifically, given a manipulation task, a set of demonstrations\nğ’Ÿ\n=\n{\n(\nğ\ni\n,\nğ€\ni\n)\n}\ni\n=\n1\nN\n\\mathcal{D}=\\{(\\mathbf{O}_{i},\\mathbf{A}_{i})\\}^{N}_{i=1}\nare collected, containing\nN\nN\npaired demonstrations with visual observation\nğ\ni\n=\n{\no\ni\nt\n}\nt\n=\n0\nT\n\\mathbf{O}_{i}=\\{o^{t}_{i}\\}^{T}_{t=0}\nand the corresponding actions\nğ€\ni\n=\n{\na\ni\nt\n}\nt\n=\n0\nT\n\\mathbf{A}_{i}=\\{a^{t}_{i}\\}^{T}_{t=0}\n.\nM\n3\nA augments the collected demonstrations\nğ’Ÿ\n\\mathcal{D}\n, where the real-world material representations are extracted and injected into the target objects within\nğ\ni\n\\mathbf{O}_{i}\n.\nThrough M\n3\nA, realistic material variations, including surface reflectance, texture, and transparency, are introduced to enrich multi-material demonstrations without additional human data collection. Combining the original (\nğ’Ÿ\n\\mathcal{D}\n) and augmented (\nğ’Ÿ\nâ€²\n\\mathcal{D^{\\prime}}\n) demonstrations, the policy trained on M\n3\nbenchmark effectively achieves the improved generalization across diverse materials.\n3.2\nMutable Material Manipulation Augmentation\nPrior studies in computational material perception\n[\nsharan2013recognizing\n]\nshowed that materials can be systematically categorized based on their reflectance behavior rather than simple color or texture cues. More recently, Beveridge et al.\n[\nbeveridge2025hierarchical\n]\nintroduced a hierarchical representation that links local appearance patterns to global material categories, emphasizing that fine-scale reflectance and roughness jointly determine material identity.\nIn robotic manipulation, material-related visual features, like reflectance, roughness, transparency, and specular highlights, hinder generalization to unseen materials when policies are trained on limited distributions.\nBy applying representative materials with distinct reflectance and texture profiles across broad categories, we convert each objectâ€™s single material into a diverse material set whose synthesized appearances remain photometrically close to real unseen ones. This approach reduces the discrepancy between simulated and real materials, and further enables the generation of extensive material-rich data with limited real-world data collection.\nInspired by computational photography, M\n3\nA identifies specific material by its unique visual appearance under different scenarios. Subsequently, realistic augmentation can be achieved by material representations and modifications in visual feature space. Overall, the process of M\n3\nA for the\ni\ni\n-th demonstration can be formulated as:\nğ\ni\nâ€²\n=\nM\n3\nA\nâ€‹\n(\nğ\ni\n,\nğŒ\ni\n,\nğƒ\ni\n,\nf\nz\nm\n)\n,\n\\mathbf{O}_{i}^{\\prime}=\\text{M${}^{3}$A}(\\mathbf{O}_{i},\\mathbf{M}_{i},\\mathbf{D}_{i},f_{z_{m}}),\n(1)\nwhere\nğ\ni\n\\mathbf{O}_{i}\nand\nğ\ni\nâ€²\n\\mathbf{O}^{\\prime}_{i}\nare the original and enhanced observations for the\ni\ni\n-th demonstration,\nğŒ\ni\n=\n{\nm\ni\nt\n}\nt\n=\n0\nT\n\\mathbf{M}_{i}=\\{m^{t}_{i}\\}^{T}_{t=0}\nand\nğƒ\ni\n=\n{\nd\ni\nt\n}\nt\n=\n0\nT\n\\mathbf{D}_{i}=\\{d^{t}_{i}\\}^{T}_{t=0}\nrepresent the masks of the target object and depth maps for each frame, and\nf\nz\nm\nf_{z_{m}}\ndenotes the representation for specific material\nz\nm\nz_{m}\nfrom a set of material exemplars.\nFinally, the augmented demonstration set\nğ’Ÿ\nâ€²\n\\mathcal{D^{\\prime}}\nconsists of enhanced observations and original actions:\nğ’Ÿ\nâ€²\n=\n{\n(\nğ\ni\nâ€²\n,\nğ€\ni\n)\n}\ni\n=\n1\nN\n\\mathcal{D^{\\prime}}=\\{(\\mathbf{O}^{\\prime}_{i},\\mathbf{A}_{i})\\}_{i=1}^{N}\n. Combining two demonstration sets, our M\n3\nbenchmark\nğ’Ÿ\n^\n=\nğ’Ÿ\nâ€²\nâˆª\nğ’Ÿ\n\\mathcal{\\widehat{D}}=\\mathcal{D^{\\prime}}\\cup\\mathcal{D}\nenables learning material-generalized policies without additional data collection burden. In the following, we elaborate on the motivations and technical details for integrating each component.\nMask Extraction.\nFor a specific real-world manipulation task, the material typically varies only for the target object, whereas the materials of task-irrelevant objects and environments remain unchanged. To this end, we extract task-relevant foreground masks to enable the precise transfer of realistic materials to target objects.\nTechnically, to generate task-relevant masks, the Grounded-SAM2\n[\nren2024grounded\n]\n, a powerful Vision-Language segmentation model, is utilized to segment target objects that are semantically grounded in the task specification. The process can be formulated as:\nğŒ\ni\n=\nâ„³\nâ€‹\n(\nğ\ni\n,\np\n)\n,\n\\mathbf{M}_{i}=\\mathcal{M}(\\mathbf{O}_{i},p),\n(2)\nwhere\nâ„³\nâ€‹\n(\nâ‹…\n,\nâ‹…\n)\n\\mathcal{M}(\\cdot,\\cdot)\nrefers to the Grounded-SAM2 to provide foreground masks\nğŒ\ni\n\\mathbf{M}_{i}\n, given all observations\nğ\ni\n\\mathbf{O}_{i}\nand a task prompt\np\np\nfor the\ni\ni\n-th demonstration. The task prompt\np\np\ncan be either a textual description (e.g., â€œthe red cubeâ€) or a visual prompt (e.g., a key point or bounding box to highlight the target object). Furthermore, by taking the whole sequence\nğ\ni\n\\mathbf{O}_{i}\nas input, the consistency of\nğŒ\ni\n\\mathbf{M}_{i}\nis enhanced by referring to the correlations among observations.\nDepth Map Estimation.\nIn real-world scenarios, even objects with an identical material can appear different due to the geometrical variations, such as lighting positions and shapes. To address this issue, we incorporate depth images to provide geometric priors about both the object and the environment. The geometric information enables M\n3\nA to simulate material appearance variations across different scenarios, ensuring realistic multi-material augmentation.\nIn simulators, physically accurate depth images are available.\nHowever, in real-world settings, obtaining accurate depth information is challenging due to the limitations of current depth cameras towards diverse scenarios\n[\nhaider2022can\n]\n.\nAlternatively, we use DPT-Hybrid (MiDaS), a depth prediction foundation model pretrained on large-scale data, to estimate robust depth images for each RGB observation:\nğƒ\ni\n=\n{\nğ’Ÿ\nâ€‹\n(\no\ni\nt\n)\n,\no\ni\nt\nâˆˆ\nğ\ni\n}\n.\n\\mathbf{D}_{i}=\\{\\mathcal{D}(o_{i}^{t}),o_{i}^{t}\\in\\mathbf{O}_{i}\\}.\n(3)\nMaterials Transfer.\nTo simulate diverse material properties in the physical world, we establish an exemplar materials set\nğ™\n=\n{\nz\nm\n}\nm\n=\n1\nN\nz\n\\mathbf{Z}=\\{z_{m}\\}_{m=1}^{N_{z}}\n, where each material corresponds to a texture image\nz\nm\nz_{m}\n.\nThe CLIP vision encoder\nÏ•\nCLIP\nâ€‹\n(\nâ‹…\n)\n\\phi_{\\text{CLIP}}(\\cdot)\n[\nradford2021learning\n]\nand an IP-Adapter\nÎµ\nIP\nâ€‹\n(\nâ‹…\n)\n\\varepsilon_{\\text{IP}}(\\cdot)\n[\nye2023ip\n]\nare then employed to extract visual features from texture images, serving as the unique representation for each material:\nf\nz\nm\n=\nÎµ\nIP\nâ€‹\n(\nÏ•\nCLIP\nâ€‹\n(\nz\nm\n)\n)\n.\nf_{z_{m}}=\\varepsilon_{\\text{IP}}(\\phi_{\\text{CLIP}}(z_{m})).\n(4)\nAs shown in Eq.\n1\n, we randomly sample a material feature,\nf\nz\nm\nf_{z_{m}}\n, and inject it into the bottleneck layer of a U-Net-based Stable Diffusion model\n[\nrombach2022high\n]\nto inpaint a novel material onto the target object in\nğ\ni\n\\mathbf{O}_{i}\n.\nThe final demonstration set is the combination of the original and augmented demonstrations with shared actions:\nğ’Ÿ\n^\n=\n{\n(\nğ\ni\nâ€²\n,\nğ€\ni\n)\n}\ni\n=\n1\nN\nâˆª\nğ’Ÿ\n\\mathcal{\\widehat{D}}=\\{(\\mathbf{O}^{\\prime}_{i},\\mathbf{A}_{i})\\}_{i=1}^{N}\\cup\\mathcal{D}\n.\nNotably, the M\n3\nA pipeline can convert a single target object into multiple material appearances by simply varying the reference image,\nz\nm\nz_{m}\n. This enables efficient scaling of material types, resulting in a comprehensive multi-material manipulation (M\n3\n) benchmark. Benefiting from data diversity, policies trained on the M\n3\nbenchmark are compelled to rely on material-agnostic geometric invariants (e.g., grasp points or edge contours) to perform manipulation, thereby achieving material generalization.\n3.3\nPolicy Training\nM\n3\nA is an efficient and general augmentation pipeline that can be used as a plug-and-play module for training material-generalized policies. In this work, we focus on diffusion-based policies\n[\nchi2025diffusion\n]\n, trained under the imitation learning paradigm. Mathematically, our goal is to learn a policy\nÏ€\nÎ¸\nâ€‹\n(\na\nt\n|\no\nt\n)\n\\pi_{\\theta}(a_{t}|o_{t})\nfrom the augmented demonstration set\nğ’Ÿ\n^\n\\mathcal{\\widehat{D}}\n, where the\ni\ni\n-th trajectory is denoted as\nÏ„\ni\n=\n{\no\nt\nâ€²\n,\na\nt\n}\nt\n=\n0\nT\n\\tau_{i}=\\{o^{\\prime}_{t},a_{t}\\}_{t=0}^{T}\n. For simplicity, we omit the trajectory index\ni\ni\nin the following.\nDiffusion-based policies formulate action prediction as a conditional denoising process over observations. During training, a random Gaussian noise\nÏµ\nK\n\\epsilon^{K}\nis added to the noise-free actions\na\nt\na_{t}\nin\nÏ„\ni\n\\tau_{i}\n, producing noisy actions\na\nt\nK\na_{t}^{K}\n. The policy then learns to iteratively predict and remove the noise over\nK\nK\nsteps to recover the original actions. Specifically, at the\nk\nk\n-th iteration, the policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nis trained to predict the added noise\nÏµ\nk\n\\epsilon^{k}\nby minimizing the following objective:\nâ„’\nD\nâ€‹\nP\n=\nâˆ¥\nÏµ\nk\nâˆ’\nÏ€\nÎ¸\nâ€‹\n(\na\nt\nk\n,\nk\n,\no\nt\nâ€²\n)\nâˆ¥\n2\n.\n\\mathcal{L}_{DP}=\\lVert\\epsilon^{k}-\\pi_{\\theta}(a_{t}^{k},k,o^{\\prime}_{t})\\rVert^{2}.\n(5)\nBy conditioning on the augmented observation\no\nt\nâ€²\no^{\\prime}_{t}\n, the diffusion-based policy learns action patterns that are invariant to material variations, thereby achieving material-generalized manipulation.\nFigure 3\n:\nMaterial transfer results produced by M\n3\nA in both simulation and the real world.\nThe top row shows the original camera observations, while the bottom row presents the corresponding material-transferred outputs. The four examples illustrate: (1) red plastic to wood, (2) dark gray plastic to metal, (3) white plastic to glass, and (4) white plastic to gemstone.\nTable 1\n:\nSuccess rates for the\nPickCube\ntask across different materials in simulation experiment.\n\\rowcolor\ngray!15\nMethods\nOverall\nMetal\nWood\nFabric\nPlastic\nStone\nGlass\nLeather\nGems\nCeramic\nPaint\nPaper\nOther\nDP\n11.3%\n10.6%\n13.8%\n17.5%\n11.3%\n10.6%\n7.5%\n11.9%\n10.6%\n10.6%\n17.5%\n10.6%\n10.6%\nDP-Render\n21.9%\n18.1%\n21.9%\n19.4%\n21.3%\n21.9%\n21.3%\n20.0%\n20.6%\n21.3%\n16.9%\n20.6%\n18.1%\nDP-M\n3\nA\n34.4%\n30.6%\n36.9%\n31.9%\n29.4%\n33.8%\n27.5%\n27.5%\n24.4%\n33.1%\n31.3%\n32.5%\n31.9%\n3.4\nBenchmark Design and Evaluation\nTo fairly evaluate the material generalization capability of different policies, we establish a Mutable Material Manipulation (M\n3\n) benchmark built upon RoboVerse\n[\ngeng2025roboverse\n]\n, an open-source platform that supports high-fidelity robotic manipulation tasks in simulation.\nAs a result, the policies that achieve high performances on our benchmark can be considered to achieve comparable material generalization capability in the physical world.\nOur benchmark is designed to answer two principal research questions:\nâ€¢\nSimulation Rendering vs. Computational Photography: can computational photography enhance material generalization by mitigating the sim-to-real visual gap?\nâ€¢\nZero-shot in material domain: can a policy acquire zero-shot capability regarding materials for real-world manipulation tasks after seeing a diverse set of objects with extensive materials?\nTo this end, in simulation settings, the benchmark compares policies trained using conventional simulation renderings against those augmented with computational photography in M\n3\nA across multiple tasks. In real-world settings, the benchmark evaluates policies trained from demonstrations involving physical objects with diverse materials and those incorporating M\n3\nA, measuring their generalization ability and zero-shot performance on material domain.\n4\nExperiments\nTo evaluate the effectiveness of the proposed M\n3\nA framework in improving material generalization for robotic manipulation, we conduct comprehensive experiments in both simulation and real-world environments. The experiments are designed to assess how well one policy adapts to objects with varying material properties, such as surface texture, reflectance, and color. The primary evaluation metric is the manipulation success rate across different material domains, reflecting the policyâ€™s generalization capability.\nFor the M\n3\nA implementation, we first collect demonstrations with simple baseline materials (e.g., plastic). We then apply M\n3\nA transfer to the observation images of these demonstrations to generate a rich set of multi-material training data. The resulting transferred materials are illustrated in Fig.\n3\n, demonstrating the ability of M\n3\nA to produce demonstrations with realistic and diverse materials.\n4.1\nSimulation experiments\n4.1.1\nFramework Overview\nAll simulation experiments are conducted using the RoboVerse platform\n[\ngeng2025roboverse\n]\n, which unifies a wide range of robotic manipulation tasks across multiple robotic arms and provides consistent evaluation protocols and a unified API for common simulators such as IsaacLab\n[\nmittal2023orbit\n]\nand MuJoCo\n[\ntodorov2012mujoco\n]\n. We primarily employ IsaacLab for our experiments due to its high-fidelity rendering and ability to enable material randomization, both of which are essential for generating realistic material appearances and interactions.\n4.1.2\nExperimental Setup\nTask Descriptions.\nIn the simulation, a Franka Emika Panda robotic arm is employed to evaluate our method on three manipulation tasks to assess material generalization:\nâ€¢\nPickCube\n. This task requires the robot to pick up a textured cube. Its primary purpose is to rigorously evaluate the modelâ€™s generalization to novel, unseen materials, isolating appearance variation from geometric complexity.\nâ€¢\nStackCube\n. This task involves picking up a cube and placing it on another. It tests the methodâ€™s effectiveness in a dynamic task where visual appearance and precise placement must be coordinated.\nâ€¢\nCloseBox\n. This task requires closing a box lid, a motion that involves contact with a daily object, assessing the methodâ€™s ability beyond simple cube manipulation.\nBenchmarking Policies.\nWe collect expert demonstration trajectories from three distinct sources to train three policies:\n(1) DP. Demonstrations containing objects with default materials given by RoboVerse.\n(2) DP-Render. Demonstrations from the original environment, modified with varied material and lighting conditions through RoboVerse to increase basic visual diversity.\n(3) DP-M\n3\nA. Demonstrations produced by our M\n3\nA framework, which transfers realistic materials to the manipulated objects while strictly preserving the motion trajectory consistency.\nTraining Configuration.\nAll three policies are trained using DP\n[\nchi2025diffusion\n]\n. We train all policies for 150 epochs using a learning rate of\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\nand Adam optimizer\n[\nkingma2014adam\n]\n.\n4.1.3\nExperimental Results\nMaterial-wise Generalization.\nFor the PickCube task, all materials are first grouped into twelve categories.\nA total of 160 trajectories collected within the RoboVerse environment are used as base demonstrations to provide action labels for training three benchmark policies.\nWe then evaluate each policyâ€™s performance separately on each material category.\nBesides, the overall performance is computed on a fixed set of materials, including samples from all categories.\nFrom the quantitative results inÂ Tab.\n1\n, the proposed M\n3\nA policy outperforms the other methods, DP and DP-Render.\nThe original DP exhibits notable performance degradation on the material categories with specular reflections or complex textures, revealing a critical dependency on the appearance characteristics.\nNotably, while the Rendered baseline provides a marginal average improvement by introducing basic visual variability, its gains are inconsistent and fail to generalize robustly across all material types.\nIn contrast, the proposed DP-M\n3\nA framework achieves superior success rates in every material category, increasing about 12.5% success rate than DP-Render.\nThis consistent performance uplift, especially on challenging materials like metals and transparent surfaces, demonstrates that the computational photography technology can improve the accuracy of robotic policy due to the more realistic material appearances than those rendered from simulators.\nThe results confirm that the proposed M\n3\nA is effective in improving material generalization capability of robotic policy.\nEvaluation on Manipulation Tasks.\nThe effectiveness of our M\n3\nA framework extends beyond material-specific generalization to enhance robustness across diverse manipulation tasks, as summarized in Tab.\n2\n. On both the StackCube and CloseBox tasks, which involve dynamic multi-object interaction and articulation, policies trained with our augmented demonstrations consistently outperform those trained on original data. The performance improvement is particularly significant as these tasks integrate geometric, spatial, and physical reasoning alongside visual perception.\nBy exposing the policy to the diverse realistic material appearances during training, the policy focuses more on task-relevant geometric and physical features, rather than overfitting to specific visual correlations.\nTable 2\n:\nComparison of success rates between DP and our M\n3\nA method across three simulated manipulation tasks.\nMethods\nAverage\nPickCube\nCloseBox\nStackCube\nDP\n10.16%\n11.3%\n16.7%\n2.5%\nDP-M\n3\nA\n22.80%\n34.4%\n27.1%\n6.9%\n4.2\nReal World Experiments\nTable 3\n:\nComparison of real-world performance across three cube-manipulation tasks involving eleven material types.\nMethods\nAverage\nWhite\nBeech\nRubber\nWool\nSilk\nFoam\nGlass\nMirror\nWalnut\nLeather\nFlash\n\\rowcolor\ngray!15\nPicking Task\nDP\n22.35%\n100.0%\n4.2%\n12.5%\n45.8%\n12.5%\n0.0%\n4.2%\n37.5%\n8.3%\n4.2%\n16.7%\nDP-6\n48.86%\n87.5%\n87.5%\n62.5%\n62.5%\n75.0%\n87.5%\n12.5%\n25.0%\n12.5%\n12.5%\n12.5%\nDP-M\n3\nA\n89.40%\n95.8%\n66.7%\n100.0%\n100.0%\n100.0%\n87.5%\n75.0%\n100.0%\n79.2%\n79.2%\n100.0%\n\\rowcolor\ngray!15\nPicking & Placing Task\nDP\n30.68%\n100.0%\n0.0%\n100.0%\n100.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n37.5%\nDP-6\n59.09%\n100.0%\n87.5%\n100.0%\n87.5%\n37.5%\n62.5%\n75.0%\n0.0%\n25.0%\n0.0%\n75.0%\nDP-M\n3\nA\n68.18%\n100.0%\n87.5%\n87.5%\n87.5%\n75.0%\n87.5%\n37.5%\n12.5%\n25.0%\n50.0%\n100.0%\n\\rowcolor\ngray!15\nLong-horizon Picking & Placing Task\nDP\n24.24%\n91.7%\n8.3%\n58.3%\n25.0%\n41.7%\n0.0%\n0.0%\n8.3%\n0.0%\n0.0%\n33.3%\nDP-6\n57.57%\n91.7%\n66.7%\n91.7%\n66.7%\n83.3%\n100.0%\n0.0%\n83.3%\n0.0%\n8.3%\n41.7%\nDP-M\n3\nA\n93.94%\n91.7%\n100.0%\n91.7%\n83.3%\n91.7%\n83.3%\n100.0%\n91.7%\n100.0%\n100.0%\n100.0%\n4.2.1\nExperimental Setup\nFigure 4\n:\nReal-world experiment settings.\nThe FR3 manipulates cubes with eleven different materials to finish three tasks: (1) Picking, (2) Picking & placing, (3) Long-horizon picking & placing.\nFor real-world experiments, as shown inÂ Fig.\n4\n, we use\n5\nÃ—\n5\nÃ—\n5\n5\\times 5\\times 5\ncm cubes with 11 diverse materials in three robotic manipulation tasks, enabling the evaluation of material generalization under consistent geometry.\nTask Descriptions.\nThe details of three real-world tasks: (1)\nPicking\n, (2)\nPicking & Placing\n, and (3)\nLong-Horizon Picking & Placing\n, are elaborated as follows:\nâ€¢\nPicking\n. The robotic arm picks a cube of specific material from random positions with a clean background.\nâ€¢\nPicking & Placing\n. The robotic arm picks a cube of specific material and places them into the target plate with a messy environment with distractors.\nâ€¢\nLong-horizon Picking & Placing\n. The robotic arm first grasps and opens a drawer, picks a cube of a specific material from the table, and places it inside the drawer.\nExperimental Configurations.\nFor the hardware, a Franka Emika Research 3 (FR3) and two RealSense D455 cameras are employed for both demonstration collection and manipulation. For the software, we follow the configuration in HIL-SERL\n[\nluo2025precise\n]\n, and run the control system on a PC equipped with an NVIDIA RTX 5080 GPU (16 GB).\nBenchmarking Policies.\nThree kinds of DP are compared: (1) DP is trained only with a white plastic cube in demonstrations, (2) DP-6 is trained using demonstrations with cubes of six materials, and (3) our DP-M\n3\nA is trained with demonstrations augmented by the proposed M\n3\nA.\nNotably, all material images used for augmentation in M\n3\nA are collected from the web and exhibit discrepancies compared to their real-world visual appearances.\nThus, the performance of DP (M\n3\nA) in real-world settings reflects its zero-shot capability on materials in the physical environment.\n4.2.2\nExperimental Results\nThe real-world experimental results are summarized in Tab.\n4.2\n.\nSpecifically, the underlined\nmaterials\nindicate those used in demonstrations to train DP-6 while the remaining materials are unseen during DP-6 training.\nThe proposed M\n3\nA strategy substantially enhances generalization on materials, achieving the highest average success rates of 89.40%, 68.18%, and 93.94% in the respective tasks.\nPicking.\nBoth DP and DP-6 perform well on seen materials, attaining 100% and 75% success rates, respectively. However, their performance drops sharply on unseen materials, reaching only on an average of 15% on materials excluding white for DP and and 15% on unseen materials for DP-6. The significant drop of success rate presents their weakness in material generalization capabilities.\nIn contrast, the proposed DP-M\n3\nA maintains consistent performance across all materials, achieving more than 75.0% success rate in most of the materials, despite being trained solely on augmented data collected from online materials instead of the data collected in the real world.\nThis demonstrates the strong generalization ability of the M\n3\nA framework.\nPicking & Placing.\nThe robotic manipulation faces the problems of distractors, limiting the performance of DP. As we can see from the table, DP and DP-6 fail to pick cubes in some materials at all, with 0% success rate.\nHowever, after augmenting the demonstrations, the DP-M\n3\nA can achieve manipulating the cubes with these materials, reflecting the effectiveness of the proposed M\n3\nA framework.\nHowever, the performance of Task 2 is not as high as that of Task 1. This may result from the imprecise mask prediction and depth estimation due to the clustered environments.\nLong-horizon Picking & Placing.\nThe policies face the substantial challenges for DP and material augmentation due to their accumulated errors and strong temporal dependencies.\nRemarkably, however, the DP-M\n3\nA achieves an overall 93.94% success rate on all provided materials, outperforming the origin DP and DP-6, validating the effectiveness and robustness of our method.\nFor the DP and DP-6, the success rate drops (91.7% to 17.5% and 83.4% to 26.7%) also happen in this tasks.\nIn the real-world experiments, the traditional DP algorithm performs reliably on objects with seen materials but shows clear limitations when encountering unseen materials.\nBy contrast, the proposed M\n3\nA framework significantly enhances material generalization through realistic computational-photography rendering.\nNotably, even though M\n3\nA relies solely on online material images, the resulting DP-M\n3\nA policy still achieves strong performance on real-world objects, exhibiting a clear zero-shot capability. These findings demonstrate that computational-photography-based material augmentation can effectively transfer to the real world and equip robotic policies with robust zero-shot material generalization.\n5\nConclusion\nIn this work, we present a unified framework for material-generalized robotic manipulation, bridging the gap between visual diversity and task adaptability. By drawing inspiration from computational photography, we introduce a material editing mechanism that effectively decouples manipulation skills from material appearances, enabling efficient augmentation of imitation learning data. Furthermore, we establish a systematic benchmark to evaluate cross-material generalization and verify our approach across both simulated and real environments. Extensive results demonstrate that our method achieves substantial gains in success rate and robustness, particularly on unseen materials, highlighting its potential for scalable and material-agnostic robotic learning in the real world.\nHowever, the proposed method still has limitations. In real-world settings, we observe that the accuracy and consistency of material transfer are influenced by the mask quality, particularly in messy environments with a cluster of distractors. In the future, we aim to improve mask prediction to solve this issue.\n\\thetitle\nSupplementary Material\nSupplementary Experiment\nAs a supplementary analysis to the simulation experiments, we evaluated the performance of the DP-M\n3\nA method across different training epochs, as shown in the figure\n5\n.\nFigure 5\n:\nSuccess rate of simulation tasks under varying DP training epochs.\nIn the PickCube and CloseBox tasks, the performance improved with increasing training epochs up to 150 epochs. However, after 150 epochs, additional training resulted in a decrease in success rate. For the StackCube task, the success rate was 0 for fewer than 100 training epochs, but as the number of epochs increased, the success rate improved, reaching higher levels within 200 epochs. This difference across tasks may be due to the higher complexity of the StackCube task compared to the PickCube and CloseBox tasks, where fewer training epochs are insufficient for the robotic arm to learn the necessary features and strategies effectively.\nExperiment Videos\nFor all the simulations and real-world experiments mentioned in the paper, we provide corresponding video files that demonstrate the successful execution of the tasks, showcasing the effectiveness of the M\n3\nA method across different scenarios.\nSimulation Tasks.\nFor the simulation tasks, we offer the following video files, each demonstrating the successful execution of the tasks under three different kinds of materials:\nâ€¢\nCloseBox_simulation.mp4\n: A silent video showing the CloseBox task.\nâ€¢\nPickCube_simulation.mp4\n: A silent video displaying the PickCube task.\nâ€¢\nStackCube_simulation.mp4\n: A silent video illustrating the StackCube task.\nReal-World Experiments.\nSimilarly, for the real-world experiments, we provide video files that show the successful execution of tasks on physical cubes made of multiple materials:\nâ€¢\npicking.mp4\n: A silent video demonstrating the execution of the Picking task.\nâ€¢\npicking_and_placing.mp4\n: A silent video showcasing the performance in the Picking & Placing task.\nâ€¢\nlong-horizon.mp4\n: A silent video illustrating the process of the Long-horizon Picking & Placing task.",
    "preview_text": "Material generalization is essential for real-world robotic manipulation, where robots must interact with objects exhibiting diverse visual and physical properties. This challenge is particularly pronounced for objects made of glass, metal, or other materials whose transparent or reflective surfaces introduce severe out-of-distribution variations. Existing approaches either rely on simulated materials in simulators and perform sim-to-real transfer, which is hindered by substantial visual domain gaps, or depend on collecting extensive real-world demonstrations, which is costly, time-consuming, and still insufficient to cover various materials. To overcome these limitations, we resort to computational photography and introduce Mutable Material Manipulation Augmentation (M$^3$A), a unified framework that leverages the physical characteristics of materials as captured by light transport for photometric re-rendering. The core idea is simple yet powerful: given a single real-world demonstration, we photometrically re-render the scene to generate a diverse set of highly realistic demonstrations with different material properties. This augmentation effectively decouples task-specific manipulation skills from surface appearance, enabling policies to generalize across materials without additional data collection. To systematically evaluate this capability, we construct the first comprehensive multi-material manipulation benchmark spanning both simulation and real-world environments. Extensive experiments show that the M$^3$A policy significantly enhances cross-material generalization, improving the average success rate across three real-world tasks by 58.03\\%, and demonstrating robust performance on previously unseen materials.\n\nğŒ\nğŸ‘\nâ€‹\nğ€\n\\mathbf{M^{3}A}\nPolicy: Mutable Material Manipulation Augmentation Policy through Photometric Re-rendering\nJiayi Li\n1,2,âˆ—\n, Yuxuan Hu\n2\n, Haoran Geng\n3\n, Xiangyu Chen\n2\n, Chuhao Zhou\n2\n,\nZiteng Cui\n4\nand Jianfei Yang\n2,â€ \n1\nTsinghua University\n",
    "is_relevant": null,
    "relevance_score": 0.0,
    "extracted_keywords": [],
    "one_line_summary": "",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T09:31:10Z",
    "created_at": "2026-01-10T10:30:38.601183",
    "updated_at": "2026-01-10T10:30:38.601193",
    "flag": true
}