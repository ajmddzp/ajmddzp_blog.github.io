{
    "id": "2512.01715v1",
    "title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models",
    "authors": [
        "Wanpeng Zhang",
        "Ye Wang",
        "Hao Luo",
        "Haoqi Yuan",
        "Yicheng Feng",
        "Sipeng Zheng",
        "Qin Jin",
        "Zongqing Lu"
    ],
    "abstract": "åŸºäºæµåŒ¹é…è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œä½†å…¶æ€§èƒ½åœ¨åˆ†å¸ƒåç§»å’Œå¤æ‚å¤šæ­¥éª¤ä»»åŠ¡ä¸­å¸¸å‡ºç°é€€åŒ–ï¼Œè¡¨æ˜æ‰€å­¦è¡¨å¾å¯èƒ½æœªèƒ½ç¨³å¥æ•æ‰ä»»åŠ¡ç›¸å…³è¯­ä¹‰ã€‚æˆ‘ä»¬æå‡ºDiG-Flowâ€”â€”ä¸€ä¸ªé€šè¿‡å‡ ä½•æ­£åˆ™åŒ–å¢å¼ºVLAé²æ£’æ€§çš„ç†è®ºæ¡†æ¶ã€‚æ ¸å¿ƒæ´è§åœ¨äºï¼šè§‚æµ‹ä¸åŠ¨ä½œåµŒå…¥é—´çš„åˆ†å¸ƒå·®å¼‚èƒ½æä¾›æœ‰æ„ä¹‰çš„å‡ ä½•ä¿¡å·â€”â€”è¾ƒä½çš„ä¼ è¾“æˆæœ¬è¡¨å¾å…¼å®¹æ€§ï¼Œè€Œè¾ƒé«˜æˆæœ¬åˆ™æš—ç¤ºæ½œåœ¨é”™ä½ã€‚DiG-Flowé€šè¿‡ä¸‰ä¸ªæ­¥éª¤å®ç°ï¼šè®¡ç®—è§‚æµ‹ä¸åŠ¨ä½œåµŒå…¥ç»éªŒåˆ†å¸ƒçš„å·®å¼‚åº¦é‡ï¼Œé€šè¿‡å•è°ƒå‡½æ•°å°†å…¶æ˜ å°„ä¸ºè°ƒåˆ¶æƒé‡ï¼Œå¹¶åœ¨æµåŒ¹é…å‰å¯¹è§‚æµ‹åµŒå…¥æ–½åŠ æ®‹å·®æ›´æ–°ã€‚å…³é”®çš„æ˜¯ï¼Œè¿™ç§å¹²é¢„åœ¨è¡¨å¾å±‚é¢è¿ä½œï¼Œæ— éœ€ä¿®æ”¹æµåŒ¹é…è·¯å¾„æˆ–ç›®æ ‡å‘é‡åœºã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºè¯æ˜ï¼šå·®å¼‚å¼•å¯¼çš„è®­ç»ƒèƒ½ä¸¥æ ¼é™ä½è®­ç»ƒç›®æ ‡å‡½æ•°ï¼Œä¸”å¼•å¯¼çš„æ¨æ–­ä¼˜åŒ–è¿‡ç¨‹å…·æœ‰æ”¶ç¼©æ”¶æ•›æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒDiG-Flowèƒ½ä»¥å¯å¿½ç•¥çš„å¼€é”€é›†æˆåˆ°ç°æœ‰VLAæ¶æ„ä¸­ï¼ŒæŒç»­æå‡æ¨¡å‹æ€§èƒ½ï¼Œåœ¨å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡åŠæœ‰é™è®­ç»ƒæ•°æ®åœºæ™¯ä¸‹æå‡å°¤ä¸ºæ˜¾è‘—ã€‚",
    "url": "https://arxiv.org/abs/2512.01715v1",
    "html_url": "https://arxiv.org/html/2512.01715v1",
    "html_content": "\\cthnewtheorem\ntheoremTheorem\n\\cthnewtheorem\nassumptionAssumption\n\\cthnewtheorem\nlemmaLemma\n\\webpage\nhttps://beingbeyond.github.io/DiG-Flow\nDiG-Flow\n: Discrepancy-Guided Flow Matching for Robust VLA Models\nWanpeng Zhang\n1,2\nYe Wang\n2,3\nHao Luo\n1,2\nHaoqi Yuan\n1,2\nYicheng Feng\n1,2\nSipeng Zheng\n2\nQin Jin\n3\nZongqing Lu\n1,2,â€ \n1\nPeking University\n2\nBeingBeyond\n3\nRenmin University of China\nAbstract\nVision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce\nDiG-Flow\n, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment.\nDiG-Flow\ncomputes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically,\nDiG-Flow\nintegrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.\n\\checkdata\n[Date]Dec 1, 2025\n2\n2\nfootnotetext:\nCorrespondence to Zongqing Lu\n<\n<\nlu@beingbeyond.com\n>\n>\n.\n1\nIntroduction\nVision-Language-Action models represent a paradigm shift in robotic learning by leveraging pretrained vision-language representations to enable flexible, instruction-following manipulation policies\n[\n1\n,\n2\n,\n3\n]\n. By combining powerful vision-language backbones with flow matching or diffusion-based action generation, these models have achieved impressive success across diverse tasks. However, recent studies reveal concerning fragility: performance degrades substantially under modest distribution shifts such as lighting changes, texture variations, or camera angle perturbations\n[\n2\n,\n4\n]\n. This problem is particularly pronounced on complex tasks where sequential decisions compound, and errors in early steps cascade into failure.\nThe fundamental question is whether the learned representations robustly capture task-relevant semantics or inadvertently encode spurious patterns. Flow matching provides an elegant framework for learning action distributions by regressing neural vector fields\n[\n5\n,\n3\n]\n, yet the regression objective alone may not sufficiently incentivize semantically grounded representations. We propose a complementary perspective: the geometric relationship between observation features and action embeddings reveals representational quality. When observation features from the vision-language backbone and action embeddings from the policy head exhibit low distributional discrepancy, their representations are geometrically compatible, suggesting semantic coherence. Conversely, high discrepancy signals potential misalignment that may indicate spurious patterns or out-of-distribution observations.\nTo solve this problem, we introduce the\nDiG-Flow\nframework with 3 main components: a discrepancy function that quantifies distributional distance, a monotone weight mapping that transforms discrepancy into a modulation factor, and a lightweight residual operator that adjusts observation features. The framework defaults to Wasserstein distance for its geometric interpretability but accommodates alternative discrepancies. Importantly,\nDiG-Flow\nintervenes at the representation level before flow matching, leaving the probability path and target vector field unchanged. This design enables seamless integration into existing architectures while providing theoretical guarantees on training dynamics and inference convergence.\nOur contributions are threefold. First, we establish a discrepancy-guided framework for robust VLA learning. Second, we discuss theoretical motivation for the general problem and provide theoretical guarantee for our method design. Third, we validate that\nDiG-Flow\nconsistently improves performance across both simulation and real-world benchmarks, with pronounced gains on complex multi-step tasks and in data-limited scenarios, while maintaining negligible computational overhead.\n2\nRelated Work\nVision-Language-Action Models.\nVision Language Models (VLMs)\n[\n6\n,\n7\n]\ntypically combine LLM reasoning\n[\n8\n,\n9\n,\n10\n]\nwith modal-specialized encoders\n[\n11\n,\n12\n,\n13\n,\n14\n]\nfor unified multimodal understanding\n[\n15\n,\n16\n]\n.\nPioneering works\n[\n17\n,\n18\n,\n19\n,\n20\n]\ndemonstrated strong few-shot instruction-following capability. With these visual backbones, VLA models extend them to generate robot actions from visual observations and language instructions, representing a paradigm shift in robot learning\n[\n1\n,\n21\n,\n2\n,\n3\n,\n22\n]\n. Early work like RT-1\n[\n1\n]\ndemonstrated transformer-based architectures could scale to diverse tasks, while RT-2\n[\n21\n]\nshowed web-scale vision-language pretraining improves robotic control. Recent advances follow two primary approaches: (1)\nDirect action prediction\n: OpenVLA\n[\n2\n]\nscales to 7B parameters with open-source datasets, Pi0\n[\n3\n]\nand its enhanced version Pi0.5\n[\n4\n]\nuse flow matching for continuous control, while models like Octo\n[\n23\n]\nprovide generalist policies across embodiments; (2)\nVisual prediction\n: PaLM-E\n[\n22\n]\nembeds sensors into language models, GR-3\n[\n24\n,\n25\n]\nuses video pretraining for action generation. Specialized variants address specific challenges: SpatialVLA\n[\n26\n]\nincorporates 3D spatial representations, Otter builds fine-grained visual feature extraction\n[\n27\n]\n, Pi0-Fast\n[\n28\n]\nimproves inference efficiency through action tokenization, gr00t-N1\n[\n29\n]\ntargets humanoid control, and CoT-VLA\n[\n30\n]\nadds visual chain-of-thought reasoning. Being-H0\n[\n31\n]\nincorporates physical instruction tuning for better motion understanding. OneTwoVLA proposes a unified architecture for better adaptive reasoning\n[\n32\n]\n. Despite these advances, long-horizon performance and robustness to distributional drift remain open challenges, which we address through geometric regularization.\nFlow Matching and Optimal Transport.\nFlow Matching\n[\n5\n]\ntrains neural ODEs without simulation, directly learning vector fields that transform distributions. This paradigm improves on Continuous Normalizing Flows\n[\n33\n,\n34\n,\n35\n]\nwhich require expensive ODE solving. Recent developments include Conditional Flow Matching\n[\n36\n]\nfor simplified training with conditional distributions, Rectified Flow\n[\n37\n]\nfor straighter transport paths, and Stochastic Interpolants\n[\n38\n]\nunifying flows and diffusions. OT-CFM\n[\n39\n,\n40\n]\nuses optimal transport for trajectory optimization, finding better paths between distributions at the level of flow matching dynamics.\nOptimal transport theory\n[\n41\n,\n42\n]\nprovides principled tools for comparing distributions geometrically. Computational advances have made OT practical for machine learning: Sinkhorn distances\n[\n43\n]\nadd entropic regularization for efficient approximation, while Sliced Wasserstein\n[\n44\n,\n45\n]\nprojects to 1D for linear complexity in the number of samples. Our key innovation differs from prior OT applications in VLA: rather than using optimal transport to modify the flow matching trajectory (as in OT-CFM), we use Wasserstein distance as an\nauxiliary signal\nto modulate representation learning. This representation-side intervention complements the flow matching objective and enables robustness to distributional drift without altering the fundamental action generation dynamics.\nRobustness in Robot Learning.\nRobustness to distribution shift and spurious correlations has been studied across robot learning contexts\n[\n46\n]\n. Domain randomization techniques\n[\n47\n]\nattempt to improve generalization by training under diverse conditions, while domain adaptation methods\n[\n48\n]\nexplicitly align distributions between source and target environments. Recent work has explored ways to identify and mitigate reliance on spurious features through causal reasoning\n[\n49\n]\nor by learning invariant representations\n[\n50\n]\n. However, these approaches typically operate during training data collection\n[\n51\n]\nor through architectural constraints, rather than providing adaptive mechanisms that respond to distributional drift during deployment. Our geometric modulation approach offers a complementary perspective: by dynamically measuring and responding to observation-action alignment through Wasserstein distance, we enable the model to adjust its behavior as it encounters distributional shift in long-horizon tasks.\n3\nPreliminaries\nFigure 1\n:\nOverview of\nDiG-Flow\nand integration of the\nDiG-Block\n.\n(a)\nWe attach the\nDiG-Block\nin the pretrained VLM backbone. Observations are projected into a shared discrepancy space (\nobs proj.\n), while ground-truth (for training) or predicted actions (for inference) are mapped into the same space (\nact proj.\n).\nFrom these two streams, the block estimates a transport-aware discrepancy\nD\nD\nand converts it into a gate\ng\n=\nÏ•\nâ€‹\n(\nD\n)\ng=\\phi(D)\n, which softly modulates the backbone features before they are passed to the flow-matching action head.\nThe flow-matching head produces action trajectories from noise as in standard VLA models, and the\nDiG-Refine\nmodule can apply a small number of refinement steps entirely within the action head to further polish the predicted actions.\n(b)\nGiven input embeddings, the backbone first applies standard attention.\nThe post-attention features are then normalized and fed to\nDiG-Block\n, which uses the act/obs projections to compute\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\nD(\\mu_{H},\\mu_{Z})\nand a gate\ng\ng\n, and performs a gated residual update.\nThis design allows\nDiG-Flow\nto modulate the high-level representation using discrepancy signals from actions, while keeping the pretrained backbone architecture and attention blocks intact.\nWe establish notation and background in this section.\nFlow Matching.\nFlow matching learns a time-dependent vector field\nv\nÎ¸\nâ€‹\n(\nx\n,\nt\n)\nv_{\\theta}(x,t)\nfor\nt\nâˆˆ\n[\n0\n,\n1\n]\nt\\in[0,1]\nthat transports a base distribution\np\n0\np_{0}\nto a data distribution\np\n1\np_{1}\nvia the ODE:\nd\nâ€‹\nx\nt\nd\nâ€‹\nt\n=\nv\nÎ¸\nâ€‹\n(\nx\nt\n,\nt\n)\n,\nx\n0\nâˆ¼\np\n0\n.\n\\frac{dx_{t}}{dt}=v_{\\theta}(x_{t},t),x_{0}\\sim p_{0}.\nTraining minimizes the squared regression loss against a target vector field\nv\nâ‹†\nv^{\\star}\n:\nâ„’\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nt\n,\nx\n1\n,\nx\n0\nâ€‹\n[\nâ€–\nv\nÎ¸\nâ€‹\n(\nx\nt\n,\nt\n)\nâˆ’\nv\nâ‹†\nâ€‹\n(\nx\nt\n,\nt\n)\nâ€–\n2\n]\n,\n\\mathcal{L}(\\theta)=\\mathbb{E}_{t,x_{1},x_{0}}\\big[\\|v_{\\theta}(x_{t},t)-v^{\\star}(x_{t},t)\\|^{2}\\big],\n(1)\nwhere\nx\nt\nx_{t}\ninterpolates between\nx\n0\nâˆ¼\np\n0\nx_{0}\\sim p_{0}\nand\nx\n1\nâˆ¼\np\n1\nx_{1}\\sim p_{1}\nalong a chosen probability path.\nVLA Architecture and Action Chunks.\nA VLA model processes multimodal observations\no\n=\n(\no\nvis\n,\no\nlang\n,\no\nprop\n)\no=(o_{\\text{vis}},o_{\\text{lang}},o_{\\text{prop}})\ncomprising visual images, language instructions, and proprioceptive states. A vision-language backbone produces observation features:\nH\n=\n(\nh\n1\n,\nâ€¦\n,\nh\nT\n)\nâˆˆ\nâ„\nT\nÃ—\nd\nH=(h_{1},\\ldots,h_{T})\\in\\mathbb{R}^{T\\times d}\n, where\nT\nT\nis the context length and\nd\nd\nis the feature dimension. Current VLA implementations typically adopt an action chunk formulation: at decision time\nt\nt\n, the policy predicts a sequence of future actions:\na\nt\n:\nt\n+\nK\nâˆ’\n1\n=\n(\na\nt\n,\na\nt\n+\n1\n,\nâ€¦\n,\na\nt\n+\nK\nâˆ’\n1\n)\nâˆˆ\nâ„\nK\nÃ—\nd\na\n,\na_{t:t+K-1}=(a_{t},a_{t+1},\\ldots,a_{t+K-1})\\in\\mathbb{R}^{K\\times d_{a}},\n(2)\nwhere\nK\nK\nis the action horizon and\nd\na\nd_{a}\nis the raw action dimension. After executing this chunk, the model re-observes the environment at time\nt\n+\nK\nt+K\nand generates a new chunk, making the policy effectively memoryless between chunks. Flow matching generates these action chunks by conditioning the vector field on observation features:\nv\nÎ¸\nâ€‹\n(\nx\n,\nt\n|\nH\n)\nv_{\\theta}(x,t|H)\n.\nAction Embeddings.\nTo enable geometric comparison between observations and actions, we map raw actions to a latent space via an encoder\nf\n:\nâ„\nd\na\nâ†’\nâ„\nd\nf:\\mathbb{R}^{d_{a}}\\to\\mathbb{R}^{d}\n. For a raw action\na\nt\na_{t}\n, we obtain an action embedding:\nz\nt\n=\nf\nâ€‹\n(\na\nt\n)\nâˆˆ\nâ„\nd\n.\nz_{t}=f(a_{t})\\in\\mathbb{R}^{d}.\nDuring training, ground-truth actions from the dataset are encoded to form ground-truth action embeddings\nz\nt\ngt\n=\nf\nâ€‹\n(\na\nt\ngt\n)\nz_{t}^{\\text{gt}}=f(a_{t}^{\\text{gt}})\n. During inference, predicted actions are encoded as\nz\n^\nt\n=\nf\nâ€‹\n(\na\n^\nt\n)\n\\hat{z}_{t}=f(\\hat{a}_{t})\n. Collecting embeddings over the chunk horizon yields:\nZ\n=\n(\nz\n1\n,\nâ€¦\n,\nz\nK\n)\nâˆˆ\nâ„\nK\nÃ—\nd\n.\nZ=(z_{1},\\ldots,z_{K})\\in\\mathbb{R}^{K\\times d}.\nThe encoder\nf\nf\nis implemented as a lightweight linear projection and trained jointly with the policy. This design allows observations\nH\nH\nand action embeddings\nZ\nZ\nto reside in a common feature space where geometric comparisons are meaningful.\nGiven feature sequences\nH\nâˆˆ\nâ„\nT\nÃ—\nd\nH\\in\\mathbb{R}^{T\\times d}\nand\nZ\nâˆˆ\nâ„\nK\nÃ—\nd\nZ\\in\\mathbb{R}^{K\\times d}\n, we form empirical distributions:\nÎ¼\nH\n=\n1\nT\nâ€‹\nâˆ‘\ni\n=\n1\nT\nÎ´\nh\ni\n,\nÎ¼\nZ\n=\n1\nK\nâ€‹\nâˆ‘\nj\n=\n1\nK\nÎ´\nz\nj\n,\n\\mu_{H}=\\frac{1}{T}\\sum_{i=1}^{T}\\delta_{h_{i}},\\quad\\mu_{Z}=\\frac{1}{K}\\sum_{j=1}^{K}\\delta_{z_{j}},\n(3)\nwhere\nÎ´\nx\n\\delta_{x}\ndenotes a Dirac mass at\nx\nx\n. These distributions enable us to quantify the geometric relationship between observations and actions via distributional discrepancy measures.\n4\nMethod\nWe present\nDiG-Flow\n, a framework for enhancing VLA robustness through geometric regularization of representations. Figure\n1\n(a) shows the overall structure: a pretrained vision-language backbone produces multimodal observation tokens, a lightweight\nDiG-Block\nsits at the interface between the backbone and the flow-matching head, and an optional\nDiG-Refine\nmodule operates purely inside the action expert head.\n4.1\nDiscrepancy-guided architecture\nDiscrepancy Function.\nWe define a discrepancy function\nD\n:\nğ’«\nâ€‹\n(\nâ„\nd\n)\nÃ—\nğ’«\nâ€‹\n(\nâ„\nd\n)\nâ†’\nâ„\n+\nD:\\mathcal{P}(\\mathbb{R}^{d})\\times\\mathcal{P}(\\mathbb{R}^{d})\\to\\mathbb{R}_{+}\nthat quantifies the geometric distance between probability distributions. Our default choice is the squared 2-Wasserstein distance:\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\n=\ninf\nÎ³\nâˆˆ\nÎ“\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\nâˆ«\nâ€–\nx\nâˆ’\ny\nâ€–\n2\nâ€‹\nğ‘‘\nÎ³\nâ€‹\n(\nx\n,\ny\n)\n,\nD(\\mu_{H},\\mu_{Z})=\\inf_{\\gamma\\in\\Gamma(\\mu_{H},\\mu_{Z})}\\int\\|x-y\\|^{2}\\,d\\gamma(x,y),\n(4)\nwhere\nÎ“\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\n\\Gamma(\\mu_{H},\\mu_{Z})\ndenotes the set of couplings between\nÎ¼\nH\n\\mu_{H}\nand\nÎ¼\nZ\n\\mu_{Z}\n. The Wasserstein distance provides a geometrically meaningful measure of distributional alignment: lower transport cost indicates that observation and action representations are compatible, while higher cost suggests potential misalignment.\nFor computational efficiency, we approximate\nW\n2\n2\nW_{2}^{2}\nusing the sliced Wasserstein distance. We sample\nM\nM\nrandom unit directions\nÏ‰\n1\n,\nâ€¦\n,\nÏ‰\nM\nâˆ¼\nUniform\nâ€‹\n(\nğ•Š\nd\nâˆ’\n1\n)\n\\omega_{1},\\ldots,\\omega_{M}\\sim\\text{Uniform}(\\mathbb{S}^{d-1})\nand compute:\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\nâ‰ˆ\n1\nM\nâ€‹\nâˆ‘\nm\n=\n1\nM\nW\n2\n2\nâ€‹\n(\n(\nÏ‰\nm\nâŠ¤\n)\n#\nâ€‹\nÎ¼\nH\n,\n(\nÏ‰\nm\nâŠ¤\n)\n#\nâ€‹\nÎ¼\nZ\n)\n,\nD(\\mu_{H},\\mu_{Z})\\approx\\frac{1}{M}\\sum_{m=1}^{M}W_{2}^{2}\\big((\\omega_{m}^{\\top})_{\\#}\\mu_{H},(\\omega_{m}^{\\top})_{\\#}\\mu_{Z}\\big),\n(5)\nwhere\n(\nÏ‰\nm\nâŠ¤\n)\n#\nâ€‹\nÎ¼\n(\\omega_{m}^{\\top})_{\\#}\\mu\ndenotes the pushforward of\nÎ¼\n\\mu\nunder projection onto\nÏ‰\nm\n\\omega_{m}\n. Each 1D Wasserstein distance is computed via the quantile formula:\nW\n2\n2\nâ€‹\n(\n(\nÏ‰\nm\nâŠ¤\n)\n#\nâ€‹\nÎ¼\nH\n,\n(\nÏ‰\nm\nâŠ¤\n)\n#\nâ€‹\nÎ¼\nZ\n)\n=\nâˆ«\n0\n1\n|\nF\nH\nâˆ’\n1\nâ€‹\n(\nu\n)\nâˆ’\nF\nZ\nâˆ’\n1\nâ€‹\n(\nu\n)\n|\n2\nâ€‹\nğ‘‘\nu\n,\nW_{2}^{2}\\big((\\omega_{m}^{\\top})_{\\#}\\mu_{H},(\\omega_{m}^{\\top})_{\\#}\\mu_{Z}\\big)=\\int_{0}^{1}|F_{H}^{-1}(u)-F_{Z}^{-1}(u)|^{2}du,\n(6)\nwhich reduces to sorting projected features and computing mean squared differences. Alternative discrepancies such as Sinkhorn divergence, maximum mean discrepancy, or cosine distance can be substituted within the same framework.\nWeight Mapping.\nWe map the discrepancy to a modulation weight via a monotone decreasing function\nÏ•\n:\nâ„\n+\nâ†’\n[\ng\nmin\n,\n1\n]\n\\phi:\\mathbb{R}_{+}\\to[g_{\\min},1]\n.\nConcretely, we use an exponentially decaying map with a lower clip:\ng\n=\nÏ•\nâ€‹\n(\nD\n)\n=\nmax\nâ¡\n{\ng\nmin\n,\nexp\nâ¡\n(\nâˆ’\nÏ„\nâ€‹\nD\n)\n}\n,\ng=\\phi(D)=\\max\\{g_{\\min},\\exp(-\\tau D)\\},\n(7)\nwhere\nÏ„\n>\n0\n\\tau>0\nis a temperature parameter and\ng\nmin\nâˆˆ\n(\n0\n,\n1\n)\ng_{\\min}\\in(0,1)\nprevents the gate from vanishing even for large discrepancies.\nLow-discrepancy (well-aligned) pairs thus receive weights close to\n1\n1\n, while high-discrepancy pairs are down-weighted but still retain a non-zero contribution.\nThe map\nÏ•\n\\phi\nis monotone decreasing and Lipschitz continuous, matching Assumption\n8.1\n.\nResidual Operator.\nWe define a lightweight residual operator\nâ„›\n:\nâ„\nT\nÃ—\nd\nâ†’\nâ„\nT\nÃ—\nd\n\\mathcal{R}:\\mathbb{R}^{T\\times d}\\to\\mathbb{R}^{T\\times d}\nthat transforms observation features. In our implementation,\nâ„›\n\\mathcal{R}\nis a single linear layer with spectral norm regularization:\nâ„›\nâ€‹\n(\nH\n)\n=\nW\nâ„›\nâ€‹\nH\n+\nb\nâ„›\n,\n\\mathcal{R}(H)=W_{\\mathcal{R}}H+b_{\\mathcal{R}},\n(8)\nwhere\nW\nâ„›\nâˆˆ\nâ„\nd\nÃ—\nd\nW_{\\mathcal{R}}\\in\\mathbb{R}^{d\\times d}\nand\nb\nâ„›\nâˆˆ\nâ„\nd\nb_{\\mathcal{R}}\\in\\mathbb{R}^{d}\nare learned parameters, and\nâ€–\nW\nâ„›\nâ€–\n2\nâ‰¤\nB\nâ„›\n\\|W_{\\mathcal{R}}\\|_{2}\\leq B_{\\mathcal{R}}\nis controlled via spectral normalization.\nThe enhanced observation features are computed via a residual update modulated by the gate:\nH\n~\n=\nH\n+\nÎ»\nâ‹…\ng\nâ‹…\nâ„›\nâ€‹\n(\nH\n)\n,\n\\tilde{H}=H+\\lambda\\cdot g\\cdot\\mathcal{R}(H),\n(9)\nwhere\nÎ»\n>\n0\n\\lambda>0\nis the residual strength parameter. This design ensures that when\ng\ng\nis large (low discrepancy), the residual adjustment is fully applied, while when\ng\ng\nis small (high discrepancy), the adjustment is suppressed. The enhanced features\nH\n~\n\\tilde{H}\nare then fed to the flow matching action head in place of the original features\nH\nH\n.\nWe integrate these components as a module that can be dropped into existing transformer layers without changing their backbone structure.\nFigure\n1\n(b) illustrates how the\nDiG-Block\nis integrated between multi-head attention and the feedforward network.\nFigure 2\n:\nDiscrepancy-guided gating at training and inference time.\nLeft (training).\nObservation embeddings\nh\nobs\nh_{\\mathrm{obs}}\nand ground-truth action embeddings\nz\ngt\nz^{\\mathrm{gt}}\ndefine empirical distributions\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\n(\\mu_{H},\\mu_{Z})\n.\nThe discrepancy\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\nD(\\mu_{H},\\mu_{Z})\nis mapped to a gate\ng\n=\nÏ•\nâ€‹\n(\nD\n)\ng=\\phi(D)\n, which modulates a residual update on\nh\nobs\nh_{\\mathrm{obs}}\nand reweights the flow-matching loss via the gated objective\nJ\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n)\n]\nJ(\\theta)=\\mathbb{E}[g\\,\\ell(\\theta)]\n.\nRight (inference).\nThe same mechanism is applied using encoded model predictions\nz\n1\n,\nâ€¦\n,\nz\n^\nz_{1},\\ldots,\\hat{z}\ninstead of ground-truth actions.\nThe gate is computed from the discrepancy between observation embeddings and predicted action embeddings and is then used either once or multiple steps inside the flow-matching head.\n4.2\nTraining with Transport-aware Gating\nAlgorithm 1\nDiG-Flow\nTraining\n0:\nObservation\no\no\n, ground-truth actions\na\ngt\n=\n{\na\n1\ngt\n,\nâ€¦\n,\na\nK\ngt\n}\na^{\\text{gt}}=\\{a_{1}^{\\text{gt}},\\dots,a_{K}^{\\text{gt}}\\}\n, flow-matching network\nv\nÎ¸\nv_{\\theta}\n, residual operator\nâ„›\n\\mathcal{R}\n,\nM\nM\n(sliced projections),\nÏ„\n\\tau\n(temperature),\nÎ»\n\\lambda\n(residual strength),\ng\nmin\ng_{\\min}\n(gate floor)\n1:\nExtract observation features:\nH\nâ†\nVLM\nâ€‹\n(\no\n)\nâˆˆ\nâ„\nT\nÃ—\nd\nH\\leftarrow\\mathrm{VLM}(o)\\in\\mathbb{R}^{T\\times d}\n2:\nEncode ground-truth actions:\nz\nk\ngt\nâ†\nf\nâ€‹\n(\na\nk\ngt\n)\nz_{k}^{\\text{gt}}\\leftarrow f(a_{k}^{\\text{gt}})\nfor\nk\n=\n1\n,\nâ€¦\n,\nK\nk=1,\\ldots,K\n3:\nMean-pool and broadcast:\nz\nÂ¯\nâ†\n1\nK\nâ€‹\nâˆ‘\nk\n=\n1\nK\nz\nk\ngt\n\\bar{z}\\leftarrow\\frac{1}{K}\\sum_{k=1}^{K}z_{k}^{\\text{gt}}\n, replicate\nz\nÂ¯\n\\bar{z}\nto length\nT\nT\nto form\nZ\ngt\nâˆˆ\nâ„\nT\nÃ—\nd\nZ^{\\text{gt}}\\in\\mathbb{R}^{T\\times d}\n4:\nCompute discrepancy via sliced Wasserstein:\n5:\nfor\nm\n=\n1\nm=1\nto\nM\nM\ndo\n6:\nSample unit direction\nÏ‰\nm\nâˆ¼\nUniform\nâ€‹\n(\nğ•Š\nd\nâˆ’\n1\n)\n\\omega_{m}\\sim\\mathrm{Uniform}(\\mathbb{S}^{d-1})\n7:\nProject:\nÏ€\nH\n(\nm\n)\nâ†\nH\nâ€‹\nÏ‰\nm\n,\nÏ€\nZ\n(\nm\n)\nâ†\nZ\ngt\nâ€‹\nÏ‰\nm\n\\pi_{H}^{(m)}\\leftarrow H\\omega_{m},\\quad\\pi_{Z}^{(m)}\\leftarrow Z^{\\text{gt}}\\omega_{m}\n8:\nSort:\nÏ€\nH\n(\nm\n)\n,\nâ†‘\nâ†\nsort\nâ€‹\n(\nÏ€\nH\n(\nm\n)\n)\n,\nÏ€\nZ\n(\nm\n)\n,\nâ†‘\nâ†\nsort\nâ€‹\n(\nÏ€\nZ\n(\nm\n)\n)\n\\pi_{H}^{(m),\\uparrow}\\leftarrow\\mathrm{sort}(\\pi_{H}^{(m)}),\\quad\\pi_{Z}^{(m),\\uparrow}\\leftarrow\\mathrm{sort}(\\pi_{Z}^{(m)})\n9:\nAccumulate:\nD\nm\nâ†\n1\nT\nâ€‹\nâˆ‘\ni\n=\n1\nT\n(\nÏ€\nH\n,\ni\n(\nm\n)\n,\nâ†‘\nâˆ’\nÏ€\nZ\n,\ni\n(\nm\n)\n,\nâ†‘\n)\n2\nD_{m}\\leftarrow\\frac{1}{T}\\sum_{i=1}^{T}(\\pi_{H,i}^{(m),\\uparrow}-\\pi_{Z,i}^{(m),\\uparrow})^{2}\n10:\nend\nfor\n11:\nD\nâ†\n1\nM\nâ€‹\nâˆ‘\nm\n=\n1\nM\nD\nm\nD\\leftarrow\\frac{1}{M}\\sum_{m=1}^{M}D_{m}\n12:\nCompute gate (stop gradient):\ng\nâ†\nstop\nâ€‹\n_\nâ€‹\ngrad\nâ€‹\n(\nmax\nâ¡\n{\ng\nmin\n,\nexp\nâ¡\n(\nâˆ’\nÏ„\nâ€‹\nD\n)\n}\n)\ng\\leftarrow\\mathrm{stop\\_grad}(\\max\\{g_{\\min},\\exp(-\\tau D)\\})\n13:\nCompute enhanced features:\nH\n~\nâ†\nH\n+\nÎ»\nâ‹…\ng\nâ‹…\nâ„›\nâ€‹\n(\nH\n)\n\\tilde{H}\\leftarrow H+\\lambda\\cdot g\\cdot\\mathcal{R}(H)\n14:\nCompute flow-matching loss (conditional on\nH\n~\n\\tilde{H}\n):\nâ„“\nâ†\nğ”¼\nt\n,\nx\nt\n|\nx\n0\n[\nâˆ¥\nv\nÎ¸\n(\nx\nt\n,\nt\nâˆ£\nH\n~\n)\nâˆ’\nv\nâ‹†\n(\nx\nt\n,\nt\n)\nâˆ¥\n2\n]\n\\ell\\leftarrow\\mathbb{E}_{t,x_{t}|x_{0}}[\\|v_{\\theta}(x_{t},t\\mid\\tilde{H})-v^{\\star}(x_{t},t)\\|^{2}]\n15:\nCompute gated objective:\nJ\nâ†\ng\nâ‹…\nâ„“\nJ\\leftarrow g\\cdot\\ell\n16:\nBackpropagate\nâˆ‡\nÎ¸\nJ\n\\nabla_{\\theta}J\nand update parameters\nDuring training,\nDiG-Flow\nuses ground-truth action chunks to construct the geometric signal; the flow-matching objective itself remains unchanged. For a training sample\n(\no\n,\na\ngt\n)\n(o,a^{\\text{gt}})\n, the vision-language backbone produces observation features\nH\n=\nVLM\nâ€‹\n(\no\n)\nâˆˆ\nâ„\nT\nÃ—\nd\n,\nH=\\mathrm{VLM}(o)\\in\\mathbb{R}^{T\\times d},\nand the action encoder maps ground-truth actions to embeddings\nz\nk\ngt\n=\nf\nâ€‹\n(\na\nk\ngt\n)\nz_{k}^{\\text{gt}}=f(a_{k}^{\\text{gt}})\n.\nWe form an action semantic centroid by mean-pooling and broadcasting,\nz\nÂ¯\n=\n1\nK\nâ€‹\nâˆ‘\nk\n=\n1\nK\nz\nk\ngt\n,\nZ\ngt\n=\n(\nz\nÂ¯\n,\nâ€¦\n,\nz\nÂ¯\n)\nâˆˆ\nâ„\nT\nÃ—\nd\n,\n\\bar{z}\\;=\\;\\frac{1}{K}\\sum_{k=1}^{K}z_{k}^{\\text{gt}},\\quad Z^{\\text{gt}}\\;=\\;(\\bar{z},\\dots,\\bar{z})\\in\\mathbb{R}^{T\\times d},\n(10)\nwhich induces empirical measures\nÎ¼\nH\n\\mu_{H}\nand\nÎ¼\nZ\ngt\n\\mu_{Z^{\\text{gt}}}\n.\nThis design implies that we effectively model the target action representation as a\ndegenerate empirical distribution\nconcentrated at the semantic centroid\nz\nÂ¯\n\\bar{z}\n.\nThis serves two purposes: (i) it summarizes the â€œsemantic intentâ€ of the action chunk, and (ii) it makes the discrepancy computation invariant to the temporal ordering of actions, focusing instead on global compatibility.\nCrucially, we explicitly compute the discrepancy\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\ngt\n)\nD(\\mu_{H},\\mu_{Z^{\\text{gt}}})\nusing the standard Sliced Wasserstein formulation (Algorithm\n1\n).\nThis approach interprets the observation features\nH\nH\nas a distribution and measures the transport cost required to condense them into the action centroid, implicitly penalizing variance and ensuring geometric alignment.\nWhile the sorting of\nZ\ngt\nZ^{\\text{gt}}\nbecomes an identity operation in this degenerate case, we retain the general sorting step in Algorithm\n1\nto maintain the standard formulation, allowing extensions to non-degenerate action distributions.\nThe gate is obtained by a monotone map\ng\n=\nmax\nâ¡\n{\ng\nmin\n,\nexp\nâ¡\n(\nâˆ’\nÏ„\nâ€‹\nD\n)\n}\ng=\\max\\{g_{\\min},\\exp(-\\tau D)\\}\n, with gradients stopped through\ng\ng\n. The observation features are then modulated by a lightweight residual operator,\nH\n~\n=\nH\n+\nÎ»\nâ€‹\ng\nâ€‹\nâ„›\nâ€‹\n(\nH\n)\n,\n\\tilde{H}\\;=\\;H+\\lambda\\,g\\,\\mathcal{R}(H),\n(11)\nand\nH\n~\n\\tilde{H}\nis fed into the flow-matching head.\nThe final training objective is\nJ\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nâ€‹\n[\nsg\nâ¡\n(\ng\n)\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n~\n,\nt\n)\n]\n,\nJ(\\theta)\\;=\\;\\mathbb{E}\\big[\\operatorname{sg}(g)\\,\\ell(\\theta;\\tilde{H},t)\\big],\n(12)\nwhere\nâ„“\n\\ell\nis the per-sample flow-matching loss and\nsg\nâ¡\n(\nâ‹…\n)\n\\operatorname{sg}(\\cdot)\ndenotes the stop-gradient operator.\nIntuitively,\ng\ng\nacts purely as a data-dependent confidence weight: it suppresses shortcut-like examples with large discrepancy, while avoiding trivial solutions where the backbone collapses features solely to minimize\nD\nD\n.\nFigure\n2\nsummarizes this mechanism.\nAll parameters of the backbone, action encoder, residual operator, and flow-matching head are optimized jointly by differentiating\nJ\nâ€‹\n(\nÎ¸\n)\nJ(\\theta)\n.\n4.3\nInference-time Refinement\nAlgorithm 2\nDiG-Flow\nInference with optional refinement\n0:\nObservation\no\no\n, number of refinement iterations\nN\nrefine\nN_{\\text{refine}}\n1:\nExtract observation features:\nH\nâ†\nVLM\nâ€‹\n(\no\n)\nH\\leftarrow\\mathrm{VLM}(o)\n2:\nGenerate initial action chunk:\na\n(\n0\n)\nâ†\nFlowModel\nâ€‹\n(\nH\n)\na^{(0)}\\leftarrow\\mathrm{FlowModel}(H)\n3:\nfor\ni\n=\n1\ni=1\nto\nN\nrefine\nN_{\\text{refine}}\ndo\n4:\nEncode previous prediction:\nZ\n(\ni\nâˆ’\n1\n)\nâ†\n{\nf\nâ€‹\n(\na\nk\n(\ni\nâˆ’\n1\n)\n)\n}\nk\n=\n1\nK\nZ^{(i-1)}\\leftarrow\\{f(a_{k}^{(i-1)})\\}_{k=1}^{K}\n, aligned to length\nT\nT\n5:\nCompute discrepancy:\nD\n(\ni\nâˆ’\n1\n)\nâ†\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n(\ni\nâˆ’\n1\n)\n)\nD^{(i-1)}\\leftarrow D(\\mu_{H},\\mu_{Z^{(i-1)}})\n6:\nCompute gate:\ng\n(\ni\nâˆ’\n1\n)\nâ†\nmax\nâ¡\n{\ng\nmin\n,\nexp\nâ¡\n(\nâˆ’\nÏ„\nâ€‹\nD\n(\ni\nâˆ’\n1\n)\n)\n}\ng^{(i-1)}\\leftarrow\\max\\{g_{\\min},\\exp(-\\tau D^{(i-1)})\\}\n7:\nEnhance features:\nH\n~\n(\ni\nâˆ’\n1\n)\nâ†\nH\n+\nÎ»\nâ‹…\ng\n(\ni\nâˆ’\n1\n)\nâ‹…\nâ„›\nâ€‹\n(\nH\n)\n\\tilde{H}^{(i-1)}\\leftarrow H+\\lambda\\cdot g^{(i-1)}\\cdot\\mathcal{R}(H)\n8:\nGenerate refined action:\na\n(\ni\n)\nâ†\nFlowModel\nâ€‹\n(\nH\n~\n(\ni\nâˆ’\n1\n)\n)\na^{(i)}\\leftarrow\\mathrm{FlowModel}(\\tilde{H}^{(i-1)})\n9:\nend\nfor\n10:\nreturn\na\n(\nN\nrefine\n)\na^{(N_{\\text{refine}})}\nAt inference time, our default configuration mirrors the training-time modulation but avoids extra iterations. Given an observation\no\no\n, the model first computes\nH\n=\nVLM\nâ€‹\n(\no\n)\nH=\\mathrm{VLM}(o)\nand predicts an action chunk from the flow-matching head. The predicted actions from the previous decision step are encoded and summarized in the same way as during training, yielding an action embedding sequence\nZ\nZ\nand empirical measure\nÎ¼\nZ\n\\mu_{Z}\n. The discrepancy is used to form enhanced features\nH\n~\n=\nH\n+\nÎ»\nâ€‹\ng\nâ€‹\nâ„›\nâ€‹\n(\nH\n)\n\\tilde{H}=H+\\lambda\\,g\\,\\mathcal{R}(H)\n, from which the next action chunk is generated. This single-pass, training-style enhancement is used for all main results.\nWhen additional compute is available,\nDiG-Flow\nalso supports an optional iterative refinement scheme. Starting from an initial prediction\na\n(\n0\n)\na^{(0)}\nproduced using\nH\nH\n, we can repeatedly encode\na\n(\ni\n)\na^{(i)}\n, recompute\nD\n(\ni\n)\nD^{(i)}\nand\ng\n(\ni\n)\ng^{(i)}\n, form\nH\n~\n(\ni\n)\n=\nH\n+\nÎ»\nâ€‹\ng\n(\ni\n)\nâ€‹\nâ„›\nâ€‹\n(\nH\n)\n\\tilde{H}^{(i)}=H+\\lambda\\,g^{(i)}\\,\\mathcal{R}(H)\n, and obtain a refined action\na\n(\ni\n+\n1\n)\na^{(i+1)}\nfrom the flow-matching head. In practice, performance saturates within\n2\n2\nâ€“\n3\n3\nrefinement steps. A fixed-gate variant of this refinement admits a contraction guarantee (Theorem\n5.1\n). Algorithm\n2\ndescribes inference-time usage. We first run the base policy once to obtain an initial action chunk\na\n(\n0\n)\na^{(0)}\n. Refinement iterations then reuse the same discrepancy function\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n(\ni\nâˆ’\n1\n)\n)\nD(\\mu_{H},\\mu_{Z^{(i-1)}})\n, but now\nZ\n(\ni\nâˆ’\n1\n)\nZ^{(i-1)}\nis computed from the previous prediction instead of the ground truth. The gate\ng\n(\ni\nâˆ’\n1\n)\ng^{(i-1)}\ncontrols how much residual correction is applied to\nH\nH\nbefore feeding it into the flow model again. We call this procedure\nDiG-Refine\n.\n5\nTheoretical Analysis\n5.1\nOptimization and refinement guarantees\nWe now establish theoretical properties of\nDiG-Flow\nthat clarify how discrepancy-guided modulation affects optimization and inference.\nOur analysis addresses three questions that correspond directly to the design choices in\nDiG-Flow\n:\n(i) does training with discrepancy-guided gates define a well-behaved optimization objective?\n(ii) do the residual feature updates help rather than hurt?\n(iii) can a simple refinement scheme converge?\nWe first state the main results and discuss their implications; formal assumptions and proofs appear in Appendix\n8\n.\n{cththeorem}\nGated descent on the weighted objective\n\nConsider the gated flow-matching objective\nJ\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n]\nJ(\\theta)=\\mathbb{E}\\big[g\\,\\ell(\\theta;H,t)\\big]\n, where\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n\\ell(\\theta;H,t)\nis the per-sample flow-matching loss and\ng\n=\nÏ•\nâ€‹\n(\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\n)\nâˆˆ\n[\ng\nmin\n,\n1\n]\ng=\\phi\\big(D(\\mu_{H},\\mu_{Z})\\big)\\in[g_{\\min},1]\nis a data-dependent but\nparameter-independent\nweight\n(\ni.e\n., gradients are stopped through\ng\ng\nwhen differentiating with respect to\nÎ¸\n\\theta\n).\nAssume that\nJ\nJ\nis\nL\nJ\nL_{J}\n-smooth in\nÎ¸\n\\theta\n.\nThen for any step size\n0\n<\nÎ±\n<\n2\n/\nL\nJ\n0<\\alpha<2/L_{J}\n, the gradient descent update\nÎ¸\n+\n=\nÎ¸\nâˆ’\nÎ±\nâ€‹\nâˆ‡\nÎ¸\nJ\nâ€‹\n(\nÎ¸\n)\n\\theta^{+}=\\theta-\\alpha\\,\\nabla_{\\theta}J(\\theta)\nsatisfies\nJ\nâ€‹\n(\nÎ¸\n+\n)\nâ‰¤\nJ\nâ€‹\n(\nÎ¸\n)\nâˆ’\nc\n1\nâ€‹\nâ€–\nâˆ‡\nÎ¸\nJ\nâ€‹\n(\nÎ¸\n)\nâ€–\n2\n,\nJ(\\theta^{+})\\;\\leq\\;J(\\theta)\\;-\\;c_{1}\\,\\|\\nabla_{\\theta}J(\\theta)\\|^{2},\n(13)\nwhere\nc\n1\n=\nÎ±\nâ€‹\n(\n1\nâˆ’\nÎ±\nâ€‹\nL\nJ\n2\n)\n>\n0\nc_{1}=\\alpha\\bigl(1-\\tfrac{\\alpha L_{J}}{2}\\bigr)>0\n.\nMoreover, under Assumption\n8.1\n, for all\nÎ¸\n\\theta\n,\ng\nmin\nâ€‹\nâ„’\nâ€‹\n(\nÎ¸\n)\nâ‰¤\nJ\nâ€‹\n(\nÎ¸\n)\nâ‰¤\nâ„’\nâ€‹\n(\nÎ¸\n)\n,\ng_{\\min}\\,\\mathcal{L}(\\theta)\\;\\leq\\;J(\\theta)\\;\\leq\\;\\mathcal{L}(\\theta),\n(14)\nso minimizing\nJ\nJ\ncontrols the original flow-matching loss\nâ„’\n\\mathcal{L}\nup to the fixed factor\n1\n/\ng\nmin\n1/g_{\\min}\n.\nTheorem\n5.1\nshows that training with discrepancy-guided gates yields a standard smooth optimization problem:\ngradient descent on\nJ\nJ\nenjoys the usual descent guarantee (\n13\n), and\nJ\nJ\nuniformly brackets the original loss\nâ„’\n\\mathcal{L}\nvia (\n14\n).\nIn other words, the gates reweight contributions of different samples while keeping\nJ\nJ\nand\nâ„’\n\\mathcal{L}\nuniformly equivalent as objective functions.\nTreating\ng\ng\nas parameter-independent (via stop-gradient) avoids second-order cross terms and preserves this standard descent behaviour.\nIn practice, the observation features\nH\nH\nand action embeddings\nZ\nZ\nare updated jointly with\nÎ¸\n\\theta\n, so the gates\ng\ng\nevolve over training.\nTheorem\n5.1\ntherefore characterizes the dynamics of the pseudo-gradient\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâˆ‡\nÎ¸\nâ„“\n]\n\\mathbb{E}[g\\,\\nabla_{\\theta}\\ell]\nunder a fixed-gate idealization,\nwhich matches the implemented stop-gradient update and is empirically observed to lead to stable and robust training.\n{cththeorem}\nResidual update improvement\n\nSuppose Assumption\n8.1\nholds, and assume there exists\nÎ±\n0\n>\n0\n\\alpha_{0}>0\nsuch that the residual operator satisfies the gated descent condition\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâŸ¨\nâˆ‡\nH\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n,\nâ„›\nâ€‹\n(\nH\n)\nâŸ©\n]\nâ‰¤\nâˆ’\nÎ±\n0\n.\n\\mathbb{E}\\big[g\\,\\langle\\nabla_{H}\\ell(\\theta;H,t),\\,\\mathcal{R}(H)\\rangle\\big]\\;\\leq\\;-\\alpha_{0}.\n(15)\nThen there exists a threshold\nÎ»\nmax\n>\n0\n\\lambda_{\\max}>0\nsuch that\nfor all residual strengths\n0\n<\nÎ»\nâ‰¤\nÎ»\nmax\n0<\\lambda\\leq\\lambda_{\\max}\n,\nthe gated residual update\nH\n~\n=\nH\n+\nÎ»\nâ€‹\ng\nâ€‹\nâ„›\nâ€‹\n(\nH\n)\n\\tilde{H}=H+\\lambda\\,g\\,\\mathcal{R}(H)\nsatisfies\nğ”¼\nâ€‹\n[\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n~\n,\nt\n)\n]\nâ‰¤\nğ”¼\nâ€‹\n[\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n]\nâˆ’\nÎ²\nâ€‹\nÎ»\n,\n\\mathbb{E}\\big[\\ell(\\theta;\\tilde{H},t)\\big]\\;\\leq\\;\\mathbb{E}\\big[\\ell(\\theta;H,t)\\big]\\;-\\;\\beta\\,\\lambda,\n(16)\nwith\nÎ²\n=\nÎ±\n0\n/\n2\n>\n0\n\\beta=\\alpha_{0}/2>0\n.\nTheorem\n5.1\naddresses the effect of the residual operator on the loss.\nThe gated descent condition (\n15\n) states that, on average, the direction\nâ„›\nâ€‹\n(\nH\n)\n\\mathcal{R}(H)\naligns with the negative feature gradient once weighted by the gate\ng\ng\n.\nUnder smoothness of\nâ„“\n\\ell\nin\nH\nH\n, sufficiently small gated residual steps strictly decrease the expected loss:\nthe linear improvement from moving in a descent direction dominates the quadratic penalty for small enough\nÎ»\n\\lambda\n.\nIn practice,\nâ„›\n\\mathcal{R}\nis a small linear network trained jointly with the policy and implemented with spectral normalization.\nBecause its parameters are optimized to minimize the gated objective\nJ\nâ€‹\n(\nÎ¸\n)\nJ(\\theta)\n, the network is encouraged to produce residual directions that correlate with loss reduction:\nif\nâ„›\nâ€‹\n(\nH\n)\n\\mathcal{R}(H)\nwere uncorrelated with\nâˆ‡\nH\nâ„“\n\\nabla_{H}\\ell\n, it would contribute noise rather than useful signal and be effectively regularized away.\nWe therefore view Eq. (\n15\n) as a structural assumption capturing the typical behaviour of a jointly trained residual operator, rather than an externally imposed constraint.\n{cththeorem}\nFixed-gate refinement convergence\n\nConsider an iterative refinement update in action-embedding space:\nZ\n(\nk\n+\n1\n)\n=\nZ\n(\nk\n)\nâˆ’\nÎ±\nâ€‹\nE\nâ€‹\n(\nZ\n(\nk\n)\n;\ng\n)\n,\nZ^{(k+1)}=Z^{(k)}-\\alpha E(Z^{(k)};g),\n(17)\nwhere the gate\ng\ng\nis fixed and\nE\nâ€‹\n(\nâ‹…\n;\ng\n)\nE(\\cdot;g)\nrepresents an error-correction field.\nIf\nE\nâ€‹\n(\nâ‹…\n;\ng\n)\nE(\\cdot;g)\nis\nL\nE\nL_{E}\n-Lipschitz and\nÎ¼\n\\mu\n-strongly monotone (see Appendix\n8.1\n), then for step sizes\n0\n<\nÎ±\n<\n2\nâ€‹\nÎ¼\n/\nL\nE\n2\n0<\\alpha<2\\mu/L_{E}^{2}\n, the update defines a contraction mapping with rate\nÏ\nâˆˆ\n(\n0\n,\n1\n)\n\\rho\\in(0,1)\n:\nâ€–\nZ\n(\nk\n)\nâˆ’\nZ\nâ‹†\nâ€–\nF\nâ‰¤\nÏ\nk\nâ€‹\nâ€–\nZ\n(\n0\n)\nâˆ’\nZ\nâ‹†\nâ€–\nF\n,\n\\|Z^{(k)}-Z^{\\star}\\|_{F}\\leq\\rho^{k}\\|Z^{(0)}-Z^{\\star}\\|_{F},\n(18)\nwhere\nZ\nâ‹†\nZ^{\\star}\nis the unique fixed point.\nTheorem\n5.1\nisolates a fixed-gate refinement scheme and shows that it admits a contraction guarantee under standard conditions.\nWhen the refinement field is Lipschitz and strongly monotone (\ne.g\n., the gradient of a strongly convex functional), the refinement mapping contracts toward a unique fixed point, with the error decaying at rate\nÏ\nk\n\\rho^{k}\n.\nOur implementation uses a more practical variant that recomputes\ng\ng\nfrom the current prediction rather than keeping it fixed.\nEmpirically, as the predicted actions improve, the discrepancy\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n(\ni\n)\n)\nD(\\mu_{H},\\mu_{Z^{(i)}})\ndecreases and the sequence of gates\n{\ng\n(\ni\n)\n}\n\\{g^{(i)}\\}\nquickly stabilizes;\nthe local dynamics then behave like a contraction with a nearly fixed gate, and performance saturates within 3 refinement steps (Section\n6.3.4\n).\nWe therefore view iterative refinement as an optional, well-behaved post-processing step whose behaviour is well captured by this idealized fixed-g analysis.\n5.2\nTheoretical Intuition\nFigure 3\n:\nDiG-Flow\ndetects and suppresses shortcut learning through transport-guided gating.\nFlow matching transforms noise into actions, and multiple solutions can achieve low training loss. Some of these solutions correspond to semantically meaningful alignments between observations and actions (\nblue\ndistribution), while others exploit spurious correlations or shortcuts (\nred\ndistribution).\nDiG-Flow\nmeasures the Wasserstein distance\nD\nD\nbetween observation and action features to distinguish these pathways. The gate\ng\n=\nÏ•\nâ€‹\n(\nD\n)\ng=\\phi(D)\nselectively modulates learning:\nGreen path\n(\nD\nlow\nD_{\\text{low}}\n): semantically aligned features with low transport distance receive strong gates, reinforcing robust behavior;\nYellow path\n(\nD\nmid\nD_{\\text{mid}}\n): intermediate features receive moderate gating;\nRed path\n(\nD\nhigh\nD_{\\text{high}}\n): shortcut-like patterns with high transport distance are down-weighted, discouraging spurious solutions.\nThis geometric reweighting complements the flow-matching loss and helps steer optimization toward representations that remain stable under distribution shift. In this way,\nDiG-Flow\npushes more intelligence toward the foundation model\n, making VLAs generate more robust actions for general manipulation.\nThe previous subsection establishes that the gated objective\nJ\nâ€‹\n(\nÎ¸\n)\nJ(\\theta)\nis smooth, that the residual update yields improvement for\nÎ»\n\\lambda\nin a nontrivial interval, and that the refinement field defines a contraction. We now give a more explicit intuition in terms of how the discrepancy-controlled gate with above properties reshapes the loss landscape.\nDataset decomposition by discrepancy.\nLet\nD\nâ€‹\n(\nH\n,\nZ\n)\nD(H,Z)\ndenote the discrepancy between observation features and action embeddings for a particular sample, and let\ng\n=\nÏ•\nâ€‹\n(\nD\n)\ng=\\phi(D)\nwith\nÏ•\n\\phi\nmonotone decreasing. Consider partitioning the dataset into two regions:\nğ’œ\n\\displaystyle\\mathcal{A}\n=\n{\n(\nH\n,\nZ\n)\n:\nD\nâ€‹\n(\nH\n,\nZ\n)\nâ‰¤\nÎ´\n}\n(geometrically aligned)\n,\n\\displaystyle=\\{(H,Z):D(H,Z)\\leq\\delta\\}\\quad\\text{(geometrically aligned)},\n(19)\nğ’®\n\\displaystyle\\mathcal{S}\n=\n{\n(\nH\n,\nZ\n)\n:\nD\nâ€‹\n(\nH\n,\nZ\n)\n>\nÎ´\n}\n(geometrically misaligned / shortcut-like)\n,\n\\displaystyle=\\{(H,Z):D(H,Z)>\\delta\\}\\quad\\text{(geometrically misaligned / shortcut-like)},\n(20)\nfor some threshold\nÎ´\n>\n0\n\\delta>0\n. Denote the corresponding probabilities by\np\nğ’œ\n=\nPr\nâ¡\n[\n(\nH\n,\nZ\n)\nâˆˆ\nğ’œ\n]\n,\np\nğ’®\n=\nPr\nâ¡\n[\n(\nH\n,\nZ\n)\nâˆˆ\nğ’®\n]\n,\np\nğ’œ\n+\np\nğ’®\n=\n1\n.\np_{\\mathcal{A}}=\\Pr[(H,Z)\\in\\mathcal{A}],\\quad p_{\\mathcal{S}}=\\Pr[(H,Z)\\in\\mathcal{S}],\\quad p_{\\mathcal{A}}+p_{\\mathcal{S}}=1.\n(21)\nUsing this decomposition, the gated objective can be written as\nJ\nâ€‹\n(\nÎ¸\n)\n\\displaystyle J(\\theta)\n=\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n]\n\\displaystyle=\\mathbb{E}\\big[g\\,\\ell(\\theta;H,t)\\big]\n(22)\n=\np\nğ’œ\nâ€‹\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\nâˆ£\nğ’œ\n]\n+\np\nğ’®\nâ€‹\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\nâˆ£\nğ’®\n]\n,\n\\displaystyle=p_{\\mathcal{A}}\\,\\mathbb{E}\\big[g\\,\\ell(\\theta;H,t)\\mid\\mathcal{A}\\big]+p_{\\mathcal{S}}\\,\\mathbb{E}\\big[g\\,\\ell(\\theta;H,t)\\mid\\mathcal{S}\\big],\n(23)\nand its gradient decomposes as\nâˆ‡\nÎ¸\nJ\nâ€‹\n(\nÎ¸\n)\n=\np\nğ’œ\nâ€‹\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâˆ‡\nÎ¸\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\nâˆ£\nğ’œ\n]\n+\np\nğ’®\nâ€‹\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâˆ‡\nÎ¸\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\nâˆ£\nğ’®\n]\n.\n\\nabla_{\\theta}J(\\theta)=p_{\\mathcal{A}}\\,\\mathbb{E}\\big[g\\,\\nabla_{\\theta}\\ell(\\theta;H,t)\\mid\\mathcal{A}\\big]+p_{\\mathcal{S}}\\,\\mathbb{E}\\big[g\\,\\nabla_{\\theta}\\ell(\\theta;H,t)\\mid\\mathcal{S}\\big].\n(24)\nBecause\nÏ•\n\\phi\nis decreasing in\nD\nD\n, we typically have\nğ”¼\nâ€‹\n[\ng\nâˆ£\nğ’œ\n]\nâ‰ˆ\n1\n,\nğ”¼\nâ€‹\n[\ng\nâˆ£\nğ’®\n]\nâ‰ª\n1\n\\mathbb{E}[g\\mid\\mathcal{A}]\\approx 1,\\quad\\mathbb{E}[g\\mid\\mathcal{S}]\\ll 1\n(25)\nwhenever aligned pairs exhibit small discrepancy and shortcut-like pairs exhibit large discrepancy. Thus, even if the raw gradients\nğ”¼\nâ€‹\n[\nâˆ‡\nÎ¸\nâ„“\nâˆ£\nğ’œ\n]\n\\mathbb{E}[\\nabla_{\\theta}\\ell\\mid\\mathcal{A}]\nand\nğ”¼\nâ€‹\n[\nâˆ‡\nÎ¸\nâ„“\nâˆ£\nğ’®\n]\n\\mathbb{E}[\\nabla_{\\theta}\\ell\\mid\\mathcal{S}]\nhave comparable magnitudes, the effective contributions to the gated gradient in (\n24\n) are reweighted by the expected gates. As optimization proceeds,\nJ\nâ€‹\n(\nÎ¸\n)\nJ(\\theta)\ntherefore behaves as if it were more strongly influenced by the aligned region\nğ’œ\n\\mathcal{A}\nand less by the shortcut region\nğ’®\n\\mathcal{S}\n.\nConnection to the residual update condition.\nThe residual update analysis for Theorem\n5.1\nuses the alignment condition\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâŸ¨\nâˆ‡\nH\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n,\nâ„›\nâ€‹\n(\nH\n)\nâŸ©\n]\nâ‰¤\nâˆ’\nÎ±\n0\n.\n\\mathbb{E}\\big[g\\,\\langle\\nabla_{H}\\ell(\\theta;H,t),\\mathcal{R}(H)\\rangle\\big]\\;\\leq\\;-\\alpha_{0}.\n(26)\nUsing the same decomposition\nğ’œ\nâˆª\nğ’®\n\\mathcal{A}\\cup\\mathcal{S}\n, this can be rewritten as\np\nğ’œ\nâ€‹\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâŸ¨\nâˆ‡\nH\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n,\nâ„›\nâ€‹\n(\nH\n)\nâŸ©\nâˆ£\nğ’œ\n]\n+\np\nğ’®\nâ€‹\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâŸ¨\nâˆ‡\nH\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n,\nâ„›\nâ€‹\n(\nH\n)\nâŸ©\nâˆ£\nğ’®\n]\nâ‰¤\nâˆ’\nÎ±\n0\n.\np_{\\mathcal{A}}\\,\\mathbb{E}\\big[g\\,\\langle\\nabla_{H}\\ell(\\theta;H,t),\\mathcal{R}(H)\\rangle\\mid\\mathcal{A}\\big]+p_{\\mathcal{S}}\\,\\mathbb{E}\\big[g\\,\\langle\\nabla_{H}\\ell(\\theta;H,t),\\mathcal{R}(H)\\rangle\\mid\\mathcal{S}\\big]\\leq-\\alpha_{0}.\n(27)\nIf the residual operator\nâ„›\n\\mathcal{R}\nis designed to follow the descent direction of the flow-matching loss on aligned samples (so that\nğ”¼\nâ€‹\n[\nâŸ¨\nâˆ‡\nH\nâ„“\n,\nâ„›\nâ€‹\n(\nH\n)\nâŸ©\nâˆ£\nğ’œ\n]\n<\n0\n\\mathbb{E}[\\langle\\nabla_{H}\\ell,\\mathcal{R}(H)\\rangle\\mid\\mathcal{A}]<0\n), then the gate\ng\ng\nfurther amplifies these contributions relative to those from\nğ’®\n\\mathcal{S}\n. Intuitively, the residual update â€œtrustsâ€ the directions suggested by geometrically consistent pairs and downweights those arising from high-discrepancy configurations. This is exactly the intuition formalized in Theorem\n5.1\n: for small enough\nÎ»\n\\lambda\n, the first-order improvement from aligned regions dominates the second-order smoothness penalty.\nRefinement as gated fixed-point iteration.\nFinally, the refinement operator\nT\nâ€‹\n(\nZ\n)\n=\nZ\nâˆ’\nÎ±\nâ€‹\nE\nâ€‹\n(\nZ\n;\ng\n)\nT(Z)=Z-\\alpha\\,E(Z;g)\ncan be viewed as a fixed-point iteration on the space of action embeddings. Under strong monotonicity and Lipschitz regularity, Theorem\n5.1\nshows that\nT\nT\nis a contraction. In practice, this means that once the gate\ng\ng\nhas identified â€œreliableâ€ observationâ€“action configurations, the refinement map can iteratively pull predicted actions toward a geometry-consistent fixed point\nZ\nâ‹†\nZ^{\\star}\nwithout exploding or oscillating. The contraction rate\nÏ\n<\n1\n\\rho<1\nquantifies how quickly refinement concentrates probability mass around\nZ\nâ‹†\nZ^{\\star}\n.\nFigure\n3\nsummarizes the overall picture: discrepancy-guided gating reshapes the effective gradient contributions of different regions of the data space, the residual update exploits this reweighting to improve features in a controlled range of strengths, and the refinement operator provides a stable way to further align actions with observation geometry at inference time.\n5.3\nApproximation stability of transport-based discrepancy\nFinally, we discuss how the theoretical guarantees behave when the exact 2-Wasserstein distance is replaced by practical approximations such as sliced Wasserstein or Sinkhorn divergence.\nSliced Wasserstein concentration.\nAssume that observation and action embeddings lie in a bounded ball of radius\nR\nR\nin\nâ„\nd\n\\mathbb{R}^{d}\n. For each random projection direction\nÏ‰\nm\nâˆˆ\nğ•Š\nd\nâˆ’\n1\n\\omega_{m}\\in\\mathbb{S}^{d-1}\n, the corresponding one-dimensional Wasserstein distance between the projected empirical measures is bounded by\n(\n2\nâ€‹\nR\n)\n2\n(2R)^{2}\n. The sliced Wasserstein estimator averages\nM\nM\nsuch terms, so by Hoeffdingâ€™s inequality, for any\nÏµ\n>\n0\n\\epsilon>0\n,\nPr\nâ¡\n[\n|\nD\n^\nâˆ’\nğ”¼\nâ€‹\n[\nD\n^\n]\n|\n>\nÏµ\n]\nâ‰¤\n2\nâ€‹\nexp\nâ¡\n(\nâˆ’\nM\nâ€‹\nÏµ\n2\n2\nâ‹…\n(\n2\nâ€‹\nR\n)\n4\n)\n.\n\\Pr\\!\\big[\\,|\\widehat{D}-\\mathbb{E}[\\widehat{D}]|>\\epsilon\\,\\big]\\;\\leq\\;2\\exp\\!\\Bigl(-\\frac{M\\epsilon^{2}}{2\\cdot(2R)^{4}}\\Bigr).\n(28)\nThus the estimation error scales as\nO\nâ€‹\n(\nM\nâˆ’\n1\n/\n2\n)\nO(M^{-1/2})\nwith high probability. Since the gate is obtained via a Lipschitz weight map\ng\n=\nÏ•\nâ€‹\n(\nD\n)\ng=\\phi(D)\nwith constant\nL\nÏ•\nL_{\\phi}\n(Assumption\n8.1\n), we have\n|\ng\nâˆ’\nğ”¼\nâ€‹\n[\ng\n]\n|\n=\n|\nÏ•\nâ€‹\n(\nD\n^\n)\nâˆ’\nÏ•\nâ€‹\n(\nğ”¼\nâ€‹\n[\nD\n^\n]\n)\n|\nâ‰¤\nL\nÏ•\nâ€‹\n|\nD\n^\nâˆ’\nğ”¼\nâ€‹\n[\nD\n^\n]\n|\n,\n|g-\\mathbb{E}[g]|\\;=\\;|\\phi(\\widehat{D})-\\phi(\\mathbb{E}[\\widehat{D}])|\\;\\leq\\;L_{\\phi}\\,|\\widehat{D}-\\mathbb{E}[\\widehat{D}]|,\n(29)\nso the gate error inherits the same\nO\nâ€‹\n(\nM\nâˆ’\n1\n/\n2\n)\nO(M^{-1/2})\nconcentration rate. In turn, the descent and contraction inequalities in Theorems\n5.1\nâ€“\n5.1\nremain valid up to multiplicative constants that depend smoothly on\nL\nÏ•\nL_{\\phi}\n.\nSinkhorn divergence.\nFor the Sinkhorn divergence\nS\nÎµ\nS_{\\varepsilon}\nwith entropic regularization\nÎµ\n>\n0\n\\varepsilon>0\n, standard results on entropic optimal transport show that\n|\nS\nÎµ\nâˆ’\nW\n2\n2\n|\nâ‰¤\nb\nâ€‹\n(\nÎµ\n)\n,\nb\nâ€‹\n(\nÎµ\n)\nâ†’\n0\nâ€‹\nas\nâ€‹\nÎµ\nâ†’\n0\n,\n\\big|S_{\\varepsilon}-W_{2}^{2}\\big|\\;\\leq\\;b(\\varepsilon),\\quad b(\\varepsilon)\\to 0\\text{ as }\\varepsilon\\to 0,\n(30)\nand empirical estimators of\nS\nÎµ\nS_{\\varepsilon}\nconcentrate around their population values at rate\nO\nâ€‹\n(\nn\nâˆ’\n1\n/\n2\n)\nO(n^{-1/2})\nin the number of samples\nn\nn\n, with constants depending on\nÎµ\n\\varepsilon\n. Propagating this perturbation through the Lipschitz map\nÏ•\n\\phi\nagain yields a bounded perturbation of the gate:\n|\nÏ•\nâ€‹\n(\nS\n^\nÎµ\n)\nâˆ’\nÏ•\nâ€‹\n(\nW\n2\n2\n)\n|\nâ‰¤\nL\nÏ•\nâ€‹\n(\nb\nâ€‹\n(\nÎµ\n)\n+\nC\nâ€‹\n(\nÎµ\n)\nâ€‹\nn\nâˆ’\n1\n/\n2\n)\n,\n\\big|\\phi(\\widehat{S}_{\\varepsilon})-\\phi(W_{2}^{2})\\big|\\;\\leq\\;L_{\\phi}\\Bigl(b(\\varepsilon)+C(\\varepsilon)\\,n^{-1/2}\\Bigr),\n(31)\nfor some constant\nC\nâ€‹\n(\nÎµ\n)\nC(\\varepsilon)\n. Consequently, using Sinkhorn divergence instead of exact Wasserstein distance only affects the numerical constants in our bounds; the qualitative guarantees (existence of a positive descent region in\nÎ»\n\\lambda\n, and contraction of the fixed-gate refinement) remain unchanged.\n6\nExperiments\nWe evaluate\nDiG-Flow\non challenging robotic manipulation benchmarks to assess its ability to improve robustness and generalization.\nOur experiments are designed to answer three questions:\n1.\nDoes discrepancy-guided flow matching improve VLA performance?\n2.\nHow does\nDiG-Flow\ncompare to state-of-the-art VLA baselines?\n3.\nWhich design choices of\nDiG-Flow\nare most important?\nTo demonstrate that\nDiG-Flow\nis backbone-agnostic, we select two of the most widely used flow-matching based VLA models,\ni.e\n.,\nÏ€\n0.5\n\\pi_{0.5}\n[\n4\n]\nand GR00T-N1\n[\n29\n]\n, as backbone architectures to evaluate the effectiveness of\nDiG-Flow\nacross different model designs. For\nÏ€\n0.5\n\\pi_{0.5}\n, we follow the official default configuration. We then insert\nDiG-Block\nbefore the final transformer layer of the VLM backbone, sharing the same observation features as the original policy head. For GR00T-N1, we use the standard tabletop configuration and insert\nDiG-Block\nat the final token representation used by the action head. In both models,\nDiG-Block\noperates purely at the representation level and does not modify the underlying flow-matching architecture or training objective. We refer to the two variants as\nÏ€\n0.5\n\\pi_{0.5}\n-\nDiG\nand GR00T-N1-\nDiG\n, respectively.\n6.1\nSimulation Experiments\n6.1.1\nExperimental Setup\nLIBERO Benchmark.\nLIBERO\n[\n52\n]\nis a standardized benchmark for tabletop robotic manipulation with language-conditioned tasks. It consists of four 10-task suites: LIBERO-Spatial (spatial reasoning and object placement), LIBERO-Object (diverse object properties and appearances), LIBERO-Goal (goal-conditioned instructions), and LIBERO-Long (long-horizon, multi-step tasks). Models are evaluated by success rate over 50 rollouts per task, following the official protocol.\nRoboCasa Benchmark.\nRoboCasa\n[\n53\n]\nis a scalable household robotics simulation platform built on NVIDIA Isaac Sim, providing photorealistic kitchen scenes and diverse manipulation tasks such as pick-and-place, opening cabinets, and interacting with appliances. We follow the 24 atomic manipulation tasks used in prior work on RoboCasa tabletop evaluation, which group naturally into three categories: (i) pick-and-place, (ii) opening and closing doors or drawers, and (iii) other manipulation skills such as pressing buttons or sliding objects. Each task comes with 50 human demonstrations and thousands of additional synthetic rollouts; to stress generalization in a low-data regime,\nwe train all models using only 50 demonstrations per task\n(one trajectory per demonstration), leaving the synthetic data unused.\nTraining details.\nUnless otherwise specified, we use the same optimizer and data pipeline as the corresponding backbone. For\nÏ€\n0.5\n\\pi_{0.5}\n-DiG on LIBERO, we train for 30K steps using AdamW with a global batch size of 256 across 8 A100 GPUs, a peak learning rate of\n5\nÃ—\n10\nâˆ’\n5\n5\\times 10^{-5}\n, and cosine decay. The flow-matching head follows the standard squared regression loss and training-time enhancement. By default, our\nDiG-Block\nmodules use 32 sliced projections for the discrepancy, a temperature of\nÏ„\n=\n1.0\n\\tau=1.0\n, residual strength\nÎ»\n=\n0.4\n\\lambda=0.4\n, and a spectral norm bound of\nB\nâ„›\n=\n2.0\nB_{\\mathcal{R}}=2.0\n. For GR00T-N1-\nDiG\n, we follow the official GR00T-N1 training and evaluation protocol and similarly enable\nDiG-Block\nduring fine-tuning.\n6.1.2\nMain Results on LIBERO\nTable\n1\nshows that\nDiG-Flow\nimproves both backbones and achieves state-of-the-art results on LIBERO. On top of the strong\nÏ€\n0.5\n\\pi_{0.5}\nbaseline,\nÏ€\n0.5\n\\pi_{0.5}\n-DiG increases the average success rate from\n96.9\n%\n96.9\\%\nto\n98.3\n%\n98.3\\%\n, with the largest gain on LIBERO-Long (\n92.4\n%\nâ†’\n96.4\n%\n92.4\\%\\!\\to\\!96.4\\%\n,\n+\n4.0\n+4.0\npoints). This suite contains long-horizon, multi-step tasks, so the improvement supports our hypothesis that discrepancy-guided modulation reduces error accumulation by encouraging semantically aligned representations.\nTable 1\n:\nSuccess rates (%) on LIBERO.\nWe report mean success over 50 evaluation episodes per task.\nDiG-Flow\nconsistently improves both the\nÏ€\n0.5\n\\pi_{0.5}\nand GR00T-N1 backbones. The largest gains appear on LIBERO-Long, which contains complex multi-step tasks.\nMethod\nLIBERO-Spatial\nLIBERO-Object\nLIBERO-Goal\nLIBERO-Long\nAverage\nDiffusion Policy\n[\n54\n]\n78.5\n87.5\n73.5\n64.8\n76.1\nSpatialVLA\n[\n26\n]\n88.2\n89.9\n78.6\n55.5\n78.1\nCoT-VLA\n[\n30\n]\n87.5\n91.6\n87.6\n69.0\n83.9\nOpenVLA\n[\n2\n]\n84.7\n88.4\n79.2\n53.7\n76.5\nOpenVLA-OFT\n[\n55\n]\n97.6\n98.4\n97.9\n94.5\n97.1\nGR00T-N1\n[\n29\n]\n94.4\n97.6\n93.0\n90.6\n93.9\nÏ€\n0\n\\pi_{0}\n[\n3\n]\n98.0\n96.8\n94.4\n88.4\n94.4\nÏ€\n0\n\\pi_{0}\n-Fast\n[\n28\n]\n96.4\n96.8\n88.6\n60.2\n85.5\nÏ€\n0.5\n\\pi_{0.5}\n[\n4\n]\n98.8\n98.2\n98.0\n92.4\n96.9\nGR00T-N1-DiG (Ours)\n96.0\n98.4\n94.8\n92.1\n95.3\nÏ€\n0.5\n\\pi_{0.5}\n-DiG (Ours)\n99.2\n99.0\n98.6\n96.4\n98.3\nGR00T-N1-DiG exhibits similar behavior, improving the average success rate from\n93.9\n%\n93.9\\%\nto\n95.3\n%\n95.3\\%\n. Again, the gain is most pronounced on LIBERO-Long (\n90.6\n%\nâ†’\n92.1\n%\n90.6\\%\\!\\to\\!92.1\\%\n), suggesting that\nDiG-Flow\nconsistently helps different architectures handle long-horizon reasoning. Importantly, the relative ordering of backbones is preserved:\nÏ€\n0.5\n\\pi_{0.5}\n-DiG remains stronger than GR00T-N1-DiG, indicating that\nDiG-Flow\nacts as a plug-and-play robustness layer rather than fundamentally changing the underlying policy capacity.\n6.1.3\nMain Results on RoboCasa (Few-Shot)\nRoboCasa poses a substantially harder generalization challenge than LIBERO: scenes are more diverse, the observation space is more visually complex, and most tasks involve contact-rich manipulation in cluttered kitchens. Moreover, our training regime is intentionally low-data, using only 50 demonstrations per task, which is roughly the number of human demonstrations provided per atomic task in the official benchmark.\nAs shown in Table\n2\n,\nDiG-Flow\nyields large gains in this setting. For\nÏ€\n0.5\n\\pi_{0.5}\n, integrating\nDiG-Flow\nimproves the average success rate from\n41.4\n%\n41.4\\%\nto\n52.6\n%\n52.6\\%\n(\n+\n11.2\n+11.2\npoints). Improvements are visible in all three categories, but are especially pronounced for door/drawer manipulation (\n+\n15.6\n+15.6\npoints), where precise contact and long-horizon geometry are critical. GR00T-N1 also benefits: GR00T-N1-DiG improves the average success rate from\n36.0\n%\n36.0\\%\nto\n43.2\n%\n43.2\\%\n(\n+\n7.2\n+7.2\npoints). The fact that both backbones gain substantially from the same\nDiG-Flow\nmodule, under severely limited data, reinforces our interpretation of discrepancy as a useful geometric regularizer that reduces overfitting to spurious correlations.\nTable 2\n:\nRoboCasa 24-task benchmark with 50 demonstrations per task.\nWe report success rates (%) averaged within categories and across all tasks. All methods are trained with only 50 demonstrations per task.\nDiG-Flow\nsubstantially improves performance in this challenging few-shot regime.\nMethod\nPick & Place\nDoors / Drawers\nOthers\n24-Task Avg\nÏ€\n0.5\n\\pi_{0.5}\n21.5\n57.8\n44.9\n41.4\nÏ€\n0.5\n\\pi_{0.5}\n-DiG (Ours)\n27.2\n73.4\n57.2\n52.6\nGR00T-N1\n18.6\n50.2\n39.1\n36.0\nGR00T-N1-DiG (Ours)\n22.4\n60.3\n47.0\n43.2\n6.1.4\nRobustness Analysis (Simulation)\nTo further probe robustness in a controlled setting, we adopt non-stationary perturbations following previous standards in non-stationarity studies\n[\n56\n,\n49\n]\n. These perturbations apply time-varying sinusoidal noise to both visual observations and proprioceptive states:\nc\n1\nâ€‹\ncos\nâ¡\n(\nc\n2\nâ€‹\nt\n)\n,\nc\n3\nâ€‹\nsin\nâ¡\n(\nc\n4\nâ€‹\nt\n)\nc_{1}\\cos(c_{2}t),c_{3}\\sin(c_{4}t)\n, with coefficients\nc\ni\nâˆ¼\nğ’©\nâ€‹\n(\n0.01\n,\n0.5\n)\nc_{i}\\sim\\mathcal{N}(0.01,0.5)\n. Such perturbations are designed to disrupt brittle correlations that depend on static visual patterns or stereotyped trajectories, while leaving the underlying task semantics intact.\nTable 3\n:\nRobustness to non-stationary perturbations.\nSuccess rates (%) under different time-varying noise patterns that disrupt memorized shortcuts.\nDiG-Flow\nshows consistent improvements across perturbation types, with the most significant gains on complex long-horizon tasks.\nPerturbation\nMethod\nLIBERO-Spatial\nLIBERO-Object\nLIBERO-Goal\nLIBERO-Long\nAvg\nCosine only\nw/o\nDiG-Flow\n87.6\n89.8\n86.4\n70.2\n83.5\nw/\nDiG-Flow\n87.4\n-0.2%\n89.8\n0%\n91.6\n+6.0%\n80.0\n+14.0%\n87.2\n+4.4%\nSine only\nw/o\nDiG-Flow\n88.2\n88.4\n87.2\n68.6\n83.1\nw/\nDiG-Flow\n90.0\n+2.0%\n90.6\n+2.5%\n91.2\n+4.6%\n80.4\n+17.2%\n88.1\n+6.0%\nCosine + sine\nw/o\nDiG-Flow\n87.8\n87.2\n86.0\n67.8\n82.2\nw/\nDiG-Flow\n86.4\n-1.6%\n88.2\n+1.1%\n90.8\n+5.6%\n79.2\n+16.8%\n86.2\n+4.0%\nAs shown in Table\n3\n,\nDiG-Flow\nconsistently improves robustness across all perturbation types, with average gains between roughly 4â€“6 points. The improvements are most dramatic on LIBERO-Long, where long-horizon execution is particularly sensitive to small systematic biases. Spatial suites show smaller changes, and in a few cases performance is slightly reduced, reflecting that not all perturbations exclusively target shortcuts. Overall, these results support the view that discrepancy-guided gating makes policies less reliant on brittle, non-stationary correlations and more grounded in the geometric relationship between observations and actions.\n6.2\nReal Robot Experiments\n6.2.1\nReal Robot Setup\nFigure 4\n:\n(a)\nReal robot hardware setup (single-arm + dexterous hand.\n(b)\nExamples of real robot tasks. The two rows correspond to the two camera views.\nWe build a real robot setup similar to LIBERO and RoboCasa to validate our experiments in a more challenging real-world physical environment. We use a 7-DoF Franka Research 3 (FR3) arm equipped with a 6-DoF Inspire dexterous hand. Perception is provided by two Intel RealSense cameras mounted at complementary viewpoints, forming a stereo-like arrangement that reduces single-view occlusions. Both cameras stream synchronized RGB frames. The VLM backbone receives a fused representation of these views together with proprioceptive inputs (joint positions, velocities, and gripper states). Commands are sent to the low-level controller at 20â€‰Hz. Following the chunked-action protocol, the policy outputs action chunks over a short horizon, which are then executed in open-loop without additional model-predictive control or trajectory optimization. The overall hardware layout is illustrated in Figure\n4\n(a).\nCompared to a simple two-finger gripper, the dexterous hand substantially increases the control dimensionality and contact complexity: the policy must not only reach the correct pose, but also coordinate multiple finger joints for stable power and precision grasps, tool operation (\ne.g\n., spraying, wiping), and in-hand adjustments. This task setting further exposes policy errors: inaccurate predictions cannot be easily corrected by low-level planners, making robustness and alignment particularly important.\nTo assess realâ€world applicability of\nDiG-Flow\n, we design four distinct tasks covering different manipulation challenges. These tasks impose realistic 3D geometry, contact dynamics, and visual complexity, making them ideal for testing the hypothesis that discrepancy-guided modulation improves semantic grounding and robustness. We now detail the four real-robot tasks. These tasks are designed to mirror the difficulty patterns observed in LIBERO and RoboCasa while exposing additional challenges from real-world physics, sensor noise, and human interference. The tasksâ€™ example settings are shown in Figure\n4\n(b).\nâ€¢\nStack-Bowls\n: stack bowls from scattered initial positions into a tower; tests precise multi-object grasping and placement, as well as collision avoidance between the hand and previously stacked bowls.\nâ€¢\nSpray-Plant\n: pick up a spray bottle and apply water mist to a potted plant along its foliage; tests tool-use behavior, fine motion control, and spatial anticipation to cover the plant surface.\nâ€¢\nWipe-Whiteboard\n: use a cloth to erase pen marks from a whiteboard; tests contact-rich surface interaction, maintaining stable pressure, and long-horizon sweeping motions.\nâ€¢\nSort-Into-Drawer\n: pick three varied objects and place them into a drawer one by one; tests sequential multi-step reasoning, diverse grasping strategies, and error accumulation over multiple object transfers.\nFor each task we only collect 50 human-teleoperated demonstrations for training, and evaluate for both whole-task success and sub-task success. Stack-Bowls has 5 subtasks, while the other tasks have 3 subtasks each. We report overall success rates and detailed analysis on perturbation and robustness tests.\nFigure 5\n:\nReal robot rollouts on four manipulation tasks.\nEach row shows a successful execution by\nÏ€\n0.5\n\\pi_{0.5}\n-DiG from left to right.\n(a)\nStack-Bowls\n: the robot sequentially grasps and stacks three bowls into a stable tower.\n(b)\nSpray-Plant\n: the robot picks up a spray bottle and waters the plant along its foliage.\n(c)\nWipe-Whiteboard\n: the robot wipes away pen marks on the board using a cloth.\n(d)\nSort-Into-Drawer\n: the robot picks and places three objects into a drawer, a long-horizon task that compounds errors across steps.\nThese qualitative rollouts illustrate that\nDiG-Flow\nenables stable multi-step behaviours across diverse contact-rich settings.\nTable 4\n:\nSuccess rates (%) of our models vs baseline on real robot tasks.\nWe show sub-task / whole-task success rates.\nTask\nStack-Bowls\nSpray-Plant\nWipe-Whiteboard\nSort-Into-Drawer\nÏ€\n0.5\n\\pi_{0.5}\n(baseline)\n58 / 40\n66 / 45\n62 / 38\n52 / 33\nÏ€\n0.5\n\\pi_{0.5}\n-DiG (Ours)\n62\n/\n48\n72\n/\n52\n70\n/\n48\n60\n/\n41\nTable\n4\nshows that\nDiG-Flow\nconsistently improves performance across all four tasks. On the long-horizon Sort-Into-Drawer task, whole-task success increases from 33% to 41% (+8 pts), illustrating that\nDiG-Flow\nhelps mitigate error accumulation in sequential decision chains. The more moderate gains on Spray-Plant and Wipe-Whiteboard reflect shorter horizons but still show benefit, particularly in sub-task success.\n6.2.2\nRobustness Analysis (Real Robot)\nWe next analyze how\nDiG-Flow\nbehaves under real-world perturbations that were not in the training demonstrations. The perturbed setups are illustrated in Figure\n6\n, and the corresponding success rates are reported in Table\n5\n.\nTable 5\n:\nSuccess rates (%) under unseen / perturbed conditions.\nWe show sub-task / whole-task success rates.\nMethod\nStack-Bowls (BG)\nSort-Into-Drawer (BG)\nSpray-Plant (Human)\nWipe-Whiteboard (Human)\nÏ€\n0.5\n\\pi_{0.5}\n42 / 15\n48 / 20\n35 / 10\n44 / 20\nÏ€\n0.5\n\\pi_{0.5}\n-DiG (Ours)\n65 / 40\n60 / 35\n62 / 30\n58 / 30\nFigure 6\n:\nUnseen and interfered real-world settings.\nWe evaluate robustness under\n(a)\nBackground Shifts\n, where table textures, cloths, and distractor objects are changed for Stack-Bowls and Sort-Into-Drawer, and\n(b)\nHuman Interference\n, where a human hand moves the plant or writes on the whiteboard during wiping.\nThese perturbations introduce visual and dynamic distribution shifts beyond the training data and correspond to the robustness results in Table\n5\n.\nBackground Shifts.\nFor Stack-Bowls and Sort-Into-Drawer, we change the table cloth color and pattern, add additional objects near the workspace, and vary global lighting conditions. Despite these shifts, our policy with\nDiG-Flow\nmaintains high success rates that are close to the unperturbed setting: most failures are due to rare extreme occlusions rather than systematic mistakes. Qualitatively, we observe that the robot continues to focus on the geometry of task-relevant objects (bowls, drawer, target items) even when the background appearance changes substantially, suggesting that the transport-based gating helps deprioritize accidental correlations with background textures.\nHuman Interference.\nFor Spray-Plant and Wipe-Whiteboard, we introduce a human hand that moves the plant or writes new strokes on the board while the robot is executing the policy. These interventions create both visual distractions and true dynamical changes (targets moving during execution). We find that the policy often adapts by slightly adjusting its trajectory to follow the plant motion or to cover newly written strokes, and overall success remains high relative to the no-interference baseline. Failures typically arise when the human intervention is adversarially timed (\ne.g\n., moving the plant exactly as the robot closes its grasp), which is challenging for any open-loop controller.\nTable\n5\nshows that all four perturbed variants are challenging: both policies see noticeable drops\ncompared to the clean setting, but\nÏ€\n0.5\n\\pi_{0.5}\n-DiG remains consistently more\nrobust. On\nStack-Bowls (BG)\nand\nSpray-Plant (Human)\n, the baseline\nwhole-task success falls to\n15\n%\n15\\%\nand\n10\n%\n10\\%\n, while\nÏ€\n0.5\n\\pi_{0.5}\n-DiG still\nachieves\n40\n%\n40\\%\nand\n30\n%\n30\\%\n, respectively. Even on the harder long-horizon\nSort-Into-Drawer (BG)\nscenario,\nÏ€\n0.5\n\\pi_{0.5}\n-DiG improves whole-task\nsuccess from\n20\n%\n20\\%\nto\n35\n%\n35\\%\n. A similar advantage appears for\nWipe-Whiteboard (Human)\n, where\nÏ€\n0.5\n\\pi_{0.5}\n-DiG reaches\n30\n%\n30\\%\nvs.\n20\n%\n20\\%\nfor the baseline. Overall, the relative gap between the two models is preserved\nor enlarged under background and human interference, indicating that\ndiscrepancy-guided modulation helps maintain stable observation-action\nalignment rather than overfitting to specific training scenes.\nOverall, these results support the view that geometry-aware gating improves robustness in the real world: the policy is less sensitive to superficial background statistics and more focused on the observationâ€“action alignment relevant for the task.\n6.2.3\nHigh-DoF Humanoid with Active View Control\nFigure 7\n:\nHigh-DoF humanoid setup.\nOur 31-DoF humanoid platform with dual dexterous hands and an actively controlled head. The head mounts dual RGB cameras, while the torso and arms provide upper-body motion. The task is to clear 1-3 objects from the table into the box. This setting requires the policy to jointly coordinate head, body, and hands, rather than controlling only a low-DoF gripper from a fixed camera.\nTo further stress-test\nDiG-Flow\non more complex embodiments, we deploy\nÏ€\n0.5\n\\pi_{0.5}\nand\nÏ€\n0.5\n\\pi_{0.5}\n-DiG on a 31-DoF upper-body humanoid platform equipped with dual dexterous hands and an actively controlled head camera. As illustrated in Figure\n7\n, the robot has independently actuated joints for the torso, neck, arms, and multi-finger hands, resulting in substantially higher dimensionality and contact complexity than traditional real-robot experiments. Unlike prior VLA settings that operate from a fixed third-person view and control only a low-DoF end-effector, our policy must simultaneously coordinate head,\nbody, and both hands, making both state estimation and control more challenging.\nThe head houses a pair of RGB cameras that provide an egocentric, movable viewpoint. During teleoperation, the operator naturally moves the head to obtain task-relevant views. The policy is trained to imitate this behavior and thus learns to actively adjust the viewpoint while manipulating objects. Figure\n8\nshows an example rollout: the robot first orients the head to obtain a clear view of the workspace, then uses dexterous hands to pick up objects and place them into a box while continuously refining its viewing angle. This active-view setup is particularly sensitive to representation misalignment.\nFigure 8\n:\nEgocentric active-view rollouts.\nExample sequence from the head-mounted camera during a clean-up episode. The humanoid first adjusts its viewpoint to obtain a clear observation of the workspace, then successively grasps objects with its dexterous hands and places them into the box.\nDiG-Flow\nhelps maintain robust observationâ€“action alignment under camera motion and self-occlusions.\nWe collect 1000 teleoperated demonstrations of general tabletop clean-up tasks, where the robot is instructed to place 1-3 objects on the table into a box from diverse initial layouts.\nBoth\nÏ€\n0.5\n\\pi_{0.5}\nand\nÏ€\n0.5\n\\pi_{0.5}\n-DiG are trained on this dataset with a batch size of 256 for 20K training steps, using the same flow-matching configuration as in our other real-robot experiments. We evaluate each model on 20 rollouts per condition, and report success rates in Table\n6\n. The â€œSeenâ€ setting uses the same object categories and background\nas training, while the â€œUnseenâ€ setting replaces both objects and background textures to induce distribution shift.\nTable 6\n:\nSuccess rates (%) on high-DoF humanoid clean-up tasks.\nEach entry reports the success rate over 20 evaluation rollouts for clearing 1-3 objects into the box. â€œSeenâ€ uses the same object categories and background as the demonstrations, while â€œUnseenâ€ uses novel objects and background textures.\nSeen\nUnseen & Perturbed\nMethod\n1 obj\n2 objs\n3 objs\n1 obj\n2 objs\n3 objs\nÏ€\n0.5\n\\pi_{0.5}\n75\n60\n35\n55\n30\n25\nÏ€\n0.5\n\\pi_{0.5}\n-DiG\n75\n65\n45\n65\n45\n40\nOn this challenging real-robot setting, the standard\nÏ€\n0.5\n\\pi_{0.5}\nbaseline achieves only moderate performance, particularly when the scene contains multiple objects or unseen visual conditions. In contrast,\nÏ€\n0.5\n\\pi_{0.5}\n-DiG consistently improves success rates by 5-10 points across seen objects and backgrounds. As for unseen objects and background shift, the gains are even more pronounced. The advantages brought by\nDiG-Flow\nare also more obvious in complex multi-object tasks.\nThese results indicate that discrepancy-guided modulation remains effective even when the policy coordinates a high-DoF humanoid with active view control, providing both stronger performance and more robust behavior under visual and dynamical perturbations.\n6.3\nFurther Discussion on Method Design\nIn this section, we further explain the method design of\nDiG-Flow\nthrough ablation studies and mechanism analyses on the standard LIBERO benchmark.\n6.3.1\nDiscrepancy and Gating Ablations\nOur method has two key design choices: the discrepancy\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\nD(\\mu_{H},\\mu_{Z})\nthat measures observationâ€“action mismatch, and the scalar gate\ng\n=\nÏ•\nâ€‹\n(\nD\n)\ng=\\phi(D)\nthat maps this signal back to the backbone.\nWe therefore ablate these two components separately.\nTable 7\n:\nEffect of discrepancy choice on LIBERO (success %).\nAll rows use the same\nÏ€\n0.5\n\\pi_{0.5}\nbackbone, scalar mapping\ng\n=\nexp\nâ¡\n(\nâˆ’\nÏ„\nâ€‹\nD\n)\ng=\\exp(-\\tau D)\n, and residual operator; only the discrepancy\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\nD(\\mu_{H},\\mu_{Z})\nis varied.\nWasserstein-based discrepancies provide the strongest signal, especially on LIBERO-Long.\nDiscrepancy\nLIBERO-Spatial\nLIBERO-Object\nLIBERO-Goal\nLIBERO-Long\nAvg\nCosine distance\n98.2\n-1.0%\n97.8\n-1.2%\n97.2\n-1.4%\n92.6\n-3.8%\n96.5\n-1.8%\nMMD (RBF kernel)\n98.4\n-0.8%\n98.2\n-0.8%\n97.6\n-1.0%\n93.4\n-3.0%\n96.8\n-1.5%\nSinkhorn (\nÎµ\n=\n0.1\n\\varepsilon=0.1\n)\n98.8\n-0.4%\n98.6\n-0.4%\n98.2\n-0.4%\n95.8\n-0.6%\n97.9\n-0.4%\nSliced Wasserstein (Ours)\n99.2\n99.0\n98.6\n96.4\n98.3\nTable\n7\nexamines how different discrepancy functions affect performance when applied to\nÏ€\n0.5\n\\pi_{0.5}\n-DiG on LIBERO.\nAll variants use the same scalar mapping\ng\n=\nexp\nâ¡\n(\nâˆ’\nÏ„\nâ€‹\nD\n)\ng=\\exp(-\\tau D)\nand residual operator; only\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\nD(\\mu_{H},\\mu_{Z})\nis changed.\nNon-transport discrepancies such as cosine distance and MMD already provide moderate gains over the backbone (average\n96.5\n%\n96.5\\%\nand\n96.8\n%\n96.8\\%\n), confirming that even coarse measures of observationâ€“action similarity can be beneficial.\nHowever, Wasserstein-based discrepancies consistently perform better.\nSinkhorn divergence (entropic OT) achieves\n97.9\n%\n97.9\\%\naverage success, while our default sliced 2-Wasserstein distance attains\n98.3\n%\n98.3\\%\nand the best results on LIBERO-Long.\nThis matches our theoretical intuition: Wasserstein distances respect the underlying geometry of the feature space, yielding a more faithful measure of distributional misalignment and a better-behaved supervision signal for the gate.\nTable 8\n:\nComparison of gating mechanisms.\nSuccess rates (%) for different gating strategies with the discrepancy fixed to sliced 2-Wasserstein.\nIn contrast to fixed or random modulation, our transport gate converts the geometric discrepancy into a meaningful confidence signal, yielding the best performance, especially on LIBERO-Long.\nGating Strategy\nLIBERO-Spatial\nLIBERO-Object\nLIBERO-Goal\nLIBERO-Long\nAvg\nFixed Gate (\ng\n=\n0.5\ng=0.5\n)\n96.8\n-2.4%\n95.4\n-3.6%\n92.2\n-6.5%\n84.6\n-12.2%\n92.3\n-6.1%\nRandom Gate (\ng\nâˆ¼\nğ’°\nâ€‹\n(\n0\n,\n1\n)\ng\\sim\\mathcal{U}(0,1)\n)\n95.2\n-4.0%\n93.8\n-5.3%\n89.4\n-9.3%\n80.8\n-16.2%\n89.8\n-8.7%\nTransport Gate (Ours)\n99.2\n99.0\n98.6\n96.4\n98.3\nTable\n8\ncomplements this study by fixing the sliced 2-Wasserstein discrepancy and varying only the gating mechanism.\nReplacing our transport gate with a fixed scalar (\ng\n=\n0.5\ng=0.5\n) reduces performance to\n92.3\n%\n92.3\\%\naverage success (-6.1%), indicating that uniform feature modulation cannot distinguish between semantically useful and spurious patterns.\nRandom gating performs even worse at\n89.8\n%\n89.8\\%\n(-8.7%), showing that arbitrary feature suppression actively harms learning by disrupting both beneficial and harmful correlations.\nThe degradation is particularly severe on LIBERO-Long (up to -16.2%), where long-horizon reasoning amplifies the effect of shortcut solutions.\nIn contrast, the discrepancy-guided transport gate preserves performance on simpler suites and yields the largest gains on LIBERO-Long, consistent with our interpretation that geometric alignment is most critical in sequential tasks.\nTaken together, Tables\n7\nand\n8\nshow that both the choice of discrepancy and the shape of the gate\nÏ•\nâ€‹\n(\nD\n)\n\\phi(D)\nmatter for robust optimization.\n6.3.2\nRefinement Ablation\nIn Section\n5.1\n, we state that performance saturates within 3 refinement steps. Here we provide a more detailed analysis of this behavior on the standard LIBERO benchmark.\nFigure 9\n:\nEffect of refinement steps\nN\nrefine\nN_{\\text{refine}}\non LIBERO.\nWe vary the number of refinement iterations at inference time while keeping the training setup fixed.\nEven without refinement (\nN\nrefine\n=\n0\nN_{\\text{refine}}{=}0\n), the discrepancy-guided flow model already surpasses the backbone policy. Adding a small number of refinement steps further improves performance and quickly saturates around\nN\nrefine\n=\n3\nN_{\\text{refine}}{=}3\n.\nFigure\n9\nreports the average LIBERO success rate when we vary the number of refinement iterations\nN\nrefine\nN_{\\text{refine}}\nat test time.\nWe observe three consistent trends:\nâ€¢\nPerformance gains even with\nN\nrefine\n=\n0\nN_{\\text{refine}}{=}0\n.\nThe curve shows that the\nN\nrefine\n=\n0\nN_{\\text{refine}}{=}0\nvariant already achieves a strong success rate on LIBERO.\nThis indicates that the discrepancy-guided residual update improves the learned policy itself, rather than only acting as a test-time heuristic.\nâ€¢\nFew refinement steps bring stable improvements.\nIncreasing\nN\nrefine\nN_{\\text{refine}}\nfrom 0 to 3 yields consistent performance gains and quickly brings the curve close to saturation.\nThis supports our choice of using\nN\nrefine\n=\n3\nN_{\\text{refine}}{=}3\nby default: it captures most of the benefit of refinement while keeping the additional computation per control step modest.\nâ€¢\nBeyond 3 steps, performance saturates with small fluctuations.\nFor\nN\nrefine\n>\n3\nN_{\\text{refine}}>3\n, the success rate fluctuates slightly around the same plateau level, without systematic degradation.\nWe attribute the small variations to evaluation noise and the fact that once the flow model has already produced a near-optimal action chunk, further refinement only provides marginal adjustments.\nOverall, these results confirm that (i)\nDiG-Flow\nis already useful without refinement, and (ii) a small number of refinement steps provides a good trade-off between robustness/performance and inference cost.\n6.3.3\nHyperparameter Analysis\nFigure 10\n:\nHyperparameter sensitivity of\nDiG-Flow\n.\n(a)\nSuccess rate as a function of the number of sliced projections\nK\nK\nused for approximating the Wasserstein discrepancy.\nPerformance improves steadily from very small\nK\nK\nand saturates once\nK\nâ‰ˆ\n28\nK\\approx 28\nâ€“\n32\n32\n, indicating that a moderate number of projections is sufficient for a stable transport estimate.\n(b)\nJoint effect of the residual strength\nÎ»\n\\lambda\n(in Eq.\n11\n) and the temperature\nÏ„\n\\tau\nin\ng\n=\nmax\nâ¡\n{\ng\nmin\n,\nexp\nâ¡\n(\nâˆ’\nÏ„\nâ€‹\nD\n)\n}\ng=\\max\\{g_{\\min},\\exp(-\\tau D)\\}\n.\nWe observe a broad plateau around\n(\nÎ»\n,\nÏ„\n)\nâ‰ˆ\n(\n0.4\n,\n1.0\n)\n(\\lambda,\\tau)\\approx(0.4,1.0)\nand an elongated ridge roughly following\nÎ»\nâ€‹\nÏ„\nâ‰ˆ\n0.4\n\\lambda\\tau\\approx 0.4\n, showing that the two hyperparameters jointly control an effective gating strength while still providing some flexibility: increasing\nÎ»\n\\lambda\ncan be partially compensated by decreasing\nÏ„\n\\tau\n, and vice versa, but moving too far from the ridge degrades performance.\nFigure\n10\nsummarizes the sensitivity of\nDiG-Flow\nto its main hyperparameters. Panel\n10\n(a) varies the number of sliced projections\nK\nK\nused to approximate the Wasserstein discrepancy. We observe a smooth, monotone improvement as\nK\nK\nincreases from\n4\n4\nto\n20\n20\n, after which performance quickly saturates around\nK\nâ‰ˆ\n28\nK\\approx 28\nâ€“\n32\n32\n. This matches the theoretical trade-off: more projections reduce Monte Carlo variance in the sliced Wasserstein estimate, but beyond a moderate budget the marginal benefit becomes negligible relative to the dominant transformer cost. In all reported experiments we therefore fix\nK\n=\n32\nK=32\n, which lies in the flat regime and offers a good balance between accuracy and overhead.\nPanel\n10\n(b) studies the joint effect of the residual strength\nÎ»\n\\lambda\n(in Eq.(\n11\n)) and the temperature\nÏ„\n\\tau\nin the gate\ng\n=\nexp\nâ¡\n(\nâˆ’\nÏ„\nâ€‹\nD\n)\ng=\\exp(-\\tau D)\n. The heatmap reveals two important properties. First, there is a broad plateau around\n(\nÎ»\n,\nÏ„\n)\nâ‰ˆ\n(\n0.4\n,\n1.0\n)\n(\\lambda,\\tau)\\approx(0.4,1.0)\n, indicating that\nDiG-Flow\nis not overly sensitive to small mis-tuning and that our default choice lies in a robust operating region rather than at a sharp optimum. Second, the high-performance region forms an elongated ridge approximately following\nÎ»\nâ€‹\nÏ„\nâ‰ˆ\nconst\n\\lambda\\tau\\approx\\text{const}\n, which is consistent with the interpretation that\nÎ»\n\\lambda\nand\nÏ„\n\\tau\njointly control an effective gating strength\nÎ»\nâ€‹\ng\nâ€‹\n(\nÏ„\n)\n\\lambda g(\\tau)\n: increasing\nÎ»\n\\lambda\ncan be partially compensated by reducing\nÏ„\n\\tau\n(which softens the gate), and vice versa. The slight diagonal elongation reflects the fact that the effective step size in feature space scales like\nâ€–\nH\n~\nâˆ’\nH\nâ€–\nF\nâ‰ˆ\nÎ»\nâ€‹\ng\nâ€‹\nâ€–\nâ„›\nâ€‹\n(\nH\n)\nâ€–\nF\nâ‰ˆ\nÎ»\nâ€‹\nmax\nâ¡\n{\ng\nmin\n,\nexp\nâ¡\n(\nâˆ’\nÏ„\nâ€‹\nD\n)\n}\nâ€‹\nâ€–\nâ„›\nâ€‹\n(\nH\n)\nâ€–\nF\n,\n\\|\\tilde{H}-H\\|_{F}\\;\\approx\\;\\lambda\\,g\\,\\|\\mathcal{R}(H)\\|_{F}\\;\\approx\\;\\lambda\\,\\max\\{g_{\\min},\\exp(-\\tau D)\\}\\,\\|\\mathcal{R}(H)\\|_{F},\n(32)\nAt the same time, the ridge is not perfectly flat, so\nÎ»\n\\lambda\nand\nÏ„\n\\tau\nare not fully redundant. In particular, large\nÎ»\n\\lambda\nwith aggressive\nÏ„\n\\tau\ncan lead to over-amplified residuals that violate the small-step assumption in Theorem\n5.1\n, while very small\nÎ»\n\\lambda\nwith extremely soft gates under-utilizes the residual pathway and reduces the advantage over the baseline. The heatmap therefore supports the theoretical picture: there exists a nontrivial band of\n(\nÎ»\n,\nÏ„\n)\n(\\lambda,\\tau)\nvalues where the transport-guided residual update yields consistent gains, and our defaults\n(\nÎ»\n=\n0.4\n,\nÏ„\n=\n1.0\n)\n(\\lambda=0.4,\\tau=1.0)\nsit well inside this stable band rather than being a knife-edge configuration.\n6.3.4\nTransport-Cost Dynamics and Gating Behaviour\nFigure 11\n:\nDynamics of the transport discrepancy\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\nD(\\mu_{H},\\mu_{Z})\nduring training.\nWe plot the average sliced\nW\n2\n2\nW_{2}^{2}\nbetween observation and action embeddings over training\niterations (shaded region indicates variability across batches). The cost decreases steadily in the\nearly phase and then stabilizes in a medium range, instead of collapsing to zero, matching the\nintended role of\nD\nD\nas a discriminative geometric signal rather than a pure minimization target.\nIn addition to task-level success rates, we also monitor the behaviour of the transport discrepancy\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\nD(\\mu_{H},\\mu_{Z})\nduring training. Figure\n11\nreports the evolution of the\naverage sliced\n2\n2\n-Wasserstein cost between the empirical observation and action embedding\ndistributions over training iterations for\nDiG-Flow\non LIBERO.\nThree observations are worth highlighting.\n(i) Training phases.\nAt the beginning of training the transport cost is relatively high, reflecting the fact that\nobservation features\nH\nH\nand action embeddings\nZ\nZ\nlive in poorly aligned regions of the feature\nspace. As optimisation proceeds,\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\nD(\\mu_{H},\\mu_{Z})\ndecreases monotonically and then enters a\nsteady-state regime where it fluctuates around a stable value. This behaviour is consistent with\nthe interpretation of\nD\nD\nas measuring semantic compatibility between observation and action\nrepresentations: the model gradually learns to place both in a geometrically coherent region.\n(ii)\nD\nD\nas a signal, not a loss term.\nImportantly, the curve in Figure\n11\ndoes not keep decreasing towards zero.\nInstead, it converges to a medium range where\nD\nD\nis neither vanishing nor exploding. This is\nexactly the regime in which the gate\ng\n=\nmax\nâ¡\n{\ng\nmin\n,\nexp\nâ¡\n(\nâˆ’\nÏ„\nâ€‹\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\n)\n}\ng\\;=\\;\\max\\bigl\\{g_{\\min},\\exp\\bigl(-\\tau\\,D(\\mu_{H},\\mu_{Z})\\bigr)\\bigr\\}\nretains discriminative power: samples with relatively low discrepancy still receive stronger gates,\nwhile samples with larger discrepancy are down-weighted, but no regime dominates completely.\nDriving\nD\nD\nto (near-)zero would make\ng\nâ‰ˆ\n1\ng\\approx 1\nfor almost all samples and therefore remove\nthe benefit of discrepancy-guided modulation; conversely, extremely large\nD\nD\nwould force\ng\nâ‰ˆ\n0\ng\\approx 0\nand effectively deactivate the residual path.\n(iii) Connection to gated residual theory and hyperparameters.\nThe observed range of\nD\nD\nprovides empirical support for the theoretical assumptions used in\nSection\n5\n. Theorem\n5.1\nrequires that the gated residual update\nH\n~\n=\nH\n+\nÎ»\nâ€‹\ng\nâ€‹\nâ„›\nâ€‹\n(\nH\n)\n\\tilde{H}\\;=\\;H+\\lambda\\,g\\,\\mathcal{R}(H)\nremains in a regime where the first-order improvement term dominates the quadratic penalty\ncontrolled by\nL\nH\nL_{H}\nand\nB\nâ„›\nB_{\\mathcal{R}}\n(Assumption\n8.1\n). The fact that\nD\nD\nstabilizes in a moderate range implies that the effective step size (Eq. (\n32\n)) naturally stays within such a â€œsmall but non-negligibleâ€ band throughout training, matching the\nregime in which Theorem\n5.1\nguarantees expected loss reduction. This also explains the shape of the hyperparameter landscape in\nFigure\n10\n. Since\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\nD(\\mu_{H},\\mu_{Z})\nconcentrates in a relatively narrow\ninterval once training has converged, there exists a family of\n(\nÎ»\n,\nÏ„\n)\n(\\lambda,\\tau)\npairs that produce\nsimilar effective magnitudes of\nÎ»\nâ€‹\ng\n\\lambda g\non typical samples. The broad plateau in\nFigure\n10\n(b) and the smooth one-dimensional curves over\nÎ»\n\\lambda\nand\nÏ„\n\\tau\nare consistent with this picture:\nDiG-Flow\nis robust to moderate changes in either\nparameter, as long as the resulting transport cost keeps the gate in the informative middle range\nillustrated in Figure\n11\n.\n7\nConclusions and Limitations\nWe presented\nDiG-Flow\n, a discrepancy-guided flow matching framework that regularizes VLA representations through a simple geometric signal between observation and action embeddings. By mapping this distributional discrepancy to a scalar gate and applying residual updates in feature space,\nDiG-Flow\nleaves the flow-matching path and objective intact while shaping the representations used by the action head. Our analysis shows that the gated objective admits standard descent guarantees, that suitably small residual updates provably reduce the loss under an alignment condition, and that a fixed-gate refinement scheme forms a contraction.\nWhile\nDiG-Flow\ndemonstrates strong performance in both simulation and real-world experiments, several limitations warrant discussion.\nFirst, our theoretical analysis assumes that features have bounded norms (\ne.g\n.,\nâ€–\nH\nâ€–\nF\nâ‰¤\nR\n\\|H\\|_{F}\\leq R\n). This is reasonable for normalized representations but may require additional scaling or normalization mechanisms when applied to architectures that do not enforce such bounds.\nSecond, the transport distance computation currently relies on batch-level statistics, which can be influenced by outliers or small batch sizes. In principle, this could make the gate slightly sensitive to the composition of a mini-batch. Exploring alternatives such as running statistics, more robust discrepancy measures, or fully instance-wise variants would be an interesting direction for future work.\nFinally,\nDiG-Flow\nuses ground-truth actions during supervised training to compute meaningful observationâ€“action discrepancies. Extending the same principle to self-supervised or reinforcement learning regimes would require alternative alignment signals (\ne.g\n., consistency across rollouts, value-based or advantage-based weighting), which we leave for future research.\nReferences\n[1]\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich.\nRt-1: Robotics transformer for real-world control at scale, 2023.\n[2]\nMoo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Baljekar, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al.\nOpenvla: An open-source vision-language-action model.\narXiv preprint arXiv:2406.09246\n, 2024.\n[3]\nKevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al.\nÏ€\n0\n\\pi_{0}\n: A vision-language-action flow model for general robot control.\narXiv preprint arXiv:2410.24164\n, 2024.\n[4]\nPhysical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al.\nÏ€\n0.5\n\\pi_{0.5}\n: a vision-language-action model with open-world generalization.\narXiv preprint arXiv:2504.16054\n, 2025.\n[5]\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.\nFlow matching for generative modeling.\narXiv preprint arXiv:2210.02747\n, 2023.\n[6]\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4: Enhancing vision-language understanding with advanced large language models.\narXiv preprint arXiv:2304.10592\n, 2023.\n[7]\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al.\nQwen2-vl: Enhancing vision-language modelâ€™s perception of the world at any resolution.\narXiv preprint arXiv:2409.12191\n, 2024.\n[8]\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971\n, 2023.\n[9]\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288\n, 2023.\n[10]\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al.\nQwen technical report.\narXiv preprint arXiv:2309.16609\n, 2023.\n[11]\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\nLearning transferable visual models from natural language supervision.\nIn\nInternational conference on machine learning\n, pages 8748â€“8763. PMLR, 2021.\n[12]\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.\nSigmoid loss for language image pre-training.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n, pages 11975â€“11986, 2023.\n[13]\nYicheng Feng, Yijiang Li, Wanpeng Zhang, Sipeng Zheng, Hao Luo, Zihao Yue, and Zongqing Lu.\nVideoorion: Tokenizing object dynamics in videos.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)\n, pages 20401â€“20412, October 2025.\n[14]\nHao Luo, Zihao Yue, Wanpeng Zhang, Yicheng Feng, Sipeng Zheng, Deheng Ye, and Zongqing Lu.\nOpenMMEgo: Enhancing egocentric understanding for LMMs with open weights and data.\nIn\nThe Thirty-ninth Annual Conference on Neural Information Processing Systems\n, 2025.\n[15]\nWanpeng Zhang, Zilong Xie, Yicheng Feng, Yijiang Li, Xingrun Xing, Sipeng Zheng, and Zongqing Lu.\nFrom pixels to tokens: Byte-pair encoding on quantized visual modalities.\nIn\nThe Thirteenth International Conference on Learning Representations\n, 2025.\n[16]\nWanpeng Zhang, Yicheng Feng, Hao Luo, Yijiang Li, Zihao Yue, Sipeng Zheng, and Zongqing Lu.\nUnified multimodal understanding via byte-pair visual encoding.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)\n, pages 12976â€“12986, October 2025.\n[17]\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.\nFlamingo: a visual language model for few-shot learning.\nAdvances in neural information processing systems\n, 35:23716â€“23736, 2022.\n[18]\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pages 26296â€“26306, 2024.\n[19]\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning.\nAdvances in neural information processing systems\n, 36:34892â€“34916, 2023.\n[20]\nYadong Lu, Chunyuan Li, Haotian Liu, Jianwei Yang, Jianfeng Gao, and Yelong Shen.\nAn empirical study of scaling instruct-tuned large multimodal models.\narXiv preprint arXiv:2309.09958\n, 2023.\n[21]\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich.\nRt-2: Vision-language-action models transfer web knowledge to robotic control, 2023.\n[22]\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.\nPalm-e: An embodied multimodal language model.\nIn\nInternational Conference on Machine Learning\n, pages 8469â€“8488. PMLR, 2023.\n[23]\nOcto Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine.\nOcto: An open-source generalist robot policy, 2024.\n[24]\nChi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, and Minzhao Zhu.\nGr-2: A generative video-language-action model with web-scale knowledge for robot manipulation.\narXiv preprint arXiv:2410.06158\n, 2024.\n[25]\nChilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, et al.\nGr-3 technical report.\narXiv preprint arXiv:2507.15493\n, 2025.\n[26]\nDelin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, and Xuelong Li.\nSpatialvla: Exploring spatial representations for visual-language-action model, 2025.\n[27]\nHuang Huang, Fangchen Liu, Letian Fu, Tingfan Wu, Mustafa Mukadam, Jitendra Malik, Ken Goldberg, and Pieter Abbeel.\nOtter: A vision-language-action model with text-aware visual feature extraction.\narXiv preprint arXiv:2503.03734\n, 2025.\n[28]\nKarl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine.\nFast: Efficient action tokenization for vision-language-action models, 2025.\n[29]\nJohan Bjorck, Fernando CastaÃ±eda, Nikita Cherniaiev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al.\nGr00t n1: An open foundation model for generalist humanoid robots.\narXiv preprint arXiv:2503.14734\n, 2025.\n[30]\nQingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, and Tsung-Yi Lin.\nCot-vla: Visual chain-of-thought reasoning for vision-language-action models, 2025.\n[31]\nHao Luo, Yicheng Feng, Wanpeng Zhang, Sipeng Zheng, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, and Zongqing Lu.\nBeing-h0: vision-language-action pretraining from large-scale human videos.\narXiv preprint arXiv:2507.15597\n, 2025.\n[32]\nFanqi Lin, Ruiqian Nai, Yingdong Hu, Jiacheng You, Junming Zhao, and Yang Gao.\nOnetwovla: A unified vision-language-action model with adaptive reasoning.\narXiv preprint arXiv:2505.11917\n, 2025.\n[33]\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud.\nNeural ordinary differential equations.\nIn\nAdvances in neural information processing systems\n, volume 31, 2018.\n[34]\nWill Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud.\nFfjord: Free-form continuous dynamics for scalable reversible generative models.\narXiv preprint arXiv:1810.01367\n, 2018.\n[35]\nGeorge Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan.\nNormalizing flows for probabilistic modeling and inference.\nJournal of Machine Learning Research\n, 22(57):1â€“64, 2021.\n[36]\nAlexander Tong, Jessie Huang, Guy Wolf, David Van Dijk, and Smita Krishnaswamy.\nConditional flow matching: Simulation-free training of continuous normalizing flows.\narXiv preprint arXiv:2302.00482\n, 2023.\n[37]\nXingchao Liu, Chengyue Gong, and Qiang Liu.\nFlow straight and fast: Learning to generate and transfer data with rectified flow.\narXiv preprint arXiv:2209.03003\n, 2022.\n[38]\nMichael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden.\nStochastic interpolants: A unifying framework for flows and diffusions.\narXiv preprint arXiv:2303.08797\n, 2023.\n[39]\nAram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky TQ Chen.\nMultisample flow matching: Straightening flows with minibatch couplings.\narXiv preprint arXiv:2304.14772\n, 2023.\n[40]\nAlexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio.\nImproving and generalizing flow-based generative models with minibatch optimal transport.\narXiv preprint arXiv:2302.00482\n, 2024.\n[41]\nCÃ©dric Villani.\nOptimal transport: old and new\n, volume 338.\nSpringer, 2009.\n[42]\nGabriel PeyrÃ© and Marco Cuturi.\nComputational optimal transport.\nFoundations and Trends in Machine Learning\n, 11(5-6):355â€“607, 2019.\n[43]\nMarco Cuturi.\nSinkhorn distances: Lightspeed computation of optimal transport.\nAdvances in neural information processing systems\n, 26, 2013.\n[44]\nNicolas Bonneel, Julien Rabin, Gabriel PeyrÃ©, and Hanspeter Pfister.\nSliced and radon wasserstein barycenters of measures.\nIn\nJournal of Mathematical Imaging and Vision\n, volume 51, pages 22â€“45, 2015.\n[45]\nSoheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde.\nGeneralized sliced wasserstein distances.\nAdvances in neural information processing systems\n, 32, 2019.\n[46]\nRobert Geirhos, JÃ¶rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann.\nShortcut learning in deep neural networks.\nNature Machine Intelligence\n, 2(11):665â€“673, 2020.\n[47]\nJosh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel.\nDomain randomization for transferring deep neural networks from simulation to the real world.\nIn\n2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)\n, pages 23â€“30. IEEE, 2017.\n[48]\nEric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Pieter Abbeel, Sergey Levine, Kate Saenko, and Trevor Darrell.\nAdapting deep visuomotor representations with weak pairwise constraints.\nIn\nAlgorithmic Foundations of Robotics XII: Proceedings of the Twelfth Workshop on the Algorithmic Foundations of Robotics\n, pages 688â€“703. Springer, 2020.\n[49]\nWanpeng Zhang, Yilin Li, Boyu Yang, and Zongqing Lu.\nTackling non-stationarity in reinforcement learning via causal-origin representation.\nIn\nProceedings of the 41st International Conference on Machine Learning\n, volume 235, pages 59264â€“59288. PMLR, 2024.\n[50]\nDavid Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville.\nOut-of-distribution generalization via risk extrapolation (rex).\nIn\nInternational conference on machine learning\n, pages 5815â€“5826. PMLR, 2021.\n[51]\nYouguang Xing, Xu Luo, Junlin Xie, Lianli Gao, Hengtao Shen, and Jingkuan Song.\nShortcut learning in generalist robot policies: The role of dataset diversity and fragmentation.\narXiv preprint arXiv:2508.06426\n, 2025.\n[52]\nBo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone.\nLibero: Benchmarking knowledge transfer for lifelong robot learning.\narXiv preprint arXiv:2306.03310\n, 2023.\n[53]\nSoroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu.\nRobocasa: Large-scale simulation of everyday tasks for generalist robots.\narXiv preprint arXiv:2406.02523\n, 2024.\n[54]\nCheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song.\nDiffusion policy: Visuomotor policy learning via action diffusion.\narXiv preprint arXiv:2303.04137\n, 2023.\n[55]\nMoo Jin Kim, Chelsea Finn, and Percy Liang.\nFine-tuning vision-language-action models: Optimizing speed and success.\narXiv preprint arXiv:2502.19645\n, 2025.\n[56]\nFan Feng, Biwei Huang, Kun Zhang, and Sara Magliacane.\nFactored adaptation for non-stationary reinforcement learning.\nAdvances in Neural Information Processing Systems\n, 35:31957â€“31971, 2022.\n\\beginappendix\n8\nFull Proofs and Further Discussions\nThis appendix provides formal assumptions and complete proofs of the theoretical results stated in Section\n5\n. We keep the notation from the main text and make explicit the regularity conditions under which the guarantees hold.\n8.1\nStanding Assumptions\nRecall that for a single training example with observation features\nH\nH\nand time\nt\nt\n, the per-sample flow-matching loss is\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n=\nâ€–\nv\nÎ¸\nâ€‹\n(\nH\n,\nt\n)\nâˆ’\nv\nâ‹†\nâ€‹\n(\nH\n,\nt\n)\nâ€–\n2\n,\n\\ell(\\theta;H,t)\\;=\\;\\big\\|v_{\\theta}(H,t)-v^{\\star}(H,t)\\big\\|^{2},\n(33)\nand the dataset objective is\nâ„’\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nâ€‹\n[\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n]\n.\n\\mathcal{L}(\\theta)\\;=\\;\\mathbb{E}\\big[\\ell(\\theta;H,t)\\big].\n(34)\nThe discrepancy-guided objective used by\nDiG-Flow\nis\nJ\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n]\n,\ng\n=\nÏ•\nâ€‹\n(\nD\nâ€‹\n(\nÎ¼\nH\n,\nÎ¼\nZ\n)\n)\n,\nJ(\\theta)\\;=\\;\\mathbb{E}\\big[g\\,\\ell(\\theta;H,t)\\big],\\quad g=\\phi\\big(D(\\mu_{H},\\mu_{Z})\\big),\n(35)\nwhere\nÎ¼\nH\n\\mu_{H}\nand\nÎ¼\nZ\n\\mu_{Z}\nare the empirical measures defined in the main text, and gradients are stopped through\ng\ng\nwhen differentiating with respect to\nÎ¸\n\\theta\n.\nWe use\nâˆ¥\nâ‹…\nâˆ¥\n\\|\\cdot\\|\nfor the Euclidean norm on vectors and\nâˆ¥\nâ‹…\nâˆ¥\nF\n\\|\\cdot\\|_{F}\nfor the Frobenius norm on matrices or tensor-valued sequences (such as\nH\nâˆˆ\nâ„\nT\nÃ—\nd\nH\\in\\mathbb{R}^{T\\times d}\nor\nZ\nâˆˆ\nâ„\nK\nÃ—\nd\nZ\\in\\mathbb{R}^{K\\times d}\n). The inner product between two tensors of the same shape is written as\nâŸ¨\nâ‹…\n,\nâ‹…\nâŸ©\n\\langle\\cdot,\\cdot\\rangle\n.\nWe now state the regularity assumptions used in the proofs.\n{cthassumption}\nParameter smoothness in\nÎ¸\n\\theta\nThere exists\nL\nÎ¸\n>\n0\nL_{\\theta}>0\nsuch that\nâ„’\nâ€‹\n(\nÎ¸\n)\n\\mathcal{L}(\\theta)\nis\nL\nÎ¸\nL_{\\theta}\n-smooth in\nÎ¸\n\\theta\n,\ni.e\n., for all\nÎ¸\n,\nÎ¸\nâ€²\n\\theta,\\theta^{\\prime}\n,\nâ„’\nâ€‹\n(\nÎ¸\nâ€²\n)\nâ‰¤\nâ„’\nâ€‹\n(\nÎ¸\n)\n+\nâŸ¨\nâˆ‡\nÎ¸\nâ„’\nâ€‹\n(\nÎ¸\n)\n,\nÎ¸\nâ€²\nâˆ’\nÎ¸\nâŸ©\n+\nL\nÎ¸\n2\nâ€‹\nâ€–\nÎ¸\nâ€²\nâˆ’\nÎ¸\nâ€–\n2\n.\n\\mathcal{L}(\\theta^{\\prime})\\;\\leq\\;\\mathcal{L}(\\theta)+\\big\\langle\\nabla_{\\theta}\\mathcal{L}(\\theta),\\theta^{\\prime}-\\theta\\big\\rangle+\\frac{L_{\\theta}}{2}\\,\\|\\theta^{\\prime}-\\theta\\|^{2}.\n(36)\n{cthassumption}\nGate properties\n\nThe weight mapping\nÏ•\n:\nâ„\n+\nâ†’\n[\ng\nmin\n,\n1\n]\n\\phi:\\mathbb{R}_{+}\\to[g_{\\min},1]\nsatisfies:\nâ€¢\nMonotone decreasing:\nif\ns\n1\nâ‰¤\ns\n2\ns_{1}\\leq s_{2}\nthen\nÏ•\nâ€‹\n(\ns\n1\n)\nâ‰¥\nÏ•\nâ€‹\n(\ns\n2\n)\n\\phi(s_{1})\\geq\\phi(s_{2})\n.\nâ€¢\nLipschitz:\nthere exists\nL\nÏ•\nL_{\\phi}\nsuch that\n|\nÏ•\nâ€‹\n(\ns\n)\nâˆ’\nÏ•\nâ€‹\n(\ns\nâ€²\n)\n|\nâ‰¤\nL\nÏ•\nâ€‹\n|\ns\nâˆ’\ns\nâ€²\n|\n|\\phi(s)-\\phi(s^{\\prime})|\\leq L_{\\phi}|s-s^{\\prime}|\nfor all\ns\n,\ns\nâ€²\nâ‰¥\n0\ns,s^{\\prime}\\geq 0\n.\nâ€¢\nBounded away from zero:\ng\n=\nÏ•\nâ€‹\n(\nD\n)\nâˆˆ\n[\ng\nmin\n,\n1\n]\ng=\\phi(D)\\in[g_{\\min},1]\nwith\ng\nmin\n>\n0\ng_{\\min}>0\n.\nIn particular, for all\nÎ¸\n\\theta\nwe have the bracketing inequality in the main text,\ng\nmin\nâ€‹\nâ„’\nâ€‹\n(\nÎ¸\n)\nâ‰¤\nJ\nâ€‹\n(\nÎ¸\n)\nâ‰¤\nâ„’\nâ€‹\n(\nÎ¸\n)\n,\ng_{\\min}\\,\\mathcal{L}(\\theta)\\;\\leq\\;J(\\theta)\\;\\leq\\;\\mathcal{L}(\\theta),\n(37)\nwhich is Eq. (\n14\n).\n{cthassumption}\nFeature smoothness and residual boundedness\n\nFor fixed\n(\nÎ¸\n,\nt\n)\n(\\theta,t)\n, the loss as a function of the observation features\nH\nH\nis\nL\nH\nL_{H}\n-smooth in Frobenius norm: for all\nH\n,\nH\nâ€²\nH,H^{\\prime}\n,\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\nâ€²\n,\nt\n)\nâ‰¤\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n+\nâŸ¨\nâˆ‡\nH\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n,\nH\nâ€²\nâˆ’\nH\nâŸ©\n+\nL\nH\n2\nâ€‹\nâ€–\nH\nâ€²\nâˆ’\nH\nâ€–\nF\n2\n.\n\\ell(\\theta;H^{\\prime},t)\\;\\leq\\;\\ell(\\theta;H,t)+\\big\\langle\\nabla_{H}\\ell(\\theta;H,t),H^{\\prime}-H\\big\\rangle+\\frac{L_{H}}{2}\\,\\|H^{\\prime}-H\\|_{F}^{2}.\n(38)\nMoreover, the residual operator\nâ„›\n\\mathcal{R}\nhas bounded operator norm: there exists\nB\nâ„›\n>\n0\nB_{\\mathcal{R}}>0\nsuch that\nâ€–\nâ„›\nâ€‹\n(\nH\n)\nâ€–\nF\nâ‰¤\nB\nâ„›\nâ€‹\nâ€–\nH\nâ€–\nF\n,\nâˆ€\nH\n.\n\\|\\mathcal{R}(H)\\|_{F}\\;\\leq\\;B_{\\mathcal{R}}\\,\\|H\\|_{F},\\quad\\forall H.\n(39)\n{cthassumption}\nBounded feature second moment\n\nThere exists a constant\nC\nH\n>\n0\nC_{H}>0\nsuch that\nğ”¼\nâ€‹\n[\nâ€–\nH\nâ€–\nF\n2\n]\nâ‰¤\nC\nH\n2\n\\mathbb{E}\\big[\\|H\\|_{F}^{2}\\big]\\;\\leq\\;C_{H}^{2}\n(40)\nover the training distribution.\n{cthassumption}\nRefinement field regularity\n\nFor each fixed gate\ng\ng\n, the refinement error field\nE\nâ€‹\n(\nâ‹…\n;\ng\n)\n:\nâ„\nK\nÃ—\nd\nâ†’\nâ„\nK\nÃ—\nd\nE(\\cdot;g):\\mathbb{R}^{K\\times d}\\to\\mathbb{R}^{K\\times d}\n(41)\nused in the optional\nDiG-Refine\nprocedure is\nL\nE\nL_{E}\n-Lipschitz and\nÎ¼\n\\mu\n-strongly monotone,\ni.e\n. for all\nZ\n1\n,\nZ\n2\nZ_{1},Z_{2}\n,\nâ€–\nE\nâ€‹\n(\nZ\n1\n;\ng\n)\nâˆ’\nE\nâ€‹\n(\nZ\n2\n;\ng\n)\nâ€–\nF\n\\displaystyle\\big\\|E(Z_{1};g)-E(Z_{2};g)\\big\\|_{F}\nâ‰¤\nL\nE\nâ€‹\nâ€–\nZ\n1\nâˆ’\nZ\n2\nâ€–\nF\n,\n\\displaystyle\\leq L_{E}\\,\\|Z_{1}-Z_{2}\\|_{F},\n(42)\nâŸ¨\nZ\n1\nâˆ’\nZ\n2\n,\nE\nâ€‹\n(\nZ\n1\n;\ng\n)\nâˆ’\nE\nâ€‹\n(\nZ\n2\n;\ng\n)\nâŸ©\n\\displaystyle\\big\\langle Z_{1}-Z_{2},\\,E(Z_{1};g)-E(Z_{2};g)\\big\\rangle\nâ‰¥\nÎ¼\nâ€‹\nâ€–\nZ\n1\nâˆ’\nZ\n2\nâ€–\nF\n2\n,\nÎ¼\n>\n0\n.\n\\displaystyle\\geq\\mu\\,\\|Z_{1}-Z_{2}\\|_{F}^{2},\\quad\\mu>0.\n(43)\nAssumptions\n8.1\nâ€“\n8.1\nare standard in analyses of smooth neural-network objectives:\nthey hold when\nv\nÎ¸\nv_{\\theta}\nis implemented by a network with Lipschitz activations and bounded weights, and\nâ„“\n\\ell\nis the squared error\nbetween\nv\nÎ¸\nv_{\\theta}\nand a target field.\nAssumption\n8.1\nis satisfied by the clipped exponential mapping\nÏ•\nâ€‹\n(\nD\n)\n=\nmax\nâ¡\n{\ng\nmin\n,\nexp\nâ¡\n(\nâˆ’\nÏ„\nâ€‹\nD\n)\n}\n\\phi(D)=\\max\\{g_{\\min},\\exp(-\\tau D)\\}\nused in our implementation, which is monotone and Lipschitz on\nâ„\n+\n\\mathbb{R}_{+}\n.\nAssumption\n8.1\ncaptures the usual regularity of gradient-like refinement fields appearing in fixed-point analyses.\nIn addition to these regularity conditions, the residual-update result in Theorem\n5.1\nalso uses a gated descent\ncondition (Eq. (\n15\n)) on the learned residual field;\nalthough we state this condition directly in the theorem rather than as a separate standing assumption,\nit can be interpreted as a description of the average behaviour of the jointly trained residual operator, as discussed below.\nRemark\n(On the gated descent condition)\n.\nThe gated descent condition in Eq. (\n15\n), used in Theorem\n5.1\n,\nis not intended as an arbitrary extra constraint, but as a structural description of the learned residual field.\nWhen optimizing a composite objective of the form\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n~\n,\nt\n)\n]\n\\mathbb{E}[g\\,\\ell(\\theta;\\tilde{H},t)]\n(as in our implementation), the parameters of\nâ„›\n\\mathcal{R}\nreceive gradients that encourage them to produce directions aligned with loss reduction.\nIf\nâ„›\nâ€‹\n(\nH\n)\n\\mathcal{R}(H)\nwere orthogonal to\nâˆ‡\nH\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n\\nabla_{H}\\ell(\\theta;H,t)\non average, it would not consistently decrease the loss\nand would be effectively regularized away.\nThus, under successful training, it is natural to expect\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâŸ¨\nâˆ‡\nH\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n,\nâ„›\nâ€‹\n(\nH\n)\nâŸ©\n]\n<\n0\n\\mathbb{E}[g\\,\\langle\\nabla_{H}\\ell(\\theta;H,t),\\,\\mathcal{R}(H)\\rangle]<0\n,\nso Eq. (\n15\n) can be viewed as codifying the typical behaviour of a jointly trained residual operator\nrather than imposing a qualitatively new regularity assumption.\n8.2\nSmoothness of the gated objective\nWe first record a simple lemma that links the smoothness of\nâ„’\n\\mathcal{L}\nto that of\nJ\nJ\n, clarifying why the gated objective remains well-behaved.\n{cthlemma}\nSmoothness of the gated objective\n\nUnder Assumptions\n8.1\nand\n8.1\n, the gated objective\nJ\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n]\nJ(\\theta)\\;=\\;\\mathbb{E}\\big[g\\,\\ell(\\theta;H,t)\\big]\n(44)\nis\nL\nJ\nL_{J}\n-smooth in\nÎ¸\n\\theta\nwith\nL\nJ\nâ‰¤\nL\nÎ¸\nL_{J}\\leq L_{\\theta}\n.\nProof.\nFor each sample, treating\ng\ng\nas constant w.r.t.\nÎ¸\n\\theta\n(gradients are stopped through\ng\ng\n), we have\nâˆ‡\nÎ¸\n[\ng\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n]\n=\ng\nâ€‹\nâˆ‡\nÎ¸\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n.\n\\nabla_{\\theta}\\big[g\\,\\ell(\\theta;H,t)\\big]\\;=\\;g\\,\\nabla_{\\theta}\\ell(\\theta;H,t).\n(45)\nBy\nL\nÎ¸\nL_{\\theta}\n-smoothness of\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n\\ell(\\theta;H,t)\nin\nÎ¸\n\\theta\n,\nâ€–\nâˆ‡\nÎ¸\nâ„“\nâ€‹\n(\nÎ¸\nâ€²\n;\nH\n,\nt\n)\nâˆ’\nâˆ‡\nÎ¸\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\nâ€–\nâ‰¤\nL\nÎ¸\nâ€‹\nâ€–\nÎ¸\nâ€²\nâˆ’\nÎ¸\nâ€–\n.\n\\|\\nabla_{\\theta}\\ell(\\theta^{\\prime};H,t)-\\nabla_{\\theta}\\ell(\\theta;H,t)\\|\\;\\leq\\;L_{\\theta}\\,\\|\\theta^{\\prime}-\\theta\\|.\n(46)\nTherefore\nâ€–\nâˆ‡\nÎ¸\n[\ng\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\nâ€²\n;\nH\n,\nt\n)\n]\nâˆ’\nâˆ‡\nÎ¸\n[\ng\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n]\nâ€–\n\\displaystyle\\big\\|\\nabla_{\\theta}\\big[g\\,\\ell(\\theta^{\\prime};H,t)\\big]-\\nabla_{\\theta}\\big[g\\,\\ell(\\theta;H,t)\\big]\\big\\|\n=\ng\nâ€‹\nâ€–\nâˆ‡\nÎ¸\nâ„“\nâ€‹\n(\nÎ¸\nâ€²\n;\nH\n,\nt\n)\nâˆ’\nâˆ‡\nÎ¸\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\nâ€–\n\\displaystyle=g\\,\\|\\nabla_{\\theta}\\ell(\\theta^{\\prime};H,t)-\\nabla_{\\theta}\\ell(\\theta;H,t)\\|\n(47)\nâ‰¤\nL\nÎ¸\nâ€‹\nâ€–\nÎ¸\nâ€²\nâˆ’\nÎ¸\nâ€–\n,\n\\displaystyle\\leq L_{\\theta}\\,\\|\\theta^{\\prime}-\\theta\\|,\n(48)\nbecause\ng\nâ‰¤\n1\ng\\leq 1\n.\nThus each term\ng\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\ng\\,\\ell(\\theta;H,t)\nis\nL\nÎ¸\nL_{\\theta}\n-smooth in\nÎ¸\n\\theta\n, and\nJ\nâ€‹\n(\nÎ¸\n)\nJ(\\theta)\n, being their expectation, is\nL\nJ\nL_{J}\n-smooth with\nL\nJ\nâ‰¤\nL\nÎ¸\nL_{J}\\leq L_{\\theta}\n.\nâˆ\n8.3\nProof of Theorem\n5.1\n(Gated descent)\nWe now prove the gated-descent guarantee stated in the main text.\nProof of Theorem\n5.1\n.\nBy Lemma\n8.2\n,\nJ\nJ\nis\nL\nJ\nL_{J}\n-smooth in\nÎ¸\n\\theta\n. For any\nL\nJ\nL_{J}\n-smooth function\nF\nF\n, the standard descent inequality holds: for a gradient descent step\nÎ¸\n+\n=\nÎ¸\nâˆ’\nÎ±\nâ€‹\nâˆ‡\nF\nâ€‹\n(\nÎ¸\n)\n\\theta^{+}=\\theta-\\alpha\\nabla F(\\theta)\nwith\n0\n<\nÎ±\n<\n2\n/\nL\nJ\n0<\\alpha<2/L_{J}\n,\nF\nâ€‹\n(\nÎ¸\n+\n)\nâ‰¤\nF\nâ€‹\n(\nÎ¸\n)\nâˆ’\nÎ±\nâ€‹\n(\n1\nâˆ’\nÎ±\nâ€‹\nL\nJ\n2\n)\nâ€‹\nâ€–\nâˆ‡\nF\nâ€‹\n(\nÎ¸\n)\nâ€–\n2\n.\nF(\\theta^{+})\\;\\leq\\;F(\\theta)-\\alpha\\Bigl(1-\\frac{\\alpha L_{J}}{2}\\Bigr)\\,\\|\\nabla F(\\theta)\\|^{2}.\n(49)\nApplying this to\nF\n=\nJ\nF=J\ngives\nJ\nâ€‹\n(\nÎ¸\n+\n)\nâ‰¤\nJ\nâ€‹\n(\nÎ¸\n)\nâˆ’\nÎ±\nâ€‹\n(\n1\nâˆ’\nÎ±\nâ€‹\nL\nJ\n2\n)\nâ€‹\nâ€–\nâˆ‡\nÎ¸\nJ\nâ€‹\n(\nÎ¸\n)\nâ€–\n2\n,\nJ(\\theta^{+})\\;\\leq\\;J(\\theta)-\\alpha\\Bigl(1-\\frac{\\alpha L_{J}}{2}\\Bigr)\\,\\|\\nabla_{\\theta}J(\\theta)\\|^{2},\n(50)\nwhich matches Eq. (\n13\n) in the main text with\nc\n1\n=\nÎ±\nâ€‹\n(\n1\nâˆ’\nÎ±\nâ€‹\nL\nJ\n2\n)\n>\n0\n.\nc_{1}\\;=\\;\\alpha\\Bigl(1-\\frac{\\alpha L_{J}}{2}\\Bigr)>0.\n(51)\nThe bracketing relation\ng\nmin\nâ€‹\nâ„’\nâ€‹\n(\nÎ¸\n)\nâ‰¤\nJ\nâ€‹\n(\nÎ¸\n)\nâ‰¤\nâ„’\nâ€‹\n(\nÎ¸\n)\ng_{\\min}\\mathcal{L}(\\theta)\\leq J(\\theta)\\leq\\mathcal{L}(\\theta)\n(Eq. (\n14\n)) follows directly from Assumption\n8.1\n: since\ng\nâˆˆ\n[\ng\nmin\n,\n1\n]\ng\\in[g_{\\min},1]\nalmost surely,\ng\nmin\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\nâ‰¤\ng\nâ€‹\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\nâ‰¤\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n,\ng_{\\min}\\,\\ell(\\theta;H,t)\\;\\leq\\;g\\,\\ell(\\theta;H,t)\\;\\leq\\;\\ell(\\theta;H,t),\n(52)\nand taking expectations over the data distribution gives\ng\nmin\nâ€‹\nâ„’\nâ€‹\n(\nÎ¸\n)\nâ‰¤\nJ\nâ€‹\n(\nÎ¸\n)\nâ‰¤\nâ„’\nâ€‹\n(\nÎ¸\n)\n.\ng_{\\min}\\,\\mathcal{L}(\\theta)\\;\\leq\\;J(\\theta)\\;\\leq\\;\\mathcal{L}(\\theta).\n(53)\nThis proves Theorem\n5.1\n.\nâˆ\n8.4\nProof of Theorem\n5.1\n(Residual update improvement)\nWe next analyze the effect of the gated residual update\nH\n~\n=\nH\n+\nÎ»\nâ€‹\ng\nâ€‹\nâ„›\nâ€‹\n(\nH\n)\n\\tilde{H}=H+\\lambda\\,g\\,\\mathcal{R}(H)\n(54)\non the expected loss, under the alignment condition in Eq. (\n15\n) of the main text.\nProof of Theorem\n5.1\n.\nFix\n(\nÎ¸\n,\nt\n)\n(\\theta,t)\nand consider\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n\\ell(\\theta;H,t)\nas a function of\nH\nH\n. By Assumption\n8.1\n, for any perturbation\nÎ”\nâ€‹\nH\n\\Delta H\nwe have\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n+\nÎ”\nâ€‹\nH\n,\nt\n)\nâ‰¤\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n+\nâŸ¨\nâˆ‡\nH\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n,\nÎ”\nâ€‹\nH\nâŸ©\n+\nL\nH\n2\nâ€‹\nâ€–\nÎ”\nâ€‹\nH\nâ€–\nF\n2\n.\n\\ell(\\theta;H+\\Delta H,t)\\;\\leq\\;\\ell(\\theta;H,t)+\\big\\langle\\nabla_{H}\\ell(\\theta;H,t),\\Delta H\\big\\rangle+\\frac{L_{H}}{2}\\,\\|\\Delta H\\|_{F}^{2}.\n(55)\nWe apply this with the gated residual update\nÎ”\nâ€‹\nH\n=\nÎ»\nâ€‹\ng\nâ€‹\nâ„›\nâ€‹\n(\nH\n)\n\\Delta H=\\lambda g\\,\\mathcal{R}(H)\n, so that\nH\n~\n=\nH\n+\nÎ”\nâ€‹\nH\n\\tilde{H}=H+\\Delta H\n. Substituting gives\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n~\n,\nt\n)\n\\displaystyle\\ell(\\theta;\\tilde{H},t)\nâ‰¤\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n+\nÎ»\nâ€‹\ng\nâ€‹\nâŸ¨\nâˆ‡\nH\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n,\nâ„›\nâ€‹\n(\nH\n)\nâŸ©\n+\nL\nH\n2\nâ€‹\nâ€–\nÎ»\nâ€‹\ng\nâ€‹\nâ„›\nâ€‹\n(\nH\n)\nâ€–\nF\n2\n\\displaystyle\\leq\\ell(\\theta;H,t)+\\lambda g\\,\\big\\langle\\nabla_{H}\\ell(\\theta;H,t),\\mathcal{R}(H)\\big\\rangle+\\frac{L_{H}}{2}\\,\\|\\lambda g\\,\\mathcal{R}(H)\\|_{F}^{2}\nâ‰¤\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n+\nÎ»\nâ€‹\ng\nâ€‹\nâŸ¨\nâˆ‡\nH\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n,\nâ„›\nâ€‹\n(\nH\n)\nâŸ©\n+\nL\nH\n2\nâ€‹\nÎ»\n2\nâ€‹\ng\n2\nâ€‹\nB\nâ„›\n2\nâ€‹\nâ€–\nH\nâ€–\nF\n2\n,\n\\displaystyle\\leq\\ell(\\theta;H,t)+\\lambda g\\,\\big\\langle\\nabla_{H}\\ell(\\theta;H,t),\\mathcal{R}(H)\\big\\rangle+\\frac{L_{H}}{2}\\,\\lambda^{2}g^{2}B_{\\mathcal{R}}^{2}\\,\\|H\\|_{F}^{2},\nwhere we used\nâ€–\nâ„›\nâ€‹\n(\nH\n)\nâ€–\nF\nâ‰¤\nB\nâ„›\nâ€‹\nâ€–\nH\nâ€–\nF\n\\|\\mathcal{R}(H)\\|_{F}\\leq B_{\\mathcal{R}}\\|H\\|_{F}\n.\nTaking expectations over the training distribution and using the gated descent condition (Eq. (\n15\n)),\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâŸ¨\nâˆ‡\nH\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n,\nâ„›\nâ€‹\n(\nH\n)\nâŸ©\n]\nâ‰¤\nâˆ’\nÎ±\n0\n,\n\\mathbb{E}\\big[g\\,\\langle\\nabla_{H}\\ell(\\theta;H,t),\\mathcal{R}(H)\\rangle\\big]\\;\\leq\\;-\\alpha_{0},\n(56)\nwe obtain\nğ”¼\nâ€‹\n[\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n~\n,\nt\n)\n]\n\\displaystyle\\mathbb{E}\\big[\\ell(\\theta;\\tilde{H},t)\\big]\nâ‰¤\nğ”¼\nâ€‹\n[\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n]\n+\nÎ»\nâ€‹\nğ”¼\nâ€‹\n[\ng\nâ€‹\nâŸ¨\nâˆ‡\nH\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n,\nâ„›\nâ€‹\n(\nH\n)\nâŸ©\n]\n+\nL\nH\nâ€‹\nB\nâ„›\n2\n2\nâ€‹\nÎ»\n2\nâ€‹\nğ”¼\nâ€‹\n[\ng\n2\nâ€‹\nâ€–\nH\nâ€–\nF\n2\n]\n\\displaystyle\\leq\\mathbb{E}\\big[\\ell(\\theta;H,t)\\big]+\\lambda\\,\\mathbb{E}\\big[g\\,\\langle\\nabla_{H}\\ell(\\theta;H,t),\\mathcal{R}(H)\\rangle\\big]+\\frac{L_{H}B_{\\mathcal{R}}^{2}}{2}\\,\\lambda^{2}\\mathbb{E}\\big[g^{2}\\|H\\|_{F}^{2}\\big]\nâ‰¤\nğ”¼\nâ€‹\n[\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n]\nâˆ’\nÎ±\n0\nâ€‹\nÎ»\n+\nL\nH\nâ€‹\nB\nâ„›\n2\n2\nâ€‹\nÎ»\n2\nâ€‹\nğ”¼\nâ€‹\n[\ng\n2\nâ€‹\nâ€–\nH\nâ€–\nF\n2\n]\n.\n\\displaystyle\\leq\\mathbb{E}\\big[\\ell(\\theta;H,t)\\big]-\\alpha_{0}\\lambda+\\frac{L_{H}B_{\\mathcal{R}}^{2}}{2}\\,\\lambda^{2}\\mathbb{E}\\big[g^{2}\\|H\\|_{F}^{2}\\big].\nUsing\ng\nâ‰¤\n1\ng\\leq 1\nand Assumption\n8.1\n, we may further bound\nğ”¼\nâ€‹\n[\ng\n2\nâ€‹\nâ€–\nH\nâ€–\nF\n2\n]\nâ‰¤\nğ”¼\nâ€‹\n[\nâ€–\nH\nâ€–\nF\n2\n]\nâ‰¤\nC\nH\n2\n,\n\\mathbb{E}\\big[g^{2}\\|H\\|_{F}^{2}\\big]\\;\\leq\\;\\mathbb{E}\\big[\\|H\\|_{F}^{2}\\big]\\;\\leq\\;C_{H}^{2},\n(57)\nso\nğ”¼\nâ€‹\n[\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n~\n,\nt\n)\n]\nâ‰¤\nğ”¼\nâ€‹\n[\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n]\nâˆ’\nÎ±\n0\nâ€‹\nÎ»\n+\nL\nH\nâ€‹\nB\nâ„›\n2\nâ€‹\nC\nH\n2\n2\nâ€‹\nÎ»\n2\n.\n\\mathbb{E}\\big[\\ell(\\theta;\\tilde{H},t)\\big]\\;\\leq\\;\\mathbb{E}\\big[\\ell(\\theta;H,t)\\big]-\\alpha_{0}\\lambda+\\frac{L_{H}B_{\\mathcal{R}}^{2}C_{H}^{2}}{2}\\,\\lambda^{2}.\n(58)\nWe would like the linear improvement\nâˆ’\nÎ±\n0\nâ€‹\nÎ»\n-\\alpha_{0}\\lambda\nto dominate the quadratic term. A sufficient condition is\nÎ±\n0\nâ€‹\nÎ»\nâ‰¥\nL\nH\nâ€‹\nB\nâ„›\n2\nâ€‹\nC\nH\n2\n2\nâ€‹\nÎ»\n2\nâŸº\n0\n<\nÎ»\nâ‰¤\nÎ»\n~\nmax\n:=\n2\nâ€‹\nÎ±\n0\nL\nH\nâ€‹\nB\nâ„›\n2\nâ€‹\nC\nH\n2\n.\n\\alpha_{0}\\lambda\\;\\geq\\;\\frac{L_{H}B_{\\mathcal{R}}^{2}C_{H}^{2}}{2}\\,\\lambda^{2}\\quad\\Longleftrightarrow\\quad 0<\\lambda\\leq\\tilde{\\lambda}_{\\max}:=\\frac{2\\alpha_{0}}{L_{H}B_{\\mathcal{R}}^{2}C_{H}^{2}}.\n(59)\nFor any\n0\n<\nÎ»\nâ‰¤\nÎ»\n~\nmax\n0<\\lambda\\leq\\tilde{\\lambda}_{\\max}\nwe then have\nğ”¼\nâ€‹\n[\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n~\n,\nt\n)\n]\nâ‰¤\nğ”¼\nâ€‹\n[\nâ„“\nâ€‹\n(\nÎ¸\n;\nH\n,\nt\n)\n]\nâˆ’\nÎ±\n0\n2\nâ€‹\nÎ»\n,\n\\mathbb{E}\\big[\\ell(\\theta;\\tilde{H},t)\\big]\\;\\leq\\;\\mathbb{E}\\big[\\ell(\\theta;H,t)\\big]-\\frac{\\alpha_{0}}{2}\\,\\lambda,\n(60)\nwhich is of the form stated in Theorem\n5.1\nwith\nÎ²\n=\nÎ±\n0\n/\n2\n\\beta=\\alpha_{0}/2\n. This shows that there exists a positive threshold (for example\nÎ»\n~\nmax\n\\tilde{\\lambda}_{\\max}\n) such that the gated residual update strictly decreases the expected loss for all\n0\n<\nÎ»\nâ‰¤\nÎ»\n~\nmax\n0<\\lambda\\leq\\tilde{\\lambda}_{\\max}\n.\nâˆ\nRemark\n(On the constant\nÎ»\nmax\n\\lambda_{\\max}\n)\n.\nThe proof above makes explicit one convenient sufficient choice\nÎ»\n~\nmax\n=\n2\nâ€‹\nÎ±\n0\nL\nH\nâ€‹\nB\nâ„›\n2\nâ€‹\nC\nH\n2\n,\n\\tilde{\\lambda}_{\\max}=\\frac{2\\alpha_{0}}{L_{H}B_{\\mathcal{R}}^{2}C_{H}^{2}},\n(61)\nwhich depends on the data-dependent second-moment constant\nC\nH\nC_{H}\n. In the main text, Eq. (\n16\n) only requires the existence of a nontrivial interval\n(\n0\n,\nÎ»\nmax\n]\n(0,\\lambda_{\\max}]\nfor which the residual update improves the loss; the specific closed-form expression reported there can be viewed as an equivalent parameterization after absorbing bounded data constants (such as\nC\nH\n2\nC_{H}^{2}\n) into\nÎ±\n0\n\\alpha_{0}\nand using the fact that\ng\nâˆˆ\n[\ng\nmin\n,\n1\n]\ng\\in[g_{\\min},1]\n. Importantly, none of our qualitative conclusions depend on the exact numerical value of\nÎ»\nmax\n\\lambda_{\\max}\n: what matters is that there exists a strictly positive range of\nÎ»\n\\lambda\nwhere the discrepancy-guided residual update provably reduces the objective.\n8.5\nProof of Theorem\n5.1\n(Fixed-gate refinement convergence)\nWe now prove the contraction result for the fixed-gate refinement scheme.\nProof of Theorem\n5.1\n.\nConsider the refinement map\nT\nâ€‹\n(\nZ\n)\n=\nZ\nâˆ’\nÎ±\nâ€‹\nE\nâ€‹\n(\nZ\n;\ng\n)\n,\nT(Z)\\;=\\;Z-\\alpha\\,E(Z;g),\n(62)\nwhere the gate\ng\ng\nis fixed and\nE\nâ€‹\n(\nâ‹…\n;\ng\n)\nE(\\cdot;g)\nsatisfies Assumption\n8.1\n. For any\nZ\n1\n,\nZ\n2\nZ_{1},Z_{2}\nwe have\nâ€–\nT\nâ€‹\n(\nZ\n1\n)\nâˆ’\nT\nâ€‹\n(\nZ\n2\n)\nâ€–\nF\n2\n\\displaystyle\\big\\|T(Z_{1})-T(Z_{2})\\big\\|_{F}^{2}\n=\nâ€–\n(\nZ\n1\nâˆ’\nZ\n2\n)\nâˆ’\nÎ±\nâ€‹\n(\nE\nâ€‹\n(\nZ\n1\n;\ng\n)\nâˆ’\nE\nâ€‹\n(\nZ\n2\n;\ng\n)\n)\nâ€–\nF\n2\n\\displaystyle=\\big\\|(Z_{1}-Z_{2})-\\alpha\\big(E(Z_{1};g)-E(Z_{2};g)\\big)\\big\\|_{F}^{2}\n=\nâ€–\nZ\n1\nâˆ’\nZ\n2\nâ€–\nF\n2\nâˆ’\n2\nâ€‹\nÎ±\nâ€‹\nâŸ¨\nZ\n1\nâˆ’\nZ\n2\n,\nE\nâ€‹\n(\nZ\n1\n;\ng\n)\nâˆ’\nE\nâ€‹\n(\nZ\n2\n;\ng\n)\nâŸ©\n+\nÎ±\n2\nâ€‹\nâ€–\nE\nâ€‹\n(\nZ\n1\n;\ng\n)\nâˆ’\nE\nâ€‹\n(\nZ\n2\n;\ng\n)\nâ€–\nF\n2\n\\displaystyle=\\|Z_{1}-Z_{2}\\|_{F}^{2}-2\\alpha\\,\\big\\langle Z_{1}-Z_{2},E(Z_{1};g)-E(Z_{2};g)\\big\\rangle+\\alpha^{2}\\big\\|E(Z_{1};g)-E(Z_{2};g)\\big\\|_{F}^{2}\nâ‰¤\nâ€–\nZ\n1\nâˆ’\nZ\n2\nâ€–\nF\n2\nâˆ’\n2\nâ€‹\nÎ±\nâ€‹\nÎ¼\nâ€‹\nâ€–\nZ\n1\nâˆ’\nZ\n2\nâ€–\nF\n2\n+\nÎ±\n2\nâ€‹\nL\nE\n2\nâ€‹\nâ€–\nZ\n1\nâˆ’\nZ\n2\nâ€–\nF\n2\n\\displaystyle\\leq\\|Z_{1}-Z_{2}\\|_{F}^{2}-2\\alpha\\mu\\,\\|Z_{1}-Z_{2}\\|_{F}^{2}+\\alpha^{2}L_{E}^{2}\\,\\|Z_{1}-Z_{2}\\|_{F}^{2}\n=\n(\n1\nâˆ’\n2\nâ€‹\nÎ±\nâ€‹\nÎ¼\n+\nÎ±\n2\nâ€‹\nL\nE\n2\n)\nâ€‹\nâ€–\nZ\n1\nâˆ’\nZ\n2\nâ€–\nF\n2\n,\n\\displaystyle=\\bigl(1-2\\alpha\\mu+\\alpha^{2}L_{E}^{2}\\bigr)\\,\\|Z_{1}-Z_{2}\\|_{F}^{2},\nwhere we used strong monotonicity and Lipschitz continuity of\nE\nâ€‹\n(\nâ‹…\n;\ng\n)\nE(\\cdot;g)\n. Define\nÏ\n2\n:=\n1\nâˆ’\n2\nâ€‹\nÎ±\nâ€‹\nÎ¼\n+\nÎ±\n2\nâ€‹\nL\nE\n2\n.\n\\rho^{2}:=1-2\\alpha\\mu+\\alpha^{2}L_{E}^{2}.\n(63)\nIf\n0\n<\nÎ±\n<\n2\nâ€‹\nÎ¼\n/\nL\nE\n2\n0<\\alpha<2\\mu/L_{E}^{2}\n, then\nÏ\n2\n<\n1\n\\rho^{2}<1\nand hence\nÏ\nâˆˆ\n(\n0\n,\n1\n)\n\\rho\\in(0,1)\n, and we obtain\nâ€–\nT\nâ€‹\n(\nZ\n1\n)\nâˆ’\nT\nâ€‹\n(\nZ\n2\n)\nâ€–\nF\nâ‰¤\nÏ\nâ€‹\nâ€–\nZ\n1\nâˆ’\nZ\n2\nâ€–\nF\n.\n\\big\\|T(Z_{1})-T(Z_{2})\\big\\|_{F}\\;\\leq\\;\\rho\\,\\|Z_{1}-Z_{2}\\|_{F}.\n(64)\nThus\nT\nT\nis a contraction mapping with rate\nÏ\n\\rho\n.\nBy Banachâ€™s fixed-point theorem,\nT\nT\nadmits a unique fixed point\nZ\nâ‹†\nZ^{\\star}\nand, for any initialization\nZ\n(\n0\n)\nZ^{(0)}\n,\nâ€–\nZ\n(\nk\n)\nâˆ’\nZ\nâ‹†\nâ€–\nF\n=\nâ€–\nT\nk\nâ€‹\n(\nZ\n(\n0\n)\n)\nâˆ’\nZ\nâ‹†\nâ€–\nF\nâ‰¤\nÏ\nk\nâ€‹\nâ€–\nZ\n(\n0\n)\nâˆ’\nZ\nâ‹†\nâ€–\nF\n.\n\\big\\|Z^{(k)}-Z^{\\star}\\big\\|_{F}\\;=\\;\\big\\|T^{k}(Z^{(0)})-Z^{\\star}\\big\\|_{F}\\;\\leq\\;\\rho^{k}\\,\\big\\|Z^{(0)}-Z^{\\star}\\big\\|_{F}.\n(65)\nThis is exactly the convergence guarantee stated in Theorem\n5.1\n.\nâˆ",
    "preview_text": "Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.\n\n\\cthnewtheorem\ntheoremTheorem\n\\cthnewtheorem\nassumptionAssumption\n\\cthnewtheorem\nlemmaLemma\n\\webpage\nhttps://beingbeyond.github.io/DiG-Flow\nDiG-Flow\n: Discrepancy-Guided Flow Matching for Robust VLA Models\nWanpeng Zhang\n1,2\nYe Wang\n2,3\nHao Luo\n1,2\nHaoqi Yuan\n1,2\nYicheng Feng\n1,2\nSipeng Zheng\n2\nQin Jin\n3\nZongqing Lu\n1,2,â€ \n1\nPeking University\n2\nBeingBeyond\n3\nRenmin University of China\nAbstract\nVision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tas",
    "is_relevant": null,
    "relevance_score": 0.0,
    "extracted_keywords": [],
    "one_line_summary": "",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T14:21:15Z",
    "created_at": "2026-01-09T17:23:41.811499",
    "updated_at": "2026-01-09T17:23:41.811508",
    "flag": true
}