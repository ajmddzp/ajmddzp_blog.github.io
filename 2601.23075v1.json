{
    "id": "2601.23075v1",
    "title": "RN-D: Discretized Categorical Actors with Regularized Networks for On-Policy Reinforcement Learning",
    "authors": [
        "Yuexin Bian",
        "Jie Feng",
        "Tao Wang",
        "Yijiang Li",
        "Sicun Gao",
        "Yuanyuan Shi"
    ],
    "abstract": "åœ¨è¿ç»­æ§åˆ¶é¢†åŸŸï¼ŒåŸºäºç­–ç•¥çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†æ ‡å‡†å®ç°é€šå¸¸ä¾èµ–é«˜æ–¯ç­–ç•¥æ‰§è¡Œå™¨å’Œç›¸å¯¹æµ…å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºç­–ç•¥ç½‘ç»œï¼Œå½“æ¢¯åº¦å™ªå£°è¾ƒå¤§ä¸”ç­–ç•¥æ›´æ–°å¿…é¡»ä¿å®ˆæ—¶ï¼Œè¿™ç§é…ç½®å¾€å¾€å¯¼è‡´è„†å¼±çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚æœ¬æ–‡é‡æ–°å®¡è§†ç­–ç•¥è¡¨ç¤ºæ–¹æ³•ï¼Œå°†å…¶ä½œä¸ºåŸºäºç­–ç•¥ä¼˜åŒ–çš„é¦–è¦è®¾è®¡é€‰æ‹©ã€‚æˆ‘ä»¬ç ”ç©¶äº†ç¦»æ•£åŒ–åˆ†ç±»æ‰§è¡Œå™¨ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç¦»æ•£åŒºé—´ä¸Šçš„åˆ†å¸ƒæ¥è¡¨ç¤ºæ¯ä¸ªåŠ¨ä½œç»´åº¦ï¼Œä»è€Œäº§ç”Ÿç±»ä¼¼äºäº¤å‰ç†µæŸå¤±çš„ç­–ç•¥ç›®æ ‡å‡½æ•°ã€‚å€Ÿé‰´ç›‘ç£å­¦ä¹ çš„æ¶æ„è¿›å±•ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºæ­£åˆ™åŒ–æ‰§è¡Œå™¨ç½‘ç»œï¼ŒåŒæ—¶ä¿æŒè¯„ä»·å™¨è®¾è®¡ä¸å˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…ç”¨æˆ‘ä»¬æå‡ºçš„ç¦»æ•£åŒ–æ­£åˆ™åŒ–æ‰§è¡Œå™¨æ›¿ä»£æ ‡å‡†æ‰§è¡Œå™¨ç½‘ç»œï¼Œå³å¯åœ¨ä¸åŒè¿ç»­æ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­å–å¾—ç¨³å®šæ€§èƒ½æå‡ï¼Œå¹¶è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚",
    "url": "https://arxiv.org/abs/2601.23075v1",
    "html_url": "https://arxiv.org/html/2601.23075v1",
    "html_content": "RN-D: Discretized Categorical Actors with Regularized Networks\nfor On-Policy Reinforcement Learning\nYuexin Bian\nâˆ—\nJie Feng\nâˆ—\nTao Wang\nYijiang Li\nSicun Gao\nYuanyuan Shi\nAbstract\nOn-policy deep reinforcement learning remains a dominant paradigm for continuous control, yet standard implementations rely on Gaussian actors and relatively shallow MLP policies, often leading to brittle optimization when gradients are noisy and policy updates must be conservative. In this paper, we revisit policy representation as a first-class design choice for on-policy optimization. We study discretized categorical actors that represent each action dimension with a distribution over bins, yielding a policy objective that resembles a cross-entropy loss. Building on architectural advances from supervised learning, we further propose regularized actor networks, while keeping critic design fixed. Our results show that simply replacing the standard actor network with our discretized regularized actor yields consistent gains and achieve the state-of-the-art performance across diverse continuous-control benchmarks.\nMachine Learning, ICML\n1\nIntroduction\nOn-policy reinforcement learning (RL) is a central paradigm for sequential decision-making, underpinning widely used algorithms such as policy gradient methods and their modern variants\n(Sutton\net al.\n,\n1998\n; Schulman\net al.\n,\n2015a\n; Mnih\net al.\n,\n2016b\n; Schulman\net al.\n,\n2017\n)\n. Despite their favorable theoretical properties and widespread adoption, these methods remain notoriously difficult to optimize in practice. For example, Proximal Policy Optimization (PPO) is often reported to exhibit unstable training dynamics, pronounced sensitivity to hyperparameters, and strong dependence on policy parameterization choices\n(Andrychowicz\net al.\n,\n2020\n)\n. Even on standard continuous-control benchmarks, achieving reliable performance typically requires careful â€œcode-levelâ€ optimization\n(Huang\net al.\n,\n2022a\n)\n. These observations suggest that the primary challenges of on-policy RL are not solely due to insufficient data or model capacity, but are closely tied to the optimization behavior induced by architectural and parameterization choices.\nTremendous efforts have been devoted to understanding and improving the training performance of on-policy reinforcement learning algorithms, including the development of improved advantage estimators\n(Schulman\net al.\n,\n2015c\n)\n, analyses highlighting the role of value estimation\n(Wang\net al.\n,\n2025\n)\n, and the design of constrained policy update mechanisms\n(Schulman\net al.\n,\n2015a\n,\n2017\n; Xie\net al.\n,\n2025\n)\n. In contrast, the parameterization of the policy itself is often treated as a secondary design choice, with continuous control tasks typically adopting Gaussian policies with diagonal covariance as a default since\n(Williams,\n1992\n)\n, largely due to its analytical and computational convenience.\nRecent work has begun to challenge the conventional from two complementary directions.\nFirst, earlier on-policy studies show that discretizing continuous actions and optimizing categorical policies can yield substantial gains over Gaussian actors\n(Tang and Agrawal,\n2020\n; Zhu\net al.\n,\n2024\n)\n.\nSecond, value-function learning has increasingly embraced classification-style objectives: distributional RL trains categorical return distributions with a cross-entropy loss\n(Bellemare\net al.\n,\n2017\n)\n, and recent work shows that replacing MSE with cross-entropy can improve robustness to noisy bootstrapped targets and support scaling to higher-capacity networks\n(Farebrother\net al.\n,\n2024\n; Hafner\net al.\n,\n2025\n)\n.\nThese advances are particularly pronounced in off-policy RL, where the value function is the primary object being optimized and directly shapes the policy update.\nA symmetric perspective applies to on-policy RL: the actor is the object being optimized, and discretized categorical actors turn the policy update into a cross-entropy-like objective over action bins, making architectural and optimization choices that benefit classification especially relevant.\nA closely related phenomenon appears in on-policy RL for language models, where the policy is categorical over tokens and PPO-style optimization is widely used for RLHF\n(Yue\net al.\n,\n2025\n; Guo\net al.\n,\n2025\n; Yu\net al.\n,\n2025\n; Sheng\net al.\n,\n2025\n)\n. Moreover, on the theoretical side,\n(Agarwal\net al.\n,\n2021\n)\nshows that policy gradient methods admit global convergence guarantees with the softmax policy parameterization under the condition of exact gradients and adequate exploration.\nMotivated by this connection, we revisit actor design in on-policy RL for continuous control tasks through the lens of classification-style objectives.\nWe make a simple drop-in change to the policy parameterization:\ninstead of a Gaussian policy with continuous action outputs, we use a factorized categorical policy, which turns the on-policy policy loss into a cross-entropy-like objective over selected bins.\nWe then pair this parameterization with regularized actor networks, which promote more stable training.\nContributions of this paper can be summarized as:\nâ€¢\nWe revisit actor parameterization and architecture as underexplored levers for improving on-policy continuous control, and study discretized categorical policies whose optimization resembles cross-entropy loss.\nâ€¢\nWe propose a regularized actor network design for discretized on-policy learning that improves optimization stability and reduces gradient variance during training.\nâ€¢\nWe demonstrate that our approach consistently improves control performance and accelerates convergence across standard locomotion benchmarks and ManiSkill, covering both state-based and vision-based tasks.\n2\nRelated Works\nOn-policy reinforcement learning.\nOn-policy policy gradient methods optimize the expected return by differentiating through a\nstochastic policy, dating back to REINFORCE\n(Williams,\n1992\n)\nand the policy gradient\ntheorem with function approximation\n(Sutton\net al.\n,\n1999\n)\n. Modern deep actorâ€“critic\nsystems (e.g., A3C) improved stability and throughput via parallel data collection and actorâ€“critic\nupdates\n(Mnih\net al.\n,\n2016a\n)\n. A central challenge in on-policy learning is balancing stability and\nsample reuse. Natural-gradient and trust-region perspectives\n(Cobbe\net al.\n,\n2021\n; Wu\net al.\n,\n2017\n)\nmotivated algorithms that constrain policy updates, most notably TRPO\n(Schulman\net al.\n,\n2015b\n)\n.\nPPO\n(Schulman\net al.\n,\n2017\n)\npopularized a simpler clipped surrogate objective that supports multiple\nepochs of minibatch updates, and generalized advantage estimation (GAE) further improved\nvarianceâ€“bias tradeoffs in advantage computation\n(Schulman\net al.\n,\n2015c\n)\n.\nIn parallel, several large-scale empirical studies have shown that the performance of on-policy\nmethods depends strongly on â€œimplementation ingredientsâ€ beyond the high-level objective, including\nnormalization, clipping, network design, and optimization details\n(Andrychowicz\net al.\n,\n2020\n; Gronauer\net al.\n,\n2021\n)\n.\nMotivated by these findings, our work targets a complementary axis: we study how the\npolicy\nrepresentation itself\n, in particular categorical parameterizations with neural network architecture design affects optimization behavior under standard on-policy training.\nActor policy parameterization for continuous control.\nMost continuous-control actorâ€“critic implementations parameterize the policy with a diagonal\nGaussian distribution\n(Andrychowicz\net al.\n,\n2020\n)\ndue to its simplicity and reparameterization-friendly sampling; however, this\nchoice can interact with exploration and gradient variance.\nTo address the limitations of Gaussian policies with bounded action spaces, prior work replaces them with the finite-support Beta distribution\n(Chou\net al.\n,\n2017\n)\n, yielding a bias-free, lower-variance policy that performs well empirically. Further, considering the extremes along each action dimension, a Bernoulli distribution is applied to replace the Gaussian parametrization of continuous control methods and show improved performance on several continuous control benchmarks\n(Seyde\net al.\n,\n2021\n)\n.\nWithin PPO-style training, PPO-CMA adapts the exploration covariance inspired by CMA-ES to mitigate\npremature variance collapse in Gaussian policies\n(HÃ¤mÃ¤lÃ¤inen\net al.\n,\n2020\n)\n.\nAn orthogonal line of work replaces continuous distributions with\ndiscretized\ncategorical\npolicies over per-dimension action bins. Tang and Agrawal\n(Tang and Agrawal,\n2020\n)\nshowed that factorized\ncategorical policies can scale to high-dimensional control and often improve on-policy optimization, and further proposed ordinal parameterizations to explicitly encode the natural ordering of action bins. More recently, unimodal discrete\nparameterizations (e.g., Poisson-based) were introduced to enforce ordering structure and to reduce undesirable multimodality\nand gradient variance in discretized actors\n(Zhu\net al.\n,\n2024\n)\n.\nNetwork architectures.\nRecent advances in computer vision and NLP have been largely driven by scaling model capacity\n(Lee\net al.\n,\n2024\n)\n.\nIn supervised learning, architectural choices, including skip connections, normalization, and depth/width scaling, are known to strongly influence optimization. Residual networks\n(He\net al.\n,\n2016\n)\nimprove the trainability of deep models via identity pathways, while batch normalization\n(Ioffe and Szegedy,\n2015\n)\nand layer normalization\n(Ba\net al.\n,\n2016\n)\nhelp stabilize activations and gradients. In contrast, network design and systematic scaling have been comparatively less explored in deep reinforcement learning, where many standard benchmarks and implementations\n(Huang\net al.\n,\n2022b\n; Andrychowicz\net al.\n,\n2020\n)\nstill rely on relatively shallow MLP backbones (e.g., 2-3 layers) for the actor and critic.\nSeveral recent works revisit careful network designs for deep RL, often highlighting how network capacity can substantially influence performance\n(Lee\net al.\n,\n2024\n; Hansen\net al.\n,\n2024\n,\n2025\n)\n.\nSimBa\n(Lee\net al.\n,\n2024\n)\nand its follow-up SimBa-v2\n(Lee\net al.\n,\n2025\n)\npropose scalable MLP backbones for deep RL, showing that residual pathways and improved normalization can unlock consistent performance gains as model capacity increases.\nBroNet provides an extensive study of critic scaling and introduces a regularized residual MLP\narchitecture to unlock performance gains when increasing critic capacity\n(Nauman\net al.\n,\n2025\n)\n.\nWhile these works primarily improve performance by scaling backbones and strengthening value learning (often in off-policy settings), a systematic understanding of how\nactor\nnetwork architecture influences\non-policy\noptimization remains less mature. We complement this line of research by studying the actor network design principles under on-policy training, and by considering discretized categorical actors whose policy objective resembles a cross-entropy loss over action bins.\n3\nPreliminaries\nWe consider standard episodic reinforcement learning in continuous-control Markov decision processes (MDPs) with state space\nğ’®\n\\mathcal{S}\n, action space\nğ’œ\nâŠ‚\nâ„\nm\n\\mathcal{A}\\subset\\mathbb{R}^{m}\n, transition dynamics\nP\n(\nâ‹…\nâˆ£\ns\n,\na\n)\nP(\\cdot\\mid s,a)\n, reward function\nr\nâ€‹\n(\ns\n,\na\n)\nr(s,a)\n, and discount factor\nÎ³\nâˆˆ\n(\n0\n,\n1\n)\n\\gamma\\in(0,1)\n. A (stochastic) actor policy\nÏ€\nÎ¸\nâ€‹\n(\na\nâˆ£\ns\n)\n\\pi_{\\theta}(a\\mid s)\nis parameterized by\nÎ¸\n\\theta\nand induces the discounted return\nJ\nâ€‹\n(\nÎ¸\n)\nâ‰œ\nğ”¼\nÏ„\nâˆ¼\nÏ€\nÎ¸\nâ€‹\n[\nâˆ‘\nt\n=\n0\nâˆ\nÎ³\nt\nâ€‹\nr\nâ€‹\n(\ns\nt\n,\na\nt\n)\n]\n,\nJ(\\theta)\\triangleq\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t},a_{t})\\Big],\n(1)\nwhere\nÏ„\n=\n(\ns\n0\n,\na\n0\n,\ns\n1\n,\na\n1\n,\nâ€¦\n)\n\\tau=(s_{0},a_{0},s_{1},a_{1},\\ldots)\nis a trajectory generated by interacting with the environment under\nÏ€\nÎ¸\n\\pi_{\\theta}\n.\n3.1\nOn-policy actor optimization\nOn-policy methods such as Proximal Policy Optimization (PPO)\n(Schulman\net al.\n,\n2017\n)\nupdate the actor using data collected from the current policy. Let\nÏ€\nÎ¸\nold\n\\pi_{\\theta_{\\text{old}}}\ndenote the behavior policy used to sample a batch of trajectories. PPO maximizes a clipped surrogate objective that stabilizes policy updates by constraining the change in action probabilities:\nâ„’\nPPO\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nâ€‹\n[\nmin\nâ¡\n(\nr\nt\nâ€‹\n(\nÎ¸\n)\nâ€‹\nA\n^\nt\n,\nclip\nâ€‹\n(\nr\nt\nâ€‹\n(\nÎ¸\n)\n,\n1\nâˆ’\nÏµ\n,\n1\n+\nÏµ\n)\nâ€‹\nA\n^\nt\n)\n]\n.\n\\mathcal{L}^{\\mathrm{PPO}}(\\theta)=\\mathbb{E}\\Big[\\min\\big(r_{t}(\\theta)\\hat{A}_{t},\\;\\mathrm{clip}(r_{t}(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_{t}\\big)\\Big].\n(2)\nwhere\nr\nt\nâ€‹\n(\nÎ¸\n)\nâ‰œ\nÏ€\nÎ¸\nâ€‹\n(\na\nt\nâˆ£\ns\nt\n)\nÏ€\nÎ¸\nold\nâ€‹\n(\na\nt\nâˆ£\ns\nt\n)\nr_{t}(\\theta)\\triangleq\\frac{\\pi_{\\theta}(a_{t}\\mid s_{t})}{\\pi_{\\theta_{\\text{old}}}(a_{t}\\mid s_{t})}\nis the importance ratio,\nÏµ\n>\n0\n\\epsilon>0\nis the clipping threshold, and\nA\n^\nt\n\\hat{A}_{t}\nis an advantage estimate computed from a learned value function\nV\nÏ•\nâ€‹\n(\ns\n)\nV_{\\phi}(s)\n(e.g., using generalized advantage estimation).\nUnless stated otherwise, we consider the standard actorâ€“critic setup where\nÏ•\n\\phi\nis updated by minimizing a squared-error value loss, and the actor is updated by maximizingÂ (\n2\n).\n3.2\nPolicy parameterizations for continuous control\nIn this work, we consider two common stochastic actor families for continuous control: a continuous Gaussian policy and a discrete categorical policy constructed via action discretization. Without loss of generality, we assume the action space\nğ’œ\n=\n[\nâˆ’\n1\n,\n1\n]\nm\n\\mathcal{A}=[-1,1]^{m}\n.\nContinuous Gaussian actor.\nThe continuous actor models\nÏ€\nÎ¸\nâ€‹\n(\na\nâˆ£\ns\n)\n\\pi_{\\theta}(a\\mid s)\nas a multivariate Gaussian distribution\nÏ€\nc\nâ€‹\n(\na\nâˆ£\ns\n)\n=\nğ’©\nâ€‹\n(\nÎ¼\nÎ¸\nâ€‹\n(\ns\n)\n,\nÎ£\nÎ¸\nâ€‹\n(\ns\n)\n)\n,\n\\pi^{c}(a\\mid s)=\\mathcal{N}\\!\\big(\\mu_{\\theta}(s),\\,\\Sigma_{\\theta}(s)\\big),\n(3)\nwhere\na\nâˆˆ\nâ„\nm\na\\in\\mathbb{R}^{m}\nis an\nm\nm\n-dimensional continuous action vector. The mean\nÎ¼\nÎ¸\nâ€‹\n(\ns\n)\nâˆˆ\nâ„\nm\n\\mu_{\\theta}(s)\\in\\mathbb{R}^{m}\nis produced by a neural network. The covariance is typically chosen to be diagonal,\nÎ£\nÎ¸\nâ€‹\n(\ns\n)\n=\nDiag\nâ€‹\n(\nÏƒ\nÎ¸\n2\nâ€‹\n(\ns\n)\n)\nâˆˆ\nâ„\nm\nÃ—\nm\n,\n\\Sigma_{\\theta}(s)=\\mathrm{Diag}\\big(\\sigma_{\\theta}^{2}(s)\\big)\\in\\mathbb{R}^{m\\times m},\n(4)\nwith\nÏƒ\nÎ¸\nâ€‹\n(\ns\n)\nâˆˆ\nâ„\n+\nm\n\\sigma_{\\theta}(s)\\in\\mathbb{R}_{+}^{m}\nparameterized by\nÎ¸\n\\theta\n(either state-dependent or state-independent).\nDiscrete categorical actor via uniform discretization.\nIn contrast to continuous parameterizations, a discrete actor represents the policy as a categorical distribution over a finite set of discretized actions. Without loss of generality, we assume\nğ’œ\n=\n[\nâˆ’\n1\n,\n1\n]\nm\n\\mathcal{A}=[-1,1]^{m}\n. For each action dimension\ni\nâˆˆ\n{\n1\n,\nâ€¦\n,\nm\n}\ni\\in\\{1,\\ldots,m\\}\n, we define a uniform discretization of\n[\nâˆ’\n1\n,\n1\n]\n[-1,1]\ninto\nK\nK\nbins:\nğ’œ\ni\n=\n{\n2\nâ€‹\nj\nK\nâˆ’\n1\nâˆ’\n1\n|\nj\n=\n0\n,\n1\n,\nâ€¦\n,\nK\nâˆ’\n1\n}\n.\n\\mathcal{A}_{i}=\\Bigl\\{\\,\\tfrac{2j}{K-1}-1\\;\\Big|\\;j=0,1,\\ldots,K-1\\Bigr\\}.\n(5)\nThe policy over these\nK\nK\ndiscrete choices is parameterized by logits\nz\nÎ¸\nâ€‹\n(\ns\n)\ni\n,\nj\nz_{\\theta}(s)_{i,j}\nand a softmax:\nÏ€\nd\nâ€‹\n(\na\nj\ni\nâˆ£\ns\n)\n=\nexp\nâ¡\n(\nz\nÎ¸\nâ€‹\n(\ns\n)\ni\n,\nj\n)\nâˆ‘\nk\n=\n1\nK\nexp\nâ¡\n(\nz\nÎ¸\nâ€‹\n(\ns\n)\ni\n,\nk\n)\n,\nj\n=\n1\n,\nâ€¦\n,\nK\n.\n\\pi^{d}(a^{i}_{j}\\mid s)=\\frac{\\exp\\big(z_{\\theta}(s)_{i,j}\\big)}{\\sum_{k=1}^{K}\\exp\\big(z_{\\theta}(s)_{i,k}\\big)},\\quad j=1,\\ldots,K.\n(6)\nWe assume conditional independence across action dimensions, yielding a factorized joint policy\nÏ€\nd\nâ€‹\n(\na\nâˆ£\ns\n)\n=\nâˆ\ni\n=\n1\nm\nÏ€\nd\nâ€‹\n(\na\nj\ni\ni\nâˆ£\ns\n)\n,\n\\pi^{d}(a\\mid s)=\\prod_{i=1}^{m}\\pi^{d}(a^{i}_{j_{i}}\\mid s),\n(7)\nwhere\na\n=\n(\na\nj\n1\n1\n,\nâ€¦\n,\na\nj\nm\nm\n)\na=(a^{1}_{j_{1}},\\ldots,a^{m}_{j_{m}})\ncorresponds to selecting one bin index\nj\ni\nj_{i}\nper dimension. The resulting action lies in the original range\n[\nâˆ’\n1\n,\n1\n]\nm\n[-1,1]^{m}\nby construction.\nNote that beyond the plain softmax over per-bin logits, some works\n(Tang and Agrawal,\n2020\n; Zhu\net al.\n,\n2024\n)\nexplore structured discrete actor parameterizations that explicitly exploit the ordinal structure of discretized action bins to improve optimization stability. In this paper, we instead focus on the simplest baseline: uniform discretization with an independent softmax categorical policy per action dimension, so as to isolate the effect of discretizing the actor without additional architectural constraints.\nTraining with PPO.\nBoth the continuous Gaussian actor and the discretized categorical actor can be trained under the same on-policy optimization framework (e.g., PPO). The only policy-specific ingredient is the log-likelihood\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\nt\nâˆ£\ns\nt\n)\n\\log\\pi_{\\theta}(a_{t}\\mid s_{t})\nused to form the importance ratio in the PPO surrogate objective: it is evaluated under a Gaussian density for\nÏ€\nc\n\\pi^{c}\nand under a categorical probability mass function for\nÏ€\nd\n\\pi^{d}\n. All other components of the training pipeline, on-policy rollout collection, advantage estimation, and clipped surrogate maximization can be identical.\n4\nRevisiting Discrete Categorical Policies\nThis section revisits discretized categorical policies for continuous control from two complementary viewpoints. First, we analyze how the policy-gradient estimator behaves under a factorized categorical policy and how its variance depends on the discretization. Second, we provide an optimization perspective that connects PPO actor updates to standard supervised losses: cross-entropy for categorical actors and squared-error (under Gaussian likelihood) for continuous actors. These perspectives motivate the empirical and algorithmic choices studied in later sections.\nFigure 1:\nUnderstanding Continuous Gaussian and Discrete categorical actor policy from two perspective.\n4.1\nPolicy-gradient estimator and variance\nOn-policy policy gradient.\nTo analyze the stochastic gradient structure underlying PPO, we consider the policy-gradient term obtained by differentiating the loss terms inÂ (\n2\n) with respect to\nÎ¸\n\\theta\n:\ng\nâ€‹\n(\nÎ¸\n)\nâ‰œ\nğ”¼\n(\ns\nt\n,\na\nt\n)\nâˆ¼\nÏ€\nÎ¸\nold\nâ€‹\n[\nw\nt\nâ€‹\n(\nÎ¸\n)\nâ€‹\nA\n^\nt\nâ€‹\nâˆ‡\nÎ¸\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\nt\nâˆ£\ns\nt\n)\n]\n,\ng(\\theta)\\triangleq\\mathbb{E}_{(s_{t},a_{t})\\sim\\pi_{\\theta_{\\mathrm{old}}}}\\Big[w_{t}(\\theta)\\,\\hat{A}_{t}\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}\\mid s_{t})\\Big],\n(8)\nwhere\nw\nt\nâ€‹\n(\nÎ¸\n)\nw_{t}(\\theta)\ndenotes the effective PPO importance weight induced by clipping,\nw\nt\nâ€‹\n(\nÎ¸\n)\n=\n{\nr\nt\nâ€‹\n(\nÎ¸\n)\n,\nif unclipped\n,\n0\n,\nif clipped\n,\nw_{t}(\\theta)=\\begin{cases}r_{t}(\\theta),&\\text{if unclipped},\\\\\n0,&\\text{if clipped},\\end{cases}\n(9)\nr\nt\nâ€‹\n(\nÎ¸\n)\nr_{t}(\\theta)\nis the importance ratio and\nA\n^\nt\n\\hat{A}_{t}\nis an advantage estimator.\nIn practice,Â (\n8\n) is estimated from finite on-policy rollouts, so its optimization behavior is strongly influenced by the variance of the random vector\nw\nt\nâ€‹\n(\nÎ¸\n)\nâ€‹\nA\n^\nt\nâ€‹\nâˆ‡\nÎ¸\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\nt\nâˆ£\ns\nt\n)\nw_{t}(\\theta)\\,\\hat{A}_{t}\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}\\mid s_{t})\n.\nA vanilla policy-gradient toy setting.\nTo isolate the intrinsic stochasticity of the score function, we consider a simplified one-step on-policy setting with a fixed state\ns\ns\nand the vanilla REINFORCE estimator (i.e.,\nw\nt\nâ€‹\n(\nÎ¸\n)\nâ‰¡\n1\nw_{t}(\\theta)\\equiv 1\n). We further assume a constant return\nR\nt\nâ‰¡\nR\nR_{t}\\equiv R\nindependent of the sampled action\n(Zhu\net al.\n,\n2024\n)\n. The stochastic gradient estimator becomes\ng\n^\n(\nÎ¸\n)\n=\nR\nâˆ‡\nÎ¸\nlog\nÏ€\nÎ¸\n(\na\nâˆ£\ns\n)\n,\na\nâˆ¼\nÏ€\nÎ¸\n(\nâ‹…\nâˆ£\ns\n)\n.\n\\hat{g}(\\theta)\\;=\\;R\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(a\\mid s),\\quad a\\sim\\pi_{\\theta}(\\cdot\\mid s).\n(10)\nAlthough the true gradient is zero in this setting (since\nR\nR\ndoes not depend on\na\na\n), the estimatorÂ (\n10\n) generally has nonzero variance; this is precisely the variance that baselines/advantages are designed to reduce in practice.\nProposition 4.1\n(Gradient variance for Gaussian vs. categorical policies)\n.\nFix a state\ns\ns\nand consider the one-step REINFORCE estimator\ng\n^\nâ€‹\n(\nÎ¸\n)\n=\nR\nâ€‹\nâˆ‡\nÎ¸\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\nâˆ£\ns\n)\n\\hat{g}(\\theta)=R\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(a\\mid s)\nwith\na\nâˆ¼\nÏ€\nÎ¸\n(\nâ‹…\nâˆ£\ns\n)\na\\sim\\pi_{\\theta}(\\cdot\\mid s)\nand constant return\nR\nR\n.\nThen the conditional covariance for the two policy families are:\n[1]\nGaussian (gradient w.r.t. mean).\nDenote\nÏ€\nc\nâ€‹\n(\na\nâˆ£\ns\n)\n=\nğ’©\nâ€‹\n(\nÎ¼\n,\nÎ£\n)\n\\pi^{c}(a\\mid s)=\\mathcal{N}(\\mu,\\Sigma)\nwith diagonal\nÎ£\n=\nDiag\nâ€‹\n(\nÏƒ\n2\n)\n\\Sigma=\\mathrm{Diag}(\\sigma^{2})\nand treat\nÎ¼\nâˆˆ\nâ„\nm\n\\mu\\in\\mathbb{R}^{m}\nas the parameter (with\nÎ£\n\\Sigma\nfixed). Then\nğ”¼\nâ€‹\n[\nâ€–\ng\n^\nÎ¼\nâ€–\n2\n2\nâˆ£\ns\n]\n=\nR\n2\nâ€‹\nTr\nâ€‹\n(\nÎ£\nâˆ’\n1\n)\n=\nâˆ‘\ni\n=\n1\nm\nR\n2\nÏƒ\ni\n2\n.\n\\mathbb{E}\\!\\big[\\|\\hat{g}_{\\mu}\\|_{2}^{2}\\mid s\\big]=R^{2}\\,\\mathrm{Tr}(\\Sigma^{-1})=\\sum_{i=1}^{m}\\frac{R^{2}}{\\sigma_{i}^{2}}.\n(11)\n[2]\nCategorical (gradient w.r.t. logits).\nDenote the discretized categorical policy factorize across\nm\nm\naction dimensions as\nÏ€\nd\nâ€‹\n(\na\nâˆ£\ns\n)\n=\nâˆ\ni\n=\n1\nm\nÏ€\nd\nâ€‹\n(\na\nj\ni\ni\nâˆ£\ns\n)\n\\pi^{d}(a\\mid s)=\\prod_{i=1}^{m}\\pi^{d}(a^{i}_{j_{i}}\\mid s)\n, where\np\ni\nâ€‹\n(\ns\n)\nâˆˆ\nÎ”\nK\nâˆ’\n1\np_{i}(s)\\in\\Delta^{K-1}\nis the per-dimension softmax probability vector over\nK\nK\nbins and\nj\ni\nâˆ¼\nCat\nâ€‹\n(\np\ni\nâ€‹\n(\ns\n)\n)\nj_{i}\\sim\\mathrm{Cat}(p_{i}(s))\n.\nTreat the per-dimension logits\nz\ni\nâ€‹\n(\ns\n)\nâˆˆ\nâ„\nK\nz_{i}(s)\\in\\mathbb{R}^{K}\nas parameters and let\nz\nâ€‹\n(\ns\n)\n=\n[\nz\n1\nâ€‹\n(\ns\n)\n;\nâ€¦\n;\nz\nm\nâ€‹\n(\ns\n)\n]\nâˆˆ\nâ„\nm\nâ€‹\nK\nz(s)=[z_{1}(s);\\ldots;z_{m}(s)]\\in\\mathbb{R}^{mK}\n.\nThen\nğ”¼\nâ€‹\n[\nâ€–\ng\n^\nz\nâ€–\n2\n2\nâˆ£\ns\n]\n=\nR\n2\nâ€‹\nâˆ‘\ni\n=\n1\nm\n(\n1\nâˆ’\nâ€–\np\ni\nâ€‹\n(\ns\n)\nâ€–\n2\n2\n)\nâ‰¤\nm\nâ€‹\nR\n2\nâ€‹\n(\n1\nâˆ’\n1\nK\n)\n\\displaystyle\\mathbb{E}\\!\\left[\\|\\hat{g}_{z}\\|_{2}^{2}\\mid s\\right]=R^{2}\\sum_{i=1}^{m}\\Big(1-\\|p_{i}(s)\\|_{2}^{2}\\Big)\\leq mR^{2}\\Big(1-\\frac{1}{K}\\Big)\n(12)\nMoreover, the inequality is tight if and only if\np\ni\nâ€‹\n(\ns\n)\np_{i}(s)\nis uniform over the\nK\nK\nbins\nfor all\ni\ni\n.\nProof.\nThe full proof is provided in Appendix\nA\n.\nâˆ\nInsight:\nFor continuous Gaussian actors, gradient variance can grow sharply as the policy standard\ndeviation decreases, a behavior that commonly occurs in practice as exploration is reduced\nover the course of training.\nEven at early training stages, where the standard deviation remains relatively large, e.g.,\nÏƒ\n=\n1\n\\sigma=1\n, the per-dimension gradient-variance gap between Gaussian and categorical policies is lower bounded by\n1\n/\nK\n1/K\n, and thus the total gap scales linearly with the action dimension\nm\nm\n.\n4.2\nOptimization view: cross-entropy vs. squared error\nGaussian actor: (weighted) squared error.\nFor a diagonal Gaussian with (state-dependent or state-independent) variance, the negative log-likelihood is\nâˆ’\nlog\nâ¡\nÏ€\nÎ¸\nc\nâ€‹\n(\na\nâˆ£\ns\n)\n\\displaystyle-\\log\\pi^{c}_{\\theta}(a\\mid s)\n=\n1\n2\nâ€‹\n(\na\nâˆ’\nÎ¼\nÎ¸\nâ€‹\n(\ns\n)\n)\nâŠ¤\nâ€‹\nÎ£\nÎ¸\nâ€‹\n(\ns\n)\nâˆ’\n1\nâ€‹\n(\na\nâˆ’\nÎ¼\nÎ¸\nâ€‹\n(\ns\n)\n)\n\\displaystyle=\\frac{1}{2}(a-\\mu_{\\theta}(s))^{\\top}\\Sigma_{\\theta}(s)^{-1}(a-\\mu_{\\theta}(s))\n(13)\n+\n1\n2\nâ€‹\nlog\nâ¡\n|\nÎ£\nÎ¸\nâ€‹\n(\ns\n)\n|\n+\nconstant.\n\\displaystyle\\;+\\;\\frac{1}{2}\\log|\\Sigma_{\\theta}(s)|\\;+\\;\\text{constant.}\nIf\nÎ£\nÎ¸\nâ€‹\n(\ns\n)\n=\nÏƒ\n2\nâ€‹\nI\n\\Sigma_{\\theta}(s)=\\sigma^{2}I\nis fixed, then minimizingÂ (\n13\n) with respect to\nÎ¼\nÎ¸\nâ€‹\n(\ns\n)\n\\mu_{\\theta}(s)\nis equivalent to minimizing a mean squared error:\nâˆ’\nlog\nâ¡\nÏ€\nÎ¸\nc\nâ€‹\n(\na\nâˆ£\ns\n)\nâˆ\nâ€–\na\nâˆ’\nÎ¼\nÎ¸\nâ€‹\n(\ns\n)\nâ€–\n2\n2\n.\n-\\log\\pi^{c}_{\\theta}(a\\mid s)\\;\\propto\\;\\|a-\\mu_{\\theta}(s)\\|_{2}^{2}.\n(14)\nIn combination with equationÂ (\n8\n), the Gaussian actor update admits an interpretation as a weighted mean squared error regression, where the weights are determined by the advantage and the importance ratio.\nCategorical actor: (weighted) cross-entropy.\nFor a discretized categorical policy, a sampled action corresponds to a bin index\nj\ni\nj_{i}\nper dimension.\nThe negative log-likelihood (NLL) for each dimension is\nâˆ’\nlog\nâ¡\nÏ€\nÎ¸\nd\nâ€‹\n(\na\ni\nâˆ£\ns\n)\n=\nâˆ’\nlog\nâ¡\np\ni\nâ€‹\n(\nj\ni\nâˆ£\ns\n)\n,\n-\\log\\pi^{d}_{\\theta}(a^{i}\\mid s)=-\\log p_{i}(j_{i}\\mid s),\n(15)\nwhich is exactly the cross-entropy between a one-hot target\ne\nj\ni\ne_{j_{i}}\nand the predicted distribution\np\ni\nâ€‹\n(\ns\n)\np_{i}(s)\n.\nUnder equationÂ (\n8\n), the objective reduces to a weighted cross-entropy loss, where the weights are given by the advantage-modulated importance ratio (with clipping).\nAccordingly, the categorical actor update corresponds to minimizing this weighted cross-entropy over the sampled action bins.\nWith strong empirical results to support the use of\ncross-entropy as a â€œdrop-inâ€ replacement for the mean\nsquared error (MSE) regression loss in deep RL for value function learning\n(Farebrother\net al.\n,\n2024\n)\n, few work address this for the actor network.\nInsight:\nFrom this perspective, On-policy RL training differs across policy families primarily through the likelihood model that defines\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\nâˆ£\ns\n)\n\\log\\pi_{\\theta}(a\\mid s)\n: categorical policies induce (weighted) cross-entropy objectives over bins, while Gaussian policies induce (weighted) quadratic objectives over continuous action.\nAlthough prior work provides strong empirical evidence that cross-entropy can replace the mean squared error (MSE) regression loss for value function learning in deep reinforcement learning\n(Farebrother\net al.\n,\n2024\n; Lee\net al.\n,\n2025\n)\n, the consequences of analogous loss choices for actor network learning remain largely unexplored. Motivated by viewing the policy objective through this loss-based lens: weighted MSE for Gaussian actors versus weighted cross-entropy for categorical actors, we hypothesize that the\nclassification\n-style structure of the categorical update can better exploit neural network capacity. This perspective further motivates our use of discrete categorical actors as a scalable design choice for training large RL policies.\n5\nActor Network Architecture\nFigure 2:\nProposed\nR\negularized\nN\network for\nD\niscrete action policies (RN-D). The actor consists of a feature extractor (MLP or CNN) followed by pre-LayerNorm residual MLP blocks, enabling stable optimization and improved scalability. The output layer produces categorical logits over\nK\nK\nbins for each action dimension.\nFigure 3:\nAggregate learning curves across benchmarks. Each subplot reports the mean performance over tasks within a benchmark (MuJoCo locomotion: normalized return; ManiSkill: success rate) as a function of environment steps. Curves are averaged over 5 random seeds; shaded regions denote 95% stratified bootstrap confidence intervals. The red annotations indicate a sample-efficiency speedup.\nWe adopt a unified actor network architecture for discrete categorical policies that emphasizes optimization stability and scalability across high-dimensional action spaces.\nThe architecture is composed of a feature extraction stage, a stack of residual feedforward blocks, and a final output projection to categorical action logits.\nFeature Extraction.\nThe actor first maps raw observations to a latent feature representation using a task-dependent encoder.\nFor low-dimensional state inputs, we employ a multi-layer perceptron (MLP) encoder composed of linear layers and nonlinear activations.\nFor high-dimensional observations (e.g., images), a convolutional neural network (CNN) encoder is used instead.\nThe encoder produces a shared latent representation that is consumed by subsequent feedforward blocks.\nResidual Feedforward Blocks.\nFollowing feature extraction, the actor applies a stack of\nN\nN\nresidual feedforward blocks.\nEach block is equipped with a residual connection,\nğ¡\nâ„“\n+\n1\n=\nğ¡\nâ„“\n+\nFFN\nâ€‹\n(\nLN\nâ€‹\n(\nğ¡\nâ„“\n)\n)\n,\n\\mathbf{h}_{\\ell+1}=\\mathbf{h}_{\\ell}+\\mathrm{FFN}(\\mathrm{LN}(\\mathbf{h}_{\\ell})),\nwhere\nLN\nâ€‹\n(\nâ‹…\n)\n\\mathrm{LN}(\\cdot)\ndenotes layer normalization.\nWe employ pre-layer normalization by applying LayerNorm before each feedforward block, a design that improves optimization stability in deep architectures, including Transformer models\n(Xiong\net al.\n,\n2020\n)\n.\nFollowing\n(Vaswani\net al.\n,\n2017\n)\n, the feedforward network adopts an inverted bottleneck structure.\nThe hidden dimension is first expanded from\nd\nh\nd_{h}\nto\n4\nâ€‹\nd\nh\n4d_{h}\nand passed through a ReLU activation, and a second linear layer projects the representation back to\nd\nh\nd_{h}\n.\nOutput Layer.\nThe final hidden representation is passed through a linear projection that outputs logits for a discrete categorical distribution.\nSpecifically, for an action space with\nd\nd\ndimensions and\nK\nK\nbins per dimension, the output layer produces\nd\nÃ—\nK\nd\\times K\nlogits, which parameterize independent categorical distributions over each action dimension.\nRelation to Prior Architectures.\nThe proposed actor architecture shares structural similarities with several existing designs.\nResidual MLPs with normalization have been explored in prior reinforcement learning works such as SimBa\n(Lee\net al.\n,\n2024\n,\n)\nand BroNet\n(Nauman\net al.\n,\n2024\n)\n, as well as in the feedforward sublayers of Transformer models\n(Vaswani\net al.\n,\n2017\n)\n.\nOur contribution does not lie in introducing a novel network module, but rather in demonstrating that this architectural choice is particularly well-suited for discrete categorical actor policies.\nAs shown in our experiments, combining our network with categorical action parameterization leads to improved optimization behavior compared to conventional shallow MLP actors commonly used in on-policy RL.\n6\nExperiment\nWe evaluate the proposed discrete policy with regularized network (RN-D) under PPO across a diverse set of locomotion and manipulation benchmarks. A full list of tasks, environment details, and hyperparameters is provided in Appendix\nB\n.\nExperiment Setup.\nWe evaluate our methods on three environment families: Gym locomotion\n(Brockman\net al.\n,\n2016\n)\n, ManiSkill\n(Tao\net al.\n,\n2025\n)\nwith state-based observations, and ManiSkill with RGB-based observations.\nThese tasks cover a range of control challenges, including locomotion and dexterous manipulation, varying embodiments, and different observation modalities.\nSpecifically, we consider 5 standard Gym locomotion tasks, 10 ManiSkill manipulation tasks with low-dimensional state observations, and 5 ManiSkill tasks with image-based observations, where policies operate on raw RGB inputs via a convolutional encoder.\nBaselines.\nAll experiments are conducted with Proximal Policy Optimization (PPO)\n(Schulman\net al.\n,\n2017\n)\n, using implementations adapted from the CleanRL\n(Huang\net al.\n,\n2022b\n)\nand ManiSkill codebases\n(Tao\net al.\n,\n2025\n)\n.\nWe compare\nRN-D\n, our regularized-network discrete (categorical) actor, against three baselines:\n(i)\nRN-C\n, a continuous policy parameterized by a factorized Gaussian with the same regularized network;\n(ii)\nMLP-C\n1\n1\n1\nMLP-C\nrefers to the widely adopted PPO actor in standard implementations.\n, a continuous factorized-Gaussian policy with a standard MLP actor; and\n(iii)\nMLP-D\n, a discrete (categorical) policy with a standard MLP actor.\nAcross all four settings, we keep the critic (value network) architecture identical to ensure a fair comparison.\nFor discrete actors, we use\nK\n=\n41\nK=41\nbins throughout.\nMetrics.\nTo aggregate performance across benchmarks, we use task-standard metrics and normalize returns when needed. For Gym MuJoCo locomotion, we report TD3-normalized return\n(Fujimoto\net al.\n,\n2018\n)\n. For ManiSkill benchmarks, we report success rate, i.e., the fraction of evaluation episodes that achieve the task goal.\nOverall Performance.\nFigure\n3\nshows that\nRN-D (ours)\nconsistently achieves the best performance across both locomotion and ManiSkill.\nOn Gym locomotion, RN-D attains higher normalized returns than the Gaussian baselines, with particularly clear improvements on harder tasks such as Ant and Humanoid. On ManiSkill, RN-D yields the\nhighest average success on the 10 state-based tasks and is the only variant that reliably solves the challenging StackCube task, where\nother methods remain near-zero success for most of training. The same trend holds for vision-based ManiSkill tasks: RN-D maintains its\nadvantage when paired with CNN encoders and reaches near-saturated success more reliably than baselines.\nBeyond final performance, RN-D also reaches strong performance with fewer environment steps. The red annotations indicate a sample-efficiency speedup: RN-D matches the best baselineâ€™s performance using roughly\n1.3\n1.3\nâ€“\n1.9\nÃ—\n1.9\\times\nfewer interaction steps.\nComparing ablations reveals complementary effects. For state-based settings (locomotion and ManiSkill-State), RN-C versus MLP-C (blue/orange) shows that the regularized backbone improves performance even with Gaussian actors, while MLP-D versus MLP-C (green/orange) indicates that discretization alone can also be beneficial. For vision-based tasks, the CNN encoder provides a strong representation and narrows the gap among actor variants; in some cases MLP-C is already competitive. Nevertheless, RN-D still achieves the best final performance and consistently improves sample efficiency.\nGradient Variance.\nWe plot the policy-gradient variance (log scale) on five MuJoCo tasks as a representative illustration (Fig\n4\n). Across training, the Gaussian policy (RN-C/MLP-C) exhibits consistently larger variance, often by orders of magnitude, than its discrete categorical counterpart (RN-D/MLP-D). This disparity is most pronounced for the standard MLP actor: MLP-C (orange) stays far above MLP-D (green) and increases steadily with environment steps, which is consistent with the Gaussian actorâ€™s standard deviation shrinking over training and amplifying gradient variability. Replacing the MLP with a regularized network reduces variance for both policy classes. Overall, the discrete categorical policy with a regularized network (RN-D, purple) achieves the lowest policy-gradient variance throughout training.\nFigure 4:\nThe evolution of the policy-gradient variance over training (log scale).\nComponent Analysis.\nWe visualize the gradient signal-to-noise ratio (SNR) and normalized return on Gym locomotion tasks, where SNR is computed as the squared norm of the mean policy gradient divided by its variance across mini-batches during training. Following prior work\n(Andrychowicz\net al.\n,\n2020\n)\n, we report the 95th-percentile performance over training, and summarize gradient signal quality using the mean SNR. Figure\n5\n(a) shows that residual connections and layer normalization each improve SNR relative to a plain MLP actor. Figure\n5\n(b) shows a consistent trend in performance, suggesting a strong correlation between gradient signal quality (SNR) and normalized return across architectures.\nFigure 5:\nComponent Analysis. (a) Gradient signal-to-noise ratio (SNR) on Gym locomotion tasks. Bars show the mean and error bars denote one standard deviation across tasks. (b) Average normalized return. Higher SNR correlates with higher returns, with RN-D achieving the best performance.\nFigure 6:\nExtended studies on sample efficiency, optimization objective, and scaling.\n7\nExtended Study\nAdditional Comparisons with State-of-the-Art Methods.\nIn Appendix\nC.1\n, we further compare our method to strong state-of-the-art baselines, TD3\n(Fujimoto\net al.\n,\n2018\n)\nand TD-MPC2\n(Hansen\net al.\n,\n2024\n)\n, on Humanoid-v4 and StackCube-v1.\nIn Figure\n6\n(a), the proposed approach achieves strong wall-clock efficiency and performs well on both tasks. In contrast, TD-MPC2 is considerably less computationally efficient, and TD3 performs well on Humanoid-v4 but struggles on StackCube-v1.\nRole of Cross-Entropy-Like\nOptimization.\nWe introduce a loss-swap ablation in Appendix\nC.2\n.\nThe proposed discrete actor continues to output logits over\nK\nK\ndiscretized bins per action dimension, but instead of sampling from the categorical distribution, we compute the expected action as the probability-weighted average of bin centers and treat it as the mean of a Gaussian policy. Policy updates are then performed using the standard Gaussian log-likelihood, resulting in a weighted squared-error objective.\nFigure\n6\n(b) compares RN-D, RN-C and the loss-swapped variant.\nThese results show that discretizing the action space alone is insufficient to explain the observed gains.\nEven with the same discretized representation, switching from a cross-entropy-like objective to a squared-error objective eliminates most of the performance improvement, highlighting the central role of the categorical likelihood objective.\nScaling the actor network.\nIn Appendix\nC.3\n, we keep the critic fixed and scale the RN-D actor by varying its hidden width from\nd\nh\n=\n64\nd_{h}=64\nto\n512\n512\n.\nWe find that wider actors consistently converge faster on both Humanoid-v4 and StackCube-v1, indicating that increased capacity in the proposed RN-D policy can be effectively leveraged to improve learning speed under the same PPO training protocol.\nScaling the discrete bins.\nIn the appendix\nC.4\n, we conduct an ablation that sweeps the discretization granularity by varying the number of action bins from 11 to 1001 (with 41 bins used in the main experiments).\nWe find that moderate bin counts perform best. Performance degrades as the number of bins grows, often remaining stable around 11â€“101 bins.\nOverall, discrete-policy performance is sensitive to bin granularity: small bin counts preserve the optimization benefits of discretization, whereas very large bin counts require overly fine-grained predictions that can exceed policy capacity and lead to performance degradation.\n8\nConclusion\nWe revisited actor design for on-policy reinforcement learning in continuous control and showed that policy parameterization and architecture provide a strong, underexplored lever for improving PPO-style training. Our approach replaces the standard diagonal-Gaussian actor with a discretized categorical policy over action bins and pairs it with a regularized actor network, while keeping the critic and the underlying on-policy objective fixed. Across locomotion benchmarks and ManiSkill (state- and vision-based) tasks, this simple actor-side change consistently improves final performance, accelerates convergence, and yields more stable learning.\nSeveral directions appear promising. First, it is natural to extend our findings beyond PPO and evaluate discretized categorical actors with other on-policy algorithms.\nSecond, while this work isolates actor-side effects by holding the critic fixed, understanding actorâ€“critic interactions remains an important direction.\nFinally, we envision applying the approach to more complex embodied settings, including long-horizon manipulation, contact-rich control, and multi-object tasks, to better characterize its benefits and limitations in challenging real-world regimes.\nImpact Statement\nThis paper presents work whose goal is to advance the field of reinforcement learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.\nAcknowledgements\nWe thank Haiwen Yu for the contributions to the framework visualization design.\nReferences\nA. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan (2021)\nOn the theory of policy gradient methods: optimality, approximation, and distribution shift\n.\nJournal of Machine Learning Research\n22\n(\n98\n),\npp.Â 1â€“76\n.\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nM. Andrychowicz, A. Raichuk, P. StaÅ„czyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist, O. Pietquin, M. Michalski, S. Gelly, and O. Bachem (2020)\nWhat matters in on-policy reinforcement learning? a large-scale empirical study\n.\nExternal Links:\n2006.05990\n,\nLink\nCited by:\nÂ§1\n,\nÂ§2\n,\nÂ§2\n,\nÂ§2\n,\nÂ§6\n.\nJ. L. Ba, J. R. Kiros, and G. E. Hinton (2016)\nLayer normalization\n.\narXiv preprint arXiv:1607.06450\n.\nCited by:\nÂ§2\n.\nM. G. Bellemare, W. Dabney, and R. Munos (2017)\nA distributional perspective on reinforcement learning\n.\nIn\nProceedings of the 34th International Conference on Machine Learning\n,\nD. Precup and Y. W. Teh (Eds.)\n,\nProceedings of Machine Learning Research\n, Vol.\n70\n,\npp.Â 449â€“458\n.\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nG. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba (2016)\nOpenai gym\n.\narXiv preprint arXiv:1606.01540\n.\nCited by:\nÂ§6\n.\nP. Chou, D. Maturana, and S. Scherer (2017)\nImproving stochastic policy gradients in continuous control with deep reinforcement learning using the beta distribution\n.\nIn\nInternational conference on machine learning\n,\npp.Â 834â€“843\n.\nCited by:\nÂ§2\n.\nK. W. Cobbe, J. Hilton, O. Klimov, and J. Schulman (2021)\nPhasic policy gradient\n.\nIn\nInternational Conference on Machine Learning\n,\npp.Â 2020â€“2027\n.\nCited by:\nÂ§2\n.\nJ. Farebrother, J. Orbay, Q. Vuong, A. A. TaÃ¯ga, Y. Chebotar, T. Xiao, A. Irpan, S. Levine, P. S. Castro, A. Faust,\net al.\n(2024)\nStop regressing: training value functions via classification for scalable deep rl\n.\narXiv preprint arXiv:2403.03950\n.\nCited by:\nÂ§1\n,\nÂ§4.2\n,\nÂ§4.2\n.\nS. Fujimoto, H. Hoof, and D. Meger (2018)\nAddressing function approximation error in actor-critic methods\n.\nIn\nInternational conference on machine learning\n,\npp.Â 1587â€“1596\n.\nCited by:\nÂ§C.1\n,\nÂ§6\n,\nÂ§7\n.\nS. Gronauer, M. Gottwald, and K. Diepold (2021)\nThe successful ingredients of policy gradient algorithms.\n.\nIn\nIJCAI\n,\npp.Â 2455â€“2461\n.\nCited by:\nÂ§2\n.\nD. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi,\net al.\n(2025)\nDeepseek-r1: incentivizing reasoning capability in llms via reinforcement learning\n.\narXiv preprint arXiv:2501.12948\n.\nCited by:\nÂ§1\n.\nD. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap (2025)\nMastering diverse control tasks through world models\n.\nNature\n,\npp.Â 1â€“7\n.\nCited by:\nÂ§1\n.\nP. HÃ¤mÃ¤lÃ¤inen, A. Babadi, X. Ma, and J. Lehtinen (2020)\nPPO-cma: proximal policy optimization with covariance matrix adaptation\n.\nIn\n2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP)\n,\npp.Â 1â€“6\n.\nCited by:\nÂ§2\n.\nN. Hansen, H. Su, and X. Wang (2024)\nTD-mpc2: scalable, robust world models for continuous control\n.\nExternal Links:\n2310.16828\n,\nLink\nCited by:\nÂ§C.1\n,\nÂ§2\n,\nÂ§7\n.\nN. Hansen, H. Su, and X. Wang (2025)\nLearning massively multitask world models for continuous control\n.\nExternal Links:\n2511.19584\n,\nLink\nCited by:\nÂ§2\n.\nK. He, X. Zhang, S. Ren, and J. Sun (2016)\nDeep residual learning for image recognition\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 770â€“778\n.\nCited by:\nÂ§2\n.\nS. Huang, R. F. J. Dossa, A. Raffin, A. Kanervisto, and W. Wang (2022a)\nThe 37 implementation details of proximal policy optimization\n.\nThe ICLR Blog Track 2023\n.\nCited by:\nÂ§1\n.\nS. Huang, R. F. J. Dossa, C. Ye, J. Braga, D. Chakraborty, K. Mehta, and J. G. AraÃƒÅ¡jo (2022b)\nCleanrl: high-quality single-file implementations of deep reinforcement learning algorithms\n.\nJournal of Machine Learning Research\n23\n(\n274\n),\npp.Â 1â€“18\n.\nCited by:\nÂ§B.1\n,\nÂ§2\n,\nÂ§6\n.\nS. Ioffe and C. Szegedy (2015)\nBatch normalization: accelerating deep network training by reducing internal covariate shift\n.\nIn\nInternational conference on machine learning\n,\npp.Â 448â€“456\n.\nCited by:\nÂ§2\n.\nH. Lee, D. Hwang, D. Kim, H. Kim, J. J. Tai, K. Subramanian, P. R. Wurman, J. Choo, P. Stone, and T. Seno (2024)\nSimba: simplicity bias for scaling up parameters in deep reinforcement learning\n.\narXiv preprint arXiv:2410.09754\n.\nCited by:\nÂ§2\n,\nÂ§2\n,\nÂ§5\n.\n[21]\nH. Lee, Y. Lee, T. Seno, D. Kim, P. Stone, and J. Choo\nHyperspherical normalization for scalable deep reinforcement learning\n.\nIn\nForty-second International Conference on Machine Learning\n,\nCited by:\nÂ§5\n.\nH. Lee, Y. Lee, T. Seno, D. Kim, P. Stone, and J. Choo (2025)\nHyperspherical normalization for scalable deep reinforcement learning\n.\narXiv preprint arXiv:2502.15280\n.\nCited by:\nÂ§2\n,\nÂ§4.2\n.\nV. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu (2016a)\nAsynchronous methods for deep reinforcement learning\n.\nIn\nInternational conference on machine learning\n,\npp.Â 1928â€“1937\n.\nCited by:\nÂ§2\n.\nV. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu (2016b)\nAsynchronous methods for deep reinforcement learning\n.\nIn\nProceedings of The 33rd International Conference on Machine Learning\n,\nM. F. Balcan and K. Q. Weinberger (Eds.)\n,\nProceedings of Machine Learning Research\n, Vol.\n48\n,\nNew York, New York, USA\n,\npp.Â 1928â€“1937\n.\nCited by:\nÂ§1\n.\nM. Nauman, M. Cygan, C. Sferrazza, A. Kumar, and P. Abbeel (2025)\nBigger, regularized, categorical: high-capacity value functions are efficient multi-task learners\n.\narXiv preprint arXiv:2505.23150\n.\nCited by:\nÂ§2\n.\nM. Nauman, M. Ostaszewski, K. Jankowski, P. MiÅ‚oÅ›, and M. Cygan (2024)\nBigger, regularized, optimistic: scaling for compute and sample efficient continuous control\n.\nAdvances in neural information processing systems\n37\n,\npp.Â 113038â€“113071\n.\nCited by:\nÂ§5\n.\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz (2015a)\nTrust region policy optimization\n.\nIn\nProceedings of the 32nd International Conference on Machine Learning\n,\nF. Bach and D. Blei (Eds.)\n,\nProceedings of Machine Learning Research\n, Vol.\n37\n,\nLille, France\n,\npp.Â 1889â€“1897\n.\nCited by:\nÂ§1\n,\nÂ§1\n.\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz (2015b)\nTrust region policy optimization\n.\nIn\nInternational conference on machine learning\n,\npp.Â 1889â€“1897\n.\nCited by:\nÂ§2\n.\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel (2015c)\nHigh-dimensional continuous control using generalized advantage estimation\n.\narXiv preprint arXiv:1506.02438\n.\nCited by:\nÂ§1\n,\nÂ§2\n.\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017)\nProximal policy optimization algorithms\n.\narXiv preprint arXiv:1707.06347\n.\nCited by:\nÂ§1\n,\nÂ§1\n,\nÂ§2\n,\nÂ§3.1\n,\nÂ§6\n.\nT. Seyde, I. Gilitschenski, W. Schwarting, B. Stellato, M. Riedmiller, M. Wulfmeier, and D. Rus (2021)\nIs bang-bang control all you need? solving continuous control with bernoulli policies\n.\nAdvances in Neural Information Processing Systems\n34\n,\npp.Â 27209â€“27221\n.\nCited by:\nÂ§2\n.\nG. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu (2025)\nHybridFlow: a flexible and efficient rlhf framework\n.\nIn\nProceedings of the Twentieth European Conference on Computer Systems\n,\nEuroSys â€™25\n,\nNew York, NY, USA\n,\npp.Â 1279â€“1297\n.\nExternal Links:\nISBN 9798400711961\nCited by:\nÂ§1\n.\nR. S. Sutton, A. G. Barto,\net al.\n(1998)\nReinforcement learning: an introduction\n.\nVol.\n1\n,\nMIT press Cambridge\n.\nCited by:\nÂ§1\n.\nR. S. Sutton, D. McAllester, S. Singh, and Y. Mansour (1999)\nPolicy gradient methods for reinforcement learning with function approximation\n.\nAdvances in neural information processing systems\n12\n.\nCited by:\nÂ§2\n.\nY. Tang and S. Agrawal (2020)\nDiscretizing continuous action space for on-policy optimization\n.\nIn\nProceedings of the aaai conference on artificial intelligence\n,\nVol.\n34\n,\npp.Â 5981â€“5988\n.\nCited by:\nÂ§1\n,\nÂ§2\n,\nÂ§3.2\n.\nS. Tao, F. Xiang, A. Shukla, Y. Qin, X. Hinrichsen, X. Yuan, C. Bao, X. Lin, Y. Liu, T. Chan, Y. Gao, X. Li, T. Mu, N. Xiao, A. Gurha, V. N. Rajesh, Y. W. Choi, Y. Chen, Z. Huang, R. Calandra, R. Chen, S. Luo, and H. Su (2025)\nManiSkill3: gpu parallelized robotics simulation and rendering for generalizable embodied ai\n.\nRobotics: Science and Systems\n.\nCited by:\nÂ§B.1\n,\nÂ§B.2\n,\nÂ§6\n,\nÂ§6\n.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, and I. Polosukhin (2017)\nAttention is all you need\n.\nAdvances in neural information processing systems\n30\n.\nCited by:\nÂ§5\n,\nÂ§5\n.\nT. Wang, R. Zhang, and S. Gao (2025)\nImproving value estimation critically enhances vanilla policy gradient\n.\nIn\nForty-second International Conference on Machine Learning\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nR. J. Williams (1992)\nSimple statistical gradient-following algorithms for connectionist reinforcement learning\n.\nMachine learning\n8\n(\n3\n),\npp.Â 229â€“256\n.\nCited by:\nÂ§1\n,\nÂ§2\n.\nY. Wu, E. Mansimov, R. B. Grosse, S. Liao, and J. Ba (2017)\nScalable trust-region method for deep reinforcement learning using kronecker-factored approximation\n.\nAdvances in neural information processing systems\n30\n.\nCited by:\nÂ§2\n.\nZ. Xie, Q. Zhang, F. Yang, M. Hutter, and R. Xu (2025)\nSimple policy optimization\n.\nIn\nForty-second International Conference on Machine Learning\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nR. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu (2020)\nOn layer normalization in the transformer architecture\n.\nIn\nInternational conference on machine learning\n,\npp.Â 10524â€“10533\n.\nCited by:\nÂ§5\n.\nQ. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu, H. Lin, Z. Lin, B. Ma, G. Sheng, Y. Tong, C. Zhang, M. Zhang, W. Zhang, H. Zhu, J. Zhu, J. Chen, J. Chen, C. Wang, H. Yu, W. Dai, Y. Song, X. Wei, H. Zhou, J. Liu, W. Ma, Y. Zhang, L. Yan, M. Qiao, Y. Wu, and M. Wang (2025)\nDAPO: an open-source llm reinforcement learning system at scale\n.\nExternal Links:\n2503.14476\n,\nLink\nCited by:\nÂ§1\n.\nY. Yue, Z. Chen, R. Lu, A. Zhao, Z. Wang, Y. Yue, S. Song, and G. Huang (2025)\nDoes reinforcement learning really incentivize reasoning capacity in LLMs beyond the base model?\n.\nIn\nThe Thirty-ninth Annual Conference on Neural Information Processing Systems\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nY. Zhu, Z. Wang, Y. Zhu, C. Chen, and D. Zhao (2024)\nDiscretizing continuous action space with unimodal probability distributions for on-policy reinforcement learning\n.\nIEEE Transactions on Neural Networks and Learning Systems\n.\nCited by:\nÂ§1\n,\nÂ§2\n,\nÂ§3.2\n,\nÂ§4.1\n.\nAppendix A\nProof for Proposition\n4.1\nProof.\nFix a state\ns\ns\nand write\ng\n^\nâ€‹\n(\nÎ¸\n)\n=\nR\nâ€‹\nâˆ‡\nÎ¸\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\nâˆ£\ns\n)\n\\hat{g}(\\theta)=R\\,\\nabla_{\\theta}\\log\\pi_{\\theta}(a\\mid s)\nwith\na\nâˆ¼\nÏ€\nÎ¸\n(\nâ‹…\nâˆ£\ns\n)\na\\sim\\pi_{\\theta}(\\cdot\\mid s)\nand constant\nR\nR\n.\nIn both cases below we use the standard identity\nğ”¼\nâ€‹\n[\nâˆ‡\nÎ¸\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\nâˆ£\ns\n)\nâˆ£\ns\n]\n=\nâˆ‡\nÎ¸\nâ€‹\nâˆ«\nÏ€\nÎ¸\nâ€‹\n(\na\nâˆ£\ns\n)\nâ€‹\nğ‘‘\na\n=\nâˆ‡\nÎ¸\n1\n=\n0\n,\n\\mathbb{E}\\big[\\nabla_{\\theta}\\log\\pi_{\\theta}(a\\mid s)\\mid s\\big]=\\nabla_{\\theta}\\int\\pi_{\\theta}(a\\mid s)\\,da=\\nabla_{\\theta}1=0,\n(16)\nso\nğ”¼\nâ€‹\n[\ng\n^\nâˆ£\ns\n]\n=\n0\n\\mathbb{E}[\\hat{g}\\mid s]=0\nand therefore the conditional covariance satisfies\nCov\nâ€‹\n(\ng\n^\nâˆ£\ns\n)\n=\nğ”¼\nâ€‹\n[\ng\n^\nâ€‹\ng\n^\nâŠ¤\nâˆ£\ns\n]\n\\mathrm{Cov}(\\hat{g}\\mid s)=\\mathbb{E}[\\hat{g}\\hat{g}^{\\top}\\mid s]\nand, in particular,\nğ”¼\nâ€‹\n[\nâ€–\ng\n^\nâ€–\n2\n2\nâˆ£\ns\n]\n=\nTr\nâ€‹\n(\nCov\nâ€‹\n(\ng\n^\nâˆ£\ns\n)\n)\n\\mathbb{E}[\\|\\hat{g}\\|_{2}^{2}\\mid s]=\\mathrm{Tr}(\\mathrm{Cov}(\\hat{g}\\mid s))\n.\n[1] Gaussian policy (gradient w.r.t. mean).\nLet\nÏ€\nc\nâ€‹\n(\na\nâˆ£\ns\n)\n=\nğ’©\nâ€‹\n(\nÎ¼\n,\nÎ£\n)\n\\pi^{c}(a\\mid s)=\\mathcal{N}(\\mu,\\Sigma)\nwith fixed diagonal\nÎ£\n=\nDiag\nâ€‹\n(\nÏƒ\n2\n)\n\\Sigma=\\mathrm{Diag}(\\sigma^{2})\nand parameter\nÎ¼\nâˆˆ\nâ„\nm\n\\mu\\in\\mathbb{R}^{m}\n.\nThe log-density is\nlog\nâ¡\nÏ€\nc\nâ€‹\n(\na\nâˆ£\ns\n)\n=\nâˆ’\n1\n2\nâ€‹\n(\na\nâˆ’\nÎ¼\n)\nâŠ¤\nâ€‹\nÎ£\nâˆ’\n1\nâ€‹\n(\na\nâˆ’\nÎ¼\n)\n+\nC\n,\n\\log\\pi^{c}(a\\mid s)=-\\tfrac{1}{2}(a-\\mu)^{\\top}\\Sigma^{-1}(a-\\mu)+C,\nhence\nâˆ‡\nÎ¼\nlog\nâ¡\nÏ€\nc\nâ€‹\n(\na\nâˆ£\ns\n)\n=\nÎ£\nâˆ’\n1\nâ€‹\n(\na\nâˆ’\nÎ¼\n)\n.\n\\nabla_{\\mu}\\log\\pi^{c}(a\\mid s)=\\Sigma^{-1}(a-\\mu).\n(17)\nTherefore\ng\n^\nÎ¼\n=\nR\nâ€‹\nÎ£\nâˆ’\n1\nâ€‹\n(\na\nâˆ’\nÎ¼\n)\n\\hat{g}_{\\mu}=R\\,\\Sigma^{-1}(a-\\mu)\nand\nğ”¼\nâ€‹\n[\nâ€–\ng\n^\nÎ¼\nâ€–\n2\n2\nâˆ£\ns\n]\n\\displaystyle\\mathbb{E}\\!\\big[\\|\\hat{g}_{\\mu}\\|_{2}^{2}\\mid s\\big]\n=\nR\n2\nâ€‹\nğ”¼\nâ€‹\n[\n(\na\nâˆ’\nÎ¼\n)\nâŠ¤\nâ€‹\nÎ£\nâˆ’\n2\nâ€‹\n(\na\nâˆ’\nÎ¼\n)\nâˆ£\ns\n]\n\\displaystyle=R^{2}\\,\\mathbb{E}\\!\\big[(a-\\mu)^{\\top}\\Sigma^{-2}(a-\\mu)\\mid s\\big]\n=\nR\n2\nâ€‹\nTr\nâ€‹\n(\nÎ£\nâˆ’\n2\nâ€‹\nğ”¼\nâ€‹\n[\n(\na\nâˆ’\nÎ¼\n)\nâ€‹\n(\na\nâˆ’\nÎ¼\n)\nâŠ¤\nâˆ£\ns\n]\n)\n\\displaystyle=R^{2}\\,\\mathrm{Tr}\\!\\Big(\\Sigma^{-2}\\,\\mathbb{E}\\big[(a-\\mu)(a-\\mu)^{\\top}\\mid s\\big]\\Big)\n=\nR\n2\nâ€‹\nTr\nâ€‹\n(\nÎ£\nâˆ’\n2\nâ€‹\nÎ£\n)\n=\nR\n2\nâ€‹\nTr\nâ€‹\n(\nÎ£\nâˆ’\n1\n)\n.\n\\displaystyle=R^{2}\\,\\mathrm{Tr}\\!\\big(\\Sigma^{-2}\\Sigma\\big)=R^{2}\\,\\mathrm{Tr}(\\Sigma^{-1}).\nFor diagonal\nÎ£\n=\nDiag\nâ€‹\n(\nÏƒ\n2\n)\n\\Sigma=\\mathrm{Diag}(\\sigma^{2})\n,\nTr\nâ€‹\n(\nÎ£\nâˆ’\n1\n)\n=\nâˆ‘\ni\n=\n1\nm\nÏƒ\ni\nâˆ’\n2\n\\mathrm{Tr}(\\Sigma^{-1})=\\sum_{i=1}^{m}\\sigma_{i}^{-2}\n, yieldingÂ (\n11\n).\n[2] Categorical policy (gradient w.r.t. logits).\nConsider one action dimension\ni\ni\nwith logits\nz\ni\nâˆˆ\nâ„\nK\nz_{i}\\in\\mathbb{R}^{K}\nand softmax probabilities\np\ni\n=\nsoftmax\nâ€‹\n(\nz\ni\n)\nâˆˆ\nÎ”\nK\nâˆ’\n1\np_{i}=\\mathrm{softmax}(z_{i})\\in\\Delta^{K-1}\n.\nLet\nj\ni\nâˆ¼\nCat\nâ€‹\n(\np\ni\n)\nj_{i}\\sim\\mathrm{Cat}(p_{i})\nand denote the sampled one-hot vector by\ne\nj\ni\nâˆˆ\nâ„\nK\ne_{j_{i}}\\in\\mathbb{R}^{K}\n.\nFor the log-probability\nlog\nâ¡\nÏ€\nd\nâ€‹\n(\na\nj\ni\ni\nâˆ£\ns\n)\n=\nlog\nâ¡\np\ni\n,\nj\ni\n\\log\\pi^{d}(a^{i}_{j_{i}}\\mid s)=\\log p_{i,j_{i}}\n, the softmax score w.r.t. logits is\nâˆ‡\nz\ni\nlog\nâ¡\np\ni\n,\nj\ni\n=\ne\nj\ni\nâˆ’\np\ni\n.\n\\nabla_{z_{i}}\\log p_{i,j_{i}}\\;=\\;e_{j_{i}}-p_{i}.\n(18)\nHence the per-dimension REINFORCE gradient is\ng\n^\nz\ni\n=\nR\nâ€‹\n(\ne\nj\ni\nâˆ’\np\ni\n)\n\\hat{g}_{z_{i}}=R\\,(e_{j_{i}}-p_{i})\n, and its conditional second moment is\nğ”¼\nâ€‹\n[\nâ€–\ng\n^\nz\ni\nâ€–\n2\n2\nâˆ£\ns\n]\n\\displaystyle\\mathbb{E}\\!\\big[\\|\\hat{g}_{z_{i}}\\|_{2}^{2}\\mid s\\big]\n=\nR\n2\nâ€‹\nğ”¼\nâ€‹\n[\nâ€–\ne\nj\ni\nâˆ’\np\ni\nâ€–\n2\n2\nâˆ£\ns\n]\n\\displaystyle=R^{2}\\,\\mathbb{E}\\!\\big[\\|e_{j_{i}}-p_{i}\\|_{2}^{2}\\mid s\\big]\n=\nR\n2\nâ€‹\nğ”¼\nâ€‹\n[\nâ€–\ne\nj\ni\nâ€–\n2\n2\n+\nâ€–\np\ni\nâ€–\n2\n2\nâˆ’\n2\nâ€‹\ne\nj\ni\nâŠ¤\nâ€‹\np\ni\nâˆ£\ns\n]\n\\displaystyle=R^{2}\\,\\mathbb{E}\\!\\big[\\|e_{j_{i}}\\|_{2}^{2}+\\|p_{i}\\|_{2}^{2}-2\\,e_{j_{i}}^{\\top}p_{i}\\mid s\\big]\n=\nR\n2\nâ€‹\n(\n1\n+\nâ€–\np\ni\nâ€–\n2\n2\nâˆ’\n2\nâ€‹\nğ”¼\nâ€‹\n[\np\ni\n,\nj\ni\nâˆ£\ns\n]\n)\n\\displaystyle=R^{2}\\,\\Big(1+\\|p_{i}\\|_{2}^{2}-2\\,\\mathbb{E}[p_{i,j_{i}}\\mid s]\\Big)\n=\nR\n2\nâ€‹\n(\n1\n+\nâ€–\np\ni\nâ€–\n2\n2\nâˆ’\n2\nâ€‹\nâˆ‘\nk\n=\n1\nK\np\ni\n,\nk\n2\n)\n=\nR\n2\nâ€‹\n(\n1\nâˆ’\nâ€–\np\ni\nâ€–\n2\n2\n)\n.\n\\displaystyle=R^{2}\\,\\Big(1+\\|p_{i}\\|_{2}^{2}-2\\sum_{k=1}^{K}p_{i,k}^{2}\\Big)=R^{2}\\,(1-\\|p_{i}\\|_{2}^{2}).\nNow assume the\nm\nm\naction dimensions factorize:\nÏ€\nd\nâ€‹\n(\na\nâˆ£\ns\n)\n=\nâˆ\ni\n=\n1\nm\nÏ€\nd\nâ€‹\n(\na\nj\ni\ni\nâˆ£\ns\n)\n\\pi^{d}(a\\mid s)=\\prod_{i=1}^{m}\\pi^{d}(a^{i}_{j_{i}}\\mid s)\n, so the full log-probability is the sum across\ni\ni\nand\nthe full gradient w.r.t.\nz\n=\n[\nz\n1\n;\nâ€¦\n;\nz\nm\n]\nz=[z_{1};\\dots;z_{m}]\nis the concatenation of per-dimension gradients:\ng\n^\nz\n=\n[\ng\n^\nz\n1\n;\nâ€¦\n;\ng\n^\nz\nm\n]\n\\hat{g}_{z}=[\\hat{g}_{z_{1}};\\dots;\\hat{g}_{z_{m}}]\n.\nTherefore\nâ€–\ng\n^\nz\nâ€–\n2\n2\n=\nâˆ‘\ni\n=\n1\nm\nâ€–\ng\n^\nz\ni\nâ€–\n2\n2\n\\|\\hat{g}_{z}\\|_{2}^{2}=\\sum_{i=1}^{m}\\|\\hat{g}_{z_{i}}\\|_{2}^{2}\n, and\nğ”¼\nâ€‹\n[\nâ€–\ng\n^\nz\nâ€–\n2\n2\nâˆ£\ns\n]\n=\nâˆ‘\ni\n=\n1\nm\nğ”¼\nâ€‹\n[\nâ€–\ng\n^\nz\ni\nâ€–\n2\n2\nâˆ£\ns\n]\n=\nR\n2\nâ€‹\nâˆ‘\ni\n=\n1\nm\n(\n1\nâˆ’\nâ€–\np\ni\nâ€‹\n(\ns\n)\nâ€–\n2\n2\n)\n,\n\\mathbb{E}\\!\\left[\\|\\hat{g}_{z}\\|_{2}^{2}\\mid s\\right]=\\sum_{i=1}^{m}\\mathbb{E}\\!\\big[\\|\\hat{g}_{z_{i}}\\|_{2}^{2}\\mid s\\big]=R^{2}\\sum_{i=1}^{m}\\big(1-\\|p_{i}(s)\\|_{2}^{2}\\big),\n(19)\nwhich proves the first equality inÂ (\n12\n).\nFor the upper bound, for any\np\nâˆˆ\nÎ”\nK\nâˆ’\n1\np\\in\\Delta^{K-1}\n,\nCauchyâ€“Schwarz gives\nâ€–\np\nâ€–\n2\n2\nâ‰¥\n1\nK\nâ€‹\n(\nâ€–\np\nâ€–\n1\n)\n2\n=\n1\nK\n\\|p\\|_{2}^{2}\\geq\\frac{1}{K}(\\|p\\|_{1})^{2}=\\frac{1}{K}\n, hence\n1\nâˆ’\nâ€–\np\nâ€–\n2\n2\nâ‰¤\n1\nâˆ’\n1\nK\n1-\\|p\\|_{2}^{2}\\leq 1-\\frac{1}{K}\n.\nApplying this to each\np\ni\nâ€‹\n(\ns\n)\np_{i}(s)\nyields\nğ”¼\nâ€‹\n[\nâ€–\ng\n^\nz\nâ€–\n2\n2\nâˆ£\ns\n]\nâ‰¤\nR\n2\nâ€‹\nâˆ‘\ni\n=\n1\nm\n(\n1\nâˆ’\n1\nK\n)\n=\nm\nâ€‹\nR\n2\nâ€‹\n(\n1\nâˆ’\n1\nK\n)\n.\n\\mathbb{E}\\!\\left[\\|\\hat{g}_{z}\\|_{2}^{2}\\mid s\\right]\\leq R^{2}\\sum_{i=1}^{m}\\Big(1-\\frac{1}{K}\\Big)=mR^{2}\\Big(1-\\frac{1}{K}\\Big).\nMoreover,\nâ€–\np\nâ€–\n2\n2\n=\n1\n/\nK\n\\|p\\|_{2}^{2}=1/K\nholds if and only if\np\np\nis uniform over the\nK\nK\nbins, so the inequality is tight\nif and only if\np\ni\nâ€‹\n(\ns\n)\np_{i}(s)\nis uniform for all\ni\ni\n.\nâˆ\nAppendix B\nExperimental Details\nB.1\nHyperparameters\nOur implementation is based on the\nclean_rl\nPPO codebase\n(Huang\net al.\n,\n2022b\n)\nand the ManiSkill benchmark suite\n(Tao\net al.\n,\n2025\n)\n. Unless otherwise specified, we adopt the default hyperparameters from these codebases without task-specific retuning. For completeness, we report the full set of default parameters in Table\n1\n. For RGB-based tasks, we follow the benchmark suite and use a NatureCNN feature extractor to encode observations into a latent representation, whose dimensionality is given by the extractorâ€™s output feature size.\nTable 1:\nDefault PPO hyperparameters.\nWe report the default configurations used for each benchmark family.\nCategory\nHyperparameter\nGym\nManiSkill (state)\nManiSkill (RGB)\nCommon\nDiscount factor\nÎ³\n\\gamma\n0.99\n0.80\n0.80\nGAE parameter\nÎ»\n\\lambda\n0.95\n0.90\n0.90\nPPO clipping\nÏµ\n\\epsilon\n0.2\n0.2\n0.2\nValue loss coefficient\n0.5\n0.5\n0.5\nEntropy coefficient\n0\n0\n0\nMax grad norm\n0.5\n0.5\n0.5\nNum epochs per update\n10\n4\n8\nNum minibatches\n64\n32\n32\nAdvantage normalization\nTrue\nTrue\nTrue\nNum envs\n16\n4096\n1024\nNum steps\n1024\n50\n50\nTotal timesteps\n5M\n50M\n50M\nOptimization\nOptimizer\nAdam\nAdam\nAdam\nLearning rate\n3\nÃ—\n10\nâˆ’\n4\n3\\times 10^{-4}\n3\nÃ—\n10\nâˆ’\n4\n3\\times 10^{-4}\n3\nÃ—\n10\nâˆ’\n4\n3\\times 10^{-4}\nWeight decay\n1\nÃ—\n10\nâˆ’\n5\n1\\times 10^{-5}\n1\nÃ—\n10\nâˆ’\n5\n1\\times 10^{-5}\n1\nÃ—\n10\nâˆ’\n5\n1\\times 10^{-5}\nCritic Network\nHidden dim\n64\n256\n512\nActivation function\nTanh\nTanh\nReLU\nNumber of Hidden Layers\n1\n2\n1\nActor Network (MLP)\nHidden dim\n256\n256\n512\nActivation function\nTanh\nTanh\nReLU\nNumber of Hidden Layers\n2\n2\n1\nActor Network (RN)\nHidden dim\nd\nh\nd_{h}\n256\n128\n512\nNumber of Blocks\nN\nN\n2\n2\n1\nB.2\nEnvironments\nAll experiments use the standard benchmark implementations from Gymnasium/MuJoCo and ManiSkill. Unless otherwise specified, we adopt the default environment settings provided by the corresponding libraries and report results as the mean over five random seeds. For the main results, Gymnasium/MuJoCo experiments are run on an NVIDIA H800, while ManiSkill experiments are run on 2Ã— NVIDIA GeForce RTX 5090 GPUs.\nGym â€“ Locomotion.\nWe evaluate on five widely-used MuJoCo locomotion tasks from Gymnasium (\n-v4\nversions):\nHalfCheetah-v4\n,\nAnt-v4\n,\nHopper-v4\n,\nHumanoid-v4\n, and\nWalker2d-v4\n.\nWe follow standard practice and do not apply additional preprocessing beyond the default observation and\naction interfaces of the environments.\nWhen aggregating scores across tasks, we report\nnormalized return\nusing TD3-normalization,\nTD3\nâ€‹\n-\nâ€‹\nNormalized\nâ€‹\n(\nx\n)\n:=\nx\nâˆ’\nx\nrandom\nx\nTD3\nâˆ’\nx\nrandom\n,\n\\mathrm{TD3\\text{-}Normalized}(x):=\\frac{x-x_{\\mathrm{random}}}{x_{\\mathrm{TD3}}-x_{\\mathrm{random}}},\n(20)\nwhere\nx\nx\ndenotes the raw episodic return, and\n(\nx\nrandom\n,\nx\nTD3\n)\n(x_{\\mathrm{random}},x_{\\mathrm{TD3}})\nare the random-policy\nand TD3 reference scores, respectively (Table\n2\n).\nTable 2:\nTD3-normalization reference scores\nused for Gym locomotion aggregation.\nEnvironment\nRandom\nTD3\nAnt-v4\n-70.288\n3942\nHalfCheetah-v4\n-289.415\n10574\nHopper-v4\n18.791\n3226\nHumanoid-v4\n120.423\n5165\nWalker2d-v4\n2.791\n3946\nManiSkill (state).\nWe additionally benchmark on state-based manipulation tasks from ManiSkill\n(Tao\net al.\n,\n2025\n)\n.\nEach environment provides low-dimensional proprioceptive/state observations, and we evaluate performance\nusing the standard ManiSkill success metric (success rate).\nWe use the default task configurations and consider 10 tasks.\nAcross tasks, we keep the default algorithmic hyperparameters fixed and only vary throughput-related\nsettings (e.g., number of parallel environments and rollout length). All runs use a total budget of\n50M environment steps per task.\nManiSkill (RGB).\nFor vision-based control, we evaluate on five ManiSkill tasks with RGB observations.\nWe use the default RGB observation interface provided by ManiSkill and report success rate.\nSimilar to the state-based setting, we keep the algorithmic hyperparameters fixed and only adjust\nthroughput-related settings for stable training. All runs use 50M environment steps per task.\nAppendix C\nExtended Study\nFigure 7:\nExtended studies on sample efficiency, optimization objective, and scaling.\nC.1\nAdditional Comparisons with State-of-the-Art Methods\nTo complement our main evaluation, we compare against two recent state-of-the-art continuous-control methods: TD3\n(Fujimoto\net al.\n,\n2018\n)\nand TD-MPC2\n(Hansen\net al.\n,\n2024\n)\n.\nSince these methods differ in their interaction patterns and training pipelines, we report wall-clock time curves.\nOur method and TD3 are run on the same hardware configuration (2\nÃ—\n\\times\nRTX 5090 GPUs), while TD-MPC2 is run on an NVIDIA A200 GPU, which is typically faster, and this makes the\nwall-clock comparison conservative with respect to TD-MPC2.\nWe use the authorsâ€™ published packages/official implementations for TD3 and TD-MPC2, and follow their standard training protocols. TD-MPC2 are evaluated on the same tasks with 3 random seeds\n(Hansen\net al.\n,\n2024\n)\n.\nFigure\n7\n(a) compares wall-clock learning curves on Humanoid-v4 and StackCube-v1.\nOn Humanoid-v4, TD3 improves quickly early on, reaching high normalized return within a short time window, whereas TD-MPC2 makes little progress under the same horizon. Our approach is on-policy and built on PPO, it achieves substantial gains and outperforms standard PPO baselines, continuing to improve with more interaction.\nOn StackCube-v1, our method rapidly reaches high success rates, while TD3 fails to learn and TD-MPC2 only improves much later in training. Overall, these results highlight that the proposed on-policy discretized actor is particularly effective for challenging manipulation tasks where exploration and optimization are difficult, and that it remains competitive in locomotion while offering favorable wall-clock efficiency.\nC.2\nAblation study on the role of cross-entropy-like optimization\nDiscretized categorical policies differ from Gaussian policies in two tightly coupled aspects:\n(i) the discretized action representation induced by binning, and\n(ii) the likelihood model used for optimization, which yields a cross-entropy-like objective rather than a squared-error objective.\nTo disentangle these effects, we introduce a\nloss-swap\nablation that isolates the role of the optimization objective while holding the action representation space fixed.\nStarting from our discrete categorical policy, we modify only the likelihood used for policy optimization.\nThe actor still outputs logits over\nK\nK\nbins for each action dimension, inducing probabilities\np\nÎ¸\n,\ni\nâ€‹\n(\nk\nâˆ£\ns\n)\n=\nsoftmax\nâ€‹\n(\nz\nÎ¸\n,\ni\nâ€‹\n(\ns\n)\n)\nk\np_{\\theta,i}(k\\mid s)=\\mathrm{softmax}(z_{\\theta,i}(s))_{k}\n.\nInstead of sampling from the categorical distribution, we compute the expected action as the probability-weighted\naverage of bin centers\n{\nc\nk\n}\nk\n=\n1\nK\n\\{c_{k}\\}_{k=1}^{K}\n,\nÎ¼\nÎ¸\n,\ni\nâ€‹\n(\ns\n)\n=\nâˆ‘\nk\n=\n1\nK\np\nÎ¸\n,\ni\nâ€‹\n(\nk\nâˆ£\ns\n)\nâ€‹\nc\nk\n,\n\\mu_{\\theta,i}(s)\\;=\\;\\sum_{k=1}^{K}p_{\\theta,i}(k\\mid s)\\,c_{k},\n(21)\nand treat\nÎ¼\nÎ¸\nâ€‹\n(\ns\n)\n\\mu_{\\theta}(s)\nas the mean of a Gaussian policy,\nÏ€\nÎ¸\nswap\nâ€‹\n(\na\nâˆ£\ns\n)\n=\nğ’©\nâ€‹\n(\na\n;\nÎ¼\nÎ¸\nâ€‹\n(\ns\n)\n,\nÎ£\n)\n.\n\\pi^{\\text{swap}}_{\\theta}(a\\mid s)\\;=\\;\\mathcal{N}\\!\\big(a;\\mu_{\\theta}(s),\\Sigma\\big).\n(22)\nPolicy updates are then performed using the standard Gaussian log-likelihood,\nlog\nâ¡\nÏ€\nÎ¸\nswap\nâ€‹\n(\na\nâˆ£\ns\n)\n=\nâˆ’\n1\n2\nâ€‹\n(\na\nâˆ’\nÎ¼\nÎ¸\nâ€‹\n(\ns\n)\n)\nâŠ¤\nâ€‹\nÎ£\nâˆ’\n1\nâ€‹\n(\na\nâˆ’\nÎ¼\nÎ¸\nâ€‹\n(\ns\n)\n)\n+\nconst\n,\n\\log\\pi^{\\text{swap}}_{\\theta}(a\\mid s)=-\\tfrac{1}{2}(a-\\mu_{\\theta}(s))^{\\top}\\Sigma^{-1}(a-\\mu_{\\theta}(s))+\\mathrm{const},\n(23)\nwhich is equivalent to a weighted squared-error objective under fixed\nÎ£\n\\Sigma\n.\nThis loss-swapped variant uses the same network architecture and discretized action representation as the\noriginal categorical policy, differing only in the likelihood (and thus the optimization loss) used for policy updates.\nFigure\n7\n(b) compares RN-D, the loss-swapped variant, and a continuous Gaussian baseline (RN-C).\nReplacing the categorical likelihood with a Gaussian likelihood leads to a substantial performance drop, with the loss-swapped variant closely matching the continuous Gaussian baseline.\nThese results show that discretizing the action space alone is insufficient to explain the observed gains.\nEven with a discretized representation, switching from a cross-entropy-like objective to a squared-error objective eliminates most of the performance improvement, highlighting the central role of the categorical likelihood and its induced optimization geometry.\nC.3\nAblation study on the capacity of the actor network\nTo study how actor capacity affects on-policy learning with discretized categorical policies, we scale the actor network by varying the hidden width\nd\nh\nâˆˆ\n{\n64\n,\n128\n,\n256\n,\n512\n}\nd_{h}\\in\\{64,128,256,512\\}\n. Note that the maximum hidden dimension could be 4\nd\nh\nd_{h}\n.\nWe keep the rest of the training pipeline fixed, including the PPO objective, the critic architecture, and all other\nhyperparameters, so that differences can be attributed to actor capacity.\nFigure\n7\n(c) shows that scaling actor width generally improves optimization speed on both tasks.\nOn Humanoid-v4, wider actors learn faster and attain higher returns during much of training, with the largest gains when increasing from\nd\nh\n=\n64\nd_{h}=64\nto\nd\nh\nâˆˆ\n{\n128\n,\n256\n}\nd_{h}\\in\\{128,256\\}\n. However, the benefits do not always translate to higher asymptotic performance, and the widest setting (\nd\nh\n=\n512\nd_{h}=512\n) can yield similar or even slightly lower final returns.\nOn StackCube-v1, larger actors typically reach high success earlier, while final success rates are comparable.\nOverall, actor capacity is an effective lever for accelerating on-policy learning, but its impact on final performance can be non-monotonic.\nC.4\nAblation study on the discrete bins\nAnother factor affecting the performance of discrete policies is the number of action bins, which controls the granularity of the discretized action space. Fewer bins lead to a coarser representation, while more bins provide finer resolution and more closely approximate continuous control. To examine this effect, we sweep the number of action bins from 21 to 1001, with 41 bins used in the main experiments.\nFigure\n7\nshows how policy performance varies with the number of action bins. For MLP-based discrete policies without residual structure (MLP-D), performance generally degrades as the number of bins increases, but the degradation is relatively mild. When the number of bins is smaller than the hidden dimension, MLP-D performs better than or is at least comparable to the continuous baseline (MLP-C). On StackCube-v1, the success rate decreases almost linearly with the logarithm of the number of action bins (the x-axis is log-scaled), even though the size of the factorized action space grows linearly with the number of bins. This indicates that the performance gains of discrete policies are not solely due to finer action resolution, but are also influenced by optimization behavior.\nFor RN-D methods, discrete policies achieve strong performance with a small number of bins and consistently outperform the off-policy TD3 baseline on Humanoid. As the number of bins increases, the mean return decreases. On Humanoid, although the average performance drops, the variance across five random seeds remains relatively stable when increasing the number of bins from 11 to 101. A similar trend appears on StackCube-v1, where performance is largely preserved for small to moderate bin counts, followed by a sharp decline once the number of bins exceeds 100.\nOverall, these results show that discrete policy performance is sensitive to discretization granularity. With relatively few bins, the optimization advantages introduced by discretizationâ€”such as a simpler optimization landscapeâ€”outweigh the loss in action resolution. However, as the number of bins becomes large, the policy must predict increasingly fine-grained action distributions, which likely exceeds the effective representational or optimization capacity of the network, leading to the observed performance degradation. A promising way to mitigate this issue is to introduce more efficient action tokenization schemes, which aim to preserve high action precision while retaining the optimization advantages of discrete policies.\nAppendix D\nComplete main results\nHere we provide the complete learning curves for all evaluated tasks in Figure\n8\n. The figure includes per-task performance over environment stepsâ€”normalized return for MuJoCo benchmarks and success rate for ManiSkill manipulation benchmarks, including both state-based and RGB-based settings where applicable. Curves show the mean across five random seeds with shaded regions indicating variability, and are included here for completeness and transparency beyond the aggregated main-text results.\nFigure 8:\nComplete learning curves across all tasks. We report normalized return on MuJoCo benchmarks (top row) and success rate on ManiSkill manipulation benchmarks (remaining rows), including both state-based and RGB-based variants. Curves are averaged over 5 random\nseeds; shaded regions denote 95% stratified bootstrap confidence intervals.",
    "preview_text": "On-policy deep reinforcement learning remains a dominant paradigm for continuous control, yet standard implementations rely on Gaussian actors and relatively shallow MLP policies, often leading to brittle optimization when gradients are noisy and policy updates must be conservative. In this paper, we revisit policy representation as a first-class design choice for on-policy optimization. We study discretized categorical actors that represent each action dimension with a distribution over bins, yielding a policy objective that resembles a cross-entropy loss. Building on architectural advances from supervised learning, we further propose regularized actor networks, while keeping critic design fixed. Our results show that simply replacing the standard actor network with our discretized regularized actor yields consistent gains and achieve the state-of-the-art performance across diverse continuous-control benchmarks.\n\nRN-D: Discretized Categorical Actors with Regularized Networks\nfor On-Policy Reinforcement Learning\nYuexin Bian\nâˆ—\nJie Feng\nâˆ—\nTao Wang\nYijiang Li\nSicun Gao\nYuanyuan Shi\nAbstract\nOn-policy deep reinforcement learning remains a dominant paradigm for continuous control, yet standard implementations rely on Gaussian actors and relatively shallow MLP policies, often leading to brittle optimization when gradients are noisy and policy updates must be conservative. In this paper, we revisit policy representation as a first-class design choice for on-policy optimization. We study discretized categorical actors that represent each action dimension with a distribution over bins, yielding a policy objective that resembles a cross-entropy loss. Building on architectural advances from supervised learning, we further propose regularized actor networks, while keeping critic design fixed. Our results show that simply replacing the standard actor network with our discretized regularized actor yields consistent gains and achieve the state-of-the-art performance across diverse",
    "is_relevant": true,
    "relevance_score": 3.0,
    "extracted_keywords": [
        "Reinforcement Learning"
    ],
    "one_line_summary": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¦»æ•£åŒ–åˆ†ç±»è¡ŒåŠ¨è€…å’Œæ­£åˆ™åŒ–ç½‘ç»œçš„æ–°æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›åœ¨ç­–ç•¥å¼ºåŒ–å­¦ä¹ ä¸­çš„è¿ç»­æ§åˆ¶ä»»åŠ¡ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T15:24:34Z",
    "created_at": "2026-02-03T15:53:10.207327",
    "updated_at": "2026-02-03T15:53:10.207335"
}