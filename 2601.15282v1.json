{
    "id": "2601.15282v1",
    "title": "Rethinking Video Generation Model for the Embodied World",
    "authors": [
        "Yufan Deng",
        "Zilin Pan",
        "Hongyu Zhang",
        "Xiaojie Li",
        "Ruoqing Hu",
        "Yufei Ding",
        "Yiming Zou",
        "Yan Zeng",
        "Daquan Zhou"
    ],
    "abstract": "ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÊòæËëóÊé®Âä®‰∫ÜÂÖ∑Ë∫´Êô∫ËÉΩÁöÑÂèëÂ±ïÔºå‰∏∫ÁîüÊàêÊ∂µÁõñÁâ©ÁêÜ‰∏ñÁïåÊÑüÁü•„ÄÅÊé®ÁêÜ‰∏éË°åÂä®ÁöÑÂ§öÊ†∑ÂåñÊú∫Âô®‰∫∫Êï∞ÊçÆÂºÄËæü‰∫ÜÊñ∞ÂèØËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂêàÊàêËÉΩÂáÜÁ°ÆÂèçÊò†ÁúüÂÆûÊú∫Âô®‰∫∫‰∫§‰∫íÁöÑÈ´òË¥®ÈáèËßÜÈ¢ë‰ªçÂÖ∑ÊåëÊàòÊÄßÔºå‰∏îÁº∫‰πèÊ†áÂáÜÂåñÂü∫ÂáÜÈôêÂà∂‰∫ÜÂÖ¨Âπ≥ÊØîËæÉ‰∏éÁ†îÁ©∂ËøõÂ±ï„ÄÇ‰∏∫Â°´Ë°•Ëøô‰∏ÄÁ©∫ÁôΩÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜÁªºÂêàÊÄßÊú∫Âô®‰∫∫Âü∫ÂáÜRBenchÔºåÊó®Âú®ÈÄöËøá‰∫î‰∏™‰ªªÂä°È¢ÜÂüüÂíåÂõõÁßç‰∏çÂêåÂÖ∑Ë∫´ÂΩ¢ÊÄÅËØÑ‰º∞Èù¢ÂêëÊú∫Âô®‰∫∫ÁöÑËßÜÈ¢ëÁîüÊàêËÉΩÂäõ„ÄÇËØ•Âü∫ÂáÜÈÄöËøáÂèØÂ§çÁé∞ÁöÑÂ≠êÊåáÊ†áÔºàÂåÖÊã¨ÁªìÊûÑ‰∏ÄËá¥ÊÄß„ÄÅÁâ©ÁêÜÂêàÁêÜÊÄß‰∏éÂä®‰ΩúÂÆåÊï¥ÊÄßÔºâÂêåÊó∂ËØÑ‰º∞‰ªªÂä°Á∫ßÂáÜÁ°ÆÊÄß‰∏éËßÜËßâ‰øùÁúüÂ∫¶„ÄÇÂØπ25‰∏™‰ª£Ë°®ÊÄßÊ®°ÂûãÁöÑËØÑ‰º∞Êè≠Á§∫‰∫ÜÂΩìÂâçÊ®°ÂûãÂú®ÁîüÊàêÁâ©ÁêÜÁúüÂÆûÊú∫Âô®‰∫∫Ë°å‰∏∫ÊñπÈù¢ÁöÑÊòæËëó‰∏çË∂≥„ÄÇÊ≠§Â§ñÔºåËØ•Âü∫ÂáÜ‰∏é‰∫∫Á±ªËØÑ‰º∞ÁöÑÊñØÁöÆÂ∞îÊõºÁõ∏ÂÖ≥Á≥ªÊï∞Ëææ0.96ÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇÂ∞ΩÁÆ°RBench‰∏∫ËØÜÂà´Ëøô‰∫õÁº∫Èô∑Êèê‰æõ‰∫ÜÂøÖË¶ÅËßÜËßíÔºå‰ΩÜÂÆûÁé∞Áâ©ÁêÜÁúüÂÆûÊÄßÈúÄË¶ÅË∂ÖË∂äËØÑ‰º∞Â±ÇÈù¢ÔºåËß£ÂÜ≥È´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆ‰∏•ÈáçÁü≠Áº∫ÁöÑÊ†∏ÂøÉÈóÆÈ¢ò„ÄÇÂü∫‰∫éÊ≠§Ê¥ûËßÅÔºåÊàë‰ª¨ÊèêÂá∫Á≤æÁÇºÁöÑÂõõÈò∂ÊÆµÊï∞ÊçÆÊµÅÁ®ãÔºåÊûÑÂª∫Âá∫RoVid-X‚Äî‚ÄîÁõÆÂâçÊúÄÂ§ßÁöÑÂºÄÊ∫êÊú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÊï∞ÊçÆÈõÜÔºåÂåÖÂê´400‰∏á‰∏™Ê†áÊ≥®ËßÜÈ¢ëÁâáÊÆµÔºåË¶ÜÁõñÊï∞ÂçÉÁßç‰ªªÂä°ÔºåÂπ∂ÈÖçÊúâÂÖ®Èù¢ÁöÑÁâ©ÁêÜÂ±ûÊÄßÊ†áÊ≥®„ÄÇËøô‰∏ÄËØÑ‰º∞‰∏éÊï∞ÊçÆÂçèÂêåÁöÑÁîüÊÄÅÁ≥ªÁªüÔºå‰∏∫ËßÜÈ¢ëÊ®°ÂûãÁöÑ‰∏•Ë∞®ËØÑ‰º∞‰∏éËßÑÊ®°ÂåñËÆ≠ÁªÉÂ•†ÂÆö‰∫ÜÂùöÂÆûÂü∫Á°ÄÔºåÂ∞ÜÂä†ÈÄüÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩÂêëÈÄöÁî®Êô∫ËÉΩÁöÑÊºîËøõ„ÄÇ",
    "url": "https://arxiv.org/abs/2601.15282v1",
    "html_url": "https://arxiv.org/html/2601.15282v1",
    "html_content": "1]Peking University\n2]ByteDance Seed\nRethinking Video Generation Model for the Embodied World\nYufan Deng\n1,2‚àó\nZilin Pan\n1‚àó\nHongyu Zhang\n1‚àó\nXiaojie Li\n2\nRuoqing Hu\n2\nYufei Ding\n1\nYiming Zou\n1\nYan Zeng\n2\nDaquan Zhou\n1\n[\n[\nAbstract\nVideo generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world.\nHowever, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress.\nTo address this gap, we introduce a comprehensive robotics benchmark,\nRBench\n, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments.\nIt assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness.\nEvaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness.\nWhile RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in\nRoVid-X\n, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations.\nCollectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.\n\\checkdata\n[\nProject Page]\nhttps://dagroup-pku.github.io/ReVidgen.github.io/\n\\checkdata\n[\nGitHub Repo]\nhttps://github.com/DAGroup-PKU/ReVidgen/\n\\checkdata\n[\nHuggingFace Dataset]\nhttps://huggingface.co/datasets/DAGroup-PKU/RoVid-X/\nFigure 1\n:\nOverview of the comprehensive robotics benchmark and dataset for video generation.\nTop:\nWe present\nRBench\nthat includes the embodiment-based evaluation set and automated evaluation metrics. Our evaluation results on 25 video models show a high level of agreement with subjective human assessments.\nBottom:\nWe introduce a large-scale high-quality robotic dataset (\nRoVid-X\n) specifically designed for training video generation models, with data sourced from internet videos and open-source embodied videos.\n1\nIntroduction\nRecent advancements in diffusion models\n[\n44\n,\n84\n,\n79\n]\nand video generation\n[\n83\n,\n80\n,\n35\n,\n98\n,\n92\n]\nhave led to significant breakthroughs, enabling applications like video editing, multi-subject generation, and motion control\n[\n51\n,\n53\n,\n20\n,\n21\n,\n96\n]\n. These models have been extended to areas such as 3D scenes\n[\n57\n,\n82\n]\n, autonomous driving\n[\n28\n,\n104\n]\n, and world modeling\n[\n54\n,\n5\n]\n, showing strong generalization across tasks. A recent study\n[\n97\n]\nsuggests that, similar to LLMs in natural language processing, video models are evolving into unified foundation models for machine vision. Additionally, video models are being increasingly used in robot learning and action prediction\n[\n37\n,\n113\n,\n45\n,\n111\n,\n38\n,\n61\n]\n, as well as controllable simulators for synthesizing robotic video trajectories, addressing the lack of large-scale human teleoperation data\n[\n47\n,\n8\n,\n86\n]\n. These advancements highlight the growing potential of video models in the perception-reasoning-action loop of embodied agents, paving the way for more generalizable intelligent systems in the physical world.\nDespite these strides, systematic evaluation for robotic video generation remains underdeveloped. Current practices rely mostly on perceptual metrics, focusing on visual quality\n[\n46\n,\n66\n,\n42\n]\n, while existing physics-based benchmarks often lack task-specific datasets and criteria\n[\n72\n,\n36\n]\n.\nConsequently, evaluations frequently overlook critical aspects such as task completion, action-goal alignment, and physical feasibility. This leads to overly optimistic conclusions, where high scores are assigned even to videos containing unnatural movements or incomplete tasks.\nThe core challenge lies in rigorously assessing whether generated videos faithfully reproduce robotic behaviors.\nThis necessitates evaluation protocols that transcend perceptual metrics, incorporating both the physical plausibility of actions and their alignment with instructions to ensure discriminative and reproducible assessments.\nTo address this challenge, we propose\nRBench\n, a benchmark designed to evaluate the fidelity and utility of video-based world models in robotic video generation. To the best of our knowledge, it is the first comprehensive benchmark with fine-grained metrics for robotic video generation, consisting of 650 image‚Äìtext pairs across five task categories and four robot types. Evaluations are based on two dimensions:\ntask completion\nand\nvisual quality\n, incorporating sub-metrics like structural consistency, physical plausibility, and execution completeness.\nBased on RBench, we conduct qualitative and quantitative assessments of 25 representative models. The results highlight that general video foundation models still have significant room for improvement in physical robot video generation, revealing a persistent gap between these models and the requirements of embodied robotic tasks. This underscores the need for systematic advancements in both robotic video data and training methodologies.\nAdvancing general robotic video generation with human-like capabilities and adaptability requires diverse, scalable, and comprehensive training data\n[\n78\n,\n12\n]\n. However, unlike computer vision and natural language processing, which can leverage vast web-scale datasets, robotic interaction data has long been constrained by both scale and diversity\n[\n10\n,\n110\n,\n26\n]\n. Even the largest existing collections are smaller and less varied than those for vision or language. More critically, many datasets have narrow distributions along key axes such as environment, object set, task spectrum, and robot morphology\n[\n105\n,\n95\n]\n, often confined to specific robot types, low-resolution recordings, or limited task ranges, which hampers the generalization of video foundation models. To address these gaps, we integrate over 20 open-source datasets and multi-source video platforms, creating a four-stage end-to-end data pipeline. The stages include robot video collection, video quality filtering, task segmentation and captioning, and physical property annotation, resulting in\nRoVid-X\n, a large-scale, high-quality embodied robotic video dataset (see Table\n1\n). To our knowledge, RoVid-X is currently the largest dataset specifically designed for embodied video generation models, covering a broad range of robot morphologies and task types. It aims to enhance video foundation models with physical interaction priors and task semantic diversity, driving further advancements in the field. Overall, the main contributions are summarized as follows:\n‚Ä¢\nA systematic benchmark tailored for robotic video generation. We propose RBench, which comprehensively evaluates the performance of video foundation models across five robotic tasks and four robot types with 650 meticulously curated evaluation samples, while introducing reproducible automated evaluation metrics.\n‚Ä¢\nKey insights into robotic video generation for embodied research. We conduct a systematic evaluation of 25 representative video models, including open-source, commercial, and robotics-specific ones, revealing the limitations of current video foundation models and potential directions for improvement, offering new perspectives for researchers exploring the embodied domain using video world models.\n‚Ä¢\nA large-scale, high-quality robotic video dataset. We construct RoVid-X, a dataset containing approximately 4 million curated robotic videos with standardized task descriptions and physical property annotations, providing essential support for the training and evaluation of embodied video models.\n2\nRelated Work\n2.1\nVideo World Modeling for Robotics\nThe latest breakthroughs in video generation technology have led to the development of powerful models capable of generating high-quality videos from text or image prompts\n[\n83\n,\n80\n,\n75\n,\n59\n,\n92\n]\n. With the advancement of these technologies, an increasing number of studies have begun applying them to the field of embodied intelligence\n[\n25\n,\n91\n,\n2\n,\n11\n]\n. Video provides a rich source of information for robot training\n[\n14\n]\n. On one hand, video generation models can be used to synthesize robot trajectories\n[\n47\n,\n8\n,\n7\n]\n, serving as an alternative to the time-consuming and labor-intensive process of human teleoperation data. And executable actions can be extracted through inverse dynamics models (IDM)\n[\n89\n,\n4\n,\n22\n,\n112\n]\nor latent action models\n[\n88\n,\n107\n]\n. On the other hand, recent studies have explored using video to simulate task dynamics and predict future states, thereby assisting in policy learning. Specifically, video generation models are used to initialize robot policies for training\n[\n63\n,\n50\n,\n100\n]\nor to simultaneously train policies and inverse dynamics models, enabling co-training with robot data\n[\n37\n,\n113\n,\n60\n]\n. These efforts demonstrate the significant potential of video generation models in the field of embodied robotics, highlighting their important value in improving training efficiency and task performance.\n2.2\nDatasets for Robot Learning\nA core challenge in robot learning is the lack of large-scale, diverse datasets that facilitate the training of general-purpose robots with physical interaction capabilities\n[\n78\n,\n8\n]\n. Currently, datasets used in the embodied intelligence community for robot learning can be broadly classified into three categories: real-world robot data\n[\n10\n,\n78\n,\n110\n,\n26\n,\n71\n]\n, human video data\n[\n18\n,\n31\n,\n32\n]\n, and synthetic robot data\n[\n105\n,\n95\n,\n74\n,\n70\n,\n52\n,\n90\n]\n. As a key element in training physical AI models, most existing real-world robot datasets are collected through robotic teleoperation\n[\n102\n,\n27\n,\n1\n]\nor by teams of human operators\n[\n12\n,\n68\n,\n9\n]\n, which leads to high collection costs and limited data scale. Furthermore, these datasets predominantly focus on similar types of robots, resulting in issues of limited diversity and restricted environments\n[\n23\n,\n56\n]\n. Additionally, inconsistent data collection and storage methods across different datasets make it difficult to enable effective cross-dataset co-training. Our focus is on collecting robot data for video generation that spans various robot morphologies and entities, and providing a unified set of physical attributes for all data sources, thereby advancing cross-entity research in robot learning.\nFigure 2\n:\nQualitative illustration of failure modes captured by RBench.\nUnlike conventional metrics that focus primarily on pixel-level fidelity, RBench provides a granular evaluation across multiple dimensions, including physical plausibility and task-level consistency. These results highlight persistent challenges in robotic video generation, such as structural distortion, floating components, and key action omission, which are accurately identified by our proposed sub-metrics. More cases are shown in the Appendix\nB\n.\n2.3\nBenchmarks for Video Generation\nEstablishing robust evaluation frameworks is essential for measuring the progress of video generation models. Currently, evaluation methodologies can be categorized into three primary streams: visual fidelity and semantics, which assess basic clarity and text-video alignment\n[\n67\n,\n85\n,\n108\n]\n; temporal dynamics, focusing on motion consistency and long-range narrative coherence\n[\n48\n,\n62\n,\n13\n]\n; and physical plausibility, which examines adherence to fundamental laws such as inertia and collision dynamics\n[\n72\n,\n6\n,\n73\n,\n93\n]\n. While these benchmarks provide valuable insights into general video quality, they are largely decoupled from the specific requirements of embodied AI. Specifically, existing frameworks often rely on isolated physical constraints or local visual metrics, failing to capture the complex interplay between robotic actions and environmental responses. Furthermore, there is a distinct lack of systematic evaluation for task-level correctness and spatial constraints in multi-embodiment scenarios. To bridge this gap, we propose a comprehensive benchmark specifically tailored for robotic video generation, introducing reproducible metrics that unify physical realism with task-oriented action completeness.\n3\nRBench\nFigure 3\n:\nStatistics in RBench.\nThe benchmark covers diverse tasks, object categories, and environments, demonstrating the high quality and comprehensiveness of the evaluation set, highlighting its high applicability to a wide range of robotic video generation scenarios.\nExisting video generation benchmarks primarily focus on evaluating model performance in general scenes\n[\n46\n,\n42\n]\n, while other benchmarks specifically designed for physical scenarios mainly assess models‚Äô capabilities in physical reasoning\n[\n72\n,\n36\n]\n. In this paper, we design a benchmark tailored for robotic physical scenarios, aimed at comprehensively evaluating the performance of video generation models in robotic tasks. This benchmark differs from existing general scene benchmarks by focusing on evaluating video generation models‚Äô capabilities in robotic physical environments. As shown in Figure\n2\n, our benchmark highlights common failure modes in robotic video generation, including issues such as robot shape distortion, object attribute drift, non-contact attachment, and others. Section\n3.1\noutlines the process of benchmark construction, while Section\n3.2\ndiscusses the automatic metrics used for evaluation.\n3.1\nBenchmark Construction\nTo comprehensively evaluate the capabilities of video generation models in robotic scenarios, the designed evaluation dimensions must cover a wide range of task scenarios and embodiment types, ensuring that these scenarios reflect realistic robotic action semantics. To this end, we design a diversified benchmark from two aspects: task categories and embodiment types, containing a total of 650 evaluation cases. The task-oriented categories include five representative tasks:\nCommon Manipulation\n,\nLong-horizon Planning\n,\nMulti-entity Collaboration\n,\nSpatial Relationship\n, and\nVisual Reasoning\n, with a total of 250 image-text pairs, 50 samples for each task. The embodiment-specific categories cover four mainstream embodiment types:\nDual-arm robots\n,\nHumanoid robots\n,\nSingle-arm robots\n, and\nQuadruped robots\n, with a total of 400 image-text pairs, 100 samples for each embodiment type.\nThe benchmark includes a variety of text prompts and high-quality robot reference images. Each sample image is a keyframe extracted from high-quality videos sourced from public datasets or online sources, and each image is manually verified to ensure its accuracy.\nTo avoid overlap with the training data, we ensure that the selected videos in the evaluation set do not appear in the subsequent training database, and we redesign new task prompts for each reference image, effectively avoiding the risk of content overlap. All samples are verified and filtered by human annotators to ensure that the generated task prompts align with realistic logic. Figure\n3\nillustrates the high aesthetic quality of the reference images (a), the broad range of testing scenarios including various objects, tasks, and action skills (b, c), and the diversity of environments in our evaluation set (d). Additionally, we record the metadata for each sample, such as the manipulated object, embodiment type, and camera viewpoint (first-person/third-person), to support more detailed analysis. See more details in the Appendix\nA\n.\n3.2\nAutomatic Metrics\nExisting video generation evaluation protocols, such as the representative VBench\n[\n46\n]\n, primarily focus on perceptual quality, assessing aspects like frame clarity, texture fidelity, and motion smoothness. However, they lack task-specific criteria tailored to robotic scenarios. Recently, several studies\n[\n85\n,\n33\n,\n94\n]\nhave utilized multimodal large language models (MLLMs) as zero-shot evaluators for generated videos. Building upon this, we extend this approach to the domain of robotic video evaluation and propose a set of automatic evaluation metrics, incorporating manually designed indicators to assess both the visual realism and task-level validity of generated robotic videos. Following previous practices, we select the open-source Qwen3-VL\n[\n3\n]\nand the closed-source GPT-5\n[\n76\n]\nas our MLLM evaluation models. In the following sections, we introduce the evaluation methods for task completion and visual quality, respectively. Further details on metrics design and mathematical definitions are provided in the Appendix\nB\n.\n3.2.1\nTask Completion\nPhysical-Semantic Plausibility.\nThis metric targets everyday physical and semantic plausibility violations that standard perception scores often miss. As shown in Figure\n1\n, we evaluate temporal grids of uniformly sampled frames with a VQA-style protocol using MLLM. Beyond assessing physical-semantic plausibility, we place special emphasis on the following frequent failure modes:\n(i)\nFloating/Penetration\n: parts of the robot or objects are not grounded or interpenetrate with solid objects;\n(ii)\nSpontaneous emergence\n: entities appear/disappear without causal motion;\n(iii)\nNon-contact attachment/Incorrect grasp\n: objects move with the robot without visible contact or with improper gripper closure.\nTask-Adherence Consistency.\nThis metric evaluates whether the video aligns with the intent and sequence defined by the prompt. Typical deviations include missing actions (e.g., approach without grasping or placing), incorrect order (e.g., placing before grasping), semantic drift (e.g., \"wiping\" becomes \"touching\"), and non-responsiveness. We construct temporal grids and apply an MLLM-based VQA checklist, which covers:\n(i)\nTask responsiveness\n, ensuring the goal state is reached without premature interruption;\n(ii)\nKey actions\n, verifying that required actions (e.g., grasp, place, open/close) occur and align with the prompt.\n3.2.2\nVisual Quality\nMotion Amplitude\nThis metric measures the motion amplitude of the robotic subject while discounting apparent movement caused by camera motion, thereby penalizing videos that appear smooth but lack meaningful subject activity.\nFollowing VMBench\n[\n64\n]\n, active subjects are localized with GroundingDINO\n[\n65\n]\n, temporally stable masks are produced by GroundedSAM\n[\n81\n]\n, and salient points are tracked via CoTracker\n[\n55\n]\n. Let\nD\n¬Ø\nt\n\\bar{D}_{t}\nbe the mean displacement of tracked points on the subject at frame\nt\nt\n. The Motion Amplitude Score (MAS) is\nMAS\n=\n1\nT\n‚Äã\n‚àë\nt\n=\n1\nT\nmin\n‚Å°\n(\nD\n¬Ø\nt\n,\n1\n)\n,\n\\mathrm{MAS}\\,=\\,\\frac{1}{T}\\sum_{t=1}^{T}\\min\\!\\bigl(\\bar{D}_{t},\\,1\\bigr),\n(1)\nwhere a lower MAS indicates insufficient subject motion and complements smoothness by revealing smooth-but-inactive failure modes.\nRobot-Subject Stability.\nThis metric assesses the stability of robot morphology and target object attributes over time. Typical failures include gripper/hand shape drifting into non-mechanical forms, extra/missing manipulators, link-length/topology changes, joint inversion, object misidentification or attribute drift (class, color, position), and impossible deformation of rigid items. We adopt a contrastive VQA setup based on MLLM, which compares a reference frame and a generated frame and assigns a consistency score targeting the above failures.\nMotion Smoothness\nThis metric quantifies temporal continuity and natural dynamics, targeting artifacts from low-level aliasing to high-level jitter/blur. Following VMBench\n[\n64\n]\n, we measure frame-to-frame quality stability with the Q-Align aesthetic score\n[\n99\n]\n. For frames\n{\nf\nt\n}\nt\n=\n1\nT\n\\{f_{t}\\}_{t=1}^{T}\nand per-frame score\nQ\n‚Äã\n(\nf\nt\n)\nQ(f_{t})\n, define:\nŒî\n‚Äã\nQ\nt\n=\nQ\n‚Äã\n(\nf\nt\n‚àí\n1\n)\n‚àí\nQ\n‚Äã\n(\nf\nt\n)\n.\n\\Delta Q_{t}\\,=\\,Q(f_{t-1})-Q(f_{t}).\n(2)\nA temporal anomaly is flagged when\nŒî\n‚Äã\nQ\nt\n\\Delta Q_{t}\nexceeds an adaptive threshold\nœÑ\ns\n‚Äã\n(\nt\n)\n\\tau_{s}(t)\ndetermined by the robotic subject‚Äôs motion. The Motion Smoothness Score (MSS) is\nMSS\n=\n1\n‚àí\n1\nT\n‚Äã\n‚àë\nt\n=\n2\nT\nùïÄ\n‚Äã\n(\nŒî\n‚Äã\nQ\nt\n>\nœÑ\ns\n‚Äã\n(\nt\n)\n)\n,\n\\mathrm{MSS}\\,=\\,1-\\frac{1}{T}\\sum_{t=2}^{T}\\mathbb{I}\\!\\bigl(\\Delta Q_{t}>\\tau_{s}(t)\\bigr),\n(3)\nwhere\nùïÄ\n‚Äã\n(\n‚ãÖ\n)\n\\mathbb{I}(\\cdot)\nis the indicator function. A higher MSS indicates smoother motion.\nFigure 4\n:\nOverview of RoVid-X Construction and Descriptive Statistics.\n(a) shows the four-stage pipeline for constructing the RoVid-X. (b) presents descriptive statistics, covering frame intervals, skill distribution, and common objects, highlighting the dataset‚Äôs diversity and suitability for robotic task training and video generation.\n4\nRoVid-X\nIn this section, we introduce the construction of a high-quality robotic video dataset, resulting in RoVid-X. The dataset is developed through a refined four-stage pipeline, as shown in Figure\n4\n(a). The dataset primarily comes from internet-sourced robotic videos that are public domain or non-copyrighted, as well as open-source embodied video datasets, all of which are licensed for use. We then introduce the construction process of the dataset and provide statistical information.\n4.1\nDataset Construction\nOur data processing workflow consists of four distinct stages, each designed to ensure the quality, diversity, and relevance of the collected data. These stages are outlined as follows:\nRobot Video Collection.\nIn the first stage, we collect raw robotic videos from large-scale internet video platforms and over 20 open-source embodied video datasets. These datasets cover a variety of robot types and task scenarios, ensuring the breadth and diversity of the data. To improve dataset relevance and quality, we employ the GPT-5 model\n[\n76\n]\nto automatically filter the content of each video and remove low-quality or irrelevant video clips that do not align with the research objectives. During the filtering process, GPT-5 identifies videos related to robotic tasks and actions based on visual content and subtitles, ensuring that all collected videos effectively support the training and evaluation of robotic tasks. After this filtering process, we identify approximately 3 million raw robotic video clips, covering different actions, tasks, and robot types.\nVideo Quality Filtering.\nIn this stage, we perform a rigorous filtering procedure on the collected videos to remove low-quality and irrelevant video clips that do not align with the research objectives. First, we apply scene segmentation detection to remove all video data unrelated to robots. Then, we use a video quality scoring system to assess the videos from multiple dimensions, including clarity, dynamic effects, aesthetic performance, and optical character recognition (OCR), among other metrics. Each video clip is assigned a quality score based on these criteria, ensuring that the videos retained in the final dataset meet high-quality standards.\nTable 1\n:\nComparison of representative robotic video datasets.\nDataset\nYear\n#Videos\n#Skills\nResolution\nOptical Flow\nDiverse Robotic Forms\nDiverse Captions\nRoboTurk\n[\n69\n]\n2018\n2.1k\n2\n480P\n‚úó\n‚úó\n‚úó\nRoboNet\n[\n19\n]\n2019\n162k\nN/A\n240P\n‚úó\n‚úó\n‚úó\nBridgeData\n[\n23\n]\n2021\n7.2k\n4\n480P\n‚úó\n‚úó\n‚úó\nRH20T\n[\n24\n]\n2023\n13k\n33\n720P\n‚úó\n‚úó\n‚úó\nDROID\n[\n56\n]\n2024\n76k\n86\n720P\n‚úó\n‚úó\n‚úó\nOpen X-Embodiment\n[\n78\n]\n2024\n1.4M\n217\n64P‚Äî720P\n‚úó\n‚úì\n‚úó\nRoboMIND\n[\n101\n]\n2024\n107k\n38\n480P\n‚úó\n‚úó\n‚úó\nRoboCOIN\n[\n103\n]\n2025\n180k\n36\n480P\n‚úó\n‚úó\n‚úó\nGalaxea\n[\n49\n]\n2025\n100k\n58\n720P\n‚úó\n‚úó\n‚úó\nInternData-A1\n[\n90\n]\n2025\n630k\n18\n480P\n‚úó\n‚úó\n‚úó\nFourier ActionNet\n[\n26\n]\n2025\n13k\n16\n800P\n‚úó\n‚úó\n‚úó\nHumanoid Everyday\n[\n110\n]\n2025\n10.3k\n221\n320P‚Äî720P\n‚úó\n‚úó\n‚úó\nAgibot World\n[\n12\n]\n2025\n1M\n87\n480P\n‚úó\n‚úó\n‚úó\nRoVid-X (Ours)\n2026\n4M\n1300+\n720P\n‚úì\n‚úì\n‚úì\nTask Segmentation and Captioning.\nIn this stage, we use a video understanding model\n[\n34\n]\nand a specially designed prompt template to automatically analyze the robot actions within the videos. The system segments the videos into different task segments based on timestamps, generating short subtitles for each task segment that accurately describe the robot‚Äôs actions and operational details in that task.\nThe action recognition and description process for each task segment follows these steps: First, the system identifies all dynamic actions within the video and excludes static scenes or irrelevant actions (e.g., waiting or remaining still). The time range for each action (start and end times) is precisely labeled to ensure accuracy. Next, using the MLLM model\n[\n34\n]\n, textual descriptions of each action are automatically generated, including the action subject (e.g., \"right arm\" or \"left gripper\"), the object being manipulated (e.g., \"nameplate\" or \"box\"), and the specific operation details (e.g., \"grasp and move\" or \"remove from the table\"). Finally, the subtitles for each task segment are output in a standardized format, ensuring that the action descriptions for each video clip are clear, concise, and aligned with the task requirements.\nPhysical Property Annotation.\nTo ensure consistency and realism of robot actions within physical space, we apply physical attribute enhancement to the videos. Specifically, we use FlashVSR\n[\n114\n]\nto improve the video resolution, making the images clearer and enhancing the details of the actions. Then, using the AllTracker tool\n[\n43\n]\n, we annotate a unified optical flow for the subjects in the videos, ensuring consistency in tracking and recording robot actions across different scenes. Additionally, using Video Depth Anything\n[\n16\n]\n, we generate relative depth maps to accurately describe the spatial relationships and depth information of objects in the scene. The goal of these physical attribute annotations is to provide researchers with more precise reference data, aiding in the training and evaluation of robot video generation models and offering richer physical data support for future research.\nTable 2\n:\nRBench quantitative results.\nEvaluations across task-oriented and embodiment-specific dimensions for 25 models from open-source, commercial, and robotics-specific families. The \"Avg.\" column shows the mean score across nine indicators, with task performance in the left block and embodiment performance in the right block. In the table, a \"#\" next to the Sora2 model in the top right corner indicates review limitations from the official Sora API, where approximately 50 out of 650 videos could not be generated. The scores derived from sub-metrics are reported in the Appendix\nG\n.\nModels\nRank\nAvg.\nTasks\nEmbodiments\nManipulation\nSpatial\nMulti-entity\nLong-horizon\nReasoning\nSingle arm\nDual arm\nQuadruped\nHumanoid\nOpen-source\nWan2.2_A14B\n[\n92\n]\n8\n0.507\n0.381\n0.454\n0.373\n0.501\n0.330\n0.608\n0.582\n0.690\n0.648\nHunyuanVideo 1.5\n[\n98\n]\n10\n0.460\n0.442\n0.316\n0.312\n0.438\n0.364\n0.513\n0.526\n0.634\n0.595\nLongCat-Video\n[\n87\n]\n11\n0.437\n0.372\n0.310\n0.220\n0.384\n0.186\n0.586\n0.576\n0.681\n0.621\nWan2.1_14B\n[\n92\n]\n14\n0.399\n0.344\n0.268\n0.282\n0.335\n0.205\n0.464\n0.497\n0.595\n0.599\nLTX-2\n[\n40\n]\n15\n0.381\n0.284\n0.304\n0.233\n0.386\n0.164\n0.453\n0.424\n0.622\n0.555\nWan2.2_5B\n[\n92\n]\n16\n0.380\n0.331\n0.313\n0.142\n0.318\n0.234\n0.436\n0.448\n0.590\n0.607\nSkyReels\n[\n15\n]\n18\n0.361\n0.203\n0.276\n0.203\n0.254\n0.234\n0.507\n0.477\n0.586\n0.509\nLTX-Video\n[\n39\n]\n19\n0.344\n0.302\n0.176\n0.210\n0.280\n0.241\n0.440\n0.456\n0.526\n0.464\nFramePack\n[\n109\n]\n20\n0.339\n0.206\n0.258\n0.173\n0.169\n0.170\n0.440\n0.464\n0.626\n0.548\nHunyuanVideo\n[\n59\n]\n21\n0.303\n0.177\n0.180\n0.108\n0.147\n0.035\n0.454\n0.480\n0.625\n0.524\nCogVideoX_5B\n[\n106\n]\n23\n0.256\n0.116\n0.112\n0.098\n0.212\n0.079\n0.338\n0.385\n0.465\n0.496\nCommercial\nWan 2.6\n[\n92\n]\n1\n0.607\n0.546\n0.656\n0.479\n0.514\n0.531\n0.666\n0.681\n0.723\n0.667\nSeedance 1.5 pro\n[\n17\n]\n2\n0.584\n0.577\n0.495\n0.484\n0.570\n0.470\n0.648\n0.641\n0.680\n0.692\nWan 2.5\n[\n92\n]\n3\n0.570\n0.527\n0.576\n0.402\n0.496\n0.437\n0.680\n0.634\n0.726\n0.654\nHailuo v2\n[\n41\n]\n4\n0.565\n0.560\n0.637\n0.386\n0.545\n0.474\n0.594\n0.611\n0.640\n0.635\nVeo 3\n[\n30\n]\n5\n0.563\n0.521\n0.508\n0.430\n0.530\n0.504\n0.634\n0.610\n0.689\n0.637\nSeedance 1.0\n[\n29\n]\n6\n0.551\n0.542\n0.425\n0.448\n0.454\n0.442\n0.622\n0.641\n0.698\n0.686\nKling 2.6 pro\n[\n58\n]\n7\n0.534\n0.529\n0.598\n0.364\n0.530\n0.358\n0.570\n0.605\n0.637\n0.613\nSora v2 Pro\n#\n[\n77\n]\n17\n0.362\n0.208\n0.268\n0.186\n0.255\n0.115\n0.476\n0.513\n0.664\n0.561\nSora v1\n[\n75\n]\n22\n0.266\n0.151\n0.223\n0.111\n0.166\n0.139\n0.314\n0.324\n0.544\n0.419\nRobotics-specific\nCosmos 2.5\n[\n2\n]\n9\n0.464\n0.358\n0.338\n0.201\n0.496\n0.399\n0.544\n0.560\n0.658\n0.626\nDreamGen(gr1)\n[\n47\n]\n12\n0.420\n0.312\n0.372\n0.297\n0.334\n0.215\n0.564\n0.532\n0.579\n0.575\nDreamGen(droid)\n[\n47\n]\n13\n0.405\n0.358\n0.348\n0.214\n0.316\n0.339\n0.499\n0.476\n0.542\n0.556\nVidar\n[\n25\n]\n24\n0.206\n0.073\n0.106\n0.050\n0.054\n0.050\n0.382\n0.410\n0.374\n0.357\nUnifoLM-WMA-0\n[\n91\n]\n25\n0.123\n0.036\n0.040\n0.018\n0.062\n0.000\n0.268\n0.194\n0.293\n0.200\n4.2\nDataset Analysis\nRoVid-X is the first open-source large-scale robotic video dataset specifically designed for training video generation models, containing 4 million robotic video clips. This dataset is designed to address the physical challenges that video generation models face when generating robotic videos, providing high-quality data for both training and evaluation. RoVid-X aims to bridge the gap between traditional video generation tasks and the unique demands of embodied robot learning, where physical interaction, spatial relationships, and real-world dynamics play a crucial role.\nThe dataset includes a diverse range of robotic actions, tasks, and robot types, ensuring its applicability across different robotic domains. By incorporating videos from various robot types and scenarios, RoVid-X provides comprehensive coverage of the physical properties and task requirements needed for robot training. As shown in Figure\n4\n(b), detailed statistics of the dataset are provided, illustrating the variety in terms of action skills, task types, and interaction objects. The wide-ranging data distribution of RoVid-X is critical for supporting the development of robust video generation models that can simulate realistic robot behaviors in dynamic environments.\n5\nExperiment\nFigure 5\n:\nQualitative comparison across representative tasks.\nWe visualize the generated results for three representative tasks:\nVisual Reasoning\n,\nLong-horizon Planning\n, and\nSpatial Relationship\n, across six models. Each row displays temporally sampled frames from the same generated video, with captions below indicating the corresponding task instruction. More cases are shown in the Appendix.\n5.1\nEvaluation Setups\nEvaluation Models.\nWe evaluate 25 state-of-the-art video generation models, grouped into three types. Specifically, the closed-source models include Hailuo\n[\n41\n]\n, Wan\n[\n92\n]\n, Veo 3\n[\n30\n]\n, Sora\n[\n75\n,\n77\n]\n, Kling\n[\n58\n]\n, Seedance\n[\n29\n,\n17\n]\n, and others, while the open-source models include several representative models such as HunyuanVideo\n[\n59\n,\n98\n]\n, LTX\n[\n40\n,\n39\n]\nand CogVideoX\n[\n106\n]\n. Additionally, we assess models specifically designed for robotic tasks, such as DreamGen\n[\n47\n]\n, Vidar\n[\n25\n]\n, and Cosmos 2.5\n[\n2\n]\n. The evaluations of these models cover various types of embodiments and multiple tasks, providing a comprehensive perspective on model performance.\nImplementation Details.\nTo ensure a fair comparison, all open-source models generate videos using their official default configurations to ensure consistency with the model‚Äôs preset settings. For closed-source video models, we use their official APIs, strictly following the methods recommended by the developers for invoking and using the models. In the benchmark testing, we generate the videos for each image-text pair. To minimize errors, we generate three videos for each model sample and take the average as the final score for that sample.\nThese generated videos are evaluated using the automated evaluation metrics that we propose, which are designed to comprehensively assess multiple aspects of the generated videos, including task completion, action consistency, physical plausibility, and more. Further details on the model setup and configuration parameters are provided in the Appendix\nC\n.\nTable 3:\nComparison between human preference scores and RBench scores.\nThis table demonstrates a high correlation between the two sets of scores, as reflected in the similar ranking orders.\nModel\nHuman\nRBench\nr\nh\nr_{h}\nr\nb\nr_{b}\nŒî\n‚Äã\nr\n\\Delta r\nWan 2.5\n0.573\n0.570\n1\n1\n0\nVeo 3\n0.540\n0.563\n2\n3\n1\nHailuo v2\n0.513\n0.565\n3\n2\n-1\nSeedance 1.0\n0.505\n0.551\n4\n4\n0\nCosmos 2.5\n0.500\n0.464\n5\n5\n0\nDreamGen\n0.482\n0.420\n6\n7\n1\nLongCat-Video\n0.480\n0.437\n7\n6\n-1\nWan2.1-14B\n0.378\n0.399\n8\n8\n0\nCogVideoX-5B\n0.333\n0.256\n9\n10\n1\nLTX-Video\n0.246\n0.344\n10\n9\n-1\nTable 4:\nRoVid-X effectiveness validation experiment.\nThe experimental results using different models for finetuning show stable improvements across various dimensions, validating the effectiveness of the dataset.\nModel\nManip.\nLong.\nMulti.\nSpatial.\nReason.\nWan2.1_14B\n0.344\n0.335\n0.282\n0.268\n0.205\nWan2.1_14B+Ours\n0.376\n0.389\n0.295\n0.314\n0.298\nWan2.2_5B\n0.331\n0.318\n0.142\n0.313\n0.234\nWan2.2_5B+Ours\n0.373\n0.387\n0.221\n0.403\n0.284\nModel\nSingle\nDual\nQuad.\nHumanoid\nTotal\nWan2.1_14B\n0.464\n0.497\n0.595\n0.599\n0.399\nWan2.1_14B + Ours\n0.526\n0.546\n0.639\n0.628\n0.446\nWan2.2_5B\n0.436\n0.448\n0.590\n0.607\n0.380\nWan2.2_5B + Ours\n0.514\n0.503\n0.628\n0.641\n0.439\n5.2\nMain Analysis\n5.2.1\nQuantitative Results\nTable\n2\npresents a comprehensive quantitative evaluation across varying model architectures, tasks, and embodiments. Beyond standard performance metrics, the results reveal a pivotal paradigm shift in the video generation landscape.\nFrom Visual Fidelity to Physical Intelligence.\nThe most significant trend observed is the transition of video generation models from pursuing high-fidelity visualization to addressing the complex dynamics of the physical world. While traditional metrics prioritize pixel-level quality, our benchmark highlights that top-tier commercial models (e.g., Wan 2.6, Seedance 1.5 Pro) are beginning to emerge as effective\nWorld Simulators\n.\" This indicates that the field is moving towards a new stage:\nPhysical AI\n, where models must understand and simulate interaction-rich, physically challenging real-world scenarios rather than merely generating aesthetically pleasing videos.\nIterative Scaling Unlocks Physical Capabilities.\nAnalyzing model evolution reveals a strong correlation between model iteration and physical reasoning capabilities. For instance, the\nWan\nseries exhibits a dramatic performance leap: from Wan 2.1 (Rank 14, 0.399) to Wan 2.6 (Rank 1, 0.607). Similarly, Seedance evolves from 1.0 to 1.5 Pro, climbing from Rank 6 to Rank 2. These substantial gains suggest that scaling laws and iterative optimization are not just improving visual quality but are actively refining the model‚Äôs understanding of physics, distinct motion patterns, and control logic.\nThe \"Media-Simulation\" Gap in Consumer Models.\nSurprisingly, widely recognized consumer-oriented models like the Sora series perform sub-optimally on this benchmark (Sora v2 Pro at Rank 17, Avg 0.362). This counter-intuitive result highlights a critical \"domain gap\": models optimized for media consumption prioritize visual smoothness and cinematic transitions, often at the expense of physical fidelity and precise motion control. This discrepancy suggests that proficiency in creative video generation does not naturally transfer to Embodied AI tasks, underlining the necessity for physically-grounded training data.\nClosed-source Models Lead in Performance.\nCommercial closed-source models occupy the top 7 positions in our benchmark, demonstrating a clear and consistent advantage over open-source counterparts. The significant performance margin between the state-of-the-art commercial model (Wan 2.6) and the leading open-source model (Wan 2.2) highlights a substantial capability gap. This disparity underscores a critical urgency for the open-source community: to democratize high-capability foundation models, more concerted efforts are needed in scaling physical training data and optimizing architectures for embodied video tasks.\nThe Dilemma of Specialization: Domain Data vs. World Knowledge.\nWhile General Foundation Models lead the leaderboard, the robotics-specific model Cosmos 2.5 demonstrates remarkable resilience. Despite trailing top-tier commercial models, it outperforms significantly larger open-source video models, confirming that training with physical data yields stable gains in robotic tasks. Conversely, models fine-tuned on specific robot entities (e.g., Vidar, UnifoLM) struggle significantly, ranking at the bottom of the benchmark. This contrast highlights a critical trade-off: while domain-specific data is valuable for control precision, it cannot fully compensate for the deficit in \"World Knowledge\" provided by large-scale pretraining. Balancing proprietary robot data with generalizable representations remains a pivotal challenge for future research.\nCognitive and Fine-grained Control Bottlenecks.\nA consistent trend across all model families is that tasks requiring high-level logic or precise interaction represent the most significant performance bottlenecks.\nFirst, regarding cognitive capabilities, we observe a substantial \"Cognitive Gap\": while top-tier models like Wan 2.6 excel in execution-oriented tasks, their performance drops sharply in\nVisual Reasoning\n(0.531). Furthermore, analyzing specific embodiments reveals a \"Manipulation Gap\": models consistently score higher on coarse-grained locomotion tasks (Quadruped, Humanoid) than on fine-grained manipulation. This implies that for current video generators, mastering the fine-grained contact dynamics required for object interaction is physically more challenging than generating the rhythmic patterns of legged locomotion.\n5.2.2\nQualitative Results\nWe conduct a qualitative analysis of representative tasks, and the partial results are shown in Figure\n5\n. For the visual reasoning task, Seedance 1.0\n[\n29\n]\nand Hailuo\n[\n41\n]\ncorrectly identify the blue clothing and the hollow basket, while Wan 2.5\n[\n92\n]\nmistakenly identifies the woven basket as the hollow basket. In the long-horizon planning task, Wan 2.5 successfully completes all actions in the correct sequence, while Hailuo lacks the \"turn-on\" action, leading to a violation of physical logic. In the spatial relationship task, Hailuo correctly places the bok choy to the left of the pan, whereas other models mistakenly place it inside the pan. Notably, LongCat-Video introduces an unrealistic human arm intervention, disrupting physical plausibility. More detailed analysis and qualitative results can be found in the Appendix\nF\n.\nThese models each have their strengths, but there is still significant room for improvement in their overall performance. This further highlights the necessity of designing such a benchmark to advance video generation models in robotic tasks.\n5.3\nHuman Preference Study\nWe conduct a human preference study to assess how well automatic metrics align with human perception. Thirty participants are invited to participate. For each comparison, two model outputs for the same prompt and video instance are presented side-by-side, and annotators choose from three options: ‚ÄúA is better,‚Äù ‚ÄúB is better,‚Äù or ‚ÄúTie‚Äù. Votes are aggregated into per-model scores: a win contributes\n5\n5\n, a loss contributes\n1\n1\n, and a tie contributes\n3\n3\nto both models.\nWe then compare these model-level human scores with the corresponding RBench benchmark scores. On the ten-model subset used in the study, the Spearman rank correlation between human scores and RBench scores is\nœÅ\n=\n0.96\n\\rho=0.96\n(two-sided\np\n<\n10\n‚àí\n3\np<10^{-3}\n).\nTable\n4\npresents the human scores, RBench scores, and ranks for the ten selected models, where the\nŒî\n‚Äã\nr\n\\Delta r\ncolumn denotes the rank difference\n(\nr\nb\n‚àí\nr\nh\n)\n(r_{b}-r_{h})\n.\nOverall, models that rank highest under the benchmark largely match human judgments, while the remaining small discrepancies highlight opportunities to further refine the metric for improved human alignment. The high degree of consistency further demonstrates the validity and effectiveness of our automated metrics in evaluating video generation models, indicating that the metrics accurately reflect human perception and thus provide a reliable evaluation standard for robotic video generation tasks. Please refer to the Appendix\nD\nfor more details.\n5.4\nValidation of RoVid-X\nTo assess the effectiveness and robustness of RoVid-X, we finetune models initialized with Wan2.1 14B and Wan2.2 5B weights, using MSE loss exclusively. Due to computational constraints, we randomly sample 200k instances from the original RoVid-X dataset. The results, shown in Table\n4\n, highlight that our dataset significantly enhances performance across five task domains and four distinct embodiments. These improvements validate both the proposed dataset and our data collection pipeline.\n6\nConclusion\nIn this work, we rethink video generation models for the embodied world and introduce RBench, a new benchmark designed to fill a critical gap in evaluating robot-oriented video generation models. Unlike previous methods that primarily rely on perceptual metrics, RBench incorporates both task-level accuracy and visual fidelity, using a comprehensive evaluation suite with detailed sub-metrics such as structural consistency and physical plausibility. The evaluation of 25 models highlights that current video generation models still require significant improvements to generate physically realistic robot behaviors. The strong correlation between RBench scores and human evaluations further validates the benchmark‚Äôs effectiveness. Additionally, RoVid-X overcomes the limitations of existing robotic datasets by offering a large-scale and diverse resource for video generation tasks. Together, RBench and RoVid-X provide a robust foundation for advancing video generation models in robotics. Our findings highlight the shortcomings of current video foundation models and suggest possible avenues for improvement, providing researchers with fresh perspectives for exploring the embodied domain through video world models.\nFuture Work\n. We aim to bridge the gap between video generation and actionable robot policy. We plan to employ Inverse Dynamics Models (IDM) to recover executable actions from generated videos, enabling closed-loop control experiments in both simulation environments and on real-world hardware. Furthermore, we intend to develop more automated and physically grounded evaluation metrics to rigorously assess the kinematic and dynamic feasibility of generated behaviors. Additionally, we will focus on training video generation models with improved physical capabilities, enabling the generation of robot videos that perform high-fidelity actions. Ultimately, these efforts will accelerate the development of a comprehensive solution for video-driven embodied intelligence.\nReferences\nAldaco et al. [2024]\nJorge Aldaco, Travis Armstrong, Robert Baruch, Jeff Bingham, Sanky Chan, Kenneth Draper, Debidatta Dwibedi, Chelsea Finn, Pete Florence, Spencer Goodrich, et al.\nAloha 2: An enhanced low-cost hardware for bimanual teleoperation.\narXiv preprint arXiv:2405.02292\n, 2024.\nAli et al. [2025]\nArslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, et al.\nWorld simulation with video foundation models for physical ai.\narXiv preprint arXiv:2511.00062\n, 2025.\nBai et al. [2025]\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, and Tang.\nQwen3-vl technical report.\narXiv preprint arXiv\n, 2025.\nBaker et al. [2022]\nBowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.\nVideo pretraining (vpt): Learning to act by watching unlabeled online videos.\nAdvances in Neural Information Processing Systems\n, 35:24639‚Äì24654, 2022.\nBall et al. [2025]\nPhilip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, and Frank Perbet.\nGenie 3: A new frontier for world models.\narXiv preprint arXiv\n, 2025.\nBansal et al. [2024]\nHritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover.\nVideophy: Evaluating physical commonsense for video generation.\narXiv preprint arXiv:2406.03520\n, 2024.\nBharadhwaj et al. [2024]\nHomanga Bharadhwaj, Debidatta Dwibedi, Abhinav Gupta, Shubham Tulsiani, Carl Doersch, Ted Xiao, Dhruv Shah, Fei Xia, Dorsa Sadigh, and Sean Kirmani.\nGen2act: Human video generation in novel scenarios enables generalizable robot manipulation.\narXiv preprint arXiv:2409.16283\n, 2024.\nBjorck et al. [2025]\nJohan Bjorck, Fernando Casta√±eda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al.\nGr00t n1: An open foundation model for generalist humanoid robots.\narXiv preprint arXiv:2503.14734\n, 2025.\nBlack et al. [2024]\nKevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al.\nœÄ\n\\pi\n: A vision-language-action flow model for general robot control.\nCoRR\n, 2024.\nBrohan et al. [2022]\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al.\nRt-1: Robotics transformer for real-world control at scale.\narXiv preprint arXiv:2212.06817\n, 2022.\nBruce et al. [2024]\nJake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al.\nGenie: Generative interactive environments.\nIn\nForty-first International Conference on Machine Learning\n, 2024.\nBu et al. [2025]\nQingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al.\nAgibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems.\narXiv preprint arXiv:2503.06669\n, 2025.\nBugliarello et al. [2023]\nEmanuele Bugliarello, H Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan, Vittorio Ferrari, Pieter-Jan Kindermans, and Paul Voigtlaender.\nStorybench: A multifaceted benchmark for continuous story visualization.\nAdvances in Neural Information Processing Systems\n, 36:78095‚Äì78125, 2023.\nCheang et al. [2024]\nChi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al.\nGr-2: A generative video-language-action model with web-scale knowledge for robot manipulation.\narXiv preprint arXiv:2410.06158\n, 2024.\nChen et al. [2025a]\nGuibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, and Yahui Zhou.\nSkyreels-v2: Infinite-length film generative model, 2025a.\nURL\nhttps://arxiv.org/abs/2504.13074\n.\nChen et al. [2025b]\nSili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang.\nVideo depth anything: Consistent depth estimation for super-long videos.\narXiv:2501.12375\n, 2025b.\nChen et al. [2025c]\nSiyan Chen, Yanfei Chen, Ying Chen, Zhuo Chen, Feng Cheng, Xuyan Chi, Jian Cong, Qinpeng Cui, Qide Dong, Junliang Fan, et al.\nSeedance 1.5 pro: A native audio-visual joint generation foundation model.\narXiv preprint arXiv:2512.13507\n, 2025c.\nDamen et al. [2018]\nDima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al.\nScaling egocentric vision: The epic-kitchens dataset.\nIn\nProceedings of the European conference on computer vision (ECCV)\n, pages 720‚Äì736, 2018.\nDasari et al. [2019]\nSudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn.\nRobonet: Large-scale multi-robot learning.\narXiv preprint arXiv:1910.11215\n, 2019.\nDeng et al. [2025a]\nYufan Deng, Xun Guo, Yizhi Wang, Jacob Zhiyuan Fang, Angtian Wang, Shenghai Yuan, Yiding Yang, Bo Liu, Haibin Huang, and Chongyang Ma.\nCinema: Coherent multi-subject video generation via mllm-based guidance.\narXiv preprint arXiv:2503.10391\n, 2025a.\nDeng et al. [2025b]\nYufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, et al.\nMagref: Masked guidance for any-reference video generation.\narXiv preprint arXiv:2505.23742\n, 2025b.\nDu et al. [2023]\nYilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel.\nLearning universal policies via text-guided video generation.\nAdvances in neural information processing systems\n, 36:9156‚Äì9172, 2023.\nEbert et al. [2021]\nFrederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine.\nBridge data: Boosting generalization of robotic skills with cross-domain datasets.\narXiv preprint arXiv:2109.13396\n, 2021.\nFang et al. [2023]\nHao-Shu Fang, Hongjie Fang, Zhenyu Tang, Jirong Liu, Chenxi Wang, Junbo Wang, Haoyi Zhu, and Cewu Lu.\nRh20t: A comprehensive robotic dataset for learning diverse skills in one-shot.\narXiv preprint arXiv:2307.00595\n, 2023.\nFeng et al. [2025]\nYao Feng, Hengkai Tan, Xinyi Mao, Chendong Xiang, Guodong Liu, Shuhe Huang, Hang Su, and Jun Zhu.\nVidar: Embodied video diffusion model for generalist manipulation.\narXiv preprint arXiv:2507.12898\n, 2025.\nFourier ActionNet Team [2025]\nYao Mu Fourier ActionNet Team.\nActionnet: A dataset for dexterous bimanual manipulation.\narXiv preprint arXiv\n, 2025.\nFu et al. [2024]\nZipeng Fu, Tony Z Zhao, and Chelsea Finn.\nMobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation.\narXiv preprint arXiv:2401.02117\n, 2024.\nGao et al. [2025a]\nRuiyuan Gao, Kai Chen, Bo Xiao, Lanqing Hong, Zhenguo Li, and Qiang Xu.\nMagicdrive-v2: High-resolution long video generation for autonomous driving with adaptive control.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, pages 28135‚Äì28144, 2025a.\nGao et al. [2025b]\nYu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al.\nSeedance 1.0: Exploring the boundaries of video generation models.\narXiv preprint arXiv:2506.09113\n, 2025b.\nGoogle DeepMind [2025]\nGoogle DeepMind.\nVeo-3 technical report.\nTechnical report, Google DeepMind, May 2025.\nURL\nhttps://storage.googleapis.com/deepmind-media/veo/Veo-3-Tech-Report.pdf\n.\nGoyal et al. [2017]\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al.\nThe\" something something\" video database for learning and evaluating visual common sense.\nIn\nProceedings of the IEEE international conference on computer vision\n, pages 5842‚Äì5850, 2017.\nGrauman et al. [2022]\nKristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.\nEgo4d: Around the world in 3,000 hours of egocentric video.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pages 18995‚Äì19012, 2022.\nGu et al. [2025]\nJing Gu, Xian Liu, Yu Zeng, Ashwin Nagarajan, Fangrui Zhu, Daniel Hong, Yue Fan, Qianqi Yan, Kaiwen Zhou, Ming-Yu Liu, et al.\n\" phyworldbench\": A comprehensive evaluation of physical realism in text-to-video models.\narXiv preprint arXiv:2507.13428\n, 2025.\nGuo et al. [2025a]\nDong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al.\nSeed1. 5-vl technical report.\narXiv preprint arXiv:2505.07062\n, 2025a.\nGuo et al. [2024a]\nXun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, Haibin Huang, and Chongyang Ma.\nI2V-adapter: A general image-to-video adapter for diffusion models.\nIn\nACM SIGGRAPH 2024 Conference Papers\n, pages 1‚Äì12, 2024a.\nGuo et al. [2025b]\nXuyang Guo, Jiayan Huo, Zhenmei Shi, Zhao Song, Jiahao Zhang, and Jiale Zhao.\nT2vphysbench: A first-principles benchmark for physical consistency in text-to-video generation.\narXiv preprint arXiv:2505.00337\n, 2025b.\nGuo et al. [2024b]\nYanjiang Guo, Yucheng Hu, Jianke Zhang, Yen-Jen Wang, Xiaoyu Chen, Chaochao Lu, and Jianyu Chen.\nPrediction with action: Visual policy learning via joint denoising process.\nAdvances in Neural Information Processing Systems\n, 37:112386‚Äì112410, 2024b.\nGuo et al. [2025c]\nYanjiang Guo, Lucy Xiaoyang Shi, Jianyu Chen, and Chelsea Finn.\nCtrl-world: A controllable generative world model for robot manipulation.\narXiv preprint arXiv:2510.10125\n, 2025c.\nHaCohen et al. [2024]\nYoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi.\nLtx-video: Realtime video latent diffusion.\narXiv preprint arXiv:2501.00103\n, 2024.\nHaCohen et al. [2026]\nYoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz, Dudu Moshe, Eitan Porat, et al.\nLtx-2: Efficient joint audio-visual foundation model.\narXiv preprint arXiv:2601.03233\n, 2026.\nHailuo [2025]\nHailuo.\nHailuo.\nHailuo Lab\n, 2025.\nURL\nhttps://hailuoai.video/\n.\nHan et al. [2025]\nHui Han, Siyuan Li, Jiaqi Chen, Yiwen Yuan, Yuling Wu, Yufan Deng, Chak Tou Leong, Hanwen Du, Junchen Fu, Youhua Li, et al.\nVideo-bench: Human-aligned video generation benchmark.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 18858‚Äì18868, 2025.\nHarley et al. [2025]\nAdam W. Harley, Yang You, Xinglong Sun, Yang Zheng, Nikhil Raghuraman, Yunqi Gu, Sheldon Liang, Wen-Hsuan Chu, Achal Dave, Pavel Tokmakov, Suya You, Rares Ambrus, Katerina Fragkiadaki, and Leonidas J. Guibas.\nAllTracker: Efficient dense point tracking at high resolution.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, 2025.\nHo et al. [2020]\nJonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising diffusion probabilistic models.\nAdvances in neural information processing systems\n, 33:6840‚Äì6851, 2020.\nHu et al. [2024]\nYucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen.\nVideo prediction policy: A generalist robot policy with predictive visual representations.\narXiv preprint arXiv:2412.14803\n, 2024.\nHuang et al. [2024]\nZiqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al.\nVbench: Comprehensive benchmark suite for video generative models.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 21807‚Äì21818, 2024.\nJang et al. [2025]\nJoel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, et al.\nDreamgen: Unlocking generalization in robot learning through video world models.\narXiv preprint arXiv:2505.12705\n, 2025.\nJi et al. [2024]\nPengliang Ji, Chuyang Xiao, Huilin Tai, and Mingxiao Huo.\nT2vbench: Benchmarking temporal dynamics for text-to-video generation.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops\n, pages 5325‚Äì5335, June 2024.\nJiang et al. [2025a]\nTao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao, Huazhe Xu, and Hang Zhao.\nGalaxea open-world dataset and g0 dual-system vla model.\narXiv preprint arXiv:2509.00576\n, 2025a.\nJiang et al. [2025b]\nYuming Jiang, Siteng Huang, Shengke Xue, Yaxi Zhao, Jun Cen, Sicong Leng, Kehan Li, Jiayan Guo, Kexiang Wang, Mingxiu Chen, et al.\nRynnvla-001: Using human demonstrations to improve robot manipulation.\narXiv preprint arXiv:2509.15212\n, 2025b.\nJiang et al. [2025c]\nZeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu.\nVace: All-in-one video creation and editing.\narXiv preprint arXiv:2503.07598\n, 2025c.\nJiang et al. [2025d]\nZhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Jim Fan, and Yuke Zhu.\nDexmimicgen: Automated data generation for bimanual dexterous manipulation via imitation learning.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 16923‚Äì16930. IEEE, 2025d.\nJu et al. [2025]\nXuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, et al.\nEditverse: Unifying image and video editing and generation with in-context learning.\narXiv preprint arXiv:2509.20360\n, 2025.\nKang et al. [2024]\nBingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng.\nHow far is video generation from world model: A physical law perspective.\narXiv preprint arXiv:2411.02385\n, 2024.\nKaraev et al. [2024]\nNikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht.\nCotracker3: Simpler and better point tracking by pseudo-labelling real videos.\nIn\nProc. arXiv:2410.11831\n, 2024.\nKhazatsky et al. [2024]\nAlexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al.\nDroid: A large-scale in-the-wild robot manipulation dataset.\narXiv preprint arXiv:2403.12945\n, 2024.\nKim et al. [2025]\nGeonung Kim, Janghyeok Han, and Sunghyun Cho.\nVideofrom3d: 3d scene video generation via complementary image and video diffusion models.\narXiv preprint arXiv:2509.17985\n, 2025.\nKling [2025]\nKling.\nImage to video elements feature, 2025.\nURL\nhttps://klingai.com/global/\n.\nKong et al. [2024]\nWeijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al.\nHunyuanvideo: A systematic framework for large video generative models.\narXiv preprint arXiv:2412.03603\n, 2024.\nLi et al. [2025]\nShuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran Song.\nUnified video action model.\narXiv preprint arXiv:2503.00200\n, 2025.\nLiang et al. [2025]\nJunbang Liang, Pavel Tokmakov, Ruoshi Liu, Sruthi Sudhakar, Paarth Shah, Rares Ambrus, and Carl Vondrick.\nVideo generators are robot policies.\narXiv preprint arXiv:2508.00795\n, 2025.\nLiao et al. [2024]\nMingxiang Liao, Qixiang Ye, Wangmeng Zuo, Fang Wan, Tianyu Wang, Yuzhong Zhao, Jingdong Wang, Xinyu Zhang, et al.\nEvaluation of text-to-video generation models: A dynamics perspective.\nAdvances in Neural Information Processing Systems\n, 37:109790‚Äì109816, 2024.\nLiao et al. [2025]\nYue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, et al.\nGenie envisioner: A unified world foundation platform for robotic manipulation.\narXiv preprint arXiv:2508.05635\n, 2025.\nLing et al. [2025]\nXinran Ling, Chen Zhu, Meiqi Wu, Hangyu Li, Xiaokun Feng, Cundian Yang, Aiming Hao, Jiashu Zhu, Jiahong Wu, and Xiangxiang Chu.\nVmbench: A benchmark for perception-aligned video motion generation.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, pages 13087‚Äì13098, 2025.\nLiu et al. [2023a]\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al.\nGrounding dino: Marrying dino with grounded pre-training for open-set object detection.\narXiv preprint arXiv:2303.05499\n, 2023a.\nLiu et al. [2024]\nYaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan.\nEvalcrafter: Benchmarking and evaluating large video generation models.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 22139‚Äì22149, 2024.\nLiu et al. [2023b]\nYuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou.\nFetv: A benchmark for fine-grained evaluation of open-domain text-to-video generation.\nAdvances in Neural Information Processing Systems\n, 36:62352‚Äì62387, 2023b.\nLynch et al. [2023]\nCorey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence.\nInteractive language: Talking to robots in real time.\nIEEE Robotics and Automation Letters\n, 2023.\nMandlekar et al. [2018]\nAjay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Max Spero, Albert Tung, Julian Gao, John Emmons, Anchit Gupta, Emre Orbay, et al.\nRoboturk: A crowdsourcing platform for robotic skill learning through imitation.\nIn\nConference on Robot Learning\n, pages 879‚Äì893. PMLR, 2018.\nMandlekar et al. [2023]\nAjay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and Dieter Fox.\nMimicgen: A data generation system for scalable robot learning using human demonstrations.\narXiv preprint arXiv:2310.17596\n, 2023.\nMao et al. [2024]\nJiageng Mao, Siheng Zhao, Siqi Song, Tianheng Shi, Junjie Ye, Mingtong Zhang, Haoran Geng, Jitendra Malik, Vitor Guizilini, and Yue Wang.\nLearning from massive human videos for universal humanoid pose control.\narXiv preprint arXiv:2412.14172\n, 2024.\nMeng et al. [2024a]\nFanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo.\nTowards world simulator: Crafting physical commonsense-based benchmark for video generation.\narXiv preprint arXiv:2410.05363\n, 2024a.\nMeng et al. [2024b]\nFanqing Meng, Wenqi Shao, Lixin Luo, Yahong Wang, Yiran Chen, Quanfeng Lu, Yue Yang, Tianshuo Yang, Kaipeng Zhang, Yu Qiao, et al.\nPhybench: A physical commonsense benchmark for evaluating text-to-image models.\narXiv preprint arXiv:2406.11802\n, 2024b.\nNasiriany et al. [2024]\nSoroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu.\nRobocasa: Large-scale simulation of everyday tasks for generalist robots.\narXiv preprint arXiv:2406.02523\n, 2024.\nOpenAI [2024]\nOpenAI.\nSora, 2024.\nURL\nhttps://openai.com/sora/\n.\nAccessed: 2025-02-26.\nOpenAI [2025a]\nOpenAI.\nIntroducing gpt‚Äë5.\nhttps://openai.com/index/introducing-gpt-5/\n, 2025a.\nOpenAI [2025b]\nOpenAI.\nSora2, 2025b.\nURL\nhttps://openai.com/zh-Hans-CN/index/sora-2/\n.\nO‚ÄôNeill et al. [2024]\nAbby O‚ÄôNeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al.\nOpen x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 6892‚Äì6903. IEEE, 2024.\nPeebles and Xie [2023]\nWilliam Peebles and Saining Xie.\nScalable diffusion models with transformers.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, pages 4195‚Äì4205, 2023.\nPika [2025]\nPika.\nPika art 2.0‚Äôs scene ingredients: Redefining personalized video creation, 2025.\nURL\nhttps://pikartai.com/scene-ingredients/\n.\nAccessed: 2025-02-26.\nRen et al. [2024]\nTianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang.\nGrounded sam: Assembling open-world models for diverse visual tasks, 2024.\nRen et al. [2025]\nXuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas M√ºller, Alexander Keller, Sanja Fidler, and Jun Gao.\nGen3c: 3d-informed world-consistent video generation with precise camera control.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 6121‚Äì6132, 2025.\nRunway [2025]\nRunway.\nRunway, 2025.\nURL\nhttps://runwayml.com/\n.\nAccessed: 2025-02-26.\nSong et al. [2020]\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456\n, 2020.\nSun et al. [2025]\nKaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu.\nT2v-compbench: A comprehensive benchmark for compositional text-to-video generation.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 8406‚Äì8416, 2025.\nTeam et al. [2025a]\nGigaBrain Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jie Li, Jiagang Zhu, Lv Feng, et al.\nGigabrain-0: A world model-powered vision-language-action model.\narXiv preprint arXiv:2510.19430\n, 2025a.\nTeam et al. [2025b]\nMeituan LongCat Team, Xunliang Cai, Qilong Huang, Zhuoliang Kang, Hongyu Li, Shijun Liang, Liya Ma, Siyu Ren, Xiaoming Wei, Rixu Xie, and Tong Zhang.\nLongcat-video technical report, 2025b.\nURL\nhttps://arxiv.org/abs/2510.22200\n.\nTharwat et al. [2025]\nBahey Tharwat, Yara Nasser, Ali Abouzeid, and Ian Reid.\nLatent action pretraining through world modeling.\narXiv preprint arXiv:2509.18428\n, 2025.\nTian et al. [2024]\nYang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang.\nPredictive inverse dynamics models are scalable learners for robotic manipulation.\nIn\nThe Thirteenth International Conference on Learning Representations\n, 2024.\nTian et al. [2025]\nYang Tian, Yuyin Yang, Yiman Xie, Zetao Cai, Xu Shi, Ning Gao, Hangxu Liu, Xuekun Jiang, Zherui Qiu, Feng Yuan, et al.\nInterndata-a1: Pioneering high-fidelity synthetic data for pre-training generalist policy.\narXiv preprint arXiv:2511.16651\n, 2025.\nUnitree [2025]\nUnitree.\nUnifolm-wma-0: A world-model-action (wma) framework under unifolm family, 2025.\nWan et al. [2025]\nTeam Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al.\nWan: Open and advanced large-scale video generative models.\narXiv preprint arXiv:2503.20314\n, 2025.\nWang et al. [2025a]\nJing Wang, Ao Ma, Ke Cao, Jun Zheng, Zhanjie Zhang, Jiasong Feng, Shanyuan Liu, Yuhang Ma, Bo Cheng, Dawei Leng, et al.\nWisa: World simulator assistant for physics-aware text-to-video generation.\narXiv preprint arXiv:2503.08153\n, 2025a.\nWang et al. [2025b]\nYiping Wang, Xuehai He, Kuan Wang, Luyao Ma, Jianwei Yang, Shuohang Wang, Simon Shaolei Du, and Yelong Shen.\nIs your world simulator a good story presenter? a consecutive events-based benchmark for future long video generation.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 13629‚Äì13638, 2025b.\nWang et al. [2023]\nYufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, and Chuang Gan.\nRobogen: Towards unleashing infinite data for automated robot learning via generative simulation.\narXiv preprint arXiv:2311.01455\n, 2023.\nWang et al. [2024]\nZhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan.\nMotionctrl: A unified and flexible motion controller for video generation.\nIn\nACM SIGGRAPH 2024 Conference Papers\n, pages 1‚Äì11, 2024.\nWiedemer et al. [2025]\nThadd√§us Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos.\nVideo models are zero-shot learners and reasoners.\narXiv preprint arXiv:2509.20328\n, 2025.\nWu et al. [2025a]\nBing Wu, Chang Zou, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Jack Peng, Jianbing Wu, Jiangfeng Xiong, Jie Jiang, et al.\nHunyuanvideo 1.5 technical report.\narXiv preprint arXiv:2511.18870\n, 2025a.\nWu et al. [2023]\nHaoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Chunyi Li, Liang Liao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtai Zhai, and Weisi Lin.\nQ-align: Teaching lmms for visual scoring via discrete text-defined levels.\narXiv preprint arXiv:2312.17090\n, 2023.\nEqual Contribution by Wu, Haoning and Zhang, Zicheng. Project Lead by Wu, Haoning. Corresponding Authors: Zhai, Guangtai and Lin, Weisi.\nWu et al. [2025b]\nHongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong.\nUnleashing large-scale video generative pre-training for visual robot manipulation.\nIn\nThe Twelfth International Conference on Learning Representations\n, 2025b.\nWu et al. [2024a]\nKun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che, Xiaozhu Ju, Zhuqin Yang, Meng Li, Yinuo Zhao, Zhiyuan Xu, Guang Yang, et al.\nRobomind: Benchmark on multi-embodiment intelligence normative data for robot manipulation.\narXiv preprint arXiv:2412.13877\n, 2024a.\nWu et al. [2024b]\nPhilipp Wu, Yide Shentu, Zhongke Yi, Xingyu Lin, and Pieter Abbeel.\nGello: A general, low-cost, and intuitive teleoperation framework for robot manipulators.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, pages 12156‚Äì12163. IEEE, 2024b.\nWu et al. [2025c]\nShihan Wu, Xuecheng Liu, Shaoxuan Xie, Pengwei Wang, Xinghang Li, Bowen Yang, Zhe Li, Kai Zhu, Hongyu Wu, Yiheng Liu, et al.\nRobocoin: An open-sourced bimanual robotic data collection for integrated manipulation.\narXiv preprint arXiv:2511.17441\n, 2025c.\nYan et al. [2025]\nTianyi Yan, Wencheng Han, Xia Zhou, Xueyang Zhang, Kun Zhan, Cheng-zhong Xu, and Jianbing Shen.\nRlgf: Reinforcement learning with geometric feedback for autonomous driving video generation.\narXiv preprint arXiv:2509.16500\n, 2025.\nYang et al. [2025]\nLujie Yang, HJ Suh, Tong Zhao, Bernhard Paus Graesdal, Tarik Kelestemur, Jiuguang Wang, Tao Pang, and Russ Tedrake.\nPhysics-driven data generation for contact-rich manipulation via trajectory optimization.\narXiv preprint arXiv:2502.20382\n, 2025.\nYang et al. [2024]\nZhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al.\nCogvideox: Text-to-video diffusion models with an expert transformer.\narXiv preprint arXiv:2408.06072\n, 2024.\nYe et al. [2025]\nSeonghyeon Ye, Joel Jang, Byeongguk Jeon, Se June Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al.\nLatent action pretraining from videos.\nIn\nThe Thirteenth International Conference on Learning Representations\n, 2025.\nYuan et al. [2025]\nShenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin, Jiebo Luo, and Li Yuan.\nOpens2v-nexus: A detailed benchmark and million-scale dataset for subject-to-video generation.\narXiv preprint arXiv:2505.20292\n, 2025.\nZhang and Agrawala [2025]\nLvmin Zhang and Maneesh Agrawala.\nPacking input frame contexts in next-frame prediction models for video generation.\nArxiv\n, 2025.\nZhao et al. [2025]\nZhenyu Zhao, Hongyi Jing, Xiawei Liu, Jiageng Mao, Abha Jha, Hanwen Yang, Rong Xue, Sergey Zakharor, Vitor Guizilini, and Yue Wang.\nHumanoid everyday: A comprehensive robotic dataset for open-world humanoid manipulation.\narXiv preprint arXiv:2510.08807\n, 2025.\nZhen et al. [2025]\nHaoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, and Chuang Gan.\nTesseract: learning 4d embodied world models.\narXiv preprint arXiv:2504.20995\n, 2025.\nZhou et al. [2024]\nSiyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, and Chuang Gan.\nRobodreamer: Learning compositional world models for robot imagination.\narXiv preprint arXiv:2404.12377\n, 2024.\nZhu et al. [2025]\nChuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, and Abhishek Gupta.\nUnified world models: Coupling video and action diffusion for pretraining on large robotic datasets.\narXiv preprint arXiv:2504.02792\n, 2025.\nZhuang et al. [2025]\nJunhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, and Tianfan Xue.\nFlashvsr: Towards real-time diffusion-based streaming video super-resolution, 2025.\nURL\nhttps://arxiv.org/abs/2510.12747\n.\nRethinking Video Generation Model for the Embodied World\nAppendix\nAppendix A\nEvaluation Set Details\nA.1\nTask-Oriented Evaluation Set\nTo systematically evaluate the multi-dimensional task execution capabilities of video generation models in robotic scenarios, RBench constructs a task-oriented evaluation set with five core task dimensions:\nCommon Manipulation\n,\nLong-Horizon Planning\n,\nMulti-Entity Collaboration\n,\nSpatial Relationship\n, and\nVisual Reasoning\n. For each task, we collect 50 images as initial frames from open-source datasets or public web sources. Human annotators then create and verify corresponding text prompts to ensure both correctness and diversity of language descriptions. Together, these image-text pairs define a diverse evaluation corpus that covers a wide range of everyday manipulation, complex planning, multi-entity interaction, spatial reasoning, and visual-semantic reasoning scenarios.\nA.1.1\nCommon Manipulation\nThis task evaluates the ability of video generation models to produce diverse manipulation behaviors in basic object interaction scenarios. The scenes cover single-arm, dual-arm, and humanoid robots performing typical manipulation actions such as grasping, placing, pushing, rotating, and pressing. The dataset focuses on whether the model can generate physically plausible, temporally coherent, and natural manipulation behaviors that achieve the specified goals for everyday object handling.\nA.1.2\nLong-Horizon Planning\nThis task evaluates the capability of video generation models to understand and generate long-horizon robotic behaviors that involve multi-stage action planning. Each instance in the evaluation set is composed of multiple sequential sub-actions, including:\n‚Ä¢\nObject Manipulation Sequences:\ne.g., The robot opens the refrigerator door, takes the green box out of the refrigerator, and then closes the refrigerator door,‚Äù which requires clearly delineated action stages and physically reasonable transitions.\n‚Ä¢\nMulti-Step Spatial Planning:\ne.g., The humanoid robot picks up the bag, turns around, climbs up the stairs, and walks across the wooden plank,‚Äù emphasizing continuous spatial transitions and modeling of multi-step action chaining.\n‚Ä¢\nPhysical Motion and Body Coordination:\ne.g., ‚ÄúThe quadruped robot performs a front flip, lands steadily, then leans forward and balances upside down on its front legs,‚Äù which assesses temporal coherence and physical plausibility in complex motion and body control.\nOverall, the dataset spans a broad range of tasks from everyday interactions to dynamic control, focusing on a model‚Äôs capability in action decomposition, stage transitions, and cross-time reasoning for comprehensive long-horizon planning.\nA.1.3\nMulti-Entity Collaboration\nThis task focuses on evaluating the capability of video generation models to depict collaborative and interactive behaviors in multi-entity robotic scenarios. Each scene contains a\nPrimary Entity\nand a\nSecondary Entity\n. The Primary Entity can be a single-arm robot, a dual-arm robot, a humanoid robot, or a quadruped robot, while the Secondary Entity can be a human, an animal, or another robot. The task covers diverse interaction types such as object handover and usage, dressing assistance, collaborative task completion, following, and guidance. The dataset is designed to assess whether the model can generate natural, temporally coherent, and task-consistent multi-entity collaboration behaviors at semantic, temporal, and physical levels.\nA.1.4\nSpatial Relationship\nThis task evaluates the ability of video generation models to understand and express spatial relationships in generated videos. We construct scenes where humanoid robots, single-arm robots, and quadruped robots interact with clearly defined objects while satisfying various spatial relations, such as above/below, left/right, and front/behind. The dataset requires models to correctly present relative positions, orientations, and motion trajectories between entities, revealing their competence in spatial understanding and geometric reasoning. Consistent spatial layouts and motion patterns across time are essential to correctly reflect the described spatial relations.\nA.1.5\nVisual Reasoning\nThis task aims to evaluate the visual-semantic reasoning capabilities of video generation models in complex scenes. The evaluation set includes a wide range of visual concepts and multi-level semantic logic, such as:\n‚Ä¢\nColor recognition (e.g., pick up the sky-blue book);\n‚Ä¢\nNumerical and ordering reasoning (e.g., the robot places the apple, water bottle, and Rubik‚Äôs cube into the bag in that order);\n‚Ä¢\nAttribute and category matching (e.g., the robot gripper places the white bottle of baby powder onto the shelf, aligning it with other identical bottles in the same column);\n‚Ä¢\nGeometric and object-property understanding (e.g., the robot picks up the tallest orange object and places it into the basket);\n‚Ä¢\nText and semantic understanding (e.g., the left manipulator places the cup under the white dispenser labeled ‚ÄòJasmine Tea,‚Äô and the right manipulator opens the dispenser to pour jasmine tea into the cup);\n‚Ä¢\nVisual feature understanding (e.g., the robot grasps the book with a portrait of a person on its cover).\nThis dataset is designed to emphasize fine-grained visual grounding, logical consistency, and the ability to align robot actions with high-level visual-semantic reasoning requirements.\nA.2\nEmbodiment-Specific Evaluation Set\nDifferent types of robots exhibit substantial variations in morphology, degrees of freedom, control modes, and task objectives. These factors directly influence the modeling complexity and generalization challenges faced by video generation models in robotics contexts. To more systematically analyze model performance across heterogeneous robot embodiments, RBench constructs embodiment-specific evaluation subsets that encompass four representative robot categories: dual-arm robots, humanoid robots, single-arm robots, and quadruped robots. For each embodiment, 100 initial-frame images are sourced from open-access or publicly available datasets, and human annotators create and verify the corresponding prompts for accuracy and linguistic diversity.\nEach subset includes a diverse range of robot models, action types, manipulated objects, scene environments, and both first-person and third-person perspectives. This design introduces embodiment-specific challenges: dual-arm robots emphasize coordinated bimanual manipulation, humanoid robots prioritize tool use and natural full-body postures, single-arm robots focus on precise object interactions, and quadruped robots predominantly test terrain adaptation and motion continuity. Along with the task-oriented splits, these embodiment-specific subsets provide a comprehensive and structured dataset for benchmarking video generation models in the embodied world.\nEvaluating models across these four robot categories further uncovers the current biases and capability preferences of image-to-video generation models. For instance, due to extensive pre-training on large-scale human activity datasets, many models tend to exhibit higher task completion rates and better visual quality in humanoid-robot scenarios, while they often struggle with fine-grained single-arm manipulation. By systematically comparing performance across different embodiments, RBench makes such imbalances explicit and offers a principled framework to identify where current models excel or fail.\nMore broadly, embodiment-aware datasets like RBench are pivotal for advancing video foundation models in robotics. They promote the development of architectures and training strategies capable of generalizing beyond human-centric motion priors, enabling models to learn and adapt to a broader distribution of robot-specific motion patterns, rather than relying solely on human demonstrations. Furthermore, they facilitate fair and transparent comparisons between models, allowing evaluation results to clearly identify which models perform best for specific robot embodiments. Finally, such datasets help bridge the gap between generic video generation and physically grounded embodied intelligence, fostering the transition from visually appealing but brittle outputs toward robust, controllable, and deployment-ready generative models for real-world robotic systems.\nAppendix B\nAutomatic Metrics Details\nTo quantitatively assess the core capabilities of different models in robot video generation, we design five fine-grained metrics:\nPhysical-Semantic Plausibility\n,\nTask-Adherence Consistency\n,\nMotion Amplitude\n,\nRobot-Subject Stability\n, and\nMotion Smoothness\n. These metrics are evaluated using an MLLM-based, VQA-style protocol applied to grid images composed of key frames sampled from each generated video. Additionally, the evaluation is supplemented by low-level, non-MLLM computational indicators that capture pixel-level motion statistics and temporal dynamics. Together, these two layers of evaluation provide a comprehensive assessment of both task completion and visual quality in robotic video generation.\nB.1\nPhysical-Semantic Plausibility\nFigure 6\n:\nVisualization of robot and subject floating.\nIn robotics video generation, models often produce physically implausible or commonsense-violating artifacts, such as grippers passing through objects, floating objects, or the sudden appearance of irrelevant entities. These errors are typically undetectable by standard visual perception metrics, yet they directly highlight limitations in a model‚Äôs understanding of physical laws and semantic causality.\nFigure 7\n:\nVisualization of robot interpenetration.\nFigure 8\n:\nVisualization of robot/subject sudden appearance, disappearance, or duplication.\nTo capture these issues, we introduce the\nPhysical-Semantic Plausibility\nmetric, implemented via a VQA-style evaluation pipeline. The MLLM receives a grid image composed of key frames from the generated video and is prompted to detect the following types of violations:\n‚Ä¢\nFloating and unsupported entities.\nAs illustrated in Figure\n6\n, the metallic spoon and the single-arm robot‚Äôs manipulator are suspended in mid-air without any physically plausible support.\n‚Ä¢\nInterpenetration.\nAs shown in Figure\n7\n, the humanoid robot hand unrealistically penetrates the box, indicating a severe violation of rigid-body constraints.\n‚Ä¢\nSudden appearance, disappearance, or duplication.\nAs demonstrated in Figure\n8\n, (a) the robotic arm suddenly disappears in later frames, (b) human hands and a new notebook suddenly appear, and (c) the number of instant noodle packs is spuriously duplicated.\n‚Ä¢\nNon-contact attachment and incorrect grasping.\nFor example, in Figure\n6\n(a), the metallic spoon moves rigidly with the gripper even though there is no clear contact or gripper closure, resulting in an unrealistic ‚Äústicking‚Äù effect.\nThese anomalies are treated as severe physical violations that significantly reduce the credibility of the generated video. Beyond local error detection, the evaluator is also required to assess whether the overall action sequence and causal progression are reasonable, thereby characterizing the extent to which the model produces videos that are consistent with basic physical laws and human common sense.\nB.2\nTask-Adherence Consistency\nFigure 9\n:\nVisualization of task responsiveness and key actions completeness.\nRobotics video generation models often exhibit task-level deviations, such as ignoring the specified objective or omitting critical action stages. To measure this behavior, we design the\nTask-Adherence Consistency\nmetric, using a VQA-style evaluation protocol. The MLLM inspects the grid of key frames and assesses the following:\n‚Ä¢\nTask responsiveness.\nAs shown in Figure\n9\n(a), the failure case illustrates a robot gripper that does not respond to the instruction to grasp the mouse; the gripper remains static, and the intended task is never initiated or completed.\n‚Ä¢\nKey action completeness.\nAs illustrated in Figure\n9\n(b), the failure case omits crucial actions such as\nturn on\nand\nfill\n: the faucet is never visibly operated, yet water still flows from the tap, disrupting the causal chain between actions and outcomes.\nThese phenomena reflect shortcomings in semantic understanding, action planning, and execution consistency with respect to the prompt. Importantly, they are also difficult to capture with conventional low-level perception metrics, highlighting the necessity of explicit task-adherence evaluation in robotics contexts.\nConcretely, Task-Adherence Consistency is instantiated with task-specific criteria for the five task families introduced in Section\nA.1\n, with a focus on the following:\n‚Ä¢\nCommon Manipulation.\nTask adherence is primarily assessed through: (i)\nTask Completion\n, which checks whether the robot successfully accomplishes the manipulation objective described in the prompt while exhibiting reasonable intermediate phases (e.g., approach\n‚Üí\n\\rightarrow\ngrasp\n‚Üí\n\\rightarrow\nmove\n‚Üí\n\\rightarrow\nplace); and (ii)\nAction Effectiveness\n, which evaluates the physical plausibility and dynamic coherence of the manipulation, including natural gripper closure, appropriate contact locations, and smooth trajectories. Attempts with obviously discontinuous, incomplete, or physically implausible actions are regarded as failures.\n‚Ä¢\nMulti-Entity Collaboration.\nFor collaborative scenes involving a Primary and a Secondary Entity, task adherence is assessed through two aspects: (i)\nTask Completion\n, requiring that both entities execute their respective roles and complete all required interaction steps in a temporally coherent and logically consistent manner; and (ii)\nAction Effectiveness\n, which measures the completeness and coordination of interaction behaviors. For contact-based interactions, a full sequence of ‚Äúapproach\n‚Üí\n\\rightarrow\ncontact\n‚Üí\n\\rightarrow\nrelease/transfer‚Äù is expected; for non-contact interactions (e.g., following, joint motion), a coherent process of ‚Äúinitiation\n‚Üí\n\\rightarrow\nalignment\n‚Üí\n\\rightarrow\nsustained coordination‚Äù is required. Missing stages, asynchronous responses, or logically inconsistent behaviors are treated as unsuccessful.\n‚Ä¢\nSpatial Relationship.\nIn spatial reasoning scenarios, task adherence is assessed through: (i)\nSpatial Relation Accuracy\n, which checks whether the spatial relations between entities (e.g., above/below, left/right, front/behind) match the textual description with consistent orientation, scale, and viewpoint; and (ii)\nManipulation Feasibility\n, which examines whether the direction, trajectory, and intent of the robot‚Äôs motion are compatible with the described spatial relations (e.g., moving leftward when instructed to move ‚Äúto the left of‚Äù). Trajectories that contradict the described direction or result in physically unreasonable motions are considered incorrect.\n‚Ä¢\nVisual Reasoning.\nIn visually and semantically complex scenes, task adherence is assessed through two aspects: (i)\nVisual Reasoning Accuracy\n, evaluated via an automatic Question Chain mechanism: given the prompt, MLLM first generates a set of stepwise verification questions covering the trigger-feedback-outcome logic. The same MLLM then answers these questions based on the generated video, and a score is computed as follows:\nScore\n=\n5\n√ó\ncompleted questions\ntotal questions\n,\n\\text{Score}=5\\times\\frac{\\text{completed questions}}{\\text{total questions}},\n(4)\nwhere missing or incorrect events are treated as unfulfilled steps.\nThis encourages the video to satisfy both the visual and logical requirements of the task. Additionally, (ii)\nAction Effectiveness\nmeasures the physical plausibility and dynamic coherence of the robot‚Äôs motions, penalizing clearly discontinuous, incomplete, or physically implausible actions even if some high-level reasoning appears correct.\n‚Ä¢\nLong-Horizon Planning.\nFor long-horizon tasks composed of multiple ordered sub-events, task adherence is assessed through: (i)\nEvent Completion Rate\n. For each sample, an event list is defined as an ordered set of events. This list is reformulated into a numbered sequence, e.g.,\n1. open the refrigerator door; 2. take out the green box; 3. close the door\n, and the final score is computed as follows:\nScore\n=\n5\n√ó\ncompleted events\ntotal events\n,\n\\text{Score}=5\\times\\frac{\\text{completed events}}{\\text{total events}},\n(5)\nthis measures how completely the required event sequence is executed. Complementarily, (ii)\nAction Effectiveness\nagain assesses the physical plausibility and temporal coherence of the underlying motions (e.g., natural body coordination, stable landing, and smooth transitions between stages), ensuring that partially correct high-level event ordering without valid execution is not over-rewarded.\nB.3\nRobot-Subject Stability\nFigure 10\n:\nVisualization of robot structural stability.\nFigure 11\n:\nVisualization of subject appearance stability.\nIn robotics video generation, maintaining stable structure and appearance for both the robot and the manipulated objects is essential for assessing generation quality. In practice, models often exhibit abnormal changes in robot morphology or severe distortions of subject attributes. To systematically evaluate these issues, we propose the\nRobot-Subject Stability\nmetric, which separately measures the visual and semantic consistency of the robot and the target subject throughout the generation process.\nWe adopt a comparative VQA mechanism: the system simultaneously observes two frames, with the left frame as a reference image and the right frame as a generated frame, and focuses on a specified entity (e.g., the\nrobotic gripper\nor the\ntarget subject\n). The MLLM is prompted to judge how well the entity‚Äôs appearance, structure, and semantics are preserved between the two frames. Specifically, the evaluator identifies:\n‚Ä¢\nRobot structural stability.\nAs shown in Figure\n10\n, (a) a humanoid robot degenerates into a single-arm robot, (b) a quadruped robot morphs into a small humanoid robot, (c) a single-arm robot transforms into a humanoid robot, and (d) a parallel gripper deforms into a dexterous robotic hand. These cases reveal structural drift and inconsistency in robot embodiment over time.\n‚Ä¢\nSubject appearance stability.\nAs illustrated in Figure\n11\n, (a) a rectangular knitted sleeve transforms into a long-sleeve sweater, and (b) a green plastic cup on the table becomes a round yellow mug, indicating a loss of identity-preserving appearance.\nBeyond these explicit examples, we also observe a range of additional anomalies: changes in the number of robot links or arms during task execution, the spontaneous generation of extra manipulators, and unnatural variations in arm length, connectivity, or joint bending direction over time. Target objects may also undergo unrealistic material changes, such as a rigid object bending like a deformable one. The Robot-Subject Stability metric is designed to capture such inconsistencies, providing a focused measure of whether the model can preserve both robot morphology and object identity across the video sequence.\nB.4\nMotion Amplitude\nFigure 12\n:\nVisualization of robot motion amplitude.\nMotivation.\nA common failure mode in robotic video generation is that the robot remains nearly static while the generated frames appear visually smooth, as illustrated in Figure\n12\n(a)(b).\nThis makes pure smoothness-based metrics insufficient.\nFollowing the perceptual motion estimation idea introduced in VMBench\n[\n64\n]\n, a\nMotion Amplitude Score (MAS)\nis used to measure the perceptible dynamic behavior of the robot while explicitly compensating for camera motion.\nRobot Localization and Tracking.\nThe robot is first localized using GroundingDINO, and temporally stable segmentation masks are obtained via SAM2. CoTracker is then used to track a dense grid of keypoints inside the robot mask, ensuring that the estimated motion truly reflects robot articulation rather than background drift or mask leakage.\nFrame-Level Motion.\nLet\nùê©\nt\n,\nk\n\\mathbf{p}_{t,k}\ndenote the 2D location of the\nk\nk\n-th tracked point at frame\nt\nt\n.\nThe raw frame-to-frame displacement is computed as:\nD\n¬Ø\nt\n=\n1\nK\n‚Äã\n‚àë\nk\n=\n1\nK\n‚Äñ\nùê©\nt\n,\nk\n‚àí\nùê©\nt\n‚àí\n1\n,\nk\n‚Äñ\n2\n.\n\\bar{D}_{t}=\\frac{1}{K}\\sum_{k=1}^{K}\\bigl\\lVert\\mathbf{p}_{t,k}-\\mathbf{p}_{t-1,k}\\bigr\\rVert_{2}.\n(6)\nTo ensure consistency across resolutions, the motion is normalized by the video diagonal:\nD\n~\nt\n=\nD\n¬Ø\nt\nW\n2\n+\nH\n2\n.\n\\tilde{D}_{t}=\\frac{\\bar{D}_{t}}{\\sqrt{W^{2}+H^{2}}}.\n(7)\nCamera-Motion Compensation.\nTo estimate camera-induced movement, the robot mask is inverted and the same tracking procedure is applied to the background region.\nLet\nD\n~\nt\nbg\n\\tilde{D}^{\\mathrm{bg}}_{t}\ndenote the normalized background motion.\nA\nsoft-zero\nstrategy is adopted: if the robot motion does not exceed the background motion, the small residual value is retained:\nD\n^\nt\n=\n{\nD\n~\nt\n‚àí\nD\n~\nt\nbg\n,\nD\n~\nt\n>\nD\n~\nt\nbg\n,\nD\n~\nt\n,\nD\n~\nt\n‚â§\nD\n~\nt\nbg\n.\n\\hat{D}_{t}=\\begin{cases}\\tilde{D}_{t}-\\tilde{D}^{\\mathrm{bg}}_{t},&\\tilde{D}_{t}>\\tilde{D}^{\\mathrm{bg}}_{t},\\\\[4.0pt]\n\\tilde{D}_{t},&\\tilde{D}_{t}\\leq\\tilde{D}^{\\mathrm{bg}}_{t}.\\end{cases}\n(8)\nThis behavior matches our implementation and improves robustness against tracking noise or partial occlusion, while effectively treating the robot as ‚Äústatic‚Äù.\nFinal Score.\nFinally, following VMBench, the compensated displacement is clipped to stabilize extreme values:\nMAS\n=\n1\nT\n‚Äã\n‚àë\nt\n=\n1\nT\nmin\n‚Å°\n(\nD\n^\nt\n,\n1\n)\n.\n\\mathrm{MAS}=\\frac{1}{T}\\sum_{t=1}^{T}\\min\\bigl(\\hat{D}_{t},\\,1\\bigr).\n(9)\nDiscussion.\nMAS captures whether the robot exhibits meaningful articulation rather than merely inheriting background or camera movement.\nBy incorporating localization, mask-based tracking, background compensation, and a soft-zero strategy, MAS remains stable across scenes, tracking configurations, and robotic embodiments.\nB.5\nMotion Smoothness\nFigure 13\n:\nVisualization of robot motion smoothness.\nThis metric evaluates the temporal continuity and naturalness of motion, aiming to detect frame-level discontinuities such as low-level temporal artifacts and high-level motion blur. As illustrated in Figure\n13\n, various robot embodiments, including quadruped robots, humanoids, and single-arm manipulators, exhibit different degrees of motion-induced distortion that substantially degrade perceived video quality.\nThe assessment is based on the motion-smoothness principle introduced in VMBench\n[\n64\n]\n, with temporal consistency estimated using Q-Align aesthetic quality scores. For each video, frames are processed with a sliding window of size\nw\nw\n(default\nw\n=\n3\nw=3\n). Each window is fed into Q-Align to obtain a per-frame quality score sequence\n{\nQ\nt\n}\nt\n=\n1\nT\n\\{Q_{t}\\}_{t=1}^{T}\n. Temporal quality fluctuation is then measured by the magnitude of adjacent-frame differences:\nŒî\n‚Äã\nQ\nt\n=\n|\nQ\nt\n‚àí\nQ\nt\n‚àí\n1\n|\n,\nt\n=\n2\n,\n‚Ä¶\n,\nT\n.\n\\Delta Q_{t}=\\lvert Q_{t}-Q_{t-1}\\rvert,\\qquad t=2,\\dots,T.\n(10)\nTo ensure comparability across videos with different motion intensities, the threshold for detecting abnormal temporal variations is determined by the\nMotion Amplitude\nvalue\nm\nm\ndefined in Section\nB.4\n. A piecewise adaptive threshold function is used:\nœÑ\ns\n‚Äã\n(\nm\n)\n=\n{\n0.01\n,\nm\n<\n0.1\n,\n0.015\n,\n0.1\n‚â§\nm\n<\n0.3\n,\n0.025\n,\n0.3\n‚â§\nm\n<\n0.5\n,\n0.03\n,\nm\n‚â•\n0.5\n.\n\\tau_{s}(m)=\\begin{cases}0.01,&m<0.1,\\\\[2.0pt]\n0.015,&0.1\\leq m<0.3,\\\\[2.0pt]\n0.025,&0.3\\leq m<0.5,\\\\[2.0pt]\n0.03,&m\\geq 0.5.\\end{cases}\n(11)\nLower-motion videos therefore adopt a stricter threshold for detecting subtle temporal inconsistencies, while higher-motion videos receive a relaxed threshold to avoid penalizing naturally rapid movements. The function is determined through grid search on a validation split to ensure reproducibility.\nA frame\nt\nt\nis marked as temporally abnormal if its score fluctuation exceeds the adaptive threshold:\nI\nt\n=\nùïÄ\n‚Äã\n[\nŒî\n‚Äã\nQ\nt\n>\nœÑ\ns\n‚Äã\n(\nm\n)\n]\n,\nI_{t}=\\mathbb{I}[\\Delta Q_{t}>\\tau_{s}(m)],\n(12)\nwhere\nùïÄ\n‚Äã\n[\n‚ãÖ\n]\n\\mathbb{I}[\\cdot]\ndenotes the indicator function. To robustly capture abrupt artifacts such as frame drops or transient distortions, adjacent frames of each abnormal index are also flagged.\nThe final Motion Smoothness Score (MSS) is computed as the proportion of ‚Äúnormal‚Äù frames in the entire sequence:\nMSS\n=\n1\n‚àí\n1\nT\n‚Äã\n‚àë\nt\n=\n2\nT\nI\nt\n.\n\\mathrm{MSS}=1-\\frac{1}{T}\\sum_{t=2}^{T}I_{t}.\n(13)\nA higher MSS indicates smoother and more temporally coherent motion, whereas videos with frequent jitter, abrupt discontinuities, or artifact-heavy transitions yield lower MSS values.\nB.6\nScore Aggregation\nWe consolidate five fine-grained evaluation signals into two final indicators,\nTask Completion\nand\nVisual Quality\n.\nNotation.\nWe denote the normalized values of the five fine-grained metrics as follows:\n(1)\nPSS\n: Physical-Semantic Plausibility,\n(2)\nTAC\n: Task-Adherence Consistency,\n(3)\nRSS\n: Robot-Subject Stability,\n(4)\nMS\n: Motion Smoothness,\n(5)\nMA\n: Motion Amplitude.\nNormalization.\nGiven a raw metric value\ns\ns\ndefined over range\n[\ns\nmin\n,\ns\nmax\n]\n[s_{\\min},s_{\\max}]\n, its normalized value is\ns\n‚Üê\nclip\n[\n0\n,\n1\n]\n‚Äã\n(\ns\n‚àí\ns\nmin\ns\nmax\n‚àí\ns\nmin\n)\n.\ns\\leftarrow\\mathrm{clip}_{[0,1]}\\!\\left(\\frac{s-s_{\\min}}{s_{\\max}-s_{\\min}}\\right).\n(14)\nPenalty terms.\nTwo penalty terms are used to down-weight videos with insufficient subject motion or unstable visual composition.\nMotion-amplitude penalty. Let MA denote the normalized motion amplitude. A soft penalty is applied when MA falls below the threshold\nt\nt\n:\nP\nMA\n‚Äã\n(\nMA\n)\n=\n{\n(\nt\n‚àí\nMA\n)\n+\nŒ¥\n,\nMA\n<\nt\nlow\n,\nt\n‚àí\nMA\n,\nt\nlow\n‚â§\nMA\n<\nt\n,\n0\n,\nMA\n‚â•\nt\n,\nP_{\\mathrm{MA}}(\\mathrm{MA})=\\begin{cases}(t-\\mathrm{MA})+\\delta,&\\mathrm{MA}<t_{\\mathrm{low}},\\\\[3.0pt]\nt-\\mathrm{MA},&t_{\\mathrm{low}}\\leq\\mathrm{MA}<t,\\\\[3.0pt]\n0,&\\mathrm{MA}\\geq t,\\end{cases}\n(15)\nwith\nt\n=\n0.1\nt=0.1\n,\nt\nlow\n=\n0.05\nt_{\\mathrm{low}}=0.05\n, and\nŒ¥\n=\n0.1\n\\delta=0.1\n.\nStability-consistency penalty. Robot and object level stability grades are mapped to penalty magnitudes:\np\n‚Äã\n(\ng\n)\n‚àà\n{\n0.2\n,\n0.4\n,\n0.6\n,\n0.8\n}\nfor grades\n‚Äã\ng\n‚àà\n{\nB\n,\nC\n,\nD\n,\nE\n}\n,\np(g)\\in\\{0.2,0.4,0.6,0.8\\}\\quad\\text{for grades }g\\in\\{B,C,D,E\\},\n(16)\nwhile grade A incurs zero penalty.\nLet\ng\nr\ng_{r}\nand\ng\no\ng_{o}\ndenote robot- and object-related stability grades:\nP\nRSS\n=\n{\np\n‚Äã\n(\ng\nr\n)\n+\np\n‚Äã\n(\ng\no\n)\n2\n,\nif both exist\n,\np\n‚Äã\n(\ng\nr\n)\n,\nif only\n‚Äã\ng\nr\n‚Äã\nexists\n,\n0\n,\notherwise\n.\nP_{\\mathrm{RSS}}=\\begin{cases}\\dfrac{p(g_{r})+p(g_{o})}{2},&\\text{if both exist},\\\\[6.0pt]\np(g_{r}),&\\text{if only }g_{r}\\text{ exists},\\\\[4.0pt]\n0,&\\text{otherwise}.\\end{cases}\n(17)\nFinal indicators.\nTask Completion (TC). Task correctness is computed from Physical-Semantic Plausibility (PSS) and Task-Adherence Consistency (TAC):\nTC\n=\nPSS\n+\nTAC\n2\n.\n\\mathrm{TC}=\\frac{\\mathrm{PSS}+\\mathrm{TAC}}{2}.\n(18)\nVisual Quality (VQ). Visual realism and temporal coherence are expressed as a weighted combination of RSS and MS, penalized by low motion amplitude and visual instability:\nVQ\n=\nmax\n‚Å°\n(\n0\n,\n0.8\n‚ãÖ\nRSS\n+\n0.2\n‚ãÖ\nMS\n‚àí\nP\nMA\n‚Äã\n(\nMA\n)\n‚àí\nP\nRSS\n)\n.\n\\mathrm{VQ}=\\max\\!\\Bigl(0,\\;0.8\\cdot\\mathrm{RSS}+0.2\\cdot\\mathrm{MS}-P_{\\mathrm{MA}}(\\mathrm{MA})-P_{\\mathrm{RSS}}\\Bigr).\n(19)\nModel-level aggregation.\nFor each model, TC and VQ are computed for all evaluation samples.\nThe final model score corresponds to the mean values of TC and VQ across samples,\nwhich are used for quantitative comparison and ranking in our benchmark.\nAppendix C\nModel Descriptions and Implementation Setups\nC.1\nCommercial Models\nWan 2.6.\nWan is a comprehensive family of open-source foundational video generative models built on the Diffusion Transformer architecture. It supports multiple downstream tasks including T2V, I2V, editing, inpainting, and video-to-audio. We use the official API to generate 5-second 720P videos at 30 fps.\nWan 2.5.\nWan is a comprehensive family of open-source foundational video generative models built on the Diffusion Transformer architecture. It supports multiple downstream tasks including T2V, I2V, editing, inpainting, and video-to-audio. We use the official API to generate 5-second 720P videos at 24 fps.\nHailuo.\nHailuo provides multimodal models for T2V, I2V, and T2A tasks, supporting resolutions up to 1080p and long-duration outputs with high temporal coherence. We generate 6 second videos at 1364 √ó 768 and 24 fps using the official Hailuo API.\nVeo3.\nVeo 3 is Google‚Äôs latest foundational video generation model supporting high-resolution (1080p), long-duration (up to 60 seconds), and audio-integrated video synthesis using a large-scale Diffusion Transformer. We use the official Veo 3 API to generate videos up to 720p, 8 seconds, and 24 fps.\nKling 2.6 pro.\nWe use the official Kling 2.6 pro model with default parameters to generate a 5-second video at 1920 √ó 1080 resolution and 24 fps.\nSeedance 1.0.\nSeedance 1.0 is a large-scale video generation model from ByteDance, supporting text-to-video and image-to-video generation with high aesthetic quality and temporal stability. It integrates a 3D causal VAE with a 4√ó16√ó16 compression ratio. We use the Seedance 1.0 model to generate 5-second videos at 1280 √ó 720 resolution and 24 fps.\nSeedance 1.5.\nSeedance 1.5 is a large-scale video generation model from ByteDance, supporting text-to-video and image-to-video generation with high aesthetic quality and temporal stability. We use the Seedance 1.5 model to generate 5-second videos at 1280 √ó 720 resolution and 24 fps.\nSora v1.\nWe use the official Sora v1 model with default parameters to generate a 5-second video at 1280 √ó 720 resolution and 30 fps.\nSora v2 Pro.\nWe use the official Sora v2 Pro model with default parameters to generate a 4-second video at 1280 √ó 720 resolution and 30 fps.\nC.2\nOpen-source Models\nWan2.2_A14B.\nWan2.2_A14B is an open-source large-scale video generation model that incorporates a Mixture-of-Experts (MoE) architecture. This architecture dynamically allocates specialized expert networks to enhance the model‚Äôs capacity and temporal understanding. It supports multimodal inputs (text and images) for generating open-domain videos at 1280 √ó 720 resolution (720P), with a typical duration of 5 seconds (120 frames) at 24 fps. Compared to its predecessors, Wan2.2_A14B demonstrates superior performance in generalizing across diverse scenes, modeling complex motions, and achieving fine-grained aesthetic control. The technical report and model weights are publicly available. We utilize the official Wan2.2_A14B model with its default parameters to generate a 5-second video (81 frames) at a spatial resolution of 1280 √ó 720 and a frame rate of 16 fps. The model supports both text and image inputs.\nLongCat-Video.\nLongCat-Video is an open-source foundational video generation model with 13.6B parameters, developed by the Meituan LongCat Team. It unifies text-to-video (T2V), image-to-video (I2V), and video continuation (VC) tasks within a single Diffusion Transformer (DiT) architecture, supporting efficient minute-long video generation without quality degradation. The model employs multi-reward RLHF optimization (Group Relative Policy Optimization) to enhance visual quality, motion coherence, and text alignment. We employ the official LongCat-Video model with default configurations, generating 1280 √ó 704 resolution videos at 15 fps using a coarse-to-fine generation strategy.\nWan2.2_5B.\nWan2.2_5B is a medium-scale model in the Wan2.2 series with 5 billion parameters, utilizing a Transformer backbone architecture. It supports multi-modal video generation from text and images, generating 5-second (121 frames) open-domain videos at 1280 √ó 720 and 24 fps. Model weights and documentation are fully open-sourced. We use the official Wan2.2_5B model with default settings to generate a 5-second video (120 frames) at a resolution of 1248 √ó 704 and 24 fps.\nWan2.1_14B.\nWan2.1_14B is an early large-parameter video generation model in the Wan series with 14 billion parameters, based on a multi-modal diffusion architecture. It generates 5-second (120-frame) open-domain videos at 1280 √ó 720 and 24 fps, emphasizing complex scene modeling and object motion understanding. We use the official Wan2.1_14B model with default parameters to generate a 5-second video (81 frames) at 832 √ó 480 resolution and 16 fps.\nHunyuanVideo.\nWe use the official HunyuanVideo model with default parameters to generate a 5-second video at 1248 √ó 704 resolution and 24 fps.\nHunyuanVideo 1.5.\nWe use the official HunyuanVideo 1.5 model with default parameters to generate a 5-second video at 848 √ó 480 resolution and 24 fps.\nSkyReels-V2.\nSkyReels-V2 is an open-source infinite-length film generative model developed by Skywork AI. It supports text-to-video, image-to-video, and video continuation tasks, with modules including SkyCaptioner-V1, multi-stage pretraining, RL for motion quality, and a diffusion forcing framework for long video synthesis. We use the official SkyReels-V2 model with default settings to generate 960 √ó 544 resolution videos at 24 fps for approximately 4 seconds.\nLTX-Video.\nLTX-Video is an open-source transformer-based latent diffusion model developed by Lightricks. It integrates a Video-VAE and denoising transformer with a 1:192 compression ratio using spatiotemporal downscaling, enabling efficient latent-space processing. It supports both text-to-video and image-to-video generation. We use the official LTX-Video model with default parameters to generate a 5-second video at 832 √ó 480 resolution and 16 fps.\nLTX-2.\nLTX-2 is an open-source transformer-based latent diffusion model. We use the official LTX-2 model with default parameters to generate a 5-second video at 1536 √ó 1024 resolution and 24 fps.\nFramePack.\nFramePack is a neural structure for next-frame prediction designed to avoid forgetting and drifting in video generation. It compresses input frames by importance to maintain a fixed transformer context, enabling long video synthesis with low computational cost. It supports T2V and I2V and can be integrated with models such as HunyuanVideo or Wan. We employ FramePack with a base model to generate 5-second videos at 832 √ó 480 resolution.\nCogVideoX.\nCogVideoX is an open-source text-to-video generation model employing a diffusion transformer and a 3D VAE. It introduces expert-adaptive LayerNorm and multi-resolution frame packing to enhance motion consistency. It produces up to 10-second videos at 720 √ó 1280 and 16 fps.\nWe use the official CogVideoX weights and inference scripts with default settings to generate 6-second videos (160 frames) at 720 √ó 480 and 8 fps.\nC.3\nRobotics-specific Models\nCosmos 2.5.\nCosmos-Predict2.5 and Cosmos-Transfer2.5 are NVIDIA‚Äôs world simulation foundation models for physical AI, supporting Text2World, Image2World, and Video2World simulations with flow-based architectures.\nWe generate videos at 1280 √ó 720 resolution and 16 fps using official NVIDIA codebase.\nDreamGen (GR).\nGRL is DreamGen‚Äôs generalizable robot learner model integrating synthetic neural trajectories and real teleoperation data for policy learning. We generate all trajectory videos at 768 √ó 432 resolution and 16 fps.\nDreamGen (DROID).\nDROID is a dynamics-aware imitation learning model using video diffusion with inverse dynamics modeling for realistic physical trajectories.\nWe generate all videos at up to 768 √ó 432 resolution and 16 fps.\nUnifoLM-WMA-0.\nUnifoLM-WMA-0 integrates video world modeling and action policy learning for generalizable robotic reasoning and control.\nWe generate videos up to 320 √ó 512 resolution, 30 fps.\nVidar.\nVidar is an embodied video diffusion model designed for robotic manipulation, integrating a strong video diffusion prior and a Masked Inverse Dynamics Model (MIDM).\nVidar generates 4-second multi-view interactive videos at 704 √ó 480 resolution and 15 fps with automatically annotated action trajectories.\nAppendix D\nHuman Preference Study Details\nFigure 14\n:\nVisualization of the Questionnaire for User Study\nTo complement the main paper‚Äôs human preference analysis, this section provides additional statistical examinations that further characterize the agreement between human judgments and our automatic benchmark. The questionnaire used in our human preference study is shown in Figure\n14\n. As described in the main paper, thirty participants compared pairs of generated videos for the same prompt and selected the better one (or \"Tie\"). These votes were converted into per-model human scores using the\n5\n/\n3\n/\n1\n5/3/1\nwin-tie-loss scheme, and the resulting model-level ranking exhibited a strong correlation with RBench (\nœÅ\n=\n0.96\n\\rho=0.96\n).\nWhile rank correlation quantifies the consistency of\nordering\n, it does not measure whether the two scoring methods agree in an\nabsolute\nsense (i.e., whether they assign comparable magnitudes). To assess this complementary notion of agreement, we conduct a Bland-Altman analysis between human scores and benchmark scores. Because the two score scales may differ by a systematic offset or slope, we additionally apply a leave-one-out (LOO) linear calibration to correct for potential scale mismatch without overfitting. The following subsections detail the calibration procedure, Bland-Altman computation, accompanying figures, and per-model statistics.\nFigure 15\n:\nBland‚ÄìAltman plot after linear leave-one-out calibration (\nH\n‚àó\nH^{\\ast}\n).\nPoints are models; x-axis\nm\ni\n=\nB\ni\n+\nH\ni\n‚àó\n2\nm_{i}=\\tfrac{B_{i}+H_{i}^{\\ast}}{2}\n, y-axis\nd\ni\n=\nB\ni\n‚àí\nH\ni\n‚àó\nd_{i}=B_{i}-H_{i}^{\\ast}\n.\nThe solid line indicates the bias (\nd\n¬Ø\n\\bar{d}\n); dashed lines show the 95% limits of agreement (LoA).\nIn our study the legend reports\nBias\n=\n0.002\n\\text{Bias}=0.002\nand\nLoA\n=\n[\n‚àí\n0.108\n,\n0.112\n]\n\\text{LoA}=[-0.108,\\,0.112]\n.\nBland‚ÄìAltman basics.\nBland‚ÄìAltman analysis assesses\nagreement\nbetween two measurement methods by plotting, for each item\ni\ni\n, the difference\nd\ni\n=\nB\ni\n‚àí\nH\ni\nd_{i}=B_{i}-H_{i}\nagainst the mean\nm\ni\n=\nB\ni\n+\nH\ni\n2\nm_{i}=\\tfrac{B_{i}+H_{i}}{2}\n, where\nH\nH\nand\nB\nB\ndenote human and automatic scores, respectively.\nAs shown in Fig.\n15\n, the horizontal solid line represents the\nbias\nd\n¬Ø\n=\n1\nn\n‚Äã\n‚àë\ni\nd\ni\n\\bar{d}=\\tfrac{1}{n}\\sum_{i}d_{i}\n(average difference).\nThe dashed lines denote the\n95% limits of agreement (LoA)\n:\nd\n¬Ø\n¬±\n1.96\n‚Äã\ns\nd\n\\bar{d}\\pm 1.96\\,s_{d}\n, where\ns\nd\ns_{d}\nis the standard deviation of the differences.\nNarrower LoA and small bias indicate stronger agreement beyond mere correlation.\nLinear LOO calibration.\nHuman and automatic scores can exhibit\nscale\nmismatch.\nTo mitigate such systematic differences, we apply a\nleave-one-out (LOO) linear calibration\nto the benchmark.\nMathematical formulation.\nLet\n{\n(\nB\ni\n,\nH\ni\n)\n}\ni\n=\n1\nn\n\\{(B_{i},H_{i})\\}_{i=1}^{n}\nbe the automatic benchmark and human scores for\nn\nn\nmodels.\nFor each\ni\n‚àà\n{\n1\n,\n‚Ä¶\n,\nn\n}\ni\\in\\{1,\\dots,n\\}\n, define the training index set\nS\n‚àí\ni\n=\n{\nj\n:\nj\n‚â†\ni\n}\nS_{-i}=\\{j:\\,j\\neq i\\}\nand estimate the OLS (Ordinary Least Squares) calibration parameters by\n(\nŒ±\n^\n‚àí\ni\n,\nŒ≤\n^\n‚àí\ni\n)\n=\narg\n‚Å°\nmin\nŒ±\n,\nŒ≤\n‚Äã\n‚àë\nj\n‚àà\nS\n‚àí\ni\n(\nB\nj\n‚àí\nŒ±\n‚àí\nŒ≤\n‚Äã\nH\nj\n)\n2\n.\n(\\hat{\\alpha}_{-i},\\hat{\\beta}_{-i})\\penalty 10000\\ =\\penalty 10000\\ \\arg\\min_{\\alpha,\\beta}\\sum_{j\\in S_{-i}}\\bigl(B_{j}-\\alpha-\\beta H_{j}\\bigr)^{2}.\n(20)\nThe calibrated (benchmark-aligned) score for model\ni\ni\nis\nH\ni\n‚àó\n=\nŒ±\n^\n‚àí\ni\n+\nŒ≤\n^\n‚àí\ni\n‚Äã\nB\ni\n.\nH_{i}^{\\ast}\\;=\\;\\hat{\\alpha}_{-i}+\\hat{\\beta}_{-i}\\,B_{i}.\n(21)\nBland‚ÄìAltman with calibrated scores.\nWe then perform Bland‚ÄìAltman analysis on\n(\nB\n,\nH\n‚àó\n)\n(B,H^{\\ast})\nby computing, for each\ni\ni\n,\nd\ni\n=\nB\ni\n‚àí\nH\ni\n‚àó\n,\nm\ni\n=\nB\ni\n+\nH\ni\n‚àó\n2\n.\nd_{i}\\;=\\;B_{i}-H_{i}^{\\ast},\\qquad m_{i}\\;=\\;\\tfrac{B_{i}+H_{i}^{\\ast}}{2}.\n(22)\nThe bias\nd\n¬Ø\n\\bar{d}\nand the 95% LoA\nd\n¬Ø\n¬±\n1.96\n‚Äã\ns\nd\n\\bar{d}\\pm 1.96\\,s_{d}\nare computed from\n{\nd\ni\n}\ni\n=\n1\nn\n\\{d_{i}\\}_{i=1}^{n}\nas in the basics above.\nFigure\n15\nshows the Bland-Altman plot after linear LOO calibration, with the bias and LoA reported in the legend.\nTable\n5\nlists, for each model, the mean\nm\ni\nm_{i}\n, the difference\nd\ni\nd_{i}\n, and the corresponding LOO calibration parameters\nŒ±\n^\n‚àí\ni\n\\hat{\\alpha}_{-i}\nand\nŒ≤\n^\n‚àí\ni\n\\hat{\\beta}_{-i}\nused to obtain\nH\ni\n‚àó\nH_{i}^{\\ast}\n.\nAs a complement to the correlation analysis (Spearman\nœÅ\n=\n0.96\n\\rho=0.96\nbetween human scores and the automatic benchmark), the Bland-Altman view quantifies absolute agreement: the small bias and tight LoA indicate that the calibrated benchmark scores are in close agreement with human judgments.\nModel\nmean\n‚Äã\n(\nB\n,\nH\n‚àó\n)\n\\mathrm{mean}(B,H^{\\ast})\ndiff\n‚Äã\n(\nB\n‚àí\nH\n‚àó\n)\n\\mathrm{diff}(B-H^{\\ast})\nŒ±\nLOO\n\\alpha_{\\mathrm{LOO}}\nŒ≤\nLOO\n\\beta_{\\mathrm{LOO}}\nLTX-Video\n0.295\n0.098\n0.141\n0.613\nHailuo\n0.539\n0.052\n0.205\n0.496\nSeedance 1.0\n0.528\n0.046\n0.203\n0.500\nVeo 3\n0.551\n0.024\n0.204\n0.503\nWan2.1 14b\n0.388\n0.021\n0.193\n0.524\nWan2.5\n0.571\n-0.003\n0.199\n0.517\nCosmos2.5\n0.482\n-0.036\n0.200\n0.523\nLongCat-Video\n0.458\n-0.043\n0.202\n0.520\nDreamgen\n0.451\n-0.062\n0.203\n0.522\nCogvideox-5B\n0.295\n-0.077\n0.242\n0.446\nTable 5\n:\nBland‚ÄìAltman statistics after linear LOO calibration.\nm\ni\n=\nB\ni\n+\nH\ni\n‚àó\n2\nm_{i}=\\tfrac{B_{i}+H_{i}^{\\ast}}{2}\n,\nd\ni\n=\nB\ni\n‚àí\nH\ni\n‚àó\nd_{i}=B_{i}-H_{i}^{\\ast}\n.\nŒ±\nLOO\n\\alpha_{\\mathrm{LOO}}\nand\nŒ≤\nLOO\n\\beta_{\\mathrm{LOO}}\nare the intercept and slope learned from all\nother\nmodels when calibrating\nH\ni\n‚àó\nH_{i}^{\\ast}\n.\nAll values are rounded to three decimals.\nAppendix E\nPrompt Template\nThis section introduces the prompt design used for the Visual Reasoning task in our MLLM-based evaluation pipeline. Among all tasks, Visual Reasoning involves the most structured form of reasoning and thus provides a representative example for illustrating our prompt design. We provide the MLLM with essential contextual information recorded during dataset construction, including the video viewpoint, a high-level content description, and the identities of the robotic manipulator and manipulated object. This background knowledge serves as the foundation for the model‚Äôs subsequent reasoning and scoring process.\nUnlike other tasks, Visual Reasoning requires explicit verification of logical dependencies between robot actions. Therefore, it adopts a two-part structure consisting of a question-chain constructor and a video assessment prompt. The full templates are shown below.\nFigure 16\n:\nQuestion-chain construction for Visual Reasoning.\nThis component analyzes the original instruction and transforms it into a sequence of binary verification questions that define causal and temporal dependencies in the robot‚Äôs intended actions.\nQuestion-chain construction.\nThe first component converts the original text instruction into a sequence of binary verification questions (Figure\n16\n).\nThe model analyzes the semantics of the instruction and generates a short chain of stepwise questions that reflect the intended causal and temporal structure of the robot‚Äôs behavior.\nThis question chain acts as an explicit reasoning scaffold and is subsequently fed into the main evaluation prompt.\nFigure 17\n:\nVisual Reasoning evaluation prompt (Part I).\nThe first part of the evaluation prompt integrates the structured reasoning chain with contextual information extracted during dataset construction.\nFigure 18\n:\nVisual Reasoning evaluation prompt (Part II).\nThe second part specifies the structured scoring protocol and the JSON output format used to ensure consistent and interpretable evaluation results.\nVideo assessment prompt.\nThe second component integrates the generated question chain with a 3√ó2 grid of chronologically ordered frames extracted from the video (Figures\n17\nand\n18\n).\nIt also incorporates the contextual background information provided at the beginning‚Äînamely, the view perspective, video content summary, robotic manipulator type, and manipulated object identity.\nWith these inputs, the prompt instructs the model to determine whether each reasoning step has been successfully completed and to evaluate the stability, consistency, and physical plausibility of the robot and objects throughout the sequence.\nA strict scoring protocol and a structured JSON output format ensure reproducible and interpretable evaluation across models.\nAppendix F\nAdditional Qualitative Comparisons\nThis section provides supplementary qualitative results that extend the \"Qualitative Comparison Across Representative Tasks\" analysis presented in the main paper. For each of the five task categories in RBench, namely Common Manipulation, Long-Horizon Planning, Multi-Entity Collaboration, Spatial Relationship, and Visual Reasoning, we select two representative cases and visualize the generated videos from ten state-of-the-art image-to-video (I2V) models. As shown in Figures\n19\n,\n20\n,\n21\n,\n22\n,\n23\n, these examples offer a more detailed inspection of model behaviors under diverse embodied scenarios, complementing the quantitative results discussed in the main text.\nFigure 19\n:\nVisualization of Common Manipulation.\nThe first row contains the reference image, and the accompanying text shows the input prompt.\nFigure 20\n:\nVisualization of Long-Horizon Planning.\nThe first row contains the reference image, and the accompanying text shows the input prompt.\nFigure 21\n:\nVisualization of Multi-Entity Collaboration.\nThe first row contains the reference image, and the accompanying text shows the input prompt.\nFigure 22\n:\nVisualization of Spatial Relationship.\nThe first row contains the reference image, and the accompanying text shows the input prompt.\nFigure 23\n:\nVisualization of Visual Reasoning.\nThe first row contains the reference image, and the accompanying text shows the input prompt.\nAppendix G\nComprehensive Quantitative Results\nThis section presents the complete quantitative evaluation results obtained from both GPT-based and Qwen-based evaluators. We report detailed per-model scores across all five tasks (Common Manipulation, Long-Horizon Planning, Multi-Entity Collaboration, Spatial Relationship, and Visual Reasoning) and four robot embodiments (Dual Arm, Humanoid, Single Arm, and Quadruped).\nTo ensure clarity and readability, we summarize below the abbreviations used in all tables. For task-level metrics, each abbreviation corresponds to a specific VQA-style prompt used in our MLLM-based evaluation; the construction of these prompts and the computation of the total score (\nTS\n) are illustrated with representative example in Section\nE\n. The robot-level metrics are defined in Section\nB\n.\nTask-level Metrics (per-task evaluation):\n‚Ä¢\nAES\n: Action Execution Score\n‚Ä¢\nTCS\n: Task Completion Score\n‚Ä¢\nOCS\n: Object Consistency Score\n‚Ä¢\nRCS\n: Robot Consistency Score\n‚Ä¢\nPSS\n: Physical Semantic Score\n‚Ä¢\nECR\n: Event Completion Ratio\n‚Ä¢\nECS\n: Entity Consistency Score\n‚Ä¢\nACS\n: Action Coordination Score\n‚Ä¢\nSRS\n: Spatial Relation Score\n‚Ä¢\nMFS\n: Manipulation Feasibility Score\n‚Ä¢\nVRS\n: Visual Reasoning Score\n‚Ä¢\nTS\n: Total Score\nRobot-level Metrics (per-robot embodiment evaluation):\n‚Ä¢\nPSS\n: Physical Semantic Score\n‚Ä¢\nTAC\n: Task-Adherence Consistency\n‚Ä¢\nRSS\n: Robot‚ÄìSubject Stability\n‚Ä¢\nMSS\n: Motion Smoothness Score\n‚Ä¢\nMAS\n: Motion Amplitude Score\n‚Ä¢\nTC\n: Task Completion\n‚Ä¢\nVQ\n: Visual Quality\n‚Ä¢\nTS\n: Total Score\nThe following tables report the complete results for all models evaluated using both GPT and Qwen.\nAs shown in Table\n7\n, Table\n7\n,\nTable\n9\n, Table\n9\n, and\nTable\n11\n, the GPT-based evaluator provides assessments across\nfive tasks.\nSimilarly, the Qwen-based results are presented in\nTable\n15\n, Table\n17\n,\nTable\n17\n, Table\n19\n, and\nTable\n19\n.\nFor completeness, we also provide per-embodiment results. The detailed evaluation tables for\ndual-arm, humanoid, single-arm, and quadruped robots under GPT are shown in\nTable\n11\n, Table\n13\n,\nTable\n13\n, and Table\n15\n.\nThe corresponding Qwen-based tables are provided in\nTable\n21\n, Table\n21\n,\nTable\n23\n, and Table\n23\n.\nTable 6\n:\nResults on Common Manipulation with GPT\nModel\nAES\nTCS\nOCS\nRCS\nPSS\nTS\nOpen-source\nWan2.2_A14B\n0.479\n0.410\n0.635\n0.765\n0.520\n0.381\nHunyuanVideo 1.5\n0.505\n0.480\n0.575\n0.695\n0.490\n0.442\nLongCat-Video\n0.469\n0.408\n0.591\n0.739\n0.510\n0.371\nWan2.1_14B\n0.446\n0.375\n0.552\n0.692\n0.510\n0.344\nLTX-2\n0.340\n0.330\n0.515\n0.620\n0.380\n0.284\nWan2.2_5B\n0.416\n0.395\n0.540\n0.670\n0.495\n0.331\nSkyreels\n0.348\n0.230\n0.545\n0.740\n0.465\n0.202\nLTX-Video\n0.414\n0.307\n0.552\n0.718\n0.484\n0.302\nFramePack\n0.346\n0.188\n0.637\n0.739\n0.556\n0.205\nHunyuanVideo\n0.307\n0.190\n0.660\n0.755\n0.545\n0.177\nCogVideoX-5B\n0.245\n0.140\n0.490\n0.635\n0.335\n0.115\nClosed-source\nWan 2.6\n0.581\n0.596\n0.637\n0.750\n0.668\n0.545\nSeedance 1.5 pro\n0.654\n0.642\n0.591\n0.750\n0.556\n0.576\nWan 2.5\n0.565\n0.600\n0.635\n0.770\n0.605\n0.527\nHailuo v2\n0.576\n0.625\n0.625\n0.745\n0.595\n0.559\nVeo 3\n0.572\n0.602\n0.607\n0.729\n0.540\n0.520\nSeedance 1.0\n0.591\n0.590\n0.640\n0.730\n0.620\n0.542\nKling 2.6 pro\n0.561\n0.565\n0.610\n0.760\n0.590\n0.528\nSora v2 Pro\n0.354\n0.229\n0.637\n0.719\n0.561\n0.207\nSora v1\n0.280\n0.170\n0.445\n0.605\n0.360\n0.151\nRobotics-specific\nCosmos 2.5\n0.495\n0.370\n0.585\n0.705\n0.505\n0.358\nDreamGen(gr1)\n0.420\n0.341\n0.566\n0.683\n0.469\n0.311\nDreamGen(droid)\n0.420\n0.369\n0.604\n0.640\n0.421\n0.358\nVidar\n0.180\n0.090\n0.430\n0.470\n0.310\n0.073\nUnifoLM-WMA-0\n0.105\n0.045\n0.400\n0.300\n0.220\n0.036\nMean\n0.427\n0.371\n0.572\n0.685\n0.492\n0.338\nTable 7\n:\nResults on Long-Horizon Planning with GPT\nModel\nAES\nECR\nOCS\nRCS\nPSS\nTS\nOpen-source\nWan2.2_A14B\n0.569\n0.626\n0.715\n0.722\n0.590\n0.500\nHunyuanVideo 1.5\n0.537\n0.527\n0.606\n0.725\n0.537\n0.437\nLongCat-Video\n0.507\n0.485\n0.598\n0.742\n0.598\n0.384\nWan2.1_14B\n0.482\n0.404\n0.589\n0.687\n0.553\n0.335\nLTX-2\n0.485\n0.446\n0.566\n0.676\n0.529\n0.386\nWan2.2_5B\n0.444\n0.517\n0.601\n0.666\n0.425\n0.317\nSkyreels\n0.394\n0.311\n0.673\n0.740\n0.500\n0.253\nLTX-Video\n0.433\n0.347\n0.566\n0.633\n0.458\n0.279\nFramePack\n0.301\n0.145\n0.655\n0.732\n0.560\n0.168\nHunyuanVideo\n0.241\n0.101\n0.651\n0.714\n0.553\n0.147\nCogVideoX-5B\n0.301\n0.223\n0.543\n0.629\n0.448\n0.212\nClosed-source\nWan 2.6\n0.640\n0.545\n0.701\n0.743\n0.634\n0.514\nSeedance 1.5 pro\n0.638\n0.710\n0.677\n0.763\n0.625\n0.569\nWan 2.5\n0.603\n0.519\n0.743\n0.737\n0.615\n0.495\nHailuo v2\n0.600\n0.677\n0.725\n0.706\n0.637\n0.544\nVeo 3\n0.608\n0.681\n0.709\n0.729\n0.641\n0.530\nSeedance 1.0\n0.606\n0.603\n0.712\n0.727\n0.628\n0.454\nKling 2.6 pro\n0.618\n0.685\n0.710\n0.750\n0.697\n0.530\nSora v2 Pro\n0.422\n0.296\n0.646\n0.715\n0.603\n0.255\nSora v1\n0.250\n0.133\n0.629\n0.689\n0.543\n0.166\nRobotics-specific\nCosmos 2.5\n0.568\n0.556\n0.706\n0.656\n0.593\n0.495\nDreamGen(gr1)\n0.444\n0.284\n0.750\n0.731\n0.611\n0.333\nDreamGen(droid)\n0.491\n0.505\n0.655\n0.637\n0.517\n0.316\nVidar\n0.108\n-0.023\n0.550\n0.491\n0.375\n0.054\nUnifoLM-WMA-0\n0.037\n0.026\n0.398\n0.175\n0.120\n0.061\nMean\n0.453\n0.413\n0.643\n0.677\n0.544\n0.349\nTable 8\n:\nResults on Multi-Entity Collaboration with GPT\nModel\nACS\nTCS\nECS\nOCS\nPSS\nTS\nOpen-source\nWan2.2_A14B\n0.351\n0.441\n0.712\n0.632\n0.654\n0.373\nHunyuanVideo 1.5\n0.301\n0.375\n0.687\n0.625\n0.585\n0.311\nLongCat-Video\n0.190\n0.244\n0.696\n0.642\n0.654\n0.220\nWan2.1_14B\n0.261\n0.318\n0.693\n0.545\n0.551\n0.282\nLTX-2\n0.243\n0.289\n0.651\n0.572\n0.578\n0.233\nWan2.2_5B\n0.125\n0.181\n0.687\n0.568\n0.537\n0.141\nSkyreels\n0.189\n0.195\n0.695\n0.664\n0.621\n0.203\nLTX-Video\n0.207\n0.255\n0.686\n0.574\n0.563\n0.209\nFramePack\n0.186\n0.186\n0.709\n0.598\n0.529\n0.173\nHunyuanVideo\n0.100\n0.100\n0.672\n0.683\n0.650\n0.107\nCogVideoX-5B\n0.128\n0.113\n0.630\n0.482\n0.440\n0.098\nClosed-source\nWan 2.6\n0.443\n0.541\n0.738\n0.708\n0.654\n0.478\nSeedance 1.5 pro\n0.456\n0.591\n0.743\n0.689\n0.689\n0.483\nWan 2.5\n0.392\n0.453\n0.750\n0.633\n0.662\n0.401\nHailuo v2\n0.378\n0.422\n0.744\n0.678\n0.619\n0.385\nVeo 3\n0.450\n0.432\n0.731\n0.737\n0.621\n0.430\nSeedance 1.0\n0.422\n0.488\n0.750\n0.702\n0.696\n0.447\nKling 2.6 pro\n0.357\n0.392\n0.738\n0.698\n0.676\n0.364\nSora v2 Pro\n0.155\n0.191\n0.733\n0.558\n0.591\n0.186\nSora v1\n0.128\n0.107\n0.595\n0.494\n0.523\n0.111\nRobotics-specific\nCosmos 2.5\n0.203\n0.244\n0.727\n0.659\n0.670\n0.201\nDreamGen(gr1)\n0.262\n0.347\n0.707\n0.664\n0.628\n0.296\nDreamGen(droid)\n0.211\n0.260\n0.657\n0.548\n0.548\n0.214\nVidar\n0.056\n0.062\n0.454\n0.431\n0.329\n0.049\nUnifoLM-WMA-0\n0.017\n0.034\n0.392\n0.301\n0.295\n0.018\nMean\n0.248\n0.290\n0.679\n0.603\n0.583\n0.256\nTable 9\n:\nResults on Spatial Relationship with GPT\nModel\nSRS\nMFS\nOCS\nRCS\nPSS\nTS\nOpen-source\nWan2.2_A14B\n0.604\n0.596\n0.709\n0.758\n0.669\n0.454\nHunyuanVideo 1.5\n0.370\n0.344\n0.629\n0.750\n0.534\n0.315\nLongCat-Video\n0.390\n0.375\n0.679\n0.734\n0.648\n0.310\nWan2.1_14B\n0.316\n0.338\n0.669\n0.713\n0.610\n0.267\nLTX-2\n0.401\n0.383\n0.616\n0.687\n0.562\n0.304\nWan2.2_5B\n0.441\n0.389\n0.602\n0.705\n0.485\n0.312\nSkyreels\n0.388\n0.416\n0.601\n0.685\n0.574\n0.276\nLTX-Video\n0.224\n0.215\n0.612\n0.681\n0.491\n0.176\nFramePack\n0.364\n0.321\n0.628\n0.742\n0.557\n0.257\nHunyuanVideo\n0.163\n0.192\n0.605\n0.740\n0.586\n0.179\nCogVideoX-5B\n0.193\n0.241\n0.451\n0.653\n0.370\n0.111\nClosed-source\nWan 2.6\n0.787\n0.704\n0.719\n0.734\n0.734\n0.655\nSeedance 1.5 pro\n0.675\n0.608\n0.700\n0.716\n0.666\n0.494\nWan 2.5\n0.750\n0.598\n0.757\n0.750\n0.674\n0.576\nHailuo v2\n0.764\n0.654\n0.742\n0.750\n0.720\n0.636\nVeo 3\n0.601\n0.553\n0.726\n0.767\n0.684\n0.508\nSeedance 1.0\n0.484\n0.445\n0.710\n0.742\n0.679\n0.425\nKling 2.6 pro\n0.757\n0.681\n0.727\n0.757\n0.696\n0.598\nSora v2 Pro\n0.392\n0.392\n0.654\n0.714\n0.571\n0.267\nSora v1\n0.351\n0.305\n0.546\n0.583\n0.481\n0.223\nRobotics-specific\nCosmos 2.5\n0.419\n0.395\n0.661\n0.693\n0.620\n0.338\nDreamGen(gr1)\n0.467\n0.411\n0.677\n0.725\n0.677\n0.371\nDreamGen(droid)\n0.400\n0.433\n0.625\n0.691\n0.591\n0.348\nVidar\n0.163\n0.250\n0.517\n0.500\n0.405\n0.105\nUnifoLM-WMA-0\n0.065\n0.141\n0.445\n0.532\n0.315\n0.040\nMean\n0.437\n0.415\n0.640\n0.700\n0.584\n0.342\nTable 10\n:\nResults on Visual Reasoning with GPT\nModel\nAES\nVRS\nOCS\nRCS\nPSS\nTS\nOpen-source\nWan2.2_A14B\n0.424\n0.401\n0.651\n0.709\n0.552\n0.330\nHunyuanVideo 1.5\n0.453\n0.456\n0.604\n0.709\n0.447\n0.364\nLongCat-Video\n0.271\n0.211\n0.559\n0.722\n0.516\n0.186\nWan2.1_14B\n0.250\n0.263\n0.565\n0.646\n0.429\n0.204\nLTX-2\n0.267\n0.224\n0.500\n0.610\n0.360\n0.163\nWan2.2_5B\n0.283\n0.304\n0.583\n0.700\n0.438\n0.233\nSkyreels\n0.290\n0.267\n0.587\n0.668\n0.447\n0.233\nLTX-Video\n0.283\n0.287\n0.644\n0.688\n0.516\n0.241\nFramePack\n0.243\n0.203\n0.570\n0.743\n0.397\n0.169\nHunyuanVideo\n0.096\n0.058\n0.647\n0.744\n0.522\n0.035\nCogVideoX-5B\n0.136\n0.120\n0.428\n0.577\n0.261\n0.079\nClosed-source\nWan 2.6\n0.544\n0.622\n0.700\n0.733\n0.605\n0.530\nSeedance 1.5 pro\n0.494\n0.570\n0.635\n0.701\n0.570\n0.470\nWan 2.5\n0.482\n0.488\n0.693\n0.732\n0.539\n0.437\nHailuo v2\n0.511\n0.541\n0.666\n0.727\n0.566\n0.473\nVeo 3\n0.511\n0.610\n0.633\n0.711\n0.577\n0.504\nSeedance 1.0\n0.505\n0.505\n0.705\n0.733\n0.644\n0.441\nKling 2.6 pro\n0.477\n0.410\n0.627\n0.733\n0.555\n0.357\nSora v2 Pro\n0.193\n0.159\n0.642\n0.738\n0.556\n0.115\nSora v1\n0.219\n0.158\n0.475\n0.646\n0.250\n0.139\nRobotics-specific\nCosmos 2.5\n0.482\n0.493\n0.664\n0.744\n0.590\n0.399\nDreamGen(gr1)\n0.317\n0.262\n0.652\n0.743\n0.585\n0.215\nDreamGen(droid)\n0.412\n0.401\n0.616\n0.738\n0.529\n0.338\nVidar\n0.090\n0.050\n0.556\n0.659\n0.431\n0.050\nUnifoLM-WMA-0\n0.017\n-0.056\n0.062\n0.306\n0.176\n0.000\nMean\n0.330\n0.320\n0.587\n0.686\n0.482\n0.268\nTable 11\n:\nResults on Dual Arm with GPT\nModel\nPSS\nTAC\nRSS\nMS\nMA\nTC\nVQ\nTS\nOpen-source\nWan2.2_A14B\n0.638\n0.570\n0.764\n0.915\n0.204\n0.604\n0.561\n0.582\nHunyuanVideo 1.5\n0.612\n0.622\n0.649\n0.951\n0.370\n0.618\n0.434\n0.526\nLongCat-Video\n0.620\n0.540\n0.771\n0.937\n0.244\n0.580\n0.572\n0.576\nWan2.1_14B\n0.600\n0.540\n0.650\n0.850\n0.261\n0.570\n0.424\n0.497\nLTX-2\n0.488\n0.415\n0.637\n0.848\n0.378\n0.451\n0.396\n0.423\nWan2.2_5B\n0.575\n0.498\n0.606\n0.940\n0.269\n0.536\n0.360\n0.448\nSkyreels\n0.598\n0.498\n0.658\n0.884\n0.252\n0.548\n0.406\n0.477\nLTX-Video\n0.530\n0.442\n0.698\n0.812\n0.143\n0.486\n0.425\n0.455\nFramePack\n0.550\n0.395\n0.712\n0.885\n0.103\n0.472\n0.457\n0.464\nHunyuanVideo\n0.510\n0.280\n0.794\n0.959\n0.107\n0.395\n0.564\n0.479\nCogVideoX-5B\n0.480\n0.358\n0.638\n0.752\n0.143\n0.419\n0.352\n0.385\nClosed-source\nWan 2.6\n0.655\n0.708\n0.819\n0.984\n0.333\n0.681\n0.680\n0.680\nSeedance 1.5 pro\n0.668\n0.800\n0.721\n0.960\n0.399\n0.734\n0.547\n0.640\nWan 2.5\n0.670\n0.740\n0.758\n0.970\n0.347\n0.705\n0.563\n0.633\nHailuo v2\n0.658\n0.720\n0.751\n0.983\n0.312\n0.689\n0.534\n0.611\nVeo 3\n0.665\n0.682\n0.711\n0.973\n0.262\n0.674\n0.546\n0.610\nSeedance 1.0\n0.668\n0.648\n0.801\n0.972\n0.294\n0.658\n0.623\n0.640\nKling 2.6 pro\n0.672\n0.615\n0.770\n0.965\n0.210\n0.644\n0.567\n0.605\nSora v2 Pro\n0.565\n0.364\n0.776\n0.950\n0.272\n0.465\n0.560\n0.512\nSora v1\n0.422\n0.312\n0.531\n0.880\n0.210\n0.368\n0.279\n0.323\nRobotics-specific\nCosmos 2.5\n0.665\n0.575\n0.746\n0.930\n0.127\n0.620\n0.500\n0.560\nDreamGen(gr1)\n0.630\n0.492\n0.779\n0.939\n0.123\n0.561\n0.503\n0.532\nDreamGen(droid)\n0.610\n0.542\n0.668\n0.863\n0.201\n0.576\n0.375\n0.475\nVidar\n0.450\n0.238\n0.781\n0.933\n0.025\n0.344\n0.475\n0.409\nUnifoLM-WMA-0\n0.348\n0.252\n0.348\n0.497\n0.120\n0.300\n0.089\n0.194\nMean\n0.581\n0.513\n0.701\n0.901\n0.228\n0.547\n0.471\n0.509\nTable 12\n:\nResults on Humanoid with GPT\nModel\nPSS\nTAC\nRSS\nMS\nMA\nTC\nVQ\nTS\nOpen-source\nWan2.2_A14B\n0.678\n0.748\n0.787\n0.966\n0.105\n0.712\n0.584\n0.647\nHunyuanVideo 1.5\n0.660\n0.800\n0.703\n0.922\n0.206\n0.730\n0.460\n0.595\nLongCat-Video\n0.652\n0.668\n0.801\n0.969\n0.089\n0.660\n0.583\n0.621\nWan2.1_14B\n0.635\n0.660\n0.765\n0.938\n0.167\n0.648\n0.550\n0.599\nLTX-2\n0.622\n0.608\n0.736\n0.880\n0.231\n0.615\n0.495\n0.554\nWan2.2_5B\n0.668\n0.695\n0.755\n0.963\n0.152\n0.681\n0.533\n0.607\nSkyreels\n0.610\n0.598\n0.725\n0.808\n0.086\n0.604\n0.414\n0.509\nLTX-Video\n0.602\n0.630\n0.671\n0.628\n0.050\n0.616\n0.312\n0.463\nFramePack\n0.598\n0.572\n0.776\n0.864\n0.069\n0.585\n0.511\n0.548\nHunyuanVideo\n0.595\n0.465\n0.763\n0.933\n0.096\n0.530\n0.517\n0.523\nCogVideoX-5B\n0.570\n0.565\n0.698\n0.647\n0.127\n0.568\n0.424\n0.496\nClosed-source\nWan 2.6\n0.665\n0.818\n0.794\n0.980\n0.132\n0.741\n0.593\n0.667\nSeedance 1.5 pro\n0.682\n0.838\n0.793\n0.955\n0.158\n0.760\n0.623\n0.691\nWan 2.5\n0.698\n0.782\n0.779\n0.981\n0.122\n0.740\n0.568\n0.653\nHailuo v2\n0.682\n0.815\n0.741\n0.970\n0.133\n0.749\n0.521\n0.635\nVeo 3\n0.655\n0.785\n0.776\n0.968\n0.132\n0.720\n0.554\n0.637\nSeedance 1.0\n0.675\n0.752\n0.833\n0.964\n0.166\n0.714\n0.658\n0.686\nKling 2.6 pro\n0.672\n0.762\n0.750\n0.959\n0.113\n0.718\n0.508\n0.613\nSora v2 Pro\n0.638\n0.565\n0.774\n0.936\n0.085\n0.602\n0.520\n0.561\nSora v1\n0.542\n0.450\n0.597\n0.900\n0.216\n0.496\n0.342\n0.419\nRobotics-specific\nCosmos 2.5\n0.650\n0.720\n0.797\n0.925\n0.071\n0.685\n0.566\n0.625\nDreamGen(gr1)\n0.652\n0.595\n0.781\n0.885\n0.079\n0.624\n0.526\n0.575\nDreamGen(droid)\n0.620\n0.700\n0.704\n0.843\n0.137\n0.660\n0.453\n0.556\nVidar\n0.445\n0.298\n0.694\n0.855\n0.025\n0.371\n0.343\n0.357\nUnifoLM-WMA-0\n0.270\n0.282\n0.386\n0.512\n0.112\n0.276\n0.125\n0.200\nMean\n0.617\n0.646\n0.735\n0.886\n0.122\n0.632\n0.491\n0.561\nTable 13\n:\nResults on Single Arm with GPT\nModel\nPSS\nTAC\nRSS\nMS\nMA\nTC\nVQ\nTS\nOpen-source\nWan2.2_A14B\n0.638\n0.582\n0.783\n0.942\n0.263\n0.610\n0.607\n0.608\nHunyuanVideo 1.5\n0.510\n0.622\n0.651\n0.949\n0.355\n0.566\n0.460\n0.513\nLongCat-Video\n0.562\n0.530\n0.807\n0.945\n0.298\n0.546\n0.625\n0.585\nWan2.1_14B\n0.542\n0.472\n0.677\n0.849\n0.282\n0.507\n0.422\n0.464\nLTX-2\n0.490\n0.410\n0.681\n0.919\n0.464\n0.450\n0.456\n0.453\nWan2.2_5B\n0.518\n0.480\n0.619\n0.943\n0.313\n0.499\n0.372\n0.435\nSkyreels\n0.525\n0.495\n0.712\n0.911\n0.286\n0.510\n0.504\n0.507\nLTX-Video\n0.492\n0.408\n0.686\n0.804\n0.158\n0.450\n0.431\n0.440\nFramePack\n0.445\n0.318\n0.760\n0.888\n0.104\n0.381\n0.498\n0.439\nHunyuanVideo\n0.445\n0.265\n0.809\n0.963\n0.118\n0.355\n0.552\n0.453\nCogVideoX-5B\n0.405\n0.335\n0.582\n0.815\n0.256\n0.370\n0.307\n0.338\nClosed-source\nWan 2.6\n0.652\n0.710\n0.796\n0.983\n0.392\n0.681\n0.651\n0.666\nSeedance 1.5 pro\n0.635\n0.832\n0.752\n0.960\n0.419\n0.734\n0.561\n0.647\nWan 2.5\n0.668\n0.802\n0.787\n0.969\n0.412\n0.735\n0.624\n0.679\nHailuo v2\n0.665\n0.752\n0.680\n0.989\n0.396\n0.709\n0.479\n0.594\nVeo 3\n0.642\n0.755\n0.750\n0.977\n0.362\n0.699\n0.568\n0.633\nSeedance 1.0\n0.658\n0.655\n0.769\n0.967\n0.306\n0.656\n0.589\n0.622\nKling 2.6 pro\n0.622\n0.640\n0.714\n0.958\n0.305\n0.631\n0.508\n0.569\nSora v2 Pro\n0.490\n0.310\n0.784\n0.962\n0.324\n0.400\n0.552\n0.476\nSora v1\n0.350\n0.320\n0.532\n0.884\n0.247\n0.335\n0.293\n0.314\nRobotics-specific\nCosmos 2.5\n0.632\n0.592\n0.737\n0.888\n0.206\n0.612\n0.475\n0.543\nDreamGen(gr1)\n0.618\n0.620\n0.716\n0.932\n0.263\n0.619\n0.508\n0.563\nDreamGen(droid)\n0.568\n0.568\n0.691\n0.930\n0.214\n0.568\n0.430\n0.499\nVidar\n0.415\n0.272\n0.726\n0.929\n0.068\n0.344\n0.420\n0.382\nUnifoLM-WMA-0\n0.428\n0.315\n0.437\n0.709\n0.389\n0.371\n0.164\n0.267\nMean\n0.544\n0.522\n0.705\n0.918\n0.288\n0.533\n0.482\n0.507\nTable 14\n:\nResults on Quadruped Robot with GPT\nModel\nPSS\nTAC\nRSS\nMS\nMA\nTC\nVQ\nTS\nOpen-source\nWan2.2_A14B\n0.760\n0.712\n0.800\n0.888\n0.196\n0.736\n0.643\n0.689\nHunyuanVideo 1.5\n0.715\n0.738\n0.713\n0.940\n0.403\n0.726\n0.542\n0.634\nLongCat-Video\n0.742\n0.628\n0.827\n0.923\n0.179\n0.685\n0.676\n0.680\nWan2.1_14B\n0.722\n0.670\n0.706\n0.850\n0.303\n0.696\n0.495\n0.595\nLTX-2\n0.715\n0.670\n0.758\n0.845\n0.287\n0.692\n0.552\n0.622\nWan2.2_5B\n0.712\n0.678\n0.688\n0.921\n0.273\n0.695\n0.486\n0.590\nSkyreels\n0.732\n0.605\n0.722\n0.853\n0.163\n0.669\n0.503\n0.586\nLTX-Video\n0.698\n0.678\n0.668\n0.676\n0.122\n0.688\n0.364\n0.526\nFramePack\n0.720\n0.575\n0.827\n0.876\n0.069\n0.648\n0.605\n0.626\nHunyuanVideo\n0.730\n0.602\n0.788\n0.953\n0.127\n0.666\n0.584\n0.625\nCogVideoX-5B\n0.655\n0.560\n0.624\n0.618\n0.220\n0.608\n0.322\n0.464\nClosed-source\nWan 2.6\n0.755\n0.792\n0.813\n0.970\n0.316\n0.774\n0.672\n0.723\nSeedance 1.5 pro\n0.748\n0.820\n0.746\n0.884\n0.407\n0.784\n0.577\n0.680\nWan 2.5\n0.785\n0.792\n0.809\n0.948\n0.322\n0.789\n0.664\n0.726\nHailuo v2\n0.748\n0.738\n0.711\n0.961\n0.354\n0.742\n0.538\n0.640\nVeo 3\n0.745\n0.695\n0.798\n0.961\n0.214\n0.720\n0.658\n0.689\nSeedance 1.0\n0.768\n0.735\n0.791\n0.945\n0.334\n0.751\n0.645\n0.698\nKling 2.6 pro\n0.740\n0.738\n0.736\n0.861\n0.258\n0.739\n0.535\n0.637\nSora v2 Pro\n0.731\n0.670\n0.789\n0.922\n0.239\n0.701\n0.626\n0.663\nSora v1\n0.700\n0.672\n0.620\n0.863\n0.282\n0.686\n0.401\n0.543\nRobotics-specific\nCosmos 2.5\n0.752\n0.622\n0.808\n0.892\n0.137\n0.688\n0.629\n0.658\nDreamGen(gr1)\n0.712\n0.655\n0.706\n0.788\n0.164\n0.684\n0.474\n0.579\nDreamGen(droid)\n0.705\n0.568\n0.687\n0.854\n0.160\n0.636\n0.448\n0.542\nVidar\n0.528\n0.472\n0.552\n0.749\n0.074\n0.500\n0.247\n0.373\nUnifoLM-WMA-0\n0.475\n0.390\n0.410\n0.497\n0.132\n0.432\n0.154\n0.293\nMean\n0.711\n0.659\n0.723\n0.857\n0.229\n0.685\n0.521\n0.603\nTable 15\n:\nResults on Common Manipulation with Qwen\nModel\nAES\nTCS\nOCS\nRCS\nPSS\nTS\nOpen-source\nWan2.2_A14B\n0.804\n0.815\n0.934\n0.913\n0.923\n0.708\nLongCat-Video\n0.681\n0.704\n0.840\n0.920\n0.897\n0.677\nWan2.2_5B\n0.620\n0.630\n0.940\n0.890\n0.860\n0.597\nWan2.1_14B\n0.663\n0.653\n0.903\n0.932\n0.807\n0.687\nSkyreels\n0.547\n0.559\n0.928\n0.916\n0.952\n0.546\nLTX-Video\n0.630\n0.666\n0.845\n0.904\n0.880\n0.450\nFramePack\n0.336\n0.347\n0.913\n0.956\n0.934\n0.455\nCogVideoX-5B\n0.352\n0.340\n0.625\n0.704\n0.647\n0.289\nClosed-source\nWan 2.5\n0.928\n0.946\n0.946\n0.964\n0.973\n0.887\nHailuo v2\n0.916\n0.952\n0.940\n0.940\n0.976\n0.843\nVeo 3\n0.953\n0.962\n0.962\n0.981\n0.990\n0.896\nSeedance 1.0\n0.946\n0.928\n0.955\n0.946\n0.964\n0.856\nRobotics-specific\nCosmos 2.5\n0.687\n0.708\n0.895\n0.916\n0.864\n0.687\nDreamGen(gr1)\n0.625\n0.656\n0.906\n0.937\n0.906\n0.507\nDreamGen(droid)\n0.630\n0.690\n0.785\n0.845\n0.773\n0.465\nUnifoLM-WMA-0\n0.043\n0.043\n0.532\n0.478\n0.369\n0.028\nVidar\n0.142\n0.166\n0.833\n0.833\n0.761\n0.117\nMean\n0.597\n0.611\n0.867\n0.858\n0.835\n0.559\nTable 16\n:\nResults on Long-Horizon Planning with Qwen\nModel\nAES\nECS\nOCS\nRCS\nPSS\nTS\nOpen-source\nWan2.2_A14B\n0.836\n0.883\n0.942\n0.971\n0.951\n0.680\nLongCat-Video\n0.650\n0.706\n0.925\n0.950\n0.912\n0.489\nWan2.2_5B\n0.520\n0.594\n0.947\n0.937\n0.895\n0.449\nWan2.1_14B\n0.681\n0.669\n0.931\n0.931\n0.875\n0.465\nSkyreels\n0.416\n0.511\n0.944\n0.958\n0.861\n0.324\nLTX-Video\n0.352\n0.414\n0.779\n0.808\n0.705\n0.357\nFramePack\n0.212\n0.252\n0.862\n0.987\n0.937\n0.196\nCogVideoX-5B\n0.276\n0.292\n0.855\n0.763\n0.684\n0.096\nClosed-source\nWan 2.5\n0.714\n0.836\n0.964\n0.964\n0.955\n0.719\nHailuo v2\n0.808\n0.903\n0.950\n0.991\n0.908\n0.705\nVeo 3\n0.812\n0.903\n0.984\n0.968\n0.945\n0.854\nSeedance 1.0\n0.824\n0.889\n0.990\n0.990\n0.925\n0.715\nRobotics-specific\nCosmos 2.5\n0.731\n0.810\n0.953\n0.981\n0.935\n0.596\nDreamGen(gr1)\n0.475\n0.587\n0.950\n0.937\n0.887\n0.353\nDreamGen(droid)\n0.602\n0.717\n0.882\n0.882\n0.852\n0.301\nUnifoLM-WMA-0\n0.000\n-0.041\n0.645\n0.500\n0.500\n0.000\nVidar\n0.050\n0.058\n0.800\n0.866\n0.666\n0.019\nMean\n0.505\n0.565\n0.893\n0.886\n0.833\n0.417\nTable 17\n:\nResults on Multi-Entity Collaboration with Qwen\nModel\nACS\nTCS\nECS\nOCS\nPSS\nTS\nOpen-source\nWan2.2_A14B\n0.941\n0.941\n0.985\n0.992\n1.000\n0.920\nLongCat-Video\n0.887\n0.912\n0.968\n0.962\n0.968\n0.814\nWan2.2_5B\n0.763\n0.796\n0.993\n0.960\n0.967\n0.721\nWan2.1_14B\n0.750\n0.786\n0.975\n0.945\n0.957\n0.702\nSkyreels\n0.743\n0.736\n1.000\n1.000\n0.993\n0.686\nLTX-Video\n0.806\n0.806\n0.975\n0.975\n0.993\n0.734\nFramePack\n0.710\n0.703\n0.980\n0.993\n0.947\n0.630\nCogVideoX-5B\n0.522\n0.536\n0.933\n0.882\n0.889\n0.426\nClosed-source\nWan 2.5\n0.896\n0.908\n0.987\n0.975\n1.000\n0.915\nHailuo v2\n0.986\n0.993\n0.952\n0.972\n0.986\n0.892\nVeo 3\n0.914\n0.914\n0.993\n0.987\n1.000\n0.924\nSeedance 1.0\n0.960\n0.967\n0.973\n0.960\n1.000\n0.898\nRobotics-specific\nCosmos 2.5\n0.864\n0.878\n0.993\n1.000\n0.972\n0.768\nDreamGen(gr1)\n0.878\n0.878\n0.957\n0.963\n0.969\n0.848\nDreamGen(droid)\n0.763\n0.819\n0.958\n0.951\n0.930\n0.591\nUnifoLM-WMA-0\n0.044\n0.044\n0.507\n0.477\n0.301\n0.025\nVidar\n0.083\n0.097\n0.743\n0.736\n0.583\n0.082\nMean\n0.714\n0.724\n0.933\n0.928\n0.910\n0.657\nTable 18\n:\nResults on Spatial Relationship with Qwen\nModel\nSRS\nMFS\nOCS\nRCS\nPSS\nTS\nOpen-source\nWan2.2_A14B\n0.833\n0.833\n1.000\n1.000\n1.000\n0.660\nLongCat-Video\n0.636\n0.636\n1.000\n1.000\n1.000\n0.465\nWan2.2_5B\n0.600\n0.625\n1.000\n0.900\n0.900\n0.402\nWan2.1_14B\n0.636\n0.636\n1.000\n1.000\n1.000\n0.400\nSkyreels\n0.750\n0.750\n1.000\n0.875\n0.875\n0.400\nLTX-Video\n0.750\n0.750\n1.000\n1.000\n1.000\n0.382\nFramePack\n0.400\n0.400\n1.000\n1.000\n1.000\n0.240\nCogVideoX-5B\n0.500\n0.535\n1.000\n0.857\n0.714\n0.240\nClosed-source\nWan 2.5\n0.916\n0.916\n1.000\n0.979\n1.000\n0.825\nHailuo v2\n1.000\n1.000\n1.000\n1.000\n1.000\n0.840\nVeo 3\n0.933\n0.933\n1.000\n1.000\n1.000\n0.740\nSeedance 1.0\n0.666\n0.666\n1.000\n1.000\n1.000\n0.665\nRobotics-specific\nCosmos 2.5\n0.875\n0.875\n1.000\n1.000\n1.000\n0.512\nDreamGen(gr1)\n0.642\n0.642\n1.000\n1.000\n1.000\n0.500\nDreamGen(droid)\n0.545\n0.545\n1.000\n1.000\n1.000\n0.505\nUnifoLM-WMA-0\n0.200\n0.250\n0.625\n0.575\n0.500\n0.065\nVidar\n0.367\n0.382\n0.691\n0.647\n0.573\n0.140\nMean\n0.666\n0.674\n0.962\n0.935\n0.920\n0.464\nTable 19\n:\nResults on Visual Reasoning with Qwen\nModel\nAES\nVRS\nOCS\nRCS\nPSS\nTS\nOpen-source\nWan2.2_A14B\n0.727\n0.701\n1.000\n1.000\n1.000\n0.550\nLongCat-Video\n0.361\n0.358\n0.944\n0.958\n0.847\n0.354\nWan2.2_5B\n0.291\n0.218\n0.906\n0.906\n0.906\n0.420\nWan2.1_14B\n0.397\n0.401\n0.886\n0.897\n0.784\n0.269\nSkyreels\n0.420\n0.401\n0.960\n0.930\n0.830\n0.357\nLTX-Video\n0.390\n0.410\n0.859\n0.937\n0.859\n0.285\nFramePack\n0.350\n0.343\n0.980\n0.990\n0.950\n0.345\nCogVideoX-5B\n0.073\n0.044\n0.779\n0.632\n0.544\n0.030\nClosed-source\nWan 2.5\n0.809\n0.770\n0.988\n1.000\n0.952\n0.737\nHailuo v2\n0.790\n0.882\n1.000\n1.000\n0.900\n0.820\nVeo 3\n0.847\n0.853\n0.945\n1.000\n0.989\n0.750\nSeedance 1.0\n0.927\n0.945\n0.979\n0.989\n1.000\n0.789\nRobotics-specific\nCosmos 2.5\n0.593\n0.632\n0.984\n1.000\n1.000\n0.506\nDreamGen(gr1)\n0.437\n0.425\n0.937\n0.958\n0.822\n0.404\nDreamGen(droid)\n0.600\n0.575\n0.800\n0.825\n0.787\n0.386\nUnifoLM-WMA-0\n0.000\n-0.098\n0.177\n0.348\n0.289\n0.000\nVidar\n0.000\n-0.062\n0.947\n0.937\n0.906\n0.029\nMean\n0.452\n0.438\n0.890\n0.882\n0.825\n0.395\nTable 20\n:\nResults on Dual Arm Robot with Qwen\nModel\nPSS\nTAC\nRSS\nMS\nMA\nTC\nVQ\nTS\nOpen-source\nWan2.2_A14B\n0.852\n0.760\n0.767\n0.915\n0.204\n0.806\n0.550\n0.678\nLongCat-Video\n0.738\n0.638\n0.741\n0.937\n0.244\n0.688\n0.517\n0.602\nWan2.2_5B\n0.688\n0.645\n0.658\n0.940\n0.269\n0.666\n0.402\n0.534\nWan2.1_14B\n0.790\n0.730\n0.639\n0.850\n0.261\n0.760\n0.364\n0.562\nSkyreels\n0.755\n0.700\n0.675\n0.884\n0.252\n0.728\n0.419\n0.573\nLTX-Video\n0.615\n0.535\n0.692\n0.812\n0.143\n0.575\n0.399\n0.487\nFramePack\n0.612\n0.495\n0.719\n0.885\n0.103\n0.554\n0.445\n0.499\nCogVideoX-5B\n0.505\n0.540\n0.613\n0.752\n0.143\n0.522\n0.323\n0.422\nClosed-source\nWan 2.5\n0.920\n0.880\n0.761\n0.970\n0.347\n0.900\n0.588\n0.744\nHailuo v2\n0.908\n0.848\n0.744\n0.983\n0.312\n0.878\n0.534\n0.706\nVeo 3\n0.870\n0.802\n0.777\n0.973\n0.262\n0.836\n0.581\n0.708\nSeedance 1.0\n0.895\n0.810\n0.801\n0.972\n0.294\n0.852\n0.608\n0.730\nRobotics-specific\nCosmos 2.5\n0.792\n0.708\n0.791\n0.930\n0.127\n0.750\n0.543\n0.646\nDreamGen(gr1)\n0.780\n0.638\n0.801\n0.939\n0.123\n0.709\n0.555\n0.632\nDreamGen(droid)\n0.722\n0.678\n0.711\n0.863\n0.201\n0.700\n0.441\n0.570\nUnifoLM-WMA-0\n0.110\n0.222\n0.266\n0.497\n0.120\n0.166\n0.046\n0.106\nVidar\n0.295\n0.240\n0.804\n0.933\n0.025\n0.268\n0.511\n0.389\nMean\n0.685\n0.622\n0.694\n0.880\n0.205\n0.651\n0.449\n0.550\nTable 21\n:\nResults on Humanoid Robot with Qwen\nModel\nPSS\nTAC\nRSS\nMS\nMA\nTC\nVQ\nTS\nOpen-source\nWan2.2_A14B\n0.935\n0.800\n0.806\n0.966\n0.105\n0.898\n0.557\n0.727\nLongCat-Video\n0.918\n0.765\n0.826\n0.969\n0.089\n0.841\n0.579\n0.710\nWan2.2_5B\n0.880\n0.758\n0.791\n0.963\n0.152\n0.819\n0.544\n0.681\nWan2.1_14B\n0.895\n0.712\n0.785\n0.938\n0.167\n0.804\n0.524\n0.664\nSkyreels\n0.842\n0.670\n0.803\n0.808\n0.086\n0.756\n0.500\n0.628\nLTX-Video\n0.852\n0.670\n0.808\n0.628\n0.050\n0.761\n0.445\n0.603\nFramePack\n0.815\n0.622\n0.838\n0.864\n0.069\n0.719\n0.550\n0.634\nCogVideoX-5B\n0.712\n0.602\n0.710\n0.647\n0.127\n0.658\n0.390\n0.524\nClosed-source\nWan 2.5\n0.935\n0.835\n0.826\n0.981\n0.122\n0.885\n0.600\n0.742\nHailuo v2\n0.952\n0.848\n0.796\n0.970\n0.133\n0.891\n0.548\n0.719\nVeo 3\n0.955\n0.802\n0.831\n0.968\n0.132\n0.829\n0.604\n0.716\nSeedance 1.0\n0.936\n0.814\n0.829\n0.964\n0.166\n0.904\n0.614\n0.759\nRobotics-specific\nCosmos 2.5\n0.868\n0.730\n0.841\n0.925\n0.071\n0.799\n0.578\n0.688\nDreamGen(gr1)\n0.858\n0.682\n0.823\n0.885\n0.079\n0.770\n0.542\n0.656\nDreamGen(droid)\n0.842\n0.736\n0.739\n0.843\n0.137\n0.788\n0.478\n0.633\nUnifoLM-WMA-0\n0.168\n0.285\n0.349\n0.512\n0.112\n0.226\n0.115\n0.170\nVidar\n0.440\n0.305\n0.700\n0.855\n0.025\n0.372\n0.357\n0.364\nMean\n0.806\n0.674\n0.770\n0.862\n0.113\n0.740\n0.501\n0.620\nTable 22\n:\nResults on Single Arm Robot with Qwen\nModel\nPSS\nTAC\nRSS\nMS\nMA\nTC\nVQ\nTS\nOpen-source\nWan2.2_A14B\n0.815\n0.802\n0.755\n0.942\n0.263\n0.809\n0.568\n0.688\nLongCat-Video\n0.750\n0.712\n0.812\n0.945\n0.298\n0.736\n0.660\n0.698\nWan2.2_5B\n0.605\n0.650\n0.626\n0.943\n0.313\n0.628\n0.394\n0.511\nWan2.1_14B\n0.700\n0.616\n0.638\n0.849\n0.282\n0.659\n0.379\n0.519\nSkyreels\n0.760\n0.699\n0.731\n0.911\n0.286\n0.732\n0.492\n0.612\nLTX-Video\n0.535\n0.525\n0.710\n0.804\n0.158\n0.530\n0.443\n0.486\nFramePack\n0.400\n0.500\n0.674\n0.888\n0.104\n0.394\n0.413\n0.403\nCogVideoX-5B\n0.460\n0.460\n0.553\n0.815\n0.256\n0.460\n0.288\n0.374\nClosed-source\nWan 2.5\n0.895\n0.918\n0.747\n0.969\n0.412\n0.895\n0.589\n0.742\nHailuo v2\n0.910\n0.906\n0.705\n0.989\n0.396\n0.902\n0.489\n0.695\nVeo 3\n0.898\n0.891\n0.756\n0.977\n0.362\n0.890\n0.594\n0.742\nSeedance 1.0\n0.852\n0.818\n0.747\n0.967\n0.306\n0.835\n0.580\n0.707\nRobotics-specific\nCosmos 2.5\n0.795\n0.765\n0.751\n0.888\n0.206\n0.776\n0.516\n0.646\nDreamGen(gr1)\n0.812\n0.728\n0.782\n0.932\n0.263\n0.770\n0.550\n0.660\nDreamGen(droid)\n0.745\n0.675\n0.721\n0.930\n0.214\n0.710\n0.468\n0.589\nUnifoLM-WMA-0\n0.382\n0.495\n0.391\n0.709\n0.389\n0.439\n0.140\n0.289\nVidar\n0.220\n0.200\n0.736\n0.929\n0.068\n0.210\n0.479\n0.344\nMean\n0.658\n0.648\n0.679\n0.898\n0.263\n0.649\n0.454\n0.551\nTable 23\n:\nResults on Quadruped Robot with Qwen\nModel\nPSS\nTAC\nRSS\nMS\nMA\nTC\nVQ\nTS\nOpen-source\nWan2.2_A14B\n0.860\n0.698\n0.746\n0.888\n0.196\n0.779\n0.561\n0.670\nLongCat-Video\n0.870\n0.685\n0.747\n0.923\n0.179\n0.778\n0.554\n0.666\nWan2.2_5B\n0.858\n0.635\n0.721\n0.921\n0.273\n0.746\n0.529\n0.637\nWan2.1_14B\n0.845\n0.665\n0.674\n0.850\n0.303\n0.755\n0.453\n0.604\nSkyreels\n0.862\n0.652\n0.749\n0.853\n0.163\n0.758\n0.550\n0.654\nLTX-Video\n0.818\n0.690\n0.709\n0.676\n0.122\n0.754\n0.423\n0.588\nFramePack\n0.765\n0.540\n0.881\n0.876\n0.069\n0.652\n0.687\n0.669\nCogVideoX-5B\n0.705\n0.575\n0.639\n0.618\n0.220\n0.640\n0.349\n0.494\nClosed-source\nWan 2.5\n0.902\n0.740\n0.769\n0.948\n0.322\n0.821\n0.601\n0.711\nHailuo v2\n0.888\n0.722\n0.654\n0.961\n0.354\n0.805\n0.458\n0.631\nVeo 3\n0.880\n0.722\n0.793\n0.961\n0.214\n0.801\n0.652\n0.726\nSeedance 1.0\n0.865\n0.710\n0.728\n0.945\n0.334\n0.788\n0.560\n0.674\nRobotics-specific\nCosmos 2.5\n0.850\n0.612\n0.755\n0.892\n0.137\n0.731\n0.547\n0.639\nDreamGen(gr1)\n0.852\n0.672\n0.701\n0.788\n0.164\n0.762\n0.460\n0.611\nDreamGen(droid)\n0.745\n0.592\n0.724\n0.854\n0.160\n0.669\n0.500\n0.584\nUnifoLM-WMA-0\n0.310\n0.300\n0.399\n0.497\n0.132\n0.305\n0.197\n0.251\nVidar\n0.495\n0.445\n0.575\n0.749\n0.074\n0.470\n0.290\n0.380\nMean\n0.782\n0.612\n0.705\n0.825\n0.198\n0.701\n0.494\n0.597",
    "preview_text": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.\n\n1]Peking University\n2]ByteDance Seed\nRethinking Video Generation Model for the Embodied World\nYufan Deng\n1,2‚àó\nZilin Pan\n1‚àó\nHongyu Zhang\n1‚àó\nXiaojie Li\n2\nRuoqing Hu\n2\nYufei Ding\n1\nYiming Zou\n1\nYan Zeng\n2\nDaquan Zhou\n1\n[\n[\nAbstract\nVideo generation models hav",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "VLA",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "ËØ•ËÆ∫ÊñáÊèêÂá∫Êú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÂü∫ÂáÜRBenchÂíåÊï∞ÊçÆÈõÜRoVid-XÔºå‰ª•ËØÑ‰º∞ÂíåÊèêÂçáÂÖ∑Ë∫´AI‰∏≠ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑÁâ©ÁêÜÁúüÂÆûÊÄßÂíå‰ªªÂä°Ê≠£Á°ÆÊÄß„ÄÇ",
    "detailed_summary": "## ËÆ∫ÊñáÊëòË¶ÅÔºö„Ää‰∏∫ÂÖ∑Ë∫´‰∏ñÁïåÈáçÊñ∞ÊÄùËÄÉËßÜÈ¢ëÁîüÊàêÊ®°Âûã„Äã\n\n### 1. Á†îÁ©∂ËÉåÊôØÂíåÂä®Êú∫\nËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®Êé®Âä®ÂÖ∑Ë∫´Êô∫ËÉΩÔºàEmbodied AIÔºâÂèëÂ±ïÊñπÈù¢Â±ïÁé∞Âá∫Â∑®Â§ßÊΩúÂäõÔºåËÉΩÂ§ü‰∏∫Êú∫Âô®‰∫∫Â≠¶‰π†ÁîüÊàêÂåÖÂê´ÊÑüÁü•„ÄÅÊé®ÁêÜÂíåË°åÂä®ÁöÑÂ§öÊ†∑ÂåñÊï∞ÊçÆ„ÄÇÁÑ∂ËÄåÔºåÂΩìÂâçÊ®°ÂûãÂú®ÁîüÊàê**Áâ©ÁêÜÁúüÂÆû„ÄÅ‰ªªÂä°ÂáÜÁ°Æ**ÁöÑÊú∫Âô®‰∫∫‰∫§‰∫íËßÜÈ¢ëÊñπÈù¢‰ªçÈù¢‰∏¥ÊåëÊàòÔºå‰∏îÁº∫‰πè‰∏Ä‰∏™**Ê†áÂáÜÂåñ„ÄÅÂèØÂ§çÁé∞ÁöÑËØÑ‰º∞Âü∫ÂáÜ**Êù•ÂÖ¨Âπ≥Ë°°ÈáèÊ®°ÂûãÊÄßËÉΩÂπ∂Êé®Âä®È¢ÜÂüüËøõÊ≠•„ÄÇÁé∞ÊúâËØÑ‰º∞Â§öÂÖ≥Ê≥®ËßÜËßâ‰øùÁúüÂ∫¶ÔºåËÄåÂøΩËßÜ‰∫Ü‰ªªÂä°ÂÆåÊàêÂ∫¶„ÄÅÂä®‰ΩúÁâ©ÁêÜÂêàÁêÜÊÄßÁ≠âÂØπÊú∫Âô®‰∫∫Â∫îÁî®Ëá≥ÂÖ≥ÈáçË¶ÅÁöÑÁª¥Â∫¶„ÄÇ\n\n### 2. Ê†∏ÂøÉÊñπÊ≥ïÂíåÊäÄÊúØÂàõÊñ∞\nÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂçèÂêåÁöÑËØÑ‰º∞‰∏éÊï∞ÊçÆÁîüÊÄÅÁ≥ªÁªüÔºö\n*   **ËØÑ‰º∞Âü∫ÂáÜ RBench**ÔºöÈ¶ñ‰∏™Èù¢ÂêëÊú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÁöÑÁªºÂêàÊÄßÂü∫ÂáÜ„ÄÇÂÆÉÂåÖÂê´**650‰∏™**Á≤æÂøÉËÆæËÆ°ÁöÑËØÑ‰º∞Ê†∑Êú¨ÔºåË¶ÜÁõñ**5Á±ª‰ªªÂä°**ÔºàÂ¶ÇÂ∏∏ËßÅÊìç‰Ωú„ÄÅÈïøÁ®ãËßÑÂàí„ÄÅËßÜËßâÊé®ÁêÜÔºâÂíå**4ÁßçÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ**ÔºàÂ¶ÇÂèåËáÇ„ÄÅ‰∫∫ÂΩ¢„ÄÅÂõõË∂≥Ôºâ„ÄÇRBenchÊèêÂá∫‰∫Ü**ÂèØËá™Âä®ËÆ°ÁÆóÁöÑÁªÜÁ≤íÂ∫¶ÊåáÊ†á**Ôºå‰ªé**‰ªªÂä°ÂÆåÊàêÂ∫¶**ÔºàÁâ©ÁêÜËØ≠‰πâÂêàÁêÜÊÄß„ÄÅ‰ªªÂä°‰∏ÄËá¥ÊÄßÔºâÂíå**ËßÜËßâË¥®Èáè**ÔºàËøêÂä®ÂπÖÂ∫¶„ÄÅÊú∫Âô®‰∫∫‰∏ª‰ΩìÁ®≥ÂÆöÊÄß„ÄÅËøêÂä®Âπ≥ÊªëÂ∫¶Ôºâ‰∏§‰∏™Áª¥Â∫¶ËøõË°åËØÑ‰º∞„ÄÇ\n*   **Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜ RoVid-X**ÔºöÈíàÂØπÈ´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆÂåÆ‰πèÁöÑÈóÆÈ¢òÔºå‰ΩúËÄÖÊûÑÂª∫‰∫ÜÁõÆÂâçÊúÄÂ§ßÁöÑÂºÄÊ∫êÊú∫Âô®‰∫∫ËßÜÈ¢ëÊï∞ÊçÆÈõÜ„ÄÇÈÄöËøá‰∏Ä‰∏™**ÂõõÈò∂ÊÆµÊï∞ÊçÆÊµÅÊ∞¥Á∫ø**ÔºàËßÜÈ¢ëÊî∂ÈõÜ„ÄÅË¥®ÈáèËøáÊª§„ÄÅ‰ªªÂä°ÂàÜÂâ≤‰∏éÊèèËø∞„ÄÅÁâ©ÁêÜÂ±ûÊÄßÊ†áÊ≥®ÔºâÔºåÊï¥Âêà‰∫ÜË∂ÖËøá20‰∏™ÂºÄÊ∫êÊï∞ÊçÆÈõÜÂíåÁΩëÁªúËßÜÈ¢ëÊ∫êÔºåÊúÄÁªàÂΩ¢Êàê‰∫ÜÂåÖÂê´**Á∫¶400‰∏á**‰∏™Ê†áÊ≥®ËßÜÈ¢ëÁâáÊÆµÁöÑÊï∞ÊçÆÈõÜÔºåÊ∂µÁõñ‰∫ÜÊï∞ÂçÉÁßç‰ªªÂä°Âíå‰∏∞ÂØåÁöÑÁâ©ÁêÜÂ±ûÊÄßÊ≥®Èáä„ÄÇ\n\n### 3. ‰∏ªË¶ÅÂÆûÈ™åÁªìÊûú\n*   ÂØπ**25‰∏™**‰ª£Ë°®ÊÄßËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºàÂºÄÊ∫ê„ÄÅÂïÜ‰∏ö„ÄÅÊú∫Âô®‰∫∫‰∏ìÁî®ÔºâÂú®RBench‰∏äËøõË°å‰∫ÜÁ≥ªÁªüËØÑ‰º∞„ÄÇ\n*   ÁªìÊûúÊòæÁ§∫ÔºåÂç≥‰ΩøÊòØÈ°∂Â∞ñÁöÑÂïÜ‰∏öÊ®°ÂûãÔºàÂ¶ÇWan 2.6ÔºâÔºåÂú®ÁîüÊàêÈúÄË¶Å**Á≤æÁªÜÁâ©ÁêÜ‰∫§‰∫íÂíåÈ´òÁ∫ßËÆ§Áü•Êé®ÁêÜ**ÁöÑ‰ªªÂä°ËßÜÈ¢ëÊó∂ÔºåÊÄßËÉΩ‰ªçÊúâÊòæËëó‰∏ãÈôçÔºåÊè≠Á§∫‰∫ÜÂΩìÂâçËßÜÈ¢ëÂü∫Á°ÄÊ®°Âûã‰∏éÂÖ∑Ë∫´‰ªªÂä°Ë¶ÅÊ±Ç‰πãÈó¥ÁöÑ**‚ÄúÁâ©ÁêÜÊô∫ËÉΩÈ∏øÊ≤ü‚Äù**„ÄÇ\n*   ‰ª§‰∫∫ÊÑèÂ§ñÁöÑÊòØÔºåÈù¢ÂêëÊ∂àË¥πÁ∫ßÂ™í‰ΩìÁöÑÁü•ÂêçÊ®°ÂûãÔºàÂ¶ÇSoraÁ≥ªÂàóÔºâÂú®ËØ•Âü∫ÂáÜ‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåÂá∏Êòæ‰∫Ü**‚ÄúÂ™í‰Ωì-‰ªøÁúü‚ÄùÈ¢ÜÂüüÂ∑ÆË∑ù**„ÄÇ\n*   RBenchÁöÑËá™Âä®ËØÑÂàÜ‰∏é‰∫∫Â∑•ÂÅèÂ•ΩËØÑ‰º∞ÁªìÊûúÈ´òÂ∫¶‰∏ÄËá¥Ôºà**ÊñØÁöÆÂ∞îÊõºÁõ∏ÂÖ≥Á≥ªÊï∞Ëææ0.96**ÔºâÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ\n*   ‰ΩøÁî®RoVid-XÂæÆË∞ÉÊ®°ÂûãËÉΩÁ®≥ÂÆöÊèêÂçáÂÖ∂Âú®ÂêÑÈ°π‰ªªÂä°ÂíåÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ‰∏äÁöÑÊÄßËÉΩÔºåËØÅÊòé‰∫ÜÊï∞ÊçÆÈõÜÁöÑ‰ª∑ÂÄº„ÄÇ\n\n### 4. Á†îÁ©∂ÊÑè‰πâÂíå‰ª∑ÂÄº\nÊú¨Á†îÁ©∂‰∏∫Êú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÈ¢ÜÂüüÂª∫Á´ã‰∫Ü**‰∏•Ë∞®ËØÑ‰º∞ÂíåËßÑÊ®°ÂåñËÆ≠ÁªÉ**ÁöÑÂùöÂÆûÂü∫Á°Ä„ÄÇ\n*   **RBench** ÂÉè‰∏ÄÈù¢‚ÄúÈïúÂ≠ê‚ÄùÔºåÁ≤æÂáÜÊè≠Á§∫‰∫ÜÂΩìÂâçÊ®°ÂûãÂú®Áâ©ÁêÜÁúüÂÆûÊÄßÂíå‰ªªÂä°ÈÄªËæëÊÄß‰∏äÁöÑÁº∫Èô∑Ôºå‰∏∫Ê®°ÂûãÊîπËøõÊåáÊòé‰∫ÜÊñπÂêë„ÄÇ\n*   **RoVid-X** ÂàôÊèê‰æõ‰∫Ü‚ÄúÁáÉÊñô‚ÄùÔºåÈÄöËøáÂ§ßËßÑÊ®°„ÄÅÈ´òË¥®Èáè„ÄÅÂ§öÊ†∑ÂåñÁöÑÊï∞ÊçÆÔºåÊúâÊúõËµãËÉΩ‰∏ã‰∏Ä‰ª£ËßÜÈ¢ëÊ®°ÂûãÂ≠¶‰π†Êõ¥Âº∫Â§ßÁöÑÁâ©ÁêÜ‰∫§‰∫íÂÖàÈ™å„ÄÇ\n*   ‰∫åËÄÖÂÖ±ÂêåÊûÑÊàê‰∫Ü‰∏Ä‰∏™**ÂçèÂêåÁîüÊÄÅÁ≥ªÁªü**ÔºåÂ∞ÜÂä†ÈÄüËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰ªéËøΩÊ±ÇËßÜËßâÈÄºÁúüÂà∞ÁêÜËß£Âπ∂Ê®°ÊãüÁâ©ÁêÜ‰∏ñÁïåÁöÑÊºîËøõÔºåÊé®Âä®ÂÖ∑Ë∫´AIÂêëÈÄöÁî®Êô∫ËÉΩÂèëÂ±ï„ÄÇ",
    "qa_pairs": [
        {
            "question": "ËøôÁØáËÆ∫ÊñáÁöÑÊ†∏ÂøÉÂàõÊñ∞ÁÇπÊòØ‰ªÄ‰πàÔºå‰ªñÊÉ≥Ëß£ÂÜ≥‰ªÄ‰πàÈóÆÈ¢òÔºåÊÄé‰πàËß£ÂÜ≥ÁöÑÔºü",
            "answer": "## ËÆ∫ÊñáÊ†∏ÂøÉÂàÜÊûêÔºö„ÄäRethinking Video Generation Model for the Embodied World„Äã\n\nËøôÁØáËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥**ÂÖ∑Ë∫´Êô∫ËÉΩÔºàEmbodied AIÔºâÈ¢ÜÂüü‰∏≠ÔºåËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁº∫‰πèÊúâÊïàËØÑ‰º∞Ê†áÂáÜÂíåÈ´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆ**ÁöÑÊ†∏ÂøÉÁì∂È¢à„ÄÇÂÖ∂Ê†∏ÂøÉÊÄùÊÉ≥ÊòØÔºåÂΩìÂâçÈÄöÁî®ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®ÁîüÊàêÁ¨¶ÂêàÁâ©ÁêÜËßÑÂæã„ÄÅ‰ªªÂä°ÈÄªËæëÁöÑÊú∫Âô®‰∫∫Ë°å‰∏∫ËßÜÈ¢ëÊñπÈù¢Â≠òÂú®ÊòæËëóÁº∫Èô∑ÔºåËÄåËøô‰∏ÄÁº∫Èô∑ÈòªÁ¢ç‰∫ÜËßÜÈ¢ë‰∏ñÁïåÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Â≠¶‰π†„ÄÅ‰ªøÁúüÂíåÁ≠ñÁï•ÁîüÊàê‰∏≠ÁöÑÂ∫îÁî®„ÄÇ\n\n### ‰∏Ä„ÄÅ Ê†∏ÂøÉÈóÆÈ¢ò\nËÆ∫ÊñáÊòéÁ°ÆÊåáÂá∫Âπ∂ËØïÂõæËß£ÂÜ≥‰ª•‰∏ã‰∏§‰∏™ÂÖ≥ÈîÆÈóÆÈ¢òÔºö\n1.  **ËØÑ‰º∞Ê†áÂáÜÁº∫Â§±**ÔºöÁé∞ÊúâËßÜÈ¢ëÁîüÊàêËØÑ‰º∞Âü∫ÂáÜÔºàÂ¶ÇVBenchÔºâ‰∏ªË¶ÅÂÖ≥Ê≥®**ËßÜËßâ‰øùÁúüÂ∫¶**ÔºàÊ∏ÖÊô∞Â∫¶„ÄÅÁæéËßÇÂ∫¶ÔºâÔºå‰∏•ÈáçÁº∫‰πèÂØπ**Áâ©ÁêÜÂêàÁêÜÊÄß**„ÄÅ**‰ªªÂä°ÂÆåÊàêÂ∫¶**Âíå**Âä®‰ΩúÈÄªËæë‰∏ÄËá¥ÊÄß**ÁöÑËØÑ‰º∞„ÄÇËøôÂØºËá¥Ê®°ÂûãÁîüÊàêÁöÑËßÜÈ¢ëÂèØËÉΩ‚ÄúÁúãËµ∑Êù•ÂæàÁæé‚ÄùÔºå‰ΩÜÊú∫Âô®‰∫∫Ë°å‰∏∫ËøùÂèçÁâ©ÁêÜÂ∏∏ËØÜÊàñÊó†Ê≥ïÂÆåÊàê‰ªªÂä°„ÄÇ\n2.  **ËÆ≠ÁªÉÊï∞ÊçÆÂåÆ‰πè**Ôºö‰∏éËÆ°ÁÆóÊú∫ËßÜËßâÂíåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÊã•ÊúâÊµ∑ÈáèÁΩëÁªúÊï∞ÊçÆ‰∏çÂêåÔºåÊú∫Âô®‰∫∫‰∫§‰∫íËßÜÈ¢ëÊï∞ÊçÆÂú®**ËßÑÊ®°**„ÄÅ**Â§öÊ†∑ÊÄß**ÔºàÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ„ÄÅ‰ªªÂä°Á±ªÂûã„ÄÅÁéØÂ¢ÉÔºâÂíå**Ë¥®Èáè**ÔºàÂàÜËæ®Áéá„ÄÅÊ†áÊ≥®Ôºâ‰∏äÂ≠òÂú®‰∏•Èáç‰∏çË∂≥ÔºåÈôêÂà∂‰∫ÜËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂ≠¶‰π†Áâ©ÁêÜ‰∫§‰∫íÂÖàÈ™åÁü•ËØÜÁöÑËÉΩÂäõ„ÄÇ\n\n### ‰∫å„ÄÅ Ê†∏ÂøÉÂàõÊñ∞ÁÇπ‰∏éËß£ÂÜ≥ÊñπÊ°à\nËÆ∫ÊñáÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™‚ÄúËØÑ‰º∞-Êï∞ÊçÆ‚ÄùÂçèÂêåÁîüÊÄÅÁ≥ªÁªüÔºåÁ≥ªÁªüÊÄßÂú∞Ëß£ÂÜ≥‰∫Ü‰∏äËø∞ÈóÆÈ¢ò„ÄÇ\n\n#### ÂàõÊñ∞ÁÇπ‰∏ÄÔºöÊèêÂá∫È¶ñ‰∏™Èù¢ÂêëÊú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÁöÑÁªºÂêàËØÑ‰º∞Âü∫ÂáÜ **RBench**\n- **ËÆæËÆ°ÊÄùË∑Ø**ÔºöË∂ÖË∂äÂÉèÁ¥†Á∫ßËØÑ‰º∞ÔºåËÅöÁÑ¶**‰ªªÂä°Ê≠£Á°ÆÊÄß**Âíå**ËßÜËßâË¥®Èáè**‰∏§‰∏™Áª¥Â∫¶„ÄÇ\n- **ÂÖ≥ÈîÆÊûÑÊàê**Ôºö\n    - **Â§öÊ†∑ÂåñËØÑ‰º∞ÈõÜ**ÔºöÂåÖÂê´650‰∏™Á≤æÂøÉËÆæËÆ°ÁöÑÂõæÂÉè-ÊñáÊú¨ÂØπÔºåË¶ÜÁõñ**5Á±ª‰ªªÂä°**ÔºàÂ∏∏ËßÅÊìç‰Ωú„ÄÅÈïøÊó∂Á®ãËßÑÂàí„ÄÅÂ§öÂÆû‰ΩìÂçè‰Ωú„ÄÅÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÅËßÜËßâÊé®ÁêÜÔºâÂíå**4ÁßçÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ**ÔºàÂçïËáÇ„ÄÅÂèåËáÇ„ÄÅ‰∫∫ÂΩ¢„ÄÅÂõõË∂≥Ôºâ„ÄÇ\n    - **Ëá™Âä®Âåñ„ÄÅÂèØÂ§çÁé∞ÁöÑÁªÜÁ≤íÂ∫¶ÊåáÊ†á**ÔºöÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫Èõ∂Ê†∑Êú¨ËØÑ‰º∞Âô®ÔºåÊèêÂá∫‰∏ÄÁ≥ªÂàóÂ≠êÊåáÊ†áÔºö\n        - **‰ªªÂä°ÂÆåÊàêÂ∫¶**ÔºöËØÑ‰º∞**Áâ©ÁêÜËØ≠‰πâÂêàÁêÜÊÄß**ÔºàÂ¶ÇÊºÇÊµÆ„ÄÅÁ©øÈÄè„ÄÅÈùûÊé•Ëß¶ÈôÑÁùÄÔºâÂíå**‰ªªÂä°‰æù‰ªé‰∏ÄËá¥ÊÄß**ÔºàÂÖ≥ÈîÆÂä®‰ΩúÊòØÂê¶ÈΩêÂÖ®„ÄÅÈ°∫Â∫èÊòØÂê¶Ê≠£Á°ÆÔºâ„ÄÇ\n        - **ËßÜËßâË¥®Èáè**ÔºöËØÑ‰º∞**ËøêÂä®ÂπÖÂ∫¶**ÔºàÊÉ©ÁΩöÈùôÊÄÅÊú∫Âô®‰∫∫Ôºâ„ÄÅ**Êú∫Âô®‰∫∫-‰∏ª‰ΩìÁ®≥ÂÆöÊÄß**ÔºàÂΩ¢ÊÄÅ„ÄÅÂ±ûÊÄßÊòØÂê¶ÊºÇÁßªÔºâ„ÄÅ**ËøêÂä®Âπ≥ÊªëÂ∫¶**„ÄÇ\n- **È™åËØÅÊúâÊïàÊÄß**ÔºöÂú®25‰∏™‰ª£Ë°®ÊÄßÊ®°Âûã‰∏äÁöÑËØÑ‰º∞ÁªìÊûú‰∏é‰∫∫Á±ªÂÅèÂ•ΩËØÑÂàÜÈ´òÂ∫¶Áõ∏ÂÖ≥ÔºàÊñØÁöÆÂ∞îÊõºÁõ∏ÂÖ≥Á≥ªÊï∞Ëææ **0.96**ÔºâÔºåËØÅÊòé‰∫ÜÂÖ∂ËØÑ‰º∞ÁöÑÊúâÊïàÊÄß„ÄÇ\n\n#### ÂàõÊñ∞ÁÇπ‰∫åÔºöÊûÑÂª∫ÁõÆÂâçÊúÄÂ§ßËßÑÊ®°ÁöÑÂºÄÊ∫êÊú∫Âô®‰∫∫ËßÜÈ¢ëÊï∞ÊçÆÈõÜ **RoVid-X**\n- **ËÆæËÆ°ÊÄùË∑Ø**Ôºö‰∏∫Ëß£ÂÜ≥ËÆ≠ÁªÉÊï∞ÊçÆÁì∂È¢àÔºåÊûÑÂª∫‰∏Ä‰∏™Â§ßËßÑÊ®°„ÄÅÈ´òË¥®Èáè„ÄÅÊ†áÊ≥®‰∏∞ÂØåÁöÑÊú∫Âô®‰∫∫ËßÜÈ¢ëÊï∞ÊçÆÈõÜ„ÄÇ\n- **ÂõõÈò∂ÊÆµÊï∞ÊçÆÊµÅÊ∞¥Á∫ø**Ôºö\n    1.  **Êú∫Âô®‰∫∫ËßÜÈ¢ëÊî∂ÈõÜ**Ôºö‰ªé‰∫íËÅîÁΩëËßÜÈ¢ëÂπ≥Âè∞Âíå20Â§ö‰∏™ÂºÄÊ∫êÊï∞ÊçÆÈõÜ‰∏≠Êî∂ÈõÜÂéüÂßãËßÜÈ¢ë„ÄÇ\n    2.  **ËßÜÈ¢ëË¥®ÈáèËøáÊª§**ÔºöÂü∫‰∫éÊ∏ÖÊô∞Â∫¶„ÄÅÂä®ÊÄÅÊïàÊûúÁ≠âÂ§öÁª¥Â∫¶ËøõË°åËá™Âä®ÊâìÂàÜËøáÊª§„ÄÇ\n    3.  **‰ªªÂä°ÂàÜÂâ≤‰∏éÊèèËø∞**Ôºö‰ΩøÁî®ËßÜÈ¢ëÁêÜËß£Ê®°ÂûãËá™Âä®ÂàÜÂâ≤‰ªªÂä°ÁâáÊÆµÂπ∂ÁîüÊàêÊ†áÂáÜÂåñÂä®‰ΩúÊèèËø∞„ÄÇ\n    4.  **Áâ©ÁêÜÂ±ûÊÄßÊ†áÊ≥®**ÔºöÁªü‰∏ÄÊ†áÊ≥®**ÂÖâÊµÅ**ÔºàÂä®‰Ωú‰∏ÄËá¥ÊÄßÔºâÂíå**Ê∑±Â∫¶Âõæ**ÔºàÁ©∫Èó¥ÂÖ≥Á≥ªÔºâÔºå‰∏∫Ê®°ÂûãÊèê‰æõ‰∏∞ÂØåÁöÑÁâ©ÁêÜÂÖàÈ™å„ÄÇ\n- **Êï∞ÊçÆÈõÜËßÑÊ®°‰∏éÁâπÁÇπ**ÔºöÂåÖÂê´Á∫¶ **400‰∏á** ‰∏™Á≤æÈÄâËßÜÈ¢ëÁâáÊÆµÔºåË¶ÜÁõñ **1300+** ÁßçÊäÄËÉΩÔºåÂàÜËæ®ÁéáËææ720PÔºåÊòØÈ¶ñ‰∏™‰∏ì‰∏∫ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãËÆæËÆ°ÁöÑÂ§ßËßÑÊ®°Êú∫Âô®‰∫∫Êï∞ÊçÆÈõÜ„ÄÇ\n\n### ‰∏â„ÄÅ ‰∏ªË¶ÅÂÆûÈ™åÂèëÁé∞‰∏éÊ¥ûËßÅ\nÈÄöËøáÂØπ25‰∏™Ê®°ÂûãÔºàÂºÄÊ∫ê„ÄÅÂïÜ‰∏ö„ÄÅÊú∫Âô®‰∫∫‰∏ìÁî®ÔºâÁöÑËØÑ‰º∞ÔºåËÆ∫ÊñáÊè≠Á§∫‰∫ÜÈáçË¶ÅË∂ãÂäøÔºö\n- **‰ªé‚ÄúËßÜËßâ‰øùÁúü‚ÄùÂà∞‚ÄúÁâ©ÁêÜÊô∫ËÉΩ‚ÄùÁöÑËåÉÂºèËΩ¨Âèò**ÔºöÈ°∂Á∫ßÂïÜ‰∏öÊ®°ÂûãÔºàÂ¶ÇWan 2.6ÔºâÂºÄÂßãÂ±ïÁé∞Âá∫‰Ωú‰∏∫**‰∏ñÁïåÊ®°ÊãüÂô®**ÁöÑÊΩúÂäõÔºåËÄå‰∏ç‰ªÖÈôê‰∫éÁîüÊàêÁæéËßÇËßÜÈ¢ë„ÄÇ\n- **Ëø≠‰ª£Áº©ÊîæËß£ÈîÅÁâ©ÁêÜËÉΩÂäõ**ÔºöÊ®°ÂûãËø≠‰ª£‰∏éÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõÊèêÂçáÂº∫Áõ∏ÂÖ≥ÔºàÂ¶ÇWanÁ≥ªÂàó‰ªé2.1Âà∞2.6ÁöÑÊòæËëóÈ£ûË∑ÉÔºâ„ÄÇ\n- **‚ÄúÂ™í‰Ωì-‰ªøÁúü‚ÄùÈ∏øÊ≤ü**ÔºöÈù¢ÂêëÊ∂àË¥πÁ∫ßÂ™í‰ΩìÁîüÊàêÁöÑÁü•ÂêçÊ®°ÂûãÔºàÂ¶ÇSoraÁ≥ªÂàóÔºâÂú®ËØ•Âü∫ÂáÜ‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåËØ¥Êòé**ÂàõÊÑèËßÜÈ¢ëÁîüÊàêËÉΩÂäõ‰∏çËÉΩËá™ÁÑ∂ËøÅÁßªÂà∞ÂÖ∑Ë∫´AI‰ªªÂä°**„ÄÇ\n- **Èó≠Ê∫êÊ®°ÂûãÈ¢ÜÂÖà**ÔºöÂïÜ‰∏öÈó≠Ê∫êÊ®°ÂûãÂú®ÊéíË°åÊ¶ú‰∏äÂç†ÊçÆÁªùÂØπ‰ºòÂäøÔºåÁ™ÅÊòæ‰∫ÜÂºÄÊ∫êÁ§æÂå∫Âú®Áâ©ÁêÜÊï∞ÊçÆËßÑÊ®°ÂíåÊû∂ÊûÑ‰ºòÂåñ‰∏äÁöÑÁ¥ßËø´ÈúÄÊ±Ç„ÄÇ\n- **‰∏ì‰∏öÂåñÂõ∞Â¢É**Ôºö‰∏ìÁî®Êú∫Âô®‰∫∫Ê®°ÂûãÔºàÂ¶ÇCosmos 2.5ÔºâËôΩËêΩÂêé‰∫éÈ°∂Á∫ßÈÄöÁî®Ê®°ÂûãÔºå‰ΩÜ‰ºò‰∫éÂ§ßÂûãÂºÄÊ∫êËßÜÈ¢ëÊ®°ÂûãÔºåË°®Êòé**Áâ©ÁêÜÊï∞ÊçÆËÆ≠ÁªÉËÉΩÂ∏¶Êù•Á®≥ÂÆöÊî∂Áõä**„ÄÇÁÑ∂ËÄåÔºåËøáÂ∫¶‰∏ìÊ≥®‰∫éÁâπÂÆöÊú∫Âô®‰∫∫ÁöÑÊ®°ÂûãÔºàÂ¶ÇVidarÔºâË°®Áé∞ÂæàÂ∑ÆÔºåËØ¥Êòé**È¢ÜÂüüÊï∞ÊçÆÊó†Ê≥ïÂÆåÂÖ®Âº•Ë°•Â§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÊèê‰æõÁöÑ‚Äú‰∏ñÁïåÁü•ËØÜ‚Äù**„ÄÇ\n- **ËÆ§Áü•‰∏éÁ≤æÁªÜÊéßÂà∂Áì∂È¢à**ÔºöÊâÄÊúâÊ®°ÂûãÂú®ÈúÄË¶Å**È´òÁ∫ßÈÄªËæëÊé®ÁêÜ**ÔºàËßÜËßâÊé®ÁêÜ‰ªªÂä°ÔºâÂíå**Á≤æÁªÜÊìç‰Ωú**ÔºàÂçïËáÇ/ÂèåËáÇÊú∫Âô®‰∫∫‰ªªÂä°Ôºâ‰∏äË°®Áé∞ÊúÄÂ∑ÆÔºåÊè≠Á§∫‰∫ÜÂΩìÂâçÊ®°ÂûãÁöÑÁâ©ÁêÜÁêÜËß£ÊûÅÈôê„ÄÇ\n\n### Âõõ„ÄÅ ÂÆûÈôÖ‰ª∑ÂÄº‰∏éÊÑè‰πâ\n1.  **‰∏∫È¢ÜÂüüËÆæÁ´ãÊ†áÂáÜ**Ôºö**RBench** Êèê‰æõ‰∫ÜÈ¶ñ‰∏™Á≥ªÁªü„ÄÅÂèØÂ§çÁé∞ÁöÑËØÑ‰º∞Â∑•ÂÖ∑Ôºå‰Ωø‰∏çÂêåËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Âú∫ÊôØ‰∏ãÁöÑÊÄßËÉΩÂèØ‰ª•ËøõË°åÂÖ¨Âπ≥„ÄÅÂÖ®Èù¢ÁöÑÊØîËæÉÔºåÂ∞ÜÊé®Âä®Á†îÁ©∂ÂêëÁâ©ÁêÜÁúüÂÆûÊÄßÂíå‰ªªÂä°ÂÆûÁî®ÊÄßÊñπÂêëÂèëÂ±ï„ÄÇ\n2.  **Êèê‰æõÂÖ≥ÈîÆËÆ≠ÁªÉËµÑÊ∫ê**Ôºö**RoVid-X** Êï∞ÊçÆÈõÜÊûÅÂ§ßÁºìËß£‰∫ÜÊú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÈ¢ÜÂüüÁöÑÊï∞ÊçÆËçíÔºå‰∏∫ËÆ≠ÁªÉ‰∏ã‰∏Ä‰ª£ÂÖ∑Â§áÁâ©ÁêÜÂ∏∏ËØÜÁöÑËßÜÈ¢ë‰∏ñÁïåÊ®°ÂûãÂ•†ÂÆö‰∫ÜÊï∞ÊçÆÂü∫Á°Ä„ÄÇ\n3.  **ÊåáÊòéÁ†îÁ©∂ÊñπÂêë**ÔºöËÆ∫ÊñáÁöÑËØÑ‰º∞ÁªìÊûúÊ∏ÖÊô∞Âú∞ÊåáÂá∫‰∫ÜÂΩìÂâçÊ®°ÂûãÁöÑÁü≠ÊùøÔºàÂ¶ÇÁâ©ÁêÜÂêàÁêÜÊÄß„ÄÅÈïøÊó∂Á®ãËßÑÂàíÔºâÔºåÂπ∂ÊöóÁ§∫‰∫ÜÊîπËøõÊñπÂêëÔºö**ÈúÄË¶ÅËûçÂêàÂ§ßËßÑÊ®°ÈÄöÁî®È¢ÑËÆ≠ÁªÉ‰∏éÈ´òË¥®ÈáèÈ¢ÜÂüüÁâπÂÆöÊï∞ÊçÆ**ÔºåÂπ∂ËÆæËÆ°ËÉΩÊõ¥Â•ΩÁêÜËß£Áâ©ÁêÜÂíåÂõ†ÊûúÂÖ≥Á≥ªÁöÑÊ®°ÂûãÊû∂ÊûÑ„ÄÇ\n\n**ÊÄªÁªì**ÔºöËØ•ËÆ∫ÊñáÈÄöËøá**È¶ñÂàõÁöÑËØÑ‰º∞Âü∫ÂáÜRBench**Âíå**Â§ßËßÑÊ®°È´òË¥®ÈáèÊï∞ÊçÆÈõÜRoVid-X**ÔºåÁ≥ªÁªüÊÄßÂú∞ËØäÊñ≠Âπ∂ÁùÄÊâãËß£ÂÜ≥‰∫ÜÊú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®ËØÑ‰º∞ÂíåËÆ≠ÁªÉÊñπÈù¢ÁöÑÊ†∏ÂøÉÊåëÊàò„ÄÇÂÆÉ‰∏ç‰ªÖÊè≠Á§∫‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄßÔºåÊõ¥‰∏∫Êï¥‰∏™È¢ÜÂüüÊú™Êù•ÁöÑÂèëÂ±ïÊèê‰æõ‰∫ÜÂùöÂÆûÁöÑËØÑ‰º∞Ê°ÜÊû∂ÂíåÊï∞ÊçÆÂü∫Á°ÄÔºåÂä†ÈÄü‰∫ÜËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂêëÁúüÊ≠£ÂèØÁî®ÁöÑ‚ÄúÁâ©ÁêÜ‰∏ñÁïåÊ®°ÊãüÂô®‚ÄùÁöÑÊºîËøõ„ÄÇ",
            "timestamp": "2026-01-27T16:02:02.903245",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "Âü∫‰∫é‰ªñÁöÑÂâç‰ΩúÔºåÊ¢≥ÁêÜËøô‰∏™ÊñπÂêëÁöÑÊï¥‰∏™ÂèëÂ±ïËÑâÁªúÔºåÊØè‰∏ÄÊ≠•Áõ∏ÊØî‰∫é‰πãÂâçÁöÑÂ∑•‰ΩúÈÉΩÊîπËøõ‰∫Ü‰ªÄ‰πàÔºåÁùÄÈáç‰∫éÂá†‰∏™‰∏çÂêåÁöÑÂèëÂ±ïÊñπÂêë„ÄÇ",
            "answer": "## ËÆ∫Êñá„ÄäRethinking Video Generation Model for the Embodied World„ÄãÂèëÂ±ïËÑâÁªúÊ¢≥ÁêÜ\n\nÊú¨ÊñáÊòØËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®ÂÖ∑Ë∫´Êô∫ËÉΩÔºàEmbodied AIÔºâÈ¢ÜÂüüÂ∫îÁî®ÁöÑ‰∏ÄÊ¨°Á≥ªÁªüÊÄßÊÄªÁªì‰∏éÊé®Ëøõ„ÄÇÂÖ∂Ê†∏ÂøÉË¥°ÁåÆÂú®‰∫é**Â°´Ë°•‰∫Ü‚ÄúËØÑ‰º∞‚Äù‰∏é‚ÄúÊï∞ÊçÆ‚Äù‰∏§Â§ßÂÖ≥ÈîÆÁ©∫ÁôΩ**Ôºå‰∏∫È¢ÜÂüüÂèëÂ±ïÂª∫Á´ã‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜÂíåÂü∫Á°ÄËÆæÊñΩ„ÄÇË¶ÅÁêÜËß£ÂÖ∂ÂÆö‰ΩçÔºåÈúÄÊ¢≥ÁêÜËØ•ÊñπÂêë‰ªéÈÄöÁî®ËßÜÈ¢ëÁîüÊàêÂà∞ÂÖ∑Ë∫´‰∏ñÁïåÂª∫Ê®°ÁöÑÊºîËøõËÑâÁªú„ÄÇ\n\n### ÂèëÂ±ïËÑâÁªú‰∏éÂÖ≥ÈîÆÊîπËøõ\n\nËØ•È¢ÜÂüüÁöÑÂèëÂ±ïÂèØ‰ª•Ê¶ÇÊã¨‰∏∫‰ªé**ÈÄöÁî®ËßÜËßâÁîüÊàê**Âà∞**Áâ©ÁêÜ‰∏ñÁïåÊ®°Êãü**ÔºåÂÜçÂà∞**ÂÖ∑Ë∫´‰ªªÂä°‰∏ìÁî®**ÁöÑÊºîËøõËøáÁ®ãÔºåÊØè‰∏ÄÊ≠•ÈÉΩÈíàÂØπÂâç‰ΩúÁöÑÂ±ÄÈôêÊÄßËøõË°å‰∫ÜÂÖ≥ÈîÆÊîπËøõ„ÄÇ\n\n#### 1. Á¨¨‰∏ÄÈò∂ÊÆµÔºöÈÄöÁî®ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑÂÖ¥Ëµ∑‰∏éÂ±ÄÈôê\n*   **Ââç‰Ωú‰ª£Ë°®**ÔºöSora„ÄÅVeo„ÄÅWan„ÄÅHunyuanVideoÁ≠âÂ§ßËßÑÊ®°Êâ©Êï£Ê®°Âûã„ÄÇ\n*   **Ê†∏ÂøÉËÉΩÂäõ**ÔºöÂÆûÁé∞‰∫Ü‰ªéÊñáÊú¨/ÂõæÂÉèÁîüÊàêÈ´ò‰øùÁúü„ÄÅÈïøÂ∫èÂàó„ÄÅÂØåÊúâÂàõÊÑèËßÜÈ¢ëÁöÑÁ™ÅÁ†¥ÔºåÂÖ≥Ê≥®**ËßÜËßâÈÄºÁúüÂ∫¶**ÔºàÊ∏ÖÊô∞Â∫¶„ÄÅÁ∫πÁêÜ„ÄÅËøêÂä®Âπ≥ÊªëÂ∫¶ÔºâÂíå**ÊñáÊú¨-ËßÜÈ¢ëÂØπÈΩê**„ÄÇ\n*   **‰∏ªË¶ÅÂ±ÄÈôêÔºàÊú¨ÊñáÊåáÂá∫Ôºâ**Ôºö\n    *   **ËØÑ‰º∞ËÑ±ËäÇ**ÔºöÁé∞ÊúâËØÑÊµãÂü∫ÂáÜÔºàÂ¶ÇVBenchÔºâ‰∏ªË¶ÅÈù¢ÂêëÂ™í‰ΩìÊ∂àË¥πÂú∫ÊôØÔºåÁº∫‰πèÂØπ**Áâ©ÁêÜÂêàÁêÜÊÄß**„ÄÅ**‰ªªÂä°ÂÆåÊàêÂ∫¶**Âíå**Êú∫Âô®‰∫∫Ë°å‰∏∫ÁâπÂºÇÊÄß**ÁöÑËØÑ‰º∞„ÄÇ\n    *   **Êï∞ÊçÆÂÅèÂ∑Æ**ÔºöËÆ≠ÁªÉÊï∞ÊçÆ‰ª•‰∫∫Á±ª‰∏≠ÂøÉ„ÄÅÂΩ±ËßÜÂåñÂÜÖÂÆπ‰∏∫‰∏ªÔºåÁº∫‰πèÂ§ßËßÑÊ®°„ÄÅÈ´òË¥®ÈáèÁöÑÊú∫Âô®‰∫∫‰∫§‰∫íËßÜÈ¢ëÔºåÂØºËá¥Ê®°ÂûãÂØπÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ„ÄÅÁâ©‰Ωì‰∫§‰∫íÁâ©ÁêÜ„ÄÅ‰ªªÂä°ÈÄªËæëÁöÑÁêÜËß£‰∏çË∂≥„ÄÇ\n    *   **‚ÄúÂ™í‰Ωì-Ê®°Êãü‚ÄùÈ∏øÊ≤ü**Ôºö‰∏∫ËßÜËßâÁæéÊÑü‰ºòÂåñÁöÑÊ®°ÂûãÔºåÂÖ∂ÁîüÊàêÁªìÊûúÂú®Áâ©ÁêÜ‰∏äÂ∏∏Â∏∏‰∏çÂêàÁêÜÔºàÂ¶ÇÁâ©‰ΩìÊºÇÊµÆ„ÄÅÁ©øÈÄè„ÄÅÂä®‰ΩúÈÄªËæëÈîôËØØÔºâ„ÄÇ\n\n#### 2. Á¨¨‰∫åÈò∂ÊÆµÔºöÈù¢ÂêëÁâ©ÁêÜ‰∏ñÁïåÁöÑËßÜÈ¢ëÁîüÊàêÂü∫ÂáÜÂàùÊé¢\n*   **Ââç‰Ωú‰ª£Ë°®**ÔºöVideoPhy„ÄÅPhyBench„ÄÅT2VPhysBenchÁ≠â„ÄÇ\n*   **Ê†∏ÂøÉÊîπËøõ**ÔºöÂºÄÂßãÂÖ≥Ê≥®ÁîüÊàêËßÜÈ¢ëÁöÑ**Áâ©ÁêÜÂ∏∏ËØÜ**ÔºåËØÑ‰º∞Ê®°ÂûãÂØπÂü∫Á°ÄÁâ©ÁêÜÂÆöÂæãÔºàÂ¶ÇÈáçÂäõ„ÄÅÁ¢∞Êíû„ÄÅÊÉØÊÄßÔºâÁöÑÈÅµÂÆàÊÉÖÂÜµ„ÄÇ\n*   **‰∏ªË¶ÅÂ±ÄÈôêÔºàÊú¨ÊñáÊåáÂá∫Ôºâ**Ôºö\n    *   **ËØÑ‰º∞Áª¥Â∫¶Âçï‰∏Ä**ÔºöÂ§öÈõÜ‰∏≠‰∫éÂ≠§Á´ãÁöÑÁâ©ÁêÜÁ∫¶ÊùüËøùÂèçÊ£ÄÊµãÔºå**Êú™ËÉΩ‰∏éÂÖ∑‰ΩìÁöÑÊú∫Âô®‰∫∫‰ªªÂä°ÂíåÂä®‰ΩúËØ≠‰πâÁõ∏ÁªìÂêà**„ÄÇ\n    *   **Áº∫‰πè‰ªªÂä°ÂØºÂêë**ÔºöÊ≤°ÊúâÁ≥ªÁªüËØÑ‰º∞**‰ªªÂä°Á∫ßÊ≠£Á°ÆÊÄß**ÔºàÂ¶ÇÊìç‰ΩúÊ≠•È™§ÊòØÂê¶ÂÆåÊï¥„ÄÅÈ°∫Â∫èÊòØÂê¶Ê≠£Á°ÆÔºâÂíå**Â§öÂΩ¢ÊÄÅÊú∫Âô®‰∫∫**ÔºàEmbodimentÔºâ‰∏ãÁöÑÁ©∫Èó¥Á∫¶Êùü„ÄÇ\n    *   **Êï∞ÊçÆÈõÜÁº∫Â§±**ÔºöÁº∫‰πè‰∏Ä‰∏™Ë¶ÜÁõñÂ§öÊ†∑Êú∫Âô®‰∫∫ÂΩ¢ÊÄÅÂíåÂ§çÊùÇ‰ªªÂä°ÁöÑÊ†áÂáÜÂåñËØÑÊµãÈõÜ„ÄÇ\n\n#### 3. Á¨¨‰∏âÈò∂ÊÆµÔºöÊú∫Âô®‰∫∫Â≠¶‰π†‰∏ìÁî®Êï∞ÊçÆÈõÜÁöÑÁßØÁ¥Ø\n*   **Ââç‰Ωú‰ª£Ë°®**ÔºöOpen X-Embodiment„ÄÅBridgeData„ÄÅDROID„ÄÅHumanoid EverydayÁ≠âÂ§ßËßÑÊ®°Êú∫Âô®‰∫∫Êï∞ÊçÆÈõÜ„ÄÇ\n*   **Ê†∏ÂøÉÊîπËøõ**ÔºöÊî∂ÈõÜ‰∫ÜÁúüÂÆû‰∏ñÁïåÊàñ‰ªøÁúüÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÊï∞ÊçÆÔºåÁî®‰∫éËÆ≠ÁªÉÊú∫Âô®‰∫∫Á≠ñÁï•Ê®°ÂûãÔºåÊé®Âä®‰∫ÜÊú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ\n*   **‰∏ªË¶ÅÂ±ÄÈôêÔºàÊú¨ÊñáÊåáÂá∫Ôºâ**Ôºö\n    *   **ÁõÆÁöÑ‰∏çÂêå**ÔºöËøô‰∫õÊï∞ÊçÆÈõÜ‰∏ªË¶Å‰∏∫**Á≠ñÁï•Â≠¶‰π†**ËÆæËÆ°ÔºåËÄåÈùû‰∏∫**ËßÜÈ¢ëÁîüÊàêÊ®°Âûã**ÁöÑËÆ≠ÁªÉÂíåËØÑ‰º∞‰ºòÂåñ„ÄÇ\n    *   **Ë¥®Èáè‰∏éÊ†áÊ≥®‰∏çË∂≥**ÔºöËßÜÈ¢ëÂàÜËæ®Áéá‰Ωé„ÄÅÂ§öÊ†∑ÊÄßÊúâÈôêÔºàÁâπÂÆöÊú∫Âô®‰∫∫„ÄÅÁã≠Á™Ñ‰ªªÂä°ÂàÜÂ∏ÉÔºâÔºå‰∏îÁº∫‰πèÂØπÁâ©ÁêÜÂ±ûÊÄßÔºàÂ¶ÇÂÖâÊµÅ„ÄÅÊ∑±Â∫¶ÔºâÂíåÁªÜÁ≤íÂ∫¶‰ªªÂä°ÊèèËø∞ÁöÑÊ†áÂáÜÂåñÊ†áÊ≥®„ÄÇ\n    *   **Ê†ºÂºè‰∏çÁªü‰∏Ä**Ôºö‰∏çÂêåÊï∞ÊçÆÈõÜÈááÈõÜÂíåÂ≠òÂÇ®ÊñπÂºèÂêÑÂºÇÔºåÈöæ‰ª•Áõ¥Êé•Áî®‰∫éËÆ≠ÁªÉÁªü‰∏ÄÁöÑËßÜÈ¢ëÁîüÊàêÂü∫Á°ÄÊ®°Âûã„ÄÇ\n\n#### 4. Á¨¨ÂõõÈò∂ÊÆµÔºöÊú¨ÊñáÂ∑•‰Ωú‚Äî‚ÄîÊûÑÂª∫ËØÑ‰º∞‰∏éÊï∞ÊçÆÁöÑÂçèÂêåÁîüÊÄÅÁ≥ªÁªü\nÊú¨ÊñáÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÔºå**È¶ñÊ¨°Á≥ªÁªüÊÄßÂú∞Â∞ÜËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑËØÑ‰º∞ÂíåËÆ≠ÁªÉÔºå‰∏éÂÖ∑Ë∫´Êô∫ËÉΩÁöÑÁã¨ÁâπÈúÄÊ±ÇÂØπÈΩê**ÔºåËß£ÂÜ≥‰∫ÜÂâçËø∞ÊâÄÊúâÈò∂ÊÆµÁöÑÈÅóÁïôÈóÆÈ¢ò„ÄÇ\n\n*   **ÊîπËøõ1ÔºöÊèêÂá∫È¶ñ‰∏™Èù¢ÂêëÊú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÁöÑÁªºÂêàËØÑÊµãÂü∫ÂáÜ `RBench`**\n    *   **Áõ∏ÊØî‰∫éÈÄöÁî®ËØÑÊµã**Ôºö‰ªé‚ÄúËßÜËßâË¥®Èáè‚ÄùËΩ¨Âêë **‚ÄúÁâ©ÁêÜÊô∫ËÉΩ‚Äù** ÔºåËØÑ‰º∞Áª¥Â∫¶ÂåÖÊã¨**‰ªªÂä°ÂÆåÊàêÂ∫¶**ÔºàÁâ©ÁêÜËØ≠‰πâÂêàÁêÜÊÄß„ÄÅ‰ªªÂä°‰æù‰ªé‰∏ÄËá¥ÊÄßÔºâÂíå**ËßÜËßâË¥®Èáè**ÔºàÊú∫Âô®‰∫∫‰∏ª‰ΩìÁ®≥ÂÆöÊÄß„ÄÅËøêÂä®ÂπÖÂ∫¶‰∏éÂπ≥ÊªëÂ∫¶Ôºâ„ÄÇ\n    *   **Áõ∏ÊØî‰∫éÁâ©ÁêÜÂ∏∏ËØÜËØÑÊµã**ÔºöÂ∞ÜÁâ©ÁêÜËØÑ‰º∞**ÂÖ∑Ë±°ÂåñÂà∞Êú∫Âô®‰∫∫Êìç‰ΩúÂú∫ÊôØ**ÔºåËÆæËÆ°‰∫Ü‰∫îÂ§ß‰ªªÂä°Á±ªÂà´ÔºàÂ∏∏ËßÅÊìç‰Ωú„ÄÅÈïøÊó∂Á®ãËßÑÂàíÁ≠âÔºâÂíåÂõõÁßçÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÔºåËØÑÊµãÊõ¥ÂÖ∑ÈíàÂØπÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ\n    *   **‰ª∑ÂÄº**ÔºöÊèê‰æõ‰∫ÜÂèØÂ§çÁé∞ÁöÑËá™Âä®ÂåñËØÑÊµãÊåáÊ†áÔºàÂü∫‰∫éMLLMÔºâÔºåÂπ∂È™åËØÅ‰∫Ü‰∏é‰∫∫Á±ªËØÑ‰ª∑ÁöÑÈ´òÁõ∏ÂÖ≥ÊÄßÔºàSpearman œÅ=0.96ÔºâÔºåÊè≠Á§∫‰∫ÜÂΩìÂâçÈ°∂Â∞ñÊ®°ÂûãÔºàÂ¶ÇSoraÔºâÂú®ÂÖ∑Ë∫´‰ªªÂä°‰∏äÁöÑÊòæËëóÁº∫Èô∑„ÄÇ\n\n*   **ÊîπËøõ2ÔºöÊûÑÂª∫ÁõÆÂâçÊúÄÂ§ß„ÄÅÊúÄ‰ºòË¥®ÁöÑÊú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÊï∞ÊçÆÈõÜ `RoVid-X`**\n    *   **Áõ∏ÊØî‰∫éÊú∫Âô®‰∫∫Â≠¶‰π†Êï∞ÊçÆÈõÜ**Ôºö**‰∏ì‰∏∫ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãËÆ≠ÁªÉËÆæËÆ°**„ÄÇÈÄöËøáÂõõÈò∂ÊÆµÊµÅÊ∞¥Á∫øÔºàÊî∂ÈõÜ„ÄÅË¥®ÈáèËøáÊª§„ÄÅ‰ªªÂä°ÂàÜÂâ≤‰∏éÊèèËø∞„ÄÅÁâ©ÁêÜÂ±ûÊÄßÊ†áÊ≥®ÔºâÔºåÊèê‰æõ‰∫Ü400‰∏áÊù°È´òË¥®Èáè„ÄÅÊ†áÊ≥®‰∏∞ÂØåÁöÑËßÜÈ¢ëÁâáÊÆµ„ÄÇ\n    *   **ÂÖ≥ÈîÆÂ¢ûÂº∫**ÔºöÂåÖÂê´‰∫Ü**Ê†áÂáÜÂåñ‰ªªÂä°ÊèèËø∞**„ÄÅ**ÂÖâÊµÅ**Âíå**Ê∑±Â∫¶Âõæ**Á≠âÁâ©ÁêÜÂ±ûÊÄßÊ≥®ÈáäÔºåËøôÊòØ‰ª•ÂæÄÊï∞ÊçÆÈõÜÊâÄ‰∏çÂÖ∑Â§áÁöÑÔºåËÉΩ‰∏∫Ê®°ÂûãÊ≥®ÂÖ•Êõ¥Âº∫ÁöÑÁâ©ÁêÜ‰∫§‰∫íÂÖàÈ™å„ÄÇ\n    *   **‰ª∑ÂÄº**ÔºöËß£ÂÜ≥‰∫ÜËÆ≠ÁªÉÊï∞ÊçÆÂú®ËßÑÊ®°„ÄÅÂ§öÊ†∑ÊÄß„ÄÅË¥®Èáè‰∏äÁöÑÁì∂È¢àÔºåÂÆûÈ™åËØÅÊòéÂÖ∂ËÉΩÁ®≥ÂÆöÊèêÂçá‰∏çÂêåËßÜÈ¢ëÊ®°ÂûãÂú®`RBench`‰∏äÁöÑÊÄßËÉΩ„ÄÇ\n\n*   **ÊîπËøõ3ÔºöÈÄöËøáÂ§ßËßÑÊ®°ËØÑÊµãÊè≠Á§∫ÂÖ≥ÈîÆÈ¢ÜÂüüÊ¥ûËßÅ**\n    *   **ËåÉÂºèËΩ¨Âèò**ÔºöÊåáÂá∫È¢ÜÂÖàÁöÑÂïÜ‰∏öÊ®°ÂûãÔºàÂ¶ÇWan 2.6ÔºâÊ≠£‰ªé‚ÄúËßÜËßâÁîüÊàêÂô®‚ÄùÊºîÂèò‰∏∫ **‚Äú‰∏ñÁïåÊ®°ÊãüÂô®‚Äù** ÔºåÊ†áÂøóÁùÄËßÜÈ¢ëÁîüÊàêËøõÂÖ• **‚ÄúÁâ©ÁêÜAI‚Äù** Êñ∞Èò∂ÊÆµ„ÄÇ\n    *   **ÊÄßËÉΩÈ∏øÊ≤ü**ÔºöÊè≠Á§∫‰∫Ü**Èó≠Ê∫êÊ®°ÂûãÊòæËëóÈ¢ÜÂÖà‰∫éÂºÄÊ∫êÊ®°Âûã**Ôºå‰ª•Âèä**ÈÄöÁî®Âü∫Á°ÄÊ®°Âûã‰ºò‰∫é‰∏ìÁî®Êú∫Âô®‰∫∫Ê®°Âûã**ÁöÑÁé∞Áä∂ÔºåÂº∫Ë∞É‰∫Ü**‰∏ñÁïåÁü•ËØÜ**‰∏é**ËßÑÊ®°ÂåñÁâ©ÁêÜÊï∞ÊçÆ**ÁöÑÈáçË¶ÅÊÄß„ÄÇ\n    *   **Áì∂È¢àËØäÊñ≠**ÔºöÊòéÁ°Æ‰∫Ü**ÈúÄË¶ÅÈ´òÂ±ÇÈÄªËæëÁöÑËßÜËßâÊé®ÁêÜ‰ªªÂä°**Âíå**ÈúÄË¶ÅÁ≤æÁªÜÊé•Ëß¶Âä®ÂäõÂ≠¶**ÁöÑÊìç‰Ωú‰ªªÂä°ÔºåÊòØÂΩìÂâçÊâÄÊúâÊ®°ÂûãÁöÑÊúÄÂ§ßÁü≠Êùø„ÄÇ\n\n### ÁùÄÈáçÂèëÂ±ïÁöÑÂá†‰∏™‰∏çÂêåÊñπÂêë\n\nÂü∫‰∫éÊú¨ÊñáÁöÑÊ¢≥ÁêÜÔºåËØ•È¢ÜÂüüÊú™Êù•Â∞ÜÊ≤ø‰ª•‰∏ãÂá†‰∏™ÂÖ≥ÈîÆÊñπÂêëÊ∑±ÂåñÔºö\n\n1.  **ËØÑÊµã‰ΩìÁ≥ªÁöÑÁ≤æÁªÜÂåñ‰∏éËá™Âä®Âåñ**ÔºöÂèëÂ±ïÊõ¥Â§ö**Âü∫‰∫éÁâ©ÁêÜÂÆöÂæã**ÔºàËøêÂä®Â≠¶„ÄÅÂä®ÂäõÂ≠¶ÔºâÁöÑËá™Âä®ÂåñËØÑÊµãÊåáÊ†áÔºåÂáèÂ∞ëÂØπMLLMÁöÑ‰æùËµñÔºå‰ΩøËØÑ‰º∞Êõ¥ÂÆ¢ËßÇ„ÄÅÂèØËß£Èáä„ÄÇ\n2.  **ËßÜÈ¢ëÁîüÊàê‰∏éÊú∫Âô®‰∫∫Á≠ñÁï•ÁöÑÈó≠ÁéØ**ÔºöÊ†∏ÂøÉÊñπÂêëÊòØ**‰ªéÁîüÊàêÁöÑËßÜÈ¢ë‰∏≠ÂèçÊºîÂá∫ÂèØÊâßË°åÁöÑÂä®‰Ωú**ÔºàÈÄöËøáÈÄÜÂä®ÂäõÂ≠¶Ê®°ÂûãIDMÔºâÔºåÂπ∂Áî®‰∫é**Èó≠ÁéØÊéßÂà∂**ÔºåÁúüÊ≠£ÊâìÈÄö‚ÄúËßÜÈ¢ë‰∏ñÁïåÊ®°Âûã‚ÄùÂà∞‚ÄúÊú∫Âô®‰∫∫Ë°åÂä®‚ÄùÁöÑÈìæË∑Ø„ÄÇ\n3.  **Áâ©ÁêÜÂ¢ûÂº∫ÁöÑËßÜÈ¢ëÊ®°ÂûãËÆ≠ÁªÉ**ÔºöÂà©Áî®`RoVid-X`ËøôÁ±ªÂØåÂê´Áâ©ÁêÜÊ†áÊ≥®ÁöÑÊï∞ÊçÆÔºåËÆæËÆ°Êñ∞ÁöÑÊ®°ÂûãÊû∂ÊûÑÊàñËÆ≠ÁªÉÁõÆÊ†áÔºå**ÊòæÂºèÂú∞ËÆ©Ê®°ÂûãÂ≠¶‰π†Áâ©ÁêÜ‰∫§‰∫íËßÑÂæã**ÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØËßÜËßâÊ®°Âºè„ÄÇ\n4.  **Ë∑®ÂΩ¢ÊÄÅÊ≥õÂåñ‰∏éÁªÑÂêàÊÄß**ÔºöÊé¢Á¥¢Â¶Ç‰ΩïËÆ©ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÊõ¥Â•ΩÂú∞ÁêÜËß£Âπ∂ÁîüÊàê‰∏çÂêåÂΩ¢ÊÄÅÊú∫Âô®‰∫∫ÔºàÂèåË∂≥„ÄÅÂõõË∂≥„ÄÅÊú∫Ê¢∞ËáÇÔºâÁöÑË°å‰∏∫ÔºåÂπ∂Â§ÑÁêÜÊ∂âÂèäÂ§öÂÆû‰ΩìÂçè‰Ωú„ÄÅÈïøÈìæÊù°ÈÄªËæëÁöÑÂ§çÊùÇ‰ªªÂä°„ÄÇ\n5.  **‰ªøÁúü‰∏éÁúüÂÆû‰∏ñÁïåÁöÑÊ°•Ê¢Å**ÔºöÂà©Áî®È´òË¥®ÈáèÁîüÊàêÁöÑÊú∫Âô®‰∫∫ËßÜÈ¢ë‰Ωú‰∏∫**‰ΩéÊàêÊú¨„ÄÅÂèØÊâ©Â±ïÁöÑ‰ªøÁúüÂô®**Ôºå‰∏∫Êú∫Âô®‰∫∫Á≠ñÁï•ËÆ≠ÁªÉÊèê‰æõÊó†ÈôêÊï∞ÊçÆÔºåÂä†ÈÄüÂÖ∂Âú®ÁúüÂÆû‰∏ñÁïåÁöÑÈÉ®ÁΩ≤„ÄÇ\n\n**ÊÄªÁªì**ÔºöÊú¨ÊñáÊ†áÂøóÁùÄËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®ÂÖ∑Ë∫´Êô∫ËÉΩÈ¢ÜÂüüÁöÑÁ†îÁ©∂Ôºå‰ªé**ÂàÜÊï£ÁöÑ„ÄÅ‰æßÈáç‰∫éËßÜËßâÊàñÂ≠§Á´ãÁâ©ÁêÜÂ±ûÊÄßÁöÑÂ∞ùËØï**ÔºåËøõÂÖ•‰∫Ü**Âª∫Á´ãÁ≥ªÁªüÂåñËØÑ‰º∞Ê†áÂáÜ‰∏éÊï∞ÊçÆÂü∫Á°ÄËÆæÊñΩ**ÁöÑÊñ∞Èò∂ÊÆµ„ÄÇÂÆÉ‰∏ç‰ªÖÊòØÁé∞ÊúâÈóÆÈ¢òÁöÑ‚ÄúËØäÊñ≠‰π¶‚ÄùÔºåÊõ¥ÊòØÊú™Êù•ÂèëÂ±ïÁöÑ‚ÄúË∑ØÁ∫øÂõæ‚ÄùÔºåÈÄöËøá`RBench`Âíå`RoVid-X`ÁöÑÂçèÂêåÔºå‰∏∫ËÆ≠ÁªÉÁúüÊ≠£ÁêÜËß£Âπ∂Ê®°ÊãüÁâ©ÁêÜ‰∫§‰∫íÁöÑ‚Äú‰∏ñÁïåÊ®°Âûã‚ÄùÂ•†ÂÆö‰∫ÜÂùöÂÆûÂü∫Á°Ä„ÄÇ",
            "timestamp": "2026-01-27T16:03:04.168582",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "‰ªñÁöÑÂâç‰ΩúÊúâÂì™‰∫õÔºü‰ΩøÁî®Ë°®Ê†º‰ªîÁªÜËÆ≤ËÆ≤‰ªñÁöÑÊØèÁØáÂâç‰ΩúÔºå‰ªñÂíåÂâç‰ΩúÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºå‰∏ªË¶ÅÊîπÂñÑÊòØ‰ªÄ‰πàÔºüÁùÄÈáç‰∫éÂÖ∑‰ΩìÁõ∏ÊØî‰∫é‰πãÂâçÊñáÁ´†ÁöÑÊîπÂä®",
            "answer": "## ËÆ∫ÊñáÂâç‰ΩúÂàÜÊûê\n\nÊ†πÊçÆÊèê‰æõÁöÑËÆ∫ÊñáÂÜÖÂÆπÔºå‰ΩúËÄÖÂõ¢ÈòüÔºà‰∏ªË¶ÅÊù•Ëá™Âåó‰∫¨Â§ßÂ≠¶ÂíåÂ≠óËäÇË∑≥Âä®SeedÔºâÂú®ËßÜÈ¢ëÁîüÊàê‰∏éÂÖ∑Ë∫´Êô∫ËÉΩ‰∫§ÂèâÈ¢ÜÂüüÊúâÊåÅÁª≠ÁöÑÁ†îÁ©∂ÁßØÁ¥Ø„ÄÇ‰ª•‰∏ãÊòØÂÖ∂‰ª£Ë°®ÊÄßÂâç‰ΩúÂèäÂÖ∂‰∏éÊú¨ÁØáËÆ∫ÊñáÔºàRethinking Video Generation Model for the Embodied WorldÔºâÁöÑËØ¶ÁªÜÂØπÊØîÂàÜÊûê„ÄÇ\n\n### Ââç‰ΩúÊ¶ÇËßàË°®\n\n| ËÆ∫ÊñáÊ†áÈ¢ò | ÂèëË°®Âπ¥‰ªΩ/Áä∂ÊÄÅ | Ê†∏ÂøÉË¥°ÁåÆ | ‰∏éÊú¨ÁØáËÆ∫ÊñáÔºàRBench & RoVid-XÔºâÁöÑ‰∏ªË¶ÅÂå∫Âà´ | Êú¨ÁØáËÆ∫ÊñáÁöÑ‰∏ªË¶ÅÊîπÂñÑ‰∏éÂàõÊñ∞ÁÇπ |\n| :--- | :--- | :--- | :--- | :--- |\n| **Cinema: Coherent Multi-Subject Video Generation via MLLM-Based Guidance** (Deng et al., 2025a) | 2025 (arXiv) | ÊèêÂá∫**CinemaÊ°ÜÊû∂**ÔºåÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÂºïÂØºÔºåËß£ÂÜ≥**Â§ö‰∏ª‰ΩìËßÜÈ¢ëÁîüÊàê**‰∏≠ÁöÑ‰∏ÄËá¥ÊÄßÈóÆÈ¢ò„ÄÇ | **Á†îÁ©∂ÁÑ¶ÁÇπ‰∏çÂêå**ÔºöÂâç‰ΩúËÅöÁÑ¶‰∫é**ÈÄöÁî®ËßÜÈ¢ëÁîüÊàê**‰∏≠ÁöÑÂ§ö‰∏ª‰Ωì‰∏ÄËá¥ÊÄßÈóÆÈ¢òÔºàÂ¶Ç‰∫∫Áâ©„ÄÅÁâ©‰ΩìÔºâÔºåÊó®Âú®ÊèêÂçáËßÜËßâËøûË¥ØÊÄßÂíåÂèô‰∫ãÈÄªËæë„ÄÇ | **1. Á†îÁ©∂È¢ÜÂüüËΩ¨Âêë**Ôºö‰ªéÈÄöÁî®ËßÜÈ¢ëÁîüÊàê**ËΩ¨Âêë**ÂÖ∑Ë∫´Êú∫Âô®‰∫∫È¢ÜÂüüÁöÑ‰∏ìÁî®ËßÜÈ¢ëÁîüÊàê‰∏éËØÑ‰º∞„ÄÇ<br>**2. ÈóÆÈ¢òÊ∑±Âåñ**Ôºö‰ªéÂÖ≥Ê≥®‚ÄúËßÜËßâ‰∏ÄËá¥ÊÄß‚ÄùÊ∑±Âåñ‰∏∫ÂÖ≥Ê≥®**Áâ©ÁêÜÂêàÁêÜÊÄß„ÄÅ‰ªªÂä°ÂÆåÊàêÂ∫¶„ÄÅÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÁ®≥ÂÆöÊÄß**Á≠âÂÖ∑Ë∫´Ê†∏ÂøÉÊåëÊàò„ÄÇ<br>**3. ÊñπÊ≥ïËÆ∫ÂçáÁ∫ß**Ôºö‰ªéÂâç‰ΩúÁöÑMLLMÂºïÂØº**ÁîüÊàê**ÔºåÊâ©Â±ïÂà∞Âà©Áî®MLLMËøõË°å**Ëá™Âä®Âåñ„ÄÅÁªÜÁ≤íÂ∫¶ËØÑ‰º∞**ÔºàRBenchÔºâÔºåÂπ∂ÊûÑÂª∫Â§ßËßÑÊ®°ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºàRoVid-XÔºâ„ÄÇ |\n| **MagRef: Masked Guidance for Any-Reference Video Generation** (Deng et al., 2025b) | 2025 (arXiv) | ÊèêÂá∫**MagRefÊñπÊ≥ï**ÔºåÈÄöËøáÊé©Á†ÅÂºïÂØºÂÆûÁé∞**‰ªªÊÑèÂèÇËÄÉ**Êù°‰ª∂‰∏ãÁöÑËßÜÈ¢ëÁîüÊàêÔºåÂ¢ûÂº∫ÂØπÂèÇËÄÉÂõæÂÉèÔºàÂ¶ÇÈ£éÊ†º„ÄÅÂ∏ÉÂ±ÄÔºâÁöÑÊéßÂà∂ËÉΩÂäõ„ÄÇ | **‰ªªÂä°‰∏éÂú∫ÊôØ‰∏çÂêå**ÔºöÂâç‰ΩúËá¥Âäõ‰∫éÊèêÂçáÁîüÊàêÊ®°ÂûãÂØπ**‰ªªÊÑèÁªôÂÆöÂèÇËÄÉÂõæÂÉè**ÁöÑÈÄÇÂ∫îÂíåÂèØÊéßÊÄßÔºåÂ±û‰∫éÈÄöÁî®ÂèØÊéßÁîüÊàêËåÉÁï¥„ÄÇ | **1. Âü∫ÂáÜ‰∏éÊï∞ÊçÆÈ¶ñÂàõ**ÔºöÊú¨ÁØáÈ¶ñÊ¨°Á≥ªÁªüÊÄßÂú∞ÊèêÂá∫‰∫ÜÈù¢ÂêëÊú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÁöÑ**Âü∫ÂáÜÊµãËØïRBench**Âíå**ÊúÄÂ§ßËßÑÊ®°ÂºÄÊ∫êÊï∞ÊçÆÈõÜRoVid-X**ÔºåÂ°´Ë°•‰∫ÜÈ¢ÜÂüüÁ©∫ÁôΩ„ÄÇÂâç‰ΩúÊó†Ê≠§Ë¥°ÁåÆ„ÄÇ<br>**2. ËØÑ‰º∞Áª¥Â∫¶‰∏ì‰∏öÂåñ**ÔºöÂºïÂÖ•‰∫ÜÈíàÂØπÊú∫Âô®‰∫∫Âú∫ÊôØÁöÑ‰∏ìÂ±ûËØÑ‰º∞ÊåáÊ†áÔºàÂ¶ÇÁâ©ÁêÜËØ≠‰πâÂêàÁêÜÊÄß„ÄÅ‰ªªÂä°ÈÅµ‰ªé‰∏ÄËá¥ÊÄß„ÄÅÊú∫Âô®‰∫∫‰∏ª‰ΩìÁ®≥ÂÆöÊÄßÔºâÔºåËøúË∂ÖÂâç‰ΩúÂÖ≥Ê≥®ÁöÑÈÄöÁî®ËßÜËßâË¥®Èáè„ÄÇ<br>**3. Á≥ªÁªüÊÄßÂàÜÊûê**ÔºöÂØπ25‰∏™‰ª£Ë°®ÊÄßÊ®°ÂûãËøõË°å‰∫ÜË∑®‰ªªÂä°„ÄÅË∑®ÂÖ∑Ë∫´ÁöÑÁ≥ªÁªüÊÄßËØÑ‰º∞ÔºåÊè≠Á§∫‰∫Ü‚ÄúÂ™í‰Ωì-‰ªøÁúü‚ÄùÈ∏øÊ≤ü„ÄÅËÆ§Áü•Áì∂È¢àÁ≠âÊ∑±ÂàªÊ¥ûËßÅÔºåËøôÊòØÂâç‰ΩúÊú™Ê∂âÂèäÁöÑ„ÄÇ |\n| **OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation** (Yuan et al., 2025) | 2025 (arXiv) | ÊûÑÂª∫‰∫ÜÈù¢Âêë**‰∏ª‰ΩìÈ©±Âä®ËßÜÈ¢ëÁîüÊàê**ÁöÑÂ§ßËßÑÊ®°Âü∫ÂáÜ‰∏éÊï∞ÊçÆÈõÜÔºå‰∏ìÊ≥®‰∫éËØÑ‰º∞Ê®°ÂûãÂú®‰øùÊåÅÁâπÂÆö‰∏ª‰ΩìË∫´‰ªΩÂíåÂ±ûÊÄß‰∏ÄËá¥ÊÄßÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇ | **‰∏ª‰Ωì‰∏éÂú∫ÊôØÂ∑ÆÂºÇ**ÔºöÂâç‰ΩúÂÖ≥Ê≥®**ÈÄöÁî®‰∏ª‰Ωì**ÔºàÂ¶ÇÁâπÂÆö‰∫∫Áâ©„ÄÅÂä®Áâ©ÔºâÂú®ËßÜÈ¢ë‰∏≠ÁöÑ‰∏ÄËá¥ÊÄßÔºåÂú∫ÊôØÊòØÂºÄÊîæÂüüÁöÑ„ÄÇ | **1. È¢ÜÂüüÁâπÂºÇÊÄß**ÔºöÊú¨ÁØáÂÆåÂÖ®‰∏ìÊ≥®‰∫é**Êú∫Âô®‰∫∫‰∏ª‰Ωì**ÂèäÂÖ∂Âú®**Áâ©ÁêÜ‰∫§‰∫íÂú∫ÊôØ**‰∏≠ÁöÑÁîüÊàê‰∏éËØÑ‰º∞ÔºåÂØπÁâ©ÁêÜËßÑÂæãÂíå‰ªªÂä°ÈÄªËæëÁöÑË¶ÅÊ±ÇËøúÈ´ò‰∫éÈÄöÁî®‰∏ª‰ΩìÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ<br>**2. ËØÑ‰º∞Â§çÊùÇÊÄß**ÔºöRBenchÁöÑËØÑ‰º∞‰∏ç‰ªÖË¶ÅÊ±Ç‰∏ª‰ΩìÔºàÊú∫Âô®‰∫∫ÔºâÂ§ñËßÇÁ®≥ÂÆöÔºåÊõ¥Ë¶ÅÊ±ÇÂÖ∂**ËøêÂä®Â≠¶ÁªìÊûÑÁ®≥ÂÆö**ÔºàÂ¶ÇÂÖ≥ËäÇÊï∞‰∏çÂèò„ÄÅËøûÊùÜÈïøÂ∫¶ÊÅíÂÆöÔºâÔºåÂπ∂ËØÑ‰º∞ÂÖ∂**Âä®‰ΩúÁöÑÁâ©ÁêÜÂèØË°åÊÄßÂíå‰ªªÂä°ÁõÆÊ†áÂÆåÊàêÂ∫¶**ÔºåËØÑ‰º∞Áª¥Â∫¶Êõ¥Â§çÊùÇ„ÄÅÊõ¥Ë¥¥ËøëÁúüÂÆûÊú∫Âô®‰∫∫Â∫îÁî®ÈúÄÊ±Ç„ÄÇ<br>**3. Êï∞ÊçÆËßÑÊ®°‰∏éÊ†áÊ≥®**ÔºöRoVid-XÔºà400‰∏áËßÜÈ¢ëÁâáÊÆµÔºâÂú®Êú∫Âô®‰∫∫ËßÜÈ¢ëÈ¢ÜÂüüËßÑÊ®°ÊúÄÂ§ßÔºå‰∏îÊèê‰æõ‰∫Ü**Áâ©ÁêÜÂ±ûÊÄßÊ†áÊ≥®**ÔºàÂ¶ÇÂÖâÊµÅ„ÄÅÊ∑±Â∫¶ÂõæÔºâÔºåÁõ¥Êé•ÊúçÂä°‰∫éÊèêÂçáÊ®°ÂûãÁöÑÁâ©ÁêÜ‰∏ñÁïåÁêÜËß£ÔºåËÄåÂâç‰ΩúÊï∞ÊçÆÈõÜÊó†Ê≠§Á±ªÊ†áÊ≥®„ÄÇ |\n\n### Ê†∏ÂøÉÂå∫Âà´‰∏é‰∏ªË¶ÅÊîπÂñÑÊÄªÁªì\n\n1.  **Á†îÁ©∂ËåÉÂºèÁöÑÊ†πÊú¨ËΩ¨Âèò**Ôºö\n    *   **Ââç‰Ωú**ÔºöÂ±û‰∫é**ÈÄöÁî®ËßÜÈ¢ëÁîüÊàê**ËåÉÁï¥ÁöÑÊîπËøõÁ†îÁ©∂ÔºåÊó®Âú®ÊèêÂçáÁîüÊàêËßÜÈ¢ëÁöÑËßÜËßâË¥®Èáè„ÄÅÂ§ö‰∏ª‰Ωì‰∏ÄËá¥ÊÄßÊàñÂèÇËÄÉÊéßÂà∂ËÉΩÂäõ„ÄÇ\n    *   **Êú¨ÁØá**ÔºöÂÆûÁé∞‰∫ÜÂêë**ÂÖ∑Ë∫´Êô∫ËÉΩÔºàEmbodied AIÔºâ** È¢ÜÂüüÁöÑÊàòÁï•ËÅöÁÑ¶„ÄÇÊ†∏ÂøÉÁõÆÊ†áÊòØÊé®Âä®ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÊàê‰∏∫ÂêàÊ†ºÁöÑ‚Äú**‰∏ñÁïåÊ®°ÊãüÂô®**‚ÄùÔºåËÉΩÂ§üÁîüÊàê**Áâ©ÁêÜÂêàÁêÜ„ÄÅ‰ªªÂä°ÂèØË°å**ÁöÑÊú∫Âô®‰∫∫Ë°å‰∏∫ËßÜÈ¢ëÔºå‰ª•ÊúçÂä°‰∫éÊú∫Âô®‰∫∫Â≠¶‰π†„ÄÅ‰ªøÁúüÁ≠â‰∏ãÊ∏∏Â∫îÁî®„ÄÇ\n\n2.  **‰ªé‚ÄúÁîüÊàêÊîπËøõ‚ÄùÂà∞‚ÄúËØÑ‰º∞‰∏éÊï∞ÊçÆÂü∫Âª∫‚Äù**Ôºö\n    *   **Ââç‰Ωú**Ôºö‰∏ªË¶ÅË¥°ÁåÆÂú®‰∫éÊèêÂá∫Êñ∞ÁöÑ**ÁîüÊàêÊ®°ÂûãÊàñÁîüÊàêÂºïÂØºÊñπÊ≥ï**ÔºàÂ¶ÇCinema, MagRefÔºâ„ÄÇ\n    *   **Êú¨ÁØá**Ôºö‰∏ªË¶ÅË¥°ÁåÆÂú®‰∫éÊûÑÂª∫È¢ÜÂüü**Âü∫Á°ÄËÆæÊñΩ**Ôºö\n        *   **RBenchÔºàËØÑ‰º∞Âü∫ÂáÜÔºâ**ÔºöÈ¶ñ‰∏™ÈíàÂØπÊú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÁöÑÁªºÂêàÊÄß„ÄÅÁªÜÁ≤íÂ∫¶ËØÑ‰º∞Âü∫ÂáÜ„ÄÇÂÆÉ**Ë∂ÖË∂ä‰∫Ü‰º†ÁªüÊÑüÁü•ÊåáÊ†á**ÔºåÈÄöËøáÂ§öÁª¥Â∫¶ÊåáÊ†áÔºàÁâ©ÁêÜËØ≠‰πâ„ÄÅ‰ªªÂä°ÈÅµ‰ªé„ÄÅËøêÂä®Âπ≥ÊªëÂ∫¶Á≠âÔºâÁ≥ªÁªüÊÄßÂú∞Ë°°ÈáèÊ®°ÂûãÁöÑ‚ÄúÁâ©ÁêÜÊô∫ËÉΩ‚ÄùÊ∞¥Âπ≥„ÄÇ\n        *   **RoVid-XÔºàËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºâ**ÔºöÈÄöËøáÂõõÈò∂ÊÆµÊµÅÊ∞¥Á∫øÊûÑÂª∫ÁöÑ„ÄÅËøÑ‰ªäÊúÄÂ§ßËßÑÊ®°ÁöÑÊú∫Âô®‰∫∫ËßÜÈ¢ëÊï∞ÊçÆÈõÜ„ÄÇÂÆÉ‰∏ç‰ªÖËßÑÊ®°Â§ßÔºåËÄå‰∏î**Ë¥®ÈáèÈ´ò„ÄÅÂ§öÊ†∑ÊÄßÂ•Ω**ÔºàË¶ÜÁõñ5Â§ß‰ªªÂä°„ÄÅ4Á±ªÊú∫Âô®‰∫∫„ÄÅ1300+ÊäÄËÉΩÔºâÔºåÂπ∂ÂåÖÂê´**Áâ©ÁêÜÂ±ûÊÄßÂ¢ûÂº∫Ê†áÊ≥®**ÔºåÊó®Âú®‰ªéÊ†πÊú¨‰∏äËß£ÂÜ≥ËÆ≠ÁªÉÊï∞ÊçÆÁ®ÄÁº∫ÂíåË¥®ÈáèÈóÆÈ¢ò„ÄÇ\n\n3.  **ÊäÄÊúØË¥°ÁåÆÁöÑÊ∑±Â∫¶‰∏éÂπøÂ∫¶**Ôºö\n    *   **Ââç‰Ωú**ÔºöÊäÄÊúØË¥°ÁåÆÁõ∏ÂØπ**ËÅöÁÑ¶**ÔºåËß£ÂÜ≥ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊüê‰∏™ÁâπÂÆöÈóÆÈ¢òÔºà‰∏ÄËá¥ÊÄß„ÄÅÂèØÊéßÊÄßÔºâ„ÄÇ\n    *   **Êú¨ÁØá**ÔºöÊäÄÊúØË¥°ÁåÆÊòØ**Á≥ªÁªüÊÄßÂíåÁîüÊÄÅÂåñ**ÁöÑ„ÄÇÂÆÉÂåÖÂê´‰∫ÜÔºö\n        *   **‰∏•Ë∞®ÁöÑËØÑ‰º∞‰ΩìÁ≥ª**ÔºöËÆæËÆ°‰∫ÜÂèØÂ§çÁé∞ÁöÑËá™Âä®ÂåñËØÑ‰º∞ÊµÅÁ®ãÔºåÂπ∂‰∏é‰∫∫Á±ªËØÑ‰º∞È´òÂ∫¶Áõ∏ÂÖ≥ÔºàSpearmanÁ≥ªÊï∞0.96Ôºâ„ÄÇ\n        *   **Ê∑±Â∫¶ÁöÑÊ®°ÂûãÂàÜÊûê**ÔºöÂØπ25‰∏™Ê®°ÂûãÁöÑÂ§ßËßÑÊ®°ËØÑ‰º∞Êè≠Á§∫‰∫ÜÂΩìÂâçSOTAÊ®°ÂûãÔºàÂåÖÊã¨SoraÁ≥ªÂàóÔºâÂú®Áâ©ÁêÜÊú∫Âô®‰∫∫‰ªªÂä°‰∏äÁöÑ‰∏•Èáç‰∏çË∂≥Ôºå‰ª•ÂèäÈó≠Ê∫ê‰∏éÂºÄÊ∫êÊ®°ÂûãÈó¥ÁöÑÊòæËëóÂ∑ÆË∑ùÁ≠âÈáçË¶ÅÂèëÁé∞„ÄÇ\n        *   **Â§ßËßÑÊ®°Êï∞ÊçÆÂ∑•Á®ã**ÔºöÂ±ïÁ§∫‰∫Ü‰ªéÊï∞ÊçÆÊî∂ÈõÜ„ÄÅËøáÊª§„ÄÅÊ†áÊ≥®Âà∞Â¢ûÂº∫ÁöÑÂÆåÊï¥Êï∞ÊçÆÊµÅÊ∞¥Á∫ø„ÄÇ\n        *   **ÊúâÊïàÊÄßÈ™åËØÅ**ÔºöÈÄöËøáÂæÆË∞ÉÂÆûÈ™åËØÅÊòé‰∫ÜRoVid-XÊï∞ÊçÆÈõÜËÉΩÁ®≥ÂÆöÊèêÂçáÊ®°ÂûãÂú®ÂêÑÈ°πÊú∫Âô®‰∫∫‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇ\n\n**ÁªìËÆ∫**ÔºöÊú¨ÁØáËÆ∫ÊñáÊ†áÂøóÁùÄ‰ΩúËÄÖÂõ¢ÈòüÁöÑÁ†îÁ©∂‰∏ªÁ∫ø‰ªé‚ÄúÊèêÂçáÈÄöÁî®ËßÜÈ¢ëÁîüÊàêË¥®Èáè‚ÄùËΩ¨Âêë‰∫Ü‚Äú‰∏∫ÂÖ∑Ë∫´Êô∫ËÉΩÊûÑÂª∫ËßÜÈ¢ë‰∏ñÁïåÊ®°ÂûãÁöÑÂü∫Á°ÄËÆæÊñΩ‚Äù„ÄÇÂÖ∂Ê†∏ÂøÉÊîπÂñÑÂú®‰∫é**ËØÜÂà´Âπ∂ÁùÄÊâãËß£ÂÜ≥ÂÖ∑Ë∫´ËßÜÈ¢ëÁîüÊàêÈ¢ÜÂüüÁöÑÊ†πÊú¨ÊÄßÁì∂È¢à‚Äî‚ÄîÁº∫‰πèÂèØÈù†ÁöÑËØÑ‰º∞Ê†áÂáÜÂíåÈ´òË¥®ÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆ**„ÄÇÈÄöËøáÂàõÂª∫RBenchÂíåRoVid-XËøô‰∏Ä‚ÄúËØÑ‰º∞-Êï∞ÊçÆ‚ÄùÂçèÂêåÁîüÊÄÅÁ≥ªÁªüÔºåÊú¨Êñá‰∏∫ËØ•È¢ÜÂüüÁöÑÂÆ¢ËßÇÊØîËæÉ„ÄÅÊ®°ÂûãËø≠‰ª£ÂíåÊú™Êù•ÂèëÂ±ïÂ•†ÂÆö‰∫ÜÂùöÂÆûÂü∫Á°ÄÔºåÂÖ∂Ë¥°ÁåÆÊõ¥ÂÖ∑Âü∫Á°ÄÊÄßÂíåÈïøËøúÂΩ±ÂìçÂäõ„ÄÇ",
            "timestamp": "2026-01-27T16:03:59.920022",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ËÆ∫ÊñáÊèêÂá∫‰∫ÜÂì™‰∫õÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïÔºåËØ∑ÂàóË°®Ê†ºÂÖ∑‰ΩìËØ¶ÁªÜËØ¥ÊòéÊäÄÊúØÁªÜËäÇÔºåÈúÄË¶ÅÂåÖÂê´ÂÖ∑‰ΩìÁöÑÊï∞Â≠¶ÂéüÁêÜÊé®ÂØºÔºå‰ª•ÂèäÂÖ∑‰ΩìÂèÇÊï∞„ÄÇ",
            "answer": "Â•ΩÁöÑÔºå‰Ωú‰∏∫‰∏ÄÂêç‰∏ì‰∏öÁöÑÂ≠¶ÊúØËÆ∫ÊñáÂàÜÊûêÂä©ÊâãÔºåÊàëÂ∞Ü‰∏∫ÊÇ®Ê∑±ÂÖ•ÂàÜÊûêËøôÁØáÈ¢ò‰∏∫„ÄäRethinking Video Generation Model for the Embodied World„ÄãÁöÑËÆ∫ÊñáÔºåÂπ∂ËØ¶ÁªÜÈòêËø∞ÂÖ∂ÊèêÂá∫ÁöÑÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ï„ÄÇ\n\n## ËÆ∫ÊñáÊ†∏ÂøÉÊäÄÊúØÂàõÊñ∞‰∏é‰ª∑ÂÄºÂàÜÊûê\n\nÊú¨ÊñáÁöÑÊ†∏ÂøÉË¥°ÁåÆÂú®‰∫é**Á≥ªÁªüÊÄßÂú∞Ëß£ÂÜ≥‰∫ÜÈù¢ÂêëÂÖ∑Ë∫´Êô∫ËÉΩÔºàEmbodied AIÔºâÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®ËØÑ‰º∞‰∏éËÆ≠ÁªÉÊï∞ÊçÆÊñπÈù¢ÁöÑ‰∏§Â§ßÁì∂È¢à**„ÄÇÂÖ∂ÊäÄÊúØÂàõÊñ∞Âπ∂ÈùûÊèêÂá∫‰∏Ä‰∏™ÂÖ®Êñ∞ÁöÑÁîüÊàêÊ®°ÂûãÊû∂ÊûÑÔºåËÄåÊòØÊûÑÂª∫‰∫Ü‰∏Ä‰∏™**ËØÑ‰º∞-Êï∞ÊçÆÂçèÂêåÁîüÊÄÅÁ≥ªÁªü**ÔºåÊó®Âú®ÂºïÂØºÂíåÂä†ÈÄüËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂêë‚ÄúÁâ©ÁêÜAI‚ÄùÊàñ‚Äú‰∏ñÁïåÊ®°ÊãüÂô®‚ÄùÁöÑËåÉÂºèÊºîËøõ„ÄÇ\n\n**ÂÆûÈôÖ‰ª∑ÂÄº**Ôºö\n1.  **Ê†áÂáÜÂåñËØÑ‰º∞**Ôºö‰∏∫Êú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÈ¢ÜÂüüÊèê‰æõ‰∫ÜÈ¶ñ‰∏™ÂÖ®Èù¢„ÄÅÂèØÂ§çÁé∞ÁöÑÂü∫ÂáÜÊµãËØïÔºàRBenchÔºâÔºåÊîπÂèò‰∫Ü‰ª•ÂæÄ‰æùËµñ‰∏ªËßÇÊÑüÁü•ÊåáÊ†á„ÄÅÂøΩËßÜÁâ©ÁêÜÈÄªËæëÂíå‰ªªÂä°ÂÆåÊàêÂ∫¶ÁöÑËØÑ‰º∞Áé∞Áä∂„ÄÇ\n2.  **Êï∞ÊçÆÈ©±Âä®ËøõÊ≠•**ÔºöÊûÑÂª∫‰∫ÜÁõÆÂâçÊúÄÂ§ßËßÑÊ®°„ÄÅÈ´òË¥®Èáè„ÄÅÂ§öÂΩ¢ÊÄÅÁöÑÊú∫Âô®‰∫∫ËßÜÈ¢ëÊï∞ÊçÆÈõÜÔºàRoVid-XÔºâÔºå‰∏∫ËÆ≠ÁªÉÂÖ∑Â§áÁâ©ÁêÜÂ∏∏ËØÜÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÊèê‰æõ‰∫ÜÂÖ≥ÈîÆÁáÉÊñô„ÄÇ\n3.  **Êè≠Á§∫ÊÄßËÉΩÂ∑ÆË∑ù**ÔºöÈÄöËøáÂØπ25‰∏™‰ª£Ë°®ÊÄßÊ®°ÂûãÁöÑÁ≥ªÁªüËØÑ‰º∞ÔºåÊ∏ÖÊô∞Êè≠Á§∫‰∫ÜÂΩìÂâçÈÄöÁî®ËßÜÈ¢ëÂ§ßÊ®°ÂûãÂú®ÁîüÊàêÁ¨¶ÂêàÁâ©ÁêÜËßÑÂæãÁöÑÊú∫Âô®‰∫∫Ë°å‰∏∫ÊñπÈù¢Â≠òÂú®ÊòæËëóÁº∫Èô∑ÔºåÂ∞§ÂÖ∂ÊòØÂïÜ‰∏öÊ®°Âûã‰∏éÂºÄÊ∫êÊ®°Âûã„ÄÅÂ™í‰ΩìÊ∂àË¥πÊ®°Âûã‰∏éÁâ©ÁêÜ‰ªøÁúüÊ®°Âûã‰πãÈó¥ÁöÑ‚ÄúÈ¢ÜÂüüÈ∏øÊ≤ü‚Äù„ÄÇ\n\n---\n\n## ÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïËØ¶Ëø∞\n\nËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ï‰∏ªË¶ÅÂõ¥Áªï**Âü∫ÂáÜÊµãËØïÊûÑÂª∫ÔºàRBenchÔºâ** Âíå**Êï∞ÊçÆÈõÜÊûÑÂª∫ÔºàRoVid-XÔºâ** ‰∏§Êù°‰∏ªÁ∫øÂ±ïÂºÄ„ÄÇ‰ª•‰∏ãÊòØÂÖ∑‰ΩìÊäÄÊúØÁªÜËäÇÁöÑË°®Ê†ºÂåñËØ¥ÊòéÔºö\n\n### 1. Êú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÂü∫ÂáÜÊµãËØï RBench\n\n#### 1.1 Âü∫ÂáÜÊûÑÂª∫ÊñπÊ≥ïËÆ∫\n\n| ÊñπÊ≥ïÁªÑÊàêÈÉ®ÂàÜ | ÊäÄÊúØÁªÜËäÇ‰∏éÁõÆÁöÑ | ÂÖ∑‰ΩìÂèÇÊï∞/ËÆæËÆ° |\n| :--- | :--- | :--- |\n| **ÂèåÈáçËØÑ‰º∞Áª¥Â∫¶** | ‰ªé**‰ªªÂä°ÂÆåÊàêÂ∫¶**Âíå**ËßÜËßâË¥®Èáè**‰∏§‰∏™Ê†∏ÂøÉÁª¥Â∫¶ËØÑ‰º∞ÁîüÊàêËßÜÈ¢ëÔºåË∂ÖË∂ä‰º†Áªü‰ªÖÂÖ≥Ê≥®ÂÉèÁ¥†‰øùÁúüÂ∫¶ÁöÑÂÅöÊ≥ï„ÄÇ | - **‰ªªÂä°ÂÆåÊàêÂ∫¶**ÔºöÂÖ≥Ê≥®Áâ©ÁêÜËØ≠‰πâÂêàÁêÜÊÄßÂíå‰ªªÂä°‰æù‰ªé‰∏ÄËá¥ÊÄß„ÄÇ<br>- **ËßÜËßâË¥®Èáè**ÔºöÂÖ≥Ê≥®ËøêÂä®ÂπÖÂ∫¶„ÄÅÊú∫Âô®‰∫∫‰∏ª‰ΩìÁ®≥ÂÆöÊÄßÂíåËøêÂä®Âπ≥ÊªëÂ∫¶„ÄÇ |\n| **‰ªªÂä°ÂØºÂêëËØÑ‰º∞ÈõÜ** | Ê∂µÁõñÊú∫Âô®‰∫∫Â∏∏ËßÅÁöÑ‰∫îÁ±ª‰ªªÂä°ÔºåÁ°Æ‰øùËØÑ‰º∞ÁöÑÂÖ®Èù¢ÊÄß„ÄÇ | ÂÖ±250‰∏™ÂõæÂÉè-ÊñáÊú¨ÂØπÔºàÊØèÁ±ª‰ªªÂä°50‰∏™ÔºâÔºö<br>1. **Â∏∏ËßÅÊìç‰Ωú**ÔºöÊäìÂèñ„ÄÅÊîæÁΩÆÁ≠âÂü∫Á°ÄÂä®‰Ωú„ÄÇ<br>2. **ÈïøÁ®ãËßÑÂàí**ÔºöÂ§öÊ≠•È™§È°∫Â∫è‰ªªÂä°ÔºàÂ¶ÇÂºÄÈó®->ÂèñÁâ©->ÂÖ≥Èó®Ôºâ„ÄÇ<br>3. **Â§öÂÆû‰ΩìÂçè‰Ωú**ÔºöÊú∫Âô®‰∫∫‰∏é‰∫∫Á±ª„ÄÅÂÖ∂‰ªñÊú∫Âô®‰∫∫ÁöÑ‰∫§‰∫í„ÄÇ<br>4. **Á©∫Èó¥ÂÖ≥Á≥ª**ÔºöÁêÜËß£Âπ∂ÊâßË°å‚ÄúÂ∑¶/Âè≥‚Äù„ÄÅ‚Äú‰∏ä/‰∏ã‚ÄùÁ≠âÁ©∫Èó¥Êåá‰ª§„ÄÇ<br>5. **ËßÜËßâÊé®ÁêÜ**ÔºöÊ∂âÂèäÈ¢úËâ≤„ÄÅÊï∞Èáè„ÄÅÂ±ûÊÄßÁ≠âÈ´òÁ∫ßËØ≠‰πâÁêÜËß£ÁöÑ‰ªªÂä°„ÄÇ |\n| **ÂÖ∑Ë∫´ÂΩ¢ÊÄÅËØÑ‰º∞ÈõÜ** | Ë¶ÜÁõñÂõõÁßç‰∏ªÊµÅÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÔºåËØÑ‰º∞Ê®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ | ÂÖ±400‰∏™ÂõæÂÉè-ÊñáÊú¨ÂØπÔºàÊØèÁßçÂΩ¢ÊÄÅ100‰∏™ÔºâÔºö<br>- ÂçïËáÇÊú∫Âô®‰∫∫„ÄÅÂèåËáÇÊú∫Âô®‰∫∫„ÄÅ‰∫∫ÂΩ¢Êú∫Âô®‰∫∫„ÄÅÂõõË∂≥Êú∫Âô®‰∫∫„ÄÇ |\n| **Ëá™Âä®ÂåñËØÑ‰º∞ÊåáÊ†á** | Âà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫Èõ∂Ê†∑Êú¨ËØÑ‰º∞Âô®ÔºåÁªìÂêà‰º†ÁªüËÆ°ÁÆóÊú∫ËßÜËßâÁÆóÊ≥ïÔºåËÆæËÆ°ÂèØÂ§çÁé∞ÁöÑÈáèÂåñÊåáÊ†á„ÄÇ | ‰ΩøÁî® **Qwen3-VL** Âíå **GPT-5** ‰Ωú‰∏∫MLLMËØÑ‰º∞Âô®„ÄÇËØÑ‰º∞Âü∫‰∫é‰ªéÁîüÊàêËßÜÈ¢ë‰∏≠ÂùáÂåÄÈááÊ†∑ÁöÑÂÖ≥ÈîÆÂ∏ßÁΩëÊ†ºÔºàÂ¶Ç3x2Ôºâ„ÄÇ |\n\n#### 1.2 Ê†∏ÂøÉËØÑ‰º∞ÊåáÊ†áÁöÑÊï∞Â≠¶ÂéüÁêÜ‰∏éÊé®ÂØº\n\n‰ª•‰∏ã‰∫î‰∏™ÁªÜÁ≤íÂ∫¶ÊåáÊ†áÈÄöËøáÂä†ÊùÉËÅöÂêàÔºåÊúÄÁªàÂæóÂà∞‚Äú‰ªªÂä°ÂÆåÊàê‚ÄùÂíå‚ÄúËßÜËßâË¥®Èáè‚Äù‰∏§‰∏™ÊÄªÂàÜ„ÄÇ\n\n| ÊåáÊ†áÂêçÁß∞ | ËØÑ‰º∞ÁõÆÊ†á | Êï∞Â≠¶ÂéüÁêÜ/ËÆ°ÁÆóÁªÜËäÇ | ÂÖ≥ÈîÆÂèÇÊï∞ |\n| :--- | :--- | :--- | :--- |\n| **Áâ©ÁêÜËØ≠‰πâÂêàÁêÜÊÄß** | Ê£ÄÊµãËøùÂèçÁâ©ÁêÜÂ∏∏ËØÜÂíåËØ≠‰πâÈÄªËæëÁöÑÂºÇÂ∏∏ÔºàÂ¶ÇÊºÇÊµÆ„ÄÅÁ©øÈÄè„ÄÅÁ™ÅÁÑ∂Âá∫Áé∞Ôºâ„ÄÇ | **MLLM VQAÂçèËÆÆ**ÔºöÂêëMLLMËæìÂÖ•Â∏ßÁΩëÊ†ºÂíåÁ≤æÂøÉËÆæËÆ°ÁöÑÊèêÁ§∫ËØçÔºåËÆ©ÂÖ∂Âà§Êñ≠ÊòØÂê¶Â≠òÂú®ÁâπÂÆöÁ±ªÂûãÁöÑËøùËßÑ„ÄÇÊó†ÊòæÂºèÊï∞Â≠¶ÂÖ¨ÂºèÔºå‰æùËµñ‰∫éMLLMÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ | ÊèêÁ§∫ËØçÂ∑•Á®ãÊòØÂÖ≥ÈîÆÔºåÊòéÁ°ÆÂàó‰∏æÂ§±Ë¥•Ê®°ÂºèÔºö`Floating/Penetration`, `Spontaneous emergence`, `Non-contact attachment`„ÄÇ |\n| **‰ªªÂä°‰æù‰ªé‰∏ÄËá¥ÊÄß** | ËØÑ‰º∞ËßÜÈ¢ëÊòØÂê¶ÂÆåÊï¥„ÄÅÊåâÈ°∫Â∫èÊâßË°å‰∫ÜÊèêÁ§∫ËØçË¶ÅÊ±ÇÁöÑÂä®‰Ωú„ÄÇ | **MLLM VQAÊ∏ÖÂçï**ÔºöÈíàÂØπ‰∏çÂêå‰ªªÂä°Á±ªÂûãËÆæËÆ°Ê£ÄÊü•Ê∏ÖÂçï„ÄÇ‰æãÂ¶ÇÔºåÂØπÈïøÁ®ãËßÑÂàí‰ªªÂä°ÔºåËÆ°ÁÆó**‰∫ã‰ª∂ÂÆåÊàêÁéá**Ôºö<br>`Score = 5 * (completed_events / total_events)` | ÂØπ‰∫éËßÜËßâÊé®ÁêÜ‰ªªÂä°ÔºåÈááÁî®**ÈóÆÈ¢òÈìæ**Êú∫Âà∂ÔºöMLLMÂÖàÊ†πÊçÆÊåá‰ª§ÁîüÊàê‰∏ÄÁ≥ªÂàóÈ™åËØÅÈóÆÈ¢òÔºåÂÜçÊ†πÊçÆËßÜÈ¢ëÂõûÁ≠îÔºåËÆ°ÁÆóÊ≠£Á°ÆÁéá„ÄÇ |\n| **ËøêÂä®ÂπÖÂ∫¶** | ÈáèÂåñÊú∫Âô®‰∫∫‰∏ª‰ΩìÁöÑÁúüÂÆûËøêÂä®ÔºåÊéíÈô§Áõ∏Êú∫ËøêÂä®Âπ≤Êâ∞ÔºåÊÉ©ÁΩö‚ÄúÂπ≥Êªë‰ΩÜÈùôÊ≠¢‚ÄùÁöÑËßÜÈ¢ë„ÄÇ | 1. **‰∏ª‰ΩìÂÆö‰Ωç‰∏éË∑üË∏™**Ôºö‰ΩøÁî®GroundingDINOÊ£ÄÊµãÔºåGroundedSAMÁîüÊàêÊé©Á†ÅÔºåCoTrackerË∑üË∏™Êé©Á†ÅÂÜÖÂØÜÈõÜÁÇπ„ÄÇ<br>2. **ËøêÂä®ËÆ°ÁÆó**ÔºöÁ¨¨tÂ∏ßÂΩí‰∏ÄÂåñÂêéÁöÑÂπ≥ÂùáÁÇπ‰ΩçÁßª `DÃÉ_t`„ÄÇ<br>3. **Áõ∏Êú∫ËøêÂä®Ë°•ÂÅø**ÔºöËÆ°ÁÆóËÉåÊôØÂå∫ÂüüËøêÂä® `DÃÉ_t^bg`ÔºåË°•ÂÅøÂêé‰ΩçÁßªÔºö<br>   `DÃÇ_t = { DÃÉ_t - DÃÉ_t^bg, if DÃÉ_t > DÃÉ_t^bg; DÃÉ_t, otherwise }`<br>4. **ÂæóÂàÜËÆ°ÁÆó**Ôºö<br>   `MAS = (1/T) * Œ£_{t=1}^T min(DÃÇ_t, 1)` | `T`ÔºöÊÄªÂ∏ßÊï∞„ÄÇ`min(¬∑, 1)`Áî®‰∫éË£ÅÂâ™ÊûÅÂÄºÔºåÁ®≥ÂÆöÂæóÂàÜ„ÄÇ |\n| **Êú∫Âô®‰∫∫‰∏ª‰ΩìÁ®≥ÂÆöÊÄß** | ËØÑ‰º∞Êú∫Âô®‰∫∫ÂíåÁõÆÊ†áÁâ©‰ΩìÁöÑÂΩ¢ÊÄÅ„ÄÅÂ§ñËßÇÂú®Êó∂Èó¥‰∏äÁöÑÁ®≥ÂÆöÊÄß„ÄÇ | **ÂØπÊØîÂºèMLLM VQA**ÔºöËæìÂÖ•ÂèÇËÄÉÂ∏ßÔºàÈ¶ñÂ∏ßÔºâÂíåÁîüÊàêÂ∏ßÔºåË¶ÅÊ±ÇMLLMÊØîËæÉÊåáÂÆöÂÆû‰ΩìÔºàÂ¶ÇÊú∫Ê¢∞Áà™ÔºâÁöÑÂ§ñËßÇ„ÄÅÁªìÊûÑ‰∏ÄËá¥ÊÄßÔºåÂπ∂ÁªôÂá∫ËØÑÂàÜ„ÄÇ | ÂÖ≥Ê≥®**Êú∫Âô®‰∫∫ÁªìÊûÑÁ®≥ÂÆöÊÄß**ÔºàÂ¶ÇÂÖ≥ËäÇÊï∞Á™ÅÂèò„ÄÅÂΩ¢Áä∂Áï∏ÂèòÔºâÂíå**‰∏ª‰ΩìÂ§ñËßÇÁ®≥ÂÆöÊÄß**ÔºàÂ¶ÇÁâ©‰ΩìÈ¢úËâ≤„ÄÅÁ±ªÂà´ÊºÇÁßªÔºâ„ÄÇ |\n| **ËøêÂä®Âπ≥ÊªëÂ∫¶** | Ê£ÄÊµãÊó∂Èó¥‰∏äÁöÑ‰∏çËøûÁª≠„ÄÅÊäñÂä®ÂíåÊ®°Á≥äÁ≠â‰º™ÂΩ±„ÄÇ | 1. **Ë¥®ÈáèÊ≥¢Âä®Ê£ÄÊµã**Ôºö‰ΩøÁî®Q-AlignÊ®°ÂûãËé∑ÂèñÊØèÂ∏ßÁæéÂ≠¶Ë¥®ÈáèÂàÜÊï∞ `Q(f_t)`ÔºåËÆ°ÁÆóÁõ∏ÈÇªÂ∏ßÂ∑ÆÂºÇÔºö<br>   `ŒîQ_t = Q(f_{t-1}) - Q(f_t)`<br>2. **Ëá™ÈÄÇÂ∫îÈòàÂÄº**ÔºöÊ†πÊçÆ**ËøêÂä®ÂπÖÂ∫¶(MAS)** `m` ËÆæÂÆöÈòàÂÄº `œÑ_s(m)`ÔºåÂÖÅËÆ∏È´òËøêÂä®ËßÜÈ¢ëÊúâÊõ¥Â§ßÊ≥¢Âä®„ÄÇ<br>3. **ÂæóÂàÜËÆ°ÁÆó**ÔºöÊ†áËÆ∞ÂºÇÂ∏∏Â∏ß `I_t = ùïÄ(ŒîQ_t > œÑ_s(m))`ÔºåÂàôÔºö<br>   `MSS = 1 - (1/T) * Œ£_{t=2}^T I_t` | Ëá™ÈÄÇÂ∫îÈòàÂÄºÂáΩÊï∞ÔºàÂàÜÊÆµÁ∫øÊÄßÔºâÔºö<br>`œÑ_s(m) = {0.01 if m<0.1; 0.015 if 0.1‚â§m<0.3; 0.025 if 0.3‚â§m<0.5; 0.03 if m‚â•0.5}` |\n\n#### 1.3 ÂàÜÊï∞ËÅöÂêàÊñπÊ≥ï\nÊúÄÁªàÂæóÂàÜÁî±‰∏äËø∞ÁªÜÁ≤íÂ∫¶ÊåáÊ†áÂΩí‰∏ÄÂåñÂêéÔºåÁªèËøáÊÉ©ÁΩöÈ°πË∞ÉÊï¥ÔºåÂä†ÊùÉËÅöÂêàËÄåÊàê„ÄÇ\n- **‰ªªÂä°ÂÆåÊàêÂàÜ**Ôºö`TC = (PSS + TAC) / 2`\n- **ËßÜËßâË¥®ÈáèÂàÜ**Ôºö`VQ = max(0, 0.8*RSS + 0.2*MS - P_MA(MA) - P_RSS)`\n    - `P_MA(MA)` ÊòØËøêÂä®ÂπÖÂ∫¶Ëøá‰ΩéÊÉ©ÁΩö„ÄÇ\n    - `P_RSS` ÊòØ‰∏ª‰ΩìÁ®≥ÂÆöÊÄßÂ∑ÆÁöÑÊÉ©ÁΩö„ÄÇ\n\n### 2. Â§ßËßÑÊ®°Êú∫Âô®‰∫∫ËßÜÈ¢ëÊï∞ÊçÆÈõÜ RoVid-X\n\n#### 2.1 ÂõõÈò∂ÊÆµÊï∞ÊçÆÊûÑÂª∫ÊµÅÊ∞¥Á∫ø\n\n| Èò∂ÊÆµ | ÊäÄÊúØÁªÜËäÇ‰∏éÁõÆÁöÑ | ‰ΩøÁî®ÁöÑÂ∑•ÂÖ∑/Ê®°Âûã | ‰∫ßÂá∫‰∏éËßÑÊ®° |\n| :--- | :--- | :--- | :--- |\n| **1. Êú∫Âô®‰∫∫ËßÜÈ¢ëÊî∂ÈõÜ** | ‰ªé‰∫íËÅîÁΩëËßÜÈ¢ëÂπ≥Âè∞Âíå20+‰∏™ÂºÄÊ∫êÊï∞ÊçÆÈõÜ‰∏≠Êî∂ÈõÜÂéüÂßãËßÜÈ¢ë„ÄÇ | **GPT-5**ÔºöÁî®‰∫éËá™Âä®ËøáÊª§ÂÜÖÂÆπÔºå‰øùÁïô‰∏éÊú∫Âô®‰∫∫‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËßÜÈ¢ë„ÄÇ | Á∫¶300‰∏á‰∏™ÂéüÂßãËßÜÈ¢ëÁâáÊÆµ„ÄÇ |\n| **2. ËßÜÈ¢ëË¥®ÈáèËøáÊª§** | ÂâîÈô§‰ΩéË¥®Èáè„ÄÅ‰∏éÊú∫Âô®‰∫∫Êó†ÂÖ≥ÁöÑËßÜÈ¢ë„ÄÇ | - **Âú∫ÊôØÂàÜÂâ≤Ê£ÄÊµã**ÔºöËøáÊª§ÈùûÊú∫Âô®‰∫∫Âú∫ÊôØ„ÄÇ<br>- **ËßÜÈ¢ëË¥®ÈáèËØÑÂàÜÁ≥ªÁªü**Ôºö‰ªéÊ∏ÖÊô∞Â∫¶„ÄÅÂä®ÊÄÅÊïàÊûú„ÄÅÁæéÂ≠¶„ÄÅOCRÁ≠âÂ§öÁª¥Â∫¶ÊâìÂàÜ„ÄÇ | È´òË¥®ÈáèËßÜÈ¢ëÂ≠êÈõÜ„ÄÇ |\n| **3. ‰ªªÂä°ÂàÜÂâ≤‰∏éÊèèËø∞** | Â∞ÜÈïøËßÜÈ¢ëÂàÜÂâ≤ÊàêÁã¨Á´ãÁöÑ‰ªªÂä°ÁâáÊÆµÔºåÂπ∂ÁîüÊàêÁ≤æÂáÜÁöÑÊñáÂ≠óÊèèËø∞„ÄÇ | **ËßÜÈ¢ëÁêÜËß£Ê®°Âûã**ÔºöËá™Âä®ËØÜÂà´Âä®‰ΩúÊó∂Èó¥ËæπÁïå„ÄÇ<br>**MLLM**ÔºöÊ†πÊçÆËÆæËÆ°Â•ΩÁöÑÊèêÁ§∫Ê®°ÊùøÔºåÁîüÊàêÁªìÊûÑÂåñÊèèËø∞Ôºà‰∏ª‰Ωì„ÄÅÂØπË±°„ÄÅÊìç‰ΩúÔºâ„ÄÇ | Á∫¶400‰∏á‰∏™Â∏¶Ê†áÊ≥®ÁöÑËßÜÈ¢ëÁâáÊÆµ„ÄÇÊèèËø∞Ê†ºÂºèÊ†áÂáÜÂåñ„ÄÇ |\n| **4. Áâ©ÁêÜÂ±ûÊÄßÊ†áÊ≥®** | ‰∏∫ËßÜÈ¢ëÊ∑ªÂä†‰∏∞ÂØåÁöÑÁâ©ÁêÜÂ±ûÊÄßÊ≥®ÈáäÔºåÂ¢ûÂº∫Êï∞ÊçÆÁöÑÁâ©ÁêÜ‰ø°ÊÅØ„ÄÇ | - **FlashVSR**ÔºöËßÜÈ¢ëË∂ÖÂàÜËæ®ÁéáÔºåÊèêÂçáÊ∏ÖÊô∞Â∫¶„ÄÇ<br>- **AllTracker**ÔºöÁîüÊàê**Áªü‰∏ÄÁöÑÂÖâÊµÅÊ†áÊ≥®**ÔºåÁî®‰∫éÂä®‰Ωú‰∏ÄËá¥ÊÄßÂàÜÊûê„ÄÇ<br>- **Video Depth Anything**Ôºö",
            "timestamp": "2026-01-27T16:05:11.495801",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "‰ªñ‰ΩøÁî®‰∫ÜÂì™‰∫õËØÑ‰ª∑ÊåáÊ†á‰∏éÊï∞ÊçÆÈõÜÔºåÂàóË°®Ê†ºÂÖ∑‰ΩìËÆ≤ËÆ≤‰ªñÁöÑËØÑ‰ª∑ÊåáÊ†áÁöÑÁªÜËäÇ‰∏éÊï∞ÊçÆÈõÜÁöÑÁªÜËäÇ",
            "answer": "## ËÆ∫ÊñáÂàÜÊûêÔºöRethinking Video Generation Model for the Embodied World\n\nÊú¨ÊñáÈíàÂØπÂÖ∑Ë∫´Êô∫ËÉΩÔºàEmbodied AIÔºâÈ¢ÜÂüüÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºåÊèêÂá∫‰∫Ü‰∏Ä‰∏™**Á≥ªÁªüÊÄßËØÑ‰º∞Âü∫ÂáÜÔºàRBenchÔºâ**Âíå‰∏Ä‰∏™**Â§ßËßÑÊ®°È´òË¥®ÈáèÊï∞ÊçÆÈõÜÔºàRoVid-XÔºâ**ÔºåÊó®Âú®Ëß£ÂÜ≥ÂΩìÂâçÊ®°ÂûãÂú®ÁîüÊàêÁâ©ÁêÜÁúüÂÆû„ÄÅ‰ªªÂä°ÂØºÂêëÁöÑÊú∫Âô®‰∫∫ËßÜÈ¢ëÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇ\n\n### ‰∏Ä„ÄÅ Ê†∏ÂøÉË¥°ÁåÆÊ¶ÇËø∞\n\n1.  **RBenchÔºàËØÑ‰º∞Âü∫ÂáÜÔºâ**ÔºöÈ¶ñ‰∏™‰∏∫Êú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêËÆæËÆ°ÁöÑÁªºÂêàÊÄßÂü∫ÂáÜÔºåÂåÖÂê´**650‰∏™ËØÑ‰º∞Ê†∑Êú¨**ÔºåË¶ÜÁõñ**5Áßç‰ªªÂä°Á±ªÂûã**Âíå**4ÁßçÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ**ÔºåÂπ∂ÊèêÂá∫‰∫ÜË∂ÖË∂ä‰º†ÁªüËßÜËßâ‰øùÁúüÂ∫¶ÁöÑ„ÄÅÂèØÂ§çÁé∞ÁöÑËá™Âä®ÂåñËØÑ‰º∞ÊåáÊ†á„ÄÇ\n2.  **RoVid-XÔºàÊï∞ÊçÆÈõÜÔºâ**ÔºöÁõÆÂâçÊúÄÂ§ßÁöÑÂºÄÊ∫êÊú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÊï∞ÊçÆÈõÜÔºåÂåÖÂê´**Á∫¶400‰∏á‰∏™Ê†áÊ≥®ËßÜÈ¢ëÁâáÊÆµ**ÔºåÊ∂µÁõñÊï∞ÂçÉÁßç‰ªªÂä°ÔºåÂπ∂ÈôÑÊúâ‰∏∞ÂØåÁöÑÁâ©ÁêÜÂ±ûÊÄßÊ†áÊ≥®„ÄÇ\n\n### ‰∫å„ÄÅ ËØÑ‰ª∑ÊåáÊ†áÔºàRBenchÔºâËØ¶Ëß£\n\nRBenchÁöÑËØÑ‰º∞‰ΩìÁ≥ªÂàÜ‰∏∫‰∏§Â§ßÁª¥Â∫¶Ôºö**‰ªªÂä°ÂÆåÊàêÂ∫¶**Âíå**ËßÜËßâË¥®Èáè**„ÄÇÊØè‰∏™Áª¥Â∫¶‰∏ãÂåÖÂê´Â§ö‰∏™ÁªÜÁ≤íÂ∫¶ÊåáÊ†áÔºå‰∏ªË¶ÅÂà©Áî®**Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã**ËøõË°åËá™Âä®ÂåñËØÑ‰º∞„ÄÇ\n\n| ËØÑ‰º∞Áª¥Â∫¶ | ÊåáÊ†áÂêçÁß∞ | Ê†∏ÂøÉÂÖ≥Ê≥®ÁÇπ | ËØÑ‰º∞ÊñπÊ≥ï/ÁªÜËäÇ |\n| :--- | :--- | :--- | :--- |\n| **‰ªªÂä°ÂÆåÊàêÂ∫¶** | **Áâ©ÁêÜËØ≠‰πâÂêàÁêÜÊÄß** | ËßÜÈ¢ëÊòØÂê¶ËøùÂèçÂü∫Êú¨Áâ©ÁêÜÂÆöÂæãÂíåÂ∏∏ËØÜ„ÄÇ | ‰ΩøÁî®MLLMÔºàÂ¶ÇGPT-5, Qwen3-VLÔºâÂàÜÊûêÂÖ≥ÈîÆÂ∏ßÁΩëÊ†ºÔºåÊ£ÄÊµãÁâπÂÆöÂ§±Ë¥•Ê®°ÂºèÔºö<br>- **ÊºÇÊµÆ/Á©øÈÄè**ÔºöÁâ©‰ΩìÊàñÊú∫Âô®‰∫∫ÈÉ®‰ª∂ÊÇ¨Á©∫ÊàñÁõ∏‰∫íÁ©øÈÄè„ÄÇ<br>- **Ëá™ÂèëÂá∫Áé∞/Ê∂àÂ§±**ÔºöÂÆû‰ΩìÊó†Âõ†ÊûúÂú∞Âá∫Áé∞ÊàñÊ∂àÂ§±„ÄÇ<br>- **ÈùûÊé•Ëß¶ÈôÑÁùÄ/ÈîôËØØÊäìÂèñ**ÔºöÁâ©‰ΩìÂú®Ê≤°ÊúâÂèØËßÅÊé•Ëß¶ÊàñÊäìÂèñÂô®Êú™Èó≠ÂêàÁöÑÊÉÖÂÜµ‰∏ãÈöèÊú∫Âô®‰∫∫ÁßªÂä®„ÄÇ |\n| | **‰ªªÂä°‰æù‰ªé‰∏ÄËá¥ÊÄß** | ÁîüÊàêËßÜÈ¢ëÁöÑÂä®‰ΩúÂ∫èÂàóÊòØÂê¶‰∏éÊñáÊú¨Êåá‰ª§ÁöÑÊÑèÂõæÂíåÈ°∫Â∫è‰∏ÄËá¥„ÄÇ | ‰ΩøÁî®MLLMÂü∫‰∫éVQAÊ£ÄÊü•Ê∏ÖÂçïËøõË°åËØÑ‰º∞Ôºö<br>- **‰ªªÂä°ÂìçÂ∫îÊÄß**ÔºöÊú∫Âô®‰∫∫ÊòØÂê¶ÂêØÂä®Âπ∂ÂÆåÊàê‰∫ÜÁõÆÊ†áÁä∂ÊÄÅ„ÄÇ<br>- **ÂÖ≥ÈîÆÂä®‰ΩúÂÆåÊï¥ÊÄß**ÔºöÊåá‰ª§Ë¶ÅÊ±ÇÁöÑÂÖ≥ÈîÆÂä®‰ΩúÔºàÂ¶ÇÊäìÂèñ„ÄÅÊîæÁΩÆÔºâÊòØÂê¶ÂèëÁîü‰∏îÈ°∫Â∫èÊ≠£Á°Æ„ÄÇ |\n| **ËßÜËßâË¥®Èáè** | **ËøêÂä®ÂπÖÂ∫¶** | Êú∫Âô®‰∫∫‰∏ª‰ΩìÁöÑÊúâÊïàËøêÂä®ÂπÖÂ∫¶ÔºåÊéíÈô§Áõ∏Êú∫ËøêÂä®Âπ≤Êâ∞„ÄÇ | 1. ‰ΩøÁî®**GroundingDINO**ÂÆö‰ΩçÊú∫Âô®‰∫∫‰∏ª‰Ωì„ÄÇ<br>2. ‰ΩøÁî®**GroundedSAM**ÁîüÊàêÁ®≥ÂÆöÊé©Á†Å„ÄÇ<br>3. ‰ΩøÁî®**CoTracker**Ë∑üË∏™Êé©Á†ÅÂÜÖÁöÑÂØÜÈõÜÂÖ≥ÈîÆÁÇπ„ÄÇ<br>4. ËÆ°ÁÆóË°•ÂÅøËÉåÊôØËøêÂä®ÂêéÁöÑÂπ≥Âùá‰ΩçÁßªÔºåÂæóÂàÜË∂ä‰ΩéË°®Á§∫‰∏ª‰ΩìËøêÂä®Ë∂ä‰∏çË∂≥„ÄÇ |\n| | **Êú∫Âô®‰∫∫-‰∏ª‰ΩìÁ®≥ÂÆöÊÄß** | Êú∫Âô®‰∫∫ÂΩ¢ÊÄÅÂíåÁõÆÊ†áÁâ©‰ΩìÂ±ûÊÄßÂú®Êó∂Èó¥‰∏äÁöÑÁ®≥ÂÆöÊÄß„ÄÇ | ÈááÁî®**ÂØπÊØîÂºèVQA**ÔºöMLLMÊØîËæÉÂèÇËÄÉÂ∏ßÂíåÁîüÊàêÂ∏ßÔºåËØÑ‰º∞‰∏ÄËá¥ÊÄß„ÄÇÂÖ≥Ê≥®Ôºö<br>- **Êú∫Âô®‰∫∫ÁªìÊûÑÊºÇÁßª**ÔºöÂ¶ÇÊú∫Ê¢∞ËáÇÊï∞Èáè„ÄÅÂÖ≥ËäÇÊãìÊâë„ÄÅÂ§πÁà™ÂΩ¢Áä∂ÂèëÁîü‰∏çÂêàÁêÜÂèòÂåñ„ÄÇ<br>- **‰∏ª‰ΩìÂ§ñËßÇÊºÇÁßª**ÔºöÂ¶ÇÁâ©‰ΩìÁ±ªÂà´„ÄÅÈ¢úËâ≤„ÄÅÂΩ¢Áä∂ÂèëÁîü‰∏çÂêàÁêÜÂèòÂåñ„ÄÇ |\n| | **ËøêÂä®Âπ≥ÊªëÂ∫¶** | Êó∂Èó¥ËøûÁª≠ÊÄßÂíåÂä®ÊÄÅËá™ÁÑ∂Â∫¶ÔºåÊ£ÄÊµãÂ∏ßÈó¥ÊäñÂä®„ÄÅÊ®°Á≥äÁ≠â‰º™ÂΩ±„ÄÇ | 1. ‰ΩøÁî®**Q-Align**Ê®°ÂûãËÆ°ÁÆóÊØèÂ∏ßÁöÑÁæéÂ≠¶Ë¥®ÈáèÂàÜÊï∞„ÄÇ<br>2. ËÆ°ÁÆóÁõ∏ÈÇªÂ∏ßÁöÑË¥®ÈáèÂàÜÊï∞Ê≥¢Âä®„ÄÇ<br>3. Ê†πÊçÆ**ËøêÂä®ÂπÖÂ∫¶**ËÆæÂÆöËá™ÈÄÇÂ∫îÈòàÂÄºÔºåÊ≥¢Âä®Ë∂ÖËøáÈòàÂÄºÂàôÊ†áËÆ∞‰∏∫ÂºÇÂ∏∏„ÄÇ<br>4. ÂæóÂàÜË∂äÈ´òË°®Á§∫ËøêÂä®Ë∂äÂπ≥Êªë„ÄÇ |\n\n**ÊåáÊ†áËÅöÂêà**ÔºöÊúÄÁªàÊ®°ÂûãÂæóÂàÜÁî±**‰ªªÂä°ÂÆåÊàê**Âíå**ËßÜËßâË¥®Èáè**‰∏§‰∏™ÁªºÂêàÂàÜÊï∞Âπ≥ÂùáÂæóÂà∞ÔºåÂÖ∂‰∏≠ËßÜËßâË¥®ÈáèÂàÜÊï∞‰ºöÂõ†ËøêÂä®ÂπÖÂ∫¶‰∏çË∂≥ÂíåÁ®≥ÂÆöÊÄßÂ∑ÆËÄåÂèóÂà∞ÊÉ©ÁΩö„ÄÇ\n\n### ‰∏â„ÄÅ Êï∞ÊçÆÈõÜÔºàRoVid-XÔºâËØ¶Ëß£\n\nRoVid-XÊó®Âú®‰∏∫ËÆ≠ÁªÉÂÖ∑Â§áÁâ©ÁêÜ‰∫§‰∫íÂÖàÈ™åÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÊèê‰æõÂ§ßËßÑÊ®°„ÄÅÈ´òË¥®Èáè„ÄÅÂ§öÊ†∑ÂåñÁöÑÊï∞ÊçÆ„ÄÇÂÖ∂ÊûÑÂª∫ÈÄöËøá‰∏Ä‰∏™**ÂõõÈò∂ÊÆµÊµÅÊ∞¥Á∫ø**ÂÆåÊàê„ÄÇ\n\n| Èò∂ÊÆµ | ÂêçÁß∞ | Ê†∏ÂøÉÊìç‰Ωú‰∏éÁõÆÊ†á | ‰∫ßÂá∫ÁªÜËäÇ |\n| :--- | :--- | :--- | :--- |\n| **1** | **Êú∫Âô®‰∫∫ËßÜÈ¢ëÊî∂ÈõÜ** | ‰ªé‰∫íËÅîÁΩëËßÜÈ¢ëÂπ≥Âè∞Âíå**20Â§ö‰∏™**ÂºÄÊ∫êÂÖ∑Ë∫´Êï∞ÊçÆÈõÜ‰∏≠Êî∂ÈõÜÂéüÂßãËßÜÈ¢ë„ÄÇ‰ΩøÁî®**GPT-5**ËøõË°åÂÜÖÂÆπËøáÊª§Ôºå‰øùÁïô‰∏éÊú∫Âô®‰∫∫‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËßÜÈ¢ë„ÄÇ | ÂàùÊ≠•Á≠õÈÄâÂá∫Á∫¶300‰∏á‰∏™ÂéüÂßãËßÜÈ¢ëÁâáÊÆµ„ÄÇ |\n| **2** | **ËßÜÈ¢ëË¥®ÈáèËøáÊª§** | ËøõË°å‰∏•Ê†ºÁöÑË¥®ÈáèÁ≠õÈÄâÔºö<br>1. Âú∫ÊôØÂàÜÂâ≤Ê£ÄÊµãÔºåÁßªÈô§ÈùûÊú∫Âô®‰∫∫ËßÜÈ¢ë„ÄÇ<br>2. Â§öÁª¥Â∫¶Ë¥®ÈáèËØÑÂàÜÁ≥ªÁªüÔºàÊ∏ÖÊô∞Â∫¶„ÄÅÂä®ÊÄÅÊïàÊûú„ÄÅÁæéÂ≠¶„ÄÅOCRÁ≠âÔºâ„ÄÇ | Á°Æ‰øùÊúÄÁªàÊï∞ÊçÆÈõÜÁöÑËßÜÈ¢ëÂú®ËßÜËßâ‰∏äÊ∏ÖÊô∞„ÄÅÂÜÖÂÆπÁõ∏ÂÖ≥‰∏îË¥®ÈáèÈ´ò„ÄÇ |\n| **3** | **‰ªªÂä°ÂàÜÂâ≤‰∏éÂ≠óÂπïÁîüÊàê** | ‰ΩøÁî®ËßÜÈ¢ëÁêÜËß£Ê®°ÂûãËá™Âä®ÂàÜÊûêÊú∫Âô®‰∫∫Âä®‰ΩúÔºö<br>1. Âü∫‰∫éÊó∂Èó¥Êà≥Â∞ÜËßÜÈ¢ëÂàÜÂâ≤‰∏∫Áã¨Á´ãÁöÑ‰ªªÂä°ÁâáÊÆµ„ÄÇ<br>2. ‰∏∫ÊØè‰∏™ÁâáÊÆµÁîüÊàêÊèèËø∞ÊÄßÂ≠óÂπïÔºåÂáÜÁ°ÆÊèèËø∞Êú∫Âô®‰∫∫Âä®‰Ωú„ÄÅÊìç‰ΩúÂØπË±°ÂíåÁªÜËäÇ„ÄÇ | Â≠óÂπïÊ†ºÂºèÊ†áÂáÜÂåñÔºåÊ∏ÖÊô∞ÊèèËø∞ÊØè‰∏™ËßÜÈ¢ëÁâáÊÆµÁöÑÂä®‰ΩúËØ≠‰πâ„ÄÇ |\n| **4** | **Áâ©ÁêÜÂ±ûÊÄßÊ†áÊ≥®** | ‰∏∫ËßÜÈ¢ëÊ∑ªÂä†Áªü‰∏ÄÁöÑÁâ©ÁêÜÂ±ûÊÄßÊ≥®ÈáäÔºåÂ¢ûÂº∫Ê®°ÂûãÁöÑÁâ©ÁêÜÁ©∫Èó¥‰∏ÄËá¥ÊÄßÁêÜËß£Ôºö<br>- **FlashVSR**ÔºöËßÜÈ¢ëË∂ÖÂàÜËæ®ÁéáÔºåÊèêÂçáÊ∏ÖÊô∞Â∫¶„ÄÇ<br>- **AllTracker**ÔºöÊ†áÊ≥®**Áªü‰∏ÄÁöÑÂÖâÊµÅ**ÔºåÁ°Æ‰øùË∑®Âú∫ÊôØÂä®‰ΩúË∑üË∏™‰∏ÄËá¥ÊÄß„ÄÇ<br>- **Video Depth Anything**ÔºöÁîüÊàê**Áõ∏ÂØπÊ∑±Â∫¶Âõæ**ÔºåÊèèËø∞Á©∫Èó¥ÂÖ≥Á≥ª„ÄÇ | Êèê‰æõ‰∏∞ÂØåÁöÑÂÖâÊµÅÂíåÊ∑±Â∫¶‰ø°ÊÅØÔºå‰∏∫Ê®°ÂûãÂ≠¶‰π†Áâ©ÁêÜ‰∫§‰∫íÂíå‰∏âÁª¥ÁªìÊûÑÊèê‰æõÂÖ≥ÈîÆÊï∞ÊçÆÊîØÊåÅ„ÄÇ |\n\n**Êï∞ÊçÆÈõÜÂÖ≥ÈîÆÁªüËÆ°‰∏éÁâπÁÇπ**Ôºö\n- **ËßÑÊ®°**ÔºöÁ∫¶ **4Áôæ‰∏á** ‰∏™ËßÜÈ¢ëÁâáÊÆµ„ÄÇ\n- **Â§öÊ†∑ÊÄß**ÔºöË¶ÜÁõñ **1300+** ÁßçÂä®‰ΩúÊäÄËÉΩÔºå**4Áßç‰∏ªÊµÅÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ**ÔºàÂçïËáÇ„ÄÅÂèåËáÇ„ÄÅ‰∫∫ÂΩ¢„ÄÅÂõõË∂≥ÔºâÔºå‰ª•Âèä‰∏∞ÂØåÁöÑ‰ªªÂä°Á±ªÂûãÂíå‰∫§‰∫íÁâ©‰Ωì„ÄÇ\n- **Ë¥®Èáè**ÔºöÂàÜËæ®Áéá‰∏ªË¶Å‰∏∫ **720P**ÔºåÈôÑÂ∏¶**ÂÖâÊµÅ**Âíå**Ê∑±Â∫¶**Ê†áÊ≥®„ÄÇ\n- **Áã¨ÁâπÊÄß**Ôºö‰∏éÁé∞ÊúâÊú∫Âô®‰∫∫Êï∞ÊçÆÈõÜÔºàÂ¶ÇOpen X-Embodiment, DROIDÁ≠âÔºâÁõ∏ÊØîÔºå**RoVid-X** ÊòØÈ¶ñ‰∏™**‰∏ì‰∏∫ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãËÆæËÆ°**„ÄÅ**ËßÑÊ®°ÊúÄÂ§ß**„ÄÅ‰∏îÊèê‰æõ**Áªü‰∏ÄÁâ©ÁêÜÂ±ûÊÄßÊ†áÊ≥®**ÁöÑÂºÄÊ∫êÊï∞ÊçÆÈõÜ„ÄÇ\n\n### Âõõ„ÄÅ ÊäÄÊúØÂàõÊñ∞‰∏éÂÆûÈôÖ‰ª∑ÂÄº\n\n1.  **ËØÑ‰º∞ËåÉÂºèÁöÑËΩ¨Âèò**Ôºö‰ªéËøΩÊ±Ç‚ÄúËßÜËßâÈÄºÁúü‚ÄùËΩ¨ÂêëËØÑ‰º∞‚ÄúÁâ©ÁêÜÊô∫ËÉΩ‚Äù„ÄÇ**RBench** È¶ñÊ¨°Á≥ªÁªüÊÄßÂú∞Â∞ÜÁâ©ÁêÜÂêàÁêÜÊÄß„ÄÅ‰ªªÂä°ÈÄªËæëÂíåÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÁ®≥ÂÆöÊÄßÁ∫≥ÂÖ•ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑËØÑ‰º∞‰ΩìÁ≥ªÔºåÊè≠Á§∫‰∫ÜÂΩìÂâçSOTAÊ®°ÂûãÔºàÂ¶ÇSoraÁ≥ªÂàóÔºâÂú®ÂÖ∑Ë∫´‰ªªÂä°‰∏äÂ≠òÂú®ÁöÑÊòæËëó‚ÄúÈ¢ÜÂüüÈ∏øÊ≤ü‚Äù„ÄÇ\n2.  **Êï∞ÊçÆÈ©±Âä®ÁöÑËß£ÂÜ≥ÊñπÊ°à**ÔºöÊòéÁ°ÆÊåáÂá∫È´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂåÆ‰πèÊòØÁì∂È¢à„ÄÇ**RoVid-X** ‰∏ç‰ªÖÊèê‰æõ‰∫ÜÊï∞ÊçÆÈáèÔºåÊõ¥ÈáçË¶ÅÁöÑÊòØÈÄöËøá**Á´ØÂà∞Á´ØÁöÑÂ§ÑÁêÜÊµÅÊ∞¥Á∫ø**Âíå**Áâ©ÁêÜÂ±ûÊÄßÂ¢ûÂº∫**ÔºåÂàõÈÄ†‰∫ÜÈÄÇÁî®‰∫éÂ≠¶‰π†Áâ©ÁêÜ‰∫§‰∫íÂÖàÈ™åÁöÑÁªìÊûÑÂåñÊï∞ÊçÆ„ÄÇ\n3.  **ÂçèÂêåÁîüÊÄÅÁ≥ªÁªü**ÔºöËÆ∫ÊñáÊûÑÂª∫‰∫Ü **‚ÄúËØÑ‰º∞-Êï∞ÊçÆ‚ÄùÂçèÂêåÁöÑÁîüÊÄÅÁ≥ªÁªü**„ÄÇRBenchËØäÊñ≠Ê®°ÂûãÁº∫Èô∑ÔºåRoVid-XÊèê‰æõÊ≤ªÁñó‚ÄúËçØÊñπ‚Äù„ÄÇÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî®RoVid-XÂæÆË∞ÉÁé∞ÊúâÊ®°ÂûãÔºåËÉΩÁ®≥ÂÆöÊèêÂçáÂÖ∂Âú®RBenchÂêÑÈ°πÊåáÊ†á‰∏äÁöÑË°®Áé∞„ÄÇ\n4.  **ÂØπÈ¢ÜÂüüÁöÑÊ∑±ÂàªÊ¥ûÂØü**ÔºöÈÄöËøáÂØπ25‰∏™‰ª£Ë°®ÊÄßÊ®°ÂûãÁöÑËØÑ‰º∞ÔºåÂæóÂá∫ÂÖ≥ÈîÆÁªìËÆ∫ÔºöÂïÜ‰∏öÈó≠Ê∫êÊ®°ÂûãÈ¢ÜÂÖàÔºõÊ®°ÂûãËø≠‰ª£ÊòæËëóÊèêÂçáÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõÔºõ**Á≤æÁªÜÊìç‰ΩúÂíåÈ´òÁ∫ßÈÄªËæëÊé®ÁêÜÊòØÂΩìÂâçÊ®°ÂûãÁöÑÂÖ±ÂêåÁì∂È¢à**Ôºõ‰∏ìÁî®‰∫éÁâπÂÆöÊú∫Âô®‰∫∫ÁöÑÊ®°ÂûãÊó†Ê≥ïÂº•Ë°•‚Äú‰∏ñÁïåÁü•ËØÜ‚ÄùÁöÑÁº∫Â§±„ÄÇ\n\n**ÊÄªÁªì**ÔºöÊú¨Â∑•‰Ωú‰∏∫ÂÖ∑Ë∫´AIÈ¢ÜÂüüÁöÑËßÜÈ¢ëÁîüÊàêÁ†îÁ©∂Â•†ÂÆö‰∫ÜÂùöÂÆûÁöÑËØÑ‰º∞ÂíåÊï∞ÊçÆÂü∫Á°Ä„ÄÇÂÆÉÊé®Âä®ËØ•È¢ÜÂüü‰ªéÁîüÊàê‚ÄúÂ•ΩÁúãÁöÑËßÜÈ¢ë‚ÄùÂêëÊûÑÂª∫ËÉΩÂ§üÊ®°ÊãüÁúüÂÆûÁâ©ÁêÜ‰∫§‰∫íÁöÑ‚Äú‰∏ñÁïåÊ®°ÊãüÂô®‚ÄùËøàËøõÔºåÂØπ‰∫éÂèëÂ±ïÈÄöÁî®ÂÖ∑Ë∫´Êô∫ËÉΩÂÖ∑ÊúâÈáçË¶ÅÁöÑÁêÜËÆ∫ÂíåÂÆûË∑µ‰ª∑ÂÄº„ÄÇ",
            "timestamp": "2026-01-27T16:06:12.565893",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ËÆ∫ÊñáÂú®Âì™‰∫õÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºü‰∏ªË¶ÅÁöÑËØÑ‰º∞ÊåáÊ†áÂíåÊÄßËÉΩÊèêÂçáÊòØÂ§öÂ∞ëÔºü",
            "answer": "## ËÆ∫ÊñáÂÆûÈ™åÊï∞ÊçÆÈõÜ„ÄÅËØÑ‰º∞ÊåáÊ†á‰∏éÊÄßËÉΩÊèêÂçáÂàÜÊûê\n\n### ‰∏Ä„ÄÅ ÂÆûÈ™åÊï∞ÊçÆÈõÜ\nËÆ∫ÊñáÁöÑÂÆûÈ™å‰∏ªË¶ÅÂõ¥ÁªïÂÖ∂ÊèêÂá∫ÁöÑ‰∏§‰∏™Ê†∏ÂøÉË¥°ÁåÆÂ±ïÂºÄÔºö**ËØÑ‰º∞Âü∫ÂáÜ RBench** Âíå**ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ RoVid-X**„ÄÇ\n\n1.  **ËØÑ‰º∞Âü∫ÂáÜÔºöRBench**\n    *   **ÊÄßË¥®**Ôºö‰∏Ä‰∏™‰∏ìÈó®‰∏∫Êú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàê‰ªªÂä°ËÆæËÆ°ÁöÑÁªºÂêàÊÄßËØÑ‰º∞ÈõÜ„ÄÇ\n    *   **ËßÑÊ®°**ÔºöÂåÖÂê´ **650** ‰∏™Á≤æÂøÉÁ≠ñÂàíÁöÑËØÑ‰º∞Ê†∑Êú¨ÔºàÂõæÂÉè-ÊñáÊú¨ÂØπÔºâ„ÄÇ\n    *   **ÊûÑÊàê**Ôºö\n        *   **‰ªªÂä°ÂØºÂêë**ÔºöÊ∂µÁõñ5‰∏™‰ª£Ë°®ÊÄßÊú∫Âô®‰∫∫‰ªªÂä°È¢ÜÂüüÔºåÂêÑ50‰∏™Ê†∑Êú¨„ÄÇ\n            *   Â∏∏ËßÅÊìç‰Ωú\n            *   ÈïøÊó∂Á®ãËßÑÂàí\n            *   Â§öÂÆû‰ΩìÂçè‰Ωú\n            *   Á©∫Èó¥ÂÖ≥Á≥ª\n            *   ËßÜËßâÊé®ÁêÜ\n        *   **ÂÖ∑Ë∫´ÂØºÂêë**ÔºöÊ∂µÁõñ4Áßç‰∏ªÊµÅÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÔºåÂêÑ100‰∏™Ê†∑Êú¨„ÄÇ\n            *   ÂèåËáÇÊú∫Âô®‰∫∫\n            *   ‰∫∫ÂΩ¢Êú∫Âô®‰∫∫\n            *   ÂçïËáÇÊú∫Âô®‰∫∫\n            *   ÂõõË∂≥Êú∫Âô®‰∫∫\n    *   **Êù•Ê∫ê**ÔºöÊ†∑Êú¨ÂõæÂÉèÊù•Ëá™ÂºÄÊ∫êÊï∞ÊçÆÈõÜÊàñÁΩëÁªúÂÖ¨ÂºÄÁöÑÈ´òË¥®ÈáèÊú∫Âô®‰∫∫ËßÜÈ¢ëÂÖ≥ÈîÆÂ∏ßÔºåÂπ∂ÁªèËøá‰∫∫Â∑•È™åËØÅÂíåÈáçÊñ∞ËÆæËÆ°ÊñáÊú¨ÊèêÁ§∫Ôºå‰ª•ÈÅøÂÖç‰∏éËÆ≠ÁªÉÊï∞ÊçÆÈáçÂè†„ÄÇ\n\n2.  **ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºöRoVid-X**\n    *   **ÊÄßË¥®**Ôºö‰∏Ä‰∏™Â§ßËßÑÊ®°„ÄÅÈ´òË¥®Èáè„ÄÅ‰∏ì‰∏∫ÂÖ∑Ë∫´ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãËÆ≠ÁªÉËÆæËÆ°ÁöÑÂºÄÊ∫êÊú∫Âô®‰∫∫ËßÜÈ¢ëÊï∞ÊçÆÈõÜ„ÄÇ\n    *   **ËßÑÊ®°**ÔºöÂåÖÂê´Á∫¶ **400‰∏á** ‰∏™ÁªèËøáÁ≠õÈÄâÂíåÊ†áÊ≥®ÁöÑÊú∫Âô®‰∫∫ËßÜÈ¢ëÁâáÊÆµ„ÄÇ\n    *   **Êù•Ê∫ê‰∏éÊûÑÂª∫**ÔºöÈÄöËøá‰∏Ä‰∏™ÂõõÈò∂ÊÆµÊµÅÊ∞¥Á∫øÊï¥Âêà‰∫ÜË∂ÖËøá20‰∏™ÂºÄÊ∫êÊï∞ÊçÆÈõÜÂíåÁΩëÁªúËßÜÈ¢ëÂπ≥Âè∞„ÄÇ\n        1.  Êú∫Âô®‰∫∫ËßÜÈ¢ëÊî∂ÈõÜ\n        2.  ËßÜÈ¢ëË¥®ÈáèËøáÊª§\n        3.  ‰ªªÂä°ÂàÜÂâ≤‰∏éÊèèËø∞ÁîüÊàê\n        4.  Áâ©ÁêÜÂ±ûÊÄßÊ†áÊ≥®ÔºàÂ¶ÇÂÖâÊµÅ„ÄÅÊ∑±Â∫¶ÂõæÔºâ\n    *   **ÁâπÁÇπ**ÔºöË¶ÜÁõñÂπøÊ≥õÁöÑÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÂíå‰ªªÂä°Á±ªÂûãÔºåÊèê‰æõÊ†áÂáÜÂåñÁöÑ‰ªªÂä°ÊèèËø∞Âíå‰∏∞ÂØåÁöÑÁâ©ÁêÜÂ±ûÊÄßÊ≥®Èáä„ÄÇ\n\n### ‰∫å„ÄÅ ‰∏ªË¶ÅËØÑ‰º∞ÊåáÊ†á\nËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂ•óË∂ÖË∂ä‰º†ÁªüËßÜËßâ‰øùÁúüÂ∫¶„ÄÅ‰∏ìÊ≥®‰∫éÊú∫Âô®‰∫∫‰ªªÂä°ÂÆåÊàêÂ∫¶ÂíåÁâ©ÁêÜÁúüÂÆûÊÄßÁöÑËá™Âä®ÂåñËØÑ‰º∞ÊåáÊ†á„ÄÇËøô‰∫õÊåáÊ†áÈÄöËøáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÂíåÂ∫ïÂ±ÇËßÜËßâËÆ°ÁÆóÁõ∏ÁªìÂêàÁöÑÊñπÂºèËøõË°åËØÑ‰º∞„ÄÇ\n\n**Ê†∏ÂøÉÊåáÊ†áÂàÜ‰∏∫‰∏§Â§ßÁª¥Â∫¶Ôºö**\n\n1.  **‰ªªÂä°ÂÆåÊàêÂ∫¶**\n    *   **Áâ©ÁêÜËØ≠‰πâÂêàÁêÜÊÄß**ÔºöËØÑ‰º∞ËßÜÈ¢ëÊòØÂê¶ËøùÂèçÂü∫Êú¨Áâ©ÁêÜÂÆöÂæãÂíåÂ∏∏ËØÜÔºàÂ¶ÇÁâ©‰ΩìÊºÇÊµÆ„ÄÅÁ©øÈÄè„ÄÅÈùûÊé•Ëß¶ÈôÑÁùÄ„ÄÅÂÆû‰ΩìÊó†ÊïÖÂá∫Áé∞/Ê∂àÂ§±Ôºâ„ÄÇ\n    *   **‰ªªÂä°ÈÅµÂæ™‰∏ÄËá¥ÊÄß**ÔºöËØÑ‰º∞ÁîüÊàêËßÜÈ¢ëÁöÑÂä®‰ΩúÂ∫èÂàóÊòØÂê¶‰∏éÊñáÊú¨Êåá‰ª§ÁöÑÊÑèÂõæÂíåÈ°∫Â∫è‰∏ÄËá¥ÔºàÂ¶ÇÂÖ≥ÈîÆÂä®‰ΩúÊòØÂê¶ÂÆåÊï¥„ÄÅÈ°∫Â∫èÊòØÂê¶Ê≠£Á°ÆÔºâ„ÄÇ\n\n2.  **ËßÜËßâË¥®Èáè**\n    *   **ËøêÂä®ÂπÖÂ∫¶**ÔºöÈáèÂåñÊú∫Âô®‰∫∫‰∏ª‰ΩìÁöÑÊúâÊïàËøêÂä®ÂπÖÂ∫¶ÔºåÊéíÈô§Áõ∏Êú∫ËøêÂä®ÁöÑÂΩ±ÂìçÔºåÊÉ©ÁΩö‚ÄúÂπ≥Êªë‰ΩÜÈùôÊ≠¢‚ÄùÁöÑÂ§±Ë¥•Ê°à‰æã„ÄÇ\n    *   **Êú∫Âô®‰∫∫-‰∏ª‰ΩìÁ®≥ÂÆöÊÄß**ÔºöËØÑ‰º∞Êú∫Âô®‰∫∫ÁöÑÂΩ¢ÊÄÅÁªìÊûÑ‰ª•ÂèäÁõÆÊ†áÁâ©‰ΩìÁöÑÂ±ûÊÄßÔºàÂ¶ÇÁ±ªÂà´„ÄÅÈ¢úËâ≤„ÄÅÂΩ¢Áä∂ÔºâÂú®Êó∂Èó¥‰∏äÁöÑÁ®≥ÂÆöÊÄß„ÄÇ\n    *   **ËøêÂä®Âπ≥ÊªëÂ∫¶**ÔºöËØÑ‰º∞ËßÜÈ¢ëÂú®Êó∂Èó¥‰∏äÁöÑËøûÁª≠ÊÄßÂíåÂä®ÊÄÅËá™ÁÑ∂Â∫¶ÔºåÊ£ÄÊµãÂ∏ßÈó¥ÊäñÂä®„ÄÅÊ®°Á≥äÁ≠â‰º™ÂΩ±„ÄÇ\n\n**ÊåáÊ†áËÅöÂêà**ÔºöÊúÄÁªàÔºå‰∏äËø∞‰∫î‰∏™ÁªÜÁ≤íÂ∫¶ÊåáÊ†áË¢´ËÅöÂêàÊàê‰∏§‰∏™ÊÄªÂàÜÔºö**‰ªªÂä°ÂÆåÊàêÊÄªÂàÜ** Âíå **ËßÜËßâË¥®ÈáèÊÄªÂàÜ**ÔºåÁî®‰∫éÊ®°ÂûãÈó¥ÁöÑÁªºÂêàÊØîËæÉ„ÄÇ\n\n### ‰∏â„ÄÅ ‰∏ªË¶ÅÊÄßËÉΩÊèêÂçá‰∏éÂÖ≥ÈîÆÂèëÁé∞\nËÆ∫ÊñáÈÄöËøáÂú® **RBench** ‰∏äËØÑ‰º∞ **25‰∏™** ‰ª£Ë°®ÊÄßËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºàÂåÖÊã¨ÂºÄÊ∫ê„ÄÅÂïÜ‰∏öÂíåÊú∫Âô®‰∫∫‰∏ìÁî®Ê®°ÂûãÔºâÔºåÂπ∂Âà©Áî® **RoVid-X** ËøõË°åÂæÆË∞ÉÂÆûÈ™åÔºåÂæóÂá∫‰∫Ü‰ª•‰∏ãÊ†∏ÂøÉÊÄßËÉΩÂàÜÊûêÂíåÊèêÂçáÈ™åËØÅÔºö\n\n1.  **Âü∫ÂáÜËØÑ‰º∞Êè≠Á§∫ÁöÑÊ®°ÂûãËÉΩÂäõÂ∑ÆË∑ù**\n    *   **ÂïÜ‰∏öÊ®°ÂûãÈ¢ÜÂÖà**ÔºöÊéíÂêçÂâç7ÁöÑÂùá‰∏∫ÂïÜ‰∏öÈó≠Ê∫êÊ®°ÂûãÔºàÂ¶ÇWan 2.6, Seedance 1.5 ProÔºâÔºåÂÖ∂**Âπ≥ÂùáÂæóÂàÜÔºà~0.58Ôºâ**ÊòæËëóÈ´ò‰∫éÊúÄÂ•ΩÁöÑÂºÄÊ∫êÊ®°ÂûãÔºàWan2.2_A14BÔºåÂæóÂàÜ0.507Ôºâ„ÄÇ\n    *   **‚ÄúÂ™í‰Ωì-Ê®°Êãü‚ÄùÈ∏øÊ≤ü**ÔºöÈù¢ÂêëÊ∂àË¥πÁ∫ßÂ™í‰ΩìÁöÑÁü•ÂêçÊ®°ÂûãÔºàÂ¶ÇSoraÁ≥ªÂàóÔºâÂú®ËØ•Âü∫ÂáÜ‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºàSora v2 ProÊéíÂêçÁ¨¨17ÔºåÂæóÂàÜ0.362ÔºâÔºåË°®Êòé**ËßÜËßâÊµÅÁïÖÊÄß‰∏çÁ≠â‰∫éÁâ©ÁêÜÁúüÂÆûÊÄß**„ÄÇ\n    *   **ËÆ§Áü•‰∏éÁ≤æÁªÜÊéßÂà∂ÊòØÁì∂È¢à**ÔºöÊâÄÊúâÊ®°ÂûãÂú®ÈúÄË¶Å**È´òÁ∫ßÈÄªËæëÊé®ÁêÜ**ÁöÑ‰ªªÂä°‰∏äË°®Áé∞ÊúÄÂ∑Æ„ÄÇÂú®Êú∫Âô®‰∫∫ÂΩ¢ÊÄÅ‰∏äÔºåÊ®°ÂûãÂú®**Á≤æÁªÜÊìç‰Ωú**‰∏äÁöÑÂæóÂàÜÊôÆÈÅç‰Ωé‰∫é**Á≤óÁ≤íÂ∫¶ËøêÂä®**„ÄÇ\n    *   **È¢ÜÂüüÊï∞ÊçÆ vs. ‰∏ñÁïåÁü•ËØÜ**ÔºöÊú∫Âô®‰∫∫‰∏ìÁî®Ê®°ÂûãÔºàÂ¶ÇCosmos 2.5ÔºâË°®Áé∞Á®≥ÂÅ•Ôºå‰ΩÜÁ∫ØÊú∫Âô®‰∫∫Êï∞ÊçÆËÆ≠ÁªÉÁöÑÊ®°ÂûãÔºàÂ¶ÇVidarÔºâÊéíÂêçÈù†ÂêéÔºåËØ¥Êòé**Â§ßËßÑÊ®°ÈÄöÁî®È¢ÑËÆ≠ÁªÉÊèê‰æõÁöÑ‚Äú‰∏ñÁïåÁü•ËØÜ‚Äù‰∏çÂèØÊàñÁº∫**„ÄÇ\n\n2.  **RoVid-XÊï∞ÊçÆÈõÜÁöÑÊúâÊïàÊÄßÈ™åËØÅ**\n    *   **ÂÆûÈ™åËÆæÁΩÆ**Ôºö‰ΩøÁî®RoVid-XÁöÑÂ≠êÈõÜÔºà20‰∏áÂÆû‰æãÔºâÂØπ‰∏§‰∏™ÂºÄÊ∫êÂü∫Á°ÄÊ®°ÂûãÔºàWan2.1_14B Âíå Wan2.2_5BÔºâËøõË°åÂæÆË∞É„ÄÇ\n    *   **ÊÄßËÉΩÊèêÂçá**ÔºöÂæÆË∞ÉÂêéÔºåÊ®°ÂûãÂú®**ÊâÄÊúâ‰∫î‰∏™‰ªªÂä°È¢ÜÂüüÂíåÂõõÁßçÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ**‰∏äÁöÑËØÑ‰º∞ÂæóÂàÜÂùáËé∑ÂæóÁ®≥ÂÆöÊèêÂçá„ÄÇ\n    *   **ÂÖ∑‰ΩìÊï∞ÊçÆÁ§∫‰æã**Ôºö\n        *   **Wan2.1_14BÊ®°Âûã**Ôºö\n            *   **‰ªªÂä°Áª¥Â∫¶**ÔºöÂú®‚ÄúÈïøÊó∂Á®ãËßÑÂàí‚Äù‰ªªÂä°‰∏äÔºåÂæóÂàÜ‰ªé **0.335** ÊèêÂçáËá≥ **0.389**ÔºõÂú®‚ÄúËßÜËßâÊé®ÁêÜ‚Äù‰ªªÂä°‰∏äÔºå‰ªé **0.205** Â§ßÂπÖÊèêÂçáËá≥ **0.298**„ÄÇ\n            *   **ÂΩ¢ÊÄÅÁª¥Â∫¶**ÔºöÂú®‚ÄúÂçïËáÇÊú∫Âô®‰∫∫‚Äù‰∏äÔºåÂæóÂàÜ‰ªé **0.464** ÊèêÂçáËá≥ **0.526**ÔºõÂú®‚ÄúÂõõË∂≥Êú∫Âô®‰∫∫‚Äù‰∏äÔºå‰ªé **0.595** ÊèêÂçáËá≥ **0.639**„ÄÇ\n            *   **ÊÄª‰ΩìÂπ≥ÂùáÂàÜ**Ôºö‰ªé **0.399** ÊèêÂçáËá≥ **0.446**„ÄÇ\n        *   **Wan2.2_5BÊ®°Âûã**‰πüÊòæÁ§∫Âá∫Á±ª‰ººÁöÑÂÖ®Èù¢ÊèêÂçáË∂ãÂäø„ÄÇ\n\n3.  **ËØÑ‰º∞ÊåáÊ†á‰∏é‰∫∫Á±ªÂÅèÂ•ΩÈ´òÂ∫¶‰∏ÄËá¥**\n    *   ÈÄöËøá‰∫∫Â∑•ÂÅèÂ•ΩÁ†îÁ©∂ÔºåËÆ∫ÊñáËÆ°ÁÆó‰∫ÜRBenchËá™Âä®ËØÑÂàÜ‰∏é‰∫∫Á±ªËØÑÂàÜ‰πãÈó¥ÁöÑ**ÊñØÁöÆÂ∞îÊõºÁ≠âÁ∫ßÁõ∏ÂÖ≥Á≥ªÊï∞**ÔºåËææÂà∞‰∫Ü **0.96**ÔºåËØÅÊòé‰∫ÜÂÖ∂ÊèêÂá∫ÁöÑËá™Âä®ÂåñËØÑ‰º∞ÊåáÊ†áËÉΩÂ§üÊúâÊïàÂèçÊò†‰∫∫Á±ªÁöÑ‰∏ªËßÇÂà§Êñ≠„ÄÇ\n\n### ÊÄªÁªì\nËÆ∫ÊñáÈÄöËøáÊûÑÂª∫**RBenchËØÑ‰º∞Âü∫ÂáÜ**Âíå**RoVid-XËÆ≠ÁªÉÊï∞ÊçÆÈõÜ**ÔºåÂª∫Á´ã‰∫Ü‰∏Ä‰∏™‰ªéËØÑ‰º∞Âà∞ËÆ≠ÁªÉÁöÑÂÆåÊï¥ÁîüÊÄÅÁ≥ªÁªü„ÄÇÂÆûÈ™åË°®ÊòéÔºö\n*   **ÂΩìÂâçÊúÄÂÖàËøõÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®ÁîüÊàêÁâ©ÁêÜÁúüÂÆûÁöÑÊú∫Âô®‰∫∫Ë°å‰∏∫ÊñπÈù¢‰ªçÊúâÊòæËëó‰∏çË∂≥**ÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÂ§çÊùÇÊé®ÁêÜÂíåÁ≤æÁªÜÊìç‰ΩúÁöÑ‰ªªÂä°‰∏ä„ÄÇ\n*   **‰ΩøÁî®È´òË¥®Èáè„ÄÅÂ§öÊ†∑ÂåñÁöÑÊú∫Âô®‰∫∫ËßÜÈ¢ëÊï∞ÊçÆÔºàRoVid-XÔºâËøõË°åÂæÆË∞ÉÔºåÂèØ‰ª•Á®≥ÂÆöÊèêÂçáÊ®°ÂûãÂú®ÂêÑÈ°πÊú∫Âô®‰∫∫‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ**„ÄÇ\n*   ËÆ∫ÊñáÊèêÂá∫ÁöÑ**ËØÑ‰º∞ÊåáÊ†á‰ΩìÁ≥ª**‰∏ç‰ªÖËá™Âä®ÂåñÁ®ãÂ∫¶È´òÔºåËÄå‰∏î‰∏é‰∫∫Á±ªËØÑ‰ª∑È´òÂ∫¶‰∏ÄËá¥Ôºå‰∏∫Êú∫Âô®‰∫∫ËßÜÈ¢ëÁîüÊàêÈ¢ÜÂüüÁöÑÊ®°ÂûãÂèëÂ±ïÂíåÊØîËæÉÊèê‰æõ‰∫ÜÂèØÈù†„ÄÅÂèØÂ§çÁé∞ÁöÑË°°ÈáèÊ†áÂáÜ„ÄÇ",
            "timestamp": "2026-01-27T16:07:05.585809",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ËÆ∫ÊñáÁöÑ‰∏ªË¶ÅÂ±ÄÈôêÊÄßÊúâÂì™‰∫õÔºüÊú™Êù•ÂèØËÉΩÁöÑÊîπËøõÊñπÂêëÊòØ‰ªÄ‰πàÔºü",
            "answer": "## ËÆ∫Êñá‰∏ªË¶ÅÂ±ÄÈôêÊÄßÂàÜÊûê\n\nÂü∫‰∫éÂØπËÆ∫ÊñáÂÜÖÂÆπÁöÑÂàÜÊûêÔºåÂÖ∂ÊèêÂá∫ÁöÑ**RBench**Âü∫ÂáÜÂíå**RoVid-X**Êï∞ÊçÆÈõÜËôΩÁÑ∂‰∏∫ÂÖ∑Ë∫´Êô∫ËÉΩÈ¢ÜÂüüÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãËØÑ‰º∞‰∏éËÆ≠ÁªÉÊèê‰æõ‰∫ÜÈáçË¶ÅÂü∫Á°ÄÔºå‰ΩÜÁ†îÁ©∂Êú¨Ë∫´‰ªçÂ≠òÂú®‰∏Ä‰∫õÂ±ÄÈôêÊÄßÔºå‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰ª•‰∏ãÂá†‰∏™ÊñπÈù¢Ôºö\n\n### 1. ËØÑ‰º∞Âü∫ÂáÜÁöÑÂ±ÄÈôêÊÄß\n- **Ëá™Âä®ÂåñËØÑ‰º∞ÁöÑ‰æùËµñ‰∏éÂÅèÂ∑Æ**ÔºöRBenchÁöÑÊ†∏ÂøÉËØÑ‰º∞ÊåáÊ†á‰∏•Èáç‰æùËµñÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàÂ¶ÇGPT-5„ÄÅQwen3-VLÔºâ‰Ωú‰∏∫‚ÄúÈõ∂Ê†∑Êú¨ËØÑ‰º∞Âô®‚Äù„ÄÇËôΩÁÑ∂ËÆ∫ÊñáÊä•Âëä‰∫Ü‰∏é‰∫∫Á±ªËØÑ‰º∞ÁöÑÈ´òÁõ∏ÂÖ≥ÊÄßÔºàSpearman œÅ=0.96ÔºâÔºå‰ΩÜMLLMËá™Ë∫´ÁöÑÂÅèËßÅ„ÄÅÂØπÁâ©ÁêÜÂ∏∏ËØÜÁêÜËß£ÁöÑÂ±ÄÈôêÊÄßÔºå‰ª•ÂèäÊèêÁ§∫ËØçÂ∑•Á®ãÔºàPrompt EngineeringÔºâÁöÑÊïèÊÑüÊÄßÔºåÈÉΩÂèØËÉΩÂºïÂÖ•Á≥ªÁªüÊÄßËØØÂ∑Æ„ÄÇËØÑ‰º∞ÁªìÊûúÂèØËÉΩÊó†Ê≥ïÂÆåÂÖ®ÊçïÊçâ‰∫∫Á±ªÂú®Âà§Êñ≠Áâ©ÁêÜÂêàÁêÜÊÄßÂíå‰ªªÂä°ÂÆåÊàêÂ∫¶Êó∂ÁöÑÁªÜÂæÆÂ∑ÆÂà´„ÄÇ\n- **ËØÑ‰º∞ËåÉÂõ¥ÁöÑÊúâÈôêÊÄß**ÔºöRBenchÂåÖÂê´‰∫Ü5Á±ª‰ªªÂä°Âíå4ÁßçÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÔºåÂÖ±ËÆ°650‰∏™ËØÑ‰º∞Ê†∑Êú¨„ÄÇÂ∞ΩÁÆ°ÂÖ∑ÊúâÂ§öÊ†∑ÊÄßÔºå‰ΩÜÂØπ‰∫éÊ∂µÁõñÊó†ÈôêÂèØËÉΩÁöÑÁúüÂÆûÊú∫Âô®‰∫∫‰∫§‰∫í‰∏ñÁïåËÄåË®ÄÔºåÂÖ∂ËßÑÊ®°ÂíåÂú∫ÊôØË¶ÜÁõñÂ∫¶‰ªçÁÑ∂ÊúâÈôê„ÄÇÂÆÉÂèØËÉΩÊó†Ê≥ïÂÖÖÂàÜËØÑ‰º∞Ê®°ÂûãÂú®Êõ¥Â§çÊùÇ„ÄÅÊõ¥ÁΩïËßÅÊàñÊ∂âÂèäÂ§öÊ®°ÊÄÅÂèçÈ¶àÔºàÂ¶ÇÂäõËßâ„ÄÅËß¶ËßâÔºâÂú∫ÊôØ‰∏ãÁöÑË°®Áé∞„ÄÇ\n- **Áº∫‰πèÂØπÁîüÊàêÂä®‰Ωú‚ÄúÂèØÊâßË°åÊÄß‚ÄùÁöÑËØÑ‰º∞**ÔºöÂΩìÂâçËØÑ‰º∞‰∏ªË¶ÅÂÖ≥Ê≥®ËßÜÈ¢ëÁöÑ‚ÄúËßÇÁúã‚ÄùË¥®ÈáèÔºàËßÜËßâ‰øùÁúüÂ∫¶„ÄÅÁâ©ÁêÜÂêàÁêÜÊÄß„ÄÅ‰ªªÂä°‰∏ÄËá¥ÊÄßÔºâÔºå‰ΩÜÊú™Áõ¥Êé•ËØÑ‰º∞ÁîüÊàêÁöÑÂä®‰ΩúÂ∫èÂàóÊòØÂê¶ËÉΩÂ§üË¢´ÁúüÂÆûÁöÑÊú∫Âô®‰∫∫Á°¨‰ª∂ÂÆâÂÖ®„ÄÅÁ®≥ÂÆöÂú∞ÊâßË°å„ÄÇËøôÂøΩÁï•‰∫Ü‰ªé‚ÄúËßÜÈ¢ëÊ®°Êãü‚ÄùÂà∞‚ÄúÂèØÊâßË°åÁ≠ñÁï•‚ÄùÁöÑÂÖ≥ÈîÆËΩ¨ÂåñÊ≠•È™§„ÄÇ\n\n### 2. Êï∞ÊçÆÈõÜÁöÑÂ±ÄÈôêÊÄß\n- **Êï∞ÊçÆÊù•Ê∫ê‰∏éË¥®ÈáèÁöÑ‰∏çÂùáÊÄß**ÔºöRoVid-XÊï¥Âêà‰∫ÜË∂ÖËøá20‰∏™ÂºÄÊ∫êÊï∞ÊçÆÈõÜÂíåÁΩëÁªúËßÜÈ¢ëÔºåËôΩÁÑ∂ÈÄöËøáÂõõÈò∂ÊÆµÊµÅÊ∞¥Á∫øËøõË°å‰∫ÜËøáÊª§ÂíåÊ†áÊ≥®Ôºå‰ΩÜÂéüÂßãÊï∞ÊçÆÂú®ÂàÜËæ®Áéá„ÄÅÊãçÊëÑËßÜËßí„ÄÅ‰ªªÂä°ËßÑËåÉÊÄßÂíåÂô™Â£∞Ê∞¥Âπ≥‰∏äÂ≠òÂú®Â∑ÆÂºÇ„ÄÇËøôÁßçÂºÇË¥®ÊÄßÂèØËÉΩÂØπÊ®°ÂûãËÆ≠ÁªÉ‰∫ßÁîü‰∏çÂèØÈ¢ÑÊµãÁöÑÂΩ±ÂìçÔºåÂπ∂Èöæ‰ª•‰øùËØÅÊâÄÊúâÊï∞ÊçÆÁâáÊÆµÈÉΩËææÂà∞ÂêåÁ≠âÁöÑÈ´òÁâ©ÁêÜ‰øùÁúüÂ∫¶Ê†áÂáÜ„ÄÇ\n- **Ê†áÊ≥®ÁöÑËá™Âä®Âåñ‰∏éÁ≤æÂ∫¶Áì∂È¢à**ÔºöÁâ©ÁêÜÂ±ûÊÄßÊ†áÊ≥®ÔºàÂ¶ÇÂÖâÊµÅ„ÄÅÊ∑±Â∫¶ÂõæÔºâ‰æùËµñ‰∫éFlashVSR„ÄÅAllTracker„ÄÅVideo Depth AnythingÁ≠âËá™Âä®ÂåñÂ∑•ÂÖ∑„ÄÇËøô‰∫õÂ∑•ÂÖ∑Âú®Â§çÊùÇÂä®ÊÄÅÂú∫ÊôØ„ÄÅÈÅÆÊå°ÊàñÂø´ÈÄüËøêÂä®‰∏ãÂèØËÉΩ‰∫ßÁîüÈîôËØØÊàñÂô™Â£∞Ôºå‰ªéËÄåÈôêÂà∂‰∫ÜÊ†áÊ≥®Êï∞ÊçÆÁöÑÁªùÂØπÁ≤æÂ∫¶ÂíåÂèØÈù†ÊÄß„ÄÇ\n- **‚Äú‰ªøÁúü-Áé∞ÂÆû‚ÄùÈ∏øÊ≤üÁöÑÈÅóÁïô**ÔºöÂ∞ΩÁÆ°RoVid-XÊó®Âú®Êèê‰æõÁâ©ÁêÜ‰∫§‰∫íÂÖàÈ™åÔºå‰ΩÜÂÆÉÊú¨Ë¥®‰∏ä‰ªçÊòØËßÇÂØüÊÄßËßÜÈ¢ëÊï∞ÊçÆÁöÑÈõÜÂêàÔºåËÄåÈùûÂú®ÂÖ∑ÊúâÁ≤æÁ°ÆÁâ©ÁêÜÂºïÊìéÁöÑ‰ªøÁúüÁéØÂ¢É‰∏≠ÁîüÊàêÁöÑ„ÄÅÈôÑÂ∏¶ÂÆåÊï¥Áä∂ÊÄÅÂíåÂä®‰ΩúÊï∞ÊçÆÁöÑÊï∞ÊçÆ„ÄÇÂõ†Ê≠§ÔºåÊ®°Âûã‰ªé‰∏≠Â≠¶Âà∞ÁöÑÁâ©ÁêÜËßÑÂæãÂèØËÉΩÊòØ‰∏çÂÆåÊï¥ÊàñËøë‰ººÁöÑÔºåÈöæ‰ª•Ê≥õÂåñÂà∞ÂÖ®Êñ∞ÁöÑÁâ©ÁêÜÈÖçÁΩÆÊàñÊûÅÁ´Ø‰∫§‰∫í‰∏≠„ÄÇ\n\n### 3. Ê®°ÂûãËØÑ‰º∞ÁªìËÆ∫ÁöÑÂ±ÄÈôêÊÄß\n- **ÈùôÊÄÅÂø´ÁÖßËØÑ‰º∞**ÔºöËÆ∫ÊñáÂØπ25‰∏™Ê®°ÂûãÁöÑËØÑ‰º∞ÊòØ‰∏Ä‰∏™‚ÄúÈùôÊÄÅ‚ÄùÊØîËæÉÔºåÂèçÊò†‰∫ÜÁâπÂÆöÊó∂Èó¥ÁÇπÊ®°ÂûãÁöÑÊÄßËÉΩÊéíÂêç„ÄÇËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÈ¢ÜÂüüÂèëÂ±ïËøÖÈÄüÔºåÊ®°ÂûãÁöÑËø≠‰ª£ÂíåÊõ¥Êñ∞ÂèØËÉΩÂæàÂø´ÊîπÂèòËøô‰∏ÄÊ†ºÂ±Ä„ÄÇÊ≠§Â§ñÔºåËØÑ‰º∞Êú™Ê∑±ÂÖ•Êé¢Á©∂‰∏çÂêåÊ®°ÂûãÊû∂ÊûÑÔºàÂ¶ÇDiffusion Transformer, MoEÔºâ‰∏éÊÄßËÉΩÂ∑ÆÂºÇ‰πãÈó¥ÁöÑÂõ†ÊûúÂÖ≥Á≥ª„ÄÇ\n- **ÂØπÂ§±Ë¥•Ê®°ÂºèÊ†πÊ∫êÂàÜÊûê‰∏çË∂≥**ÔºöËÆ∫ÊñáÊåáÂá∫‰∫ÜÊ®°ÂûãÂú®ËßÜËßâÊé®ÁêÜ„ÄÅÁ≤æÁªÜÊìç‰ΩúÁ≠âÊñπÈù¢ÁöÑÁì∂È¢àÔºå‰ΩÜÂØπÂÖ∂Ê†πÊú¨ÂéüÂõ†ÔºàÂ¶ÇËÆ≠ÁªÉÊï∞ÊçÆÂÅèÂ∑Æ„ÄÅÊ®°ÂûãÂÆπÈáèÈôêÂà∂„ÄÅÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Áº∫Èô∑ÔºâÁöÑÂàÜÊûêÁõ∏ÂØπË°®Èù¢ÔºåÊõ¥Â§öÊòØÁé∞Ë±°ÊèèËø∞ËÄåÈùûÊú∫ÁêÜÊé¢Á©∂„ÄÇ\n\n---\n\n## Êú™Êù•ÂèØËÉΩÁöÑÊîπËøõÊñπÂêë\n\nËÆ∫ÊñáÂú®ÁªìËÆ∫ÂíåÊú™Êù•Â∑•‰ΩúÈÉ®ÂàÜÂ∑≤ÊèêÂá∫‰∏Ä‰∫õÊñπÂêëÔºåÁªìÂêàÂÖ∂Â±ÄÈôêÊÄßÔºåÂèØÂΩíÁ∫≥Âá∫‰ª•‰∏ãÊõ¥ÂÖ∑‰ΩìÁöÑÊîπËøõË∑ØÂæÑÔºö\n\n### 1. ËØÑ‰º∞‰ΩìÁ≥ªÁöÑÊ∑±Âåñ‰∏éÊãìÂ±ï\n- **ÂºÄÂèëÊõ¥È≤ÅÊ£í„ÄÅÊõ¥Áâ©ÁêÜÂåñÁöÑËá™Âä®ËØÑ‰º∞ÊåáÊ†á**ÔºöÂáèÂ∞ëÂØπÈªëÁõíMLLMÁöÑ‰æùËµñÔºåÊé¢Á¥¢ÁªìÂêà**Áâ©ÁêÜÂºïÊìé‰ªøÁúüÈ™åËØÅ**ÁöÑËØÑ‰º∞ÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂ∞ÜÁîüÊàêÁöÑËßÜÈ¢ëËΩ®ËøπËß£Êûê‰∏∫Êú∫Âô®‰∫∫ÂÖ≥ËäÇÁä∂ÊÄÅÂ∫èÂàóÔºåÂú®‰ªøÁúü‰∏≠ÈáçÊîæÂπ∂Ê£ÄÊü•ÂÖ∂Âä®ÂäõÂ≠¶ÂèØË°åÊÄß„ÄÅÁ¢∞ÊíûÊÉÖÂÜµ‰ª•ÂèäÊúÄÁªàÊòØÂê¶ËææÊàê‰ªªÂä°ÁõÆÊ†á„ÄÇ\n- **ÊûÑÂª∫ÂàÜÂ±Ç„ÄÅÂèØÊâ©Â±ïÁöÑÂü∫ÂáÜ**ÔºöÂª∫Á´ãÊ∂µÁõñ‰ªéÁÆÄÂçïÁâ©ÁêÜËßÑÂàôÔºàÁâ©‰ΩìÂù†ËêΩ„ÄÅÁ¢∞ÊíûÔºâÂà∞Â§çÊùÇÈïøÁ®ã‰ªªÂä°ËßÑÂàíÁöÑ**ÂàÜÂ±ÇËØÑ‰º∞‰ΩìÁ≥ª**„ÄÇÂêåÊó∂ÔºåËÆæËÆ°Âü∫ÂáÜÁöÑ**ÂèØÊâ©Â±ïÊú∫Âà∂**ÔºåÂÖÅËÆ∏Á§æÂå∫ÊåÅÁª≠Ë¥°ÁåÆÊñ∞ÁöÑ„ÄÅÊõ¥ÂÖ∑ÊåëÊàòÊÄßÁöÑÊµãËØïÊ°à‰æã„ÄÇ\n- **ÂºïÂÖ•‚ÄúÁ≠ñÁï•ÂèØÊâßË°åÊÄß‚ÄùËØÑ‰º∞Áª¥Â∫¶**Ôºö‰∏éÈÄÜÂä®ÂäõÂ≠¶Ê®°ÂûãÔºàIDMÔºâÊàñÊäÄËÉΩÊèêÂèñÊ®°ÂûãÁªìÂêàÔºåËØÑ‰º∞‰ªéÁîüÊàêËßÜÈ¢ë‰∏≠ÊèêÂèñÂá∫ÁöÑÂä®‰ΩúÊåá‰ª§Âú®‰ªøÁúüÊàñÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÁöÑÊâßË°åÊàêÂäüÁéáÂíåÊïàÁéáÔºåÁúüÊ≠£ËøûÊé•ÁîüÊàê‰∏éÊâßË°å„ÄÇ\n\n### 2. Êï∞ÊçÆ‰∏éËÆ≠ÁªÉÊñπÊ≥ïÁöÑÈù©Êñ∞\n- **‰ªøÁúü‰∏éÁúüÂÆûÊï∞ÊçÆËûçÂêà**ÔºöÊûÑÂª∫**Ê∑∑ÂêàÊï∞ÊçÆÊµÅÊ∞¥Á∫ø**ÔºåÂ∞ÜÈ´ò‰øùÁúüÁâ©ÁêÜ‰ªøÁúüÁîüÊàêÁöÑÁªìÊûÑÂåñÊï∞ÊçÆÔºàÁ≤æÁ°ÆÁä∂ÊÄÅ„ÄÅÂä®‰Ωú„ÄÅÂ•ñÂä±Ôºâ‰∏éRoVid-XÁ≠âÁúüÂÆû‰∏ñÁïåËßÜÈ¢ëÊï∞ÊçÆÁõ∏ÁªìÂêà„ÄÇ‰ªøÁúüÊï∞ÊçÆÂèØ‰ª•Êèê‰æõÊó†Âô™Â£∞„ÄÅÂÖ®Ë¶ÜÁõñÁöÑÁâ©ÁêÜ‰∫§‰∫íËåÉ‰æãÔºåËÄåÁúüÂÆûÊï∞ÊçÆÂàôÊèê‰æõÂ§ñËßÇÂ§öÊ†∑ÊÄßÂíåÁé∞ÂÆûÂô™Â£∞Ôºå‰∫åËÄÖ‰∫íË°•„ÄÇ\n- **ÂèëÂ±ïÁâ©ÁêÜÂºïÂØºÁöÑËßÜÈ¢ëÁîüÊàêÊ®°Âûã**ÔºöÂú®Ê®°ÂûãÊû∂ÊûÑÊàñËÆ≠ÁªÉÁõÆÊ†á‰∏≠ÊòæÂºèÂú∞ÂºïÂÖ•**Áâ©ÁêÜÁ∫¶Êùü**„ÄÇ‰æãÂ¶ÇÔºåÂú®Êâ©Êï£ËøáÁ®ã‰∏≠Âä†ÂÖ•Âü∫‰∫éÁâ©ÁêÜÂÆöÂæãÁöÑÂºïÂØºÈ°πÔºåÊàñËÆ≠ÁªÉ‰∏Ä‰∏™ËæÖÂä©ÁöÑÁâ©ÁêÜÂêàÁêÜÊÄßÂà§Âà´Âô®Êù•ÂçèÂêåËÆ≠ÁªÉÁîüÊàêÊ®°Âûã„ÄÇ\n- **Êé¢Á¥¢Êõ¥È´òÊïàÁöÑÊï∞ÊçÆË°®Á§∫‰∏éÂà©Áî®ÊñπÂºè**ÔºöÁ†îÁ©∂Â¶Ç‰ΩïÊõ¥Â•ΩÂú∞Âà©Áî®RoVid-X‰∏≠ÁöÑÂ§öÊ®°ÊÄÅÊ†áÊ≥®ÔºàÂÖâÊµÅ„ÄÅÊ∑±Â∫¶Ôºâ„ÄÇ‰æãÂ¶ÇÔºåËÆæËÆ°Ê®°ÂûãÂêåÊó∂Â≠¶‰π†ÁîüÊàêRGBËßÜÈ¢ëÂíåÂØπÂ∫îÁöÑÊ∑±Â∫¶ÊàñÂÖâÊµÅÂú∫Ôºå‰ª•ÂÜÖÈÉ®Âº∫ÂåñÂÖ∂Âá†‰Ωï‰∏éËøêÂä®ÁêÜËß£„ÄÇ\n\n### 3. ËøàÂêëÈó≠ÁéØ‰∏éÂÖ∑Ë∫´Êô∫ËÉΩ\n- **ÂÆûÁé∞‚ÄúËßÜÈ¢ëÁîüÊàê-Á≠ñÁï•ÊèêÂèñ-Èó≠ÁéØÊéßÂà∂‚ÄùÁöÑÂÆåÊï¥ÈìæË∑Ø**ÔºöËøôÊòØËÆ∫ÊñáÊòéÁ°ÆÊåáÂá∫ÁöÑÊ†∏ÂøÉÊú™Êù•ÊñπÂêë„ÄÇÈúÄË¶ÅÊ∑±ÂÖ•Á†îÁ©∂Â¶Ç‰Ωï‰ªéÁîüÊàêÁöÑËßÜÈ¢ë‰∏≠**Á®≥ÂÅ•Âú∞ÊèêÂèñÂèØÊâßË°åÁöÑÂä®‰ΩúË°®Á§∫**ÔºàÈÄöËøáÊîπËøõÁöÑIDMÊàñÊΩúÂú®Âä®‰ΩúÊ®°ÂûãÔºâÔºåÂπ∂Â∞ÜÊ≠§ÊµÅÁ®ã‰∏éÊú∫Âô®‰∫∫Á≠ñÁï•Â≠¶‰π†Ê°ÜÊû∂ÔºàÂ¶ÇÂº∫ÂåñÂ≠¶‰π†„ÄÅÊ®°‰ªøÂ≠¶‰π†ÔºâÈó≠ÁéØÈõÜÊàêÔºåÂú®‰ªøÁúüÂíåÁé∞ÂÆû‰∏≠ËøõË°åÁ´ØÂà∞Á´ØÈ™åËØÅ„ÄÇ\n- **ÂºÄÂèëÈù¢ÂêëÊéßÂà∂ÁöÑËßÜÈ¢ë‰∏ñÁïåÊ®°Âûã**ÔºöÂ∞ÜËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÊâ©Â±ï‰∏∫**ÂèØ‰∫§‰∫íÁöÑ‰∏ñÁïåÊ®°Âûã**Ôºå‰∏ç‰ªÖËÉΩÂ§üÊ†πÊçÆÊåá‰ª§ÁîüÊàêËßÜÈ¢ëÔºåËøòËÉΩÊ®°ÊãüÊú∫Âô®‰∫∫ÊâßË°åÁâπÂÆöÂä®‰ΩúÂêéÁöÑÊú™Êù•Áä∂ÊÄÅÔºåÁî®‰∫éÂü∫‰∫éÊ®°ÂûãÁöÑËßÑÂàí‰∏éÁ≠ñÁï•Â≠¶‰π†„ÄÇ\n- **ÂÖ≥Ê≥®Ê≥õÂåñ‰∏éÈÄÇÂ∫îËÉΩÂäõ**ÔºöÁ†îÁ©∂Â¶Ç‰Ωï‰ΩøÊ®°ÂûãËÉΩÂ§üÂø´ÈÄüÈÄÇÂ∫îÊñ∞ÁöÑÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ„ÄÅÊñ∞ÁöÑÁâ©‰ΩìÊàñÊñ∞ÁöÑÁéØÂ¢É„ÄÇËøôÂèØËÉΩÊ∂âÂèäÂÖÉÂ≠¶‰π†„ÄÅÂü∫‰∫éÊèêÁ§∫ÁöÑË∞ÉÊï¥ÊàñÊõ¥ÁÅµÊ¥ªÁöÑÂ§öÊ®°ÊÄÅÁºñÁ†ÅÂô®ËÆæËÆ°„ÄÇ\n\n### 4. ÂºÄÊîæÁßëÂ≠¶‰∏éÁîüÊÄÅÂª∫ËÆæ\n- **ÊåÅÁª≠Áª¥Êä§‰∏éÊõ¥Êñ∞Âü∫ÂáÜ‰∏éÊï∞ÊçÆÈõÜ**ÔºöÂª∫Á´ãÊú∫Âà∂Ôºå‰ΩøRBenchÂíåRoVid-XËÉΩÂ§üÈöèÁùÄÈ¢ÜÂüüÂèëÂ±ïËÄåËø≠‰ª£Êõ¥Êñ∞ÔºåÁ∫≥ÂÖ•Êñ∞ÁöÑÊåëÊàò‰ªªÂä°ÂíåÊõ¥È´òË¥®ÈáèÁöÑÊï∞ÊçÆÔºå‰øùÊåÅÂÖ∂Áõ∏ÂÖ≥ÊÄßÂíåÂâçÊ≤øÊÄß„ÄÇ\n- **Ê∑±ÂÖ•ÁöÑÂºÄÊ∫ê‰∏éÂàÜÊûê**ÔºöÈô§‰∫ÜÂèëÂ∏ÉÂü∫ÂáÜÂíåÊï∞ÊçÆÈõÜÔºåËøõ‰∏ÄÊ≠•ÂºÄÊ∫êËØ¶ÁªÜÁöÑÊ®°ÂûãËØÑ‰º∞Êó•Âøó„ÄÅÂ§±Ë¥•Ê°à‰æãÂàÜÊûêÂíåÊ∂àËûçÂÆûÈ™å‰ª£Á†ÅÔºåÂ∏ÆÂä©Á§æÂå∫Êõ¥Ê∑±ÂÖ•Âú∞ÁêÜËß£Ê®°ÂûãË°å‰∏∫ÁöÑÂÜ≥ÂÆöÂõ†Á¥†ÔºåÊé®Âä®Ê†πÊú¨ÊÄßÂàõÊñ∞„ÄÇ\n\n**ÊÄªÁªìËÄåË®ÄÔºåËØ•ËÆ∫ÊñáÁöÑÂ±ÄÈôêÊÄß‰∏ªË¶ÅÂú®‰∫éËØÑ‰º∞ÊñπÊ≥ïÁöÑ‰æùËµñÊÄß‰∏éÊï∞ÊçÆÈõÜÁöÑÂõ∫ÊúâËæπÁïåÔºåËÄåÂÖ∂ÊúÄÂ§ßÁöÑ‰ª∑ÂÄºÂú®‰∫éÊ∏ÖÊô∞Âú∞Êè≠Á§∫‰∫ÜÂΩìÂâçËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰∏éÂÖ∑Ë∫´Êô∫ËÉΩÈúÄÊ±Ç‰πãÈó¥ÁöÑ‚ÄúÁâ©ÁêÜÈ∏øÊ≤ü‚Äù„ÄÇÊú™Êù•ÁöÑÊîπËøõÂ∞ÜÂõ¥ÁªïÊûÑÂª∫Êõ¥‰∏•Ë∞®ÁöÑËØÑ‰º∞‰ΩìÁ≥ª„ÄÅÂàõÈÄ†ËûçÂêà‰ªøÁúü‰∏éÁúüÂÆûÁöÑÈ´òË¥®ÈáèÊï∞ÊçÆ„ÄÅÂú®Ê®°Âûã‰∏≠ÂµåÂÖ•Áâ©ÁêÜÂÖàÈ™åÔºåÂπ∂ÊúÄÁªàÊâìÈÄö‰ªéËßÜÈ¢ëÁîüÊàêÂà∞Êú∫Âô®‰∫∫Âä®‰ΩúÊâßË°åÁöÑÂÆåÊï¥Èó≠ÁéØÔºåËøôÊòØÂÆûÁé∞ÈÄöÁî®Áâ©ÁêÜAIÁöÑÂÖ≥ÈîÆË∑ØÂæÑ„ÄÇ**",
            "timestamp": "2026-01-27T16:08:05.393198",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T18:59:18Z",
    "created_at": "2026-01-27T15:53:25.143637",
    "updated_at": "2026-01-27T15:53:25.143644"
}