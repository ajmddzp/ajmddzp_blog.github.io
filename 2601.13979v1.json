{
    "id": "2601.13979v1",
    "title": "Active Cross-Modal Visuo-Tactile Perception of Deformable Linear Objects",
    "authors": [
        "Raffaele Mazza",
        "Ciro Natale",
        "Pietro Falco"
    ],
    "abstract": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¨æ¨¡æ€è§†è§‰-è§¦è§‰æ„ŸçŸ¥æ¡†æ¶ï¼Œç”¨äºå¯å˜å½¢çº¿æ€§ç‰©ä½“çš„ä¸‰ç»´å½¢çŠ¶é‡å»ºï¼Œç‰¹åˆ«å…³æ³¨å­˜åœ¨ä¸¥é‡è§†è§‰é®æŒ¡çš„çº¿ç¼†åœºæ™¯ã€‚ä¸ç°æœ‰ä¸»è¦ä¾èµ–è§†è§‰çš„æ–¹æ³•ä¸åŒâ€”â€”è¿™äº›æ–¹æ³•åœ¨å…‰ç…§å˜åŒ–ã€èƒŒæ™¯æ‚ä¹±æˆ–å±€éƒ¨å¯è§æ€§å·®æ—¶æ€§èƒ½ä¼šä¸‹é™ï¼Œæœ¬æ–¹æ³•å°†åŸºäºåŸºç¡€æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥ä¸è‡ªé€‚åº”è§¦è§‰æ¢ç´¢ç›¸ç»“åˆã€‚è§†è§‰å¤„ç†æµç¨‹åˆ©ç”¨SAMè¿›è¡Œå®ä¾‹åˆ†å‰²ï¼ŒFlorenceè¿›è¡Œè¯­ä¹‰ç»†åŒ–ï¼Œéšåè¿›è¡Œéª¨æ¶åŒ–ã€ç«¯ç‚¹æ£€æµ‹å’Œç‚¹äº‘æå–ã€‚è¢«é®æŒ¡çš„çº¿ç¼†æ®µé€šè¿‡è§¦è§‰ä¼ æ„Ÿå™¨è‡ªä¸»è¯†åˆ«å¹¶æ¢ç´¢ï¼Œè¯¥ä¼ æ„Ÿå™¨æä¾›å±€éƒ¨ç‚¹äº‘ï¼Œå¹¶é€šè¿‡æ¬§å‡ é‡Œå¾—èšç±»å’Œæ‹“æ‰‘ä¿æŒèåˆä¸è§†è§‰æ•°æ®åˆå¹¶ã€‚åŸºäºç«¯ç‚¹å¼•å¯¼çš„ç‚¹æ’åºé©±åŠ¨çš„Bæ ·æ¡æ’å€¼ï¼Œå®ç°äº†çº¿ç¼†å½¢çŠ¶çš„å¹³æ»‘å®Œæ•´é‡å»ºã€‚ä½¿ç”¨é…å¤‡RGB-Dç›¸æœºå’Œè§¦è§‰å«çš„æœºå™¨äººæ“çºµå™¨è¿›è¡Œçš„å®éªŒéªŒè¯è¡¨æ˜ï¼Œå³ä½¿å¤§éƒ¨åˆ†çº¿ç¼†è¢«é®æŒ¡ï¼Œæ‰€æå‡ºçš„æ¡†æ¶ä¹Ÿèƒ½å‡†ç¡®é‡å»ºç®€å•å’Œé«˜å¼¯æ›²åº¦çš„å•æ ¹æˆ–å¤šæ ¹çº¿ç¼†å½¢æ€ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åŸºç¡€æ¨¡å‹å¢å¼ºçš„è·¨æ¨¡æ€æ„ŸçŸ¥åœ¨æ¨è¿›æœºå™¨äººå¯å˜å½¢ç‰©ä½“æ“æ§æ–¹é¢çš„æ½œåŠ›ã€‚",
    "url": "https://arxiv.org/abs/2601.13979v1",
    "html_url": "https://arxiv.org/html/2601.13979v1",
    "html_content": "Active Cross-Modal Visuo-Tactile Perception\nof Deformable Linear Objects\nRaffaele Mazza, Ciro Natale, and Pietro Falco\nRaffaele Mazza and Ciro Natale are with Dipartimento di Ingegneria,\nUniversitÃ  degli Studi della Campania Luigi Vanvitelli, 81031 Aversa (CE), Italy, Pietro Falco is with Dipartimento di Ingegneria dellâ€™Informazione, UniversitÃ  degli Studi di Padova, 35122 Padova, Italy.\n(Corr. author: R. Mazza\nraffaele.mazza@unicampania.it\n)\nAbstract\nThis paper presents a novel cross-modal visuoâ€“tactile perception framework for the 3D shape reconstruction of deformable linear objects (DLOs), with a specific focus on cables subject to severe visual occlusions. Unlike existing methods relying predominantly on vision, whose performance degrades under varying illumination, background clutter, or partial visibility, the proposed approach integrates foundation-model-based visual perception with adaptive tactile exploration. The visual pipeline exploits SAM for instance segmentation and Florence for semantic refinement, followed by skeletonization, endpoint detection, and point-cloud extraction. Occluded cable segments are autonomously identified and explored with a tactile sensor, which provides local point clouds that are merged with the visual data through Euclidean clustering and topology-preserving fusion. A B-spline interpolation driven by endpoint-guided point sorting yields a smooth and complete reconstruction of the cable shape. Experimental validation using a robotic manipulator equipped with an RGB-D camera and a tactile pad demonstrates that the proposed framework accurately reconstructs both simple and highly curved single or multiple cable configurations, even when large portions are occluded. These results highlight the potential of foundation-model-enhanced cross-modal perception for advancing robotic manipulation of deformable objects.\nI\nIntroduction\nRobotic manipulation of deformable linear objects (DLOs), such as cables, hoses, and wires, is a fundamental capability for automation in manufacturing, logistics, and domestic environments. Tasks including wiring harness assembly, cable routing, and switchgear cabling require accurate knowledge of the object geometry during interaction. However, the elastic and highly deformable nature of DLOs makes their shape difficult to estimate reliably, particularly under realistic sensing conditions. Most existing approaches rely predominantly on visual perception, using RGB or RGB-D sensing to segment and reconstruct the object shape. While vision-based pipelines can achieve high accuracy in controlled settings, their performance degrades in the presence of occlusions, background clutter, illumination changes, or partial visibilityâ€”conditions that frequently occur in real industrial scenarios. As a result, visual perception alone is often insufficient to provide a complete and reliable representation of DLO geometry.\nFigure 1:\nBlock diagram of the proposed cross-modal visuo-tactile perception framework: visual perception is triggered by a natural language prompt for a state-of-the-art segmentation network; a clustering algorithm isolates different cables based on their colour; the segmented image is converted into point clouds and post-processed triggering the tactile exploration; finally a B-spline interpolation provides a model of the DLO.\nTactile sensing offers a complementary modality capable of capturing local geometric information even when visual data are unavailable. Prior work has shown that tactile feedback can significantly improve robustness during manipulation. However, tactile sensing is typically exploited only locally, after grasping, and is rarely integrated into a global shape reconstruction framework.\nInspired by human perception, which adaptively combines vision and touch based on local reliability, this paper proposes a cross-modal visuoâ€“tactile perception framework for global DLO shape reconstruction under partial observability. The basic idea was preliminary shown in the extended abstract\n[\n15\n]\n. The approach integrates foundation-model-based visual perception with autonomous tactile exploration to compensate for missing or unreliable visual information. Visual segmentation leverages state-of-the-art foundation models to extract a topology-aware representation of cable segments, while visually occluded regions are actively explored using a tactile sensor. The resulting visual and tactile point clouds are fused into a unified geometric model, yielding a smooth and continuous reconstruction of the DLO shape.\nThe proposed method is validated on a real robotic platform equipped with an RGB-D camera and a tactile sensor. Experimental results demonstrate reliable reconstruction of single and multiple cables under severe occlusions, inclined surfaces, and self-intersections, highlighting the effectiveness of foundation-model-enhanced cross-modal perception for deformable object manipulation.\nII\nRelated Work\nIn this section, we first review general surveys on deformable object manipulation, and then focus on deformable linear objects (DLOs) such as cables and wires. We finally position our contribution with respect to the existing works.\nII-\n1\nSurveys on Deformable Object Manipulation\nAn early comprehensive survey by SÃ¡nchez et al. review robotic manipulation and sensing of deformable objects across domestic and industrial applications, emphasizing the variety of sensing modalities and control strategies required compared to rigid objects\n[\n21\n]\n. Yin et al. provide a structured overview of modeling, learning, control, and planning methods for deformable object manipulation, highlighting the need to combine analytical models with data-driven approaches\n[\n29\n]\n.\nZhu et al. outline key challenges and outlooks in deformable object manipulation (DOM), stressing the need for integrated perceptionâ€“planningâ€“control pipelines and standardized benchmarks\n[\n31\n]\n. These works underline that, despite rapid progress, perception of deformable objects under occlusions and in cluttered industrial scenes remains a central bottleneck.\nII-\n2\nState Estimation of Deformable Linear Objects\nWithin DOM, DLOs such as cables and wires are especially relevant for industrial tasks such as wiring harness assembly. A first family of works focuses on visual segmentation and geometric reconstruction. Caporali et al. proposed FASTDLO, a fast instance-segmentation pipeline tailored to DLOs that combines synthetic training data and skeleton extraction for robust segmentation in RGB images\n[\n7\n]\n. Sun et al. extend this idea to 3D, presenting a robust pipeline from RGB-D segmentation to 3D reconstruction of DLOs that explicitly addresses sparse depth, self-occlusions, and background clutter\n[\n23\n]\n. Keipour et al. introduce a deformable one-dimensional object detector for routing and manipulation, focusing on robust detection of cables and hoses in cluttered scenes\n[\n14\n]\n.\nBeyond pure vision, several works utilise multi-view or temporal information for DLO state estimation. Xiang et al. introduce TrackDLO, which tracks DLOs under occlusion by enforcing motion coherence over a sequence of RGB-D frames, achieving real-time performance without requiring markers or simulation\n[\n26\n]\n. Huang et al. address the perception of multiple DLOs of unknown number in complex backgrounds and propose a deep-learning approach for untangling and unknotting from visual input\n[\n13\n]\n.\nLearning-based state estimation bridges the gap between perception and physics. Yan et al. propose a self-supervised pipeline that learns to estimate the state of linear deformable objects from images, combined with a differentiable dynamics model for model-predictive control\n[\n28\n]\n. Zhang et al. address robust vision-based control of DLOs in the presence of occlusions, coupling vision-based state estimation with feedback control\n[\n30\n]\n. These works provide important building blocks, but typically assume static camera viewpoints and do not explicitly optimize sensor poses to reduce occlusions.\nII-\n3\nActive Perception\nThe notion of active perception, formalized by Bajcsy as the intentional control of sensing to improve perception\n[\n1\n]\n, is particularly relevant for occlusion-heavy DLO scenarios. Breyer et al. presented a closed-loop NBV strategy for target-driven grasping, where future camera poses are selected based on grasp metrics and uncertainty\n[\n3\n]\n.\nIn the context of deformable objects, Weng et al. introduce Dynamic Active Vision Space (DAVS) for interactive perception of deformable objects, coupling camera motion with object manipulation in a sequential decision-making framework\n[\n25\n]\n.\nRecent work explores occlusion-aware active perception of tangled DLOs using Gaussian Splatting, where a volumetric representation is used to reason about occluded cable parts and plan informative viewpoints\n[\n8\n]\n.\nThese contributions show that NBV and active vision can drastically improve perception under occlusions. However, they typically either address rigid objects, or treat DLOs at a purely geometric level, without exploiting the specific topological structure of cables or integrating tactile/force cues.\nII-\n4\nInteractive Perception and Visuoâ€“Tactile Fusion\nInteractive perception uses purposeful manipulation to probe the environment and improve state estimation. For DLOs, Caporali et al. proposed a method for manipulating DLOs while estimating model parameters online, thereby coupling interaction and parameter identification in the loop\n[\n5\n]\n. SÃ¼berkrÃ¼b et al. present an interactive segmentation strategy based on motion correlation: by moving the cable, the robot segments it from the background via correlated motion in the image\n[\n22\n]\n. McConachie and Berenson study learning-based interactive perception for deformable objects without demonstrations, using reinforcement learning to decide exploratory actions\n[\n16\n]\n. From a mechatronic perspective, visuoâ€“tactile fusion is particularly attractive for the scientific community.\nPecyna et al. integrate visual and tactile sensing in an RL framework for following deformable linear objects, demonstrating that tactile information significantly improves robustness\n[\n19\n]\n.\nPalli et al. report a case study on tactile-based control for switchgear cabling, showing that tactile feedback can be used to stabilize contact and reduce damage in industrial wiring tasks\n[\n18\n]\n. Datta et al. propose 3D-ViTac, a visuoâ€“tactile framework for fine-grained manipulation that combines 3D vision and tactile features in a unified representation\n[\n12\n]\n. These works highlight that multi-modal sensing is key for robust DLO manipulation, but active viewpoint planning is limited or not explicitly optimized.\nII-\n5\nEmerging Generative Approaches\nLarge-scale learning and foundation models are beginning to influence DLO perception and manipulation. Caporali et al. introduced DLO Perceiver, which grounds a large language model to perform text-guided segmentation and perception of DLOs from images\n[\n6\n]\n. Emerging works explore multi-robot assembly of DLOs using multi-modal perception\n[\n9\n]\nor generative 3D state estimation from partial views\n[\n24\n]\n. These methods suggest that combining rich generative models, multi-modal sensing, and active perception can improve performance in complex manipulation tasks of DLOs.\nII-\n6\nOur Contribution\nTo the best of our knowledge, no prior work provides a\nfully autonomous foundation-model-driven\nand\ncross-modal\nreconstruction pipeline capable of estimating the\nglobal\nshape of a DLO\nwhen large portions are visually occluded. Existing cross-modal approaches\nfocus mainly on object recognition\n[\n17\n,\n11\n]\n, but do not tackle the reconstruction\nof continuous deformable structures such as cables.\nWithin this context, the present work advances the state of the art by introducing a novel visuoâ€“tactile perception framework that unifies foundation models, autonomous tactile exploration, and spline-based geometric reconstruction. The main contributions are:\nâ€¢\nTopology-aware visual representation through skeletonization and endpoint detection.\nUnlike classical vision pipelines that treat DLOs as unstructured point\nclusters, our method extracts the underlying curve structure of the cable.\nThis yields a representation that is robust to noise, downsampling, and\npartial occlusions, and facilitates principled integration with tactile data.\nâ€¢\nAutonomous tactile exploration of visually occluded regions.\nWhen the visual reconstruction is incomplete, an occlusion detector triggers\nan active scanning procedure with a tactile sensor. The resulting tactile point\nclouds are segmented and merged with visual data to restore missing geometry.\nSuch an active compensation mechanism is absent from existing DLO reconstruction\nmethods.\nâ€¢\nUnified point-cloud fusion and B-spline reconstruction.\nWe propose an endpoint-guided point sorting strategy followed by B-spline\ninterpolation to obtain a smooth and globally consistent cable model. This\nreconstruction remains effective even in challenging cases involving multiple\ndisconnected occluded segments.\nIII\nMethodology\nThis work proposes a deterministic cross-modal visuo-tactile methodology for reconstructing the shape of DLOs, specifically cables, under partial observability due to visual occlusions or imperfect processing of visual data. The approach integrates visual perception and active tactile exploration within a unified geometric reconstruction framework depicted in Fig.\n1\nwhose modules are discussed in detail hereafter, while a formal description is provided in Algorithm\n1\n. The active tactile exploration strategy employed to recover occluded cable segments is formalized in Algorithm\n2\n.\nIII-A\nVisual Perception\nAn RGB-D camera mounted on the robot end effector acquires an image\nâ„\n\\mathcal{I}\nof the scene. The pre-trained network Florence2\n[\n20\n]\nperforms a semantic segmentation by using the prompt\nâ€œshelf and cablesâ€\n. The results of Florence2 are the bounding boxes of the objects in the scene, then processed by SAM2\n[\n27\n]\nto create their segmentation masks\nâ„³\n\\mathcal{M}\n.\nIII-B\nImage Processing\nThe corresponding pixels of the segmentation masks are processed in a different way. The pixels of the shelf represented by\nâ„\nB\n\\mathcal{I}_{B}\nare converted into a point cloud\nğ’«\nB\n\\mathcal{P}_{B}\nto apply the RANSAC algorithm, available in the Point Cloud Library (PCL), which creates a new point cloud matching the model of a plane. The plane coefficients allow obtaining the vector\nz\n^\n\\hat{z}\n, normal to the plane, used to set the orientation of the end effector during the tactile exploration. The pixels of the detected cables\nâ„\nD\nâ€‹\nL\nâ€‹\nO\n\\mathcal{I}_{DLO}\nare processed with two standard operations available in the OpenCV library: image blurring and contour removal. These preliminary steps are necessary to facilitate the unsupervised clustering algorithm HDBSCAN\n[\n4\n]\n, which returns the set of clusters\nğ’\n\\mathcal{C}\n. Subsequently, a skeletonization algorithm is applied to reduce the number of pixels of interest without losing information about the topology of the DLO. Finally, the skeletonized cluster\nğ’\ni\n\\mathcal{C}_{i}\nis converted into a point cloud\nğ’«\ns\nâ€‹\nk\nâ€‹\ne\nâ€‹\nl\nâ€‹\ne\nâ€‹\nt\nâ€‹\no\nâ€‹\nn\n\\mathcal{P}_{skeleton}\n.\nIII-C\nPoint Cloud Processing\nTo reduce the number of points, a downsampling method is applied: all points in a 3D cube are approximated to their centroids. The downsampled point cloud\nğ’«\nd\nâ€‹\no\nâ€‹\nw\nâ€‹\nn\n\\mathcal{P}_{down}\nmay have some points that are too close to each other and, in turn, can generate a non-smooth curve, making the sorting algorithm used to search for the DLO endpoints hard to converge because the algorithm performs well only when the point cloud of the cables is smooth with respect to the variation in direction between two points of the point cloud. So, when the Euclidean distance between two points of the point cloud is below a threshold\nt\nP\nt_{P}\n, they are replaced by the midpoint. Finally, the plane normal\nz\n^\n\\hat{z}\nis used to project all the points of\nğ’«\nd\nâ€‹\no\nâ€‹\nw\nâ€‹\nn\n\\mathcal{P}_{down}\nonto the plane and obtain the point cloud\nğ’«\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n\\mathcal{P}_{proj}\n.\nIII-D\nEndpoints Search\nThe active tactile perception requires the endpoints of each cluster; therefore, endpoint detection is performed using a sorting algorithm based on the idea of following the cable direction on the plane. The details of the algorithm can be found in\n[\n15\n]\n.\nFinally, the first point and the last point of the sorted point cloud\nğ’«\ns\nâ€‹\no\nâ€‹\nr\nâ€‹\nt\nâ€‹\ne\nâ€‹\nd\n\\mathcal{P}_{sorted}\nrepresent the endpoints of the cables. However, the detected points may not be the true endpoints due to either the presence of occluded parts of the DLO or imperfect segmentation, which can be emphasized through the removal of some pixels or points during the previous steps of the pipeline. In conclusion, a single cluster that represents a cable could be split into a certain number of segments with their own endpoints after the sorting. The set of endpoints\nâ„°\n\\mathcal{E}\nis exploited during the tactile exploration to reconstruct the missing parts of the DLO.\nIII-E\nActive Tactile Exploration\nThe robot relies on both the knowledge acquired through visual perception and endpoint detection to perform the gradient-based tactile exploration illustrated in Algorithm\n2\n. For each endpoint\ne\ne\n, the initial end-effector orientation\nR\nd\nR_{d}\nis computed by using the plane normal\nz\n^\n\\hat{z}\nand the sorted point cloud\nğ’«\ns\nâ€‹\no\nâ€‹\nr\nâ€‹\nt\nâ€‹\ne\nâ€‹\nd\n\\mathcal{P}_{sorted}\n. The sorted point cloud is useful to select the previous point with respect to the endpoint\ne\ne\nand to compute the desired direction along the\ny\ny\naxis of the end effector frame, while the direction along the\nz\nz\naxis is defined by the estimated normal\nz\n^\n\\hat{z}\n. Note how the\ny\ny\ndirection has been chosen as the direction along the cable because it is orthogonal to the longest side of the tactile pad (see Fig.\n2\n), hence more taxels will likely be activated at the contact. Since the\ny\ny\naxis represents the direction along the cable, the new exploration point is defined as the endpoint with an additional term\nÎ”\nâ€‹\ny\n\\Delta y\nalong the direction of the cable. Once the robot has been brought to the new exploration point\nt\ne\nb\nt^{b}_{e}\n, there is a descending phase along the direction\nz\n^\n\\hat{z}\nby the offset\nÎ”\nâ€‹\nz\n\\Delta z\nuntil the tactile sensor detects a touch. The tactile map\nğ’¯\n\\mathcal{T}\nis acquired at each contact. Let\nH\ni\nâ€‹\nj\nH_{ij}\nbe the\ni\nâ€‹\nj\nij\n-th element of a\n6\nÃ—\n2\n6\\times 2\nmatrix\nH\nH\ncontaining the norms of the Hessian matrices\nâˆ‡\n2\nğ’¯\ni\nâ€‹\nj\n\\nabla^{2}\\mathcal{T}_{ij}\ncomputed in each point of the tactile map\nğ’¯\n\\mathcal{T}\n. The\nindicator\ncomputed as\nâ€–\nH\nâ€–\n\\bigl\\lVert H\\bigr\\rVert\nis used as a curvature-based metric to discriminate between contacts with the flat support surface and contacts with the cable. If the metric exceeds a predefined threshold\nt\nH\nt_{H}\n, the centroid of the tactile map is projected onto the plane and added to the visual point cloud\nğ’«\nt\nâ€‹\na\nâ€‹\nc\nâ€‹\nt\nâ€‹\ni\nâ€‹\nl\nâ€‹\ne\n\\mathcal{P}_{tactile}\n. The last added point (the endpoint\ne\ne\nat the first iteration) and the actual sampled point\np\nn\nâ€‹\ne\nâ€‹\nw\np_{new}\nare used to compute the new end effector orientation, while the target point is defined by following the same logic explained above. The algorithm proceeds iteratively and terminates after all the endpoints are processed. The exploration along the cable is stopped when the Euclidean distance between\np\nn\nâ€‹\ne\nâ€‹\nw\np_{new}\nand at least one endpoint is below the threshold\nd\nm\nâ€‹\ni\nâ€‹\nn\nd_{min}\n. After reaching an endpoint of the cable, it is marked as a non-endpoint and will not be evaluated as a starting point for another iteration of the algorithm. If the metric does not exceed the threshold\nt\nH\nt_{H}\n, the desired orientation\nR\nd\nR_{d}\nis modified by applying a rotation about\nz\n^\n\\hat{z}\nwith a rotation matrix\nR\nz\nâ€‹\n(\nÎ¸\n)\nR_{z}(\\theta)\nin order to explore along other directions before concluding that there are no cable parts. While the thresholds are empirically chosen, they have been demonstrated to be stable across all tested scenarios, suggesting robustness to variations in contact conditions. All parameters of the algorithms are summarized in Tab.\nI\n.\nIII-F\nInterpolation\nThe point cloud\nğ’«\nm\nâ€‹\ne\nâ€‹\nr\nâ€‹\ng\nâ€‹\ne\nâ€‹\nd\n\\mathcal{P}_{merged}\nis obtained through the merging operation between\nğ’«\ns\nâ€‹\no\nâ€‹\nr\nâ€‹\nt\nâ€‹\ne\nâ€‹\nd\n\\mathcal{P}_{sorted}\nand\nğ’«\nt\nâ€‹\na\nâ€‹\nc\nâ€‹\nt\nâ€‹\ni\nâ€‹\nl\nâ€‹\ne\n\\mathcal{P}_{tactile}\n, and it needs to be sorted again to apply the B-spline and obtain the model of the cable. In conclusion, the result is the point cloud\nğ’«\ni\nâ€‹\nn\nâ€‹\nt\nâ€‹\ne\nâ€‹\nr\nâ€‹\np\nâ€‹\no\nâ€‹\nl\nâ€‹\na\nâ€‹\nt\nâ€‹\ne\nâ€‹\nd\n\\mathcal{P}_{interpolated}\nwhere the first point and the last point of the cloud represent the true endpoints of the DLO.\nInput :\nVisual point cloud\nğ’«\nv\n\\mathcal{P}_{v}\n; parameters\nd\nm\n,\nt\nP\n,\nt\nH\nd_{m},t_{P},t_{H}\nOutput :\nReconstructed B-spline curve\nğ’®\n\\mathcal{S}\nâ„\nâ†\n\\mathcal{I}\\leftarrow\nAcquireImage\n(\n)\n()\nâ„³\nâ†\n\\mathcal{M}\\leftarrow\nSemanticSegmentation\n(\nâ„\n)\n(\\mathcal{I})\nâ„\nB\n,\n\\mathcal{I}_{B},\nâ„\nD\nâ€‹\nL\nâ€‹\nO\n\\mathcal{I}_{DLO}\nâ†\n\\leftarrow\nMaskProcessing\n(\nâ„³\n)\n(\\mathcal{M})\nğ’«\nB\nâ†\n\\mathcal{P}_{B}\\leftarrow\nImageToPointCloud\n(\nâ„\nâ„¬\n)\n(\\mathcal{I_{B}})\nz\n^\nâ†\n\\hat{z}\\leftarrow\nRANSAC\n(\nğ’«\nB\n)\n(\\mathcal{P}_{B})\nğ’\nâ†\n\\mathcal{C}\\leftarrow\nHDBSCAN\n(\nâ„\nD\nâ€‹\nL\nâ€‹\nO\n)\n(\\mathcal{I}_{DLO})\nforeach\nğ’\ni\nâˆˆ\nğ’\n\\mathcal{C}_{i}\\in\\mathcal{C}\ndo\nğ’\nS\nâ†\n\\mathcal{C}_{S}\\leftarrow\nSkeletonization\n(\nğ’\ni\n)\n(\\mathcal{C}_{i})\nğ’«\ns\nâ€‹\nk\nâ€‹\ne\nâ€‹\nl\nâ€‹\ne\nâ€‹\nt\nâ€‹\no\nâ€‹\nn\nâ†\n\\mathcal{P}_{skeleton}\\leftarrow\nImageToPointCloud\n(\nğ’\nS\n)\n(\\mathcal{C}_{S})\nğ’«\nd\nâ€‹\no\nâ€‹\nw\nâ€‹\nn\nâ†\n\\mathcal{P}_{down}\\leftarrow\nDownsampleAndFilter\n(\nğ’«\ns\nâ€‹\nk\nâ€‹\ne\nâ€‹\nl\nâ€‹\ne\nâ€‹\nt\nâ€‹\no\nâ€‹\nn\n,\nd\nm\n,\nt\nP\n)\n(\\mathcal{P}_{skeleton},\\ d_{m},\\ t_{P})\nğ’«\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\nâ†\n\\mathcal{P}_{proj}\\leftarrow\nPlaneProjection\n(\nğ’«\nd\nâ€‹\no\nâ€‹\nw\nâ€‹\nn\n,\nz\n^\n)\n(\\mathcal{P}_{down},\\ \\hat{z})\nğ’«\ns\nâ€‹\no\nâ€‹\nr\nâ€‹\nt\nâ€‹\ne\nâ€‹\nd\n,\nâ„°\nâ†\n\\mathcal{P}_{sorted},\\ \\mathcal{E}\\leftarrow\nEndpointsDetection\n(\nğ’«\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n)\n(\\mathcal{P}_{proj})\nğ’«\nt\nâ€‹\na\nâ€‹\nc\nâ€‹\nt\nâ€‹\ni\nâ€‹\nl\nâ€‹\ne\nâ†\n\\mathcal{P}_{tactile}\\leftarrow\nTactileExploration\n(\nğ’«\ns\nâ€‹\no\nâ€‹\nr\nâ€‹\nt\nâ€‹\ne\nâ€‹\nd\n,\nâ„°\n,\nz\n^\n,\nt\nH\n)\n(\\mathcal{P}_{sorted},\\ \\mathcal{E},\\ \\hat{z},\\ t_{H})\nğ’«\nm\nâ€‹\ne\nâ€‹\nr\nâ€‹\ng\nâ€‹\ne\nâ€‹\nd\nâ†\n\\mathcal{P}_{merged}\\leftarrow\nğ’«\ns\nâ€‹\no\nâ€‹\nr\nâ€‹\nt\nâ€‹\ne\nâ€‹\nd\nâˆª\nğ’«\nt\nâ€‹\na\nâ€‹\nc\nâ€‹\nt\nâ€‹\ni\nâ€‹\nl\nâ€‹\ne\n\\mathcal{P}_{sorted}\\ \\cup\\ \\mathcal{P}_{tactile}\nğ’«\ns\nâ€‹\no\nâ€‹\nr\nâ€‹\nt\nâ€‹\ne\nâ€‹\nd\n,\nâ„°\nâ†\n\\mathcal{P}_{sorted},\\ \\mathcal{E}\\leftarrow\nEndpointsDetection\n(\nğ’«\nm\nâ€‹\ne\nâ€‹\nr\nâ€‹\ng\nâ€‹\ne\nâ€‹\nd\n)\n(\\mathcal{P}_{merged})\nğ’«\ni\nâ€‹\nn\nâ€‹\nt\nâ€‹\ne\nâ€‹\nr\nâ€‹\np\nâ€‹\no\nâ€‹\nl\nâ€‹\na\nâ€‹\nt\nâ€‹\ne\nâ€‹\nd\nâ†\n\\mathcal{P}_{interpolated}\\leftarrow\nBSplineInterpolation\n(\nğ’«\ns\nâ€‹\no\nâ€‹\nr\nâ€‹\nt\nâ€‹\ne\nâ€‹\nd\n)\n(\\mathcal{P}_{sorted})\nAlgorithmÂ 1\nCross-Modal Visuo-Tactile DLO Reconstruction\nInput :\nSorted Visual Point Cloud\nğ’«\ns\nâ€‹\no\nâ€‹\nr\nâ€‹\nt\nâ€‹\ne\nâ€‹\nd\n\\mathcal{P}_{sorted}\n, Endpoints Set\nâ„°\n\\mathcal{E}\n, Estimated Plan Normal\nz\n^\n\\hat{z}\n, parameters\nt\nH\n,\nÎ”\nâ€‹\ny\n,\nÎ”\nâ€‹\nz\n,\nd\nm\nâ€‹\ni\nâ€‹\nn\n,\nÎ¸\nt_{H},\\ \\Delta y,\\ \\Delta z,\\ d_{min},\\ \\theta\nOutput :\nTactile Point Cloud\nğ’«\nt\nâ€‹\na\nâ€‹\nc\nâ€‹\nt\nâ€‹\ni\nâ€‹\nl\nâ€‹\ne\n\\mathcal{P}_{tactile}\nforeach\ne\nâˆˆ\nâ„°\ne\\in\\mathcal{E}\ndo\nR\nd\nâ†\nR_{d}\\leftarrow\nComputeEEOrientation(\ne\n,\nz\n^\n,\nğ’«\ns\nâ€‹\no\nâ€‹\nr\nâ€‹\nt\nâ€‹\ne\nâ€‹\nd\ne,\\ \\hat{z},\\ \\mathcal{P}_{sorted}\n)\nwhile\nÂ¬\n\\neg\nstop\ndo\nt\ne\nb\nâ†\nt^{b}_{e}\\leftarrow\nComputeExplorationPoint(\nÎ”\nâ€‹\ny\n\\Delta y\n)\nRobot.goTo(\nR\nd\nR_{d}\n,\nt\ne\nb\nt^{b}_{e}\n)\nwhile\nÂ¬\n\\neg\ntouched\ndo\nt\ne\nb\nâ†\nt^{b}_{e}\\leftarrow\nComputeExplorationPoint(\nÎ”\nâ€‹\nz\n\\Delta z\n)\nRobot.goTo(\nR\nd\nR_{d}\n,\nt\ne\nb\nt^{b}_{e}\n)\nif\ntouched\nthen\nğ’¯\nâ†\n\\mathcal{T}\\leftarrow\nAcquireTactileMap()\nindicator\nâ†\n\\leftarrow\ncompute_indicator(\nğ’¯\n\\mathcal{T}\n)\nif\nindicator\n>\nt\nH\n>t_{H}\nthen\np\nn\nâ€‹\ne\nâ€‹\nw\nâ†\np_{new}\\leftarrow\nComputeCentroid(\nğ’¯\n\\mathcal{T}\n)\nğ’«\nt\nâ€‹\na\nâ€‹\nc\nâ€‹\nt\nâ€‹\ni\nâ€‹\nl\nâ€‹\ne\n\\mathcal{P}_{tactile}\n.\nadd(\np\nn\nâ€‹\ne\nâ€‹\nw\np_{new}\n)\nforeach\ne\np\nâˆˆ\nâ„°\ne_{p}\\in\\mathcal{E}\ndo\nif\nEuclideanDistance(\np\nn\nâ€‹\ne\nâ€‹\nw\n,\ne\np\np_{new},e_{p}\n)\n<\nd\nm\nâ€‹\ni\nâ€‹\nn\n<d_{min}\nthen\nstop\nâ†\n\\leftarrow\ntrue\nR\nd\nR_{d}\nâ†\n\\leftarrow\nComputeEEOrientation(\np\nn\nâ€‹\ne\nâ€‹\nw\n,\nz\n^\np_{new},\\ \\hat{z}\n,\nğ’«\ns\nâ€‹\no\nâ€‹\nr\nâ€‹\nt\nâ€‹\ne\nâ€‹\nd\n\\mathcal{P}_{sorted}\n)\nelse\nR\nd\nâ†\nR\nd\nâ€‹\nR\nz\nâ€‹\n(\nÎ¸\n)\nR_{d}\\leftarrow R_{d}R_{z}(\\theta)\nAlgorithmÂ 2\nGradient-based Tactile Exploration\nFigure 2:\nExperimental setup: robot arm with camera and tactile sensor.\nTABLE I:\nParameters of the cable reconstruction algorithm\nd\nm\nâ€‹\ni\nâ€‹\nn\nd_{min}\\,\n[m]\nd\nm\nd_{m}\\,\n[m]\nt\nP\nt_{P}\\,\n[m]\nt\nH\nt_{H}\\,\n[m]\nÎ”\nâ€‹\ny\n\\Delta y\\,\n[m]\nÎ”\nâ€‹\nz\n\\Delta z\\,\n[m]\nÎ¸\n\\theta\\,\n[Â°]\n0.0150\n0.0200\n0.0080\n0.0011\n0.0100\n0.0015\n15\nIV\nExperimental Results\nThe experimental setup (see Fig.\n2\n) includes a 7-axis Yaskawa Motoman SIA5F robot equipped with an eye-in-hand Intel Realsense D435i RGB-D camera and a SUNTouch tactile\n[\n10\n]\nsensor. The camera is used with an RGB resolution of\n1280\nÃ—\n720\n1280\\times 720\nand a depth resolution of\n848\nÃ—\n480\n848\\times 480\nboth at\n30\n30\\,\nfps, while the tactile sensor has a taxel matrix with dimensions\n6\nÃ—\n2\n6\\times 2\nand provides tactile readings at\n1000\n1000\\,\nHz.\nThe experiments are carried out in two case studies: a single cable on an inclined plane (CS1) and two cables on a horizontal plane (CS2). For each case study, we show two situations: (i) cables are not occluded, (ii) some cable parts are occluded by another object.\nIV-A\nSingle cable on an inclined plane\nCable with no occlusions\nThe robot is brought into the home position to acquire the image of the scene, subsequently, the image is processed through the semantic segmentation with the prompt\nshelf and cables\n. By exploiting the segmentation masks, it is possible to isolate the pixels of the cable and of the support surface (see Fig.\n3\n).\nThe experiments in this section treat the case of a cable intersecting itself on an inclined plane (as illustrated in the video attached as supplementary material). The estimation of the plane is necessary to correctly define the end-effector orientation during tactile exploration. Fig.\n3\nshows the results of the image processing.\nSince there is a single cable, the result of the clustering algorithm is the same image obtained after the segmentation and reported at the bottom-left in Fig.\n3\n. Both the cable image and the segmented surface are then converted into point clouds to apply the processing steps reported in Fig.\n4\n.\nFigure 3:\nCS1 (i): camera RGB image (top-left), result of the semantic segmentation by Florence2/SAM2 (top-right), segmented cable (bottom-left) and support surface (bottom-right).\nFigure 4:\nCS1 (i): point clouds of the skeletonised cable (top-left) and of the inclined support surface (top-right). The skeleton is downsampled (bottom-left) and subsequently processed to replace the closer points with the midpoint, and then all points are projected onto the plane identified by RANSAC (bottom-right).\nThe tactile exploration phase introduces additional points belonging to the connectors of the cable, and the detection of the true endpoints, now completed successfully, is followed by the last interpolation step that produces the B-spline modeling the cable (see Fig.\n5\n).\nFigure 5:\nCS1 (i): first sorted point cloud with endpoints in red (top-left), point cloud obtained by merging both visual and tactile point clouds (top-right), resulting point cloud of the new sorting step (bottom-left), interpolated point cloud (bottom-right).\nCable with occlusions\nIn this experiment, as shown in Fig.\n6\n, the cable is in the same configuration as the previous one, but a cable part is now occluded by an object. This scenario emphasizes a critical aspect of the proposed approach. In particular, the occluding object is placed over a cable intersection, and this could introduce some problems during tactile exploration as discussed below. Fig.\n6\nreports the results of the image processing.\nFigure 6:\nCS1 (ii): camera RGB image (top-left), result of the semantic segmentation by Florence2/SAM2 (top-right), segmented cable (bottom-left) and support surface (bottom-right).\nFig.\n7\nillustrates the related point clouds, where empty parts correspond to cable segments under the occluding object.\nFigure 7:\nCS1 (ii): point clouds of the skeletonised cable (top-left) and of the inclined support surface (top-right). The skeleton is downsampled (bottom-left) and subsequently processed to replace the closer points with the midpoint, and then all points are projected onto the plane identified by RANSAC (bottom-right).\nThe endpoint detection on the visual point cloud is correctly performed as shown in Fig.\n8\n(left). The result of the tactile exploration (right) is characterised by additional points along the cable portions that were already explored. This behaviour is clearly shown in the attached video, and it is due to the cable intersection. In particular, if cable segments intersect with a small intersection angle (less than\n45\nâˆ˜\n45^{\\circ}\n), the tactile exploration could return a more dense tactile point cloud. In general, the rate of information is good, but the real problem is the search for the true endpoints: the sorting algorithm could find more than two endpoints although the point cloud is completely reconstructed. This is a limitation of the proposed approach because the sorting algorithm performs well if the point cloud is not dense; for this reason, downsampling and post-processing of closer points are necessary before sorting the point cloud.\nFigure 8:\nCS1 (ii): first sorted point cloud with endpoints in red (left), point cloud obtained by merging both visual and tactile point clouds (right).\nTo mitigate this issue, the reconstructed point cloud needs to be downsampled again and eventually post-processed before sorting the point cloud and obtaining the real endpoints. The results of the second step of downsampling and post-processing, in addition to the sorting and the interpolation, are depicted in Fig.\n9\n. In this case, the post-processing step following the downsampling does not produce any changes in the point cloud because the points are sufficiently spaced. Finally, both the search for the true endpoints and the interpolation are completed.\nFigure 9:\nCS1 (ii): resulting point cloud after the second downsampling step of the reconstructed point cloud (top-left), point cloud obtained by the second post-processing step of the reconstructed point cloud (top-right), resulting point cloud of the sorting step (bottom-left), interpolated point cloud (bottom-right).\nIV-B\nTwo cables on a horizontal plane\nIn the second case study (CS2) the robot has to reconstruct two cables of different colours on a horizontal plane. To underline the importance of the clustering algorithm, and for brevity, this section does not report the dense point clouds of the skeletonized cable and support surface.\nFigure 10:\nCS2 (i): camera RGB image (top-left), result of the semantic segmentation by Florence2/SAM2 (top-right), segmented cables (bottom-left) and support surface (bottom-right).\nFigure 11:\nCS2 (i): cluster of the black cable (top-left), cluster of the blue cable (top-right), post-processed point cloud of the black cable (bottom-left) and post-processed point cloud of the blue cable (bottom-right).\nCables with no occlusions\nFig.\n10\nshows the results of the image processing, while Fig.\n11\nillustrates the results of the clustering algorithm and the corresponding post-processed point clouds.\nThe clusters are independently processed and Fig.\n12\nshows the results of the sorting step and of the tactile exploration. For each reconstructed cluster, both the detection of the true endpoints and the interpolation have been performed successfully, as shown in Fig.\n13\n.\nFigure 12:\nCS2 (i): first sorted point cloud of the black cable with endpoints in red (top-left), first sorted point cloud of the blue cable with endpoints in red (top-right), point cloud of the black cable obtained by merging both visual and tactile point clouds (bottom-left) point cloud of the blue cable obtained by merging both visual and tactile point clouds (bottom-right).\nFigure 13:\nCS2 (i): resulting point cloud of the black cable of the new sorting step (top-left), resulting point cloud of the blue cable of the new sorting step(top-right), interpolated point cloud of the black cable (bottom-left), interpolated point cloud of the blue cable (bottom-right).\nFigure 14:\nCS2 (ii): camera RGB image (top-left), result of the semantic segmentation by Florence2/SAM2 (top-right), segmented cables (bottom-left) and support surface (bottom-right).\nCables with occlusions\nFig.\n14\nreports the acquired image and the results of the semantic segmentation. Although the bounding boxes are correctly placed, SAM2 was not able to segment the cables correctly (in particular the black one): some pixels do not belong to the segmentation mask identified by the red colour in Fig.\n14\ntop-right. Therefore, tactile exploration is useful to reconstruct the missing parts arising from semantic segmentation. The results of the clustering and the corresponding post-processed point clouds are shown in Fig.\n15\n. Fig.\n16\nreports the results of the first sorting and of the tactile exploration. Fig.\n17\nillustrates the detection of the true endpoints of the cables and the interpolated point clouds.\nFigure 15:\nCS2 (ii): cluster of the black cable (top-left), cluster of the blue cable (top-right), post-processed point cloud of the black cable (bottom-left) and post-processed point cloud of the blue cable (bottom-right).\nFigure 16:\nCS2 (ii): first sorted point cloud of the black cable with endpoints in red (top-left), first sorted point cloud of the blue cable with endpoints in red (top-right), point cloud of the black cable obtained by merging both visual and tactile point clouds (bottom-left) point cloud of the blue cable obtained by merging both visual and tactile point clouds (bottom-right).\nFigure 17:\nCS2 (ii): resulting point cloud of the black cable of the new sorting step (top-left), resulting point cloud of the blue cable of the new sorting step(top-right), interpolated point cloud of the black cable (bottom-left), interpolated point cloud of the blue cable (bottom-right).\nIV-C\nEvaluation of the reconstruction accuracy\nThe\nIterative Closest Point\n(ICP) algorithm performs registration between two point clouds, specifically aligning a source point cloud with a target one\n[\n2\n]\n. It can be used to numerically evaluate the accuracy of the reconstruction through the root mean squared error of the performed alignment. The target point clouds are the dense point clouds of the cables obtained by transforming the segmented cables into a point cloud before applying any operation in the scenarios with no occlusions, while the source ones are the reconstructed point clouds obtained by performing the tactile exploration in the presence of occlusions. Fig.\n18\nrepresents the overlapped source and target point clouds for each cable in the presented case studies; Table\nII\nshows the corresponding root mean squared errors. A direct quantitative comparison with existing approaches is not included, since most prior methods focus on vision-only perception and are not designed for cross-modal visuoâ€“tactile reconstruction with active tactile exploration.\nFigure 18:\nReconstructed point cloud (green dots) of the cable in CS1 (ii) overlapped to the dense target point cloud of the segmented cable in CS1 (i) (top), reconstructed point cloud (green dots) of the black cable in CS2 (ii) overlapped to the dense target point cloud of the segmented cable in CS2 (i) (bottom-left), reconstructed point cloud (green dots) of the blue cable in CS2 (ii) overlapped to the dense target point cloud of the segmented cable in CS2 (i) (bottom-right).\nTABLE II:\nRoot mean squared errors of the ICP in the two case studies\nCS1 [m]\nCS2 (black cable) [m]\nCS2 (blue cable) [m]\n0.00174\n0.00650\n0.00459\nV\nConclusion\nThis paper presented a cross-modal visuo-tactile framework for reconstructing deformable linear objects under partial observability. By combining foundation-model-based visual perception with autonomous tactile exploration, the proposed method addresses the limitations of vision-only pipelines in the presence of occlusions and imperfect segmentation.\nThe approach exploits a topology-aware representation based on skeletonization and endpoint-guided point sorting, enabling principled fusion of visual and tactile point clouds, and a unified B-spline reconstruction of the cable geometry. Experimental results on a real robotic platform demonstrate reliable reconstruction across a variety of scenarios, including occlusions, inclined planes, self-intersections, and multiple cables.\nWhile the method shows robustness, endpoint detection can become sensitive in cases of dense point clouds arising from small-angle self-intersections.\nFuture work will focus on improving topological reasoning and adaptive resampling, as well as tighter integration between perception and manipulation. Overall, the results indicate that foundation-model-enhanced visuo-tactile perception is a promising direction for robust deformable-object reconstruction in real-world robotic applications.\nReferences\n[1]\nR. Bajcsy\n(1988)\nActive perception\n.\nProceedings of the IEEE\n76\n(\n8\n),\npp.Â 996â€“1005\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n3\n.\n[2]\nP.J. Besl and N. D. McKay\n(1992)\nA method for registration of 3-d shapes\n.\nIEEE Trans. Pattern Anal. Mach. Intell.\n14\n(\n2\n),\npp.Â 239â€“256\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nIV-C\n.\n[3]\nM. Breyer, J. J. Chung, L. Ott, and R. Siegwart\n(2022)\nClosed-loop next-best-view planning for target-driven grasping\n.\nIn\nProc. IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)\n,\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n3\n.\n[4]\nR. J. G. B. Campello, D. Moulavi, and J. Sander\n(2013)\nDensity-based clustering based on hierarchical density estimates\n.\nIn\nAdvances in Knowledge Discovery and Data Mining\n,\nJ. Pei, V. S. Tseng, L. Cao, H. Motoda, and G. Xu (Eds.)\n,\nBerlin, Heidelberg\n,\npp.Â 160â€“172\n.\nCited by:\nÂ§\nIII-B\n.\n[5]\nA. Caporali, K. Galassi, and G. Palli\n(2024)\nDeformable linear objects manipulation with online model parameters estimation\n.\nIEEE/ASME Trans. Mechatronics\n29\n(\n1\n),\npp.Â 245â€“256\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n4\n.\n[6]\nA. Caporali, K. Galassi, and G. Palli\n(2024)\nDLO Perceiver: grounding large language model for deformable linear objects perception\n.\nIEEE Robot. Autom. Lett.\n9\n(\n12\n),\npp.Â 11385â€“11392\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n5\n.\n[7]\nA. Caporali, K. Galassi, R. Zanella, and G. Palli\n(2022)\nFASTDLO: fast deformable linear objects instance segmentation\n.\nIEEE Robot. Autom. Lett.\n7\n(\n4\n),\npp.Â 9075â€“9082\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n2\n.\n[8]\nK. Chen, J. Liang, and K. Hauser\n(2024)\nOcclusion-aware active perception for tangled DLOs using gaussian splatting\n.\narXiv preprint\n.\nExternal Links:\n2403.11290\nCited by:\nÂ§\nII-\n3\n.\n[9]\nK. Chen\net al.\n(2025)\nMulti-robot assembly of deformable linear objects using multi-modal perception\n.\narXiv preprint\n.\nExternal Links:\nDocument\n,\n2506.22034\nCited by:\nÂ§\nII-\n5\n.\n[10]\nM. Costanzo, G. De Maria, C. Natale, and S. Pirozzi\n(2019)\nDesign and calibration of a force/tactile sensor for dexterous manipulation\n.\nSensors\n19\n(\n4\n),\npp.Â 927\n.\nExternal Links:\nDocument\nCited by:\nÂ§IV\n.\n[11]\nP. Falco, S. Lu, C. Natale, S. Pirozzi, and D. Lee\n(2019)\nA transfer learning approach to cross-modal object recognition: from visual observation to robotic haptic exploration\n.\nIEEE Trans. Robot.\n35\n(\n4\n),\npp.Â 987â€“998\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n6\n.\n[12]\nB. Huang, Y. Wang, X. Yang, Y. Luo, and Y. Li\n(2024)\n3d-vitac: learning fine-grained manipulation with visuo-tactile sensing\n.\narXiv preprint arXiv:2410.24091\n.\nCited by:\nÂ§\nII-\n4\n.\n[13]\nX. Huang, D. Chen, Y. Guo, X. Jiang, and Y. Liu\n(2024)\nUntangling multiple deformable linear objects in unknown quantities with complex backgrounds\n.\nIEEE Trans. Autom. Sci. Eng.\n21\n(\n1\n),\npp.Â 671â€“683\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n2\n.\n[14]\nA. Keipour, M. Bandari, and S. Schaal\n(2022)\nDeformable one-dimensional object detection for routing and manipulation\n.\nIEEE Robot. Autom. Lett.\n7\n(\n2\n),\npp.Â 4329â€“4336\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n2\n.\n[15]\nR. Mazza, P. Falco, and C. Natale\n(2024)\nA cross-modal approach to shape reconstruction of deformable linear objects\n.\nIn\n2024 I-RIM Conference\n,\nRome, I\n,\npp.Â 147â€“150\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n,\nÂ§\nIII-D\n.\n[16]\nD. McConachie and D. Berenson\n(2021)\nLearning to manipulate deformable objects without demonstrations using interactive perception\n.\nThe Int. Journ. of Rob. Res.\n40\n(\n8â€“9\n),\npp.Â 989â€“1012\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n4\n.\n[17]\nP. Murali\net al.\n(2022)\nDeep active cross-modal visuo-tactile transfer learning\n.\nIEEE Robot. Autom. Lett.\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n6\n.\n[18]\nG. Palli, S. Pirozzi, and C. Natale\n(2021)\nRobotic wires manipulation for switchgear cabling: a case study on tactile-based control\n.\nIEEE Transactions on Industrial Informatics\n17\n(\n10\n),\npp.Â 6907â€“6916\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n4\n.\n[19]\nL. Pecyna, S. Dong, and S. Luo\n(2022)\nVisual-tactile multimodality for following deformable linear objects using reinforcement learning\n.\nIEEE Robot. Autom. Lett.\n7\n(\n4\n),\npp.Â 9322â€“9329\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n4\n.\n[20]\nN. Ravi, V. Gabeur, Y. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. RÃ¤dle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, K. V. Alwala, N. Carion, C. Wu, R. Girshick, P. Dollar, and C. Feichtenhofer\n(2025)\nSAM 2: segment anything in images and videos\n.\nIn\nThe Thirteenth Int. Conference on Learning Representations\n,\nCited by:\nÂ§\nIII-A\n.\n[21]\nJ. Sanchez, J. Corrales, Behzad-C. Bouzgarrou, and Y. Mezouar\n(2018)\nRobotic manipulation and sensing of deformable objects in domestic and industrial applications: a survey\n.\nThe Int. Journ. of Rob. Res.\n37\n(\n7\n),\npp.Â 688â€“716\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n1\n.\n[22]\nT. SÃ¼berkrÃ¼b, T. K. Oguz, and D. Wollherr\n(2025)\nInteractive robotic moving cable segmentation by motion correlation\n.\nIEEE Robot. Autom. Lett.\n10\n(\n1\n),\npp.Â 112â€“119\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n4\n.\n[23]\nZ. Sun, H. Zhou, N. Li, L. Chen, J. Zhu, and R. B. Fisher\n(2023)\nA robust deformable linear object perception pipeline in 3d: from segmentation to reconstruction\n.\nIEEE Robot. Autom. Lett.\n9\n(\n1\n),\npp.Â 843â€“850\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n2\n.\n[24]\nY. Tang, T. Yang, J. Huang, X. Chu, and K. S. Au\n(2025)\nGenerative 3d state estimation for dlos from partial observations\n.\nIEEE Robot. Autom. Lett.\n11\n(\n1\n),\npp.Â 434â€“441\n.\nCited by:\nÂ§\nII-\n5\n.\n[25]\nZ. Weng, P. Zhou, H. Yin, A. Kravberg, A. Varava, D. Navarro-Alarcon, and D. Kragic\n(2024)\nInteractive perception for deformable object manipulation\n.\nIEEE Robot. Autom. Lett.\n.\nCited by:\nÂ§\nII-\n3\n.\n[26]\nJ. Xiang, H. Dinkel, H. Zhao, N. Gao, B. Coltin, T. Smith, and T. Bretl\n(2023)\nTrackDLO: tracking deformable linear objects under occlusion with motion coherence\n.\nIEEE Robot. Autom. Lett.\n8\n(\n10\n),\npp.Â 6179â€“6186\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n2\n.\n[27]\nB. Xiao, H. Wu, W. Xu, X. Dai, H. Hu, Y. Lu, M. Zeng, C. Liu, and L. Yuan\n(2024-06)\nFlorence-2: advancing a unified representation for a variety of vision tasks\n.\nIn\nProc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 4818â€“4829\n.\nCited by:\nÂ§\nIII-A\n.\n[28]\nM. Yan, Y. Zhu, N. Jin, and J. Bohg\n(2020)\nSelf-supervised learning of state estimation for manipulating deformable linear objects\n.\nIEEE Robot. Autom. Lett.\n5\n(\n2\n),\npp.Â 2372â€“2379\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n2\n.\n[29]\nH. Yin, A. Varava, and D. Kragic\n(2021)\nModeling, learning, perception, and control methods for deformable object manipulation\n.\nScience Robotics\n6\n(\n54\n),\npp.Â eabd8803\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n1\n.\n[30]\nX. Zhang, Y. Wang, and C. Cheah\n(2022)\nRobust vision-based control of deformable linear objects with occlusion\n.\nIEEE Trans. Control Syst. Technol.\n30\n(\n5\n),\npp.Â 2221â€“2228\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n2\n.\n[31]\nJ. Zhu, A. Cherubini, C. Dune, D. Navarro-Alarcon, F. Alambeigi, D. Berenson, F. Ficuciello, K. Harada, X. Li, J. Pan, and W. Yuan\n(2022)\nChallenges and outlook in robotic manipulation of deformable objects\n.\nIEEE Robot. Autom. Mag.\n29\n(\n3\n),\npp.Â 67â€“77\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-\n1\n.",
    "preview_text": "This paper presents a novel cross-modal visuo-tactile perception framework for the 3D shape reconstruction of deformable linear objects (DLOs), with a specific focus on cables subject to severe visual occlusions. Unlike existing methods relying predominantly on vision, whose performance degrades under varying illumination, background clutter, or partial visibility, the proposed approach integrates foundation-model-based visual perception with adaptive tactile exploration. The visual pipeline exploits SAM for instance segmentation and Florence for semantic refinement, followed by skeletonization, endpoint detection, and point-cloud extraction. Occluded cable segments are autonomously identified and explored with a tactile sensor, which provides local point clouds that are merged with the visual data through Euclidean clustering and topology-preserving fusion. A B-spline interpolation driven by endpoint-guided point sorting yields a smooth and complete reconstruction of the cable shape. Experimental validation using a robotic manipulator equipped with an RGB-D camera and a tactile pad demonstrates that the proposed framework accurately reconstructs both simple and highly curved single or multiple cable configurations, even when large portions are occluded. These results highlight the potential of foundation-model-enhanced cross-modal perception for advancing robotic manipulation of deformable objects.\n\nActive Cross-Modal Visuo-Tactile Perception\nof Deformable Linear Objects\nRaffaele Mazza, Ciro Natale, and Pietro Falco\nRaffaele Mazza and Ciro Natale are with Dipartimento di Ingegneria,\nUniversitÃ  degli Studi della Campania Luigi Vanvitelli, 81031 Aversa (CE), Italy, Pietro Falco is with Dipartimento di Ingegneria dellâ€™Informazione, UniversitÃ  degli Studi di Padova, 35122 Padova, Italy.\n(Corr. author: R. Mazza\nraffaele.mazza@unicampania.it\n)\nAbstract\nThis paper presents a novel cross-modal visuoâ€“tactile perception framework for the 3D shape reconstruction of deformable",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "cross-modal perception",
        "visuo-tactile",
        "deformable linear objects",
        "3D shape reconstruction",
        "robotic manipulation",
        "foundation models",
        "SAM",
        "Florence",
        "B-spline interpolation"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰å’Œè§¦è§‰æ„ŸçŸ¥çš„è·¨æ¨¡æ€æ¡†æ¶ï¼Œç”¨äºåœ¨è§†è§‰é®æŒ¡ä¸¥é‡çš„æƒ…å†µä¸‹é‡å»ºå¯å˜å½¢çº¿æ€§ç‰©ä½“ï¼ˆå¦‚ç”µç¼†ï¼‰çš„3Då½¢çŠ¶ï¼Œä½†æœªæ¶‰åŠå¼ºåŒ–å­¦ä¹ ã€æ‰©æ•£æ¨¡å‹æˆ–å…¨èº«æ§åˆ¶ç­‰å…³é”®è¯ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T13:57:59Z",
    "created_at": "2026-01-27T15:53:11.426882",
    "updated_at": "2026-01-27T15:53:11.426890"
}