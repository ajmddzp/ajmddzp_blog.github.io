{
  "id": "2512.02017v1",
  "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion",
  "authors": [
    "Shaowei Liu",
    "David Yifan Yao",
    "Saurabh Gupta",
    "Shenlong Wang"
  ],
  "abstract": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",
  "url": "https://arxiv.org/abs/2512.02017v1",
  "html_url": "https://arxiv.org/html/2512.02017v1",
  "html_content": "VisualSync: Multi‚ÄëCamera Synchronization via Cross‚ÄëView Object Motion\nShaowei Liu\n1\nDavid Yifan Yao\n1‚àó\nSaurabh Gupta\n1‚Ä†\nShenlong Wang\n1‚Ä†\n1\nUniversity of Illinois Urbana-Champaign\nhttps://stevenlsw.github.io/visualsync\nEqual¬†contribution;\n‚Ä†\n\\dagger\nEqual¬†advising.\nAbstract\nToday, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras.\nHowever, synchronizing these cross‚Äëcamera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware.\nWe present VisualSync, an optimization framework based on multi‚Äëview dynamics that aligns unposed, unsynchronized videos at millisecond accuracy.\nOur key insight is that any moving 3D point, when co‚Äëvisible in two cameras, obeys epipolar constraints once properly synchronized.\nTo exploit this, VisualSync leverages off‚Äëthe‚Äëshelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross‚Äëview correspondences.\nIt then jointly minimizes the epipolar error to estimate each camera‚Äôs time offset.\nExperiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.\nFigure 1:\nVisualSync Overview.\nGiven multiple unsynchronized videos capturing the same dynamic scene from different viewpoints, VisualSync recovers globally time‚Äêaligned video streams by estimating temporal offsets between views. For example, in the volleyball scene, before synchronization the player‚Äôs motion is misaligned across videos; afterwards, a given timestamp in all three streams corresponds to the same moment.\n1\nIntroduction\nRecording dynamic scenes from multiple viewpoints has become increasingly common in everyday life. From concerts and sports events to lectures and birthday parties, people often capture the same moment using different handheld devices. These multi-view recordings present a rich opportunity to reconstruct scenes in 4D, enable bullet-time effects, or enhance the capabilities of existing vision models. However, these videos are typically captured independently, without synchronization or known camera poses, making it difficult to align and fuse them coherently.\nExisting synchronization methods rely on controlled environments, manual annotations, specific patterns (\ne.g\n. human pose), audio signals (\ne.g\n. flashes or claps), or expensive hardware setups such as time-coded devices, none of which are available in casually captured videos. In this work, we design a versatile and robust algorithm for synchronizing videos without requiring specialized capture or making assumptions about the scene content. Our key insight is that, at the correct synchronization, the scene is static and thus the epipolar relationship (\ni.e\n.\nx\n‚Ä≤\n‚Å£\nT\n‚Äã\nF\n‚Äã\nx\n=\n0\nx^{\\prime T}Fx=0\nfor correspondent points\nx\nx\nand\nx\n‚Ä≤\nx^{\\prime}\nin two views) must hold true for all correspondences, whether on static or dynamic objects\n[\nhartley2003multiple\n]\n. See\nFig.\nÀú\n2\nfor an illustration.\nWhile the insight follows directly from first principles and has been used in past attempts on this problem\n[\nalbl2017two\n,\nlu2012robust\n,\nwhitehead2005temporal\n,\nrao2003view\n,\nli2003reconstruction\n,\nvo2016spatiotemporal\n,\nelhayek2012feature\n]\n, building a practical and robust system that works on videos in the wild is challenging. We need a reliable estimate for the fundamental matrix between camera pairs, dynamic objects are a priori unknown and are generally small and blurry, and not all views may have an overlap. Our key contribution is to leverage recent advances in computer vision, specifically dense tracking, cross-view correspondences, and robust structure-from-motion, to build a robust and versatile system that can reliably synchronize challenging videos.\nSpecifically, we formulate a joint energy function that measures the violations to the epipolar constraints between correspondences between videos and adopt a three-stage optimization procedure. In Stage 0, we use VGGT\n[\nwang2025vggt\n]\nto estimate fundamental matrices between each video pair, use MAST3R\n[\nleroy2024grounding\n]\nto establish correspondences across videos, and use CoTracker3\n[\nkaraev2024cotracker\n,\nkaraev2024cotracker3\n]\nto establish dense tracks within each video. This gives us access to quantities (correspondences and fundamental matrices) necessary to evaluate the joint energy. Optimizing this joint energy directly is challenging. Therefore, in Stage 1, we decompose this energy into pairwise energy terms and estimate the best temporal alignment between each video pair via a brute force search. In Stage 2, we synchronize the temporal offsets across all video pairs to assign a globally consistent temporal offset to each video.\nWe validate our approach on diverse datasets and show strong performance across different scenes, motions, and camera setups, and achieve high-precision synchronization even under severe viewpoints. Specifically, we outperform SyncNerf\n[\nkim2024sync\n]\n, a recent method for this task by radiance field optimization, and adaptations of two recent methods Uni4D\n[\nyao2025uni4d\n]\nand MAST3R\n[\nleroy2024grounding\n]\n. These results demonstrate the robustness and generality of our approach and open the door to scalable, unconstrained multi-view 4D scene understanding.\n2\nRelated work\nFigure 2:\nEpipolar‚Äêgeometry cue for video sync:\nWhen cameras are time-aligned, keypoint tracks align with epipolar lines (bottom); misalignment causes deviations (middle). Minimizing these deviations across tracklets recovers the correct time offset.\nTracking and Correspondence.\nEstablishing reliable correspondences across time and views is fundamental for synchronizing multi-view videos\n[\nedstedt2024roma\n,\nbay2006surf\n,\nblack1996robust\n,\ndetone2018superpoint\n,\nsarlin2020superglue\n,\nbazin2016actionsnapping\n,\nsun2021loftr\n]\n. Recent models like CoTracker\n[\nkaraev2024cotracker\n,\nkaraev2024cotracker3\n,\nharley2022particle\n,\ndoersch2022tap\n,\ndoersch2023tapir\n]\ntrack points densely over time, offering strong temporal coherence. However, they do not model spatial correspondences across different viewpoints. On the other hand, MASt3R\n[\nleroy2024grounding\n,\nwang2024dust3r\n]\nfocuses on spatial matching and stereo reconstruction, providing dense cross-view correspondences, but it does not handle temporal dynamics, especially in moving scenes. Our method bridges this gap by constructing spatio-temporal cross-view correspondences, integrating both temporal tracking and spatial matching to enable accurate synchronization in dynamic, multi-view video settings.\nMulti-View Structure-from-Motion.\nStructure-from-Motion (SfM) techniques\n[\nagarwal2011building\n,\nwang2024dust3r\n,\nschonberger2016structure\n,\nsweeney2015optimizing\n,\noliensis2000critique\n,\npollefeys2008detailed\n,\nsnavely2006photo\n,\nwu2013towards\n]\n, such as COLMAP\n[\nschonberger2016structure\n]\n, have significantly advanced 3D reconstruction pipelines by producing accurate camera poses from multi-view images and videos. More recent models\n[\nbrachmann2024scene\n,\ntang2018ba\n]\nlike HLOC\n[\nsarlin2020superglue\n,\nsarlin2019coarse\n]\nand VGGT\n[\nwang2025vggt\n,\nwang2024vggsfm\n]\nbuild on this progress using learning-based features and transformers to handle large-scale matching and pose estimation. While these methods achieve strong performance in estimating camera geometry, they fall short in synchronizing dynamic scenes, as they rely predominantly on static visual cues. In contrast, our approach explicitly decomposes the scene into static and dynamic components, leveraging static cues for pose estimation and dynamic cues from moving objects to perform robust temporal synchronization across views.\nVideo Synchronization.\nVideo synchronization has been explored from multiple perspectives\n[\nliu2024advancing\n,\nshrstha2007synchronization\n,\nwang2016motion\n,\nwhitehead2005temporal\n,\npundik2010video\n,\nzheng2015sparse\n]\n. Geometry-based methods\n[\nalbl2017two\n,\nvo2016spatiotemporal\n,\nelhayek2012feature\n,\nrao2003view\n,\nlu2012robust\n,\npundik2010video\n]\n, such as those by Albl\net al\n.\n[\nalbl2017two\n]\nand Li\net al\n.\n[\nli2003reconstruction\n]\n, estimate temporal offsets using epipolar geometry, but typically assume static scenes or fixed viewpoint. Human-centric approaches use human pose as a synchronization signal, benefiting from its strong visual priors\n[\nchoi2024humans\n,\nyin2022self\n,\nmuller2024reconstructing\n,\nlee2022extrinsic\n,\njavia2023posesync\n]\n, yet these approaches are limited by the accuracy of human pose estimation, the number of people present in the scene, and they struggle in diverse scenarios without prominent human activity. Audio-based approaches\n[\nshrstha2007synchronization\n,\niashin2024synchformer\n]\nuse audio cues for synchronization, which can only work in quiet environments and not generally applicable to in-the-wild settings where audio is noisy or unavailable. Learning-based methods like Sync-NeRF\n[\nkim2024sync\n]\njointly optimize camera poses and temporal offsets, but are often constrained to specific environments or object types. Our work overcomes these limitations by leveraging pretrained visual foundation models and framing synchronization as an epipolar-based optimization problem. By reasoning jointly over static structures and dynamic foreground motion, we deliver a generalizable solution for aligning asynchronous, unposed videos in complex real-world scenarios.\n3\nApproach\nFigure 3:\nProposed framework:\nGiven unsynchronized videos, VisualSync follows a three-stage pipeline. Stage 0 estimates camera parameters with VGGT\n[\nwang2025vggt\n]\n, dense correspondences with CoTracker3\n[\nkaraev2024cotracker3\n]\n, cross-view matches with MAST3R\n[\nleroy2024grounding\n]\n, and dynamic objects with DEVA\n[\ncheng2023tracking\n]\n. In Stage 1, we estimate pairwise frame offsets by minimizing epipolar violations over matched trajectories. Stage 2 globally optimizes individual offsets to produce synchronized videos.\n3.1\nProblem Formulation\nGiven a set of\nN\nN\nasynchronous videos\n{\nùêï\ni\n}\ni\n=\n1\nN\n\\{\\mathbf{V}^{i}\\}_{i=1}^{N}\ncapturing the same dynamic scene from different viewpoints, our goal is to synchronize them and recover a globally aligned timestamp. Formally, we aim to estimate a time offset\ns\ni\n‚àà\n‚Ñù\ns^{i}\\in\\mathbb{R}\nfor each video\ni\ni\n, to be applied to its original out‚Äëof‚Äësync clock time. After synchronization, frames sharing the same clock time will correspond to the exact same moment across all videos.\nKey Insight.\nOur key insight lies in the epipolar geometry between two cameras that capture the same scene. In\nFig.\nÀú\n2\n, two cameras with known poses capture the same dynamic scene (e.g., a moving person and their dog). We track and associate a keypoint across both videos (here, the human‚Äôs hand), yielding a pair of tracklets (yellow and purple). If the videos are synchronized, then for any pair of frames with the same timestamp, the keypoint observations will satisfy epipolar geometry‚Äîfor example, one keypoint will lie on the epipolar line of its counterpart. Conversely, if the videos are not synchronized, this property does not hold, and the keypoint may deviate from the epipolar line.\nFormally, let\nùê±\ni\n‚Äã\n(\nt\n)\n\\mathbf{x}^{i}(t)\nand\nùê±\nj\n‚Äã\n(\nt\n)\n\\mathbf{x}^{j}(t)\nbe a pair of matched 2D tracklets in homogeneous coordinates between cameras\ni\ni\nand\nj\nj\n, forming continuous‚Äêtime point trajectories and describing the same dynamic 3D point the world. Let\nùêä\ni\n,\nùêä\nj\n\\mathbf{K}_{i},\\mathbf{K}_{j}\ndenote the known (or estimated) intrinsics, and\nùêì\ni\n‚Äã\n(\nt\n)\n,\nùêì\nj\n‚Äã\n(\nt\n)\n\\mathbf{T}^{i}(t),\\mathbf{T}^{j}(t)\nthe corresponding extrinsic trajectories. If the true synchronization offset between cameras\ni\ni\nand\nj\nj\nis\nŒî\n\\Delta\n, then the epipolar constraint holds:\n(\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n)\n‚ä§\n‚Äã\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚Äã\nùê±\nj\n‚Äã\n(\nt\n)\n‚â°\n0\n;\n\\bigl(\\mathbf{x}^{i}(t+\\Delta)\\bigr)^{\\top}\\mathbf{F}^{ij}_{t+\\Delta,t}\\,\\mathbf{x}^{j}(t)\\equiv 0;\n(1)\nwhere\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n\\mathbf{F}^{ij}_{t+\\Delta,t}\nis the fundamental matrix between camera\ni\ni\nat time\nt\n+\nŒî\nt+\\Delta\nand camera\nj\nj\nat time\nt\nt\n.\nOtherwise, it may not be equal to zero.\nBy leveraging this cue, we formulate an optimization problem that finds the time offset minimizing the epipolar distance of all associated keypoint trajectories for each camera pair with known poses. We will then extend this approach to multi‚Äëcamera and moving‚Äëcamera scenarios.\nProblem Formulation.\nInspired by our discussion above, we formulate global synchronization as an energy minimization problem over\n{\ns\ni\n}\n\\{s^{i}\\}\n. Specifically, we aim to find offsets that best align all video pairs by minimizing their pairwise synchronization error:\n{\ns\ni\n}\n=\narg\n‚Å°\nmin\n{\ns\ni\n}\n‚Äã\n‚àë\ni\n<\nj\nE\ni\n‚Äã\nj\n‚Äã\n(\nŒî\ni\n‚Äã\nj\n)\n,\nwhere\n‚Äã\nŒî\ni\n‚Äã\nj\n=\ns\nj\n‚àí\ns\ni\n\\{s^{i}\\}=\\arg\\min_{\\{s^{i}\\}}\\sum_{i<j}E_{ij}(\\Delta^{ij}),\\quad\\text{where }\\Delta^{ij}=s^{j}-s^{i}\n(2)\nHere,\nŒî\ni\n‚Äã\nj\n\\Delta^{ij}\ndenotes the relative temporal offset between videos\ni\ni\nand\nj\nj\n, and\nE\ni\n,\nj\n‚Äã\n(\nŒî\ni\n‚Äã\nj\n)\nE_{i,j}(\\Delta^{ij})\nmeasures the misalignment error under this candidate offset in terms of the Sampson geometric error between associated tracklet pairs that are covisible between camera\ni\ni\nand\nj\nj\n.\nPairwise Term.\nThe key idea of\nE\ni\n‚Äã\nj\n‚Äã\n(\nŒî\n)\nE_{ij}(\\Delta)\nis to quantify how much the paired tracklets violate epipolar geometry. Among various epipolar‚Äêerror measures, we adopt the Sampson error\n[\nhartley2003multiple\n,\nluong1996fundamental\n,\nsampson1982fitting\n,\nrydell2024revisiting\n]\n, which approximates the squared Euclidean distance from a point to its corresponding epipolar line. By linearizing the epipolar constraint, it admits a closed‚Äêform expression and is computationally efficient for real‚Äëworld optimization. Detailed derivation are presented in\nAppendix\nÀú\nA\n. Formally, we write:\nE\ni\n‚Äã\nj\n‚Äã\n(\nŒî\n)\n=\n‚àë\n(\nùê±\ni\n,\nùê±\nj\n)\n‚àë\nt\n(\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n‚ä§\n‚Äã\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚Äã\nùê±\nj\n‚Äã\n(\nt\n)\n)\n2\n‚Äñ\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚Äã\nùê±\nj\n‚Äã\n(\nt\n)\n‚Äñ\n1\n,\n2\n2\n+\n‚Äñ\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚ä§\n‚Äã\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n‚Äñ\n1\n,\n2\n2\n,\nE_{ij}(\\Delta)=\\displaystyle\\sum_{(\\mathbf{x}^{i},\\mathbf{x}^{j})}\\displaystyle\\sum_{t}{\\frac{\\bigl(\\mathbf{x}^{i}(t+\\Delta)^{\\top}\\,\\mathbf{F}^{ij}_{t+\\Delta,t}\\,\\mathbf{x}^{j}(t)\\bigr)^{2}}{\\|\\mathbf{F}^{ij}_{t+\\Delta,t}\\,\\mathbf{x}^{j}(t)\\|_{1,2}^{2}+\\|\\mathbf{F}^{ij\\top}_{t+\\Delta,t}\\,\\mathbf{x}^{i}(t+\\Delta)\\|_{1,2}^{2}}}\\,,\n(3)\nwhere\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n\\mathbf{F}^{ij}_{t+\\Delta,t}\nis the fundamental matrix between camera\ni\ni\nat time\nt\n+\nŒî\nt+\\Delta\nand camera\nj\nj\nat time\nt\nt\n, and\n(\nùê±\ni\n‚Äã\n(\nt\n)\n,\nùê±\nj\n‚Äã\n(\nt\n)\n)\n(\\mathbf{x}^{i}(t),\\mathbf{x}^{j}(t))\nare matched continuous‚Äêtime point‚Äêtrajectory tracklets between cameras\ni\ni\nand\nj\nj\n.\nIntuitively, the numerator is the squared algebraic epipolar residual, and the denominator sums the squared lengths of the two epipolar‚Äêline normals. This normalization converts the raw residual into an approximation of the squared point‚Äêto‚Äêline distance, closely matching the true reprojection error, while remaining a fast and closed‚Äêform computation.\n3.2\nInference\nChallenges.\nMinimizing the energy in\nEq.\nÀú\n2\nposes three challenges. First, the optimization problem is highly non‚Äëconvex. Second, the formulation is continuous‚Äëtime, yet observations arrive at discrete frame times. Third, in real‚Äëworld scenarios, it is difficult to associate dense trajectories across cameras with significantly different viewpoints and to estimate accurate poses for moving cameras.\nOverview.\nWe address the challenge via a three‚Äêstage optimization strategy.\nStage 0\nleverages large, pretrained vision models for dense pose‚Äêtrajectory tracking, feedforward camera‚Äêpose and intrinsic estimation, and extreme‚Äêviewpoint matching, making energy evaluation tractable. Then we adopt a divide‚Äêand‚Äêconquer approach to minimize the proposed energy optimization.\nStage 1\nperforms per‚Äêpair, discrete‚Äêtime surrogate optimizations via exhaustive search to find each camera pair‚Äôs optimal alignment.\nStage 2\naggregates these pairwise alignments to recover the global, continuous‚Äêtime offset via solving a robust least square problem.\nStage 0: Visual Cue Extraction.\nComputing\nE\ni\n‚Äã\nj\nE_{ij}\ndefined in\nEq.\nÀú\n4\nrequires camera parameters (intrinsics and poses) and dynamic point trajectory pairs across views. To obtain these, we use VGGT\n[\nwang2025vggt\n]\nto jointly reasons about all cameras‚Äô intrinsics and pose trajectories from static background regions in a common coordinate system. We apply GPT4o, SAM and DEVA\n[\ncheng2023tracking\n,\nkirillov2023segment\n,\nren2024grounded\n,\nliu2023grounding\n]\nand CoTrackerV3\n[\nkaraev2024cotracker3\n]\nto segment dynamic objects and track dense 2D point trajectories within each video, and we employ MASt3R\n[\nleroy2024grounding\n]\nto match these per‚Äëview tracklets across cameras by comparing sampled keyframes, yielding the cross‚Äëview correspondences needed for Sampson error evaluation. We provide more details in the appendix.\nStage 1: Estimating Pairwise Offsets.\nTo handle the joint‚Äêoptimization challenge with non-convexity and only discrete visual evidence at each frame, we drop the constraint in\nEq.\nÀú\n2\nand instead search over a discrete set of offsets\nŒî\ni\n‚Äã\nj\n‚àà\nùíÆ\n\\Delta^{ij}\\in\\mathcal{S}\n, for each camera pair\n(\ni\n,\nj\n)\n(i,j)\n. In this way, we can independently minimize each pairwise energy term:\n‚àÄ\n(\ni\n,\nj\n)\n:\nŒî\ni\n‚Äã\nj\n‚Å£\n‚àó\n=\narg\nmin\nŒî\n‚àà\nùíÆ\nE\ni\n‚Äã\nj\n(\nŒî\n)\n,\n\\forall(i,j):\\quad\\Delta^{ij*}=\\arg\\min_{\\Delta\\in\\mathcal{S}}E_{ij}(\\Delta),\n(4)\nwhere\nùíÆ\n\\mathcal{S}\nis a finite set of time offsets, step size is determined by the frame rate and range is a hyperparameter.\nNote that not all camera pairs\n(\ni\n,\nj\n)\n(i,j)\nyield reliable time‚Äêoffset estimates. In practice, some pairs have minimal temporal overlap, others lack sufficient viewpoint overlap, or our visual cues may be noisy. Using the per‚Äëpair estimates in\nEq.\nÀú\n4\n, We discard any pair whose ratio of the optimal energy to the next-best local minimum falls below\n0.1\n0.1\nor more than two local minima found, resulting in\n‚Ñ∞\n\\mathcal{E}\nof reliable pairs for global synchronization.\nStage 2: Global Offset Esimation.\nThe goal in this stage is to recover the global offsets\n{\ns\ni\n}\n\\{s^{i}\\}\nfor all videos from the pairwise estimation\nŒî\ni\n‚Äã\nj\n\\Delta^{ij}\n. Since the discrete, imperfect estimates\nŒî\ni\n‚Äã\nj\n\\Delta^{ij}\nmay not admit a solution\n{\ns\ni\n}\n\\{s^{i}\\}\nperfectly satisfying\ns\nj\n‚àí\ns\ni\n=\nŒî\ni\n‚Äã\nj\ns^{j}-s^{i}=\\Delta^{ij}\nfor every pair\n(\ni\n,\nj\n)\n‚àà\n‚Ñ∞\n(i,j)\\in\\mathcal{E}\n, we formulate a robust least‚Äësquares problem:\n{\ns\ni\n}\n‚àó\n=\narg\n‚Å°\nmin\n{\ns\ni\n}\n‚Äã\n‚àë\n(\ni\n,\nj\n)\n‚àà\n‚Ñ∞\nœÅ\nŒ¥\n‚Äã\n(\ns\nj\n‚àí\ns\ni\n‚àí\nŒî\ni\n‚Äã\nj\n)\n,\n\\{s^{i}\\}^{*}=\\arg\\min_{\\{s^{i}\\}}\\sum_{(i,j)\\in\\mathcal{E}}\\rho_{\\delta}\\bigl(s^{j}-s^{i}-\\Delta^{ij}\\bigr),\n(5)\nwhere\nœÅ\nŒ¥\n\\rho_{\\delta}\nis the Huber loss. We solve this with an iteratively reweighted least squares (IRLS) procedure\n[\ndaubechies2010iteratively\n]\n, yielding the final global synchronization offsets\n{\ns\ni\n}\n‚àó\n\\{s^{i}\\}^{*}\n.\n4\nExperiments\nFigure 4:\nQualitative Comparison of synchronization on Egohumans\n[\nkhirodkar2023egohumans\n]\nacross baselines\nWe visually assess temporal synchronization by presenting magnified views of the shuttlecock‚Äôs position across time. In this complex scenario‚Äîmarked by large temporal discrepancies, a small dynamic element, and moving cameras‚ÄîVisual Sync achieves the most accurate alignment.\nFigure 5:\nQualitative Comparison of Video Sync across datasets.\nWe show the synchronized videos on CMU-Panoptic, UDBD, 3D-POP and Egohumans dataset. Top 3 rows shows the estimated synchronized time stamps from 3 different views. The bottom row shows synchronized timelines across multiple videos. Our method performs robustly across diverse scenes.\n4.1\nExperimental Setup\nImplementation details.\nGiven the input videos, we first extract dynamic object categories using GPT\n[\nachiam2023gpt\n]\nand apply Grounded-SAM\n[\nren2024grounded\n,\nleroy2024grounding\n]\nto obtain initial per-frame segmentations. We then run DEVA\n[\ncheng2023tracking\n]\nto track these instance masks across time, producing temporally consistent segmentations for each moving object. For each tracked instance, we apply CoTracker3\n[\nkaraev2024cotracker3\n]\nto perform per-instance temporal tracking. To establish cross-view correspondences, we sample keyframes every 10 frames and query Mast3R\n[\nleroy2024grounding\n]\nwithin the dynamic instance masks, linking tracklets across views. Camera poses are estimated by VGGT\n[\nwang2025vggt\n]\n. We compute pairwise synchronization energy over the maximum overlapping time window between video pairs and evaluate energy across different offsets. Finally, we select reliable pairs for global synchronization. More details can be found in\nAppendix\nÀú\nA\n.\nTable 1:\nVideo Evaluation Results\n. For each dataset, we show mean and median errors (ms) for video metrics. We\nbold\nand\nunderline\nthe best and second best results respectively. Methods with\n‚àó\nindicates using GT camera pose as input.\nWithout relying on any GT input, our method achieves the best overall performance across all four datasets, spanning diverse subjects and scenes.\nEgohumans\nCMU Panoptic\n3D-POP\nUDBD\nMethod\nŒ¥\nm\n‚Äã\ne\n‚Äã\na\n‚Äã\nn\n\\delta_{mean}\n‚Üì\n\\downarrow\nŒ¥\nm\n‚Äã\ne\n‚Äã\nd\n\\delta_{med}\n‚Üì\n\\downarrow\nŒ¥\nm\n‚Äã\ne\n‚Äã\na\n‚Äã\nn\n\\delta_{mean}\n‚Üì\n\\downarrow\nŒ¥\nm\n‚Äã\ne\n‚Äã\nd\n\\delta_{med}\n‚Üì\n\\downarrow\nŒ¥\nm\n‚Äã\ne\n‚Äã\na\n‚Äã\nn\n\\delta_{mean}\n‚Üì\n\\downarrow\nŒ¥\nm\n‚Äã\ne\n‚Äã\nd\n\\delta_{med}\n‚Üì\n\\downarrow\nŒ¥\nm\n‚Äã\ne\n‚Äã\na\n‚Äã\nn\n\\delta_{mean}\n‚Üì\n\\downarrow\nŒ¥\nm\n‚Äã\ne\n‚Äã\nd\n\\delta_{med}\n‚Üì\n\\downarrow\nUni4D\n‚àó\n[\nyao2025uni4d\n]\n447.4\n222.1\n777.9\n99.9\n1600.1\n1265.4\n103.2\n25.1\nMast3R\n[\nleroy2024grounding\n]\n742.3\n263.8\n113.4\n58.1\n150.3\n72.2\n10.1\n7.4\nSync-NeRF\n‚àó\n[\nkim2024sync\n]\n-\n-\n919.5\n866.7\n1138.9\n1100.0\n0.4\n0.2\nOurs\n122.1\n46.6\n112.6\n41.5\n114.7\n77.8\n20.2\n5.9\nTable 2:\nStage-1 Pairwise Evaluation Results\n. For each dataset, we show mean and median errors (ms) for pairwise metrics. We\nbold\nand\nunderline\nthe best and second best results respectively. Methods with\n‚àó\nindicates using GT camera pose as input. Our method outperforms other baselines on both metrics across datasets.\nEgohumans\nCMU Panoptic\n3D-POP\nUDBD\nMethod\nA@100\n‚Üë\n\\uparrow\nA@500\n‚Üë\n\\uparrow\nA@100\n‚Üë\n\\uparrow\nA@500\n‚Üë\n\\uparrow\nA@100\n‚Üë\n\\uparrow\nA@500\n‚Üë\n\\uparrow\nA@100\n‚Üë\n\\uparrow\nA@500\n‚Üë\n\\uparrow\nUni4D\n‚àó\n[\nyao2025uni4d\n]\n23.8\n49.4\n32.3\n60.7\n0.9\n9.5\n46.2\n74.1\nMast3R\n[\nleroy2024grounding\n]\n24.3\n50.4\n29.6\n49.8\n15.7\n69.1\n77.8\n95.4\nSync-NeRF\n‚àó\n[\nkim2024sync\n]\n-\n-\n3.0\n13.8\n0.0\n8.2\n86.7\n97.35\nOurs\n33.9\n55.8\n26.0\n51.2\n33.3\n69.3\n82.1\n94.3\nDatasets.\nWe evaluate our method on a comprehensive suite of multi-view video datasets capturing dynamic scenes. These datasets vary across several dimensions‚Äîincluding camera motion (static vs. dynamic), environment (indoor vs. outdoor), realism (real vs. synthetic), and motion type (human and non-human). CMU Panoptic\n[\nJoo_2017_TPAMI\n]\nfeatures a real-world indoor dataset with 30 static cameras captured at 30fps capturing human interactions. Egohumans\n[\nkhirodkar2023egohumans\n]\nis a challenging multi-view egocentric and static cameras capturing various sports taking place both indoors and outdoors at different resolutions. 3DPOP\n[\nnaik20233d\n]\nis a large scale 2D to 3D posture, identity and trajectory dataset featuring moving pigeons.Unsynchronized Dynamic Blender Dataset (UDBD) is a synthetic toy example created with dynamic blender assets used in SyncNeRF\n[\nkim2024sync\n]\n.\nTo prepare these multi-view datasets for multi-video synchronization, we take subsequences of each video while ensuring that they all have a common overlap. Each sequence is roughly 10 seconds long, with a random cropping of around 2-3 seconds from the front and back to simulate offsets and unsynchronized videos. These offsets are used for evaluative purposes.\nBaselines.\nFor baselines, we explore leading methods using different strategies for multi-video synchronization. Following Uni4D\n[\nyao2025uni4d\n]\n, we adopt a geometric approach using metric depth estimation to compute Chamfer distances between projected dynamic pixels. Ground-truth camera poses are used to triangulate scene points and resolve per-image scale ambiguity. For a learning-based approach, we use Mast3r\n[\nleroy2024grounding\n]\n, which leverages attention and confidence maps shown to capture motion and rigidity\n[\nchen2025easi3r\n]\n. We compute energy for each offset as the mean confidence between keyframe pairs within dynamic masks. Sync-NeRF\n[\nkim2024sync\n]\nincorporates temporal offsets into photometric optimization. We adapted its codebase for varying intrinsics, but excluded Egohuman due to egocentric camera challenges. For Uni4D and Mast3r, we compute pairwise offsets followed by global optimization, similar to our method.\nMetrics.\nWe evaluate both pairwise and per-video offset performance across all datasets. For pairwise evaluation, predicted offsets are compared with ground-truth relative offsets between every pair of cameras within the same dynamic scene. We report the AUC for error thresholds at 100ms (A@100) and 500ms (A@500) respectively. For video evaluation, we compute a single offset per-video while fixing the offset for the same reference camera. We report both the mean (\nŒ¥\nm\n‚Äã\ne\n‚Äã\na\n‚Äã\nn\n\\delta_{mean}\n) and median (\nŒ¥\nm\n‚Äã\ne\n‚Äã\nd\n\\delta_{med}\n) error. Since our datasets have different FPS, we report our results in milliseconds. The mean synchronization error ( ¬†100 ms) is heavily influenced by a small number of extremely challenging camera views, as we did not exclude any views from our evaluation. Typically, the mean error is more sensitive to a few challenging camera views, as we did not exclude any from evaluation.\nTable 3:\nAblation of key components on Egohumans dataset\n. We\nbold\nand\nunderline\nthe best and second best results respectively. The\n1st\ntwo rows show the oracle performance leveraging GT information as input. The\n2nd\nblock compares different camera pose estimations, the\n3rd\nblock compares different energy terms, the\n4th\nblock compare different solvers for global optimization. Our proposed pipeline achieves the best overall performance.\nDesign Choices\nSolver\nPairwise\nVideo\nSegmentation\nCorrespondence\nCamera\nEnergy\nA@100\n‚Üë\n\\uparrow\nA@500\n‚Üë\n\\uparrow\nŒ¥\nm\n‚Äã\ne\n‚Äã\na\n‚Äã\nn\n\\delta_{mean}\n‚Üì\n\\downarrow\nŒ¥\nm\n‚Äã\ne\n‚Äã\nd\n\\delta_{med}\n‚Üì\n\\downarrow\nGT\nGT\nGT\nSampson\nIRLS\n94.8\n97.9\n11.3\n2.0\nDEVA\nCoTracker+Mast3R\nGT\nSampson\nIRLS\n56.0\n85.0\n72.4\n28.6\nDEVA\nCoTracker+Mast3R\nhloc\nSampson\nIRLS\n38.2\n68.8\n199.5\n75.3\nDEVA\nCoTracker+Mast3R\nransac\nInlier\nIRLS\n20.1\n39.5\n1656.5\n1544.8\nDEVA\nCoTracker+Mast3R\nvggt\nCosine\nIRLS\n28.0\n61.1\n239.8\n94.6\nDEVA\nCoTracker+Mast3R\nvggt\nAlgebraic\nIRLS\n32.6\n63.6\n167.3\n57.9\nDEVA\nCoTracker+Mast3R\nvggt\nEpipolar\nIRLS\n36.2\n69.4\n125.0\n35.4\nDEVA\nCoTracker+Mast3R\nvggt\nSampson\nLS\n20.3\n56.7\n205.9\n118.0\nDEVA\nCoTracker+Mast3R\nvggt\nSampson\nIRLS\n40.2\n73.4\n122.1\n46.6\n4.2\nResults\nQuantitative.\nOur analysis of offset results across diverse datasets and baselines is detailed in\nTab.\nÀú\n1\n(video synchronization) and\nTab.\nÀú\n2\n(pairwise synchronization). Notably, on the challenging EgoHumans dataset\n[\nkhirodkar2023egohumans\n]\n, VisualSync demonstrates superior performance, achieving successful video synchronization with a median error of just 46.6 milliseconds. The geometric approach, Uni4D, exhibits strong performance on datasets where dynamic objects are in closer proximity to the camera. This suggests that more accurate metric estimations in these scenarios directly translate to improved alignment results. Conversely, Uni4D‚Äôs performance significantly deteriorates on the 3D-POP dataset\n[\nnaik20233d\n]\n, where small, distant dynamic objects (pigeons) likely introduce inconsistencies in multi-view metric estimations.\nThe energy landscape of Uni4D is also noisy and there is no clear cues to remove spurious pairwise results, resulting in worse global synchronization. The optimization-based method, Sync-NeRF, jointly optimizes temporal offsets and photometric loss. However, consistent with findings in related work\n[\nchoi2024humans\n]\n, Sync-NeRF struggles to calibrate more complex dynamic scenes beyond UDBD\n[\nkim2024sync\n]\nin settings of large offsets like ours, likely due to a lack of strong priors and effective offset initialization. Interestingly, the learning-based approach, Mast3R\n[\nleroy2024grounding\n]\n, showcases surprisingly strong generalization capabilities. Despite not being explicitly trained for this task, it outperforms the other two baselines on several datasets in both pairwise and video synchronization evaluations. However, its performance on the EgoHumans dataset\n[\nkhirodkar2023egohumans\n]\nis notably weaker, potentially indicating a limitation in handling challenging egocentric views and motion blur.\nQualitative.\nWe present qualitative comparisons on the EgoHumans dataset\n[\nkhirodkar2023egohumans\n]\nagainst Uni4D and Mast3R in\nFig.\nÀú\n4\n. As shown in\nFig.\nÀú\n4\n, both methods struggle with highly dynamic motions and egocentric‚Äìthird-person alignment. The timeline below shows ground-truth key events and sequence lengths. We further demonstrate our method on four datasets (\nFig.\nÀú\n5\n) and in-the-wild sports footage (NBA, EFL) (\nFig.\nÀú\n6\n), effectively handling rapid motion, zooms, and dynamic camera movements across diverse scenes. Additional qualitative results can be seen in\nFig.\nÀú\n13\nand\nFig.\nÀú\n14\nunder\nAppendix\nÀú\nC\n.\n4.3\nAnalysis\nKey components.\nWe evaluate the contribution of key components in our framework through an ablation study on the EgoHumans dataset, as shown in\nTab.\nÀú\n3\n. Note A@100 and A@500 are intermediate metrics across all video pairs, including those with opposite viewpoints or no temporal overlap.\nThe\nfirst block\nreports oracle results using ground-truth segmentation, camera poses, and correspondences for all video pairs‚Äîregardless of overlap‚Äîdemonstrating near-perfect performance with perfect inputs and serving as an upper bound. The\nsecond block\nexamines different camera pose estimation methods. Our framework performs consistently across alternatives, with VGGT achieving the best results. Compared to the\n28.6\n28.6\nms oracle result (2nd row), using estimated camera poses from VGGT achieves a\n46.6\n46.6\nms median error, demonstrating the robustness of our approach under imperfect pose inputs and its effectiveness in practical conditions. We further report camera pose and synchronization results (relative angular error in rotation and translation following VGGT) across randomly selected EgoHumans sports videos in\nTab.\nÀú\n7\n. The\nthird block\nablates various pairwise energy terms used for synchronization. We include a baseline method inspired by\n[\nalbl2017two\n]\n, which relies solely on dynamic tracklets and uses RANSAC to compute inlier matches as the pairwise energy metric. In contrast, our method leverages both static background and camera pose information. We also evaluate three geometric energy terms‚Äî\ncosine error\n,\nalgebraic error\n[\nlee2020geometric\n]\n, and\nsymmetric epipolar distance\n‚Äîwhich are detailed in\nAppendix\nÀú\nA\n. Among all energy terms, the\nSampson error\nperforms best, as it explicitly models noise in the tracklets and provides a lower bound on the true epipolar error through a linear approximation under noisy estimates. The\nlast block\ncompares different solvers. A least-squares solver performs reasonably but is sensitive to outliers. Our IRLS-based solver achieves better accuracy by down-weighting unreliable estimates. Overall, our configuration achieves the best performance, validating the effectiveness of each design choice.\nTable 4:\nAblation of input settings, spurious pair detection, and stage contribution on the Egohumans dataset.\nThe first\n3\nrows ablate number of input pairs. RST denotes a Random Spanning Tree using only the minimal number of pairs needed to form connectivity. For each setting, we run 10 times and report the mean and variance for each metric. Pairwise metrics in\n‚àó\n*\nare computed after global sychronization except for\n4th\nrow. In\n5th\nrow, we show the importance of removing spurious pairs. Our method achieves comparable performance even only using\n50\n%\n50\\%\nof pairs input.\nInput Setting\nStage\nPairwise*\nVideo\nPairs Ratio\nSpurious Det.\nA@100\n‚Üë\n\\uparrow\nA@500\n‚Üë\n\\uparrow\nŒ¥\nm\n‚Äã\ne\n‚Äã\na\n‚Äã\nn\n\\delta_{mean}\n‚Üì\n\\downarrow\nŒ¥\nm\n‚Äã\ne\n‚Äã\nd\n\\delta_{med}\n‚Üì\n\\downarrow\nRST\n‚úì\nFull\n19.5\n¬±\n\\pm\n1.9\n43.2\n¬±\n\\pm\n3.1\n436.4\n¬±\n\\pm\n63.1\n130.0\n¬±\n\\pm\n24.5\n50%\n‚úì\nFull\n28.9\n¬±\n\\pm\n0.5\n59.8\n¬±\n\\pm\n1.0\n212.6\n¬±\n\\pm\n9.5\n70.7\n¬±\n\\pm\n1.3\n80%\n‚úì\nFull\n35.4\n¬±\n\\pm\n0\n69.2\n¬±\n\\pm\n0\n144.4\n¬±\n\\pm\n0\n44.7\n¬±\n\\pm\n0\n100%\n‚úì\nStage-1\n33.9\n55.8\n-\n-\n100%\n‚úó\nFull\n30.7\n58.1\n371.4\n111.5\n100%\n‚úì\nFull\n40.1\n73.4\n122.1\n46.6\nTable 5:\nAblation of varying frame rates\non the CMU Panoptic dataset. We keep 30 fps for the constant setting and downsample them to 5‚Äì30 fps for the varying setting, achieving similar performance without any pipeline changes.\nInput FPS\nŒ¥\nm\n‚Äã\ne\n‚Äã\na\n‚Äã\nn\n\\delta_{mean}\n‚Üì\n\\downarrow\nŒ¥\nm\n‚Äã\ne\n‚Äã\nd\n\\delta_{med}\n‚Üì\n\\downarrow\nConstant\n112.6\n41.5\nVarying\n103.9\n51.5\nTable 6:\nAblation of low frame Rates\non the CMU Panoptic dataset. Downsampling from 30 fps to 15 fps causes a slight performance drop due to reduced temporal overlap, yet the method remains robust under the low-fps setting.\nInput FPS\nŒ¥\nm\n‚Äã\ne\n‚Äã\na\n‚Äã\nn\n\\delta_{mean}\n‚Üì\n\\downarrow\nŒ¥\nm\n‚Äã\ne\n‚Äã\nd\n\\delta_{med}\n‚Üì\n\\downarrow\n30\n112.6\n41.5\n15\n157.2\n45.6\nInput and different stage contributions.\nTab.\nÀú\n4\nevaluates the impact of input pair selection, spurious pair removal, and multi-stage optimization. In the\nfirst block\n, we ablate the number of input pairs. RST uses only the minimal set to form a connected graph, yet achieves\n<\n<\n150‚Äâms median error. Even with 50% of pairs, performance remains close to the full setting, showing robustness to limited input. The\nsecond block\nhighlights the benefits of spurious pair filtering and two-stage optimization. By exploiting energy landscape structure, our method filters unreliable pairs and improves global accuracy. The final row, using the full pipeline, yields the best results.\nInput frame rate.\nTo evaluate the robustness of our method under different temporal conditions, we conduct two ablation study on the CMU Panoptic dataset\n[\nJoo_2017_TPAMI\n]\n. In\nTab.\nÀú\n6\n, we test synchronization across videos with varying frame rates (2nd row), sampling each video between 5 fps and 30 fps. Our method is applied directly without any pipeline changes, achieving 51.5 ms performance‚Äîcomparable to 41.5 ms at the original 30 fps setting‚Äîdemonstrating strong adaptability to frame rate variations in real-world data. In\nTab.\nÀú\n6\n, we test robustness to low frame rates by downsampling videos from 30 fps to 15 fps. While performance slightly degrades due to reduced temporal overlap, the method remains accurate, demonstrating resilience under sparse sampling. Notably, our preprocessing modules (Co-Tracker\n[\nkaraev2024cotracker3\n]\nand DEVA\n[\ncheng2023tracking\n]\n) are optimized for high frame rates, making 15 fps particularly challenging.\nRuntime.\nMotion segmentation and VGGT\n[\nwang2025vggt\n]\npose estimation run at 0.3‚Äâs and 0.35‚Äâs per frame, respectively. Tracking\n[\nkaraev2024cotracker3\n]\ntakes 120‚Äâs per 10s video, and Mast3R\n[\nleroy2024grounding\n]\ntakes 60‚Äâs per video pair. Energy evaluation and global sync are efficient, taking under 10‚Äâs and 1‚Äâs per pair, respectively. Although our method is\nùí™\n‚Äã\n(\nN\n2\n)\n\\mathcal{O}(N^{2})\nin the number of videos, we show in\nTab.\nÀú\n4\nthat using only 50% of pairs or a Random Spanning Tree (RST) offers a good trade-off. All runtimes are measured on a single A6000 GPU. On the CMU Panoptic dataset (15 videos, ¬†200 frames each), our method takes about\n3.3\n3.3\nhours‚Äîcomparable to Uni4D (\n3.9\n3.9\nhrs) and Sync-NeRF (\n4.2\n4.2\nhrs), though slower than MAST3R (\n1.2\n1.2\nhrs). Efficiency can be further improved with lightweight modules and additional computing resources with parallel preprocessing, making the method practical for offline multi-camera applications such as sports analysis, film production, and surveillance.\nLimitations.\nOur method has three main limitations. First, it requires a subset of reliable camera poses (not necessary for the entire sequence). Second, it can‚Äôt handle clips containing non-uniform motion speeds‚Äîfor example, videos that alternate between slow-motion and fast-motion segments. Third, the pairwise estimation step scales quadratically (\nùí™\n‚Äã\n(\nN\n2\n)\n\\mathcal{O}(N^{2})\n) with the number of videos, which can affect efficiency in large-scale setups. Please see\nAppendix\nÀú\nB\nand\nFig.\nÀú\n12\nfor more detailed analysis.\nFigure 6:\nQualitative Synchronization of VisualSync on In-the-Wild Sports Videos.\nWe showcase VisualSync on challenging multi-view sports footage with large camera motions, motion blur, and zoom variations. Despite these real-world challenges, our method achieves accurate synchronization. In the absence of ground-truth alignments, we qualitatively verify accuracy through the precise alignment of key events (e.g., ball release, contact) across views.\nFigure 7:\nK-Plane Rendering on VisualSync Results.\nWe train K-Planes on CMU Panoptic\n[\nJoo_2017_TPAMI\n]\nmulti-view videos for novel view synthesis. Unsynchronized inputs (1st) produce blurry results, while our synchronized outputs (2nd) are sharp and comparable to ground-truth sync (3rd). The 4th column shows real images. Our method enables high-quality synthesis from unsynchronized inputs.\n4.4\nApplication\nVideo synchronization unlocks downstream applications like multi-view dynamic scene reconstruction. As demonstrated in\nFig.\nÀú\n7\n, directly applying K-Planes\n[\nfridovich2023k\n]\nto unsynchronized data yields unsatisfactory novel view renderings. In contrast, our Video Sync approach enables significantly improved novel view synthesis, achieving results comparable to those obtained with ground truth synchronized video. This demonstrates that our method can serve as a fundamental tool for downstream applications such as novel-view synthesis in real, multi-view unsynchronized dynamic world.\n5\nConclusion\nWe presented\nVisualSync\n, a robust framework for synchronizing unposed, unsynchronized multi-camera videos with millisecond accuracy. By minimizing epipolar error over dense correspondences, it recovers precise time offsets across diverse scenes and motions. Experiments on four datasets show VisualSync outperforms recent methods. Our approach contributes a practical step toward enabling dynamic multi-view video motion understanding and related downstream tasks.\nAcknowledgements\nThis project is supported by NSF Awards #2525287, #2404385, #2414227, #2340254, #2312102, and #2331878, the IBM IIDAI Grant, and an Intel Research Gift. We greatly appreciate the NCSA for providing computing resources.\nAppendix A\nImplementation Details\nFigure 8:\nGrounding-DINO\n[\nliu2023grounding\n]\n+ SAM2\n[\nravi2024sam\n]\nSegmentation results\nWe visualize Grounding-DINO‚Äôs proposed bounding box given the dynamic labels produced by GPT4o, along with SAM2 segmentation results with confidence scores. Note that objects that are generally not dynamic (eg. basketball, football, chest) is identified as dynamic in these specific scenes due to inputting video frames into GPT4o.\nFigure 9:\nVisualization of Cotracker and Mast3R correspondences\nWe visualize actual spatial-temporal correspondences predicted using CotrackerV3 (temporal) and Mast3R (spatial).\nFigure 10:\nVGGT\n[\nwang2025vggt\n]\nPredicted Camera poses compared to GT poses for all datasets\nWe visualize VGGT predicted camera poses and ground truth camera poses for Egohumans\n[\nkhirodkar2023egohumans\n]\n, Panoptic\n[\nJoo_2017_TPAMI\n]\n, UDBD\n[\nkim2024sync\n]\n, and 3D-POP\n[\nnaik20233d\n]\nrespectively. Different colors represent different multi-view cameras, while the corresponding lighter palette represents ground truth camera poses.\nFigure 11:\nGPT4o Prompt used for automatic motion segmentation\nWe sample every 20th frame from our video and input to GPT4o with the following context and prompt to identify motion classes to be given to GroundingDINO module for robust video motion segmentation.\nMotion Segmentation.\nFor dynamic object segmentation, we follow the pipeline of Uni4D\n[\nyao2025uni4d\n]\nwith key modifications. Unlike Uni4D, which relies on Recognize Anything\n[\nzhang2024recognize\n]\nand LLM filtering, we directly feed video frames into GPT-4o to identify dynamic classes. The GPT4o Prompt used for automatic motion segmentation is shown in\nFig.\nÀú\n11\n. Every 20th frame is sampled as input, and the detected classes guide GroundingDINO\n[\nliu2023grounding\n]\nto generate bounding boxes, which then prompt SAM 2 for precise segmentation masks. As shown in\nFig.\nÀú\n8\n, this pipeline achieves robust dynamic object segmentation, with DEVA\n[\ncheng2023tracking\n]\napplied afterward for temporally consistent motion segmentation.\nSpatial-Temporal Correspondence.\nTo capture motion trajectories of dynamic objects, we apply CoTracker\n[\nkaraev2024cotracker\n]\nto each video\ni\ni\nwithin its dynamic region\n{\nùêå\nt\ni\n}\n\\{\\mathbf{M}^{i}_{t}\\}\n, producing a set of 2D tracklets\n{\nùêó\ni\n}\n\\{\\mathbf{X}^{i}\\}\n. Each tracklet\nùê±\ni\n=\n{\nùê±\ni\n‚Äã\n(\nt\n)\n}\n\\mathbf{x}^{i}=\\{\\mathbf{x}^{i}(t)\\}\nrepresents the observed image-space trajectory of a dynamic 3D point over time.\nTo establish spatio-temporal correspondences across views, we perform cross-view matching using Mast3R\n[\nleroy2024grounding\n]\n. For each tracklet\nùê±\ni\n\\mathbf{x}^{i}\nin video\ni\ni\n, we seek its matching tracklet\nùê±\nj\n\\mathbf{x}^{j}\nin another video\nj\nj\n. To ensure robust matching under asynchronous capture, we sample a subset of keyframes from each tracklet and compute pairwise similarity between all keyframe pairs across views. For each pair of sampled frames, we construct a candidate tracklet pair\n(\nùê±\ni\n,\nùê±\nj\n)\n(\\mathbf{x}^{i},\\mathbf{x}^{j})\nfrom the correspondences obtained by Mast3R\n[\nleroy2024grounding\n]\nin the two views. The visualization output is shown in\nFig.\nÀú\n9\n.\nCorrespondence Filtering.\nTo further suppress noise, we leverage instance segmentation matching from DEVA to filter correspondences across video pairs. We first construct a pairwise matching matrix by counting correspondences between each instance pair over a sampled subset of frames. Bipartite matching is then applied to obtain optimal one-to-one assignments between instances in the two videos. To ensure reliability, we discard matched instance pairs with fewer than 100 correspondences. After this instance-level matching, we retain only Mast3R correspondences whose endpoints belong to the same matched instance. All remaining correspondences are aggregated into a unified set of spatio-temporal matches, which serve as input to our energy-based synchronization process.\nCamera Parameters.\nWe use VGGT\n[\nwang2025vggt\n]\nto extract camera poses and intrinsics in our preprocessing pipeline. For static cameras, we feed only the first frame as input, while for dynamic cameras, all available frames are used to estimate poses and intrinsics. To manage memory constraints, we subsample dynamic sequences to ensure that every offset computation has overlapping frames with predicted camera poses. The outputs of VGGT include per-frame extrinsics\n{\nùêè\nt\ni\n‚àà\nSE\n‚Äã\n(\n3\n)\n}\n\\{\\mathbf{P}^{i}_{t}\\in\\mathrm{SE}(3)\\}\nand intrinsics\nùêä\nt\ni\n\\mathbf{K}^{i}_{t}\nfor each video\ni\ni\n, where\nùêè\nt\ni\n=\n[\nùêë\nt\ni\n‚à£\nùê≠\nt\ni\n]\n\\mathbf{P}^{i}_{t}=[\\mathbf{R}^{i}_{t}\\mid\\mathbf{t}^{i}_{t}]\n. These parameters are then used to compute relative poses and fundamental matrices. Example output is shown in\nFig.\nÀú\n10\n.\nSampson Error.\nWe use the Sampson error in\nSec.\nÀú\n3.1\nto quantify epipolar constraint violations. Below, we analyze it as a linearized approximation of the true epipolar distance. Let\nùê±\nt\n=\n[\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n;\nùê±\nj\n‚Äã\n(\nt\n)\n]\n\\mathbf{x}_{t}=[\\mathbf{x}^{i}(t+\\Delta);\\mathbf{x}^{j}(t)]\ndenote a noisy spatial-temporal correspondence at time\nt\n+\nŒî\nt+\\Delta\nin video\ni\ni\nand\nt\nt\nin video\nj\nj\n. Let\nùê≥\nt\n\\mathbf{z}_{t}\nbe the underlying clean correspondence. To quantify the deviation of the noisy observation\nùê±\nt\n\\mathbf{x}_{t}\nfrom satisfying the epipolar constraint, we define the energy\n‚Ñ∞\n\\mathcal{E}\nas:\n‚Ñ∞\n2\n=\nmin\nùê≥\nt\n\\displaystyle\\mathcal{E}^{2}=\\min_{\\mathbf{z}_{t}}\\quad\n‚Äñ\nùê≥\nt\n‚àí\nùê±\nt\n‚Äñ\n2\n\\displaystyle\\|\\mathbf{z}_{t}-\\mathbf{x}_{t}\\|^{2}\n(6)\ns.t.\nC\n‚Äã\n(\nùê≥\nt\n)\n=\n0\n\\displaystyle C(\\mathbf{z}_{t})=0\n(7)\nThis energy measures the minimal correction required to project\nùê±\nt\n\\mathbf{x}_{t}\nonto the epipolar manifold defined by\nC\n‚Äã\n(\nùê≥\nt\n)\n=\n0\nC(\\mathbf{z}_{t})=0\n. The constraint function\nC\n‚Äã\n(\nùê≥\nt\n)\nC(\\mathbf{z}_{t})\nevaluates the epipolar consistency between two views:\nC\n‚Äã\n(\nùê≥\nt\n)\n=\nùê≥\ni\n‚Äã\n(\nt\n+\nŒî\n)\n‚ä§\n‚Äã\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚Äã\nùê≥\nj\n‚Äã\n(\nt\n)\nC(\\mathbf{z}_{t})=\\mathbf{z}^{i}(t+\\Delta)^{\\top}\\mathbf{F}^{ij}_{t+\\Delta,t}\\,\\mathbf{z}^{j}(t)\n(8)\nwhere\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n\\mathbf{F}^{ij}_{t+\\Delta,t}\nis the fundamental matrix at frames\nt\n+\nŒî\nt+\\Delta\nand\nt\nt\n.\nSince this constrained optimization is nonlinear and difficult to solve directly, we approximate it by linearizing the constraint around\nùê±\nt\n\\mathbf{x}_{t}\n. Using a first-order Taylor expansion, we obtain:\nC\n‚Äã\n(\nùê≥\nt\n)\n‚âà\nC\n‚Äã\n(\nùê±\nt\n)\n+\nùêâ\nt\n‚Äã\n(\nùê≥\nt\n‚àí\nùê±\nt\n)\nC(\\mathbf{z}_{t})\\approx C(\\mathbf{x}_{t})+\\mathbf{J}_{t}(\\mathbf{z}_{t}-\\mathbf{x}_{t})\n(9)\nwhere\nùêâ\nt\n\\mathbf{J}_{t}\nis the Jacobian of\nC\nC\nwith respect to\nùê±\nt\n\\mathbf{x}_{t}\n. Solving for the optimal correction yields the Sampson approximation of\n‚Ñ∞\n2\n\\mathcal{E}^{2}\n, which provides a first-order lower bound on the original energy.\n‚Ñ∞\nS\n‚Äã\na\n‚Äã\nm\n‚Äã\np\n‚Äã\ns\n‚Äã\no\n‚Äã\nn\n2\n=\n(\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n‚ä§\n‚Äã\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚Äã\nùê±\nj\n‚Äã\n(\nt\n)\n)\n2\n‚Äñ\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚Äã\nùê±\nj\n‚Äã\n(\nt\n)\n‚Äñ\n1\n,\n2\n2\n+\n‚Äñ\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚ä§\n‚Äã\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n‚Äñ\n1\n,\n2\n2\n,\n\\mathcal{E}_{Sampson}^{2}=\\frac{\\bigl(\\mathbf{x}^{i}(t+\\Delta)^{\\top}\\,\\mathbf{F}^{ij}_{t+\\Delta,t}\\,\\mathbf{x}^{j}(t)\\bigr)^{2}}{\\|\\mathbf{F}^{ij}_{t+\\Delta,t}\\,\\mathbf{x}^{j}(t)\\|_{1,2}^{2}+\\|\\mathbf{F}^{ij\\top}_{t+\\Delta,t}\\,\\mathbf{x}^{i}(t+\\Delta)\\|_{1,2}^{2}}\\,,\n(10)\nOther Energy Terms.\nOur ablation study also compares other three distinct geometric energy terms: symmetric epipolar distance, cosine error, and algebraic error. Following the definitions proposed by Terekohov et al.\n[\nterekhov2023tangent\n]\n, these terms are formally defined as:\n‚Ñ∞\nE\n‚Äã\np\n‚Äã\ni\n‚Äã\np\n‚Äã\no\n‚Äã\nl\n‚Äã\na\n‚Äã\nr\n2\n=\n|\n(\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n)\nùñ≥\n‚Äã\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚Äã\nùê±\nj\n‚Äã\n(\nt\n)\n|\n2\n‚Äñ\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚Äã\nùê±\nj\n‚Äã\n(\nt\n)\n‚Äñ\n1\n,\n2\n2\n+\n|\n(\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n)\nùñ≥\n‚Äã\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚Äã\nùê±\nj\n‚Äã\n(\nt\n)\n|\n2\n‚Äñ\n(\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n)\nùñ≥\n‚Äã\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n‚Äñ\n1\n,\n2\n2\n\\mathcal{E}_{Epipolar}^{2}=\\frac{|(\\mathbf{x}^{i}(t+\\Delta))^{\\mathsf{T}}\\mathbf{F}_{t+\\Delta,t}^{ij}\\mathbf{x}^{j}(t)|^{2}}{\\|\\mathbf{F}_{t+\\Delta,t}^{ij}\\mathbf{x}^{j}(t)\\|^{2}_{1,2}}+\\frac{|(\\mathbf{x}^{i}(t+\\Delta))^{\\mathsf{T}}\\mathbf{F}_{t+\\Delta,t}^{ij}\\mathbf{x}^{j}(t)|^{2}}{\\|(\\mathbf{F}_{t+\\Delta,t}^{ij})^{\\mathsf{T}}\\mathbf{x}^{i}(t+\\Delta)\\|^{2}_{1,2}}\n(11)\n‚Ñ∞\nC\n‚Äã\no\n‚Äã\ns\n‚Äã\ni\n‚Äã\nn\n‚Äã\ne\n2\n=\n|\n(\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n)\nùñ≥\n‚Äã\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚Äã\nùê±\nj\n‚Äã\n(\nt\n)\n|\n2\n‚Äñ\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n‚Äñ\n2\n‚Äã\n‚Äñ\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚Äã\nùê±\nj\n‚Äã\n(\nt\n)\n‚Äñ\n2\n+\n|\n(\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n)\nùñ≥\n‚Äã\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚Äã\nùê±\nj\n‚Äã\n(\nt\n)\n|\n2\n‚Äñ\n(\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n)\nùñ≥\n‚Äã\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n‚Äñ\n2\n‚Äã\n‚Äñ\nùê±\nj\n‚Äã\n(\nt\n)\n‚Äñ\n2\n\\mathcal{E}_{Cosine}^{2}=\\frac{|(\\mathbf{x}^{i}(t+\\Delta))^{\\mathsf{T}}\\mathbf{F}_{t+\\Delta,t}^{ij}\\mathbf{x}^{j}(t)|^{2}}{\\|\\mathbf{x}^{i}(t+\\Delta)\\|^{2}\\|\\mathbf{F}_{t+\\Delta,t}^{ij}\\mathbf{x}^{j}(t)\\|^{2}}+\\frac{|(\\mathbf{x}^{i}(t+\\Delta))^{\\mathsf{T}}\\mathbf{F}_{t+\\Delta,t}^{ij}\\mathbf{x}^{j}(t)|^{2}}{\\|(\\mathbf{F}_{t+\\Delta,t}^{ij})^{\\mathsf{T}}\\mathbf{x}^{i}(t+\\Delta)\\|^{2}\\|\\mathbf{x}^{j}(t)\\|^{2}}\n(12)\n‚Ñ∞\nA\n‚Äã\nl\n‚Äã\ng\n‚Äã\ne\n‚Äã\nb\n‚Äã\nr\n‚Äã\na\n‚Äã\ni\n‚Äã\nc\n=\n|\n(\nùê±\ni\n‚Äã\n(\nt\n+\nŒî\n)\n)\nùñ≥\n‚Äã\nùêÖ\nt\n+\nŒî\n,\nt\ni\n‚Äã\nj\n‚Äã\nùê±\nj\n‚Äã\n(\nt\n)\n|\n\\mathcal{E}_{Algebraic}=|(\\mathbf{x}^{i}(t+\\Delta))^{\\mathsf{T}}\\mathbf{F}_{t+\\Delta,t}^{ij}\\mathbf{x}^{j}(t)|\n(13)\nAs shown in\n[\nrydell2024revisiting\n,\nterekhov2023tangent\n]\nand our experiments in Tab. 3, the Sampson error is more robust to input noise. In our setting, we extend the point-wise Sampson error to a spatial-temporal formulation by considering tracklets as input.\nFigure 12:\nFailure Case Visualizations\nWe visualize failure cases across our camera pose, motion segmentation, and spatial correspondence modules. Specifically, observe the incorrect predicted camera poses for the dynamic camera (red) against the ground truth (pink). Background individuals are occasionally mis-segmented for motion segmentation, and some segmentations appear fragmented. Furthermore, Mast3R sometimes generates incorrect correspondences within dynamic masks.\nAppendix B\nFailure Case\nVisualSync depends on several upstream modules and is thus sensitive to their prediction errors. As shown in\nFig.\nÀú\n12\n, inaccuracies in camera pose estimation, motion segmentation, or correspondence matching can propagate through the pipeline and introduce error in synchronization. However, most unreliable cases could be detected by analyzing the pairwise energy landscape. Estimates with ambiguous or low-confidence minima are discarded to prevent degradation of global synchronization. Despite occasional failures, our robust preprocessing effectively limits their impact, enabling VisualSync to maintain strong performance for in-the-wild videos. Its modular design also ensures that improvements to individual components directly enhance overall synchronization quality.\nTable 7:\nCamera pose error vs. synchronization accuracy.\nEven with large rotation and translation errors in camera pose estimates (relative angular errors from VGGT), our method maintains low synchronization errors (\ne.g\n., 9.6 ms for\nFencing\nand 19.4 ms for\nTagging\n). The higher error in\nTennis\nstems from distant camera placement with minimal observable motion.\nMetric\nFencing\nVolleyball\nLegoAssemble\nBadminton\nTagging\nBasketball\nTennis\nCam Rot Err\n‚Üì\n\\downarrow\n10.9\n8.5\n3.1\n10.2\n5.8\n4.0\n2.7\nCam Trans Err\n‚Üì\n\\downarrow\n14.0\n14.6\n7.1\n11.8\n9.1\n12.2\n13.9\nŒ¥\nm\n‚Äã\ne\n‚Äã\nd\n\\delta_{med}\n‚Üì\n\\downarrow\n9.6\n30.8\n38.3\n34.6\n19.4\n27.5\n113.6\nFigure 13:\nQualitative Comparison of VisualSync across datasets\nWe show the synchronized\nvideos on CMU-Panoptic, UDBD, 3D-POP and Egohumans dataset using VisualSync. The top 3 rows show the estimated\nsynchronized time stamps from 3 different views. The bottom row shows synchronized timelines between multiple videos. Our method performs robustly across diverse scenes.\nFigure 14:\nQualitative Comparison of synchronization on Egohumans across baselines\nWe visualize synchronization results in the challenging volleyball sequence in Egohumans. Notice that VisualSync achieves the most accurate alignment even for egocentric views (orange highlight).\nAppendix C\nAdditional Results\nCamera pose error vs. synchronization accuracy.\nWe report camera pose and synchronization results (relative angular errors in rotation and translation following VGGT) across randomly selected EgoHumans sports videos in\nTab.\nÀú\n7\n, demonstrating the robustness of our approach under varying pose estimation quality. Even with large pose errors, our method maintains low synchronization errors (e.g., 9.6 ms for Fencing and 19.4 ms for Tagging), indicating that pose noise does not directly lead to synchronization failure.\nQualitative results.\nTo further demonstrate VisualSync‚Äôs capabilities, we present additional qualitative results across 4 datasets, specifically Egohumans\n[\nkhirodkar2023egohumans\n]\n, Panoptic\n[\nJoo_2017_TPAMI\n]\n, UDBD\n[\nkim2024sync\n]\n, and 3D-POP\n[\nnaik20233d\n]\n, in\nFig.\nÀú\n13\n. We also provide comprehensive comparisons with baselines Uni4D\n[\nyao2025uni4d\n]\nand Mast3R\n[\nleroy2024grounding\n]\nin\nFig.\nÀú\n14\n. Note that we excluded Sync-NeRF\n[\nkim2024sync\n]\nfor comparison as it struggled to produce meaningful offset predictions beyond its simpler native UDBD dataset. For visualization, we show three selected camera views. The first view (blue highlight) serves as the reference, and corresponding frames from other views are aligned accordingly. The timeline below depicts ground-truth keyframe alignments, marking the synchronized frame positions across camera sequences. These qualitative results demonstrate VisualSync‚Äôs strong synchronization performance across datasets and baselines. Full video visualizations can be found on the project page at\nhttps://stevenlsw.github.io/visualsync\n.",
  "preview_text": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.\n\nVisualSync: Multi‚ÄëCamera Synchronization via Cross‚ÄëView Object Motion\nShaowei Liu\n1\nDavid Yifan Yao\n1‚àó\nSaurabh Gupta\n1‚Ä†\nShenlong Wang\n1‚Ä†\n1\nUniversity of Illinois Urbana-Champaign\nhttps://stevenlsw.github.io/visualsync\nEqual¬†contribution;\n‚Ä†\n\\dagger\nEqual¬†advising.\nAbstract\nToday, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras.\nHowever, synchronizing these cross‚Äëcamera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware.\nWe present VisualSync, an optimization framework based on multi‚Äëview dynamics that aligns unposed, unsynchronized videos at millisecond accuracy.\nOur key insight is that any moving 3D point, when co‚Äëvisible in two cameras, obeys epipolar constraints once properly synchronized.\nTo exploit this, VisualSync leverages off‚Äëthe‚Äëshelf 3D reconstru",
  "is_relevant": false,
  "relevance_score": 1.0,
  "extracted_keywords": [
    "multi-camera synchronization",
    "epipolar constraints",
    "3D reconstruction",
    "feature matching",
    "dense tracking"
  ],
  "one_line_summary": "ËØ•ËÆ∫ÊñáÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éË∑®ËßÜËßíÁâ©‰ΩìËøêÂä®ÁöÑÂ§öÊëÑÂÉèÂ§¥ËßÜÈ¢ëÂêåÊ≠•ÊñπÊ≥ïÔºå‰∏çÊ∂âÂèäÁîüÊàêÊ®°ÂûãÊàñËØ≠Ë®ÄÊ®°ÂûãÁõ∏ÂÖ≥ÊäÄÊúØ„ÄÇ",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "2025-12-01T18:59:57Z",
  "created_at": "2026-01-09T09:59:26.339826",
  "updated_at": "2026-01-09T09:59:26.339837"
}