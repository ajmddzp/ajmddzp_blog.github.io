{
    "id": "2601.10268v1",
    "title": "The impact of tactile sensor configurations on grasp learning efficiency -- a comparative evaluation in simulation",
    "authors": [
        "Eszter Birtalan",
        "MiklÃ³s Koller"
    ],
    "abstract": "è§¦è§‰ä¼ æ„Ÿå™¨æ­£é€æ­¥è¿›å…¥æœºå™¨äººé¢†åŸŸï¼Œä¸ºæ¥è§¦è¡¨é¢æä¾›ç›´æ¥ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¥è§¦äº‹ä»¶ã€æ»‘åŠ¨äº‹ä»¶ä¹ƒè‡³çº¹ç†è¯†åˆ«ã€‚è¿™äº›äº‹ä»¶å¯¹æœºå™¨äººæ‰‹éƒ¨è®¾è®¡ï¼ˆåŒ…æ‹¬å‡è‚¢ï¼‰å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºå®ƒä»¬èƒ½æ˜¾è‘—æå‡æŠ“æ¡ç¨³å®šæ€§ã€‚ç„¶è€Œï¼Œç›®å‰å¤§å¤šæ•°å·²å‘å¸ƒçš„æœºå™¨äººæ‰‹éƒ¨è®¾è®¡åœ¨æ‰‹éƒ¨è¡¨é¢é‡‡ç”¨å·®å¼‚å·¨å¤§çš„ä¼ æ„Ÿå™¨å¯†åº¦ä¸å¸ƒå±€æ–¹æ¡ˆï¼Œä¸”å¾€å¾€å æ®å¤§éƒ¨åˆ†å¯ç”¨ç©ºé—´ã€‚æˆ‘ä»¬é€šè¿‡ä»¿çœŸå®éªŒè¯„ä¼°äº†å…­ç§ä¸åŒå¯†åº¦ä¸å¸ƒå±€çš„è§¦è§‰ä¼ æ„Ÿå™¨é…ç½®ï¼Œé‡ç‚¹å…³æ³¨å…¶å¯¹å¼ºåŒ–å­¦ä¹ æ•ˆæœçš„å½±å“ã€‚æˆ‘ä»¬é‡‡ç”¨çš„åŒç³»ç»Ÿæ¶æ„ç¡®ä¿äº†ç ”ç©¶ç»“æœçš„ç¨³å¥æ€§ï¼Œå…¶ç»“è®ºä¸ä¾èµ–äºç‰¹å®šç‰©ç†æ¨¡æ‹Ÿå™¨ã€æœºå™¨äººæ‰‹æ¨¡å‹æˆ–æœºå™¨å­¦ä¹ ç®—æ³•ã€‚ç ”ç©¶ç»“æœæ—¢æ­ç¤ºäº†å…­ç§ä¼ æ„Ÿå™¨é…ç½®ä»¿çœŸä¸­çš„ç‰¹å®šç³»ç»Ÿæ•ˆåº”ï¼Œä¹Ÿå‘ç°äº†è·¨ç³»ç»Ÿçš„æ™®éè§„å¾‹ï¼Œå¹¶ç¡®å®šå…¶ä¸­ä¸€ç§é…ç½®åœ¨ä¸¤ç§å®éªŒç³»ç»Ÿä¸­å‡èƒ½æŒç»­æä¾›æœ€ä¼˜æ€§èƒ½ã€‚è¿™äº›å‘ç°å¯ä¸ºæœªæ¥æœºå™¨äººæ‰‹éƒ¨ï¼ˆåŒ…æ‹¬å‡è‚¢ï¼‰çš„è®¾è®¡ç ”ç©¶æä¾›å‚è€ƒã€‚",
    "url": "https://arxiv.org/abs/2601.10268v1",
    "html_url": "https://arxiv.org/html/2601.10268v1",
    "html_content": "The impact of tactile sensor configurations on grasp learning efficiency - a comparative evaluation in simulation\nEszter Birtalan\nCorresponding author: birtalan.eszter@itk.ppke.hu\nFaculty of Information Technology and Bionics, PÃ¡zmÃ¡ny PÃ©ter Catholic University, Budapest, Hungary\nMiklÃ³s Koller\nFaculty of Information Technology and Bionics, PÃ¡zmÃ¡ny PÃ©ter Catholic University, Budapest, Hungary\nAbstract\nTactile sensors are breaking into the field of robotics to provide direct information related to contact surfaces, including contact events, slip events and even texture identification. These events are especially important for robotic hand designs, including prosthetics, as they can greatly improve grasp stability. Most presently published robotic hand designs, however, implement them in vastly different densities and layouts on the hand surface, often reserving the majority of the available space. We used simulations to evaluate 6 different tactile sensor configurations with different densities and layouts, based on their impact on reinforcement learning. Our two-setup system allows for robust results that are not dependent on the use of a given physics simulator, robotic hand model or machine learning algorithm. Our results show setup-specific, as well as generalized effects across the 6 sensorized simulations, and we identify one configuration as consistently yielding the best performance across both setups. These results could help future research aimed at robotic hand designs, including prostheses.\nFigure 1\n:\nSchematic overview of the methodology used to evaluate tactile sensor configurations.\nThe same configurations have been implemented on two 3D hand models using different physics simulators. The effects of the different sensor layouts were evaluated using two reinforcement learning algorithms to ensure the robustness of the results and compared against configurations, setups and control groups.\nIntroduction\nDexterous manipulation and grasping have been captivating fields of research in robotics, as they are useful both in autonomous robots made for complex task solving and also in the field of robotic prosthetics. Object manipulation, especially in the dark, is still a task that humans perform significantly better than robots do. Early approaches to solve this problem aimed to describe the exact model of contact, position, and motion parameters, as well as the control framework and motion planning, which worked reliably and well for low complexity environments and tasks\n[\n27\n]\n.\nWith the increase in complexity, other solutions relying on machine learning, such as deep learning and deep reinforcement learning, have become more widespread\n[\n30\n,\n29\n,\n4\n]\n. Unlike with previous approaches, here the exact state of the robot is not known, but may be inferred by the use of sensors. Traditionally, visual sensors and proprioceptive information have been used for state estimation\n[\n26\n]\n. In more recent works, however, tactile sensing for object interaction has been shown to be of great value\n[\n17\n]\nin addition to other modalities. For example, visual feedback may be used for state estimation while tactile sensors are used for slip detection and correction\n[\n32\n,\n18\n]\n, or the combination of proprioceptive and tactile data may be used to boost learning and performance for object manipulation tasks, foregoing visual information altogether\n[\n17\n]\n. A notable advantage of tactile sensors is their variability and adaptability for various environments. Sensor mechanisms may range from simple piezo-electric\n[\n20\n,\n21\n,\n9\n]\n, piezo-resistive\n[\n6\n]\n, optical sensors\n[\n12\n,\n33\n]\n, to complex sensor designs with intricate 3D structures\n[\n14\n]\nmeasuring light scattering to derive information about contacts. Modern designs also allow for high-density solutions involving sensor arrays with conductive ink\n[\n6\n]\n, or vision-based contact sensing, transforming up to 70% of the palmar side of the hand to a sensing surface\n[\n35\n]\n.\nWith limited space on a robotic hand, serious considerations need to be made when choosing between the available sensor types. For autonomous robots, high coverage and high density solutions seem to be preferred\n[\n35\n,\n24\n,\n11\n,\n19\n]\n; however, in the case of robotic prosthetics, low-resolution, array-like, or even single-point tactile sensing could be more advantageous\n[\n7\n,\n22\n,\n20\n,\n21\n,\n1\n,\n34\n,\n5\n]\n. It is important to note that in nearly all of these examples, both the coverage and the number or positioning of contact sensing elements vary greatly, with no clear consensus.\nNavaraj et al.\n[\n20\n,\n21\n]\ndeveloped a novel tactile sensor capable of both static and dynamic sensing, mimicking the fast- and slow-adapting mechanoreceptors of the human hand. One of these sensors covered most of the area of a single phalanx and has not been implemented on the palm.\nCipriani et al.\n[\n7\n]\nincluded only 4 tactile sensors in total in their hand design, focusing them on the area of the hand that interacts most with objects: the thumb and index fingers.\nThe flexible matrix electrode developed by Abbass et al.\n[\n1\n]\nmay contain 4, up to 16 sensors, which were screen-printed onto a flexible substrate. Their proposed sensor distribution on a prosthetic hand included 7 sensors on the index fingertip, and 4-4 sensors on the middle and proximal phalanges in order to participate in sensory feedback for the user.\nZhang et al.\n[\n34\n]\ndeveloped their own prosthetic hand, equipped with 13 contact sensing units aimed at selecting appropriate grasping force and performing slip detection for a more stable grasp. The tactile sensing occurred on fingertips only, which contained all 13 sensing units.\nOsborn et al.\n[\n22\n]\nfocused on developing a new type of tactile sensing that could create a low-cost, adaptable sensor array designed specifically for use on any type of prosthetic limb. The sensing cuffs could be made to any dimensions, achieving a variety of spatial resolutions based on need. The stretchable cuffs could be attached to the phalanges and to the palm as well, resulting in high coverage across the hand.\nUnlike in autonomous robots or more general robotic-arm designs and simulations, tactile sensors in prosthetic hands focus more on sensory feedback or slip detection, however, it would be feasible to involve them in grasp generation as well. To facilitate this, we investigated the effect of tactile sensor density and layout on reinforcement learning using hand models that may be used as prostheses (Modular Prosthetic Limb\n[\n13\n]\nand Shadow Dexterous Hand\n[\n8\n]\n). These investigations could allow future research to direct fewer resources into sensor layout design, and avoid potential dead-ends. In this research we focused specifically on grasping motions, as these are commonly offered by robotic prosthetic limbs, and evaluated configurations of up to 9 sensors per phalanx, to keep possible implementation costs low. Aside from the 9-sensor configuration, all our other layouts permit space for other sensors to be possibly implemented, much like in the works of Cipriani et al.\n[\n7\n]\n.\nA possible hurdle that may affect such projects with machine learning is that simulation results may be applicable only to the exact setup used. Either due to the physical parameters of the models, the inner workings of the physics simulator, the machine learning algorithm, or other, unknown factors. To make our results more robust, we created two simulated setups, which differ in every aspect except for the task, sensor configurations, and sensor output (Fig.\n1\n). The first setup, created using the PyBullet\n[\n10\n]\nphysics simulator, included the Modular Prosthetic Limb (MPL) and a Proximal Policy Optimization (PPO) algorithm\n[\n25\n]\n. The second setup, made in the MuJoCo\n[\n28\n]\nphysics simulator, included the Shadow Dexterous Hand (Shadow Hand), and a combination of Deep Deterministic Policy Gradient\n[\n16\n]\nand Hindsight Experience Replay\n[\n3\n]\n(DDPG+HER), based on the works of Melnik et al.\n[\n17\n]\n. The task of grasping and lifting a small cube remained the same across the setups, as did the layouts of the tactile sensors and their Boolean-type output. We thought it imperative that the task should include a hand model facing palm down, as in such a case, any grasping technique that wasnâ€™t forming a stable grip would result in a failed task due to the object dropping. To mitigate the variability in learning performance as a result of this type of experimental setup, we used bootstrapping methods to analyze our data. Our results show that it is possible to positively benefit from tactile sensors using a lower-resolution layout, thus allowing space to include other sensor modalities in the prosthetic device, and that sensor layout affects learning results significantly, even if the number of sensors used remains the same.\nFigure 2\n:\nVisual representation of the sensor configurations implemented in both setups.\nA-F: Sensor configurations corresponding to Configurations 1-6, respectively. The number and layout of the sensors shown on the index finger are repeated across the other digits.\n1\nResults\nFigure 3\n:\nSample-efficiency curves showing success rates as a function of epochs in the PyBullet setup.\nA-F: Sample-efficiency curves corresponding to Configurations 1-6 (shown on diagrams in the right hand corner of each panel), respectively. Thick lines show the IQM scores (bootstrap replication 50000), with the confidence intervals represented as shaded areas. Yellow is used to show results from the sensorized group, while blue is used for the controls. Training occurred for 500000 timesteps, roughly generating 500 epochs. Both main and control groups represent the result of 10 seeds.\nWe evaluated the effects of sensor placement on reinforcement learning by establishing two simulated setups, where each agent learns to grasp and lift a cube using different hand models and learning algorithms, while keeping the sensor configurations the same. During reinforcement learning, each training episode begins with the object (a cube) placed directly under the palm of the hand model, which has its fingers in a slightly curved position. The agent needs to learn to establish a stable grasp around the object, that prevents a drop when the hand is lifted. If the grasp proves to be unsecure, the object will fall to the ground, and the task cannot be completed. This way the agent has a much lower chance to learn semi-secure holds, at the cost of larger deviations in task success. We train with 6 different sensor configurations (Figure\n2\n) to observe the effects of different sensor densities, sensor locations (Figure\n2\nB and D), sensing in the middle of the phalanx (Figure\n2\nC and E), and finally a single sensor design (Figure\n2\nF). Each configuration is complemented with 4 sensors on the palm, placed at the base of each finger, excluding the thumb. Thus, Configurations 1-6 have a total of 139, 79, 64, 79, 19, and 19 tactile sensors, respectively. In the presence of active sensor measurements (called main run type), the sensors return Boolean values based on activation, while in absence of active measurements (no real tactile data, this is called control run type) these values are â€™no contact detectedâ€™ for each query. With this control scheme, we ensure that the input layer of the neural network always receives a matrix of the same size within configurations, regardless of run type (main or control).\nLearning results are evaluated through success-rate measurement, which calculates the ratio of successful episodes to all episodes for each epoch (PPO) or each test cycle (DDPG+HER), based on the algorithm. We use the RLiable Python library to compute stratified bootstrap with 95% confidence intervals for our aggregated performance metrics (Median, Interquartile Mean - IQM, and Mean for converged success-rates) across seeds, following the methodology of Agarwal et al.\n[\n2\n]\n. Converged performance is calculated according to Melnik et al.\n[\n17\n]\n, by transforming the learning curves to histograms projected onto the Y axis and taking the performance value paired with the largest bin of the histogram. We also include sample-efficiency curves generated by RLiable to visualize the learning curves for each configuration by presenting bootstrapped IQM scores as a function of epochs.\nDue to the long training time, and several configurations used, we were unable to produce large sample sizes. Mitigation of issues arising from this sample size was our reasoning behind choosing the stratified bootstrap method. This way, we can resample our data to generate thousands of resampled versions from a single case of Configuration and run-type combination and calculate the main metrics (median, IQM, mean) for each. Then we calculate the confidence interval (CI) by keeping only the central 95% of the bootstrap distribution. This reduces the influence of rare resampling artifacts while providing a robust estimate of the variability of the performance metrics under repeated experiments. We use these intervals to qualify how well our main metrics represent the expected performance, with small CIs representing stable, and wide CIs representing volatile results, where repeated experiments may yield a wide range of performances. We also use the overlap between these CIs to measure how well the performance of two groups separate from each other to allow qualitative and indicative comparisons. This is especially crucial as data from reinforcement learning is often non-symmetric and non-parametric, making classical statistical comparisons poorly suited.\n1.1\nPyBullet results\nResults of learning over time in the PyBullet setup show that initial learning is negatively affected by the lack of the central sensor when using Configuration 3 (Figure\n3\nC) with tactile information. Other than this, initial learning does not appear to differ between sensorized (main group) and control versions (control group). After the initial phase, however, a distinct separation can be observed between runs with and without tactile information. In the cases of Configurations 1, 2, and 6, runs with tactile information perform consistently better than control versions (Figure\n3\nA, B, F), whereas with Configuration 3, this effect shows up delayed, affecting only the very end of training (Figure\n3\nC). Runs with Configurations 4 and 5 appear to perform similarly regardless of the addition of tactile information or not (Figure\n3\nD, E). Based on the results of the learning curves, we conclude that adding tactile information resulted in better overall performance in 3 of the 6 configurations (Configurations 1, 2, and 6), and that the missing central tactile sensor in Configuration 3 caused a noticeable negative effect on learning.\nThe bootstrapped converged metrics for both sensorized and control versions are shown in Figure\n4\n, and their values are collected in Table\n1\n. The RLiable library calculates the Interquartile Mean (IQM) by discarding the lowest and highest 25% of the data to provide a clearer picture of performance, which is not skewed by large outliers. In this next section, we will take a look at how the converged performances compare between sensorized and control, and also between the various sensorized versions, using IQM values and their confidence intervals (CIs).\nFigure 4\n:\nVisual representation of the converged success rates in the PyBullet setup.\nA-B: The Median, IQM and Mean of the converged success rates (bootstrap replication 1000) for each Configuration in the sensorized and control groups, respectively. Confidence intervals are represented as shaded areas. Each configuration was run with 10 seeds; the exact values represented here can be found in Table\n1\n.\nUsing Configuration 1, the main group achieved a success rate of 65%, representing a 3% improvement over the control, with only 38% overlap between the corresponding confidence intervals (CIs). CI overlap is always calculated relative to the control, meaning what percentage of the control groupâ€™s CI overlaps with the main groupâ€™s CI. With a similar CI overlap of 39%, the main group of Configuration 2 converged to a 66% success rate compared to 61% for the control. The CI for the control group of Configuration 3 fell entirely within the CI of the main group, which had a 1% higher success rate. The main group of Configuration 4 achieved a success rate of 59%, compared to 64% for the control, with a 52% overlap in CIs. Configuration 5 had similar results, with a 93% overlap between CIs, achieving 57% and 61% success rates for the main and control groups, respectively. Finally, using Configuration 6, the main group performed with a 64% success rate, 4% higher than the control group, with a 24% overlap in CIs compared to the control.\nComparing the results from the main groups shows that despite the different number of sensors used in Configuration 1 (139) and Configuration 2 (79), the agent achieved similar success rates (65 and 66%, respectively), with similarly sized CIs (6-7%). The performance of Configuration 3, with the missing middle sensor, was not far off with a 64% success rate; however, its CI was the highest among all the groups, 18%. Configuration 4 had the same amount of tactile information as Configuration 2; however, the different sensor layout resulted in a lower, 59% success rate. The size of its CI was 7%, only 1% off from Configuration 2.\nConfigurations 5 and 6 had a single central sensor on each phalanx, with different activation area sizes. Configuration 5, with the smaller activation area, performed with the lowest success rate among the main groups, and had the second highest CI, 57% and 11%, respectively. Configuration 6, with the larger activation area, on the other hand, performed close to Configurations 1 and 2 with a 64% success rate and an 8% CI.\nFigure 5\n:\nSample-efficiency curves showing success rates as a function of epochs in the MuJoCo setup.\nA-F: Sample-efficiency curves corresponding to Configurations 1-6 (shown on diagrams in the right hand corner of each panel), respectively. Thick lines show the IQM scores (bootstrap replication 50000), with the confidence intervals represented as shaded areas. Yellow is used to show results from the sensorized group, while blue is used for the controls. Training occurred for 5000000 timesteps, generating 500 epochs. Both main and control groups represent the result of 10 seeds.\n1.2\nMuJoCo results\nResults of the MuJoCo environment, paired with a DDPG+HER algorithm, generated larger CIs (16-30%) than the PyBullet environment with PPO. Therefore, learning tendencies based on the efficiency plots (Figure\n5\n) can be inferred in a more limited manner. Runs with tactile information still outperform control versions at the end of training when using Configurations 1 and 2 (Figure\n5\nA,B), whereas Configuration 5 shows a great reduction in the main groupâ€™s performance(Figure\n5\nE). Configurations 2, 3, and 6, on the other hand, do not show clear tendencies for group separation (Figure\n5\nB, C, F).\nTherefore, to better analyze these results, we should look at the converged success rates using bootstrapping, shown in Figure\n6\nand Table\n2\n. The IQM of the converged success rate for the main group using Configuration 1 was only 0.8% higher than the control, and its CI covered in its entirety the CI of the control group. The main group of Configuration 2, however, performed with a 7.5% higher success rate than its control, with a 65% overlap in CIs. The IQM success rate using Configuration 3 was the same for both main and control groups, with a 79% overlap in CIs. With the same amount of overlap, the main group using Configuration 4 achieved a 0.8% lower success rate than its control. The main group of Configuration 5 had a decidedly lower performance than the control group, with a success rate 12.5% lower than the control, and only 42% overlap between CIs. Finally, the control group, using Configuration 6, performed with a success rate 1.7% higher than the main group, with 100% of its CI overlapping.\nWhen comparing the results of the main groups in the MuJoCo environment, we can see that Configuration 1 did not perform as similarly to other configurations as in the PyBullet environment. With a success rate of 84% and a CI of 23%, it underperformed relative to Configurations 2 and 3. Configuration 2, with fewer tactile sensors, achieved the highest success rate among all groups, including controls, with an 88% success rate and a 17% CI. Configuration 3, with the missing central sensor in this setup, performed with a similar success rate and CI range as Configuration 2, 87% and 16%, respectively. Configuration 4, which had the same number of sensors as Configuration 2, had a lower success rate with 84%, and a similar CI, with 18%, showing again how the different sensor layout affects learning. Configurations 5 and 6, which had a single central sensor, performed worse than other configurations, both in terms of success rate and CI. Configuration 5, with the small central sensor, performed worse with only a 73% success rate, paired with a CI of 28%. Compared to this, Configuration 6 did better, with a 78% success rate and a CI of 30%.\nFigure 6\n:\nVisual representation of the converged success rates in the MuJoCo setup.\nA-B: The Median, IQM and Mean of the converged success rates (bootstrap replication 1000) for each Configuration in the sensorized and control groups, respectively. Confidence intervals are represented as shaded areas. Each configuration was run with 10 seeds; the exact values represented here can be found in Table\n2\n.\nWhen looking at the results from both setups side-by-side (still analyzing IQMs), we can see that the main difference is in the average values for success rates and CIs. Although the MuJoCo setup resulted in higher success rates on average, it also generated larger CIs and overlaps between them. In both setups, Configurations 1 and 2 had higher success rates with sensory input, whereas Configurations 4 and 5 had higher success rates for the controls. Configuration 5 had the lowest performance, achieving the lowest success rate and high CI using sensors. Configuration 2, on the other hand, achieved the highest success rate in both setups, with a relatively small CI, compared to other configurations in the setup. The same configuration also achieved the highest lower and upper brackets of CIs in either setup. Another phenomenon that remained the same in both cases was the different performances of Configurations 2 and 4. These two had the same number of tactile sensors installed, but were positioned to produce different layouts, both of which included a central sensor. With their results, we can see how a different layout alone affects learning, as Configuration 4 had a 4-7% worse success rate, depending on the setup, without a change in the size of CIs.\n2\nDiscussion\nIn this paper, we presented results on how different tactile sensor densities and layouts affect the performance of a machine learning agent when executing a grasp-and-lift task using a 3D hand model. With increasing interest in tactile sensors, new research is needed to investigate how this new modality may be integrated into existing robotic control schemes. These experiments aimed to support the use of tactile sensors in robotic hand control schemes by investigating their effects in simulated environments. We created two separate setups in two of the most commonly used simulators: PyBullet and MuJoCo. The different setups not only ensure that our results are robust, it also makes them relevant to a wider audience. The setup in PyBullet was made with the Modular Prosthetic Limb by Johns Hopkins\n[\n13\n]\nand a Proximal Policy Optimization\n[\n25\n]\nalgorithm, which has been used increasingly in machine learning. This setup yielded results with fairly small variations, providing a clear insight into the positive effect of tactile information as well as showcasing how different sensor densities and layouts affect learning. The MuJoCo setup, on the other hand, used the Shadow Dexterous hand\n[\n8\n]\n, a widely used hand model, and a combination of Deep Deterministic Policy Gradient and Hindsight Experience Replay\n[\n3\n]\n, which yielded, on average, higher success rates, but also gave more variable results, as a whole.\nAll generated data were analyzed following the methods proposed by Agarwal et al.\n[\n2\n]\n, using the RLiable Python library and Melnik et al.\n[\n17\n]\nto calculate converged success rates. Both setups used 6 different sensor layouts with varying densities, with control run type versions matching the number of sensors, but returning no tactile information. With the task being considered completed if the grabbed object is held securely above the ground, the agents learned how to pick up a cube with no visual feedback, only proprioceptive, or proprioceptive and tactile data. Our results have shown that, although the task is learnable even without tactile information, including it resulted in better performance using Configurations 1-3 and 6 in the PyBullet setup and Configurations 1-2 in the MuJoCo setup. In both setups, Configuration 2 performed the best out of all groups and configurations. This is an especially influential finding, as it performed better than Configuration 1, which had a higher density of sensors. This aligns with the findings of Melnik et al.\n[\n17\n]\n, namely that there is a certain density, after which performance no longer increases with the number of tactile sensors. This also suggests that a robotic hand does not need to be covered maximally with tactile pads to benefit from their addition; some space can be reserved for other modalities without sacrificing performance. We also showed that two configurations with matching sensor numbers performed differently, with Configuration 2 outperforming Configuration 4, where the only distinction was the layout of sensors. Although our results already show some effects that persist across different setups, they also highlight the benefits of further exploring this subject. With the variability of tactile sensors, virtually any layout and density may be simulated and tested, decreasing the amount of individual experimentation needed for robotic hand development. Utilizing additional learning algorithms, control schemes, and hand models would also shine light on which effects are robust across certain or even all setups.\nWe identified several unexplored directions that may further expand on our experiments, but did not have the capacity to address. Our tuning of the learning algorithms extended only as much as it was needed to ensure that learning occurred, but no optimization was conducted. For example, each configuration was taught with the same neural network, which means that the size of the network may have been optimal for some configurations but sub-optimal for others. We hoped to visualize these differences through the control-type runs, as the results of these versions should only be affected by how much the neural network size is compatible with the size of the observation matrices. We also debated about the proper way to define the control type setups. Keeping the observation matrices the same by including an all-zero sensor output seemed to match the investigative nature of our work best, whereas deducting the sensor data altogether seemed the most realistic choice. For the sake of easy comparisons, we chose to include the all-zero sensor data; however, we acknowledge that the complete absence of it would be a viable alternative as well, especially when looking at a sim-to-real transfer of these experiments. Our future work involves introducing other objects into the setup to examine the effects of tactile sensors on learning to lift objects of different shapes, which may require inherently different grasp types. We are also working towards equipping the Shadow Hand model with a soft covering on the palmar side, as an additional, passive tool to aid in the formation and maintenance of secure grips on objects. Finally, we are also considering the implementation of a second version of control-type runs that would not include the zeroed sensor data to better cater to real-life implementation methods.\n3\nMethods\n3.1\nPyBullet setup\nIn the PyBullet\n[\n10\n]\nsetup, we used the Modular Prosthetic Limb (MPL,\n[\n13\n]\n), which originally had 26 DoF when looking at the whole arm. From this model, we only used the hand, that was modified to allow for the continuous movement of each of the fingers along a given trajectory through PID control and to allow movement for the whole hand in all three axes of the 3D space. The fixed trajectories of the fingers were implemented to reduce the size of the action space, and facilitate learning. Each finger can be used to bend or straighten, but individual joint positions cannot be set. We also found it necessary to couple the actions of the thumb and little fingers, as this seemed to be the cornerstone that allowed for learning to begin. With that, possible actions of the hand model included: hand movements along the x, y and z axes, fingers 1 and 5 bending together, finger 2 bending, finger 3 bending and finger 4 bending, creating an action space of size 7. State observation included the object ID, Cartesian positions of both the robot and the object, as well as the states of the first joints of the five fingers. It also included the touch-sensor output (139/79/64/79/19/19 based on sensor configurations, as described below), which was set to zero in the control versions to ensure a consistent observation matrix size across the run types for each configuration.\nThe custom-made tactile sensors returned 1 if a contact point between the hand model and the environment (excluding the ground) fell within their sensing area, and 0 if it did not. On the phalanges, the sensor radius, the spaces between the rows and columns were 2 mm, 4.5 mm, 6.5 mm, respectively. (Please note that the default unit of the XML file format is in meters, and values should be adjusted accordingly.) Sensors on the palm had the same radius and were positioned at the base of the index, middle, ring, and little fingers. Figure\n2\nvisualizes the 6 Configurations in which the tactile sensors were arranged. For Configuration 6, the sensor radius was set to 3.8 mm. Please note that the pattern shown on the index finger were repeated on all fingers, resulting in a total of 139, 79, 64, 79, 19 and 19 tactile sensors in Configurations 1-6, respectively.\n3.1.1\nReinforcement learning algorithm\nLearning in the PyBullet setup is carried out by a Proximal Policy Optimization\n[\n25\n,\n31\n]\n(PPO) algorithm. The main parameters of the implemented version of PPO were as follows:\nâ€¢\nActor network: 3\nÃ—\n\\times\n256, lr =\n3\nÃ—\n10\nâˆ’\n4\n3\\times 10^{-4}\n, ReLU, Tanh output\nâ€¢\nCritic network: 3\nÃ—\n\\times\n256, lr =\n1\nÃ—\n10\nâˆ’\n3\n1\\times 10^{-3}\n, ReLU, Identity output\nâ€¢\nAdam optimizer\n[\n15\n]\nâ€¢\nAction distribution: Gaussian, diagonal covariance (\nÏƒ\n2\n=\n0.05\n\\sigma^{2}=0.05\n)\nâ€¢\nDiscount factor:\nÎ³\n=\n0.99\n\\gamma=0.99\nâ€¢\nAdvantage estimator: GAE (\nÎ»\n=\n0.97\n\\lambda=0.97\n)\nâ€¢\nClip range: 0.2\nâ€¢\nMinibatch size: 6\nâ€¢\nMax. timesteps per episode: 100\nâ€¢\nTimesteps per epoch: 1000\nâ€¢\nTotal timesteps: 500000\nâ€¢\nUpdates per epoch: 80\nâ€¢\nEntropy coefficient: 0.0\nâ€¢\nGradient clipping: 0.5\nThe learned policy outputs an action vector of size 7 to control the hand and its fingers, which contains one pair of coupled digits (thumb and little fingers). The values range from -1 to 1, which are implemented directly, in relation to the current position and within the limits of the hand model. Episodes begin with partially closed fingers, and the object is placed directly under the hand model. The task requires the hand to grab and lift the object, keeping it securely in the air for 5000 unrecorded simulation steps. The reward function is one of the most crucial parts of a PPO algorithm. In our case, the reward function was built to return different values based on task progression. While the object has not been moved from the ground, the agent receives a reward based on the closeness of the hand and fingers to the object center.\nR\n=\nexp\nâ¡\n(\nâˆ’\nâ€–\nğ¡ğšğ§ğ\nâˆ’\nğ¨ğ›ğ£\nâ€–\n2\n)\n+\nÎ±\nâ€‹\nâˆ‘\ni\n=\n1\n5\nexp\nâ¡\n(\nâˆ’\nâ€–\nğŸ\ni\nâˆ’\nğ¨ğ›ğ£\nâ€–\n2\n)\nR=\\exp\\Big(-\\|\\mathbf{hand}-\\mathbf{obj}\\|^{2}\\Big)+\\alpha\\sum_{i=1}^{5}\\exp\\Big(-\\|\\mathbf{f}_{i}-\\mathbf{obj}\\|^{2}\\Big)\nwhere\nÎ±\n\\alpha\n=0.2,\nhand\nand\nobj\nare the Cartesian coordinates of the hand and the object, respectively, and\nf\ndenotes the Cartesian coordinates of the distal phalanges. Alpha is used to scale the rewards from the fingers so that their maximal value cannot exceed 1. If there was a failed attempt at lifting the object off the ground, the agent will receive an additional reward of 5. If the object was lifted into the air successfully, the hand is moved to a height of 1 meter and kept there for 5000 unrecorded simulation steps. After this, if the object is still in the air, the agent is given a reward of 1000, and the episode ends. If the object is dropped, it will still receive the extra reward of 5, and the episode ends. If the episode reaches the maximal timesteps per episode, the episode ends, and the agent receives a reward of -100. We found that this reward scheme encourages object manipulation without enabling the option to get high rewards without completing the task.\n3.2\nMuJoCo setup\nIn the MuJoCo\n[\n28\n]\nsetup, we used the Shadow Dexterous Hand (Shadow Hand)\n[\n8\n]\nfrom the codebase of Melnik et al.\n[\n17\n]\n. This model originally had 24 DoF, which we extended to allow movement in 3D space. Possible actions included joint values for the five fingers (20) and hand movement along the x,y, and z axes to create an action space of size 23. State observation included the angle values and velocities of the robotâ€™s joints, as well as the Cartesian position and rotation of the object. It also included the touch-sensor output (139/79/64/79/19/19 based on sensor configurations, as described below), which was set to zero in the control versions to ensure a consistent observation matrix size across runs for each configuration.\nThe touch sensors in this setup are built-in MuJoCo touch sensors that return Boolean values for each query. Due to variations in phalange size, the sensor radius and inter-column spacing were fixed at 1.5 mm and 5 mm in this setup, whereas the inter-row spacing was set to 6 mm, 3 mm, and 7.5 mm for the proximal, middle, and distal phalanges, respectively. Sensors on the palm had the same radius and were positioned at the base of the index, middle, ring, and little fingers. Figure\n2\nvisualizes the 6 Configurations in which the tactile sensors were arranged. For Configuration 6, the radius of the sensor was set to 3.5 mm. Please note that the pattern shown on the index finger were repeated on all fingers, resulting in a total of 139, 79, 64, 79, 19 and 19 tactile sensors in Configurations 1-6, respectively.\n3.2.1\nReinforcement learning algorithm\nLearning in the MuJoCo setup is carried out by a combination of Deep Deterministic Policy Gradient\n[\n16\n]\n(DDPG) and Hindsight Experience Replay\n[\n3\n]\n(HER) algorithms as described by Melnik et al.\n[\n17\n]\n(2021) and Plappert et al.\n[\n23\n]\n(2018). The hyperparameters were as follows\n[\n17\n]\n:\nâ€¢\nActor and critic networks: 3 layers with 256 units each and ReLU non-linearities\nâ€¢\nAdam optimizer\n[\n15\n]\nwith\n10\nâˆ’\n3\n10^{-3}\nfor training both actor and critic\nâ€¢\nBuffer size:\n10\n6\n10^{6}\ntransitions\nâ€¢\nPolyak-averaging coefficient: 0.95\nâ€¢\nAction L2 norm coefficient: 1.0\nâ€¢\nObservation clipping:\n[\nâˆ’\n200\n,\n200\n]\n[-200,200]\nâ€¢\nBatch size: 256\nâ€¢\nRollouts per MPI worker: 2\nâ€¢\nNumber of MPI workers: 19\nâ€¢\nTotal timesteps: 5000000\nâ€¢\nCycles per epoch: 50\nâ€¢\nBatches per cycle: 40\nâ€¢\nTest rollouts per epoch: 10\nâ€¢\nProbability of random actions: 0.3\nâ€¢\nScale of additive Gaussian noise: 0.2\nâ€¢\nProbability of HER experience replay: 0.8\nâ€¢\nNormalized clipping:\n[\nâˆ’\n5\n,\n5\n]\n[-5,5]\nThe policy output includes 23 continuous values in the range of -1 and 1 to control the 23 actuated joints of the hand model. These normalized values are converted to joint positions using linear mapping (actuation center + action Ã— actuation range/2)\n[\n17\n]\n. Episodes begin with partially closed fingers, and the object is placed directly under the hand model. The goal position is always located above the hand, so that no part of the object or the hand may touch the ground when reached. The task requires the hand to grab and move the object to the target location and keep it there until the end of the episode. Rewards are given at each timestep according to the distance between the objectâ€™s current and desired locations, with a scaling of 10. Rotation of the object is not considered in neither the reward shaping nor goal achievement. A reward of 0 is given if the object is delivered to the target location, with some tolerance\n[\n17\n]\n. A reward of -1 is given if the target has not been reached by the end of the episode. Both the starting and goal locations (and orientations) are set to be the same across all runs. Simulations were run with 2 parallel environments per MPI worker.\nAcknowledgements\nThis work was supported by the Ministry of Culture and Innovation of Hungary from the National Research, Development and Innovation Fund, financed under the TKP2021-NKTA funding scheme (project no. TKP2021-NKTA-66) and under the KDP-2023 funding scheme (project no. 2023-2.1.2-KDP-2023-00011 C2299141). We would also like to thank the contributions of Domonkos KÃ¶rmendy and Jedlik Innovation LLC to the realization of this paper.\nAuthor contributions\nM.K. and E.B. formed the underlying idea and design of the research. With the supervision of M.K. E.B. developed the code, ran the simulations and analyzed the results. E.B. wrote the manuscript, and M.K. reviewed and consulted on its content.\nCompeting interests\nAll authors declare no financial or non-financial competing interests.\nData availability\nThe data generated and analysed during the current study will be made available in a Zonedo repository.\nCode availability\nThe underlying code for this study is not publicly available but may be made available to qualified researchers on reasonable request from the corresponding author.\nReferences\n[1]\nY. Abbass, M. Saleh, S. Dosen, and M. Valle\n(2021-10)\nEmbedded electrotactile feedback system for hand prostheses using matrix electrode and electronic skin\n.\n15\n(\n5\n),\npp.Â 912â€“925\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[2]\nR. Agarwal, M. Schwarzer, P. S. Castro, A. Courville, and M. G. Bellemare\n(2022-01-05)\nDeep reinforcement learning at the edge of the statistical precipice\n.\narXiv\n.\nExternal Links:\nDocument\n,\n2108.13264 [cs]\nCited by:\nÂ§1\n,\nÂ§2\n.\n[3]\nM. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba\n(2018-02-23)\nHindsight experience replay\n.\narXiv\n.\nExternal Links:\nDocument\n,\n1707.01495 [cs]\nCited by:\nÂ§2\n,\nÂ§3.2.1\n,\nIntroduction\n.\n[4]\nO. M. Andrychowicz, B. Baker, M. Chociej, R. JÃ³zefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba\n(2020-01-01)\nLearning dexterous in-hand manipulation\n.\n39\n(\n1\n),\npp.Â 3â€“20\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[5]\nG. Cannata and M. Maggiali\n(2008)\nDesign of a tactile sensor for robot hands\n.\nIn\nSensors - Focus on Tactile Force and Stress Sensors\n,\nJ. G. Rocha and S. Lanceros-Mendez (Eds.)\n,\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[6]\nJ. Castellanos-Ramos, R. Navas-Gonzalez, H. Macicior, E. Ochoteco, and F. Vidal-VerdÃº\n(2009-05-18)\nTactile sensors based on conductive polymers\n.\nIn\nSmart Sensors, Actuators, and MEMS IV\n,\nVol.\n7362\n,\npp.Â 140â€“148\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[7]\nC. Cipriani, M. Controzzi, and M. C. Carrozza\n(2011-05-22)\nThe SmartHand transradial prosthesis\n.\n8\n(\n1\n),\npp.Â 29\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n,\nIntroduction\n.\n[8]\nS. R. Company\n(2013)\nShadow dexterous hand technical specification\n.\nNote:\nhttps://www.shadowrobot.com/products/dexterous-hand\nCited by:\nÂ§2\n,\nÂ§3.2\n,\nIntroduction\n.\n[9]\nD. P. J. Cotton, P. H. Chappell, A. Cranny, N. M. White, and S. P. Beeby\n(2007-05)\nA novel thick-film piezoelectric slip sensor for a prosthetic hand\n.\n7\n(\n5\n),\npp.Â 752â€“761\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[10]\nE. Coumans and Y. Bai\n(2016)\nPyBullet, a python module for physics simulation for games, robotics and machine learning\n.\nIn\nhttp://pybullet.org\n,\nCited by:\nÂ§3.1\n,\nIntroduction\n.\n[11]\nW. Fukui, F. Kobayashi, F. Kojima, H. Nakamoto, N. Imamura, T. Maeda, and H. Shirasawa\n(2011)\nHigh-speed tactile sensing for array-type tactile sensor and object manipulation based on tactile information\n.\n2011\n(\n1\n),\npp.Â 691769\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[12]\nJ. W. James, N. Pestell, and N. F. Lepora\n(2018-10)\nSlip detection with a biomimetic tactile sensor\n.\n3\n(\n4\n),\npp.Â 3340â€“3346\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[13]\nM. S. Johannes, E. L. Faulring, K. D. Katyal, M. P. Para, J. B. Helder, A. Makhlin, T. Moyer, D. Wahl, J. Solberg, S. Clark, R. S. Armiger, T. Lontz, K. Geberth, C. W. Moran, B. A. Wester, T. Van Doren, and J. J. Santos-Munne\n(2020-01-01)\nChapter 21 - the modular prosthetic limb\n.\nIn\nWearable Robotics\n,\nJ. Rosen and P. W. Ferguson (Eds.)\n,\npp.Â 393â€“444\n.\nExternal Links:\nDocument\nCited by:\nÂ§2\n,\nÂ§3.1\n,\nIntroduction\n.\n[14]\nP. Kampmann and F. Kirchner\n(2014-04)\nIntegration of fiber-optic sensor arrays into a multi-modal tactile sensor processing system for robotic end-effectors\n.\n14\n(\n4\n),\npp.Â 6854â€“6876\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[15]\nD. P. Kingma and J. Ba\n(2017-01-30)\nAdam: a method for stochastic optimization\n.\narXiv\n.\nExternal Links:\nDocument\n,\n1412.6980 [cs]\nCited by:\n3rd item\n,\n2nd item\n.\n[16]\nT. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra\n(2019-07-05)\nContinuous control with deep reinforcement learning\n.\narXiv\n.\nExternal Links:\nDocument\n,\n1509.02971 [cs]\nCited by:\nÂ§3.2.1\n,\nIntroduction\n.\n[17]\nA. Melnik, L. Lach, M. Plappert, T. Korthals, R. Haschke, and H. Ritter\n(2021-06-29)\nUsing tactile sensing to improve the sample efficiency and performance of deep deterministic policy gradients for simulated in-hand manipulation tasks\n.\n8\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n,\nÂ§2\n,\nÂ§3.2.1\n,\nÂ§3.2.1\n,\nÂ§3.2\n,\nIntroduction\n,\nIntroduction\n.\n[18]\nM. R. Motamedi, J. Chossat, J. Roberge, and V. Duchaine\n(2016-05)\nHaptic feedback for improved robotic arm control during simple grasp, slippage, and contact detection tasks\n.\nIn\n2016 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 4894â€“4900\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[19]\nT. Mouri, H. Kawasaki, K. Yoshikawa, J. Takai, and S. Ito\n(2002-01)\nAnthropomorphic robot hand: gifu hand iii\n.\nIn\nProc. of Int. Conf. ICCAS2002\n,\npp.Â 1288â€“1293\n.\nCited by:\nIntroduction\n.\n[20]\nW. T. Navarai, O. Ozioko, and R. Dahiya\n(2018-10)\nCapacitive-piezoelectric tandem architecture for biomimetic tactile sensing in prosthetic hand\n.\nIn\nIEEE Sensors Journal\n,\npp.Â 1â€“4\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n,\nIntroduction\n.\n[21]\nW. T. Navaraj, H. Nassar, and R. Dahiya\n(2019-05)\nProsthetic hand with biomimetic tactile sensing and force feedback\n.\nIn\n2019 IEEE International Symposium on Circuits and Systems (ISCAS)\n,\npp.Â 1â€“4\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n,\nIntroduction\n.\n[22]\nL. Osborn, W. W. Lee, R. Kaliki, and N. Thakor\n(2014-08)\nTactile feedback in upper limb prosthetic devices using flexible textile force sensors\n.\nIn\n5th IEEE RAS/EMBS International Conference on Biomedical Robotics and Biomechatronics\n,\npp.Â 114â€“119\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[23]\nM. Plappert, M. Andrychowicz, A. Ray, B. McGrew, B. Baker, G. Powell, J. Schneider, J. Tobin, M. Chociej, P. Welinder, V. Kumar, and W. Zaremba\n(2018-03-10)\nMulti-goal reinforcement learning: challenging robotics environments and request for research\n.\nExternal Links:\nDocument\n,\n1802.09464 [cs]\nCited by:\nÂ§3.2.1\n.\n[24]\nA. Schmitz, P. Maiolino, M. Maggiali, L. Natale, G. Cannata, and G. Metta\n(2011-06)\nMethods and technologies for the implementation of large-scale robot tactile sensors\n.\n27\n(\n3\n),\npp.Â 389â€“400\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[25]\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov\n(2017-08-28)\nProximal policy optimization algorithms\n.\narXiv\n.\nExternal Links:\nDocument\n,\n1707.06347 [cs]\nCited by:\nÂ§2\n,\nÂ§3.1.1\n,\nIntroduction\n.\n[26]\nC. Shi, D. Yang, J. Zhao, and H. Liu\n(2020-09)\nComputer vision-based grasp pattern recognition with application to myoelectric control of dexterous hand prosthesis\n.\n28\n(\n9\n),\npp.Â 2090â€“2099\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[27]\nB. Sundaralingam and T. Hermans\n(2017)\nRelaxed-rigidity constraints: in-grasp manipulation using purely kinematic trajectory optimization\n.\n13\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[28]\nE. Todorov, T. Erez, and Y. Tassa\n(2012-10)\nMuJoCo: a physics engine for model-based control\n.\nIn\n2012 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp.Â 5026â€“5033\n.\nExternal Links:\nDocument\nCited by:\nÂ§3.2\n,\nIntroduction\n.\n[29]\nE. Valarezo AÃ±azco, P. Rivera Lopez, N. Park, J. Oh, G. Ryu, M. A. Al-antari, and T. Kim\n(2021-02-01)\nNatural object manipulation using anthropomorphic robotic hand through deep reinforcement learning and deep grasping probability network\n.\n51\n(\n2\n),\npp.Â 1041â€“1055\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[30]\nH. Xu, Y. Luo, S. Wang, T. Darrell, and R. Calandra\n(2022-10)\nTowards learning to play piano with dexterous hands and touch\n.\nIn\n2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 10410â€“10416\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[31]\nE. Yu\n(2023)\nPPO-for-beginners: a simple and well-styled ppo implementation\n.\nNote:\nhttps://github.com/ericyangyu/PPO-for-Beginners\nGitHub repository\nCited by:\nÂ§3.1.1\n.\n[32]\nK. Yu and A. Rodriguez\n(2018-05)\nRealtime state estimation with tactile and visual sensing. application to planar manipulation\n.\nIn\n2018 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 7778â€“7785\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[33]\nW. Yuan, S. Dong, and E. H. Adelson\n(2017-12)\nGelSight: high-resolution robot tactile sensors for estimating geometry and force\n.\n17\n(\n12\n),\npp.Â 2762\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[34]\nT. Zhang, L. Jiang, and H. Liu\n(2018-07)\nDesign and functional evaluation of a dexterous myoelectric hand prosthesis with biomimetic tactile sensor\n.\n26\n(\n7\n),\npp.Â 1391â€“1399\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n.\n[35]\nZ. Zhao, W. Li, Y. Li, T. Liu, B. Li, M. Wang, K. Du, H. Liu, Y. Zhu, Q. Wang, K. Althoefer, and S. Zhu\n(2025-06)\nEmbedding high-resolution touch across robotic hands enables adaptive human-like grasping\n.\n7\n(\n6\n),\npp.Â 889â€“900\n.\nExternal Links:\nDocument\nCited by:\nIntroduction\n,\nIntroduction\n.\n4\nFigure legends\nFigure 1. Schematic overview of the methodology used to evaluate tactile sensor configurations. The same configurations have been implemented on two 3D hand models using different physics simulators. The effects of the different sensor layouts were evaluated using two reinforcement learning algorithms to ensure the robustness of the results and compared against configurations, setups and control groups.\nFigure 2. Visual representation of the sensor configurations implemented in both setups. A-F: Sensor configurations corresponding to Configurations 1-6, respectively. The number and layout of the sensors shown on the index finger are repeated across the other digits.\nFigure 3. Sample-efficiency curves showing success rates as a function of epochs in the PyBullet setup. A-F: Sample-efficiency curves corresponding to Configurations 1-6 (shown on diagrams in the right hand corner of each panel), respectively. Thick lines show the IQM scores (bootstrap replication 50000), with the confidence intervals represented as shaded areas. Yellow is used to show results from the sensorized group, while blue is used for the controls. Training occurred for 500000 timesteps, roughly generating 500 epochs. Both main and control groups represent the result of 10 seeds.\nFigure 4. Visual representation of the converged success rates in the PyBullet setup. A-B: The Median, IQM and Mean of the converged success rates (bootstrap replication 1000) for each Configuration in the sensorized and control groups, respectively. Confidence intervals are represented as shaded areas. Each configuration was run with 10 seeds; the exact values represented here can be found in Table 1.\nFigure 5. Sample-efficiency curves showing success rates as a function of epochs in the MuJoCo setup. A-F: Sample-efficiency curves corresponding to Configurations 1-6 (shown on diagrams in the right hand corner of each panel), respectively. Thick lines show the IQM scores (bootstrap replication 50000), with the confidence intervals represented as shaded areas. Yellow is used to show results from the sensorized group, while blue is used for the controls. Training occurred for 5000000 timesteps, generating 500 epochs. Both main and control groups represent the result of 10 seeds.\nFigure 6. Visual representation of the converged success rates in the MuJoCo setup. A-B: The Median, IQM and Mean of the converged success rates (bootstrap replication 1000) for each Configuration in the sensorized and control groups, respectively. Confidence intervals are represented as shaded areas. Each configuration was run with 10 seeds; the exact values represented here can be found in Table 2.\nConfig.\nType\nMedian\nIQM\nMean\nMedian CI, 95%\nIQM CI, 95%\nMean CI, 95%\n1\nmain\n0.65\n0.65\n0.65\n[0.62, 0.67]\n[0.61, 0.68]\n[0.62, 0.67]\n1\ncontrol\n0.60\n0.62\n0.60\n[0.52, 0.65]\n[0.54, 0.66]\n[0.52, 0.65]\n2\nmain\n0.66\n0.66\n0.66\n[0.62, 0.68]\n[0.63, 0.69]\n[0.62, 0.68]\n2\ncontrol\n0.61\n0.62\n0.61\n[0.57, 0.65]\n[0.57, 0.67]\n[0.57, 0.65]\n3\nmain\n0.58\n0.64\n0.58\n[0.45, 0.68]\n[0.50, 0.68]\n[0.45, 0.68]\n3\ncontrol\n0.62\n0.63\n0.62\n[0.60, 0.65]\n[0.60, 0.65]\n[0.60, 0.65]\n4\nmain\n0.59\n0.59\n0.59\n[0.56, 0.62]\n[0.55, 0.63]\n[0.56, 0.62]\n4\ncontrol\n0.63\n0.64\n0.63\n[0.60, 0.65]\n[0.59, 0.65]\n[0.60, 0.65]\n5\nmain\n0.58\n0.57\n0.58\n[0.54, 0.62]\n[0.52, 0.64]\n[0.54, 0.62]\n5\ncontrol\n0.58\n0.61\n0.58\n[0.50, 0.63]\n[0.53, 0.64]\n[0.50, 0.63]\n6\nmain\n0.64\n0.64\n0.64\n[0.61, 0.67]\n[0.61, 0.68]\n[0.61, 0.67]\n6\ncontrol\n0.56\n0.60\n0.56\n[0.44, 0.64]\n[0.49, 0.64]\n[0.44, 0.64]\nTable 1:\nConverged success rates in the PyBullet setup.\nMedian, Interquartile Mean (IQM) and Mean values were derived from converged success rates considering the entire training period. Each configuration was run with 10 seeds.\nConfig.\nType\nMedian\nIQM\nMean\nMedian CI, 95%\nIQM CI, 95%\nMean CI, 95%\n1\nmain\n0.82\n0.84\n0.82\n[0.72, 0.90]\n[0.70, 0.93]\n[0.72, 0.90]\n1\ncontrol\n0.85\n0.83\n0.85\n[0.80, 0.90]\n[0.78, 0.90]\n[0.80, 0.90]\n2\nmain\n0.88\n0.88\n0.88\n[0.82, 0.94]\n[0.79, 0.96]\n[0.82, 0.94]\n2\ncontrol\n0.81\n0.80\n0.81\n[0.75, 0.86]\n[0.74, 0.88]\n[0.75, 0.86]\n3\nmain\n0.87\n0.87\n0.87\n[0.82, 0.93]\n[0.79, 0.95]\n[0.82, 0.93]\n3\ncontrol\n0.83\n0.87\n0.83\n[0.70, 0.90]\n[0.76, 0.92]\n[0.70, 0.90]\n4\nmain\n0.80\n0.84\n0.80\n[0.70, 0.88]\n[0.72, 0.90]\n[0.70, 0.88]\n4\ncontrol\n0.81\n0.85\n0.81\n[0.70, 0.90]\n[0.70, 0.93]\n[0.70, 0.90]\n5\nmain\n0.70\n0.73\n0.70\n[0.56, 0.81]\n[0.56, 0.83]\n[0.56, 0.81]\n5\ncontrol\n0.84\n0.86\n0.84\n[0.76, 0.92]\n[0.77, 0.93]\n[0.76, 0.92]\n6\nmain\n0.73\n0.78\n0.73\n[0.55, 0.86]\n[0.59, 0.89]\n[0.55, 0.86]\n6\ncontrol\n0.80\n0.79\n0.80\n[0.75, 0.85]\n[0.73, 0.85]\n[0.75, 0.85]\nTable 2:\nConverged success rates in the MuJoCo setup.\nMedian, Interquartile Mean (IQM) and Mean values were derived from converged success rates considering the entire training period. Each configuration was run with 10 seeds.",
    "preview_text": "Tactile sensors are breaking into the field of robotics to provide direct information related to contact surfaces, including contact events, slip events and even texture identification. These events are especially important for robotic hand designs, including prosthetics, as they can greatly improve grasp stability. Most presently published robotic hand designs, however, implement them in vastly different densities and layouts on the hand surface, often reserving the majority of the available space. We used simulations to evaluate 6 different tactile sensor configurations with different densities and layouts, based on their impact on reinforcement learning. Our two-setup system allows for robust results that are not dependent on the use of a given physics simulator, robotic hand model or machine learning algorithm. Our results show setup-specific, as well as generalized effects across the 6 sensorized simulations, and we identify one configuration as consistently yielding the best performance across both setups. These results could help future research aimed at robotic hand designs, including prostheses.\n\nThe impact of tactile sensor configurations on grasp learning efficiency - a comparative evaluation in simulation\nEszter Birtalan\nCorresponding author: birtalan.eszter@itk.ppke.hu\nFaculty of Information Technology and Bionics, PÃ¡zmÃ¡ny PÃ©ter Catholic University, Budapest, Hungary\nMiklÃ³s Koller\nFaculty of Information Technology and Bionics, PÃ¡zmÃ¡ny PÃ©ter Catholic University, Budapest, Hungary\nAbstract\nTactile sensors are breaking into the field of robotics to provide direct information related to contact surfaces, including contact events, slip events and even texture identification. These events are especially important for robotic hand designs, including prosthetics, as they can greatly improve grasp stability. Most presently published robotic hand designs, however, implement them in vastly different densities and layouts on the hand surface, often reserving the ma",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡é€šè¿‡æ¨¡æ‹Ÿè¯„ä¼°è§¦è§‰ä¼ æ„Ÿå™¨é…ç½®å¯¹å¼ºåŒ–å­¦ä¹ æŠ“å–æ•ˆç‡çš„å½±å“ï¼Œä¸æœºå™¨äººæ§åˆ¶ç›¸å…³ï¼Œä½†æœªæ¶‰åŠVLAã€diffusionã€Flow Matchingã€VLMç­‰å…³é”®è¯ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-15T10:38:14Z",
    "created_at": "2026-01-20T17:49:54.836745",
    "updated_at": "2026-01-20T17:49:54.836752"
}