{
    "id": "2601.08034v1",
    "title": "Fiducial Exoskeletons: Image-Centric Robot State Estimation",
    "authors": [
        "Cameron Smith",
        "Basile Van Hoorick",
        "Vitor Guizilini",
        "Yue Wang"
    ],
    "abstract": "Êàë‰ª¨ÊèêÂá∫‚ÄúÂü∫ÂáÜÂ§ñÈ™®È™º‚ÄùÊñπÊ≥ïÔºåËøôÊòØ‰∏ÄÁßçÂü∫‰∫éÂõæÂÉèÁöÑÊú∫Âô®‰∫∫‰∏âÁª¥Áä∂ÊÄÅ‰º∞ËÆ°ÈáçÊûÑÊñπÊ°àÔºåÈÄöËøáÂçïÂ∏ßÂõæÂÉèÊé®ÁêÜÊõø‰ª£‰∫Ü‰º†ÁªüÁπÅÁêêÊµÅÁ®ã‰∏éÁîµÊú∫‰∏≠ÂøÉÂåñÂ§ÑÁêÜÁÆ°Á∫ø„ÄÇ‰º†ÁªüÊñπÊ≥ï‚Äî‚ÄîÁâπÂà´ÊòØÊú∫Âô®‰∫∫-Áõ∏Êú∫Â§ñÂèÇÊ†áÂÆö‚Äî‚ÄîÈÄöÂ∏∏‰æùËµñÈ´òÁ≤æÂ∫¶ÊâßË°åÂô®ÔºåÂπ∂ÈúÄË¶ÅÂ¶ÇÊâãÁúºÊ†áÂÆöÁ≠âËÄóÊó∂ÊµÅÁ®ã„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÁé∞‰ª£Âü∫‰∫éÂ≠¶‰π†ÁöÑÊú∫Âô®‰∫∫ÊéßÂà∂Ê≠£Ë∂äÊù•Ë∂äÂ§öÂú∞Âü∫‰∫é‰ΩéÊàêÊú¨Á°¨‰ª∂ÁöÑRGBËßÇÊµãÊï∞ÊçÆËøõË°åËÆ≠ÁªÉ‰∏éÈÉ®ÁΩ≤„ÄÇ\n\nÊàë‰ª¨ÁöÑÊ†∏ÂøÉÊ¥ûËßÅÂåÖÂê´‰∏§ÊñπÈù¢„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨Â∞ÜÊú∫Âô®‰∫∫Áä∂ÊÄÅ‰º∞ËÆ°ÈáçÊûÑ‰∏∫ÂçïÂ∏ßRGBÂõæÂÉè‰∏≠ÊØè‰∏™ËøûÊùÜÁöÑ6D‰ΩçÂßø‰º∞ËÆ°ÈóÆÈ¢òÔºöÊú∫Âô®‰∫∫-Áõ∏Êú∫Âü∫Â∫ßÂèòÊç¢ÂèØÁõ¥Êé•ÈÄöËøá‰º∞ËÆ°ÁöÑÂü∫Â∫ßËøûÊùÜ‰ΩçÂßøËé∑ÂæóÔºåÂÖ≥ËäÇÁä∂ÊÄÅÂàôÈÄöËøáËΩªÈáèÁ∫ßÂÖ®Â±Ä‰ºòÂåñÊÅ¢Â§çÔºåËØ•‰ºòÂåñÂº∫Âà∂Êª°Ë∂≥‰∏éËßÇÊµãËøûÊùÜ‰ΩçÂßøÁöÑËøêÂä®Â≠¶‰∏ÄËá¥ÊÄßÔºàÂèØÈÄâÊã©‰ª•ÁºñÁ†ÅÂô®ËØªÊï∞ËøõË°åÁÉ≠ÂêØÂä®Ôºâ„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ÈÄöËøáÂºïÂÖ•Âü∫ÂáÜÂ§ñÈ™®È™ºÔºå‰ΩøÂçïËøûÊùÜ6D‰ΩçÂßø‰º∞ËÆ°ÂèòÂæóÈ≤ÅÊ£íËÄåÁÆÄÂçï‚Äî‚ÄîÂç≥‰ΩøÊó†ÈúÄÂ≠¶‰π†ËøáÁ®ãÔºöËøôÊòØ‰∏ÄÁßçËΩªÈáèÁ∫ß3DÊâìÂç∞ÊîØÊû∂ÔºåÊØè‰∏™ËøûÊùÜÈÖçÂ§áÂü∫ÂáÜÊ†áËÆ∞ÁÇπÔºå‰∏îÊ†áËÆ∞ÁÇπ-ËøûÊùÜÂá†‰ΩïÂÖ≥Á≥ªÂ∑≤Áü•„ÄÇ\n\nËØ•ËÆæËÆ°ÈÄöËøáÂçïÂ∏ßÂõæÂÉèÂç≥ÂèØÂÆûÁé∞È≤ÅÊ£íÁöÑÁõ∏Êú∫-Êú∫Âô®‰∫∫Â§ñÂèÇÊ†áÂÆö„ÄÅÂêÑËøûÊùÜSE(3)‰ΩçÂßø‰º∞ËÆ°ÂèäÂÖ≥ËäÇËßíÂ∫¶Áä∂ÊÄÅËé∑ÂèñÔºåÂç≥‰ΩøÂú®Êñ≠ÁîµÊú∫Âô®‰∫∫‰∏ä‰πüËÉΩËøõË°åÈ≤ÅÊ£íÁä∂ÊÄÅ‰º∞ËÆ°„ÄÇÂú®‰ΩéÊàêÊú¨Êú∫Ê¢∞ËáÇ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÂü∫ÂáÜÂ§ñÈ™®È™ºÂú®ÊòæËëóÁÆÄÂåñÁ≥ªÁªüÈÖçÁΩÆÁöÑÂêåÊó∂ÔºåÊèêÂçá‰∫ÜÊ†áÂÆöÁ≤æÂ∫¶„ÄÅÁä∂ÊÄÅÂáÜÁ°ÆÊÄßÂèä‰∏ãÊ∏∏‰∏âÁª¥ÊéßÂà∂ÊÄßËÉΩ„ÄÇÊàë‰ª¨ÂºÄÊ∫ê‰∫Ü‰ª£Á†Å‰∏éÂèØÊâìÂç∞Á°¨‰ª∂ËÆæËÆ°Ôºå‰ª•‰øÉËøõÁÆóÊ≥ï-Á°¨‰ª∂ÁöÑÂçèÂêåËÆæËÆ°ÂèëÂ±ï„ÄÇ",
    "url": "https://arxiv.org/abs/2601.08034v1",
    "html_url": "https://arxiv.org/html/2601.08034v1",
    "html_content": "Fiducial Exoskeletons: Image-Centric Robot State Estimation\nCameron Smith\n1\nBasile Van Hoorick\n2\nVitor Guizilini\n2‚àó\nYue Wang\n1‚àó\n1\nUSC Physical Superintelligence (PSI) Lab\n2\nToyota Research Institute\n*Equal Advising\ncameronosmith.github.io/fiducial_exoskeleton\nAbstract\nWe introduce Fiducial Exoskeletons, an image-based reformulation of 3D robot state estimation that replaces cumbersome procedures and¬†motor-centric pipelines with single-image inference. Traditional approaches‚Äîespecially robot‚Äìcamera extrinsic estimation‚Äîoften rely on high-precision actuators and require time-consuming routines such as hand‚Äìeye calibration. In contrast, modern learning-based robot control is increasingly trained and deployed from RGB observations on lower-cost hardware.\nOur key insight is twofold. First, we cast robot state estimation as 6D pose estimation of each link from a single RGB image: the robot‚Äìcamera base transform is obtained directly as the estimated base-link pose, and the joint state is recovered via a lightweight global optimization that enforces kinematic consistency with the observed link poses (optionally warm-started with encoder readings). Second, we make per-link 6D pose estimation robust and simple‚Äîeven without learning‚Äîby introducing the fiducial exoskeleton: a lightweight 3D-printed mount with a fiducial marker on each link and known marker‚Äìlink geometry.\nThis design yields robust camera‚Äìrobot extrinsics, per-link SE(3) poses, and joint-angle state from a single image, enabling robust state estimation even on unplugged robots. Demonstrated on a low-cost ($100) robot arm, fiducial exoskeletons substantially simplify setup while improving calibration, state accuracy, and downstream 3D control performance. We release code and printable hardware designs to enable further algorithm‚Äìhardware co-design.\nFigure 1:\nFidEx Overview\n. After installing a lightweight 3D-printed fiducial exoskeleton ‚Äî one marker-equipped piece per link ‚Äî FidEx enables instant 3D state estimation from a single RGB image. Given one view, we recover the external camera pose (robot base pose), the full joint state by optimizing over detected 6D link poses, and the joint calibration by aligning optimized joint angles with raw motor readings.\nI\nIntroduction\nAccurate 3D state estimation and precise 3D control are fundamental components of robotics.\nThe dominant paradigm for both is to use forward kinematics (FK), which integrates joint angles down the kinematic chain.\nHowever, the accuracy using FK depends on highly accurate motors and accumulates noise along the kinematic chain otherwise.\nConsequently, any pipelines requiring accurate 3D control and state estimation are typically demonstrated on highly expensive robot arms (usually over $10K)\n[\n20\n,\n21\n,\n28\n]\n, and are difficult to deploy on lower-cost robots, which often have larger backlash and less precise encoder readings.\nOther components of the 3D robotics stack are similarly restrictive and cumbersome.\nCamera-robot pose estimation requires an iterative approach which is tedious and importantly depends on highly accurate kinematic state estimation, similarly making it difficult for lower-cost hardware.\nRobot calibration is likewise clunky, either depending on operators to match an approximate a ‚Äútarget pose‚Äù iteratively move to the minimum and maximum joint angles, or requires complex hardware with homing sequences\n[\n22\n,\n4\n]\n.\nThese requirements render the 3D robotics control stack slow, brittle, and inaccurate for lower-cost motors.\nFigure 2:\nOverview of using Fiducial Exoskeletons for robot state estimation.\n(1a, top-left) Each link is fitted with a\nfiducial exoskeleton\n, a 3D-printed mount with a flat marker plane and a known marker-to-link coordinate transformation, enabling single-image 6D pose estimation for each link without any iterative calibration (Section\nIII-B\n).\n(1b, top-right) The exoskeleton for each link is printed and mounted on the robot.\n(2, bottom) From an RGB image, we estimate all link poses, also immediately yielding the camera pose directly from the base‚Äôs marker [2a].\nA fast optimization recovers the joint angles [2b] which best match the observed link poses (Section\nIII-A\n).\nWe also recover the robot‚Äôs calibration offset [2c] by comparing the optimized joints to the raw encoder joints.\nThe final joint estimate tightly matches the physical robot, observed by the rendered robot overlay (rightmost).\nTo address these hardware and state-estimation challenges, we opt to use\nvision\nfor robot state estimation, which has exciting potential as it observes a ‚Äúholistic viewpoint‚Äù over the full robot state, instead of just integrating per-motor observations.\nIn this work, we make vision a first-class signal for robot state estimation and control.\nWe reformulate the entire 3D estimation and control stack around 6D pose estimation of each link from an RGB image and a simple optimization to recover joint angles.\nFrom the same set of poses, we also recover the robot to camera pose directly as well as robot calibration.\nWe further make control more precise by using state-based refinement: after naively moving to the target state, we estimate the robot state from vision, compute the delta to the target state, and finally move the robot with the delta direction.\nTo facilitate per-link pose estimation, we introduce the\nfiducial exoskeleton\n: a lightweight 3D-printed mount with a fiducial marker\n[\n5\n,\n17\n]\nfor each link, providing an unambiguous marker-to-link transformation. The fiducial exoskeleton enables us to perform simple 6D pose estimation of each link from a single RGB image and without iterative calibration procedures which depend on the internal kinematics of the robot.\nIn summary, we introduce the following contributions:\n‚Ä¢\nA vision-centric reformulation of robot state estimation and control based on 6D link pose estimates, enabling recovery of joint angles, camera extrinsics, and robot calibration, from a single image and with significantly increased precision.\n‚Ä¢\nA practical mechanism ‚Äì the fiducial exoskeleton ‚Äì which simplifies per-link pose estimation.\n‚Ä¢\nOn a low-cost 6-DoF arm, our method reduces end-effector state estimation and control error by\n‚àº\n\\sim\n75% and\n‚àº\n\\sim\n45%, respectively, over traditional forward-kinematics based estimation.\nOn a $100 robot arm (SO-100\n[\n2\n]\n), our approach enables highly accurate state estimation compared to that from forward kinematics, as well as high-precision control.\nFigure 3:\nThe pseudo-code for our visual state-estimating control\n.\nTop: the pseudo-code of our control loop, where we first move the arm naively to the target position, estimate the current robot state, move the delta between the observed robot and target robot state, and finally re-calibrate the robot offsets for the next target motion.\nBottom: Illustrated steps of the control loop with the target state (Left), then the naive motion execution (Middle), and lastly the state-based refinement to better match the target state.\nWe also highlight insets on the end-effector to better emphasize the differences between the target and physical states.\nFigure 4:\nComparison to Dr.¬†Robot on robot-camera pose and joint state estimation.\nWe compare our (left) robot-camera pose and joint state estimates to those from Dr. Robot\n[\n14\n]\n(right), a rendering-based robot state estimation method, visualizing the input image (top row) and the re-rendered image (bottom row) using the inferred joints and camera pose. Dr. Robot‚Äôs rendering is visualized using their internal splatting renderer and ours using the simulator renderer with our inferred parameters. Dr.¬†Robot infers pose and joints parameters using differentiable rendering, which is known to be sensitive to local minima and initialization; its recovered pose is frequently misaligned with the input whereas our inferred parameters align near pixel-perfect with the input.\nFigure 5:\nState estimation results.\n(Left) Using raw encoder readings for state estimation (left), the rendered-robot is not well aligned with the physical robot.\nUsing the fiducial exoskeletons, even without using any encoder readings as input, the robot re-render is aligned with the physical robot (Right).\nFor each method, we plot the full-image overlay (inner left) and a highlighted inset on the end-effector (inner right).\nII\nRelated Work\nII-A\nClassical Camera Pose Estimation\nThe robot-camera pose is an important component in many robotic pipelines, linking the image space to the robot 3D workspace.\nThe standard way to obtain the camera pose is through hand-eye calibration\n[\n7\n]\n, an iterative procedure where a robot operator attaches a fiducial marker on the end-effector and moves the robot through several kinematic states while capturing image observations.\nThe pose is recovered via an\nA\n‚Äã\nX\n=\nX\n‚Äã\nB\nAX=XB\noptimization\n[\n11\n,\n7\n]\n.\nNote that this iterative data collection and the\nA\n‚Äã\nX\n=\nX\n‚Äã\nB\nAX=XB\noptimization is required only because the relationship between the fiducial marker and the end-effector is unknown.\nThis hand-eye calibration is iterative, considered tedious to set up, has to be performed each time the robot or camera moves, and critically depends on accurate joint kinematics in order to provide correct motion pairs to the solver.\nAlso note that this pipeline is not only tedious but difficult and inaccessible for low-cost robots with potentially noisy encoders.\nIn contrast, we simplify this design greatly with our fiducial exoskeleton by utilizing 3D-printed mounts where the relationship between the marker and the link is known, circumventing iterative data collection and the\nA\n‚Äã\nX\n=\nX\n‚Äã\nB\nAX=XB\noptimization.\nII-B\nRobot Keypoint Prediction for Camera Pose Estimation\nRecent learning-based approaches aim to recover the camera pose from a single image, typically by predicting 2D keypoints of link centers\n[\n10\n,\n6\n,\n24\n,\n16\n,\n15\n,\nlabb√©2021singleviewrobotposejoint\n,\n1\n]\n.\nThese approaches then solve a PnP optimization from the 2D keypoints to the 3D model obtained from the forward kinematics and known joint states.\nWhile reducing operator effort in avoiding the fiducial marker mounting and collection of multiple image observations, these methods similarly require highly accurate joint information, are only demonstrated on high-end robots (\n>\n$\n‚Äã\n10\n>\\mathdollar 10\nk) with precise encoders, have questionable generalization, and have awkward processing of occluded links.\nIn contrast, because our formulation predicts a full 6D pose for each link rather than sparse 2D locations, our method is able to not only recover the joint angles as well as the extrinsic calibration of external cameras with respect to the robot.\nII-C\nRobot Pose Estimation from Vision\nEarlier work also explored markerless robot arm pose estimation with similar motivations of circumventing potentially unreliable kinematic state.\nThese approaches aimed to recover joint angles but often involved complex pipelines, often leveraging depth maps and heuristic methods for link clustering and segmentation\n[\nbiliƒá2023distancegeometricmethodrecoveringrobot\n,\n27\n,\n31\n]\n.\nThey also importantly do not recover the robot to camera pose.\nIn contrast, our approach simplifies these designs greatly without using any training data, while offering not just joint recovery but also camera pose estimation.\nMore recently, differentiable rendering approaches such as\nDr. Robot\n[\n14\n]\nattempt to estimate robot state by optimizing the parameters of a differentiable robot appearance model (e.g., Gaussian splatting\n[\n9\n,\n30\n,\n18\n]\n) to match input images.\nThese methods are powerful for enabling image-space-to-robot gradients and have been proposed as a way to integrate predictions from large vision models into robot pipelines.\nHowever, they require accurate robot segmentation, are sensitive to lighting and appearance conditions, and optimize in the raw RGB space ‚Äì making them slow, unconstrained, and often unstable.\nIn contrast, our method leverages vision\nnot at the pixel level\nbut at the structured level of per-link 6D poses, yielding a much more constrained, low-dimensional, real-time, and accurate optimization for robot state estimation.\nII-D\nLatent Inverse Robot Dynamics from Videos\nAn orthogonal line of work aims to learn robot actions directly from video, often inferring\nlatent\nactions to explain the observed visual trajectories\n[\n12\n,\n19\n,\n3\n,\n8\n,\n29\n,\n25\n,\n23\n,\n26\n]\n.\nFor instance, DreamGen proposes a multi-stage pipeline that fine-tunes a video generator with tele-operated video data to generate novel robot data, without leveraging explicit robot actions, and an inverse dynamics model or latent action model to label trajectories.\nOur method, in contrast, aims to recover instantaneous and explicit robot state.\nThese video-based latent dynamics models are thus complementary to our method: they provide implicit priors for action generation and policies, whereas our method offers explicit and reliable state estimation, perhaps to be used in tandem.\nIII\nAccurate State Estimation and Control of Robots using Fiducial Exoskeletons\nOur aim is to estimate the full 3D state of the robot (including the robot base to camera pose as well as the joint parameters) from a single RGB image, without depending on the robot‚Äôs internal motor readings (which we consider here to be potentially noisy and unreliable).\nTo achieve this goal of estimating the robot state from vision alone, we propose to first estimate the 6D pose of each link and then perform a global optimization over joint states to match the observed link poses. We also introduce the\nfiducial exoskeleton\nto facilitate link pose estimation, which is effectively a 3D-printed attachment for each link with a fiducial marker.\nBelow, we first describe the global optimization to recover joint states from link pose estimates\nIII-A\n, then how we estimate the poses of links using fiducial exoskeletons\nIII-B\n, and conclude with how we leverage this state estimation to control the robot with precision\nIII-D\n.\nState Estimation\nMethod\nMask-IoU\n‚Üë\n\\uparrow\nEff. Trans\n‚Üì\n\\downarrow\nEff. Rot\n‚Üì\n\\downarrow\nOurs Enc.\n.85\n.06\n.27\nOurs No-Enc.\n.84\n.07\n.35\nJust Enc.\n.78\n.18\n1.1\nControl\nMethod\nMask-IoU\n‚Üë\n\\uparrow\nEff. Trans\n‚Üì\n\\downarrow\nEff. Rot\n‚Üì\n\\downarrow\nOurs w/ Delta\n.79\n.21\n2.6\nOurs w/o Delta\n.78\n.22\n2.8\nNaive Move\n.63\n.37\n3.8\nTABLE I:\nState estimation (Left) and control (Right) results with fiducial exoskeletons are far stronger than using encoder-readings alone. For both state estimation and control, we compare the re-rendered robot using the estimated sensor state to the ground-truth robot mask (Mask IoU), as well as the rotation and translation distance for the estimated and true end-effector position. For state estimation, we compare the raw-encoder state (Just Enc.) to our method without using encoder states as initialization (Ours No-Enc.) and then with using the encoder states as well (Ours Enc.). For control, we compare the target robot state to the naive encoder motion (Naive Move), ours without the delta refinement (Ours w/o Delta), and with the delta refinement (Ours w/ Delta).\nIII-A\nRecovery of Robot Joint States from Link Poses\nVisual estimation of the 6D pose for each link provides direct constraints on the joint parameters of the robot: by comparing the link poses induced by forward kinematics to those observed from vision, we can optimize the joint parameters to match the visual observations.\nForward kinematics induces a set of per-link poses\n{\nT\nj\n‚Äã\n(\nŒ∏\n)\n}\nj\n=\n1\nL\n\\{T_{j}(\\theta)\\}_{j=1}^{L}\nby integrating the link transformations and joint angles down the kinematic chain. That is, for a\nd\nd\n-DOF robot with joint parameters\nŒ∏\n=\n(\nŒ∏\n1\n,\n‚Ä¶\n,\nŒ∏\nd\n)\n\\theta=(\\theta_{1},\\ldots,\\theta_{d})\n,\nT\nj\n‚Äã\n(\nŒ∏\n)\n=\n‚àè\ni\n=\n1\nj\nT\ni\ni\n‚àí\n1\n‚Äã\n(\nŒ∏\ni\n)\n,\nT_{j}(\\theta)=\\prod_{i=1}^{j}\\,T_{i}^{i-1}(\\theta_{i}),\n(1)\nwhere\nT\ni\ni\n‚àí\n1\n‚Äã\n(\nŒ∏\ni\n)\n‚àà\nS\n‚Äã\nE\n‚Äã\n(\n3\n)\n\\,T_{i}^{i-1}(\\theta_{i})\\in SE(3)\nis the transformation of joint\ni\ni\nwith respect to its parent frame\ni\n‚àí\n1\ni-1\n. Note that the forward kinematics integration is differentiable with respect to the joint states, an important property which we will leverage below.\nIn parallel, we assume access to a visual estimate of the same set of link poses,\nùí´\nobs\n=\n{\nP\nj\nobs\n}\nj\n=\n1\nL\n,\n\\mathcal{P}_{\\text{obs}}=\\{P^{\\text{obs}}_{j}\\}_{j=1}^{L},\n(2)\nwhere each\nP\nj\nobs\n‚àà\nS\n‚Äã\nE\n‚Äã\n(\n3\n)\nP^{\\text{obs}}_{j}\\in SE(3)\nis the 6D pose of link\nj\nj\n.\nWe can then\nsolve\nfor the optimal joint states which best aligns the observed link poses to the ones induced by forward kinematics:\nŒ∏\n‚ãÜ\n=\narg\n‚Å°\nmin\nŒ∏\n‚Äã\n‚àë\nj\n=\n1\nL\nd\n‚Äã\n(\nT\nj\n‚Äã\n(\nŒ∏\n)\n,\nP\nj\nobs\n)\n2\n,\n\\theta^{\\star}=\\arg\\min_{\\theta}\\sum_{j=1}^{L}d\\!\\left(T_{j}(\\theta),\\,P^{\\text{obs}}_{j}\\right)^{2},\n(3)\nwhere\nd\n‚Äã\n(\n‚ãÖ\n,\n‚ãÖ\n)\nd(\\cdot,\\cdot)\nis a distance metric on\nS\n‚Äã\nE\n‚Äã\n(\n3\n)\nSE(3)\n.\nWhile this optimization is nonlinear, it is low-dimensional and can be solved with off-the-shelf nonlinear solvers (e.g. L-BFGS\n[\n13\n]\n) in real-time.\nAlso, note that we can leverage encoder readings when available as the initial\nŒ∏\n\\theta\nvalue used in the optimization, and just initialize the joint values with zeros otherwise.\nWe find that the joint-state optimization typically converges to the same values with or without the encoder readings as initialization, and only observe an improvement in using the encoder readings as initialization when most links are occluded.\nAlso note that while the robot base is not technically a link, we assume the robot base pose is also estimated in this set of observed poses, and transform all link poses to the robot coordinate frame. In the next section, we also describe estimating the robot base pose with respect to external cameras.\nIII-B\nLink Pose Estimation via Fiducial Exoskeletons\nAbove we illustrated how we can leverage 6D link poses to estimate the state of the robot.\nWhile there are indeed several potential parameterizations of the per-link pose regression, here we highlight another direction of\nsimply attaching a fiducial marker to each robot link\n: fiducial markers provide SE(3) marker-to-camera pose estimates from a single RGB image, robust to partial occlusions and generalize trivially to a wide distribution of orientations.\nThe key difficulty here, however, lies in registering the marker coordinate frame to the coordinate frame of the link they represent: while the standard approach of registering markers to the robot frame is the classical and multi-observation\nA\n‚Äã\nX\n=\nX\n‚Äã\nB\nAX=XB\ncalibration, this assumes access to accurate joint kinematics and a known camera pose.\nIn the case of imprecise motors, accurate kinematic state or camera pose is difficult to obtain for such calibration.\nTo remedy these issues, we introduce the\nfiducial exoskeleton\n.\nFor each link, we 3D-model a lightweight 3D-printed mount that attaches unambiguously onto the link.\nThe fiducial exoskeleton consists of two parts: the component which mounts to the link, and a flat plane which holds the fiducial marker (see Fig.\n2\nfor illustration).\nImportantly, this 3D model provides us the direct transformation between the plane to the link coordinate frame, which allows us to recover the 6D pose of the link from the 6D marker estimate, without any iterative calibration, kinematic state, or camera pose.\nFormally, the link pose in the camera frame is simply:\nT\nlink\ncam\n=\nT\naruco\ncam\n‚Äã\nT\nexo\naruco\n‚Äã\nT\nlink\nexo\n,\nT^{\\text{cam}}_{\\text{link}}\\;=\\;T^{\\text{cam}}_{\\text{aruco}}\\;\\;T^{\\text{aruco}}_{\\text{exo}}\\;\\;T^{\\text{exo}}_{\\text{link}},\n(4)\nwhere\nT\naruco\ncam\nT^{\\text{cam}}_{\\text{aruco}}\nis the pose of the fiducial marker in the camera frame,\nT\nexo\naruco\nT^{\\text{aruco}}_{\\text{exo}}\nis the transform from the marker to the exoskeleton,\nFigure 6:\nRobustness study\n.\n(Top) State estimation results when masking out\n1\n1\n,\n3\n3\n, and\n4\n4\nfiducial markers.\nEven with all but one marker occluded, our vision-based state estimation still produces strong results.\nEven with significant occlusion, our optimization remains stable, with encoder initialization helping in the most under-constrained cases.\n(Bottom) Unlike learning-based approaches which may struggle with out-of-distribution robot configurations, here we show state estimation even in an ‚Äòupside-down‚Äô state is similarly accurate for our method‚Äôs estimation.\nMetric\nFidEx (Ours)\nDr.¬†Robot\nJoints (L2)\n0.0855\n3.3852\nCamera Translation (m)\n0.0053\n0.7392\nCamera Rotation (\n‚àò\n)\n0.2148\n109.36\nTime (s)\n0.0427\n4.9311\nTABLE II:\nQuantitative comparison of state estimation (joints and robot-camera pose) accuracy with Dr. Robot\n[\n14\n]\n.\nLower is better for all metrics. ‚ÄúTime‚Äù denotes optimization time to perform inference.\nMethod\n1 Occ.\n3 Occ.\n4 Occ.\nUpside Down\nOurs (Enc)\n.88\n.87\n.87\n.82\nOurs (No-Enc)\n.88\n.87\n.75\n.82\nJust Enc\n.81\n.81\n.81\n.76\nTABLE III:\nState estimation results (re-rendered robot mask IoU) under varying marker occlusions and robot orientation conditions. Note how even without any sensor information (Ours No-Enc), our optimization only falls into a degenerate minima when only the end-effector is visible.\nIII-C\nCamera Robot Pose Estimation and Robot Calibration from a Single Image for Free\nIII-C\n1\nCamera Robot Pose\nIn the same way we attach fiducial exoskeletons to each link, we attach one to the robot base as well.\nThis immediately delivers robot-to-camera extrinsics, rendering it as simple as estimating the pose of the base‚Äôs fiducial marker:\nT\ncam\nrobot\n=\n(\nT\naruco\ncam\n‚Äã\nT\nexo\naruco\n‚Äã\nT\nrobot\nexo\n)\n‚àí\n1\n.\nT^{\\text{robot}}_{\\text{cam}}\\;=\\;(T^{\\text{cam}}_{\\text{aruco}}\\;\\;T^{\\text{aruco}}_{\\text{exo}}\\;\\;T^{\\text{exo}}_{\\text{robot}})^{-1}.\n(5)\nThis direct estimation eliminates the need for tedious and iterative hand-eye calibration procedures.\nIII-C\n2\nRobot Calibration\nAlso note that since our joint estimation does not require known encoder states, we can perform robot calibration from a single image as well. The joint offset is recovered as the difference of the raw encoder joints\nŒ∏\nE\n‚Äã\nn\n‚Äã\nc\n\\theta^{Enc}\nand the optimized joints\nŒ∏\n‚ãÜ\n\\theta^{\\star}\n:\nŒî\n‚Äã\nŒ∏\n=\nŒ∏\n‚ãÜ\n‚àí\nŒ∏\nE\n‚Äã\nn\n‚Äã\nc\n.\n\\Delta\\theta=\\theta^{\\star}-\\theta^{Enc}.\n(6)\nNote this calibration procedure is performed from a single image and is dramatically simpler than current calibration procedures, which often involve manually moving the robot to a ‚Äòtarget pose‚Äô for alignment or require iterative and slow homing procedures.\nFigure 7:\nFidEx on other embodiments. Fiducial exoskeletons can be installed on a wide range of robot embodiments; here we show installation and single-image state recovery on the ARX arm, with additional platforms to be supported in future work.\nFigure 8:\nFiducial cube and analytical grasp with FidEx. Left: our 3D-printed ArUco fiducial cube and robot setup, used to evaluate object grasp success with an analytical IK grasp as a proxy task for calibration accuracy. We compare grasp success under our FidEx single-image calibration versus a common approximate manual (‚Äúmiddle-pose‚Äù) calibration. Right: an example analytical grasp execution, showing the planned pose overlay in white and the rendered robot in red, illustrating the tight alignment achieved with our calibration.\nIII-D\nAccurate Control via State Estimation Refinement\nFor the same reason that state estimation of a robot with potentially inaccurate sensors can be difficult ‚Äì local regions of backlash, drift, etc. ‚Äì commanding the robot to a target kinematic state\nŒ∏\nT\n‚Äã\na\n‚Äã\nr\n‚Äã\ng\n\\theta^{Targ}\nwith precision can be difficult as well.\nWe introduce a simple control loop to leverage our state estimation for pose refinement.\nFor each kinematic target, we perform on-the-fly robot calibration, command the robot to the target state\nŒ∏\nT\n‚Äã\na\n‚Äã\nr\n‚Äã\ng\n\\theta^{Targ}\n, estimate the current robot state\nŒ∏\n‚àó\n‚Ä≤\n\\theta^{*^{\\prime}}\n, and move the robot with the delta between the observed and target state\nŒ∏\nT\n‚Äã\na\n‚Äã\nr\n‚Äã\ng\n‚àí\n(\nŒ∏\n‚àó\n‚Ä≤\n‚àí\nŒ∏\nT\n‚Äã\na\n‚Äã\nr\n‚Äã\ng\n)\n\\theta^{Targ}-(\\theta^{*^{\\prime}}-\\theta^{Targ})\n.\nThe procedure pseudocode and visual results are described in Fig.\n3\n.\nWith this control loop, we are able to greatly increase the precision at which we can move the robot to a target kinematic state compared to naive execution without requiring high-precision encoders.\nIV\nExperiments\nWe benchmark our method in both robot state estimation and control on a low-cost robot in Figures\n5\n,\n9\nand Tables\nI\n,\nIII\n.\nFor state estimation, we evaluate how well we can estimate the joint parameters and 6D link positions in\nIV-A\n, and for control\nIV-C\n, how accurately we can move the robot links to target kinematic states or 6D link positions.\nIV-A\nEstimating Robot State\nFor estimating the robot state, we compare the traditional approach of using FK (‚ÄòJust Enc.‚Äô) for the 6D position of each link, to our method ‚Äì both with using the raw encoder states as input (Ours Enc.) and then without (Ours No-Enc).\nTo compare the state estimation, we plot the robot mesh render overlayed on the robot in Fig.\n5\n, and report Mask IoU as well as the 6D pose error (position and rotation losses) for the end-effector, on a dataset of diverse robot configurations, in Table\nI\n.\nOur method produces state estimation which are much more aligned with the physical robot than that of naively using FK-based integration.\nSee Fig.\n5\nto qualitatively observe how much more aligned the robot re-rendering is with the physical robot.\nAnd quantitatively, see Tab.\nI\nwhere we report a\n‚àº\n\\sim\n75% decrease in error in estimating the end-effector‚Äôs SE(3) position.\nWe additionally compare against Dr.¬†Robot\n[\n14\n]\n, a recent method that estimates robot state via differentiable rendering, optimizing robot-camera pose and joint parameters to match an observed image.\nIn contrast to FidEx, which leverages fiducial markers (making the visual background largely irrelevant), Dr.¬†Robot relies on a splatting-based rendering optimization, which makes the optimization sensitive to initialization and local minima.\nWe plot qualitative results in Fig.\n4\n, showing the input image and the corresponding re-render using the recovered camera and joint parameters.\nWhile FidEx produces near pixel-perfect alignment between the input and re-render, Dr.¬†Robot often converges to substantially misaligned solutions, reflecting the difficulty of solving full pose and joints recovery purely via rendering-based optimization.\nQuantitatively, Table\nII\nshows that FidEx achieves dramatically lower joint and camera errors while running over two orders of magnitude faster (e.g.,\n97.5\n%\n97.5\\%\nlower joint error,\n99.3\n%\n99.3\\%\nlower camera translation error,\n99.8\n%\n99.8\\%\nlower camera rotation error, and\n99.1\n%\n99.1\\%\nlower runtime).\nFigure 9:\nRobot control illustrations\n.\nFor each target state (blue outline) we compare the difference to the physical robot in using just the raw naive motion encoders (Raw Motion), the execution with our fiducial exoskeleton but without our delta refinement (No Delta), and finally with our delta refinement (With Delta).\nWe also plot highlighted insets on the end-effector (Bottom).\nWithout delta correction, the end-effector location is still close to the target, and the delta refinement further closes the gap between the target and executed motion.\nIV-B\nRobustness Studies\nOne reasonable question is how our method performs when some subset of the links are occluded, or the robot is in ‚Äòout-of-distribution‚Äô positions.\nWe cover various numbers of link markers (1, 3 and 4 markers), as well as move the robot to an upside-down position, and report accuracy in Table\nIII\nand plot illustrations in Figure\n6\n.\nWhen all links are occluded, our method reduces to the case of just using the raw encoder states, and when even one link (such as the end-effector) is visible, even this single constraint increases the state estimation accuracy significantly.\nWe only find that in the case of not using any encoder state (not relevant in practice and more so a demonstration of surprising robustness) and only one link is visible, our estimation can fall into a degenerate minima.\nAnd since our state estimation does not involve any dataset or learning, there is no notion of out-of-distribution pose estimation: our pose estimation reduces to that of fiducial marker estimation, a classical method with robust implementation, and so this ‚Äòupside-down‚Äô state is trivially recovered as well.\nSee Fig.\n6\n(bottom) to observe that our estimation in this ‚Äòupside-down‚Äô configuration is similarly aligned with the physical robot.\nFigure 10:\nChallenges of 3D state estimation on low-cost robots.\n(\nTop\n)\nBacklash:\ndue to imperfect mechanical alignment in the internal motors, each joint has a margin of rotation in which encoders register no change.\nThis unmeasured change accumulates down the kinematic chain, leading to large state error at the end-effector.\n(\nBottom\n)\nCalibration:\njoint offsets are often calibrated by manually matching one or a small set of ‚Äòtarget‚Äô poses, yielding imperfect mappings.\nIV-C\nControl Precision Studies\nTo measure how precisely we can move the robot to a target 6D and kinematic state, we record a diverse dataset of target positions, and move the robot to each target state using our control loop.\nWe similarly plot the robot mesh re-render of the target state onto the final reached state in Fig.\n9\n, as well as quantitative Mask IoU and end-effector 6D position accuracy, in Table\nI\n.\nWe find that each component of the control loop is vital for accurate positioning, comparing our full method to the naive method of just moving the robot to the target state, as well as not using our state-estimating delta position refinement.\nSpecifically, we find that even without using the delta-correction (effectively just re-calibrating the robot before moving), our method reduces the end-effector control error by\n‚àº\n\\sim\n40%, and adding our delta-correction further decreases error by an additional\n‚àº\n\\sim\n5%.\nWe also evaluate grasping precision on a small 3D-printed puck with an ArUco marker to indicate the target grasp location, using inverse kinematics to plan an analytical grasp, first executed with our FidEx single-image calibration and the common middle-pose alignment.\nWith FidEx, the robot successfully grasped the puck in 9 out of 10 trials, whereas the middle-pose calibration only succeeded in 4 out of 10 trials, highlighting the improved control accuracy enabled by our method.\nV\nDiscussion\nWe have introduced Fiducial Exoskeletons, a simple and robust design for robot state estimation and control which does not depend on highly accurate and expensive motors, instead leveraging holistic image-based estimation.\nOur method is significantly more accurate the standard method (forward kinematics with raw sensor encoders) for robot state estimation and control on a low-cost 6DoF robot arm.\nWe believe our design paves the way towards simpler and more accurate 3D robot state estimation and control, as well as increases capabilities for lower-cost robot arms.\nV-A\nLimitations\nFiducial Exoskeletons have several limitations that suggest exciting future explorations.\nFirst, the robot has to be observed by an external camera in which at least the base marker and one other marker are clearly visible: exploring the camera mount design to ensure proper visibility is an interesting constraint and future design consideration.\nOne such solution could be to use one mounted camera just focused on observing the robot state and another for the robot and the rest of the scene together.\nThe markers also have to be designed per-embodiment and per-link, which is laborious but is a constant design time per embodiment and can be distributed to all robot operators via 3D CAD files.\nAnd while aesthetics are not the primary concern of future robot policy learning, the markers are large and occlude much of the robot; future work on integrating more seamless and subtle marker design is interesting as well.\nWhile there are indeed physical design limitations here, we hope this direction of 3D robot co-design increases the breadth of robots by which we can leverage analytical robot control and 3D-based policy learning, especially in the lower-cost regime of robots with less precise motors.\nReferences\n[1]\nS. Ban, J. Fan, X. Ma, W. Zhu, Y. Qiao, and Y. Wang\n(2024)\nReal-time holistic robot pose estimation with unknown states\n.\nExternal Links:\n2402.05655\n,\nLink\nCited by:\n¬ß\nII-B\n.\n[2]\nR. Cadene, S. Alibert, A. Soare, Q. Gallouedec, A. Zouitine, S. Palma, P. Kooijmans, M. Aractingi, M. Shukor, D. Aubakirova, M. Russi, F. Capuano, C. Pascal, J. Choghari, J. Moss, and T. Wolf\n(2024)\nLeRobot: state-of-the-art machine learning for real-world robotics in pytorch\n.\nNote:\nhttps://github.com/huggingface/lerobot\nCited by:\n¬ßI\n.\n[3]\nJ. A. Collins, L. Cheng, K. Aneja, A. Wilcox, B. Joffe, and A. Garg\n(2025)\nAMPLIFY: actionless motion priors for robot learning from videos\n.\nExternal Links:\n2506.14198\n,\nLink\nCited by:\n¬ß\nII-D\n.\n[4]\nT. Edition and J. J. Craig\n(2005)\nIntroduction to robotics\n.\nCited by:\n¬ßI\n.\n[5]\nS. Garrido-Jurado, R. Mu√±oz-Salinas, F. J. Madrid-Cuevas, and R. Medina-Carnicer\n(2014)\nAutomatic generation and detection of highly reliable fiducial markers under occlusion\n.\nPattern Recognition\n47\n(\n6\n),\npp.¬†2280‚Äì2292\n.\nCited by:\n¬ßI\n.\n[6]\nR. G. Goswami, P. Krishnamurthy, Y. LeCun, and F. Khorrami\n(2025)\nRoboPEPP: vision-based robot pose and joint angle estimation through embedding predictive pre-training\n.\nExternal Links:\n2411.17662\n,\nLink\nCited by:\n¬ß\nII-B\n.\n[7]\nR. Horaud and F. Dornaika\n(1995-06)\nHand-eye calibration\n.\nThe International Journal of Robotics Research\n14\n(\n3\n),\npp.¬†195‚Äì210\n.\nExternal Links:\nISSN 1741-3176\n,\nLink\n,\nDocument\nCited by:\n¬ß\nII-A\n.\n[8]\nJ. Jang\net al.\n(2025)\nDreamGen: unlocking generalization in robot learning through neural trajectories\n.\narXiv preprint arXiv:2505.12705\n.\nCited by:\n¬ß\nII-D\n.\n[9]\nB. Kerbl, G. Kopanas, T. Leimk√ºhler, and G. Drettakis\n(2023)\n3d gaussian splatting for real-time radiance field rendering\n.\nACM Transactions on Graphics (ToG)\n42\n(\n4\n),\npp.¬†1‚Äì14\n.\nCited by:\n¬ß\nII-C\n.\n[10]\nT. E. Lee, J. Tremblay, T. To, J. Cheng, T. Mosier, O. Kroemer, D. Fox, and S. Birchfield\n(2020)\nCamera-to-robot pose estimation from a single image\n.\nExternal Links:\n1911.09231\n,\nLink\nCited by:\n¬ß\nII-B\n.\n[11]\nR. K. Lenz and R. Y. Tsai\n(1989)\nCalibrating a cartesian robot with eye-on-hand configuration independent of eye-to-hand relationship\n.\nIEEE Transactions on Pattern Analysis & Machine Intelligence\n11\n(\n09\n),\npp.¬†916‚Äì928\n.\nCited by:\n¬ß\nII-A\n.\n[12]\nS. Li, Y. Gao, D. Sadigh, and S. Song\n(2025)\nUnified video action model\n.\nExternal Links:\n2503.00200\n,\nLink\nCited by:\n¬ß\nII-D\n.\n[13]\nD. C. Liu and J. Nocedal\n(1989)\nOn the limited memory bfgs method for large scale optimization\n.\nMathematical Programming\n45\n(\n1‚Äì3\n),\npp.¬†503‚Äì528\n.\nCited by:\n¬ß\nIII-A\n.\n[14]\nR. Liu, A. Canberk, S. Song, and C. Vondrick\n(2024)\nDifferentiable robot rendering\n.\nExternal Links:\n2410.13851\n,\nLink\nCited by:\nFigure 4\n,\n¬ß\nII-C\n,\nTABLE II\n,\n¬ß\nIV-A\n.\n[15]\nJ. Lu, Z. Liang, T. Xie, F. Ritcher, S. Lin, S. Liu, and M. C. Yip\n(2024)\nCtRNet-x: camera-to-robot pose estimation in real-world conditions using a single camera\n.\nExternal Links:\n2409.10441\n,\nLink\nCited by:\n¬ß\nII-B\n.\n[16]\nJ. Lu, F. Richter, and M. C. Yip\n(2023)\nMarkerless camera-to-robot pose estimation via self-supervised sim-to-real transfer\n.\nExternal Links:\n2302.14332\n,\nLink\nCited by:\n¬ß\nII-B\n.\n[17]\nE. Olson\n(2011)\nAprilTag: a robust and flexible visual fiducial system\n.\nIn\nProceedings of the IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.¬†3400‚Äì3407\n.\nCited by:\n¬ßI\n.\n[18]\nF. Remondino\net al.\n(2023)\nA critical analysis of nerf-based 3d reconstruction\n.\nRemote Sensing\n.\nCited by:\n¬ß\nII-C\n.\n[19]\nN. Research\n(2025)\nDreamGen\n.\nNote:\nProject page\nhttps://research.nvidia.com/labs/gear/dreamgen/\nCited by:\n¬ß\nII-D\n.\n[20]\nW. Shen, G. Yang, A. Yu, J. Wong, L. P. Kaelbling, and P. Isola\n(2023)\nDistilled feature fields enable few-shot language-guided manipulation\n.\nIn\n7th Annual Conference on Robot Learning\n,\nCited by:\n¬ßI\n.\n[21]\nA. Simeonov, Y. Du, A. Tagliasacchi, J. B. Tenenbaum, A. Rodriguez, P. Agrawal, and V. Sitzmann\n(2022)\nNeural descriptor fields: se(3)-equivariant object representations for manipulation\n.\nCited by:\n¬ßI\n.\n[22]\nM. W. Spong, S. Hutchinson, and M. Vidyasagar\n(2004)\nRobot dynamics and control\n.\nCited by:\n¬ßI\n.\n[23]\nY. Tian, S. Yang, J. Zeng, P. Wang, D. Lin, H. Dong, and J. Pang\n(2024)\nPredictive inverse dynamics models are scalable learners for robotic manipulation\n.\nExternal Links:\n2412.15109\n,\nLink\nCited by:\n¬ß\nII-D\n.\n[24]\nY. Tian, J. Zhang, G. Huang, B. Wang, P. Wang, J. Pang, and H. Dong\n(2024)\nRoboKeyGen: robot pose and joint angles estimation via diffusion-based 3d keypoint generation\n.\nExternal Links:\n2403.18259\n,\nLink\nCited by:\n¬ß\nII-B\n.\n[25]\nA. Villar-Corrales\net al.\n(2025)\nPlaySlot: learning inverse latent dynamics for controllable object-centric video prediction and planning\n.\nIn\nICML\n,\nCited by:\n¬ß\nII-D\n.\n[26]\nY. Wen, J. Lin, Y. Zhu, J. Han, H. Xu, S. Zhao, and X. Liang\n(2024)\nVidMan: exploiting implicit dynamics from video diffusion model for effective robot manipulation\n.\nExternal Links:\n2411.09153\n,\nLink\nCited by:\n¬ß\nII-D\n.\n[27]\nF. Widmaier, D. Kappler, S. Schaal, and J. Bohg\n(2016)\nRobot arm pose estimation by pixel-wise regression of joint angles\n.\nIn\n2016 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.¬†616‚Äì623\n.\nCited by:\n¬ß\nII-C\n.\n[28]\nA. Wilcox, M. Ghanem, M. Moghani, P. Barroso, B. Joffe, and A. Garg\n(2025)\nAdapt3R: adaptive 3d scene representation for domain transfer in imitation learning\n.\nExternal Links:\n2503.04877\n,\nLink\nCited by:\n¬ßI\n.\n[29]\nS. Ye, J. Jang, B. Jeon, S. Joo, J. Yang, B. Peng, A. Mandlekar, R. Tan, Y. Chao, B. Y. Lin, L. Liden, K. Lee, J. Gao, L. Zettlemoyer, D. Fox, and M. Seo\n(2025)\nLatent action pretraining from videos\n.\nExternal Links:\n2410.11758\n,\nLink\nCited by:\n¬ß\nII-D\n.\n[30]\nX. Zhang\net al.\n(2024)\nNeRF in robotics: a survey\n.\narXiv preprint arXiv:2405.01333\n.\nCited by:\n¬ß\nII-C\n.\n[31]\nY. Zuo, W. Qiu, L. Xie, F. Zhong, Y. Wang, and A. L. Yuille\n(2025)\nCRAVES: controlling robotic arm with a vision-based economic system\n.\nExternal Links:\n1812.00725\n,\nLink\nCited by:\n¬ß\nII-C\n.",
    "preview_text": "We introduce Fiducial Exoskeletons, an image-based reformulation of 3D robot state estimation that replaces cumbersome procedures and motor-centric pipelines with single-image inference. Traditional approaches - especially robot-camera extrinsic estimation - often rely on high-precision actuators and require time-consuming routines such as hand-eye calibration. In contrast, modern learning-based robot control is increasingly trained and deployed from RGB observations on lower-cost hardware.\n  Our key insight is twofold. First, we cast robot state estimation as 6D pose estimation of each link from a single RGB image: the robot-camera base transform is obtained directly as the estimated base-link pose, and the joint state is recovered via a lightweight global optimization that enforces kinematic consistency with the observed link poses (optionally warm-started with encoder readings). Second, we make per-link 6D pose estimation robust and simple - even without learning - by introducing the fiducial exoskeleton: a lightweight 3D-printed mount with a fiducial marker on each link and known marker-link geometry.\n  This design yields robust camera-robot extrinsics, per-link SE(3) poses, and joint-angle state from a single image, enabling robust state estimation even on unplugged robots. Demonstrated on a low-cost robot arm, fiducial exoskeletons substantially simplify setup while improving calibration, state accuracy, and downstream 3D control performance. We release code and printable hardware designs to enable further algorithm-hardware co-design.\n\nFiducial Exoskeletons: Image-Centric Robot State Estimation\nCameron Smith\n1\nBasile Van Hoorick\n2\nVitor Guizilini\n2‚àó\nYue Wang\n1‚àó\n1\nUSC Physical Superintelligence (PSI) Lab\n2\nToyota Research Institute\n*Equal Advising\ncameronosmith.github.io/fiducial_exoskeleton\nAbstract\nWe introduce Fiducial Exoskeletons, an image-based reformulation of 3D robot state estimation that replaces cumbersome procedures and¬†motor-centric pipelines with",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "robot state estimation",
        "image-based",
        "fiducial markers",
        "6D pose estimation",
        "calibration"
    ],
    "one_line_summary": "ËØ•ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂõæÂÉèÂíåÂü∫ÂáÜÊ†áËÆ∞ÁöÑÊú∫Âô®‰∫∫Áä∂ÊÄÅ‰º∞ËÆ°ÊñπÊ≥ïÔºåÁî®‰∫éÁÆÄÂåñÊú∫Âô®‰∫∫-Áõ∏Êú∫Â§ñÂèÇÊ†áÂÆöÂíåÂÖ≥ËäÇÁä∂ÊÄÅÊÅ¢Â§çÔºå‰∏éÂº∫ÂåñÂ≠¶‰π†„ÄÅÊâ©Êï£Ê®°ÂûãÁ≠âÂÖ≥ÈîÆËØçÊó†ÂÖ≥„ÄÇ",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T22:04:25Z",
    "created_at": "2026-01-21T12:09:13.545540",
    "updated_at": "2026-01-21T12:09:13.545547",
    "recommend": 0
}