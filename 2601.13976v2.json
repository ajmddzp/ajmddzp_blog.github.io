{
    "id": "2601.13976v2",
    "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
    "authors": [
        "Jing Zuo",
        "Lingzhou Mu",
        "Fan Jiang",
        "Chengcheng Ma",
        "Mu Xu",
        "Yonggang Qi"
    ],
    "abstract": "åœ¨è§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»»åŠ¡ä¸­è¾¾åˆ°äººç±»æ°´å¹³çš„è¡¨ç°ï¼Œè¦æ±‚å…·èº«æ™ºèƒ½ä½“èƒ½å¤ŸåŒæ—¶ç†è§£å¤šæ¨¡æ€æŒ‡ä»¤ä¸è§†è§‰ç©ºé—´ä¸Šä¸‹æ–‡ï¼Œå¹¶åœ¨é•¿åŠ¨ä½œåºåˆ—ä¸­è¿›è¡Œæ¨ç†ã€‚è¿‘æœŸç ”ç©¶å¦‚NavCoTå’ŒNavGPT-2å±•ç¤ºäº†æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†åœ¨æå‡å¯è§£é‡Šæ€§ä¸é•¿ç¨‹è§„åˆ’èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼ŒOctoNav-R1å’ŒCoT-VLAç­‰å¤šæ¨¡æ€æ‰©å±•å·¥ä½œè¿›ä¸€æ­¥éªŒè¯äº†CoTä½œä¸ºå®ç°ç±»äººå¯¼èˆªæ¨ç†çš„æœ‰æ•ˆè·¯å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨æ˜æ˜¾ç¼ºé™·ï¼šçº¯æ–‡æœ¬CoTç¼ºä¹ç©ºé—´åŸºç¡€ä¸”æ˜“å—ç¨€ç–æ ‡æ³¨æ¨ç†æ­¥éª¤çš„è¿‡æ‹Ÿåˆå½±å“ï¼Œè€Œå¤šæ¨¡æ€CoTå› ç”Ÿæˆæƒ³è±¡çš„è§†è§‰è§‚å¯Ÿå¯¼è‡´ä¸¥é‡çš„æ ‡è®°è†¨èƒ€ï¼Œä½¿å¾—å®æ—¶å¯¼èˆªéš¾ä»¥å®ç°ã€‚æœ¬æ–‡æå‡ºFantasyVLNâ€”â€”ä¸€ä¸ªç»Ÿä¸€çš„éšå¼æ¨ç†æ¡†æ¶ï¼Œåœ¨ä¿ç•™CoTæ¨ç†ä¼˜åŠ¿çš„åŒæ—¶é¿å…äº†æ˜¾å¼çš„æ ‡è®°å¼€é”€ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨CoTæ¨ç†è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡é¢„è®­ç»ƒçš„è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ˆVARï¼‰å°†æƒ³è±¡çš„è§†è§‰æ ‡è®°ç¼–ç è‡³ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶é‡‡ç”¨ç»Ÿä¸€çš„å¤šCoTç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿè”åˆå­¦ä¹ æ–‡æœ¬ã€è§†è§‰åŠå¤šæ¨¡æ€CoTæ¨¡å¼ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç›´æ¥æ‰§è¡ŒæŒ‡ä»¤åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼ŒåŒæ—¶ä»èƒ½ä¿æŒæ¨ç†æ„ŸçŸ¥çš„è¡¨å¾èƒ½åŠ›ã€‚åœ¨LH-VLNæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†å…¼å…·æ¨ç†æ„ŸçŸ¥ä¸å®æ—¶æ€§çš„å¯¼èˆªï¼Œåœ¨æ˜¾è‘—æå‡æˆåŠŸç‡ä¸æ•ˆç‡çš„åŒæ—¶ï¼Œå°†æ¨ç†å»¶è¿Ÿè¾ƒæ˜¾å¼CoTæ–¹æ³•é™ä½äº†ä¸€ä¸ªæ•°é‡çº§ã€‚",
    "url": "https://arxiv.org/abs/2601.13976v2",
    "html_url": "https://arxiv.org/html/2601.13976v2",
    "html_content": "FantasyVLN\n: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation\nJing Zuo\n1,2\n*\nÂ§\n\\S\n,â€ƒLingzhou Mu\n1,3\n*\nÂ§\n\\S\n,â€ƒFan Jiang\n1\n*\nâ€ \n\\dagger\nâ€¡\n\\ddagger\n,â€ƒChengcheng Ma\n1\n,\nMu Xu\n1\n,\nYonggang Qi\n2\nâ€¡\n\\ddagger\n1\nFantasy AIGC Team,\n2\nBeijing University of Posts and Telecommunications,\n3\nTsinghua University\njiangfan0576@gmail.com\nqiyg@bupt.edu.cn\nAbstract\nAchieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose\nFantasyVLN\n, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods. Project:\nhttps://fantasy-amap.github.io/fantasy-vln/\n1\n1\nfootnotetext:\nEqual contribution.\n2\n2\nfootnotetext:\nProject leader.\n3\n3\nfootnotetext:\nCorresponding author.\n4\n4\nfootnotetext:\nWork done during internship at Fantasy AIGC Team.\nFigure 1:\nOverview of\nFantasyVLN\n.\nFantasyVLN\nis a VLN framework that integrates the strengths of textual and visual CoT reasoning modes, thereby jointly modeling semantic planning and spatial understanding.\n1\nIntroduction\nVision-and-Language Navigation (VLN) aims to enable an embodied agent to follow natural-language instructions and navigate complex visual environments\nKrantz\net al.\n(\n2020\n); Wu\net al.\n(\n2024\n); Anderson\net al.\n(\n2018\n); Gu\net al.\n(\n2022\n)\n. Solving this task requires the joint understanding of semantics from language and spatial geometry from visual observations, along with long-horizon reasoning to plan a sequence of actions. In particular, for multi-stage and long-horizon navigation scenarios as proposed in\n(Song\net al.\n,\n2025\n)\n, the ability to perform robust multimodal reasoning, i.e., to integrate linguistic intent with visual-spatial context over extended temporal dependencies, is especially critical. Despite the progress made by recent multimodal large models, achieving effective cross-modal reasoning in VLN remains challenging due to the semanticâ€“spatial gap and the need for interpretable yet sample-efficient reasoning mechanisms.\nThe recent success of large language models (LLMs) has inspired the integration of Chain-of-Thought (CoT) reasoning into embodied navigation to improve interpretability and long-horizon decision-making. Methods such as NavCoT\n(Lin\net al.\n,\n2025b\n)\nand NavGPT-2\n(Zhou\net al.\n,\n2024\n)\nemploy step-by-step textual reasoning to decompose navigation instructions or generate intermediate subgoals. However, their reasoning remains confined to the textual modality, typically by translating observations into captions, thereby limiting the joint modeling of semantic planning and spatial understanding, both essential for successful navigation. This limitation is compounded by the difficulty of annotating CoT supervision in VLN, as highlighted by EvolveNav\n(Lin\net al.\n,\n2025a\n)\n, where multiple valid action sequences often exist. Moreover, explicitly supervised CoT reasoning tends to overfit training distributions and generalize poorly to unseen environments.\nLately, works such as CoT-VLA\n(Zhao\net al.\n,\n2025\n)\n, VISTA\n(Huang\net al.\n,\n2025\n)\n, RBF++\n(Chen\net al.\n,\n2025\n)\n, OctoNav-R1\n(Gao\net al.\n,\n2025\n)\n, and OmniNav\n(Xue\net al.\n,\n2025\n)\nhave extended CoT reasoning into visual or multimodal domains to better couple semantic and spatial reasoning for generalizability. While this multimodal CoT paradigm marks an important step forward, it also introduces new challenges for long-horizon navigation. In particular, modeling reasoning chains across both language and vision requires the model to iteratively generate and interpret imagined intermediate observations at each step, leading to severe token inflation.\nA typical reasoning step spanning 5â€“7 actions expands into over 3kâ€“5k tokens, an order of magnitude larger than purely textual CoTs (usually\n<\n<\n500 tokens).\nThis explosion in sequence length drastically increases both training and inference latency, rendering real-time navigation infeasible even on high-end GPUs.\nTo address these challenges, we propose a unified implicit reasoning framework that retains the benefits of CoT-style reasoning while eliminating its explicit token overhead during inference. The key idea is twofold: (i) During training, we encode the imagined observation tokens generated by multimodal CoT reasoning into a compact latent space using a pretrained Visual AutoRegressive (VAR) model. This significantly reduces sequence length and training cost without compromising the richness of visual reasoning. (ii) At inference, the agent performs direct instruction-to-action mapping while still leveraging reasoning-aware representations, inspired by the\ntrain-with-CoT, infer-without-CoT\nparadigm of Aux-Think\n(Wang\net al.\n,\n2025\n)\n.\nConcretely, we introduce a unified multi-CoT training strategy that jointly learns from textual-only, visual-only, and textualâ€“visual CoT modes using a special tag token to indicate each mode. This design unifies both the input format and model parameters within a single framework. During training, we align the action predictions from CoT-based reasoning modes with those from direct prediction (without CoT), enforcing modality-invariant reasoning representations. Consequently, the model learns implicit reasoning capabilities that generalize effectively without explicit CoT supervision or overfitting to training distributions.\nTo this end, our contributions are summarized as follows: (i) We propose the first unified implicit CoT reasoning framework that integrates textual, visual, and multimodal CoT paradigms within a single model. Unlike prior explicit CoT methods, our approach trains with diverse reasoning modes but performs inference without generating CoT sequences, achieving reasoning-aware yet real-time navigation. (ii) We introduce a gating-based multi-CoT learning mechanism that allows seamless switching among reasoning modes and direct action prediction under shared parameters. By aligning CoT-driven and direct action predictions, our model learns consistent, modality-invariant reasoning representations. (iii) To reduce the token explosion in multimodal reasoning, we compress imagined observation tokens into a compact latent space using a pretrained Visual AutoRegressor (VAR), improving training efficiency while preserving semanticâ€“spatial reasoning capacity. (iv) Extensive experiments on the challenging LH-VLN benchmark demonstrate that our method substantially improves navigation success and efficiency in multi-stage and long-horizon scenarios, while reducing inference latency by an order of magnitude compared to explicit CoT approaches.\nFigure 2:\nOverview of our unified multimodal Chain-of-Thought reasoning framework. The model supports four reasoning modes under a shared architecture: (a) non-CoT reasoning for real-time inference, (b) textual CoT, (c) visual CoT enabled by VAR-compressed imagined observations, and (d) multimodal CoT combining textual and visual reasoning. A gating mechanism switches the model across reasoning modes, while the action predictions from CoT modes are consistently aligned with the non-CoT mode.\n2\nRelated Works\n2.1\nVision-and-Language Navigation\nEarly VLN models typically separate perception, instruction understanding, and action planning into discrete modules, and rely on imitation\n(Nguyen\net al.\n,\n2019\n; Wang\net al.\n,\n2022\n; Wu\net al.\n,\n2020\n)\nor reinforcement learning\n(Xu\net al.\n,\n2023\n; Wang\net al.\n,\n2020\n)\nwith auxiliary tasks such as progress monitoring or instruction reweighting.\nHowever, these methods, built on panoramic observations in discrete environments (e.g., R2R\n(Anderson\net al.\n,\n2018\n)\nand RxR\n(Ku\net al.\n,\n2020\n)\n), suffer from poor semantic alignment and limited generalization in continuous or unseen environments (e.g. VLN-CE\n(Krantz\net al.\n,\n2020\n)\n).\nTo address these limitations, recent studies have shifted toward end-to-end navigation policy learning with pretrained vision-language models.\nFor example, Poliformer\n(Zeng\net al.\n,\n2025\n)\nintroduces transformer-based on-policy reinforcement learning for video-level navigation.\nNaVid\n(Zhang\net al.\n,\n2024a\n)\nand Uni-NaVid\n(Zhang\net al.\n,\n2025a\n)\nextend this paradigm by performing monocular video-based navigation without depth or maps and unifying multiple embodied tasks.\nNaVILA\n(Cheng\net al.\n,\n2024\n)\nfurther integrates VLN with legged robot locomotion, achieving impressive cross-embodiment generalization.\nWhile achieving remarkable progress on short-term tasks, they still struggle to reason and plan for long-horizon, multi-stage tasks.\nMore recently, CoT reasoning has emerged as a crucial paradigm for Embodied AI tasks.\nIn VLN, NavGPT leverages the zero-shot CoT reasoning ability of GPT-4, while Aux-Think introduces auxiliary CoT supervision to internalize reasoning patterns during training.\nYet, existing CoT-based VLN methods confine reasoning within a single modality, leaving multimodal CoT largely unexplored.\nIn this paper, we follow the continuous environment setting and systematically investigate multimodal CoT reasoning in VLN.\n2.2\nChain-of-Thought Reasoning\nChain-of-Thought (CoT) reasoning enables large language models (LLMs) to solve complex problems by explicitly generating intermediate steps\n(Wei\net al.\n,\n2022\n)\n.\nSubsequent variants, such as Self-Consistency\n(Wang\net al.\n,\n2023\n)\nand Least-to-Most Prompting\n(Zhou\net al.\n,\n2023\n)\n, further enhance reasoning robustness and compositionality.\nRecent studies have extended CoT to vision-language models\n(Zhang\net al.\n,\n2024b\n)\n, which can be categorized into three types based on the modality of reasoning steps: Textual CoT, Visual CoT, and Multimodal CoT.\nSpecifically, Textual CoT\n(Zhang\net al.\n,\n2024c\n)\nin VLMs typically follows the format of vanilla LLM CoT.\nVisual CoT methods, such as CoT-VLA\n(Zhao\net al.\n,\n2025\n)\nand DreamVLA\n(Zhang\net al.\n,\n2025b\n)\n, generate future frames before action prediction in manipulation tasks, while Multimodal CoT\n(Cheng\net al.\n,\n2025\n)\njointly predicts paired textual and visual reasoning steps in multimodal tasks.\nTo the best of our knowledge,\nFantasyVLN\nis the first unified CoT reasoning framework that integrates these three reasoning paradigms.\n3\nMethods\n3.1\nOverview\nWe propose\nFantasyVLN\n, a VLN framework that integrates multimodal reasoning modes as its core design, while enabling implicit reasoning for efficient inference. As shown in Figure\n2\n,\nFantasyVLN\ninternalizes diverse CoT reasoning patterns across modalities through end-to-end joint training, and enhances the non-CoT reasoning mode via a cross-mode alignment constraint.\nThis enables combining the advantages of both textual and visual CoT reasoning without incurring explicit CoT reasoning time.\nMoreover, we perform visual CoT reasoning in the latent space of the VAR model\n(Tian\net al.\n,\n2024\n)\n, which improves training and inference efficiency compared with pixel-space methods.\nBelow, we introduce the problem setup, cross-mode alignment constraint, unified multimodal implicit reasoning and latent visual CoT learning.\n3.2\nProblem Setup\nVLN aims to develop an embodied agent\nÏ€\nÎ¸\n\\pi_{\\theta}\nthat navigates continuous 3D environments\nğ’ª\n\\mathcal{O}\nbased on a natural language instruction\nâ„\n\\mathcal{I}\nand visual observations, which can be formulated as a non-Markovian temporal decision problem.\nLet\ns\n0\ns_{0}\ndenote the initial state, i.e., location and orientation, and\nğ’°\n\\mathcal{U}\ndenote the action space.\nAt each timestep\nt\nt\n, the agent\nÏ€\nÎ¸\n\\pi_{\\theta}\nreceives multi-view visual observations\no\nt\nâˆˆ\nğ’ª\no_{t}\\in\\mathcal{O}\nand predicts future actions\nğ’œ\nt\nâˆˆ\nğ’°\n\\mathcal{A}_{t}\\in\\mathcal{U}\nconditioned on the instruction\nâ„\n\\mathcal{I}\nand historical observations\n{\no\nâ‰¤\nt\n}\n\\{o_{\\leq t}\\}\n.\nSubsequently, the predicted actions\nğ’œ\nt\n\\mathcal{A}_{t}\nare executed, transferring the agent\nÏ€\nÎ¸\n\\pi_{\\theta}\nto a new state according to the environment dynamics.\nThis interaction process continues until a stop action is executed or the maximum step\nT\nT\nis reached.\n3.3\nCompact Visual Chain-of-Thought\nConventional V-CoT reasoning predicts thousands of visual tokens at each reasoning step, resulting in low training efficiency and high inference latency.\nTo address this issue, we present\nCompact Visual Chain-of-Thought\n(CompV-CoT), which trains\nQwen2.5-VL\nto directly generate a compact set of visual tokens in the latent space of a pretrained VAR model, yielding a novel compressed visual chain-of-thought representation with far fewer tokens.\nThe VAR model follows a next-scale prediction paradigm to hierarchically encode visual information, achieving higher efficiency than conventional autoencoding approaches such as VAE, VQ-VAE\n(Gafni\net al.\n,\n2022\n)\nor RAE\n(Zheng\net al.\n,\n2025\n)\n.\nGiven a\n256\nÃ—\n256\n256\\times 256\nimage, the VAR model enables precise reconstruction using the corresponding low-scale representations, which contain only\n30\n30\nvisual tokens.\nAs shown in Table\n1\n, VAR achieves a higher compression ratio under comparable reconstruction quality.\nTable 1:\nComparison of compression ratio and reconstruction error (MSE) across different visual compressors.\nCompressors\nComp. Ratio\nMSE\nRAE-DINOv2-B\n1/256\n0.012\nRAE-SigLIP2-B\n1/256\n0.011\nVAE\n1/64\n0.005\nVQVAE\n1/64\n0.007\nVAR\n1/2185\n0.039\nSpecifically, we employ the VAR model as the visual decoder of the VLM and perform V-CoT in the VAR latent space.\nThe VLM first takes navigation instructions and visual observations as input and then generates latent representations of future observations before predicting actions.\nThe VAR model finally decodes generated representations into pixel frames.\nWe freeze the VAR model during training, while the VLM first learns to predict latent future observations and then infers the corresponding actions.\nDuring inference, we use only the VLM to perform visual CoT-based navigation without explicit VAR decoding.\nOwing to the highly efficient visual information compression and non-display image decoding, the proposed CompV-CoT method improves both training and inference efficiency.\n3.4\nUnified Multimodal Chain-of-Thought\nBuilding on CompV-CoT, we further present a\nUnified Multimodal Chain-of-Thought\n(UM-CoT) framework that integrates textual, compressed visual, and multimodal reasoning within a single agent.\nTextual CoT in VLN.\nTextual CoT (T-CoT) models the agentâ€™s reasoning as an explicit semantic planning process that bridges language understanding and action decision.\nInstead of directly mapping instructions to actions, the agent first generates textual intermediate reasoning steps\nğ’¯\n^\nt\n\\widehat{\\mathcal{T}}_{t}\n.\nThese reasoning steps then provide structured causal guidance for predicting subsequent actions\nğ’œ\n^\nt\n\\widehat{\\mathcal{A}}_{t}\n, enabling interpretable and more reliable decision-making.\nSpecifically, the intermediate steps typically involve inferring subgoals from the instruction, assessing progress through current and historical visual observations, and identifying actionable cues for achieving the next objective.\nCompV-CoT as Visual CoT.\nFor visual CoT (V-CoT), we directly adopt the CompV-CoT introduced in Sec.\n3.3\nas the visual reasoning mode in UM-CoT.\nIn this setting, the agent imagines future observations in the VAR latent space by predicting compressed visual tokens, and then infers actions conditioned on the imagined latent trajectory.\nCompared with pixel-space prediction, this CompV-CoT design yields more efficient and stable visual reasoning.\nMultimodal CoT in VLN.\nMultimodal CoT (MM-CoT) is defined as a native combination of T-CoT and CompV-CoT, where the agent is required to generate paired textualâ€“visual reasoning steps.\nWe denote the multimodal reasoning trace as\nâ„³\nt\n=\n[\nğ’¯\nt\n,\nğ’±\nt\n]\n\\mathcal{M}_{t}=[\\mathcal{T}_{t},\\mathcal{V}_{t}]\n, which jointly encodes semantic plans and imagined future observations, and use it to guide subsequent action prediction.\nUnified Multimodal CoT Reasoning Framework.\nTo unify the above reasoning modes within a single framework, we introduce two binary gating signals\ng\nğ’¯\ng_{\\mathcal{T}}\nand\ng\nğ’±\ng_{\\mathcal{V}}\nthat control whether textual and visual reasoning is activated.\nGiven\nâ„\n\\mathcal{I}\n,\n{\no\nâ‰¤\nt\n}\n\\{o_{\\leq t}\\}\n, and\n(\ng\nğ’¯\n,\ng\nğ’±\n)\n(g_{\\mathcal{T}},g_{\\mathcal{V}})\n, the agent jointly predicts reasoning traces and actions:\n[\nâ„›\n^\nt\n,\nğ’œ\n^\nt\n]\n=\nÏ€\nÎ¸\nâ€‹\n(\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\ng\nğ’¯\n,\ng\nğ’±\n)\n,\n[\\widehat{\\mathcal{R}}_{t},\\widehat{\\mathcal{A}}_{t}]=\\pi_{\\theta}\\big(\\mathcal{I},\\{o_{\\leq t}\\},g_{\\mathcal{T}},g_{\\mathcal{V}}\\big),\n(1)\nwhere\nâ„›\n^\nt\n=\n{\nNone\n,\nif\nâ€‹\n(\ng\nğ’¯\n,\ng\nğ’±\n)\n=\n(\n0\n,\n0\n)\n,\nğ’¯\n^\nt\n,\nif\nâ€‹\n(\ng\nğ’¯\n,\ng\nğ’±\n)\n=\n(\n1\n,\n0\n)\n,\nğ’±\n^\nt\n,\nif\nâ€‹\n(\ng\nğ’¯\n,\ng\nğ’±\n)\n=\n(\n0\n,\n1\n)\n,\nâ„³\n^\nt\n,\nif\nâ€‹\n(\ng\nğ’¯\n,\ng\nğ’±\n)\n=\n(\n1\n,\n1\n)\n.\n\\widehat{\\mathcal{R}}_{t}=\\begin{cases}\\text{None},&\\text{if }(g_{\\mathcal{T}},g_{\\mathcal{V}})=(0,0),\\\\[4.0pt]\n\\widehat{\\mathcal{T}}_{t},&\\text{if }(g_{\\mathcal{T}},g_{\\mathcal{V}})=(1,0),\\\\[4.0pt]\n\\widehat{\\mathcal{V}}_{t},&\\text{if }(g_{\\mathcal{T}},g_{\\mathcal{V}})=(0,1),\\\\[4.0pt]\n\\widehat{\\mathcal{M}}_{t},&\\text{if }(g_{\\mathcal{T}},g_{\\mathcal{V}})=(1,1).\\end{cases}\n(2)\nThis gating mechanism allows a single policy to flexibly operate in\nCoT, T-CoT, CompV-CoT, and MM-CoT modes.\nJoint Training via Data Mixture.\nGiven the navigation instruction\nâ„\n\\mathcal{I}\n, visual observations\n{\no\nâ‰¤\nt\n}\n\\{o_{\\leq t}\\}\nand the ground truth action\nğ’œ\nt\n\\mathcal{A}_{t}\n.\nTo enable end-to-end training, we organize the expert navigation dataset\nğ’Ÿ\n\\mathcal{D}\ninto five-tuples:\n[\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\nğ’¯\nt\n,\nğ’±\nt\n,\nğ’œ\nt\n]\nâˆˆ\nğ’Ÿ\n,\n[\\mathcal{I},\\{o_{\\leq t}\\},\\mathcal{T}_{t},\\mathcal{V}_{t},\\mathcal{A}_{t}]\\in\\mathcal{D},\n(3)\nwhere\nğ’¯\nt\n\\mathcal{T}_{t}\nand\nğ’±\nt\n\\mathcal{V}_{t}\ndenote the ground truth textual reasoning steps and CompV-CoT visual reasoning steps, respectively.\nWe employ\nQwen-VL-Max\nto generate textual reasoning traces\nğ’¯\nt\n\\mathcal{T}_{t}\n.\nDuring training,\n(\ng\nğ’¯\n,\ng\nğ’±\n)\n(g_{\\mathcal{T}},g_{\\mathcal{V}})\nare uniformly sampled and integrated with\nâ„\n\\mathcal{I}\nand\n{\no\nâ‰¤\nt\n}\n\\{o_{\\leq t}\\}\nto form the query, while the answer is constructed according to Eq.Â (\n2\n) by selecting\nâ„›\nt\nâˆˆ\n{\nNone\n,\nğ’¯\nt\n,\nğ’±\nt\n,\nâ„³\nt\n}\n\\mathcal{R}_{t}\\in\\{\\text{None},\\mathcal{T}_{t},\\mathcal{V}_{t},\\mathcal{M}_{t}\\}\ntogether with\nğ’œ\nt\n\\mathcal{A}_{t}\n.\nThe joint objective is defined as:\nâ„’\nJoint\n\\displaystyle\\mathcal{L}_{\\text{Joint}}\n=\n(\nÂ¬\ng\nğ’¯\nâˆ§\nÂ¬\ng\nğ’±\n)\nâ€‹\nâ„’\nCE\nâ€‹\n(\nğ’œ\n^\nt\n,\nğ’œ\nt\n)\n\\displaystyle=(\\neg g_{\\mathcal{T}}\\land\\neg g_{\\mathcal{V}})\\,\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{A}}_{t},\\mathcal{A}_{t}\\big)\n(4)\n+\n(\ng\nğ’¯\nâˆ§\nÂ¬\ng\nğ’±\n)\nâ€‹\nâ„’\nCE\nâ€‹\n(\n[\nğ’¯\n^\nt\n,\nğ’œ\n^\nt\n]\n,\n[\nğ’¯\nt\n,\nğ’œ\nt\n]\n)\n\\displaystyle+(g_{\\mathcal{T}}\\land\\neg g_{\\mathcal{V}})\\,\\mathcal{L}_{\\text{CE}}\\big([\\widehat{\\mathcal{T}}_{t},\\widehat{\\mathcal{A}}_{t}],[\\mathcal{T}_{t},\\mathcal{A}_{t}]\\big)\n+\n(\nÂ¬\ng\nğ’¯\nâˆ§\ng\nğ’±\n)\nâ€‹\nâ„’\nCE\nâ€‹\n(\n[\nğ’±\n^\nt\n,\nğ’œ\n^\nt\n]\n,\n[\nğ’±\nt\n,\nğ’œ\nt\n]\n)\n\\displaystyle+(\\neg g_{\\mathcal{T}}\\land g_{\\mathcal{V}})\\,\\mathcal{L}_{\\text{CE}}\\big([\\widehat{\\mathcal{V}}_{t},\\widehat{\\mathcal{A}}_{t}],[\\mathcal{V}_{t},\\mathcal{A}_{t}]\\big)\n+\n(\ng\nğ’¯\nâˆ§\ng\nğ’±\n)\nâ€‹\nâ„’\nCE\nâ€‹\n(\n[\nâ„³\n^\nt\n,\nğ’œ\n^\nt\n]\n,\n[\nâ„³\nt\n,\nğ’œ\nt\n]\n)\n,\n\\displaystyle+(g_{\\mathcal{T}}\\land g_{\\mathcal{V}})\\,\\mathcal{L}_{\\text{CE}}\\big([\\widehat{\\mathcal{M}}_{t},\\widehat{\\mathcal{A}}_{t}],[\\mathcal{M}_{t},\\mathcal{A}_{t}]\\big),\nwhere\nâ„’\nCE\n\\mathcal{L}_{\\text{CE}}\ndenotes the causal cross-entropy loss.\nAlgorithm 1\nCross-Mode Aligned Joint Training\n1:\nInput:\nDataset\nğ’Ÿ\n\\mathcal{D}\n, parameters\nÎ¸\n\\theta\n, learning rate\nÎ·\n\\eta\n, alignment weight\nÎ»\nalign\n\\lambda_{\\text{align}}\n2:\nOutput:\nTrained parameters\nÎ¸\nâˆ—\n\\theta^{*}\n3:\nwhile\nnot converged\ndo\n4:\n[\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\nğ’¯\nt\n,\nğ’±\nt\n,\nğ’œ\nt\n]\nâˆ¼\nğ’Ÿ\n[\\mathcal{I},\\{o_{\\leq t}\\},\\mathcal{T}_{t},\\mathcal{V}_{t},\\mathcal{A}_{t}]\\sim\\mathcal{D}\n5:\nğ’œ\n^\nt\nâ†\nÏ€\nÎ¸\n(\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\ng\nğ’¯\n=\n0\n,\ng\nğ’±\n=\n0\n)\n\\widehat{\\mathcal{A}}_{t}\\leftarrow\\pi_{\\theta}(\\mathcal{I},\\{o_{\\leq t}\\},g_{\\mathcal{T}}{=}0,g_{\\mathcal{V}}{=}0)\n6:\nÎ¸\nâ†\nÎ¸\nâˆ’\nÎ·\nâ€‹\nâˆ‡\nÎ¸\nâ„’\nCE\nâ€‹\n(\nğ’œ\n^\nt\n,\nğ’œ\nt\n)\n\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}\\mathcal{L}_{\\text{CE}}(\\widehat{\\mathcal{A}}_{t},\\mathcal{A}_{t})\n7:\nğ’œ\n~\nt\nâ†\nsg\n[\nÏ€\nÎ¸\n(\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\ng\nğ’¯\n=\n0\n,\ng\nğ’±\n=\n0\n)\n]\n\\widetilde{\\mathcal{A}}_{t}\\leftarrow\\text{sg}\\big[\\pi_{\\theta}(\\mathcal{I},\\{o_{\\leq t}\\},g_{\\mathcal{T}}{=}0,g_{\\mathcal{V}}{=}0)\\big]\n8:\n[\nğ’¯\n^\nt\n,\nğ’œ\n^\nt\nğ’¯\n]\nâ†\nÏ€\nÎ¸\n(\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\ng\nğ’¯\n=\n1\n,\ng\nğ’±\n=\n0\n)\n[\\widehat{\\mathcal{T}}_{t},\\widehat{\\mathcal{A}}_{t}^{\\mathcal{T}}]\\leftarrow\\pi_{\\theta}(\\mathcal{I},\\{o_{\\leq t}\\},g_{\\mathcal{T}}{=}1,g_{\\mathcal{V}}{=}0)\n9:\n[\nğ’±\n^\nt\n,\nğ’œ\n^\nt\nğ’±\n]\nâ†\nÏ€\nÎ¸\n(\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\ng\nğ’¯\n=\n0\n,\ng\nğ’±\n=\n1\n)\n[\\widehat{\\mathcal{V}}_{t},\\widehat{\\mathcal{A}}_{t}^{\\mathcal{V}}]\\leftarrow\\pi_{\\theta}(\\mathcal{I},\\{o_{\\leq t}\\},g_{\\mathcal{T}}{=}0,g_{\\mathcal{V}}{=}1)\n10:\n[\nâ„³\n^\nt\n,\nğ’œ\n^\nt\nâ„³\n]\nâ†\nÏ€\nÎ¸\n(\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\ng\nğ’¯\n=\n1\n,\ng\nğ’±\n=\n1\n)\n[\\widehat{\\mathcal{M}}_{t},\\widehat{\\mathcal{A}}_{t}^{\\mathcal{M}}]\\leftarrow\\pi_{\\theta}(\\mathcal{I},\\{o_{\\leq t}\\},g_{\\mathcal{T}}{=}1,g_{\\mathcal{V}}{=}1)\n11:\nCompute\nâ„’\nJoint\nâˆ—\n\\mathcal{L}_{\\text{Joint}}^{*}\nusing Eq.Â (\n7\n)\n12:\nÎ¸\nâ†\nÎ¸\nâˆ’\nÎ·\nâ€‹\nâˆ‡\nÎ¸\nâ„’\nJoint\nâˆ—\n\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}\\mathcal{L}_{\\text{Joint}}^{*}\n13:\nend\nwhile\n14:\nÎ¸\nâˆ—\nâ†\nÎ¸\n\\theta^{*}\\leftarrow\\theta\n15:\nreturn\nÎ¸\nâˆ—\n\\theta^{*}\n3.5\nCross-Mode Alignment Constraint\nTo prevant conflict between different reasoning modes, we introduce a\nCross-Mode Alignment Constraint\nthat regularizes the unified multimodal CoT training.\nThe key idea is to use the non-CoT reasoning mode as a supervisory signal to align all CoT variants, thereby embedding diverse reasoning behaviors into a shared latent policy.\nLet\nğ’œ\n^\nt\n,\nğ’œ\n^\nt\nğ’¯\n,\nğ’œ\n^\nt\nğ’±\n\\widehat{\\mathcal{A}}_{t},\\widehat{\\mathcal{A}}_{t}^{\\mathcal{T}},\\widehat{\\mathcal{A}}_{t}^{\\mathcal{V}}\n, and\nğ’œ\n^\nt\nâ„³\n\\widehat{\\mathcal{A}}_{t}^{\\mathcal{M}}\ndenote the action predictions from the non-CoT, T-CoT, V-CoT, and MM-CoT reasoning modes, respectively.\nIn each iteration, we first optimize the non-CoT reasoning mode with the objective:\nâ„’\nnon-CoT\n=\nâ„’\nCE\nâ€‹\n(\nğ’œ\n^\nt\n,\nğ’œ\nt\n)\n,\n\\mathcal{L}_{\\text{non-CoT}}=\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{A}}_{t},\\mathcal{A}_{t}\\big),\n(5)\nwhere\nğ’œ\n^\nt\n=\nÏ€\nÎ¸\n(\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\ng\nğ’¯\n=\n0\n,\ng\nğ’±\n=\n0\n)\n.\n\\widehat{\\mathcal{A}}_{t}=\\pi_{\\theta}\\big(\\mathcal{I},\\{o_{\\leq t}\\},g_{\\mathcal{T}}=0,g_{\\mathcal{V}}=0\\big).\n(6)\nWe then obtain the soft targets\nğ’œ\n~\nt\n\\widetilde{\\mathcal{A}}_{t}\nby rerunning the forward processÂ (\n6\n).\nFinally, we incorporate the cross-mode alignment constraint into the joint objective of unified multimodal CoT reasoning:\nâ„’\nJoint\nâˆ—\n=\nâ„’\nAlign\n+\nâ„’\nCoT\n,\n\\mathcal{L}_{\\text{Joint}}^{*}=\\mathcal{L}_{\\text{Align}}+\\mathcal{L}_{\\text{CoT}},\n(7)\nwhere\nâ„’\nAlign\n=\nâ„’\nCE\nâ€‹\n(\nğ’œ\n^\nt\nğ’¯\n,\nğ’œ\n~\nt\n)\n\\displaystyle\\mathcal{L}_{\\text{Align}}=\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{A}}_{t}^{\\mathcal{T}},\\widetilde{\\mathcal{A}}_{t}\\big)\n(8)\n+\nâ„’\nCE\nâ€‹\n(\nğ’œ\n^\nt\nğ’±\n,\nğ’œ\n~\nt\n)\n+\nâ„’\nCE\nâ€‹\n(\nğ’œ\n^\nt\nâ„³\n,\nğ’œ\n~\nt\n)\n,\n\\displaystyle\\quad+\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{A}}_{t}^{\\mathcal{V}},\\widetilde{\\mathcal{A}}_{t}\\big)+\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{A}}_{t}^{\\mathcal{M}},\\widetilde{\\mathcal{A}}_{t}\\big),\nand\nâ„’\nCoT\n=\nâ„’\nCE\nâ€‹\n(\nğ’¯\n^\nt\n,\nğ’¯\nt\n)\n\\displaystyle\\mathcal{L}_{\\text{CoT}}=\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{T}}_{t},\\mathcal{T}_{t}\\big)\n(9)\n+\nâ„’\nCE\nâ€‹\n(\nğ’±\n^\nt\n,\nğ’±\nt\n)\n+\nâ„’\nCE\nâ€‹\n(\nâ„³\n^\nt\n,\nâ„³\nt\n)\n.\n\\displaystyle\\quad+\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{V}}_{t},\\mathcal{V}_{t}\\big)+\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{M}}_{t},\\mathcal{M}_{t}\\big).\nWe alternately minimize the non-CoT objectiveÂ (\n5\n) and the cross-mode aligned joint objectiveÂ (\n7\n) until the training losses\nâ„’\nnon-CoT\n\\mathcal{L}_{\\text{non-CoT}}\nand\nâ„’\nJoint\nâˆ—\n\\mathcal{L}_{\\text{Joint}}^{*}\nconverge.\nDuring this alternating optimization, all reasoning modes operate on similar inputs, share network parameters, and are aligned to identical supervisory signals, thereby implicitly embedding diverse CoT reasoning patterns into a unified latent representation.\nThe overall algorithm is presented in Algorithm\n1\n.\n3.6\nVLN During Inference\nDue to the real-time demands of VLN and the inference latency introduced by explicit CoT token decoding, we follow Aux-Think\n(Wang\net al.\n,\n2025\n)\nand adopt the non-CoT reasoning mode during inference.\nSimilar to Aux-Think\n(Wang\net al.\n,\n2025\n)\n, our framework serves as an implicit reasoning mechanism that internalizes diverse CoT patterns and implicitly enhances non-CoT reasoning through cross-mode alignment and joint training across reasoning modes.\nTable 2:\nComparison of navigation accuracy across different VLN methods on LH-VLN benchmark. The best and second-best results are marked in bold and underlined, respectively.\nCoT Modal\nMethods\nSR\nISR\nCSR\nCGT\nNone/ZS\nRandom\n0\n0\n0\n0\nGLM-4v prompt\n0\n0\n0\n0\nGPT-4 + NaviLLM\n0\n2.19\n1.45\n2.61\nMGDM\n0\n2.34\n1.65\n2.91\nVisual\nCoT-VLA\n0\n0\n0\n0\nWorldVLA\n0\n0\n0\n0\nTextual\nAux-Think\n0.65\n3.16\n2.04\n1.47\nunified multimodal\nFantasyVLN\n2.44\n11.01\n9.64\n8.99\nTable 3:\nComparison of navigation accuracy across different reasoning mode combinations on LH-VLN.\nnon-CoT\nT-CoT\nV-CoT\nMM-CoT\nSR\nISR\nCSR\nCGT\nâœ“\n0\n2.01\n1.51\n1.55\nâœ“\nâœ“\n0.98\n8.26\n6.60\n6.15\nâœ“\nâœ“\n1.46\n11.19\n9.66\n8.84\nâœ“\nâœ“\n0.49\n7.77\n6.48\n8.89\nâœ“\nâœ“\nâœ“\nâœ“\n2.44\n11.01\n9.64\n8.99\n4\nExperiments\n4.1\nExperimental Setup\nBenchmark.\nWe evaluate\nFantasyVLN\non the challenging LH-VLN\n(Song\net al.\n,\n2025\n)\nbenchmark, which is characterized by multi-stage tasks and long navigation trajectories.\nOn one hand, multi-stage navigation requires the agent to sequentially reach multiple goals, imposing higher demands on reasoning and planning.\nOn the other hand, longer navigation trajectories amplify cumulative errors compared to shorter ones.\nFollowing the standard LH-VLN setting, we perform online evaluation on the test set, where both the tasks and scenes are unseen.\nBaselines.\nWe compared the proposed\nFantasyVLN\nwith several representative methods. They can be divided into four categories:\n(i) textual CoT-based methods, Aux-Think\n(Wang\net al.\n,\n2025\n)\n;\n(ii) visual CoT-based methods, CoT-VLA and WorldVLA;\n(iii) memory-based methods, MGDM;\n(iv) other baselines provided by LH-VLN, GLM-4v prompt, NaviLLM and GPT-4 + NaviLLM.\nFor fair comparison, all methods are trained on the same LH-VLN training set, and the validation set is used to select the best checkpoint for each method.\nFor Aux-Think and CoT-VLA, we implement their methods based on the descriptions in their papers, as the training codes are not publicly available.\nFor WorldVLA, we adapt the official implementation by modifying the preprocessing pipeline to support training on the LH-VLN dataset.\nFor all other methods, we use the implementations provided by LH-VLN.\nMetrics.\nFollowing\n(Song\net al.\n,\n2025\n)\n, we use Success Rate (SR), Independent Success Rate (ISR), Conditional Success Rate (CSR), and CSR weighted by Ground Truth (CGT) to measure multi-stage navigation accuracy.\nSR denotes the success rate of multi-stage task navigation, ISR represents the success rate of individual subtasks, CSR weights the ISR according to the success of preceding subtasks, and CGT further weights the CSR based on the length of the expert trajectory.\nMoreover, we introduce the Action Per Second (APS) to evaluate inference efficiency:\nAPS\n=\nN\nact\nT\nnav\n,\n\\text{APS}=\\frac{N_{\\text{act}}}{T_{\\text{nav}}},\n(10)\nwhere\nN\nact\nN_{\\text{act}}\ndenotes the total number of executed actions, and\nT\nnav\nT_{\\text{nav}}\nrepresents the total navigation time in seconds.\nTable 4:\nComparison of inference efficiency across different CoT reasoning methods. The best results are marked in bold.\nReasoning Mode\nMethods\nModel Size\nAPS\nExplicit\nCoT-VLA\n7B\n0.19\nImplicit\nWorldVLA\n7B\n1.02\nAux-Think\n8B\n0.97\nFantasyVLN\n7B\n1.03\nFigure 3:\nISR variation with respect to different VAR scales.\nFigure 4:\nQualitative comparison of image reconstruction results produced by the VAR model using latent inputs across different scales. For each image, the VAR model receives the ground truth latents up to a specified scale and predicts all remaining scales; the final reconstruction is obtained by decoding the combined ground truth and predicted latents.\n4.2\nMain Results\nNavigation Accuracy.\nTable\n2\npresents the quantitative results of navigation accuracy across different VLN methods on the LH-VLN benchmark.\nFantasyVLN\nachieves superior performance across all metrics, with SR, ISR, CSR, and CGT of 2.44, 11.01, 9.64, and 8.99, respectively, significantly surpassing all baselines.\nAux-Think shows suboptimal results in SR, ISR, and CSR, indicating that T-CoT enhances navigation robustness compared to non-CoT approaches.\nHowever, its performance still exhibits a notable gap compared to\nFantasyVLN\n, owing to the limitations of single-modal CoT modeling and the lack of an explicitâ€“implicit alignment mechanism.\nMGDM performs relatively well among non-CoT baselines, particularly in CGT, suggesting that memory mechanisms offer limited yet tangible benefits.\nOverall, the results demonstrate that our unified multimodal implicit reasoning framework is crucial for tackling the complex multi-stage VLN task.\nInference Efficiency.\nTo quantify the inference efficiency of different CoT reasoning methods, we report APS in Table\n4\n.\nImplicit reasoning models, including\nFantasyVLN\n, Aux-Think, and WorldVLA, exhibit comparable efficiency and outperform the explicit approach CoT-VLA by a substantial margin.\nThis outcome is expected.\nImplicit reasoning predicts each action by decoding a single token, while explicit reasoning requires generating CoT reasoning steps with thousands of tokens.\nUnder similar model sizes, implicit CoT reasoning predicts approximately one action per second, while explicit CoT reasoning yields only 0.19 actions per second.\nTherefore, implicit reasoning better satisfies the real-time requirements of the VLN task.\n4.3\nAblation Studies\nContribution of Each Reasoning Mode.\nFantasyVLN\nintegrates diverse reasoning modes within a unified framework.\nTo verify the contribution of each reasoning mode to the overall framework, we explore various combinations of non-CoT, T-CoT, V-CoT, and MM-CoT modes during training.\nAs shown in Table\n3\n, combining any CoT reasoning mode with non-CoT reasoning consistently improves navigation performance across all metrics.\nIntegrating all four reasoning modes further enhances the overall performance.\nVAR Scale Selection.\nAs detailed in Section\n3.3\n, we perform V-CoT in the latent space of VAR.\nTo select the optimal VAR scale for latent V-CoT learning, we conduct comprehensive ablation studies on a subset of LH-VLN.\nWe first report the ISR results across different VAR scales, ranging from 1 to 10, as shown in Figure\n3\n.\nThe results show that scaleÂ 4 achieves the best performance.\nWe attribute this to smaller scales lacking sufficient visual information, while larger scales leading redundancy.\nTo validate this argument, we randomly sample 100 images from LH-VLN and employ a pretrained VAR model to reconstruct them.\nSpecifically, VAR takes the ground truth latents up to a given scale as input and predicts the remaining latent scales.\nThe reconstructed images are then obtained by decoding both the ground truth and predicted latents together.\nAs shown in Figure\n4\n, the results are consistent with our argument.\nEffect of Cross-Mode Alignment Constraint.\nWe introduce a cross-mode alignment constraint into the joint training of\nFantasyVLN\n.\nTable\n5\ncompares SR, ISR, CSR, and CGT performance with and without this constraint.\nWithout the cross-mode alignment constraint (âœ—),\nFantasyVLN\nexhibits weak navigation ability, achieving only marginal ISR, CSR, and CGT scores.\nAdopting this constraint (âœ“) yields substantial improvements across all metrics, with SR increasing from 0 to 2.44, ISR from 2.39 to 11.01, CSR from 1.19 to 9.64, and CGT from 1.28 to 8.99.\nThis indicates that cross-mode alignment is essential for\nFantasyVLN\n, a unified framework that integrates diverse reasoning modes.\nTable 5:\nComparison of SR, ISR, CSR, and CGT performance with and without cross-mode alignment.\nAlignment Constraint\nSR\nISR\nCSR\nCGT\nâœ—\n0\n2.39\n1.19\n1.28\nâœ“\n2.44\n11.01\n9.64\n8.99\n4.4\nMore Results\nTraining Efficiency.\nAs shown in Table\n2\n, existing visual CoT methods (e.g., WorldVLA) achieve limited navigation accuracy on the LH-VLN benchmark, failing to generalize to long-horizon scenarios. To understand the underlying cause, we compare their training efficiency with our unified multimodal formulation.\nAs shown in Figure\n5\n, WorldVLA exhibits slow and unstable convergence, requiring over\n10\n10\nk iterations to reach moderate token prediction accuracy. This indicates that pixel-level V-CoT learning delivers weak gradient signals, as the model must reconstruct high-dimensional visual tokens for each reasoning step. In contrast,\nFantasyVLN\nconverges rapidly within a few thousand iterations, reflecting stable supervision and more efficient learning dynamics. This improvement stems from our CompV-CoT design, where visual reasoning operates in a compact latent space encoded by the pretrained VAR compressor. By replacing dense pixel reconstruction with compressed latent prediction, the model learns richer multimodal reasoning cues under a substantially lighter optimization burden.\nOverall, these results highlight that CompV-CoT not only enhances reasoning efficiency but also yields more stable and interpretable learning behavior, contributing to the superior navigation accuracy of\nFantasyVLN\nin long-horizon tasks.\nFigure 5:\nComparison of training efficiency between\nFantasyVLN\nand WorldVLA.\nExplicit vs. Implicit Reasoning.\nFantasyVLN\nsupports both explicit and implicit reasoning, enabling us to systematically compare their effectiveness across different CoT modalities. As summarized in Table\n6\n, we evaluate the T-CoT, V-CoT, and MM-CoT models under two inference modes. For each case, the model is jointly trained with its corresponding CoT mode and the direct prediction pathway. During inference, using the CoT branch corresponds to explicit reasoning, while employing the direct pathway corresponds to implicit reasoning.\nOverall, implicit reasoning consistently yields higher navigation accuracy, particularly under multimodal settings. In MM-CoT, implicit inference achieves the best performance with\n2.44\n2.44\nSR and\n11.01\n11.01\nISR, surpassing the explicit counterpart by a large margin. This result aligns with the observations in Aux-Think\n(Wang\net al.\n,\n2025\n)\n, suggesting that explicit CoT decoding may amplify cumulative reasoning errors across long trajectories. We attribute this phenomenon to two key factors: (i) the limited training data of LH-VLN (only\n18\n18\nk trajectory slices of five steps each) makes explicit CoT sequences prone to overfitting and error propagation; (ii) explicit reasoning expands temporal dependencies, causing misaligned textual or visual CoT tokens to accumulate deviations over time. In contrast, implicit reasoning benefits from cross-mode alignment during training, allowing the model to internalize reasoning cues while maintaining stable and efficient inference.\nTable 6:\nComparison of explicit and implicit CoT reasoning across modalities.\nMetrics\nMode\nT-CoT\nV-CoT\nMM-CoT\nSR\nexplicit\n0.98\n0.49\n0.98\nimplicit\n0.49\n1.46\n2.44\nISR\nexplicit\n8.26\n7.34\n8.62\nimplicit\n6.06\n11.19\n11.01\n5\nConclusion\nWe introduced\nFantasyVLN\n, a unified implicit reasoning framework that preserves the benefits of Chain-of-Thought supervision while avoiding the token explosion inherent to explicit textual or multimodal CoTs. By compressing imagined visual observations into a compact latent space via a pretrained VAR model and jointly training across textual, visual, and multimodal CoT modes under a unified multi-CoT strategy, the framework learns modality-invariant reasoning representations without requiring explicit CoT generation at inference. As a result, the agent performs direct instruction-to-action mapping while retaining reasoning-aware behavior. Experiments on the challenging LH-VLN benchmark show that this formulation substantially improves navigation accuracy and efficiency, while reducing inference latency by an order of magnitude compared to explicit CoT baselines. These findings demonstrate that implicit multimodal reasoning provides a practical pathway toward real-time embodied navigation, and highlight the potential of compact latent reasoning signals for closing the gap between semantic intent and spatial decision-making in complex environments.\nReferences\nP. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. SÃ¼nderhauf, I. Reid, S. Gould, and A. Van Den Hengel (2018)\nVision-and-language navigation: interpreting visually-grounded navigation instructions in real environments\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 3674â€“3683\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nQ. Chen, L. Qin, J. Liu, Y. Liao, J. Wang, J. Zhou, and W. Che (2025)\nRBF++: quantifying and optimizing reasoning boundaries across measurable and unmeasurable capabilities for chain-of-thought reasoning\n.\narXiv preprint arXiv:2505.13307\n.\nCited by:\nÂ§1\n.\nA. Cheng, Y. Ji, Z. Yang, Z. Gongye, X. Zou, J. Kautz, E. BÄ±yÄ±k, H. Yin, S. Liu, and X. Wang (2024)\nNavila: legged robot vision-language-action model for navigation\n.\narXiv preprint arXiv:2412.04453\n.\nCited by:\nÂ§2.1\n.\nZ. Cheng, Q. Chen, X. Xu, J. Wang, W. Wang, H. Fei, Y. Wang, A. J. Wang, Z. Chen, W. Che,\net al.\n(2025)\nVisual thoughts: a unified perspective of understanding multimodal chain-of-thought\n.\narXiv preprint arXiv:2505.15510\n.\nCited by:\nÂ§2.2\n.\nO. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, and Y. Taigman (2022)\nMake-a-scene: scene-based text-to-image generation with human priors\n.\nIn\nEuropean conference on computer vision\n,\npp.Â 89â€“106\n.\nCited by:\nÂ§3.3\n.\nC. Gao, L. Jin, X. Peng, J. Zhang, Y. Deng, A. Li, H. Wang, and S. Liu (2025)\nOctoNav: towards generalist embodied navigation\n.\narXiv preprint arXiv:2506.09839\n.\nCited by:\nÂ§1\n.\nJ. Gu, E. Stefani, Q. Wu, J. Thomason, and X. E. Wang (2022)\nVision-and-language navigation: a survey of tasks, methods, and future directions\n.\narXiv preprint arXiv:2203.12667\n.\nCited by:\nÂ§1\n.\nY. Huang, M. Wu, R. Li, and Z. Tu (2025)\nVISTA: generative visual imagination for vision-and-language navigation\n.\narXiv preprint arXiv:2505.07868\n.\nCited by:\nÂ§1\n.\nJ. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee (2020)\nBeyond the nav-graph: vision-and-language navigation in continuous environments\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 104â€“120\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nA. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge (2020)\nRoom-across-room: multilingual vision-and-language navigation with dense spatiotemporal grounding\n.\narXiv preprint arXiv:2010.07954\n.\nCited by:\nÂ§2.1\n.\nB. Lin, Y. Nie, K. Loun Zai, Z. Wei, M. Han, R. Xu, M. Niu, J. Han, L. Lin, C. Lu,\net al.\n(2025a)\nEvolvenav: self-improving embodied reasoning for llm-based vision-language navigation\n.\narXiv e-prints\n,\npp.Â arXivâ€“2506\n.\nCited by:\nÂ§1\n.\nB. Lin, Y. Nie, Z. Wei, J. Chen, S. Ma, J. Han, H. Xu, X. Chang, and X. Liang (2025b)\nNavcot: boosting llm-based vision-and-language navigation via learning disentangled reasoning\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n.\nCited by:\nÂ§1\n.\nK. Nguyen, D. Dey, C. Brockett, and B. Dolan (2019)\nVision-based navigation with language-based assistance via imitation learning with indirect intervention\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 12527â€“12537\n.\nCited by:\nÂ§2.1\n.\nX. Song, W. Chen, Y. Liu, W. Chen, G. Li, and L. Lin (2025)\nTowards long-horizon vision-language navigation: platform, benchmark and method\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 12078â€“12088\n.\nCited by:\nÂ§1\n,\nÂ§4.1\n,\nÂ§4.1\n.\nK. Tian, Y. Jiang, Z. Yuan, B. Peng, and L. Wang (2024)\nVisual autoregressive modeling: scalable image generation via next-scale prediction\n.\nAdvances in neural information processing systems\n37\n,\npp.Â 84839â€“84865\n.\nCited by:\nÂ§3.1\n.\nH. Wang, W. Liang, L. V. Gool, and W. Wang (2022)\nTowards versatile embodied navigation\n.\nAdvances in neural information processing systems\n35\n,\npp.Â 36858â€“36874\n.\nCited by:\nÂ§2.1\n.\nS. Wang, Y. Wang, W. Li, X. Cai, Y. Wang, M. Chen, K. Wang, Z. Su, D. Li, and Z. Fan (2025)\nAux-think: exploring reasoning strategies for data-efficient vision-language navigation\n.\nAdvances in Neural Information Processing Systems\n.\nCited by:\nÂ§1\n,\nÂ§3.6\n,\nÂ§4.1\n,\nÂ§4.4\n.\nX. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y. Wang, W. Y. Wang, and L. Zhang (2020)\nVision-language navigation policy learning and adaptation\n.\nIEEE transactions on pattern analysis and machine intelligence\n43\n(\n12\n),\npp.Â 4205â€“4216\n.\nCited by:\nÂ§2.1\n.\nX. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou (2023)\nSelf-consistency improves chain of thought reasoning in language models\n.\nIn\nInternational Conference on Learning Representations\n,\nCited by:\nÂ§2.2\n.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou,\net al.\n(2022)\nChain-of-thought prompting elicits reasoning in large language models\n.\nAdvances in neural information processing systems\n35\n,\npp.Â 24824â€“24837\n.\nCited by:\nÂ§2.2\n.\nQ. Wu, X. Gong, K. Xu, D. Manocha, J. Dong, and J. Wang (2020)\nTowards target-driven visual navigation in indoor scenes via generative imitation learning\n.\nIEEE Robotics and Automation Letters\n6\n(\n1\n),\npp.Â 175â€“182\n.\nCited by:\nÂ§2.1\n.\nW. Wu, T. Chang, X. Li, Q. Yin, and Y. Hu (2024)\nVision-language navigation: a survey and taxonomy\n.\nNeural Computing and Applications\n36\n(\n7\n),\npp.Â 3291â€“3316\n.\nCited by:\nÂ§1\n.\nZ. Xu, B. Liu, X. Xiao, A. Nair, and P. Stone (2023)\nBenchmarking reinforcement learning techniques for autonomous navigation\n.\nIn\nICRA\n,\nCited by:\nÂ§2.1\n.\nX. Xue, J. Hu, M. Luo, X. Shichao, J. Chen, Z. Xie, Q. Kuichen, G. Wei, M. Xu, and Z. Chu (2025)\nOmniNav: a unified framework for prospective exploration and visual-language navigation\n.\narXiv preprint arXiv:2509.25687\n.\nCited by:\nÂ§1\n.\nK. Zeng, Z. Zhang, K. Ehsani, R. Hendrix, J. Salvador, A. Herrasti, R. Girshick, A. Kembhavi, and L. Weihs (2025)\nPoliFormer: scaling on-policy rl with transformers results in masterful navigators\n.\nIn\nConference on Robot Learning\n,\npp.Â 408â€“432\n.\nCited by:\nÂ§2.1\n.\nJ. Zhang, K. Wang, S. Wang, M. Li, H. Liu, S. Wei, Z. Wang, Z. Zhang, and H. Wang (2025a)\nUni-navid: a video-based vision-language-action model for unifying embodied navigation tasks\n.\nRobotics: Science and Systems\n.\nCited by:\nÂ§2.1\n.\nJ. Zhang, K. Wang, R. Xu, G. Zhou, Y. Hong, X. Fang, Q. Wu, Z. Zhang, and H. Wang (2024a)\nNaVid: video-based vlm plans the next step for vision-and-language navigation\n.\nRobotics: Science and Systems\n.\nCited by:\nÂ§2.1\n.\nW. Zhang, H. Liu, Z. Qi, Y. Wang, X. Yu, J. Zhang, R. Dong, J. He, H. Wang, Z. Zhang,\net al.\n(2025b)\nDreamVLA: a vision-language-action model dreamed with comprehensive world knowledge\n.\nIn\nAdvances in Neural Information Processing Systems\n,\nCited by:\nÂ§2.2\n.\nZ. Zhang, A. Zhang, M. Li, G. Karypis, A. Smola,\net al.\n(2024b)\nMultimodal chain-of-thought reasoning in language models\n.\nTransactions on Machine Learning Research\n.\nCited by:\nÂ§2.2\n.\nZ. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola (2024c)\nMultimodal chain-of-thought reasoning in language models\n.\nTransactions on Machine Learning Research\n2024\n.\nCited by:\nÂ§2.2\n.\nQ. Zhao, Y. Lu, M. J. Kim, Z. Fu, Z. Zhang, Y. Wu, Z. Li, Q. Ma, S. Han, C. Finn,\net al.\n(2025)\nCot-vla: visual chain-of-thought reasoning for vision-language-action models\n.\nIn\nProceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference\n,\npp.Â 1702â€“1713\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nB. Zheng, N. Ma, S. Tong, and S. Xie (2025)\nDiffusion transformers with representation autoencoders\n.\narXiv preprint arXiv:2510.11690\n.\nCited by:\nÂ§3.3\n.\nD. Zhou, N. SchÃ¤rli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. V. Le,\net al.\n(2023)\nLeast-to-most prompting enables complex reasoning in large language models\n.\nIn\nInternational Conference on Learning Representations\n,\nCited by:\nÂ§2.2\n.\nG. Zhou, Y. Hong, Z. Wang, X. E. Wang, and Q. Wu (2024)\nNavgpt-2: unleashing navigational reasoning capability for large vision-language models\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 260â€“278\n.\nCited by:\nÂ§1\n.\nAppendix\nAppendix A\nData Preparation\nA.1\nPreprocessing\nAn expert trajectory in LH-VLN comprises a temporal sequence of visualâ€“action pairs\n{\no\nt\n,\na\nt\n}\n1\n:\nT\n\\{o_{t},a_{t}\\}_{1:T}\ntogether with the natural-language task instruction\nâ„\n\\mathcal{I}\n.\nSince VLN is an online sequential decision-making problem with continual interaction with the environment, the agent\nÏ€\nÎ¸\n\\pi_{\\theta}\nmust act in real time based on its current and historical visual observations\n{\no\nâ‰¤\nt\n}\n\\{o_{\\leq t}\\}\n.\nTo construct training samples, we partition each navigation trajectory\nğ’¥\ni\n\\mathcal{J}_{i}\ninto non-overlapping slices.\nEach slice contains the task instruction\nâ„\n\\mathcal{I}\n, visual observations\n{\no\nâ‰¤\nt\n}\n\\{o_{\\leq t}\\}\n, and the future\nk\nk\nactions\nğ’œ\nt\n\\mathcal{A}_{t}\n:\n{\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\nğ’œ\nt\n}\nt\nâˆˆ\nS\ni\nâˆ¼\nSlice\nâ€‹\n(\nğ’¥\ni\n)\n,\ni\n=\n1\n,\nâ€¦\n,\nN\n,\n\\{\\mathcal{I},\\{o_{\\leq t}\\},\\mathcal{A}_{t}\\}_{t\\in S_{i}}\\sim\\text{Slice}(\\mathcal{J}_{i}),\\quad i=1,\\ldots,N,\n(11)\nwhere\nğ’œ\nt\n=\n{\na\nt\n,\na\nt\n+\n1\n,\nâ€¦\n,\na\nt\n+\nk\nâˆ’\n1\n}\n\\mathcal{A}_{t}=\\{a_{t},a_{t+1},\\ldots,a_{t+k-1}\\}\n,\nS\ni\n=\n{\n1\n,\n1\n+\nk\n,\nâ€¦\n,\nT\ni\n}\nS_{i}=\\{1,1+k,\\ldots,T_{i}\\}\n,\nT\ni\nT_{i}\nis the number of actions in\nğ’¥\ni\n\\mathcal{J}_{i}\n, and\nN\nN\nis the number of navigation trajectories in LH-VLN.\nIn practice, we set\nk\n=\n5\nk=5\n.\nA.2\nNavigation Prompt\nWe use the following prompts to enable instruction-driven navigation behaviors across different reasoning modes.\nFor single-stage task, we set the prompt as:\nYou are an autonomous navigation robot. You will get a task with historical and current pictures you see. Based on this information, you need to decide your next 5 actions, which could involve\n<|left|>\n,\n<|right|>\n,\n<|forward|>\n. If you finish your mission, output\n<|stop|>\n. Your current observations are left side:\n<image>\n, front side:\n<image>\n, right side:\n<image>\n. Your historical pictures are:\n<image>\nâ€¦\n<image>\n. Your mission is: [instruction].\nIn multi-stage navigation tasks, the agent must stop upon completing each subtask and maintain awareness of how many subtasks have been finished. To this end, we further extend the prompt with the following description:\nPS: The mission is complex. You may infer several subtasks within the mission, and output\n<|stop|>\nwhen a sub-task is achieved. So far, you have output\n<|stop|>\n0 times. Historical information reflects progress up to the current subgoal.\nA.3\nT-CoT Data Annotation\nWe employ\nQwen-VL-Max\nto generate T-CoT annotations for each navigation slice (see Eq.Â (\n11\n)). All 18,554 navigation slices from the LH-VLN training set are annotated. The annotation prompt is as follows:\nYou are a professional AI data annotator. Your task is to label the intermediate CoT reasoning for VLN trajectory slices. Input includes: user instruction,\nâ‰¤\n\\leq\n20 historical images (front view), 3 current images (left, front, and right view), and the GT action sequence.\nAnnotation steps:\n1.\nSemantic Planning\n: Break the mission into precise sub-tasks with clear spatial goals (e.g., â€œreach the cabinet table in the living room,â€ â€œexit the living room,â€ â€œenter the office,â€ â€œapproach the office tableâ€). Sub-tasks should reflect stepwise navigation milestones rather than abstract summaries.\n2.\nVisual Description\n: Based on the semantic plan, describe what the historical and current images reveal about completed and upcoming sub-tasks. Be concise, factual, and avoid vague wording.\n3.\nAction Decision-Making\n: Predict the next action sequence (5 steps) aligned with the semantic plan and supported by the visual description. Provide a brief justification for the predicted steps. Use natural, instruction-like phrasing instead of raw action codes.\n4.\nVisual Imagination\n: Describe the expected scene after executing the predicted actions, focusing on landmarks or key objects that should become visible or reachable.\n# Output format:\n<think>\nSemantic Plan:\nâ‹¯\n\\cdots\nVisual Description:\nâ‹¯\n\\cdots\nAction Planning:\nâ‹¯\n\\cdots\nVisual Imagination:\nâ‹¯\n\\cdots\n</think>\nNote: Replace â€œ\nâ‹¯\n\\cdots\nâ€ with actual CoT content. Do not output quotes or ellipses.\n# User Instruction: [Instruction]\n# GT Action:\n<|forward|>\nâ‹¯\n\\cdots\n<|right|>\nA.4\nData Augmentation\nTo improve the robustness of the instruction-following model under diverse visual histories, we augment each training example by perturbing only the historical image sequence while keeping the final three observation images unchanged. Given an original sample with N historical frames\n{\nh\n1\n,\nâ€¦\n,\nh\nN\n}\n\\{h_{1},\\dots,h_{N}\\}\nand the last three observation frames\n{\no\nâ„“\n,\no\nf\n,\no\nr\n}\n\\{o_{\\ell},o_{f},o_{r}\\}\n, we generate up to two additional augmented variants per sample. The augmentation operations are stochastic and applied with independent Bernoulli trials.\nUniform Subsampling.\nFor trajectories with at least ten historical images, we apply uniform subsampling with probability 0.5. Specifically, we replace the original history\n{\nh\ni\n}\ni\n=\n1\nN\n\\{h_{i}\\}_{i=1}^{N}\nwith a stride-2 subsequence\n{\nh\n1\n,\nh\n3\n,\nh\n5\n,\nâ€¦\n}\n\\{h_{1},h_{3},h_{5},\\dots\\}\nwhile preserving the three observation images. This operation reduces temporal redundancy and encourages the model to rely on coarser but more informative state transitions.\nStochastic History Trimming.\nFor trajectories with at least seven historical frames, we further apply stochastic trimming. Two perturbations may occur:\n(i) with probability 0.5, we remove the first two historical frames, yielding\n{\nh\n3\n,\nh\n4\n,\nâ€¦\n,\nh\nN\n}\n\\{h_{3},h_{4},\\dots,h_{N}\\}\n;\n(ii) with probability 0.5, and only when the remaining length is at least seven, we randomly select an index\nk\nk\nand remove two consecutive frames\n{\nh\nk\n,\nh\nk\n+\n1\n}\n\\{h_{k},h_{k+1}\\}\n.\nAt least one of the above operations must be triggered for the augmented sample to be retained.\nThis procedure introduces temporal uncertainty, forcing the model to rely on stable, task-relevant cues rather than positional biases.\nAppendix B\nDetail Formulations.\nHere, we provide the formal formulation of the proposed\nFantasyVLN\n, which unifies Non-CoT, T-CoT, V-CoT, and MM-CoT reasoning modes within a single reasoning framework. We introduce binary gating signals\ng\nğ’¯\ng_{\\mathcal{T}}\nand\ng\nğ’±\ng_{\\mathcal{V}}\nto enable flexibly switching among the four reasoning modes.\nNon-CoT Reasoning.\nGiven the task instruction\nâ„\n\\mathcal{I}\nand visual observations\n{\no\nâ‰¤\nt\n}\n\\{o_{\\leq t}\\}\n, the non-CoT reasoning mode aims to directly predict actions\nğ’œ\n^\nt\n\\widehat{\\mathcal{A}}_{t}\nbased on the instruction\nâ„\n\\mathcal{I}\nand observations\n{\no\nâ‰¤\nt\n}\n\\{o_{\\leq t}\\}\n:\nğ’œ\n^\nt\nâˆ¼\nÏ€\nÎ¸\n(\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\ng\nğ’¯\n=\n0\n,\ng\nğ’±\n=\n0\n)\n,\n\\widehat{\\mathcal{A}}_{t}\\sim\\pi_{\\theta}\\big(\\mathcal{I},\\{o_{\\leq t}\\},g_{\\mathcal{T}}=0,g_{\\mathcal{V}}=0\\big),\n(12)\nwhere\nÏ€\nÎ¸\n\\pi_{\\theta}\nis the navigation agent. We employ a pretrained VLM as the navigation agent\nÏ€\nÎ¸\n\\pi_{\\theta}\nand transfer its multimodal prior to the navigation task through supervised fine-tuning (SFT):\narg\nâ¡\nmin\nÎ¸\nâ¡\nâ„’\nCE\nâ€‹\n(\nğ’œ\n^\nt\n,\nğ’œ\nt\n)\n,\n\\arg\\min_{\\theta}\\,\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{A}}_{t},\\mathcal{A}_{t}\\big),\n(13)\nwhere\nâ„’\nCE\n\\mathcal{L}_{\\text{CE}}\ndenotes the causal cross-entropy loss and\nğ’œ\nt\n\\mathcal{A}_{t}\nrepresents the ground truth actions.\nT-CoT reasoning.\nTextual CoT reasoning generates intermediate reasoning steps\nğ’¯\n^\nt\n\\widehat{\\mathcal{T}}_{t}\nbefore action prediction:\n[\nğ’¯\n^\nt\n,\nğ’œ\n^\nt\n]\nâˆ¼\nÏ€\nÎ¸\n(\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\ng\nğ’¯\n=\n1\n,\ng\nğ’±\n=\n0\n)\n.\n[\\widehat{\\mathcal{T}}_{t},\\widehat{\\mathcal{A}}_{t}]\\sim\\pi_{\\theta}\\big(\\mathcal{I},\\{o_{\\leq t}\\},g_{\\mathcal{T}}=1,g_{\\mathcal{V}}=0\\big).\n(14)\nWe define the intermediate reasoning steps\nğ’¯\n^\nt\n\\widehat{\\mathcal{T}}_{t}\nas a structured chain of thought that guides the navigation process. The agent\nÏ€\nÎ¸\n\\pi_{\\theta}\nfirst decomposes the instruction\nâ„\n\\mathcal{I}\ninto a sequence of subgoals, and then infers the current goal from the visual observations\n{\no\nâ‰¤\nt\n}\n\\{o_{\\leq t}\\}\n.\nFinally,\nÏ€\nÎ¸\n\\pi_{\\theta}\nidentifies the decision evidence from the current visual observation\no\nt\no_{t}\n.\nThe training objective is formulated as:\narg\nâ¡\nmin\nÎ¸\nâ¡\nâ„’\nCE\nâ€‹\n(\nğ’¯\n^\nt\n,\nğ’¯\nt\n)\n+\nâ„’\nCE\nâ€‹\n(\nğ’œ\n^\nt\n,\nğ’œ\nt\n)\n,\n\\arg\\min_{\\theta}\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{T}}_{t},\\mathcal{T}_{t}\\big)+\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{A}}_{t},\\mathcal{A}_{t}\\big),\n(15)\nwhere\nğ’¯\nt\n\\mathcal{T}_{t}\nis the ground truth of textual reasoning steps.\nCompV-CoT reasoning.\nVisual CoT reasoning enhances spatial understanding by generating future visual observations\nğ’±\n^\nt\n\\widehat{\\mathcal{V}}_{t}\n, which serve as the conditions for action prediction.\nWe propose CompV-CoT that conducts V-CoT in the latent space of a VAR model. Instead of producing pixel-level images, the agent\nÏ€\nÎ¸\n\\pi_{\\theta}\npredicts low-scale latent representations of VAR\nâ„‹\n^\nt\n\\widehat{\\mathcal{H}}_{t}\n:\n[\nâ„‹\n^\nt\n,\nğ’œ\n^\nt\n]\nâˆ¼\nÏ€\nÎ¸\n(\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\ng\nğ’¯\n=\n0\n,\ng\nğ’±\n=\n1\n)\n.\n[\\widehat{\\mathcal{H}}_{t},\\widehat{\\mathcal{A}}_{t}]\\sim\\pi_{\\theta}\\big(\\mathcal{I},\\{o_{\\leq t}\\},g_{\\mathcal{T}}=0,g_{\\mathcal{V}}=1\\big).\n(16)\nThe predicted representations are then passed through the VAR model to reconstruct pixel observations:\nğ’±\n^\nt\nâˆ¼\ng\nâ€‹\n(\nâ„‹\n^\nt\n)\n,\n\\widehat{\\mathcal{V}}_{t}\\sim g\\big(\\widehat{\\mathcal{H}}_{t}\\big),\n(17)\nwhere\ng\ng\ndenotes the generation pipeline based on next-scale prediction for VAR.\nDuring training, the VAR is frozen.\nThe training process is defined as:\narg\nâ¡\nmin\nÎ¸\nâ¡\nâ„’\nCE\nâ€‹\n(\nğ’±\n^\nt\n,\nğ’±\nt\n)\n+\nâ„’\nCE\nâ€‹\n(\nğ’œ\n^\nt\n,\nğ’œ\nt\n)\n,\n\\arg\\min_{\\theta}\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{V}}_{t},\\mathcal{V}_{t}\\big)+\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{A}}_{t},\\mathcal{A}_{t}\\big),\n(18)\nwhere\nğ’±\nt\n\\mathcal{V}_{t}\nis the ground truth future visual observations.\nMM-CoT reasoning\nWe employ paired textual-visual CoT reasoning steps as the villain Multimodal CoT reasoning steps\nâ„³\n^\nt\n\\widehat{\\mathcal{M}}_{t}\n:\nâ„³\n^\nt\n=\n[\nğ’¯\n^\nt\n,\nâ„‹\n^\nt\n]\n.\n\\widehat{\\mathcal{M}}_{t}=[\\widehat{\\mathcal{T}}_{t},\\widehat{\\mathcal{H}}_{t}].\n(19)\nMM-CoT reasoning first generates multimodal reasoning steps\nâ„³\n^\nt\n\\widehat{\\mathcal{M}}_{t}\nand then predicts future actions\nğ’œ\n^\nt\n\\widehat{\\mathcal{A}}_{t}\n:\n[\nâ„³\n^\nt\n,\nğ’œ\n^\nt\n]\nâˆ¼\nÏ€\nÎ¸\n(\nâ„\n,\n{\no\nâ‰¤\nt\n}\n,\ng\nğ’¯\n=\n1\n,\ng\nğ’±\n=\n1\n)\n.\n[\\widehat{\\mathcal{M}}_{t},\\widehat{\\mathcal{A}}_{t}]\\sim\\pi_{\\theta}\\big(\\mathcal{I},\\{o_{\\leq t}\\},g_{\\mathcal{T}}=1,g_{\\mathcal{V}}=1\\big).\n(20)\nThe training objective is formulated as:\narg\nâ¡\nmin\nÎ¸\nâ¡\nâ„’\nCE\nâ€‹\n(\nâ„³\n^\nt\n,\nâ„³\nt\n)\n+\nâ„’\nCE\nâ€‹\n(\nğ’œ\n^\nt\n,\nğ’œ\nt\n)\n.\n\\arg\\min_{\\theta}\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{M}}_{t},\\mathcal{M}_{t}\\big)+\\mathcal{L}_{\\text{CE}}\\big(\\widehat{\\mathcal{A}}_{t},\\mathcal{A}_{t}\\big).\n(21)\nUM-CoT reasoning.\nFantasyVLN\nis a unified multimodal CoT reasoning framework that integrates the non-CoT, T-CoT, V-CoT, and MM-CoT reasoning modes. The formulation of UM-CoT is provided in Section\n3.4\n.\nAppendix C\nImplementation Details\nTraining Details.\nWe adopt\nQwen2.5-VL\nas the base model and apply LoRA-based parameter-efficient tuning to both the language layers and the visionâ€“language projection modules. Training is conducted on 64 H20 GPUs, each equipped with 141â€‰GB of memory. We use the AdamW optimizer with a learning rate of\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\n, a weight decay of\n0.1\n0.1\n, and a cosine schedule with a\n5\n%\n5\\%\nwarmup ratio. The per-device batch size is set to 4, supported by 32 dataloader workers. We employ\nbfloat16\nprecision, enable gradient checkpointing, and adopt DeepSpeed ZeRO-2 for memory-efficient optimization.\nEvaluation.\nWe perform online evaluation for all methods. Given an initial position, the agent interacts with the simulator to execute multi-stage navigation tasks. At each step, the agent receives visual observations and predicts subsequent actions; the environment then applies the predicted action and updates the agentâ€™s state. When the agent outputs\n<|stop|>\n, the environment verifies whether the current subtask is completed (i.e., the distance to the target location is within 1â€‰m) and then proceeds to the next subtask. If the agent exhausts its action budget before completing the subtask, the subtask is marked as failed. This procedure continues until all subtasks are completed or terminated.\nSpecial Tokens.\nLeveraging the vocabulary extensibility of autoregressive models, we implement the required functionalities through systematic vocabulary expansion.\nSpecifically, we introduce (i) action tokens\n<|forward|>\n,\n<|left|>\n,\n<|right|>\n,\n<|stop|>\nfor navigation action prediction; (ii) VAR latent tokens\n<|1|>\nâ€“\n<|4096|>\nfor CompV-CoT and MM-CoT latent-space visual reasoning; (iii) system tokens such as\n<|NAV|>\n,\n<think>\n,\n</think>\nto regulate narrative formatting; and (iv) gating tokens\n<textual_think>\n,\n<no_textual_think>\n,\n<visual_think>\n,\n<no_visual_think>\nthat serve as the binary signals\ng\nğ’¯\ng_{\\mathcal{T}}\nand\ng\nğ’±\ng_{\\mathcal{V}}\nfor unified multimodal CoT control.",
    "preview_text": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.\n\nFantasyVLN\n: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation\nJing Zuo\n1,2\n*\nÂ§\n\\S\n,â€ƒLingzhou Mu\n1,3\n*\nÂ§\n\\S\n,â€ƒFan Jiang\n1\n*\nâ€ \n\\dagger\nâ€¡\n\\ddagger\n,â€ƒChengcheng Ma\n1\n,\nMu Xu\n1\n,\nYonggang Qi\n2\nâ€¡\n\\ddagger\n1\nFantasy AIGC Team,\n2\nBeijing University of Posts and Telecommunications,\n3\nTsinghua University\njiangfan0576@gmail.com\nqiyg@bupt.edu.cn\nAbstract\nAchieving human-level performan",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "VLA",
        "VLM",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºFantasyVLNæ¡†æ¶ï¼Œé€šè¿‡éšå¼æ¨ç†å®ç°è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†ï¼Œæå‡å®æ—¶æ€§å’ŒæˆåŠŸç‡ã€‚",
    "detailed_summary": "## FantasyVLN è®ºæ–‡æ‘˜è¦\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nè§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰è¦æ±‚æ™ºèƒ½ä½“æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨å¤æ‚è§†è§‰ç¯å¢ƒä¸­å¯¼èˆªï¼Œè¿™éœ€è¦ç»“åˆå¤šæ¨¡æ€ç†è§£å’Œé•¿åºåˆ—æ¨ç†ã€‚**æ€ç»´é“¾ï¼ˆCoTï¼‰** æ¨ç†å·²è¢«è¯æ˜èƒ½æå‡å¯¼èˆªçš„å¯è§£é‡Šæ€§å’Œé•¿ç¨‹è§„åˆ’èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼š**çº¯æ–‡æœ¬CoT**ç¼ºä¹ç©ºé—´æ„ŸçŸ¥ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆï¼›è€Œ**å¤šæ¨¡æ€CoT**ï¼ˆå¦‚ç”Ÿæˆæƒ³è±¡çš„è§†è§‰è§‚æµ‹ï¼‰ä¼šå¯¼è‡´**ä¸¥é‡çš„ä»¤ç‰Œè†¨èƒ€**ï¼ˆä¸€ä¸ªæ¨ç†æ­¥éª¤å¯äº§ç”Ÿ3000-5000ä¸ªä»¤ç‰Œï¼‰ï¼Œä½¿å¾—å®æ—¶å¯¼èˆªå˜å¾—ä¸åˆ‡å®é™…ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§æ—¢èƒ½ä¿ç•™CoTæ¨ç†ä¼˜åŠ¿ï¼Œåˆèƒ½åœ¨æ¨ç†æ—¶é¿å…æ˜¾å¼ä»¤ç‰Œå¼€é”€çš„é«˜æ•ˆæ–¹æ³•ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡ºäº† **FantasyVLN**ï¼Œä¸€ä¸ªç»Ÿä¸€çš„**éšå¼æ¨ç†æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š\n- **ç´§å‡‘è§†è§‰CoTï¼ˆCompV-CoTï¼‰**ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„**è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ˆVARï¼‰**ï¼Œå°†æƒ³è±¡çš„è§†è§‰è§‚æµ‹ç¼–ç åˆ°ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œæå¤§å‡å°‘äº†åºåˆ—é•¿åº¦å’Œè®¡ç®—å¼€é”€ã€‚\n- **ç»Ÿä¸€å¤šæ¨¡æ€CoTï¼ˆUM-CoTï¼‰è®­ç»ƒç­–ç•¥**ï¼šåœ¨å•ä¸€æ¨¡å‹ä¸­ï¼Œé€šè¿‡**é—¨æ§æœºåˆ¶**è”åˆå­¦ä¹ **æ–‡æœ¬CoTã€è§†è§‰CoTå’Œå¤šæ¨¡æ€CoT**ä¸‰ç§æ¨ç†æ¨¡å¼ï¼Œå¹¶ä¸**éCoTï¼ˆç›´æ¥é¢„æµ‹ï¼‰æ¨¡å¼**è¿›è¡Œ**è·¨æ¨¡æ€å¯¹é½çº¦æŸ**ï¼Œä½¿æ¨¡å‹å­¦ä¹ åˆ°ä¸æ¨¡æ€æ— å…³çš„æ¨ç†è¡¨å¾ã€‚\n- **â€œè®­ç»ƒç”¨CoTï¼Œæ¨ç†ä¸ç”¨CoTâ€èŒƒå¼**ï¼šåœ¨è®­ç»ƒæ—¶åˆ©ç”¨ä¸°å¯Œçš„å¤šæ¨¡æ€CoTè¿›è¡Œç›‘ç£ï¼Œåœ¨æ¨ç†æ—¶ä»…ä½¿ç”¨é«˜æ•ˆçš„éCoTæ¨¡å¼è¿›è¡Œç›´æ¥æŒ‡ä»¤åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼Œä»è€Œå®ç°äº†**å…¼å…·æ¨ç†æ„è¯†ä¸å®æ—¶æ€§**çš„å¯¼èˆªã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„**é•¿è§†é‡VLNï¼ˆLH-VLNï¼‰** åŸºå‡†æµ‹è¯•ä¸Šï¼ŒFantasyVLNå–å¾—äº†æ˜¾è‘—ä¼˜åŠ¿ï¼š\n- **å¯¼èˆªç²¾åº¦å…¨é¢é¢†å…ˆ**ï¼šæˆåŠŸç‡ï¼ˆSRï¼‰è¾¾åˆ°2.44%ï¼Œç‹¬ç«‹å­ä»»åŠ¡æˆåŠŸç‡ï¼ˆISRï¼‰è¾¾åˆ°11.01%ï¼Œæ˜¾è‘—ä¼˜äºçº¯æ–‡æœ¬CoTæ–¹æ³•ï¼ˆå¦‚Aux-Thinkï¼‰å’Œæ˜¾å¼è§†è§‰CoTæ–¹æ³•ï¼ˆå¦‚CoT-VLAï¼‰ã€‚\n- **æ¨ç†æ•ˆç‡å¤§å¹…æå‡**ï¼šæ¨ç†æ—¶**æ¯ç§’å¯æ‰§è¡Œçº¦1.03ä¸ªåŠ¨ä½œï¼ˆAPSï¼‰**ï¼Œæ¯”æ˜¾å¼CoTæ–¹æ³•ï¼ˆå¦‚CoT-VLAçš„0.19 APSï¼‰**å¿«ä¸€ä¸ªæ•°é‡çº§**ï¼Œæ»¡è¶³äº†å®æ—¶å¯¼èˆªéœ€æ±‚ã€‚\n- **æ¶ˆèå®éªŒéªŒè¯æœ‰æ•ˆæ€§**ï¼šå®éªŒè¯æ˜äº†**è·¨æ¨¡æ€å¯¹é½çº¦æŸ**ã€**å¤šæ¨¡å¼è”åˆè®­ç»ƒ**ä»¥åŠ**VARæ½œåœ¨ç©ºé—´å‹ç¼©**å„è‡ªå¯¹æ€§èƒ½æå‡çš„å…³é”®ä½œç”¨ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\nFantasyVLNä¸ºå…·èº«æ™ºèƒ½å¯¼èˆªæä¾›äº†ä¸€æ¡**é«˜æ•ˆå®ç”¨çš„æ–°è·¯å¾„**ã€‚å…¶ä»·å€¼åœ¨äºï¼š\n- **æŠ€æœ¯è´¡çŒ®**ï¼šé¦–æ¬¡æå‡ºäº†ç»Ÿä¸€æ–‡æœ¬ã€è§†è§‰åŠå¤šæ¨¡æ€CoTçš„éšå¼æ¨ç†æ¡†æ¶ï¼Œå¹¶åˆ›æ–°æ€§åœ°åˆ©ç”¨VARè¿›è¡Œè§†è§‰ä¿¡æ¯å‹ç¼©ï¼Œè§£å†³äº†å¤šæ¨¡æ€æ¨ç†ä¸­çš„ä»¤ç‰Œçˆ†ç‚¸éš¾é¢˜ã€‚\n- **å®é™…åº”ç”¨**ï¼šå®ç°äº†**ç²¾åº¦ä¸æ•ˆç‡çš„å¹³è¡¡**ï¼Œä½¿å¾—åœ¨ä¿æŒé«˜æ°´å¹³å¯¼èˆªæˆåŠŸç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿè¿›è¡Œå®æ—¶å†³ç­–ï¼Œæ¨åŠ¨äº†VLNæŠ€æœ¯å‘å®é™…æœºå™¨äººåº”ç”¨çš„è¿ˆè¿›ã€‚\n- **å¯å‘æ„ä¹‰**ï¼šå…¶â€œ**å†…éƒ¨åŒ–æ¨ç†**â€çš„æ€æƒ³ä¸ºå…¶ä»–éœ€è¦å¤æ‚å¤šæ¨¡æ€æ¨ç†çš„å…·èº«AIä»»åŠ¡æä¾›äº†å¯å€Ÿé‰´çš„èŒƒå¼ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šFantasyVLN\n\n### **ä¸€ã€ è®ºæ–‡æ—¨åœ¨è§£å†³çš„æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡é’ˆå¯¹**è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰** ä»»åŠ¡ä¸­ï¼Œç°æœ‰**æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•**å­˜åœ¨çš„ä¸¤å¤§å…³é”®ç¼ºé™·ï¼š\n1.  **æ¨¡æ€å‰²è£‚ä¸æ³›åŒ–æ€§å·®**ï¼š\n    - **çº¯æ–‡æœ¬CoT**ï¼ˆå¦‚NavCoTï¼‰ï¼šä»…é€šè¿‡æ–‡æœ¬æè¿°è¿›è¡Œæ¨ç†ï¼Œç¼ºä¹å¯¹è§†è§‰ç©ºé—´çš„ç›´æ¥å»ºæ¨¡ï¼Œå¯¼è‡´**ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›å¼±**ï¼Œä¸”å®¹æ˜“è¿‡æ‹Ÿåˆåˆ°æœ‰é™çš„æ ‡æ³¨æ•°æ®ä¸Šã€‚\n    - **å•æ¨¡æ€CoT**ï¼šæ— æ³•æœ‰æ•ˆèåˆè¯­ä¹‰ï¼ˆè¯­è¨€ï¼‰å’Œå‡ ä½•ï¼ˆè§†è§‰ï¼‰ä¿¡æ¯ï¼Œåœ¨é•¿è§†é‡ã€å¤šé˜¶æ®µå¯¼èˆªä»»åŠ¡ä¸­æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚\n2.  **æ¨ç†æ•ˆç‡ä½ä¸‹ï¼Œæ— æ³•å®æ—¶å¯¼èˆª**ï¼š\n    - **å¤šæ¨¡æ€CoT**ï¼ˆå¦‚CoT-VLAï¼‰ï¼šéœ€è¦æ˜¾å¼ç”Ÿæˆâ€œæƒ³è±¡ä¸­â€çš„ä¸­é—´è§†è§‰è§‚æµ‹ï¼ˆå›¾åƒï¼‰ï¼Œå¯¼è‡´**åºåˆ—é•¿åº¦çˆ†ç‚¸å¼å¢é•¿**ï¼ˆä¸€ä¸ªæ¨ç†æ­¥å¯èƒ½äº§ç”Ÿ3000-5000ä¸ªtokenï¼‰ã€‚è¿™ä½¿å¾—è®­ç»ƒå’Œæ¨ç†å»¶è¿Ÿæé«˜ï¼Œ**æ— æ³•æ»¡è¶³VLNå¯¹å®æ—¶æ€§çš„è¦æ±‚**ã€‚\n\n**æ€»ç»“**ï¼šç°æœ‰æ–¹æ³•åœ¨**æ•ˆæœ**ï¼ˆå¤šæ¨¡æ€èåˆã€æ³›åŒ–æ€§ï¼‰å’Œ**æ•ˆç‡**ï¼ˆå®æ—¶æ€§ï¼‰ä¹‹é—´éš¾ä»¥å…¼å¾—ã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº† **FantasyVLN**ï¼Œä¸€ä¸ª**ç»Ÿä¸€çš„éšå¼å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†æ¡†æ¶**ï¼Œå…¶åˆ›æ–°æ€§ä½“ç°åœ¨ä»¥ä¸‹å››ä¸ªå±‚é¢ï¼š\n\n1.  **ç»Ÿä¸€çš„éšå¼æ¨ç†èŒƒå¼**ï¼š\n    - **æ ¸å¿ƒç†å¿µ**ï¼š**â€œè®­ç»ƒç”¨CoTï¼Œæ¨ç†ä¸ç”¨CoTâ€**ã€‚åœ¨è®­ç»ƒæ—¶åˆ©ç”¨å¤šæ¨¡æ€CoTè¿›è¡Œç›‘ç£ï¼Œè®©æ¨¡å‹å­¦ä¹ æ¨ç†èƒ½åŠ›ï¼›åœ¨æ¨ç†æ—¶ï¼Œæ¨¡å‹ç›´æ¥è¿›è¡Œâ€œæŒ‡ä»¤->åŠ¨ä½œâ€çš„æ˜ å°„ï¼Œ**å®Œå…¨é¿å…ç”Ÿæˆå†—é•¿çš„CoTåºåˆ—**ã€‚\n    - **ç»“æœ**ï¼šå®ç°äº†**å…·å¤‡æ¨ç†æ„è¯†çš„é«˜æ•ˆå®æ—¶å¯¼èˆª**ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå°†æ¨ç†å»¶è¿Ÿé™ä½äº†ä¸€ä¸ªæ•°é‡çº§ã€‚\n\n2.  **ç´§å‡‘çš„è§†è§‰æ€ç»´é“¾ï¼ˆCompV-CoTï¼‰**ï¼š\n    - **æŠ€æœ¯åˆ›æ–°**ï¼šä¸ºè§£å†³è§†è§‰CoTçš„tokenè†¨èƒ€é—®é¢˜ï¼Œå¼•å…¥é¢„è®­ç»ƒçš„**è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ˆVARï¼‰**ã€‚æ¨¡å‹ä¸å†ç”Ÿæˆåƒç´ å›¾åƒï¼Œè€Œæ˜¯é¢„æµ‹VAR**æ½œç©ºé—´ä¸­çš„ç´§å‡‘è§†è§‰token**ï¼ˆä¾‹å¦‚ï¼Œä¸€å¼ 256x256çš„å›¾åƒä»…ç”¨30ä¸ªtokenè¡¨ç¤ºï¼‰ã€‚\n    - **ä»·å€¼**ï¼šæå¤§å‹ç¼©äº†è§†è§‰æ¨ç†çš„åºåˆ—é•¿åº¦ï¼Œæ˜¾è‘—æå‡äº†**è®­ç»ƒå’Œæ¨ç†æ•ˆç‡**ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸°å¯Œçš„è§†è§‰è¯­ä¹‰ä¿¡æ¯ã€‚\n\n3.  **ç»Ÿä¸€çš„å¤šæ¨¡æ€CoTè®­ç»ƒç­–ç•¥**ï¼š\n    - **æ–¹æ³•åˆ›æ–°**ï¼šè®¾è®¡äº†ä¸€ä¸ªæ”¯æŒ**å››ç§æ¨ç†æ¨¡å¼**çš„å•ä¸€æ¨¡å‹æ¶æ„ï¼š\n        - **éCoTæ¨¡å¼**ï¼šç›´æ¥é¢„æµ‹åŠ¨ä½œï¼ˆç”¨äºæ¨ç†ï¼‰ã€‚\n        - **æ–‡æœ¬CoTæ¨¡å¼**ï¼šç”Ÿæˆæ–‡æœ¬æ¨ç†é“¾ã€‚\n        - **è§†è§‰CoTæ¨¡å¼**ï¼šç”ŸæˆVARæ½œç©ºé—´ä¸­çš„è§†è§‰æ¨ç†é“¾ã€‚\n        - **å¤šæ¨¡æ€CoTæ¨¡å¼**ï¼šåŒæ—¶ç”Ÿæˆæ–‡æœ¬å’Œè§†è§‰æ¨ç†é“¾ã€‚\n    - **å®ç°æœºåˆ¶**ï¼šé€šè¿‡**é—¨æ§ä¿¡å·**ï¼ˆ`g_T`, `g_V`ï¼‰çµæ´»åˆ‡æ¢ä¸åŒæ¨¡å¼ï¼Œæ‰€æœ‰æ¨¡å¼å…±äº«åŒä¸€å¥—æ¨¡å‹å‚æ•°ã€‚\n\n4.  **è·¨æ¨¡å¼å¯¹é½çº¦æŸ**ï¼š\n    - **è®­ç»ƒåˆ›æ–°**ï¼šä¸ºç¡®ä¿ä¸åŒæ¨ç†æ¨¡å¼å­¦ä¹ åˆ°ä¸€è‡´çš„ç­–ç•¥ï¼Œæå‡ºäº†ä¸€ä¸ªå¯¹é½æŸå¤±ã€‚**ä»¥éCoTæ¨¡å¼é¢„æµ‹çš„åŠ¨ä½œä½œä¸ºâ€œè½¯ç›®æ ‡â€**ï¼Œå¼ºåˆ¶æ‰€æœ‰CoTæ¨¡å¼é¢„æµ‹çš„åŠ¨ä½œå‘å…¶å¯¹é½ã€‚\n    - **ä»·å€¼**ï¼šä¿ƒè¿›äº†**æ¨¡æ€ä¸å˜æ¨ç†è¡¨å¾**çš„å­¦ä¹ ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨ä¸åŒæ¨ç†è·¯å¾„ä¸‹å†³ç­–çš„ä¸€è‡´æ€§ï¼Œæ˜¯éšå¼æ¨ç†æˆåŠŸçš„å…³é”®ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆçš„æ€»ä½“æ¡†æ¶**\n1.  **è®­ç»ƒé˜¶æ®µ**ï¼š\n    - å°†å¯¼èˆªè½¨è¿¹åˆ‡ç‰‡ï¼Œå¹¶åˆ©ç”¨å¤§æ¨¡å‹ï¼ˆQwen-VL-Maxï¼‰è‡ªåŠ¨ç”Ÿæˆ**æ–‡æœ¬CoTæ ‡æ³¨**ã€‚\n    - æ„å»ºåŒ…å«æŒ‡ä»¤ã€å†å²è§‚æµ‹ã€æ–‡æœ¬CoTã€è§†è§‰CoTï¼ˆVARæ½œç ï¼‰ã€çœŸå®åŠ¨ä½œçš„**äº”å…ƒç»„è®­ç»ƒæ•°æ®**ã€‚\n    - åœ¨è®­ç»ƒä¸­**éšæœºé‡‡æ ·é—¨æ§ä¿¡å·**ï¼Œè®©æ¨¡å‹äº¤æ›¿å­¦ä¹ å››ç§æ¨ç†æ¨¡å¼ã€‚\n    - é€šè¿‡**è”åˆæŸå¤±å‡½æ•°**ï¼ˆåŒ…å«CoTç”ŸæˆæŸå¤±å’Œè·¨æ¨¡å¼å¯¹é½æŸå¤±ï¼‰ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å…¶å†…éƒ¨åŒ–å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚\n\n2.  **æ¨ç†é˜¶æ®µ**ï¼š\n    - å°†é—¨æ§ä¿¡å·è®¾ç½®ä¸º`(0,0)`ï¼Œå³**å¯ç”¨éCoTæ¨¡å¼**ã€‚\n    - æ¨¡å‹æ¥æ”¶å½“å‰æŒ‡ä»¤å’Œè§†è§‰è§‚æµ‹ï¼Œ**ç›´æ¥è¾“å‡ºåŠ¨ä½œåºåˆ—**ï¼Œè¿‡ç¨‹é«˜æ•ˆã€‚\n    - ç”±äºè®­ç»ƒæ—¶çš„å¯¹é½çº¦æŸï¼Œæ­¤æ—¶æ¨¡å‹åšå‡ºçš„å†³ç­–å·²**å†…åŒ–äº†ä»å¤šæ¨¡æ€CoTä¸­å­¦åˆ°çš„æ¨ç†é€»è¾‘**ã€‚\n\n### **å››ã€ å®é™…ä»·å€¼ä¸å®éªŒéªŒè¯**\n- **æ€§èƒ½æå‡**ï¼šåœ¨æå…·æŒ‘æˆ˜çš„é•¿è§†é‡VLNåŸºå‡†ï¼ˆ**LH-VLN**ï¼‰ä¸Šï¼ŒFantasyVLNåœ¨**æˆåŠŸç‡ï¼ˆSRï¼‰ã€ç‹¬ç«‹æˆåŠŸç‡ï¼ˆISRï¼‰ç­‰æ‰€æœ‰æŒ‡æ ‡ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æœ€ä½³æ–¹æ³•**ï¼ˆåŒ…æ‹¬çº¯æ–‡æœ¬CoTçš„Aux-Thinkå’Œæ˜¾å¼å¤šæ¨¡æ€CoTæ–¹æ³•ï¼‰ã€‚\n- **æ•ˆç‡é£è·ƒ**ï¼šæ¨ç†é€Ÿåº¦ï¼ˆAPSï¼‰è¾¾åˆ°**çº¦1åŠ¨ä½œ/ç§’**ï¼Œä¸éšå¼æ–¹æ³•ç›¸å½“ï¼Œä½†æ¯”æ˜¾å¼CoTæ–¹æ³•ï¼ˆå¦‚CoT-VLAçš„0.19åŠ¨ä½œ/ç§’ï¼‰**å¿«5å€ä»¥ä¸Š**ï¼Œå®ç°äº†å®æ—¶å¯¼èˆªã€‚\n- **æ³›åŒ–ä¸é²æ£’æ€§**ï¼šç»Ÿä¸€çš„æ¡†æ¶å’Œè·¨æ¨¡å¼å¯¹é½æœºåˆ¶ä½¿æ¨¡å‹èƒ½æ›´å¥½åœ°èåˆè¯­ä¹‰ä¸ç©ºé—´ä¿¡æ¯ï¼Œåœ¨**å¤šé˜¶æ®µã€é•¿è½¨è¿¹ã€æœªè§è¿‡çš„ç¯å¢ƒ**ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„è§„åˆ’å’Œæ³›åŒ–èƒ½åŠ›ã€‚\n\n**ç»“è®º**ï¼šFantasyVLNé€šè¿‡**å°†å¤šæ¨¡æ€CoTæ¨ç†å‹ç¼©å¹¶å†…åŒ–åˆ°ä¸€ä¸ªé«˜æ•ˆçš„å•æ­¥å†³ç­–æ¨¡å‹ä¸­**ï¼Œå·§å¦™åœ°è§£å†³äº†VLNä¸­â€œå¤æ‚æ¨ç†â€ä¸â€œå®æ—¶æ‰§è¡Œâ€ä¹‹é—´çš„çŸ›ç›¾ï¼Œä¸ºè¿ˆå‘å®ç”¨åŒ–çš„å…·èº«æ™ºèƒ½å¯¼èˆªæä¾›äº†ä¸€æ¡å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚",
            "timestamp": "2026-01-27T15:54:44.745825",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡å‘å±•è„‰ç»œæ¢³ç†ï¼šä»å•æ¨¡æ€åˆ°ç»Ÿä¸€éšå¼å¤šæ¨¡æ€æ¨ç†çš„VLNæ¼”è¿›\n\næœ¬æ–‡ï¼ˆFantasyVLNï¼‰æ˜¯è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯ç»“åˆæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ–¹å‘ä¸Šçš„æœ€æ–°è¿›å±•ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äº**é¦–æ¬¡ç»Ÿä¸€äº†æ–‡æœ¬ã€è§†è§‰å’Œå¤šæ¨¡æ€CoTæ¨ç†æ¨¡å¼ï¼Œå¹¶é€šè¿‡éšå¼æ¨ç†æœºåˆ¶è§£å†³äº†æ˜¾å¼CoTå¸¦æ¥çš„ä»¤ç‰Œçˆ†ç‚¸å’Œå®æ—¶æ€§éš¾é¢˜**ã€‚ä»¥ä¸‹å°†æ¢³ç†è¯¥æ–¹å‘çš„å‘å±•è„‰ç»œï¼Œå¹¶ç€é‡åˆ†æå‡ ä¸ªå…³é”®çš„å‘å±•æ–¹å‘ã€‚\n\n### å‘å±•è„‰ç»œæ€»è§ˆ\n\nè¯¥é¢†åŸŸçš„å‘å±•å¯ä»¥æ¦‚æ‹¬ä¸ºä»**æ¨¡å—åŒ–ã€å•æ¨¡æ€å­¦ä¹ **ï¼Œæ¼”è¿›åˆ°**ç«¯åˆ°ç«¯ã€å¤šæ¨¡æ€å¤§æ¨¡å‹é©±åŠ¨**ï¼Œå†è¿›ä¸€æ­¥æ¢ç´¢**æ˜¾å¼æ¨ç†ï¼ˆCoTï¼‰ä»¥æå‡å¯è§£é‡Šæ€§å’Œé•¿ç¨‹è§„åˆ’èƒ½åŠ›**ï¼Œæœ€ç»ˆé¢ä¸´**å¤šæ¨¡æ€æ¨ç†çš„æ•ˆç‡ç“¶é¢ˆ**ï¼Œå¹¶ç”±æœ¬æ–‡æå‡º**ç»Ÿä¸€éšå¼æ¨ç†**çš„è§£å†³æ–¹æ¡ˆã€‚\n\n---\n\n### ç¬¬ä¸€é˜¶æ®µï¼šä¼ ç»ŸVLNæ–¹æ³•ï¼ˆæ¨¡å—åŒ–ä¸å•æ¨¡æ€å­¦ä¹ ï¼‰\n\n*   **ä»£è¡¨å·¥ä½œ**ï¼šæ—©æœŸåŸºäºR2Rã€RxRç­‰ç¦»æ•£ç¯å¢ƒæ•°æ®é›†çš„æ–¹æ³•ï¼Œå¦‚ä½¿ç”¨æ¨¡ä»¿å­¦ä¹ æˆ–å¼ºåŒ–å­¦ä¹ ï¼Œå¹¶è¾…ä»¥è¿›åº¦ç›‘æ§ç­‰è¾…åŠ©ä»»åŠ¡ã€‚\n*   **æ ¸å¿ƒæ€è·¯**ï¼šå°†æ„ŸçŸ¥ã€æŒ‡ä»¤ç†è§£å’ŒåŠ¨ä½œè§„åˆ’æ‹†åˆ†ä¸ºç¦»æ•£æ¨¡å—ã€‚\n*   **ä¸»è¦æ”¹è¿›ä¸å±€é™**ï¼š\n    *   **æ”¹è¿›**ï¼šå»ºç«‹äº†VLNçš„åŸºæœ¬ä»»åŠ¡æ¡†æ¶å’Œè¯„ä¼°åŸºå‡†ã€‚\n    * **å±€é™**ï¼š\n        1.  **è¯­ä¹‰å¯¹é½å·®**ï¼šåœ¨è¿ç»­æˆ–æœªè§ç¯å¢ƒä¸­æ³›åŒ–èƒ½åŠ›å¼±ã€‚\n        2.  **ç¼ºä¹æ·±åº¦æ¨ç†**ï¼šéš¾ä»¥å¤„ç†éœ€è¦å¤šé˜¶æ®µã€é•¿è§†é‡è§„åˆ’çš„å¤æ‚ä»»åŠ¡ã€‚\n        3.  **éç«¯åˆ°ç«¯**ï¼šæ¨¡å—é—´çš„è¯¯å·®ä¼šç´¯ç§¯ã€‚\n\n### ç¬¬äºŒé˜¶æ®µï¼šç«¯åˆ°ç«¯ä¸å¤§æ¨¡å‹é©±åŠ¨\n\n*   **ä»£è¡¨å·¥ä½œ**ï¼šPoliformer, NaVid, Uni-NaVid, NaVILAã€‚\n*   **æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œç«¯åˆ°ç«¯çš„å¯¼èˆªç­–ç•¥å­¦ä¹ ï¼Œç»Ÿä¸€æ„ŸçŸ¥ä¸å†³ç­–ã€‚\n*   **ä¸»è¦æ”¹è¿›**ï¼š\n    1.  **æ›´å¼ºçš„è¯­ä¹‰-ç©ºé—´å¯¹é½**ï¼šå¾—ç›Šäºå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œæ¨¡å‹å¯¹æŒ‡ä»¤å’Œè§†è§‰åœºæ™¯çš„ç†è§£æ›´æ·±åˆ»ã€‚\n    2.  **æ›´å¥½çš„æ³›åŒ–æ€§**ï¼šåœ¨æœªè§ç¯å¢ƒå’Œè·¨å…·èº«ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚\n    3.  **ç»Ÿä¸€æ¡†æ¶**ï¼šå¼€å§‹å°†å¤šç§å…·èº«ä»»åŠ¡ï¼ˆå¦‚å¯¼èˆªã€æ“ä½œï¼‰æ•´åˆåˆ°å•ä¸€æ¨¡å‹ä¸­ã€‚\n*   **é—ç•™é—®é¢˜**ï¼šè™½ç„¶å¤„ç†çŸ­æœŸä»»åŠ¡èƒ½åŠ›å¢å¼ºï¼Œä½†å¯¹**é•¿è§†é‡ã€å¤šé˜¶æ®µä»»åŠ¡æ‰€éœ€çš„å¤æ‚æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ä»ç„¶ä¸è¶³**ã€‚å†³ç­–è¿‡ç¨‹åƒä¸€ä¸ªâ€œé»‘ç®±â€ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ã€‚\n\n### ç¬¬ä¸‰é˜¶æ®µï¼šå¼•å…¥æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼ˆæ˜¾å¼ã€å•æ¨¡æ€ï¼‰\n\nå—åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰CoTæˆåŠŸçš„å¯å‘ï¼Œç ”ç©¶è€…å¼€å§‹å°†æ˜¾å¼æ¨ç†æ­¥éª¤å¼•å…¥VLNï¼Œæ—¨åœ¨æå‡å¯è§£é‡Šæ€§å’Œé•¿ç¨‹è§„åˆ’ã€‚\n\n#### å‘å±•æ–¹å‘ä¸€ï¼š**æ–‡æœ¬CoT (T-CoT)**\n*   **ä»£è¡¨å·¥ä½œ**ï¼šNavCoT, NavGPT-2, Aux-Thinkã€‚\n*   **æ ¸å¿ƒæ€è·¯**ï¼šæ¨¡å‹åœ¨è¾“å‡ºåŠ¨ä½œå‰ï¼Œå…ˆç”Ÿæˆä¸€ç³»åˆ—æ–‡æœ¬ä¸­é—´æ¨ç†æ­¥éª¤ï¼ˆå¦‚åˆ†è§£å­ç›®æ ‡ã€è¯„ä¼°è¿›åº¦ã€è¯†åˆ«å¯è¡ŒåŠ¨çº¿ç´¢ï¼‰ã€‚\n*   **ä¸»è¦æ”¹è¿›**ï¼š\n    1.  **å¯è§£é‡Šæ€§**ï¼šæä¾›äº†å†³ç­–ä¾æ®ï¼Œä½¿æ¨¡å‹è¡Œä¸ºæ›´é€æ˜ã€‚\n    2.  **è§„åˆ’èƒ½åŠ›**ï¼šé€šè¿‡åˆ†æ­¥æ¨ç†ï¼Œç†è®ºä¸Šèƒ½æ›´å¥½åœ°å¤„ç†å¤æ‚æŒ‡ä»¤ã€‚\n    3.  **è®­ç»ƒæ•°æ®é«˜æ•ˆ**ï¼ˆAux-Thinkï¼‰ï¼šæå‡ºâ€œ**è®­ç»ƒç”¨CoTï¼Œæ¨ç†ä¸ç”¨CoT**â€çš„èŒƒå¼ï¼Œå°†æ¨ç†æ¨¡å¼å†…åŒ–ï¼Œé¿å…æ¨ç†æ—¶ç”ŸæˆCoTå¸¦æ¥çš„å»¶è¿Ÿã€‚\n*   **å±€é™**ï¼š\n    1.  **ç¼ºä¹ç©ºé—´åŸºç¡€**ï¼šçº¯æ–‡æœ¬æ¨ç†æ— æ³•å……åˆ†åˆ©ç”¨è§†è§‰è§‚å¯Ÿä¸­çš„å‡ ä½•å’Œç©ºé—´ä¿¡æ¯ã€‚\n    2.  **æ˜“è¿‡æ‹Ÿåˆ**ï¼šå¯¹æœ‰é™çš„ã€äººå·¥æ ‡æ³¨çš„æ¨ç†æ­¥éª¤æ•°æ®å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œæ³›åŒ–æ€§å—é™ã€‚\n    3.  **æ¨¡æ€å‰²è£‚**ï¼šæœªèƒ½å®ç°è¯­ä¹‰è§„åˆ’ä¸ç©ºé—´ç†è§£çš„**è”åˆå»ºæ¨¡**ã€‚\n\n#### å‘å±•æ–¹å‘äºŒï¼š**è§†è§‰CoT (V-CoT)**\n*   **ä»£è¡¨å·¥ä½œ**ï¼šCoT-VLA, DreamVLAã€‚\n*   **æ ¸å¿ƒæ€è·¯**ï¼šæ¨¡å‹åœ¨å†³ç­–å‰ï¼Œå…ˆâ€œæƒ³è±¡â€ç”Ÿæˆæœªæ¥å¯èƒ½çš„è§†è§‰è§‚å¯Ÿï¼ˆå¸§ï¼‰ï¼ŒåŸºäºè¿™äº›æƒ³è±¡çš„ç”»é¢æ¥é¢„æµ‹åŠ¨ä½œã€‚\n*   **ä¸»è¦æ”¹è¿›**ï¼š\n    1.  **ç©ºé—´æ¨ç†å¼ºåŒ–**ï¼šç›´æ¥å¯¹è§†è§‰æœªæ¥è¿›è¡Œå»ºæ¨¡ï¼Œå¢å¼ºäº†ç©ºé—´ç†è§£å’Œå‰ç»æ€§è§„åˆ’èƒ½åŠ›ã€‚\n    2.  **æ›´æ¥è¿‘äººç±»**ï¼šæ¨¡ä»¿äº†äººç±»åœ¨è„‘æµ·ä¸­æ¨¡æ‹Ÿè¡ŒåŠ¨åæœçš„æ¨ç†æ–¹å¼ã€‚\n*   **å±€é™**ï¼š\n    1.  **ä»¤ç‰Œçˆ†ç‚¸**ï¼šç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒå¯¹åº”çš„åƒç´ ä»¤ç‰Œæ•°é‡å·¨å¤§ï¼ˆä¸€æ¬¡æ¨ç†å¯è¾¾3k-5kä»¤ç‰Œï¼‰ï¼Œå¯¼è‡´**è®­ç»ƒå’Œæ¨ç†å»¶è¿Ÿæé«˜ï¼Œæ— æ³•å®æ—¶å¯¼èˆª**ã€‚\n    2.  **è®­ç»ƒä¸ç¨³å®š**ï¼šåƒç´ çº§é‡å»ºä»»åŠ¡å›°éš¾ï¼Œæ¢¯åº¦ä¿¡å·å¼±ï¼Œæ”¶æ•›æ…¢ã€‚\n\n#### å‘å±•æ–¹å‘ä¸‰ï¼š**å¤šæ¨¡æ€CoT (MM-CoT)**\n*   **ä»£è¡¨å·¥ä½œ**ï¼šOctoNav-R1, CoT-VLAï¼ˆä¹Ÿæ¶‰åŠå¤šæ¨¡æ€ï¼‰ï¼Œä»¥åŠæœ¬æ–‡ä¹‹å‰çš„ä¸€äº›åˆæ­¥æ¢ç´¢ã€‚\n*   **æ ¸å¿ƒæ€è·¯**ï¼šåŒæ—¶ç”Ÿæˆé…å¯¹çš„æ–‡æœ¬å’Œè§†è§‰æ¨ç†æ­¥éª¤ï¼Œä»¥æœŸç»“åˆè¯­ä¹‰å’Œç©ºé—´æ¨ç†çš„ä¼˜åŠ¿ã€‚\n*   **ä¸»è¦æ”¹è¿›**ï¼šç†è®ºä¸Šèƒ½å®ç°æœ€å…¨é¢çš„è”åˆæ¨ç†ã€‚\n*   **å±€é™**ï¼š**ç»§æ‰¿äº†V-CoTçš„æ‰€æœ‰æ•ˆç‡é—®é¢˜ï¼Œä¸”æ›´ä¸ºä¸¥é‡**ã€‚ä»¤ç‰Œå¼€é”€æˆä¸ºä¸å¯æ‰¿å—ä¹‹é‡ï¼Œæ˜¯é˜»ç¢å…¶å®é™…åº”ç”¨çš„æœ€å¤§ç“¶é¢ˆã€‚\n\n### ç¬¬å››é˜¶æ®µï¼šç»Ÿä¸€ä¸éšå¼å¤šæ¨¡æ€æ¨ç†ï¼ˆæœ¬æ–‡çš„è´¡çŒ®ï¼‰\n\n*   **ä»£è¡¨å·¥ä½œ**ï¼š**FantasyVLN**ã€‚\n*   **æ ¸å¿ƒæ€è·¯**ï¼š**â€œå–å…¶ç²¾åï¼Œå»å…¶ç³Ÿç²•â€**ã€‚åœ¨è®­ç»ƒæ—¶å¸æ”¶å¤šæ¨¡æ€CoTçš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨æ¨ç†æ—¶ä»…è¿›è¡Œç›´æ¥çš„æŒ‡ä»¤-åŠ¨ä½œæ˜ å°„ï¼Œå®ç°é«˜æ•ˆä¸”å…·å¤‡æ¨ç†æ„è¯†çš„å¯¼èˆªã€‚\n*   **å…³é”®æŠ€æœ¯åˆ›æ–°ä¸æ”¹è¿›**ï¼š\n\n    1.  **ç´§å‡‘è§†è§‰CoT (CompV-CoT)**ï¼š\n        *   **æ”¹è¿›ç‚¹**ï¼šé’ˆå¯¹V-CoTçš„ä»¤ç‰Œçˆ†ç‚¸é—®é¢˜ã€‚\n        *   **æ–¹æ³•**ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ˆVARï¼‰ï¼Œå°†æƒ³è±¡çš„è§†è§‰è§‚å¯Ÿ**å‹ç¼©åˆ°ç´§å‡‘çš„æ½œåœ¨ç©ºé—´**ï¼ˆå¦‚ä»…ç”¨30ä¸ªä»¤ç‰Œè¡¨ç¤ºä¸€å¼ 256x256çš„å›¾åƒï¼‰ï¼Œè€Œéåƒç´ ç©ºé—´ã€‚\n        *   **æ•ˆæœ**ï¼šæå¤§å‡å°‘äº†åºåˆ—é•¿åº¦ï¼Œæå‡äº†è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸°å¯Œçš„è§†è§‰ä¿¡æ¯ã€‚\n\n    2.  **ç»Ÿä¸€å¤šæ¨¡æ€CoT (UM-CoT) æ¡†æ¶**ï¼š\n        *   **æ”¹è¿›ç‚¹**ï¼šé¦–æ¬¡å°†éCoTã€T-CoTã€V-CoTã€MM-CoTå››ç§æ¨¡å¼**ç»Ÿä¸€åœ¨å•ä¸€æ¨¡å‹å’Œæ¡†æ¶ä¸‹**ã€‚\n        *   **æ–¹æ³•**ï¼šå¼•å…¥äºŒå…ƒé—¨æ§ä¿¡å· `(g_T, g_V)`ï¼Œæ§åˆ¶æ˜¯å¦æ¿€æ´»æ–‡æœ¬æˆ–è§†è§‰æ¨ç†ã€‚é€šè¿‡æ•°æ®æ··åˆè¿›è¡Œè”åˆè®­ç»ƒã€‚\n        *   **æ•ˆæœ**ï¼šä¸€ä¸ªæ¨¡å‹å…·å¤‡å¤šç§æ¨ç†èƒ½åŠ›ï¼Œç»“æ„ç®€æ´ï¼Œå‚æ•°å…±äº«ã€‚\n\n    3.  **è·¨æ¨¡å¼å¯¹é½çº¦æŸ**ï¼š\n        *   **æ”¹è¿›ç‚¹**ï¼šè§£å†³å¤šæ¨¡å¼è”åˆè®­ç»ƒå¯èƒ½äº§ç”Ÿçš„å†²çªå’Œæ¨¡å¼ä¸ä¸€è‡´é—®é¢˜ã€‚\n        *   **æ–¹æ³•**ï¼šä½¿ç”¨**éCoTæ¨¡å¼çš„åŠ¨ä½œé¢„æµ‹ä½œä¸ºâ€œè½¯ç›®æ ‡â€**ï¼Œæ¥å¯¹é½æ‰€æœ‰CoTæ¨¡å¼ï¼ˆT, V, MMï¼‰çš„åŠ¨ä½œé¢„æµ‹ã€‚\n        *   **æ•ˆæœ**ï¼šè¿«ä½¿æ¨¡å‹å­¦ä¹ åˆ°**æ¨¡æ€ä¸å˜çš„é€šç”¨çš„æ¨ç†è¡¨å¾**ï¼Œå°†ä¸åŒæ¨¡å¼çš„æ¨ç†çŸ¥è¯†å†…åŒ–åˆ°å…±äº«çš„æ½œåœ¨ç­–ç•¥ä¸­ï¼Œå¢å¼ºäº†æ³›åŒ–æ€§å’Œç¨³å®šæ€§ã€‚\n\n    4.  **éšå¼æ¨ç†èŒƒå¼**ï¼š\n        *   **æ”¹è¿›ç‚¹**ï¼šå½»åº•è§£å†³æ˜¾å¼CoTæ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚\n        *   **æ–¹æ³•**ï¼šæ¨ç†æ—¶ï¼Œå°†é—¨æ§ä¿¡å·è®¾ä¸º `(0,0)`ï¼Œæ¨¡å‹ç›´æ¥è¾“å‡ºåŠ¨ä½œï¼Œ**ä¸ç”Ÿæˆä»»ä½•ä¸­é—´æ¨ç†ä»¤ç‰Œ**ã€‚\n        *   **æ•ˆæœ**ï¼šå®ç°äº†**â€œæ¨ç†æ„ŸçŸ¥çš„å®æ—¶å¯¼èˆªâ€**ã€‚å¦‚è¡¨4æ‰€ç¤ºï¼Œæ¨ç†é€Ÿåº¦ï¼ˆAPSï¼‰æ¯”æ˜¾å¼CoTæ–¹æ³•ï¼ˆå¦‚CoT-VLAï¼‰**æå‡äº†ä¸€ä¸ªæ•°é‡çº§**ï¼Œä¸å…¶å®ƒéšå¼æ–¹æ³•ç›¸å½“ï¼Œä½†å¯¼èˆªç²¾åº¦æ˜¾è‘—æ›´é«˜ã€‚\n\n### æ€»ç»“ï¼šå‘å±•æ–¹å‘ä¸æ¼”è¿›é€»è¾‘\n\n1.  **ä»æ˜¾å¼åˆ°éšå¼**ï¼šè¿™æ˜¯æœ€æ ¸å¿ƒçš„æ¼”è¿›æ–¹å‘ã€‚ä»NavGPT-2ã€CoT-VLAç­‰éœ€è¦ç”Ÿæˆå†—é•¿ä¸­é—´æ­¥éª¤çš„**æ˜¾å¼æ¨ç†**ï¼Œå‘å±•åˆ°Aux-Thinkã€FantasyVLNç­‰å°†æ¨ç†è¿‡ç¨‹å†…åŒ–ã€ä»…è¾“å‡ºæœ€ç»ˆåŠ¨ä½œçš„**éšå¼æ¨ç†**ï¼Œæ ¹æœ¬æ€§åœ°è§£å†³äº†éƒ¨ç½²ä¸­çš„å®æ—¶æ€§ç“¶é¢ˆã€‚\n2.  **ä»å•æ¨¡æ€åˆ°å¤šæ¨¡æ€ç»Ÿä¸€**ï¼šä»çº¯æ–‡æœ¬ï¼ˆNavCoTï¼‰æˆ–çº¯è§†è§‰ï¼ˆCoT-VLAï¼‰çš„**å•æ¨¡æ€CoT**ï¼Œå‘å±•åˆ°æœ¬æ–‡æå‡ºçš„**ç»Ÿä¸€å¤šæ¨¡æ€CoTæ¡†æ¶**ã€‚è¿™ä¸ä»…åœ¨èƒ½åŠ›ä¸Šæ›´å…¨é¢ï¼Œè€Œä¸”é€šè¿‡ç»Ÿä¸€è®­ç»ƒå’Œè·¨æ¨¡å¼å¯¹é½ï¼Œäº§ç”Ÿäº†â€œ1+1>2â€çš„ååŒæ•ˆåº”ã€‚\n3.  **ä»åƒç´ ç©ºé—´åˆ°æ½œåœ¨ç©ºé—´**ï¼šé’ˆå¯¹è§†è§‰æ¨ç†çš„æ•ˆç‡ç—›ç‚¹ï¼Œä»ç›´æ¥åœ¨åƒç´ ç©ºé—´ç”Ÿæˆå›¾åƒï¼ˆä»¤ç‰Œçˆ†ç‚¸ï¼‰ï¼Œæ¼”è¿›åˆ°åœ¨**é«˜æ•ˆå‹ç¼©çš„æ½œåœ¨ç©ºé—´ï¼ˆå¦‚VARï¼‰è¿›è¡Œè§†è§‰æƒ³è±¡**ã€‚è¿™æ˜¯å®ç°é«˜æ•ˆå¤šæ¨¡æ€æ¨ç†çš„å…³é”®æŠ€æœ¯çªç ´ã€‚\n4.  **ä»ç‹¬ç«‹è®­ç»ƒåˆ°å¯¹é½è®­ç»ƒ**ï¼šæ—©æœŸçš„å¤šæ¨¡å¼å°è¯•å¯èƒ½æ˜¯ç‹¬ç«‹æˆ–ç®€å•æ‹¼æ¥çš„ã€‚æœ¬æ–‡é€šè¿‡**è·¨æ¨¡å¼å¯¹é½çº¦æŸ**ï¼Œå®ç°äº†ä¸åŒæ¨ç†æ¨¡å¼åœ¨è¡¨å¾å±‚é¢çš„æ·±åº¦èåˆä¸çŸ¥è¯†è’¸é¦ï¼Œç¡®ä¿äº†éšå¼æ¨ç†æ—¶èƒ½è°ƒç”¨åˆ°æœ€æœ‰æ•ˆçš„è”åˆçŸ¥è¯†ã€‚\n\n**ç»“è®º**ï¼šFantasyVLNæ ‡å¿—ç€VLN-CoTæ–¹å‘è¿›å…¥äº†ä¸€ä¸ªæ–°é˜¶æ®µï¼Œå…¶æ ¸å¿ƒä»·å€¼åœ¨äº**åœ¨ä¸å¤§å¹…",
            "timestamp": "2026-01-27T15:55:58.043055",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## FantasyVLN çš„å‰ä½œåˆ†æä¸å¯¹æ¯”\n\nFantasyVLN çš„ç ”ç©¶å»ºç«‹åœ¨è¿‘å¹´æ¥ä¸€ç³»åˆ—å°†æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å¼•å…¥è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»»åŠ¡çš„å·¥ä½œä¹‹ä¸Šã€‚ä»¥ä¸‹æ˜¯å…¶ä¸»è¦å‰ä½œçš„è¯¦ç»†æ¢³ç†ï¼Œå¹¶ä»¥è¡¨æ ¼å½¢å¼å¯¹æ¯”å…¶æ ¸å¿ƒæ–¹æ³•ã€è´¡çŒ®åŠä¸ FantasyVLN çš„åŒºåˆ«ã€‚\n\n### å‰ä½œæ¦‚è§ˆè¡¨æ ¼\n\n| å‰ä½œåç§° | å‘è¡¨å¹´ä»½/å‡ºå¤„ | æ ¸å¿ƒæ–¹æ³• | ä¸»è¦è´¡çŒ® | ä¸ FantasyVLN çš„ä¸»è¦åŒºåˆ«åŠ FantasyVLN çš„æ”¹è¿› |\n| :--- | :--- | :--- | :--- | :--- |\n| **NavCoT** | Lin et al., 2025b | **çº¯æ–‡æœ¬CoT**ã€‚å°†å¯¼èˆªæŒ‡ä»¤åˆ†è§£ä¸ºå­ç›®æ ‡ï¼Œç”Ÿæˆæ–‡æœ¬ä¸­é—´æ¨ç†æ­¥éª¤ï¼ˆå¦‚è¯­ä¹‰è§„åˆ’ã€è¿›åº¦è¯„ä¼°ï¼‰æ¥æŒ‡å¯¼åŠ¨ä½œé¢„æµ‹ã€‚ | é¦–æ¬¡ç³»ç»Ÿåœ°å°†CoTèŒƒå¼å¼•å…¥VLNï¼Œæå‡äº†é•¿è§†é‡è§„åˆ’çš„**å¯è§£é‡Šæ€§**ã€‚ | **åŒºåˆ«**ï¼š1. **æ¨¡æ€å•ä¸€**ï¼šä»…ä½¿ç”¨æ–‡æœ¬CoTï¼Œç¼ºä¹å¯¹è§†è§‰ç©ºé—´çš„æ˜¾å¼å»ºæ¨¡ï¼Œå­˜åœ¨â€œè¯­ä¹‰-ç©ºé—´é¸¿æ²Ÿâ€ã€‚2. **æ˜¾å¼æ¨ç†**ï¼šæ¨ç†æ­¥éª¤åœ¨æ¨ç†æ—¶éœ€é€è¯ç”Ÿæˆï¼Œå¢åŠ å»¶è¿Ÿã€‚**FantasyVLNæ”¹è¿›**ï¼š1. **ç»Ÿä¸€å¤šæ¨¡æ€**ï¼šæ•´åˆäº†æ–‡æœ¬ã€è§†è§‰åŠå¤šæ¨¡æ€CoTã€‚2. **éšå¼æ¨ç†**ï¼šè®­ç»ƒæ—¶å­¦ä¹ CoTï¼Œæ¨ç†æ—¶ç›´æ¥æ˜ å°„ï¼Œå®ç°å®æ—¶å¯¼èˆªã€‚ |\n| **NavGPT-2** | Zhou et al., 2024 | åˆ©ç”¨**å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰çš„é›¶æ ·æœ¬CoTèƒ½åŠ›**ã€‚å°†è§†è§‰è§‚å¯Ÿè½¬åŒ–ä¸ºæ–‡æœ¬æè¿°ï¼Œè®©LLMè¿›è¡ŒåŸºäºæ–‡æœ¬çš„é€æ­¥æ¨ç†å’ŒåŠ¨ä½œç”Ÿæˆã€‚ | è¯æ˜äº†å¤§å‹åŸºç¡€æ¨¡å‹åœ¨VLNä¸­é›¶æ ·æœ¬æ¨ç†çš„æ½œåŠ›ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šè®­ç»ƒã€‚ | **åŒºåˆ«**ï¼š1. **ä¾èµ–æè¿°**ï¼šä¸¥é‡ä¾èµ–è§†è§‰åˆ°æ–‡æœ¬çš„è½¬æ¢ï¼ˆcaptioningï¼‰ï¼Œä¿¡æ¯æœ‰æŸã€‚2. **éç«¯åˆ°ç«¯**ï¼šé€šå¸¸æ˜¯æ¨¡å—åŒ–ç³»ç»Ÿï¼Œè€Œéç»Ÿä¸€ç­–ç•¥ã€‚3. **æ•ˆç‡ä½ä¸‹**ï¼šè°ƒç”¨å¤§æ¨¡å‹æˆæœ¬é«˜ã€å»¶è¿Ÿå¤§ã€‚**FantasyVLNæ”¹è¿›**ï¼š1. **ç«¯åˆ°ç«¯è®­ç»ƒ**ï¼šç»Ÿä¸€æ¨¡å‹ç›´æ¥å¤„ç†å¤šæ¨¡æ€è¾“å…¥ã€‚2. **éšå¼è¡¨å¾**ï¼šå†…éƒ¨åŒ–æ¨ç†æ¨¡å¼ï¼Œæ‘†è„±å¯¹æ˜¾å¼æ–‡æœ¬ç”Ÿæˆå’Œå¤–éƒ¨å¤§æ¨¡å‹çš„ä¾èµ–ã€‚ |\n| **Aux-Think** | Wang et al., 2025 | **æ–‡æœ¬CoTè¾…åŠ©è®­ç»ƒ**ã€‚åœ¨è®­ç»ƒæ—¶ï¼Œä½¿ç”¨æ–‡æœ¬CoTä½œä¸º**è¾…åŠ©ç›‘ç£ä¿¡å·**ï¼Œè®©æ¨¡å‹å­¦ä¹ æ¨ç†æ¨¡å¼ï¼›æ¨ç†æ—¶åˆ™**ç›´æ¥é¢„æµ‹åŠ¨ä½œ**ï¼ˆTrain-with-CoT, Infer-without-CoTï¼‰ã€‚ | æå‡ºäº†**éšå¼æ¨ç†**çš„å…³é”®æ€æƒ³ï¼Œè§£å†³äº†æ˜¾å¼CoTåœ¨æ¨ç†æ—¶çš„æ•ˆç‡é—®é¢˜ï¼Œå¹³è¡¡äº†æ€§èƒ½ä¸é€Ÿåº¦ã€‚ | **åŒºåˆ«**ï¼š1. **æ¨¡æ€å±€é™**ï¼šä»…ä½¿ç”¨äº†**æ–‡æœ¬CoT**ä½œä¸ºè¾…åŠ©ä¿¡å·ï¼Œæœªèƒ½åˆ©ç”¨è§†è§‰æƒ³è±¡è¿›è¡Œç©ºé—´æ¨ç†ã€‚**FantasyVLNæ”¹è¿›**ï¼š1. **å¤šæ¨¡æ€CoTç»Ÿä¸€**ï¼šå¼•å…¥äº†**è§†è§‰CoTï¼ˆV-CoTï¼‰**å’Œ**å¤šæ¨¡æ€CoTï¼ˆMM-CoTï¼‰**ã€‚2. **ç´§å‡‘è§†è§‰è¡¨å¾**ï¼šç”¨VARæ¨¡å‹å°†æƒ³è±¡çš„è§†è§‰è§‚å¯Ÿå‹ç¼©åˆ°æ½œç©ºé—´ï¼Œæå¤§å‡å°‘äº†tokenæ•°é‡ã€‚3. **è·¨æ¨¡æ€å¯¹é½**ï¼šæ–°å¢çº¦æŸï¼Œä½¿ä¸åŒæ¨ç†æ¨¡å¼è¾“å‡ºçš„åŠ¨ä½œé¢„æµ‹ä¸ç›´æ¥é¢„æµ‹æ¨¡å¼å¯¹é½ï¼Œå­¦ä¹ æ›´ä¸€è‡´çš„æ¨ç†è¡¨å¾ã€‚ |\n| **CoT-VLA** | Zhao et al., 2025 | **è§†è§‰CoTï¼ˆV-CoTï¼‰**ã€‚åœ¨æœºå™¨äººæ“ä½œç­‰ä»»åŠ¡ä¸­ï¼Œè®©æ¨¡å‹åœ¨é¢„æµ‹åŠ¨ä½œå‰ï¼Œå…ˆ**ç”Ÿæˆæœªæ¥çš„è§†è§‰å¸§ï¼ˆè§†é¢‘ï¼‰**ä½œä¸ºæ¨ç†æ­¥éª¤ã€‚ | å°†CoTæ‰©å±•åˆ°è§†è§‰æ¨¡æ€ï¼Œå¼ºè°ƒäº†**å‰ç»æ€§è§†è§‰æƒ³è±¡**å¯¹äºåŠ¨ä½œè§„åˆ’çš„é‡è¦æ€§ã€‚ | **åŒºåˆ«**ï¼š1. **åƒç´ çº§ç”Ÿæˆ**ï¼šåœ¨åƒç´ ç©ºé—´ç”Ÿæˆæœªæ¥å¸§ï¼Œå¯¼è‡´**ä¸¥é‡çš„tokenè†¨èƒ€**ï¼ˆä¸€ä¸ªæ¨ç†é“¾å¯è¾¾3k-5k tokenï¼‰ï¼Œè®­ç»ƒå’Œæ¨ç†æˆæœ¬æé«˜ï¼Œæ— æ³•å®æ—¶ã€‚2. **ä»»åŠ¡å·®å¼‚**ï¼šä¸»è¦é’ˆå¯¹æœºæ¢°è‡‚æ“ä½œï¼Œæœªåœ¨é•¿è§†é‡VLNä¸Šå……åˆ†éªŒè¯ã€‚**FantasyVLNæ”¹è¿›**ï¼š1. **æ½œç©ºé—´è§†è§‰CoT**ï¼šåˆ©ç”¨VARæ¨¡å‹åœ¨**é«˜åº¦å‹ç¼©çš„æ½œç©ºé—´**ï¼ˆå¦‚ä»…30ä¸ªtokenï¼‰è¿›è¡Œè§†è§‰æƒ³è±¡ï¼Œè§£å†³äº†tokençˆ†ç‚¸é—®é¢˜ã€‚2. **ä¸“ä¸ºVLNä¼˜åŒ–**ï¼šæ¡†æ¶é’ˆå¯¹é•¿è§†é‡ã€å¤šé˜¶æ®µå¯¼èˆªçš„æŒ‘æˆ˜è®¾è®¡ã€‚ |\n| **OctoNav-R1 / VISTA** | Gao et al., 2025 / Huang et al., 2025 | **å¤šæ¨¡æ€CoTæ¢ç´¢**ã€‚è¿™äº›å·¥ä½œå°è¯•ç»“åˆæ–‡æœ¬å’Œè§†è§‰è¿›è¡Œæ¨ç†ï¼Œä¾‹å¦‚ç”Ÿæˆé…å¯¹çš„å¤šæ¨¡æ€æ¨ç†è½¨è¿¹ã€‚ | éªŒè¯äº†å¤šæ¨¡æ€CoTæ˜¯æå‡å¯¼èˆªæ³›åŒ–èƒ½åŠ›å’Œç©ºé—´ç†è§£çš„ä¸€æ¡æœ‰å‰æ™¯çš„è·¯å¾„ã€‚ | **åŒºåˆ«**ï¼š1. **æ•ˆç‡æŒ‘æˆ˜**ï¼šæ—©æœŸçš„å¤šæ¨¡æ€CoTé€šå¸¸ä¹Ÿæ˜¯æ˜¾å¼çš„ï¼Œé¢ä¸´ä¸CoT-VLAç±»ä¼¼çš„**tokenè†¨èƒ€å’Œå»¶è¿Ÿé—®é¢˜**ã€‚2. **ç¼ºä¹ç»Ÿä¸€æ¡†æ¶**ï¼šä¸åŒæ¨¡æ€çš„æ¨ç†å¯èƒ½åœ¨ä¸åŒæ¨¡å—æˆ–ä¸åŒè®­ç»ƒé˜¶æ®µè¿›è¡Œã€‚**FantasyVLNæ”¹è¿›**ï¼š1. **ç»Ÿä¸€éšå¼æ¡†æ¶**ï¼šé¦–æ¬¡åœ¨ä¸€ä¸ªæ¨¡å‹å†…**ç»Ÿä¸€äº†æ–‡æœ¬ã€è§†è§‰ã€å¤šæ¨¡æ€åŠæ— CoTå››ç§æ¨ç†æ¨¡å¼**ï¼Œé€šè¿‡é—¨æ§ä¿¡å·çµæ´»åˆ‡æ¢ã€‚2. **é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥**ï¼šé€šè¿‡è·¨æ¨¡æ€å¯¹é½å’Œæ½œç©ºé—´å‹ç¼©ï¼Œå®ç°äº†é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†å­¦ä¹ ã€‚ |\n\n### æ€»ç»“ï¼šFantasyVLN çš„æ ¸å¿ƒåˆ›æ–°ä¸ä¸»è¦æ”¹å–„\n\nFantasyVLN å¹¶éç®€å•è¿­ä»£ï¼Œè€Œæ˜¯é’ˆå¯¹å‰ä½œä½“ç³»çš„æ ¹æœ¬æ€§ç“¶é¢ˆæå‡ºäº†ä¸€ä¸ª**ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆ**ã€‚å…¶å…·ä½“æ”¹å–„ä½“ç°åœ¨ä¸‰ä¸ªå±‚é¢ï¼š\n\n1.  **æ¨¡æ€å±‚é¢ï¼šä»å•ä¸€åˆ°ç»Ÿä¸€èåˆ**\n    *   **å‰ä½œå±€é™**ï¼šNavCoTã€Aux-Thinkä»…ç”¨æ–‡æœ¬ï¼›CoT-VLAä»…ç”¨è§†è§‰ï¼›å…¶ä»–å¤šæ¨¡æ€å·¥ä½œæ•ˆç‡ä½ä¸‹ã€‚\n    *   **FantasyVLNæ”¹å–„**ï¼šæå‡º**ç»Ÿä¸€å¤šæ¨¡æ€CoTï¼ˆUM-CoTï¼‰æ¡†æ¶**ï¼Œç”¨ä¸€ä¸ªæ¨¡å‹ã€ä¸€å¥—å‚æ•°æ”¯æŒå››ç§æ¨ç†æ¨¡å¼ï¼ˆæ— CoTã€æ–‡æœ¬CoTã€è§†è§‰CoTã€å¤šæ¨¡æ€CoTï¼‰ã€‚è¿™æ˜¯é¦–ä¸ªå¦‚æ­¤å…¨é¢æ•´åˆçš„å·¥ä½œã€‚\n\n2.  **æ•ˆç‡å±‚é¢ï¼šä»æ˜¾å¼è†¨èƒ€åˆ°éšå¼ç´§å‡‘**\n    *   **å‰ä½œå±€é™**ï¼šæ˜¾å¼ç”ŸæˆCoTæ­¥éª¤ï¼ˆå°¤å…¶æ˜¯è§†è§‰å¸§ï¼‰å¯¼è‡´**tokenæ•°é‡æ¿€å¢**ï¼Œæ¨ç†å»¶è¿Ÿå¢åŠ ä¸€ä¸ªæ•°é‡çº§ï¼Œæ— æ³•å®æ—¶ã€‚\n    *   **FantasyVLNæ”¹å–„**ï¼š\n        *   **æŠ€æœ¯åˆ›æ–°**ï¼šå¼•å…¥**ç´§å‡‘è§†è§‰CoTï¼ˆCompV-CoTï¼‰**ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰æ¨¡å‹ï¼Œå°†æƒ³è±¡çš„è§†è§‰è§‚å¯Ÿç¼–ç åˆ°æä½ç»´çš„æ½œç©ºé—´ï¼ˆå¦‚1/2185å‹ç¼©æ¯”ï¼‰ï¼Œå½»åº•è§£å†³tokenè†¨èƒ€ã€‚\n        *   **èŒƒå¼åˆ›æ–°**ï¼šåšå®šé‡‡ç”¨ **â€œè®­ç»ƒç”¨CoTï¼Œæ¨ç†ä¸ç”¨CoTâ€** çš„éšå¼æ¨ç†èŒƒå¼ã€‚æ¨¡å‹åœ¨è®­ç»ƒæ—¶é€šè¿‡å¤šæ¨¡å¼CoTå’Œ**è·¨æ¨¡æ€å¯¹é½çº¦æŸ**å­¦ä¹ åˆ°ä¸°å¯Œçš„æ¨ç†è¡¨å¾ï¼Œåœ¨æ¨ç†æ—¶ç›´æ¥è¿›è¡ŒæŒ‡ä»¤åˆ°åŠ¨ä½œçš„é«˜æ•ˆæ˜ å°„ã€‚\n\n3.  **æ€§èƒ½ä¸æ³›åŒ–å±‚é¢ï¼šä»è¿‡æ‹Ÿåˆåˆ°å¯¹é½ä¸€è‡´**\n    *   **å‰ä½œå±€é™**ï¼šçº¯æ–‡æœ¬CoTç¼ºä¹ç©ºé—´ groundingï¼›æ˜¾å¼CoTå®¹æ˜“è¿‡æ‹Ÿåˆåˆ°æœ‰é™çš„æ ‡æ³¨æ•°æ®ï¼›ä¸åŒæ¨ç†æ¨¡å¼å¯èƒ½äº§ç”Ÿå†²çªã€‚\n    *   **FantasyVLNæ”¹å–„**ï¼š\n        *   **å¼•å…¥è·¨æ¨¡æ€å¯¹é½çº¦æŸ**ï¼šå¼ºåˆ¶è¦æ±‚æ–‡æœ¬CoTã€è§†è§‰CoTã€å¤šæ¨¡æ€CoTåˆ†æ”¯é¢„æµ‹çš„åŠ¨ä½œï¼Œå¿…é¡»ä¸ç›´æ¥ï¼ˆæ— CoTï¼‰é¢„æµ‹åˆ†æ”¯çš„åŠ¨ä½œåˆ†å¸ƒå¯¹é½ã€‚è¿™ç¡®ä¿äº†ä¸åŒæ¨ç†æ¨¡å¼æ”¶æ•›åˆ°**ä¸€è‡´ã€æ¨¡æ€ä¸å˜çš„æ½œåœ¨ç­–ç•¥**ï¼Œæå‡äº†æ³›åŒ–èƒ½åŠ›å’Œç¨³å®šæ€§ã€‚\n        *   **ç»“æœ**ï¼šåœ¨æå…·æŒ‘æˆ˜çš„é•¿è§†é‡VLNï¼ˆLH-VLNï¼‰åŸºå‡†ä¸Šï¼Œå¯¼èˆªæˆåŠŸç‡ï¼ˆSRç­‰æŒ‡æ ‡ï¼‰æ˜¾è‘—è¶…è¶Šæ‰€æœ‰å‰ä½œï¼ŒåŒæ—¶**æ¨ç†é€Ÿåº¦ï¼ˆAPSï¼‰æ¯”æ˜¾å¼CoTæ–¹æ³•å¿«çº¦5å€**ï¼Œå®ç°äº†â€œå…·æœ‰æ¨ç†æ„è¯†ä¸”å®æ—¶â€çš„å¯¼èˆªã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼ŒFantasyVLNçš„æ ¸å¿ƒçªç ´åœ¨äºï¼šå®ƒé€šè¿‡ä¸€ä¸ªåˆ›æ–°çš„ã€ç»Ÿä¸€çš„éšå¼æ¨ç†æ¡†æ¶ï¼Œé¦–æ¬¡åŒæ—¶è§£å†³äº†å¤šæ¨¡æ€CoTçš„è¡¨ç¤ºèƒ½åŠ›é—®é¢˜ã€æ˜¾å¼CoTçš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€ä»¥åŠå•æ¨¡æ€CoTçš„æ³›åŒ–å±€é™é—®é¢˜ï¼Œä¸ºè¿ˆå‘å®æ—¶ã€é²æ£’ã€ç±»äººçš„ embodied å¯¼èˆªæ¨ç†æä¾›äº†ä¸€æ¡åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚**",
            "timestamp": "2026-01-27T15:57:01.680163",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†å¯¹ã€ŠFantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigationã€‹è¿™ç¯‡è®ºæ–‡è¿›è¡Œæ·±å…¥åˆ†æï¼Œå¹¶è¯¦ç»†é˜è¿°å…¶æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ä¸ä»·å€¼åˆ†æ\n\næœ¬æ–‡çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ç§**ç»Ÿä¸€çš„éšå¼å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†æ¡†æ¶**ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€å¯¼èˆªä»»åŠ¡ä¸­ï¼Œä¼ ç»Ÿæ˜¾å¼æ€ç»´é“¾æ–¹æ³•å­˜åœ¨çš„**ç©ºé—´ä¿¡æ¯ç¼ºå¤±**å’Œ**æ¨ç†åºåˆ—è¿‡é•¿å¯¼è‡´å®æ—¶æ€§å·®**ä¸¤å¤§å…³é”®é—®é¢˜ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ **â€œè®­ç»ƒæ—¶ä½¿ç”¨å¤šæ¨¡æ€CoTï¼Œæ¨ç†æ—¶ç›´æ¥æ˜ å°„â€** ï¼Œä»è€Œåœ¨ä¿æŒæ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°å®æ—¶å¯¼èˆªã€‚\n\n## å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è§£\n\nä¸‹è¡¨è¯¦ç»†åˆ—å‡ºäº†è®ºæ–‡æå‡ºçš„å››é¡¹å…³é”®æŠ€æœ¯ï¼Œå¹¶åŒ…å«äº†å…¶æ•°å­¦åŸç†ã€å®ç°ç»†èŠ‚å’Œå…·ä½“å‚æ•°ã€‚\n\n| æŠ€æœ¯æ–¹æ³• | æ ¸å¿ƒç›®æ ‡ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°ä¸å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. ç´§å‡‘è§†è§‰æ€ç»´é“¾** | è§£å†³è§†è§‰CoTçš„**ä»¤ç‰Œçˆ†ç‚¸**é—®é¢˜ï¼Œå°†é«˜ç»´åƒç´ é¢„æµ‹è½¬ä¸ºä½ç»´æ½œç©ºé—´é¢„æµ‹ï¼Œæå¤§æå‡è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚ | **æ•°å­¦åŸç†**ï¼š<br>1. **æ½œç©ºé—´å‹ç¼©**ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼Œå°†å›¾åƒ \\( I \\in \\mathbb{R}^{H \\times W \\times 3} \\) ç¼–ç ä¸ºç´§å‡‘çš„æ½œè¡¨ç¤ºåºåˆ— \\( \\mathcal{H} = \\{h_1, h_2, ..., h_L\\} \\)ï¼Œå…¶ä¸­ \\( L \\ll H \\times W \\)ã€‚VARé‡‡ç”¨**åˆ†å±‚ä¸‹ä¸€å°ºåº¦é¢„æµ‹**èŒƒå¼ï¼Œä¾‹å¦‚å¯¹äºä¸€ä¸ª256x256å›¾åƒï¼Œä»…ç”¨30ä¸ªä»¤ç‰Œå³å¯é«˜ä¿çœŸé‡å»ºï¼Œå‹ç¼©æ¯”é«˜è¾¾1/2185ï¼ˆè§è¡¨1ï¼‰ã€‚<br>2. **è®­ç»ƒç›®æ ‡**ï¼šæ¨¡å‹ï¼ˆVLMï¼‰å­¦ä¹ é¢„æµ‹æœªæ¥åŠ¨ä½œå¯¹åº”çš„æ½œè¡¨ç¤º \\( \\hat{\\mathcal{H}}_t \\) å’ŒåŠ¨ä½œ \\( \\hat{\\mathcal{A}}_t \\)ã€‚æŸå¤±å‡½æ•°ä¸ºäº¤å‰ç†µæŸå¤±ï¼š<br> \\( \\mathcal{L}_{V-CoT} = \\mathcal{L}_{CE}(\\hat{\\mathcal{H}}_t, \\mathcal{H}_t) + \\mathcal{L}_{CE}(\\hat{\\mathcal{A}}_t, \\mathcal{A}_t) \\) <br>å…¶ä¸­ \\( \\mathcal{H}_t \\) æ˜¯VARç¼–ç çš„æœªæ¥è§‚æµ‹çœŸå€¼ã€‚æ¨ç†æ—¶ï¼Œ**ä»…ä½¿ç”¨VLMç”Ÿæˆæ½œè¡¨ç¤ºå’ŒåŠ¨ä½œï¼Œæ— éœ€VARè§£ç æˆåƒç´ **ï¼Œé¿å…äº†æ˜¾å¼å›¾åƒç”Ÿæˆçš„å¼€é”€ã€‚ | **å…·ä½“å‚æ•°**ï¼š<br>- **è§†è§‰å‹ç¼©å™¨**ï¼šé‡‡ç”¨é¢„è®­ç»ƒçš„VARæ¨¡å‹ï¼Œè¾“å…¥å›¾åƒåˆ†è¾¨ç‡256x256ï¼Œè¾“å‡ºæ½œè¡¨ç¤ºå°ºåº¦ä¸º4ï¼ˆæ ¹æ®å›¾3æ¶ˆèå®éªŒé€‰å®šï¼‰ï¼Œå¯¹åº”30ä¸ªè§†è§‰ä»¤ç‰Œã€‚<br>- **å¯¹æ¯”**ï¼šä¼ ç»Ÿåƒç´ çº§V-CoTï¼ˆå¦‚CoT-VLAï¼‰æ¯æ­¥äº§ç”Ÿ3000-5000ä»¤ç‰Œï¼Œè€ŒCompV-CoTä»…äº§ç”Ÿçº¦30ä¸ªä»¤ç‰Œï¼Œæ•ˆç‡æå‡ç™¾å€ã€‚ |\n| **2. ç»Ÿä¸€å¤šæ¨¡æ€æ€ç»´é“¾æ¡†æ¶** | å°†**æ–‡æœ¬CoTã€è§†è§‰CoTã€å¤šæ¨¡æ€CoT**å’Œ**éCoTï¼ˆç›´æ¥é¢„æµ‹ï¼‰** å››ç§æ¨ç†æ¨¡å¼ç»Ÿä¸€åœ¨å•ä¸€æ¨¡å‹æ¶æ„ä¸‹ï¼Œé€šè¿‡é—¨æ§ä¿¡å·çµæ´»åˆ‡æ¢ã€‚ | **æ•°å­¦åŸç†**ï¼š<br>1. **é—¨æ§æœºåˆ¶**ï¼šå¼•å…¥ä¸¤ä¸ªäºŒå…ƒé—¨æ§ä¿¡å· \\( (g_{\\mathcal{T}}, g_{\\mathcal{V}}) \\in \\{0,1\\}^2 \\) æ§åˆ¶æ¨ç†æ¨¡å¼ï¼š<br>   - (0,0): éCoTæ¨¡å¼ï¼Œç›´æ¥é¢„æµ‹åŠ¨ä½œã€‚<br>   - (1,0): æ–‡æœ¬CoTæ¨¡å¼ï¼Œç”Ÿæˆæ–‡æœ¬æ¨ç†æ­¥éª¤ \\( \\hat{\\mathcal{T}}_t \\) å’ŒåŠ¨ä½œã€‚<br>   - (0,1): è§†è§‰CoTæ¨¡å¼ï¼Œç”Ÿæˆæ½œè§†è§‰è¡¨ç¤º \\( \\hat{\\mathcal{V}}_t \\) å’ŒåŠ¨ä½œã€‚<br>   - (1,1): å¤šæ¨¡æ€CoTæ¨¡å¼ï¼Œç”Ÿæˆé…å¯¹çš„å¤šæ¨¡æ€æ¨ç†æ­¥éª¤ \\( \\hat{\\mathcal{M}}_t = [\\hat{\\mathcal{T}}_t, \\hat{\\mathcal{V}}_t] \\) å’ŒåŠ¨ä½œã€‚<br>2. **ç»Ÿä¸€å»ºæ¨¡**ï¼šæ¨¡å‹å‡½æ•°è¡¨ç¤ºä¸ºï¼š<br> \\( [\\hat{\\mathcal{R}}_t, \\hat{\\mathcal{A}}_t] = \\pi_{\\theta}(\\mathcal{I}, \\{o_{\\leq t}\\}, g_{\\mathcal{T}}, g_{\\mathcal{V}}) \\) <br> å…¶ä¸­ \\( \\hat{\\mathcal{R}}_t \\) æ ¹æ®é—¨æ§ä¿¡å·é€‰æ‹©ä¸ºNone, \\( \\hat{\\mathcal{T}}_t \\), \\( \\hat{\\mathcal{V}}_t \\), æˆ– \\( \\hat{\\mathcal{M}}_t \\)ã€‚<br>3. **æ··åˆæ•°æ®è®­ç»ƒ**ï¼šåœ¨è®­ç»ƒæ—¶ï¼Œéšæœºå‡åŒ€é‡‡æ ·é—¨æ§ä¿¡å· \\( (g_{\\mathcal{T}}, g_{\\mathcal{V}}) \\)ï¼Œå¹¶æ„é€ ç›¸åº”çš„è¾“å…¥-è¾“å‡ºå¯¹è¿›è¡Œè®­ç»ƒï¼Œç›®æ ‡å‡½æ•°ä¸ºå„æ¨¡å¼ä¸‹çš„äº¤å‰ç†µæŸå¤±ä¹‹å’Œï¼ˆå…¬å¼4ï¼‰ã€‚ | **å…·ä½“å‚æ•°**ï¼š<br>- **åŸºç¡€æ¨¡å‹**ï¼šQwen2.5-VLã€‚<br>- **ç‰¹æ®Šä»¤ç‰Œ**ï¼šæ‰©å±•è¯æ±‡è¡¨ï¼Œå¼•å…¥åŠ¨ä½œä»¤ç‰Œï¼ˆ`<|forward|>`ç­‰ï¼‰ã€VARæ½œä»¤ç‰Œï¼ˆ`<|1|>`-`<|4096|>`ï¼‰ã€ç³»ç»Ÿä»¤ç‰Œï¼ˆ`<think>`ï¼‰ä»¥åŠ**é—¨æ§ä»¤ç‰Œ**ï¼ˆ`<textual_think>`, `<no_textual_think>`, `<visual_think>`, `<no_visual_think>`ï¼‰ä½œä¸ºé—¨æ§ä¿¡å· \\( g_{\\mathcal{T}}, g_{\\mathcal{V}} \\) çš„è¾“å…¥ã€‚ |\n| **3. è·¨æ¨¡å¼å¯¹é½çº¦æŸ** | ç¡®ä¿ä¸åŒæ¨ç†æ¨¡å¼å­¦ä¹ åˆ°**ä¸€è‡´ä¸”æ¨¡æ€ä¸å˜**çš„æ½œåœ¨ç­–ç•¥è¡¨ç¤ºï¼Œé˜²æ­¢æ¨¡å¼å†²çªï¼Œå¹¶è®©éCoTæ¨¡å¼ä»å¤šæ¨¡æ€CoTè®­ç»ƒä¸­å—ç›Šã€‚ | **æ•°å­¦åŸç†**ï¼š<br>1. **å¯¹é½ç›®æ ‡**ï¼šä»¥éCoTæ¨¡å¼é¢„æµ‹çš„åŠ¨ä½œåˆ†å¸ƒä½œä¸ºâ€œè½¯ç›®æ ‡â€ï¼Œçº¦æŸå…¶ä»–CoTæ¨¡å¼é¢„æµ‹çš„åŠ¨ä½œå‘å…¶å¯¹é½ã€‚<br>2. **ä¸¤é˜¶æ®µä¼˜åŒ–**ï¼ˆè§ç®—æ³•1ï¼‰ï¼š<br>   a. **éCoTä¼˜åŒ–**ï¼šæœ€å°åŒ– \\( \\mathcal{L}_{non-CoT} = \\mathcal{L}_{CE}(\\hat{\\mathcal{A}}_t, \\mathcal{A}_t) \\)ï¼Œæ›´æ–°æ¨¡å‹å‚æ•°ã€‚<br>   b. **è·å–è½¯ç›®æ ‡**ï¼šå†æ¬¡å‰å‘ä¼ æ’­éCoTæ¨¡å¼ï¼Œå¾—åˆ°åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ \\( \\tilde{\\mathcal{A}}_t \\)ï¼ˆä½¿ç”¨`stop_gradient`ï¼‰ã€‚<br>   c. **è”åˆä¼˜åŒ–**ï¼šæœ€å°åŒ–å¯¹é½æŸå¤±ä¸CoTç”ŸæˆæŸå¤±ï¼š<br> \\( \\mathcal{L}_{Joint}^* = \\mathcal{L}_{Align} + \\mathcal{L}_{CoT} \\) <br> å…¶ä¸­ï¼š<br> \\( \\mathcal{L}_{Align} = \\mathcal{L}_{CE}(\\hat{\\mathcal{A}}_t^{\\mathcal{T}}, \\tilde{\\mathcal{A}}_t) + \\mathcal{L}_{CE}(\\hat{\\mathcal{A}}_t^{\\mathcal{V}}, \\tilde{\\mathcal{A}}_t) + \\mathcal{L}_{CE}(\\hat{\\mathcal{A}}_t^{\\mathcal{M}}, \\tilde{\\mathcal{A}}_t) \\) <br> \\( \\mathcal{L}_{CoT} = \\mathcal{L}_{CE}(\\hat{\\mathcal{T}}_t, \\mathcal{T}_t) + \\mathcal{L}_{CE}(\\hat{\\mathcal{V}}_t, \\mathcal{V}_t) + \\mathcal{L}_{CE}(\\hat{\\mathcal{M}}_t, \\mathcal{M}_t) \\) <br> é€šè¿‡äº¤æ›¿ä¼˜åŒ–(a)å’Œ(c)ï¼Œæ¨¡å‹å°†å¤šæ ·çš„CoTæ¨ç†æ¨¡å¼éšå¼åœ°ç¼–ç åˆ°ç»Ÿä¸€çš„æ½œåœ¨è¡¨ç¤ºä¸­ã€‚ | **å…·ä½“å‚æ•°**ï¼š<br>- **å¯¹é½æƒé‡**ï¼šåœ¨æ€»æŸå¤± \\( \\mathcal{L}_{Joint}^* \\) ä¸­ï¼Œå¯¹é½æŸå¤± \\( \\mathcal{L}_{Align} \\) å’ŒCoTç”ŸæˆæŸå¤± \\( \\mathcal{L}_{CoT} \\) çš„æƒé‡ä¸º1:1ï¼ˆå…¬å¼7ï¼‰ã€‚<br>- **æ•ˆæœ**ï¼šå¦‚è¡¨5æ‰€ç¤ºï¼ŒåŠ å…¥è¯¥çº¦æŸåï¼ŒSRä»0æå‡è‡³2.44ï¼ŒISRä»2.39æå‡è‡³11.01ï¼Œè¯æ˜å…¶å¯¹æ€§èƒ½è‡³å…³é‡è¦ã€‚ |\n| **4. éšå¼æ¨ç†æœºåˆ¶** | **æ¨ç†æ—¶å®Œå…¨æ‘’å¼ƒæ˜¾å¼çš„ã€é€æ­¥çš„CoTä»¤ç‰Œç”Ÿæˆ**ï¼Œç›´æ¥ä½¿ç”¨éCoTæ¨¡å¼è¿›è¡ŒæŒ‡ä»¤åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼Œå®ç°å®æ—¶å¯¼èˆªã€‚ | **æ•°å­¦åŸç†**ï¼š<br>è¿™æ˜¯æ•´ä¸ªæ¡†æ¶çš„**æœ€ç»ˆåº”ç”¨å½¢æ€**ï¼Œå…¶æœ‰æ•ˆæ€§ä¾èµ–äºå‰ä¸‰ä¸ªæŠ€æœ¯çš„è®­ç»ƒã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡**UM-CoTæ¡†æ¶**æ¥è§¦äº†ä¸°å¯Œçš„å¤šæ¨¡æ€æ¨ç†æ•°æ®ï¼Œå¹¶é€šè¿‡**è·¨æ¨¡å¼å¯¹é½çº¦æŸ**å°†æ¨ç†èƒ½åŠ›è’¸é¦åˆ°äº†éCoTæ¨¡å¼çš„å‚æ•°ä¸­ã€‚å› æ­¤ï¼Œåœ¨æ¨ç†æ—¶ï¼Œå°½ç®¡æ¨¡å‹åªè¿è¡Œ \\( (g_{\\mathcal{T}}=0, g_{\\mathcal{V}}=0) \\) çš„æ¨¡å¼ï¼Œå…¶å†…éƒ¨è¡¨ç¤ºå·²ç»å…·å¤‡äº†â€œæ€è€ƒâ€è¿‡çš„ç‰¹æ€§ï¼Œèƒ½å¤Ÿåšå‡ºæ›´é²æ£’ã€æ›´é•¿è¿œçš„å†³ç­–ã€‚ | **å…·ä½“å‚æ•°**ï¼š<br>- **æ¨ç†æ¨¡å¼**ï¼šå›ºå®šä½¿ç”¨ `(g_{\\mathcal{T}}=0, g_{\\mathcal{V}}=0)`ï¼Œè¾“å…¥ä¸ºæŒ‡ä»¤å’Œå†å²åŠå½“å‰è§‚æµ‹ï¼Œç›´æ¥è¾“å‡ºåŠ¨ä½œä»¤ç‰Œã€‚<br>- **æ•ˆç‡æå‡**ï¼šå¦‚è¡¨4æ‰€ç¤ºï¼Œéšå¼æ¨ç†çš„**æ¯ç§’åŠ¨ä½œæ•°**é«˜è¾¾1.03ï¼Œè€Œæ˜¾å¼CoTæ–¹æ³•ï¼ˆCoT-VLAï¼‰ä»…ä¸º0.19ï¼Œ**æ¨ç†å»¶è¿Ÿé™ä½äº†ä¸€ä¸ªæ•°é‡çº§**ï¼Œæ»¡è¶³äº†VLNçš„å®æ—¶æ€§è¦æ±‚ã€‚ |\n\n## ",
            "timestamp": "2026-01-27T15:58:12.074125",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡åœ¨LH-VLNåŸºå‡†ä¸Šä½¿ç”¨äº†ä»¥ä¸‹å››ä¸ªæ ¸å¿ƒæŒ‡æ ‡æ¥è¯„ä¼°å¤šé˜¶æ®µå¯¼èˆªä»»åŠ¡çš„æ€§èƒ½ï¼Œå…·ä½“ç»†èŠ‚å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æŒ‡æ ‡åç§° | è‹±æ–‡å…¨ç§° | è®¡ç®—å…¬å¼/å®šä¹‰ | è¯„ä¼°ç›®çš„ä¸ç»†èŠ‚ |\n| :--- | :--- | :--- | :--- |\n| **æˆåŠŸç‡ (SR)** | Success Rate | `SR = (æˆåŠŸå®Œæˆçš„ä»»åŠ¡æ•°) / (æ€»ä»»åŠ¡æ•°)` | **è¯„ä¼°æ•´ä½“ä»»åŠ¡å®Œæˆèƒ½åŠ›**ã€‚ä¸€ä¸ªâ€œä»»åŠ¡â€åŒ…å«å¤šä¸ªæŒ‰é¡ºåºå®Œæˆçš„å­ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼šå…ˆå»å®¢å…ï¼Œå†å»å¨æˆ¿ï¼‰ã€‚åªæœ‰**æ‰€æœ‰å­ä»»åŠ¡**éƒ½æˆåŠŸå®Œæˆï¼Œè¯¥ä»»åŠ¡æ‰è®¡ä¸ºæˆåŠŸã€‚è¿™æ˜¯æœ€ä¸¥æ ¼çš„æŒ‡æ ‡ã€‚ |\n| **ç‹¬ç«‹å­ä»»åŠ¡æˆåŠŸç‡ (ISR)** | Independent Success Rate | `ISR = (æˆåŠŸçš„å­ä»»åŠ¡æ•°) / (æ€»å­ä»»åŠ¡æ•°)` | **è¯„ä¼°å•ä¸ªå­ä»»åŠ¡çš„å¯¼èˆªèƒ½åŠ›**ï¼Œä¸è€ƒè™‘å­ä»»åŠ¡ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚å³ä½¿æ•´ä¸ªä»»åŠ¡å¤±è´¥ï¼Œå…¶ä¸­æˆåŠŸå®Œæˆçš„å­ä»»åŠ¡ä¹Ÿä¼šè¢«è®¡å…¥ã€‚åæ˜ äº†æ¨¡å‹åœ¨å±€éƒ¨è·¯å¾„è§„åˆ’ä¸Šçš„ç¨³å¥æ€§ã€‚ |\n| **æ¡ä»¶æˆåŠŸç‡ (CSR)** | Conditional Success Rate | `CSR = Î£ (å­ä»»åŠ¡æƒé‡ * å­ä»»åŠ¡æˆåŠŸä¸å¦) / æ€»å­ä»»åŠ¡æ•°`ï¼Œå…¶ä¸­æƒé‡å–å†³äºå‰åºå­ä»»åŠ¡æ˜¯å¦æˆåŠŸã€‚ | **è¯„ä¼°åœ¨ä¾èµ–é“¾ä¸Šçš„æ€§èƒ½**ã€‚ä¸ºæ¯ä¸ªå­ä»»åŠ¡åˆ†é…ä¸€ä¸ªæƒé‡ï¼Œè¯¥æƒé‡åŸºäº**ä¹‹å‰æ‰€æœ‰å­ä»»åŠ¡æ˜¯å¦éƒ½æˆåŠŸ**ã€‚å¦‚æœå‰åºå­ä»»åŠ¡å¤±è´¥ï¼Œå½“å‰å­ä»»åŠ¡çš„æƒé‡ä¼šé™ä½ï¼ˆç”šè‡³ä¸º0ï¼‰ï¼Œè¿™æƒ©ç½šäº†é”™è¯¯ç´¯ç§¯ï¼Œæ›´èƒ½åæ˜ é•¿è§†é‡è§„åˆ’çš„å®é™…æ•ˆæœã€‚ |\n| **åŸºäºä¸“å®¶è½¨è¿¹åŠ æƒçš„æ¡ä»¶æˆåŠŸç‡ (CGT)** | CSR weighted by Ground Truth | åœ¨CSRçš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥ç”¨**ä¸“å®¶æ¼”ç¤ºè½¨è¿¹çš„é•¿åº¦**å¯¹æ¯ä¸ªå­ä»»åŠ¡çš„è´¡çŒ®è¿›è¡ŒåŠ æƒã€‚ | **è¯„ä¼°å¯¼èˆªæ•ˆç‡**ã€‚ä¸ä»…è€ƒè™‘æˆåŠŸä¸å¦ï¼Œè¿˜è€ƒè™‘è·¯å¾„è´¨é‡ã€‚æ›´æ¥è¿‘ä¸“å®¶ï¼ˆæœ€ä¼˜ï¼‰è½¨è¿¹é•¿åº¦çš„æˆåŠŸå¯¼èˆªä¼šè·å¾—æ›´é«˜æƒé‡ï¼Œé¼“åŠ±æ¨¡å‹å¯»æ‰¾æ›´çŸ­ã€æ›´é«˜æ•ˆçš„è·¯å¾„ã€‚ |\n\n**æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†ä¸€ä¸ªæ•ˆç‡æŒ‡æ ‡ï¼š**\n*   **æ¯ç§’åŠ¨ä½œæ•° (APS)**: `APS = N_act / T_nav`ã€‚å…¶ä¸­ `N_act` æ˜¯æ‰§è¡Œçš„åŠ¨ä½œæ€»æ•°ï¼Œ`T_nav` æ˜¯æ€»å¯¼èˆªæ—¶é—´ï¼ˆç§’ï¼‰ã€‚è¯¥æŒ‡æ ‡ç”¨äº**é‡åŒ–æ¨ç†æ•ˆç‡**ï¼Œå¯¹æ¯”æ˜¾å¼CoTï¼ˆç”Ÿæˆå¤§é‡ä¸­é—´ä»¤ç‰Œï¼‰å’Œéšå¼CoTï¼ˆç›´æ¥é¢„æµ‹åŠ¨ä½œï¼‰æ–¹æ³•çš„å®æ—¶æ€§å·®å¼‚ã€‚\n\n### äºŒã€ ä½¿ç”¨çš„æ•°æ®é›†\n\nè®ºæ–‡åœ¨ **LH-VLN** æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚\n\n| æ•°æ®é›†å±æ€§ | ç»†èŠ‚æè¿° |\n| :--- | :--- |\n| **æ•°æ®é›†åç§°** | **LH-VLN** (Towards Long-Horizon Vision-Language Navigation) |\n| **æ ¸å¿ƒç‰¹ç‚¹** | 1. **å¤šé˜¶æ®µä»»åŠ¡**ï¼šæ¯ä¸ªå¯¼èˆªæŒ‡ä»¤åŒ…å«å¤šä¸ªéœ€è¦æŒ‰é¡ºåºåˆ°è¾¾çš„å­ç›®æ ‡ã€‚<br>2. **é•¿è§†é‡è½¨è¿¹**ï¼šå¯¼èˆªè·¯å¾„é•¿ï¼Œå¯¹æ¨¡å‹çš„é•¿æœŸæ¨ç†å’Œè§„åˆ’èƒ½åŠ›è¦æ±‚é«˜ã€‚<br>3. **è¿ç»­ç¯å¢ƒ**ï¼šåœ¨é€¼çœŸçš„ã€è¿ç»­çš„3Dä»¿çœŸç¯å¢ƒï¼ˆå¦‚Habitatï¼‰ä¸­è¿›è¡Œï¼Œè€Œéç¦»æ•£çš„å¯¼èˆªå›¾ã€‚ |\n| **ä»»åŠ¡å½¢å¼** | æ™ºèƒ½ä½“æ¥æ”¶è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼ˆå¦‚â€œå…ˆå»å®¢å…çš„æ²™å‘æ—ï¼Œç„¶åè¿›å…¥å¨æˆ¿æ‰“å¼€å†°ç®±â€ï¼‰å’Œå¤šè§†è§’è§†è§‰è§‚å¯Ÿï¼Œè¾“å‡ºåŠ¨ä½œåºåˆ—ï¼ˆå‰è¿›ã€å·¦è½¬ã€å³è½¬ã€åœæ­¢ï¼‰ã€‚ |\n| **æ•°æ®åˆ’åˆ†** | é‡‡ç”¨**åœ¨çº¿è¯„ä¼°**ï¼Œåœ¨**æµ‹è¯•é›†**ä¸Šè¿›è¡Œã€‚æµ‹è¯•é›†ä¸­çš„**ä»»åŠ¡åœºæ™¯å’ŒæŒ‡ä»¤éƒ½æ˜¯è®­ç»ƒæ—¶æœªè§è¿‡çš„**ï¼Œç”¨äºæ£€éªŒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ |\n| **è®­ç»ƒæ•°æ®ç»†èŠ‚** | 1. ä»ä¸“å®¶æ¼”ç¤ºè½¨è¿¹ä¸­åˆ‡ç‰‡ï¼Œæ¯ä¸ªè®­ç»ƒæ ·æœ¬åŒ…å«ï¼šæŒ‡ä»¤ã€å†å²è§†è§‰è§‚å¯Ÿåºåˆ—ã€å½“å‰å¤šè§†è§’è§‚å¯Ÿã€æœªæ¥5ä¸ªåŠ¨ä½œçš„åºåˆ—ã€‚<br>2. ä½¿ç”¨ **Qwen-VL-Max** æ¨¡å‹ä¸ºæ¯ä¸ªåˆ‡ç‰‡è‡ªåŠ¨ç”Ÿæˆ**æ–‡æœ¬CoTï¼ˆT-CoTï¼‰** æ ‡æ³¨ï¼ˆå…±18,554ä¸ªåˆ‡ç‰‡ï¼‰ã€‚<br>3. è¿›è¡Œäº†æ•°æ®å¢å¼ºï¼ŒåŒ…æ‹¬å¯¹å†å²å›¾åƒåºåˆ—çš„å‡åŒ€ä¸‹é‡‡æ ·å’Œéšæœºä¿®å‰ªï¼Œä»¥æé«˜æ¨¡å‹å¯¹è§†è§‰å†å²å˜åŒ–çš„é²æ£’æ€§ã€‚ |\n| **æŒ‘æˆ˜æ€§** | ç”±äºä»»åŠ¡å¤æ‚ã€è½¨è¿¹é•¿ï¼Œå¾®å°çš„å†³ç­–é”™è¯¯ä¼šéšæ—¶é—´ç´¯ç§¯ï¼Œå¯¼è‡´æœ€ç»ˆå¤±è´¥ã€‚å› æ­¤ï¼Œè¯¥æ•°æ®é›†èƒ½æœ‰æ•ˆåŒºåˆ†ä¸åŒæ¨¡å‹åœ¨å¤æ‚æ¨ç†å’Œé•¿æœŸè§„åˆ’ä¸Šçš„èƒ½åŠ›ã€‚ |\n\n### æ€»ç»“\n**FantasyVLN** è®ºæ–‡é€šè¿‡ **LH-VLN** è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šé˜¶æ®µé•¿è§†é‡å¯¼èˆªæ•°æ®é›†ï¼Œå…¨é¢è¯„ä¼°äº†å…¶ç»Ÿä¸€å¤šæ¨¡æ€æ¨ç†æ¡†æ¶çš„æ€§èƒ½ã€‚è¯„ä»·æŒ‡æ ‡ä¸ä»…æ¶µç›–äº†å¯¼èˆªæˆåŠŸç‡ï¼ˆSR, ISRï¼‰ï¼Œè¿˜é€šè¿‡**CSRå’ŒCGT**å¼ºè°ƒäº†ä»»åŠ¡ä¾èµ–å…³ç³»å’Œè·¯å¾„æ•ˆç‡ï¼Œå¹¶é€šè¿‡**APS**çªå‡ºäº†å…¶æ–¹æ³•åœ¨å®ç°â€œæ¨ç†æ„ŸçŸ¥çš„å®æ—¶å¯¼èˆªâ€æ–¹é¢çš„æ ¸å¿ƒä¼˜åŠ¿ã€‚",
            "timestamp": "2026-01-27T15:58:49.601881",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ **LH-VLN** åŸºå‡†ä¸Šè¿›è¡Œäº†ä¸»è¦å®éªŒå’Œè¯„ä¼°ã€‚\n- **æ•°æ®é›†å…¨ç§°**: Long-Horizon Vision-Language Navigation (LH-VLN)\n- **å…³é”®ç‰¹æ€§**:\n    - **å¤šé˜¶æ®µä»»åŠ¡**: ä»£ç†éœ€è¦æŒ‰é¡ºåºåˆ°è¾¾å¤šä¸ªç›®æ ‡ç‚¹ã€‚\n    - **é•¿è½¨è¿¹å¯¼èˆª**: å¯¼èˆªè·¯å¾„æ›´é•¿ï¼Œæ”¾å¤§äº†ç´¯ç§¯è¯¯å·®ã€‚\n    - **è¯„ä¼°è®¾ç½®**: åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œ**åœ¨çº¿è¯„ä¼°**ï¼Œä»»åŠ¡å’Œåœºæ™¯å‡ä¸º**æœªè§è¿‡çš„**ï¼Œä»¥æµ‹è¯•æ³›åŒ–èƒ½åŠ›ã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡ä½¿ç”¨äº†LH-VLNæ ‡å‡†åŒ–çš„å¤šé˜¶æ®µå¯¼èˆªç²¾åº¦æŒ‡æ ‡ï¼Œä»¥åŠä¸€ä¸ªè‡ªå®šä¹‰çš„æ¨ç†æ•ˆç‡æŒ‡æ ‡ï¼š\n\n1.  **å¯¼èˆªç²¾åº¦æŒ‡æ ‡**:\n    - **æˆåŠŸç‡ (Success Rate, SR)**: å®Œæˆæ•´ä¸ªå¤šé˜¶æ®µå¯¼èˆªä»»åŠ¡çš„æˆåŠŸç‡ã€‚\n    - **ç‹¬ç«‹æˆåŠŸç‡ (Independent Success Rate, ISR)**: æ¯ä¸ªç‹¬ç«‹å­ä»»åŠ¡çš„æˆåŠŸç‡ã€‚\n    - **æ¡ä»¶æˆåŠŸç‡ (Conditional Success Rate, CSR)**: æ ¹æ®å‰åºå­ä»»åŠ¡çš„æˆåŠŸæƒ…å†µåŠ æƒè®¡ç®—çš„ISRã€‚\n    - **åŸºäºçœŸå®è½¨è¿¹åŠ æƒçš„æ¡ä»¶æˆåŠŸç‡ (CSR weighted by Ground Truth, CGT)**: è¿›ä¸€æ­¥æ ¹æ®ä¸“å®¶è½¨è¿¹é•¿åº¦å¯¹CSRè¿›è¡ŒåŠ æƒã€‚\n\n2.  **æ¨ç†æ•ˆç‡æŒ‡æ ‡**:\n    - **æ¯ç§’åŠ¨ä½œæ•° (Actions Per Second, APS)**: è¡¡é‡æ¨ç†é€Ÿåº¦ã€‚`APS = æ‰§è¡Œçš„æ€»åŠ¨ä½œæ•° / æ€»å¯¼èˆªæ—¶é—´(ç§’)`ã€‚æ›´é«˜çš„APSä»£è¡¨æ›´å¿«çš„å®æ—¶æ¨ç†èƒ½åŠ›ã€‚\n\n### ä¸‰ã€ ä¸»è¦æ€§èƒ½æå‡\nåœ¨LH-VLNåŸºå‡†ä¸Šï¼ŒFantasyVLNç›¸æ¯”å„ç±»åŸºçº¿æ–¹æ³•å–å¾—äº†æ˜¾è‘—æå‡ã€‚\n\n#### 1. å¯¼èˆªç²¾åº¦ (ä¸»è¦ç»“æœè§è¡¨2)\nFantasyVLNåœ¨**æ‰€æœ‰ç²¾åº¦æŒ‡æ ‡ä¸Šå‡å–å¾—æœ€ä½³æ€§èƒ½**ï¼š\n\n| æŒ‡æ ‡ | FantasyVLN æ€§èƒ½ | å¯¹æ¯”æœ€å¼ºåŸºçº¿ (Aux-Think) | ç›¸å¯¹æå‡ |\n| :--- | :--- | :--- | :--- |\n| **SR** | **2.44** | 0.65 | **æå‡çº¦275%** |\n| **ISR** | **11.01** | 3.16 | **æå‡çº¦248%** |\n| **CSR** | **9.64** | 2.04 | **æå‡çº¦372%** |\n| **CGT** | **8.99** | 2.91 (MGDM) | **æå‡çº¦209%** |\n\n- **ç»“è®º**: FantasyVLNæ˜¾è‘—è¶…è¶Šäº†çº¯æ–‡æœ¬CoTæ–¹æ³•ï¼ˆå¦‚Aux-Thinkï¼‰ã€è§†è§‰CoTæ–¹æ³•ï¼ˆå¦‚CoT-VLAã€WorldVLAï¼‰ä»¥åŠåŸºäºè®°å¿†çš„æ–¹æ³•ï¼ˆMGDMï¼‰ã€‚è¿™è¯æ˜äº†å…¶**ç»Ÿä¸€å¤šæ¨¡æ€éšå¼æ¨ç†æ¡†æ¶**åœ¨å¤„ç†å¤æ‚é•¿è§†é‡å¯¼èˆªä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚\n\n#### 2. æ¨ç†æ•ˆç‡ (ä¸»è¦ç»“æœè§è¡¨4)\nFantasyVLNå®ç°äº†**é«˜ç²¾åº¦ä¸‹çš„å®æ—¶æ¨ç†**ï¼š\n\n| æ–¹æ³• | æ¨ç†æ¨¡å¼ | æ¨¡å‹å¤§å° | APS | å¯¹æ¯”åˆ†æ |\n| :--- | :--- | :--- | :--- | :--- |\n| **CoT-VLA** | æ˜¾å¼æ¨ç† | 7B | 0.19 | åŸºçº¿ï¼ˆæ˜¾å¼ç”ŸæˆCoTåºåˆ—ï¼Œé€Ÿåº¦æ…¢ï¼‰ |\n| **WorldVLA** | éšå¼æ¨ç† | 7B | 1.02 | - |\n| **Aux-Think** | éšå¼æ¨ç† | 8B | 0.97 | - |\n| **FantasyVLN** | **éšå¼æ¨ç†** | **7B** | **1.03** | **ä¸æœ€ä¼˜éšå¼æ–¹æ³•ç›¸å½“ï¼Œä½†ç²¾åº¦æ›´é«˜** |\n\n- **å…³é”®æå‡**: ç›¸æ¯”æ˜¾å¼CoTæ–¹æ³•ï¼ˆCoT-VLAï¼‰ï¼Œ**æ¨ç†é€Ÿåº¦æå‡äº†çº¦5.4å€**ï¼ˆAPSä»0.19æå‡è‡³1.03ï¼‰ã€‚\n- **æ ¸å¿ƒä»·å€¼**: FantasyVLNæˆåŠŸè§£å†³äº†å¤šæ¨¡æ€CoTå¸¦æ¥çš„**ä»¤ç‰Œçˆ†ç‚¸**é—®é¢˜ï¼Œé€šè¿‡éšå¼æ¨ç†å’ŒVARæ½œåœ¨ç©ºé—´å‹ç¼©ï¼Œåœ¨**ä¸ç‰ºç‰²ç²¾åº¦**çš„å‰æä¸‹ï¼Œå®ç°äº†**ä¸€ä¸ªæ•°é‡çº§**çš„æ¨ç†åŠ é€Ÿï¼Œæ»¡è¶³äº†VLNä»»åŠ¡çš„å®æ—¶æ€§è¦æ±‚ã€‚\n\n#### 3. æ¶ˆèå®éªŒéªŒè¯çš„å…³é”®æå‡ç‚¹ (è§è¡¨3ã€5)\n- **å¤šæ¨¡æ€èåˆçš„ä»·å€¼**: åŒæ—¶ä½¿ç”¨æ–‡æœ¬CoTã€è§†è§‰CoTå’Œå¤šæ¨¡æ€CoTè¿›è¡Œè®­ç»ƒï¼Œæ€§èƒ½ä¼˜äºä»»ä½•å•ä¸€æˆ–éƒ¨åˆ†ç»„åˆæ¨¡å¼ã€‚\n- **è·¨æ¨¡æ€å¯¹é½çº¦æŸçš„é‡è¦æ€§**: ä½¿ç”¨è¯¥çº¦æŸåï¼ŒSRä»**0æå‡è‡³2.44**ï¼ŒISRä»2.39æå‡è‡³11.01ã€‚è¿™è¯æ˜äº†è¯¥æœºåˆ¶å¯¹äºç»Ÿä¸€æ¡†æ¶ä¸‹æ•´åˆä¸åŒæ¨ç†æ¨¡å¼ã€å­¦ä¹ æ¨¡æ€ä¸å˜è¡¨å¾è‡³å…³é‡è¦ã€‚\n\n### æ€»ç»“\nFantasyVLNåœ¨**LH-VLN**è¿™ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿è§†é‡ã€å¤šé˜¶æ®µå¯¼èˆªæ•°æ®é›†ä¸Šï¼Œé€šè¿‡å…¶åˆ›æ–°çš„ç»Ÿä¸€å¤šæ¨¡æ€éšå¼æ¨ç†æ¡†æ¶ï¼Œå®ç°äº†ï¼š\n1.  **å¯¼èˆªç²¾åº¦çš„æ˜¾è‘—é£è·ƒ**ï¼šåœ¨SRã€ISRç­‰æ ¸å¿ƒæŒ‡æ ‡ä¸Šï¼Œç›¸å¯¹ç°æœ‰æœ€ä½³æ–¹æ³•æå‡çº¦2.5-3.7å€ã€‚\n2.  **æ¨ç†æ•ˆç‡çš„é©å‘½æ€§æ”¹è¿›**ï¼šç›¸æ¯”æ˜¾å¼CoTæ–¹æ³•ï¼Œæ¨ç†é€Ÿåº¦æå‡ä¸€ä¸ªæ•°é‡çº§ï¼ˆçº¦5å€ï¼‰ï¼Œè¾¾åˆ°å®æ—¶å¯¼èˆªè¦æ±‚ã€‚\n3.  **æŠ€æœ¯åˆ›æ–°çš„æœ‰æ•ˆéªŒè¯**ï¼šæ¶ˆèå®éªŒè¯å®äº†å…¶**å¤šæ¨¡æ€èåˆ**ã€**æ½œåœ¨ç©ºé—´å‹ç¼©(CompV-CoT)** å’Œ**è·¨æ¨¡æ€å¯¹é½**ç­‰æ ¸å¿ƒè®¾è®¡çš„å¿…è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚",
            "timestamp": "2026-01-27T15:59:30.857724",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹ã€ŠFantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigationã€‹çš„æ·±å…¥é˜…è¯»ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•è™½ç„¶å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä¸€äº›å›ºæœ‰çš„å±€é™æ€§ã€‚\n\n### **ä¸»è¦å±€é™æ€§**\n\n1.  **å¯¹é¢„è®­ç»ƒè§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ˆVARï¼‰çš„å¼ºä¾èµ–**\n    - **å‹ç¼©ç“¶é¢ˆ**ï¼šæ•´ä¸ª**ç´§å‡‘è§†è§‰æ€ç»´é“¾ï¼ˆCompV-CoTï¼‰** çš„æ ¸å¿ƒä¾èµ–äºVARæ¨¡å‹å°†é«˜ç»´è§†è§‰è§‚å¯Ÿå‹ç¼©åˆ°æä½ç»´çš„æ½œåœ¨ç©ºé—´ï¼ˆå¦‚30ä¸ªtokenï¼‰ã€‚è¿™å¸¦æ¥äº†ä¸¤ä¸ªé£é™©ï¼š\n        - **ä¿¡æ¯æŸå¤±**ï¼šVARæœ¬èº«çš„é‡æ„è¯¯å·®ï¼ˆMSEä¸º0.039ï¼Œé«˜äºå…¶ä»–å‹ç¼©å™¨ï¼‰å¯èƒ½å¯¼è‡´å…³é”®ç©ºé—´æˆ–è¯­ä¹‰ç»†èŠ‚çš„ä¸¢å¤±ï¼Œå½±å“é•¿ç¨‹è§„åˆ’çš„å‡†ç¡®æ€§ã€‚\n        - **æ¨¡å‹è€¦åˆ**ï¼šFantasyVLNçš„æ€§èƒ½ä¸Šé™å—é™äºVARçš„å‹ç¼©ä¸é‡å»ºèƒ½åŠ›ã€‚VARçš„æ”¹è¿›æˆ–å¤±è´¥ä¼šç›´æ¥ä¼ å¯¼è‡³å¯¼èˆªæ¨¡å‹ã€‚\n\n2.  **è®­ç»ƒæ•°æ®çš„è§„æ¨¡ä¸è´¨é‡é™åˆ¶**\n    - **æ•°æ®ç¨€ç¼º**ï¼šè®ºæ–‡æŒ‡å‡ºLH-VLNåŸºå‡†çš„è®­ç»ƒæ•°æ®æœ‰é™ï¼ˆä»…18kä¸ªè½¨è¿¹åˆ‡ç‰‡ï¼‰ã€‚åœ¨**æ•°æ®æ•ˆç‡**æ–¹é¢ï¼Œæ¨¡å‹å¯èƒ½å°šæœªå¾—åˆ°å……åˆ†éªŒè¯ã€‚\n    - **æ ‡æ³¨å™ªå£°**ï¼š**æ–‡æœ¬æ€ç»´é“¾ï¼ˆT-CoTï¼‰** çš„ç›‘ç£ä¿¡å·ä¾èµ–äº`Qwen-VL-Max`è‡ªåŠ¨ç”Ÿæˆï¼Œè€Œéäººå·¥ç²¾ç¡®æ ‡æ³¨ã€‚è¿™å¼•å…¥äº†**æ ‡æ³¨å™ªå£°**å’Œ**ä¸€è‡´æ€§é£é™©**ï¼Œå¯èƒ½é™åˆ¶æ¨¡å‹å­¦ä¹ æœ€ä¼˜æ¨ç†æ¨¡å¼çš„èƒ½åŠ›ã€‚\n\n3.  **â€œè®­ç»ƒç”¨CoTï¼Œæ¨ç†ä¸ç”¨CoTâ€èŒƒå¼çš„å›ºæœ‰çŸ›ç›¾**\n    - **è¡¨å¾å¯¹é½çš„ä¸å®Œå…¨æ€§**ï¼šè™½ç„¶é€šè¿‡**è·¨æ¨¡æ€å¯¹é½çº¦æŸ**å¼ºåˆ¶ä¸åŒæ¨ç†æ¨¡å¼è¾“å‡ºä¸€è‡´çš„åŠ¨ä½œï¼Œä½†â€œéšå¼æ¨ç†â€åœ¨æµ‹è¯•æ—¶å®Œå…¨æ‘’å¼ƒäº†æ˜¾å¼çš„ä¸­é—´æ­¥éª¤ç”Ÿæˆã€‚è¿™å­˜åœ¨ä¸€ä¸ª**å‡è®¾**ï¼šè®­ç»ƒæ—¶å­¦åˆ°çš„â€œæ¨ç†æ„ŸçŸ¥è¡¨å¾â€èƒ½å®Œå…¨ç­‰ä»·äºè¿è¡Œæ—¶æ‰€éœ€çš„æ¨ç†èƒ½åŠ›ã€‚å¯¹äºæå…¶å¤æ‚æˆ–è®­ç»ƒåˆ†å¸ƒå¤–çš„åœºæ™¯ï¼Œè¿™ç§**è¡¨å¾è¿ç§»çš„æœ‰æ•ˆæ€§**å¯èƒ½ä¸‹é™ã€‚\n\n4.  **è¯„ä¼°åœºæ™¯çš„ç‰¹å®šæ€§**\n    - **åŸºå‡†å•ä¸€**ï¼šæ‰€æœ‰å®éªŒå‡åœ¨**LH-VLN**ï¼ˆé•¿è§†é‡ã€å¤šé˜¶æ®µï¼‰åŸºå‡†ä¸Šè¿›è¡Œã€‚è™½ç„¶è¯¥åŸºå‡†å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½†æ¨¡å‹åœ¨**æ›´å¹¿æ³›VLNä»»åŠ¡**ï¼ˆå¦‚R2Rã€VLN-CEç­‰ï¼‰ä¸Šçš„æ³›åŒ–èƒ½åŠ›å°šæœªå¾—åˆ°éªŒè¯ã€‚\n    - **æˆåŠŸç‡çš„ç»å¯¹æ°´å¹³è¾ƒä½**ï¼šå³ä½¿æ˜¯æœ€ä½³æ¨¡å‹ï¼Œå…¶**ä»»åŠ¡æˆåŠŸç‡ï¼ˆSRï¼‰** ä¹Ÿä»…ä¸º2.44%ã€‚è¿™è™½ç„¶è¶…è¶Šäº†åŸºçº¿ï¼Œä½†è¡¨æ˜åœ¨**æç«¯é•¿ç¨‹ã€å¤šé˜¶æ®µå¯¼èˆª**é—®é¢˜ä¸Šï¼Œä»æœ‰å·¨å¤§çš„æ€§èƒ½æå‡ç©ºé—´ï¼Œæ–¹æ³•å°šæœªå®Œå…¨è§£å†³è¯¥éš¾é¢˜ã€‚\n\n5.  **å®æ—¶æ€§å¯¹æ¨¡å‹èƒ½åŠ›çš„æ½œåœ¨åˆ¶çº¦**\n    - **æ•ˆç‡ä¸æ€§èƒ½çš„æƒè¡¡**ï¼šä¸ºäº†è¿½æ±‚**å®æ—¶æ¨ç†ï¼ˆ~1 APSï¼‰**ï¼Œæ¨¡å‹å½»åº•æ”¾å¼ƒäº†æ˜¾å¼CoTç”Ÿæˆã€‚è¿™å›ºç„¶æå‡äº†æ•ˆç‡ï¼Œä½†ä¹Ÿ**æ°¸ä¹…å¤±å»äº†**åœ¨æµ‹è¯•æ—¶åˆ©ç”¨æ˜¾å¼ã€å¯è§£é‡Šçš„ä¸­é—´æ­¥éª¤è¿›è¡Œ**åŠ¨æ€çº é”™**æˆ–**äººç±»å¹²é¢„**çš„å¯èƒ½æ€§ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œæœªæ¥çš„ç ”ç©¶å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹å‘å±•å¼€ï¼š\n\n### **1. æå‡è§†è§‰è¡¨å¾çš„é²æ£’æ€§ä¸æ•ˆç‡**\n    - **å¼€å‘æ›´ä¼˜çš„è§†è§‰å‹ç¼©å™¨**ï¼šç ”ç©¶å…¼å…·**é«˜å‹ç¼©æ¯”**ã€**ä½é‡æ„è¯¯å·®**å’Œ**å¼ºè¯­ä¹‰ä¿æŒèƒ½åŠ›**çš„æ–°å‹è§†è§‰è¡¨å¾æ¨¡å‹ï¼Œä»¥æ›¿ä»£æˆ–æ”¹è¿›VARã€‚\n    - **æ¢ç´¢éé‡å»ºå¼è¡¨å¾**ï¼šç ”ç©¶ä¸ä¾èµ–äºåƒç´ çº§é‡å»ºçš„è§†è§‰è¡¨å¾å­¦ä¹ ï¼ˆå¦‚åŸºäºå¯¹æ¯”å­¦ä¹ æˆ–ä»»åŠ¡é©±åŠ¨çš„ç‰¹å¾ï¼‰ï¼Œç›´æ¥å­¦ä¹ å¯¹å¯¼èˆªå†³ç­–è‡³å…³é‡è¦çš„**å‡ ä½•ä¸è¯­ä¹‰ç‰¹å¾**ã€‚\n\n### **2. å¢å¼ºæ•°æ®åˆ©ç”¨ä¸è®­ç»ƒç­–ç•¥**\n    - **å¤§è§„æ¨¡åˆæˆæ•°æ®ç”Ÿæˆ**ï¼šåˆ©ç”¨**ä»¿çœŸå¼•æ“**æˆ–**ä¸–ç•Œæ¨¡å‹**è‡ªåŠ¨ç”Ÿæˆæµ·é‡ã€å¤šæ ·åŒ–çš„å¯¼èˆªè½¨è¿¹ä¸å¯¹åº”çš„å¤šæ¨¡æ€CoTæ•°æ®ï¼Œä»¥çªç ´çœŸå®æ•°æ®è§„æ¨¡çš„ç“¶é¢ˆã€‚\n    - **è‡ªç›‘ç£ä¸è‡ªæ”¹è¿›æœºåˆ¶**ï¼šå¼•å…¥**è‡ªè®­ç»ƒ**æˆ–**å¼ºåŒ–å­¦ä¹ **æ¡†æ¶ï¼Œä½¿æ¨¡å‹èƒ½åœ¨ä¸ç¯å¢ƒçš„äº¤äº’ä¸­**è‡ªæˆ‘ç”Ÿæˆé«˜è´¨é‡CoTæ•°æ®**å¹¶è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼Œå¦‚å€Ÿé‰´EvolveNavçš„æ€è·¯ã€‚\n\n### **3. ä¼˜åŒ–éšå¼-æ˜¾å¼æ¨ç†ååŒæœºåˆ¶**\n    - **åŠ¨æ€å¯åˆ‡æ¢æ¨ç†**ï¼šè®¾è®¡ä¸€ä¸ª**è‡ªé€‚åº”é—¨æ§æœºåˆ¶**ï¼Œå…è®¸æ¨¡å‹åœ¨è¿è¡Œæ—¶æ ¹æ®ä»»åŠ¡å¤æ‚åº¦ã€ç½®ä¿¡åº¦æˆ–è®¡ç®—èµ„æºï¼Œ**åŠ¨æ€é€‰æ‹©**ä½¿ç”¨éšå¼æ¨ç†ï¼ˆé«˜æ•ˆï¼‰æˆ–æ˜¾å¼æ¨ç†ï¼ˆé«˜ç²¾åº¦ï¼‰æ¨¡å¼ã€‚\n    - **è½»é‡çº§æ˜¾å¼æ¨ç†**ï¼šæ¢ç´¢åœ¨æ¨ç†æ—¶ç”Ÿæˆ**æåº¦ç²¾ç®€ä½†å…³é”®**çš„ä¸­é—´æ ‡è®°ï¼ˆå¦‚å‡ ä¸ªå…³é”®è·¯æ ‡æˆ–å†³ç­–ç‚¹ï¼‰ï¼Œåœ¨å¯æ‰¿å—çš„å¼€é”€å†…ä¿ç•™éƒ¨åˆ†å¯è§£é‡Šæ€§å’Œçº é”™èƒ½åŠ›ã€‚\n\n### **4. æ‹“å±•ä»»åŠ¡ä¸åœºæ™¯çš„æ³›åŒ–æ€§**\n    - **è·¨åŸºå‡†ä¸è·¨é¢†åŸŸè¯„ä¼°**ï¼šåœ¨æ›´å¤šæ ·åŒ–çš„VLNæ•°æ®é›†ï¼ˆåŒ…æ‹¬å®¤å†…ã€å®¤å¤–ã€è¿ç»­ã€ç¦»æ•£ç¯å¢ƒï¼‰ä¸ŠéªŒè¯FantasyVLNæ¡†æ¶çš„**æ³›åŒ–èƒ½åŠ›**ã€‚\n    - **é›†æˆæ›´ä¸°å¯Œçš„æ¨¡æ€**ï¼šè€ƒè™‘èå…¥**æ·±åº¦ä¿¡æ¯**ã€**éŸ³é¢‘**ï¼ˆå¦‚æŒ‡ä»¤è¯­éŸ³ï¼‰æˆ–**ç‰©ç†å¸¸è¯†**ï¼Œæ„å»ºæ›´å…¨é¢çš„ä¸–ç•Œæ¨¡å‹ï¼Œä»¥åº”å¯¹æ›´å¤æ‚çš„ç°å®ç¯å¢ƒã€‚\n\n### **5. ç†è®ºåˆ†æä¸æ¨¡å‹å¯è§£é‡Šæ€§æ·±åŒ–**\n    - **è§£è€¦ä¸å½’å› åˆ†æ**ï¼šé€šè¿‡æ¶ˆèå®éªŒå’Œå¯è§†åŒ–æŠ€æœ¯ï¼Œæ›´æ·±å…¥åœ°ç†è§£**æ–‡æœ¬CoT**ã€**è§†è§‰CoT**å’Œ**å¤šæ¨¡æ€CoT**åˆ†åˆ«å¯¹æœ€ç»ˆå¯¼èˆªå†³ç­–çš„è´¡çŒ®ç¨‹åº¦ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•åœ¨æ½œåœ¨ç©ºé—´ä¸­èåˆã€‚\n    - **å­¦ä¹ åŠ¨åŠ›å­¦ç ”ç©¶**ï¼šåˆ†æ**è·¨æ¨¡æ€å¯¹é½çº¦æŸ**å¦‚ä½•å½±å“æ¨¡å‹ä¸åŒå±‚çš„è¡¨å¾å­¦ä¹ ï¼Œä»ç†è®ºå±‚é¢éªŒè¯â€œéšå¼æ¨ç†â€æœºåˆ¶çš„æœ‰æ•ˆæ€§è¾¹ç•Œã€‚\n\n**æ€»ç»“**ï¼šFantasyVLNçš„åˆ›æ–°æ€§åœ¨äºé€šè¿‡**ç»Ÿä¸€çš„å¤šæ¨¡æ€éšå¼æ¨ç†æ¡†æ¶**ï¼Œå·§å¦™åœ°å¹³è¡¡äº†æ¨ç†èƒ½åŠ›ä¸å®æ—¶æ€§éœ€æ±‚ã€‚æœªæ¥çš„å·¥ä½œåº”è‡´åŠ›äº**å¤¯å®å…¶åŸºç¡€ç»„ä»¶ï¼ˆå¦‚è§†è§‰ç¼–ç ï¼‰**ã€**çªç ´æ•°æ®é™åˆ¶**ã€**è®¾è®¡æ›´çµæ´»çš„æ¨ç†æœºåˆ¶**ï¼Œå¹¶æœ€ç»ˆåœ¨**æ›´å¹¿æ³›ã€æ›´å¤æ‚çš„å…·èº«æ™ºèƒ½ä»»åŠ¡**ä¸­éªŒè¯å’Œæ‹“å±•è¿™ä¸€èŒƒå¼ã€‚",
            "timestamp": "2026-01-27T16:00:20.153651",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T13:54:10Z",
    "created_at": "2026-01-27T15:53:11.088331",
    "updated_at": "2026-01-27T15:53:11.088338"
}