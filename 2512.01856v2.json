{
  "id": "2512.01856v2",
  "title": "Is Image-based Object Pose Estimation Ready to Support Grasping?",
  "authors": [
    "Eric C. Joyce",
    "Qianwen Zhao",
    "Nathaniel Burgdorfer",
    "Long Wang",
    "Philippos Mordohai"
  ],
  "abstract": "We present a framework for evaluating 6-DoF instance-level object pose estimators, focusing on those that require a single RGB (not RGB-D) image as input. Besides gaining intuition about how accurate these estimators are, we are interested in the degree to which they can serve as the sole perception mechanism for robotic grasping. To assess this, we perform grasping trials in a physics-based simulator, using image-based pose estimates to guide a parallel gripper and an underactuated robotic hand in picking up 3D models of objects. Our experiments on a subset of the BOP (Benchmark for 6D Object Pose Estimation) dataset compare five open-source object pose estimators and provide insights that were missing from the literature.",
  "url": "https://arxiv.org/abs/2512.01856v2",
  "html_url": "https://arxiv.org/html/2512.01856v2",
  "html_content": "Is Image-based Object Pose Estimation Ready to Support Grasping?\nEric C. Joyce\na\n, Qianwen Zhao\na\n, Nathaniel Burgdorfer\na\n, Long Wang\na\n, Philippos Mordohai\na\na\nStevens Institute of Technology, Hoboken, NJ 07030, USA.\n{ejoyce, qzhao10, nburgdor, lwang4, pmordoha}@stevens.edu\nThis research was supported in part by NSF Grant CMMI-2138896.\nAbstract\nWe present a framework for evaluating 6-DoF instance-level object pose estimators, focusing on those that require a single RGB (not RGB-D) image as input. Besides gaining intuition about how accurate these estimators are, we are interested in the degree to which they can serve as the sole perception mechanism for robotic grasping. To assess this, we perform grasping trials in a physics-based simulator, using image-based pose estimates to guide a parallel gripper and an underactuated robotic hand in picking up 3D models of objects. Our experiments on a subset of the BOP (Benchmark for 6D Object Pose Estimation) dataset compare five open-source object pose estimators and provide insights that were missing from the literature.\nI\nIntroduction\nHow successfully can object pose estimates made from a single RGB image guide the downstream task of robotic grasping? Most robots (even advanced ones\n[\nmorgan2022complex\n,\nchen2023autobag\n,\nqi2023general\n,\nzhang2023flowbot++\n]\n) rely on RGB-D sensors. Here we investigate the effectiveness of commodity RGB cameras in the instance-level variant of pose estimation, when the 3D shape and appearance of the objects are known a priori. This setting is crucial for robots that operate in industrial and residential environments and should be able to grasp and manipulate known objects given guidance from visual stimuli. Reliance on RGB only (instead of depth sensors) is important for facilitating both indoor and outdoor operation.\nRemarkable progress in object pose estimation has been made in the past few years\n[\nfan2022deep\n,\nhodan2018bop\n,\nsahin2020review\n,\nsundermeyer2023bop\n,\nthalhammer2023challenges\n]\n, primarily driven by deep learning and the capability to reduce the so-called\nsim2real gap\n, enabling end-to-end system training on large amounts of synthetic data with precise ground truth. These systems either predict the pose directly or predict various forms of 2D-3D correspondences which are then fed to a Perspective\nn\nn\nPoint (PnP) solver to generate the pose (see Section\nII\n).\nDespite comprehensive benchmarks such as the Benchmark for 6D Object Pose Estimation (BOP)\n[\nhodan2018bop\n,\nsundermeyer2023bop\n]\n1\n1\n1\nhttps://bop.felk.cvut.cz/home/\n, the potential for deploying these pose estimators in downstream robotic applications remains unclear. One contributing factor may be that the metrics used to evaluate 6-DoF object pose can sometimes belie subtle geometric errors that cause grasps to fail. Figure\n1\nillustrates some discrepancies obfuscated by ADD(-S) and MSSD, two metrics defined by BOP and found throughout the literature (see Section\nIV-B\n). In assessing how well pose estimates guide different types of robot grippers, we complement the BOP metrics with straightforward rotation and translation errors.\nFigure 1:\nPose estimates as green overlays and their corresponding ground-truth poses as solid objects. All estimates here measured better than average ADD(-S) and MSSD and yet exhibit significant rotation and translation errors. Our trials attempt to grasp according to the estimated poses, and all estimates seen here were poor enough to cause grasping trial failures.\nOur framework for evaluating a pose estimator comprises the following steps. For each gripper, we first specify a\nreference grasp\nfor every object in a dataset. These grasps will be attempted by virtual grippers in open-loop fashion in a simulator. After defining reference grasps for each object-gripper pair, we run the pose estimator on an image containing an object of interest and record the predicted pose, as well as the ground truth that comes with the dataset. We place a virtual model of the object in isolation at the ground-truth pose in the simulator, while the gripper is instructed to grasp it according to the estimated pose, as shown in Fig.\n1\n. We consider a grasping trial successful if the centroid of the object is within a certain tolerance of its target location at the end of the reference grasp (well above the support surface) and remains steadily held for 15 seconds.\nThis study makes the following assumptions: 3D models of the objects are available; the objects are rigid and of uniform density; the intrinsics of the camera are known. Our experiments are focused on small objects contained in the BOP datasets, and we approximate their weights and friction coefficients with the grippers. The objects are isolated in the simulator. Most of these assumptions can be relaxed as our approach is further developed.\nOur experimental results (Section\nIV\n) show that improved predictions across the estimators we study\n[\nhodan2020epos\n,\nhuang2022ncf\n,\ntremblay2018deep\n,\nsu2022zebrapose\n,\nwang2021gdr\n]\ngenerally produce higher grasp success rates, though this trend falters for more complex shapes. Certain combinations of estimator, gripper, and object type are more sensitive to error than others.\nIn summary, the main contributions of our work are:\n‚Ä¢\na framework for evaluating 6-DoF object pose estimation, considering whether pose estimates can be used to guide successful grasping in simulation,\n‚Ä¢\nthe integration of visual perception on real imagery with a grasping simulator, enabling efficient evaluation of different grippers and grasping success,\n‚Ä¢\nan assessment of several representative recent image-based pose estimators that yields new insights on their effectiveness as components of a robotic system.\nThis analysis serves as the foundation for a subsequent study\n[\njoyce2025consensus\n]\non learning to predict the success of a robotic grasp before the grasp is attempted.\nII\nRelated Work\nIn this section, we review related work on instance-level 6-DoF pose estimation from images and on underactuated robotic hands.\nEstimation of 6-DoF poses for known objects from single RGB images has progressed through several phases in the past few years, as shown in surveys\n[\nfan2022deep\n,\nsahin2020review\n,\nthalhammer2023challenges\n]\nand the BOP website. The current era of estimation roughly begins with PoseCNN\n[\nxiang2017posecnn\n]\n, which directly regresses a quaternion and decouples estimates of rotation and translation.\nRobustness to occlusion and handling poses made ambiguous due to symmetry have been the primary concerns motivating developments in deep-learning methods.\nOther research related to 6-DoF pose estimation focuses on improving the performance of PnP\n[\nliu2023linear\n]\n, on adapting PnP for end-to-end training\n[\nchen2022epro\n]\n, or on providing statistically defensible bounds for pose estimates\n[\nyang2023object\n]\n. To emphasize the relevant ideas in this section, we group the paradigms of 6-DoF estimators into sparse, dense, and iterative categories.\nII-A\nSparse Methods\nSparse methods predict a handful of key-points from which pose is computed. Both BB8\n[\nrad2017bb8\n]\nand DOPE\n[\ntremblay2018deep\n]\npredict 3D bounding boxes. DOPE learns to predict belief maps about the box‚Äôs eight corners and vector fields pointing to the predicted object‚Äôs centroid.\nInspired by YOLO\n[\nredmon2016yolo\n]\n, Tekin et al.\n[\ntekin2018real\n]\npropose a network that performs a single forward pass to predict labels and projections of 3D control points.\nSundermeyer et al.\n[\nsundermeyer2018implicit\n]\npropose an augmented auto-encoder that learns latent-space representations of objects. These are grouped into a code book used to retrieve rotations, while translation is estimated separately using bounding-box diagonals.\nII-B\nDense Methods\nThough NOCS\n[\nwang2019nocs\n]\nreceives RGB-D as input and predicts poses at the category level, its dense intermediate representation proves useful against occlusion and motivates dense methods for instance-level RGB-only pose estimation.\nPVNet\n[\npeng2019pvnet\n]\nlearns to predict a per-pixel vector field for each detected object. Vectors indicate perceived object key-points passed to an uncertainty-weighted version of PnP.\nPix2Pose\n[\npark2019pix2pose\n]\nlearns to predict 3D coordinates for every pixel of a detected object, even when that object is heavily occluded.\nEPOS\n[\nhodan2020epos\n]\naims at robustness against textureless and symmetric objects by defining objects as sets of fragments. The network learns to predict probabilities for fragments to which a pixel might belong.\nGeometry-Guided Direct Regression, or GDR-Net\n[\nwang2021gdr\n]\n, combines correspondence-based estimation and direct pose regression. GDR-Net generates dense 2D-3D correspondences as intermediate features before directly regressing pose using a learned, patch-based PnP approximator. Wang et al. credit their method‚Äôs success to thoughtful representations for rotation\n[\nzhou2019continuity\n]\nand translation\n[\nli2019cdpn\n]\n, and to a loss function that combines pose and geometry.\nZebraPose\n[\nsu2022zebrapose\n]\nproduces dense 2D-3D correspondences by first learning region-specific codes for object vertices. For all objects, vertices are partitioned into iteratively halved regions and assigned a binary feature descriptor to be learned by an encoder-decoder. These codes are ultimately arbitrary, but by training the network in a coarse-to-fine manner, ZebraPose ensures that bits come to represent scales of locality. Once a network has been trained for each object, decoder output for each pixel in a given region of interest is the binary code of the 3D vertex (or neighborhood) corresponding to that 2D pixel. Pose is computed using these correspondences.\nNeural Correspondence Field\n[\nhuang2022ncf\n]\n(NCF) samples inside the camera frustum to derive 3D query points rather than pixels for correspondences. This approach aims at mitigating the effects of self-occlusion. NCF then predicts dense 3D-3D correspondences between its query points and points on the object, as well as a signed distance value for each point.\nSurfEmb\n[\nhaugaard2022surfemb\n]\nlearns to predict dense correspondence distributions over object surfaces without any prior knowledge about object symmetries. This distribution may then be sampled to form and refine pose hypotheses.\nII-C\nIterative Refinement Methods\nDeepIM\n[\nli2018deepim\n]\nlearns to predict a relative pose adjustment that improves upon a given initial pose estimate, and DenseFusion\n[\nwang2019densefusion\n]\n(an RGB-D method) makes pose refinement a differentiable, iterative process. CosyPose\n[\nlabbe2020cosypose\n]\nand MegaPose\n[\nlabbe2022megapose\n]\nuse an iterative, ‚Äúrender-and-compare‚Äù approach to estimate poses. MegaPose simultaneously learns to generalize to object categories. RNNPose\n[\nxu2022rnnpose\n]\nmakes an initial, coarse estimate and improves on it using a recursive refinement module that treats 2D and 3D features separately.\nII-D\nUnderactuated Robotic Hands and Physics Simulations\nIn addition to the widely used Franka Hand\n[\nhaddadin2022franka\n]\n, a parallel gripper, we also simulate grasping trials with a tendon-driven underactuated hand\n[\nAukes2014\n,\nCatalano2014\n,\nchen2020underactuation\n,\nCiocarlie2009\n,\nCiocarlie2014\n,\nDollar2010\n,\nGosselin2008\n,\nOdhner2014\n,\nStuart2017\n,\nwang2011highly\n]\n. These hands have become appealing for a number of reasons, including their mechanical compliance which allows for a simplified, open-loop control scheme and adapts to object shape variations when grasping.\nThe low cost and light-weight designs of underactuated hands enable use at scale. Compared to their counterpart, fully-actuated dexterous hands\n[\nJacobsen1986\n,\nLoucks1987\n,\nDeshpande2013\n]\n, underactuated hands can have higher and more realistic tolerance when object pose estimation errors are present.\nThe underactuated hand in our work is a recent design\n[\nchen2020underactuation\n]\n, the physics simulation of which has been used previously in deep reinforcement learning\n[\nchen2020hardware\n]\n. Our grasping trials are performed in MuJoCo\n[\nmujoco\n]\n.\nIII\nMethod\nIII-A\nObject Pose Estimation\nGrasping an object requires an estimate of its pose, which can be obtained by any 6-DoF estimator. Given a single RGB image, the estimator predicts a rotation and translation for all instances of known, rigid objects in the scene. Specifically, object\ni\ni\nas perceived in image\nj\nj\nyields a predicted pose\nùêì\n^\nO\ni\n,\nj\nW\ni\n,\nj\n{}^{W_{i,j}}\\hat{\\mathbf{T}}_{O_{i,j}}\n. This notation signifies a rigid transformation from the object‚Äôs frame\n{\nO\n}\n\\{O\\}\nto the world frame\n{\nW\n}\n\\{W\\}\n.\nOur interest is in assessing the quality of the estimated pose as applied to the downstream task of grasping. To make this assessment we use the ground-truth poses included in the dataset. For object\ni\ni\nin image\nj\nj\n, the ground-truth pose is\nùêì\nO\ni\n,\nj\nW\ni\n,\nj\n{}^{W_{i,j}}\\mathbf{T}_{O_{i,j}}\n. Poses of symmetric objects may be ambiguous, so measuring error in these cases requires special considerations described in Section\nIV-B\n. Our Physics-based Grasping Simulation module receives the estimated and ground-truth poses from the Object Pose Estimation module.\nIII-B\nPhysics-based Grasping Simulation\nPhysics-based grasping simulation is used to output a binary success score for each pose estimation. For each object-gripper pair, we handcraft a reference grasp plan based on the ground-truth object pose and an open-loop control policy.\nIII-B1\nParallel and Underactuated Grippers\nThe virtual parallel gripper used in our simulator trials is the Franka Hand\n[\nhaddadin2022franka\n]\n, and the underactuated hand used is the design case III presented in\n[\nchen2020underactuation\n]\n. We anticipate that the more advanced underactuated hand will better tolerate pose error. Comparing their performances will indicate how successfully pose estimators can mitigate this disparity.\nIII-B2\nOpen-loop Control Policy and Reference Grasps\nWe use a simplified, rigid, and open-loop control policy to execute a grasping and picking task for each object. It is termed open-loop because the system does not utilize any sensory feedback, except for an initial object pose estimate to be used in the pre-planned open-loop trajectory.\nFigure\n2\nillustrates the four stages of the open-loop control policy.\nThe available control actions include the position and orientation of the gripper base (6-DoF) and the single actuator that has one DoF for both grippers. In Stage 0, the gripper is positioned and oriented to an initial pose that is free from collisions with the environment or objects; in Stage I, the gripper is moved to a pre-grasp configuration close to the object; in Stages II and III, the gripper is actuated to close and then to pick up the object.\nIt is worth noting that the most critical part is Stage I, in which the pre-grasp gripper position and orientation are determined by the pose given by an estimator.\nWhen using the ground-truth object pose to generate the grasp commands for a given object-gripper pair, we term this set of commands a\nreference grasp\n. For the experiments shown in Section\nIV\n, we have selected a total of 15 objects from the LM-O and YCB-V datasets.\nAll selected objects and\none of the two reference grasps for each of them are illustrated in Fig.\n3\n.\nFigure 2:\nBreakdown of different stages of a simulated grasping task using a simplified open-loop control policy.\nFigure 3:\nExample reference grasps for selected objects in the LM-O dataset (a-h) and YCB-V dataset (i-o). All grasping trials are attempted with both the parallel gripper and the underactuated hand.\nIII-B3\nGenerating Grasping Results for Given Object Pose Estimates\nEach pose estimate generated by an estimator may deviate from the ground truth. We capture this deviation and then apply it to a reference grasp as follows.\nThe estimated and the ground-truth pose of the\ni\nth\ni^{\\text{th}}\nobject in the\nj\nth\nj^{\\text{th}}\nimage are both described in a shared World,\n{\nW\ni\n,\nj\n}\n\\{W_{i,j}\\}\n.\nTherefore, the estimate‚Äôs deviation is expressed as:\nùêì\nŒî\n‚Äã\nEst.\nj\n/\nGT.\nW\ni\n,\nj\n=\nùêì\n^\nO\ni\n,\nj\nW\ni\n,\nj\n‚Äã\n(\nùêì\nO\ni\n,\nj\n,\nGT\nW\ni\n,\nj\n)\n‚àí\n1\n{}^{W_{i,j}}\\mathbf{T}_{\\Delta\\text{Est.}_{j}/\\text{GT.}}={}^{W_{i,j}}\\hat{\\mathbf{T}}_{O_{i,j}}\\;\\Big({}^{W_{i,j}}\\mathbf{T}_{O_{i,j,\\text{GT}}}\\Big)^{-1}\n(1)\nThen, using the following equations, we rewrite the above pose errors in the physics simulator‚Äôs world.\nùêì\nŒî\n‚Äã\nEst.\nj\n/\nGT.\nO\ni\n,\nj\n,\nGT\n=\n(\nùêì\nO\ni\n,\nj\n,\nGT\nW\ni\n,\nj\n)\n‚àí\n1\n‚Äã\nùêì\nŒî\n‚Äã\nEst.\nj\n/\nGT.\nW\ni\n,\nj\n‚Äã\nùêì\nO\ni\n,\nj\n,\nGT\nW\ni\n,\nj\n\\displaystyle\\begin{array}[]{l}{}^{O_{i,j,\\text{GT}}}\\mathbf{T}_{\\Delta\\text{Est.}_{j}/\\text{GT.}}=\\\\[6.0pt]\n\\qquad\\left({}^{W_{i,j}}\\mathbf{T}_{O_{i,j,\\text{GT}}}\\right)^{-1}\\;\\;{}^{W_{i,j}}\\mathbf{T}_{\\Delta\\text{Est.}_{j}/\\text{GT.}}\\;\\;{}^{W_{i,j}}\\mathbf{T}_{O_{i,j,\\text{GT}}}\\end{array}\n(4)\nùêì\nŒî\n‚Äã\nEst.\nj\n/\nGT.\nW\nSim.\n=\n(\nùêì\nW\nSim.\nO\ni\n,\nGT\n)\n‚àí\n1\n‚Äã\nùêì\nŒî\n‚Äã\nEst.\nj\n/\nGT.\nO\ni\n,\nGT\n‚Äã\nùêì\nW\nSim.\nO\ni\n,\nGT\n\\displaystyle\\begin{array}[]{l}{}^{W_{\\text{Sim.}}}\\mathbf{T}_{\\Delta\\text{Est.}_{j}/\\text{GT.}}=\\\\[6.0pt]\n\\qquad\\left({}^{O_{i,\\text{GT}}}\\mathbf{T}_{W_{\\text{Sim.}}}\\right)^{-1}\\;\\;{}^{O_{i,\\text{GT}}}\\mathbf{T}_{\\Delta\\text{Est.}_{j}/\\text{GT.}}\\;\\;{}^{O_{i,\\text{GT}}}\\mathbf{T}_{W_{\\text{Sim.}}}\\end{array}\n(7)\nFinally, the updated grasp plan can thereby be executed in the physics simulation.\nùêì\nPlan\ni\n,\nj\nW\nSim.\n=\nùêì\nŒî\n‚Äã\nEst.\nj\n/\nGT.\nW\nSim.\nW\nSim.\n‚Äã\nùêì\nH\ni\n,\nref.\n{}^{W_{\\text{Sim.}}}\\mathbf{T}_{\\text{Plan}_{i,j}}\\;=\\;{}^{W_{\\text{Sim.}}}\\mathbf{T}_{\\Delta\\text{Est.}_{j}/\\text{GT.}}\\;\\;^{W_{\\text{Sim.}}}\\mathbf{T}_{H_{i},\\text{ref.}}\n(8)\nwhere a gripper‚Äôs reference grasp is denoted as\nùêì\nH\ni\n,\nref.\nW\nSim.\n{}^{W_{\\text{Sim.}}}\\mathbf{T}_{H_{i},\\text{ref.}}\nand\n{\nH\n}\n\\{H\\}\nis the hand frame. The precise definition of success used in our experiments is provided in Section\nIV-B\n.\nIV\nExperimental Results\nHere, we first introduce the datasets, define the metrics, and review the pose estimators used in this study. Then, we present quantitative results and draw conclusions from them. Please see our video for qualitative results.\nIV-A\nDatasets\nThe BOP Challenge\n[\nsundermeyer2023bop\n]\nunifies several datasets for training and evaluating 6-DoF pose estimators. Each dataset includes 3D models of the objects and annotations specifying object symmetries. A dataset contains one or more scenes for training and for testing. Each scene has RGB images, camera intrinsics, and ground-truth 6-DoF poses.\nWe report experimental results on two of the more popular BOP datasets. YCB-V contains scenes of common household objects and groceries. Its RGB images have (640\n√ó\n\\times\n480) resolution. Although YCB-V has a total of 21 objects, we limit our experiments to only seven of the least challenging items, following the selection made by the authors of DOPE\n[\ntremblay2018deep\n]\n. We constrain the other estimators to this same subset for fair comparison. LM-O, also (640\n√ó\n\\times\n480), is a single scene of eight objects in a cluttered environment. The LM-O objects have more complex shapes than YCB-V, being mostly small toys and handheld tools. We exclude from trials all frames in which target objects have less than 0.5 visibility. Figure\n3\nshows the shapes of the objects relative to the grippers.\nWhile the dimensions of the objects are precisely captured by the dataset, BOP metadata do not include information on the weights or friction coefficients of objects, which are needed for our simulations. However, their physical details are straightforward to estimate. We also assume that objects are non-deformable and their densities are uniform.\nIV-B\nMetrics\nRotation error\ne\nùêë\n(\ni\n,\nj\n)\ne_{\\mathbf{R}}^{(i,j)}\nand\ntranslation error\ne\nùê≠\n(\ni\n,\nj\n)\ne_{\\mathbf{t}}^{(i,j)}\nare derived from predicted and true poses, which we define as:\nùêì\n^\nO\ni\n,\nj\nW\ni\n,\nj\n=\n[\nùêë\n^\nùê≠\n^\nùüé\n1\n]\n,\nW\ni\n,\nj\nùêì\nO\ni\n,\nj\n,\nGT\n=\n[\nùêë\nùê≠\nùüé\n1\n]\n{}^{W_{i,j}}\\hat{\\mathbf{T}}_{O_{i,j}}=\\begin{bmatrix}\\mathbf{\\hat{R}}&\\mathbf{\\hat{t}}\\\\\n\\mathbf{0}&1\\end{bmatrix},\\\\\n\\qquad^{W_{i,j}}\\mathbf{T}_{O_{i,j,\\text{GT}}}=\\begin{bmatrix}\\mathbf{R}&\\mathbf{t}\\\\\n\\mathbf{0}&1\\end{bmatrix}\n(9)\nTo avoid misrepresenting estimates for symmetric objects with ambiguous poses, computation of rotation error considers discrete and continuous symmetries, which are included in BOP metadata. For the former, BOP specifies a set\nùêí\n\\mathbf{S}\nof symmetric rotations, and for the latter, a unit-vector axis of symmetry\na\n. Rotation error in the discrete case is determined by the minimizing symmetry:\ne\nùêë\n(\ni\n,\nj\n)\n=\nmin\nS\n‚àà\nùêí\n‚Å°\narccos\n‚Å°\n(\ntrace\n‚Å°\n(\nùêë\n^\n‚Äã\nS\n‚Äã\nùêë\n‚ä∫\n)\n‚àí\n1\n2\n)\ne_{\\mathbf{R}}^{(i,j)}=\\min_{S\\in\\mathbf{S}}\\arccos\\left(\\frac{\\operatorname{trace}(\\mathbf{\\hat{R}}S\\mathbf{R}^{\\intercal})-1}{2}\\right)\n(10)\nRotation error in the continuous case is measured as deviation from the axis of symmetry:\ne\nùêë\n(\ni\n,\nj\n)\n=\narccos\n‚Å°\n(\na\n‚ä∫\n‚Äã\nùêë\n^\n‚ä∫\n‚Äã\nùêë\n‚ä∫\n‚Äã\nùêë\n^\n‚Äã\na\n)\ne_{\\mathbf{R}}^{(i,j)}=\\arccos\\big(\\textbf{a}^{\\intercal}\\mathbf{\\hat{R}}^{\\intercal}\\mathbf{R}^{\\intercal}\\mathbf{\\hat{R}}\\textbf{a}\\big)\n(11)\nMaximum Symmetry-Aware Surface Distance\n(\ne\nMSSD\ne_{\\text{MSSD}}\n)\n[\nhodan2020bop\n]\n: This measures prediction misalignment as the single greatest distance between object points in their estimated and in their true poses.\ne\nMSSD\ne_{\\text{MSSD}}\nis made ‚Äúsymmetry-aware‚Äù by selecting the symmetry that minimizes the greatest distance.\ne\nMSSD\n‚Äã\n(\nùêì\n^\n,\nùêì\n,\nùêí\n,\nùêó\n)\n=\nmin\nS\n‚àà\nùêí\n‚Äã\n(\nmax\nx\n‚àà\nùêó\n‚Äã\n‚Äñ\nùêì\n^\n‚Äã\nx\n‚àí\nùêì\n‚Äã\nS\n‚Äã\nx\n‚Äñ\n2\n)\n\\begin{array}[]{l}e_{\\text{MSSD}}\\big(\\hat{\\mathbf{T}},\\mathbf{T},\\mathbf{S},\\mathbf{X}\\big)=\\underset{S\\in\\mathbf{S}}{\\text{min}}\\big(\\underset{\\textbf{x}\\in\\mathbf{X}}{\\text{max}}\\big\\|\\hat{\\mathbf{T}}\\textbf{x}-\\mathbf{T}S\\textbf{x}\\big\\|_{2}\\big)\\end{array}\n(12)\nFor an object under consideration,\nùêí\n\\mathbf{S}\nis the set of symmetries, and\nùêó\n\\mathbf{X}\nis the set of vertices.\nMaximum Symmetry-Aware Projection Distance\n(\ne\nMSPD\ne_{\\text{MSPD}}\n)\n[\nhodan2020bop\n]\n: This metric behaves similarly to\ne\nMSSD\ne_{\\text{MSSD}}\nbut measures the single greatest distance between pixels of object points projected from predicted and ground-truth poses.\ne\nMSPD\n‚Äã\n(\nùêì\n^\n,\nùêì\n,\nùêí\n,\nùêó\n)\n=\nmin\nS\n‚àà\nùêí\n‚Äã\n(\nmax\nx\n‚àà\nùêó\n‚Äã\n‚Äñ\nœÄ\n‚Äã\n(\nùêì\n^\n‚Äã\nx\n)\n‚àí\nœÄ\n‚Äã\n(\nùêì\n‚Äã\nS\n‚Äã\nx\n)\n‚Äñ\n2\n)\n\\begin{array}[]{l}e_{\\text{MSPD}}\\big(\\hat{\\mathbf{T}},\\mathbf{T},\\mathbf{S},\\mathbf{X}\\big)=\\underset{S\\in\\mathbf{S}}{\\text{min}}\\big(\\underset{\\textbf{x}\\in\\mathbf{X}}{\\text{max}}\\big\\|\\pi(\\hat{\\mathbf{T}}\\textbf{x})-\\pi(\\mathbf{T}S\\textbf{x})\\big\\|_{2}\\big)\\end{array}\n(13)\nœÄ\n‚Äã\n(\n‚ãÖ\n)\n\\pi(\\cdot)\ndenotes projection to 2D. The intuition in both\ne\nMSSD\ne_{\\text{MSSD}}\nand\ne\nMSPD\ne_{\\text{MSPD}}\nis that we penalize the most egregious misalignment, given the most forgiving symmetry.\nAverage Distance of Distinguishable Model Points (Symmetric)\n(\nADD(-S)\n):\nThe ADD(-S) metric is still used in the literature, even as the BOP metrics above deprecate it.\nADD(-S)\nis assigned the Average Distance of Distinguishable Model Points (ADD) or Average Distance of Indistinguishable Model Points (ADI) as applicable, given an object‚Äôs symmetry. The former averages all distances between corresponding points; the latter seeks each point‚Äôs nearest neighbor without considering correspondence. The metrics currently advanced by BOP are more rigorous while still making allowances for symmetry.\nGrasping Success\n: In addition to the above metrics, we introduce a novel measure of grasping success. According to our definition, success requires the object to be within a tolerance of its ideal target location at the end of the reference grasp. Here, we set the tolerance to 5 cm, which means that the distance between the robot hand base and the centroid of the object must be within 5 cm of the target distance 15 seconds after Stage III (in Fig.\n2\n). The grasp is specified to end at a sufficient elevation with respect to the table, and any failure to grasp or hold on to the object will be counted as a failure. Unintentional grasps far from the contact points specified by the reference grasp are also likely to be considered failures, depending on the tolerance.\nIV-C\nPose Estimators\nThe estimators we have chosen form a representative set of recent works with publicly available code. We use DOPE\n2\n2\n2\nhttps://github.com/NVlabs/Deep_Object_Pose\n, NCF\n3\n3\n3\nhttps://github.com/LinHuang17/NCF-code\n, EPOS\n4\n4\n4\nhttps://github.com/thodan/epos\n, ZebraPose\n5\n5\n5\nhttps://github.com/suyz526/ZebraPose\n, and GDRNPP\n6\n6\n6\nhttps://github.com/shanice-l/gdrnpp_bop2022\nas provided, without any further training and without using GDRNPP‚Äôs refinement module. (GDRNPP is a later iteration of GDR-Net\n[\nwang2021gdr\n]\n.) In cases where authors offer several sets of weights for the same model, we use the weights that minimize rotation and translation errors on our 15 objects. Although the authors of DOPE provide weights for the YCB-V bottle of bleach, these weights do not yield any successful grasps. We therefore omit this object from DOPE‚Äôs statistics.\nTABLE I:\nAll metrics except the 90th percentile of translation error and success rates are medians.\nYCB-V\nRot. Err.\n(deg)\n‚Üì\n\\downarrow\nTrans. Err.\n(mm)\n‚Üì\n\\downarrow\n90th perc.\nTr. Err. (mm)\n‚Üì\n\\downarrow\nADD(-S)\n(mm)\n‚Üì\n\\downarrow\nMSSD\n(mm)\n‚Üì\n\\downarrow\nMSPD\n(pixels)\n‚Üì\n\\downarrow\nSuccess Rate\n(Parallel)\n‚Üë\n\\uparrow\nSuccess Rate\n(Underactuated)\n‚Üë\n\\uparrow\nDOPE\n[\ntremblay2018deep\n]\nCracker box\n4.028\n17.200\n66.040\n18.884\n24.916\n12.927\n0.525\n0.850\nSugar box\n5.327\n27.888\n66.712\n28.428\n32.286\n12.607\n0.341\n0.610\nSoup can\n10.213\n27.145\n62.565\n27.355\n33.579\n11.699\n0.096\n0.478\nMustard bottle\n26.876\n22.736\n48.676\n27.182\n42.364\n30.409\n0.009\n0.549\nGelatin box\n15.987\n25.543\n47.534\n28.078\n34.392\n20.083\n0.569\n0.667\nPotted meat can\n8.188\n16.831\n36.586\n17.647\n24.258\n11.610\n0.299\n0.727\nNCF\n[\nhuang2022ncf\n]\nCracker box\n3.313\n21.944\n42.299\n22.654\n29.981\n11.085\n0.364\n0.620\nSugar box\n2.755\n16.182\n28.834\n16.328\n19.238\n9.224\n0.795\n0.936\nSoup can\n12.687\n34.566\n51.291\n35.363\n40.744\n34.287\n0.218\n0.406\nMustard bottle\n2.121\n23.700\n33.123\n23.716\n26.427\n12.215\n0.347\n0.507\nGelatin box\n6.083\n21.466\n30.599\n21.820\n26.947\n20.161\n0.587\n0.787\nPotted meat can\n8.157\n23.106\n49.570\n24.127\n30.534\n23.174\n0.099\n0.398\nBleach cleanser\n6.013\n17.352\n44.766\n18.044\n24.876\n11.094\n0.117\n0.747\nEPOS\n[\nhodan2020epos\n]\nCracker box\n2.038\n6.102\n20.575\n6.440\n8.417\n6.665\n0.743\n1.000\nSugar box\n1.347\n7.784\n18.294\n8.037\n10.436\n4.924\n0.909\n0.992\nSoup can\n4.279\n8.772\n36.759\n10.306\n13.197\n7.152\n0.347\n0.742\nMustard bottle\n3.205\n4.236\n7.544\n5.601\n9.076\n7.255\n0.407\n0.993\nGelatin box\n1.487\n7.824\n26.228\n7.828\n8.897\n3.427\n0.920\n0.933\nPotted meat can\n1.978\n9.159\n40.335\n9.319\n11.346\n4.747\n0.663\n0.890\nBleach cleanser\n3.782\n9.955\n57.802\n11.355\n17.824\n9.921\n0.090\n0.723\nGDRNPP\n[\nliu2022gdrnpp_bop\n]\nCracker box\n2.442\n9.364\n14.388\n9.872\n13.766\n7.257\n0.850\n1.000\nSugar box\n1.396\n5.492\n10.136\n5.629\n7.338\n4.342\n1.000\n1.000\nSoup can\n2.727\n6.986\n15.101\n7.065\n8.701\n6.240\n0.535\n0.918\nMustard bottle\n2.913\n4.607\n7.651\n4.828\n7.846\n5.275\n0.907\n1.000\nGelatin box\n7.181\n5.076\n27.415\n6.137\n10.944\n7.027\n0.813\n1.000\nPotted meat can\n1.663\n4.126\n21.308\n4.254\n5.551\n3.697\n0.878\n0.928\nBleach cleanser\n2.985\n8.606\n36.355\n9.009\n12.740\n8.454\n0.273\n0.773\nZebraPose\n[\nsu2022zebrapose\n]\nCracker box\n1.551\n8.791\n13.118\n8.841\n12.126\n6.199\n0.856\n1.000\nSugar box\n1.573\n6.899\n14.527\n6.997\n9.028\n5.204\n0.925\n1.000\nSoup can\n1.967\n5.123\n16.242\n5.405\n6.855\n5.804\n0.620\n0.918\nMustard bottle\n2.863\n3.719\n11.034\n4.721\n8.415\n7.026\n0.547\n0.980\nGelatin box\n1.392\n5.296\n17.733\n5.363\n6.825\n3.261\n0.960\n1.000\nPotted meat can\n1.580\n8.278\n21.112\n8.285\n9.313\n4.990\n0.796\n0.961\nBleach cleanser\n2.862\n8.250\n55.863\n8.758\n12.473\n7.957\n0.203\n0.763\nTABLE II:\nAll metrics except the 90th percentile of translation error and success rates are medians. Note that it is not possible to grasp the egg box object using the parallel gripper, regardless of the quality of the pose estimate.\nLM-O\nRot. Err.\n(deg)\n‚Üì\n\\downarrow\nTrans. Err.\n(mm)\n‚Üì\n\\downarrow\n90th perc.\nTr. Err. (mm)\n‚Üì\n\\downarrow\nADD(-S)\n(mm)\n‚Üì\n\\downarrow\nMSSD\n(mm)\n‚Üì\n\\downarrow\nMSPD\n(pixels)\n‚Üì\n\\downarrow\nSuccess Rate\n(Parallel)\n‚Üë\n\\uparrow\nSuccess Rate\n(Underactuated)\n‚Üë\n\\uparrow\nEPOS\n[\nhodan2020epos\n]\nApe\n6.276\n24.997\n66.190\n24.702\n28.742\n6.356\n0.043\n0.389\nCan\n4.553\n20.478\n62.804\n20.981\n27.774\n6.547\n0.714\n0.794\nCat\n13.349\n30.751\n76.806\n31.622\n42.467\n8.635\n0.073\n0.452\nDrill\n3.479\n15.970\n56.227\n16.457\n21.863\n6.482\n0.528\n0.494\nDuck\n9.448\n12.004\n37.394\n13.912\n19.477\n7.341\n0.314\n0.571\nEgg box\n39.309\n82.298\n918.043\n38.115\n205.819\n87.161\n-\n0.180\nGlue\n6.508\n29.666\n88.713\n12.461\n36.163\n7.564\n0.405\n0.587\nHole-puncher\n5.316\n22.048\n45.440\n22.333\n27.814\n6.759\n0.267\n0.314\nGDRNPP\n[\nliu2022gdrnpp_bop\n]\nApe\n3.948\n9.604\n22.075\n9.819\n12.292\n4.539\n0.142\n0.821\nCan\n3.405\n11.347\n23.162\n11.931\n16.127\n5.250\n0.888\n0.949\nCat\n3.948\n13.426\n30.240\n13.675\n17.509\n4.134\n0.274\n0.847\nDrill\n3.112\n11.156\n26.569\n11.733\n16.898\n5.254\n0.590\n0.708\nDuck\n7.659\n19.038\n31.551\n19.900\n24.307\n6.008\n0.308\n0.385\nEgg box\n6.686\n51.891\n694.946\n20.611\n198.821\n84.249\n-\n0.286\nGlue\n6.214\n14.888\n45.016\n6.980\n19.166\n6.670\n0.787\n0.951\nHole-puncher\n4.431\n22.042\n39.907\n22.082\n26.724\n6.057\n0.171\n0.181\nZebraPose\n[\nsu2022zebrapose\n]\nApe\n3.613\n8.398\n18.365\n8.480\n10.533\n4.981\n0.219\n0.825\nCan\n2.958\n5.752\n12.421\n6.783\n9.723\n4.349\n0.944\n0.983\nCat\n3.520\n10.338\n24.029\n10.978\n14.901\n3.940\n0.390\n0.878\nDrill\n2.522\n9.326\n20.680\n9.546\n13.089\n4.936\n0.669\n0.792\nDuck\n7.423\n7.293\n14.117\n8.742\n13.705\n5.902\n0.596\n0.519\nEgg box\n5.199\n23.191\n1041.358\n10.242\n177.739\n81.047\n-\n0.632\nGlue\n4.112\n11.980\n32.020\n5.018\n14.751\n4.769\n0.882\n0.958\nHole-puncher\n4.271\n9.562\n21.111\n10.003\n13.803\n6.150\n0.601\n0.684\nTABLE III:\nSelect examples of areas under the curve when grasp failure rate is plotted as a function of each increasing error. Perfect performance for a given estimator, object, and gripper leaves zero area under the curve.\nParallel\nUnderactuated\nYCB-V\nAUC\nRot.Err.\n‚Üì\n\\downarrow\nAUC\nTrans. Err.\n‚Üì\n\\downarrow\nAUC\nADD(-S)\n‚Üì\n\\downarrow\nAUC\nMSSD\n‚Üì\n\\downarrow\nAUC\nRot.Err.\n‚Üì\n\\downarrow\nAUC\nTrans. Err.\n‚Üì\n\\downarrow\nAUC\nADD(-S)\n‚Üì\n\\downarrow\nAUC\nMSSD\n‚Üì\n\\downarrow\nNCF\n[\nhuang2022ncf\n]\nSugar box\n4.575\n2.823\n2.750\n2.568\n0.834\n0.242\n0.239\n0.285\nSoup can\n80.041\n69.513\n70.126\n65.826\n58.321\n33.700\n34.097\n30.760\nMustard bottle\n45.174\n41.231\n41.235\n38.018\n38.256\n20.275\n19.680\n20.770\nEPOS\n[\nhodan2020epos\n]\nSugar box\n1.829\n1.307\n1.238\n0.787\n0.009\n0.029\n0.018\n0.004\nSoup can\n48.646\n31.658\n29.760\n29.811\n6.201\n9.421\n3.689\n3.702\nMustard bottle\n38.913\n43.484\n31.722\n29.632\n0.128\n0.003\n0.003\n0.003\nGDRNPP\n[\nliu2022gdrnpp_bop\n]\nSugar box\n0\n0\n0\n0\n0\n0\n0\n0\nSoup can\n29.714\n13.573\n13.448\n13.587\n1.327\n0.516\n0.513\n0.475\nMustard bottle\n4.029\n13.178\n10.422\n10.064\n0\n0\n0\n0\nZebraPose\n[\nsu2022zebrapose\n]\nSugar box\n1.247\n0.302\n0.302\n0.296\n0\n0\n0\n0\nSoup can\n31.318\n8.788\n8.769\n9.292\n5.525\n0.560\n0.547\n0.453\nMustard bottle\n46.768\n32.963\n28.053\n26.131\n2.610\n0.017\n0.017\n0.017\nLM-O\nEPOS\n[\nhodan2020epos\n]\nCan\n19.251\n8.843\n8.184\n8.307\n13.391\n5.318\n4.468\n4.607\nDrill\n41.827\n21.772\n21.241\n21.430\n40.100\n17.977\n17.919\n17.555\nDuck\n60.396\n45.463\n45.328\n45.180\n39.468\n34.914\n36.749\n37.220\nGDRNPP\n[\nliu2022gdrnpp_bop\n]\nCan\n7.158\n2.645\n2.458\n2.792\n1.453\n0.252\n0.275\n0.202\nDrill\n34.231\n22.092\n20.166\n19.229\n26.539\n8.458\n8.167\n8.763\nDuck\n68.248\n55.686\n55.438\n56.739\n47.296\n51.029\n49.592\n47.107\nZebraPose\n[\nsu2022zebrapose\n]\nCan\n3.692\n0.386\n0.441\n0.719\n0.312\n0.014\n0.014\n0.014\nDrill\n23.268\n14.383\n14.722\n12.824\n12.965\n4.278\n4.355\n4.182\nDuck\n36.898\n23.623\n24.266\n25.465\n44.644\n45.165\n44.899\n44.917\nIV-D\nQuantitative Results\nTable\nI\nreports per-object median errors and average grasp success rates for YCB-V. We take the expected behavior observed here as validation of our study. Both grippers perform better on YCB-V than on LM-O (compare Table\nII\n), and we attribute this to the relatively simple shapes of the YCB-V objects: prismatic (three boxes), cylindrical (soup can), and ergonomic (two squeeze bottles). Though the parallel gripper lags behind the underactuated gripper, grasping success for both tends to increase as errors decrease, and the least challenging objects saturate first, namely the prisms. On these objects, both grippers can tolerate some rotation and translation error. Objects for which we measure competitive\nmedian\nerrors may nevertheless remain challenging for the parallel gripper if an estimator‚Äôs high-end (90th percentile) translation errors are large. When we observe low geometric error and low success rates, as for the parallel gripper on non-prisms, we may conclude that the gripper has become the limiting factor.\nThe general alignment between reduced error and increased grasp success becomes less reliable in the more challenging LM-O set, summarized in Table\nII\n. LM-O contains no prisms or cylinders. The parallel gripper‚Äôs performance on ‚Äúfree-form‚Äù objects such as Ape, Cat, and Duck indicates that it is not suited for these objects. Even as pose estimates improve, the concavity and curvature of these small figurines make parallel grasps highly sensitive to error.\nFigure 4:\nCumulative distribution curves for grasp\nfailure\nrate as a function of our four metrics. These curves average together all objects, for all estimators. Dashed lines are for the parallel gripper, while solid lines are for the underactuated hand. The metric with least area under its curve is the strongest predictor for grasp success. Here we see the overall superiority of the underactuated hand, the pronounced tolerance to rotation error, and the correlation between translation error and the two BOP metrics.\nFigure\n4\nplots cumulative grasp\nfailure\nrates as a function of each of our four\nincreasing\nmetrics. Each area under the curve (AUC) indicates the predictive power of that metric for that gripper. An ideal predictor‚Äôs cumulative distribution should include all the\nsuccesses\nfirst as we admit more trial results and therefore correspond to the lowest possible AUC. A meaningless predictor is essentially random and would approach a horizontal line at the average failure rate for all trials. In general, rotation error is the least informative predictor of failure.\nTable\nIII\nreports select AUCs for illustrative estimator-object pairs. As grasp success on prisms saturates, their AUCs drop to zero: when performance is perfect, there is no failure to indicate.\nAnalysis of AUCs reveals that grasp failure for the majority of our objects is determined by translation error. Since ADD(-S) and MSSD are strongly correlated with translation error, their predictive powers are similar.\nDecomposing the translation errors across all estimators and objects, we can see that at least 80% of\ne\nùê≠\n(\ni\n,\nj\n)\ne_{\\mathbf{t}}^{(i,j)}\noccurs along the viewing direction, orthogonal to the camera‚Äôs image plane. This is to be expected, given the lack of an input depth channel. Rotation seems especially insignificant for cylinders, which makes sense given that rotations around their axis of symmetry does not affect grasp. Ergonomic objects and the parallel gripper exhibit sensitivity to rotation. These grasps fail when closure of the parallel pincers does not align with the objects‚Äô minor axes. Recall that in the physics simulator, objects are non-deformable. In real life, ergonomic objects could be squeezed, and misaligned parallel grasps might succeed. The underactuated hand is sensitive to rotation on free-form objects.\nThe rotation errors and the arbitrariness in objects‚Äô 3D shapes lead to circumstances in which underactuated fingers slide away from stable force closure configurations, causing object ejection\n[\nbirglen2003force\n]\n.\nV\nConclusions\nIn this paper, for the first time we have attempted to measure how successful a robot hand would be in grasping objects following an open-loop policy based on pose estimates from an RGB image. Whether image-based object pose estimation is ready to support grasping depends on which gripper is used and on the shape of the target object. Our experiments with several object-pose estimators demonstrate that errors are shrinking as the estimators improve, but that a gripper unsuited to its target will become an impediment regardless of the quality of the estimate. We have seen that even poor pose estimates may be tolerated for prismatic objects, but that intricate shapes demand greater accuracy and dexterity. We conclude that a state of the art, competitive pose estimator is necessary, and that the simpler, parallel gripper may serve if the only objects to be grasped are prisms. The underactuated hand has higher tolerance for rotation errors, due to its larger working area, and can succeed where the parallel gripper fails.",
  "preview_text": "We present a framework for evaluating 6-DoF instance-level object pose estimators, focusing on those that require a single RGB (not RGB-D) image as input. Besides gaining intuition about how accurate these estimators are, we are interested in the degree to which they can serve as the sole perception mechanism for robotic grasping. To assess this, we perform grasping trials in a physics-based simulator, using image-based pose estimates to guide a parallel gripper and an underactuated robotic hand in picking up 3D models of objects. Our experiments on a subset of the BOP (Benchmark for 6D Object Pose Estimation) dataset compare five open-source object pose estimators and provide insights that were missing from the literature.\n\nIs Image-based Object Pose Estimation Ready to Support Grasping?\nEric C. Joyce\na\n, Qianwen Zhao\na\n, Nathaniel Burgdorfer\na\n, Long Wang\na\n, Philippos Mordohai\na\na\nStevens Institute of Technology, Hoboken, NJ 07030, USA.\n{ejoyce, qzhao10, nburgdor, lwang4, pmordoha}@stevens.edu\nThis research was supported in part by NSF Grant CMMI-2138896.\nAbstract\nWe present a framework for evaluating 6-DoF instance-level object pose estimators, focusing on those that require a single RGB (not RGB-D) image as input. Besides gaining intuition about how accurate these estimators are, we are interested in the degree to which they can serve as the sole perception mechanism for robotic grasping. To assess this, we perform grasping trials in a physics-based simulator, using image-based pose estimates to guide a parallel gripper and an underactuated robotic hand in picking up 3D models of objects. Our experiments on a subset of the BOP (Benchmark for 6D Object Pose Estimation) dataset compare five open-source object pose estimators and provide insights that were missing from the literature.\nI\nIntroduction\nHow successfully can object pose estimates made from a single RGB image guide the downstream task of robotic grasping? Most robots (even advanced ones\n[\nmorgan2022comp",
  "is_relevant": false,
  "relevance_score": 1.0,
  "extracted_keywords": [
    "object pose estimation",
    "6-DoF estimation",
    "RGB image",
    "robotic grasping",
    "BOP dataset",
    "physics-based simulator"
  ],
  "one_line_summary": "Êú¨ÊñáÁ†îÁ©∂Âü∫‰∫éÂçïÂº†RGBÂõæÂÉèÁöÑÁâ©‰Ωì‰ΩçÂßø‰º∞ËÆ°Âú®Êú∫Âô®‰∫∫ÊäìÂèñ‰ªªÂä°‰∏≠ÁöÑÂèØÁî®ÊÄßÔºåÂ±û‰∫éÊú∫Âô®‰∫∫ËßÜËßâ‰∏éÊÑüÁü•È¢ÜÂüü„ÄÇ",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "2025-12-01T16:39:39Z",
  "created_at": "2026-01-09T11:31:04.497905",
  "updated_at": "2026-01-09T11:31:04.497926"
}