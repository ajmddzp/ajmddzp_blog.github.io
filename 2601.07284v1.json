{
    "id": "2601.07284v1",
    "title": "AdaMorph: Unified Motion Retargeting via Embodiment-Aware Adaptive Transformers",
    "authors": [
        "Haoyu Zhang",
        "Shibo Jin",
        "Lvsong Li",
        "Jun Li",
        "Liang Lin",
        "Xiaodong He",
        "Zecui Zeng"
    ],
    "abstract": "将人类动作重定向到异构机器人是机器人学中的一个基本挑战，这主要源于不同实体间严重的运动学和动力学差异。现有解决方案通常依赖于训练针对特定实体的模型，这种方法扩展性差且无法利用共享的运动语义。为解决这一问题，我们提出了AdaMorph——一个统一的神经重定向框架，使单一模型能够将人类动作适配到多样化的机器人形态上。我们的方法将重定向视为条件生成任务：首先将人类动作映射到与形态无关的潜在意图空间，并利用双功能提示机制来约束生成过程。不同于简单的输入拼接，我们采用自适应层归一化（AdaLN）技术，基于实体约束动态调节解码器的特征空间。此外，通过课程式训练目标强制保证物理合理性，借助积分运算确保方向与轨迹的一致性。在12种不同人形机器人上的实验结果表明，AdaMorph能有效统一异构拓扑结构的控制，对未见过的复杂动作展现出强大的零样本泛化能力，同时保留了源行为的动态本质。",
    "url": "https://arxiv.org/abs/2601.07284v1",
    "html_url": "https://arxiv.org/html/2601.07284v1",
    "html_content": "AdaMorph: Unified Motion Retargeting via Embodiment-Aware Adaptive Transformers\nHaoyu Zhang, Shibo Jin, Lvsong Li, Jun Li, Liang Lin, Xiaodong He, Zecui Zeng1\n1Corresponding author: teacher@example.com\nAbstract\nRetargeting human motion to heterogeneous robots is a fundamental challenge in robotics, primarily due to the severe kinematic and dynamic discrepancies between varying embodiments. Existing solutions typically resort to training embodiment-specific models, which scales poorly and fails to exploit shared motion semantics. To address this, we present\nAdaMorph\n, a unified neural retargeting framework that enables a single model to adapt human motion to diverse robot morphologies. Our approach treats retargeting as a conditional generation task. We map human motion into a morphology-agnostic latent intent space and utilize a dual-purpose prompting mechanism to condition the generation. Instead of simple input concatenation, we leverage\nAdaptive Layer Normalization (AdaLN)\nto dynamically modulate the decoder’s feature space based on embodiment constraints. Furthermore, we enforce physical plausibility through a curriculum-based training objective that ensures orientation and trajectory consistency via integration. Experimental results on 12 distinct humanoid robots demonstrate that AdaMorph effectively unifies control across heterogeneous topologies, exhibiting strong zero-shot generalization to unseen complex motions while preserving the dynamic essence of the source behaviors.\nI\nIntroduction\nData-driven motion retargeting has emerged as a critical technology for equipping robots with natural, human-like behaviors. By leveraging large-scale human motion datasets (e.g., AMASS\n[\n8\n]\n), robots can acquire diverse skills ranging from locomotion to gesturing. The goal is to establish a mapping function that translates a source human motion into a target robot trajectory while preserving the semantic intent.\nHowever, this task is plagued by the\ncorrespondence problem\n: humans and robots possess distinct kinematic chains, joint limits, and mass distributions. Traditional pipelines largely rely on geometric Inverse Kinematics (IK) or optimization-based approaches\n[\n4\n,\n3\n]\n. While physically accurate, these methods are computationally expensive, require meticulous manual tuning for each new robot, and often struggle with noisy input data. Recent learning-based approaches\n[\n5\n]\noffer a promising alternative but typically train separate, specialized networks for each embodiment. This one-robot-one-model paradigm ignores the shared semantic structure of motion and limits scalability.\nWe argue that a generalist retargeting system should decouple high-level\nsemantic intent\n(e.g., “walking forward”) from low-level\nmorphological execution\n. To this end, we introduce\nAdaMorph\n, a unified Transformer-based framework capable of retargeting motion to heterogeneous robots using a single shared model.\nOur approach diverges from standard methods in two key aspects. First, rather than simply appending robot descriptors to the input, we employ Adaptive Layer Normalization (AdaLN)\n[\n9\n]\n. We treat the robot embodiment as a style condition that globally modulates the normalization statistics of the motion decoder, effectively switching the generative dynamics to match the target kinematics. Second, to address the physical artifacts common in neural retargeting, we introduce a physics-compatible representation centered on base-frame velocities and enforce long-horizon consistency through differentiable integration and SO(3) projection.\nIn summary, our main contributions are as follows:\n•\nWe propose\nAdaMorph\n, a unified neural retargeting framework that decouples semantic intent from morphological execution, enabling a single policy to control heterogeneous robots.\n•\nWe introduce a\ndual-pathway embodiment prompting\nmechanism (combining token-level attention and layer-wise AdaLN modulation) to align shared motion intents with diverse robot kinematic manifolds.\n•\nWe design a\nphysics-constrained optimization\nscheme that incorporates differentiable dead-reckoning, ensuring global trajectory consistency from local velocity predictions.\n•\nWe demonstrate that our unified model achieves\ncompetitive performance\nand robust motion tracking across multiple humanoid platforms without requiring embodiment-specific retraining.\nII\nRelated Work\nII-A\nData-Driven Motion Retargeting\nTranslating motion between different morphologies is a classic problem in computer graphics and robotics. Optimization-based methods\n[\n4\n]\nminimize geometric energy functions to satisfy constraints but suffer from high computational costs. Learning-based methods treat retargeting as a supervised translation problem. approaches like\n[\n13\n,\n1\n]\nutilize recurrent neural networks for skeleton-aware retargeting. However, most existing works focus on human-to-human or homogenous transfer. In robotics, methods typically train separate policies for each robot\n[\n10\n]\n, limiting scalability. Our work unifies this process, learning a single generator for multiple robotic agents.\nII-B\nConditioning Mechanisms in Generation\nTo control generative models, various conditioning mechanisms have been proposed. Simple concatenation of condition tokens is widely used in language models. However, for continuous signal generation, feature-wise modulation often yields better results. Techniques like FiLM\n[\n11\n]\nand Adaptive Instance Normalization (AdaIN)\n[\n7\n]\nhave demonstrated success in style transfer. Recently, Diffusion Transformers (DiT)\n[\n9\n]\nintroduced AdaLN for class-conditional image generation. We adapt this insight to the motion domain, utilizing AdaLN to modulate motion features based on robot embodiment embeddings.\nII-C\nPhysics-Compatible Motion Representation\nStandard neural networks often output absolute positions, leading to global drift and physical inconsistencies. Phase-based representations\n[\n12\n]\nhelp synchronize cyclic motions but are complex to implement. Recently, MDME\n[\n6\n]\ndemonstrated the efficacy of using local frame velocities and projected gravity for robust motion matching on legged robots. Building on this, we adopt a base-frame kinematic representation and further explicitly enforce physical consistency by integrating predicted velocities during training, penalizing deviations in the reconstructed global trajectory.\nFigure 1\n:\nOverview of the proposed\nAdaMorph\nframework.\nThe architecture unifies cross-embodiment retargeting through a\nCanonical Base-Frame Representation\nthat standardizes human motion features into local velocities and articulation (\n𝐯\nt\n,\n𝝎\nt\n,\n𝐠\nt\n,\n𝐉\nt\n\\mathbf{v}_{t},\\bm{\\omega}_{t},\\mathbf{g}_{t},\\mathbf{J}_{t}\n).\n(Left) A\nMorphology-Agnostic Intent Encoder\nmaps these inputs, conditioned on SMPL shape parameters\n𝜷\n\\bm{\\beta}\nvia a Dynamic Human Prompt, to a shared latent intent\n𝐳\nt\n\\mathbf{z}_{t}\n.\n(Right) To bridge kinematic disparities,\nLearnable Static Robot Prompts\n(\n𝐏\nr\n\\mathbf{P}_{r}\n) drive the decoding process using\nAdaLin-Style Modulation\n, injecting embodiment-specific priors into the shared stream before\nEmbodiment-Specific Output Adapters\nproject the motion onto the target robot’s joint space.\nIII\nPreliminaries\nIII-A\nSMPL-Based Human Representation\nWe utilize the Skinned Multi-Person Linear (SMPL) model to represent human motion and morphology. The SMPL model is a vertex-based kinematic model parameterized by shape parameters\n𝜷\n\\bm{\\beta}\nand pose parameters\n𝜽\n\\bm{\\theta}\n.\nIn our framework, the shape parameters\n𝜷\n\\bm{\\beta}\nplay a crucial role in the\nDynamic Human Prompt\n(Section\nIV\n), allowing the network to explicitly perceive the source subject’s morphology (e.g., limb lengths and body proportions).\nThe pose parameters\n𝜽\n\\bm{\\theta}\n, describing the relative rotation of body joints, serve as the source for articulation features.\nIII-B\nProblem Formulation as Probabilistic Inference\nWe formulate unified motion retargeting as learning a conditional probability distribution over ground-truth robot motions\n𝐘\n(\nk\n)\n\\mathbf{Y}^{(k)}\ngiven human demonstrations and embodiment descriptors. Let\n𝒳\nh\n\\mathcal{X}_{h}\nbe the domain of human motions and\n{\nℳ\nr\n(\nk\n)\n}\nk\n=\n1\nK\n\\{\\mathcal{M}_{r}^{(k)}\\}_{k=1}^{K}\nbe the family of feasible kinematic manifolds for\nK\nK\ndistinct robot embodiments.\nThe central challenge lies in mapping the human motion sequence\n𝐗\nh\n\\mathbf{X}_{h}\nto a robot-specific motion sequence\n𝐘\n^\n(\nk\n)\n\\hat{\\mathbf{Y}}^{(k)}\n, while strictly respecting the kinematic and dynamic constraints imposed by each robot’s manifold\nℳ\nr\n(\nk\n)\n\\mathcal{M}_{r}^{(k)}\n.\nMathematically, we seek to approximate the conditional distribution:\np\nθ\n​\n(\n𝐘\n(\nk\n)\n∣\n𝐗\nh\n,\n𝜷\n,\nk\n)\np_{\\theta}(\\mathbf{Y}^{(k)}\\mid\\mathbf{X}_{h},\\bm{\\beta},k)\n(1)\nIn practice, we predict the mean of an isotropic Gaussian distribution, reducing the maximum likelihood estimation (MLE) to minimizing the\nℓ\n2\n\\ell_{2}\nreconstruction distance. This formulation allows us to learn a shared latent intent space\n𝒵\n\\mathcal{Z}\nthat is invariant across the manifold family, while the decoding process projects this intent onto specific embodiment constraints.\nIII-C\nCanonical Base-Frame Motion Representation\nTo facilitate cross-manifold transfer and eliminate global drift, we adopt a\ncanonical base-frame representation\n. We define a local coordinate frame\nℱ\nt\n\\mathcal{F}_{t}\nattached to the agent’s root (pelvis for humans, trunk for robots). For any time step\nt\nt\n, the root state comprises:\n•\nLocal Linear Velocity (\n𝐯\nt\n\\mathbf{v}_{t}\n):\nComputed via first-order finite differences in the world frame (\n𝐯\nt\nw\n​\no\n​\nr\n​\nl\n​\nd\n=\n(\n𝐩\nt\n−\n𝐩\nt\n−\n1\n)\n/\nΔ\n​\nt\n\\mathbf{v}_{t}^{world}=(\\mathbf{p}_{t}-\\mathbf{p}_{t-1})/\\Delta t\n) and expressed in the base frame via\n𝐯\nt\n=\n𝐑\nt\n⊤\n​\n𝐯\nt\nw\n​\no\n​\nr\n​\nl\n​\nd\n\\mathbf{v}_{t}=\\mathbf{R}_{t}^{\\top}\\mathbf{v}_{t}^{world}\n. We use the root orientation\n𝐑\nt\n∈\nS\n​\nO\n​\n(\n3\n)\n\\mathbf{R}_{t}\\in SO(3)\nat time\nt\nt\nto ensure consistency with causal state estimation.\n•\nLocal Angular Velocity (\nω\nt\n\\bm{\\omega}_{t}\n):\nDerived from the relative rotation\nΔ\n​\n𝐑\nt\n=\n𝐑\nt\n−\n1\n⊤\n​\n𝐑\nt\n\\Delta\\mathbf{R}_{t}=\\mathbf{R}_{t-1}^{\\top}\\mathbf{R}_{t}\nvia the logarithmic map:\n𝝎\nt\n=\nvee\n​\n(\nlog\n⁡\n(\nΔ\n​\n𝐑\nt\n)\n)\n/\nΔ\n​\nt\n\\bm{\\omega}_{t}=\\mathrm{vee}(\\log(\\Delta\\mathbf{R}_{t}))/\\Delta t\n, where\nvee\n​\n(\n⋅\n)\n\\mathrm{vee}(\\cdot)\nmaps skew-symmetric matrices in\n𝔰\n​\n𝔬\n​\n(\n3\n)\n\\mathfrak{so}(3)\nto axis-angle vectors.\n•\nProjected Gravity (\n𝐠\nt\n\\mathbf{g}_{t}\n):\nThe gravity vector\n[\n0\n,\n0\n,\n−\n1\n]\n⊤\n[0,0,-1]^{\\top}\nprojected into the local base frame:\n𝐠\nt\n=\n𝐑\nt\n⊤\n​\n[\n0\n,\n0\n,\n−\n1\n]\n⊤\n\\mathbf{g}_{t}=\\mathbf{R}_{t}^{\\top}[0,0,-1]^{\\top}\n. This feature is bounded by construction and kept unnormalized.\nThis representation disentangles local kinetics from global posture, serving as a shared interface for heterogeneous agents.\nIV\nMethodology\nWe introduce\nAdaMorph\n, a unified framework designed to disentangle semantic intent from morphological execution. As shown in Figure\n1\n, AdaMorph employs a shared intent encoder, a dual-pathway embodiment-conditioned decoder, and embodiment-specific output adapters.\nIV-A\nInput and Output Definition\nBased on the canonical representation, we formally define the input and output spaces for the unified retargeting task.\nIV-A\n1\nHuman Input Sequence\nThe input to the shared encoder, denoted as\n𝐗\nh\n∈\nℝ\nT\n×\nD\ni\n​\nn\n\\mathbf{X}_{h}\\in\\mathbb{R}^{T\\times D_{in}}\n, represents the human demonstration sequence derived from the SMPL model. For each time step\nt\nt\n, the feature vector\n𝐱\nt\n(\nh\n)\n\\mathbf{x}_{t}^{(h)}\nis formulated as:\n𝐱\nt\n(\nh\n)\n=\n[\n𝐯\nt\n,\n𝝎\nt\n,\n𝐠\nt\n,\n𝐉\nt\n]\n\\mathbf{x}_{t}^{(h)}=[\\mathbf{v}_{t},\\bm{\\omega}_{t},\\mathbf{g}_{t},\\mathbf{J}_{t}]\n(2)\nwhere\n𝐯\nt\n,\n𝝎\nt\n,\n𝐠\nt\n∈\nℝ\n3\n\\mathbf{v}_{t},\\bm{\\omega}_{t},\\mathbf{g}_{t}\\in\\mathbb{R}^{3}\ndenote the root linear velocity, angular velocity, and projected gravity in the canonical base frame.\nCrucially,\n𝐉\nt\n∈\nℝ\nJ\n×\n6\n\\mathbf{J}_{t}\\in\\mathbb{R}^{J\\times 6}\nrepresents the local articulation. It is obtained by converting the SMPL pose parameters\n𝜽\nt\n\\bm{\\theta}_{t}\n(excluding the global root orientation) into the 6D continuous rotation representation\n[\n14\n]\n.\nThis transformation avoids the discontinuity issues associated with axis-angle or Euler angle representations while preserving the full kinematic information of the human subject.\nIV-A\n2\nRobot Output Sequence\nThe output for a specific robot\nk\nk\n, denoted as\n𝐘\n^\n(\nk\n)\n∈\nℝ\nT\n×\nD\no\n​\nu\n​\nt\n(\nk\n)\n\\hat{\\mathbf{Y}}^{(k)}\\in\\mathbb{R}^{T\\times D_{out}^{(k)}}\n, is generated by the embodiment-specific adapter. The predicted state vector\n𝐲\n^\nt\n(\nk\n)\n\\hat{\\mathbf{y}}_{t}^{(k)}\nat time\nt\nt\nexplicitly disentangles the base dynamics from the articulation:\n𝐲\n^\nt\n(\nk\n)\n=\n[\n𝐯\n^\nt\n,\n𝝎\n^\nt\n,\n𝐠\n^\nt\n,\n𝐪\n^\nt\n]\n\\hat{\\mathbf{y}}_{t}^{(k)}=[\\hat{\\mathbf{v}}_{t},\\hat{\\bm{\\omega}}_{t},\\hat{\\mathbf{g}}_{t},\\hat{\\mathbf{q}}_{t}]\n(3)\n•\n𝐯\n^\nt\n,\n𝝎\n^\nt\n∈\nℝ\n3\n\\hat{\\mathbf{v}}_{t},\\hat{\\bm{\\omega}}_{t}\\in\\mathbb{R}^{3}\n: The predicted linear and angular velocities of the robot base, used for trajectory integration.\n•\n𝐠\n^\nt\n∈\nℝ\n3\n\\hat{\\mathbf{g}}_{t}\\in\\mathbb{R}^{3}\n: The predicted gravity vector, serving as a supervision signal for base orientation stability.\n•\n𝐪\n^\nt\n∈\nℝ\nN\nk\n\\hat{\\mathbf{q}}_{t}\\in\\mathbb{R}^{N_{k}}\n: The joint positions (DoF) specific to the robot’s topology, where\nN\nk\nN_{k}\nis the number of controllable motors for robot\nk\nk\n.\nIV-B\nDual-Pathway Embodiment Prompting\nTo bridge the gap between the shared intent and specific robot manifolds, we introduce a prompting mechanism that operates at two distinct structural scales.\nDynamic Human Prompt.\nAn MLP adapter maps the SMPL shape parameters\n𝜷\n\\bm{\\beta}\nto a sequence of soft tokens\n𝐏\nh\n\\mathbf{P}_{h}\n, prefixing the encoder input to enable shape-aware intent extraction.\nStatic Robot Prompt.\nFor each robot\nk\nk\n, we learn a specific prompt bank\n𝐏\nr\n(\nk\n)\n∈\nℝ\nL\np\n×\nd\nm\n​\no\n​\nd\n​\ne\n​\nl\n\\mathbf{P}_{r}^{(k)}\\in\\mathbb{R}^{L_{p}\\times d_{model}}\n. This prompt drives the decoding process through two complementary pathways:\n1. Token-Level Context via Cross-Attention:\nThe full prompt sequence acts as fine-grained memory. Within each decoder layer\nl\nl\n, we compute cross-attention using the standard formulation:\nAttn\n​\n(\n𝐐\n,\n𝐊\n,\n𝐕\n)\n=\nsoftmax\n​\n(\n𝐐𝐊\n⊤\nd\nk\n)\n​\n𝐕\n\\mathrm{Attn}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\mathrm{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{d_{k}}}\\right)\\mathbf{V}\n(4)\nwhere the query\n𝐐\n=\n𝐇\nl\n​\n𝐖\nQ\n\\mathbf{Q}=\\mathbf{H}_{l}\\mathbf{W}_{Q}\ncomes from the decoder hidden states, and keys/values are instantiated by the robot prompt:\n𝐊\n=\n𝐏\nr\n(\nk\n)\n​\n𝐖\nK\n\\mathbf{K}=\\mathbf{P}_{r}^{(k)}\\mathbf{W}_{K}\n,\n𝐕\n=\n𝐏\nr\n(\nk\n)\n​\n𝐖\nV\n\\mathbf{V}=\\mathbf{P}_{r}^{(k)}\\mathbf{W}_{V}\n. Here,\nd\nk\nd_{k}\ndenotes the attention head dimension. This mechanism allows the model to retrieve specific kinematic details from the prompt tokens.\n2. Layer-Wise Modulation via AdaLN:\nWe pool the prompt into a global vector\n𝐜\ne\n​\nm\n​\nb\n(\nk\n)\n=\nMean\n​\n(\n𝐏\nr\n(\nk\n)\n)\n\\mathbf{c}_{emb}^{(k)}=\\text{Mean}(\\mathbf{P}_{r}^{(k)})\nto drive Adaptive Layer Normalization (AdaLN). We regress\nlayer-specific\nscale\n𝜸\nl\n\\bm{\\gamma}_{l}\nand shift\n𝐛\nl\n\\mathbf{b}_{l}\nparameters from\n𝐜\ne\n​\nm\n​\nb\n(\nk\n)\n\\mathbf{c}_{emb}^{(k)}\n:\nAdaLN\nl\n​\n(\n𝐡\n,\n𝐜\ne\n​\nm\n​\nb\n)\n=\n(\n1\n+\n𝜸\nl\n​\n(\n𝐜\ne\n​\nm\n​\nb\n)\n)\n⊙\nLN\n​\n(\n𝐡\n)\n+\n𝐛\nl\n​\n(\n𝐜\ne\n​\nm\n​\nb\n)\n\\text{AdaLN}_{l}(\\mathbf{h},\\mathbf{c}_{emb})=(1+\\bm{\\gamma}_{l}(\\mathbf{c}_{emb}))\\odot\\text{LN}(\\mathbf{h})+\\mathbf{b}_{l}(\\mathbf{c}_{emb})\n(5)\nwhere\nLN\n​\n(\n⋅\n)\n\\text{LN}(\\cdot)\nis standard LayerNorm without affine parameters.\nZero-Initialization Property:\nWe initialize the\nlast linear layer\nof each modulation head to zero. Consequently, at the start of training,\n𝜸\nl\n→\n𝟎\n\\bm{\\gamma}_{l}\\to\\mathbf{0}\nand\n𝐛\nl\n→\n𝟎\n\\mathbf{b}_{l}\\to\\mathbf{0}\n, reducing AdaLN to standard\nLN\n​\n(\n𝐡\n)\n\\text{LN}(\\mathbf{h})\n. This ensures that optimization begins from a stable, shared decoder behavior before gradually specializing to distinct robot dynamics.\nIV-C\nMorphology-Agnostic Intent Encoding\nWe employ a shared Transformer encoder to extract a morphology-agnostic intent representation. The input sequence concatenates the human prompt\n𝐏\nh\n\\mathbf{P}_{h}\nand projected motion features\n𝐗\nh\n\\mathbf{X}_{h}\n. The encoder yields a latent intent sequence\n𝐙\n∈\nℝ\nT\n×\nd\nm\n​\no\n​\nd\n​\ne\n​\nl\n\\mathbf{Z}\\in\\mathbb{R}^{T\\times d_{model}}\n. We explicitly discard the prompt portion of the output to ensure\n𝐙\n\\mathbf{Z}\nrepresents the pure motion semantics rather than the conditioning context.\nIV-D\nEmbodiment-Specific Output Adapters\nTo resolve topology mismatch, we employ embodiment-specific MLP heads\nψ\nk\n\\psi_{k}\n. As defined in the output formulation, these heads project the shared latent representation\n𝐳\nt\n\\mathbf{z}_{t}\nto the robot-specific space\nℝ\n9\n+\nN\nk\n\\mathbb{R}^{9+N_{k}}\n.\nMathematically, for robot\nk\nk\n:\n𝐲\n^\nt\n(\nk\n)\n=\nψ\nk\n​\n(\nDec\n​\n(\n𝐳\nt\n,\n𝐏\nr\n(\nk\n)\n)\n)\n\\hat{\\mathbf{y}}_{t}^{(k)}=\\psi_{k}(\\text{Dec}(\\mathbf{z}_{t},\\mathbf{P}_{r}^{(k)}))\n(6)\nThis architecture isolates the dimensional variance (\nN\nk\nN_{k}\n) within the lightweight final layers, preserving a unified parameter space for the heavy transformer backbone.\nIV-E\nPhysics-Constrained Optimization\nOur training objective combines feature-space reconstruction with differentiable physical consistency constraints.\nIV-E\n1\nFeature-Space Reconstruction\nWe minimize the reconstruction error in the network’s standardized feature space:\nℒ\ni\n​\nn\n​\ns\n​\nt\n=\n1\nT\n​\n∑\nt\n=\n1\nT\n‖\n𝐲\n^\nt\n(\nk\n)\n−\n𝐲\nt\n(\nk\n)\n‖\n2\n2\n\\mathcal{L}_{inst}=\\frac{1}{T}\\sum_{t=1}^{T}\\|\\hat{\\mathbf{y}}_{t}^{(k)}-\\mathbf{y}_{t}^{(k)}\\|_{2}^{2}\n(7)\nIV-E\n2\nDifferentiable Kinematic Consistency\nTo enforce long-horizon validity, we model the trajectory reconstruction as a\ndiscrete-time kinematic system\n. We first recover the physical units from the network outputs (i.e., linear velocity\n𝐯\n^\np\n​\nh\n​\ny\n​\ns\n\\hat{\\mathbf{v}}_{phys}\nand angular velocity\n𝝎\n^\np\n​\nh\n​\ny\n​\ns\n\\hat{\\bm{\\omega}}_{phys}\n). The global state evolves according to:\n𝐑\n^\nt\n\\displaystyle\\hat{\\mathbf{R}}_{t}\n=\n𝐑\n^\nt\n−\n1\n​\nexp\n⁡\n(\n𝝎\n^\np\n​\nh\n​\ny\n​\ns\n,\nt\n​\nΔ\n​\nt\n)\n\\displaystyle=\\hat{\\mathbf{R}}_{t-1}\\exp(\\hat{\\bm{\\omega}}_{phys,t}\\Delta t)\n(8)\n𝐩\n^\nt\n\\displaystyle\\hat{\\mathbf{p}}_{t}\n=\n𝐩\n^\nt\n−\n1\n+\n𝐑\n^\nt\n​\n𝐯\n^\np\n​\nh\n​\ny\n​\ns\n,\nt\n​\nΔ\n​\nt\n\\displaystyle=\\hat{\\mathbf{p}}_{t-1}+\\hat{\\mathbf{R}}_{t}\\hat{\\mathbf{v}}_{phys,t}\\Delta t\n(9)\nwhere\n𝐑\n^\n0\n\\hat{\\mathbf{R}}_{0}\nand\n𝐩\n^\n0\n\\hat{\\mathbf{p}}_{0}\nare initialized from ground truth during training.\nOrientation Consistency.\nWe minimize the geodesic distance on\nS\n​\nO\n​\n(\n3\n)\nSO(3)\nbetween the integrated state\n𝐑\n^\nt\n\\hat{\\mathbf{R}}_{t}\nand the ground truth\n𝐑\ng\n​\nt\n,\nt\n\\mathbf{R}_{gt,t}\nusing a trace-based loss:\nℒ\nr\n​\no\n​\nt\n=\n1\nT\n​\n∑\nt\n=\n1\nT\n(\n1\n−\nTr\n​\n(\n𝐑\n^\nt\n⊤\n​\n𝐑\ng\n​\nt\n,\nt\n)\n−\n1\n2\n)\n\\mathcal{L}_{rot}=\\frac{1}{T}\\sum_{t=1}^{T}\\left(1-\\frac{\\text{Tr}(\\hat{\\mathbf{R}}_{t}^{\\top}\\mathbf{R}_{gt,t})-1}{2}\\right)\n(10)\nTo ensure geometric validity and prevent numerical drift, the integrated rotation matrices are projected back onto the\nS\n​\nO\n​\n(\n3\n)\nSO(3)\nmanifold via Gram-Schmidt orthogonalization after each integration step.\nTrajectory Consistency.\nWe match the reconstructed displacement\n𝐝\n^\nt\n=\n𝐩\n^\nt\n−\n𝐩\n^\n0\n\\hat{\\mathbf{d}}_{t}=\\hat{\\mathbf{p}}_{t}-\\hat{\\mathbf{p}}_{0}\nagainst the ground truth using a robust regression loss\nρ\n​\n(\n⋅\n)\n\\rho(\\cdot)\n:\nℒ\nt\n​\nr\n​\na\n​\nj\n=\n1\nT\n​\n∑\nt\n=\n1\nT\nρ\n​\n(\n𝐝\n^\nt\n,\n𝐩\ng\n​\nt\n,\nt\n−\n𝐩\ng\n​\nt\n,\n0\n)\n\\mathcal{L}_{traj}=\\frac{1}{T}\\sum_{t=1}^{T}\\rho(\\hat{\\mathbf{d}}_{t},\\mathbf{p}_{gt,t}-\\mathbf{p}_{gt,0})\n(11)\nIV-E\n3\nCurriculum Learning Schedule\nIntegrating predictions can lead to unstable gradients early in training. We employ a time-dependent weighting schedule:\nℒ\nt\n​\no\n​\nt\n​\na\n​\nl\n=\nℒ\ni\n​\nn\n​\ns\n​\nt\n+\nλ\n​\n(\ns\n)\n​\n(\nℒ\nr\n​\no\n​\nt\n+\nℒ\nt\n​\nr\n​\na\n​\nj\n)\n\\mathcal{L}_{total}=\\mathcal{L}_{inst}+\\lambda(s)(\\mathcal{L}_{rot}+\\mathcal{L}_{traj})\n(12)\nwhere\ns\ns\nis the global training step, and\nλ\n​\n(\ns\n)\n\\lambda(s)\nlinearly warms up from\n0\nto\nλ\nm\n​\na\n​\nx\n\\lambda_{max}\n.\nAdditionally, we employ a Teacher Forcing strategy: we linearly decay the teacher-forcing ratio\nα\n​\n(\ns\n)\n\\alpha(s)\nthroughout training, progressively replacing the ground-truth orientation\n𝐑\ng\n​\nt\n\\mathbf{R}_{gt}\nwith the integrated prediction\n𝐑\n^\n\\hat{\\mathbf{R}}\nin the velocity-rotation step.\nV\nExperiments\nTo validate the effectiveness of our unified neural retargeting framework, we conducted extensive experiments on a large-scale dataset involving heterogeneous robot embodiments. We aim to answer three primary questions: (1) Can a single unified model effectively control diverse robot morphologies? (2) Does the model capture meaningful topological semantics within its learned embodiment representations? (3) Does the framework generalize to out-of-distribution motion sequences?\nV-A\nData Preparation and Preprocessing\nV-A\n1\nDataset Construction\nWe construct our training dataset based on the AMASS database\n[\n8\n]\n, which provides a large-scale collection of high-quality human motion capture data. To obtain the corresponding ground-truth control signals for robotic agents, we leverage the state-of-the-art optimization-based method,\nGeneral Motion Retargeting\n[\n2\n]\n. This method allows us to solve for physically feasible kinematic mappings from SMPL human parameters to robot joint spaces.\nWe initially considered a candidate pool of 16 distinct humanoid robot models. After a rigorous filtering process based on kinematic compatibility and solvability by the retargeting optimizer, we retained 12 robots for training and evaluation, excluding 4 models where physically valid kinematic solutions could not be reliably converged upon by the retargeting optimizer.\nFigure 2\n:\nQualitative validation in the MuJoCo physics simulator.\nThe unified model successfully retargets input human motions to all 12 trained robot embodiments.\nDespite the differences in link lengths and joint configurations, the robots faithfully reproduce the source behaviors, demonstrating the efficacy of our soft-prompted unified architecture.\nV-A\n2\nTemporal Formatting and Augmentation\nTo ensure the system meets the requirements of real-time control, we standardize the temporal resolution and window size of the data:\n•\nDownsampling:\nThe original human motion data, captured at 120Hz, is downsampled by a factor of 4 to match the standard control frequency of robotic platforms (30Hz).\n•\nWindowing:\nWe slice the continuous motion into short horizon segments. We empirically select a window size of\nT\n=\n60\nT=60\nframes (approx. 2s). This horizon strikes a balance between capturing sufficient dynamic context for intent inference and maintaining low latency for real-time deployment.\nThe final processed dataset consists of approximately 30 million paired motion segments\n(\n𝐚\nh\n,\n𝐚\nr\n)\n(\\mathbf{a}_{h},\\mathbf{a}_{r})\n, covering a wide range of locomotion and manipulation behaviors.\nV-B\nImplementation Details\nArchitecture Setup.\nThe shared Transformer backbone is configured with\nd\nm\n​\no\n​\nd\n​\ne\n​\nl\n=\n768\nd_{model}=768\n, 12 layers, and 12 attention heads. The prompt length is set to\nL\np\n=\n16\nL_{p}=16\n.\nTraining Strategy.\nThe model is trained for 200k steps using the AdamW optimizer (\nl\n​\nr\n=\n5\n×\n10\n−\n5\nlr=5\\times 10^{-5}\n) with a cosine schedule and a short linear warmup. We apply a dropout rate of 0.1. To mitigate gradient interference between heterogeneous embodiments, we employ a Round-Robin sampling strategy where each training step exclusively samples a batch from a single robot type.\nData Processing.\nAll motion data is downsampled to 30Hz with a fixed past window size of\nW\n=\n60\nW=60\n. Velocities and DoF positions are standardized using dataset statistics; projected gravity and shape parameters are kept in their original scales.\nV-C\nPreliminary Results\nWe trained the model on the prepared 30M-sample dataset. We observed stable convergence of both the reconstruction and cross-modality losses, indicating that the shared latent space is capable of aligning human kinematics with diverse robot morphologies.\nTo qualitatively validate the results, we visualized the generated motions in the MuJoCo physics simulator. As shown in Figure\n2\n, the unified model successfully retargets input human motions to all 12 trained robot embodiments. Despite the differences in link lengths and joint configurations, the robots faithfully reproduce the source behaviors, demonstrating the efficacy of our soft-prompted unified architecture.\nV-D\nAnalysis of Learned Robot Representations\nBeyond qualitative retargeting performance, we investigated whether the proposed Soft-Prompted AdaLN architecture captures the underlying morphological semantics of different robots. We extracted the learnable robot prompt tokens from the trained model and conducted a comprehensive analysis of the learned latent space.\nFigure 3\n:\nVisualization of Learned Robot Representations.\n(a)\nCosine Similarity Matrix:\nThe block-diagonal structure indicates high correlation between robots with similar kinematic chains (e.g., the Unitree family), proving that the model captures topological similarities.\n(b)\nt-SNE Projection:\nThe projection of 16 learnable tokens per robot reveals semantic clustering. The model automatically groups robots by morphological similarity (e.g., G1 and H1 are proximal), forming stable identity signatures.\nTopological Awareness in Latent Space.\nFigure\n3\n(a) displays the cosine similarity matrix of the robot prompts. The matrix exhibits a clear block-diagonal structure, where robots from the same series display significantly higher similarity scores compared to morphologically distinct robots. This indicates that the AdaLN module successfully learns to map similar kinematic constraints to proximal regions in the parameter space.\nSemantic Clustering.\nThe t-SNE visualization of the prompt tokens, shown in Figure\n3\n(b), further reveals the semantic structure of the latent space. We observe two key properties:\n•\nIntra-Robot Consistency:\nThe 16 tokens for each robot form tight, well-separated clusters, confirming that the model learns a stable and unique “identity signature” for each embodiment.\n•\nInter-Robot Semantics:\nThe relative distances between clusters reflect morphological affinities. For instance, the clusters for the\nUnitree G1\nand\nUnitree H1\nfamilies are located in the same neighborhood, whereas the\nBooster\nand\nFourier\nseries form distant clusters.\nThese results demonstrate that our unified model does not simply memorize robot IDs but learns a structured, topology-aware representation space that facilitates effective knowledge transfer across diverse embodiments.\nV-E\nSemantic Consistency and Activity Analysis\nTo strictly quantify the preservation of motion semantics during the retargeting process, we evaluated the rhythmic alignment between the source human motions and the generated robot behaviors. We employed the Pearson Correlation Coefficient (PCC) as the primary metric to assess temporal dynamics across two key dimensions: Root Velocity Consistency and Whole-Body Activity Consistency.\nFigure 4\n:\nQuantitative Evaluation of Semantic Consistency.\n(a)\nRoot Velocity Consistency:\nThe PCC between the root speed profiles of the human input and robot output. High correlations across diverse embodiments indicate that the robots accurately follow the human’s movement rhythm (e.g., acceleration and deceleration).\n(b)\nWhole-Body Activity Consistency:\nThe PCC of the mean joint velocity magnitudes. The consistently high scores demonstrate that the model effectively transfers the overall energy and intensity of the motion, regardless of the robot’s specific kinematic configuration.\nRhythmic Alignment of Root Trajectories.\nFigure\n4\n(a) presents the PCC results for root velocity consistency across 12 distinct robot embodiments. We observe that the model achieves high correlation scores across the board, with the Unitree G1 and H1 series exhibiting exceptional performance (PCC\n≈\n0.95\n\\approx 0.95\n). This indicates that despite significant differences in leg length and stride capacity, the robots faithfully replicate the temporal structure of the human locomotion, synchronizing their acceleration and deceleration phases with the source input. Even for morphologically distinct platforms like the\nHightorque Hi\nand\nKuavo S45\n, the median PCC remains above 0.8, confirming the robustness of our unified policy in preserving global movement intent.\nPreservation of Motion Intensity.\nWe further analyzed the Whole-Body Activity Consistency, defined as the correlation between the aggregate joint velocity magnitudes of the human and the robot. As shown in Figure\n4\n(b), the model demonstrates remarkable consistency, with median PCC values exceeding 0.85 for all tested embodiments. Notably, the\nBooster T1 (29 DoF)\nand\nUnitree G1\nachieve near-perfect correlation scores, suggesting that the “energy” of the motion, whether dynamic sprinting or subtle idling, is accurately translated into the robot’s joint space. These results substantiate that our Soft-Prompted AdaLN architecture goes beyond static pose matching to capture the dynamic essence of human motion.\nV-F\nZero-Shot Generalization to Unseen Motion Domains\nTo assess the robustness of our learned representations, we evaluated the model’s performance on out-of-distribution motion sequences. We curated a specific hold-out test set consisting of distinct stylized ethnic dance performances (e.g., recordings from subjects such as\nStefanos Theodorou\nand\nVaso Aristeidou\n), which were strictly excluded from the training corpus. These sequences involve complex footwork, rapid directional changes, and rhythmic upper-body coordinations that differ significantly from the standard locomotion and manipulation tasks dominating the training distribution.\nFigure 5\n:\nZero-Shot Retargeting on Unseen Folk Dances.\nSnapshots of diverse robots performing complex ethnic dance movements. Although these specific motion styles and subjects were completely absent from the training data, the unified model successfully transfers the intricate footwork and posture to the robot embodiments without any fine-tuning.\nDespite the domain shift, our model successfully retargets these unseen dance motions to the target robots in a zero-shot manner. As illustrated in Figure\n5\n, the robots faithfully replicate the artistic nuances and high-frequency dynamics of the source performance. This indicates that the shared intent encoder has learned a generalized understanding of human kinematics rather than merely memorizing specific training patterns. The ability to handle such distinct motion styles without fine-tuning further validates the efficacy of the proposed disentangled representation learning framework in capturing the underlying physics of motion.\nVI\nConclusion\nWe have presented a unified neural retargeting framework that enables a single model to control heterogeneous robot embodiments. By mapping human motion into an\nImplicit Intent\nspace and adapting it via\nSoft Prompts\n, our approach effectively decouples semantic intent from morphological constraints. Experiments across 12 distinct humanoids demonstrate that our method achieves high-fidelity, real-time motion retargeting without the need for embodiment-specific training or offline optimization. Future work will focus on extending this framework to physics-based control for robust sim-to-real transfer.\nAcknowledgments\nReferences\n[1]\nK. Aberman, Y. Weng, D. Lischinski, D. Cohen-Or, and B. Chen\n(2020)\nUnpaired motion style transfer from video to animation\n.\nACM Transactions On Graphics (TOG)\n39\n(\n4\n),\npp. 64–1\n.\nCited by:\n§\nII-A\n.\n[2]\nJ. P. Araujo, Y. Ze, P. Xu, J. Wu, and C. K. Liu\n(2025)\nRetargeting matters: general motion retargeting for humanoid motion tracking\n.\narXiv preprint arXiv:2510.02252\n.\nCited by:\n§\nV-A\n1\n.\n[3]\nK. Ayusawa and E. Yoshida\n(2017)\nMotion retargeting for humanoid robots based on simultaneous morphing parameter identification and motion optimization\n.\nIEEE Transactions on Robotics\n33\n(\n6\n),\npp. 1343–1357\n.\nCited by:\n§I\n.\n[4]\nM. Gleicher\n(1998)\nRetargetting motion to new characters\n.\nIn\nProceedings of the 25th annual conference on Computer graphics and interactive techniques\n,\npp. 33–42\n.\nCited by:\n§I\n,\n§\nII-A\n.\n[5]\nA. Gupta, L. Fan, S. Ganguli, and L. Fei-Fei\n(2022)\nMetaMorph: learning universal controllers with transformers\n.\nIn\nInternational Conference on Learning Representations\n,\nExternal Links:\nLink\nCited by:\n§I\n.\n[6]\nM. Heyrman, C. Li, V. Klemm, D. Kang, S. Coros, and M. Hutter\n(2025)\nMulti-domain motion embedding: expressive real-time mimicry for legged robots\n.\narXiv preprint arXiv:2512.07673\n.\nCited by:\n§\nII-C\n.\n[7]\nX. Huang and S. Belongie\n(2017)\nArbitrary style transfer in real-time with adaptive instance normalization\n.\nIn\nProceedings of the IEEE international conference on computer vision\n,\npp. 1501–1510\n.\nCited by:\n§\nII-B\n.\n[8]\nN. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black\n(2019)\nAMASS: archive of motion capture as surface shapes\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp. 5442–5451\n.\nCited by:\n§I\n,\n§\nV-A\n1\n.\n[9]\nW. Peebles and S. Xie\n(2023)\nScalable diffusion models with transformers\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp. 4195–4205\n.\nCited by:\n§I\n,\n§\nII-B\n.\n[10]\nX. B. Peng, P. Abbeel, S. Levine, and M. Van de Panne\n(2018)\nDeepmimic: example-guided deep reinforcement learning of physics-based character skills\n.\nACM Transactions On Graphics (TOG)\n37\n(\n4\n),\npp. 1–14\n.\nCited by:\n§\nII-A\n.\n[11]\nE. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville\n(2018)\nFilm: visual reasoning with a general conditioning layer\n.\nIn\nProceedings of the AAAI conference on artificial intelligence\n,\nVol.\n32\n.\nCited by:\n§\nII-B\n.\n[12]\nS. Starke, I. Mason, and T. Komura\n(2022)\nDeepphase: periodic autoencoders for learning motion phase manifolds\n.\nACM Transactions on Graphics (ToG)\n41\n(\n4\n),\npp. 1–13\n.\nCited by:\n§\nII-C\n.\n[13]\nR. Villegas, J. Yang, D. Ceylan, and H. Lee\n(2018)\nNeural kinematic networks for unsupervised motion retargetting\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp. 8639–8648\n.\nCited by:\n§\nII-A\n.\n[14]\nY. Zhou, C. Barnes, J. Lu, J. Yang, and H. Li\n(2019)\nOn the continuity of rotation representations in neural networks\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp. 5745–5753\n.\nCited by:\n§\nIV-A\n1\n.",
    "preview_text": "Retargeting human motion to heterogeneous robots is a fundamental challenge in robotics, primarily due to the severe kinematic and dynamic discrepancies between varying embodiments. Existing solutions typically resort to training embodiment-specific models, which scales poorly and fails to exploit shared motion semantics. To address this, we present AdaMorph, a unified neural retargeting framework that enables a single model to adapt human motion to diverse robot morphologies. Our approach treats retargeting as a conditional generation task. We map human motion into a morphology-agnostic latent intent space and utilize a dual-purpose prompting mechanism to condition the generation. Instead of simple input concatenation, we leverage Adaptive Layer Normalization (AdaLN) to dynamically modulate the decoder's feature space based on embodiment constraints. Furthermore, we enforce physical plausibility through a curriculum-based training objective that ensures orientation and trajectory consistency via integration. Experimental results on 12 distinct humanoid robots demonstrate that AdaMorph effectively unifies control across heterogeneous topologies, exhibiting strong zero-shot generalization to unseen complex motions while preserving the dynamic essence of the source behaviors.\n\nAdaMorph: Unified Motion Retargeting via Embodiment-Aware Adaptive Transformers\nHaoyu Zhang, Shibo Jin, Lvsong Li, Jun Li, Liang Lin, Xiaodong He, Zecui Zeng1\n1Corresponding author: teacher@example.com\nAbstract\nRetargeting human motion to heterogeneous robots is a fundamental challenge in robotics, primarily due to the severe kinematic and dynamic discrepancies between varying embodiments. Existing solutions typically resort to training embodiment-specific models, which scales poorly and fails to exploit shared motion semantics. To address this, we present\nAdaMorph\n, a unified neural retargeting framework that enables a single model to adapt human motion to diverse robot morphologies. Our approa",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "AdaMorph是一个通过自适应变换器实现统一运动重定向的框架，能够将人类运动适应到多种机器人形态，与强化学习和全身控制相关，但未直接涉及VLA、diffusion、Flow Matching或VLM。",
    "detailed_summary": "## 论文摘要：AdaMorph: 基于具身感知自适应Transformer的统一运动重定向\n\n### 1. 研究背景和动机\n将人类运动重定向到异构机器人是机器人学的一项基础挑战，主要源于不同机器人形态之间巨大的运动学和动力学差异。现有方案通常需要为每个机器人单独训练模型，这导致扩展性差，且无法利用共享的运动语义。因此，本文旨在开发一个**统一的神经重定向框架**，使单个模型能够适应多种机器人形态，解决传统方法中“一机一模型”的局限性。\n\n### 2. 核心方法和技术创新\n本文提出了 **AdaMorph** 框架，其核心创新在于：\n- **统一架构**：将重定向视为条件生成任务，通过一个共享模型将人类运动映射到**形态无关的潜在意图空间**。\n- **双路径具身提示机制**：结合**动态人类提示**（编码源人体形态）和**静态机器人提示**（编码目标机器人形态），后者通过**交叉注意力**和**自适应层归一化（AdaLN）** 两种方式调节解码器，动态切换生成动态以匹配目标运动学。\n- **物理约束优化**：采用**基于基座坐标系的运动表示**（局部速度、重力投影），并通过**可微分运动学积分**和SO(3)投影来强制保持长期的方向和轨迹一致性，确保物理合理性。\n- **课程学习策略**：逐步引入物理一致性损失，稳定训练过程。\n\n### 3. 主要实验结果\n在包含12种不同人形机器人的大规模数据集上验证了AdaMorph的有效性：\n- **统一控制**：单个模型成功为所有12种机器人形态生成了高质量的运动，在MuJoCo仿真中实现了逼真的行为复现。\n- **语义保持**：定量评估显示，模型在**根速度一致性**和**全身活动一致性**上均保持了高皮尔逊相关系数（PCC > 0.8），表明其准确保留了源运动的节奏和强度。\n- **零样本泛化**：模型能够成功地将训练中未见的复杂民族舞蹈动作**零样本**重定向到各机器人，证明了其强大的泛化能力。\n- **可解释表征**：对学习的机器人提示进行分析，发现其在潜在空间中形成了清晰的**语义聚类**，能反映机器人之间的拓扑相似性。\n\n### 4. 研究意义和价值\nAdaMorph的研究意义在于：\n- **技术价值**：提出了一种**可扩展的、统一的**运动重定向新范式，通过解耦运动语义与形态执行，显著降低了为每个新机器人定制模型的成本与复杂度。\n- **实际应用**：为实现**通用机器人技能学习**和**跨平台行为控制**提供了关键技术，使得利用丰富的人类运动数据驱动多样化机器人平台成为可能，推动了机器人自然交互与仿生运动的发展。\n- **方法论贡献**：将AdaLN等条件调制机制创新性地应用于连续运动生成领域，并结合物理约束进行优化，为后续的具身AI和机器人控制研究提供了新思路。",
    "qa_pairs": [
        {
            "question": "这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？",
            "answer": "## 论文核心分析\n\n### **核心问题**\n论文旨在解决**异构机器人运动重定向的统一控制问题**。传统方法通常为每种机器人形态训练一个专用模型，这导致**扩展性差**（每增加一个新机器人就需要重新训练）且**无法利用不同机器人之间共享的运动语义**。\n\n### **核心创新点**\n论文提出了 **AdaMorph** 框架，其创新主要体现在以下三个层面：\n\n#### **1. 统一架构与解耦思想**\n- **核心思想**：将**高层语义意图**（如“向前走”）与**低层形态执行**（如机器人具体的关节运动）进行解耦。\n- **实现方式**：设计了一个**单一共享模型**，通过一个**形态无关的潜在意图空间**来编码人类运动语义，再通过条件机制将其适配到不同机器人上。这打破了“一个机器人一个模型”的范式。\n\n#### **2. 双路径形态提示机制**\n这是技术实现的核心创新，用于将共享意图映射到具体机器人形态。\n- **动态人体提示**：将SMPL人体模型的形状参数编码为软提示，使编码器能感知源人体的形态（如肢体比例）。\n- **静态机器人提示**：为每个机器人学习一组可学习的提示令牌，通过**两条路径**影响解码过程：\n    - **令牌级上下文（交叉注意力）**：提示令牌作为细粒度记忆，通过交叉注意力为解码提供具体的运动学细节。\n    - **层间调制（自适应层归一化 - AdaLN）**：将机器人提示池化为全局向量，用于动态调节解码器每一层LayerNorm的缩放和偏移参数。这相当于将机器人形态作为一种“风格”条件，全局地调制生成动态，以匹配目标机器人的运动学约束。\n    - **零初始化特性**：调制头的最后一层线性层初始化为零，确保训练初期模型行为稳定，再逐步学习针对不同机器人的 specialization。\n\n#### **3. 物理约束优化方案**\n为了解决神经网络输出常导致的物理不合理性（如全局漂移），论文设计了物理兼容的训练目标。\n- **规范基坐标系表示**：输入和输出均采用附着在根部的局部坐标系下的表示（线速度、角速度、投影重力、关节位姿），从而解耦局部动力学与全局姿态，消除全局漂移。\n- **可微分运动学一致性损失**：\n    ```python\n    # 通过积分从预测的局部速度重建全局轨迹\n    R_hat[t] = R_hat[t-1] * exp(omega_hat[t] * dt)  # 方向积分\n    p_hat[t] = p_hat[t-1] + R_hat[t] * v_hat[t] * dt  # 位置积分\n    ```\n    - **方向一致性损失**：最小化积分得到的方向与真实方向在SO(3)流形上的测地距离。\n    - **轨迹一致性损失**：最小化积分得到的位移与真实位移的误差。\n- **课程学习策略**：逐步增加物理一致性损失的权重，并线性衰减教师强制比率，让模型逐渐学会依赖自身积分状态进行预测，提高闭环稳定性。\n\n### **解决方案总结**\n**AdaMorph** 的解决方案可以概括为：**一个统一模型 + 双路径条件调制 + 物理约束训练**。\n1.  **编码**：使用共享Transformer编码器，结合人体形态提示，将人类运动映射到形态无关的潜在意图空间。\n2.  **解码与调制**：使用共享Transformer解码器，但通过**机器人提示驱动的交叉注意力和AdaLN**进行动态调制，使同一解码器能产生适应不同机器人运动学的特征。\n3.  **输出适配**：使用轻量级的、机器人特定的MLP头，将调制后的共享特征投影到各机器人自己的关节空间。\n4.  **物理约束训练**：通过可微分积分和课程学习，强制模型输出在物理上长期一致的运动。\n\n### **实际价值**\n- **可扩展性**：一个模型即可控制多种形态迥异的机器人，极大降低了为机器人新平台部署运动技能的开发和训练成本。\n- **零样本泛化能力**：实验表明，该模型能成功地将训练中未见的复杂舞蹈动作重定向到所有机器人上，证明了其学到的运动语义具有强泛化性。\n- **物理合理性**：通过积分约束生成的轨迹全局一致，更适合直接用于需要物理仿真的控制或sim-to-real迁移。\n- **语义保持**：定量分析显示，模型能很好地保持源运动的节奏、强度和整体动态本质。",
            "timestamp": "2026-01-21T12:10:15.977234",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "基于他的前作，梳理这个方向的整个发展脉络，每一步相比于之前的工作都改进了什么，着重于几个不同的发展方向。",
            "answer": "## 论文发展脉络梳理：从特定模型到统一框架的运动重定向\n\n本文提出的 **AdaMorph** 代表了数据驱动运动重定向领域的一个重要演进节点。其核心思想是从为每个机器人训练特定模型的“专家”范式，转向使用单一模型适应多种形态的“通才”范式。以下梳理该方向的关键发展步骤、改进之处及不同发展方向。\n\n### 发展脉络与关键改进\n\n#### 1. 传统优化方法 (Pre-2010s)\n- **代表工作**： Gleicher (1998) 的几何能量最小化方法 [4]， Ayusawa & Yoshida (2017) 的同步形变参数识别与运动优化 [3]。\n- **核心思想**： 将重定向视为一个受运动学、动力学约束的优化问题，通过求解逆运动学（IK）或优化能量函数来生成机器人轨迹。\n- **改进与局限**：\n    - **改进**： 能够产生物理上精确、满足关节限位等硬约束的运动。\n    - **局限**：\n        1.  **计算昂贵**： 需要为每个运动片段进行在线优化，难以实时应用。\n        2.  **泛化性差**： 针对新机器人需要重新设计和手动调整优化目标与权重，扩展性差。\n        3.  **对噪声敏感**： 对输入的人体运动数据质量要求高。\n\n#### 2. 早期数据驱动与特定模型学习 (2010s - 早期2020s)\n- **代表工作**： DeepMimic [10]（为特定形态学习策略）， Neural Kinematic Networks [13]（用于骨架间迁移）。\n- **核心思想**： 利用神经网络（如RNN、MLP）从数据中学习从源运动到目标运动的映射函数，将重定向视为监督学习问题。\n- **改进与局限**：\n    - **改进**：\n        1.  **效率提升**： 训练后，前向推理速度快，可实现实时重定向。\n        2.  **数据驱动**： 能够从大规模运动数据（如AMASS [8]）中学习复杂、自然的运动模式。\n    - **局限**：\n        1.  **“一机一模型”范式**： 如MetaMorph [5] 所述，通常为每个机器人训练一个专用策略或网络。这忽略了不同形态间运动的**共享语义**，导致参数效率低下，无法跨平台复用知识。\n        2.  **物理不一致性**： 早期网络直接输出绝对关节位置，容易产生脚滑、全局轨迹漂移等物理不合理现象。\n\n#### 3. 引入物理兼容表示与循环一致性 (中期2020s)\n- **代表工作**： DeepPhase [12]（相位流形）， MDME [6]（基于局部速度的表示）。\n- **核心思想**： 设计更符合物理规律的中间表示，作为网络的输入和输出，以隐式或显式地保证生成运动的物理合理性。\n- **改进**：\n    1.  **解决漂移问题**： 采用**局部坐标系下的速度**（线速度、角速度）而非全局位置作为核心表示。通过积分重建轨迹，能有效避免累积误差导致的全局漂移。\n    2.  **增强运动质量**： 引入**投影重力**等特征，为网络提供朝向稳定性的监督信号。\n    3.  **改善周期性运动**： 如DeepPhase学习运动的相位流形，有助于生成节奏连贯的循环运动（如行走）。\n\n#### 4. 统一框架与条件生成探索 (近期)\n- **代表工作**： MetaMorph [5]（使用Transformer学习通用控制器）， 以及本文的 **AdaMorph**。\n- **核心思想**： 探索**单一模型**控制多种机器人形态的可能性。将重目标视为**条件生成**任务，其中机器人形态作为生成条件。\n- **关键改进（AdaMorph相较于前作的突出贡献）**：\n    1.  **创新的条件机制**： 超越了简单的条件向量拼接。引入了**双路提示机制**：\n        - **静态机器人提示**： 通过交叉注意力提供细粒度上下文。\n        - **自适应层归一化（AdaLN）**： 将机器人形态作为“风格”条件，动态调制解码器每一层的特征统计量（`scale` 和 `shift`），从**结构层面**改变生成动态以适应目标运动学。这是受DiT [9]启发在运动领域的成功迁移。\n    2.  **显式的物理约束优化**： 不仅使用了物理兼容的表示，更进一步在训练损失中加入了**可微分运动学一致性**约束。通过积分预测的速度来重建全局轨迹和朝向，并计算与真实值的误差（`ℒ_rot`, `ℒ_traj`），强制网络做出长期一致的速度预测。\n    3.  **彻底的意图-形态解耦**： 通过**形态无关的意图编码器**提取高级运动语义（如“向前走”的意图），再通过**形态特定的输出适配器**将其投射到具体机器人的关节空间。这种设计清晰地分离了“做什么”和“如何做”。\n    4.  **强大的零样本泛化能力**： 实验表明，单一AdaMorph模型在12种人形机器人上表现优异，并能将训练中未见的复杂舞蹈动作零样本重定向到所有机器人，证明了其学到的意图表示具有高度的泛化性和语义性。\n\n### 着重发展的几个不同方向\n\n1.  **从特定到通用（Scalability & Generalization）**：\n    - **方向**： 如何让一个模型适应尽可能多的不同机器人形态，实现“一个模型控制所有”。\n    - **演进**： `专用网络` -> `条件生成（简单拼接）` -> `条件生成（高级特征调制，如AdaMorph的AdaLN）`。\n    - **核心挑战**： 如何有效编码和注入形态信息，并让模型理解不同形态间的运动学相似性与差异性。\n\n2.  **从几何到物理（Physical Plausibility）**：\n    - **方向**： 如何确保神经网络生成的运动不仅看起来像，而且在物理上是可行、稳定、无漂移的。\n    - **演进**： `输出绝对位姿` -> `输出相位/循环特征` -> `输出局部速度并积分` -> `在训练中显式加入积分一致性损失（AdaMorph）`。\n    - **核心挑战**： 在数据驱动的灵活性与物理规则的硬约束之间取得平衡。\n\n3.  **表示学习与解耦（Representation Learning）**：\n    - **方向**： 如何学习一个良好、紧凑的中间表示，能够解耦运动的核心语义（意图）与执行的具体形态。\n    - **演进**： `端到端学习黑箱映射` -> `设计手工特征表示（如局部速度）` -> `学习形态无关的潜在意图空间（AdaMorph）`。\n    - **核心挑战**： 定义和分离“意图”，并构建能够有效提取该意图的网络架构。\n\n4.  **条件机制的设计（Conditioning Mechanism）**：\n    - **方向**： 如何将条件信息（如机器人形态、运动风格）最有效地注入生成模型。\n    - **演进**： `输入拼接` -> `特征仿射变换（FiLM [11]）` -> `自适应实例归一化（AdaIN [7]）` -> `自适应层归一化（AdaLN [9]）`。\n    - **核心挑战**： 实现条件对生成过程的**全局、结构化**影响，而非局部、浅层的影响。\n\n**总结**： AdaMorph 站在了这些发展方向交汇的前沿。它通过**AdaLN条件调制**和**双路提示**解决了通用性问题；通过**基坐标系速度表示**和**可微分积分损失**解决了物理合理性问题；通过**意图编码器与输出适配器**的分离设计推进了解耦表示学习。其工作表明，一个精心设计的统一框架，能够在保持高效率的同时，超越一系列特定模型，实现更优的泛化性能和物理一致性。",
            "timestamp": "2026-01-21T12:11:13.788706",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他的前作有哪些？使用表格仔细讲讲他的每篇前作，他和前作的区别是什么，主要改善是什么？着重于具体相比于之前文章的改动",
            "answer": "## AdaMorph 与前作的对比分析\n\n根据论文内容，AdaMorph 是一项创新性工作，旨在解决**统一**的运动重定向问题。论文在“相关工作”部分提到了几类相关研究，但并未明确指出本文作者团队的具体“前作”。因此，我将基于论文中引用的、与本文方法形成直接对比的**代表性先前工作**进行分析，以阐明 AdaMorph 的核心创新和具体改进。\n\n以下表格梳理了与 AdaMorph 最相关的几类前作，并详细对比了区别与改进：\n\n| 前作类别 / 代表性工作 | 核心方法 / 范式 | 与 AdaMorph 的主要区别 | AdaMorph 的具体改进与创新 |\n| :--- | :--- | :--- | :--- |\n| **1. 传统优化方法** <br> Gleicher (1998) [4], <br> Ayusawa & Yoshida (2017) [3] | 基于几何能量函数或同时进行形态参数识别与运动优化的迭代求解。追求物理精确性。 | - **方法本质**：基于模型和优化的数值求解。<br>- **泛化性**：每换一个机器人需重新手动调整或求解，无法泛化。<br>- **效率**：计算成本高，难以实时应用。 | - **方法本质**：数据驱动、基于学习的**前向生成模型**。<br>- **泛化性**：**单一模型**通过条件机制适应多种机器人，实现“一次训练，多处应用”。<br>- **效率**：前向推理速度快，满足实时控制需求。 |\n| **2. 早期学习式方法 (特定模型)** <br> Peng et al. (2018) [10], <br> 及其他未指名的“separate policies”方法 | 为**每个**机器人训练一个独立的策略或网络模型（如DeepMimic为每个技能训练独立策略）。 | - **模型策略**：**“一机一模型”** 范式。<br>- **可扩展性**：机器人数量增加时，训练、存储、部署成本线性增长。<br>- **知识共享**：忽略不同机器人运动语义的共享结构，无法跨形态迁移知识。 | - **模型策略**：**“多机一模型”** 的统一范式。<br>- **可扩展性**：增加新机器人只需添加轻量级的**静态提示符**和**输出适配器**，主干网络参数完全共享，扩展成本极低。<br>- **知识共享**：显式设计**形态无关的意图编码器**，学习共享的运动语义空间。 |\n| **3. 基于条件生成的通用方法** <br> **简单拼接条件** (常见于语言模型) <br> **FiLM [11], AdaIN [7]** (风格迁移) | - **简单拼接**：将条件描述符（如机器人ID）作为额外Token与输入序列拼接。<br>- **特征调制**：FiLM/AdaIN 通过对特征图进行仿射变换（缩放和偏移）来注入条件信息。 | - **条件注入方式**：<br>  1. **拼接**：条件信息仅在输入层注入，在网络深层可能被稀释。<br>  2. **FiLM/AdaIN**：通常在CNN中用于风格迁移，调制尺度较粗（常为通道级）。<br>- **物理一致性**：通常只监督瞬时输出，缺乏对长时程运动物理合理性的显式约束。 | - **条件注入方式**：提出**双通路提示机制**：<br>  1. **Token级**：通过交叉注意力，让解码器从机器人提示符中动态检索细节。<br>  2. **层级别**：引入 **Adaptive Layer Normalization (AdaLN)**，用机器人提示的全局向量为**每一层**的LayerNorm动态生成缩放和偏移参数，实现对整个解码器特征空间的**全局、细粒度调制**。<br>- **物理一致性**：设计**物理约束优化方案**，通过**可微分运动学积分**（公式8,9）和SO(3)投影，显式保证方向与轨迹的长时一致性（公式10,11），并用课程学习策略稳定训练。 |\n| **4. 运动表示方法** <br> **相位表示**[12] <br> **MDME**[6] (局部帧速度) | - **相位表示**：学习运动的周期相位流形，复杂且对非周期运动不友好。<br>- **MDME**：在足式机器人上证明了使用基座帧局部速度和投影重力的有效性。 | - **表示通用性**：相位表示较复杂；MDME主要针对足式机器人。<br>- **全局一致性处理**：MDME等利用局部表示避免了绝对位姿，但未在训练中显式集成和约束生成的轨迹。 | - **表示通用性**：系统化提出**规范基座帧表示**，统一人/机的运动描述（局部线速度、角速度、投影重力、关节位姿），彻底解耦局部动力学与全局位姿。<br>- **全局一致性处理**：不仅采用局部表示，更进一步将**可微分积分器作为网络训练的一部分**，通过`ℒ_rot`和`ℒ_traj`损失直接监督积分后的全局状态，这是确保物理合理性的**关键改进**。 |\n| **5. 最近的统一控制尝试** <br> Gupta et al. (2022) MetaMorph [5] | 使用Transformer学习通用控制器，可能涉及多形态控制。 | 论文未详细展开对比，但指出现有学习方案通常训练专用网络。推测MetaMorph可能更侧重于通过元学习或大规模数据学习通用策略，而**未必专门针对“运动重定向”中的人-机对应问题和解耦语义与形态的核心挑战**进行建模。 | - **问题聚焦**：**专门针对“运动重定向”**，核心创新在于**意图与形态执行的解耦**。<br>- **架构设计**：专门设计了**动态人体提示**（编码源人体形态）和**静态机器人提示**的双提示机制，并配合AdaLN，更精准地桥接异构形态。<br>- **评估重点**：除了性能，还深入分析了**学习到的机器人表示空间具有拓扑语义**（图3），证明了模型理解了形态相似性，而不仅仅是记忆ID。 |\n\n### 总结：AdaMorph的核心创新与改进\n\n1.  **范式转变**：从 **“一机一模型”** 转变为 **“多机一模型”** 的统一框架，解决了可扩展性瓶颈。\n2.  **条件机制革新**：超越了简单的条件拼接或浅层特征调制，提出了**双通路（注意力+AdaLN）提示机制**，实现了对生成过程更深层、更灵活的条件控制。\n3.  **物理约束内化**：将**可微分运动学积分**作为训练目标的一部分，显式地强制长时程的物理一致性，这是相比许多仅进行瞬时监督的神经重定向方法的重大改进。\n4.  **表示与解耦**：采用**规范基座帧表示**并设计**形态无关的意图编码器**，清晰地将高级运动语义与低级形态执行分离开，这是实现统一泛化的理论基础。\n5.  **零样本泛化能力**：实验表明，该框架不仅能处理训练中见过的机器人，还能将**完全未见过运动风格**（如复杂民族舞蹈）高质量地重定向到所有机器人上，证明了其学习到的是本质的运动物理规律，而非简单的数据拟合。\n\n总之，AdaMorph 的核心贡献在于**系统性地提出并实现了一个解耦、统一、物理可信的神经运动重定向框架**，其具体技术改进体现在**条件调制方式、物理约束损失和层次化解耦架构**等多个层面，显著推进了该领域从专用模型向通用模型的发展。",
            "timestamp": "2026-01-21T12:12:10.629903",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文提出了哪些关键技术方法，请列表格具体详细说明技术细节，需要包含具体的数学原理推导，以及具体参数。",
            "answer": "好的，作为一名专业的学术论文分析助手，我将对《AdaMorph: Unified Motion Retargeting via Embodiment-Aware Adaptive Transformers》一文进行深入分析，并详细阐述其提出的关键技术方法。\n\n## 论文核心技术创新与价值分析\n\n**核心问题**：如何用一个统一的模型，将人类运动迁移到多种形态各异的机器人上，解决传统方法“一个机器人一个模型”的扩展性差、无法利用共享运动语义的问题。\n\n**核心思想**：将运动重定向解耦为**语义意图**（做什么）和**形态执行**（怎么做），通过一个共享模型学习通用意图，再根据具体机器人形态进行适配。\n\n**实际价值**：\n1.  **可扩展性**：一个模型适配多种机器人，无需为每个新机器人重新训练，极大降低了部署成本。\n2.  **泛化性**：模型能学习到运动的物理本质，对未见过的复杂运动（如舞蹈）具有零样本泛化能力。\n3.  **实时性**：基于Transformer的架构和局部速度表示，适合实时控制。\n4.  **物理一致性**：通过可微积分约束，确保生成的运动轨迹在物理上是 plausible 的，减少了抖动和漂移。\n\n---\n\n## 关键技术方法详细说明\n\n下表详细列出了论文提出的四项核心技术，并包含了具体的数学原理和参数细节。\n\n| 技术方法 | 核心目标 | 技术细节与数学原理 | 具体参数/实现 |\n| :--- | :--- | :--- | :--- |\n| **1. 规范化基坐标系运动表示** | 建立人与机器人之间统一的、无全局漂移的运动描述接口，解耦局部动力学与全局位姿。 | **数学原理**：<br>• **局部线速度** \\( \\mathbf{v}_t = \\mathbf{R}_t^\\top (\\mathbf{p}_t - \\mathbf{p}_{t-1}) / \\Delta t \\) <br>  &nbsp;&nbsp;&nbsp;&nbsp;在世界坐标系计算速度，再转换到t时刻的基坐标系（由根关节旋转矩阵 \\( \\mathbf{R}_t \\in SO(3) \\) 定义）。<br>• **局部角速度** \\( \\bm{\\omega}_t = \\mathrm{vee}(\\log(\\Delta\\mathbf{R}_t)) / \\Delta t \\) <br>  &nbsp;&nbsp;&nbsp;&nbsp;其中 \\( \\Delta\\mathbf{R}_t = \\mathbf{R}_{t-1}^\\top \\mathbf{R}_t \\)，`log` 是 \\( SO(3) \\) 到李代数 \\( \\mathfrak{so}(3) \\) 的对数映射，`vee` 将反对称矩阵映射为轴角向量。<br>• **投影重力** \\( \\mathbf{g}_t = \\mathbf{R}_t^\\top [0, 0, -1]^\\top \\) <br>  &nbsp;&nbsp;&nbsp;&nbsp;提供绝对朝向参考。<br>• **关节表征**：使用6D连续旋转表示[14]编码人体关节姿态 \\( \\mathbf{J}_t \\)，避免欧拉角或四元数的奇异性。 | **输入特征**：\\( \\mathbf{x}_t^{(h)} = [\\mathbf{v}_t, \\bm{\\omega}_t, \\mathbf{g}_t, \\mathbf{J}_t] \\in \\mathbb{R}^{D_{in}} \\) <br>**输出特征**：\\( \\hat{\\mathbf{y}}_t^{(k)} = [\\hat{\\mathbf{v}}_t, \\hat{\\bm{\\omega}}_t, \\hat{\\mathbf{g}}_t, \\hat{\\mathbf{q}}_t] \\in \\mathbb{R}^{9+N_k} \\) <br> &nbsp;&nbsp;&nbsp;&nbsp;其中 \\( N_k \\) 是机器人k的关节数。 |\n| **2. 双通路形态提示机制** | 将机器人形态信息高效、多层次地注入共享解码器，引导其从通用意图生成特定形态的运动。 | **a) 静态机器人提示**：为每个机器人k学习一组可学习的提示令牌 \\( \\mathbf{P}_r^{(k)} \\in \\mathbb{R}^{L_p \\times d_{model}} \\)。<br><br>**b) 令牌级交叉注意力**：<br>• **数学原理**：在解码器每一层，用隐藏状态 \\( \\mathbf{H}_l \\) 作为查询（Query），机器人提示 \\( \\mathbf{P}_r^{(k)} \\) 作为键（Key）和值（Value），进行注意力计算：<br> \\( \\mathrm{Attn}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\mathrm{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\right)\\mathbf{V} \\) <br> &nbsp;&nbsp;&nbsp;&nbsp;其中 \\( \\mathbf{Q}=\\mathbf{H}_l\\mathbf{W}_Q, \\mathbf{K}=\\mathbf{P}_r^{(k)}\\mathbf{W}_K, \\mathbf{V}=\\mathbf{P}_r^{(k)}\\mathbf{W}_V \\)。<br> &nbsp;&nbsp;&nbsp;&nbsp;这使得模型能从提示中检索细粒度的运动学细节。<br><br>**c) 层间自适应层归一化**：<br>• **数学原理**：将机器人提示池化为全局向量 \\( \\mathbf{c}_{emb}^{(k)} = \\text{Mean}(\\mathbf{P}_r^{(k)}) \\)，用于预测每层AdaLN的缩放因子 \\( \\bm{\\gamma}_l \\) 和偏移量 \\( \\mathbf{b}_l \\)：<br> \\( \\text{AdaLN}_{l}(\\mathbf{h}, \\mathbf{c}_{emb}) = (1 + \\bm{\\gamma}_l(\\mathbf{c}_{emb})) \\odot \\text{LN}(\\mathbf{h}) + \\mathbf{b}_l(\\mathbf{c}_{emb}) \\) <br> &nbsp;&nbsp;&nbsp;&nbsp;其中 \\( \\text{LN}(\\cdot) \\) 是无仿射参数的标准层归一化。这相当于用形态“风格”全局调制解码器的特征空间。 | **提示长度**：\\( L_p = 16 \\) <br>**模型维度**：\\( d_{model} = 768 \\) <br>**注意力头维度**：\\( d_k = d_{model} / \\text{num\\_heads} = 768 / 12 = 64 \\) <br>**零初始化**：每个调制头（预测 \\( \\bm{\\gamma}_l, \\mathbf{b}_l \\)）的最后一层线性层权重初始化为0，确保训练初期 \\( \\text{AdaLN} \\approx \\text{LN} \\)，稳定优化。 |\n| **3. 物理约束优化方案** | 确保神经网络预测的局部速度在积分后能得到物理上一致的全局轨迹，避免累积误差和漂移。 | **a) 可微运动学积分**：<br>• **旋转积分**：\\( \\hat{\\mathbf{R}}_t = \\hat{\\mathbf{R}}_{t-1} \\exp(\\hat{\\bm{\\omega}}_{phys, t} \\Delta t) \\) <br> &nbsp;&nbsp;&nbsp;&nbsp;其中 `exp` 是 \\( \\mathfrak{so}(3) \\) 到 \\( SO(3) \\) 的指数映射。<br>• **位置积分**：\\( \\hat{\\mathbf{p}}_t = \\hat{\\mathbf{p}}_{t-1} + \\hat{\\mathbf{R}}_t \\hat{\\mathbf{v}}_{phys, t} \\Delta t \\) <br><br>**b) 一致性损失函数**：<br>• **旋转损失**（测地距离）：<br> \\( \\mathcal{L}_{rot} = \\frac{1}{T} \\sum_{t=1}^{T} \\left( 1 - \\frac{\\text{Tr}(\\hat{\\mathbf{R}}_t^\\top \\mathbf{R}_{gt, t}) - 1}{2} \\right) \\) <br>• **轨迹损失**（鲁棒回归）：<br> \\( \\mathcal{L}_{traj} = \\frac{1}{T} \\sum_{t=1}^{T} \\rho(\\hat{\\mathbf{d}}_t, \\mathbf{p}_{gt, t} - \\mathbf{p}_{gt, 0}) \\) <br> &nbsp;&nbsp;&nbsp;&nbsp;其中 \\( \\hat{\\mathbf{d}}_t = \\hat{\\mathbf{p}}_t - \\hat{\\mathbf{p}}_0 \\)。<br><br>**c) 流形投影**：每一步积分后，对 \\( \\hat{\\mathbf{R}}_t \\) 进行 Gram-Schmidt 正交化，强制其停留在 \\( SO(3) \\) 流形上。 | **总损失函数**：<br> \\( \\mathcal{L}_{total} = \\mathcal{L}_{inst} + \\lambda(s)(\\mathcal{L}_{rot} + \\mathcal{L}_{traj}) \\) <br> &nbsp;&nbsp;&nbsp;&nbsp;• \\( \\mathcal{L}_{inst} \\)：瞬时特征MSE损失。<br> &nbsp;&nbsp;&nbsp;&nbsp;• \\( \\lambda(s) \\)：从0线性增加到 \\( \\lambda_{max} \\) 的课程学习权重。<br>**教师强制**：训练初期使用真实 \\( \\mathbf{R}_{gt} \\) 进行下一步积分，后期逐步用预测的 \\( \\hat",
            "timestamp": "2026-01-21T12:13:13.481702",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他使用了哪些评价指标与数据集，列表格具体讲讲他的评价指标的细节与数据集的细节",
            "answer": "## 论文评价指标与数据集分析\n\n### 一、 评价指标\n\n论文使用了定性和定量两类指标来全面评估AdaMorph框架的性能。具体指标细节如下：\n\n| 指标类别 | 具体指标 | 计算方式与细节 | 评估目的 |\n| :--- | :--- | :--- | :--- |\n| **定性评估** | **MuJoCo仿真可视化** | 在MuJoCo物理仿真器中，将生成的机器人运动序列进行可视化呈现（如图2、图5）。 | 直观验证模型能否将复杂的人类运动（包括未见过的舞蹈）成功重定向到12种不同的机器人形态上，观察运动是否自然、物理上合理。 |\n| **定量评估 (语义一致性)** | **1. 根速度一致性** | 计算**人类输入**与**机器人输出**的**根速度剖面**之间的**皮尔逊相关系数**。根速度剖面指根节点（躯干）的线速度大小随时间变化的序列。 | 评估机器人是否准确复现了人类运动的**全局节奏和动态**，例如加速、减速的时序。高相关性表明模型成功保留了运动的“意图”和节律。 |\n| | **2. 全身活动一致性** | 计算**人类**与**机器人**的**平均关节速度大小**时间序列之间的**皮尔逊相关系数**。关节速度大小反映了整体运动的“能量”或强度。 | 评估模型是否成功转移了运动的**整体强度和动态特性**，而不仅仅是静态姿势。高相关性表明运动的“活力”被准确传递。 |\n| **定量评估 (表征分析)** | **1. 提示词余弦相似度矩阵** | 计算每个机器人对应的16个可学习静态提示词`P_r`之间的平均余弦相似度，形成矩阵并可视化（图3a）。 | 分析模型是否学习到了机器人之间的**形态学相似性**。期望看到相似机器人（如同一系列）的提示词在潜在空间中更接近（块对角结构）。 |\n| | **2. 提示词t-SNE可视化** | 将所有机器人的提示词（共12×16个）用t-SNE降维到2D空间进行可视化（图3b）。 | 直观展示模型学习到的**机器人表征的聚类结构**。期望看到：1) 同一机器人的16个token紧密聚类；2) 形态相似的机器人群簇在空间中彼此靠近。 |\n| **隐含评估指标** | **训练损失** | 1. **瞬时特征重建损失 (`L_inst`)**：预测特征与真实特征的L2距离。<br>2. **旋转一致性损失 (`L_rot`)**：基于SO(3)上测地距离的旋转矩阵差异。<br>3. **轨迹一致性损失 (`L_traj`)**：积分重建的位移与真实位移的鲁棒回归损失。 | 用于模型训练和收敛性监控。`L_inst`确保局部特征匹配，`L_rot`和`L_traj`通过可微分积分强制长期物理一致性，是保证生成运动物理合理性的关键技术。 |\n\n### 二、 数据集\n\n论文构建了一个大规模、多形态的数据集用于训练和评估。\n\n| 数据集方面 | 具体细节 |\n| :--- | :--- |\n| **源数据** | **AMASS数据库**：大规模、高质量的人体运动捕捉数据集。提供了SMPL模型的形状参数`β`和姿态参数`θ`。 |\n| **目标数据生成** | **方法**：使用**基于优化的通用运动重定向方法**（引用论文[2]）为每个机器人生成配对数据。<br>**过程**：将AMASS中的人类SMPL参数作为输入，通过优化器求解，得到满足目标机器人运动学和动力学约束的、物理可行的关节轨迹作为**地面真值**。 |\n| **机器人形态** | **初始候选**：16种不同的人形机器人模型。<br>**最终使用**：**12种**。过滤掉了4种因运动学兼容性或优化器无法可靠收敛而无法生成有效地面真值的模型。 |\n| **数据预处理** | 1. **时间标准化**：<br>   - **降采样**：从120Hz降至30Hz，以匹配机器人标准控制频率。<br>   - **窗口化**：将连续运动切片为固定长度`T=60`帧（约2秒）的片段。平衡了动态上下文捕获和实时部署的低延迟需求。<br>2. **特征标准化**：速度和关节位置使用数据集统计量进行标准化；投影重力和形状参数保持原始尺度。 |\n| **数据集规模** | 最终处理后的数据集包含约 **3000万** 个配对运动片段 `(a_h, a_r)`，涵盖了广泛的运动行为（如移动、操作）。 |\n| **零样本测试集** | **构建方式**：从训练语料中**严格排除**特定的、风格化的民族舞蹈表演序列（例如来自Stefanos Theodorou和Vaso Aristeidou的录制数据）。<br>**特点**：包含复杂的步法、快速的方向变化和节奏性的上半身协调，与训练数据中主导的标准运动模式有显著差异。<br>**目的**：用于评估模型对**未见过的运动领域**的泛化能力。 |\n\n### 总结\nAdaMorph的评价体系设计全面：**定性仿真**验证了直观效果；**定量相关性分析**（PCC）从“节奏”和“能量”两个维度客观度量了语义保持能力；**表征分析**则深入揭示了模型内部如何学习并组织不同机器人的形态知识。数据集构建严谨，利用优化方法生成高质量配对数据，并通过精心设计的预处理和严格的零样本划分，为训练一个通用的、可泛化的模型奠定了坚实基础。这些指标和数据集共同有力地支撑了论文关于**单模型统一控制**、**语义保持**和**零样本泛化**的核心主张。",
            "timestamp": "2026-01-21T12:13:55.496291",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文在哪些数据集上进行了实验？主要的评估指标和性能提升是多少？",
            "answer": "## 论文实验数据集与评估分析\n\n### 一、 实验数据集\n论文在以下数据集上进行了实验：\n\n1.  **主要训练与评估数据集**：\n    - **AMASS 数据库**：大规模、高质量的人体运动捕捉数据集。论文基于此构建了用于训练和评估的配对数据。\n    - **机器人运动数据**：通过**General Motion Retargeting**（一种基于优化的方法）将AMASS中的人体运动映射到12种不同的人形机器人上，生成了对应的机器人关节空间控制信号作为**地面真值**。\n\n2.  **数据规模与处理**：\n    - **机器人数量**：最终筛选并使用了**12种**不同的人形机器人模型进行训练和评估（从16种候选模型中筛选）。\n    - **数据量**：处理后的数据集包含约**3000万**个配对运动片段 `(ah, ar)`。\n    - **预处理**：\n        - **时间分辨率**：将原始120Hz的人体运动数据下采样至**30Hz**，以匹配机器人标准控制频率。\n        - **时间窗口**：将连续运动切片为 `T=60` 帧（约2秒）的片段，以平衡动态上下文捕获和实时部署的低延迟需求。\n\n3.  **零样本泛化测试集**：\n    - **特定风格舞蹈数据**：包含**Stefanos Theodorou**和**Vaso Aristeidou**等表演者的**民族舞蹈**运动序列。这些数据在**训练过程中被严格排除**，用于测试模型对未见过的复杂运动风格（如复杂步法、快速转向、节奏性上身协调）的泛化能力。\n\n### 二、 主要评估指标\n论文采用了**定性和定量**相结合的方式进行评估：\n\n1.  **定性评估**：\n    - 在**MuJoCo物理模拟器**中可视化生成的运动，观察机器人是否能够忠实、自然地复现源人体行为（如图2、图5所示）。\n\n2.  **定量评估指标**：\n    - **皮尔逊相关系数**：用于严格量化运动语义在重定向过程中的保持程度。\n        - **根速度一致性**：源人体与生成机器人**根速度剖面**之间的PCC。评估全局运动节奏（加速、减速）的匹配度。\n        - **全身活动一致性**：源人体与生成机器人**平均关节速度大小**之间的PCC。评估整体运动能量和强度的传递效果。\n    - **学习到的机器人表示分析**：\n        - **余弦相似度矩阵**：分析不同机器人提示符（`Pr(k)`）之间的相似性，验证模型是否捕捉到了机器人之间的拓扑相似性。\n        - **t-SNE可视化**：将每个机器人的16个提示符投影到二维空间，观察其聚类情况，分析模型是否学习了结构化的、拓扑感知的表示空间。\n\n### 三、 性能表现与提升\n论文的核心性能提升在于其**统一模型**的能力，而非在单一指标上超越所有特定模型。其主要优势体现在：\n\n1.  **统一的跨形态控制**：\n    - **核心成就**：**单个AdaMorph模型**成功实现了对**12种**不同拓扑结构人形机器人的运动重定向，无需为每个机器人训练独立模型。\n    - **性能表现**：在MuJoCo模拟中，所有12种机器人都能“忠实地复现源行为”（图2），证明了框架的有效性。\n\n2.  **语义一致性保持（高相关分数）**：\n    - **根速度一致性**：PCC值普遍较高。例如，Unitree G1和H1系列达到约**0.95**；即使对于形态差异较大的平台（如Hightorque Hi, Kuavo S45），中位数PCC也保持在**0.8以上**（图4a）。这表明模型很好地保留了全局运动意图。\n    - **全身活动一致性**：所有测试机器人的中位数PCC值均**超过0.85**，Booster T1和Unitree G1接近完美相关（图4b）。证明模型准确传递了运动的“能量”和动态本质。\n\n3.  **零样本泛化能力**：\n    - 在完全未参与训练的**复杂民族舞蹈动作**上，模型能够成功进行零样本重定向（图5）。机器人复现了“艺术细微差别和高频动态”，这证明了模型学习到的是**泛化的人类运动学理解**，而非记忆特定模式。\n\n4.  **学习表示的拓扑感知性**：\n    - 余弦相似度矩阵显示出清晰的**块对角结构**，表明具有相似运动链的机器人（如Unitree家族）在潜在空间中更接近（图3a）。\n    - t-SNE可视化显示，每个机器人的16个提示符形成**紧密、分离的簇**，且簇间的距离反映了形态亲和力（图3b）。这证明模型学习了有语义的、可迁移的机器人“身份签名”。\n\n**总结**：AdaMorph的主要性能提升不在于传统意义上的“指标超越”，而在于其**范式创新**——用一个模型解决了多机器人重定向问题，同时在**语义保持**、**物理一致性**和**零样本泛化**方面都展现了强大且鲁棒的性能，为构建可扩展的通用机器人运动控制系统提供了有效方案。",
            "timestamp": "2026-01-21T12:14:32.457703",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文的主要局限性有哪些？未来可能的改进方向是什么？",
            "answer": "## 论文主要局限性分析\n\n基于对论文内容的分析，AdaMorph框架虽然取得了显著成果，但仍存在以下主要局限性：\n\n- **数据依赖与生成质量**：模型的性能高度依赖于**大规模、高质量的配对数据**（30M样本）。这些数据通过优化方法（如General Motion Retargeting）生成，若优化器在某些复杂运动或机器人形态上无法收敛，则会导致数据缺失或质量下降（如文中提到的4个机器人被排除）。这限制了模型在数据稀缺或形态极端异构场景下的应用。\n\n- **物理仿真的简化**：论文主要在**MuJoCo仿真环境**中进行定性验证。虽然引入了基于速度积分的运动学一致性约束，但并未完全集成**刚体动力学（如接触力、摩擦、质量分布）** 或**闭环反馈控制**。这可能导致生成的动作在真实物理世界中面临稳定性挑战，例如在快速动态运动或不平坦地形上。\n\n- **泛化能力的边界**：尽管模型展示了优秀的零样本泛化能力（如对未见过民族舞蹈的迁移），但这种泛化可能仍局限于**与训练数据分布相近的“人形”机器人**。对于非人形（如多足、轮式、软体机器人）或关节自由度、运动范围差异极大的机器人，其有效性尚未验证。\n\n- **实时控制与延迟**：模型采用**固定长度时间窗口（T=60帧，约2秒）** 进行前向预测。这虽然满足了30Hz的实时控制需求，但对于需要更长时序上下文或更快速反应（如平衡恢复）的复杂任务，可能引入延迟或信息不完整性。\n\n- **提示学习的可解释性**：虽然论文分析了学习到的机器人提示（Prompt）在潜在空间中的聚类和相似性，但其**具体的语义（如具体对应哪种运动学约束）仍缺乏明确的、可解释的映射**。这给模型的调试和针对特定机器人的针对性改进带来困难。\n\n## 未来可能的改进方向\n\n针对上述局限性，论文在结论部分暗示并可以推导出以下未来研究方向：\n\n- **强化物理仿真与Sim-to-Real迁移**：论文明确指出未来工作将聚焦于**扩展至基于物理的控制以实现鲁棒的Sim-to-Real迁移**。具体可能包括：\n    - 在训练目标中引入**基于物理的奖励或约束**（如零力矩点ZMP稳定性、关节扭矩限制）。\n    - 结合**强化学习（RL）** 对生成的动作进行物理精炼，使其能适应动态扰动。\n    - 开发**在线自适应模块**，根据本体感知（如IMU、力觉）实时微调输出。\n\n- **框架扩展与更广泛的形态泛化**：\n    - 将框架推广至**非人形机器人**（如四足、双轮平衡、机械臂），验证其统一表征的普适性。\n    - 探索**更少依赖成对数据**的方法，如结合无监督学习、自监督学习或从视频中直接学习。\n\n- **模型效率与架构优化**：\n    - 研究**更高效的Transformer变体**（如线性注意力、状态空间模型）以减少计算开销，支持更长时序预测。\n    - 优化**提示机制**，使其能动态适应在线任务指令或环境变化，而不仅仅是静态的形态描述。\n\n- **可解释性与可控性提升**：\n    - 对学习到的**“语义意图”空间和“形态提示”空间进行解耦分析**，使用户能通过更直观的语义标签（如“稳健”、“敏捷”）进行控制。\n    - 探索**分层生成框架**，将高层任务规划与低层运动执行进一步分离。\n\n- **数据与评估体系完善**：\n    - 构建**更丰富、多模态的机器人运动数据集**，包含真实机器人数据而不仅仅是仿真优化数据。\n    - 建立**更全面的评估指标**，不仅评估运动相似性，还需评估能量效率、控制平滑性、物理可行性等。\n\n**总结**：AdaMorph的核心创新在于通过**统一模型和自适应调制机制**解决了跨形态运动复刻的规模化问题，但其当前阶段仍是一个**开环、运动学主导的生成模型**。未来的最大价值在于将其发展为一个**能够闭环运行、物理可信、且能泛化到更广泛机器人形态的通用运动控制基础模型**，从而真正实现“一个模型控制万千机器人”的愿景。",
            "timestamp": "2026-01-21T12:15:04.951586",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T07:39:38Z",
    "created_at": "2026-01-21T12:09:08.699558",
    "updated_at": "2026-01-21T12:09:08.699564"
}