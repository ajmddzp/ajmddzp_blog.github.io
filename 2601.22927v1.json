{
    "id": "2601.22927v1",
    "title": "Toward Fully Autonomous Driving: AI, Challenges, Opportunities, and Needs",
    "authors": [
        "Lars Ullrich",
        "Michael Buchholz",
        "Klaus Dietmayer",
        "Knut Graichen"
    ],
    "abstract": "自动驾驶技术前景广阔，但实现完全自动驾驶的转型过程，除其他因素外，还受到真实且不断变化的开放世界及其带来的挑战制约。然而，自动驾驶领域的研究表明，人工智能能够超越传统方法，处理更复杂的场景，并将自主性提升至新高度。与此同时，人工智能的应用也引发了安全性与可迁移性等新问题。为厘清人工智能给自动驾驶功能带来的挑战与机遇，我们分析了当前自动驾驶技术的发展现状，阐述了现有局限，并指出了可预见的技术可能性。在此基础上，本文结合未来发展趋势探讨了多项延伸挑战，从而立足人工智能领域的发展成果重新审视完全自动驾驶的实现路径，并系统梳理了相关技术需求及由此衍生的研究课题。",
    "url": "https://arxiv.org/abs/2601.22927v1",
    "html_url": "https://arxiv.org/html/2601.22927v1",
    "html_content": "\\NewSpotColorSpace\nPANTONE\n\\AddSpotColor\nPANTONE PANTONE3015C PANTONE\n\\SpotSpace\n3015\n\\SpotSpace\nC 1 0.3 0 0.2\n\\SetPageColorSpace\nPANTONE\nIEEE copyright notice\n© 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nPublished in\nIEEE Access\n, 29 January 2026.\nCite as:\nL. Ullrich, M. Buchholz, K. Dietmayer, and K. Graichen, \"Toward Fully Autonomous Driving: AI, Challenges, Opportunities, and Needs,\" in\nIEEE Access\n, 29 January 2026, pp. 1–26, doi: 10.1109/ACCESS.2026.3659192.\nBIBT\nE\nX:\n⬇\n@article\n{\nullrich\n2024\nadstack\n,\ntitle\n={\nToward\nFully\nAutonomous\nDriving\n:\nAI\n,\nChallenges\n,\nOpportunities\n,\nand\nNeeds\n},\nauthor\n={\nUllrich\n,\nLars\nand\nBuchholz\n,\nMichael\nand\nDietmayer\n,\nKlaus\nand\nGraichen\n,\nKnut\n},\njournal\n={\nIEEE\nAccess\n},\nyear\n={2026},\npages\n={1--26},\ndoi\n={10.1109/\nACCESS\n.2026.3659192},\npublisher\n={\nIEEE\n}\n}\n\\history\nReceived 1 December 2025, accepted 26 January 2026.\n10.1109/ACCESS.2026.3659192\n\\tfootnote\nThis research is accomplished within the project ”AUTOtech.agil” (FKZ 01IS22088Y, FKZ 01IS22088W). We acknowledge the financial support for the project by the Federal Ministry of Education and Research of Germany (BMBF).\n\\corresp\nCorresponding author: Lars Ullrich (e-mail: lars.ullrich@fau.de).\nToward Fully Autonomous Driving:\nAI, Challenges, Opportunities, and Needs\nLARS ULLRICH\n{}^{\\href https://orcid.org/0009-0001-8166-3118}\n1\nMICHAEL BUCHHOLZ\n{}^{\\href https://orcid.org/0000-0001-5973-0794}\n2\nKLAUS DIETMAYER\n{}^{\\href https://orcid.org/0000-0002-1651-014X}\n2\nand KNUT GRAICHEN\n{}^{\\href https://orcid.org/0000-0003-2865-8093}\n1\nChair of Automatic Control, Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), 91058 Erlangen, Germany\nInstitute of Measurement, Control and Microtechnology, Ulm University, 89081 Ulm, Germany\nAbstract\nEfficient scalability of automated driving (AD) is key to reducing costs, enhancing safety, conserving resources, and maximizing impact. However, research focuses on specific vehicles and context, while broad deployment requires scalability across various configurations and environments. Differences in vehicle types, sensors, actuators, but also traffic regulations, legal requirements, cultural dynamics, or even ethical paradigms demand high flexibility of data-driven developed capabilities. In this paper, we address the challenge of scalable adaptation of generic capabilities to desired systems and environments. Our concept follows a two-stage fine-tuning process. In the first stage, fine-tuning to the specific environment takes place through a country-specific reward model that serves as an interface between technological adaptations and socio-political requirements. In the second stage, vehicle-specific transfer learning facilitates system adaptation and governs the validation of design decisions. In sum, our concept offers a data-driven process that integrates both technological and socio-political aspects, enabling effective scalability across technical, legal, cultural, and ethical differences.\nAbstract\nAutomated driving (AD) is promising, but the transition to fully autonomous driving is, among other things, subject to the real, ever-changing open world and the resulting challenges. However, research in the field of AD demonstrates the ability of artificial intelligence (AI) to outperform classical approaches, handle higher complexities, and reach a new level of autonomy. At the same time, the use of AI raises further questions of safety and transferability. To identify the challenges and opportunities arising from AI concerning autonomous driving functionalities, we have analyzed the current state of AD, outlined limitations, and identified foreseeable technological possibilities. Thereby, various further challenges are examined in the context of prospective developments. In this way, this article reconsiders fully autonomous driving with respect to advancements in the field of AI and carves out the respective needs and resulting research questions.\nIndex Terms:\nAutomated driving, limitations, artificial intelligence, challenges, opportunities, fully autonomous driving, needs\n\\titlepgskip\n=-21pt\nI\nINTRODUCTION\nAutomated driving (AD) represents a well-known safety-critical application that operates in the real world environment striving for safer and more efficient mobility\n[\n1\n]\n. While several simple driver assistance systems do not require the use of artificial intelligence (AI)\n[\n2\n,\n3\n]\n, it is becoming increasingly important in highly automated vehicles\n[\n4\n,\n5\n,\n6\n,\n7\n,\n8\n]\n. This is due to the fact that increasing responsibility for operational processes in complex and changing environments leads to more demanding decision-making\n[\n9\n]\n. In particular, the inherent complexity of the task itself, the uncertainty in the predictability of other road users due to the emergent behavior, and the complexity of interactions of various local and decentralized technical systems with non-technical participants pose a tremendous challenge\n[\n10\n]\n.\nCurrently, the complexity of AD is primarily handled by a modular, service-oriented software architecture\n[\n11\n,\n12\n]\n, where AI is increasingly being used in the individual sub-modules, such as perception\n[\n13\n]\nor planning\n[\n14\n]\n. This generally enables an increase in the performance of the individual modules, but raises questions about safety\n[\n15\n]\nand explainability\n[\n16\n]\n. This field of tension between improved performance and growing concerns regarding safety and explainability is further amplified in fully monolithic, AI-only architectures, where the absence of modular validation and interpretability layers exacerbates these challenges\n[\n17\n]\n.\nAt the same time, AI methodology is increasingly evolving. In particular, new learning techniques are promising, such as zero-shot\n[\n18\n]\n,\n[\n19\n]\n, one-shot\n[\n20\n,\n21\n]\n, few-shot\n[\n22\n]\n,\n[\n23\n]\n, and meta-learning\n[\n24\n,\n25\n,\n26\n]\n. These techniques are especially relevant with respect to generalization, which is crucial for the application of AI in the real world\n[\n27\n]\n,\n[\n28\n]\n. In addition, foundation models (FMs) show tremendous performance in areas such as language\n[\n29\n,\n30\n]\nand vision\n[\n31\n,\n32\n,\n33\n]\n. Moreover, FM offer new opportunities in scenarios engineering (SE)\n[\n34\n,\n35\n]\n, but also pose new challenges\n[\n36\n,\n37\n,\n38\n]\n.\nAccordingly, it is necessary to reconsider the current state of AD with regard to the aim of fully autonomous driving. This is in particular relevant in response to existing changes in technological developments and possibilities. This paper is dedicated to this task and therefore contributes in the following respects:\n1.\nA critical examination of the current state of AD is presented, focusing on current limitations of AD towards fully autonomous driving.\n2.\nAn analysis of the necessary steps to achieve higher autonomy and advance towards fully autonomous driving is conducted, considering emerging technologies, particularly AI, and identifying potential developments.\n3.\nAn extended analysis of the challenges associated with the widespread deployment of autonomous driving is conducted, going beyond purely functional aspects. This extended analysis identifies potential countermeasures necessary for scalable deployment across various vehicle configurations and environments, while opening up new research questions.\nThe aim of the paper is to outline the current status and limitations of AD systems, highlighting the need for adaptation. While emphasizing functionality, related aspects such as scalability are also considered. The focus is on analyzing the challenges and opportunities of AI for fully autonomous driving, alongside exploring emerging needs and new perspectives. To achieve this, the methodology combines a qualitative, knowledge-driven literature review with forward-looking perspectives, incorporating novel insights and conceptual frameworks to address prospective opportunities, challenges, and emerging needs.\nThe paper is structured as follows: AD and the current limitations are described in Section\nII\n. Building on this, possibilities towards fully autonomous driving are outlined in Section\nIII\n. In this regard, prospective challenges and opportunities are identified and outlined in Section\nIV\n. Finally, Section\nV\npresents the discussion, followed by the conclusion in Section\nVI\nII\nAutomated Driving and Current Limitations\nThis section is dedicated to AD, the state of the art (SOTA) tech stack and the increasing use of AI. In addition, emerging research areas and corresponding challenges concerning fully autonomous driving are discussed.\nII-A\nModular Service-Oriented Software Architecture & AI\nThe current SOTA AD stack is\npredominantly characterized by a modular, service-oriented\nsoftware architecture and the increasing use of AI subsystems.\nThis is illustrated in more detail in the following.\nII-A\n1\nSOTA Architecture\nCurrent approaches to manage complexity are often based on a modular and service-oriented information processing chain\n[\n11\n,\n12\n,\n39\n,\n40\n,\n41\n]\n. Individual modules encapsulate functionalities and responsibilities such as perception or planning. These higher-level logically separated functions are ultimately implemented by a large number of various sub-modules or services. Figure\n1\nshows an example of the modular service-oriented AD architecture for the core functions between sensors and actuators. Perception, for instance, collects lidar, radar, and camera data as well as pre-processed vehicle state estimation data in order to subsequently generate an environment model, which may consist of free space and occupancy maps.\nFigure 1\n:\nHigh-level illustration of an exemplary modular service-oriented software architecture for AD on the sensor/actor level.\nII-A\n2\nSOTA AI Usage\nIn individual sub-modules, research on AI has been conducted in a targeted manner. In particular, publicly available datasets\n[\n42\n,\n43\n,\n44\n,\n45\n,\n46\n,\n47\n,\n48\n,\n49\n,\n50\n]\nand associated challenges make an important contribution to researching and comparing different approaches, e.g., in the field of perception\n[\n13\n]\n, in object detection\n[\n51\n,\n52\n,\n4\n]\n, and in object tracking\n[\n53\n,\n54\n,\n5\n]\n. But also in the area of planning\n[\n14\n]\n, more precisely in the area of prediction of other road users\n[\n55\n,\n56\n,\n57\n,\n58\n]\n, AI methods are increasingly applied. Especially the superior performance in terms of accuracy compared to traditional non-AI methods justifies their exploration\n[\n59\n,\n60\n,\n61\n]\n. Even though improved performance provides greater safety, the lack of validation of methods increases the risk of catastrophic consequences when humans are no longer in the loop. While the recognition of traffic signs in road traffic already works via AI systems such as\n[\n62\n,\n63\n]\nand presents the corresponding information to the driver via head-up displays\n[\n64\n]\n, the functionality does not yet decisively take control of the system but merely informs the driver\n[\n65\n]\n, who processes the information on his own responsibility.\nII-A\n3\nHurdle of AI Usage\nIf AI components are used in a highly automated or even fully autonomous vehicles, it must be ensured that either no errors occur at all time or that a monitoring component detects errors and prevents catastrophic consequences by intervening accordingly\n[\n66\n,\n67\n]\n. Consequently, ensuring safety is a key aspect in order to ultimately use AI methods without risk, thus ensuring the overarching goal of fully autonomous driving–safer traffic.\nII-A\n4\nRelated Safety Assurance\nSecuring and guaranteeing the correct functionality takes up the current trend of modular service-oriented architectures\n[\n68\n,\n69\n]\n. Taking into account hard-coded interfaces between services, it is possible to secure individual services independently\n[\n70\n,\n71\n]\n. As a result, it is possible to update and upgrade services without having to secure the entire system again\n[\n72\n,\n73\n]\n. This requires that the respective service performs according to its specifications.\nIn terms of AI safety assurance, existing challenges\n[\n74\n,\n75\n]\nneed to be addressed. As shown in\n[\n15\n]\n, the importance of this research domain has increased significantly in recent years but still poses various open questions. Due to the fact that other articles deal with the analysis and discussion of AI assurance\n[\n76\n]\n, trustworthiness\n[\n77\n]\n, and governance\n[\n78\n]\nin general, but also the AI safety assurance\n[\n79\n]\nand the explainability\n[\n80\n,\n16\n,\n81\n]\nwith respect to AD, we refer to the corresponding literature for these aspects, while focusing on the functionality. Accordingly, the relevant function-specific research areas are reviewed and the prospective challenges towards fully autonomous driving are discussed within this paper.\nII-B\nSituation Awareness for Autonomy Advancements\nWhile the AD stack and the increasing use of AI within the AD stack were discussed previously, this subsection deals with situation awareness (SA), which is of particular importance for achieving improvements in autonomy. Hereby, while SA is broadly relevant, e.g., for HMI\n[\n82\n]\nand explainable AI\n[\n83\n]\n, the following concentrates on the role of SA within the AD stack and driving functionalities, in line with the focus of this article.\nII-B\n1\nGeneral Introduction\nSA is taken up according to the most widespread definition of\n[\n84\n,\n85\n]\n, stating that SA comprises three levels: the perception of the environment (Level 1 SA), the comprehension of meaning (Level 2 SA), and the projection of the future (Level 3 SA) and forms the basis of subsequent decision-making. Consequently, in the chain of sensing, perception, planning, control, and actuation, SA represents the cognitive link between perception and planning, which is important for the progress towards fully autonomous driving by increasing the functional responsibility for operational processes. Accordingly, SA comprises a functional sub-area within the AD stack that is less well settled compared to sensing or trajectory tracking. Therefore, SA is a key area of current research\n[\n86\n,\n87\n,\n88\n]\nand exhibits considerable research questions. In line with this, research in terms of functionality is focusing on increasingly complex situations and the associated demand for greater cognition, which is in turn stimulating the increasing deployment of high-performing AI systems in actual applications, e.g.,\n[\n89\n,\n90\n,\n91\n,\n92\n,\n93\n]\n.\nDue to the fact that Level 1 SA is represented by perception, and Level 2 SA, comprehension, directly builds on Level 1 SA, SA has always been strongly linked to perception from a functional point of view. This is why SA is sometimes regarded as the further development of traditional perception to active perception, also referred to as situation-aware environment perception\n[\n94\n]\n. However, the comprehension of meaning and significance of Level 2 SA goes beyond a situation-aware environment perception and Level 3 SA, projection, even reaches profoundly into planning\n[\n95\n]\n. Indeed, the projection, also called prediction, e.g., of other road users\n[\n96\n]\nand consecutive projection of a collision likelihood\n[\n97\n]\nwithin predictive planning\n[\n98\n,\n99\n]\n, is in turn a central component of trajectory planning. As a result, it is clear that the SA is not an individual, logically separated functional module of the AD stack, but rather spans functionalities and responsibilities across the AD stack. The current status is discussed in more detail below based on the individual SA Levels.\nII-B\n2\nLevel 1 SA\nThis stage covers classic perception topics from object detection\n[\n51\n,\n52\n,\n4\n]\n, to object tracking\n[\n53\n,\n54\n,\n5\n]\nas well as the acquisition of daytime, weather conditions and data provided by the environment\n[\n95\n]\n. While appropriate hardware enables multi-modal, multi-redundant sensor systems to provide a comprehensive environmental perception\n[\n100\n]\n, current research is concerned with real-time capability and resource efficiency. Therefore, initial approaches were based on sequential perception and subsequent filtering using heuristics, such as the proximity of objects\n[\n101\n,\n102\n,\n103\n]\n, or a-priori contexts\n[\n104\n,\n105\n]\n. More recent approaches such as\n[\n94\n]\n, on the other hand, rely on a high-level situation analysis, which in turn directly influences perception processing and thus actively directs perception. In addition to situation-aware environment perception approaches\n[\n86\n,\n87\n,\n94\n,\n88\n,\n106\n,\n107\n,\n108\n,\n109\n,\n110\n,\n111\n]\n, which demonstrate an increasing fusion of Level 1 SA and Level 2 SA, new concepts are also being explored through hardware. For example, event-based neuromorphic vision\n[\n112\n]\n, which already incorporates a paradigm shift in the hardware from continuous to event-triggered perception processing and thus reduces the resource demand without requiring a situation analysis.\n(a)\nUrban driving situation with schoolchildren (red circled) intending to cross the road. The plain perception-based free space is shown from the bird’s eye view (BEV) perspective.\n(b)\nHighway driving situation with a congestion on the right lane. The plain perception-based free space is shown from the BEV perspective.\nFigure 2\n:\nComparison of different driving situations with comparably perceived free space. In case (a), the context of the situation allows the free space to be used. In situation (b), however, caution is advised. The differences result from the context. This illustrates that not all free space is equal.\nII-B\n3\nLevel 2 SA\nThis stage considers the comprehension of meaning and significance, and, thus represents the transition from perceiving to understanding the environment\n[\n95\n]\n. In this regard, comprehension builds on perception by interpreting relationships between entities in the environment, which according to\n[\n113\n]\n, is also referred to as scene modeling. Following the definitions of\n[\n113\n]\n, a scene is transferred to a situation through pattern recognition, interpretation and goal and value consideration, leading to the entirety of circumstances to be reflected in subsequent behavior and motion planning. The major challenge for autonomous driving in this context is to achieve efficient, real-time capable, effective and nuanced comprehension that is able to process the numerous edge cases in a reasonable way. In this context, capabilities according to\n[\n114\n,\n115\n]\nare limited. To illustrate the complexity, consider the scenes depicted in Figure\n2\n. In both cases, the perception-based free space is highlighted. However, it is the responsibility of Level 2 SA to discern the nuances between the two situations. In Figure\n2(b)\n, the free space is drivable, whereas in Figure\n2(a)\n, the free space is not unconditionally drivable. Level 2 SA must understand that in Figure\n2(a)\n, there is a school on the right-hand side and a school bus waiting on the left, at a time when school has just let out. This suggests that children may be crossing between parked vehicles on the right side. Consequently, the maximum allowable speed might not be appropriate. In contrast, Figure\n2(b)\npresents a situation where, according to the applicable traffic rules, the free space can reasonably be considered unconditionally drivable. Although more recent approaches such as\n[\n86\n,\n87\n,\n94\n,\n88\n]\nare beginning to address related concerns, the human driver is still superior at integrating and nuancing all relevant information. For instance, in the scenario in Figure\n2(a)\n, whether it is a weekday or a holiday can have a significant impact. In this respect, AI is a promising way to improve capabilities. However, while the above example highlights a specific situation, countless other difficult situations reflect the complexity of achieving a reliable level 2 SA.\nII-B\n4\nLevel 3 SA\nThis stage deals with the projection into the future. Due to the emergent behavior of other road users, it is not possible to make statements with certainty\n[\n95\n]\n. The goals of other road users and their respective behavioral decisions are also not known with certainty. For this reason, a variety of motion prediction approaches have been developed, as outlined in\n[\n116\n]\n. While learning-based multi-modal joint predictions, that are interaction-aware, are currently widely used\n[\n117\n,\n118\n,\n119\n]\n, other approaches are still emerging, e.g., occupancy flow\n[\n120\n,\n121\n,\n122\n,\n123\n]\n. Nevertheless, current research is particularly concerned with the appropriate integration of prediction into planning\n[\n124\n]\n. Traditionally, prediction and planning are sequential tasks. Depending on the conditioning of the predictions, the ego-vehicle behaves more or less aggressively. While predictions conditioned to the ego-plan lead to more aggressive behaviors\n[\n125\n,\n126\n]\n, plans based on predictions that are unconditioned to the ego-plan are inherently more reactive and less aggressive\n[\n127\n,\n128\n]\n. Therefore, increasing efforts are being made\n[\n129\n,\n130\n]\nto overcome the sequential prediction-planning process in favor of an enhanced bi-directional interaction between prediction and planning to achieve neither aggressive nor overly passive behaviors.\nII-B\n5\nConclusion\nIt can be seen that the perception of the environment (Level 1 SA) can be improved by means of a high-level situation analysis (Level 2 SA, Level 3 SA). At the same time, it is evident that the comprehension of meaning (Level 2 SA) is largely based on the perception of the environment (Level 1 SA) and can benefit from a projection of the future (Level 3 SA). The projection of the future (Level 3 SA) is in turn highly dependent on a comprehensive and nuanced understanding of the situation (Level 2 SA) and thus indirectly on the perception (Level 1 SA). Furthermore, it can be recognized that the decision-making in terms of trajectory planning is also increasingly bi-directionally affected by the prediction of other road users (Level 3 SA). Overall, it is therefore apparent that the individual levels of SA and, with regard to the AD stack, the individual modules and functionalities have recognizable dependencies in the pursuit of the desired autonomy.\nHowever, a demanding consequence of these observations and developments is the increasing circularity, which is also the main concern of Endsley’s SA model\n[\n131\n]\n. This in turn means that with increasing bi-directionality and interdependency, modular safeguarding\n[\n70\n,\n71\n]\nreaches its limits, as interfaces and passed values are permissible, but circular reasoning may lead to undesirable behavior. Beyond this, decision-making requirements also arise with increasing SA. For instance, in order to be able to react appropriately to the nuanced SA, it is necessary to move away from decision trees and rule-based \"if this, then that\" structures within decision-making in order to deal with the diversity and complexity of the real world\n[\n17\n]\n.\nTo sum up, the SA-driven limitation analysis provides indications for further developments alongside foreseeable challenges. Thereby, in particular, the theory of SA in dynamic systems\n[\n85\n]\n, illustrated in Figure\n3\n, enables an even broader perspective of analysis. In particular, with regard to individual factors such as information processing mechanisms and individual abilities, or external/environmental factors such as task complexity and system-environment interfaces. While within this subsection the SA and the individual factors, e.g., the need for flexible information processing mechanisms\n[\n132\n,\n133\n,\n39\n]\nhave been considered with respect to SOTA limitations, the following section will broaden the perspective of limitation analysis towards external factors.\nFigure 3\n:\nEndsley’s model of situation awareness in dynamic decision making according to\n[\n85\n]\n.\nII-C\nIncreased Interconnectivity Mitigates Complexity\nOne way to address the complexity of SA in dynamic decision-making, Figure\n3\n, and autonomous driving in general, is offered by increasing interconnectivity and collaboration\n[\n134\n,\n135\n]\n. Joint fused environment models\n[\n136\n,\n137\n,\n138\n]\nenable information to be extracted from areas that are not individually visible. In this context, mobile-edge computing\n[\n139\n]\noffers the possibility to efficiently create infrastructure environment models, that can complement the vehicle’s perception to extend the vehicle’s field of view (FOV) and improve the resulting behavior, as demonstrated in\n[\n140\n]\n. Furthermore, while traffic surveillance\n[\n141\n]\nand traffic analysis\n[\n142\n]\ncould be a preliminary stage in this process, but also for the uncertainty reduction in future projection, shared goals and trajectories offer significant uncertainty and complexity reduction opportunities, thus enabling more efficient operations\n[\n143\n]\n. Decentralized cooperative behavior and trajectory planning reinforce this further\n[\n144\n,\n145\n,\n146\n]\n. At the same time, this requires decentralized computing capacity\n[\n143\n]\nand raises new challenges. Besides technical difficulties like latency, cyber security is a key aspect in this regard\n[\n147\n,\n148\n]\n. Communication with road side units as well as the closure of safety contracts with infrastructure reduce the danger of misinterpretations and cut the complexity\n[\n149\n,\n150\n,\n151\n]\n.\nAlthough these methods offer undeniable advantages, they require additional resources and the renewal of the infrastructure. Moreover, the difficulty of interpreting and predicting other road users, e.g., pedestrians in collaboratively shared spaces, remains, as a disruptive step towards purely autonomous vehicles is neither foreseeable nor targeted. In addition, independent interpretation and decision-making is an essential safety factor and a crucial fallback component in the event of malfunctions in infrastructure or decentralized services. Accordingly, while the research area of increased interconnectivity contributes to mitigate complexity, it does not provide a comprehensive solution to the task-inherent complexity. Overall, interconnectivity provides extensive information to improve SA and decision-making, but also increases the challenge of focusing on and extracting the most important information. This is another area where AI could be promising to process the multitude of external information and target it according to context-specific relevance.\nII-D\nSemantic Gap Challenges Operation\nA more general challenge within AD is the semantic gap, namely the discrepancy between implied system functionality and realized system functionality\n[\n152\n]\n, that becomes particularly visible during deployment. Identified gaps are usually eliminated by extending the functional scope. New functionalities are implemented through additional services or operating modes\n[\n153\n]\n. However, numerous corner cases and the continuous changes in the open world lead to an open long-tail distribution\n[\n154\n,\n17\n]\n, as illustrated in Figure\n4\n. This poses particular challenges for the resilience and robustness of the systems. Currently, this issue is emphasized by the increasing deployment of automated vehicles in the USA. Everyday situations, which are handled by humans, can cause particular difficulties. For instance, automated vehicles get stuck in concrete under construction\n[\n155\n]\n, hit a pedestrian in a complex situation\n[\n156\n]\n, or fail to clear the way for emergency vehicles\n[\n157\n]\n.\nSince the obstruction of emergency vehicles is a hazard to the general public, addressing this problem is of significant relevance. Functionalities such as machine listening, which recognize siren noises including the direction of origin and distinguish them from fakes, are being developed\n[\n158\n,\n159\n]\n. While such functionalities, or also dedicated intercommunication, address the emergency clearance, it does not provide a solution for other challenging situations, e.g., the concrete under construction failure\n[\n155\n]\n. Through the dedicated handling of critical use cases, automated vehicles are becoming increasingly complex systems of systems, with expertise being induced into the architecture via the dedicated solutions and respective integrations. This counteracts the flexibility required to handle the open long-tail distribution of corner cases, as the semantic gap is not generally addressed. Accordingly, the current methodology contradicts the human approach to cope with complex situations through dynamic decision-making, cognitive flexibility and adaptability to the environment and its changes\n[\n9\n,\n160\n]\n, especially at the core of the cognitive processes from perception to planning.\nDue to the fact that the necessary level of cognition has not yet been achieved, there exist approaches for resolving challenging situations by means of infrastructure or even external support, e.g., such as fallback teleoperation via control rooms\n[\n161\n]\n, to accomplish AD missions. Nevertheless, fully autonomous driving aims to address the open-world scenarios through a universal approach. In this regard, AI and the generalization of AI could provide a remedy.\nFigure 4\n:\nThe spectrum of scenarios in autonomous driving in the real world is described as an open long-tail distribution. In addition to scenarios that occur very frequently, there are a large number of rare scenarios and the challenge of novel scenarios that arise due to the characteristics of the open world.\nII-E\n(Modular) End-to-End Learning\nThe existing limitations of SOTA modular AD stacks\n[\n162\n,\n163\n,\n164\n,\n165\n,\n11\n,\n12\n,\n166\n,\n146\n]\n, e.g., in SA and cognition in general, and especially the emerging capabilities of AI motivated researching the use of AI across the overall AD Stack at once, referred to as end-to-end (E2E) AD architectures\n[\n167\n,\n168\n,\n169\n,\n170\n,\n171\n,\n172\n,\n173\n,\n174\n,\n175\n,\n176\n]\n. In this regard, the recently popular FMs, such as large language models (LLM)\n[\n177\n,\n178\n]\n, vision-language models (VLM)\n[\n179\n,\n180\n]\n, and vision-language-action models (VLA)\n[\n181\n,\n182\n]\nare also receiving more attention within AD, from vision\n[\n183\n]\nto motion planning\n[\n184\n]\nand E2E AD\n[\n185\n,\n186\n,\n187\n,\n188\n,\n189\n,\n190\n]\nin general. Overall, E2E AD architectures counteract compounded errors, objective mismatches, and information losses\n[\n191\n,\n192\n,\n128\n,\n193\n,\n194\n,\n195\n]\nof modular information processing chains and enable improved performance. Nevertheless, E2E AD stacks have significant drawbacks, e.g., the loss of interpretability, which raises concerns regarding debugging, verification, and safety guarantees. Furthermore, initial E2E AD stacks are more sensitive to distribution shifts\n[\n196\n,\n171\n,\n197\n]\n. For this reasons and the respective open research questions of AI explainability\n[\n80\n,\n16\n]\n, trustworthiness\n[\n77\n]\n, and safety assurance\n[\n79\n]\n, modular AD stacks are still predominantly deployed\n[\n198\n,\n199\n,\n200\n]\n.\nHowever, besides modular and E2E AD architectures various intermediate architectures are currently under investigation. For instance, there exist AI-driven E2E architectures that improve the interpretability, e.g., by means of intermediate auxiliary objectives or representations\n[\n127\n,\n201\n,\n128\n,\n196\n,\n202\n,\n203\n,\n204\n,\n205\n,\n176\n]\n, or even through in-cooperated prior knowledge\n[\n127\n,\n206\n,\n207\n]\n. In this regard, FMs for AD, which provide text-based scene interpretation and decision-making explanation, are also considered as interpretable E2E AD systems\n[\n185\n,\n186\n,\n187\n,\n188\n,\n189\n]\n.\nIn contrast, the modular-driven AD stacks increasingly integrate the subtasks of detection, tracking, prediction and planning, e.g., through joint optimization\n[\n208\n,\n209\n,\n210\n,\n58\n,\n193\n,\n194\n]\n, downstream metrics in-cooperation\n[\n211\n,\n212\n]\n, or even joint task coverage\n[\n192\n,\n213\n,\n214\n]\n. Since the prevailing object-driven detection, tracking, and prediction alongside SA is limited\n[\n128\n,\n215\n]\n, several researchers argue in favor of treating SA at once. In particular, by directly predicting temporal-spatial occupancy maps\n[\n216\n,\n120\n,\n57\n,\n122\n,\n123\n,\n217\n]\n. In this way, existing SA shortcomings such as limited information and uncertainty propagation or vulnerability to distribution shifts are addressed at the expense of SA interpretability. Furthermore, current research explores the growing intertwining of prediction and planning towards bidirectional frameworks\n[\n218\n,\n219\n,\n220\n,\n221\n]\n, alongside the pursuit of differentiability across AI-driven and traditional systems, e.g. from prediction to control\n[\n222\n]\n. A general overview of the various architectural approaches is presented in Figure\n5\n, which is aligned with the SA model\n[\n85\n]\n.\n[\n223\n,\n224\n]\n[\n225\n,\n226\n]\n[\n209\n,\n227\n]\n[\n228\n,\n208\n]\n,\n[\n229\n,\n230\n,\n231\n]\n[\n232\n,\n233\n]\n[\n52\n]\n,\n[\n94\n]\n,\n[\n209\n,\n106\n,\n107\n,\n108\n,\n109\n,\n110\n,\n111\n]\n[\n56\n,\n58\n,\n57\n,\n117\n]\n,\n[\n234\n,\n235\n,\n236\n,\n237\n,\n184\n]\n[\n238\n,\n239\n]\n[\n240\n,\n241\n,\n242\n,\n243\n]\n[\n168\n,\n169\n,\n170\n,\n171\n,\n172\n,\n244\n,\n245\n,\n246\n,\n173\n,\n174\n,\n247\n,\n248\n,\n249\n,\n175\n,\n250\n,\n251\n]\n[\n167\n,\n252\n,\n253\n,\n254\n,\n214\n,\n220\n,\n220\n,\n255\n]\n[\n192\n,\n256\n,\n257\n]\n,\n[\n213\n,\n120\n,\n121\n,\n122\n,\n123\n,\n217\n]\n[\n127\n,\n128\n,\n196\n,\n176\n,\n215\n,\n258\n]\n,\n[\n202\n,\n201\n,\n203\n,\n204\n,\n206\n,\n205\n,\n259\n,\n185\n,\n186\n,\n187\n,\n188\n,\n189\n,\n260\n]\n[\n261\n,\n262\n,\n263\n,\n264\n]\n,\n[\n218\n,\n219\n]\n[\n265\n,\n222\n]\n,\n[\n219\n]\n[\n212\n,\n266\n,\n267\n,\n268\n,\n269\n]\nFigure 5\n:\nGeneral overview of architectural AD stack approaches aligned with the SA model\n[\n85\n]\n.\nAs shown in Figure\n5\n, besides modular and vanilla E2E AD stacks various intermediate architectures are emerging to combine the advantages of both paradigms. Depending on their origin, these architectures are referred to as interpretable E2E architectures, modular E2E planning\n[\n17\n]\n, differentiable and modular\n[\n222\n]\nor planning-oriented AD stack\n[\n212\n]\n. Overall, current research generally emphasizes the need for a certain degree of modularity for interpretability combined with data-driven E2E adjustments and downstream task alignment, to achieve targeted outcomes. Throughout this paper, this set of AD stack architectures is referred to as modular E2E (M-E2E) architectures. These M-E2E AD stacks acknowledge and address limitations in AD and offer initial insights into measures required to progress toward fully autonomous driving. In this regard, the references in Figure\n5\ndemonstrate, that AI is used across all stages and architectural designs. Thereby, transformers are predominantly used in current approaches\n[\n224\n,\n236\n,\n234\n,\n107\n,\n214\n,\n212\n]\n, while foundation models (FMs) emerge as an active and promising research area, offering potential for improved generalization yet presenting numerous open research questions\n[\n183\n,\n186\n,\n187\n,\n188\n]\n. Accordingly, while classical modular AD stack are still dominantly deployed, M-E2E concepts are also emerging in the industry, e.g. Tesla and Wayve\n[\n17\n]\n.\nII-F\nDiscussion\nIt becomes apparent, that the classical modular AD stack is limited in multiple perspectives. The SA-driven analysis reveals, that the situation understanding, Level 2 SA, is particularly restricted. Increased interconnectivity to reduce complexity reinforces rethinking modularity towards increased flexibility. In addition, the semantic gap remains a significant challenge, underscoring the importance of dynamic decision-making and advanced cognitive processes. Although recent research has identified and started addressing several limitations, the realization of fully autonomous driving is still a long-term objective. Therefore, the subsequent section analyzes the steps required to advance autonomy.\nIII\nTowards Fully Autonomous Driving\nThis section deals with the open question of how the challenges carved out in the previous section can be addressed and reconciled with current technological developments. Thereby, this section directly builds up on the latest research. To this end, possible further developments towards autonomous driving are identified.\nIII-A\nFrom Hard to Flexible Connections\nBased on the analysis of the current SOTA in Section\nII\n, achieving fully autonomous driving requires a more comprehensive transfer of self-responsibility and, above all, fewer hard-coded connections, e.g., between modules. Especially as hard-coded connections represent hand-crafted, knowledge-induced structures that limit the autonomy required to overcome open-world challenges. Therefore, in addition to the general shift in engineering processes toward iterative development and renewal\n[\n270\n]\n, evolving the underlying AD stack to achieve greater flexibility is essential. However, not only the functional requirements but also the immense demands on data processing and computational efficiency emphasize the importance of moving towards more flexible, adaptable, and less prevalent AD stacks.\nBased on current technological knowledge, one way to achieve human-like dynamic, context-specific flexibility, and adaptability for appropriate decision-making in complex situations could be the amplified use of attention mechanisms\n[\n271\n]\n. These mimic cognitive attention and provide soft weighting depending on the context. In other words, the weights and, thus, the interconnection of individual modules can be varied at runtime depending on the context. While this approach is used in individual sub-modules such as perception\n[\n272\n,\n273\n]\n, prediction of other road users\n[\n274\n,\n234\n,\n275\n]\n, or planning\n[\n276\n,\n277\n,\n212\n]\n, it is also conceivable to use it across the entire information processing chain in the area of interfaces. In this way, attention is used in LLMs\n[\n177\n,\n178\n]\n, which have already been applied in the AD context to tasks such as vision\n[\n183\n]\nor prediction\n[\n184\n]\n.\nAn overview of the internal interfaces within the AD stack of currently emerging approaches is provided in Table\nIII-A\n. While inspired by Figure\n5\n, the table focuses on a promising subset of AD stack methods that achieve a balance between modularity and flexibility, while considering the increasing intertwining of functionalities. These approaches are particularly interpretable and modular E2E approaches as well as integrated and/or differentiable prediction to planning concepts.\nAs can be seen from Table\nIII-A\n, the interpretable E2E (I-E2E) interfaces relied initially on semantic representations, which are also widely used in traditional modular AD stacks. More recently, however, latent, query-based and, in particular, token-based interfaces have also been introduced. While latent representations predominate in vanilla E2E approaches, they are less prevalent in the area of I-E2E methods, as the implicit characteristic has a negative impact on interpretability. Moreover, there are also token-based ex-/implicit interfaces used within I-E2E AD stacks. Depending on their extraction from semantic or latent variables, these tend to be implicit or explicit. Through their use within the I-E2E approaches by means of FMs, such as LLMs, or VLMs , they offer increased situation assessment due to their generalization and interpretation capability, regardless of the internal interface characteristics.\nTable I\n:\nComparison of AD stack internal interfaces of interpretable E2E (I-E2E), integrated motion prediction and planning (I-M-P&P), differentiable prediction-to-control (DIFF) and modular E2E planning (M-E2E-P) approaches considered in Fig.\n5\n.\nMethod\nInterface\nCharacteristic\nDetails\nI-E2E\nNMP’19\n[\n127\n]\nsemantic rep.\nexplicit\n3D detections, future motion forecast, space-time cost volume\nP3’20\n[\n128\n]\nsemantic rep.\nexplicit\nlidar & map features, occupancy rep., recurrent occupancy forecasting\nDSDNet’20\n[\n201\n]\nsemantic rep.\nexplicit\nfeature map, actor & trajectory feature, 2D BEV waypoints\nLSS’20\n[\n206\n]\nsemantic/latent rep.\nmixed\nBEV scene & cost map, segmentation, frustums, latent depth distrib.\nLookOut’21\n[\n202\n]\nlatent rep.\nimplicit\nimplicit latent variable model, latent sampler\nMP3’21\n[\n196\n]\nsemantic rep.\nexplicit\ngeometric & semantic features, online map, dynamic occupancy field\nNEAT’21\n[\n203\n]\nsemantic/latent rep.\nmixed\nimage features, attention map, NEAT feature, BEV semantic pred.\nLAV’22\n[\n204\n]\nsemantic/latent rep.\nmixed\nspatial features, RoI features, sparse pillar features, embeddings\nST-P3’22\n[\n205\n]\nsemantic rep.\nexplicit\nBEV rep. and future scenes predictions\nMILE’22\n[\n176\n]\nlatent variables\nimplicit\ntemporal dynamics, deterministic history\nDriveLM’23\n[\n186\n]\ntoken-based\nex-/implicit\ngraph-structured question answering\nDriveLLM’23\n[\n185\n]\nsemantic rep.\nexplicit\nbidirectional semantic LLM AD stack bridge\nDrive-GPT4’24\n[\n187\n]\ntoken-based\nex-/implicit\ntext and video tokens\nDriveVLM’24\n[\n189\n]\ntoken-based\nex-/implicit\nimage- and text-tokens\nV2X-VLM’24\n[\n188\n]\ntoken-based\nex-/implicit\ntextual prompts and multimodal visual inputs\nQuAD’24\n[\n215\n]\nquery-based\nhybrid\nspatio-temporal occupancy query points\nAlphamayo-R1’25\n[\n258\n]\ntoken-based\nex-/implicit\ntextual reasoning trace\nI-M-P&P\nPiP’20\n[\n261\n]\nlatent rep.\nimplicit\nplanning-, observation-, target, fused-target-tensor\nFutureFree’21\n[\n262\n]\nsemantic rep.\nexplicit\nfreespace-centric rep. from sensors & forecasted future freespace\nSafePathNet’23\n[\n263\n]\nquery-based/(semantic rep.)\nhybrid\nSDV- & agent-queries internal, (semantic rep. in- & output)\nIJP’23\n[\n264\n]\nsemantic rep.\nexplicit\nreference-, ego-sample-, predicted-trajectories\nGameFormer’23\n[\n218\n]\nquery-based\nhybrid\nagent history-, modality embedding-, future-query in single transformer\n[2.0pt/2pt]\nDTPP’24\n[\n219\n]\nsemantic rep.\nexplicit\ntree-structured planner, trajectory- & scenario-tree\n[2.0pt/2pt]\nDIFF\nDIPP’23\n[\n265\n]\nsemantic rep./query-based\nhybrid\nmulti-agent predictions, ego-trajectory, agent-query\nDiffStack’23\n[\n222\n]\nsemantic rep.\nexplicit\nmulti-modal trajectory predictions, ego trajectories\nM-E2E-P\nUniAD’23\n[\n212\n]\nquery-based\nhybrid\nmap-, track-, motion-, occ.-queries\nVAD’23\n[\n266\n]\nquery-based\nhybrid\nmap-, BEV-, agent-, ego-queries\nFusionAD’23\n[\n267\n]\nquery-based\nhybrid\nBEV, plan, ego, motion-queries\nVADv2’24\n[\n268\n]\ntoken-based\nex-/implicit\nmap-, image-, agent-, traffic, planning-tokens\nGenAD’24\n[\n269\n]\nquery-/token-based\nmixed\nmap-, BEV-, agent-, ego-tokens, self-, deformable-/cross-attention queries\nIn comparison, the approaches of integrated motion prediction and planning (I-M-P&P) and differentiable prediction-to-control (DIFF) rely mainly on semantic and query-based interfaces. In addition, query-based interfaces dominate in the area of modular E2E planning (M-E2E-P). Query-based interfaces are fundamentally built on attention mechanisms and are classified as hybrid, as they usually use explicit queries to extract implicit information. While almost all of the listed approaches in Table\nIII-A\nuse transformers and attention mechanisms internally for specific sub-functionalities, this shows that the use of attention mechanisms is also increasing across sub-functionalities in terms of interfaces. The combination with token-based representations, which can be used as precisely defined, discretized and modularized internal information bricks, as has been done in e.g.\n[\n269\n]\n, appears promising.\nHowever, the pure use of FMs and token-based interfaces along AD stacks does not lead to the desired intertwining of the different sub-modules or -functionalities. Although FMs are currently popular and achieve considerable performance in SA, they do not enable reasoning in the true sense, whereas approaches such as those in M-E2E-P with query-based interfaces that build up on various attention mechanisms appear more promising. Moreover, query-based interfaces uniquely enable flexibility through request-driven mechanisms, allowing for adaptive and self-responsible information flow guidance throughout data processing and information compression.\nAccordingly, with respect to the AD stack, attention mechanisms connecting various modules could be interpreted as an open long-tail distribution countermeasure at the highest AD system level. This means that attention mechanisms addressing the issue of handcrafted, rule-based AD stack architectures at the highest system level and could be considered as learning-based, self-responsible system orchestrators. Nevertheless, other options are also conceivable, e.g., orchestrators with increased flexibility\n[\n39\n]\nin combination with a differentiable and modular AD stack\n[\n222\n]\n. However, finding a suitable cost function that is needed in such orchestrators could be challenging. For instance, this is evident in the subarea of trajectory planning, which is why learned weights or even entire learned cost functions are increasingly being used\n[\n221\n]\n. Although learned cost functions reduce knowledge induction, they are usually not context-specific. However, it stands to reason that in an optimization-based orchestration of services, the costs and weightings of individual aspects such as safety and progress could also be adjusted in a context-specific manner. In comparison, attention mechanisms enable such context-specific adjustments implicitly and also have no specific requirements for individual modules as they are able process implicit and explicit information. Nevertheless, attention mechanisms have also respective demands, e.g., they implicitly require within the given application cross-context training of the AD stack. It turns out that there are various possibilities foreseeable with individual pros and cons. Nevertheless, attention mechanisms have largely proven their capabilities. Furthermore, they are implicitly appropriate for highly autonomous orchestration. Therefore, query-based, attention-driven interfaces and orchestration could be quite promising.\nIII-B\nTowards Improved Situation Awareness\nAs outlined in Section\nII\n, SA is key towards fully autonomous driving. A concise and nuanced scene, situation, scenario, and context representation, i.e., SA in general, is crucial from a functional perspective. In addition, the individual SA level representations are also relevant for system interpretability. To review the state of research in this regard, Table\nIII-B\nprovides a more detailed overview of the different levels of SA. In addition to the I-E2E, I-M-P&P, DIFF and\nM-E2E-P\nof Table\nIII-A\n, the overview in Table\nIII-B\nalso considers SA, Sensor-2-Plan (S2P) and vanilla E2E autonomy approaches. In particular, the various black-box-like autonomy methods are also considered here, as a consistent, constantly updated context consideration is desirable, which could be accounted for in both interpretable and non-interpretable intertwined AD stack approaches.\nTable II\n:\nOverview of the SA consideration and representation across various AD stack levels and concepts.\nMethod\nLevel 1 SA\nLevel 2 SA\nLevel 3 SA\nVanilla E2E Autonomy\nE2E-CNN’16\n[\n168\n]\n✗/CNN\n✗/CNN\n✗/CNN\nDRL-Framework’17\n[\n169\n]\nlatent input vector\nspatial features\n✗/RL\nCIL’18\n[\n170\n]\njoint input representation\n✗/CIL\n✗/CIL\nCILRS’19\n[\n171\n]\nlatent perception state\n✗/CIL\n✗/CIL\nUD-CIL’20\n[\n172\n]\nintermediate learned perception features\n✗/CIL\n✗/CIL\nMaRLn’20\n[\n244\n]\n✗/RL\n✗/RL\npredict affordances/RL state\nDeepImitative’20\n[\n245\n]\nlatent (MapFeat, PastRNN)\nlatent (MapFeat, JointFeat)\nlatent (FutureMLP, FutureRNN)\nLSD’20\n[\n246\n]\n✗/BC\ncontext-embeddings/BC\nsituation-specific policy predictions/BC\nLBC’20\n[\n173\n]\n✗/teacher-guided policy learning\n✗/teacher-guided policy learning\n✗/teacher-guided policy learning\nWOR’21\n[\n174\n]\n✗/learned policy\n✗/learned policy\n✗/learned policy\nRoach’21\n[\n247\n]\nBEV rep. & measurement vector\n✗/IL-agent&RL-coach\n✗/IL-agent&RL-coach\nTCP’22\n[\n248\n]\nimage & measurement features\n✗\ntrajectory-guided control prediction\nUrbanDriver’22\n[\n249\n]\nvectorized rep.\nmid-level vectorized rep.\ndifferentiable traffic simulator\nTRAVL’22\n[\n175\n]\nBEV tensor\n✗/learned policy\nimagined data & look-ahead\nThinkTwice’23\n[\n250\n]\nBEV features\ncompact env. & mission vector\nprediction feature\nHydra-MDP++’24\n[\n251\n]\nimage & lidar tokens\nenvironment token\n✗/teacher-student model\nS2P Autonomy\nALVINN’88\n[\n167\n]\n✗/NN\n✗/NN\n✗/NN\nSeg2WP’18\n[\n252\n]\nsegmentation map\n✗/CIL\n✗/CIL\nTransFuser’22\n[\n254\n]\nauxiliary map, depth, bounding boxes\nauxiliary semantic seg.\nfeature vector of global context\nPlanT’22\n[\n214\n]\ninput & embedding tokens\nlearnable [CLS] tokens\nauxiliary vehicle future prediction\nDriveAdapter’23\n[\n220\n]\nBEV features\n✗/teacher-student model\n✗/teacher-student model\nDriveMLM’23\n[\n255\n]\nimage & cloud token embeddings\n(✗)\nLLM-based reasoning & explanation\nSA Autonomy\nEmergentOccupancy’22\n[\n257\n]\nego-centric freespace\n(✗)\nfuture occupancy predictions\nOccFlow’22\n[\n120\n]\nBEV map\noccupancy grids\noccupancy flow flields\nOccNet’23\n[\n122\n]\nBEV feature\nsemantic scene completion\n(flow annotation)\nImplicitO’23\n[\n121\n]\nfeature map\n(✗)\noccupancy probability & flow over time\nLetOccFlow’24\n[\n123\n]\nBEV feature\n3D occupancy\n3D occupancy flow\nViewFormer’25\n[\n217\n]\nBEV feature\n3D occupancy\noccupancy flow\nI-E2E\nNMP’19\n[\n127\n]\n3D detections\n(✗)\nmotion forecast\nP3’20\n[\n128\n]\nlidar & map features\nfused features\nsemantic occupancy forecast\nDSDNet’20\n[\n201\n]\nfeature map & detection\n(✗)\nprobab. multimodal social pred.\nLSS’20\n[\n206\n]\nBEV rep.\nBEV semantic segmentation\nBEV predictions\nLookOut’21\n[\n202\n]\nobject detector\nactor context\nlatent scene dynamics & decoder\nMP3’21\n[\n196\n]\ngeometric & semantic features\nonline map\ndynamic occupancy field\nNEAT’21\n[\n203\n]\n2D image features\nneural attention fields\nBEV semantic prediction\nLAV’22\n[\n204\n]\nmap-view features\n(✗)\nmulti-modal trajectory predictions\nST-P3’22\n[\n205\n]\nBEV features\n(✗)\ndual pathway & occupancy fields\nMILE’22\n[\n176\n]\nBEV features\nlatent rep. world model\nlatent state trajectory forecasting\nDriveLM’23\n[\n186\n]\ngraph visual question answering (GVQA)\nGVQA-based reasoning\nGVQA-based prediction\nDriveLLM’23\n[\n185\n]\nany existing AD stack\nLLM-based reasoning\nLLM-based decision-making\nDrive-GPT4’24\n[\n187\n]\nquestion-answering (QA)\nQA-based reasoning\nQA-based prediction\nDriveVLM’24\n[\n189\n]\nchain-of-though (CoT) description\nCoT-based analysis\nCoT-based prediction\nV2X-VLM’24\n[\n188\n]\nscene description prompting\nVLM-based interpretation\n(✗) / VLM internal motion prediction\nQuAD’24\n[\n215\n]\nBEV latent rep.\nBEV & occupancy\nimplicit occupancy model\nAlphamayo-R1’25\n[\n258\n]\nmulti-camera cideo tokens\nchain-of-causation (CoC)\nCosmos-Reason\n[\n278\n]\nbased prediction\nI-M-P&P\nPiP’20\n[\n261\n]\nobservation tensor\nsocial context encoding\nfused target encoding\nFutureFree’21\n[\n262\n]\nfreespace\n(✗)\nforecasted future freespace\nSafePathNet’23\n[\n263\n]\n✗\n(assume: vectorized scene rep.)\nSDV & agent future trajectories\nIJP’23\n[\n264\n]\n✗\n✗\npredicted trajectories\nGameFormer’23\n[\n218\n]\n✗\n✗\ngame theory inspired transformer\n[2.0pt/2pt]\nDTPP’24\n[\n219\n]\n✗\nenv. encoding\nego-conditioned predictions\n[2.0pt/2pt]\nDIFF\nDIPP’23\n[\n265\n]\n✗\nlocal scene context encoder\nfuture decoder\nDiffStack’23\n[\n222\n]\n✗\n✗\nTrajectron++\n[\n279\n]\nM-E2E-P\nUniAD’23\n[\n212\n]\nBEV features\n(✗) / MotionFormer\nOccFormer\nVAD’23\n[\n266\n]\nBEV features\nvectorized map\nvectorized agent motion\nFusionAD’23\n[\n267\n]\nBEV camera, lidar features\nfused BEV feature\nmodality self-attn. & refinement net.\nVADv2’24\n[\n268\n]\n(✗) / map & image tokens\nscene tokens\nagent & traffic element tokens\nGenAD’24\n[\n269\n]\nBEV tokens\nscene tokens\nlatent future generation\nAs shown in Table\nIII-B\n, both vanilla and S2P autonomy approaches often rely on a holistic approach to learning policies, i.e., mappings between inputs (e.g., sensors) and outputs (e.g., trajectory plans or control inputs). While early approaches considered neural networks (NN) or convolutional neural networks (CNN), there was first research on reinforcement learning (RL), behavioral cloning (BC), and imitation learning (IL), whilst recently developed approaches concentrate on conditional imitation learning (CIL) and teacher-student models. All of these approaches implicitly take SA into account and do not model the scene or context considerations. In contrast, within SA autonomy, the various SA levels are considered more explicitly from BEV over occupancy map to occupancy (flow) predictions/field. Thus, providing inspiration for a continuous context representation for improved SA. In contrast to the traditional modular AD stack, SA Autonomy follows an occupancy-based rather than an object-based approach, which expands the scope of responsibility of the used AI method as there is less hand-crafted object-based reasoning. On the one hand, occupancy-based approaches show good performance and improved generalization abilities, on the other hand, human reasoning is object-driven.\nWithin the I-E2E subset in Table\nIII-B\n, two main directions exist. On the one hand, some approaches consider Level 2 SA only implicitly and to a limited extent, as indicated by (✗) in the Level 2 SA column. On the other hand, some FM-based approaches address Level 2 SA constraints in particular but have inherent FM-related concerns and restrictions. Apart from that, the I-M-P&P and DIFF methods mainly deal with Level 3 SA but indicate the relevance of scene/environment encoding for subsequent forecasting.\nFurthermore, M-E2E-P within Table\nIII-B\nconsiders all SA levels successively and continuously like SA autonomy. However, while within SA autonomy occupancy-based representations are dominant across Level 2 and 3 SA, within M-E2E-P there is a greater conceptual diversity. Reaching from vectorized and token-based Level 2 SA representations to a multitude of different Level 3 SA representation concepts. In general M-E2E-P concepts bridge machine readability with human interetability along SA. Thereby, some approaches tend to maintain machine-readable representations internally along processing, while branching human-readable conversions out for interpretability. Others focus on machine- and human-readable interfaces between functionalities.\nOverall, it becomes apparent, that diverse concepts to consider SA relevant context information exist. While vanilla E2E and S2P autonomy approaches predominantly focus on implicit handling, I-E2E, I-M-P&P, DIFF, and M-E2E-P target more explicit scene and context modeling. Thereby, not all representations are directly human-interpretable. Moreover, the capabilities of FMs such as LLMs, VLMs, and VLAs are promising to supplement existing AD stacks. For instance,\n[\n255\n]\nenhances existing AD stacks using a multi-modal LLM (MLM). Nevertheless, while FMs are promising and showing impressive results, they typically do not provide the human-like reasoning, that is necessary for fully autonomous driving. Yet, latest research demonstrates via Nvidia’s Alpamayo-R1\n[\n258\n]\n, a VLA model that integrates Chain of Causation (CoC) reasoning on top of the physically pretrained VLM backbone Cosmos-Reason1\n[\n278\n]\n, thereby providing a first step toward improved reasonability. Despite this, we imagine FM as eighter one functional capability block or parallel integrated system. For instance, as one functional capability and suitable human-machine interface (HMI), that provides situation interpretations in tokenized representations and textual descriptions. Or parallel integrated to classical methods\n[\n280\n,\n281\n]\nas System 1 and System 2 framework in accordance to\n[\n282\n]\n. In general, in combination with query-based attention interfaces information flow of various representations and sources could be promising.\nOne conceptual way of improving SA, in general, is a directed information processing chain across the entire AD stack in which all data is constantly acquired and processed to generate a consistent, constantly updated context representation. Thereby, e.g., the initial context knowledge can be used to reduce the amount of data in the individual processing steps. New context knowledge can be derived from the processing steps. Thus, with increasing processing depth, the data volume can be reduced while context knowledge can be enhanced simultaneously to provide highly aggregated information. This way, data is only reduced and sorted out if it will no longer be necessary for downstream steps due to the context knowledge at hand. A respective conceptual approach is illustrated in Figure\n6\n.\nFigure 6\n:\nVisualization of the directed context consideration. At the beginning of the processing chain there is a lot of data as well as external information. With increasing processing depth, the data can be sorted out and consolidated in a highly aggregated manner, which results in a context gain in parallel.\nAlthough the illustration is simplified, e.g., with respect to the overall actualization process, that is indispensable, as the context rapidly becomes outdated due to temporal and spatial changes in dynamic environments, the illustration suggests architectural implications. For instance, a context layer could be incorporated over the entire information processing chain from perception to action, which consistently summarizes the increasing knowledge about the context. This context status could in turn act on the interfaces along the AD stack and functionalities in general. While such an approach seems reasonable, it raises the open question of how the context should be modeled. Advances in the field of AI, especially in context-based AI systems, indicate that the use of latent representations and tokenization appears to be effective. Nevertheless, while this is evidenced by the progress made by AI systems, on the other hand, visual scene representations are used as context information\n[\n196\n,\n128\n]\nto increase interpretability. However, these visual representations are usually limited to a few contextual aspects. A different direction is demonstrated by latest FM, that enable tokenized context aggregation along chain of thought (CoT) or chain or chain of causation (CoC) and textual explenations for interpretations\n[\n189\n,\n258\n]\n. Overall, whether a latent representation, tokenized representation or a combination with a simplified situation representation is ultimately the most effective way is an area of potential further research.\nFinally, it should be noted, that the analysis of methods in Table\nIII-B\nshows, that first steps towards such a context consideration are made. For instance, through BEV information compression and successive reuse. Nevertheless, current efforts are limited in several respects. Especially, as either or is currently predominant, as described in more detail in the following. Currently, one sensor source and one context representation are mostly employed. For instance, either occupancy-based or object-based, either more traditional or FM-based, either LiDAR-based or vision-based is applied. Although this reduces the complexity in the already wide-ranging AD stack research, it also limits the capabilities. In addition, the step-by-step consideration of the context along the processing chain is predominant. However, recent research results show that functionalities along the AD stack must also be intertwined across successive processing steps. Therefore, a dedicated and decoupled context layer that evolves over space and time appears to be promising.\nIII-C\nExternal Sourcing of Contexts\nIn addition to internal context aggregation and interpretation, querying external context is a valuable complement to on-board capabilities. This involves connected and automated vehicles (CAVs) using vehicle-to-anything (V2X) communication, standardized for example by the European Telecommunications Standards Institute (ETSI)\n[\n283\n]\n. Key message formats include cooperative awareness messages (CAM)\n[\n284\n]\n, collective perception messages (CPM)\n[\n285\n]\n, and maneuver coordination messages (MCM)\n[\n286\n]\n, which facilitate the exchange of contextual information to enhance external sensing and situational awareness. The Infrastructure Support Levels for Automated Driving (ISAD)\n[\n287\n]\nprovides a classification scheme for road infrastructure capabilities, similar to Society of Automotive Engineers (SAE) levels for vehicle capabilities. Consistent with previous subsections, Table\nIII\nsummarizes both internal and external sources utilized in the surveyed methods. It also highlights a subset of real-world commercial systems\n[\n198\n,\n288\n,\n289\n,\n290\n,\n200\n,\n291\n,\n292\n,\n293\n,\n294\n,\n295\n,\n296\n,\n297\n]\nand V2X research efforts\n[\n140\n,\n298\n,\n299\n,\n136\n]\n. In general, it becomes apparent that while advanced digital infrastructure (ISAD levels C, B, and A) is increasingly explored in V2X research, advancing AD stack architectures and existing commercial systems predominantly operate under conventional infrastructure conditions (ISAD levels D and E).\nTable III\n:\nComparison of internal and external sources used in the methods surveyed.\nMethod\nInternal Sources\nExternal Sources\nClassification\nVanilla E2E Autonomy\nE2E-CNN’16\n[\n168\n]\nmulti-view images\n✗\nvision-based\nDLR-Framework’17\n[\n169\n]\nraw sensor input vector\n✗\nn/a\nCIL’18\n[\n170\n]\ncamera images, ego speed, high-level command\n✗\nmapless, vision-driven\nCILRS’19\n[\n171\n]\nsingle image, ego speed, high-level command\n✗\nmapless, vision-driven\nUD-CIL’20\n[\n172\n]\nmulti-view images, route command\n✗\nmapless, vision-based\nMaRLn’20\n[\n244\n]\ncamera images\n✗\nmapless, vision-based\nDeepImitative’20\n[\n245\n]\nmainly Lidar (possible camera images or both)\ntraffic light signal\nmapless\nLSD’20\n[\n246\n]\ncamera images, ego speed, high-level command\n✗\nmapless, camera-driven\nLBC’20\n[\n173\n]\ncamera images, ego speed, high-level command\n(map to train privileged agent)\nmapless, camera-driven\nWOR’21\n[\n174\n]\ncamera images, ego speed, high-level command\n(maps to train world model)\nmapless, camera-driven\nRoach’21\n[\n247\n]\nmeasurement & (BEV images) vector\n✗\nintermediate rep.\nTCP’22\n[\n248\n]\ncamera images, ego speed, high-level command\n✗\nmapless, vision-driven\nUrbanDriver’22\n[\n249\n]\n(perception module output)\nHD map\nintermediate rep.\nTRAVL’22\n[\n175\n]\n(agents motion history)\nHD map\nintermediate rep.\nThinkTwice’23\n[\n250\n]\nconsecutive LiDAR sweeps & RGB cameras\n✗\nmapless, multi-modal\nHydra-MDP++’24\n[\n300\n]\ncamera images, measurements, high-level command\n✗\nmapless, vision-driven\nS2P Autonomy\nALVINN’88\n[\n167\n]\ncamera images & laser range finder\n✗\nmapless, multi-modal\nSeg2WP’18\n[\n252\n]\ncamera images, high-level command\n✗\nmapless, vision-driven\nTransFuser’22\n[\n254\n]\nLiDAR sweeps & RGB cameras\n✗\nmapless, multi-modal\nPlanT’22\n[\n214\n]\nobject-based rep. build on\n[\n210\n]\n✗\nintermediate rep.\nDriveAdapter’23\n[\n220\n]\nconsecutive LiDAR sweeps & multi-view images\n(ground-truth BEV for teacher)\nmapless, multi-modal\nDriveMLM’23\n[\n255\n]\n(textual AD bridge: LiDAR, camera, human input)\n(HD map API)\nAD LLM enhancement\nSA Autonomy\nEmergentOccupancy’22\n[\n257\n]\nconsecutive LiDAR sweeps\n✗\nmapless, LiDAR-based\nOccFlow’22\n[\n120\n]\npast agent states\ntraffic light status, road structure\nsparse agent & env. states\nOccNet’23\n[\n122\n]\nmulti-view camera images\n✗\nmapless, vision-based\nImplicitO’23\n[\n121\n]\nconsecutive LiDAR sweeps\nHD map\nmap- & LiDAR-based\nLetOccFlow’24\n[\n123\n]\ntemp. seq. multi-view camera images\n✗\nmapless, vision-based\nViewFormer’25\n[\n217\n]\nmulti-view camera images\n✗\nmapless, vision-based\nI-E2E\nNMP’19\n[\n127\n]\nconsecutive LiDAR sweeps\nHD map\nmap- & LiDAR-based\nP3’20\n[\n128\n]\nconsecutive LiDAR sweeps\nHD map\nmap- & LiDAR-based\nDSDNet’20\n[\n201\n]\nconsecutive LiDAR sweeps\nHD map\nmap- & LiDAR-based\nLSS’20\n[\n206\n]\nn\nn\nimages, incl. extrinsic & intrinsic parameters\n✗\nmapless, vision-based\nLookOut’21\n[\n202\n]\nconsecutive LiDAR sweeps\nHD map\nmap- & LiDAR-based\nMP3’21\n[\n196\n]\nconsecutive LiDAR sweeps, high-level command\n✗\nmapless, LiDAR-based\nNEAT’21\n[\n203\n]\nRGB cameras\n✗\nmapless, vision-based\nLAV’22\n[\n204\n]\nconsecutive LiDAR sweeps & RGB cameras\n✗\nmapless, multi-modal\nST-P3’22\n[\n205\n]\nmulti-view images, high-level command\n✗\nmapless, vision-based\nMILE’22\n[\n176\n]\ncamera images\n✗\nmapless, vision-only\nDriveLM’23\n[\n186\n]\nfront-view image\n✗\nmapless, LLM-based\nDriveLLM’23\n[\n185\n]\ntext\n✗\nmapless, LLM AD enhancement\nDrive-GPT4’24\n[\n187\n]\nfront-view RGB camera video sequence & text\n✗\nmapless, multi-modal LLM\nDriveVLM’24\n[\n189\n]\nsequence of images\n✗\nmapless, VLM-based\nV2X-VLM’24\n[\n188\n]\nimages & text\ninfrastructure images & text\nmapless, multi-modal LLM\nQuAD’24\n[\n215\n]\nconsecutive LiDAR sweeps\nHD map\nmap- & LiDAR-based\nAlphamayo-R1’25\n[\n258\n]\nconsecutive multi-view images, textual instructions\n✗\nmapless, VLA incl. CoC\nM-E2E-P\nUniAD’23\n[\n212\n]\nmulti-view images\n✗\nmapless vision-based\nVAD’23\n[\n266\n]\nmulti-view images\n✗\nmapless, vision-based\nFusionAD’23\n[\n267\n]\nconsecutive LiDAR sweeps & multi-view images\n✗\nmapless, multi-modal\nVADv2’24\n[\n268\n]\nmulti-view images\n(map during training)\nmapless in deployment\nGenAD’24\n[\n269\n]\nmulti-view images\nmap information\nmap- & vision-based\nReal Sys. / Commercial\nUber ATG’18\n[\n198\n]\nLiDAR, radar, camera, etc.\nHD map\nmap-based, multi-modal\nLyft’20\n[\n288\n]\nLiDAR, radar, camera, etc.\nHD map\nmap-based, multi-modal\nArgo AI’21\n[\n289\n]\nLiDAR, radar, camera, etc.\nHD map\nmap-based, multi-modal\nMotional’21\n[\n290\n]\nLiDAR, radar, camera, etc.\nHD map\nmap-based, multi-modal\nWaymo’21\n[\n200\n]\nLiDAR, radar, camera, etc.\nHD map\nmap-based, multi-modal\nZoox’21\n[\n291\n]\nLiDAR, radar, camera, etc.\nHD map\nmap-based, multi-modal\nCruise LLC’22\n[\n292\n]\nLiDAR, radar, camera, etc.\nHD map\nmap-based, multi-modal\nNvidia’24\n[\n293\n]\nLiDAR, radar, camera, etc.\nHD map\nmap-based, multi-modal\nMobileye’24\n[\n294\n]\nLiDAR, radar, camera, etc.\nHD map\nmap-based, multi-modal\nTesla’21\n[\n295\n]\nCameras\n✗\nmapless, vision-based\nToyota’22\n[\n296\n]\nLiDAR, radar, camera, etc.\nHD map\nmap-based, multi-modal\nMercedes-Benz’23\n[\n297\n]\nLiDAR, radar, camera, etc.\nHD map\nmap-based, multi-modal\nWayve’25\n[\n301\n,\n302\n]\ncamera, radar, (opt. LiDAR)\n✗\nmapless, vision-first\nV2X\nCould-CEM’20\n[\n136\n]\ninternal multi-modal\nmultiple multi-modal vehicles\ncollective env. model\nUULM-Bosch’21\n[\n140\n]\nLiDAR, radar, camera\nHD map, infrastructure sensors\nmulti-modal, connected\nProvidentia++’23\n[\n298\n]\nonboard sensors\nmulti-modal infrastructure RSU\nreal-time digital twin\nV2X-Percep.’24\n[\n299\n,\n136\n]\nonboard sensors\ninfrastructure sensors\nmulti-modal, multi-system\nIn Table\nIII\nit becomes visible that the investigated methods do not match the real commercial system configurations. While in real systems, except Tesla\n[\n295\n]\nand partly Wayve\n[\n301\n,\n302\n]\n, a wide range of internal sensor inputs are incorporated\n[\n198\n,\n288\n,\n289\n,\n290\n,\n200\n,\n291\n,\n292\n,\n293\n,\n294\n,\n296\n,\n297\n]\n, the investigated methods mostly focus on vision- or LiDAR-based systems. Even in multi-modal systems, LiDAR and camera are only incorporated\n[\n250\n,\n167\n,\n254\n,\n220\n,\n204\n,\n187\n,\n188\n,\n267\n]\n. Although V2X research is expanding information acquisition, it is neither part of the AD stacks currently under research nor part of the real systems.\nHowever, increased interconnectivity facilitates the versatile use of external data and information. Beyond the information currently considered, such as to extend the vehicle’s FOV, high-definition (HD) maps\n[\n303\n,\n304\n]\noffer further possibilities by providing rich situation semantics. HD maps can serve as virtual sensors, aggregating knowledge from physical sensors and prior data to represent the environment. They contain information about, for example, an adjacent building being a school, about a bus stop across the street, or about being on a highway. Furthermore, besides speed limits also information regarding traffic-calmed areas or permanent traffic barriers can be stored in HD map\n[\n304\n]\n, supporting perception, planning, and decision-making in automated vehicles. Beyond that, there is the possibility that through increased interconnectivity and cooperation additional information such as the global destination of other local participants is known. All this information represents extensive context knowledge, which can be taken into account. Such external sourcing of context offers the opportunity to reach enhanced situation understanding, required for nuanced SA and decision-making for the desired level of autonomy. However, it also increases the burden of targeted data processing and information aggregation while simultaneously increasing dependencies. Although this poses challenges, the external sourcing of context offers the opportunity to minimize internal expenses, e.g. the recognition of a school. One approach that address the respective challenges is the continued consideration of external sources within the context layer discussed above. Nevertheless, a measure of trustworthiness should be included for external data in accordance to a respective security level in order to ensure reliability and integrity. Especially the mutual reassurance between external information and internal awareness represent a hedging methodology, with a variety of consecutive open research questions.\nIII-D\nHypothetical Structure of Architecture\nA conceivable consideration of the previously outlined aspects is sketched in Figure\n7\nin an abstract hypothetical sense. Similar to a meta-learning approach like conditional neural processes\n[\n305\n]\n, the external knowledge at hand (HD map, V2X, control room, …), referred to as the local world model, could be summarized (e1) and encoded into a context state (c1). This context state could be conglomarate of various representations like VLM and LLM driven tokens, but also latent or semantic representations. Building on such a context (c1), it could be used in the first attention stage to context-dependently constrain the information density of the pre-processed raw data. In the second attention stage, the weighting of the individual sensor modalities could be adjusted depending on the context. For example, when turning right at an intersection without any other participants, the first stage could specifically adjust the perception task in a context-specific manner, while realizing a data reduction. The second stage, on the other hand, could take, e.g., weather conditions into account and reduce the impact of sensors that exhibit weather-related degradation via adjusted weights.\nIn addition, the newly acquired context could be fed to the context state by internal processing steps in order to enrich it. Besides that, it is also imaginable that collective environment models and collaborative planning are directly taken into account. Also, general external information (e1) can be used as input for the respective processing sub-modules, whereby the context is a crucial factor in pre-filtering their significance and exploitation. Overall, the context is decisive with respect to what extent the respective data, information, or even knowledge is to be considered.\nNote, Figure\n7\nillustrates a unique conceptual framework of a hypothetical approach that takes into account the flexibilization of modules through attention driven query-based interfaces, the increase of SA through a consistent and continuously updated context representation, the direct and indirect consideration of external sourcing, and thus takes up various previous aspects. However, the illustrated architecture does not represent a solution but rather serves as a linked representation of the aforementioned aspects for further discussion.\nFigure 7\n:\nPossible architecture integrating situational awareness.\nIII-E\nPerspectives & Implications on AD Stack Developments\nThe aforementioned conglomerate of observations shows that an increase in autonomy is accompanied by an increase in system complexity, which in turn requires a change in the AD stack. Initially opposing approaches such as modular service-oriented AD stacks and vanilla E2E AD stacks are converging more and more via I-E2E and M-E2E-P and combining the respective advantages. However, internal and external sourcing as well as V2X communication and collaboration are limited. In this area of tension, query-based interfaces can be further developed from simple interfaces to more comprehensive interfaces and even orchestrators that control the flow of information. In this way, different sources and functional capabilities can be queried and taken into account via query-based attention mechanisms. Increasing coupling with FM, for example, would also be conceivable. If internal uncertainty increases significantly, for example, the LLM and its reasoning capabilities might be called upon.\nOne major implication with respect to corresponding further developments is the increasing need to train learnable interfaces and orchestrators. Here, a combination of bottom-up modular functionalities that allow for permissible functional and expert knowledge without affecting the generalization capability of the overall system, together with a learning-oriented top-down approach that addresses the inherent complexity by means of an implicit data-driven approach, seems promising for closing the semantic gap and approaching towards the desired autonomy. In addition, there is the possibility that the efforts of modular service-oriented architecture, such as the use of quality vectors\n[\n153\n]\n, that specifically supplement payload data, in order to increase reliability, could be integrated as a self-assessment. For instance, self-assessment quality data could be integrated across the stack while simultaneously guaranteeing interpretability along the module interfaces or branched out semantic representations. We refer to such an increasingly self-responsible yet interpretable and debuggable architecture, which represents a combination of modular service-oriented and (M-)E2E architecture, as a service-oriented modular end-to-end (SO-M-E2E) architecture.\nIII-F\nImplications on Safeguarding\nIn the context of interpretable AD stacks, the bottom-up approach is primarily pursued, integrating heavily extracted system and expert knowledge into the architecture and tech stack\n[\n153\n]\n. However, as discussed fully autonomous vehicles require new degrees of freedom. The consequences of underlying paradigm shifts, such as the transition from hard-coded connections to flexible soft-coded connections, illustrate changed framework conditions which, in addition to development, also have an impact on safeguarding.\nModular assurance, e.g.\n[\n72\n,\n71\n]\n, is a target-oriented approach in the context of a modular service-oriented architecture in AD. Moreover, recent approaches acknowledge the increasing use of learned components by means of data-based interface definitions for modular safety approval\n[\n306\n]\n. However, the current modular assurance methodology is expected to reach limits in fully autonomous driving with respect to accounting for circular reasoning. Nevertheless, the modular assurance and in particular the data-driven approach of\n[\n306\n]\nprovides a basis for extensions that could address circular reasoning on this basis. Since prospective architectures for fully autonomous driving, such as the hypothetical architecture (Figure\n7\n), or the discussed SO-M-E2E architecture in general, must necessarily take into account the given context. Here as well, the targeted supplementation of existing bottom-up approaches with deliberate top-down approaches to address the emerging challenges is conceivable. In this regard, RL, CIL and teacher-student approaches of top-down vanilla E2E approaches could be integrated on or along the SO-M-E2E architectures in the future for this purpose.\nIII-G\nDiscussion\nIn line with\n[\n17\n]\n, it could be assumed that future highly autonomous systems will need generalizing AI components, especially to address the multitude of corner cases at a higher level. However, this does not mean that task-specific, narrow AI systems or working engineered subsystems should be eliminated. Instead, these sub-module-specific high-performing systems should be retained, which also facilitates better explainability and debugging. However, generalizing AI offers the advantage of being able to respond autonomously to changing environments as well as to changing availabilities of external information. In addition, multiple perceptual modalities could be addressed across different vehicles. In the course of generalizing AI, approaches such as few-shot\n[\n22\n,\n23\n]\n, meta-learning\n[\n24\n,\n25\n,\n26\n]\n, and FMs are currently very popular. Combined with tightly connected AI systems and extensive knowledge-informing data and expertise that can be integrated top-down, highly AD could achieve astounding performance. In this way, today’s obvious problems of automated vehicles in the United States\n[\n155\n,\n157\n,\n156\n]\ncould be addressed, thus achieving the desired resilience and robustness. In this regard, an SO-M-E2E architecture seams to be promising concept for ongoing research. Moreover, it also may be noted that the SO-M-E2E architecture could be interpreted as the first concretization of Yann LeCun’s world model of a human-like AI\n[\n307\n]\n, but tailored towards autonomous driving. To be specific, these are recurrent, object-driven world models, that to a certain extent, mimic model predictive control (MPC) in neural networks. In this way, intelligence in the manner of Konrad Lorenz\n[\n308\n]\n, thus operating in imaginary space, could be realized.\nFurthermore, the increasing use of AI across modular system boundaries, shows the increasing importance of corresponding data, both for system development and for verification and validation. This was analyzed and discussed in the context of\n[\n270\n]\n, whereby a corresponding concept for an iterative development, verification, and validation of emerging complex systems that include AI was presented with specific emphasis on data relevance. Moreover, in this regard, appropriate databases that could be used exist for many years, especially for accident data. For example, the Fatal Analysis Reporting System (FARS), the Crash Report Sampling Systems (CRSS) or the Crash Investigation Sampling Systems (CISS) and others as shown in\n[\n309\n]\n. Furthermore, manufacturers of automated vehicles are required to report crash data. Accordingly, a transition within the field of safety analysis towards data-based AI safety assurance\n[\n79\n]\nis also foreseeable.\nIV\nProspect Challenges and Opportunities\nThe previous sections briefly outlined the current status of AD, highlighted current limitations, and presented prospects for advancing towards fully autonomous driving. Up to this point, the focus has primarily been on functionality and technological possibilities. However, the widespread deployment of autonomous driving necessitates scalability across various vehicle setups and execution environments, demanding flexibility for efficient adaptability. Therefore, this section extends the discussion to application-specific hurdles that have a decisive impact on the widespread availability of autonomous driving, identifying both prospect challenges and opportunities.\nIV-A\nChallenges of Prospective Autonomous Driving\nAs Section\nIII\nshowed, there are various aspects that would enable the limitations to be overcome and autonomy to be increased. Regardless of the specific technological realization, it is becoming visible that the use of AI and the importance of data appear to be increasing. This results in specific challenges for the present application, which are examined in more detail below.\nAmong other things, compliance with the applicable rules and laws is crucial for the introduction of autonomous driving. Besides compliance with existing regulations, the expanding regulation of AI poses a considerable challenge. This is especially true as different AI regulatory approaches are being pursued\n[\n310\n,\n311\n,\n312\n,\n313\n,\n314\n,\n315\n]\nand the regulatory landscape tends to remain dynamic and heterogeneous in the near future. This leads to the subsequent challenge of ensuring autonomous driving takes various regulatory requirements into account, e.g. by means of a system design integrated adaptive verification procedure of obligations.\nIn order to outline some regulatory requirements, the following mentions a few aspects of the recently enacted EU AI Act\n[\n312\n]\n, which is one of the first far-reaching laws on the use of AI. This law takes a risk-based approach, whereby autonomous driving is considered a high-risk system and is therefore subject to numerous obligations (e.g. Articles 16, 18, 23, 24, 25, 26). Worth mentioning here is the required risk management system (Article 9), which should consist of a continuous, iterative process throughout the entire lifecycle. Also worth mentioning are the requirements for data and data governance (Article 10), which set demands for training, testing and validation datasets, quality criteria, design decisions and data collection processes. In addition to these aspects, many others are subject to requirements such as human oversight (Article 14), post-market surveillance (Article 72), and serious incidents reporting (Article 73). The above illustrates the comprehensive regulatory demands based solely on the EU AI Act. It is to be expected that the regulatory landscape will (initially) be diverse and changeable across the globe\n[\n79\n]\n.\nDerived need:\nAutonomous driving should already take different regulatory requirements into account at the design stage and offer flexibility with regard to the verification of regulatory requirements in order to be prepared for different markets and changes over time.\nAnother challenge, which is related to both the regulatory requirements and the technical system, is safeguarding. Especially if AI is used along the critical path from perception over decision-making to execution, which is likely according to previous analyses. In this context, established standards such as ISO 26262 (Functional Safety, FuSa)\n[\n316\n]\n, ISO 21448 (Safety of the Intended Functionality, SOTIF)\n[\n317\n]\n, ISO/PAS 8800 (Safety and AI in road vehicles)\n[\n318\n]\n, and UL 4600 (Safety of Autonomous Products)\n[\n319\n]\nprovide foundational guidance but remain difficult to apply consistently to complex, data-driven AI systems. Consequently, new concepts such as the AI Safety Integrity Level (AI-SIL)\n[\n320\n]\nhave emerged, highlighting the need for a shift towards data-driven AI safety assurance\n[\n79\n]\n. In this regard, a complementary perspective based on control-theoretic methodologies has recently been proposed\n[\n321\n]\n. Even though this approach recognizes the fundamental shift from explicit mathematical to implicit data-driven systems, along addressing the broader challenge of predominantly non-agnostic safety assurance methods\n[\n15\n]\n, many open research questions regarding AI safety remain\n[\n74\n]\n.\nBesides the general challenges, there are also application-specific challenges in using AI for autonomous driving. In particular, due to the data-driven modeling foundations and assumptions, continuously checking the completeness of datasets is essential for applications operating in open, long-tail distributions, such as AD. Companies such as Waymo and Tesla address these challenges through iterative, lifecycle-oriented processes aimed at continuously improving system performance. While Waymo’s Safety Determination Lifecycle\n[\n322\n]\nprovides a more generic framework, Tesla’s Data Engine\n[\n323\n]\nspecifically targets the particularities of AI systems. To integrate these frameworks, an extension of the V-model for iterative, data-based development has been proposed, representing a generalized process reference model for complex systems incorporating AI\n[\n270\n]\n. These approaches can be regarded as risk management systems under Article 9 of the EU AI Act, highlighting the importance of iterative improvement alongside addressing AI safety concerns. As noted in\n[\n309\n]\n, such processes support both prospective safety analysis and retrospective evaluation.\nDerived need:\nAutonomous driving should be designed such that iterative further developments and releases are possible in an efficient manner throughout the entire lifecycle.\nIn line with the requirement for an iterative refinement process, data is an crucial basis for developing and ensuring the safety of complex, AI-inclusive systems such as autonomous driving. Thereby, data from critical situations is of particular interest, which is why data related challenges should be taken into account at an early stage.\nConsidering automated and autonomous vehicles operating around the globe, harvesting and sharing data, there is a myriad of data available. This data includes data from different countries with different traffic rules and behaviors. Furthermore, the data originates from different vehicle setups, types, and manufacturers, which is why different sensor modalities, types, and qualities serve as sources, and a wide variety of data is available. Thus, according to such a data harvesting convention, data of different characteristics from relevant and critical situations is available.\nDerived need:\nThe various trigger data should be generically usable, but also transferable to different use cases, e.g. vehicle configurations and environments.\nWhile the previous aspect refers to the diversity of systems and environments in terms of data, it can also pose challenges in terms of functional transferability. In this respect, human-like autonomy is desirable as humans are able to adapt their abilities, e.g., perception, decision-making, and planning, to the given circumstances. For example, when humans switch vehicles, they can adapt their perception and planning. Even if they change the execution environment, e.g., to a different country with different rules and behavior patterns, they can adapt and operate safely in this different open world.\nDerived need:\nHuman-like autonomy of functionalities, especially from perception to trajectory planning, is desirable. The aspired autonomy should be adaptable to different Operational Design Domains (ODDs), i.e., domains that define the specific conditions under which an AD system can operate safely. Furthermore, the autonomy should also be able to handle vehicles with different equipment, such as sensors and actuators, to enable efficient and scalable development and deployment of autonomous driving.\nIn addition to the technological and regulatory challenges, there are also socio-political challenges. These include, for example, the socio-political definition of acceptable risks, but also society’s trust in autonomous vehicles. Therefore, participation in the discourse on ethical issues and the agreement on acceptable risks demand a comprehensible, understandable, and publicly transparent development and validation process. Furthermore, traceability and understandability, also referred to as human-interpretable, are the basis for enabling efficient assurance and approval procedures.\nDerived need:\nComprehensible and understandable processes as well as a publicly accessible safety argumentation are aspirational for socio-political questions of trustworthiness and thus the acceptance of autonomous vehicles. Likewise, the behavior of the vehicles must be interpretable, comprehensible and acceptable, not only for passengers but also for external parties.\nIV-B\nCountermeasures and Opportunities for Effective Transferability and Efficient Scalability\nWhile the previous subsection outlines challenges of prospect autonomous driving and derived needs, this subsection outlines opportunities alongside the consideration of the previous derived needs.\nAccording to the desirable human-like autonomy, it is preferable to consider the core functionalities for safe and admissible trajectory planning independent of countries and data sources. This idea could be addressed by training a baseline perception-to-trajectory model (P2T), e.g., on the basis of a SO-M-E2E stack, which is primarily designed to generate trajectories in a variety of situations. In this context, a P2T model could be trained similar to a language model, which first learns the language and then in a further step post-trains the meaning, correctness, and ethics. Nevertheless, classical language and foundation models are not capable of human-like internal planning. In contrast, the hypothetical architecture from Section\nIII\n, or more generally the SO-M-E2E architecture, takes existing submodules, adapts their use in the overall AD stack, and represents a first approach for a foreseeable applicable realization of objective-driven recurrent world model that offer human-like planning\n[\n307\n]\n. Consequently, in the course of the development of such a P2T system, the cognitive interfaces must initially be trained on the basis of the general data. This P2T world-model, as we call it, thus draws on a wide range of knowledge. However, such a model cannot correctly meet the requirements of a dedicated country-specific ODD. This is underscored by findings from a transfer learning study on motion prediction\n[\n324\n]\n, which suggests that multi-task learning performs less effectively across diverse contexts compared to targeted fine-tuning tailored to respective settings. Therefore, as with the human, adaptation to the ODD at hand is necessary. Furthermore, an adjustment to the system, the vehicle at hand, is also inevitable.\nThe adaptation to the ODD at hand requires data of the specific ODD. At the same time, the worldwide found trigger conditions from different ODDs should be taken up and considered. In this context, the current state of the art is a scenario-based approach that extracts, defines, and generates corresponding scenarios, often manually. To minimize human hand engineering and the corresponding human-based impact, an automated procedure would be desirable. In this course, the first goal is to transfer all relevant and critical cases of each ODD into the ODD at hand. The second goal is to provide a unified all-modality perception. In other words, every relevant type of data source considered in autonomous vehicles should be available.\nConsidering, for example, a critical situation detected in the USA by a Tesla, the corresponding recorded data would not contain radar nor lidar data, as Tesla pursues a vision only approach. However, the ODD of the system under development could indicate Germany, among others. Accordingly, the first goal is to transfer the critical situation to a German ODD. Therefore, the scenario is to be generated with German traffic signs and road markings. The second goal concerns the sensor modality, here the objective is that all modalities are available to use the data across divers vehicle setups. Consequently, synthetic radar and lidar data is also required, as the Tesla recorded dataset does not contain those. In order to avoid recreating each situation individually in a simulation environment, an automated procedure is proposed. This should generate synthetic scenarios that ultimately represent an artificial but realistic synthetic simulation. In the best case, a system is available that transfers all global trigger conditions to the ODD under development and artificially generates missing sensor signals. The result would be an ODD-specific master dataset that maximizes the provision of information. In this way, the challenges of different application areas and different legal and law enforcement areas could be taken into account. For instance, the realization of such a scenario generator is closely related to SE. Within the field of SE possibilities of advanced AI methodologies and systems, such as OpenAI’s Sora, are discussed in terms of imaginative intelligence\n[\n35\n]\n. In particular, the emerging imaginative intelligence offers great potential towards a scenario generator, and in general, a even more data-driven development and verification and validation in alignment to\n[\n270\n]\n.\nThereby, implicit considerations of expert knowledge about safe and ethical behavior in the respective operational area, e.g., Germany, would be worthwhile. Nevertheless, while transferring corresponding ODD-specific reasoning and decision-making to the system, it would be beneficial to eliminate the need for human explanation of relationships. The advantage of that would be, that rules do not have to be explicitly extracted and communicated to the system. Especially in corner cases, there is usually an overlay of rules where humans are able to make instant decisions based on situation understanding\n[\n160\n,\n9\n]\n. However, even humans have difficulty describing the correct behavior in tough situations based on rules. Moreover, there are usually several different rule reasonings that lead to very similar system behavior. Rather than extracting all the rules in each situation, prioritizing them, and possibly coming to a halt due to conflicting rules, the goal of the autonomous system is to find a safe and appropriate solution in each situation. Meaning, to realize safe instant decision making based on enhanced contextual awareness. Therefore, new approaches are needed for the integration and learning of high-level reasoning and decision making. In this context, broad accessibility and thus the social discussion of a permissible acceptable risks is also of significance. Overall, a procedure that simplifies the approval process while considering regulatory requirements, such as human oversight of EU AIA, along increasing social acceptance is worth striving for.\nWhile the process outlined so far illustrates how a model can be developed that operates according to specific traffic signs, traffic rules, behavioral rules, as well as the regulations of a particular ODD, the model still assumes all sensor modalities to be given. In a next step, the model that has achieved simulative safety on synthetic data can be adapted to a concrete system. Data from the system’s sensors can be used in this process. Since each manufacturer uses different sensor setups, there will likely be an adjustment in sensor modalities and data characteristics. During this model transfer, the model can be fine-tuned and optimized to work with the given sensor setup. Thereby, the methodology provides the ability to evaluate relevance and sensitivity for individual sensor modalities. Thus, minimum requirements for the sensors are implicitly stipulated. After certification of transfer reliability and implicit release of the sensor setup, evaluation and release can take place in the real vehicle. Thus, the methodology offers the possibility to adapt an ODD-specific P2T model to a variety of vehicles of different vehicle bodies, types and manufacturers.\nUltimately, it is conceivable to integrate the individual process steps into an iterative cycle, as outlined within\n[\n270\n]\n, that enables continuous refinement and adaptation to the changing real world. Moreover, it is demonstrated that data is a central component. Furthermore, the prospect enables resources to be pooled centrally and collectively drive the core of autonomy as well as define and ensure safety across the board. In terms of model transfer, each vendor can in turn access and fine-tune the system. In terms of governance, this creates incentives for data sharing, although critical data sharing is considered to be essential and mandatory according to divers regulations. Moreover, it is also imaginable to establish a federated learning concept in order to protect vendor- and customer-specifics despite sharing data in general.\nCentralization and concentration reduce technical effort. By pooling data, a better system might be created than any one individual can create, at a lower cost and in less time. Licensing fees can offset the cost of creating country-specific models while ensuring safety across all manufacturers. The world model and country-specific models could be developed and overseen by an international, interdisciplinary group. Thus, in the spirit of open innovation and close collaboration, companies, countries, and organizations could work together towards safer mobility.\nIn addition to the countermeasures and opportunities mentioned so far, a separate option is discussed below in order to do justice to the diversity of possibilities. These are large foundation models, which dispense with dedicated sub-modules by solving the entire task in an end-to-end manner. This seems to be radical at the moment, but in the past, recurrent models such as RNN, GRU, LSTM and also transformer models, which were initially introduced via natural language processing in the field of AI, have become very important in technical modeling. In line with this are the latest developments such as Nvidia’s Alpamayo-R1\n[\n258\n]\n. Ultimately, these approaches can also only be realized with a corresponding amount of data and computing power. In particular, the combination with few-shot learning techniques promises great potential, although these models are currently still difficult to interpret and construct, which is reflected, for example, in the large variance of the parameter sizes used. However, these methods are a possibility. And indeed, we can already see some first realizations of trajectory predictions\n[\n184\n]\nand autonomous driving in general\n[\n185\n]\n. In addition, LLM can also contribute to the human interface in the future of autonomous driving\n[\n325\n]\n. Although this seems promising, we share the view of\n[\n307\n]\n, stating that a human-like AI requires a higher level of intelligence and must be able to plan and reasoning internally.\nV\nDiscussion\nWhile there is a large number of review articles in the field of AD\n[\n146\n,\n60\n,\n59\n,\n326\n]\n, the rapid technological advancement requires a continuous review of the field. In this regard, the significant development of AI methods\n[\n22\n,\n76\n,\n327\n,\n328\n]\nare leading to breakthroughs in the field of AD. Accordingly, existing approaches are constantly being reassessed. Recently, FMs such as LLMs, VLMs, or VLAs have raised new possibilities and questions. First reviews already deal with these questions\n[\n183\n,\n329\n,\n182\n,\n258\n]\n. At the same time, it is becoming apparent that the difference between the AD systems currently being researched and the more classic, modular, service-oriented AD systems is widening. Research is also very fragmented. Accordingly, a large number of reviews look at individual sub-research areas such as functional subareas\n[\n166\n,\n330\n,\n116\n,\n4\n,\n96\n,\n221\n]\n, V2X connectivity\n[\n134\n,\n147\n]\nand collaboration\n[\n141\n,\n298\n]\n, and more\n[\n88\n,\n331\n,\n79\n]\n. In addition, most of the studies are in the predefined SAE automation stages.\nThis review therefore differs from existing reviews in several dimensions. Firstly, it builds a bridge between service-oriented AD stack systems and the latest developments. Building on this, it anticipates opportunities and the resulting challenges. While the focus is on the functionalities and implications of AI, it also builds a bridge to broader aspects such as transferability, scalability as well as socio-political aspects. Finally, this review also distinguishes itself through its in-depth SA and cognition-guided analysis of existing capabilities and necessary developments. This functional and cognitive perspective facilitates decoupling the underlying system requirements and systematic challenges from technical solutions to maintain validity for further developments. At the same time, the current technological possibilities are also taken into account. This functional and holistic approach across several dimensions distinguishes the present study from existing ones and thus offers a new perspective on the field.\nVI\nConclusion\nIt was shown that the modular service-oriented software architecture is increasingly complemented by AI methods and thus performance can be improved, but certain structural limitations are still integral. The analysis highlighted that increased autonomy requires particularly to address the challenge of the open long-tail distribution of corner cases in the real world and therefore the overcoming of rule-based approaches and system orchestration. Furthermore, this revealed that SA is of particular importance in order to imitate humans in terms of dynamic, adaptive, and context-specific decision-making. At the same time, the analysis resulted in relevant requirements that are necessary for corresponding improvements.\nA possible architecture that takes SA integral into account was conceptualized based on the findings. This concept considers attention-based mechanisms as a central enabler that ensures the desired flexibility, adaptivity, and context specificity at the orchestration level without relying on human-extracted and hand-crafted rules. This is promising for achieving the desired human-like responsiveness in corner cases that are previously unknown. At the same time, the possible architecture raises open research questions regarding its realization. Primarily with regard to the generation, aggregation, and use of related data for the training of such a cognitive orchestrator. Possibilities such as federated learning or end-to-end learning have also been identified in this regard.\nIn relation to the existing AD stack, the proposed SO-M-E2E architecture can be interpreted as a coherent extension that integrates existing knowledge and functionalities. Also with regard to the quality vectors proposed in\n[\n153\n]\n, it can be considered as a possible extension, as the cognitive orchestration ultimately resembles quality vectors in an integral, human-like manner. In addition, further challenges and opportunities from regulation and socio-political issues to commercial aspects of scalability and transferability were examined and open research questions outlined.\nIn general, it can be seen that AI offers many opportunities at a functional perspective, but also raises corresponding research questions. Overall, however, it is clear that AI and the use of AI is a promising tool for achieving previously unattainable levels of autonomy, albeit with implications for the AD stack as well as respective safeguarding, transferability, and scalability. Accordingly, a large number of open research questions have been identified, which need to be addressed in the future while promising significant added value.\nReferences\n[1]\nT. J. Crayton and B. M. Meier, “Autonomous vehicles: Developing a public\nhealth research agenda to frame the future of transportation policy,”\nJournal of Transport & Health (JTH)\n, vol. 6, pp. 245–252, 2017.\n[2]\nE. Liebemann, K. Meder, J. Schuh\net al.\n, “Safety and performance\nenhancement: The Bosch electronic stability control (ESP),” SAE Technical\nPaper 2004-21-0060, Tech. Rep., 2004.\n[3]\nH. Winner, B. Danner, and J. Steinle, “Adaptive Cruise Control,” in\nHandbuch Fahrerassistenzsysteme: Grundlagen, Komponenten und Systeme\nfür aktive Sicherheit und Komfort\n, A. Eskandarian, Ed. Berlin, Germany: Springer, 2012.\n[4]\nA. Gupta, A. Anpalagan, L. Guan\net al.\n, “Deep learning for object\ndetection and scene perception in self-driving cars: Survey, challenges, and\nopen issues,”\nArray\n, vol. 10, pp. 1–20, 2021, Art. no. 100057.\n[5]\nS. Guo, S. Wang, Z. Yang\net al.\n, “A Review of Deep Learning-Based\nVisual Multi-Object Tracking Algorithms for Autonomous Driving,”\nAppl. Sci.\n, vol. 12, no. 21, pp. 1–27, 2022, Art. no. 10741.\n[6]\nL. Claussmann, M. Revilloud, S. Glaser\net al.\n, “A Study on AI-based\nApproaches for High-Level Decision Making in Highway Autonomous Driving,”\nin\nProc. IEEE Int. Conf. Syst. Man Cybern. (SMC)\n, 2017, pp.\n3671–3676.\n[7]\nE. Leurent, “Safe and Efficient Reinforcement Learning for Behavioural\nPlanning in Autonomous Driving,” Ph.D. dissertation, Université de\nLille, 2020.\n[8]\nA. Filos, P. Tigkas, R. McAllister\net al.\n, “Can Autonomous Vehicles\nIdentify, Recover From, and Adapt to Distribution Shifts?” in\nProc.\n37th Int. Conf. Mach. Learn. (ICML)\n. PMLR, 2020, pp. 3145–3153.\n[9]\nJ. Canas, J. Quesada, A. Antolí\net al.\n, “Cognitive flexibility\nand adaptability to environmental changes in dynamic complex problem-solving\ntasks,”\nErgonomics\n, vol. 46, no. 5, pp. 482–501, 2003.\n[10]\nS. Burton and B. Herd, “Addressing uncertainty in the safety assurance of\nmachine-learning,”\nFront. Comput. Sci.\n, vol. 5, pp. 1–17, 2023,\nArt. no. 1132580.\n[11]\nA.-M. Hellmund, S. Wirges, Ö. Ş. Taş\net al.\n, “Robot\nOperating System: A Modular Software Framework for Automated Driving,” in\nProc. IEEE 19th Int. Conf. Intell. Transp. Syst. (ITSC)\n, 2016, pp.\n1564–1570.\n[12]\nS. Kugele, P. Obergfell, M. Broy\net al.\n, “On service-orientation for\nautomotive software,” in\n2017 IEEE International Conference on\nSoftware Architecture (ICSA)\n, 2017, pp. 193–202.\n[13]\nM. Horn, N. Engel, V. Belagiannis\net al.\n, “DeepCLR:\nCorrespondence-Less Architecture for Deep End-to-End Point Cloud\nRegistration,” in\nProc. 23rd IEEE Int. Conf. Intell. Transp. Syst.\n(ITSC))\n, 2020, pp. 1–7.\n[14]\nL. Claussmann, M. Revilloud, D. Gruyer\net al.\n, “A Review of Motion\nPlanning for Highway Autonomous Driving,”\nIEEE Trans. Intell. Transp.\nSyst.\n, vol. 21, no. 5, pp. 1826–1848, 2019.\n[15]\nA. V. S. Neto, J. B. Camargo, J. R. Almeida\net al.\n, “Safety Assurance\nof Artificial Intelligence-Based Systems: A Systematic Literature Review on\nthe State of the Art and Guidelines for Future Work,”\nIEEE Access\n,\nvol. 10, pp. 130 733–130 770, 2022.\n[16]\nA. Kuznietsov, B. Gyevnar, C. Wang\net al.\n, “Explainable AI for Safe\nand Trustworthy Autonomous Driving: A Systematic Review,” 2024. [Online].\nAvailable:\nhttps://arxiv.org/abs/2402.10086\n[17]\nL. Chen, P. Wu, K. Chitta\net al.\n, “End-to-end Autonomous Driving:\nChallenges and Frontiers,”\nIEEE Trans. Pattern Anal. Mach. Intell.\n,\n2024.\n[18]\nB. Romera-Paredes and P. Torr, “An embarrassingly simple approach to\nzero-shot learning,” in\nProc. 32nd Int. Conf. Mach. Learn.\n(ICML)\n. PMLR, 2015, pp. 2152–2161.\n[19]\nW. Wang, V. W. Zheng, H. Yu\net al.\n, “A Survey of Zero-Shot Learning:\nSettings, Methods, and Applications,”\nACM Trans. Intell. Syst.\nTechnol.\n, vol. 10, no. 2, pp. 1–37, 2019, Art. no. 13.\n[20]\nL. Fei-Fei, R. Fergus, and P. Perona, “One-shot learning of object\ncategories,”\nIEEE Trans. Pattern Anal. Mach. Intell.\n, vol. 28,\nno. 4, pp. 594–611, 2006.\n[21]\nO. Vinyals, C. Blundell, T. Lillicrap\net al.\n, “Matching Networks for\nOne Shot Learning,” in\nProc. 29th Int. Conf. Neural Inf. Process.\nSyst. (NeurIPS)\n, 2016.\n[22]\nY. Wang, Q. Yao, J. T. Kwok\net al.\n, “Generalizing from a Few Examples:\nA Survey on Few-shot Learning,”\nACM Comput. Surv.\n, vol. 53, no. 3,\npp. 1–34, 2020.\n[23]\nS. Kadam and V. Vaidya, “Review and Analysis of Zero, One and Few Shot\nLearning Approaches,” in\nProc. 18th Int. Conf. Intell. Syst. Design\nAppl. (ISDA)\n. Springer, 2018, pp.\n100–112.\n[24]\nS. Hochreiter, A. S. Younger, and P. R. Conwell, “Learning to Learn Using\nGradient Descent,” in\nProc. 11st Int. Conf. Art. Neural Netw.\n(ICANN)\n. Springer, 2001, pp. 87–94.\n[25]\nC. Finn, P. Abbeel, and S. Levine, “Model-Agnostic Meta-Learning for Fast\nAdaptation of Deep Networks,” in\nProc. 34th Int. Conf. Mach. Learn.\n(ICML)\n. PMLR, 2017, pp. 1126–1135.\n[26]\nJ. Vanschoren, “Meta-learning,” in\nAutomated Machine Learning:\nMethods, Systems, Challenges\n. Springer, 2019, pp. 35–61.\n[27]\nA. Dubey, V. Ramanathan, A. Pentland\net al.\n, “Adaptive methods for\nreal-world domain generalization,” in\nProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n, 2021, pp.\n14 340–14 349.\n[28]\nT. Maharaj, “Generalizing in the Real World with Representation Learning,”\nPh.D. dissertation, Polytechnique Montréal, 2022.\n[29]\nOpenAI, J. Achiam, S. Adler\net al.\n, “GPT-4 Technical Report,” 2023.\n[30]\nH. Touvron, T. Lavril, G. Izacard\net al.\n, “LLaMA: Open and Efficient\nFoundation Language Models,”\narXiv preprint arXiv:2302.13971\n, 2023.\n[31]\nA. Chowdhery, S. Narang, J. Devlin\net al.\n, “PaLM: Scaling Language\nModeling with Pathways ,”\nJ. Mach. Learn. Res.\n, vol. 24, no. 240,\npp. 1–113, 2023.\n[32]\nA. Kirillov, E. Mintun, N. Ravi\net al.\n, “Segment Anything,” pp.\n4015–4026, October 2023.\n[33]\nZ. Liu, H. Hu, Y. Lin\net al.\n, “Swin Transformer V2: Scaling Up\nCapacity and Resolution,” in\nProc. IEEE/CVF Comput. Soc. Conf.\nComput. Vis. Pattern Recognit. (CVPR)\n, June 2022, pp. 12 009–12 019.\n[34]\nX. Li, Y. Tian, P. Ye\net al.\n, “A Novel Scenarios Engineering\nMethodology for Foundation Models in Metaverse,”\nIEEE Trans. Syst.\nMan Cybern.: Syst.\n, vol. 53, no. 4, pp. 2148–2159, 2022.\n[35]\nF.-Y. Wang, Q. Miao, L. Li\net al.\n, “When Does Sora Show: The Beginning\nof TAO to Imaginative Intelligence and Scenarios Engineering,”\nIEEE/CAA J. Autom. Sin.\n, vol. 11, no. 4, pp. 809–815, 2024.\n[36]\nC. Raffel, N. Shazeer, A. Roberts\net al.\n, “Exploring the Limits of\nTransfer Learning with a Unified Text-to-Text Transformer,”\nJ. Mach.\nLearn. Res.\n, vol. 21, no. 140, pp. 1–67, 2020.\n[37]\nL. Floridi and M. Chiriatti, “GPT-3: Its Nature, Scope, Limits, and\nConsequences,”\nMinds Mach.\n, vol. 30, pp. 681–694, 2020.\n[38]\nE. Kasneci, K. Seßler, S. Küchemann\net al.\n, “ChatGPT for good?\nOn opportunities and challenges of large language models for education,”\nLearn. Individ. Differ.\n, vol. 103, pp. 1–9, 2023, Art. no. 102274.\n[39]\nA. Kampmann, B. Alrifaee, M. Kohout\net al.\n, “A Dynamic\nService-Oriented Software Architecture for Highly Automated Vehicles,” in\nProc. 22th IEEE Int. Conf. Intell. Transp. Syst. (ITSC)\n, 2019, pp.\n2101–2108.\n[40]\nJ. Becker, M. Sagar, and D. Pangercic, “A Safety-Certified Vehicle OS to\nEnable Software-Defined Vehicles,” in\nAutomatisiertes Fahren 2021\n,\nT. Bertram, Ed. Wiesbaden: Springer\nFachmedien Wiesbaden, 2021, pp. 51–67.\n[41]\nH. Zhu, W. Zhou, Z. Li\net al.\n, “Requirements-Driven Automotive\nElectrical/Electronic Architecture: A Survey and Prospective Trends,”\nIEEE Access\n, vol. 9, pp. 100 096–100 112, 2021.\n[42]\nA. Geiger, P. Lenz, C. Stiller\net al.\n, “Vision meets robotics: The\nKITTI dataset,”\nInt. J. Robot. Res.\n, vol. 32, no. 11, pp.\n1231–1237, 2013.\n[43]\nX. Huang, P. Wang, X. Cheng\net al.\n, “The ApolloScape Open Dataset for\nAutonomous Driving and Its Application,”\nIEEE Trans. Pattern Anal.\nMach. Intell.\n, vol. 42, no. 10, pp. 2702–2719, 2019.\n[44]\nH. Caesar, V. Bankiti, A. H. Lang\net al.\n, “nuScenes: A Multimodal\nDataset for Autonomous Driving,” in\nProc. IEEE/CVF Comput. Soc. Conf.\nComput. Vis. Pattern Recognit. (CVPR)\n, 2020, pp. 11 621–11 631.\n[45]\nP. Sun, H. Kretzschmar, X. Dotiwalla\net al.\n, “Scalability in\nPerception for Autonomous Driving: Waymo Open Dataset,” in\nProc.\nIEEE/CVF Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)\n, 2020, pp.\n2446–2454.\n[46]\nM.-F. Chang, J. Lambert, P. Sangkloy\net al.\n, “Argoverse: 3D Tracking\nand Forecasting With Rich Maps,” in\nProc. IEEE/CVF Comput. Soc. Conf.\nComput. Vis. Pattern Recognit. (CVPR)\n, 2019, pp. 8748–8757.\n[47]\nW. Zhan, L. Sun, D. Wang\net al.\n, “INTERACTION Dataset: An\nINTERnational, Adversarial and Cooperative moTION Dataset in Interactive\nDriving Scenarios with Semantic Maps,”\narXiv preprint\narXiv:1910.03088\n, 2019.\n[48]\nA. Malinin, N. Band, Y. Gal\net al.\n, “Shifts: A Dataset of Real\nDistributional Shift Across Multiple Large-Scale Tasks,” in\nProc.\n35th Int. Conf. Neural Inf. Process. Syst. (NeurIPS)\n, 2021, pp. 1–15.\n[49]\nJ. Houston, G. Zuidhof, L. Bergamini\net al.\n, “One Thousand and One\nHours: Self-driving Motion Prediction Dataset,” in\nProc. Conf. on\nRob. Learn.\nPMLR, 2021, pp. 409–418.\n[50]\nH. Caesar, J. Kabzan, K. S. Tan\net al.\n, “NuPlan: A closed-loop\nML-based planning benchmark for autonomous vehicles,”\narXiv preprint\narXiv:2106.11810\n, 2021.\n[51]\nS. Wirges, M. Reith-Braun, M. Lauer\net al.\n, “Capturing Object\nDetection Uncertainty in Multi-Layer Grid Maps,” in\nIEEE Intell. Veh.\nSymp. Proc. (IV)\n, 2019, pp. 1520–1526.\n[52]\nD. Feng, C. Haase-Schütz, L. Rosenbaum\net al.\n, “Deep Multi-Modal\nObject Detection and Semantic Segmentation for Autonomous Driving: Datasets,\nMethods, and Challenges,”\nIEEE Trans. Intell. Transp. Syst.\n,\nvol. 22, no. 3, pp. 1341–1360, 2020.\n[53]\nC. Luo, X. Yang, and A. Yuille, “Exploring Simple 3D Multi-Object Tracking\nfor Autonomous Driving,” in\nProc. IEEE/CVF Int. Conf. Comput. Vis.\n(ICCV)\n, October 2021, pp. 10 488–10 497.\n[54]\nH.-k. Chiu, J. Li, R. Ambruş\net al.\n, “Probabilistic 3D\nMulti-Modal, Multi-Object Tracking for Autonomous Driving,” in\nProc.\nIEEE Int. Conf. Robot. Autom. (ICRA)\n. IEEE, 2021, pp. 14 227–14 233.\n[55]\nZ. Ding, Y. Hu, R. Ge\net al.\n, “1st Place Solution for Waymo Open\nDataset Challenge – 3D Detection and Domain Adaptation,”\narXiv\npreprint arXiv:2006.15505\n, 2020.\n[56]\nB. Varadarajan, A. Hefny, A. Srivastava\net al.\n, “MultiPath++:\nEfficient Information Fusion and Trajectory Aggregation for Behavior\nPrediction,” in\nProc. IEEE Int. Conf. Robot. Autom. (ICRA)\n, 2022,\npp. 7814–7821.\n[57]\nH. Liu, Z. Huang, and C. Lv, “Multi-modal Hierarchical Transformer for\nOccupancy Flow Field Prediction in Autonomous Driving,” in\nProc. IEEE\nInt. Conf. Robot. Autom. (ICRA)\n. IEEE, 2023, pp. 1449–1455.\n[58]\nS. Casas, C. Gulino, S. Suo\net al.\n, “Implicit Latent Variable Model\nfor Scene-Consistent Motion Forecasting,” in\nProc. 16th Eur. Conf.\nComput. Vis. (ECCV)\n. Springer, 2020,\npp. 624–641.\n[59]\nS. Grigorescu, B. Trasnea, T. Cocias\net al.\n, “A survey of deep\nlearning techniques for autonomous driving,”\nJ. Field Robot.\n,\nvol. 37, no. 3, pp. 362–386, 2020.\n[60]\nY. Ma, Z. Wang, H. Yang\net al.\n, “Artificial Intelligence Applications\nin the Development of Autonomous Vehicles: A Survey,”\nIEEE/CAA J.\nAutom. Sin.\n, vol. 7, no. 2, pp. 315–329, 2020.\n[61]\nP. Karle, M. Geisslinger, J. Betz\net al.\n, “Scenario Understanding and\nMotion Prediction for Autonomous Vehicles—Review and Comparison,”\nIEEE Trans. Intell. Transp. Syst.\n, vol. 23, no. 10, pp.\n16 962–16 982, 2022.\n[62]\nJ. Stallkamp, M. Schlipsing, J. Salmen\net al.\n, “Man vs. computer:\nBenchmarking machine learning algorithms for traffic sign recognition,”\nNeural Netw.\n, vol. 32, pp. 323–332, 2012.\n[63]\nD. Tabernik and D. Skočaj, “Deep Learning for Large-Scale Traffic-Sign\nDetection and Recognition,”\nIEEE Trans. Intell. Transp. Syst.\n,\nvol. 21, no. 4, pp. 1427–1440, 2019.\n[64]\nL. Abdi and A. Meddeb, “Deep Learning Traffic Sign Detection, Recognition and\nAugmentation,” in\nProc. ACM Symp. Appl. Comput.\n, 2017, pp. 131–136.\n[65]\nS. Murugan, A. Sampathkumar, S. Kanaga Suba Raja\net al.\n, “Autonomous\nVehicle Assisted by Heads up Display (HUD) with Augmented Reality Based on\nMachine Learning Techniques,” in\nVirtual and Augmented Reality for\nAutomobile Industry: Innovation Vision and Applications\n. Cham: Springer, 2022, pp. 45–64. [Online].\nAvailable:\nhttps://doi.org/10.1007/978-3-030-94102-4_3\n[66]\nM. Cunneen, M. Mullins, and F. Murphy, “Autonomous Vehicles and Embedded\nArtificial Intelligence: The Challenges of Framing Machine Driving\nDecisions,”\nAppl. Artif. Intell.\n, vol. 33, no. 8, pp. 706–731,\n2019.\n[67]\nA. Taeihagh, “Governance of artificial intelligence,”\nPolicy Soc.\n,\nvol. 40, no. 2, pp. 137–157, 2021.\n[68]\nY. Dajsuren and M. van den Brand,\nAutomotive Systems and Software\nEngineering – State of the Art and Future Trends\n. Berlin, Germany: Springer, 2019.\n[69]\nC. Gómez-Huélamo, A. Diaz-Diaz, J. Araluce\net al.\n, “How to\nbuild and validate a safe and reliable Autonomous Driving stack? A ROS based\nsoftware modular architecture baseline,” in\nIEEE Intell. Veh. Symp.\nProc. (IV)\n, 2022, pp. 1282–1289.\n[70]\nJ. Philion, A. Kar, and S. Fidler, “Learning to Evaluate Perception Models\nUsing Planner-Centric Metrics,” in\nProc. IEEE/CVF Comput. Soc. Conf.\nComput. Vis. Pattern Recognit. (CVPR)\n, 2020, pp. 14 055–14 064.\n[71]\nB. Klamann and H. Winner, “Introducing the detailed semantic interface\ndescription to support a modular safety approval of automated vehicles –\nS2I2,” in\nSafety and Reliability\n, vol. 42, no. 1. Taylor & Francis, 2023, pp. 16–55.\n[72]\nT. Stolte, R. Graubohm, I. Jatzkowski\net al.\n, “Towards safety concepts\nfor automated vehicles by the example of the project UNICARagil,” in\nProc. of 29th Aachen Colloquium Sustainable Mobility\n, 2020.\n[73]\nH. Guissouma, C. P. Hohl, F. Lesniak\net al.\n, “Lifecycle Management of\nAutomotive Safety-Critical Over the Air Updates: A Systems Approach,”\nIEEE Access\n, vol. 10, pp. 57 696–57 717, 2022.\n[74]\nD. Amodei, C. Olah, J. Steinhardt\net al.\n, “Concrete Problems in AI\nSafety,” 2016. [Online]. Available:\nhttps://arxiv.org/abs/1606.06565\n[75]\nH. Tabani, L. Kosmidis, J. Abella\net al.\n, “Assessing the Adherence of\nan Industrial Autonomous Driving Framework to ISO 26262 Software\nGuidelines,” in\nProceedings of the 56th Annual Design Automation\nConference 2019\n, 2019, pp. 1–6.\n[76]\nF. A. Batarseh, L. Freeman, and C.-H. Huang, “A survey on artificial\nintelligence assurance,”\nJ. Big Data\n, vol. 8, no. 60, pp. 1–30,\n2021.\n[77]\nN. Díaz-Rodríguez, J. Del Ser, M. Coeckelbergh\net al.\n,\n“Connecting the dots in trustworthy Artificial Intelligence: From AI\nprinciples, ethics, and key requirements to responsible AI systems and\nregulation,”\nInf. Fusion\n, vol. 99, pp. 1–24, 2023, Art. no.\n101896.\n[78]\nA. Dafoe, “AI Governance: A Research Agenda,”\nGovernance of AI\nProgram, Future of Humanity Inst., Univ. Oxford, Oxford, U.K.\n, vol. 1442,\npp. 1–54, 2018, Art. no. 1443.\n[79]\nL. Ullrich, M. Buchholz, K. Dietmayer\net al.\n, “AI Safety Assurance for\nIntelligent Vehicles: A Survey on Research, Standardization, Regulation,”\nIEEE Trans. Intell. Veh.\n, 2024.\n[80]\nS. Atakishiyev, M. Salameh, H. Yao\net al.\n, “Explainable Artificial\nIntelligence for Autonomous Driving: A Comprehensive Overview and Field Guide\nfor Future Research Directions,”\nIEEE Access\n, 2024.\n[81]\nS. Atakishiyev, M. Salameh, and R. Goebel, “Safety implications of explainable\nartificial intelligence in end-to-end autonomous driving,”\narXiv\npreprint arXiv:2403.12176\n, 2024.\n[82]\nS. Atakishiyev, M. Salameh, and R. Goebel, “Incorporating Explanations into\nHuman-Machine Interfaces for Trust and Situation Awareness in Autonomous\nVehicles,” in\nProc. IEEE Intell. Veh. Symp. (IV)\n, 2024, pp.\n2948–2955.\n[83]\nL. Sanneman and J. A. Shah, “The Situation Awareness Framework for\nExplainable AI (SAFE-AI) and Human Factors Considerations for XAI Systems,”\nInt. J. Hum.-Comput. Interact.\n, vol. 38, no. 18-20, pp. 1772–1788,\n2022.\n[84]\nM. R. Endsley, “Design and Evaluation for Situation Awareness Enhancement,”\nin\nProc. Hum. Factors Soc. Annu. Meet\n, vol. 32, no. 2. Sage Publications Sage CA: Los Angeles, CA, 1988, pp.\n97–101.\n[85]\nM. R. Endsley, “Toward a theory of situation awareness in dynamic systems,”\nHuman factors\n, vol. 37, no. 1, pp. 32–64, 1995.\n[86]\nD. Sirkin, N. Martelaro, M. Johns\net al.\n, “Toward Measurement of\nSituation Awareness in Autonomous Vehicles,” in\nConf. Hum. Factors\nComput. Syst. - Proc.\n, 2017, pp. 405–415.\n[87]\nM. Gerwien, A. Jungmann, and R. Voßwinkel, “Towards Situation-Aware\nDecision-Making for Automated Driving,” in\nProc. IEEE 7th Int. Conf.\nAutom. Robot. Appl. (ICARA)\n, 2021, pp. 185–189.\n[88]\nH. A. Ignatious, H. El-Sayed, M. A. Khan\net al.\n, “Analyzing Factors\nInfluencing Situation Awareness in Autonomous Vehicles—A Survey,”\nSensors\n, vol. 23, no. 8, p. 4075, 2023, Art. no. 4075.\n[89]\nE. Romera, J. M. Alvarez, L. M. Bergasa\net al.\n, “ERFNet: Efficient\nResidual Factorized ConvNet for Real-Time Semantic Segmentation,”\nIEEE Trans. Intell. Transp. Syst.\n, vol. 19, no. 1, pp. 263–272, 2017.\n[90]\nA. Wischnewski, M. Geisslinger, J. Betz\net al.\n, “Indy Autonomous\nChallenge - Autonomous Race Cars at the Handling Limits,” in\n12th\nInternational Munich Chassis Symposium\n. Berlin, Heidelberg: Springer Berlin Heidelberg, 2022, pp.\n163–182.\n[91]\nC.-Y. Wang, A. Bochkovskiy, and H.-Y. M. Liao, “YOLOv7: Trainable\nBag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors,”\nin\nProc. IEEE/CVF Comput. Soc. Conf. Comput. Vis. Pattern Recognit.\n(CVPR)\n, 2023, pp. 7464–7475.\n[92]\nA. Munir, A. Aved, and E. Blasch, “Situational Awareness: Techniques,\nChallenges, and Prospects,”\nAI\n, vol. 3, no. 1, pp. 55–77, 2022.\n[93]\nM. Ravanbakhsh, M. Baydoun, D. Campo\net al.\n, “Learning Multi-Modal\nSelf-Awareness Models for Autonomous Vehicles from Human Driving,” in\nProc. IEEE 21st Int. Conf. Inf. Fusion (FUSION)\n, 2018, pp. 1866–1873.\n[94]\nM. Henning, J. Müller, F. Gies\net al.\n, “Situation-Aware\nEnvironment Perception Using a Multi-Layer Attention Map,”\nIEEE\nTrans. Intell. Veh.\n, vol. 8, no. 1, pp. 481–491, 2022.\n[95]\nM. R. Endsley, “Situation Awareness in Driving,”\nHandbook of Human\nFactors for Automated, Connected, and Intelligent Vehicles, London: Taylor\nand Francis\n, 2020.\n[96]\nY. Huang, J. Du, Z. Yang\net al.\n, “A Survey on Trajectory-Prediction\nMethods for Autonomous Driving,”\nIEEE Transactions on Intelligent\nVehicles\n, vol. 7, no. 3, pp. 652–674, 2022.\n[97]\nL. Tolksdorf, C. Birkner, A. Tejada\net al.\n, “Fast Collision\nProbability Estimation for Automated Driving using Multi-circular Shape\nApproximations,”\narXiv preprint arXiv:2405.10765\n, 2024.\n[98]\nM. Althoff, O. Stursberg, and M. Buss, “Model-Based Probabilistic Collision\nDetection in Autonomous Driving,”\nIEEE Trans. Intell. Transp. Syst.\n,\nvol. 10, no. 2, pp. 299–310, 2009.\n[99]\nW. Schwarting, J. Alonso-Mora, L. Paull\net al.\n, “Safe Nonlinear\nTrajectory Generation for Parallel Autonomy With a Dynamic Vehicle Model,”\nIEEE Trans. Intell. Transp. Syst.\n, vol. 19, no. 9, pp. 2994–3008,\n2017.\n[100]\nL. Liu, S. Lu, R. Zhong\net al.\n, “Computing Systems for Autonomous\nDriving: State of the Art and Challenges,”\nIEEE Internet Things J.\n,\nvol. 8, no. 8, pp. 6469–6486, 2020.\n[101]\nR. Bajcsy, Y. Aloimonos, and J. K. Tsotsos, “Revisiting active perception,”\nAutonomous Robots\n, vol. 42, pp. 177–196, 2018.\n[102]\nA. Rasouli, P. Lanillos, G. Cheng\net al.\n, “Attention-based active\nvisual search for mobile robots,”\nAutonomous Robots\n, vol. 44, no. 2,\npp. 131–146, 2020.\n[103]\nA. Pal, S. Mondal, and H. I. Christensen, “\"looking at the right stuff\" -\nguided semantic-gaze for autonomous driving,” in\nProc. IEEE/CVF\nComput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)\n, June 2020.\n[104]\nW. Xu, J. Snider, J. Wei\net al.\n, “Context-Aware Tracking of Moving\nObjects for Distance Keeping,” in\nProc. IEEE Intell. Veh. Symp.\n(IV)\n, 2015, pp. 1380–1385.\n[105]\nY. Nager, A. Censi, and E. Frazzoli, “What lies in the shadows? safe and\ncomputation-aware motion planning for autonomous vehicles using intent-aware\ndynamic shadow regions,” in\nProc. IEEE Int. Conf. Robot. Autom.\n(ICRA)\n, 2019, pp. 5800–5806.\n[106]\nB. Cheng, I. Misra, A. G. Schwing\net al.\n, “Masked-Attention Mask\nTransformer for Universal Image Segmentation,” in\nProc. IEEE/CVF\nComput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)\n, 2022, pp.\n1290–1299.\n[107]\nZ. Li, W. Wang, H. Li\net al.\n, “BEVFormer: Learning Bird’s-Eye-View\nRepresentation from Multi-camera Images via Spatiotemporal Transformers,”\nin\nProc. Eur. Conf. Comput. Vis. (ECCV)\n. Springer, 2022, pp. 1–18.\n[108]\nZ. Liu, H. Tang, A. Amini\net al.\n, “BEVFusion: Multi-Task Multi-Sensor\nFusion with Unified Bird’s-Eye View Representation,” in\nProc. IEEE\nInt. Conf. Robot. Autom. (ICRA)\n, 2023, pp. 2774–2781.\n[109]\nY. Zhang, Z. Zhu, and D. Du, “OccFormer: Dual-path Transformer for\nVision-based 3D Semantic Occupancy Prediction,” in\nProc. IEEE/CVF\nInt. Conf. Comput. Vis. (ICCV)\n, 2023, pp. 9433–9443.\n[110]\nY. Huang, W. Zheng, Y. Zhang\net al.\n, “Tri-Perspective View for\nVision-Based 3D Semantic Occupancy Prediction,” in\nProc. IEEE/CVF\nComput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)\n, 2023, pp.\n9223–9232.\n[111]\nD. Wei, T. Gao, Z. Jia\net al.\n, “BEV-CLIP: Multi-modal BEV Retrieval\nMethodology for Complex Scene in Autonomous Driving,”\narXiv preprint\narXiv:2401.01065\n, 2024.\n[112]\nG. Chen, H. Cao, J. Conradt\net al.\n, “Event-Based Neuromorphic Vision\nfor Autonomous Driving: A Paradigm Shift for Bio-Inspired Visual Sensing and\nPerception,”\nIEEE Signal Process. Mag.\n, vol. 37, no. 4, pp. 34–49,\n2020.\n[113]\nS. Ulbrich, T. Menzel, A. Reschka\net al.\n, “Defining and Substantiating\nthe Terms Scene, Situation, and Scenario for Automated Driving,” in\nProc. 18th IEEE Int. Conf. Intell. Transp. Syst. (ITSC)\n, 2015, pp.\n982–988.\n[114]\nP. Koopman and M. Wagner, “Challenges in Autonomous Vehicle Testing and\nValidation,”\nSAE Int. J. Transp. Saf.\n, vol. 4, no. 1, pp. 15–24,\n2016.\n[115]\nP. Koopman and M. Wagner, “Autonomous Vehicle Safety: An Interdisciplinary\nChallenge,”\nIEEE Intell. Transp. Syst. Mag.\n, vol. 9, no. 1, pp.\n90–96, 2017.\n[116]\nM. Gulzar, Y. Muhammad, and N. Muhammad, “A Survey on Motion Prediction of\nPedestrians and Vehicles for Autonomous Driving,”\nIEEE Access\n,\nvol. 9, pp. 137 957–137 969, 2021.\n[117]\nV. Trentin, A. Artuñedo, J. Godoy\net al.\n, “Multi-Modal\nInteraction-Aware Motion Prediction at Unsignalized Intersections,”\nIEEE Trans. Intell. Veh.\n, vol. 8, no. 5, pp. 3349–3365, 2023.\n[118]\nS. Shi, L. Jiang, D. Dai\net al.\n, “MTR++: Multi-Agent Motion Prediction\nwith Symmetric Scene Modeling and Guided Intention Querying,”\nIEEE\nTrans. Pattern Anal. Mach. Intell. (T-PAMI)\n, 2024.\n[119]\nY. Gan, H. Xiao, Y. Zhao\net al.\n, “MGTR: Multi-Granular Transformer for\nMotion Prediction with LiDAR,”\narXiv preprint arXiv:2312.02409\n,\n2023.\n[120]\nR. Mahjourian, J. Kim, Y. Chai\net al.\n, “Occupancy Flow Fields for\nMotion Forecasting in Autonomous Driving,”\nIEEE Robot. Autom. Lett.\n,\nvol. 7, no. 2, pp. 5639–5646, 2022.\n[121]\nB. Agro, Q. Sykora, S. Casas\net al.\n, “Implicit Occupancy Flow Fields\nfor Perception and Prediction in Self-Driving,” in\nProc. IEEE/CVF\nComput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)\n, 2023, pp.\n1379–1388.\n[122]\nW. Tong, C. Sima, T. Wang\net al.\n, “Scene as Occupancy,” in\nProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV)\n, 2023, pp. 8406–8415.\n[123]\nY. Liu, L. Mou, X. Yu\net al.\n, “Let Occ Flow: Self-Supervised 3D\nOccupancy Flow Prediction,” in\nProc. Conf. Robo. Learn. (CoRL)\n,\n2024, pp. 1–18.\n[124]\nS. Hagedorn, M. Hallgarten, M. Stoll\net al.\n, “Rethinking the\nIntegration of Prediction and Planning in Deep Learning-Based Automated\nDriving Systems: A Review,”\narXiv preprint arXiv:2308.05731\n, 2023.\n[125]\nC. Tang and R. R. Salakhutdinov, “Multiple Futures Prediction,” in\nProc. 32th Int. Conf. Neural Inf. Process. Syst. (NeurIPS)\n, 2019.\n[126]\nE. Schmerling, K. Leung, W. Vollprecht\net al.\n, “Multimodal\nProbabilistic Model-Based Planning for Human-Robot Interaction,” in\nProc. IEEE Int. Conf. Robot. Autom. (ICRA)\n, 2018, pp. 3399–3406.\n[127]\nW. Zeng, W. Luo, S. Suo\net al.\n, “End-To-End Interpretable Neural\nMotion Planner,” in\nProc. IEEE/CVF Comput. Soc. Conf. Comput. Vis.\nPattern Recognit. (CVPR)\n, 2019, pp. 8660–8669.\n[128]\nA. Sadat, S. Casas, M. Ren\net al.\n, “Perceive, Predict, and Plan: Safe\nMotion Planning Through Interpretable Semantic Representations,” in\nProc. 16th Eur. Conf. Comput. Vis. (ECCV)\n. Springer, 2020, pp. 414–430.\n[129]\nQ. Sun, X. Huang, J. Gu\net al.\n, “M2I: From Factored Marginal\nTrajectory Prediction to Interactive Prediction,” in\nProc. IEEE/CVF\nComput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)\n, 2022, pp.\n6543–6552.\n[130]\nN. Rhinehart, J. He, C. Packer\net al.\n, “Contingencies from\nObservations: Tractable Contingency Planning with Learned Behavior Models,”\nin\nProc. IEEE Int. Conf. Robot. Autom. (ICRA)\n, 2021, pp.\n13 663–13 669.\n[131]\nJ. M. Flach, “Situation awareness: Proceed with caution,”\nHuman\nFactors\n, vol. 37, no. 1, pp. 149–157, 1995.\n[132]\nJ. Kim, G. Bhatia, R. Rajkumar\net al.\n, “SAFER: System-level\nArchitecture for Failure Evasion in Real-time Applications,” in\nProc.\n33rd IEEE Real-Time Syst. Symp.\n, 2012, pp. 227–236.\n[133]\nJ. Schlatow, M. Moestl, and R. Ernst, “An Extensible Autonomous\nReconfiguration Framework for Complex Component-Based Embedded Systems,” in\n2015 IEEE Int. Conf. Autono. Comp.\n, 2015, pp. 239–242.\n[134]\nJ. Wang, J. Liu, and N. Kato, “Networking and Communications in Autonomous\nDriving: A Survey,”\nIEEE Commun. Surv. Tutor.\n, vol. 21, no. 2, pp.\n1243–1274, 2018.\n[135]\nH. Bagheri, M. Noor-A-Rahim, Z. Liu\net al.\n, “5G NR-V2X: Toward\nConnected and Cooperative Autonomous Driving,”\nIEEE Commun. Stand.\nMag.\n, vol. 5, no. 1, pp. 48–54, 2021.\n[136]\nB. Lampe, R. van Kempen, T. Woopen\net al.\n, “Reducing Uncertainty by\nFusing Dynamic Occupancy Grid Maps in a Cloud-based Collective Environment\nModel,” in\nIEEE Intell. Veh. Symp. Proc. (IV)\n, 2020, pp. 837–843.\n[137]\nJ. Godoy, V. Jiménez, A. Artuñedo\net al.\n, “A Grid-Based\nFramework for Collective Perception in Autonomous Vehicles,”\nSensors\n, vol. 21, no. 3, pp. 1–21, 2021, Art. no. 744.\n[138]\nG. Volk, Q. Delooz, F. A. Schiegg\net al.\n, “Towards Realistic\nEvaluation of Collective Perception for Connected and Automated Driving,”\nin\nProc. 24th IEEE Int. Conf. Intell. Transp. Syst. (ITSC)\n, 2021, pp.\n1049–1056.\n[139]\nK. Zhang, Y. Mao, S. Leng\net al.\n, “Mobile-Edge Computing for Vehicular\nNetworks: A Promising Network Paradigm with Predictive Off-Loading,”\nIEEE Veh. Technol. Mag.\n, vol. 12, no. 2, pp. 36–44, 2017.\n[140]\nM. Buchholz, J. Müller, M. Herrmann\net al.\n, “Handling Occlusions\nin Automated Driving Using a Multiaccess Edge Computing Server-Based\nEnvironment Model From Infrastructure Sensors,”\nIEEE Intell. Transp.\nSyst. Mag.\n, vol. 14, no. 3, pp. 106–120, 2021.\n[141]\nS. R. E. Datondji, Y. Dupuis, P. Subirats\net al.\n, “A survey of\nvision-based traffic monitoring of road intersections,”\nIEEE Trans.\nIntell. Transp. Syst.\n, vol. 17, no. 10, pp. 2681–2698, 2016.\n[142]\nT. Fleck, K. Daaboul, M. Weber\net al.\n, “Towards Large Scale Urban\nTraffic Reference Data: Smart Infrastructure in the Test Area Autonomous\nDriving Baden-Württemberg,” in\nProc. Intell. Auton. Sys.\n(IAS)\n. Springer, 2019, pp. 964–982.\n[143]\nB. Lampe, L. Reiher, T. Woopen\net al.\n, “Cloud Intelligence and\nCollective Learning for Automated and Connected Driving,”\nATZelectronics worldwide\n, vol. 17, no. 11, pp. 44–47, 2022.\n[144]\nÍ. B. Viana, H. Kanchwala, and N. Aouf, “Cooperative Trajectory Planning\nfor Autonomous Driving Using Nonlinear Model Predictive Control,” in\nProc. IEEE Int. Conf. Connected Veh. Expo (ICCVE)\n, 2019, pp. 1–6.\n[145]\nC. Burger, T. Schneider, and M. Lauer, “Interaction aware cooperative\ntrajectory planning for lane change maneuvers in dense traffic,” in\nProc. 23rd IEEE Int. Conf. Intell. Transp. Syst. (ITSC)\n, 2020, pp.\n1–8.\n[146]\nE. Yurtsever, J. Lambert, A. Carballo\net al.\n, “A Survey of Autonomous\nDriving: Common Practices and Emerging Technologies,”\nIEEE Access\n,\nvol. 8, pp. 58 443–58 469, 2020.\n[147]\nA. Alnasser, H. Sun, and J. Jiang, “Cyber security challenges and solutions\nfor V2X communications: A survey,”\nComputer Networks\n, vol. 151, pp.\n52–67, 2019.\n[148]\nK. Kim, J. S. Kim, S. Jeong\net al.\n, “Cybersecurity for autonomous\nvehicles: Review of attacks and defense,”\nComput. Secur.\n, vol. 103,\npp. 1–27, 2021, Art. no. 102150.\n[149]\nN. Arechiga, “Specifying Safety of Autonomous Vehicles in Signal Temporal\nLogic,” in\nIEEE Intell. Veh. Symp. Proc. (IV)\n, 2019, pp. 58–63.\n[150]\nM. Grobelna, J.-V. Zacchi, P. Schleiß\net al.\n, “Dynamic Risk\nManagement for Safely Automating Connected Driving Maneuvers,” in\nProc. IEEE 17th Euro. Depend. Comp. Conf. (EDCC)\n, 2021, pp. 9–16.\n[151]\nJ.-V. Zacchi and M. Trapp, “Towards Collaborative Perception in Automated\nDriving: Combining Vehicle and Infrastructure Perspectives,” in\nCARS\n2021 6th International Workshop on Critical Automotive Applications:\nRobustness & Safety\n, 2021.\n[152]\nS. Burton, I. Habli, T. Lawton\net al.\n, “Mind the gaps: Assuring the\nsafety of autonomous systems from an engineering, ethical, and legal\nperspective,”\nArtif. Intell.\n, vol. 279, pp. 1–16, 2020, Art. no.\n103201.\n[153]\nR. van Kempen, B. Lampe, M. Leuffen\net al.\n, “AUTOtech.agil:\nArchitecture and Technologies for Orchestrating Automotive Agility,” in\nProc. of 32nd Aachen Colloquium Sustainable Mobility\n, 2023.\n[154]\nZ. Liu, Z. Miao, X. Zhan\net al.\n, “Large-Scale Long-Tailed Recognition\nin an Open World,” in\nProc. IEEE/CVF Comput. Soc. Conf. Comput. Vis.\nPattern Recognit. (CVPR)\n, 2019, pp. 2537–2546.\n[155]\nB. Templeton, “Cruise Robotaxi Drives Into Wet Concrete; Waymo Shows Off Same\nRoute,” Forbes, August 2023. [Online]. Available:\nhttps://www.forbes.com/sites/bradtempleton/2023/08/17/cruise-robotaxi-drives-into-wet-concrete-waymo-shows-off-same-route/?sh=152367e96bfd\n[156]\nNational Highway Traffic Safety Administration (NHTSA), “Part 573 Safety\nRecall Report 23E-086,” Tech. Rep., November 2023, OMB Control No.:\n2127-0004. [Online]. Available:\nhttps://static.nhtsa.gov/odi/rcl/2023/RCLRPT-23E086-7725.PDF\n[157]\nD. Craft and S. Paul, “California regulator probes crashes involving GM’s\nCruise robotaxis,” Reuters, August 2023. [Online]. Available:\nhttps://www.reuters.com/business/autos-transportation/gms-cruise-robotaxi-collides-with-fire-truck-san-francisco-2023-08-19/\n[158]\nT. Keutgens, C. Klas, A. Zlocki\net al.\n, “Sound.AI – Teaching\nVehicles How to Hear,” in\nProc. of 28th Aachen Colloquium Sustainable\nMobility\n, 2019.\n[159]\nM. Kwade, A. Erraji, T. Böttcher\net al.\n, “The Hearing Sense of\nVehicles – Recognition of Sound Patterns in the Vehicle Environment for\nEnhanced Situation Awareness,” in\nProc. of 12th Aachen Acoustics\nColloquium\n, 2021.\n[160]\nB. Brehmer, “Dynamic decision making: Human control of complex systems,”\nActa Psychol.\n, vol. 81, no. 3, pp. 211–241, 1992.\n[161]\nJ. Feiler, S. Hoffmann, and F. Diermeyer, “Concept of a Control Center for an\nAutomated Vehicle Fleet,” in\nProc. 23rd IEEE Int. Conf. Intell.\nTransp. Syst. (ITSC)\n, 2020, pp. 1–6.\n[162]\nC. Urmson, J. Anhalt, D. Bagnell\net al.\n, “Autonomous driving in urban\nenvironments: Boss and the urban challenge,”\nJ. Field Robot.\n,\nvol. 25, no. 8, pp. 425–466, 2008.\n[163]\nJ. Levinson, J. Askeland, J. Becker\net al.\n, “Towards fully autonomous\ndriving: Systems and algorithms,” in\nIEEE Intell. Veh. Symp. Proc.\n(IV)\n. IEEE, 2011, pp. 163–168.\n[164]\nJ. Wei, J. M. Snider, J. Kim\net al.\n, “Towards a viable autonomous\ndriving research platform,” in\nIEEE Intell. Veh. Symp. Proc. (IV)\n,\n2013, pp. 763–770.\n[165]\nJ. Ziegler, P. Bender, M. Schreiber\net al.\n, “Making Bertha Drive—An\nAutonomous Journey on a Historic Route,”\nIntell. Transp. Syst. Mag.\n,\nvol. 6, no. 2, pp. 8–20, 2014.\n[166]\nB. Paden, M. Čáp, S. Z. Yong\net al.\n, “A survey of motion\nplanning and control techniques for self-driving urban vehicles,”\nIEEE\nTrans. Intell. Veh. (T-IV)\n, vol. 1, no. 1, pp. 33–55, 2016.\n[167]\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,”\nin\nProc. 1st Int. Conf. Neural Inf. Process. Syst. (NeurIPS)\n, 1988,\npp. 305–313.\n[168]\nM. Bojarski, “End to end learning for self-driving cars,”\narXiv\npreprint arXiv:1604.07316\n, 2016.\n[169]\nA. E. Sallab, M. Abdou, E. Perot\net al.\n, “Deep Reinforcement Learning\nframework for Autonomous Driving,”\nElectronic Imaging\n, vol. 29,\nno. 19, pp. 70–76, 2017.\n[170]\nF. Codevilla, M. Müller, A. López\net al.\n, “End-to-End Driving\nVia Conditional Imitation Learning,” in\nProc. IEEE Int. Conf. Robot.\nAutom. (ICRA)\n, 2018, pp. 4693–4700.\n[171]\nF. Codevilla, E. Santana, A. M. López\net al.\n, “Exploring the\nLimitations of Behavior Cloning for Autonomous Driving,” in\nProc.\nIEEE/CVF Int. Conf. Comput. Vis. (ICCV)\n, 2019, pp. 9329–9338.\n[172]\nJ. Hawke, R. Shen, C. Gurau\net al.\n, “Urban Driving with Conditional\nImitation Learning,” in\nProc. IEEE Int. Conf. Robot. Autom. (ICRA)\n,\n2020, pp. 251–257.\n[173]\nD. Chen, B. Zhou, V. Koltun\net al.\n, “Learning by Cheating,” in\nProc. Conf. Robo. Learn. (CoRL)\n. PMLR, 2020, pp. 66–75.\n[174]\nD. Chen, V. Koltun, and P. Krähenbühl, “Learning To Drive From a\nWorld on Rails,” in\nProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV)\n,\n2021, pp. 15 590–15 599.\n[175]\nC. Zhang, R. Guo, W. Zeng\net al.\n, “Rethinking Closed-Loop Training for\nAutonomous Driving,” in\nProc. Eur. Conf. Comput. Vis. (ECCV)\n. Springer, 2022, pp. 264–282.\n[176]\nA. Hu, G. Corrado, N. Griffiths\net al.\n, “Model-Based Imitation\nLearning for Urban Driving,” in\nProc. 35th Int. Conf. Neural Inf.\nProcess. Syst. (NeurIPS)\n, 2022, pp. 20 703–20 716.\n[177]\nP. P. Ray, “ChatGPT: A comprehensive review on background, applications, key\nchallenges, bias, ethics, limitations and future scope,”\nInternet\nThings Cyber-Phys. Syst.\n, vol. 3, 2023.\n[178]\nN. Kitaev, S. Cao, and D. Klein, “Multilingual Constituency Parsing with\nSelf-Attention and Pre-Training,” in\nProc. 57th Annual Meeting of the\nAssociation for Computational Linguistics (ACL)\n, 2019, pp. 3499–3505.\n[179]\nD. Zhu, J. Chen, X. Shen\net al.\n, “MiniGPT-4: Enhancing Vision-Language\nUnderstanding with Advanced Large Language Models,”\narXiv preprint\narXiv:2304.10592\n, 2023.\n[180]\nH. Liu, C. Li, Q. Wu\net al.\n, “Visual Instruction Tuning,” in\nProc. 37th Int. Conf. Neural Inf. Process. Syst. (NeurIPS)\n. PMLR, 2024, pp. 1–26.\n[181]\nB. Zitkovich, T. Yu, S. Xu\net al.\n, “RT-2: Vision-Language-Action\nModels Transfer Web Knowledge to Robotic Control,” in\nConference on\nRobot Learning\n. PMLR, 2023, pp.\n2165–2183.\n[182]\nM. J. Kim, K. Pertsch, S. Karamcheti\net al.\n, “OpenVLA: An Open-Source\nVision-Language-Action Model,”\narXiv preprint arXiv:2406.09246\n,\n2024.\n[183]\nX. Zhou, M. Liu, E. Yurtsever\net al.\n, “Vision Language Models in\nAutonomous Driving: A Survey and Outlook,”\nIEEE Trans. Intell. Veh.\n,\npp. 1–20, 2024.\n[184]\nA. Seff, B. Cera, D. Chen\net al.\n, “MotionLM: Multi-Agent Motion\nForecasting as Language Modeling,” in\nProc. IEEE/CVF Int. Conf.\nComput. Vis. (ICCV)\n, 2023, pp. 8579–8590.\n[185]\nY. Cui, S. Huang, J. Zhong\net al.\n, “DriveLLM: Charting the Path Toward\nFull Autonomous Driving With Large Language Models,”\nIEEE Trans.\nIntell. Veh.\n, 2023.\n[186]\nC. Sima, K. Renz, K. Chitta\net al.\n, “DriveLM: Driving with Graph\nVisual Question Answering,”\narXiv preprint arXiv:2312.14150\n, 2023.\n[187]\nZ. Xu, Y. Zhang, E. Xie\net al.\n, “DriveGPT4: Interpretable End-to-End\nAutonomous Driving Via Large Language Model,”\nIEEE Robot. Autom.\nLett.\n, 2024.\n[188]\nJ. You, H. Shi, Z. Jiang\net al.\n, “V2X-VLM: End-to-End V2X Cooperative\nAutonomous Driving Through Large Vision-Language Models,”\narXiv\npreprint arXiv:2408.09251\n, 2024.\n[189]\nX. Tian, J. Gu, B. Li\net al.\n, “Drivevlm: The convergence of autonomous\ndriving and large vision-language models,”\narXiv preprint\narXiv:2402.12289\n, 2024.\n[190]\nX. Zhou, X. Han, F. Yang\net al.\n, “OpenDriveVLA: Towards End-to-end\nAutonomous Driving with Large Vision Language Action Model,”\narXiv\npreprint arXiv:2503.23463\n, 2025.\n[191]\nW. Luo, B. Yang, and R. Urtasun, “Fast and Furious: Real Time End-to-End 3D\nDetection, Tracking and Motion Forecasting With a Single Convolutional\nNet,” in\nProc. IEEE/CVF Comput. Soc. Conf. Comput. Vis. Pattern\nRecognit. (CVPR)\n, 2018, pp. 3569–3577.\n[192]\nM. Liang, B. Yang, W. Zeng\net al.\n, “PnPNet: End-to-End Perception and\nPrediction With Tracking in the Loop,” in\nProc. IEEE/CVF Comput. Soc.\nConf. Comput. Vis. Pattern Recognit. (CVPR)\n, 2020, pp. 11 553–11 562.\n[193]\nX. Weng, B. Ivanovic, and M. Pavone, “MTP: Multi-hypothesis Tracking and\nPrediction for Reduced Error Propagation,” in\nIEEE Intell. Veh. Symp.\nProc. (IV)\n. IEEE, 2022, pp.\n1218–1225.\n[194]\nX. Weng, B. Ivanovic, K. Kitani\net al.\n, “Whose Track Is It Anyway?\nImproving Robustness to Tracking Errors With Affinity-Based Trajectory\nPrediction,” in\nProc. IEEE/CVF Comput. Soc. Conf. Comput. Vis.\nPattern Recognit. (CVPR)\n, 2022, pp. 6573–6582.\n[195]\nB. Eysenbach, A. Khazatsky, S. Levine\net al.\n, “Mismatched No More:\nJoint Model-Policy Optimization for Model-Based RL,” in\nProc. 35th\nInt. Conf. Neural Inf. Process. Syst. (NeurIPS)\n, 2022, pp. 23 230–23 243.\n[196]\nS. Casas, A. Sadat, and R. Urtasun, “MP3: A Unified Model To Map, Perceive,\nPredict and Plan,” in\nProc. IEEE/CVF Comput. Soc. Conf. Comput. Vis.\nPattern Recognit. (CVPR)\n, 2021, pp. 14 403–14 412.\n[197]\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and\nStructured Prediction to No-Regret Online Learning,” in\nInt. Conf.\nArtif. Intell. Stat. (AISTATS)\n. JMLR\nWorkshop and Conference Proceedings, 2011, pp. 627–635.\n[198]\nUber Advanced Technologies Group, “A principled approach to safety,”\n2020. [Online]. Available:\nhttps://uber.app.box.com/v/UberATGSafetyReport\n[199]\nMotional, “Voluntary safety self-assessment,” 2021. [Online]. Available:\nhttps://drive.google.com/file/d/1JjfQByU_hWvSfkWzQ8PK2ZOZfVCqQGDB/view\n[200]\nWaymo, “Safety report,” 2021. [Online]. Available:\nhttps://waymo.com/safety/safety-report\n[201]\nW. Zeng, S. Wang, R. Liao\net al.\n, “DSDNet: Deep Structured\nSelf-driving Network,” in\nProc. Eur. Conf. Comput. Vis.\n(ECCV)\n. Springer, 2020, pp. 156–172.\n[202]\nA. Cui, S. Casas, A. Sadat\net al.\n, “LookOut: Diverse Multi-Future\nPrediction and Planning for Self-Driving,” in\nProc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV)\n, 2021, pp. 16 107–16 116.\n[203]\nK. Chitta, A. Prakash, and A. Geiger, “NEAT: Neural Attention Fields for\nEnd-to-End Autonomous Driving,” in\nProc. IEEE/CVF Int. Conf. Comput.\nVis. (ICCV)\n, 2021, pp. 15 793–15 803.\n[204]\nD. Chen and P. Krähenbühl, “Learning From All Vehicles,” in\nProc. IEEE/CVF Comput. Soc. Conf. Comput. Vis. Pattern Recognit.\n(CVPR)\n, 2022, pp. 17 222–17 231.\n[205]\nS. Hu, L. Chen, P. Wu\net al.\n, “ST-P3: End-to-End Vision-Based\nAutonomous Driving via Spatial-Temporal Feature Learning,” in\nProc.\nEur. Conf. Comput. Vis. (ECCV)\n. Springer, 2022, pp. 533–549.\n[206]\nJ. Philion and S. Fidler, “Lift, Splat, Shoot: Encoding Images from Arbitrary\nCamera Rigs by Implicitly Unprojecting to 3D,” in\nProc. Eur. Conf.\nComput. Vis. (ECCV)\n. Springer, 2020,\npp. 194–210.\n[207]\nS. Casas, C. Gulino, S. Suo\net al.\n, “The Importance of Prior Knowledge\nin Precise Multimodal Prediction,” in\nIEEE Int. Conf. Intell. Robots\nSyst.(IROS)\n. IEEE, 2020, pp.\n2295–2302.\n[208]\nA. Sadat, M. Ren, A. Pokrovsky\net al.\n, “Jointly Learnable Behavior and\nTrajectory Planning for Self-Driving Vehicles,” in\nIEEE Int. Conf.\nIntell. Robots Syst.(IROS)\n, 2019, pp. 3949–3956.\n[209]\nS. Casas, W. Luo, and R. Urtasun, “IntentNet: Learning to Predict Intention\nfrom Raw Sensor Data,” in\nProc. Conf. Robo. Learn. (CoRL)\n. PMLR, 2018, pp. 947–956.\n[210]\nX. Zhou, V. Koltun, and P. Krähenbühl, “Tracking Objects as\nPoints,” in\nProc. Eur. Conf. Comput. Vis. (ECCV)\n. Springer, 2020, pp. 474–490.\n[211]\nB. Ivanovic and M. Pavone, “Injecting Planning-Awareness into Prediction and\nDetection Evaluation,” in\nProc. IEEE Intell. Veh. Symp. (IV)\n. IEEE, 2022, pp. 821–828.\n[212]\nY. Hu, J. Yang, L. Chen\net al.\n, “Planning-Oriented Autonomous\nDriving,” in\nProc. IEEE/CVF Comput. Soc. Conf. Comput. Vis. Pattern\nRecognit. (CVPR)\n, 2023, pp. 17 853–17 862.\n[213]\nJ. Gu, C. Hu, T. Zhang\net al.\n, “ViP3D: End-to-End Visual Trajectory\nPrediction via 3D Agent Queries,” in\nProc. IEEE/CVF Comput. Soc.\nConf. Comput. Vis. Pattern Recognit. (CVPR)\n, 2023, pp. 5496–5506.\n[214]\nK. Renz, K. Chitta, O.-B. Mercea\net al.\n, “PlanT: Explainable Planning\nTransformers via Object-Level Representations,” in\nProc. Conf. Robo.\nLearn. (CoRL)\n. PMLR, 2022, pp.\n459–470.\n[215]\nS. Biswas, S. Casas, Q. Sykora\net al.\n, “Quad: Query-based interpretable\nneural motion planning for autonomous driving,”\narXiv preprint\narXiv:2404.01486\n, 2024.\n[216]\nS. Hoermann, M. Bach, and K. Dietmayer, “Dynamic Occupancy Grid Prediction\nfor Urban Autonomous Driving: A Deep Learning Approach with Fully Automatic\nLabeling,” in\nProc. IEEE Int. Conf. Robot. Autom. (ICRA)\n. IEEE, 2018, pp. 2056–2063.\n[217]\nJ. Li, X. He, C. Zhou\net al.\n, “Viewformer: Exploring spatiotemporal\nmodeling for multi-view 3d occupancy perception via view-guided\ntransformers,” in\nProc. Eur. Conf. Comput. Vis. (ECCV)\n. Springer, 2025, pp. 90–106.\n[218]\nZ. Huang, H. Liu, and C. Lv, “GameFormer: Game-theoretic Modeling and\nLearning of Transformer-based Interactive Prediction and Planning for\nAutonomous Driving,” in\nProc. IEEE/CVF Int. Conf. Comput. Vis.\n(ICCV)\n, 2023, pp. 3903–3913.\n[219]\nZ. Huang, P. Karkus, B. Ivanovic\net al.\n, “DTPP: Differentiable Joint\nConditional Prediction and Cost Evaluation for Tree Policy Planning in\nAutonomous Driving,” in\nProc. IEEE Int. Conf. Robot. Autom.\n(ICRA)\n. IEEE, 2024, pp. 6806–6812.\n[220]\nX. Jia, Y. Gao, L. Chen\net al.\n, “DriveAdapter: Breaking the Coupling\nBarrier of Perception and Planning in End-to-End Autonomous Driving,” in\nProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV)\n, 2023, pp. 7953–7963.\n[221]\nS. Hagedorn, M. Hallgarten, M. Stoll\net al.\n, “The integration of\nprediction and planning in deep learning automated driving systems: A\nreview,”\nIEEE Trans. Intell. Veh.\n, 2024.\n[222]\nP. Karkus, B. Ivanovic, S. Mannor\net al.\n, “DiffStack: A Differentiable\nand Modular Control Stack for Autonomous Vehicles,” in\nProc. Conf.\nRobo. Learn. (CoRL)\n. PMLR, 2023, pp.\n2170–2180.\n[223]\nN. Carion, F. Massa, G. Synnaeve\net al.\n, “End-to-End Object Detection\nwith Transformers,” in\nProc. Eur. Conf. Comput. Vis. (ECCV)\n. Springer, 2020, pp. 213–229.\n[224]\nF. Zeng, B. Dong, Y. Zhang\net al.\n, “MOTR: End-to-end multiple-object\ntracking with transformer,” in\nProc. Eur. Conf. Comput. Vis.\n(ECCV)\n. Springer, 2022, pp. 659–675.\n[225]\nS. Ulbrich and M. Maurer, “Situation Assessment in Tactical Lane Change\nBehavior Planning for Automated Vehicles,” in\nProc. 18th IEEE Int.\nConf. Intell. Transp. Syst. (ITSC)\n. IEEE, 2015, pp. 975–981.\n[226]\nO. Scheel, N. S. Nagaraja, L. Schwarz\net al.\n, “Recurrent Models for\nLane Change Prediction and Situation Assessment,”\nIEEE Trans. Intell.\nTransp. Syst.\n, vol. 23, no. 10, pp. 17 284–17 300, 2022.\n[227]\nT. Phan-Minh, E. C. Grigore, F. A. Boulton\net al.\n, “CoverNet:\nMultimodal Behavior Prediction Using Trajectory Sets,” in\nProc.\nIEEE/CVF Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)\n, 2020, pp.\n14 074–14 083.\n[228]\nW. Lim, S. Lee, M. Sunwoo\net al.\n, “Hierarchical trajectory planning of\nan autonomous car based on the integration of a sampling and an optimization\nmethod,”\nIEEE Trans. Intell. Transp. Syst.\n, vol. 19, no. 2, pp.\n613–626, 2018.\n[229]\nY. Chen, P. Karkus, B. Ivanovic\net al.\n, “Tree-structured Policy\nPlanning with Learned Behavior Models,” in\nProc. IEEE Int. Conf.\nRobot. Autom. (ICRA)\n. IEEE, 2023, pp.\n7902–7908.\n[230]\nM. Vitelli, Y. Chang, Y. Ye\net al.\n, “SafetyNet: Safe Planning for\nReal-World Self-Driving Vehicles Using Machine-Learned Policies,” in\nProc. IEEE Int. Conf. Robot. Autom. (ICRA)\n. IEEE, 2022, pp. 897–904.\n[231]\nJ. Mao, Y. Qian, J. Ye\net al.\n, “GPT-Driver: Learning to Drive with\nGPT,” in\nNeurIPS 2023 Foundation Models for Decision Making\nWorkshop\n, 2023.\n[232]\nY. Yoon, J. Shin, H. J. Kim\net al.\n, “Model-predictive active steering\nand obstacle avoidance for autonomous ground vehicles,”\nControl Eng.\nPract.\n, vol. 17, no. 7, pp. 741–750, 2009.\n[233]\nX. Ji, X. He, C. Lv\net al.\n, “Adaptive-neural-network-based robust\nlateral motion control for autonomous vehicle at driving limits,”\nControl Eng. Pract.\n, vol. 76, pp. 41–53, 2018.\n[234]\nS. Shi, L. Jiang, D. Dai\net al.\n, “Motion Transformer with Global\nIntention Localization and Local Movement Refinement,” in\nProc. 36th\nInt. Conf. Neural Inf. Process. Syst. (NeurIPS)\n, 2022, pp. 6531–6543.\n[235]\nN. Lee, W. Choi, P. Vernaza\net al.\n, “DESIRE: Distant Future Prediction\nin Dynamic Scenes With Interacting Agents,” in\nProc. IEEE/CVF Comput.\nSoc. Conf. Comput. Vis. Pattern Recognit. (CVPR)\n, 2017, pp. 336–345.\n[236]\nJ. Ngiam, V. Vasudevan, B. Caine\net al.\n, “Scene Transformer: A unified\narchitecture for predicting future trajectories of multiple agents,” in\nProc. Conf. Robo. Learn. (CoRL)\n, 2023.\n[237]\nX. Jia, P. Wu, L. Chen\net al.\n, “HDGT: Heterogeneous Driving Graph\nTransformer for Multi-Agent Trajectory Prediction via Scene Encoding,”\nIEEE transactions on pattern analysis and machine intelligence\n, 2023.\n[238]\nJ. Wei, J. M. Snider, T. Gu\net al.\n, “A behavioral planning framework\nfor autonomous driving,” in\nProc. IEEE Intell. Veh. Symp. (IV)\n. IEEE, 2014, pp. 458–464.\n[239]\nP. F. Orzechowski, C. Burger, and M. Lauer, “Decision-Making for Automated\nVehicles Using a Hierarchical Behavior-Based Arbitration Scheme,” in\nProc. IEEE Intell. Veh. Symp. (IV)\n. IEEE, 2020, pp. 767–774.\n[240]\nZ. Li, P. Zhao, C. Jiang\net al.\n, “A Learning-Based Model Predictive\nTrajectory Planning Controller for Automated Driving in Unstructured Dynamic\nEnvironments,”\nIEEE Trans. Veh. Technol.\n, vol. 71, no. 6, pp.\n5944–5959, 2022.\n[241]\nM. Menner, K. Berntorp, M. N. Zeilinger\net al.\n, “Inverse Learning for\nData-Driven Calibration of Model-Based Statistical Path Planning,”\nIEEE Trans. Intell. Veh.\n, vol. 6, no. 1, pp. 131–145, 2020.\n[242]\nS. Bae, D. Isele, A. Nakhaei\net al.\n, “Lane-Change in Dense Traffic\nWith Model Predictive Control and Neural Networks,”\nIEEE Trans.\nControl Syst. Technol.\n, vol. 31, no. 2, pp. 646–659, 2022.\n[243]\nH. Sha, Y. Mu, Y. Jiang\net al.\n, “LanguageMPC: Large Language Models as\nDecision Makers for Autonomous Driving,”\narXiv preprint\narXiv:2310.03026\n, 2023.\n[244]\nM. Toromanoff, E. Wirbel, and F. Moutarde, “End-to-End Model-Free\nReinforcement Learning for Urban Driving Using Implicit Affordances,” in\nProc. IEEE/CVF Comput. Soc. Conf. Comput. Vis. Pattern Recognit.\n(CVPR)\n, 2020, pp. 7153–7162.\n[245]\nN. Rhinehart, R. McAllister, and S. Levine, “Deep Imitative Models for\nFlexible Inference, Planning, and Control,” in\nInt. Conf. Learn.\nRepresent. (ICLR)\n, 2020, pp. 1–20.\n[246]\nE. Ohn-Bar, A. Prakash, A. Behl\net al.\n, “Learning Situational\nDriving,” in\nProc. IEEE/CVF Comput. Soc. Conf. Comput. Vis. Pattern\nRecognit. (CVPR)\n, 2020, pp. 11 296–11 305.\n[247]\nZ. Zhang, A. Liniger, D. Dai\net al.\n, “End-to-End Urban Driving by\nImitating a Reinforcement Learning Coach,” in\nProc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV)\n, 2021, pp. 15 222–15 232.\n[248]\nP. Wu, X. Jia, L. Chen\net al.\n, “Trajectory-guided Control Prediction\nfor End-to-end Autonomous Driving: A Simple yet Strong Baseline,”\nProc. 35th Int. Conf. Neural Inf. Process. Syst. (NeurIPS)\n, pp.\n6119–6132, 2022.\n[249]\nO. Scheel, L. Bergamini, M. Wolczyk\net al.\n, “Urban Driver: Learning to\nDrive from Real-world Demonstrations Using Policy Gradients,” in\nProc. Conf. Robo. Learn. (CoRL\n. PMLR, 2022, pp. 718–728.\n[250]\nX. Jia, P. Wu, L. Chen\net al.\n, “Think Twice Before Driving: Towards\nScalable Decoders for End-to-End Autonomous Driving,” in\nProc.\nIEEE/CVF Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)\n, 2023, pp.\n21 983–21 994.\n[251]\nK. Li, Z. Li, S. Lan\net al.\n, “Hydra-MDP++: Advancing End-to-End\nDriving via Hydra-Distillation with Expert-Guided Decision Analysis,” in\nProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition workshops\n, 2025, pp. 1–13.\n[252]\nM. Müller, A. Dosovitskiy, B. Ghanem\net al.\n, “Driving Policy\nTransfer via Modularity and Abstraction,” in\nProc. Conf. Robo. Learn.\n(CoRL)\n. PMLR, 2018, pp. 2170–2180.\n[253]\nA. Prakash, K. Chitta, and A. Geiger, “Multi-Modal Fusion Transformer for\nEnd-to-End Autonomous Driving,” in\nProc. IEEE/CVF Comput. Soc. Conf.\nComput. Vis. Pattern Recognit. (CVPR)\n, 2021, pp. 7077–7087.\n[254]\nK. Chitta, A. Prakash, B. Jaeger\net al.\n, “TransFuser: Imitation With\nTransformer-Based Sensor Fusion for Autonomous Driving,”\nIEEE Trans.\nPattern Anal. Mach. Intell.\n, vol. 45, no. 11, pp. 12 878–12 895, 2022.\n[255]\nW. Wang, J. Xie, C. Hu\net al.\n, “DriveMLM: Aligning Multi-Modal Large\nLanguage Models with Behavioral Planning States for Autonomous Driving,”\narXiv preprint arXiv:2312.09245\n, 2023.\n[256]\nA. Hu, Z. Murez, N. Mohan\net al.\n, “FIERY: Future Instance Prediction\nin Bird’s-Eye View From Surround Monocular Cameras,” in\nProc.\nIEEE/CVF Int. Conf. Comput. Vis. (ICCV)\n, 2021, pp. 15 273–15 282.\n[257]\nT. Khurana, P. Hu, A. Dave\net al.\n, “Differentiable Raycasting for\nSelf-Supervised Occupancy Forecasting,” in\nProc. IEEE Eur. Control\nConf. (ECC)\n. Springer, 2022, pp.\n353–369.\n[258]\nY. Wang, W. Luo, J. Bai\net al.\n, “Alpamayo-R1: Bridging Reasoning and\nAction Prediction for Generalizable Autonomous Driving in the Long Tail,”\narXiv preprint arXiv:2511.00088\n, 2025.\n[259]\nH. Shao, L. Wang, R. Chen\net al.\n, “Safety-Enhanced Autonomous Driving\nUsing Interpretable Sensor Fusion Transformer,” in\nProc. Conf. Robo.\nLearn. (CoRL)\n. PMLR, 2023, pp.\n726–737.\n[260]\nL. Chen, O. Sinavski, J. Hünermann\net al.\n, “Driving with LLMs:\nFusing Object-Level Vector Modality for Explainable Autonomous Driving,” in\nProc. IEEE Int. Conf. Robot. Autom. (ICRA)\n. IEEE, 2024, pp. 14 093–14 100.\n[261]\nH. Song, W. Ding, Y. Chen\net al.\n, “PiP: Planning-Informed Trajectory\nPrediction for Autonomous Driving,” in\nProc. Eur. Conf. Comput. Vis.\n(ECCV)\n. Springer, 2020, pp. 598–614.\n[262]\nP. Hu, A. Huang, J. Dolan\net al.\n, “Safe Local Motion Planning With\nSelf-Supervised Freespace Forecasting,” in\nProc. IEEE/CVF Comput.\nSoc. Conf. Comput. Vis. Pattern Recognit. (CVPR)\n, 2021, pp.\n12 732–12 741.\n[263]\nS. Pini, C. S. Perone, A. Ahuja\net al.\n, “Safe Real-World Autonomous\nDriving by Learning to Predict and Plan with a Mixture of Experts,” in\nProc. IEEE Int. Conf. Robot. Autom. (ICRA)\n. IEEE, 2023, pp. 10 069–10 075.\n[264]\nY. Chen, S. Veer, P. Karkus\net al.\n, “Interactive Joint Planning for\nAutonomous Vehicles,”\nIEEE Robot. Autom. Lett.\n, 2023.\n[265]\nZ. Huang, H. Liu, J. Wu\net al.\n, “Differentiable Integrated Motion\nPrediction and Planning With Learnable Cost Function for Autonomous\nDriving,”\nIEEE Trans. Neural Netw. Learn. Syst.\n, 2023.\n[266]\nB. Jiang, S. Chen, Q. Xu\net al.\n, “VAD: Vectorized Scene Representation\nfor Efficient Autonomous Driving,” in\nProc. IEEE/CVF Comput. Soc.\nConf. Comput. Vis. Pattern Recognit. (CVPR)\n, 2023, pp. 8340–8350.\n[267]\nT. Ye, W. Jing, C. Hu\net al.\n, “FusionAD: Multi-modality Fusion for\nPrediction and Planning Tasks of Autonomous Driving,”\narXiv preprint\narXiv:2308.01006\n, 2023.\n[268]\nS. Chen, B. Jiang, H. Gao\net al.\n, “VADv2: End-to-End Vectorized\nAutonomous Driving via Probabilistic Planning,”\narXiv preprint\narXiv:2402.13243\n, 2024.\n[269]\nW. Zheng, R. Song, X. Guo\net al.\n, “GenAD: Generative End-to-End\nAutonomous Driving,” in\nProc. Eur. Conf. Comput. Vis. (ECCV)\n. Springer, 2025, pp. 87–104.\n[270]\nL. Ullrich, M. Buchholz, K. Dietmayer\net al.\n, “Expanding the Classical\nV-Model for the Development of Complex Systems Incorporating AI,”\nIEEE Trans. Intell. Veh.\n, 2024.\n[271]\nA. Vaswani, N. Shazeer, N. Parmar\net al.\n, “Attention is All you\nNeed,” in\nProc. 30th Int. Conf. Neural Inf. Process. Syst.\n(NeurIPS)\n, 2017.\n[272]\nD. Tian, C. Lin, J. Zhou\net al.\n, “SA-YOLOv3: An Efficient and Accurate\nObject Detector Using Self-Attention Mechanism for Autonomous Driving,”\nIEEE Trans. Intell. Transp. Syst.\n, vol. 23, no. 5, pp. 4099–4110,\n2020.\n[273]\nL. Wang, Z. Song, X. Zhang\net al.\n, “SAT-GCN: Self-attention graph\nconvolutional network-based 3D object detection for autonomous driving,”\nKnowl.-Based Syst.\n, vol. 259, 2023, Art. no. 110080.\n[274]\nY. Xie, Y. Xiong, and Y. Zhu, “SAST-GNN: A Self-Attention Based\nSpatio-Temporal Graph Neural Network for Traffic Prediction,” in\nProc. 25th Int. Conf. Database Syst. Adv. Appl. (DASFAA)\n. Jeju, South Korea: Springer, 2020, pp. 707–714.\n[275]\nZ. Lin, M. Li, Z. Zheng\net al.\n, “Self-Attention ConvLSTM for\nSpatiotemporal Prediction,” in\nProc. AAAI Conf. Artif. Intell.\n,\nvol. 34, no. 07, 2020, pp. 11 531–11 538.\n[276]\nL. Ye, Z. Wang, X. Chen\net al.\n, “GSAN: Graph Self-Attention Network\nfor Learning Spatial–Temporal Interaction Representation in Autonomous\nDriving,”\nIEEE Internet Things J.\n, vol. 9, no. 12, pp. 9190–9204,\n2021.\n[277]\nJ. Wen, Z. Zhao, J. Cui\net al.\n, “Model-Based Reinforcement Learning\nwith Self-attention Mechanism for Autonomous Driving in Dense Traffic,” in\nProf. Int. Conf. Neural Inf. Process. (ICONIP)\n. Springer, 2022, pp. 317–330.\n[278]\nA. Azzolini, J. Bai, H. Brandon\net al.\n, “Cosmos-Reason1: From Physical\nCommon Sense To Embodied Reasoning,”\narXiv preprint\narXiv:2503.15558\n, 2025.\n[279]\nT. Salzmann, B. Ivanovic, P. Chakravarty\net al.\n, “Trajectron++:\nDynamically-Feasible Trajectory Forecasting with Heterogeneous Data,” in\nProc. Eur. Conf. Comput. Vis. (ECCV)\n. Springer, 2020, pp. 683–700.\n[280]\nJ. Buchner. (2019) Automated Driving and AI: Interview in English. Robert\nBosch GmbH. Accessed on July 12.2025. [Online]. Available:\nhttps://www.bosch-presse.de/pressportal/de/en/automated-driving-and-ai-interview-in-english-277393.html\n[281]\nY. Yao, “Navigating Global SDV Trends: Insights from Market Demands and\nEnd-to-End Roadmaps,” Keynote, Robert Bosch GmbH, Germany, 2025, presented\nat 36th IEEE IV, Cluj-Napoca, Romania.\n[282]\nD. Kahneman,\nThinking, Fast and Slow\n. New York, NY, USA: Farrar, Straus and Giroux, 2011.\n[283]\n“\nIntelligent Transport Systems (ITS); ITS-G5 Access layer\nspecification for Intelligent Transport Systems operating in the 5 GHz\nfrequency band\n,” Std. ETSI EN 302 663, 2019, final draft V1.3.1.\n[284]\n“\nIntelligent Transport Systems (ITS); Vehicular Communications; Basic\nSet of Applications; Part 2: Specification of Cooperative Awareness Basic\nService\n,” Std. ETSI ES 202 637-2, 2019, final draft V1.4.1.\n[285]\n“\nIntelligent Transport Systems (ITS); Vehicular Communications; Basic\nSet of Applications; Analysis of the Collective Perception Service (CPS);\nRelease 2\n,” Std. ETSI TR 103 562, 2019, final draft V2.1.1.\n[286]\n“\nIntelligent Transport Systems (ITS); Vehicular Communications;\nManoeuvre Coordination Service (MCS); Pre-standardization study; Release\n2\n,” Std. ETSI TR 103 578, 2024, final draft V2.1.1.\n[287]\nINFRAMIX Consortium, “Infrastructure Categorization: ISAD Levels,”\nhttps://www.inframix.eu/infrastructure-categorization/\n, 2021.\n[288]\nLyft Inc., “Safety Transparency Report,”\nhttps://assets.ctfassets.net/vz6nkkbc6q75/3yrO0aP4mPfTTvyaUZHJfJ/f77d145864edc540aa9f7fe530c6bcec/Safety_Transparency_Report_2020-2022.pdf\n,\n2022.\n[289]\nArgo AI LCC., “Developing a Self-Driving System You Can Trust,”\nhttps://fleetwiki.net/documents/ArgoSafetyReport.pdf\n, 2021.\n[290]\nMotional Inc., “Voluntary safety self-assessment (VSSA),”\nhttps://motional.com/sites/default/files/inline-files/Motional_Voluntary_Safety_Self-Assessment_2024.pdf\n,\n2021.\n[291]\nZoox Inc., “Introducing Zoox safety innovations,”\nhttps://www.datocms-assets.com/106048/1696536139-zoox_safety_report_volume2_2021_v2.pdf\n,\n2021.\n[292]\nCruise LLC., “Cruise Safety Report 2022,”\nhttps://assets.ctfassets.net/95kuvdv8zn1v/zKJHD7X22fNzpAJztpd5K/ac6cd2419f2665000e4eac3b7d16ad1c/Cruise_Safety_Report_2022_sm-optimized.pdf\n,\n2022.\n[293]\nNVIDIA Corporation, “Self-Driving Safety Report,”\nhttps://images.nvidia.com/aem-dam/en-zz/Solutions/auto-self-driving-safety-report.pdf\n,\n2024.\n[294]\nMobileye Global Inc., “Mobileye Drive-Enabling autonomous mobility,”\nhttps://www.mobileye.com/solutions/drive/\n, 2024.\n[295]\nTesla Inc., “A Principled Approach to Safety,”\nhttps://www.tesla.com/ns_videos/2021-tesla-impact-report.pdf\n, 2021.\n[296]\nToyota Research Institute, “Toyota Automated Driving-Whitepaper,”\nhttps://amrd.toyota.com/app/uploads/2022/02/ATwhitepaper.pdf\n, 2022.\n[297]\nMercedes-Benz Group AG, “Introducing DRIVE PILOT: An Automated Driving\nSystem for the Highway,”\nhttps://group.mercedes-benz.com/dokumente/innovation/sonstiges/2023-03-06-vssa-mercedes-benz-drive-pilot.pdf\n,\n2023.\n[298]\nC. Creß, Z. Bing, and A. C. Knoll, “Intelligent Transportation Systems\nUsing Roadside Infrastructure: A Literature Survey,”\nIEEE Trans.\nIntell. Transp. Syst.\n, 2023.\n[299]\nK. Shan, M. Penlington, S. Gunner\net al.\n, “Experimental Study of\nMulti-Camera Infrastructure Perception for V2X-Assisted Automated Driving in\nHighway Merging,”\nIEEE Trans. Intell. Transp. Syst.\n, 2024.\n[300]\nZ. Li, K. Li, S. Wang\net al.\n, “Hydra-MDP: End-to-end Multimodal\nPlanning with Multi-target Hydra-Distillation,”\narXiv preprint\narXiv:2406.06978\n, 2024.\n[301]\nJ. Hawke, V. Badrinarayanan, A. Kendall\net al.\n, “Reimagining an\nautonomous vehicle,”\narXiv preprint arXiv:2108.05805\n, 2021.\n[302]\nWayve Technologies Ltd., “Wayve’s AV2.0 Approach,”\nhttps://wayve.ai/technology/#AV2.0\n, 2025.\n[303]\nK. Wong, Y. Gu, and S. Kamijo, “Mapping for Autonomous Driving: Opportunities\nand Challenges,”\nIEEE Intell. Transp. Syst. Mag.\n, vol. 13, no. 1,\npp. 91–106, 2020.\n[304]\nG. Elghazaly, R. Frank, S. Harvey\net al.\n, “High-definition maps:\nComprehensive survey, challenges, and future perspectives,”\nIEEE Open\nJ. Intell. Transp. Syst. (OJ-ITS)\n, vol. 4, pp. 527–550, 2023.\n[305]\nM. Garnelo, D. Rosenbaum, C. Maddison\net al.\n, “Conditional Neural\nProcesses,” in\nProc. 35th Int. Conf. Mach. Learn. (ICML)\n. PMLR, 2018, pp. 1704–1713.\n[306]\nA. Blödel, B. Klamann, and S. Peters, “Towards a Data-Based Interface\nDefinition to Support a Modular Safety Approval of Highly Automated\nVehicles,” in\nInternational Stuttgart Symposium\n. Springer, 2024, pp. 85–108.\n[307]\nY. LeCun, “A Path Towards Autonomous Machine Intelligence Version 0.9.2,\n2022-06-27,”\nOpen Review\n, vol. 62, 2022.\n[308]\nK. Lorenz,\nDie Rückseite des Spiegels. Versuch einer Naturgeschichte\nmenschlichen Erkennens\n. München\nand Zürich: Piper & Co Verlag, 1973.\n[309]\nK. D. Kusano, J. M. Scanlon, Y.-H. Chen\net al.\n, “Comparison of Waymo\nRider-Only Crash Data to Human Benchmarks at 7.1 Million Miles,”\narXiv preprint arXiv:2312.12675\n, 2023.\n[310]\nMinistry of Economy, Trade and Industry (METI). (2021, July 9)\nCall\nfor Public Comments on \"AI Governance Guidelines for Implementation of AI\nPrinciples Ver. 1.0\" Opens\n. [Online]. Available:\nhttps://www.meti.go.jp/english/press/2021/0709_004.html\n[311]\nDigital Society Initiative, “Positionspapier: Ein Rechtsrahmen für\nKünstliche Intelligenz,” in\nWorkshop of the DSI Strategy\nLab\n. Balsthal: Digital Society\nInitiative, 2021, pp. 1–7, 26 August 2021 – 28 August 2021.\n[312]\nEuropean Parliament, “Corrigendum to the European Parliament’s position on\nthe Artificial Intelligence Act (P9_TA(2024)0138),” Brussels, April 19\n2024. [Online]. Available:\nhttps://www.europarl.europa.eu/doceo/document/TA-9-2024-0138-FNL-COR01_EN.pdf\n[313]\nHouse of Commons of Canada, First Session. (2022, June)\nBill C-27,\nConsumer Privacy Protection Act, PART 3 Artificial Intelligence and Data\nAct\n. [Online]. Available:\nhttps://www.parl.ca/DocumentViewer/en/44-1/bill/C-27/first-reading\n[314]\nNational Committee on the Governance of the New Generation of Artificial\nIntelligence. (2019, June)\nDeveloping Responsible Artificial\nIntelligence: Release of the New Generation of Artificial Intelligence\nGovernance Principles\n. [Online]. Available:\nhttps://www.most.gov.cn/kjbgz/201906/t20190617_147107.html\n[315]\nE. Tabassi, “Artificial Intelligence Risk Management Framework (AI RMF\n1.0),”\nNational Institute of Standards and Technology (NIST),\nGaithersburg, MD\n, 2023, DOI: 10.6028/NIST.AI.100-1.\n[316]\n“\nRoad Vehicles — Functional Safety\n,” Std. ISO 26262:2018, 2018.\n[317]\n“\nRoad Vehicles — Safety of the Intended Functionality\n,” Std. ISO\n21448:2022, 2022.\n[318]\n“\nRoad Vehicles — Safety and Artificial Intelligence\n,” Std.\nISO/PAS 8800:2024, 2024.\n[319]\n“\nStandard for Evaluation of Autonomous Products\n,” Std. ANSI/UL\n4600, 2020.\n[320]\nS. Diemert, L. Millet, J. Groves\net al.\n, “Safety integrity levels for\nartificial intelligence,” in\nProc. Int. Conf. Comput. Saf., Rel.,\nSecur.\n, 2023, pp. 397–409.\n[321]\nL. Ullrich, W. Zimmer, R. Greer\net al.\n, “A New Perspective On AI\nSafety Through Control Theory Methodologies,”\nIEEE Open J. Intell.\nTransp. Syst. (OJ-ITS)\n, 2025.\n[322]\nF. Favaro, L. Fraade-Blanar, S. Schnelle\net al.\n, “Building a Credible\nCase for Safety: Waymo’s Approach for the Determination of Absence of\nUnreasonable Risk,”\narXiv preprint arXiv:2306.01917\n, 2023.\n[323]\nA. Karpathy, “Keynote,” in\nProc. WAD CVPR’21\n, A. Karpathy, Ed. Tesla, 2021, accessed: Feb. 19, 2024.\n[Online]. Available:\nhttps://www.youtube.com/watch?v=g6bOwQdCJrc&list=PLvXze1V52Yy2OY67mz2Jy-JcnEw8GUZEl&index=14\n[324]\nL. Ullrich, A. McMaster, and K. Graichen, “Transfer Learning Study of Motion\nTransformer-based Trajectory Predictions,” in\nIEEE Intell. Veh. Symp.\nProc. (IV)\n, 2024, pp. 110–117.\n[325]\nC. Huang, O. Mees, A. Zeng\net al.\n, “Visual Language Maps for Robot\nNavigation,” in\nProc. IEEE Int. Conf. Robot. Autom. (ICRA)\n, 2023,\npp. 10 608–10 615.\n[326]\nY. Huang and Y. Chen, “Survey of State-of-Art Autonomous Driving Technologies\nwith Deep Learning,” in\nProc. Int. Conf. Softw. Qual., Reliab. Secur.\nCompanion (QRS-C)\n. IEEE, 2020, pp.\n221–228.\n[327]\nT. Hospedales, A. Antoniou, P. Micaelli\net al.\n, “Meta-learning in\nneural networks: A survey,”\nIEEE Trans. Pattern Anal. Mach. Intell.\n(TPAMI)\n, vol. 44, no. 9, pp. 5149–5169, 2021.\n[328]\nK. Zhou, Z. Liu, Y. Qiao\net al.\n, “Domain generalization: A survey,”\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n, 2022.\n[329]\nZ. Yang, X. Jia, H. Li\net al.\n, “LLM4Drive: A Survey of Large Language\nModels for Autonomous Driving,” in\nNeurIPS 2024 Workshop on\nOpen-World Agents\n, 2024.\n[330]\nX. Jin, G. Yin, and N. Chen, “Advanced estimation techniques for vehicle\nsystem dynamic state: A survey,”\nSensors\n, vol. 19, no. 19, p. 4289,\n2019.\n[331]\nW. Ding, C. Xu, M. Arief\net al.\n, “A survey on safety-critical driving\nscenario generation—a methodological perspective,”\nIEEE Transactions\non Intelligent Transportation Systems\n, 2023.\nLars Ullrich\nreceived the M.Sc. degree in mechatronics and the Ph.D. (Dr.-Ing.) degree from Friedrich–Alexander–Universität Erlangen–Nürnberg, Germany, in 2022 and 2025, respectively. His research has focused on probabilistic trajectory planning for safe and reliable autonomous driving in uncertain, dynamic environments, with a particular emphasis on challenges arising from the use of AI in automated driving. His current research interests center on safe embodied AI. Since early 2025, he has been elected Vice-Chair of the IEEE Intelligent Transportation Systems Society Germany Chapter.\nMichael Buchholz\nreceived his Diploma degree in Electrical Engineering and Information Technology as well as his Ph.D. from the Faculty of Electrical Engineering and Information Technology at University of Karlsruhe (TH)/Karlsruhe Institute of Technology, Germany. He is a research group leader and lecturer at the Institute of Measurement, Control, and Microtechnology at Ulm University, where he earned his\nHabilitation\n(post-doctoral lecturing qualification) for Automation Technology in 2022 and the title of an\napl. Professor\n(adjunct professor) in 2025. His research interests comprise connected automated driving, electric mobility, modelling and control of mechatronic systems, and system identification.\nKnut Graichen\n(Senior Member, IEEE) received the Diploma-Ing. degree in engineering cybernetics and the Ph.D. (Dr.-Ing.) degree from the University of Stuttgart, Stuttgart, Germany, in 2002 and 2006, respectively.\nIn 2007, he was a Post-Doctoral Researcher with the Center Automatique et Systèmes, MINES ParisTech, France. In 2008, he joined the Automation and Control Institute, Vienna University of Technology, Vienna, Austria, as a Senior Researcher. In 2010, he became a Professor with the Institute of Measurement, Control and Microtechnology, Ulm University, Ulm, Germany. Since 2019, he has been the Head of the Chair of Automatic Control, Friedrich–Alexander–Universität Erlangen–Nürnberg, Germany. His current research interests include distributed and learning control and model predictive control of dynamical systems for automotive, mechatronic, and robotic applications.\nDr. Graichen is the Editor-in-Chief of Control Engineering Practice.\n\\EOD",
    "preview_text": "Automated driving (AD) is promising, but the transition to fully autonomous driving is, among other things, subject to the real, ever-changing open world and the resulting challenges. However, research in the field of AD demonstrates the ability of artificial intelligence (AI) to outperform classical approaches, handle higher complexities, and reach a new level of autonomy. At the same time, the use of AI raises further questions of safety and transferability. To identify the challenges and opportunities arising from AI concerning autonomous driving functionalities, we have analyzed the current state of AD, outlined limitations, and identified foreseeable technological possibilities. Thereby, various further challenges are examined in the context of prospective developments. In this way, this article reconsiders fully autonomous driving with respect to advancements in the field of AI and carves out the respective needs and resulting research questions.\n\n\\NewSpotColorSpace\nPANTONE\n\\AddSpotColor\nPANTONE PANTONE3015C PANTONE\n\\SpotSpace\n3015\n\\SpotSpace\nC 1 0.3 0 0.2\n\\SetPageColorSpace\nPANTONE\nIEEE copyright notice\n© 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nPublished in\nIEEE Access\n, 29 January 2026.\nCite as:\nL. Ullrich, M. Buchholz, K. Dietmayer, and K. Graichen, \"Toward Fully Autonomous Driving: AI, Challenges, Opportunities, and Needs,\" in\nIEEE Access\n, 29 January 2026, pp. 1–26, doi: 10.1109/ACCESS.2026.3659192.\nBIBT\nE\nX:\n⬇\n@article\n{\nullrich\n2024\nadstack\n,\ntitle\n={\nToward\nFully\nAutonomous\nDriving\n:\nAI\n,\nChallenges\n,\nOpportunities\n,\nand\nNeeds\n},\nauthor\n={\nUllrich\n,\nLars\nand\nBuchholz\n,\nMichael\nand\nDietmayer\n,\nKlaus\nand\nGraichen\n,\nKnu",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "AI",
        "autonomous driving"
    ],
    "one_line_summary": "这篇论文探讨了人工智能在自动驾驶领域的应用、挑战和机遇，但未涉及强化学习、VLA、机器人控制等具体技术。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T12:45:44Z",
    "created_at": "2026-02-03T15:53:08.171768",
    "updated_at": "2026-02-03T15:53:08.171777"
}