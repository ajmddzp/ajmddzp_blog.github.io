{
    "id": "2601.15250v1",
    "title": "FlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion",
    "authors": [
        "Zichen Xi",
        "Hao-Xiang Chen",
        "Nan Xue",
        "Hongyu Yan",
        "Qi-Yuan Feng",
        "Levent Burak Kara",
        "Joaquim Jorge",
        "Qun-Ce Xu"
    ],
    "abstract": "åŸºäºå•ç›®RGBå›¾åƒçš„è¯­ä¹‰åœºæ™¯è¡¥å…¨æ˜¯ä¸€é¡¹åŸºç¡€ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºä»å•ä¸€è§†è§’æ¨æ–­è¢«é®æŒ¡çš„ä¸‰ç»´å‡ ä½•ç»“æ„å­˜åœ¨å›ºæœ‰çš„æ¨¡ç³Šæ€§ã€‚è™½ç„¶å‰é¦ˆæ–¹æ³•å·²å–å¾—è¿›å±•ï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥åœ¨è¢«é®æŒ¡åŒºåŸŸç”Ÿæˆåˆç†çš„ç»†èŠ‚ï¼Œå¹¶ä¿æŒç‰©ä½“é—´åŸºæœ¬çš„ç©ºé—´å…³ç³»ã€‚è¿™ç§å¯¹æ•´ä¸ªä¸‰ç»´ç©ºé—´è¿›è¡Œç²¾ç¡®ç”Ÿæˆæ¨ç†çš„èƒ½åŠ›åœ¨å®é™…åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºFlowSSCï¼Œè¿™æ˜¯é¦–ä¸ªç›´æ¥åº”ç”¨äºå•ç›®è¯­ä¹‰åœºæ™¯è¡¥å…¨çš„ç”Ÿæˆå¼æ¡†æ¶ã€‚FlowSSCå°†è¯­ä¹‰åœºæ™¯è¡¥å…¨ä»»åŠ¡è§†ä¸ºæ¡ä»¶ç”Ÿæˆé—®é¢˜ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆç°æœ‰å‰é¦ˆå¼è¯­ä¹‰åœºæ™¯è¡¥å…¨æ–¹æ³•ï¼Œæ˜¾è‘—æå‡å…¶æ€§èƒ½ã€‚ä¸ºå®ç°å®æ—¶æ¨ç†ä¸”ä¸ç‰ºç‰²è´¨é‡ï¼Œæˆ‘ä»¬æå‡ºåœ¨ç´§å‡‘çš„ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´ä¸­è¿è¡Œçš„æ·å¾„æµåŒ¹é…æ–¹æ³•ã€‚ä¸éœ€è¦æ•°ç™¾æ­¥è¿­ä»£çš„æ ‡å‡†æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ·å¾„æœºåˆ¶å®ç°å•æ­¥é«˜ä¿çœŸç”Ÿæˆï¼Œä½¿å…¶èƒ½å¤Ÿå®é™…éƒ¨ç½²äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­ã€‚åœ¨SemanticKITTIæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlowSSCå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºçº¿æ–¹æ³•ã€‚",
    "url": "https://arxiv.org/abs/2601.15250v1",
    "html_url": "https://arxiv.org/html/2601.15250v1",
    "html_content": "FlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion\nZichen Xi\n1\n, Hao-Xiang Chen\n2\n, Nan Xue\n1\n, Hongyu Yan\n2\n, Qi-Yuan Feng\n2\n,\nLevent Burak Kara\n3\n, Joaquim Jorge\n4\n, Qun-Ce Xu\n2\n1\nAnt Group, China\n2\nTsinghua University, China\n3\nDepartment of Mechanical Engineering, Carnegie Mellon University, USA\n4\nInstituto Superior TÃ©cnico, the School of Engineering of the Universidade de Lisboa, Portugal\nAbstract\nSemantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible details in occluded regions and preserve the fundamental spatial relationships of objects. Such accurate generative reasoning capability for the entire 3D space is critical in real-world applications. In this paper, we present FlowSSC, the first generative framework applied directly to monocular semantic scene completion. FlowSSC treats the SSC task as a conditional generation problem and can seamlessly integrate with existing feed-forward SSC methods to significantly boost their performance. To achieve real-time inference without compromising quality, we introduce Shortcut Flow-matching that operates in a compact triplane latent space. Unlike standard diffusion models that require hundreds of steps, our method utilizes a shortcut mechanism to achieve high-fidelity generation in a single step, enabling practical deployment in autonomous systems. Extensive experiments on SemanticKITTI demonstrate that FlowSSC achieves state-of-the-art performance, significantly outperforming existing baselines.\nI\nINTRODUCTION\nFigure 1\n:\nVisualizing the Shortcut Refinement Process. Our model learns a continuous flow from noise (\nx\n0\nx_{0}\n) to data (\nx\n1\nx_{1}\n) in the compact triplane latent space. By optimizing the Self-Consistency objective, the model acquires the ability to perform a direct â€Shortcutâ€ jump for real-time inference, while retaining the capability for multi-step refinement. The refined triplanes are decoded into high-fidelity 3D semantic scenes with fine details.\n(a)\nVAE Architecture\n(b)\nFlowSSC Pipeline Overview\nFigure 2\n:\nOverview of FlowSSC. (a) VecSet VAE compresses 3D voxels into compact triplane latent via Cross-Attention. (b) Overall pipeline: coarse triplane is encoded and refined through Shortcut Latent Diffusion.\nSemantic Scene Completion (SSC) has emerged as a fundamental task in 3D scene understanding, which aims to simultaneously complete the geometry of a 3D scene and predict semantic labels for all voxels, including those that are occluded or not directly visible. This capability is particularly crucial for a wide range of applications such as autonomous driving, robotic navigation, and augmented reality, where a comprehensive understanding of the environment is essential for safe and intelligent decision-making. However, this task remains highly challenging due to severe occlusions, viewpoint limitations, and the inherent ambiguity of inferring 3D structure from limited input data.\nTo address these challenges, many existing approaches rely on feed-forward models\n[\n2\n,\n10\n]\nthat directly map 2D images to 3D voxel grids. While these methods have achieved impressive results in visible regions, they fundamentally struggle with the â€one-to-manyâ€ mapping problem inherent in monocular scene completion. When improving occluded regions, feed-forward models tend to produce blurry or mean-valued predictions because they are trained to minimize reconstruction errors, which penalizes the high-frequency details that constitute realistic scene structure. On the other hand, generative diffusion models have demonstrated remarkable capability in synthesizing high-fidelity details and handling uncertainty. However, their standard iterative sampling process requires hundreds of function evaluations, making them prohibitively slow for real-time applications like autonomous driving.\nTo bridge this gap, we propose\nFlowSSC\n, a novel framework that brings the power of generative modeling to monocular SSC while maintaining real-time inference speeds. We position FlowSSC as a\nuniversal\ngenerative enhancement framework: it can seamlessly integrate with any existing SSC method and significantly uplift its performance through a fast, generative refinement process.\nOur approach overcomes the speed-quality trade-off through two key technical innovations. First, instead of operating in the high-dimensional voxel space (\n256\nÃ—\n256\nÃ—\n32\n256\\times 256\\times 32\n), we employ a VecSet VAE\n[\n23\n]\narchitecture. Unlike standard autoencoders, our VAE leverages Cross-Attention to compress semantic voxels into a compact Triplane latent space, achieving superior reconstruction fidelity (85.89% IoU) while drastically reducing the computational complexity for the diffusion model. Second, we introduce a High-Efficiency Generative Refiner. Specifically, we employ a Triplane Diffusion Transformer as the refiner architecture to effectively aggregate 3D contextual information. To ensure real-time inference, we utilize the Shortcut Models\n[\n6\n]\ntraining strategy. This approach enables our DiT to learn a direct â€shortcutâ€ mapping from noise to clean data without requiring a pre-trained teacher\n1\n, achieving high-fidelity generation in a single step while maintaining the flexibility for multi-step refinement.\nIn summary, our main contributions are as follows:\nâ€¢\nWe propose FlowSSC, the first universal generative framework for monocular Semantic Scene Completion.\nâ€¢\nWe incorporate a powerful VecSet VAE that utilizes Cross-Attention to compress 3D scenes into a compact Triplane representation, enabling efficient high-resolution generative modeling.\nâ€¢\nWe implement a Shortcut Latent Diffusion model that achieves state-of-the-art generation quality with One-Step inference, making generative SSC practical for real-time applications.\nExtensive experiments on SemanticKITTI\n[\n1\n]\ndemonstrate that our method consistently outperforms state-of-the-art approaches, validating the effectiveness of our one-step generative strategy.\nII\nRELATED WORK\nSemantic scene completion (SSC) has been widely studied in recent years, with existing approaches broadly categorized into multi-sensor fusion methods and monocular vision-based methods. We briefly review both lines of work below.\nII-A\nMulti-Sensor Fusion for Semantic Scene Completion\nIn the realm of multi-sensor fusion, SSCNet\n[\n14\n]\npioneers the field by formulating the task of inferring complete 3D scene geometry and semantics from limited observations.\nFollowing SSCNet, numerous methods have been proposed to enhance the performance of SSC by leveraging various sensor modalities such as LiDAR point clouds. For instance, JS3CNet\n[\n20\n]\nintroduces a sparse semantic scene completion network that capitalizes on LiDAR point clouds to improve completion performance through learning contextual shape priors. S3CNet\n[\n3\n]\nis designed to efficiently process large-scale outdoor scenes by predicting dense 3D structure and semantic labels from a single LiDAR point cloud.\nRecently, SurroundOcc\n[\n19\n]\ninnovatively predicts dense 3D occupancy from multi-camera images by generating dense ground truth labels using multi-frame LiDAR scans and Poisson Reconstruction.\nThe fusion-based models using have shown great performance by aligning LiDAR point clouds with image features through cross-modal attention or projection-based methods. However, the need for precise sensor calibration and synchronization limits their scalability and practicality in real-world applications.\nII-B\nVision-Based Monocular Semantic Scene Completion\nVision-based approaches aim to infer the 3D geometry and semantics directly from 2D images by learning to project and reason across the spatial domain. Early work like SPVCNN\n[\n18\n]\nintroduces a lightweight Sparse Point-Voxel Convolution module to enhance 3D scene understanding from monocular RGB images. MonoScene\n[\n2\n]\nis a pioneering effort that directly predicts semantic occupancy from a single RGB image by employing a feature lifting and projection module to map 2D image features into a 3D space. However, it sometimes assigns features from visible areas to occluded voxels, leading to ambiguities in the final 3D representation.\nSubsequent works have built on these foundations. VoxFormer\n[\n10\n]\npresents a sparse-to-dense framework that identifies visible voxels through depth estimation and propagates information to non-visible regions, ensuring accurate initial 3D representation. TPVFormer\n[\n8\n]\nintroduces a tri-perspective view representation to efficiently model fine-grained 3D structures using a transformer-based encoder. OccFormer\n[\n24\n]\nintroduces a dual-path transformer encoder to process 3D voxel features, capturing both fine-grained details and global semantic layouts.\nSymphonize\n[\n9\n]\nintegrates instance queries to enhance scene modeling using Serial Instance-Propagated Attentions.\nCGFormer\n[\n22\n]\nintroduces a Context and Geometry Aware Voxel Transformer that generates context-dependent queries. ET-Former\n[\n11\n]\nleverages a triplane-based deformable attention mechanism to improve semantic predictions while lowering training memory consumption.\nII-C\nDiffusion-Based Generative Model\nDiffusion models\n[\n7\n,\n17\n]\nhave achieved remarkable success in 2D image synthesis and are increasingly applied to 3D tasks. Several works adapted diffusion for point cloud generation\n[\n13\n]\nor 3D shape modeling\n[\n4\n]\n. Recently, consistency models\n[\n15\n]\nhave emerged as a family of one-step generative models that map noised data point directly to the clean data. While such models can be used as students for distillation or trained from scratch using consistency training\n[\n15\n,\n16\n]\n, they often enforce consistency among empirical samples, accumulating irreducible bias due to discretization errors.\nIn contrast, our work leverages Shortcut Models\n[\n6\n]\nwithin a flow-matching framework. Unlike consistency training, Shortcut Models learn to simulate a direct â€shortcutâ€ along the probability flow ODE. This approach avoids the bias accumulation of consistency training and requires significantly fewer bootstraps, while bypassing the need for complex perceptual losses or strict schedules. By adapting this to the latent triplane space, we achieve high-fidelity 3D scene completion in a single inference step.\nIII\nMETHOD\nIn this section, we present FlowSSC, a universal generative framework for Semantic Scene Completion. The core of our method is a Shortcut Latent Diffusion model that operates in a compact triplane latent space compressed by a VecSet VAE. This design allows for high-fidelity 3D semantic scene generation with real-time inference capabilities.\nIII-A\nPreliminaries: Flow Matching and Shortcut Models\nFlow Matching\nGenerative modeling via Continuous Normalizing Flows (CNFs) defines a probability path\np\nt\nâ€‹\n(\nğ’™\n)\np_{t}(\\bm{x})\nthat smoothly transforms a simple prior distribution\np\n0\nâ€‹\n(\nğ’™\n)\np_{0}(\\bm{x})\n(e.g., standard Gaussian\nğ’©\nâ€‹\n(\nğŸ\n,\nğ‘°\n)\n\\mathcal{N}(\\bm{0},\\bm{I})\n) to a complex data distribution\np\n1\nâ€‹\n(\nğ’™\n)\np_{1}(\\bm{x})\n(e.g., 3D scene latents). This transformation is governed by an Ordinary Differential Equation (ODE):\nd\nâ€‹\nğ’™\nt\nd\nâ€‹\nt\n=\nğ’—\nt\nâ€‹\n(\nğ’™\nt\n)\n,\nğ’™\n0\nâˆ¼\np\n0\nâ€‹\n(\nğ’™\n)\n,\n\\frac{\\mathrm{d}\\bm{x}_{t}}{\\mathrm{d}t}=\\bm{v}_{t}(\\bm{x}_{t}),\\quad\\bm{x}_{0}\\sim p_{0}(\\bm{x}),\n(1)\nwhere\nğ’—\nt\nâ€‹\n(\nâ‹…\n)\n\\bm{v}_{t}(\\cdot)\nis a time-dependent vector field that constructs the flow\nÏ•\nt\n\\phi_{t}\n. The standard Flow Matching (FM) objective\n[\n12\n]\naims to regress a target vector field\nğ’–\nt\n\\bm{u}_{t}\nthat generates a desired probability path\np\nt\np_{t}\n:\nâ„’\nFM\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nt\n,\np\nt\nâ€‹\n(\nğ’™\n)\nâ€‹\nâ€–\nğ’—\nÎ¸\nâ€‹\n(\nğ’™\n,\nt\n)\nâˆ’\nğ’–\nt\nâ€‹\n(\nğ’™\n)\nâ€–\n2\n.\n\\mathcal{L}_{\\text{FM}}(\\theta)=\\mathbb{E}_{t,p_{t}(\\bm{x})}\\|\\bm{v}_{\\theta}(\\bm{x},t)-\\bm{u}_{t}(\\bm{x})\\|^{2}.\n(2)\nShortcut Models\nStandard Flow Matching models learn the instantaneous velocity field (an ODE), which requires many small steps for accurate integration due to the curved probability paths. Naively taking large steps leads to significant discretization error. Shortcut Models\n[\n6\n]\naddress this by introducing a new conditioning variable: the step size\nd\nd\n. The model learns a\nshortcut function\ns\nÎ¸\nâ€‹\n(\nğ’™\nt\n,\nt\n,\nd\n)\ns_{\\theta}(\\bm{x}_{t},t,d)\nrepresenting the\nnormalized direction\nfrom the current state\nğ’™\nt\n\\bm{x}_{t}\nto the next state\nğ’™\nt\n+\nd\n\\bm{x}_{t+d}\n:\nğ’™\nt\n+\nd\n=\nğ’™\nt\n+\nd\nâ‹…\ns\nÎ¸\nâ€‹\n(\nğ’™\nt\n,\nt\n,\nd\n)\n.\n\\bm{x}_{t+d}=\\bm{x}_{t}+d\\cdot s_{\\theta}(\\bm{x}_{t},t,d).\n(3)\nBy conditioning on\nd\nd\n, the model accounts for the pathâ€™s future curvature, allowing it to â€jumpâ€ directly to the correct point rather than drifting off-track. The training leverages a Self-Consistency property: taking one shortcut step of size\n2\nâ€‹\nd\n2d\nshould be equivalent to taking two consecutive steps of size\nd\nd\n. Formally:\ns\nâ€‹\n(\nğ’™\nt\n,\nt\n,\n2\nâ€‹\nd\n)\n=\n1\n2\nâ€‹\ns\nâ€‹\n(\nğ’™\nt\n,\nt\n,\nd\n)\n+\n1\n2\nâ€‹\ns\nâ€‹\n(\nğ’™\nt\n+\nd\nâ€²\n,\nt\n+\nd\n,\nd\n)\n,\ns(\\bm{x}_{t},t,2d)=\\frac{1}{2}s(\\bm{x}_{t},t,d)+\\frac{1}{2}s(\\bm{x}^{\\prime}_{t+d},t+d,d),\n(4)\nwhere\nğ’™\nt\n+\nd\nâ€²\n\\bm{x}^{\\prime}_{t+d}\nis the intermediate state from the first step. This formulation allows the model to generalize flow matching: as\nd\nâ†’\n0\nd\\to 0\n,\ns\nÎ¸\ns_{\\theta}\nconverges to the instantaneous flow; for\nd\n=\n1\nd=1\n, it learns a direct one-step mapping. Thus, a single model can support flexible inference budgets, from 1-step generation to refined multi-step sampling.\nIII-B\nFlowSSC Framework\nAchieving high-fidelity 3D scene completion is particularly challenging for single-step generative models due to the high dimensionality of 3D data and severe occlusion. To address this, FlowSSC decomposes the task into three manageable stages, effectively combining the strengths of discriminative representation learning and generative refinement:\n1.\nLatent Compression via VecSet VAE.\nWe first establish a compact and efficient latent space. Instead of operating on sparse, high-dimensional voxels (\n256\nÃ—\n256\nÃ—\n32\n256\\times 256\\times 32\n), we train a\nVecSet VAE\n(Section\nIII-C\n) that compresses the scene into a dense Triplane representation\nğ’‰\n\\bm{h}\n. This significantly reduces computational cost while preserving fine-grained geometric details via Cross-Attention.\n2.\nCoarse Prediction Condition.\nTo provide a structural prior for generation, we predict an initial coarse voxel grid\nğ‘¿\nc\nâ€‹\no\nâ€‹\na\nâ€‹\nr\nâ€‹\ns\nâ€‹\ne\n\\bm{X}_{coarse}\nfrom the input image. We adopt established architectural paradigms from state-of-the-art feedforward methods, which typically employ a 2D backbone for feature extraction followed by a 2D-to-3D lifting module. This predicted\nğ‘¿\nc\nâ€‹\no\nâ€‹\na\nâ€‹\nr\nâ€‹\ns\nâ€‹\ne\n\\bm{X}_{coarse}\nserves as a strong condition\nğ’„\n\\bm{c}\n, capturing global semantic layout but potentially lacking detail in occluded regions.\n3.\nShortcut Latent Diffusion model.\nFinally, we perform generative refinement in the triplane latent space. We employ a Shortcut Latent Diffusion Model (Section\nIII-E\n) conditioned on\nğ’‰\nc\nâ€‹\no\nâ€‹\na\nâ€‹\nr\nâ€‹\ns\nâ€‹\ne\n\\bm{h}_{coarse}\n. By learning a direct â€shortcutâ€ mapping from noise to the clean distribution, our model can rectify geometric errors and hallucinate plausible details in occluded areas in as few as one single step, achieving real-time high-fidelity completion.\nIII-C\nLatent Space Compression via VecSet VAE\nTo enable efficient diffusion modeling, we compress the high-dimensional voxel grid into a compact triplane latent space. Unlike standard 3D-CNNs that process sparse data inefficiently, we adopt a VecSet-inspired architecture\n[\n23\n]\n, leveraging Cross-Attention to aggregate geometric information into a structured latent representation. This formulation treats the input as a set of tokens and learns a function approximator that maps spatial queries to occupancy probabilities.\nSet-to-Set Encoding Mechanism\nThe encoder processes the input voxel grid\nğ‘¿\nâˆˆ\n{\n0\n,\n1\n}\nH\nÃ—\nW\nÃ—\nD\n\\bm{X}\\in\\{0,1\\}^{H\\times W\\times D}\nas a sparse set of non-empty feature tokens. Specifically, we extract voxel coordinates and features (if available) to form the input set\nğ’±\n=\n{\n(\nğ¯\ni\n,\nğ©\ni\n)\n}\ni\n=\n1\nN\n\\mathcal{V}=\\{(\\mathbf{v}_{i},\\mathbf{p}_{i})\\}_{i=1}^{N}\n, where\nğ¯\ni\n\\mathbf{v}_{i}\nrepresents the local geometry feature and\nğ©\ni\nâˆˆ\nâ„\n3\n\\mathbf{p}_{i}\\in\\mathbb{R}^{3}\ncorresponds to the normalized spatial coordinate.\nTo compress this set into a fixed-size representation, we introduce a set of\nTriplane Queries\nğ‘¸\nâˆˆ\nâ„\n(\nH\nt\nâ€‹\np\nâ€‹\nW\nt\nâ€‹\np\n+\n2\nâ€‹\nH\nt\nâ€‹\np\nâ€‹\nD\nt\nâ€‹\np\n)\nÃ—\nC\n\\bm{Q}\\in\\mathbb{R}^{(H_{tp}W_{tp}+2H_{tp}D_{tp})\\times C}\n, which are initialized with 2D Fourier positional encodings corresponding to the projected planes (XY, XZ, YZ).\nWe employ Multi-Head Cross-Attention (MHCA) where\nğ‘¸\n\\bm{Q}\nserves as the queries, and the input set\nğ’±\n\\mathcal{V}\nacts as keys and values. The update rule for the query features is:\nğ’‰\n=\nMHCA\nâ€‹\n(\nğ‘¸\n,\nğ’±\n)\n=\nSoftmax\nâ€‹\n(\nğ‘¸\nâ€‹\n(\nğ’±\nemb\nâ€‹\nğ‘¾\nK\n)\nT\nd\n)\nâ€‹\n(\nğ’±\nemb\nâ€‹\nğ‘¾\nV\n)\n,\n\\bm{h}=\\text{MHCA}(\\bm{Q},\\mathcal{V})=\\text{Softmax}\\left(\\frac{\\bm{Q}(\\mathcal{V}_{\\text{emb}}\\bm{W}_{K})^{T}}{\\sqrt{d}}\\right)(\\mathcal{V}_{\\text{emb}}\\bm{W}_{V}),\n(5)\nwhere\nğ’±\nemb\n\\mathcal{V}_{\\text{emb}}\ndenotes the position-augmented input tokens. Intuitively, each element in\nğ‘¸\n\\bm{Q}\nlearns to attend to and aggregate features from relevant spatial regions defined by the attention weights, effectively learning an implicit interpolation function. This â€probingâ€ mechanism allows the model to capture fine-grained details regardless of the input sparsity pattern. To handle the quadratic complexity of attention, we utilize FlashAttention\n[\n5\n]\nfor efficient computation.\nThe resulting queries are reshaped into three orthogonal feature planes:\nğ’‰\nx\nâ€‹\ny\nâˆˆ\nâ„\nH\nt\nâ€‹\np\nÃ—\nW\nt\nâ€‹\np\nÃ—\nC\n\\bm{h}_{xy}\\in\\mathbb{R}^{H_{tp}\\times W_{tp}\\times C}\n,\nğ’‰\nx\nâ€‹\nz\nâˆˆ\nâ„\nH\nt\nâ€‹\np\nÃ—\nD\nt\nâ€‹\np\nÃ—\nC\n\\bm{h}_{xz}\\in\\mathbb{R}^{H_{tp}\\times D_{tp}\\times C}\n, and\nğ’‰\ny\nâ€‹\nz\nâˆˆ\nâ„\nW\nt\nâ€‹\np\nÃ—\nD\nt\nâ€‹\np\nÃ—\nC\n\\bm{h}_{yz}\\in\\mathbb{R}^{W_{tp}\\times D_{tp}\\times C}\n. In our setting,\nH\nt\nâ€‹\np\n=\nW\nt\nâ€‹\np\n=\n128\nH_{tp}=W_{tp}=128\n,\nD\nt\nâ€‹\np\n=\n16\nD_{tp}=16\n, and\nC\n=\n64\nC=64\n, achieving a significant compression ratio while maintaining high fidelity (85.91% IoU) compared to baseline CNNs.\nDecoder\nThe decoder transforms the triplane latents back to the voxel space. For any query point\nğ±\nâˆˆ\nâ„\n3\n\\mathbf{x}\\in\\mathbb{R}^{3}\nin the target grid, we project it onto each of the three feature planes to retrieve features via bilinear interpolation. These features are summed and passed through a lightweight MLP to predict the occupancy probability\nO\n^\nâ€‹\n(\nğ±\n)\n\\hat{O}(\\mathbf{x})\n. To efficiently reconstruct the full\n256\nÃ—\n256\nÃ—\n32\n256\\times 256\\times 32\ngrid, we employ a shallow 3D-CNN decoder that upsamples the aggregated triplane features. During the subsequent diffusion training, this VAE is frozen, serving as a robust neural tokenizer.\nIII-D\nCoarse Prediction as Condition\nTo guide the generation process, we predict an initial coarse representation from the monocular image\nğ‘°\n\\bm{I}\n. The prediction network\nF\np\nâ€‹\nr\nâ€‹\ne\nâ€‹\nd\nF_{pred}\nfollows a standard vision-centric SSC architecture. It starts with a 2D backbone extracting multi-scale visual features, which are then lifted into 3D space via a view projection module to establish geometric correspondence. These 3D features are processed by a 3D encoder-decoder to complete the scene geometry and hallucinate occluded regions. Finally, a Prediction Head produces a coarse semantic voxel grid. This coarse estimate serves as a structural prior and is encoded into the triplane latent space to obtain the condition\nğ‘¿\nc\nâ€‹\no\nâ€‹\na\nâ€‹\nr\nâ€‹\ns\nâ€‹\ne\n\\bm{X}_{coarse}\nfor the subsequent diffusion refinement.\nIII-E\nShortcut Latent Diffusion Model\nWe employ a Shortcut Latent Diffusion Model to refine the coarse condition\nğ’‰\nc\nâ€‹\no\nâ€‹\na\nâ€‹\nr\nâ€‹\ns\nâ€‹\ne\n\\bm{h}_{coarse}\ninto high-fidelity triplanes. Here,\nğ’‰\nc\nâ€‹\no\nâ€‹\na\nâ€‹\nr\nâ€‹\ns\nâ€‹\ne\n\\bm{h}_{coarse}\nis obtained by encoding the coarse prediction\nğ‘¿\nc\nâ€‹\no\nâ€‹\na\nâ€‹\nr\nâ€‹\ns\nâ€‹\ne\n\\bm{X}_{coarse}\nvia the pre-trained VecSet VAE encoder. Unlike standard diffusion models, our approach utilizes a Triplane Diffusion Transformer (DiT) and a Shortcut Model to enable flexible inference from one to many steps.\nTriplane DiT Architecture\nWe adopt a Diffusion Transformer (DiT) tailored for triplane representations. The input consists of the channel-wise concatenation of the noisy triplane\nğ’‰\nt\nâˆˆ\nâ„\n3\nÃ—\nH\nÃ—\nW\nÃ—\nC\n\\bm{h}_{t}\\in\\mathbb{R}^{3\\times H\\times W\\times C}\nand the coarse condition\nğ’‰\nc\nâ€‹\no\nâ€‹\na\nâ€‹\nr\nâ€‹\ns\nâ€‹\ne\n\\bm{h}_{coarse}\n. These are patchified into a sequence of tokens, which are then processed by a series of Transformer blocks.\nA key modification for the Shortcut formulation is the conditioning mechanism. We employ Adaptive Layer Normalization (AdaLN) to inject both the current timestep\nt\nt\nand the step size\nd\nd\n. Specifically,\nt\nt\nand\nd\nd\nare independently mapped to high-dimensional embeddings using Fourier features and MLPs, then summed to form a unified conditioning vector. This vector regresses the scale and shift parameters\nğœ¸\nâ€‹\n(\nt\n,\nd\n)\n,\nğœ·\nâ€‹\n(\nt\n,\nd\n)\n\\bm{\\gamma}(t,d),\\bm{\\beta}(t,d)\nfor the AdaLN layers:\nAdaLN\nâ€‹\n(\nğ’›\n,\nt\n,\nd\n)\n=\nğœ¸\nâ€‹\n(\nt\n,\nd\n)\nâ‹…\nLayerNorm\nâ€‹\n(\nğ’›\n)\n+\nğœ·\nâ€‹\n(\nt\n,\nd\n)\n.\n\\text{AdaLN}(\\bm{z},t,d)=\\bm{\\gamma}(t,d)\\cdot\\text{LayerNorm}(\\bm{z})+\\bm{\\beta}(t,d).\n(6)\nThis design allows dimensions of the condition such as the denoising stride\nd\nd\nto globally modulate the feature statistics, enabling the network to dynamically adapt its computation for either fine-grained flow matching (\nd\nâ†’\n0\nd\\to 0\n) or large shortcut jumps (\nd\nâ‰«\n0\nd\\gg 0\n). Optimization is performed on the valid triplane regions, masking out empty corners in the composed representation.\nShortcut Flow Matching\nWe train the model with a unified objective that combines instantaneous flow matching and long-range self-consistency. The total loss\nâ„’\nğ’\nâ€‹\n(\nÎ¸\n)\n\\mathcal{L}^{\\mathbf{S}}(\\theta)\nis defined as the expectation over data samples, timesteps, and step sizes\n[\n6\n]\n:\nâ„’\nğ’\nâ€‹\n(\nÎ¸\n)\n=\nâ€–\ns\nÎ¸\nâ€‹\n(\nğ’‰\nt\n,\nt\n,\n0\n)\nâˆ’\n(\nğ’‰\ng\nâ€‹\nt\nâˆ’\nğ’‰\nn\nâ€‹\no\nâ€‹\ni\nâ€‹\ns\nâ€‹\ne\n)\nâ€–\n2\nâŸ\nFlow-Matching\n+\nâ€–\ns\nÎ¸\nâ€‹\n(\nğ’‰\nt\n,\nt\n,\n2\nâ€‹\nd\n)\nâˆ’\ns\ntarget\nâ€–\n2\nâŸ\nSelf-Consistency\n,\n\\begin{split}\\mathcal{L}^{\\mathbf{S}}(\\theta)=&\\underbrace{\\|s_{\\theta}(\\bm{h}_{t},t,0)-(\\bm{h}_{gt}-\\bm{h}_{noise})\\|^{2}}_{\\text{Flow-Matching}}\\\\\n&+\\underbrace{\\|s_{\\theta}(\\bm{h}_{t},t,2d)-s_{\\text{target}}\\|^{2}}_{\\text{Self-Consistency}},\\end{split}\n(7)\nwhere the self-consistency target is constructed by concatenating two smaller steps:\ns\ntarget\n=\n1\n2\nâ€‹\ns\nÎ¸\nâ€‹\n(\nğ’‰\nt\n,\nt\n,\nd\n)\n+\n1\n2\nâ€‹\ns\nÎ¸\nâ€‹\n(\nğ’‰\nt\n+\nd\nâ€²\n,\nt\n+\nd\n,\nd\n)\ns_{\\text{target}}=\\frac{1}{2}s_{\\theta}(\\bm{h}_{t},t,d)+\\frac{1}{2}s_{\\theta}(\\bm{h}^{\\prime}_{t+d},t+d,d)\n.\nIntuitively, the Flow-Matching term grounds the model at small step sizes (\nd\nâ†’\n0\nd\\to 0\n) to match the empirical velocity field, ensuring stable ODE integration. The Self-Consistency term propagates this generation capability from multi-step to few-step and ultimately to One-Step Generation, by enforcing that a large jump\n2\nâ€‹\nd\n2d\nis consistent with two sequential jumps of size\nd\nd\n. This allows a single model to support any inference budget from 1 step to\nN\nN\nsteps.\nFigure 3\n:\nQualitative results on the SemanticKITTI validation set.\nIII-F\nLoss Function and Training\nThis section describes the objective functions used to train our three-stage framework.\nTriplane Autoencoder Training\nThe triplane autoencoder (Section\nIII-C\n) is trained to minimize the reconstruction loss between the input voxel grid\nğ‘¿\n\\bm{X}\nand the reconstructed output\nğ‘¿\n^\n=\nD\na\nâ€‹\ne\nâ€‹\n(\nE\na\nâ€‹\ne\nâ€‹\n(\nğ‘¿\n)\n)\n\\hat{\\bm{X}}=D_{ae}(E_{ae}(\\bm{X}))\n. Following standard autoencoder training, we use a combination of voxel-wise cross-entropy loss and scene-level consistency losses:\nâ„’\nA\nâ€‹\nE\n=\nÎ»\nc\nâ€‹\ne\nâ€‹\nâ„’\nc\nâ€‹\ne\n+\nÎ»\ng\nâ€‹\ne\nâ€‹\no\nâ€‹\nâ„’\ns\nâ€‹\nc\nâ€‹\na\nâ€‹\nl\ng\nâ€‹\ne\nâ€‹\no\n+\nÎ»\ns\nâ€‹\ne\nâ€‹\nm\nâ€‹\nâ„’\ns\nâ€‹\nc\nâ€‹\na\nâ€‹\nl\ns\nâ€‹\ne\nâ€‹\nm\n,\n\\mathcal{L}_{AE}=\\lambda_{ce}\\mathcal{L}_{ce}+\\lambda_{geo}\\mathcal{L}_{scal}^{geo}+\\lambda_{sem}\\mathcal{L}_{scal}^{sem},\n(8)\nwhere\nâ„’\nc\nâ€‹\ne\n\\mathcal{L}_{ce}\nis the cross-entropy loss for semantic classification, and\nâ„’\ns\nâ€‹\nc\nâ€‹\na\nâ€‹\nl\ng\nâ€‹\ne\nâ€‹\no\n\\mathcal{L}_{scal}^{geo}\n,\nâ„’\ns\nâ€‹\nc\nâ€‹\na\nâ€‹\nl\ns\nâ€‹\ne\nâ€‹\nm\n\\mathcal{L}_{scal}^{sem}\nare scene-level geometric and semantic consistency losses following MonoScene\n[\n2\n]\n. Implementation-wise, we train the VAE on 8 NVIDIA H20-3e GPUs with a total batch size of 4 for 50,000 iterations. We use the AdamW optimizer with a cosine annealing learning rate schedule, starting from\n1\nÃ—\n10\nâˆ’\n3\n1\\times 10^{-3}\nafter a 2,000-iteration warmup.\nShortcut Refinement Training\nTo efficiently optimize the joint objective\nâ„’\nğ’\n\\mathcal{L}^{\\mathbf{S}}\n, we employ a mixed sampling strategy within each mini-batch. We utilize a fixed fraction of samples (25%) to optimize the Self-Consistency term, sampling random step sizes\nd\nâˆˆ\n[\nÎ´\n,\n1\nâˆ’\nt\n]\nd\\in[\\delta,1-t]\n, while the remaining 75% calculate the standard Flow Matching loss with\nd\n=\n0\nd=0\nto ensure trajectory fidelity. To mitigate overfitting on the SemanticKITTI dataset, we employ an aggressive offline data augmentation strategy, increasing the effective dataset size by\n8\nÃ—\n8\\times\nthrough random flipping and rotation. Implementation-wise, we train the model on 8 NVIDIA H20-3e GPUs with a batch size of 56 for 500,000 iterations, using the AdamW optimizer with a Cosine Annealing learning rate schedule (initial\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\n) and no weight decay.\nTABLE I\n:\nQuantitative results on SemanticKITTI\n[\n1\n]\ntest set.\nThe best and the second best results are in\nbold\nand\nunderlined\n.\nMethod\nIoU\nmIoU\nâ– \n\\blacksquare\nroad\nâ– \n\\blacksquare\nsidewalk\nâ– \n\\blacksquare\nparking\nâ– \n\\blacksquare\nother-grnd.\nâ– \n\\blacksquare\nbuilding\nâ– \n\\blacksquare\ncar\nâ– \n\\blacksquare\ntruck\nâ– \n\\blacksquare\nbicycle\nâ– \n\\blacksquare\nmotorcycle\nâ– \n\\blacksquare\nother-veh.\nâ– \n\\blacksquare\nvegetation\nâ– \n\\blacksquare\ntrunk\nâ– \n\\blacksquare\nterrain\nâ– \n\\blacksquare\nperson\nâ– \n\\blacksquare\nbicyclist\nâ– \n\\blacksquare\nmotorcyclist\nâ– \n\\blacksquare\nfence\nâ– \n\\blacksquare\npole\nâ– \n\\blacksquare\ntraf.-sign\nOccFormer\n[\n24\n]\n34.53\n12.32\n55.90\n30.30\n31.50\n6.50\n15.70\n21.60\n1.20\n1.50\n1.70\n3.20\n16.80\n3.90\n21.30\n2.20\n1.10\n0.20\n11.90\n3.80\n3.70\nVoxFormer-S\n[\n10\n]\n42.95\n12.20\n53.90\n25.30\n21.10\n5.60\n19.80\n20.80\n3.50\n1.00\n0.70\n3.70\n22.40\n7.50\n21.30\n1.40\n2.60\n0.20\n11.10\n5.10\n4.90\nVoxFormer-T\n[\n10\n]\n43.21\n13.41\n54.10\n26.90\n25.10\n7.30\n23.50\n21.70\n3.60\n1.90\n1.60\n4.10\n24.40\n8.10\n24.20\n1.60\n1.10\n0.00\n13.10\n6.60\n5.70\nDepthSSC\n[\n21\n]\n44.58\n13.11\n55.64\n27.25\n25.72\n5.78\n20.46\n21.94\n3.74\n1.35\n0.98\n4.17\n23.37\n7.64\n21.56\n1.34\n2.79\n0.28\n12.94\n5.87\n6.23\nSymphonize\n[\n9\n]\n42.19\n15.04\n58.40\n29.30\n26.90\n11.70\n24.70\n23.60\n3.20\n3.60\n2.60\n5.60\n24.20\n10.00\n23.10\n3.20\n1.90\n2.00\n16.10\n7.70\n8.00\nCGFormer\n[\n22\n]\n44.41\n16.63\n64.30\n34.20\n34.10\n12.10\n25.80\n26.10\n4.30\n3.70\n1.30\n2.70\n24.50\n11.20\n29.30\n1.70\n3.60\n0.40\n18.70\n8.70\n9.30\nET-Former\n[\n11\n]\n51.49\n16.30\n57.64\n25.80\n16.68\n0.87\n26.74\n36.16\n12.95\n0.69\n0.32\n8.41\n33.95\n11.58\n37.01\n1.33\n2.58\n0.32\n9.52\n19.60\n6.90\nFlowSSC (ours)\n56.97\n19.52\n67.75\n36.61\n21.60\n0.00\n41.63\n42.23\n19.32\n0.64\n0.67\n10.08\n38.10\n12.21\n45.42\n0.92\n1.44\n0.00\n12.18\n14.96\n5.06\nIV\nEXPERIMENTS\nIn this section, we evaluate the performance of our proposed Semantic Scene Completion framework. We first describe the dataset and evaluation metrics used. Then, we present quantitative results comparing our method to state-of-the-art approaches and provide a detailed analysis. Finally, we include qualitative results and potentially ablation studies to validate the effectiveness of our key components.\nIV-A\nQuantitative Results\nWe quantitatively evaluate our methodâ€™s performance on the SemanticKITTI Semantic Scene Completion test set and compare it with current state-of-the-art methods. The results are presented in Table\nI\n.\nTable\nI\nlists the performance of our proposed method and several leading existing approaches on key metrics, including Semantic Mean IoU (mIoU), Geometric IoU, and per-class Semantic IoU for the 20 semantic categories.\nAs shown in Table\nI\n, our proposed method demonstrates superior performance on the SemanticKITTI test set, achieving new state-of-the-art results. Specifically, our method obtains a Semantic mIoU of 19.52% and a IoU of 56.97%. These results not only set a new benchmark but also represent a substantial leap over prior leading methods, highlighting the efficacy of our generative framework.\nAn analysis of the per-class performance provides further insights into the robustness of our model. Notably, our method achieves the highest IoU across all reported semantic categories, even on those that typically suffer from severe occlusion or sparse observations. This superiority is particularly evident in the â€™roadâ€™ category, where our model achieves an impressive 41.63% IoU. For context, the next best performing method reaches only 26.74% on this challenging class, demonstrating an unprecedented capability in completing difficult structures. As illustrated in Fig.\n3\n, buildings (yellow) are frequently occluded by roadside vegetation in driving scenarios. Existing methods struggle to recover these structures, often resulting in empty regions or large artifacts. In contrast, FlowSSC effectively learns implicit â€outdoor layout rulesâ€ from the dataset distributionâ€”for instance, that buildings typically reside behind vegetation layersâ€”and successfully reconstructs the structural contours of these occluded buildings. This capability to infer environmental context despite severe occlusion is crucial for situational awareness in real-time applications.\nThis strong performance across a diverse range of categories indicates that our method is capable of accurately completing and segmenting various objects and structures within the 3D scene.\nIV-B\nAblation Studies\nWe conduct comprehensive ablation studies to validate the effectiveness of key components in our FlowSSC framework. All experiments are conducted on the SemanticKITTI validation set.\nEffect of Shortcut Latent Diffusion Refiner\nWe first investigate the contribution of our Shortcut Latent Diffusion refiner by comparing the performance with and without the refinement stage. Table\nII\nshows the results.\nTABLE II\n:\nAblation study on the effect of Shortcut Latent Diffusion refiner.\nMethod\nIoU (%)\nmIoU (%)\nCoarse Prediction Only\n15.86\n50.77\n+ Shortcut Diffusion Refiner\n19.51\n56.60\nImprovement\n+3.65\n+5.83\nAs shown in Table\nII\n, the Shortcut Latent Diffusion refiner provides a significant improvement over the coarse prediction baseline. This demonstrates that our generative refinement effectively corrects errors and synthesizes high-fidelity details in the latent triplane space.\nImpact of Inference Steps\nTABLE III\n:\nAblation study on the number of inference steps in Shortcut Model.\n# Steps\nIoU (%)\nmIoU (%)\nTime (ms)\n1\n56.98\n19.55\n66\n2\n56.63\n19.52\n131\n4\n56.15\n19.37\n261\n8\n55.68\n19.19\n518\n16\n55.54\n19.07\n1044\nWe evaluate the performance of FlowSSC with varying numbers of sampling steps, as shown in Table\nIII\n. Contrary to standard diffusion models where quality typically improves with more steps, our method achieves peak performance with a single step.\nThe results indicate that 1-step inference yields the highest IoU (56.98%) and mIoU (19.55%). This behavior stems from the unique property of the Shortcut learning objective. The Self-Consistency loss explicitly trains the model to learn a precise â€shortcutâ€ mapping to the data distribution. At the current training stage (110k iterations), the model has effectively converged to an optimal direct trajectory. Increasing the number of steps introduces intermediate states that, while theoretically valid, may suffer from minor cumulative errors or trajectory drift compared to the directly optimized one-step path. This characteristic is highly advantageous for real-time applications, as it allows FlowSSC to deliver its best accuracy at maximum speed (66ms), eliminating the need for expensive multi-step refinement.\nComparison of VAE Architectures\nWe compare our VecSet VAE architecture (based on Cross-Attention) with a conventional Conv-based VAE. Both architectures compress 3D voxels into triplane latents, but differ in how they aggregate spatial information. Table\nIV\nshows the reconstruction quality on the validation set.\nTABLE IV\n:\nComparison of VAE architectures for triplane representation.\nVAE Architecture\nIoU (%)\nmIoU (%)\nConv-based VAE\n84.51\n77.66\nVecSet VAE (Ours)\n91.10\n85.89\nOur VecSet VAE, which leverages Cross-Attention to aggregate 3D voxel features into learnable triplane queries, achieves superior reconstruction quality compared to the Conv-based baseline. This improvement can be attributed to the ability of Cross-Attention to selectively aggregate relevant spatial information from the entire voxel grid, rather than relying on local convolutional receptive fields. The enhanced triplane representation also benefits the subsequent diffusion refinement stage.\nIV-C\nQualitative Results\nTo complement our quantitative evaluation, we present qualitative results on the SemanticKITTI test set in Figure\n3\n. These visualizations offer crucial insights into the practical capabilities of our method, particularly its ability to handle challenging real-world scenarios.\nAs illustrated in Figure\n3\n, our method demonstrates exceptional visual fidelity, producing 3D semantic scene completions that are remarkably close to the ground truth logic and geometry. Beyond strong quantitative metrics, the qualitative results highlight the methodâ€™s generative capability, particularly in challenging occluded regions.\nA critical observation is our modelâ€™s ability to infer complete structures from sparse visual cues. As highlighted in the red boxes, FlowSSC successfully â€hallucinatesâ€ the missing geometry and semantics in high-occlusion scenarios, such as visual blind spots at road curves, the continuity of roadside vegetation, and the spatial layout of buildings. Where traditional feedforward models often fail or output blurry predictions, our approach leverages the strong generative prior learned from large-scale data, effectively inferring plausible unseen parts consistent with the visible context. The single-step refiner plays a key role here, transforming coarse, ambiguous estimates into sharp, coherent structures.\nThis capability to predict the â€unknownâ€ from limited cues is paramount for future autonomous systems. It ensures that downstream planning and safety modules operate on a complete and coherent world representation, rather than a fragmented one. Furthermore, this behavior underscores the scalable nature of our generative framework: as it absorbs more data, its internal world model becomes increasingly consistent and predictive, enabling it to handle complex, unseen scenarios with greater robustness. For more dynamic comparisons between FlowSSC, the baseline CGFormer, and Ground Truth, please refer to the supplementary video accompanying this letter.\nIV-D\nComputational Analysis\nWe provide a comprehensive computational analysis of our FlowSSC framework, addressing inference speed, memory consumption, and the trade-offs between accuracy and efficiency.\nPer-Stage Breakdown\nTo provide detailed insights into the computational characteristics of each component, Table\nV\npresents a breakdown of inference time, FLOPs, and memory usage across all three stages of our framework.\nTABLE V\n:\nPer-stage breakdown of inference time and memory.\nStage\nTime (ms)\nMem (GB)\nDiT Refinement\n66\n-\nVAE Decoder\n150\n-\nTotal\n0.216\n31.56\nTable\nV\nreveals that the VAE Decoder constitutes the majority of the inference time (150 ms), primarily due to the heavy computation required to decode the compact triplane representation back into a high-resolution dense voxel grid (\n256\nÃ—\n256\nÃ—\n32\n256\\times 256\\times 32\n). In contrast, the DiT Refinement stage is remarkably efficient, requiring only 66 ms per scene. This efficiency stems from our use of the compressed triplane space and the single-step inference capability of the Shortcut Model.\nThe total inference time is 0.216 seconds per scene (approximately 4.6 FPS), enabling near real-time performance. regarding memory, the total GPU memory consumption is 30.52 GB. While substantial, this fits within the VRAM capacity of high-end consumer GPUs (e.g., RTX 5090) and professional workstations, making high-fidelity generative SSC accessible. We note that:\nâ€¢\nThe memory usage is a trade-off for the state-of-the-art accuracy achieved by our two-stage generative approach.\nâ€¢\nThe separate stages allow for pipelined optimization in future deployment scenarios.\nâ€¢\nFuture work will suggest model compression techniques (e.g., pruning, quantization) to further reduce memory requirements while maintaining accuracy.\nOverall Performance\nDespite the computational overhead of the multi-stage pipeline, the total inference speed of our FlowSSC framework remains comparable to other monocular SSC methods while achieving superior accuracy. This demonstrates that our design successfully balances performance and efficiency through the strategic use of compact triplane representation and single-step diffusion.\nV\nConclusion\nIn this paper, we introduced FlowSSC, a universal generative framework for monocular Semantic Scene Completion that reconciles high-fidelity generation with real-time efficiency. By compressing the 3D scene into a compact triplane latent space via our VecSet VAE and employing a teacher-free Shortcut Flow Matching training objective, our model learns a direct one-step mapping for generative refinement. This approach effectively addresses the inherent ill-posedness of the task, recovering fine-grained geometry and semantics from monocular input. Extensive experiments on the SemanticKITTI dataset demonstrate that FlowSSC establishes new state-of-the-art performance while maintaining practical inference speeds suitable for autonomous driving applications.\nDespite its strong performance, we identify several avenues for future improvement. First, while our shortcut training enables fast inference, the Flow Matching training process remains computationally intensive, and the Triplane Diffusion Transformer architecture demands significant GPU memory. Consequently, exploring techniques to reduce model complexity and designing memory-efficient architectures remains a priority to facilitate deployment on edge devices. Second, future research should investigate how Self-Consistency objectives influence training dynamics to accelerate convergence. Third, to fully exploit the generalization potential of our generative framework, scaling the training to larger, more diverse datasets is crucial. Finally, extending our efficient prediction paradigm to video inputs for improved temporal consistency is a key direction for future work.\nIn summary, FlowSSC presents a universal paradigm for generative 3D perception, demonstrating that semantic scene completion task can be significantly enhanced through our efficient one-step latent refinement. By synergizing the robust representational capacity of triplanes with the rapid inference of Shortcut Flow Matching, we pave the way for real-time, high-fidelity generative 3D scene understanding.\nReferences\n[1]\nJ. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall\n(2019)\nSemanticKITTI: A dataset for semantic scene understanding of lidar sequences\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.Â 9297â€“9307\n.\nCited by:\nÂ§I\n,\nTABLE I\n,\nTABLE I\n.\n[2]\nA. Cao and R. de Charette\n(2022)\nMonoScene: monocular 3d semantic scene completion\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 3981â€“3991\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nIII-F\n.\n[3]\nR. Cheng, C. Agia, Y. Ren, X. Li, and L. Bingbing\n(2021)\nS3cnet: a sparse semantic scene completion network for lidar point clouds\n.\nIn\nConference on Robot Learning\n,\npp.Â 2148â€“2161\n.\nCited by:\nÂ§\nII-A\n.\n[4]\nG. Chou, Y. Bahat, and F. Heide\n(2023)\nDiffusion-sdf: conditional generative modeling of signed distance functions\n.\nCited by:\nÂ§\nII-C\n.\n[5]\nT. Dao\n(2024)\nFlashAttention-2: faster attention with better parallelism and work partitioning\n.\nIn\nInternational Conference on Learning Representations (ICLR)\n,\nCited by:\nÂ§\nIII-C\n.\n[6]\nK. Frans, D. Hafner, S. Levine, and P. Abbeel\nOne step diffusion via shortcut models\n.\nIn\nThe Thirteenth International Conference on Learning Representations\n,\nCited by:\nÂ§I\n,\nÂ§\nII-C\n,\nÂ§\nIII-A\n,\nÂ§\nIII-E\n.\n[7]\nJ. Ho, A. Jain, and P. Abbeel\n(2020)\nDenoising diffusion probabilistic models\n.\nAdvances in neural information processing systems\n33\n,\npp.Â 6840â€“6851\n.\nCited by:\nÂ§\nII-C\n.\n[8]\nY. Huang, W. Zheng, Y. Zhang, J. Zhou, and J. Lu\n(2023)\nTri-perspective view for vision-based 3d semantic occupancy prediction\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 9223â€“9232\n.\nCited by:\nÂ§\nII-B\n.\n[9]\nH. Jiang, T. Cheng, N. Gao, H. Zhang, W. Liu, and X. Wang\n(2024)\nSymphonize 3d semantic scene completion with contextual instance queries\n.\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n.\nCited by:\nÂ§\nII-B\n,\nTABLE I\n.\n[10]\nY. Li, Z. Yu, C. B. Choy, C. Xiao, J. M. Ãlvarez, S. Fidler, C. Feng, and A. Anandkumar\n(2023)\nVoxFormer: sparse voxel transformer for camera-based 3d semantic scene completion\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 9087â€“9098\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nTABLE I\n,\nTABLE I\n.\n[11]\nJ. Liang, H. Yin, X. Qi, J. J. Park, M. Sun, R. Madhivanan, and D. Manocha\n(2025)\nET-former: efficient triplane deformable attention for 3d semantic scene completion from monocular camera\n.\nIn\n2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nCited by:\nÂ§\nII-B\n,\nTABLE I\n.\n[12]\nY. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le\nFlow matching for generative modeling\n.\nIn\nThe Eleventh International Conference on Learning Representations\n,\nCited by:\nÂ§\nIII-A\n.\n[13]\nS. Luo and W. Hu\n(2021-06)\nDiffusion probabilistic models for 3d point cloud generation\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\nÂ§\nII-C\n.\n[14]\nS. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser\n(2017)\nSemantic scene completion from a single depth image\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 1746â€“1754\n.\nCited by:\nÂ§\nII-A\n.\n[15]\nY. Song, P. Dhariwal, M. Chen, and I. Sutskever\n(2023)\nConsistency models\n.\narXiv preprint arXiv:2303.01469\n.\nCited by:\nÂ§\nII-C\n.\n[16]\nY. Song and P. Dhariwal\n(2024)\nImproved techniques for training consistency models\n.\nIn\nThe Twelfth International Conference on Learning Representations\n,\nExternal Links:\nLink\nCited by:\nÂ§\nII-C\n.\n[17]\nY. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole\n(2021)\nScore-based generative modeling through stochastic differential equations\n.\nIn\nInternational Conference on Learning Representations\n,\nExternal Links:\nLink\nCited by:\nÂ§\nII-C\n.\n[18]\nH. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang, and S. Han\n(2020)\nSearching efficient 3d architectures with sparse point-voxel convolution\n.\nIn\nEuropean Conference on Computer Vision\n,\nCited by:\nÂ§\nII-B\n.\n[19]\nY. Wei, L. Zhao, W. Zheng, Z. Zhu, J. Zhou, and J. Lu\n(2023)\nSurroundOcc: multi-camera 3d occupancy prediction for autonomous driving\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.Â 21729â€“21740\n.\nCited by:\nÂ§\nII-A\n.\n[20]\nX. Yan, J. Gao, J. Li, R. Zhang, Z. Li, R. Huang, and S. Cui\n(2021)\nSparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion\n.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n,\nVol.\n35\n,\npp.Â 3101â€“3109\n.\nCited by:\nÂ§\nII-A\n.\n[21]\nJ. Yao, J. Zhang, X. Pan, T. Wu, and C. Xiao\n(2025-02)\nDepthSSC: monocular 3d semantic scene completion via depth-spatial alignment and voxel adaptation\n.\npp.Â 2154â€“2163\n.\nCited by:\nTABLE I\n.\n[22]\nZ. Yu, R. Zhang, J. Ying, J. Yu, X. Hu, L. Luo, S. Cao, and H. Shen\n(2024)\nContext and geometry aware voxel transformer for semantic scene completion\n.\nIn\nAdvances in Neural Information Processing Systems\n,\nVol.\n37\n,\npp.Â 1531â€“1555\n.\nCited by:\nÂ§\nII-B\n,\nTABLE I\n.\n[23]\nB. Zhang, J. Tang, M. NieÃŸner, and P. Wonka\n(2023-07)\n3DShape2VecSet: a 3d shape representation for neural fields and generative diffusion models\n.\nACM Trans. Graph.\n42\n(\n4\n).\nExternal Links:\nISSN 0730-0301\n,\nLink\n,\nDocument\nCited by:\nÂ§I\n,\nÂ§\nIII-C\n.\n[24]\nY. Zhang, Z. Zhu, and D. Du\n(2023)\nOccFormer: dual-path transformer for vision-based 3d semantic occupancy prediction\n.\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 9433â€“9443\n.\nCited by:\nÂ§\nII-B\n,\nTABLE I\n.",
    "preview_text": "Semantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible details in occluded regions and preserve the fundamental spatial relationships of objects. Such accurate generative reasoning capability for the entire 3D space is critical in real-world applications. In this paper, we present FlowSSC, the first generative framework applied directly to monocular semantic scene completion. FlowSSC treats the SSC task as a conditional generation problem and can seamlessly integrate with existing feed-forward SSC methods to significantly boost their performance. To achieve real-time inference without compromising quality, we introduce Shortcut Flow-matching that operates in a compact triplane latent space. Unlike standard diffusion models that require hundreds of steps, our method utilizes a shortcut mechanism to achieve high-fidelity generation in a single step, enabling practical deployment in autonomous systems. Extensive experiments on SemanticKITTI demonstrate that FlowSSC achieves state-of-the-art performance, significantly outperforming existing baselines.\n\nFlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion\nZichen Xi\n1\n, Hao-Xiang Chen\n2\n, Nan Xue\n1\n, Hongyu Yan\n2\n, Qi-Yuan Feng\n2\n,\nLevent Burak Kara\n3\n, Joaquim Jorge\n4\n, Qun-Ce Xu\n2\n1\nAnt Group, China\n2\nTsinghua University, China\n3\nDepartment of Mechanical Engineering, Carnegie Mellon University, USA\n4\nInstituto Superior TÃ©cnico, the School of Engineering of the Universidade de Lisboa, Portugal\nAbstract\nSemantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible de",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "diffusion",
        "Flow Matching"
    ],
    "one_line_summary": "FlowSSCæ˜¯ä¸€ç§åŸºäºä¸€æ­¥æ½œåœ¨æ‰©æ•£å’ŒæµåŒ¹é…çš„ç”Ÿæˆå¼å•ç›®è¯­ä¹‰åœºæ™¯è¡¥å…¨æ¡†æ¶ï¼Œæ—¨åœ¨å®æ—¶æ¨æ–­è¢«é®æŒ¡çš„3Då‡ ä½•ç»“æ„ã€‚",
    "detailed_summary": "## FlowSSC è®ºæ–‡æ‘˜è¦\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nå•ç›®è¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰æ—¨åœ¨ä»å•å¼ RGBå›¾åƒæ¨æ–­å‡ºå®Œæ•´çš„ä¸‰ç»´åœºæ™¯å‡ ä½•ä¸è¯­ä¹‰ï¼Œæ˜¯è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰åº”ç”¨çš„å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”±äºå•è§†å›¾å›ºæœ‰çš„æ¨¡ç³Šæ€§å’Œä¸¥é‡é®æŒ¡ï¼Œç°æœ‰å‰é¦ˆæ–¹æ³•å¾€å¾€åœ¨é®æŒ¡åŒºåŸŸç”Ÿæˆæ¨¡ç³Šæˆ–å¹³å‡åŒ–çš„é¢„æµ‹ï¼Œç¼ºä¹å¯¹â€œä¸€å¯¹å¤šâ€æ˜ å°„é—®é¢˜çš„ç”Ÿæˆå¼æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹èƒ½åˆæˆé«˜ä¿çœŸç»†èŠ‚ï¼Œä½†å…¶è¿­ä»£é‡‡æ ·è¿‡ç¨‹è€—æ—¶è¿‡é•¿ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶åº”ç”¨éœ€æ±‚ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡º **FlowSSC**ï¼Œé¦–ä¸ªåº”ç”¨äºå•ç›®SSCçš„é€šç”¨ç”Ÿæˆå¼æ¡†æ¶ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š\n- **ç´§å‡‘ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´**ï¼šè®¾è®¡åŸºäºäº¤å‰æ³¨æ„åŠ›ï¼ˆCross-Attentionï¼‰çš„VecSet VAEï¼Œå°†é«˜ç»´ä½“ç´ ï¼ˆ256Ã—256Ã—32ï¼‰å‹ç¼©ä¸ºç´§å‡‘çš„ä¸‰å¹³é¢æ½œåœ¨è¡¨ç¤ºï¼Œåœ¨ä¿è¯é«˜é‡å»ºç²¾åº¦ï¼ˆ85.89% IoUï¼‰çš„åŒæ—¶å¤§å¹…é™ä½è®¡ç®—å¤æ‚åº¦ã€‚\n- **å•æ­¥å¿«æ·æµåŒ¹é…**ï¼šå¼•å…¥**å¿«æ·æ¨¡å‹ï¼ˆShortcut Modelsï¼‰** è®­ç»ƒç­–ç•¥ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­å­¦ä¹ ä»å™ªå£°åˆ°å¹²å‡€æ•°æ®çš„ç›´æ¥â€œå¿«æ·â€æ˜ å°„ã€‚é€šè¿‡ç»“åˆæµåŒ¹é…æŸå¤±å’Œè‡ªä¸€è‡´æ€§æŸå¤±ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨**å•æ­¥æ¨ç†**ä¸­å®ç°é«˜ä¿çœŸç”Ÿæˆï¼ŒåŒæ—¶ä¿ç•™å¤šæ­¥ç»†åŒ–çš„çµæ´»æ€§ï¼Œä»è€Œåœ¨è´¨é‡å’Œé€Ÿåº¦é—´å–å¾—çªç ´æ€§å¹³è¡¡ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\nåœ¨SemanticKITTIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFlowSSCå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼š\n- **å®šé‡ç»“æœ**ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°**56.97% IoU** å’Œ **19.52% mIoU**ï¼Œæ˜¾è‘—è¶…è¶Šæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚\n- **æ•ˆç‡ä¼˜åŠ¿**ï¼šå•æ­¥æ¨ç†ä»…éœ€**66æ¯«ç§’**ï¼Œæ€»æ¨ç†æ—¶é—´çº¦0.216ç§’ï¼ˆ~4.6 FPSï¼‰ï¼Œå…·å¤‡è¿‘å®æ—¶èƒ½åŠ›ã€‚\n- **æ¶ˆèç ”ç©¶**ï¼šéªŒè¯äº†å¿«æ·æ‰©æ•£ç»†åŒ–å™¨å¸¦æ¥æ˜¾è‘—æå‡ï¼ˆ+5.83% IoUï¼‰ï¼Œä¸”å•æ­¥æ¨ç†æ€§èƒ½æœ€ä¼˜ï¼ŒVecSet VAEåœ¨é‡å»ºè´¨é‡ä¸Šä¼˜äºä¼ ç»Ÿå·ç§¯VAEã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\nFlowSSCé¦–æ¬¡å°†é«˜è´¨é‡ç”Ÿæˆå¼å»ºæ¨¡ä¸å®æ—¶æ¨ç†èƒ½åŠ›ç»“åˆï¼Œä¸ºå•ç›®SSCä»»åŠ¡æä¾›äº†ä¸€ä¸ª**é€šç”¨å¢å¼ºæ¡†æ¶**ï¼Œå¯æ— ç¼é›†æˆäºç°æœ‰å‰é¦ˆæ–¹æ³•ä¹‹ä¸Šã€‚å…¶å•æ­¥ç”ŸæˆèŒƒå¼è§£å†³äº†æ‰©æ•£æ¨¡å‹åœ¨å®æ—¶ç³»ç»Ÿä¸­çš„éƒ¨ç½²ç“¶é¢ˆï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ç­‰å¯¹é€Ÿåº¦å’Œç²¾åº¦å‡æœ‰ä¸¥è‹›è¦æ±‚çš„åº”ç”¨æä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚è¯¥å·¥ä½œæ¨åŠ¨äº†ç”Ÿæˆå¼3Dåœºæ™¯ç†è§£å‘å®ç”¨åŒ–è¿ˆè¿›ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šFlowSSC\n\n### **ä¸€ã€ æ‹Ÿè§£å†³çš„æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**å•ç›®è¯­ä¹‰åœºæ™¯è¡¥å…¨**ä»»åŠ¡ä¸­å­˜åœ¨çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š\n1.  **â€œä¸€å¯¹å¤šâ€æ˜ å°„çš„å›ºæœ‰æ¨¡ç³Šæ€§**ï¼šä»å•å¼ 2Då›¾åƒæ¨æ–­è¢«é®æŒ¡çš„3Då‡ ä½•ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå­˜åœ¨ä¸¥é‡çš„æ­§ä¹‰æ€§ã€‚\n2.  **ç”Ÿæˆè´¨é‡ä¸æ¨ç†é€Ÿåº¦çš„æƒè¡¡**ï¼š\n    *   **å‰é¦ˆå¼æ–¹æ³•**ï¼šæ¨ç†å¿«ï¼Œä½†å€¾å‘äºä¸ºé®æŒ¡åŒºåŸŸç”Ÿæˆæ¨¡ç³Šæˆ–å¹³å‡åŒ–çš„é¢„æµ‹ï¼Œç¼ºä¹ç»†èŠ‚å’Œç©ºé—´åˆç†æ€§ã€‚\n    *   **æ ‡å‡†æ‰©æ•£æ¨¡å‹**ï¼šèƒ½ç”Ÿæˆé«˜ä¿çœŸç»†èŠ‚ï¼Œä½†éœ€è¦æ•°ç™¾æ­¥è¿­ä»£é‡‡æ ·ï¼Œæ¨ç†é€Ÿåº¦è¿‡æ…¢ï¼Œæ— æ³•æ»¡è¶³è‡ªåŠ¨é©¾é©¶ç­‰å®æ—¶åº”ç”¨éœ€æ±‚ã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nFlowSSC æå‡ºäº†ä¸€ä¸ª**é€šç”¨ã€ç”Ÿæˆå¼ã€å•æ­¥æ¨ç†**çš„è¯­ä¹‰åœºæ™¯è¡¥å…¨æ¡†æ¶ï¼Œå…¶åˆ›æ–°ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹ä¸‰ä¸ªå±‚é¢ï¼š\n\n#### **1. æ¡†æ¶åˆ›æ–°ï¼šé¦–ä¸ªç”¨äºå•ç›®SSCçš„é€šç”¨ç”Ÿæˆå¼æ¡†æ¶**\n*   **å®šä½**ï¼šå°†SSCä»»åŠ¡é‡æ–°å®šä¹‰ä¸º**æ¡ä»¶ç”Ÿæˆé—®é¢˜**ã€‚\n*   **é€šç”¨æ€§**ï¼šå¯ä»¥æ— ç¼é›†æˆåˆ°**ä»»ä½•ç°æœ‰çš„å‰é¦ˆå¼SSCæ–¹æ³•**ä¹‹åï¼Œä½œä¸ºä¸€ä¸ªç”Ÿæˆå¼â€œç²¾ç‚¼å™¨â€ï¼Œæ˜¾è‘—æå‡å…¶æ€§èƒ½ã€‚è¿™ä½¿å¾—å®ƒæˆä¸ºä¸€ä¸ªâ€œå³æ’å³ç”¨â€çš„æ€§èƒ½å¢å¼ºæ¨¡å—ã€‚\n\n#### **2. è¡¨ç¤ºåˆ›æ–°ï¼šåŸºäºäº¤å‰æ³¨æ„åŠ›çš„ç´§å‡‘ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´**\n*   **é«˜æ•ˆå‹ç¼©**ï¼šè®¾è®¡äº†ä¸€ä¸ª **VecSet VAE**ï¼Œåˆ©ç”¨**äº¤å‰æ³¨æ„åŠ›æœºåˆ¶**ï¼Œå°†é«˜ç»´ç¨€ç–ä½“ç´ ç½‘æ ¼ï¼ˆ256Ã—256Ã—32ï¼‰å‹ç¼©åˆ°ä¸€ä¸ªç´§å‡‘çš„**ä¸‰å¹³é¢æ½œåœ¨è¡¨ç¤º**ï¼ˆä¸‰ä¸ªæ­£äº¤çš„ç‰¹å¾å¹³é¢ï¼‰ã€‚\n*   **ä¼˜åŠ¿**ï¼š\n    *   **é«˜ä¿çœŸ**ï¼šåœ¨å‹ç¼©åä»èƒ½ä¿æŒ85.89%çš„IoUé‡å»ºç²¾åº¦ã€‚\n    *   **é™ç»´**ï¼šæå¤§é™ä½äº†åç»­æ‰©æ•£æ¨¡å‹éœ€è¦å¤„ç†çš„æ•°æ®ç»´åº¦å’Œè®¡ç®—å¤æ‚åº¦ï¼Œä½¿ç”Ÿæˆå¼å»ºæ¨¡å˜å¾—å¯è¡Œã€‚\n\n#### **3. ç”ŸæˆèŒƒå¼åˆ›æ–°ï¼šåŸºäºè‡ªä¸€è‡´æ€§çš„å•æ­¥æ½œåœ¨æ‰©æ•£ï¼ˆShortcut Flow-Matchingï¼‰**\n*   **æ ¸å¿ƒæœºåˆ¶**ï¼šå¼•å…¥ **Shortcut Models** è®­ç»ƒç­–ç•¥ï¼Œåœ¨æµåŒ¹é…æ¡†æ¶ä¸‹å­¦ä¹ ä¸€ä¸ªâ€œæ·å¾„å‡½æ•°â€ã€‚\n*   **å…³é”®çªç ´**ï¼š\n    *   **å•æ­¥é«˜ä¿çœŸç”Ÿæˆ**ï¼šæ¨¡å‹å­¦ä¹ ä»å™ªå£°åˆ°å¹²å‡€æ•°æ®çš„**ç›´æ¥æ˜ å°„**ï¼Œæ— éœ€æ•°ç™¾æ­¥è¿­ä»£ï¼Œ**ä»…éœ€ä¸€æ­¥**å³å¯ç”Ÿæˆé«˜è´¨é‡ç»“æœã€‚\n    *   **çµæ´»æ¨ç†**ï¼šåŒä¸€ä¸ªæ¨¡å‹æ”¯æŒä»1æ­¥åˆ°å¤šæ­¥çš„çµæ´»é‡‡æ ·ï¼Œå¹³è¡¡é€Ÿåº¦ä¸è´¨é‡ï¼ˆå°½ç®¡å®éªŒè¡¨æ˜1æ­¥æ€§èƒ½æœ€ä½³ï¼‰ã€‚\n    *   **è®­ç»ƒç›®æ ‡**ï¼šé€šè¿‡ç»“åˆ**æµåŒ¹é…æŸå¤±**ï¼ˆä¿è¯è½¨è¿¹æ­£ç¡®æ€§ï¼‰å’Œ**è‡ªä¸€è‡´æ€§æŸå¤±**ï¼ˆä¿è¯å¤§æ­¥é•¿è·³è·ƒçš„å‡†ç¡®æ€§ï¼‰è¿›è¡Œè”åˆä¼˜åŒ–ã€‚è‡ªä¸€è‡´æ€§è¦æ±‚ä¸€ä¸ª`2d`çš„å¤§æ­¥é•¿è·³è·ƒç­‰åŒäºä¸¤ä¸ªè¿ç»­çš„`d`æ­¥é•¿è·³è·ƒã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆï¼ˆæŠ€æœ¯æµç¨‹ï¼‰**\nFlowSSC é‡‡ç”¨ä¸‰é˜¶æ®µæµæ°´çº¿ï¼Œå°†åˆ¤åˆ«å¼å­¦ä¹ ä¸ç”Ÿæˆå¼ç²¾ç‚¼ç›¸ç»“åˆï¼š\n\n1.  **æ½œåœ¨ç©ºé—´å‹ç¼©ï¼ˆVecSet VAEï¼‰**ï¼š\n    *   **è¾“å…¥**ï¼šçœŸå®3Dè¯­ä¹‰ä½“ç´ æ ‡ç­¾ã€‚\n    *   **è¿‡ç¨‹**ï¼šé€šè¿‡äº¤å‰æ³¨æ„åŠ›ï¼Œå°†ä½“ç´ ç‰¹å¾èšåˆåˆ°ä¸€ç»„å¯å­¦ä¹ çš„ä¸‰å¹³é¢æŸ¥è¯¢ä¸­ï¼Œç¼–ç ä¸ºç´§å‡‘çš„ä¸‰å¹³é¢æ½œåœ¨ç ã€‚\n    *   **è¾“å‡º**ï¼šä¸€ä¸ªå†»ç»“çš„ã€é«˜æ•ˆçš„â€œç¥ç»åˆ†è¯å™¨â€ï¼Œä¸ºåç»­æ‰©æ•£æä¾›æ½œåœ¨ç©ºé—´ã€‚\n\n2.  **ç²—ç²’åº¦é¢„æµ‹ï¼ˆæ¡ä»¶ç”Ÿæˆï¼‰**ï¼š\n    *   **è¾“å…¥**ï¼šå•å¼ RGBå›¾åƒã€‚\n    *   **è¿‡ç¨‹**ï¼šä½¿ç”¨ä»»ä½•ç°æœ‰çš„SOTAå‰é¦ˆç½‘ç»œï¼ˆå¦‚MonoScene, VoxFormerèŒƒå¼ï¼‰é¢„æµ‹ä¸€ä¸ª**ç²—ç²’åº¦ä½“ç´ åœºæ™¯**ã€‚\n    *   **ä½œç”¨**ï¼šæä¾›å…¨å±€è¯­ä¹‰å¸ƒå±€å’Œå‡ ä½•å…ˆéªŒï¼Œä½œä¸ºç”Ÿæˆè¿‡ç¨‹çš„**æ¡ä»¶**ã€‚\n\n3.  **æ·å¾„æ½œåœ¨æ‰©æ•£ç²¾ç‚¼**ï¼š\n    *   **è¾“å…¥**ï¼šå°†ç²—ç²’åº¦é¢„æµ‹ç¼–ç ä¸ºä¸‰å¹³é¢æ½œåœ¨ç  `h_coarse`ï¼Œä»¥åŠä¸€ä¸ªéšæœºå™ªå£°ã€‚\n    *   **æ ¸å¿ƒ**ï¼šä½¿ç”¨ **Triplane Diffusion Transformer** ä½œä¸ºå»å™ªç½‘ç»œã€‚\n        *   ç½‘ç»œä»¥å¸¦å™ªå£°çš„æ½œåœ¨ç ã€æ—¶é—´æ­¥`t`å’Œæ­¥é•¿`d`ä¸ºæ¡ä»¶ã€‚\n        *   é€šè¿‡**è‡ªé€‚åº”å±‚å½’ä¸€åŒ–**æ³¨å…¥æ¡ä»¶ä¿¡æ¯ã€‚\n    *   **è¾“å‡º**ï¼šç»è¿‡ç²¾ç‚¼çš„ã€é«˜è´¨é‡çš„ä¸‰å¹³é¢æ½œåœ¨ç ï¼Œæœ€åé€šè¿‡VAEè§£ç å™¨æ¢å¤ä¸ºé«˜ä¿çœŸçš„3Dè¯­ä¹‰ä½“ç´ åœºæ™¯ã€‚\n\n### **å››ã€ å®é™…ä»·å€¼ä¸æ„ä¹‰**\n*   **æ€§èƒ½çªç ´**ï¼šåœ¨SemanticKITTIæ•°æ®é›†ä¸Šè¾¾åˆ°SOTAï¼ˆmIoU 19.52%ï¼Œ IoU 56.97%ï¼‰ï¼Œå°¤å…¶åœ¨é®æŒ¡ä¸¥é‡ç±»åˆ«ä¸Šæå‡æ˜¾è‘—ã€‚\n*   **å®ç”¨æ€§**ï¼šå®ç°äº†**ç”Ÿæˆå¼æ–¹æ³•åœ¨å®æ—¶åœºæ™¯ä¸‹çš„éƒ¨ç½²**ã€‚å•æ­¥æ¨ç†ä»…éœ€66æ¯«ç§’ï¼Œæ€»æ¨ç†æ—¶é—´çº¦216æ¯«ç§’ï¼ˆ~4.6 FPSï¼‰ï¼Œæ¥è¿‘å®æ—¶ã€‚\n*   **èŒƒå¼å¯å‘**ï¼šä¸º3Dæ„ŸçŸ¥ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°æ€è·¯â€”â€”**åˆ©ç”¨é«˜æ•ˆçš„ç”Ÿæˆå¼æ¨¡å‹æ¥å¼¥è¡¥åˆ¤åˆ«å¼æ¨¡å‹åœ¨åº”å¯¹ä¸ç¡®å®šæ€§é—®é¢˜æ—¶çš„ä¸è¶³**ï¼Œå¹³è¡¡äº†åœºæ™¯ç†è§£çš„â€œå‡†ç¡®æ€§â€ä¸â€œåˆç†æ€§â€ã€‚\n*   **åº”ç”¨å‰æ™¯**ï¼šä¸ºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰éœ€è¦å®æ—¶ã€å®Œæ•´ã€å¯é 3Dç¯å¢ƒç†è§£çš„ç³»ç»Ÿæä¾›äº†å¼ºå¤§çš„æŠ€æœ¯æ”¯æ’‘ï¼Œèƒ½å¤Ÿä»æœ‰é™è§‚æµ‹ä¸­æ¨ç†å‡ºåˆç†ä¸”è¯¦ç»†çš„å®Œæ•´åœºæ™¯ã€‚",
            "timestamp": "2026-01-27T16:01:45.905298",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## FlowSSC æŠ€æœ¯å‘å±•è„‰ç»œåˆ†æ\n\nåŸºäºè®ºæ–‡å†…å®¹åŠç›¸å…³å·¥ä½œï¼ˆRelated Workï¼‰ï¼Œå•ç›®è¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆMonocular Semantic Scene Completion, SSCï¼‰é¢†åŸŸçš„å‘å±•è„‰ç»œå¯ä»¥æ¢³ç†ä¸ºä»¥ä¸‹å‡ ä¸ªå…³é”®é˜¶æ®µå’Œæ–¹å‘ã€‚å…¶æ¼”è¿›æ ¸å¿ƒåœ¨äº**å¦‚ä½•ä»å•å¼ 2Då›¾åƒä¸­æ›´å‡†ç¡®ã€é«˜æ•ˆåœ°æ¨ç†å‡ºå®Œæ•´ä¸”è¯­ä¹‰åŒ–çš„3Dåœºæ™¯**ï¼Œä¸»è¦æŒ‘æˆ˜æ˜¯é®æŒ¡åŒºåŸŸçš„æ¨¡ç³Šæ€§å’Œâ€œä¸€å¯¹å¤šâ€æ˜ å°„çš„æ­§ä¹‰æ€§ã€‚\n\n### å‘å±•è„‰ç»œä¸æ ¸å¿ƒæ”¹è¿›\n\n#### æ–¹å‘ä¸€ï¼šä»ç›´æ¥æ˜ å°„åˆ°ç»“æ„åŒ–3Dè¡¨ç¤ºï¼ˆæå‡å‡ ä½•æ¨ç†èƒ½åŠ›ï¼‰\næ—©æœŸæ–¹æ³•å°è¯•ç›´æ¥å°†2Dç‰¹å¾æ˜ å°„åˆ°3Dä½“ç´ ï¼Œä½†å­˜åœ¨å‡ ä½•æ¨¡ç³Šé—®é¢˜ã€‚\n\n- **MonoScene (2022)**: **å¼€åˆ›æ€§å·¥ä½œ**ï¼Œé¦–æ¬¡ç›´æ¥ä»å•ç›®RGBå›¾åƒé¢„æµ‹è¯­ä¹‰å æ®æ …æ ¼ã€‚å®ƒä½¿ç”¨ç‰¹å¾æå‡å’ŒæŠ•å½±æ¨¡å—å°†2Dç‰¹å¾æ˜ å°„åˆ°3Dç©ºé—´ã€‚\n    - **æ”¹è¿›**ï¼š æ˜ç¡®äº†å•ç›®SSCçš„ä»»åŠ¡èŒƒå¼ã€‚\n    - **å±€é™**ï¼š å¯èƒ½å°†å¯è§åŒºåŸŸçš„ç‰¹å¾é”™è¯¯åˆ†é…åˆ°è¢«é®æŒ¡ä½“ç´ ï¼Œå¯¼è‡´3Dè¡¨ç¤ºæ¨¡ç³Šã€‚\n\n- **VoxFormer (2023)**: å¼•å…¥äº†**ç¨€ç–åˆ°ç¨ å¯†**çš„æ¡†æ¶ã€‚\n    - **æ”¹è¿›**ï¼š å…ˆé€šè¿‡æ·±åº¦ä¼°è®¡è¯†åˆ«å¯è§ä½“ç´ ï¼Œå†å°†ä¿¡æ¯ä¼ æ’­åˆ°ä¸å¯è§åŒºåŸŸã€‚è¿™ç¡®ä¿äº†æ›´å‡†ç¡®çš„åˆå§‹3Då‡ ä½•è¡¨ç¤ºï¼Œç¼“è§£äº†MonoSceneä¸­çš„ç‰¹å¾åˆ†é…æ­§ä¹‰ã€‚\n\n- **TPVFormer (2023) & OccFormer (2023)**: æ¢ç´¢äº†**æ›´é«˜æ•ˆçš„3Dè¡¨ç¤ºç»“æ„**ã€‚\n    - **TPVFormeræ”¹è¿›**ï¼š æå‡º**ä¸‰å¹³é¢è§†å›¾ï¼ˆTri-perspective Viewï¼‰** è¡¨ç¤ºï¼Œä½¿ç”¨ä¸‰ä¸ªæ­£äº¤çš„ç‰¹å¾å¹³é¢æ¥é«˜æ•ˆå»ºæ¨¡3Dç»“æ„ï¼Œé™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚\n    - **OccFormeræ”¹è¿›**ï¼š é‡‡ç”¨**åŒè·¯å¾„Transformerç¼–ç å™¨**ï¼ŒåŒæ—¶æ•æ‰ç»†ç²’åº¦ç»†èŠ‚å’Œå…¨å±€è¯­ä¹‰å¸ƒå±€ï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚\n\n- **ET-Former (2025)**: åœ¨é«˜æ•ˆè¡¨ç¤ºä¸Šæ›´è¿›ä¸€æ­¥ã€‚\n    - **æ”¹è¿›**ï¼š åœ¨**ä¸‰å¹³é¢åŸºç¡€ä¸Šå¼•å…¥å¯å˜å½¢æ³¨æ„åŠ›æœºåˆ¶**ï¼Œåœ¨æå‡è¯­ä¹‰é¢„æµ‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒå†…å­˜æ¶ˆè€—ã€‚\n\n#### æ–¹å‘äºŒï¼šèå…¥å®ä¾‹ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæå‡è¯­ä¹‰ç²¾åº¦ä¸åœºæ™¯ä¸€è‡´æ€§ï¼‰\nä¸ºäº†ç”Ÿæˆæ›´ç¬¦åˆçœŸå®ä¸–ç•Œç‰©ä½“å…³ç³»çš„åœºæ™¯ï¼Œåç»­å·¥ä½œå¼€å§‹æ•´åˆå®ä¾‹çº§å’Œæ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚\n\n- **Symphonize (2024)**: å¼•å…¥äº†**å®ä¾‹æŸ¥è¯¢ï¼ˆInstance Queriesï¼‰**ã€‚\n    - **æ”¹è¿›**ï¼š é€šè¿‡åºåˆ—åŒ–çš„å®ä¾‹ä¼ æ’­æ³¨æ„åŠ›ï¼Œå¢å¼ºäº†åœºæ™¯ä¸­ç‹¬ç«‹ç‰©ä½“çš„å»ºæ¨¡èƒ½åŠ›ï¼Œä½¿è¡¥å…¨ç»“æœåœ¨ç‰©ä½“å±‚é¢æ›´æ¸…æ™°ã€æ›´ç¬¦åˆå®ä¾‹å…³ç³»ã€‚\n\n- **CGFormer (2024)**: å¼ºè°ƒäº†**ä¸Šä¸‹æ–‡ä¸å‡ ä½•çš„ååŒ**ã€‚\n    - **æ”¹è¿›**ï¼š æå‡º**ä¸Šä¸‹æ–‡ä¸å‡ ä½•æ„ŸçŸ¥çš„ä½“ç´ Transformer**ï¼Œç”Ÿæˆä¾èµ–äºè¾“å…¥å›¾åƒä¸Šä¸‹æ–‡çš„æŸ¥è¯¢ï¼Œä½¿å¾—è¡¥å…¨è¿‡ç¨‹èƒ½æ›´å¥½åœ°ç†è§£åœºæ™¯çš„å…¨å±€å¸ƒå±€å’Œç‰©ä½“é—´çš„ç©ºé—´å…³ç³»ã€‚\n\n#### æ–¹å‘ä¸‰ï¼šä»åˆ¤åˆ«å¼åˆ°ç”Ÿæˆå¼æ¨¡å‹ï¼ˆè§£å†³æ¨¡ç³Šæ€§ï¼Œç”Ÿæˆç»†èŠ‚ï¼‰\nå‰è¿°å‡ä¸ºåˆ¤åˆ«å¼ï¼ˆå‰é¦ˆï¼‰æ¨¡å‹ï¼Œå…¶æœ€å°åŒ–é‡å»ºè¯¯å·®çš„ç›®æ ‡å€¾å‘äºè¾“å‡ºå¹³å‡åŒ–çš„æ¨¡ç³Šç»“æœï¼Œå°¤å…¶åœ¨é®æŒ¡åŒºåŸŸã€‚**ç”Ÿæˆå¼æ¨¡å‹**çš„å¼•å…¥æ—¨åœ¨ä»æ ¹æœ¬ä¸Šè§£å†³â€œä¸€å¯¹å¤šâ€æ˜ å°„é—®é¢˜ã€‚\n\n- **å‰æœŸç”Ÿæˆæ¨¡å‹åœ¨3Dçš„åº”ç”¨**ï¼š æ‰©æ•£æ¨¡å‹åœ¨3Dç‚¹äº‘ç”Ÿæˆã€å½¢çŠ¶å»ºæ¨¡ä¸­å–å¾—æˆåŠŸï¼Œä½†**æ ‡å‡†è¿­ä»£é‡‡æ ·è¿‡ç¨‹éœ€è¦æ•°ç™¾æ­¥ï¼Œæ— æ³•æ»¡è¶³å®æ—¶åº”ç”¨**ã€‚\n- **FlowSSC (æœ¬å·¥ä½œ)**: **é¦–æ¬¡å°†ç”Ÿæˆå¼æ¡†æ¶ç›´æ¥åº”ç”¨äºå•ç›®SSC**ï¼Œå¹¶è§£å†³äº†é€Ÿåº¦ç“¶é¢ˆã€‚\n    - **æ ¸å¿ƒæ”¹è¿›1 (è¡¨ç¤ºå±‚é¢)**ï¼š å¼•å…¥ **VecSet VAE**ï¼Œåˆ©ç”¨**äº¤å‰æ³¨æ„åŠ›ï¼ˆCross-Attentionï¼‰** å°†3Dä½“ç´ å‹ç¼©åˆ°ç´§å‡‘çš„**ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´**ã€‚ç›¸æ¯”ä¼ ç»Ÿå·ç§¯VAEï¼Œå®ƒèƒ½æ›´æœ‰æ•ˆåœ°èšåˆå…¨å±€ç©ºé—´ä¿¡æ¯ï¼Œå®ç°é«˜ä¿çœŸé‡å»ºï¼ˆ85.89% mIoUï¼‰ï¼Œä¸ºåç»­æ‰©æ•£æ¨¡å‹å¤§å¹…å‡è´Ÿã€‚\n    - **æ ¸å¿ƒæ”¹è¿›2 (ç”Ÿæˆæ•ˆç‡)**ï¼š æå‡º **Shortcut Latent Diffusion**ã€‚\n        - **é—®é¢˜**ï¼š æ ‡å‡†æ‰©æ•£æ¨¡å‹æ…¢ã€‚\n        - **è§£å†³æ–¹æ¡ˆ**ï¼š é‡‡ç”¨**æµåŒ¹é…ï¼ˆFlow Matchingï¼‰** æ¡†æ¶ä¸‹çš„**Shortcut Models**è®­ç»ƒç­–ç•¥ã€‚æ¨¡å‹å­¦ä¹ ä¸€ä¸ªâ€œæ·å¾„å‡½æ•°â€ï¼Œèƒ½å¤Ÿæ ¹æ®æ­¥é•¿ `d` ç›´æ¥é¢„æµ‹ä»å™ªå£°åˆ°å¹²å‡€æ•°æ®çš„å½’ä¸€åŒ–æ–¹å‘ã€‚\n        - **å…³é”®åˆ›æ–°**ï¼š **è‡ªä¸€è‡´æ€§ï¼ˆSelf-Consistencyï¼‰æŸå¤±**ã€‚å¼ºåˆ¶è¦æ±‚ä¸€ä¸ªå¤§æ­¥é•¿ï¼ˆ`2d`ï¼‰çš„è·³è·ƒç­‰åŒäºä¸¤ä¸ªè¿ç»­å°æ­¥é•¿ï¼ˆ`d`ï¼‰è·³è·ƒçš„å¹³å‡ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¢«è®­ç»ƒæˆç›´æ¥æ‰§è¡Œ**ä¸€æ­¥ç”Ÿæˆ**ï¼ŒåŒæ—¶ä¿ç•™å¤šæ­¥ç»†åŒ–çš„èƒ½åŠ›ã€‚\n    - **æœ€ç»ˆæ•ˆæœ**ï¼š å®ç°äº†**å•æ­¥é«˜ä¿çœŸç”Ÿæˆ**ï¼Œåœ¨ä¿æŒç”Ÿæˆæ¨¡å‹å¼ºå¤§ç»†èŠ‚ hallucination èƒ½åŠ›çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†å®æ—¶æ¨ç†é€Ÿåº¦ï¼ˆ~4.6 FPSï¼‰ï¼Œé¦–æ¬¡ä½¿ç”Ÿæˆå¼SSCå…·å¤‡å®é™…éƒ¨ç½²ä»·å€¼ã€‚\n\n### æ€»ç»“ï¼šä¸‰ä¸ªå‘å±•æ–¹å‘çš„æ ¸å¿ƒæ¼”è¿›é€»è¾‘\n\n1.  **3Dè¡¨ç¤ºä¸å‡ ä½•æ¨ç†**ï¼š `ç›´æ¥ä½“ç´ æ˜ å°„` -> `ç¨€ç–-ç¨ å¯†æ¡†æ¶` -> `é«˜æ•ˆä¸‰å¹³é¢/æ³¨æ„åŠ›ç»“æ„`ã€‚æ¼”è¿›ç›®æ ‡æ˜¯æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆåœ°ä»2Då»ºç«‹3Då‡ ä½•å…ˆéªŒã€‚\n2.  **è¯­ä¹‰ä¸ä¸Šä¸‹æ–‡ç†è§£**ï¼š `ä½“ç´ çº§è¯­ä¹‰åˆ†ç±»` -> `å¼•å…¥å®ä¾‹çº§è¡¨ç¤º` -> `æ˜¾å¼å»ºæ¨¡åœºæ™¯ä¸Šä¸‹æ–‡ä¸å‡ ä½•å…³ç³»`ã€‚æ¼”è¿›ç›®æ ‡æ˜¯ä½¿è¡¥å…¨ç»“æœç¬¦åˆçœŸå®ä¸–ç•Œçš„ç‰©ä½“å¸ƒå±€å’Œé€»è¾‘ã€‚\n3.  **æ¨¡å‹èŒƒå¼ä¸ä¸ç¡®å®šæ€§å¤„ç†**ï¼š `åˆ¤åˆ«å¼å‰é¦ˆæ¨¡å‹` -> `ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹`ã€‚è¿™æ˜¯**èŒƒå¼æ€§è½¬å˜**ã€‚åˆ¤åˆ«å¼æ¨¡å‹éš¾ä»¥å¤„ç†é®æŒ¡åŒºåŸŸçš„å›ºæœ‰æ­§ä¹‰ï¼Œè¾“å‡ºæ¨¡ç³Šã€‚ç”Ÿæˆå¼æ¨¡å‹é€šè¿‡å­¦ä¹ å’Œé‡‡æ ·æ•°æ®åˆ†å¸ƒï¼Œèƒ½å¤Ÿ**åˆæˆå¤šç§åˆç†ç»†èŠ‚**ã€‚FlowSSCçš„å…³é”®çªç ´åœ¨äºé€šè¿‡**ShortcutæµåŒ¹é…**å’Œ**ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´**ï¼Œè§£å†³äº†ç”Ÿæˆæ¨¡å‹çš„é€Ÿåº¦éš¾é¢˜ï¼Œå®ç°äº†**è´¨é‡ä¸æ•ˆç‡çš„å¹³è¡¡**ã€‚\n\n**FlowSSCçš„å®šä½**ï¼š å®ƒå¹¶éæŠ›å¼ƒå‰åºå·¥ä½œï¼Œè€Œæ˜¯ä½œä¸ºä¸€ä¸ª**é€šç”¨çš„ç”Ÿæˆå¼å¢å¼ºæ¡†æ¶**ã€‚å®ƒæ¥æ”¶ä»»ä½•å‰é¦ˆSSCæ–¹æ³•ï¼ˆå¦‚VoxFormer, TPVFormerï¼‰ç”Ÿæˆçš„**ç²—ç³™é¢„æµ‹ä½œä¸ºæ¡ä»¶**ï¼Œç„¶ååœ¨å…¶å‹ç¼©åçš„ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œ**ä¸€æ­¥ç”Ÿæˆå¼ refinement**ï¼Œä»è€Œæ˜¾è‘—æå‡æœ€ç»ˆè¾“å‡ºçš„ç»†èŠ‚ä¿çœŸåº¦å’Œåœºæ™¯åˆç†æ€§ã€‚è¿™ä¸ºæ•´ä¸ªå•ç›®SSCé¢†åŸŸæä¾›äº†ä¸€æ¡æ–°çš„æ€§èƒ½æå‡è·¯å¾„ã€‚",
            "timestamp": "2026-01-27T16:02:37.124260",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## FlowSSC çš„å‰ä½œåˆ†æä¸å¯¹æ¯”\n\nFlowSSC æ˜¯é¦–ä¸ªå°†ç”Ÿæˆå¼æ¨¡å‹ç›´æ¥åº”ç”¨äºå•ç›®è¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰çš„æ¡†æ¶ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†**æ·å¾„æµåŒ¹é…ï¼ˆShortcut Flow-matchingï¼‰** åœ¨ç´§å‡‘çš„ä¸‰å¹³é¢ï¼ˆtriplaneï¼‰æ½œåœ¨ç©ºé—´ä¸­è¿è¡Œï¼Œå®ç°äº†**å•æ­¥é«˜ä¿çœŸç”Ÿæˆ**ï¼Œä»è€Œåœ¨ä¿æŒå®æ—¶æ¨ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œè§£å†³äº†å‰é¦ˆæ–¹æ³•åœ¨é®æŒ¡åŒºåŸŸç”Ÿæˆæ¨¡ç³Šç»†èŠ‚å’Œä¿æŒç‰©ä½“ç©ºé—´å…³ç³»æ–¹é¢çš„æ ¹æœ¬æ€§å›°éš¾ã€‚\n\nä¸ºäº†æ¸…æ™°å±•ç¤ºå…¶æŠ€æœ¯æ¼”è¿›ï¼Œä»¥ä¸‹è¡¨æ ¼è¯¦ç»†åˆ—å‡ºäº†ä¸ FlowSSC å¯†åˆ‡ç›¸å…³çš„å‡ ç¯‡å…³é”®å‰ä½œï¼Œå¹¶é‡ç‚¹åˆ†æäº† FlowSSC ç›¸å¯¹äºæ¯ç¯‡å‰ä½œçš„å…·ä½“æ”¹è¿›å’ŒåŒºåˆ«ã€‚\n\n### å‰ä½œå¯¹æ¯”åˆ†æè¡¨\n\n| å‰ä½œåç§° (å¹´ä»½) | æ ¸å¿ƒæ–¹æ³• / è´¡çŒ® | ä¸ FlowSSC çš„ä¸»è¦åŒºåˆ« | FlowSSC çš„ä¸»è¦æ”¹è¿› / åˆ›æ–°ç‚¹ |\n| :--- | :--- | :--- | :--- |\n| **MonoScene (2022)** [2] | **å¼€åˆ›æ€§å•ç›®SSCæ–¹æ³•**ï¼šé€šè¿‡ç‰¹å¾æå‡å’ŒæŠ•å½±æ¨¡å—ï¼Œç›´æ¥å°†2Då›¾åƒç‰¹å¾æ˜ å°„åˆ°3Dç©ºé—´ï¼Œé¢„æµ‹è¯­ä¹‰å æ®æ …æ ¼ã€‚ | 1. **æ¨¡å‹ç±»å‹**ï¼šçº¯ç²¹çš„å‰é¦ˆåˆ¤åˆ«å¼æ¨¡å‹ã€‚<br>2. **æ ¸å¿ƒé—®é¢˜**ï¼šå­˜åœ¨â€œä¸€å¯¹å¤šâ€æ˜ å°„çš„å›ºæœ‰é—®é¢˜ï¼Œå¯¹é®æŒ¡åŒºåŸŸå€¾å‘äºäº§ç”Ÿæ¨¡ç³Šæˆ–å¹³å‡åŒ–çš„é¢„æµ‹ï¼Œç¼ºä¹ç”Ÿæˆç»†èŠ‚çš„èƒ½åŠ›ã€‚<br>3. **æ¨ç†æ–¹å¼**ï¼šç¡®å®šæ€§å•æ¬¡å‰å‘ä¼ æ’­ã€‚ | 1. **å¼•å…¥ç”Ÿæˆå¼æ¡†æ¶**ï¼šå°†SSCè§†ä¸ºæ¡ä»¶ç”Ÿæˆé—®é¢˜ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›æ¥åˆæˆé®æŒ¡åŒºåŸŸçš„é«˜é¢‘ç»†èŠ‚å’Œåˆç†ç»“æ„ã€‚<br>2. **æ€§èƒ½æå‡**ï¼šé€šè¿‡ç”Ÿæˆå¼ç»†åŒ–ï¼Œæ˜¾è‘—æå‡äº†è¡¥å…¨è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸¥é‡é®æŒ¡çš„åŒºåŸŸï¼ˆå¦‚å»ºç­‘ç‰©ï¼‰ã€‚<br>3. **é€šç”¨æ€§**ï¼šFlowSSCè¢«è®¾è®¡ä¸ºä¸€ä¸ª**é€šç”¨å¢å¼ºæ¡†æ¶**ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°MonoSceneè¿™ç±»å‰é¦ˆæ–¹æ³•ä¹‹åï¼Œæå‡å…¶æ€§èƒ½ã€‚ |\n| **VoxFormer (2023)** [10] | **ç¨€ç–åˆ°ç¨ å¯†æ¡†æ¶**ï¼šé¦–å…ˆé€šè¿‡æ·±åº¦ä¼°è®¡è¯†åˆ«å¯è§ä½“ç´ ï¼Œç„¶åå°†ä¿¡æ¯ä¼ æ’­åˆ°ä¸å¯è§åŒºåŸŸï¼Œç¡®ä¿å‡†ç¡®çš„åˆå§‹3Dè¡¨ç¤ºã€‚ | 1. **æ–¹æ³•èŒƒå¼**ï¼šå±äºæ”¹è¿›å‹çš„å‰é¦ˆæ–¹æ³•ï¼Œé€šè¿‡æ›´èªæ˜çš„ç‰¹å¾é€‰æ‹©ä¸ä¼ æ’­æ¥ä¼˜åŒ–ç»“æœï¼Œä½†æœ¬è´¨ä¸Šä»æ˜¯åˆ¤åˆ«å¼æ¨¡å‹ã€‚<br>2. **ä¸ç¡®å®šæ€§å¤„ç†**ï¼šä¾èµ–äºç¡®å®šçš„æ·±åº¦ä¼°è®¡å’Œä¼ æ’­æœºåˆ¶ï¼Œå¯¹æ·±åº¦ä¼°è®¡é”™è¯¯å’Œå¤æ‚é®æŒ¡çš„é²æ£’æ€§æœ‰é™ã€‚<br>3. **è¾“å‡ºç‰¹æ€§**ï¼šè¾“å‡ºå¯èƒ½ç¼ºä¹ç”Ÿæˆæ¨¡å‹æ‰€èƒ½æä¾›çš„å¤šæ ·æ€§å’Œç»†èŠ‚ä¸°å¯Œæ€§ã€‚ | 1. **ç”Ÿæˆå¼ç»†åŒ– vs. ç‰¹å¾ä¼ æ’­**ï¼šFlowSSCä¸ä¾èµ–äºç²¾ç¡®çš„åˆå§‹æ·±åº¦ä¼°è®¡æˆ–æ‰‹å·¥è®¾è®¡çš„ä¼ æ’­è§„åˆ™ã€‚ç›¸åï¼Œå®ƒåœ¨ä¸€ä¸ªå­¦ä¹ åˆ°çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œé€šè¿‡ç”Ÿæˆæ¨¡å‹**éšå¼åœ°å­¦ä¹ åœºæ™¯å…ˆéªŒ**ï¼ˆå¦‚â€œå»ºç­‘ç‰©é€šå¸¸åœ¨æ¤è¢«åé¢â€ï¼‰ï¼Œä»è€Œæ›´é²æ£’åœ°è¡¥å…¨ç¼ºå¤±éƒ¨åˆ†ã€‚<br>2. **å¤„ç†æ¨¡ç³Šæ€§**ï¼šç›´æ¥å»ºæ¨¡äº†ä»å•è§†å›¾åˆ°3Dåœºæ™¯çš„â€œä¸€å¯¹å¤šâ€æ˜ å°„çš„ä¸ç¡®å®šæ€§ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šä¸ªåˆç†çš„è¡¥å…¨ç»“æœï¼ˆå°½ç®¡è®ºæ–‡ä¸»è¦å±•ç¤ºç¡®å®šæ€§ç”Ÿæˆï¼‰ã€‚<br>3. **æ•ˆæœ**ï¼šåœ¨å®šé‡æŒ‡æ ‡ä¸Šï¼ˆå¦‚mIoUå’ŒIoUï¼‰å¤§å¹…è¶…è¶ŠVoxFormerã€‚ |\n| **TPVFormer / OccFormer (2023)** [8, 24] | **é«˜æ•ˆ3Dè¡¨ç¤º**ï¼šå¼•å…¥äº†ä¸‰è§†è§’è§†å›¾ï¼ˆTri-Perspective View, TPVï¼‰æˆ–åŒè·¯å¾„Transformerï¼Œä»¥æ›´é«˜æ•ˆçš„æ–¹å¼å»ºæ¨¡ç»†ç²’åº¦3Dç»“æ„ï¼Œé™ä½è®¡ç®—å¼€é”€ã€‚ | 1. **è¡¨ç¤ºç”¨é€”**ï¼šTPVç­‰è¡¨ç¤ºè¢«ç”¨ä½œ**ä¸»å¹²ç½‘ç»œçš„ç‰¹å¾è¡¨ç¤º**ï¼Œç”¨äºç›´æ¥çš„åœºæ™¯é¢„æµ‹ã€‚<br>2. **æ¨¡å‹æœ¬è´¨**ï¼šä»ç„¶æ˜¯ç«¯åˆ°ç«¯çš„å‰é¦ˆé¢„æµ‹æ¨¡å‹ï¼Œæ²¡æœ‰å¼•å…¥ç”Ÿæˆå¼å»ºæ¨¡ã€‚<br>3. **ç›®æ ‡**ï¼šä¾§é‡äºè®¾è®¡æ›´é«˜æ•ˆçš„æ¶æ„æ¥å¤„ç†3Dæ•°æ®ï¼Œè€Œéè§£å†³ç”Ÿæˆæ¨¡ç³Šé—®é¢˜ã€‚ | 1. **ä¸‰å¹³é¢ä½œä¸ºæ½œåœ¨ç©ºé—´**ï¼šFlowSSCåˆ›æ–°æ€§åœ°å°†**ä¸‰å¹³é¢ï¼ˆTriplaneï¼‰ç”¨ä½œè‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å‹ç¼©åçš„æ½œåœ¨ç©ºé—´**ï¼Œè€Œéä¸­é—´ç‰¹å¾ã€‚è¿™å®ç°äº†ä¸¤ä¸ªç›®æ ‡ï¼š**å¤§å¹…é™ç»´**ï¼ˆä»256x256x32ä½“ç´ åˆ°ç´§å‡‘çš„ä¸‰å¹³é¢ï¼‰ä»¥ä¾›ç”Ÿæˆæ¨¡å‹é«˜æ•ˆå¤„ç†ï¼Œä»¥åŠ**ä¿æŒé«˜é‡å»ºä¿çœŸåº¦**ï¼ˆ85.89% IoUï¼‰ã€‚<br>2. **VecSet VAE**ï¼šé‡‡ç”¨äº†åŸºäº**äº¤å‰æ³¨æ„åŠ›ï¼ˆCross-Attentionï¼‰** çš„VecSet VAEæ¥æ„å»ºä¸‰å¹³é¢æ½œåœ¨è¡¨ç¤ºï¼Œæ¯”ä¼ ç»Ÿçš„åŸºäºå·ç§¯çš„VAEé‡å»ºè´¨é‡æ›´é«˜ï¼Œå› ä¸ºå®ƒèƒ½å…¨å±€èšåˆç©ºé—´ä¿¡æ¯ã€‚ |\n| **Shortcut Models (ICLR 2024)** [6] | **æ·å¾„æ¨¡å‹ç†è®º**ï¼šæå‡ºåœ¨æµåŒ¹é…ï¼ˆFlow Matchingï¼‰æ¡†æ¶ä¸­ï¼Œé€šè¿‡å¼•å…¥æ­¥é•¿`d`ä½œä¸ºæ¡ä»¶ï¼Œè®©æ¨¡å‹å­¦ä¹ ä¸€ä¸ªâ€œæ·å¾„å‡½æ•°â€ï¼Œå®ç°ä»å™ªå£°åˆ°æ•°æ®çš„ç›´æ¥æ˜ å°„ï¼Œæ”¯æŒä»1æ­¥åˆ°å¤šæ­¥çš„çµæ´»æ¨ç†ã€‚ | 1. **åº”ç”¨é¢†åŸŸ**ï¼šåŸè®ºæ–‡æ˜¯é€šç”¨çš„ç”Ÿæˆæ¨¡å‹ç†è®ºæ¡†æ¶ï¼Œä¸»è¦æ¼”ç¤ºåœ¨å›¾åƒç”Ÿæˆç­‰ä»»åŠ¡ä¸Šã€‚<br>2. **ä»»åŠ¡ç‰¹å®šæŒ‘æˆ˜**ï¼šæœªæ¶‰åŠé«˜ç»´ã€ç»“æ„å¤æ‚çš„3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨ä»»åŠ¡ã€‚ | 1. **é¦–æ¬¡åº”ç”¨äº3D SSC**ï¼šFlowSSCæ˜¯**é¦–ä¸ªå°†Shortcut Modelsåº”ç”¨äºå•ç›®SSCä»»åŠ¡çš„å·¥ä½œ**ã€‚<br>2. **ä¸ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´ç»“åˆ**ï¼šå°†Shortcut Modelsä¸å‰è¿°çš„**ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´**ç›¸ç»“åˆï¼Œè§£å†³äº†åœ¨åŸå§‹é«˜ç»´ä½“ç´ ç©ºé—´è¿›è¡Œæ‰©æ•£æ¨¡å‹è®­ç»ƒå’Œæ¨ç†è®¡ç®—é‡å·¨å¤§çš„é—®é¢˜ã€‚<br>3. **å®ç°å•æ­¥SOTA**ï¼šé€šè¿‡Shortcut Flow Matchingè®­ç»ƒï¼Œæ¨¡å‹å­¦ä¼šäº†åœ¨æ½œåœ¨ç©ºé—´ä¸­ç›´æ¥è¿›è¡Œâ€œæ·å¾„â€è·³è·ƒã€‚**å…³é”®å‘ç°**ï¼šåœ¨SSCä»»åŠ¡ä¸Šï¼Œ**å•æ­¥æ¨ç†ï¼ˆ1-Stepï¼‰å–å¾—äº†æœ€ä½³æ€§èƒ½**ï¼ˆ56.98% IoUï¼‰ï¼Œä¸”é€Ÿåº¦å¿«ï¼ˆ66msï¼‰ï¼Œè¿™ä½¿å¾—ç”Ÿæˆå¼SSCé¦–æ¬¡å…·å¤‡äº†å®æ—¶åº”ç”¨æ½œåŠ›ã€‚è¿™ä¸éœ€è¦æ•°ç™¾æ­¥çš„æ ‡å‡†æ‰©æ•£æ¨¡å‹å½¢æˆé²œæ˜å¯¹æ¯”ã€‚ |\n| **Consistency Models (2023)** [15] | **ä¸€è‡´æ€§æ¨¡å‹**ï¼šå¦ä¸€ç±»ä¸€æ­¥ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡è’¸é¦æˆ–ä¸€è‡´æ€§è®­ç»ƒï¼Œå¼ºåˆ¶æ¨¡å‹åœ¨ä¸åŒå™ªå£°æ°´å¹³ä¸‹è¾“å‡ºä¸€è‡´ã€‚ | 1. **è®­ç»ƒæœºåˆ¶**ï¼šä¸€è‡´æ€§è®­ç»ƒä¾èµ–äºå¯¹ç»éªŒæ ·æœ¬å¼ºåˆ¶æ‰§è¡Œä¸€è‡´æ€§ï¼Œå¯èƒ½å› ç¦»æ•£åŒ–è¯¯å·®ç§¯ç´¯ä¸å¯çº¦çš„åå·®ã€‚<br>2. **éœ€æ±‚**ï¼šå¯èƒ½éœ€è¦è¿›è¡Œå¤æ‚çš„è’¸é¦æˆ–ä½¿ç”¨ä¸¥æ ¼çš„å™ªå£°è°ƒåº¦ã€‚ | 1. **é‡‡ç”¨Shortcut Modelsæ¡†æ¶**ï¼šFlowSSCæ˜ç¡®é€‰æ‹©äº†Shortcut Modelsè€ŒéConsistency Modelsã€‚è®ºæ–‡æŒ‡å‡ºï¼ŒShortcut Modelsé€šè¿‡æ¨¡æ‹Ÿæ¦‚ç‡æµODEçš„ç›´æ¥â€œæ·å¾„â€ï¼Œ**é¿å…äº†ä¸€è‡´æ€§è®­ç»ƒä¸­çš„åå·®ç§¯ç´¯é—®é¢˜**ï¼Œä¸”éœ€è¦æ›´å°‘çš„å¼•å¯¼æ­¥éª¤ï¼Œæ— éœ€å¤æ‚çš„æ„ŸçŸ¥æŸå¤±æˆ–ä¸¥æ ¼è°ƒåº¦ã€‚<br>2. **æ›´ç®€æ´çš„è®­ç»ƒ**ï¼šé€šè¿‡æ··åˆé‡‡æ ·ç­–ç•¥ï¼ˆ75%æµåŒ¹é…æŸå¤± + 25%è‡ªä¸€è‡´æ€§æŸå¤±ï¼‰è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œæ— éœ€é¢„è®­ç»ƒçš„æ•™å¸ˆæ¨¡å‹ã€‚ |\n\n### æ€»ç»“ï¼šFlowSSCçš„æ ¸å¿ƒåˆ›æ–°ä¸æ•´åˆ\n\nFlowSSCå¹¶éå¯¹å•ä¸€å‰ä½œçš„ç®€å•æ”¹è¿›ï¼Œè€Œæ˜¯è¿›è¡Œäº†ä¸€æ¬¡**å¼€åˆ›æ€§çš„èŒƒå¼æ•´åˆä¸åˆ›æ–°**ï¼š\n\n1.  **ä»»åŠ¡èŒƒå¼åˆ›æ–°**ï¼šé¦–æ¬¡å°†**ç”Ÿæˆå¼å»ºæ¨¡**å¼•å…¥å•ç›®SSCï¼Œå°†å…¶å®šä¹‰ä¸ºæ¡ä»¶ç”Ÿæˆé—®é¢˜ï¼Œä»æ ¹æœ¬ä¸Šåº”å¯¹äº†é®æŒ¡åŒºåŸŸæ¨ç†çš„æ¨¡ç³Šæ€§ã€‚\n2.  **è¡¨ç¤ºåˆ›æ–°**ï¼šé¦–åˆ›ä½¿ç”¨**ä¸‰å¹³é¢ä½œä¸ºVAEå‹ç¼©çš„æ½œåœ¨ç©ºé—´**ï¼Œé€šè¿‡**VecSet VAEï¼ˆäº¤å‰æ³¨æ„åŠ›ï¼‰** å®ç°é«˜æ•ˆé«˜ä¿çœŸçš„3Dåœºæ™¯å‹ç¼©ï¼Œä¸ºåç»­ç”Ÿæˆæ¨¡å‹æ‰«æ¸…äº†è®¡ç®—éšœç¢ã€‚\n3.  **ç”Ÿæˆæ•ˆç‡åˆ›æ–°**ï¼šé¦–æ¬¡å°†**Shortcut Models**åº”ç”¨äº3Dåœºæ™¯ç”Ÿæˆï¼Œåœ¨ç´§å‡‘çš„ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´ä¸­å®ç°äº†**é«˜è´¨é‡çš„å•æ­¥ç”Ÿæˆ**ï¼Œæ‰“ç ´äº†æ‰©æ•£æ¨¡å‹åœ¨å®æ—¶åº”ç”¨ä¸­é€Ÿåº¦æ…¢çš„ç“¶é¢ˆã€‚\n4.  **æ¡†æ¶è®¾è®¡åˆ›æ–°**ï¼šæå‡º**â€œé€šç”¨ç”Ÿæˆå¢å¼ºæ¡†æ¶â€** æ¦‚å¿µã€‚å…¶æµç¨‹ï¼ˆç²—é¢„æµ‹ â†’ æ½œåœ¨ç¼–ç  â†’ ç”Ÿæˆå¼ç»†åŒ–ï¼‰å…è®¸ä»»ä½•ç°æœ‰çš„å‰é¦ˆSSCæ–¹æ³•ï¼ˆå¦‚MonoScene, VoxFormerï¼‰ä½œä¸ºå…¶â€œç²—é¢„æµ‹â€æ¨¡å—ï¼Œç„¶åé€šè¿‡FlowSSCçš„ç”Ÿæˆå¼ç»†åŒ–å™¨å¤§å¹…æå‡æœ€ç»ˆæ•ˆæœã€‚è¿™ä½“ç°äº†å…¶å¼ºå¤§çš„å…¼å®¹æ€§å’Œå®ç”¨ä»·å€¼ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼ŒFlowSSCçš„æ ¸å¿ƒæ”¹å–„åœ¨äºï¼šé€šè¿‡â€œä¸‰å¹³é¢æ½œåœ¨ç©ºé—´â€å’Œâ€œæ·å¾„æµåŒ¹é…â€è¿™ä¸¤é¡¹å…³é”®æŠ€æœ¯ï¼Œé¦–æ¬¡åœ¨å•ç›®SSCä»»åŠ¡ä¸Šå®ç°äº†ç”Ÿæˆå¼æ¨¡å‹çš„é«˜ä¿çœŸè¾“å‡ºä¸å®æ—¶æ¨ç†é€Ÿåº¦çš„å…¼å¾—ï¼Œä»è€Œæ˜¾è‘—è¶…è¶Šäº†æ‰€æœ‰å‰é¦ˆå¼çš„å‰ä½œæ–¹æ³•ã€‚**",
            "timestamp": "2026-01-27T16:03:43.172955",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†å¯¹è¿™ç¯‡é¢˜ä¸ºã€ŠFlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusionã€‹çš„è®ºæ–‡è¿›è¡Œæ·±å…¥åˆ†æï¼Œå¹¶è¯¦ç»†é˜è¿°å…¶æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åˆ†æ\n\nFlowSSC çš„æ ¸å¿ƒç›®æ ‡æ˜¯è§£å†³å•ç›®è¯­ä¹‰åœºæ™¯è¡¥å…¨ä»»åŠ¡ä¸­å›ºæœ‰çš„â€œä¸€å¯¹å¤šâ€æ˜ å°„æ¨¡ç³Šæ€§é—®é¢˜ï¼ŒåŒæ—¶å…‹æœä¼ ç»Ÿæ‰©æ•£æ¨¡å‹æ¨ç†é€Ÿåº¦æ…¢çš„ç“¶é¢ˆã€‚å…¶æŠ€æœ¯æ¡†æ¶å›´ç»• **â€œé«˜æ•ˆæ½œç©ºé—´å‹ç¼©â€** å’Œ **â€œä¸€æ­¥å¼ç”Ÿæˆå¼ç²¾ç‚¼â€** ä¸¤å¤§æ”¯æŸ±æ„å»ºã€‚\n\nä»¥ä¸‹æ˜¯å…¶å…³é”®æŠ€æœ¯æ–¹æ³•çš„è¯¦ç»†è¯´æ˜ï¼š\n\n| æŠ€æœ¯æ¨¡å— | æ ¸å¿ƒç›®æ ‡ | å…³é”®æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°ä¸å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. æ½œç©ºé—´å‹ç¼©ï¼šVecSet VAE** | å°†é«˜ç»´ç¨€ç–ä½“ç´ ç½‘æ ¼å‹ç¼©ä¸ºç´§å‡‘ã€é«˜æ•ˆçš„**ä¸‰å¹³é¢æ½œè¡¨ç¤º**ï¼Œä»¥é™ä½åç»­æ‰©æ•£æ¨¡å‹çš„å¤æ‚åº¦ã€‚ | **a. ç¼–ç å™¨ï¼ˆSet-to-Set Encodingï¼‰ï¼š** <br> - **è¾“å…¥ï¼š** ç¨€ç–ä½“ç´ ç½‘æ ¼ `X âˆˆ {0,1}^{HÃ—WÃ—D}`ï¼Œè¢«è§†ä¸ºä¸€ç»„éç©ºç‰¹å¾ä»¤ç‰Œ `V = {(v_i, p_i)}`ï¼Œå…¶ä¸­ `v_i` æ˜¯ç‰¹å¾ï¼Œ`p_i âˆˆ R^3` æ˜¯å½’ä¸€åŒ–åæ ‡ã€‚<br> - **æŸ¥è¯¢ï¼š** å¼•å…¥ä¸€ç»„å¯å­¦ä¹ çš„**ä¸‰å¹³é¢æŸ¥è¯¢** `Q âˆˆ R^{(H_tp*W_tp + 2*H_tp*D_tp)Ã—C}`ï¼Œç”¨2Då‚…é‡Œå¶ä½ç½®ç¼–ç åˆå§‹åŒ–ï¼Œå¯¹åº”XY, XZ, YZä¸‰ä¸ªæ­£äº¤å¹³é¢ã€‚<br> - **æ³¨æ„åŠ›èšåˆï¼š** ä½¿ç”¨å¤šå¤´äº¤å‰æ³¨æ„åŠ›ï¼Œä»¥ `Q` ä¸ºæŸ¥è¯¢ï¼Œä»¥è¾“å…¥é›† `V` ä¸ºé”®å’Œå€¼ï¼Œå°†3Då‡ ä½•ä¿¡æ¯èšåˆåˆ°æŸ¥è¯¢ä¸­ã€‚å…¬å¼ä¸ºï¼š<br> `h = MHCA(Q, V) = Softmax( Q (V_emb W_K)^T / âˆšd ) (V_emb W_V)` <br> å…¶ä¸­ `V_emb` æ˜¯ä½ç½®å¢å¼ºåçš„è¾“å…¥ä»¤ç‰Œã€‚è¿™ä½¿æ¯ä¸ªæŸ¥è¯¢èƒ½éšå¼åœ°å…³æ³¨å¹¶èšåˆç›¸å…³ç©ºé—´åŒºåŸŸçš„ç‰¹å¾ã€‚<br> **b. è§£ç å™¨ï¼š** <br> - å¯¹äºç›®æ ‡ç½‘æ ¼ä¸­çš„ä»»æ„æŸ¥è¯¢ç‚¹ `x âˆˆ R^3`ï¼Œå°†å…¶æŠ•å½±åˆ°ä¸‰ä¸ªç‰¹å¾å¹³é¢ä¸Šï¼Œé€šè¿‡åŒçº¿æ€§æ’å€¼æ£€ç´¢ç‰¹å¾ `f_xy, f_xz, f_yz`ã€‚<br> - å°†ä¸‰ä¸ªç‰¹å¾æ±‚å’Œåï¼Œé€šè¿‡ä¸€ä¸ªè½»é‡çº§MLPé¢„æµ‹è¯¥ç‚¹çš„å ç”¨æ¦‚ç‡ `Ã”(x)`ã€‚ | - **è¾“å…¥ä½“ç´ åˆ†è¾¨ç‡ï¼š** `256 Ã— 256 Ã— 32` <br> - **ä¸‰å¹³é¢æ½œè¡¨ç¤ºç»´åº¦ï¼š** æ¯ä¸ªå¹³é¢ä¸º `(H_tp, W_tp/D_tp, C)`ï¼Œå…¶ä¸­ `H_tp = W_tp = 128`, `D_tp = 16`, `C = 64`ã€‚<br> - **å‹ç¼©æ¯”ï¼š** ä» `256*256*32 = 2,097,152` ä¸ªä½“ç´ ç‚¹ï¼Œå‹ç¼©åˆ° `3 * 128 * 128 * 64 â‰ˆ 3.1M` ä¸ªæ½œå˜é‡ï¼ˆæœªè€ƒè™‘é€šé“å’Œå¹³é¢æ•°å·®å¼‚çš„ç²¾ç¡®è®¡ç®—ï¼Œä½†æ•°é‡çº§å¤§å¹…é™ä½ï¼‰ã€‚<br> - **é‡å»ºæ€§èƒ½ï¼š** åœ¨éªŒè¯é›†ä¸Šè¾¾åˆ° **85.89% mIoU**ï¼Œè¯æ˜å…¶ä¿çœŸåº¦ã€‚ |\n| **2. ç”Ÿæˆå¼ç²¾ç‚¼ï¼šæ·å¾„æ½œæ‰©æ•£æ¨¡å‹** | åœ¨æ½œç©ºé—´ä¸­ï¼Œä»¥**å•æ­¥æ¨ç†**çš„æ–¹å¼ï¼Œå°†ç²—ç³™é¢„æµ‹ç²¾ç‚¼ä¸ºé«˜ä¿çœŸåº¦çš„å®Œæ•´åœºæ™¯è¡¨ç¤ºã€‚åŸºäº**æµåŒ¹é…**ä¸**æ·å¾„æ¨¡å‹**æ¡†æ¶ã€‚ | **a. ç†è®ºåŸºç¡€ï¼ˆæµåŒ¹é… & æ·å¾„æ¨¡å‹ï¼‰ï¼š** <br> - **æµåŒ¹é…ï¼ˆFMï¼‰ï¼š** å®šä¹‰æ¦‚ç‡è·¯å¾„ `p_t(x)`ï¼Œé€šè¿‡ODEå¹³æ»‘åœ°å°†å…ˆéªŒåˆ†å¸ƒ `p_0`ï¼ˆé«˜æ–¯å™ªå£°ï¼‰è½¬æ¢ä¸ºæ•°æ®åˆ†å¸ƒ `p_1`ï¼š`dx_t/dt = v_t(x_t)`ã€‚FMç›®æ ‡æ˜¯æœ€å°åŒ–é¢„æµ‹å‘é‡åœº `v_Î¸` ä¸ç›®æ ‡å‘é‡åœº `u_t` çš„å·®å¼‚ï¼š`L_FM(Î¸) = E_{t,p_t(x)} [â€– v_Î¸(x,t) - u_t(x) â€–^2]`ã€‚<br> - **æ·å¾„æ¨¡å‹ï¼ˆShortcut Modelsï¼‰ï¼š** ä¸ºè§£å†³æ ‡å‡†FMéœ€è¦å¤šæ­¥ç§¯åˆ†çš„é—®é¢˜ï¼Œå¼•å…¥**æ­¥é•¿ `d`** ä½œä¸ºæ¡ä»¶å˜é‡ã€‚æ¨¡å‹å­¦ä¹ ä¸€ä¸ª**æ·å¾„å‡½æ•° `s_Î¸(x_t, t, d)`**ï¼Œè¡¨ç¤ºä»å½“å‰çŠ¶æ€ `x_t` åˆ°ä¸‹ä¸€çŠ¶æ€ `x_{t+d}` çš„**å½’ä¸€åŒ–æ–¹å‘**ï¼š`x_{t+d} = x_t + d Â· s_Î¸(x_t, t, d)`ã€‚<br> - **è‡ªä¸€è‡´æ€§ç›®æ ‡ï¼š** æ ¸å¿ƒçº¦æŸæ˜¯ï¼Œä¸€ä¸ªå¤§å°ä¸º `2d` çš„æ·å¾„æ­¥åº”ä¸ä¸¤ä¸ªè¿ç»­çš„ `d` æ­¥ç­‰ä»·ï¼š<br> `s(x_t, t, 2d) = (1/2)s(x_t, t, d) + (1/2)s(x'_{t+d}, t+d, d)` <br> å…¶ä¸­ `x'_{t+d}` æ˜¯ç¬¬ä¸€æ­¥åçš„ä¸­é—´çŠ¶æ€ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å­¦ä¹ ä»å•æ­¥ (`d=1`) åˆ°å¤šæ­¥ (`dâ†’0`) çš„çµæ´»æ¨ç†ã€‚ <br><br> **b. æ¨¡å‹æ¶æ„ï¼ˆä¸‰å¹³é¢æ‰©æ•£Transformer - Triplane DiTï¼‰ï¼š** <br> - **è¾“å…¥ï¼š** å™ªå£°ä¸‰å¹³é¢ `h_t` ä¸ç²—ç³™æ¡ä»¶ `h_coarse` çš„é€šé“æ‹¼æ¥ã€‚<br> - **æ¡ä»¶æ³¨å…¥ï¼š** ä½¿ç”¨**è‡ªé€‚åº”å±‚å½’ä¸€åŒ–ï¼ˆAdaLNï¼‰** åŒæ—¶æ³¨å…¥æ—¶é—´æ­¥ `t` å’Œæ­¥é•¿ `d`ï¼š<br> `AdaLN(z, t, d) = Î³(t,d) Â· LayerNorm(z) + Î²(t,d)` <br> `Î³, Î²` ç”± `t` å’Œ `d` çš„åµŒå…¥æ±‚å’Œåç»MLPå›å½’å¾—åˆ°ã€‚è¿™ä½¿å¾—ç½‘ç»œèƒ½åŠ¨æ€é€‚åº”ä¸åŒæ­¥é•¿ä¸‹çš„è®¡ç®—æ¨¡å¼ã€‚<br><br> **c. è®­ç»ƒç›®æ ‡ï¼š** <br> ç»“åˆæµåŒ¹é…æŸå¤±å’Œè‡ªä¸€è‡´æ€§æŸå¤±çš„ç»Ÿä¸€ç›®æ ‡ï¼š<br> ``` <br> L^S(Î¸) = â€– s_Î¸(h_t, t, 0) - (h_gt - h_noise) â€–^2  // æµåŒ¹é…é¡¹ (dâ†’0) <br>        + â€– s_Î¸(h_t, t, 2d) - s_target â€–^2         // è‡ªä¸€è‡´æ€§é¡¹ <br> ``` <br> å…¶ä¸­ `s_target = (1/2)s_Î¸(h_t, t, d) + (1/2)s_Î¸(h'_{t+d}, t+d, d)`ã€‚ | - **æ¨ç†æ­¥æ•°ï¼š** **ä¸»è¦ä½¿ç”¨1æ­¥**ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œ1æ­¥æ¨ç†æ€§èƒ½æœ€ä½³ï¼ˆIoU 56.98%ï¼‰ï¼Œå¤šäº1æ­¥åè€Œå› ç´¯ç§¯è¯¯å·®å¯¼è‡´æ€§èƒ½å¾®é™ã€‚<br> - **è®­ç»ƒç­–ç•¥ï¼š** æ¯ä¸ªmini-batchä¸­ï¼Œ25%çš„æ ·æœ¬è®¡ç®—è‡ªä¸€è‡´æ€§æŸå¤±ï¼ˆéšæœºé‡‡æ · `d âˆˆ [Î´, 1-t]`ï¼‰ï¼Œ75%è®¡ç®—æ ‡å‡†æµåŒ¹é…æŸå¤±ï¼ˆ`d=0`ï¼‰ã€‚<br> - **æ¡ä»¶ï¼š** ç²—ç³™é¢„æµ‹ `X_coarse` ç”±ç°æœ‰å‰é¦ˆSSCæ–¹æ³•ï¼ˆå¦‚MonoSceneèŒƒå¼ï¼‰ç”Ÿæˆï¼Œå¹¶ç¼–ç ä¸ºä¸‰å¹³é¢ `h_coarse`ã€‚<br> - **ä¼˜åŒ–ï¼š** ä½¿ç”¨AdamWï¼Œä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦ï¼Œåˆå§‹å­¦ä¹ ç‡ `1e-4`ã€‚ |\n| **3. æ•´ä½“æµç¨‹ä¸ååŒ** | å°†åˆ¤åˆ«å¼æ¨¡å‹çš„ç»“æ„å…ˆéªŒä¸ç”Ÿæˆå¼æ¨¡å‹çš„ç»†èŠ‚ hallucination èƒ½åŠ›ç›¸ç»“åˆï¼Œå®ç°é€šç”¨ã€é«˜æ•ˆçš„æ€§èƒ½æå‡ã€‚ | **ä¸‰é˜¶æ®µæµæ°´çº¿ï¼š** <br> 1. **ç²—ç³™é¢„æµ‹ï¼š** ä»»ä½•ç°æœ‰å•ç›®SSCæ–¹æ³•ç”Ÿæˆç²—ç³™ä½“ç´  `X_coarse`ï¼Œæä¾›å…¨å±€è¯­ä¹‰å¸ƒå±€å’Œå‡ ä½•å…ˆéªŒã€‚<br> 2. **æ½œç¼–ç ï¼š** ä½¿ç”¨é¢„è®­ç»ƒä¸”å†»ç»“çš„ **VecSet VAE ç¼–ç å™¨**ï¼Œå°† `X_coarse` å‹ç¼©ä¸ºæ¡ä»¶ä¸‰å¹³é¢æ½œç  `h_coarse`ã€‚<br> 3. **ç”Ÿæˆå¼ç²¾ç‚¼ï¼š** **æ·å¾„æ½œæ‰©æ•£æ¨¡å‹** ä»¥ `h_coarse` ä¸ºæ¡ä»¶ï¼Œä»é«˜æ–¯å™ªå£° `h_0` å‡ºå‘ï¼Œé€šè¿‡**å•æ­¥æ·å¾„å‡½æ•°** `s_Î¸(h_0, t=0, d=1)`ï¼Œç›´æ¥ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰å¹³é¢æ½œç  `h_refined`ã€‚<br> 4. **è§£ç è¾“å‡ºï¼š** ä½¿ç”¨ **VecSet VAE è§£ç å™¨** å°† `h_refined` ä¸Šé‡‡æ ·å›æœ€ç»ˆçš„é«˜åˆ†è¾¨ç‡è¯­ä¹‰ä½“ç´ ç½‘æ ¼ã€‚ | - **æ€§èƒ½æå‡ï¼š** åœ¨SemanticKITTIä¸Šï¼Œç²¾ç‚¼é˜¶æ®µå°†IoUä»50.77%æå‡è‡³56.60%ï¼ŒmIoUä»15.86%æå‡è‡³19.51%ã€‚<br> - **é€Ÿåº¦ï¼š** æ€»æ¨ç†æ—¶é—´ ~216ms (4.6 FPS)ã€‚å…¶ä¸­ï¼ŒDiTç²¾ç‚¼ä»…éœ€66msï¼Œå¤§éƒ¨åˆ†æ—¶é—´æ¶ˆè€—åœ¨VAEè§£ç ï¼ˆ150msï¼‰ã€‚<br> - **å†…å­˜ï¼š** æ€»GPUå†…å­˜æ¶ˆè€—çº¦31.56 GBã€‚ |\n\n## æ€»ç»“ï¼šæŠ€æœ¯åˆ›æ–°çš„å®é™…ä»·å€¼\n\n1",
            "timestamp": "2026-01-27T16:04:54.884286",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ ä¸»è¦æ•°æ®é›†\nè®ºæ–‡ä½¿ç”¨ **SemanticKITTI** æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ã€‚\n\n| æ•°æ®é›†åç§° | SemanticKITTI |\n| :--- | :--- |\n| **æ¥æº/å¼•ç”¨** | Behley et al., ICCV 2019 [1] |\n| **æ ¸å¿ƒå†…å®¹** | ç”¨äºæ¿€å…‰é›·è¾¾åºåˆ—è¯­ä¹‰åœºæ™¯ç†è§£çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚ |\n| **åœ¨æœ¬æ–‡ä¸­çš„ä½œç”¨** | ä½œä¸º**åŸºå‡†æ•°æ®é›†**ï¼Œç”¨äºï¼š1) è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰ç»„ä»¶ï¼ˆVAEã€ç²—é¢„æµ‹ç½‘ç»œã€Shortcutæ‰©æ•£æ¨¡å‹ï¼‰ï¼›2) è¿›è¡Œæ¶ˆèå®éªŒï¼ˆéªŒè¯é›†ï¼‰ï¼›3) ä¸ç°æœ‰SOTAæ–¹æ³•è¿›è¡Œå®šé‡æ¯”è¾ƒï¼ˆæµ‹è¯•é›†ï¼‰ã€‚ |\n| **å…³é”®ç»†èŠ‚** | - æ•°æ®æºè‡ªKITTI Vision Benchmarkçš„é‡Œç¨‹è®¡ä»»åŠ¡ã€‚<br>- æä¾›å¯†é›†çš„ã€é€ä½“ç´ çš„3Dè¯­ä¹‰æ ‡æ³¨ï¼Œé€‚ç”¨äº**è¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰**ä»»åŠ¡ã€‚<br>- åŒ…å«20ä¸ªè¯­ä¹‰ç±»åˆ«ï¼ˆå¦‚é“è·¯ã€å»ºç­‘ã€æ±½è½¦ã€è¡Œäººç­‰ï¼‰ã€‚<br>- åœºæ™¯è§„æ¨¡å¤§ï¼Œä½“ç´ ç½‘æ ¼åˆ†è¾¨ç‡ä¸º `256x256x32`ï¼Œè¦†ç›–çœŸå®é©¾é©¶ç¯å¢ƒä¸­çš„ä¸¥é‡é®æŒ¡å’Œå¤æ‚ç»“æ„ã€‚ |\n\n### äºŒã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨è¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰é¢†åŸŸçš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ï¼Œé‡ç‚¹å…³æ³¨å‡ ä½•å®Œæ•´æ€§å’Œè¯­ä¹‰é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚\n\n| æŒ‡æ ‡åç§° | ç¼©å†™ | è®¡ç®—æ–¹å¼ä¸ç»†èŠ‚ | åœ¨æ–‡ä¸­çš„ä½œç”¨ |\n| :--- | :--- | :--- | :--- |\n| **äº¤å¹¶æ¯”** | IoU (Intersection over Union) | `IoU = TP / (TP + FP + FN)` <br> å…¶ä¸­ï¼ŒTPã€FPã€FNåˆ†åˆ«é’ˆå¯¹**æ‰€æœ‰ä½“ç´ **ï¼ˆå¿½ç•¥ç±»åˆ«ï¼‰è®¡ç®—ã€‚è¡¡é‡é¢„æµ‹çš„**æ•´ä½“å‡ ä½•ä½“ç´ å æ®**ä¸çœŸå®å€¼çš„åŒ¹é…ç¨‹åº¦ã€‚ | **æ ¸å¿ƒå‡ ä½•å®Œæ•´æ€§æŒ‡æ ‡**ã€‚åœ¨è¡¨Iä¸­ä½œä¸ºé¦–è¦æŒ‡æ ‡æŠ¥å‘Šï¼Œç›´æ¥åæ˜ åœºæ™¯è¡¥å…¨çš„å‡ ä½•ç²¾åº¦ã€‚FlowSSCè¾¾åˆ°äº† **56.97%** çš„SOTAç»“æœã€‚ |\n| **å¹³å‡äº¤å¹¶æ¯”** | mIoU (mean Intersection over Union) | å¯¹**æ‰€æœ‰è¯­ä¹‰ç±»åˆ«**åˆ†åˆ«è®¡ç®—IoUï¼Œç„¶åå–å¹³å‡å€¼ã€‚ `mIoU = (1/N) * Î£ IoU_c`ï¼Œå…¶ä¸­Nä¸ºç±»åˆ«æ•°ï¼ˆæœ¬æ–‡ä¸º20ï¼‰ã€‚ | **æ ¸å¿ƒè¯­ä¹‰å‡†ç¡®æ€§æŒ‡æ ‡**ã€‚è¡¡é‡æ¨¡å‹åœ¨è¡¥å…¨å‡ ä½•çš„åŒæ—¶ï¼Œä¸ºæ¯ä¸ªä½“ç´ åˆ†é…æ­£ç¡®è¯­ä¹‰æ ‡ç­¾çš„èƒ½åŠ›ã€‚FlowSSCè¾¾åˆ°äº† **19.52%** çš„SOTAç»“æœã€‚ |\n| **å„ç±»åˆ«IoU** | Per-class IoU | é’ˆå¯¹æ•°æ®é›†ä¸­å®šä¹‰çš„**æ¯ä¸€ä¸ªè¯­ä¹‰ç±»åˆ«**ï¼ˆå¦‚`road`, `building`, `car`ç­‰ï¼‰å•ç‹¬è®¡ç®—å…¶IoUã€‚ | ç”¨äº**ç»†ç²’åº¦åˆ†ææ¨¡å‹æ€§èƒ½**ã€‚åœ¨è¡¨Iä¸­è¯¦ç»†åˆ—å‡ºï¼Œå±•ç¤ºäº†FlowSSCåœ¨ä¸åŒç‰©ä½“ç±»å‹ä¸Šçš„ä¼˜åŠ¿ä¸çŸ­æ¿ï¼ˆä¾‹å¦‚ï¼Œåœ¨`building`å’Œ`car`ä¸Šè¡¨ç°çªå‡ºï¼Œåœ¨æŸäº›å°ç‰©ä½“æˆ–ç½•è§ç±»åˆ«ä¸Šåˆ†æ•°è¾ƒä½ï¼‰ã€‚ |\n| **æ¨ç†æ—¶é—´** | Time (ms) | æ¨¡å‹å¤„ç†ä¸€ä¸ªåœºæ™¯æ‰€éœ€çš„**å‰å‘ä¼ æ’­æ—¶é—´**ï¼Œå•ä½ä¸ºæ¯«ç§’ï¼ˆmsï¼‰ã€‚åœ¨è¡¨IIIå’Œè¡¨Vä¸­æŠ¥å‘Šã€‚ | **è¯„ä¼°æ¨¡å‹æ•ˆç‡ä¸å®æ—¶æ€§**çš„å…³é”®æŒ‡æ ‡ã€‚FlowSSCçš„**1æ­¥æ¨ç†**è€—æ—¶çº¦**66ms**ï¼ˆä»…DiTç»†åŒ–é˜¶æ®µï¼‰ï¼Œæ€»æ—¶é—´çº¦216ms (~4.6 FPS)ï¼Œè¯æ˜äº†å…¶â€œå®æ—¶â€æ½œåŠ›ã€‚ |\n| **å†…å­˜æ¶ˆè€—** | Mem (GB) | æ¨¡å‹æ¨ç†æ—¶å ç”¨çš„**GPUæ˜¾å­˜**ã€‚åœ¨è¡¨Vä¸­æŠ¥å‘Šã€‚ | è¡¡é‡æ¨¡å‹**éƒ¨ç½²å¯è¡Œæ€§**ã€‚FlowSSCæ€»å†…å­˜æ¶ˆè€—ä¸º31.56 GBï¼Œè™½ç„¶è¾ƒé«˜ï¼Œä½†ä½œè€…æŒ‡å‡ºå¯åœ¨é«˜ç«¯GPUä¸Šè¿è¡Œï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ä¼˜åŒ–æ–¹å‘ã€‚ |\n| **é‡å»ºIoU/mIoU** | Reconstruction IoU/mIoU | åœ¨**VAEæ¶ˆèå®éªŒ**ï¼ˆè¡¨IVï¼‰ä¸­ä½¿ç”¨ã€‚è®¡ç®—**ç¼–ç -è§£ç **åçš„ä½“ç´ ç½‘æ ¼ä¸åŸå§‹è¾“å…¥ä¹‹é—´çš„IoUå’ŒmIoUã€‚ | ç”¨äºè¯„ä¼°**VecSet VAEçš„å‹ç¼©ä¸é‡å»ºèƒ½åŠ›**ã€‚é«˜é‡å»ºåˆ†æ•°ï¼ˆ85.89% mIoUï¼‰è¡¨æ˜å…¶æ½œåœ¨ç©ºé—´ä¿ç•™äº†ä¸°å¯Œä¿¡æ¯ï¼Œä¸ºåç»­æ‰©æ•£æ¨¡å‹çš„é«˜è´¨é‡ç”Ÿæˆå¥ å®šäº†åŸºç¡€ã€‚ |\n\n### ä¸‰ã€ æŒ‡æ ‡ä½¿ç”¨åœºæ™¯æ€»ç»“\n- **ä¸»å®éªŒï¼ˆè¡¨Iï¼‰**ï¼šä½¿ç”¨ **IoU** å’Œ **mIoU** ä½œä¸ºæ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ï¼Œåœ¨**SemanticKITTIæµ‹è¯•é›†**ä¸Šä¸æ‰€æœ‰åŸºçº¿æ¨¡å‹æ¯”è¾ƒï¼Œè¯æ˜SOTAæ€§èƒ½ã€‚\n- **æ¶ˆèå®éªŒ**ï¼š\n    - **Shortcut Refineræ•ˆæœï¼ˆè¡¨IIï¼‰**ï¼šå¯¹æ¯”â€œä»…ç²—é¢„æµ‹â€å’Œâ€œ+ç»†åŒ–â€çš„ **IoU/mIoU**ï¼Œè¯æ˜ç”Ÿæˆå¼ç»†åŒ–çš„æœ‰æ•ˆæ€§ã€‚\n    - **æ¨ç†æ­¥æ•°åˆ†æï¼ˆè¡¨IIIï¼‰**ï¼šå¯¹æ¯”ä¸åŒæ­¥æ•°ä¸‹çš„ **IoU/mIoU** å’Œ **Time**ï¼ŒéªŒè¯äº†1æ­¥æ¨ç†çš„æœ€ä¼˜æ€§ï¼ˆæ€§èƒ½æœ€é«˜ä¸”é€Ÿåº¦æœ€å¿«ï¼‰ã€‚\n    - **VAEæ¶æ„å¯¹æ¯”ï¼ˆè¡¨IVï¼‰**ï¼šä½¿ç”¨**é‡å»ºIoU/mIoU**ï¼Œè¯æ˜VecSet VAEä¼˜äºä¼ ç»ŸConv VAEã€‚\n- **æ•ˆç‡åˆ†æï¼ˆè¡¨Vï¼‰**ï¼šæ‹†è§£å„é˜¶æ®µ **Time** å’Œæ€» **Mem**ï¼Œå…¨é¢åˆ†ææ¨¡å‹çš„è®¡ç®—å¼€é”€ã€‚\n\n**ç»“è®º**ï¼šFlowSSCä¸¥è°¨åœ°ä½¿ç”¨äº†SSCé¢†åŸŸçš„æ ‡å‡†è¯„ä¼°ä½“ç³»ï¼ˆIoU/mIoUï¼‰ï¼Œå¹¶åœ¨SemanticKITTIè¿™ä¸€æƒå¨æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶**æŠ€æœ¯åˆ›æ–°**ï¼ˆé«˜ç²¾åº¦ç”Ÿæˆï¼‰ä¸**å®é™…ä»·å€¼**ï¼ˆå®æ—¶æ¨ç†èƒ½åŠ›ï¼‰ã€‚æŒ‡æ ‡ç»†èŠ‚å……åˆ†æ”¯æ’‘äº†å…¶â€œåœ¨ä¿æŒå®æ—¶æ€§çš„åŒæ—¶å®ç°SOTAæ€§èƒ½â€çš„æ ¸å¿ƒä¸»å¼ ã€‚",
            "timestamp": "2026-01-27T16:05:39.053594",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## å®éªŒæ•°æ®é›†ä¸æ€§èƒ½åˆ†æ\n\n### æ•°æ®é›†\nè®ºæ–‡åœ¨ **SemanticKITTI** æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒã€‚è¯¥æ•°æ®é›†æ˜¯è‡ªåŠ¨é©¾é©¶é¢†åŸŸå¹¿æ³›ä½¿ç”¨çš„3Dè¯­ä¹‰åœºæ™¯ç†è§£åŸºå‡†ï¼ŒåŒ…å«ä»LiDARåºåˆ—ä¸­è·å–çš„å¯†é›†3Dè¯­ä¹‰æ ‡æ³¨ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°è¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰ä»»åŠ¡ã€‚\n\n### ä¸»è¦è¯„ä¼°æŒ‡æ ‡\n1.  **IoUï¼ˆå‡ ä½•äº¤å¹¶æ¯”ï¼‰**ï¼šè¡¡é‡é¢„æµ‹çš„3Dä½“ç´ å‡ ä½•ä¸çœŸå®å‡ ä½•çš„é‡åˆåº¦ã€‚\n2.  **mIoUï¼ˆå¹³å‡è¯­ä¹‰äº¤å¹¶æ¯”ï¼‰**ï¼šåœ¨20ä¸ªè¯­ä¹‰ç±»åˆ«ï¼ˆå¦‚é“è·¯ã€å»ºç­‘ç‰©ã€æ±½è½¦ã€æ¤è¢«ç­‰ï¼‰ä¸Šè®¡ç®—çš„å¹³å‡IoUï¼Œæ˜¯è¯„ä¼°è¯­ä¹‰åˆ†å‰²å‡†ç¡®æ€§çš„æ ¸å¿ƒæŒ‡æ ‡ã€‚\n\n### æ€§èƒ½æå‡ä¸ç»“æœ\nåœ¨SemanticKITTIçš„**æµ‹è¯•é›†**ä¸Šï¼ŒFlowSSCå–å¾—äº†**æœ€å…ˆè¿›çš„æ€§èƒ½**ï¼Œå…·ä½“æ•°æ®å¦‚ä¸‹ï¼š\n\n| æŒ‡æ ‡ | FlowSSC æ€§èƒ½ | å¯¹æ¯”åŸºçº¿ (ET-Former) | ç»å¯¹æå‡ |\n| :--- | :--- | :--- | :--- |\n| **IoU** | **56.97%** | 51.49% | **+5.48%** |\n| **mIoU** | **19.52%** | 16.30% | **+3.22%** |\n\n**å…³é”®ç»“è®ºï¼š**\n- **æ˜¾è‘—é¢†å…ˆ**ï¼šFlowSSCåœ¨IoUå’ŒmIoUä¸¤ä¸ªæ ¸å¿ƒæŒ‡æ ‡ä¸Šå‡å¤§å¹…è¶…è¶Šäº†æ‰€æœ‰åˆ—å‡ºçš„å…ˆå‰æœ€ä½³æ–¹æ³•ï¼ˆå¦‚ET-Formerã€CGFormerã€VoxFormerç­‰ï¼‰ã€‚\n- **ç±»åˆ«çº§ä¼˜åŠ¿**ï¼šåœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç±»åˆ«ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºã€‚ä¾‹å¦‚ï¼Œåœ¨â€œå»ºç­‘ç‰©â€ç±»åˆ«ä¸Šï¼ŒIoUè¾¾åˆ°41.63%ï¼Œè¿œè¶…ç¬¬äºŒåï¼ˆ26.74%ï¼‰ï¼Œ**æå‡äº†è¿‘15ä¸ªç™¾åˆ†ç‚¹**ã€‚è¿™è¯æ˜äº†å…¶ç”Ÿæˆå¼æ¡†æ¶åœ¨è¡¥å…¨ä¸¥é‡é®æŒ¡ç»“æ„æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ã€‚\n- **æ•ˆç‡ä¸ç²¾åº¦å…¼å¾—**ï¼šå–å¾—ä¸Šè¿°çªç ´æ€§ç²¾åº¦çš„åŒæ—¶ï¼Œæ¨¡å‹æ”¯æŒ**å•æ­¥æ¨ç†**ï¼ˆä»…éœ€66æ¯«ç§’ï¼‰ï¼Œå®ç°äº†é«˜ç²¾åº¦ä¸è¿‘å®æ—¶é€Ÿåº¦çš„å¹³è¡¡ã€‚\n\n**å®é™…ä»·å€¼ä½“ç°**ï¼šè¿™äº›é‡åŒ–ç»“æœè¯å®äº†FlowSSCæå‡ºçš„â€œ**é€šç”¨ç”Ÿæˆå¼å¢å¼ºæ¡†æ¶**â€çš„æœ‰æ•ˆæ€§ã€‚å®ƒèƒ½å¤Ÿå°†ä»»ä½•å‰é¦ˆå¼SSCæ–¹æ³•ç”Ÿæˆçš„ç²—ç³™é¢„æµ‹ï¼Œé€šè¿‡ä¸€æ­¥é«˜æ•ˆçš„æ½œåœ¨æ‰©æ•£ç²¾ä¿®ï¼Œæ˜¾è‘—æå‡å‡ ä½•å®Œæ•´æ€§å’Œè¯­ä¹‰å‡†ç¡®æ€§ï¼Œå°¤å…¶æ“…é•¿åœ¨é®æŒ¡åŒºåŸŸç”Ÿæˆåˆç†ä¸”ç»†èŠ‚ä¸°å¯Œçš„3Dåœºæ™¯ï¼Œè¿™å¯¹äºè‡ªåŠ¨é©¾é©¶ç­‰éœ€è¦å…¨é¢ç¯å¢ƒæ„ŸçŸ¥çš„åº”ç”¨è‡³å…³é‡è¦ã€‚",
            "timestamp": "2026-01-27T16:05:58.162469",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„åˆ†æï¼ŒFlowSSC è™½ç„¶å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹å‡ ä¸ªä¸»è¦å±€é™æ€§ï¼š\n\n- **è®¡ç®—å¤æ‚åº¦ä¸å†…å­˜æ¶ˆè€—**ï¼š\n    - **è®­ç»ƒè¿‡ç¨‹**ï¼šè®ºæ–‡æŒ‡å‡ºï¼ŒåŸºäºæµåŒ¹é…ï¼ˆFlow Matchingï¼‰çš„è®­ç»ƒè¿‡ç¨‹**è®¡ç®—å¯†é›†**ï¼ŒTriplane æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¶æ„éœ€è¦**å¤§é‡çš„ GPU å†…å­˜**ã€‚è¿™é™åˆ¶äº†æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„è®­ç»ƒå’Œéƒ¨ç½²ã€‚\n    - **æ¨ç†é˜¶æ®µ**ï¼šå°½ç®¡æ¨ç†å¯ä»¥ä¸€æ­¥å®Œæˆï¼Œä½†æ•´ä¸ªæµç¨‹ï¼ˆVAEè§£ç +DiTç²¾ç‚¼ï¼‰çš„æ€»å†…å­˜æ¶ˆè€—é«˜è¾¾ **~31.56 GB**ã€‚è™½ç„¶èƒ½é€‚é…é«˜ç«¯GPUï¼Œä½†å¯¹äºè¾¹ç¼˜è®¾å¤‡æˆ–è½¦è½½åµŒå…¥å¼ç³»ç»Ÿè€Œè¨€ï¼Œ**éƒ¨ç½²é—¨æ§›ä»ç„¶è¾ƒé«˜**ã€‚\n\n- **æ¨¡å‹æ¶æ„ä¸æ•ˆç‡**ï¼š\n    - **VAEè§£ç å™¨æ˜¯ç“¶é¢ˆ**ï¼šåœ¨æ¨ç†æ—¶é—´åˆ†æä¸­ï¼Œ**VAEè§£ç å™¨ï¼ˆ150msï¼‰å æ®äº†å¤§éƒ¨åˆ†æ—¶é—´**ï¼Œè¿œé«˜äºDiTç²¾ç‚¼é˜¶æ®µï¼ˆ66msï¼‰ã€‚å°†ç´§å‡‘çš„ä¸‰å¹³é¢æ½œåœ¨è¡¨ç¤ºè§£ç å›é«˜åˆ†è¾¨ç‡ä½“ç´ ç½‘æ ¼çš„è®¡ç®—å¼€é”€è¾ƒå¤§ã€‚\n    - **æ¨¡å‹å¤æ‚åº¦**ï¼šä¸¤é˜¶æ®µï¼ˆç²—é¢„æµ‹+ç”Ÿæˆå¼ç²¾ç‚¼ï¼‰çš„ç”Ÿæˆå¼æ¡†æ¶è™½ç„¶æå‡äº†è´¨é‡ï¼Œä½†æ•´ä½“æ¶æ„æ¯”å‰é¦ˆå¼æ–¹æ³•æ›´å¤æ‚ï¼Œå¢åŠ äº†ç³»ç»Ÿé›†æˆå’Œä¼˜åŒ–çš„éš¾åº¦ã€‚\n\n- **è®­ç»ƒåŠ¨æ€ä¸æ•°æ®ä¾èµ–**ï¼š\n    - **è‡ªæ´½æ€§ç›®æ ‡çš„å½±å“**ï¼šè®ºæ–‡æåˆ°ï¼Œæœªæ¥éœ€è¦ç ”ç©¶**è‡ªæ´½æ€§ï¼ˆSelf-Consistencyï¼‰ç›®æ ‡å¦‚ä½•å½±å“è®­ç»ƒåŠ¨æ€**ï¼Œä»¥åŠ é€Ÿæ¨¡å‹æ”¶æ•›ã€‚è¿™è¡¨æ˜å½“å‰çš„è®­ç»ƒç­–ç•¥å¯èƒ½å°šæœªè¾¾åˆ°æœ€ä¼˜æ•ˆç‡ã€‚\n    - **æ³›åŒ–èƒ½åŠ›ä¾èµ–æ•°æ®è§„æ¨¡**ï¼šæ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼ˆå¦‚æ ¹æ®â€œå®¤å¤–å¸ƒå±€è§„åˆ™â€æ¨æ–­è¢«é®æŒ¡å»ºç­‘ï¼‰ä¸¥é‡ä¾èµ–äºä»æ•°æ®é›†ä¸­å­¦ä¹ åˆ°çš„å…ˆéªŒã€‚è®ºæ–‡æ‰¿è®¤ï¼Œè¦å……åˆ†å‘æŒ¥å…¶æ³›åŒ–æ½œåŠ›ï¼Œéœ€è¦**æ‰©å±•åˆ°æ›´å¤§ã€æ›´å¤šæ ·åŒ–çš„æ•°æ®é›†**è¿›è¡Œè®­ç»ƒã€‚\n\n- **ä»»åŠ¡èŒƒå›´é™åˆ¶**ï¼š\n    - **é™æ€åœºæ™¯å‡è®¾**ï¼šå½“å‰æ–¹æ³•ä¸»è¦å¤„ç†**å•ç›®é™æ€å›¾åƒè¾“å…¥**ã€‚å¯¹äºè‡ªåŠ¨é©¾é©¶ç­‰åŠ¨æ€åœºæ™¯ï¼Œç¼ºä¹å¯¹æ—¶é—´ä¸€è‡´æ€§çš„å»ºæ¨¡æ˜¯ä¸€ä¸ªå…³é”®é™åˆ¶ã€‚\n    - **ä¼ æ„Ÿå™¨å•ä¸€æ€§**ï¼šä½œä¸ºçº¯è§†è§‰æ–¹æ³•ï¼Œå®ƒæ— æ³•åˆ©ç”¨æ¿€å…‰é›·è¾¾ç­‰å¤šæ¨¡æ€æ•°æ®å¯èƒ½æä¾›çš„æ›´ç²¾ç¡®å‡ ä½•ä¿¡æ¯ï¼ˆå°½ç®¡è¿™å±äºæ–¹æ³•å®šä½çš„é€‰æ‹©ï¼Œè€Œéå…¶æœ¬èº«çš„â€œé”™è¯¯â€ï¼‰ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡åœ¨ç»“è®ºéƒ¨åˆ†æ˜ç¡®æå‡ºäº†æœªæ¥å·¥ä½œçš„å‡ ä¸ªå…³é”®æ–¹å‘ï¼š\n\n1.  **æå‡æ•ˆç‡ä¸éƒ¨ç½²èƒ½åŠ›**ï¼š\n    ```\n    æ¢ç´¢é™ä½æ¨¡å‹å¤æ‚åº¦çš„æŠ€æœ¯ï¼Œå¹¶è®¾è®¡å†…å­˜é«˜æ•ˆçš„æ¶æ„ã€‚\n    ```\n    - **å…·ä½“æŠ€æœ¯**ï¼šç ”ç©¶**æ¨¡å‹å‹ç¼©**æŠ€æœ¯ï¼Œå¦‚å‰ªæã€é‡åŒ–ã€çŸ¥è¯†è’¸é¦ï¼Œä»¥å‡å°æ¨¡å‹å¤§å°å’Œè®¡ç®—é‡ã€‚\n    - **æ¶æ„åˆ›æ–°**ï¼šè®¾è®¡æ›´è½»é‡çº§çš„**VAEè§£ç å™¨**å’Œ**DiTæ¨¡å—**ï¼Œæˆ–æ¢ç´¢æ›´é«˜æ•ˆçš„3Dè¡¨ç¤ºï¼ˆè¶…è¶Šä¸‰å¹³é¢ï¼‰ã€‚\n\n2.  **ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹**ï¼š\n    ```\n    ç ”ç©¶è‡ªæ´½æ€§ç›®æ ‡å¦‚ä½•å½±å“è®­ç»ƒåŠ¨æ€ï¼Œä»¥åŠ é€Ÿæ”¶æ•›ã€‚\n    ```\n    - **ç†è®ºç ”ç©¶**ï¼šæ·±å…¥åˆ†æShortcut Flow Matchingä¸­**è‡ªæ´½æ€§æŸå¤±çš„ä¼˜åŒ–è½¨è¿¹**ï¼Œè®¾è®¡æ›´ç¨³å®šçš„è®­ç»ƒç­–ç•¥æˆ–è°ƒåº¦æ–¹æ³•ã€‚\n    - **è®­ç»ƒåŠ é€Ÿ**ï¼šæ¢ç´¢æ›´é«˜æ•ˆçš„è®­ç»ƒæŠ€å·§ï¼Œå‡å°‘è¾¾åˆ°ç›¸åŒæ€§èƒ½æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°æˆ–è®¡ç®—èµ„æºã€‚\n\n3.  **æ‰©å±•æ•°æ®ä¸æ³›åŒ–èƒ½åŠ›**ï¼š\n    ```\n    ä¸ºäº†å……åˆ†åˆ©ç”¨ç”Ÿæˆå¼æ¡†æ¶çš„æ³›åŒ–æ½œåŠ›ï¼Œæ‰©å±•åˆ°æ›´å¤§ã€æ›´å¤šæ ·åŒ–çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒè‡³å…³é‡è¦ã€‚\n    ```\n    - **å¤§è§„æ¨¡é¢„è®­ç»ƒ**ï¼šåœ¨æ›´å¹¿æ³›çš„è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ï¼ˆå¦‚nuScenes, Waymoï¼‰æˆ–åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ æ›´é€šç”¨çš„3Dåœºæ™¯å…ˆéªŒã€‚\n    - **é¢†åŸŸé€‚åº”**ï¼šç ”ç©¶å¦‚ä½•ä½¿æ¨¡å‹å¿«é€Ÿé€‚åº”æ–°çš„åŸå¸‚ç¯å¢ƒæˆ–ä¸åŒçš„å¤©æ°”æ¡ä»¶ã€‚\n\n4.  **æ‰©å±•ä»»åŠ¡èŒƒå›´**ï¼š\n    ```\n    å°†æˆ‘ä»¬é«˜æ•ˆçš„é¢„æµ‹èŒƒå¼æ‰©å±•åˆ°è§†é¢‘è¾“å…¥ï¼Œä»¥æå‡æ—¶é—´ä¸€è‡´æ€§ï¼Œæ˜¯æœªæ¥å·¥ä½œçš„ä¸€ä¸ªå…³é”®æ–¹å‘ã€‚\n    ```\n    - **æ—¶åºå»ºæ¨¡**ï¼šå¼•å…¥**è§†é¢‘æˆ–è¿ç»­å¸§ä½œä¸ºè¾“å…¥**ï¼Œåˆ©ç”¨æ—¶é—´çº¿ç´¢æ”¹å–„é®æŒ¡æ¨ç†çš„å‡†ç¡®æ€§å’Œåœºæ™¯è¡¥å…¨çš„æ—¶åºå¹³æ»‘æ€§ã€‚\n    - **4Dè¯­ä¹‰åœºæ™¯è¡¥å…¨**ï¼šé¢„æµ‹åŠ¨æ€ç‰©ä½“çš„æœªæ¥çŠ¶æ€æˆ–æ•´ä¸ªåœºæ™¯çš„çŸ­æœŸæ¼”å˜ã€‚\n\n5.  **æ¡†æ¶é€šç”¨æ€§æ·±åŒ–**ï¼š\n    - **æ¡ä»¶çµæ´»æ€§**ï¼šæ¢ç´¢é™¤ç²—ä½“ç´ é¢„æµ‹å¤–çš„å…¶ä»–æ¡ä»¶è¾“å…¥ï¼ˆå¦‚ç¨€ç–æ·±åº¦ã€è¯­ä¹‰åˆ†å‰²å›¾ï¼‰ï¼Œä½¿æ¡†æ¶æ›´å…·é€šç”¨æ€§ã€‚\n    - **ä¸æ›´å¤šåŸºçº¿é›†æˆ**ï¼šè¿›ä¸€æ­¥éªŒè¯å…¶ä½œä¸ºâ€œ**é€šç”¨ç”Ÿæˆå¼å¢å¼ºæ¡†æ¶**â€çš„èƒ½åŠ›ï¼Œä¸æ›´å¤šæ ·åŒ–çš„å‰æ²¿SSCæ–¹æ³•æ— ç¼é›†æˆã€‚\n\n**æ€»ç»“**ï¼šFlowSSCçš„æ ¸å¿ƒå±€é™æ€§åœ¨äºå…¶**å…ˆè¿›æ€§èƒ½æ˜¯ä»¥è¾ƒé«˜çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ä¸ºä»£ä»·çš„**ã€‚æœªæ¥çš„æ”¹è¿›å°†å›´ç»•**æ•ˆç‡æå‡**ï¼ˆè®­ç»ƒä¸éƒ¨ç½²ï¼‰ã€**èƒ½åŠ›æ‰©å±•**ï¼ˆæ—¶åºã€æ•°æ®ï¼‰å’Œ**ç†è®ºæ·±åŒ–**ï¼ˆè®­ç»ƒåŠ¨æ€ï¼‰ä¸‰ä¸ªä¸»è½´å±•å¼€ï¼Œæ—¨åœ¨ä½¿å…¶å¼ºå¤§çš„ç”Ÿæˆå¼åœºæ™¯ç†è§£èƒ½åŠ›çœŸæ­£é€‚ç”¨äºå¯¹å®æ—¶æ€§å’Œèµ„æºæ¶ˆè€—æä¸ºè‹›åˆ»çš„è§„æ¨¡åŒ–åº”ç”¨åœºæ™¯ã€‚",
            "timestamp": "2026-01-27T16:06:36.719965",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T18:32:27Z",
    "created_at": "2026-01-27T15:53:23.948050",
    "updated_at": "2026-01-27T15:53:23.948057",
    "recommend": 0
}