{
    "id": "2601.10814v1",
    "title": "SurfSLAM: Sim-to-Real Underwater Stereo Reconstruction For Real-Time SLAM",
    "authors": [
        "Onur Bagoren",
        "Seth Isaacson",
        "Sacchin Sundar",
        "Yung-Ching Sun",
        "Anja Sheppard",
        "Haoyu Ma",
        "Abrar Shariff",
        "Ram Vasudevan",
        "Katherine A. Skinner"
    ],
    "abstract": "å®šä½ä¸å»ºå›¾æ˜¯æ°´ä¸‹æœºå™¨äººçš„æ ¸å¿ƒæ„ŸçŸ¥èƒ½åŠ›ã€‚ç«‹ä½“ç›¸æœºä¸ºç›´æ¥ä¼°è®¡åº¦é‡æ·±åº¦æä¾›äº†ä½æˆæœ¬æ‰‹æ®µä»¥æ”¯æŒè¿™äº›ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå°½ç®¡é™†åœ°åœºæ™¯çš„ç«‹ä½“æ·±åº¦ä¼°è®¡æŠ€æœ¯è¿‘æœŸå–å¾—è¿›å±•ï¼Œåœ¨æ°´ä¸‹åœºæ™¯ä¸­é€šè¿‡å›¾åƒå¯¹è®¡ç®—æ·±åº¦ä»å…·æŒ‘æˆ˜æ€§ã€‚æ°´ä¸‹ç¯å¢ƒä¸­ï¼Œå›¾åƒä¼šå› å…‰è¡°å‡ã€è§†è§‰ä¼ªå½±å’ŒåŠ¨æ€å…‰ç…§æ¡ä»¶è€Œé€€åŒ–ã€‚æ­¤å¤–ï¼ŒçœŸå®æ°´ä¸‹åœºæ™¯å¸¸ç¼ºä¹å¯¹ç«‹ä½“æ·±åº¦ä¼°è®¡å’Œä¸‰ç»´é‡å»ºæœ‰ç”¨çš„ä¸°å¯Œçº¹ç†ã€‚å› æ­¤ï¼Œåœ¨ç©ºæ°”ä¸­æ•°æ®ä¸Šè®­ç»ƒçš„ç«‹ä½“ä¼°è®¡ç½‘ç»œæ— æ³•ç›´æ¥è¿ç§»è‡³æ°´ä¸‹é¢†åŸŸã€‚åŒæ—¶ï¼Œç°æœ‰çœŸå®ä¸–ç•Œæ°´ä¸‹ç«‹ä½“æ•°æ®é›†ä¸è¶³ä»¥æ”¯æ’‘ç¥ç»ç½‘ç»œçš„ç›‘ç£å¼è®­ç»ƒã€‚åŸºäºç«‹ä½“çš„åŒæ­¥å®šä½ä¸å»ºå›¾ç®—æ³•ä¸­ï¼Œæ°´ä¸‹æ·±åº¦ä¼°è®¡çš„ç¼ºé™·è¢«è¿›ä¸€æ­¥æ”¾å¤§ï¼Œè¿™æˆä¸ºæ°´ä¸‹æœºå™¨äººæ„ŸçŸ¥çš„æ ¹æœ¬æ€§æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°é¢–æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿæ•°æ®ä¸è‡ªç›‘ç£å¾®è°ƒå®ç°æ°´ä¸‹ç«‹ä½“è§†å·®ä¼°è®¡ç½‘ç»œçš„ä»¿çœŸåˆ°çœŸå®è®­ç»ƒã€‚åˆ©ç”¨ä¹ å¾—çš„æ·±åº¦é¢„æµ‹ï¼Œæˆ‘ä»¬å¼€å‘äº†\\algnameâ€”â€”ä¸€ç§èåˆç«‹ä½“ç›¸æœºã€æƒ¯æ€§æµ‹é‡å•å…ƒã€æ°”å‹è®¡ä¸å¤šæ™®å‹’é€Ÿåº¦è®¡æµ‹é‡çš„å®æ—¶æ°´ä¸‹SLAMæ–°æ¡†æ¶ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨æ°´ä¸‹æœºå™¨äººé‡‡é›†äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œæ²‰èˆ¹å‹˜æµ‹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡24,000ç»„ç«‹ä½“å›¾åƒå¯¹ï¼Œä»¥åŠç”¨äºè¯„ä¼°çš„é«˜è´¨é‡å¯†é›†æ‘„å½±æµ‹é‡æ¨¡å‹ä¸å‚è€ƒè½¨è¿¹ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æè®­ç»ƒæ–¹æ³•åœ¨çœŸå®æ•°æ®ä¸Šçš„ä¼˜åŠ¿ï¼šä¸ä»…èƒ½æå‡æ°´ä¸‹ç«‹ä½“ä¼°è®¡æ€§èƒ½ï¼Œè¿˜èƒ½å®ç°å¯¹å¤æ‚æ²‰èˆ¹é—å€çš„ç²¾ç¡®è½¨è¿¹ä¼°è®¡ä¸ä¸‰ç»´é‡å»ºã€‚",
    "url": "https://arxiv.org/abs/2601.10814v1",
    "html_url": "https://arxiv.org/html/2601.10814v1",
    "html_content": "SurfSLAM: Sim-to-Real Underwater Stereo Reconstruction For Real-Time SLAM\nOnur Bagoren\nâˆ—\nSeth Isaacson\nâˆ—\nSacchin Sundar\nYung-Ching Sun\nAnja Sheppard\nHaoyu Ma\nAbrar Shariff\nRam Vasudevan\nKatherine A. Skinner\nAbstract\nLocalization and mapping are core perceptual capabilities for underwater robots.\nStereo cameras provide a low-cost means of directly estimating metric depth to support these tasks.\nHowever, despite recent advances in stereo depth estimation on land, computing depth from image pairs in underwater scenes remains challenging.\nIn underwater environments, images are degraded by light attenuation, visual artifacts, and dynamic lighting conditions.\nFurthermore, real-world underwater scenes frequently lack rich texture useful for stereo depth estimation and 3D reconstruction.\nAs a result, stereo estimation networks trained on in-air data cannot transfer directly to the underwater domain.\nIn addition, there is a lack of real-world underwater stereo datasets for supervised training of neural networks.\nPoor underwater depth estimation is compounded in stereo-based Simultaneous Localization and Mapping (SLAM) algorithms, making it a fundamental challenge for underwater robot perception.\nTo address these challenges, we propose a novel framework that enables sim-to-real training of underwater stereo disparity estimation networks using simulated data and self-supervised finetuning.\nWe leverage our learned depth predictions to develop SurfSLAM, a novel framework for real-time underwater SLAM that fuses stereo cameras with IMU, barometric, and Doppler Velocity Log (DVL) measurements.\nLastly, we collect a challenging real-world dataset of shipwreck surveys using an underwater robot.\nOur dataset features over 24,000 stereo pairs, along with high-quality, dense photogrammetry models and reference trajectories for evaluation.\nThrough extensive experiments, we demonstrate the advantages of the proposed training approach on real-world data for improving stereo estimation in the underwater domain and for enabling accurate trajectory estimation and 3D reconstruction of complex shipwreck sites.\nUpon publication, code, real-world underwater data, and simulated training data will be made publicly available at\nhttps://umfieldrobotics.github.io/SurfSLAM/\n.\nhttps://umfieldrobotics.github.io/SurfSLAM/\nFigure 1\n:\nSurfSLAM performs real-time SLAM in challenging underwater environments by fusing proprioceptive data with global registration. Accurate global registration is enabled by an underwater stereo disparity estimation algorithm that is fine-tuned via a novel sim-to-real pipeline. Illustrated is our BlueROV data collection platform navigating a shipwreck. Two stereo frames along the robotâ€™s path (drawn in pink) are visualized. Background image courtesy of the National Oceanic and Atmospheric Administration Thunder Bay National Marine Sanctuary.\nâ€ \nâ€ \nâˆ—\nDenotes equal contribution\nâ€ \nâ€ \nThis work was supported by the National Science Foundation under Award No. 2337774.\nâ€ \nâ€ \nAll authors are with the Department of Robotics, University of Michigan, Ann Arbor, MI 48109, USA\nâ€ \nâ€ \nCorresponding author e-mail:\nobagoren@umich.edu\nI\nIntroduction\nUnderwater simultaneous localization and mapping (SLAM) is a key challenge in marine robotics, with wide-ranging applications such as underwater inspection\n[\n22\n]\nand semantic mapping\n[\n61\n]\n.\nVision plays a crucial role in this context.\nCameras are a relatively low-cost sensor, and stereo cameras can provide metric depth information useful for reconstructing the scene geometry.\nHowever, underwater imaging presents several fundamental differences from in-air imagery.\nThe presence of haze, attenuation, dynamic lighting, and suspended particles reduces contrast and detail.\nFurther, large areas of real-world underwater scenes often lack distinctive, trackable textures useful for estimating geometry\n[\n2\n,\n33\n]\n.\nThese underwater effects can vary significantly between locations and conditions, making it challenging to develop perception systems that remain robust across diverse underwater environments\n[\n27\n]\n.\nClassical, hand-designed methods for stereo depth estimation struggle in textureless and hazy regions; as a result, they do not generalize well to the underwater domain\n[\n20\n]\n.\nFor the in-air domain, significant attention has been devoted to the monocular depth estimation problem\n[\n83\n]\n. In the underwater domain, several works have adapted in-air monocular depth estimation networks to account for underwater image characteristics\n[\n86\n,\n84\n,\n87\n]\n.\nStill, monocular depth estimation cannot reliably provide metric-scale depth.\nRecent deep stereo networks achieve excellent performance in estimating in-air metric depth\n[\n76\n,\n78\n,\n28\n]\n.\nHowever, as we demonstrate in this work, their accuracy degrades significantly underwater due to the domain shift induced by haze, attenuation, and dynamic lighting.\nSeveral works have proposed learning-based solutions to underwater depth estimation\n[\n85\n,\n63\n,\n77\n]\n, but a lack of foundation-scale underwater training data and limitations in simulation fidelity have limited their accuracy.\nIn this work, we propose\nSurfSLAM\n, illustrated in Figure\n1\n. SurfSLAM is a novel framework for\nS\nim-to-real\nU\nnderwater stereo\nR\neconstruction\nF\nor real-time\nSLAM\n.\nOur sim-to-real stereo depth estimation pipeline leverages knowledge of underwater imaging effects to recover dense, reliable metric depth from underwater stereo imagery.\nWe demonstrate that our sim-to-real training enables state-of-the-art stereo depth estimation in the underwater domain.\nAs a result, the dense depth maps estimated by our approach provide strong geometric priors to improve navigation and mapping capabilities.\nOur proposed SLAM framework fuses depth predictions with measurements from an IMU, Doppler Velocity Log (DVL), and barometer to track vehicle pose.\nThe vehicle pose and dense depth maps are then used to estimate a dense 3D map of the scene.\nOur method is evaluated on a dataset of long-duration shipwreck surveys for visual-acoustic-inertial SLAM.\nOur dataset includes reference 3D reconstructions and vehicle trajectories to support qualitative and quantitative evaluation.\nWe present extensive experiments demonstrating that SurfSLAM improves both trajectory accuracy and reconstruction quality for underwater SLAM in challenging real-world environments compared to existing state-of-the-art methods.\nCode, model weights, real-world underwater data, and simulated training data will be made publicly available upon publication.\nII\nRelated Work\nSurfSLAM is a method that combines underwater SLAM, stereo depth estimation, and the transfer of deep learning methods to the underwater domain. We review the relevant literature below.\nII.A\nVisual SLAM\nThe task of estimating camera motion from stereo images, optionally fused with inertial sensing, has received enormous attention.\nORB-SLAM and its variants\n[\n7\n]\noperate by matching and triangulating image features, tracking camera motion relative to feature maps with inertial priors, and jointly optimizing trajectories with a map via bundle adjustment\n[\n7\n]\n.\nThese methods also implement a keypoint-based visual place recognition method that helps mitigate long-term drift.\nWhile ORB-SLAM3 provides outstanding trajectory accuracy in visually rich scenes, it is prone to tracking loss in featureless regions and does not output a dense map useful for scene understanding.\nAs an alternative to feature-based methods, Droid-SLAM uses a learned optical flow module on images for dense correspondence matching and a learned pose update operator for incrementally estimating the state and producing a dense map representation\n[\n66\n]\n.\nStill, Droid-SLAM struggles in feature-sparse regions and lacks loop closure detection.\nMore recent approaches have applied large vision foundation models as the backbone to visual SLAM.\nProminent work leverages vision transformers (ViTs) pretrained on large-scale simulated in-air data for zero-shot feature matching or 3D reconstruction\n[\n32\n,\n72\n]\n, with MASt3R-SLAM integrating MASt3R pointmaps into a monocular keyframe-based estimator\n[\n39\n]\n.\nVGGT-SLAM instead jointly estimates structure and motion on the\nğš‚ğ™»\nâ€‹\n(\n4\n)\n\\mathtt{SL}(4)\ngroup from uncalibrated sequences\n[\n36\n]\n.\nThese systems remain image-centric, heavily rely on ViT, and perform poorly in underwater domains where images fall out of the training distribution.\nIn contrast, our proposed method, SurfSLAM, fuses acoustic-inertial data with learned stereo depth estimation and visual place recognition to produce accurate trajectories in both featureless and feature-rich environments.\nII.B\nUnderwater Visual SLAM\nEarly underwater visual SLAM focused on monocular-based navigation for large-scale inspections using pose-graph formulations\n[\n14\n,\n30\n,\n31\n]\n.\nThese works introduced informative visual constraint selection based on local geometry and demonstrated robust state estimation by fusing vision with DVL and inertial measurements for large-scale inspections\n[\n43\n,\n4\n]\n.\nLater works tightly coupled acoustic and inertial sensors with visual information for more robust state estimation and mapping performance\n[\n79\n,\n70\n,\n3\n]\n.\nPan et. al. present a tightly integrated imaging sonar-IMU-camera system\n[\n44\n]\n, while SVIn2 fuses a barometer and a mechanically scanning sonar into a sonar-visual-acoustic state estimator\n[\n48\n]\n.\nThoms et. al. integrate a camera, DVL, IMU, and LiDAR for an autonomous surface vehicle, providing foundational work on the integration of the DVL into a pose-graph-based formulation\n[\n67\n]\n.\nAQUA-SLAM similarly integrates a DVL into a visual-inertial odometry (VIO) framework for underwater applications, with online extrinsic calibration\n[\n81\n]\n.\nThese approaches yield robust pose estimates but produce sparse maps that lack the dense information available from stereo cameras present in each of the methods.\nSong et. al. produce dense maps through depth fusion of stereo depth maps along trajectories estimated by an acoustic-inertial tracker\n[\n64\n]\n.\nWang et al. perform a similar process by using a Sum of Absolute Differences to compute stereo disparities, combined with poses estimated from SVIn2\n[\n73\n]\n.\nOur work incorporates metric depth measurements estimated by a stereo network to form global registration factors, and combines them into an acoustic-visuo-inertial pose graph. As a result, SurfSLAM both maintains accurate tracking in featureless regions via acoustic-inertial fusion and mitigates long-term drift via global place recognition.\nFurthermore, with dense stereo depth estimation, SurfSLAM is able to output a dense map of complex underwater environments.\nII.C\nStereo Depth Estimation\nClassical stereo and 3D reconstruction methods rely on hand-crafted pipelines for matching cost computation, disparity optimization, and refinement\n[\n55\n,\n21\n,\n49\n,\n5\n]\n.\nCommonly used methods include EpicFlow-based matching\n[\n5\n]\n,\n[\n71\n]\n, and Agisoft Metashape\n[\n1\n]\n.\nThese approaches tend to be highly sensitive to hyperparameters, which may need to be re-tuned depending on the scene.\nOther methods focus on matching keypoints between images, then triangulating and optimizing 3D map points\n[\n57\n,\n58\n]\n.\nWhile this may provide good geometric accuracy, these methods cannot operate in textureless regions. Furthermore, they can take several hours to several days to run on real-world scenes, limiting their utility in the field.\nFigure 2\n:\nAn overview of SurfSLAM. We take as input measurements from a barometer, IMU, DVL, and a stereo image pair. We maintain an acoustic-inertial pose graph that preintegrates measurements to maintain a pose throughout operation. In parallel, we use our finetuned underwater stereo network to produce metric depth maps. These depth estimates and stereo images are used to perform geometric tracking to perform global registration, and reduce drift over operation, producing accurate trajectories and dense maps during operation.\nThe introduction of deep learning for stereo depth estimation offers an alternative to hand-crafted approaches.\nRAFT-Stereo\n[\n34\n]\nemploys recurrent feature matching and iterative refinement with a typical encoder-decoder network architecture to produce disparity maps from a stereo image pair.\nHowever, this approach often requires finetuning on a target domain for optimal performance.\nThe recent release of FoundationStereo\n[\n76\n]\nhas demonstrated the potential of models trained on large-scale simulated datasets to achieve excellent cross-domain, generalizable zero-shot performance on in-air data.\nFoundation stereo leverages a pre-trained and frozen foundation model encoder from DepthAnything V2\n[\n82\n]\n.\nThe foundation model features are fused with side-tuned CNN features and processed by a combination of convolutions, transformer mechanisms, and iterative refinement to produce high-quality disparity estimates\n[\n76\n]\n.\nDEFOM-Stereo\n[\n28\n]\nalso uses a foundation model to generate monocular depth estimation cues, but leverages a RAFT-Stereo backbone.\nIGEV++\n[\n78\n]\nforgoes foundation model encoders and instead introduces multi-range geometry encoding volumes to capture different levels of encoding granularity, leading to strong performance in large-disparity scenes.\nWhile IGEV++ cannot achieve the level of detail and accuracy of\n[\n76\n,\n28\n]\n, it produces competitive results with significantly lower memory footprint and inference time than foundation model-based methods.\nA prominent feature of deep learning methods for stereo prediction networks is the iterative refinement process, typically modeled as a variant of a gated recurrent unit (GRU) layer\n[\n76\n]\nor an incremental disparity update step\n[\n28\n]\n.\nThe iterative refinement step updates an initial disparity estimate based on correlation features extracted early in the network architecture, providing context relevant for refining disparity estimates.\nStill, since these methods are developed and trained for in-air stereo depth estimation, they lack generalizability to underwater conditions such as turbidity, low light, caustics, and attenuation.\nThe lack of large-scale training sets for training underwater stereo networks limits the ability to re-train these networks on real stereo imagery from underwater scenes.\nII.D\nDeep Learning for Underwater Stereo\nA class of work that aims to overcome data scarcity in underwater stereo matching trains a network with direct supervision, using augmentations to make the training data more closely resemble the appearance of underwater images\n[\n88\n,\n35\n,\n85\n]\n.\nIn\n[\n88\n]\n, the authors employ a style transfer network to transform a reference underwater image into a target in-air stereo pair image with ground-truth disparity, and then train the network using these augmented images.\nOther works have leveraged a physics-based approach, following underwater image formation models to augment large in-air datasets with water column effects\n[\n35\n]\n.\nWhile simulated data mitigate the data scarcity problem, accurately simulating the complex underwater image formation remains challenging.\nSelf-supervised approaches aim to overcome this by leveraging stereo image pair consistency objectives.\nUWStereoNet\n[\n63\n]\npre-trains an unsupervised Siamese network for disparity estimation on in-air data and then fine-tunes it on underwater data using a self-supervised objective of warping the stereo pairs onto each other and computing a photometric error between them.\nMore recently, StereoAdapter\n[\n77\n]\ntargets underwater domain shift by parameter-efficiently adapting a monocular foundation encoder and a self-supervised objective that similarly minimizes a photometric error between stereo pairs with additional losses to handle occlusions and improve depths at edges.\nIn\n[\n77\n]\n, a dataset of 40,000 simulated images is also released, consisting of augmentations, texture maps and environments to better match the underwater image conditions, and perform pre-training with this dataset.\nWe propose a fusion of the two approaches.\nWe first develop a novel, comprehensive train-time augmentation pipeline to generate realistic underwater images from large-scale in-air datasets, producing simulated data to train a model with direct supervision on the stereo disparity estimates.\nThis train-time augmentation is made possible due to the simulated data being coupled with the stereo intrinsic and extrinsic parameters, a key addition coupled with our simulated data.\nAs a fine-tuning approach, similar to\n[\n63\n,\n77\n]\n, we employ a warping loss for self-supervised training.\nAs a novel contribution over existing methods, we propose additional regularization to guide the model to make correct predictions when the background (i.e., water column) is present in the stereo image pairs.\nThe depths from the finetuned model are then used in our SurfSLAM framework to enable tracking and mapping in environments where the geometric cues allow for place recognition, global registration, and loop closures.\nIII\nMethod Overview\nWe propose\nSurfSLAM\n, a novel framework for\nS\nim-to-real\nU\nnderwater stereo\nR\neconstruction\nF\nor real-time\nSLAM\n.\nAn overview of the proposed method is shown in Figure\n2\n. First, our sim-to-real stereo depth estimation pipeline leverages knowledge of underwater imaging effects to recover dense, reliable metric depth from underwater stereo imagery.\nThen, SurfSLAM fuses our dense stereo depth predictions using global registration with measurements from an IMU, DVL, and barometer for tracking vehicle pose. The poses and depth maps are then used to build a dense 3D reconstruction of the scene.\nThe discussion of the proposed technical approach is organized as follows: Subsection\nIII.A\nprovides the notation and coordinate conventions used throughout the paper.\nSection\nIV\npresents our proposed methodology for sim-to-real training of underwater stereo disparity estimation.\nThen, Section\nV\ndescribes our SurfSLAM system, which fuses stereo depth estimation with acoustic and inertial sensing for real-time localization and dense mapping of complex underwater scenes.\nIII.A\nPreliminaries\nIII.A.1\nNotation\nWe denote scalars as lowercase italics (\nx\n,\ny\n,\nz\n,\nâ€¦\nx,y,z,\\dots\n), matrices as uppercase bold\n(\nğ—\n,\nğ˜\n,\nğ™\n,\nâ€¦\n)\n\\left(\\mathbf{X},\\mathbf{Y},\\mathbf{Z},\\dots\\right)\nand vectors as lowercase bold\n(\nğ±\n,\nğ²\n,\nğ³\n,\nâ€¦\n)\n\\left(\\mathbf{x},\\mathbf{y},\\mathbf{z},\\dots\\right)\n.\nSensor measurements are denoted as capitalized calligraphic\n(\nğ’³\n,\nğ’´\n,\nğ’µ\n,\nâ€¦\n)\n\\left(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\dots\\right)\n,\nand estimated variables are denoted with a bar\nx\nÂ¯\n\\bar{x}\n.\nIII.A.2\nFrame Definitions\nThe reference frames are shown in Figure\n3\n, where red indicates the\nx\nx\n-axis, green the\ny\ny\n-axis, and blue the\nz\nz\n-axis. The reference frames include the fixed world frame,\nğš†\n\\mathtt{W}\n, and the North-East-Down (NED) frame,\nğ™½\n\\mathtt{N}\n.\nThe base frame,\nğ™±\n\\mathtt{{B}}\n, and IMU frame,\nğ™¸\n\\mathtt{I}\n, are shown to be coincident, similar to that of\n[\n15\n]\n, and the sensor frames are represented as the DVL frame,\nğ™³\n\\mathtt{D}\n, the barometer frame,\nğ™¿\n\\mathtt{P}\n, and the left camera frame,\nğ™²\n\\mathtt{C}\n.\nAll transformations and their notations follow the definitions in\n[\n16\n]\n.\nTable 1\n:\nA summary of underwater SLAM and stereo datasets. We propose two new datasets: a large simulated dataset with comprehensive underwater augmentation and a real-world dataset. âœ“* denotes that the dataset will be released upon publication. Photogrammetry is shortened to PGM, and simulation is shortened to Sim. for the ground truth column.\nWater Effects\nAdditional Sensors\nName\nCamera\nSplits\nNo.\nImages\nPublicly\nAvail.\nGround Truth\nTurbid\nLow Light\nDir. Light\nDVL\nIMU\nBaro.\nTrain\n4,047\nâœ“\nUWStereoNet\n[\n63\n]\nStereo\nTest\n15\nâœ“\nLiDAR\nâœ“\nSQUID\n[\n5\n]\nStereo\n-\n57\nâœ“\nPGM\nâœ“\nFLSea\n[\n49\n]\nStereo\n-\n19,596\nâœ“\nPGM\nâœ“\nâœ“\nSOTRUE\n[\n37\n]\nStereo\n-\n8,497\nâœ“\nEncoder\nâœ“\nâœ“\nUWStereo\n[\n35\n]\nSim.\n-\n29,568\nâœ•\nSim.\nâœ“\nâœ“\nStereoAdapter\n[\n77\n]\nSim.\n-\n40,000\nâœ“\nSim.\nâœ“\nâœ“\nâœ“\n\\cellcolor\n[HTML]ECF4FF\nUWSim (Ours)\n\\cellcolor\n[HTML]ECF4FFSim.\n\\cellcolor\n[HTML]ECF4FF-\n\\cellcolor\n[HTML]ECF4FF105,600\n\\cellcolor\n[HTML]ECF4FFâœ“*\n\\cellcolor\n[HTML]ECF4FFSim.\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FF\n\\cellcolor\n[HTML]ECF4FF\n\\cellcolor\n[HTML]ECF4FF\n\\cellcolor\n[HTML]ECF4FF\n\\cellcolor\n[HTML]ECF4FF\n\\cellcolor\n[HTML]ECF4FFTrain\n\\cellcolor\n[HTML]ECF4FF19,950\n\\cellcolor\n[HTML]ECF4FF\n\\cellcolor\n[HTML]ECF4FF-\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FF\n\\cellcolor\n[HTML]ECF4FF\n\\cellcolor\n[HTML]ECF4FF\nStereo\n\\cellcolor\n[HTML]ECF4FF\nSUDS (Ours)\n\\cellcolor\n[HTML]ECF4FFStereo\n\\cellcolor\n[HTML]ECF4FFTest\n\\cellcolor\n[HTML]ECF4FF1,486\n\\cellcolor\n[HTML]ECF4FFâœ“*\n\\cellcolor\n[HTML]ECF4FFPGM\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FF\n\\cellcolor\n[HTML]ECF4FF\n\\cellcolor\n[HTML]ECF4FF\nUWslam\n[\n6\n]\nStereo\n-\n3,111\nâœ“\nPGM\nâœ“\nFLSea\n[\n49\n]\nMono.\n-\n22,456\nâœ“\nPGM\nâœ“\nâœ“\nâœ“\nSVIn2\n[\n48\n]\nCustom\n-\n25,145\nâœ“\nPGM\nâœ“\nâœ“\nâœ“\nâœ“\nâœ“\nTank\n[\n80\n]\nStereo\n-\n40,649\nâœ“\nAprilTag\nâœ“\nâœ“\nâœ“\nâœ“\nâœ“\nâœ“\nSLAM\n\\cellcolor\n[HTML]ECF4FF\nSUDS (Ours)\n\\cellcolor\n[HTML]ECF4FFStereo\n\\cellcolor\n[HTML]ECF4FF-\n\\cellcolor\n[HTML]ECF4FF29,319\n\\cellcolor\n[HTML]ECF4FFâœ“*\n\\cellcolor\n[HTML]ECF4FFPGM\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FFâœ“\n\\cellcolor\n[HTML]ECF4FFâœ“\nFigure 3\n:\nCoordinate frames of the robot. Red indicates the\nx\nx\n-axis, green the\ny\ny\n-axis, and blue the\nz\nz\n-axis. The IMU (\nğ™¸\n\\mathtt{I}\n) and base (\nğ™±\n\\mathtt{{B}}\n) frames are coincident. The DVL (\nğ™³\n\\mathtt{D}\n) and barometer (\nğ™¿\n\\mathtt{P}\n) frames share the same frame convention and the camera frame (\nğ™²\n\\mathtt{C}\n) follows the OpenCV frame convention\n[\n42\n]\n.\nIV\nUnderwater Stereo Estimation\nThis section introduces our proposed method for learning-based underwater stereo depth estimation, including data generation, data augmentations, implementation details, and training loss.\nIV.A\nSimulated Data Generation\nMotivated by the lack of real-world underwater stereo datasets with ground truth depth, and inspired by the success of stereo disparity estimation in\n[\n76\n]\n, we leverage simulated data for network training.\nTo generate training data, we first leverage the Omniverse RTX renderer\n[\n40\n]\nto generate rendered images in an IsaacSim environment\n[\n41\n]\n, as shown in the first column of Figure\n4\n.\nObjects and scene assets are collected from Epic Fab\n[\n13\n]\nand Sketchfab\n[\n62\n]\n, totaling eight scenes.\nThese scenes are selected for their similarity in terrain properties (i.e., rocky and sandy seafloors) and structure (i.e, open water, caves).\nThe assets are selected to reflect common underwater structures, including large and small shipwrecks, rocks spanning a range of scales and textures, and marine vegetation.\nDuring the initial simulated data generation stage, the position of the stereo camera pair is randomly sampled from a pre-recorded trajectory in each scene, and a random orientation at the position is applied to ensure diversity of views of the scene.\nWith each render, the stereo pair image is generated, along with the camera intrinsics, depth image, and scene normals.\nWe note that when rendering the initial simulated images, we do not apply any underwater effects.\nWhile there has been a recent rise in GPU-accelerated simulators for the underwater domain\n[\n47\n,\n52\n,\n65\n]\n, simulated effects are limited to haze and attenuation. We also seek to model caustics, dynamic lighting, and suspended particles. Thus, we propose an augmentation pipeline that takes in simulated in-air images and applies train-time augmentations. This has the added benefit of randomizing augmentations at training time, yielding multiple configurations of underwater image parameters for each simulated image.\nIV.B\nUnderwater Data Augmentation\nThis section presents a training-time augmentation pipeline that transforms the simulated in-air stereo imagery into physically plausible underwater renderings while preserving dense disparity ground truth.\nWe explicitly model dominant components of underwater image formation and apply them as composable operators to the simulated dataset. Concretely, the pipeline comprises four augmentation modules: (1) ambient water-column illumination, (2) surface-induced caustics from refracted sunlight, (3) directional and diffuse sunlight, and (4) specular highlights from suspended sediment particles.\nThe cumulative effect of these operators on the input images is illustrated in Figure\n4\n.\nNote that since the presented augmentations only require the simulated in-air stereo images, ground-truth depths, the camera intrinsics, and the stereo extrinsics, it is broadly transferable to other in-air datasets that contain dense ground truth depth.\nIV.B.1\nWater Column Effects\nWe model water-column effects using the light transport model described in\n[\n56\n,\n60\n]\n.\nFor an in-air image,\nJ\nc\nJ_{c}\n, with depth map,\nğ³\n\\mathbf{z}\n, the simulated underwater pixel intensity,\nI\nc\nI_{c}\n, at location\nğ±\n=\n(\nu\n,\nv\n)\n\\mathbf{x}=(u,v)\nis given by\nI\nc\nâ€‹\n(\nğ±\n,\nğ³\n)\n=\nJ\nc\nâ€‹\n(\nğ±\n)\nâ‹…\nt\nc\nâ€‹\n(\nğ³\n)\n+\nB\nc\n,\nâˆ\nâ‹…\n(\n1\nâˆ’\nt\nc\nâ€‹\n(\nğ³\n)\n)\n\\displaystyle I_{c}(\\mathbf{x},\\mathbf{z})=J_{c}(\\mathbf{x})\\cdot t_{c}(\\mathbf{z})+B_{c,\\infty}\\cdot(1-t_{c}(\\mathbf{z}))\n(1)\nt\nc\nâ€‹\n(\nğ³\n)\n=\nexp\nâ¡\n(\nâˆ’\nÎ²\nc\nâ€‹\nğ³\n)\n\\displaystyle t_{c}(\\mathbf{z})=\\exp\\left(-\\beta_{c}\\mathbf{z}\\right)\n(2)\nwhere\nÎ²\nc\n\\beta_{c}\nis a per-channel attenuation coefficient for\nc\nâˆˆ\n{\nR\n,\nG\n,\nB\n}\nc\\in\\{R,G,B\\}\nand\nB\nc\n,\nâˆ\nB_{c,\\infty}\nis the per-channel veiling-light (backscatter) term at infinite range.\nWe randomly sample\nÎ²\nc\n\\beta_{c}\naccording to Jerlov water types\n[\n27\n]\n, and we sample\nB\nc\n,\nâˆ\nB_{c,\\infty}\nover a range of conditions to span diverse underwater appearances at each time we apply the augmentation.\nThe effect of this model applied to a simulated in-air image is shown in the second column of Figure\n4\n.\nIV.B.2\nCaustics\nCaustics arise from sunlight refraction at the dynamic water surface\n[\n68\n]\n; while analytic, surface-based caustic rendering is available in real-time graphics engines\n[\n65\n]\n, it is impractical to integrate such pipelines within a train-time augmentation.\nWe instead model underwater caustics via texture mapping.\nGiven an image\nJ\nc\nJ_{c}\nwith depth map\nğ³\nâ€‹\n(\nğ±\n)\n\\mathbf{z}\\left(\\mathbf{x}\\right)\n, we compute per pixel scene normals,\nğ\nâ€‹\n(\nğ±\n)\n\\mathbf{N}\\left(\\mathbf{x}\\right)\n, and apply a caustic texture map,\nT\ncaustic\nT_{\\text{caustic}}\n, under a planar light source aligned with the scene\n+\nz\n+z\n-axis with direction\nğ‹\nz\n\\mathbf{L}_{z}\n.\nThe resulting caustic intensity image is\nI\ncaustic\nâ€‹\n(\nğ±\n)\n=\nT\ncaustic\nâ€‹\n(\nğ±\n)\nâ€‹\nmax\nâ¡\n(\n0\n,\nâŸ¨\nğ‹\nz\n,\nğ\nâ€‹\n(\nğ±\n)\nâŸ©\n)\n.\nI_{\\text{caustic}}(\\mathbf{x})=T_{\\text{caustic}}(\\mathbf{x})\\max(0,\\langle\\mathbf{L}_{z},\\mathbf{N}(\\mathbf{x})\\rangle).\n(3)\nThis single-channel image is used to relight the image\nJ\nc\nJ_{c}\n.\nFor stereo pairs, we use the known stereo extrinsics to warp the texture map for consistent caustic projection.\nThe effect of the caustic simulation is shown in the third column in Figure\n4\n.\nIn practice, the normals are computed using Kaolin\n[\n25\n]\n.\nFigure 4\n:\nA demonstration of our underwater augmentation pipeline. From left to right, the first five columns demonstrate increasing levels of augmentation. The rightmost column shows the reference image that the augmentations aim to match.\nIV.B.3\nDirectional Light\nInspired by the presence of directional lighting in underwater datasets (see Table\n1\n) and by methods that explicitly exploit lighting for reconstruction\n[\n53\n]\n, we incorporate directional light simulation into our dataset generation.\nGiven the depth map,\nğ³\nâ€‹\n(\nğ±\n)\n\\mathbf{z}\\left(\\mathbf{x}\\right)\n, and its computed normals,\nğ\nâ€‹\n(\nğ±\n)\n\\mathbf{N}\\left(\\mathbf{x}\\right)\n, we place a point light source at position\np\np\nin the camera frame with pose\nP\nP\nand color\nC\nc\nC_{c}\n.\nFor a pixel at location\nğ±\n=\n(\nu\n,\nv\n)\n\\mathbf{x}=\\left(u,v\\right)\n, the light direction is\nğ‹\nâ€‹\n(\nğ±\n)\n=\np\nâˆ’\nğ³\nâ€‹\n(\nğ±\n)\nâ€‹\nK\nâˆ’\n1\nâ€‹\n[\nu\nv\n1\n]\nT\n.\n\\displaystyle\\mathbf{L}(\\mathbf{x})=p-\\mathbf{z}\\left(\\mathbf{x}\\right)K^{-1}\\begin{bmatrix}u&v&1\\end{bmatrix}^{T}.\n(4)\nwhere\nK\nK\nis the camera intrinsic matrix. The contribution of the directional light to the original image is\nI\nlight,c\nâ€‹\n(\nğ±\n)\n=\nP\nâ€‹\nC\nc\nâˆ¥\nğ‹\nâ€‹\n(\nğ±\n)\nâˆ¥\n2\nâ€‹\nmax\nâ¡\n(\n0\n,\nâŸ¨\nğ‹\nâ€‹\n(\nğ±\n)\n,\nğ\nâŸ©\n)\nâ€‹\nt\nc\nâ€‹\n(\nâˆ¥\nğ‹\nâ€‹\n(\nğ±\n)\nâˆ¥\n)\n,\n\\displaystyle I_{\\text{light,c}}(\\mathbf{x})=\\frac{PC_{c}}{\\lVert\\mathbf{L}(\\mathbf{x})\\rVert^{2}}\\max\\left(0,\\langle\\mathbf{L}(\\mathbf{x}),\\mathbf{N}\\rangle\\right)t_{c}(\\lVert\\mathbf{L}(\\mathbf{x})\\rVert),\n(5)\nwhere\nt\nc\nt_{c}\nis depth-dependent, per-channel attenuation defined in (\n2\n).\nFor a stereo pair, we keep\np\np\nfixed in the left camera frame and simulate the light contribution on the right image.\nThe effect of directional lighting is shown in the fourth column in Figure\n4\n.\nIV.B.4\nSunlight\nIn shallow-water applications, sunlight significantly affects the captured image.\nThis is evident in the real-world image shown in the bottom row of Figure\n4\n.\nEffects like crepuscular rays are prevalent in these environments, and the strong sunlight source contributes to a gradient of color change in the water column\n[\n45\n]\n.\nWe model two dominant sunlight modes purely in image space, without relying on the 3D scene geometry: planar and radial illumination.\nFor planar illumination, we impose a vertical intensity gradient in the image, with pixels near the top receiving the strongest contribution.\nThis is parametrized by the sun color,\nC\nsun\nC_{\\text{sun}}\n, the additional brightness,\nb\nplanar\nb_{\\text{planar}}\n, and saturation distance in image space,\nd\nsat\nd_{\\text{sat}}\n.\nColor and saturation changes are computed consistently with the water column parameters,\nÎ²\nc\n\\beta_{c}\nand\nB\nâˆ\nB_{\\infty}\n, used to add the water column effects.\nFor radial illumination, we define\nC\nsun\nC_{\\text{sun}}\nand a sun position,\np\nsun\np_{\\text{sun}}\n, in image coordinates; radial distance from\np\nsun\np_{\\text{sun}}\ncontrols the color and intensity change, again tied to\nÎ²\nc\n\\beta_{c}\nand\nB\nâˆ\nB_{\\infty}\n.\nSampling\np\nsun\np_{\\text{sun}}\nabove the image plane increases curvature of the halo, mimicking a sun just outside the cameraâ€™s field of view.\nThe effect of a planar sunlight on the augmented image is shown in the fourth column in Figure\n4\n.\nIV.B.5\nParticles\nSuspended particles are a common feature of underwater imagery, arising from high turbidity or sediment in the water column.\nWe model only particles located between the scene and the camera.\nTo simulate particle appearance, we apply a procedurally generated particle texture map directly in image space. For each particle texture map, we randomly place a set of ellipsoids across the image, varying in orientation and size.\nEach ellipsoidâ€™s opacity decreases radially outward from its center, simulating the motion blur experienced during image capture.\nThe resulting particle texture map is additively blended with the base image.\nThe effect of simulated particles is shown in the fifth row of Figure\n4\n.\nIV.B.6\nFull Augmentation Pipeline\nFor the full augmentation pipeline, shown graphically in Figure\n4\n, we apply all lighting effects first, then add water haze to simulate an underwater image.\nWe define a relit image\nI\nrelit\n,\nc\nI_{\\text{relit},c}\nas\nI\nrelit\n,\nc\nâ€‹\n(\nğ±\n)\n=\nJ\nc\nâ€‹\n(\nğ±\n)\nâ€‹\n(\n1\n+\nI\nlight\n,\nc\nâ€‹\n(\nğ±\n)\n)\nâ€‹\n(\n1\n+\nI\ncaustic\nâ€‹\n(\nğ±\n)\n)\n.\n\\displaystyle I_{\\text{relit},c}(\\mathbf{x})=J_{c}(\\mathbf{x})\\left(1+I_{\\text{light},c}(\\mathbf{x})\\right)\\left(1+I_{\\text{caustic}}(\\mathbf{x})\\right).\n(6)\nWe apply the water-column model to\nI\nrelit\n,\nc\nI_{\\text{relit},c}\nto ensure all illumination is subject to medium effects:\nI\nwater\n,\nc\nâ€‹\n(\nğ±\n,\nğ³\n)\n=\nI\nrelit\n,\nc\nâ€‹\nt\nc\nâ€‹\n(\nğ³\n)\n+\nB\nâˆ\nâ€‹\n(\n1\nâˆ’\nt\nc\nâ€‹\n(\nğ³\n)\n)\n.\n\\displaystyle I_{\\text{water},c}(\\mathbf{x},\\mathbf{z})=I_{\\text{relit},c}t_{c}(\\mathbf{z})+B_{\\infty}(1-t_{c}(\\mathbf{z})).\n(7)\nAs a final step, we apply the sunlight and particles directly onto\nI\nwater\nI_{\\text{water}}\n.\nThe parameters of each augmentation are randomly sampled to form a large distribution of water effects.\nIV.C\nStereo Training\nIn this section, we describe a unified training strategy for fine-tuning existing stereo depth networks in underwater environments.\nWe combine supervised objectives on our augmented synthetic data with self-supervised objectives on real underwater stereo data and apply this strategy across a range of model sizes and architectures\n[\n28\n,\n76\n]\n.\nIV.C.1\nLoss Functions\nStereo networks produce a set of refined disparity estimates\nğ\n^\n=\n(\nd\n^\n0\n,\nâ€¦\n,\nd\n^\nR\n)\n\\hat{\\mathbf{d}}=(\\hat{d}_{0},\\dots,\\hat{d}_{R})\n, where\nd\n^\nr\n\\hat{d}_{r}\nis the final prediction after\nR\nR\nrefinement steps.\nThe training loss switches between supervised (\nâ„’\nsup\n\\mathcal{L}_{\\text{sup}}\n) and self-supervised (\nâ„’\nself\n)\n\\mathcal{L}_{\\text{self}})\nlosses depending on whether the ground-truth disparity,\nd\nâˆ—\nd^{*}\n, is available. That is,\nâ„’\nâ€‹\n(\nğ\n^\n)\n=\n{\nâ„’\nsup\nâ€‹\n(\nğ\n^\n,\nd\nâˆ—\n)\nd\nâˆ—\nâ€‹\nknown\nÎ»\nself\nâ‹…\nâ„’\nself\nâ€‹\n(\nd\n^\nR\n)\notherwise\n\\mathcal{L}(\\hat{\\mathbf{d}})=\\begin{cases}\\mathcal{L}_{\\text{sup}}(\\hat{\\mathbf{d}},d^{*})&d^{*}\\text{ known}\\\\\n\\lambda_{\\text{self}}\\cdot\\mathcal{L}_{\\text{self}}(\\hat{d}_{R})&\\text{ otherwise}\\end{cases}\n(8)\nwhere\nÎ»\nself\n\\lambda_{\\text{self}}\nis a scalar hyperparameter.\nIV.C.2\nDirect Disparity Supervision\nGround-truth disparity,\nd\nâˆ—\nd^{*}\n, is available only for simulated data.\nFor training on simulated data, to avoid supervising on regions of the image severely degraded by water column effects, we use the known per-channel attenuation\nt\nc\nâ€‹\n(\nğ±\n)\n=\nexp\nâ¡\n(\nâˆ’\nÎ²\nc\nâ€‹\nğ±\n)\nt_{c}(\\mathbf{x})=\\exp(-\\beta_{c}\\mathbf{x})\nfrom our augmentation pipeline.\nWe mask out regions where\nt\nc\nâ€‹\n(\nğ±\n)\n<\nt\nm\nâ€‹\ni\nâ€‹\nn\n,\nc\nt_{c}(\\mathbf{x})<t_{min,c}\nfor any channel, retaining only pixels that carry information about the scene geometry.\nd\nâˆ—\nâ€‹\n(\nğ±\n)\n=\nd\nâˆ—\nâ€‹\n(\nğ±\n)\nâ€‹\nğŸ™\nâ€‹\n[\nt\nc\nâ€‹\n(\nğ±\n)\nâ‰¥\nt\nm\nâ€‹\ni\nâ€‹\nn\n,\nc\n]\nd^{*}(\\mathbf{x})=d^{*}(\\mathbf{x})\\,\\mathbb{1}\\!\\left[t_{c}(\\mathbf{x})\\geq t_{min,c}\\right]\n(9)\nWith this masked ground-truth, we use the same loss function as in\n[\n76\n]\n:\nâ„’\nsup\nâ€‹\n(\nğ\n^\n,\nd\nâˆ—\n)\n=\n|\nd\n^\n0\nâˆ’\nd\nâˆ—\n|\nsmooth\n+\nâˆ‘\nr\n=\n1\nR\nÎ³\nR\nâˆ’\nr\nâ€‹\nâˆ¥\nd\n^\nr\nâˆ’\nd\nâˆ—\nâˆ¥\n1\n\\displaystyle\\begin{split}\\mathcal{L}_{\\text{sup}}(\\hat{\\mathbf{d}},d^{*})=\\lvert\\hat{d}_{0}-d^{*}\\rvert_{\\text{smooth}}+\\sum_{r=1}^{R}\\gamma^{R-r}\\lVert\\hat{d}_{r}-d^{*}\\rVert_{1}\\end{split}\n(10)\nwhere\n|\nâ‹…\n|\nsmooth\n\\lvert\\cdot\\rvert_{\\text{smooth}}\ndenotes the Smooth L1 loss and\nâˆ¥\nâ‹…\nâˆ¥\n1\n\\lVert\\cdot\\rVert_{1}\ndenotes the standard L1 Loss, as described by\n[\n17\n,\n76\n]\n.\nIV.C.3\nSelf-Supervised Warping Loss\nWhen ground-truth disparity is not available, we use an alternate supervision strategy.\nFollowing several past works\n[\n69\n,\n63\n]\n, we use a self-supervising loss that uses the estimated disparity to warp each image to the frame of the other.\nThat is, given a rectified pair of images\n(\nI\nL\n,\nI\nR\n)\n\\left(I_{L},I_{R}\\right)\nalong with the rectified intrinsics,\nK\nK\n, and the stereo baseline,\nb\nb\n, each image is back-projected to 3D by converting the disparities to depths.\nThen, the 3D points are projected into the frame of the other image and interpolated to form the final image.\nWe denote this operation\nI\n^\nL\n=\nwarp\nâ€‹\n(\nI\nR\n,\nd\n)\n\\hat{I}_{L}=\\texttt{warp}(I_{R},d)\nand\nI\n^\nR\n=\nwarp\nâ€‹\n(\nI\nL\n,\nd\n)\n\\hat{I}_{R}=\\texttt{warp}(I_{L},d)\n, where the dependence on calibrations is omitted for brevity.\nIn practice, this warp operation is implemented by the open-source Kornia library\n[\n51\n]\n.\nUsing the estimated images and ground-truth, the following loss is computed:\nI\n^\nL\n=\nwarp\nâ€‹\n(\nI\nR\n,\nd\n^\nK\n)\nI\n^\nR\n=\nwarp\nâ€‹\n(\nI\nL\n,\nd\n^\nK\n)\n\\displaystyle\\hat{I}_{L}=\\texttt{warp}(I_{R},\\hat{d}_{K})\\hskip 16.0pt\\hat{I}_{R}=\\texttt{warp}(I_{L},\\hat{d}_{K})\n(11)\nâ„’\nwarp\nâ€‹\n(\nd\n^\nK\n)\n=\n1\n2\nâ€‹\n[\nâ„’\nDTD\nâ€‹\n(\nI\n^\nL\n,\nI\nL\n)\n+\nâ„’\nDTD\nâ€‹\n(\nI\n^\nR\n,\nI\nR\n)\n]\n,\n\\displaystyle\\mathcal{L}_{\\text{warp}}(\\hat{d}_{K})=\\frac{1}{2}\\bigg[\\mathcal{L}_{\\text{DTD}}(\\hat{I}_{L},I_{L})+\\mathcal{L}_{\\text{DTD}}(\\hat{I}_{R},I_{R})\\bigg],\n(12)\nwhere\nâ„’\nDTD\nâ€‹\n(\nI\n,\nI\n^\n)\n=\nÎ»\nDTD\nâ‹…\n|\nI\nâˆ’\nI\n^\n|\n1\n+\n(\n1\nâˆ’\nÎ»\nDTD\n)\nâ‹…\nSSIM\nâ€‹\n(\nI\n,\nI\n^\n)\n\\displaystyle\\begin{split}\\mathcal{L}_{\\text{DTD}}(I,\\hat{I})=&\\lambda_{\\text{DTD}}\\cdot\\lvert I-\\hat{I}\\rvert_{1}\\\\\n+&(1-\\lambda_{\\text{DTD}})\\cdot\\text{SSIM}(I,\\hat{I})\\end{split}\n(13)\nis the loss function proposed by\n[\n69\n]\n. Unlike the directly supervised loss in (\n10\n), the self-supervised loss is applied only to the final upsampled prediction\nd\n^\nK\n\\hat{d}_{K}\n.\nWhile\n[\n69\n]\napplies warping only from the right image to the left, we follow\n[\n63\n]\nand apply the loss in both directions.\nAlthough (\n12\n) is written as the average of the two losses, in practice, the warped image may contain invalid pixels at the edges (caused by parts of the scene being out of view in one camera).\nIn such cases, we keep only the valid cameraâ€™s loss and weight it twice, while discarding the invalid cameraâ€™s loss.\nIV.C.4\nOccam Regularizer\nEven though (\n12\n) provides effective self-supervision in textured regions, low-texture areas cause many disparities to produce the same warped image, especially in background regions where disparity should be zero.\nTo resolve this, we add an\nOccam regularizer\nthat favors zero disparity when it yields results similar to larger disparities.\nLet\nâ„’\nwarp\nâ€‹\n(\nd\n^\nK\n)\n\\mathcal{L}_{\\text{warp}}(\\hat{d}_{K})\nbe the photometric loss in (\n12\n) for the predicted disparity map\nd\n^\nK\n\\hat{d}_{K}\n.\nWe compute an alternative loss\nâ„’\nwarp\nâ€‹\n(\n0\n)\n\\mathcal{L}_{\\text{warp}}(0)\ncorresponding to a\nzero-disparity hypothesis\nby directly comparing the unwarped stereo pair.\nThen, we define\nÎ”\noccam\nâ€‹\n(\nd\n^\nK\n)\n=\nâ„’\nwarp\nâ€‹\n(\nd\n^\nK\n)\nâˆ’\nâ„’\nwarp\nâ€‹\n(\n0\n)\n,\n\\displaystyle\\Delta_{\\text{occam}}(\\hat{d}_{K})=\\mathcal{L}_{\\text{warp}}(\\hat{d}_{K})-\\mathcal{L}_{\\text{warp}}(0),\n(14)\nwhich measures the gain from the predicted disparity over zero disparity.\nIf the prediction is significantly worse than zero disparity, we add a penalty:\nâ„’\noccam\nâ€‹\n(\nd\n^\nK\n)\n=\nReLU\nâ€‹\n(\nÏ„\noccam\n+\nÎ”\noccam\nâ€‹\n(\nd\n^\nK\n)\n)\n,\n\\displaystyle\\mathcal{L}_{\\text{occam}}(\\hat{d}_{K})=\\texttt{ReLU}(\\tau_{\\text{occam}}+\\Delta_{\\text{occam}}(\\hat{d}_{K})),\n(15)\nwhere\nÏ„\no\nâ€‹\nc\nâ€‹\nc\nâ€‹\na\nâ€‹\nm\n\\tau_{occam}\nis a margin parameter.\nIV.C.5\nTotal Loss Function\nFinally, as in\n[\n69\n]\n, we add the edge-aware smoothness regularizer,\nâ„’\nsmooth\n\\mathcal{L}_{\\text{smooth}}\n, from\n[\n18\n]\n. Thus, the final self-supervised loss is\nâ„’\nself\nâ€‹\n(\nd\n^\nK\n)\n=\nÎ»\nwarp\nâ€‹\nâ„’\nwarp\nâ€‹\n(\nd\n^\nK\n)\n+\nÎ»\noccam\nâ€‹\nâ„’\noccam\nâ€‹\n(\nd\n^\nK\n)\n+\nÎ»\nsmooth\nâ€‹\nâ„’\nsmooth\nâ€‹\n(\nd\n^\nK\n)\n\\displaystyle\\begin{split}\\mathcal{L}_{\\text{self}}(\\hat{d}_{K})=&\\lambda_{\\text{warp}}\\mathcal{L}_{\\text{warp}}(\\hat{d}_{K})\\\\\n+&\\lambda_{\\text{occam}}\\mathcal{L}_{\\text{occam}}(\\hat{d}_{K})\\\\\n+&\\lambda_{\\text{smooth}}\\mathcal{L}_{\\text{smooth}}(\\hat{d}_{K})\\end{split}\n(16)\nwhere\nÎ»\nwarp\n,\nÎ»\noccam\n\\lambda_{\\text{warp}},\\lambda_{\\text{occam}}\nand\nÎ»\nsmooth\n\\lambda_{\\text{smooth}}\nare scalar weights determined experimentally.\nIn practice, we train the model with two stages.\nIn the first stage, we train using only simulated data for a fixed number of iterations.\nWe then fine-tune on the second stage using a mixture of simulated and real-world data.\nV\nSurfSLAM\nOur proposed algorithm SurfSLAM applies the finetuned stereo depth estimation module and tightly integrates the IMU, DVL, barometer, and stereo depth maps to estimate the robotâ€™s pose and to build a dense 3D reconstruction of the scene.\nWe base our tracking system on TURTLMap\n[\n64\n]\ndue to its metrically consistent odometry in low-texture underwater environments.\nWe treat the stereo-derived geometry as a high-level measurement used to correct drift with global registration factors while relying on acousticâ€“inertial sensing for locally consistent motion.\nFor the sake of clarity, we describe the full acoustic-inertial-stereo method below.\nV.A\nState Definition\nThe robot state at a keyframe\ni\ni\nis represented as a combination of the pose, velocity, and sensor biases:\nğ±\ni\n=\n[\nğ‘\ni\n,\nğ©\ni\n,\nğ¯\ni\n,\nğ›\ni\ng\n,\nğ›\ni\na\n,\nğ›\ni\nv\n]\nâˆˆ\nSO\nâ€‹\n(\n3\n)\nÃ—\nâ„\n15\n,\n\\mathbf{x}_{i}=\\left[\\mathbf{R}_{i},\\mathbf{p}_{i},\\mathbf{v}_{i},\\mathbf{b}_{i}^{g},\\mathbf{b}_{i}^{a},\\mathbf{b}_{i}^{v}\\right]\\in\\mathrm{SO}(3)\\times\\mathbb{R}^{15},\n(17)\nwhere\nğ‘\ni\nâˆˆ\nSO\nâ€‹\n(\n3\n)\n\\mathbf{R}_{i}\\in\\text{SO}(3)\nand\nğ©\ni\nâˆˆ\nâ„\n3\n\\mathbf{p}_{i}\\in\\mathbb{R}^{3}\nrepresent the orientation and position in the NED frame,\nğ¯\ni\nâˆˆ\nâ„\n3\n\\mathbf{v}_{i}\\in\\mathbb{R}^{3}\nrepresents the body frame linear velocity,\nğ›\ni\ng\n,\nğ›\ni\na\nâˆˆ\nâ„\n3\n\\mathbf{b}_{i}^{g},\\mathbf{b}_{i}^{a}\\in\\mathbb{R}^{3}\nare the IMU gyroscope and accelerometer biases, and\nğ›\ni\nv\nâˆˆ\nâ„\n3\n\\mathbf{b}_{i}^{v}\\in\\mathbb{R}^{3}\nis the DVL velocity bias.\nWe maintain the states over a set of keyframes\nğ’³\nn\n=\n{\nğ±\ni\n}\ni\nâˆˆ\nğ–ª\nn\n,\n\\mathcal{X}_{n}=\\{\\mathbf{x}_{i}\\}_{i\\in\\mathsf{K}_{n}},\n(18)\nwhere\nğ–ª\nn\n\\mathsf{K}_{n}\nrepresents the keyframes up until time\nt\nn\nt_{n}\n.\nV.B\nMeasurement Definitions\nWe consider sensor measurements from an IMU, a DVL, and a barometer between two consecutive keyframes\ni\ni\nand\nj\nj\n.\nFor a stereo camera, we consider the measurements between a query keyframe at time\nq\nq\nand a matched keyframe at time\nm\nm\n.\nThe depth estimates from the stereo camera are denoted\nğ’®\ni\nâ€‹\nj\n\\mathcal{S}_{ij}\n.\nThe IMU measurements are denoted as\nâ„\ni\nâ€‹\nj\n\\mathcal{I}_{ij}\n, and are composed of a set of angular velocity\nÏ‰\n~\nâˆˆ\nâ„\n3\n\\tilde{\\mathbf{\\omega}}\\in\\mathbb{R}^{3}\nand linear acceleration\nğš\n~\nâˆˆ\nâ„\n3\n\\tilde{\\mathbf{a}}\\in\\mathbb{R}^{3}\nmeasurements.\nThe DVL measurements, denoted\nğ’Ÿ\ni\nâ€‹\nj\n\\mathcal{D}_{ij}\n, consist of a linear velocity\nğ¯\n~\nâˆˆ\nâ„\n3\n\\tilde{\\mathbf{v}}\\in\\mathbb{R}^{3}\nmeasurement, and the barometer measurement, denoted as\nP\ni\nâ€‹\nj\nP_{ij}\nis composed of a pressure measurement,\nz\n~\n\\tilde{z}\n.\nThe full set of measurements is denoted\nğ’µ\nn\n=\n{\nğ’®\nq\nâ€‹\nm\n,\nâ„\ni\nâ€‹\nj\n,\nğ’Ÿ\ni\nâ€‹\nj\n,\nğ’«\ni\nâ€‹\nj\n}\ni\n,\nj\n,\nq\n,\nm\nâˆˆ\nğ–ª\nn\n.\n\\mathcal{Z}_{n}=\\{\\mathcal{S}_{qm},\\mathcal{I}_{ij},\\mathcal{D}_{ij},\\mathcal{P}_{ij}\\}_{i,j,q,m\\in\\mathsf{K}_{n}}.\n(19)\nV.C\nAcoustic-Inertial Pose Graph Formulation\nWe follow the formulation presented in\n[\n64\n]\nfor acoustic-inertial odometry.\nWe denote the subset of acoustic-inertial measurements as\nğ’µ\nÂ¯\nk\n\\underline{\\mathcal{Z}}_{k}\nconsisting of the IMU, DVL and barometer until time\nt\nk\nt_{k}\n:\nğ’µ\nÂ¯\nk\n=\n{\nâ„\ni\nâ€‹\nj\n,\nğ’Ÿ\ni\nâ€‹\nj\n,\nğ’«\ni\nâ€‹\nj\n}\ni\n,\nj\nâˆˆ\nğ–ª\nk\n.\n\\underline{\\mathcal{Z}}_{k}=\\{\\mathcal{I}_{ij},\\mathcal{D}_{ij},\\mathcal{P}_{ij}\\}_{i,j\\in\\mathsf{K}_{k}}.\n(20)\nThe estimator optimizes state estimates to maximize the posterior distribution given the measurements\nğ’µ\nÂ¯\nk\n\\underline{\\mathcal{Z}}_{k}\nand a prior distribution over the states\nğ’³\n0\n\\mathcal{X}_{0}\n:\nğ’³\nk\nâˆ—\n\\displaystyle\\mathcal{X}_{k}^{*}\n=\nargmax\nğ’³\nk\np\nâ€‹\n(\nğ’³\nk\n|\nğ’µ\nÂ¯\nk\n)\n\\displaystyle=\\operatorname*{argmax}_{\\mathcal{X}_{k}}p\\left(\\mathcal{X}_{k}|\\underline{\\mathcal{Z}}_{k}\\right)\n(21)\nâˆ\nargmax\nğ’³\nk\np\nâ€‹\n(\nğ’³\n0\n)\nâ€‹\np\nâ€‹\n(\nğ’µ\nÂ¯\nk\n|\nğ’³\nk\n)\n\\displaystyle\\propto\\operatorname*{argmax}_{\\mathcal{X}_{k}}p\\left(\\mathcal{X}_{0}\\right)p(\\underline{\\mathcal{Z}}_{k}|\\mathcal{X}_{k})\n(22)\nWe assume that the sensor measurements are conditionally independent and have zero-mean Gaussian noise.\nFollowing this, we can transform the objective function into a weighted least squares problem by minimizing the log-likelihood of each measurement and the prior over the states, weighted by their associated uncertainties.\nThis objective is given as\nğ’³\nk\nâˆ—\n=\n\\displaystyle\\mathcal{X}^{*}_{k}=\nargmin\nğ’³\nk\nâˆ¥\nğ«\n0\nâˆ¥\nÎ£\n0\n2\n\\displaystyle\\operatorname*{argmin}_{\\mathcal{X}_{k}}\\lVert\\mathbf{r}_{0}\\rVert^{2}_{\\Sigma_{0}}\n+\nâˆ‘\ni\n,\nj\nâˆˆ\nğ–ª\nk\n(\nâˆ¥\nğ«\nâ„\ni\nâ€‹\nj\nâˆ¥\nÎ£\nâ„\ni\nâ€‹\nj\n2\n+\nâˆ¥\nğ«\nğ’Ÿ\ni\nâ€‹\nj\nâˆ¥\nÎ£\nğ’Ÿ\ni\nâ€‹\nj\n2\n+\nâˆ¥\nğ«\nğ’«\ni\nâ€‹\nj\nâˆ¥\nÎ£\nğ’«\ni\nâ€‹\nj\n2\n)\n,\n\\displaystyle+\\sum_{i,j\\in\\mathsf{K}_{k}}\\left(\\lVert\\mathbf{r}_{\\mathcal{I}_{ij}}\\rVert^{2}_{\\Sigma_{\\mathcal{I}_{ij}}}+\\lVert\\mathbf{r}_{\\mathcal{D}_{ij}}\\rVert^{2}_{\\Sigma_{\\mathcal{D}_{ij}}}+\\lVert\\mathbf{r}_{\\mathcal{P}_{ij}}\\rVert^{2}_{\\Sigma_{\\mathcal{P}_{ij}}}\\right),\n(23)\nwhere\nâˆ¥\nâ‹…\nâˆ¥\nÎ£\n2\n\\lVert\\cdot\\rVert^{2}_{\\Sigma}\ndenotes the Mahalanobis distance with covariance\nÎ£\n\\Sigma\n, and\nğ«\n\\mathbf{r}\ndenotes the residual associated with the factor for the measurements\nğ’µ\nÂ¯\nk\n\\underline{\\mathcal{Z}}_{k}\nand the prior belief\nğ’³\n0\n\\mathcal{X}_{0}\n.\nIn practice, this optimizer is solved iteratively as measurements arrive using iSAM2\n[\n29\n]\n.\nThe residuals and noise models for the sensors are derived in\n[\n67\n,\n15\n,\n64\n]\n.\nV.D\nGlobal Registration\nThis section describes how the tracking method from\n[\n64\n]\nis augmented with global registration.\nGlobal registration is performed in three steps: (1) global place recognition, (2) visual feature matching, and (3) geometric alignment. Each step is described below.\nV.D.1\nGlobal Place Recognition\nLet\nğ–ª\nq\n\\mathsf{K}_{q}\ndenote the set of keyframes up to a time\nt\nq\nâ‰¤\nt\nk\nt_{q}\\leq t_{k}\n, and let\nk\nq\nâˆˆ\nğ–ª\nq\nk_{q}\\in\\mathsf{K}_{q}\nbe the\nq\nt\nâ€‹\nh\nq^{th}\nkeyframe that is used to query the place recognition system.\nThe left image of\nk\nq\nk_{q}\n, denoted\nI\nq\n,\nl\nI_{q,l}\n, is first histogram equalized using Contrast-Limited Adaptive Histogram Equalization (CLAHE)\n[\n46\n]\n. Then, SuperPoint features (as proposed and trained by\n[\n10\n,\n54\n]\n) are extracted from the equalized image.\nThe local feature descriptors from the SuperPoint features are accumulated into an image-level descriptor using the Vector of Locally Aggregated Descriptors (VLAD) method\n[\n26\n]\n.\nIn practice, the K-nearest neighbors process within VLAD is implemented via\n[\n12\n]\n.\nThe image-level descriptors computed by VLAD are compared with other keyframes that are at least 20 seconds old.\nThe top three most similar images are retrieved and passed to the visual feature matching step.\nV.D.2\nVisual Feature Matching\nWe denote a matched image candidate as\nI\nm\n,\nl\nI_{m,l}\n.\nInitial feature matching is performed using SuperGlue\n[\n54\n]\n.\nOutliers are rejected by computing the essential matrix between the query and match images using RANSAC.\nIf RANSAC estimates sufficient inliers, the match candidate proceeds to the geometric validation step.\nV.D.3\nGeometric Validation\nUsing the stereo estimation described in\nIV\n, a depth image is computed for both the query and matched images.\nUsing these depth images, the filtered 2D correspondences estimated by SuperGlue are lifted to 3D.\nThese 3D feature correspondences are used to estimate a 3D transformation,\nğ“\nq\nâ€‹\nm\nC\n\\mathbf{T}_{qm}^{C}\n, between the two keyframes.\nThis is again performed using RANSAC to reject poor correspondences. If insufficient inliers are detected, the registration is rejected.\nOtherwise, the RANSAC-estimated transform is refined using GICP\n[\n59\n]\non the full point clouds estimated by the stereo depth estimation network, yielding a final geometric relative pose measurement\nğ“\n~\nq\nâ€‹\nm\nC\nâˆˆ\nSE\nâ€‹\n(\n3\n)\n\\tilde{\\mathbf{T}}_{qm}^{C}\\in\\mathrm{SE}(3)\n, which we simply denote as the stereo measurement,\nğ’®\nq\nâ€‹\nm\n\\mathcal{S}_{qm}\n.\nWhile global place recognition produces three candidate frames, at most one is passed to the optimizer. That is, the first match candidate to survive the above heuristics is passed to the pose graph, while any remaining candidates are discarded.\nV.D.4\nPose Graph Formulation\nWe incorporate\nğ’®\nq\nâ€‹\nm\n\\mathcal{S}_{qm}\nas a factor added to the acoustic-inertial tracking to constrain the relative motion between the keyframes.\nBefore adding the registration to the pose graph, a final probabilistic test is performed.\nA stereo factor,\nğ’®\nq\nâ€‹\nm\n\\mathcal{S}_{qm}\n, is added to the graph after keyframes at time\nt\nq\nt_{q}\nand\nt\nm\nt_{m}\nhave been optimized using the acoustic-inertial factors described in Section\nV.B\n.\nAs a result, the tracking system can be queried for a belief over\nT\nq\nâ€‹\nm\nC\nT^{C}_{qm}\n, including mean\nÎ¼\nT\nq\nâ€‹\nm\nC\n\\mu_{T^{C}_{qm}}\nand marginal covariance\nÎ£\nT\nq\nâ€‹\nm\nC\n\\Sigma_{T^{C}_{qm}}\n.\nWe compute the Mahalanobis distance between the stereo registration result and the trackerâ€™s estimate according to\nd\nq\nâ€‹\nm\n=\nâ€–\nT\nq\nâ€‹\nm\nC\nâˆ’\nÎ¼\nT\nq\nâ€‹\nm\nC\nâ€–\nÎ£\nT\nq\nâ€‹\nm\nC\n2\nd_{qm}=\\left\\|\\,T^{C}_{qm}-\\mu_{T^{C}_{qm}}\\right\\|_{\\Sigma_{T^{C}_{qm}}}^{2}\n(24)\nRegistrations with\nd\nq\nâ€‹\nm\n>\n2.5\nd_{qm}>2.5\nare treated as outliers and discarded; only those satisfying\nd\nq\nâ€‹\nm\nâ‰¤\n2.5\nd_{qm}\\leq 2.5\nare added to the pose graph as stereo factors\nğ’®\nq\nâ€‹\nm\n\\mathcal{S}_{qm}\n.\nIn practice, each of the outlier rejection methods is tuned to add as many registrations to the factor graph as possible.\nTo deal with the noisy estimates resulting from this strategy, registration factors are weighted by a robust Huber kernel\n[\n23\n]\nin the backend to reject incorrect registrations that survive the outlier rejection process.\nThe final objective becomes\nğ’³\nk\nâˆ—\n=\n\\displaystyle\\mathcal{X}^{*}_{k}=\nargmin\nğ’³\nk\nâˆ¥\nğ«\n0\nâˆ¥\nÎ£\n0\n2\n\\displaystyle\\operatorname*{argmin}_{\\mathcal{X}_{k}}\\lVert\\mathbf{r}_{0}\\rVert^{2}_{\\Sigma_{0}}\n+\nâˆ‘\ni\n,\nj\nâˆˆ\nğ–ª\nk\n(\nâˆ¥\nğ«\nâ„\ni\nâ€‹\nj\nâˆ¥\nÎ£\nâ„\ni\nâ€‹\nj\n2\n+\nâˆ¥\nğ«\nğ’Ÿ\ni\nâ€‹\nj\nâˆ¥\nÎ£\nğ’Ÿ\ni\nâ€‹\nj\n2\n+\nâˆ¥\nğ«\nğ’«\ni\nâ€‹\nj\nâˆ¥\nÎ£\nğ’«\ni\nâ€‹\nj\n2\n)\n\\displaystyle+\\sum_{i,j\\in\\mathsf{K}_{k}}\\left(\\lVert\\mathbf{r}_{\\mathcal{I}_{ij}}\\rVert^{2}_{\\Sigma_{\\mathcal{I}_{ij}}}+\\lVert\\mathbf{r}_{\\mathcal{D}_{ij}}\\rVert^{2}_{\\Sigma_{\\mathcal{D}_{ij}}}+\\lVert\\mathbf{r}_{\\mathcal{P}_{ij}}\\rVert^{2}_{\\Sigma_{\\mathcal{P}_{ij}}}\\right)\n(25)\n+\nâˆ‘\nq\n,\nm\nâˆˆ\nğ–ª\nq\nÏ\nâ€‹\n(\nâˆ¥\nğ«\nğ’®\nq\nâ€‹\nm\nâˆ¥\nÎ£\nğ’®\nq\nâ€‹\nm\n2\n)\n,\n\\displaystyle+\\sum_{q,m\\in\\mathsf{K}_{q}}\\rho\\left(\\lVert\\mathbf{r}_{\\mathcal{S}_{qm}}\\rVert^{2}_{\\Sigma_{\\mathcal{S}_{qm}}}\\right),\n(26)\nwhere the stereo pose registration residual\nğ«\nğ’®\nq\nâ€‹\nm\n\\mathbf{r}_{\\mathcal{S}_{qm}}\nis defined as\nğ«\nğ’®\nq\nâ€‹\nm\n=\nLog\nâ€‹\n(\nT\n~\nq\nâ€‹\nm\nâˆ’\n1\nâ€‹\nT\nÂ¯\nq\nâ€‹\nm\n)\nâˆˆ\nğ”°\nâ€‹\nğ”¢\nâ€‹\n(\n3\n)\n\\mathbf{r}_{\\mathcal{S}_{qm}}=\\text{Log}\\left(\\tilde{T}_{qm}^{-1}\\bar{T}_{qm}\\right)\\in\\mathfrak{se}(3)\n(27)\nwhere\nT\n~\nq\nâ€‹\nm\n\\tilde{T}_{qm}\nrepresents the pose measurement from the GICP estimate,\nT\nÂ¯\nq\nâ€‹\nm\n\\bar{T}_{qm}\nrepresents the estimated relative pose,\nLog\nrepresents the mapping from the Lie group\nSE\nâ€‹\n(\n3\n)\n\\mathrm{SE}(3)\nto its Lie algebra\nğ”°\nâ€‹\nğ”¢\nâ€‹\n(\n3\n)\n\\mathfrak{se}(3)\nand\nÏ\n\\rho\ndenotes the robust Huber kernel.\nV.E\nMapping\nFor mapping, we adopt a similar approach to that of\n[\n66\n]\nand maintain the stereo depth at each keyframe as a map frame primitive.\nEach incoming keyframe contributes a depth image that is back-projected into a local point cloud using the calibrated stereo intrinsics and extrinsics.\nThese keyframe point sets are accumulated to form an explicit geometric reconstruction of the environment.\nTo address noise introduced to the pointcloud by depth discontinuities, we apply a lightweight, GPU-based outlier rejection and noise cleaning step prior to the map frame construction.\nSpecifically, we apply an edge cleaning, followed by a statistical outlier removal and radius outlier removal step.\nThis results in a map with minimal noise and errors and maintains the global registrations that anchor the pose during the tracking phase of SurfSLAM.\nVI\nExperiments\nVI.A\nData Collection and Generation\nReal-world underwater stereo data was collected at the Thunder Bay National Marine Sanctuary in Alpena, MI, with multiple shipwreck sites and in an outdoor test tank across multiple days.\nMore details of the field sites will be included upon completion of the anonymous review.\nThe data collection platform is a human-operated BlueROV2 with a forward-facing stereo camera.\nField sequences span depths from 4 to 20 m and rely on natural illumination; additional sequences were acquired around an artificial rock target in an outdoor pool (both day and night), where the BlueROV2 lights provide additional lighting as needed.\nThe dataset exhibits a broad range of turbidity, caustics, lighting, and suspended particulates (see the rightmost column of Â Figure\n4\nand leftmost column of Figure\n5\n).\nFigure 5\n:\nAn example from three of the nine stereo sequences is shown. The first column shows the left rectified image; the second column shows the disparity estimated from our metric photogrammetry pipeline; the last column shows the manually-annotated foreground mask overlaid on the left image. The disparity color maps are so that lighter (yellow) is higher disparity and darker (purple) is lower disparity, with black being zero disparity.\nFigure 6\n:\nA visualization of a subset of the ground truth metric reconstructions from the stereo (first row) and SLAM sequences (second row) used for evaluations.\nThe SLAM sequences show the ground-truth poses as red camera frustums and a sparse COLMAP reconstruction.\nThe Engine SLAM sequence, not pictured, is a second sequence around the right-hand side of the Long reconstruction.\nOur collected dataset, which we call\nS\ntereo\nU\nnderwater\nD\nataset for\nS\nhipwrecks (\nSUDS\n) contains three subsets: (i) stereo evaluation, (ii) stereo training, and (iii) SLAM evaluation.\nOur simulation dataset, which we call\nU\nnder\nW\nater\nSim\nulation (\nUWSim\n) contains simulated data of realistic underwater scenes generated as described in Section\nIV.A\n, and is used for training our stereo networks.\nThis differs from alternative datasets used in underwater stereo training, which use either simulated images from non-underwater images or lack the visual properties of underwater sequences.\nSLAM evaluation comprises three sequences on a single shipwreck under varying haze, texture, and geometry.\nThe dataset statistics are provided in Table\n1\n.\nVI.A.1\nReal-world Stereo Dataset\nThe real-world stereo training dataset consists of twenty-two real-world sequences without accompanying ground-truth data, intended for self-supervision, with detailed statistics in Table\n1\n.\nThis dataset is composed of a variety of underwater lighting conditions, turbidity levels, and distractors.\nIn addition to the stereo images, this dataset also consists of the stereo intrinsic and extrinsic parameters, making the dataset suitable for self-supervised training, as described in Section\nIV.C.3\n.\nThe SUDS stereo evaluation dataset comprises nine sequences across five shipwreck sites, totaling 1,468 stereo pairs with labeled ground-truth disparity, poses, and 3D reconstruction.\nThe ground truth for the stereo evaluation dataset was obtained through a metric photogrammetry pipeline.\nWe provide stereo intrinsic and extrinsic parameters, and per-frame depth maps through the metric photogrammetry pipeline.\nReconstructions of four sequences are shown in the first row of Figure\n6\n.\nWe note that a key evaluation metric is to assess the success of stereo depth estimation methods in the water column.\nStereo depth estimation methods trained in air tend to mistake the water column for scene geometry.\nTo assess the ability of stereo networks to reject the background, we manually annotated each image with foreground-background masks.\nThese manual annotations were augmented with automated ground plane annotations.\nTo reconstruct the ground plane, the distance to the ground plane was computed for each camera pose using the DVLâ€™s altitude measurements from its four beams.\nA plane was fit to these ground measurements and projected into each camera view.\nThe foreground-background masks are used only for evaluation and reporting metrics, and are never used in training.\nSample outputs from the evaluation dataset are shown inÂ Figure\n5\n.\nAll stereo evaluations are conducted on our real-world dataset.\nVI.A.2\nReal-world SLAM Datasets\nWith SUDS, we provide three sequences for evaluating SurfSLAM.\nEach sequence includes 1080p stereo video at 30 fps with time-synchronized IMU, DVL, and barometer measurements.\nThe IMU is recorded at 200 Hz, the DVL at 8 HZ and the barometer at 5 Hz.\nGround truth is generated by taking a downsampled 5 fps version of the 1080p stereo video and running the metric photogrammetry pipeline, detailed in Section\nVI.B.1\n.\nThis ensured that even in regions where there is low texture, the photogrammetry would track the camera poses and yield a near-full trajectory of the robot with minimal dropouts.\nTo the extent of our knowledge, the SLAM sequence of the SUDS dataset presents the first publicly released in-the-wild dataset with ground truth trajectory and maps for a robot equipped with an IMU, DVL, barometer, and stereo camera.\nWe refer to these three scenes as\nBoiler\n,\nEngine\n, and\nLong\n.\nA reconstruction of the\nBoiler\nand\nLong\nscenes, along with the ground truth camera trajectories are shown in the second row of Figure\n6\n.\nThe\nBoiler\nsequence is a 180-second-long trajectory circling a large boiler followed by a short traversal over it.\nThe\nEngine\nsequence is a 284-second-long run alongside the engine section of a shipwreck, transitioning from motion along the engine to lateral passes in front of the stern while inspecting the propeller.\nOf the three sequences, the Engine sequence is the most consistently visually rich sequence.\nThe\nLong\nsequence is a 550-second-long survey spanning long-distances along the shipwreck, including extended intervals over low-texture regions observing the terrain.\nVI.B\nGenerating Ground Truth for Real Data\nFor data collected in the field, ground truth is generated using a metric photogrammetry pipeline.\nVI.B.1\nMetric Photogrammetry Pipeline\nWe follow a two-step approach to obtain accurate metric reconstructions, camera poses, intrinsic calibration, and extrinsic calibration of both the stereo pair and IMU.\nFirst, structure-from-motion (SfM) is run on sequences of uncalibrated stereo images using COLMAP\n[\n57\n,\n58\n]\n.\nAn approximate intrinsic calibration produced by Kalibr\n[\n50\n]\nis used as an initial guess for COLMAP.\nWe then apply a custom optimizer to resolve the metric scale of the scene.\nThe formulation of this metric optimization differs between sequences used for SLAM and those used for stereo evaluation.\nVI.B.2\nStereo-IMU SLAM Metric Optimization\nTo recover the metric scale of the scene, we develop a custom optimizer.\nKalibr\n[\n50\n]\nis first used to estimate initial camera intrinsics and stereo-IMU extrinsics.\nFeature tracks are then exported from COLMAP and imported into a factor graph optimizer.\nIMU data is recovered from data logs and temporally associated with the corresponding images.\nFollowing an implementation closely based on\n[\n8\n]\n, map points are marginalized while camera poses, intrinsics, and extrinsics are jointly optimized.\nThis optimization is solved in batch over all SLAM sequences to enforce a consistent calibration.\nAfter batch optimization, the intrinsic and extrinsic calibrations are fixed, and camera poses and map points are refined on a per-sequence basis.\nDuring per-sequence refinement, map points are explicitly treated as optimization variables to allow robust kernels to downweight poor correspondences from COLMAP.\nVI.B.3\nStereo-Only Stereo Metric Optimization\nFor stereo-only sequences, no IMU data is available due to equipment failure during field testing.\nAs a result, the scene scale cannot be directly optimized when the stereo baseline is unknown.\nIn this case, the stereo baseline is fixed to the value estimated by Kalibr.\nWithout IMU factors, a two-stage refinement is unnecessary.\nInstead, camera poses, intrinsics, stereo extrinsics, and map points are jointly optimized to minimize reprojection error.\nThis optimization is performed in batch across all stereo sequences.\nVI.C\nStereo Training Details\nWe provide details on the training done for fine-tuning existing stereo networks for underwater stereo depth prediction.\nVI.C.1\nTraining Datasets\nDatasets For Supervised Training\nSupervised pretraining uses three simulated stereo datasets: UWSim (ours), TartanAir, and FlyingThings3D\n[\n75\n,\n38\n]\n.\nWe apply the full underwater augmentation pipeline only to UWSim and TartanAir, and keep FlyingThings3D unaugmented to preserve a reference domain.\nThis reduces the effective domain shift introduced by augmentations and helps prevent optimization instabilities that can otherwise lead to divergence and degraded accuracy in model performance.\nDatasets for Self-Supervised Training\nFor self-supervised training, we utilize real-world SUDS sequences in conjunction with additional in-the-wild underwater stereo data from SVIn2 and UWslam (Lizard Island), leveraging the provided calibrated stereo intrinsic and extrinsic parameters\n[\n48\n,\n6\n]\n.\nThese sequences include broad viewpoint variation and diverse water and scene conditions, expanding the coverage of underwater appearance distribution used during the self-supervised finetuning.\nTable 2:\nTraining parameters for the model finetuned for underwater stereo disparity estimation.\nParameter\nDescription\nValue\nLearning Rate\nOptimizer step size\n1\nâ€‹\ne\nâˆ’\n5\n1\\mathrm{e}{-5}\nÎ»\noccam\n\\lambda_{\\text{occam}}\nOccam loss weight\n1.0\nÏ„\noccam\n\\tau_{\\text{occam}}\nOccam margin\n0.01\nÎ»\nsmooth\n\\lambda_{\\text{smooth}}\nSmoothness weight\n0.005\nÎ»\nwarp\n\\lambda_{\\text{warp}}\nPhotometric loss weight\n10.0\nt\nmin\n,\nc\nt_{\\text{min},c}\nVisibility threshold\n0.05\nVI.C.2\nModel Architecture\nWe finetune three different models in our stereo training: FoundationStereo, DEFOM with VIT-L weights, and DEFOM with VIT-S weights.\nWe elect to finetune architectures with a transformer backbone, as it has been widely shown that with large-scale datasets with heavy augmentation, vitiion transformer models (ViTs) scale better compared to alternative architectures\n[\n11\n,\n9\n]\n.\nThe selected models include both ViT-L and ViT-S weights, with the ViT-L weights targeted towards more accurate, offline tasks and the ViT-S weights targeted for real-time stereo depth estimation, which we show can be used for real-time SLAM.\nThe parameters used to finetune each of the three models are shown in Table\n2\n.\nVI.D\nBaselines\nWe evaluate the stereo disparity estimation and SLAM accuracy separately against different sets of baselines.\nVI.D.1\nStereo Baselines\nWe evaluate the proposed method against state-of-the-art stereo depth estimation networks.\nThese include FoundationStereo\n[\n76\n]\n, DEFOM-Stereo\n[\n28\n]\n, IGEV++ Stereo\n[\n78\n]\nand RAFT-Stereo\n[\n34\n]\n.\nAdditionally, we compare our performance with a stereo depth estimation method developed for the underwater domain, trained on data augmented with water column effects only\n[\n85\n]\n.\nFor models with multiple weights, we report the best observed performance.\n\\useunder\n\\ul\nTable 3\n:\nWe separate stereo evaluations into Large Models with ViT-L backbones and Small Models that use ViT-S or no vision transformer. In both cases, â€˜Oursâ€™ refers to DEFOM with our simulation and self-supervised training. EPE (End Point Error) is measured in pixels. BP-1.0 and D1 are described in Section\nVI.E.1\n. Lower is better for all metrics.\nModel\nCombined\nOn Geometry Only\nWater Column Only\nMethod\nParams. (M)\nEPE\nBP-1.0\nD1\nEPE\nBP-1.0\nD1\nEPE\nBP-1.0\nD1\nLarge\nModels\nFoundationStereo (ViT-L)\n[\n76\n]\n39.20\n13.34\n67.77\n\\ul\n23.59\n1.89\n59.48\n3.86\n56.57\n\\ul\n99.70\n\\ul\n99.15\nDEFOM (ViT-L)\n[\n28\n]\n47.30\n\\ul\n5.57\n\\ul\n62.37\n23.98\n\\ul\n2.04\n52.78\n\\ul\n4.17\n\\ul\n18.21\n99.86\n99.69\nOurs\n47.30\n1.99\n46.51\n4.85\n2.06\n\\ul\n56.34\n4.47\n1.88\n8.78\n7.76\nSmall\nModels\nIGEV++\n[\n78\n]\n14.53\n10.54\n66.03\n24.09\n\\ul\n2.08\n58.11\n4.57\n44.15\n99.75\n99.32\nFoundationStereo (ViT-S)\n[\n76\n]\n37.55\n12.96\n71.18\n24.03\n2.00\n64.40\n4.72\n57.00\n99.63\n98.91\nUnderwaterStereo\n[\n85\n]\n2.97\n14.93\n67.64\n28.33\n7.93\n59.62\n9.35\n41.64\n99.55\n98.63\nDEFOM (ViT-S)\n[\n28\n]\n18.51\n\\ul\n3.74\n\\ul\n63.21\n\\ul\n18.71\n2.21\n56.98\n4.22\n\\ul\n7.87\n\\ul\n81.88\n\\ul\n64.45\nOurs\n18.51\n2.06\n48.85\n5.05\n2.09\n\\ul\n57.73\n4.22\n2.40\n20.39\n11.17\nVI.D.2\nSLAM Baselines\nFor our SLAM evaluation, we compare against existing state-of-the-art methods in VIO and SLAM, including SVIn2\n[\n48\n]\n, a method developed for underwater state estimation, and ORB-SLAM3\n[\n7\n]\n.\nFor SVIn2, we compare the maps produced by the method coupled with\n[\n73\n]\n.\nWe additionally compare our tracking and mapping against Droid-SLAM\n[\n66\n]\n, MASt3R-SLAM\n[\n39\n]\n, and VGGT-SLAM\n[\n36\n]\n, three representative methods for learning-based visual SLAM.\nFinally, SLAM results are compared against TURTLMap\n[\n64\n]\nas a baseline acoustic-inertial fusion method.\nVI.E\nEvaluation Metrics\nVI.E.1\nDisparity Metrics\nFor disparity evaluation, we use the metrics from FoundationStereo\n[\n76\n]\n.\nWe report the end-point error (EPE), defined as the average per-pixel disparity error, and BP-X, the percentage of pixels whose predicted disparity deviates from the ground truth by more than X pixels.\nIn addition, we report the D1 metric, which measures the percentage of pixels with disparity errors exceeding both 3 pixels and 5% of the ground-truth disparity.\nStereo metrics are decomposed into three regions using the manually annotated foreground/background masks.\nCombined\nconsiders all pixels,\nOn Geometry\nconsiders pixels with valid ground-truth disparity from the photogrammetry pipeline, and\nWater Column\nconsiders the manually labeled water-column pixels where the correct disparity is zero.\nIn all configurations, pixels labeled as foreground that lack a corresponding ground-truth depth are considered holes in the ground truth. As a result, these pixels are excluded from all metrics.\nVI.E.2\nTracking Metrics\nThe absolute pose error (APE) is measured between the reference and estimated trajectories.\nWe use the popular Evo package\n[\n19\n]\nto compute the tracking metrics.\nSince MASt3R-SLAM and VGGT-SLAM are monocular methods, we compute trajectory metrics for these methods up to scale.\nFigure 7\n:\nQualitative comparison of the small models, including those based on the Small Vision Transformer (ViT-S) or without any transformer backbone. Ours is a fine-tuned DEFOM ViT-S, whereas Underwater Stereo and IGEV++ are both convolutional architectures. Of the baseline methods, Underwater Stereo is the only one to be trained on (simulated) underwater images.\nFigure 8\n:\nQualitative results of the Large Vision Transformer (ViT-L) models. Ours, based on DEFOM ViT-L, balances strong on-geometry performance with effective background removal.\nVI.E.3\nMapping Metrics\nWe follow the procedure from\n[\n24\n]\nto evaluate maps.\nEach map is first downsampled to a 5cm resolution.\nThen, the accuracy, completion, precision, and recall are computed.\nAccuracy measures the mean distance from each point in the estimate to each point in the ground truth: a high accuracy value indicates a large number of points that are far from the ground truth.\nCompletion is the complementary measure that evaluates the mean distance from each ground truth point to each point in the estimate: a high completion value indicates missing portions of the reconstruction.\nPrecision computes the proportion of points in the estimate that are within 0.1m of a point in the ground truth.\nFinally, recall computes the proportion of points in the ground truth where there is a point in the estimate within 0.1m.\nVI.F\nComputation Details\nDifferent compute systems were used for stereo training, stereo inference, metric ground truth reconstructions, and SLAM experiments.\nThe compute- and memory-intensive ground-truth optimizations were performed on a server equipped with dual AMD Epyc 7742 64-core CPUs and 4 TB of RAM.\nThe stereo networks were fine-tuned on a system with an AMD Threadripper Pro 7975WX 32-core CPU and four Nvidia Blackwell Pro 6000 GPUs.\nInference was performed on a separate machine with a single Nvidia Blackwell Pro 6000.\nAll SLAM baselines were run on a system with an AMD Ryzen 5950X CPU and NVidia RTX A6000 GPU.\nOur SLAM method was evaluated both on that desktop machine and on an Nvidia Jetson Thor platform, which is suitable for deployment in the field.\nVII\nStereo Evaluation Results\nTable\n3\nprovides a quantitative comparison of our trained models against baseline methods for stereo depth estimation, where we report the best performing models from the three that we finetuned.\nWe compare our models against three different model classifications: models with a ViT-L backbone\n[\n76\n,\n28\n]\n, ViT-S backbone\n[\n76\n,\n28\n]\n, and models that lack a ViT component\n[\n78\n,\n85\n]\n.\nNote that our method uses a DEFOM-Stereo architecture (large and small models) with our proposed train-time augmentations and loss function, as it was the best performing model from our finetuning.\nIn quantitative evaluations, the most drastic improvement seen in our proposed model is the ability to correctly identify the water column in the depth prediction, as noted in the\nWater Column Only\nevaluation.\nDespite the slight deterioration in performance on predicting depth on the geometry, the\nCombined\nmetric, which encapsulates the overall performance of each model, demonstrates that our proposed training improves stereo depth estimation in underwater environments.\nFigure\n7\nshows qualitative comparisons of the small models and Figure\n8\nshows qualitative comparisons of the large models.\nThe qualitative results demonstrate that existing stereo depth estimation methods fail to transfer to underwater settings, producing either noisy depth estimates or systematic errors such as failure to estimate depth in the water column.\nOur model is able to remove the water column from challenging images across different water column types and haze levels.\nVII.A\nAblation Study on Stereo Depth Estimation\nWe perform an ablation over the key components of the proposed stereo network finetuning.\nThe evaluation of the ablation is done by comparing the EPE on the\nOn Geometry\n,\nWater Column\n, and\nCombined\nmetrics.\nThe ablation results are shown in Table\n4\n.\nEach ablation removes a single component while keeping all others fixed, allowing isolation of its effect on geometry estimation and water-column estimation.\nThe ablation reveals three distinct effects.\nThe full augmentations (FA) are necessary for reducing predictions in the water column.\nThe self-supervised warping loss (WL) primarily improves depth accuracy on observed geometry, while inclusion of in-air data (IA) prevents degradation under heavy augmentation.\nThe combination of WL and IA yields the best trade-off between geometric fidelity and incorrect predictions on the water column.\nTable 4\n:\nWe ablate four features: H, haze and water-column simulation; FA, full augmentation (lighting, caustics, particles); WL, inclusion of a self-supervised warping loss; and IA, in-air data to limit drift from the initialization. Performance is measured by EPE (pixels; lower is better). All results are obtained by fine-tuning DEFOM ViT-L.\nAblation Setting\nCombined\nOn\nWater\nH\nFA\nWL\nIA\nGeom.\nColumn\nâœ“\nâœ•\nâœ•\nâœ•\n3.81\n2.20\n9.51\nâœ“\nâœ“\nâœ•\nâœ•\n2.21\n2.29\n1.91\nâœ“\nâœ“\nâœ“\nâœ•\n2.12\n2.23\n1.70\nâœ“\nâœ“\nâœ•\nâœ“\n2.17\n2.31\n1.61\nâœ“\nâœ“\nâœ“\nâœ“\n2.09\n2.21\n1.58\nVIII\nSLAM Evaluation Results\nWe evaluate the trajectories from each SLAM method.\nORB-SLAM3 is excluded from mapping metrics since the sparse keypoint map is not intended to be used as a structural map.\nWe note that VGGT-SLAM, which is a calibration-free monocular SLAM method\n[\n36\n]\n, was unable to produce a trajectory estimate on any of the sequences and is thus excluded from the result tables.\nFor SVIn2, the mapping metrics are obtained using the dense fusion method proposed in the authorsâ€™ follow-up work\n[\n73\n]\n.\nVIII.A\nTrajectory Tracking Evaluation\nTable 5\n:\nEstimated trajectories are compared against the ground truth. The reported metric is Root Mean Square Absolute Pose Error (APE). Each result is the median of five runs. MASt3R-SLAM is a monocular method, so scale-invariant trajectory evaluations were conducted for MASt3R-SLAM only. Lower is better.\nMethod\nPlatform\nEngine\nBoiler\nLong\nDROID-SLAM\n[\n66\n]\nCPU+GPU\n0.163\n3.121\n5.794\nMASt3R SLAM\n[\n39\n]\nCPU+GPU\n0.414\n0.477\n2.025\nORB-SLAM3\n[\n7\n]\nCPU\n1.664\n2.851\n6.754\nSVIn2\n[\n48\n]\nCPU\n1.006\n2.885\n3.052\nTURTLMap\n[\n64\n]\nCPU\n0.440\n0.477\n1.212\nOurs\nCPU+GPU\n0.190\n0.349\n0.386\nResults of the SLAM trajectory evaluation are shown in Table\n5\n.\nOur proposed method shows the best performance on all three SLAM sequences.\nTURTLMap, which does not apply global registration, produces reasonable trajectories on all sequences. Yet, without a mechanism to mitigate accumulated drift, it underperforms against methods that are able to successfully register against a global map.\nOf the purely vision-based methods, DROID-SLAM and MASt3R-SLAM perform the best.\nThis indicates that even without finetuning, learning-based SLAM is better-equipped to deal with the challenges of the underwater domain compared to ORB-SLAM3, which uses carefully tuned feature-matching heuristics.\nSVIn2 converges on all sequences but drifts significantly. We hypothesize that this drift results from the IMU initialization procedure in SVIn2, which benefits from the robot starting at rest. In our field trials, it was not feasible for the robot to be stationary due to dynamic environmental conditions.\nVIII.B\nMapping Evaluation\nFigure 9\n:\nQualitative comparison of the maps generated by each method on the\nBoiler\nsequence. Methods that produce vastly incorrect maps due to their incorrect pose estimates (specifically, DroidSLAM\n[\n66\n]\n, SVIn2\n[\n73\n]\n, and TURTLMap\n[\n64\n]\n) are cropped for visualization purposes. The SVIn2 image refers to running the SVIn2\n[\n48\n]\nfor trajectory estimation and using the dense mapping method from\n[\n74\n]\n. Our map produces a more accurate, cleaner, and more complete map than those produced by the baseline methods.\nQuantitative map evaluations are presented in Table\n6\n.\nWe present the mapping metrics from the trajectories evaluated on the desktop platform, and the results indicate that SurfSLAM has comparable or better performance across these metrics.\nOn the completion and recall metrics, SurfSLAM demonstrates significant improvements, indicating more complete reconstructions.\nFor the accuracy and precision metrics, performance is degraded due to a few erroneous pose estimates, resulting in a misaligned projected depth map.\nThis is supported by the qualitative results shown in Figure\n9\n, where the map produced by SurfSLAM shows consistently more accurate measurements and fewer measurements misaligned with the larger reconstructed map.\nOn the\nLong\nsequence, which features the most low-texture regions, the advantages of SurfSLAM are most apparent.\nDROID-SLAM excels in accuracy and precision on the feature-rich\nEngine\nscene.\nYet, the completeness and precision are comparatively worse than those of SurfSLAM, indicating less complete maps.\nSVIn2â€™s dense fusion struggles, likely due to its difficulty with trajectory estimation (See Table\n5\n).\nRunning TURTLMap with our stereo network is comparable to SurfSLAM.\nHowever, we show consistent improvement in the completeness of the reconstructions with comparable accuracy on longer sequences.\nQualitative mapping results are shown in Figure\n9\n.\nOur method produces the most complete and geometrically consistent reconstruction among the compared approaches.\nFor DroidSLAM, the trajectory estimate diverges as the robot traverses the boiler and loses tracking, leading to an incomplete map, consistent with the quantitative results.\nFor MASt3R-SLAM, despite achieving the second-best trajectory accuracy, depth prediction errors introduce noticeable artifacts and reduce map accuracy.\nFor SVIn2 and TURTLMap, accumulated drift distorts and misaligns the reconstruction: SVIn2 exhibits pronounced distortion on the left side of the map, while TURTLMap shows missing geometry due to depth misalignment, causing valid observations to be filtered during mapping.\nFinally, the map produced by TURTLMap using our proposed depths demonstrates a higher fidelity map compared to TURTLMap alone, but also consists of missing portions of the map due to drift accumulated over the trajectory.\nTable 6\n:\nMaps were evaluated using photogrammetry ground-truth. The TURTL Base column indicates accumulating depths from the TURTLMap trajectory estimates using a baseline DEFOM ViT-S stereo depth estimator. In contrast, TURTL Ours indicates using the TURTLMap trajectories and our proposed DEFOM ViT-S stereo depth estimator. All numbers are the median of five runs. For accuracy and completion, lower is better. For precision and recall, higher is better.\nDroid\nMast3R\nSVin2\nTURTL\nTURTL\nOurs\nSLAM\nSLAM\nBase\nOurs\nBoiler\nAcc.\n0.29\n0.34\n1.16\n0.87\n0.61\n0.23\nComp.\n0.37\n0.10\n0.26\n0.21\n0.14\n0.05\nPrec.\n0.17\n0.35\n0.14\n0.24\n0.28\n0.41\nRec.\n0.07\n0.80\n0.50\n0.34\n0.54\n0.96\nEngine\nAcc.\n0.04\n0.44\n0.31\n0.31\n0.10\n0.15\nComp.\n0.33\n0.19\n0.09\n0.31\n0.26\n0.05\nPrec.\n0.98\n0.32\n0.26\n0.68\n0.74\n0.59\nRec.\n0.14\n0.79\n0.85\n0.49\n0.55\n0.98\nLong\nAcc.\n1.36\n0.69\n0.78\n0.35\n0.15\n0.18\nComp.\n2.09\n3.62\n2.99\n0.30\n0.24\n0.03\nPrec.\n0.01\n0.20\n0.12\n0.53\n0.58\n0.53\nRec.\n0.00\n0.27\n0.21\n0.39\n0.48\n0.99\nVIII.C\nCompute Comparison and Runtime\nThis section compares the performance of our method between the desktop platform and the NVidia Jetson Thor.\nTable\n7\nshow that SurfSLAM achieves comparable performance on both desktop and embedded platforms, reinforcing its suitability for field deployment. On two of the three sequences, the Jetson yields slightly lower tracking error than the desktop. Because the Jetson drops more frames during stereo matching (see Table\n8\n), a plausible explanation is that it skips a frame that would otherwise induce an erroneous registration on the desktop platform.\nWe present the runtime of key components of the algorithm in Table\n8\n.\nOn each of the platforms used to evaluate the system, we report two key timing metrics: time for stereo inference and time to solve the backend optimization.\nIn practice, several of the components run in parallel.\nTo ensure real-time operation, each component drops stale data.\nTable\n8\nalso reports statistics on how many frames are dropped within the system.\nOn the desktop platform, 98.4% of input stereo frames are processed by the stereo inference module.\nAll of those frames are processed by the registration module to search for global registrations.\nOn the Jetson platform, the same 98.4% of the input stereo frames are processed.\nHowever, due to the slower stereo inference time, 32.4% of frames processed by the stereo matcher become stale and are dropped before registration is attempted.\nTable 7\n:\nWe compare the tracking performance of our method on each of the three sequences on two platforms, a high-performance desktop machine and an NVidia Jetson Thor. The reported metric is Root Mean Square Absolute Pose Error (APE).\nPlatform\nEngine\nBoiler\nLong\nJetson Thor\n0.184\n0.330\n0.432\nDesktop\n0.190\n0.349\n0.386\nIX\nDiscussion & Conclusion\nThis paper presents a comprehensive methodology for stereo-based state estimation and scene reconstruction in challenging underwater environments.\nWe first introduced a sim-to-real training pipeline for underwater stereo disparity estimation and demonstrated that it outperforms the state-of-the-art in underwater domains.\nNext, we presented SurfSLAM, a method that fuses stereo images with IMU, DVL, and barometer measurements to perform long-term state estimation and mapping in real-world underwater environments.\nIn addition to the algorithm contributions of this paper, we present two new datasets for underwater stereo perception and underwater SLAM. The first dataset comprises both a curated dataset of scenes that mimic a variety of underwater environments and an augmentation pipeline that randomizes underwater imaging effects. We also present a comprehensive dataset of real-world shipwrecks.\nTable 8\n:\nWe report the runtime of both stereo inference and the backend optimization (mean +/- standard deviation), averaged across all sequences and 5 trials. During real-time operation of the system, stale frames are dropped to prevent the system from falling behind. For instance, an N% registration drop rate means that of all the frames the registration module received, it discarded N% of the frames and processed the rest.\nGroup\nMetric\nOurs (Desktop)\nOurs (Jetson)\nTiming\nStereo Inference\n359.3\nÂ±\n26.4\n359.3\\pm 26.4\nms\n743.4\nÂ±\n41.9\n743.4\\pm 41.9\nms\nOptimization\n6.7\nÂ±\n3.4\n6.7\\pm 3.4\nms\n9.4\nÂ±\n4.6\n9.4\\pm 4.6\nms\nDrops\nStereo\n1.6\n1.6\n%\n1.6\n1.6\n%\nRegistration\n0.0\n0.0\n%\n32.3\n32.3\n%\nOverall\n1.6\n1.6\n%\n33.4\n33.4\n%\nThe results of our experiments demonstrate state-of-the-art performance on stereo estimation in the underwater domain.\nWhile several related works simulate underwater haze, we additionally simulate directional lighting, caustics, and suspended particles.\nExperiments demonstrate that this comprehensive simulation improves performance over the haze-only simulation.\nFurther, while baseline methods most often train either on simulated data or via self-supervised training on real underwater images, we demonstrate that a combined approach yields superior results.\nThe results of our SLAM evaluation support the approach of relying on acoustic and inertial sensors as the primary local tracker, while using a highly accurate but slow vision model to correct drift when possible.\nThe trajectory evaluation results support the conclusion that this leads to stronger performance in challenging environments than methods that rely on feature tracking, which is susceptible to failure in low-texture underwater scenes.\nA limitation of our existing evaluations arises from the difficulty of reconstructing metric ground-truth from images under highly turbid conditions.\nWhile we hypothesize that our sim-to-real pipeline will have a greater impact in more turbid water, it is challenging to test this hypothesis because photogrammetry methods cannot reconstruct ground truth in such conditions.\nA potential avenue for future work is to use other sensors, such as 3D SONAR or Underwater LiDAR, to generate high-quality ground truth in such environments.\nLastly, an avenue for future work to improve SurfSLAM is to augment the SLAM pipeline with bundle adjustment rather than relying on frame-to-frame registration.\nIn practice, this is challenging in our target domain, which includes large, textureless regions that can push the stability of existing bundle-adjustment methods.\nReferences\n[1]\nAgisoft LLC\n(2018)\nAgisoft Metashape Professional, Version 1.4.5\n.\nNote:\nSoftware\nExternal Links:\nLink\nCited by:\nÂ§II.C\n.\n[2]\nD. Akkaynak and T. Treibitz\n(2018-06)\nA revised underwater image formation model\n.\nIn\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\nÂ§I\n.\n[3]\nL. Avanthey and L. Beaudoin\n(2024-12)\nDense In Situ Underwater 3D Reconstruction by Aggregation of Successive Partial Local Clouds\n.\nRemote Sensing\n16\n(\n24\n),\npp.Â 4737\n(\nen\n).\nExternal Links:\nISSN 2072-4292\n,\nLink\n,\nDocument\nCited by:\nÂ§II.B\n.\n[4]\nAyoung Kim and R. Eustice\n(2009-10)\nPose-graph visual SLAM with geometric model selection for autonomous underwater ship hull inspection\n.\nIn\n2009 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\nSt. Louis, MO\n,\npp.Â 1559â€“1565\n(\nen\n).\nExternal Links:\nISBN 978-1-4244-3803-7\n,\nLink\n,\nDocument\nCited by:\nÂ§II.B\n.\n[5]\nD. Berman, D. Levy, S. Avidan, and T. Treibitz\n(2020)\nUnderwater single image color restoration using haze-lines and a new quantitative dataset\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n43\n(\n8\n),\npp.Â 2822â€“2837\n.\nCited by:\nÂ§II.C\n,\nTable 1\n.\n[6]\nG. Billings, R. Camilli, and M. Johnson-Roberson\n(2022)\nHybrid visual slam for underwater vehicle manipulator systems\n.\nIEEE Robotics and Automation Letters\n7\n(\n3\n),\npp.Â 6798â€“6805\n.\nExternal Links:\nDocument\nCited by:\nTable 1\n,\nÂ§VI.C.1\n.\n[7]\nC. Campos, R. Elvira, J. J. G. RodrÃ­guez, J. M. M. Montiel, and J. D. TardÃ³s\n(2021)\nORB-slam3: an accurate open-source library for visual, visualâ€“inertial, and multimap slam\n.\nIEEE Transactions on Robotics\n37\n(\n6\n),\npp.Â 1874â€“1890\n.\nExternal Links:\nDocument\nCited by:\nÂ§II.A\n,\nÂ§VI.D.2\n,\nTable 5\n.\n[8]\nL. Carlone, Z. Kira, C. Beall, V. Indelman, and F. Dellaert\n(2014)\nEliminating conditionally independent sets in factor graphs: a unifying perspective based on smart factors\n.\nIn\n2014 IEEE International Conference on Robotics and Automation (ICRA)\n,\nVol.\n,\npp.Â 4290â€“4297\n.\nExternal Links:\nDocument\nCited by:\nÂ§VI.B.2\n.\n[9]\nX. Chen, C. Hsieh, and B. Gong\n(2021)\nWhen vision transformers outperform resnets without pretraining or strong data augmentations\n.\narXiv preprint arXiv:2106.01548\n.\nCited by:\nÂ§VI.C.2\n.\n[10]\nD. DeTone, T. Malisiewicz, and A. Rabinovich\n(2018)\nSuperPoint: self-supervised interest point detection and description\n.\nIn\n2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\n,\nVol.\n,\npp.Â 337â€“33712\n.\nExternal Links:\nDocument\nCited by:\nÂ§V.D.1\n.\n[11]\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby\n(2021)\nAn image is worth 16x16 words: transformers for image recognition at scale\n.\nICLR\n.\nCited by:\nÂ§VI.C.2\n.\n[12]\nM. Douze, A. Guzhva, C. Deng, J. Johnson, G. Szilvasy, P. MazarÃ©, M. Lomeli, L. Hosseini, and H. JÃ©gou\n(2025)\nTHE faiss library\n.\nIEEE Transactions on Big Data\n(\n),\npp.Â 1â€“17\n.\nExternal Links:\nDocument\nCited by:\nÂ§V.D.1\n.\n[13]\nEpic Games\n(2024)\nFab\n(Website)\nNote:\nEpicâ€™s unified marketplace for digital assets\nExternal Links:\nLink\nCited by:\nÂ§IV.A\n.\n[14]\nR. Eustice, H. Singh, J. Leonard, M. Walter, and R. Ballard\n(2005-06)\nVisually Navigating the RMS Titanic with SLAM Information Filters\n.\nIn\nRobotics: Science and Systems I\n,\n(\nen\n).\nExternal Links:\nISBN 978-0-262-70114-3\n,\nLink\n,\nDocument\nCited by:\nÂ§II.B\n.\n[15]\nC. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza\n(2017-02)\nOn-manifold preintegration for real-time visualâ€“inertial odometry\n.\nIEEE Transactions on Robotics\n33\n(\n1\n),\npp.Â 1â€“21\n.\nExternal Links:\nISSN 1941-0468\n,\nLink\n,\nDocument\nCited by:\nÂ§III.A.2\n,\nÂ§V.C\n.\n[16]\nP. Furgale\n(2025)\nRepresenting robot pose: the good, the bad, and the ugly.\n(Website)\nExternal Links:\nLink\nCited by:\nÂ§III.A.2\n.\n[17]\nR. Girshick\n(2015)\nFast r-cnn\n.\nExternal Links:\n1504.08083\n,\nLink\nCited by:\nÂ§IV.C.2\n.\n[18]\nC. Godard, O. Mac Aodha, and G. J. Brostow\n(2017)\nUnsupervised monocular depth estimation with left-right consistency\n.\nIn\nCVPR\n,\nCited by:\nÂ§IV.C.5\n.\n[19]\nM. Grupp\n(2017)\nEvo: python package for the evaluation of odometry and slam.\n.\nNote:\nhttps://github.com/MichaelGrupp/evo\nCited by:\nÂ§VI.E.2\n.\n[20]\nH. Hirschmuller\n(2007)\nStereo processing by semiglobal matching and mutual information\n.\nIEEE Transactions on pattern analysis and machine intelligence\n30\n(\n2\n),\npp.Â 328â€“341\n.\nCited by:\nÂ§I\n.\n[21]\nH. Hirschmuller\n(2008)\nStereo processing by semiglobal matching and mutual information\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n30\n(\n2\n),\npp.Â 328â€“341\n.\nExternal Links:\nDocument\nCited by:\nÂ§II.C\n.\n[22]\nS. Hou, D. Jiao, B. Dong, H. Wang, and G. Wu\n(2022)\nUnderwater inspection of bridge substructures using sonar and deep convolutional network\n.\nAdvanced Engineering Informatics\n52\n,\npp.Â 101545\n.\nCited by:\nÂ§I\n.\n[23]\nP. J. Huber\n(1992)\nRobust estimation of a location parameter\n.\nIn\nBreakthroughs in statistics: Methodology and distribution\n,\npp.Â 492â€“518\n.\nCited by:\nÂ§V.D.4\n.\n[24]\nS. Isaacson, P. Kung, M. Ramanagopal, R. Vasudevan, and K. A. Skinner\n(2024)\nLONER: lidar only neural representations for real-time slam\n.\nExternal Links:\n2309.04937\n,\nLink\nCited by:\nÂ§VI.E.3\n.\n[25]\nK. M. Jatavallabhula, E. Smith, J. Lafleche, C. F. Tsang, A. Rozantsev, W. Chen, T. Xiang, R. Lebaredian, and S. Fidler\n(2019)\nKaolin: a pytorch library for accelerating 3d deep learning research\n.\nExternal Links:\n1911.05063\n,\nLink\nCited by:\nÂ§IV.B.2\n.\n[26]\nH. JÃ©gou, M. Douze, C. Schmid, and P. PÃ©rez\n(2010)\nAggregating local descriptors into a compact image representation\n.\nIn\n2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n,\nVol.\n,\npp.Â 3304â€“3311\n.\nExternal Links:\nDocument\nCited by:\nÂ§V.D.1\n.\n[27]\nN. G. Jerlov\n(1968)\nOptical oceanography\n.\nVol.\n5\n,\nElsevier\n.\nCited by:\nÂ§I\n,\nÂ§IV.B.1\n.\n[28]\nH. Jiang, Z. Lou, L. Ding, R. Xu, M. Tan, W. Jiang, and R. Huang\n(2025)\nDEFOM-stereo: depth foundation model based stereo matching\n.\nIn\n2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nVol.\n,\npp.Â 21857â€“21867\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n,\nÂ§II.C\n,\nÂ§IV.C\n,\nÂ§VI.D.1\n,\nTable 3\n,\nTable 3\n,\nÂ§VII\n.\n[29]\nM. Kaess, H. Johannsson, R. Roberts, V. Ila, J. J. Leonard, and F. Dellaert\n(2012)\nISAM2: incremental smoothing and mapping using the bayes tree\n.\nThe International Journal of Robotics Research\n31\n(\n2\n),\npp.Â 216â€“235\n.\nExternal Links:\nDocument\n,\nLink\n,\nhttps://doi.org/10.1177/0278364911430419\nCited by:\nÂ§V.C\n.\n[30]\nA. Kim and R. M. Eustice\n(2011-09)\nCombined visually and geometrically informative link hypothesis for pose-graph visual SLAM using bag-of-words\n.\nIn\n2011 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\nSan Francisco, CA\n,\npp.Â 1647â€“1654\n(\nen\n).\nExternal Links:\nISBN 978-1-61284-456-5 978-1-61284-454-1 978-1-61284-455-8\n,\nLink\n,\nDocument\nCited by:\nÂ§II.B\n.\n[31]\nA. Kim and R. M. Eustice\n(2013-06)\nReal-Time Visual SLAM for Autonomous Underwater Hull Inspection Using Visual Saliency\n.\nIEEE Trans. Robot.\n29\n(\n3\n),\npp.Â 719â€“733\n(\nen\n).\nExternal Links:\nISSN 1552-3098, 1941-0468\n,\nLink\n,\nDocument\nCited by:\nÂ§II.B\n.\n[32]\nV. Leroy, Y. Cabon, and J. Revaud\n(2024)\nGrounding image matching in 3d with mast3r\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 71â€“91\n.\nCited by:\nÂ§II.A\n.\n[33]\nJ. Li, K. A. Skinner, R. M. Eustice, and M. Johnson-Roberson\n(2017)\nWaterGAN: unsupervised generative network to enable real-time color correction of monocular underwater images\n.\nIEEE Robotics and Automation Letters\n,\npp.Â 1â€“1\n.\nExternal Links:\nISSN 2377-3774\n,\nLink\n,\nDocument\nCited by:\nÂ§I\n.\n[34]\nL. Lipson, Z. Teed, and J. Deng\n(2021)\nRAFT-Stereo: multilevel recurrent field transforms for stereo matching\n.\nIn\nInternational Conference on 3D Vision (3DV)\n,\nCited by:\nÂ§II.C\n,\nÂ§VI.D.1\n.\n[35]\nQ. Lv, J. Dong, Y. Li, S. Chen, H. Yu, S. Zhang, and W. Wang\n(2025)\nUWStereo: a large synthetic dataset for underwater stereo matching\n.\nIEEE Transactions on Circuits and Systems for Video Technology\n.\nCited by:\nÂ§II.D\n,\nTable 1\n.\n[36]\nD. Maggio, H. Lim, and L. Carlone\n(2025)\nVggt-slam: dense rgb slam optimized on the sl (4) manifold\n.\narXiv preprint arXiv:2505.12549\n.\nCited by:\nÂ§II.A\n,\nÂ§VI.D.2\n,\nÂ§VIII\n.\n[37]\nA. Marburg and M. Micatka\n(2025)\nA dataset for the assessment of underwater slam degradation in turbid water\n.\nIn\nOCEANS 2025 Chicago\n,\nCited by:\nTable 1\n.\n[38]\nN. Mayer, E. Ilg, P. HÃ¤usser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox\n(2016)\nA large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation\n.\nIn\nIEEE International Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nNote:\narXiv:1512.02134\nExternal Links:\nLink\nCited by:\nÂ§VI.C.1\n.\n[39]\nR. Murai, E. Dexheimer, and A. J. Davison\n(2025)\nMASt3R-slam: real-time dense slam with 3d reconstruction priors\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 16695â€“16705\n.\nCited by:\nÂ§II.A\n,\nÂ§VI.D.2\n,\nTable 5\n.\n[40]\nNVIDIA Corporation\nOmniverse\n.\nNote:\nSoftware\nExternal Links:\nLink\nCited by:\nÂ§IV.A\n.\n[41]\nNVIDIA Corporation\n(2025)\nNVIDIA isaac sim (version 4.5.1)\n.\nNote:\nGitHub repositoryAvailable at\nhttps://github.com/isaac-sim/IsaacSim\nCited by:\nÂ§IV.A\n.\n[42]\nOpenCV: Camera Calibration and 3D Reconstruction\n.\nExternal Links:\nLink\nCited by:\nFigure 3\n,\nFigure 3\n.\n[43]\nP. Ozog and R. M. Eustice\n(2013-11)\nReal-time SLAM with piecewise-planar surface models and sparse 3D point clouds\n.\nIn\n2013 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\nTokyo\n,\npp.Â 1042â€“1049\n(\nen\n).\nExternal Links:\nISBN 978-1-4673-6358-7 978-1-4673-6357-0\n,\nLink\n,\nDocument\nCited by:\nÂ§II.B\n.\n[44]\nS. Pan, Z. Hong, Z. Hu, X. Xu, W. Lu, and L. Hu\n(2025-03)\nRUSSO: Robust Underwater SLAM with Sonar Optimization against Visual Degradation\n.\narXiv\n(\nen\n).\nNote:\narXiv:2503.01434 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§II.B\n.\n[45]\nC. Papadopoulos and G. Papaioannou\n(2010-08)\nRealistic real-time underwater caustics and godrays\n.\n19th International Conference on Computer Graphics and Vision, GraphiConâ€™2009 - Conference Proceedings\n,\npp.\n.\nCited by:\nÂ§IV.B.4\n.\n[46]\nS.M. Pizer, R.E. Johnston, J.P. Ericksen, B.C. Yankaskas, and K.E. Muller\n(1990)\nContrast-limited adaptive histogram equalization: speed and effectiveness\n.\nIn\n[1990] Proceedings of the First Conference on Visualization in Biomedical Computing\n,\nVol.\n,\npp.Â 337â€“345\n.\nExternal Links:\nDocument\nCited by:\nÂ§V.D.1\n.\n[47]\nE. Potokar, S. Ashford, M. Kaess, and J. Mangelson\n(2022-05)\nHoloOcean: an underwater robotics simulator\n.\nIn\nProc. IEEE Intl. Conf. on Robotics and Automation, ICRA\n,\nPhiladelphia, PA, USA\n.\nCited by:\nÂ§IV.A\n.\n[48]\nS. Rahman, A. Q. Li, and I. Rekleitis\n(2019)\nSVIn2: an underwater slam system using sonar, visual, inertial, and depth sensor\n.\nIn\n2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nVol.\n,\npp.Â 1861â€“1868\n.\nExternal Links:\nDocument\nCited by:\nÂ§II.B\n,\nTable 1\n,\nÂ§VI.C.1\n,\nÂ§VI.D.2\n,\nFigure 9\n,\nFigure 9\n,\nTable 5\n.\n[49]\nY. Randall\n(2023)\nFLSea: underwater visual-inertial and stereo-vision forward-looking datasets\n.\nMasterâ€™s Thesis\n,\nUniversity of Haifa (Israel)\n.\nCited by:\nÂ§II.C\n,\nTable 1\n,\nTable 1\n.\n[50]\nJ. Rehder, J. Nikolic, T. Schneider, T. Hinzmann, and R. Siegwart\n(2016)\nExtending kalibr: calibrating the extrinsics of multiple imus and of individual axes\n.\nIn\n2016 IEEE International Conference on Robotics and Automation (ICRA)\n,\nVol.\n,\npp.Â 4304â€“4311\n.\nExternal Links:\nDocument\nCited by:\nÂ§VI.B.1\n,\nÂ§VI.B.2\n.\n[51]\nE. Riba, D. Mishkin, D. Ponsa, E. Rublee, and G. Bradski\n(2020)\nKornia: an open source differentiable computer vision library for pytorch\n.\nIn\nWinter Conference on Applications of Computer Vision\n,\nExternal Links:\nLink\nCited by:\nÂ§IV.C.3\n.\n[52]\nB. Romrell, A. Austin, B. Meyers, R. Anderson, C. Noh, and J. G. Mangelson\n(2025)\nA preview of holoocean 2.0\n.\nExternal Links:\n2510.06160\n,\nLink\nCited by:\nÂ§IV.A\n.\n[53]\nM. Roznere, P. Mordohai, I. Rekleitis, and A. Q. Li\n(2023)\n3-d reconstruction using monocular camera and lights: multi-view photometric stereo for non-stationary robots\n.\nIn\n2023 IEEE International Conference on Robotics and Automation (ICRA)\n,\nVol.\n,\npp.Â 1026â€“1032\n.\nExternal Links:\nDocument\nCited by:\nÂ§IV.B.3\n.\n[54]\nP. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich\n(2020)\nSuperGlue: learning feature matching with graph neural networks\n.\nIn\n2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nVol.\n,\npp.Â 4937â€“4946\n.\nExternal Links:\nDocument\nCited by:\nÂ§V.D.1\n,\nÂ§V.D.2\n.\n[55]\nD. Scharstein and R. Szeliski\n(2002)\nA taxonomy and evaluation of dense two-frame stereo correspondence algorithms\n.\nInternational journal of computer vision\n47\n,\npp.Â 7â€“42\n.\nCited by:\nÂ§II.C\n.\n[56]\nY.Y. Schechner and N. Karpel\n(2005)\nRecovery of underwater visibility and structure by polarization analysis\n.\nIEEE Journal of Oceanic Engineering\n30\n(\n3\n),\npp.Â 570â€“587\n.\nExternal Links:\nDocument\nCited by:\nÂ§IV.B.1\n.\n[57]\nJ. L. SchÃ¶nberger and J. Frahm\n(2016)\nStructure-from-Motion Revisited\n.\nIn\nConference on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\nÂ§II.C\n,\nÂ§VI.B.1\n.\n[58]\nJ. L. SchÃ¶nberger, E. Zheng, M. Pollefeys, and J. Frahm\n(2016)\nPixelwise view selection for unstructured multi-view stereo\n.\nIn\nEuropean Conference on Computer Vision (ECCV)\n,\nCited by:\nÂ§II.C\n,\nÂ§VI.B.1\n.\n[59]\nA. Segal, D. HÃ¤hnel, and S. Thrun\n(2009)\nGeneralized-icp.\n.\nIn\nRobotics: Science and Systems\n,\nJ. Trinkle, Y. Matsuoka, and J. A. Castellanos (Eds.)\n,\nExternal Links:\nISBN 978-0-262-51463-7\n,\nLink\nCited by:\nÂ§V.D.3\n.\n[60]\nA. V. Sethuraman, M. S. Ramanagopal, and K. A. Skinner\n(2023)\nWaterNeRF: neural radiance fields for underwater scenes\n.\nIn\nOCEANS 2023-MTS/IEEE US Gulf Coast\n,\npp.Â 1â€“7\n.\nCited by:\nÂ§IV.B.1\n.\n[61]\nK. Singh, J. Hong, N. R. Rypkema, and J. J. Leonard\n(2024)\nOpti-acoustic semantic slam with unknown objects in underwater environments\n.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 1169â€“1176\n.\nCited by:\nÂ§I\n.\n[62]\nSketchfab, Inc.\n(2025)\nSketchfab\n(Website)\nNote:\nOnline platform for publishing, sharing, and discovering 3D content\nExternal Links:\nLink\nCited by:\nÂ§IV.A\n.\n[63]\nK. A. Skinner, J. Zhang, E. A. Olson, and M. Johnson-Roberson\n(2019)\nUWStereoNet: unsupervised learning for depth estimation and color correction of underwater stereo imagery\n.\nIn\n2019 International Conference on Robotics and Automation (ICRA)\n,\nVol.\n,\npp.Â 7947â€“7954\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n,\nÂ§II.D\n,\nÂ§II.D\n,\nTable 1\n,\nÂ§IV.C.3\n,\nÂ§IV.C.3\n.\n[64]\nJ. Song, O. Bagoren, R. Andigani, A. Sethuraman, and K. A. Skinner\n(2024)\nTURTLMap: real-time localization and dense mapping of low-texture underwater environments with a low-cost unmanned underwater vehicle\n.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 1191â€“1198\n.\nCited by:\nÂ§II.B\n,\nÂ§V.C\n,\nÂ§V.C\n,\nÂ§V.D\n,\nÂ§V\n,\nÂ§VI.D.2\n,\nFigure 9\n,\nFigure 9\n,\nTable 5\n.\n[65]\nJ. Song, H. Ma, O. Bagoren, A. V. Sethuraman, Y. Zhang, and K. A. Skinner\n(2025)\nOceanSim: a GPU-accelerated underwater robot perception simulation framework\n.\nIn\n2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nExternal Links:\nLink\nCited by:\nÂ§IV.A\n,\nÂ§IV.B.2\n.\n[66]\nZ. Teed and J. Deng\n(2021)\nDroid-slam: deep visual slam for monocular, stereo, and rgb-d cameras\n.\nAdvances in neural information processing systems\n34\n,\npp.Â 16558â€“16569\n.\nCited by:\nÂ§II.A\n,\nÂ§V.E\n,\nÂ§VI.D.2\n,\nFigure 9\n,\nFigure 9\n,\nTable 5\n.\n[67]\nA. Thoms, G. Earle, N. Charron, and S. Narasimhan\n(2023)\nTightly coupled, graph-based dvl/imu fusion and decoupled mapping for slam-centric maritime infrastructure inspection\n.\nIEEE Journal of Oceanic Engineering\n48\n(\n3\n),\npp.Â 663â€“676\n.\nExternal Links:\nDocument\nCited by:\nÂ§II.B\n,\nÂ§V.C\n.\n[68]\nC. Upstill\n(1979)\nLight caustics from rippling water\n.\nProceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences\n365\n(\n1720\n),\npp.Â 95â€“104\n.\nExternal Links:\nISSN 00804630\n,\nLink\nCited by:\nÂ§IV.B.2\n.\n[69]\nM. Vankadari, S. Hodgson, S. Shin, K. Zhou, A. Markham, and N. Trigoni\n(2024)\nDusk till dawn: self-supervised nighttime stereo depth estimation using visual foundation models\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\nVol.\n,\npp.Â 17976â€“17982\n.\nExternal Links:\nDocument\nCited by:\nÂ§IV.C.3\n,\nÂ§IV.C.3\n,\nÂ§IV.C.3\n,\nÂ§IV.C.5\n.\n[70]\nE. Vargas, R. Scona, J. S. Willners, T. Luczynski, Y. Cao, S. Wang, and Y. R. Petillot\n(2021-05)\nRobust Underwater Visual SLAM Fusing Acoustic Sensing\n.\nIn\n2021 IEEE International Conference on Robotics and Automation (ICRA)\n,\nXiâ€™an, China\n,\npp.Â 2140â€“2146\n(\nen\n).\nExternal Links:\nISBN 978-1-7281-9077-8\n,\nLink\n,\nDocument\nCited by:\nÂ§II.B\n.\n[71]\nD. Wang and Y. J. Pang\n(2015)\nResearch on disparity map generation method of underwater target based on the improved sift algorithm\n.\nApplied Mechanics and Materials\n741\n,\npp.Â 701â€“704\n.\nCited by:\nÂ§II.C\n.\n[72]\nJ. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny\n(2025)\nVggt: visual geometry grounded transformer\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 5294â€“5306\n.\nCited by:\nÂ§II.A\n.\n[73]\nW. Wang, B. Joshi, N. Burgdorfer, K. Batsos, A. Q. Li, P. Mordohai, and I. Rekleitis\n(2023-04)\nReal-Time Dense 3D Mapping of Underwater Environments\n.\narXiv\n(\nen\n).\nNote:\narXiv:2304.02704 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§II.B\n,\nÂ§VI.D.2\n,\nFigure 9\n,\nFigure 9\n,\nÂ§VIII\n.\n[74]\nW. Wang, B. Joshi, N. Burgdorfer, K. Batsos, A. Q. Li, P. Mordohai, and I. Rekleitis\n(2023)\nReal-time dense 3d mapping of underwater environments\n.\nExternal Links:\n2304.02704\n,\nLink\nCited by:\nFigure 9\n,\nFigure 9\n.\n[75]\nW. Wang, D. Zhu, X. Wang, Y. Hu, Y. Qiu, C. Wang, Y. Hu, A. Kapoor, and S. Scherer\n(2020-10)\nTartanAir: A dataset to push the limits of visual SLAM\n.\nIn\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 4909â€“4916\n.\nExternal Links:\nDocument\n,\nLink\n,\n2003.14338\nCited by:\nÂ§VI.C.1\n.\n[76]\nB. Wen, M. Trepte, J. Aribido, J. Kautz, O. Gallo, and S. Birchfield\n(2025)\nFoundationStereo: zero-shot stereo matching\n.\narXiv preprint arXiv:2501.09898\n.\nCited by:\nÂ§I\n,\nÂ§II.C\n,\nÂ§IV.A\n,\nÂ§IV.C.2\n,\nÂ§IV.C.2\n,\nÂ§IV.C\n,\nÂ§VI.D.1\n,\nÂ§VI.E.1\n,\nTable 3\n,\nTable 3\n,\nÂ§VII\n.\n[77]\nZ. Wu, Y. Wang, Y. Wen, Z. Zhang, B. Wu, and H. Tang\n(2025)\nStereoAdapter: adapting stereo depth estimation to underwater scenes\n.\nExternal Links:\n2509.16415\n,\nLink\nCited by:\nÂ§I\n,\nÂ§II.D\n,\nÂ§II.D\n,\nTable 1\n.\n[78]\nG. Xu, X. Wang, Z. Zhang, J. Cheng, C. Liao, and X. Yang\n(2025)\nIgev++: iterative multi-range geometry encoding volumes for stereo matching\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n.\nCited by:\nÂ§I\n,\nÂ§II.C\n,\nÂ§VI.D.1\n,\nTable 3\n,\nÂ§VII\n.\n[79]\nS. Xu, T. Luczynski, J. S. Willners, Z. Hong, K. Zhang, Y. R. Petillot, and S. Wang\n(2021-09)\nUnderwater Visual Acoustic SLAM with Extrinsic Calibration\n.\nIn\n2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nPrague, Czech Republic\n,\npp.Â 7647â€“7652\n(\nen\n).\nExternal Links:\nISBN 978-1-6654-1714-3\n,\nLink\n,\nDocument\nCited by:\nÂ§II.B\n.\n[80]\nS. Xu, J. Scharff Willners, J. Roe, S. Katagiri, T. Luczynski, Y. Petillot, and S. Wang\n(2025)\nTank dataset: an underwater multi-sensor dataset for slam evaluation\n.\nThe International Journal of Robotics Research\n,\npp.Â 02783649251364904\n.\nCited by:\nTable 1\n.\n[81]\nS. Xu, K. Zhang, and S. Wang\n(2025)\nAQUA-slam: tightly-coupled underwater acoustic-visual-inertial slam with sensor calibration\n.\nExternal Links:\n2503.11420\n,\nLink\nCited by:\nÂ§II.B\n.\n[82]\nL. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao\n(2024)\nDepth anything v2\n.\nIn\nAdvances in Neural Information Processing Systems\n,\nA. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (Eds.)\n,\nVol.\n37\n,\npp.Â 21875â€“21911\n.\nExternal Links:\nLink\nCited by:\nÂ§II.C\n.\n[83]\nL. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao\n(2024)\nDepth anything v2\n.\nAdvances in Neural Information Processing Systems\n37\n,\npp.Â 21875â€“21911\n.\nCited by:\nÂ§I\n.\n[84]\nX. Ye, Y. Chang, R. Xu, and H. Li\n(2025)\nUw-adapter: adapting monocular depth estimation model in underwater scenes\n.\nIEEE Transactions on Multimedia\n.\nCited by:\nÂ§I\n.\n[85]\nX. Ye, J. Zhang, Y. Yuan, R. Xu, Z. Wang, and H. Li\n(2023)\nUnderwater depth estimation via stereo adaptation networks\n.\nIEEE Transactions on Circuits and Systems for Video Technology\n33\n(\n9\n),\npp.Â 5089â€“5101\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n,\nÂ§II.D\n,\nÂ§VI.D.1\n,\nTable 3\n,\nÂ§VII\n.\n[86]\nB. Yu, J. Wu, and M. J. Islam\n(2022)\nUdepth: fast monocular depth estimation for visually-guided underwater robots\n.\narXiv preprint arXiv:2209.12358\n.\nCited by:\nÂ§I\n.\n[87]\nH. Zhang, G. Billings, and S. B. Williams\n(2025)\nSPADE: sparsity adaptive depth estimator for zero-shot, real-time, monocular depth estimation in underwater environments\n.\narXiv preprint arXiv:2510.25463\n.\nCited by:\nÂ§I\n.\n[88]\nL. Zhu, Y. Ju, and Y. Gao\n(2025)\nA novel stereo matching network for underwater scenes\n.\nIn\n2025 IEEE International Symposium on Circuits and Systems (ISCAS)\n,\nVol.\n,\npp.Â 1â€“5\n.\nExternal Links:\nDocument\nCited by:\nÂ§II.D\n.",
    "preview_text": "Localization and mapping are core perceptual capabilities for underwater robots. Stereo cameras provide a low-cost means of directly estimating metric depth to support these tasks. However, despite recent advances in stereo depth estimation on land, computing depth from image pairs in underwater scenes remains challenging. In underwater environments, images are degraded by light attenuation, visual artifacts, and dynamic lighting conditions. Furthermore, real-world underwater scenes frequently lack rich texture useful for stereo depth estimation and 3D reconstruction. As a result, stereo estimation networks trained on in-air data cannot transfer directly to the underwater domain. In addition, there is a lack of real-world underwater stereo datasets for supervised training of neural networks. Poor underwater depth estimation is compounded in stereo-based Simultaneous Localization and Mapping (SLAM) algorithms, making it a fundamental challenge for underwater robot perception. To address these challenges, we propose a novel framework that enables sim-to-real training of underwater stereo disparity estimation networks using simulated data and self-supervised finetuning. We leverage our learned depth predictions to develop \\algname, a novel framework for real-time underwater SLAM that fuses stereo cameras with IMU, barometric, and Doppler Velocity Log (DVL) measurements. Lastly, we collect a challenging real-world dataset of shipwreck surveys using an underwater robot. Our dataset features over 24,000 stereo pairs, along with high-quality, dense photogrammetry models and reference trajectories for evaluation. Through extensive experiments, we demonstrate the advantages of the proposed training approach on real-world data for improving stereo estimation in the underwater domain and for enabling accurate trajectory estimation and 3D reconstruction of complex shipwreck sites.\n\nSurfSLAM: Sim-to-Real Underwater Stereo Reconstruction For Real-Time SLAM\nOnur Bagoren\nâˆ—\nSeth Isa",
    "is_relevant": false,
    "relevance_score": 0.0,
    "extracted_keywords": [
        "SLAM",
        "stereo reconstruction",
        "underwater",
        "sim-to-real",
        "depth estimation",
        "robotics"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨¡æ‹Ÿåˆ°çœŸå®è®­ç»ƒçš„æ°´ä¸‹ç«‹ä½“é‡å»ºæ¡†æ¶ï¼Œç”¨äºå®æ—¶SLAMï¼Œä½†æœªæ¶‰åŠå¼ºåŒ–å­¦ä¹ ã€VLAã€æ‰©æ•£æ¨¡å‹ã€Flow Matchingã€è¿åŠ¨æ§åˆ¶ã€VLMæˆ–å…¨èº«æ§åˆ¶ç­‰å…³é”®è¯ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-15T19:25:33Z",
    "created_at": "2026-01-20T17:49:57.062766",
    "updated_at": "2026-01-20T17:49:57.062773",
    "recommend": 0
}