{
    "id": "2512.01280v1",
    "title": "Visibility-aware Cooperative Aerial Tracking with Decentralized LiDAR-based Swarms",
    "authors": [
        "Longji Yin",
        "Yunfan Ren",
        "Fangcheng Zhu",
        "Liuyu Shi",
        "Fanze Kong",
        "Benxu Tang",
        "Wenyi Liu",
        "Ximin Lyu",
        "Fu Zhang"
    ],
    "abstract": "åŸºäºæ— äººæœºçš„è‡ªä¸»ç©ºä¸­è·Ÿè¸ªåœ¨ç›‘æ§ã€å½±è§†æ‹æ‘„å’Œå·¥ä¸šæ£€æµ‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚å°½ç®¡å•æœºè·Ÿè¸ªç³»ç»Ÿå·²å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œä½†åŸºäºæ— äººæœºé›†ç¾¤çš„ç›®æ ‡è·Ÿè¸ªä»å¤„äºæ¢ç´¢é˜¶æ®µï¼Œè€Œåè€…å‡­å€Ÿå…¶åˆ†å¸ƒå¼æ„ŸçŸ¥ã€å®¹é”™å†—ä½™ä»¥åŠå¤šè§’åº¦ç›®æ ‡è¦†ç›–ç­‰ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºä¸€ç§å…¨æ–°çš„å»ä¸­å¿ƒåŒ–ã€åŸºäºæ¿€å…‰é›·è¾¾ï¼ˆLiDARï¼‰çš„é›†ç¾¤è·Ÿè¸ªæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°å…·å¤‡å¯è§æ€§æ„ŸçŸ¥èƒ½åŠ›çš„ååŒç›®æ ‡è·Ÿè¸ªï¼Œå……åˆ†æŒ–æ˜é›†ç¾¤ç³»ç»Ÿçš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚\n\nä¸ºåº”å¯¹ç¯å¢ƒé®æŒ¡é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åŸºäºçƒé¢ç¬¦å·è·ç¦»åœºï¼ˆSpherical Signed Distance Field, SSDFï¼‰çš„æ–°å‹ä¸‰ç»´ç¯å¢ƒé®æŒ¡è¡¨å¾åº¦é‡æ–¹æ³•ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§é«˜æ•ˆç®—æ³•ï¼Œæ”¯æŒåœ¨æœºè½½å¹³å°ä¸Šå®æ—¶æ›´æ–°SSDFã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§é€šç”¨çš„è§†åœºï¼ˆField-of-View, FOVï¼‰å¯¹é½ä»£ä»·å‡½æ•°ï¼Œå¯å…¼å®¹å¼‚æ„LiDARé…ç½®ï¼Œç¡®ä¿ç›®æ ‡è§‚æµ‹çš„ä¸€è‡´æ€§ã€‚åœ¨é›†ç¾¤ååŒæ–¹é¢ï¼Œé€šè¿‡å¼•å…¥åä½œä»£ä»·é¡¹å®ç°å¤šé‡ä¼˜åŒ–ç›®æ ‡ï¼šä¿éšœæ— äººæœºé—´çš„å®‰å…¨é—´è·ã€é¿å…ç›¸äº’é®æŒ¡ï¼Œå¹¶åˆ›æ–°æ€§åœ°é‡‡ç”¨ä¸€ç§å—é™ç”µåŠ¿å¯å‘çš„åˆ†å¸ƒåº¦é‡æœºåˆ¶ï¼Œä¿ƒè¿›ä¸‰ç»´ç©ºé—´ä¸­å¯¹ç›®æ ‡çš„å¤šå‘åŒ…å›´ã€‚ä¸Šè¿°æŠ€æœ¯è¢«é›†æˆäºä¸€ä¸ªåˆ†å±‚è§„åˆ’å™¨ä¸­ï¼Œç»“åˆè¿åŠ¨åŠ¨åŠ›å­¦å‰ç«¯æœç´¢å™¨ä¸æ—¶ç©º$SE(3)$åç«¯ä¼˜åŒ–å™¨ï¼Œç”Ÿæˆæ— ç¢°æ’ä¸”è§†è§‰å¯è§æ€§æœ€ä¼˜çš„è½¨è¿¹ã€‚\n\næœ¬ç³»ç»Ÿéƒ¨ç½²äºé…å¤‡å¼‚æ„LiDARçš„æ— äººæœºé›†ç¾¤ï¼Œå®Œå…¨å»ä¸­å¿ƒåŒ–è¿è¡Œï¼Œæ”¯æŒååŒæ„ŸçŸ¥ã€åˆ†å¸ƒå¼è§„åˆ’åŠåŠ¨æ€é›†ç¾¤é‡æ„ã€‚é€šè¿‡åœ¨æ‚ä¹±æˆ·å¤–ç¯å¢ƒä¸­å¼€å±•çš„å¤§é‡çœŸå®å®éªŒéªŒè¯ï¼Œæ‰€æç³»ç»Ÿå±•ç°å‡ºå¯¹é«˜é€Ÿç§»åŠ¨ç›®æ ‡ï¼ˆå¦‚æ— äººæœºã€è¡Œäººï¼‰é²æ£’çš„ååŒè·Ÿè¸ªèƒ½åŠ›ï¼ŒåŒæ—¶æ˜¾è‘—æå‡äº†å¯¹ç›®æ ‡çš„å¯è§æ€§ç»´æŒæ•ˆæœã€‚",
    "url": "https://arxiv.org/abs/2512.01280v1",
    "html_url": "https://arxiv.org/html/2512.01280v1",
    "html_content": "Visibility-aware Cooperative Aerial Tracking with Decentralized LiDAR-based Swarms\nLongji Yin\n1\n, Yunfan Ren\n1\n, Fangcheng Zhu\n1\n, Liuyu Shi\n1\n, Fanze Kong\n1\n,\nBenxu Tang\n1\n, Wenyi Liu\n1\n, Ximin Lyu\n2\n, and Fu Zhang\n1\n1\nThe authors are with the Department of Mechanical Engineering, The University of Hong Kong, Hong Kong.\n2\nX. Lyu is with the School of Intelligent System Engineering, Sun Yat-sen University, Shenzhen, China.Corresponding author: Fu Zhang,\nfuzhang@hku.hk.\nAbstract\nAutonomous aerial tracking with drones offers vast potential for surveillance, cinematography, and industrial inspection applications. While single-drone tracking systems have been extensively studied, swarm-based target tracking remains underexplored, despite its unique advantages of distributed perception, fault-tolerant redundancy, and multidirectional target coverage. To bridge this gap, we propose a novel decentralized LiDAR-based swarm tracking framework that enables visibility-aware, cooperative target tracking in complex environments, while fully harnessing the unique capabilities of swarm systems. To address visibility, we introduce a novel Spherical Signed Distance Field (SSDF)-based metric for 3-D environmental occlusion representation, coupled with an efficient algorithm that enables real-time onboard SSDF updating. A general Field-of-View (FOV) alignment cost supporting heterogeneous LiDAR configurations is proposed for consistent target observation. Swarm coordination is enhanced through cooperative costs that enforce inter-robot safe clearance, prevent mutual occlusions, and notably facilitate 3-D multidirectional target encirclement via a novel electrostatic-potential-inspired distribution metric. These innovations are integrated into a hierarchical planner, combining a kinodynamic front-end searcher with a spatiotemporal\nS\nâ€‹\nE\nâ€‹\n(\n3\n)\nSE(3)\nback-end optimizer to generate collision-free, visibility-optimized trajectories. The proposed approach undergoes thorough evaluation through comprehensive benchmark comparisons and ablation studies. Deployed on heterogeneous LiDAR swarms, our fully decentralized implementation features collaborative perception, distributed planning, and dynamic swarm reconfigurability. Validated through rigorous real-world experiments in cluttered outdoor environments, the proposed system demonstrates robust cooperative tracking of agile targets (drones, humans) while achieving superior visibility maintenance. This work establishes a systematic solution for swarm-based target tracking, and its source code will be released to benefit the community.\nI\nIntroduction\nAutonomous aerial tracking with UAVs, empowered by advances in onboard sensors and computing, is now extensively applied in fields ranging from cinematography to surveillance and industrial inspection. Recent studies highlight the unique suitability of UAVs for tracking dynamic targets in complex environments, owing to their highly agile three-dimensional (3-D) maneuverability. While substantial progress has been made in single-UAV tracking, the swarm-based aerial tracking remains underexplored. Nevertheless, cooperative UAV swarms surpass the capabilities of individual trackers by exploiting distributed perception, fault-tolerant redundancy, and multidirectional target coverageâ€”features unattainable in single-UAV paradigms. In this work, we bridge this gap by introducing a new framework for visibility-aware target tracking, utilizing a cooperative team of decentralized UAVs.\nFigure 1:\nA swarm of four autonomous drones is cooperatively tracking a human runner using heterogeneous LiDAR configurations. The LiDAR setup consists of one upward-facing Mid360 LiDAR (marked by blue dashed lines), one downward-facing Mid360 LiDAR (green dashed lines), and two Avia LiDARs (red dashed lines). The swarm forms a 3-D distribution to track the target, with each tracker positioned optimally to suit its FOV settings.\nEffective agile aerial tracking with autonomous swarms primarily relies on three criteria: visibility, coordination, and portability. Visibility, as a fundamental requirement, demands persistent sensor-based observation of the target by the trackers during the flight. Swarm coordination requires the drone to track the target cooperatively without impeding teammate performance. Portability necessitates a scalable and computationally efficient planner to enable real-time deployment on decentralized swarms. In the existing studies, a framework that can effectively fulfill all three criteria is still lacking. As outlined below, three key technical challenges in current swarm tracking systems still limit the tracking performance.\nThe first challenge lies in designing accurate metrics to quantify the visibility conditions in target tracking tasks. Line-of-sight (LOS) visibility, commonly adopted since it matches with the sensor measurement principles (\ne.g.\nLiDAR and camera), imposes two key requisites in aerial tracking: (1) an obstacle-free LOS between tracker and target to prevent occlusions, and (2) LOS alignment within the trackerâ€™s field-of-view (FOV) to sense the target. The first requisite needs a robust metric formulation to penalize occlusion and steer the LOS away from obstructed areas, but existing solutions are constrained by many deficiencies like non-differentiable formulations\n[\n1\n,\n2\n]\n, inflexible dependencies on specific FOV shapes\n[\n3\n,\n4\n]\n, and oversimplified 2-D occlusion models\n[\n5\n]\n. For the second FOV requisite, many studies enforce fixed tracker-target altitude alignment\n[\n6\n,\n7\n,\n8\n]\n, a heuristic that disregards the FOV geometries and wastes the trackerâ€™s vertical agility. Current swarm tracking works rarely consider the FOV configurations in a swarm, ignoring the FOV configurationsâ€™ critical impact on proper swarm spatial distribution for collective target perception. Thus, developing reliable and effective visibility metrics remains a crucial challenge.\nThe second challenge is creating metrics to coordinate the swarm tracking maneuvers in 3-D space. Beyond ensuring inter-agent safety, effective coordination in swarm tracking entails two more objectives: mutual occlusion avoidance and multi-angle target coverage. Each drone must prevent blocking teammatesâ€™ LOS to the target, ensuring persistent collective visibility during tracking. Concurrently, the swarm needs to optimize spatial distribution to enable multidirectional target coverage, which mitigates single-direction occlusions, supports target measurement fusion from diverse perspectives, and grants each tracker a larger angular space to respond to adverse situations. This multidirectional approach can also maximize the utility of heterogeneous sensors by positioning agents at vantage points suited to their FOV modalities. The existing frameworks, however, either disregard these cooperation issues or rely on simplistic 2-D equidistant leader-follower formations to separate the trackers. Such strategies waste the swarmâ€™s 3-D maneuverability, constraining agents to planar configurations that limit the volumetric coordination potential.\nThe third technical challenge involves the decentralization of the whole swarm tracking system. Decentralized architecture underpins the scalability, fault tolerance, and efficient use of computational resources. However, unlike centralized approaches, where a single entity directly manages the entire swarm, decentralization introduces more challenges in coordinating swarm behavior. A decentralized swarm demands adaptive individual planners that reconcile local motion with global swarm objectives, coupled with solid system architectures supporting peer-to-peer communication and scalable computation. Meanwhile, collaborative perception, where agents exchange sensory data to collectively enhance tracking robustness, is also vital to the performance of a decentralized system.\nBased on the above analysis and insights, we propose a complete swarm tracking system that addresses all the outlined challenges. To resolve the LOS occlusion challenge, we novelly adapt the Spherical Signed Distance Field (SSDF), a spatial representation originally used for graphical rendering\n[\n9\n,\n10\n,\n11\n,\n12\n]\n, into a visibility metric that encodes the 3-D environmental occlusion around the target. The metricâ€™s practicality is supported by our efficient SSDF update algorithm tailored for real-time tracking operations. A differentiable SSDF-based occlusion penalty is derived for visibility-aware trajectory optimization. To ensure FOV alignments, we formulate a unified FOV constraint supporting heterogeneous LiDAR configurations. To tackle the mutual occlusion challenge, we regulate a minimum angular separation clearance between the swarm members around the target. For uniform 3-D multidirectional target encirclement, we draw inspiration from Thomsonâ€™s classical electron distribution problem, innovatively framing the optimal swarm distribution as minimizing the electrostatic potential energy of charged particles (UAVs) repelling each other around a central nucleus (the target). Unlike plain 2-D equidistant formations in existing works\n[\n6\n,\n7\n,\n8\n,\n13\n,\n14\n,\n15\n]\n, our formulation achieves true 3-D distribution enclosing the target. Additional metrics quantifying the tracking distance, obstacle avoidance, dynamic feasibility, and inter-agent collision avoidance are also formulated to assess the tracking motion.\nWe incorporate our proposed metrics into a two-stage decentralized planning framework consisting of a front-end kinodynamic path searcher and a back-end spatiotemporal trajectory optimizer. Given a sequence of target future positions interpolated by the prediction module, the kinodynamic searcher generates discrete candidate paths by expanding motion primitives temporally aligned with the targetâ€™s prediction stamps. The searcher determines the optimal guiding path by assessing the primitives against the proposed tracking metrics. In the back-end stage, a corridor-constrained spatiotemporal optimizer generates an executable tracking trajectory along the front-end path. Since the trackerâ€™s 3-D FOV is governed by both its position and orientation, we perform\nS\nâ€‹\nE\nâ€‹\n(\n3\n)\nSE(3)\nfull-state optimization for the back-end. By embedding our metrics in both stages, the planning framework systematically trades off safety, visibility, and spatial coordination, enabling cooperative visibility-aware swarm tracking in dense environments.\nAt the system level, we present a fully decentralized swarm architecture, incorporating decentralized localization, mapping, planning, and control modules to address the third outlined technical challenge of decentralization. Trajectories are synchronized over a wireless network to facilitate tight swarm coordination. Real-time local mapping data and target measurements from individual drones are shared among the swarm to enable distributed collaborative perception. We rigorously validate the proposed planner through comprehensive simulation tests. Comparative benchmark against cutting-edge methods confirms our plannerâ€™s superior ability to maintain persistent target visibility. Integrating the modules into decentralized swarms with heterogeneous LiDAR FOVs, we conduct extensive real-world experiments in different complex environments, successfully achieving cooperative tracking of agile targets (drones, human runners). Notably, the swarm demonstrates dynamic reconfigurability (agent joining/leaving) during live target tracking in constrained outdoor scenarios, highlighting advanced decentralization and system robustness.\nThe contributions of this paper are listed as:\n1.\nA novel Spherical Signed Distance Field (SSDF)-based environmental occlusion representation is proposed for target tracking, coupled with an efficient SSDF update algorithm that enables real-time onboard computation.\n2.\nNew differentiable metrics for line-of-sight (LOS) visibility assessment are developed, including an SSDF-derived occlusion penalty and a general FOV alignment cost adaptable to heterogeneous FOV settings.\n3.\nNovel differentiable swarm coordination metrics are formulated for cooperative tracking, notably an electrostatic potential-inspired swarm distribution cost enabling true 3-D multidirectional target encirclement.\n4.\nA two-stage planning framework is designed, comprising a kinodynamic front-end searcher and a spatiotemporal\nS\nâ€‹\nE\nâ€‹\n(\n3\n)\nSE(3)\nback-end optimizer, systematically trading off the metrics for visibility-aware cooperative swarm tracking.\n5.\nA complete decentralized multi-UAV tracking system is presented, featuring collaborative perception, decentralized planning, and dynamic swarm reconfiguration. Rigorous simulations and real-world experiments demonstrate its robust performance in complex environments.\n6.\nThe source code of the proposed system will be publicly released for the reference of the research community.\nIn what follows, we review related works in Sec.\nII\nand\nprovide a system overview in Sec.\nIII\n. The SSDF-based spatial visibility representation and update algorithms are introduced in Sec.\nIV\n. Planning metrics for visibility-aware swarm tracking are formulated in Sec.\nV\n, followed by details on front-end kinodynamic searching and back-end trajectory optimization in Secs.\nVI\nand\nVII\n. Benchmark and experimental results are given in Sec.\nVIII\nand Sec.\nIX\n. Sec.\nX\nconcludes this work.\nII\nRelated works\nII-A\nSingle-UAV Target Tracking\nSeveral earlier studies\n[\n16\n,\n17\n]\ntreat vision-based aerial tracking as a local control problem, designing control laws to regulate feedback error in image spaces. However, these control laws struggle to account for constraints like visibility and cannot plan for future target motions over a horizon. Han\net al.\n[\n18\n]\nproposed an optimized-based tracking planner consisting of a spatiotemporal optimizer and kinodynamic searcher, but they only focus on maintaining tracking distance while neglecting visibility. Penin\net al.\n[\n19\n]\nformulate a nonlinear MPC problem which penalizes the occlusion using the obstacle projection on the image plane. However, their method assumes ellipsoid-shaped obstacles, limiting the application to unstructured scenes. Bonatti\net al.\n[\n20\n]\ndeveloped an aerial cinematography framework considering obstacle avoidance, LOS visibility, and smoothness, but ignored the sensorâ€™s FOV geometry in visibility constraints. In\n[\n3\n]\n, Wang\net al.\ndesign an occlusion cost that penalizes the intersection area of obstacles and the FOV, aiming to eliminate all such intersections. However, the proposed constraint is too strict to be satisfied in dense spaces. Besides, their cost is specifically formulated for conic FOVs and not compatible with omnidirectional sensors like 360\nâˆ˜\nLiDARs, as the obstacle-intersection area can always exist in the omnidirectional view. Jeon\net al.\n[\n2\n]\npresent a visibility metric based on ESDFs. They assess the risk of occlusion by evaluating the minimum ESDF value along the tracker-target LOS. However, their visibility metric lacks differentiability, making it unsuitable for back-end optimization. Ji\net al.\n[\n5\n]\npropose a new method, which generates a sequence of 2-D sector-shaped visible regions for visibility-aware tracking. But these 2-D sectors represent visibility only at a specific height, which is unable to facilitate 3-D occlusion avoidance. Additionally, the seed (tracker position) for building visibility sectors must be in visible areas, making it impossible to quantify visibility when the seed is already occluded. This is unfavorable for target-shared swarm tracking, as trackers can temporarily endure occlusion but still require an instructive effort within the occluded area to escape it. To systematically tackle all the above drawbacks, we novelly adopt Spherical Signed Distance Fields (SSDFs)\n[\n21\n,\n9\n,\n10\n,\n11\n]\nto encode the 3-D spatial visibility around the tracked target. Our metric formulation is differentiable, remains independent of any specific FOV type, and models full 3-D visibility without seed restrictions. Moreover, a general geometric constraint for target-FOV alignment is designed to prevent visibility loss caused by heterogeneous FOV shapes.\nII-B\nMultiple-UAV Target Tracking\nMotion planning for multi-UAV target tracking has gained growing interest in recent research\n[\n6\n,\n13\n,\n22\n,\n23\n,\n8\n,\n24\n,\n25\n]\n. Zhou\net al.\n[\n6\n]\npresent a swarm of drones tracking a target using a fixed 2-D leader-follower formation, where the targetâ€™s position, observed by one tracker, is shared with others to enhance occlusion resistance. In\n[\n13\n]\n, Tallamraju\net al.\nemploy a 2-D equidistant formation to minimize fused uncertainty in target estimation and use model predictive controllers (MPC) for both formation control and obstacle avoidance. While the drones in\n[\n6\n,\n13\n]\nshare target observations to temporarily endure occlusion, they lack strategies to proactively mitigate visibility loss. To address this, Nageli\net al.\n[\n24\n]\npropose an MPC-based framework that uses a horizon plane, derived from obstacles, to separate visible and invisible regions. However, their approach assumes obstacles are ellipsoidal, making it unsuitable for arbitrarily shaped obstacles. In\n[\n8\n]\n, Lee\net al.\ncheck the occupancy along the LOS to avoid obstacle occlusion and employ 2-D Inter-Visibility Cells (IVC) to prevent the inter-agent occlusion. However, they only consider the target visibility at the front-end stage but conduct pure path smoothing in the back-end. Such inconsistency could still lead to constraint violation on final trajectories. Bucker\net al.\n[\n22\n]\ndivide the space around the target into discrete cells, assigning each cell a score based on its occlusion avoidance potential. A centralized greedy algorithm is then used to generate motion sequences that minimize visibility loss for each drone. However, the predefined priority in their greedy search can result in sub-optimal outcomes. Ho\net al.\n[\n23\n]\npropose a framework for generating swarm trajectories in aerial 3-D reconstruction. Their approach allows the follower formation to rotate around the target, with a centralized front-end planner using dynamic programming to find the optimal sequence of the rotation angles. Nonetheless, the fixed formation assumption is prone to failure in cluttered environments, and the centralized design is susceptible to node failures. The existing multi-UAV tracking works\n[\n6\n,\n7\n,\n8\n,\n14\n,\n15\n]\n, commonly suggest a 2-D equidistant swarm formation to encircle the target. However, such 2-D formations severely waste the 3-D maneuverability of UAVs and may oppose other tracking requirements like the FOV constraint. To address this deficiency, we draw inspiration from a classic electrostatic problem and design a novel metric to instruct the uniform 3-D swarm encirclement in real 3-D spaces. Unlike the centralized approaches\n[\n23\n,\n22\n]\n, our method adopts a fully decentralized framework, where the formation is encoded as a slack cost to enable a flexible trade-off between swarm distribution and other objectives. Despite being decentralized, our approach ensures cooperative maneuvers among the drones while consistently preserving high visibility of the target.\nFigure 2:\nAn overview of our complete decentralized swarm tracking system, including the decentralized swarm localization, shared mapping, collaborative target estimation, onboard control, and motion planning modules. A hierarchical planner is designed to generate optimal trajectories for swarm tracking. All critical operational data is exchanged between swarm members via a UDP-based wireless network.\nIII\nSystem Overview\nThe overall autonomous swarm tracking system is illustrated in Fig.\n2\n. This decentralized swarm system aims to cooperatively track a passive target in unknown, cluttered environments using onboard LiDAR sensors. Each drone employs Swarm-LIO\n[\n26\n]\n, a decentralized swarm LiDAR-inertial odometry, for self-localization and mutual state estimation. The drones in the swarm exchange critical operational data with their teammates via a wireless UDP network and transform the received data into the local frame through spatiotemporal alignment, using swarm extrinsics and calibrated system time lag from Swarm-LIO. Besides the data used by Swarm-LIO, three types of information are shared: planned trajectories, local map data, and target measurements. Trajectories are broadcast for swarm cooperation. Each drone receives its teammatesâ€™ trajectories to formulate the coordination constraints and optimizes its own motion plan accordingly. For local mapping, the system uses an occupancy grid map\n[\n27\n]\nas the map format. The local map updates of each agent are synchronized across the swarm using a bandwidth-efficient method\n[\n28\n]\n, allowing the trackers to enhance environmental perception collectively. In this work, the target is identified by high-reflective markers, with its position measured by clustering the sensed point clouds. Each tracker shares its target measurements with teammates, while a Kalman filter on each drone fuses its own measurements with those received from others, enabling multi-source robust target state estimation. More system details are presented in Sec.\nIX-A\n.\nThe planning of local tracking trajectories is hierarchical. Given the fused target states, the prediction module first generates a sequence of target future positions via velocity-based linear interpolation with fixed time intervals. An SSDF is then built at each predicted target position to model 3-D occlusion and enable visibility-aware planning (Sec.\nIV\n). Collectively considering all the tracking metrics proposed in Sec.\nV\n, we employ a kinodynamic searcher to expand motion primitives and select an optimal path (Sec.\nVI\n), used for safe corridor generation and back-end trajectory initialization. Afterwards, the trajectoryâ€™s spatial and temporal profiles are efficiently optimized by the back-end to maximize the tracking performance (Sec.\nVII\n). The planning process cycles periodically with a receding horizon, and the generated trajectories are finally executed via a model predictive controller\n[\n29\n]\n.\nIV\nSpherical Signed Distance Field\nWe introduce a novel visibility-aware approach for target tracking that utilizes the Spherical Signed Distance Fields (SSDFs). Traditionally applied as a visibility model for spatial shading in computer graphics\n[\n9\n,\n11\n,\n10\n]\n, SSDFs inherently align with visibility problems through their radial environmental representation centered at the focal point (the target). Lines of sight emanating from the target partition the space into visible and occluded sectors. At a specified tracking distance (radius), the degree of occlusion can be indicated by the signed spherical distance from the trackerâ€™s position to the closest visible sector boundaries. This section details our efficient SSDF updating method for real-time visibility-aware planning.\nIV-A\nVisibility Map Update\nTo construct an SSDF around the target, we first need to spherically parameterize the 3-D space and update a visibility map around the target. The visibility map, denoted as\nğ’±\n\\mathcal{V}\n, is a 3-D grid where each cell has a binary state: a state of\n1\n1\nmeans it is visible, while\n0\nsignifies its occlusion from the target. The visibility map is discretized into a spherical grid defined by\nÎ¸\n\\theta\n(polar/latitudinal angle),\nÏ•\n\\phi\n(azimuthal/longitudinal angle), and\nr\nr\n(radial distance), with the target position as the gridâ€™s origin. Here,\nÎ¸\ni\n\\theta_{i}\n,\nÏ•\nj\n\\phi_{j}\n, and\nr\nk\nr_{k}\nrepresent the respective indices of these parameters. Fig.\n3\n(a) shows the spherical grid. LOS visibility has a clear yet useful property: for a given direction\n(\nÎ¸\n,\nÏ•\n)\n(\\theta,\\phi)\n, visibility deteriorates monotonically as the radial distance\nr\nr\nincreases. If the LOS along the direction\n(\nÎ¸\n,\nÏ•\n)\n(\\theta,\\phi)\nis firstly blocked by an obstacle located at\n[\nÎ¸\n,\nÏ•\n,\nr\nm\nâ€‹\ni\nâ€‹\nn\n]\nS\n[\\,\\theta,\\phi,r_{min}]_{S}\n, then all positions\n[\nÎ¸\n,\nÏ•\n,\nr\n]\nS\n[\\,\\theta,\\phi,r]_{S}\nwith\nr\n>\nr\nm\nâ€‹\ni\nâ€‹\nn\nr>r_{min}\nare also occluded. Fig.\n3\n(b) shows this property. We use\n[\nâ‹…\n]\nS\n[\\cdot]_{S}\nto denote the spherical coordinates. Observing this monotonic property, we can update the visibility map by identifying the closest occluded radius\nr\nm\nâ€‹\ni\nâ€‹\nn\nr_{min}\nfor each direction\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\n, denoting the radial index of\nr\nm\nâ€‹\ni\nâ€‹\nn\nr_{min}\nas\nk\nm\nâ€‹\ni\nâ€‹\nn\nk_{min}\n. In Alg.\n1\n, we traverse the occupancy map\nğ’\n\\mathcal{C}\naround the target within a radius\nr\nm\nâ€‹\na\nâ€‹\nx\nr_{max}\nand select the smallest\nk\nm\nâ€‹\ni\nâ€‹\nn\nk_{min}\nfor every direction by comparison. Then, along each direction in map\nğ’±\n\\mathcal{V}\n, the grids with radial index\nk\nk\ngreater than the minimal occluded index\nk\nm\nâ€‹\ni\nâ€‹\nn\nk_{min}\ncan be directly set as occluded by\nsetOcclusion()\n. The auxiliary array\nS\nS\nin Line 13 is prepared for\nthe further updates in Sec.\nIV-C\n.\nAlgorithm 1\nVisibility Map Update\n1:\nthe grid map\nğ’\n\\mathcal{C}\nand cells\nc\nin\nğ’\n\\mathcal{C}\n; the visibility map\nğ’±\n\\mathcal{V}\ndiscretized by latitude\nÎ¸\ni\n\\theta_{i}\n, longitude\nÏ•\nj\n\\phi_{j}\n, and radial distance\nr\nk\nr_{k}\n; the index of closest occluded radial distance\nk\nm\nâ€‹\ni\nâ€‹\nn\nâ€‹\n[\ni\n,\nj\n]\nk_{min}[i,j]\nfor direction\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\n; The resolution\nN\nr\nN_{r}\nof the radial dimension\nr\nr\n; An auxiliary array\nS\nS\nof length\nN\nr\nN_{r}\n;\n2:\nthe visibility map\nğ’±\n\\mathcal{V}\nis updated;\n3:\nInitialize:\nk\nm\nâ€‹\ni\nâ€‹\nn\nâ†\n(\nN\nr\nâˆ’\n1\n)\nk_{min}\\leftarrow(N_{r}-1)\n,\n4:\nfor\neach\nc\nin\nğ’\n\\mathcal{C}\ndo\n5:\nif\ncell\nc\nis occupied\nthen\n6:\n[\nÎ¸\ni\n,\nÏ•\nj\n,\nr\nk\n]\nS\nâ†\n[\\theta_{i},\\phi_{j},r_{k}]_{S}\\leftarrow\nToSphericalCoordinate\n(\nc\n)\n(\\textbf{c})\n;\n7:\nif\nk\nm\nâ€‹\ni\nâ€‹\nn\nâ€‹\n[\ni\n,\nj\n]\n>\nk_{min}[i,j]>\nk\nk\nthen\n8:\nk\nm\nâ€‹\ni\nâ€‹\nn\nâ€‹\n[\ni\n,\nj\n]\nâ†\nk\nk_{min}[i,j]\\leftarrow k\n;\n9:\nend\nif\n10:\nend\nif\n11:\nend\nfor\n12:\nfor\neach direction\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\nin\nğ’±\n\\mathcal{V}\ndo\n13:\nh\nâ†\nk\nm\nâ€‹\ni\nâ€‹\nn\nâ€‹\n[\ni\n,\nj\n]\nh\\leftarrow k_{min}[i,j]\n;\n14:\nsetOcclusion\n(\nğ’±\n\\mathcal{V}\n,\nh\nh\n);\n15:\nS\nâ€‹\n[\nh\n]\nS[h]\n.PUSH(\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\n);\n16:\nend\nfor\nFigure 3:\n(a)\nAn illustration of the spherical discretization and the definition of the visibility map. A map cell is flagged as occluded if obstacles block the LOS.\n(b)\nAn illustration of the monotonic property used in visibility maps. Along the direction\n(\n0\n,\nÏ•\n)\n(0,\\phi)\nin the figure, an obstacle blockage is at radius\nr\nm\nâ€‹\ni\nâ€‹\nn\nr_{min}\n. Then all grids with radii larger than\nr\nm\nâ€‹\ni\nâ€‹\nn\nr_{min}\ncan be directly set as occluded.\nIV-B\n2-D Spherical Distance Transform\nIn this section, we introduce how to update a 2-D SSDF from a 2-D visibility map parameterized by\nÎ¸\n\\theta\nand\nÏ•\n\\phi\n. A 2-D visibility map layer\nğ’±\nr\n\\mathcal{V}_{r}\ncan be extracted from the 3-D map\nğ’±\n\\mathcal{V}\nby fixing the radial dimension to a specific\nr\nr\n. Then the obtained 2-D layer\nğ’±\nr\n\\mathcal{V}_{r}\ndescribes the visibility of all directions at radius\nr\nr\n.\nGiven two directions\nv\nâ€‹\n(\nÎ¸\n,\nÏ•\n)\n\\textbf{v}(\\theta,\\phi)\nand\nu\nâ€‹\n(\nÎ¸\nâ€²\n,\nÏ•\nâ€²\n)\n\\textbf{u}(\\theta^{\\prime},\\phi^{\\prime})\non\nğ’±\nr\n\\mathcal{V}_{r}\n, the angular distance\nâ„’\n\\mathcal{L}\nbetween these two directions is given by the spherical law of cosine:\nâ„’\nâ€‹\n{\nv\n,\nu\n}\n=\ncos\n-\nâ€‹\n1\nâ¡\n(\ncos\nâ¡\nÎ¸\nâ€²\nâ€‹\ncos\nâ¡\nÎ¸\n+\nsin\nâ¡\nÎ¸\nâ€²\nâ€‹\nsin\nâ¡\nÎ¸\nâ€‹\ncos\nâ¡\n|\nÏ•\nâˆ’\nÏ•\nâ€²\n|\n)\n.\n\\mathcal{L}\\{\\textbf{v},\\textbf{u}\\}=\\cos^{\\text{-}1}(\\,\\cos\\theta^{\\prime}\\cos\\theta+\\sin\\theta^{\\prime}\\sin\\theta\\cos|\\phi-\\phi^{\\prime}|\\,).\n(1)\nFigure 4:\nIllustrations of the SSDF definition.\n(a)\nOn the surface of a unit sphere, an occluded area is shadowed by an obstacle. For two queried directions\nv\na\n\\textbf{v}_{a}\nand\nv\nb\n\\textbf{v}_{b}\n, the black dashed curves on the sphere indicate the directionsâ€™ Spherical Signed Distances (SSD) to the closest visibility boundary.\n(b)\nA top-down view of figure (a).\n(c)\nThe corresponding occluded cells on the discretized 2-D\nÎ¸\n\\theta\n-\nÏ•\n\\phi\ngrid. The red dashed curves indicate the SSDF values of the queried directions\nv\na\n\\textbf{v}_{a}\nand\nv\nb\n\\textbf{v}_{b}\non the discrete grid.\nA 2-D spherical distance field\nğ’Ÿ\nr\n\\mathcal{D}_{r}\nis a more informative visibility model based on\nğ’±\nr\n\\mathcal{V}_{r}\n, where for a queried direction\nv\n, it stores a signed angular distance to the directionâ€™s closest visibility boundary. According to\n[\n9\n,\n11\n,\n10\n]\n, it is defined as\nğ’Ÿ\nr\nâ€‹\n(\nv\n)\n=\n{\n+\nmin\nğ’±\nr\nâ€‹\n(\nu\n)\n=\n0\nâ¡\nâ„’\nâ€‹\n{\nu\n,\nv\n}\n,\nğ’±\nr\nâ€‹\n(\nv\n)\n=\n1\n,\nâˆ’\nmin\nğ’±\nr\nâ€‹\n(\nu\n)\n=\n1\nâ¡\nâ„’\nâ€‹\n{\nu\n,\nv\n}\n,\nğ’±\nr\nâ€‹\n(\nv\n)\n=\n0\n,\n\\mathcal{D}_{r}(\\textbf{v})=\\left\\{\\!\\begin{array}[]{ll}\\!+\\min\\limits_{\\mathcal{V}_{r}(\\textbf{u})=0}\\mathcal{L}\\{\\textbf{u},\\textbf{v}\\},&\\mathcal{V}_{r}(\\textbf{v})=1,\\\\\n\\!-\\min\\limits_{\\mathcal{V}_{r}(\\textbf{u})=1}\\mathcal{L}\\{\\textbf{u},\\textbf{v}\\},&\\mathcal{V}_{r}(\\textbf{v})=0,\\\\\n\\end{array}\\right.\n(2)\nwhere\nu\nis a direction vector. By definition, SSDFs can quantify the degree of occlusion using the distance value, enabling trackers to query the minimum angular distance required to escape occluded regions. Fig.\n4\n(a)-(b) illustrate the definition, and Fig.\n4\n(c) shows the 2-D\nÎ¸\n\\theta\n-\nÏ•\n\\phi\ngrid for discrete distance transform. Euclidean Signed Distance Fields (ESDFs) are widely used in navigation. The update algorithms of ESDFs are based on dimensionality reduction\n[\n30\n,\n31\n,\n32\n,\n33\n]\n. However, unlike 2D ESDFs, the non-Euclidean and asymmetric nature of the 2D\nÎ¸\n\\theta\n-\nÏ•\n\\phi\nspherical grid in SSDFs prevents direct application of these algorithms. Nevertheless, as shown in\n[\n11\n]\n, SSDFs can still be correctly updated by reducing dimensions in a specific order: first scanning latitudes, then longitudes.\nWe provide an overview of the algorithm in\n[\n11\n]\nfor clarity, which is a spherical version of\n[\n30\n]\nfollowing a certain scanning order. By definition in Eq.\n2\n, updating the spherical distance transform needs to find the closest visibility boundary for all the discretized directions\nv\nâ€‹\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n\\textbf{v}(\\theta_{i},\\phi_{j})\n. The process takes two phases. The first phase is scanning every latitudinal column\nÎ¸\ni\n\\theta_{i}\n. Along each latitude\nÎ¸\ni\n\\theta_{i}\n, we traverse all the longitudes\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\nto find each\nÏ•\nj\n\\phi_{j}\nâ€™s closest longitudinal visibility boundary\nÏ•\nj\nc\nâ€‹\nl\nâ€‹\ns\n\\phi_{j}^{cls}\nthat minimizes the objective\nâ„’\nâ€‹\n{\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n,\n(\nÎ¸\ni\n,\nÏ•\nj\nc\nâ€‹\nl\nâ€‹\ns\n)\n}\n\\mathcal{L}\\{(\\theta_{i},\\phi_{j}),(\\theta_{i},\\phi_{j}^{cls})\\}\n. This is a 1-D distance transform, which can be updated by Alg.\n2\n, a variant of the\nL\n1\nL_{1}\n-distance transform algorithm in\n[\n30\n]\n. In Alg.\n2\n, the array\ng\nâ€‹\n[\ni\n,\nj\n]\ng[i,j]\nrecords the updated closest visible boundary direction\n(\nÎ¸\ni\n,\nÏ•\nj\nc\nâ€‹\nl\nâ€‹\ns\n)\n(\\theta_{i},\\phi_{j}^{cls})\nfor each\nÏ•\nj\n\\phi_{j}\nalong latitude\nÎ¸\ni\n\\theta_{i}\n. Initially, all entries in array\ng\ng\nare assigned a virtual direction\nN\nâ€‹\nU\nâ€‹\nL\nâ€‹\nL\nNULL\n, where the distance from any direction to\nN\nâ€‹\nU\nâ€‹\nL\nâ€‹\nL\nNULL\nis defined as infinite.\nAlgorithm 2\nLatitudinal Scan\n1:\nthe visibility map\nğ’±\nr\n\\mathcal{V}_{r}\ndiscretized by by latitude\nÎ¸\ni\n\\theta_{i}\nand longitude\nÏ•\nj\n\\phi_{j}\n; The latitudinal resolution\nN\nÎ¸\nN_{\\theta}\nand the longitudinal resolution\nN\nÏ•\nN_{\\phi}\n; An auxiliary direction\nh\nh\n;\n2:\nthe array\ng\ng\nis correctly updated;\n3:\nInitialize:\ng\nâ†\nN\nâ€‹\nU\nâ€‹\nL\nâ€‹\nL\ng\\leftarrow NULL\n,\n4:\nfor\ni\n=\n0\ni=0\nto\nN\nÎ¸\nâˆ’\n1\nN_{\\theta}-1\ndo\n5:\nfor\nj\n=\n0\nj=0\nto\nN\nÏ•\nâˆ’\n1\nN_{\\phi}-1\ndo\n6:\nif\nğ’±\nr\nâ€‹\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n\\mathcal{V}_{r}(\\theta_{i},\\phi_{j})\nis visible\nthen\n7:\ng\nâ€‹\n[\ni\n,\nj\n]\nâ†\n(\nÎ¸\ni\n,\nÏ•\nj\n)\ng[i,j]\\leftarrow(\\theta_{i},\\phi_{j})\n;\n8:\nelse\n9:\ng\nâ€‹\n[\ni\n,\nj\n]\nâ†\ng\nâ€‹\n[\ni\n,\nj\nâˆ’\n1\n]\ng[i,j]\\leftarrow g[i,j-1]\n;\n10:\nend\nif\n11:\nend\nfor\n12:\nh\nâ†\ng\nâ€‹\n[\ni\n,\nN\nÏ•\nâˆ’\n1\n]\nh\\leftarrow g[i,N_{\\phi}-1]\n;\n13:\nfor\nj\n=\nN\nÏ•\nâˆ’\n1\nj=N_{\\phi}-1\nto\n0\ndo\n14:\nif\nâ„’\nâ€‹\n{\ng\nâ€‹\n[\ni\n,\nj\n]\n,\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n}\n<\nâ„’\nâ€‹\n{\nh\n,\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n}\n\\mathcal{L}\\{g[i,j],(\\theta_{i},\\phi_{j})\\}<\\mathcal{L}\\{h,(\\theta_{i},\\phi_{j})\\}\nthen\n15:\nh\nâ†\ng\nâ€‹\n[\ni\n,\nj\n]\nh\\leftarrow g[i,j]\n;\n16:\nend\nif\n17:\ng\nâ€‹\n[\ni\n,\nj\n]\nâ†\nh\ng[i,j]\\leftarrow h\n;\n18:\nend\nfor\n19:\nend\nfor\nAlgorithm 3\nLongitudinal Scan\n1:\nthe visibility map\nğ’±\nr\n\\mathcal{V}_{r}\ndiscretized by latitude\nÎ¸\ni\n\\theta_{i}\nand longitude\nÏ•\nj\n\\phi_{j}\n; The latitudinal resolution\nN\nÎ¸\nN_{\\theta}\nand the longitudinal resolution\nN\nÏ•\nN_{\\phi}\n; Two auxiliary arrays\nv\nv\nand\nz\nz\n;\nThe array\nB\nâ€‹\n[\ni\n,\nj\n]\nB[i,j]\nrecording closest visibility boundaries.\n2:\nthe closest boundary\nD\nD\nis correctly updated;\n3:\nfor\nj\n=\n0\nj=0\nto\nN\nÏ•\nâˆ’\n1\nN_{\\phi}-1\ndo\n4:\nn\nâ†\n0\nn\\leftarrow 0\n;\n5:\nv\nâ€‹\n[\n0\n]\nâ†\n0\nv[0]\\leftarrow 0\n;\n6:\nz\nâ€‹\n[\n0\n]\nâ†\nâˆ’\nâˆ\nz[0]\\leftarrow-\\infty\n;\n7:\nz\nâ€‹\n[\n1\n]\nâ†\n+\nâˆ\nz[1]\\leftarrow+\\infty\n;\n8:\nfor\ni\n=\n1\ni=1\nto\nN\nÎ¸\nâˆ’\n1\nN_{\\theta}-1\ndo\n9:\nh\nh\nâ†\n\\leftarrow\nÎ˜\nâ€‹\n(\ng\nâ€‹\n[\ni\n,\nj\n]\n,\ng\nâ€‹\n[\nv\nâ€‹\n[\nn\n]\n,\nj\n]\n,\nj\n)\n\\Theta(\\,g[i,j],g[\\,v[n],j],j)\n;\n10:\nwhile\nh\nâ‰¤\nh\\leq\nz\nâ€‹\n[\nn\n]\nz[n]\ndo\n11:\nn\nâ†\nn\nâˆ’\n1\nn\\leftarrow n-1\n;\n12:\nh\nh\nâ†\n\\leftarrow\nÎ˜\nâ€‹\n(\ng\nâ€‹\n[\ni\n,\nj\n]\n,\ng\nâ€‹\n[\nv\nâ€‹\n[\nn\n]\n,\nj\n]\n,\nj\n)\n\\Theta(\\,g[i,j],g[\\,v[n],j],j)\n;\n13:\nend\nwhile\n14:\nn\nâ†\nn\n+\n1\nn\\leftarrow n+1\n;\n15:\nv\nâ€‹\n[\nn\n]\nâ†\ni\nv[n]\\leftarrow i\n;\n16:\nz\nâ€‹\n[\nn\n]\nâ†\nh\nz[n]\\leftarrow h\n;\n17:\nz\nâ€‹\n[\nn\n+\n1\n]\nâ†\n+\nâˆ\nz[n+1]\\leftarrow+\\infty\n;\n18:\nend\nfor\n19:\nn\nâ†\n0\nn\\leftarrow 0\n20:\nfor\ni\n=\n0\ni=0\nto\nN\nÎ¸\nâˆ’\n1\nN_{\\theta}-1\ndo\n21:\nwhile\nz\nâ€‹\n[\nn\n+\n1\n]\n<\ni\nz[n+1]<i\ndo\n22:\nn\nâ†\nn\n+\n1\nn\\leftarrow n+1\n23:\nend\nwhile\n24:\nB\nâ€‹\n[\ni\n,\nj\n]\nâ†\ng\nâ€‹\n[\nv\nâ€‹\n[\nn\n]\n,\nj\n]\nB[i,j]\\leftarrow g[\\,v[n],j]\n;\n25:\nend\nfor\n26:\nresetAuxiliaryArrays()\n;\n27:\nend\nfor\nAfter deriving\nÏ•\nj\nc\nâ€‹\nl\nâ€‹\ns\n\\phi_{j}^{cls}\nin the first phase, the second phase updates the other dimension,\nÎ¸\ni\n\\theta_{i}\n, to minimize the angular distance objective in Eq.\n1\n. To achieve the second phase, we conduct a variant of the\nL\n2\nL_{2}\n-distance transform scanning in\n[\n30\n]\nalong each longitude\nÏ•\nj\n\\phi_{j}\n, as detailed in Alg.\n3\n. While the distance objective\nâ„’\n\\mathcal{L}\nis not strictly an\nL\n2\nL_{2}\n-distance, it shares the same single intersection property\n[\n11\n]\n, enabling the variant\nL\n2\nL_{2}\n-distance algorithm from\n[\n30\n]\nto be applied. For two distinct directions\nv\n1\nâ€‹\n(\nÎ¸\n1\n,\nÏ•\n1\n)\n\\textbf{v}_{1}(\\theta_{1},\\phi_{1})\nand\nv\n2\nâ€‹\n(\nÎ¸\n2\n,\nÏ•\n2\n)\n\\textbf{v}_{2}(\\theta_{2},\\phi_{2})\n, and a given longitude\nÏ•\n0\n\\phi_{0}\n, the function\nÎ˜\nâ€‹\n(\nv\n1\n,\nv\n2\n,\nÏ•\n0\n)\n\\Theta(\\textbf{v}_{1},\\textbf{v}_{2},\\phi_{0})\nin Alg.\n3\ncomputes a latitude\nÎ¸\n0\n\\theta_{0}\n. At this latitude, the direction\nv\n0\nâ€‹\n(\nÎ¸\n0\n,\nÏ•\n0\n)\n\\textbf{v}_{0}(\\theta_{0},\\phi_{0})\nsatisfies\nâ„’\nâ€‹\n{\nv\n1\n,\nv\n0\n}\n=\nâ„’\nâ€‹\n{\nv\n2\n,\nv\n0\n}\n\\mathcal{L}\\{\\textbf{v}_{1},\\textbf{v}_{0}\\}=\\mathcal{L}\\{\\textbf{v}_{2},\\textbf{v}_{0}\\}\n. The single intersection property ensures that\nv\n0\n\\textbf{v}_{0}\nis the only intersection where the distances from\nv\n1\n\\textbf{v}_{1}\nand\nv\n2\n\\textbf{v}_{2}\nare equal. The latitude\nÎ¸\n0\n\\theta_{0}\nis given by\nÎ˜\nâ€‹\n(\nv\n1\n,\nv\n2\n,\nÏ•\n0\n)\n=\n{\nÏ€\n/\n2\n,\nP\n=\nQ\n,\ntan\n-\nâ€‹\n1\nâ¡\n(\nR\nP\nâˆ’\nQ\n)\n,\nR\nP\nâˆ’\nQ\nâ‰¥\n0\n,\ntan\n-\nâ€‹\n1\nâ¡\n(\nR\nP\nâˆ’\nQ\n)\n+\nÏ€\n,\nR\nP\nâˆ’\nQ\n<\n0\n,\n\\Theta(\\textbf{v}_{1},\\textbf{v}_{2},\\phi_{0})=\\left\\{\\!\\begin{array}[]{ll}\\!\\pi/2,&P=Q,\\\\\n\\!\\tan^{\\text{-}1}({\\frac{R}{P-Q}}),&\\frac{R}{P-Q}\\geq 0,\\\\\n\\!\\tan^{\\text{-}1}({\\frac{R}{P-Q}})+\\pi,&\\frac{R}{P-Q}<0,\\\\\n\\end{array}\\right.\n(3)\nwhere\nP\n=\nsin\nâ¡\nÎ¸\n1\nâ€‹\ncos\nâ¡\n(\nÏ•\n1\nâˆ’\nÏ•\n0\n)\nP=\\sin\\theta_{1}\\cos(\\phi_{1}-\\phi_{0})\n,\nQ\n=\nsin\nâ¡\nÎ¸\n2\nâ€‹\ncos\nâ¡\n(\nÏ•\n2\nâˆ’\nÏ•\n0\n)\nQ=\\sin\\theta_{2}\\cos(\\phi_{2}-\\phi_{0})\n, and\nR\n=\ncos\nâ¡\nÎ¸\n2\nâˆ’\ncos\nâ¡\nÎ¸\n1\nR=\\cos\\theta_{2}-\\cos\\theta_{1}\n. In Alg.\n3\n, the array\nB\nâ€‹\n[\ni\n,\nj\n]\nB[i,j]\nrecords the final closest visibility boundary for each direction\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\n. Then the spherical distance transform\nğ’Ÿ\nr\n\\mathcal{D}_{r}\nof direction\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\nis the distance to direction\nB\nâ€‹\n[\ni\n,\nj\n]\n{B}[i,j]\n. For more details of this two-phase algorithm, readers can refer to\n[\n11\n]\nand\n[\n30\n]\n.\nDuring target tracking, trackers should maintain LOS visibility to the target. In visible regions where\nğ’±\nr\n=\n1\n\\mathcal{V}_{r}=1\n, the LOS visibility constraints are already satisfied. Thus, in practice, we can solely focus on updating the distance fields of invisible areas. The updated spherical distances are then used to penalize the trackerâ€™s trajectories within these occluded regions, preventing visibility loss. The necessary angular clearance around the invisible area can be achieved by inflating the occluded grid when updating the visibility map\nğ’±\n\\mathcal{V}\n. Thus, under this updating rule, the definition in Eq.\n2\nis reformulated as:\nğ’Ÿ\nr\nâ€‹\n(\nv\n)\n=\n{\n0\n,\nğ’±\nr\nâ€‹\n(\nv\n)\n=\n1\n,\nâˆ’\nmin\nğ’±\nr\nâ€‹\n(\nu\n)\n=\n1\nâ¡\nâ„’\nâ€‹\n{\nu\n,\nv\n}\n,\nğ’±\nr\nâ€‹\n(\nv\n)\n=\n0\n.\n\\mathcal{D}_{r}(\\textbf{v})=\\left\\{\\!\\begin{array}[]{ll}\\!0,&\\mathcal{V}_{r}(\\textbf{v})=1,\\\\\n\\!-\\min\\limits_{\\mathcal{V}_{r}(\\textbf{u})=1}\\mathcal{L}\\{\\textbf{u},\\textbf{v}\\},&\\mathcal{V}_{r}(\\textbf{v})=0.\\\\\n\\end{array}\\right.\n(4)\nUsing this definition, the two-phase algorithm can be applied exclusively to the occluded areas, reducing the computation by skipping updates for visible directions where\nğ’±\nr\n=\n1\n\\mathcal{V}_{r}=1\n. Although the spherical distance in Eq.\n4\nis truncated, we still refer to the updated distance field\nğ’Ÿ\nr\n\\mathcal{D}_{r}\nas the â€SSDFâ€.\nIV-C\nIncremental 3-D SSDF Update\nIn Sec.\nIV-B\n, we introduced the two-phase algorithm for updating the SSDF\nğ’Ÿ\nr\n\\mathcal{D}_{r}\nof a 2-D visibility map\nğ’±\nr\n\\mathcal{V}_{r}\n, where\nğ’±\nr\n\\mathcal{V}_{r}\nis a single layer extracted from the 3-D visibility map\nğ’±\n\\mathcal{V}\nby fixing the radial distance\nr\nr\n. Our next goal is to update the complete SSDF\nğ’Ÿ\n\\mathcal{D}\nfor the entire map\nğ’±\n\\mathcal{V}\n, enabling the trackers to query the visibility in 3-D space by\nğ’Ÿ\nâ€‹\n(\np\n)\n\\mathcal{D}(\\textbf{p})\n.\nğ’Ÿ\nâ€‹\n(\np\n)\n\\mathcal{D}(\\textbf{p})\nis defined by the value of the corresponding 2-D SSDF layer at\nr\np\nr_{\\textbf{p}}\n:\nğ’Ÿ\nâ€‹\n(\np\n)\n=\nğ’Ÿ\nr\np\nâ€‹\n(\nv\np\n)\n,\n\\mathcal{D}(\\textbf{p})=\\mathcal{D}_{r_{\\textbf{p}}}(\\textbf{v}_{\\textbf{p}}),\n(5)\nwhere\np\nâˆˆ\nâ„\n3\n\\textbf{p}\\in\\mathbb{R}^{3}\nis the queried position,\nr\np\nr_{\\textbf{p}}\nis the radial distance at position\np\n, and\nv\np\n\\textbf{v}_{\\textbf{p}}\nis the direction vector at\np\n. A straightforward approach to update the complete 3-D SSDF\nğ’Ÿ\n\\mathcal{D}\nis traversing all the radial distances\nr\nk\nr_{k}\ndiscretized in\nğ’±\n\\mathcal{V}\nand repetitively applying the two-phase algorithm to the 2-D SSDF layer at each\nr\nk\nr_{k}\n. However, this brutal approach leads to large overall computational overhead, especially when the resolution of\nr\nk\nr_{k}\nis high. Thus, there is a necessity for a more efficient method to construct the 3-D SSDFs.\nFigure 5:\nAn illustration of the incremental SSDF update strategy on a 2-D visibility map. The SSDF is updated layer by layer, starting from the outermost 1-D visibility layer at\nr\n6\nr_{6}\nto the innermost layer at\nr\n1\nr_{1}\n. Proceeding to each layer, only the newly visible cells (in red boxes) and value-changed cells (in blue boxes) are identified and updated to compute the SSDF. In this example, only two cells in blue boxes require new value calculations throughout the update.\nAs discussed in Sec.\nIV-A\n, LOS visibility has a crucial monotonic property: along each direction\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\n, visibility decreases as the radial distance\nr\nk\nr_{k}\nincreases. Thus, among all the 2-D visibility map layers in\nğ’±\n\\mathcal{V}\n, the outermost layer\nğ’±\nr\nm\nâ€‹\na\nâ€‹\nx\n\\mathcal{V}_{r_{max}}\nat radial distance\nr\nm\nâ€‹\na\nâ€‹\nx\nr_{max}\nhas the fewest visible grids. As the radial distance\nr\nk\nr_{k}\ndecreases from\nr\nm\nâ€‹\na\nâ€‹\nx\nr_{max}\ntowards the origin, the number of visible grids in layer\nğ’±\nr\nk\n\\mathcal{V}_{r_{k}}\nincreases monotonically. Drawing inspiration from incremental updates for ESDFs\n[\n34\n,\n35\n,\n36\n,\n37\n]\n, we propose an incremental strategy for 3-D SSDF computation leveraging this monotonic property. Let\nN\nr\nN_{r}\ndenote the resolution of the radial dimension. We begin by updating the 2-D SSDF of the outermost visibility map layer\nğ’±\nr\nm\nâ€‹\na\nâ€‹\nx\n\\mathcal{V}_{r_{max}}\nat radial index\nk\n=\nN\nr\nâˆ’\n1\nk=N_{r}-1\nusing the two-phase algorithm. Then, for the next inner layer at\nk\n=\nN\nr\nâˆ’\n2\nk=N_{r}-2\n, which must contain the same or more visible grids, we simply need to insert the newly incremented visible grids from the current layer into the SSDF of the last outermost layer. This incremental insertion process continues sequentially, with each layer being updated based on the SSDF of the previous one, until the innermost layer, with index\nk\n=\n0\nk=0\n, is completed. Fig.\n5\nillustrates our incremental strategy using a 2-D visibility map.\nTo implement our strategy, the incremental algorithm in FIESTA\n[\n34\n]\nis adopted. Given that our strategy solely concerns the insertion operation, we only use the\ninsertQueue\nin\n[\n34\n]\nto hold the newly incremented visible grids of each layer. These increments have already been collected in Alg.\n1\nwithin an auxiliary array\nS\nS\nof length\nN\nr\nN_{r}\n, where the entry\nS\nâ€‹\n[\nk\n]\nS[k]\ncontains all the directions whose state changes from occluded to visible at the\nk\nth\nk^{\\text{th}}\nlayer. In Alg.\n4\n, we employ the Breadth-First-Search (BFS) method in\n[\n34\n]\nto insert these newly visible directions at the\nk\nth\nk^{\\text{th}}\nlayer into the SSDF of the previous\n(\nk\n+\n1\n)\nth\n(k+1)^{\\text{th}}\nlayer. Two arrays,\nB\nâ€‹\n[\ni\n,\nj\n]\nB[i,j]\nand\nğ’Ÿ\nâ€‹\n[\ni\n,\nj\n]\n\\mathcal{D}[i,j]\n, record the grid\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\nâ€™s closest visibility boundary and its spherical distance transform. BFS is used to update the neighbors by comparison. Since the BFS-based incremental algorithm cannot be perfectly accurate, as analyzed in\n[\n34\n]\n, a comparison with the ground-true SSDF values is provided in Sec.\nVIII-E\nto numerically validate the proposed algorithm. By sequentially applying Alg.\n4\nto the layers from the\n(\nN\nr\nâˆ’\n2\n)\nth\n(N_{r}-2)^{\\text{th}}\nto the innermost one, every grid in the 3-D visibility map\nğ’±\n\\mathcal{V}\nis assigned a spherical distance value, completing the update of the entire 3-D SSDF\nğ’Ÿ\nâ€‹\n(\np\n)\n\\mathcal{D}(\\textbf{p})\n. Fig.\n6\nvisualized an example of updated SSDF.\nAlgorithm 4\nIncremental SSDF Update\n1:\ni\nâ€‹\nn\nâ€‹\ns\nâ€‹\ne\nâ€‹\nr\nâ€‹\nt\nâ€‹\nQ\nâ€‹\nu\nâ€‹\ne\nâ€‹\nu\nâ€‹\ne\ninsertQueue\nas the queue for SSDF inserting;\nk\nk\nis the radial index of the current SSDF layer to update; The array\nB\nâ€‹\n[\ni\n,\nj\n]\nB[i,j]\nrecords grid\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\nâ€™s closest visibility boundary; The array\nğ’Ÿ\nâ€‹\n[\ni\n,\nj\n]\n\\mathcal{D}[i,j]\nrecords the spherical distance transform at\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\n; Arrays\nB\nB\nand\nğ’Ÿ\n\\mathcal{D}\nare initialized by the results of the previous\n(\nk\n+\n1\n)\nt\nâ€‹\nh\n(k+1)^{th}\nlayer;\n2:\nthe SSDF of the\nk\nth\nk^{\\text{th}}\nlayer is updated;\n3:\nInitialize:\ni\nâ€‹\nn\nâ€‹\ns\nâ€‹\ne\nâ€‹\nr\nâ€‹\nt\nâ€‹\nQ\nâ€‹\nu\nâ€‹\ne\nâ€‹\nu\nâ€‹\ne\nâ†\nE\nâ€‹\nm\nâ€‹\np\nâ€‹\nt\nâ€‹\ny\ninsertQueue\\leftarrow Empty\n;\n4:\nfor\neach newly visible direction\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\nin\nS\nâ€‹\n[\nk\n]\nS[k]\ndo\n5:\nB\nâ€‹\n[\ni\n,\nj\n]\nâ†\n(\nÎ¸\ni\n,\nÏ•\nj\n)\nB[i,j]\\leftarrow(\\theta_{i},\\phi_{j})\n;\n6:\nğ’Ÿ\nâ€‹\n[\ni\n,\nj\n]\nâ†\n0\n\\mathcal{D}[i,j]\\leftarrow 0\n;\n7:\ni\nâ€‹\nn\nâ€‹\ns\nâ€‹\ne\nâ€‹\nr\nâ€‹\nt\nâ€‹\nQ\nâ€‹\nu\nâ€‹\ne\nâ€‹\nu\nâ€‹\ne\ninsertQueue\n.PUSH(\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\n);\n8:\nend\nfor\n9:\nwhile\ni\nâ€‹\nn\nâ€‹\ns\nâ€‹\ne\nâ€‹\nr\nâ€‹\nt\nâ€‹\nQ\nâ€‹\nu\nâ€‹\ne\nâ€‹\nu\nâ€‹\ne\ninsertQueue\nnot empty\ndo\n10:\n(\nÎ¸\ni\n,\nÏ•\nj\n)\nâ†\ni\nâ€‹\nn\nâ€‹\ns\nâ€‹\ne\nâ€‹\nr\nâ€‹\nt\nâ€‹\nQ\nâ€‹\nu\nâ€‹\ne\nâ€‹\nu\nâ€‹\ne\n(\\theta_{i},\\phi_{j})\\leftarrow insertQueue\n.FRONT();\n11:\ni\nâ€‹\nn\nâ€‹\ns\nâ€‹\ne\nâ€‹\nr\nâ€‹\nt\nâ€‹\nQ\nâ€‹\nu\nâ€‹\ne\nâ€‹\nu\nâ€‹\ne\ninsertQueue\n.POP();\n12:\nfor\neach neighbor\n(\nÎ¸\nm\n,\nÏ•\nn\n)\n(\\theta_{m},\\phi_{n})\nof\n(\nÎ¸\ni\n,\nÏ•\nj\n)\n(\\theta_{i},\\phi_{j})\ndo\n13:\nif\nâ„’\nâ€‹\n{\nB\nâ€‹\n[\ni\n,\nj\n]\n,\n(\nÎ¸\nm\n,\nÏ•\nn\n)\n}\n<\nğ’Ÿ\nâ€‹\n[\nm\n,\nn\n]\n\\mathcal{L}\\{B[i,j],(\\theta_{m},\\phi_{n})\\}<\\mathcal{D}[m,n]\nthen\n14:\nB\nâ€‹\n[\nm\n,\nn\n]\nâ†\nB\nâ€‹\n[\ni\n,\nj\n]\nB[m,n]\\leftarrow B[i,j]\n;\n15:\nğ’Ÿ\nâ€‹\n[\nm\n,\nn\n]\nâ†\nâ„’\nâ€‹\n{\nB\nâ€‹\n[\ni\n,\nj\n]\n,\n(\nÎ¸\nm\n,\nÏ•\nn\n)\n}\n\\mathcal{D}[m,n]\\leftarrow\\mathcal{L}\\{B[i,j],(\\theta_{m},\\phi_{n})\\}\n;\n16:\ni\nâ€‹\nn\nâ€‹\ns\nâ€‹\ne\nâ€‹\nr\nâ€‹\nt\nâ€‹\nQ\nâ€‹\nu\nâ€‹\ne\nâ€‹\nu\nâ€‹\ne\ninsertQueue\n.PUSH(\n(\nÎ¸\nm\n,\nÏ•\nn\n)\n(\\theta_{m},\\phi_{n})\n);\n17:\nend\nif\n18:\nend\nfor\n19:\nend\nwhile\nIV-D\nSSDFs Update on Target Prediction\nFigure 6:\nAn example of the SSDF computed for an occluded scene.\n(a)\nThe SSDF values on the outermost spherical surface.\n(b)\nThe horizontal cross-section of the updated SSDF at the\nX\nX\n-\nY\nY\nplane.\nThe last subsections outlined the procedures to update an SSDF around a target in 3-D spaces. To apply these SSDFs in target tracking, we first predict the targetâ€™s movements over a future time horizon. A linear velocity-based interpolation module is employed to render a sequence of future target positions according to a time step\nÎ´\nâ€‹\nT\n\\delta T\nand a prediction horizon\nT\np\nT_{p}\n. The target position sequence is expressed as\nğ’¬\nt\nâ€‹\na\nâ€‹\nr\nâ€‹\ng\nâ€‹\ne\nâ€‹\nt\n=\n{\nğƒ\nk\nâˆˆ\nâ„\n3\n|\n0\nâ‰¤\nk\nâ‰¤\nN\np\n,\nt\nk\n=\nk\nâ‹…\nÎ´\nâ€‹\nT\n}\n,\n\\mathcal{Q}_{target}=\\{\\bm{\\xi}_{k}\\in\\mathbb{R}^{3}\\;|\\;0\\leq k\\leq N_{p},\\;t_{k}=k\\cdot\\delta T\\},\n(6)\nwhere\nN\np\nN_{p}\nis the total prediction step number and\nt\nk\nt_{k}\nis the timestamp corresponding to the target position at the\nk\nt\nâ€‹\nh\nk^{th}\nstep from now. To ensure SSDF-based visibility constraints throughout the tracking, we update SSDFs at every predicted position\nğƒ\nk\n\\bm{\\xi}_{k}\n. As the computations for these\nN\np\nN_{p}\nSSDFs are independent, they are updated in parallel, keeping the overall computation time efficient. To enable gradient-based optimization with SSDFs, we perform interpolation on the SSDFs to generate the required distance gradients. The application of SSDF for visibility-aware planning is detailed in Sec.\nV\n.\nV\nVisibility-aware Cooperative Swarm Tracking\nIncorporating the targetâ€™s visibility into planning is essential for robust aerial tracking of passive targets. To ensure continuous target observation, trackers must avoid environmental occlusions, maintain an appropriate tracking distance, and keep the target positioned in the sensorâ€™s FOV. Meanwhile, the swarm system necessitates the planner to effectively coordinate the multiple trackers, such that each drone can avoid teammate occlusion and make full use of the space surrounding the target. In this section, we introduce several cost functions that model these metrics for swarm tracking. These costs will be utilized in our front-end searching and back-end optimization to quantify the tracking performance. In what follows, the ego droneâ€™s position is denoted as\np\nâˆˆ\nâ„\n3\n\\textbf{p}\\in\\mathbb{R}^{3}\nand assigned as the\ni\nt\nâ€‹\nh\ni^{th}\ndrone in the swarm, with the target position\nğƒ\nâˆˆ\nâ„\n3\n\\bm{\\xi}\\in\\mathbb{R}^{3}\n.\nN\nN\ndenotes the number of drones in the swarm, and\np\nj\n\\textbf{p}_{j}\ndenotes the position of the\nj\nt\nâ€‹\nh\nj^{th}\ndrone in the swarm.\nV-A\nEnvironmental Occlusion Cost\nThis cost\nğ’¥\nv\nâ€‹\ni\nâ€‹\ns\n\\mathcal{J}_{vis}\nis introduced to preserve the targetâ€™s LOS visibility against static obstacle occlusions. The occlusion relationships in the environment w.r.t. the target are encoded by an SSDF introduced in Sec.\nIV\n. The environmental occlusion cost at position\np\nis designed as\nğ’¥\nv\nâ€‹\ni\nâ€‹\ns\n=\nâˆ’\nğ’Ÿ\nâ€‹\n(\np\n)\n.\n\\mathcal{J}_{vis}=-\\mathcal{D}(\\textbf{p}).\n(7)\nAs stated in Sec.\nIV\n,\nğ’Ÿ\nâ€‹\n(\np\n)\n\\mathcal{D}(\\textbf{p})\nreturns the angular distance to the closest visibility boundary around\np\nif it is occluded, otherwise\nğ’Ÿ\nâ€‹\n(\np\n)\n\\mathcal{D}(\\textbf{p})\nreturns zero. Both the front-end searching and back-end optimization use this cost to impose penalties on the occluded areas, thereby preventing visibility loss. To conduct numerical optimization with\nğ’¥\nv\nâ€‹\ni\nâ€‹\ns\n\\mathcal{J}_{vis}\n, we derive the cost gradient as\nâˆ‚\nğ’¥\nv\nâ€‹\ni\nâ€‹\ns\n/\nâˆ‚\np\n=\nâˆ’\nâˆ‡\nğ’Ÿ\nâ€‹\n(\np\n)\n,\n{\\partial\\mathcal{J}_{vis}}/{\\partial\\textbf{p}}=-\\nabla\\mathcal{D}(\\textbf{p}),\n(8)\nwhere\nâˆ‡\nğ’Ÿ\nâ€‹\n(\np\n)\n\\nabla\\mathcal{D}(\\textbf{p})\nis the interpolated gradient of SSDF at point\np\n.\nV-B\nField-of-View Cost\nThis field-of-view (FOV) cost is to ensure that the LOS between the target and tracker can always remain within the LiDARâ€™s FOV during the flight, thus the target can be sensed. We denote the world frame using\nw\nw\n, the body frame using\nb\nb\n, and the LiDAR frame using\nl\nl\n. In our system, the LiDARâ€™s FOV is attached to the droneâ€™s body frame. Transforming the target position\nğƒ\nw\n{}^{w}\\bm{\\xi}\nin the world frame to the LiDAR frame, we can derive\nğƒ\nl\n{}^{l}\\bm{\\xi}\nas:\nl\nğƒ\n=\nw\nb\nğ‘\n(\nw\nb\nq\n)\n(\nw\nğƒ\nâˆ’\np\n)\nâˆ’\nb\nl\nt\n,\n^{l}\\bm{\\xi}=\\,_{w}^{b}\\mathbf{R}(_{w}^{b}\\textbf{q})\\,(^{w}\\bm{\\xi}-\\textbf{p})-\\,_{b}^{l}\\textrm{{t}},\n(9)\nwhere\nq\nw\nb\n{}_{w}^{b}\\textbf{q}\nis the rotation quaternion between the world and body frame,\nğ‘\nw\nb\n{}_{w}^{b}\\mathbf{R}\nis the corresponding rotation matrix, and\nt\nb\nl\n{}_{b}^{l}\\textbf{t}\nis the translation between the LiDAR frame and the body frame. The FOV cost formulation\nğ’¥\nf\nâ€‹\no\nâ€‹\nv\n\\mathcal{J}_{fov}\nshould consist of vertical and horizontal parts. For omnidirectional sensors,\ne.g.\nMID360 LiDAR, only the vertical FOV needs to be regulated to contain the target. We denote the LiDARâ€™s vertical FOV as\nÎ¸\nv\nâ€‹\nr\nâ€‹\nt\n\\theta_{vrt}\n. Given the target at\nğƒ\nl\n=\n[\nx\nl\n,\ny\nl\n,\nz\nl\n]\nT\n{}^{l}\\bm{\\xi}=[x_{l},y_{l},z_{l}]^{T}\n, we use an auxiliary point\np\nc\n\\textbf{p}_{c}\n, which is the vertical projection of\nğƒ\nl\n{}^{l}\\bm{\\xi}\non the angle bisector of the vertical FOV, as depicted in Fig.\n7\n(c). It can be written as\np\nc\n=\n[\nx\nl\n,\ny\nl\n,\nx\nl\n2\n+\ny\nl\n2\nâ‹…\ntan\nâ¡\nÎ¸\nc\nâ€‹\nt\nâ€‹\nr\n]\n,\n\\textbf{p}_{c}=[x_{l},y_{l},\\sqrt{x_{l}^{2}+y_{l}^{2}}\\cdot\\tan{\\theta_{ctr}}],\n(10)\nwhere\nÎ¸\nc\nâ€‹\nt\nâ€‹\nr\n\\theta_{ctr}\nis the angle between the bisector of the vertical FOV and the horizon level. As shown in Fig.\n7\n(c), to contain the target in vertical FOV, the constraint expects the vertical angle\nÏˆ\nv\nâ€‹\nr\nâ€‹\nt\n\\psi_{vrt}\nbetween local vectors\nğƒ\nl\n{}^{l}\\bm{\\xi}\nand\np\nc\n\\textbf{p}_{c}\nto satisfy\nÏˆ\nv\nâ€‹\nr\nâ€‹\nt\n<\nÎ¸\nv\nâ€‹\nr\nâ€‹\nt\n/\n2\n\\psi_{vrt}<\\theta_{vrt}/2\n, thus the vertical FOV cost can be formulated as\nğ’¥\nf\nâ€‹\no\nâ€‹\nv\nv\nâ€‹\nr\nâ€‹\nt\n=\ncos\nâ¡\nÎ¸\nv\nâ€‹\nr\nâ€‹\nt\n2\nâˆ’\nğƒ\nl\nâ‹…\np\nc\nâˆ¥\nl\nğƒ\nâˆ¥\nâˆ¥\np\nc\nâˆ¥\n,\n\\mathcal{J}_{fov}^{vrt}=\\cos{\\frac{\\theta_{vrt}}{2}}-\\frac{{}^{l}\\bm{\\xi}\\cdot\\textbf{p}_{c}}{\\|^{l}\\bm{\\xi}\\|\\,\\|\\textbf{p}_{c}\\|},\n(11)\nFigure 7:\n(a)\nAn overview of the Mid360 FOV.\n(b)\nAn overview of the Avia FOV.\n(c)\nThe vertical cross-section of the Mid360 FOV, with the angular bisector marked by the green arrow. To meet the FOV constraint, the angle\nÏˆ\nv\nâ€‹\nr\nâ€‹\nt\n\\psi_{vrt}\nshould be less than\nÎ¸\nv\nâ€‹\nr\nâ€‹\nt\n/\n2\n\\theta_{vrt}/2\n.\n(d)\nThe horizontal cross-section of the Avia FOV, where the angle\nÏˆ\nh\nâ€‹\nr\nâ€‹\nz\n\\psi_{hrz}\nis expected to be zero to align with the target.\nFor sensors with conic FOVs,\ne.g.\nAvia LiDAR, the horizontal FOV cost should be additionally imposed to make the drone face towards the target. The horizontal angle\nÏˆ\nh\nâ€‹\nr\nâ€‹\nz\n\\psi_{hrz}\nbetween the target and the heading axis in Fig.\n7\n(d) is expected to be zero. Thus, the horizontal FOV cost can be given as\nğ’¥\nf\nâ€‹\no\nâ€‹\nv\nh\nâ€‹\nr\nâ€‹\nz\n=\n1\nâˆ’\nx\nl\nx\nl\n2\n+\ny\nl\n2\n.\n\\mathcal{J}_{fov}^{hrz}=1-\\frac{x_{l}}{\\sqrt{x_{l}^{2}+y_{l}^{2}}}.\n(12)\nFor a FOV that perfectly heads towards the target, this horizontal cost equals zero. Itâ€™s worth noticing that the droneâ€™s position\np\nand attitude\nq\nin Eq.\n9\ncan jointly affect the target position\nğƒ\nl\n{}^{l}\\bm{\\xi}\nin LiDAR frame, hence the proposed\nğ’¥\nf\nâ€‹\no\nâ€‹\nv\n\\mathcal{J}_{fov}\nis an\nS\nâ€‹\nE\nâ€‹\n(\n3\n)\nSE(3)\ncost. The gradient w.r.t.\np\nis derived by chain rule as\nâˆ‚\nğ’¥\nf\nâ€‹\no\nâ€‹\nv\nâˆ‚\np\n=\nâˆ‚\nğƒ\nğ’\nâˆ‚\np\nâ€‹\nâˆ‚\nğ’¥\nf\nâ€‹\no\nâ€‹\nv\nâˆ‚\nğƒ\nğ’\n,\nâˆ‚\nğƒ\nğ’\nâˆ‚\np\n=\nâˆ’\nw\nb\nğ‘\nT\n.\n\\frac{\\partial\\mathcal{J}_{fov}}{\\partial\\textbf{p}}=\\frac{\\partial\\,\\bm{{}^{l}\\xi}}{\\partial\\textbf{p}}\\frac{\\partial\\mathcal{J}_{fov}}{\\partial\\,\\bm{{}^{l}\\xi}},\\;\\;\\frac{\\partial\\,\\bm{{}^{l}\\xi}}{\\partial\\textbf{p}}=-_{w}^{b}\\mathbf{R}^{T}.\n(13)\nThe gradient w.r.t. the attitude quaternion can be derived as\nâˆ‚\nğ’¥\nf\nâ€‹\no\nâ€‹\nv\nâˆ‚\nw\nb\nq\n=\n(\nâˆ‚\nğƒ\nğ’\nâˆ‚\nw\nb\nq\nâ€‹\nâˆ‚\nğ’¥\nf\nâ€‹\no\nâ€‹\nv\nâˆ‚\nğƒ\nğ’\n)\nâˆ’\n1\n,\n\\frac{\\partial\\mathcal{J}_{fov}}{\\partial\\,_{w}^{b}\\textbf{q}}=(\\frac{\\partial\\,\\bm{{}^{l}\\xi}}{\\partial\\,_{w}^{b}\\textbf{q}}\\frac{\\partial\\mathcal{J}_{fov}}{\\partial\\,\\bm{{}^{l}\\xi}})^{-1},\n(14)\nwhere\n(\nâ‹…\n)\nâˆ’\n1\n(\\cdot)^{-1}\ndenotes the inversion of the quaternion, and\nâˆ‚\nğƒ\nğ’\n/\nâˆ‚\nw\nb\nq\n\\partial\\,\\bm{{}^{l}\\xi}/\\partial\\,_{w}^{b}\\textbf{q}\nrefers to the Jacobian of a rotated vector w.r.t. the quaternion. To optimize the proposed FOV costs in the droneâ€™s flat-output space in the latter Sec.\nVII\n, the above gradients should be further transformed to gradients w.r.t. the flat-output variables according to the models in\n[\n38\n]\n.\nV-C\nTracking Distance Cost\nA drone is expected to track the target from a proper distance. We expect the Euclidean distance\nd\nd\nto satisfy\nd\nl\nâ€‹\nb\nâ‰¤\nd\nâ‰¤\nd\nu\nâ€‹\nb\nd_{lb}\\leq d\\leq d_{ub}\n, where\nd\nu\nâ€‹\nb\nd_{ub}\nand\nd\nl\nâ€‹\nb\nd_{lb}\nare the upper and lower bounds of the desired tracking distance. Then the cost is defined as\nğ’¥\nd\nâ€‹\ni\nâ€‹\ns\n=\n{\n5\nâ€‹\n(\nd\nl\nâ€‹\nb\nâˆ’\nd\n)\n3\n,\nd\n<\nd\nl\nâ€‹\nb\n,\n0\n,\nd\nl\nâ€‹\nb\nâ‰¤\nd\nâ‰¤\nd\nu\nâ€‹\nb\n,\n(\nd\nâˆ’\nd\nu\nâ€‹\nb\n)\n2\n/\n2\n,\nd\n>\nd\nu\nâ€‹\nb\n.\n\\mathcal{J}_{dis}=\\left\\{\\begin{array}[]{ll}5\\,(d_{lb}-d)^{3},&d<d_{lb},\\\\\n0,&d_{lb}\\leq d\\leq d_{ub},\\\\\n(d-d_{ub})^{2}\\,/\\,2,&d>d_{ub}.\\\\\n\\end{array}\\right.\n(15)\nA mild penalty is applied for exceeding\nd\nu\nâ€‹\nb\nd_{ub}\ndue to the LiDARâ€™s long sensing range, but\nd\nl\nâ€‹\nb\nd_{lb}\nis strictly enforced for the targetâ€™s safety. Unlike the work\n[\n6\n,\n7\n]\n, we do not rigidly align the altitude of the trackers with the target to meet the FOV requirements. With the costs elaborated in Sec.\nV-B\n, the drones can flexibly adjust their altitude according to the needs.\nV-D\nTeammate Occlusion Cost\nTo enforce teammate occlusion avoidance, trackers must maintain a minimum angular clearance\nÎ¸\nc\n\\theta_{c}\nfrom each other relative to the target, ensuring the LOS between the tracker and the target is not blocked by teammates. Let\nÎ·\nj\n\\eta_{j}\ndenotes the cosine value of the angular distance between teammate\nj\nj\nand the ego drone at\np\ni\n\\textbf{p}_{i}\n, we have\nÎ·\nj\n=\n(\np\ni\nâˆ’\nğƒ\n)\nâ‹…\n(\np\nj\nâˆ’\nğƒ\n)\nâ€–\np\ni\nâˆ’\nğƒ\nâ€–\nâ€‹\nâ€–\np\nj\nâˆ’\nğƒ\nâ€–\n.\n\\eta_{j}=\\frac{(\\textbf{p}_{i}-\\bm{\\xi})\\cdot(\\textbf{p}_{j}-\\bm{\\xi})}{\\|\\textbf{p}_{i}-\\bm{\\xi}\\|\\,\\|\\textbf{p}_{j}-\\bm{\\xi}\\|}.\n(16)\nWhen the angular distance occlusion clearance\nÎ¸\nc\n\\theta_{c}\nis violated,\ni.e.\nÎ·\nc\n>\ncos\nâ¡\nÎ¸\nc\n\\eta_{c}>\\cos\\theta_{c}\n, we impose a occlusion cost term to repulse the tracker away from the teammate\nj\nj\n, then\nğ’¥\nt\nâ€‹\no\nâ€‹\nc\n\\mathcal{J}_{toc}\ndenotes the total collection of the costs on all other teammate drones:\nğ’¥\nt\nâ€‹\no\nâ€‹\nc\n=\nâˆ‘\nj\nâ‰ \ni\nN\n(\nÎ·\nj\nâˆ’\ncos\nâ¡\nÎ¸\nc\n)\n3\n.\n\\mathcal{J}_{toc}=\\sum_{j\\neq i}^{N}\\,(\\eta_{j}-\\cos\\theta_{c})^{3}.\n(17)\nThe gradients can be easily derived by differentiating Eq.\n16\n.\nV-E\nSwarm Distribution Cost\nIn addition to keeping the minimum teammate angular clearance\nÎ¸\nc\n\\theta_{c}\n, the trackers are also expected to achieve uniform multidirectional target coverage in three-dimensional space. The 3-D equidistant encirclement distribution serves two key purposes. Firstly, it allows the swarm to make full use of the vicinity around the target, providing each tracker with the largest angular space to respond to any adverse situations like occlusions or collisions. Secondly, for a swarm tracking system using LiDAR sensors, this strategy can maximize the diversity of target measurement angles, thus the whole swarm can obtain a more complete point cloud of the target for further target state estimation with multi-source fusion. Although for the two-dimensional space, the maximum evenly-spaced angular distance for a swarm of\nN\nN\ndrones is clearly\n2\nâ€‹\nÏ€\n/\nN\n2\\pi/N\n, the same problem in 3-D space is nontrivial. In this context, we introduce a new formulation for the 3-D scenario.\nFigure 8:\nDistributions of a four-drone swarm on a sphere centered at the target.\n(a)\nConventional 2-D equidistant distribution that confines all drones in a plane.\n(b)\nThe 3-D uniform encirclement distribution. Surrounding the target as a nucleus, the drones are modeled as electrons repelling each other with Coulombâ€™s forces\nF\nC\nâ€‹\no\nâ€‹\nu\nâ€‹\nl\nF_{Coul}\n, forming an optimal tetrahedral configuration to minimize total electrostatic potential energy.\nInspired by the famous Thomson problem\n[\n39\n]\n, we notice that our desired equidistant target encirclement formation resembles the geometry of the optimal configuration described in this problem. The Thomson problem is to determine the minimum electrostatic potential energy configuration of\nN\nN\nelectrons constrained on the surface of a unit sphere that repel each other with a Coulomb force, which results in a spatial distribution that exhibits our desired distancing feature. Thus, we transform the initial distribution requirement into the goal of minimizing the total electrostatic potential energy of the swarm. We adopt a logarithmic variant of the original Thomson problem as our cost formulation, which is from the 7th of the eighteen unsolved mathematics problems proposed by Steve Smale - â€Distribution of points on the 2-sphereâ€\n[\n40\n]\n. The distribution cost for the\ni\nt\nâ€‹\nh\ni^{th}\ndrone is then designed as\nğ’¥\nf\nâ€‹\nr\nâ€‹\nm\n=\nâˆ‘\nj\nâ‰ \ni\nN\nk\ne\nâ€‹\nlog\nâ¡\n1\nâ€–\np\ni\nâˆ’\np\nj\nâ€–\n,\n\\mathcal{J}_{frm}=\\sum_{j\\neq i}^{N}k_{e}\\log\\frac{1}{\\|\\textbf{p}_{i}-\\textbf{p}_{j}\\|},\n(18)\nwhere\nk\ne\nk_{e}\nis an energy constant. A uniform multidirectional distribution could minimize this logarithmic potential energy objective. Fig.\n8\nillustrates the model and compares the proposed 3-D distribution with the conventional planar formation. Compared to the 2-D square formation, the proposed distribution achieves a\n109.5\nâˆ˜\n109.5^{\\circ}\nangle between teammate lines of sight, offering greater angular space for each tracker and increased diversity in viewing angles. This proposed cost describes the ideal distribution. However, in practical tracking, visibility requirements, including FOV compliance, must be prioritized. The FOV configurations in the swarm may not always allow for achieving the ideal distribution, but the planner strives to optimize the distribution cost within the FOV constraints.\nV-F\nOther Costs\nOther costs serve as the basic requirements for safe swarm navigation. Safe flight corridors are employed for obstacle avoidance. The trajectory of each tracker should be constrained in the polyhedral safe corridors. The formulation is given as\nğ’¥\no\nâ€‹\nb\nâ€‹\ns\n=\nğ€\nc\nâ€‹\np\nâˆ’\nb\nc\n,\n\\mathcal{J}_{obs}=\\mathbf{A}_{c}\\,\\textbf{p}-b_{c},\n(19)\nwhere\nğ€\nc\n\\mathbf{A}_{c}\nand\nb\nc\nb_{c}\nare from the\nâ„‹\n\\mathcal{H}\n-representation of corridors. For dynamic Feasibility, we limit the maximum amplitudes of the trackerâ€™s velocity, acceleration, and angular velocity to keep the trajectory executable. The velocity cost is given as\nğ’¥\nd\nâ€‹\ny\nâ€‹\nn\nv\nâ€‹\ne\nâ€‹\nl\n=\nâ€–\nv\nâ€–\n2\nâˆ’\nv\nm\nâ€‹\na\nâ€‹\nx\n2\n,\n\\mathcal{J}_{dyn}^{vel}=\\|\\textbf{v}\\|^{2}-v_{max}^{2},\n(20)\nwhere\nv\ndenotes the velocity and\nv\nm\nâ€‹\na\nâ€‹\nx\nv_{max}\ndenotes the upper bounds. The costs for acceleration and angular velocity are in the same form. To enforce inter-vehicle collision avoidance, we expect each drone to maintain a distance clearance\nr\ns\nr_{s}\nto all other teammates. For the ego drone at\np\ni\n\\textbf{p}_{i}\nand one teammate drone at\np\nj\n\\textbf{p}_{j}\n, the reciprocal clearance cost is given as\nğ’¥\ns\nâ€‹\nw\nâ€‹\nm\nj\n=\nmax\nâ¡\n{\nr\ns\n2\nâˆ’\nâ€–\np\ni\nâˆ’\np\nj\nâ€–\n2\n,\n0\n}\n.\n\\mathcal{J}_{swm}^{j}=\\max\\{r_{s}^{2}-\\|\\textbf{p}_{i}-\\textbf{p}_{j}\\|^{2},0\\}.\n(21)\nThen the total clearance cost\nğ’¥\ns\nâ€‹\nw\nâ€‹\nm\n\\mathcal{J}_{swm}\nfor ego drone\ni\ni\nis the sum of the costs on all other teammate drones.\nVI\nKinodynamic Searching\nOur kinodynamic front-end generates a reference path by expanding motion primitives in a discretized control space. Unlike traditional hybrid A* searchers\n[\n41\n,\n42\n,\n43\n]\nthat prioritize minimizing control effort along the path, our method scores each primitive based on the tracking performance metrics defined in the previous section, ensuring high consistency between the front-end and the overall task objectives.\nVI-A\nPrimitive Expansion and Rejection\nThe state\nx\nâˆˆ\nâ„\n6\n\\textbf{x}\\in\\mathbb{R}^{6}\nof the tracker drone includes its position\np\n=\n[\np\nx\n,\np\ny\n,\np\nz\n]\nT\n\\textbf{p}=[p_{x},p_{y},p_{z}]^{T}\nand velocity\nv\n=\n[\nv\nx\n,\nv\ny\n,\nv\nz\n]\nT\n\\textbf{v}=[v_{x},v_{y},v_{z}]^{T}\n. Acceleration is used as the control input for each dimension, and the input space is discretized as\nu\nd\n=\n{\nâˆ’\na\nm\nâ€‹\na\nâ€‹\nx\n,\n0\n,\na\nm\nâ€‹\na\nâ€‹\nx\n}\n\\textbf{u}_{d}=\\{-a_{max},0,a_{max}\\}\n, where\na\nm\nâ€‹\na\nâ€‹\nx\na_{max}\nis the acceleration limit. As mentioned in Sec.\nIV-D\n, a prediction module interpolates future target positions based on the time step\nÎ´\nâ€‹\nT\n\\delta T\nand prediction horizon\nT\np\nT_{p}\n, represented by Eq.\n6\n. In the front-end, motion primitives for the tracker drone are expanded directly using the prediction interval\nÎ´\nâ€‹\nT\n\\delta T\n, ensuring that the timestamp\nt\nk\nt_{k}\nof each new node\nx\nk\n\\textbf{x}_{k}\naligns with the stamp of target prediction\nğƒ\nk\n\\bm{\\xi}_{k}\n. Motion primitive expansion follows the double-integrator dynamics. To meet the task requirements and accelerate the searching, we design pruning strategies to reject the infeasible nodes in the primitive expansion process timely. After each expansion step, all resultant nodes are checked in terms of obstacle avoidance, dynamic feasibility, and inter-vehicle safety. For mutual collision avoidance, we first query the current positions of teammate drones on the broadcast trajectories. Then a node is considered safe if the distances between the node and all its teammates are larger than a clearance\nr\ns\nr_{s}\n. Fig.\n9\nshows examples of primitive rejection.\nFigure 9:\nAn illustration of the primitive selection mechanism in our task-oriented kinodynamic searcher. Primitives\nb\nand\nd\nare rejected by inter-UAV safety check and obstacle check, respectively. Primitives\na\nand\ne\nare penalized due to inter-UAV occlusion and environmental occlusion, respectively.\nVI-B\nCost Functions\nAfter the expansion and rejection process, every remaining node\nx\nk\n\\textbf{x}_{k}\nis assigned a cost\ng\nk\ng_{k}\nas a coarse assessment of its tracking quality with target\nğƒ\nk\n\\bm{\\xi}_{k}\n. We evaluate the performance of each node in terms of obstacle occlusion\nğ’¥\nv\nâ€‹\ni\nâ€‹\ns\n\\mathcal{J}_{vis}\n, tracking distance\nğ’¥\nd\nâ€‹\ni\nâ€‹\ns\n\\mathcal{J}_{dis}\n, teammate occlusion\nğ’¥\nt\nâ€‹\no\nâ€‹\nc\n\\mathcal{J}_{toc}\n, and swarm distribution\nğ’¥\nf\nâ€‹\nr\nâ€‹\nm\n\\mathcal{J}_{frm}\n. In the existing works\n[\n23\n,\n7\n]\n, the searcher penalizes the obstacle occlusion using a measure of voxel occupancy along the LOS between target and tracker, which is not yet a reasonable quantification for occlusion. The severity of occlusion should be defined by the difficulty of the tracker escaping from the invisible area, which can be described by the angular distance to the closest visibility boundary in SSDFs. Thus, we use the cost\nğ’¥\nv\nâ€‹\ni\nâ€‹\ns\n\\mathcal{J}_{vis}\nto penalize the occlusion. Synthesizing all the terms, we have the node cost\ng\nk\ng_{k}\nas\ng\nn\n=\n[\nğ’¥\nv\nâ€‹\ni\nâ€‹\ns\n,\nğ’¥\nd\nâ€‹\ni\nâ€‹\ns\n,\nğ’¥\nt\nâ€‹\no\nâ€‹\nc\n,\nğ’¥\nf\nâ€‹\nr\nâ€‹\nm\n]\nâ‹…\nw\n,\ng_{n}=[\\mathcal{J}_{vis},\\,\\mathcal{J}_{dis},\\,\\mathcal{J}_{toc},\\,\\mathcal{J}_{frm}]\\cdot w,\n(22)\nwhere\nw\nw\nis the weight vector to trade off cost priorities. In our planner, safety constraints are prioritized first, followed by LOS visibility requirements, and lastly swarm distribution considerations. The searching terminates when one primitive reaches the target prediction horizon\nT\np\nT_{p}\n, determining the final path. We utilize the remaining expansion time\nh\nn\n=\nT\np\nâˆ’\nt\nk\nh_{n}=T_{p}-t_{k}\nas a heuristic function to speed up the searching process.\nVI-C\nFlight Corridor Generation\nAfter finding the minimum-cost path, an efficient method in\n[\n44\n]\nis applied to generate a safe flight corridor of connected polyhedra along the path, each polyhedron is denoted as\nğ’«\n=\n{\nx\nâˆˆ\nâ„\n3\n|\nğ€\nc\nâ€‹\nx\nâ‰¤\nb\nc\n}\n.\n\\mathcal{P}=\\{x\\in\\mathbb{R}^{3}\\;|\\;\\mathbf{A}_{c}\\,x\\leq b_{c}\\}.\n(23)\nThe corridors will be used as the safe constraints.\nVII\nSpatiotemporal Trajectory Optimization\nVII-A\nTrajectory Optimization Problem Formulation\nIn this work, we use the MINCO representation\n[\n45\n]\n, a minimum control effort polynomial trajectory class to conduct spatiotemporal deformation of the flat-output trajectory\nğ”—\nM\nâ€‹\nI\nâ€‹\nN\nâ€‹\nC\nâ€‹\nO\n=\n\\displaystyle\\mathfrak{T}_{MINCO}=\n{\np\n(\nt\n)\n:\n[\n0\n,\nT\nÎ£\n]\nâ†¦\nâ„\nm\n|\nc\n=\nğ’\n(\nÏ±\n,\nT\n)\n,\n\\displaystyle\\{\\textbf{p}(t):[0,T_{\\Sigma}]\\mapsto\\mathbb{R}^{m}|\\;\\textbf{c}=\\mathcal{C}(\\bm{\\varrho},\\textbf{T}),\n(24)\nÏ±\nâˆˆ\nâ„\nm\nâ€‹\n(\nM\nâˆ’\n1\n)\n,\nT\nâˆˆ\nâ„\n>\n0\nM\n}\n,\n\\displaystyle\\bm{\\varrho}\\in\\mathbb{R}^{m(M-1)},\\textbf{T}\\in\\mathbb{R}_{>0}^{M}\\},\nwhere\nc\n=\n(\nc\n1\nT\n,\nâ‹¯\n,\nc\nM\nT\n)\nT\n\\textbf{c}=(\\textbf{c}_{1}^{T},\\cdots,\\textbf{c}_{M}^{T})^{T}\nis the polynomial coefficient,\nÏ±\n=\n(\nÏ±\n1\n,\nâ‹¯\n,\nÏ±\nM\nâˆ’\n1\n)\n\\bm{\\varrho}=(\\varrho_{1},\\cdots,\\varrho_{M-1})\nthe intermediate points between trajectory pieces,\nT\n=\n(\nT\n1\n,\nâ‹¯\n,\nT\nM\n)\nT\n\\textbf{T}=(T_{1},\\cdots,T_{M})^{T}\nthe time vector,\nğ’\nâ€‹\n(\nÏ±\n,\nT\n)\n\\mathcal{C}(\\bm{\\varrho},\\textbf{T})\nis the linear-complexity parameter mapping from\n[\n45\n]\n, and\nT\nÎ£\nT_{\\Sigma}\nis the total trajectory duration. For the\ns\ns\n-order\nğ”—\nM\nâ€‹\nI\nâ€‹\nN\nâ€‹\nC\nâ€‹\nO\ns\n\\mathfrak{T}_{MINCO}^{s}\n, a\nm\nm\n-dimensional\nM\nM\n-piece trajectory\np\nâ€‹\n(\nt\n)\n\\textbf{p}(t)\nis defined as\np\nâ€‹\n(\nt\n)\n=\np\ni\nâ€‹\n(\nt\nâˆ’\nt\ni\nâˆ’\n1\n)\n,\nâˆ€\nt\nâˆˆ\n[\nt\ni\nâˆ’\n1\n,\nt\ni\n)\n,\n\\textbf{p}(t)=\\textbf{p}_{i}(t-t_{i-1}),~~\\forall{t}\\in[t_{i-1},t_{i}),\nand the\ni\nt\nâ€‹\nh\ni^{th}\npiece trajectory is represented by a\n(\n2\nâ€‹\ns\nâˆ’\n1\n)\n(2s-1)\n-degree polynomial\np\ni\nâ€‹\n(\nt\n)\n=\nc\ni\nT\nâ€‹\nğœ·\nâ€‹\n(\nt\n)\n,\nâˆ€\nt\nâˆˆ\n[\n0\n,\nT\ni\n]\n.\n\\textbf{p}_{i}(t)=\\textbf{c}_{i}^{T}\\bm{\\beta}(t),~~\\forall{t}\\in[0,T_{i}].\nMINCO trajectories are all compactly parameterized by\nÏ±\n\\bm{\\varrho}\nand\nT\n. With mapping\nğ’\nâ€‹\n(\nÏ±\n,\nT\n)\n\\mathcal{C}(\\bm{\\varrho},\\textbf{T})\n, the cost of the polynomial trajectories in\nğ”—\nM\nâ€‹\nI\nâ€‹\nN\nâ€‹\nC\nâ€‹\nO\n\\mathfrak{T}_{MINCO}\ncan be evaluated by\nÏ±\n\\bm{\\varrho}\nand\nT\nby\nğ’¥\nâ€‹\n(\nÏ±\n,\nT\n)\n=\nâ„±\nâ€‹\n(\nc\n,\nT\n)\n=\nâ„±\nâ€‹\n(\nğ’\nâ€‹\n(\nÏ±\n,\nT\n)\n,\nT\n)\n.\n\\mathcal{J}(\\bm{\\varrho},\\textbf{T})=\\mathcal{F}(\\textbf{c},\\textbf{T})=\\mathcal{F}(\\mathcal{C}(\\bm{\\varrho},\\textbf{T}),\\textbf{T}).\nThus, we can conduct trajectory optimization over objective\nğ’¥\n\\mathcal{J}\nusing the gradients\nâˆ‚\nğ’¥\n/\nâˆ‚\nÏ±\n\\partial\\mathcal{J}/\\partial\\bm{\\varrho}\nand\nâˆ‚\nğ’¥\n/\nâˆ‚\nT\n\\partial\\mathcal{J}/\\partial\\textbf{T}\n, which are propagated from gradients\nâˆ‚\nâ„±\n/\nâˆ‚\nc\n\\partial\\mathcal{F}/\\partial\\textbf{c}\nand\nâˆ‚\nâ„±\n/\nâˆ‚\nT\n\\partial\\mathcal{F}/\\partial\\textbf{T}\naccordingly.\nWith\nğ”—\nM\nâ€‹\nI\nâ€‹\nN\nâ€‹\nC\nâ€‹\nO\ns\n\\mathfrak{T}_{MINCO}^{s}\n, the trajectory optimization problem can be formulated as follows:\nmin\nÏ±\n,\nT\n\\displaystyle\\min_{\\bm{\\varrho},\\textbf{T}}\\,\\,\nğ’¥\nE\n=\nâˆ«\n0\nT\nÎ£\nâ€–\np\n(\ns\n)\nâ€‹\n(\nt\n)\nâ€–\n2\nâ€‹\nğ‘‘\nt\n\\displaystyle\\mathcal{J}_{E}=\\int_{0}^{T_{\\Sigma}}\\|\\textbf{p}^{(s)}(t)\\|^{2}\\,dt\n(25a)\ns.t.\nT\nÎ£\n=\nT\np\n,\n\\displaystyle T_{\\Sigma}=T_{p},\n(25b)\nğ’¢\nâ€‹\n(\np\n[\ns\nâˆ’\n1\n]\nâ€‹\n(\nt\n)\n)\nâ‰¤\nğŸ\n,\nâˆ€\nt\nâˆˆ\n[\n0\n,\nT\nÎ£\n]\n,\n\\displaystyle\\mathcal{G}(\\textbf{p}^{[s-1]}(t))\\leq\\mathbf{0},\\,\\forall t\\in[0,T_{\\Sigma}],\n(25c)\nâ„‹\nâ€‹\n(\np\n[\ns\nâˆ’\n1\n]\nâ€‹\n(\nt\n)\n)\nâ‰¤\nğŸ\n,\nâˆ€\nt\nâˆˆ\nğ’¯\n,\n\\displaystyle\\mathcal{H}(\\textbf{p}^{[s-1]}(t))\\leq\\mathbf{0},\\,\\forall t\\in\\mathcal{T},\n(25d)\nwhere\nT\np\nT_{p}\nis the target prediction horizon. Cost function Eq.\n25a\nis the control effort objective. In Eq.\n25b\n, the total trajectory duration\nT\nÎ£\nT_{\\Sigma}\nis set equal to\nT\np\nT_{p}\n, meaning the tracking trajectory is generated only up to the target prediction horizon. This ensures that the trajectoryâ€™s terminal state at\nT\nÎ£\nT_{\\Sigma}\ncan correspond to the targetâ€™s last predicted position. Thus, the trajectory duration is fixed to\nT\np\nT_{p}\nusing the equality constraint. Eq.\n25c\ndefines inequality constraints enforced continuously over the entire duration, and Eq.\n25d\nare the inequality constraints that are discretely enforced at the targetâ€™s predicted timestamps\nğ’¯\n\\mathcal{T}\n.\nIn typical hierarchical tracking planners, the terminal position of the trajectory is set to the last state of the front-path, and the terminal velocity is set to the target velocity to keep up with the target. In this work, we fix solely the terminal velocity and rather treat the other terminal states as decision variables in optimization to compute the optimal states. We have some tracking constraints for the unfixed terminal states, which are included in Eq.\n25d\nwith\nt\n=\nT\nÎ£\nt=T_{\\Sigma}\n.\nTo conveniently solve the continuous constrained optimization problem, we transform it into an unconstrained optimization. The temporal constraint Eq.\n25b\ncan be eliminated by variable substitution. The inequality constraints Eq.\n25c\nand Eq.\n25d\ncan be converted into penalty terms using the penalty method. Then the original problem Eq.\n25\nis converted into the following unconstrained format:\nmin\nÏ±\n,\nT\nâ¡\nğ’¥\nE\n+\nâˆ«\n0\nT\nÎ£\nğ’¥\nğ’¢\nâ€‹\nğ‘‘\nt\n+\nâˆ‘\nt\nâˆˆ\nğ’¯\nğ’¥\nâ„‹\n,\n\\min_{\\bm{\\varrho},\\textbf{T}}\\mathcal{J}_{E}+\\int_{0}^{T_{\\Sigma}}\\mathcal{J}_{\\mathcal{G}}\\,dt+\\sum_{t\\in\\mathcal{T}}\\mathcal{J}_{\\mathcal{H}},\n(26)\nwhere\nğ’¥\nğ’¢\n\\mathcal{J}_{\\mathcal{G}}\nis the penalty function of the continuous constraints Eq.\n25c\n, and\nğ’¥\nâ„‹\n\\mathcal{J}_{\\mathcal{H}}\nis the penalty function of the discrete constraints Eq.\n25d\n. â€„In this work, we adopt\nğ”—\nM\nâ€‹\nI\nâ€‹\nN\nâ€‹\nC\nâ€‹\nO\ns\n=\n4\n\\mathfrak{T}_{MINCO}^{\\,s=4}\ntrajectories (\ni.e.\n7-degree polynomials) to represent the drone position. For cases requiring yaw planning, such as the Avia tracker, a\nğ”—\nM\nâ€‹\nI\nâ€‹\nN\nâ€‹\nC\nâ€‹\nO\ns\n=\n2\n\\mathfrak{T}_{MINCO}^{\\,s=2}\ntrajectory (\ni.e.\na 3-degree polynomial) is used to represent the droneâ€™s yaw angle. The position and yaw trajectories are jointly optimized upon the cost objectives in Eq.\n26\n. The positional trajectories are initialized using the path obtained from the front-end kinodynamic searching, the yaw angles are initially set to head toward the target positions, and the initial piece duration\nT\ni\n=\nT\np\n/\nM\nT_{i}=T_{p}/M\n.\nVII-B\nContinuous Relative-time Penalty\nDenote the continuous penalty term (the second term) in Eq.\n26\nas\nğ’¥\nC\n\\mathcal{J}_{C}\n. These penalty functions are evaluated on relative times along the whole trajectory. To efficiently compute violations, we transform the continuous constraints into finite ones using constraint transcription\n[\n46\n]\n. Penalties\nğ’¥\nğ’¢\n\\mathcal{J}_{\\mathcal{G}}\nare numerically integrated by sampling all the pieces of the trajectory evenly with time step\nT\ni\n/\nÎº\ni\nT_{i}/\\kappa_{i}\n, where\nÎº\ni\n\\kappa_{i}\ndenotes the sample number of the\ni\nt\nâ€‹\nh\ni^{th}\npiece. So we have\nğ’¥\nC\n=\nâˆ«\n0\nT\nÎ£\nğ’¥\nğ’¢\nâ€‹\nğ‘‘\nt\n=\nâˆ‘\ni\nM\nT\ni\nÎº\ni\nâ€‹\nâˆ‘\nj\n=\n0\nÎº\ni\nÏ‰\nÂ¯\nj\nâ€‹\nğ’¥\nğ’¢\nâ€‹\n(\nt\nj\n)\n,\n\\mathcal{J}_{C}=\\int_{0}^{T_{\\Sigma}}\\mathcal{J}_{\\mathcal{G}}\\,dt=\\sum_{i}^{M}\\frac{T_{i}}{\\kappa_{i}}\\sum_{j=0}^{\\kappa_{i}}\\bar{\\omega}_{j}\\mathcal{J}_{\\mathcal{G}}(t_{j}),\n(27)\nwhere\nt\nj\n=\n(\nj\n/\nÎº\ni\n)\nâ€‹\nT\ni\nt_{j}=(j/\\kappa_{i})T_{i}\nrefers to the relative time sampled on the\ni\nt\nâ€‹\nh\ni^{th}\npiece. Integral coefficients\n(\nÏ‰\nÂ¯\n0\n,\nÏ‰\nÂ¯\n1\n,\nâ‹¯\n,\nÏ‰\nÂ¯\nÎº\ni\nâˆ’\n1\n,\nÏ‰\nÂ¯\nÎº\ni\n)\n=\n(\n1\n/\n2\n,\n1\n,\nâ‹¯\n,\n1\n,\n1\n/\n2\n)\n(\\bar{\\omega}_{0},\\bar{\\omega}_{1},\\cdots,\\bar{\\omega}_{\\kappa_{i}-1},\\bar{\\omega}_{\\kappa_{i}})=(1/2,1,\\cdots,1,1/2)\nfollow the trapezoidal rule. The continuous penalties include obstacle avoidance, dynamic feasibility and swarm reciprocal clearance, which can be listed as\nğ’¥\nğ’¢\n=\nÎ»\nğ’¢\nâ€‹\n[\nğ’¥\no\nâ€‹\nb\nâ€‹\ns\n,\nğ’¥\nd\nâ€‹\ny\nâ€‹\nn\n,\nğ’¥\ns\nâ€‹\nw\nâ€‹\nm\n]\nT\n,\n\\mathcal{J}_{\\mathcal{G}}=\\lambda_{\\mathcal{G}}\\,[\\mathcal{J}_{obs},\\mathcal{J}_{dyn},\\mathcal{J}_{swm}]^{T},\n(28)\nwhere\nÎ»\nğ’¢\n\\lambda_{\\mathcal{G}}\nis the penalty weight vector. These constraints can be divided into two categories: one is at the single-UAV navigation level, and the other is for the swarming.\nVII-B1\nSingle-UAV Navigation Constraints\nThe constraints for obstacle avoidance\nğ’¥\no\nâ€‹\nb\nâ€‹\ns\n\\mathcal{J}_{obs}\nand dynamic feasibility\nğ’¥\nd\nâ€‹\ny\nâ€‹\nn\n\\mathcal{J}_{dyn}\nact as the basic constraints for drone navigation. In this category,\nğ’¥\nğ’¢\n\\mathcal{J}_{\\mathcal{G}}\nis a function of ego states\np\ni\n[\ns\nâˆ’\n1\n]\nâ€‹\n(\nt\nj\n)\n\\textbf{p}^{[s-1]}_{i}(t_{j})\n, thus we can derive the gradients of\nğ’¥\nC\n\\mathcal{J}_{C}\nin Eq.\n27\nw.r.t.\nc\ni\n\\textbf{c}_{i}\nand\nT\ni\nT_{i}\nby chain rule\nâˆ‚\nğ’¥\nC\nâˆ‚\nc\ni\n=\nâˆ‚\nğ’¥\nC\nâˆ‚\nğ’¥\nğ’¢\nâ€‹\nâˆ‚\nğ’¥\nğ’¢\nâˆ‚\nc\ni\n,\n\\frac{\\partial\\mathcal{J}_{C}}{\\partial\\textbf{c}_{i}}=\\frac{\\partial\\mathcal{J}_{C}}{\\partial\\mathcal{J}_{\\mathcal{G}}}\\frac{\\partial\\mathcal{J}_{\\mathcal{G}}}{\\partial\\textbf{c}_{i}},\n(29)\nâˆ‚\nğ’¥\nC\nâˆ‚\nT\ni\n=\nğ’¥\nC\nT\ni\n+\nâˆ‚\nğ’¥\nC\nâˆ‚\nğ’¥\nğ’¢\nâ€‹\nâˆ‚\nğ’¥\nğ’¢\nâˆ‚\nt\nj\nâ€‹\nâˆ‚\nt\nj\nâˆ‚\nT\ni\n,\n\\frac{\\partial\\mathcal{J}_{C}}{\\partial T_{i}}=\\frac{\\mathcal{J}_{C}}{T_{i}}+\\frac{\\partial\\mathcal{J}_{C}}{\\partial\\mathcal{J}_{\\mathcal{G}}}\\frac{\\partial\\mathcal{J}_{\\mathcal{G}}}{\\partial t_{j}}\\frac{\\partial t_{j}}{\\partial T_{i}},\\;\n(30)\nwhere the gradients\nâˆ‚\nğ’¥\nğ’¢\nâˆ‚\nc\ni\n=\nâˆ‘\nk\n=\n0\ns\nâˆ’\n1\nâˆ‚\np\ni\n(\nk\n)\nâˆ‚\nc\ni\nâ€‹\nâˆ‚\nğ’¥\nğ’¢\nâˆ‚\np\ni\n(\nk\n)\n=\nâˆ‘\nk\n=\n0\ns\nâˆ’\n1\nğœ·\n(\nk\n)\nâ€‹\n(\nt\nj\n)\nâ€‹\nâˆ‚\nğ’¥\nğ’¢\nâˆ‚\np\ni\n(\nk\n)\n,\n\\frac{\\partial\\mathcal{J}_{\\mathcal{G}}}{\\partial\\textbf{c}_{i}}=\\sum_{k=0}^{s-1}\\frac{\\partial\\textbf{p}_{i}^{(k)}}{\\partial\\textbf{c}_{i}}\\frac{\\partial\\mathcal{J}_{\\mathcal{G}}}{\\partial\\textbf{p}^{(k)}_{i}}=\\sum_{k=0}^{s-1}\\bm{\\beta}^{(k)}(t_{j})\\frac{\\partial\\mathcal{J}_{\\mathcal{G}}}{\\partial\\textbf{p}^{(k)}_{i}},\n(31)\nâˆ‚\nğ’¥\nğ’¢\nâˆ‚\nt\nj\n=\nâˆ‘\nk\n=\n0\ns\nâˆ’\n1\nâˆ‚\np\ni\n(\nk\n)\nâˆ‚\nt\nj\nâ€‹\nâˆ‚\nğ’¥\nğ’¢\nâˆ‚\np\ni\n(\nk\n)\n=\nâˆ‘\nk\n=\n0\ns\nâˆ’\n1\np\ni\n(\nk\n+\n1\n)\nâ€‹\nâˆ‚\nğ’¥\nğ’¢\nâˆ‚\np\ni\n(\nk\n)\n,\n\\frac{\\partial\\mathcal{J}_{\\mathcal{G}}}{\\partial t_{j}}=\\sum_{k=0}^{s-1}\\frac{\\partial\\textbf{p}_{i}^{(k)}}{\\partial t_{j}}\\frac{\\partial\\mathcal{J}_{\\mathcal{G}}}{\\partial\\textbf{p}^{(k)}_{i}}=\\sum_{k=0}^{s-1}\\textbf{p}_{i}^{(k+1)}\\frac{\\partial\\mathcal{J}_{\\mathcal{G}}}{\\partial\\textbf{p}^{(k)}_{i}},\n(32)\nand\nâˆ‚\nt\nj\n/\nâˆ‚\nT\ni\n=\nj\n/\nÎº\ni\n\\partial t_{j}/\\partial T_{i}=j/\\kappa_{i}\n. The derivatives\nâˆ‚\nğ’¥\nğ’¢\n/\nâˆ‚\np\ni\n(\nk\n)\n\\partial\\mathcal{J}_{\\mathcal{G}}/\\partial\\textbf{p}_{i}^{(k)}\nare determined by the formulations of the constraints.\nVII-B2\nSwarm Navigation Constraints\nWith the communication network, each drone receives teammatesâ€™ trajectories to formulate swarm constraints and optimizes its own trajectory accordingly. The constraint of safe clearance\nğ’¥\ns\nâ€‹\nw\nâ€‹\nm\n\\mathcal{J}_{swm}\nin Eq.\n21\naccounts for the swarm mutual collision avoidance. Unlike the single-UAV constraints,\nğ’¥\ns\nâ€‹\nw\nâ€‹\nm\n\\mathcal{J}_{swm}\nis not only a function of ego position\np\ni\nâ€‹\n(\nt\nj\n)\n\\textbf{p}_{i}(t_{j})\n, but also involves the teammatesâ€™ positions. We need to use the relative time\nt\nj\n=\n(\nj\n/\nÎº\ni\n)\nâ€‹\nT\ni\nt_{j}=(j/\\kappa_{i})T_{i}\nfor the ego trajectory and the corresponding absolute timestamp\nÏ„\n\\tau\nto query the teammate positions on received trajectories, where\nÏ„\n=\nâˆ‘\nl\n=\n1\ni\nâˆ’\n1\nT\nl\n+\n(\nj\n/\nÎº\ni\n)\nâ€‹\nT\ni\n.\n\\tau=\\sum_{l=1}^{i-1}T_{l}+(j/\\kappa_{i})T_{i}.\nFor a swarm of\nN\nN\ndrones, let\np\nÏ•\nâ€‹\n(\nÏ„\n)\n\\textbf{p}_{\\phi}(\\tau)\ndenote the trajectory from the teammate drone\nÏ•\n\\phi\nand\nğ’¥\ns\nâ€‹\nw\nâ€‹\nm\nÏ•\n\\mathcal{J}_{swm}^{\\phi}\ndenote the clearance constraint associated with teammate\nÏ•\n\\phi\n, then the violation\nğ’¥\nC\n\\mathcal{J}_{C}\nof this constraint is formulated as\nğ’¥\nC\n=\nâˆ‘\ni\nM\nT\ni\nÎº\ni\nâ€‹\nâˆ‘\nj\n=\n0\nÎº\ni\nÏ‰\nÂ¯\nj\nâ€‹\nâˆ‘\nÏ•\nN\nğ’¥\ns\nâ€‹\nw\nâ€‹\nm\nÏ•\nâ€‹\n(\np\ni\nâ€‹\n(\nt\nj\n)\n,\np\nÏ•\nâ€‹\n(\nÏ„\n)\n)\n.\n\\mathcal{J}_{C}=\\sum_{i}^{M}\\!\\frac{T_{i}}{\\kappa_{i}}\\!\\sum_{j=0}^{\\kappa_{i}}\\bar{\\omega}_{j}\\!\\sum_{\\phi}^{N}\\mathcal{J}_{swm}^{\\phi}(\\textbf{p}_{i}(t_{j}),\\textbf{p}_{\\phi}(\\tau)).\n(33)\nNote that term\nÏ„\n\\tau\nintroduces\nğ’¥\ns\nâ€‹\nw\nâ€‹\nm\nÏ•\n\\mathcal{J}_{swm}^{\\phi}\nthe gradient dependence on its preceding pieces\nT\nl\nT_{l}\nthrough\np\nÏ•\nâ€‹\n(\nÏ„\n)\n\\textbf{p}_{\\phi}(\\tau)\n. So temporal gradient\nâˆ‚\nğ’¥\ns\nâ€‹\nw\nâ€‹\nm\nÏ•\nâˆ‚\nT\nl\n=\nâˆ‚\nt\nj\nâˆ‚\nT\nl\nâ€‹\nâˆ‚\np\ni\nâˆ‚\nt\nj\nâ€‹\nâˆ‚\nğ’¥\ns\nâ€‹\nw\nâ€‹\nm\nÏ•\nâˆ‚\np\ni\n+\nâˆ‚\nÏ„\nâˆ‚\nT\nl\nâ€‹\nâˆ‚\np\nÏ•\nâˆ‚\nÏ„\nâ€‹\nâˆ‚\nğ’¥\ns\nâ€‹\nw\nâ€‹\nm\nÏ•\nâˆ‚\np\nÏ•\n,\n\\frac{\\partial\\mathcal{J}_{swm}^{\\phi}}{\\partial T_{l}}=\\frac{\\partial t_{j}}{\\partial T_{l}}\\frac{\\partial\\textbf{p}_{i}}{\\partial t_{j}}\\frac{\\partial\\mathcal{J}_{swm}^{\\phi}}{\\partial\\textbf{p}_{i}}+\\frac{\\partial\\tau}{\\partial T_{l}}\\frac{\\partial\\textbf{p}_{\\phi}}{\\partial\\tau}\\frac{\\partial\\mathcal{J}_{swm}^{\\phi}}{\\partial\\textbf{p}_{\\phi}},\n(34)\nâˆ‚\nt\nj\nâˆ‚\nT\nl\n=\n{\nj\nÎº\ni\n,\nl\n=\ni\n,\n0\n,\nl\n<\ni\n,\nâ€‹\nâˆ‚\nÏ„\nâˆ‚\nT\nl\n=\n{\nj\nÎº\ni\n,\nl\n=\ni\n,\n1\n,\nl\n<\ni\n.\n\\frac{\\partial t_{j}}{\\partial T_{l}}=\\left\\{\\!\\begin{array}[]{ll}\\!\\frac{j}{\\kappa_{i}},&l=i,\\\\\n\\!0,&l<i,\\\\\n\\end{array}\\right.\\;\\;\\,\\,\\frac{\\partial\\tau}{\\partial T_{l}}=\\left\\{\\!\\begin{array}[]{ll}\\!\\frac{j}{\\kappa_{i}},&l=i,\\\\\n\\!1,&l<i.\\\\\n\\end{array}\\right.\n(35)\nOther components of gradients\nâˆ‚\nğ’¥\nC\n/\nâˆ‚\nc\ni\n\\partial\\mathcal{J}_{C}/\\partial\\textbf{c}_{i}\nand\nâˆ‚\nğ’¥\nC\n/\nâˆ‚\nT\ni\n\\partial\\mathcal{J}_{C}/\\partial T_{i}\nare in the same forms as the ones in the single-UAV constraints.\nVII-C\nDiscrete Absolute-time Penalty\nDenote the discrete penalty (the third term) in Eq.\n26\nas\nğ’¥\nD\n\\mathcal{J}_{D}\n. To impose these penalties related to target tracking requirements, constraint violation is evaluated at absolute time\nt\nk\nt_{k}\n, which is the\nk\nt\nâ€‹\nh\nk^{th}\ntimestamp of the target prediction, with the corresponding penalty assessed using the\nk\nt\nâ€‹\nh\nk^{th}\ntarget position of the prediction series. So we have\nğ’¥\nD\n=\nâˆ‘\nk\n=\n1\nN\np\nÎ´\nâ€‹\nT\nâ‹…\nğ’¥\nâ„‹\nâ€‹\n(\nt\nk\n)\n,\n\\mathcal{J}_{D}=\\sum_{k=1}^{N_{p}}\\delta T\\cdot\\mathcal{J}_{\\mathcal{H}}(t_{k}),\n(36)\nwhere\nÎ´\nâ€‹\nT\n\\delta T\nand\nN\np\nN_{p}\nare the time interval and total number of predictions.\nğ’¥\nâ„‹\n\\mathcal{J}_{\\mathcal{H}}\nincludes all the tracking-related constraints introduced in Sec.\nV-A\nto Sec.\nV-E\n, which can be listed as\nğ’¥\nâ„‹\n=\nÎ»\nâ„‹\nâ€‹\n[\nğ’¥\nv\nâ€‹\ni\nâ€‹\ns\n,\nğ’¥\nf\nâ€‹\no\nâ€‹\nv\n,\nğ’¥\nd\nâ€‹\ni\nâ€‹\ns\n,\nğ’¥\nt\nâ€‹\no\nâ€‹\nc\n,\nğ’¥\nf\nâ€‹\nr\nâ€‹\nm\n]\nT\n,\n\\mathcal{J}_{\\mathcal{H}}=\\lambda_{\\mathcal{H}}\\,[\\mathcal{J}_{vis},\\mathcal{J}_{fov},\\mathcal{J}_{dis},\\mathcal{J}_{toc},\\mathcal{J}_{frm}]^{T},\n(37)\nwhere\nÎ»\nâ„‹\n\\lambda_{\\mathcal{H}}\nis the penalty weight vector. These tracking constraints can be divided into two categories: one is at the single-UAV level, and the other is for swarm cooperation.\nVII-C1\nSingle-UAV Tracking Constraints\nIn this category, constraints\nğ’¥\nv\nâ€‹\ni\nâ€‹\ns\n,\nğ’¥\nf\nâ€‹\no\nâ€‹\nv\n,\n\\mathcal{J}_{vis},\\mathcal{J}_{fov},\nand\nğ’¥\nd\nâ€‹\ni\nâ€‹\ns\n\\mathcal{J}_{dis}\nare associated with the tracking performance from the single-UAV aspect. Thus\nğ’¥\nâ„‹\n\\mathcal{J}_{\\mathcal{H}}\nis a function of solely ego state\np\ni\n[\ns\nâˆ’\n1\n]\nâ€‹\n(\nt\nk\n)\n\\textbf{p}^{[s-1]}_{i}(t_{k})\n. Assume that\nt\nk\nt_{k}\nis located on the\ni\nt\nâ€‹\nh\ni^{th}\npiece of the trajectory, then the corresponding relative time\nt\nr\nt_{r}\nof\nt\nk\nt_{k}\non the\ni\nt\nâ€‹\nh\ni^{th}\npiece becomes\nt\nr\n=\nt\nk\nâˆ’\nâˆ‘\nl\n=\n1\ni\nâˆ’\n1\nT\nl\n,\nt_{r}=t_{k}-\\sum_{l=1}^{i-1}T_{l},\nwhere\nT\nl\nT_{l}\ndenotes the preceding piece duration. Note that the formulation of\nt\nt\nhere brings in the gradient dependence on all\nT\nl\nT_{l}\nwith\n1\nâ‰¤\nl\nâ‰¤\ni\n1\\leq l\\leq i\n. Thus, the gradients of\nğ’¥\nD\n\\mathcal{J}_{D}\nare derived as\nâˆ‚\nğ’¥\nD\nâˆ‚\nc\ni\n=\nâˆ‚\nğ’¥\nâ„‹\nâˆ‚\nc\ni\nâ€‹\nâˆ‚\nğ’¥\nD\nâˆ‚\nğ’¥\nâ„‹\n,\nâˆ‚\nğ’¥\nD\nâˆ‚\nT\nl\n=\nâˆ‚\nğ’¥\nâ„‹\nâˆ‚\nT\nl\nâ€‹\nâˆ‚\nğ’¥\nD\nâˆ‚\nğ’¥\nâ„‹\n,\n\\frac{\\partial\\mathcal{J}_{D}}{\\partial\\textbf{c}_{i}}=\\frac{\\partial\\mathcal{J}_{\\mathcal{H}}}{\\partial\\textbf{c}_{i}}\\frac{\\partial\\mathcal{J}_{D}}{\\partial\\mathcal{J}_{\\mathcal{H}}},\\;\\frac{\\partial\\mathcal{J}_{D}}{\\partial T_{l}}=\\frac{\\partial\\mathcal{J}_{\\mathcal{H}}}{\\partial T_{l}}\\frac{\\partial\\mathcal{J}_{D}}{\\partial\\mathcal{J}_{\\mathcal{H}}},\n(38)\nâˆ‚\nğ’¥\nâ„‹\nâˆ‚\nc\ni\n=\nâˆ‘\nk\n=\n0\ns\nâˆ’\n1\nâˆ‚\np\ni\n(\nk\n)\nâˆ‚\nc\ni\nâ€‹\nâˆ‚\nğ’¥\nâ„‹\nâˆ‚\np\ni\n(\nk\n)\n=\nâˆ‘\nk\n=\n0\ns\nâˆ’\n1\nğœ·\n(\nk\n)\nâ€‹\n(\nt\nr\n)\nâ€‹\nâˆ‚\nğ’¥\nâ„‹\nâˆ‚\np\ni\n(\nk\n)\n,\n\\frac{\\partial\\mathcal{J}_{\\mathcal{H}}}{\\partial\\textbf{c}_{i}}=\\sum_{k=0}^{s-1}\\frac{\\partial\\textbf{p}_{i}^{(k)}}{\\partial\\textbf{c}_{i}}\\frac{\\partial\\mathcal{J}_{\\mathcal{H}}}{\\partial\\textbf{p}^{(k)}_{i}}=\\sum_{k=0}^{s-1}\\bm{\\beta}^{(k)}(t_{r})\\frac{\\partial\\mathcal{J}_{\\mathcal{H}}}{\\partial\\textbf{p}^{(k)}_{i}},\n(39)\nâˆ‚\nğ’¥\nâ„‹\nâˆ‚\nT\nl\n=\nâˆ‚\nt\nr\nâˆ‚\nT\nl\nâ€‹\nâˆ‘\nk\n=\n0\ns\nâˆ’\n1\np\ni\n(\nk\n+\n1\n)\nâ€‹\nâˆ‚\nğ’¥\nâ„‹\nâˆ‚\np\ni\n(\nk\n)\n,\nâˆ‚\nt\nr\nâˆ‚\nT\nl\n=\n{\n0\n,\nl\n=\ni\n,\nâˆ’\n1\n,\nl\n<\ni\n,\n\\frac{\\partial\\mathcal{J}_{\\mathcal{H}}}{\\partial T_{l}}=\\frac{\\partial t_{r}}{\\partial T_{l}}\\sum_{k=0}^{s-1}\\textbf{p}_{i}^{(k+1)}\\frac{\\partial\\mathcal{J}_{\\mathcal{H}}}{\\partial\\textbf{p}_{i}^{(k)}},\\,\\frac{\\partial t_{r}}{\\partial T_{l}}=\\left\\{\\!\\begin{array}[]{ll}\\!0,&l=i,\\\\\n\\!-1,&l<i,\\\\\n\\end{array}\\right.\n(40)\nThe derivatives\nâˆ‚\nğ’¥\nâ„‹\n/\nâˆ‚\np\ni\n(\nk\n)\n\\partial\\mathcal{J}_{\\mathcal{H}}/\\partial\\textbf{p}_{i}^{(k)}\nare determined by the specific formulations of the constraints.\nVII-C2\nSwarm Tracking Constraints\nThe constraints\nğ’¥\nt\nâ€‹\no\nâ€‹\nc\n\\mathcal{J}_{toc}\nand\nğ’¥\nf\nâ€‹\nr\nâ€‹\nm\n\\mathcal{J}_{frm}\nare designed to coordinate the swarm during the tracking process. Unlike the continuous reciprocal collision constraint\nğ’¥\ns\nâ€‹\nw\nâ€‹\nm\n\\mathcal{J}_{swm}\n, the teammate position\np\nÏ•\nâ€‹\n(\nt\nk\n)\n\\textbf{p}_{\\phi}(t_{k})\nin\nğ’¥\nt\nâ€‹\no\nâ€‹\nc\n\\mathcal{J}_{toc}\nand\nğ’¥\nf\nâ€‹\nr\nâ€‹\nm\n\\mathcal{J}_{frm}\nproduces no extra gradients. It is because both teammate and ego trajectories in the discrete constraints are queried using the same fixed absolute time\nt\nk\nt_{k}\n, thus the teammate position\np\nÏ•\nâ€‹\n(\nt\nk\n)\n\\textbf{p}_{\\phi}(t_{k})\nis a constant throughout the optimization iterations. Therefore, all the gradients of\nğ’¥\nâ„‹\n\\mathcal{J}_{\\mathcal{H}}\nhave identical forms as the ones in single-UAV tracking constraints.\nVII-D\nTemporal Constraint Elimination\nWe use the variable substitution from\n[\n45\n]\nto eliminate the temporal equality constraint of Eq.\n25b\n. Denote by\nğœ¾\n=\n(\nÎ¹\n1\n,\nâ€¦\n,\nÎ¹\nM\n)\nâˆˆ\nâ„\nM\n\\bm{\\iota}=(\\iota_{1},...,\\iota_{M})\\in\\mathbb{R}^{M}\nthe new temporal variables to be optimized. The transformation is given by the map below for\n1\n<\ni\n<\nM\n1<i<M\nT\ni\n=\ne\nÎ¹\ni\n1\n+\nâˆ‘\nj\n=\n1\nM\nâˆ’\n1\ne\nÎ¹\nj\nâ€‹\nT\np\n,\nT\nM\n=\nT\np\nâˆ’\nâˆ‘\nj\n=\n1\nM\nâˆ’\n1\nT\nj\n.\nT_{i}=\\frac{e^{\\iota_{i}}}{1+\\sum_{j=1}^{M-1}e^{\\iota_{j}}}T_{p},\\;T_{M}=T_{p}-\\sum_{j=1}^{M-1}T_{j}.\n(41)\nWith the new decision variables\nğœ¾\n\\bm{\\iota}\n, the temporal constraint can be satisfied by default. As the total duration\nT\nÎ£\nT_{\\Sigma}\nis fixed to\nT\np\nT_{p}\n, the constraints\nâ„‹\nâ€‹\n(\np\n[\ns\nâˆ’\n1\n]\nâ€‹\n(\nt\n)\n)\n\\mathcal{H}(\\textbf{p}^{[s-1]}(t))\nin Eq.\n25d\nexerted on the terminal states at\nt\n=\nT\nÎ£\nt=T_{\\Sigma}\ndo not have dependence on time anymore (since\nT\np\nT_{p}\nis a constant). Thus, the terminal penalties only have dependence on the spatial variation of the terminal states, and the temporal gradients at\nT\nÎ£\nT_{\\Sigma}\nare zero.\nVIII\nSimulation and Benchmark\nIn this section, we conduct extensive comparative studies to validate the performance of our swarm tracking planner. All simulations in this section are run on an Intel Core i9-12900K CPU with an NVIDIA GeForce RTX 3070Ti GPU.\nVIII-A\nGeneral Benchmark of Swarm Tracking Planners\nTo demonstrate the advantages of our method, benchmark comparisons are conducted against other cutting-edge swarm tracking works. The proposed planner is compared with Zhouâ€™s work\n[\n6\n]\n, Hoâ€™s work\n[\n23\n]\n, and Yinâ€™s work\n[\n7\n]\n. As a baseline method, Zhouâ€™s planner\n[\n6\n]\nutilizes a predefined constant leader-follower formation to track the target. Hoâ€™s planner\n[\n23\n]\nand Yinâ€™s planner\n[\n7\n]\nactively consider the target visibility. Ho\net al.\n[\n23\n]\nuses a centralized dynamic programming to search the occlusion-free formation configuration for the swarm. Yin\net al.\n[\n7\n]\nbuild the planar visible sectors to avoid environmental occlusion. Each planner is fine-tuned to its best tracking performance.\nTo compare the planners fairly, we simulate four swarms, each with four trackers, chasing an identical target drone simultaneously. Each swarm runs one benchmarked planner, tested on two dense random maps with different obstacle types: Forest (Fig.\n10\n(a)) and Walls (Fig.\n10\n(b)). Since the target maneuverability directly influences tracking difficulty, we implement and test two target velocity modes: slow (\n1.0\nâ€‹\nm\n/\ns\n1.0m/s\n) and fast (\n2.5\nâ€‹\nm\n/\ns\n2.5m/s\n). Moreover, to reflect the impact of FOV shapes on swarm tracking, we design two swarm configurations with varied FOV settings. The first configuration (Swarm Configuration A) comprises four drones equipped with regular upward-facing Mid360 LiDARs, as shown in Fig.\n10\n(c). The second configuration (Swarm Configuration B) integrates two drones with the upward-facing LiDARs and two drones with downward-facing Mid360 LiDARs, as shown in Fig.\n10\n(d). Testing is limited to horizontally omnidirectional FOVs due to the lack of yaw planning in the other benchmarked planners.\nThe tracking performance is evaluated over four metrics from\n[\n7\n]\n: average target visibility (\nÏ‘\na\nâ€‹\nv\nâ€‹\ng\n\\vartheta_{avg}\n), worst-case visibility (\nÏ‘\nw\nâ€‹\nr\nâ€‹\ns\nâ€‹\nt\n\\vartheta_{wrst}\n), full-visibility time ratio (\nÎ³\nv\nâ€‹\ni\nâ€‹\ns\n\\gamma_{vis}\n), and average tracking distance (\nd\na\nâ€‹\nv\nâ€‹\ng\nd_{avg}\n). A tracker is considered losing the target if the LOS is blocked by obstacles/teammates, or the target exits the FOV, or the tracking distance becomes too close. The swarm visibility\nÏ‘\nâ€‹\n(\nt\n)\n\\vartheta(t)\nis defined as the count of trackers that are not losing the target at time\nt\nt\n. For a tracking task with duration\nT\nb\nT_{b}\n, the average visibility\nÏ‘\na\nâ€‹\nv\nâ€‹\ng\n\\vartheta_{avg}\nis calculated by\nÏ‘\na\nâ€‹\nv\nâ€‹\ng\n=\n1\nT\nb\nâ€‹\nâˆ«\n0\nT\nb\nÏ‘\nâ€‹\n(\nt\n)\nâ€‹\nğ‘‘\nt\n.\n\\vartheta_{avg}=\\frac{1}{T_{b}}\\int_{0}^{T_{b}}\\vartheta(t)dt.\n(42)\nPractically, this metric\nÏ‘\na\nâ€‹\nv\nâ€‹\ng\n\\vartheta_{avg}\nis numerically integrated with a 0.05-second sampling interval. The metric\nÏ‘\nw\nâ€‹\nr\nâ€‹\ns\nâ€‹\nt\n\\vartheta_{wrst}\nrepresents the lowest swarm visibility over the task. Let\nT\nv\nâ€‹\ni\nâ€‹\ns\nT_{vis}\ndenote the total time duration in which the target is fully visible to all trackers; then metric\nÎ³\nv\nâ€‹\ni\nâ€‹\ns\n\\gamma_{vis}\nrefers to the ratio of\nT\nv\nâ€‹\ni\nâ€‹\ns\nT_{vis}\nto\nT\nb\nT_{b}\n. The tracking distance\nd\na\nâ€‹\nv\nâ€‹\ng\nd_{avg}\nis averaged in the same manner as\nÏ‘\na\nâ€‹\nv\nâ€‹\ng\n\\vartheta_{avg}\n. Each test case undergoes four independent trials, and the trial with the highest\nÏ‘\na\nâ€‹\nv\nâ€‹\ng\n\\vartheta_{avg}\nis selected as the benchmark.\nFigure 10:\n(a)\nThe Forest map used in the general benchmark.\n(b)\nThe Walls map.\n(c)\nAn illustration of the tracker drone with a regular upward-facing Mid360 LiDAR unit.\n(d)\nAn illustration of the tracker drone with an inverted downward-facing Mid360 LiDAR unit.\nFigure 11:\nHistograms of swarm-wide cumulative target loss duration with breakdowns by failure types (target velocity =\n2.5\nâ€‹\nm\n/\ns\n2.5m/s\n).\nLeft\n: Histograms for the Forest map;\nRight\n: Histograms for the Walls map.\nThe results are summarized in Tab.\nI\n. Our method outperforms other works in terms of\nÏ‘\na\nâ€‹\nv\nâ€‹\ng\n\\vartheta_{avg}\n,\nÏ‘\nw\nâ€‹\ns\nâ€‹\nt\n\\vartheta_{wst}\n, and\nÎ³\nv\nâ€‹\ni\nâ€‹\ns\n\\gamma_{vis}\nin all cases with satisfactory distances\nd\na\nâ€‹\nv\nâ€‹\ng\nd_{avg}\n. Notably, Zhouâ€™s and Hoâ€™s methods occasionally yield zero swarm visibility in worst-case scenarios, which could be fatal in real-world tracking applications. Significant performance degradation is observed in the three benchmarked planners when the target speed increases from\n1.0\nâ€‹\nm\n/\ns\n1.0m/s\nto\n2.5\nâ€‹\nm\n/\ns\n2.5m/s\n. Fig.\n11\npresents the cumulative target loss duration across all swarm trackers in the high-speed scenarios, along with its breakdown by specific failure reasons. The results in Tab.\nI\nalso indicate worse tracking performance in the Walls environment compared to the Forest map, attributed to challenges such as difficult occlusion recovery from wall-shaped structures, increased vertical obstacle complexity, and greater target velocity fluctuations in the scenario. Importantly, our planner maintains near-complete swarm visibility (\n>\n99\n%\n>99\\%\n) across all test conditions, demonstrating its robust adaptability to different environments, varied FOV settings, and fast target dynamics.\nTABLE I:\nVisibility-aware Swarm Tracking Benchmark Results\nConfiguration\nSwarm Configuration A\nSwarm Configuration B\nScenario\nMethod\nMetric\nÏ‘\na\nâ€‹\nv\nâ€‹\ng\n\\bm{\\vartheta}_{avg}\nÏ‘\nw\nâ€‹\nr\nâ€‹\ns\nâ€‹\nt\n\\bm{\\vartheta}_{wrst}\nğœ¸\nv\nâ€‹\ni\nâ€‹\ns\n\\bm{\\gamma}_{vis}\n(\n%\n\\%\n)\nd\na\nâ€‹\nv\nâ€‹\ng\nd_{avg}\n(\nm\nm\n)\nÏ‘\na\nâ€‹\nv\nâ€‹\ng\n\\bm{\\vartheta}_{avg}\nÏ‘\nw\nâ€‹\nr\nâ€‹\ns\nâ€‹\nt\n\\bm{\\vartheta}_{wrst}\nğœ¸\nv\nâ€‹\ni\nâ€‹\ns\n\\bm{\\gamma}_{vis}\n(\n%\n\\%\n)\nd\na\nâ€‹\nv\nâ€‹\ng\nd_{avg}\n(\nm\nm\n)\nForest\n(\n1.0\nâ€‹\nm\n/\ns\n)\n(1.0\\,m/s)\nZhou\net al.\n[\n6\n]\n3.757\n2.0\n77.407\n2.025\n3.732\n2.0\n75.388\n2.018\nHo\net al.\n[\n23\n]\n3.821\n2.0\n83.509\n1.952\n3.780\n2.0\n81.109\n1.925\nYin\net al.\n[\n7\n]\n3.932\n3.0\n93.231\n1.891\n3.904\n2.0\n90.599\n1.877\nProposed\n4.000\n4.0\n100.000\n2.016\n4.000\n4.0\n100.000\n2.012\nForest\n(\n2.5\nâ€‹\nm\n/\ns\n)\n(2.5\\,m/s)\nZhou\net al.\n[\n6\n]\n3.535\n0.0\n62.323\n2.052\n3.538\n0.0\n60.786\n2.035\nHo\net al.\n[\n23\n]\n3.475\n1.0\n61.774\n1.938\n3.511\n1.0\n62.096\n1.880\nYin\net al.\n[\n7\n]\n3.773\n2.0\n80.298\n1.992\n3.666\n1.0\n73.652\n2.006\nProposed\n3.997\n3.0\n99.686\n1.970\n4.000\n4.0\n100.000\n1.980\nWalls\n(\n1.0\nâ€‹\nm\n/\ns\n)\n(1.0\\,m/s)\nZhou\net al.\n[\n6\n]\n3.433\n1.0\n64.613\n2.030\n3.430\n1.0\n61.069\n2.020\nHo\net al.\n[\n23\n]\n3.514\n1.0\n67.214\n1.988\n3.514\n1.0\n67.214\n1.988\nYin\net al.\n[\n7\n]\n3.919\n3.0\n91.917\n1.925\n3.874\n2.0\n88.274\n1.905\nProposed\n4.000\n4.0\n100.000\n1.926\n4.000\n4.0\n100.000\n1.976\nWalls\n(\n2.5\nâ€‹\nm\n/\ns\n)\n(2.5\\,m/s)\nZhou\net al.\n[\n6\n]\n3.182\n0.0\n53.383\n2.072\n3.194\n1.0\n49.590\n2.101\nHo\net al.\n[\n23\n]\n3.252\n1.0\n50.037\n1.955\n3.365\n0.0\n58.451\n1.962\nYin\net al.\n[\n7\n]\n3.758\n1.0\n80.223\n1.950\n3.684\n1.0\n75.577\n1.940\nProposed\n3.993\n3.0\n99.331\n1.932\n3.996\n3.0\n99.628\n1.951\nFigure 12:\nTracking performance in the case study.\nTop\n: The orange trajectory is executed by the target. Four areas, â‘ -â‘£, are highlighted, featuring\nSparse\nclutter,\nDense\nclutter,\nVertical\nstructures, and\nNarrow\npassages.\nRows\nâ‘ -â‘£\n: Snapshots of swarm tracking in areas â‘ -â‘£. Subfigures (a)-(c) show the swarm behaviour of the proposed method. Benchmark results are shown in subfigures (d)-(f). The LiDAR FOVsâ€™ vertical cross-sections are depicted as gray sectors.\nVIII-B\nCase Study of Swarm Tracking Planners\nThis section presents a case study showcasing the performance of the compared methods. We establish a representative test map containing diverse obstacle conditions (\nSparse\nclutter,\nDense\nclutter,\nVertical\nstructures, and\nNarrow\npassages). For each method, we simulate a four-drone swarm equipped with upward-facing LiDARs tracking the target through the heterogeneous environment. Fig.\n12\nillustrates the test map and the tracking behaviors. The visible Lines of Sight are marked by the green arrows connecting the trackers and the target, whereas red arrows indicate the visibility loss.\nIn the\nSparse\narea, Zhouâ€™s method has visibility loss due to its simple constant leader-follower formation strategy. Other methods manage to avoid the occlusion by adjusting the swarm. In the\nDense\narea, only the proposed method maintains full swarm visibility. Hoâ€™s method loses visibility as it cannot find feasible occlusion-free configurations for its rigid square formation under dense clutter. Yinâ€™s work has occlusion due to the insufficient agility of its 2-D costs. The proposed planner ensures flexible occlusion-free swarm navigation in tight spaces with FOV compliance. When encountering the\nVertical\nobstacles, Yinâ€™s planner fails to timely avoid the occlusion along the Z-axis, as its visible sectors can only consider the 2-D visibility at a certain height. In contrast, our SSDF framework encodes the target visibility in the whole 3-D space, enabling the tracker to adjust LOS promptly against the 3-D occlusion when tracking a vertically moving target. In\nNarrow\npassages, the visibility of the other three planners degrades due to the maneuverability limitations imposed by their 2-D motion constraints. Conversely, the proposed method formulates all tracking requisites within a true 3-D framework. This empowers the planner to coordinate the swarm at different heights, adaptively exploiting the vertical space to achieve full-visibility tracking with strict FOV compliance, even in highly confined scenes.\nVIII-C\nAblation Study\nAblation studies are conducted to further validate the necessity of the proposed modules and costs for swarm tracking. To verify the kinodynamic front-endâ€™s contribution, we replace it with a vanilla A* searcher from\n[\n5\n]\n, designating this variant as\nw/o KinoSearch\n. We then systematically remove all SSDF-based occlusion avoidance costs throughout the planning pipeline to create the\nw/o Visibility\nvariant, while canceling all equidistant 3-D swarm distribution costs gives the variant\nw/o Formation\n. Finally, our full\nProposed\nplanner and Zhouâ€™s work\n[\n6\n]\n, as the\nBaseline\n, are also evaluated for comparison.\nAll variants are tested in random Forest maps with five different tree density levels, varying from sparse (\n1\n/\n32\nâ€‹\nt\nâ€‹\nr\nâ€‹\ne\nâ€‹\ne\n/\nm\n2\n1/32\\,tree/m^{2}\n) to dense (\n1\n/\n9\nâ€‹\nt\nâ€‹\nr\nâ€‹\ne\nâ€‹\ne\n/\nm\n2\n1/9\\,tree/m^{2}\n). Each cylindrical tree obstacle has a height of\n4\nâ€‹\nm\n4m\nand a diameter of\n1\nâ€‹\nm\n1m\n. In each trial, five trackers with upward-facing Mid360 LiDARs collaborate to pursue a target traversing the map at velocities up to\n1.4\nâ€‹\nm\n/\ns\n1.4m/s\n.\nThe ablation results are shown in Fig.\n13\n, which also depicts the visibility loss duration in the densest map. The\nBaseline\nmethod exhibits the largest swarm visibility degradation as obstacle density increases. The A* searching in the variant\nw/o KinoSearch\nmerely minimizes the path distance while disregarding other task requirements, thereby rendering low-performance path topologies that overburden the back-end optimizer. Without the information from SSDFs, the variant\nw/o Visibility\nbecomes highly vulnerable to occlusions. While the variant\nw/o Formation\nleverages the kinodynamic searcher and SSDFs to reduce obstacle occlusions, the lack of swarm distribution guidance makes trackers prone to clustering behind the target in highly constrained scenes, inducing the risks of mutual occlusion. In contrast, the\nProposed\nemploys the distribution costs in both front-end and back-end to effectively mitigate the inter-vehicle interference, hence greatly enhancing the swarm visibility in dense areas. Through component ablation, we validate the contribution of the proposed modules and costs in ensuring the frameworkâ€™s overall performance.\nFigure 13:\nAblation study results.\nTop-left\n: The average visibility (\nÏ‘\na\nâ€‹\nv\nâ€‹\ng\n\\vartheta_{avg}\n) profiles across all variants.\nTop-right\n: The full visibility ratio (\nÎ³\nv\nâ€‹\ni\nâ€‹\ns\n\\gamma_{vis}\n) profiles across all variants.\nBottom\n: The target loss duration histograms recording the test cases in the densest map (\n1\n/\n9\nâ€‹\nt\nâ€‹\nr\nâ€‹\ne\nâ€‹\ne\n/\nm\n2\n1/9\\,tree/m^{2}\n).\nVIII-D\nStudy on Swarm Size\nIn this section, we conduct a scalability analysis of the proposed planner, investigating the impact of increasing swarm size on both tracking performance and computational overhead. Yinâ€™s work, the top performer among the benchmarked methods, serves as the baseline. Five swarms of 2 to 10 drones are simulated, with each swarm maintaining a balanced sensor setup: half the drones are equipped with upward Mid360 LiDARs and half with downward Mid360 LiDARs. Subfigures (a)-(d) in Fig.\n14\nvisualize the tracking behaviors of the eight-drone swarm, while (e)-(f) record the full-visibility time ratio throughout the mission and the average runtime per replanning cycle. As depicted in Fig.\n14\n(a)-(b), with the proposed FOV costs and spatial distribution costs, our planner can coordinate the swarm to exploit vertical airspace while ensuring strict FOV compliance. The drones adaptively configure into a 3-D polyhedral formation where downward-facing drones establish the upper facet and upward-facing drones form the lower facet. This distribution optimally leverages the free space for occlusion-averse maneuvering. Conversely, Yinâ€™s method exhibits inefficient spatial utilization and large visibility degradation as the swarm size scales up, which is due to its 2-D motion constraints and the lack of FOV-aware planning. In Fig.\n14\n(f), both methods maintain a manageable average replanning time (\n<\n16\nâ€‹\nm\nâ€‹\ns\n\\textless 16ms\n) as the swarm size scales up, which is a benefit of their decentralized architectures. The computation time of the proposed method exceeds Yinâ€™s by\n4\n4\n-\n5\nâ€‹\nm\nâ€‹\ns\n5ms\nlargely due to the\nS\nâ€‹\nE\nâ€‹\n(\n3\n)\nSE(3)\nback-end optimization.\nFigure 14:\nResults of the swarm size study.\n(a)-(b)\nTop-down and side views of the proposed 8-drone tracking behavior, highlighting agile occlusion avoidance with a 3-D polyhedral swarm distribution.\n(c)-(d)\nTop-down and side views of the baseline tracking behavior, showing the failure to sustain full swarm visibility.\n(e)\nThe full-visibility time ratio as the swarm scales up.\n(f)\nThe computation time per replanning cycle as the swarm scales up.\nVIII-E\nStudy on SSDF Updating Method\nWe compare the performance of our incremental SSDF update strategy (Sec.\nIV-C\n) against the brute-force traversal approach. The brute-force traversal method iterates along the radial dimension in spherical coordinates (\nÎ¸\n\\theta\n-\nÏ•\n\\phi\n-\nr\nr\n), repetitively applying the 2-D SSDF updating process\n[\n11\n]\nto each\nÎ¸\n\\theta\n-\nÏ•\n\\phi\nlayer at every radial discretization\nr\nr\n. As introduced in Sec.\nIV-C\n, our incremental method strategically leverages the monotonic properties of occlusion across adjacent 2-D SSDF layers to reduce the overall update operations.\nFigure 15:\nTop-down views of the test scenes used in the SSDF updating study, along with cross-sections of the updated SSDFs at a height of zero.\nWe construct three scenes with increasing obstacles and set the origin as the target point. The SSDF is parameterized with a\n5\n5\nm radial boundary,\n0.1\n0.1\nm radial resolution, and\n0.1\n0.1\nrad angular resolution. Fig.\n15\nshows the scenesâ€™ top-down views and the cross-sections of SSDFs at zero height. The baseline brute-force traversal method\n[\n11\n]\nprovides the ground truth values of updated SSDFs. Tab.\nII\nrecords the average computation time for both approaches and the proposed methodâ€™s cumulative value error across all grids in the SSDF volume. As shown in Tab.\nII\n, the proposed method achieves significantly faster average update time (\n<\n4\nâ€‹\nm\nâ€‹\ns\n\\textless 4ms\n) compared to the baseline method (\n>\n12\nâ€‹\nm\nâ€‹\ns\n\\textgreater 12ms\n) with negligible cumulative error. This confirms the efficiency and accuracy of our incremental strategy in enabling real-time SSDF integration for high-frequency replanning.\nTABLE II:\nSSDF Update Benchmark Results\nScene\nMetric\nProposed Update\nTime (\nm\nâ€‹\ns\nms\n)\nBaseline Update\nTime (\nm\nâ€‹\ns\nms\n)\nCumulative Value\nError (\nr\nâ€‹\na\nâ€‹\nd\nrad\n)\nScene 1\n3.332\n12.177\n5.32\nÃ—\n10\nâˆ’\n6\n\\times 10^{-6}\nScene 2\n3.688\n12.230\n8.31\nÃ—\n10\nâˆ’\n6\n\\times 10^{-6}\nScene 3\n4.026\n12.391\n8.57\nÃ—\n10\nâˆ’\n6\n\\times 10^{-6}\nIX\nReal world experiments\nFigure 16:\nFour drones cooperatively track an agile target drone in a forest.\n(a)\nThe pink curve depicts the target route from\nPoint 1\nto\nPoint 2\n, and the blue curve shows the return route. Four keyframes from\nb\nb\nto\ne\ne\nare selected for further illustration.\n(b)-(e)\nEach keyframe includes a snapshot with a 360Â° camera image (top) and an RViz visualization (bottom). Two tree obstacles, labeled\nTree1\nand\nTree2\n, along with their corresponding SSDFs, are highlighted to demonstrate occlusion avoidance. The heterogeneous drone FOVs are also visualized to illustrate the FOV compliance.\nIX-A\nSystem Setup and Implementation Details\nWe integrate the proposed framework with a decentralized LiDAR-based aerial swarm system. The swarm system is composed of autonomous drones with heterogeneous LiDAR configurations, including upward-facing Mid360 LiDAR, downward-facing Mid360 LiDAR, and Avia LiDAR. Each drone is equipped with a PixHawk flight controller and an onboard computer, the Intel NUC with an i7-12700 CPU. The entire swarm is localized using a decentralized swarm LiDAR-inertial odometry (Swarm-LIO)\n[\n26\n]\n, which provides 100Hz state estimation and 25Hz point clouds for each tracker. All drones are controlled by an on-manifold MPC\n[\n29\n]\n. The drones communicate with each other via a UDP wireless network. Swarm-LIO handles system clock calibration and global extrinsic transformations for the swarm. Once receiving data from a teammate via UDP, the tracker synchronizes the timestamps and transforms the data from the teammateâ€™s frame into its own frame using the global extrinsic. This process is called spatiotemporal alignment.\nIn the experiments, the targets do not exchange any information with the trackers. It is marked by high-reflectivity materials. The tracker leverages the reflectivity information in LiDAR measurements to detect the target, extracting the largest high-reflection cluster in the point clouds to filter the target out. Once the target position is acquired, the tracker broadcasts its measurement via UDP. The target states are estimated by a constant-velocity Error State Kalman Filter (ESKF). The ESKF fuses the measurements from the ego LiDAR and the spatiotemporally-aligned target measurements from teammates, yielding enhanced target estimation through multi-source fusion. ROG-Map\n[\n27\n]\nis used for occupancy grid mapping. For a more informed swarm navigation with limited-FOV sensors (\ne.g.\n, AVIA LiDARs), we extend the environmental perception by employing a bandwidth-efficient map synchronization framework\n[\n28\n]\n, where trackers encode and compress newly updated map voxels into transmittable chunks\n[\n28\n]\nand share them via UDP. Receivers then align the shared data spatiotemporally and merge it into their local maps. This approach allows Avia-equipped drones to maintain environmental awareness beyond their native FOV, ensuring safe flight while strictly orienting sensors toward the target. Our planner solves\nS\nâ€‹\nE\nâ€‹\n(\n3\n)\nSE(3)\ntrajectory optimization problems using LBFGS-lite\n[\n45\n]\n, executing 15Hz replanning cycles in experiments. Newly planned trajectories undergo immediate broadcasting across the swarm to enable tight coordination. All modules, including estimation, perception, planning, and control, are operating onboard in real-time.\nIX-B\nAgile Target Tracking in Dense Forest\nTo validate the real-world performance of our method, we test the tracking system in an unknown dense forest. A heterogeneous LiDAR-based swarm (one upward-facing Mid360, one downward-facing Mid360, and two Avia LiDARs) is deployed. The swarm tracks a manually triggered target drone flying through the forest with a velocity up to\n3\nâ€‹\nm\n/\ns\n3m/s\n. The prediction horizon of the target motion is\n1.8\nâ€‹\ns\n1.8s\n.\nFig.\n16\ndetails the whole swarm tracking task. The target flies from\nPoint 1\nto\nPoint 2\nin Fig.\n16\n(a) and then returns via a distinct route. During the flight, our decentralized swarm performs visibility-aware cooperative target tracking without collisions or occlusions. An Insta360 camera on the target drone records the process. Four snapshots (Figs.\n16\n(b)-(e)) illustrate how the trackers agilely adjust the swarm distribution to avoid LOS blockages by\nTree1\nand\nTree2\n, leveraging the SSDFs for occlusion-resistant tracking in real scenes. Driven by our joint costs, our decentralized swarm self-organized into a tetrahedron distribution while still ensuring FOV compliance. Tab.\nIII\nsummarizes the average onboard computation time for each stage, including SSDF update (\nt\nSSDF\nt_{\\text{SSDF}}\n), front-end searching (\nt\nsearch\nt_{\\text{search}}\n), corridor generation (\nt\nSFC\nt_{\\text{SFC}}\n), and back-end optimization (\nt\noptimize\nt_{\\text{optimize}}\n). The results characterize the runtime performance of our method in cluttered forest environments. Compared to other LiDAR configurations, drones with Avia LiDARs exhibit longer back-end runtime due to the computational demands of joint yaw trajectory optimization for target alignment.\nTABLE III:\nAverage Onboard Computation Time (milliseconds)\nt\nSSDF\nt_{\\text{SSDF}}\nt\nsearch\nt_{\\text{search}}\nt\nSFC\nt_{\\text{SFC}}\nt\noptimize\nt_{\\text{optimize}}\nt\ntotal\nt_{\\text{total}}\nUpward Mid360\n7.76\n0.26\n3.17\n9.35\n20.54\nDownward Mid360\n7.61\n0.22\n3.10\n8.93\n19.86\nAVIA\n8.53\n0.35\n3.19\n13.20\n25.27\nIX-C\nCooperative Human Runner Tracking\nFigure 17:\nFour drones cooperatively track a human runner as the target.\n(a)\nThe write curve depicts the runnerâ€™s route. Three keyframes along the path are highlighted by the adjacent image snapshots, and two spots, labeled\nb\nb\nand\nc\nc\n, are marked for detailed illustration in subfigures (b) and (c).\n(b)\nA composite image captures the swarm agilely compressing its tracking distribution as the target runs through a gate at spot\nb\nb\n.\n(c)\nA series of 360Â° camera snapshots shows the swarm rotating the distribution to prevent occlusion as the target walks through the pillar obstacles at spot\nc\nc\n.\nTo further validate our methodâ€™s practicality, we deploy the swarm system to track a human runner. The runnerâ€™s motion transitions between walking (\nâˆ¼\n1\nâ€‹\nm\n/\ns\n\\sim 1m/s\n) and running (\nâˆ¼\n2.5\nâ€‹\nm\n/\ns\n\\sim 2.5m/s\n). The swarm uses the same LiDAR setup as Sec.\nIX-B\n. The runner wears a high-reflectivity vest for fast LiDAR detection. Fig.\n17\n(a) shows the scenario and the target route. Fig.\n17\n(b) is a composite image recording the swarm motion when the target is running through a gate. To preserve target visibility, the trackers flexibly compress the swarm distribution to traverse the constrained gateway and then elastically resume the tetrahedron tracking formation in open space, demonstrating the agility of our swarm. Fig.\n17\n(c) presents Insta360 snapshots when the target walks through pillar obstacles. Driven by SSDFs, the swarm rotates the distribution to avoid occlusions. The swarmâ€™s full visibility throughout the targetâ€™s walking and running phases also confirms the systemâ€™s reactive adaptation to target velocity changes.\nIX-D\nSwarm Tracking with Dynamic Joining and Leaving\nTo demonstrate the systemâ€™s dynamic swarm reconfigurability, we conduct an experiment that supports teammates joining and leaving during live target tracking. A quadrotor marked by high-reflectivity tapes serves as the target, pursued by four drones (UAV1-4) with heterogeneous LiDAR setups: UAV1 and UAV4 (upward Mid360 LiDARs), UAV2 (downward Mid360 LiDAR), and UAV3 (Avia LiDAR). Fig.\n18\nrecords the tracking mission. Before starting, UAV1-3 are positioned in area\nP\n1\nP_{1}\n, where they complete swarm initialization\n[\n26\n]\nto calibrate extrinsic transformations and form a three-drone swarm. UAV4, placed in area\nP\n2\nP_{2}\n, begins as an isolated agent, excluded from initial target detection and the swarm.\nFigure 18:\nSwarm tracking experiment with dynamic joining and leaving.\n(a)\nThe white curve shows the target droneâ€™s route. Four areas, labeled as\nP\n1\nP_{1}\nto\nP\n4\nP_{4}\n, are marked for further illustration in the following subfigures.\n(b)-(e)\n: As the target enters area\nP\n1\nP_{1}\n, the swarm of UAV1-3 detects the target, starts cooperative tracking, and forms a regular triangle distribution.\n(f)-(i)\n: In\nP\n2\nP_{2}\n, UAV4 detects the target and initially performs solo tracking in (f)-(g). After the online calibration is complete in (h), UAV4 joins the swarm and then forms a tetrahedron encirclement with its teammates.\n(j)-(k)\n: In\nP\n3\nP_{3}\n, the swarm reconfigures the distribution from a tetrahedron to a regular triangle after UAV2 drops out.\n(j)-(k)\n: In\nP\n4\nP_{4}\n, the swarm transitions to collinear encirclement after UAV3 leaves.\nThe mission begins as the target enters area\nP\n1\nP_{1}\n, where the three-drone swarm detects it and starts cooperative tracking. Although the FOV limitations prevent UAV2 and UAV3 from direct target observation, UAV1 first detects the target and shares its target measurements with UAV2 and UAV3 at once via UDP, facilitating immediate coordinated tracking maneuvers across all agents (Fig.\n18\n(b)-(c)). Driven by the swarm distribution cost, the three drones form a triangular encirclement to track the target (Fig.\n18\n(d)-(e)). UAV4 remains at its initial position in\nP\n2\nP_{2}\nand actively attempts to detect the target. When the target enters area\nP\n2\nP_{2}\n, UAV4 successfully detects it and starts tracking. Since UAV4 has not conducted the prior swarm initialization\n[\n26\n]\n, its extrinsic transformations are not calibrated with the other drones, isolating it from the swarm. Hence, UAV4 tracks the target solo and treats other drones as dynamic obstacles (Fig.\n18\n(g)). Meanwhile, the Swarm-LIO modules perform online initialization for UAV4 and the swarm. Once completed, UAV4 joins the swarm (Fig.\n18\n(h)) and then forms a new tetrahedron distribution with the other swarm members (Fig.\n18\n(i)). As the target enters area\nP\n3\nP_{3}\n, UAV2 is deliberately terminated to emulate agent failure. The system detects the dropout and adaptively reconfigures the swarm into a regular triangle formation to continue the tracking (Fig.\n18\n(j)-(h)). Later, in area\nP\n4\nP_{4}\n, the swarm size further decreases to two by dropping out UAV3. The left two trackers then form a collinear encirclement to optimize the distribution cost (Fig.\n18\n(l)-(m)).\nIn the experiment, our swarm maintains uninterrupted target tracking during dynamic membership changes, highlighting the systemâ€™s superior decentralized swarm reconfigurability. The inherent scalability of the proposed swarm tracking cost functions enables automatic adaptation to swarm-size variations without any hardcoded rules. The dropout tests further verify the systemâ€™s robustness and fault tolerance.\nX\nConclusion\nThis paper proposed a systematic solution for visibility-aware cooperative target tracking using decentralized LiDAR-based swarms. To address environmental occlusion, we introduced a Spherical Signed Distance Field (SSDF) that encodes 3-D visibility and supports real-time onboard computation with an efficient update algorithm. Comprehensive differentiable metrics were developed for swarm tracking, notably including FOV-alignment costs for heterogeneous LiDARs and an electrostatic-potential-inspired swarm distribution cost for coordinated 3-D target coverage. These components powered a two-stage planning framework comprising a kinodynamic searcher and a spatiotemporal\nS\nâ€‹\nE\nâ€‹\n(\n3\n)\nSE(3)\noptimizer, trading off objectives for visibility-aware cooperative swarm tracking. Deployed on real-world LiDAR-based swarms featuring collaborative perception and dynamic reconfigurability, our decentralized system demonstrated robust performance in complex outdoor environments through rigorous experiments.\nReferences\n[1]\nB.Â F. Jeon and H.Â J. Kim, â€œOnline trajectory generation of a mav for chasing a moving target in 3d dense environments,â€ in\n2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n.â€ƒIEEE, 2019, pp. 1115â€“1121.\n[2]\nB.Â Jeon, Y.Â Lee, and H.Â J. Kim, â€œIntegrated motion planner for real-time aerial videography with a drone in a dense environment,â€ in\n2020 IEEE International Conference on Robotics and Automation (ICRA)\n.â€ƒIEEE, 2020, pp. 1243â€“1249.\n[3]\nQ.Â Wang, Y.Â Gao, J.Â Ji, C.Â Xu, and F.Â Gao, â€œVisibility-aware trajectory optimization with application to aerial tracking,â€ in\n2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n.â€ƒIEEE, 2021, pp. 5249â€“5256.\n[4]\nY.Â Gao, J.Â Ji, Q.Â Wang, R.Â Jin, Y.Â Lin, Z.Â Shang, Y.Â Cao, S.Â Shen, C.Â Xu, and F.Â Gao, â€œAdaptive tracking and perching for quadrotor in dynamic scenarios,â€\nIEEE Transactions on Robotics\n, vol.Â 40, pp. 499â€“519, 2023.\n[5]\nJ.Â Ji, N.Â Pan, C.Â Xu, and F.Â Gao, â€œElastic tracker: A spatio-temporal trajectory planner for flexible aerial tracking,â€ in\n2022 International Conference on Robotics and Automation (ICRA)\n.â€ƒIEEE, 2022, pp. 47â€“53.\n[6]\nX.Â Zhou, X.Â Wen, Z.Â Wang, Y.Â Gao, H.Â Li, Q.Â Wang, T.Â Yang, H.Â Lu, Y.Â Cao, C.Â Xu\netÂ al.\n, â€œSwarm of micro flying robots in the wild,â€\nScience Robotics\n, vol.Â 7, no.Â 66, p. eabm5954, 2022.\n[7]\nL.Â Yin, F.Â Zhu, Y.Â Ren, F.Â Kong, and F.Â Zhang, â€œDecentralized swarm trajectory generation for lidar-based aerial tracking in cluttered environments,â€ in\n2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n.â€ƒIEEE, 2023, pp. 9285â€“9292.\n[8]\nY.Â Lee, J.Â Park, and H.Â J. Kim, â€œDmvc-tracker: Distributed multi-agent trajectory planning for target tracking using dynamic buffered voronoi and inter-visibility cells,â€\narXiv preprint arXiv:2411.18086\n, 2024.\n[9]\nJ.Â Wang, P.Â Ren, M.Â Gong, J.Â Snyder, and B.Â Guo, â€œAll-frequency rendering of dynamic, spatially-varying reflectance,â€ in\nACM SIGGRAPH Asia 2009 papers\n, 2009, pp. 1â€“10.\n[10]\nR.Â Wang, M.Â Pan, W.Â Chen, Z.Â Ren, K.Â Zhou, W.Â Hua, and H.Â Bao, â€œAnalytic double product integrals for all-frequency relighting,â€\nIEEE Transactions on Visualization and Computer Graphics\n, vol.Â 19, no.Â 7, pp. 1133â€“1142, 2012.\n[11]\nR.Â Wang, M.Â Pan, X.Â Han, W.Â Chen, and H.Â Bao, â€œParallel and adaptive visibility sampling for rendering dynamic scenes with spatially varying reflectance,â€\nComputers & graphics\n, vol.Â 38, pp. 374â€“381, 2014.\n[12]\nK.Â Iwasaki, K.Â Mizutani, Y.Â Dobasbi, and T.Â Nisbita, â€œInteractive cloth rendering of microcylinder appearance model under environment lighting,â€ in\nComputer Graphics Forum\n, vol.Â 33, no.Â 2.â€ƒWiley Online Library, 2014, pp. 333â€“340.\n[13]\nR.Â Tallamraju, E.Â Price, R.Â Ludwig, K.Â Karlapalem, H.Â H. BÃ¼lthoff, M.Â J. Black, and A.Â Ahmad, â€œActive perception based formation control for multiple aerial vehicles,â€\nIEEE Robotics and Automation Letters\n, vol.Â 4, no.Â 4, pp. 4491â€“4498, 2019.\n[14]\nX.Â Liu, K.Â Liu, T.Â Hu, and Q.Â Zhang, â€œFormation control for moving target enclosing via relative localization,â€ in\n2023 62nd IEEE Conference on Decision and Control (CDC)\n.â€ƒIEEE, 2023, pp. 1400â€“1405.\n[15]\nX.Â Liu, D.Â Zhang, Q.Â Zhang, and T.Â Hu, â€œFormation control for enclosing and tracking via relative localization,â€\nIEEE Transactions on Aerospace and Electronic Systems\n, 2025.\n[16]\nA.Â G. Kendall, N.Â N. Salvapantula, and K.Â A. Stol, â€œOn-board object tracking control of a quadcopter with monocular vision,â€ in\n2014 international conference on unmanned aircraft systems (ICUAS)\n.â€ƒIEEE, 2014, pp. 404â€“411.\n[17]\nH.Â Cheng, L.Â Lin, Z.Â Zheng, Y.Â Guan, and Z.Â Liu, â€œAn autonomous vision-based target tracking system for rotorcraft unmanned aerial vehicles,â€ in\n2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)\n.â€ƒIEEE, 2017, pp. 1732â€“1738.\n[18]\nZ.Â Han, R.Â Zhang, N.Â Pan, C.Â Xu, and F.Â Gao, â€œFast-tracker: A robust aerial system for tracking agile target in cluttered environments,â€ in\n2021 IEEE International Conference on Robotics and Automation (ICRA)\n.â€ƒIEEE, 2021, pp. 328â€“334.\n[19]\nB.Â Penin, P.Â R. Giordano, and F.Â Chaumette, â€œVision-based reactive planning for aggressive target tracking while avoiding collisions and occlusions,â€\nIEEE Robotics and Automation Letters\n, vol.Â 3, no.Â 4, pp. 3725â€“3732, 2018.\n[20]\nR.Â Bonatti, W.Â Wang, C.Â Ho, A.Â Ahuja, M.Â Gschwindt, E.Â Camci, E.Â Kayacan, S.Â Choudhury, and S.Â Scherer, â€œAutonomous aerial cinematography in unstructured environments with learned artistic decision-making,â€\nJournal of Field Robotics\n, vol.Â 37, no.Â 4, pp. 606â€“641, 2020.\n[21]\nT.Â Michikawa and H.Â Suzuki, â€œSpherical distance transforms,â€ in\n2008 International Conference on Computational Sciences and Its Applications\n.â€ƒIEEE, 2008, pp. 405â€“412.\n[22]\nA.Â Bucker, R.Â Bonatti, and S.Â Scherer, â€œDo you see what i see? coordinating multiple aerial cameras for robot cinematography,â€ in\n2021 IEEE International Conference on Robotics and Automation (ICRA)\n.â€ƒIEEE, 2021, pp. 7972â€“7979.\n[23]\nC.Â Ho, A.Â Jong, H.Â Freeman, R.Â Rao, R.Â Bonatti, and S.Â Scherer, â€œ3d human reconstruction in the wild with collaborative aerial cameras,â€ in\n2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n.â€ƒIEEE, 2021, pp. 5263â€“5269.\n[24]\nT.Â NÃ¤geli, L.Â Meier, A.Â Domahidi, J.Â Alonso-Mora, and O.Â Hilliges, â€œReal-time planning for automated multi-view drone cinematography,â€\nACM Transactions on Graphics (TOG)\n, vol.Â 36, no.Â 4, pp. 1â€“10, 2017.\n[25]\nV.Â KrÃ¡tká»³, A.Â AlcÃ¡ntara, J.Â CapitÃ¡n, P.Â Å tÄ›pÃ¡n, M.Â Saska, and A.Â Ollero, â€œAutonomous aerial filming with distributed lighting by a team of unmanned aerial vehicles,â€\nIEEE Robotics and Automation Letters\n, vol.Â 6, no.Â 4, pp. 7580â€“7587, 2021.\n[26]\nF.Â Zhu, Y.Â Ren, L.Â Yin, F.Â Kong, Q.Â Liu, R.Â Xue, W.Â Liu, Y.Â Cai, G.Â Lu, H.Â Li\netÂ al.\n, â€œSwarm-lio2: Decentralized, efficient lidar-inertial odometry for uav swarms,â€\nIEEE Transactions on Robotics\n, 2024.\n[27]\nY.Â Ren, Y.Â Cai, F.Â Zhu, S.Â Liang, and F.Â Zhang, â€œRog-map: An efficient robocentric occupancy grid map for large-scene and high-resolution lidar-based motion planning,â€ in\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n.â€ƒIEEE, 2024, pp. 8119â€“8125.\n[28]\nL.Â Shi, L.Â Yin, F.Â Kong, Y.Â Ren, F.Â Zhu, B.Â Tang, and F.Â Zhang, â€œReal-time bandwidth-efficient occupancy grid map synchronization for multi-robot systems,â€ in\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n.â€ƒIEEE, 2024, pp. 8489â€“8496.\n[29]\nG.Â Lu, W.Â Xu, and F.Â Zhang, â€œOn-manifold model predictive control for trajectory tracking on robotic systems,â€\nIEEE Transactions on Industrial Electronics\n, vol.Â 70, no.Â 9, pp. 9192â€“9202, 2022.\n[30]\nP.Â F. Felzenszwalb and D.Â P. Huttenlocher, â€œDistance transforms of sampled functions,â€\nTheory of computing\n, vol.Â 8, no.Â 1, pp. 415â€“428, 2012.\n[31]\nA.Â Meijster, J.Â B. Roerdink, and W.Â H. Hesselink, â€œA general algorithm for computing distance transforms in linear time,â€\nMathematical Morphology and its applications to image and signal processing\n, pp. 331â€“340, 2000.\n[32]\nC.Â R. Maurer, R.Â Qi, and V.Â Raghavan, â€œA linear time algorithm for computing exact euclidean distance transforms of binary images in arbitrary dimensions,â€\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n, vol.Â 25, no.Â 2, pp. 265â€“270, 2003.\n[33]\nD.Â W. Paglieroni, â€œA unified distance transform algorithm and architecture,â€\nMachine Vision and Applications\n, vol.Â 5, no.Â 1, pp. 47â€“55, 1992.\n[34]\nL.Â Han, F.Â Gao, B.Â Zhou, and S.Â Shen, â€œFiesta: Fast incremental euclidean distance fields for online motion planning of aerial robots,â€ in\n2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n.â€ƒIEEE, 2019, pp. 4423â€“4430.\n[35]\nD.Â Zhu, C.Â Wang, W.Â Wang, R.Â Garg, S.Â Scherer, and M.Â Q.-H. Meng, â€œVdb-edt: An efficient euclidean distance transform algorithm based on vdb data structure,â€\narXiv preprint arXiv:2105.04419\n, 2021.\n[36]\nH.Â Oleynikova, Z.Â Taylor, M.Â Fehr, R.Â Siegwart, and J.Â Nieto, â€œVoxblox: Incremental 3d euclidean signed distance fields for on-board mav planning,â€ in\n2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n.â€ƒIEEE, 2017, pp. 1366â€“1373.\n[37]\nY.Â Pan, Y.Â Kompis, L.Â Bartolomei, R.Â Mascaro, C.Â Stachniss, and M.Â Chli, â€œVoxfield: Non-projective signed distance fields for online planning and 3d reconstruction,â€ in\n2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n.â€ƒIEEE, 2022, pp. 5331â€“5338.\n[38]\nM.Â Watterson and V.Â Kumar, â€œControl of quadrotors using the hopf fibration on so (3),â€ in\nRobotics Research: The 18th International Symposium ISRR\n.â€ƒSpringer, 2019, pp. 199â€“215.\n[39]\nJ.Â Tomson, â€œOn the structure of the atom: an investigation of the stability and periods of osciletion of a number of corpuscles arranged at equal intervals around the circumference of a circle; with application of the results to the theory atomic structure,â€\nPhilos. Mag. Series\n, vol.Â 6, no.Â 7, p. 237, 1904.\n[40]\nS.Â Smale, â€œMathematical problems for the next century,â€\nThe Mathematical Intelligencer\n, vol.Â 20, pp. 7â€“15, 1998. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:1331144\n[41]\nD.Â Dolgov, S.Â Thrun, M.Â Montemerlo, and J.Â Diebel, â€œPractical search techniques in path planning for autonomous driving,â€\nann arbor\n, vol. 1001, no. 48105, pp. 18â€“80, 2008.\n[42]\nS.Â Liu, N.Â Atanasov, K.Â Mohta, and V.Â Kumar, â€œSearch-based motion planning for quadrotors using linear quadratic minimum time control,â€ in\n2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)\n.â€ƒIEEE, 2017, pp. 2872â€“2879.\n[43]\nB.Â Zhou, F.Â Gao, L.Â Wang, C.Â Liu, and S.Â Shen, â€œRobust and efficient quadrotor trajectory generation for fast autonomous flight,â€\nIEEE Robotics and Automation Letters\n, vol.Â 4, no.Â 4, pp. 3529â€“3536, 2019.\n[44]\nS.Â Liu, M.Â Watterson, K.Â Mohta, K.Â Sun, S.Â Bhattacharya, C.Â J. Taylor, and V.Â Kumar, â€œPlanning dynamically feasible trajectories for quadrotors using safe flight corridors in 3-d complex environments,â€\nIEEE Robotics and Automation Letters\n, 2017.\n[45]\nZ.Â Wang, X.Â Zhou, C.Â Xu, and F.Â Gao, â€œGeometrically constrained trajectory optimization for multicopters,â€\nIEEE Transactions on Robotics\n, vol.Â 38, no.Â 5, pp. 3259â€“3278, 2022.\n[46]\nK.Â L. Teo, V.Â Rehbock, and L.Â S. Jennings, â€œA new computational algorithm for functional inequality constrained optimization problems,â€\nAutomatica\n, vol.Â 29, no.Â 3, pp. 789â€“792, 1993.",
    "preview_text": "Autonomous aerial tracking with drones offers vast potential for surveillance, cinematography, and industrial inspection applications. While single-drone tracking systems have been extensively studied, swarm-based target tracking remains underexplored, despite its unique advantages of distributed perception, fault-tolerant redundancy, and multidirectional target coverage. To bridge this gap, we propose a novel decentralized LiDAR-based swarm tracking framework that enables visibility-aware, cooperative target tracking in complex environments, while fully harnessing the unique capabilities of swarm systems. To address visibility, we introduce a novel Spherical Signed Distance Field (SSDF)-based metric for 3-D environmental occlusion representation, coupled with an efficient algorithm that enables real-time onboard SSDF updating. A general Field-of-View (FOV) alignment cost supporting heterogeneous LiDAR configurations is proposed for consistent target observation. Swarm coordination is enhanced through cooperative costs that enforce inter-robot safe clearance, prevent mutual occlusions, and notably facilitate 3-D multidirectional target encirclement via a novel electrostatic-potential-inspired distribution metric. These innovations are integrated into a hierarchical planner, combining a kinodynamic front-end searcher with a spatiotemporal $SE(3)$ back-end optimizer to generate collision-free, visibility-optimized trajectories.Deployed on heterogeneous LiDAR swarms, our fully decentralized implementation features collaborative perception, distributed planning, and dynamic swarm reconfigurability. Validated through rigorous real-world experiments in cluttered outdoor environments, the proposed system demonstrates robust cooperative tracking of agile targets (drones, humans) while achieving superior visibility maintenance.\n\nVisibility-aware Cooperative Aerial Tracking with Decentralized LiDAR-based Swarms\nLongji Yin\n1\n, Yunfan Ren\n1\n, Fangcheng Zhu\n1\n, Liuyu Shi\n1\n, Fan",
    "is_relevant": null,
    "relevance_score": 0.0,
    "extracted_keywords": [],
    "one_line_summary": "",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T04:52:50Z",
    "created_at": "2026-01-08T10:08:07.035593",
    "updated_at": "2026-01-08T10:08:07.035604",
    "flag": true
}