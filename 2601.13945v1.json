{
    "id": "2601.13945v1",
    "title": "Efficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework",
    "authors": [
        "Yixuan Deng",
        "Tongrun Wu",
        "Donghao Wu",
        "Zeyu Wei",
        "Jiayuan Wang",
        "Zhenglong Sun",
        "Yuqing Tang",
        "Xiaoqiang Ji"
    ],
    "abstract": "éšç€å…·èº«äººå·¥æ™ºèƒ½ç³»ç»Ÿä»ç ”ç©¶åŸå‹èµ°å‘ç°å®ä¸–ç•Œéƒ¨ç½²ï¼Œå…¶æ¼”è¿›é€Ÿåº¦æ—¥ç›ŠåŠ å¿«ï¼ŒåŒæ—¶éœ€è¦åœ¨å·¥ä½œè´Ÿè½½å˜åŒ–å’Œå±€éƒ¨æ•…éšœä¸‹ä¿æŒå¯é æ€§ã€‚å®è·µä¸­ï¼Œè®¸å¤šéƒ¨ç½²ç³»ç»Ÿä»…å®ç°éƒ¨åˆ†è§£è€¦ï¼šä¸­é—´ä»¶è´Ÿè´£æ¶ˆæ¯ä¼ é€’ï¼Œä½†å…±äº«ä¸Šä¸‹æ–‡ä¸åé¦ˆè¯­ä¹‰å¾€å¾€éšå¼å­˜åœ¨ï¼Œå¯¼è‡´æ¥å£æ¼‚ç§»ã€è·¨æ¨¡å—å¹²æ‰°ä»¥åŠå¤§è§„æ¨¡åœºæ™¯ä¸‹çš„è„†å¼±æ¢å¤æœºåˆ¶ã€‚æœ¬æ–‡æå‡ºANCHORâ€”â€”ä¸€ä¸ªå°†è§£è€¦ä¸é²æ£’æ€§æ˜ç¡®ä½œä¸ºç³»ç»Ÿçº§åŸè¯­çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†ç¦»ï¼ˆiï¼‰å¯æ¼”åŒ–çš„æ ‡å‡†åŒ–å…±äº«çŠ¶æ€å¥‘çº¦ï¼ˆè§„èŒƒè®°å½•ï¼‰ä¸ï¼ˆiiï¼‰æ”¯æŒå¤šå¯¹å¤šä¼ æ’­åŠé¢å‘åé¦ˆåè°ƒçš„é€šä¿¡æ€»çº¿ï¼Œæ„å»ºå‡ºå¯è§‚æµ‹çš„ç«¯åˆ°ç«¯é—­ç¯ã€‚æˆ‘ä»¬åœ¨åŒ¿ååŒ–å·¥ä½œæµå®ä¾‹ä¸ŠéªŒè¯äº†é—­ç¯å¯è¡Œæ€§ï¼Œåˆ†æäº†ä¸åŒè´Ÿè½½è§„æ¨¡ä¸å‘å¸ƒé€Ÿç‡ä¸‹çš„å»¶è¿Ÿåˆ†å¸ƒç‰¹å¾ï¼Œå¹¶æ¼”ç¤ºäº†å³ä½¿åœ¨å…±äº«å†…å­˜ä¸¢å¤±æƒ…å†µä¸‹é­é‡ç¡¬å´©æºƒä¸é‡å¯åï¼Œç³»ç»Ÿä»èƒ½å®ç°æ•°æ®æµè‡ªåŠ¨æ¢å¤ã€‚æ€»ä½“è€Œè¨€ï¼ŒANCHORå°†ä¸´æ—¶é›†æˆç²˜åˆå±‚è½¬åŒ–ä¸ºæ˜¾å¼å¥‘çº¦ï¼Œä½¿é—­ç¯äººå·¥æ™ºèƒ½ç³»ç»Ÿèƒ½å¤Ÿåœ¨è´Ÿè½½ä¸‹å®ç°å¯æ§é™çº§ä¸è‡ªæ„ˆæ¢å¤ï¼Œä»è€Œæ”¯æŒå¯æ‰©å±•éƒ¨ç½²ã€‚",
    "url": "https://arxiv.org/abs/2601.13945v1",
    "html_url": "https://arxiv.org/html/2601.13945v1",
    "html_content": "[\nrole=First Author,\n]\n [\nrole=co-First Author,\n]\n [\n]\n[\n]\n[\n]\n [\n]\n [\n]\n[\nrole=Corresponding Author,\norcid=0000-0002-8556-3579\n]\n\\credit\nConceptualization, Methodology, Software, Writing - Original Draft\n\\cortext\n[cor1]Corresponding author\nEfficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework\nYixuan Deng\nTongrun Wu\nDonghao Wu\nZeyu Wei\nJiayuan Wang\nZhenglong Sun\nYuqing Tang\nXiaoqiang Ji\njixiaoqiang@cuhk.edu.cn\nSchool of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, 2001 Longxiang Boulevard, Shenzhen, China\nSchool of Artificial Intelligence, The Chinese University of Hong Kong, Shenzhen, 2001 Longxiang Boulevard, Shenzhen, China\nShenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China\nInternational Digital Economy Academy, Shenzhen-Hong Kong Collaborative Innovation Center, Shenzhen, China\nSchool of Data Science, The Chinese University of Hong Kong, Shenzhen, 2001 Longxiang Boulevard, Shenzhen, China\nThe School of Computer Science, The University of Sydney, Sydney, Australia\nAbstract\nAs Embodied AI systems move from research prototypes to real world deployments, they tend to evolve rapidly while remaining reliable under workload changes and partial failures.\nIn practice, many deployments are only\npartially decoupled\n: middleware moves messages, but shared context and feedback semantics are implicit, causing interface drift, cross-module interference, and brittle recovery at scale.\nWe present\nANCHOR\n, a modular framework that makes decoupling and robustness explicit system-level primitives.\nANCHOR\nseparates (i)\nCanonical Records\n, an evolvable contract for the standardized shared state, from (ii) a\ncommunication bus\nfor many-to-many dissemination and feedback-oriented coordination, forming an inspectable end-to-end loop.\nWe validate closed-loop feasibility on a de-identified workflow instantiation, characterize latency distributions under varying payload sizes and publish rates, and demonstrate automatic stream resumption after hard crashes and restarts even with shared-memory loss.\nOverall,\nANCHOR\nturns ad-hoc integration glue into explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems.\nkeywords:\nEmbodied AI\n\\sep\nComputing Architecture\n\\sep\nRobustness\n1\nIntroduction\nEmbodied AI has progressed rapidly across autonomous driving, robotics, and aerial autonomy.\nIn driving, unified end-to-end frameworks increasingly model the full stack from perception to planning (e.g., UniAD, CVPR 2023)\n[\nhu2023planning\n]\n;\nin robotics, learning-based controllers that execute reliably on real hardware have advanced quickly (e.g., diffusion policy, RSS 2023)\n[\nchi2023diffusion\n]\n;\nand in aerial autonomy, highly dynamic real-world systems have reached (and in some settings rivaled) top human performance (e.g., the swift drone racing system, Nature 2023)\n[\nkaufmann2023champion\n]\n.\nThis fast-moving landscape places growing pressure on the\nsystem layer\n: practical deployments must support rapid iteration, where heterogeneous components evolve and recombine, while still providing dependable runtime behavior as task demands expand.\nTo keep pace with iteration pressure, the system layer must provide\ndecoupling\nas a default engineering property.\nIn practice, embodied deployments rarely evolve as a cleanly separated pipeline; instead, perception, state maintenance, decision making, tool use, and execution control\nco-evolve at different cadences and are repeatedly recomposed under changing tasks and hardware.\nDecoupling matters because it allows components to be replaced independently, constrains how load spikes and faults propagate across modules, and reduces integration overhead.\nHowever, many deployed stacks remain only\npartially decoupled\n.\nEven when publishâ€“subscribe middleware is used, interface contracts are often implicit (e.g., undocumented message semantics, ad-hoc topic conventions, duplicated context),\nand coordination logic tends to leak across module boundaries as the system grows.\nAs a result, iteration frequently induces interface drift and hidden coupling, and integration cost can dominate development cycles\n[\nmacenski2022robot\n,\neugster2003many\n]\n.\nThis motivates treating decoupling not merely as a middleware choice, but as a\nsystem-level design commitment\n.\nThis motivation underlies the long-standing use of modular middleware and componentized architectures in robotics, ranging from earlier robot platforms and middleware\n(e.g., YARP and Orocos)\n[\nmetta2006yarp\n,\nbruyninckx2001open\n]\nto lightweight robotics messaging systems (e.g., LCM)\n[\nhuang2010lcm\n]\n,\nto modern deployments such as ROSÂ 2\n[\nmacenski2022robot\n]\n,\nas well as microservice-based control architectures that emphasize reuse and interruptible concurrency in safety-critical mobile robots\n[\nschrick2025microservice\n]\n.\nThese systems also reflect general publishâ€“subscribe principles that decouple producers and consumers\n[\neugster2003many\n]\n.\nWe therefore view decoupling-enabled modularity as an engineering amplifier: it does not prescribe a learning paradigm (end-to-end or otherwise), but it can substantially improve maintainability, evolvability, and robustness in deployment.\nBeyond evolvability, real-world physical environments impose a second constraint: robustness under long-horizon operation and deployment-time disturbances.\nIn real-world physical environments, embodied agents must operate for long durations under imperfect sensing and actuation, while continuously interacting with humans, other agents, and changing environments.\nThese settings inevitably introduce distribution shifts, long-tail events, and uncertainty, raising the bar for system-level safety and robustness.\nAccordingly, recent work increasingly treats safety and robustness as first-class goals, including unified views of runtime assurance and safety filters\n[\nhsu2023safety\n]\n,\nsafe motion planning under uncertainty in traffic\n[\nlei2025safe\n]\n,\nsafe and platform-aware navigation modeling in complex terrain\n[\nroth2025learned\n]\n,\nbroader discussions of robustness challenges for autonomous vehicles\n[\nchen2025toward\n]\n,\nand scalable learning-based safety filters in practice\n[\nnguyen2025gameplay\n]\n.\nHowever, integrating these ideas into a\nreusable and evolvable systems framework\nremains an important engineering challenge as embodied AI scales to diverse deployments: interfaces drift as modules evolve, shared context becomes fragmented across components, and recovery and degradation behaviors are often not made explicit at the system level\n[\nmacenski2022robot\n,\neugster2003many\n]\n. In many deployments, recovery, degradation, and fault-containment behaviors are not made explicit at the system layer and instead emerge from ad-hoc glue code and local conventions.\nThis implicitness makes robustness fragile under scale: when components are tightly coupled through shared-but-undefined context, anomalies and load fluctuations can amplify across the stack\n[\nmacenski2022robot\n,\neugster2003many\n]\n.\nMotivated by these requirements, we introduce\nANCHOR\n, a modular framework for embodied AI systems that provides stable engineering support amid rapid iteration and real-world deployment.\nANCHOR does not assume an end-to-end or a staged realization; instead, it offers clear system abstractions and interface semantics along the sensing-to-execution chain, enabling key capabilities to be replaced, extended, and composed independently.\nConcretely, ANCHOR organizes the system around two system-level, stable interfaces:\n(i) Canonical Records\n, a shared and evolvable representation of normalized observations and system context, and\n(ii) Communication Bus\nthat supports many-to-many dissemination, concurrency-aware delivery, and feedback interactions for closed-loop execution\n[\neugster2003many\n]\n.\nBy making shared state and inter-component communication explicit system mechanisms (rather than implicit, ad-hoc glue),\nANCHOR aims to provide clearer operational behavior under load, partial failures, and restarts, while keeping pace with evolving task demands and model capabilities.\nThe main contributions of this work are as follows:\n1.\nA modular framework viewpoint for embodied deployments.\nWe formulate the systems-layer need for an evolvable framework that supports rapid iteration, heterogeneous component recomposition, and robust runtime operation in real deployments\n[\nmacenski2022robot\n,\neugster2003many\n]\n.\n2.\nCanonical Records as an explicit shared-state interface.\nWe introduce\ncanonical records\nas a stable and evolvable interface for normalized observations and shared system context, intended to reduce interface drift and implicit coupling across components.\n3.\nA high-concurrency inter-component communication bus.\nWe design and implement a high-concurrency communication mechanism based on multi-producer multi-consumer (MPMC) dissemination and concurrency governance, enabling multiple command sources, multiple executors, and multiple observers, and supporting feedback interactions for closed-loop execution\n[\neugster2003many\n]\n.\n4.\nEvidence of modularity and robustness in a closed-loop framework.\nWe provide empirical evidence that ANCHOR supports modular recomposition via explicit contracts, controlled degradation under load, and automatic recovery after hard crashes and restarts, validating the system-level properties targeted by our design.\nThe rest of this paper is organized as follows.\nSectionÂ 2 reviews related work.\nSectionÂ 3 presents the design of ANCHOR and provides an overall system view.\nSectionÂ 4 presents empirical evaluation results.\nSectionÂ 5 analyzes key mechanisms and critical paths.\nSectionÂ 6 discusses limitations and future directions.\nFinally, SectionÂ 7 concludes the paper.\n2\nRelated Work\nWe review prior work from three perspectives that motivate ANCHOR:\n(i) embodied system architectures and middleware, (ii) robustness under deployment constraints, and\n(iii) extensibility for rapid iteration.\nAnd we distinguish what is\nexplicitly defined and supported as a system-level primitive\nfrom what is left to downstream integration choices or external extensions.\n2.1\nEmbodied AI system architectures\nWhen embodied AI systems transition from research prototypes to real deployments, their architectures are rarely rebuilt from scratch.\nInstead, they typically build on mature middleware and tooling ecosystems, organizing perception, planning, and control as composable components\nintegrated through message-based communication.\nThis line of work primarily targets engineering concerns such as integration, reuse, and coordination, rather than prescribing whether higher-level\nalgorithms must be end-to-end or staged.\nROS provides a widely adopted communications layer and tooling ecosystem to connect functional modules across heterogeneous compute nodes\n[\nQuigley2009ROS\n]\n,\nand ROSÂ 2 re-architects the stack toward scalability and deployment readiness with documented real applications\n[\nmacenski2022robot\n]\n.\nBeyond ROS, modern embodied architectures exhibit recurring trade-offs:\nthey promote modularity and reuse while balancing concurrency, real-time constraints, and production readiness against overall complexity and maintainability.\nFor example, XBot2 emphasizes multi-threaded, mixed real-time (RT) and non-RT execution within a single system,\ntogether with hardware abstraction and pluggable components for cross-platform reuse\n[\nlaurenzi2023xbot2\n]\n.\nIn parallel, software-architecture research systematizes how architecture-based self-adaptation and reconfiguration\naddress runtime uncertainty and environmental changes\n[\nalberts2025software\n]\n.\nDespite these ecosystems, decoupling is often\nincomplete\nat the system level.\nMiddleware enables message exchange, but higher-level interface contractsâ€”such as the semantics and lifecycle of shared context, and how feedback is incorporated across componentsâ€”are frequently left to downstream integration choices.\nAs systems scale, these implicit contracts become a common source of hidden coupling and unpredictable cross-module interactions.\nRecent real-time systems studies further show that even within ROSÂ 2-style middleware, communication paths and scheduling interactions can materially shape end-to-end timing behavior\n[\nluo2025ros2\n]\n.\nComplementary modeling work highlights that inter-process communication introduces nontrivial delay components that require explicit reasoning\n[\nluo2023modeling\n]\n.\nMoreover, predictability can be sensitive to cross-layer priority effects when multiple components contend for shared resources\n[\nkim2025cros\n]\n. Executor policies and shared-resource scheduling in multi-threaded ROSÂ 2 can further alter end-to-end behavior, effectively becoming a hidden coupling point across otherwise modular components\n[\nal2024dynamic\n]\n.\nTogether, these observations suggest that concurrency governance and delivery semantics should be treated as first-order system concerns rather than incidental configuration.\nAs embodied tasks become longer-horizon, more collaborative, and faster-evolving, architectural concerns increasingly shift from merely â€œwiring modules to runâ€\ntoward enabling concurrent coordination among components and continual sensingâ€“adjustment during execution.\nSurveys on integrating foundation/large language models into robotics systems similarly organize the landscape around component views\n(e.g., communication, perception, planning, and control) and emphasize filtering/correction mechanisms for stable real-world execution\n[\nkim2024survey\n]\n.\nMotivated by this shift and without presupposing any learning paradigm, ANCHOR adopts a modular-framework perspective:\nclear component boundaries and communication organization serve as the substrate for concurrent coordination and feedback-driven interaction,\nsupporting rapid iteration and extensible capability growth\n[\nwang2024large\n]\n.\nTable 1:\nExplicit design coverage of system-level mechanisms.\nâœ“: explicitly provided as a system-level primitive with defined semantics/interfaces;\nÃ—\n\\times\n: not a primary system-level primitive (even if achievable via extensions).\nWork\nMPMC\npubâ€“sub\nFeedback\nprimitives\nQoS /\nbackpressure\nObservability\n& replay\nCanonical\nshared state\nROS/ROS2\nâœ“\nâœ“\nâœ“\nâœ“\nÃ—\n\\times\nBehavior Trees (pattern)\nÃ—\n\\times\nÃ—\n\\times\nÃ—\n\\times\nÃ—\n\\times\nÃ—\n\\times\nMicroservice control architectures\nÃ—\n\\times\nâœ“\nâœ“\nâœ“\nÃ—\n\\times\nANCHOR (this work)\nâœ“\nâœ“\nâœ“\nâœ“\nâœ“\n2.2\nRobustness under deployment constraints\nIn real deployments, embodied systems must not only complete tasks but also maintain stable and safe execution under uncertainty and dynamic interactions,\nmaking robustness and safety first-class requirements.\nOn the algorithmic side, runtime assurance and safety-filtering paradigms monitor and (when necessary) intervene on nominal policies during execution\n[\nhsu2023safety\n]\n.\nOn the systems and architecture side, recent efforts emphasize engineering mechanisms such as interruptibility, concurrency, and fail-safety,\nand explore distributed organizational structures to mitigate brittleness from centralized logic\n[\nschrick2025microservice\n]\n.\nMany approaches further introduce explicit and interpretable execution structuresâ€”including hierarchical state machines,\nbehavior trees, and crash-only recovery-style designsâ€”to support online monitoring, rapid interruption, and error recovery,\nwhile decomposing complex tasks into reusable units\n[\nharel1987statecharts\n,\ncolledanchise2018behavior\n,\ncandea2003crash\n]\n.\nAt the architecture level, organizing robot functionality into finer-grained services/components and reducing coupling to centralized logic\ncan improve concurrent coordination and contain fault propagation under anomalies\n[\nschrick2025microservice\n]\n.\nBehavior trees are widely used in practice due to their modular conditions/actions and natural support for fallback and retries;\nrecent work also systematizes BT properties and evaluation metrics, highlighting the need for consistent measurements\n[\ngugliermo2024evaluating\n]\n.\nThese directions provide important foundations for robust execution via execution structure, monitoring, and recovery.\nHowever, in real deployments robustness is often constrained by systems-layer coupling:\nwhen many modules run concurrently, information fans out to multiple consumers, and feedback must be incorporated online, robustness depends on how coordination is realized across components.\nWithout an explicit shared representation of state and context, interface drift and implicit coupling can be amplified by iteration and load fluctuations, increasing fault propagation and debugging costs.\nRecent analyses of executor behavior in ROSÂ 2 highlight that runtime scheduling choices can measurably affect end-to-end responsiveness in component pipelines\n[\ntang2023real\n]\n.\nSeparately, timing analysis of processing chains with data refreshing shows that seemingly local design decisions can propagate into system-level latency and stability properties\n[\ntang2024timing\n]\n.\nTogether, these observations reinforce a trend toward making coordination, feedback, and fault-containment\nexplicit\nas system mechanisms rather than emergent behavior from glue code.\nANCHOR targets this systems-layer gap by aligning (i) a unified yet evolvable shared representation (\ncanonical records\n) and\n(ii) concurrency-friendly inter-component communication and feedback as explicit system mechanisms.\n2.3\nExtensibility for rapid iteration\nRapid iteration in embodied systems often occurs at the level of capability composition:\nsensing modalities are added or replaced, models and policies are updated frequently,\nand execution stacks evolve with changing hardware form factors and task demands.\nDeployment-oriented systems therefore must allow components to iterate at different cadences and minimize full-stack rewiring and interface refactoring;\notherwise, integration overhead quickly dominates development cycles\n[\nmacenski2022robot\n]\n.\nIn practice, extensibility is accumulated through infrastructure at different layers.\nMixed RT/non-RT middleware emphasizes pluginized components and hardware abstraction to reduce coupling when integrating new devices and control modules\n[\nlaurenzi2023xbot2\n]\n,\nwhile mature open-source planning/\nmanipulation stacks often rely on community-driven extensions to close capability gaps and evolve via incremental additions\n[\nfresnillo2023extending\n]\n.\nAs embodied systems move toward heterogeneous integration and multi-agent collaboration, extensibility becomes an organizational capability across platforms, vendors, and subsystems:\nfleet and infrastructure management frameworks highlight standardized interfaces and\nadapter-based integration to append new robots and devices without rewriting the entire system\n[\nvalner2022scalable\n]\n.\nDespite these building blocks, the literature consistently notes that rapid iteration remains constrained by interface drift, dependency growth, and runtime uncertainty;\nconsequently, architecture-level mechanisms for adaptation and evolvability remain actively studied yet not fully converged\n[\nalberts2025software\n]\n. Empirical comparisons also suggest that middleware backends can exhibit distinct latency and robustness behaviors under dynamic network conditions, turning backend selection and tuning into a recurring integration decision as systems scale\n[\nchovet2025performance\n]\n.\nRecent work on workflow-oriented and context-aware orchestration in ROS-based deployments emphasizes that integration increasingly involves explicit workflow logic and shared context management,\nrather than simple point-to-point connections\n[\nochoa2024dynamic\n]\n.\nRelated frameworks for hybrid edgeâ€“cloud robot-assisted operations likewise highlight the practical need to manage shared context and task assignment across distributed resources\n[\nchauhan2024kriota\n]\n.\nThese trends amplify the value of stable interfaces for shared context and coordination as systems evolve and scale.\nFrom this perspective, ANCHOR is positioned as a modular framework: it emphasizes clear component boundaries and an evolvable shared representation (\nCanonical Records\n),\nand uses concurrency-friendly inter-component communication to sustain multi-component coordination, turning repeated refactoring into controlled, incremental evolution.\n2.4\nOperational definition of first-class mechanisms\nTo make comparisons concrete (Table\n1\n), we use\nfirst-class mechanism\nin an operational sense:\na concern is first-class if it is\nexplicitly defined and supported by the system layer\n(with clear semantics, interfaces, and lifecycle),\nrather than being left to ad-hoc downstream integration or achievable only via external extensions.\nThis definition distinguishes what a framework provides as a direct primitive from what may be possible but is not a primary design commitment.\nTo make this landscape concrete, Table\n1\nsummarizes whether representative systems and patterns\nexplicitly\ndefine several modularity and robustness concerns as first-class mechanisms (per the above definition).\nWe emphasize that this checklist reflects\nexplicit design emphasis\nrather than absolute capability.\nTable\n1\nhighlights that prior systems often provide strong support for communication ecosystems,\nQoS and operational tooling, or architectural isolation at specific layers\n[\nQuigley2009ROS\n,\nmacenski2022robot\n,\nschrick2025microservice\n]\n.\nANCHOR builds on these foundations and aims to further elevate a canonical shared-state contract (\ncanonical records\n)\nas an explicit system-level mechanism, aligning shared context with concurrency-friendly dissemination and feedback to support robust coordination under deployment constraints.\n3\nANCHOR Design\n3.1\nSystem Overview\nANCHOR is a modular framework for embodied AI systems designed to support rapid iteration and robust runtime operation.\nRather than committing to an end-to-end or staged realization, ANCHOR provides stable system-level abstractions and interface semantics along the sensing-to-execution chain,\nso heterogeneous components can evolve independently while remaining composable in deployment.\nAt high level, ANCHOR organizes the system around two explicit primitives:\ncanonical records\n, which provide a shared and evolvable representation for normalized observations and system context,\nand a\ncommunication bus\n, which provides many-to-many dissemination with concurrency-aware delivery and explicit support for feedback.\nThis separation makes both shared context and inter-component coordination explicit at the system layer, avoiding ad-hoc â€œglueâ€ logic scattered across components.\nFigure 1\n:\nANCHOR system overview. Upstream ingestion normalizes heterogeneous inputs into Canonical Records; inference consumes records and publishes commands/actions to the communication bus; execution subscribes and produces status/events; feedback is materialized back into Canonical Records, forming an explicit closed loop without a centralized monolithic controller.\nA typical closed-loop runtime flow (Fig.\n1\n) proceeds as follows.\nUpstream modules ingest raw signals and write normalized outputs into\ncanonical records\n.\nInference modules read the current context from\ncanonical records\nand publish action/control messages to the communication bus.\nExecution modules subscribe to these messages, carry out corresponding actions, and publish status/events back to the bus.\nDesignated subscribers absorb feedback and materialize it as state updates in\ncanonical records\n, closing the loop while keeping component boundaries intact.\nBecause cross-component context and coordination are mediated by stable primitives, the system supports recomposition with reduced rewiring,\nand its operational boundaries under concurrency can be reasoned about more directly.\n3.2\nDesign Goals\nANCHOR is guided by two primary goals:\nmodularity\nand\nrobustness under deployment constraints\n.\nWe make these goals concrete in terms of what the system layer must provide.\nModularity.\nEmbodied deployments evolve quickly: sensors, models, and execution stacks change at different cadences.\nThe system layer should therefore provide stable interfaces that (i) localize changes within a component, (ii) minimize cross-module rewiring when components are added or replaced, and\n(iii) reduce implicit coupling caused by undocumented message contracts or fragmented shared context.\nANCHOR pursues modularity by explicitly separating shared context (\ncanonical records\n) from coordination (\ncommunication bus\n), and by defining clear semantics for both.\nRobustness.\nReal-world operation introduces uncertainty, load fluctuations, and partial failures.\nRobustness in our setting is a property of runtime behavior under stress: the system should degrade in a controlled manner rather than exhibiting cascading failures,\nand it should recover cleanly after transient disconnections or restarts.\nANCHOR pursues robustness through concurrency-friendly dissemination (MPMC), explicit feedback pathways, and runtime mechanisms such as bounded buffering, prioritization/QoS knobs,\nand liveness monitoring for failure detection and recovery.\n3.3\nCanonical Records\nCanonical records\nprovide an explicit shared-state interface for cross-component context.\nThe key motivation is to reduce interface drift and implicit coupling when multiple components must consume or update overlapping state.\nStandardized, evolvable representation.\nInstead of passing heterogeneous raw payloads through chains of point-to-point adapters, upstream modules normalize observations into a canonical schema.\nThis schema acts as a contract: fields have explicit semantics and update conventions, enabling independently developed components to interoperate with fewer hidden assumptions.\nAs the system evolves, the schema can be extended in a controlled manner, preserving compatibility where needed.\nEfficient sharing and persistence.\nCanonical records\nare implemented on a shared memory region backed by\nmemmap\n, enabling efficient read/write access across processes while also supporting persistence for logging and replay.\nOn top of\nmemmap\n, ANCHOR adopts a uniform array-oriented layout contract, so heterogeneous components can access shared context without exposing internal implementation details.\nThis design reduces redundant copies and avoids forcing every interaction to go through the communication bus, while keeping state exchange explicit and inspectable at the system boundary.\nFigure 2\n:\nCanonical Records as an explicit shared-state contract. A memmap-backed region stores normalized observations and system context with an agreed layout and update conventions, enabling cross-process sharing, persistence, and controlled schema evolution.\nExplicit responsibilities in closed-loop execution.\nCanonical records\nare where normalized context is accumulated and where feedback is materialized as state updates (Fig.\n2\n).\nInference modules treat\nCanonical records\nas read-only context for decision making, while write paths are explicitly associated with ingestion and feedback handlers.\nThis separation preserves clear responsibilities and simplifies debugging under concurrency.\n3.4\nCommunication Bus\nWhile\nCanonical records\nanchor shared context, the communication bus anchors runtime coordination.\nIt adopts a publishâ€“subscribe abstraction to decouple producers and consumers, and is designed to support many-to-many (MPMC) dissemination as a first-class pattern.\nMessages and interaction patterns.\nThe bus carries two primary message classes:\n(i)\ncommands/actions\npublished by inference (or other decision-making) modules, and\n(ii)\nstatus/events\npublished by execution modules.\nBoth message classes follow the same dissemination mechanism, which makes feedback an explicit part of the runtime interaction model rather than an afterthought.\nRoles and routing.\nTo support both intra-cluster and cross-cluster coordination, the bus organizes routing into three roles:\nnodes\n(publishers/subscribers), a\nmaster\nthat maintains subscriptions and forwards messages within a cluster, and\ngateways\nthat bridge traffic across clusters and re-inject messages into target clusters.\nThis structure preserves local publishâ€“subscribe semantics while enabling scale-out beyond a single node group.\nFigure 3\n:\nCommunication bus roles and routing. A master maintains subscriptions and forwards messages within a cluster; gateways bridge cross-cluster traffic; nodes publish/subscribe to commands/actions and status/events for many-to-many coordination and feedback.\nTopic structure and basic QoS.\nTopics follow a hierarchical structure such as\n/channel/region/(nodeId)/prio\n.\nThe\nregion\nfield controls dissemination scope (e.g., local vs cross-cluster),\nnodeId\nsupports directed delivery when needed, and\nprio\nprovides a basic priority signal that can be mapped from message types.\nThis organization helps keep addressing explicit and supports priority-aware handling under load.\nHigh-load handling.\nTo reduce overhead under high throughput, nodes use shared-memory caches and asynchronous sending, and the bus supports batching for efficient transmission.\nTo avoid excessive delays at low traffic, a maximum residence time bounds how long messages may wait for batching.\nTogether, these mechanisms aim to keep delivery behavior controlled across a wide range of load regimes.\nLiveness and recovery.\nFor transient disconnects and process failures, the bus uses liveness monitoring (e.g., heartbeats) to detect outages and trigger reconnection.\nReconnection includes re-registering identity and subscriptions so routes can be reconstructed without manual intervention.\nThese mechanisms provide a systems-layer basis for recovery and reduce the chance that a single component failure permanently breaks closed-loop operation.\nRuntime subscription management.\nIn deployment, subscribers may need to adjust what they consume based on task phase and resource constraints.\nThe bus supports runtime subscription management so nodes can change subscriptions without global rewiring,\nenabling the system to adapt component coordination under evolving workloads.\n3.5\nModularity and Robustness in Practice\nANCHOR combines\ncanonical records\nand the communication bus to realize the design goals of\nmodularity\nand\nrobustness\nin practice.\nCanonical records\nstabilize cross-component context by making shared state explicit and evolvable, which localizes change and reduces interface drift as components iterate.\nThe communication bus complements this by governing concurrent coordination and feedback through explicit dissemination semantics, keeping interactions decoupled and inspectable at runtime.\nFrom a modularity perspective, explicit shared context and publishâ€“subscribe coordination reduce repeated integration work and global rewiring when components are added, replaced, or composed.\nFrom a robustness perspective, bounded buffering and prioritization provide controlled behavior under overload, while liveness monitoring and reconnection logic enable clean recovery from transient failures or restarts.\nWe later characterize these runtime properties empirically through latency distributions and recovery behavior under load and failures.\n4\nEmpirical Evaluation\n4.1\nOverview and Scope\nThis section empirically characterizes the runtime behavior of ANCHOR using an event-driven closed-loop automation workflow.\nMultiple upstream producers ingest heterogeneous observation streams and periodically perform window-based preprocessing and aggregation, writing normalized state into\ncanonical records\nas a shared contract across components.\nA policy inference module consumes the records and publishes action commands to a message bus; downstream executors subscribe to these commands, interact with remote service endpoints, and publish runtime status and outcome events back to the bus.\nSelected feedback signals are integrated into\ncanonical records\nand archived for replay, enabling traceability across iterations.\nOur goal is to validate end-to-end feasibility and characterize runtime properties of the messaging layer under concurrency and transient disruptions, rather than optimizing task-specific performance.\nTo avoid domain-specific disclosure, we abstract application details while preserving the system structure, interfaces, and representative workloads relevant to ANCHOR.\nWe evaluate ANCHOR along two axes.\nFirst, we describe and validate a complete closed-loop execution patternâ€”from upstream aggregation into\ncanonical records\n, to command publication, to downstream execution and feedback materializationâ€”to establish feasibility and traceability.\nSecond, we stress the message bus under increasing load and under transient disconnect/reconnect events, reporting distributional latency (ECDF and tail percentiles) and recovery behavior to characterize robustness at runtime.\n4.2\nImplementation and Measurement Setup\nCurrent testbed.\nAll experiments are conducted on a single physical machine in our current test environment (CPU: AMD EPYC 9754 with 16 CPU cores available to the experimental environment; memory: 128â€‰GB DRAM; GPU: NVIDIA RTX 3090 with 24â€‰GB memory for inference-related runs; OS: Ubuntu 22.04).\nThe reported numbers should be interpreted as an empirical characterization under this testbed and the current implementation, rather than a hardware-agnostic performance bound.\nWorkflow.\nThe evaluation pipeline is instantiated as a set of independent component processes connected via\ncanonical records\nand\nmessage bus\n.\nUpstream producers ingest heterogeneous observation streams and perform periodic window-based aggregation, writing the aggregated state into\ncanonical records\n.\nThe policy module reads\ncanonical records\nand publishes action commands; one or more executor processes subscribe to these commands, invoke remote endpoints, and publish runtime status and outcome events.\nFor replay and debugging, commands and events are recorded during execution.\nMetrics.\nWe report message-processing latency in a distributional form by plotting empirical cumulative distribution functions (ECDFs) and summarizing key percentiles (P50, P90, and P99).\nFor transient disconnect/reconnect tests, we additionally report detection time and reconnection time.\nUnless otherwise stated, we reuse the same setup and measurement conventions throughout this section.\n4.3\nClosed-Loop Execution Pattern\nWe validate a complete closed-loop execution pattern supported by ANCHOR.\nThe system proceeds in repeated cycles.\nUpstream producers ingest raw inputs, parse and sanitize the inputs, normalize them into a unified representation, and aggregate them over a fixed window to obtain a shareable state; the state is then written into Canonical Records.\nAlgorithm\n1\nshows simplified pseudocode of the upstream preprocessing stage, where the key step is to map raw inputs into a canonical feature representation and update windowed aggregates in\ncanonical records\n.\n(a)\nPayload=128B, Rate=1000 msg/s.\n(b)\nPayload=128B, Rate=5000 msg/s.\n(c)\nPayload=1024B, Rate=1000 msg/s.\n(d)\nPayload=1024B, Rate=5000 msg/s.\nFigure 4\n:\nMessage-bus delivery latency ECDFs under a 1\nÃ—\n\\times\n1 setup for four (payload size, publish rate) configurations.\nAt each cycle boundary, the policy module reads the latest state from\ncanonical records\n, generates the next action, and publishes it as a commands/actions message to the message bus.\nExecution-side components subscribe to the corresponding commands/actions stream; upon receiving a command, they orchestrate remote endpoint invocations and execution steps, and then publish runtime state changes, anomalies, and outcomes as status/events back to the same message bus.\nDesignated subscribers consume these status/events signals and materialize selected feedback into\ncanonical records\n, providing updated context for subsequent decision making.\nBecause commands and feedback share the same message-bus path, closed-loop interactions can be strengthened incrementally without breaking component boundaries, while remaining traceable in deployment settings.\nAlgorithm 1\nSimplified upstream preprocessing and record update.\n1:\nInput:\nobservation stream\nğ’®\n\\mathcal{S}\n, window size\nW\nW\n2:\nState:\nwindow buffer\nâ„¬\n\\mathcal{B}\n, running aggregates\nğ€\n\\mathbf{A}\n3:\nfor\neach observation\no\nâˆˆ\nğ’®\no\\in\\mathcal{S}\ndo\n4:\nx\nâ†\nParse\nâ€‹\n(\no\n)\nx\\leftarrow\\textsc{Parse}(o)\n5:\nx\nâ†\nClean\nâ€‹\n(\nx\n)\nx\\leftarrow\\textsc{Clean}(x)\n6:\nf\nâ†\nNormalize\nâ€‹\n(\nx\n)\nf\\leftarrow\\textsc{Normalize}(x)\nâŠ³\n\\triangleright\nmap to canonical features\n7:\nâ„¬\n.\nAppend\nâ€‹\n(\nf\n)\n\\mathcal{B}.\\textsc{Append}(f)\n8:\nif\nâ„¬\n\\mathcal{B}\nspans window\nW\nW\nthen\n9:\nğ€\nâ†\nAggregate\nâ€‹\n(\nâ„¬\n)\n\\mathbf{A}\\leftarrow\\textsc{Aggregate}(\\mathcal{B})\n10:\nWriteRecords\nâ€‹\n(\nğ€\n)\n\\textsc{WriteRecords}(\\mathbf{A})\nâŠ³\n\\triangleright\nupdate Canonical Records\n11:\nâ„¬\n.\nEvictOld\nâ€‹\n(\n)\n\\mathcal{B}.\\textsc{EvictOld}()\n12:\nend\nif\n13:\nend\nfor\nThe inference module is organized around projects.\nUpstream inputs are grouped into project-specific inputs, and each project is pre-configured with its inference policy at deployment time, including the selected model and associated inference parameters.\nAt runtime, the inference module reads the latest project state from\ncanonical records\nand runs inference according to the configuration bound to that project to produce an action command; the command is then published to the message bus for downstream execution.\nExecution modules subscribe to the commands/actions channel and wait for new commands in an event-driven manner.\nOnce a command arrives, an executor parses the command payload and triggers the corresponding execution logic.\nDuring execution, the executor organizes key state transitions (e.g., start, success, or failure) and relevant runtime information into feedback events; after the action completes, it publishes the feedback as a status/events message back to the bus.\nDesignated subscribers consume these feedback messages and materialize selected signals into\ncanonical records\n.\nAlgorithm\n2\nshows simplified pseudocode of subscription, consumption, feedback publication, and record updates on the execution side.\nAlgorithm 2\nSimplified execution-side subscription and feedback integration.\n1:\nInput:\ncommands/actions channel\nğ’\n\\mathcal{C}\non the message bus\n2:\nloop\n3:\nc\nâ†\nSubscribeAndReceive\nâ€‹\n(\nğ’\n)\nc\\leftarrow\\textsc{SubscribeAndReceive}(\\mathcal{C})\n4:\nRecord\nâ€‹\n(\nc\n)\n\\textsc{Record}(c)\n5:\nr\nâ†\nExecute\nâ€‹\n(\nc\n)\nr\\leftarrow\\textsc{Execute}(c)\nâŠ³\n\\triangleright\nr\nr\ncontains status/outcome\n6:\ne\nâ†\nPackEvent\nâ€‹\n(\nr\n)\ne\\leftarrow\\textsc{PackEvent}(r)\n7:\nPublish\nâ€‹\n(\ne\n)\n\\textsc{Publish}(e)\nâŠ³\n\\triangleright\npublish to status/events channel\n8:\nRecord\nâ€‹\n(\ne\n)\n\\textsc{Record}(e)\n9:\nUpdateRecords\nâ€‹\n(\ne\n)\n\\textsc{UpdateRecords}(e)\nâŠ³\n\\triangleright\nmaterialize feedback into Canonical Records\n10:\nend\nloop\nWith execution feedback materialized in\ncanonical records\n, subsequent inference steps observe updated context and issue new actions, completing the closed loop.\n4.4\nMessage Bus Latency Characterization\nWe characterize message-delivery latency under a single-publisher/single-subscriber setup.\nWe vary payload size and publish rate and summarize delivery latency in a distributional form using ECDFs and percentiles.\nTable\n2\nreports P50, P90, and P99 for four representative (size, rate) configurations, and Fig.\n4\nshows the corresponding ECDF curves.\nOverall, increasing payload size or publish rate shifts the latency distribution to the right; meanwhile, within the tested load range, P99 remains at the millisecond scale and we do not observe a pronounced runaway long tail.\nTable 2\n:\nMessage-bus delivery latency percentiles under a 1\nÃ—\n\\times\n1 setup. Latencies are in\nÎ¼\n\\mu\ns.\nPayload (B)\nRate (msg/s)\nP50\nP90\nP99\n128\n1000\n627.5\n1333.5\n1487.0\n128\n5000\n1322.5\n1746.0\n1840.0\n1024\n1000\n1363.0\n1828.0\n1941.0\n1024\n5000\n1901.0\n2392.2\n2781.1\n4.5\nRecovery Under Service Restarts\nWe evaluate recoverability via a controlled crash-and-restart experiment under a hard-crash setting.\nWe use a single-publisher/single-subscriber setup: the sender continuously publishes fixed-payload messages at a constant rate, and the receiver measures delivered throughput using fixed time bins.\nThe experiment begins in a steady regime where throughput stays close to the target rate.\nWe then inject a fault on the service node by force-killing the process and removing the shared-memory segment to emulate a cold restart; delivered throughput immediately drops to zero, indicating a disrupted stream.\nAfter an intentionally configured downtime window, we restart the service node.\nAs the service becomes available again, clients trigger reconnection and rebuild the communication path, and delivered throughput returns to the steady regime with a brief transient around the recovery point.\nFig.\n5\nsummarizes the full trace.\nThe result indicates that, even under a worst-case hard crash with shared-memory loss in our testbed, the system can automatically resume delivery after the service comes back online without manual intervention.\nFigure 5\n:\nDelivered throughput over time under a controlled hard crash and restart.\n5\nMechanism Analysis\nThis section analyzes the mechanisms behind the latency distributions and recovery behavior observed in Sec.\n4\n.\nWe focus on two critical paths: (i) the message-bus delivery path, which explains how latency distributions shift as payload size and publish rate increase, and\n(ii) the connection-management path, which explains how the system resumes delivery after a service crash and restart.\nOur goal is to provide a concise, reproducible analysis grounded in observable runtime behaviors, without introducing new comparative baselines.\nFor the message-bus delivery path, end-to-end delivery latency can be decomposed into three stages:\npublisher-side preparation and enqueueing, service-side routing and buffer handling, and subscriber-side dequeueing and dispatch.\nIncreasing payload size raises per-message costs (e.g., payload handling and buffer management), while increasing publish rate increases queue occupancy and contention.\nTogether, these effects produce the distribution-wide right shift observed in Sec.\n4.4\nand the consistent increase of P50/P90/P99 across configurations.\nThe ECDF curves in Sec.\n4.4\nfurther suggest that higher load primarily introduces additional processing and waiting time rather than sporadic extreme outliers.\nWhen publish rate increases, the system spends a larger fraction of time in non-empty-queue regimes, and percentile growth reflects accumulated queueing delay.\nWhen payload size increases, both the service and clients incur higher per-message handling costs, shifting the entire distribution.\nWithin the tested regime, this behavior is consistent with\ncontrolled degradation\nunder load rather than a qualitative runaway long tail.\nRecovery, in contrast, is governed by control flow rather than steady-state queueing.\nFig.\n5\nshows a sharp transition from steady delivery to zero throughput after fault injection, followed by resumption after service restart.\nThis trace indicates that the system can rebuild connectivity and re-establish subscriptions automatically after a hard crash with shared-memory loss, returning to a steady delivery regime without manual intervention (Sec.\n4.5\n).\nMechanistically, the recovery procedure can be abstracted into three stages.\nFirst, a\nfault-detection\nstage triggers recovery upon error signals (e.g., connection errors, prolonged silence, or heartbeat timeouts).\nSecond, a\ncleanup-and-reconnect\nstage closes stale connections, cancels pending operations, and applies a delayed reconnect window to avoid flapping under unstable conditions.\nThird, a\nre-registration\nstage re-advertises client identity and subscription information after connectivity is re-established so that routing state can be reconstructed and delivery can resume.\nThis staged view explains the end-to-end resumption pattern in Fig.\n5\n.\nIn summary, Sec.\n4\nexhibits two complementary runtime properties:\n(i) under increasing payload and publish rate, delivery latency grows in a controlled, distribution-wide manner consistent with load-induced processing and queueing costs; and\n(ii) under a worst-case service crash with shared-memory loss, the system resumes steady delivery after restart through an automated recovery control flow.\nThese observations motivate the parameterization and optimization directions discussed next.\n6\nFuture Work\nANCHOR targets modularity and robustness at the system layer, but real-world deployments introduce additional requirements that remain outside the scope of this work.\nWe highlight two clusters of future directions: (i) security and privacy mechanisms that integrate with explicit shared state and inter-component communication, and\n(ii) low-latency and real-time support under deployment constraints.\n6.1\nSecurity and privacy for shared state and inter-component communication\nReal-world distributed, highly connected deployments turn shared context and message dissemination into key attack surfaces, requiring security and privacy to be treated as consistent system-level concerns across Canonical Records and the communication bus. This involves privacy-aware data handling for shared records with secure formats, encryption-at-rest and auditable access control for persisted and replayable scenarios\n[\ntanimu2025addressing\n,\ndurlik2024cybersecurity\n]\n; strengthened inter-component communication security via authenticated channels, least-privilege access control and blast-radius containment mechanisms\n[\ndurlik2024cybersecurity\n,\nhamad2023security\n]\n; integrated runtime intrusion detection, auditing and scalable verification for critical logic\n[\nhamad2023security\n]\n; as well as post-quantum cryptography evaluation for long-lived systems to address classical crypto limitations\n[\ndurlik2024cybersecurity\n]\n.\n6.2\nLow-latency and real-time support under deployment constraints\nThe rising time-sensitivity of embodied tasks makes minimizing end-to-end and tail latency a priority, with the core challenge of preserving security and robustness while meeting real-time requirements. Future work should explore integration with real-time OS and kernel mechanisms like\nPREEMPT-RT to boost scheduling determinism and reduce latency for time-critical loops\n[\nye2023ros2\n]\n; develop adaptive QoS strategies for publish-subscribe communication to balance reliability and responsiveness under variable workloads\n[\njalil2023performance\n]\n; optimize middleware and IPC via zero-copy transfer and enhanced serialization to cut coordination overhead\n[\nkwok2025hprm\n]\n; and leverage edge and in-network computing to move processing closer to data sources, reducing round-trip delays for fast closed-loop responses in distributed setups\n[\nchinta2024edge\n]\n.\n7\nConclusion\nIn this paper, we presented\nANCHOR\n, a modular framework for embodied AI systems that targets predictable runtime behavior under rapid iteration and deployment-time disturbances.\nANCHOR\norganizes cross-component interaction around explicit system-level interface contracts:\nCanonical Records provide a unified and evolvable representation of normalized observations and system context, while a publishâ€“subscribe messaging substrate enables decoupled coordination and many-to-many dissemination, supporting closed-loop execution and traceability without introducing tight coupling.\nWe empirically evaluated\nANCHOR\nusing an event-driven closed-loop workflow that instantiates the full sensing-to-execution loop:\nupstream preprocessing materializes normalized state into\ncanonical records\n, inference produces and publishes commands according to project-bound configurations,\nand execution components subscribe, act, and publish runtime status/events that are selectively materialized back into the shared context.\nThis evaluation demonstrates how explicit representation contracts and message-mediated interaction preserve clear component boundaries as modules evolve, enabling traceable operation and iterative debugging.\nWe further characterized message-delivery latency under increasing load.\nBy varying payload size and publish rate, we observed a smooth distribution-wide right shift in latency as load increases,\nwith tail percentiles remaining at the millisecond scale within the tested regime and without exhibiting runaway long tails.\nThese results are consistent with the intended system behavior: controlled degradation under overload rather than non-linear latency spikes.\nFinally, we evaluated recoverability under a controlled crash-and-restart setting.\nA hard crash causes delivered throughput to drop to zero, and delivery resumes after restart without manual intervention.\nThis behavior follows from the recovery control flow for fault detection, cleanup, reconnection, and re-registration, and remains effective even under worst-case conditions in our testbed such as shared-memory loss (Fig.\n5\n).\nLooking forward, we will prioritize two directions.\nFirst, strengthening safety and security mechanismsâ€”including access control, data isolation, and end-to-end transport protectionâ€”to reduce risk when operating closed-loop systems in open environments.\nSecond, improving end-to-end responsiveness and determinism by optimizing critical paths and developing parameterized QoS and real-time support to further reduce tail latency under complex workloads.\nWe believe that extending safety and low-latency properties on top of a long-lived modular framework such as\nANCHOR\nis a practical path toward scalable and robust embodied-AI deployment.\nReferences",
    "preview_text": "As Embodied AI systems move from research prototypes to real world deployments, they tend to evolve rapidly while remaining reliable under workload changes and partial failures. In practice, many deployments are only partially decoupled: middleware moves messages, but shared context and feedback semantics are implicit, causing interface drift, cross-module interference, and brittle recovery at scale. We present ANCHOR, a modular framework that makes decoupling and robustness explicit system-level primitives. ANCHOR separates (i) Canonical Records, an evolvable contract for the standardized shared state, from (ii) a communication bus for many-to-many dissemination and feedback-oriented coordination, forming an inspectable end-to-end loop. We validate closed-loop feasibility on a de-identified workflow instantiation, characterize latency distributions under varying payload sizes and publish rates, and demonstrate automatic stream resumption after hard crashes and restarts even with shared-memory loss. Overall, ANCHOR turns ad-hoc integration glue into explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems.\n\n[\nrole=First Author,\n]\n [\nrole=co-First Author,\n]\n [\n]\n[\n]\n[\n]\n [\n]\n [\n]\n[\nrole=Corresponding Author,\norcid=0000-0002-8556-3579\n]\n\\credit\nConceptualization, Methodology, Software, Writing - Original Draft\n\\cortext\n[cor1]Corresponding author\nEfficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework\nYixuan Deng\nTongrun Wu\nDonghao Wu\nZeyu Wei\nJiayuan Wang\nZhenglong Sun\nYuqing Tang\nXiaoqiang Ji\njixiaoqiang@cuhk.edu.cn\nSchool of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, 2001 Longxiang Boulevard, Shenzhen, China\nSchool of Artificial Intelligence, The Chinese University of Hong Kong, Shenzhen, 2001 Longxiang Boulevard, Shenzhen, China\nShenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, Chin",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "Embodied AI",
        "modular framework",
        "system-level shared state",
        "coordination",
        "robustness",
        "closed-loop systems"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºANCHORæ¡†æ¶ï¼Œé€šè¿‡æ ‡å‡†åŒ–å…±äº«çŠ¶æ€å’Œé€šä¿¡æ€»çº¿å®ç°å…·èº«AIç³»ç»Ÿçš„æ¨¡å—åŒ–ã€è§£è€¦å’Œé²æ£’åè°ƒï¼Œä½†æœªæ¶‰åŠå¼ºåŒ–å­¦ä¹ ã€æ‰©æ•£æ¨¡å‹æˆ–è¿åŠ¨æ§åˆ¶ç­‰å…³é”®è¯ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T13:21:52Z",
    "created_at": "2026-01-27T15:53:10.718382",
    "updated_at": "2026-01-27T15:53:10.718388",
    "recommend": 0
}