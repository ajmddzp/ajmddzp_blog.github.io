{
    "id": "2601.22467v1",
    "title": "CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control",
    "authors": [
        "Jiaqi Shi",
        "Xulong Zhang",
        "Xiaoyang Qu",
        "Jianzong Wang"
    ],
    "abstract": "视觉-语言-动作（VLA）模型在机器人控制领域的最新进展展现出巨大潜力，但其对动作监督的依赖限制了可扩展性与泛化能力。为解决这一挑战，我们提出了CARE框架，该框架专为训练机器人任务执行的VLA模型而设计。与现有方法在预训练阶段依赖动作标注不同，CARE仅通过视频-文本对进行训练，无需显式动作标签。这些弱对齐的数据源使模型能够通过新设计的多任务预训练目标，学习连续的潜在动作表示。在微调阶段，仅需少量标注数据即可训练动作控制头。在多种仿真任务中的实验结果表明，CARE在任务成功率、语义可解释性及避免捷径学习方面均表现优异。这些发现印证了CARE在弱监督机器人控制中具备的可扩展性、可解释性与高效性。",
    "url": "https://arxiv.org/abs/2601.22467v1",
    "html_url": "https://arxiv.org/html/2601.22467v1",
    "html_content": "CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control\nAbstract\nRecent advances in Vision-Language-Action (VLA) models have shown promise for robot control, but their dependence on action supervision limits scalability and generalization. To address this challenge, we introduce CARE, a novel framework designed to train VLA models for robotic task execution. Unlike existing methods that depend on action annotations during pretraining, CARE eliminates the need for explicit action labels by leveraging only video-text pairs. These weakly aligned data sources enable the model to learn continuous latent action representations through a newly designed multi-task pretraining objective. During fine-tuning, a small set of labeled data is used to train the action head for control. Experimental results across various simulation tasks demonstrate CARE’s superior success rate, semantic interpretability, and ability to avoid shortcut learning. These results underscore CARE’s scalability, interpretability, and effectiveness in robotic control with weak supervision.\nIndex Terms\n—\nVisual-Language-Action (VLA), Multimodal Large Language Models, Embodied AI, Robotics\n1\nIntroduction\nFig. 1\n:\nOverview of CARE.\nCompared to (a) the VLA training approach that requires a large number of action labels, (b) our proposed multitask learning-based pre-training method, CARE, uses only instruction-image pairs as input. Without relying on action labels, it still enables the VLM to predict potential actions.\nRecent advancements in Visual-Language-Action (VLA) models show significant potential in robotic control by integrating visual perception, natural language understanding, and motion control to perform tasks like grasping, manipulation, and tool use\n[\nbrohan2022rt\n,\ndriess2023palm\n,\nbjorck2025gr00t\n,\nnav\n,\njiaziqi\n,\njiaziqi2\n,\naaai\n,\npointaction\n]\n. While these models effectively bridge the gap between high-level language instructions and low-level execution, their development is heavily constrained by the costly and time-consuming nature of large-scale action supervision during pretraining\n[\no2024open\n]\n. Obtaining precise action annotations, such as joint angles or end-effector trajectories, is often impractical, especially when scaling to diverse robotic morphologies or transferring datasets across domains.\nA promising direction for scalable robotic development involves leveraging abundant, action-unlabeled video data from diverse sources. The Genie model\n[\nbruce2024genie\n]\nintroduced a scalable Latent Action Model (LAM) that enables generative models to create interactive environments. Using an encoder-decoder architecture with a small discrete codebook, Genie extracts discrete latent actions, providing a foundation for subsequent work. LAPA\n[\nye2024latent\n]\nfirst learns discrete latent actions between image frames using a Vector Quantized Variational Autoencoder (VQ-VAE) objective\n[\nvan2017neural\n]\n. However, the pixel-level reconstruction objective in LAM often leads models to focus on static image features, such as background textures or color distributions, rather than action-driven changes\n[\nmccarthy2024towards\n]\n. The Moto model\n[\nchen2412moto\n]\naddressed this by shifting the training objective from next-frame reconstruction to predicting high-level semantic embeddings using a Vision Transformer (ViT) encoder. Nevertheless, these approaches represent latent actions as discrete codebooks. The fixed size of discrete codebooks limits representation capacity, potentially failing to capture fine-grained variations in continuous action spaces. To address this, the COMO model\n[\nyang2025learning\n]\nproposed continuous latent action representations, replacing the codebook with a cross-attention mechanism to capture more granular latent action representations.\nDespite these advancements, three challenges remain in latent action model research. First, bias propagation persists. Quantization errors and codebook collapse in VQ-VAE-based LAMs are fully inherited by VLA models, which cannot correct these errors. Second, latent action representations lack explicit action encoding.\n[\nmccarthy2024towards\n]\nnoted that, as the training objective focuses on next-frame reconstruction, inferred latent actions merely compress differences between consecutive frames, even when control actions are not the cause of these differences. Third, models risk ”shortcut learning”. The single training objective and LAM structure can lead models to degenerate into future frame predictors rather than latent action modelers\n[\nschmidt2023learning\n]\n.\nTo address these issues, we propose a novel pretraining strategy named CARE. We integrate LAM-training into the pre-training process of Vision Language Models (VLMs). Specifically, we replace the LAM encoder with a VLM and add multiple decoders for different tasks, forming an encoder-multi-decoder architecture. To address the second and third issues, we employ multi-task learning to enhance the explicit representation of latent actions. By guiding the VLM to generate latent actions through multi-task training and validating these representations with multiple decoders, we improve action encoding. Unlike prior work relying on next-frame reconstruction, we introduce keypoint trajectory prediction as a training objective. This allows the model to reconstruct the next frame while predicting keypoint trajectory changes, emphasizing action-driven positional changes. Experiments demonstrate that this multi-task learning approach enhances explicit action representation and prevents ”shortcut learning”. Our contributions are as follows:\n•\nWe propose CARE, an unsupervised pretraining strategy that seamlessly integrates LAM training into the VLM pretraining pipeline, reducing the overall VLA training process from four stages to three.\n•\nWe introduce a multi-task learning approach with keypoint trajectory prediction to enrich latent action representations, thereby improving explicit action encoding and mitigating shortcut learning.\n•\nWe demonstrate through extensive simulations that our pretraining strategy provides significant advantages over prior label-free approaches while remaining competitive with methods that rely on action labels.\n2\nMETHODOLOGY\nFig. 2\n:\nDatasets of pre-training stage and post-training stage.\nThis section introduces our CARE pretraining framework for learning a latent vision-language model without action labels (see Fig.\n1\n). The training process comprises two stages: (1) unsupervised pretraining of the latent VLM, and (2) supervised fine-tuning with a small-scale robot dataset.\n2.1\nLatent Vision-Language Model\nOur Latent Vision-Language Model (VLM) architecture is built upon the Prismatic-7B VLM\n[\nkaramcheti2024prismatic\n]\n, with a core structure consisting of a visual encoder, a projector, and a 7B-parameter Llama Large Language Model (LLM) backbone. In the pretraining stage, we fine-tune this backbone to learn latent continuous action representations.\nWe define the latent action prediction problem as a “vision-language” task, where the input consists of an observation image and a natural language task instruction. Formally, a VLM takes as input an image\nx\ni\n​\nm\n​\ng\nx_{img}\nand a prompt\np\np\n. The model receives latent-action placeholders appended to\np\np\nso it can predict the action dimensions. Then, we put\np\np\nto the tokenizer where the llm backbone uses to get the prompt tokens\nx\ni\n​\nd\n​\ns\nx_{ids}\n. The text embedding encoder takes\nx\ni\n​\nd\n​\ns\nx_{ids}\nas input and outputs text prompt embeddings\nf\nT\nf_{T}\n.\nThe visual encoder is composed of two pretrained models, SigLIP\n[\nzhai2023sigmoid\n]\nand DinoV2\n[\noquab2023dinov2\n]\n. The visual encoder processes\nx\ni\n​\nm\n​\ng\nx_{img}\nto produce two feature sequences\nf\ns\n​\ni\n​\ng\n∈\nℝ\nN\np\n×\nD\nv\nf_{sig}\\in\\mathbb{R}^{N_{p}\\times D_{v}}\nand\nf\nd\n​\ni\n​\nn\n​\no\n∈\nℝ\nN\np\n×\nD\nv\nf_{dino}\\in\\mathbb{R}^{N_{p}\\times D_{v}}\n. These features are concatenated into\nf\nc\n​\na\n​\nt\n∈\nℝ\nN\np\n×\n2\n​\nD\nv\nf_{cat}\\in\\mathbb{R}^{N_{p}\\times 2D_{v}}\nand then projected by a two-layer MLP\nP\n​\nr\n​\no\n​\nj\nζ\n​\n(\n⋅\n)\n∈\nℝ\n2\n​\nD\nv\n→\nD\nl\nProj_{\\zeta}(\\cdot)\\in\\mathbb{R}^{2D_{v}\\to D_{l}}\nto obtain the visual features\nf\nv\nf_{v}\n. Finally, we concatenate the text features\nf\nT\nf_{T}\nand\nf\nv\nf_{v}\nto form the LLM’s full input. The final hidden states\nh\nl\n​\na\n​\ns\n​\nt\n=\nL\n​\nM\nθ\n​\n(\n[\nf\nT\n;\nf\nv\n]\n)\nh_{last}=LM_{\\theta}([f_{T};f_{v}])\nare generated, and the part corresponding to the action dimensions is used as the continuous latent action representation\nz\nz\nfor multi-task learning.\n2.2\nContinuous Latent Action with Multi-task Learning\nMulti-task learning involves the problem of optimizing a model with respect to multiple objectives. In our work, we apply multitask learning to generate latent continuous action representations.\nMulti-Task Definition.\nWe define two training objectives. The first training objective is similar to the one used in previous work for training LAM (\n[\nye2024latent\n,\nbruce2024genie\n,\nchen2412moto\n]\n), which involves inputting the vision feature\nf\nv\nt\n∈\nℝ\nB\n×\nN\np\n×\nD\nl\nf_{v}^{t}\\in\\mathbb{R}^{B\\times N_{p}\\times D_{l}}\nof the current frame and the latent action representation\nz\nz\nto predict the vision feature\nf\nv\nt\n+\n1\nf_{v}^{t+1}\nof the next frame.\nHowever, the single task of predicting the next frame, as discussed earlier, has certain issues. Therefore, we introduce a new training objective - point track prediction. In the point track prediction task, the input\nk\nt\n∈\nℝ\nB\n×\n256\n×\n2\nk_{t}\\in\\mathbb{R}^{B\\times 256\\times 2}\nconsists of the two-dimensional coordinates(\nx\nx\nand\ny\ny\n) of evenly distributed points in the current frame, with a shape of\n256\n×\n2\n256\\times 2\n. The two-dimensional coordinates of the current frame points\nk\nt\nk_{t}\nand the latent action representation\nz\nt\nz_{t}\nas inputs to predict the two-dimensional coordinates of the points in the next frame\nk\nt\n+\n1\nk_{t+1}\n. Specially, we use Co-Tracker\n[\nkaraev23cotracker\n]\n, a pre-trained model, to acquire\nk\nt\nk_{t}\nand\nk\nt\n+\n1\nk_{t+1}\n.\nFor the frame prediction task, we use the cross-attention mechanism to compute the relationship\nz\nf\nz_{f}\nbetween current frame feature and latent action representation as\nz\nf\n=\ns\n​\no\n​\nf\n​\nt\n​\nm\n​\na\n​\nx\n​\n(\nQ\n​\nK\nT\nd\n)\n​\nV\nz_{f}=softmax(\\frac{QK^{T}}{\\sqrt{d}})V\n, where\nQ\n=\nz\nt\n​\nW\nz\n+\nb\nz\nQ=z_{t}W_{z}+b_{z}\nrepresents the query projection, and\nK\n,\nV\n=\nf\nv\nt\n​\nW\nf\n+\nb\nf\nK,V=f_{v}^{t}W_{f}+b_{f}\nrepresent the key and value projections, respectively. Then, we input\nz\nf\nz_{f}\ninto the frame decoder to predict the feature vector\nf\nv\nt\n+\n1\nf^{t+1}_{v}\nof the next frame. For the point track prediction task, we similarly first fuse\nk\nt\nk_{t}\nand\nz\nt\nz_{t}\nusing cross-attention to obtain\nz\nk\nz_{k}\n. Then, we input\nz\nk\nz_{k}\ninto the point decoder to predict the point coordinates\nk\nt\n+\n1\nk_{t+1}\nof the next frame.\nLoss Function Definition.\nGiven a video-text pair dataset\n𝒟\n=\n{\n(\nx\nt\n,\np\nt\n)\nt\n=\n1\nT\ni\n}\ni\n=\n1\n𝒩\n\\mathcal{D}=\\{(x_{t},p_{t})_{t=1}^{T_{i}}\\}_{i=1}^{\\mathcal{N}}\n, where\n𝒩\n\\mathcal{N}\nrepresents the number of demonstrations. We aim to pretrain a latent policy\nπ\nϕ\n:\n𝒳\n×\n𝒫\n→\n𝒵\n\\pi_{\\phi}:\\mathcal{X}\\times\\mathcal{P}\\to\\mathcal{Z}\nwith multi-task learning. The loss of frame prediction task could be described as\nℒ\nf\nt\n=\nM\n​\nS\n​\nE\n​\n(\nf\nv\nt\n+\n1\n,\nf\n^\nv\nt\n+\n1\n)\n\\mathcal{L}_{f}^{t}=MSE(f_{v}^{t+1},\\hat{f}_{v}^{t+1})\n.\nThe loss of point track prediction could be described as\nℒ\np\nt\n=\nM\n​\nS\n​\nE\n​\n(\nk\nt\n+\n1\n,\nk\n^\nt\n+\n1\n)\n\\mathcal{L}_{p}^{t}=MSE(k^{t+1},\\hat{k}^{t+1})\n.\nWe adopt the Uncertainty-Weighted Loss (UWL) framework of\n[\nkendall2018multi\n]\nto combine the objectives in our multi-task training. This approach is derived by maximizing a Gaussian likelihood under the assumption of task-independent uncertainty. For Latent VLM, the model produces two predictions:\nk\n^\nt\n+\n1\n\\hat{k}^{t+1}\nand\nf\n^\nt\n+\n1\n\\hat{f}^{t+1}\n. We treat\nk\n^\nt\n+\n1\n\\hat{k}^{t+1}\nwith a Gaussian likelihood, while\nf\n^\nt\n+\n1\n\\hat{f}^{t+1}\nis handled via a softmax-based likelihood.\nThe joint loss\nℒ\n\\mathcal{L}\nis defined as below:\nℒ\nt\n=\n1\nσ\n1\n2\n​\nℒ\nf\nt\n+\n1\nσ\n1\n2\n​\nL\np\nt\n+\nl\n​\no\n​\ng\n​\nσ\n1\n+\nl\n​\no\n​\ng\n​\nσ\n2\n,\n\\mathcal{L}^{t}=\\frac{1}{\\sigma_{1}^{2}}\\mathcal{L}_{f}^{t}+\\frac{1}{\\sigma_{1}^{2}}L_{p}^{t}+log\\sigma_{1}+log\\sigma_{2},\nwhere\nσ\n1\n\\sigma_{1}\nand\nσ\n2\n\\sigma_{2}\nas learning the relative weight of the losses\nℒ\nf\n\\mathcal{L}_{f}\nand\nℒ\np\n\\mathcal{L}_{p}\nadaptively.\n2.3\nFine-tuning VLA with action labels\nLatent VLM that are pretrained to predict latent actions are not directly excutable on robots. Because latent actions are not actual end-effector actions or joint actions. To map latent actions to actual robot actions, we need to fine-tune latent VLM with an action head on a set of robot demonstrations datasets that contain ground truth labeled actions.\nWe implement the action prediction module as a lightweight residual MLP attached to the decoder output. It takes the hidden representation from the last decoder layer as input and regresses real-valued actions, following the design pattern in\n[\nkim2025fine\n]\n. Training is carried out by optimizing an L1 regression objective between predicted and ground-truth actions. In this stage , we adapting our model via LoRA fine-tuning\n[\nhu2022lora\n]\n.\n3\nEXPERIMENT\nTable 1\n:\nResults on LIBERO benchmark across four evaluation\nsuites.\n✓Methods do not utilize action labels during the pre-training stage. We reproduced results of LAPA and CoMo using the Prismatic-7B VLM and the same datasets on human, Bridge and RT-1. In the original CoMo paper, it is implemented based on the Diffusion Policy.\n†\n\\dagger\nMethods use additional wrist-view camera inputs. ”w/o” indicates that no action labels are used during pre-training. Both methods for latent action are fine-tuned on RT-1 using action labels.\nMethods\nw/o action label\nGoal SR(\n↑\n\\uparrow\n))\nSpatial SR(\n↑\n\\uparrow\n)\nObject SR(\n↑\n\\uparrow\n))\nLong SR(\n↑\n\\uparrow\n))\nAvg.\nOpenVLA\n[\nkim2024openvla\n]\n✗\n76.5\n±\n\\pm\n1.0%\n82.6\n±\n\\pm\n0.9%\n88.2\n±\n\\pm\n0.8%\n52.8\n±\n\\pm\n1.3%\n75.0\nOcto\n[\nteam2024octo\n]\n✗\n82.9\n±\n\\pm\n0.9%\n76.1\n±\n\\pm\n1.0%\n84.3\n±\n\\pm\n0.9%\n51.6\n±\n\\pm\n1.8%\n73.7\nDiffusion Policy\n[\nchi2023diffusion\n]\n✗\n68.3\n±\n\\pm\n1.2%\n78.3\n±\n\\pm\n1.1%\n92.5\n±\n\\pm\n0.7%\n50.5\n±\n\\pm\n2.4%\n72.4\nMDT\n†\n[\nreuss2024multimodal\n]\n✗\n71.2\n±\n\\pm\n0.8%\n77.3\n±\n\\pm\n1.0%\n88.1\n±\n\\pm\n1.2%\n62.6\n±\n\\pm\n1.4%\n74.8\nLAPA\n[\nye2024latent\n]\n✓\n57.1\n±\n\\pm\n2.4%\n74.3\n±\n\\pm\n3.1%\n72.2\n±\n\\pm\n1.6%\n53.6\n±\n\\pm\n3.4%\n64.3\nCoMo\n[\nyang2025learning\n]\n✓\n63.2\n±\n\\pm\n1.2%\n76. 0\n±\n\\pm\n0.8%\n83.1\n±\n\\pm\n1.4%\n54.6\n±\n\\pm\n1.8%\n69.2\nOurs\n(human only)\n✓\n64.3\n±\n\\pm\n2.1%\n70.2\n±\n\\pm\n1.9%\n69.5\n±\n\\pm\n2.4%\n49.2\n±\n\\pm\n3.5%\n63.3\nOurs\n(Bridge only)\n✓\n67.5\n±\n\\pm\n1.7%\n75. 0\n±\n\\pm\n1.5%\n72.2\n±\n\\pm\n1.9%\n58.5\n±\n\\pm\n2.6%\n68.3\nOurs\n(Bridge+RT-1)\n✓\n72. 0\n±\n\\pm\n1.6%\n80.5\n±\n\\pm\n2. 0%\n77.6\n±\n\\pm\n1.8%\n63.8\n±\n\\pm\n2. 0%\n73.5\nOurs (human+Bridge+RT-1)\n✓\n77.9\n±\n\\pm\n1.4%\n81.2\n±\n\\pm\n1.6%\n86.4\n±\n\\pm\n1.2%\n65.3\n±\n\\pm\n1.9%\n77.7\nTable 2\n:\nThe ablation experiments results on the LIBERO benchmark.\nMethods\nGoal\nSpatial\nObject\nLong\nLP-MSE(\n↓\n\\downarrow\n)\nS-PCFC(\n↓\n\\downarrow\n)\nLP-MSE(\n↓\n\\downarrow\n)\nS-PCFC(\n↓\n\\downarrow\n)\nLP-MSE(\n↓\n\\downarrow\n)\nS-PCFC(\n↓\n\\downarrow\n)\nLP-MSE(\n↓\n\\downarrow\n)\nS-PCFC(\n↓\n\\downarrow\n)\nCoMo\n0.839\n0.899\n0.881\n0.892\n0.662\n0.902\n0.754\n0.910\nLAPA\n1.241\n0.980\n0.924\n0.992\n1.136\n0.942\n0.741\n0.945\nOurs\n0.647\n0.833\n0.717\n0.945\n0.690\n0.860\n0.643\n0.820\nIn this section, we demonstrate the effectiveness of CARE as a latent action pretraining framework. Specifically, we focus on the following questions:\nQ1\n(Performance): Can the CARE VLM trained without labels efficiently learn real actions during the fine-tuning stage to improve policy performance?\nQ2\n(Interpretability): Do the continuous latent actions obtained by CARE have better action representation capabilities compared to latent actions obtained by other methods?\nQ3\n(Shortcut Learning): Does our method effectively prevent shortcut learning?\n3.1\nSetups\nDatasets.\nAs shown in Fig.\n2\n, we use two types of video datasets: robot and human manipulation videos. For robot data, we use 140k trajectories from the Open X-Embodiment\n[\no2024open\n]\n. For human videos, we use approximately 100k clips of daily activities from Something-Something v2\n[\ngoyal2017something\n]\n. In total, our pretraining dataset comprises around 240k trajectories and video clips. For the fine-tuning stage, we use a 3% uniform sample from the RT-1 dataset\n[\nbrohan2022rt\n]\n, which contains action labels.\nBenchmark and Baselines.\nWe used LIBERO\n[\nliu2023libero\n]\nas our benchmark. Our experiments were performed on four task suites: Spatial, Object, Goal, and Long. Each containing 10 tasks with 50 human-teleoperated demonstrations. We followed the OpenVLA\n[\nkim2024openvla\n]\ndata processing pipeline for all datasets. Our baselines include OpenVLA\n[\nkim2024openvla\n]\n, Octo\n[\nteam2024octo\n]\n, DP\n[\nchi2023diffusion\n]\n, MDT\n[\nreuss2024multimodal\n]\n, LAPA\n[\nye2024latent\n]\n, and CoMo\n[\nyang2025learning\n]\n.\nEvaluation Metrics.\nTo address Q1, we evaluate baseline models and CARE in the LIBERO simulator using task success rate, where higher is better.\nFor Q2, we conduct two experiments: (1) linear probing, following CoMo, where latent actions from CARE and CoMo are fed into an MLP to predict ground-truth actions, evaluated by LP-MSE (lower is better); (2) semantic label prediction on LAPA, CoMo, and CARE, evaluated by Semantic Accuracy (higher is better).\nFor Q3, we adopt the S-PCFC metric\n[\nyang2025learning\n]\n, where lower values indicate weaker shortcut learning.\n3.2\nMain Results\nThe results presented in Tab.\n2\ndemonstrate that Our method CARE achieves higher success rates compared to other pre-training approaches without action labels, while remaining competitive with methods that utilize action labels for pre-training. Specifically, our method outperforms the action label-based pre-trained OpenVLA model by 1.4% and 12.5% in the Goal and Long tests, respectively. Moreover, compared to the label-free pre-training approaches of LAPA and CoMo based on autoregressive models, our method achieves superior performance across all four tasks. While slightly inferior to the diffusion policy-based CoMo approach in the Goal and Object tasks, our method demonstrates higher success rates in the Spatial and Long tasks. Furthermore, we observe that the success rate of the final experimental results improves as the pre-training data scales, validating that our method follows the scaling law.\n3.3\nAblation Study on Interpretability\nTo validate whether our pre-training method yields more interpretable latent action representations, we conducted linear probing experiments and semantic label prediction experiments, with results shown in Tab.\n2\nand Tab.\n3\nrespectively. The results demonstrate that our method achieves lower LP-MSE scores than both CoMo and LAPA across Goal, Spatial, and Long tasks. Specifically:For the Goal task: 22.8% reduction compared to CoMo and 47.8% reduction compared to LAPA. For the Spatial task: 18.6% reduction compared to CoMo and 22.4% reduction compared to LAPA. For the Long task: 14.7% reduction compared to CoMo and 13.2% reduction compared to LAPA. As shown in Tab.\n3\n, the semantic label predictor trained solely on latent actions from the initial frame and subsequent 9 frames achieves 84.2% accuracy on LIBERO Goal. This performance is comparable to that using 10 consecutive frames. These two experiments demonstrate that the latent actions obtained by our method exhibit strong semantic interpretability.\nTable 3\n:\nSemantic label prediction.\nInput\nSemantic Acc.(\n↑\n\\uparrow\n)\nInitial frame\n0.310\nInitial frame repeated by 10 times\n0.326\nInitial frame + 9 subsequent frames\n0.804\nInitial frame + 9 LAPA latent actions\n0.641\nInitial frame + 9 CoMo latent actions\n0.712\nInitial frame + 9 Ours latent actions\n0.842\n3.4\nAblation Study on Shortcut Learning\nRegarding S-PCFC, the results in Tab.\n2\ndemonstrate that our method’s latent actions effectively circumvent shortcut learning. Specifically, LAPA achieves near-perfect scores (0.992) across different tasks, indicating that VQ-VAE-based methods for extracting discrete latent actions remain susceptible to shortcut learning. The Q-former approach CoMo for extracting continuous latent actions partially mitigates this issue (0.892). Our method fundamentally addresses this through pretraining modification by leveraging VLM-based latent action extraction, it significantly avoids shortcut learning (0.833).\n3.5\nAblation Study on Multi-task Learning\nTo evaluate multi-task pre-training against single-task pre-training, we conducted ablation experiments with varied decoders while maintaining identical settings: multi-task, next-frame prediction, and keypoint trajectory prediction. As shown in Tab.\n4\n, the multi-task Prismatic-7B model achieves a success rate (SR) of 77.7% with a latent loss of 0.046, outperforming single-task models (67.8% SR for frame prediction with 0.012 loss, and 54.3% SR for point tracking with 0.009 loss). The lower SR of keypoint tracking supports prior findings favoring frame prediction, and these results confirm the synergistic benefits (1+1\n>\n>\n2) of multi-task pre-training.\nTable 4\n:\nTraining details for different pre-training tasks.\nSIMO denotes single-input multi-output, and latent loss refers to the task loss value at convergence during the experiment.\nModel\nObjective\nSIMO\nlatent loss\nSR\nPrismatic-7B\nframe predict\n✗\n0.012\n67.8\npoint track\n✗\n0.009\n54.3\nmulti-task\n✓\n0.046\n77.7\n4\nConclusion\nIn this work, we introduce CARE, a multi-task pre-training approach that learns continuous latent action representations. By leveraging two decoders, we jointly pre-train a latent VLM. Compared to prior methods that rely on a separate latent action model, CARE achieves superior performance in simulations and produces latent actions that more effectively encode motions. Although a performance gap still exists compared to action-label-based pre-training, we plan to narrow this gap by incorporating action chunking, and multi-dimensional perceptual perspectives in future work.\nReferences",
    "preview_text": "Recent advances in Vision-Language-Action (VLA) models have shown promise for robot control, but their dependence on action supervision limits scalability and generalization. To address this challenge, we introduce CARE, a novel framework designed to train VLA models for robotic task execution. Unlike existing methods that depend on action annotations during pretraining, CARE eliminates the need for explicit action labels by leveraging only video-text pairs. These weakly aligned data sources enable the model to learn continuous latent action representations through a newly designed multi-task pretraining objective. During fine-tuning, a small set of labeled data is used to train the action head for control. Experimental results across various simulation tasks demonstrate CARE's superior success rate, semantic interpretability, and ability to avoid shortcut learning. These results underscore CARE's scalability, interpretability, and effectiveness in robotic control with weak supervision.\n\nCARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control\nAbstract\nRecent advances in Vision-Language-Action (VLA) models have shown promise for robot control, but their dependence on action supervision limits scalability and generalization. To address this challenge, we introduce CARE, a novel framework designed to train VLA models for robotic task execution. Unlike existing methods that depend on action annotations during pretraining, CARE eliminates the need for explicit action labels by leveraging only video-text pairs. These weakly aligned data sources enable the model to learn continuous latent action representations through a newly designed multi-task pretraining objective. During fine-tuning, a small set of labeled data is used to train the action head for control. Experimental results across various simulation tasks demonstrate CARE’s superior success rate, semantic interpretability, and ability to avoid shortcut learning. These results under",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "VLA",
        "Vision-Language-Action Model",
        "fine tune",
        "offline Reinforcement Learning",
        "world model"
    ],
    "one_line_summary": "CARE是一种通过视频-文本对进行多任务预训练，学习连续潜在动作表示的VLA框架，减少对动作监督的依赖，提升机器人控制的泛化能力。",
    "detailed_summary": "## 论文摘要：CARE: 用于机器人控制中潜在连续动作表示的多任务预训练\n\n**1. 研究背景和动机**\n近年来，视觉-语言-动作（VLA）模型在机器人控制领域展现出巨大潜力。然而，现有VLA模型在预训练阶段严重依赖大量精确的动作标注（如关节角度），这极大地限制了其可扩展性和泛化能力。为了克服这一瓶颈，本文提出CARE框架，旨在利用海量无动作标注的视频-文本对数据进行预训练，从而学习到更具语义性和泛化能力的潜在连续动作表示。\n\n**2. 核心方法和技术创新**\nCARE的核心创新在于提出了一种**无需动作标注的多任务预训练策略**。其技术要点包括：\n- **架构设计**：采用“编码器-多解码器”架构，将潜在动作模型（LAM）的训练无缝集成到视觉语言模型（VLM）的预训练流程中，简化了整体训练流程。\n- **多任务学习**：设计了两个并行的预训练目标：1）**下一帧特征预测**（传统LAM目标）；2）**关键点轨迹预测**。后者强制模型关注由动作驱动的、物体在空间中的位置变化，从而增强潜在动作表示对真实运动的编码能力。\n- **损失函数**：采用不确定性加权损失（UWL）自适应地平衡两个任务的损失，优化训练过程。\n- **微调阶段**：预训练完成后，仅需使用少量带动作标签的机器人数据，通过一个轻量级的动作头进行微调，即可将学到的潜在动作映射到真实的机器人控制指令。\n\n**3. 主要实验结果**\n在LIBERO仿真基准测试上的实验表明：\n- **性能优越**：CARE在无需动作标注的预训练方法中取得了最高的平均任务成功率（77.7%），甚至在某些任务上超越了部分依赖动作标注的方法。\n- **可解释性强**：线性探测实验显示，CARE学到的潜在动作能更准确地预测真实动作（LP-MSE更低）。语义标签预测准确率达84.2%，证明其潜在动作具有高度的语义信息。\n- **有效避免捷径学习**：CARE的S-PCFC指标显著低于基线方法，表明其成功避免了模型退化为简单的未来帧预测器，而是真正建模了动作。\n- **多任务有效性**：消融实验证实，结合帧预测与关键点预测的多任务学习策略，其效果显著优于任一单任务（成功率77.7% vs. 67.8% / 54.3%），实现了“1+1>2”的协同效应。\n\n**4. 研究意义和价值**\n本研究的价值在于：\n- **推动弱监督机器人学习**：CARE证明了仅利用弱对齐的视频-文本数据预训练高性能VLA模型的可行性，大幅降低了对昂贵动作标注的依赖，为机器人技术的规模化应用提供了新路径。\n- **提升模型可解释性与鲁棒性**：通过引入关键点轨迹预测任务，使学到的潜在动作表示更明确地编码运动语义，并有效缓解了捷径学习问题，增强了模型的可靠性和可解释性。\n- **提供高效训练范式**：所提出的多任务预训练框架将VLA训练流程从四阶段简化为三阶段，为后续研究提供了一个高效且性能优异的基线模型。",
    "qa_pairs": [
        {
            "question": "这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？",
            "answer": "## 论文核心分析：CARE\n\n### **核心问题**\n现有视觉-语言-动作（VLA）模型在机器人控制中严重依赖**大规模、精确标注的动作数据**（如关节角度、末端轨迹）进行预训练。这导致了：\n1.  **可扩展性差**：标注成本高昂，难以扩展到多样化的机器人形态或跨领域数据。\n2.  **泛化能力受限**：模型容易过拟合到特定的动作标注，而非学习通用的动作表示。\n3.  **潜在动作模型（LAM）的固有缺陷**：现有方法（如VQ-VAE）提取的离散潜在动作存在量化误差、代码本坍缩、易陷入“捷径学习”（如仅预测静态背景而非动作）等问题。\n\n### **核心创新点**\nCARE提出了一种**无需动作标注的、多任务预训练框架**，旨在从海量的视频-文本对中学习**连续的潜在动作表示**。其创新主要体现在三个方面：\n\n1.  **训练流程创新：无缝集成LAM与VLM预训练**\n    - **传统流程**：通常需要四个阶段（视觉编码器预训练、语言模型预训练、LAM训练、VLA微调）。\n    - **CARE流程**：将LAM训练**直接整合**到视觉语言模型（VLM）的预训练流程中，简化为三个阶段。它使用一个**编码器-多解码器**架构，其中VLM（如Prismatic-7B）作为编码器，直接输出潜在动作表示，省去了独立的LAM训练阶段。\n\n2.  **表示学习创新：多任务学习增强动作显式编码**\n    - **单一任务的不足**：仅使用“下一帧预测”作为目标，模型容易关注无关的像素变化（如光照、纹理），而非动作本身。\n    - **CARE的解决方案**：引入**双任务联合训练**：\n        - **任务一：下一帧特征预测**（继承自传统LAM）。\n        - **任务二：关键点轨迹预测**（**核心创新**）。使用预训练模型（Co-Tracker）获取视频帧中均匀分布点的轨迹，让模型预测这些点在动作影响下的位置变化。\n    - **作用**：关键点轨迹预测强制模型关注**由动作驱动的、结构化的位置变化**，从而学习到更具语义和动作解释性的潜在表示，有效缓解了“捷径学习”。\n\n3.  **架构与优化创新：连续表示与自适应损失**\n    - **连续潜在动作**：摒弃了离散的代码本（VQ-VAE），采用**交叉注意力机制**生成连续的潜在动作向量 `z`，能捕捉更细粒度的动作变化。\n    - **不确定性加权损失（UWL）**：自动学习并平衡两个预测任务（帧预测与点轨迹预测）的损失权重 (`σ₁, σ₂`)，使多任务训练更稳定高效。\n\n### **解决方案总览**\n1.  **预训练阶段（无动作标签）**：\n    - **输入**：海量视频-文本对（如人类日常活动视频、机器人演示视频）。\n    - **过程**：VLM编码当前图像和指令，输出潜在动作表示 `z`。`z` 同时输入两个解码器（帧解码器、点轨迹解码器）进行多任务预测。\n    - **目标**：学习一个能生成高质量连续潜在动作表示的“潜在策略” `π_φ`。\n\n2.  **微调阶段（少量动作标签）**：\n    - **输入**：小规模带动作标注的机器人数据集（如RT-1的3%数据）。\n    - **过程**：冻结预训练好的VLM主干，仅使用**LoRA**等技术微调一个轻量级的**动作头**（残差MLP）。该动作头将潜在动作 `z` 映射到真实的机器人控制指令（如关节角度）。\n    - **目标**：使模型具备实际控制能力。\n\n### **实际价值**\n- **降低数据依赖与成本**：仅需大量易得的无标注视频和少量标注数据，大幅提升了机器人技能学习的可扩展性。\n- **提升模型性能与泛化性**：在LIBERO基准测试上，CARE在**不使用动作标签预训练**的方法中达到最佳性能（平均成功率77.7%），甚至在某些任务上超越了**使用动作标签预训练**的基线模型（如OpenVLA）。\n- **增强可解释性与鲁棒性**：\n    - 学到的潜在动作具有更好的**语义可解释性**（线性探针MSE更低，语义标签预测准确率高达84.2%）。\n    - 有效**避免了捷径学习**（S-PCFC指标显著低于基线方法）。\n\n**总结**：CARE的核心是通过**多任务预训练（尤其是引入关键点轨迹预测）**，在**无需动作监督**的条件下，让VLM学习到**高质量、连续、可解释的潜在动作表示**，从而为机器人控制提供了一个更** scalable、高效且性能强劲**的VLA模型训练新范式。",
            "timestamp": "2026-02-03T15:54:32.027163",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "基于他的前作，梳理这个方向的整个发展脉络，每一步相比于之前的工作都改进了什么，着重于几个不同的发展方向。",
            "answer": "## 视觉-语言-动作（VLA）模型中潜在动作表示的发展脉络分析\n\n基于论文《CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control》及其引用的相关工作，该领域的发展脉络可以清晰地梳理为几个关键阶段和不同的技术发展方向。核心目标是从依赖大量动作标注数据转向利用更易得的无标注视频数据，学习能够有效控制机器人的潜在动作表示。\n\n### 发展脉络总览\n\n整个发展过程呈现出从**有监督动作学习** -> **无监督离散潜在动作学习** -> **无监督连续潜在动作学习** -> **多任务驱动的、更具语义性的潜在动作学习**的演进路径。同时，模型架构从**独立的潜在动作模型（LAM）** 逐渐与**视觉语言模型（VLM）** 进行融合。\n\n---\n\n### 第一阶段：有监督的视觉-语言-动作（VLA）模型\n**代表工作：** RT-1, RT-2, PALM-E, OpenVLA 等。\n*   **核心思想：** 直接使用包含精确动作标签（如关节角度、末端执行器轨迹）的大规模机器人演示数据进行端到端训练，将视觉、语言指令映射到具体的控制动作。\n*   **改进与价值：** 首次证明了大型模型能够统一理解语言指令、视觉场景并输出低级控制信号，实现了开箱即用的多种机器人技能。\n*   **局限性：** **严重依赖昂贵、难以获取的动作标注数据**，限制了模型的扩展性和对新机器人形态、新任务的泛化能力。这成为了后续研究试图解决的核心问题。\n\n### 第二阶段：无监督的离散潜在动作模型（LAM）\n**代表工作：** Genie, LAPA。\n*   **核心思想：** 放弃直接学习真实动作，转而从无标注的视频序列中学习一个**离散的潜在动作空间**。模型将视频帧之间的变化编码为一个离散的代码（code），这个代码被视为“潜在动作”。学习目标通常是重建下一帧。\n*   **关键技术：** 使用**向量量化变分自编码器（VQ-VAE）** 来构建离散的代码本（codebook）。\n*   **相比于第一阶段的改进：** **摆脱了对动作标签的依赖**，可以利用海量的互联网视频或未标注的机器人视频进行预训练，极大地提升了数据可扩展性。\n*   **存在的挑战（论文指出）：**\n    1.  **表示能力有限：** 离散代码本的固定大小限制了其捕捉细粒度、连续动作变化的能力。\n    2.  **语义模糊：** 由于训练目标是像素级重建，模型可能更关注背景纹理等静态特征，而非由动作引起的动态变化，导致学到的“动作”不纯粹。\n    3.  **捷径学习：** 模型可能退化为一个简单的未来帧预测器，而非真正理解了驱动状态变化的动作。\n    4.  **误差传播：** VQ-VAE固有的量化误差和代码本坍塌问题会直接传递给下游的VLA模型。\n\n### 第三阶段：无监督的连续潜在动作模型\n**代表工作：** Moto, COMO。\n*   **核心思想：** 将离散的潜在动作表示升级为**连续的潜在动作表示**，以克服离散表示容量不足的问题。\n*   **具体改进方向：**\n    *   **Moto:** 将训练目标从下一帧的**像素重建**改为预测下一帧的**高级语义嵌入**（使用ViT编码器提取）。这引导模型关注更高层次的语义变化，而非低级像素。\n    *   **COMO:** 彻底摒弃VQ-VAE的代码本，采用**交叉注意力（Cross-Attention）机制**（如Q-Former）来生成连续的潜在动作向量。这提供了更灵活、粒度更细的表示能力。\n*   **相比于第二阶段的改进：** **显著提升了潜在动作的表示能力和连续性**，能更好地建模复杂的动作空间。COMO的实验表明，连续表示通常优于离散表示。\n*   **尚未解决的问题：** 尽管表示形式变为连续，但**训练目标单一**（下一帧预测）的根本问题仍在。模型学到的“动作”是否真正对应控制语义，以及“捷径学习”风险，仍未得到针对性解决。\n\n### 第四阶段：多任务预训练以增强动作语义（本文工作 CARE）\n**代表工作：** CARE。\n*   **核心思想：** 不满足于仅生成连续的潜在动作，而是通过**多任务学习**来明确地增强潜在动作的**动作编码语义**，并防止模型走捷径。同时，将LAM训练与VLM预训练流程**深度融合**。\n*   **具体技术创新：**\n    1.  **架构融合：** 将VLM（Prismatic-7B）作为潜在动作的**编码器**，替代了独立的LAM编码器。构建了一个“编码器（VLM）-多解码器”的架构，简化了VLA训练流程（四阶段变三阶段）。\n    2.  **多任务目标设计：**\n        *   **任务一（继承）：** 下一视觉特征预测（继承自Moto等）。\n        *   **任务二（创新）：** **关键点轨迹预测**。使用预训练的点追踪模型（Co-Tracker）获取视频中稀疏关键点的运动轨迹，让模型预测这些点的移动。\n    3.  **训练策略：** 采用不确定性加权损失（UWL）平衡多任务。\n*   **相比于第三阶段的改进：**\n    *   **更强的动作语义：** 关键点轨迹预测任务**强制模型关注物体或场景部分的运动**，这更贴近“动作”的本质（引起位置/状态变化），从而学到更具解释性的潜在动作。线性探测（LP-MSE）和语义标签预测实验证明了其优越性。\n    *   **有效缓解捷径学习：** 多任务约束使得模型不能只优化单一的未来帧预测路径，必须生成一个能同时解释外观特征变化和几何位置变化的潜在表示。S-PCFC指标显著降低证实了这一点。\n    *   **性能提升：** 在LIBERO基准测试上，CARE在使用无动作标签预训练的方法中取得了最佳性能，甚至在某些任务上超越了部分使用动作标签预训练的方法，验证了其有效性。\n\n### 总结：几个不同的发展方向\n\n1.  **表示形式的发展：** **离散代码 (VQ-VAE) -> 连续向量 (Cross-Attention)**。这是提升模型表示能力和适应连续控制任务的基础。\n2.  **训练目标的发展：** **像素重建 -> 语义特征重建 -> 多任务（特征重建 + 运动轨迹预测）**。这一发展旨在让潜在动作更纯粹地对应“引起变化的动作”，而非混杂其他视觉信息。\n3.  **模型架构的发展：** **独立LAM -> 与VLM深度融合**。早期LAM是独立模块，需要与VLM拼接。CARE将其整合进VLM预训练，实现了更统一的框架和参数效率。\n4.  **监督信号的发展：** **强监督（动作标签） -> 弱监督/无监督（视频-文本对）**。这是贯穿始终的、推动领域前进的根本动力，旨在利用互联网规模的弱标注数据。\n\n**CARE** 的工作可以看作是站在前作肩膀上的一个综合创新：它采纳了**连续表示**（如COMO），设计了**新颖的多任务目标**来提升语义，并进行了**深度的VLM-LAM架构融合**，系统地应对了之前方法在语义明确性和抗捷径学习方面的挑战，代表了当前弱监督机器人VLA模型预训练的一个先进方向。",
            "timestamp": "2026-02-03T15:55:30.986522",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他的前作有哪些？使用表格仔细讲讲他的每篇前作，他和前作的区别是什么，主要改善是什么？着重于具体相比于之前文章的改动",
            "answer": "## 前作梳理与分析\n\n根据论文内容，CARE 的研究建立在几项关键的先前工作之上，这些工作主要围绕**隐式动作模型** 和**视觉-语言-动作模型** 展开。以下表格详细梳理了每篇前作的核心思想、CARE 与其的区别以及主要改进之处。\n\n| 前作名称 (模型/论文) | 核心思想 / 方法 | 与 CARE 的主要区别 | CARE 的主要改进 / 创新点 |\n| :--- | :--- | :--- | :--- |\n| **Genie** [bruce2024genie] | 提出了**隐式动作模型** 的概念。使用编码器-解码器架构和**离散码本**，从视频中学习离散的隐式动作，以生成交互式环境。 | 1. **动作表示**：使用**离散**的码本表示动作。<br>2. **训练目标**：主要基于**像素级下一帧重建**。<br>3. **模型定位**：是一个独立的生成模型，未与VLM深度集成。 | 1. **连续动作表示**：CARE 使用**连续**的隐式动作表示（通过LLM生成），避免了离散码本容量有限和量化误差的问题。<br>2. **多任务学习**：引入了**关键点轨迹预测**作为新的训练目标，与帧预测共同优化，使模型更关注动作驱动的变化，而非静态背景。<br>3. **端到端VLM集成**：将LAM训练无缝集成到VLM的预训练流程中，构建了**编码器-多解码器**的统一架构，而非独立训练LAM。 |\n| **LAPA** [ye2024latent] | 基于 **VQ-VAE** 目标，学习图像帧之间的**离散隐式动作**。是首个将离散LAM应用于机器人任务的工作。 | 1. **动作表示**：**离散**（VQ-VAE码本）。<br>2. **训练缺陷**：继承了VQ-VAE的**量化误差**和**码本坍塌**问题，且训练目标单一（下一帧重建），容易导致模型关注非动作相关的像素变化（如纹理）。<br>3. **“捷径学习”**：论文指出其方法容易陷入“捷径学习”，即模型退化为未来帧预测器，而非真正的动作建模器。 | 1. **解决偏差传播**：CARE 使用VLM作为编码器，通过多任务学习生成连续动作，**避免了直接继承VQ-VAE的固有缺陷**。<br>2. **增强动作显式编码**：通过**多任务学习**（帧预测+关键点预测）明确引导模型编码与运动相关的信息，改善了LAPA中隐式动作仅压缩帧间差异的问题。<br>3. **缓解捷径学习**：多任务目标和VLM的语义理解能力有助于模型学习更具因果性的动作表示，实验证明其S-PCFC指标显著优于LAPA。 |\n| **Moto** [chen2412moto] | 改进了LAM的训练目标，从**像素重建**转向预测**高级语义嵌入**（使用ViT编码器），以学习更具语义意义的动作表示。 | 1. **动作表示**：仍使用**离散**码本。<br>2. **改进方向**：虽然关注了语义，但未改变动作的离散本质，且结构上仍是独立的LAM。 | 1. **连续化与集成**：CARE 将动作表示**连续化**，并更进一步，**将动作学习过程直接嵌入到VLM的预训练中**，使视觉、语言和动作表示在统一模型内共同优化。<br>2. **更细粒度的监督**：除了高级语义特征，CARE 额外引入了**关键点轨迹**这种中等层次、几何明确的监督信号，使动作表示更加精确和可解释。 |\n| **CoMo** [yang2025learning] | 提出了**连续隐式动作表示**，用**交叉注意力机制**替代离散码本，以捕捉更细粒度的动作变化。 | 1. **架构基础**：CoMo 基于**Diffusion Policy** 实现，其连续动作表示依赖于特定的Q-Former模块。<br>2. **训练目标**：虽然也是连续表示，但其预训练可能仍侧重于单一的重建任务（论文未明确其多任务性）。<br>3. **与VLM关系**：是一个为VLA模型提供隐式动作的**前置模块**，训练流程是分离的。 | 1. **训练流程简化**：CARE **将LAM训练与VLM预训练合并**，将传统的四阶段VLA训练（VLM预训练、LAM训练、融合、微调）简化为**三阶段**（CARE预训练、动作头微调）。<br>2. **多任务驱动的表示学习**：明确采用**多任务学习**来丰富和显式化动作表示，这是CoMo未强调的。实验表明，CARE的隐式动作在**线性探测**和**语义预测**任务上均优于CoMo，表明其动作表示更具解释性和对真实动作的映射能力。<br>3. **更强的抗捷径学习能力**：CARE的S-PCFC指标低于CoMo，表明其通过多任务和VLM集成，更有效地避免了模型学习数据中的虚假关联。 |\n\n### 总结：CARE 相对于前作的核心演进\n\n1.  **表示形式演进**：**离散码本 (Genie, LAPA) -> 连续表示 (Moto-语义, CoMo) -> 连续表示 + 多任务显式引导 (CARE)**。CARE 在连续化的基础上，通过多任务学习确保该表示能明确编码动作信息。\n2.  **架构集成演进**：**独立LAM模块 (前所有工作) -> LAM与VLM预训练流程深度融合 (CARE)**。CARE 不再是VLA的一个独立前置组件，而是其预训练阶段的内在部分，实现了更高效的联合优化。\n3.  **训练目标演进**：**单一像素重建 -> 高级语义重建 -> 多任务（语义重建 + 几何运动预测）**。CARE 引入关键点轨迹预测，迫使模型理解物体/场景的**运动结构**，这是针对机器人控制任务非常关键的改进。\n4.  **问题针对性演进**：前作分别暴露了**离散表示缺陷**、**捷径学习**、**动作表示不显式**等问题。CARE 通过**连续VLM集成**、**多任务学习**和**几何监督信号**，系统地针对这些问题提出了综合解决方案。",
            "timestamp": "2026-02-03T15:56:24.485561",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文提出了哪些关键技术方法，请列表格具体详细说明技术细节，需要包含具体的数学原理推导，以及具体参数。",
            "answer": "好的，作为一名专业的学术论文分析助手，我将为您详细解析论文《CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control》提出的关键技术方法。\n\n## 论文核心方法概述\n\nCARE 的核心创新在于提出了一种**无需动作标注**的预训练框架，通过**多任务学习**让视觉语言模型（VLM）学习**连续的潜在动作表示**，从而为机器人控制任务打下基础。其关键技术方法可总结为下表：\n\n| 方法模块 | 核心目标 | 技术细节与数学原理 | 具体参数/模型 |\n| :--- | :--- | :--- | :--- |\n| **1. 潜在视觉语言模型架构** | 构建一个能接收图像和指令，并输出连续潜在动作表示 \\( z \\) 的编码器。 | 1. **输入处理**：<br>   - 图像 \\( x_{img} \\) 通过双视觉编码器（SigLIP + DinoV2）提取特征 \\( f_{sig} \\) 和 \\( f_{dino} \\)。<br>   - 文本提示 \\( p \\) 经分词和文本嵌入层得到 \\( f_T \\)。<br>2. **特征融合**：<br>   - 视觉特征拼接：\\( f_{cat} = \\text{Concat}(f_{sig}, f_{dino}) \\in \\mathbb{R}^{N_p \\times 2D_v} \\)。<br>   - 投影：\\( f_v = \\text{Proj}_{\\zeta}(f_{cat}) \\in \\mathbb{R}^{N_p \\times D_l} \\)，其中 \\( \\text{Proj}_{\\zeta} \\) 是一个两层MLP。<br>3. **潜在动作生成**：<br>   - 将 \\( f_T \\) 和 \\( f_v \\) 拼接后输入LLM主干：\\( h_{last} = \\text{LM}_{\\theta}([f_T; f_v]) \\)。<br>   - 从 \\( h_{last} \\) 中提取对应动作维度的部分作为连续潜在动作表示 \\( z \\)。 | - **主干模型**: Prismatic-7B VLM (基于 Llama 7B)。<br>- **视觉编码器**: SigLIP 和 DinoV2 (预训练)。<br>- **投影层**: \\( \\text{Proj}_{\\zeta}: \\mathbb{R}^{2D_v} \\to \\mathbb{R}^{D_l} \\)，具体维度未明确给出，但 \\( D_l \\) 应与LLM隐藏层维度对齐。 |\n| **2. 多任务学习目标** | 通过两个并行的解码任务，引导VLM学习具有明确动作语义的潜在表示 \\( z \\)，避免“捷径学习”。 | **任务一：下一帧特征预测**<br> - **输入**: 当前帧视觉特征 \\( f_v^t \\) 和潜在动作 \\( z_t \\)。<br> - **融合**: 使用交叉注意力机制计算 \\( z_t \\) 与 \\( f_v^t \\) 的关系：<br>   \\( Q = z_t W_z + b_z, \\quad K = V = f_v^t W_f + b_f \\)<br>   \\( z_f = \\text{softmax}(\\frac{QK^T}{\\sqrt{d}}) V \\)<br> - **输出**: \\( z_f \\) 输入**帧解码器**（MLP），预测下一帧特征 \\( \\hat{f}_v^{t+1} \\)。<br> - **损失**: \\( \\mathcal{L}_f^t = \\text{MSE}(f_v^{t+1}, \\hat{f}_v^{t+1}) \\)<br><br>**任务二：关键点轨迹预测**<br> - **输入**: 当前帧均匀分布的256个关键点坐标 \\( k_t \\in \\mathbb{R}^{256 \\times 2} \\) 和潜在动作 \\( z_t \\)。<br> - **融合**: 同样使用交叉注意力，将 \\( k_t \\) 视为序列输入，与 \\( z_t \\) 融合得到 \\( z_k \\)。<br> - **输出**: \\( z_k \\) 输入**关键点解码器**（MLP），预测下一帧关键点坐标 \\( \\hat{k}^{t+1} \\)。<br> - **损失**: \\( \\mathcal{L}_p^t = \\text{MSE}(k^{t+1}, \\hat{k}^{t+1}) \\)<br> - **关键点来源**: 使用预训练的 Co-Tracker 模型从视频中提取 \\( k_t \\) 和 \\( k_{t+1} \\)。 | - **关键点数量**: 256个。<br>- **坐标维度**: 2维 (x, y)。<br>- **预训练跟踪器**: Co-Tracker。<br>- **解码器**: 均为MLP，具体结构未详述。 |\n| **3. 不确定性加权损失** | 自适应地平衡两个多任务损失，实现稳定训练。 | 基于 Kendall 等人的多任务不确定性加权损失框架：<br> \\( \\mathcal{L}^t = \\frac{1}{\\sigma_1^2} \\mathcal{L}_f^t + \\frac{1}{\\sigma_2^2} \\mathcal{L}_p^t + \\log \\sigma_1 + \\log \\sigma_2 \\) <br> **原理推导**:<br> 1. 假设每个任务的预测误差服从高斯分布，其方差 \\( \\sigma^2 \\) 代表了该任务的“不确定性”或噪声水平。<br> 2. 最大化给定模型参数下数据的对数似然。对于回归任务（MSE损失），其负对数似然正比于 \\( \\frac{1}{\\sigma^2} \\mathcal{L} + \\log \\sigma \\)。<br> 3. 将两个任务的负对数似然相加，即得到上述联合损失 \\( \\mathcal{L}^t \\)。<br> **作用**: 模型在训练中同时学习参数 \\( \\sigma_1, \\sigma_2 \\)。不确定性高（\\( \\sigma \\) 大）的任务，其损失权重 \\( 1/\\sigma^2 \\) 会自动降低，反之亦然。 | - **可学习参数**: \\( \\sigma_1, \\sigma_2 \\) (初始值通常设为1或0)。<br>- **损失类型**: \\( \\mathcal{L}_f \\) 和 \\( \\mathcal{L}_p \\) 均采用均方误差（MSE）。 |\n| **4. 监督微调与动作头** | 将学到的潜在动作 \\( z \\) 映射到真实的机器人关节或末端执行器动作。 | 1. **架构**: 在预训练好的潜在VLM后，附加一个轻量级的**残差MLP**作为动作头。<br>2. **输入**: 解码器的最后一层隐藏表示（即与生成 \\( z \\) 相关的特征）。<br>3. **输出**: 回归得到真实动作值（如关节角度、末端位姿）。<br>4. **损失**: L1 回归损失：\\( \\mathcal{L}_{action} = \\| a_{pred} - a_{gt} \\|_1 \\)<br>5. **训练策略**: 采用 **LoRA** 对主干模型进行高效微调，仅更新低秩适配器参数，冻结大部分预训练权重。 | - **微调数据**: RT-1 数据集的 3% 子集（包含动作标签）。<br>- **微调方法**: LoRA (Low-Rank Adaptation)。<br>- **动作头**: 残差MLP，具体层数和维度未详述。 |\n\n## 技术创新与实际价值总结\n\n1.  **范式创新**：将潜在动作模型（LAM）的训练无缝集成到VLM的预训练流程中，将传统的“预训练VLM -> 训练LAM -> 微调VLA”四阶段流程，简化为“多任务预训练潜在VLM -> 微调动作头”三阶段流程，提升了效率。\n\n2.  **表示创新**：提出**连续**的潜在动作表示，克服了基于VQ-VAE的离散表示方法存在的量化误差、码本坍塌和表示容量有限的问题。\n\n3.  **目标函数创新**：引入**关键点轨迹预测**作为辅助任务。该任务强制模型关注由动作引起的**物体/场景的结构性、位置性变化**，而非静态纹理，从而增强了潜在动作 \\( z \\) 对**运动本身**的编码能力，提高了语义可解释性，并有效缓解了模型仅通过像素变化进行“捷径学习”的问题。\n\n4.  **实际价值**：\n    *   **降低数据依赖**：仅需大量易得的**视频-文本对**进行预训练，无需昂贵、难以获取的精确动作标注，极大地提升了机器人技能学习的可扩展性。\n    *   **提升泛化与解释性**：学到的潜在动作空间更具语义意义，有利于跨任务、跨形态的迁移，并为理解模型决策提供了可能。\n    *   **性能竞争力**：在LIBERO基准测试上，使用无动作标签预训练的CARE，其性能可媲美甚至部分超越需要使用动作标签预训练的先进方法（如OpenVLA），证明了弱监督方法的巨大潜力。",
            "timestamp": "2026-02-03T15:57:33.921827",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他使用了哪些评价指标与数据集，列表格具体讲讲他的评价指标的细节与数据集的细节",
            "answer": "## 论文分析：CARE的评价指标与数据集\n\n### 一、 使用的数据集\n\n论文在两个阶段使用了不同的数据集：**预训练阶段**（无动作标签）和**微调阶段**（有动作标签）。\n\n| 阶段 | 数据集名称 | 来源/类型 | 数据规模 | 关键特点与用途 |\n| :--- | :--- | :--- | :--- | :--- |\n| **预训练** | Open X-Embodiment | 机器人操作视频 | 约 140k 条轨迹 | 大规模、多样化的机器人演示数据，用于无监督学习潜在动作表示。 |\n| **预训练** | Something-Something v2 | 人类日常活动视频 | 约 100k 个视频片段 | 包含丰富的人类行为，提供更广泛的视觉-语言-动作关联，增强模型泛化能力。 |\n| **微调** | RT-1 (3%均匀采样) | 机器人演示（带动作标签） | RT-1 数据集的 3% | 包含真实的机器人末端执行器或关节动作标签，用于训练轻量级的“动作头”，将潜在动作映射到真实控制指令。 |\n\n**数据集使用总结**：\n- **总预训练数据**：约 240k 条轨迹/视频片段，均为**视频-文本对**，**不含动作标签**。\n- **核心思想**：利用大量易得的无标签视频进行预训练，仅需少量有标签数据进行微调，解决了动作标注成本高昂的瓶颈。\n\n### 二、 使用的评价指标\n\n论文围绕三个核心研究问题（Q1性能，Q2可解释性，Q3捷径学习）设计了多组评价指标。\n\n| 研究问题 | 评价指标 | 缩写/全称 | 计算方式与细节 | 指标含义（越高/低越好） |\n| :--- | :--- | :--- | :--- | :--- |\n| **Q1: 性能** | 任务成功率 | SR (Success Rate) | 在LIBERO仿真环境中执行任务，计算成功完成的比率。报告了**Goal, Spatial, Object, Long**四个任务套件的平均成功率。 | **越高越好**。直接衡量经过微调后，模型控制机器人完成具体任务的能力。 |\n| **Q2: 可解释性** | 线性探测均方误差 | LP-MSE (Linear Probing MSE) | 1. 冻结预训练好的CARE模型，提取其生成的**潜在动作表示**。 <br> 2. 在这些潜在动作上训练一个简单的**MLP线性层**，去预测**真实的地面真值动作**。 <br> 3. 计算预测动作与真实动作之间的均方误差。 | **越低越好**。该指标衡量潜在动作表示与真实动作空间的**线性相关性**。误差越低，说明潜在动作蕴含的真实控制信息越丰富、越直接，可解释性越强。 |\n| **Q2: 可解释性** | 语义标签预测准确率 | Semantic Accuracy | 1. 使用预训练模型提取的**潜在动作序列**（如初始帧+后续9帧的潜在动作）。 <br> 2. 训练一个分类器，仅根据这些潜在动作（**不输入后续图像**）来预测任务的**语义标签**（即判断是哪个任务）。 <br> 3. 计算分类准确率。 | **越高越好**。该指标衡量潜在动作表示是否编码了**高级语义信息**。准确率高说明潜在动作不仅能驱动运动，还能反映任务的高层目标，具有语义可解释性。 |\n| **Q3: 捷径学习** | 序列预测一致性分数 | S-PCFC (Sequential Predictive Consistency Fading Coefficient) | 引用自CoMo论文的指标。其核心思想是：如果模型发生了“捷径学习”（例如，仅根据当前帧预测未来帧，而忽略了动作的作用），那么当输入一系列相同的潜在动作时，其预测的未来状态会迅速收敛到一个固定点。S-PCFC量化了这种收敛的速度。 | **越低越好**。分数越低，表明模型对相同动作输入的响应变化越平缓，**越不容易发生“捷径学习”**，说明学到的潜在动作是真正具有控制意义的动态表示，而非静态特征的压缩。 |\n\n**评价指标总结**：\n- **性能评估**：采用标准的机器人任务**成功率**，在LIBERO基准上进行。\n- **可解释性评估**：创新性地使用了 **LP-MSE** 和 **语义准确率** 两个指标，分别从**低层动作关联**和**高层语义关联**两个角度量化潜在动作表示的质量。\n- **抗捷径学习评估**：采用了专门的 **S-PCFC** 指标，科学地验证了所提多任务预训练方法在解决模型退化问题上的有效性。\n\n这些指标共同构成了一个全面、深入的评估体系，不仅证明了CARE在任务性能上的优势，更重要的是揭示了其方法在**学习质量**和**表示能力**上的内在提升。",
            "timestamp": "2026-02-03T15:58:12.057381",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文在哪些数据集上进行了实验？主要的评估指标和性能提升是多少？",
            "answer": "## 论文实验数据集、评估指标与性能提升分析\n\n### 一、 实验数据集\n论文在两个阶段使用了不同的数据集：\n\n#### 1. **预训练阶段（无动作标签）**\n- **机器人视频数据**：来自 **Open X-Embodiment** 数据集，包含约 **140k** 条轨迹。\n- **人类视频数据**：来自 **Something-Something v2** 数据集，包含约 **100k** 个日常活动视频片段。\n- **总计**：约 **240k** 条轨迹/视频片段，均为**视频-文本对**，**不包含动作标签**。\n\n#### 2. **微调阶段（有动作标签）**\n- 使用 **RT-1** 数据集的 **3%** 均匀采样数据，该数据集包含真实的机器人动作标签（如关节角度、末端执行器轨迹）。\n\n#### 3. **评估基准**\n- 使用 **LIBERO** 基准进行最终策略性能评估。该基准包含四个任务套件，每个套件有10个任务：\n    - **Goal**：目标导向任务\n    - **Spatial**：空间推理任务\n    - **Object**：物体操作任务\n    - **Long**：长时程任务\n\n### 二、 主要评估指标\n论文针对三个核心研究问题（Q1-Q3）设计了不同的评估指标：\n\n#### **Q1: 性能 (Performance)**\n- **主要指标**：**任务成功率 (Success Rate, SR)**，在LIBERO模拟器中评估，越高越好。\n- **目的**：验证CARE预训练的VLM在少量有标签数据微调后，能否学习到有效的真实动作以提升策略性能。\n\n#### **Q2: 可解释性 (Interpretability)**\n1.  **线性探测均方误差 (Linear Probing MSE, LP-MSE)**：\n    - 将学习到的潜在动作输入一个MLP来预测真实动作，计算预测值与真实值的均方误差。**越低越好**，表明潜在动作与真实动作的线性关联更强。\n2.  **语义标签预测准确率 (Semantic Accuracy)**：\n    - 仅使用初始帧和后续的潜在动作序列来预测任务的语义标签。**越高越好**，表明潜在动作包含了丰富的、可解释的语义信息。\n\n#### **Q3: 捷径学习规避 (Shortcut Learning)**\n- **指标**：**S-PCFC (State-Prediction from Current Frame and Control)**。\n    - 该指标衡量模型是否退化成了简单的未来帧预测器（即“走捷径”），而非真正的动作建模器。\n    - **值越低越好**，表明模型更少地依赖捷径学习。\n\n### 三、 性能提升结果\n根据论文中的实验结果（主要见表1和表2），CARE方法展现出显著优势：\n\n#### **1. 整体任务成功率 (vs. 无标签预训练方法)**\n- CARE (**77.7%** 平均成功率) 显著优于其他**无需动作标签**的预训练方法：\n    - **LAPA**：64.3% （+13.4%）\n    - **CoMo**：69.2% （+8.5%）\n- 甚至**媲美或超越**部分**使用动作标签**的预训练方法：\n    - **OpenVLA** (使用标签): 75.0% （+2.7%）\n    - **Octo** (使用标签): 73.7% （+4.0%）\n- **关键亮点**：在**长时程任务(Long)**上，CARE (65.3%) 相比OpenVLA (52.8%) 提升了 **12.5%**，显示出其在复杂、多步任务上的强大泛化能力。\n\n#### **2. 可解释性指标提升**\n- **LP-MSE (越低越好)**：CARE在多个任务上均取得最低误差，证明其潜在动作与真实动作空间的对齐度最高。\n    - 例如在Goal任务上，相比CoMo降低22.8%，相比LAPA降低47.8%。\n- **语义准确率 (越高越好)**：使用CARE的潜在动作进行语义预测，准确率达到 **84.2%**，显著高于CoMo (71.2%) 和LAPA (64.1%)，表明其潜在动作具有更强的语义信息编码能力。\n\n#### **3. 有效规避捷径学习**\n- **S-PCFC指标 (越低越好)**：CARE在Goal任务上得分为 **0.833**，远低于LAPA (0.980) 和CoMo (0.899)，证明其通过多任务预训练有效迫使模型学习动作的本质变化，而非依赖静态特征的捷径。\n\n#### **4. 多任务学习的有效性 (消融实验)**\n- 单独使用“下一帧预测”任务：成功率 67.8%\n- 单独使用“关键点轨迹预测”任务：成功率 54.3%\n- **多任务学习 (CARE)**：成功率 **77.7%**\n- **结论**：多任务学习产生了显著的协同效应（1+1>2），是性能提升的关键。\n\n### 总结\nCARE框架通过利用大规模、易获取的**无动作标签视频-文本对**进行多任务预训练，成功学习到了**高质量、连续、可解释的潜在动作表示**。在仅需**少量有标签数据微调**后，其在机器人控制任务上的性能**显著超越同类无监督方法**，并**达到甚至部分超越了有监督方法的水平**。其核心优势体现在**更高的任务成功率、更强的潜在动作语义可解释性，以及有效规避模型退化（捷径学习）的能力**。",
            "timestamp": "2026-02-03T15:58:54.934615",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文的主要局限性有哪些？未来可能的改进方向是什么？",
            "answer": "## 论文主要局限性分析\n\n基于对论文内容的分析，CARE框架存在以下主要局限性：\n\n- **性能差距**：尽管CARE在无动作标签预训练方法中表现优异，但其性能（平均成功率77.7%）与**完全依赖动作标签进行预训练**的模型（如OpenVLA，75.0%）相比，优势并不显著，且在某些任务上仍存在差距。这表明仅使用视频-文本对的弱监督学习，在动作表示的精确性上仍无法完全替代有监督的预训练。\n\n- **数据依赖与泛化**：模型的性能高度依赖于预训练数据的**规模和质量**。实验表明，使用更广泛的数据集（Human+Bridge+RT-1）能显著提升效果（77.7%），而仅使用单一数据源（如Human only）则效果较差（63.3%）。这暗示模型可能对数据分布敏感，在**未见过的机器人形态或任务领域**的泛化能力有待进一步验证。\n\n- **架构复杂性**：CARE采用了**多任务学习**和**编码器-多解码器架构**，增加了模型的复杂性和训练成本。虽然这带来了性能提升，但也可能影响训练效率和可扩展性。\n\n- **动作表示的间接性**：预训练阶段学习的**连续潜在动作表示**并非真实的机器人动作（如关节角度），仍需一个额外的**微调阶段**和**动作头**来映射到具体控制指令。这个映射过程依赖于小规模的有标签数据，未能完全摆脱对动作监督的依赖。\n\n- **实验环境的局限性**：论文的所有实验均在**仿真环境（LIBERO）** 中进行验证。在仿真中表现良好的模型，在**真实物理世界**中可能面临感知噪声、执行误差、延迟等挑战，其有效性有待实地机器人平台检验。\n\n## 未来可能的改进方向\n\n针对上述局限性，论文在结论部分及分析中暗示了以下未来工作方向：\n\n- **融入动作先验与分块**：论文提到计划通过引入 **“动作分块”** 技术来缩小与全监督方法的性能差距。动作分块可以将低层动作组合成更有意义的宏动作，有望提升动作表示的抽象能力和控制效率。\n\n- **扩展多维度感知**：计划纳入**多维度感知视角**，例如结合腕部相机等多视角视觉输入（如基线MDT所做），以提供更丰富、鲁棒的环境状态信息，从而改善在复杂操作任务中的表现。\n\n- **探索更高效的多任务学习**：当前的多任务损失加权（UWL）和双解码器架构是有效的，但未来可以研究**更优的任务组合、损失函数或动态权重调整机制**，以进一步提升训练效率并避免任务间的负迁移。\n\n- **增强数据利用与合成**：为了减少对大规模高质量真实数据的依赖，可以探索使用**合成数据**、**数据增强**技术或更先进的**跨域迁移学习**方法，使模型能更好地利用现有多样化的视频数据。\n\n- **向真实机器人部署推进**：最关键的下一步是将CARE框架部署到**真实机器人平台**进行端到端评估，验证其在真实物理交互中的有效性、鲁棒性和安全性，并针对实际挑战（如实时性、校准误差）进行优化。\n\n- **探索更广泛的基准任务**：除了LIBERO，应在更多样化、更复杂的机器人操作基准（如MetaWorld, RLBench等）上进行测试，以全面评估其泛化能力和局限性。\n\n**总结**：CARE的核心创新在于通过多任务预训练从弱对齐数据中学习连续潜在动作表示，其最大价值在于**降低了对昂贵动作标注的依赖，提升了可扩展性**。未来的改进将围绕**提升性能至媲美全监督方法、增强泛化能力、并最终实现高效可靠的真实世界部署**而展开。",
            "timestamp": "2026-02-03T15:59:26.004631",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T02:28:32Z",
    "created_at": "2026-02-03T15:53:04.510137",
    "updated_at": "2026-02-03T15:53:04.510143"
}