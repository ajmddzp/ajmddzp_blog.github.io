{
    "id": "2601.23087v1",
    "title": "Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation",
    "authors": [
        "Wu Songwei",
        "Jiang Zhiduo",
        "Xie Guanghu",
        "Liu Yang",
        "Liu Hong"
    ],
    "abstract": "Â≠¶‰π†ÈïøÊó∂Á®ãÊú∫Âô®‰∫∫Êìç‰ΩúÈúÄË¶ÅÂêåÊó∂ÂÆûÁé∞È´òË°®Áé∞ÂäõÁöÑË°å‰∏∫Âª∫Ê®°„ÄÅÂÆûÊó∂Êé®ÁêÜ‰∏éÁ®≥ÂÆöÊâßË°åÔºåËøôÂØπÁé∞ÊúâÁîüÊàêÂºèÁ≠ñÁï•‰ªçÂÖ∑ÊåëÊàò„ÄÇÂü∫‰∫éÊâ©Êï£ÁöÑÊñπÊ≥ïËôΩÂÖ∑Â§áÂº∫Â§ßÂª∫Ê®°ËÉΩÂäõÔºå‰ΩÜÈÄöÂ∏∏‰º¥ÈöèÈ´òÊé®ÁêÜÂª∂ËøüÔºõËÄåÊµÅÂåπÈÖçÊäÄÊúØËôΩËÉΩÂÆûÁé∞Âø´ÈÄüÂçïÊ≠•ÁîüÊàêÔºåÁõ¥Êé•Âú®ÂéüÂßãÂä®‰ΩúÁ©∫Èó¥Â∫îÁî®Êó∂ÂæÄÂæÄÂØºËá¥ÊâßË°å‰∏çÁ®≥ÂÆö„ÄÇ\n\nÊàë‰ª¨ÊèêÂá∫LG-FlowÁ≠ñÁï•ÔºåËøôÊòØ‰∏ÄÁßçÂú®ËøûÁª≠ÊΩúÂú®Âä®‰ΩúÁ©∫Èó¥ËøõË°åÊµÅÂåπÈÖçÁöÑËΩ®ËøπÁ∫ßÊ®°‰ªøÂ≠¶‰π†Ê°ÜÊû∂„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜÂä®‰ΩúÂ∫èÂàóÁºñÁ†Å‰∏∫Êó∂Èó¥Ê≠£ÂàôÂåñÁöÑÊΩúÂú®ËΩ®ËøπÔºåÂπ∂Â≠¶‰π†ÊòæÂºèÁöÑÊΩúÂú®Á©∫Èó¥ÊµÅÔºåÂÆûÁé∞‰∫ÜÂÖ®Â±ÄËøêÂä®ÁªìÊûÑ‰∏éÂ∫ïÂ±ÇÊéßÂà∂Âô™Â£∞ÁöÑËß£ËÄ¶Ôºå‰ªéËÄåËé∑ÂæóÂπ≥ÊªëÂèØÈù†ÁöÑÈïøÊó∂Á®ãÊâßË°åÊïàÊûú„ÄÇ\n\nLG-FlowÁ≠ñÁï•Ëøõ‰∏ÄÊ≠•ËûçÂêà‰∫ÜÂá†‰ΩïÊÑüÁü•ÁÇπ‰∫ëÊù°‰ª∂Êú∫Âà∂‰∏éÊâßË°åÊó∂Â§öÊ®°ÊÄÅË∞ÉÂà∂ÂäüËÉΩÔºåÂÖ∂‰∏≠ËßÜËßâÁ∫øÁ¥¢‰Ωú‰∏∫Áé∞ÂÆûÂú∫ÊôØ‰∏≠ÁöÑ‰ª£Ë°®ÊÄßÊ®°ÊÄÅËøõË°åËØÑ‰º∞„ÄÇ‰ªøÁúü‰∏éÂÆû‰ΩìÊú∫Âô®‰∫∫Âπ≥Âè∞ÂÆûÈ™åË°®ÊòéÔºåÁõ∏ËæÉ‰∫éÂú®ÂéüÂßãÂä®‰ΩúÁ©∫Èó¥ËøêË°åÁöÑÊµÅÂåπÈÖçÂü∫Á∫øÊñπÊ≥ïÔºåLG-FlowÁ≠ñÁï•ÂÆûÁé∞‰∫ÜÊé•ËøëÂçïÊ≠•ÁöÑÊé®ÁêÜÈÄüÂ∫¶ÔºåÊòæËëóÊèêÂçá‰∫ÜËΩ®ËøπÂπ≥ÊªëÂ∫¶‰∏é‰ªªÂä°ÊàêÂäüÁéáÔºåÂêåÊó∂‰ªçÊØîÂü∫‰∫éÊâ©Êï£ÁöÑÁ≠ñÁï•Êõ¥‰∏∫È´òÊïà„ÄÇ",
    "url": "https://arxiv.org/abs/2601.23087v1",
    "html_url": "https://arxiv.org/html/2601.23087v1",
    "html_content": "Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation\nWu Songwei, Jiang Zhiduo , Xie Guanghu, Liu Yang\n‚Ä†\n, Liu Hong\n‚Ä†\nCorresponding author: Liu Yang(liuyanghit@hit.edu.cn).*This work was supported by the Natural Science Foundation of Hei longjiang Province for Excellent Young Scholars (Grant No. YQ2024E018)\nand the Youth Talent Support Program of the China (Grant No. 2022-JCJQQT-061).\nAll authors are with with the State Key Laboratory of Robotics and Systems,\nHarbin Institute of Technology, Harbin 150001, Heilongjiang, China\nAbstract\nLearning long-horizon robotic manipulation requires jointly achieving expressive behavior modeling,\nreal-time inference, and stable execution, which remains challenging for existing generative policies.\nDiffusion-based approaches offer strong modeling capacity but incur high inference latency,\nwhile flow matching enables fast one-step generation yet often suffers from unstable execution when operating directly in the raw action space.\nWe propose\nLG-Flow Policy\n, a trajectory-level imitation learning framework that performs flow matching in a continuous latent action space.\nBy encoding action sequences into temporally regularized latent trajectories and learning an explicit latent-space flow,\nLG-Flow Policy decouples global motion structure from low-level control noise, enabling smooth and reliable long-horizon execution.\nThe framework further integrates geometry-aware point cloud conditioning and execution-time multimodal modulation, with visual cues as a representative modality evaluated in real-world settings for robust deployment.\nExperiments in simulation and on real robots show that LG-Flow Policy achieves near single-step inference,\nimproves trajectory smoothness by up to\n93.7%\nand task success by up to\n25%\nover raw action-space flow baselines,\nwhile remaining significantly faster than diffusion-based policies.\nI\nIntroduction\nDiffusion-based policies (DP)\n[\n6\n]\nhave demonstrated strong performance in robotic visuomotor imitation learning,\nparticularly for complex manipulation tasks in both simulation and real-world environments.\nBy modeling multimodal action distributions conditioned on rich sensory observations such as images\n[\n6\n,\n17\n]\nor point clouds\n[\n32\n,\n25\n,\n26\n]\n, these methods enable expressive behavior modeling and generalization across diverse task variations.\nMore recently, flow matching\n[\n16\n,\n5\n,\n10\n]\nhas emerged as an alternative generative paradigm for robotic control\n[\n34\n,\n7\n,\n2\n]\n.\nBy formulating action generation as the solution of an ordinary differential equation (ODE),\nflow matching enables near one-step inference and substantially improves inference efficiency compared to diffusion-based approaches,\nmaking it a promising framework for real-time imitation learning.\nDespite these advances, deploying generative policies on physical robots remains challenging.\nDiffusion policies rely on iterative denoising, which introduces nontrivial inference latency and limits real-time applicability.\nFlow matching methods, while efficient, generate trajectories directly in the raw action space, where modeling noise\nand numerical errors tend to accumulate over long horizons, resulting in jittery trajectories and degraded execution stability.\nAs a result, existing approaches struggle to simultaneously achieve expressive behavior modeling, fast inference, and stable execution.\nSeveral recent works have explored different strategies to alleviate these issues.\nDP3\n[\n32\n]\nand iDP3\n[\n31\n]\nimprove perception efficiency through lightweight point cloud encoders,\nRDP\n[\n28\n]\nintroduces a hierarchical Fast‚ÄìSlow control structure for long-horizon behaviors,\nand Flow Policy\n[\n34\n]\nincorporates consistency constraints to enhance execution performance.\nHowever, our experimental results indicate that the fundamental trade-off between inference efficiency and execution stability\nhas not yet been fully resolved.\nTo address this challenge, we propose\nLG-Flow Policy\n, a trajectory-level action generation framework that performs flow matching\nin a continuous latent action space.\nBy mapping action sequences into temporally regularized latent trajectory representations and learning an explicit latent-space flow,\nLG-Flow Policy preserves the inference efficiency of flow-based methods while substantially improving execution smoothness and stability.\nOperating on motion-level latent representations decouples global trajectory structure from low-level control noise,\nmitigating error amplification during trajectory generation and enabling more reliable long-horizon behaviors.\nLG-Flow Policy further integrates geometry-aware point cloud conditioning and execution-time visual modulation from a wrist-mounted camera,\nproviding rich environmental awareness without perturbing latent trajectory dynamics.\nExtensive simulation and real-world experiments demonstrate that the proposed approach consistently achieves smooth trajectories,\nhigh task success rates, and fast response times, outperforming both diffusion-based and raw action-space flow-based baselines.\nOur main contributions are summarized as follows:\n‚Ä¢\nWe introduce a trajectory-level latent flow matching framework that reconciles fast inference with smooth and stable robotic manipulation.\n‚Ä¢\nWe show that performing flow matching in a continuous latent action space significantly reduces trajectory jitter\nand improves execution reliability.\n‚Ä¢\nWe demonstrate through comprehensive simulation and real-world experiments that the proposed method achieves robust performance across diverse tasks,\nincluding improved tolerance to heterogeneous and suboptimal demonstrations.\nFigure 1:\nOverall architecture of the proposed LG-Flow Policy. The system first encodes point cloud observations into geometry-aware scene features, then performs latent-space flow matching to generate temporally coherent latent action trajectories, which are finally decoded into executable control commands under visual conditioning.\nII\nRelated Work\nII-A\nDiffusion and Flow-Based Policies for Robotics\nDiffusion-based policies\n[\n14\n,\n6\n,\n32\n,\n31\n,\n26\n,\n17\n]\nmodel visuomotor control as conditional trajectory generation and achieve strong performance in complex manipulation tasks.\nHowever, their reliance on iterative denoising leads to high inference latency and sensitivity to execution noise, limiting real-time deployment.\nAcceleration strategies such as DDIM\n[\n21\n]\nor hierarchical designs like RDP\n[\n28\n]\nalleviate but do not eliminate the need for multi-step inference, leaving the recursive nature of diffusion a fundamental bottleneck.\nFlow-based generative models\n[\n16\n,\n5\n]\nprovide a more direct alternative by learning vector fields that define probability paths via ODEs.\nFlow Matching enables fast inference by directly regressing velocity fields, and recent consistency-based variants\n[\n34\n,\n7\n,\n2\n]\nfurther reduce sampling steps.\nDespite their efficiency, flow-based policies often exhibit trajectory oscillations when applied directly in raw action spaces, especially under high-dimensional actions and complex visual inputs.\nOur work builds on consistency flow matching while addressing this limitation through trajectory-level latent modeling and geometry-aware conditioning.\nII-B\nVisual Imitation Learning with 3D Representations\nEarly visual imitation learning methods rely primarily on 2D image observations\n[\n6\n,\n9\n]\n, processed by convolutional networks\n[\n13\n]\nor vision transformers\n[\n8\n]\n.\nRecent 3D-based approaches\n[\n32\n,\n31\n,\n25\n,\n12\n]\nincorporate depth or point clouds and improve geometric reasoning, but commonly face two challenges.\nFirst, prediction-and-planning formulations\n[\n27\n,\n33\n]\nestimate 3D keyframes or target poses, introducing additional optimization at test time and scaling poorly to high-dimensional control.\nSecond, efficient point cloud encoding remains difficult: PointNet-style architectures\n[\n19\n]\ncan be computationally expensive\n[\n25\n]\n, while simplified encoders\n[\n32\n,\n31\n]\ntrade geometric sensitivity for speed.\nIn contrast, our work follows a policy-centric design that directly outputs continuous actions, integrating structured 3D geometric representations while maintaining real-time-capable inference.\nII-C\nTrajectory-Level Modeling and Dexterous Manipulation\nDexterous manipulation has traditionally been addressed using reinforcement learning or task-specific pipelines\n[\n23\n,\n33\n]\n, often requiring extensive training or carefully engineered structures.\nRecent imitation learning methods model action sequences or chunks\n[\n6\n,\n34\n]\n, but may still suffer from temporal inconsistency under long-horizon, high-dimensional control.\nTrajectory-level modeling improves long-horizon coherence and robustness to noisy or suboptimal demonstrations, with latent action representation playing a central role.\nDiscrete latents via VQ-VAE\n[\n24\n]\nare widely adopted\n[\n1\n,\n3\n,\n4\n]\nto enforce information bottlenecks, but may limit expressiveness.\nRecent studies show that appropriately structured continuous latent representations can achieve strong performance without sacrificing stability\n[\n11\n,\n15\n]\n.\nFor example, CoMo\n[\n29\n]\nmitigates shortcut learning through temporal difference modeling, and LAOM\n[\n18\n]\ndemonstrates the benefits of higher-capacity continuous bottlenecks.\nThese results motivate trajectory-level continuous latent action modeling as an effective mechanism for generating smooth and temporally coherent dexterous behaviors, which we adopt in this work.\nIII\nMETHODS\nIII-A\nCoherent Latent Action Representation\nLong-horizon robotic manipulation requires action trajectories that are both expressive and execution-stable.\nApplying flow-based generative models directly in the raw action space often amplifies high-frequency noise during trajectory integration,\nleading to jittery and unreliable behaviors.\nTo address this issue at its source, we introduce a\ntrajectory-level continuous latent action representation\nthat explicitly regularizes the geometry of the generation space before flow matching is applied.\nThe overall architecture is illustrated in Fig.\n2\n.\nTrajectory-Level Latent Modeling\nInstead of modeling individual control commands, we encode short-horizon action segments into latent variables that represent motion-level abstractions.\nGiven an action trajectory\nùêÄ\nt\n:\nt\n+\nH\n‚àí\n1\n‚àà\n‚Ñù\nH\n√ó\nd\na\n\\mathbf{A}_{t:t+H-1}\\in\\mathbb{R}^{H\\times d_{a}}\n, we divide it into\nK\nK\nconsecutive chunks\n{\nùêÄ\n(\n1\n)\n,\n‚Ä¶\n,\nùêÄ\n(\nK\n)\n}\n\\{\\mathbf{A}^{(1)},\\ldots,\\mathbf{A}^{(K)}\\}\n, each spanning\nc\nc\ntime steps.\nEach chunk is mapped to a compact feature embedding and processed by a recurrent encoder with gated memory.\nSpecifically, we employ a GRU-based encoder to aggregate action chunks over time,\nùê°\nk\n=\nGRU\n‚Äã\n(\nùê±\nk\n,\nùê°\nk\n‚àí\n1\n)\n,\n\\mathbf{h}_{k}=\\mathrm{GRU}(\\mathbf{x}_{k},\\mathbf{h}_{k-1}),\n(1)\nwhere\nùê±\nk\n\\mathbf{x}_{k}\ndenotes the feature embedding of the\nk\nk\n-th action chunk, obtained via a lightweight temporal convolution.\nThe recurrent structure plays a critical role in shaping the latent space.\nBy conditioning each latent code on the entire history of preceding action segments, the GRU introduces a strong temporal inductive bias\nthat suppresses high-frequency variations and enforces trajectory-level coherence.\nAs a result, the latent space forms a smooth trajectory manifold rather than a collection of independent step-level representations,\nwhich is particularly beneficial for stabilizing downstream ODE-based flow matching.\nEmpirically, we observe that replacing the recurrent encoder with non-recurrent, per-chunk independent encoders\nleads to noticeably increased trajectory jitter.\nFigure 2:\nConditional variational autoencoder for latent action modeling.\nA GRU encoder enforces temporal coherence in the latent space, while an MLP decoder with FiLM-conditioned wrist-camera features enables visually adaptive action execution.\nVariational Regularization\nTo further regularize the latent action space, we adopt a variational formulation.\nThe KL divergence between the posterior and a standard normal prior encourages compactness and continuity of the latent distribution,\nimproving robustness to noisy or suboptimal demonstrations.\nImportantly, this regularization operates at the trajectory level rather than the control-step level,\nmaking it well suited for long-horizon manipulation.\nVisual-Conditioned Action Decoding\nThe proposed decoding mechanism is designed to flexibly incorporate execution-time sensory information without interfering with latent trajectory generation.\nIn principle, when available, diverse modalities (e.g., vision, touch, and language) can be incorporated into the decoder through FiLM, enabling flexible multimodal conditioning while preserving coherent latent trajectory dynamics.\nSince the decoder operates at low computational cost, incorporating additional conditioning signals at this stage allows the policy to leverage richer information without sacrificing real-time responsiveness.\nIn this work, we instantiate this design using wrist-mounted camera observations in real-world experiments.\nSpecifically, images captured at execution time are processed by a lightweight visual encoder, and the resulting features are used to modulate the decoder through FiLM layers.\nGiven a latent action code\nùê≥\n‚Äã\nk\n\\mathbf{z}k\n, the decoder reconstructs the corresponding action chunk as\nùêÄ\n^\n(\nk\n)\n=\nf\n‚Äã\nŒ∏\n‚Äã\n(\nùê≥\nk\n‚à£\nùêØ\nt\n)\n,\n\\hat{\\mathbf{A}}^{(k)}=f\\theta(\\mathbf{z}_{k}\\mid\\mathbf{v}_{t}),\n(2)\nwhere\nùêØ\nt\n\\mathbf{v}_{t}\ndenotes the encoded visual feature.\nBy confining perceptual conditioning to the decoding stage, the system cleanly separates latent trajectory planning from execution-time adaptation,\nallowing low-level action realization to react to scene variations while maintaining the smoothness and temporal coherence of the latent plan.\nTraining Objective\nThe latent action representation is trained using a standard variational objective with a reconstruction term and a KL regularizer,\naugmented by a lightweight smoothness constraint between consecutive latent codes.\nWhile the smoothness term serves as an auxiliary regularizer,\nthe primary source of temporal coherence arises from the recurrent encoding structure itself.\nOverall, the proposed representation defines a low-dimensional, continuous, and temporally regularized latent action space.\nBy reshaping the generation manifold prior to flow matching, it lays the foundation for stable and efficient\nlong-horizon trajectory generation, which we describe next.\nIII-B\nLatent-Space Action Generation via Flow Matching\nGiven a sequence of latent action codes\n{\nùê≥\n1\n,\n‚Ä¶\n,\nùê≥\nK\n}\n\\{\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{K}\\}\nobtained from the trajectory-level encoder,\nwe generate long-horizon manipulation behaviors by modeling their evolution in continuous time using flow matching.\nUnlike diffusion-based policies that rely on iterative denoising, flow matching enables efficient one-step generation.\nHowever, when applied directly in the raw action space, flow-based models are highly sensitive to local inconsistencies and noise.\nWe therefore perform flow matching exclusively in the regularized latent action space, which is explicitly designed to be smooth and temporally coherent\n(Sec.\nIII-A\n).\nLatent-Space Flow Dynamics\nLet\nùê≥\n\\mathbf{z}\ndenote a latent action code sampled from the target latent distribution and\nùê≥\n~\n\\tilde{\\mathbf{z}}\na corresponding sample from a simple base distribution.\nWe learn a time-dependent vector field\nùùÇ\nŒ∏\n‚Äã\n(\nt\n,\nùê≥\n)\n\\boldsymbol{\\nu}_{\\theta}(t,\\mathbf{z})\nthat defines a continuous probability path from the source to the target distribution,\ngoverned by the ordinary differential equation\nd\n‚Äã\nŒæ\nùê≥\n‚Äã\n(\nt\n)\nd\n‚Äã\nt\n=\nùùÇ\nŒ∏\n‚Äã\n(\nt\n,\nŒæ\nùê≥\n‚Äã\n(\nt\n)\n)\n,\nŒæ\nùê≥\n‚Äã\n(\n0\n)\n=\nùê≥\n~\n.\n\\frac{d\\xi_{\\mathbf{z}}(t)}{dt}=\\boldsymbol{\\nu}_{\\theta}(t,\\xi_{\\mathbf{z}}(t)),\\quad\\xi_{\\mathbf{z}}(0)=\\tilde{\\mathbf{z}}.\n(3)\nOperating on motion-level latent representations rather than raw control signals substantially improves numerical stability,\nas the smooth geometry of the latent space suppresses the amplification of high-frequency variations during trajectory generation.\nConsistency Flow Matching in Latent Space\nTo efficiently learn the latent vector field across noise levels, we adopt consistency flow matching (CFM) and define the flow function\nf\nŒ∏\n‚Äã\n(\nt\n,\nùê≥\n)\n=\nùê≥\n+\n(\n1\n‚àí\nt\n)\n‚Äã\nùùÇ\nŒ∏\n‚Äã\n(\nt\n,\nùê≥\n)\n,\nf_{\\theta}(t,\\mathbf{z})=\\mathbf{z}+(1-t)\\,\\boldsymbol{\\nu}_{\\theta}(t,\\mathbf{z}),\n(4)\nwhere\nt\n‚àà\n[\n0\n,\n1\n]\nt\\in[0,1]\ninterpolates between the source and target latent distributions.\nHere,\nùùÇ\nŒ∏\n‚Äã\n(\nt\n,\nùê≥\n)\n\\boldsymbol{\\nu}_{\\theta}(t,\\mathbf{z})\nrepresents a learned velocity field in the latent action space, and the factor\n(\n1\n‚àí\nt\n)\n(1-t)\nattenuates update magnitudes as the trajectory approaches the target distribution, improving stability near convergence.\nWe further apply a time-dependent input normalization\nc\nin\n‚Äã\n(\nt\n)\n=\n1\nt\n2\n+\n(\n1\n‚àí\nt\n)\n2\n,\nc_{\\mathrm{in}}(t)=\\frac{1}{\\sqrt{t^{2}+(1-t)^{2}}},\n(5)\nto balance latent state scales across the time horizon.\nOne-Step Latent Trajectory Generation\nAt inference time, latent action codes are sampled from the base distribution and transformed via a single evaluation of the learned flow:\nùê≥\n^\nk\n=\nf\nŒ∏\n‚Äã\n(\n1\n,\nùê≥\n~\nk\n)\n.\n\\hat{\\mathbf{z}}_{k}=f_{\\theta}(1,\\tilde{\\mathbf{z}}_{k}).\n(6)\nIn contrast to diffusion-based policies that require iterative denoising through multiple inference steps,\nour approach learns an explicit latent-space flow field during training and therefore performs single-step integration at test time.\nThis enables one-shot generation of the entire latent action trajectory without iterative sampling or numerical integration loops,\nsubstantially reducing inference latency.\nThe resulting latent trajectory preserves temporal coherence by construction and is decoded into executable action segments using the\nvisual-conditioned decoder described in Sec.\nIII-A\n.\nBy decoupling trajectory generation from execution-level adaptation, the policy achieves both fast inference and stable, smooth manipulation behavior.\nIII-C\nGeometry-Aware 3D Scene Conditioning\nStable execution of latent action trajectories requires accurate awareness of scene geometry,\nas geometric errors can propagate across multiple decoded action segments over long horizons.\nWe therefore condition the latent flow policy on a compact, geometry-aware 3D scene representation\nderived from point clouds.\nPoint Cloud Encoding\nScene point clouds are reconstructed from depth images captured by a fixed global camera,\nfollowed by workspace cropping to remove irrelevant background.\nFarthest Point Sampling (FPS) is applied to select a sparse set of center points that uniformly cover the workspace,\nserving as anchors for local neighborhood construction.\nTo capture geometric information at multiple spatial scales, we adopt a dual-branch encoder consisting of\na local encoder\nf\nl\nf_{l}\nand a center encoder\nf\nc\nf_{c}\n, whose overall architecture is illustrated in Fig.\n3\n.\nThe local encoder aggregates relative point offsets within each neighborhood using a residual convolutional backbone\nwith combined max‚Äìmean pooling, focusing on contact-level geometry that is invariant to global translation.\nIn parallel, the center encoder maps the sampled center coordinates to a compact global context vector\nvia a lightweight MLP, capturing coarse scene layout with minimal computational overhead:\nùêü\nl\n=\nf\nl\n‚Äã\n(\npcd\nlocal\n)\n,\nùêü\nc\n=\nf\nc\n‚Äã\n(\npcd\ncenter\n)\n.\n\\mathbf{f}_{l}=f_{l}(\\text{pcd}_{\\text{local}}),\\quad\\mathbf{f}_{c}=f_{c}(\\text{pcd}_{\\text{center}}).\n(7)\nFigure 3:\nGeometry-aware point cloud encoder.\nLocal neighborhoods around farthest-sampled centers capture translation-invariant local geometry via residual convolutions and max‚Äìmean pooling, while a lightweight center encoder provides compact global scene context for latent trajectory conditioning.\nHierarchical Geometric Conditioning\nThe extracted geometric features are injected into the latent flow network through a two-stage\nfeature-wise linear modulation (FiLM) mechanism.\nGiven an intermediate activation\nùê°\n\\mathbf{h}\n, local geometric modulation is applied first,\nfollowed by center-level conditioning:\nùê°\n‚Ä≤\n\\displaystyle\\mathbf{h}^{\\prime}\n=\nùú∏\nl\n‚äô\nùê°\n+\nùú∑\nl\n,\n[\nùú∏\nl\n,\nùú∑\nl\n]\n=\nMLP\nl\n‚Äã\n(\nùêü\nl\n)\n,\n\\displaystyle=\\boldsymbol{\\gamma}_{l}\\odot\\mathbf{h}+\\boldsymbol{\\beta}_{l},\\quad[\\boldsymbol{\\gamma}_{l},\\boldsymbol{\\beta}_{l}]=\\mathrm{MLP}_{l}(\\mathbf{f}_{l}),\n(8)\nùê°\n‚Ä≤‚Ä≤\n\\displaystyle\\mathbf{h}^{\\prime\\prime}\n=\nùú∏\nc\n‚äô\nùê°\n‚Ä≤\n+\nùú∑\nc\n,\n[\nùú∏\nc\n,\nùú∑\nc\n]\n=\nMLP\nc\n‚Äã\n(\nùêü\nc\n)\n.\n\\displaystyle=\\boldsymbol{\\gamma}_{c}\\odot\\mathbf{h}^{\\prime}+\\boldsymbol{\\beta}_{c},\\quad[\\boldsymbol{\\gamma}_{c},\\boldsymbol{\\beta}_{c}]=\\mathrm{MLP}_{c}(\\mathbf{f}_{c}).\n(9)\nApplying local modulation prior to center-level conditioning allows the policy to first adapt to\ncontact-level constraints and subsequently adjust according to global scene structure,\nwhich we find to be more effective than the reverse ordering.\nOverall, this geometry-aware conditioning provides the latent action model with robust spatial context,\nenabling stable long-horizon manipulation without compromising inference efficiency.\nIV\nExperiments\nIV-A\nEvaluation Metrics\nWe evaluate all methods using three complementary metrics that jointly characterize task performance, real-time efficiency, and execution quality: task success rate, response time, and trajectory smoothness.\nTask Success Rate.\nTask success rate is defined as the percentage of trials that satisfy task-specific success criteria within the episode horizon.\nSuccess is determined by predefined geometric constraints, including object pose errors below threshold and stable grasps or placements maintained for a minimum duration.\nResponse Time.\nResponse time measures the end-to-end inference latency from receiving sensory observations to outputting control actions, reflecting the real-time computational efficiency and closed-loop responsiveness of each policy.\nTrajectory Smoothness.\nTo quantify long-horizon execution stability, we define a trajectory smoothness metric that captures both time-domain and frequency-domain characteristics of executed joint trajectories.\nSpecifically, smoothness is computed as a weighted combination of discrete-time jerk and high-frequency spectral energy.\nGiven a sequence of joint actions\n{\nùêö\nt\n}\nt\n=\n1\nT\n\\{\\mathbf{a}_{t}\\}_{t=1}^{T}\nwith control timestep\nŒî\n‚Äã\nt\n\\Delta t\n, jerk is approximated using a third-order finite difference:\nùê£\nt\n=\nùêö\nt\n+\n1\n‚àí\n3\n‚Äã\nùêö\nt\n+\n3\n‚Äã\nùêö\nt\n‚àí\n1\n‚àí\nùêö\nt\n‚àí\n2\n(\nŒî\n‚Äã\nt\n)\n3\n.\n\\mathbf{j}_{t}=\\frac{\\mathbf{a}_{t+1}-3\\mathbf{a}_{t}+3\\mathbf{a}_{t-1}-\\mathbf{a}_{t-2}}{(\\Delta t)^{3}}.\n(10)\nThe time-domain smoothness term is defined as the mean squared jerk magnitude:\nùíÆ\njerk\n=\n1\nT\n‚àí\n2\n‚Äã\n‚àë\nt\n=\n3\nT\n‚Äñ\nùê£\nt\n‚Äñ\n2\n2\n.\n\\mathcal{S}_{\\mathrm{jerk}}=\\frac{1}{T-2}\\sum_{t=3}^{T}\\|\\mathbf{j}_{t}\\|_{2}^{2}.\n(11)\nTo further capture high-frequency oscillations not fully reflected by jerk alone, we compute the frequency-domain energy of each joint trajectory using the discrete Fourier transform (DFT).\nLet\nùêö\n^\nf\n\\hat{\\mathbf{a}}_{f}\ndenote the Fourier coefficients of\n{\nùêö\nt\n}\n\\{\\mathbf{a}_{t}\\}\n.\nThe high-frequency energy ratio is defined as\nùíÆ\nfreq\n=\n‚àë\nf\n>\nf\nc\n‚Äñ\nùêö\n^\nf\n‚Äñ\n2\n2\n‚àë\nf\n‚Äñ\nùêö\n^\nf\n‚Äñ\n2\n2\n,\n\\mathcal{S}_{\\mathrm{freq}}=\\frac{\\sum_{f>f_{c}}\\|\\hat{\\mathbf{a}}_{f}\\|_{2}^{2}}{\\sum_{f}\\|\\hat{\\mathbf{a}}_{f}\\|_{2}^{2}},\n(12)\nwhere\nf\nc\nf_{c}\nis a cutoff frequency separating low- and high-frequency components.\nThe final trajectory smoothness metric is given by\nùíÆ\nsmooth\n=\nŒ±\n‚Äã\nùíÆ\njerk\n+\nŒ≤\n‚Äã\nùíÆ\nfreq\n,\n\\mathcal{S}_{\\mathrm{smooth}}=\\alpha\\,\\mathcal{S}_{\\mathrm{jerk}}+\\beta\\,\\mathcal{S}_{\\mathrm{freq}},\n(13)\nwhere lower values indicate smoother trajectories with reduced abrupt variations and high-frequency oscillations.\nBased on extensive empirical evaluation and normalization of the two terms, we set\nŒ±\n=\n0.25\n\\alpha=0.25\nand\nŒ≤\n=\n0.75\n\\beta=0.75\nin all experiments.\nTABLE I:\nMain results for simulation experiments.\nSuccess rates (%) are reported as mean\n¬±\n\\pm\nstandard deviation.\nAvg. Success rate averages results over Adroit and MetaWorld.\nŒî\n\\Delta\nSuccess /\nŒî\n\\Delta\nTime denote relative changes w.r.t. DP3 (Baseline) in average success rate and per-step inference latency.\nMethods\nSuccess rate in Adroit\nSuccess rate in MetaWorld\nAvg. Success rate\nùö´\n\\boldsymbol{\\Delta}\nSuccess\nAvg. Time\nùö´\n\\boldsymbol{\\Delta}\nTime\nDiffusion Policy\n26.0\n¬±\n4.0\n26.0\\pm 4.0\n37.3\n¬±\n2.3\n37.3\\pm 2.3\n31.7\n¬±\n6.9\n31.7\\pm 6.9\n‚àí\n34.0\n-34.0\n35.7\n‚Äã\nms\n35.7\\text{ ms}\n‚àí\n20.2\n-20.2\nFlow Policy\n56.7\n¬±\n2.3\n56.7\\pm 2.3\n65.3\n¬±\n2.3\n65.3\\pm 2.3\n61.0\n¬±\n5.2\n61.0\\pm 5.2\n‚àí\n4.7\n-4.7\n6.1\n‚Äã\nms\n\\mathbf{6.1\\text{ ms}}\n‚àí\n49.8\n\\mathbf{-49.8}\nDP3 (Baseline)\n62.0\n¬±\n2.0\n62.0\\pm 2.0\n69.3\n¬±\n3.1\n69.3\\pm 3.1\n65.7\n¬±\n4.6\n65.7\\pm 4.6\n0.0\n0.0\n55.9\n‚Äã\nms\n55.9\\text{ ms}\n0.0\n0.0\niDP3\n62.7\n¬±\n2.3\n62.7\\pm 2.3\n71.3\n¬±\n2.3\n71.3\\pm 2.3\n67.0\n¬±\n5.2\n67.0\\pm 5.2\n+\n1.3\n+1.3\n58.9\n‚Äã\nms\n58.9\\text{ ms}\n+\n3.0\n+3.0\nRDP\n70.0\n¬±\n2.0\n70.0\\pm 2.0\n76.0\n¬±\n2.0\n76.0\\pm 2.0\n73.0\n¬±\n3.7\n73.0\\pm 3.7\n+\n7.3\n+7.3\n57.3\n‚Äã\nms\n57.3\\text{ ms}\n+\n1.4\n+1.4\nLG-Flow Policy (Ours)\n76.0\n¬±\n2.0\n\\mathbf{76.0\\pm 2.0}\n80.7\n¬±\n3.1\n\\mathbf{80.7\\pm 3.1}\n78.3\n¬±\n3.4\n\\mathbf{78.3\\pm 3.4}\n+\n12.6\n\\mathbf{+12.6}\n7.5\n‚Äã\nms\n7.5\\text{ ms}\n‚àí\n48.4\n-48.4\nIV-B\nSimulation Experiments\nIV-B\n1\nSimulation Setup\nSimulation Benchmarks.\nWe evaluate all methods on two widely used manipulation benchmarks, Adroit\n[\n20\n]\nand Meta-World\n[\n30\n]\n, covering 37 tasks in total.\nAdroit focuses on contact-rich dexterous manipulation with a high-dimensional articulated hand, while Meta-World consists of diverse arm-based manipulation tasks with varying difficulty.\nBoth benchmarks are built upon the MuJoCo\n[\n22\n]\nphysics simulator.\nFor both benchmarks, expert demonstrations are collected using well-tuned heuristic policies to ensure consistent data quality.\nFigure 4:\nTrajectory smoothness comparison across simulated manipulation tasks. Lower values indicate smoother execution.\nBaselines.\nWe compare our method against representative diffusion- and flow-based visuomotor policies.\nDP3\n[\n32\n]\nserves as the primary point-cloud-based diffusion baseline, with iDP3\n[\n31\n]\nand Reactive Diffusion Policy (RDP\n[\n28\n]\n) included as advanced variants.\nDue to environment constraints, RDP is evaluated without force or pressure sensing, and employs a standard VAE as a latent-space encoder on top of DP.\nWe further include a 2D RGB-based Diffusion Policy\n[\n6\n]\nand a standard Flow Policy\n[\n34\n]\nto contrast 3D conditioning and latent-space modeling.\nTraining and Implementation Details.\nAll methods condition on the two most recent observations and predict actions for the subsequent four steps.\nFor each task, 30 expert demonstrations are collected, and performance is evaluated over 50 rollouts.\nRGB observations are cropped to\n84\n√ó\n84\n84\\times 84\n, and point clouds are downsampled using farthest point sampling to 512 points.\nModels are trained for 150 epochs using AdamW with a learning rate of\n1\n√ó\n10\n‚àí\n4\n1\\times 10^{-4}\n, batch size 96, and EMA with decay rate 0.95.\nState and action inputs are normalized to\n[\n‚àí\n1\n,\n1\n]\n[-1,1]\n, with actions unnormalized before execution.\nAll experiments are conducted on identical hardware with an Intel Core i7-14700KF CPU and an NVIDIA RTX 4090D GPU.\nIn simulation, no additional execution-time sensory signals are provided to the decoder, and action decoding relies solely on the latent action representation.\nFigure 5:\nReal-world experimental setup and observations.\nLeft: Franka Emika Panda robot with a LEAP Hand and the visual sensing setup (global L515 and wrist-mounted D435).\nRight: Example multimodal observations (RGB images, point clouds, wrist views) and task objects used in real-world experiments.\nIV-B\n2\nSimulation Results Analysis\nSimulation results are evaluated from three perspectives: trajectory smoothness, inference latency and task success.\nTrajectory Smoothness.\nFig.\n4\nreports trajectory smoothness across all simulated tasks.\nLG-Flow Policy achieves the lowest average smoothness score of\n0.052\n0.052\n, reducing the metric by\n51.4\n%\n51.4\\%\n,\n69.4\n%\n69.4\\%\n, and\n77.2\n%\n77.2\\%\nrelative to RDP, DP3, and Flow Policy, respectively.\nRDP exhibits lower smoothness than other diffusion-based baselines, indicating that latent action representations partially improve trajectory stability.\nHowever, its trajectories remain less smooth than those of LG-Flow Policy, as RDP does not impose explicit temporal smoothness constraints in the latent space.\nBy contrast, LG-Flow Policy performs flow matching over temporally coherent latent trajectories, consistently suppressing high-frequency oscillations.\nTask Success Rate and Response Time.\nTable\nI\nsummarizes task success rates and inference latency across simulated benchmarks.\nLG-Flow Policy achieves the highest average success rate of\n78.3\n%\n78.3\\%\n, outperforming Flow Policy (\n+\n17.3\n%\n+17.3\\%\n) and DP3 (\n+\n12.6\n%\n+12.6\\%\n), and improving over RDP (\n73.0\n%\n73.0\\%\n) by\n5.3\n%\n5.3\\%\n.\nWhile Flow Policy attains the lowest latency (6.1‚Äâms), LG-Flow Policy incurs only a marginal increase (7.5‚Äâms) and remains in the same real-time regime.\nNotably, LG-Flow Policy is approximately\n7.5\n√ó\n7.5\\times\nfaster than DP3 (55.9‚Äâms), demonstrating a favorable success‚Äìlatency trade-off with minimal loss in responsiveness.\nOverall, simulation results show that LG-Flow Policy achieves a favorable trade-off among smooth execution, fast inference, and high task success, which is essential for real-time robotic manipulation.\nIV-C\nReal-World Experiments\nIV-C\n1\nReal-World Experimental Setup\nPlatform and Sensing.\nReal-world experiments are conducted on a Franka Emika Panda arm with an interchangeable end-effector configuration,\nwhere the LEAP Hand can be replaced by a parallel gripper depending on the task (Fig.\n5\n, left).\nA RealSense L515 global camera provides scene-level RGB-D observations, while a wrist-mounted RealSense D435 supplies local visual feedback.\nBoth cameras operate at\n640\n√ó\n480\n640\\times 480\nresolution and 30‚Äâfps.\nDepth images are converted to point clouds using calibrated intrinsics and extrinsics.\nIn contrast to simulation, RGB images from the wrist-mounted camera are additionally used as execution-time conditioning inputs to the decoder,\nallowing low-level action realization to adapt to local visual variations during real-world execution.\nTasks.\nWe evaluate three real-world manipulation tasks:\npick-and-place\n, performed with both the LEAP Hand and a parallel gripper to cover different manipulation modalities;\npeg-in-hole\n, which requires precise insertion; and\nobstacle-avoidance grasping\n, where a target object must be grasped while avoiding surrounding obstacles.\nRepresentative objects and environments are shown in Fig.\n5\n(top right).\nDemonstrations and Baselines.\nExpert demonstrations are collected via human teleoperation, with 30 demonstrations per task.\nWe compare LG-Flow Policy against DP3\n[\n32\n]\n, Reactive Diffusion Policy (RDP)\n[\n28\n]\n, and Flow Policy\n[\n34\n]\n.\nFor RDP, tactile inputs are replaced with wrist camera images to ensure a fair comparison under visual-only settings.\nEvaluation Protocol.\nEach task is evaluated over 3 independent runs with different random seeds, each consisting of 10 trials (30 trials per task).\nSuccess rates are reported as mean\n¬±\n\\pm\nstandard deviation.\nResponse time measures end-to-end policy inference latency, excluding sensor acquisition and environment stepping.\nAll experiments are conducted on identical hardware with an Intel Core i9-14900KF CPU and an NVIDIA RTX 4080 GPU.\nIV-C\n2\nReal-World Results Analysis\nQuantitative real-world results are summarized in Table\nII\nand Fig.\n6\n, evaluating trajectory smoothness, task success, and response time.\nTABLE II:\nMain results for real-world robot experiments.\nSuccess rates (%) are reported as mean\n¬±\n\\pm\nstandard deviation over 30 trials.\nAvg. Success denotes the average success rate across all tasks.\nAvg. Response Time measures end-to-end policy inference latency per control step.\nMethods\nPick & Place\nPick & Place (Gripper)\nPeg-in-Hole\nObstacle Avoidance\nAvg. Success\nAvg. Response Time\nDP3\n66.7\n¬±\n5.8\n66.7\\pm 5.8\n80.0\n¬±\n5.8\n80.0\\pm 5.8\n35.0\n¬±\n5.8\n35.0\\pm 5.8\n50.0\n¬±\n10.0\n50.0\\pm 10.0\n57.9\n57.9\n29.29\n‚Äã\nms\n29.29\\text{ ms}\nRDP\n73.3\n¬±\n5.8\n73.3\\pm 5.8\n93.3\n¬±\n5.8\n93.3\\pm 5.8\n50.0\n¬±\n0.0\n50.0\\pm 0.0\n63.3\n¬±\n5.8\n63.3\\pm 5.8\n70.0\n70.0\n31.82\n‚Äã\nms\n31.82\\text{ ms}\nFlow Policy\n60.0\n¬±\n0.0\n60.0\\pm 0.0\n73.3\n¬±\n5.8\n73.3\\pm 5.8\n30.0\n¬±\n10.0\n30.0\\pm 10.0\n46.7\n¬±\n5.8\n46.7\\pm 5.8\n52.5\n52.5\n6.54\n‚Äã\nms\n\\mathbf{6.54\\text{ ms}}\nLG-Flow Policy (Ours)\n83.3\n¬±\n5.8\n\\mathbf{83.3\\pm 5.8}\n96.7\n¬±\n5.8\n\\mathbf{96.7\\pm 5.8}\n60.0\n¬±\n0.0\n\\mathbf{60.0\\pm 0.0}\n70.0\n¬±\n5.8\n\\mathbf{70.0\\pm 5.8}\n77.5\n\\mathbf{77.5}\n8.59\n‚Äã\nms\n8.59\\text{ ms}\nTrajectory Smoothness.\nFigure 6:\nTrajectory smoothness comparison across real-world manipulation tasks. Lower values indicate smoother execution.\nAs shown in Fig.\n6\n, LG-Flow Policy achieves the lowest mean and maximum smoothness scores among all methods, indicating substantially reduced jerk and high-frequency oscillations.\nCompared to Flow Policy, the mean smoothness metric is reduced by over\n93.7\n%\n93.7\\%\n, demonstrating strong robustness under real-world sensing noise and actuation delays.\nConsistent with simulation results, RDP exhibits lower smoothness than other diffusion-based baselines, suggesting that latent action representations improve execution stability in physical settings.\nHowever, its trajectories remain less smooth than those of LG-Flow Policy, as RDP does not impose explicit temporal smoothness constraints in the latent space.\nThis effect is further illustrated in Fig.\n7\n, where Flow Policy produces pronounced high-frequency oscillations, while LG-Flow Policy yields smooth and temporally coherent joint motions under identical initial conditions.\nFigure 7:\nComparison of real-world joint trajectories under identical initial conditions.\nFlow Policy exhibits high-frequency oscillations and abrupt variations, while LG-Flow Policy generates smoother and more temporally coherent trajectories, consistent with the quantitative smoothness metrics.\nTask Success Rate and Response Time.\nTable\nII\nreports task success rates and inference latency in real-world experiments.\nLG-Flow Policy achieves the highest average success rate of\n77.5\n%\n77.5\\%\n, outperforming DP3 (\n57.9\n%\n57.9\\%\n), RDP (\n70.0\n%\n70.0\\%\n), and Flow Policy (\n52.5\n%\n52.5\\%\n).\nWhile Flow Policy attains the lowest latency (6.54‚Äâms), its unstable trajectories often lead to execution failures.\nLG-Flow Policy incurs only a modest increase in response time (8.59‚Äâms) while achieving substantially higher task success.\nOne contributing factor to the improved success of both RDP and LG-Flow Policy is the incorporation of wrist-mounted camera observations during action decoding, providing execution-time visual feedback for precise manipulation.\nLG-Flow Policy remains over\n3\n√ó\n3\\times\nfaster than DP3 and RDP, indicating that significant real-world performance gains are achieved at minimal computational cost.\nOverall, real-world results confirm that LG-Flow Policy strikes a robust balance between inference efficiency, trajectory smoothness, and task success, supporting its suitability for stable real-world deployment.\nIV-D\nAblation Study\nFigure 8:\nAblation study on trajectory smoothness and task success rate in real world manipulation tasks.\nFig.\n8\nsummarizes ablation results in both simulation and real-world settings, analyzing the contributions of geometry-aware perception (GE-Flow) and latent action modeling (L-Flow).\nAcross both domains, the two components exhibit complementary effects on task success and trajectory smoothness.\nIn simulation, geometry-aware encoding improves average success from\n61\n%\n61\\%\nto\n64.3\n%\n64.3\\%\nbut slightly degrades trajectory smoothness, indicating that enhanced spatial perception alone does not resolve temporal instability.\nIn contrast, latent action modeling substantially reduces the smoothness metric (\n0.228\n‚Üí\n0.046\n0.228\\rightarrow 0.046\n) while increasing success to\n72.4\n%\n72.4\\%\n.\nCombining both components further improves success to\n78.3\n%\n78.3\\%\nwhile maintaining low oscillations.\nA similar trend is observed in real-world experiments.\nWhile geometry-aware perception alone yields only marginal success gains (\n52.5\n%\n‚Üí\n54.2\n%\n52.5\\%\\rightarrow 54.2\\%\n), it noticeably increases trajectory oscillations, often leading to execution failures under real-world noise and actuation delays.\nLatent action modeling, by contrast, improves both metrics, increasing success to\n71.7\n%\n71.7\\%\nand reducing smoothness by over\n95\n%\n95\\%\n.\nCombining both components achieves the best real-world performance, reaching a success rate of\n77.5\n%\n77.5\\%\nwith minimal oscillations.\nOverall, these results indicate that geometry-aware perception enhances spatial understanding, whereas stable manipulation critically relies on enforcing temporal coherence through latent action modeling, particularly in real-world settings.\nV\nConclusion\nThis work addresses real-time robotic manipulation under long-horizon and high-dimensional control,\nwhere existing generative policies struggle to jointly achieve expressive behavior modeling,\nfast inference, and stable execution, limiting their deployment on physical systems.\nWe propose\nLG-Flow Policy\n, a trajectory-level action generation framework that performs flow matching\nin a continuous latent action space.\nBy mapping action sequences into a temporally regularized latent trajectory manifold and learning an explicit\nlatent-space flow, LG-Flow Policy enables near single-step inference while substantially improving execution\nsmoothness and stability.\nOperating on motion-level latent representations decouples global trajectory structure from low-level control noise,\nmitigating error amplification during generation and leading to more reliable long-horizon behaviors.\nLG-Flow Policy further integrates geometry-aware point cloud conditioning and execution-time decoding modulation,\nallowing additional sensory signals to be incorporated without perturbing latent trajectory dynamics.\nThis design enables richer multimodal execution while preserving fast response, as demonstrated by visual-conditioned\ndecoding with a wrist-mounted camera in real-world experiments.\nExtensive simulation and real-robot results show smooth trajectories, high success rates, and rapid inference,\noutperforming both diffusion-based and raw action-space flow-based baselines.\nDespite its effectiveness, the current system adopts relatively lightweight multimodal fusion and does not\nexplicitly model complex contact dynamics.\nFuture work will explore richer multimodal representations and contact-aware latent dynamics to extend\nLG-Flow Policy toward more challenging contact-rich manipulation.\nReferences\n[1]\nJ. Bjorck, F. Casta√±eda, N. Cherniadev, X. Da, R. Ding, L. Fan, Y. Fang, D. Fox, F. Hu, S. Huang,\net al.\n(2025)\nGr00t n1: an open foundation model for generalist humanoid robots\n.\narXiv preprint arXiv:2503.14734\n.\nCited by:\n¬ß\nII-C\n.\n[2]\nM. Braun, N. Jaquier, L. Rozo, and T. Asfour\nRiemannian flow matching policy for robot motion learning. in 2024 ieee\n.\nIn\nRSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.¬†5144‚Äì5151\n.\nCited by:\n¬ßI\n,\n¬ß\nII-A\n.\n[3]\nJ. Bruce, M. D. Dennis, A. Edwards, J. Parker-Holder, Y. Shi, E. Hughes, M. Lai, A. Mavalankar, R. Steigerwald, C. Apps,\net al.\n(2024)\nGenie: generative interactive environments\n.\nIn\nForty-first International Conference on Machine Learning\n,\nCited by:\n¬ß\nII-C\n.\n[4]\nQ. Bu, J. Cai, L. Chen, X. Cui, Y. Ding, S. Feng, X. He, X. Huang,\net al.\n(2025)\nAgibot world colosseo: a large-scale manipulation platform for scalable and intelligent embodied systems\n.\nIn\n2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nCited by:\n¬ß\nII-C\n.\n[5]\nR. T. Chen and Y. Lipman\n(2023)\nFlow matching on general geometries\n.\narXiv preprint arXiv:2302.03660\n.\nCited by:\n¬ßI\n,\n¬ß\nII-A\n.\n[6]\nC. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song\n(2025)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nThe International Journal of Robotics Research\n44\n(\n10-11\n),\npp.¬†1684‚Äì1704\n.\nCited by:\n¬ßI\n,\n¬ß\nII-A\n,\n¬ß\nII-B\n,\n¬ß\nII-C\n,\n¬ß\nIV-B\n1\n.\n[7]\nH. Ding, N. Jaquier, J. Peters, and L. Rozo\n(2025)\nFast and robust visuomotor riemannian flow matching policy\n.\nIEEE Transactions on robotics\n.\nCited by:\n¬ßI\n,\n¬ß\nII-A\n.\n[8]\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby\n(2021)\nAn image is worth 16x16 words: transformers for image recognition at scale\n.\nICLR\n.\nCited by:\n¬ß\nII-B\n.\n[9]\nP. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch, and J. Tompson\n(2022)\nImplicit behavioral cloning\n.\nIn\nConference on robot learning\n,\npp.¬†158‚Äì168\n.\nCited by:\n¬ß\nII-B\n.\n[10]\nS. Fotiadis, N. D. Brenowitz, T. Geffner, Y. Cohen, M. Pritchard, A. Vahdat, and M. Mardani\nAdaptive flow matching for resolving small-scale physics\n.\nIn\nForty-second International Conference on Machine Learning\n,\nCited by:\n¬ßI\n.\n[11]\nS. Gao, S. Zhou, Y. Du, J. Zhang, and C. Gan\n(2025)\nAdaWorld: learning adaptable world models with latent actions\n.\nIn\nInternational Conference on Machine Learning (ICML)\n,\nCited by:\n¬ß\nII-C\n.\n[12]\nA. Goyal, J. Xu, Y. Guo, V. Blukis, Y. Chao, and D. Fox\n(2023)\nRvt: robotic view transformer for 3d object manipulation\n.\nIn\nConference on Robot Learning\n,\npp.¬†694‚Äì710\n.\nCited by:\n¬ß\nII-B\n.\n[13]\nK. He, X. Zhang, S. Ren, and J. Sun\n(2016)\nDeep residual learning for image recognition\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.¬†770‚Äì778\n.\nCited by:\n¬ß\nII-B\n.\n[14]\nJ. Ho, A. Jain, and P. Abbeel\n(2020)\nDenoising diffusion probabilistic models\n.\nAdvances in neural information processing systems\n33\n,\npp.¬†6840‚Äì6851\n.\nCited by:\n¬ß\nII-A\n.\n[15]\nA. Liang, P. Czempin, M. Hong, Y. Zhou, E. Biyik, and S. Tu\n(2025)\nClam: continuous latent action models for robot learning from unlabeled demonstrations\n.\narXiv preprint arXiv:2505.04999\n.\nCited by:\n¬ß\nII-C\n.\n[16]\nY. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le\n(2022)\nFlow matching for generative modeling\n.\narXiv preprint arXiv:2210.02747\n.\nCited by:\n¬ßI\n,\n¬ß\nII-A\n.\n[17]\nZ. Ni, Y. He, L. Qian, J. Mao, F. Fu, W. Sui, H. Su, J. Peng, Z. Wang, and B. He\n(2025)\nVO-dp: semantic-geometric adaptive diffusion policy for vision-only robotic manipulation\n.\narXiv preprint arXiv:2510.15530\n.\nCited by:\n¬ßI\n,\n¬ß\nII-A\n.\n[18]\nA. Nikulin, I. Zisman, D. Tarasov, N. Lyubaykin, A. Polubarov, I. Kiselev, and V. Kurenkov\n(2025)\nLatent action learning requires supervision in the presence of distractors\n.\nIn\nInternational Conference on Machine Learning (ICML)\n,\nCited by:\n¬ß\nII-C\n.\n[19]\nC. R. Qi, H. Su, K. Mo, and L. J. Guibas\n(2017)\nPointnet: deep learning on point sets for 3d classification and segmentation\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.¬†652‚Äì660\n.\nCited by:\n¬ß\nII-B\n.\n[20]\nA. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine\n(2017)\nLearning complex dexterous manipulation with deep reinforcement learning and demonstrations\n.\narXiv preprint arXiv:1709.10087\n.\nCited by:\n¬ß\nIV-B\n1\n.\n[21]\nJ. Song, C. Meng, and S. Ermon\n(2020)\nDenoising diffusion implicit models\n.\narXiv preprint arXiv:2010.02502\n.\nCited by:\n¬ß\nII-A\n.\n[22]\nE. Todorov, T. Erez, and Y. Tassa\n(2012)\nMujoco: a physics engine for model-based control\n.\nIn\n2012 IEEE/RSJ international conference on intelligent robots and systems\n,\npp.¬†5026‚Äì5033\n.\nCited by:\n¬ß\nIV-B\n1\n.\n[23]\nD. Turpin, T. Zhong, S. Zhang, G. Zhu, E. Heiden, M. Macklin, S. Tsogkas, S. Dickinson, and A. Garg\n(2023)\nFast-grasp‚Äôd: dexterous multi-finger grasp generation through differentiable simulation\n.\nIn\nICRA\n,\nCited by:\n¬ß\nII-C\n.\n[24]\nA. Van Den Oord, O. Vinyals,\net al.\n(2017)\nNeural discrete representation learning\n.\nAdvances in neural information processing systems\n30\n.\nCited by:\n¬ß\nII-C\n.\n[25]\nC. Wang, H. Shi, W. Wang, R. Zhang, L. Fei-Fei, and C. K. Liu\n(2024)\nDexCap: scalable and portable mocap data collection system for dexterous manipulation\n.\narXiv preprint arXiv:2403.07788\n.\nCited by:\n¬ßI\n,\n¬ß\nII-B\n,\n¬ß\nII-B\n.\n[26]\nD. Wang, C. Liu, F. Chang, and Y. Xu\n(2025)\nHierarchical diffusion policy: manipulation trajectory generation via contact guidance\n.\nIEEE Transactions on Robotics\n.\nCited by:\n¬ßI\n,\n¬ß\nII-A\n.\n[27]\nR. Wang, J. Zhang, J. Chen, Y. Xu, P. Li, T. Liu, and H. Wang\n(2022)\nDexgraspnet: a large-scale robotic dexterous grasp dataset for general objects based on simulation\n.\narXiv preprint arXiv:2210.02697\n.\nCited by:\n¬ß\nII-B\n.\n[28]\nH. Xue, J. Ren, W. Chen, G. Zhang, Y. Fang, G. Gu, H. Xu, and C. Lu\n(2025)\nReactive diffusion policy: slow-fast visual-tactile policy learning for contact-rich manipulation\n.\nIn\nProceedings of Robotics: Science and Systems (RSS)\n,\nCited by:\n¬ßI\n,\n¬ß\nII-A\n,\n¬ß\nIV-B\n1\n,\n¬ß\nIV-C\n1\n.\n[29]\nJ. Yang, Y. Shi, H. Zhu, M. Liu, K. Ma, Y. Wang, G. Wu, T. He, and L. Wang\n(2025)\nCoMo: learning continuous latent motion from internet videos for scalable robot learning\n.\narXiv preprint arXiv:2505.17006\n.\nCited by:\n¬ß\nII-C\n.\n[30]\nT. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine\n(2020)\nMeta-world: a benchmark and evaluation for multi-task and meta reinforcement learning\n.\nIn\nConference on robot learning\n,\npp.¬†1094‚Äì1100\n.\nCited by:\n¬ß\nIV-B\n1\n.\n[31]\nY. Ze, Z. Chen, W. Wang, T. Chen, X. He, Y. Yuan, X. B. Peng, and J. Wu\n(2025)\nGeneralizable humanoid manipulation with 3d diffusion policies\n.\nIn\n2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.¬†2873‚Äì2880\n.\nCited by:\n¬ßI\n,\n¬ß\nII-A\n,\n¬ß\nII-B\n,\n¬ß\nII-B\n,\n¬ß\nIV-B\n1\n.\n[32]\nY. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, and H. Xu\n(2024)\n3D diffusion policy: generalizable visuomotor policy learning via simple 3d representations\n.\nIn\nProceedings of Robotics: Science and Systems (RSS)\n,\nCited by:\n¬ßI\n,\n¬ßI\n,\n¬ß\nII-A\n,\n¬ß\nII-B\n,\n¬ß\nII-B\n,\n¬ß\nIV-B\n1\n,\n¬ß\nIV-C\n1\n.\n[33]\nJ. Zhang, H. Liu, D. Li, X. Yu, H. Geng, Y. Ding, J. Chen, and H. Wang\n(2024)\nDexgraspnet 2.0: learning generative dexterous grasping in large-scale synthetic cluttered scenes\n.\nIn\n8th Annual Conference on Robot Learning\n,\nCited by:\n¬ß\nII-B\n,\n¬ß\nII-C\n.\n[34]\nQ. Zhang, Z. Liu, H. Fan, G. Liu, B. Zeng, and S. Liu\n(2025)\nFlowpolicy: enabling fast and robust 3d flow-based policy via consistency flow matching for robot manipulation\n.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n,\nVol.\n39\n,\npp.¬†14754‚Äì14762\n.\nCited by:\n¬ßI\n,\n¬ßI\n,\n¬ß\nII-A\n,\n¬ß\nII-C\n,\n¬ß\nIV-B\n1\n,\n¬ß\nIV-C\n1\n.",
    "preview_text": "Learning long-horizon robotic manipulation requires jointly achieving expressive behavior modeling, real-time inference, and stable execution, which remains challenging for existing generative policies. Diffusion-based approaches provide strong modeling capacity but typically incur high inference latency, while flow matching enables fast one-step generation yet often leads to unstable execution when applied directly in the raw action space.\n  We propose LG-Flow Policy, a trajectory-level imitation learning framework that performs flow matching in a continuous latent action space. By encoding action sequences into temporally regularized latent trajectories and learning an explicit latent-space flow, the proposed approach decouples global motion structure from low-level control noise, resulting in smooth and reliable long-horizon execution.\n  LG-Flow Policy further incorporates geometry-aware point cloud conditioning and execution-time multimodal modulation, with visual cues evaluated as a representative modality in real-world settings. Experimental results in simulation and on physical robot platforms demonstrate that LG-Flow Policy achieves near single-step inference, substantially improves trajectory smoothness and task success over flow-based baselines operating in the raw action space, and remains significantly more efficient than diffusion-based policies.\n\nTemporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation\nWu Songwei, Jiang Zhiduo , Xie Guanghu, Liu Yang\n‚Ä†\n, Liu Hong\n‚Ä†\nCorresponding author: Liu Yang(liuyanghit@hit.edu.cn).*This work was supported by the Natural Science Foundation of Hei longjiang Province for Excellent Young Scholars (Grant No. YQ2024E018)\nand the Youth Talent Support Program of the China (Grant No. 2022-JCJQQT-061).\nAll authors are with with the State Key Laboratory of Robotics and Systems,\nHarbin Institute of Technology, Harbin 150001, Heilongjiang, China\nAbstract\nLearning long-horizon robotic manip",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "Flow Matching",
        "Inference Acceleration",
        "fine tune",
        "whole body control",
        "locomotion",
        "Reinforcement Learning",
        "VLA",
        "VLM",
        "Vision-Language-Action Model",
        "VLA for Robotics",
        "Diffusion",
        "Edge Deployment",
        "offline Reinforcement Learning",
        "constrained reinforcement learning",
        "safe reinforcement learning",
        "goal conditioned reinforcement learning",
        "world model"
    ],
    "one_line_summary": "ËØ•ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊΩúÂú®Âä®‰ΩúÊµÅÂåπÈÖçÁöÑÊ®°‰ªøÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰ΩúÔºåÈÄöËøáÊΩúÂú®Á©∫Èó¥Âª∫Ê®°ÂÆûÁé∞Âø´ÈÄüÊé®ÁêÜÂíåÁ®≥ÂÆöÊâßË°åÔºå‰∏éÊµÅÂåπÈÖç„ÄÅÊé®ÁêÜÂä†ÈÄüÂíåÊú∫Âô®‰∫∫ÊéßÂà∂Áõ∏ÂÖ≥Ôºå‰ΩÜ‰∏éËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂíåÂº∫ÂåñÂ≠¶‰π†Â≠êÈ¢ÜÂüüÂÖ≥ËÅîËæÉÂº±„ÄÇ",
    "detailed_summary": "## ËÆ∫ÊñáÊëòË¶ÅÔºöÂü∫‰∫éÊΩúÂú®Âä®‰ΩúÊµÅÂåπÈÖçÁöÑÊó∂Â∫è‰∏ÄËá¥ÊÄßÊ®°‰ªøÂ≠¶‰π†Áî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú\n\n### 1. Á†îÁ©∂ËÉåÊôØÂíåÂä®Êú∫\nÂú®Êú∫Âô®‰∫∫Ê®°‰ªøÂ≠¶‰π†‰∏≠ÔºåÁé∞ÊúâÁîüÊàêÂºèÁ≠ñÁï•Èöæ‰ª•ÂêåÊó∂ÂÆûÁé∞**È´òË°®ËææËÉΩÂäõ„ÄÅÂø´ÈÄüÊé®ÁêÜÂíåÁ®≥ÂÆöÊâßË°å**„ÄÇÂü∫‰∫éÊâ©Êï£ÁöÑÁ≠ñÁï•Âª∫Ê®°ËÉΩÂäõÂº∫Ôºå‰ΩÜÊé®ÁêÜÂª∂ËøüÈ´òÔºõËÄåÊµÅÂåπÈÖçÊñπÊ≥ïËôΩËÉΩÂÆûÁé∞Âø´ÈÄü‰∏ÄÊ≠•ÁîüÊàêÔºå‰ΩÜÂú®ÂéüÂßãÂä®‰ΩúÁ©∫Èó¥‰∏≠Áõ¥Êé•Êìç‰ΩúÊó∂ÔºåÂæÄÂæÄÂõ†Âô™Â£∞Á¥ØÁßØÂØºËá¥ËΩ®ËøπÊäñÂä®ÂíåÊâßË°å‰∏çÁ®≥ÂÆö„ÄÇÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Ëøô‰∏ÄÊ†∏ÂøÉÁüõÁõæÔºåÊèêÂá∫‰∏ÄÁßçËÉΩÂ§üÂÖºÈ°æ**Âø´ÈÄüÊé®ÁêÜ‰∏éÂπ≥ÊªëÁ®≥ÂÆöÊâßË°å**ÁöÑÊ°ÜÊû∂„ÄÇ\n\n### 2. Ê†∏ÂøÉÊñπÊ≥ïÂíåÊäÄÊúØÂàõÊñ∞\nÊú¨ÊñáÊèêÂá∫‰∫Ü **LG-Flow Policy**ÔºåÂÖ∂Ê†∏ÂøÉÂàõÊñ∞Âú®‰∫é**Âú®ËøûÁª≠ÊΩúÂú®Âä®‰ΩúÁ©∫Èó¥‰∏≠ËøõË°åÊµÅÂåπÈÖç**„ÄÇ‰∏ªË¶ÅÊäÄÊúØË¥°ÁåÆÂåÖÊã¨Ôºö\n- **Êó∂Â∫è‰∏ÄËá¥ÁöÑÊΩúÂú®Âä®‰ΩúË°®Á§∫**Ôºö‰ΩøÁî®Âü∫‰∫éGRUÁöÑÁºñÁ†ÅÂô®Â∞ÜÂä®‰ΩúÂ∫èÂàóÁºñÁ†Å‰∏∫**ËΩ®ËøπÁ∫ßÁöÑËøûÁª≠ÊΩúÂú®Ë°®Á§∫**ÔºåÈÄöËøáÂæ™ÁéØÁªìÊûÑÂíåÂèòÂàÜÊ≠£ÂàôÂåñÔºåÂú®ÊΩúÂú®Á©∫Èó¥‰∏≠Âº∫Âà∂ÊñΩÂä†Êó∂Â∫è‰∏ÄËá¥ÊÄßÔºåËß£ËÄ¶ÂÖ®Â±ÄËøêÂä®ÁªìÊûÑ‰∏éÂ∫ïÂ±ÇÊéßÂà∂Âô™Â£∞„ÄÇ\n- **ÊΩúÂú®Á©∫Èó¥ÊµÅÂåπÈÖç**ÔºöÂú®Âπ≥ÊªëÁöÑÊΩúÂú®Âä®‰ΩúÁ©∫Èó¥ÔºàËÄåÈùûÂéüÂßãÈ´òÁª¥Âä®‰ΩúÁ©∫Èó¥Ôºâ‰∏≠Â≠¶‰π†ÊòæÂºèÁöÑÊµÅÂú∫ÔºåÈÄöËøá**‰∏ÄËá¥ÊÄßÊµÅÂåπÈÖç**ÂÆûÁé∞Ëøë‰πé‰∏ÄÊ≠•ÁöÑÂø´ÈÄüÊé®ÁêÜÔºåÂêåÊó∂ÈÅøÂÖç‰∫ÜÊï∞ÂÄºËØØÂ∑ÆÂú®ÈïøÊó∂Á®ã‰∏≠ÁöÑÊîæÂ§ß„ÄÇ\n- **Â§öÊ®°ÊÄÅÊÑüÁü•‰∏éË∞ÉÂà∂**ÔºöÈááÁî®**Âá†‰ΩïÊÑüÁü•ÁöÑÁÇπ‰∫ëÁºñÁ†ÅÂô®**ÔºàÂèåÂàÜÊîØÂ±ÄÈÉ®-ÂÖ®Â±ÄÁâπÂæÅÊèêÂèñÔºâËøõË°åÂú∫ÊôØÊù°‰ª∂ËæìÂÖ•ÔºõÂπ∂Âú®Ëß£Á†ÅÈò∂ÊÆµÈÄöËøá**FiLMÊú∫Âà∂**ÁÅµÊ¥ªËûçÂÖ•ÊâßË°åÊó∂ÁöÑÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÔºàÂ¶ÇËÖïÈÉ®Áõ∏Êú∫ËßÜËßâÂèçÈ¶àÔºâÔºåÂÆûÁé∞ÊÑüÁü•Ëá™ÈÄÇÂ∫îËÄå‰∏çÂπ≤Êâ∞ÊΩúÂú®ËΩ®ËøπÁöÑÂä®ÊÄÅ„ÄÇ\n\n### 3. ‰∏ªË¶ÅÂÆûÈ™åÁªìÊûú\nÂú®‰ªøÁúüÔºàAdroit, Meta-WorldÔºâÂíåÁúüÂÆûÊú∫Âô®‰∫∫ÔºàFranka Panda + LEAP HandÔºâ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºö\n- **Êé®ÁêÜÊïàÁéá**ÔºöÂπ≥ÂùáÂìçÂ∫îÊó∂Èó¥‰ªÖÁ∫¶ **7.5 ms**ÔºåÊØîÊâ©Êï£Á≠ñÁï•ÔºàDP3ÔºâÂø´Á∫¶ **7.5ÂÄç**ÔºåÊé•ËøëÂéüÂßãÊµÅÂåπÈÖçÁ≠ñÁï•ÁöÑÈÄüÂ∫¶„ÄÇ\n- **ÊâßË°åÂπ≥ÊªëÊÄß**ÔºöËΩ®ËøπÂπ≥ÊªëÂ∫¶ÊåáÊ†áÁõ∏ÊØîÂéüÂßãÂä®‰ΩúÁ©∫Èó¥ÊµÅÂü∫Á∫øÊèêÂçáÊúÄÈ´òËææ **93.7%**ÔºåÊúâÊïàÊäëÂà∂‰∫ÜÈ´òÈ¢ëÊäñÂä®„ÄÇ\n- **‰ªªÂä°ÊàêÂäüÁéá**ÔºöÂú®‰ªøÁúüÂíåÁúüÂÆû‰ªªÂä°‰∏≠ÔºåÂπ≥ÂùáÊàêÂäüÁéáÂàÜÂà´ËææÂà∞ **78.3%** Âíå **77.5%**ÔºåÊòæËëó‰ºò‰∫éÊâ©Êï£Âü∫Á∫øÂíåÂéüÂßãÊµÅÂåπÈÖçÂü∫Á∫øÔºàÊèêÂçáÊúÄÈ´òËææ **25%**Ôºâ„ÄÇ\n- **Ê∂àËûçÂÆûÈ™å**ËØÅÂÆû‰∫Ü**ÊΩúÂú®Âä®‰ΩúÂª∫Ê®°**ÂØπÊèêÂçáÂπ≥ÊªëÂ∫¶ÂíåÊàêÂäüÁéáÁöÑÂÖ≥ÈîÆ‰ΩúÁî®Ôºå‰ª•Âèä**Âá†‰ΩïÊÑüÁü•**‰∏éÊΩúÂú®Âª∫Ê®°ÁöÑ‰∫íË°•‰ª∑ÂÄº„ÄÇ\n\n### 4. Á†îÁ©∂ÊÑè‰πâÂíå‰ª∑ÂÄº\nÊú¨Â∑•‰Ωú‰∏∫**ÂÆûÊó∂„ÄÅÈïøÊó∂Á®ã„ÄÅÈ´òÁª¥Â∫¶ÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú**Êèê‰æõ‰∫Ü‰∏Ä‰∏™È´òÊïàÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÂÆÉÊàêÂäüÂú∞Â∞Ü**ÊµÅÂåπÈÖçÁöÑÊé®ÁêÜÈÄüÂ∫¶**‰∏é**ÊΩúÂú®Á©∫Èó¥Âª∫Ê®°ÁöÑÁ®≥ÂÆöÊÄß**Áõ∏ÁªìÂêàÔºåÊâìÁ†¥‰∫ÜÁîüÊàêÂºèÁ≠ñÁï•Âú®ÊïàÁéá‰∏éÁ®≥ÂÆöÊÄß‰πãÈó¥ÁöÑÊùÉË°°„ÄÇÊâÄÊèêÂá∫ÁöÑÊ°ÜÊû∂Êòì‰∫éÊâ©Â±ïÂ§öÊ®°ÊÄÅÊÑüÁü•Ôºå‰∏∫Âú®Áâ©ÁêÜÁ≥ªÁªü‰∏äÈÉ®ÁΩ≤Á®≥ÂÅ•„ÄÅËá™ÈÄÇÂ∫îÁöÑÊ®°‰ªøÂ≠¶‰π†Á≠ñÁï•Â•†ÂÆö‰∫ÜÈáçË¶ÅÂü∫Á°ÄÔºåÊé®Âä®‰∫ÜÁîüÊàêÂºèÊú∫Âô®‰∫∫Á≠ñÁï•ÂêëÂÆûÈôÖÂ∫îÁî®ËøàËøõ„ÄÇ",
    "qa_pairs": [
        {
            "question": "ËøôÁØáËÆ∫ÊñáÁöÑÊ†∏ÂøÉÂàõÊñ∞ÁÇπÊòØ‰ªÄ‰πàÔºå‰ªñÊÉ≥Ëß£ÂÜ≥‰ªÄ‰πàÈóÆÈ¢òÔºåÊÄé‰πàËß£ÂÜ≥ÁöÑÔºü",
            "answer": "## ËÆ∫ÊñáÊ†∏ÂøÉÂàÜÊûê\n\n### **‰∏Ä„ÄÅ Á†îÁ©∂ÈóÆÈ¢ò**\nËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥**Êú∫Âô®‰∫∫Ê®°‰ªøÂ≠¶‰π†‰∏≠ÁîüÊàêÂºèÁ≠ñÁï•Èù¢‰∏¥ÁöÑÊ†∏ÂøÉÊùÉË°°ÈóÆÈ¢ò**Ôºö\n1.  **Êé®ÁêÜÊïàÁéá‰∏éÂª∫Ê®°ËÉΩÂäõ**ÔºöÂü∫‰∫éÊâ©Êï£ÁöÑÁ≠ñÁï•Âª∫Ê®°ËÉΩÂäõÂº∫Ôºå‰ΩÜËø≠‰ª£ÂéªÂô™ÂØºËá¥Êé®ÁêÜÂª∂ËøüÈ´òÔºåÈöæ‰ª•ÂÆûÊó∂ÈÉ®ÁΩ≤„ÄÇ\n2.  **Êé®ÁêÜÈÄüÂ∫¶‰∏éÊâßË°åÁ®≥ÂÆöÊÄß**ÔºöÂü∫‰∫éÊµÅÂåπÈÖçÁöÑÁ≠ñÁï•ÂèØÂÆûÁé∞ÂçïÊ≠•Âø´ÈÄüÊé®ÁêÜÔºå‰ΩÜÁõ¥Êé•Âú®ÂéüÂßãÂä®‰ΩúÁ©∫Èó¥ÁîüÊàêËΩ®ËøπÊó∂ÔºåÂô™Â£∞ÂíåËØØÂ∑Æ‰ºöÁ¥ØÁßØÔºåÂØºËá¥**ËΩ®ËøπÊäñÂä®„ÄÅÊâßË°å‰∏çÁ®≥ÂÆö**ÔºåÂ∞§ÂÖ∂Âú®ÈïøÊó∂Á®ã„ÄÅÈ´òÁª¥Êìç‰Ωú‰ªªÂä°‰∏≠„ÄÇ\n\nÁé∞ÊúâÊñπÊ≥ïÔºàÂ¶ÇÂä†ÈÄüÊâ©Êï£„ÄÅÂàÜÂ±ÇËÆæËÆ°„ÄÅ‰∏ÄËá¥ÊÄßÁ∫¶ÊùüÔºâÊú™ËÉΩ‰ªéÊ†πÊú¨‰∏äËß£ÂÜ≥Ëøô‰∏ÄÊùÉË°°ÔºåÈôêÂà∂‰∫ÜÁîüÊàêÂºèÁ≠ñÁï•Âú®Áâ©ÁêÜÊú∫Âô®‰∫∫‰∏äÁöÑÁ®≥ÂÆö„ÄÅÂÆûÊó∂ÈÉ®ÁΩ≤„ÄÇ\n\n### **‰∫å„ÄÅ Ê†∏ÂøÉÂàõÊñ∞ÁÇπ**\nËÆ∫ÊñáÊèêÂá∫‰∫Ü **‚ÄúLG-Flow Policy‚Äù** Ê°ÜÊû∂ÔºåÂÖ∂Ê†∏ÂøÉÂàõÊñ∞Âú®‰∫é**Âú®ËøûÁª≠ÊΩúÂú®Âä®‰ΩúÁ©∫Èó¥‰∏≠ÊâßË°åËΩ®ËøπÁ∫ßÊµÅÂåπÈÖç**Ôºå‰ªéËÄåÂ∞ÜÂø´ÈÄüÊé®ÁêÜ‰∏éÂπ≥ÊªëÁ®≥ÂÆöÁöÑÊâßË°åÁªü‰∏ÄËµ∑Êù•„ÄÇ\n\nÂÖ∑‰ΩìÂàõÊñ∞ÁÇπÂàÜËß£Â¶Ç‰∏ãÔºö\n\n- **1. ËΩ®ËøπÁ∫ßËøûÁª≠ÊΩúÂú®Âä®‰ΩúË°®Á§∫**\n    - **ÊÄùË∑Ø**Ôºö‰∏çÁõ¥Êé•Âª∫Ê®°Âçï‰∏™ÊéßÂà∂ÂëΩ‰ª§ÔºåËÄåÊòØÂ∞ÜÁü≠Êó∂Á®ãÁöÑÂä®‰ΩúÁâáÊÆµÁºñÁ†Å‰∏∫Ë°®Á§∫**ËøêÂä®Á∫ßÊäΩË±°**ÁöÑÊΩúÂú®ÂèòÈáè„ÄÇ\n    - **ÂÖ≥ÈîÆËÆæËÆ°**Ôºö\n        - **GRUÁºñÁ†ÅÂô®**ÔºöÂØπÂä®‰ΩúÂùóËøõË°åÊó∂Â∫èÁºñÁ†ÅÔºåÂºïÂÖ•Âº∫Êó∂Â∫èÂΩíÁ∫≥ÂÅèÁΩÆÔºåÊäëÂà∂È´òÈ¢ëÂèòÂåñÔºåÂº∫Âà∂ËΩ®ËøπÁ∫ßËøûË¥ØÊÄßÔºåÂΩ¢ÊàêÂπ≥ÊªëÁöÑÊΩúÂú®ËΩ®ËøπÊµÅÂΩ¢„ÄÇ\n        - **ÂèòÂàÜÊ≠£ÂàôÂåñ**ÔºöÂú®ËΩ®ËøπÁ∫ßÂà´ÔºàËÄåÈùûÊéßÂà∂Ê≠•Á∫ßÂà´Ôºâ‰ΩøÁî®KLÊï£Â∫¶Ê≠£ÂàôÂåñÔºåÈºìÂä±ÊΩúÂú®ÂàÜÂ∏ÉÁöÑÁ¥ßÂáëÊÄßÂíåËøûÁª≠ÊÄßÔºåÊèêÂçáÂØπÂô™Â£∞ÊàñÊ¨°‰ºòÊºîÁ§∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ\n    - **ÊïàÊûú**Ôºö‰ªéÊ†πÊú¨‰∏äÈáçÂ°ë‰∫ÜÁîüÊàêÁ©∫Èó¥Ôºå‰∏∫ÂêéÁª≠Á®≥ÂÆöÁöÑÊµÅÂåπÈÖçÂ•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇ\n\n- **2. ÊΩúÂú®Á©∫Èó¥ÊµÅÂåπÈÖçÁîüÊàê**\n    - **ÊÄùË∑Ø**ÔºöÂú®**‰∏äËø∞Ê≠£ÂàôÂåñÂêéÁöÑÂπ≥ÊªëÊΩúÂú®Á©∫Èó¥**‰∏≠Â≠¶‰π†ÊòæÂºèÁöÑÊµÅÔºàÂêëÈáèÂú∫ÔºâÔºåËÄåÈùûÂú®ÂéüÂßãÈ´òÁª¥„ÄÅÂòàÊùÇÁöÑÂä®‰ΩúÁ©∫Èó¥‰∏≠„ÄÇ\n    - **ÂÖ≥ÈîÆËÆæËÆ°**Ôºö\n        - ÈááÁî®**‰∏ÄËá¥ÊÄßÊµÅÂåπÈÖç**ÔºåÂ≠¶‰π†ÂÆö‰πâÊ¶ÇÁéáË∑ØÂæÑÁöÑÂêëÈáèÂú∫„ÄÇ\n        - Êé®ÁêÜÊó∂ÔºåÈÄöËøáÂØπÂ≠¶‰π†Âà∞ÁöÑÊµÅÂú∫ËøõË°å**ÂçïÊ≠•ËØÑ‰º∞**ÔºåÂç≥ÂèØ‰ªéÁÆÄÂçïÂü∫ÂàÜÂ∏ÉÁîüÊàêËøûË¥ØÁöÑÊΩúÂú®ËΩ®Ëøπ„ÄÇ\n    - **ÊïàÊûú**ÔºöÂÆûÁé∞‰∫Ü**Êé•ËøëÂçïÊ≠•ÁöÑÂø´ÈÄüÊé®ÁêÜ**ÔºåÂêåÊó∂Áî±‰∫éÊΩúÂú®Á©∫Èó¥ÁöÑÂπ≥ÊªëÁâπÊÄßÔºåÂú®ËΩ®ËøπÁîüÊàêËøáÁ®ã‰∏≠ÊäëÂà∂‰∫ÜËØØÂ∑ÆÊîæÂ§ßÂíåÈ´òÈ¢ëÊåØËç°„ÄÇ\n\n- **3. Ëß£ËÄ¶ÁöÑÊÑüÁü•‰∏éÊâßË°åÊû∂ÊûÑ**\n    - **ÊÄùË∑Ø**ÔºöÂ∞Ü**ËΩ®ËøπËßÑÂàíÔºàÊΩúÂú®Á©∫Èó¥Ôºâ** ‰∏é**ÊâßË°åÊó∂ÈÄÇÂ∫îÔºàËß£Á†ÅÈò∂ÊÆµÔºâ** Ê∏ÖÊô∞ÂàÜÁ¶ª„ÄÇ\n    - **ÂÖ≥ÈîÆËÆæËÆ°**Ôºö\n        - **Âá†‰ΩïÊÑüÁü•ÁöÑÁÇπ‰∫ëÊù°‰ª∂Âåñ**ÔºöÈÄöËøá‰∏Ä‰∏™ÂèåÂàÜÊîØÔºàÂ±ÄÈÉ®+ÂÖ®Â±ÄÔºâÁöÑËΩªÈáèÁ∫ßÁÇπ‰∫ëÁºñÁ†ÅÂô®ÔºåÂ∞Ü3DÂá†‰Ωï‰ø°ÊÅØ‰ª•FiLMÊñπÂºèÊ≥®ÂÖ•Âà∞**ÊΩúÂú®ÊµÅÁΩëÁªú**‰∏≠ÔºåÁî®‰∫éËΩ®ËøπËßÑÂàí„ÄÇ\n        - **ÊâßË°åÊó∂Â§öÊ®°ÊÄÅË∞ÉÂà∂**ÔºöÂ∞ÜÊâßË°åÊó∂ÁöÑÊÑüÂÆò‰ø°ÊÅØÔºàÂ¶ÇËÆ∫Êñá‰∏≠È™åËØÅÁöÑËÖïÈÉ®Áõ∏Êú∫ËßÜËßâÁâπÂæÅÔºâ‰ªÖÈÄöËøáFiLMÊ≥®ÂÖ•Âà∞**Âä®‰ΩúËß£Á†ÅÂô®**‰∏≠ÔºåÁî®‰∫é‰ΩéÂ±ÇÂä®‰ΩúÁöÑÂÆûÊó∂ÂæÆË∞É„ÄÇ\n    - **ÊïàÊûú**ÔºöÂÖÅËÆ∏Á≠ñÁï•Âà©Áî®‰∏∞ÂØåÁöÑÁéØÂ¢É‰ø°ÊÅØÔºàÂá†‰Ωï+ËßÜËßâÔºâËøõË°åÈ≤ÅÊ£íËßÑÂàíÔºåÂêåÊó∂‰øùÊåÅÊΩúÂú®ËΩ®ËøπÁöÑÂä®ÊÄÅËøûË¥ØÊÄß‰∏çÂèòÔºåÂÆûÁé∞‰∫Ü**ÊÑüÁü•‰∏∞ÂØåÊÄß**‰∏é**ÊâßË°åÁ®≥ÂÆöÊÄß**ÁöÑÂÖºÂæó„ÄÇ\n\n### **‰∏â„ÄÅ Ëß£ÂÜ≥ÊñπÊ°àÊÄªÁªì**\nËÆ∫ÊñáÈÄöËøá‰∏Ä‰∏™**‰∏âÂ±ÇÁ∫ßËÅîÁöÑÊû∂ÊûÑ**Á≥ªÁªüÊÄßÂú∞Ëß£ÂÜ≥‰∫ÜÈóÆÈ¢òÔºö\n1.  **ÂâçÁ´ØÁºñÁ†Å**ÔºöÂ∞ÜÂéüÂßãÂä®‰ΩúÂ∫èÂàóÈÄöËøá**Êó∂Â∫èGRUÁºñÁ†ÅÂô®**Êò†Â∞ÑÂà∞Âπ≥Êªë„ÄÅËøûÁª≠ÁöÑÊΩúÂú®ËΩ®ËøπÁ©∫Èó¥„ÄÇ\n2.  **‰∏≠Á´ØÁîüÊàê**ÔºöÂú®**Êù°‰ª∂Âåñ**ÔºàÂá†‰ΩïÁÇπ‰∫ëÔºâÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠ÔºåÂà©Áî®**‰∏ÄËá¥ÊÄßÊµÅÂåπÈÖç**ËøõË°åÂø´ÈÄü„ÄÅÂçïÊ≠•ÁöÑËΩ®ËøπÁîüÊàê„ÄÇ\n3.  **ÂêéÁ´ØËß£Á†Å**ÔºöÂ∞ÜÁîüÊàêÁöÑÊΩúÂú®ËΩ®ËøπÈÄöËøá**Êù°‰ª∂Âåñ**ÔºàÊâßË°åÊó∂ËßÜËßâÁ≠âÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÔºâÁöÑËß£Á†ÅÂô®ÔºåÈáçÂª∫‰∏∫ÂèØÊâßË°åÁöÑÂÖ∑‰ΩìÂä®‰ΩúÂ∫èÂàó„ÄÇ\n\n**ÊäÄÊúØÊ†∏ÂøÉ**Âú®‰∫éÔºö**Â∞Ü‚ÄúÊµÅÂåπÈÖç‚ÄùÁöÑÂø´ÈÄüÁîüÊàê‰ºòÂäøÔºå‰∏é‚ÄúÊΩúÂú®Ë°®Á§∫‚ÄùÁöÑÂπ≥ÊªëÊª§Ê≥¢‰ºòÂäøÁõ∏ÁªìÂêà**ÔºåÂπ∂ÈÄöËøá**Êù°‰ª∂Âåñ‰ΩçÁΩÆÁöÑÁ≤æÂøÉËÆæËÆ°**ÔºàÂá†‰Ωï‰ø°ÊÅØÂΩ±ÂìçËßÑÂàíÔºåÁû¨Êó∂ÊÑüÁü•ÂΩ±ÂìçÊâßË°åÔºâÊù•ÂÖºÈ°æÂÖ®Â±ÄÁªìÊûÑ‰∏éÂ±ÄÈÉ®ÈÄÇÂ∫î„ÄÇ\n\n### **Âõõ„ÄÅ ÂÆûÈôÖ‰ª∑ÂÄº**\n- **ÊÄßËÉΩÊèêÂçá**ÔºöÂú®‰ªøÁúüÂíåÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠ÔºåÁõ∏ÊØîÂéüÂßãÂä®‰ΩúÁ©∫Èó¥ÁöÑÊµÅÂåπÈÖçÂü∫Á∫øÔºåËΩ®ËøπÂπ≥ÊªëÂ∫¶ÊèêÂçáÈ´òËææ **93.7%**Ôºå‰ªªÂä°ÊàêÂäüÁéáÊèêÂçáÈ´òËææ **25%**„ÄÇ\n- **ÊïàÁéá‰ºòÂäø**ÔºöÊé®ÁêÜÈÄüÂ∫¶Ôºà~7.5 msÔºâ‰∏éÂø´ÈÄüÊµÅÂåπÈÖçÁ≠ñÁï•Ôºà~6.1 msÔºâÂ§Ñ‰∫éÂêå‰∏ÄÈáèÁ∫ßÔºå‰ΩÜÊØîÊâ©Êï£Á≠ñÁï•Ôºà~55.9 msÔºâÂø´Á∫¶ **7.5ÂÄç**„ÄÇ\n- **ÈÉ®ÁΩ≤ÂèØË°åÊÄß**ÔºöÈ¶ñÊ¨°Âú®ÁîüÊàêÂºèÁ≠ñÁï•‰∏≠**ÂêåÊó∂ÂÆûÁé∞‰∫Ü**Êé•ËøëÂçïÊ≠•ÁöÑÊé®ÁêÜÈÄüÂ∫¶„ÄÅÈ´ò‰ªªÂä°ÊàêÂäüÁéáÂíåÊûÅÈ´òÁöÑËΩ®ËøπÂπ≥ÊªëÂ∫¶Ôºå‰∏∫**È´òÁª¥„ÄÅÈïøÊó∂Á®ãÁöÑÂÆûÊó∂Êú∫Âô®‰∫∫Êìç‰Ωú**Êèê‰æõ‰∫ÜÂàáÂÆûÂèØË°åÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ",
            "timestamp": "2026-02-03T15:54:29.987359",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "Âü∫‰∫é‰ªñÁöÑÂâç‰ΩúÔºåÊ¢≥ÁêÜËøô‰∏™ÊñπÂêëÁöÑÊï¥‰∏™ÂèëÂ±ïËÑâÁªúÔºåÊØè‰∏ÄÊ≠•Áõ∏ÊØî‰∫é‰πãÂâçÁöÑÂ∑•‰ΩúÈÉΩÊîπËøõ‰∫Ü‰ªÄ‰πàÔºåÁùÄÈáç‰∫éÂá†‰∏™‰∏çÂêåÁöÑÂèëÂ±ïÊñπÂêë„ÄÇ",
            "answer": "## Âü∫‰∫éÂâç‰ΩúÁöÑÊú∫Âô®‰∫∫Ê®°‰ªøÂ≠¶‰π†ÂèëÂ±ïËÑâÁªúÊ¢≥ÁêÜ\n\nÊú¨ÊñáÊèêÂá∫ÁöÑ **LG-Flow Policy** ÊòØÊú∫Âô®‰∫∫ËßÜËßâËøêÂä®Ê®°‰ªøÂ≠¶‰π†È¢ÜÂüüÔºåÁâπÂà´ÊòØÂü∫‰∫éÁîüÊàêÊ®°ÂûãÁöÑÁ≠ñÁï•Â≠¶‰π†ÊñπÂêë‰∏äÁöÑÊúÄÊñ∞ËøõÂ±ï„ÄÇÂÖ∂ÂèëÂ±ïËÑâÁªúÂèØ‰ª•Ê∏ÖÊô∞Âú∞Ê¢≥ÁêÜ‰∏∫Âá†‰∏™ÂÖ≥ÈîÆÈò∂ÊÆµÂíå‰∏çÂêåÁöÑÊäÄÊúØÂèëÂ±ïÊñπÂêë„ÄÇ\n\n### ÂèëÂ±ïËÑâÁªúÊÄªËßà\n\nËØ•È¢ÜÂüüÁöÑÂèëÂ±ïÊ†∏ÂøÉÂõ¥ÁªïÁùÄËß£ÂÜ≥ **‚ÄúË°®ËææËÉΩÂäõ-Êé®ÁêÜÊïàÁéá-ÊâßË°åÁ®≥ÂÆöÊÄß‚Äù** Ëøô‰∏Ä‰∏çÂèØËÉΩ‰∏âËßíÂ±ïÂºÄ„ÄÇÊó©ÊúüÂ∑•‰Ωú‰æßÈáç‰∫éË°®ËææËÉΩÂäõÔºåÈöèÂêéÂàÜÂà´ÂêëÊèêÂçáÊïàÁéáÂíåÁ®≥ÂÆöÊÄß‰∏§‰∏™ÊñπÂêëÊºîËøõÔºåËÄåÊú¨ÊñáÁöÑÂ∑•‰ΩúÂàôËØïÂõæÂú®ÊΩúÁ©∫Èó¥‰∏≠Â∞ÜËøô‰∏§‰∏™ÊñπÂêëËøõË°åËûçÂêà‰∏éÁªü‰∏Ä„ÄÇ\n\n```mermaid\ntimeline\n    title Êú∫Âô®‰∫∫ÁîüÊàêÂºèÁ≠ñÁï•ÂèëÂ±ïËÑâÁªú\n    section Ë°®ËææËÉΩÂäõ‰ºòÂÖà\n        Êâ©Êï£Ê®°ÂûãÂ•†Âü∫ : Âü∫‰∫éDDPM/DDIM<br>Âº∫Â§ßË°®ËææËÉΩÂäõ<br>È´òÊé®ÁêÜÂª∂Ëøü\n        3DË°®ÂæÅÂ¢ûÂº∫ : ÂºïÂÖ•ÁÇπ‰∫ëÁ≠â3D‰ø°ÊÅØ<br>ÊèêÂçáÂá†‰ΩïÊÑüÁü•‰∏éÊ≥õÂåñ\n    section ÊïàÁéá‰∏éÁ®≥ÂÆöÊÄßÂàÜÂèâÊºîËøõ\n        ÊµÅÂêëÂåπÈÖçÂºïÂÖ• : Âü∫‰∫éODEÁöÑÂçïÊ≠•Êé®ÁêÜ<br>Â§ßÂπÖÊèêÂçáÊïàÁéá<br>ÂéüÁîüÂä®‰ΩúÁ©∫Èó¥‰∏çÁ®≥ÂÆö\n        ÂàÜÂ±Ç‰∏é‰∏ÄËá¥ÊÄßÊîπËøõ : RDP: Âø´ÊÖ¢Â±ÇÊ¨°ÁªìÊûÑ<br>FlowPolicy: ‰∏ÄËá¥ÊÄßÁ∫¶Êùü<br>ÈÉ®ÂàÜÁºìËß£ÈóÆÈ¢ò\n        ÊΩúÂä®‰ΩúÂª∫Ê®°Êé¢Á¥¢ : RDP/CoMoÁ≠â‰ΩøÁî®ÊΩúÁ©∫Èó¥<br>ÊèêÂçáÊó∂Â∫è‰∏ÄËá¥ÊÄß<br>Â§ö‰∏∫Á¶ªÊï£ÊàñÈùû‰∏ìÈó®ËÆæËÆ°\n    section ËûçÂêà‰∏éÁªü‰∏Ä\n        LG-Flow Policy : ËøûÁª≠Êó∂Â∫èÊ≠£ÂàôÂåñÊΩúÁ©∫Èó¥<br>ÊΩúÁ©∫Èó¥ÊµÅÂåπÈÖç<br>Âá†‰ΩïÊÑüÁü•+Â§öÊ®°ÊÄÅËß£Á†Å<br>ÂÖºÈ°æ‰∏âËÄÖ\n```\n\n### ÂÖ≥ÈîÆÂèëÂ±ïÈò∂ÊÆµ‰∏éÊäÄÊúØÊîπËøõ\n\n#### 1. Â•†Âü∫‰∏éË°®ËææËÉΩÂäõÁ™ÅÁ†¥ÔºöÊâ©Êï£Ê®°Âûã‰∏ªÂØºÊúü\n*   **‰ª£Ë°®Â∑•‰Ωú**Ôºö**Diffusion Policy (DP)** [6], **DP3** [32]\n*   **Ê†∏ÂøÉÊÄùÊÉ≥**ÔºöÂ∞ÜÊú∫Âô®‰∫∫Âä®‰ΩúÂ∫èÂàóÁîüÊàêÂª∫Ê®°‰∏∫Êù°‰ª∂Êâ©Êï£ËøáÁ®ãÔºåÈÄöËøáËø≠‰ª£ÂéªÂô™ÁîüÊàêÂ§öÊ®°ÊÄÅÂä®‰ΩúÂàÜÂ∏É„ÄÇ\n*   **Áõ∏ÊØî‰∫é‰πãÂâçÁöÑRL/BCÊñπÊ≥ï**Ôºö\n    *   **ÊîπËøõ**ÔºöÊûÅÂ§ßÂú∞ÊèêÂçá‰∫ÜÁ≠ñÁï•ÁöÑ**Ë°®ËææËÉΩÂäõ**Âíå**Ê≥õÂåñËÉΩÂäõ**ÔºåËÉΩÂ§ü‰ªéÊºîÁ§∫Êï∞ÊçÆ‰∏≠Â≠¶‰π†Â§çÊùÇÁöÑ„ÄÅÂ§öÊ®°ÊÄÅÁöÑÂä®‰ΩúÂàÜÂ∏ÉÔºåÂú®Â§ÑÁêÜÊé•Ëß¶‰∏∞ÂØå„ÄÅÈïøËßÜÈáé‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ\n    *   **ÈÅóÁïôÈóÆÈ¢ò**Ôºö**Êé®ÁêÜÂª∂ËøüÈ´ò**Ôºå‰æùËµñÂ§öÊ≠•Ëø≠‰ª£ÈááÊ†∑ÔºåÈöæ‰ª•Êª°Ë∂≥ÂÆûÊó∂ÊéßÂà∂Ë¶ÅÊ±ÇÔºõÂØπÊâßË°åÊó∂ÁöÑÂô™Â£∞ÊïèÊÑüÔºåÈïøËßÜÈáé‰ªªÂä°‰∏≠ËØØÂ∑ÆÂÆπÊòìÁ¥ØÁßØ„ÄÇ\n\n#### 2. ÊÑüÁü•ÊïàÁéá‰∏éÊû∂ÊûÑ‰ºòÂåñÔºö3DË°®ÂæÅ‰∏éÂàÜÂ±ÇËÆæËÆ°\n*   **ÂèëÂ±ïÊñπÂêë‰∏ÄÔºöÊÑüÁü•ÊïàÁéáÊèêÂçá**\n    *   **‰ª£Ë°®Â∑•‰Ωú**Ôºö**DP3** [32], **iDP3** [31]\n    *   **Ê†∏ÂøÉÊÄùÊÉ≥**ÔºöÁî®ËΩªÈáèÂåñÁöÑÁÇπ‰∫ëÁºñÁ†ÅÂô®ÔºàÂ¶ÇÁÆÄÂåñÁâàPointNetÔºâÊõø‰ª£Ê≤âÈáçÁöÑ2DÂõæÂÉèÁºñÁ†ÅÂô®ÔºåËæìÂÖ•3DÁÇπ‰∫ë‰Ωú‰∏∫ËßÇÊµã„ÄÇ\n    *   **Áõ∏ÊØî‰∫éÁ∫Ø2D DP**Ôºö\n        *   **ÊîπËøõ**ÔºöÊèê‰æõ‰∫Ü**Âá†‰ΩïÊÑüÁü•**ËÉΩÂäõÔºåÂØπÁâ©‰ΩìÂßøÊÄÅ„ÄÅÁ©∫Èó¥ÂÖ≥Á≥ªÊõ¥È≤ÅÊ£íÔºõÁΩëÁªúÊõ¥ËΩªÈáèÔºå**ÈÉ®ÂàÜÊèêÂçá‰∫ÜÊÑüÁü•ÁºñÁ†ÅÊïàÁéá**„ÄÇ\n        *   **ÈÅóÁïôÈóÆÈ¢ò**ÔºöÊú™Ëß£ÂÜ≥Êâ©Êï£Ê®°ÂûãÂõ∫ÊúâÁöÑ**È´òÊé®ÁêÜÂª∂Ëøü**Ê†πÊú¨ÈóÆÈ¢òÔºõ‰∏∫ËøΩÊ±ÇÈÄüÂ∫¶ÂèØËÉΩÁâ∫Áâ≤‰∫ÜÂá†‰ΩïÁºñÁ†ÅÁöÑÁªÜËäÇ„ÄÇ\n\n*   **ÂèëÂ±ïÊñπÂêë‰∫åÔºöÊéßÂà∂Êû∂ÊûÑÂàÜÂ±Ç**\n    *   **‰ª£Ë°®Â∑•‰Ωú**Ôºö**Reactive Diffusion Policy (RDP)** [28]\n    *   **Ê†∏ÂøÉÊÄùÊÉ≥**ÔºöËÆæËÆ°‚ÄúÂø´-ÊÖ¢‚ÄùÂàÜÂ±ÇÁªìÊûÑ„ÄÇÊÖ¢ÂõûË∑ØÔºàÊâ©Êï£Ê®°ÂûãÔºâÁîüÊàêÁ≤óÁ≤íÂ∫¶ÁöÑÊΩúÂä®‰ΩúËÆ°ÂàíÔºàÂ¶Ç‰ΩøÁî®VAEÔºâÔºåÂø´ÂõûË∑ØÔºàËΩªÈáèÁΩëÁªúÔºâÊ†πÊçÆÂÆûÊó∂‰º†ÊÑüÔºàÂ¶ÇËß¶ËßâÔºâËøõË°åÂèçÂ∫îÂºèË∞ÉÊï¥„ÄÇ\n    *   **Áõ∏ÊØî‰∫éÂçïÂ±ÇDP**Ôºö\n        *   **ÊîπËøõ**ÔºöÈÄöËøáÂ∞ÜÈïøËßÜÈáéËßÑÂàí‰∏éÁû¨Êó∂ÂèçÂ∫îËß£ËÄ¶Ôºå**ÊèêÂçá‰∫ÜÈïø‰ªªÂä°ÊâßË°åÁöÑÈ≤ÅÊ£íÊÄß**ÂíåÂØπÂÆûÊó∂Âπ≤Êâ∞ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ**È¶ñÊ¨°ÊòéÁ°ÆÂºïÂÖ•‰∫ÜÊΩúÂä®‰ΩúÁ©∫Èó¥**Áî®‰∫éÈ´òÂ±ÇËßÑÂàí„ÄÇ\n        *   **ÈÅóÁïôÈóÆÈ¢ò**ÔºöÊÖ¢ÂõûË∑Ø‰ªçÊòØÊâ©Êï£Ê®°ÂûãÔºå**Êé®ÁêÜÈÄüÂ∫¶Áì∂È¢à‰ªçÂú®**ÔºõÂÖ∂ÊΩúÂä®‰ΩúÁ©∫Èó¥ÔºàÂ¶ÇÈÄöËøáÊ†áÂáÜVAEËé∑ÂæóÔºâÂπ∂Èùû‰∏∫Êó∂Â∫èËøûË¥ØÊÄß‰∏ìÈó®ËÆæËÆ°ÔºåÁ®≥ÂÆöÊÄßÊèêÂçáÊúâÈôê„ÄÇ\n\n#### 3. Êé®ÁêÜÊïàÁéáÈù©ÂëΩÔºöÊµÅÂåπÈÖçÊ®°ÂûãÂºïÂÖ•\n*   **‰ª£Ë°®Â∑•‰Ωú**Ôºö**Flow Policy** [34], **Consistency Flow Matching** Áõ∏ÂÖ≥Á†îÁ©∂ [7,2]\n*   **Ê†∏ÂøÉÊÄùÊÉ≥**ÔºöÁî®ÊµÅÂåπÈÖçÔºàFlow MatchingÔºâÊõø‰ª£Êâ©Êï£Ê®°Âûã„ÄÇÈÄöËøáÂ≠¶‰∏Ä‰∏™ÂêëÈáèÂú∫ÔºàODEÔºâÔºåÂÆûÁé∞‰ªéÁÆÄÂçïÂàÜÂ∏ÉÂà∞ÁõÆÊ†áÂàÜÂ∏ÉÁöÑÂèòÊç¢ÔºåÊîØÊåÅ**ÂçïÊ≠•ÊàñÊûÅÂ∞ëÊ≠•Êé®ÁêÜ**„ÄÇ\n*   **Áõ∏ÊØî‰∫éÊâ©Êï£Ê®°Âûã**Ôºö\n    *   **ÊîπËøõ**ÔºöÂÆûÁé∞‰∫Ü**Êï∞ÈáèÁ∫ßÁ∫ßÂà´ÁöÑÊé®ÁêÜÂä†ÈÄü**Ôºà‰ªéÂá†ÂçÅÊØ´ÁßíÈôçËá≥Âá†ÊØ´ÁßíÔºâÔºåÊª°Ë∂≥‰∫ÜÂÆûÊó∂ÊéßÂà∂ÈúÄÊ±Ç„ÄÇ\n    *   **ÈÅóÁïôÈóÆÈ¢ò**ÔºöÁõ¥Êé•Âú®**ÂéüÂßãÈ´òÁª¥Âä®‰ΩúÁ©∫Èó¥**ËøõË°åÊµÅÂåπÈÖçÔºåÂØπÊï∞ÂÄºËØØÂ∑ÆÂíåÂô™Â£∞ÈùûÂ∏∏ÊïèÊÑüÔºåÂÆπÊòì‰∫ßÁîü**ËΩ®ËøπÊäñÂä®ÂíåÊâßË°å‰∏çÁ®≥ÂÆö**ÔºåÂ∞§ÂÖ∂Âú®ÈïøËßÜÈáé‰ªªÂä°‰∏≠„ÄÇ\n\n#### 4. ÊâßË°åÁ®≥ÂÆöÊÄßÊîªÂùöÔºöÊΩúÂä®‰Ωú‰∏éÊó∂Â∫èÂª∫Ê®°\n*   **ÂèëÂ±ïÊñπÂêëÔºöÊΩúÂä®‰ΩúË°®Á§∫Â≠¶‰π†**\n    *   **Áõ∏ÂÖ≥Êé¢Á¥¢**Ôºö**CoMo** [29], **LAOM** [18]Ôºå‰ª•ÂèäRDP‰∏≠‰ΩøÁî®ÁöÑVAE„ÄÇ\n    *   **Ê†∏ÂøÉÊÄùÊÉ≥**Ôºö‰∏çÁõ¥Êé•ÁîüÊàêÂéüÂßãÂä®‰ΩúÔºåËÄåÊòØÁîüÊàê‰ΩéÁª¥„ÄÅÊäΩË±°ÁöÑ‚ÄúÊΩúÂä®‰Ωú‚ÄùÔºåÂÜçËß£Á†Å‰∏∫ÂÖ∑‰ΩìÂä®‰Ωú„ÄÇËøôÂèØ‰ª•ÊñΩÂä†‰ø°ÊÅØÁì∂È¢àÔºåËøáÊª§Âô™Â£∞„ÄÇ\n    *   **Êó©ÊúüÊñπÊ≥ïÁöÑÂ±ÄÈôê**Ôºö\n        *   **Á¶ªÊï£ÊΩúÁ©∫Èó¥ÔºàVQ-VAEÔºâ**ÔºöÂèØËÉΩÈôêÂà∂Ë°®ËææËÉΩÂäõÔºåÊó†Ê≥ïË°®ÂæÅÁªÜËÖªÁöÑËøûÁª≠ËøêÂä®„ÄÇ\n        *   **ÈùûÊó∂Â∫èÊïèÊÑüÁöÑÁºñÁ†Å**ÔºöÁã¨Á´ãÁºñÁ†ÅÊØè‰∏ÄÂ∏ßÊàñÂä®‰ΩúÂùóÔºåÁº∫‰πèÂØπËΩ®ËøπÊï¥‰ΩìÂπ≥ÊªëÊÄßÁöÑÁ∫¶Êùü„ÄÇ\n        *   **‰∏éÁîüÊàêÊ®°ÂûãËÄ¶Âêà‰∏çÁ¥ß**ÔºöÊΩúÁºñÁ†ÅÂô®ËÆæËÆ°ÔºàÂ¶ÇÊ†áÂáÜVAEÔºâÊú™‰∏ìÈó®ÈíàÂØπ‰∏ãÊ∏∏ÁöÑÊµÅÂåπÈÖçÁîüÊàêËøáÁ®ãËøõË°å‰ºòÂåñ„ÄÇ\n\n#### 5. Êú¨ÊñáÂ∑•‰ΩúÔºöLG-Flow Policy ‚Äî‚Äî ÊïàÁéá‰∏éÁ®≥ÂÆöÁöÑÁªü‰∏Ä\nÊú¨ÊñáÁ´ôÂú®ÂâçËø∞Â∑•‰ΩúÁöÑËÇ©ËÜÄ‰∏äÔºå**Ê†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂ∞Ü‚ÄúÈ´òÊïàÁéáÁöÑÊµÅÂåπÈÖç‚Äù‰∏é‚Äú‰∏ì‰∏∫Á®≥ÂÆöÊÄßËÆæËÆ°ÁöÑÊó∂Â∫èËøûË¥ØÊΩúÁ©∫Èó¥‚ÄùËøõË°åÊ∑±Â∫¶ËÄ¶Âêà**„ÄÇ\n\n*   **Áõ∏ÊØî‰∫é Flow Policy [34]ÔºàÊïàÁéáÊñπÂêëÔºâ**Ôºö\n    *   **ÊîπËøõ**ÔºöÂ∞ÜÊµÅÂåπÈÖç‰ªé**ÂéüÂßãÂä®‰ΩúÁ©∫Èó¥**ÁßªËá≥**ËøûÁª≠Êó∂Â∫èÊ≠£ÂàôÂåñÊΩúÁ©∫Èó¥**„ÄÇÈÄöËøáGRUÁºñÁ†ÅÂô®ÂØπÂä®‰ΩúÂùóÂ∫èÂàóËøõË°åÁºñÁ†ÅÔºåÊòæÂºèÂú∞Â°ëÈÄ†Âá∫‰∏Ä‰∏™Âπ≥ÊªëÁöÑÊΩúËΩ®ËøπÊµÅÂΩ¢Ôºå‰ªéÊ†πÊú¨‰∏äÊäëÂà∂‰∫ÜÈ´òÈ¢ëÂô™Â£∞Âú®ODEÁßØÂàÜËøáÁ®ã‰∏≠ÁöÑÊîæÂ§ßÔºå**Âú®‰øùÊåÅËøë‰πéÂçïÊ≠•Êé®ÁêÜÈÄüÂ∫¶ÁöÑÂêåÊó∂ÔºåÂ§ßÂπÖÊèêÂçá‰∫ÜËΩ®ËøπÂπ≥ÊªëÊÄßÔºàÊúÄÈ´ò93.7%ÔºâÂíå‰ªªÂä°ÊàêÂäüÁéá**„ÄÇ\n\n*   **Áõ∏ÊØî‰∫é RDP [28]ÔºàÁ®≥ÂÆöÊÄß/ÂàÜÂ±ÇÊñπÂêëÔºâ**Ôºö\n    *   **ÊîπËøõ**Ôºö\n        1.  **ÁîüÊàêÊ®°ÂûãÊïàÁéá**ÔºöÁî®**ÂçïÊ≠•ÊµÅÂåπÈÖç**Êõø‰ª£‰∫Ü**Ëø≠‰ª£Êâ©Êï£Ê®°Âûã**ÔºåËß£ÂÜ≥‰∫ÜÊÖ¢ÂõûË∑ØÁöÑÊ†πÊú¨Âª∂ËøüÁì∂È¢à„ÄÇ\n        2.  **ÊΩúÁ©∫Èó¥ËÆæËÆ°**Ôºö‰ΩøÁî®‰∫Ü**ËøûÁª≠ÊΩúÁ©∫Èó¥**ËÄåÈùûÁ¶ªÊï£Á©∫Èó¥ÔºåÂπ∂ÂºïÂÖ•**Âº∫Êó∂Â∫èÂΩíÁ∫≥ÂÅèÁΩÆÔºàGRUÔºâ**Ôºå‰∏ìÈó®‰∏∫ÁîüÊàêÂπ≥ÊªëËΩ®ËøπËÄåËÆæËÆ°„ÄÇRDPÁöÑÊΩúÁ©∫Èó¥‰∏ªË¶ÅÊúçÂä°‰∫éÂàÜÂ±ÇÔºåËÄåÈùûÊòæÂºè‰ºòÂåñÂπ≥ÊªëÊÄß„ÄÇ\n        3.  **Â§öÊ®°ÊÄÅËûçÂêà**ÔºöÊèêÂá∫‰∫ÜÊõ¥ÁÅµÊ¥ªÁöÑ**ÊâßË°åÊó∂Â§öÊ®°ÊÄÅË∞ÉÂà∂**ÔºàÈÄöËøáDecoderÁöÑFiLMÔºâÔºåÂ∞ÜÊÑüÁü•ÂèçÈ¶àÔºàÂ¶ÇËÖïÈÉ®Áõ∏Êú∫Ôºâ‰∏éÂä®‰ΩúÁîüÊàêËß£ËÄ¶Ôºå‰∏çÂΩ±ÂìçÊΩúËΩ®ËøπÁöÑËøûË¥ØÊÄß„ÄÇ\n\n*   **Áõ∏ÊØî‰∫é DP3/iDP3ÔºàÊÑüÁü•ÊñπÂêëÔºâ**Ôºö\n    *   **ÊîπËøõ**ÔºöÂú®ÁªßÊâøÂÖ∂**Âá†‰ΩïÊÑüÁü•ÁÇπ‰∫ëÁºñÁ†Å**‰ºòÁÇπÁöÑÂêåÊó∂ÔºåÈÄöËøáÊΩúÁ©∫Èó¥ÊµÅÂåπÈÖçÊ°ÜÊû∂Ôºå**ÂêåÊó∂ÂÆûÁé∞‰∫ÜËøúË∂ÖÂÆÉ‰ª¨ÁöÑÊé®ÁêÜÈÄüÂ∫¶Ôºà7.5ms vs. 55.9msÔºâÂíåÊõ¥È´òÁöÑ‰ªªÂä°ÊàêÂäüÁéá**„ÄÇ\n\n### ÊÄªÁªìÔºö‰∏çÂêåÁöÑÂèëÂ±ïÊñπÂêë‰∏éËûçÂêà\n1.  **Ë°®ËææËÉΩÂäõÊñπÂêë**ÔºöÊâ©Êï£Ê®°Âûã ‚Üí Êõ¥Âº∫Â§ßÁöÑÁîüÊàêËÉΩÂäõ„ÄÇ\n2.  **ÊÑüÁü•ÊïàÁéáÊñπÂêë**Ôºö2D RGB ‚Üí 3DÁÇπ‰∫ë ‚Üí ËΩªÈáèÂåñ3DÁºñÁ†Å„ÄÇ\n3.  **Êé®ÁêÜÊïàÁéáÊñπÂêë**ÔºöÂ§öÊ≠•Êâ©Êï£ ‚Üí (‰∏ÄËá¥ÊÄßÊ®°Âûã) ‚Üí **ÂçïÊ≠•/Â∞ëÊ≠•ÊµÅÂåπÈÖç**„ÄÇ\n4.  **ÊâßË°åÁ®≥ÂÆöÊÄßÊñπÂêë**ÔºöÂéüÂßãÂä®‰ΩúÁîüÊàê ‚Üí ÂàÜÂ±ÇËßÑÂàí ‚Üí **ÊΩúÂä®‰ΩúË°®Á§∫** ‚Üí **Êó∂Â∫èËøûË¥ØÁöÑÊΩúËΩ®ËøπÂª∫Ê®°**„ÄÇ\n\n**LG-Flow Policy ÁöÑË¥°ÁåÆÂú®‰∫é**ÔºöÂÆÉÊïèÈîêÂú∞Êäì‰Ωè‰∫Ü‚Äú**Âú®ÂéüÂßãÁ©∫Èó¥ÂÅöÊµÅÂåπÈÖç‰∏çÁ®≥ÂÆö**‚ÄùËøô‰∏ÄÂÖ≥ÈîÆÁì∂È¢àÔºåÂàõÈÄ†ÊÄßÂú∞Â∞Ü**ÂèëÂ±ïÊñπÂêë3ÔºàÊµÅÂåπÈÖçÔºâ** ‰∏é**ÂèëÂ±ïÊñπÂêë4ÔºàÊó∂Â∫èËøûË¥ØÊΩúÁ©∫Èó¥Ôºâ** Ê∑±Â∫¶ËûçÂêà„ÄÇÂÆÉ‰∏çÊòØÁÆÄÂçïÁöÑÁªÑÂêàÔºåËÄåÊòØÈÄöËøá**ËΩ®ËøπÁ∫ßÊΩúÁºñÁ†Å**Âíå**ÊΩúÁ©∫Èó¥ÊµÅÂåπÈÖç**Ëøô‰∏ÄÊ†∏ÂøÉËÆæËÆ°ÔºåÂÆûÁé∞‰∫ÜÊïàÁéá‰∏éÁ®≥ÂÆöÊÄßÁöÑÂçèÂêåÊèêÂçáÔºå‰ª£Ë°®‰∫ÜÂΩìÂâçËß£ÂÜ≥‚Äú‰∏çÂèØËÉΩ‰∏âËßí‚ÄùÈóÆÈ¢òÁöÑ‰∏Ä‰∏™ÂâçÊ≤ø‰∏îÊúâÊïàÁöÑÊÄùË∑Ø„ÄÇÂÖ∂**Âá†‰ΩïÊÑüÁü•ÁºñÁ†Å**Âíå**ÊâßË°åÊó∂Â§öÊ®°ÊÄÅËß£Á†Å**ÁöÑËÆæËÆ°ÔºåÂàôÊòØÂØπÂèëÂ±ïÊñπÂêë2ÂíåÊõ¥ÂπøÊ≥õÂ§öÊ®°ÊÄÅËûçÂêàÁöÑÂª∂Áª≠‰∏éÊãìÂ±ï„ÄÇ",
            "timestamp": "2026-02-03T15:55:37.447000",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "‰ªñÁöÑÂâç‰ΩúÊúâÂì™‰∫õÔºü‰ΩøÁî®Ë°®Ê†º‰ªîÁªÜËÆ≤ËÆ≤‰ªñÁöÑÊØèÁØáÂâç‰ΩúÔºå‰ªñÂíåÂâç‰ΩúÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºå‰∏ªË¶ÅÊîπÂñÑÊòØ‰ªÄ‰πàÔºüÁùÄÈáç‰∫éÂÖ∑‰ΩìÁõ∏ÊØî‰∫é‰πãÂâçÊñáÁ´†ÁöÑÊîπÂä®",
            "answer": "## ËÆ∫ÊñáÂâç‰ΩúÂàÜÊûê\n\nÊ†πÊçÆËÆ∫ÊñáÂÜÖÂÆπÔºå‰ΩúËÄÖÂõ¢ÈòüÔºàÁâπÂà´ÊòØÈÄöËÆØ‰ΩúËÄÖ Liu YangÔºâÂèäÂÖ∂Âêà‰ΩúËÄÖÂú®Êú∫Âô®‰∫∫ËßÜËßâËøêÂä®Ê®°‰ªøÂ≠¶‰π†È¢ÜÂüüÊúâ‰∏ÄÁ≥ªÂàóÁõ∏ÂÖ≥Á†îÁ©∂„ÄÇ‰ª•‰∏ãË°®Ê†ºÊ¢≥ÁêÜ‰∫ÜÊñá‰∏≠ÊòéÁ°ÆÂºïÁî®ÁöÑ„ÄÅÂèØ‰Ωú‰∏∫ÂÖ∂‚ÄúÂâç‰Ωú‚ÄùÊàñÁ¥ßÂØÜÁõ∏ÂÖ≥Âü∫Á∫øÁöÑÂ∑•‰ΩúÔºåÂπ∂ÈáçÁÇπÂàÜÊûêÊú¨ÊñáÔºàLG-Flow PolicyÔºâ‰∏éÂÆÉ‰ª¨ÁöÑÊ†∏ÂøÉÂå∫Âà´ÂíåÊîπËøõ„ÄÇ\n\n| Ââç‰Ωú/Âü∫Á∫øËÆ∫Êñá | Ê†∏ÂøÉÊñπÊ≥ï | ‰∏éÊú¨ÊñáÔºàLG-Flow PolicyÔºâÁöÑ‰∏ªË¶ÅÂå∫Âà´ | Êú¨ÊñáÁöÑ‰∏ªË¶ÅÊîπËøõ/ÂàõÊñ∞ÁÇπ |\n| :--- | :--- | :--- | :--- |\n| **Diffusion Policy (DP)** <br> Chi et al., 2025 | **Âü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÁ≠ñÁï•**ÔºöÂ∞ÜÂä®‰ΩúÁîüÊàêÂª∫Ê®°‰∏∫Êù°‰ª∂ËΩ®ËøπÂéªÂô™ËøáÁ®ã„ÄÇ‰ΩøÁî®Ëø≠‰ª£ÈááÊ†∑ÔºåÂª∫Ê®°ËÉΩÂäõÂº∫Ôºå‰ΩÜÊé®ÁêÜÂª∂ËøüÈ´ò„ÄÇ | 1. **ÁîüÊàêËåÉÂºè‰∏çÂêå**ÔºöDP‰ΩøÁî®**Êâ©Êï£Ê®°Âûã**ÔºåÈúÄË¶ÅÂ§öÊ≠•Ëø≠‰ª£ÂéªÂô™ÔºõÊú¨Êñá‰ΩøÁî®**ÊµÅÂåπÈÖçÔºàFlow MatchingÔºâ**ÔºåÊó®Âú®ÂÆûÁé∞ÂçïÊ≠•ÁîüÊàê„ÄÇ<br>2. **Âä®‰ΩúÁ©∫Èó¥‰∏çÂêå**ÔºöDPÁõ¥Êé•Âú®**ÂéüÂßãÈ´òÁª¥Âä®‰ΩúÁ©∫Èó¥**ËøõË°åÁîüÊàêÔºõÊú¨ÊñáÂú®**ËøûÁª≠„ÄÅÊó∂Èó¥Ê≠£ÂàôÂåñÁöÑÊΩúÂú®Âä®‰ΩúÁ©∫Èó¥**ËøõË°åÊµÅÂåπÈÖç„ÄÇ<br>3. **Á®≥ÂÆöÊÄßÂÖ≥Ê≥®ÁÇπ‰∏çÂêå**ÔºöDP‰∏ªË¶ÅÂÖ≥Ê≥®Ë°®ËææËÉΩÂäõÂíåÊ≥õÂåñÊÄßÔºåÂØπÈïøÊó∂ÂüüÊâßË°åÁöÑ**Êó∂Èó¥Áõ∏Âπ≤ÊÄß**Âíå**Âπ≥ÊªëÊÄß**ÂÖ≥Ê≥®ËæÉÂ∞ë„ÄÇ | 1. **Êé®ÁêÜÈÄüÂ∫¶**ÔºöÂ∞ÜÂπ≥ÂùáÊØèÊ≠•Êé®ÁêÜÊó∂Èó¥‰ªé **~55.9 ms (DP3)** Èôç‰ΩéÂà∞ **~7.5 ms**ÔºåÊèêÂçáÁ∫¶ **7.5ÂÄç**ÔºåÊé•ËøëÂçïÊ≠•Êé®ÁêÜ„ÄÇ<br>2. **ÊâßË°åÂπ≥ÊªëÊÄß**ÔºöÈÄöËøáÊΩúÂú®Á©∫Èó¥Âª∫Ê®°ÂíåÊµÅÂåπÈÖçÔºåÂ∞ÜËΩ®ËøπÂπ≥ÊªëÂ∫¶ÊåáÊ†áÔºàÂÄºË∂ä‰ΩéË∂äÂπ≥ÊªëÔºâÁõ∏ÊØîDP3Âü∫Á∫ø**ÊèêÂçá51.4%**ÔºåÊòæËëóÂáèÂ∞ëÊäñÂä®„ÄÇ<br>3. **‰ªªÂä°ÊàêÂäüÁéá**ÔºöÂú®Ê®°ÊãüÂÆûÈ™å‰∏≠ÔºåÂπ≥ÂùáÊàêÂäüÁéáÁõ∏ÊØîDP3ÊèêÂçá **12.6%**„ÄÇ |\n| **3D Diffusion Policy (DP3)** <br> Ze et al., 2024 | **3DÊâ©Êï£Á≠ñÁï•**ÔºöDPÁöÑ3DÊâ©Â±ïÁâàÔºå‰ΩøÁî®**ËΩªÈáèÁ∫ßÁÇπ‰∫ëÁºñÁ†ÅÂô®**ËøõË°åÂá†‰ΩïÊÑüÁü•ÁöÑÊù°‰ª∂ÁîüÊàê„ÄÇÊòØÊú¨ÊñáÁöÑ‰∏ªË¶ÅÊâ©Êï£Âü∫Á∫ø„ÄÇ | 1. **ÊÑüÁü•‰∏éÁîüÊàêÁöÑËÄ¶Âêà**ÔºöDP3ÁöÑÊâ©Êï£ËøáÁ®ãÁõ¥Êé•‰ª•ÁÇπ‰∫ëÁâπÂæÅ‰∏∫Êù°‰ª∂„ÄÇÊú¨ÊñáÂ∞Ü**Âá†‰ΩïÊÑüÁü•ÔºàÁÇπ‰∫ëÁºñÁ†ÅÔºâ** ‰∏é**Âä®‰ΩúÁîüÊàêÔºàÊΩúÂú®ÊµÅÂåπÈÖçÔºâ** Ëß£ËÄ¶ÔºåÂπ∂ÈÄöËøáFiLMËøõË°åÂ±ÇÊ¨°ÂåñË∞ÉÂà∂„ÄÇ<br>2. **Áº∫‰πèÊòæÂºèÁöÑÊó∂Èó¥Ê≠£ÂàôÂåñ**ÔºöDP3Âú®Âä®‰ΩúÂ∫èÂàóÂ±ÇÈù¢ËøõË°åÈ¢ÑÊµãÔºå‰ΩÜÊ≤°ÊúâÂºïÂÖ•**ÈÄíÂΩíÁºñÁ†ÅÂô®**Á≠âÊú∫Âà∂Êù•ÊòæÂºèÂú∞Âº∫Âà∂ÊΩúÂú®ËΩ®ËøπÁöÑÊó∂Èó¥Âπ≥ÊªëÊÄß„ÄÇ | 1. **Ëß£ËÄ¶ÁöÑÊû∂ÊûÑ**ÔºöÊèêÂá∫**ËΩ®ËøπÁ∫ßÊΩúÂú®Âä®‰ΩúË°®Á§∫**Ôºå‰ΩøÁî®GRUÁºñÁ†ÅÂô®Â∞ÜÂä®‰ΩúÂùóÁºñÁ†Å‰∏∫Êó∂Èó¥Áõ∏Âπ≤ÁöÑÊΩúÂú®‰ª£Á†ÅÔºåÂ∞ÜÂÖ®Â±ÄËøêÂä®ÁªìÊûÑ‰∏éÂ∫ïÂ±ÇÊéßÂà∂Âô™Â£∞Ëß£ËÄ¶„ÄÇ<br>2. **Â±ÇÊ¨°ÂåñÂá†‰ΩïÊù°‰ª∂**ÔºöÈááÁî®**ÂèåÂàÜÊîØÁÇπ‰∫ëÁºñÁ†ÅÂô®**ÔºàÂ±ÄÈÉ®+‰∏≠ÂøÉÔºâÂíå**‰∏§Èò∂ÊÆµFiLMË∞ÉÂà∂**ÔºåÊõ¥Á≤æÁªÜÂú∞ËûçÂêàÂá†‰Ωï‰ø°ÊÅØÔºåËÄå‰∏çÂπ≤Êâ∞ÊΩúÂú®ËΩ®ËøπÁöÑÂä®ÊÄÅÁâπÊÄß„ÄÇ |\n| **iDP3** <br> Ze et al., 2025 | **ÊîπËøõÁöÑ3DÊâ©Êï£Á≠ñÁï•**ÔºöDP3ÁöÑÊîπËøõÁâàÊú¨ÔºåËøõ‰∏ÄÊ≠•‰ºòÂåñ‰∫Ü3DË°®Á§∫ÂíåÁ≠ñÁï•Â≠¶‰π†„ÄÇ | ‰∏éDP3Á±ª‰ººÔºåÊ†∏ÂøÉÂå∫Âà´‰ªçÂú®‰∫é**Êâ©Êï£Ê®°Âûã vs. ÊµÅÂåπÈÖç**Ôºå‰ª•Âèä**ÂéüÂßãÁ©∫Èó¥ vs. ÊΩúÂú®Á©∫Èó¥ÁîüÊàê**„ÄÇiDP3Âú®DP3Âü∫Á°Ä‰∏äÊèêÂçá‰∫ÜÊÑüÁü•ÊïàÁéáÔºå‰ΩÜÊú™Ëß£ÂÜ≥Êâ©Êï£Ê®°ÂûãÂõ∫ÊúâÁöÑ**Êé®ÁêÜÂª∂Ëøü**ÂíåÊµÅÂåπÈÖçÂú®ÂéüÂßãÂä®‰ΩúÁ©∫Èó¥ÁöÑ**‰∏çÁ®≥ÂÆö**ÈóÆÈ¢ò„ÄÇ | Êú¨Êñá‰ªéÊ†πÊú¨‰∏äÊîπÂèò‰∫ÜÁîüÊàêËåÉÂºèÔºå**Áî®ÊΩúÂú®Á©∫Èó¥ÊµÅÂåπÈÖçÊõø‰ª£‰∫ÜÊâ©Êï£Ê®°Âûã**Ôºå‰ªéËÄåÂú®‰øùÊåÅÈ´òÊàêÂäüÁéáÁöÑÂêåÊó∂ÔºåËé∑Âæó‰∫Ü**Êï∞ÈáèÁ∫ßÊèêÂçáÁöÑÊé®ÁêÜÈÄüÂ∫¶**Âíå**ÊòæËëóÊîπÂñÑÁöÑÊâßË°åÂπ≥ÊªëÊÄß**„ÄÇ |\n| **Reactive Diffusion Policy (RDP)** <br> Xue et al., 2025 | **ÂàÜÂ±ÇÂèçÂ∫îÂºèÊâ©Êï£Á≠ñÁï•**ÔºöÈááÁî®‚ÄúÂø´-ÊÖ¢‚ÄùÂàÜÂ±ÇÁªìÊûÑÔºåÊÖ¢Ë∑ØÂæÑËßÑÂàíÔºåÂø´Ë∑ØÂæÑÔºàÂü∫‰∫éVAEÊΩúÂú®Á©∫Èó¥ÔºâÂèçÂ∫îÂºèË∞ÉÊï¥„ÄÇÂºïÂÖ•‰∫Ü**ÊΩúÂú®Âä®‰ΩúË°®Á§∫**„ÄÇ | 1. **ÊΩúÂú®Á©∫Èó¥ÁöÑÊÄßË¥®**ÔºöRDP‰ΩøÁî®**VAE**Ëé∑ÂæóÊΩúÂú®Ë°®Á§∫Ôºå‰∏ªË¶ÅÁî®‰∫é**Â±ÇÊ¨°ÂåñÊéßÂà∂‰∏≠ÁöÑÂø´ÈÄüÂèçÂ∫îÂ±Ç**ÔºåÂÖ∂ÊΩúÂú®Á©∫Èó¥ÂèØËÉΩÁº∫‰πè**ÊòæÂºèÁöÑÊó∂Èó¥Áõ∏Âπ≤ÊÄßÁ∫¶Êùü**„ÄÇ<br>2. **ÁîüÊàêÊ®°Âûã**ÔºöRDPÁöÑÊÖ¢Ë∑ØÂæÑ‰ªçÊòØ**Êâ©Êï£Ê®°Âûã**ÔºåÂ≠òÂú®Êé®ÁêÜÂª∂Ëøü„ÄÇÂÖ∂Âø´ÈÄüË∑ØÂæÑËôΩÂú®ÊΩúÂú®Á©∫Èó¥Ôºå‰ΩÜÁõÆÁöÑÊòØÂ±ÄÈÉ®Ë∞ÉÊï¥ÔºåËÄåÈùûÂÖ®Â±ÄËΩ®ËøπÁîüÊàê„ÄÇ | 1. **‰∏ì‰∏∫Âπ≥ÊªëÊÄßËÆæËÆ°ÁöÑÊΩúÂú®Á©∫Èó¥**ÔºöÊú¨ÊñáÁöÑÊΩúÂú®Á©∫Èó¥ÈÄöËøá**GRUÁºñÁ†ÅÂô®ÂíåÂèòÂàÜÊ≠£ÂàôÂåñ** explicitly designed for temporal coherenceÔºåËÄåRDPÁöÑVAEÊΩúÂú®Á©∫Èó¥‰∏ªË¶Å‰∏∫‰∫ÜÂéãÁº©ÂíåÂø´ÈÄüËß£Á†Å„ÄÇ<br>2. **Áªü‰∏ÄÁöÑÊµÅÂåπÈÖçÁîüÊàê**ÔºöÊú¨Êñá**Êï¥‰∏™ËΩ®Ëøπ**ÈÉΩÂú®Êó∂Èó¥Áõ∏Âπ≤ÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠ÈÄöËøá**‰∏ÄËá¥ÊÄßÊµÅÂåπÈÖç**‰∏ÄÊ¨°ÊÄßÁîüÊàêÔºåÂÆûÁé∞‰∫Ü**Âø´ÈÄü**‰∏é**Âπ≥Êªë**ÁöÑÁªü‰∏Ä„ÄÇRDP‰ªç‰æùËµñÊâ©Êï£Ê®°ÂûãËøõË°åËßÑÂàíÔºåÈÄüÂ∫¶ËæÉÊÖ¢„ÄÇ<br>3. **Âπ≥ÊªëÊÄßÁªìÊûú**ÔºöÂÆûÈ™åË°®ÊòéÔºåÂç≥‰ΩøÂú®RDPÂ∑≤‰ΩøÁî®ÊΩúÂú®Ë°®Á§∫ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊú¨ÊñáÁöÑËΩ®ËøπÂπ≥ÊªëÂ∫¶‰ªçÊòæËëó‰ºò‰∫éRDPÔºàÊ®°Êãü‰∏≠ÊèêÂçá51.4%Ôºâ„ÄÇ |\n| **Flow Policy** <br> Zhang et al., 2025 | **Âü∫‰∫éÊµÅÂåπÈÖçÁöÑÁ≠ñÁï•**ÔºöÂ∞Ü‰∏ÄËá¥ÊÄßÊµÅÂåπÈÖçÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰ΩúÔºåÁõ¥Êé•Âú®**ÂéüÂßãÂä®‰ΩúÁ©∫Èó¥**ËøõË°åÂø´ÈÄüÁîüÊàê„ÄÇ | **ËøôÊòØÊúÄÁõ¥Êé•ÁöÑÂâç‰ΩúÂØπÊØî**„ÄÇÊ†∏ÂøÉÂå∫Âà´Âú®‰∫é**Âä®‰ΩúÁîüÊàêÁ©∫Èó¥**Ôºö<br>- Flow PolicyÔºö**ÂéüÂßãÂä®‰ΩúÁ©∫Èó¥**ÁöÑÊµÅÂåπÈÖç„ÄÇ<br>- LG-Flow PolicyÔºö**Êó∂Èó¥Ê≠£ÂàôÂåñÊΩúÂú®Âä®‰ΩúÁ©∫Èó¥**ÁöÑÊµÅÂåπÈÖç„ÄÇ | 1. **Ëß£ÂÜ≥ÂÖ≥ÈîÆÁº∫Èô∑**ÔºöFlow PolicyÂú®ÂéüÂßãÁ©∫Èó¥ÊµÅÂåπÈÖç‰ºö‰∫ßÁîü**ËΩ®ËøπÊäñÂä®Âíå‰∏çÁ®≥ÂÆöÊâßË°å**„ÄÇÊú¨ÊñáÈÄöËøáËøÅÁßªÂà∞**Âπ≥ÊªëÁöÑÊΩúÂú®Á©∫Èó¥**Ôºå‰ªéÊ†πÊú¨‰∏äËß£ÂÜ≥‰∫ÜËøô‰∏ÄÈóÆÈ¢ò„ÄÇ<br>2. **ÊÄßËÉΩÈ£ûË∑É**ÔºöÂú®ÁúüÂÆû‰∏ñÁïåÂÆûÈ™å‰∏≠ÔºåÊú¨ÊñáÁõ∏ÊØîFlow PolicyÂ∞Ü**ËΩ®ËøπÂπ≥ÊªëÂ∫¶ÊèêÂçá‰∫Ü93.7%**ÔºåÂêåÊó∂Â∞Ü**Âπ≥Âùá‰ªªÂä°ÊàêÂäüÁéá‰ªé52.5%ÊèêÂçáËá≥77.5%**ÔºåËÄåÊé®ÁêÜÊó∂Èó¥‰ªÖÁï•ÊúâÂ¢ûÂä†Ôºà6.54ms -> 8.59msÔºâ„ÄÇ |\n\n### ÊÄªÁªìÔºöÊú¨ÊñáÁõ∏ÂØπ‰∫éÂâç‰ΩúÁöÑÊ†∏ÂøÉÊîπÂä®‰∏éÊîπÂñÑ\n\n1.  **ËåÉÂºèÂàõÊñ∞Ôºö‰ªé‚ÄúÊâ©Êï£/ÂéüÂßãÁ©∫Èó¥ÊµÅÂåπÈÖç‚ÄùÂà∞‚ÄúÊΩúÂú®Á©∫Èó¥ÊµÅÂåπÈÖç‚Äù**\n    *   **ÊîπÂä®**ÔºöÊèêÂá∫‰∫Ü **LG-Flow Policy** Ê°ÜÊû∂ÔºåÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫é**Âú®ËøûÁª≠„ÄÅÊó∂Èó¥Áõ∏Âπ≤ÁöÑÊΩúÂú®Âä®‰ΩúÁ©∫Èó¥ÊâßË°åÊµÅÂåπÈÖç**„ÄÇ\n    *   **ÊîπÂñÑ**Ôºö**‰∏Ä‰∏æ‰∏âÂæó**Ôºö\n        *   **‰øùÁïô‰∫ÜÊµÅÂåπÈÖçÁöÑÂø´ÈÄüÊé®ÁêÜ‰ºòÂäø**ÔºàËøëÂçïÊ≠•ÁîüÊàêÔºâ„ÄÇ\n        *   **ÂÖãÊúç‰∫ÜÂéüÂßãÁ©∫Èó¥ÊµÅÂåπÈÖçÁöÑËΩ®ËøπÊäñÂä®ÈóÆÈ¢ò**ÔºàÂπ≥ÊªëÂ∫¶ÊûÅÂ§ßÊèêÂçáÔºâ„ÄÇ\n        *   **ÈÅøÂÖç‰∫ÜÊâ©Êï£Ê®°ÂûãÁöÑÂ§öÊ≠•Ëø≠‰ª£Âª∂Ëøü**ÔºàÊé®ÁêÜÈÄüÂ∫¶ÊØîÊâ©Êï£Âø´‰∏Ä‰∏™Êï∞ÈáèÁ∫ßÔºâ„ÄÇ\n\n2.  **ÊäÄÊúØÊ†∏ÂøÉÔºöÊûÑÂª∫Êó∂Èó¥Áõ∏Âπ≤ÁöÑÊΩúÂú®Âä®‰ΩúÁ©∫Èó¥**\n    *   **ÊîπÂä®**ÔºöÂºïÂÖ•**ËΩ®ËøπÁ∫ßËøûÁª≠ÊΩúÂú®Âä®‰ΩúË°®Á§∫**Ôºå‰ΩøÁî®**GRUÁºñÁ†ÅÂô®**ÂØπÂä®‰ΩúÂùóÂ∫èÂàóËøõË°åÁºñÁ†ÅÔºåÂπ∂ÊñΩÂä†**ÂèòÂàÜÊ≠£ÂàôÂåñ**„ÄÇ\n    *   **ÊîπÂñÑ**ÔºöËØ•ÊΩúÂú®Á©∫Èó¥**ÊòæÂºèÂú∞Âº∫Âà∂‰∫ÜÊó∂Èó¥Âπ≥ÊªëÊÄßÂíåÂÖ®Â±ÄËøêÂä®ÁªìÊûÑ**ÔºåÂ∞ÜÂ∫ïÂ±ÇÊéßÂà∂Âô™Â£∞‰∏éÈ´òÁ∫ßËøêÂä®ËßÑÂàíËß£ËÄ¶Ôºå‰ΩøÂæóÂú®ÂÖ∂‰∏äËøõË°åÁöÑÊµÅÂåπÈÖçÊï∞ÂÄºÊõ¥Á®≥ÂÆöÔºåÁîüÊàêÈïøÊó∂ÂüüËΩ®ËøπÊõ¥ÂèØÈù†„ÄÇ\n\n3.  **Êû∂ÊûÑËÆæËÆ°ÔºöÊÑüÁü•„ÄÅÁîüÊàê‰∏éÊâßË°åËß£ËÄ¶**\n    *   **ÊîπÂä®**ÔºöÈááÁî®**Â±ÇÊ¨°ÂåñÂá†‰ΩïÊÑüÁü•ÔºàÁÇπ‰∫ëÁºñÁ†ÅÔºâ** Êù°‰ª∂ÂåñÊΩúÂú®ÊµÅÔºåÂêåÊó∂**Âú®ÊâßË°åÊó∂ÈÄöËøáËß£Á†ÅÂô®ÈõÜÊàêÂ§öÊ®°ÊÄÅÂèçÈ¶à**ÔºàÂ¶ÇËÖïÈÉ®Áõ∏Êú∫Ôºâ„ÄÇ\n    *   **ÊîπÂñÑ**Ôºö\n        *   **Âá†‰ΩïÊÑüÁü•‰∏çÂπ≤Êâ∞ËΩ®ËøπÁîüÊàê**ÔºöÁÇπ‰∫ëÁâπÂæÅÈÄöËøáFiLMË∞ÉÂà∂Ê≥®ÂÖ•Ôºå‰∏çÂΩ±ÂìçÊΩúÂú®ËΩ®ËøπÁöÑÂä®ÊÄÅÂπ≥ÊªëÊÄß„ÄÇ\n        *   **ÊâßË°åÊó∂ÁÅµÊ¥ªÈÄÇÂ∫î**ÔºöËÖïÈÉ®Áõ∏Êú∫Á≠âÂÆûÊó∂ÊÑüÂÆò‰ø°ÊÅØ‰ªÖÂú®**‰ΩéÊàêÊú¨Ëß£Á†ÅÂô®**Èò∂ÊÆµÂºïÂÖ•ÔºåÂÖÅËÆ∏ÂØπÂú∫ÊôØÂèòÂåñËøõË°åÂæÆË∞ÉÔºåËÄå‰∏çÊîπÂèòÂ∑≤ÁîüÊàêÁöÑÈ´òÂ±ÇÂπ≥ÊªëËΩ®Ëøπ„ÄÇ\n\n**ÁªìËÆ∫**ÔºöÊú¨ÊñáÂπ∂ÈùûÂØπÂçï‰∏ÄÂâç‰ΩúÁöÑÁÆÄÂçïÊîπËøõÔºåËÄåÊòØÈíàÂØπ‚Äú**Êâ©Êï£Ê®°ÂûãÊÖ¢**‚ÄùÂíå‚Äú**ÂéüÂßãÊµÅÂåπÈÖçÊäñ**‚ÄùËøô‰∏§‰∏™È¢ÜÂüüÊ†∏ÂøÉÁóõÁÇπÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßç**ËûçÂêà‰∫åËÄÖ‰ºòÁÇπ„ÄÅËßÑÈÅøÂÖ∂Áº∫ÁÇπ**ÁöÑÊñ∞Ê°ÜÊû∂„ÄÇÂÖ∂Ê†∏ÂøÉË¥°ÁåÆÂú®‰∫é**ÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÁöÑÊΩúÂú®Á©∫Èó¥‰Ωú‰∏∫‚Äú‰∏≠‰ªã‚Äù**ÔºåÊàêÂäüÂú∞Â∞Ü**ÊµÅÂåπÈÖçÁöÑÊé®ÁêÜÈÄüÂ∫¶**‰∏é**Êâ©Êï£Á∫ßÁöÑË°å‰∏∫Âª∫Ê®°Ë¥®ÈáèÂèäÊâßË°åÁ®≥ÂÆöÊÄß**ÁªìÂêàËµ∑Êù•ÔºåÂπ∂ÈÄöËøáÁ≥ªÁªüÁöÑÂÆûÈ™åÈ™åËØÅ‰∫ÜÂÖ∂Âú®‰ªøÁúüÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰ªªÂä°‰∏äÁöÑÁªºÂêà‰ºòÂäø„ÄÇ",
            "timestamp": "2026-02-03T15:56:43.258665",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ËÆ∫ÊñáÊèêÂá∫‰∫ÜÂì™‰∫õÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïÔºåËØ∑ÂàóË°®Ê†ºÂÖ∑‰ΩìËØ¶ÁªÜËØ¥ÊòéÊäÄÊúØÁªÜËäÇÔºåÈúÄË¶ÅÂåÖÂê´ÂÖ∑‰ΩìÁöÑÊï∞Â≠¶ÂéüÁêÜÊé®ÂØºÔºå‰ª•ÂèäÂÖ∑‰ΩìÂèÇÊï∞„ÄÇ",
            "answer": "Â•ΩÁöÑÔºå‰Ωú‰∏∫‰∏ÄÂêç‰∏ì‰∏öÁöÑÂ≠¶ÊúØËÆ∫ÊñáÂàÜÊûêÂä©ÊâãÔºåÊàëÂ∞Ü‰∏∫ÊÇ®ËØ¶ÁªÜÂàÜÊûêËøôÁØáÈ¢ò‰∏∫„ÄäTemporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation„ÄãÁöÑËÆ∫ÊñáÊâÄÊèêÂá∫ÁöÑÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ï„ÄÇ\n\n## ËÆ∫ÊñáÊ†∏ÂøÉÊñπÊ≥ïÂàÜÊûê\n\nÊú¨ÊñáÁöÑÊ†∏ÂøÉÁõÆÊ†áÊòØËß£ÂÜ≥Êú∫Âô®‰∫∫Ê®°‰ªøÂ≠¶‰π†‰∏≠**Êé®ÁêÜÈÄüÂ∫¶**„ÄÅ**Ë°å‰∏∫Ë°®ËææËÉΩÂäõ**Âíå**ÊâßË°åÁ®≥ÂÆöÊÄß**‰πãÈó¥ÁöÑÊùÉË°°ÈöæÈ¢ò„ÄÇ‰∏∫Ê≠§Ôºå‰ΩúËÄÖÊèêÂá∫‰∫Ü **LG-Flow Policy** Ê°ÜÊû∂ÔºåÂÖ∂ÂÖ≥ÈîÆÊäÄÊúØÂèØÂΩíÁ∫≥‰∏∫‰ª•‰∏ã‰∏â‰∏™Áõ∏‰∫íÂÖ≥ËÅîÁöÑÊ®°ÂùóÔºö\n\n| ÂÖ≥ÈîÆÊäÄÊúØÊ®°Âùó | Ê†∏ÂøÉÁõÆÊ†á | ÊäÄÊúØÁªÜËäÇ‰∏éÊï∞Â≠¶ÂéüÁêÜ | ÂÖ∑‰ΩìÂèÇÊï∞/ËÆæËÆ°ÈÄâÊã© |\n| :--- | :--- | :--- | :--- |\n| **1. Êó∂Â∫èËøûË¥ØÁöÑÊΩúÂú®Âä®‰ΩúË°®Á§∫** | Â∞ÜÂéüÂßãÈ´òÁª¥„ÄÅÊòìÂô™ÁöÑÂä®‰ΩúÂ∫èÂàóÁºñÁ†ÅÂà∞‰∏Ä‰∏™**Âπ≥Êªë„ÄÅ‰ΩéÁª¥„ÄÅÊó∂Â∫èËøûË¥ØÁöÑÊΩúÂú®ÊµÅÂΩ¢**‰∏≠Ôºå‰∏∫ÂêéÁª≠ÁöÑÊµÅÂåπÈÖçÊèê‰æõ‰∏Ä‰∏™Á®≥ÂÆöÁöÑÁîüÊàêÁ©∫Èó¥„ÄÇ | **a. ËΩ®ËøπÁ∫ßÂàÜÂùóÁºñÁ†ÅÔºö** <br> Â∞ÜÈïøÂ∫¶‰∏∫ `H` ÁöÑÂä®‰ΩúËΩ®Ëøπ $\\mathbf{A}_{t:t+H-1} \\in \\mathbb{R}^{H \\times d_a}$ ÂàíÂàÜ‰∏∫ `K` ‰∏™ËøûÁª≠ÁöÑÂùó $\\{\\mathbf{A}^{(1)}, ..., \\mathbf{A}^{(K)}\\}$ÔºåÊØèÂùóÂåÖÂê´ `c` ‰∏™Êó∂Èó¥Ê≠•„ÄÇ <br><br> **b. Âü∫‰∫éGRUÁöÑÊó∂Â∫èÁºñÁ†ÅÂô®Ôºö** <br> ÊØè‰∏™Âä®‰ΩúÂùóÈÄöËøá‰∏Ä‰∏™ËΩªÈáèÁ∫ßÊó∂Â∫èÂç∑ÁßØÁΩëÁªúÊò†Â∞Ñ‰∏∫ÁâπÂæÅÂµåÂÖ• $\\mathbf{x}_k$ÔºåÁÑ∂ÂêéËæìÂÖ•GRU‰ª•ËÅöÂêàÂéÜÂè≤‰ø°ÊÅØÔºåÂº∫Âà∂ÂºïÂÖ•Êó∂Â∫èËøûË¥ØÊÄßÔºö <br> $\\mathbf{h}_k = \\mathrm{GRU}(\\mathbf{x}_k, \\mathbf{h}_{k-1})$ <br> ÊΩúÂú®ÂèòÈáè $\\mathbf{z}_k$ ‰ªé $\\mathbf{h}_k$ ÁöÑÂêéÈ™åÂàÜÂ∏É‰∏≠ÈááÊ†∑„ÄÇ <br><br> **c. ÂèòÂàÜÊ≠£ÂàôÂåñÔºö** <br> ‰ΩøÁî®ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÊ°ÜÊû∂ÔºåÁî®KLÊï£Â∫¶Á∫¶ÊùüÂêéÈ™åÂàÜÂ∏É $q(\\mathbf{z}_k | \\mathbf{A}^{(k)}, \\mathbf{h}_{k-1})$ Êé•ËøëÊ†áÂáÜÊ≠£ÊÄÅÂÖàÈ™å $\\mathcal{N}(0, I)$Ôºå‰ª•ÈºìÂä±ÊΩúÂú®Á©∫Èó¥ÁöÑËøûÁª≠ÊÄßÂíåÁ¥ßÂáëÊÄß„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÂê´**ÈáçÊûÑÊçüÂ§±**Âíå**KLÊ≠£ÂàôÈ°π**„ÄÇ <br><br> **d. ËßÜËßâÊù°‰ª∂Ëß£Á†ÅÔºö** <br> Ëß£Á†ÅÂô® $f_\\theta$ Â∞ÜÊΩúÂú®‰ª£Á†Å $\\mathbf{z}_k$ ‰∏éÊâßË°åÊó∂ÁöÑËßÜËßâÁâπÂæÅ $\\mathbf{v}_t$ÔºàÊù•Ëá™ËÖïÈÉ®Áõ∏Êú∫ÔºâÈÄöËøá **FiLMÔºàFeature-wise Linear ModulationÔºâ** Â±ÇÁªìÂêàÔºåÈáçÂª∫Âä®‰ΩúÂùóÔºö <br> $\\hat{\\mathbf{A}}^{(k)} = f_\\theta(\\mathbf{z}_k | \\mathbf{v}_t)$ <br> FiLMÊìç‰ΩúÔºö$\\mathbf{h}' = \\boldsymbol{\\gamma} \\odot \\mathbf{h} + \\boldsymbol{\\beta}$ÔºåÂÖ∂‰∏≠ $[\\boldsymbol{\\gamma}, \\boldsymbol{\\beta}] = \\mathrm{MLP}(\\mathbf{v}_t)$„ÄÇ | - **ÂàÜÂùóÂ§ßÂ∞èÔºö** Êñá‰∏≠Êú™ÊòéÁ°ÆÁªôÂá∫ `c` Âíå `K` ÁöÑÂÖ∑‰ΩìÂÄºÔºå‰ΩÜÊåáÂá∫È¢ÑÊµãÊú™Êù•4Ê≠•Âä®‰Ωú (`H=4`)„ÄÇ <br> - **ÁºñÁ†ÅÂô®Ôºö** GRU + Êó∂Â∫èÂç∑ÁßØ„ÄÇ <br> - **ÊΩúÂú®Á©∫Èó¥Áª¥Â∫¶Ôºö** ‰Ωé‰∫éÂéüÂßãÂä®‰ΩúÁª¥Â∫¶ $d_a$ÔºåÂÖ∑‰ΩìÂÄºÊú™ÁªôÂá∫„ÄÇ <br> - **Ëß£Á†ÅË∞ÉÂà∂Ôºö** ‰ΩøÁî®FiLMÂ±ÇËûçÂêàËÖïÈÉ®Áõ∏Êú∫ËßÜËßâÁâπÂæÅÔºåÂÆûÁé∞ÊâßË°åÊó∂Ëá™ÈÄÇÂ∫î„ÄÇ |\n| **2. ÊΩúÂú®Á©∫Èó¥ÊµÅÂåπÈÖçÂä®‰ΩúÁîüÊàê** | Âú®Â∑≤Ê≠£ÂàôÂåñÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠Â≠¶‰π†‰∏Ä‰∏™**Á°ÆÂÆöÊÄßÊµÅ**ÔºåÂÆûÁé∞**ÂçïÊ≠•Êé®ÁêÜ**ÁîüÊàêÊï¥‰∏™ÊΩúÂú®Âä®‰ΩúËΩ®ËøπÔºåÂÖºÂÖ∑Êâ©Êï£Ê®°ÂûãÁöÑË°®ËææËÉΩÂäõÂíåÊµÅÂåπÈÖçÁöÑÈÄüÂ∫¶„ÄÇ | **a. ‰∏ÄËá¥ÊÄßÊµÅÂåπÈÖçÔºàCFMÔºâÂÖ¨ÂºèÔºö** <br> Â≠¶‰π†‰∏Ä‰∏™Êó∂Èó¥Áõ∏ÂÖ≥ÁöÑÂêëÈáèÂú∫ $\\boldsymbol{\\nu}_\\theta(t, \\mathbf{z})$ÔºåÂÆö‰πâ‰ªéÁÆÄÂçïÂü∫ÂàÜÂ∏ÉÔºàÂ¶ÇÈ´òÊñØÂô™Â£∞ÔºâÂà∞ÁõÆÊ†áÊΩúÂú®ÂàÜÂ∏ÉÁöÑ**Ê¶ÇÁéáË∑ØÂæÑ**„ÄÇËΩ®Ëøπ $\\xi_\\mathbf{z}(t)$ Áî±ODEÊèèËø∞Ôºö <br> $\\frac{d\\xi_\\mathbf{z}(t)}{dt} = \\boldsymbol{\\nu}_\\theta(t, \\xi_\\mathbf{z}(t)), \\quad \\xi_\\mathbf{z}(0) = \\tilde{\\mathbf{z}} \\sim \\mathcal{N}(0, I)$ <br><br> **b. ÊµÅÂáΩÊï∞ÂèÇÊï∞ÂåñÔºö** <br> ÈááÁî®**‰∏ÄËá¥ÊÄßÊµÅÂåπÈÖç**ÁöÑÂèÇÊï∞ÂåñÂΩ¢ÂºèÔºåÁõ¥Êé•ÂõûÂΩí‰ªéÂô™Â£∞Âà∞ÁõÆÊ†áÊï∞ÊçÆÁöÑÂêëÈáèÂú∫Ôºö <br> $f_\\theta(t, \\mathbf{z}) = \\mathbf{z} + (1-t) \\cdot \\boldsymbol{\\nu}_\\theta(t, \\mathbf{z})$ <br> ÂÖ∂‰∏≠ $t \\in [0,1]$ÔºåÂΩì $t=1$ Êó∂Ôºå$f_\\theta(1, \\tilde{\\mathbf{z}}) \\approx \\mathbf{z}$ÔºàÁõÆÊ†áÊΩúÂú®‰ª£Á†ÅÔºâ„ÄÇÂõ†Â≠ê $(1-t)$ Âú®Êé•ËøëÁõÆÊ†áÊó∂Ë°∞ÂáèÊõ¥Êñ∞ÂπÖÂ∫¶ÔºåÊèêÂçáÁ®≥ÂÆöÊÄß„ÄÇ <br><br> **c. ËæìÂÖ•Ê†áÂáÜÂåñÔºö** <br> ÂºïÂÖ•Êó∂Èó¥‰æùËµñÁöÑËæìÂÖ•Áº©ÊîæÔºå‰ª•Âπ≥Ë°°‰∏çÂêåÊó∂Èó¥Ê≠•ÁöÑÊΩúÂú®Áä∂ÊÄÅÂ∞∫Â∫¶Ôºö <br> $c_{in}(t) = \\frac{1}{\\sqrt{t^2 + (1-t)^2}}$ <br><br> **d. ÂçïÊ≠•ÁîüÊàêÔºö** <br> Âú®Êé®ÁêÜÊó∂Ôºå‰ªéÂü∫ÂàÜÂ∏ÉÈááÊ†∑Âô™Â£∞ÊΩúÂú®‰ª£Á†Å $\\tilde{\\mathbf{z}}_k$ÔºåÈÄöËøá**ÂçïÊ¨°ÂâçÂêë‰º†Êí≠**ÂæóÂà∞ÁõÆÊ†áÊΩúÂú®ËΩ®ËøπÔºö <br> $\\hat{\\mathbf{z}}_k = f_\\theta(1, \\tilde{\\mathbf{z}}_k)$ <br> Êó†ÈúÄËø≠‰ª£ÂéªÂô™ÊàñÊï∞ÂÄºÁßØÂàÜÂæ™ÁéØ„ÄÇ | - **ÊµÅÊ®°ÂûãÔºö** ‰∏ÄËá¥ÊÄßÊµÅÂåπÈÖçÔºàCFMÔºâ„ÄÇ <br> - **Êó∂Èó¥ÂèÇÊï∞Ôºö** $t \\in [0,1]$„ÄÇ <br> - **ÁΩëÁªúËæìÂá∫Ôºö** ÂêëÈáèÂú∫ $\\boldsymbol{\\nu}_\\theta(t, \\mathbf{z})$„ÄÇ <br> - **Êé®ÁêÜÊ≠•È™§Ôºö** **1Ê≠•**„ÄÇ‰∏éÊâ©Êï£Á≠ñÁï•ÔºàÈÄöÂ∏∏ÈúÄ10-20Ê≠•ÔºâÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ |\n| **3. Âá†‰ΩïÊÑüÁü•ÁöÑ3DÂú∫ÊôØÊù°‰ª∂** | ‰∏∫ÊµÅÂåπÈÖçÁΩëÁªúÊèê‰æõ**È´òÊïà‰∏îÂØπÂá†‰ΩïÂèòÂåñÈ≤ÅÊ£í**ÁöÑ3DÂú∫ÊôØË°®Á§∫ÔºåÊèêÂçáÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊìç‰ΩúÊàêÂäüÁéá„ÄÇ | **a. ÁÇπ‰∫ëÈ¢ÑÂ§ÑÁêÜÔºö** <br> ‰ªéÂÖ®Â±ÄÊ∑±Â∫¶Áõ∏Êú∫Ëé∑ÂèñÁÇπ‰∫ëÔºåË£ÅÂâ™Â∑•‰ΩúÁ©∫Èó¥Ôºå‰ΩøÁî®**ÊúÄËøúÁÇπÈááÊ†∑ÔºàFPSÔºâ** ÈÄâÂèñ `N` ‰∏™‰∏≠ÂøÉÁÇπ„ÄÇ <br><br> **b. ÂèåÂàÜÊîØÁºñÁ†ÅÂô®Ôºö** <br> - **Â±ÄÈÉ®ÁºñÁ†ÅÂô® $f_l$Ôºö** ‰ª•ÊØè‰∏™FPS‰∏≠ÂøÉÁÇπ‰∏∫‰∏≠ÂøÉÊûÑÂª∫Â±ÄÈÉ®ÈÇªÂüüÔºåÈÄöËøá**ÊÆãÂ∑ÆÂç∑ÁßØ**ÁªìÂêà**ÊúÄÂ§ß-ÂùáÂÄºÊ±†Âåñ**ÊèêÂèñÂ±ÄÈÉ®Âá†‰ΩïÁâπÂæÅ $\\mathbf{f}_l$„ÄÇËØ•ÁâπÂæÅÂØπÂÖ®Â±ÄÂπ≥ÁßªÂÖ∑Êúâ‰∏çÂèòÊÄß„ÄÇ <br> - **‰∏≠ÂøÉÁºñÁ†ÅÂô® $f_c$Ôºö** Â∞ÜFPSÈááÊ†∑ÁöÑ‰∏≠ÂøÉÁÇπÂùêÊ†áÈÄöËøá‰∏Ä‰∏™ËΩªÈáèÁ∫ßMLPÔºåÊò†Â∞Ñ‰∏∫ÂÖ®Â±ÄÂú∫ÊôØ‰∏ä‰∏ãÊñáÁâπÂæÅ $\\mathbf{f}_c$„ÄÇ <br> $\\mathbf{f}_l = f_l(\\text{pcd}_{\\text{local}}), \\quad \\mathbf{f}_c = f_c(\\text{pcd}_{\\text{center}})$ <br><br> **c. ÂàÜÂ±ÇÁâπÂæÅË∞ÉÂà∂ÔºàFiLMÔºâÔºö** <br> Â∞ÜÂá†‰ΩïÁâπÂæÅÂàÜ‰∏§Â±ÇÊ≥®ÂÖ•ÊµÅÂåπÈÖçÁΩëÁªúÔºö <br> 1. **Â±ÄÈÉ®Âá†‰ΩïË∞ÉÂà∂Ôºö** $\\mathbf{h}' = \\boldsymbol{\\gamma}_l \\odot \\mathbf{h} + \\boldsymbol{\\beta}_l, \\quad [\\boldsymbol{\\gamma}_l, \\boldsymbol{\\beta}_l] = \\mathrm{MLP}_l(\\mathbf{f}_l)$ <br> 2. **ÂÖ®Â±Ä‰∏ä‰∏ãÊñáË∞ÉÂà∂Ôºö** $\\mathbf{h}'' = \\boldsymbol{\\gamma}_c \\odot \\mathbf{h}' + \\boldsymbol{\\beta}_c, \\quad [\\boldsymbol{\\gamma}_c, \\boldsymbol{\\beta}_c] = \\mathrm{MLP}_c(\\mathbf{f}_c)$ <br> **ÂÖàÂ±ÄÈÉ®ÂêéÂÖ®Â±Ä**ÁöÑÈ°∫Â∫èË¢´ËØÅÊòéÊõ¥ÊúâÊïà„ÄÇ | - **ÁÇπ‰∫ëËßÑÊ®°Ôºö** ‰∏ãÈááÊ†∑Ëá≥512‰∏™ÁÇπÔºàFPSÔºâ„ÄÇ <br> - **Â±ÄÈÉ®ÈÇªÂüüÔºö** Âõ¥ÁªïÊØè‰∏™FPS‰∏≠ÂøÉÁÇπÊûÑÂª∫„ÄÇ <br> - **ÁºñÁ†ÅÂô®ÁªìÊûÑÔºö** ÊÆãÂ∑ÆÂç∑ÁßØÔºàÂ±ÄÈÉ®Ôºâ+ MLPÔºàÂÖ®Â±ÄÔºâ„ÄÇ <br> - **Êù°‰ª∂Ê≥®ÂÖ•ÊñπÂºèÔºö** ÂàÜÂ±ÇFiLM„ÄÇ |\n\n## ÊäÄÊúØÂàõÊñ∞‰∏éÂÆûÈôÖ‰ª∑ÂÄºÊÄªÁªì\n\n1.  **Ê†∏ÂøÉÂàõÊñ∞ÁÇπ**Ôºö**‚ÄúÂú®Êó∂Â∫èÊ≠£ÂàôÂåñÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠ËøõË°åÊµÅÂåπÈÖç‚Äù**„ÄÇËøôÂπ∂ÈùûÁÆÄÂçï‰∏≤ËÅîVAEÂíåCFMÔºåËÄåÊòØÈÄöËøá**GRUÁºñÁ†ÅÂô®**Âíå**ËΩ®ËøπÁ∫ßÂèòÂàÜÊ≠£ÂàôÂåñ**Ôºå**‰∏ªÂä®Â°ëÈÄ†**Âá∫‰∏Ä‰∏™ÈÄÇÂêàÊµÅÂåπÈÖçÁöÑÂπ≥ÊªëÊΩúÂú®ÊµÅÂΩ¢Ôºå‰ªéÊ†πÊú¨‰∏äËß£ÂÜ≥‰∫ÜÂéüÂßãÂä®‰ΩúÁ©∫Èó¥ÊµÅÂåπÈÖçÁöÑÊäñÂä®ÈóÆÈ¢ò„ÄÇ\n\n2.  **Êï∞Â≠¶ÂéüÁêÜÁöÑÂ∑ßÂ¶ôÁªìÂêà**Ôºö\n    *   Â∞Ü**ÂèòÂàÜÊé®Êñ≠**ÔºàÁî®‰∫éÂ≠¶‰π†Âπ≥ÊªëÊΩúÂú®Á©∫Èó¥Ôºâ‰∏é**ÊµÅÂåπÈÖç**ÔºàÁî®‰∫éÈ´òÊïàÁ°ÆÂÆöÊÄßÁîüÊàêÔºâÁõ∏ÁªìÂêà„ÄÇ\n    *   ÊµÅÂåπÈÖçÈÉ®ÂàÜÈááÁî®**‰∏ÄËá¥ÊÄßÊµÅÂåπÈÖçÔºàCFMÔºâ** ÂÖ¨ÂºèÔºåÂÖ∂ÊçüÂ§±ÂáΩÊï∞Êó®Âú®‰Ωø‰ªé‰∏çÂêåÂô™Â£∞Ê∞¥Âπ≥Âá∫ÂèëÁöÑÊµÅËΩ®ËøπÂú®Áõ∏ÂêåÊó∂Èó¥ÁÇπÊ±áËÅöÂà∞Âêå‰∏ÄÁÇπÔºåËøôËá™ÁÑ∂‰øÉËøõ‰∫ÜÂçïÊ≠•Êé®ÁêÜÁöÑÂèØË°åÊÄß„ÄÇ\n\n3.  **Á≥ªÁªüÁ∫ßËÆæËÆ°‰ª∑ÂÄº**Ôºö\n",
            "timestamp": "2026-02-03T15:57:53.654749",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "‰ªñ‰ΩøÁî®‰∫ÜÂì™‰∫õËØÑ‰ª∑ÊåáÊ†á‰∏éÊï∞ÊçÆÈõÜÔºåÂàóË°®Ê†ºÂÖ∑‰ΩìËÆ≤ËÆ≤‰ªñÁöÑËØÑ‰ª∑ÊåáÊ†áÁöÑÁªÜËäÇ‰∏éÊï∞ÊçÆÈõÜÁöÑÁªÜËäÇ",
            "answer": "## ËÆ∫ÊñáËØÑ‰ª∑ÊåáÊ†á‰∏éÊï∞ÊçÆÈõÜÂàÜÊûê\n\n### ‰∏Ä„ÄÅ ËØÑ‰ª∑ÊåáÊ†á\n\nËÆ∫Êñá‰ΩøÁî®‰∫Ü‰∏â‰∏™Ê†∏ÂøÉÊåáÊ†áÊù•ÂÖ®Èù¢ËØÑ‰º∞Á≠ñÁï•ÁöÑÊÄßËÉΩÔºåÊ∂µÁõñ‰∫Ü‰ªªÂä°ÊàêÂäüÁéá„ÄÅÂÆûÊó∂ÊïàÁéáÂíåÊâßË°åË¥®Èáè„ÄÇ\n\n| ÊåáÊ†áÁ±ªÂà´ | ÂÖ∑‰ΩìÊåáÊ†á | ËÆ°ÁÆóÊñπÂºè‰∏éÁªÜËäÇ | ÁõÆÁöÑ‰∏éÊÑè‰πâ |\n| :--- | :--- | :--- | :--- |\n| **‰ªªÂä°ÊÄßËÉΩ** | **‰ªªÂä°ÊàêÂäüÁéá** | - **ÂÆö‰πâ**ÔºöÂú®‰ªªÂä°Êó∂Èó¥ËåÉÂõ¥ÂÜÖÔºåÊª°Ë∂≥‰ªªÂä°ÁâπÂÆöÊàêÂäüÊ†áÂáÜÁöÑËØïÈ™åÁôæÂàÜÊØî„ÄÇ<br>- **Âà§ÂÆöÊ†áÂáÜ**ÔºöÂü∫‰∫éÈ¢ÑÂÆö‰πâÁöÑÂá†‰ΩïÁ∫¶ÊùüÔºå‰æãÂ¶ÇÁâ©‰Ωì‰ΩçÂßøËØØÂ∑Æ‰Ωé‰∫éÈòàÂÄº„ÄÅÁ®≥ÂÆöÊäìÂèñÊàñÊîæÁΩÆ‰øùÊåÅÊúÄÂ∞èÊåÅÁª≠Êó∂Èó¥„ÄÇ | Ë°°ÈáèÁ≠ñÁï•ÂÆåÊàêÊåáÂÆö‰ªªÂä°ÁöÑÊ†πÊú¨ËÉΩÂäõÔºåÊòØÊúÄÁõ¥Êé•ÁöÑÊÄßËÉΩÊåáÊ†á„ÄÇ |\n| **ÂÆûÊó∂ÊïàÁéá** | **ÂìçÂ∫îÊó∂Èó¥** | - **ÂÆö‰πâ**Ôºö‰ªéÊé•Êî∂‰º†ÊÑüÂô®ËßÇÊµãÂà∞ËæìÂá∫ÊéßÂà∂Âä®‰ΩúÁöÑÁ´ØÂà∞Á´ØÊé®ÁêÜÂª∂Ëøü„ÄÇ<br>- **ÊµãÈáè**Ôºö‰ªÖÂåÖÂê´Á≠ñÁï•Êé®ÁêÜÊó∂Èó¥Ôºå**ÊéíÈô§**‰º†ÊÑüÂô®ÈááÈõÜÂíåÁéØÂ¢ÉÊ≠•ËøõÊó∂Èó¥„ÄÇ | ËØÑ‰º∞Á≠ñÁï•ÁöÑÂÆûÊó∂ËÆ°ÁÆóÊïàÁéáÂíåÈó≠ÁéØÂìçÂ∫îËÉΩÂäõÔºåÂØπÁâ©ÁêÜÊú∫Âô®‰∫∫ÈÉ®ÁΩ≤Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ |\n| **ÊâßË°åË¥®Èáè** | **ËΩ®ËøπÂπ≥ÊªëÂ∫¶** | - **ÁªºÂêàÊåáÊ†á**ÔºöÁªìÂêàÊó∂ÂüüÂíåÈ¢ëÂüüÁâπÊÄßÔºåÂÆö‰πâ‰∏∫ `ùíÆ_smooth = Œ±¬∑ùíÆ_jerk + Œ≤¬∑ùíÆ_freq`Ôºå**ÂÄºË∂ä‰ΩéË°®Á§∫ËΩ®ËøπË∂äÂπ≥Êªë**„ÄÇ<br> - **Êó∂ÂüüÂπ≥ÊªëÂ∫¶ (ùíÆ_jerk)**ÔºöËÆ°ÁÆóÂÖ≥ËäÇÂä®‰ΩúÂ∫èÂàóÁöÑÁ¶ªÊï£Âä†Âä†ÈÄüÂ∫¶ÔºàjerkÔºâÁöÑÂùáÊñπÂÄºÔºåÂèçÊò†Âä®‰ΩúÁöÑÁ™ÅÂèòÁ®ãÂ∫¶„ÄÇÂÖ¨ÂºèÔºö`ùê£_t = (ùêö_{t+1} - 3ùêö_t + 3ùêö_{t-1} - ùêö_{t-2}) / (Œît)^3`Ôºõ`ùíÆ_jerk = mean(‚Äñùê£_t‚Äñ¬≤)`„ÄÇ<br> - **È¢ëÂüüÂπ≥ÊªëÂ∫¶ (ùíÆ_freq)**ÔºöËÆ°ÁÆóÂÖ≥ËäÇËΩ®ËøπÈ´òÈ¢ëËÉΩÈáèÂç†ÊÄªËÉΩÈáèÁöÑÊØî‰æãÔºåÂèçÊò†‰∏çÂøÖË¶ÅÁöÑÈ´òÈ¢ëÊåØËç°„ÄÇÂÖ¨ÂºèÔºö`ùíÆ_freq = Œ£_{f>f_c} ‚ÄñùêöÃÇ_f‚Äñ¬≤ / Œ£_f ‚ÄñùêöÃÇ_f‚Äñ¬≤`ÔºåÂÖ∂‰∏≠ `f_c` ‰∏∫Êà™Ê≠¢È¢ëÁéá„ÄÇ<br>- **ÊùÉÈáç**ÔºöÊ†πÊçÆÁªèÈ™åËØÑ‰º∞ÂíåÂΩí‰∏ÄÂåñÔºåËÆæÁΩÆ‰∏∫ `Œ± = 0.25`, `Œ≤ = 0.75`ÔºåÊõ¥‰æßÈáçÈ¢ëÂüüÂπ≥ÊªëÂ∫¶„ÄÇ | ÈáèÂåñÈïøÊó∂Á®ãÊâßË°åÁöÑÁ®≥ÂÆöÊÄß„ÄÇÂπ≥ÊªëÁöÑËΩ®ËøπËÉΩÂáèÂ∞ëÊú∫Ê¢∞Á£®Êçü„ÄÅÊèêÈ´òÊâßË°åÂèØÈù†ÊÄßÔºåÊòØÊú¨ÊñáËß£ÂÜ≥**ÂéüÂßãÂä®‰ΩúÁ©∫Èó¥ÊµÅÂåπÈÖçÂØºËá¥ËΩ®ËøπÊäñÂä®**ÈóÆÈ¢òÁöÑÊ†∏ÂøÉÈ™åËØÅÊåáÊ†á„ÄÇ |\n\n### ‰∫å„ÄÅ Êï∞ÊçÆÈõÜ‰∏éÂÆûÈ™åÂπ≥Âè∞\n\nËÆ∫ÊñáÂú®‰ªøÁúüÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰∏§‰∏™Âπ≥Âè∞‰∏äËøõË°å‰∫ÜÂÖ®Èù¢ËØÑ‰º∞„ÄÇ\n\n| ÂÆûÈ™åÁéØÂ¢É | Êï∞ÊçÆÈõÜ/Âπ≥Âè∞ÂêçÁß∞ | ÁªÜËäÇÊèèËø∞ | ‰ªªÂä°‰∏éÊï∞ÊçÆ |\n| :--- | :--- | :--- | :--- |\n| **‰ªøÁúüÂÆûÈ™å** | **Adroit** | - **Êù•Ê∫ê**ÔºöÂπøÊ≥õ‰ΩøÁî®ÁöÑÁÅµÂ∑ßÊìç‰ΩúÂü∫ÂáÜ„ÄÇ<br>- **ÁâπÁÇπ**Ôºö‰∏ìÊ≥®‰∫é**Êé•Ëß¶‰∏∞ÂØåÁöÑÁÅµÂ∑ßÊìç‰Ωú**Ôºå‰ΩøÁî®È´òËá™Áî±Â∫¶ÁöÑÊãü‰∫∫ÂåñÊâãÔºàShadow HandÔºâ„ÄÇ<br>- **‰ªøÁúüÂô®**ÔºöMuJoCoÁâ©ÁêÜÂºïÊìé„ÄÇ | - **‰ªªÂä°ËåÉÂõ¥**ÔºöÂåÖÂê´Â§öÁßçÁÅµÂ∑ßÊìç‰Ωú‰ªªÂä°„ÄÇ<br>- **‰∏ìÂÆ∂Êï∞ÊçÆ**Ôºö‰ΩøÁî®Ë∞É‰ºòÂ•ΩÁöÑÂêØÂèëÂºèÁ≠ñÁï•Êî∂ÈõÜÔºåÁ°Æ‰øùÊï∞ÊçÆË¥®Èáè‰∏ÄËá¥„ÄÇÊØè‰∏™‰ªªÂä°Êî∂ÈõÜ**30Êù°**ÊºîÁ§∫„ÄÇ |\n| | **Meta-World** | - **Êù•Ê∫ê**ÔºöÂπøÊ≥õ‰ΩøÁî®ÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÂü∫ÂáÜ„ÄÇ<br>- **ÁâπÁÇπ**ÔºöÂåÖÂê´**Â§öÊ†∑Âåñ„ÄÅ‰∏çÂêåÈöæÂ∫¶**ÁöÑÂü∫‰∫éÊú∫Ê¢∞ËáÇÁöÑÊìç‰Ωú‰ªªÂä°„ÄÇ<br>- **‰ªøÁúüÂô®**ÔºöMuJoCoÁâ©ÁêÜÂºïÊìé„ÄÇ | - **‰ªªÂä°ËåÉÂõ¥**ÔºöÊ∂µÁõñÂ§öÁßçËáÇÂºèÊìç‰Ωú‰ªªÂä°„ÄÇ‰∏éAdroitÂêàËÆ°ÂÖ±**37‰∏™**‰ªªÂä°„ÄÇ<br>- **‰∏ìÂÆ∂Êï∞ÊçÆ**ÔºöÊî∂ÈõÜÊñπÂºèÂêåAdroitÔºåÊØè‰∏™‰ªªÂä°**30Êù°**ÊºîÁ§∫„ÄÇ |\n| **ÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™å** | **Ëá™ÂÆö‰πâÁúüÂÆûÂπ≥Âè∞** | - **Êú∫Âô®‰∫∫**ÔºöFranka Emika Panda Êú∫Ê¢∞ËáÇÔºåÈÖçÂ§áÂèØ‰∫íÊç¢ÁöÑÊú´Á´ØÊâßË°åÂô®ÔºàLEAP Hand ÊàñÂπ≥Ë°åÂ§πÁà™Ôºâ„ÄÇ<br>- **ÊÑüÁü•Á≥ªÁªü**Ôºö<br> 1. **ÂÖ®Â±ÄÁõ∏Êú∫**ÔºöRealSense L515ÔºåÊèê‰æõÂú∫ÊôØÁ∫ßRGB-DËßÇÊµã„ÄÇ<br> 2. **ËÖïÈÉ®Áõ∏Êú∫**ÔºöRealSense D435ÔºåÊèê‰æõÊâßË°åÊó∂Â±ÄÈÉ®ËßÜËßâÂèçÈ¶à„ÄÇ<br>- **ÈÖçÁΩÆ**ÔºöÂèåÁõ∏Êú∫Âùá‰ª•640x480ÂàÜËæ®Áéá„ÄÅ30fpsËøêË°å„ÄÇÊ∑±Â∫¶ÂõæÈÄöËøáÊ†áÂÆöÁöÑÂÜÖÂ§ñÂèÇËΩ¨Êç¢‰∏∫ÁÇπ‰∫ë„ÄÇ | - **‰ªªÂä°**ÔºöËØÑ‰º∞‰∫Ü3Á±ªÊìç‰Ωú‰ªªÂä°Ôºö<br> 1. **ÊäìÂèñ-ÊîæÁΩÆ**ÔºöÂàÜÂà´‰ΩøÁî®LEAP HandÂíåÂπ≥Ë°åÂ§πÁà™ÊâßË°å„ÄÇ<br> 2. **ÊèíÂ≠î**ÔºöÈúÄË¶ÅÁ≤æÁ°ÆÊèíÂÖ•„ÄÇ<br> 3. **ÈÅøÈöúÊäìÂèñ**ÔºöÂú®ÈÅøÂºÄÂë®Âõ¥ÈöúÁ¢çÁâ©ÁöÑÂêåÊó∂ÊäìÂèñÁõÆÊ†áÁâ©‰Ωì„ÄÇ<br>- **‰∏ìÂÆ∂Êï∞ÊçÆ**ÔºöÈÄöËøá**‰∫∫Á±ªÈÅ•Êìç‰Ωú**Êî∂ÈõÜÔºåÊØè‰∏™‰ªªÂä°**30Êù°**ÊºîÁ§∫„ÄÇ |\n\n### ‰∏â„ÄÅ ËØÑ‰º∞ÂçèËÆÆÁªÜËäÇ\n- **‰ªøÁúüËØÑ‰º∞**ÔºöÊØè‰∏™‰ªªÂä°ËøõË°å**50Ê¨°** rollout Êù•ËØÑ‰º∞ÊÄßËÉΩ„ÄÇ\n- **ÁúüÂÆû‰∏ñÁïåËØÑ‰º∞**ÔºöÊØè‰∏™‰ªªÂä°ËøõË°å**3Ê¨°**Áã¨Á´ãËøêË°åÔºà‰∏çÂêåÈöèÊú∫ÁßçÂ≠êÔºâÔºåÊØèÊ¨°ÂåÖÂê´**10Ê¨°**ËØïÈ™åÔºåÂÖ±ËÆ°**30Ê¨°**ËØïÈ™å/‰ªªÂä°ÔºåÊä•ÂëäÂπ≥ÂùáÊàêÂäüÁéáÂèäÊ†áÂáÜÂ∑Æ„ÄÇ\n- **ËßÇÊµã‰∏éÈ¢ÑÊµã**ÔºöÊâÄÊúâÊñπÊ≥ïÂùáÂü∫‰∫é**ÊúÄËøë‰∏§Ê¨°ËßÇÊµã**ÔºåÈ¢ÑÊµã**ÂêéÁª≠ÂõõÊ≠•**ÁöÑÂä®‰Ωú„ÄÇ\n- **ËæìÂÖ•Â§ÑÁêÜ**ÔºöRGBÂõæÂÉèË£ÅÂâ™Ëá≥84x84ÔºõÁÇπ‰∫ë‰ΩøÁî®ÊúÄËøúÁÇπÈááÊ†∑‰∏ãÈááÊ†∑Ëá≥512‰∏™ÁÇπ„ÄÇ\n\n### ÊÄªÁªì\nÊú¨ÊñáÁöÑËØÑ‰ª∑‰ΩìÁ≥ªËÆæËÆ°Á≤æËâØÔºö\n1.  **ÊåáÊ†áÂÖ®Èù¢**ÔºöÂêåÊó∂Ë°°Èáè‰∫Ü**ÊïàÊûú**ÔºàÊàêÂäüÁéáÔºâ„ÄÅ**ÊïàÁéá**ÔºàÂìçÂ∫îÊó∂Èó¥ÔºâÂíå**Ë¥®Èáè**ÔºàÂπ≥ÊªëÂ∫¶ÔºâÔºåÁõ¥ÂáªÊú∫Âô®‰∫∫ÈÉ®ÁΩ≤ÁöÑÊ†∏ÂøÉÁóõÁÇπ„ÄÇ\n2.  **Âπ≥ÊªëÂ∫¶ÊåáÊ†áÂàõÊñ∞**ÔºöÊèêÂá∫ÁöÑÂ§çÂêàÂπ≥ÊªëÂ∫¶ÊåáÊ†áÔºàÁªìÂêàjerkÂíåÈ¢ëÂüüËÉΩÈáèÔºâÊØîÂçï‰∏ÄÊåáÊ†áÊõ¥ËÉΩÂèØÈù†Âú∞ÈáèÂåñËΩ®ËøπÁöÑÁ®≥ÂÆöÊÄßÂíåËøûË¥ØÊÄßÔºåÊòØÈ™åËØÅÂÖ∂**ÊΩúÂú®Âä®‰ΩúÊµÅÂåπÈÖç**ÊäÄÊúØ‰ºòÂäøÁöÑÂÖ≥ÈîÆ„ÄÇ\n3.  **Êï∞ÊçÆÈõÜÂÖ∑Êúâ‰ª£Ë°®ÊÄß**ÔºöÊ∂µÁõñ‰∫Ü‰ªé‰ªøÁúüÂà∞ÁúüÂÆû„ÄÅ‰ªéÁÅµÂ∑ßÊâãÂà∞ÁÆÄÂçïÂ§πÁà™„ÄÅ‰ªéÂ∏∏ËßÑ‰ªªÂä°Âà∞Á≤æÂØÜ‰ªªÂä°ÔºàÊèíÂ≠îÔºâÁöÑÂ§öÁßçÂú∫ÊôØÔºåËØÅÊòé‰∫ÜÊñπÊ≥ïÁöÑ**Ê≥õÂåñÊÄßÂíåÈ≤ÅÊ£íÊÄß**„ÄÇ\n4.  **ÂØπÊØîÂÖ¨Âπ≥**ÔºöÂú®Áõ∏ÂêåÁöÑÊï∞ÊçÆÈõÜ„ÄÅÊºîÁ§∫Êï∞Èáè„ÄÅÁ°¨‰ª∂Âπ≥Âè∞ÂíåËæìÂÖ•ÈÖçÁΩÆ‰∏ã‰∏é‰∏ªÊµÅÂü∫Á∫øËøõË°åÂØπÊØîÔºåÂ¢ûÂº∫‰∫ÜÁªìÊûúÁöÑÂèØ‰ø°Â∫¶„ÄÇ",
            "timestamp": "2026-02-03T15:58:40.722902",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ËÆ∫ÊñáÂú®Âì™‰∫õÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºü‰∏ªË¶ÅÁöÑËØÑ‰º∞ÊåáÊ†áÂíåÊÄßËÉΩÊèêÂçáÊòØÂ§öÂ∞ëÔºü",
            "answer": "## ËÆ∫ÊñáÂÆûÈ™åÊï∞ÊçÆÈõÜ„ÄÅËØÑ‰º∞ÊåáÊ†á‰∏éÊÄßËÉΩÊèêÂçáÂàÜÊûê\n\n### ‰∏Ä„ÄÅ ÂÆûÈ™åÊï∞ÊçÆÈõÜ\nËÆ∫ÊñáÂú®**‰ªøÁúü**Âíå**ÁúüÂÆûÊú∫Âô®‰∫∫**‰∏§‰∏™Âπ≥Âè∞‰∏äËøõË°å‰∫ÜÂÖ®Èù¢ÂÆûÈ™åÔºåÂÖ∑‰ΩìÊï∞ÊçÆÈõÜ/‰ªªÂä°Â¶Ç‰∏ãÔºö\n\n#### 1. ‰ªøÁúüÂÆûÈ™åÊï∞ÊçÆÈõÜ\n- **Adroit**Ôºö‰∏ìÊ≥®‰∫é**È´òÁª¥ÁÅµÂ∑ßÊâã**ÁöÑÊé•Ëß¶ÂºèÊìç‰Ωú‰ªªÂä°ÔºàÂ¶ÇÁâ©‰ΩìÊóãËΩ¨„ÄÅÊäìÊè°Á≠âÔºâ„ÄÇ\n- **Meta-World**ÔºöÂåÖÂê´Â§öÁßç**Êú∫Ê¢∞ËáÇ**Êìç‰Ωú‰ªªÂä°ÔºåÈöæÂ∫¶ÂêÑÂºÇ„ÄÇ\n- **ÊÄªËÆ°**ÔºöË¶ÜÁõñ **37 ‰∏™**‰∏çÂêåÁöÑÊìç‰Ωú‰ªªÂä°ÔºåÂü∫‰∫é MuJoCo Áâ©ÁêÜÊ®°ÊãüÂô®ÊûÑÂª∫„ÄÇ\n- **Êï∞ÊçÆÊù•Ê∫ê**Ôºö‰ΩøÁî®Ë∞É‰ºòÁöÑÂêØÂèëÂºèÁ≠ñÁï•Êî∂ÈõÜ‰∏ìÂÆ∂ÊºîÁ§∫ÔºàÊØè‰∏™‰ªªÂä° 30 Êù°ËΩ®ËøπÔºâ„ÄÇ\n\n#### 2. ÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™åÂπ≥Âè∞‰∏é‰ªªÂä°\n- **Á°¨‰ª∂Âπ≥Âè∞**ÔºöFranka Emika Panda Êú∫Ê¢∞ËáÇÔºåÈÖçÂ§á **LEAP Hand**ÔºàÁÅµÂ∑ßÊâãÔºâÊàñ**Âπ≥Ë°åÂ§πÁà™**„ÄÇ\n- **ÊÑüÁü•Á≥ªÁªü**Ôºö\n    - ÂÖ®Â±ÄÁõ∏Êú∫ÔºöRealSense L515ÔºàÊèê‰æõÂú∫ÊôØ RGB-D ËßÇÊµãÔºâ„ÄÇ\n    - ËÖïÈÉ®Áõ∏Êú∫ÔºöRealSense D435ÔºàÊèê‰æõÊâßË°åÊó∂ËßÜËßâÂèçÈ¶àÔºâ„ÄÇ\n- **ËØÑ‰º∞‰ªªÂä°**ÔºàÂÖ± 4 Á±ªÔºâÔºö\n    1. **Pick-and-Place**Ôºà‰ΩøÁî®ÁÅµÂ∑ßÊâãÔºâ„ÄÇ\n    2. **Pick-and-Place**Ôºà‰ΩøÁî®Âπ≥Ë°åÂ§πÁà™Ôºâ„ÄÇ\n    3. **Peg-in-Hole**ÔºàÈúÄË¶ÅÁ≤æÁ°ÆÊèíÂÖ•Ôºâ„ÄÇ\n    4. **Obstacle-Avoidance Grasping**ÔºàÂú®ÈöúÁ¢çÁâ©ÁéØÂ¢É‰∏≠ÊäìÂèñÁõÆÊ†áÁâ©‰ΩìÔºâ„ÄÇ\n- **Êï∞ÊçÆÊù•Ê∫ê**ÔºöÈÄöËøá**‰∫∫Á±ªÈÅ•Êìç‰Ωú**Êî∂ÈõÜ‰∏ìÂÆ∂ÊºîÁ§∫ÔºàÊØè‰∏™‰ªªÂä° 30 Êù°ËΩ®ËøπÔºâ„ÄÇ\n\n### ‰∫å„ÄÅ ‰∏ªË¶ÅËØÑ‰º∞ÊåáÊ†á\nËÆ∫ÊñáÈááÁî®‰∏â‰∏™Ê†∏ÂøÉÊåáÊ†áÔºåÂÖ®Èù¢ËØÑ‰º∞Á≠ñÁï•ÁöÑÊÄßËÉΩ„ÄÅÊïàÁéá‰∏éÊâßË°åË¥®ÈáèÔºö\n\n| ÊåáÊ†á | ÂÆö‰πâ‰∏éËÆ°ÁÆóÊñπÂºè | Áâ©ÁêÜÊÑè‰πâ |\n| :--- | :--- | :--- |\n| **‰ªªÂä°ÊàêÂäüÁéá** | Âú® episode ÂÜÖÊª°Ë∂≥‰ªªÂä°ÁâπÂÆöÊàêÂäüÊù°‰ª∂ÁöÑËØïÈ™åÁôæÂàÜÊØîÔºàÂ¶ÇÁâ©‰Ωì‰ΩçÂßøËØØÂ∑Æ‰Ωé‰∫éÈòàÂÄº„ÄÅÁ®≥ÂÆöÊäìÊè°/ÊîæÁΩÆÁª¥ÊåÅÊúÄÂ∞èÊó∂ÈïøÔºâ„ÄÇ | Ë°°ÈáèÁ≠ñÁï•ÂÆåÊàê‰ªªÂä°ÁöÑ**ÊúâÊïàÊÄß**Âíå**ÂèØÈù†ÊÄß**„ÄÇ |\n| **ÂìçÂ∫îÊó∂Èó¥** | ‰ªéÊé•Êî∂‰º†ÊÑüÂô®ËßÇÊµãÂà∞ËæìÂá∫ÊéßÂà∂Âä®‰ΩúÁöÑ**Á´ØÂà∞Á´ØÊé®ÁêÜÂª∂Ëøü**ÔºàÊØ´ÁßíÔºâ„ÄÇ | Ë°°ÈáèÁ≠ñÁï•ÁöÑ**ÂÆûÊó∂ËÆ°ÁÆóÊïàÁéá**Âíå**Èó≠ÁéØÂìçÂ∫îËÉΩÂäõ**„ÄÇ |\n| **ËΩ®ËøπÂπ≥ÊªëÂ∫¶** | ÁªºÂêàÊó∂ÂüüÂíåÈ¢ëÂüüÁâπÂæÅÁöÑÂ§çÂêàÊåáÊ†áÔºåËÆ°ÁÆóÂÖ¨ÂºèÔºö<br> `ùíÆ_smooth = Œ± * ùíÆ_jerk + Œ≤ * ùíÆ_freq` <br> - **ùíÆ_jerk**ÔºöÂÖ≥ËäÇËΩ®ËøπÂä†Âä†ÈÄüÂ∫¶ÔºàjerkÔºâÁöÑÂùáÊñπÂÄºÔºàË°°ÈáèÊó∂ÂüüÁ™ÅÂèòÔºâ„ÄÇ<br> - **ùíÆ_freq**ÔºöÂÖ≥ËäÇËΩ®ËøπÈ´òÈ¢ëËÉΩÈáèÂç†ÊØîÔºàË°°ÈáèÈ¢ëÂüüÊåØËç°Ôºâ„ÄÇ<br> - ËÆ∫ÊñáËÆæÂÆö `Œ±=0.25`, `Œ≤=0.75`„ÄÇ | **ÂÄºË∂ä‰ΩéË°®Á§∫ËΩ®ËøπË∂äÂπ≥Êªë**„ÄÇÁõ¥Êé•ÂèçÊò†ÈïøÊúüÊâßË°å‰∏≠ÁöÑ**Á®≥ÂÆöÊÄß**Âíå**ÊäñÂä®Á®ãÂ∫¶**ÔºåÂØπÁúüÂÆûÊú∫Âô®‰∫∫ÈÉ®ÁΩ≤Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ |\n\n### ‰∏â„ÄÅ ‰∏ªË¶ÅÊÄßËÉΩÊèêÂçáÔºàLG-Flow Policy vs. Âü∫Á∫øÊñπÊ≥ïÔºâ\n\n#### 1. ‰ªøÁúüÂÆûÈ™åÁªìÊûúÔºàÂÖ≥ÈîÆÊï∞ÊçÆÔºâ\n- **ÂØπÊØîÂü∫Á∫ø**ÔºöDP3Ôºà‰∏ªË¶ÅÊâ©Êï£Âü∫Á∫øÔºâ„ÄÅFlow PolicyÔºàÂéüÂßãÂä®‰ΩúÁ©∫Èó¥ÊµÅÂåπÈÖçÔºâ„ÄÅRDPÔºàÂàÜÂ±ÇÊâ©Êï£Á≠ñÁï•ÔºâÁ≠â„ÄÇ\n- **ÊÄßËÉΩÊèêÂçá**Ôºö\n    - **Âπ≥Âùá‰ªªÂä°ÊàêÂäüÁéá**ÔºöËææÂà∞ **78.3%**ÔºåÁõ∏ÊØî DP3 ÊèêÂçá **+12.6%**ÔºåÁõ∏ÊØî Flow Policy ÊèêÂçá **+17.3%**„ÄÇ\n    - **ËΩ®ËøπÂπ≥ÊªëÂ∫¶**ÔºöÂπ≥ÂùáÂπ≥ÊªëÂ∫¶ÂæóÂàÜ **0.052**ÔºåÁõ∏ÊØî RDP Èôç‰Ωé **51.4%**ÔºåÁõ∏ÊØî DP3 Èôç‰Ωé **69.4%**ÔºåÁõ∏ÊØî Flow Policy Â§ßÂπÖÈôç‰Ωé **77.2%**„ÄÇ\n    - **ÂìçÂ∫îÊó∂Èó¥**Ôºö**7.5 ms**ÔºåÊé•ËøëÊúÄÂø´ÁöÑ Flow Policy (6.1 ms)Ôºå‰ΩÜÊØî DP3 (55.9 ms) Âø´Á∫¶ **7.5ÂÄç**ÔºåÂÆûÁé∞‰∫ÜÊïàÁéá‰∏éÊÄßËÉΩÁöÑ‰ºòÂºÇÂπ≥Ë°°„ÄÇ\n\n#### 2. ÁúüÂÆûÊú∫Âô®‰∫∫ÂÆûÈ™åÁªìÊûúÔºàÂÖ≥ÈîÆÊï∞ÊçÆÔºâ\n- **ÂØπÊØîÂü∫Á∫ø**ÔºöÂêå‰∏äÔºåÂ¢ûÂä†‰∫ÜÁúüÂÆû‰∏ñÁïåÂô™Â£∞ÂíåÂª∂ËøüÊåëÊàò„ÄÇ\n- **ÊÄßËÉΩÊèêÂçá**Ôºö\n    - **Âπ≥Âùá‰ªªÂä°ÊàêÂäüÁéá**ÔºöËææÂà∞ **77.5%**ÔºåÊòæËëóÈ´ò‰∫é DP3 (57.9%)„ÄÅRDP (70.0%) Âíå Flow Policy (52.5%)„ÄÇ\n    - **ËΩ®ËøπÂπ≥ÊªëÂ∫¶**ÔºöÁõ∏ÊØî **Flow Policy**ÔºåÂπ≥ÊªëÂ∫¶ÊåáÊ†áÈôç‰Ωé‰∫Ü **93.7%**ÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÁúüÂÆûÂô™Â£∞ÁéØÂ¢É‰∏ãÁöÑÂçìË∂äÁ®≥ÂÆöÊÄß„ÄÇ\n    - **ÂìçÂ∫îÊó∂Èó¥**Ôºö**8.59 ms**ÔºåËôΩÁï•È´ò‰∫é Flow Policy (6.54 ms)Ôºå‰ΩÜÊØî DP3 (29.29 ms) Âíå RDP (31.82 ms) Âø´ **3ÂÄç‰ª•‰∏ä**Ôºå‰øùÊåÅ‰∫ÜÂÆûÊó∂ÊÄß„ÄÇ\n\n#### 3. Ê†∏ÂøÉ‰ºòÂäøÊÄªÁªì\n- **ÊäÄÊúØÂàõÊñ∞‰ª∑ÂÄº**ÔºöÈÄöËøá**Âú®ÊΩúÂú®Âä®‰ΩúÁ©∫Èó¥ËøõË°åÊµÅÂåπÈÖç**ÔºåLG-Flow Policy ÊàêÂäü**Ëß£ËÄ¶‰∫ÜÂÖ®Â±ÄËøêÂä®ËßÑÂàí‰∏éÂ∫ïÂ±ÇÊéßÂà∂Âô™Â£∞**„ÄÇ\n- **ÂÆûÈôÖÈÉ®ÁΩ≤‰ª∑ÂÄº**ÔºöÂêåÊó∂ÂÆûÁé∞‰∫Ü **‚ë† Êé•ËøëÂçïÊ≠•ÁöÑÂø´ÈÄüÊé®ÁêÜ**„ÄÅ**‚ë° Â§ßÂπÖÊèêÂçáÁöÑËΩ®ËøπÂπ≥ÊªëÂ∫¶Ôºà>90% ÊîπÂñÑÔºâ** Âíå **‚ë¢ ÊòæËëóÊõ¥È´òÁöÑ‰ªªÂä°ÊàêÂäüÁéáÔºàÊèêÂçáÊúÄÈ´òËææ 25%Ôºâ**ÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÁîüÊàêÂºèÁ≠ñÁï•Âú®‚ÄúË°®ËææËÉΩÂäõ-Êé®ÁêÜÈÄüÂ∫¶-ÊâßË°åÁ®≥ÂÆöÊÄß‚Äù‰πãÈó¥ÁöÑÊ†πÊú¨ÊùÉË°°ÈóÆÈ¢òÔºå‰∏∫**ÂÆûÊó∂„ÄÅÁ®≥ÂÆöÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÈÉ®ÁΩ≤**Êèê‰æõ‰∫ÜÊúâÊïàÊñπÊ°à„ÄÇ",
            "timestamp": "2026-02-03T15:59:22.312446",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ËÆ∫ÊñáÁöÑ‰∏ªË¶ÅÂ±ÄÈôêÊÄßÊúâÂì™‰∫õÔºüÊú™Êù•ÂèØËÉΩÁöÑÊîπËøõÊñπÂêëÊòØ‰ªÄ‰πàÔºü",
            "answer": "## ËÆ∫Êñá‰∏ªË¶ÅÂ±ÄÈôêÊÄßÂàÜÊûê\n\nÂü∫‰∫éÂØπËÆ∫ÊñáÂÜÖÂÆπÁöÑÂàÜÊûêÔºå**LG-Flow Policy** ÊñπÊ≥ïËôΩÁÑ∂ÂèñÂæó‰∫ÜÊòæËëóÊàêÊûúÔºå‰ΩÜ‰ªçÂ≠òÂú®‰ª•‰∏ã‰∏ªË¶ÅÂ±ÄÈôêÊÄßÔºö\n\n- **Â§öÊ®°ÊÄÅËûçÂêàÁõ∏ÂØπÁÆÄÂçï**ÔºöËÆ∫ÊñáÊòéÁ°ÆÊåáÂá∫ÔºåÂΩìÂâçÁ≥ªÁªüÈááÁî®‰∫Ü‚ÄúÁõ∏ÂØπËΩªÈáèÁ∫ßÁöÑÂ§öÊ®°ÊÄÅËûçÂêà‚ÄùÔºàrelatively lightweight multimodal fusionÔºâ„ÄÇËøôÊÑèÂë≥ÁùÄÂú®Êï¥ÂêàËßÜËßâ„ÄÅËß¶Ëßâ„ÄÅËØ≠Ë®ÄÁ≠âÂ§öÁßçÊÑüÁü•‰ø°Âè∑Êó∂ÔºåÊñπÊ≥ïÂèØËÉΩ‰∏çÂ§üÊ∑±ÂÖ•ÊàñÈ´òÊïàÔºåÈôêÂà∂‰∫ÜÂú®Êõ¥Â§çÊùÇ„ÄÅÈúÄË¶ÅÂ§öÊÑüÂÆòÂçèÂêåÁöÑÂú∫ÊôØ‰∏ãÁöÑÊÄßËÉΩ„ÄÇ\n\n- **Êú™ÊòæÂºèÂª∫Ê®°Â§çÊùÇÊé•Ëß¶Âä®ÂäõÂ≠¶**ÔºöÊñπÊ≥ïÊ≤°ÊúâÊòæÂºèÂú∞ÂØπÊú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÂÖ≥ÈîÆÁöÑ„ÄÅÂ§çÊùÇÁöÑÊé•Ëß¶Âä®ÂäõÂ≠¶ËøõË°åÂª∫Ê®°„ÄÇËøôÂØπ‰∫é**Êé•Ëß¶‰∏∞ÂØåÁöÑÁÅµÂ∑ßÊìç‰Ωú‰ªªÂä°**ÔºàÂ¶ÇÁ≤æÁªÜË£ÖÈÖç„ÄÅÈùûÂàöÊÄßÁâ©‰ΩìÊìç‰ΩúÔºâËá≥ÂÖ≥ÈáçË¶ÅÔºåÁº∫‰πèÊ≠§Á±ªÂª∫Ê®°ÂèØËÉΩÂΩ±ÂìçÁ≠ñÁï•Âú®È´òÂ∫¶‰æùËµñÁâ©ÁêÜ‰∫§‰∫íÁöÑ‰ªªÂä°‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇ\n\n- **ÊΩúÂú®Âä®‰ΩúÁ©∫Èó¥ÁöÑË°®ËææËÉΩÂäõ‰∏éËßÑÊï¥ÊÄßÊùÉË°°**ÔºöËôΩÁÑ∂ÈÄöËøáGRUÁºñÁ†ÅÂô®ÂíåÂèòÂàÜÊ≠£ÂàôÂåñÂ°ëÈÄ†‰∫ÜÂπ≥ÊªëÁöÑÊΩúÂú®ËΩ®ËøπÁ©∫Èó¥Ôºå‰ΩÜËøôÊú¨Ë¥®‰∏äÊòØ‰∏ÄÁßç**‰ø°ÊÅØÁì∂È¢à**„ÄÇËøáÂ∫¶ËßÑÊï¥ÂèØËÉΩ‰ºöÈôêÂà∂Á≠ñÁï•ÂØπÈ´òÂ∫¶Âä®ÊÄÅÊàñÁ™ÅÂèëÊÄß‰ªªÂä°ÁöÑÈÄÇÂ∫îËÉΩÂäõÔºåÂ¶Ç‰ΩïÊõ¥‰ºòÂú∞Âπ≥Ë°°ÊΩúÂú®Á©∫Èó¥ÁöÑË°®ËææËÉΩÂäõÂíåÊó∂Â∫èÁõ∏Âπ≤ÊÄß‰ªçÊòØ‰∏Ä‰∏™ÂºÄÊîæÈóÆÈ¢ò„ÄÇ\n\n- **ÂÆûÈ™å‰ªªÂä°ÁöÑÂπøÂ∫¶‰∏éÂ§çÊùÇÊÄß**ÔºöÂ∞ΩÁÆ°Âú®Ê®°ÊãüÂíåÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠ËøõË°å‰∫ÜÈ™åËØÅÔºå‰ΩÜ‰ªªÂä°Á±ªÂûã‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÊäìÂèñ„ÄÅÊîæÁΩÆ„ÄÅÊèíÂÖ•Á≠â„ÄÇÂØπ‰∫éÈúÄË¶Å**ÈïøÊó∂Á®ãËßÑÂàí„ÄÅÂ∑•ÂÖ∑‰ΩøÁî®ÊàñÈùûÈ¢ÑÂÆö‰πâÁõÆÊ†áÁöÑÂºÄÊîæÂºè‰ªªÂä°**ÔºåÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÊ£ÄÈ™å„ÄÇ\n\n- **ÂØπÊºîÁ§∫Êï∞ÊçÆË¥®ÈáèÁöÑ‰æùËµñ**Ôºö‰Ωú‰∏∫Ê®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÔºåÂÖ∂ÊÄßËÉΩ‰∏äÈôêÂú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÂèóÈôê‰∫é‰∏ìÂÆ∂ÊºîÁ§∫Êï∞ÊçÆÁöÑË¥®ÈáèÂíåÂ§öÊ†∑ÊÄß„ÄÇËÆ∫ÊñáÊèêÂà∞ÂØπ‚ÄúÂºÇÊûÑÂíåÊ¨°‰ºòÊºîÁ§∫‚ÄùÊúâÊîπËøõÁöÑÂÆπÂøçÂ∫¶Ôºå‰ΩÜÂπ∂Êú™ÂÆåÂÖ®Ê∂àÈô§ÂØπÊ≠§ÁöÑ‰æùËµñ„ÄÇ\n\n---\n\n## Êú™Êù•ÂèØËÉΩÁöÑÊîπËøõÊñπÂêë\n\nÈíàÂØπ‰∏äËø∞Â±ÄÈôêÊÄßÔºåËÆ∫ÊñáÂú®ÁªìËÆ∫ÈÉ®ÂàÜÂèäÂàÜÊûê‰∏≠ÊöóÁ§∫‰∫Ü‰ª•‰∏ãÊú™Êù•Á†îÁ©∂ÊñπÂêëÔºö\n\n1.  **Êé¢Á¥¢Êõ¥‰∏∞ÂØåÁöÑÂ§öÊ®°ÊÄÅË°®ÂæÅ**Ôºö\n    - ÂºÄÂèëÊõ¥Âº∫Â§ßÁöÑÊû∂ÊûÑÔºåÊ∑±Â∫¶ËûçÂêàËßÜËßâÔºàÂÖ®Â±Ä‰∏éÂ±ÄÈÉ®Ôºâ„ÄÅËß¶Ëßâ„ÄÅÂäõËßâ„ÄÅÈü≥È¢ëÁîöËá≥ËØ≠Ë®ÄÊåá‰ª§ÔºåÂÆûÁé∞**Ë∑®Ê®°ÊÄÅÁöÑÈ≤ÅÊ£íÂØπÈΩê‰∏éÊé®ÁêÜ**Ôºå‰ª•Â∫îÂØπÊõ¥Â§çÊùÇ„ÄÅ‰ø°ÊÅØÊ®°Á≥äÁöÑÁúüÂÆûÁéØÂ¢É„ÄÇ\n\n2.  **ÈõÜÊàêÊé•Ëß¶ÊÑüÁü•ÁöÑÊΩúÂú®Âä®ÂäõÂ≠¶Ê®°Âûã**Ôºö\n    - Âú®ÊΩúÂú®Á©∫Èó¥Âª∫Ê®°ÊàñËß£Á†ÅËøáÁ®ã‰∏≠Ôºå**ÊòæÂºèÂú∞ÂºïÂÖ•Êé•Ëß¶Áä∂ÊÄÅ„ÄÅÂäõ/ÂäõÁü©È¢ÑÊµãÊ®°Âùó**„ÄÇËøôÂèØ‰ª•‰ΩøÁ≠ñÁï•‚ÄúÁêÜËß£‚Äù‰∫§‰∫íÁâ©ÁêÜÔºåÁîüÊàêÊõ¥Á¨¶ÂêàÊé•Ëß¶Á∫¶ÊùüÁöÑÂä®‰ΩúÔºå‰ªéËÄåÁõ¥Êé•Â∫îÂØπ‚ÄúÊú™ÊòæÂºèÂª∫Ê®°Â§çÊùÇÊé•Ëß¶Âä®ÂäõÂ≠¶‚ÄùËøô‰∏ÄÂ±ÄÈôê„ÄÇ\n\n3.  **ÂèëÂ±ïÊõ¥Ëá™ÈÄÇÂ∫îÁöÑÊΩúÂú®Á©∫Èó¥Â≠¶‰π†Êú∫Âà∂**Ôºö\n    - Á†îÁ©∂Â¶Ç‰Ωï**Âä®ÊÄÅË∞ÉÊï¥ÊΩúÂú®Á©∫Èó¥ÁöÑËßÑÊï¥Âº∫Â∫¶**ÔºåÊàñËÆ∏ÂèØ‰ª•ÈÄöËøá‰ªªÂä°ÈöæÂ∫¶Êàñ‰∏çÁ°ÆÂÆöÊÄßËøõË°åË∞ÉÂà∂Ôºå‰ΩøÂÖ∂Âú®ÈúÄË¶ÅÁ®≥ÂÆöÊÄßÁöÑ‰ªªÂä°‰∏≠‰øùÊåÅÂπ≥ÊªëÔºåÂú®ÈúÄË¶ÅÁÅµÊ¥ªÊÄßÁöÑ‰ªªÂä°‰∏≠Â¢ûÂä†Ë°®ËææËÉΩÂäõ„ÄÇ\n\n4.  **ÂêëÂàÜÂ±Ç‰∏éÈïøÁ®ãËßÑÂàíÊâ©Â±ï**Ôºö\n    - Â∞ÜÂΩìÂâçÁöÑËΩ®ËøπÁ∫ßÁîüÊàêÊ°ÜÊû∂Êâ©Â±ï‰∏∫**ÂàÜÂ±ÇÁ≠ñÁï•**ÔºöÈ´òÂ±ÇÂú®Êõ¥ÊäΩË±°ÁöÑÊΩúÂú®Á©∫Èó¥ËøõË°åÈïøÁ®ãÁõÆÊ†áÊàñÂ≠ê‰ªªÂä°ËßÑÂàíÔºåÂ∫ïÂ±ÇÔºàÂç≥ÂΩìÂâçÊñπÊ≥ïÔºâË¥üË¥£Âπ≥ÊªëÊâßË°å„ÄÇËøôÊúâÂä©‰∫éËß£ÂÜ≥Êõ¥ÈïøÊó∂Á®ãÂíåÊõ¥Â§çÊùÇÈÄªËæëÁöÑ‰ªªÂä°„ÄÇ\n\n5.  **ÁªìÂêàÂº∫ÂåñÂ≠¶‰π†‰∏éÁ¶ªÁ∫øÊï∞ÊçÆ**Ôºö\n    - Âú®Ê®°‰ªøÂ≠¶‰π†ÁöÑÂü∫Á°Ä‰∏äÔºåÂºïÂÖ•**Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÊàñÂü∫‰∫éÊ®°ÂûãÁöÑ‰ºòÂåñ**ÔºåÂà©Áî®Êî∂ÈõÜÂà∞ÁöÑÊ¨°‰ºòÊï∞ÊçÆÊàñÈÄöËøá‰ªøÁúüËá™ÊàëÊîπËøõÔºåÁ™ÅÁ†¥‰∏ìÂÆ∂ÊºîÁ§∫ÁöÑÁì∂È¢àÔºåÂÆûÁé∞Á≠ñÁï•ÁöÑËá™‰∏ªÊèêÂçá‰∏éÈÄÇÂ∫î„ÄÇ\n\n6.  **ÊèêÂçáËÆ°ÁÆó‰∏éÈááÊ†∑ÊïàÁéáÁöÑÊûÅËá¥Âåñ**Ôºö\n    - Â∞ΩÁÆ°Â∑≤ÂÆûÁé∞ËøëÂçïÊ≠•Êé®ÁêÜÔºå‰ΩÜ‰ªçÂèØÊé¢Á¥¢**Êõ¥È´òÊïàÁöÑÁΩëÁªúÊû∂ÊûÑ„ÄÅËí∏È¶èÊäÄÊúØÊàñÁ®ÄÁñèÊøÄÊ¥ªÊú∫Âà∂**ÔºåÂú®ËæπÁºòËÆ°ÁÆóËÆæÂ§á‰∏äËøõ‰∏ÄÊ≠•Èôç‰ΩéÂª∂Ëøü‰∏éÂäüËÄóÔºå‰øÉËøõÂ§ßËßÑÊ®°ÈÉ®ÁΩ≤„ÄÇ\n\n**ÊÄªÁªìËÄåË®ÄÔºåLG-Flow PolicyÁöÑÊ†∏ÂøÉ‰ª∑ÂÄºÂú®‰∫éÈÄöËøá‚ÄúÊΩúÂú®Á©∫Èó¥ÊµÅÂåπÈÖç‚ÄùËøô‰∏ÄÂàõÊñ∞ËÆæËÆ°Ôºå‰ºòÈõÖÂú∞ÊùÉË°°‰∫ÜÊé®ÁêÜÈÄüÂ∫¶„ÄÅÊó∂Â∫èÂπ≥ÊªëÊÄßÂíå‰ªªÂä°ÊÄßËÉΩ„ÄÇÂÖ∂Êú™Êù•ÁöÑËøõÂåñÂ∞ÜÂõ¥ÁªïÂ¶Ç‰ΩïËµã‰∫àËøô‰∏™Âπ≥ÊªëÁöÑÊΩúÂú®Á©∫Èó¥‰ª•Êõ¥‰∏∞ÂØåÁöÑÁâ©ÁêÜËØ≠‰πâ„ÄÅÊõ¥Âº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£ËÉΩÂäõÂíåÊõ¥Â±ÇÊ¨°ÂåñÁöÑÂÜ≥Á≠ñËÉΩÂäõÂ±ïÂºÄÔºåÊúÄÁªàÁõÆÊ†áÊòØÂÆûÁé∞ËÉΩÂú®Â§çÊùÇ„ÄÅÂä®ÊÄÅÁé∞ÂÆû‰∏ñÁïå‰∏≠Á®≥ÂÆö„ÄÅÈ´òÊïà„ÄÅÊô∫ËÉΩÊìç‰ΩúÁöÑÊú∫Âô®‰∫∫Á≥ªÁªü„ÄÇ**",
            "timestamp": "2026-02-03T15:59:54.350696",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T15:36:43Z",
    "created_at": "2026-02-03T15:53:10.653375",
    "updated_at": "2026-02-03T15:53:10.653385"
}