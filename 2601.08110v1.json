{
    "id": "2601.08110v1",
    "title": "Efficient Incremental SLAM via Information-Guided and Selective Optimization",
    "authors": [
        "Reza Arablouei"
    ],
    "abstract": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¢é‡å¼SLAMåç«¯æ–¹æ³•ï¼Œåœ¨ä¿æŒå…¨æ‰¹é‡ä¼˜åŒ–ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•èåˆäº†ä¸¤ç§äº’è¡¥æ€æƒ³ï¼šä¿¡æ¯å¼•å¯¼é—¨æ§æœºåˆ¶ä¸é€‰æ‹©æ€§å±€éƒ¨ä¼˜åŒ–ç­–ç•¥ã€‚ä¿¡æ¯å¼•å¯¼é—¨æ§æœºåˆ¶åŸºäºä¿¡æ¯çŸ©é˜µå¯¹æ•°è¡Œåˆ—å¼çš„ä¿¡æ¯è®ºå‡†åˆ™ï¼Œé‡åŒ–æ–°æµ‹é‡å€¼çš„è´¡çŒ®åº¦ï¼Œä»…å½“è§‚æµ‹åˆ°æ˜¾è‘—ä¿¡æ¯å¢ç›Šæ—¶æ‰è§¦å‘å…¨å±€ä¼˜åŒ–ï¼Œä»è€Œé¿å…åœ¨è¾“å…¥æ•°æ®æä¾›é¢å¤–ä¿¡æ¯è¾ƒå°‘æ—¶è¿›è¡Œä¸å¿…è¦çš„é‡æ–°çº¿æ€§åŒ–ä¸çŸ©é˜µåˆ†è§£ã€‚é€‰æ‹©æ€§å±€éƒ¨ä¼˜åŒ–ç­–ç•¥æ‰§è¡Œå¤šè½®é«˜æ–¯ç‰›é¡¿è¿­ä»£æ›´æ–°ï¼Œä½†å°†æ¯æ¬¡è¿­ä»£é™åˆ¶åœ¨å—æ–°æµ‹é‡å½±å“æœ€å¤§çš„å˜é‡å­é›†ä¸Šï¼Œå¹¶åŠ¨æ€ä¼˜åŒ–è¯¥æ´»è·ƒé›†ç›´è‡³æ”¶æ•›ã€‚è¿™ä¸¤ç§æœºåˆ¶ååŒå·¥ä½œï¼Œæ—¢ä¿ç•™å…¨éƒ¨æµ‹é‡æ•°æ®ä»¥ç»´æŒå…¨å±€ä¸€è‡´æ€§ï¼Œåˆå°†è®¡ç®—èµ„æºé›†ä¸­ä½œç”¨äºå›¾ç»“æ„ä¸­æ”¶ç›Šæœ€å¤§çš„éƒ¨åˆ†ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®Œæ•´ä¿æŒäº†é«˜æ–¯ç‰›é¡¿æ³•çš„æ”¶æ•›æ€§ä¿è¯ã€‚åœ¨æ ‡å‡†SLAMæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¸æ‰¹é‡æ±‚è§£å™¨çš„ä¼°è®¡ç²¾åº¦ä¿æŒä¸€è‡´ï¼ŒåŒæ—¶ç›¸æ¯”ä¼ ç»Ÿå¢é‡æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„è®¡ç®—æ•ˆç‡æå‡ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨ç²¾åº¦ä¸æ•ˆç‡ä¹‹é—´å»ºç«‹äº†ç†è®ºå¹³è¡¡ï¼Œä¸ºåŠ¨æ€æ•°æ®å¯†é›†ç¯å¢ƒä¸‹çš„å®æ—¶æ“ä½œæä¾›äº†ç¨³å¥ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
    "url": "https://arxiv.org/abs/2601.08110v1",
    "html_url": "https://arxiv.org/html/2601.08110v1",
    "html_content": "Efficient Incremental SLAM via Information-Guided and Selective Optimization\nReza Arablouei\nAbstract\nWe present an efficient incremental SLAM back-end that achieves the accuracy of full batch optimization while substantially reducing computational cost. The proposed approach combines two complementary ideas: information-guided gating (IGG) and selective partial optimization (SPO). IGG employs an information-theoretic criterion based on the log-determinant of the information matrix to quantify the contribution of new measurements, triggering global optimization only when a significant information gain is observed. This avoids unnecessary relinearization and factorization when incoming data provide little additional information. SPO executes multi-iteration Gauss-Newton (GN) updates but restricts each iteration to the subset of variables most affected by the new measurements, dynamically refining this active set until convergence. Together, these mechanisms retain all measurements to preserve global consistency while focusing computation on parts of the graph where it yields the greatest benefit. We provide theoretical analysis showing that the proposed approach maintains the convergence guarantees of full GN. Extensive experiments on benchmark SLAM datasets show that our approach consistently matches the estimation accuracy of batch solvers, while achieving significant computational savings compared to conventional incremental approaches. The results indicate that the proposed approach offers a principled balance between accuracy and efficiency, making it a robust and scalable solution for real-time operation in dynamic data-rich environments.\n1\nIntroduction\nSimultaneous localization and mapping (SLAM) is a fundamental capability for autonomous robots, enabling the continuous estimation of both a robotâ€™s pose and the surrounding environment map. Modern SLAM systems often adopt graph-based nonlinear optimization formulations, in which robot poses are represented as nodes and spatial constraints as edges in a factor graph\n[\n35\n,\n17\n,\n19\n]\n. Compared to filtering-based methods, this full smoothing approach typically yields higher accuracy and improved consistency\n[\n11\n]\n, but also leads to a continually expanding estimation problem. In long-term, data-rich deployments, the number of poses and measurements can grow without bound, substantially increasing memory and computation requirements. Repeatedly solving the full SLAM problem from scratch becomes prohibitive for real-time applications, motivating extensive research into incremental SLAM methods that update the solution efficiently as new data arrive\n[\n21\n]\n.\nEarly SLAM algorithms were predominantly based on recursive filtering, with the extended Kalman filter (EKF) as a widely used example. Although EKF-SLAM supports real-time operation, its dense covariance representation leads to\nğ’ª\nâ€‹\n(\nN\n2\n)\n\\mathcal{O}(N^{2})\ncomplexity, and accumulated linearization errors can degrade map consistency over time. Delayed-state filters mitigate computational load by maintaining only a fixed-size window of recent keyframes\n[\n16\n]\n, but as the window grows to preserve accuracy, costs again become significant. In contrast, smoothing-based SLAM formulates the problem as a nonlinear least-squares optimization over a pose graph\n[\n12\n]\n, exploiting the sparsity of the underlying information matrix.\nEarly smoothing and mapping (SAM) approaches, such as Square Root SAM\n[\n11\n]\n, demonstrated that solving the full SLAM problem via sparse linear algebra (e.g., QR or Cholesky factorization of the information matrix) improves both accuracy and efficiency compared to EKF-based methods. Open-source frameworks such as g2o\n[\n34\n]\nand GTSAM\n[\n10\n]\nhave further streamlined deployment by providing efficient graph construction, factorization, and solver interfaces. Beyond direct solvers, iterative approaches\n[\n9\n,\n26\n,\n25\n]\nleverage sparse matrixâ€“vector products to reduce computational cost, and specialized fast pose-graph optimizers\n[\n37\n]\ncan handle poor initial estimates effectively.\nAmong incremental SAM algorithms, iSAM\n[\n30\n]\nis a landmark contribution, maintaining the square-root information matrix and updating it with new measurements via low-rank sparse matrix factor updates\n[\n18\n]\n. This enables real-time updates while reusing previously computed structure. However, the original iSAM required periodic batch relinearization and variable reordering to maintain consistency, effectively performing occasional full optimizations to correct linearization drift.\nIts successor, iSAM2\n[\n29\n]\n, addressed these limitations by introducing the Bayes tree, a junction-tree-based data structure that supports fluid relinearization and incremental variable reordering. iSAM2 uses a threshold-based wildfire strategy to relinearize only variables affected by new information, eliminating the need for expensive batch resets while preserving accuracy.\nOther advances in incremental SLAM include robust solvers and alternative inference strategies. The RISE algorithm\n[\n41\n]\nincorporates a trust-region method (Powellâ€™s dog-leg) into an incremental framework, improving robustness to strong nonlinearities and ill-conditioned systems while maintaining speeds comparable to Gauss-Newton (GN) methods. NF-iSAM\n[\n22\n]\nextends incremental smoothing to non-Gaussian estimation by using normalizing flows to represent arbitrary posteriors, retaining the sparsity and efficiency of iSAM2â€™s Bayes tree under non-Gaussian measurement models. Robustness has also been pursued in riSAM\n[\n36\n]\n, which leverages graduated non-convexity\n[\n31\n,\n45\n,\n7\n]\nto handle outliers and non-convexity. In parallel, works such as incremental Cholesky factorization\n[\n39\n]\nand AprilSAM\n[\n44\n]\nhave focused on improving update efficiency through algorithmic refinements that accelerate matrix updates. Collectively, these developments reflect the SLAM communityâ€™s drive toward back-ends that are both online-efficient and robust under real-world conditions.\nDespite these advances, scalable SLAM in highly dynamic, data-rich environments remains challenging. Robots operating for extended durations or equipped with high-frequency sensors (e.g., dense visual or LiDAR) can receive a stream of measurements, many redundant or only marginally informative. Incorporating every measurement into the pose graph and triggering a full solver update at each increment is computationally wasteful and may degrade real-time performance.\nTo address this, researchers have explored measurement sparsification or selection based on information content. Several works\n[\n32\n,\n33\n,\n24\n]\nemploy information-theoretic criteria to design sparse yet reliable pose graphs that approximate the estimation quality of the full graph, selecting a near-D-optimal subset of loop closures or constraints to maximize the determinant of the Fisher information matrix.\nApproaches to information-driven graph sparsification and active SLAM\n[\n38\n]\nhave used well-defined quality measures such as the log-determinant of the Fisher information matrix (D-optimality) and the algebraic connectivity (Fiedler eigenvalue of the graph Laplacian). Examples include information-theoretic loop-closure detection\n[\n4\n]\n, factor-based node marginalization\n[\n2\n]\n, and conservative edge sparsification\n[\n3\n]\n. Similarly, Doherty\net al.\n[\n13\n]\npropose a spectral sparsification method that retains edges maximizing algebraic connectivity, a property correlated with SLAM accuracy. By maximizing the Fiedler value of the measurement graph, their method preserves global consistency while reducing stored constraints.\nThese studies show that significant computational savings are possible through graph pruning or edge selection guided by principled information measures. However, permanently discarding measurements risks loss of consistency or robustness if the selection is imperfect\n[\n14\n]\n. Moreover, many sparsification methods operate offline or as a separate preprocessing step, rather than being tightly integrated into the live optimization loop to decide, in real time, whether a new measurement should be incorporated.\n1.1\nContributions\nWe propose an efficient incremental SLAM framework that integrates information-based variable selection directly into the optimization back-end, thereby avoiding unnecessary computations while preserving global consistency. The proposed approach comprises two key components:\n(i) information-guided gating (IGG), and (ii) selective partial optimization (SPO).\nBeyond algorithmic innovations, we also establish theoretical guarantees, showing that the proposed heuristics preserve the convergence properties of full GN optimization. In summary, our main contributions are:\nâ€¢\nInformation-Guided Gating (IGG)\n: An information-theoretic mechanism that monitors the change in the log-determinant of the information matrix to evaluate the contribution of new measurements. Only when the predicted information gain exceeds a threshold is a global update triggered. Otherwise, optimization is restricted to local variables directly affected by the new measurements.\nâ€¢\nSelective Partial Optimization (SPO)\n: A multi-iteration nonlinear GN solver that, at each iteration, updates and relinearizes only the variables that have not yet converged. This subset is dynamically refined based on convergence thresholds and graph connectivity, focusing computation where it yields the greatest gains.\nâ€¢\nTheoretical Guarantees\n: Rigorous analysis demonstrating that the proposed approach combining IGG and SPO converges to the same stationary point and achieves the same local rate as full GN under standard smoothness assumptions, thereby providing principled support for the observed efficiency gains.\nUnlike sparsification methods, which may discard information and compromise global consistency, our approach retains all measurements while limiting optimization to the most impactful updates. The result is a robust and scalable SLAM back-end that delivers batch-level accuracy at a fraction of the per-update computational cost. In contrast to iSAM2, which heuristically limits relinearization yet still solves the full global system at every increment, our approach provides a principled alternative: by unifying information-guided gating with selective partial optimization, it achieves the same convergence guarantees and accuracy of full GN while directing computation strictly to variables most affected by new information.\n2\nBackground\nSLAM involves the joint estimation of a robotâ€™s trajectory and a map of the environment. Modern SLAM back-ends typically formulate this as a large-scale nonlinear least-squares optimization over a\npose graph\n, where nodes represent robot poses (and possibly landmarks) and edges represent spatial measurements between them.\nConsider a set of measurement residuals\nğ«\nj\n=\nğ¦\nj\nâˆ’\nğŸ\nj\nâ€‹\n(\nğ±\nğ’±\nj\n)\n,\n\\mathbf{r}_{j}=\\mathbf{m}_{j}-\\mathbf{f}_{j}(\\mathbf{x}_{\\mathcal{V}_{j}}),\n(1)\nwhere\nğ¦\nj\n\\mathbf{m}_{j}\nis the\nj\nj\n-th observed measurement (e.g., a relative pose or landmark observation),\nğŸ\nj\nâ€‹\n(\nâ‹…\n)\n\\mathbf{f}_{j}(\\cdot)\nis the measurement prediction function, and\nğ±\nğ’±\nj\n\\mathbf{x}_{\\mathcal{V}_{j}}\ndenotes the subset of state variables involved in measurement\nj\nj\n(indexed by\ni\ni\n).\nAssuming Gaussian measurement noise with covariance\nğšº\nj\n\\boldsymbol{\\Sigma}_{j}\n, the maximum a posteriori (MAP) estimate can be obtained by minimizing the cost function\nc\nâ€‹\n(\nğ±\n)\n=\n1\n2\nâ€‹\nâˆ‘\nj\n=\n1\nM\nâ€–\nğ«\nj\nâ€–\nğšº\nj\n2\n=\n1\n2\nâ€‹\nâˆ‘\nj\n=\n1\nM\nâ€–\nğ¦\nj\nâˆ’\nğŸ\nj\nâ€‹\n(\nğ±\nğ’±\nj\n)\nâ€–\nğšº\nj\n2\n,\nc(\\mathbf{x})=\\frac{1}{2}\\sum_{j=1}^{M}\\|\\mathbf{r}_{j}\\|^{2}_{\\boldsymbol{\\Sigma}_{j}}=\\frac{1}{2}\\sum_{j=1}^{M}\\|\\mathbf{m}_{j}-\\mathbf{f}_{j}(\\mathbf{x}_{\\mathcal{V}_{j}})\\|^{2}_{\\boldsymbol{\\Sigma}_{j}},\n(2)\nwhere\nâ€–\nğ«\nj\nâ€–\nğšº\nj\n2\nâ‰œ\nğ«\nj\nğ–³\nâ€‹\nğšº\nj\nâˆ’\n1\nâ€‹\nğ«\nj\n\\|\\mathbf{r}_{j}\\|^{2}_{\\boldsymbol{\\Sigma}_{j}}\\triangleq\\mathbf{r}_{j}^{\\mathsf{T}}\\boldsymbol{\\Sigma}_{j}^{-1}\\mathbf{r}_{j}\n.\nThis nonlinear least-squares problem is typically solved using iterative methods such as the GN algorithm or its Levenberg-Marquardt variant. Starting from an initial guess, GN linearizes the measurement functions around the current state estimate, producing the linearized normal equations\n(\nğ‰\nğ–³\nâ€‹\nğ‰\n)\nâ€‹\nğ\n=\nğ‰\nğ–³\nâ€‹\nğ«\n,\n(\\mathbf{J}^{\\mathsf{T}}\\mathbf{J})\\mathbf{d}=\\mathbf{J}^{\\mathsf{T}}\\mathbf{r},\n(3)\nwhere\nğ‰\n\\mathbf{J}\nis the stacked Jacobian of all residuals with respect to the full state vector\nğ±\n\\mathbf{x}\n, and\nğ«\n\\mathbf{r}\nis the stacked residual vector. Solving for\nğ\n\\mathbf{d}\nyields the state increment, and the estimate is updated as\nğ±\nâ†\nğ±\n+\nğ\n\\mathbf{x}\\leftarrow\\mathbf{x}+\\mathbf{d}\n. This process is repeated (with relinearization at each step) until convergence.\nGraph-based SLAM problems exhibit strong structural sparsity that can be exploited for computational efficiency. The information matrix (the approximate Hessian)\nğ‡\n=\nğ‰\nğ–³\nâ€‹\nğ‰\n\\mathbf{H}=\\mathbf{J}^{\\mathsf{T}}\\mathbf{J}\nis typically large but sparse and block-structured, reflecting the local connectivity of the pose graph: each variable\ni\ni\n(pose or landmark) is directly linked to only a few others through shared measurements\nj\nj\n.\nIn pure exploration scenarios without loop closures, the sparsity pattern is approximately block-tridiagonal. When loop closures occur, long-range links introduce additional nonzero blocks (fill-in), but the matrix remains sparse. Furthermore, in many SLAM problems, the diagonal blocks (self-information from priors or odometry) dominate the off-diagonal terms, leading to a form of near block-diagonal dominance.\nThis sparsity implies that naive dense linear algebra is inefficient: factorizing a dense\nN\nÃ—\nN\nN\\times N\nmatrix costs\nğ’ª\nâ€‹\n(\nN\n3\n)\n\\mathcal{O}(N^{3})\n, whereas exploiting sparsity can reduce the complexity dramatically. Efficient SLAM back-ends therefore rely on sparse matrix factorization or iterative methods to leverage this structure.\nIn practice, the linear system arising in each GN iteration can be solved either (i)\ndirectly\n, through sparse matrix factorization (e.g., Cholesky or QR) followed by forward/backward substitution, or (ii)\niteratively\n, using solvers such as conjugate gradient that exploit sparse matrix-vector products. In this work, we adopt a direct sparse factorization approach because it offers high numerical accuracy, facilitates efficient reuse of factors in incremental updates, and enables rapid computation of information-theoretic quantities (e.g., log-determinants) that are essential for our IGG strategy.\n3\nProposed Approach\nThe proposed approach, summarized in Algorithm 1, is an incremental nonlinear least-squares optimizer for SLAM that integrates two key ideas of information-guided gating (IGG) and selective partial optimization (SPO).\nWe formulate SLAM as a pose-graph optimization problem and maintain, throughout execution, the sparse Cholesky factor\nğ‘\n\\mathbf{R}\nof the information matrix\nğ‡\n=\nğ‰\nğ–³\nâ€‹\nğ‰\n\\mathbf{H}=\\mathbf{J}^{\\mathsf{T}}\\mathbf{J}\n. The algorithm operates in discrete increments, where each increment corresponds to the arrival of one or more new measurements. The framework supports both batch mode (processing multiple measurements received in rapid succession) and streaming mode (processing measurements one at a time). At each increment\nt\nt\n, the algorithm determines which variables to update and how the update is performed, according to the following procedure.\n3.1\nGraph Update and Initial Linearization\nWhen new measurements arrive, We first update the pose graph\nğ’¢\nt\n\\mathcal{G}_{t}\nby adding the corresponding edges, which connect the relevant robot poses or landmarks. If these edges introduce new poses or landmarks (and hence new variables) we also append their initial estimates to the state vector. We denote the total number of state variables after incorporating the new measurements as\nN\nt\nN_{t}\n.\nSubsequently, We linearize the new edges about the current state estimate,\nğ±\nt\nâˆ’\n1\n\\mathbf{x}_{t-1}\n. Specifically, we compute the Jacobians of the new measurements with respect to the involved variables, evaluated at their current estimates. This process augments the system by adding new rows to the Jacobian\nğ‰\nt\n\\mathbf{J}_{t}\nand new entries to the residual vector\nğ«\nt\n\\mathbf{r}_{t}\n, and, if new variables are present, by adding corresponding columns to\nğ‰\nt\n\\mathbf{J}_{t}\n.\nTo efficiently incorporate these changes, we perform a low-rank Cholesky update to extend the existing factor\nğ‘\nt\nâˆ’\n1\n\\mathbf{R}_{t-1}\nto\nğ‘\nt\n\\mathbf{R}_{t}\n. This produces the updated linear system\n(\nğ‘\nt\nğ–³\nâ€‹\nğ‘\nt\n)\nâ€‹\nğ\nt\n=\nğ›\nt\n(\\mathbf{R}_{t}^{\\mathsf{T}}\\mathbf{R}_{t})\\mathbf{d}_{t}=\\mathbf{b}_{t}\n, where\nğ›\nt\n=\nğ‰\nt\nğ–³\nâ€‹\nğ«\nt\n\\mathbf{b}_{t}=\\mathbf{J}_{t}^{\\mathsf{T}}\\mathbf{r}_{t}\n. To preserve sparsity and reduce fill-in during factorization, we incrementally update the variable ordering\nğ…\nt\n\\boldsymbol{\\pi}_{t}\nusing the constrained column approximate minimum degree (CCOLAMD) algorithm\n[\n8\n]\n. In addition, we incrementally update the elimination tree associated with\nğ‘\nt\n\\mathbf{R}_{t}\n, denoted by\nğ©\nt\n\\mathbf{p}_{t}\n, to reflect changes in the graph structure.\n3.2\nInformation-Guided Gating\nWe define an information-theoretic quantity\nÎ·\nt\n\\eta_{t}\nto measure the systemâ€™s average information content after incorporating the new measurements. Specifically, let\nÎ·\nt\n=\nâˆ‘\ni\n=\n1\nN\nt\nln\nâ¡\n|\nÏ\nt\n,\ni\n|\n,\n\\eta_{t}=\\sum_{i=1}^{N_{t}}\\ln|\\rho_{t,i}|,\n(4)\nwhere\nÏ\nt\n,\ni\n\\rho_{t,i}\nis the\ni\ni\n-th diagonal entry of\nğ‘\nt\n\\mathbf{R}_{t}\n. Since, we have\nâˆ‘\ni\nln\nâ¡\n|\nÏ\nt\n,\ni\n|\n=\nln\nâ€‹\ndet\n(\nğ‘\nt\n)\n=\n1\n2\nâ€‹\nln\nâ€‹\ndet\n(\nğ‰\nt\nğ–³\nâ€‹\nğ‰\nt\n)\n,\n\\sum_{i}\\ln|\\rho_{t,i}|=\\ln\\det(\\mathbf{R}_{t})=\\frac{1}{2}\\ln\\det(\\mathbf{J}_{t}^{\\mathsf{T}}\\mathbf{J}_{t}),\n(5)\nÎ·\nt\n\\eta_{t}\nis equal to half the log-determinant of the information matrix\nğ‡\nt\n=\nğ‰\nt\nğ–³\nâ€‹\nğ‰\nt\n\\mathbf{H}_{t}=\\mathbf{J}_{t}^{\\mathsf{T}}\\mathbf{J}_{t}\n, which serves as a proxy for the information content or, equivalently, the negative log of the uncertainty volume\n[\n42\n]\n.\nTo isolate the effect of the new measurements from the trivial growth in\nÎ·\nt\n\\eta_{t}\ncaused by adding new variables, we compute the detrended change\nÎ”\nâ€‹\nÎ·\nt\n=\nÎ·\nt\nâˆ’\nN\nt\nâˆ’\n1\nN\nt\nâ€‹\nÎ·\nt\nâˆ’\n1\n\\Delta\\eta_{t}=\\eta_{t}-\\frac{N_{t-1}}{N_{t}}\\eta_{t-1}\n(6)\nand compare it against a preset threshold\nÏ„\nÎ·\n\\tau_{\\eta}\n. if\nÎ”\nâ€‹\nÎ·\nt\n<\nÏ„\nÎ·\n\\Delta\\eta_{t}<\\tau_{\\eta}\n, he added measurements do not significantly reduce the systemâ€™s uncertainty. In this case, only the variables directly involved in the new edges are marked as\npotentially affected\n, and their indices are stored in the set\nğ’®\nt\n\\mathcal{S}_{t}\n. This avoids unnecessary re-optimization in scenarios such as pure odometry updates with negligible incremental information.\nAlgorithm 1\nIncremental SLAM with Information-Guided Gating and Selective Partial Optimization\n1:\ninitial estimate\nğ±\n0\n\\mathbf{x}_{0}\n,\nthresholds\nÏ„\nÎ·\n\\tau_{\\eta}\n(information gain),\nÏ„\nd\n\\tau_{d}\n(increment magnitude),\nÏ„\nGN\n\\tau_{\\mathrm{GN}}\n(max GN iterations)\n2:\nupdated estimate\nğ±\nt\n\\mathbf{x}_{t}\nafter each increment\nt\nt\n3:\nfor\nt\n=\n1\n,\n2\n,\nâ€¦\nt=1,2,\\dots\ndo\n4:\nincorporate new measurements into\nğ’¢\nt\nâˆ’\n1\n\\mathcal{G}_{t-1}\nto form\nğ’¢\nt\n\\mathcal{G}_{t}\n5:\nğ’®\nt\nâ†\n\\mathcal{S}_{t}\\leftarrow\nvariables involved in new measurements\n6:\nupdate\nğ‰\nt\n\\mathbf{J}_{t}\n,\nğ«\nt\n\\mathbf{r}_{t}\n,\nğ‘\nt\n\\mathbf{R}_{t}\n,\nğ›\nt\n\\mathbf{b}_{t}\n,\nğ…\nt\n\\boldsymbol{\\pi}_{t}\n, and\nğ©\nt\n\\mathbf{p}_{t}\nincrementally based on new measurements\n7:\nÎ·\nt\nâ†\nâˆ‘\ni\n=\n1\nN\nt\nln\nâ¡\n|\ndiag\nâ€‹\n(\nğ‘\nt\n)\n|\n\\eta_{t}\\leftarrow\\sum_{i=1}^{N_{t}}\\ln|\\mathrm{diag}(\\mathbf{R}_{t})|\n8:\nif\nÎ·\nt\nâˆ’\nN\nt\nâˆ’\n1\nN\nt\nâ€‹\nÎ·\nt\nâˆ’\n1\n>\nÏ„\nÎ·\n\\eta_{t}-\\frac{N_{t-1}}{N_{t}}\\eta_{t-1}>\\tau_{\\eta}\nthen\n9:\nğ’®\nt\nâ†\n{\n1\n,\nâ€¦\n,\nN\nt\n}\n\\mathcal{S}_{t}\\leftarrow\\{1,\\dots,N_{t}\\}\n10:\nend\nif\n11:\nfor\ni\nGN\n=\n1\n,\n2\n,\nâ€¦\n,\nÏ„\nGN\ni_{\\mathrm{GN}}=1,2,\\dots,\\tau_{\\mathrm{GN}}\ndo\n12:\nsolve\n(\nğ‘\nt\nğ–³\nâ€‹\nğ‘\nt\n)\nâ€‹\nğ\n=\nğ›\nt\n(\\mathbf{R}_{t}^{\\mathsf{T}}\\mathbf{R}_{t})\\mathbf{d}=\\mathbf{b}_{t}\nover\nğ’®\nt\n\\mathcal{S}_{t}\nto obtain\nğ\nğ’®\nt\n\\mathbf{d}_{\\mathcal{S}_{t}}\n13:\nupdate\nğ’®\nt\n\\mathcal{S}_{t}\nbased on\n|\nğ\nğ’®\nt\n|\n>\nÏ„\nd\n|\\mathbf{d}_{\\mathcal{S}_{t}}|>\\tau_{d}\nand the connectivity in\nğ’¢\nt\n\\mathcal{G}_{t}\n14:\nif\nğ’®\nt\n=\nâˆ…\n\\mathcal{S}_{t}=\\emptyset\nthen\nbreak\n15:\nend\nif\n16:\nğ±\nğ’®\nt\nâ†\nğ±\nğ’®\nt\nâˆ’\nğ\nğ’®\nt\n\\mathbf{x}_{\\mathcal{S}_{t}}\\leftarrow\\mathbf{x}_{\\mathcal{S}_{t}}-\\mathbf{d}_{\\mathcal{S}_{t}}\n17:\nupdate\nğ‰\nt\n\\mathbf{J}_{t}\n,\nğ«\nt\n\\mathbf{r}_{t}\n,\nğ‘\nt\n\\mathbf{R}_{t}\n, and\nğ›\nt\n\\mathbf{b}_{t}\nfor edges involving\nğ’®\nt\n\\mathcal{S}_{t}\n18:\nend\nfor\n19:\nend\nfor\nConversely, if\nÎ”\nâ€‹\nÎ·\nt\nâ‰¥\nÏ„\nÎ·\n\\Delta\\eta_{t}\\geq\\tau_{\\eta}\n, the new measurements are deemed sufficiently informative to potentially influence the entire graph, for example, in the case of a loop closure or a high-accuracy pose prior. We therefore set\nğ’®\nt\n=\n{\n1\n,\n2\n,\nâ€¦\n,\nN\nt\n}\n\\mathcal{S}_{t}=\\{1,2,\\dots,N_{t}\\}\n, marking all variables as\npotentially affected\n. While it is possible to attempt a more selective choice (e.g., restricting to the connected component impacted by the new edges), in typical SLAM graphs the system forms a single connected component, and determining the exact subset of globally affected variables is itself computationally expensive, potentially negating the benefits of selectivity.\nNote that at this stage we identify only the\npotentially affected\nvariables. The subset of\nactually affected\nvariables (those updated and relinearized) is determined later, after the partial solve stage described in Section\n3.3\n, via the procedure in Section\n3.4\n.\n3.3\nSelective Partial Solve\nGiven the current set of\npotentially affected\nvariables\nğ’®\nt\n\\mathcal{S}_{t}\n(identified from the IGG step or carried over from the previous GN iteration), we solve the linear system arising in the GN update:\n(\nğ‘\nt\nğ–³\nâ€‹\nğ‘\nt\n)\nâ€‹\nğ\nt\n=\nğ›\nt\n.\n(\\mathbf{R}_{t}^{\\mathsf{T}}\\mathbf{R}_{t})\\mathbf{d}_{t}=\\mathbf{b}_{t}.\n(7)\nIf\nğ’®\nt\n\\mathcal{S}_{t}\ncontains all variables, (\n7\n) is solved by standard forward-backward substitution:\nğ‘\nt\nğ–³\nâ€‹\nğ²\n=\nğ›\nt\n,\nğ‘\nt\nâ€‹\nğ\nt\n=\nğ²\n.\n\\mathbf{R}_{t}^{\\mathsf{T}}\\mathbf{y}=\\mathbf{b}_{t},\\qquad\\mathbf{R}_{t}\\mathbf{d}_{t}=\\mathbf{y}.\n(8)\nWhen\n|\nğ’®\nt\n|\nâ‰ª\nN\nt\n|\\mathcal{S}_{t}|\\ll N_{t}\n, it is more efficient to restrict computation to the dynamic subset\nğ’®\nt\n\\mathcal{S}_{t}\nwhile treating the remaining variables\nğ’°\nt\n\\mathcal{U}_{t}\nas static.\nFor clarity, we temporarily drop the increment index\nt\nt\nand denote these sets simply as\nğ’®\n\\mathcal{S}\n(dynamic) and\nğ’°\n\\mathcal{U}\nstatic, with\nğ’°\n=\n{\n1\n,\n.\n.\n,\nN\n}\nâˆ–\nğ’®\n\\mathcal{U}=\\{1,..,N\\}\\setminus\\mathcal{S}\n. While the global variable ordering remains fixed by the factorization, we locally permute rows and columns of\nğ‘\n\\mathbf{R}\nand\nğ›\n\\mathbf{b}\nto form the block structure\nğ‘\n=\n[\nğ‘\nğ’°\nâ€‹\nğ’°\nğ‘\nğ’°\nâ€‹\nğ’®\nğŸ\nğ‘\nğ’®\nâ€‹\nğ’®\n]\n,\nğ›\n=\n[\nğ›\nğ’°\nğ›\nğ’®\n]\n,\nand\nâ€‹\nğ›\n=\n[\nğ\nğ’°\nğ\nğ’®\n]\n.\n\\mathbf{R}=\\begin{bmatrix}\\mathbf{R}_{\\mathcal{U}\\mathcal{U}}&\\mathbf{R}_{\\mathcal{U}\\mathcal{S}}\\\\\n\\mathbf{0}&\\mathbf{R}_{\\mathcal{S}\\mathcal{S}}\\end{bmatrix},\\mathbf{b}=\\begin{bmatrix}\\mathbf{b}_{\\mathcal{U}}\\\\\n\\mathbf{b}_{\\mathcal{S}}\\end{bmatrix},\\text{ and }\\mathbf{b}=\\begin{bmatrix}\\mathbf{d}_{\\mathcal{U}}\\\\\n\\mathbf{d}_{\\mathcal{S}}\\end{bmatrix}.\n(9)\nThe linear system\n(\nğ‘\nğ–³\nâ€‹\nğ‘\n)\nâ€‹\nğ\n=\nğ›\n(\\mathbf{R}^{\\mathsf{T}}\\mathbf{R})\\mathbf{d}=\\mathbf{b}\nthen takes the block form:\n[\nğ‘\nğ’°\nâ€‹\nğ’°\nğ–³\nâ€‹\nğ‘\nğ’°\nâ€‹\nğ’°\nğ‘\nğ’°\nâ€‹\nğ’°\nğ–³\nâ€‹\nğ‘\nğ’°\nâ€‹\nğ’®\nğ‘\nğ’°\nâ€‹\nğ’®\nğ–³\nâ€‹\nğ‘\nğ’°\nâ€‹\nğ’°\nğ‘\nğ’®\nâ€‹\nğ’®\nğ–³\nâ€‹\nğ‘\nğ’®\nâ€‹\nğ’®\n+\nğ‘\nğ’°\nâ€‹\nğ’®\nğ–³\nâ€‹\nğ‘\nğ’°\nâ€‹\nğ’®\n]\nâ€‹\n[\nğ\nğ’°\nğ\nğ’®\n]\n=\n[\nğ›\nğ’°\nğ›\nğ’®\n]\n.\n\\begin{bmatrix}\\mathbf{R}_{\\mathcal{U}\\mathcal{U}}^{\\mathsf{T}}\\mathbf{R}_{\\mathcal{U}\\mathcal{U}}&\\mathbf{R}_{\\mathcal{U}\\mathcal{U}}^{\\mathsf{T}}\\mathbf{R}_{\\mathcal{U}\\mathcal{S}}\\\\[6.0pt]\n\\mathbf{R}_{\\mathcal{U}\\mathcal{S}}^{\\mathsf{T}}\\mathbf{R}_{\\mathcal{U}\\mathcal{U}}&\\mathbf{R}_{\\mathcal{S}\\mathcal{S}}^{\\mathsf{T}}\\mathbf{R}_{\\mathcal{S}\\mathcal{S}}+\\mathbf{R}_{\\mathcal{U}\\mathcal{S}}^{\\mathsf{T}}\\mathbf{R}_{\\mathcal{U}\\mathcal{S}}\\end{bmatrix}\\begin{bmatrix}\\mathbf{d}_{\\mathcal{U}}\\\\[6.0pt]\n\\mathbf{d}_{\\mathcal{S}}\\end{bmatrix}=\\begin{bmatrix}\\mathbf{b}_{\\mathcal{U}}\\\\[6.0pt]\n\\mathbf{b}_{\\mathcal{S}}\\end{bmatrix}.\n(10)\nSince the unaffected (static) variables remain fixed (i.e.,\nğ\nğ’°\n=\nğŸ\n\\mathbf{d}_{\\mathcal{U}}=\\mathbf{0}\n), the first block row of (\n10\n) becomes\nğ‘\nğ’°\nâ€‹\nğ’°\nğ–³\nâ€‹\nğ‘\nğ’°\nâ€‹\nğ’®\nâ€‹\nğ\nğ’®\n=\nğ›\nğ’°\n.\n\\mathbf{R}_{\\mathcal{U}\\mathcal{U}}^{\\mathsf{T}}\\mathbf{R}_{\\mathcal{U}\\mathcal{S}}\\mathbf{d}_{\\mathcal{S}}=\\mathbf{b}_{\\mathcal{U}}.\n(11)\nBy introducing the intermediate vector\nğ²\nğ’°\n=\nğ‘\nğ’°\nâ€‹\nğ’®\nâ€‹\nğ\nğ’®\n,\n\\mathbf{y}_{\\mathcal{U}}=\\mathbf{R}_{\\mathcal{U}\\mathcal{S}}\\mathbf{d}_{\\mathcal{S}},\n(12)\nwe can transform (\n11\n) into the triangular linear system\nğ‘\nğ’°\nâ€‹\nğ’°\nğ–³\nâ€‹\nğ²\nğ’°\n=\nğ›\nğ’°\n\\mathbf{R}_{\\mathcal{U}\\mathcal{U}}^{\\mathsf{T}}\\mathbf{y}_{\\mathcal{U}}=\\mathbf{b}_{\\mathcal{U}}\n(13)\nThis equation can be solved efficiently by exploiting the cached\nğ‘\nğ’°\nâ€‹\nğ’°\n\\mathbf{R}_{\\mathcal{U}\\mathcal{U}}\n.\nSubstituting (\n12\n) into the second block row of (\n10\n) gives\n(\nğ‘\nğ’®\nâ€‹\nğ’®\nğ–³\nâ€‹\nğ‘\nğ’®\nâ€‹\nğ’®\n)\nâ€‹\nğ\nğ’®\n\\displaystyle(\\mathbf{R}_{\\mathcal{S}\\mathcal{S}}^{\\mathsf{T}}\\mathbf{R}_{\\mathcal{S}\\mathcal{S}})\\mathbf{d}_{\\mathcal{S}}\n=\nğ›\nğ’®\nâˆ’\n(\nğ‘\nğ’°\nâ€‹\nğ’®\nğ–³\nâ€‹\nğ‘\nğ’°\nâ€‹\nğ’®\n)\nâ€‹\nğ\nğ’®\n\\displaystyle=\\mathbf{b}_{\\mathcal{S}}-(\\mathbf{R}_{\\mathcal{U}\\mathcal{S}}^{\\mathsf{T}}\\mathbf{R}_{\\mathcal{U}\\mathcal{S}})\\mathbf{d}_{\\mathcal{S}}\n(14)\n=\nğ›\nğ’®\nâˆ’\nğ‘\nğ’°\nâ€‹\nğ’®\nğ–³\nâ€‹\nğ²\nğ’°\n,\n\\displaystyle=\\mathbf{b}_{\\mathcal{S}}-\\mathbf{R}_{\\mathcal{U}\\mathcal{S}}^{\\mathsf{T}}\\mathbf{y}_{\\mathcal{U}},\n(15)\nHere, the term\nâˆ’\nğ‘\nğ’°\nâ€‹\nğ’®\nğ–³\nâ€‹\nğ²\nğ’°\n-\\mathbf{R}_{\\mathcal{U}\\mathcal{S}}^{\\mathsf{T}}\\mathbf{y}_{\\mathcal{U}}\nrepresents the correction contributed by the static blockâ€™s equations. Intuitively, even though the static variables are held fixed, their residual constraints still exert an influence, which propagates to the dynamic variables through this coupling term.\nBy caching the factorization and solver associated with the static block\nğ‘\nğ’°\nâ€‹\nğ’°\n\\mathbf{R}_{\\mathcal{U}\\mathcal{U}}\n(i.e., intermediate Schur complements), redundant computations are avoided across successive GN iterations, yielding substantial efficiency gains.\nThis strategy is conceptually analogous to restricting computation to the Markov blanket\n1\n1\n1\nIn a pose graph, the Markov blanket of a node comprises its directly connected neighbors. Conditioning on this set renders the node independent of all others. Similarly, in SLAM, the effect of new measurements propagates primarily through the immediate neighbors of the affected variables. The cached blockâ€“Schur complement method leverages this principle: only variables reachable from the affected set via the elimination tree are re-solved, while the rest are left untouched.\n, thereby localizing updates to the region of the pose graph most impacted by the new measurements.\nConsequently, the partial solver computes the increments\nğ\nğ’®\nt\n\\mathbf{d}_{\\mathcal{S}_{t}}\nefficiently by reusing cached static-block solutions and solving only the reduced dynamic block through the Schur complement. This significantly lowers computational cost and enables scalable incremental SLAM.\n3.4\nDetermining Affected Variables\nAfter each partial solve, we determine which variables still require updates and thus remain relevant for the next GN iteration. This amounts to updating the active set\nğ’®\nt\n\\mathcal{S}_{t}\nthrough two complementary steps: pruning converged variables and expanding to maintain consistency.\nPruning\n:\nVariables whose increments are sufficiently small are considered converged and are removed from the active set. Formally, we apply the threshold\nÏ„\nd\n\\tau_{d}\nto the increment vector\nğ\nğ’®\nt\n\\mathbf{d}_{\\mathcal{S}_{t}}\n, retaining only those indices with significant updates, i.e.,\nğ’®\nt\nâ†\n{\ni\nâˆˆ\nğ’®\nt\n:\n|\nd\nt\n,\ni\n|\n>\nÏ„\nd\n}\n,\n\\mathcal{S}_{t}\\;\\leftarrow\\;\\big\\{\\,i\\in\\mathcal{S}_{t}:|d_{t,i}|>\\tau_{d}\\,\\big\\},\n(16)\nwhere\nd\nt\n,\ni\nd_{t,i}\ndenotes the\ni\ni\n-th entry of\nğ\nğ’®\nt\n\\mathbf{d}_{\\mathcal{S}_{t}}\n.\nSince poses and landmarks are represented by blocks of variables, we apply the pruning conservatively at the block level: if any variable within a node is retained, we keep the entire block of that node.\nExpansion\n:\nAdjustments to the variables in\nğ’®\nt\n\\mathcal{S}_{t}\ncan induce new inconsistencies in the variables of neighboring nodes through their shared measurements (edges). To capture this effect, we collect all edges incident to the nodes whose variables are in\nğ’®\nt\n\\mathcal{S}_{t}\n:\nâ„°\nt\n=\nâ‹ƒ\nn\n:\nvars\nâ€‹\n(\nn\n)\nâŠ†\nğ’®\nt\nedges\nâ€‹\n(\nn\n)\n,\n\\mathcal{E}_{t}\\;=\\;\\bigcup_{n:\\,\\mathrm{vars}(n)\\subseteq\\mathcal{S}_{t}}\\mathrm{edges}(n),\n(17)\nwhere\nvars\nâ€‹\n(\nn\n)\n\\mathrm{vars}(n)\ndenotes the variables of node\nn\nn\nand\nedges\nâ€‹\n(\nn\n)\n\\mathrm{edges}(n)\nits incident edges. We then enlarge the active set to include the variables of all nodes participating in these edges:\nğ’®\nt\nâ†\nğ’®\nt\nâˆª\nâ‹ƒ\ne\nâˆˆ\nâ„°\nt\nâ‹ƒ\nn\nâˆˆ\nends\nâ€‹\n(\ne\n)\nvars\nâ€‹\n(\nn\n)\n.\n\\mathcal{S}_{t}\\;\\leftarrow\\;\\mathcal{S}_{t}\\;\\cup\\;\\bigcup_{e\\in\\mathcal{E}_{t}}\\bigcup_{n\\in\\mathrm{ends}(e)}\\mathrm{vars}(n).\n(18)\nTo build intuition for the expansion step, consider an updated variable\nx\ni\nâˆˆ\nğ’®\nt\nx_{i}\\in\\mathcal{S}_{t}\nthat participates in a scalar measurement\nf\ni\n,\nk\nâ€‹\n(\nx\ni\n,\nx\nk\n)\nf_{i,k}(x_{i},x_{k})\nwith a neighbor\nx\nk\nâˆ‰\nğ’®\nt\nx_{k}\\notin\\mathcal{S}_{t}\n. The residual for this edge is\nr\ni\nâ€‹\nk\n=\nf\ni\n,\nk\nâ€‹\n(\nx\ni\n,\nx\nk\n)\n,\nr_{ik}=f_{i,k}(x_{i},x_{k}),\n(19)\nwith Jacobians\nJ\ni\n=\nâˆ‚\nf\ni\n,\nk\nâˆ‚\nx\ni\nJ_{i}=\\tfrac{\\partial f_{i,k}}{\\partial x_{i}}\nand\nJ\nk\n=\nâˆ‚\nf\ni\n,\nk\nâˆ‚\nx\nk\nJ_{k}=\\tfrac{\\partial f_{i,k}}{\\partial x_{k}}\n.\nIf\nx\ni\nx_{i}\nis updated by an increment\nd\ni\nd_{i}\n, the residual changes approximately as\nÎ”\nâ€‹\nr\ni\n,\nk\nâ‰ˆ\nJ\ni\nâ€‹\nd\ni\n.\n\\Delta r_{i,k}\\;\\approx\\;J_{i}d_{i}.\n(20)\nFor the edge to remain consistent,\nx\nk\nx_{k}\nwould need to be adjusted by an increment\nd\nk\nd_{k}\nsuch that\nJ\ni\nâ€‹\nd\ni\n+\nJ\nk\nâ€‹\nd\nk\nâ‰ˆ\n0\n.\nJ_{i}d_{i}+J_{k}d_{k}\\;\\approx\\;0.\n(21)\nHence, if\n|\nJ\ni\nâ€‹\nd\ni\n|\n|J_{i}d_{i}|\nexceeds a threshold, the neighbor\nx\nk\nx_{k}\nmust also be included in\nğ’®\nt\n\\mathcal{S}_{t}\n. This illustrates how updates propagate through measurement connections, motivating the expansion rule.\nIn summary, the updated active set after each partial solve consists of 1) variables whose increments remain significant, 2) their full node blocks, and 3) all neighboring variables connected through incident edges. This prune-expand cycle ensures that only genuinely affected parts of the graph are revisited in the next iteration, balancing accuracy and efficiency.\n3.5\nSelective Partial Optimization\nOnce the active set of variables\nğ’®\nt\n\\mathcal{S}_{t}\nhas been initialized at increment\nt\nt\nfrom the new measurements (and possibly expanded via IGG, cf. Section\n3.2\n), GN iterations are carried out selectively until either\nğ’®\nt\n\\mathcal{S}_{t}\nbecomes empty or the maximum number of iterations\nÏ„\nGN\n\\tau_{\\mathrm{GN}}\nis reached. Each iteration proceeds as follows:\n1.\nPartial solve\n: compute the GN step for the active variables by solving\n(\nğ‘\nt\nğ–³\nâ€‹\nğ‘\nt\n)\nâ€‹\nğ\n=\nğ›\nt\n(\\mathbf{R}_{t}^{\\mathsf{T}}\\mathbf{R}_{t})\\mathbf{d}=\\mathbf{b}_{t}\nrestricted to\nğ\nğ’®\nt\n\\mathbf{d}_{\\mathcal{S}_{t}}\n, i.e., the increments for the current active set, as described in section\n3.3\n.\n2.\nActive-set update\n: prune converged variables (those with\n|\nd\nt\n,\ni\n|\nâ‰¤\nÏ„\nd\n|d_{t,i}|\\leq\\tau_{d}\n) and expand to include additional affected variables, preserving block structure and respecting pose-graph connectivity, to obtain the updated\nğ’®\nt\n\\mathcal{S}_{t}\n, as detailed in Section\n3.4\n.\n3.\nConvergence check\n: terminate the GN iterations if\nğ’®\nt\n=\nâˆ…\n\\mathcal{S}_{t}=\\emptyset\n.\n4.\nState update\n: update the current estimate for the active variables,\nğ±\nğ’®\nt\nâ†\nğ±\nğ’®\nt\nâˆ’\nğ\nğ’®\nt\n\\mathbf{x}_{\\mathcal{S}_{t}}\\;\\leftarrow\\;\\mathbf{x}_{\\mathcal{S}_{t}}-\\mathbf{d}_{\\mathcal{S}_{t}}\n.\n5.\nRelinearization\n: recompute Jacobians and residuals only for edges involving variables in\nğ’®\nt\n\\mathcal{S}_{t}\n. Since all other variables remain fixed, this requires updating only the corresponding rows of\nğ‰\nt\n\\mathbf{J}_{t}\nand entries of\nğ‘\nt\n\\mathbf{R}_{t}\nand\nğ›\nt\n\\mathbf{b}_{t}\n. This can be implemented as a lightweight refactorization of the affected portions of\nğ‘\nt\n\\mathbf{R}_{t}\n, e.g., via sparse low-rank factor update techniques or localized Bayes tree refactorization.\nThis loop terminates when all increments fall below tolerance\nÏ„\nd\n\\tau_{d}\nor\nÏ„\nGN\n\\tau_{\\mathrm{GN}}\niterations have been performed. The outcome is an updated state estimate\nğ±\nt\n\\mathbf{x}_{t}\nthat approximately minimizes the nonlinear least-squares cost after including the new measurements at increment\nt\nt\n. Crucially, we also maintain an updated factor\nğ‘\nt\n\\mathbf{R}_{t}\n, valid for the current linearization around\nğ±\nt\n\\mathbf{x}_{t}\n, which is reused in subsequent increments, thus avoiding costly refactorization from scratch.\nTo build intuition, consider two limiting scenarios:\nâ€¢\nHighly informative increments:\nIf each new measurement provides strong information (e.g., frequent loop closures), the information gain\nÎ”\nâ€‹\nÎ·\nt\n\\Delta\\eta_{t}\nwill exceed the threshold\nÏ„\nÎ·\n\\tau_{\\eta}\n, and\nğ’®\nt\n\\mathcal{S}_{t}\nwill initially include all variables. If all entries of\nğ\nt\n\\mathbf{d}_{t}\nare as large as\nÏ„\nd\n\\tau_{d}\n, this effectively becomes batch optimization at each increment, ensuring maximum accuracy but at high computational cost.\nâ€¢\nWeakly informative increments:\nIf new measurements add little information (e.g., small odometry steps or redundant observations), only a small subset of recent variables enters\nğ’®\nt\n\\mathcal{S}_{t}\n. Updates are then highly localized, with minimal computational effort.\nIn practice, the behavior lies between these extremes. Odometry typically yields small or negligible updates, while loop closures or globally informative measurements trigger broader updates. The thresholds\nÏ„\nÎ·\n\\tau_{\\eta}\nand\nÏ„\nd\n\\tau_{d}\nplay a stabilizing role: small improvements accumulate until\nÏ„\nÎ·\n\\tau_{\\eta}\nis exceeded, at which point more variables are included and multiple GN iterations propagate corrections globally. This amortizes computation across increments and yields scalable performance in incremental SLAM.\n3.6\nComputational Complexity\nEach increment in our approach consists of up to\nÏ„\nGN\n\\tau_{\\mathrm{GN}}\nGN iterations, each involving: (i) relinearization of a subset of variables, (ii) incremental (rank) Cholesky updates or downdates to the factor\nğ‘\nt\n\\mathbf{R}_{t}\n, and (iii) a partial solution of the underlying system of linear equations restricted to the affected variables. By keeping\nÏ„\nGN\n\\tau_{\\mathrm{GN}}\nsmall (e.g., 5-10), we bound the per-increment cost similarly to iSAM2â€™s fluid relinearization scheme\n[\n29\n]\n, but with an important distinction: computations are focused strictly on the active subset\nğ’®\nt\n\\mathcal{S}_{t}\n, which is typically a small fraction of the full state. This yields substantial savings in large graphs, where loop closures are infrequent or mostly local.\nThe main computational effort arises from two operations:\nâ€¢\nCholesky up(down)date\n: When a variable is relinearized, the corresponding columns of\nğ‘\nt\n\\mathbf{R}_{t}\nare modified. The cost of these operations is approximately\nmin\nâ¡\n(\n2\nâ€‹\nâˆ‘\ni\nâˆˆ\nğ’®\nt\nÎº\nt\n,\ni\n2\n,\nâˆ‘\ni\n=\n1\nN\nÎº\nt\n,\ni\n2\n)\n,\n\\min\\left(2\\sum_{i\\in\\mathcal{S}_{t}}\\kappa_{t,i}^{2},\\;\\sum_{i=1}^{N}\\kappa_{t,i}^{2}\\right),\n(22)\nwhere\nÎº\nt\n,\ni\n\\kappa_{t,i}\ndenotes the number of nonzeros in column\ni\ni\nof\nğ‘\nt\n\\mathbf{R}_{t}\n.\nThis expression shows that the update cost is always bounded by that of a full refactorization. For new edges, the cost is halved as no downdate is required.\nâ€¢\nPartial solve\n: Once the system has been updated, the increment for the affected variables is computed via sparse triangular forward and backward substitutions. The cost is roughly\n2\nâ€‹\nâˆ‘\ni\nâˆˆ\nğ’®\nt\nÎº\ni\nâ€‹\nt\n,\n2\\sum_{i\\in\\mathcal{S}_{t}}\\kappa_{it},\n(23)\nwhich scales linearly with the number of nonzeros in the relevant columns of\nğ‘\nt\n\\mathbf{R}_{t}\n. Equivalently, this corresponds to solving a small linear system restricted to the Markov blanket of\nğ’®\nt\n\\mathcal{S}_{t}\n, whose size is typically modest.\nBecause\nğ‘\nt\n\\mathbf{R}_{t}\nis sparse and exhibits low fill-in under good orderings (e.g., planar or chain-like SLAM graphs), these operations are highly efficient in practice. Other operations, Jacobian construction, symbolic updates (e.g., reordering, elimination tree maintenance), and evaluation of information gain, incur negligible overhead compared to matrix factorization and solves.\nOver\nT\nT\nincrements, the total cost is driven by the cumulative size of the affected subsets\nğ’®\nt\n\\mathcal{S}_{t}\n. In the common case where loop closures or new observations only modify a bounded region of the graph, the accumulated cost scales linearly with\nT\nT\n. In contrast, worst-case behavior, such as every increment triggering a global update, results in a cost proportional to\nT\nT\nfull solves, though this is rare in real-world SLAM scenarios.\nHence, the overall cost depends on the total number of affected variables across all increments, rather than the total number of variables. In realistic scenarios, most updates remain local, and only a small fraction of increments trigger global corrections. This yields near-linear cumulative complexity in\nT\nT\n.\nIn our implementation, we set\nÏ„\nGN\n=\n10\n\\tau_{\\mathrm{GN}}=10\n). Larger values yield diminishing returns within a single increment, since very large loop closures are uncommon in well-designed SLAM trajectories. If\nÏ„\nGN\n=\n1\n\\tau_{\\mathrm{GN}}=1\n, the method reduces to an iSAM2-style update (one linear solve per step, with occasional full relinearization, e.g., when\nÏ„\nÎ·\n\\tau_{\\eta}\nis exceeded). At the other extreme, setting\nÏ„\nÎ·\n=\n0\n\\tau_{\\eta}=0\n(always update) and choosing large\nÏ„\nGN\n\\tau_{\\mathrm{GN}}\nreproduces batch bundle adjustment at every step, which is accurate but prohibitively expensive. Our approach therefore spans the spectrum between naive incremental and full batch optimization, with thresholds\nÏ„\nÎ·\n\\tau_{\\eta}\nand\nÏ„\nd\n\\tau_{d}\ngoverning the trade-off between efficiency and accuracy.\nAs shown in Section\n5\n, our information-guided gating allows most increments to proceed with localized updates only, while limiting the number of global updates to a small fraction of\nT\nT\n. This preserves accuracy while substantially reducing runtime, leading to performance that is competitive with or better than existing incremental solvers, particularly on large graphs with frequent local updates.\n4\nTheoretical Analysis\nIn this section, we show that the two heuristics at the core of our approach, (i) the information-guided gating and (ii) the selective partial optimization, do not compromise convergence. Under standard smoothness assumptions, they attain the same stationary point and local convergence rate as a full GN solver, while enabling the substantial FLOP savings reported in Section\n5\n.\nRecall the cost functionÂ (\n2\n) and let\nğ \nâ€‹\n(\nğ±\n)\n=\nâˆ‡\nc\nâ€‹\n(\nğ±\n)\n\\mathbf{g}(\\mathbf{x})=\\nabla c(\\mathbf{x})\n. We assume that the residual functions\nğ«\nj\n\\mathbf{r}_{j}\ninÂ (\n2\n) are twice differentiable and that the GN Hessian\nâˆ‡\n2\nc\nâ€‹\n(\nğ±\n)\n\\nabla^{2}c(\\mathbf{x})\nis Lipschitz-continuous in a neighborhood of the solution\nğ±\nâ‹†\n\\mathbf{x}^{\\star}\n, i.e.,\nâ€–\nâˆ‡\n2\nc\nâ€‹\n(\nğ±\n)\nâˆ’\nâˆ‡\n2\nc\nâ€‹\n(\nğ²\n)\nâ€–\nâ‰¤\nL\nâ€‹\nâ€–\nğ±\nâˆ’\nğ²\nâ€–\n,\nâˆ€\nğ±\n,\nğ²\nâ€‹\nnear\nâ€‹\nğ±\nâ‹†\n,\n\\bigl\\|\\nabla^{2}c(\\mathbf{x})-\\nabla^{2}c(\\mathbf{y})\\bigr\\|\\leq L\\,\\bigl\\|\\mathbf{x}-\\mathbf{y}\\bigr\\|,\\qquad\\forall\\,\\mathbf{x},\\mathbf{y}\\text{ near }\\mathbf{x}^{\\star},\n(24)\nfor some constant\nL\n>\n0\nL>0\n.\n4.1\nIGG with Localized Updates\nLet\nğ’®\nâ€²\n\\mathcal{S}^{\\prime}\ndenote the set of variables involved in the newly arrived measurements at increment\nt\nt\n. If the information gain\nÎ”\nâ€‹\nÎ·\nt\n\\Delta\\eta_{t}\nfalls below the threshold\nÏ„\nÎ·\n\\tau_{\\eta}\n, the algorithm does not trigger a full graph update. Instead, it restricts computation to\nğ’®\nâ€²\n\\mathcal{S}^{\\prime}\nand solves the corresponding GN sub-system. The resulting step is therefore an inexact GN update whose residual satisfies\nâ€–\nğ \nt\nâˆ’\nâˆ‡\nğ’®\nâ€²\n2\nc\nâ€‹\n(\nğ±\nt\nâˆ’\n1\n)\nâ€‹\nğ\nğ’®\nâ€²\nâ€–\nâ‰¤\nÎ¾\nt\nâ€‹\nâ€–\nğ \nt\nâˆ’\n1\nâ€–\n,\nÎ¾\nt\n=\n1\nâˆ’\nÎ”\nâ€‹\nÎ·\nt\nÏ„\nÎ·\n<\n1\n.\n\\bigl\\|\\mathbf{g}_{t}-\\nabla^{2}_{\\mathcal{S}^{\\prime}}c(\\mathbf{x}_{t-1})\\,\\mathbf{d}_{\\mathcal{S}^{\\prime}}\\bigr\\|\\leq\\xi_{t}\\,\\|\\mathbf{g}_{t-1}\\|,\\qquad\\xi_{t}=1-\\frac{\\Delta\\eta_{t}}{\\tau_{\\eta}}<1.\n(25)\nSince\nÎ¾\nt\n<\n1\n\\xi_{t}<1\nwhenever any positive information is assimilated, the step satisfies the forcingâ€term condition in the Eisenstat-Walker inexact Newton framework\n[\n15\n]\n, yielding the following results:\nLemma 4.1\n.\nSuppose\nc\nâ€‹\n(\nğ±\n)\nc(\\mathbf{x})\nhas a Lipschitz-continuous GN Hessian in a neighborhood of the solution\nğ±\nâ‹†\n\\mathbf{x}^{\\star}\n. Then, the updates generated under the IGG trigger, with localized solves on\nğ’®\nâ€²\n\\mathcal{S}^{\\prime}\nwhen\nÎ”\nâ€‹\nÎ·\nt\n<\nÏ„\nÎ·\n\\Delta\\eta_{t}<\\tau_{\\eta}\n, produce a monotonically non-increasing cost sequence and converge to a stationary point of\nc\nâ€‹\n(\nğ±\n)\nc(\\mathbf{x})\n.\nProof.\nIf\nÎ”\nâ€‹\nÎ·\nt\nâ‰¥\nÏ„\nÎ·\n\\Delta\\eta_{t}\\geq\\tau_{\\eta}\n, a standard GN step is taken, which is a descent direction for\nc\nâ€‹\n(\nğ±\n)\nc(\\mathbf{x})\n.\nIf\nÎ”\nâ€‹\nÎ·\nt\n<\nÏ„\nÎ·\n\\Delta\\eta_{t}<\\tau_{\\eta}\n, the algorithm updates only the block\nğ’®\nâ€²\n\\mathcal{S}^{\\prime}\n, yielding the exact block Newton step\nğ\nğ’®\nâ€²\n\\mathbf{d}_{\\mathcal{S}^{\\prime}}\n. Since the block Hessian\nâˆ‡\nğ’®\nâ€²\n2\nc\nâ€‹\n(\nğ±\n)\n\\nabla^{2}_{\\!\\mathcal{S}^{\\prime}}c(\\mathbf{x})\nis positive definite in the neighborhood, the quadratic model ensures\nc\nâ€‹\n(\nğ±\nt\nâˆ’\n1\n)\nâˆ’\nc\nâ€‹\n(\nğ±\nt\n)\nâ‰¥\nÎ±\nâ€‹\nâ€–\nğ\nğ’®\nâ€²\nâ€–\n2\nc(\\mathbf{x}_{t-1})-c(\\mathbf{x}_{t})\\geq\\alpha\\|\\mathbf{d}_{\\mathcal{S}^{\\prime}}\\|^{2}\n(26)\nfor some constant\nÎ±\n>\n0\n\\alpha>0\n.\nMoreover, the forcing-term bound of the Eisenstat-Walker inexact Newton framework\n[\n15\n]\nis satisfied with\nÎ¾\nt\n=\n1\nâˆ’\nÎ”\nâ€‹\nÎ·\nt\n/\nÏ„\nÎ·\n<\n1\n\\xi_{t}=1-\\Delta\\eta_{t}/\\tau_{\\eta}<1\n. Therefore, the classical convergence theory for inexact Newton applies, implying global convergence to a stationary point.\nâˆ\nCorollary 4.2\n.\nUnder the assumptions of Lemma\n4.1\n, the iterates generated by the IGG trigger converge to the same stationary point\nğ±\nâ‹†\n\\mathbf{x}^{\\star}\nas a full GN solver. Moreover, in the neighborhood of\nğ±\nâ‹†\n\\mathbf{x}^{\\star}\n, the local convergence rate is identical to that of standard GN: linear in general, and superlinear (quadratic) when the residuals vanish at the solution.\nProof.\nThe proof follows directly from classical inexact Newton theory\n[\n15\n]\n. Since the forcing term satisfies\nÎ¾\nt\n<\n1\n\\xi_{t}<1\n, global convergence is ensured. In the local regime, the accuracy of the block solve ensures that the forcing term can be made arbitrarily small as\nÎ”\nâ€‹\nÎ·\nt\nâ†’\nÏ„\nÎ·\n\\Delta\\eta_{t}\\to\\tau_{\\eta}\n, which recovers the standard GN rate. Therefore, the method achieves the same asymptotic convergence behavior as full GN: linear when residuals persist at the solution, and quadratic when\nğ«\nâ€‹\n(\nğ±\nâ‹†\n)\nâ†’\n0\n\\mathbf{r}(\\mathbf{x}^{\\star})\\to 0\n.\nâˆ\n4.2\nSPO as Block-Coordinate Descent\nWhen the IGG trigger fires, we form a subset\nğ’®\nt\nâŠ†\n{\n1\n,\nâ€¦\n,\nN\nt\n}\n\\mathcal{S}_{t}\\subseteq\\{1,\\dots,N_{t}\\}\nof variables whose corresponding gradient entries exceed the tolerance\nÏ„\nd\n\\tau_{d}\n, and solve the GN subproblem restricted to\nğ’®\nt\n\\mathcal{S}_{t}\n. This procedure is equivalent to a greedy block-coordinate GN step.\nFor smooth functions with positive-definite GN blocks, such steps guarantee a sufficient decrease of the form\nc\nâ€‹\n(\nğ±\nt\n,\nl\n)\nâ‰¤\nc\nâ€‹\n(\nğ±\nt\n,\nl\nâˆ’\n1\n)\nâˆ’\nÎ±\nâ€‹\nâ€–\nâˆ‡\nğ’®\nt\nc\nâ€‹\n(\nğ±\nt\n,\nl\nâˆ’\n1\n)\nâ€–\n2\nc(\\mathbf{x}_{t,l})\\leq c(\\mathbf{x}_{t,l-1})-\\alpha\\|\\nabla_{\\mathcal{S}_{t}}c(\\mathbf{x}_{t,l-1})\\|^{2}\n(27)\nfor some constant\nÎ±\n>\n0\n\\alpha>0\n. This ensures convergence provided that every variable enters the active set infinitely often\n[\n43\n,\n40\n]\n. Here,\nl\nl\ndenotes the GN iteration index within increment\nt\nt\n. To avoid clutter, we suppress this index and simply write\nğ±\nt\n\\mathbf{x}_{t}\n, unless the inner iteration index is explicitly required. Our selection rule guarantees this condition by retaining each variable in\nğ’®\nt\n\\mathcal{S}_{t}\nuntil its local gradient falls below\nÏ„\nd\n\\tau_{d}\n. Therefore, we have the following result:\nLemma 4.3\n.\nSuppose each active block\nğ’®\nt\n\\mathcal{S}_{t}\nis solved exactly, and that the residual gradient satisfies\nâ€–\nâˆ‡\nâˆ\nc\nâ€‹\n(\nğ±\nt\n,\nl\n)\nâ€–\nâ‰¤\nÏ„\nd\n\\|\\nabla_{\\infty}c(\\mathbf{x}_{t,l})\\|\\leq\\tau_{d}\nat the end of each increment. Then, the SPO scheme converges to the same stationary point\nğ±\nt\nâ‹†\n\\mathbf{x}_{t}^{\\star}\nas full GN, with local linear convergence. Moreover, if\nÏ„\nd\nâ†’\n0\n\\tau_{d}\\to 0\nand\nğ«\nâ€‹\n(\nğ±\nâ‹†\n)\nâ†’\n0\n\\mathbf{r}(\\mathbf{x}^{\\star})\\to 0\n, the local rate is superlinear (quadratic in the ideal case), matching that of full GN.\nProof.\nEach SPO step is a greedy block Newton update on a smooth function with positive-definite block Hessian, and thus satisfies the sufficient decrease condition\nc\nâ€‹\n(\nğ±\nt\n,\nl\n)\nâ‰¤\nc\nâ€‹\n(\nğ±\nt\n,\nl\nâˆ’\n1\n)\nâˆ’\nÎ±\nâ€‹\nâ€–\nâˆ‡\nğ’®\nt\nc\nâ€‹\n(\nğ±\nt\n,\nl\nâˆ’\n1\n)\nâ€–\n2\nc(\\mathbf{x}_{t,l})\\leq c(\\mathbf{x}_{t,l-1})-\\alpha\\|\\nabla_{\\mathcal{S}_{t}}c(\\mathbf{x}_{t,l-1})\\|^{2}\n[\n43\n, TheoremÂ 2.1]\n. Because every variable whose gradient exceeds\nÏ„\nd\n\\tau_{d}\nwill eventually be added to\nğ’®\nt\n\\mathcal{S}_{t}\n, each coordinate is updated infinitely often or its gradient remains\nâ‰¤\nÏ„\nd\n\\leq\\tau_{d}\n. This guarantees convergence to a first-order stationary point. Standard GN theory\n[\n1\n, Ch.Â 6]\nthen implies local linear convergence, and superlinear (quadratic) convergence if\nÏ„\nd\nâ†’\n0\n\\tau_{d}\\to 0\nand the residual vanishes at the solution.\nâˆ\n4.3\nCombined Convergence Guarantee\nCombining Lemmas\n4.1\nand\n4.3\nyields:\nTheorem 4.4\n.\nAssume\nc\nâ€‹\n(\nğ±\n)\nc(\\mathbf{x})\nsatisfies the standard smoothness conditions for local GN convergence. Then, the proposed approach combining the IGG scheme with threshold\nÏ„\nÎ·\n>\n0\n\\tau_{\\eta}>0\nand the SPO strategy with tolerance\nÏ„\nd\n>\n0\n\\tau_{d}>0\n, generates a sequence\nğ±\nt\n\\mathbf{x}_{t}\nthat (i) yields a monotonically non-increasing cost\nc\nâ€‹\n(\nğ±\nt\n)\nc(\\mathbf{x}_{t})\n, (ii) remains bounded, and (iii) converges to the same stationary point\nğ±\nâ‹†\n\\mathbf{x}^{\\star}\nas batch GN. Moreover, the local asymptotic rate matches that of full GN, while empirical FLOP counts (Section\n5\n) demonstrate a substantial reduction in numerical work.\nCorollary 4.5\n.\nThe proposed approach is a computationally efficient realization of GN: it achieves the same stationary point\nğ±\nâ‹†\n\\mathbf{x}^{\\star}\nand local convergence rate as batch GN, while asymptotically requiring fewer floating-point operations due to selective triggering and partial updates.\nThese theoretical results mirror our empirical findings: final accuracy is indistinguishable from full batch optimization, while computational cost is significantly reduced because (i) many low-information increments trigger only local updates, and (ii) later GN iterations act on progressively smaller active sets. A more refined analysis, such as incorporating adaptive forcing terms or stochastic update rules, remains an interesting direction for future work.\n5\nExperiments\n5.1\nSetup\nDatasets\n: We evaluate the proposed approach against the most closely related contenders on several standard SLAM benchmark datasets, summarized in Table\n1\n. These datasets, provided in g2o\n[\n34\n]\nand TORO\n[\n20\n]\nformats and available from the online repository\n[\n6\n]\n, cover scenarios with varying frequencies of loop closures and exhibit typical SLAM sparsity patterns\n[\n5\n]\n.\nTo emulate realistic incremental operation, we reorder the edges in each dataset so that measurements arrive in the natural acquisition order, and loop-closure edges are incorporated as soon as both associated nodes have been initialized. This contrasts with the original datasets, where loop-closures are sometimes deferred until the end.\nIn addition, we construct a dataset, called MIT-P, by adding position priors to every\n50\n50\nposes in the MIT dataset, with Gaussian noise of standard deviation\n1\n1\\,\nm on each axis. These priors simulate scenarios where reasonably accurate external pose estimates are intermittently available, such as from indoor localization systems (e.g., UWB or WiFi-based) or from global navigation satellite systems (GNSS) outdoors. Since the considered datasets do not provide ground-truth poses, we use the batch solution as a surrogate ground truth for generating the position priors.\nTable 1:\nUtilized benchmark SLAM datasets.\ndataset\nposes\nedges\nloop closures\nÏ„\nd\n\\tau_{d}\nÏ„\nÎ·\n\\tau_{\\eta}\nMIT, MIT-P\n808\n827\n20\n10\nâˆ’\n3\n10^{-3}\n1\nFR079\n989\n1217\n229\n10\nâˆ’\n4\n10^{-4}\n0.6\nCSAIL\n1045\n1172\n128\n10\nâˆ’\n5\n10^{-5}\n0.95\nIntel\n1228\n1483\n256\n10\nâˆ’\n6\n10^{-6}\n0.72\nFRH\n1316\n2820\n1505\n10\nâˆ’\n7\n10^{-7}\n0.45\nAccuracy measures\n: We use the normalized chi-squared error, denoted by\nN\nâ€‹\nÏ‡\nt\n2\n\\mathrm{N}\\chi^{2}_{t}\n, and the absolute trajectory error (ATE) as our accuracy measures. For each approach, we report both their final values (after the last increment) and their average values (over all increments). The normalized chi-squared error is directly related to the nonlinear least-squares costÂ (\n2\n) as\nN\nâ€‹\nÏ‡\nt\n2\n=\n2\nâ€‹\nc\nâ€‹\n(\nğ±\nt\n)\nM\nt\n\\mathrm{N}\\chi^{2}_{t}=\\frac{2c(\\mathbf{x}_{t})}{M_{t}}\n(28)\nwhere\nM\nt\nM_{t}\nis the number of scalar measurement equations available at increment\nt\nt\n.\nThe ATE quantifies the deviation of the estimated trajectory from ground truth. It is computed as the root mean squared error (RMSE) between the estimated poses\nğ±\nt\n,\np\n\\mathbf{x}_{t,p}\nand the corresponding ground-truth poses\nğ±\np\nâ‹†\n\\mathbf{x}^{\\star}_{p}\n, after alignment by a rigid-body transformation, such as the Kabsch algorithm\n[\n27\n,\n28\n]\n, i.e.,\nATE\n=\n1\nP\nt\nâ€‹\nâˆ‘\np\n=\n1\nP\nt\nâ€–\nğ±\np\nâ‹†\nâˆ’\nğ“\nt\nâ€‹\nğ±\nt\n,\np\nâ€–\n2\n,\n\\mathrm{ATE}=\\sqrt{\\frac{1}{P_{t}}\\sum_{p=1}^{P_{t}}\\bigl\\|\\mathbf{x}^{\\star}_{p}-\\mathbf{T}_{t}\\,\\mathbf{x}_{t,p}\\bigr\\|^{2}},\n(29)\nwhere\nP\nt\nP_{t}\nis the number of poses at increment\nt\nt\n, and\nğ“\nt\nâˆˆ\nSE\nâ€‹\n(\n2\n)\n\\mathbf{T}_{t}\\in\\mathrm{SE}(2)\ndenotes the optimal alignment transformation at that increment. As before, we take the batch solution as the reference ground truth.\nConsidered approaches\n: We include the following approaches in our evaluations:\nâ€¢\nGN1: Performs a\nsingle\nGN iteration per increment. This setting resembles iSAM2\n[\n29\n]\nwithout any relinearization threshold but with a variable-update threshold. For fairness, new measurements are relinearized exactly as in our approach, ensuring comparable treatment of updates.\nâ€¢\nGNi: Performs\nmultiple\nGN iterations at each increment without any selectivity, i.e., no gating or partial optimization. All variables may be updated and relinearized whenever new measurements arrive. To maintain consistency with other approaches, the threshold\nÏ„\nd\n\\tau_{d}\nis used to decide early termination of GN iterations (cf. Algorithm\n2\n).\nThis algorithm resembles a standard incremental solver without periodic batch steps. It is maximally responsive and accurate but may perform considerable redundant work when information gain is small.\nâ€¢\nGNi-LCG: Similar to GNi, but performs GN iterations\nonly\nwhen a loop closure is detected, hence the term loop-closure gating (LCG). Loop closures are identified as measurements that connect previously unconnected parts of the graph (e.g., a pose-pose constraint between non-consecutive poses). This strategy is comparable to approaches proposed in\n[\n39\n,\n23\n]\n.\nâ€¢\nGNi-IGG: Similar to GNi, but performs GN iterations\nonly\nwhen the information gain exceeds the threshold\nÏ„\nÎ·\n\\tau_{\\eta}\n, according to the proposed IGG scheme.\nâ€¢\nGNi-SPO: Extends GNi by incorporating the proposed SPO scheme. Multiple GN iterations may be performed at each increment, but updates are restricted to the active set of affected variables. No gating is applied, meaning all variables are considered potentially affected at every increment.\nâ€¢\nGNi-SPO-LCG: Combines SPO with LCG. As in GNi-SPO, only the active set of affected variables is updated and relinearized, while global GN optimization is triggered solely at loop closures. When loop closures are the primary source of information, this approach is expected to behave similarly to the proposed approach.\nâ€¢\nGNi-SPO-IGG: The proposed algorithm, capable of performing multiple GN iterations at each increment while incorporating both SPO and IGG.\nAlgorithm 2\nIncremental SLAM with no Gating or Partial Optimization\n1:\ninitial estimate\nğ±\n0\n\\mathbf{x}_{0}\n, thresholds\nÏ„\nd\n\\tau_{d}\nand\nÏ„\nGN\n\\tau_{\\mathrm{GN}}\n2:\nupdated estimate\nğ±\nt\n\\mathbf{x}_{t}\nafter each increment\nt\nt\n3:\nfor\nt\n=\n1\n,\n2\n,\nâ€¦\nt=1,2,\\dots\ndo\n4:\nupdate\nğ‰\nt\n\\mathbf{J}_{t}\n,\nğ«\nt\n\\mathbf{r}_{t}\n,\nğ‘\nt\n\\mathbf{R}_{t}\n,\nğ›\nt\n\\mathbf{b}_{t}\n, and\nğ©\nt\n\\mathbf{p}_{t}\nbased on new measurements\n5:\nfor\ni\nGN\n=\n1\n,\n2\n,\nâ€¦\n,\nÏ„\nGN\ni_{\\mathrm{GN}}=1,2,\\dots,\\tau_{\\mathrm{GN}}\ndo\n6:\nsolve\n(\nğ‘\nt\nğ–³\nâ€‹\nğ‘\nt\n)\nâ€‹\nğ\nt\n=\nğ›\nt\n(\\mathbf{R}_{t}^{\\mathsf{T}}\\mathbf{R}_{t})\\mathbf{d}_{t}=\\mathbf{b}_{t}\nfor\nğ\nt\n\\mathbf{d}_{t}\n7:\nif\nmax\nâ¡\n|\nğ\nt\n|\nâ‰¤\nÏ„\nd\n\\max|\\mathbf{d}_{t}|\\leq\\tau_{d}\nthen\nbreak\n8:\nend\nif\n9:\nğ±\nt\nâ†\nğ±\nt\nâˆ’\nğ\nt\n\\mathbf{x}_{t}\\leftarrow\\mathbf{x}_{t}-\\mathbf{d}_{t}\n10:\nrecalculate\nğ‰\nt\n\\mathbf{J}_{t}\n,\nğ«\nt\n\\mathbf{r}_{t}\n,\nğ‘\nt\n\\mathbf{R}_{t}\n,\nğ›\nt\n\\mathbf{b}_{t}\n, and\nğ©\nt\n\\mathbf{p}_{t}\nbased on new\nğ±\nt\n\\mathbf{x}_{t}\n11:\nend\nfor\n12:\nend\nfor\nParameters\n: We set\nÏ„\nGN\n=\n10\n\\tau_{\\mathrm{GN}}=10\nfor all approaches, except for GN1, which corresponds to GNi with\nÏ„\nGN\n=\n1\n\\tau_{\\mathrm{GN}}=1\n. We set the thresholds\nÏ„\nd\n\\tau_{d}\nand\nÏ„\nÎ·\n\\tau_{\\eta}\nseparately for each dataset, as listed in Table\n1\n. We perform minimal tuning of these thresholds to ensure that the proposed approach attains accuracy comparable to its competitors, thereby allowing for fair efficiency comparisons. We consider each incremental step involve one measurement (edge).\n5.2\nResults\nIn Table\n2\n, we summarizes the performance evaluation results for all considered approaches across the benchmark datasets. Alongside final and mean N\nÏ‡\n2\n\\chi^{2}\nand ATE values, which capture estimation accuracy, we also report average floating-point operation (FLOP) counts for Cholesky up(down)date (\nupdate\n) and partial-solve (\nsolve\n) steps, as defined in Section\n3.6\n, providing a direct measure of computational efficiency over all increments.\nFor GNi-SPO-IGG, GNi-SPO-LCG, and GNi-SPO, we additionally report (in parentheses) the mean solve FLOPs for when partial solves are replaced with full solves (see lineÂ 10 of Algorithm\n1\n), while still retaining partial variable update and relinearization. This variant isolates the benefit of the partial-solve strategy itself in terms of computational savings. Conceptually, it is analogous to iSAM2 with fluid relinearization when the same variable update and relinearization threshold\nÏ„\nd\n\\tau_{d}\nis applied. Note that for this full-solve variant, accuracy and mean update FLOPs remain unchanged compared to the partial-solve case as only the solve FLOPs differ.\nTo evaluate both intermediate accuracy and computational progression, we plot the evolution of ATE and cumulative update FLOPs over all increments for all considered approaches on four representative datasets, as shown in Figs.\n1\nand\n2\n, respectively.\nTo provide further insight into the behavior of the proposed IGG scheme, in Fig.\n3\n, we plot the information gain\nÎ”\nâ€‹\nÎ·\nt\n\\Delta\\eta_{t}\nacross all increments for four datasets. The plots also indicate increments at which loop closures are detected or position priors are introduced.\nTo examine the influence of the threshold values\nÏ„\nd\n\\tau_{d}\nand\nÏ„\nÎ·\n\\tau_{\\eta}\non the accuracy-efficiency trade-off of the proposed approach, in Fig.\n4(a)\n, we plot the mean ATE and mean update FLOPs of GNi-SPO on the MIT dataset as functions of\nÏ„\nd\n\\tau_{d}\n, and, in Fig.\n5(a)\n, those of GNi-IGG on the FR079 dataset as functions of\nÏ„\nÎ·\n\\tau_{\\eta}\n. For comparison with relevant baselines, Fig.\n4(a)\nalso includes the corresponding curves for GNi and GN1, while Fig.\n5(a)\nincludes those for GNi-LCG, GNi, and GN1.\nTable 2:\nPerformance of different approaches on the considered datasets.\ndataset\napproach\nN\nÏ‡\n2\n\\chi^{2}\nATE\nmean FLOPs\nfinal\nmean\nfinal\nmean\nsolve\nupdate\nMIT\nGNi-SPO-IGG\n1.65918\nÃ—\n10\nâˆ’\n2\n1.65918{\\times}10^{-2}\n1.84891\nÃ—\n10\nâˆ’\n2\n1.84891{\\times}10^{-2}\n3.67389\nÃ—\n10\nâˆ’\n4\n3.67389{\\times}10^{-4}\n5.802394\n5.802394\n2,028 (36,926)\n66,541\nGNi-SPO-LCG\n1.65918\nÃ—\n10\nâˆ’\n2\n1.65918{\\times}10^{-2}\n1.84891\nÃ—\n10\nâˆ’\n2\n1.84891{\\times}10^{-2}\n3.67389\nÃ—\n10\nâˆ’\n4\n3.67389{\\times}10^{-4}\n5.802394\n5.802394\n2,028 (36,926)\n66,541\nGNi-SPO\n1.65915\nÃ—\n10\nâˆ’\n2\n1.65915{\\times}10^{-2}\n1.84891\nÃ—\n10\nâˆ’\n2\n1.84891{\\times}10^{-2}\n3.47418\nÃ—\n10\nâˆ’\n4\n3.47418{\\times}10^{-4}\n5.802397\n5.802397\n19,036 (36,925)\n66,565\nGNi-IGG\n32.5859\n32.5859\n14.6328\n14.6328\n30.4408\n30.4408\n20.3804\n20.3804\n3,696\n98,087\nGNi-LCG\n32.5859\n32.5859\n14.6328\n14.6328\n30.4408\n30.4408\n20.3804\n20.3804\n3,696\n98,087\nGNi\n1.65914\nÃ—\n10\nâˆ’\n2\n1.65914{\\times}10^{-2}\n1.84841\nÃ—\n10\nâˆ’\n2\n1.84841{\\times}10^{-2}\n5.20559\nÃ—\n10\nâˆ’\n11\n5.20559{\\times}10^{-11}\n5.802427\n5.802427\n36,661\n438,548\nGN1\n1.65914\nÃ—\n10\nâˆ’\n2\n1.65914{\\times}10^{-2}\n780\n,\n578\n780,578\n1.05711\nÃ—\n10\nâˆ’\n10\n1.05711{\\times}10^{-10}\n5.850329\n5.850329\n17,704\n405,989\nFR079\nGNi-SPO-IGG\n1.02983\nÃ—\n10\nâˆ’\n2\n1.02983{\\times}10^{-2}\n1.06656\nÃ—\n10\nâˆ’\n2\n1.06656{\\times}10^{-2}\n3.75248\nÃ—\n10\nâˆ’\n5\n3.75248{\\times}10^{-5}\n6.02654\nÃ—\n10\nâˆ’\n2\n6.02654{\\times}10^{-2}\n8,377 (50,546)\n85,685\nGNi-SPO-LCG\n1.02983\nÃ—\n10\nâˆ’\n2\n1.02983{\\times}10^{-2}\n1.06653\nÃ—\n10\nâˆ’\n2\n1.06653{\\times}10^{-2}\n3.75245\nÃ—\n10\nâˆ’\n5\n3.75245{\\times}10^{-5}\n6.02615\nÃ—\n10\nâˆ’\n2\n6.02615{\\times}10^{-2}\n7,355 (50,620)\n87,015\nGNi-SPO\n1.02983\nÃ—\n10\nâˆ’\n2\n1.02983{\\times}10^{-2}\n1.06653\nÃ—\n10\nâˆ’\n2\n1.06653{\\times}10^{-2}\n3.75247\nÃ—\n10\nâˆ’\n5\n3.75247{\\times}10^{-5}\n6.02615\nÃ—\n10\nâˆ’\n2\n6.02615{\\times}10^{-2}\n28,318 (50,620)\n87,034\nGNi-IGG\n5.84489\nÃ—\n10\nâˆ’\n2\n5.84489{\\times}10^{-2}\n5.36751\nÃ—\n10\nâˆ’\n1\n5.36751{\\times}10^{-1}\n3.89112\nÃ—\n10\nâˆ’\n2\n3.89112{\\times}10^{-2}\n8.87086\nÃ—\n10\nâˆ’\n2\n8.87086{\\times}10^{-2}\n12,085\n130,969\nGNi-LCG\n5.84489\nÃ—\n10\nâˆ’\n2\n5.84489{\\times}10^{-2}\n1.81984\nÃ—\n10\nâˆ’\n1\n1.81984{\\times}10^{-1}\n3.89112\nÃ—\n10\nâˆ’\n2\n3.89112{\\times}10^{-2}\n1.22274\nÃ—\n10\nâˆ’\n1\n1.22274{\\times}10^{-1}\n9,334\n101,979\nGNi\n1.02983\nÃ—\n10\nâˆ’\n2\n1.02983{\\times}10^{-2}\n1.06651\nÃ—\n10\nâˆ’\n2\n1.06651{\\times}10^{-2}\n9.95967\nÃ—\n10\nâˆ’\n15\n9.95967{\\times}10^{-15}\n6.02609\nÃ—\n10\nâˆ’\n2\n6.02609{\\times}10^{-2}\n50,620\n460,584\nGN1\n1.02983\nÃ—\n10\nâˆ’\n2\n1.02983{\\times}10^{-2}\n1.06652\nÃ—\n10\nâˆ’\n2\n1.06652{\\times}10^{-2}\n2.78002\nÃ—\n10\nâˆ’\n12\n2.78002{\\times}10^{-12}\n6.02635\nÃ—\n10\nâˆ’\n2\n6.02635{\\times}10^{-2}\n24,808\n437,461\nCSAIL\nGNi-SPO-IGG\n1.10797\nÃ—\n10\nâˆ’\n2\n1.10797{\\times}10^{-2}\n2.80792\nÃ—\n10\nâˆ’\n3\n2.80792{\\times}10^{-3}\n1.23096\nÃ—\n10\nâˆ’\n6\n1.23096{\\times}10^{-6}\n9.11474\nÃ—\n10\nâˆ’\n2\n9.11474{\\times}10^{-2}\n10,245 (51,960)\n268,636\nGNi-SPO-LCG\n1.10797\nÃ—\n10\nâˆ’\n2\n1.10797{\\times}10^{-2}\n2.80776\nÃ—\n10\nâˆ’\n3\n2.80776{\\times}10^{-3}\n1.23113\nÃ—\n10\nâˆ’\n6\n1.23113{\\times}10^{-6}\n9.11517\nÃ—\n10\nâˆ’\n2\n9.11517{\\times}10^{-2}\n10,147 (52,127)\n270,687\nGNi-SPO\n1.10797\nÃ—\n10\nâˆ’\n2\n1.10797{\\times}10^{-2}\n2.80776\nÃ—\n10\nâˆ’\n3\n2.80776{\\times}10^{-3}\n1.23121\nÃ—\n10\nâˆ’\n6\n1.23121{\\times}10^{-6}\n9.11517\nÃ—\n10\nâˆ’\n2\n9.11517{\\times}10^{-2}\n29,651 (52,128)\n271,536\nGNi-IGG\n1.10797\nÃ—\n10\nâˆ’\n2\n1.10797{\\times}10^{-2}\n2.34198\n2.34198\n4.13827\nÃ—\n10\nâˆ’\n14\n4.13827{\\times}10^{-14}\n2.25203\nÃ—\n10\nâˆ’\n1\n2.25203{\\times}10^{-1}\n14,657\n386,088\nGNi-LCG\n1.10797\nÃ—\n10\nâˆ’\n2\n1.10797{\\times}10^{-2}\n3.8143\n3.8143\n5.18019\nÃ—\n10\nâˆ’\n7\n5.18019{\\times}10^{-7}\n2.25553\nÃ—\n10\nâˆ’\n1\n2.25553{\\times}10^{-1}\n15,134\n401,049\nGNi\n1.10797\nÃ—\n10\nâˆ’\n2\n1.10797{\\times}10^{-2}\n2.80718\nÃ—\n10\nâˆ’\n3\n2.80718{\\times}10^{-3}\n2.29725\nÃ—\n10\nâˆ’\n14\n2.29725{\\times}10^{-14}\n9.11515\nÃ—\n10\nâˆ’\n2\n9.11515{\\times}10^{-2}\n52,053\n978,461\nGN1\n1.10797\nÃ—\n10\nâˆ’\n2\n1.10797{\\times}10^{-2}\n2.80897\nÃ—\n10\nâˆ’\n3\n2.80897{\\times}10^{-3}\n1.35270\nÃ—\n10\nâˆ’\n6\n1.35270{\\times}10^{-6}\n9.11548\nÃ—\n10\nâˆ’\n2\n9.11548{\\times}10^{-2}\n23,974\n825,271\nIntel\nGNi-SPO-IGG\n4.85217\nÃ—\n10\nâˆ’\n2\n4.85217{\\times}10^{-2}\n3.42609\nÃ—\n10\nâˆ’\n2\n3.42609{\\times}10^{-2}\n1.01812\nÃ—\n10\nâˆ’\n7\n1.01812{\\times}10^{-7}\n1.40955\nÃ—\n10\nâˆ’\n1\n1.40955{\\times}10^{-1}\n28,609 (77,951)\n330,270\nGNi-SPO-LCG\n4.85121\nÃ—\n10\nâˆ’\n2\n4.85121{\\times}10^{-2}\n3.42331\nÃ—\n10\nâˆ’\n2\n3.42331{\\times}10^{-2}\n4.06794\nÃ—\n10\nâˆ’\n7\n4.06794{\\times}10^{-7}\n1.40951\nÃ—\n10\nâˆ’\n1\n1.40951{\\times}10^{-1}\n20,362 (78,423)\n286,034\nGNi-SPO\n4.85121\nÃ—\n10\nâˆ’\n2\n4.85121{\\times}10^{-2}\n3.42397\nÃ—\n10\nâˆ’\n2\n3.42397{\\times}10^{-2}\n1.18840\nÃ—\n10\nâˆ’\n7\n1.18840{\\times}10^{-7}\n1.40951\nÃ—\n10\nâˆ’\n1\n1.40951{\\times}10^{-1}\n49,525 (78,686)\n357,216\nGNi-IGG\n69.5308\n69.5308\n126.965\n126.965\n1.80246\n1.80246\n5.97116\nÃ—\n10\nâˆ’\n1\n5.97116{\\times}10^{-1}\n39,008\n435,542\nGNi-LCG\n6.01180\n6.01180\n19\n,\n366.4\n19,366.4\n2.86431\n2.86431\n1.27203\n1.27203\n26,905\n336,683\nGNi\n4.85121\nÃ—\n10\nâˆ’\n2\n4.85121{\\times}10^{-2}\n3.42216\nÃ—\n10\nâˆ’\n2\n3.42216{\\times}10^{-2}\n2.18560\nÃ—\n10\nâˆ’\n11\n2.18560{\\times}10^{-11}\n1.40951\nÃ—\n10\nâˆ’\n1\n1.40951{\\times}10^{-1}\n77,391\n709,119\nGN1\n4.85121\nÃ—\n10\nâˆ’\n2\n4.85121{\\times}10^{-2}\n1.06152\n1.06152\n1.72933\nÃ—\n10\nâˆ’\n11\n1.72933{\\times}10^{-11}\n1.40979\nÃ—\n10\nâˆ’\n1\n1.40979{\\times}10^{-1}\n31,523\n488,865\nFRH\nGNi-SPO-IGG\n2.28294\nÃ—\n10\nâˆ’\n8\n2.28294{\\times}10^{-8}\n1.11147\nÃ—\n10\nâˆ’\n8\n1.11147{\\times}10^{-8}\n8.95582\nÃ—\n10\nâˆ’\n11\n8.95582{\\times}10^{-11}\n3.03247\nÃ—\n10\nâˆ’\n4\n3.03247{\\times}10^{-4}\n34,307 (98,596)\n1,308,496\nGNi-SPO-LCG\n2.28294\nÃ—\n10\nâˆ’\n8\n2.28294{\\times}10^{-8}\n1.11142\nÃ—\n10\nâˆ’\n8\n1.11142{\\times}10^{-8}\n8.95220\nÃ—\n10\nâˆ’\n11\n8.95220{\\times}10^{-11}\n3.03360\nÃ—\n10\nâˆ’\n4\n3.03360{\\times}10^{-4}\n35,814 (99,467)\n1,465,775\nGNi-SPO\n2.28294\nÃ—\n10\nâˆ’\n8\n2.28294{\\times}10^{-8}\n1.11142\nÃ—\n10\nâˆ’\n8\n1.11142{\\times}10^{-8}\n8.95004\nÃ—\n10\nâˆ’\n11\n8.95004{\\times}10^{-11}\n3.03360\nÃ—\n10\nâˆ’\n4\n3.03360{\\times}10^{-4}\n57,870 (99,467)\n1,469,435\nGNi-IGG\n2.28294\nÃ—\n10\nâˆ’\n8\n2.28294{\\times}10^{-8}\n5.90106\nÃ—\n10\nâˆ’\n7\n5.90106{\\times}10^{-7}\n1.94511\nÃ—\n10\nâˆ’\n13\n1.94511{\\times}10^{-13}\n3.46361\nÃ—\n10\nâˆ’\n4\n3.46361{\\times}10^{-4}\n55,132\n3,614,516\nGNi-LCG\n6.16115\nÃ—\n10\nâˆ’\n3\n6.16115{\\times}10^{-3}\n2.82510\nÃ—\n10\nâˆ’\n6\n2.82510{\\times}10^{-6}\n1.66055\nÃ—\n10\nâˆ’\n4\n1.66055{\\times}10^{-4}\n3.46707\nÃ—\n10\nâˆ’\n4\n3.46707{\\times}10^{-4}\n56,504\n3,742,194\nGNi\n2.28294\nÃ—\n10\nâˆ’\n8\n2.28294{\\times}10^{-8}\n1.11140\nÃ—\n10\nâˆ’\n8\n1.11140{\\times}10^{-8}\n8.60015\nÃ—\n10\nâˆ’\n14\n8.60015{\\times}10^{-14}\n3.03360\nÃ—\n10\nâˆ’\n4\n3.03360{\\times}10^{-4}\n99,467\n6,141,656\nGN1\n2.28294\nÃ—\n10\nâˆ’\n8\n2.28294{\\times}10^{-8}\n1.11140\nÃ—\n10\nâˆ’\n8\n1.11140{\\times}10^{-8}\n1.51830\nÃ—\n10\nâˆ’\n13\n1.51830{\\times}10^{-13}\n3.03360\nÃ—\n10\nâˆ’\n4\n3.03360{\\times}10^{-4}\n49,737\n6,135,811\nMIT-P\nGNi-SPO-IGG\n1.72741\nÃ—\n10\nâˆ’\n2\n1.72741{\\times}10^{-2}\n2.06264\nÃ—\n10\nâˆ’\n2\n2.06264{\\times}10^{-2}\n1.73090\nÃ—\n10\nâˆ’\n3\n1.73090{\\times}10^{-3}\n1.384435\n1.384435\n3,799 (40,348)\n100,436\nGNi-SPO-LCG\n1.79920\nÃ—\n10\nâˆ’\n2\n1.79920{\\times}10^{-2}\n9.21434\nÃ—\n10\nâˆ’\n1\n9.21434{\\times}10^{-1}\n5.77128\nÃ—\n10\nâˆ’\n2\n5.77128{\\times}10^{-2}\n2.324933\n2.324933\n2,650 (40,664)\n77,984\nGNi-SPO\n1.72743\nÃ—\n10\nâˆ’\n2\n1.72743{\\times}10^{-2}\n1.99909\nÃ—\n10\nâˆ’\n2\n1.99909{\\times}10^{-2}\n1.64191\nÃ—\n10\nâˆ’\n3\n1.64191{\\times}10^{-3}\n2.101596\n2.101596\n25,159 (56,904)\n187,886\nGNi-IGG\n71.4849\n71.4849\n40.6574\n40.6574\n25.0316\n25.0316\n24.25101\n24.25101\n7,762\n194,298\nGNi-LCG\n93.2878\n93.2878\n45.3580\n45.3580\n36.7726\n36.7726\n31.01479\n31.01479\n4,239\n115,386\nGNi\n1.72738\nÃ—\n10\nâˆ’\n2\n1.72738{\\times}10^{-2}\n1.99852\nÃ—\n10\nâˆ’\n2\n1.99852{\\times}10^{-2}\n3.85749\nÃ—\n10\nâˆ’\n12\n3.85749{\\times}10^{-12}\n2.101118\n2.101118\n56,753\n803,583\nGN1\n1.72738\nÃ—\n10\nâˆ’\n2\n1.72738{\\times}10^{-2}\n1.50452\n1.50452\n9.22196\nÃ—\n10\nâˆ’\n6\n9.22196{\\times}10^{-6}\n1.537856\n1.537856\n17,722\n406,185\n(a)\nCSAIL\n(b)\nIntel\n(c)\nFRH\n(d)\nMIT-P\nFigure 1:\nATE over increments for four datasets.\n(a)\nCSAIL\n(b)\nIntel\n(c)\nFRH\n(d)\nMIT-P\nFigure 2:\nCumulative update FLOPs over increments for four dataset.\n(a)\nCSAIL\n(b)\nIntel\n(c)\nFRH\n(d)\nMIT-P\nFigure 3:\nInformation gain over increments for four dataset.\n(a)\nMean ATE versus\nÏ„\nd\n\\tau_{d}\n.\n(b)\nMean update FLOPs versus\nÏ„\nd\n\\tau_{d}\n.\nFigure 4:\nPerformance of the proposed algorithm on the MIT dataset for different values of the update threshold\nÏ„\nd\n\\tau_{d}\n.\n(a)\nMean ATE versus\nÏ„\nÎ·\n\\tau_{\\eta}\n.\n(b)\nMean update FLOPs versus\nÏ„\nÎ·\n\\tau_{\\eta}\n.\nFigure 5:\nPerformance of the proposed approach on the FR079 dataset for different values of the information-gain threshold\nÏ„\nÎ·\n\\tau_{\\eta}\n.\n5.3\nDiscussion\nFrom the results in Table\n2\nand Fig.\n1\n, several consistent patterns emerge across datasets of different scale and structure. The first and most important trend is that the proposed GNi-SPO-IGG achieves accuracy essentially indistinguishable from full GNi while requiring far less computation. Both final and mean N\nÏ‡\n2\n\\chi^{2}\nand ATE values closely track those of the batch-equivalent baseline, demonstrating that selectively solving for and relinearizing only affected variables, when guided by the information-gain criterion, preserves estimation quality. For instance, on the FR079 and FRH datasets, the discrepancies in error between GNi-SPO-IGG and GNi are negligible, yet the proposed approach reduces solve and update FLOPs significantly. This confirms that careful gating and partial optimization can provide substantial efficiency without accuracy degradation.\nThe efficiency gains are particularly pronounced in larger and denser problems such as CSAIL, Intel, and FRH. As shown in Fig.\n2\n, the computational cost of always performing full updates grows rapidly in these cases, whereas GNi-SPO-IGG effectively contains this growth by adaptively restricting updates to variables most affected by new information. This yields a two- to eight-fold reduction in computation compared to GNi, while both final and mean errors remain essentially unchanged. Even in smaller-scale datasets such as MIT and FR079, where graphs are less dense and updates relatively inexpensive, the proposed method still delivers notable efficiency improvements without sacrificing accuracy. These results suggest that the scalability advantage of the proposed approach will become even more significant as problem size increases.\nA comparison between GNi-SPO-IGG and GNi-SPO-LCG further illustrates the value of information-guided gating. In datasets such as MIT, where most of the informative content arises from a small number of loop-closure measurements, both approaches attain similar accuracy and complexity. However, when substantial information is introduced through non-loop-closure measurements, such as position priors in MIT-P, GNi-SPO-LCG fails to exploit this information effectively. By contrast, GNi-SPO-IGG leverages such priors through its information-gain-based gating, achieving markedly better accuracy while maintaining comparable computational cost. This demonstrates the broader advantage of IGG: its ability to incorporate diverse sources of information beyond loop closures, ensuring consistent accuracy across heterogeneous sensing scenarios and enhancing scalability to real-world, long-term SLAM deployments.\nApproaches based solely on gating (GNi-LCG and GNi-IGG) exhibit significant instability. While their final error values can sometimes appear acceptable, mean errors often increase by orders of magnitude, particularly on the MIT-P and Intel datasets. GNi-IGG performs only marginally better than GNi-LCG, yet still produces substantially larger errors than when SPO is employed. This highlights the limitations of gating alone, whether based on loop closures or information gain, which cannot adequately respond to incremental odometry or prior information, leaving large portions of the trajectory insufficiently corrected. The outcome is poor trajectory quality for much of the run, even if later corrections at loop closures or high-information events produce seemingly reasonable final results. These findings underscore that loop closures or information gain alone are insufficient for reliable incremental smoothing, and that continuous incremental corrections are essential.\nThe GN1 baseline, which mirrors iSAM2â€™s single-GN-iteration strategy, provides another instructive comparison. While GN1 often achieves final accuracy comparable to GNi, it does so at the cost of significantly elevated mean errors. For example, on the Intel dataset, GN1 attains a similar final error to GNi but with mean errors two orders of magnitude higher, while also requiring more computations compared to all considered approaches except GNi. Its poor intermediate accuracy on some datasets can be attributed to its inability to converge within a single GN iteration per increment, particularly when successive high-information measurements arrive in quick succession. In such cases, GN1 cannot keep pace with the influx of information, leaving large errors uncorrected until much later. These observations highlight the importance of adaptivity not only in deciding\nwhen\nand\nwhat\nto update, but also in determining\nhow much\nto update at each increment.\nTrajectory-level analysis, as illustrated in Fig.\n1\n, reinforces these observations. Algorithms incorporating SPO maintain ATE values comparable to GNi across the increments, while gating-only methods (GNi-LCG and GNi-IGG) exhibit spiking or oscillatory error patterns whenever increments contain only low-information odometry measurements. On the synthetic MIT-P dataset, which includes intermittent external priors, these differences are even more pronounced: GNi-LCG and GNi-IGG exhibit very high intermediate errors, with mean values several orders of magnitude larger than those of the other algorithms, whereas the remaining approaches maintain low final errors and reasonable mean error performance. Notably, GNi-SPO-IGG achieves accuracy similar to GNi but at significantly lower computational cost compared to GN1, GNi, and GNi-SPO, demonstrating robustness to external information while avoiding wasted computation. Although GNi-SPO-LCG incurs somewhat lower computational cost than GNi-SPO-IGG, its final and mean errors are substantially higher, owing to its failure to trigger highly beneficial global updates when informative but non-loop-closure position priors arrive. This underscores a key design lesson: information-gain-based gating is essential for effectively leveraging diverse external cues, such as GNSS, UWB, or vision-based priors, ensuring not only accuracy with minimal computation, but also scalability to long-term, multimodal SLAM deployments in real-world environments.\nThe results presented in Table\n2\nand Figs.\n1\nand\n2\ncorrespond to settings where GNi-SPO-IGG achieves accuracy comparable to the baseline GNi. However, as shown in Figs.\n4(a)\nand\n5(a)\n, the proposed approach provides a tunable trade-off between accuracy and computational cost. By adjusting the thresholds\nÏ„\nd\n\\tau_{d}\nand\nÏ„\nÎ·\n\\tau_{\\eta}\n, one can balance estimation accuracy against computational efficiency according to application requirements.\n6\nConcluding Remarks\nWe presented a novel incremental SLAM optimizer that achieves a principled balance between accuracy and efficiency by combining an information-theoretic update trigger with selective partial optimization. The key idea is to monitor the change in system entropy (approximated via the log-determinant of the information matrix) induced by new measurements and to use this to decide whether to perform a global update or restrict attention to local variables. When a global update is triggered, GN iterations are executed to convergence, but each iteration is confined to the subset of variables most affected by the recent measurements. This selective update strategy exploits the conditional independence structure of SLAM and yields solutions nearly as accurate as batch optimization while requiring substantially fewer updates and significantly lower computational cost.\nExtensive experiments on diverse SLAM benchmark datasets demonstrate that the proposed approach matches the accuracy of batch and full incremental solvers while consistently achieving large computational savings. In particular, its final and mean normalized chi-squared and absolute trajectory errors remain essentially identical to those of GNi, while solve and update FLOPs are reduced by factors of two to eight, especially in large-scale datasets such as CSAIL, Intel, and FRH. The information-gain-based gating strategy generalizes loop-closure detection by capturing all high-impact events, including loop closures, dense reobservations, and external priors such as GNSS updates, while simultaneously filtering out low-information increments that contribute little beyond increased computational burden. The results also highlight the robustness of the proposed approach to external priors: unlike loop-closure-only approaches, our algorithm can seamlessly exploit intermittent position priors without destabilizing the trajectory estimate.\nA central feature of our approach is the selective update of only those variables that remain unconverged during GN iterations. Following highly informative measurements, such as major loop closures, typically only a small fraction of the state variables requires further refinement in subsequent iterations. SPO exploits this by restricting computation to the active subset, thereby achieving substantial cost savings without sacrificing accuracy. By contrast, GNi, the full-update algorithm with no selectivity, converges to the same solution but incurs several times the computational cost during large re-optimizations.\nIn relation to iSAM2, one of the most widely recognized incremental SLAM back-ends, we emphasize an important conceptual and practical distinction. The core of iSAM2â€™s fluid relinearization strategy is a heuristic, meaning it decides to relinearize a variable if the magnitude of its update exceed a threshold. While this can reduce computational cost by avoiding some recomputations, it also carries the risk of accumulating errors if small, but frequent, updates are repeatedly ignored. In contrast, our approach ensures consistent linearizations and prevents drift by always relinearizing any variable that is updated. We apply thresholds to the update step itself, determining if a variable requires further optimization, rather than to the relinearization process as iSAM2 does. Our unshown experiments indicate that iSAM2â€™s thresholds often need to be set very conservatively to prevent the system from diverging, which can make it particularly sensitive to poor initializations. Furthermore, unlike our approach, iSAM2 lacks both gating and partial optimization. It still solves the entire global linear system at every increment, regardless of which variables are actually updated. Our approach, as established in Section\n4\n, is grounded in a more robust framework. It strategically combines information-guided gating with selective partial optimization to achieve both high accuracy and computational efficiency.\nAlthough our current implementation is in MATLAB/Octave and intended primarily for clarity and reproducibility, the algorithmic FLOP counts and structural complexity analysis strongly indicate that a C++ implementation would yield substantial runtime gains. In fact, the proposed framework is fully compatible with established SLAM libraries such as g2o and GTSAM, where fast factor reuse (e.g., via Bayes trees) could be combined with our selective multi-iteration optimization to achieve both scalability and real-time performance. We make our MATLAB/Octave code publicly available\n2\n2\n2\nThe code and data used to produce the results in this paper are available at\ngithub.com/Reza219/Incremental_SLAM\n.\nto ensure reproducibility, while noting that a C++ integration is a straightforward engineering extension and an important direction for future work.\nThe threshold parameters\nÏ„\nÎ·\n\\tau_{\\eta}\nand\nÏ„\nd\n\\tau_{d}\nwere set empirically, but future work can explore adaptive schemes based on statistical criteria (e.g., significance tests on normalized chi-squared error growth). Additional possible future directions include incorporating robust cost functions (e.g., dynamic covariance scaling or switchable constraints) to improve resilience to outlier, applying the approach to fixed-lag smoothing where information gain can also inform safe marginalization, and integrating the same framework in active SLAM, where information gain can guide both update scheduling and exploration decisions in resource-constrained scenarios.\nIn summary, we have developed an efficient and principled incremental SLAM solver that delivers batch-level accuracy at a fraction of the computational cost. By bridging the gap between batch and incremental optimization, the proposed method enables accurate real-time SLAM in computationally constrained environments. It provides a solid foundation for high-precision mapping and localization on embedded platforms, and can be naturally extended to applications in 3D SLAM, multi-robot mapping, and adaptive perception.\nReferences\n[1]\nÃ…. BjÃ¶rck\n(1996)\nNumerical methods for least squares problems\n.\nSociety for Industrial and Applied Mathematics\n,\nPhiladelphia, PA\n.\nExternal Links:\nISBN 978-0-89871-360-2\n,\nDocument\nCited by:\nÂ§4.2\n.\n[2]\nN. Carlevaris-Bianco and R. M. Eustice\n(2013)\nGeneric factor-based node marginalization and edge sparsification for poseâ€‘graph slam\n.\nIn\nProceedings of the IEEE International Conference on Robotics and Automation (ICRA)\n,\nKarlsruhe, Germany\n,\npp.Â 5748â€“5755\n.\nCited by:\nÂ§1\n.\n[3]\nN. Carlevaris-Bianco and R. M. Eustice\n(2014)\nConservative edge sparsification for graph slam node removal\n.\nIn\nProceedings of the IEEE International Conference on Robotics and Automation (ICRA)\n,\nHong Kong, China\n,\npp.Â 854â€“860\n.\nCited by:\nÂ§1\n.\n[4]\nN. Carlevaris-Bianco and R. M. Eustice\n(2013)\nInformation-theoretic loop closure detection in slam\n.\nThe International Journal of Robotics Research\n32\n(\n10\n),\npp.Â 1155â€“1176\n.\nCited by:\nÂ§1\n.\n[5]\nL. Carlone, R. Tron, K. Daniilidis, and F. Dellaert\n(2015)\nInitialization techniques for 3D SLAM: a survey on rotation estimation and its use in pose graph optimization\n.\nIn\nProceedings of the IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 4597â€“4604\n.\nCited by:\nÂ§5.1\n.\n[6]\nL. Carlone\n(2025)\nDatasets, 2D Pose Graph Optimization\n.\nExternal Links:\nLink\nCited by:\nÂ§5.1\n.\n[7]\nS. Choi, W. Kang, J. Chung, J. Kim, and T. Kim\n(2023)\nAdaptive graduated non-convexity for pose graph optimization\n.\narXiv preprint arXiv:2308.11444\n.\nCited by:\nÂ§1\n.\n[8]\nT. A. Davis\n(2006)\nDirect methods for sparse linear systems\n.\nSociety for Industrial and Applied Mathematics\n.\nCited by:\nÂ§3.1\n.\n[9]\nF. Dellaert, J. Carlson, V. Ila, K. Ni, and C. E. Thorpe\n(2010)\nSubgraphâ€‘preconditioned conjugate gradients for largeâ€‘scale slam\n.\nIn\nProceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nTaipei, Taiwan\n,\npp.Â 2566â€“2571\n.\nCited by:\nÂ§1\n.\n[10]\nF. Dellaert and G. Contributors\n(2022-05)\nBorglab/gtsam\n.\nGeorgia Tech Borg Lab\n.\nExternal Links:\nDocument\n,\nLink\nCited by:\nÂ§1\n.\n[11]\nF. Dellaert and M. Kaess\n(2006)\nSquare root sam: simultaneous localization and mapping via square root information smoothing\n.\nThe International Journal of Robotics Research\n25\n(\n12\n),\npp.Â 1181â€“1203\n.\nCited by:\nÂ§1\n,\nÂ§1\n.\n[12]\nF. Dellaert and M. Kaess\n(2017)\nFactor graphs for robot perception\n.\nFoundations and Trends in Robotics, Vol. 6\n.\nCited by:\nÂ§1\n.\n[13]\nK. J. Doherty, D. M. Rosen, and J. J. Leonard\n(2022)\nSpectral measurement sparsification for poseâ€‘graph slam\n.\narXiv preprint arXiv:2203.13897\n.\nCited by:\nÂ§1\n.\n[14]\nK. Doherty, A. Papalia, Y. Huang, D. Rosen, B. Englot, and J. J. Leonard\n(2024)\nMAC: graph sparsification by maximizing algebraic connectivity\n.\narXiv preprint arXiv:2403.19879\n.\nCited by:\nÂ§1\n.\n[15]\nS. C. Eisenstat and H. F. Walker\n(1996)\nChoosing the forcing terms in an inexact newton method\n.\nSIAM Journal on Scientific Computing\n17\n(\n1\n),\npp.Â 16â€“32\n.\nExternal Links:\nDocument\nCited by:\nÂ§4.1\n,\nÂ§4.1\n,\nÂ§4.1\n.\n[16]\nR. M. Eustice, H. Singh, and J. J. Leonard\n(2006)\nExactly sparse delayed-state filters for view-based slam\n.\nIEEE Transactions on Robotics\n22\n(\n6\n),\npp.Â 1100â€“1114\n.\nCited by:\nÂ§1\n.\n[17]\nU. Frese\n(2006)\nA discussion of simultaneous localization and mapping\n.\nAutonomous Robots\n20\n(\n1\n),\npp.Â 25â€“42\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n.\n[18]\nP. E. Gill, W. Murray, and M. H. Wright\n(1974)\nThe design and implementation of a computer-based system for solving non-linear least-squares problems\n.\nDepartment of Computer Science, Stanford University\n.\nCited by:\nÂ§1\n.\n[19]\nG. Grisetti, R. KÃ¼mmerle, C. Stachniss, and W. Burgard\n(2010)\nA tutorial on graph-based slam\n.\nIEEE Intelligent Transportation Systems Magazine\n2\n(\n4\n),\npp.Â 31â€“43\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n.\n[20]\nG. Grisetti, C. Stachniss, and W. Burgard\n(2009)\nNonlinear constraint network optimization for efficient map learning\n.\nIEEE Transactions on Intelligent Transportation Systems\n10\n(\n3\n),\npp.Â 428â€“439\n.\nExternal Links:\nDocument\nCited by:\nÂ§5.1\n.\n[21]\nJ.E. Guivant and E.M. Nebot\n(2001)\nOptimization of the simultaneous localization and map-building algorithm for real-time implementation\n.\nIEEE Transactions on Robotics and Automation\n17\n(\n3\n),\npp.Â 242â€“257\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n.\n[22]\nQ. Huang, C. Pu, D. Fourie, K. Khosoussi, J. P. How, and J. J. Leonard\n(2021)\nNF-isam: incremental smoothing and mapping via normalizing flows\n.\nIn\n2021 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 1095â€“1102\n.\nCited by:\nÂ§1\n.\n[23]\nV. Ila, L. Polok, M. Solony, and P. Svoboda\n(2017)\nSLAM++ â€“ A highly efficient and temporally scalable incremental SLAM framework\n.\nThe International Journal of Robotics Research\n36\n(\n2\n),\npp.Â 210â€“230\n.\nCited by:\n3rd item\n.\n[24]\nV. Ila, J. M. Porta, and J. Andrade-Cetto\n(2010)\nInformation-based compact pose slam\n.\nIEEE Transactions on Robotics\n26\n(\n1\n),\npp.Â 78â€“93\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n.\n[25]\nY. Jian and F. Dellaert\n(2014)\niSPCG: incremental subgraph-preconditioned conjugate gradient method for online SLAM with many loop-closures\n.\nIn\nProceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nChicago, IL, USA\n,\npp.Â 2647â€“2653\n.\nCited by:\nÂ§1\n.\n[26]\nY. Jian, D. C. Balcan, I. Panageas, P. Tetali, and F. Dellaert\n(2013)\nSupportâ€‘theoretic subgraph preconditioners for largeâ€‘scale slam\n.\nIn\nProceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 9â€“16\n.\nCited by:\nÂ§1\n.\n[27]\nW. Kabsch\n(1976)\nA solution for the best rotation to relate two sets of vectors\n.\nActa Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography\n32\n(\n5\n),\npp.Â 922â€“923\n.\nCited by:\nÂ§5.1\n.\n[28]\nW. Kabsch\n(1978)\nA discussion of the solution for the best rotation to relate two sets of vectors\n.\nActa Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography\n34\n(\n5\n),\npp.Â 827â€“828\n.\nCited by:\nÂ§5.1\n.\n[29]\nM. Kaess, H. Johannsson, R. Roberts, V. Ila, J. J. Leonard, and F. Dellaert\n(2012)\nISAM2: incremental smoothing and mapping using the bayes tree\n.\nThe International Journal of Robotics Research\n31\n(\n2\n),\npp.Â 216â€“235\n.\nCited by:\nÂ§1\n,\nÂ§3.6\n,\n1st item\n.\n[30]\nM. Kaess, A. Ranganathan, and F. Dellaert\n(2008)\nISAM: incremental smoothing and mapping\n.\nIEEE Transactions on Robotics\n24\n(\n6\n),\npp.Â 1365â€“1378\n.\nCited by:\nÂ§1\n.\n[31]\nW. Kang, J. Kim, J. Chung, S. Choi, and T. Kim\n(2024)\nEfficient graduated nonâ€convexity for pose graph optimization\n.\nIn\nInternational Conference on Control, Automation and Systems\n,\npp.Â 545â€“548\n.\nCited by:\nÂ§1\n.\n[32]\nK. Khosoussi, G. S. Sukhatme, S. Huang, and G. Dissanayake\n(2020)\nDesigning sparse reliable pose-graph slam: a graph-theoretic approach\n.\nIn\nAlgorithmic Foundations of Robotics XII: Proceedings of the Twelfth Workshop on the Algorithmic Foundations of Robotics\n,\npp.Â 17â€“32\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n.\n[33]\nH. Kretzschmar and C. Stachniss\n(2012)\nInformation-theoretic compression of pose graphs for laser-based slam\n.\nThe International Journal of Robotics Research\n31\n(\n11\n),\npp.Â 1219â€“1230\n.\nCited by:\nÂ§1\n.\n[34]\nR. KÃ¼mmerle, G. Grisetti, H. Strasdat, K. Konolige, and W. Burgard\n(2011)\nG2o: a general framework for graph optimization\n.\nIn\n2011 IEEE International Conference on Robotics and Automation\n,\nShanghai, China\n,\npp.Â 3607â€“3613\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n,\nÂ§5.1\n.\n[35]\nJ. Leonard, H. Durrant-Whyte, and I.J. Cox\n(1990)\nDynamic map building for autonomous mobile robot\n.\nIn\nEEE International Workshop on Intelligent Robots and Systems, Towards a New Frontier of Applications\n,\nVol.\n,\npp.Â 89â€“96 vol.1\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n.\n[36]\nD. McGann, J. G. R. III, and M. Kaess\n(2023)\nRobust incremental smoothing and mapping (risam)\n.\nIn\nIEEE International Conference on Robotics and Automation (ICRA)\n,\nLondon, UK\n,\npp.Â 4157â€“4163\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n.\n[37]\nE. Olson\n(2006)\nFast iterative optimization of pose graphs with poor initial estimates\n.\nIn\nProceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 2262â€“2269\n.\nCited by:\nÂ§1\n.\n[38]\nJ. A. Placed, J. Strader, H. Carrillo, N. Atanasov, V. Indelman, L. Carlone, and J. A. Castellanos\n(2023-06)\nA survey on active simultaneous localization and mapping: state of the art and new frontiers\n.\nIEEE Transactions on Robotics\n39\n(\n3\n),\npp.Â 1686â€“1705\n.\nExternal Links:\nISSN 1552-3098\n,\nDocument\nCited by:\nÂ§1\n.\n[39]\nL. Polok, V. Ila, M. Solony, P. SmrÅ¾, and P. ZemÄÃ­k\n(2013)\nIncremental block cholesky factorization for nonlinear least squares in robotics\n.\nIn\nProceedings of Robotics: Science and Systems (RSS)\n,\nBerlin, Germany\n.\nCited by:\nÂ§1\n,\n3rd item\n.\n[40]\nP. RichtÃ¡rik and M. TakÃ¡Ä\n(2016)\nParallel coordinate descent methods for big data optimization\n.\nMathematical Programming\n156\n(\n1â€“2\n),\npp.Â 433â€“484\n.\nExternal Links:\nDocument\nCited by:\nÂ§4.2\n.\n[41]\nD. M. Rosen, M. Kaess, and J. J. Leonard\n(2014)\nRISE: an incremental trustâ€region method for robust online sparse leastâ€squares estimation\n.\nIEEE Transactions on Robotics\n30\n(\n5\n),\npp.Â 1091â€“1108\n.\nCited by:\nÂ§1\n.\n[42]\nL. N. Trefethen and D. Bau\n(1997)\nNumerical linear algebra\n.\nSociety for Industrial and Applied Mathematics\n.\nCited by:\nÂ§3.2\n.\n[43]\nP. Tseng and S. Yun\n(2009)\nA coordinate gradient descent method for nonsmooth separable minimization\n.\nMathematical Programming, SeriesÂ B\n117\n(\n1â€“2\n),\npp.Â 387â€“423\n.\nExternal Links:\nDocument\nCited by:\nÂ§4.2\n,\nÂ§4.2\n.\n[44]\nX. Wang, R. J. Marcotte, G. Ferrer, and E. Olson\n(2018)\nApriISAM: realâ€‘time smoothing and mapping\n.\nIn\nProceedings of the IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 2486â€“2493\n.\nCited by:\nÂ§1\n.\n[45]\nH. Yang, P. Antonante, V. Tzoumas, and L. Carlone\n(2020)\nGraduated non-convexity for robust spatial perception: from non-minimal solvers to global outlier rejection\n.\nIEEE Robotics and Automation Letters\n5\n(\n2\n),\npp.Â 1127â€“1134\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n.",
    "preview_text": "We present an efficient incremental SLAM back-end that achieves the accuracy of full batch optimization while substantially reducing computational cost. The proposed approach combines two complementary ideas: information-guided gating (IGG) and selective partial optimization (SPO). IGG employs an information-theoretic criterion based on the log-determinant of the information matrix to quantify the contribution of new measurements, triggering global optimization only when a significant information gain is observed. This avoids unnecessary relinearization and factorization when incoming data provide little additional information. SPO executes multi-iteration Gauss-Newton (GN) updates but restricts each iteration to the subset of variables most affected by the new measurements, dynamically refining this active set until convergence. Together, these mechanisms retain all measurements to preserve global consistency while focusing computation on parts of the graph where it yields the greatest benefit. We provide theoretical analysis showing that the proposed approach maintains the convergence guarantees of full GN. Extensive experiments on benchmark SLAM datasets show that our approach consistently matches the estimation accuracy of batch solvers, while achieving significant computational savings compared to conventional incremental approaches. The results indicate that the proposed approach offers a principled balance between accuracy and efficiency, making it a robust and scalable solution for real-time operation in dynamic data-rich environments.\n\nEfficient Incremental SLAM via Information-Guided and Selective Optimization\nReza Arablouei\nAbstract\nWe present an efficient incremental SLAM back-end that achieves the accuracy of full batch optimization while substantially reducing computational cost. The proposed approach combines two complementary ideas: information-guided gating (IGG) and selective partial optimization (SPO). IGG employs an information-theoretic criterio",
    "is_relevant": false,
    "relevance_score": 0.0,
    "extracted_keywords": [
        "SLAM",
        "incremental optimization",
        "information-guided gating",
        "selective partial optimization",
        "computational efficiency",
        "real-time operation"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¢é‡SLAMåç«¯æ–¹æ³•ï¼Œé€šè¿‡ä¿¡æ¯å¼•å¯¼çš„é—¨æ§å’Œé€‰æ‹©æ€§éƒ¨åˆ†ä¼˜åŒ–ï¼Œåœ¨ä¿æŒå…¨æ‰¹é‡ä¼˜åŒ–ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-13T01:01:36Z",
    "created_at": "2026-01-20T17:49:40.546028",
    "updated_at": "2026-01-20T17:49:40.546036",
    "recommend": 0
}