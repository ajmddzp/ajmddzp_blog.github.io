{
    "id": "2601.08868v1",
    "title": "Residual Cross-Modal Fusion Networks for Audio-Visual Navigation",
    "authors": [
        "Yi Wang",
        "Yinfeng Yu",
        "Bin Ren"
    ],
    "abstract": "è§†å¬å…·èº«å¯¼èˆªæ—¨åœ¨é€šè¿‡åˆ©ç”¨å¬è§‰çº¿ç´¢ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨æœªè§è¿‡çš„ä¸‰ç»´ç¯å¢ƒä¸­è‡ªä¸»å®šä½å¹¶æŠµè¾¾å£°æºã€‚è¯¥ä»»åŠ¡çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå¤šæ¨¡æ€èåˆè¿‡ç¨‹ä¸­å¦‚ä½•æœ‰æ•ˆå»ºæ¨¡å¼‚æ„ç‰¹å¾é—´çš„äº¤äº’ï¼Œä»¥é¿å…å•æ¨¡æ€ä¸»å¯¼æˆ–ä¿¡æ¯é€€åŒ–ï¼Œå°¤å…¶åœ¨è·¨åŸŸåœºæ™¯ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§è·¨æ¨¡æ€æ®‹å·®èåˆç½‘ç»œï¼Œé€šè¿‡åœ¨éŸ³é¢‘ä¸è§†è§‰æµä¹‹é—´å¼•å…¥åŒå‘æ®‹å·®äº¤äº’ï¼Œå®ç°äº’è¡¥å»ºæ¨¡ä¸ç»†ç²’åº¦å¯¹é½ï¼ŒåŒæ—¶ä¿æŒå„è‡ªè¡¨å¾çš„ç‹¬ç«‹æ€§ã€‚ç›¸è¾ƒäºä¼ ç»Ÿä¾èµ–ç®€å•æ‹¼æ¥æˆ–æ³¨æ„åŠ›é—¨æ§çš„æ–¹æ³•ï¼Œè¯¥ç½‘ç»œé€šè¿‡æ®‹å·®è¿æ¥æ˜¾å¼å»ºæ¨¡è·¨æ¨¡æ€äº¤äº’ï¼Œå¹¶ç»“åˆç¨³å®šåŒ–æŠ€æœ¯ä»¥æå‡æ”¶æ•›æ€§ä¸é²æ£’æ€§ã€‚åœ¨Replicaå’ŒMatterport3Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥ç½‘ç»œæ˜¾è‘—ä¼˜äºå½“å‰æœ€ä¼˜çš„èåˆåŸºçº¿æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºæ›´å¼ºçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®éªŒè¿˜å‘ç°æ™ºèƒ½ä½“åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå·®å¼‚åŒ–çš„æ¨¡æ€ä¾èµ–æ€§ã€‚è¿™ä¸€ç°è±¡çš„å‘ç°ä¸ºç†è§£å…·èº«æ™ºèƒ½ä½“çš„è·¨æ¨¡æ€åä½œæœºåˆ¶æä¾›äº†æ–°è§†è§’ã€‚",
    "url": "https://arxiv.org/abs/2601.08868v1",
    "html_url": "https://arxiv.org/html/2601.08868v1",
    "html_content": "Residual Cross-Modal Fusion Networks for Audio-Visual Navigation\nYi Wang â€ƒâ€ƒYinfeng Yu\nSchool of Computer Science and Technology, Xinjiang University, Urumqi, China\nJoint International Research Laboratory of Silk Road Multilingual Cognitive\nYinfeng Yu is the corresponding author (Email: yuyinfeng@xju.edu.cn)\nBin Ren\nSchool of Mechatronic Engineering and Automation\nShanghai University, Shanghai, China\nAbstract\nAudio-visual embodied navigation aims to enable an agent to autonomously localize and reach a sound source in unseen 3D environments by leveraging auditory cues. The key challenge of this task lies in effectively modeling the interaction between heterogeneous features during multimodal fusion, so as to avoid single-modality dominance or information degradation, particularly in cross-domain scenarios. To address this, we propose a Cross-Modal Residual Fusion Network, which introduces bidirectional residual interactions between audio and visual streams to achieve complementary modeling and fine-grained alignment, while maintaining the independence of their representations. Unlike conventional methods that rely on simple concatenation or attention gating, CRFN explicitly models cross-modal interactions via residual connections and incorporates stabilization techniques to improve convergence and robustness. Experiments on the Replica and Matterport3D datasets demonstrate that CRFN significantly outperforms state-of-the-art fusion baselines and achieves stronger cross-domain generalization. Notably, our experiments also reveal that agents exhibit differentiated modality dependence across different datasets. The discovery of this phenomenon provides a new perspective for understanding the cross-modal collaboration mechanism of embodied agents.\nKeywords: Audio-Visual Navigation, Multimodal Fusion, Residual Network, Embodied Intelligence\n1\nIntroduction\nWith the rapid development of embodied intelligence\n[\n11\n,\n13\n,\n26\n,\n35\n,\n34\n]\n, enabling agents to accomplish complex tasks in real or simulated environments has become a critical research direction in the field of artificial intelligence\n[\n15\n,\n25\n]\n. Among related tasks, audio-visual navigation (AVN) is a key task for embodied intelligence, which requires agents to autonomously locate and navigate to target sound sources in unknown environments by leveraging both visual information and auditory cues. Such tasks hold broad value in application scenarios, including service robotics, human-computer interaction, and security monitoring, thus attracting increasing attention.\nFigure 1:\nHumans can effortlessly integrate visual and auditory information, yet agents often suffer from performance degradation due to modal imbalance or domain discrepancy.\nIn a bustling, noisy shopping mall, you faintly hear someone calling your name. Your ears first pick up the soundâ€™s general direction; you then turn your head and scan the crowd with your gaze. Only when you catch sight of a familiar figure do you confirm the callerâ€™s position. Amid this chaotic setting, hearing narrows down the search range for you, while vision finalizes the identification and locking in of the target. The two work in seamless harmony, requiring hardly any deliberate effort on your part.\nFigure 2:\nIllustration of the basic architecture\n. The agentâ€™s navigation process operates in three stages: (1) Observation and Encoder, where the agent processes visual (RGB or Depth) and auditory (spectrogram) inputs via respective encoders to extract features; (2) Interaction and Integration, where our proposed CRFN module performs bidirectional residual updates to refine features and adaptively balances them using a Fusion Controller; and (3) Policy Network, where the fused representation is fed into a GRU-based Actor-Critic model to capture temporal dependencies and predict the final navigation action\na\nt\na_{t}\n.\nThis kind of sensory synergy, which seems effortless to humans, poses a significant challenge for embodied agents: while these agents can acquire both visual and auditory information, the two modalities often operate in isolation, lacking genuine complementarity and interactionâ€”this leads to a substantial decline in navigation performance in complex environments. On the other hand, an interesting phenomenon was also observed in the experiments of this paper: the fusion effect exhibits significant differences across different environments. In synthetic high-fidelity scenes (e.g., Replica\n[\n23\n]\n), the visual modality often becomes dominant due to its clearer texture and geometric information, gradually weakening the audio modality. In contrast, in complex real-world scenes (e.g., Matterport3D\n[\n6\n]\n), a single modality can hardly complete the navigation task independently, and cross-modal complementarity instead becomes the key to performance improvement. This dynamic shift between modality dominance and cross-modal complementarity reveals the instability of existing methods in cross-environment and cross-domain generalization, and further underscores the necessity of designing a robust fusion mechanism.\nTo address these challenges, this paper introduces a novel Cross-Modal Residual Fusion Network (CRFN). The proposed method is built upon two core design principles: bidirectional residual interaction and a fusion control mechanism. Specifically, reciprocal residual pathways are established between audio and visual features to enable mutual refinement and complementarity while preserving the independence of each modality. In addition, a lightweight fusion controller is incorporated to adaptively regulate modality contributions during interaction, with an output normalization constraint applied to suppress single-modality dominance and stabilize training. This modular design not only enhances the robustness and generalization ability of the fusion process but also provides stronger interpretability: the evolution of residual coefficients offers an intuitive view into the dynamic collaboration between modalities.\nIn summary, our main contributions are as follows:\nâ€¢\nA novel cross-modal fusion framework.\nWe propose the Cross-Modal Residual Fusion Network (CRFN), which introduces bidirectional residual interactions and a lightweight fusion controller to explicitly model reciprocal refinement between audio and visual modalities while preserving their independence.\nâ€¢\nImproved robustness and generalization.\nBy combining small-value initialization of residual scaling factors with output normalization, CRFN suppresses modality imbalance, stabilizes training, and consistently outperforms concatenation and attention-based baselines on both Replica\n[\n23\n]\nand Matterport3D\n[\n6\n]\nbenchmarks.\nâ€¢\nNew empirical insights into modality dependence.\nThrough the analysis of residual coefficients, we uncover a novel phenomenon: in synthetic high-fidelity environments, the navigation policy tends to be dominated by the visual modality, whereas in real-world complex environments, cross-modal complementarity becomes essential. This provides a new perspective for understanding modality collaboration in embodied audio-visual navigation.\nFigure 3:\nArchitecture of the Cross-Modal Feature Fusion Module.\nThis module enables visual and audio features to refine and complement each other through bidirectional residual paths. Features of each modality are updated and mutually influenced in the bidirectional interaction, ensuring a balanced exchange of information.\n2\nRelated Work\nEmbodied intelligence aims to enable agents to learn and accomplish complex tasks through interaction with the environment. Navigation is one of its core capabilities, and early research has primarily focused on visual navigation, such as PointGoal navigation\n[\n2\n]\nand ObjectGoal navigation\n[\n4\n]\n. In these tasks, goals are typically specified by visual or textual information, such as coordinates or object categories. While the aforementioned works have achieved significant progress in visual navigation, they limit the agentâ€™s perception to a single visual modality, which is far from the way humans and animals perceive the real world using multi-sensory synergy. In real physical environments, auditory perception provides a crucial information channel that complements vision. Sound signals possess the property of omni-directionality, allowing them to penetrate visual occlusions and provide key directional cues for targets outside the agentâ€™s field of view. To endow embodied agents with such more powerful and biologically intuitive perceptual and action capabilities, AVN has been proposed.\nThe formal definition of the AVN task and the establishment of large-scale benchmarks mark the starting point for the development of this field. Chen et al. have made pioneering contributions in this regard: based on the Habitat\n[\n19\n,\n24\n]\nplatform, they constructed SoundSpaces\n[\n9\n]\non the Replica\n[\n23\n]\nand Matterport3D\n[\n6\n]\ndatasets, which is the first large-scale and high-fidelity audio-visual simulation environment and provides a unified experimental platform and testing benchmark for subsequent research. This work has established the core problems and evaluation criteria for the AVN field; however, its fusion method is relatively elementary, making it difficult to address modal imbalance in complex environments. In parallel, Gan et al. explored a decomposed modular framework, attempting to decouple the perception, localization, and planning components to enhance interpretability and scalability\n[\n14\n]\n. These two types of methods represent the two early mainstream approaches-end-to-end learning and modular planning-respectively, and together they have established the research baseline and development direction for the AVN field.\nEarly AVN frameworks exhibited low efficiency when handling long-range navigation tasks\n[\n30\n]\n. To address this, AV-WaN\n[\n10\n]\nintroduced an â€œacoustic memory mapâ€ for the first time, which structurally records sound information perceived by the agent during movement. This work innovatively proposed a hierarchical reinforcement learning framework: the high-level policy dynamically predicts mid-to-long-term â€œwaypointsâ€ based on multi-modal perception, while the low-level planner executes specific paths to reach these waypoints. Subsequently, SAVi\n[\n8\n]\nfurther pointed out the limitations of existing tasks-namely, the assumption that sound sources emit continuously and lack semantic relevance. Accordingly, they proposed a more challenging semantic AVN task, where sounds are sporadic and transient, and semantically consistent with the category of objects producing the sounds (e.g., the flushing sound of a toilet).\nAs fundamental issues have been addressed, researchers in the AVN field have shifted their focus toward more realistic and complex real-world scenarios. SAAVN\n[\n32\n,\n31\n,\n16\n]\ntook the lead in investigating the navigation robustness in acoustically complex environments: it designed an adversarial environment that incorporates a â€œsound attacker,â€ which actively moves and adjusts both volume and sound category to interfere with the navigating agent. Later, a benchmark for dynamic AVN was proposed, marking the first time an agent was required to track a moving sound source\n[\n28\n]\n. Almost simultaneously, FSAAVN\n[\n29\n]\nalso focused on the moving sound source tracking task and pointed out that the simple feature concatenation-based fusion methods used in previous works might overlook contextual information, and this was also the first time researchers paid attention to the issue of audio-visual cross-modal fusion since the inception of the AVN task. Subsequently, ORAN\n[\n12\n]\nleveraged Cross-Task Curriculum Policy Distillation to transfer the knowledge of a pre-trained â€œexpertâ€ policy (trained on the point-goal navigation task) to the AVN task. CAVEN\n[\n18\n]\nexplored the role of human-computer interaction in AVN, introducing human interaction, large language models, and a new benchmark. RILA\n[\n27\n]\nmade the first attempt to solve the AVN task in a zero-shot manner, completely eliminating the need for reinforcement learning training in the environment. Shi et al. contributed BeDAVIN, a large-scale audio dataset, as well as ENMUSÂ³-an, an architecture specifically designed for multi-sound-source scenarios\n[\n20\n]\n.\nRegrettably, although AVN is inherently a cross-modal task\n[\n33\n]\n, the focus of existing research has largely been on innovations in navigation strategies and memory mechanisms. Its core audio-visual fusion module is to a large extent treated as a standardized component, and its inherent synergy potential has not been fully exploited.\n3\nProposed Method\n3.1\nFramework Overview\nAs shown in the figure, the agent receives multimodal observations from the 3D environment, including visual images and audio signals. The visual modality consists of either depth maps or RGB images (one of which is used in the task setting), while the audio modality is represented as binaural spectrograms. First, visual and audio data are processed by dedicated encoders to extract modality-specific features, which are then fed into the proposed CRFN network. The primary role of CRFN is to establish bidirectional residual interactions and maintain a dynamic balance between modalities within a shared feature space, thereby producing robust fused representations. The fused features are subsequently passed into a recurrent unit (GRU) to capture historical information and temporal dependencies\n[\n21\n]\n. Finally, an Actor-Critic policy head leverages the temporal representation to predict action distributions and state values, driving the agent to accomplish goal-directed navigation tasks.\nTo achieve robust and stable cross-modal fusion, the CRFN proposed in this paper consists of two main modules: a Modal Interaction Module and a Fusion Control Module. The Modal Interaction Module explicitly models the mutual interaction between visual and audio features through bidirectional residual paths, ensuring fine-grained modal complementarity and representation alignment. The Fusion Control Module, on the other hand, adaptively adjusts the contribution ratio of each modality via learnable residual scaling factors and normalization constraints, effectively suppressing the dominance of a single modality and stabilizing the training process. The former is responsible for establishing efficient information exchange channels, while the latter acts as a â€œtraffic controllerâ€ to dynamically regulate the intensity of such exchange, thereby achieving robust navigation performance in variable scenarios. CRFN adopts a concise and lightweight architecture, avoiding redundant network layers and complex structures to ensure efficient computation and training. With this design, the model can efficiently achieve cross-modal information fusion and deliver excellent performance in audio-visual navigation tasks. Sections\n3.2\nand\n3.3\nbelow will delve into the design details and implementation methods of these two modules, respectively.\n3.2\nBidirectional Residual Interaction\nThis module aims to construct a symmetric information pathway, enabling the feature representations of the two modalities to complement and refine each other. Specifically, at each time step\nt\nt\n, the visual features\nv\nt\n(\nf\nv\n(\nV\n)\nv_{t}(f_{v}(V)\nand audio features\na\nt\nâ€‹\n(\nf\nA\nâ€‹\n(\nA\n)\n)\na_{t}(f_{A}(A))\nobtained through encoders first pass through their respective non-linear transformation networks\nU\nv\nâ€‹\n(\nâ‹…\n)\nU_{v}(\\cdot)\nand\nU\na\nâ€‹\n(\nâ‹…\n)\nU_{a}(\\cdot)\nto be mapped to a shared representation space that facilitates interaction. Subsequently, we average the transformed features to generate an interaction vector\nh\ni\nâ€‹\nn\nâ€‹\nt\nâ€‹\ne\nâ€‹\nr\nâ€‹\na\nâ€‹\nc\nâ€‹\nt\nh_{interact}\nthat integrates information from both modalities:\nh\ni\nâ€‹\nn\nâ€‹\nt\nâ€‹\ne\nâ€‹\nr\nâ€‹\na\nâ€‹\nc\nâ€‹\nt\n=\n1\n2\nâ€‹\n(\nU\nv\nâ€‹\n(\nv\nt\n)\n+\nU\na\nâ€‹\n(\na\nt\n)\n)\n,\nh_{interact}=\\frac{1}{2}(U_{v}(v_{t})+U_{a}(a_{t})),\n(1)\nThis interaction vector\nh\ni\nâ€‹\nn\nâ€‹\nt\nâ€‹\ne\nâ€‹\nr\nâ€‹\na\nâ€‹\nc\nâ€‹\nt\nh_{interact}\ncaptures the core associations between audio and visual modalities and serves as a shared residual signal, which will be used to simultaneously update the features of both original modalities. This symmetric residual design ensures bidirectional flow of information between the two modalities.\nInput:\nVisual feature\nğ¯\nt\n\\mathbf{v}_{t}\n, Audio feature\nğš\nt\n\\mathbf{a}_{t}\n, Interaction vector\nğ¡\ni\nâ€‹\nn\nâ€‹\nt\nâ€‹\ne\nâ€‹\nr\nâ€‹\na\nâ€‹\nc\nâ€‹\nt\n\\mathbf{h}_{interact}\nOutput:\nUpdated features\nğ¯\n^\nt\n\\hat{\\mathbf{v}}_{t}\n,\nğš\n^\nt\n\\hat{\\mathbf{a}}_{t}\nLearnable parameters:\nÎ²\nv\n\\beta_{v}\n,\nÎ²\na\n\\beta_{a}\n1ex\nStep 1: Normalize modal features\nğ¯\nn\nâ€‹\no\nâ€‹\nr\nâ€‹\nm\nâ†\nLayerNorm\nâ€‹\n(\nğ¯\nt\n)\n\\mathbf{v}_{norm}\\leftarrow\\mathrm{LayerNorm}(\\mathbf{v}_{t})\nğš\nn\nâ€‹\no\nâ€‹\nr\nâ€‹\nm\nâ†\nLayerNorm\nâ€‹\n(\nğš\nt\n)\n\\mathbf{a}_{norm}\\leftarrow\\mathrm{LayerNorm}(\\mathbf{a}_{t})\n1ex\nStep 2: Compute residual updates\nğ¯\nr\nâ€‹\ne\nâ€‹\ns\nâ†\nğ¯\nn\nâ€‹\no\nâ€‹\nr\nâ€‹\nm\n+\nÎ²\nv\nâ‹…\nğ¡\ni\nâ€‹\nn\nâ€‹\nt\nâ€‹\ne\nâ€‹\nr\nâ€‹\na\nâ€‹\nc\nâ€‹\nt\n\\mathbf{v}_{res}\\leftarrow\\mathbf{v}_{norm}+\\beta_{v}\\cdot\\mathbf{h}_{interact}\nğš\nr\nâ€‹\ne\nâ€‹\ns\nâ†\nğš\nn\nâ€‹\no\nâ€‹\nr\nâ€‹\nm\n+\nÎ²\na\nâ‹…\nğ¡\ni\nâ€‹\nn\nâ€‹\nt\nâ€‹\ne\nâ€‹\nr\nâ€‹\na\nâ€‹\nc\nâ€‹\nt\n\\mathbf{a}_{res}\\leftarrow\\mathbf{a}_{norm}+\\beta_{a}\\cdot\\mathbf{h}_{interact}\n1ex\nStep 3: Apply activation to obtain final features\nğ¯\n^\nt\nâ†\ntanh\nâ¡\n(\nğ¯\nr\nâ€‹\ne\nâ€‹\ns\n)\n\\hat{\\mathbf{v}}_{t}\\leftarrow\\tanh(\\mathbf{v}_{res})\nğš\n^\nt\nâ†\ntanh\nâ¡\n(\nğš\nr\nâ€‹\ne\nâ€‹\ns\n)\n\\hat{\\mathbf{a}}_{t}\\leftarrow\\tanh(\\mathbf{a}_{res})\n1ex\nreturn\nğ¯\n^\nt\n\\hat{\\mathbf{v}}_{t}\n,\nğš\n^\nt\n\\hat{\\mathbf{a}}_{t}\nAlgorithmÂ 1\nFusion Controller in Cross-Modal Residual Fusion Network (CRFN)\nTable 1:\nPerformance comparison with other methods under the Depth setting. SPL, SR, SNA are percentages.\nReplica\nMatterport3D\nMethod\nHeard\nUnheard sound\nHeard\nUnheard sound\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSNA\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSNA\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSNA\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSNA\nâ†‘\n\\uparrow\nRandom\n[\n10\n]\n4.9\n18.5\n1.8\n4.9\n18.5\n1.8\n2.1\n9.1\n0.8\n2.1\n9.1\n0.8\nDirection Follower\n[\n10\n]\n54.7\n72.0\n41.1\n11.1\n17.2\n8.4\n32.3\n41.2\n23.8\n13.9\n18.0\n10.7\nFrontier W\n[\n10\n]\n44.0\n63.9\n35.2\n6.5\n14.8\n5.1\n30.6\n42.8\n22.2\n10.9\n16.4\n8.1\nSupervised W\n[\n10\n]\n59.1\n88.1\n48.5\n14.1\n43.1\n10.1\n21.0\n36.2\n16.2\n4.1\n8.8\n2.9\nGan et al.\n[\n14\n]\n57.6\n83.1\n47.9\n7.5\n15.7\n5.7\n22.8\n37.9\n17.1\n5.0\n10.2\n3.6\nSoundSpaces\n[\n9\n]\n74.4\n91.4\n48.1\n34.7\n50.9\n16.7\n54.3\n67.7\n31.3\n21.9\n33.5\n10.4\nCRFN (Ours)\n76.7\n93.1\n47.3\n41.6\n55.7\n22.5\n57.3\n70.3\n33.2\n27.7\n40.1\n13.5\n3.3\nFusion Controller\nThe core of the fusion control module lies in adaptively adjusting modal contributions and ensuring the stability of the training process. This function is not implemented via a complex dynamic network, but rather accomplished by two independent, learnable scalar parametersâ€”residual scaling factors\nÎ²\nv\n\\beta_{v}\nand\nÎ²\na\n\\beta_{a}\n. These two parameters control the update intensity of the interaction vector\nh\ni\nâ€‹\nn\nâ€‹\nt\nâ€‹\ne\nâ€‹\nr\nâ€‹\na\nâ€‹\nc\nâ€‹\nt\nh_{interact}\non visual and audio features, respectively.\nWe substitute these two factors into the residual update path to obtain the updated features\nÎ²\nv\n\\beta_{v}\nand\nÎ²\na\n\\beta_{a}\n:\nğ¯\n^\nt\n=\nact\nâ€‹\n(\nLN\nâ€‹\n(\nğ¯\nt\n)\n+\nÎ²\nv\nâ‹…\nğ¡\ninteract\n)\n,\n\\hat{\\mathbf{v}}_{t}=\\text{act}(\\text{LN}(\\mathbf{v}_{t})+\\beta_{v}\\cdot\\mathbf{h}_{\\text{interact}}),\n(2)\nğš\n^\nt\n=\nact\nâ€‹\n(\nLN\nâ€‹\n(\nğš\nt\n)\n+\nÎ²\na\nâ‹…\nğ¡\ninteract\n)\n,\n\\hat{\\mathbf{a}}_{t}=\\text{act}(\\text{LN}(\\mathbf{a}_{t})+\\beta_{a}\\cdot\\mathbf{h}_{\\text{interact}}),\n(3)\nAmong them,\nL\nâ€‹\nN\nâ€‹\n(\nâ‹…\n)\nLN(\\cdot)\nrefers to layer normalization applied to the residual path, which is used to stabilize the input scale of each modality;\na\nâ€‹\nc\nâ€‹\nt\nâ€‹\n(\nâ‹…\n)\nact(\\cdot)\ndenotes the Tanh activation function. At the start of training,\nÎ²\nv\n\\beta_{v}\nand\nÎ²\na\n\\beta_{a}\nare initialized to a small value. This initialization encourages the model to start learning from a â€œweakly coupledâ€ state, which is more conducive for the network to independently discover the appropriate modal fusion intensity based on task requirements.\nThe overall computational procedure is summarized in Algorithm\n1\n, providing a concise representation of how the residual scaling factors\nÎ²\nv\n\\beta_{v}\nand\nÎ²\na\n\\beta_{a}\noperate within the CRFN architecture.\n4\nExperiments\n4.1\nDatasets and Implementation Detail\nOur experiments were conducted in a comprehensive simulated environment, which is built on the Habitat simulator\n[\n19\n]\nand SoundSpaces\n[\n9\n]\nacoustic platform, and leverages two complementary public 3D datasets: Matterport3D\n[\n6\n]\nand Replica\n[\n23\n]\n. Among them, Matterport3D\n[\n6\n]\nis a large-scale, diverse dataset of real-world scans, containing 85 complex indoor environments. It serves as an ideal platform for evaluating the generalization ability of agents, but its models contain geometric noise inherent to real-world scanning. In contrast, Replica\n[\n23\n]\nis a small yet high-quality dataset, comprising 18 high-fidelity synthetic indoor scenes. Renowned for its clean and realistic 3D meshes, Replica\n[\n23\n]\nis highly suitable for validating the effectiveness of models in high-fidelity environments. Using the geometric and material information from these two datasets, the SoundSpaces\n[\n9\n]\nplatform generates physically realistic sound sources by convolving 102 non-repetitive natural sounds with binaural Room Impulse Responses (RIRs) corresponding to specific directions.\nImplementation Details. The model parameter configuration is as follows: the sampling rate of Room Impulse Response (RIR) is 44,100Â Hz for the Replica\n[\n23\n]\ndataset and 16,000Â Hz for the Matterport3D\n[\n6\n]\ndataset. In all experiments, the resolution of both visual and auditory observations is\n128\nÃ—\n128\n128\\times 128\n. We adopt the Proximal Policy Optimization (PPO) algorithm for training, and its hyperparameters remain consistent across the two datasets: 4 training epochs, a clip parameter of 0.1, a value loss coefficient of 0.5, and a discount factor (\nÎ³\n\\gamma\n) set to 0.99. For experiments on the Replica\n[\n23\n]\ndataset, the learning rate is set to\n2.5\nÃ—\n10\nâˆ’\n4\n2.5\\times 10^{-4}\n, while for Matterport3D\n[\n6\n]\n, the learning rate is\n2.0\nÃ—\n10\nâˆ’\n4\n2.0\\times 10^{-4}\n. During training, the number of steps per episode is limited to 500, and the model undergoes 40,000 update iterations on the Replica dataset and 60,000 update iterations on the Matterport3D\n[\n6\n]\ndataset.\nIn our experiments, there are two distinct sound source conditions: (1) Heard: the target sound source is a telephone ringtone, which is used consistently across the training, validation, and test sets; (2) Unheard: the 102 sound sources are divided into three non-overlapping groups, where 78 sound sources are used for training scenarios, 11 for validation scenarios, and the remaining 18 for test scenarios. Every scenario in the test process is new to the model-none of them were encountered in prior training or validation stages.\n4.2\nEvaluation Metrics\nWe follow the standard protocol\n[\n9\n]\nand adopt three metrics to compare the navigation performance of different methods: (1) Success weighted by Path Length (SPL)\n6\n: After an agent successfully navigates to the target sound source, this metric measures how close the agentâ€™s actual travel path is to the shortest feasible path\n[\n1\n]\n; (2) Success Rate (SR)\n4\n: The proportion of test samples in which the agent successfully reaches the target location; (3) Success-Navigation Accuracy (SNA)\n5\n: This metric evaluates the agentâ€™s ability to not only successfully reach the target during navigation but also maintain orientation toward the correct direction.\nSR\n=\n1\nN\nâ€‹\nâˆ‘\ni\n=\n1\nN\nS\ni\n,\n\\text{SR}=\\frac{1}{N}\\sum_{i=1}^{N}S_{i},\n(4)\nSNA\n=\n1\nN\nâ€‹\nâˆ‘\ni\n=\n1\nN\nS\ni\nâ‹…\nl\ni\na\ni\nâ€‹\n[\n10\n]\n,\n\\text{SNA}=\\frac{1}{N}\\sum_{i=1}^{N}S_{i}\\cdot\\frac{l_{i}}{a_{i}}~\\cite[cite]{[\\@@bibref{}{Waypoint}{}{}]},\n(5)\nSPL\n=\n1\nN\nâ€‹\nâˆ‘\ni\n=\n1\nN\nS\ni\nâ‹…\nl\ni\nmax\nâ¡\n(\np\ni\n,\nl\ni\n)\nâ€‹\n[\n1\n]\n.\n\\text{SPL}=\\frac{1}{N}\\sum_{i=1}^{N}S_{i}\\cdot\\frac{l_{i}}{\\max(p_{i},l_{i})}~\\cite[cite]{[\\@@bibref{}{SPL}{}{}]}.\n(6)\nIn the formulas,\nN\nN\nis the total number of test episodes;\nS\ni\nâˆˆ\n{\n0\n,\n1\n}\nS_{i}\\in\\{0,1\\}\nspecifies whether the\ni\ni\n-th episode is successful;\nl\ni\nl_{i}\nis the shortest feasible path length for episode\ni\ni\n;\np\ni\np_{i}\nis the actual path length traveled by the agent; and\na\ni\na_{i}\nis the number of actions executed in episode\ni\ni\n. The term\na\ni\na_{i}\nincludes inefficient operations such as in-place rotations, so\nSNA\npenalizes excessive redundant movements.\nFigure 4:\nNavigation trajectories on the top-down map in the Replica scenes.Â Agent paths transition from dark to light blue temporally, while green indicates the shortest geodesic path.\nFigure 5:\nNavigation trajectories on the top-down map in the Matterport3D scenes.Â Agent paths transition from dark to light blue temporally, while green indicates the shortest geodesic path.\n4.3\nBaselines\nWe compare our method with the following approaches:\nâ€¢\nRandom Agent\n: this is the simplest task in navigation, where the agent randomly selects actions (turn left, turn right, move forward) at each time step to search for the target.\nâ€¢\nDirection Follower\n: this method adopts a hierarchical navigation framework, where the high-level policy predicts the Direction of Arrival of the sound source relying solely on auditory signals, and sets a waypoint at a fixed distance (K meters) along this direction. After the agent reaches the current waypoint, it repeats this process to gradually approach the target.\nâ€¢\nFrontier Waypoints\n: the hierarchical baseline selects the next waypoint at the intersection of the predicted DoA and the current exploration frontier. Frontier-based waypoints are standard in visual navigation\n[\n5\n,\n22\n,\n7\n]\n, making this a representative baseline of standard practice.\nâ€¢\nSupervised Waypoints\n: the hierarchical baseline takes the RGB image and audio spectrogram as inputs and, using supervised (non-end-to-end) training, predicts waypoints within the field of view. Its design follows the supervised waypoint predictor of Bansal et al\n[\n3\n]\n.\nâ€¢\nGan et al.\n[\n14\n]\n: one AVN method developed based on the AI2-THOR platform\n[\n17\n]\n. As a representative work in the early stage of the AVN field, its core advantage lies in the first systematic implementation of collaborative modeling between visual spatial memory and auditory sound source localization, providing a benchmark paradigm for subsequent related research.\nâ€¢\nSoundSpaces\n[\n9\n]\n: the first publicly available audio-visual embodied navigation simulation platform. Our work is based on their code.\nFigure 6:\nVariation of Modal Weights with Training Progress.\nIn the figure, the curves represent the evolutionary trends of the visual update weight (\nÎ²\nv\n\\beta_{v}\n) and auditory update weight (\nÎ²\na\n\\beta_{a}\n) on the Replica (left) and Matterport3D (right) datasets, respectively.\n4.4\nQuantitative Experimental Results\nTable\n1\npresents a comparison of the proposed method (CRFN) with representative baselines on the Replica\n[\n23\n]\nand Matterport3D\n[\n6\n]\ndatasets under the Depth setting. As shown, CRFN consistently achieves the best performance across all evaluation metrics, demonstrating its effectiveness in cross-modal feature integration and robust audio-visual navigation. On the Replica dataset, CRFN surpasses all prior methods in both Heard and Unheard settings. Specifically, in the Heard scenario, the SPL improves from 74.4 to 76.7, and SR rises to 93.1, indicating that the residual cross-modal fusion effectively enhances audio-visual synergy. In the Unheard scenario, the improvement becomes more pronouncedâ€”SPL reaches 41.6, about 7.6 percentage points higher than SoundSpaces\n[\n9\n]\nâ€”showing stronger generalization and robustness to unseen sound sources. Likewise, on the more complex Matterport3D dataset, CRFN achieves the highest scores in both settings, with SPL = 57.3 and SR = 70.3 in the Heard case, significantly outperforming existing approaches. These results confirm that CRFN better balances the complementary relationship between visual and auditory signals in realistic environments, leading to improved multimodal perception, decision-making, and generalization in unseen scenes.\n4.5\nQualitative Experimental Results\nFig.\n4\nand\n5\nprovide a visual comparison of the top-down navigation trajectories between our CRFN model and the SoundSpaces baseline across a series of typical scenes. Specifically, Fig.\n4\nshowcases two representative scenes from the Replica dataset, while Fig.\n5\npresents four complex scenes from the Matterport3D\n[\n6\n]\ndataset. From these trajectories, it can be clearly observed that the navigation paths generated by CRFN are significantly superior to those of the baseline, whether in the structurally simpler scenes of Replica (e.g.,\napartment1\nand\napartment4\nin Fig.\n4\n) or in the more complex layouts of Matterport3D\n[\n6\n]\n. CRFNâ€™s trajectories adhere more closely to the shortest path (green), exhibiting less invalid exploration and backtracking, which is also corroborated by its higher SPL scores. Particularly in the task with scene ID\npa4otMbVnkk\n, CRFN almost perfectly replicates the shortest path (SPL=0.99), whereas the SoundSpaces\n[\n9\n]\nbaseline, unable to establish a stable audio-visual correspondence, wanders extensively and inefficiently within the environment, resulting in extremely low path efficiency (SPL=0.33). This indicates that our model can efficiently fuse cross-modal information to quickly lock onto the targetâ€™s bearing even in acoustically simple, open spaces, thereby suppressing aimless exploration. Furthermore, in scenarios like\napartment4\n, where the sound source may be partially occluded by walls, CRFN is still able to progressively correct its course and successfully complete the navigation, while the baseline model exhibits greater uncertainty, leading to lower path efficiency.\nTable 2:\nAblation study on the Fusion Controller of CRFN on the Replica dataset.\nMethod\nTelephone\nUnheard\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSNA\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSNA\nâ†‘\n\\uparrow\nw/o FC\n69.3\n84.9\n44.4\n34.0\n43.5\n19.2\nCRFN (Ours)\n76.7\n93.1\n47.3\n41.6\n55.7\n22.5\nTable 3:\nAblation study on the Fusion Controller of CRFN on the Matterport3D dataset.\nMethod\nTelephone\nUnheard\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSNA\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSNA\nâ†‘\n\\uparrow\nw/o FC\n53.1\n65.9\n32.2\n28.9\n37.7\n14.7\nCRFN (Ours)\n57.3\n70.3\n33.2\n27.7\n40.1\n13.5\n4.6\nModal Dependence Analysis\nTo further reveal the internal learning dynamics of our model across different environments, we visualize the temporal evolution of the update weights for the visual (\nÎ²\nv\n\\beta_{v}\n) and auditory (\nÎ²\na\n\\beta_{a}\n) modalities during training. As shown in Fig.\n6\n, we observe a notable phenomenon: rather than fusing multimodal information in a static manner, the model adaptively adjusts its modality dependency according to the structural complexity and perceptual statistics of the environment, exhibiting a dynamic balancing strategy between modalities.\nSpecifically, on the Replica\n[\n23\n]\ndataset, the model progressively reduces its reliance on the auditory modality (\nÎ²\na\n\\beta_{a}\n) in the later stages of training, eventually converging to a visually dominated navigation strategy. This behavior can be attributed to the relatively small scale and regular structure of Replica\n[\n23\n]\nenvironments, where the agent can easily memorize spatial layouts through visual cues. Once the visual signal alone becomes sufficient for localization and planning, the model naturally suppresses the contribution of the temporally uncertain audio input, leading to a more efficient unimodal decision process.\nIn contrast, on the Matterport3D\n[\n6\n]\ndataset, the model maintains a balanced dependence on both modalities throughout training. Matterport3D\n[\n6\n]\ncontains large-scale, cluttered real-world scenes with complex geometry, occlusions, and reverberations, where a single modality provides insufficient information for reliable navigation. In this case, the audio modality offers complementary global directional cues that can penetrate obstacles, compensating for the limitations of visual perception. This results in a more robust cross-modal collaboration, highlighting the modelâ€™s inherent ability to adaptively modulate modality weights across diverse environments.\n4.7\nAblation Studies\nTo evaluate the effectiveness of the Fusion Controller (FC) in our CRFN, we conducted ablation experiments on both the Replica\n[\n23\n]\nand Matterport3D\n[\n6\n]\ndatasets. As shown in Tables\n2\nand\n3\n, removing the Fusion Controller (w/o FC) leads to a clear performance drop across all metrics. On the Replica\n[\n23\n]\ndataset, the SPL decreases from 76.7 to 69.3 and SR from 93.1 to 84.9, indicating that the absence of adaptive fusion regulation causes imbalance between modalities and degrades navigation efficiency in structured synthetic scenes. On the Matterport3D\n[\n6\n]\ndataset, performance also declines, though to a lesser extent, suggesting that in complex real-world environments the model can still partially rely on natural audio-visual complementarity. Overall, these results demonstrate that the Fusion Controller plays a crucial role in stabilizing cross-modal interactions, mitigating modality dominance, and enhancing generalization across diverse environments.\nIn this experiment, we investigate the effect of the initial residual scaling factor\nÎ²\ninit\n\\beta_{\\text{init}}\non the performance of the proposed model. The residual scaling factor controls the initial strength of the cross-modal residual connections in CRFN, which is crucial for balancing the contributions from both audio and visual modalities during training. We test three different values of\nÎ²\ninit\n\\beta_{\\text{init}}\n, namely\n0.1\n0.1\n,\n0.2\n0.2\n, and\n0.3\n0.3\n, across two popular datasets, Replica and Matterport3D. The results, summarized in Table\n4\n, show that a moderate value of\nÎ²\ninit\n=\n0.2\n\\beta_{\\text{init}}=0.2\nleads to the best overall performance, achieving higher SPL, SR, and SNA across both datasets. This suggests that initializing the residual scaling factor with a moderate value helps ensure a stable learning process, avoiding overdominance of any single modality while maintaining effective cross-modal fusion.\n5\nConlusion\nTable 4:\nEffect of the residual scaling initialization factor\nÎ²\ninit\n\\beta_{\\text{init}}\non navigation performance. Results show that a moderate initialization (\nÎ²\ninit\n=\n0.2\n\\beta_{\\text{init}}=0.2\n) yields the best balance between training stability and accuracy on both Replica and Matterport3D datasets.\nÎ²\ninit\n\\beta_{\\text{init}}\nReplica\nMatterport3D\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSNA\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSNA\nâ†‘\n\\uparrow\n0.1\n76.4\n91.6\n46.4\n53.2\n64.4\n31.3\n0.2\n76.7\n93.1\n47.3\n57.3\n70.3\n33.2\n0.3\n75.2\n92.6\n46.1\n53.3\n65.5\n32.0\nThis paper presents CRFN, a cross-modal residual fusion network designed for audio-visual navigation tasks. CRFN achieves fine-grained alignment and complementary modeling between modalities through a bidirectional residual interaction mechanism, while a fusion controller dynamically adjusts the contribution of each modality, effectively suppressing imbalance and feature degradation. Experimental results demonstrate that CRFN achieves superior navigation performance and stable cross-domain generalization in diverse and complex environments, validating the effectiveness and robustness of the proposed fusion mechanism.\nMoreover, by analyzing the dynamic evolution of residual coefficients, we uncover an interesting adaptive modality dependence phenomenon: agents tend to rely more on visual cues in high-fidelity synthetic environments, whereas cross-modal complementarity becomes crucial in complex real-world scenes. This finding provides new insights into how embodied agents coordinate multimodal perception and decision-making.\nAcknowledgements\nThis research was financially supported by the National Natural Science Foundation of China (Grant No. 62463029).\nReferences\n[1]\nP. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva,\net al.\n(2018)\nOn evaluation of embodied navigation agents\n.\narXiv preprint arXiv:1807.06757\n.\nCited by:\n6\n,\nÂ§4.2\n.\n[2]\nP. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. SÃ¼nderhauf, I. Reid, S. Gould, and A. Van Den Hengel\n(2018)\nVision-and-language navigation: interpreting visually-grounded navigation instructions in real environments\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 3674â€“3683\n.\nCited by:\nÂ§2\n.\n[3]\nS. Bansal, V. Tolani, S. Gupta, J. Malik, and C. Tomlin\n(2020)\nCombining optimal control and learning for visual navigation in novel environments\n.\nIn\nConference on Robot Learning\n,\npp.Â 420â€“429\n.\nCited by:\n4th item\n.\n[4]\nD. Batra, A. Gokaslan, A. Kembhavi, O. Maksymets, R. Mottaghi, M. Savva, A. Toshev, and E. Wijmans\n(2020)\nObjectnav revisited: on evaluation of embodied agents navigating to objects\n.\narXiv preprint arXiv:2006.13171\n.\nCited by:\nÂ§2\n.\n[5]\nJ. A. Caley, N. R. Lawrance, and G. A. Hollinger\n(2019)\nDeep learning of structured environments for robot search\n.\nAutonomous Robots\n43\n(\n7\n),\npp.Â 1695â€“1714\n.\nCited by:\n3rd item\n.\n[6]\nA. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niebner, M. Savva, S. Song, A. Zeng, and Y. Zhang\n(2018)\nMatterport3D: learning from rgb-d data in indoor environments\n.\nIn\n7th IEEE International Conference on 3D Vision, 3DV 2017\n,\npp.Â 667â€“676\n.\nCited by:\n2nd item\n,\nÂ§1\n,\nÂ§2\n,\nÂ§4.1\n,\nÂ§4.1\n,\nÂ§4.4\n,\nÂ§4.5\n,\nÂ§4.6\n,\nÂ§4.7\n.\n[7]\nD. S. Chaplot, R. Salakhutdinov, A. Gupta, and S. Gupta\n(2020)\nNeural topological slam for visual navigation\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 12875â€“12884\n.\nCited by:\n3rd item\n.\n[8]\nC. Chen, Z. Al-Halah, and K. Grauman\n(2021)\nSemantic audio-visual navigation\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 15516â€“15525\n.\nCited by:\nÂ§2\n.\n[9]\nC. Chen, U. Jain, C. Schissler, S. V. A. Gari, Z. Al-Halah, V. K. Ithapu, P. Robinson, and K. Grauman\n(2020)\nSoundSpaces: audio-visual navigation in 3d environments\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 17â€“36\n.\nCited by:\nÂ§2\n,\nTable 1\n,\n6th item\n,\nÂ§4.1\n,\nÂ§4.2\n,\nÂ§4.4\n,\nÂ§4.5\n.\n[10]\nC. Chen, S. Majumder, Z. Al-Halah, R. Gao, S. K. Ramakrishnan, and K. Grauman\n(2021-May 3â€“7)\nLearning to set waypoints for audio-visual navigation\n.\nIn\n9th International Conference on Learning Representations, ICLR 2021\n,\nVirtual Event, Austria\n.\nCited by:\nÂ§2\n,\nTable 1\n,\nTable 1\n,\nTable 1\n,\nTable 1\n,\n5\n.\n[11]\nJ. Chen, B. Lin, X. Liu, L. Ma, X. Liang, and K. K. Wong\n(2025)\nAffordances-oriented planning using foundation models for continuous vision-language navigation\n.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n,\nVol.\n39\n,\npp.Â 23568â€“23576\n.\nCited by:\nÂ§1\n.\n[12]\nJ. Chen, W. Wang, S. Liu, H. Li, and Y. Yang\n(2023)\nOmnidirectional information gathering for knowledge transfer-based audio-visual navigation\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.Â 10993â€“11003\n.\nCited by:\nÂ§2\n.\n[13]\nQ. Chen, D. Pitawela, C. Zhao, G. Zhou, H. Chen, and Q. Wu\n(2024)\nWebvln: vision-and-language navigation on websites\n.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n,\nVol.\n38\n,\npp.Â 1165â€“1173\n.\nCited by:\nÂ§1\n.\n[14]\nC. Gan, Y. Zhang, J. Wu, B. Gong, and J. B. Tenenbaum\n(2020)\nLook, listen, and act: towards audio-visual embodied navigation\n.\nIn\n2020 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 9701â€“9707\n.\nCited by:\nÂ§2\n,\nTable 1\n,\n5th item\n.\n[15]\nS. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik\n(2017)\nCognitive mapping and planning for visual navigation\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 2616â€“2625\n.\nCited by:\nÂ§1\n.\n[16]\nJ. Kim, S. Lee, D. Kwak, M. Heo, J. Kim, J. Ha, and B. Zhang\n(2016)\nMultimodal residual learning for visual qa\n.\nAdvances in neural information processing systems\n29\n.\nCited by:\nÂ§2\n.\n[17]\nE. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, M. Deitke, K. Ehsani, D. Gordon, Y. Zhu,\net al.\n(2017)\nAi2-thor: an interactive 3d environment for visual ai\n.\narXiv preprint arXiv:1712.05474\n.\nCited by:\n5th item\n.\n[18]\nX. Liu, S. Paul, M. Chatterjee, and A. Cherian\n(2024)\nCaven: an embodied conversational agent for efficient audio-visual navigation in noisy environments\n.\nIn\nProceedings of the AAAI conference on artificial intelligence\n,\nVol.\n38\n,\npp.Â 3765â€“3773\n.\nCited by:\nÂ§2\n.\n[19]\nM. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik,\net al.\n(2019)\nHabitat: a platform for embodied ai research\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp.Â 9339â€“9347\n.\nCited by:\nÂ§2\n,\nÂ§4.1\n.\n[20]\nZ. Shi, L. Zhang, L. Li, and Y. Shen\n(2025)\nTowards audio-visual navigation in noisy environments: a large-scale benchmark dataset and an architecture considering multiple sound-sources\n.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n,\nVol.\n39\n,\npp.Â 14673â€“14680\n.\nCited by:\nÂ§2\n.\n[21]\nN. Srivastava and R. R. Salakhutdinov\n(2012)\nMultimodal learning with deep boltzmann machines\n.\nAdvances in neural information processing systems\n25\n.\nCited by:\nÂ§3.1\n.\n[22]\nG. J. Stein, C. Bradley, and N. Roy\n(2018)\nLearning over subgoals for efficient navigation of structured, unknown environments\n.\nIn\nConference on robot learning\n,\npp.Â 213â€“222\n.\nCited by:\n3rd item\n.\n[23]\nJ. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Ren, S. Verma,\net al.\n(2019)\nThe replica dataset: a digital replica of indoor spaces\n.\narXiv preprint arXiv:1906.05797\n.\nCited by:\n2nd item\n,\nÂ§1\n,\nÂ§2\n,\nÂ§4.1\n,\nÂ§4.1\n,\nÂ§4.4\n,\nÂ§4.6\n,\nÂ§4.7\n.\n[24]\nA. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. Turner, N. Maestre, M. Mukadam, D. S. Chaplot, O. Maksymets,\net al.\n(2021)\nHabitat 2.0: training home assistants to rearrange their habitat\n.\nAdvances in neural information processing systems\n34\n,\npp.Â 251â€“266\n.\nCited by:\nÂ§2\n.\n[25]\nE. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra\n(2019)\nDd-ppo: learning near-perfect pointgoal navigators from 2.5 billion frames\n.\narXiv preprint arXiv:1911.00357\n.\nCited by:\nÂ§1\n.\n[26]\nY. Wu, P. Zhang, M. Gu, J. Zheng, and X. Bai\n(2024)\nEmbodied navigation with multi-modal information: a survey from tasks to methodology\n.\nInformation Fusion\n112\n,\npp.Â 102532\n.\nCited by:\nÂ§1\n.\n[27]\nZ. Yang, J. Liu, P. Chen, A. Cherian, T. K. Marks, J. Le Roux, and C. Gan\n(2024)\nRila: reflective and imaginative language agent for zero-shot semantic audio-visual navigation\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 16251â€“16261\n.\nCited by:\nÂ§2\n.\n[28]\nA. Younes, D. Honerkamp, T. Welschehold, and A. Valada\n(2023)\nCatch me if you hear me: audio-visual navigation in complex unmapped environments with moving sounds\n.\nIEEE Robotics and Automation Letters\n8\n(\n2\n),\npp.Â 928â€“935\n.\nCited by:\nÂ§2\n.\n[29]\nY. Yu, L. Cao, F. Sun, X. Liu, and L. Wang\n(2022-11)\nPay self-attention to audio-visual navigation\n.\nIn\n33rd British Machine Vision Conference 2022, BMVC 2022\n,\nLondon, UK\n,\npp.Â 46\n.\nCited by:\nÂ§2\n.\n[30]\nY. Yu, L. Cao, F. Sun, C. Yang, H. Lai, and W. Huang\n(2023-04)\nEcho-enhanced embodied visual navigation\n.\nNeural Computation\n35\n(\n5\n),\npp.Â 958â€“976\n.\nExternal Links:\nISSN 0899-7667\nCited by:\nÂ§2\n.\n[31]\nY. Yu, C. Chen, L. Cao, F. Yang, and F. Sun\n(2023-08)\nMeasuring acoustics with collaborative multiple agents\n.\nIn\nProceedings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI-23)\n,\nMacao, China\n.\nCited by:\nÂ§2\n.\n[32]\nY. Yu, W. Huang, F. Sun, C. Chen, Y. Wang, and X. Liu\n(2022)\nSound adversarial audio-visual navigation\n.\nIn\nThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022\n,\nCited by:\nÂ§2\n.\n[33]\nY. Yu, Z. Jia, F. Shi, M. Zhu, W. Wang, and X. Li\n(2021)\nWeavenet: end-to-end audiovisual sentiment analysis\n.\nIn\nInternational Conference on Cognitive Systems and Signal Processing\n,\npp.Â 3â€“16\n.\nCited by:\nÂ§2\n.\n[34]\nY. Yu and D. Yang\n(2025)\nDOPE: dual object perception-enhancement network for vision-and-language navigation\n.\nIn\nProceedings of the 2025 International Conference on Multimedia Retrieval\n,\npp.Â 1739â€“1748\n.\nCited by:\nÂ§1\n.\n[35]\nD. Zheng, S. Huang, L. Zhao, Y. Zhong, and L. Wang\n(2024)\nTowards learning a generalist model for embodied navigation\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 13624â€“13634\n.\nCited by:\nÂ§1\n.",
    "preview_text": "Audio-visual embodied navigation aims to enable an agent to autonomously localize and reach a sound source in unseen 3D environments by leveraging auditory cues. The key challenge of this task lies in effectively modeling the interaction between heterogeneous features during multimodal fusion, so as to avoid single-modality dominance or information degradation, particularly in cross-domain scenarios. To address this, we propose a Cross-Modal Residual Fusion Network, which introduces bidirectional residual interactions between audio and visual streams to achieve complementary modeling and fine-grained alignment, while maintaining the independence of their representations. Unlike conventional methods that rely on simple concatenation or attention gating, CRFN explicitly models cross-modal interactions via residual connections and incorporates stabilization techniques to improve convergence and robustness. Experiments on the Replica and Matterport3D datasets demonstrate that CRFN significantly outperforms state-of-the-art fusion baselines and achieves stronger cross-domain generalization. Notably, our experiments also reveal that agents exhibit differentiated modality dependence across different datasets. The discovery of this phenomenon provides a new perspective for understanding the cross-modal collaboration mechanism of embodied agents.\n\nResidual Cross-Modal Fusion Networks for Audio-Visual Navigation\nYi Wang â€ƒâ€ƒYinfeng Yu\nSchool of Computer Science and Technology, Xinjiang University, Urumqi, China\nJoint International Research Laboratory of Silk Road Multilingual Cognitive\nYinfeng Yu is the corresponding author (Email: yuyinfeng@xju.edu.cn)\nBin Ren\nSchool of Mechatronic Engineering and Automation\nShanghai University, Shanghai, China\nAbstract\nAudio-visual embodied navigation aims to enable an agent to autonomously localize and reach a sound source in unseen 3D environments by leveraging auditory cues. The key challenge of this task lies in effectively modeling the i",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "audio-visual navigation",
        "multimodal fusion",
        "residual connections",
        "cross-modal interaction",
        "embodied agents"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºéŸ³é¢‘-è§†è§‰å¯¼èˆªçš„è·¨æ¨¡æ€æ®‹å·®èåˆç½‘ç»œï¼Œæ—¨åœ¨é€šè¿‡åŒå‘æ®‹å·®äº¤äº’å®ç°å¤šæ¨¡æ€ç‰¹å¾çš„äº’è¡¥å»ºæ¨¡å’Œç»†ç²’åº¦å¯¹é½ï¼Œä»¥æå‡åœ¨æœªè§3Dç¯å¢ƒä¸­çš„å¯¼èˆªæ€§èƒ½ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-11T12:11:36Z",
    "created_at": "2026-01-21T12:09:05.898634",
    "updated_at": "2026-01-21T12:09:05.898645",
    "recommend": 0
}