{
    "id": "2601.07768v1",
    "title": "THETA: Triangulated Hand-State Estimation for Teleoperation and Automation in Robotic Hand Control",
    "authors": [
        "Alex Huang",
        "Akshay Karthik"
    ],
    "abstract": "机械手的遥操作常受限于用于估计手部关节相对位置（XYZ）的深度相机和传感器手套的高昂成本。本文提出了一种新颖且经济高效的方法，利用三个网络摄像头进行基于三角测量的跟踪，以近似估计人类手指的相对关节角度（θ）。我们还引入了一款改进的DexHand——来自TheRobotStudio的低成本机械手，以展示THETA系统的实时应用能力。数据采集使用了三个以120度间隔排列的640x480像素网络摄像头，记录了40种不同的手势，生成了超过48,000张RGB图像。关节角度通过手动测量手指的掌指关节（MCP）、近端指间关节（PIP）和远端指间关节（DIP）的中点来确定。采集的RGB帧使用基于ResNet-50骨干网络的DeepLabV3分割模型进行多尺度手部分割处理。分割后的图像经过HSV滤波，输入到THETA架构中，该架构包含一个基于MobileNetV2的CNN分类器，专为分层空间特征提取优化，以及一个编码多视角手部表示的9通道输入张量。该分类模型将分割后的手部视图映射为离散关节角度，实现了97.18%的准确率、98.72%的召回率、0.9274的F1分数和0.8906的精确率。在实时推理中，THETA系统同步捕获帧图像，分割手部区域，进行滤波处理，并编译9通道张量进行分类。关节角度预测结果通过串行通信传输至Arduino，使DexHand能够复现手部动作。未来研究将增加数据集多样性，集成手腕跟踪功能，并应用如OpenAI-Vision等计算机视觉技术。THETA系统有望为医疗、语言和制造应用提供经济高效、用户友好的遥操作解决方案。",
    "url": "https://arxiv.org/abs/2601.07768v1",
    "html_url": "https://arxiv.org/html/2601.07768v1",
    "html_content": "THETA: Triangulated Hand-State Estimation for Teleoperation and Automation in\nRobotic Hand Control\nAkshay Karthik\nAlex Huang\nAbstract\nThe teleoperation of robotic hands is limited by the high costs of depth cameras and sensor gloves, commonly used to estimate hand relative joint positions (XYZ). We present a novel, cost-effective approach using three webcams for triangulation-based tracking to approximate relative joint angles (theta) of human fingers. We also introduce a modified DexHand, a low-cost robotic hand from TheRobotStudio, to demonstrate THETA’s real-time application. Data collection involved 40 distinct hand gestures using three 640x480p webcams arranged at 120-degree intervals, generating over 48,000 RGB images. Joint angles were manually determined by measuring midpoints of the MCP, PIP, and DIP finger joints. Captured RGB frames were processed using a DeepLabV3 segmentation model with a ResNet-50 backbone for multi-scale hand segmentation. The segmented images were then HSV-filtered and fed into THETA’s architecture, consisting of a MobileNetV2-based CNN classifier optimized for hierarchical spatial feature extraction and a 9-channel input tensor encoding multi-perspective hand representations. The classification model maps segmented hand views into discrete joint angles, achieving 97.18% accuracy, 98.72% recall, F1 Score of 0.9274, and a precision of 0.8906. In real-time inference, THETA captures simultaneous frames, segments hand regions, filters them, and compiles a 9-channel tensor for classification. Joint-angle predictions are relayed via serial to an Arduino, enabling the DexHand to replicate hand movements. Future research will increase dataset diversity, integrate wrist tracking, and apply computer vision techniques such as OpenAI-Vision. THETA potentially ensures cost-effective, user-friendly teleoperation for medical, linguistic, and manufacturing applications.\nI\nIntroduction\nThe robotic teleoperation field, where robotic systems mimic human movements, has been rapidly growing over recent years. This growth is supported by increasing demands for automation in environments that are hazardous, inaccessible, or require high precision. Teleoperated systems are playing key roles in various applications such as surgical robotics, repair industry, nuclear decommissioning, space missions, care for the elderly, and supply chain logistics. A core element of such applications is the robotic hand, which must emulate fine human motor control with high compliance and precision.\nFrom remotely suturing in telesurgery to managing mechanical parts in nuclear reactors, robotic hands are increasingly being used for operations once only feasible by humans. This expansion is reflected economically: the teleoperated robotics market is projected to grow from $40.17 billion in 2022 to over $170 billion by 2032, highlighting the urgency for scalable, adaptable, and cost-effective solutions [1]. At the heart of effective robotic hand teleoperation is the ability to accurately estimate human finger joint angles in real-time, allowing the robotic counterpart to mimic complex hand movements. Despite recent technological breakthroughs, current solutions fall short in balancing accuracy, cost, and accessibility, especially in unstructured, real-world settings.\nIn order to support the growing applications of teleoperated robotic hands, robotic hand systems must not only reproduce static hand postures but also reproduce dynamic, real-time human movement. This requires the continuous capture and interpretation of fine-grained biomechanical data, particularly joint angles of the fingers and hand, which are required in a form that is responsive, low-latency, and insensitive to environmental variation. From cutting precise sutures within a medical operating theater to maneuvering tools within a hazardous manufacturing setting, robotic hands must achieve a great degree of articulation in order to serve as reliable proxies for human dexterity. Therefore, the effectiveness of teleoperated robot hands increasingly hinges upon the efficiency and accuracy of supporting motion tracking and hand state estimation algorithms.\nII\nRelated Work\nII-A\nDepth-Sensing & Infrared Systems\nDepth-sensing and infrared (IR) cameras such as the Intel RealSense D455 and Microsoft Azure Kinect produce 3D point clouds through stereo vision or time-of-flight methods, enabling joint localization. However, their accuracy degrades under occlusion, non-frontal orientations, and motion blur. Gallo et al. illustrated how depth readings start to get noisy and unreliable as soon as the surface normals of the hand are displaced out of the camera’s line of sight [2].\nHigh-end IR motion capture systems, such as Vicon, achieve high spatial precision by tracking reflective markers, widely applied in biomechanics and animation. These systems, however, require extensive infrastructure, strict calibration, and significant financial investment, often exceeding $10,000, which limits their practical use [3]. Additionally, they are sensitive to ambient lighting and require controlled environments [4].\nBoth classes of systems calculate joint angles by forming vectors between adjacent joints and applying the cosine law. The angle at a joint is computed by the inverse cosine of the normalized dot product between vectors connecting neighboring segments. Although mathematically straightforward, this process depends entirely on consistent and high-quality 3D tracking, which remains a major barrier for depth-based and marker-based systems in unconstrained environments.\nII-B\nSensor Glove-Based Systems\nSensor gloves, including the CyberGlove II and Manus Prime X, integrate flex sensors and inertial measurement units (IMUs) to capture joint angles and hand orientation in real time, supporting applications in rehabilitation, virtual reality, and robotics. Despite high temporal resolution, these devices face persistent limitations. Dipietro et al. identified calibration drift, mechanical fatigue, and user discomfort during extended use as key concerns [5]. Further, sensor gloves require precise fitting, restrict natural hand movements, and remain costly, with prices exceeding $10,000 for CyberGlove II and $5,000 for Manus Prime X, limiting accessibility for general users [6].\nII-C\nVision-Based Estimation with RGB Cameras\nVision-based methods utilizing standard RGB cameras and machine learning have emerged as cost-effective alternatives for hand pose estimation. These approaches employ convolutional neural networks (CNNs) to detect hand landmarks or segment hand regions for joint angle prediction from 2D imagery.\nGoogle’s MediaPipe, for instance, estimates 21 hand landmarks and computes joint angles using geometric calculations. While computationally efficient, its accuracy diminishes under hand rotations, occlusions, or non-frontal poses [7].\nMore robust methods apply semantic segmentation networks such as U-Net, Mask R-CNN, SegNet, PSPNet, Attention U-Net, Cascade Mask R-CNN, and dilated convolution models to isolate hand regions prior to pose estimation, improving robustness to background noise and lighting variations [8]. However, most of these models rely on single-view 2D inputs and cannot resolve depth ambiguity under occlusion. Their limited perspective restricts their ability to capture full 3D hand motion, highlighting the need for multi-view or triangulation-based approaches that can reconstruct hand poses from multiple viewpoints for reliable joint angle estimation.\nIII\nMethodology\nThis paper presents a novel, low-cost pipeline for real-time finger joint angle estimation from a multi-view vision system and deep learning for robotic hand teleoperation. The pipeline has four major steps: hardware integration and setup, dataset annotation and creation, segmentation and classification, and real-time actuation of a robotic hand. Each of these steps was designed to be robust, low-cost, and accurate.\nThe process begins with the construction of a 3D-printed, servo-driven robotic hand and the development of a ROS 2-based control system for actuation. Next, synchronized multi-angle webcam images of hand gestures are collected, segmented using DeepLabV3, and manually annotated with corresponding joint angles. These segmented images are then fed into a MobileNetV2-based classifier trained to predict joint angles efficiently and accurately. Finally, the predicted joint angles are transmitted in real time to control the robotic hand, completing the teleoperation loop with live inference and actuation.\nIII-A\nDexHand Robotic Hand Design & ROS 2 Control\nThe hardware for the robotic hand used in the project was derived from TheRobotStudio’s open-source DexHand V1.0 design [9]. The design was remodeled using a combination of 3D-printed components, mini servos, fishing line, springs, and mechanical fasteners. The completed design included metacarpal bones, phalanges, knuckles, and a wrist mechanism. Finger articulation was achieved through three Emax ES3352 12.4g mini servos per finger: two servos controlled abduction/adduction and base flexion, and a third servo controlled distal flexion, with spring extensions providing passive fingertip retraction (Fig. 1). The total cost of constructing the robotic hand, including all mechanical and electronic components, was approximately $250, making it a very low-cost platform for research in teleoperation.\nFigure 1:\nAssembled DexHand (with personal modifications)\nFigure 2:\nROS 2-Arduino Joint Angle Transmission pipeline for robotic hand servos actuation.\nControl of the robotic hand was implemented using Python via a dual-node ROS 2 system running on a VMware virtual machine with USB passthrough to an Arduino Mega (Fig. 2). A Gesture Controller node generated arrays of 15 servo angles, published to a shared topic (dexhand_hw_command), while a USB Serial node formatted and transmitted these commands over serial to the Arduino [10]. The Arduino parsed incoming data using C++ and actuated the corresponding servos. This pipeline enabled smooth, low-latency control of the hand from real-time gesture predictions.\nIII-B\nTHETA Architectural Pipeline Multi-View Data Collection, Annotation & Segmentation\nThree RGB webcams were mounted 120 degrees relative to each other and 9 inches in distance away from the center point in order to record the hand from three sides: front, left, and right, giving an accurate 3D view of the hand state (Fig. 3). Every camera used was recording video at a resolution of 640×480 pixels and at a speed of 30 frames per second. Multi-view capture offered more complete hand pose capture, enabling the system to handle occlusion and diversity in hand orientation more effectively.\nFigure 3:\nTriangulation Data Collection Setup\n40 different hand postures were selected to record a wide range of finger positions and arrangements. For every gesture, the equivalent set of 15 joint angles was calculated manually with the use of a physical protractor. The joint angles were for the MCP (metacarpophalangeal), PIP (proximal interphalangeal), and DIP (distal interphalangeal) joints of the index, middle, ring, and pinky fingers.\nThe measurements of the joint angles were taken carefully and kept in an organized annotation file (gesture_angles.csv) so that there was an immediate ground truth data for each image frame (Table I). To enhance model robustness and enforce better generalization at inference time, ±5 degrees of jitter was added randomly to all joint angles during data capture. This augmentation simulated realistic variability in gesture appearance across users and viewpoints, preventing the model from overfitting to overly specific static poses.\nTABLE I:\nExample entries from the “gesture_angles.csv” dataset\nGesture ID\nGesture Name\nIndex MCP Angle\nIndex PIP Angle\n1\nClosed Fist\n90 (\n±\n\\pm\n5°)\n90 (\n±\n\\pm\n5°)\n2\nOpen Palm\n180 (\n±\n\\pm\n5°)\n180 (\n±\n\\pm\n5°)\n3\nNumber One\n180 (\n±\n\\pm\n5°)\n180 (\n±\n\\pm\n5°)\nA custom Python script was developed to automate the sourcing, labeling, and formatting of image and annotation pairs from the dataset for seamless integration into the model training pipeline. The final dataset contained more than 48,000 labeled images, split evenly among all gesture classes and views. The dataset served as the foundation for model training and testing and offered a rich, variegated, and balanced collection of human hand gestures to the model for precise joint angle estimation.\nTo isolate the hand before joint angle estimation, we used DeepLabV3, a state-of-the-art semantic segmentation network (Fig. 4). Its architecture employs Atrous Spatial Pyramid Pooling (ASPP) to aggregate multi-scale contextual information, enabling precise segmentation of complex hand structures under diverse poses and lighting conditions.\nFigure 4:\nMulti-View RGB Image Segmentation Using DeepLabV3 for Image Preprocessing, Feature Extraction, Segmentation Prediction, and Mask Generation.\nThe segmentation network uses ResNet-50, a deep residual CNN pretrained on the COCO dataset. Early layers were frozen to preserve low-level feature extraction, while later layers were fine-tuned for binary hand segmentation. ResNet-50 leverages identity-based residual connections to mitigate vanishing gradients and enable efficient optimization of deep architectures. These connections facilitate gradient flow during backpropagation, allowing the network to learn hierarchical features critical for segmenting complex hand shapes and articulations.\nTABLE II:\nComparison of Semantic Segmentation Models: mIoU and Parameters\nModel\nmIoU (%)\nParameters (M)\nDeepLab V3-Res50 (2018)\n78.0 / VOC12\n42\nDeepLab V3+ Xception-71 (2018)\n89.0 / VOC12\n59\nDeepLab V3+ MobileNetV3 (2021)\n71.0 / Cityscapes\n11\nBiSeNet V1 (2018)\n68.4 / Cityscapes\n5.8\nFast-SCNN (2019)\n68.0 / Cityscapes\n1.1\nPSPNet R101 (2018)\n78.4 / Cityscapes\n65\nFPN R50 (2018)\n75.9 / Cityscapes\n37\nDenseASPP (2018)\n80.6 / VOC12\n25\nHRNet-OCR (2020)\n84.5 / Cityscapes\n70\nSegFormer-B5 (2021)\n84.0 / Cityscapes\n85\nResNet-50 was selected as the segmentation backbone for its balance between accuracy and complexity (Table II). DeepLab V3-Res50 achieves 78.0% mIoU on VOC12 with 42 million parameters, offering a strong trade-off between accuracy and computational cost. Lightweight models such as MobileNetV3 (71.0%, 11M), Fast-SCNN (68.0%, 1.1M), and BiSeNet V1 (68.4%, 5.8M) are more efficient but insufficient for precise hand segmentation, while higher-performing models like DeepLab V3+ Xception-71 (89.0%, 59M), HRNet-OCR (84.5%, 70M), and SegFormer-B5 (84.0%, 85M) impose high computational demands. ResNet-50 thus provides effective feature extraction with manageable complexity. The pipeline supports grayscale, RGB, and RGB-D inputs, with the initial layer adapting to input channels. Frames are resized to 224×224, normalized, and processed by a COCO-pretrained DeepLabV3 model using Atrous Spatial Pyramid Pooling. The soft mask output is thresholded at 0.5, refined by morphological operations, and passed to the joint angle estimation module.\nIII-C\nTHETA Architecture Pipeline Segmentation Preprocessing & Joint Angle Classification\nThe next step in the THETA pipeline is joint angle prediction from hand images segmented by HSV color thresholds, which isolate red hand regions and remove background pixels. Segmented views from front, right, and left cameras are combined into a multi-view input and processed by a deep learning classifier for joint angle estimation (Fig. 5).\nFigure 5:\nMulti-View RGB Image Segmentation Using DeepLabV3 for Image Preprocessing, Feature Extraction, Segmentation Prediction, and Mask Generation.\nTABLE III:\nComparison of Lightweight and Efficient Image Classification Models\nModel\nTop-1 Accuracy (%)\nParameters (M)\nMobileNet V2 (2018)\n72.0\n3.4\nMobileNet V3-Large (2019)\n75.2\n5.4\nMnasNet-A1 (2019)\n75.2\n3.9\nEfficientNet-B0 (2019)\n77.1\n5.3\nEfficientNet-B7 (2019)\n84.3\n66\nEfficientNet V2-S (2021)\n83.9\n22\nEfficientNet V2-L (2021)\n85.7\n120\nRepVGG-A2 (2021)\n80.2\n80\nConvNeXt-Tiny (2022)\n82.5\n28.6\nCoAtNet-0 (2021)\n81.6\n25\nThis study compared lightweight architectures, including MobileNetV3, EfficientNet, and ConvNeXt (Table III). While MobileNetV3 improves MobileNetV2 in accuracy (75.2% vs. 72.0%) with higher complexity (5.4M vs. 3.4M parameters), EfficientNet and ConvNeXt reach up to 85.7% accuracy but at significantly higher costs (up to 120M parameters), making them less suitable for joint angle prediction. MobileNetV2 provides an optimal trade-off between accuracy, efficiency, and real-time inference for this task.\nMobileNetV2 leverages depthwise separable convolutions and inverted residual blocks to enable rapid feature extraction with low computational overhead, making it well-suited for multi-view hand inputs. The model outputs logits shaped as (batch_size, 15, 10), corresponding to prediction scores for 15 joints over 10 discrete angle bins. To mitigate class imbalance and overconfidence in dominant classes, softmax activation with temperature scaling (T=2.0) was applied to calibrate the output probability distribution by smoothing predictions, effectively reducing model overconfidence and improving generalization. Additionally, focal loss was combined with inverse bin frequency weighting, assigning higher loss penalties to underrepresented joint poses to address severe class imbalance across the joint angle bins, ensuring balanced learning across common and rare joint configurations. The model was trained for 10 epochs using distributed data parallelism and the Adam optimizer (learning rate 0.001), with transfer learning applied by freezing all but the final two layers. The MobileNetV2-based THETA pipeline demonstrated high accuracy, robustness, and real-time performance in joint angle prediction from segmented multi-view RGB inputs, validating its effectiveness for robotic hand teleoperation.\nIV\nResults & Analysis\nThe model achieved a training accuracy of 97.50% and a validation accuracy of 97.03%, with both training and validation losses converging to 0.0001 (Fig. 6). The close alignment between training and validation accuracy, along with minimal final loss, indicates strong generalization and effective mitigation of overfitting. Training and validation losses represent the mean error between predicted and ground-truth joint angles across the respective datasets.\nFigure 6:\n97.50% training accuracy and 97.03% validation accuracy with loss convergence to 0.0001.\nTHETA was evaluated using accuracy, precision, recall, and F1 score to assess its predictive performance on unseen hand states (Fig. 7). The model achieved 97.18% testing accuracy, with a precision of 0.8906 and a recall of 0.9872, indicating high correctness and strong detection of true joint angles. The F1 score of 0.9274 confirms a strong balance between precision and recall, particularly important for imbalanced angle distributions.\nFigure 7:\nTHETA achieves 97.18% accuracy, 0.9274 F1-score, 0.8906 precision, and 0.9872 recall in joint angle classification, ensuring precise hand pose estimation for robust motion analysis.\nThe THETA pipeline was validated in real-time using live webcam input and deployed on a servo-actuated robotic hand (Fig. 8). Predicted joint angles showed strong alignment with actual angles across various gestures and lighting conditions. Processed angles were transmitted via serial communication to an Arduino microcontroller as formatted strings, enabling rapid servo actuation. The robotic hand accurately replicated human finger movements, including flexion, extension, and split-finger poses, demonstrating precise, low-latency control through an efficient, low-cost machine learning-based teleoperation framework (Fig. 9).\nFigure 8:\nReal-time joint angle inference using THETA’s multi-view triangulation for precise and responsive robotic hand control.\nFigure 9:\nTHETA real-time joint angle prediction and inference with serial communication to the DexHand using Arduino.\nV\nConclusions & Future Works\nThe THETA pipeline provides an efficient deep learning-based framework for real-time robotic hand control, predicting finger joint angles from multi-view RGB inputs with high accuracy, low latency, and minimal hardware requirements. Despite its strong performance, limitations remain. The current dataset, though comprising over 48,000 labeled images, requires further augmentation for better generalization across users, gestures, and environmental conditions. Cloud-based training is computationally expensive, limiting frequent model updates. Additionally, the classification-based architecture discretizes joint angles into bins, reducing smoothness for tasks requiring fine-grained motion.\nFuture work will address these challenges by shifting to a regression-based model for continuous joint angle prediction and improving articulation precision. Adaptive learning through user-specific feedback will enable personalization over time. Integrating large language models for contextual reasoning may further enhance system performance in dynamic environments. THETA’s compact design, low computational cost, and strong predictive capability make it suitable for applications such as prosthetic control, robotic-assisted surgery, sign language translation, and remote teleoperation in extreme environments.\nTHETA’s simple setup and robustness has the potential to increase the accessibility of high-compliant teleoperated robotic hands, with implications for countless real-life applications.\nAcknowledgment\nThis work was supported by the ASU Sun Robotics Lab, which provided funding, resources, and research support for the development of the THETA pipeline.\nReferences\n[1]\nIMARC Group, “Robotics market size, growth trends, report and forecast 2022–2027,” [Online]. Available:\nhttps://www.imarcgroup.com/robotics-market\n[2]\nO. Gallo, R. Manduchi, and A. Rafii, “Robust curvature estimation from noisy depth data,” in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)\n, 2011, pp. 2405–2412.\n[3]\nX. Poma, E. Riba, and A. D. Sappa, “Dense 3D reconstruction with RGB-D cameras for teleoperation,”\nSensors\n, vol. 20, no. 4, p. 1163, Feb. 2020.\n[4]\nK. Berger, K. Ruhl, and M. Magnor, “Markerless motion capture using multiple color-depth sensors,” in\nProc. Vision, Modeling, and Visualization (VMV)\n, 2011, pp. 317–324.\n[5]\nL. Dipietro, A. M. Sabatini, and P. Dario, “A survey of glove-based systems and their applications,”\nIEEE Trans. Syst., Man, Cybern. C\n, vol. 38, no. 4, pp. 461–482, Jul. 2008.\n[6]\nS. Kim, J. Lee, Y. Lee, and S. B. Kim, “Soft wearable robots: Integration of soft actuators and sensors for wearable robotic applications,”\nAdv. Mater.\n, vol. 31, no. 10, p. 1806290, Mar. 2019.\n[7]\nF. Zhang, V. Bazarevsky, et al., “MediaPipe Hands: On-device real-time hand tracking,”\narXiv preprint\n, arXiv:2006.10214, 2020.\n[8]\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-decoder with atrous separable convolution for semantic image segmentation,” in\nProc. Eur. Conf. Comput. Vis. (ECCV)\n, 2018, pp. 833–851.\n[9]\nA. Open, “DexHand—An open source dexterous humanoid robot hand,” DexHand, 2023. [Online]. Available:\nhttps://www.dexhand.org\n[10]\niotdesignshop, “GitHub - iotdesignshop/dexhand_ros2_meta: Metapackage for DexHand ROS 2 Packages,” GitHub, 2023. [Online]. Available:\nhttps://github.com/iotdesignshop/dexhand_ros2_meta",
    "preview_text": "The teleoperation of robotic hands is limited by the high costs of depth cameras and sensor gloves, commonly used to estimate hand relative joint positions (XYZ). We present a novel, cost-effective approach using three webcams for triangulation-based tracking to approximate relative joint angles (theta) of human fingers. We also introduce a modified DexHand, a low-cost robotic hand from TheRobotStudio, to demonstrate THETA's real-time application. Data collection involved 40 distinct hand gestures using three 640x480p webcams arranged at 120-degree intervals, generating over 48,000 RGB images. Joint angles were manually determined by measuring midpoints of the MCP, PIP, and DIP finger joints. Captured RGB frames were processed using a DeepLabV3 segmentation model with a ResNet-50 backbone for multi-scale hand segmentation. The segmented images were then HSV-filtered and fed into THETA's architecture, consisting of a MobileNetV2-based CNN classifier optimized for hierarchical spatial feature extraction and a 9-channel input tensor encoding multi-perspective hand representations. The classification model maps segmented hand views into discrete joint angles, achieving 97.18% accuracy, 98.72% recall, F1 Score of 0.9274, and a precision of 0.8906. In real-time inference, THETA captures simultaneous frames, segments hand regions, filters them, and compiles a 9-channel tensor for classification. Joint-angle predictions are relayed via serial to an Arduino, enabling the DexHand to replicate hand movements. Future research will increase dataset diversity, integrate wrist tracking, and apply computer vision techniques such as OpenAI-Vision. THETA potentially ensures cost-effective, user-friendly teleoperation for medical, linguistic, and manufacturing applications.\n\nTHETA: Triangulated Hand-State Estimation for Teleoperation and Automation in\nRobotic Hand Control\nAkshay Karthik\nAlex Huang\nAbstract\nThe teleoperation of robotic hands is limited by the high costs of depth camera",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "❌ medical"
    ],
    "one_line_summary": "论文包含负面关键词「medical」，自动标记为不相关",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T17:50:02Z",
    "created_at": "2026-01-21T12:09:12.248337",
    "updated_at": "2026-01-21T12:09:12.248344"
}