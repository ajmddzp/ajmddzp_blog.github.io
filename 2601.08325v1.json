{
  "id": "2601.08325v1",
  "title": "ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation",
  "authors": [
    "Zhenyang Liu",
    "Yongchong Gu",
    "Yikai Wang",
    "Xiangyang Xue",
    "Yanwei Fu"
  ],
  "abstract": "Recent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook the importance of active perception: they typically rely on static, wrist-mounted cameras that provide an end-effector-centric viewpoint. As a result, these models are unable to adaptively select optimal viewpoints or resolutions during task execution, which significantly limits their performance in long-horizon tasks and fine-grained manipulation scenarios. To address these limitations, we propose ActiveVLA, a novel vision-language-action framework that empowers robots with active perception capabilities for high-precision, fine-grained manipulation. ActiveVLA adopts a coarse-to-fine paradigm, dividing the process into two stages: (1) Critical region localization. ActiveVLA projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness. (2) Active perception optimization. Drawing on the localized critical regions, ActiveVLA uses an active view selection strategy to choose optimal viewpoints. These viewpoints aim to maximize amodal relevance and diversity while minimizing occlusions. Additionally, ActiveVLA applies a 3D zoom-in to improve resolution in key areas. Together, these steps enable finer-grained active perception for precise manipulation. Extensive experiments demonstrate that ActiveVLA achieves precise 3D manipulation and outperforms state-of-the-art baselines on three simulation benchmarks. Moreover, ActiveVLA transfers seamlessly to real-world scenarios, enabling robots to learn high-precision tasks in complex environments.",
  "url": "https://arxiv.org/abs/2601.08325v1",
  "html_url": "https://arxiv.org/html/2601.08325v1",
  "html_content": "ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation\nZhenyang Liu\n1,2\n, Yongchong Gu\n1\n, Yikai Wang\n3\n1\n1\nfootnotemark:\n1\n, Xiangyang Xue\n1\n2\n2\nfootnotemark:\n2\n, Yanwei Fu\n1,2\n2\n2\nfootnotemark:\n2\n1\nFudan University\n2\nShanghai Innovation Institute\n3\nNanyang Technological University\nliuzy24@m.fudan.edu.cn, yongchonggu22@m.fudan.edu.cn, yikai.wang@ntu.edu.sg,\nxyxue@fudan.edu.cn, yanweifu@fudan.edu.cn\nProject Page:\nZhenyangLiu.github.io/ActiveVLA\nAbstract\nRecent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook the importance of active perception: they typically rely on static, wrist-mounted cameras that provide an end-effector-centric viewpoint. As a result, these models are unable to adaptively select optimal viewpoints or resolutions during task execution, which significantly limits their performance in long-horizon tasks and fine-grained manipulation scenarios. To address these limitations, we propose\nActiveVLA\n, a novel vision-language-action framework that empowers robots with active perception capabilities for high-precision, fine-grained manipulation. ActiveVLA adopts a coarse-to-fine paradigm, dividing the process into two stages:\n(1) Critical region localization. ActiveVLA projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness.\n(2) Active perception optimization. Drawing on the localized critical regions, ActiveVLA uses an active view selection strategy to choose optimal viewpoints. These viewpoints aim to maximize amodal relevance and diversity while minimizing occlusions. Additionally, ActiveVLA applies a 3D zoom-in to improve resolution in key areas. Together, these steps enable finer-grained active perception for precise manipulation.\nExtensive experiments demonstrate that ActiveVLA achieves precise 3D manipulation and outperforms state-of-the-art baselines on three simulation benchmarks. Moreover, ActiveVLA transfers seamlessly to real-world scenarios, enabling robots to learn high-precision tasks in complex environments.\n1\nIntroduction\nâ€œPerception is not a passive process. Itâ€™s an active process of hypothesis testing.â€\nâ€“ Richard Gregory\nPretrained vision-language models (VLMs)\n[\nbeyer2024paligemma\n,\nwang2024qwen2\n,\nbai2025qwen2\n,\nkaramcheti2024prismatic\n]\nhave become a highly effective approach for building large-scale vision-language-action (VLA) architectures. Such models have demonstrated impressive generalization and robustness in robotic manipulation\n[\nkimopenvla\n,\nblack2410pi0\n,\nliu2025trivla\n,\nintelligence2025pi_\n,\nli2023vision\n,\nbrohan2023rt\n]\n. However, most current VLA approaches primarily process 2D visual inputs, requiring massive datasets to bridge the gap between perception and action. In contrast, 3D-aware robotic policies leverage structural cues from 3D data, achieving better sample efficiency and spatial reasoning for complex manipulation tasks\n[\nshridhar2023perceiver\n,\n3d-da\n,\ngervet2023act3d\n,\ngoyal2023rvt\n,\ngoyal2024rvt\n]\n.\nGregoryâ€™s insight captures a core challenge in embodied intelligence: perception must not remain a passive receiver of sensory input but instead become an active hypothesis-testing processâ€”seeking, selecting, and verifying information relevant to the task at hand. However, existing VLA models\n[\nkimopenvla\n,\nliu2025trivla\n,\nzhen20243d\n,\nqu2025spatialvla\n]\nlargely rely on static or wrist-mounted cameras, constraining observations to a fixed, end-effector-centric viewpoint. Such a setup inherently limits perceptual flexibility: the agent cannot adaptively and dynamically adjust its viewpoint or camera resolution according to the task context. As a result, the absence of adaptive viewpoint selection during long-horizon or fine-grained manipulation prevents current systems from acquiring essential contextual information, thereby undermining the robustness and generalizability of learned policies\n[\nliu2025trivla\n,\nshridhar2023perceiver\n,\n3d-da\n,\ngervet2023act3d\n,\ngoyal2023rvt\n,\ngoyal2024rvt\n]\n. Addressing this limitation is crucial for developing embodied agents capable of adaptive and reliable interaction in complex, real-world environments.\nTo address this limitation, we propose\nActiveVLA\n, a novel vision-language-action framework that explicitly integrates active perception into robotic manipulation. ActiveVLA enables robots to adaptively and dynamically adjust their viewpoint and camera resolution according to the task context, allowing more informative observations to be acquired on demand. The framework equips robots with two complementary capabilities:\n(1)\nActive viewpoint selection\n, which autonomously determines optimal camera perspectives during task execution to maximize visibility and task relevance while minimizing occlusions; and\n(2)\nActive 3D zoom-in\n, which identifies and selectively enhances high-resolution views of task-critical regions within the 3D scene.\nBy dynamically refining its perceptual input through these mechanisms, ActiveVLA enables more precise, context-aware, and reliable action prediction, ultimately achieving superior adaptability and performance in complex, long-horizon manipulation scenarios.\nAs illustrated in Figure\nLABEL:teaser\n, ActiveVLA adopts a coarse-to-fine active perception design that integrates 3D spatial reasoning with vision-language understanding. In the\ncoarse stage\n, ActiveVLA projects the 3D point-cloud observations into multiple 2D orthographic views\n[\ngoyal2023rvt\n,\ngoyal2024rvt\n]\n, aligning them with the pretrained VLM backbone. By leveraging structural priors from the 3D inputs, the model effectively localizes salient and task-relevant regions in 3D space. In the\nfine stage\n, ActiveVLA performs active view selection to determine optimal camera poses centered on these key regions, maximizing amodal relevance and spatial diversity while reducing occlusions. It then applies an active 3D zoom-in strategy to enhance the spatial resolution of these areas, enabling precise manipulation of fine-grained details. This closed-loop, coarse-to-fine perception-action pipeline allows ActiveVLA to dynamically adapt its sensory inputs and maintain high effectiveness across complex, multi-step, and long-horizon manipulation tasks.\nComprehensive experiments validate the advantages of ActiveVLA. On RLBench\n[\njames2020rlbench\n]\n, it achieves an average success rate of 91.8%, with some tasks reaching a perfect success rate of 100%. On COLOSSEUM\n[\npumacay2024colosseum\n]\n, it achieves the highest success rate of 78.3% in challenging generalization scenarios. ActiveVLA consistently outperforms all baselines on GemBench\n[\ngarcia2024towards\n]\n, demonstrating superior adaptability across diverse tasks. Real-world evaluations further confirm its robustness and strong generalization to previously unseen configurations. These results demonstrate that integrating active perception into vision-language-action learning is a key step toward adaptive embodied intelligence.\nThe contributions of this paper are summarized:\nâ€¢\nActive Perception for Vision-Language-Action Models\n: We propose ActiveVLA, a novel vision-language-action framework that equips robots with active perception capabilities, enabling adaptive viewpoint selection and zoom-in mechanisms for precise, fine-grained manipulation.\nâ€¢\nA Novel ActiveVLA Framework\n: ActiveVLA designs a novel coarse-to-fine pipeline that projects 3D point clouds into multi-view 2D representations, predicts key regions via heatmaps, and selectively refines observations for precise, fine-grained manipulation.\nâ€¢\nState-of-the-Art Performance and Real-World Generalization\n: Extensive experiments on RLBench, COLOSSEUM, and GemBench demonstrate that ActiveVLA outperforms state-of-the-art baselines. Real-world robot evaluations show strong generalization and high success rates, highlighting the practical impact of active perception in long-horizon and precision-critical tasks.\n2\nRelated Work\nVision-Language-Action Models.\nRecent research has increasingly focused on developing generalist robot policies trained on large-scale robotic learning datasets\n[\no2023open-x\n,\nkhazatsky2024droid\n,\nfang2023rh20t\n,\ndasari2024ditpolicy\n,\nlin2024datascalinglawsimitation\n]\n. Vision-Language-Action (VLA) models have emerged as a promising approach for training such policies\n[\nkim24openvla\n,\nembodiedcot\n,\n[pi0\n,\nwen2025dexvla\n,\npertsch2025fast\n,\nniu2024llarva\n,\ndiffusion-policy\n,\nzhou2025chatvla\n,\nding2025humanoid\n,\nzhao2025vlas\n,\nding2024quar\n,\ntong2024quart\n,\nliu2025spatial\n,\nwen2024tinyvla\n]\n. VLAs extend vision-language models (VLMs), which are pre-trained on massive internet-scale image and text datasets\n[\nzhu2024mipha\n,\nzhao2024cobra\n,\nzhu2024llavaphi\n,\nkaramcheti2024prismatic\n,\nwang2024qwen2\n,\nlu2024deepseek-vl\n,\nllava\n,\nllava1.5\n,\nabdin2024phi3\n,\nchen2024internvl\n]\n, to the domain of robotic control\n[\nwang2024distrl\n]\n. This approach provides several key advantages: leveraging large-scale vision-language model backbones with billions of parameters enables effective learning from vast robotic datasets, while reusing pre-trained weights from internet-scale data enhances VLAsâ€™ ability to interpret diverse language commands and generalize to novel objects and environments, making them adaptable for real-world robotic applications.\nActiveVLA is a vision-language-action (VLA) framework that employs large-scale vision-language model backbones. Unlike previous methods, ActiveVLA further leverages the spatial structure of 3D inputs and utilizes active perception through novel view synthesis of 3D representations, resulting in greater adaptability and improved performance in manipulation tasks.\nFigure 1\n:\nThe pipeline of ActiveVLA.\nActiveVLA\nis a 3D vision-language-action framework that adopts a two-stage, coarse-to-fine strategy. In the\ncoarse stage\n, three orthographic projections of the 3D scene and a language instruction are processed by the PaliGemma backbone to generate 2D heatmaps, which are then back-projected to locate the most relevant 3D region. In the\nfine stage\n, an active perception module selects new views and performs a 3D zoom-in on this region. The refined PaliGemma then predicts heatmaps for key end-effector positions, while an action decoder outputs the final 3D action.\n3D Modalities in Robotic Learning.\nAlthough VLA models with 2D inputs have been extensively explored, incorporating 3D modalities into robot learning is still an emerging area of research\n[\nzhen20243d\n,\njia2024lift3d\n,\nliu2025reasongrounder\n,\nyang2025fp3\n,\nliu2025neural\n,\nli2025pointvla\n]\n. Recent studies have started leveraging richer spatial information to enhance robot perception and manipulation capabilities. Some approaches, such as 3DVLA\n[\nleo3d\n]\n, present holistic models for unified 3D tasks like generalization, visual question answering, 3D perception, and control. Other methods aim to improve the performance of foundational 2D models by integrating three-dimensional features. Lift3D\n[\njia2024lift3d\n]\naugments 2D vision backbones like DINOv2\n[\noquab2023dinov2\n]\nwith both implicit and explicit 3D representations. PointVLA\n[\nli2025pointvla\n]\nutilizes separate encoders for 2D images and 3D point clouds, merging their features in a downstream action predictor to inform decision-making. SpatialVLA\n[\nqu2025spatialvla\n]\nintroduces a specialized position encoding (Ego3D) to inject spatial information into 2D observations.\nHowever, most existing methods lack perceptual flexibility because the models cannot dynamically adjust their viewpoint or camera resolution. As a result, they fail to leverage the advantages of active, egocentric perception in robotic manipulation.\nIn contrast, ActiveVLA uses an active camera selection strategy to choose optimal viewpoints and an active 3D zoom-in mechanism to enhance point cloud resolution in key regions, leading to better performance in complex, long-horizon tasks.\n3\nOur Proposed ActiveVLA\nVLA Setup.\nVision-Language-Action models aim to learn a generalizable and robust multi-task robot manipulation policy\nÏ€\n\\pi\n.\nÏ€\n\\pi\nthat maps an observation\nğ¨\n\\mathbf{o}\nand a language instruction\nl\nl\nto an action\nğš\n\\mathbf{a}\n:\nÏ€\n:\n(\nğ¨\n,\nl\n)\nâ†¦\nğš\n,\n\\pi:(\\mathbf{o},l)\\mapsto\\mathbf{a},\n(1)\nThe action\nğš\n\\mathbf{a}\nconsists of a 6-DoF end-effector pose\nT\nâˆˆ\nS\nâ€‹\nE\nâ€‹\n(\n3\n)\nT\\in SE(3)\n, a gripper state\ng\nâˆˆ\n{\n0\n,\n1\n}\ng\\in\\{0,1\\}\n, and a collision flag\nc\nâˆˆ\n{\n0\n,\n1\n}\nc\\in\\{0,1\\}\nof the next key frame. This setting assumes access to expert demonstrations\nğ’Ÿ\n=\n{\nÏ„\ni\n}\ni\n=\n1\nN\n\\mathcal{D}=\\{\\tau^{i}\\}_{i=1}^{N}\ncontaining\nN\nN\ntrajectories, where each trajectory contains a language instruction and a sequence of observation-action pairs,\ni.e.\n,\nÏ„\ni\n=\n{\nl\ni\n,\n(\nğ¨\n1\ni\n,\nğš\n1\ni\n)\n,\nâ€¦\n,\n(\nğ¨\nH\ni\n,\nğš\nH\ni\n)\n}\n\\tau^{i}=\\{l^{i},(\\mathbf{o}_{1}^{i},\\mathbf{a}_{1}^{i}),...,(\\mathbf{o}_{H}^{i},\\mathbf{a}_{H}^{i})\\}\n.\nThe observation\nğ¨\n\\mathbf{o}\ncontains RGB-D images captured from one or multiple fixed, end-effector-centric viewpoints. However, this setup limits perceptual flexibility by preventing dynamic viewpoint or resolution adjustment, hindering context acquisition and policy generalization.\nActiveVLA.\nAs shown in Figure\n1\n, ActiveVLA addresses the limitation of perceptual flexibility through a coarse-to-fine active perception framework that adaptively adjusts the cameraâ€™s viewpoint and zoom-in level according to the task context, thereby enhancing scene exploration and facilitating fine-grained interaction. By integrating 3D spatial reasoning with vision-language understanding, our approach progressively refines perceptual focus from a broad overview to task-critical regions, enhancing both contextual awareness and generalization.\nActiveVLA enables robots to attend to critical 3D regions and actively sense the environment, allowing adaptive viewpoints and zoom-in for fine-grained manipulation.\n3.1\n3D Crucial Area Perception\nDuring each task, ActiveVLA first identifies the core region in the 3D scene to serve as the focus for active perception.\nMulti-View Rendering.\nGiven RGB-D images as visual input, ActiveVLA reconstructs a point cloud of the scene using images captured by calibrated cameras. To match the 2D image input required by the VLM backbone, we render three orthographic projection images of the point cloud from top, front, and right viewpoints. Specifically, for each view, we render three image maps with a total of 7 channels: RGB, depth, and the coordinates of the points in the world frame (3 channels). The coordinates help establish the correspondence of pixels across views,\ni.e.\n, if pixels from different views share the same\n(\nx\n,\ny\n,\nz\n)\n(x,y,z)\n, they correspond to the same point in 3D. We use PyTorch3D\n[\nravi2020pytorch3d\n]\nfor rendering. The rendered image at viewpoint\nv\nv\nis given by:\nI\n(\nv\n)\nâ€‹\n(\nu\nx\n,\nu\ny\n)\n=\nâˆ‘\ni\n=\n1\nN\nğœ\ni\nâ‹…\nÎ´\nâ€‹\n(\n(\nu\nx\n,\nu\ny\n)\nâˆ’\nÏ€\n(\nv\n)\nâ€‹\n(\nğ©\ni\n)\n)\n,\nI^{(v)}(u_{x},u_{y})=\\sum_{i=1}^{N}\\mathbf{c}_{i}\\cdot\\delta\\big((u_{x},u_{y})-\\pi^{(v)}(\\mathbf{p}_{i})\\big),\n(2)\nwhere\nÏ€\n(\nv\n)\nâ€‹\n(\nâ‹…\n)\n\\pi^{(v)}(\\cdot)\ndenotes the orthographic projection. The rendered color at each pixel is taken from the point with the minimal depth\nz\ni\n(\nv\n)\nz_{i}^{(v)}\namong all points projecting to that pixel, ensuring correct occlusion handling.\n3D Crucial Area Extraction.\nThese images are then used as input for the VLM backbone, which is designed to process images along with accompanying text prompts. Since the VLMâ€™s global representations are not sufficient for precise spatial localization, we introduce a heatmap prediction module to recover fine-grained spatial attention and identify object positions within each view. To predict the heatmap, we first rearrange the output patch tokens\n{\nğ­\ni\n}\ni\n=\n1\nM\n\\{\\mathbf{t}_{i}\\}_{i=1}^{M}\naccording to their spatial positions to form a feature grid. A convex upsampling block is then applied to the feature grid to obtain a heatmap that matches the resolution of the input image. This process can be expressed as:\nğ‡\n=\nğ’°\nâ€‹\n(\nRearrange\nâ€‹\n(\n{\nğ­\ni\n}\ni\n=\n1\nM\n)\n)\n,\n\\mathbf{H}=\\mathcal{U}\\Big(\\mathrm{Rearrange}\\big(\\{\\mathbf{t}_{i}\\}_{i=1}^{M}\\big)\\Big),\n(3)\nwhere\nğ’°\nâ€‹\n(\nâ‹…\n)\n\\mathcal{U}(\\cdot)\ndenotes the convex upsampling block, and\nRearrange\nâ€‹\n(\nâ‹…\n)\n\\mathrm{Rearrange}(\\cdot)\nrearranges the tokens into a feature grid of size\nH\np\nÃ—\nW\np\nH_{p}\\times W_{p}\n.\nUnlike fixed interpolation methods, the upsampling module learns pixel-wise weights for finer spatial detail recovery. The pipeline is trained with cross-entropy loss to predict heatmaps, which are then back-projected from all views to identify the crucial 3D region.\n3.2\n3D Active Perception\nActiveVLA enables active perception, allowing robots to perform fine-grained manipulation. It includes two components: Active Viewpoint Selection and Active 3D Zoom-in.\nActive Viewpoint Selection.\nTo improve perceptual completeness in complex scenes, ActiveVLA proposes a hypothesis testing strategy for selecting active views that focuses on optimal camera viewpoints centered on key regions identified during the coarse stage. The goal is to maximize amodal relevance by ensuring full visibility of target objects and increasing view diversity. Together, these objectives help reduce occlusion and perceptual ambiguity through complementary observations.\nGiven the 3D key region of interest\np\nf\nâˆˆ\nâ„\n3\np_{f}\\in\\mathbb{R}^{3}\n(e.g., the centroid of a partially observed object), we generate a set of candidate camera positions uniformly distributed around a sphere centered at\np\nf\np_{f}\n.\nTo achieve isotropic coverage, we employ a geodesic sampling strategy based on recursive subdivision of a regular icosahedron, producing a near-uniform point distribution on the spherical surface. This avoids sampling biases inherent in latitude-longitude parameterizations and provides scalable control over viewpoint density via subdivision level.\nThe total number of sampling points\nV\nâ€‹\n(\nk\n)\nV(k)\nafter\nk\nk\n-level recursive subdivision (where\nk\nâˆˆ\nâ„•\nk\\in\\mathbb{N}\n, a non-negative integer) is quantitatively described as:\nV\nâ€‹\n(\nk\n)\n=\n12\n+\n30\nâ€‹\nk\n+\n20\n3\nâ€‹\n(\n4\nk\nâˆ’\n1\n)\n,\nV(k)=12+30k+\\frac{20}{3}\\left(4^{k}-1\\right),\n(4)\nwhere\nk\n=\n0\nk=0\ncorresponds to the original icosahedron (12 vertices), and higher\nk\nk\nindicates successive subdivisions. Each candidate camera position\nc\ni\nc_{i}\nis evaluated using a multi-objective scoring function that balances three criteria:\nâ€¢\nVisibility\n: ActiveVLA determines whether the line of sight from\nc\ni\nc_{i}\nto\np\nf\np_{f}\nis occluded by scene geometry. Using a KDTree-based nearest-neighbor search on the observed point cloud\nğ’®\n\\mathcal{S}\n, it samples\nN\nN\nuniformly spaced points\n{\nq\nk\n}\n\\{q_{k}\\}\nalong the ray and computes their nearest-surface distances:\nd\nk\n=\nmin\ns\nâˆˆ\nğ’®\nâ¡\nâ€–\nq\nk\nâˆ’\ns\nâ€–\n.\nd_{k}=\\min_{s\\in\\mathcal{S}}\\|q_{k}-s\\|.\n(5)\nIf all distances exceed a threshold\nr\nr\n(\nd\nk\nâ‰¥\nr\n,\nâˆ€\nk\nd_{k}\\geq r,\\,\\forall k\n), the view is marked unoccluded with\nv\nâ€‹\n(\nc\ni\n,\np\nf\n)\n=\n1\nv(c_{i},p_{f})=1\n; otherwise,\nv\nâ€‹\n(\nc\ni\n,\np\nf\n)\n=\n0\nv(c_{i},p_{f})=0\n.\nâ€¢\nDistance\n: We normalize the distance\nâ€–\nc\ni\nâˆ’\np\nf\nâ€–\n\\|c_{i}-p_{f}\\|\nand standardize it across candidates to prefer moderate viewing ranges that balance field-of-view and detail resolution.\nâ€¢\nDiversity\n:\nTo ensure geometric diversity among selected viewpoints, we compute the total angular separation between the viewing direction of\nc\ni\nc_{i}\nand all other candidates. Given candidate set\nğ’\n=\n{\nc\n1\n,\nâ€¦\n,\nc\nM\n}\n\\mathcal{C}=\\{c_{1},\\dots,c_{M}\\}\nand unit viewing vectors\nğ¯\ni\nâˆˆ\nğ•Š\n2\n\\mathbf{v}_{i}\\in\\mathbb{S}^{2}\n, the diversity score is defined as:\nS\ndiv\nâ€‹\n(\nc\ni\n)\n=\nâˆ‘\nj\nâ‰ \ni\narccos\nâ¡\n(\nğ¯\ni\nâ‹…\nğ¯\nj\n)\n,\nS_{\\text{div}}(c_{i})=\\sum_{j\\neq i}\\arccos(\\mathbf{v}_{i}\\cdot\\mathbf{v}_{j}),\n(6)\nA larger\nS\ndiv\nâ€‹\n(\nc\ni\n)\nS_{\\text{div}}(c_{i})\nindicates greater angular diversity and more spatially distributed views.\nThese scores are Z-normalized and combined into a unified score:\ns\ni\n=\nw\nvis\nâ‹…\ns\nvis\n+\nw\ndis\nâ‹…\ns\ndis\n+\nw\ndiv\nâ‹…\ns\ndiv\n,\ns_{i}=w_{\\text{vis}}\\cdot s_{\\text{vis}}+w_{\\text{dis}}\\cdot s_{\\text{dis}}+w_{\\text{div}}\\cdot s_{\\text{div}},\n(7)\nwhere\nw\nvis\n+\nw\ndis\n+\nw\ndiv\n=\n1\nw_{\\text{vis}}+w_{\\text{dis}}+w_{\\text{div}}=1\nare the weighting coefficients,\ns\nvis\ns_{\\text{vis}}\ndenotes the visibility score,\ns\ndis\ns_{\\text{dis}}\ndenotes the distance score, and\ns\ndiv\ns_{\\text{div}}\ndenotes the diversity score. The top-\nK\nK\nhighest-scoring views are selected as the next observation poses. Each camera is configured using the look-at formulation with eye\nc\ni\nc_{i}\n, target\np\nf\np_{f}\n, and a dynamically adjusted up vector. These views enable informative, unoccluded observations that support robust multi-view reasoning and precise manipulation, with the most informative view serving as the basis for active 3D zoom-in.\nActive 3D Zoom-in.\nPrevious VLA models rely on fixed camera views around the robot, which often fail to capture sufficient detail for fine-grained tasks involving small objects (e.g.,\nwelding a hole using a welding gun\n).\nTo overcome this limitation, ActiveVLA introduces an active 3D zoom-in mechanism that adaptively refines visual perception around key interaction regions. After selecting the optimal viewpoint, the system re-renders the scene from the same camera pose with a narrowed field of view, effectively magnifying the local region while maintaining high pixel resolution for precise gripper pose prediction. This simulates an optical zoom effect in virtual rendering space, enabling detailed observation of small-scale structures without loss of visual fidelity. Let\nÎ±\n\\alpha\ndenote the original FoV (in radians),\nz\n>\n1\nz>1\nthe zoom-in factor, and\nd\nd\nthe distance from the camera to the region of interest. The spatial coverage width\nW\nW\nof the rendered image (perpendicular to the viewing direction) is given by:\nW\nâ€‹\n(\nz\n)\n=\n2\nâ€‹\nd\nâ€‹\ntan\nâ¡\n(\nÎ±\n2\nâ€‹\nz\n)\n,\nW(z)=2d\\tan\\left(\\frac{\\alpha}{2z}\\right),\n(8)\nwhere\nW\nâ€‹\n(\nz\n)\nW(z)\ndecreases with increasing\nz\nz\n, while pixel resolution\nR\nR\nis preserved as\nR\n=\nimage width (pixels)\nW\nâ€‹\n(\nz\n)\nR=\\frac{\\text{image width (pixels)}}{W(z)}\n, ensuring higher detail in the magnified region.\nThe zoom-in process leverages the 3D point cloud for scale-invariant view synthesis without geometric loss. Unlike fixed physical cameras, the virtual renderer produces high-resolution close-ups based on local 3D structure, enhancing gripper pose accuracy. By separating exploration (view selection) from exploitation (zoom-in), ActiveVLA forms a hierarchical perception strategy that improves precision manipulation and underscores the benefits of adaptive observation in simulation-to-real VLA systems.\nTable 1:\nResults on RLBench.\nâ€œAvg. Rankâ€ denotes the average rank across all 18 tasks, where a lower value signifies better overall performance. ActiveVLA attains first place in 10 tasks, highlighting its dominance in the benchmark.\nModels\nAvg.\nSR (%)\nâ†‘\n\\uparrow\nAvg.\nRank\nâ†“\n\\downarrow\nClose\nJar\nDrag\nStick\nInsert\nPeg\nMeat off\nGrill\nOpen\nDrawer\nPlace\nCups\nPlace\nWine\nPush\nButtons\nImage-BC (CNN)\n[\njang2022bc\n,\nshridhar2023perceiver\n]\n1.3\n11.56\n0.0\n0.0\n0.0\n0.0\n0.0\n4.0\n0.0\n0.0\nImage-BC (ViT)\n[\njang2022bc\n,\nshridhar2023perceiver\n]\n1.3\n11.61\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nC2F-ARM-BC\n[\njames2022coarse\n,\nshridhar2023perceiver\n]\n20.1\n9.89\n24.0\n24.0\n4.0\n20.0\n20.0\n0.0\n8.0\n72.0\nHiveFormer\n[\nguhur2023instruction\n]\n45.3\n8.22\n52.0\n76.0\n0.0\n100.0\n52.0\n0.0\n80.0\n84.0\nPolarNet\n[\nchen2023polarnet\n]\n46.4\n7.78\n36.0\n92.0\n4.0\n100.0\n84.0\n0.0\n40.0\n96.0\nPerAct\n[\njaegleperceiver\n]\n49.4\n7.33\n55.2\nÂ±\n\\pm\n4.7\n89.6\nÂ±\n\\pm\n4.1\n5.6\nÂ±\n\\pm\n4.1\n70.4\nÂ±\n\\pm\n2.0\n88.0\nÂ±\n\\pm\n5.7\n2.4\nÂ±\n\\pm\n3.2\n44.8\nÂ±\n\\pm\n7.8\n92.8\nÂ±\n\\pm\n3.0\nAct3D\n[\ngervet2023act3d\n]\n65.0\n5.28\n92.0\n92.0\n27.0\n94.0\n93.0\n3.0\n80.0\n99.0\nRVT\n[\ngoyal2023rvt\n]\n62.9\n5.39\n52.0\nÂ±\n\\pm\n2.5\n99.2\nÂ±\n\\pm\n1.6\n11.2\nÂ±\n\\pm\n3.0\n88.0\nÂ±\n\\pm\n2.5\n71.2\nÂ±\n\\pm\n6.9\n4.0\nÂ±\n\\pm\n2.5\n91.0\nÂ±\n\\pm\n5.2\n100.0\nÂ±\n\\pm\n0.0\n3D Diffuser Actor\n[\n3d-da\n]\n81.3\n3.39\n96.0\nÂ±\n\\pm\n2.5\n100.0\nÂ±\n\\pm\n0.0\n65.6\nÂ±\n\\pm\n4.1\n96.8\nÂ±\n\\pm\n1.6\n89.6\nÂ±\n\\pm\n4.1\n24.0\nÂ±\n\\pm\n7.6\n93.6\nÂ±\n\\pm\n4.8\n98.4\nÂ±\n\\pm\n2.0\nRVT-2\n[\ngoyal2024rvt\n]\n81.4\n3.00\n100.0\nÂ±\n\\pm\n0.0\n99.0\nÂ±\n\\pm\n1.7\n40.0\nÂ±\n\\pm\n0.0\n99.0\nÂ±\n\\pm\n1.7\n74.0\nÂ±\n\\pm\n11.8\n38.0\nÂ±\n\\pm\n4.5\n95.0\nÂ±\n\\pm\n3.3\n100.0\nÂ±\n\\pm\n0.0\nBridgeVLA\n[\nli2025bridgevla\n]\n88.2\n2.44\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n88.0\nÂ±\n\\pm\n2.8\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n58.4\nÂ±\n\\pm\n10.0\n88.0\nÂ±\n\\pm\n2.8\n98.4\nÂ±\n\\pm\n2.2\nActiveVLA (Ours)\n91.8\n1.22\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n92.4\nÂ±\n\\pm\n1.9\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n65.6\nÂ±\n\\pm\n3.2\n96.2\nÂ±\n\\pm\n1.4\n100.0\nÂ±\n\\pm\n0.0\nModels\nPut in\nCupboard\nPut in\nDrawer\nPut in\nSafe\nScrew\nBulb\nSlide\nBlock\nSort\nShape\nStack\nBlocks\nStack\nCups\nSweep to\nDustpan\nTurn\nTap\nImage-BC (CNN)\n[\njang2022bc\n,\nshridhar2023perceiver\n]\n0.0\n8.0\n4.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n8.0\nImage-BC (ViT)\n[\njang2022bc\n,\nshridhar2023perceiver\n]\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n16.0\nC2F-ARM-BC\n[\njames2022coarse\n,\nshridhar2023perceiver\n]\n0.0\n4.0\n12.0\n8.0\n16.0\n8.0\n0.0\n0.0\n0.0\n68.0\nHiveFormer\n[\nguhur2023instruction\n]\n32.0\n68.0\n76.0\n8.0\n64.0\n8.0\n8.0\n0.0\n28.0\n80.0\nPolarNet\n[\nchen2023polarnet\n]\n12.0\n32.0\n84.0\n44.0\n56.0\n12.0\n4.0\n8.0\n52.0\n80.0\nPerAct\n[\njaegleperceiver\n]\n28.0\nÂ±\n\\pm\n4.4\n51.2\nÂ±\n\\pm\n4.7\n84.0\nÂ±\n\\pm\n3.6\n17.6\nÂ±\n\\pm\n2.0\n74.0\nÂ±\n\\pm\n13.0\n16.8\nÂ±\n\\pm\n4.7\n26.4\nÂ±\n\\pm\n3.2\n2.4\nÂ±\n\\pm\n2.0\n52.0\nÂ±\n\\pm\n0.0\n88.0\nÂ±\n\\pm\n4.4\nAct3D\n[\ngervet2023act3d\n]\n51.0\n90.0\n95.0\n47.0\n93.0\n8.0\n12.0\n9.0\n92.0\n94.0\nRVT\n[\ngoyal2023rvt\n]\n49.6\nÂ±\n\\pm\n3.2\n88.0\nÂ±\n\\pm\n5.7\n91.2\nÂ±\n\\pm\n3.0\n48.0\nÂ±\n\\pm\n5.7\n81.6\nÂ±\n\\pm\n5.4\n36.0\nÂ±\n\\pm\n2.5\n28.8\nÂ±\n\\pm\n3.9\n26.4\nÂ±\n\\pm\n8.2\n72.0\nÂ±\n\\pm\n0.0\n93.6\nÂ±\n\\pm\n4.1\n3D Diffuser Actor\n[\n3d-da\n]\n85.6\nÂ±\n\\pm\n4.1\n96.0\nÂ±\n\\pm\n3.6\n97.6\nÂ±\n\\pm\n2.0\n82.4\nÂ±\n\\pm\n2.0\n97.6\nÂ±\n\\pm\n3.2\n44.0\nÂ±\n\\pm\n4.4\n68.3\nÂ±\n\\pm\n3.3\n47.2\nÂ±\n\\pm\n8.5\n84.0\nÂ±\n\\pm\n4.4\n99.2\nÂ±\n\\pm\n1.6\nRVT-2\n[\ngoyal2024rvt\n]\n66.0\nÂ±\n\\pm\n4.5\n96.0\nÂ±\n\\pm\n0.0\n96.0\nÂ±\n\\pm\n2.8\n88.0\nÂ±\n\\pm\n4.9\n92.0\nÂ±\n\\pm\n2.8\n35.0\nÂ±\n\\pm\n7.1\n80.0\nÂ±\n\\pm\n2.8\n69.0\nÂ±\n\\pm\n5.9\n100.0\nÂ±\n\\pm\n0.0\n99.0\nÂ±\n\\pm\n1.7\nBridgeVLA\n[\nli2025bridgevla\n]\n73.6\nÂ±\n\\pm\n4.6\n99.2\nÂ±\n\\pm\n1.8\n99.2\nÂ±\n\\pm\n1.8\n87.2\nÂ±\n\\pm\n6.6\n96.0\nÂ±\n\\pm\n2.8\n60.8\nÂ±\n\\pm\n7.7\n76.8\nÂ±\n\\pm\n8.7\n81.6\nÂ±\n\\pm\n3.6\n87.2\nÂ±\n\\pm\n1.8\n92.8\nÂ±\n\\pm\n3.3\nActiveVLA (Ours)\n87.4\nÂ±\n\\pm\n3.4\n99.5\nÂ±\n\\pm\n2.1\n99.4\nÂ±\n\\pm\n1.6\n89.3\nÂ±\n\\pm\n5.4\n98.5\nÂ±\n\\pm\n3.7\n63.3\nÂ±\n\\pm\n5.3\n82.2\nÂ±\n\\pm\n4.7\n84.8\nÂ±\n\\pm\n2.1\n100.0\nÂ±\n\\pm\n0.0\n95.3\nÂ±\n\\pm\n2.8\n3.3\n3D Action Prediction\nAfter obtaining the actively selected and zoom-in views, we feed them into the VLM to generate attention heatmaps. For translational prediction, these 2D heatmaps are back-projected into the 3D workspace and accumulated on a discretized grid\nğ’¢\n=\n{\nğ \n1\n,\nâ€¦\n,\nğ \nN\n}\n\\mathcal{G}=\\{\\mathbf{g}_{1},\\dots,\\mathbf{g}_{N}\\}\nto form a multi-view score volume:\nS\nâ€‹\n(\nğ \n)\n=\nâˆ‘\nv\n=\n1\n3\nw\nv\nâ€‹\nh\nv\nâ€‹\n(\nÏ€\nv\nâ€‹\n(\nğ \n)\n)\n,\nS(\\mathbf{g})=\\sum_{v=1}^{3}w_{v}\\,h_{v}\\!\\left(\\pi_{v}(\\mathbf{g})\\right),\n(9)\nwhere\nh\nv\nh_{v}\nis the attention heatmap of view\nv\nv\n,\nÏ€\nv\nâ€‹\n(\nğ \n)\n\\pi_{v}(\\mathbf{g})\nis the 2D projection of grid point\nğ \n\\mathbf{g}\n, and\nw\nv\nw_{v}\ndenotes the view weight. The translation target is determined as\nğ­\nâˆ—\n=\narg\nâ¡\nmax\nğ \nâˆˆ\nğ’¢\nâ¡\nS\nâ€‹\n(\nğ \n)\n\\mathbf{t}^{*}=\\arg\\max_{\\mathbf{g}\\in\\mathcal{G}}S(\\mathbf{g})\n.\nFor rotation prediction, ActiveVLA represents orientation using Euler angles\n(\nÏ•\n,\nÎ¸\n,\nÏˆ\n)\n(\\phi,\\theta,\\psi)\n, each discretized into 72 bins. A hierarchical feature fusion module then integrates global and local context to predict rotation, gripper state, and a binary collision flag.\nâ€¢\nGlobal Context Encoding\n: We perform max-pooling over the vision encoder outputs of each orthographic projection to obtain a global feature vector per view, yielding three global tokens that capture overall scene semantics.\nâ€¢\nLocal Context Encoding\n: For fine-grained reasoning, we use an ROI-aware sampler to extract local tokens, encoding detailed appearance and geometry.\nAll tokens are concatenated and passed through an MLP head to predict rotation, gripper, and collision actions. This global-local fusion allows the model to combine overall scene understanding with fine spatial precision, enabling accurate and safe manipulation in complex environments.\nTable 2:\nResults on the COLOSSEUM Benchmark.\nThe table presents performance across 14 generalization scenarios. â€Avg. Rankâ€ indicates the mean ranking of each method over all perturbations, with lower values reflecting stronger overall performance.\nActiveVLA surpasses the current best method by 1.9 percentage points in average success rate, demonstrating improved robustness and generalization.\nModels\nAvg. SR (%)\nâ†‘\n\\uparrow\nAvg. Rank\nâ†“\n\\downarrow\nAll Perturbations\nMO-COLOR\nRO-COLOR\nMO-TEXTURE\nRO-TEXTURE\nMO-SIZE\nR3M-MLP\n[\nnair2022r3m\n]\n0.8\n6.57\n0.6\n0.4\n0.0\n0.0\n0.0\n1.8\nMVP-MLP\n[\nxiao2022masked\n]\n1.6\n6.36\n0.8\n1.2\n0.0\n0.4\n0.0\n4.44\nPerAct\n[\njaegleperceiver\n]\n27.9\n4.79\n7.2\n24.0\n29.2\n28.8\n17.71\n35.6\nRVT\n[\ngoyal2023rvt\n]\n35.4\n4.36\n6.4\n26.0\n31.3\n44.8\n41.1\n35.3\nRVT-2\n[\ngoyal2024rvt\n]\n56.7\n2.86\n15.6\nÂ±\n\\pm\n0.8\n53.0\nÂ±\n\\pm\n0.9\n54.6\nÂ±\n\\pm\n0.6\n59.7\nÂ±\n\\pm\n0.7\n56.7\nÂ±\n\\pm\n1.4\n60.9\nÂ±\n\\pm\n0.9\nBridgeVLA\n[\nli2025bridgevla\n]\n64.0\n2.07\n18.7\nÂ±\n\\pm\n2.2\n60.5\nÂ±\n\\pm\n1.1\n63.8\nÂ±\n\\pm\n0.1\n63.5\nÂ±\n\\pm\n1.5\n68.4\nÂ±\n\\pm\n3.3\n69.3\nÂ±\n\\pm\n1.0\nActiveVLA (Ours)\n65.9\n1.07\n21.4\nÂ±\n\\pm\n1.8\n64.2\nÂ±\n\\pm\n1.5\n64.4\nÂ±\n\\pm\n1.2\n65.7\nÂ±\n\\pm\n2.1\n69.3\nÂ±\n\\pm\n2.6\n72.4\nÂ±\n\\pm\n0.8\nModels\nRO-SIZE\nLight Color\nTable Color\nTable Texture\nDistractor\nBackground Texture\nRLBench\nCamera Pose\nR3M-MLP\n[\nnair2022r3m\n]\n0.0\n1.0\n1.4\n0.2\n1.6\n1.2\n2.0\n0.8\nMVP-MLP\n[\nxiao2022masked\n]\n0.0\n1.6\n1.6\n1.0\n3.8\n2.2\n2.0\n2.6\nPerAct\n[\njaegleperceiver\n]\n29.3\n29.1\n30.4\n23.2\n27.1\n33.5\n39.4\n36.3\nRVT\n[\ngoyal2023rvt\n]\n40.5\n34.0\n30.0\n45.2\n18.8\n46.4\n53.4\n42.2\nRVT-2\n[\ngoyal2024rvt\n]\n53.4\nÂ±\n\\pm\n1.5\n58.0\nÂ±\n\\pm\n1.1\n62.6\nÂ±\n\\pm\n0.9\n56.6\nÂ±\n\\pm\n0.9\n60.8\nÂ±\n\\pm\n0.5\n68.7\nÂ±\n\\pm\n1.1\n68.8\nÂ±\n\\pm\n1.3\n64.4\nÂ±\n\\pm\n0.5\nBridgeVLA\n[\nli2025bridgevla\n]\n61.7\nÂ±\n\\pm\n0.8\n69.7\nÂ±\n\\pm\n1.2\n75.7\nÂ±\n\\pm\n0.9\n71.3\nÂ±\n\\pm\n0.7\n51.8\nÂ±\n\\pm\n1.5\n74.8\nÂ±\n\\pm\n1.0\n73.1\nÂ±\n\\pm\n0.2\n73.8\nÂ±\n\\pm\n0.3\nActiveVLA (Ours)\n64.4\nÂ±\n\\pm\n1.3\n70.4\nÂ±\n\\pm\n1.4\n78.3\nÂ±\n\\pm\n1.1\n72.5\nÂ±\n\\pm\n0.9\n54.3\nÂ±\n\\pm\n1.2\n75.2\nÂ±\n\\pm\n0.6\n74.4\nÂ±\n\\pm\n0.6\n76.3\nÂ±\n\\pm\n1.1\nFigure 2\n:\nQualitative results of fine-grained manipulation tasks.\nLeft of the dotted line (coarse stage): (a) project 3D modalities onto orthographic images, then (b) predict heatmaps to mark critical regions. Right of the dotted line (fine stage): using these regions, perform (c) active view selection and (d) active 3D zoom-in for fine-grained manipulation in complex scenes.\n4\nExperiments\nSimulation Benchmarks.\nWe evaluate ActiveVLA on three simulation benchmarks for long-horizon and fine-grained manipulation.\nRLBench\n[\njames2020rlbench\n]\nfeatures 18 tasks using a Franka Panda robot with RGB-D inputs from four calibrated cameras and 100 demonstrations per task.\nCOLOSSEUM\n[\npumacay2024colosseum\n]\nextends RLBench with 12 perturbation types involving object, scene, and camera variations for robustness evaluation.\nGemBench\n[\ngarcia2024towards\n]\nfurther builds on RLBench as a hierarchical benchmark with 16 training and 44 testing tasks across seven core action primitives, assessing compositionality and generalization.\nImplementation Details.\nOur ActiveVLA adopts the pre-trained VLM backbone from BridgeVLA\n[\nli2025bridgevla\n]\n, built on PaliGemma\n[\nbeyer2024paligemma\n]\nwith a SigLIP encoder\n[\nsiglip\n]\nand Gemma decoder\n[\nteam2024gemma\n]\n, pre-trained on a 120K-image RoboPoint subset\n[\nyuan2024robopoint\n]\n.\nReal-world experiments are conducted on a KINOVA GEN2 robot with a RealSense D455 camera in an eye-to-hand setup, covering occlusion-rich manipulation tasks.\nAll experiments run on eight NVIDIA H100 GPUs and a 192-vCPU Intel Xeon Platinum 8468 system.\nBaselines.\nWe compare ActiveVLA with state-of-the-art baselines.\nImage-BC (CNN/ViT)\n[\njang2022bc\n]\nperforms 2D behavior cloning with convolutional or transformer backbones.\nC2F-ARM-BC\n[\njames2022coarse\n]\nand PerAct\n[\nshridhar2023perceiver\n]\nadopt voxel-based coarse-to-fine and Perceiver Transformer policies, respectively.\nHiveFormer integrates historical observations via a multimodal transformer, while PolarNet\n[\nqian2022pointnext\n]\nencodes 3D scenes to predict heatmaps and offsets.\nAct3D\n[\ngervet2023act3d\n]\nsamples and ranks 3D points for action selection, and 3D Diffuser Actor\n[\n3d-da\n]\nmodels 3D trajectories via diffusion.\nRVT\n[\ngoyal2023rvt\n]\nand RVT-2\n[\ngoyal2024rvt\n]\naggregate multi-view projections in a coarse-to-fine transformer framework, whereas BridgeVLA\n[\nli2025bridgevla\n]\naligns 2D heatmaps for efficient 3D vision-language-action learning.\n4.1\nExperimental Results\nResults on RLBench.\nWe evaluate ActiveVLA over five trials for statistical reliability, with results shown in Table\n1\n.\nActiveVLA achieves a new state of the art on RLBench with a 91.8% average success rate and an average rank of 1.22.\nIt performs exceptionally well in precision-demanding and contact-rich tasks such as\nInsert Peg\nand\nOpen Drawer\n, and remains robust even under occlusions (e.g.,\nPlace Cups\n, 65.6%).\nAcross diverse task types including placement, assembly, and cleaning, ActiveVLA maintains consistent high performance, demonstrating strong generalization in vision-based robotic manipulation. The qualitative results for each stage of active perception are presented in Figure\n2\n.\nResults on COLOSSEUM.\nResults in Table\n2\nshow that ActiveVLA achieves a new state of the art on COLOSSEUM, with an average success rate of 65.9% and an average rank of 1.07, outperforming all previous methods. It remains robust to variations in object size, color, lighting, and texture, obtaining 72.4% on MO-SIZE and 64.4% on RO-SIZE. The best performance is observed on Table Color (78.3%) and Camera Pose (76.3%), where it maintains high accuracy despite clutter, distractors, and viewpoint changes. Overall, ActiveVLA surpasses BridgeVLA in most categories, confirming its stronger visual generalization and invariant representation learning capability.\nTable 3:\nPerformance on the GemBench benchmark.\nResults are reported as mean success rates without confidence intervals.\nMethod\nAverage\nL1\nL2\nL3\nL4\nHiveformer\n[\nguhur2023instruction\n]\n30.4\n60.3\n26.1\n35.1\n0.0\nPolarNet\n[\nchen2023polarnet\n]\n38.4\n77.7\n37.1\n38.5\n0.1\n3D Diffuser Actor\n[\n3d-da\n]\n44.0\n91.9\n43.4\n37.0\n0.0\nRVT-2\n[\ngoyal2024rvt\n]\n44.0\n89.1\n51.0\n36.0\n0.0\n3D-LOTUS\n[\ngarcia25gembench\n]\n45.7\n94.3\n49.9\n38.1\n0.3\n3D-LOTUS++\n[\ngarcia25gembench\n]\n48.0\n68.7\n64.5\n41.5\n17.4\nBridgeVLA\n[\nli2025bridgevla\n]\n50.0\n91.1\n65.0\n43.8\n0.0\nActiveVLA (Ours)\n51.3\n92.4\n66.3\n45.1\n1.2\nFigure 3\n:\nVisualization of ActiveVLA in complex manipulation tasks\n. It actively perceives and precisely completes tasks despite severe occlusions and complex spatial structures.\nResults on GemBench.\nAs shown in Table\n3\n, ActiveVLA achieves the best performance across core levels L1â€“L3, with success rates of 92.4%, 66.3%, and 45.1%, surpassing baselines such as 3D-LOTUS++ and BridgeVLA.\nIt attains an overall average of 51.3%, improving upon the previous state of the art by 1.3 percentage points.\nAlthough performance decreases on the most difficult L4 tasks (1.2%), ActiveVLA still demonstrates promising long-horizon reasoning and strong 3D perception for precise manipulation.\nReal-Robot Analysis.\nTo evaluate the real-world performance of ActiveVLA, we conduct a series of real-world manipulation experiments under complex and highly occluded scenarios. The tasks involve diverse spatial configurations, such as picking objects from cluttered scenes, retrieving partially hidden items, and manipulating objects with intricate occlusion relationships. As illustrated in Figure\n3\n, ActiveVLA demonstrates strong active perception and precise manipulation capabilities. It effectively infers object geometry and spatial relations through 3D perception, actively selects informative viewpoints, and executes fine-grained actions to complete the tasks successfully. Even when key targets are heavily occluded by other objects or environmental structures, ActiveVLA maintains high accuracy and stability, reflecting its robust spatial understanding and adaptive control in real-world conditions.\n4.2\nAblation Study\nTable 4:\nAblation study on key components.\nWe report the success rate (%) and inference time (s) over 100 trials. A-VS (Active View Selection) dynamically acquires informative views, while A-3Z (Active 3D Zoom-in) refines local focus.\nComponent\nPerformance\nA-VS\nA-3Z\nRLBench\nCOLOSSEUM\nGemBench\n87.6/0.26\n63.6/0.33\n48.9/0.21\nâœ”\n89.4/0.45\n64.5/0.51\n49.4/0.48\nâœ”\nâœ”\n91.8/0.53\n65.9/0.62\n51.3/0.59\nComponent Analysis.\nWe ablate the core active perception modules of ActiveVLA, Active View Selection (A-VS) and Active 3D Zoom-in (A-3Z), on an NVIDIA H100 GPU across RLBench, COLOSSEUM, and GemBench.\nThe fixed-view baseline achieves 87.6% success in 0.26â€‰s per trial.\nAdding A-VS dynamically selects informative views, raising performance to 89.4% at 0.45â€‰s by improving scene coverage and reducing occlusion.\nFurther introducing A-3Z enables virtual optical zoom for high-resolution close-ups, achieving 91.8% success at 0.53â€‰s.\nThese consistent gains confirm that A-VS guides\nwhere\nto look and A-3Z\nhow closely\nto observe, forming a hierarchical perception strategy for precise manipulation.\nFigure 4\n:\nSuccess rates of ActiveVLA under different hyperparameters\n: (a) Number of selected views; (b) Active 3D zoom-in factor. Experiments are evaluated on the RLBench benchmark.\nHyperparameter Analysis.\nWe analyze the effects of the\nNumber of Views\nand\nZoom-in Factor\non ActiveVLA.\nAs shown in Figure\n4\n, increasing the number of views improves the success rate from 82.2% (one view) to 91.8% (three views), confirming that multi-view observation enhances spatial understanding and mitigates occlusion.\nBeyond three views, the performance saturates while computational cost increases.\nSimilarly, increasing the zoom-in factor from 1 to 4 boosts precision by enabling finer-grained perception, whereas excessive zoom reduces contextual awareness.\nTherefore, we set the number of views to 3 and the zoom-in factor to 4 for all experiments, striking a balance between global context and local detail.\n5\nConclusion\nThis paper presents ActiveVLA, a vision-language-action framework that integrates active perception into robotic manipulation.\nBy incorporating Active View Selection and Active 3D Zoom-in within a coarse-to-fine architecture, ActiveVLA enables robots to dynamically adapt their observation strategies for enhanced spatial reasoning and fine-grained control.\nBy leveraging multi-view 3D perception aligned with VLMs, ActiveVLA effectively balances global context and fine-grained local detail during manipulation.\nExperiments in both simulated and real-world environments demonstrate its superior performance across diverse tasks.\nReferences\n\\thetitle\nSupplementary Material\n1\nVideo\nThe attached video demonstrates the capabilities of our proposed\nActiveVLA\n, a visionâ€“languageâ€“action framework that transforms perception from a passive sensing mechanism into an active, hypothesis-driven process. Unlike prior VLA systems that rely on static or wrist-mounted cameras and thus fail under severe occlusion, ActiveVLA explicitly integrates\nactive viewpoint selection\nand\nactive 3D zoom-in\nto dynamically acquire high-quality visual information.\nThe video consists of four manipulation scenarios intentionally designed with\ncomplex occlusion patterns, challenging spatial relationships, and fine-grained geometric constraints\n. In each scenario, the robot must actively decide both\nwhere to look\nand\nwhat region to zoom into\nbefore determining\nhow to act\n. Below, we provide a technically detailed explanation of each task.\nTask 1: â€œRetrieving a Towel from Layered Drawersâ€\nTechnical challenge summary:\nThis task contains\nself-induced structural occlusion\ncaused by multi-layer drawer geometry, where the target towel is partially or fully hidden behind drawer faces, internal shadows, and nested surfaces. The occlusion pattern is depth-dependent and viewpoint-sensitive, making monocular observation insufficient.\nIn this scenario, the towel resides inside a stack of layered drawers, where only a small portion of its surface is visible from most viewpoints. The nested geometric layout creates severe internal occlusion, preventing accurate state estimation from a single view.\nâ€¢\nActiveVLA performs\nactive viewpoint probing\n, selecting camera poses that maximize visibility of internal drawer regions and break geometric occlusion cycles.\nâ€¢\nAfter obtaining partial exposure of the towel surface, ActiveVLA triggers\nactive 3D zoom-in\nto construct a high-resolution local representation, enabling precise localization of edges, depth discontinuities, and graspable regions despite the heavy geometric occlusion.\nThis scenario demonstrates ActiveVLAâ€™s ability to resolve internal structural occlusion and extract fine-grained geometry for accurate picking.\nTask 2: â€œPicking a Red Block and Placing It onto a Green Blockâ€\nTechnical challenge summary:\nThis scene features\nmulti-object mutual occlusion\nand\nlong-horizon relational constraints\n. The red block is obscured by clutter objects, while the placement target (green block) is itself partially occluded and requires long-range spatial reasoning to ensure stable stacking.\nIn this cluttered tabletop setting, the red block is heavily obscured by surrounding objects, and its visible surface varies drastically across viewpoints. The task further requires placing the block onto a target that is also occluded.\nâ€¢\nThrough\nocclusion-aware viewpoint planning\n, ActiveVLA obtains a sequence of camera poses that reveal the red blockâ€™s true geometry and reduce ambiguity arising from clutter-induced occlusion.\nâ€¢\nWith the red and green blocks exposed, ActiveVLA applies\ntargeted 3D zoom-in\nto build a high-resolution geometric representation of both objects, enabling accurate grasping and stable long-horizon placement.\nThis task highlights ActiveVLAâ€™s capability to jointly resolve clutter occlusion and plan fine-grained manipulation over extended horizons.\nTask 3: â€œGrasping an Occluded Banana Among Multiple Fruitsâ€\nTechnical challenge summary:\nThis scenario presents\ndense, heterogeneous occlusion\nwhere the target (banana) is partially surrounded and occluded by fruits of irregular shapes. The occlusion is non-convex, multi-body, and view-dependent, making it challenging for any passive perception system to identify graspable surface regions.\nThe banana is deeply embedded within a bowl of multiple fruits, with only tiny fragments of its surface visible from typical camera views. The non-uniform shapes of the surrounding objects produce occlusion boundaries.\nâ€¢\nActiveVLA performs\nviewpoint sweeps\nto uncover minimal exposed areas of the banana, actively reducing multi-object occlusion by selecting informative perspectives.\nâ€¢\nUpon identifying a candidate grasp region, ActiveVLA uses\nactive 3D zoom-in\nto extract high-resolution local geometry, ensuring a stable and collision-free grasp while preserving the arrangement of surrounding fruits.\nTable 5:\nSuccess rates (%) on the Real-World Experiment.\nWe compare our\nActiveVLA\nwith more baselines on real-world tasks. The tasks involve complex spatial occlusion and manipulation.\nMethod\nRetrieving Towel\nRed to Green Block\nOccluded Banana\nOccluded Purple Cup\nOverall\nDiffusion Policy\n26\nÂ±\n2\n26\\scriptstyle{\\pm 2}\n38\nÂ±\n4\n38\\scriptstyle{\\pm 4}\n42\nÂ±\n2\n42\\scriptstyle{\\pm 2}\n35\nÂ±\n2\n35\\scriptstyle{\\pm 2}\n35.3\nVPP\n52\nÂ±\n2\n52\\scriptstyle{\\pm 2}\n48\nÂ±\n7\n48\\scriptstyle{\\pm 7}\n58\nÂ±\n2\n58\\scriptstyle{\\pm 2}\n64\nÂ±\n3\n64\\scriptstyle{\\pm 3}\n55.5\nTriVLA\n68\nÂ±\n4\n68\\scriptstyle{\\pm 4}\n54\nÂ±\n3\n54\\scriptstyle{\\pm 3}\n62\nÂ±\n2\n62\\scriptstyle{\\pm 2}\n72\nÂ±\n2\n72\\scriptstyle{\\pm 2}\n64.0\nRVT-2\n77\nÂ±\n4\n77\\scriptstyle{\\pm 4}\n63\nÂ±\n2\n63\\scriptstyle{\\pm 2}\n72\nÂ±\n4\n72\\scriptstyle{\\pm 4}\n78\nÂ±\n5\n78\\scriptstyle{\\pm 5}\n72.5\nActiveVLA (ours)\nğŸ—ğŸ\nÂ±\nğŸ\n\\mathbf{92\\scriptstyle{\\pm 2}}\nğŸ—ğŸ“\nÂ±\nğŸ‘\n\\mathbf{95\\scriptstyle{\\pm 3}}\nğŸ—ğŸ\nÂ±\nğŸ“\n\\mathbf{91\\scriptstyle{\\pm 5}}\nğŸ–ğŸ—\nÂ±\nğŸ\n\\mathbf{89\\scriptstyle{\\pm 2}}\n91.8\nThis task demonstrates ActiveVLAâ€™s ability to handle dense, irregular occlusion structures and perform precision manipulation in cluttered environments.\nTask 4: â€œRetrieving an Occluded Purple Cup from a Hanging Rackâ€\nTechnical challenge summary:\nThis task involves\nrelational occlusion among articulated or suspended objects\n. The purple cup is occluded by other hanging cups whose curved geometries create highly view-dependent visibility, requiring multi-view reasoning to identify graspable handles and collision-safe approach trajectories.\nThe target purple cup is positioned behind multiple hanging cups on a rack. From several viewpoints, its handle and graspable regions are entirely invisible due to occlusion.\nâ€¢\nActiveVLA evaluates the occlusion graph of the hanging rack and selects\noptimal exploratory viewpoints\nthat maximally expose the occluded cup without disturbing neighboring objects.\nâ€¢\nAfter isolating the purple cup visually,\n3D zoom-in\nis applied to recover detailed geometry around the handle region, enabling precise grasping and safe extraction.\nThis scenario showcases ActiveVLAâ€™s strength in reasoning about occlusion in relational object structures, and performing grasping under strict collision constraints.\n2\nAdditional Quantitative Results\nWe conduct further quantitative evaluations on real-world manipulation tasks that involve complex spatial arrangements and occlusions. The selected tasks include:\n1)\nRetrieving a Towel from Layered Drawers\n, which requires reasoning about objects partially hidden in layered drawer configurations;\n2)\nPicking a Red Block and Placing It onto a Green Block\n, requiring precise spatial alignment under cluttered conditions;\n3)\nGrasping an Occluded Banana Among Multiple Fruits\n, which tests the modelâ€™s ability to identify and manipulate partially obstructed objects;\n4)\nRetrieving an Occluded Purple Cup from a Hanging Rack\n, which involves fine-grained spatial reasoning.\nTable\n5\npresents the success rates of baselines, including Diffusion Policy, VPP, TriVLA, and RVT-2, as well as our proposed ActiveVLA. Across all tasks, TriVLA demonstrates strong performance, highlighting its capability to reason over spatial structures and handle occlusions. RVT-2 improves on certain tasks, particularly in aligning and grasping objects under partial visibility.\nActiveVLA further advances performance by leveraging\nactive perception\n, enabling the agent to autonomously select informative viewpoints, dynamically adjust camera zoom, and focus on regions of interest. This proactive sensing reduces uncertainty in partially observed environments, disambiguates occluded objects, and allows manipulation actions to be executed more precisely. Quantitatively, ActiveVLA improves the success rate over TriVLA by 24% on\nRetrieving a Towel\n, 41% on\nRed to Green Block\n, 29% on\nOccluded Banana\n, and 17% on\nOccluded Purple Cup\n, achieving an overall success rate of 96.3%.\nThese results demonstrate that integrating active perception with high-level policy reasoning is crucial for real-world robotic manipulation, particularly in cluttered and occluded environments. Beyond overall success rates, ActiveVLAâ€™s ability to strategically explore and adapt its observations provides a generalizable framework for robust manipulation in partially observable and dynamically structured scenarios.\nTable 6\n:\nSuccess rates (%) of ActiveVLA under different perturbations in COLOSSEUM.\nWe report mean and standard deviation over multiple trials for tasks with diverse visual and spatial perturbations.\nActiveVLA achieves consistently high performance, demonstrating robustness to environmental variations and the benefit of active perception for precise manipulation.\nTask Name\nOriginal\nAll Perturbations\nMO-COLOR\nRO-COLOR\nMO-TEXTURE\nRO-TEXTURE\nMO-SIZE\nRO-SIZE\nLight Color\nTable Color\nTable Texture\nDistractor\nBackground Texture\nRLBench\nCamera Pose\nbasketball_in_hoop\n100.0\nÂ±\n\\pm\n0.0\n6.2\nÂ±\n\\pm\n3.1\n96.3\nÂ±\n\\pm\n1.5\n96.7\nÂ±\n\\pm\n0.0\n86.5\nÂ±\n\\pm\n5.0\n-\n100.0\nÂ±\n\\pm\n0.0\n69.5\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n38.7\nÂ±\n\\pm\n2.0\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\nclose_box\n100.0\nÂ±\n\\pm\n0.0\n73.5\nÂ±\n\\pm\n1.8\n95.9\nÂ±\n\\pm\n1.2\n-\n-\n-\n94.8\nÂ±\n\\pm\n1.7\n-\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n99.3\nÂ±\n\\pm\n1.2\n99.0\nÂ±\n\\pm\n1.3\n100.0\nÂ±\n\\pm\n0.0\n98.5\nÂ±\n\\pm\n1.4\n100.0\nÂ±\n\\pm\n0.0\nclose_laptop_lid\n100.0\nÂ±\n\\pm\n0.0\n12.3\nÂ±\n\\pm\n14.5\n84.0\nÂ±\n\\pm\n3.5\n-\n-\n-\n69.5\nÂ±\n\\pm\n14.0\n-\n90.7\nÂ±\n\\pm\n7.8\n93.2\nÂ±\n\\pm\n0.0\n98.5\nÂ±\n\\pm\n3.5\n84.3\nÂ±\n\\pm\n6.5\n97.5\nÂ±\n\\pm\n3.0\n100.0\nÂ±\n\\pm\n0.0\n97.5\nÂ±\n\\pm\n0.0\nempty_dishwasher\n0.0\nÂ±\n\\pm\n0.0\n0.5\nÂ±\n\\pm\n1.5\n2.5\nÂ±\n\\pm\n1.2\n1.7\nÂ±\n\\pm\n1.8\n-\n2.0\nÂ±\n\\pm\n1.5\n4.5\nÂ±\n\\pm\n3.0\n0.0\nÂ±\n\\pm\n0.0\n0.0\nÂ±\n\\pm\n0.0\n0.0\nÂ±\n\\pm\n0.0\n0.5\nÂ±\n\\pm\n0.8\n0.5\nÂ±\n\\pm\n1.5\n1.5\nÂ±\n\\pm\n1.2\n1.7\nÂ±\n\\pm\n1.0\n0.0\nÂ±\n\\pm\n0.0\nget_ice_from_fridge\n95.5\nÂ±\n\\pm\n1.5\n6.2\nÂ±\n\\pm\n2.0\n88.5\nÂ±\n\\pm\n1.5\n91.2\nÂ±\n\\pm\n6.8\n92.0\nÂ±\n\\pm\n4.8\n-\n85.3\nÂ±\n\\pm\n3.0\n74.5\nÂ±\n\\pm\n1.5\n97.5\nÂ±\n\\pm\n3.0\n99.5\nÂ±\n\\pm\n1.5\n90.7\nÂ±\n\\pm\n7.0\n57.5\nÂ±\n\\pm\n8.0\n95.5\nÂ±\n\\pm\n1.2\n97.2\nÂ±\n\\pm\n3.0\n99.0\nÂ±\n\\pm\n1.5\nhockey\n59.5\nÂ±\n\\pm\n4.5\n10.5\nÂ±\n\\pm\n3.5\n46.7\nÂ±\n\\pm\n6.0\n52.0\nÂ±\n\\pm\n7.5\n-\n52.5\nÂ±\n\\pm\n12.0\n48.3\nÂ±\n\\pm\n7.8\n66.0\nÂ±\n\\pm\n4.8\n46.7\nÂ±\n\\pm\n1.5\n65.5\nÂ±\n\\pm\n8.0\n54.0\nÂ±\n\\pm\n1.5\n21.5\nÂ±\n\\pm\n3.0\n57.5\nÂ±\n\\pm\n5.0\n51.5\nÂ±\n\\pm\n5.0\n52.3\nÂ±\n\\pm\n5.0\ninsert_onto_square_peg\n94.7\nÂ±\n\\pm\n3.0\n24.5\nÂ±\n\\pm\n2.0\n53.5\nÂ±\n\\pm\n3.0\n95.2\nÂ±\n\\pm\n1.5\n-\n77.5\nÂ±\n\\pm\n8.0\n86.7\nÂ±\n\\pm\n3.5\n71.5\nÂ±\n\\pm\n3.0\n85.0\nÂ±\n\\pm\n0.0\n89.5\nÂ±\n\\pm\n3.0\n89.2\nÂ±\n\\pm\n3.0\n45.5\nÂ±\n\\pm\n11.0\n87.5\nÂ±\n\\pm\n1.5\n78.5\nÂ±\n\\pm\n5.0\n96.0\nÂ±\n\\pm\n0.0\nmeat_on_grill\n96.5\nÂ±\n\\pm\n0.0\n10.5\nÂ±\n\\pm\n1.5\n33.5\nÂ±\n\\pm\n0.0\n89.5\nÂ±\n\\pm\n5.0\n-\n-\n100.0\nÂ±\n\\pm\n0.0\n-\n100.0\nÂ±\n\\pm\n0.0\n93.5\nÂ±\n\\pm\n6.0\n92.0\nÂ±\n\\pm\n1.5\n99.5\nÂ±\n\\pm\n1.5\n98.7\nÂ±\n\\pm\n1.5\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\nmove_hanger\n38.7\nÂ±\n\\pm\n3.0\n3.8\nÂ±\n\\pm\n3.0\n27.5\nÂ±\n\\pm\n3.0\n48.5\nÂ±\n\\pm\n3.0\n-\n-\n-\n-\n53.5\nÂ±\n\\pm\n0.0\n85.0\nÂ±\n\\pm\n0.0\n53.7\nÂ±\n\\pm\n5.0\n53.0\nÂ±\n\\pm\n5.0\n34.5\nÂ±\n\\pm\n5.0\n44.0\nÂ±\n\\pm\n1.5\n25.0\nÂ±\n\\pm\n0.0\nopen_drawer\n97.0\nÂ±\n\\pm\n0.0\n61.5\nÂ±\n\\pm\n3.0\n98.0\nÂ±\n\\pm\n1.5\n-\n-\n-\n91.5\nÂ±\n\\pm\n1.5\n-\n89.5\nÂ±\n\\pm\n3.0\n94.5\nÂ±\n\\pm\n1.5\n100.0\nÂ±\n\\pm\n0.0\n91.5\nÂ±\n\\pm\n1.5\n100.0\nÂ±\n\\pm\n0.0\n95.0\nÂ±\n\\pm\n1.5\n96.5\nÂ±\n\\pm\n0.0\nplace_wine_at_rack_location\n89.5\nÂ±\n\\pm\n5.0\n18.5\nÂ±\n\\pm\n13.0\n84.5\nÂ±\n\\pm\n5.0\n90.7\nÂ±\n\\pm\n7.0\n-\n93.5\nÂ±\n\\pm\n6.0\n94.5\nÂ±\n\\pm\n3.5\n91.5\nÂ±\n\\pm\n3.5\n91.5\nÂ±\n\\pm\n5.0\n98.5\nÂ±\n\\pm\n1.5\n89.5\nÂ±\n\\pm\n3.0\n75.5\nÂ±\n\\pm\n3.5\n91.5\nÂ±\n\\pm\n6.0\n93.5\nÂ±\n\\pm\n3.0\n93.5\nÂ±\n\\pm\n8.0\nput_money_in_safe\n95.5\nÂ±\n\\pm\n1.5\n7.5\nÂ±\n\\pm\n4.8\n80.0\nÂ±\n\\pm\n1.5\n76.5\nÂ±\n\\pm\n1.5\n82.5\nÂ±\n\\pm\n6.5\n90.5\nÂ±\n\\pm\n5.0\n93.5\nÂ±\n\\pm\n3.0\n-\n38.5\nÂ±\n\\pm\n12.0\n85.5\nÂ±\n\\pm\n3.0\n85.5\nÂ±\n\\pm\n3.0\n85.5\nÂ±\n\\pm\n3.0\n90.5\nÂ±\n\\pm\n1.5\n87.5\nÂ±\n\\pm\n8.0\n87.5\nÂ±\n\\pm\n1.5\nreach_and_drag\n100.0\nÂ±\n\\pm\n0.0\n0.5\nÂ±\n\\pm\n0.0\n90.5\nÂ±\n\\pm\n3.5\n96.0\nÂ±\n\\pm\n0.0\n96.0\nÂ±\n\\pm\n5.0\n85.5\nÂ±\n\\pm\n5.0\n95.5\nÂ±\n\\pm\n1.5\n39.5\nÂ±\n\\pm\n5.0\n93.5\nÂ±\n\\pm\n3.0\n89.5\nÂ±\n\\pm\n5.5\n79.5\nÂ±\n\\pm\n3.5\n29.5\nÂ±\n\\pm\n8.0\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n95.5\nÂ±\n\\pm\n3.5\nscoop_with_spatula\n96.5\nÂ±\n\\pm\n3.0\n7.5\nÂ±\n\\pm\n1.5\n95.5\nÂ±\n\\pm\n1.5\n94.5\nÂ±\n\\pm\n1.5\n86.5\nÂ±\n\\pm\n3.5\n86.5\nÂ±\n\\pm\n3.5\n79.5\nÂ±\n\\pm\n3.5\n87.5\nÂ±\n\\pm\n5.0\n91.5\nÂ±\n\\pm\n1.5\n89.5\nÂ±\n\\pm\n6.0\n78.5\nÂ±\n\\pm\n1.5\n21.5\nÂ±\n\\pm\n5.0\n91.5\nÂ±\n\\pm\n6.0\n90.5\nÂ±\n\\pm\n1.5\n94.5\nÂ±\n\\pm\n1.5\nsetup_chess\n11.5\nÂ±\n\\pm\n1.5\n0.5\nÂ±\n\\pm\n0.0\n2.5\nÂ±\n\\pm\n1.5\n8.5\nÂ±\n\\pm\n0.0\n9.0\nÂ±\n\\pm\n3.0\n-\n14.5\nÂ±\n\\pm\n1.5\n-\n13.5\nÂ±\n\\pm\n5.0\n22.5\nÂ±\n\\pm\n8.0\n14.5\nÂ±\n\\pm\n3.5\n6.5\nÂ±\n\\pm\n1.5\n21.5\nÂ±\n\\pm\n5.0\n17.5\nÂ±\n\\pm\n5.0\n5.5\nÂ±\n\\pm\n3.0\nslide_block_to_target\n100.0\nÂ±\n\\pm\n0.0\n25.5\nÂ±\n\\pm\n3.0\n75.5\nÂ±\n\\pm\n1.5\n-\n93.5\nÂ±\n\\pm\n3.0\n-\n-\n-\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n99.5\nÂ±\n\\pm\n1.5\n85.5\nÂ±\n\\pm\n9.5\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\nstack_cups\n60.5\nÂ±\n\\pm\n3.5\n30.5\nÂ±\n\\pm\n1.5\n68.5\nÂ±\n\\pm\n1.5\n-\n52.0\nÂ±\n\\pm\n1.5\n-\n45.5\nÂ±\n\\pm\n3.0\n-\n64.0\nÂ±\n\\pm\n1.5\n65.5\nÂ±\n\\pm\n3.0\n66.5\nÂ±\n\\pm\n8.0\n28.5\nÂ±\n\\pm\n7.0\n74.5\nÂ±\n\\pm\n8.0\n65.5\nÂ±\n\\pm\n14.0\n73.5\nÂ±\n\\pm\n8.0\nstraighten_rope\n63.0\nÂ±\n\\pm\n6.5\n9.5\nÂ±\n\\pm\n5.0\n17.5\nÂ±\n\\pm\n5.0\n-\n49.5\nÂ±\n\\pm\n3.0\n-\n-\n-\n63.5\nÂ±\n\\pm\n9.0\n66.5\nÂ±\n\\pm\n1.5\n55.5\nÂ±\n\\pm\n8.0\n38.5\nÂ±\n\\pm\n5.0\n71.5\nÂ±\n\\pm\n8.0\n67.5\nÂ±\n\\pm\n7.5\n73.5\nÂ±\n\\pm\n6.5\nturn_oven_on\n94.5\nÂ±\n\\pm\n1.5\n86.5\nÂ±\n\\pm\n3.5\n95.5\nÂ±\n\\pm\n3.5\n-\n-\n-\n91.5\nÂ±\n\\pm\n1.5\n-\n94.5\nÂ±\n\\pm\n3.5\n95.5\nÂ±\n\\pm\n7.0\n97.0\nÂ±\n\\pm\n3.0\n97.0\nÂ±\n\\pm\n3.0\n97.0\nÂ±\n\\pm\n0.0\n89.5\nÂ±\n\\pm\n3.0\n100.0\nÂ±\n\\pm\n0.0\nwipe_desk\n0.0\nÂ±\n\\pm\n0.0\n0.5\nÂ±\n\\pm\n0.0\n0.5\nÂ±\n\\pm\n0.0\n0.5\nÂ±\n\\pm\n0.0\n0.5\nÂ±\n\\pm\n0.0\n-\n0.5\nÂ±\n\\pm\n0.0\n-\n0.0\nÂ±\n\\pm\n0.0\n0.0\nÂ±\n\\pm\n0.0\n0.0\nÂ±\n\\pm\n0.0\n0.0\nÂ±\n\\pm\n0.0\n0.0\nÂ±\n\\pm\n0.0\n0.0\nÂ±\n\\pm\n0.0\n0.0\nÂ±\n\\pm\n0.0\nTask Mean\n74.5\nÂ±\n\\pm\n0.7\n20.0\nÂ±\n\\pm\n2.0\n62.0\nÂ±\n\\pm\n1.0\n65.0\nÂ±\n\\pm\n0.1\n65.0\nÂ±\n\\pm\n1.5\n70.0\nÂ±\n\\pm\n3.0\n71.0\nÂ±\n\\pm\n1.0\n63.0\nÂ±\n\\pm\n0.8\n71.0\nÂ±\n\\pm\n1.2\n77.0\nÂ±\n\\pm\n0.9\n73.0\nÂ±\n\\pm\n0.7\n53.0\nÂ±\n\\pm\n1.5\n76.0\nÂ±\n\\pm\n1.0\n74.0\nÂ±\n\\pm\n0.2\n75.0\nÂ±\n\\pm\n0.3\n3\nAdditional Implementation Details\nTraining Stages.\nActiveVLA is optimized through sequential stages: training on RLBench, COLOSSEUM, GemBench, and finally real-world robot data. All stages share an identical optimization pipeline. Throughout the entire training process, we keep both the SigLIP vision encoder and the language token embeddings frozen to avoid representation drift and to maintain a stable perceptual backbone under actively changing viewpoints.\nRLBench Training.\nFor RLBench, we adopt a learning rate of\n8\nÃ—\n10\nâˆ’\n5\n8\\times 10^{-5}\nwith AdamW and a batch size of 192, without warmup. Fine-tuning is performed for 90,000 steps on 16 NVIDIA H100 GPUs (approximately 20 hours). This stage instills strong visuomotor priors and teaches the model to resolve structured manipulation tasks involving partial occlusions, multi-object interactions, and geometrically-constrained contact dynamics.\nCOLOSSEUM Training.\nThe COLOSSEUM stage uses the same optimization configuration as RLBench: a learning rate of\n8\nÃ—\n10\nâˆ’\n5\n8\\times 10^{-5}\n, AdamW, batch size 192, and no warmup. Training spans 90,000 steps on 16 NVIDIA H100 GPUs (about 20 hours). Compared to RLBench, COLOSSEUM introduces procedurally generated spatial layouts and high-frequency viewpoint perturbations, enabling ActiveVLA to acquire stronger invariance to transient self-occlusion, clutter-induced ambiguity, and large-scale geometric variations.\nGemBench Training.\nFor GemBench, we maintain the learning rate of\n8\nÃ—\n10\nâˆ’\n5\n8\\times 10^{-5}\nand AdamW while adjusting the batch size to 160 to accommodate longer trajectories and higher per-sample memory cost. Training is conducted for 50 epochs on 32 NVIDIA H100 GPUs (roughly 8 hours). This dataset emphasizes precise, fine-grained manipulation with dense clutter, demanding accurate feature localization and precise 3D reasoning under persistent occlusions.\nReal-robot Fine-tuning.\nIn the final stage, we adapt the model to real-world data using a smaller learning rate of\n2\nÃ—\n10\nâˆ’\n5\n2\\times 10^{-5}\nwith AdamW and a batch size of 192. Real-robot fine-tuning is performed on 8 NVIDIA H100 GPUs for 400 epochs (approximately 2 hours). This step calibrates ActiveVLAâ€™s viewpoint selection and 3D zoom-in modules to real sensor noise, illumination variations, and complex, non-procedural occlusion patterns encountered during physical execution.\nTraining Stability and Optimization Notes.\nAcross all stages, we apply gradient clipping at a maximum norm of 1.0 and use cosine learning rate decay. Freezing the vision and language encoders ensures that active viewpoint shifts do not destabilize the multimodal embedding space. Only the action head, viewpoint selection module, and 3D zoom-in module are updated during training.\nFigure 5\n:\nVisualization of 18 RLBench tasks.\nThe tasks cover a diverse set of manipulation challenges, including object grasping, tool use, and complex spatial interactions, illustrating the range of scenarios used for evaluation.\n4\nDetails of Simulation Tasks\nFor the simulation, ActiveVLA is evaluated across three major benchmarks, covering a broad distribution of manipulation difficulty, task compositionality, and robustness challenges. As shown in Figure\n5\n, these benchmarks include structured multi-step manipulation, perturbation-heavy robustness evaluation, and hierarchical skill composition, collectively forming a comprehensive simulation suite. This section provides a detailed overview of these simulation domains. The experimental results are shown in Table\n6\n.\nRLBench.\nRLBench serves as the foundational simulation benchmark for ActiveVLA, consisting of 18 visually and physically diverse manipulation tasks executed using a 7-DoF Franka Panda manipulator. Each task is equipped with 100 expert demonstrations generated in the RLBench pipeline. The robot receives synchronized RGB-D observations from four extrinsically calibrated cameras, providing multi-view coverage that enables ActiveVLA to reason about occluded objects and viewpoint-dependent geometry.\nRLBench tasks span fine-grained pick-and-place operations, articulated-object interactions, and multi-stage routines requiring precise control of contact transitions. The benchmark is built on PyRep and CoppeliaSim, offering deterministic physics, contact modeling, and scene-level randomization. These characteristics make RLBench a controlled yet sufficiently diverse domain for learning structured visuomotor policies.\nCOLOSSEUM.\nCOLOSSEUM extends RLBench into a perturbation-rich robustness benchmark by introducing 12 systematic variation types across object geometry, material, scene layout, kinematics, lighting, and camera configuration. These perturbations include distribution shifts in object shape and mass, pose-domain noise, distractor injection, background and lighting alterations, and adversarial camera placement changes that severely affect visibility.\nEach perturbation type is procedurally generated with controlled parameter ranges, enabling a quantitative evaluation of robustness under viewpoint changes and occlusion-induced ambiguity. We evaluate ActiveVLA on the same set of tasks as RLBench, but under these structured disturbances. Because COLOSSEUM preserves task semantics while altering environmental statistics, it serves as a stress test for generalization, viewpoint invariance, and policy stability under heavy covariate shift.\nFigure 6\n:\nVisualization of direct manipulation tasks.\nThese tasks evaluate baseline visuomotor skills and include cloth manipulation, highlighting varying physical and spatial challenges.\nFigure 7\n:\nVisualization of occlusion-heavy and spatially complex tasks.\nThese tasks challenge ActiveVLAâ€™s active viewpoint selection and 3D zoom-in capabilities. They include multi-layered drawer retrieval (\ntowel\n), high-precision stacking with partial occlusion (\nred to green block\n), collision-aware grasping in clutter (\noccluded banana\n), and narrow-grasp extraction from a hanging rack (\npurple cup\n), highlighting severe occlusions, complex geometry, and fine-grained 6D pose reasoning.\nGemBench.\nGemBench is a hierarchical generalization benchmark built upon RLBench. It decomposes manipulation into seven core action primitives (e.g., reaching, grasping, placing, pushing, rotation, alignment), and constructs 16 training tasks and 44 held-out evaluation tasks by composing these primitives in novel combinations. Tasks differ not only in object types and spatial arrangements but also in primitive ordering, interaction dependencies, and multi-step causal structure.\nUnlike RLBench and COLOSSEUM, where each task follows a predefined demonstration distribution, GemBench requires learning from primitive-level structure and recombining them to solve previously unseen compositions. This setting challenges the modelâ€™s ability to perform long-horizon reasoning, adapt to unfamiliar task graphs, and maintain consistent visuomotor grounding across compositional variations.\nThe benchmark uses the Franka Panda robot under RLBench physics, with additional environment-induced occlusions and clutter not present in the base tasks. Evaluating on GemBench allows us to measure ActiveVLAâ€™s capacity for skill recomposition, cross-task transfer, and manipulation generalization at the structural level.\n5\nDetails of Real-Robot Tasks\nWe evaluate ActiveVLA on a diverse set of real-robot manipulation tasks that span basic object handling, articulated-object interaction, non-prehensile control, and complex occlusion-heavy scenarios. All experiments are conducted on a Franka Panda robot equipped with an eye-in-hand RGB-D sensor and two auxiliary static cameras, enabling multi-view observations under natural occlusion.\nDirect Manipulation Tasks.\nAs shown in Figure\n6\n, the first group consists of relatively straightforward manipulation tasks designed to assess baseline visuomotor competence:\nâ€¢\nWiping the table with a towel\n: The robot grasps a deformable pink towel and wipes a specified region of a black tabletop. Mild uncertainties arise due to cloth deformation and contact variability.\nâ€¢\nPicking up an orange and placing it into a basket\n: The robot must reliably grasp a spherical object and place it into a confined container, requiring stable grasp synthesis and precise placement.\nâ€¢\nOpening an orange drawer and placing a mouse inside\n: This sequential task requires articulated-object manipulation, followed by object retrieval and placement within a restricted drawer volume.\nâ€¢\nPushing a blue bowl in front of a green plate\n: A non-prehensile manipulation task that relies on continuous contact and collision-free motion in a cluttered planar environment.\nâ€¢\nPressing a red button on the desk\n: A reach-and-interact task requiring accurate end-effector pose control but minimal physical complexity.\nOcclusion-Heavy and Spatially Complex Tasks.\nAs shown in Figure\n7\n,\nThe second group contains tasks explicitly designed to challenge ActiveVLAâ€™s active viewpoint selection and 3D zoom-in mechanisms. These tasks include severe occlusion, multi-layer geometry, and fine-grained 6D pose reasoning:\nâ€¢\nRetrieving a towel from layered drawers\n: The robot must identify the correct drawer, reason about partial occlusions, actuate the drawer, and extract the towel from a tightly constrained multi-layer structure.\nâ€¢\nPicking a red block and placing it onto a green block\n: A high-precision stacking task where the target object is partially occluded or surrounded by distractors, demanding fine-grained localization and vertical alignment.\nâ€¢\nGrasping an occluded banana among multiple fruits\n: The banana is frequently hidden beneath or between other fruits, requiring collision-aware grasp planning and viewpoint selection in dense clutter.\nâ€¢\nRetrieving an occluded purple cup from a hanging rack\n: The rack geometry induces persistent occlusions and narrow allowable grasp regions. The robot must infer the 3D pose of the cup and extract it without contacting objects.\nThese occlusion-heavy tasks pose substantially greater challenges due to compounded visibility constraints, irregular geometry, and the need for precise 6D pose inference.",
  "preview_text": "Recent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook the importance of active perception: they typically rely on static, wrist-mounted cameras that provide an end-effector-centric viewpoint. As a result, these models are unable to adaptively select optimal viewpoints or resolutions during task execution, which significantly limits their performance in long-horizon tasks and fine-grained manipulation scenarios. To address these limitations, we propose ActiveVLA, a novel vision-language-action framework that empowers robots with active perception capabilities for high-precision, fine-grained manipulation. ActiveVLA adopts a coarse-to-fine paradigm, dividing the process into two stages: (1) Critical region localization. ActiveVLA projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness. (2) Active perception optimization. Drawing on the localized critical regions, ActiveVLA uses an active view selection strategy to choose optimal viewpoints. These viewpoints aim to maximize amodal relevance and diversity while minimizing occlusions. Additionally, ActiveVLA applies a 3D zoom-in to improve resolution in key areas. Together, these steps enable finer-grained active perception for precise manipulation. Extensive experiments demonstrate that ActiveVLA achieves precise 3D manipulation and outperforms state-of-the-art baselines on three simulation benchmarks. Moreover, ActiveVLA transfers seamlessly to real-world scenarios, enabling robots to learn high-precision tasks in complex environments.\n\nActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation\nZhenyang Liu\n1,2\n, Yongchong Gu\n1\n, Yikai Wang\n3\n1\n1\nfootnotemark:\n1",
  "is_relevant": true,
  "relevance_score": 8.0,
  "extracted_keywords": [
    "VLA",
    "locomotion",
    "whole body control"
  ],
  "one_line_summary": "ActiveVLAé€šè¿‡ä¸»åŠ¨æ„ŸçŸ¥å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œå®ç°ç²¾ç¡®3Dæœºå™¨äººæ“ä½œï¼Œæå‡é•¿æ—¶ç¨‹å’Œç»†ç²’åº¦ä»»åŠ¡æ€§èƒ½ã€‚",
  "detailed_summary": "## è®ºæ–‡ã€ŠActiveVLAï¼šå°†ä¸»åŠ¨æ„ŸçŸ¥æ³¨å…¥è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä»¥å®ç°ç²¾ç¡®3Dæœºå™¨äººæ“ä½œã€‹è¯¦ç»†æ‘˜è¦\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nè¿‘å¹´æ¥ï¼ŒåŸºäºé¢„è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰VLAæ–¹æ³•å¤§å¤šä¾èµ–é™æ€æˆ–è…•éƒ¨å›ºå®šçš„æ‘„åƒå¤´ï¼Œæä¾›çš„æ˜¯å›ºå®šã€ä»¥æœ«ç«¯æ‰§è¡Œå™¨ä¸ºä¸­å¿ƒçš„è§†è§’ã€‚è¿™ç§è¢«åŠ¨æ„ŸçŸ¥æ–¹å¼æ— æ³•åœ¨æ‰§è¡Œä»»åŠ¡æ—¶è‡ªé€‚åº”åœ°é€‰æ‹©æœ€ä¼˜è§†è§’æˆ–åˆ†è¾¨ç‡ï¼Œä¸¥é‡é™åˆ¶äº†æ¨¡å‹åœ¨é•¿è§†é‡ä»»åŠ¡å’Œç»†ç²’åº¦æ“ä½œåœºæ™¯ä¸­çš„æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡º**ActiveVLA**ï¼Œæ—¨åœ¨ä¸ºæœºå™¨äººæ³¨å…¥ä¸»åŠ¨æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»¥å®ç°é«˜ç²¾åº¦ã€ç»†ç²’åº¦çš„3Dæ“ä½œã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\nActiveVLAçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ª**ç”±ç²—åˆ°ç²¾çš„ä¸¤é˜¶æ®µä¸»åŠ¨æ„ŸçŸ¥æ¡†æ¶**ï¼š\n- **å…³é”®åŒºåŸŸå®šä½ï¼ˆç²—é˜¶æ®µï¼‰**ï¼šå°†3Dç‚¹äº‘æŠ•å½±åˆ°å¤šè§†è§’ï¼ˆé¡¶ã€å‰ã€å³ï¼‰çš„2Dæ­£äº¤å›¾åƒä¸Šï¼Œè¾“å…¥é¢„è®­ç»ƒçš„VLMéª¨å¹²ç½‘ç»œï¼ˆå¦‚PaliGemmaï¼‰ã€‚é€šè¿‡ä¸€ä¸ªçƒ­å›¾é¢„æµ‹æ¨¡å—ï¼Œæ¨¡å‹èƒ½å¤Ÿå®šä½3Dåœºæ™¯ä¸­ä¸ä»»åŠ¡æœ€ç›¸å…³çš„å…³é”®åŒºåŸŸã€‚\n- **ä¸»åŠ¨æ„ŸçŸ¥ä¼˜åŒ–ï¼ˆç²¾é˜¶æ®µï¼‰**ï¼šåŸºäºå®šä½çš„å…³é”®åŒºåŸŸï¼Œæ‰§è¡Œä¸¤é¡¹ä¸»åŠ¨æ„ŸçŸ¥æ“ä½œï¼š\n    1. **ä¸»åŠ¨è§†è§’é€‰æ‹©**ï¼šåœ¨å…³é”®åŒºåŸŸå‘¨å›´ç”Ÿæˆå€™é€‰ç›¸æœºä½å§¿ï¼Œé€šè¿‡ä¸€ä¸ªç»¼åˆè€ƒè™‘**å¯è§æ€§**ï¼ˆé¿å…é®æŒ¡ï¼‰ã€**è·ç¦»**ï¼ˆé€‚ä¸­è§†è·ï¼‰å’Œ**å¤šæ ·æ€§**ï¼ˆè§†è§’å·®å¼‚ï¼‰çš„è¯„åˆ†å‡½æ•°ï¼Œé€‰æ‹©æœ€ä¼˜çš„Kä¸ªè§‚æµ‹è§†è§’ã€‚\n    2. **ä¸»åŠ¨3Dæ”¾å¤§**ï¼šåœ¨æœ€ä¼˜è§†è§’ä¸Šï¼Œé€šè¿‡è™šæ‹Ÿæ¸²æŸ“ç¼©å°è§†åœºè§’ï¼Œå¯¹å…³é”®åŒºåŸŸè¿›è¡Œâ€œæ•°å­—å˜ç„¦â€ï¼Œåœ¨ä¸æŸå¤±åƒç´ åˆ†è¾¨ç‡çš„å‰æä¸‹è·å–å±€éƒ¨é«˜åˆ†è¾¨ç‡è§†å›¾ï¼Œä»è€Œæ”¯æŒç²¾ç»†æ“ä½œã€‚\n\nè¯¥æ¡†æ¶å°†3Dç©ºé—´æ¨ç†ä¸è§†è§‰-è¯­è¨€ç†è§£ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ä¸ªåŠ¨æ€è°ƒæ•´æ„ŸçŸ¥è¾“å…¥çš„é—­ç¯ç®¡é“ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nåœ¨ä¸‰ä¸ªä»¿çœŸåŸºå‡†æµ‹è¯•å’ŒçœŸå®æœºå™¨äººå®éªŒä¸­ï¼ŒActiveVLAå‡å–å¾—äº†é¢†å…ˆæ€§èƒ½ï¼š\n- **RLBench**ï¼šåœ¨18ä¸ªä»»åŠ¡ä¸Šå¹³å‡æˆåŠŸç‡é«˜è¾¾**91.8%**ï¼Œåœ¨10ä¸ªä»»åŠ¡ä¸Šæ’åç¬¬ä¸€ï¼Œå°¤å…¶åœ¨éœ€è¦ç²¾ç¡®æ¥è§¦çš„ä»»åŠ¡ï¼ˆå¦‚â€œæ’å…¥é”€é’‰â€ï¼‰ä¸­è¡¨ç°å‡ºè‰²ã€‚\n- **COLOSSEUM**ï¼šåœ¨åŒ…å«å¤šç§æ‰°åŠ¨ï¼ˆå¦‚ç‰©ä½“é¢œè‰²ã€çº¹ç†ã€å°ºå¯¸ã€ç›¸æœºä½å§¿å˜åŒ–ï¼‰çš„æ³›åŒ–åœºæ™¯ä¸­ï¼Œå¹³å‡æˆåŠŸç‡è¾¾**65.9%**ï¼Œä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œå±•ç°äº†å¼ºå¤§çš„é²æ£’æ€§ã€‚\n- **GemBench**ï¼šåœ¨è¯„ä¼°æŠ€èƒ½ç»„åˆä¸æ³›åŒ–èƒ½åŠ›çš„åŸºå‡†ä¸Šï¼Œå¹³å‡æˆåŠŸç‡è¾¾**51.3%**ï¼Œåˆ›é€ äº†æ–°çš„æœ€ä¼˜æ€§èƒ½ã€‚\n- **çœŸå®ä¸–ç•Œå®éªŒ**ï¼šåœ¨åŒ…å«ä¸¥é‡é®æŒ¡çš„å¤æ‚æ“ä½œä»»åŠ¡ï¼ˆå¦‚ä»åˆ†å±‚æŠ½å±‰ä¸­å–æ¯›å·¾ã€ä»æ°´æœå †ä¸­æŠ“å–è¢«é®æŒ¡çš„é¦™è•‰ï¼‰ä¸­ï¼ŒActiveVLAå–å¾—äº†**91.8%**çš„æ•´ä½“æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–VLAå’Œ3Dç­–ç•¥æ¨¡å‹ã€‚\n- **æ¶ˆèå®éªŒ**è¯å®äº†ä¸»åŠ¨è§†è§’é€‰æ‹©å’Œ3Dæ”¾å¤§ä¸¤ä¸ªæ¨¡å—å¯¹æ€§èƒ½æå‡å‡æœ‰æ˜¾è‘—è´¡çŒ®ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\næœ¬æ–‡çš„è´¡çŒ®åœ¨äºï¼š\n- **æ–¹æ³•è®ºåˆ›æ–°**ï¼šé¦–æ¬¡åœ¨VLAæ¡†æ¶ä¸­ç³»ç»Ÿæ€§åœ°é›†æˆäº†ä¸»åŠ¨æ„ŸçŸ¥èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ä¸ªå¯æ“ä½œçš„ã€ç”±ç²—åˆ°ç²¾çš„ä¸»åŠ¨æ„ŸçŸ¥ç®¡é“ã€‚\n- **æ€§èƒ½çªç ´**ï¼šåœ¨å¤šä¸ªæƒå¨åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†ä¸»åŠ¨æ„ŸçŸ¥å¯¹äºæå‡æœºå™¨äººæ“ä½œç²¾åº¦ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚\n- **å®ç”¨ä»·å€¼**ï¼šæ¡†æ¶èƒ½å¤Ÿæ— ç¼è¿ç§»åˆ°çœŸå®ä¸–ç•Œï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨å¤æ‚ã€é®æŒ¡ä¸¥é‡çš„ç¯å¢ƒä¸­å®Œæˆé«˜ç²¾åº¦ä»»åŠ¡ï¼Œæ¨åŠ¨äº†å…·èº«æ™ºèƒ½å‘æ›´è‡ªé€‚åº”ã€æ›´å¯é çš„æ–¹å‘å‘å±•ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œå°†æ„ŸçŸ¥ä»è¢«åŠ¨æ¥æ”¶è½¬å˜ä¸ºä¸»åŠ¨å‡è®¾æ£€éªŒï¼Œæ˜¯è¿ˆå‘ä¸‹ä¸€ä»£è‡ªé€‚åº”æœºå™¨äººç³»ç»Ÿçš„å…³é”®ä¸€æ­¥ã€‚",
  "qa_pairs": [
    {
      "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
      "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šActiveVLA\n\n### **ä¸€ã€ æ ¸å¿ƒé—®é¢˜**\nç°æœ‰çš„å¤§å¤šæ•°è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­å­˜åœ¨ä¸€ä¸ªæ ¹æœ¬æ€§ç¼ºé™·ï¼š**æ„ŸçŸ¥æ˜¯è¢«åŠ¨çš„**ã€‚å®ƒä»¬é€šå¸¸ä¾èµ–äºå›ºå®šçš„ï¼ˆå¦‚æ‰‹è…•å®‰è£…çš„ï¼‰æ‘„åƒå¤´ï¼Œæä¾›å•ä¸€çš„ã€ä»¥æœ«ç«¯æ‰§è¡Œå™¨ä¸ºä¸­å¿ƒçš„è§†è§’ã€‚è¿™å¯¼è‡´äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š\n1.  **æ— æ³•è‡ªé€‚åº”é€‰æ‹©æœ€ä¼˜è§†è§’**ï¼šåœ¨é•¿æ—¶ç¨‹ä»»åŠ¡æˆ–å¤æ‚åœºæ™¯ä¸­ï¼Œæœºå™¨äººæ— æ³•ä¸»åŠ¨è°ƒæ•´è§†è§’ä»¥è·å–å…³é”®ä¿¡æ¯ï¼ˆå¦‚è¢«é®æŒ¡çš„ç‰©ä½“ï¼‰ã€‚\n2.  **æ— æ³•åŠ¨æ€è°ƒæ•´åˆ†è¾¨ç‡**ï¼šå¯¹äºéœ€è¦ç²¾ç»†æ“ä½œçš„åœºæ™¯ï¼ˆå¦‚æ’å…¥å°å­”ï¼‰ï¼Œå›ºå®šçš„å…¨å±€è§†å›¾æ— æ³•æä¾›è¶³å¤Ÿçš„ç»†èŠ‚ã€‚\n\nè¿™äº›é—®é¢˜ä¸¥é‡é™åˆ¶äº†VLAæ¨¡å‹åœ¨**é•¿æ—¶ç¨‹ã€ç»†ç²’åº¦ã€é«˜ç²¾åº¦**æ“ä½œä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº† **ActiveVLA** æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº**å°†ä¸»åŠ¨æ„ŸçŸ¥èƒ½åŠ›æ³¨å…¥åˆ°è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­**ã€‚è¿™å¹¶éç®€å•çš„æ¨¡å—å åŠ ï¼Œè€Œæ˜¯ä¸€ä¸ªç³»ç»Ÿæ€§çš„èŒƒå¼è½¬å˜ï¼Œä½¿æœºå™¨äººä»â€œè¢«åŠ¨çœ‹â€å˜ä¸ºâ€œä¸»åŠ¨æ¢â€ã€‚\n\nå…·ä½“åˆ›æ–°ä½“ç°åœ¨ä»¥ä¸‹ä¸‰ä¸ªå±‚é¢ï¼š\n\n1.  **èŒƒå¼åˆ›æ–°ï¼šä¸»åŠ¨æ„ŸçŸ¥çš„VLAæ¡†æ¶**\n    *   **æ ¸å¿ƒç†å¿µ**ï¼šå°†æ„ŸçŸ¥è§†ä¸ºä¸€ä¸ªä¸»åŠ¨çš„ã€å‡è®¾é©±åŠ¨çš„è¿‡ç¨‹ï¼Œæœºå™¨äººèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´â€œçœ‹å“ªé‡Œâ€å’Œâ€œçœ‹å¤šç»†â€ã€‚\n    *   **ä¸ç°æœ‰å·¥ä½œçš„åŒºåˆ«**ï¼šä¸åŒäºå¤§å¤šæ•°ä»…å¤„ç†é™æ€2Då›¾åƒæˆ–ç®€å•èåˆ3Dç‰¹å¾çš„VLAæ¨¡å‹ï¼ŒActiveVLAé¦–æ¬¡ç³»ç»Ÿåœ°å°†**ä¸»åŠ¨è§†è§’é€‰æ‹©**å’Œ**ä¸»åŠ¨3Dæ”¾å¤§**ä½œä¸ºæ ¸å¿ƒæœºåˆ¶é›†æˆåˆ°å­¦ä¹ æ¡†æ¶ä¸­ã€‚\n\n2.  **æ–¹æ³•åˆ›æ–°ï¼šç”±ç²—åˆ°ç²¾çš„ä¸¤é˜¶æ®µä¸»åŠ¨æ„ŸçŸ¥ç®¡é“**\n    ActiveVLAé‡‡ç”¨ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„**ä¸¤é˜¶æ®µï¼ˆç”±ç²—åˆ°ç²¾ï¼‰ç®¡é“**æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼š\n\n    *   **ç¬¬ä¸€é˜¶æ®µï¼šç²—ç²’åº¦ - 3Då…³é”®åŒºåŸŸå®šä½**\n        - **è¾“å…¥**ï¼šå°†3Dç‚¹äº‘æŠ•å½±åˆ°**å¤šè§†å›¾ï¼ˆé¡¶è§†ã€å‰è§†ã€å³è§†ï¼‰çš„2Dæ­£äº¤å›¾åƒ**ï¼Œä»¥é€‚é…é¢„è®­ç»ƒçš„VLMéª¨å¹²ç½‘ç»œã€‚\n        - **å¤„ç†**ï¼šVLMå¤„ç†è¿™äº›å›¾åƒå’Œè¯­è¨€æŒ‡ä»¤ï¼Œé€šè¿‡ä¸€ä¸ª**çƒ­å›¾é¢„æµ‹æ¨¡å—**æ¥å®šä½ä»»åŠ¡ç›¸å…³çš„å…³é”®åŒºåŸŸã€‚\n        - **è¾“å‡º**ï¼šå°†2Dçƒ­å›¾åæŠ•å½±å›3Dç©ºé—´ï¼Œ**è¯†åˆ«å‡ºéœ€è¦é‡ç‚¹å…³æ³¨çš„ä¸‰ç»´åŒºåŸŸ**ã€‚\n\n    *   **ç¬¬äºŒé˜¶æ®µï¼šç»†ç²’åº¦ - ä¸»åŠ¨æ„ŸçŸ¥ä¼˜åŒ–**\n        - **ä¸»åŠ¨è§†è§’é€‰æ‹©**ï¼šå›´ç»•ç¬¬ä¸€é˜¶æ®µå®šä½çš„å…³é”®3DåŒºåŸŸï¼Œç”Ÿæˆä¸€ç»„å€™é€‰ç›¸æœºä½å§¿ã€‚é€šè¿‡ä¸€ä¸ª**å¤šç›®æ ‡è¯„åˆ†å‡½æ•°**ï¼ˆç»¼åˆè€ƒè™‘**å¯è§æ€§**ã€**è·ç¦»**å’Œ**è§†è§’å¤šæ ·æ€§**ï¼‰æ¥é€‰æ‹©æœ€ä¼˜çš„Kä¸ªè§†è§’ï¼Œä»¥æœ€å¤§åŒ–ä¿¡æ¯è·å–å¹¶æœ€å°åŒ–é®æŒ¡ã€‚\n        - **ä¸»åŠ¨3Dæ”¾å¤§**ï¼šåœ¨é€‰å®šçš„æœ€ä¼˜è§†è§’ä¸Šï¼Œ**è™šæ‹Ÿåœ°ç¼©å°æ¸²æŸ“è§†åœºè§’**ï¼Œå¯¹å…³é”®åŒºåŸŸè¿›è¡Œé«˜åˆ†è¾¨ç‡â€œæ”¾å¤§â€ã€‚è¿™ç›¸å½“äºåœ¨3Dæ¸²æŸ“ç©ºé—´ä¸­è¿›è¡Œå…‰å­¦å˜ç„¦ï¼Œ**åœ¨ä¸æŸå¤±åƒç´ åˆ†è¾¨ç‡çš„å‰æä¸‹è·å–å±€éƒ¨ç»†èŠ‚**ï¼Œå…¬å¼ä¸ºï¼š`W(z) = 2d * tan(Î±/(2z))`ï¼Œå…¶ä¸­`z`ä¸ºæ”¾å¤§å› å­ã€‚\n\n3.  **æ¶æ„åˆ›æ–°ï¼š3Dæ„ŸçŸ¥ä¸VLMçš„æ·±åº¦å¯¹é½**\n    *   è®¾è®¡äº†ä¸€ä¸ª**çƒ­å›¾åæŠ•å½±ä¸å¤šè§†å›¾å¾—åˆ†èåˆ**çš„æœºåˆ¶ï¼Œå°†2D VLMçš„æ³¨æ„åŠ›å‡†ç¡®åœ°æ˜ å°„åˆ°3DåŠ¨ä½œç©ºé—´ã€‚\n    *   é‡‡ç”¨**å…¨å±€-å±€éƒ¨ç‰¹å¾èåˆ**çš„å¤´éƒ¨ç½‘ç»œï¼Œç»“åˆåœºæ™¯çš„æ•´ä½“è¯­ä¹‰å’ŒROIå†…çš„ç²¾ç»†å‡ ä½•ä¿¡æ¯ï¼Œæ¥é¢„æµ‹6-DoFæœ«ç«¯æ‰§è¡Œå™¨ä½å§¿ã€å¤¹çˆªçŠ¶æ€å’Œç¢°æ’æ ‡å¿—ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆæ€»ç»“**\n**ä¸€å¥è¯æ¦‚æ‹¬**ï¼šActiveVLAé€šè¿‡ä¸€ä¸ª**ç”±ç²—åˆ°ç²¾çš„ä¸»åŠ¨æ„ŸçŸ¥ç®¡é“**ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿå…ˆ**å®šä½**ä»»åŠ¡å…³é”®åŒºåŸŸï¼Œå†**åŠ¨æ€é€‰æ‹©**æœ€ä½³è§‚æµ‹è§†è§’å¹¶**æ”¾å¤§**å±€éƒ¨ç»†èŠ‚ï¼Œä»è€Œå®ç°äº†å¯¹å¤æ‚ã€é®æŒ¡ä¸¥é‡ã€éœ€ç²¾ç»†æ“ä½œä»»åŠ¡çš„é«˜ç²¾åº¦3Dæ“æ§ã€‚\n\n**æŠ€æœ¯è·¯å¾„**ï¼š\n```\né—®é¢˜ï¼ˆè¢«åŠ¨æ„ŸçŸ¥ï¼‰ \nâ†’ æ€è·¯ï¼ˆæ³¨å…¥ä¸»åŠ¨æ„ŸçŸ¥ï¼‰ \nâ†’ æ¡†æ¶ï¼ˆä¸¤é˜¶æ®µç®¡é“ï¼‰ \nâ†’ é˜¶æ®µä¸€ï¼ˆ3Då…³é”®åŒºåŸŸå®šä½ï¼šå¤šè§†å›¾æŠ•å½± + VLMçƒ­å›¾ï¼‰ \nâ†’ é˜¶æ®µäºŒï¼ˆä¸»åŠ¨ä¼˜åŒ–ï¼šè§†è§’é€‰æ‹© + 3Dæ”¾å¤§ï¼‰ \nâ†’ è¾“å‡ºï¼ˆé«˜ç²¾åº¦6-DoFåŠ¨ä½œï¼‰\n```\n\n### **å››ã€ å®é™…ä»·å€¼**\n1.  **æ€§èƒ½æå‡**ï¼šåœ¨RLBenchã€COLOSSEUMã€GemBenchä¸‰ä¸ªä»¿çœŸåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°**æœ€å…ˆè¿›æ°´å¹³**ï¼Œå°¤å…¶åœ¨éœ€è¦åº”å¯¹é®æŒ¡å’Œç»†èŠ‚çš„ä»»åŠ¡ä¸Šä¼˜åŠ¿æ˜æ˜¾ï¼ˆå¦‚RLBenchå¹³å‡æˆåŠŸç‡91.8%ï¼‰ã€‚\n2.  **å¼ºæ³›åŒ–ä¸é²æ£’æ€§**ï¼šåœ¨åŒ…å«å¤šç§æ‰°åŠ¨ï¼ˆé¢œè‰²ã€çº¹ç†ã€å¤§å°ã€ç›¸æœºä½å§¿ç­‰ï¼‰çš„COLOSSEUMåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶å¯¹ç¯å¢ƒå˜åŒ–çš„é²æ£’æ€§ã€‚\n3.  **ä»¿çœŸåˆ°ç°å®çš„è¿ç§»**ï¼šåœ¨çœŸå®æœºå™¨äººå®éªŒä¸­ï¼Œèƒ½å¤ŸæˆåŠŸå®Œæˆ**é«˜åº¦é®æŒ¡ã€ç»“æ„å¤æ‚**çš„ç²¾ç»†æ“ä½œä»»åŠ¡ï¼ˆå¦‚ä»åˆ†å±‚æŠ½å±‰ä¸­å–æ¯›å·¾ã€ä»æ°´æœå †ä¸­æŠ“è¢«é®æŒ¡çš„é¦™è•‰ï¼‰ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚\n4.  **æ–¹å‘æ€§è´¡çŒ®**ï¼šä¸ºå…·èº«æ™ºèƒ½å’Œæœºå™¨äººæ“ä½œé¢†åŸŸæŒ‡æ˜äº†ä¸€ä¸ªé‡è¦æ–¹å‘â€”â€”**å°†æ„ŸçŸ¥ä»é™æ€æ•°æ®é©±åŠ¨è½¬å˜ä¸ºåŠ¨æ€ä»»åŠ¡é©±åŠ¨**ï¼Œæ˜¯è¿ˆå‘æ›´è‡ªé€‚åº”ã€æ›´å¯é æœºå™¨äººçš„å…³é”®ä¸€æ­¥ã€‚",
      "timestamp": "2026-01-20T17:51:17.008796",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
      "answer": "## è®ºæ–‡åˆ†æï¼šActiveVLA åŠå…¶æŠ€æœ¯è„‰ç»œæ¢³ç†\n\nåŸºäºå¯¹è®ºæ–‡ **ã€ŠActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulationã€‹** çš„æ·±å…¥é˜…è¯»ï¼Œä»¥ä¸‹å°†æ¢³ç†ä»ä¼ ç»Ÿè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹åˆ°ActiveVLAçš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œé‡ç‚¹åˆ†ææ¯ä¸€æ­¥çš„æ ¸å¿ƒæ”¹è¿›å’Œå‡ ä¸ªå…³é”®å‘å±•æ–¹å‘ã€‚\n\n### ä¸€ã€ å‘å±•è„‰ç»œæ€»è§ˆ\n\nè¿™ä¸ªé¢†åŸŸçš„å‘å±•ä¸»çº¿æ˜¯ï¼š**ä»è¢«åŠ¨æ„ŸçŸ¥çš„2Dæ¨¡å‹ -> å¼•å…¥3Dç©ºé—´ä¿¡æ¯çš„æ¨¡å‹ -> æœ€ç»ˆå®ç°ä¸»åŠ¨æ„ŸçŸ¥çš„3Dæ¨¡å‹**ã€‚å…¶æ ¸å¿ƒé©±åŠ¨åŠ›æ˜¯è§£å†³æœºå™¨äººæ“ä½œä¸­å› **å›ºå®šè§†è§’ã€åˆ†è¾¨ç‡ä¸è¶³å’Œé®æŒ¡**å¯¼è‡´çš„æ³›åŒ–æ€§å·®å’Œç²¾ç»†æ“ä½œèƒ½åŠ›å¼±çš„é—®é¢˜ã€‚\n\n### äºŒã€ å…³é”®å‘å±•é˜¶æ®µä¸æŠ€æœ¯æ¼”è¿›\n\n#### é˜¶æ®µä¸€ï¼šåŸºäº2Då›¾åƒçš„è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹\n*   **ä»£è¡¨å·¥ä½œ**ï¼šæ—©æœŸçš„VLAæ¨¡å‹ï¼Œå¦‚åŸºäºCNNæˆ–ViTçš„Image-BCæ–¹æ³•ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šç›´æ¥åˆ©ç”¨åœ¨äº’è”ç½‘è§„æ¨¡æ•°æ®ä¸Šé¢„è®­ç»ƒçš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºä¸»å¹²ï¼Œå¤„ç†2D RGBå›¾åƒå’Œè¯­è¨€æŒ‡ä»¤ï¼Œè¾“å‡ºåŠ¨ä½œã€‚æ—¨åœ¨åˆ©ç”¨VLMå¼ºå¤§çš„è¯­ä¹‰ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ã€‚\n*   **æ”¹è¿›ä¸å±€é™**ï¼š\n    *   **æ”¹è¿›**ï¼šå¼•å…¥äº†å¤§è§„æ¨¡é¢„è®­ç»ƒçš„å…ˆéªŒçŸ¥è¯†ï¼Œå‡å°‘äº†å¯¹æœºå™¨äººä¸“ç”¨æ•°æ®çš„ä¾èµ–ï¼Œåœ¨ç†è§£å’Œå“åº”è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚\n    *   **å±€é™**ï¼š**ç¼ºä¹3Dç©ºé—´æ¨ç†èƒ½åŠ›**ã€‚æ¨¡å‹éœ€è¦æµ·é‡æ•°æ®æ¥å¼¥åˆ2Dæ„ŸçŸ¥ä¸3DåŠ¨ä½œä¹‹é—´çš„é¸¿æ²Ÿï¼Œåœ¨éœ€è¦ç²¾ç¡®ç©ºé—´å…³ç³»çš„ä»»åŠ¡ï¼ˆå¦‚æ’å…¥ã€å †å ï¼‰ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¸”å¯¹è§†è§’å˜åŒ–éå¸¸æ•æ„Ÿã€‚\n\n#### é˜¶æ®µäºŒï¼šå¼•å…¥3Dæ¨¡æ€çš„VLAæ¨¡å‹\n*   **ä»£è¡¨å·¥ä½œ**ï¼šPerAct, Act3D, RVT, 3D Diffuser Actor, BridgeVLA, ä»¥åŠè®ºæ–‡ä¸­æåˆ°çš„Lift3Dã€PointVLAã€SpatialVLAç­‰ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šè®¤è¯†åˆ°çº¯2Dè¾“å…¥çš„å±€é™æ€§ï¼Œå¼€å§‹å°†3Dç‚¹äº‘ã€ä½“ç´ ç­‰ç©ºé—´è¡¨å¾èå…¥æ¨¡å‹ã€‚è¿™äº›æ–¹æ³•é€šå¸¸å°†3Dåœºæ™¯æŠ•å½±åˆ°å¤šä¸ª2Dè§†å›¾ï¼Œæˆ–ä½¿ç”¨3Dç¼–ç å™¨ï¼Œä»¥æ•è·å‡ ä½•å’Œç»“æ„ä¿¡æ¯ã€‚\n*   **æ”¹è¿›ä¸å±€é™**ï¼š\n    *   **æ”¹è¿›**ï¼š\n        1.  **ç©ºé—´æ¨ç†å¢å¼º**ï¼šé€šè¿‡3Dæ•°æ®æä¾›äº†å›ºæœ‰çš„ç»“æ„çº¿ç´¢ï¼Œæ˜¾è‘—æå‡äº†**æ ·æœ¬æ•ˆç‡**å’Œ**ç©ºé—´æ¨ç†èƒ½åŠ›**ï¼Œåœ¨å¤æ‚æ“ä½œä»»åŠ¡ä¸Šæ€§èƒ½å¤§å¹…è¶…è¶Šçº¯2Dæ¨¡å‹ã€‚\n        2.  **è¡¨å¾æ–¹å¼å¤šæ ·**ï¼šä¸åŒå·¥ä½œæ¢ç´¢äº†ä¸åŒçš„3D-2Dèåˆæ–¹å¼ï¼ˆå¦‚å¤šè§†å›¾æ­£äº¤æŠ•å½±ã€ä½“ç´ åŒ–ã€ç‚¹äº‘ç¼–ç ã€ç‰¹æ®Šä½ç½®ç¼–ç ï¼‰ï¼Œä¸°å¯Œäº†æŠ€æœ¯è·¯å¾„ã€‚\n    *   **å±€é™**ï¼š**æ„ŸçŸ¥ä»ç„¶æ˜¯è¢«åŠ¨çš„**ã€‚å°½ç®¡è¾“å…¥æ˜¯3Dçš„ï¼Œä½†è§‚æµ‹è§†è§’é€šå¸¸æ˜¯å›ºå®šçš„ï¼ˆå¦‚å›ºå®šåœ¨æ‰‹è…•æˆ–ç¯å¢ƒä¸­çš„å‡ ä¸ªç›¸æœºï¼‰ï¼Œæ¨¡å‹**æ— æ³•åœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´è§‚æµ‹ç‚¹ä½å’Œåˆ†è¾¨ç‡**ã€‚è¿™å¯¼è‡´åœ¨é•¿è§†é‡ä»»åŠ¡æˆ–å­˜åœ¨ä¸¥é‡é®æŒ¡ã€éœ€è¦è§‚å¯Ÿç»†å¾®ç‰¹å¾æ—¶ï¼Œæ¨¡å‹æ€§èƒ½å—é™ã€‚\n\n#### é˜¶æ®µä¸‰ï¼šä¸»åŠ¨æ„ŸçŸ¥é©±åŠ¨çš„VLAæ¨¡å‹ (ActiveVLA)\n*   **ä»£è¡¨å·¥ä½œ**ï¼šæœ¬æ–‡æå‡ºçš„ **ActiveVLA**ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šå°†**ä¸»åŠ¨æ„ŸçŸ¥**ç†å¿µæ³¨å…¥VLAæ¡†æ¶ã€‚ä¸ä»…å¤„ç†3Dä¿¡æ¯ï¼Œæ›´è®©æœºå™¨äººå…·å¤‡â€œä¸»åŠ¨çœ‹â€çš„èƒ½åŠ›â€”â€”æ ¹æ®ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ä¼˜è§‚æµ‹è§†è§’ï¼Œå¹¶å¯¹å…³é”®åŒºåŸŸè¿›è¡Œâ€œæ”¾å¤§â€è§‚å¯Ÿã€‚\n*   **é©å‘½æ€§æ”¹è¿›**ï¼š\n    1.  **ä»è¢«åŠ¨æ¥å—åˆ°ä¸»åŠ¨æ¢ç´¢**ï¼šè¿™æ˜¯æœ€æ ¹æœ¬çš„èŒƒå¼è½¬å˜ã€‚æ¨¡å‹ä¸å†æ˜¯é™æ€ç¯å¢ƒçš„è¢«åŠ¨è§‚å¯Ÿè€…ï¼Œè€Œæ˜¯èƒ½ä¸»åŠ¨å‘èµ·æ„ŸçŸ¥åŠ¨ä½œï¼ˆç§»åŠ¨è§†ç‚¹ã€å˜ç„¦ï¼‰ä»¥è·å–ä¿¡æ¯çš„æ™ºèƒ½ä½“ã€‚\n    2.  **ä¸¤é˜¶æ®µç²—åˆ°ç²¾ç®¡é“**ï¼š\n        *   **ç²—é˜¶æ®µï¼ˆå®šä½ï¼‰**ï¼šåˆ©ç”¨å¤šè§†å›¾2DæŠ•å½±å’ŒVLMä¸»å¹²ï¼Œå¿«é€Ÿå®šä½3Dåœºæ™¯ä¸­çš„ä»»åŠ¡å…³é”®åŒºåŸŸã€‚\n        *   **ç»†é˜¶æ®µï¼ˆä¼˜åŒ–ï¼‰**ï¼š\n            *   **ä¸»åŠ¨è§†è§’é€‰æ‹©**ï¼šåŸºäºå…³é”®åŒºåŸŸï¼Œé€šè¿‡è¯„åˆ†å‡½æ•°ï¼ˆæƒè¡¡**å¯è§æ€§**ã€**è·ç¦»**ã€**è§†è§’å¤šæ ·æ€§**ï¼‰ä»ä¸€ç»„å€™é€‰è§†ç‚¹ä¸­é€‰å‡ºæœ€ä¼˜çš„Kä¸ªï¼Œæœ€å¤§åŒ–ä¿¡æ¯è·å–å¹¶æœ€å°åŒ–é®æŒ¡ã€‚\n            *   **ä¸»åŠ¨3Då˜ç„¦**ï¼šåœ¨æœ€ä¼˜è§†è§’ä¸Šï¼Œé€šè¿‡è™šæ‹Ÿæ¸²æŸ“ç¼©å°è§†åœºè§’ï¼Œå®ç°**æ— å‡ ä½•æŸå¤±çš„â€œæ•°å­—å˜ç„¦â€**ï¼Œå¤§å¹…æå‡å…³é”®åŒºåŸŸçš„åƒç´ åˆ†è¾¨ç‡ï¼Œä»è€Œæ”¯æŒå¾®ç±³çº§ç²¾ç»†æ“ä½œã€‚\n    3.  **è§£å†³æ ¸å¿ƒç—›ç‚¹**ï¼šç›´æ¥é’ˆå¯¹**é®æŒ¡**å’Œ**åˆ†è¾¨ç‡ä¸è¶³**è¿™ä¸¤ä¸ªå¯¼è‡´ç²¾ç»†æ“ä½œå¤±è´¥çš„æ ¸å¿ƒé—®é¢˜æä¾›äº†ç³»ç»Ÿæ€§è§£å†³æ–¹æ¡ˆã€‚\n\n### ä¸‰ã€ ç€é‡å‘å±•çš„å‡ ä¸ªä¸åŒæ–¹å‘\n\nä»è„‰ç»œä¸­å¯ä»¥çœ‹å‡ºï¼Œç ”ç©¶ä¸»è¦æ²¿ç€ä»¥ä¸‹å‡ ä¸ªæ–¹å‘æ·±åŒ–ï¼š\n\n1.  **æ„ŸçŸ¥æ¨¡æ€èåˆæ–¹å‘**ï¼š\n    *   **ç›®æ ‡**ï¼šå¦‚ä½•æœ€æœ‰æ•ˆåœ°å°†3Då‡ ä½•ä¿¡æ¯ä¸2Dçº¹ç†è¯­ä¹‰ä¿¡æ¯ç»“åˆã€‚\n    *   **æ¼”è¿›**ï¼š`2Då›¾åƒ` -> `3Dç‚¹äº‘/ä½“ç´ å•ç‹¬ç¼–ç ` -> `3DæŠ•å½±ä¸ºå¤šè§†å›¾2Då›¾åƒä¸VLMå¯¹é½` -> `ActiveVLAåœ¨åŠ¨æ€é€‰æ‹©çš„è§†å›¾ä¸Šè¿›è¡Œèåˆ`ã€‚è¶‹åŠ¿æ˜¯å¯»æ‰¾ä¸å¼ºå¤§VLMå…¼å®¹ä¸”èƒ½ä¿ç•™ä¸°å¯Œç©ºé—´ä¿¡æ¯çš„è¡¨å¾æ–¹å¼ã€‚\n\n2.  **ç©ºé—´æ¨ç†ä¸åŠ¨ä½œè¡¨å¾æ–¹å‘**ï¼š\n    *   **ç›®æ ‡**ï¼šå¦‚ä½•ä»æ„ŸçŸ¥ä¸­æ¨ç†å‡ºç²¾ç¡®çš„6-DoFåŠ¨ä½œã€‚\n    *   **æ¼”è¿›**ï¼š`ç›´æ¥å›å½’åŠ¨ä½œ` -> `é¢„æµ‹3Dçƒ­åŠ›å›¾/æŠ•ç¥¨` -> `åŸºäºå€™é€‰ç‚¹æ’åº` -> `æ‰©æ•£æ¨¡å‹ç”Ÿæˆè½¨è¿¹` -> `ActiveVLAçš„å…¨å±€-å±€éƒ¨ç‰¹å¾èåˆ`ã€‚è¶‹åŠ¿æ˜¯åŠ¨ä½œé¢„æµ‹è¶Šæ¥è¶Šä¾èµ–äºç²¾ç»†åŒ–çš„ã€åŸºäºæ³¨æ„åŠ›çš„ç©ºé—´æ¦‚ç‡åˆ†å¸ƒã€‚\n\n3.  **æ³›åŒ–ä¸é²æ£’æ€§æ–¹å‘**ï¼š\n    *   **ç›®æ ‡**ï¼šæå‡æ¨¡å‹å¯¹æœªè§è¿‡çš„ç‰©ä½“ã€åœºæ™¯ã€æŒ‡ä»¤çš„é€‚åº”èƒ½åŠ›ã€‚\n    *   **æ¼”è¿›**ï¼šä¾èµ–`å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®` -> `åˆ©ç”¨3Dç»“æ„ä¸å˜æ€§` -> `åœ¨ä»¿çœŸä¸­å¼•å…¥ç³»ç»Ÿæ‰°åŠ¨è¿›è¡Œè®­ç»ƒ` -> `ActiveVLAé€šè¿‡ä¸»åŠ¨æ„ŸçŸ¥åº”å¯¹å®æ—¶é®æŒ¡å’Œç¯å¢ƒå˜åŒ–`ã€‚è¶‹åŠ¿æ˜¯ä»æ•°æ®é©±åŠ¨çš„æ³›åŒ–è½¬å‘èµ‹äºˆæ¨¡å‹ä¸»åŠ¨åº”å¯¹ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚\n\n4.  **æ„ŸçŸ¥-è¡ŒåŠ¨é—­ç¯æ–¹å‘**ï¼š\n    *   **ç›®æ ‡**ï¼šè®©æ„ŸçŸ¥ä¸ºè¡ŒåŠ¨æœåŠ¡ï¼Œè¡ŒåŠ¨åˆå½±å“æ„ŸçŸ¥ï¼Œå½¢æˆç´§å¯†é—­ç¯ã€‚\n    *   **æ¼”è¿›**ï¼š`å¼€ç¯æ„ŸçŸ¥`ï¼ˆæ„ŸçŸ¥ä¸€æ¬¡ï¼Œæ‰§è¡Œåˆ°åº•ï¼‰-> `åŸºäºå›ºå®šç­–ç•¥çš„é‡æ„ŸçŸ¥` -> `ActiveVLAçš„é—­ç¯ä¸»åŠ¨æ„ŸçŸ¥`ã€‚**ActiveVLAåœ¨è¿™ä¸€æ–¹å‘å®ç°äº†çªç ´**ï¼Œå…¶è§†è§’é€‰æ‹©å’Œå˜ç„¦æœ¬èº«å°±æ˜¯åŸºäºå½“å‰ä»»åŠ¡çŠ¶æ€å’Œæ„ŸçŸ¥ä¸ç¡®å®šæ€§åšå‡ºçš„å†³ç­–ï¼Œæ„æˆäº†ä¸€ä¸ªå†…åµŒçš„æ„ŸçŸ¥-è§„åˆ’é—­ç¯ã€‚\n\n### å››ã€ æ€»ç»“\n\n**ActiveVLA** ç«™åœ¨äº†å‰ä½œï¼ˆç‰¹åˆ«æ˜¯BridgeVLAç­‰3D VLAæ¨¡å‹ï¼‰çš„è‚©è†€ä¸Šï¼Œå…¶æ ¸å¿ƒè´¡çŒ®åœ¨äº**é¦–æ¬¡åœ¨VLAæ¡†æ¶ä¸­ç³»ç»Ÿæ€§åœ°å®ç°äº†ä¸»åŠ¨æ„ŸçŸ¥**ã€‚å®ƒå¹¶éç®€å•æ”¹è¿›3Dè¡¨å¾ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¸€ä¸ª**åŠ¨æ€çš„ã€ä»»åŠ¡é©±åŠ¨çš„æ„ŸçŸ¥ç­–ç•¥**ã€‚ç›¸æ¯”äºä¹‹å‰æ‰€æœ‰å·¥ä½œï¼Œå®ƒçš„æ”¹è¿›åœ¨äºï¼š\n\n*   **ç›¸æ¯”äºæ‰€æœ‰è¢«åŠ¨3D VLA**ï¼šè§£å†³äº†å›ºå®šè§†è§’å¸¦æ¥çš„ä¿¡æ¯ä¸å®Œæ•´é—®é¢˜ï¼Œå°¤å…¶åœ¨é•¿è§†é‡ã€é«˜ç²¾åº¦å’Œå¼ºé®æŒ¡ä»»åŠ¡ä¸Šä¼˜åŠ¿æ˜æ˜¾ã€‚\n*   **ç›¸æ¯”äºä¼ ç»Ÿä¸»åŠ¨æ„ŸçŸ¥ï¼ˆéVLAï¼‰**ï¼šå°†ä¸»åŠ¨æ„ŸçŸ¥ä¸å¼ºå¤§çš„è§†è§‰è¯­è¨€ç†è§£åŸºç¡€æ¨¡å‹ç»“åˆï¼Œä½¿å¾—è§†è§’é€‰æ‹©å’Œå˜ç„¦èƒ½åŸºäºé«˜çº§è¯­ä¹‰ä»»åŠ¡æŒ‡ä»¤ï¼Œè€Œä¸ä»…ä»…æ˜¯ä½çº§å‡ ä½•ç‰¹å¾ã€‚\n\nå› æ­¤ï¼ŒActiveVLAä»£è¡¨äº†ä¸€ä¸ªé‡è¦çš„å‘å±•æ–¹å‘ï¼š**ä½¿å…·èº«æ™ºèƒ½ä½“ä¸ä»…èƒ½å¤Ÿâ€œç†è§£â€åœºæ™¯ï¼Œè¿˜èƒ½å­¦ä¼šå¦‚ä½•â€œæœ‰ç­–ç•¥åœ°è§‚å¯Ÿâ€åœºæ™¯**ï¼Œè¿™æ˜¯å®ç°å…¶åœ¨å¤æ‚ã€åŠ¨æ€çœŸå®ä¸–ç•Œä¸­å¯é æ“ä½œçš„å…³é”®ä¸€æ­¥ã€‚è®ºæ–‡åœ¨RLBenchã€COLOSSEUMã€GemBenchå’ŒçœŸå®æœºå™¨äººå®éªŒä¸Šçš„å…¨é¢SOTAç»“æœï¼Œå¼ºæœ‰åŠ›åœ°éªŒè¯äº†è¿™ä¸€æ–¹å‘çš„ä»·å€¼ã€‚",
      "timestamp": "2026-01-20T17:52:14.811365",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
      "answer": "## ActiveVLA çš„å‰ä½œåˆ†æ\n\nActiveVLA æ˜¯å»ºç«‹åœ¨ **Vision-Language-Action (VLA)** æ¨¡å‹å’Œ **3Dæ„ŸçŸ¥æœºå™¨äººå­¦ä¹ ** ä¸¤æ¡ç ”ç©¶ä¸»çº¿ä¹‹ä¸Šçš„å·¥ä½œã€‚å…¶æ ¸å¿ƒâ€œå‰ä½œâ€å¹¶éå•ä¸€ä½œè€…çš„è¿ç»­ä½œå“ï¼Œè€Œæ˜¯æŒ‡å…¶ç›´æ¥å€Ÿé‰´ã€å¯¹æ¯”æˆ–è¶…è¶Šçš„ä¸€ç³»åˆ—ä»£è¡¨æ€§æ¨¡å‹ã€‚ä»¥ä¸‹è¡¨æ ¼è¯¦ç»†æ¢³ç†äº†è¿™äº›å…³é”®å‰ä½œï¼Œå¹¶é‡ç‚¹åˆ†æäº† ActiveVLA ä¸å®ƒä»¬çš„åŒºåˆ«åŠä¸»è¦æ”¹è¿›ã€‚\n\n### å…³é”®å‰ä½œå¯¹æ¯”åˆ†æè¡¨\n\n| æ¨¡å‹åç§° (å‘è¡¨å¹´ä»½) | æ ¸å¿ƒæ–¹æ³• / ç‰¹ç‚¹ | ä¸ ActiveVLA çš„ä¸»è¦åŒºåˆ« | ActiveVLA çš„ä¸»è¦æ”¹è¿› / åˆ›æ–°ç‚¹ |\n| :--- | :--- | :--- | :--- |\n| **PerAct** (2022) | - åŸºäº **Perceiver Transformer** çš„ä½“ç´ åŒ–ç­–ç•¥ã€‚<br>- å°†3Dåœºæ™¯ä½“ç´ åŒ–ï¼Œç›´æ¥å¤„ç†3Dç‚¹äº‘ç‰¹å¾ã€‚ | - **è¢«åŠ¨æ„ŸçŸ¥**ï¼šä¾èµ–å›ºå®šçš„ã€é¢„å®šä¹‰çš„ç›¸æœºè§†è§’ï¼ˆé€šå¸¸æ˜¯è…•éƒ¨æˆ–å›ºå®šè§†è§’ï¼‰ã€‚<br>- **ç¼ºä¹ä¸»åŠ¨è°ƒæ•´**ï¼šæ— æ³•åœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­åŠ¨æ€é€‰æ‹©æœ€ä¼˜è§†è§’æˆ–è°ƒæ•´è§‚æµ‹åˆ†è¾¨ç‡ã€‚ | **æ³¨å…¥ä¸»åŠ¨æ„ŸçŸ¥èƒ½åŠ›**ï¼š<br>1. **ä¸»åŠ¨è§†è§’é€‰æ‹© (Active View Selection)**ï¼šæ ¹æ®ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ŒåŠ¨æ€é€‰æ‹©èƒ½æœ€å¤§åŒ–ç›®æ ‡å¯è§æ€§ã€æœ€å°åŒ–é®æŒ¡çš„æœ€ä¼˜ç›¸æœºä½å§¿ã€‚<br>2. **ä¸»åŠ¨3Dæ”¾å¤§ (Active 3D Zoom-in)**ï¼šå¯¹å…³é”®åŒºåŸŸè¿›è¡Œè™šæ‹Ÿå…‰å­¦å˜ç„¦ï¼Œæå‡å±€éƒ¨åˆ†è¾¨ç‡ï¼Œå®ç°ç»†ç²’åº¦æ“ä½œã€‚ |\n| **RVT / RVT-2** (2023, 2024) | - **å¤šè§†å›¾æŠ•å½±**ï¼šå°†3Dç‚¹äº‘æ¸²æŸ“ä¸ºå¤šä¸ªæ­£äº¤2Dè§†å›¾ï¼ˆé¡¶ã€å‰ã€ä¾§ï¼‰ã€‚<br>- **ç²—åˆ°ç»†çš„Transformeræ¶æ„**ï¼šåœ¨2Dè§†å›¾ä¸Šåº”ç”¨VLMï¼Œå¹¶èåˆå¤šè§†å›¾ä¿¡æ¯è¿›è¡ŒåŠ¨ä½œé¢„æµ‹ã€‚ | - **é™æ€å¤šè§†å›¾**ï¼šè™½ç„¶ä½¿ç”¨äº†å¤šè§†å›¾ï¼Œä½†è¿™äº›è§†å›¾æ˜¯**å›ºå®šä¸”é¢„å®šä¹‰**çš„ï¼ˆå¦‚å›ºå®šçš„ä¸‰ä¸ªæ­£äº¤è§†å›¾ï¼‰ã€‚<br>- **åˆ†è¾¨ç‡å›ºå®š**ï¼šæ‰€æœ‰åŒºåŸŸçš„è§‚æµ‹åˆ†è¾¨ç‡ç›¸åŒï¼Œæ— æ³•é’ˆå¯¹å…³é”®ç»†èŠ‚è¿›è¡Œå¢å¼ºã€‚ | **åŠ¨æ€ä¸è‡ªé€‚åº”çš„å¤šè§†å›¾**ï¼š<br>1. **è§†å›¾æ˜¯åŠ¨æ€ç”Ÿæˆçš„**ï¼šåŸºäºç²—é˜¶æ®µå®šä½çš„å…³é”®åŒºåŸŸï¼Œä¸»åŠ¨åˆæˆæ–°çš„ã€æœ€ä¼˜çš„è§‚æµ‹è§†è§’ï¼Œçªç ´äº†å›ºå®šè§†å›¾çš„å±€é™ã€‚<br>2. **åˆ†è¾¨ç‡è‡ªé€‚åº”**ï¼šé€šè¿‡â€œ3Dæ”¾å¤§â€æœºåˆ¶ï¼Œåœ¨å…³é”®åŒºåŸŸè·å¾—æ›´é«˜åˆ†è¾¨ç‡çš„è§‚æµ‹ï¼Œè§£å†³äº†å›ºå®šåˆ†è¾¨ç‡ä¸‹ç»†èŠ‚ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ã€‚ |\n| **Act3D** (2023) | - **åœ¨3Dç©ºé—´ä¸­é‡‡æ ·å¹¶æ’åºå€™é€‰åŠ¨ä½œç‚¹**ã€‚<br>- é€šè¿‡è¯„ä¼°3Dç‚¹çš„ä»·å€¼æ¥é€‰æ‹©åŠ¨ä½œã€‚ | - **åŠ¨ä½œå±‚é¢çš„â€œä¸»åŠ¨â€**ï¼šå…¶â€œä¸»åŠ¨â€ä½“ç°åœ¨å¯¹**åŠ¨ä½œå€™é€‰**çš„æœç´¢å’Œæ’åºä¸Šï¼Œè€Œé**æ„ŸçŸ¥å±‚é¢**çš„ä¸»åŠ¨è°ƒæ•´ã€‚<br>- **æ„ŸçŸ¥ä»æ˜¯è¢«åŠ¨çš„**ï¼šè§‚æµ‹è¾“å…¥ä»ç„¶æ˜¯æ¥è‡ªå›ºå®šç›¸æœºçš„é™æ€æ•°æ®ã€‚ | **æ„ŸçŸ¥å±‚é¢çš„ä¸»åŠ¨æ€§ä¸é—­ç¯**ï¼š<br>å°†â€œä¸»åŠ¨æ€§â€ä»**åŠ¨ä½œé€‰æ‹©**å‰ç½®åˆ°**æ„ŸçŸ¥è·å–**ã€‚å½¢æˆäº†ä¸€ä¸ªâ€œ**æ„ŸçŸ¥-å†³ç­–**â€çš„é—­ç¯ï¼šå…ˆä¸»åŠ¨è·å–æœ€èƒ½å‡å°‘ä¸ç¡®å®šæ€§çš„è§‚æµ‹ï¼Œå†åŸºäºä¼˜è´¨è§‚æµ‹åšå‡ºç²¾å‡†å†³ç­–ã€‚è¿™æ›´ç¬¦åˆç”Ÿç‰©ä½“çš„ä¸»åŠ¨æ„ŸçŸ¥åŸç†ã€‚ |\n| **BridgeVLA** (2025) | - **å…¸å‹çš„ç°ä»£VLAæ¡†æ¶**ï¼šä½¿ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒVLMï¼ˆå¦‚PaliGemmaï¼‰ä½œä¸ºä¸»å¹²ï¼Œå°†3Dä¿¡æ¯ï¼ˆé€šè¿‡2Dçƒ­å›¾åæŠ•å½±ï¼‰ä¸è¯­è¨€æŒ‡ä»¤å¯¹é½ä»¥é¢„æµ‹åŠ¨ä½œã€‚<br>- å¼ºè°ƒäº†2Dåˆ°3Dçš„æ¡¥æ¥ã€‚ | - **æ„ŸçŸ¥èŒƒå¼ç›¸åŒ**ï¼šåŒæ ·ä¾èµ–äº**é™æ€çš„ã€å›ºå®šçš„è§‚æµ‹è¾“å…¥**ã€‚è™½ç„¶åˆ©ç”¨äº†3Dä¿¡æ¯ï¼Œä½†è·å–è¿™äº›ä¿¡æ¯çš„â€œè§†è§’â€æ˜¯å›ºå®šçš„ã€‚<br>- æ˜¯ActiveVLAé‡‡ç”¨çš„**åŸºç¡€æ¶æ„å’Œä¸»å¹²ç½‘ç»œ**ã€‚ | **åœ¨ä¼˜ç§€æ¶æ„ä¸Šå¢åŠ â€œä¸»åŠ¨æ„ŸçŸ¥å±‚â€**ï¼š<br>ActiveVLA å¯ä»¥çœ‹ä½œæ˜¯åœ¨ BridgeVLA è¿™ç±»å…ˆè¿›ä½†è¢«åŠ¨çš„VLAæ¡†æ¶ä¸Šï¼Œå¢åŠ äº†ä¸€ä¸ª**ä¸Šæ¸¸çš„ä¸»åŠ¨æ„ŸçŸ¥æ¨¡å—**ã€‚è¿™ä¸ªæ¨¡å—åœ¨åŸå§‹å›ºå®šè§‚æµ‹çš„åŸºç¡€ä¸Šï¼ŒåŠ¨æ€åœ°ç”Ÿæˆæ›´ä¼˜çš„è§‚æµ‹ï¼Œå†é€ç»™ç±»ä¼¼BridgeVLAçš„VLAæ ¸å¿ƒè¿›è¡Œå¤„ç†ã€‚è¿™æ˜¯**ç³»ç»Ÿæ¶æ„å±‚é¢**çš„æ ¹æœ¬æ€§æ‰©å±•ã€‚ |\n| **3D Diffuser Actor** (2024) | - ä½¿ç”¨**æ‰©æ•£æ¨¡å‹**æ¥ç”Ÿæˆ3DåŠ¨ä½œè½¨è¿¹ã€‚<br>- åœ¨åŠ¨ä½œåºåˆ—çš„ç”Ÿæˆä¸Šå…·æœ‰å¼ºå¤§èƒ½åŠ›ã€‚ | - **ä¸“æ³¨äºåŠ¨ä½œç”Ÿæˆçš„å…ˆè¿›æ€§**ï¼Œè€Œéæ„ŸçŸ¥çš„é€‚åº”æ€§ã€‚<br>- å…¶æˆåŠŸä¸¥é‡ä¾èµ–äºè¾“å…¥è§‚æµ‹çš„è´¨é‡ï¼Œåœ¨å­˜åœ¨ä¸¥é‡é®æŒ¡æˆ–è§†è§’ä¸ä½³æ—¶æ€§èƒ½ä¼šä¸‹é™ã€‚ | **æå‡æ„ŸçŸ¥è¾“å…¥è´¨é‡**ï¼š<br>ActiveVLA é€šè¿‡ä¸»åŠ¨æ„ŸçŸ¥ä¸ºä¸‹æ¸¸ç­–ç•¥ï¼ˆæ— è®ºæ˜¯æ‰©æ•£æ¨¡å‹è¿˜æ˜¯å…¶ä»–åŠ¨ä½œè§£ç å™¨ï¼‰æä¾›**è´¨é‡æ›´é«˜ã€ä¿¡æ¯æ›´å…¨çš„è§‚æµ‹æ•°æ®**ã€‚è¿™ä»æºå¤´ç¼“è§£äº†å› æ„ŸçŸ¥ä¸è¶³å¯¼è‡´çš„å†³ç­–å›°éš¾ï¼Œä¸3D Diffuser Actorè¿™ç±»å…ˆè¿›åŠ¨ä½œç”Ÿæˆå™¨æ˜¯äº’è¡¥å…³ç³»ã€‚ |\n| **Lift3D, PointVLA, SpatialVLA** (è¿‘æœŸ3D-VLAå·¥ä½œ) | - æ¢ç´¢å°†3Dä¿¡æ¯ï¼ˆç‚¹äº‘ã€ä½“ç´ ã€æ·±åº¦å›¾ï¼‰æ³¨å…¥åˆ°ä»¥2D VLMä¸ºæ ¸å¿ƒçš„æ¡†æ¶ä¸­çš„ä¸åŒæ–¹å¼ã€‚<br>- ä¾‹å¦‚ï¼Œä½¿ç”¨ç‹¬ç«‹3Dç¼–ç å™¨ã€è®¾è®¡ç‰¹æ®Šçš„3Dä½ç½®ç¼–ç ç­‰ã€‚ | - **æ ¸å¿ƒç›®æ ‡ä¸åŒ**ï¼šè¿™äº›å·¥ä½œä¸»è¦è§£å†³â€œ**å¦‚ä½•æœ‰æ•ˆåœ°è¡¨ç¤ºå’Œèåˆ3Dä¿¡æ¯**â€çš„é—®é¢˜ã€‚<br>- **å…±äº«åŒä¸€å±€é™**ï¼šå®ƒä»¬éƒ½é»˜è®¤**æ„ŸçŸ¥è¾“å…¥æ˜¯ç»™å®šçš„ã€é™æ€çš„**ï¼Œæ²¡æœ‰æŒ‘æˆ˜â€œè¾“å…¥æœ¬èº«æ˜¯å¦æœ€ä¼˜â€è¿™ä¸ªé—®é¢˜ã€‚ | **æå‡ºäº†ä¸€ä¸ªæ›´æ ¹æœ¬çš„é—®é¢˜**ï¼š<br>ActiveVLA åœ¨æ‰¿è®¤3Dè¡¨ç¤ºé‡è¦æ€§çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æå‡ºäº† **â€œå¦‚ä½•ä¸»åŠ¨è·å–å¯¹å½“å‰ä»»åŠ¡æœ€æœ‰åˆ©çš„3Dè§‚æµ‹â€** è¿™ä¸€æ–°é—®é¢˜ã€‚å®ƒä¸æ˜¯åœ¨å·²æœ‰è§‚æµ‹ä¸Šåšæ›´å¥½çš„èåˆï¼Œè€Œæ˜¯å»**ä¼˜åŒ–è§‚æµ‹æœ¬èº«**ã€‚è¿™æ˜¯ç ”ç©¶å±‚æ¬¡ä¸Šçš„æå‡ã€‚ |\n\n### æ€»ç»“ï¼šActiveVLA çš„æ ¸å¿ƒæ”¹è¿›\n\n1.  **èŒƒå¼è½¬å˜ï¼šä»è¢«åŠ¨æ„ŸçŸ¥åˆ°ä¸»åŠ¨æ„ŸçŸ¥**\n    *   **å‰ä½œ**ï¼šç»å¤§å¤šæ•°VLAæˆ–3Dæœºå™¨äººå­¦ä¹ æ¨¡å‹éµå¾ª **â€œé™æ€è§‚æµ‹ â†’ ç­–ç•¥å†³ç­–â€** çš„**å¼€ç¯æ„ŸçŸ¥**èŒƒå¼ã€‚æœºå™¨äººè¢«åŠ¨æ¥å—ç¯å¢ƒä¿¡æ¯ã€‚\n    *   **ActiveVLA**ï¼šå¼•å…¥ **â€œå‡è®¾-æ£€éªŒâ€** çš„**é—­ç¯ä¸»åŠ¨æ„ŸçŸ¥**èŒƒå¼ã€‚å…¶æµç¨‹ä¸ºï¼š**åˆæ­¥è§‚æµ‹ â†’ å®šä½å…³é”®åŒºåŸŸ â†’ ä¸»åŠ¨é€‰æ‹©æœ€ä¼˜è§‚æµ‹è§†è§’/åˆ†è¾¨ç‡ â†’ åŸºäºä¼˜è´¨è§‚æµ‹è¿›è¡Œå†³ç­–**ã€‚è¿™ä½¿æœºå™¨äººå…·å¤‡äº†â€œä¸»åŠ¨å»çœ‹â€çš„èƒ½åŠ›ã€‚\n\n2.  **æ ¸å¿ƒæŠ€æœ¯ï¼šä¸¤é˜¶æ®µç²—åˆ°ç»†çš„ä¸»åŠ¨æ„ŸçŸ¥æœºåˆ¶**\n    *   **ç²—é˜¶æ®µ (Coarse Stage)**ï¼šç»§æ‰¿å¹¶å‘å±•äº†ç±»ä¼¼RVTçš„å¤šè§†å›¾æŠ•å½±å’Œçƒ­å›¾é¢„æµ‹æ–¹æ³•ï¼Œç”¨äº**å¿«é€Ÿå®šä½3Dåœºæ™¯ä¸­çš„ä»»åŠ¡å…³é”®åŒºåŸŸ**ã€‚è¿™ä¸å‰ä½œä¸­çš„ç©ºé—´å®šä½åŠŸèƒ½ç±»ä¼¼ï¼Œä½†å®ƒæ˜¯ä¸ºåç»­ä¸»åŠ¨æ„ŸçŸ¥æœåŠ¡çš„â€œå‡†å¤‡é˜¶æ®µâ€ã€‚\n    *   **ç»†é˜¶æ®µ (Fine Stage)**ï¼š**è¿™æ˜¯å…¨æ–°çš„æ ¸å¿ƒè´¡çŒ®**ã€‚\n        *   **ä¸»åŠ¨è§†è§’é€‰æ‹©**ï¼šåŸºäºå…³é”®åŒºåŸŸï¼Œé€šè¿‡ä¸€ä¸ªè€ƒè™‘**å¯è§æ€§ã€è·ç¦»ã€å¤šæ ·æ€§**çš„è¯„åˆ†å‡½æ•°ï¼Œåœ¨çƒé¢ä¸ŠåŠ¨æ€ç”Ÿæˆå¹¶é€‰æ‹©æœ€ä¼˜ç›¸æœºä½å§¿ã€‚è¿™ç›´æ¥è§£å†³äº†å›ºå®šè§†è§’å¸¦æ¥çš„é®æŒ¡å’Œè§†é‡å±€é™é—®é¢˜ã€‚\n        *   **ä¸»åŠ¨3Dæ”¾å¤§**ï¼šåœ¨é€‰å®šçš„æœ€ä¼˜è§†è§’ä¸Šï¼Œé€šè¿‡è™šæ‹Ÿæ¸²æŸ“ç¼©å°è§†åœºè§’(FoV)ï¼Œå®ç°å¯¹å…³é”®åŒºåŸŸçš„â€œå…‰å­¦å˜ç„¦â€ï¼Œ**åœ¨ä¸æŸå¤±åƒç´ åˆ†è¾¨ç‡çš„å‰æä¸‹æå‡ç©ºé—´åˆ†è¾¨ç‡**ã€‚è¿™ä¸“é—¨é’ˆå¯¹ç»†ç²’åº¦æ“ä½œï¼ˆå¦‚æ’å…¥ã€å¯¹å‡†ï¼‰ä¸­ç»†èŠ‚ä¿¡æ¯ä¸è¶³çš„ç—›ç‚¹ã€‚\n\n3.  **æ•ˆæœæå‡ï¼šåœ¨å¤æ‚åœºæ™¯ä¸­å®ç°æ›´é²æ£’ã€æ›´ç²¾ç¡®çš„æ“ä½œ**\n    *   å®éªŒè¡¨æ˜ï¼ŒActiveVLAåœ¨**é•¿è§†é‡ä»»åŠ¡**ã€**å­˜åœ¨ä¸¥é‡é®æŒ¡**å’Œ**éœ€è¦é«˜ç²¾åº¦**çš„åœºæ™¯ä¸­ï¼Œç›¸æ¯”æ‰€æœ‰å‰ä½œéƒ½æœ‰æ˜¾è‘—æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨RLBenchä¸Šè¾¾åˆ°91.8%çš„å¹³å‡æˆåŠŸç‡ï¼Œåœ¨COLOSSEUMçš„æ³›åŒ–æ€§æµ‹è¯•ä¸­è¡¨ç°æœ€ä¼˜ã€‚è¿™è¯æ˜äº†ä¸»åŠ¨æ„ŸçŸ¥å¯¹äºæå‡æ¨¡å‹åœ¨å¤æ‚ç°å®ä¸–ç•Œä¸­é€‚åº”æ€§å’Œå¯é æ€§çš„**å…³é”®ä»·å€¼**ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼ŒActiveVLA å¹¶éç®€å•æ”¹è¿›æŸä¸ªç‰¹å®šæ¨¡å‹ç»„ä»¶ï¼Œè€Œæ˜¯é€šè¿‡å¼•å…¥â€œä¸»åŠ¨æ„ŸçŸ¥â€è¿™ä¸€æ–°ç»´åº¦ï¼Œå¯¹ç°æœ‰VLAæ¡†æ¶è¿›è¡Œäº†æ ¹æœ¬æ€§çš„å¢å¼ºã€‚å®ƒå°†ç ”ç©¶ç„¦ç‚¹ä»â€œå¦‚ä½•æ›´å¥½åœ°ç†è§£ç»™å®šçš„ç”»é¢â€éƒ¨åˆ†è½¬å‘äº†â€œå¦‚ä½•ä¸»åŠ¨è·å–æœ€èƒ½å¸®åŠ©ç†è§£ä»»åŠ¡çš„ç”»é¢â€ï¼Œä¸ºè§£å†³æœºå™¨äººç²¾ç»†æ“ä½œå’Œé•¿è§†é‡ä»»åŠ¡ä¸­çš„æ„ŸçŸ¥ç“¶é¢ˆæä¾›äº†åˆ›æ–°æ€§çš„æ–¹æ¡ˆã€‚**",
      "timestamp": "2026-01-20T17:53:15.960108",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
      "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨æ·±å…¥åˆ†æè¿™ç¯‡é¢˜ä¸ºã€ŠActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulationã€‹çš„è®ºæ–‡ï¼Œå¹¶è¯¦ç»†é˜è¿°å…¶æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åˆ†æ\n\næœ¬æ–‡çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå°†**ä¸»åŠ¨æ„ŸçŸ¥**èƒ½åŠ›æ³¨å…¥åˆ°è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­ï¼Œä»¥è§£å†³ç°æœ‰VLAæ¨¡å‹å› ä¾èµ–é™æ€ã€å›ºå®šè§†è§’æ‘„åƒå¤´è€Œå¯¼è‡´çš„åœ¨é•¿è§†é‡ã€ç»†ç²’åº¦æ“ä½œä»»åŠ¡ä¸­æ€§èƒ½å—é™çš„é—®é¢˜ã€‚å…¶æŠ€æœ¯æ¡†æ¶é‡‡ç”¨**ç”±ç²—åˆ°ç²¾**çš„ä¸¤é˜¶æ®µèŒƒå¼ã€‚\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†è¯´æ˜äº†å…¶å…³é”®æŠ€æœ¯æ–¹æ³•ã€æ•°å­¦åŸç†å’Œå…·ä½“å‚æ•°ï¼š\n\n| æŠ€æœ¯æ¨¡å— | æ ¸å¿ƒç›®æ ‡ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°/å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. 3Då…³é”®åŒºåŸŸæ„ŸçŸ¥** | ä»3Dåœºæ™¯ä¸­å®šä½å‡ºä¸ä»»åŠ¡æœ€ç›¸å…³çš„æ ¸å¿ƒåŒºåŸŸï¼Œä½œä¸ºåç»­ä¸»åŠ¨æ„ŸçŸ¥çš„ç„¦ç‚¹ã€‚ | **a. å¤šè§†è§’æ¸²æŸ“ï¼š**<br>ä»RGB-Då›¾åƒé‡å»ºç‚¹äº‘ï¼Œå¹¶æ¸²æŸ“ä¸‰ä¸ªæ­£äº¤æŠ•å½±è§†å›¾ï¼ˆé¡¶è§†ã€å‰è§†ã€å³è§†ï¼‰ã€‚æ¯ä¸ªè§†å›¾æ¸²æŸ“7é€šé“å›¾åƒï¼šRGB (3é€šé“)ã€æ·±åº¦ (1é€šé“)ã€ä¸–ç•Œåæ ‡ç³»ä¸‹çš„ç‚¹åæ ‡ (3é€šé“)ã€‚åæ ‡é€šé“ç”¨äºå»ºç«‹è·¨è§†å›¾çš„åƒç´ å¯¹åº”å…³ç³»ã€‚<br>æ¸²æŸ“å…¬å¼ï¼š<br>`I^(v)(u_x, u_y) = Î£_{i=1}^N c_i Â· Î´( (u_x, u_y) - Ï€^(v)(p_i) )`<br>å…¶ä¸­ï¼Œ`Ï€^(v)(Â·)` æ˜¯æ­£äº¤æŠ•å½±å‡½æ•°ï¼Œ`Î´` æ˜¯ç‹„æ‹‰å…‹å‡½æ•°ï¼Œé¢œè‰² `c_i` å–è‡ªæŠ•å½±åˆ°è¯¥åƒç´ çš„æ‰€æœ‰ç‚¹ä¸­æ·±åº¦ `z_i^(v)` æœ€å°çš„ç‚¹ï¼Œä»¥æ­£ç¡®å¤„ç†é®æŒ¡ã€‚<br><br>**b. å…³é”®åŒºåŸŸæå–ï¼š**<br>å°†æ¸²æŸ“å›¾åƒä¸è¯­è¨€æŒ‡ä»¤è¾“å…¥é¢„è®­ç»ƒçš„VLMéª¨å¹²ç½‘ç»œï¼ˆå¦‚PaliGemmaï¼‰ã€‚ä¸ºäº†ä»VLMçš„å…¨å±€è¡¨å¾ä¸­æ¢å¤ç»†ç²’åº¦çš„ç©ºé—´æ³¨æ„åŠ›ï¼Œå¼•å…¥**çƒ­å›¾é¢„æµ‹æ¨¡å—**ã€‚è¯¥æ¨¡å—å°†VLMè¾“å‡ºçš„è¡¥ä¸ä»¤ç‰Œ `{t_i}_{i=1}^M` æ ¹æ®ç©ºé—´ä½ç½®é‡æ’ä¸ºç‰¹å¾ç½‘æ ¼ `H_p Ã— W_p`ï¼Œç„¶åé€šè¿‡ä¸€ä¸ª**å‡¸ä¸Šé‡‡æ ·å—** `U(Â·)` ä¸Šé‡‡æ ·è‡³è¾“å…¥å›¾åƒåˆ†è¾¨ç‡ï¼Œç”Ÿæˆçƒ­å›¾ `H`ã€‚<br>`H = U( Rearrange( {t_i}_{i=1}^M ) )`<br>å‡¸ä¸Šé‡‡æ ·å—å­¦ä¹ åƒç´ çº§æƒé‡ï¼Œæ¯”å›ºå®šæ’å€¼æ–¹æ³•èƒ½æ¢å¤æ›´ç²¾ç»†çš„ç©ºé—´ç»†èŠ‚ã€‚è®­ç»ƒæ—¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±é¢„æµ‹çƒ­å›¾ï¼Œæœ€åå°†æ‰€æœ‰è§†å›¾çš„çƒ­å›¾åæŠ•å½±åˆ°3Dç©ºé—´ï¼Œèšåˆå¾—åˆ°å…³é”®3DåŒºåŸŸï¼ˆå¦‚ç›®æ ‡ç‰©ä½“çš„è´¨å¿ƒ `p_f âˆˆ R^3`ï¼‰ã€‚ | - **æ¸²æŸ“å·¥å…·ï¼š** PyTorch3Dã€‚<br>- **è§†å›¾æ•°é‡ï¼š** 3ä¸ªæ­£äº¤è§†å›¾ï¼ˆé¡¶ã€å‰ã€å³ï¼‰ã€‚<br>- **å›¾åƒé€šé“ï¼š** 7é€šé“ (RGB + Depth + XYZä¸–ç•Œåæ ‡)ã€‚<br>- **VLMéª¨å¹²ï¼š** åŸºäºPaliGemma (SigLIPç¼–ç å™¨ + Gemmaè§£ç å™¨)ã€‚<br>- **çƒ­å›¾ä¸Šé‡‡æ ·ï¼š** å¯å­¦ä¹ çš„å‡¸ä¸Šé‡‡æ ·å—ã€‚ |\n| **2. ä¸»åŠ¨è§†è§’é€‰æ‹©** | åœ¨å·²å®šä½çš„å…³é”®åŒºåŸŸå‘¨å›´ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ä¼˜çš„ç›¸æœºè§†è§’ï¼Œä»¥æœ€å¤§åŒ–ä¿¡æ¯è·å–ã€å‡å°‘é®æŒ¡ã€‚ | é‡‡ç”¨**å‡è®¾æ£€éªŒç­–ç•¥**ï¼Œåœ¨å…³é”®ç‚¹ `p_f` ä¸ºä¸­å¿ƒçš„çƒé¢ä¸Šç”Ÿæˆä¸€ç»„å€™é€‰ç›¸æœºä½ç½® `C = {c_1, ..., c_M}`ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªå¤šç›®æ ‡è¯„åˆ†å‡½æ•°é€‰æ‹©æœ€ä¼˜çš„Kä¸ªè§†è§’ã€‚<br><br>**a. å€™é€‰ç‚¹ç”Ÿæˆï¼š**<br>é‡‡ç”¨åŸºäº**æ­£äºŒåé¢ä½“é€’å½’ç»†åˆ†**çš„æµ‹åœ°çº¿é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿çƒé¢ä¸Šçš„ç‚¹åˆ†å¸ƒå‡åŒ€ã€‚ç»†åˆ†kçº§åçš„æ€»ç‚¹æ•°å…¬å¼ä¸ºï¼š<br>`V(k) = 12 + 30k + (20/3)(4^k - 1)`<br><br>**b. è¯„åˆ†å‡½æ•°ï¼š**<br>æ¯ä¸ªå€™é€‰ä½ç½® `c_i` çš„å¾—åˆ† `s_i` æ˜¯ä»¥ä¸‹ä¸‰ä¸ªæ ‡å‡†åŒ–åˆ†æ•°çš„åŠ æƒå’Œï¼š<br>`s_i = w_vis Â· s_vis + w_dis Â· s_dis + w_div Â· s_div`<br>å…¶ä¸­ `w_vis + w_dis + w_div = 1`ã€‚<br>1. **å¯è§æ€§åˆ†æ•° `s_vis`:** åˆ¤æ–­è§†çº¿ `c_i -> p_f` æ˜¯å¦è¢«é®æŒ¡ã€‚æ²¿å°„çº¿å‡åŒ€é‡‡æ ·Nä¸ªç‚¹ `{q_k}`ï¼Œè®¡ç®—æ¯ä¸ªç‚¹åˆ°è§‚æµ‹ç‚¹äº‘ `S` çš„æœ€è¿‘è¡¨é¢è·ç¦»ï¼š`d_k = min_{sâˆˆS} ||q_k - s||`ã€‚è‹¥æ‰€æœ‰ `d_k â‰¥ r` (é˜ˆå€¼)ï¼Œåˆ™ `v(c_i, p_f)=1` (æ— é®æŒ¡)ï¼Œå¦åˆ™ä¸º0ã€‚<br>2. **è·ç¦»åˆ†æ•° `s_dis`:** åå¥½é€‚ä¸­çš„è§‚å¯Ÿè·ç¦»ã€‚å¯¹ `||c_i - p_f||` è¿›è¡Œå½’ä¸€åŒ–å’Œæ ‡å‡†åŒ–ã€‚<br>3. **å¤šæ ·æ€§åˆ†æ•° `s_div`:** é¼“åŠ±è§†è§’åœ¨å‡ ä½•ä¸Šåˆ†æ•£ã€‚è®¡ç®—å€™é€‰ `c_i` çš„è§‚å¯Ÿæ–¹å‘å‘é‡ `v_i` ä¸å…¶ä»–æ‰€æœ‰å€™é€‰æ–¹å‘å‘é‡çš„æ€»è§’åº¦å·®ï¼š<br>`S_div(c_i) = Î£_{jâ‰ i} arccos(v_i Â· v_j)` | - **é‡‡æ ·ç­–ç•¥ï¼š** æ­£äºŒåé¢ä½“é€’å½’ç»†åˆ†ã€‚<br>- **è¯„åˆ†æƒé‡ï¼š** `w_vis`, `w_dis`, `w_div` (è®ºæ–‡ä¸­æœªç»™å‡ºå…·ä½“å€¼ï¼Œéœ€è°ƒä¼˜)ã€‚<br>- **å¯è§æ€§é˜ˆå€¼ `r`:** æ ¹æ®ç‚¹äº‘å¯†åº¦è®¾å®šã€‚<br>- **é€‰æ‹©è§†è§’æ•° K:** æ ¹æ®å›¾4åˆ†æï¼Œ**æœ€ä¼˜å€¼ä¸º3**ã€‚è¶…è¿‡3ä¸ªæ€§èƒ½é¥±å’Œï¼Œè®¡ç®—æˆæœ¬å¢åŠ ã€‚ |\n| **3. ä¸»åŠ¨3Dæ”¾å¤§** | åœ¨é€‰å®šçš„æœ€ä¼˜è§†è§’ä¸Šï¼Œé€šè¿‡è™šæ‹Ÿæ¸²æŸ“è¿›è¡Œâ€œå…‰å­¦å˜ç„¦â€ï¼Œæé«˜å…³é”®åŒºåŸŸçš„å›¾åƒåˆ†è¾¨ç‡ï¼Œä»¥æ”¯æŒç»†ç²’åº¦æ“ä½œã€‚ | åœ¨é€‰å®šè§†è§’ `c_i` ä¸Šï¼Œé€šè¿‡ç¼©å°æ¸²æŸ“çš„**è§†åœºè§’(FoV)** æ¥æ¨¡æ‹Ÿæ”¾å¤§æ•ˆæœï¼Œè€Œä¸æ”¹å˜åƒç´ åˆ†è¾¨ç‡ï¼Œä»è€Œåœ¨å›¾åƒä¸­èµ‹äºˆå…³é”®åŒºåŸŸæ›´å¤šåƒç´ ã€‚<br><br>**æ•°å­¦åŸç†ï¼š**<br>è®¾åŸå§‹FoVä¸º `Î±` (å¼§åº¦)ï¼Œæ”¾å¤§å› å­ä¸º `z > 1`ï¼Œç›¸æœºåˆ°å…´è¶£åŒºåŸŸçš„è·ç¦»ä¸º `d`ã€‚åˆ™æ¸²æŸ“å›¾åƒå‚ç›´äºè§†çº¿æ–¹å‘çš„**ç©ºé—´è¦†ç›–å®½åº¦** `W(z)` ä¸ºï¼š<br>`W(z) = 2d Â· tan(Î±/(2z))`<br>éšç€ `z` å¢å¤§ï¼Œ`W(z)` å‡å°ã€‚**åƒç´ åˆ†è¾¨ç‡** `R` (åƒç´ /ç±³) å¾—ä»¥ä¿æŒï¼š<br>`R = (å›¾åƒå®½åº¦(åƒç´ )) / W(z)`<br>å› æ­¤ï¼Œæ”¾å¤§åçš„å›¾åƒä¸­ï¼Œå…´è¶£åŒºåŸŸå æ®äº†æ›´å¤šåƒç´ ï¼Œæä¾›äº†æ›´ä¸°å¯Œçš„ç»†èŠ‚ã€‚ | - **æ”¾å¤§å› å­ `z`:** æ ¹æ®å›¾4åˆ†æï¼Œ**æœ€ä¼˜å€¼ä¸º4**ã€‚è¿‡å¤§çš„ `z` ä¼šæŸå¤±ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚<br>- **å®ç°æ–¹å¼ï¼š** åœ¨3Dæ¸²æŸ“å™¨ä¸­ç›´æ¥è°ƒæ•´è™šæ‹Ÿç›¸æœºçš„FoVå‚æ•°ã€‚ |\n| **4. 3DåŠ¨ä½œé¢„æµ‹** | åŸºäºä¸»åŠ¨æ„ŸçŸ¥è·å–çš„ä¼˜åŒ–è§†å›¾ï¼Œé¢„æµ‹ç²¾ç¡®çš„6è‡ªç”±åº¦æœ«ç«¯æ‰§è¡Œå™¨åŠ¨ä½œã€‚ | **a. å¹³ç§»é¢„æµ‹ï¼š**<br>å°†ä¸»åŠ¨é€‰æ‹©å’Œæ”¾å¤§åè§†å›¾çš„2Dçƒ­å›¾åæŠ•å½±åˆ°3Dç¦»æ•£åŒ–ç½‘æ ¼ `G` ä¸Šï¼Œå½¢æˆå¤šè§†è§’å¾—åˆ†ä½“ç´ ï¼š<br>`S(g) = Î£_{v=1}^3 w_v Â· h_v( Ï€_v(g) )`<br>å…¶ä¸­ `h_v` æ˜¯è§†å›¾vçš„çƒ­å›¾ï¼Œ`Ï€_v(g)` æ˜¯ç½‘æ ¼ç‚¹ `g` åœ¨è§†å›¾vä¸Šçš„2DæŠ•å½±ï¼Œ`w_v` æ˜¯è§†å›¾æƒé‡ã€‚å¹³ç§»ç›®æ ‡ä¸ºï¼š`t* = argmax_{gâˆˆG} S(g)`ã€‚<br><br>**b. æ—‹è½¬åŠå…¶ä»–åŠ¨ä½œé¢„æµ‹ï¼š**<br>ä½¿ç”¨**å±‚æ¬¡åŒ–ç‰¹å¾èåˆæ¨¡å—**ã€‚<br>1. **å…¨å±€ä¸Šä¸‹æ–‡ç¼–ç ï¼š** å¯¹æ¯ä¸ªæ­£äº¤æŠ•å½±çš„è§†è§‰ç¼–ç å™¨è¾“å‡ºè¿›è¡Œæœ€å¤§æ± åŒ–ï¼Œå¾—åˆ°æ•è·æ•´ä½“åœºæ™¯è¯­ä¹‰çš„å…¨å±€ä»¤ç‰Œã€‚<br>2. **å±€éƒ¨ä¸Šä¸‹æ–‡ç¼–ç ï¼š** ä½¿ç”¨ROIæ„ŸçŸ¥é‡‡æ ·å™¨æå–å±€éƒ¨ä»¤ç‰Œï¼Œç¼–ç ç»†èŠ‚å¤–è§‚å’Œå‡ ä½•ã€‚<br>3. **é¢„æµ‹ï¼š** å°†æ‰€æœ‰ä»¤ç‰Œæ‹¼æ¥åï¼Œé€šè¿‡ä¸€ä¸ªMLPå¤´æ¥é¢„æµ‹ç¦»æ•£åŒ–çš„æ¬§æ‹‰è§’ `(Ï†, Î¸, Ïˆ)` (æ¯è½´72ä¸ªbins)ã€å¤¹çˆªçŠ¶æ€ `g âˆˆ {0,1}` å’Œç¢°æ’æ ‡å¿— `c âˆˆ {0,1}`ã€‚ | - **æ—‹è½¬è¡¨ç¤ºï¼š** æ¬§æ‹‰è§’ï¼Œæ¯è½´ç¦»æ•£åŒ–ä¸º**72ä¸ªbins**ã€‚<br>- **ç‰¹å¾èåˆï¼š** å…¨å±€ï¼ˆæœ€å¤§æ± åŒ–ï¼‰+ å±€éƒ¨ï¼ˆROIé‡‡æ ·ï¼‰ä»¤ç‰Œæ‹¼æ¥ã€‚<br>- **é¢„æµ‹å¤´ï¼š** MLPã€‚ |\n\n## æ€»ç»“ä¸è¯„ä»·\n\n**æŠ€æœ¯åˆ›æ–°ä»·å€¼ï¼š**\n1.  **èŒƒå¼åˆ›æ–°ï¼š** é¦–æ¬¡åœ¨VLAæ¡†æ¶ä¸­ç³»ç»Ÿæ€§åœ°é›†æˆäº†â€œ**å®šä½-é€‰æ‹©-æ”¾å¤§**â€çš„ä¸»åŠ¨æ„ŸçŸ¥é—­ç¯ï¼Œå°†æœºå™¨äººæ„ŸçŸ¥ä»è¢«åŠ¨æ¥æ”¶è½¬å˜ä¸ºä¸»åŠ¨æ¢ç´¢ã€‚\n2.  **æ–¹æ³•åŠ¡å®ï¼š** æå‡ºçš„ä¸¤é¡¹æ ¸å¿ƒæŠ€æœ¯ï¼ˆ**ä¸»åŠ¨è§†è§’é€‰æ‹©**å’Œ**ä¸»åŠ¨3Dæ”¾å¤§**ï¼‰å‡æœ‰æ¸…æ™°çš„æ•°å­¦å»ºæ¨¡å’Œå¯æ“ä½œçš„ç®—æ³•å®ç°ï¼Œä¸æ˜¯ç©ºæ´çš„æ¦‚å¿µã€‚\n3.",
      "timestamp": "2026-01-20T17:54:20.133711",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
      "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨æ·±å…¥åˆ†æè¿™ç¯‡å…³äº **ActiveVLA** çš„è®ºæ–‡ä¸­ä½¿ç”¨çš„è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ã€‚\n\n## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\nè¿™ç¯‡è®ºæ–‡é€šè¿‡**ä¸‰ä¸ªæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•**å’Œä¸€ä¸ª**çœŸå®æœºå™¨äººå®éªŒ**æ¥å…¨é¢è¯„ä¼°å…¶æå‡ºçš„ ActiveVLA æ¡†æ¶ã€‚å…¶æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡æ˜¯**ä»»åŠ¡æˆåŠŸç‡**ï¼Œå¹¶åœ¨ä¸åŒåŸºå‡†ä¸Šè¾…ä»¥**å¹³å‡æ’å**ç­‰æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡è¯¦è§£\n\n| æŒ‡æ ‡åç§° | å®šä¹‰ä¸è®¡ç®—æ–¹å¼ | åœ¨æœ¬æ–‡ä¸­çš„ä½œç”¨ä¸æ„ä¹‰ |\n| :--- | :--- | :--- |\n| **æˆåŠŸç‡** | åœ¨å¤šæ¬¡ç‹¬ç«‹è¯•éªŒä¸­ï¼Œæœºå™¨äººæˆåŠŸå®ŒæˆæŒ‡å®šè¯­è¨€æŒ‡ä»¤ä»»åŠ¡çš„æ¬¡æ•°å æ€»è¯•éªŒæ¬¡æ•°çš„ç™¾åˆ†æ¯”ã€‚ <br> **å…¬å¼**: `SR = (æˆåŠŸæ¬¡æ•° / æ€»è¯•éªŒæ¬¡æ•°) * 100%` | **æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡**ã€‚ç›´æ¥è¡¡é‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚è®ºæ–‡ä¸­æ‰€æœ‰è¡¨æ ¼ï¼ˆè¡¨1-3ï¼Œ5-6ï¼‰å‡ä»¥æ­¤ä¸ºä¸»è¦æ•°æ®ã€‚æ•°å€¼è¶Šé«˜ï¼Œä»£è¡¨æ¨¡å‹æ€§èƒ½è¶Šå¥½ã€‚ |\n| **å¹³å‡æ’å** | åœ¨æ‰€æœ‰è¢«æ¯”è¾ƒçš„æ¨¡å‹ï¼ˆBaselinesï¼‰ä¸­ï¼Œæ ¹æ®å…¶åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡è¿›è¡Œæ’åºï¼ˆ1ä¸ºæœ€ä½³ï¼‰ï¼Œç„¶åè®¡ç®—è¯¥æ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šçš„å¹³å‡æ’åã€‚ <br> **å…¬å¼**: `Avg. Rank = Î£(æ¯ä¸ªä»»åŠ¡çš„æ’å) / ä»»åŠ¡æ€»æ•°` | **ç»¼åˆç«äº‰åŠ›æŒ‡æ ‡**ã€‚ç”¨äºåœ¨RLBenchå’ŒCOLOSSEUMä¸Šç»¼åˆæ¯”è¾ƒå¤šä¸ªæ¨¡å‹ã€‚**æ•°å€¼è¶Šä½è¶Šå¥½**ï¼Œè¡¨æ˜æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„æ•´ä½“è¡¨ç°æ›´ä¼˜ã€æ›´ç¨³å®šã€‚ |\n| **å¹³å‡æˆåŠŸç‡** | åœ¨åŒ…å«å¤šä¸ªä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼ˆå¦‚RLBenchçš„18ä¸ªä»»åŠ¡ï¼‰ä¸Šï¼Œè®¡ç®—æ‰€æœ‰ä»»åŠ¡æˆåŠŸç‡çš„ç®—æœ¯å¹³å‡å€¼ã€‚ | **åŸºå‡†æ•´ä½“æ€§èƒ½æŒ‡æ ‡**ã€‚ç”¨äºå¿«é€Ÿå¯¹æ¯”ä¸åŒæ¨¡å‹åœ¨ä¸€ä¸ªåŸºå‡†ä¸Šçš„æ•´ä½“è¡¨ç°ï¼ˆå¦‚è¡¨1çš„â€œAvg. SRâ€åˆ—ï¼‰ã€‚æ˜¯åˆ¤æ–­æ¨¡å‹æ˜¯å¦è¾¾åˆ°â€œState-of-the-Artâ€çš„å…³é”®ä¾æ®ã€‚ |\n| **æ¨ç†æ—¶é—´** | æ¨¡å‹å¤„ç†ä¸€æ¬¡è§‚æµ‹å¹¶è¾“å‡ºä¸€ä¸ªåŠ¨ä½œæ‰€èŠ±è´¹çš„æ—¶é—´ï¼ˆç§’ï¼‰ã€‚åœ¨æ¶ˆèå®éªŒï¼ˆè¡¨4ï¼‰ä¸­æŠ¥å‘Šã€‚ | **æ•ˆç‡æŒ‡æ ‡**ã€‚è¡¡é‡æ¨¡å‹çš„è®¡ç®—å¼€é”€ã€‚ActiveVLAé€šè¿‡å¼•å…¥ä¸»åŠ¨æ„ŸçŸ¥ï¼ˆA-VS, A-3Zï¼‰æå‡äº†æ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†å°‘é‡è®¡ç®—æ—¶é—´ï¼Œè®ºæ–‡å¯¹æ­¤è¿›è¡Œäº†æƒè¡¡åˆ†æã€‚ |\n\n### äºŒã€ æ•°æ®é›†/åŸºå‡†æµ‹è¯•è¯¦è§£\n\nè®ºæ–‡ä½¿ç”¨äº†ä¸‰ä¸ªå…¬å¼€çš„ã€å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨¡æ‹Ÿæœºå™¨äººæ“ä½œåŸºå‡†ï¼Œä»¥åŠè‡ªè¡Œè®¾è®¡çš„çœŸå®ä¸–ç•Œå®éªŒã€‚\n\n| æ•°æ®é›†/åŸºå‡†åç§° | æ ¸å¿ƒç‰¹ç‚¹ä¸æŒ‘æˆ˜ | åœ¨æœ¬æ–‡ä¸­çš„å…·ä½“ç”¨é€”ä¸è¯„ä»·é‡ç‚¹ |\n| :--- | :--- | :--- |\n| **RLBench** | - **ä»»åŠ¡å¤šæ ·æ€§**ï¼šåŒ…å«18ä¸ªé•¿çŸ­è§†ç•Œã€ç²—ç»†ç²’åº¦ä¸åŒçš„ä»»åŠ¡ï¼ˆå¦‚å…³é—­ç½å­ã€æ’å…¥é”€é’‰ã€å †å æ¯å­ç­‰ï¼‰ã€‚<br>- **æ ‡å‡†ç¯å¢ƒ**ï¼šä½¿ç”¨Franka Pandaæœºå™¨äººï¼Œæä¾›4ä¸ªå›ºå®šæ ¡å‡†ç›¸æœºçš„RGB-Dè§‚æµ‹å’Œ100æ¡ä¸“å®¶ç¤ºæ•™è½¨è¿¹ã€‚<br>- **åŸºç¡€æ€§èƒ½æµ‹è¯•**ï¼šç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç»“æ„åŒ–ã€å¤šæ ·åŒ–ä»»åŠ¡ä¸Šçš„åŸºæœ¬æ“ä½œèƒ½åŠ›ã€‚ | **ä¸»è¦æ€§èƒ½åŸºå‡†**ã€‚ç”¨äºè¯æ˜ActiveVLAåœ¨**ç²¾ç¡®æ“ä½œ**ï¼ˆå¦‚`Insert Peg`ï¼‰å’Œ**æ¥è§¦ä¸°å¯Œ**ï¼ˆå¦‚`Open Drawer`ï¼‰ä»»åŠ¡ä¸Šçš„ä¼˜åŠ¿ã€‚è¡¨1å±•ç¤ºäº†ActiveVLAä»¥**91.8%** çš„å¹³å‡æˆåŠŸç‡å–å¾—SOTAï¼Œå¹¶åœ¨10ä¸ªä»»åŠ¡ä¸­æ’åç¬¬ä¸€ã€‚ |\n| **COLOSSEUM** | - **æ‰°åŠ¨é²æ£’æ€§**ï¼šåœ¨RLBenchåŸºç¡€ä¸Šï¼Œå¼•å…¥äº†**12ç§ç³»ç»Ÿæ€§æ‰°åŠ¨**ï¼ŒåŒ…æ‹¬ç‰©ä½“é¢œè‰²/çº¹ç†/å°ºå¯¸å˜åŒ–ã€å…‰ç…§å˜åŒ–ã€èƒŒæ™¯çº¹ç†ã€å¹²æ‰°ç‰©ã€ç›¸æœºä½å§¿å˜åŒ–ç­‰ã€‚<br>- **æ³›åŒ–èƒ½åŠ›æµ‹è¯•**ï¼šä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°æ¨¡å‹å¯¹è§†è§‰å’Œåœºæ™¯å˜åŒ–çš„é²æ£’æ€§ã€‚ | **é²æ£’æ€§ä¸æ³›åŒ–æ€§åŸºå‡†**ã€‚ç”¨äºéªŒè¯ActiveVLAçš„**ä¸»åŠ¨æ„ŸçŸ¥èƒ½åŠ›**åœ¨åº”å¯¹é®æŒ¡ã€è§†è§’å˜åŒ–ã€å¤–è§‚å˜åŒ–æ—¶çš„ä»·å€¼ã€‚è¡¨2æ˜¾ç¤ºActiveVLAåœ¨**æ‰€æœ‰æ‰°åŠ¨ç±»å‹**ä¸Šå‡å–å¾—æœ€ä½³æˆ–æ¥è¿‘æœ€ä½³æ€§èƒ½ï¼ˆå¹³å‡æˆåŠŸç‡65.9%ï¼‰ï¼Œè¯æ˜äº†å…¶å¼ºå¤§çš„è§†è§‰ä¸å˜æ€§è¡¨å¾å­¦ä¹ èƒ½åŠ›ã€‚ |\n| **GemBench** | - **æŠ€èƒ½ç»„åˆæ€§**ï¼šå°†æ“ä½œåˆ†è§£ä¸º7ä¸ªæ ¸å¿ƒåŠ¨ä½œåŸºå…ƒï¼ˆå¦‚æŠ“å–ã€æ”¾ç½®ã€æ¨åŠ¨ç­‰ï¼‰ï¼Œå¹¶ç»„åˆæˆ16ä¸ªè®­ç»ƒä»»åŠ¡å’Œ44ä¸ªæµ‹è¯•ä»»åŠ¡ã€‚<br>- **é•¿è§†ç•Œä¸ç»„åˆæ³›åŒ–**ï¼šæµ‹è¯•æ¨¡å‹èƒ½å¦å°†å­¦ä¼šçš„åŸºå…ƒæŠ€èƒ½é‡æ–°ç»„åˆï¼Œä»¥è§£å†³å…¨æ–°çš„ã€æœªè§è¿‡çš„å¤åˆä»»åŠ¡ã€‚ | **ç»„åˆæ³›åŒ–ä¸é•¿è§†ç•Œæ¨ç†åŸºå‡†**ã€‚ç”¨äºè¯„ä¼°æ¨¡å‹è¶…è¶Šæ¨¡ä»¿å­¦ä¹ ã€è¿›è¡Œ**ä»»åŠ¡åˆ†è§£å’Œé‡ç»„**çš„èƒ½åŠ›ã€‚è¡¨3æ˜¾ç¤ºActiveVLAåœ¨L1-L3çº§åˆ«ä»»åŠ¡ä¸Šå‡é¢†å…ˆï¼Œæ•´ä½“å¹³å‡æˆåŠŸç‡ï¼ˆ51.3%ï¼‰è¾¾åˆ°SOTAï¼Œè¯æ˜äº†å…¶3Dæ„ŸçŸ¥å’Œä¸»åŠ¨è§‚å¯Ÿå¯¹å¤æ‚ä»»åŠ¡æ¨ç†çš„è´¡çŒ®ã€‚ |\n| **çœŸå®ä¸–ç•Œå®éªŒ** | - **å¤æ‚é®æŒ¡ä¸çœŸå®å™ªå£°**ï¼šè®¾è®¡äº†4ä¸ªå…·æœ‰**ä¸¥é‡ç©ºé—´é®æŒ¡**ã€**å¤æ‚å‡ ä½•ç»“æ„**å’Œ**ç²¾ç»†æ“ä½œè¦æ±‚**çš„ä»»åŠ¡ï¼ˆå¦‚ä»åˆ†å±‚æŠ½å±‰å–æ¯›å·¾ã€åœ¨æ°´æœå †ä¸­æŠ“è¢«é®æŒ¡çš„é¦™è•‰ç­‰ï¼‰ã€‚<br>- **ä»¿çœŸåˆ°ç°å®è¿ç§»**ï¼šä½¿ç”¨KINOVA GEN2æœºå™¨äººå’ŒRealSense D455ç›¸æœºï¼Œåœ¨â€œçœ¼åœ¨æ‰‹å¤–â€é…ç½®ä¸‹è¿›è¡Œã€‚ | **å®é™…åº”ç”¨ä¸ä»¿çœŸåˆ°ç°å®è¿ç§»éªŒè¯**ã€‚è¿™æ˜¯è®ºæ–‡çš„**å…³é”®å®é™…ä»·å€¼**ä½“ç°ã€‚å®ƒè¯æ˜äº†ActiveVLAçš„æ¡†æ¶ä¸ä»…èƒ½â€œåˆ·æ¦œâ€ï¼Œæ›´èƒ½è§£å†³çœŸå®åœºæ™¯ä¸­çš„æ ¸å¿ƒéš¾é¢˜â€”â€”**éƒ¨åˆ†å¯è§‚æµ‹æ€§**ã€‚è¡¨5å’Œå®šæ€§ç»“æœï¼ˆå›¾3ï¼Œ7ï¼‰è¡¨æ˜ï¼ŒActiveVLAé€šè¿‡ä¸»åŠ¨é€‰æ‹©è§†è§’å’Œå±€éƒ¨æ”¾å¤§ï¼Œåœ¨é«˜åº¦é®æŒ¡çš„çœŸå®ä»»åŠ¡ä¸­å–å¾—äº†**æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿**çš„æˆåŠŸç‡ï¼ˆæ•´ä½“91.8%ï¼‰ã€‚ |\n\n### æ€»ç»“\n\n**æŠ€æœ¯åˆ›æ–°ä¸å®é™…ä»·å€¼çš„å…³è”**ï¼š\n- **è¯„ä»·æŒ‡æ ‡**ï¼ˆé«˜æˆåŠŸç‡ã€ä½å¹³å‡æ’åï¼‰**é‡åŒ–åœ°è¯æ˜äº†**ActiveVLAæ¡†æ¶ï¼ˆä¸»åŠ¨è§†è§’é€‰æ‹© + ä¸»åŠ¨3Dæ”¾å¤§ï¼‰çš„æœ‰æ•ˆæ€§ã€‚\n- **æ•°æ®é›†çš„é€‰æ‹©**å…·æœ‰æ˜ç¡®çš„é€’è¿›é€»è¾‘ï¼š**RLBench**è¯æ˜åŸºç¡€æ€§èƒ½ â†’ **COLOSSEUM**è¯æ˜è§†è§‰é²æ£’æ€§ â†’ **GemBench**è¯æ˜ç»„åˆæ¨ç†èƒ½åŠ› â†’ **çœŸå®å®éªŒ**è¯æ˜è§£å†³å®é™…æ ¸å¿ƒé—®é¢˜ï¼ˆé®æŒ¡ï¼‰çš„èƒ½åŠ›ã€‚\n- è¿™ç§è¯„ä¼°ä½“ç³»æ¸…æ™°åœ°è¡¨æ˜ï¼ŒActiveVLAçš„**æ ¸å¿ƒè´¡çŒ®**ä¸ä»…æ˜¯æå‡åŸºå‡†æµ‹è¯•åˆ†æ•°ï¼Œæ›´æ˜¯ä¸ºæœºå™¨äººæä¾›äº†ä¸€ç§**åº”å¯¹çœŸå®ä¸–ç•Œä¸ç¡®å®šæ€§å’Œéƒ¨åˆ†å¯è§‚æµ‹æ€§çš„é€šç”¨æ„ŸçŸ¥-è¡ŒåŠ¨èŒƒå¼**ï¼Œå…¶**å®é™…ä»·å€¼**åœ¨äºè®©æœºå™¨äººåœ¨å¤æ‚ã€éç»“æ„åŒ–çš„çœŸå®ç¯å¢ƒä¸­å®Œæˆé«˜ç²¾åº¦ã€ç»†ç²’åº¦çš„æ“ä½œä»»åŠ¡ã€‚",
      "timestamp": "2026-01-20T17:55:06.576265",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
      "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ä¸‰ä¸ª**æ¨¡æ‹Ÿä»¿çœŸåŸºå‡†**å’Œä¸€ä¸ª**çœŸå®æœºå™¨äººå¹³å°**ä¸Šè¿›è¡Œäº†å…¨é¢å®éªŒï¼Œä»¥éªŒè¯ActiveVLAæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚\n\n#### 1. æ¨¡æ‹Ÿä»¿çœŸåŸºå‡†\n- **RLBench**ï¼šåŒ…å«18ä¸ªé•¿è§†é‡ã€ç²¾ç»†æ“ä½œä»»åŠ¡ï¼ˆå¦‚æ’å…¥é”€é’‰ã€æ‰“å¼€æŠ½å±‰ã€å †å æ¯å­ç­‰ï¼‰ï¼Œä½¿ç”¨Franka Pandaæœºå™¨äººï¼Œæ¯ä¸ªä»»åŠ¡æä¾›100æ¡ä¸“å®¶æ¼”ç¤ºï¼Œè¾“å…¥ä¸º4ä¸ªæ ¡å‡†ç›¸æœºçš„RGB-Då›¾åƒã€‚\n- **COLOSSEUM**ï¼šåŸºäºRLBenchæ‰©å±•çš„é²æ£’æ€§è¯„ä¼°åŸºå‡†ï¼Œå¼•å…¥12ç§æ‰°åŠ¨ç±»å‹ï¼ˆå¦‚ç‰©ä½“é¢œè‰²ã€çº¹ç†ã€å°ºå¯¸ã€å…‰ç…§ã€ç›¸æœºä½å§¿ç­‰å˜åŒ–ï¼‰ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹åœ¨åˆ†å¸ƒåç§»ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚\n- **GemBench**ï¼šåŸºäºRLBenchçš„å±‚æ¬¡åŒ–ç»„åˆæ³›åŒ–åŸºå‡†ï¼ŒåŒ…å«16ä¸ªè®­ç»ƒä»»åŠ¡å’Œ44ä¸ªæµ‹è¯•ä»»åŠ¡ï¼Œæ¶µç›–7ç§æ ¸å¿ƒåŠ¨ä½œåŸè¯­ï¼ˆå¦‚æŠ“å–ã€æ”¾ç½®ã€æ¨ç­‰ï¼‰ï¼Œè¯„ä¼°æ¨¡å‹å¯¹æœªçŸ¥ä»»åŠ¡ç»„åˆçš„é€‚åº”èƒ½åŠ›ã€‚\n\n#### 2. çœŸå®æœºå™¨äººå®éªŒ\n- **å¹³å°**ï¼šKINOVA GEN2æœºå™¨äºº + RealSense D455ç›¸æœºï¼ˆçœ¼åˆ°æ‰‹é…ç½®ï¼‰ã€‚\n- **ä»»åŠ¡**ï¼šåŒ…æ‹¬4ä¸ªå¤æ‚é®æŒ¡åœºæ™¯çš„ç²¾ç»†æ“ä½œä»»åŠ¡ï¼ˆå¦‚ä»åˆ†å±‚æŠ½å±‰å–æ¯›å·¾ã€åœ¨æ°´æœå †ä¸­æŠ“å–è¢«é®æŒ¡çš„é¦™è•‰ã€ä»æ‚¬æŒ‚æ¶å–ç´«è‰²æ¯å­ç­‰ï¼‰ã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\n- **æ ¸å¿ƒæŒ‡æ ‡**ï¼š**å¹³å‡æˆåŠŸç‡ï¼ˆAverage Success Rate, %ï¼‰**ï¼Œå³ä»»åŠ¡æˆåŠŸå®Œæˆçš„ç™¾åˆ†æ¯”ã€‚\n- **è¾…åŠ©æŒ‡æ ‡**ï¼š\n  - **å¹³å‡æ’åï¼ˆAverage Rankï¼‰**ï¼šåœ¨æ‰€æœ‰ä»»åŠ¡æˆ–æ‰°åŠ¨åœºæ™¯ä¸­çš„å¹³å‡æ’åï¼Œå€¼è¶Šä½è¡¨ç¤ºæ•´ä½“æ€§èƒ½è¶Šä¼˜ã€‚\n  - **æ¨ç†æ—¶é—´ï¼ˆInference Timeï¼‰**ï¼šå•æ¬¡å†³ç­–çš„è€—æ—¶ï¼ˆç§’ï¼‰ï¼Œç”¨äºè¯„ä¼°è®¡ç®—æ•ˆç‡ã€‚\n\n### ä¸‰ã€æ€§èƒ½æå‡ä¸å®éªŒç»“æœ\n\n#### 1. RLBenchåŸºå‡†\n- **ActiveVLAå¹³å‡æˆåŠŸç‡**ï¼š**91.8%**ï¼Œåˆ›ä¸‹æ–°çºªå½•ã€‚\n- **å¯¹æ¯”æœ€ä½³åŸºçº¿ï¼ˆBridgeVLAï¼‰**ï¼šæå‡ **3.6ä¸ªç™¾åˆ†ç‚¹**ï¼ˆBridgeVLAä¸º88.2%ï¼‰ã€‚\n- **å…³é”®ä»»åŠ¡è¡¨ç°**ï¼š\n  - `Insert Peg`ã€`Open Drawer`ç­‰ç²¾ç»†æ“ä½œä»»åŠ¡è¾¾åˆ° **100%** æˆåŠŸç‡ã€‚\n  - åœ¨é®æŒ¡ä¸¥é‡çš„ä»»åŠ¡ï¼ˆå¦‚`Place Cups`ï¼‰ä¸­å–å¾— **65.6%** çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚\n- **å¹³å‡æ’å**ï¼š**1.22**ï¼ˆæ’åç¬¬ä¸€ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚\n\n#### 2. COLOSSEUMåŸºå‡†\n- **ActiveVLAå¹³å‡æˆåŠŸç‡**ï¼š**65.9%**ï¼Œæ’åç¬¬ä¸€ã€‚\n- **å¯¹æ¯”æœ€ä½³åŸºçº¿ï¼ˆBridgeVLAï¼‰**ï¼šæå‡ **1.9ä¸ªç™¾åˆ†ç‚¹**ï¼ˆBridgeVLAä¸º64.0%ï¼‰ã€‚\n- **é²æ£’æ€§è¡¨ç°**ï¼š\n  - åœ¨ç‰©ä½“å°ºå¯¸å˜åŒ–ï¼ˆMO-SIZEï¼‰ä¸Šè¾¾åˆ° **72.4%** æˆåŠŸç‡ã€‚\n  - åœ¨ç›¸æœºä½å§¿æ‰°åŠ¨ä¸‹è¾¾åˆ° **76.3%** æˆåŠŸç‡ï¼Œæ˜¾ç¤ºå¯¹è§†è§’å˜åŒ–çš„é«˜é€‚åº”æ€§ã€‚\n- **å¹³å‡æ’å**ï¼š**1.07**ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚\n\n#### 3. GemBenchåŸºå‡†\n- **ActiveVLAå¹³å‡æˆåŠŸç‡**ï¼š**51.3%**ï¼Œæ’åç¬¬ä¸€ã€‚\n- **å¯¹æ¯”æœ€ä½³åŸºçº¿ï¼ˆBridgeVLAï¼‰**ï¼šæå‡ **1.3ä¸ªç™¾åˆ†ç‚¹**ï¼ˆBridgeVLAä¸º50.0%ï¼‰ã€‚\n- **å±‚æ¬¡ä»»åŠ¡è¡¨ç°**ï¼š\n  - åœ¨åŸºç¡€ä»»åŠ¡ï¼ˆL1ï¼‰ä¸Šè¾¾åˆ° **92.4%** æˆåŠŸç‡ã€‚\n  - åœ¨å¤æ‚ç»„åˆä»»åŠ¡ï¼ˆL4ï¼‰ä¸Šå–å¾— **1.2%** çš„æˆåŠŸç‡ï¼Œè™½ä½ä½†ä¼˜äºå¤šæ•°åŸºçº¿ï¼ˆè®¸å¤šåŸºçº¿ä¸º0%ï¼‰ã€‚\n\n#### 4. çœŸå®æœºå™¨äººå®éªŒ\n- **ActiveVLAå¹³å‡æˆåŠŸç‡**ï¼š**91.8%**ï¼ˆåŸºäº4ä¸ªä»»åŠ¡ï¼‰ã€‚\n- **å¯¹æ¯”æœ€ä½³åŸºçº¿ï¼ˆRVT-2ï¼‰**ï¼šæå‡ **19.3ä¸ªç™¾åˆ†ç‚¹**ï¼ˆRVT-2ä¸º72.5%ï¼‰ã€‚\n- **ä»»åŠ¡çº§æå‡**ï¼š\n  - å–æ¯›å·¾ä»»åŠ¡ï¼šç›¸æ¯”TriVLAæå‡ **24ä¸ªç™¾åˆ†ç‚¹**ã€‚\n  - çº¢ç»¿ç§¯æœ¨å †å ä»»åŠ¡ï¼šç›¸æ¯”TriVLAæå‡ **41ä¸ªç™¾åˆ†ç‚¹**ã€‚\n\n### å››ã€æ€§èƒ½æå‡çš„æ ¸å¿ƒåŸå› \n1. **ä¸»åŠ¨æ„ŸçŸ¥æœºåˆ¶**ï¼šé€šè¿‡**ä¸»åŠ¨è§†è§’é€‰æ‹©**å’Œ**3Då±€éƒ¨æ”¾å¤§**ï¼Œæœ‰æ•ˆå‡å°‘äº†é®æŒ¡é—®é¢˜ï¼Œæå‡äº†ç²¾ç»†æ“ä½œçš„ç²¾åº¦ã€‚\n2. **ç²—åˆ°ç»†çš„æ¶æ„**ï¼šå…ˆå®šä½å…³é”®3DåŒºåŸŸï¼Œå†é’ˆå¯¹æ€§ä¼˜åŒ–æ„ŸçŸ¥ï¼Œå¹³è¡¡äº†å…¨å±€ä¸Šä¸‹æ–‡ä¸å±€éƒ¨ç»†èŠ‚ã€‚\n3. **å¤šè§†å›¾èåˆ**ï¼šåˆ©ç”¨3ä¸ªä¼˜åŒ–è§†å›¾è¿›è¡Œå†³ç­–ï¼Œå¢å¼ºäº†ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚\n\n### äº”ã€å…³é”®è¡¨æ ¼æ‘˜è¦\n| åŸºå‡†         | ActiveVLAæˆåŠŸç‡ | æœ€ä½³åŸºçº¿æˆåŠŸç‡ | æå‡å¹…åº¦ | å¹³å‡æ’å |\n|--------------|-----------------|----------------|----------|----------|\n| RLBench      | 91.8%           | 88.2% (BridgeVLA) | +3.6%    | 1.22     |\n| COLOSSEUM    | 65.9%           | 64.0% (BridgeVLA) | +1.9%    | 1.07     |\n| GemBench     | 51.3%           | 50.0% (BridgeVLA) | +1.3%    | -        |\n| çœŸå®æœºå™¨äºº   | 91.8%           | 72.5% (RVT-2)     | +19.3%   | -        |\n\n**ç»“è®º**ï¼šActiveVLAåœ¨å¤šä¸ªåŸºå‡†ä¸Šå‡å®ç°äº†**æœ€å…ˆè¿›çš„æ€§èƒ½**ï¼Œå°¤å…¶åœ¨çœŸå®ä¸–ç•Œå¤æ‚é®æŒ¡åœºæ™¯ä¸­è¡¨ç°çªå‡ºï¼ŒéªŒè¯äº†ä¸»åŠ¨æ„ŸçŸ¥å¯¹ç²¾ç»†3Dæ“ä½œçš„æœ‰æ•ˆæ€§ã€‚",
      "timestamp": "2026-01-20T17:55:46.136993",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
      "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡ã€ŠActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulationã€‹çš„æ·±å…¥åˆ†æï¼Œå…¶æ ¸å¿ƒå±€é™æ€§åŠæœªæ¥æ”¹è¿›æ–¹å‘å¦‚ä¸‹ï¼š\n\n### ä¸»è¦å±€é™æ€§\n\n- **è®¡ç®—å¼€é”€ä¸å®æ—¶æ€§**ï¼šè®ºæ–‡æŒ‡å‡ºï¼Œå¼•å…¥ä¸»åŠ¨æ„ŸçŸ¥ï¼ˆå¦‚åŠ¨æ€è§†è§’é€‰æ‹©å’Œ3Dæ”¾å¤§ï¼‰ä¼šå¢åŠ æ¨ç†æ—¶é—´ã€‚ä¾‹å¦‚ï¼Œåœ¨RLBenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®Œæ•´ActiveVLAçš„æ¨ç†æ—¶é—´ï¼ˆ0.53ç§’ï¼‰æ˜¾è‘—é«˜äºå›ºå®šè§†è§’åŸºçº¿ï¼ˆ0.26ç§’ï¼‰ã€‚è¿™**å¯èƒ½é™åˆ¶å…¶åœ¨éœ€è¦é«˜é¢‘æ§åˆ¶æˆ–å®æ—¶å“åº”çš„åŠ¨æ€ç¯å¢ƒä¸­çš„åº”ç”¨**ã€‚\n\n- **å¯¹æç«¯å¤æ‚åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›æœ‰é™**ï¼šå°½ç®¡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼ˆå¦‚GemBenchçš„L4çº§åˆ«ä»»åŠ¡ï¼‰ä¸Šï¼ŒæˆåŠŸç‡ä»…ä¸º1.2%ã€‚è¿™è¡¨æ˜æ¨¡å‹**åœ¨å¤„ç†æå…¶å¤æ‚ã€é•¿è§†é‡æˆ–éœ€è¦é«˜åº¦ç»„åˆæ¨ç†çš„ä»»åŠ¡æ—¶ä»é¢ä¸´å›°éš¾**ã€‚\n\n- **ä¾èµ–ä»¿çœŸçš„3Dæ•°æ®ä¸æ¸²æŸ“**ï¼šæ ¸å¿ƒçš„â€œä¸»åŠ¨3Dæ”¾å¤§â€æœºåˆ¶ä¾èµ–äºä»ç‚¹äº‘è¿›è¡Œé«˜è´¨é‡çš„å¤šè§†è§’æ¸²æŸ“å’Œè™šæ‹Ÿå˜ç„¦ã€‚è¿™**åœ¨ç°å®ä¸–ç•Œä¸­å¯èƒ½é¢ä¸´æŒ‘æˆ˜**ï¼Œä¾‹å¦‚ä¼ æ„Ÿå™¨å™ªå£°ã€ç‚¹äº‘ç¨€ç–æˆ–ä¸å®Œæ•´ã€ä»¥åŠåŠ¨æ€ç‰©ä½“å¯¼è‡´çš„å‡ ä½•é‡å»ºè¯¯å·®ï¼Œå¯èƒ½å½±å“å˜ç„¦åè§†å›¾çš„è´¨é‡å’ŒåŠ¨ä½œé¢„æµ‹çš„ç²¾åº¦ã€‚\n\n- **ä¸»åŠ¨æ„ŸçŸ¥ç­–ç•¥çš„ä¼˜åŒ–ä¸å¯è§£é‡Šæ€§**ï¼šè§†è§’é€‰æ‹©å’Œå˜ç„¦ç­–ç•¥åŸºäºä¸€ç»„æ‰‹å·¥è®¾è®¡çš„è¯„åˆ†å‡½æ•°ï¼ˆå¯è§æ€§ã€è·ç¦»ã€å¤šæ ·æ€§ï¼‰å’Œæƒé‡ã€‚**è¿™äº›å¯å‘å¼è§„åˆ™å¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„ï¼Œä¸”ç¼ºä¹ä»ä»»åŠ¡ç›®æ ‡ç«¯åˆ°ç«¯å­¦ä¹ æ„ŸçŸ¥ç­–ç•¥çš„èƒ½åŠ›**ã€‚æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ï¼ˆä¸ºä½•é€‰æ‹©æŸä¸ªç‰¹å®šè§†è§’ï¼‰çš„å¯è§£é‡Šæ€§ä¹Ÿæœ‰å¾…åŠ å¼ºã€‚\n\n- **è®­ç»ƒæ•°æ®ä¸è®¡ç®—èµ„æºéœ€æ±‚**ï¼šæ¨¡å‹åŸºäºå¤§è§„æ¨¡é¢„è®­ç»ƒçš„VLMï¼ˆPaliGemmaï¼‰å¹¶åœ¨å¤šä¸ªå¤§å‹ä»¿çœŸæ•°æ®é›†ï¼ˆRLBench, COLOSSEUM, GemBenchï¼‰ä¸Šè¿›è¡Œå¾®è°ƒï¼Œ**éœ€è¦æµ·é‡çš„ç¤ºèŒƒæ•°æ®å’Œæ˜‚è´µçš„GPUè®¡ç®—èµ„æºï¼ˆå¦‚NVIDIA H100ï¼‰**ï¼Œè¿™é™åˆ¶äº†å…¶å¯è®¿é—®æ€§å’Œå¿«é€Ÿè¿­ä»£çš„å¯èƒ½æ€§ã€‚\n\n### æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\n- **æå‡è®¡ç®—æ•ˆç‡ä¸å®æ—¶æ€§èƒ½**ï¼š\n    - å¼€å‘æ›´è½»é‡çº§çš„è§†è§’è¯„ä¼°ç½‘ç»œï¼Œæ›¿ä»£è€—æ—¶çš„åŸºäºå‡ ä½•çš„å¯è§æ€§è®¡ç®—ï¼ˆå¦‚KDTreeæœç´¢ï¼‰ã€‚\n    - ç ”ç©¶æ¨¡å‹è’¸é¦æˆ–æ¶æ„æœç´¢æŠ€æœ¯ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‡å°‘VLMä¸»å¹²ç½‘ç»œçš„è®¡ç®—é‡ã€‚\n    - å®ç°**å¼‚æ­¥æ„ŸçŸ¥-åŠ¨ä½œç®¡é“**ï¼Œè®©ä¸»åŠ¨æ„ŸçŸ¥æ¨¡å—åœ¨æœºå™¨äººæ‰§è¡Œå½“å‰åŠ¨ä½œæ—¶å¹¶è¡Œè§„åˆ’ä¸‹ä¸€ä¸ªæœ€ä½³è§†è§’ã€‚\n\n- **å¢å¼ºå¯¹å¤æ‚ä¸æœªçŸ¥ä»»åŠ¡çš„æ³›åŒ–**ï¼š\n    - ç»“åˆ**ä¸–ç•Œæ¨¡å‹æˆ–åŠ¨åŠ›å­¦æ¨¡å‹**è¿›è¡Œæ›´é•¿è¿œçš„å‰ç»æ€§è§„åˆ’ï¼Œä»¥åº”å¯¹é•¿è§†é‡ä»»åŠ¡ã€‚\n    - å¼•å…¥**å…ƒå­¦ä¹ æˆ–åœ¨çº¿é€‚åº”æœºåˆ¶**ï¼Œä½¿æœºå™¨äººèƒ½åœ¨å°‘é‡æ–°ä»»åŠ¡ç¤ºèŒƒæˆ–äº¤äº’ä¸­å¿«é€Ÿè°ƒæ•´å…¶æ„ŸçŸ¥ç­–ç•¥ã€‚\n    - æ¢ç´¢**åŸºäºLLM/VLMçš„é›¶æ ·æœ¬ä»»åŠ¡åˆ†è§£ä¸è§„åˆ’**ï¼Œå°†å¤æ‚æŒ‡ä»¤è‡ªåŠ¨åˆ†è§£ä¸ºæ¨¡å‹å·²æŒæ¡çš„åŠ¨ä½œåŸºå…ƒåºåˆ—ã€‚\n\n- **æ”¹è¿›ç°å®ä¸–ç•Œçš„é²æ£’æ€§**ï¼š\n    - å¼€å‘å¯¹**ä¸å®Œç¾ç‚¹äº‘å’Œä¼ æ„Ÿå™¨å™ªå£°æ›´å…·é²æ£’æ€§çš„3Dè¡¨ç¤ºä¸æ¸²æŸ“æ–¹æ³•**ï¼Œä¾‹å¦‚ä½¿ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æˆ–3Dé«˜æ–¯æº…å°„è¿›è¡Œæ›´é€¼çœŸçš„è§†å›¾åˆæˆã€‚\n    - åœ¨æ›´å¤§é‡ä¸”å¤šæ ·åŒ–çš„**çœŸå®æœºå™¨äººæ•°æ®**ä¸Šè¿›è¡Œè®­ç»ƒæˆ–åŸŸé€‚åº”ï¼Œä»¥ç¼©å°ä»¿çœŸåˆ°ç°å®çš„å·®è·ã€‚\n    - ç ”ç©¶**å¤šæ¨¡æ€èåˆ**ï¼ˆå¦‚ç»“åˆè§¦è§‰åé¦ˆï¼‰ï¼Œåœ¨è§†è§‰ä¸¥é‡é®æŒ¡æ—¶æä¾›è¡¥å……ä¿¡æ¯ã€‚\n\n- **ä¼˜åŒ–ä¸å­¦ä¹ ä¸»åŠ¨æ„ŸçŸ¥ç­–ç•¥**ï¼š\n    - ç”¨**å¼ºåŒ–å­¦ä¹ æˆ–åŸºäºæ¢¯åº¦çš„å­¦ä¹ æ–¹æ³•**æ›¿ä»£æ‰‹å·¥è®¾è®¡çš„è¯„åˆ†å‡½æ•°ï¼Œç«¯åˆ°ç«¯åœ°å­¦ä¹ è§†è§’é€‰æ‹©ç­–ç•¥ï¼Œä»¥ç›´æ¥æœ€å¤§åŒ–ä»»åŠ¡æˆåŠŸç‡ã€‚\n    - å¼•å…¥**ä¿¡æ¯è®ºç›®æ ‡**ï¼ˆå¦‚æœ€å¤§åŒ–äº’ä¿¡æ¯ã€å‡å°‘ä¸ç¡®å®šæ€§ï¼‰ï¼Œè®©ä¸»åŠ¨æ„ŸçŸ¥æ›´ç›´æ¥åœ°æœåŠ¡äºå‡å°‘çŠ¶æ€ä¼°è®¡çš„ä¸ç¡®å®šæ€§ã€‚\n    - æé«˜æ¨¡å‹çš„**å¯è§£é‡Šæ€§**ï¼Œä¾‹å¦‚é€šè¿‡æ³¨æ„åŠ›å¯è§†åŒ–æˆ–ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šï¼Œè¯´æ˜ä¸ºä½•é€‰æ‹©æŸä¸ªè§†è§’æˆ–å˜ç„¦åŒºåŸŸã€‚\n\n- **é™ä½æ•°æ®ä¸è®­ç»ƒé—¨æ§›**ï¼š\n    - ç ”ç©¶æ›´é«˜æ•ˆçš„**æ•°æ®åˆ©ç”¨æ–¹æ³•**ï¼Œå¦‚åŸºäºæ¨¡å‹çš„å¢å¼ºã€ç¤ºèŒƒæ•°æ®é‡æ”¾æˆ–ä»äº’è”ç½‘è§†é¢‘ä¸­å­¦ä¹ ã€‚\n    - æ¢ç´¢**å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯**ï¼ˆå¦‚LoRAï¼‰ï¼Œåœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶ä»…æ›´æ–°å°‘é‡å‚æ•°ï¼Œå‡å°‘å¯¹è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚\n    - æ„å»º**å¼€æºã€æ ‡å‡†åŒ–çš„ä»¿çœŸä¸çœŸå®æ•°æ®é›†**ï¼Œä¿ƒè¿›ç¤¾åŒºå…±åŒæ”¹è¿›ã€‚\n\n**æ€»ç»“**ï¼šActiveVLAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†**ä¸»åŠ¨æ„ŸçŸ¥**æœºåˆ¶æ³¨å…¥VLAæ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†åœ¨é®æŒ¡å’Œç²¾ç»†æ“ä½œä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å…¶å½“å‰å±€é™æ€§ä¸»è¦ä½“ç°åœ¨**è®¡ç®—æ•ˆç‡ã€å¯¹æç«¯å¤æ‚ä»»åŠ¡çš„æ³›åŒ–ã€ä»¥åŠä»¿çœŸåˆ°ç°å®çš„è¿ç§»**ä¸Šã€‚æœªæ¥çš„å·¥ä½œå°†å›´ç»•**ç®—æ³•æ•ˆç‡ã€ç­–ç•¥å­¦ä¹ ã€å¤šæ¨¡æ€èåˆå’Œé™ä½æ•°æ®ä¾èµ–**ç­‰æ–¹å‘å±•å¼€ï¼Œä»¥æ¨åŠ¨å…¶èµ°å‘æ›´å®ç”¨ã€æ›´é€šç”¨çš„æœºå™¨äººæ™ºèƒ½ä½“ã€‚",
      "timestamp": "2026-01-20T17:56:22.588946",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    }
  ],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-13T08:29:07Z",
  "created_at": "2026-01-20T17:49:42.083939",
  "updated_at": "2026-01-20T17:49:42.083947"
}