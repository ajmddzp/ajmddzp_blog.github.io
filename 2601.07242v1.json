{
    "id": "2601.07242v1",
    "title": "HERE: Hierarchical Active Exploration of Radiance Field with Epistemic Uncertainty Minimization",
    "authors": [
        "Taekbeom Lee",
        "Dabin Kim",
        "Youngseok Jang",
        "H. Jin Kim"
    ],
    "abstract": "æœ¬æ–‡æå‡ºHEREï¼Œä¸€ç§åŸºäºç¥ç»è¾å°„åœºçš„ä¸»åŠ¨ä¸‰ç»´åœºæ™¯é‡å»ºæ¡†æ¶ï¼Œå¯å®ç°é«˜ä¿çœŸéšå¼å»ºå›¾ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºé€šè¿‡ç²¾ç¡®è¯†åˆ«æœªè§‚æµ‹åŒºåŸŸæ¥é©±åŠ¨ç›¸æœºè½¨è¿¹ç”Ÿæˆçš„ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼Œä»è€Œå®ç°é«˜æ•ˆæ•°æ®é‡‡é›†ä¸ç²¾ç¡®åœºæ™¯é‡å»ºã€‚æœ¬æ–¹æ³•çš„å…³é”®åœ¨äºåŸºäºè¯æ®æ·±åº¦å­¦ä¹ çš„è®¤çŸ¥ä¸ç¡®å®šæ€§é‡åŒ–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½ç›´æ¥æ•æ‰æ•°æ®ä¸è¶³é—®é¢˜ï¼Œå¹¶ä¸é‡å»ºè¯¯å·®å‘ˆç°å¼ºç›¸å…³æ€§ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿæ›´å¯é åœ°è¯†åˆ«æœªæ¢ç´¢æˆ–é‡å»ºæ•ˆæœè¾ƒå·®çš„åŒºåŸŸï¼Œä»è€Œå®ç°æ›´å…·é’ˆå¯¹æ€§çš„æ™ºèƒ½æ¢ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†åˆ©ç”¨å­¦ä¹ æ‰€å¾—è®¤çŸ¥ä¸ç¡®å®šæ€§çš„åˆ†å±‚æ¢ç´¢ç­–ç•¥ï¼šå±€éƒ¨è§„åˆ’åŸºäºå¯è§æ€§ä»é«˜ä¸ç¡®å®šæ€§ä½“ç´ ä¸­æå–ç›®æ ‡è§†ç‚¹ä»¥ç”Ÿæˆè½¨è¿¹ï¼Œå…¨å±€è§„åˆ’åˆ™åˆ©ç”¨ä¸ç¡®å®šæ€§æŒ‡å¯¼å¤§è§„æ¨¡è¦†ç›–ï¼Œå®ç°é«˜æ•ˆå…¨é¢çš„é‡å»ºã€‚é€šè¿‡åœ¨å¤šç§å°ºåº¦çš„ç…§ç‰‡çº§çœŸå®æ„Ÿä»¿çœŸåœºæ™¯ä¸­å®ç°æ¯”ç°æœ‰æ–¹æ³•æ›´é«˜çš„é‡å»ºå®Œæ•´åº¦ï¼Œè¯æ˜äº†æ‰€ææ–¹æ³•åœ¨ä¸»åŠ¨ä¸‰ç»´é‡å»ºä¸­çš„æœ‰æ•ˆæ€§ï¼Œç¡¬ä»¶æ¼”ç¤ºè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚",
    "url": "https://arxiv.org/abs/2601.07242v1",
    "html_url": "https://arxiv.org/html/2601.07242v1",
    "html_content": "HERE: Hierarchical Active Exploration of Radiance Field with Epistemic Uncertainty Minimization\nTaekbeom Lee\nâˆ—\n,\n\nDabin Kim\nâˆ—\n, Youngseok Jang, and H. Jin Kim\n:\nâˆ—\n{}^{*}:\nEqual contribution. The authors are with the Department of Aerospace Engineering, Seoul National University, Seoul 08826, South Korea (e-mail: ltb1128, dabin404, duscjs59, hjinkim@snu.ac.kr, corresponding author: H. Jin Kim).\nThis work was supported by Samsung Research Funding & Incubation Center of Samsung Electronics under Project Number SRFC-IT2402-17.\nAbstract\nWe present\nHERE\n, an active 3D scene reconstruction framework based on neural radiance fields, enabling high-fidelity implicit mapping.\nOur approach centers around an active learning strategy for camera trajectory generation, driven by accurate identification of unseen regions, which supports efficient data acquisition and precise scene reconstruction.\nThe key to our approach is epistemic uncertainty quantification based on evidential deep learning, which directly captures data insufficiency and exhibits a strong correlation with reconstruction errors.\nThis allows our framework to more reliably identify unexplored or poorly reconstructed regions compared to existing methods, leading to more informed and targeted exploration.\nAdditionally, we design a hierarchical exploration strategy that leverages learned epistemic uncertainty, where local planning extracts target viewpoints from high-uncertainty voxels based on visibility for trajectory generation, and global planning uses uncertainty to guide large-scale coverage for efficient and comprehensive reconstruction.\nThe effectiveness of the proposed method in active 3D reconstruction is demonstrated by achieving higher reconstruction completeness compared to previous approaches on photorealistic simulated scenes across varying scales, while a hardware demonstration further validates its real-world applicability.\nI\nIntroduction\nAccurately and efficiently representing 3D environments is a fundamental challenge in robotics, computer vision, and graphics.\nWhile traditional methods such as point clouds and meshes are widely used, radiance field-based approaches such as Neural Radiance Fields (NeRF)\n[\nmildenhall2021nerf\n]\nand 3D Gaussian Splatting (3DGS)\n[\nkerbl20233d\n]\noffer superior detail and lighting capture.\nMost research focuses on reconstructing scenes from given datasets, yet achieving high-quality results that support navigation, interaction, and downstream perception tasks demands both comprehensive scene coverage and detailed observation of fine-grained structures.\nActive 3D reconstruction addresses this challenge by directing robot motion to acquire high-quality data.\nUncertainty can serve as an effective cue to identify under-reconstructed or unexplored regions that require further observations, while planning provides the means to ensure that these regions are actually and efficiently observed.\nThe objective of this letter is to propose an integrated system that enhances scene coverage, reduces computational overhead, and achieves high reconstruction accuracy.\nDespite the growing interest in 3DGS methods\n[\njin2025activegs\n,\nli2025activesplat\n]\n, we adopt NeRF for active reconstruction because NeRF yields smooth surfaces and usable signed distance fields for downstream tasks, whereas obtaining these reliably from 3DGS remains nontrivial. Additionally, it is more memoryâ€‘efficient, which is especially important for robots with limited resources.\nFigure 1\n:\nOur approach performs active scene reconstruction of neural radiance fields by leveraging epistemic uncertainty to guide exploration of an agent.\nIt identifies regions with high epistemic uncertainty and generates an informative path. This, combined with hierarchical planning, ensures both scene coverage and detailed exploration of uncertain areas, enabling high-quality reconstruction across various scales.\nAccording to the extensive literature in active SLAM\n[\nplaced2023survey\n]\nand active learning\n[\nren2021survey\n]\n, uncertainty estimation plays a pivotal role in guiding exploration and improving reconstruction efficiency.\nExisting approaches\n[\nfeng2024naruto\n,\nran2023neurar\n,\nlee2022uncertainty\n]\nintegrate Bayesian modeling into NeRF to estimate uncertainty and optimize training view selection. However, they focus on overall uncertainty without explicitly differentiating epistemic uncertainty, which reflects the modelâ€™s knowledge limitations, from aleatoric uncertainty, which stems from irreducible data noise. Approaches that quantify epistemic uncertainty, such as network parameter perturbation\n[\nyan2023active\n]\nand Fisher information analysis\n[\njiang2024fisherrf\n,\nxu2024hgs\n]\n, are impractical to use in real-time planning due to expensive computation.\nIn the second aspect of active reconstruction, we need to design an informative path planning strategy that fully covers the scene without leaving unmapped areas while selecting informative views.\nRecent active learning approaches for NeRF\n[\nyan2023active\n,\nfeng2024naruto\n]\nemploy samplingâ€‘based goal search to cope with the high dimensionality of planning, but this often fails to ensure full coverage. Moreover, focusing on information gain solely at the goal states\n[\nyan2023active\n,\nfeng2024naruto\n,\nkuang2024active\n]\nand ignoring the information accumulated along the continuous camera trajectory can lead to suboptimal, uninformative behavior.\nIn this work, we propose\nHERE\n, a system that integrates a neural mapping module with epistemic uncertainty-driven informative path planning.\nWe develop a novel uncertainty quantification (UQ) method for radiance fields that enables the use of evidential deep learning (EDL) to accurately assess how well a region has been observed, thereby allowing viewpoint evaluation based on informativeness.\nWe demonstrate through experiments that our uncertainty estimates exhibit stronger correlation with ground truth coverage compared to representative active mapping methods.\nBy storing spatial uncertainty in learnable grids, our system ensures real-time performance because estimation requires only trilinear interpolation, while ensuring stable long-term learning as updates affect only nearby voxels.\nFurthermore, inspired by batch active learning\n[\nkirsch2019batchbald\n]\n, our method considers joint information along the camera trajectory rather than combining separately computed information from individual views, leading to a more accurate approximation of the total information gathered.\nTo ensure both high reconstruction accuracy and comprehensive scene coverage, we introduce a hierarchical planning framework.\nThis structure balances global exploration with local planning, making our method effective across both small- and large-scale environments.\nIn summary, our key contributions are as follows:\nâ€¢\nAn integrated real-time active mapping framework that guides reconstruction using uncertainty-aware planning.\nâ€¢\nA novel approach to quantify epistemic uncertainty in neural implicit mapping by extending evidential deep learning to radiance fields.\nâ€¢\nA planner that ensures global coverage while refining local trajectories via epistemic uncertainty for scalable reconstruction.\nâ€¢\nState-of-the-art reconstruction completeness on datasets of various scales, with a hardware demonstration validating real-world applicability.\nII\nRelated Work\nII-A\nActive Scene Reconstruction\nActive planning for efficient scene reconstruction has been widely studied in robotics and computer vision\n[\nplaced2023survey\n]\n. While many prior methods rely on explicit representations such as meshes or occupancy grids\n[\ncarrillo2015autonomous\n,\ntabib2021autonomous\n]\n, the emergence of NeRFs as a powerful tool for high-fidelity 3D reconstruction has shifted recent efforts toward developing active learning strategies for radiance field mapping.\nSeveral studies\n[\nlee2022uncertainty\n,\npan2022activenerf\n,\nran2023neurar\n]\nintegrate NeRF with Next-Best-View (NBV) strategies by sampling candidate views and computing information gain via rendering. However, the high computational cost of NeRF rendering hinders real-time performance.\n[\nfeng2024naruto\n]\npartially reduces the computational cost by adopting hybrid scene representations\n[\nwang2023coslam\n]\n.\nFrame-level view selection is simplified into goal position search in\n[\nyan2023active\n,\nfeng2024naruto\n]\n.\nHowever, these methods restrict the action space to limited domains such as a hemisphere or a 2D plane, or neglect information gain throughout the trajectory.\nUnlike prior approaches, our method efficiently computes viewpoint information gain with explicit consideration of orientation and extends beyond frame-level selection to generate informative trajectories.\nFigure 2\n:\nAn overview of the framework. Given RGB and depth images, the neural implicit SLAM module learns an implicit map along with evidence and sufficient statistics grids, which are used to quantify epistemic uncertainty. The hierarchical active reconstruction planning method leverages this uncertainty for target view selection in local planning and frontier extraction for global region path generation. The planner then generates a camera trajectory to explore the unseen environment.\nII-B\nUncertainty in Radiance Field\nRegarding UQ in NeRF, several works utilize NeRF-specific proxies such as rendering weight\n[\nlee2022uncertainty\n]\n.\nTechniques from traditional deep learning, such as modeling NeRF predictions as Gaussian, are adapted\n[\nran2023neurar\n,\npan2022activenerf\n]\n.\nIn image-based neural rendering,\n[\njin2023neu\n]\nquantifies uncertainty inherited from input for next-best-view planning to acquire informative reference views.\nHowever, their UQ includes aleatoric uncertainty, arising from factors such as transient objects or changes in lighting conditions, which are irreducible by collecting additional observations.\nEpistemic uncertainty, caused by insufficient data, serves as a meaningful signal for active reconstruction in NeRF.\nSeveral studies try to quantify such uncertainty, particularly in the presence of missing views or occluded regions.\nStochastic NeRF\n[\nshen2022conditional\n]\nquantifies uncertainty via Monte Carlo approximation, while\n[\nsunderhauf2023density\n]\nuses an ensemble model, but multiple predictions require large computation.\nRecent works\n[\ngoli2024bayes\n,\njiang2024fisherrf\n]\nestimate epistemic uncertainty using the Laplace approximation.\nHowever, these methods are not suitable for real-time active reconstruction due to the high computational cost of back-propagation. Moreover, they rely on approximate posterior distributions over model parameters, which can limit the accuracy of uncertainty estimation.\nEDL provides an alternative framework for modeling epistemic uncertainty by predicting a prior over the predictive distribution.\n[\nchen2024nerf_localization\n]\nshowed effectiveness of EDL to model reliability of 2D-3D correspondences. For active reconstruction, we propose a novel approach that estimates the uncertainty of the geometry modelled by the radiance field. We update the prior over 3D geometry for effective identification of undertrained regions, and use explicit learnable grids for fast epistemic uncertainty estimation via trilinear interpolation.\nIII\nPreliminaries\nIII-A\nEvidential Deep Learning for Regression\nTo quantify uncertainty in regression, predictions are often modeled with a Gaussian distribution.\nHowever, a high variance alone cannot distinguish whether the data is noisy or the model is uncertain.\nEvidential Deep Learning (EDL) addresses this by placing a prior over the Gaussian parameters\nÎ¸\n=\n(\nÎ¼\n,\nÏƒ\n2\n)\n\\theta\\!=\\!(\\mu,\\sigma^{2})\nusing a Normal Inverse-Gamma (NIG) distribution with parameters\nm\n=\n(\nÎ¼\n0\n,\nÎ»\n,\nÎ±\n,\nÎ²\n)\nm\\!=\\!(\\mu_{0},\\lambda,\\alpha,\\beta)\n, in order, mean, precision, shape parameter, and scale parameter.\nSince NIG is conjugate to the Gaussian, the posterior given observations\nğ”»\n=\n{\ny\ni\n}\ni\n=\n1\nN\n\\mathbb{D}\\!=\\!\\{y_{i}\\}_{i=1}^{N}\nremains NIG, as described by the Bayesian update rule:\np\nâ€‹\n(\nÎ¸\nâˆ£\nğ”»\n,\nm\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\n)\n\\displaystyle p(\\theta\\mid\\mathbb{D},m^{post})\nâˆ\n[\nâˆ\ni\n=\n1\nN\nğ’©\nâ€‹\n(\ny\ni\nâˆ£\nÎ¸\n)\n]\nâ€‹\nNIG\nâ€‹\n(\nÎ¸\nâˆ£\nm\np\nâ€‹\nr\nâ€‹\ni\n)\n\\displaystyle\\propto\\left[\\prod_{i=1}^{N}\\mathcal{N}(y_{i}\\mid\\theta)\\right]\\mathrm{NIG}(\\theta\\mid m^{pri})\n=\nNIG\nâ€‹\n(\nÎ¸\nâˆ£\nm\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\n)\n,\n\\displaystyle=\\mathrm{NIG}(\\theta\\mid m^{post}),\n(1)\nwhere the posterior parameters\nm\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\nm^{post}\ncan be obtained in closed form\n[\nmurphy2007conjugate\n]\n.\nEq. (\nIII-A\n) can be simplified by defining\nÏ‡\n=\n(\nÎ¼\n0\n,\nÎ¼\n0\n2\n+\nÎ²\n/\nÎ±\n)\n\\chi=(\\mu_{0},\\mu_{0}^{2}+\\beta/\\alpha)\nand setting\nn\n=\nÎ»\n=\n2\nâ€‹\nÎ±\nn=\\lambda=2\\alpha\n[\ncharpentier2021natural\n]\n, leading to\nÏ‡\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\n=\nn\np\nâ€‹\nr\nâ€‹\ni\nâ€‹\nÏ‡\np\nâ€‹\nr\nâ€‹\ni\n+\nN\nâ€‹\nÏ‡\nğ”»\nn\np\nâ€‹\nr\nâ€‹\ni\n+\nN\n,\nn\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\n=\nn\np\nâ€‹\nr\nâ€‹\ni\n+\nN\n,\n\\displaystyle\\chi^{post}=\\frac{n^{pri}\\chi^{pri}+N\\chi^{\\mathbb{D}}}{n^{pri}+N},\\quad n^{post}=n^{pri}+N,\n(2)\nwith\nÏ‡\nğ”»\n=\n(\ny\nÂ¯\n,\n1\nN\nâ€‹\nâˆ‘\ni\ny\ni\n2\n)\n\\chi^{\\mathbb{D}}=\\left(\\bar{y},\\frac{1}{N}\\sum_{i}y_{i}^{2}\\right)\n.\nÏ‡\nğ”»\n\\chi^{\\mathbb{D}}\n, and\nÏ‡\np\nâ€‹\nr\nâ€‹\ni\n\\chi^{pri}\n,\nÏ‡\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\n\\chi^{post}\nare sufficient statistics for estimating\nÎ¸\n\\theta\nfrom the observations\nğ”»\n\\mathbb{D}\n, pseudo-observations of the prior and posterior data, respectively.\nn\np\nâ€‹\nr\nâ€‹\ni\nn^{pri}\nand\nn\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\nn^{post}\nare the numbers of pseudo-observations, also referred to as evidence.\nThe updated posterior parameters\nm\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\nm^{post}\nallow epistemic uncertainty to be predicted in closed form, as detailed in Sec.\nIV\n.\nIII-B\nNeural implicit mapping\nCo-SLAM\n[\nwang2023coslam\n]\nrepresents 3D geometry by predicting colors and truncated signed distance fields (TSDF) for world coordinates.\nIt uses both coordinate encoding\n[\nmuller2019neural\n]\nand sparse parametric encoding\n[\nmueller2022instant\n]\nalong with a geometry decoder and color MLP, enabling fast, high-fidelity reconstruction.\nBundle adjustments are executed by minimizing several objectives for sampled rays.\nAlong each ray\nr\nr\ndefined by camera origin\no\no\nand ray direction\nd\nd\n, 3D points are sampled at predefined depths\nt\ni\nt_{i}\nas\nx\ni\n=\no\n+\nt\ni\nâ€‹\nd\nx_{i}=o+t_{i}d\n.\nRendering weights\nw\ni\nw_{i}\nare computed from predicted TSDFs\ns\ni\ns_{i}\n, and color and depth are rendered by a weighted sum of predicted colors\nc\ni\nc_{i}\nand\nt\ni\nt_{i}\n.\nRendering losses compare the rendered color and depth with the observed color and depth.\nTo achieve accurate and smooth reconstruction, SDF losses\nâ„’\ns\nâ€‹\nd\nâ€‹\nf\n\\mathcal{L}_{sdf}\nand\nâ„’\ns\n\\mathcal{L}_{s}\nare applied to points inside (\n|\nD\nr\nâˆ’\nt\ni\n|\nâ‰¤\nt\nâ€‹\nr\n\\left|D_{r}-t_{i}\\right|\\leq tr\n) and outside the truncation region, respectively.\nâ„’\ns\nâ€‹\nd\nâ€‹\nf\n\\displaystyle\\mathcal{L}_{sdf}\n=\nÎ·\nâ€‹\nâˆ‘\nr\nâˆˆ\nR\nd\nÎ·\nr\nâ€‹\nâˆ‘\ni\nâˆˆ\nS\nr\nt\nâ€‹\nr\n(\ns\ni\nâˆ’\n(\nD\nr\nâˆ’\nt\ni\n)\n)\n2\n,\n\\displaystyle=\\eta\\sum_{r\\in R_{d}}\\eta_{r}\\sum_{i\\in S_{r}^{tr}}(s_{i}-(D_{r}-t_{i}))^{2},\n(3)\nâ„’\nf\nâ€‹\ns\n\\displaystyle\\mathcal{L}_{fs}\n=\nÎ·\nâ€‹\nâˆ‘\nr\nâˆˆ\nR\nd\nÎ·\nr\nâ€‹\nâˆ‘\ni\nâˆˆ\nS\nr\nf\nâ€‹\ns\n(\ns\ni\nâˆ’\nt\nâ€‹\nr\n)\n2\n,\n\\displaystyle=\\eta\\sum_{r\\in R_{d}}\\eta_{r}\\sum_{i\\in S_{r}^{fs}}(s_{i}-tr)^{2},\n(4)\nwhere\nD\nr\nD_{r}\nis observed depth for the ray\nr\nr\n,\nS\nr\n(\n.\n)\nS_{r}^{(.)}\nis the set of sampled points in each region,\nt\nâ€‹\nr\ntr\nis the truncation distance, and\nÎ·\nr\n=\n1\n|\nS\nr\nt\nâ€‹\nr\n|\n\\eta_{r}=\\frac{1}{|S_{r}^{tr}|}\n.\nFeature smoothness loss is performed for smooth reconstruction in unobserved free spaces.\nFor further details, we refer readers to Co-SLAM.\nIV\nEpistemic Uncertainty Quantification\nTo effectively capture epistemic uncertainty, we integrate EDL-based uncertainty quantification into neural implicit mapping.\nPrior works on NeRF uncertainty often model 3D color as Gaussian,\nc\ni\nâˆ¼\nğ’©\nâ€‹\n(\nc\nÂ¯\ni\n,\nÏƒ\ni\n2\n)\nc_{i}\\sim\\mathcal{N}(\\bar{c}_{i},\\sigma_{i}^{2})\n, and train the model through the rendered color which also follows a Gaussian\nC\n^\n=\nâˆ‘\ni\n=\n1\nM\nw\ni\nâ€‹\nc\ni\nâˆ¼\nğ’©\nâ€‹\n(\nâˆ‘\ni\n=\n1\nM\nw\ni\nâ€‹\nc\nÂ¯\ni\n,\nâˆ‘\ni\n=\n1\nM\nw\ni\n2\nâ€‹\nÏƒ\ni\n2\n)\n.\n\\hat{C}=\\sum_{i=1}^{M}w_{i}c_{i}\\sim\\mathcal{N}(\\sum_{i=1}^{M}w_{i}\\bar{c}_{i},\\sum_{i=1}^{M}w_{i}^{2}\\sigma_{i}^{2}).\nHowever, this strategy does not extend to evidential models: when a NIG prior is placed on each\nc\ni\nc_{i}\n, the resulting distribution of\nC\n^\n\\hat{C}\nno longer has a tractable form.\nThis breaks the conjugacy required for tractable posterior updates, which is an assumption essential for EDL.\nFigure 3\n:\nIllustration of\nour uncertainty quantification module.\nThe spread around\n(\nÎ¼\n,\nÏƒ\n)\n(\\mu,\\sigma)\nenlarges in poorly reconstructed regions, which we model as epistemic uncertainty.\nAlternatively, we extend Co-SLAMâ€™s SDF prediction to probabilistic modeling.\nWe model the predictive distribution as a Gaussian\nğ’©\nâ€‹\n(\ns\nâˆ£\nÎ¼\n,\nÏƒ\n2\n)\n\\mathcal{N}(s\\mid\\mu,\\sigma^{2})\n, with a NIG prior defined over its parameters\nÎ¼\n\\mu\n,\nÏƒ\n2\n\\sigma^{2}\n.\nOur model predicts the posterior update\nÏ‡\ni\n,\nn\ni\n\\chi_{i},n_{i}\nfor each 3D point\nx\ni\nx_{i}\n, effectively replacing the observation-based Bayesian update in eq. (\n2\n).\nÏ‡\ni\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\n=\nn\np\nâ€‹\nr\nâ€‹\ni\nâ€‹\nÏ‡\np\nâ€‹\nr\nâ€‹\ni\n+\nn\ni\nâ€‹\nÏ‡\ni\nn\np\nâ€‹\nr\nâ€‹\ni\n+\nn\ni\n,\nn\ni\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\n=\nn\np\nâ€‹\nr\nâ€‹\ni\n+\nn\ni\n.\n\\chi^{post}_{i}=\\frac{n^{pri}\\chi^{pri}+n_{i}\\chi_{i}}{n^{pri}+n_{i}},\\hskip 14.22636ptn^{post}_{i}=n^{pri}+n_{i}.\n(5)\nwhere we set\nÏ‡\np\nâ€‹\nr\nâ€‹\ni\n=\n(\n0\n,\n3\n)\n\\chi^{pri}\\!=\\!(0,3)\n, corresponding to a prior SDF with zero mean and variance 3, with\nn\np\nâ€‹\nr\nâ€‹\ni\n=\n1\nn^{pri}\\!=\\!1\n.\nWe treat\ns\ni\ns_{i}\npredicted by the original Co-SLAM as the first statistic\nÏ‡\ni\n,\n1\n\\chi_{i,1}\n, while a separate uncertainty learning module predicts the second statistic\nÏ‡\ni\n,\n2\n\\chi_{i,2}\nand the evidence\nn\ni\nn_{i}\n.\nThis enables uncertainty modeling without modifying the neural implicit mapping, preserving reconstruction quality and enabling modular integration with various mapping methods.\nWe implement the uncertainty learning module using two learnable grids,\nV\nÏ\nV_{\\rho}\nand\nV\nÏ„\nV_{\\tau}\n, from which 3D point\nx\ni\nx_{i}\nqueries a confidence score\nÏ\n\\rho\nand second moment\nÏ„\n=\nÎ²\nÎ±\n\\tau\\!=\\!\\frac{\\beta}{\\alpha}\nvia trilinear interpolation.\nWe compute\nn\ni\n=\nN\nS\nâˆ—\nÏ†\nâ€‹\n(\nV\nÏ\nâ€‹\n(\nx\ni\n)\n)\nn_{i}=N_{S}*\\varphi(V_{\\rho}(x_{i}))\nand\nÏ„\ni\n=\nV\nÏ„\nâ€‹\n(\nx\ni\n)\n\\tau_{i}=V_{\\tau}(x_{i})\n, where\nÏ†\n\\varphi\nis the sigmoid function and\nN\nS\nN_{S}\nis a scale constant.\nFrom\ns\ni\ns_{i}\n,\nn\ni\nn_{i}\n, and\nÏ„\ni\n\\tau_{i}\n, we obtain posterior NIG parameters\nm\ni\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\n=\n(\ns\ni\n,\nÎ»\ni\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\n,\nÎ±\ni\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\n,\nÎ²\ni\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\n)\nm^{post}_{i}\\!=\\!(s_{i},\\lambda^{post}_{i},\\alpha^{post}_{i},\\beta^{post}_{i})\nvia eq. (\n5\n) and the definition of\nn\nn\nand\nÏ‡\n\\chi\nin Sec.\nIII\n.\nThe overview of quantifying epistemic uncertainty is described in Fig.\n3\n.\nOur grid-based representation ensures spatial locality, restricting uncertainty learning to local regions.\nThis is especially beneficial in incremental mapping, where new observations affect only nearby voxels, preserving past estimates and ensuring stable long-term learning.\nWe employ the entropy of NIG distribution to quantify epistemic uncertainty of each 3D point.\nIt reflects how much the posterior NIG is uncertain in estimating the predictive Gaussian, which aligns with uncertainty caused by insufficient information about the data.\nu\ne\nâ€‹\np\nâ€‹\ni\nâ€‹\n(\nÎ»\n,\nÎ±\n,\nÎ²\n)\n=\nâ„\nâ€‹\n[\nNIG\nâ€‹\n(\nÎ¸\nâˆ£\nm\n)\n]\n\\displaystyle u_{epi}(\\lambda,\\alpha,\\beta)=\\mathbb{H}[\\mathrm{NIG}(\\theta\\mid m)]\n=\nlog\nâ¡\n(\n(\n2\nâ€‹\nÏ€\n)\n1\n2\nâ€‹\nÎ²\n3\n2\nâ€‹\nÎ“\nâ€‹\n(\nÎ±\n)\nÎ»\n1\n2\n)\nâˆ’\n(\nÎ±\n+\n3\n2\n)\nâ€‹\nÏˆ\nâ€‹\n(\nÎ±\n)\n+\nÎ±\n+\n1\n2\n\\displaystyle=\\log\\left(\\frac{(2\\pi)^{\\frac{1}{2}}\\beta^{\\frac{3}{2}}\\Gamma(\\alpha)}{\\lambda^{\\frac{1}{2}}}\\right)-\\left(\\alpha+\\frac{3}{2}\\right)\\psi(\\alpha)+\\alpha+\\frac{1}{2}\n(6)\nOur uncertainty learning module is jointly trained with neural implicit mapping during bundle adjustment.\nWe extend the SDF losses (eq. (\n3\n) and (\n4\n)) to a Bayesian formulation\n[\ncharpentier2021natural\n]\n, encouraging the predicted posterior to align with the true posterior.\nFor each 3D point with ground truth SDF\ns\ng\nâ€‹\nt\ns_{gt}\n, the Bayesian loss is derived from the NIG properties:\nâ„’\nâˆ—\nâ€‹\n(\ns\ng\nâ€‹\nt\n,\nm\n)\n=\nğ”¼\nÎ¸\nâˆ¼\nNIG\nâ€‹\n(\nÎ¸\nâˆ£\nm\n)\nâ€‹\n[\nâˆ’\nlog\nâ¡\nğ’©\nâ€‹\n(\ns\ng\nâ€‹\nt\nâˆ£\nÎ¸\n)\n]\nâˆ’\nÎ³\nâ€‹\nâ„\nâ€‹\n[\nNIG\nâ€‹\n(\nÎ¸\nâˆ£\nm\n)\n]\n\\displaystyle\\mathcal{L}^{*}\\left(s_{gt},m\\right)=\\mathbb{E}_{\\theta\\sim\\mathrm{NIG}(\\theta\\mid m)}[-\\log\\mathcal{N}(s_{gt}\\mid\\theta)]-\\gamma\\mathbb{H}[\\mathrm{NIG}(\\theta\\mid m)]\n=\n1\n2\nâ€‹\n(\nÎ±\nÎ²\nâ€‹\n(\ns\ng\nâ€‹\nt\nâˆ’\ns\n)\n2\n+\n1\nÎ»\nâˆ’\nÏˆ\nâ€‹\n(\nÎ±\n)\n+\nlog\nâ¡\n2\nâ€‹\nÏ€\nâ€‹\nÎ²\n)\nâˆ’\nÎ³\nâ€‹\nu\ne\nâ€‹\np\nâ€‹\ni\nâ€‹\n(\nÎ»\n,\nÎ±\n,\nÎ²\n)\n\\displaystyle=\\frac{1}{2}\\left(\\frac{\\alpha}{\\beta}(s_{gt}-s)^{2}+\\frac{1}{\\lambda}-\\psi(\\alpha)+\\log{2\\pi\\beta}\\right)-\\gamma u_{epi}(\\lambda,\\alpha,\\beta)\n(7)\nThe â€œpostâ€ superscript is omitted for clarity, but the loss and the epistemic uncertainty are computed with the posterior parameters\nm\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\nm^{post}\n.\nThe entropy term (eq. (\nIV\n)) also serves as a regularizer, penalizing overconfident posteriors.\nWe set\ns\ng\nâ€‹\nt\ns_{gt}\nto\nD\nr\nâˆ’\nt\ni\nD_{r}-t_{i}\nand\nt\nâ€‹\nr\ntr\nfor points inside and outside the truncation region, respectively, leading to the Bayesian SDF losses:\nâ„’\ns\nâ€‹\nd\nâ€‹\nf\nâˆ—\n\\displaystyle\\mathcal{L}_{sdf}^{*}\n=\nÎ·\nâ€‹\nâˆ‘\nr\nâˆˆ\nR\nd\nÎ·\nr\nâ€‹\nâˆ‘\ni\nâˆˆ\nS\nr\nt\nâ€‹\nr\nâ„’\nâˆ—\nâ€‹\n(\nD\nr\nâˆ’\nt\ni\n,\nm\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\n)\n\\displaystyle=\\eta\\sum_{r\\in R_{d}}\\eta_{r}\\sum_{i\\in S_{r}^{tr}}\\mathcal{L}^{*}(D_{r}-t_{i},m^{post})\n(8)\nâ„’\ns\nâ€‹\nd\nâ€‹\nf\nâˆ—\n\\displaystyle\\mathcal{L}_{sdf}^{*}\n=\nÎ·\nâ€‹\nâˆ‘\nr\nâˆˆ\nR\nd\nÎ·\nr\nâ€‹\nâˆ‘\ni\nâˆˆ\nS\nr\nt\nâ€‹\nr\nâ„’\nâˆ—\nâ€‹\n(\nt\nâ€‹\nr\n,\nm\np\nâ€‹\no\nâ€‹\ns\nâ€‹\nt\n)\n\\displaystyle=\\eta\\sum_{r\\in R_{d}}\\eta_{r}\\sum_{i\\in S_{r}^{tr}}\\mathcal{L}^{*}(tr,m^{post})\n(9)\nThese losses are applied with other Co-SLAM losses.\nV\nActive Scene Reconstruction\nAn effective active learning strategy for scene reconstruction must satisfy two key requirements: (1) selecting views that target areas of high uncertainty, and (2) achieving complete scene coverage.\nTo address the first requirement, our local planner (LP) generates viewpoints focusing on high-uncertainty regions. However, relying solely on the LP may limit exploration to previously visited areas. To overcome this, we introduce a global planner (GP) that generates paths covering discrete regions, ensuring comprehensive exploration.\nV-A\nScene Coverage Planning\nV-A\n1\nEnvironment Decomposition\nGlobal scene planning is computationally demanding, and prior sampling-based methods\n[\nfeng2024naruto\n,\nyan2023active\n]\noften yield incomplete coverage. To address this, we propose a coverage-based GP that partitions the environment into regions,\nS\nR\n=\n{\nR\ni\n}\ni\n=\n1\nN\nS_{R}=\\{R_{i}\\}_{i=1}^{N}\n, where each representative point (RP) is the centroid of unoccupied voxels. The environment is first uniformly partitioned into fixed-resolution regions within the specified 3D environment bounds.\nA connectivity graph\nğ’¢\nR\n=\n(\nS\nR\n,\nE\nR\n)\n\\mathcal{G}_{R}=(S_{R},E_{R})\n, inspired by\n[\nzhang2024falcon\n]\n, encodes traversability, where edges\nE\nR\nE_{R}\nrepresent connections validated by A\nâˆ—\nalgorithm between their RPs. To prevent losing potential connectivity, high-uncertainty voxels are treated as unoccupied. Regions are classified as\nunexplored\n,\nexploring\n, or\nexplored\n. An\nunexplored\nregion contains high-uncertainty voxels based on our UQ; an\nexploring\nregion includes frontier voxels; and an\nexplored\nregion lacks such frontiers. Frontier voxels are identifiedbased on the occupancy of voxels, evaluated through the SDFs, and the uncertainty of neighboring voxels.\nV-A\n2\nRegion-Level Coverage Planning\nFor coverage planning, we propose a method to determine the optimal visitation order for\nexploring\nregions by formulating the problem as an open-loop TSP. The cost function for each edge\ne\ni\nâ€‹\nj\ne_{ij}\nconnecting regions\nR\ni\n,\nR\nj\nR_{i},R_{j}\nis defined as\nc\nâ€‹\n(\ne\ni\nâ€‹\nj\n)\n=\nl\ni\nâ€‹\nj\nâˆ’\nk\nâ‹…\nf\ni\nâ€‹\nj\nc(e_{ij})=l_{ij}-k\\cdot f_{ij}\n, where\nl\ni\nâ€‹\nj\nl_{ij}\nis the distance between representative points,\nf\ni\nâ€‹\nj\nf_{ij}\nis the number of frontier voxels in the regions, and\nk\nk\nis a weighting parameter.\nSince the problem is NP-complete, we employ the Lin-Kernighan heuristic\n[\nlin1973effective\n]\nto approximate the solution. The resulting path achieves complete coverage by prioritizing frontier-rich regions and minimizing path length.\nV-B\nCamera Viewpoint Trajectory Generation\nFigure 4\n:\nIllustration of the process of the scene coverage planning and local trajectory planning.\nThis subsection details the design of the LP, which refines the global path into an information-rich 6DoF trajectory by selecting and connecting informative camera viewpoints.\nV-B\n1\nTarget Viewpoints Extraction\nTo generate an information-rich path, we extract 6DoF target viewpoints with visibility to voxels exhibiting high epistemic uncertainty. The viewpoint space is defined by combining discrete positions in the local planning domain with sampled orientations. Existing approaches such as occupancy-grid raycasting\n[\ncarrillo2015autonomous\n]\nor direct NeRF rendering are computationally expensive and unsuitable for real-time planning. In contrast, our method integrates a NeRF-based representation with a tailored uncertainty accumulation scheme, designed in accordance with our uncertainty quantification framework, thereby enabling efficient real-time viewpoint selection.\nThe global goal is defined as the representative point of the next region on the global path, and the local planning domain includes the current region and its neighbors.\nCandidate viewpoints are generated by pairing positions sampled regularly within this domain with orientations sampled via Fibonacci sphere sampling\n[\nswinbank2006fibonacci\n]\n.\nFor each sampled pose, uncertainty is accumulated as the sum of uncertainties from the top-\nk\nk\nhigh-uncertainty voxels that satisfy: (1) distance constraints, (2) visibility via line-of-sight checks in the SDF, and (3) inclusion within a conic field of view approximated by the cameraâ€™s mean FoV.\nWe select viewpoints to maximize the total accumulated uncertainty of the set over chosen viewpoints. This set-based criterion is optimized using a greedy strategy, which is supported by submodular maximization theory\n[\nroberts2017submodular\n,\nkirsch2019batchbald\n]\n.\nViewpoints are iteratively selected when their accumulated uncertainty surpasses a threshold, and the selection process continues until a maximum number of viewpoints is obtained.\nTo enhance coverage and reduce redundancy, previously selected voxels are filtered out.\nFor each selected viewpoint, additional rotation samples at the same position are examined to enhance coverage while minimizing unnecessary movement.\nV-B\n2\nCamera Viewpoint Trajectory Generation\nWe construct a target-viewpoint graph\nğ’¢\nV\n=\n(\nS\nV\n,\nE\nV\n)\n\\mathcal{G}_{V}=(S_{V},E_{V})\n, where\nS\nV\nS_{V}\ndenotes informative viewpoints and\nE\nV\nE_{V}\ncontains collision-free edges between them. To guide the camera toward the global goal, we select a local goal as follows: if the global goal lies within the local planning domain, it is used directly; otherwise,\nwe select the most informative target viewpoints from the region with the smallest center-to-goal distance.\nA TSP is then solved over\nğ’¢\nV\n\\mathcal{G}_{V}\n, with edge costs defined by traversal time. To enforce fixed start and goal, we introduce a dummy node\nV\nd\nV_{d}\nconnected only to the start and goal with zero cost, ensuring a valid start-to-goal solution.\nThe ordered target viewpoints are interpolated in position and orientation, resulting in a smooth and continuous camera trajectory.\nV-B\n3\nEscaping Local Optima\nAlthough the LP biases the camera toward the global goal, it may become trapped in local optima. To resolve this, we introduce a fallback strategy triggered when no informative viewpoints remain. If the current domain is fully explored, a path to the global goal is generated by running the A\nâˆ—\nalgorithm on the connectivity graph\nğ’¢\nR\n\\mathcal{G}_{R}\n. Viewpoints are then selected along the resulting region path, and collision-free trajectories are computed. If a path between consecutive regions is blocked, a voxel-level A\nâˆ—\npath toe the next regionâ€™s RP is used instead. This process continues until the global goal is reached.\nV-C\nIntegrated Planning Framework\nFor the integrated planning framework, if the current goal has been reached, the GP procedure is triggered.\nThe connectivity graph\nğ’¢\nC\n\\mathcal{G}_{C}\nis updated, and region states are refined based on frontier voxels. Solving the global TSP the provides a new region-level path.\nThe LP activates after the robot completes the previous trajectory, constructing a target viewpoints graph\nğ’¢\nV\n\\mathcal{G}_{V}\nwith local goal\nx\nl\n,\ng\nx_{l,g}\n. If no target viewpoints exist, the GP provides an alternative global path; otherwise, the LP generates the camera trajectory.\n(a)\n(b)\nFigure 5\n:\n(a) Uncertainty and SDF prediction error of the ground truth mesh on scenes from the Gibson dataset (Top: Pablo, Bottom: Swormville) for each algorithm during exploration.\nEach value is normalized, with\nred\n(1) indicating a high value and\nblue\n(0) indicating a low value.\n(b) AUSE plot for the UQ metrics evaluated on selected scenes from the Gibson dataset.\nVI\nExperiments\nVI-A\nExperiment setup\nVI-A\n1\nImplementation details\nWe use a fixed parameter set across all experiments. The evidence scale\nN\nS\nN_{S}\nis set to\ne\n15\ne^{15}\n, and\nV\nÏ\nV_{\\rho}\nis initialized to\nâˆ’\nlog\nâ¡\n(\nN\nS\n)\n-\\log(N_{S})\n, yielding a fixed initial evidence value (\nâ‰ˆ\n1\n\\approx\\!1\n) for unobserved voxels.\nV\nÏ\nV_{\\rho}\n,\nV\nÏ„\nV_{\\tau}\n, and the voxel grid used for uncertainty accumulation all use a 0.1 m resolution. The region size for hierarchical planning is 1 m. The weight\nk\nk\nin the coverage planning cost function is 0.1. For target-viewpoints extraction, positions are sampled every 0.2 m in\nx\nx\n-\ny\ny\nand 1 m in\nz\nz\n, with 30 orientations per position, and the view selection threshold\nÎ·\n\\eta\nis 10.\nVI-A\n2\nDataset and Evaluation Environment\nWe evaluate our system using the Habitat simulator\n[\nhabitat19iccv\n]\nand photorealistic indoor scene datasets.\nWe select five scenes from the MP3D dataset\n[\nMatterport3D\n]\nand nine scenes from the Gibson dataset\n[\nxiazamirhe2018gibsonenv\n]\n, which are the same scenes as\n[\nyan2023active\n]\n.\nThe planning loop is executed for 5000 time steps on MP3D and 1000 or 2000 time steps on Gibson, depending on scene size.\n1\n1\n1\nFor the choice of time steps, we followed the evaluation in\n[\nyan2023active\n]\n.\nThe system processes RGB-D images at the resolution of\n680\nÃ—\n1200\n680\\times 1200\nwith the focal length of 600.\nThe voxel size for the 3D signed distance field is set to 0.1m.\nAll experiments are done with an Intel i7-10700 CPU and an NVIDIA RTX A5000 GPU.\nVI-A\n3\nEvaluation Metrics\nThe UQ module is evaluated using the Area Under the Sparsification Error (AUSE)\n[\nilg2018uncertainty\n]\n, computed as the area between error curves from two sparsification processes: one using the SDF values of ground-truth mesh vertices and the other using their uncertainty. A low AUSE indicates that the predicted uncertainty aligns more closely with the actual prediction error.\nTo assess the quality of the mesh, we follow the metrics used in previous works\n[\nyan2023active\n,\nfeng2024naruto\n]\n,\ncompletion (cm) and completion ratio (%) with 5 cm threshold.\nWe apply mesh culling\n[\nwang2023coslam\n]\nto remove unobserved regions and points that are outside the total scene.\nVI-A\n4\nBaseline Algorithms\nThe UQ module is compared with those from other implicit neural representation (INR)-based active reconstruction methods\n[\nfeng2024naruto\n,\nkuang2024active\n]\n.\nFor assessing the overall system, we include frontier-based exploration\n[\nyamauchi1997frontier\n]\nand another INR-based method\n[\nyan2023active\n]\nas additional baselines.\nTABLE I\n:\nCompletion metrics for the Gibson and MP3D datasets.\nâ€ \n\\dagger\nindicates metrics from meshes converted from 3D Gaussians for a surface reconstruction comparison.\nGibson\nMP3D\nComp. â†‘\nComp. â†“\nComp. â†‘\nComp. â†“\n(%)\n(\nc\nâ€‹\nm\ncm\n)\n(%)\n(\nc\nâ€‹\nm\ncm\n)\nFBE\n[\nyamauchi1997frontier\n]\n68.91\n14.42\n71.18\n9.78\nANM\n[\nyan2023active\n]\n80.45\n7.44\n73.15\n9.11\nANM-S\n[\nkuang2024active\n]\n92.10\n2.83\n89.74\n4.14\nNaruto\n[\nfeng2024naruto\n]\n90.31\n4.31\n90.18\n3.00\nFisherRF\nâ€ \n[\njiang2024fisherrf\n]\n82.24\n5.06\n80.26\n6.55\nActiveSplat\nâ€ \n[\nli2025activesplat\n]\n84.71\n5.41\n76.71\n5.26\nOurs\n93.49\n2.60\n92.22\n2.90\nVI-B\nEvaluation of Uncertainty Quantification\nFor uncertainty quantification, we evaluated our epistemic UQ method against other INR-based active reconstruction methods, using both qualitative and quantitative analyses.\nFor a fair comparison, all methods were trained using the same trajectories obtained from\n[\nfeng2024naruto\n]\n, and the AUSE was computed every 500 steps and averaged over the entire run.\nAs depicted in Fig.\n5\na, our qualitative analysis visualizes the uncertainty metric and SDF error across two different scenes.\nCompared with the baselines, our uncertainty metric, which accounts for epistemic uncertainty, showed a strong correlation with SDF errors, indicating that regions with high uncertainty correspond to areas of lower accuracy.\nAs shown in Fig.\n5\nb, our UQ outperforms the baselines across all selected scenes.\nFigure 6\n:\nComparison of completion ratio and completion metrics with respect to time for scenes of the Gibson dataset.\nFigure 7\n:\nReconstructed meshes from different active reconstruction methods (GT, ANM-S\n[\nkuang2024active\n]\n, Naruto\n[\nfeng2024naruto\n]\n,\nOurs\n) for scenes from the Gibson dataset (Top:\nRibera\n, Bottom:\nSwormville\n). The magenta and red boxes indicate regions where our method achieves more accurate reconstructions than\n[\nkuang2024active\n]\nand\n[\nfeng2024naruto\n]\n, respectively, effectively avoiding artifacts and empty holes.\nVI-C\nEvaluation of Active Scene Reconstruction\nA brief overview of the evaluation on the completion of the active reconstruction methods is given in Table\nI\n.\nMore detailed assessments of individual scenes can be found in the supplementary material.\nThe result shows that the proposed method is superior in terms of completion metrics in the selected datasets.\nAs can be observed in Fig.\n7\n, our method produces a more meticulously reconstructed mesh.\nFig.\n6\ncompares our approach with the state-of-the-art INR-based active reconstruction system\n[\nfeng2024naruto\n]\nover different time steps of the planning loop on the Gibson dataset, focusing on complex scenes with more than six rooms. We applied the same mesh culling procedure used in our method to all baselines at each evaluation step.\nOur method generally outperforms the baseline, except in the early stages of reconstruction. This is due to the hierarchical planning structure â€”- initially, the local planner dominates, prioritizing nearby information over global exploration.\nAs reconstruction progresses, the global planner guides broader scene coverage while the local planner refines view selection, leading to superior completion metrics over time.\nTo validate the superiority of surface reconstruction over 3DGS-based methods, we report comparison with\n[\njiang2024fisherrf\n,\nli2025activesplat\n]\nin Table\nI\n. To ensure fairness, we convert the 3D Gaussians into a mesh following\n[\nhuang20242d\n]\n, by rendering depth images at the training viewpoints, feeding them into TSDF fusion\n[\nnewcombe2011kinectfusion\n]\n, and extracting the scene mesh. Our method achieves higher completion metrics than 3DGS-based methods, indicating a more complete surface reconstruction.\nOur system operates at 9.2 FPS. A single mapping, UQ over the entire scene, global planning, local planning, and escaping-local-minima step take 204, 2.28, 28.5, 16.1, and 158 ms, respectively, in the Gibsonâ€“Eudora scene.\nVI-D\nAblation Study\nSince the Gibson scenes include environments with varying scales and number of rooms, scene reconstruction becomes particularly challenging. Therefore, we conduct ablation studies on this dataset.\nWe evaluate the efficacy of the proposed UQ method as a plug-and-play enhancement that can be integrated with various active reconstruction planners requiring uncertainty quantification.\nWe assess the active reconstruction performance of a planner adapted from\n[\nfeng2024naruto\n]\n, where our UQ method is employed for the goal search process.\nThe average results on the Gibson dataset for the selected scenes are presented in Table\nII\n.\nThe improved reconstruction results across all metrics, compared to\n[\nfeng2024naruto\n]\n, demonstrate that the proposed UQ enhances the ability to identify informative goals for active reconstruction.\nThis suggests that inferring epistemic uncertainty is crucial for informative path planning and highlights the versatility of the proposed UQ method.\nTABLE II\n:\nAblation study on UQ in the Gibson dataset.\nComp. â†“\nComp. Ratio â†‘\n(cm)\n(%)\n[\nfeng2024naruto\n]\n4.31\n90.31\n[\nfeng2024naruto\n]\nw/ Our UQ\n2.69\n92.69\nOurs\n2.60\n93.49\nVI-E\nReal-world Experiments\nTo demonstrate the efficacy of the proposed framework, we conducted hardware experiments using a ground robot (Turtlebot3 Waffle) equipped with a RGB-D camera (Realsense D455), with pose information provided by a motion capture system. To adapt the 6D planning framework for a ground vehicle, we sampled 3D target viewpoints. The proposed framework was tested in an indoor lab environment, and the resulting scene reconstruction is shown in Fig.\n8\n. Further details are provided in the supplementary video.\nFigure 8\n:\nResult from the real-world experiment. The experimental scene (left), and the reconstructed mesh (right).\nVII\nConclusion and Discussion\nWe presented an active 3D scene reconstruction method based on NeRF by introducing epistemic uncertainty through EDL. This enables real-time online planning with uncertainty estimates that align well with model error, effectively distinguishing epistemic from aleatoric uncertainty.\nBy combining a global coverage planner with a local, information-driven planner, our system scales effectively to large environments while leveraging epistemic uncertainty and adaptively generating camera trajectories that maximize information gain.\nOur proposed method identifies informative trajectories that reduce human intervention in data collection, achieve more complete reconstructions than prior approaches, and yield high-quality meshes for downstream tasks.\nFuture work includes removing the assumption of prior localization for reconstruction in unseen environments\n[\nkim2021topology\n]\n, and incorporating object-level information\n[\nlee2024category\n]\nto support high-level tasks.\nReferences",
    "preview_text": "We present HERE, an active 3D scene reconstruction framework based on neural radiance fields, enabling high-fidelity implicit mapping. Our approach centers around an active learning strategy for camera trajectory generation, driven by accurate identification of unseen regions, which supports efficient data acquisition and precise scene reconstruction. The key to our approach is epistemic uncertainty quantification based on evidential deep learning, which directly captures data insufficiency and exhibits a strong correlation with reconstruction errors. This allows our framework to more reliably identify unexplored or poorly reconstructed regions compared to existing methods, leading to more informed and targeted exploration. Additionally, we design a hierarchical exploration strategy that leverages learned epistemic uncertainty, where local planning extracts target viewpoints from high-uncertainty voxels based on visibility for trajectory generation, and global planning uses uncertainty to guide large-scale coverage for efficient and comprehensive reconstruction. The effectiveness of the proposed method in active 3D reconstruction is demonstrated by achieving higher reconstruction completeness compared to previous approaches on photorealistic simulated scenes across varying scales, while a hardware demonstration further validates its real-world applicability.\n\nHERE: Hierarchical Active Exploration of Radiance Field with Epistemic Uncertainty Minimization\nTaekbeom Lee\nâˆ—\n,\n\nDabin Kim\nâˆ—\n, Youngseok Jang, and H. Jin Kim\n:\nâˆ—\n{}^{*}:\nEqual contribution. The authors are with the Department of Aerospace Engineering, Seoul National University, Seoul 08826, South Korea (e-mail: ltb1128, dabin404, duscjs59, hjinkim@snu.ac.kr, corresponding author: H. Jin Kim).\nThis work was supported by Samsung Research Funding & Incubation Center of Samsung Electronics under Project Number SRFC-IT2402-17.\nAbstract\nWe present\nHERE\n, an active 3D scene reconstruction framework based on neural ra",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "neural radiance fields",
        "active 3D scene reconstruction",
        "epistemic uncertainty",
        "hierarchical exploration",
        "camera trajectory generation"
    ],
    "one_line_summary": "HEREæ˜¯ä¸€ä¸ªåŸºäºç¥ç»è¾å°„åœºçš„ä¸»åŠ¨3Dåœºæ™¯é‡å»ºæ¡†æ¶ï¼Œé€šè¿‡è®¤çŸ¥ä¸ç¡®å®šæ€§æœ€å°åŒ–å’Œåˆ†å±‚æ¢ç´¢ç­–ç•¥å®ç°é«˜æ•ˆé«˜ä¿çœŸé‡å»ºã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T06:23:29Z",
    "created_at": "2026-01-21T12:09:07.967379",
    "updated_at": "2026-01-21T12:09:07.967386",
    "recommend": 0
}