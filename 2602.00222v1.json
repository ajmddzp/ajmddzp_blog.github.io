{
    "id": "2602.00222v1",
    "title": "MapDream: Task-Driven Map Learning for Vision-Language Navigation",
    "authors": [
        "Guoxin Lian",
        "Shuo Wang",
        "Yucheng Wang",
        "Yongcai Wang",
        "Maiyue Chen",
        "Kaihui Wang",
        "Bo Zhang",
        "Zhizhong Su",
        "Deying Li",
        "Zhaoxin Fan"
    ],
    "abstract": "è§†è§‰è¯­è¨€å¯¼èˆªè¦æ±‚æ™ºèƒ½ä½“åœ¨éƒ¨åˆ†å¯è§‚æµ‹çš„ä¸‰ç»´ç¯å¢ƒä¸­éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œè¿™ä¿ƒä½¿éœ€è¦æ„å»ºèƒ½èšåˆå±€éƒ¨æ„ŸçŸ¥ä¹‹å¤–ç©ºé—´ä¿¡æ¯çš„åœ°å›¾è¡¨ç¤ºã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–ç‹¬ç«‹äºå¯¼èˆªç­–ç•¥æ„å»ºçš„æ‰‹å·¥åœ°å›¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ°å›¾åº”æ˜¯ç›´æ¥ç”±å¯¼èˆªç›®æ ‡å¡‘é€ çš„å­¦ä¹ è¡¨ç¤ºï¼Œè€Œéè¯¦å°½çš„ç¯å¢ƒé‡å»ºã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMapDreamâ€”â€”ä¸€ç§åœ°å›¾åœ¨ç¯æ¡†æ¶ï¼Œå°†åœ°å›¾æ„å»ºå»ºæ¨¡ä¸ºè‡ªå›å½’é¸Ÿç°å›¾åˆæˆä»»åŠ¡ã€‚è¯¥æ¡†æ¶è”åˆå­¦ä¹ åœ°å›¾ç”Ÿæˆä¸åŠ¨ä½œé¢„æµ‹ï¼Œå°†ç¯å¢ƒä¿¡æ¯è’¸é¦ä¸ºç´§å‡‘çš„ä¸‰é€šé“é¸Ÿç°å›¾ï¼Œä»…ä¿ç•™å¯¹å¯¼èˆªè‡³å…³é‡è¦çš„ç¯å¢ƒå¯é€šè¡Œä¿¡æ¯ã€‚é€šè¿‡ç›‘ç£é¢„è®­ç»ƒæ„å»ºå¯é çš„\"åœ°å›¾-æ§åˆ¶\"æ¥å£ï¼Œè€Œè‡ªå›å½’è®¾è®¡åˆ™æ”¯æŒé€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒå®ç°ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ã€‚åœ¨R2R-CEå’ŒRxR-CEæ•°æ®é›†ä¸Šçš„å®éªŒå–å¾—äº†å•ç›®è§†è§‰å¯¼èˆªçš„æœ€ä½³æ€§èƒ½ï¼ŒéªŒè¯äº†ä»»åŠ¡é©±åŠ¨çš„ç”Ÿæˆå¼åœ°å›¾å­¦ä¹ æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
    "url": "https://arxiv.org/abs/2602.00222v1",
    "html_url": "https://arxiv.org/html/2602.00222v1",
    "html_content": "MapDream: Task-Driven Map Learning for Vision-Language Navigation\nGuoxin Lian\n1,3,âˆ—\n,\nShuo Wang\n1,3,âˆ—\n,\nYucheng Wang\n3,â€ \n,\nYongcai Wang\n1,â€¡\n,\nMaiyue Chen\n3\n,\nKaihui Wang\n3\n,\nBo Zhang\n3\n,\nZhizhong Su\n3\n,\nDeying Li\n1\n,\nZhaoxin Fan\n2,â€¡\n1\nRenmin University of China\n2\nBeijing Advanced Innovation Center for Future Blockchain and Privacy Computing\n3\nHorizon Robotics\nâˆ—\nEqual contribution\nâ€ \nProject leader\nâ€¡\nCorresponding authors\nAbstract\nVision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception.\nHowever, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy.\nWe argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions.\nBased on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive birdâ€™s-eye-view (BEV) image synthesis.\nThe framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances.\nSupervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning.\nExperiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.\n1\nIntroduction\nFigure 1\n:\nMap-in-the-Loop Architecture.\nUnlike previous approaches that either omit maps or rely on expert-designed representations, MapDream adopts a map-in-the-loop design that learns a task-driven generative map jointly with the navigation policy.\nRed dashed arrows denote training-time gradient flow from navigation objectives, illustrating how the learned map representation is directly shaped by downstream tasks.\nAbbreviations: Obs denotes observations, Inst instructions, and Act actions.\nVision-Language Navigation (VLN)\n[\n36\n,\n3\n,\n14\n]\nis a challenging task in the field of embodied artificial intelligence that requires agents to ground natural language instructions into coherent action sequences within complex environments.\nA central difficulty of VLN is partial observability. Agents perceive the environment only through local, sequential observations, which limits their understanding of the space. During navigation, they must reason over incomplete and progressively revealed spatial information.\nAs a result, in current VLN pipelines, aggregating past observations into a persistent spatial state is a standard and integral component.\nHence, most existing VLN methods\n[\n7\n,\n42\n,\n34\n]\nincorporate map representations to provide spatial context for decision making.\nMany existing map representation methods, such as topological graphs, BEV representations\n[\n1\n]\n, or grid-based memories\n[\n32\n]\n, help mitigate partial observability and provide spatial abstractions useful for navigation. These approaches enhance navigation performance by offering spatial context, particularly under partial observability, and improve decision-making\n[\n13\n,\n2\n,\n15\n]\n. The strength of these methods lies in providing valuable environmental information through maps, boosting overall navigation performance. However, most of these maps are constructed independently of the decision-making process and are consumed by the policy as fixed inputs.\nThe limitation of this approach is that map representations typically remain outside the learning loop that governs navigation behavior, preventing them from being refined through learning to align with task objectives (see Fig\n1\n). Since these maps are not directly shaped by task-driven learning signals, they cannot be adjusted during training to align with the semantics of instructions or the needs of the navigation policy, leading to a mismatch between the spatial information encoded in the map and the decision-making requirements of the navigation policy. This mismatch highlights that most existing maps are designed by experts, rather than being learned end-to-end from navigation objectives. Motivated by the above analysis, we therefore argue that effective map representations for VLN should be learned as part of the decision-making process and optimized for navigation objectives.\nUnder this task-driven formulation, maps need not encode the full state of the environment, but only a compact spatial representation sufficient for navigation decisions.\nBased on this insight, we propose MapDream, a framework that unifies spatial representation learning and decision making. The proposed method is built upon three core components: a map-in-the-loop architecture, supervised pre-training, and reinforcement fine-tuning.\nFirst, the map-in-the-loop architecture comprises a task-driven map module and a VLN policy, where BEV maps are autoregressively generated from egocentric observation histories and language instructions and then provided to the policy as structured spatial context for multi-step action prediction.\nSecond, a supervised pre-training stage constructs task-driven BEV supervision and trains both the map generator and the policy to establish a reliable mapping-to-control interface, constraining the representations to fixed resolutions and token budgets before downstream task-level optimization.\nFinally, joint reinforcement fine-tuning is performed under a unified navigation objective, using multi-source rewards, group-based rollout optimization, and relative-advantage objectives to directly shape both components toward encoding navigation-critical information rather than mere geometric reconstruction.\nWe demonstrate the effectiveness of this approach by achieving state-of-the-art performance on the standard VLN benchmarks R2R-CE and RxR-CE under the monocular setting. The framework also exhibits strong generalization to unseen environments, highlighting the practical value of task-driven map representations for real-world navigation tasks.\nOur main contributions are:\nâ€¢\nWe first introduce a task-driven perspective on map representations for VLN, reframing maps as representations shaped by downstream navigation objectives rather than fixed by expert design.\nâ€¢\nWe present MapDream, which formulates map construction as an autoregressive generative process and enables joint optimization of the map module and navigation policy under reinforcement learning.\nâ€¢\nWe achieve state-of-the-art results on the standard VLN benchmarks R2R-CE and RxR-CE under the monocular setting, with strong generalization to unseen environments, validating task-driven map representations as an effective paradigm.\n2\nRelated Works\n2.1\nVision Language Navigation\nLarge-scale pretrained vision-language models (VLMs) have recently become a strong foundation for VLN, substantially improving visual grounding and language understanding through large-scale multimodal pretraining\n[\n4\n,\n23\n]\n. Building on such pretrained VLMs, recent VLA-based VLN systems couple perception, language, and control to directly predict actions from monocular RGB streams and instruction inputs\n[\n20\n,\n40\n,\n10\n,\n35\n]\n, yielding substantial gains in generalization and transferability.\nRecent VLN research has further enhanced VLA-based navigation by injecting explicit reasoning signals and richer world priors. CoT-style supervision and progress-aware reasoning encourage models to internalize structured reasoning processes or instruction-progress states to support decision making\n[\n28\n,\n31\n,\n21\n]\n. Other lines of work address partial observability by enriching visual context, either through hallucinated panoramic cues from monocular streams\n[\n30\n]\n, or by introducing a dual implicit neural memory that separately models spatial-geometric and visual-semantic information as compact, fixed-size representations\n[\n38\n]\n. Map-based VLA systems further incorporate structured spatial states in different forms, such as MapNav\n[\n42\n]\n, which employs annotated semantic maps to replace historical frames, and Dynam3D\n[\n34\n]\n, which introduces dynamically updated layered 3D tokens as visual inputs.\nMapDream departs from these approaches by learning a compact BEV representation via generative map construction and integrating it into the VLN policy for end-to-end optimization.\nFigure 2\n:\nOverview of the MapDream Framework.\nThe diagram shows the two-stage optimization of MapDream. StageÂ 1 learns structured task-driven maps from visual observations and language instructions for initialization and supervised policy training. StageÂ 2 jointly optimizes the map module and VLN policy through reinforcement learning under a unified navigation objective, allowing the map to be shaped by downstream tasks.\n2.2\nMap Representation for Embodied Navigation\nMap representations provide spatial context beyond local perception and have long been central to embodied navigation. Classical navigation pipelines build explicit metric, semantic, or topological maps through external mapping modules and couple them to planners or hierarchical controllers\n[\n15\n,\n19\n,\n11\n]\n. These formulations treat map construction as a structured prediction or reconstruction problem that emphasizes completeness and interpretability, making them well-suited for explicit mapping pipelines but less aligned with task-driven abstractions that directly support downstream decision making.\nWithin the VLN setting, prior work has explored map-like representations as structured top-down interfaces for navigation, including multimodal BEV pre-training\n[\n1\n]\n, egocentric grid memory maps\n[\n32\n]\n, BEV scene graphs\n[\n22\n]\n, and language-conditioned egocentric semantic map prediction\n[\n13\n]\n. While effective, these representations are typically constructed using fixed design choices or auxiliary objectives, and remain weakly coupled to the downstream action policy.\nIn contrast, MapDream learns map representations in a task-driven manner, where the content and structure of the map are shaped by downstream navigation objectives rather than fixed design choices.\n2.3\nMap Construction as Image Synthesis\nViewing maps as compact decision-oriented representations removes the need for exhaustive environment reconstruction and naturally connects map construction to recent advances in generative modeling, which demonstrate that complex visual structures can be synthesized from partial and high-level conditioning signals. Both diffusion-based and autoregressive models exhibit strong controllability and semantic alignment when generating images conditioned on text or multimodal inputs, such as instruction-guided image editing and multimodal image manipulation\n[\n26\n,\n6\n,\n5\n,\n25\n]\n. These successes establish image synthesis as a powerful paradigm for generating structured visual representations.\nAutoregressive multimodal models such as Janus-Pro\n[\n9\n]\nfurther unify visual and textual generation within a single generative framework, enabling flexible conditioning over mixed-modality inputs and sequential generation over heterogeneous tokens. However, such models typically assume same-view generation and condition on a single image, making them ill-suited for VLN map construction, which requires cross-view synthesis from egocentric observations to birdâ€™s-eye-view (BEV) representations, cross-domain generation from natural images to abstract spatial maps, and conditioning on multi-frame observation histories under partial observability.\nMapDream formulates map construction as image synthesis for embodied navigation, generating compact BEV images from multi-frame egocentric observations and language instructions that are sufficient to directly support navigation decisions.\n3\nMethod\n3.1\nOverview\nMapDream is built upon three core components, as shown in Fig.\n2\n.\nSpecifically, it features\n(1) a two-module system composed of a task-driven map module and a VLN policy for spatial representation learning and action prediction;\n(2) a supervised pre-training stage that establishes a reliable mapping-to-control interface for structured BEV representations under fixed resolution and token budgets; and\n(3) a joint reinforcement fine-tuning stage under a unified navigation objective that directly shapes both the map representation and the policy.\n3.2\nMap-in-the-Loop Architecture\nMapDream adopts a two-component architecture consisting of a task-driven map module and a VLN policy that operate throughout both training stages. At each time step\nt\nt\n, the map module receives an egocentric observation history\nO\nt\nO_{t}\n, the current frame\no\nt\no_{t}\n, and the instruction\nI\nI\n, and autoregressively generates a multi-channel BEV representation\nM\nt\nM_{t}\nthat summarizes navigationally relevant spatial context. This BEV map serves as an explicit spatial interface between perception and control and is provided to the VLN policy together with\nO\nt\nO_{t}\n,\no\nt\no_{t}\n, and\nI\nI\n.\nThe VLN policy consumes the map and predicts a sequence of\nN\nN\nfuture actions\na\nt\n:\nt\n+\nN\nâˆ’\n1\na_{t:t+N-1}\n, enabling multi-step planning under partial observability. Across both supervised pre-training and reinforcement fine-tuning, the two components are optimized either with separate objectives or under a unified navigation reward, while remaining connected through the BEV representation, which functions as the sole spatial representation passed between modules.\nBy explicitly decoupling spatial abstraction from action prediction through the BEV interface, MapDream allows the map module to focus on constructing navigation-relevant representations while the policy concentrates on decision making.\n3.3\nSupervised Pre-training\nIn StageÂ 1, we train the map module and the VLN policy with separate supervised objectives to provide a stable initialization before reinforcement fine-tuning. This stage focuses on constructing task-driven BEV supervision and optimizing the two components with separate losses. It consists of three parts: task-driven map supervision, pre-training the map module, and pre-training the VLN policy.\n3.3.1\nMap Supervision\nWe adopt lightweight ground-truth map signals during supervised pre-training to encode navigation-critical cues; this design is not exclusive, and alternative compact variants are possible. In MapDream, the map supervision is represented as a three-channel BEV image consisting of Occupancy, Distance, and Landmark maps, reflecting common spatial cues exploited in embodied navigation, where traversability, goal-directed geometry, and stable semantic anchors are crucial for long-horizon planning and instruction grounding.\nThe Occupancy map uses\n{\n0\n,\n128\n,\n255\n}\n\\{0,128,255\\}\nto represent observed impassable, unobserved, and observed traversable regions, respectively. The Distance map provides a dense relative geodesic-to-goal signal centered at the agent, normalized and truncated to\n[\n0\n,\n255\n]\n[0,255]\n, while the Landmark map highlights static scene objects referenced in natural language instructions. Together, these channels form a BEV representation that captures complementary aspects of geometry, goal structure, and semantic grounding beyond the current field of view.\n3.3.2\nPre-training the Map Module\nUsing the generated task-driven maps as supervision, we train an autoregressive model to predict BEV maps from egocentric observations and language instructions:\nM\nt\n=\nG\nâ€‹\n(\nO\nt\n,\no\nt\n,\nI\n)\n,\nM_{t}=G(O_{t},o_{t},I),\n(1)\nwhere\nG\nG\ndenotes the map module. The generation process is supervised with a reconstruction objective,\nâ„’\nMap\n=\nâˆ’\nâˆ‘\nt\nlog\nâ¡\np\nÎ¸\nâ€‹\n(\ny\n^\nt\nâˆ£\ny\n^\n<\nt\n,\nO\nt\n,\no\nt\n,\nI\n)\n,\n\\mathcal{L}_{\\text{Map}}=-\\sum_{t}\\log p_{\\theta}(\\hat{y}^{t}\\mid\\hat{y}^{<t},O_{t},o_{t},I),\n(2)\nwhich encourages spatially consistent and task-relevant BEV predictions.\n3.3.3\nPre-training the VLN Policy\nThe VLN policy is trained to predict multi-step action sequences conditioned on the predicted maps and visualâ€“language context by minimizing a cross-entropy loss,\nâ„’\nAction\n=\nâˆ’\nlog\nâ¡\nÏ€\nâ€‹\n(\na\nt\n:\nt\n+\nN\nâˆ’\n1\nâˆ—\nâˆ£\nO\nt\n,\no\nt\n,\nM\nt\n,\nI\n)\n.\n\\mathcal{L}_{\\text{Action}}=-\\log\\pi\\left(a_{t:t+N-1}^{\\ast}\\mid O_{t},\\,o_{t},\\,M_{t},\\,I\\right).\n(3)\nThis objective equips the policy with multi-step decision capability and provides a strong initialization for subsequent reinforcement fine-tuning.\nTable 1\n:\nComparison of different methods on the R2R-CE Val-Unseen and RxR-CE Val-Unseen splits. Observations used include single RGB camera (S.RGB), depth sensor (Depth), panoramic view (Pano.) and map representation (Map).\nâ€ \n\\dagger\nindicates methods without using LLMs.\nMethod\nVenue\nObservation\nMap\nR2R-CE Val-Unseen\nRxR-CE Val-Unseen\nS.RGB\nDepth\nPano.\nNE\nâ†“\n\\downarrow\nOSR\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nNE\nâ†“\n\\downarrow\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nGridMM\nâ€ \n[\n32\n]\nICCV2023\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n5.11\n61.0\n49.0\n41.0\nâ€“\nâ€“\nâ€“\nETPNav\nâ€ \n[\n2\n]\nTPAMI2024\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n4.71\n65.0\n57.0\n49.0\n5.64\n54.7\n44.8\nBEVBert\nâ€ \n[\n1\n]\nICCV2023\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n4.57\n67.0\n59.0\n50.0\nâ€“\nâ€“\nâ€“\nBEVBert-FSTTA\nâ€ \n[\n12\n]\nICML2024\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n4.39\n65.0\n60.0\n51.0\nâ€“\nâ€“\nâ€“\nBEVBert-FEEDTTA\nâ€ \n[\n16\n]\nICML2025\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n4.33\n69.0\n61.0\n50.0\nâ€“\nâ€“\nâ€“\nNaVid-4D\n[\n20\n]\nICRA2025\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n5.99\n55.7\n43.8\n37.1\nâ€“\nâ€“\nâ€“\nsim2real\nâ€ \n[\n33\n]\nCoRL2024\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n5.95\n55.8\n44.9\n30.4\nâ€“\nâ€“\nâ€“\nNavMorph\nâ€ \n[\n37\n]\nICCV2025\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n5.75\n56.9\n47.9\n33.2\n8.85\n30.7\n22.8\nCM2\nâ€ \n[\n13\n]\nCVPR2022\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n7.02\n41.0\n34.0\n27.0\nâ€“\nâ€“\nâ€“\nWS-MGMap\nâ€ \n[\n7\n]\nNeurIPS2022\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n6.28\n47.0\n38.0\n34.0\nâ€“\nâ€“\nâ€“\nNaVid\n[\n41\n]\nRSS2024\nâœ“\n\\checkmark\n5.47\n49.1\n37.4\n35.9\nâ€“\nâ€“\nâ€“\nUni-NaVid\n[\n40\n]\nRSS2025\nâœ“\n\\checkmark\n5.58\n53.3\n47.0\n42.7\n6.24\n48.7\n40.9\nNaVILA\n[\n10\n]\nRSS2025\nâœ“\n\\checkmark\n5.22\n62.5\n54.0\n49.0\n6.77\n49.3\n44.0\nAux-Think\n[\n29\n]\nNeurIPS2025\nâœ“\n\\checkmark\n6.08\n60.0\n54.8\n46.9\nâ€“\nâ€“\nâ€“\nMonoDream\n[\n30\n]\nAAAI2025\nâœ“\n\\checkmark\n5.45\n61.5\n55.8\n49.1\nâ€“\nâ€“\nâ€“\nMapNav\n[\n42\n]\nACL2025\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n4.93\n53.0\n39.7\n37.2\n7.62\n32.6\n27.7\nDynam3D\n[\n34\n]\nNeurIPS2025\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n5.34\n62.1\n52.9\n45.7\nâ€“\nâ€“\nâ€“\nMapDream (Ours)\nâ€“\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n4.59\n64.4\n59.8\n54.4\n4.96\n59.4\n49.2\n3.4\nReinforcement Fine-tuning\nIn StageÂ 2, we jointly fine-tune the map module and the VLN policy under a unified navigation objective to directly optimize task performance. Optimization is guided by two reward componentsâ€”an action reward and a format rewardâ€”and is carried out using Group Relative Policy Optimization (GRPO)\n[\n27\n]\n.\nAction Reward.\nThe action reward enforces stepwise correctness in the predicted action sequence by rewarding only the longest correct prefix. Once an incorrect action appears, no further reward is assigned, reflecting the sequential credit assignment nature of navigation. It is defined as:\nr\nact\n=\nâˆ‘\ni\n=\n0\nN\nâˆ’\n1\nâˆ\nj\n=\n0\ni\nğŸ™\nâ€‹\n[\na\nt\n+\nj\n=\na\nt\n+\nj\nâˆ—\n]\n.\nr_{\\text{act}}=\\sum_{i=0}^{N-1}\\prod_{j=0}^{i}\\mathbbm{1}[a_{t+j}=a_{t+j}^{*}].\n(4)\nFormat Reward.\nThe format reward ensures that the predicted action sequence satisfies the required syntactical structure:\nr\nfmt\n=\n{\n1\n,\nif\nâ€‹\na\nt\n:\nt\n+\nN\nâˆ’\n1\nâ€‹\nis valid\n,\n0\n,\notherwise\n.\nr_{\\text{fmt}}=\\begin{cases}1,&\\text{if }a_{t:t+N-1}\\text{ is valid},\\\\\n0,&\\text{otherwise}.\\end{cases}\n(5)\nTotal Reward.\nThe total reinforcement signal is defined as\nr\ntotal\n=\nr\nact\n+\nr\nfmt\nr_{\\text{total}}=r_{\\text{act}}+r_{\\text{fmt}}\n.\nRollout.\nDuring each rollout of length\nK\nK\n, the two components operate as a coupled process, producing a sequence of predicted maps and action proposals\n{\nM\nt\n}\nt\n=\n0\nK\nâˆ’\n1\n\\{M_{t}\\}_{t=0}^{K-1}\nand\n{\na\nt\n:\nt\n+\nN\nâˆ’\n1\n}\nt\n=\n0\nK\nâˆ’\n1\n\\{a_{t:t+N-1}\\}_{t=0}^{K-1}\n.\nOptimization Objective.\nWe sample a group of candidate rollouts indexed by\nk\nk\nand compute relative advantages within the group. The optimization objective follows a GRPO-style formulation:\nâ„’\nVLN\n=\nâˆ’\nğ”¼\nk\nâ€‹\n[\nmin\nâ¡\n(\nÏ\nâ€‹\n(\nk\n)\nâ€‹\nA\nâ€‹\n(\nk\n)\n,\nclip\nâ¡\n(\nÏ\nâ€‹\n(\nk\n)\n,\n1\nâˆ’\nÏµ\n,\n1\n+\nÏµ\n)\nâ€‹\nA\nâ€‹\n(\nk\n)\n)\n]\n,\n\\mathcal{L}_{\\text{VLN}}=-\\mathbb{E}_{k}\\!\\left[\\min\\left(\\begin{aligned} &\\rho(k)\\,A(k),\\\\\n&\\operatorname{clip}\\!\\big(\\rho(k),\\,1-\\epsilon,\\,1+\\epsilon\\big)\\,A(k)\\end{aligned}\\right)\\right],\n(6)\nwhere the relative advantage ratio\nÏ\nâ€‹\n(\nk\n)\n\\rho(k)\nis defined as\nÏ\nâ€‹\n(\nk\n)\n\\displaystyle\\rho(k)\n=\nÏ€\nÎ¸\nnav\nâ€‹\n(\na\nt\n:\nt\n+\nN\nâˆ’\n1\n(\nk\n)\n|\nM\nt\n(\nk\n)\n,\nO\nt\n,\no\nt\n,\nI\n)\nÏ€\nÎ¸\nold\nnav\nâ€‹\n(\na\nt\n:\nt\n+\nN\nâˆ’\n1\n(\nk\n)\n|\nM\nt\n(\nk\n)\n,\nO\nt\n,\no\nt\n,\nI\n)\n\\displaystyle=\\frac{\\pi_{\\theta}^{\\text{nav}}\\!\\Big(a_{t:t+N-1}^{(k)}\\,\\Big|\\,M_{t}^{(k)},\\,O_{t},\\,o_{t},\\,I\\Big)}{\\pi_{\\theta_{\\text{old}}}^{\\text{nav}}\\!\\Big(a_{t:t+N-1}^{(k)}\\,\\Big|\\,M_{t}^{(k)},\\,O_{t},\\,o_{t},\\,I\\Big)}\n(7)\nâ‹…\np\nÏ•\nbev\nâ€‹\n(\nM\nt\n(\nk\n)\n|\nO\nt\n,\no\nt\n,\nI\n)\np\nÏ•\nold\nbev\nâ€‹\n(\nM\nt\n(\nk\n)\n|\nO\nt\n,\no\nt\n,\nI\n)\n,\n\\displaystyle\\quad\\cdot\\frac{p_{\\phi}^{\\text{bev}}\\!\\Big(M_{t}^{(k)}\\,\\Big|\\,O_{t},\\,o_{t},\\,I\\Big)}{p_{\\phi_{\\text{old}}}^{\\text{bev}}\\!\\Big(M_{t}^{(k)}\\,\\Big|\\,O_{t},\\,o_{t},\\,I\\Big)},\nwhere\nÏ€\nÎ¸\nnav\n\\pi_{\\theta}^{\\text{nav}}\nand\np\nÏ•\nbev\np_{\\phi}^{\\text{bev}}\ndenote the VLN policy and map module, respectively.\n(\nÎ¸\n,\nÏ•\n)\n(\\theta,\\phi)\nand\n(\nÎ¸\nold\n,\nÏ•\nold\n)\n(\\theta_{\\text{old}},\\phi_{\\text{old}})\nindicate current and reference parameters, and\nA\nâ€‹\n(\nk\n)\nA(k)\nis the normalized advantage at step\nk\nk\n.\n4\nExperiments\n4.1\nExperiment Setup\n4.1.1\nExperimental environments\nWe evaluate our method on the widely adopted continuous-environment VLN benchmarks R2R-CE\n[\n17\n]\nand RxR-CE\n[\n18\n]\n.\nResults are reported on the validation-unseen splits to assess generalization to novel environments.\nRxR is more challenging than R2R, featuring substantially longer trajectories and multilingual, fine-grained instructions that demand strong global spatial reasoning.\nWe focus on the continuous-environment (CE) protocol because continuous control introduces fine motion granularity and realistic noise, making navigation sensitive to small geometric deviations.\n4.1.2\nMetrics\nWe adopt the standard VLN evaluation protocol\n[\n17\n,\n18\n]\nto assess navigation performance using success rate (SR), oracle success rate (OSR), success weighted by path length (SPL), and navigation error (NE).\nWe primarily report SR and SPL, which capture task completion and path efficiency, respectively.\n4.2\nImplementation Details\n4.2.1\nDataset Collection\nWe leverage continuous VLN simulators from R2R-CE and RxR-CE to construct training data. Their annotated trajectories are converted into step-level samples, yielding 1200K state-action pairs. To support Stage 1 training, we generate BEV maps for these datasets. We follow standard egocentric BEV configurations used in recent VLN systems\n[\n7\n,\n1\n]\n, ensuring compatibility with indoor navigation tasks.\nThe Occupancy and Distance channels are derived from the Habitat simulator, while the Landmark channel is annotated using 3D referential grounding labels from IRef-VLA\n[\n39\n]\n. These BEV representations serve as the initial input to MapDream for map generation and policy learning. Additionally, we generate 500K non-oracle samples through exploratory rollouts in the training environments, improving robustness to out-of-distribution states and enhancing generalization across diverse scenarios.\n4.2.2\nTraining Details\nThe training of MapDream follows a two-stage procedure and is conducted on 8 NVIDIA H20 GPUs. StageÂ 1 performs supervised pre-training of both the map module and the VLN policy, while StageÂ 2 jointly fine-tunes them with reinforcement learning. StageÂ 1 runs for two epochs and takes approximately 60 hours, and StageÂ 2 performs 2000 RL steps and takes approximately 10 hours.\nIn StageÂ 1, we train Janus-Pro as the map module using ground-truth BEV maps. Janus-Pro is trained for one epoch with a batch size of 40 and a learning rate of\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\nin a supervised pre-training manner following Janus-Pro-R1\n[\n24\n]\n, extended to support multi-image and text inputs for cross-modal BEV generation. After SPT, Janus-Pro is frozen to provide stable task-driven spatial representations.\nThe generated BEV maps are then used to condition the VLN policy. The policy is initialized from pretrained NVILA-2B weights and trained with a mixture of oracle expert trajectories and DAgger-collected data. We optimize the policy with cross-entropy loss over the next three predicted action steps at each time step using a learning rate of\n1\nÃ—\n10\nâˆ’\n5\n1\\times 10^{-5}\n, providing a supervised cold-start for map-conditioned navigation.\nIn StageÂ 2, we apply GRPO optimization with a rollout size of 8, a clipping parameter of 0.28, and a KL coefficient of 0.0. The model is trained for 2000 steps with a learning rate of\n1\nÃ—\n10\nâˆ’\n6\n1\\times 10^{-6}\n, jointly fine-tuning both the map module and the VLN policy. Reinforcement signals consist of action and format rewards, enabling credit assignment to both spatial representations and policy decisions.\nTable 2\n:\nUnseen-Dataset generalization performance on the RxR-CE Val-Unseen split. All results are obtained only training on the R2R-CE training set.\nMethod\nRxR Val-Unseen\nNE\nâ†“\n\\downarrow\nOSR\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nCM2\n[\n13\n]\n8.98\n25.3\n14.4\n9.2\nWS-MGMap\n[\n7\n]\n9.83\n29.8\n15.0\n12.1\nA\n2\nNAV\n[\n8\n]\n-\n-\n16.8\n6.3\nNaVid\n[\n41\n]\n8.41\n34.5\n23.8\n21.2\nMonoDream\n[\n30\n]\n8.57\n35.9\n25.1\n21.6\nsim2real\n[\n33\n]\n8.79\n36.7\n25.5\n21.2\nMapDream (Ours)\n8.73\n38.4\n27.8\n23.3\nFigure 3\n:\nQualitative navigation example illustrating the effect of task-driven maps in MapDream.\n(Left) Trajectory comparison shows that MapDream (green) closely follows the ground-truth path (blue), while the VLN policy without maps deviates (red). (Right) Conditioned on monocular observations, MapDream generates task-driven BEV maps that retain navigation-critical spatial cues such as occupancy, distance, and landmarks. These maps provide compact task-driven abstractions for decision-making. The yellow-highlighted region marks a spatial decision point where the no-map policy deviates, whereas MapDream selects the correct path using map information.\n4.3\nComparison with State-of-the-Art Methods\nWe compare MapDream with state-of-the-art methods on the R2R-CE and RxR-CE benchmarks under a single RGB camera (monocular) observation setting. As shown in Table\n1\n, MapDream achieves the best overall performance among monocular approaches on both datasets, with the highest SR and SPL while reducing navigation error. It reaches 59.8 SR and 54.4 SPL on R2R-CE val-unseen and 59.4 SR and 49.2 SPL on RxR-CE, establishing new state-of-the-art results in this regime. Despite relying only on monocular inputs, MapDream surpasses panoramic-based methods in terms of SPL, which measures both success and path efficiency, indicating that its successful trajectories closely follow the reference demonstration paths.\nTable\n2\nfurther reports zero-shot generalization to RxR-CE when training only on R2R-CE.\nMapDream again attains the strongest SR and SPL among all competitors, suggesting that the learned maps capture transferable navigation-relevant structure rather than dataset-specific cues.\nAcross all settings, MapDream improves both success rate and path efficiency, which we attribute to its task-driven generative maps that are refined through two-stage optimization and reinforcement fine-tuning.\nThese results empirically validate that learning spatial abstractions under navigation objectives leads to more robust decision making in continuous environments.\n4.4\nQualitative Analysis\nTo better understand how task-driven maps benefit navigation, we visualize trajectories and intermediate representations in MapDream (see Fig.\n3\n). As shown on the left, MapDream produces navigation paths that closely align with ground-truth trajectories, whereas the VLN policy without maps deviates significantly. This suggests that the maps provide actionable spatial cues for decision-making during long-horizon navigation. On the right, we examine the predicted BEV maps generated from monocular observations. The maps retain navigation-relevant spatial structures, including occupancy, distance, and landmarks, and resemble the ground-truth layouts while remaining compact and task-driven. These results indicate that the learned maps serve as effective abstractions of the environment, supporting accurate navigation without requiring full scene reconstruction.\n4.5\nAblation Study\nWe conduct three ablation studies on R2R-CE that jointly probe MapDream along complementary design dimensions: optimization strategy, robustness to map initialization, and representation capacity. Concretely, we analyze the effect of two-stage training, the sensitivity of reinforcement fine-tuning to different channel initializations, and the trade-off between BEV map compactness and cold-start navigation performance.\n4.5.1\nTwo-Stage Training\nWe evaluate the effect of two-stage training in MapDream by comparing three configurations: a baseline VLN policy without maps, the map-conditioned model after StageÂ 1 supervised pre-training, and the full two-stage system with additional StageÂ 2 reinforcement fine-tuning.\nAs shown in Table\n3\n, SPT corresponds to StageÂ 1 supervised pre-training with map conditioning, while RFT denotes StageÂ 2 reinforcement fine-tuning that jointly updates the map module and the VLN policy. Introducing StageÂ 1 yields consistent improvements across all metrics over the baseline, demonstrating that generative task-driven maps provide useful spatial abstractions for instruction-following navigation. StageÂ 2 further boosts SR and SPL while reducing NE, indicating that reinforcement learning aligns the learned maps with downstream navigation behavior. Together, the two stages contribute complementary gains for long-horizon navigation.\nTable 3\n:\nEffect of staged learning on R2R-CE val-unseen.\nMap\nSPT\nRFT\nNE\nâ†“\n\\downarrow\nOSR\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\n-\n-\n-\n7.67\n45.8\n37.7\n32.1\nâœ“\nâœ“\n-\n7.03\n48.0\n42.2\n36.5\nâœ“\nâœ“\nâœ“\n6.35\n51.3\n45.6\n40.5\n4.5.2\nReinforcement Fine-tuning under Different Channel Initializations\nTo further examine the effect of reinforcement fine-tuning under different channel initializations, Table\n4\nreports results for maps initialized with only the Occupancy, Distance, or Landmark channel, as well as their full combination. Reinforcement learning consistently improves all variants, with SR gains of +3.4 (All), +5.5 (Distance), +4.1 (Landmark), and +4.2 (Occupancy), accompanied by increases in SPL (+3.6-5.5), OSR, and reduced NE.\nAlthough different channel choices yield different starting performance after supervised pretraining, reinforcement fine-tuning narrows these gaps, bringing all variants to similar final SR (43.6-45.6) and SPL (39.4-40.5). This indicates that StageÂ 2 is largely insensitive to the specific channel used for initialization. Overall, these results support MapDreamâ€™s premise that generative, task-driven BEV maps can be effectively aligned with navigation objectives through reinforcement learning.\nTable 4\n:\nEffect of Reinforcement Fine-tuning under Different Channel Initializations.\nChannel\nSPT\nRFT\nNE\nâ†“\n\\downarrow\nOSR\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nAll\nâœ“\n-\n7.03\n48.0\n42.2\n36.5\nâœ“\nâœ“\n6.35\n51.3\n45.6\n40.5\nDistance\nâœ“\n-\n7.18\n45.4\n39.4\n34.5\nâœ“\nâœ“\n6.49\n51.2\n44.9\n40.0\nLandmark\nâœ“\n-\n7.32\n48.7\n40.4\n35.3\nâœ“\nâœ“\n6.60\n51.9\n44.5\n39.7\nOccupancy\nâœ“\n-\n7.39\n45.4\n39.4\n34.8\nâœ“\nâœ“\n6.67\n49.6\n43.6\n39.4\n4.5.3\nBEV Map Resolution and Token Budget\nWe study the impact of BEV map resolution and token budget in the map generator under StageÂ 1.\nAs shown in\nTable\n5\n, increasing map size improves reconstruction fidelity but brings only marginal gains in navigation performance.\nNotably, the most compact configuration attains a comparable success rate to the largest model (42.2 vs. 43.1 SR), indicating that dense geometric reconstruction is unnecessary for effective map-conditioned navigation.\nInstead, generative task-driven maps primarily serve as actionable spatial abstractions for policy bootstrapping.\nReducing map size also substantially improves efficiency.\nIn particular, inference latency per decision step drops from 12.7â€‰s to 1.3â€‰s, making compact maps far more suitable for real-time continuous control.\nThis highlights a favorable accuracy-efficiency tradeoff and suggests that MapDream naturally operates in a low-resolution, low-token regime without sacrificing navigation performance.\nTable 5\n:\nEffect of BEV token capacity on R2R-CE val-unseen.\nResolution\nTokens\nNE\nâ†“\n\\downarrow\nOSR\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nTime(s)\n384\nÃ—\n384\n384\\times 384\n576\n6.92\n48.2\n43.1\n38.7\n12.7\n192\nÃ—\n192\n192\\times 192\n144\n7.12\n48.0\n42.7\n37.7\n4.5\n96\nÃ—\n96\n96\\times 96\n36\n7.03\n48.0\n42.2\n36.5\n1.3\n4.6\nReal-world Generalization\nTo assess real-world generalization, we deploy MapDream on a Unitree G1 humanoid equipped with a forward-facing RGB camera. The robot receives monocular observations and executes navigation actions directly from live inputs. As shown in Fig.\n4\n, MapDream generates task-driven maps that evolve with the robotâ€™s motion and encode navigation-relevant spatial affordances, allowing the robot to follow long-horizon language instructions successfully in real indoor environments. Notably, the model is trained only on the R2R-CE and RxR-CE simulators, yet transfers in a zero-shot manner to real-world, previously unseen indoor scenes.\nFigure 4\n:\nReal-World Navigation with Task-Driven Maps.\nReal-world deployment of MapDream on a humanoid platform. Given monocular input and a natural language instruction, MapDream constructs robot-centric task-driven BEV maps from the forward-facing viewpoint. The maps capture navigation-relevant spatial affordances over time and guide the robot to execute accurate long-horizon navigation in real indoor environments.\n5\nConclusion\nIn this work, we introduce MapDream, a task-driven framework designed to improve VLN by optimizing the generation of map representations. Unlike traditional VLN methods, which either omit maps or rely on hand-crafted designs, MapDream rethinks map construction as an autoregressive generative process. By jointly optimizing both map generation and navigation policies through a two-stage training approach, supervised pre-training followed by reinforcement fine-tuning, MapDream enables agents to learn compact, task-relevant map representations that evolve with the navigation policy and remain effective across different initialization choices and representation capacities. This synergistic framework significantly improves navigation success and efficiency on established VLN benchmarks, such as R2R-CE and RxR-CE, while demonstrating strong generalization to unseen environments.\nOur findings show that task-driven map representations are key to improving the performance and scalability of VLN systems. More broadly, rethinking maps through a task-driven lens may offer a scalable paradigm for future embodied AI.\nLimitations.\nMapDream currently maintains a local egocentric BEV representation that aggregates recent observations into a broader spatial context, which substantially improves over frame-level reasoning but does not explicitly model a persistent global map over arbitrarily long horizons. In addition, temporal consistency of the generated maps across timesteps is not explicitly enforced. Extending the framework to incorporate long-term global spatial memory and temporally coherent map representations is a promising direction for future work.\nReferences\nAn etÂ al. [2022]\nDong An, Yuankai Qi, Yangguang Li, Yan Huang, Liang Wang, Tieniu Tan, and Jing Shao.\nBevbert: Multimodal map pre-training for language-guided navigation.\narXiv preprint arXiv:2212.04385\n, 2022.\nAn etÂ al. [2024]\nDong An, Hanqing Wang, Wenguan Wang, Zun Wang, Yan Huang, Keji He, and Liang Wang.\nEtpnav: Evolving topological planning for vision-language navigation in continuous environments.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n, 2024.\nAnderson etÂ al. [2018]\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko SÃ¼nderhauf, Ian Reid, Stephen Gould, and Anton Van DenÂ Hengel.\nVision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n, pages 3674â€“3683, 2018.\nBai etÂ al. [2025]\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, etÂ al.\nQwen2. 5-vl technical report.\narXiv preprint arXiv:2502.13923\n, 2025.\nBrooks etÂ al. [2023]\nTim Brooks, Aleksander Holynski, and AlexeiÂ A Efros.\nInstructpix2pix: Learning to follow image editing instructions.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pages 18392â€“18402, 2023.\nChen etÂ al. [2023a]\nJingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei.\nTextdiffuser: Diffusion models as text painters.\nAdvances in Neural Information Processing Systems\n, 36:9353â€“9387, 2023a.\nChen etÂ al. [2022]\nPeihao Chen, Dongyu Ji, Kunyang Lin, Runhao Zeng, Thomas Li, Mingkui Tan, and Chuang Gan.\nWeakly-supervised multi-granularity map learning for vision-and-language navigation.\nAdvances in Neural Information Processing Systems\n, 35:38149â€“38161, 2022.\nChen etÂ al. [2023b]\nPeihao Chen, Xinyu Sun, Hongyan Zhi, Runhao Zeng, ThomasÂ H Li, Gaowen Liu, Mingkui Tan, and Chuang Gan.\nA2 nav: Action-aware zero-shot robot navigation by exploiting vision-and-language ability of foundation models.\narXiv preprint arXiv:2308.07997\n, 2023b.\nChen etÂ al. [2025]\nXiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan.\nJanus-pro: Unified multimodal understanding and generation with data and model scaling.\narXiv preprint arXiv:2501.17811\n, 2025.\nCheng etÂ al. [2024]\nAn-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem BÄ±yÄ±k, Hongxu Yin, Sifei Liu, and Xiaolong Wang.\nNavila: Legged robot vision-language-action model for navigation.\narXiv preprint arXiv:2412.04453\n, 2024.\nChiang etÂ al. [2024]\nHao-TienÂ Lewis Chiang, Zhuo Xu, Zipeng Fu, MithunÂ George Jacob, Tingnan Zhang, Tsang-WeiÂ Edward Lee, Wenhao Yu, Connor Schenck, David Rendleman, Dhruv Shah, etÂ al.\nMobility vla: Multimodal instruction navigation with long-context vlms and topological graphs.\narXiv preprint arXiv:2407.07775\n, 2024.\nGao etÂ al. [2023]\nJunyu Gao, Xuan Yao, and Changsheng Xu.\nFast-slow test-time adaptation for online vision-and-language navigation.\narXiv preprint arXiv:2311.13209\n, 2023.\nGeorgakis etÂ al. [2022]\nGeorgios Georgakis, Karl Schmeckpeper, Karan Wanchoo, Soham Dan, Eleni Miltsakaki, Dan Roth, and Kostas Daniilidis.\nCross-modal map learning for vision and language navigation.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pages 15460â€“15470, 2022.\nGu etÂ al. [2022]\nJing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, and XinÂ Eric Wang.\nVision-and-language navigation: A survey of tasks, methods, and future directions.\narXiv preprint arXiv:2203.12667\n, 2022.\nHuang etÂ al. [2022]\nChenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard.\nVisual language maps for robot navigation.\narXiv preprint arXiv:2210.05714\n, 2022.\n[16]\nSungjune Kim, Gyeongrok Oh, Heeju Ko, Daehyun Ji, Dongwook Lee, Byung-Jun Lee, Sujin Jang, and Sangpil Kim.\nTest-time adaptation for online vision-language navigation with feedback-based reinforcement learning.\nIn\nForty-second International Conference on Machine Learning\n.\nKrantz etÂ al. [2020]\nJacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, and Stefan Lee.\nBeyond the nav-graph: Vision-and-language navigation in continuous environments.\nIn\nComputer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part XXVIII 16\n, pages 104â€“120. Springer, 2020.\nKu etÂ al. [2020]\nAlexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge.\nRoom-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding.\narXiv preprint arXiv:2010.07954\n, 2020.\nLi etÂ al. [2025]\nDanyang Li, Zenghui Yang, Guangpeng Qi, Songtao Pang, Guangyong Shang, Qiang Ma, and Zheng Yang.\nOpenmap: Instruction grounding via open-vocabulary visual-language mapping.\nIn\nProceedings of the 33rd ACM International Conference on Multimedia\n, pages 7444â€“7452, 2025.\nLiu etÂ al. [2025a]\nHaoran Liu, Weikang Wan, Xiqian Yu, Minghan Li, Jiazhao Zhang, Bo Zhao, Zhibo Chen, Zhongyuan Wang, Zhizheng Zhang, and He Wang.\nNavid-4d: Unleashing spatial intelligence in egocentric rgb-d videos for vision-and-language navigation.\n2025a.\nLiu etÂ al. [2025b]\nQingxiang Liu, Ting Huang, Zeyu Zhang, and Hao Tang.\nNav-r1: Reasoning and navigation in embodied scenes.\narXiv preprint arXiv:2509.10884\n, 2025b.\nLiu etÂ al. [2023]\nRui Liu, Xiaohan Wang, Wenguan Wang, and Yi Yang.\nBirdâ€™s-eye-view scene graph for vision-language navigation.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, pages 10968â€“10980, 2023.\nLiu etÂ al. [2024]\nZhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, etÂ al.\nNvila: Efficient frontier visual language models.\narXiv preprint arXiv:2412.04468\n, 2024.\n[24]\nKaihang Pan, Yang Wu, Wendong Bu, Kai Shen, Juncheng Li, Yingting Wang, Siliang Tang, Jun Xiao, Fei Wu, Yueting Zhuang, etÂ al.\nJanus-pro-r1: Advancing collaborative visual comprehension and generation via reinforcement learning.\nIn\nThe Thirty-ninth Annual Conference on Neural Information Processing Systems\n.\nQwen [2025]\nQwen.\nQwen3-vl technical report.\narXiv preprint arXiv:2511.21631\n, 2025.\nRombach etÂ al. [2022]\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer.\nHigh-resolution image synthesis with latent diffusion models.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pages 10684â€“10695, 2022.\nShao etÂ al. [2024]\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, etÂ al.\nDeepseekmath: Pushing the limits of mathematical reasoning in open language models.\narXiv preprint arXiv:2402.03300\n, 2024.\nWang etÂ al. [2025a]\nShuo Wang, Yongcai Wang, Wanting Li, Xudong Cai, Yucheng Wang, Maiyue Chen, Kaihui Wang, Zhizhong Su, Deying Li, and Zhaoxin Fan.\nAux-think: Exploring reasoning strategies for data-efficient vision-language navigation.\nAdvances in Neural Information Processing Systems\n, 2025a.\nWang etÂ al. [2025b]\nShuo Wang, Yongcai Wang, Wanting Li, Xudong Cai, Yucheng Wang, Maiyue Chen, Kaihui Wang, Zhizhong Su, Deying Li, and Zhaoxin Fan.\nAux-think: Exploring reasoning strategies for data-efficient vision-language navigation.\narXiv preprint arXiv:2505.11886\n, 2025b.\nWang etÂ al. [2025c]\nShuo Wang, Yongcai Wang, Wanting Li, Yucheng Wang, Maiyue Chen, Kaihui Wang, Zhizhong Su, Xudong Cai, Yeying Jin, Deying Li, etÂ al.\nMonodream: Monocular vision-language navigation with panoramic dreaming.\narXiv preprint arXiv:2508.02549\n, 2025c.\nWang etÂ al. [2025d]\nShuo Wang, Yucheng Wang, Guoxin Lian, Yongcai Wang, Maiyue Chen, Kaihui Wang, Bo Zhang, Zhizhong Su, Yutian Zhou, Wanting Li, etÂ al.\nProgress-think: Semantic progress reasoning for vision-language navigation.\narXiv preprint arXiv:2511.17097\n, 2025d.\nWang etÂ al. [2023]\nZihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, and Shuqiang Jiang.\nGridmm: Grid memory map for vision-and-language navigation.\nIn\nProceedings of the IEEE/CVF International conference on computer vision\n, pages 15625â€“15636, 2023.\nWang etÂ al. [2024]\nZihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, and Shuqiang Jiang.\nSim-to-real transfer via 3d feature fields for vision-and-language navigation.\narXiv preprint arXiv:2406.09798\n, 2024.\nWang etÂ al. [2025e]\nZihan Wang, Seungjun Lee, and GimÂ Hee Lee.\nDynam3d: Dynamic layered 3d tokens empower vlm for vision-and-language navigation.\narXiv preprint arXiv:2505.11383\n, 2025e.\nWei etÂ al. [2025]\nMeng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, Wenzhe Cai, Hanqing Wang, Yilun Chen, etÂ al.\nStreamvln: Streaming vision-and-language navigation via slowfast context modeling.\narXiv preprint arXiv:2507.05240\n, 2025.\nWu etÂ al. [2024]\nWansen Wu, Tao Chang, Xinmeng Li, Quanjun Yin, and Yue Hu.\nVision-language navigation: a survey and taxonomy.\nNeural Computing and Applications\n, 36(7):3291â€“3316, 2024.\nYao etÂ al. [2025]\nXuan Yao, Junyu Gao, and Changsheng Xu.\nNavmorph: A self-evolving world model for vision-and-language navigation in continuous environments.\narXiv preprint arXiv:2506.23468\n, 2025.\nZeng etÂ al. [2025]\nShuang Zeng, Dekang Qi, Xinyuan Chang, Feng Xiong, Shichao Xie, Xiaolong Wu, Shiyi Liang, Mu Xu, and Xing Wei.\nJanusvln: Decoupling semantics and spatiality with dual implicit memory for vision-language navigation.\narXiv preprint arXiv:2509.22548\n, 2025.\nZhang etÂ al. [2025a]\nHaochen Zhang, Nader Zantout, Pujith Kachana, Ji Zhang, and Wenshan Wang.\nIref-vla: A benchmark for interactive referential grounding with imperfect language in 3d scenes.\narXiv preprint arXiv:2503.17406\n, 2025a.\nZhang etÂ al. [2024a]\nJiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang, and He Wang.\nUni-navid: A video-based vision-language-action model for unifying embodied navigation tasks.\narXiv preprint arXiv:2412.06224\n, 2024a.\nZhang etÂ al. [2024b]\nJiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, and He Wang.\nNavid: Video-based vlm plans the next step for vision-and-language navigation.\narXiv preprint arXiv:2402.15852\n, 2024b.\nZhang etÂ al. [2025b]\nL Zhang, X Hao, Q Xu, Q Zhang, X Zhang, P Wang, J Zhang, Z Wang, S Zhang, and RÂ MapNav Xu.\nA novel memory representation via annotated semantic maps for vlm-based vision-and-language navigation.\narXiv preprint arXiv:2502.13451\n, 2025b.",
    "preview_text": "Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.\n\nMapDream: Task-Driven Map Learning for Vision-Language Navigation\nGuoxin Lian\n1,3,âˆ—\n,\nShuo Wang\n1,3,âˆ—\n,\nYucheng Wang\n3,â€ \n,\nYongcai Wang\n1,â€¡\n,\nMaiyue Chen\n3\n,\nKaihui Wang\n3\n,\nBo Zhang\n3\n,\nZhizhong Su\n3\n,\nDeying Li\n1\n,\nZhaoxin Fan\n2,â€¡\n1\nRenmin University of China\n2\nBeijing Advanced Innovation Center for Future Blockchain and Privacy Computing\n3\nHorizon Robotics\nâˆ—\nEqual contribution\nâ€ \nProject leader\nâ€¡\nCorresponding authors\nAbstract\nVision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception.\nHowever, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy.\nWe argue that maps should instead be learned representations shaped directly by navigation objectives rather",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "fine tune",
        "world model"
    ],
    "one_line_summary": "MapDreamæå‡ºäº†ä¸€ç§ä»»åŠ¡é©±åŠ¨çš„åœ°å›¾å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è‡ªå›å½’é¸Ÿç°å›¾åˆæˆå’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œä¼˜åŒ–è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„åœ°å›¾è¡¨ç¤ºä¸åŠ¨ä½œé¢„æµ‹ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T17:33:16Z",
    "created_at": "2026-02-03T15:53:11.049147",
    "updated_at": "2026-02-03T15:53:11.049157"
}