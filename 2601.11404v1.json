{
    "id": "2601.11404v1",
    "title": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models",
    "authors": [
        "Linqing Zhong",
        "Yi Liu",
        "Yifei Wei",
        "Ziyu Xiong",
        "Maoqing Yao",
        "Si Liu",
        "Guanghui Ren"
    ],
    "abstract": "视觉-语言-动作（VLA）模型已成为处理多样化操作任务的关键通用机器人策略，传统上依赖于通过视觉语言模型（VLM）嵌入将多模态输入直接转换为动作。近期研究引入了显式的中间推理，例如子任务预测（语言）或目标图像合成（视觉），以指导动作生成。然而，这些中间推理往往较为间接，且本质上难以传达精确执行动作所需的完整、细粒度信息。相反，我们认为最有效的推理形式应直接在动作空间中进行推演。为此，我们提出了动作思维链（ACoT）范式，将推理过程构建为一系列结构化的粗略动作意图序列，以指导最终策略。本文提出ACoT-VLA这一实现ACoT范式的新型架构，具体引入两个互补组件：显式动作推理器（EAR）与隐式动作推理器（IAR）。前者通过生成粗略参考轨迹作为显式的动作级推理步骤，后者则从多模态输入的内部表征中提取潜在动作先验，共同构成动作思维链，为下游动作头提供条件约束，从而实现具身化的策略学习。在真实世界与仿真环境中的大量实验表明，该方法在LIBERO、LIBERO-Plus和VLABench基准上分别达到98.5%、84.1%和47.4%的性能表现，验证了其优越性。",
    "url": "https://arxiv.org/abs/2601.11404v1",
    "html_url": "https://arxiv.org/html/2601.11404v1",
    "html_content": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models\nLinqing Zhong\n1,2\nYi Liu\n2\nYifei Wei\n1,2\nZiyu Xiong\n2\nMaoqing Yao\n2∗\nSi Liu\n1∗\nGuanghui Ren\n2\n1\nBeihang University\n2\nAgiBot\nCorresponding author\nAbstract\nVision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings.\nRecent advancements have introduced explicit intermediary reasoning—such as sub-task prediction (language) or goal image synthesis (vision)—to guide action generation.\nHowever, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution.\nInstead, we posit that the most effective form of reasoning is one that deliberates directly in the action space.\nWe introduce\nAction Chain-of-Thought (ACoT)\n, a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy.\nIn this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm.\nSpecifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR).\nThe former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning.\nExtensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves\n98.5\n%\n{98.5\\%}\n,\n84.1\n%\n{84.1\\%}\n, and\n47.4\n%\n{47.4\\%}\non LIBERO, LIBERO-Plus and VLABench, respectively.\n1\nIntroduction\nFigure 1\n:\nChain-of-Thought in different space. (a) Language CoT paradigm predicts sub-tasks as intermediate reasoning. (b) Visual CoT paradigm synthesizes a goal image to provide guidance for action policy. (c) Our proposed Action CoT directly operates in action space and provides homogeneous action guidance.\nTo overcome the generalization limits of task-specific robot policies\n[\n11\n,\n61\n,\n48\n]\n, recent work has converged on Vision-Language-Action (VLA) models\n[\n25\n,\n5\n,\n41\n,\n33\n]\n, which always leverage a pre-trained Vision-Language Model (VLM)\n[\n1\n,\n46\n,\n2\n]\nto encode visual and linguistic inputs into a latent representation that conditions an action decoder.\nRecent advancements seek to improve the mapping from the input space to the action space by introducing the intermediate reasoning step by language generation, leading to more generalized and precise action outputs\n[\n22\n,\n53\n]\n, as visualized in Fig.\n1\n(a).\nA parallel thrust leverages world models\n[\n16\n,\n50\n,\n65\n]\nto simulate environmental dynamics, directly enhancing the efficacy and goal-oriented nature of the generated action sequences\n[\n62\n,\n58\n]\n, as shown in Fig.\n1\n(b).\nDespite the promising trajectory set by these paradigms, a critical challenge persists: existing generalist policies think predominantly in the vision-language (input) space, often failing to adequately address the inherent disparity between these rich, semantic representations and the requirements of precise, low-level action execution (output).\nSpecifically, the knowledge encoded within the VLM backbone of VLA models is derived from pre-training on web-scale datasets focused on semantic alignment and question-answering, yielding representations optimized for linguistic understanding rather than physical dynamics.\nSimilarly, while world models forecast future visual states conditioned on inputs, their guidance remains tethered to naturally visual representations.\nCrucially, both semantic and visual forms of reasoning only offer suboptimal, indirect guidance for generating the necessary action sequence.\nConsequently, these prevailing approaches rely on an inherently constrained information conduit, struggling to convey the full, granular knowledge of the action space essential for truly grounded and accurate robotic policy learning.\nThe inherent semantic-kinematic gap in existing policies,\ni.e\n.\n, a fundamental disconnect between high-level, abstract inputs and low-level, executable motor commands, necessitates a paradigm shift in how guidance is provided.\nWe contend that to bridge this chasm, policies require guidance that is kinematically coherent, rather than purely semantic or visual.\nThis core principle underpins our novel framework:\nAction Chain-of-Thought (ACoT)\n(Fig.\n1\n(c)).\nWe redefine the “thought” process not as a sequence of linguistic tokens, but as a structured chain of explicit, kinematically-grounded action intents.\nThis approach furnishes the policy with direct motion cues, supplanting indirect representations.\nIn a manner analogous to learning from physical demonstration, this direct conditioning on action-space information enables a substantially more efficient and veridically grounded policy learning process.\nThis foundational shift, however, introduces a critical and distinct research challenge:\n“How can we robustly and efficiently synthesize the complex, high-dimensional motion cues required for ACoT reasoning from the raw, heterogeneous multimodal inputs?”\nAction-related information manifests in two complementary forms,\ni.e\n.\n, explicit or implicit.\nThe explicit form corresponds to observable motion trajectories, such as those in human demonstrations, which directly encode executable patterns of behavior.\nIn contrast, the implicit form resides in latent cues,\ne.g\n.\n, linguistic expressions like “reach out” or “grasp”, as well as interaction intents embedded in visual contexts.\nAlthough these cues are not presented as explicit robotic trajectories, they implicitly define distributions over feasible actions within the action space.\nBuilding upon this insight, we introduce two synergistic mechanisms to generate both explicit and implicit guidance in the action space.\nWe first propose the Explicit Action Reasoner (EAR), which is realized as a light-weight transformer. Particularly, EAR synthesizes coarse-grained motion trajectories conditioned on multimodal observations, offering direct and executable guidance within the action space.\nSecondly, we devise the Implicit Action Reasoner (IAR), which infers latent action priors through applying cross-attention modeling between downsampled multimodal representations and learnable queries, thereby providing implicit behavioral priors.\nNote that these two mechanisms are inherently complementary to each other.\nSubsequently, through jointly leveraging both EAR and IAR, we develop ACoT-VLA, an integrated Action Chain-of-Thought framework that enables grounded generalist robot policy learning.\nExtensive experiments across both real-world settings and three simulation benchmarks consistently demonstrate the effectiveness and versatility of our ACoT-VLA.\nTo summarize, our main contributions are as follows:\n•\nConceptually, we introduce Action Chain of Thought (ACoT), a new paradigm for generalist robot policies. To the best of our knowledge, this is the first work to formulate the deliberative process as a structured chain of explicit action-space intents, rather than abstract linguistic or visual sub-goals.\n•\nWe delve into essential action space guidance and propose the Explicit and Implicit Action Reasoners, which provide both explicit trajectory guidance and implicit behavioral inspiration for action prediction.\n•\nBuilding upon these two modules, we further propose ACoT-VLA, a unified framework for grounded generalist robot policy learning.\n•\nEmpirically, we validate our approach through extensive simulation and real-world experiments, achieving state-of-the-art performance on multiple benchmarks,\ni.e\n.\n, 98.5%, 84.1% and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.\n2\nRelated Works\nFigure 2\n:\nArchitectural Overview of ACoT-VLA.\nThe framework consists of three main components operating on features from a shared VLM backbone.\n(a) The Explicit Action Reasoner (EAR) is a Transformer-based module that synthesizes a coarse reference trajectory, providing explicit action-space guidance.\n(b) The Implicit Action Reasoner (IAR) employs a cross-attention mechanism with learnable queries to extract latent action priors from the VLM’s internal representations.\n(c) The Action-Guided Prediction (AGP) head synergistically integrates both explicit and implicit guidances via cross-attention to condition the final denoising process, producing the executable action sequence.\nVision-Language-Action Models.\nVLA models\n[\n14\n,\n18\n,\n20\n,\n27\n]\nincorporate pre-trained VLM models to predict language-driven robotic action sequences. Early works\n[\n67\n,\n25\n]\nformulate robot control as an autoregressive sequence generation problem, discretizing continuous actions into bins. Inspired by generative modeling\n[\n38\n,\n66\n,\n31\n]\n, increasing works\n[\n5\n,\n33\n,\n19\n]\nadopt diffusion-based action policies to synthesize smooth and high-quality action trajectories. Given that robotic manipulation inherently occurs in three-dimensional space, a line of studies\n[\n52\n,\n30\n,\n59\n]\nhave sought to enhance the spatial reasoning capability of VLA models by integrating 3D priors. For instance, SpatialVLA\n[\n41\n]\nintegrates spatial embeddings to endow model with 3D awareness, while 4D-VLA\n[\n55\n]\nincorporates both spatial and temporal information to enrich representations.\nBesides, due to the scarcity of large-scale real-world robot demonstrations, a series of efforts\n[\n37\n,\n57\n,\n12\n,\n6\n,\n23\n,\n9\n]\nfocus on data-centric solutions, constructing large-scale robotic datasets through simulation or real-world collection to scale up policy learning.\nMoreover, recent large-scale co-training approaches, such as\nπ\n0.5\n\\pi_{0.5}\n[\n22\n]\n, GenieReasoner\n[\n34\n]\nand Gemini Robotics\n[\n47\n]\n, demonstrate the potential of unifying web-scale language understanding with action learning, enhancing the policy’s generalization ability while retaining the reasoning capability of pre-trained foundation models.\nWorld-Model-based Policies.\nAdvances in world models have demonstrated remarkable capability in synthesizing high-fidelity images and temporally coherent videos. Building upon such progress, emerging researches\n[\n26\n,\n36\n,\n56\n,\n29\n]\nexploit their predictive dynamics to implicitly guide action generation.\nSpecifically, CoT-VLA\n[\n60\n]\nintroduces visual chain-of-thought reasoning by forecasting sub-goal images, explicitly integrating visual reasoning into action prediction. WorldVLA\n[\n8\n]\nemploys an autoregressive architecture that unifies perception and action generation within a single framework. DreamVLA\n[\n58\n]\nextends beyond visual prediction and enriches world modeling with dynamic, depth, and semantic cues, improving the model’s physical consistency.\nCollectively, existing world-model-based approaches adopt a knowledge-forecasting perspective, incorporating primarily visual guidance into action trajectories generation.\nIn contrast to previous works focusing on visual or linguistic intermediaries for robotic policy learning, our key insight lies in investigating guidance directly within the action space, which intrinsically mitigates the heterogeneity between perception and action, enabling the model to effectively learn action-relevant priors.\n3\nMethodology\nIn this section, we present a detailed investigation into how to generate effective action space guidance and integrate it into robotic policy learning.\nWe first define the robotic manipulation problem and formulate our proposed approach in Sec.\n3.1\n.\nThe core of our method lies in two distinct action reasoners introduced in Sec.\n3.2\nand Sec.\n3.3\n, which provide explicit and implicit guidance within the action space.\nWe conclude by illustrating the policy prediction strategy that effectively integrates this action guidance during policy learning (Sec.\n3.4\n).\n3.1\nProblem Formulation\nGiven a natural language instruction\nl\nl\nand current visual observation\no\nt\no_{t}\n, the generalist robot policy\nπ\nθ\n\\pi_{\\theta}\naims to predict action sequences\na\nt\n:\nt\n+\nH\n−\n1\na_{t:t+H-1}\nthat accomplishes the specified task. The process can be formally expressed as:\na\nt\n:\nt\n+\nH\n−\n1\n=\nπ\nθ\n​\n(\no\nt\n,\nl\n)\n,\na_{t:t+H-1}=\\pi_{\\theta}(o_{t},l),\n(1)\nwhere\nH\nH\nrepresents the action horizon. Numerous works introduce additional guidance signals\ng\ng\n, which encapsulates various forms of auxiliary information to enhance policy’s prediction ability.\nSpecifically, these guidance signals can be broadly categorized into two types: language-level guidance\ng\nlang\ng_{\\text{lang}}\nand vision-level guidance\ng\nvis\ng_{\\text{vis}}\n. The former is primarily adopted by VLM-based methods,\ne.g\n.\n, leveraging LLMs’ reasoning capabilities to predict sub-tasks, while the latter is always employed by world-model-based approaches, such as simulating future observations.\nSuch relationship can be formulated as:\nπ\nθ\n​\n(\na\nt\n:\nt\n+\nH\n−\n1\n,\ng\n∣\no\nt\n,\nl\n)\n=\nπ\nθ\n​\n(\na\nt\n:\nt\n+\nH\n−\n1\n∣\no\nt\n,\nl\n,\ng\n)\n​\nπ\nθ\n​\n(\ng\n∣\no\nt\n,\nl\n)\n,\n\\pi_{\\theta}(a_{t:t+H-1},g\\mid o_{t},l)=\\pi_{\\theta}(a_{t:t+H-1}\\mid o_{t},l,g)\\pi_{\\theta}(g\\mid o_{t},l),\n(2)\nwhere\ng\n∈\n{\ng\nlang\n,\ng\nvis\n}\ng\\in\\{g_{\\text{lang}},\\,g_{\\text{vis}}\\}\n. Conversely, we shift the focus toward the action domain itself and investigate cues operating directly in the action space, symbolized as\ng\naction\ng_{\\text{action}}\n. The above guidances are extended as\ng\n∈\n{\ng\nlang\n,\ng\nvis\n,\ng\naction\n}\ng\\in\\{g_{\\text{lang}},\\,g_{\\text{vis}},\\,g_{\\text{action}}\\}\n.\nSuch action guidance can intuitively be disentangled into explicit and implicit forms. The explicit guidance\ng\naction\nex\ng_{\\text{action}}^{\\text{ex}}\nprovides direct priors in the form of reference action sequences, whereas the implicit guidance\ng\naction\nim\ng_{\\text{action}}^{\\text{im}}\narises from contextual signals,\ne.g\n.\n, action distribution inherently implied in linguistics.\n3.2\nExplicit Action Reasoner\nTo incorporate explicit action trajectories into the thinking process of\nπ\nθ\n\\pi_{\\theta}\nto generate high-quality action predictions, we propose the Explicit Action Reasoner (EAR).\nWe design a mechanism that enables the model to autonomously synthesize reference action sequences as internal guidance for policy learning.\nAnalogously, this formulation can be viewed as an action-space transfer of self-conditioning in generative models\n[\n10\n,\n35\n]\n, where incorporating prior estimates into the generation process has been shown to markedly improve sample quality.\nBuilding upon this principle, we instantiate EAR as a light-weight transformer, as shown in Fig.\n2\n(a), generating kinematically plausible action reference as explicit action-space guidance\ng\naction\nex\ng_{\\text{action}}^{\\text{ex}}\nfor downstream action policy.\nFormally, given visual observation\no\nt\no_{t}\nand language instruction\nl\nl\n, a pre-trained VLM encodes them into a contextual key-value cache:\n(\nK\n1\n:\nN\nVLM\n,\nV\n1\n:\nN\nVLM\n)\n=\nVLM\n​\n(\no\nt\n,\nl\n)\n,\n(K^{\\text{VLM}}_{1:N},V^{\\text{VLM}}_{1:N})=\\text{VLM}(o_{t},l),\n(3)\nwhere\nN\nN\nrepresents the number of layers of VLM.\nSubsequently, the EAR, denoted as\nπ\nθ\nref\n\\pi_{\\theta}^{\\text{ref}}\n, takes a noisy action sequence\na\n~\nt\n:\nt\n+\nH\nr\n​\ne\n​\nf\n−\n1\n\\tilde{a}_{t:t+H^{ref}-1}\nas input, where\nH\nr\n​\ne\n​\nf\nH^{ref}\nindicates the horizon of reference actions. The sequence is first embedded into an initial hidden representation\nh\n0\nref\nh_{0}^{\\text{ref}}\n, which serves as the input to EAR’s transformer layers. At each transformer layer\ni\ni\n, we adopt self-attention, along with cross-attention with the contextual key-value cache from the corresponding VLM layer:\nh\n~\ni\nref\n=\nSelf-Attn\n​\n(\nh\ni\n−\n1\nref\n)\n+\nCrossAttn\n​\n(\nh\ni\n−\n1\nref\n,\nK\ni\nVLM\n,\nV\ni\nVLM\n)\n,\n\\tilde{h}_{i}^{\\text{ref}}=\\text{Self-Attn}(h_{i-1}^{\\text{ref}})+\\text{CrossAttn}(h_{i-1}^{\\text{ref}},K^{\\text{VLM}}_{i},V^{\\text{VLM}}_{i}),\n(4)\nwhere self-attention module captures temporal dependencies within the action sequence and cross-attention mechanism injects multimodal contextual priors from the VLM.\nThen, the intermediate representation\nh\n~\ni\nref\n\\tilde{h}_{i}^{\\text{ref}}\nis processed by a feed-forward network (FFN) in a residual-parallel manner, updating the\ni\ni\n-th EAR representation\nh\ni\nref\nh_{i}^{\\text{ref}}\n:\nh\ni\nref\n=\nh\ni\n−\n1\nref\n+\nFFN\n​\n(\nh\n~\ni\nref\n)\n.\nh_{i}^{\\text{ref}}=h_{i-1}^{\\text{ref}}+\\text{FFN}(\\tilde{h}_{i}^{\\text{ref}}).\n(5)\nThrough training via flow matching,\nπ\nθ\nref\n\\pi_{\\theta}^{\\text{ref}}\nlearns a distribution over action trajectories, producing a denoised action sequence:\na\nt\n:\nt\n+\nH\nr\n​\ne\n​\nf\n−\n1\nr\n​\ne\n​\nf\n=\nπ\nθ\nref\n​\n(\na\n~\nt\n:\nt\n+\nH\nr\n​\ne\n​\nf\n−\n1\n,\nK\n1\n:\nN\nVLM\n,\nV\n1\n:\nN\nVLM\n)\n.\na^{ref}_{t:t+H^{ref}-1}=\\pi^{\\text{ref}}_{\\theta}(\\tilde{a}_{t:t+H^{ref}-1},K^{\\text{VLM}}_{1:N},V^{\\text{VLM}}_{1:N}).\n(6)\nThe generated sequence is then encoded via a MLP projector to obtain action embedding\nZ\nex\nZ^{\\text{ex}}\n, which serves as explicit action-space guidance\ng\naction\nex\ng_{\\text{action}}^{\\text{ex}}\nfor action policy learning.\n3.3\nImplicit Action Reasoner\nBeyond the explicit action trajectories, the multimodal latent space of VLM also encodes implicit motion cues\n[\n40\n,\n13\n]\n,\ne.g\n.\n, visual affordances and action-related semantics. Effectively extracting these action-relevant representations potentially offers complementary guidance. To this end, we introduce an Implicit Action Reasoner (IAR), which directly operates on the VLM’s key–value cache.\nConcretely, as presented in Fig.\n2\n(b), for each VLM layer\ni\n∈\n[\n1\n,\nN\n]\ni\\in[1,N]\n, we initialize a learnable matrix\nQ\ni\n∈\nℝ\nM\n×\nd\nQ_{i}\\in\\mathbb{R}^{M\\times d}\n, where\nM\nM\nis a hyperparameter and\nd\nd\nrepresents VLM’s hidden dimension.\nConsidering the information redundancy within VLM’s key–value cache and computational efficiency, we first downsample the corresponding key–value pairs into a lower-dimensional space, which is formulated as:\nQ\ni\n′\n=\nQ\ni\n​\nW\nQ\n(\ni\n)\n,\nK\ni\n′\n=\nK\ni\nVLM\n​\nW\nK\n(\ni\n)\n,\nV\ni\n′\n=\nV\ni\nVLM\n​\nW\nV\n(\ni\n)\n,\nQ_{i}^{\\prime}=Q_{i}W_{Q}^{(i)},\\quad K_{i}^{\\prime}=K_{i}^{\\text{VLM}}W_{K}^{(i)},\\quad V_{i}^{\\prime}=V_{i}^{\\text{VLM}}W_{V}^{(i)},\n(7)\nwhere\nW\nQ\n(\ni\n)\n,\nW\nK\n(\ni\n)\n,\nW\nV\n(\ni\n)\n∈\nℝ\nd\n×\nd\n′\nW_{Q}^{(i)},W_{K}^{(i)},W_{V}^{(i)}\\in\\mathbb{R}^{d\\times d^{\\prime}}\nare learnable linear projectors and\nd\n′\n≪\nd\nd^{\\prime}\\ll d\n.\nLater, cross-attention is applied to extract action-relevant information from each\nK\ni\n′\nK_{i}^{\\prime}\nand\nV\ni\n′\nV_{i}^{\\prime}\n.\nThe resulting features are subsequently integrated via average pooling and transformed through a MLP projector, as visualized in Fig.\n2\n(b), producing compact representations that capture the implicit action semantics\nz\ni\nim\nz^{\\text{im}}_{i}\nembedded in VLM’s\ni\ni\n-th layer:\nz\ni\nim\n=\nMLP\n​\n(\nPool\n​\n(\nCrossAttn\n​\n(\nQ\ni\n′\n,\nK\ni\n′\n,\nV\ni\n′\n)\n)\n)\n.\nz^{\\text{im}}_{i}=\\text{MLP}(\\text{Pool}(\\text{CrossAttn}(Q_{i}^{\\prime},K_{i}^{\\prime},V_{i}^{\\prime}))).\n(8)\nThen, through aggregating these representations across layers, we obtain implicit action-related feature\nZ\nim\nZ^{\\text{im}}\n, which serves as implicit action-space guidance\ng\naction\nim\ng_{\\text{action}}^{\\text{im}}\n, complementing the explicit motion priors.\nMethods\nGuidance\nSpatial\nObject\nGoal\nLong\nAvg.\nSR\n↑\n\\uparrow\nRank\n↓\n\\downarrow\nSR\n↑\n\\uparrow\nRank\n↓\n\\downarrow\nSR\n↑\n\\uparrow\nRank\n↓\n\\downarrow\nSR\n↑\n\\uparrow\nRank\n↓\n\\downarrow\nSR\n↑\n\\uparrow\nRank\n↓\n\\downarrow\nDiffusion Policy\n[\n11\n]\n–\n78.3\n23\n92.5\n15\n68.3\n24\n50.5\n24\n72.4\n24\nOcto\n[\n48\n]\n–\n78.9\n22\n85.7\n23\n84.6\n17\n51.1\n23\n75.1\n22\nCoT-VLA\n[\n60\n]\nVisual\n87.5\n17\n91.6\n17\n87.6\n14\n69.0\n16\n81.1\n17\nWorldVLA\n[\n8\n]\n(256*256)\nVisual\n85.6\n19\n89.0\n20\n82.6\n19\n59.0\n19\n79.1\n18\nWorldVLA\n[\n8\n]\n(512*512)\nVisual\n87.6\n16\n96.2\n12\n83.4\n18\n60.0\n18\n81.8\n16\nDreamVLA\n[\n58\n]\nVisual\n97.5\n7\n94.0\n13\n89.5\n12\n89.5\n10\n92.6\n11\nUniVLA\n[\n49\n]\nVisual\n95.4\n11\n98.8\n2\n93.6\n9\n94.0\n4\n95.5\n8\nF1\n[\n36\n]\nVisual\n98.2\n4\n97.8\n8\n95.4\n8\n91.3\n8\n95.7\n7\nGE-Act\n[\n29\n]\nVisual\n98.2\n4\n97.6\n9\n95.8\n6\n94.4\n3\n96.5\n5\nTraceVLA\n[\n64\n]\nLinguistics\n84.6\n21\n85.2\n24\n75.1\n23\n54.1\n21\n74.8\n23\nOpenVLA\n[\n25\n]\nLinguistics\n84.7\n20\n88.4\n21\n79.2\n20\n53.7\n22\n76.5\n21\nUniAct\n[\n63\n]\nLinguistics\n77.0\n24\n87.0\n22\n77.0\n22\n70.0\n15\n77.8\n20\nSpatialVLA\n[\n41\n]\nLinguistics\n88.2\n15\n89.9\n19\n78.6\n21\n55.5\n20\n78.1\n19\nThinkAct\n[\n17\n]\nLinguistics\n88.3\n14\n91.4\n18\n87.1\n15\n70.9\n14\n84.4\n15\nπ\n0\n\\pi_{0}\n-FAST\n[\n39\n]\nLinguistics\n96.4\n10\n96.8\n11\n88.6\n13\n60.2\n17\n85.5\n14\nFPC-VLA\n[\n51\n]\nLinguistics\n87.0\n18\n92.0\n16\n86.2\n16\n82.2\n12\n86.9\n13\nSmolVLA\n[\n43\n]\nLinguistics\n93.0\n13\n94.0\n13\n91.0\n11\n77.0\n13\n88.8\n12\nGR00T-N1\n[\n4\n]\nLinguistics\n94.4\n12\n97.6\n9\n93.0\n10\n90.6\n9\n93.9\n10\nπ\n0\n\\pi_{0}\n[\n5\n]\nLinguistics\n96.8\n9\n98.8\n2\n95.8\n6\n85.2\n11\n94.1\n9\nDD-VLA\n[\n28\n]\nLinguistics\n97.2\n8\n98.6\n4\n97.4\n4\n92.0\n7\n96.3\n6\nMemoryVLA\n[\n42\n]\nLinguistics\n98.4\n3\n98.4\n5\n96.4\n5\n93.4\n5\n96.7\n4\nπ\n0.5\n\\pi_{0.5}\n[\n22\n]\nLinguistics\n98.8\n2\n98.2\n7\n98.0\n2\n92.4\n6\n96.9\n3\nOpenVLA-OFT\n[\n24\n]\nLinguistics\n97.6\n6\n98.4\n5\n97.9\n3\n94.5\n2\n97.1\n2\nOurs\nAction\n99.4\n1\n99.6\n1\n98.8\n1\n96.0\n1\n98.5\n1\nTable 1\n:\nComparison on the LIBERO benchmark. The best results are highlighted in\nbold\n. All metrics are average success rates (%).\n3.4\nAction-Guided Prediction\nBuilding upon the explicit action embedding\nZ\nex\nZ^{\\text{ex}}\nproduced by EAR and implicit action-related feature\nZ\nim\nZ^{\\text{im}}\nobtained in IAR, in this section, we introduce the Action-Guided Prediction (AGP) strategy to incorporate both action guidances into policy learning.\nAs illustrated in Fig.\n2\n(c), given a noisy action segment\na\n~\nt\n:\nt\n+\nH\n−\n1\n\\tilde{a}_{t:t+H-1}\n, we first encode it into noisy action embedding via a MLP projector. Particularly, unlike previous approaches that directly feed this embedding into action head\nπ\nθ\nhead\n\\pi_{\\theta}^{\\text{head}}\n, we treat it as action query, denoted as\nQ\na\n​\nc\n​\nt\n​\ni\n​\no\n​\nn\nQ_{action}\n, which interacts with both\nZ\nex\nZ^{\\text{ex}}\nand\nZ\nim\nZ^{\\text{im}}\nto retrieve complementary priors for conditional prediction.\nSpecifically, we perform dual cross-attention operations:\nS\nex\n=\nCrossAttn\n​\n(\nQ\na\n​\nc\n​\nt\n​\ni\n​\no\n​\nn\n,\nZ\nex\n,\nZ\nex\n)\n,\nS^{\\text{ex}}=\\text{CrossAttn}(Q_{action},Z^{\\text{ex}},Z^{\\text{ex}}),\n(9)\nS\nim\n=\nCrossAttn\n​\n(\nQ\na\n​\nc\n​\nt\n​\ni\n​\no\n​\nn\n,\nZ\nim\n,\nZ\nim\n)\n,\nS^{\\text{im}}=\\text{CrossAttn}(Q_{action},Z^{\\text{im}},Z^{\\text{im}}),\n(10)\nwhere\nS\nex\nS^{\\text{ex}}\nand\nS\nim\nS^{\\text{im}}\ndenote the attended representations guided by explicit and implicit priors, respectively.\nNote that although both encode action-relevant information, they may highlight different facets of the underlying motion. For instance, explicit priors provide kinematic cues, whereas implicit priors capture latent action tendencies.\nHence, to effectively combine these complementary guidance, we concatenate the two attended features and process them through self-attention fusion block, which integrates the priors into a unified representation\nh\n¯\n\\bar{h}\n:\nh\n¯\n=\nSelf-Attn\n​\n(\n[\nS\nex\n;\nS\nim\n]\n)\n.\n\\bar{h}=\\text{Self-Attn}([S^{\\text{ex}};\\,S^{\\text{im}}]).\n(11)\nEventually, the aggregated representation\nh\n¯\n\\bar{h}\nis fed into\nπ\nθ\nhead\n\\pi_{\\theta}^{\\text{head}}\n, which predicts the denoised action sequence\na\nt\n:\nt\n+\nH\n−\n1\na_{t:t+H-1}\n.\nTraining Objectives.\nThe entire framework is optimized under a standard flow-matching mean-squared error (MSE) objective. The training losses consist of two parts,\ni.e\n.\n, flow-matching MSE for both Explicit Action Reasoner\nπ\nθ\nref\n\\pi_{\\theta}^{\\text{ref}}\nand action head\nπ\nθ\nhead\n\\pi_{\\theta}^{\\text{head}}\n, denoted as\nℒ\nπ\nθ\nref\n\\mathcal{L}_{\\pi_{\\theta}^{\\text{ref}}}\nand\nℒ\nπ\nθ\nhead\n\\mathcal{L}_{\\pi_{\\theta}^{\\text{head}}}\n, respectively. Hence, the overall objective is:\nℒ\ntotal\n=\nλ\n1\n​\nℒ\nπ\nθ\nref\n+\nλ\n2\n​\nℒ\nπ\nθ\nhead\n,\n\\mathcal{L}_{\\text{total}}=\\lambda_{1}\\mathcal{L}_{\\pi_{\\theta}^{\\text{ref}}}+\\lambda_{2}\\mathcal{L}_{\\pi_{\\theta}^{\\text{head}}},\n(12)\nwhere\nλ\n1\n\\lambda_{1}\nand\nλ\n2\n\\lambda_{2}\nare balance factors.\nTeacher Forcing Stabilization.\nDuring training, the outputs of\nπ\nθ\nref\n\\pi_{\\theta}^{\\text{ref}}\ncan be unstable.\nTo stabilize optimization, we compute\nZ\nex\nZ^{\\text{ex}}\ndirectly from ground-truth reference trajectories instead of from\nπ\nθ\nref\n\\pi_{\\theta}^{\\text{ref}}\npredictions, preventing optimization interference to\nπ\nθ\nhead\n\\pi_{\\theta}^{\\text{head}}\n.\nDuring inference, the model switches to a fully self-conditioned mode,\nwhere\nπ\nθ\nref\n\\pi_{\\theta}^{\\text{ref}}\nautonomously generates the reference actions to guide\nπ\nθ\nhead\n\\pi_{\\theta}^{\\text{head}}\nin action prediction.\nMethods\nGuidance\nCamera\nRobot\nLanguage\nLight\nBackground\nNoise\nLayout\nAvg.\nWorldVLA\n[\n8\n]\nVisual\n0.1\n27.9\n41.6\n43.7\n17.1\n10.9\n38.0\n25.0\nOpenVLA\n[\n25\n]\nLinguistics\n0.8\n3.5\n23.0\n8.1\n34.8\n15.2\n28.5\n15.6\nNORA\n[\n21\n]\nLinguistics\n2.2\n37.0\n65.1\n45.7\n58.6\n12.8\n62.1\n39.0\nUniVLA\n[\n7\n]\nLinguistics\n1.8\n46.2\n69.6\n69.0\n81.0\n21.2\n31.9\n42.9\nπ\n0\n\\pi_{0}\n-Fast\n[\n39\n]\nLinguistics\n65.1\n21.6\n61.0\n73.2\n73.2\n74.4\n68.8\n61.6\nRIPT-VLA\n[\n44\n]\nLinguistics\n55.2\n31.2\n77.6\n88.4\n91.6\n73.5\n74.2\n68.4\nOpenVLA-OFT\n[\n24\n]\nLinguistics\n56.4\n31.9\n79.5\n88.7\n93.3\n75.8\n74.2\n69.6\nOpenVLA-OFT+\n[\n15\n]\nLinguistics\n92.8\n30.3\n85.8\n94.9\n93.9\n89.3\n77.6\n79.6\nπ\n0\n∗\n\\pi_{0}^{*}\n[\n5\n]\nLinguistics\n79.6\n21.1\n72.5\n84.7\n86.2\n68.3\n69.4\n67.4\nπ\n0.5\n∗\n\\pi_{0.5}^{*}\n[\n22\n]\nLinguistics\n70.3\n41.7\n81.1\n97.3\n94.6\n71.8\n84.9\n75.7\nOurs\nAction\n91.2\n62.5\n80.3\n95.1\n91.5\n88.3\n84.9\n84.1\nTable 2\n:\nPerformance comparison on the LIBERO-Plus benchmark. Best results are highlighted in bold. An asterisk (*) denotes results reproduced by us for fair comparison.\n4\nExperiments\nIn this section, we first outline the experimental setup in Sec.\n4.1\n. Then, in Sec.\n4.2\n, we evaluate our approach on three simulation benchmarks, followed by comprehensive ablation studies in Sec.\n4.3\n. Moreover, we present real-world deployment results in Sec.\n4.4\nto evaluate real-world applicability.\n4.1\nExperimental Setup\nData Sources.\nFor simulation experiments, we strictly follow the official training splits provided by the corresponding benchmark (LIBERO\n[\n32\n]\n, LIBERO-Plus\n[\n15\n]\n, and VLABench\n[\n57\n]\n), and train our models exclusively on their standard demonstration datasets without introducing any additional data.\nFor the real-world setting, all demonstrations used for model training are collected on our own robotic platform. More details about data sources are introduced in Appendix\nA\n.\nImplementation Details.\nWe implement our approach upon\nπ\n0.5\n\\pi_{0.5}\n[\n22\n]\n. Specifically, we adopt SigLIP\n[\n54\n]\nas the visual encoder, while the LLM backbone is instantiated as Gemma 2B architecture\n[\n3\n]\nwith\nN\n=\n18\nN=18\nlayers and hidden size\nd\n=\n2048\nd=2048\n. For frame processing, each input frame is resized to\n224\n×\n224\n224\\times 224\nprior to the visual encoder. Regarding the EAR, we employ a compact Transformer-based design composed of\nN\n=\n18\nN=18\nlayers. Concerning the IAR, each learnable query matrix\nQ\ni\nQ_{i}\nis configured with a row dimension of\nM\n=\n1\nM=1\n. The reduced dimension in the downsampling strategy is set to\nd\n′\n=\n128\nd^{\\prime}=128\n.\nIn terms of model training, unless explicitly specified, the horizon of predicted reference actions\nH\nr\n​\ne\n​\nf\nH^{ref}\nand action policy output\nH\nH\nare fixed to\n15\n15\nand\n10\n10\n, with action shift set to\n2\n2\nand\n1\n1\n, respectively. To clarify, the action shift specifies the temporal interval relative to the expert demonstration. For instance, a shift of\n1\n1\nyields frame-aligned predictions, whereas a shift of\n2\n2\nskips one intermediate frame. We set the balance factors in training losses as\nλ\n1\n=\nλ\n2\n=\n0.5\n\\lambda_{1}=\\lambda_{2}=0.5\n.\nTraining Configuration.\nWe adopt a unified set of training hyperparameters across all experiments unless explicitly specified. Concretely, the learning rate follows a cosine-decay schedule with a warm-up phase of\n10\n​\nK\n10\\text{K}\nsteps, a peak learning rate of\n5\n​\ne\n−\n5\n5\\mathrm{e}{-5}\n, and a decay toward\n5\n​\ne\n−\n5\n5\\mathrm{e}{-5}\nover\n10\n​\nK\n10\\text{K}\nsteps. Optimization is performed with AdamW with gradient-norm clipping set to\n1.0\n1.0\n. An exponential moving average (EMA) of model parameters is maintained with a decay rate of\n0.999\n0.999\n.\nRegarding hardware settings, model training is performed on a single node equipped with\n8\n8\nNVIDIA H100 GPUs using bfloat16 precision. And the inference is conducted on a single NVIDIA RTX 4090.\n4.2\nSimulation Experiments\nIn this section, we conduct the simulation evaluations across three benchmarks,\ni.e\n.\n, LIBERO\n[\n32\n]\n, LIBERO-Plus\n[\n15\n]\n, and VLABench\n[\n57\n]\n, to comprehensively evaluate our approach’s performance and generalization capabilities under diverse task structures.\nLIBERO Benchmark.\nLIBERO is a widely adopted simulation benchmark for evaluating generalist robotic policies. It consists of four task suites,\ni.e\n.\n, LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long. It is designed to probe a different aspect of policy capability, including spatial awareness, object-centric manipulation, goal completion, and long-horizon reasoning. Each suite consists of\n10\n10\ntasks and provides\n50\n50\nhuman-teleoperated demonstrations per task for policy training.\nFollowing the official evaluation protocol, we evaluate our policy on all tasks within the benchmark. For each task, the policy is evaluated over\n50\n50\nindependent trials, resulting in\n500\n500\nrollouts in total.\nAs reported in Table\n1\n, the quantitative evaluation results demonstrate that our proposed approach outperforms existing methods across all tracks. Compared to the previous state-of-the-art method\nπ\n0.5\n\\pi_{0.5}\n, our approach achieves a\n1.6\n%\n1.6\\%\nabsolute improvement in average success rate, highlighting the clear advantages of incorporating action-space guidance.\nNotably, we observe a pronounced improvement on the LIBERO-Long suite, where tasks require long-horizon manipulation with strict error control. We attribute this advantage to the nature of our proposed ACoT. Particularly, unlike language- or vision-CoT, whose intermediate reasoning remains abstract or indirect with respect to action execution, our proposed ACoT naturally operates in precise representation. Through leveraging actions as intermediate reasoning tokens, the model feeds the following action head with structured and fine-grained guidance, which significantly enhances the robustness to error accumulation in long-horizon manipulation tasks.\nMethods\nGuidance\nIn-dist.\nCategory\nCommonsense\nInstruction\nTexture\nAvg.\nIS\n↑\n\\uparrow\nPS\n↑\n\\uparrow\nIS\n↑\n\\uparrow\nPS\n↑\n\\uparrow\nIS\n↑\n\\uparrow\nPS\n↑\n\\uparrow\nIS\n↑\n\\uparrow\nPS\n↑\n\\uparrow\nIS\n↑\n\\uparrow\nPS\n↑\n\\uparrow\nIS\n↑\n\\uparrow\nPS\n↑\n\\uparrow\nπ\n0\n\\pi_{0}\n[\n5\n]\nLinguistics\n67.8\n62.7\n44.0\n33.6\n54.9\n43.0\n58.0\n38.7\n50.6\n42.5\n55.0\n44.1\nπ\n0.5\n\\pi_{0.5}\n[\n22\n]\nLinguistics\n75.0\n60.8\n49.6\n35.3\n57.5\n41.6\n57.1\n30.3\n62.0\n47.4\n60.2\n43.1\nOurs\nAction\n79.8\n66.1\n54.1\n38.9\n52.3\n37.8\n56.8\n39.6\n74.6\n54.6\n63.5\n47.4\nTable 3:\nComparison on the VLABench benchmark. IS and PS represent Intention score and Progress score, respectively.\nName\nEAR\nIAR\nSpatial\nObject\nGoal\nLong\nAvg.\nBaseline\n98.8\n98.2\n98.0\n92.4\n96.9\n#1\n✓\n99.0\n99.4\n98.0\n96.6\n98.3\n#2\n✓\n99.2\n99.2\n98.2\n95.6\n98.1\n#3\n✓\n✓\n99.4\n99.6\n98.8\n96.0\n98.5\nTable 4\n:\nModule ablations. The performance is gradually improved with the continuous addition of proposed methods.\nLIBERO-Plus Benchmark.\nBuilt upon LIBERO, LIBERO-Plus is an extended robustness-oriented benchmark, designed to systematically evaluate generalist robotic policies under controlled distribution shifts. Concretely, LIBERO-Plus introduces\n7\n7\nperturbation dimensions,\ni.e\n.\n, camera-viewpoints, robot-initial-states, language-variations, lighting-conditions, background-textures, sensor-noise and object-layout, which aim to expose hidden failure modes under standard evaluations. Notably, LIBERO-Plus consists of\n10\n,\n030\n10,030\nevaluation episodes, providing statistically reliable evaluation.\nFollowing standard training configuration in LIBERO-Plus\n[\n15\n]\n, we train our policy on the provided training set for a total of\n100\n​\nK\n100\\text{K}\noptimization steps. In terms of evaluation, we adhere to the protocol established in the benchmark,\ni.e\n.\n, each episode is executed once without repeated rollouts. Note that the average success rate is computed over the entire evaluation set.\nAs shown in Table\n2\n, our method significantly boosts the policy’s performance, surpassing all previous methods by a huge margin. In particular, our approach demonstrates pronounced robustness under challenging perturbations such as camera-viewpoint shifts (\n+\n11.6\n%\n+11.6\\%\n), robot initial-state perturbations (\n+\n16.3\n%\n+16.3\\%\n), and sensor noise (\n+\n12.5\n%\n+12.5\\%\n), where existing language-guided or vision-guided policies exhibit significant degradation. These results highlight the effectiveness of our action-space guidance in improving generalization under diverse perturbation factors.\nVLABench Benchmark.\nVLABench is a large-scale evaluation suite aimed at benchmarking both VLAs and VLMs on diverse robotic tasks. Built on ManiSkill3\n[\n45\n]\n, its manipulation benchmark consists of various tabletop scenarios,\ne.g\n.\n, contact-rich interactions and articulated-object manipulation. The standard evaluation is organized into\n5\n5\npublic tracks,\ni.e\n.\n, in-distribution, cross-category, commonsense, semantic-instruction, and unseen-texture, which respectively assess standard in-distribution performance, category-level generalization, commonsense reasoning, instruction understanding, and robustness to appearance variations. Particularly, VLABench proposes Intention Score (IS) and Progress Score (PS) to evaluate robot policies.\nIn our context, we train\nπ\n0\n\\pi_{0}\n,\nπ\n0.5\n\\pi_{0.5}\n, along with our method in a unified training setup. The model training is performed on VLABench’s official training data, with a global batch size\n128\n128\n. All models are optimized for\n60\n​\nK\n60\\text{K}\nsteps. We present quantitative results in Table\n3\n. Overall, our method achieves the best performance across both IS and PS. Notably, under the unseen-texture track, it delivers substantial gains,\ni.e\n.\n,\n+\n12.6\n%\n{+12.6\\%}\nin IS and\n+\n7.2\n%\n{+7.2\\%}\nin PS, indicating strong robustness to distributional shifts. Together, these results further confirm the effectiveness of our proposed approach.\nName\nAction\nshift\nAction\nhorizon\nEqui.\nhorizon\nSpatial\nObject\nGoal\nLong\nAvg.\nBaseline\n1\n10\n10\n98.6\n99.0\n96.4\n92.2\n96.6\n1\n10\n10\n99.4\n99.4\n98.8\n95.0\n98.2\n2\n5\n10\n99.6\n99.6\n98.4\n94.4\n98.0\n1\n30\n30\n99.2\n99.2\n97.6\n95.6\n97.9\n+EAR\n2\n15\n30\n99.0\n99.4\n98.0\n96.6\n98.3\n2\n30\n60\n99.4\n99.0\n98.2\n95.0\n97.9\n3\n30\n90\n98.8\n99.4\n97.4\n96.2\n98.0\nTable 5\n:\nReference action parameter ablation. We observe that different reference-action configurations within EAR generally lead to performance improvements.\nMethods\nSpatial\nObject\nGoal\nLong\nAvg.\nBaseline\n98.8\n98.2\n98.0\n92.4\n96.9\nQuery\n98.8\n99.0\n97.2\n92.8\n97.0\nAttention Pooling\n99.4\n98.6\n98.2\n92.8\n97.3\nDownsample\n99.2\n99.2\n98.2\n95.6\n98.1\nTable 6\n:\nComparison of KV-cache interaction strategies in IAR.\n4.3\nAblation Study\nWe examine each component’s contribution via systematic ablation experiments on the LIBERO benchmark, which are shown in Table\n4\n, Table\n5\n, and Table\n6\n. Note that we adopt\nπ\n0.5\n\\pi_{0.5}\nas the “Baseline” method. More ablations in different benchmarks are in Appendix\nC\n.\nEAR.\nAs shown in Table\n4\n, compared with the baseline, the experiment “#1” introduces the Explicit Action Reasoner (EAR) module into policy learning, which lifts the average success rate from\n96.9\n%\n96.9\\%\nto\n98.3\n%\n98.3\\%\n, demonstrating that the explicit action-space guidance benefits the robotic action sequence prediction.\nA plausible explanation is that EAR introduces an intermediate reference action sequence, which injects strong inductive bias on the behavior and thereby reduces ambiguity in mapping from observations to actions.\nIAR.\nAnalogously, with the Implicit Action Reasoner (IAR) module added in “#2”, the average success rate increases from\n96.9\n%\n96.9\\%\nto\n98.1\n%\n98.1\\%\n. This gain suggests that exploiting the implicit action distribution encoded in vision–language representations can also provide effective guidance for policy learning.\nThis performance gain can be partly attributed to the fact that IAR distills action-related clues implicitly encoded within the vision–language backbone, which potentially reflects the distribution of feasible actions. Such priors encourages the policy to remain closer to coherent, task-consistent behavioral patterns.\nEAR + IAR.\nIn Table\n4\n, experiment “#3” incorporates both EAR and IAR, achieving the highest average success rate of\n98.5\n%\n98.5\\%\n. The consistent improvements demonstrate that explicit action guidance and implicit action cues extracted from VLM’s key-value cache are complementary, jointly providing stronger guidance for accurate action prediction.\nReference Action Configurations.\nTo further examine the effect of explicit action references in EAR, we investigate different settings of action shift and action horizon, as summarized in Table\n5\n. We observe various parameter combinations consistently bring improvements over the baseline, indicating that providing action cues is broadly beneficial for policy learning. Besides, we find that shorter horizons combined with moderate shifts tend to produce relatively stronger gains. These observations offer further insight into how explicit action guidance influence policy learning.\nFigure 3\n:\nVisualization of three manipulation tasks in real world.\nKV-cache Interaction Strategies.\nWe compare three strategies for extracting action-relevant cues from VLM’s key-value cache within IAR module, as presented in Table\n6\n. Concretely, “Query” method uses learnable queries to attend to VLM’s original key-value cache.\n“Attention Pooling” method forms a pooled query by averaging key-value cache and then applies cross-attention operation.\n“Downsample” method first downsamples VLM’s key-value cache and then aggregates them using learnable matrix.\nAs shown in Table\n6\n, all three variants outperform the baseline, indicating that extracting implicit action cues from VLM benefits policy learning. Notably, the “Downsample” strategy achieves the best performance, suggesting that VLM’s features may contain noisy information for action prediction. This also highlights the importance of designing appropriate interaction mechanisms to align vision-language and action.\n4.4\nReal-World Deployment\nTo further validate the effectiveness of our framework, we conduct extensive real-world experiments on the AgiBot G1 robot. We consider three manipulation tasks,\ni.e\n.\n, “Wipe Stain”, “Pour Water”, and “Open-set Pick”, which respectively assess contact-rich manipulation, fine-grained object handling, and instruction-following abilities.\nSpecifically, as visualized in Fig.\n3\n, the “Wipe Stain” task requires the robot to pick up a sponge from the table and wipe away the stain until the surface is clean. The “Pour Water” task requires the robot to grasp the kettle by its handle, locate the target cup, pour water into it without causing overflow, and finally return the kettle to the table in a stable manner. The “Open-set Pick” task instructs the robot to pick up the correct tabletop object according to given natural-language command. Additionally, to examine the cross-embodiment adaptability, we also perform the “Open-set Pick” task on the AgileX robotic platform.\nDetails about training and evaluation are provided in the Appendix\nB\n.\nAs shown in Fig.\n4\n, our approach achieves consistently higher average success rates than both\nπ\n0.5\n\\pi_{0.5}\nand\nπ\n0\n\\pi_{0}\n,\ni.e\n.\n,\n66.7\n%\n66.7\\%\nagainst\n61.0\n%\n61.0\\%\nand\n33.8\n%\n33.8\\%\n. These results demonstrate that the proposed framework maintains effectiveness under real-world sensing conditions. Moreover, the aligned improvements observed on both Agibot G1 and AgileX also indicate that our method exhibits adaptability across different robotic embodiments.\nFigure 4\n:\nEvaluation results of real-world experiments.\n5\nConclusion\nIn this work, we addressed the fundamental semantic-kinematic gap in modern robotic policies by proposing a new paradigm: Action Chain-of-Thought (ACoT). We argued that for physically grounded intelligence, deliberation should occur not in the abstract space of language or vision, but directly in the kinematically grounded space of actions. We materialized this concept in our ACoT-VLA framework, which leverages two synergistic modules,\ni.e\n.\n, an Explicit Action Reasoner (EAR) and an Implicit Action Reasoner (IAR), to generate and fuse both explicit trajectory plans and implicit behavioral priors. This action-centric guidance mechanism creates a direct, information-rich conduit between high-level intent and low-level motor control.\nOur extensive experiments across multiple simulation and real-world benchmarks demonstrate that this approach yields state-of-the-art performance, significantly improving both task success and robustness. By shifting the locus of reasoning from perception to action, our work not only provides a more effective and grounded method for robot policy learning but also opens a new avenue for research into more structured, interpretable, and capable embodied agents. We believe that learning to “think” in the language of actions is a critical step towards developing the next generation of generalist robots.\nReferences\n[1]\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat,\net al.\n(2023)\nGpt-4 technical report\n.\narXiv preprint arXiv:2303.08774\n.\nCited by:\n§1\n.\n[2]\nS. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang,\net al.\n(2025)\nQwen2. 5-vl technical report\n.\narXiv preprint arXiv:2502.13923\n.\nCited by:\n§1\n.\n[3]\nL. Beyer, A. Steiner, A. S. Pinto, A. Kolesnikov, X. Wang, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello,\net al.\n(2024)\nPaligemma: a versatile 3b vlm for transfer\n.\narXiv preprint arXiv:2407.07726\n.\nCited by:\n§4.1\n.\n[4]\nJ. Bjorck, F. Castañeda, N. Cherniadev, X. Da, R. Ding, L. Fan, Y. Fang, D. Fox, F. Hu, S. Huang,\net al.\n(2025)\nGr00t n1: an open foundation model for generalist humanoid robots\n.\narXiv preprint arXiv:2503.14734\n.\nCited by:\nTable 1\n.\n[5]\nK. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter,\net al.\nπ\n0\n\\pi_{0}\n: A vision-language-action flow model for general robot control. corr, abs/2410.24164, 2024. doi: 10.48550\n.\narXiv preprint ARXIV.2410.24164\n.\nCited by:\n§1\n,\n§2\n,\nTable 1\n,\nTable 2\n,\nTable 3\n.\n[6]\nQ. Bu, J. Cai, L. Chen, X. Cui, Y. Ding, S. Feng, S. Gao, X. He, X. Hu, X. Huang,\net al.\n(2025)\nAgibot world colosseo: a large-scale manipulation platform for scalable and intelligent embodied systems\n.\narXiv preprint arXiv:2503.06669\n.\nCited by:\n§2\n.\n[7]\nQ. Bu, Y. Yang, J. Cai, S. Gao, G. Ren, M. Yao, P. Luo, and H. Li\n(2025)\nUniVLA: learning to act anywhere with task-centric latent actions\n.\nExternal Links:\n2505.06111\n,\nLink\nCited by:\nTable 2\n.\n[8]\nJ. Cen, C. Yu, H. Yuan, Y. Jiang, S. Huang, J. Guo, X. Li, Y. Song, H. Luo, F. Wang,\net al.\n(2025)\nWorldVLA: towards autoregressive action world model\n.\narXiv preprint arXiv:2506.21539\n.\nCited by:\n§2\n,\nTable 1\n,\nTable 1\n,\nTable 2\n.\n[9]\nT. Chen, Z. Chen, B. Chen, Z. Cai, Y. Liu, Z. Li, Q. Liang, X. Lin, Y. Ge, Z. Gu,\net al.\n(2025)\nRobotwin 2.0: a scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation\n.\narXiv preprint arXiv:2506.18088\n.\nCited by:\n§2\n.\n[10]\nT. Chen, R. Zhang, and G. Hinton\n(2022)\nAnalog bits: generating discrete data using diffusion models with self-conditioning\n.\narXiv preprint arXiv:2208.04202\n.\nCited by:\n§3.2\n.\n[11]\nC. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song\n(2025)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nThe International Journal of Robotics Research\n44\n(\n10-11\n),\npp. 1684–1704\n.\nCited by:\n§1\n,\nTable 1\n.\n[12]\nS. Deng, M. Yan, S. Wei, H. Ma, Y. Yang, J. Chen, Z. Zhang, T. Yang, X. Zhang, W. Zhang,\net al.\n(2025)\nGraspvla: a grasping foundation model pre-trained on billion-scale synthetic action data\n.\narXiv preprint arXiv:2505.03233\n.\nCited by:\n§2\n.\n[13]\nD. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang,\net al.\n(2023)\nPalm-e: an embodied multimodal language model\n.\nCited by:\n§3.3\n.\n[14]\nJ. Duan, W. Yuan, W. Pumacay, Y. R. Wang, K. Ehsani, D. Fox, and R. Krishna\n(2024)\nManipulate-anything: automating real-world robots using vision-language models\n.\narXiv preprint arXiv:2406.18915\n.\nCited by:\n§2\n.\n[15]\nS. Fei, S. Wang, J. Shi, Z. Dai, J. Cai, P. Qian, L. Ji, X. He, S. Zhang, Z. Fei,\net al.\n(2025)\nLIBERO-plus: in-depth robustness analysis of vision-language-action models\n.\narXiv preprint arXiv:2510.13626\n.\nCited by:\nAppendix A\n,\nTable 2\n,\n§4.1\n,\n§4.2\n,\n§4.2\n.\n[16]\nD. Ha and J. Schmidhuber\n(2018)\nWorld models\n.\narXiv preprint arXiv:1803.10122\n2\n(\n3\n).\nCited by:\n§1\n.\n[17]\nC. Huang, Y. Wu, M. Chen, Y. F. Wang, and F. Yang\n(2025)\nThinkact: vision-language-action reasoning via reinforced visual latent planning\n.\narXiv preprint arXiv:2507.16815\n.\nCited by:\nTable 1\n.\n[18]\nS. Huang, H. Chang, Y. Liu, Y. Zhu, H. Dong, P. Gao, A. Boularias, and H. Li\n(2024)\nA3vlm: actionable articulation-aware vision language model\n.\narXiv preprint arXiv:2406.07549\n.\nCited by:\n§2\n.\n[19]\nW. Huang, C. Chen, H. Qi, C. Lv, Y. Du, and H. Yang\n(2025)\nMoTVLA: a vision-language-action model with unified fast-slow reasoning\n.\narXiv preprint arXiv:2510.18337\n.\nCited by:\n§2\n.\n[20]\nW. Huang, C. Wang, Y. Li, R. Zhang, and L. Fei-Fei\n(2024)\nRekep: spatio-temporal reasoning of relational keypoint constraints for robotic manipulation\n.\narXiv preprint arXiv:2409.01652\n.\nCited by:\n§2\n.\n[21]\nC. Hung, Q. Sun, P. Hong, A. Zadeh, C. Li, U. Tan, N. Majumder, S. Poria,\net al.\n(2025)\nNora: a small open-sourced generalist vision language action model for embodied tasks\n.\narXiv preprint arXiv:2504.19854\n.\nCited by:\nTable 2\n.\n[22]\nP. Intelligence, K. Black, N. Brown, J. Darpinian, K. Dhabalia, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai,\net al.\n(2025)\nπ\n0.5\n\\pi_{0.5}\n: A vision-language-action model with open-world generalization\n.\narXiv preprint arXiv:2504.16054\n.\nCited by:\nAppendix C\n,\n§1\n,\n§2\n,\nTable 1\n,\nTable 2\n,\n§4.1\n,\nTable 3\n.\n[23]\nT. Jiang, T. Yuan, Y. Liu, C. Lu, J. Cui, X. Liu, S. Cheng, J. Gao, H. Xu, and H. Zhao\n(2025)\nGalaxea open-world dataset and g0 dual-system vla model\n.\narXiv preprint arXiv:2509.00576\n.\nCited by:\n§2\n.\n[24]\nM. J. Kim, C. Finn, and P. Liang\n(2025)\nFine-tuning vision-language-action models: optimizing speed and success\n.\narXiv preprint arXiv:2502.19645\n.\nCited by:\nTable 1\n,\nTable 2\n.\n[25]\nM. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi,\net al.\n(2024)\nOpenvla: an open-source vision-language-action model\n.\narXiv preprint arXiv:2406.09246\n.\nCited by:\n§1\n,\n§2\n,\nTable 1\n,\nTable 2\n.\n[26]\nH. Li, P. Ding, R. Suo, Y. Wang, Z. Ge, D. Zang, K. Yu, M. Sun, H. Zhang, D. Wang,\net al.\n(2025)\nVla-rft: vision-language-action reinforcement fine-tuning with verified rewards in world simulators\n.\narXiv preprint arXiv:2510.00406\n.\nCited by:\n§2\n.\n[27]\nX. Li, M. Zhang, Y. Geng, H. Geng, Y. Long, Y. Shen, R. Zhang, J. Liu, and H. Dong\n(2024)\nManipllm: embodied multimodal large language model for object-centric robotic manipulation\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp. 18061–18070\n.\nCited by:\n§2\n.\n[28]\nZ. Liang, Y. Li, T. Yang, C. Wu, S. Mao, L. Pei, X. Yang, J. Pang, Y. Mu, and P. Luo\n(2025)\nDiscrete diffusion vla: bringing discrete diffusion to action decoding in vision-language-action policies\n.\narXiv preprint arXiv:2508.20072\n.\nCited by:\nTable 1\n.\n[29]\nY. Liao, P. Zhou, S. Huang, D. Yang, S. Chen, Y. Jiang, Y. Hu, J. Cai, S. Liu, J. Luo,\net al.\n(2025)\nGenie envisioner: a unified world foundation platform for robotic manipulation\n.\narXiv preprint arXiv:2508.05635\n.\nCited by:\n§2\n,\nTable 1\n.\n[30]\nT. Lin, G. Li, Y. Zhong, Y. Zou, Y. Du, J. Liu, E. Gu, and B. Zhao\n(2025)\nEvo-0: vision-language-action model with implicit spatial understanding\n.\narXiv preprint arXiv:2507.00416\n.\nCited by:\n§2\n.\n[31]\nY. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le\n(2022)\nFlow matching for generative modeling\n.\narXiv preprint arXiv:2210.02747\n.\nCited by:\n§2\n.\n[32]\nB. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone\n(2023)\nLibero: benchmarking knowledge transfer for lifelong robot learning\n.\nAdvances in Neural Information Processing Systems\n36\n,\npp. 44776–44791\n.\nCited by:\nAppendix A\n,\n§4.1\n,\n§4.2\n.\n[33]\nS. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su, and J. Zhu\n(2024)\nRdt-1b: a diffusion foundation model for bimanual manipulation\n.\narXiv preprint arXiv:2410.07864\n.\nCited by:\n§1\n,\n§2\n.\n[34]\nY. Liu, S. Wang, D. Wei, X. Cai, L. Zhong, J. Yang, G. Ren, J. Zhang, M. Yao, C. Li,\net al.\n(2025)\nUnified embodied vlm reasoning with robotic action via autoregressive discretized pre-training\n.\narXiv preprint arXiv:2512.24125\n.\nCited by:\n§2\n.\n[35]\nY. Liu, R. Gal, A. H. Bermano, B. Chen, and D. Cohen-Or\n(2022)\nSelf-conditioned generative adversarial networks for image editing\n.\narXiv preprint arXiv:2202.04040\n.\nCited by:\n§3.2\n.\n[36]\nQ. Lv, W. Kong, H. Li, J. Zeng, Z. Qiu, D. Qu, H. Song, Q. Chen, X. Deng, and J. Pang\n(2025)\nF1: a vision-language-action model bridging understanding and generation to actions\n.\narXiv preprint arXiv:2509.06951\n.\nCited by:\n§2\n,\nTable 1\n.\n[37]\nA. O’Neill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain,\net al.\n(2024)\nOpen x-embodiment: robotic learning datasets and rt-x models: open x-embodiment collaboration 0\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp. 6892–6903\n.\nCited by:\n§2\n.\n[38]\nW. Peebles and S. Xie\n(2023)\nScalable diffusion models with transformers\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp. 4195–4205\n.\nCited by:\n§2\n.\n[39]\nK. Pertsch, K. Stachowicz, B. Ichter, D. Driess, S. Nair, Q. Vuong, O. Mees, C. Finn, and S. Levine\n(2025)\nFast: efficient action tokenization for vision-language-action models\n.\narXiv preprint arXiv:2501.09747\n.\nCited by:\nTable 1\n,\nTable 2\n.\n[40]\nS. Qian, W. Chen, M. Bai, X. Zhou, Z. Tu, and L. E. Li\n(2024)\nAffordancellm: grounding affordance from vision language models\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp. 7587–7597\n.\nCited by:\n§3.3\n.\n[41]\nD. Qu, H. Song, Q. Chen, Y. Yao, X. Ye, Y. Ding, Z. Wang, J. Gu, B. Zhao, D. Wang,\net al.\n(2025)\nSpatialvla: exploring spatial representations for visual-language-action model\n.\narXiv preprint arXiv:2501.15830\n.\nCited by:\n§1\n,\n§2\n,\nTable 1\n.\n[42]\nH. Shi, B. Xie, Y. Liu, L. Sun, F. Liu, T. Wang, E. Zhou, H. Fan, X. Zhang, and G. Huang\n(2025)\nMemoryvla: perceptual-cognitive memory in vision-language-action models for robotic manipulation\n.\narXiv preprint arXiv:2508.19236\n.\nCited by:\nTable 1\n.\n[43]\nM. Shukor, D. Aubakirova, F. Capuano, P. Kooijmans, S. Palma, A. Zouitine, M. Aractingi, C. Pascal, M. Russi, A. Marafioti,\net al.\n(2025)\nSmolvla: a vision-language-action model for affordable and efficient robotics\n.\narXiv preprint arXiv:2506.01844\n.\nCited by:\nTable 1\n.\n[44]\nS. Tan, K. Dou, Y. Zhao, and P. Krähenbühl\n(2025)\nInteractive post-training for vision-language-action models\n.\narXiv preprint arXiv:2505.17016\n.\nCited by:\nTable 2\n.\n[45]\nS. Tao, F. Xiang, A. Shukla, Y. Qin, X. Hinrichsen, X. Yuan, C. Bao, X. Lin, Y. Liu, T. Chan,\net al.\n(2024)\nManiskill3: gpu parallelized robotics simulation and rendering for generalizable embodied ai\n.\narXiv preprint arXiv:2410.00425\n.\nCited by:\n§4.2\n.\n[46]\nG. Team, R. Anil, S. Borgeaud, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican,\net al.\n(2023)\nGemini: a family of highly capable multimodal models\n.\narXiv preprint arXiv:2312.11805\n.\nCited by:\n§1\n.\n[47]\nG. R. Team, S. Abeyruwan, J. Ainslie, J. Alayrac, M. G. Arenas, T. Armstrong, A. Balakrishna, R. Baruch, M. Bauza, M. Blokzijl,\net al.\n(2025)\nGemini robotics: bringing ai into the physical world\n.\narXiv preprint arXiv:2503.20020\n.\nCited by:\n§2\n.\n[48]\nO. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu,\net al.\n(2024)\nOcto: an open-source generalist robot policy\n.\narXiv preprint arXiv:2405.12213\n.\nCited by:\n§1\n,\nTable 1\n.\n[49]\nY. Wang, X. Li, W. Wang, J. Zhang, Y. Li, Y. Chen, X. Wang, and Z. Zhang\n(2025)\nUnified vision-language-action model\n.\narXiv preprint arXiv:2506.19850\n.\nCited by:\nTable 1\n.\n[50]\nZ. Yan, W. Dong, Y. Shao, Y. Lu, H. Liu, J. Liu, H. Wang, Z. Wang, Y. Wang, F. Remondino,\net al.\n(2025)\nRenderworld: world model with self-supervised 3d label\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp. 6063–6070\n.\nCited by:\n§1\n.\n[51]\nY. Yang, Z. Duan, T. Xie, F. Cao, P. Shen, P. Song, P. Jin, G. Sun, S. Xu, Y. You,\net al.\n(2025)\nFPC-vla: a vision-language-action framework with a supervisor for failure prediction and correction\n.\narXiv preprint arXiv:2509.04018\n.\nCited by:\nTable 1\n.\n[52]\nT. Yuan, Y. Liu, C. Lu, Z. Chen, T. Jiang, and H. Zhao\n(2025)\nDepthVLA: enhancing vision-language-action models with depth-aware spatial reasoning\n.\narXiv preprint arXiv:2510.13375\n.\nCited by:\n§2\n.\n[53]\nM. Zawalski, W. Chen, K. Pertsch, O. Mees, C. Finn, and S. Levine\n(2024)\nRobotic control via embodied chain-of-thought reasoning\n.\narXiv preprint arXiv:2407.08693\n.\nCited by:\n§1\n.\n[54]\nX. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer\n(2023)\nSigmoid loss for language image pre-training\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp. 11975–11986\n.\nCited by:\n§4.1\n.\n[55]\nJ. Zhang, Y. Chen, Y. Xu, Z. Huang, Y. Zhou, Y. Yuan, X. Cai, G. Huang, X. Quan, H. Xu,\net al.\n(2025)\n4D-vla: spatiotemporal vision-language-action pretraining with cross-scene calibration\n.\narXiv preprint arXiv:2506.22242\n.\nCited by:\n§2\n.\n[56]\nJ. Zhang, Y. Guo, Y. Hu, X. Chen, X. Zhu, and J. Chen\n(2025)\nUp-vla: a unified understanding and prediction model for embodied agent\n.\narXiv preprint arXiv:2501.18867\n.\nCited by:\n§2\n.\n[57]\nS. Zhang, Z. Xu, P. Liu, X. Yu, Y. Li, Q. Gao, Z. Fei, Z. Yin, Z. Wu, Y. Jiang,\net al.\n(2025)\nVlabench: a large-scale benchmark for language-conditioned robotics manipulation with long-horizon reasoning tasks\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp. 11142–11152\n.\nCited by:\nAppendix A\n,\n§2\n,\n§4.1\n,\n§4.2\n.\n[58]\nW. Zhang, H. Liu, Z. Qi, Y. Wang, X. Yu, J. Zhang, R. Dong, J. He, F. Lu, H. Wang,\net al.\n(2025)\nDreamvla: a vision-language-action model dreamed with comprehensive world knowledge\n.\narXiv preprint arXiv:2507.04447\n.\nCited by:\n§1\n,\n§2\n,\nTable 1\n.\n[59]\nZ. Zhang, H. Li, Y. Dai, Z. Zhu, L. Zhou, C. Liu, D. Wang, F. E. Tay, S. Chen, Z. Liu,\net al.\n(2025)\nFrom spatial to actions: grounding vision-language-action model in spatial foundation priors\n.\narXiv preprint arXiv:2510.17439\n.\nCited by:\n§2\n.\n[60]\nQ. Zhao, Y. Lu, M. J. Kim, Z. Fu, Z. Zhang, Y. Wu, Z. Li, Q. Ma, S. Han, C. Finn,\net al.\n(2025)\nCot-vla: visual chain-of-thought reasoning for vision-language-action models\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp. 1702–1713\n.\nCited by:\n§2\n,\nTable 1\n.\n[61]\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn\n(2023)\nLearning fine-grained bimanual manipulation with low-cost hardware\n.\narXiv preprint arXiv:2304.13705\n.\nCited by:\n§1\n.\n[62]\nH. Zhen, X. Qiu, P. Chen, J. Yang, X. Yan, Y. Du, Y. Hong, and C. Gan\n(2024)\n3d-vla: a 3d vision-language-action generative world model\n.\narXiv preprint arXiv:2403.09631\n.\nCited by:\n§1\n.\n[63]\nJ. Zheng, J. Li, D. Liu, Y. Zheng, Z. Wang, Z. Ou, Y. Liu, J. Liu, Y. Zhang, and X. Zhan\n(2025)\nUniversal actions for enhanced embodied foundation models\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp. 22508–22519\n.\nCited by:\nTable 1\n.\n[64]\nR. Zheng, Y. Liang, S. Huang, J. Gao, H. Daumé III, A. Kolobov, F. Huang, and J. Yang\n(2024)\nTracevla: visual trace prompting enhances spatial-temporal awareness for generalist robotic policies\n.\narXiv preprint arXiv:2412.10345\n.\nCited by:\nTable 1\n.\n[65]\nW. Zheng, W. Chen, Y. Huang, B. Zhang, Y. Duan, and J. Lu\n(2024)\nOccworld: learning a 3d occupancy world model for autonomous driving\n.\nIn\nEuropean conference on computer vision\n,\npp. 55–72\n.\nCited by:\n§1\n.\n[66]\nC. Zhou, L. Yu, A. Babu, K. Tirumala, M. Yasunaga, L. Shamis, J. Kahn, X. Ma, L. Zettlemoyer, and O. Levy\n(2024)\nTransfusion: predict the next token and diffuse images with one multi-modal model\n.\narXiv preprint arXiv:2408.11039\n.\nCited by:\n§2\n.\n[67]\nB. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid,\net al.\n(2023)\nRt-2: vision-language-action models transfer web knowledge to robotic control\n.\nIn\nConference on Robot Learning\n,\npp. 2165–2183\n.\nCited by:\n§2\n.\nAppendix A\nDataset Description\nIn this section, we present a comprehensive characterization of the benchmark datasets and the custom-collected data used for model training in our experiments. We systematically report key statistics, including the total number of episodes, frame counts, and other relevant properties, which is summarized in Table\n7\nbelow:\nType\nDataset\nEmbodiment\nDoF\nEpisodes\nFrames\nFPS\nSimulation\nLIBERO\nFranka\n7\n1,693\n273,465\n10\nLIBERO-Plus\nFranka\n7\n14,347\n2,238,036\n20\nVLABench\nFranka\n7\n4,713\n528,398\n10\nReal-World\nWipe Stain\nAgiBot G1\n22\n177\n356,316\n30\nPour Water\nAgiBot G1\n22\n1,821\n5,062,506\n30\nOpen-set Pick\nAgiBot G1\n22\n1,936\n219,824\n30\nOpen-set Pick\nAgileX\n14\n962\n251,283\n30\nTable 7\n:\nDataset statistics.\nSimulation Benchmarks.\nWe utilize three publicly released simulation datasets,\ni.e\n.\n, LIBERO\n[\n32\n]\n, LIBERO-Plus\n[\n15\n]\n, and VLABench\n[\n57\n]\n.\nSpecifically, the LIBERO dataset contains\n1\n,\n693\n1,693\nepisodes and\n273\n,\n465\n273,465\nframes, recorded at a fixed\n10\n10\nHz. Its demonstrations exhibit relatively uniform trajectory lengths and smooth motion patterns, making it widely adopted benchmark in community.\nHowever, due to the increasing performance saturation observed on LIBERO, LIBERO-Plus is recently introduced to provide a more challenging and diversified evaluation setting. LIBERO-Plus provides\n14\n,\n347\n14,347\nepisodes and\n2\n,\n238\n,\n036\n2,238,036\nframes, captured at\n20\n20\nHz.\nIn contrast to the homogeneous trajectories in LIBERO, LIBERO-Plus explicitly emphasizes a perturbation-oriented design. The demonstrations display substantially larger variations in motion magnitude and camera–robot viewpoint configuration.\nThese characteristics make it a more suitable benchmark for evaluating policy generalization under structured distribution shifts.\nBesides these two datasets, we further benchmark our method on VLABench, whose training set includes\n4\n,\n713\n4,713\nepisodes and\n528\n,\n398\n528,398\nframes, recorded at\n10\n10\nHz, which requires a higher level of visual and physical understanding from the policy.\nReal-World Experiment.\nFor real-world deployment, we collect demonstrations across\n3\n3\ntasks,\ni.e\n.\n, Wipe Stain, Pour Water, and Open-set Pick, as shown in Table\n7\n.\nThe “Wipe Stain” dataset contains\n177\n177\nepisodes with\n356\n,\n316\n356,316\nframes, characterized by dense tool–surface contact and fine-grained force control. The “Pour Water” dataset includes\n1\n,\n821\n1,821\nepisodes and\n5\n,\n062\n,\n506\n5,062,506\nframes. Its large scale stems from the task’s long-horizon and multi-stage nature. Regarding the “Open-set Pick” task, the AgiBot G1 subset provides\n1\n,\n936\n1,936\nepisodes with\n219\n,\n824\n219,824\nframes, while the AgileX subset offers\n962\n962\nepisodes with\n251\n,\n283\n251,283\nframes, both featuring diverse tabletop layouts and natural-language instructions.\nAppendix B\nTraining & Evaluation Details\nTask\nAction Space\nAction Horizon\nState\nBatch Size\nTraining Step\nLIBERO\nDelta EEF\n10\n×\n\\times\n128\n40K\nLIBERO-Plus\nDelta EEF\n10\n×\n\\times\n128\n100K\nVLABench\nAbs EEF\n10\n✓\n128\n60K\nWipe Stain\nAbs Joint\n30\n✓\n128\n50K\nPour Water\nAbs Joint\n30\n✓\n128\n240K\nOpen-set Pick\nAbs Joint\n30\n✓\n128\n50K\nOpen-set Pick\n†\nAbs Joint\n30\n✓\n128\n50K\nTable 8\n:\nTraining details. Note that the “Open-set Pick\n†\n” task is performed on AgileX platform.\nTraining Details.\nWe describe the task-specific training configurations,\ne.g\n.\n, action space and state usage, for better understanding.\nAs presented in Table\n8\n, for the LIBERO and LIBERO-Plus suites, the policy is trained using delta end-effector control (Delta EEF) with an action horizon of\n10\n10\nsteps. In particular, no privileged state information is provided during training. We utilize a global batch size of\n128\n128\nand train the policies for\n40\n​\nK\n40\\text{K}\nand\n100\n​\nK\n100\\text{K}\nsteps, respectively. Similarly, we train our models in VLABench for\n60\n​\nK\n60\\text{K}\nsteps, while adopting state input and absolute end-effector (Abs EEF) actions to align the benchmark’s control convention.\nIn terms of the real-world tasks, we utilize Abs Joint control with a longer action horizon of\n30\n30\n.\nUnlike the simulator benchmarks, these tasks additionally provide structured robot state observations to improve robustness under real-world sensing and actuation noise.\nOur models are trained for\n50\n​\nK\n50\\text{K}\n,\n240\n​\nK\n240\\text{K}\n, and\n50\n​\nK\n50\\text{K}\nsteps, in “Wipe Stain”, “Pour Water”, and “Open-set Pick” tasks, respectively, with same batch size of\n128\n128\n.\nName\nEAR\nIAR\nCamera\nRobot\nLanguage\nLight\nBackground\nNoise\nLayout\nAvg.\nBaseline\n70.3\n41.7\n81.1\n97.3\n94.6\n71.8\n84.9\n75.7\n#1\n✓\n88.7\n63.5\n80.4\n94.0\n90.2\n89.5\n84.2\n83.7\n#2\n✓\n80.7\n48.7\n82.6\n97.7\n90.9\n84.3\n86.0\n80.4\n#3\n✓\n✓\n91.2\n62.5\n80.3\n95.1\n91.5\n88.3\n84.9\n84.1\nTable 9\n:\nModule ablations on LIBERO-Plus benchmark. The performance is gradually improved with the addition of proposed methods.\nName\nAction Head\nEAR\nLIBERO\nLIBERO-Plus\nParam.\nDenoise\nParam.\nDenoise\nSpatial\nObject\nGoal\nLong\nAvg.\nCamera\nRobot\nLanguage\nLight\nBackground\nNoise\nLayout\nAvg.\nBaseline\n300M\n10\n-\n-\n98.6\n99.0\n96.4\n92.2\n96.6\n70.3\n41.7\n81.1\n97.3\n94.6\n71.8\n84.9\n75.7\n#1\n600M\n10\n-\n-\n97.6\n98.4\n97.8\n96.4\n97.6\n68.7\n44.8\n83.1\n96.4\n92.7\n66.6\n84.1\n74.9\n#2\n600M\n20\n-\n-\n97.8\n98.8\n98.0\n95.2\n97.5\n70.0\n44.8\n82.7\n97.6\n93.1\n66.7\n83.2\n75.1\n#3\n300M\n5\n300M\n5\n98.6\n99.6\n97.8\n95.4\n97.9\n88.2\n62.4\n81.5\n95.0\n91.5\n88.6\n85.3\n83.9\n#4\n300M\n10\n300M\n10\n99.0\n99.4\n98.0\n96.6\n98.3\n88.7\n63.5\n80.4\n94.0\n90.2\n89.5\n84.2\n83.7\n#4\n300M\n10\n300M\n10\n99.0\n99.4\n98.0\n96.6\n98.3\n88.7\n63.5\n80.4\n94.0\n90.2\n89.5\n84.2\n83.7\n#5\n300M\n10\n150M\n10\n99.2\n99.2\n97.8\n94.2\n97.6\n86.4\n54.3\n81.7\n92.2\n91.4\n89.1\n82.1\n81.7\n#6\n300M\n10\n250M\n10\n99.0\n98.2\n98.6\n94.2\n97.5\n87.2\n59.7\n81.1\n95.0\n93.7\n87.4\n83.5\n83.1\n#7\n300M\n10\n500M\n10\n98.4\n99.4\n96.6\n94.2\n97.0\n80.8\n57.6\n84.1\n95.6\n92.1\n79.8\n83.7\n80.9\nTable 10\n:\nEffects of parameters and denoise steps on policy performance. The best results are highlighted in\nbold\n, and the second-best results are\nunderlined\n. Note that the IAR module is not added in this experiment.\nEvaluation Details.\nNext, we illustrate the evaluation protocols and success criteria for all real-world tasks. Each task is assessed using fixed and repeatable initializations to ensure reproducibility and reduce environmental variance.\nConcretely, in terms of the “Wipe Stain” task, we predefine three initial sponge poses. For each pose, the robot is required to clean stains placed at four distinct table locations. Every configuration is executed twice, resulting in\n24\n24\ntrials in total. A trial is considered successful if the robot grasps the sponge and removes the stain from the specified location.\nAs for the “Pour Water”, we standardize six predefined relative configurations between the bottle and the glass. Then, each configuration is executed two times. A trial is counted as successful if the robot lifts the bottle, pours water into the cup, and places the bottle back onto the coaster. Note that minor spillage of water when pouring is allowed.\nEventually, regarding the “Open-set Pick” task, we initialize ten object arrangements on the table, containing both in-distribution and out-of-distribution instances. In each arrangement, the robot is instructed to grasp a specified target object using either its left or right arm, as indicated by the instruction. Each arm–object pair is evaluated twice, resulting in\n40\n40\ntrials overall. A trial is deemed successful if the robot grasps the instructed object with the correct arm.\nAcross all tasks, evaluations are carried out by trained operators with substantial prior testing experience, and success rates are computed as the proportion of successful trials relative to the total number of executed attempts.\nAppendix C\nMore Experimental Results\nIn this section, we provide additional quantitative experiments to substantiate the effectiveness of our proposed approach and to empirically uncover several insightful phenomena. Specifically, the experimental analyses comprise three parts: (1) ablation study conducted on the LIBERO-Plus benchmark in Table\n9\n, (2) an investigation of how the parameter sizes of the Action Head and Explicit Action Reasoner (EAR), as well as the number of denoising steps, influence policy performance in Table\n10\n, and (3) a comparative study examining the relationship among inference latency, model size, and performance in Table\n11\n. Note that we adopt\nπ\n0.5\n\\pi_{0.5}\n[\n22\n]\nas the baseline method, denoted as “Baseline”.\nModule Ablation.\nAs shown in Table\n9\n, incorporating the proposed reasoning modules consistently improves policy performance on the LIBERO-Plus benchmark.\nAdding the EAR module,\ni.e\n.\n, experiment “#1”, yields a clear gain over the baseline, increasing the average success rate from\n75.7\n%\n75.7\\%\nto\n83.7\n%\n83.7\\%\n. This improvement can be attributed to EAR’s ability to generate an explicit reference action trajectory, which significantly reduces the ambiguity in mapping complex visual or linguistic observations to low-level actions, such as camera shifts and background changes.\nMeanwhile, incorporating only the IAR (“#2”) also improves the performance from\n75.7\n%\n75.7\\%\nto\n80.4\n%\n80.4\\%\n, indicating that decoding the latent action-related semantics within the vision–language backbone provides useful behavioral priors.\nFinally, combining EAR and IAR (“#3”) achieves the highest success rate of\n84.1\n%\n84.1\\%\n, demonstrating their complementary effects,\ni.e\n.\n, EAR provides explicit motion guidance, while IAR supplies dense representation-level priors.\nEffect of Model Scaling & Denoising Budget.\nThen, we analyze the superiority of our method by comparing settings with matched total model parameters and denoising steps. As shown in Table\n10\n, firstly, we enlarge the model size of the action head and increase the number of denoising steps in experiments “#1” and “#2”, to construct fair baselines for subsequent comparison.\nWe observe a preliminary observation,\ni.e\n.\n, increasing the model size or denoising steps does not reliably enhance performance. Specifically, compared with the baseline, while “#1” improves performance on the LIBERO benchmark, it simultaneously drops on LIBERO-Plus. Next, comparing “#1” and “#2” reveals that further increasing denoising steps yields only negligible fluctuations.\nSubsequently, we incorporate the EAR module under fully matched overall parameterization and denoising budgets. Concretely, in both comparison pairs, “#1” with “#3” and “#2” with “#4”, we consistently observe notable performance improvements on both benchmarks, once the EAR module is introduced. This indicates that the performance gains originate from our proposed action chain-of-thought. The proposed mechanism supplies explicit reference actions that effectively mitigate the intrinsic instability of action prediction, especially under challenging external perturbations, as shown in the LIBERO-Plus, enabling a more reliable and grounded generalist robotic policy.\nEffect of EAR Scale.\nMoreover, we investigate how various scale of the EAR module influences action prediction fidelity. To isolate the effect of EAR, we keep the action head parameters and the denoising schedule strictly fixed, while scaling the EAR module to\n150\n​\nM\n150\\text{M}\n,\n250\n​\nM\n250\\text{M}\n,\n300\n​\nM\n300\\text{M}\n, and\n500\n​\nM\n500\\text{M}\nparameters via adjusting hidden size.\nAs presented in Table\n10\n, through the comparison across experiments “#4”, “#5”, “#6”, and “#7”, we find that although all EAR-equipped variants outperform non-EAR baselines on both benchmarks, the performance trend is non-monotonic. Applying moderate EAR scales,\ne.g\n.\n,\n300\n​\nM\n300\\text{M}\n, yields the greatest improvement.\nParticularly, as evidenced in “#7” in Table\n10\n, when the parameter of EAR module even exceeds that of the action head, we observe a marked drop in performance. We attribute this degradation to the tendency of an over-parameterized EAR to overfit spurious correlations during training. Therefore, it generates reference action trajectories that are systematically biased, which ultimately misdirect the action head toward suboptimal predictions.\nName\nEAR\nIAR\nParam.\nLatency\nLIBERO\nLIBERO-Plus\nAvg. SR\nAvg. SR\nBaseline\n3.35B\n91ms\n96.9\n75.7\n#1\n✓\n3.80B\n110ms\n98.3\n83.7\n#2\n✓\n3.36B\n93ms\n98.1\n80.4\n#3\n✓\n✓\n3.81B\n112ms\n98.5\n84.1\nTable 11\n:\nAblation experiment on model efficiency and performance.\nLatency Analysis.\nIn Table\n11\n, we further examine the inference efficiency of our approach in terms of both parameter count and end-to-end latency. As additional reasoning modules are introduced, we observe a slight increase in latency. Incorporating the EAR module raises latency from\n91\n​\nms\n91\\text{ms}\nto\n110\n​\nms\n110\\text{ms}\n, while adding the IAR module introduces only an additional\n2\n​\nms\n2\\text{ms}\n. However, this marginal overhead is outweighed by the substantial improvement, which reflects a favorable trade-off.\nAppendix D\nLimitations & Future Works\nIn this section, we discuss the limitations existing in our work and promising directions for future research.\nAlthough our proposed action chain-of-thought (ACoT) substantially boosts policy performance, our framework still exhibits several constraints. The reasoning modules introduce additional computational cost, which, while relatively modest compared to the performance gains, may pose challenges for deployment on resource-constrained robotic platforms.\nBesides, another limitation stems from the fact that the prevailing action representation in the community is implemented as action chunks,\ni.e\n.\n, sequences of low-level control commands such as joint angles or end-effector poses. While such representations faithfully encode the executed motions, they lack explicit geometric structure that would facilitate higher-level spatial reasoning, such as object-centric coordination and contact geometry. Hence, the potential of ACoT reasoning may not be fully unleashed. Enriching action representations with spatially grounded information to enable ACoT to operate in geometrically interpretable 3D space, constitutes an interesting and promising avenue for future exploration.\nAppendix E\nLLM Usage Statement\nIn this paper, we employ Large Language Models (LLMs) solely for minor linguistic refinement during the manuscript preparation stage, such as correcting grammatical errors. None of the technical content, implementation details, or experimental results were generated by LLMs.",
    "preview_text": "Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.\n\nACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models\nLinqing Zhong\n1,2\nYi Liu\n2\nYifei Wei\n1,2\nZiyu Xiong\n2\nMaoqing Yao\n2∗\nSi Liu\n1∗\nGuanghui Ren\n2\n1\nBeihang University\n2\nAgiBot\nCorresponding author\nAbstract\nVision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimo",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "VLA",
        "VLM",
        "whole body control"
    ],
    "one_line_summary": "该论文提出了一种名为ACoT-VLA的新架构，通过动作链式思维范式，在动作空间中进行直接推理，以提升视觉-语言-动作模型在机器人操作任务中的性能。",
    "detailed_summary": "## 论文摘要：ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models\n\n**1. 研究背景和动机**\n当前，视觉-语言-动作（VLA）模型已成为机器人通用策略的主流。然而，现有方法通常依赖语言或视觉作为中间推理（如预测子任务或合成目标图像），这些抽象表示与精确动作执行所需的低层级运动信息之间存在**语义-运动鸿沟**。这导致模型难以生成精确、鲁棒的动作序列。本文认为，最有效的推理应直接在**动作空间**中进行，从而为策略学习提供更直接、更细粒度的指导。\n\n**2. 核心方法和技术创新**\n本文提出了**动作思维链（Action Chain-of-Thought, ACoT）** 这一新范式，并构建了**ACoT-VLA**框架来实现它。其核心创新在于引入了两个互补的组件：\n- **显式动作推理器（EAR）**：一个轻量级Transformer，根据多模态输入合成一个**粗略的参考动作轨迹**，提供显式的、运动学上合理的动作空间指导。\n- **隐式动作推理器（IAR）**：通过交叉注意力机制，从预训练VLM的内部表征中提取**隐式的动作先验**（如视觉中的可操作性、语言中的动作语义）。\n这两个模块生成的显式和隐式动作指导，通过一个**动作引导预测（AGP）头**进行融合，共同引导下游的扩散策略头生成最终的可执行动作序列。\n\n**3. 主要实验结果**\n在多个仿真和真实世界基准测试中，ACoT-VLA均取得了最先进的性能：\n- **LIBERO**：平均成功率高达**98.5%**，尤其在长时程任务（LIBERO-Long）上表现突出。\n- **LIBERO-Plus**（鲁棒性基准）：平均成功率达**84.1%**，在相机视角变化、传感器噪声等扰动下展现出卓越的泛化能力。\n- **VLABench**：综合得分为**47.4%**，在未见纹理等分布外泛化任务上优势明显。\n- **真实世界实验**：在擦污渍、倒水、开放集抓取等任务上，成功率显著超越基线模型（如π0.5），证明了其实际部署的有效性。\n\n**4. 研究意义和价值**\n本研究首次将思维链推理的概念引入到**动作空间**，为解决VLA模型中的语义-运动鸿沟问题提供了新思路。ACoT-VLA通过提供直接的运动学指导，实现了更**高效、更可靠**的策略学习。这项工作不仅显著提升了通用机器人策略的性能和鲁棒性，也为未来开发更具**可解释性、更贴近物理现实**的具身智能体开辟了新的研究方向。",
    "qa_pairs": [
        {
            "question": "这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？",
            "answer": "## 论文核心分析：ACoT-VLA\n\n### **一、 核心问题：语义-运动学鸿沟**\n现有通用机器人策略（Vision-Language-Action, VLA）存在一个根本性缺陷：**中间推理过程与最终动作执行之间存在“语义-运动学鸿沟”**。\n- **传统方法**：依赖**语言链式思维**（预测子任务）或**视觉链式思维**（合成目标图像）作为中间推理。这些推理形式是**间接的、抽象的**，无法为精确的低层级动作执行提供**细粒度、运动学上连贯**的指导。\n- **后果**：从丰富的语义/视觉表示到精确动作序列的映射效率低下，导致策略在长时程、需精细操控的任务中表现不佳，泛化能力受限。\n\n### **二、 核心创新：动作链式思维**\n论文提出了 **“动作链式思维”** 这一全新范式，旨在**直接在动作空间中进行推理**。\n- **核心理念**：将“思考”过程重新定义为**一系列明确的、运动学上接地的动作意图链**，而非抽象的语言或视觉子目标。\n- **核心优势**：为策略提供**直接的运动学线索**，弥合了高层意图与低层控制之间的鸿沟，使策略学习更高效、更接地气。\n\n### **三、 解决方案：ACoT-VLA 架构**\n为了具体实现 ACoT 范式，论文提出了 **ACoT-VLA** 模型，其核心是**两个互补的推理器**，共同构成动作空间的指导。\n\n#### **1. 显式动作推理器**\n- **功能**：根据多模态观测，**合成一个粗略的参考动作轨迹**。这提供了**显式的、可执行的动作空间指导**。\n- **实现**：一个轻量级 Transformer，通过流匹配学习动作轨迹的分布，生成去噪后的参考动作序列。\n- **类比**：类似于生成模型中的“自条件化”，将先验估计纳入生成过程以提高质量。\n\n#### **2. 隐式动作推理器**\n- **功能**：从预训练 VLM 的**内部表示中提取潜在的动作先验**。这些先验隐含在语言指令或视觉上下文中，定义了动作空间上的可行分布。\n- **实现**：使用可学习的查询向量，通过跨注意力机制与 VLM 各层的键值缓存进行交互，聚合得到**隐式的动作相关特征**。\n- **作用**：提供**行为层面的隐性启发**，例如对“抓取”、“伸手”等语义的潜在编码。\n\n#### **3. 动作引导预测头**\n- **整合机制**：将 EAR 生成的**显式动作嵌入**和 IAR 提取的**隐式动作特征**，通过**双路跨注意力**与当前噪声动作查询进行交互。\n- **融合**：使用自注意力块融合两种互补的指导信息，最终输入动作头预测出可执行的动作序列。\n- **训练技巧**：采用**教师强制稳定**策略，训练时使用真实轨迹计算显式指导，推理时切换为完全自条件模式，确保训练稳定。\n\n### **四、 技术价值与实际效果**\n\n#### **技术创新**\n1.  **范式创新**：首次提出在**动作空间**进行链式推理，是思维范式的根本转变。\n2.  **架构创新**：设计了 EAR 和 IAR 两个协同模块，分别从**轨迹层面**和**表示层面**提供动作指导，形成互补。\n3.  **方法创新**：将流匹配、自条件化、键值缓存交互等技术有机结合，构建了一个高效的动作条件策略学习框架。\n\n#### **实际价值与性能**\n- **卓越性能**：在多个仿真基准测试中达到**最先进水平**：\n    - **LIBERO**: `98.5%` 平均成功率\n    - **LIBERO-Plus**: `84.1%` （在7种扰动下展现极强鲁棒性）\n    - **VLABench**: `47.4%` 综合得分\n- **关键提升**：在**长时程任务**和**存在分布偏移**的场景下（如视角变化、初始状态扰动、传感器噪声），性能提升尤为显著。这证明了动作空间指导能有效减少误差累积，提升泛化能力。\n- **现实验证**：在真实机器人平台上部署，在接触式操作、精细操作等任务上，成功率显著高于基线模型，证明了其现实可行性。\n\n### **五、 总结**\n**ACoT-VLA 的核心贡献在于，它通过让机器人策略“用动作来思考”，将推理过程与执行过程对齐，从而解决了传统VLA模型中语义理解与物理动作之间的脱节问题。** 其实验结果充分证明了这种**动作中心化**的推理范式在提升策略性能、鲁棒性和泛化能力方面的巨大潜力，为下一代通用机器人策略的发展开辟了新方向。",
            "timestamp": "2026-01-20T18:03:55.230489",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "基于他的前作，梳理这个方向的整个发展脉络，每一步相比于之前的工作都改进了什么，着重于几个不同的发展方向。",
            "answer": "## 论文分析：ACoT-VLA 及其技术脉络梳理\n\n本文提出的 **ACoT-VLA** 是 **Vision-Language-Action (VLA)** 模型领域的一项创新工作。要理解其贡献，需要梳理该领域从基础VLA模型到引入中间推理，再到本文提出的“动作思维链”的演进脉络。发展主线可以概括为：**从直接的“感知-动作”映射，到引入语言或视觉的中间推理，最终演进到直接在动作空间进行推理。**\n\n以下是基于本文及相关引用文献梳理出的几个关键发展方向及改进步骤：\n\n---\n\n### 1. 基础VLA模型：建立“感知-动作”的直接映射\n*   **代表工作**：`RT-2`、`OpenVLA`、`Octo`、`π₀`。\n*   **核心思路**：利用预训练的视觉-语言模型作为骨干网络，将视觉观察和语言指令编码为统一的特征表示，然后直接通过一个动作解码器（如扩散模型或自回归模型）预测机器人动作序列。\n*   **改进与局限**：\n    *   **改进**：首次将大规模预训练的视觉-语言知识用于机器人控制，实现了**跨任务的泛化能力**，摆脱了传统方法“一任务一模型”的局限。\n    *   **局限**：存在 **“语义-运动鸿沟”** 。VLM骨干网络在互联网数据上预训练，其表征偏向语义理解，而非物理动力学。直接将丰富的语义特征映射到低层、精确的动作序列非常困难，导致策略在长视野、需精细操作的任务上表现不佳。\n\n### 2. 发展方向一：引入语言链式思维 (Language CoT)\n*   **代表工作**：`ThinkAct`、`π₀.₅`、`TraceVLA`、`SpatialVLA`。\n*   **核心思路**：在生成最终动作之前，先利用大语言模型的推理能力，生成一系列**语言描述的子任务或步骤规划**（如“首先移动到物体上方，然后抓取…”）。这些语言中间结果作为额外条件，指导动作解码器。\n*   **改进与局限**：\n    *   **改进**：引入了**显式的、高层的任务分解**，使模型决策过程更可解释，并利用LLM的常识和逻辑推理能力，提升了在复杂任务上的规划能力。\n    *   **局限**：**推理与执行脱节**。语言描述是抽象的、离散的，无法提供执行动作所需的**连续、低层、具身的运动学信息**。“抓取”这个指令无法告诉机械臂具体的关节轨迹。\n\n### 3. 发展方向二：引入视觉链式思维 (Visual CoT / 世界模型)\n*   **代表工作**：`CoT-VLA`、`WorldVLA`、`DreamVLA`、`UniVLA`。\n*   **核心思路**：利用**世界模型**来预测未来的视觉状态（即合成子目标图像），或者生成包含动态、深度等信息的丰富视觉表征。这些预测的未来视觉场景作为“视觉思维”，指导动作生成。\n*   **改进与局限**：\n    *   **改进**：提供了**更接近感知空间的、连续的中间指导**。通过预测“下一步世界应该是什么样子”，使动作生成更具**目标导向性**和物理一致性。\n    *   **局限**：**指导仍是间接的**。模型需要先将动作意图转化为视觉目标，再反向推导出实现该视觉目标的动作。这个转换过程存在信息损失，且视觉目标本身可能模糊（例如，同一个抓取姿态可能对应多个视觉外观）。\n\n### 4. 本文工作：提出动作链式思维 (Action CoT) —— 一个范式转换\n*   **代表工作**：**`ACoT-VLA`**。\n*   **核心思路**：认为最有效的推理应**直接在动作空间中进行**。提出 **“动作思维链”** 范式，将推理过程本身构建为一系列**粗糙的动作意图序列**，以此指导最终策略。\n*   **具体实现与改进**：\n    *   **显式动作推理器**：一个轻量级Transformer，根据多模态输入**合成一个粗糙的参考动作轨迹**。这提供了**直接、可执行的运动学指导**，类似于人类演示的轨迹先验。\n    *   **隐式动作推理器**：从VLM骨干网络的内部表征中，通过可学习查询和交叉注意力，**提取潜在的动作先验**（如“伸手”、“抓握”等语义隐含的动作分布）。\n    *   **协同融合**：将上述显式（运动学）和隐式（语义/分布）的动作空间指导，通过动作引导预测头进行融合，共同条件化最终的动作去噪过程。\n*   **根本性改进**：\n    1.  **弥合语义-运动鸿沟**：直接在动作空间推理，提供了**同质的、运动学相干的指导**，避免了从语言/视觉到动作的异构转换。\n    2.  **提供细粒度、结构化指导**：相比抽象的语言步骤或模糊的视觉目标，粗糙动作轨迹提供了**更结构化、更精细的执行层面信息**，特别有利于**长视野任务**的误差控制（如表1中LIBERO-Long的显著提升）。\n    3.  **提升鲁棒性**：在存在视角变化、传感器噪声等分布外扰动时（LIBERO-Plus基准），动作空间指导比语言或视觉指导更稳定，因为它更贴近控制本质，受感知域变化的直接影响更小。\n\n---\n\n### 总结：发展脉络与核心贡献\n\n| 发展阶段 | 推理空间 | 核心改进 | 遗留问题 |\n| :--- | :--- | :--- | :--- |\n| **基础VLA** | 无显式推理 | 利用VLM实现跨任务泛化 | “语义-运动鸿沟”，执行不精确 |\n| **Language CoT** | 语言空间 | 引入高层任务分解与逻辑推理 | 抽象指令，缺乏运动学细节 |\n| **Visual CoT** | 视觉空间 | 提供连续、目标导向的视觉预测 | 间接指导，视觉到动作转换有损 |\n| **Action CoT (本文)** | **动作空间** | **提供直接、运动学相干、细粒度的动作级指导** | 引入额外计算开销；动作表征仍为底层控制指令，缺乏高层几何结构 |\n\n**ACoT-VLA的核心贡献在于完成了一次范式转换**：将机器人策略的“思考”过程从**感知空间**（语言/视觉）转移到**执行空间**（动作）。它通过**显式动作推理器**和**隐式动作推理器**的双重机制，首次实现了在动作空间内进行结构化、多粒度的推理，从而为下游动作生成提供了最直接、信息最丰富的指导，显著提升了策略的精确性、鲁棒性和长视野任务性能。这项工作为构建更接地气、更具物理常识的通用机器人策略开辟了一条新路径。",
            "timestamp": "2026-01-20T18:04:45.891715",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他的前作有哪些？使用表格仔细讲讲他的每篇前作，他和前作的区别是什么，主要改善是什么？着重于具体相比于之前文章的改动",
            "answer": "## 论文前作分析\n\n根据论文内容，作者团队（特别是通讯作者 Si Liu 和 Maoqing Yao）及其合作者在 **Vision-Language-Action (VLA)** 模型领域有一系列相关的前期工作。这些工作为本文提出的 **ACoT-VLA** 奠定了基础。以下是主要相关前作的梳理、与本文的区别以及核心改进点。\n\n### 前作概览表格\n\n| 前作名称 (引用) | 核心范式 / 指导信号 | 主要技术特点 | 与 **ACoT-VLA** 的核心区别 | **ACoT-VLA** 的主要改进 / 创新点 |\n| :--- | :--- | :--- | :--- | :--- |\n| **语言链式思考 (Language CoT)** <br> (e.g., ThinkAct [17], π₀.₅ [22], OpenVLA [25]) | **语言级指导 (Linguistic Guidance)** | 利用大语言模型(LLM)的推理能力，生成**子任务描述**或**高级步骤规划**作为中间推理，来指导动作生成。 | **推理空间不同**：在**抽象的语言/语义空间**中进行思考。指导信号是离散的文本token，与最终执行的连续动作空间存在“语义-运动鸿沟”。 | **1. 推理空间迁移**：将链式思考从**语言空间**迁移到**动作空间**。推理过程本身是结构化的**粗略动作意图序列**，提供了**运动学上连贯**的直接指导。<br>**2. 信息保真度**：动作空间的指导比语言描述包含更丰富、更细粒度的运动信息，能更直接地引导低层控制。 |\n| **视觉链式思考 (Visual CoT)** <br> (e.g., CoT-VLA [60], WorldVLA [8], DreamVLA [58]) | **视觉级指导 (Visual Guidance)** | 利用世界模型预测未来的**子目标图像**或视觉状态，作为中间推理来引导动作策略。 | **指导形式间接**：在**视觉/像素空间**中进行思考。生成的指导是图像，策略需要从图像中“解读”出所需的动作，这个过程是间接且有损的。 | **1. 指导同质性**：提出了**同质化指导**。ACoT的“思考”输出（粗略轨迹）与策略的最终输出（精细动作）**同属动作空间**，消除了跨模态翻译的误差。<br>**2. 减少歧义**：避免了从图像到动作的模糊映射问题，提供了更明确的行为约束。 |\n| **基线模型 π₀.₅ [22]** | **语言指导 (本文的Baseline)** | 一个先进的、结合网络规模语言理解与动作学习的VLA模型，代表了语言CoT范式的先进水平。 | 是本文进行性能对比和模块增量的**直接基线**。它仅使用语言指导，缺乏在动作空间内的显式推理。 | **1. 架构创新**：在π₀.₅的骨干网络上，新增了两个核心模块：**显式动作推理器(EAR)**和**隐式动作推理器(IAR)**，构成了完整的动作链式思考框架。<br>**2. 性能全面提升**：在LIBERO、LIBERO-Plus和VLABench三个基准测试上，平均成功率均显著超越π₀.₅（如LIBERO从96.9%提升至98.5%），尤其在长视野任务和抗扰动任务上优势明显。 |\n| **空间增强型VLA** <br> (e.g., SpatialVLA [41], 4D-VLA [55]) | **语言/空间指导** | 通过引入3D空间先验（如空间嵌入、时空信息）来增强VLA模型的空间推理能力。 | **关注点不同**：这些工作主要解决**感知层面**的空间理解问题，其输出仍然是语言或视觉指导，而非动作指导。 | **关注执行层面**：本文不直接增强空间感知，而是关注如何将已有的多模态感知**更有效地转化为动作**。ACoT是在感知编码**之后**、动作生成**之前**引入的推理层，专注于**动作空间的规划与引导**。 |\n\n---\n\n### 核心区别与主要改善的深入分析\n\n本文 **ACoT-VLA** 相对于前作的根本性突破在于 **“思考范式”的转变**：\n\n1.  **从“间接思考”到“直接思考”**：\n    *   **前作（语言/视觉CoT）**：策略的“思考”过程发生在与最终输出**异质**的空间（语言或图像）。这好比让机器人先“写一篇计划书”或“画一幅目标图”，再根据这些抽象信息去执行动作，存在信息损失和翻译误差。\n    *   **本文（Action CoT）**：策略的“思考”过程直接发生在**动作空间**。这好比让机器人先在大脑里“模拟演练”一遍粗略的动作流程，再基于这个流程去生成精确的执行指令。思考与执行使用“同一种语言”，效率更高，保真度更强。\n\n2.  **具体的技术实现改动与改善**：\n    *   **新增模块：双路径动作推理器**\n        *   **显式动作推理器 (EAR)**：这是一个轻量级Transformer，其功能是**直接合成一条粗略的参考动作轨迹**。这相当于生成了一个动作层面的“大纲”或“草图”，为后续精细动作生成提供了强烈的运动学归纳偏置。这是对“动作空间自条件”思想的具体实现。\n        *   **隐式动作推理器 (IAR)**：该模块通过可学习查询与VLM内部Key-Value缓存的交叉注意力，**提取隐式的动作先验**。VLM在预训练中已经学习了与动作相关的语义和功能线索（如“抓取”、“推动”），IAR将其蒸馏为紧凑的行为分布先验。\n    *   **改进训练与推理流程**：\n        *   **训练时**：采用“教师强制稳定”策略，使用真实轨迹来生成显式指导，避免早期不准确的预测干扰动作头的学习。\n        *   **推理时**：切换至完全自条件模式，由EAR自主生成参考轨迹，IAR提取隐式先验，二者共同通过**动作引导预测头(AGP)** 来条件化最终的动作去噪过程。\n    *   **解决的核心问题**：\n        *   **语义-运动鸿沟**：通过提供运动学上连贯的动作空间指导，直接桥接了高层语义意图与低层电机控制。\n        *   **长视野任务中的误差累积**：动作链式思考提供了结构化的、细粒度的中间监督，使策略在长序列生成中更加稳健（LIBERO-Long任务提升显著）。\n        *   **分布外泛化能力**：动作空间的指导比语言或视觉指导对视角变化、背景干扰等更具不变性，从而在LIBERO-Plus的严苛扰动测试中表现出卓越的鲁棒性。\n\n**总结**：ACoT-VLA并非在感知编码器或动作解码器本身进行巨大改动，而是在两者之间的**连接处**进行了一次范式创新。它通过引入**动作空间的双路径推理**，将链式思考从“说什么/看什么”转变为“**做什么**”，从而实现了更接地气、更精确、更鲁棒的通用机器人策略学习。其实验结果在多个基准上全面超越以π₀.₅为代表的语言CoT方法和以CoT-VLA为代表的视觉CoT方法，验证了这一范式转变的有效性。",
            "timestamp": "2026-01-20T18:05:42.133576",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文提出了哪些关键技术方法，请列表格具体详细说明技术细节，需要包含具体的数学原理推导，以及具体参数。",
            "answer": "## 论文关键技术方法分析\n\n本文提出的核心创新是 **Action Chain-of-Thought (ACoT)** 范式，并构建了 **ACoT-VLA** 框架来实现该范式。其关键技术方法由两个互补的模块和一个集成预测头构成，具体如下：\n\n### 关键技术方法详表\n\n| 模块名称 | 核心功能 | 技术细节与数学原理 | 关键参数与配置 |\n| :--- | :--- | :--- | :--- |\n| **1. 显式动作推理器 (Explicit Action Reasoner, EAR)** | 生成**显式的、粗粒度参考动作轨迹**，为最终策略提供明确的动作空间引导。 | **1. 问题建模：** <br> 将参考动作序列生成建模为**流匹配 (Flow Matching)** 问题。给定带噪声的动作序列 \\(\\tilde{a}_{t:t+H^{ref}-1}\\)，EAR 的目标是学习一个向量场，将其映射回真实的动作分布。 <br><br> **2. 网络架构与计算流程：** <br> - **输入编码：** 预训练的 VLM（如 SigLIP + Gemma 2B）将视觉观察 \\(o_t\\) 和语言指令 \\(l\\) 编码为上下文键值缓存 \\((K_{1:N}^{\\text{VLM}}, V_{1:N}^{\\text{VLM}})\\)。 <br> - **动作嵌入：** 带噪声的参考动作序列 \\(\\tilde{a}\\) 通过 MLP 投影为初始隐藏表示 \\(h_0^{\\text{ref}}\\)。 <br> - **Transformer 层处理（第 i 层）：** <br> \\[ \\begin{aligned} \\tilde{h}_i^{\\text{ref}} &= \\text{Self-Attn}(h_{i-1}^{\\text{ref}}) + \\text{CrossAttn}(h_{i-1}^{\\text{ref}}, K_i^{\\text{VLM}}, V_i^{\\text{VLM}}) \\\\ h_i^{\\text{ref}} &= h_{i-1}^{\\text{ref}} + \\text{FFN}(\\tilde{h}_i^{\\text{ref}}) \\end{aligned} \\] <br> 其中，自注意力捕获动作序列的**时间依赖性**，交叉注意力注入来自 VLM 的**多模态上下文先验**。 <br> - **输出：** 经过 \\(N\\) 层后，通过一个线性头输出去噪后的参考动作序列 \\(a_{t:t+H^{ref}-1}^{ref}\\)。 <br><br> **3. 训练目标：** <br> 采用流匹配的均方误差 (MSE) 损失： <br> \\[ \\mathcal{L}_{\\pi_{\\theta}^{\\text{ref}}} = \\mathbb{E}_{t, a_0, a_1} \\left[ \\| \\pi_{\\theta}^{\\text{ref}}(a_t) - (a_1 - a_0) \\|^2 \\right] \\] <br> 其中 \\(a_t = (1-t)a_0 + t a_1\\) 是真实轨迹 \\(a_0\\) 到 \\(a_1\\) 的线性插值，\\(t \\sim \\mathcal{U}(0,1)\\)。 | - **VLM 骨干：** SigLIP (视觉) + Gemma 2B (语言)，隐藏维度 \\(d=2048\\)，层数 \\(N=18\\)。 <br> - **EAR 架构：** 轻量级 Transformer，层数 \\(N=18\\)。 <br> - **参考动作视野：** \\(H^{ref} = 15\\)。 <br> - **动作偏移 (Action Shift)：** 2（即参考动作序列相对于专家演示跳过一帧）。 <br> - **训练技巧：** 采用**教师强制稳定**——训练时使用真实轨迹计算引导，推理时使用 EAR 自身预测。 |\n| **2. 隐式动作推理器 (Implicit Action Reasoner, IAR)** | 从 VLM 的内部表示（键值缓存）中提取**隐式的动作先验**，提供行为分布的潜在线索。 | **1. 核心思想：** <br> VLM 的键值缓存中编码了与动作相关的语义和视觉线索（如“抓取”意图、物体可操作性）。IAR 通过可学习的查询向量，以交叉注意力机制主动“询问”并提取这些信息。 <br><br> **2. 降维与特征提取：** <br> 为减少计算冗余，首先对 VLM 每层的键值对进行降维投影： <br> \\[ \\begin{aligned} Q_i' &= Q_i W_Q^{(i)}, \\quad W_Q^{(i)} \\in \\mathbb{R}^{d \\times d'} \\\\ K_i' &= K_i^{\\text{VLM}} W_K^{(i)}, \\quad W_K^{(i)} \\in \\mathbb{R}^{d \\times d'} \\\\ V_i' &= V_i^{\\text{VLM}} W_V^{(i)}, \\quad W_V^{(i)} \\in \\mathbb{R}^{d \\times d'} \\end{aligned} \\] <br> 其中 \\(d' \\ll d\\) 是降维后的维度。 <br><br> **3. 交叉注意力与聚合：** <br> 对每一层，使用可学习查询 \\(Q_i'\\) 与降维后的键值对进行交叉注意力： <br> \\[ \\text{Attention}(Q_i', K_i', V_i') = \\text{softmax}\\left(\\frac{Q_i' (K_i')^T}{\\sqrt{d'}}\\right) V_i' \\] <br> 将各层输出的特征进行平均池化，再通过一个 MLP 投影，得到该层的隐式动作特征 \\(z_i^{\\text{im}}\\)。 <br><br> **4. 输出：** 聚合所有层的特征，得到全局的隐式动作引导特征 \\(Z^{\\text{im}} = \\text{Aggregate}(z_1^{\\text{im}}, ..., z_N^{\\text{im}})\\)。 | - **可学习查询矩阵 \\(Q_i\\)：** 每层一个，形状为 \\(\\mathbb{R}^{M \\times d}\\)，其中 \\(M=1\\)（即单个查询向量）。 <br> - **降维维度：** \\(d' = 128\\)。 <br> - **投影矩阵：** \\(W_Q^{(i)}, W_K^{(i)}, W_V^{(i)}\\) 为可学习参数。 <br> - **聚合方式：** 平均池化 + MLP 投影。 |\n| **3. 动作引导预测头 (Action-Guided Prediction Head, AGP)** | **协同整合** EAR 提供的显式轨迹引导 (\\(Z^{\\text{ex}}\\)) 和 IAR 提供的隐式先验 (\\(Z^{\\text{im}}\\))，生成最终可执行的动作序列。 | **1. 查询构建：** <br> 将带噪声的目标动作序列 \\(\\tilde{a}_{t:t+H-1}\\) 编码为动作查询 \\(Q_{action}\\)。 <br><br> **2. 双重交叉注意力：** <br> 让动作查询分别与显式和隐式引导特征进行交互，检索互补信息： <br> \\[ \\begin{aligned} S^{\\text{ex}} &= \\text{CrossAttn}(Q_{action}, Z^{\\text{ex}}, Z^{\\text{ex}}) \\\\ S^{\\text{im}} &= \\text{CrossAttn}(Q_{action}, Z^{\\text{im}}, Z^{\\text{im}}) \\end{aligned} \\] <br><br> **3. 特征融合：** <br> 将两个注意力输出拼接，并通过一个自注意力融合块进行整合： <br> \\[ \\bar{h} = \\text{Self-Attn}([S^{\\text{ex}}; S^{\\text{im}}]) \\] <br> 这使得模型能动态权衡显式运动线索和隐式行为倾向。 <br><br> **4. 动作生成：** <br> 将融合后的表示 \\(\\bar{h}\\) 输入到最终的动作头 \\(\\pi_{\\theta}^{\\text{head}}\\)（通常是一个扩散模型或流匹配模型），通过去噪过程生成最终的动作序列 \\(a_{t:t+H-1}\\)。 <br><br> **5. 整体训练目标：** <br> \\[ \\mathcal{L}_{\\text{total}} = \\lambda_1 \\mathcal{L}_{\\pi_{\\theta}^{\\text{ref}}} + \\lambda_2 \\mathcal{L}_{\\pi_{\\theta}^{\\text{head}}} \\] <br> 其中 \\(\\mathcal{L}_{\\pi_{\\theta}^{\\text{head}}}\\) 是动作头自身的流匹配 MSE 损失。 | - **动作头 \\(\\pi_{\\theta}^{\\text{head}}\\)：** 基于扩散/流匹配的预测器，预测视野 \\(H=10\\)，动作偏移为 1。 <br> - **损失平衡因子：** \\(\\lambda_1 = \\lambda_2 = 0.5\\)。 <br> - **训练批次大小：** 全局批次大小 128。 <br> - **优化器：** AdamW，峰值学习率 \\(5e-5\\)，采用余弦衰减。 |\n\n### 核心技术创新与价值总结\n\n1.  **范式创新 (Parad",
            "timestamp": "2026-01-20T18:06:45.865559",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他使用了哪些评价指标与数据集，列表格具体讲讲他的评价指标的细节与数据集的细节",
            "answer": "## 论文评价指标与数据集分析\n\n### 一、 评价指标\n\n论文在三个主要仿真基准测试中使用了不同的评价指标，具体如下：\n\n| 基准测试 | 主要评价指标 | 指标细节与说明 |\n| :--- | :--- | :--- |\n| **LIBERO** | **平均成功率 (Average Success Rate, SR)** | - **定义**：在多次独立试验中，机器人成功完成指定任务的比率。<br>- **计算方式**：每个任务执行50次试验，计算成功次数占比。四个任务套件（Spatial, Object, Goal, Long）分别计算，再取平均得到最终“Avg. SR”。<br>- **核心价值**：直接衡量策略在多样化、长视野任务中的整体执行能力，是机器人策略最核心的评估标准。 |\n| **LIBERO-Plus** | **平均成功率 (Average Success Rate, SR)** | - **定义**：在包含多种扰动（如视角、光照、噪声等）的大规模评估集上，策略的成功率。<br>- **计算方式**：在7个扰动维度构成的10,030个评估片段中，每个片段执行一次，计算总体成功率。<br>- **核心价值**：**评估策略的鲁棒性与泛化能力**。相较于LIBERO，它更关注模型在分布外（OOD）情况下的表现，是衡量策略实用性的关键指标。 |\n| **VLABench** | **意图得分 (Intention Score, IS)** 与 **进度得分 (Progress Score, PS)** | - **意图得分 (IS)**：评估机器人动作是否符合任务的高级语义意图。例如，在“倒水”任务中，是否做出了“倾倒”的动作。<br>- **进度得分 (PS)**：评估任务完成的物理进度。例如，水是否成功倒入了杯中，倒入了多少。<br>- **核心价值**：提供**更细粒度的评估**。IS关注“动作是否正确”，PS关注“效果是否达成”，两者结合能更全面地评估策略在复杂交互任务中的表现。 |\n\n**总结**：论文的评价指标体系从 **“基础性能”** (LIBERO) 到 **“鲁棒性”** (LIBERO-Plus) 再到 **“细粒度语义与物理理解”** (VLABench)，层层递进，全面验证了ACoT-VLA模型的有效性、泛化能力和实际应用潜力。\n\n### 二、 数据集\n\n论文在仿真和真实世界环境中使用了多个数据集进行训练和评估，详细信息如下：\n\n| 数据集类型 | 数据集名称 | 核心特点与细节 | 数据规模（论文/附录） | 用途 |\n| :--- | :--- | :--- | :--- | :--- |\n| **仿真数据集** | **LIBERO** | - **设计目标**：评估通用机器人策略的多种能力（空间感知、物体操作、目标完成、长视野推理）。<br>- **内容**：包含4个任务套件，共40个任务，每个任务提供50条人类遥操作演示。<br>- **控制频率**：10 Hz。 | 1,693 个片段，273,465 帧 | **主要性能基准**。用于训练和评估模型的基础能力。 |\n| | **LIBERO-Plus** | - **设计目标**：在LIBERO基础上，**系统性地评估策略在受控分布偏移下的鲁棒性**。<br>- **内容**：引入了7个扰动维度（摄像机视角、机器人初始状态、语言指令变体、光照条件、背景纹理、传感器噪声、物体布局）。<br>- **控制频率**：20 Hz。 | 14,347 个片段，2,238,036 帧 | **鲁棒性基准**。专门用于测试模型在面对环境变化时的泛化能力。 |\n| | **VLABench** | - **设计目标**：大规模评估VLA/VLM模型在多样化桌面操作任务上的表现。<br>- **内容**：基于ManiSkill3，包含接触式交互、铰接物体操作等复杂场景，组织成5个评估轨道（分布内、跨类别、常识推理等）。<br>- **控制频率**：10 Hz。 | 4,713 个片段，528,398 帧 | **综合能力与泛化基准**。评估模型在更广泛任务类别和语义指令下的表现。 |\n| **真实世界数据集** | **Wipe Stain** | - **任务**：拿起海绵擦除污渍。<br>- **特点**：涉及密集的工具-表面接触和精细的力控制。 | 177 个片段，356,316 帧 | **真实世界验证**。评估接触式操作能力。 |\n| | **Pour Water** | - **任务**：拿起水壶向杯中倒水并放回。<br>- **特点**：长视野、多阶段、需要精确的位姿和力度控制。 | 1,821 个片段，5,062,506 帧 | **真实世界验证**。评估精细物体操作和长序列任务执行能力。 |\n| | **Open-set Pick** | - **任务**：根据自然语言指令拾取指定物体。<br>- **特点**：开集识别，指令多样化，跨平台（AgiBot G1和AgileX）。 | AgiBot: 1,936片段, 219,824帧<br>AgileX: 962片段, 251,283帧 | **真实世界验证**。评估语言指令跟随和跨 embodiment 适应能力。 |\n\n**总结**：论文的数据集选择极具代表性：\n1.  **仿真数据集**构成了一个从**标准性能**到**极端鲁棒性**再到**综合语义理解**的完整评估金字塔。\n2.  **真实世界数据集**覆盖了**接触操作**、**精细操作**和**指令跟随**等关键场景，并进行了**跨平台验证**，强有力地证明了ACoT-VLA方法的实际部署价值。\n3.  所有实验均严格使用各基准提供的**官方训练集**，确保了对比的公平性和结果的可复现性。",
            "timestamp": "2026-01-20T18:07:29.146996",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文在哪些数据集上进行了实验？主要的评估指标和性能提升是多少？",
            "answer": "## 论文实验数据集、评估指标与性能提升分析\n\n### 一、实验数据集\n论文在**三个仿真基准数据集**和一个**真实世界数据集**上进行了全面实验：\n\n| 数据集类型 | 数据集名称 | 主要特点与用途 |\n| :--- | :--- | :--- |\n| **仿真数据集** | **LIBERO** | 广泛使用的通用机器人策略评估基准，包含4个子任务套件（Spatial, Object, Goal, Long），用于评估空间感知、物体操作、目标完成和长时程推理能力。 |\n| | **LIBERO-Plus** | LIBERO的扩展版本，专注于**鲁棒性评估**，引入了7种扰动维度（如相机视角、机器人初始状态、语言变化、光照、背景、传感器噪声、物体布局），用于测试策略在分布偏移下的泛化能力。 |\n| | **VLABench** | 大规模评估套件，基于ManiSkill3构建，包含5个评估轨道（In-distribution, Cross-category, Commonsense, Semantic-instruction, Unseen-texture），用于评估标准性能、类别泛化、常识推理、指令理解和外观变化鲁棒性。 |\n| **真实世界数据集** | **自定义任务集** | 在AgiBot G1和AgileX机器人平台上收集，包含三个任务：`Wipe Stain`（接触式操作）、`Pour Water`（精细物体操作）、`Open-set Pick`（开放集指令跟随），用于验证方法在真实物理环境中的有效性。 |\n\n### 二、主要评估指标\n1.  **成功率**：在LIBERO和LIBERO-Plus上，主要评估指标是**平均成功率**。每个任务进行多次独立试验，计算成功完成任务的比率。\n2.  **综合评分**：在VLABench上，使用两个指标：\n    *   **意图得分**：衡量动作序列与任务意图的对齐程度。\n    *   **进度得分**：衡量任务完成的进度。\n3.  **鲁棒性维度得分**：在LIBERO-Plus上，分别报告了策略在7种不同扰动维度下的成功率，以全面评估其鲁棒性。\n4.  **真实世界成功率**：在真实世界实验中，基于预定义的成功标准（如成功抓取、倒水、清洁污渍），计算任务的平均成功率。\n\n### 三、关键性能提升与结果\n论文提出的 **ACoT-VLA** 方法在多个基准上取得了**最先进的性能**。\n\n#### 1. LIBERO 基准\n*   **总体性能**：ACoT-VLA 取得了 **98.5%** 的平均成功率，超越了所有对比方法。\n*   **对比基线**：相较于之前性能最好的 `π0.5` 模型（96.9%），实现了 **+1.6%** 的绝对提升。\n*   **关键优势**：在最具挑战性的 **LIBERO-Long**（长时程任务）套件上表现尤为突出，取得了 **96.0%** 的成功率，显著优于 `π0.5` 的 92.4%。这表明**动作链式思维**能有效缓解长时程任务中的误差累积问题。\n\n#### 2. LIBERO-Plus 基准（鲁棒性测试）\n*   **总体性能**：ACoT-VLA 取得了 **84.1%** 的平均成功率，大幅领先于其他方法。\n*   **鲁棒性优势**：在多个具有挑战性的扰动维度上表现出极强的鲁棒性：\n    *   **相机视角变化**：91.2% （对比最佳语言引导方法 +11.6%）\n    *   **机器人初始状态扰动**：62.5% （对比最佳语言引导方法 +16.3%）\n    *   **传感器噪声**：88.3% （对比最佳语言引导方法 +12.5%）\n*   **结论**：这证明了**动作空间引导**能更有效地帮助策略应对感知输入的变化和不确定性。\n\n#### 3. VLABench 基准\n*   **总体性能**：在**意图得分**和**进度得分**上均达到最佳，平均意图得分为 **63.5%**，平均进度得分为 **47.4%**。\n*   **泛化能力**：在 **Unseen-texture**（未见纹理）轨道上提升最为显著，意图得分达到 **74.6%**，比 `π0.5` 高出 **+12.6%**，显示了出色的外观泛化能力。\n\n#### 4. 真实世界实验\n*   **总体性能**：在三个真实世界任务上的平均成功率达到 **66.7%**。\n*   **对比基线**：显著优于 `π0.5`（61.0%）和 `π0`（33.8%）。\n*   **跨平台验证**：在AgiBot G1和AgileX两个不同的机器人平台上均表现良好，证明了方法的**跨本体适应性**。\n\n### 总结\nACoT-VLA 通过引入**动作链式思维**这一新范式，在**标准性能**、**长时程任务鲁棒性**、**分布外泛化能力**以及**真实世界部署**等多个维度上均实现了显著提升。其核心价值在于将推理过程从抽象的语言/视觉空间转移到具身的动作空间，从而更直接、更有效地弥合了高层语义与底层控制之间的“语义-运动鸿沟”。",
            "timestamp": "2026-01-20T18:08:07.003490",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文的主要局限性有哪些？未来可能的改进方向是什么？",
            "answer": "## 论文主要局限性分析\n\n根据论文内容（特别是附录D），作者明确指出了**ACoT-VLA**框架存在的几个主要局限性：\n\n### 1. **计算开销增加**\n- **问题**：引入的推理模块（显式动作推理器EAR和隐式动作推理器IAR）增加了额外的计算成本。\n- **具体表现**：如表11所示，完整模型（EAR+IAR）的推理延迟从基线的91ms增加到112ms，参数从3.35B增至3.81B。\n- **影响**：虽然性能提升显著，但额外的计算负担可能对部署在**资源受限的机器人平台**（如嵌入式系统或移动机器人）构成挑战。\n\n### 2. **动作表示的抽象层次有限**\n- **问题**：当前动作表示主要采用**动作块**形式，即一系列低级控制命令（如关节角度、末端执行器位姿）。\n- **具体表现**：这种表示虽然能忠实编码执行的运动，但**缺乏显式的几何结构**。\n- **影响**：限制了模型进行**高层次空间推理**的能力，例如：\n    - 物体中心的协调操作\n    - 接触几何的精确建模\n    - 更复杂、需几何理解的任务规划\n\n### 3. **模块规模与性能的非单调性**\n- **问题**：EAR模块的参数量与最终策略性能之间存在**非单调关系**。\n- **具体表现**：如表10实验#7所示，当EAR参数量（500M）超过动作头（300M）时，性能反而下降。\n- **原因分析**：作者认为**过度参数化的EAR可能过拟合训练数据中的虚假关联**，生成有偏差的参考轨迹，从而误导动作头的预测。\n\n### 4. **对高质量演示数据的依赖**\n- **隐含问题**：虽然论文未明确强调，但ACoT-VLA（以及大多数VLA模型）的训练严重依赖**大规模、高质量的人类演示数据**。\n- **影响**：数据收集成本高昂，且可能限制模型在缺乏演示的新任务或极端环境下的泛化能力。\n\n---\n\n## 未来可能的改进方向\n\n基于上述局限性，论文提出了明确的未来研究方向，并暗示了其他潜在改进点：\n\n### 1. **开发更高效、轻量化的推理架构**\n- **目标**：在保持性能优势的同时，显著降低计算开销。\n- **可能途径**：\n    - 设计更紧凑的EAR和IAR模块（如通过知识蒸馏、模型剪枝）。\n    - 探索动态计算机制，仅在必要时激活复杂推理。\n    - 优化跨注意力等关键操作的实现效率。\n\n### 2. **设计几何增强的动作表示**\n- **目标**：将动作表示从低级的控制命令提升到**包含几何与语义信息的层次**。\n- **具体方向**：\n    - 引入**3D空间基元**（如轨迹点、接触面、力向量）作为动作推理的基本单元。\n    - 使ACoT能够在**几何可解释的3D空间**中进行推理，从而更好地处理需要空间理解的任务（如装配、避障）。\n    - 结合**场景图**或**物体中心表示**，实现更自然的物体交互规划。\n\n### 3. **探索更鲁棒与可扩展的训练范式**\n- **目标**：降低对海量高质量演示数据的依赖，提升在数据稀缺或分布外场景下的性能。\n- **可能途径**：\n    - 结合**强化学习**进行微调，利用环境反馈优化策略。\n    - 引入**世界模型**进行**模拟演练**，生成合成数据或进行离线规划。\n    - 研究**跨任务、跨机器人的迁移学习**方法，利用已有知识快速适应新场景。\n\n### 4. **增强推理过程的可解释性与可控性**\n- **目标**：使“动作思维链”的过程对人类更透明、更易干预。\n- **可能途径**：\n    - 可视化EAR生成的**参考轨迹**和IAR提取的**隐式先验**。\n    - 允许人类操作员提供**高层指引**（如关键航点）来修正或调整推理过程。\n    - 开发**分层推理框架**，在动作空间推理之上引入更高层的任务规划。\n\n### 5. **推动跨模态对齐的深度融合**\n- **目标**：进一步弥合视觉-语言表示与动作执行之间的语义-运动鸿沟。\n- **可能途径**：\n    - 在VLM预训练阶段就引入**动作感知**的优化目标。\n    - 探索**视频-动作**或**物理仿真-动作**的联合预训练，让基础模型更早地学习物理动态。\n\n---\n\n## 总结\n**ACoT-VLA**的核心创新在于将推理过程直接置于动作空间，显著提升了策略的精确性和鲁棒性。其当前局限性主要集中在**计算效率**、**表示抽象层次**和**数据依赖**上。未来的改进将围绕**架构轻量化**、**表示几何化**、**训练范式多样化**以及**系统可解释化**展开，旨在推动下一代通用机器人策略向更高效、更智能、更易部署的方向发展。",
            "timestamp": "2026-01-20T18:08:43.738598",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-16T16:17:06Z",
    "created_at": "2026-01-20T17:50:01.192271",
    "updated_at": "2026-01-20T17:50:01.192280",
    "recommend": 0
}