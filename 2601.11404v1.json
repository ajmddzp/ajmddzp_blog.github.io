{
  "id": "2601.11404v1",
  "title": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models",
  "authors": [
    "Linqing Zhong",
    "Yi Liu",
    "Yifei Wei",
    "Ziyu Xiong",
    "Maoqing Yao",
    "Si Liu",
    "Guanghui Ren"
  ],
  "abstract": "Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.",
  "url": "https://arxiv.org/abs/2601.11404v1",
  "html_url": "https://arxiv.org/html/2601.11404v1",
  "html_content": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models\nLinqing Zhong\n1,2\nYi Liu\n2\nYifei Wei\n1,2\nZiyu Xiong\n2\nMaoqing Yao\n2∗\nSi Liu\n1∗\nGuanghui Ren\n2\n1\nBeihang University\n2\nAgiBot\nCorresponding author\nAbstract\nVision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings.\nRecent advancements have introduced explicit intermediary reasoning—such as sub-task prediction (language) or goal image synthesis (vision)—to guide action generation.\nHowever, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution.\nInstead, we posit that the most effective form of reasoning is one that deliberates directly in the action space.\nWe introduce\nAction Chain-of-Thought (ACoT)\n, a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy.\nIn this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm.\nSpecifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR).\nThe former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning.\nExtensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves\n98.5\n%\n{98.5\\%}\n,\n84.1\n%\n{84.1\\%}\n, and\n47.4\n%\n{47.4\\%}\non LIBERO, LIBERO-Plus and VLABench, respectively.\n1\nIntroduction\nFigure 1\n:\nChain-of-Thought in different space. (a) Language CoT paradigm predicts sub-tasks as intermediate reasoning. (b) Visual CoT paradigm synthesizes a goal image to provide guidance for action policy. (c) Our proposed Action CoT directly operates in action space and provides homogeneous action guidance.\nTo overcome the generalization limits of task-specific robot policies\n[\n11\n,\n61\n,\n48\n]\n, recent work has converged on Vision-Language-Action (VLA) models\n[\n25\n,\n5\n,\n41\n,\n33\n]\n, which always leverage a pre-trained Vision-Language Model (VLM)\n[\n1\n,\n46\n,\n2\n]\nto encode visual and linguistic inputs into a latent representation that conditions an action decoder.\nRecent advancements seek to improve the mapping from the input space to the action space by introducing the intermediate reasoning step by language generation, leading to more generalized and precise action outputs\n[\n22\n,\n53\n]\n, as visualized in Fig.\n1\n(a).\nA parallel thrust leverages world models\n[\n16\n,\n50\n,\n65\n]\nto simulate environmental dynamics, directly enhancing the efficacy and goal-oriented nature of the generated action sequences\n[\n62\n,\n58\n]\n, as shown in Fig.\n1\n(b).\nDespite the promising trajectory set by these paradigms, a critical challenge persists: existing generalist policies think predominantly in the vision-language (input) space, often failing to adequately address the inherent disparity between these rich, semantic representations and the requirements of precise, low-level action execution (output).\nSpecifically, the knowledge encoded within the VLM backbone of VLA models is derived from pre-training on web-scale datasets focused on semantic alignment and question-answering, yielding representations optimized for linguistic understanding rather than physical dynamics.\nSimilarly, while world models forecast future visual states conditioned on inputs, their guidance remains tethered to naturally visual representations.\nCrucially, both semantic and visual forms of reasoning only offer suboptimal, indirect guidance for generating the necessary action sequence.\nConsequently, these prevailing approaches rely on an inherently constrained information conduit, struggling to convey the full, granular knowledge of the action space essential for truly grounded and accurate robotic policy learning.\nThe inherent semantic-kinematic gap in existing policies,\ni.e\n.\n, a fundamental disconnect between high-level, abstract inputs and low-level, executable motor commands, necessitates a paradigm shift in how guidance is provided.\nWe contend that to bridge this chasm, policies require guidance that is kinematically coherent, rather than purely semantic or visual.\nThis core principle underpins our novel framework:\nAction Chain-of-Thought (ACoT)\n(Fig.\n1\n(c)).\nWe redefine the “thought” process not as a sequence of linguistic tokens, but as a structured chain of explicit, kinematically-grounded action intents.\nThis approach furnishes the policy with direct motion cues, supplanting indirect representations.\nIn a manner analogous to learning from physical demonstration, this direct conditioning on action-space information enables a substantially more efficient and veridically grounded policy learning process.\nThis foundational shift, however, introduces a critical and distinct research challenge:\n“How can we robustly and efficiently synthesize the complex, high-dimensional motion cues required for ACoT reasoning from the raw, heterogeneous multimodal inputs?”\nAction-related information manifests in two complementary forms,\ni.e\n.\n, explicit or implicit.\nThe explicit form corresponds to observable motion trajectories, such as those in human demonstrations, which directly encode executable patterns of behavior.\nIn contrast, the implicit form resides in latent cues,\ne.g\n.\n, linguistic expressions like “reach out” or “grasp”, as well as interaction intents embedded in visual contexts.\nAlthough these cues are not presented as explicit robotic trajectories, they implicitly define distributions over feasible actions within the action space.\nBuilding upon this insight, we introduce two synergistic mechanisms to generate both explicit and implicit guidance in the action space.\nWe first propose the Explicit Action Reasoner (EAR), which is realized as a light-weight transformer. Particularly, EAR synthesizes coarse-grained motion trajectories conditioned on multimodal observations, offering direct and executable guidance within the action space.\nSecondly, we devise the Implicit Action Reasoner (IAR), which infers latent action priors through applying cross-attention modeling between downsampled multimodal representations and learnable queries, thereby providing implicit behavioral priors.\nNote that these two mechanisms are inherently complementary to each other.\nSubsequently, through jointly leveraging both EAR and IAR, we develop ACoT-VLA, an integrated Action Chain-of-Thought framework that enables grounded generalist robot policy learning.\nExtensive experiments across both real-world settings and three simulation benchmarks consistently demonstrate the effectiveness and versatility of our ACoT-VLA.\nTo summarize, our main contributions are as follows:\n•\nConceptually, we introduce Action Chain of Thought (ACoT), a new paradigm for generalist robot policies. To the best of our knowledge, this is the first work to formulate the deliberative process as a structured chain of explicit action-space intents, rather than abstract linguistic or visual sub-goals.\n•\nWe delve into essential action space guidance and propose the Explicit and Implicit Action Reasoners, which provide both explicit trajectory guidance and implicit behavioral inspiration for action prediction.\n•\nBuilding upon these two modules, we further propose ACoT-VLA, a unified framework for grounded generalist robot policy learning.\n•\nEmpirically, we validate our approach through extensive simulation and real-world experiments, achieving state-of-the-art performance on multiple benchmarks,\ni.e\n.\n, 98.5%, 84.1% and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.\n2\nRelated Works\nFigure 2\n:\nArchitectural Overview of ACoT-VLA.\nThe framework consists of three main components operating on features from a shared VLM backbone.\n(a) The Explicit Action Reasoner (EAR) is a Transformer-based module that synthesizes a coarse reference trajectory, providing explicit action-space guidance.\n(b) The Implicit Action Reasoner (IAR) employs a cross-attention mechanism with learnable queries to extract latent action priors from the VLM’s internal representations.\n(c) The Action-Guided Prediction (AGP) head synergistically integrates both explicit and implicit guidances via cross-attention to condition the final denoising process, producing the executable action sequence.\nVision-Language-Action Models.\nVLA models\n[\n14\n,\n18\n,\n20\n,\n27\n]\nincorporate pre-trained VLM models to predict language-driven robotic action sequences. Early works\n[\n67\n,\n25\n]\nformulate robot control as an autoregressive sequence generation problem, discretizing continuous actions into bins. Inspired by generative modeling\n[\n38\n,\n66\n,\n31\n]\n, increasing works\n[\n5\n,\n33\n,\n19\n]\nadopt diffusion-based action policies to synthesize smooth and high-quality action trajectories. Given that robotic manipulation inherently occurs in three-dimensional space, a line of studies\n[\n52\n,\n30\n,\n59\n]\nhave sought to enhance the spatial reasoning capability of VLA models by integrating 3D priors. For instance, SpatialVLA\n[\n41\n]\nintegrates spatial embeddings to endow model with 3D awareness, while 4D-VLA\n[\n55\n]\nincorporates both spatial and temporal information to enrich representations.\nBesides, due to the scarcity of large-scale real-world robot demonstrations, a series of efforts\n[\n37\n,\n57\n,\n12\n,\n6\n,\n23\n,\n9\n]\nfocus on data-centric solutions, constructing large-scale robotic datasets through simulation or real-world collection to scale up policy learning.\nMoreover, recent large-scale co-training approaches, such as\nπ\n0.5\n\\pi_{0.5}\n[\n22\n]\n, GenieReasoner\n[\n34\n]\nand Gemini Robotics\n[\n47\n]\n, demonstrate the potential of unifying web-scale language understanding with action learning, enhancing the policy’s generalization ability while retaining the reasoning capability of pre-trained foundation models.\nWorld-Model-based Policies.\nAdvances in world models have demonstrated remarkable capability in synthesizing high-fidelity images and temporally coherent videos. Building upon such progress, emerging researches\n[\n26\n,\n36\n,\n56\n,\n29\n]\nexploit their predictive dynamics to implicitly guide action generation.\nSpecifically, CoT-VLA\n[\n60\n]\nintroduces visual chain-of-thought reasoning by forecasting sub-goal images, explicitly integrating visual reasoning into action prediction. WorldVLA\n[\n8\n]\nemploys an autoregressive architecture that unifies perception and action generation within a single framework. DreamVLA\n[\n58\n]\nextends beyond visual prediction and enriches world modeling with dynamic, depth, and semantic cues, improving the model’s physical consistency.\nCollectively, existing world-model-based approaches adopt a knowledge-forecasting perspective, incorporating primarily visual guidance into action trajectories generation.\nIn contrast to previous works focusing on visual or linguistic intermediaries for robotic policy learning, our key insight lies in investigating guidance directly within the action space, which intrinsically mitigates the heterogeneity between perception and action, enabling the model to effectively learn action-relevant priors.\n3\nMethodology\nIn this section, we present a detailed investigation into how to generate effective action space guidance and integrate it into robotic policy learning.\nWe first define the robotic manipulation problem and formulate our proposed approach in Sec.\n3.1\n.\nThe core of our method lies in two distinct action reasoners introduced in Sec.\n3.2\nand Sec.\n3.3\n, which provide explicit and implicit guidance within the action space.\nWe conclude by illustrating the policy prediction strategy that effectively integrates this action guidance during policy learning (Sec.\n3.4\n).\n3.1\nProblem Formulation\nGiven a natural language instruction\nl\nl\nand current visual observation\no\nt\no_{t}\n, the generalist robot policy\nπ\nθ\n\\pi_{\\theta}\naims to predict action sequences\na\nt\n:\nt\n+\nH\n−\n1\na_{t:t+H-1}\nthat accomplishes the specified task. The process can be formally expressed as:\na\nt\n:\nt\n+\nH\n−\n1\n=\nπ\nθ\n​\n(\no\nt\n,\nl\n)\n,\na_{t:t+H-1}=\\pi_{\\theta}(o_{t},l),\n(1)\nwhere\nH\nH\nrepresents the action horizon. Numerous works introduce additional guidance signals\ng\ng\n, which encapsulates various forms of auxiliary information to enhance policy’s prediction ability.\nSpecifically, these guidance signals can be broadly categorized into two types: language-level guidance\ng\nlang\ng_{\\text{lang}}\nand vision-level guidance\ng\nvis\ng_{\\text{vis}}\n. The former is primarily adopted by VLM-based methods,\ne.g\n.\n, leveraging LLMs’ reasoning capabilities to predict sub-tasks, while the latter is always employed by world-model-based approaches, such as simulating future observations.\nSuch relationship can be formulated as:\nπ\nθ\n​\n(\na\nt\n:\nt\n+\nH\n−\n1\n,\ng\n∣\no\nt\n,\nl\n)\n=\nπ\nθ\n​\n(\na\nt\n:\nt\n+\nH\n−\n1\n∣\no\nt\n,\nl\n,\ng\n)\n​\nπ\nθ\n​\n(\ng\n∣\no\nt\n,\nl\n)\n,\n\\pi_{\\theta}(a_{t:t+H-1},g\\mid o_{t},l)=\\pi_{\\theta}(a_{t:t+H-1}\\mid o_{t},l,g)\\pi_{\\theta}(g\\mid o_{t},l),\n(2)\nwhere\ng\n∈\n{\ng\nlang\n,\ng\nvis\n}\ng\\in\\{g_{\\text{lang}},\\,g_{\\text{vis}}\\}\n. Conversely, we shift the focus toward the action domain itself and investigate cues operating directly in the action space, symbolized as\ng\naction\ng_{\\text{action}}\n. The above guidances are extended as\ng\n∈\n{\ng\nlang\n,\ng\nvis\n,\ng\naction\n}\ng\\in\\{g_{\\text{lang}},\\,g_{\\text{vis}},\\,g_{\\text{action}}\\}\n.\nSuch action guidance can intuitively be disentangled into explicit and implicit forms. The explicit guidance\ng\naction\nex\ng_{\\text{action}}^{\\text{ex}}\nprovides direct priors in the form of reference action sequences, whereas the implicit guidance\ng\naction\nim\ng_{\\text{action}}^{\\text{im}}\narises from contextual signals,\ne.g\n.\n, action distribution inherently implied in linguistics.\n3.2\nExplicit Action Reasoner\nTo incorporate explicit action trajectories into the thinking process of\nπ\nθ\n\\pi_{\\theta}\nto generate high-quality action predictions, we propose the Explicit Action Reasoner (EAR).\nWe design a mechanism that enables the model to autonomously synthesize reference action sequences as internal guidance for policy learning.\nAnalogously, this formulation can be viewed as an action-space transfer of self-conditioning in generative models\n[\n10\n,\n35\n]\n, where incorporating prior estimates into the generation process has been shown to markedly improve sample quality.\nBuilding upon this principle, we instantiate EAR as a light-weight transformer, as shown in Fig.\n2\n(a), generating kinematically plausible action reference as explicit action-space guidance\ng\naction\nex\ng_{\\text{action}}^{\\text{ex}}\nfor downstream action policy.\nFormally, given visual observation\no\nt\no_{t}\nand language instruction\nl\nl\n, a pre-trained VLM encodes them into a contextual key-value cache:\n(\nK\n1\n:\nN\nVLM\n,\nV\n1\n:\nN\nVLM\n)\n=\nVLM\n​\n(\no\nt\n,\nl\n)\n,\n(K^{\\text{VLM}}_{1:N},V^{\\text{VLM}}_{1:N})=\\text{VLM}(o_{t},l),\n(3)\nwhere\nN\nN\nrepresents the number of layers of VLM.\nSubsequently, the EAR, denoted as\nπ\nθ\nref\n\\pi_{\\theta}^{\\text{ref}}\n, takes a noisy action sequence\na\n~\nt\n:\nt\n+\nH\nr\n​\ne\n​\nf\n−\n1\n\\tilde{a}_{t:t+H^{ref}-1}\nas input, where\nH\nr\n​\ne\n​\nf\nH^{ref}\nindicates the horizon of reference actions. The sequence is first embedded into an initial hidden representation\nh\n0\nref\nh_{0}^{\\text{ref}}\n, which serves as the input to EAR’s transformer layers. At each transformer layer\ni\ni\n, we adopt self-attention, along with cross-attention with the contextual key-value cache from the corresponding VLM layer:\nh\n~\ni\nref\n=\nSelf-Attn\n​\n(\nh\ni\n−\n1\nref\n)\n+\nCrossAttn\n​\n(\nh\ni\n−\n1\nref\n,\nK\ni\nVLM\n,\nV\ni\nVLM\n)\n,\n\\tilde{h}_{i}^{\\text{ref}}=\\text{Self-Attn}(h_{i-1}^{\\text{ref}})+\\text{CrossAttn}(h_{i-1}^{\\text{ref}},K^{\\text{VLM}}_{i},V^{\\text{VLM}}_{i}),\n(4)\nwhere self-attention module captures temporal dependencies within the action sequence and cross-attention mechanism injects multimodal contextual priors from the VLM.\nThen, the intermediate representation\nh\n~\ni\nref\n\\tilde{h}_{i}^{\\text{ref}}\nis processed by a feed-forward network (FFN) in a residual-parallel manner, updating the\ni\ni\n-th EAR representation\nh\ni\nref\nh_{i}^{\\text{ref}}\n:\nh\ni\nref\n=\nh\ni\n−\n1\nref\n+\nFFN\n​\n(\nh\n~\ni\nref\n)\n.\nh_{i}^{\\text{ref}}=h_{i-1}^{\\text{ref}}+\\text{FFN}(\\tilde{h}_{i}^{\\text{ref}}).\n(5)\nThrough training via flow matching,\nπ\nθ\nref\n\\pi_{\\theta}^{\\text{ref}}\nlearns a distribution over action trajectories, producing a denoised action sequence:\na\nt\n:\nt\n+\nH\nr\n​\ne\n​\nf\n−\n1\nr\n​\ne\n​\nf\n=\nπ\nθ\nref\n​\n(\na\n~\nt\n:\nt\n+\nH\nr\n​\ne\n​\nf\n−\n1\n,\nK\n1\n:\nN\nVLM\n,\nV\n1\n:\nN\nVLM\n)\n.\na^{ref}_{t:t+H^{ref}-1}=\\pi^{\\text{ref}}_{\\theta}(\\tilde{a}_{t:t+H^{ref}-1},K^{\\text{VLM}}_{1:N},V^{\\text{VLM}}_{1:N}).\n(6)\nThe generated sequence is then encoded via a MLP projector to obtain action embedding\nZ\nex\nZ^{\\text{ex}}\n, which serves as explicit action-space guidance\ng\naction\nex\ng_{\\text{action}}^{\\text{ex}}\nfor action policy learning.\n3.3\nImplicit Action Reasoner\nBeyond the explicit action trajectories, the multimodal latent space of VLM also encodes implicit motion cues\n[\n40\n,\n13\n]\n,\ne.g\n.\n, visual affordances and action-related semantics. Effectively extracting these action-relevant representations potentially offers complementary guidance. To this end, we introduce an Implicit Action Reasoner (IAR), which directly operates on the VLM’s key–value cache.\nConcretely, as presented in Fig.\n2\n(b), for each VLM layer\ni\n∈\n[\n1\n,\nN\n]\ni\\in[1,N]\n, we initialize a learnable matrix\nQ\ni\n∈\nℝ\nM\n×\nd\nQ_{i}\\in\\mathbb{R}^{M\\times d}\n, where\nM\nM\nis a hyperparameter and\nd\nd\nrepresents VLM’s hidden dimension.\nConsidering the information redundancy within VLM’s key–value cache and computational efficiency, we first downsample the corresponding key–value pairs into a lower-dimensional space, which is formulated as:\nQ\ni\n′\n=\nQ\ni\n​\nW\nQ\n(\ni\n)\n,\nK\ni\n′\n=\nK\ni\nVLM\n​\nW\nK\n(\ni\n)\n,\nV\ni\n′\n=\nV\ni\nVLM\n​\nW\nV\n(\ni\n)\n,\nQ_{i}^{\\prime}=Q_{i}W_{Q}^{(i)},\\quad K_{i}^{\\prime}=K_{i}^{\\text{VLM}}W_{K}^{(i)},\\quad V_{i}^{\\prime}=V_{i}^{\\text{VLM}}W_{V}^{(i)},\n(7)\nwhere\nW\nQ\n(\ni\n)\n,\nW\nK\n(\ni\n)\n,\nW\nV\n(\ni\n)\n∈\nℝ\nd\n×\nd\n′\nW_{Q}^{(i)},W_{K}^{(i)},W_{V}^{(i)}\\in\\mathbb{R}^{d\\times d^{\\prime}}\nare learnable linear projectors and\nd\n′\n≪\nd\nd^{\\prime}\\ll d\n.\nLater, cross-attention is applied to extract action-relevant information from each\nK\ni\n′\nK_{i}^{\\prime}\nand\nV\ni\n′\nV_{i}^{\\prime}\n.\nThe resulting features are subsequently integrated via average pooling and transformed through a MLP projector, as visualized in Fig.\n2\n(b), producing compact representations that capture the implicit action semantics\nz\ni\nim\nz^{\\text{im}}_{i}\nembedded in VLM’s\ni\ni\n-th layer:\nz\ni\nim\n=\nMLP\n​\n(\nPool\n​\n(\nCrossAttn\n​\n(\nQ\ni\n′\n,\nK\ni\n′\n,\nV\ni\n′\n)\n)\n)\n.\nz^{\\text{im}}_{i}=\\text{MLP}(\\text{Pool}(\\text{CrossAttn}(Q_{i}^{\\prime},K_{i}^{\\prime},V_{i}^{\\prime}))).\n(8)\nThen, through aggregating these representations across layers, we obtain implicit action-related feature\nZ\nim\nZ^{\\text{im}}\n, which serves as implicit action-space guidance\ng\naction\nim\ng_{\\text{action}}^{\\text{im}}\n, complementing the explicit motion priors.\nMethods\nGuidance\nSpatial\nObject\nGoal\nLong\nAvg.\nSR\n↑\n\\uparrow\nRank\n↓\n\\downarrow\nSR\n↑\n\\uparrow\nRank\n↓\n\\downarrow\nSR\n↑\n\\uparrow\nRank\n↓\n\\downarrow\nSR\n↑\n\\uparrow\nRank\n↓\n\\downarrow\nSR\n↑\n\\uparrow\nRank\n↓\n\\downarrow\nDiffusion Policy\n[\n11\n]\n–\n78.3\n23\n92.5\n15\n68.3\n24\n50.5\n24\n72.4\n24\nOcto\n[\n48\n]\n–\n78.9\n22\n85.7\n23\n84.6\n17\n51.1\n23\n75.1\n22\nCoT-VLA\n[\n60\n]\nVisual\n87.5\n17\n91.6\n17\n87.6\n14\n69.0\n16\n81.1\n17\nWorldVLA\n[\n8\n]\n(256*256)\nVisual\n85.6\n19\n89.0\n20\n82.6\n19\n59.0\n19\n79.1\n18\nWorldVLA\n[\n8\n]\n(512*512)\nVisual\n87.6\n16\n96.2\n12\n83.4\n18\n60.0\n18\n81.8\n16\nDreamVLA\n[\n58\n]\nVisual\n97.5\n7\n94.0\n13\n89.5\n12\n89.5\n10\n92.6\n11\nUniVLA\n[\n49\n]\nVisual\n95.4\n11\n98.8\n2\n93.6\n9\n94.0\n4\n95.5\n8\nF1\n[\n36\n]\nVisual\n98.2\n4\n97.8\n8\n95.4\n8\n91.3\n8\n95.7\n7\nGE-Act\n[\n29\n]\nVisual\n98.2\n4\n97.6\n9\n95.8\n6\n94.4\n3\n96.5\n5\nTraceVLA\n[\n64\n]\nLinguistics\n84.6\n21\n85.2\n24\n75.1\n23\n54.1\n21\n74.8\n23\nOpenVLA\n[\n25\n]\nLinguistics\n84.7\n20\n88.4\n21\n79.2\n20\n53.7\n22\n76.5\n21\nUniAct\n[\n63\n]\nLinguistics\n77.0\n24\n87.0\n22\n77.0\n22\n70.0\n15\n77.8\n20\nSpatialVLA\n[\n41\n]\nLinguistics\n88.2\n15\n89.9\n19\n78.6\n21\n55.5\n20\n78.1\n19\nThinkAct\n[\n17\n]\nLinguistics\n88.3\n14\n91.4\n18\n87.1\n15\n70.9\n14\n84.4\n15\nπ\n0\n\\pi_{0}\n-FAST\n[\n39\n]\nLinguistics\n96.4\n10\n96.8\n11\n88.6\n13\n60.2\n17\n85.5\n14\nFPC-VLA\n[\n51\n]\nLinguistics\n87.0\n18\n92.0\n16\n86.2\n16\n82.2\n12\n86.9\n13\nSmolVLA\n[\n43\n]\nLinguistics\n93.0\n13\n94.0\n13\n91.0\n11\n77.0\n13\n88.8\n12\nGR00T-N1\n[\n4\n]\nLinguistics\n94.4\n12\n97.6\n9\n93.0\n10\n90.6\n9\n93.9\n10\nπ\n0\n\\pi_{0}\n[\n5\n]\nLinguistics\n96.8\n9\n98.8\n2\n95.8\n6\n85.2\n11\n94.1\n9\nDD-VLA\n[\n28\n]\nLinguistics\n97.2\n8\n98.6\n4\n97.4\n4\n92.0\n7\n96.3\n6\nMemoryVLA\n[\n42\n]\nLinguistics\n98.4\n3\n98.4\n5\n96.4\n5\n93.4\n5\n96.7\n4\nπ\n0.5\n\\pi_{0.5}\n[\n22\n]\nLinguistics\n98.8\n2\n98.2\n7\n98.0\n2\n92.4\n6\n96.9\n3\nOpenVLA-OFT\n[\n24\n]\nLinguistics\n97.6\n6\n98.4\n5\n97.9\n3\n94.5\n2\n97.1\n2\nOurs\nAction\n99.4\n1\n99.6\n1\n98.8\n1\n96.0\n1\n98.5\n1\nTable 1\n:\nComparison on the LIBERO benchmark. The best results are highlighted in\nbold\n. All metrics are average success rates (%).\n3.4\nAction-Guided Prediction\nBuilding upon the explicit action embedding\nZ\nex\nZ^{\\text{ex}}\nproduced by EAR and implicit action-related feature\nZ\nim\nZ^{\\text{im}}\nobtained in IAR, in this section, we introduce the Action-Guided Prediction (AGP) strategy to incorporate both action guidances into policy learning.\nAs illustrated in Fig.\n2\n(c), given a noisy action segment\na\n~\nt\n:\nt\n+\nH\n−\n1\n\\tilde{a}_{t:t+H-1}\n, we first encode it into noisy action embedding via a MLP projector. Particularly, unlike previous approaches that directly feed this embedding into action head\nπ\nθ\nhead\n\\pi_{\\theta}^{\\text{head}}\n, we treat it as action query, denoted as\nQ\na\n​\nc\n​\nt\n​\ni\n​\no\n​\nn\nQ_{action}\n, which interacts with both\nZ\nex\nZ^{\\text{ex}}\nand\nZ\nim\nZ^{\\text{im}}\nto retrieve complementary priors for conditional prediction.\nSpecifically, we perform dual cross-attention operations:\nS\nex\n=\nCrossAttn\n​\n(\nQ\na\n​\nc\n​\nt\n​\ni\n​\no\n​\nn\n,\nZ\nex\n,\nZ\nex\n)\n,\nS^{\\text{ex}}=\\text{CrossAttn}(Q_{action},Z^{\\text{ex}},Z^{\\text{ex}}),\n(9)\nS\nim\n=\nCrossAttn\n​\n(\nQ\na\n​\nc\n​\nt\n​\ni\n​\no\n​\nn\n,\nZ\nim\n,\nZ\nim\n)\n,\nS^{\\text{im}}=\\text{CrossAttn}(Q_{action},Z^{\\text{im}},Z^{\\text{im}}),\n(10)\nwhere\nS\nex\nS^{\\text{ex}}\nand\nS\nim\nS^{\\text{im}}\ndenote the attended representations guided by explicit and implicit priors, respectively.\nNote that although both encode action-relevant information, they may highlight different facets of the underlying motion. For instance, explicit priors provide kinematic cues, whereas implicit priors capture latent action tendencies.\nHence, to effectively combine these complementary guidance, we concatenate the two attended features and process them through self-attention fusion block, which integrates the priors into a unified representation\nh\n¯\n\\bar{h}\n:\nh\n¯\n=\nSelf-Attn\n​\n(\n[\nS\nex\n;\nS\nim\n]\n)\n.\n\\bar{h}=\\text{Self-Attn}([S^{\\text{ex}};\\,S^{\\text{im}}]).\n(11)\nEventually, the aggregated representation\nh\n¯\n\\bar{h}\nis fed into\nπ\nθ\nhead\n\\pi_{\\theta}^{\\text{head}}\n, which predicts the denoised action sequence\na\nt\n:\nt\n+\nH\n−\n1\na_{t:t+H-1}\n.\nTraining Objectives.\nThe entire framework is optimized under a standard flow-matching mean-squared error (MSE) objective. The training losses consist of two parts,\ni.e\n.\n, flow-matching MSE for both Explicit Action Reasoner\nπ\nθ\nref\n\\pi_{\\theta}^{\\text{ref}}\nand action head\nπ\nθ\nhead\n\\pi_{\\theta}^{\\text{head}}\n, denoted as\nℒ\nπ\nθ\nref\n\\mathcal{L}_{\\pi_{\\theta}^{\\text{ref}}}\nand\nℒ\nπ\nθ\nhead\n\\mathcal{L}_{\\pi_{\\theta}^{\\text{head}}}\n, respectively. Hence, the overall objective is:\nℒ\ntotal\n=\nλ\n1\n​\nℒ\nπ\nθ\nref\n+\nλ\n2\n​\nℒ\nπ\nθ\nhead\n,\n\\mathcal{L}_{\\text{total}}=\\lambda_{1}\\mathcal{L}_{\\pi_{\\theta}^{\\text{ref}}}+\\lambda_{2}\\mathcal{L}_{\\pi_{\\theta}^{\\text{head}}},\n(12)\nwhere\nλ\n1\n\\lambda_{1}\nand\nλ\n2\n\\lambda_{2}\nare balance factors.\nTeacher Forcing Stabilization.\nDuring training, the outputs of\nπ\nθ\nref\n\\pi_{\\theta}^{\\text{ref}}\ncan be unstable.\nTo stabilize optimization, we compute\nZ\nex\nZ^{\\text{ex}}\ndirectly from ground-truth reference trajectories instead of from\nπ\nθ\nref\n\\pi_{\\theta}^{\\text{ref}}\npredictions, preventing optimization interference to\nπ\nθ\nhead\n\\pi_{\\theta}^{\\text{head}}\n.\nDuring inference, the model switches to a fully self-conditioned mode,\nwhere\nπ\nθ\nref\n\\pi_{\\theta}^{\\text{ref}}\nautonomously generates the reference actions to guide\nπ\nθ\nhead\n\\pi_{\\theta}^{\\text{head}}\nin action prediction.\nMethods\nGuidance\nCamera\nRobot\nLanguage\nLight\nBackground\nNoise\nLayout\nAvg.\nWorldVLA\n[\n8\n]\nVisual\n0.1\n27.9\n41.6\n43.7\n17.1\n10.9\n38.0\n25.0\nOpenVLA\n[\n25\n]\nLinguistics\n0.8\n3.5\n23.0\n8.1\n34.8\n15.2\n28.5\n15.6\nNORA\n[\n21\n]\nLinguistics\n2.2\n37.0\n65.1\n45.7\n58.6\n12.8\n62.1\n39.0\nUniVLA\n[\n7\n]\nLinguistics\n1.8\n46.2\n69.6\n69.0\n81.0\n21.2\n31.9\n42.9\nπ\n0\n\\pi_{0}\n-Fast\n[\n39\n]\nLinguistics\n65.1\n21.6\n61.0\n73.2\n73.2\n74.4\n68.8\n61.6\nRIPT-VLA\n[\n44\n]\nLinguistics\n55.2\n31.2\n77.6\n88.4\n91.6\n73.5\n74.2\n68.4\nOpenVLA-OFT\n[\n24\n]\nLinguistics\n56.4\n31.9\n79.5\n88.7\n93.3\n75.8\n74.2\n69.6\nOpenVLA-OFT+\n[\n15\n]\nLinguistics\n92.8\n30.3\n85.8\n94.9\n93.9\n89.3\n77.6\n79.6\nπ\n0\n∗\n\\pi_{0}^{*}\n[\n5\n]\nLinguistics\n79.6\n21.1\n72.5\n84.7\n86.2\n68.3\n69.4\n67.4\nπ\n0.5\n∗\n\\pi_{0.5}^{*}\n[\n22\n]\nLinguistics\n70.3\n41.7\n81.1\n97.3\n94.6\n71.8\n84.9\n75.7\nOurs\nAction\n91.2\n62.5\n80.3\n95.1\n91.5\n88.3\n84.9\n84.1\nTable 2\n:\nPerformance comparison on the LIBERO-Plus benchmark. Best results are highlighted in bold. An asterisk (*) denotes results reproduced by us for fair comparison.\n4\nExperiments\nIn this section, we first outline the experimental setup in Sec.\n4.1\n. Then, in Sec.\n4.2\n, we evaluate our approach on three simulation benchmarks, followed by comprehensive ablation studies in Sec.\n4.3\n. Moreover, we present real-world deployment results in Sec.\n4.4\nto evaluate real-world applicability.\n4.1\nExperimental Setup\nData Sources.\nFor simulation experiments, we strictly follow the official training splits provided by the corresponding benchmark (LIBERO\n[\n32\n]\n, LIBERO-Plus\n[\n15\n]\n, and VLABench\n[\n57\n]\n), and train our models exclusively on their standard demonstration datasets without introducing any additional data.\nFor the real-world setting, all demonstrations used for model training are collected on our own robotic platform. More details about data sources are introduced in Appendix\nA\n.\nImplementation Details.\nWe implement our approach upon\nπ\n0.5\n\\pi_{0.5}\n[\n22\n]\n. Specifically, we adopt SigLIP\n[\n54\n]\nas the visual encoder, while the LLM backbone is instantiated as Gemma 2B architecture\n[\n3\n]\nwith\nN\n=\n18\nN=18\nlayers and hidden size\nd\n=\n2048\nd=2048\n. For frame processing, each input frame is resized to\n224\n×\n224\n224\\times 224\nprior to the visual encoder. Regarding the EAR, we employ a compact Transformer-based design composed of\nN\n=\n18\nN=18\nlayers. Concerning the IAR, each learnable query matrix\nQ\ni\nQ_{i}\nis configured with a row dimension of\nM\n=\n1\nM=1\n. The reduced dimension in the downsampling strategy is set to\nd\n′\n=\n128\nd^{\\prime}=128\n.\nIn terms of model training, unless explicitly specified, the horizon of predicted reference actions\nH\nr\n​\ne\n​\nf\nH^{ref}\nand action policy output\nH\nH\nare fixed to\n15\n15\nand\n10\n10\n, with action shift set to\n2\n2\nand\n1\n1\n, respectively. To clarify, the action shift specifies the temporal interval relative to the expert demonstration. For instance, a shift of\n1\n1\nyields frame-aligned predictions, whereas a shift of\n2\n2\nskips one intermediate frame. We set the balance factors in training losses as\nλ\n1\n=\nλ\n2\n=\n0.5\n\\lambda_{1}=\\lambda_{2}=0.5\n.\nTraining Configuration.\nWe adopt a unified set of training hyperparameters across all experiments unless explicitly specified. Concretely, the learning rate follows a cosine-decay schedule with a warm-up phase of\n10\n​\nK\n10\\text{K}\nsteps, a peak learning rate of\n5\n​\ne\n−\n5\n5\\mathrm{e}{-5}\n, and a decay toward\n5\n​\ne\n−\n5\n5\\mathrm{e}{-5}\nover\n10\n​\nK\n10\\text{K}\nsteps. Optimization is performed with AdamW with gradient-norm clipping set to\n1.0\n1.0\n. An exponential moving average (EMA) of model parameters is maintained with a decay rate of\n0.999\n0.999\n.\nRegarding hardware settings, model training is performed on a single node equipped with\n8\n8\nNVIDIA H100 GPUs using bfloat16 precision. And the inference is conducted on a single NVIDIA RTX 4090.\n4.2\nSimulation Experiments\nIn this section, we conduct the simulation evaluations across three benchmarks,\ni.e\n.\n, LIBERO\n[\n32\n]\n, LIBERO-Plus\n[\n15\n]\n, and VLABench\n[\n57\n]\n, to comprehensively evaluate our approach’s performance and generalization capabilities under diverse task structures.\nLIBERO Benchmark.\nLIBERO is a widely adopted simulation benchmark for evaluating generalist robotic policies. It consists of four task suites,\ni.e\n.\n, LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long. It is designed to probe a different aspect of policy capability, including spatial awareness, object-centric manipulation, goal completion, and long-horizon reasoning. Each suite consists of\n10\n10\ntasks and provides\n50\n50\nhuman-teleoperated demonstrations per task for policy training.\nFollowing the official evaluation protocol, we evaluate our policy on all tasks within the benchmark. For each task, the policy is evaluated over\n50\n50\nindependent trials, resulting in\n500\n500\nrollouts in total.\nAs reported in Table\n1\n, the quantitative evaluation results demonstrate that our proposed approach outperforms existing methods across all tracks. Compared to the previous state-of-the-art method\nπ\n0.5\n\\pi_{0.5}\n, our approach achieves a\n1.6\n%\n1.6\\%\nabsolute improvement in average success rate, highlighting the clear advantages of incorporating action-space guidance.\nNotably, we observe a pronounced improvement on the LIBERO-Long suite, where tasks require long-horizon manipulation with strict error control. We attribute this advantage to the nature of our proposed ACoT. Particularly, unlike language- or vision-CoT, whose intermediate reasoning remains abstract or indirect with respect to action execution, our proposed ACoT naturally operates in precise representation. Through leveraging actions as intermediate reasoning tokens, the model feeds the following action head with structured and fine-grained guidance, which significantly enhances the robustness to error accumulation in long-horizon manipulation tasks.\nMethods\nGuidance\nIn-dist.\nCategory\nCommonsense\nInstruction\nTexture\nAvg.\nIS\n↑\n\\uparrow\nPS\n↑\n\\uparrow\nIS\n↑\n\\uparrow\nPS\n↑\n\\uparrow\nIS\n↑\n\\uparrow\nPS\n↑\n\\uparrow\nIS\n↑\n\\uparrow\nPS\n↑\n\\uparrow\nIS\n↑\n\\uparrow\nPS\n↑\n\\uparrow\nIS\n↑\n\\uparrow\nPS\n↑\n\\uparrow\nπ\n0\n\\pi_{0}\n[\n5\n]\nLinguistics\n67.8\n62.7\n44.0\n33.6\n54.9\n43.0\n58.0\n38.7\n50.6\n42.5\n55.0\n44.1\nπ\n0.5\n\\pi_{0.5}\n[\n22\n]\nLinguistics\n75.0\n60.8\n49.6\n35.3\n57.5\n41.6\n57.1\n30.3\n62.0\n47.4\n60.2\n43.1\nOurs\nAction\n79.8\n66.1\n54.1\n38.9\n52.3\n37.8\n56.8\n39.6\n74.6\n54.6\n63.5\n47.4\nTable 3:\nComparison on the VLABench benchmark. IS and PS represent Intention score and Progress score, respectively.\nName\nEAR\nIAR\nSpatial\nObject\nGoal\nLong\nAvg.\nBaseline\n98.8\n98.2\n98.0\n92.4\n96.9\n#1\n✓\n99.0\n99.4\n98.0\n96.6\n98.3\n#2\n✓\n99.2\n99.2\n98.2\n95.6\n98.1\n#3\n✓\n✓\n99.4\n99.6\n98.8\n96.0\n98.5\nTable 4\n:\nModule ablations. The performance is gradually improved with the continuous addition of proposed methods.\nLIBERO-Plus Benchmark.\nBuilt upon LIBERO, LIBERO-Plus is an extended robustness-oriented benchmark, designed to systematically evaluate generalist robotic policies under controlled distribution shifts. Concretely, LIBERO-Plus introduces\n7\n7\nperturbation dimensions,\ni.e\n.\n, camera-viewpoints, robot-initial-states, language-variations, lighting-conditions, background-textures, sensor-noise and object-layout, which aim to expose hidden failure modes under standard evaluations. Notably, LIBERO-Plus consists of\n10\n,\n030\n10,030\nevaluation episodes, providing statistically reliable evaluation.\nFollowing standard training configuration in LIBERO-Plus\n[\n15\n]\n, we train our policy on the provided training set for a total of\n100\n​\nK\n100\\text{K}\noptimization steps. In terms of evaluation, we adhere to the protocol established in the benchmark,\ni.e\n.\n, each episode is executed once without repeated rollouts. Note that the average success rate is computed over the entire evaluation set.\nAs shown in Table\n2\n, our method significantly boosts the policy’s performance, surpassing all previous methods by a huge margin. In particular, our approach demonstrates pronounced robustness under challenging perturbations such as camera-viewpoint shifts (\n+\n11.6\n%\n+11.6\\%\n), robot initial-state perturbations (\n+\n16.3\n%\n+16.3\\%\n), and sensor noise (\n+\n12.5\n%\n+12.5\\%\n), where existing language-guided or vision-guided policies exhibit significant degradation. These results highlight the effectiveness of our action-space guidance in improving generalization under diverse perturbation factors.\nVLABench Benchmark.\nVLABench is a large-scale evaluation suite aimed at benchmarking both VLAs and VLMs on diverse robotic tasks. Built on ManiSkill3\n[\n45\n]\n, its manipulation benchmark consists of various tabletop scenarios,\ne.g\n.\n, contact-rich interactions and articulated-object manipulation. The standard evaluation is organized into\n5\n5\npublic tracks,\ni.e\n.\n, in-distribution, cross-category, commonsense, semantic-instruction, and unseen-texture, which respectively assess standard in-distribution performance, category-level generalization, commonsense reasoning, instruction understanding, and robustness to appearance variations. Particularly, VLABench proposes Intention Score (IS) and Progress Score (PS) to evaluate robot policies.\nIn our context, we train\nπ\n0\n\\pi_{0}\n,\nπ\n0.5\n\\pi_{0.5}\n, along with our method in a unified training setup. The model training is performed on VLABench’s official training data, with a global batch size\n128\n128\n. All models are optimized for\n60\n​\nK\n60\\text{K}\nsteps. We present quantitative results in Table\n3\n. Overall, our method achieves the best performance across both IS and PS. Notably, under the unseen-texture track, it delivers substantial gains,\ni.e\n.\n,\n+\n12.6\n%\n{+12.6\\%}\nin IS and\n+\n7.2\n%\n{+7.2\\%}\nin PS, indicating strong robustness to distributional shifts. Together, these results further confirm the effectiveness of our proposed approach.\nName\nAction\nshift\nAction\nhorizon\nEqui.\nhorizon\nSpatial\nObject\nGoal\nLong\nAvg.\nBaseline\n1\n10\n10\n98.6\n99.0\n96.4\n92.2\n96.6\n1\n10\n10\n99.4\n99.4\n98.8\n95.0\n98.2\n2\n5\n10\n99.6\n99.6\n98.4\n94.4\n98.0\n1\n30\n30\n99.2\n99.2\n97.6\n95.6\n97.9\n+EAR\n2\n15\n30\n99.0\n99.4\n98.0\n96.6\n98.3\n2\n30\n60\n99.4\n99.0\n98.2\n95.0\n97.9\n3\n30\n90\n98.8\n99.4\n97.4\n96.2\n98.0\nTable 5\n:\nReference action parameter ablation. We observe that different reference-action configurations within EAR generally lead to performance improvements.\nMethods\nSpatial\nObject\nGoal\nLong\nAvg.\nBaseline\n98.8\n98.2\n98.0\n92.4\n96.9\nQuery\n98.8\n99.0\n97.2\n92.8\n97.0\nAttention Pooling\n99.4\n98.6\n98.2\n92.8\n97.3\nDownsample\n99.2\n99.2\n98.2\n95.6\n98.1\nTable 6\n:\nComparison of KV-cache interaction strategies in IAR.\n4.3\nAblation Study\nWe examine each component’s contribution via systematic ablation experiments on the LIBERO benchmark, which are shown in Table\n4\n, Table\n5\n, and Table\n6\n. Note that we adopt\nπ\n0.5\n\\pi_{0.5}\nas the “Baseline” method. More ablations in different benchmarks are in Appendix\nC\n.\nEAR.\nAs shown in Table\n4\n, compared with the baseline, the experiment “#1” introduces the Explicit Action Reasoner (EAR) module into policy learning, which lifts the average success rate from\n96.9\n%\n96.9\\%\nto\n98.3\n%\n98.3\\%\n, demonstrating that the explicit action-space guidance benefits the robotic action sequence prediction.\nA plausible explanation is that EAR introduces an intermediate reference action sequence, which injects strong inductive bias on the behavior and thereby reduces ambiguity in mapping from observations to actions.\nIAR.\nAnalogously, with the Implicit Action Reasoner (IAR) module added in “#2”, the average success rate increases from\n96.9\n%\n96.9\\%\nto\n98.1\n%\n98.1\\%\n. This gain suggests that exploiting the implicit action distribution encoded in vision–language representations can also provide effective guidance for policy learning.\nThis performance gain can be partly attributed to the fact that IAR distills action-related clues implicitly encoded within the vision–language backbone, which potentially reflects the distribution of feasible actions. Such priors encourages the policy to remain closer to coherent, task-consistent behavioral patterns.\nEAR + IAR.\nIn Table\n4\n, experiment “#3” incorporates both EAR and IAR, achieving the highest average success rate of\n98.5\n%\n98.5\\%\n. The consistent improvements demonstrate that explicit action guidance and implicit action cues extracted from VLM’s key-value cache are complementary, jointly providing stronger guidance for accurate action prediction.\nReference Action Configurations.\nTo further examine the effect of explicit action references in EAR, we investigate different settings of action shift and action horizon, as summarized in Table\n5\n. We observe various parameter combinations consistently bring improvements over the baseline, indicating that providing action cues is broadly beneficial for policy learning. Besides, we find that shorter horizons combined with moderate shifts tend to produce relatively stronger gains. These observations offer further insight into how explicit action guidance influence policy learning.\nFigure 3\n:\nVisualization of three manipulation tasks in real world.\nKV-cache Interaction Strategies.\nWe compare three strategies for extracting action-relevant cues from VLM’s key-value cache within IAR module, as presented in Table\n6\n. Concretely, “Query” method uses learnable queries to attend to VLM’s original key-value cache.\n“Attention Pooling” method forms a pooled query by averaging key-value cache and then applies cross-attention operation.\n“Downsample” method first downsamples VLM’s key-value cache and then aggregates them using learnable matrix.\nAs shown in Table\n6\n, all three variants outperform the baseline, indicating that extracting implicit action cues from VLM benefits policy learning. Notably, the “Downsample” strategy achieves the best performance, suggesting that VLM’s features may contain noisy information for action prediction. This also highlights the importance of designing appropriate interaction mechanisms to align vision-language and action.\n4.4\nReal-World Deployment\nTo further validate the effectiveness of our framework, we conduct extensive real-world experiments on the AgiBot G1 robot. We consider three manipulation tasks,\ni.e\n.\n, “Wipe Stain”, “Pour Water”, and “Open-set Pick”, which respectively assess contact-rich manipulation, fine-grained object handling, and instruction-following abilities.\nSpecifically, as visualized in Fig.\n3\n, the “Wipe Stain” task requires the robot to pick up a sponge from the table and wipe away the stain until the surface is clean. The “Pour Water” task requires the robot to grasp the kettle by its handle, locate the target cup, pour water into it without causing overflow, and finally return the kettle to the table in a stable manner. The “Open-set Pick” task instructs the robot to pick up the correct tabletop object according to given natural-language command. Additionally, to examine the cross-embodiment adaptability, we also perform the “Open-set Pick” task on the AgileX robotic platform.\nDetails about training and evaluation are provided in the Appendix\nB\n.\nAs shown in Fig.\n4\n, our approach achieves consistently higher average success rates than both\nπ\n0.5\n\\pi_{0.5}\nand\nπ\n0\n\\pi_{0}\n,\ni.e\n.\n,\n66.7\n%\n66.7\\%\nagainst\n61.0\n%\n61.0\\%\nand\n33.8\n%\n33.8\\%\n. These results demonstrate that the proposed framework maintains effectiveness under real-world sensing conditions. Moreover, the aligned improvements observed on both Agibot G1 and AgileX also indicate that our method exhibits adaptability across different robotic embodiments.\nFigure 4\n:\nEvaluation results of real-world experiments.\n5\nConclusion\nIn this work, we addressed the fundamental semantic-kinematic gap in modern robotic policies by proposing a new paradigm: Action Chain-of-Thought (ACoT). We argued that for physically grounded intelligence, deliberation should occur not in the abstract space of language or vision, but directly in the kinematically grounded space of actions. We materialized this concept in our ACoT-VLA framework, which leverages two synergistic modules,\ni.e\n.\n, an Explicit Action Reasoner (EAR) and an Implicit Action Reasoner (IAR), to generate and fuse both explicit trajectory plans and implicit behavioral priors. This action-centric guidance mechanism creates a direct, information-rich conduit between high-level intent and low-level motor control.\nOur extensive experiments across multiple simulation and real-world benchmarks demonstrate that this approach yields state-of-the-art performance, significantly improving both task success and robustness. By shifting the locus of reasoning from perception to action, our work not only provides a more effective and grounded method for robot policy learning but also opens a new avenue for research into more structured, interpretable, and capable embodied agents. We believe that learning to “think” in the language of actions is a critical step towards developing the next generation of generalist robots.\nReferences\n[1]\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat,\net al.\n(2023)\nGpt-4 technical report\n.\narXiv preprint arXiv:2303.08774\n.\nCited by:\n§1\n.\n[2]\nS. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang,\net al.\n(2025)\nQwen2. 5-vl technical report\n.\narXiv preprint arXiv:2502.13923\n.\nCited by:\n§1\n.\n[3]\nL. Beyer, A. Steiner, A. S. Pinto, A. Kolesnikov, X. Wang, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello,\net al.\n(2024)\nPaligemma: a versatile 3b vlm for transfer\n.\narXiv preprint arXiv:2407.07726\n.\nCited by:\n§4.1\n.\n[4]\nJ. Bjorck, F. Castañeda, N. Cherniadev, X. Da, R. Ding, L. Fan, Y. Fang, D. Fox, F. Hu, S. Huang,\net al.\n(2025)\nGr00t n1: an open foundation model for generalist humanoid robots\n.\narXiv preprint arXiv:2503.14734\n.\nCited by:\nTable 1\n.\n[5]\nK. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter,\net al.\nπ\n0\n\\pi_{0}\n: A vision-language-action flow model for general robot control. corr, abs/2410.24164, 2024. doi: 10.48550\n.\narXiv preprint ARXIV.2410.24164\n.\nCited by:\n§1\n,\n§2\n,\nTable 1\n,\nTable 2\n,\nTable 3\n.\n[6]\nQ. Bu, J. Cai, L. Chen, X. Cui, Y. Ding, S. Feng, S. Gao, X. He, X. Hu, X. Huang,\net al.\n(2025)\nAgibot world colosseo: a large-scale manipulation platform for scalable and intelligent embodied systems\n.\narXiv preprint arXiv:2503.06669\n.\nCited by:\n§2\n.\n[7]\nQ. Bu, Y. Yang, J. Cai, S. Gao, G. Ren, M. Yao, P. Luo, and H. Li\n(2025)\nUniVLA: learning to act anywhere with task-centric latent actions\n.\nExternal Links:\n2505.06111\n,\nLink\nCited by:\nTable 2\n.\n[8]\nJ. Cen, C. Yu, H. Yuan, Y. Jiang, S. Huang, J. Guo, X. Li, Y. Song, H. Luo, F. Wang,\net al.\n(2025)\nWorldVLA: towards autoregressive action world model\n.\narXiv preprint arXiv:2506.21539\n.\nCited by:\n§2\n,\nTable 1\n,\nTable 1\n,\nTable 2\n.\n[9]\nT. Chen, Z. Chen, B. Chen, Z. Cai, Y. Liu, Z. Li, Q. Liang, X. Lin, Y. Ge, Z. Gu,\net al.\n(2025)\nRobotwin 2.0: a scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation\n.\narXiv preprint arXiv:2506.18088\n.\nCited by:\n§2\n.\n[10]\nT. Chen, R. Zhang, and G. Hinton\n(2022)\nAnalog bits: generating discrete data using diffusion models with self-conditioning\n.\narXiv preprint arXiv:2208.04202\n.\nCited by:\n§3.2\n.\n[11]\nC. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song\n(2025)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nThe International Journal of Robotics Research\n44\n(\n10-11\n),\npp. 1684–1704\n.\nCited by:\n§1\n,\nTable 1\n.\n[12]\nS. Deng, M. Yan, S. Wei, H. Ma, Y. Yang, J. Chen, Z. Zhang, T. Yang, X. Zhang, W. Zhang,\net al.\n(2025)\nGraspvla: a grasping foundation model pre-trained on billion-scale synthetic action data\n.\narXiv preprint arXiv:2505.03233\n.\nCited by:\n§2\n.\n[13]\nD. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang,\net al.\n(2023)\nPalm-e: an embodied multimodal language model\n.\nCited by:\n§3.3\n.\n[14]\nJ. Duan, W. Yuan, W. Pumacay, Y. R. Wang, K. Ehsani, D. Fox, and R. Krishna\n(2024)\nManipulate-anything: automating real-world robots using vision-language models\n.\narXiv preprint arXiv:2406.18915\n.\nCited by:\n§2\n.\n[15]\nS. Fei, S. Wang, J. Shi, Z. Dai, J. Cai, P. Qian, L. Ji, X. He, S. Zhang, Z. Fei,\net al.\n(2025)\nLIBERO-plus: in-depth robustness analysis of vision-language-action models\n.\narXiv preprint arXiv:2510.13626\n.\nCited by:\nAppendix A\n,\nTable 2\n,\n§4.1\n,\n§4.2\n,\n§4.2\n.\n[16]\nD. Ha and J. Schmidhuber\n(2018)\nWorld models\n.\narXiv preprint arXiv:1803.10122\n2\n(\n3\n).\nCited by:\n§1\n.\n[17]\nC. Huang, Y. Wu, M. Chen, Y. F. Wang, and F. Yang\n(2025)\nThinkact: vision-language-action reasoning via reinforced visual latent planning\n.\narXiv preprint arXiv:2507.16815\n.\nCited by:\nTable 1\n.\n[18]\nS. Huang, H. Chang, Y. Liu, Y. Zhu, H. Dong, P. Gao, A. Boularias, and H. Li\n(2024)\nA3vlm: actionable articulation-aware vision language model\n.\narXiv preprint arXiv:2406.07549\n.\nCited by:\n§2\n.\n[19]\nW. Huang, C. Chen, H. Qi, C. Lv, Y. Du, and H. Yang\n(2025)\nMoTVLA: a vision-language-action model with unified fast-slow reasoning\n.\narXiv preprint arXiv:2510.18337\n.\nCited by:\n§2\n.\n[20]\nW. Huang, C. Wang, Y. Li, R. Zhang, and L. Fei-Fei\n(2024)\nRekep: spatio-temporal reasoning of relational keypoint constraints for robotic manipulation\n.\narXiv preprint arXiv:2409.01652\n.\nCited by:\n§2\n.\n[21]\nC. Hung, Q. Sun, P. Hong, A. Zadeh, C. Li, U. Tan, N. Majumder, S. Poria,\net al.\n(2025)\nNora: a small open-sourced generalist vision language action model for embodied tasks\n.\narXiv preprint arXiv:2504.19854\n.\nCited by:\nTable 2\n.\n[22]\nP. Intelligence, K. Black, N. Brown, J. Darpinian, K. Dhabalia, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai,\net al.\n(2025)\nπ\n0.5\n\\pi_{0.5}\n: A vision-language-action model with open-world generalization\n.\narXiv preprint arXiv:2504.16054\n.\nCited by:\nAppendix C\n,\n§1\n,\n§2\n,\nTable 1\n,\nTable 2\n,\n§4.1\n,\nTable 3\n.\n[23]\nT. Jiang, T. Yuan, Y. Liu, C. Lu, J. Cui, X. Liu, S. Cheng, J. Gao, H. Xu, and H. Zhao\n(2025)\nGalaxea open-world dataset and g0 dual-system vla model\n.\narXiv preprint arXiv:2509.00576\n.\nCited by:\n§2\n.\n[24]\nM. J. Kim, C. Finn, and P. Liang\n(2025)\nFine-tuning vision-language-action models: optimizing speed and success\n.\narXiv preprint arXiv:2502.19645\n.\nCited by:\nTable 1\n,\nTable 2\n.\n[25]\nM. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi,\net al.\n(2024)\nOpenvla: an open-source vision-language-action model\n.\narXiv preprint arXiv:2406.09246\n.\nCited by:\n§1\n,\n§2\n,\nTable 1\n,\nTable 2\n.\n[26]\nH. Li, P. Ding, R. Suo, Y. Wang, Z. Ge, D. Zang, K. Yu, M. Sun, H. Zhang, D. Wang,\net al.\n(2025)\nVla-rft: vision-language-action reinforcement fine-tuning with verified rewards in world simulators\n.\narXiv preprint arXiv:2510.00406\n.\nCited by:\n§2\n.\n[27]\nX. Li, M. Zhang, Y. Geng, H. Geng, Y. Long, Y. Shen, R. Zhang, J. Liu, and H. Dong\n(2024)\nManipllm: embodied multimodal large language model for object-centric robotic manipulation\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp. 18061–18070\n.\nCited by:\n§2\n.\n[28]\nZ. Liang, Y. Li, T. Yang, C. Wu, S. Mao, L. Pei, X. Yang, J. Pang, Y. Mu, and P. Luo\n(2025)\nDiscrete diffusion vla: bringing discrete diffusion to action decoding in vision-language-action policies\n.\narXiv preprint arXiv:2508.20072\n.\nCited by:\nTable 1\n.\n[29]\nY. Liao, P. Zhou, S. Huang, D. Yang, S. Chen, Y. Jiang, Y. Hu, J. Cai, S. Liu, J. Luo,\net al.\n(2025)\nGenie envisioner: a unified world foundation platform for robotic manipulation\n.\narXiv preprint arXiv:2508.05635\n.\nCited by:\n§2\n,\nTable 1\n.\n[30]\nT. Lin, G. Li, Y. Zhong, Y. Zou, Y. Du, J. Liu, E. Gu, and B. Zhao\n(2025)\nEvo-0: vision-language-action model with implicit spatial understanding\n.\narXiv preprint arXiv:2507.00416\n.\nCited by:\n§2\n.\n[31]\nY. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le\n(2022)\nFlow matching for generative modeling\n.\narXiv preprint arXiv:2210.02747\n.\nCited by:\n§2\n.\n[32]\nB. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone\n(2023)\nLibero: benchmarking knowledge transfer for lifelong robot learning\n.\nAdvances in Neural Information Processing Systems\n36\n,\npp. 44776–44791\n.\nCited by:\nAppendix A\n,\n§4.1\n,\n§4.2\n.\n[33]\nS. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su, and J. Zhu\n(2024)\nRdt-1b: a diffusion foundation model for bimanual manipulation\n.\narXiv preprint arXiv:2410.07864\n.\nCited by:\n§1\n,\n§2\n.\n[34]\nY. Liu, S. Wang, D. Wei, X. Cai, L. Zhong, J. Yang, G. Ren, J. Zhang, M. Yao, C. Li,\net al.\n(2025)\nUnified embodied vlm reasoning with robotic action via autoregressive discretized pre-training\n.\narXiv preprint arXiv:2512.24125\n.\nCited by:\n§2\n.\n[35]\nY. Liu, R. Gal, A. H. Bermano, B. Chen, and D. Cohen-Or\n(2022)\nSelf-conditioned generative adversarial networks for image editing\n.\narXiv preprint arXiv:2202.04040\n.\nCited by:\n§3.2\n.\n[36]\nQ. Lv, W. Kong, H. Li, J. Zeng, Z. Qiu, D. Qu, H. Song, Q. Chen, X. Deng, and J. Pang\n(2025)\nF1: a vision-language-action model bridging understanding and generation to actions\n.\narXiv preprint arXiv:2509.06951\n.\nCited by:\n§2\n,\nTable 1\n.\n[37]\nA. O’Neill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain,\net al.\n(2024)\nOpen x-embodiment: robotic learning datasets and rt-x models: open x-embodiment collaboration 0\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp. 6892–6903\n.\nCited by:\n§2\n.\n[38]\nW. Peebles and S. Xie\n(2023)\nScalable diffusion models with transformers\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp. 4195–4205\n.\nCited by:\n§2\n.\n[39]\nK. Pertsch, K. Stachowicz, B. Ichter, D. Driess, S. Nair, Q. Vuong, O. Mees, C. Finn, and S. Levine\n(2025)\nFast: efficient action tokenization for vision-language-action models\n.\narXiv preprint arXiv:2501.09747\n.\nCited by:\nTable 1\n,\nTable 2\n.\n[40]\nS. Qian, W. Chen, M. Bai, X. Zhou, Z. Tu, and L. E. Li\n(2024)\nAffordancellm: grounding affordance from vision language models\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp. 7587–7597\n.\nCited by:\n§3.3\n.\n[41]\nD. Qu, H. Song, Q. Chen, Y. Yao, X. Ye, Y. Ding, Z. Wang, J. Gu, B. Zhao, D. Wang,\net al.\n(2025)\nSpatialvla: exploring spatial representations for visual-language-action model\n.\narXiv preprint arXiv:2501.15830\n.\nCited by:\n§1\n,\n§2\n,\nTable 1\n.\n[42]\nH. Shi, B. Xie, Y. Liu, L. Sun, F. Liu, T. Wang, E. Zhou, H. Fan, X. Zhang, and G. Huang\n(2025)\nMemoryvla: perceptual-cognitive memory in vision-language-action models for robotic manipulation\n.\narXiv preprint arXiv:2508.19236\n.\nCited by:\nTable 1\n.\n[43]\nM. Shukor, D. Aubakirova, F. Capuano, P. Kooijmans, S. Palma, A. Zouitine, M. Aractingi, C. Pascal, M. Russi, A. Marafioti,\net al.\n(2025)\nSmolvla: a vision-language-action model for affordable and efficient robotics\n.\narXiv preprint arXiv:2506.01844\n.\nCited by:\nTable 1\n.\n[44]\nS. Tan, K. Dou, Y. Zhao, and P. Krähenbühl\n(2025)\nInteractive post-training for vision-language-action models\n.\narXiv preprint arXiv:2505.17016\n.\nCited by:\nTable 2\n.\n[45]\nS. Tao, F. Xiang, A. Shukla, Y. Qin, X. Hinrichsen, X. Yuan, C. Bao, X. Lin, Y. Liu, T. Chan,\net al.\n(2024)\nManiskill3: gpu parallelized robotics simulation and rendering for generalizable embodied ai\n.\narXiv preprint arXiv:2410.00425\n.\nCited by:\n§4.2\n.\n[46]\nG. Team, R. Anil, S. Borgeaud, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican,\net al.\n(2023)\nGemini: a family of highly capable multimodal models\n.\narXiv preprint arXiv:2312.11805\n.\nCited by:\n§1\n.\n[47]\nG. R. Team, S. Abeyruwan, J. Ainslie, J. Alayrac, M. G. Arenas, T. Armstrong, A. Balakrishna, R. Baruch, M. Bauza, M. Blokzijl,\net al.\n(2025)\nGemini robotics: bringing ai into the physical world\n.\narXiv preprint arXiv:2503.20020\n.\nCited by:\n§2\n.\n[48]\nO. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu,\net al.\n(2024)\nOcto: an open-source generalist robot policy\n.\narXiv preprint arXiv:2405.12213\n.\nCited by:\n§1\n,\nTable 1\n.\n[49]\nY. Wang, X. Li, W. Wang, J. Zhang, Y. Li, Y. Chen, X. Wang, and Z. Zhang\n(2025)\nUnified vision-language-action model\n.\narXiv preprint arXiv:2506.19850\n.\nCited by:\nTable 1\n.\n[50]\nZ. Yan, W. Dong, Y. Shao, Y. Lu, H. Liu, J. Liu, H. Wang, Z. Wang, Y. Wang, F. Remondino,\net al.\n(2025)\nRenderworld: world model with self-supervised 3d label\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp. 6063–6070\n.\nCited by:\n§1\n.\n[51]\nY. Yang, Z. Duan, T. Xie, F. Cao, P. Shen, P. Song, P. Jin, G. Sun, S. Xu, Y. You,\net al.\n(2025)\nFPC-vla: a vision-language-action framework with a supervisor for failure prediction and correction\n.\narXiv preprint arXiv:2509.04018\n.\nCited by:\nTable 1\n.\n[52]\nT. Yuan, Y. Liu, C. Lu, Z. Chen, T. Jiang, and H. Zhao\n(2025)\nDepthVLA: enhancing vision-language-action models with depth-aware spatial reasoning\n.\narXiv preprint arXiv:2510.13375\n.\nCited by:\n§2\n.\n[53]\nM. Zawalski, W. Chen, K. Pertsch, O. Mees, C. Finn, and S. Levine\n(2024)\nRobotic control via embodied chain-of-thought reasoning\n.\narXiv preprint arXiv:2407.08693\n.\nCited by:\n§1\n.\n[54]\nX. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer\n(2023)\nSigmoid loss for language image pre-training\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp. 11975–11986\n.\nCited by:\n§4.1\n.\n[55]\nJ. Zhang, Y. Chen, Y. Xu, Z. Huang, Y. Zhou, Y. Yuan, X. Cai, G. Huang, X. Quan, H. Xu,\net al.\n(2025)\n4D-vla: spatiotemporal vision-language-action pretraining with cross-scene calibration\n.\narXiv preprint arXiv:2506.22242\n.\nCited by:\n§2\n.\n[56]\nJ. Zhang, Y. Guo, Y. Hu, X. Chen, X. Zhu, and J. Chen\n(2025)\nUp-vla: a unified understanding and prediction model for embodied agent\n.\narXiv preprint arXiv:2501.18867\n.\nCited by:\n§2\n.\n[57]\nS. Zhang, Z. Xu, P. Liu, X. Yu, Y. Li, Q. Gao, Z. Fei, Z. Yin, Z. Wu, Y. Jiang,\net al.\n(2025)\nVlabench: a large-scale benchmark for language-conditioned robotics manipulation with long-horizon reasoning tasks\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp. 11142–11152\n.\nCited by:\nAppendix A\n,\n§2\n,\n§4.1\n,\n§4.2\n.\n[58]\nW. Zhang, H. Liu, Z. Qi, Y. Wang, X. Yu, J. Zhang, R. Dong, J. He, F. Lu, H. Wang,\net al.\n(2025)\nDreamvla: a vision-language-action model dreamed with comprehensive world knowledge\n.\narXiv preprint arXiv:2507.04447\n.\nCited by:\n§1\n,\n§2\n,\nTable 1\n.\n[59]\nZ. Zhang, H. Li, Y. Dai, Z. Zhu, L. Zhou, C. Liu, D. Wang, F. E. Tay, S. Chen, Z. Liu,\net al.\n(2025)\nFrom spatial to actions: grounding vision-language-action model in spatial foundation priors\n.\narXiv preprint arXiv:2510.17439\n.\nCited by:\n§2\n.\n[60]\nQ. Zhao, Y. Lu, M. J. Kim, Z. Fu, Z. Zhang, Y. Wu, Z. Li, Q. Ma, S. Han, C. Finn,\net al.\n(2025)\nCot-vla: visual chain-of-thought reasoning for vision-language-action models\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp. 1702–1713\n.\nCited by:\n§2\n,\nTable 1\n.\n[61]\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn\n(2023)\nLearning fine-grained bimanual manipulation with low-cost hardware\n.\narXiv preprint arXiv:2304.13705\n.\nCited by:\n§1\n.\n[62]\nH. Zhen, X. Qiu, P. Chen, J. Yang, X. Yan, Y. Du, Y. Hong, and C. Gan\n(2024)\n3d-vla: a 3d vision-language-action generative world model\n.\narXiv preprint arXiv:2403.09631\n.\nCited by:\n§1\n.\n[63]\nJ. Zheng, J. Li, D. Liu, Y. Zheng, Z. Wang, Z. Ou, Y. Liu, J. Liu, Y. Zhang, and X. Zhan\n(2025)\nUniversal actions for enhanced embodied foundation models\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp. 22508–22519\n.\nCited by:\nTable 1\n.\n[64]\nR. Zheng, Y. Liang, S. Huang, J. Gao, H. Daumé III, A. Kolobov, F. Huang, and J. Yang\n(2024)\nTracevla: visual trace prompting enhances spatial-temporal awareness for generalist robotic policies\n.\narXiv preprint arXiv:2412.10345\n.\nCited by:\nTable 1\n.\n[65]\nW. Zheng, W. Chen, Y. Huang, B. Zhang, Y. Duan, and J. Lu\n(2024)\nOccworld: learning a 3d occupancy world model for autonomous driving\n.\nIn\nEuropean conference on computer vision\n,\npp. 55–72\n.\nCited by:\n§1\n.\n[66]\nC. Zhou, L. Yu, A. Babu, K. Tirumala, M. Yasunaga, L. Shamis, J. Kahn, X. Ma, L. Zettlemoyer, and O. Levy\n(2024)\nTransfusion: predict the next token and diffuse images with one multi-modal model\n.\narXiv preprint arXiv:2408.11039\n.\nCited by:\n§2\n.\n[67]\nB. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid,\net al.\n(2023)\nRt-2: vision-language-action models transfer web knowledge to robotic control\n.\nIn\nConference on Robot Learning\n,\npp. 2165–2183\n.\nCited by:\n§2\n.\nAppendix A\nDataset Description\nIn this section, we present a comprehensive characterization of the benchmark datasets and the custom-collected data used for model training in our experiments. We systematically report key statistics, including the total number of episodes, frame counts, and other relevant properties, which is summarized in Table\n7\nbelow:\nType\nDataset\nEmbodiment\nDoF\nEpisodes\nFrames\nFPS\nSimulation\nLIBERO\nFranka\n7\n1,693\n273,465\n10\nLIBERO-Plus\nFranka\n7\n14,347\n2,238,036\n20\nVLABench\nFranka\n7\n4,713\n528,398\n10\nReal-World\nWipe Stain\nAgiBot G1\n22\n177\n356,316\n30\nPour Water\nAgiBot G1\n22\n1,821\n5,062,506\n30\nOpen-set Pick\nAgiBot G1\n22\n1,936\n219,824\n30\nOpen-set Pick\nAgileX\n14\n962\n251,283\n30\nTable 7\n:\nDataset statistics.\nSimulation Benchmarks.\nWe utilize three publicly released simulation datasets,\ni.e\n.\n, LIBERO\n[\n32\n]\n, LIBERO-Plus\n[\n15\n]\n, and VLABench\n[\n57\n]\n.\nSpecifically, the LIBERO dataset contains\n1\n,\n693\n1,693\nepisodes and\n273\n,\n465\n273,465\nframes, recorded at a fixed\n10\n10\nHz. Its demonstrations exhibit relatively uniform trajectory lengths and smooth motion patterns, making it widely adopted benchmark in community.\nHowever, due to the increasing performance saturation observed on LIBERO, LIBERO-Plus is recently introduced to provide a more challenging and diversified evaluation setting. LIBERO-Plus provides\n14\n,\n347\n14,347\nepisodes and\n2\n,\n238\n,\n036\n2,238,036\nframes, captured at\n20\n20\nHz.\nIn contrast to the homogeneous trajectories in LIBERO, LIBERO-Plus explicitly emphasizes a perturbation-oriented design. The demonstrations display substantially larger variations in motion magnitude and camera–robot viewpoint configuration.\nThese characteristics make it a more suitable benchmark for evaluating policy generalization under structured distribution shifts.\nBesides these two datasets, we further benchmark our method on VLABench, whose training set includes\n4\n,\n713\n4,713\nepisodes and\n528\n,\n398\n528,398\nframes, recorded at\n10\n10\nHz, which requires a higher level of visual and physical understanding from the policy.\nReal-World Experiment.\nFor real-world deployment, we collect demonstrations across\n3\n3\ntasks,\ni.e\n.\n, Wipe Stain, Pour Water, and Open-set Pick, as shown in Table\n7\n.\nThe “Wipe Stain” dataset contains\n177\n177\nepisodes with\n356\n,\n316\n356,316\nframes, characterized by dense tool–surface contact and fine-grained force control. The “Pour Water” dataset includes\n1\n,\n821\n1,821\nepisodes and\n5\n,\n062\n,\n506\n5,062,506\nframes. Its large scale stems from the task’s long-horizon and multi-stage nature. Regarding the “Open-set Pick” task, the AgiBot G1 subset provides\n1\n,\n936\n1,936\nepisodes with\n219\n,\n824\n219,824\nframes, while the AgileX subset offers\n962\n962\nepisodes with\n251\n,\n283\n251,283\nframes, both featuring diverse tabletop layouts and natural-language instructions.\nAppendix B\nTraining & Evaluation Details\nTask\nAction Space\nAction Horizon\nState\nBatch Size\nTraining Step\nLIBERO\nDelta EEF\n10\n×\n\\times\n128\n40K\nLIBERO-Plus\nDelta EEF\n10\n×\n\\times\n128\n100K\nVLABench\nAbs EEF\n10\n✓\n128\n60K\nWipe Stain\nAbs Joint\n30\n✓\n128\n50K\nPour Water\nAbs Joint\n30\n✓\n128\n240K\nOpen-set Pick\nAbs Joint\n30\n✓\n128\n50K\nOpen-set Pick\n†\nAbs Joint\n30\n✓\n128\n50K\nTable 8\n:\nTraining details. Note that the “Open-set Pick\n†\n” task is performed on AgileX platform.\nTraining Details.\nWe describe the task-specific training configurations,\ne.g\n.\n, action space and state usage, for better understanding.\nAs presented in Table\n8\n, for the LIBERO and LIBERO-Plus suites, the policy is trained using delta end-effector control (Delta EEF) with an action horizon of\n10\n10\nsteps. In particular, no privileged state information is provided during training. We utilize a global batch size of\n128\n128\nand train the policies for\n40\n​\nK\n40\\text{K}\nand\n100\n​\nK\n100\\text{K}\nsteps, respectively. Similarly, we train our models in VLABench for\n60\n​\nK\n60\\text{K}\nsteps, while adopting state input and absolute end-effector (Abs EEF) actions to align the benchmark’s control convention.\nIn terms of the real-world tasks, we utilize Abs Joint control with a longer action horizon of\n30\n30\n.\nUnlike the simulator benchmarks, these tasks additionally provide structured robot state observations to improve robustness under real-world sensing and actuation noise.\nOur models are trained for\n50\n​\nK\n50\\text{K}\n,\n240\n​\nK\n240\\text{K}\n, and\n50\n​\nK\n50\\text{K}\nsteps, in “Wipe Stain”, “Pour Water”, and “Open-set Pick” tasks, respectively, with same batch size of\n128\n128\n.\nName\nEAR\nIAR\nCamera\nRobot\nLanguage\nLight\nBackground\nNoise\nLayout\nAvg.\nBaseline\n70.3\n41.7\n81.1\n97.3\n94.6\n71.8\n84.9\n75.7\n#1\n✓\n88.7\n63.5\n80.4\n94.0\n90.2\n89.5\n84.2\n83.7\n#2\n✓\n80.7\n48.7\n82.6\n97.7\n90.9\n84.3\n86.0\n80.4\n#3\n✓\n✓\n91.2\n62.5\n80.3\n95.1\n91.5\n88.3\n84.9\n84.1\nTable 9\n:\nModule ablations on LIBERO-Plus benchmark. The performance is gradually improved with the addition of proposed methods.\nName\nAction Head\nEAR\nLIBERO\nLIBERO-Plus\nParam.\nDenoise\nParam.\nDenoise\nSpatial\nObject\nGoal\nLong\nAvg.\nCamera\nRobot\nLanguage\nLight\nBackground\nNoise\nLayout\nAvg.\nBaseline\n300M\n10\n-\n-\n98.6\n99.0\n96.4\n92.2\n96.6\n70.3\n41.7\n81.1\n97.3\n94.6\n71.8\n84.9\n75.7\n#1\n600M\n10\n-\n-\n97.6\n98.4\n97.8\n96.4\n97.6\n68.7\n44.8\n83.1\n96.4\n92.7\n66.6\n84.1\n74.9\n#2\n600M\n20\n-\n-\n97.8\n98.8\n98.0\n95.2\n97.5\n70.0\n44.8\n82.7\n97.6\n93.1\n66.7\n83.2\n75.1\n#3\n300M\n5\n300M\n5\n98.6\n99.6\n97.8\n95.4\n97.9\n88.2\n62.4\n81.5\n95.0\n91.5\n88.6\n85.3\n83.9\n#4\n300M\n10\n300M\n10\n99.0\n99.4\n98.0\n96.6\n98.3\n88.7\n63.5\n80.4\n94.0\n90.2\n89.5\n84.2\n83.7\n#4\n300M\n10\n300M\n10\n99.0\n99.4\n98.0\n96.6\n98.3\n88.7\n63.5\n80.4\n94.0\n90.2\n89.5\n84.2\n83.7\n#5\n300M\n10\n150M\n10\n99.2\n99.2\n97.8\n94.2\n97.6\n86.4\n54.3\n81.7\n92.2\n91.4\n89.1\n82.1\n81.7\n#6\n300M\n10\n250M\n10\n99.0\n98.2\n98.6\n94.2\n97.5\n87.2\n59.7\n81.1\n95.0\n93.7\n87.4\n83.5\n83.1\n#7\n300M\n10\n500M\n10\n98.4\n99.4\n96.6\n94.2\n97.0\n80.8\n57.6\n84.1\n95.6\n92.1\n79.8\n83.7\n80.9\nTable 10\n:\nEffects of parameters and denoise steps on policy performance. The best results are highlighted in\nbold\n, and the second-best results are\nunderlined\n. Note that the IAR module is not added in this experiment.\nEvaluation Details.\nNext, we illustrate the evaluation protocols and success criteria for all real-world tasks. Each task is assessed using fixed and repeatable initializations to ensure reproducibility and reduce environmental variance.\nConcretely, in terms of the “Wipe Stain” task, we predefine three initial sponge poses. For each pose, the robot is required to clean stains placed at four distinct table locations. Every configuration is executed twice, resulting in\n24\n24\ntrials in total. A trial is considered successful if the robot grasps the sponge and removes the stain from the specified location.\nAs for the “Pour Water”, we standardize six predefined relative configurations between the bottle and the glass. Then, each configuration is executed two times. A trial is counted as successful if the robot lifts the bottle, pours water into the cup, and places the bottle back onto the coaster. Note that minor spillage of water when pouring is allowed.\nEventually, regarding the “Open-set Pick” task, we initialize ten object arrangements on the table, containing both in-distribution and out-of-distribution instances. In each arrangement, the robot is instructed to grasp a specified target object using either its left or right arm, as indicated by the instruction. Each arm–object pair is evaluated twice, resulting in\n40\n40\ntrials overall. A trial is deemed successful if the robot grasps the instructed object with the correct arm.\nAcross all tasks, evaluations are carried out by trained operators with substantial prior testing experience, and success rates are computed as the proportion of successful trials relative to the total number of executed attempts.\nAppendix C\nMore Experimental Results\nIn this section, we provide additional quantitative experiments to substantiate the effectiveness of our proposed approach and to empirically uncover several insightful phenomena. Specifically, the experimental analyses comprise three parts: (1) ablation study conducted on the LIBERO-Plus benchmark in Table\n9\n, (2) an investigation of how the parameter sizes of the Action Head and Explicit Action Reasoner (EAR), as well as the number of denoising steps, influence policy performance in Table\n10\n, and (3) a comparative study examining the relationship among inference latency, model size, and performance in Table\n11\n. Note that we adopt\nπ\n0.5\n\\pi_{0.5}\n[\n22\n]\nas the baseline method, denoted as “Baseline”.\nModule Ablation.\nAs shown in Table\n9\n, incorporating the proposed reasoning modules consistently improves policy performance on the LIBERO-Plus benchmark.\nAdding the EAR module,\ni.e\n.\n, experiment “#1”, yields a clear gain over the baseline, increasing the average success rate from\n75.7\n%\n75.7\\%\nto\n83.7\n%\n83.7\\%\n. This improvement can be attributed to EAR’s ability to generate an explicit reference action trajectory, which significantly reduces the ambiguity in mapping complex visual or linguistic observations to low-level actions, such as camera shifts and background changes.\nMeanwhile, incorporating only the IAR (“#2”) also improves the performance from\n75.7\n%\n75.7\\%\nto\n80.4\n%\n80.4\\%\n, indicating that decoding the latent action-related semantics within the vision–language backbone provides useful behavioral priors.\nFinally, combining EAR and IAR (“#3”) achieves the highest success rate of\n84.1\n%\n84.1\\%\n, demonstrating their complementary effects,\ni.e\n.\n, EAR provides explicit motion guidance, while IAR supplies dense representation-level priors.\nEffect of Model Scaling & Denoising Budget.\nThen, we analyze the superiority of our method by comparing settings with matched total model parameters and denoising steps. As shown in Table\n10\n, firstly, we enlarge the model size of the action head and increase the number of denoising steps in experiments “#1” and “#2”, to construct fair baselines for subsequent comparison.\nWe observe a preliminary observation,\ni.e\n.\n, increasing the model size or denoising steps does not reliably enhance performance. Specifically, compared with the baseline, while “#1” improves performance on the LIBERO benchmark, it simultaneously drops on LIBERO-Plus. Next, comparing “#1” and “#2” reveals that further increasing denoising steps yields only negligible fluctuations.\nSubsequently, we incorporate the EAR module under fully matched overall parameterization and denoising budgets. Concretely, in both comparison pairs, “#1” with “#3” and “#2” with “#4”, we consistently observe notable performance improvements on both benchmarks, once the EAR module is introduced. This indicates that the performance gains originate from our proposed action chain-of-thought. The proposed mechanism supplies explicit reference actions that effectively mitigate the intrinsic instability of action prediction, especially under challenging external perturbations, as shown in the LIBERO-Plus, enabling a more reliable and grounded generalist robotic policy.\nEffect of EAR Scale.\nMoreover, we investigate how various scale of the EAR module influences action prediction fidelity. To isolate the effect of EAR, we keep the action head parameters and the denoising schedule strictly fixed, while scaling the EAR module to\n150\n​\nM\n150\\text{M}\n,\n250\n​\nM\n250\\text{M}\n,\n300\n​\nM\n300\\text{M}\n, and\n500\n​\nM\n500\\text{M}\nparameters via adjusting hidden size.\nAs presented in Table\n10\n, through the comparison across experiments “#4”, “#5”, “#6”, and “#7”, we find that although all EAR-equipped variants outperform non-EAR baselines on both benchmarks, the performance trend is non-monotonic. Applying moderate EAR scales,\ne.g\n.\n,\n300\n​\nM\n300\\text{M}\n, yields the greatest improvement.\nParticularly, as evidenced in “#7” in Table\n10\n, when the parameter of EAR module even exceeds that of the action head, we observe a marked drop in performance. We attribute this degradation to the tendency of an over-parameterized EAR to overfit spurious correlations during training. Therefore, it generates reference action trajectories that are systematically biased, which ultimately misdirect the action head toward suboptimal predictions.\nName\nEAR\nIAR\nParam.\nLatency\nLIBERO\nLIBERO-Plus\nAvg. SR\nAvg. SR\nBaseline\n3.35B\n91ms\n96.9\n75.7\n#1\n✓\n3.80B\n110ms\n98.3\n83.7\n#2\n✓\n3.36B\n93ms\n98.1\n80.4\n#3\n✓\n✓\n3.81B\n112ms\n98.5\n84.1\nTable 11\n:\nAblation experiment on model efficiency and performance.\nLatency Analysis.\nIn Table\n11\n, we further examine the inference efficiency of our approach in terms of both parameter count and end-to-end latency. As additional reasoning modules are introduced, we observe a slight increase in latency. Incorporating the EAR module raises latency from\n91\n​\nms\n91\\text{ms}\nto\n110\n​\nms\n110\\text{ms}\n, while adding the IAR module introduces only an additional\n2\n​\nms\n2\\text{ms}\n. However, this marginal overhead is outweighed by the substantial improvement, which reflects a favorable trade-off.\nAppendix D\nLimitations & Future Works\nIn this section, we discuss the limitations existing in our work and promising directions for future research.\nAlthough our proposed action chain-of-thought (ACoT) substantially boosts policy performance, our framework still exhibits several constraints. The reasoning modules introduce additional computational cost, which, while relatively modest compared to the performance gains, may pose challenges for deployment on resource-constrained robotic platforms.\nBesides, another limitation stems from the fact that the prevailing action representation in the community is implemented as action chunks,\ni.e\n.\n, sequences of low-level control commands such as joint angles or end-effector poses. While such representations faithfully encode the executed motions, they lack explicit geometric structure that would facilitate higher-level spatial reasoning, such as object-centric coordination and contact geometry. Hence, the potential of ACoT reasoning may not be fully unleashed. Enriching action representations with spatially grounded information to enable ACoT to operate in geometrically interpretable 3D space, constitutes an interesting and promising avenue for future exploration.\nAppendix E\nLLM Usage Statement\nIn this paper, we employ Large Language Models (LLMs) solely for minor linguistic refinement during the manuscript preparation stage, such as correcting grammatical errors. None of the technical content, implementation details, or experimental results were generated by LLMs.",
  "preview_text": "Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.\n\nACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models\nLinqing Zhong\n1,2\nYi Liu\n2\nYifei Wei\n1,2\nZiyu Xiong\n2\nMaoqing Yao\n2∗\nSi Liu\n1∗\nGuanghui Ren\n2\n1\nBeihang University\n2\nAgiBot\nCorresponding author\nAbstract\nVision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimo",
  "is_relevant": true,
  "relevance_score": 6.0,
  "extracted_keywords": [
    "VLA",
    "VLM",
    "whole body control"
  ],
  "one_line_summary": "该论文提出了一种名为ACoT-VLA的新架构，通过动作链式思维范式，在动作空间中进行直接推理，以提升视觉-语言-动作模型在机器人操作任务中的性能。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-16T16:17:06Z",
  "created_at": "2026-01-20T17:50:01.192271",
  "updated_at": "2026-01-20T17:50:01.192280"
}