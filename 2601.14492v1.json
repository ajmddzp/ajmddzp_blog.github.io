{
    "id": "2601.14492v1",
    "title": "UNCLE-Grasp: Uncertainty-Aware Grasping of Leaf-Occluded Strawberries",
    "authors": [
        "Malak Mansour",
        "Ali Abouzeid",
        "Zezhou Sun",
        "Qinbo Sun",
        "Dezhen Song",
        "Abdalla Swikir"
    ],
    "abstract": "Âú®ÈÉ®ÂàÜÈÅÆÊå°Êù°‰ª∂‰∏ãËøõË°åËçâËéìÊú∫Âô®‰∫∫ÈááÊëòÂÖ∑ÊúâÊåëÊàòÊÄßÔºåÂõ†‰∏∫Âè∂Áâá‰ºöÂºïÂèëÊòæËëóÁöÑÂá†‰Ωï‰∏çÁ°ÆÂÆöÊÄßÔºå‰ΩøÂæóÂü∫‰∫éÂçï‰∏ÄÁ°ÆÂÆöÊÄßÂΩ¢Áä∂‰º∞ËÆ°ÁöÑÊäìÂèñÂÜ≥Á≠ñ‰∏çÂèØÈù†„ÄÇ‰ªéÂçïÊ¨°Â±ÄÈÉ®ËßÇÊµã‰∏≠ÔºåÂèØËÉΩÂ≠òÂú®Â§öÁßç‰∫í‰∏çÁõ∏ÂÆπÁöÑ3DË°•ÂÖ®ÁªìÊûúÔºåÂØºËá¥Âú®Êüê‰∏ÄË°•ÂÖ®ÂΩ¢ÊÄÅ‰∏ãÂèØË°åÁöÑÊäìÂèñÊñπÊ°àÂú®Âè¶‰∏ÄÂΩ¢ÊÄÅ‰∏ãÂ§±Êïà„ÄÇÊú¨ÊñáÊèêÂá∫‰∏ÄÁßçÈíàÂØπÈÉ®ÂàÜÈÅÆÊå°ËçâËéìÁöÑ‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÊäìÂèñÊµÅÁ®ãÔºåËØ•ÊµÅÁ®ãÊòæÂºèÂª∫Ê®°‰∫ÜÁî±ÈÅÆÊå°ÂíåÂ≠¶‰π†ÂûãÂΩ¢Áä∂ÈáçÂª∫ÊâÄÂºïÂèëÁöÑË°•ÂÖ®‰∏çÁ°ÆÂÆöÊÄß„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈááÁî®Â∏¶ËíôÁâπÂç°Ê¥õÈöèÊú∫‰∏¢ÂºÉÁöÑÁÇπ‰∫ëË°•ÂÖ®ÊäÄÊúØÁîüÊàêÂ§ö‰∏™ÂΩ¢Áä∂ÂÅáËÆæÔºå‰∏∫ÊØè‰∏™Ë°•ÂÖ®ÁªìÊûúÁîüÊàêÂÄôÈÄâÊäìÂèñÊñπÊ°àÔºåÂπ∂Âü∫‰∫éÁâ©ÁêÜ grounded ÁöÑÂäõÈó≠ÂêàÊåáÊ†áËØÑ‰º∞ÊäìÂèñÂèØË°åÊÄß„ÄÇ‰∏çÂêå‰∫é‰æùËµñÂçï‰∏Ä‰º∞ËÆ°ÈÄâÊã©ÊäìÂèñÊñπÊ°àÔºåÊàë‰ª¨ÈÄöËøáËÅöÂêàÂ§öË°•ÂÖ®ÁªìÊûúÁöÑÂèØË°åÊÄßÊï∞ÊçÆÔºåÂπ∂ÈááÁî®‰øùÂÆàÁöÑ‰∏ãÁΩÆ‰ø°ÁïåÂáÜÂàôÊù•ÂÜ≥ÂÆöÊòØÂê¶ÊâßË°åÊäìÂèñÊàñÂÆâÂÖ®ÊîæÂºÉ„ÄÇÊàë‰ª¨Âú®‰ªøÁúüÁéØÂ¢ÉÂíåÂÆû‰ΩìÊú∫Âô®‰∫∫‰∏äÔºåÈíàÂØπÈÄíÂ¢ûÁ®ãÂ∫¶ÁöÑÂêàÊàêÂèäÁúüÂÆûÂè∂ÁâáÈÅÆÊå°Âú∫ÊôØËøõË°å‰∫ÜÊñπÊ≥ïËØÑ‰º∞„ÄÇÁªìÊûúË°®ÊòéÔºö‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÂÜ≥Á≠ñÊú∫Âà∂ËÉΩÂú®‰∏•ÈáçÈÅÆÊå°‰∏ãÂèØÈù†ÊîæÂºÉÈ´òÈ£éÈô©ÊäìÂèñÂ∞ùËØïÔºåÂêåÊó∂Âú®Âá†‰ΩïÁΩÆ‰ø°Â∫¶ÂÖÖË∂≥Êó∂‰øùÊåÅÁ®≥ÂÅ•ÁöÑÊäìÂèñÊâßË°åËÉΩÂäõÔºåÂú®‰ªøÁúü‰∏éÂÆû‰ΩìÊú∫Âô®‰∫∫ÂÆûÈ™å‰∏≠Âùá‰ºò‰∫éÁ°ÆÂÆöÊÄßÂü∫Á∫øÊñπÊ≥ï„ÄÇ",
    "url": "https://arxiv.org/abs/2601.14492v1",
    "html_url": "https://arxiv.org/html/2601.14492v1",
    "html_content": "UNCLE-Grasp: Uncertainty-Aware Grasping of Leaf-Occluded Strawberries\nMalak Mansour,\nAli Abouzeid,\nZezhou Sun,\nQinbo Sun,\nDezhen Song,\nAbdalla Swikir\nDepartment of Robotics, Mohamed bin Zayed University of Artificial Intelligence\nEmails:\n{malak.mansour, ali.abouzeid, zezhou.sun, qinbo.sun, dezhen.song, abdalla.swikir}@mbzuai.ac.ae\nAbstract\nRobotic strawberry harvesting is challenging under partial occlusion, where leaves induce significant geometric uncertainty and make grasp decisions based on a single deterministic shape estimate unreliable. From a single partial observation, multiple incompatible 3D completions may be plausible, causing grasps that appear feasible on one completion to fail on another. We propose an uncertainty-aware grasping pipeline for partially occluded strawberries that explicitly models completion uncertainty arising from both occlusion and learned shape reconstruction. Our approach uses point cloud completion with Monte Carlo dropout to sample multiple shape hypotheses, generates candidate grasps for each completion, and evaluates grasp feasibility using physically grounded force-closure‚Äìbased metrics. Rather than selecting a grasp based on a single estimate, we aggregate feasibility across completions and apply a conservative lower confidence bound (LCB) criterion to decide whether a grasp should be attempted or safely abstained. We evaluate the proposed method in simulation and on a physical robot across increasing levels of synthetic and real leaf occlusion. Results show that uncertainty-aware decision making enables reliable abstention from high-risk grasp attempts under severe occlusion while maintaining robust grasp execution when geometric confidence is sufficient, outperforming deterministic baselines in both simulated and physical robot experiments.\nFigure 1:\nOverview of UNCLE-Grasp: a) Increasing leaf occlusion induces centroid shift in the reconstructed point cloud. b) Completion uncertainty from MC shape completions increases with occlusion, reducing goemetric confidence; purple regions indicate high uncertainty. c) Instead of selecting a single ‚Äúbest‚Äù grasp, UNCLE-Grasp evaluates object-level risk using LCB and either attempts to grasp the strawberry or abstains to avoid damaging it.\nI\nIntroduction\nTABLE I:\nComparison with prior work that accounts for uncertainty in grasping or shape completion. The proposed method uniquely propagates learned shape uncertainty into risk-aware, object-level grasp decisions under severe occlusion.\nCapability\nPose Unc.\n[\n7\n]\nKehoe et al.\n[\n11\n]\nPhys.-Based Unc.\n[\n13\n]\nVille et al.\n[\n15\n]\nDiff. FC\n[\n14\n]\nHumanoid Unc.\n[\n1\n]\nMeasuring Unc.\n[\n4\n]\nPUGS\n[\n2\n]\nUNCLE-Grasp\nLearned Shape Completion\n‚úó\n‚úó\n‚úó\n‚úì\n‚úó\n‚úó\n‚úì\n‚úó\n‚úì\nCompletion Uncertainty Modeling\n‚úó\n‚úó\n‚úó\n‚úì\n‚úó\n‚úó\n‚úì\n‚úó\n‚úì\nUncertainty-Aware Grasp Evaluation\n‚úì\n‚úì\n‚úì\n‚úì\n‚úó\n‚úì\n‚úó\n‚úì\n‚úì\nPhysically Grounded Grasp Feasibility\n‚úì\n‚úì\n‚úì\n‚úó\n‚úì\n‚úì\n‚úó\n‚úó\n‚úì\nRisk-Aware Bounds under Uncertainty\n‚úó\n‚úì\n‚úì\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úì\nSevere Occlusion Scenarios\n‚úó\n‚úó\n‚úó\n‚úì\n‚úó\n‚úó\n‚úì\n‚úì\n‚úì\nObject-Level Abstention\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úì\nAgricultural Harvesting\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úì\nRobotic grasping in unstructured agricultural environments remains a challenging problem due to frequent occlusions, visual ambiguity, and partial observability of target objects. In fruit harvesting scenarios, strawberries are often partially covered by leaves, stems, or adjacent fruit, resulting in incomplete point cloud observations that can significantly distort geometric estimates. These errors propagate downstream to unstable or unreachable grasps, potentially resulting in fruit damage due to the robot‚Äôs poor perception and unawareness of its own uncertainty. Harvesting is a sequential process: grasping surrounding fruit can reduce occlusion for subsequent targets, making uncertainty-aware decision making on heavily occluded strawberries preferable to premature grasp attempts.\nRecent advances in learning-based grasp synthesis have demonstrated strong performance when accurate object geometry is available. Grasp prediction methods generate high-quality 6DoF grasps directly from point clouds; however, they implicitly assume that the observed geometry faithfully represents the underlying object. Under partial occlusion, this assumption no longer holds: grasp predictions become biased toward visible surfaces, leading to significant centroid shifts as occlusion increases (Fig.\n1\na). Furthermore, confidence scores produced by deterministic grasp generating models fail to reflect the ambiguity introduced by missing geometry. As a result, grasp selection becomes brittle in heavily occluded scenes, motivating the need for explicit reasoning about uncertainty in downstream grasp decisions.\nA common strategy to address partial observability and centroid shift is point cloud completion, where a model reconstructs the full object shape from partial inputs. While completion can mitigate geometric bias, it introduces an additional source of uncertainty, as multiple possible completion models may result from the same partial observation. Most existing grasping pipelines treat the output of completion networks as deterministic, ignoring this inherent uncertainty and selecting grasps based on a single reconstructed shape. In safety-critical manipulation tasks such as harvesting delicate fruit, this overconfidence can lead to grasp failures, fruit damage, or unintended collisions with neighboring objects.\nIn this work, we propose an uncertainty-aware grasping pipeline for partially occluded strawberries that explicitly models uncertainty arising from both occlusion and learned shape completion. Our approach integrates transformer-based point cloud completion with Monte Carlo (MC) dropout to generate multiple plausible reconstructions of the occluded fruit. For each reconstructed shape, candidate grasps are generated and filtered using physically motivated geometric constraints, including collision-free jaw placement, stable grasp orientation, and front-facing, kinematically feasible approach directions. In addition to these deterministic checks, we apply uncertainty-aware filtering at two levels: highly uncertain grasps are removed based on local completion variance, and strawberries with consistently high uncertainty across completion samples are rejected at the object level. Rather than selecting a grasp based on mean confidence alone, we adopt a risk-aware Lower Confidence Bound (LCB) criterion to decide whether a strawberry should be grasped at all, jointly considering the expected grasp quality and its variance across shape completions.\nIn summary, this paper makes the following contributions:\n‚Ä¢\nAn uncertainty-aware grasping pipeline for partially occluded strawberries that combines learned point cloud completion with MC dropout to enable object-level abstention, avoiding grasp attempts on highly uncertain fruit.\n‚Ä¢\nA physically grounded grasp filtering and evaluation framework that integrates geometric constraints with uncertainty-aware grasp quality metrics to enable safe, risk-aware grasp selection under learned geometric uncertainty, and to systematically assess grasp reliability as occlusion severity of strawberries increases.\nFigure 2:\nOverview of the proposed uncertainty-aware strawberry grasping pipeline.\nTop: RGB input showing multiple strawberries partially occluded by leaves is detected and segmented, then 3D projected with the depth image to obtain a partial strawberry point cloud; A transformer-based point cloud completion network\n[\n23\n]\nreconstructs the missing geometry to produce the completed point cloud; MC dropout is enabled to generate K plausible completions\n{\nùí´\ncompleted\n(\nk\n)\n}\nk\n=\n1\nK\n\\{\\mathcal{P}_{\\text{completed}}^{(k)}\\}_{k=1}^{K}\nand estimate geometric uncertainty.\nMiddle: A grasps generation model\n[\n21\n]\nthen takes each completed point cloud,\nùí´\ncompleted\n(\nk\n)\n\\mathcal{P}_{\\text{completed}}^{(k)}\n, and generates M grasps; The computed uncertainty from dropout is used to abstain from highly uncertain strawberries and filter through the grasp proposals using a) global and b) local uncertainty thresholds.\nBottom: The remaining grasps are filtered using (c) orientation constraints (front-facing and non-vertical) and (d) jaw‚Äìobject intersection checks. The filtered grasps from each completion sample k form the set\nM\n‚Ä≤\nM^{\\prime}\n, which are aggregated across all completion samples to compute an LCB-based feasibility score. A strawberry is attempted when LCB\n>\n>\n0; otherwise, the system abstains, enabling a higher grasp success rate.\nII\nRelated Work\nOur work spans robotic grasp synthesis in cluttered scenes, 3D shape completion from partial observations, and uncertainty-aware grasp planning. Table\nI\nsummarizes key characteristics of representative prior approaches and highlights that our method uniquely integrates learned shape completion, explicit uncertainty modeling, and uncertainty-aware object-level grasp selection under severe occlusion.\nII-A\nGrasp Synthesis from Partial Observations\nEarly learning-based grasp synthesis methods such as GPD\n[\n22\n]\nand Dex-Net\n[\n17\n]\ngenerate robust grasps from point clouds or synthetic depth data, but typically assume access to reasonably complete object geometry. More recent approaches, including CGNet\n[\n21\n]\nand MVGrasp\n[\n10\n]\n, operate directly on partial observations in clutter, with MVGrasp leveraging multi-view fusion to mitigate occlusions. While these methods reduce dependence on full geometry, they do not explicitly model geometric uncertainty induced by missing regions. In contrast, our approach uses shape completion and explicitly reasons about uncertainty in reconstructed geometry to enable risk-aware grasping under severe occlusion.\nII-B\nShape Completion for Robotic Manipulation\nTo overcome the limitations of partial observations, shape completion has emerged as a key component for robotic manipulation. Early learning-based approaches reconstruct full object geometry from partial inputs using voxel-based representations\n[\n24\n,\n3\n]\n, but fixed-resolution grids limit scalability and the recovery of fine geometric detail.\nMore recent work explores expressive representations and uncertainty-aware completion.\nconfidence_guided_completion [\n20\n]\nproposes an implicit shape representation that provides confidence estimates for reconstructed points and allows resolution to be adjusted at inference time, while\ncompletion_predict_uncertain [\n8\n]\nexplicitly predicts regions of high geometric uncertainty and demonstrates improved grasping by avoiding unreliable areas. Unlike these methods, which primarily use uncertainty at the local point or contact level, our approach integrates completion uncertainty into both local grasp filtering and object-level decision making by rejecting fruits with globally high uncertainty.\nIn the agricultural domain,\nmagistri_completion_agri [\n16\n]\naddresses fruit shape completion under heavy occlusion using a transformer-based model that combines learned priors with deformable templates. While this work demonstrates the feasibility of shape completion under severe occlusion in realistic greenhouse environments, it does not propagate reconstruction uncertainty to risk-aware grasp filtering or object-level abstention based on geometric confidence.\nII-C\nUncertainty-Aware Shape Completion and Grasp Planning\nWhile shape completion enables reasoning beyond partial observations, uncertainty in the reconstructed geometry introduces additional challenges for downstream manipulation. Early work on uncertainty-aware grasping builds on analytical grasp quality measures such as force closure, first formalized by\nforce_closure_nguyen [\n18\n]\n, with\nferrari_canny [\n5\n]\nlater introducing the widely used epsilon quality metric. Building on this foundation,\nforce_closure_kehoe [\n11\n]\nintroduces one of the earliest formulations of grasp planning under shape uncertainty by estimating lower bounds on force-closure probability via MC sampling over parametric perturbations of a known object geometry. More recently,\ndifferentiable_force_closure [\n14\n]\nintroduces a differentiable force-closure estimator that enables efficient optimization of physically grounded grasp quality metrics under the assumption of known, deterministic object geometry. These formulations provide strong analytical guarantees, but either assume a fixed canonical object geometry or model uncertainty only through externally imposed parametric perturbations, and therefore do not address uncertainty arising from learned shape completion.\nSubsequent work has explored physically grounded grasp evaluation under uncertainty assuming known or approximately known object geometry. Methods addressing pose uncertainty\n[\n7\n]\nand model perturbations\n[\n13\n]\nanalyze the sensitivity of grasp stability to geometric variations, while more recent approaches incorporate uncertainty-aware grasp quality metrics to improve robustness\n[\n1\n]\n. While these methods provide principled uncertainty-aware grasp quality estimates, they assume access to a single underlying object geometry and do not address uncertainty arising from partial observability, which can lead to multiple incompatible completion hypotheses.\nMore closely related to our setting, learning-based approaches such as\nville_grasp_uncertain [\n15\n]\nuse MC dropout at inference time to sample multiple plausible shape completions from partial observations, evaluating grasps across samples to improve robustness. Similarly,\nmeasuring_uncertainty [\n4\n]\nshows that uncertainty-aware heuristics can improve grasp success rates by ranking grasps. In these works, uncertainty is treated as a perceptual signal or local heuristic, with dropout applied only at inference time and not to learn calibrated uncertainty during training, and is therefore not propagated into grasp feasibility bounds or object-level grasp decisions.\nUncertainty-aware grasp selection has also been explored in degraded sensing environments beyond terrestrial manipulation, such as underwater grasping\n[\n2\n]\n. While effective in their respective domains, these methods focus on uncertainty in perception or pose estimation rather than uncertainty arising from learned shape completion, where missing geometry can lead to multiple incompatible object hypotheses, and similarly do not consider object-level abstention.\nDirect quantitative comparison with prior uncertainty-aware grasping methods is fundamentally invalid due to a mismatch in problem formulation and decision scope between them and UNCLE-Grasp (Table\nI\n). Existing approaches may estimate uncertainty from sensing noise or learned shape completion, but typically apply it at the grasp level, using local penalties or re-ranking strategies while still assuming that a grasp must be executed for every detected object. In contrast, our method operates under severe partial observability, where geometric uncertainty arises from learned shape completion and a single observation can admit multiple incompatible object hypotheses. This setting requires object-level decisions about whether a grasp should be attempted at all, which we address by aggregating feasibility across plausible completions and abstaining when geometric confidence is insufficient. Because prior benchmarks assume mandatory grasp execution and do not support object-level abstention under learned geometric ambiguity, they cannot meaningfully evaluate the risk-aware behavior that is central to our approach.\nIII\nUncertainty-Aware Grasping Pipeline\nWe propose an uncertainty-aware grasping framework designed to handle severe occlusion in agricultural environments. Rather than relying on a single deterministic estimate of object geometry, our pipeline propagates geometric uncertainty from stochastic shape completion through to a risk-aware, physically grounded grasp decision process based on LCB. While we demonstrate this approach using specific networks, the architecture is modular and compatible with any probabilistic completion or grasp synthesis model. The overall pipeline is illustrated in Fig.\n2\n, and the algorithm is detailed in Appendix\n1\n.\nIII-A\nPoint Cloud Completion with Dropout\nPartial and occluded point clouds are completed using PointAttN\n[\n23\n]\n, a transformer-based point cloud completion network that reconstructs missing geometry. Given a partial observation\nùí´\n\\mathcal{P}\n, the network produces a completed point cloud\nùí´\ncompleted\n\\mathcal{P}_{\\text{completed}}\n.\nTo estimate geometric uncertainty under occlusion, we employ MC dropout in the completion network. Dropout is enabled during training, following the Bayesian interpretation of dropout as variational inference\n[\n6\n]\n, to obtain well-calibrated uncertainty estimates. At inference time, enabling dropout yields\nK\nK\nstochastic completions,\n{\nùí´\ncompleted\n(\nk\n)\n}\nk\n=\n1\nK\n,\n\\{\\mathcal{P}_{\\text{completed}}^{(k)}\\}_{k=1}^{K},\n(1)\nfrom which we estimate per-point geometric uncertainty by computing the standard deviation of reconstructed point positions across samples.\nThis stochastic completion step directly supports our first contribution by explicitly modeling geometric uncertainty under occlusion and provides the basis for uncertainty-aware grasp generation and filtering, described next.\nIII-B\nGrasp Generation and Filtering\nIII-B\n1\nGrasp Generation\nFor each completed point cloud\nùí´\ncompleted\n(\nk\n)\n\\mathcal{P}_{\\text{completed}}^{(k)}\n, we generate a set of candidate grasps using CGNet, which predicts 6-DoF grasp poses\nùêÜ\ni\n(\nk\n)\n‚àà\nSE\n‚Äã\n(\n3\n)\n\\mathbf{G}_{i}^{(k)}\\in\\mathrm{SE}(3)\nalong with per-grasp confidence scores\ns\ni\n(\nk\n)\ns_{i}^{(k)}\n.\nFormally, grasp generation is given by\n{\nùêÜ\ni\n(\nk\n)\n,\ns\ni\n(\nk\n)\n}\ni\n=\n1\nM\nk\n=\nCGNet\n‚Äã\n(\nùí´\ncompleted\n(\nk\n)\n)\n,\n\\{\\mathbf{G}_{i}^{(k)},s_{i}^{(k)}\\}_{i=1}^{M_{k}}=\\mathrm{CGNet}(\\mathcal{P}_{\\text{completed}}^{(k)}),\n(2)\nwhere\ni\ni\nindexes grasp candidates from 1 to\nM\nM\n, number of grasps generated, for each completion sample\nk\nk\n.\nIII-B\n2\nGrasp Filtering Pipeline\nFor each sample\nk\n‚àà\n{\n1\n,\n‚Ä¶\n,\nK\n}\nk\\in\\{1,\\dots,K\\}\n, we apply a multi-stage geometric and uncertainty-based filtering procedure:\nGlobal Uncertainty Filter\nTo avoid grasp attempts on strawberries with highly ambiguous geometry, we apply a global uncertainty filter at the object level, as visualized in Grasp Filtering a) in Fig.\n2\n. Using MC dropout, we generate\nK\nK\ncompleted shapes per strawberry and compute a global geometric uncertainty measure that captures reconstruction consistency across samples. If the mean standard deviation of the points in the completion exceeds a threshold\nŒ¥\nglobal\n\\delta_{\\text{global}}\n, the strawberry is rejected and no grasp is attempted, enabling safe abstention under high ambiguity.\nLocal Uncertainty Filter\nFor strawberries that pass the global uncertainty filter, we apply a local uncertainty filter at the grasp level. For each candidate grasp, we evaluate uncertainty in the contact regions by measuring the standard deviation of completed points within the corresponding grasp slice, as shown in Grasp Filtering b) in Fig.\n2\n. Grasps whose local uncertainty exceeds a threshold\nŒ¥\nlocal\n\\delta_{\\text{local}}\nare discarded, preventing reliance on unstable or inconsistent surface regions even when the overall object-level uncertainty is low.\nApproach Direction Filter\nA grasp is rejected if its approach vector (the negative\nz\nz\n-axis of the grasp frame) does not approach the strawberry from its front, such as the sideways grasp in Grasp Filtering c) in Fig.\n2\n, which could damage the fruit and the surrounding plant. The front approach filter can be formulated as\ngrasp passes\n‚áî\nùêö\ni\n‚ãÖ\nùêü\n‚â•\nŒ∏\ndot\n,\n\\text{grasp passes}\\iff\\mathbf{a}_{i}\\cdot\\mathbf{f}\\geq\\theta_{\\text{dot},}\n(3)\nwhere\nùêö\ni\n=\nùêÜ\ni\n[\n0\n:\n3\n,\n2\n]\n\\mathbf{a}_{i}=\\mathbf{G}_{i}[0:3,2]\nis the approach vector,\nùêü\n\\mathbf{f}\nis the front-facing direction, and\nŒ∏\ndot\n\\theta_{\\text{dot}}\nis the front-facing constraint‚Äôs threshold.\nVertical Grasp Filter\nGrasps with jaws aligned vertically (above each other), as shown in Grasp Filtering c) in Fig.\n2\n, are rejected as unstable and formulated as\ngrasp rejected\n‚áî\n|\nùêÜ\ni\n[\n0\n:\n3\n,\n0\n]\n‚ãÖ\nùê≥\n|\n>\nŒ∏\nvert\n,\n\\text{grasp rejected}\\iff|\\mathbf{G}_{i}[0:3,0]\\cdot\\mathbf{z}|>\\theta_{\\text{vert}},\n(4)\nwhere\nùê≥\n\\mathbf{z}\nis the world vertical axis and\nŒ∏\nvert\n\\theta_{\\text{vert}}\nis the vertically oriented grasp threshold. This vertical orientation constraint reflects the physical instability of vertically stacked finger configurations when grasping soft, approximately elliptical objects such as strawberries, which tend to slip in such orientations. The absolute value ensures that grasps are rejected whether the X-axis points up or down.\nJaw-Object Intersection Test\nFor each grasp, we verify that the gripper jaws do not intersect the object through a geometric clearance check to make sure the gripper jaws will not harm the strawberries as illustrated in Grasp Filtering d) in Fig.\n2\n.\nThe jaw lines are\nùêã\nleft\n\\displaystyle\\mathbf{L}_{\\text{left}}\n:\nùêú\n‚àí\nw\n2\n‚Äã\nùê±\n+\nt\n‚Äã\nùêö\n\\displaystyle:\\mathbf{c}-\\frac{w}{2}\\mathbf{x}+t\\mathbf{a}\n(5)\nùêã\nright\n\\displaystyle\\mathbf{L}_{\\text{right}}\n:\nùêú\n+\nw\n2\n‚Äã\nùê±\n+\nt\n‚Äã\nùêö\n,\n\\displaystyle:\\mathbf{c}+\\frac{w}{2}\\mathbf{x}+t\\mathbf{a},\n(6)\nwhere\nùêú\n=\nùêÜ\ni\n[\n0\n:\n3\n,\n3\n]\n\\mathbf{c}=\\mathbf{G}_{i}[0:3,3]\nis the grasp center,\nùê±\n=\nùêÜ\ni\n[\n0\n:\n3\n,\n0\n]\n\\mathbf{x}=\\mathbf{G}_{i}[0:3,0]\nis the jaw-opening direction,\nw\nw\nis the gripper width,\nùêö\n\\mathbf{a}\nis the approach direction, and\nt\nt\nis the jaw length.\nA grasp is considered valid if no point in\nùí´\ncompleted\n\\mathcal{P}_{\\text{completed}}\nlies within a distance\nœÑ\n\\tau\nof either jaw line as formulated in\ngrasp passes\n‚áî\n‚àÄ\nùê©\n‚àà\nùí´\n:\nmin\n‚Å°\n(\nd\n‚Äã\n(\nùê©\n,\nùêã\nleft\n)\n,\nd\n‚Äã\n(\nùê©\n,\nùêã\nright\n)\n)\n>\nœÑ\n.\n\\text{grasp passes}\\iff\\forall\\mathbf{p}\\in\\mathcal{P}:\\min(d(\\mathbf{p},\\mathbf{L}_{\\text{left}}),d(\\mathbf{p},\\mathbf{L}_{\\text{right}}))>\\tau.\n(7)\nThis tolerance serves as a geometric clearance margin during collision checks against reconstructed point clouds, reflecting uncertainty in reconstruction accuracy and preventing gripper jaws from being placed too close to potentially inaccurate surface estimates.\nTogether, these filters realize the second contribution by eliminating unreliable grasps and enabling object-level abstention under high ambiguity. After filtering, only a subset\nM\n‚Ä≤\nM^{\\prime}\nof the original proposals\nM\nM\nremains for each completion sample,\n{\nùêÜ\nj\n(\nk\n)\n,\ns\nj\n(\nk\n)\n}\nj\n=\n1\nM\nk\n‚Ä≤\n‚äÜ\n{\nùêÜ\ni\n(\nk\n)\n,\ns\ni\n(\nk\n)\n}\ni\n=\n1\nM\nk\n,\n\\{\\mathbf{G}_{j}^{(k)},s_{j}^{(k)}\\}_{j=1}^{M^{\\prime}_{k}}\\subseteq\\{\\mathbf{G}_{i}^{(k)},s_{i}^{(k)}\\}_{i=1}^{M_{k}},\n(8)\nwhere\nM\n‚Ä≤\n‚â§\nM\nM^{\\prime}\\leq M\n. The remaining question is how to conservatively assess grasp feasibility across uncertain shape completions, which we address using physically grounded force-closure grasp quality metrics.\nIII-C\nGrasp Quality Metrics\nWe evaluate grasp feasibility using the force-closure metric, which characterizes a grasp‚Äôs ability to resist arbitrary external wrenches. Each contact contributes a friction cone parameterized by a friction coefficient\nŒº\n\\mu\n, and the resulting contact wrenches form a convex set in wrench space. We use the\nœµ\n\\epsilon\nmetric, defined as the radius of the largest ball centered at the origin contained within this convex hull, with larger values indicating greater grasp robustness.\nIII-C\n1\nContact Estimation\nFor each surviving grasp, we estimate the contact points by stepping along the approach direction until the jaw tips reach the point cloud surface. Given a grasp pose\nùêÜ\n\\mathbf{G}\nand jaw position trajectories, we compute contact points\n{\nùêú\nleft\n,\nùêú\nright\n}\n\\{\\mathbf{c}_{\\text{left}},\\mathbf{c}_{\\text{right}}\\}\nand their associated surface normals\n{\nùêß\nleft\n,\nùêß\nright\n}\n\\{\\mathbf{n}_{\\text{left}},\\mathbf{n}_{\\text{right}}\\}\nusing nearest-neighbor queries.\nIII-C\n2\nGrasp Epsilon Metric\nFor each contact pair, we compute the grasp quality using the epsilon metric (force closure measure). The friction cone is discretized into\nN\nd\n‚Äã\ni\n‚Äã\nr\nN_{dir}\ndirections, providing a conservative approximation that avoids overestimating grasp stability, particularly under uncertain contact conditions.\nThe grasp wrench matrix is constructed as\nùêñ\n=\n[\nùêÖ\n1\n‚ãØ\nùêÖ\n2\n‚Äã\nN\nd\n‚Äã\ni\n‚Äã\nr\nœÑ\n1\n‚ãØ\nœÑ\n2\n‚Äã\nN\nd\n‚Äã\ni\n‚Äã\nr\n]\n‚àà\n‚Ñù\n6\n√ó\n2\n‚Äã\nN\nd\n‚Äã\ni\n‚Äã\nr\n.\n\\mathbf{W}=\\begin{bmatrix}\\mathbf{F}_{1}&\\cdots&\\mathbf{F}_{2N_{dir}}\\\\\n\\mathbf{\\tau}_{1}&\\cdots&\\mathbf{\\tau}_{2N_{dir}}\\end{bmatrix}\\in\\mathbb{R}^{6\\times 2N_{dir}}.\n(9)\nEach column of\nùêñ\n\\mathbf{W}\nrepresents a 6D wrench. It consists of a contact force\nùêÖ\nj\n\\mathbf{F}_{j}\n, corresponding to one discretized direction of the friction cone, applied at contact point\nùêú\nj\n\\mathbf{c}_{j}\n. The resulting torque is given by\nùùâ\nj\n=\nùêú\nj\n√ó\nùêÖ\nj\n\\boldsymbol{\\tau}_{j}=\\mathbf{c}_{j}\\times\\mathbf{F}_{j}\n.\nThe epsilon metric can be interpreted as the distance from the origin to the closest supporting hyperplane of the convex hull of feasible contact wrenches. Epsilon is represented as\nœµ\n=\nmin\nj\n‚Å°\n|\nb\nj\n|\n‚Äñ\nùêö\nj\n‚Äñ\n2\n,\n\\epsilon=\\min_{j}\\frac{|b_{j}|}{\\|\\mathbf{a}_{j}\\|_{2}},\n(10)\nwhere\nùêö\nj\n‚ä§\n‚Äã\nùê∞\n+\nb\nj\n‚â§\n0\n\\mathbf{a}_{j}^{\\top}\\mathbf{w}+b_{j}\\leq 0\ndenotes the\nj\nj\n-th halfspace inequality describing the convex hull.\nAggregation Across Uncertain Completions\nTo use the\nœµ\n\\epsilon\nmetric as a feasibility signal under geometric uncertainty, we aggregate grasp quality estimates across MC-dropout completion samples. Letting\n{\nùêÜ\nj\n(\nk\n)\n,\ns\nj\n(\nk\n)\n}\nj\n=\n1\nM\n‚Ä≤\n\\{\\mathbf{G}_{j}^{(k)},s_{j}^{(k)}\\}_{j=1}^{M^{\\prime}}\ndenote the set of grasps that remain after filtering, we compute\nœµ\nk\n=\nœµ\nmetric\n‚Äã\n(\n‚ãÉ\ni\n‚àà\n{\nùêÜ\nj\n(\nk\n)\n,\ns\nj\n(\nk\n)\n}\nj\n=\n1\nM\n‚Ä≤\n{\nùêú\ni\n,\nleft\n,\nùêú\ni\n,\nright\n}\n)\n,\n\\epsilon_{k}=\\epsilon_{\\text{metric}}\\!\\left(\\bigcup_{i\\in\\{\\mathbf{G}_{j}^{(k)},s_{j}^{(k)}\\}_{j=1}^{M^{\\prime}}}\\{\\mathbf{c}_{i,\\text{left}},\\mathbf{c}_{i,\\text{right}}\\}\\right),\n(11)\nand set\nœµ\nk\n=\n0\n\\epsilon_{k}=0\nif the sample is globally rejected or yields no surviving grasps.\nFrom\n{\nœµ\nk\n}\nk\n=\n1\nK\n\\{\\epsilon_{k}\\}_{k=1}^{K}\nwe compute\nœµ\n¬Ø\n\\displaystyle\\bar{\\epsilon}\n=\n1\nK\n‚Äã\n‚àë\nk\n=\n1\nK\nœµ\nk\n,\n\\displaystyle=\\frac{1}{K}\\sum_{k=1}^{K}\\epsilon_{k},\n(12)\nœÉ\nœµ\n\\displaystyle\\sigma_{\\epsilon}\n=\n1\nK\n‚àí\n1\n‚Äã\n‚àë\nk\n=\n1\nK\n(\nœµ\nk\n‚àí\nœµ\n¬Ø\n)\n2\n.\n\\displaystyle=\\sqrt{\\frac{1}{K-1}\\sum_{k=1}^{K}(\\epsilon_{k}-\\bar{\\epsilon})^{2}}.\n(13)\nWe then define the LCB metric:\nLCB\n=\nœµ\n¬Ø\n‚àí\nz\nŒ±\n‚Äã\nœÉ\nœµ\n.\n\\mathrm{LCB}=\\bar{\\epsilon}-z_{\\alpha}\\sigma_{\\epsilon}.\n(14)\nA grasp is attempted only if\nLCB\n>\n0\n\\mathrm{LCB}>0\n, indicating that the grasp is expected to satisfy the force-closure margin with high confidence; otherwise, the system abstains. This criterion uses a one-sided lower bound, as we are specifically concerned with controlling the probability that the true grasp margin falls below zero (i.e., becomes infeasible), while any margin above zero is considered desirable. The confidence factor\nz\nŒ±\nz_{\\alpha}\nis adjusted based on the leaf occlusion, which relaxes the bound in easier (low-occlusion) cases and becomes stricter as occlusion increases.\nIV\nExperiments\nIV-A\nExperimental Setup\nIV-A\n1\nHardware and Simulation Environments\nThe system interfaces with an Intel RealSense D435i RGB-D camera mounted on a Unitree Z1 robotic arm, as shown in Fig.\n3\n. Physical robot experiments were conducted in a controlled indoor environment arranged to replicate an indoor greenhouse strawberry plantation, with multiple hanging fruits and varied occlusion patterns.\nAll physical robot experiments were conducted using offboard control from the same workstation used for simulation experiments (Intel x86_64 CPU with an NVIDIA RTX 5080 GPU). Simulation experiments were performed in NVIDIA Isaac Sim, replicating the physical robot setup, including the strawberry plant geometry, robot arm, gripper, and camera configuration. Identical robot hardware parameters and environmental conditions are used across simulation and physical experiments to ensure fair comparison across methods.\nFigure 3:\n(Left) Physical robot setup replicating an indoor greenhouse strawberry plantation. (Right) Simulated strawberry field in NVIDIA Isaac Sim. The strawberry plant, Unitree Z1 robotic arm, grasping mechanism, and Intel RealSense D435i RGB-D camera are annotated.\nIV-A\n2\nPreprocessing and Segmentation\nRaw RGB-D point clouds are cleaned by removing invalid values (NaNs and infinities) and statistical outliers beyond three standard deviations from the median. Strawberry instances are detected using a two-stage pipeline: YOLOv8\n[\n9\n]\nprovides coarse 2D bounding boxes, which are refined using SAM2\n[\n19\n]\nto obtain precise instance masks. Each mask is projected into 3D to extract a segmented partial point cloud\nùí´\n\\mathcal{P}\ncorresponding to an individual strawberry.\nIV-A\n3\nOcclusion Levels and Trial Protocol\nTo systematically evaluate grasping performance under partial observability, we synthetically occlude strawberry point clouds using the geometric leaf model described in Appendix\n-C\n, producing structured occlusions that resemble real leaf coverage. Experiments are conducted across five synthetically generated occlusion levels corresponding to empirical point removal rates of\n{\n\\displaystyle\\{\n0\n%\n,\n\\displaystyle 0\\%,\n(15)\n6.94\n¬±\n4.60\n%\n,\n\\displaystyle 94\\pm 60\\%,\n28.83\n¬±\n12.24\n%\n,\n\\displaystyle 883\\pm 224\\%,\n63.12\n¬±\n15.86\n%\n,\n\\displaystyle 312\\pm 586\\%,\n87.45\n¬±\n13.86\n%\n}\n.\n\\displaystyle 745\\pm 386\\%\\,\\}.\nFor each occlusion level, we perform ten trials in simulation, each consisting of grasp attempts on five strawberries, and five trials on the physical robot, each with grasp attempts on four strawberries. Across all three grasp selection strategies, this results in a total of 750 simulated grasp attempts and 300 physical robot grasp attempts.\nIV-B\nGrasp Selection Strategies\nTABLE II:\nEach row of the ablation methods indicates which components of the grasping pipeline are enabled.\nMethod\nShape\nCompletion\nCGNet\nGeometric\nFiltering\nUncertainty\nFiltering\nCGNet (Partial)\n‚úó\n‚úì\n‚úó\n‚úó\nCGNet + Geometry (Partial)\n‚úó\n‚úì\n‚úì\n‚úó\nCentroid (Completed)\n‚úì\n‚úó\n‚úó\n‚úó\nBaseline\n‚úì\n‚úì\n‚úó\n‚úó\nNo Dropout\n‚úì\n‚úì\n‚úì\n‚úó\nDropout\n‚úì\n‚úì\n‚úì\n‚úì\nAs Table\nII\nillustrates, we evaluate the effectiveness of uncertainty-aware grasp selection under partial occlusion through a controlled comparison of six grasp selection strategies:\nCGNet (Partial)\n,\nCGNet + Geometry (Partial)\n,\nCentroid (Completed)\n,\nBaseline\n,\nNo-Dropout\n, and\nDropout\n.\nIV-B\n1\nDiagnostic Ablations\nWe evaluate a small set of diagnostic ablations designed to isolate the causal contributions of individual pipeline components. These ablations are conducted in simulation only, as several variants deliberately remove safety mechanisms and would pose undue risk to the physical robot.\nAs summarized in the first three rows of Table\nII\n, we consider: (i)\nCGNet (Partial)\nwhere we apply CGNet directly to partial point clouds without shape completion, (ii)\nCGNet + Geometry (Partial)\nwhere we augment partial-point-cloud CGNet grasping with deterministic geometric filtering, and (iii)\nCentroid (Completed)\nwhere we implement centroid-based grasping on completed point clouds without CGNet. Together, these ablations disentangle the roles of shape completion, grasp generation, and geometric constraints under increasing occlusion.\nIV-B\n2\nBaseline\nThe baseline strategy applies CGNet to a single completed point cloud and selects the grasp with the highest predicted confidence score. No uncertainty modeling, geometric filtering, or abstention mechanism is applied, resulting in a fast but deterministic grasp selection policy.\nIV-B\n3\nNo-Dropout Selection\nThe No-Dropout strategy disables dropout during point cloud completion, and therefore does not produce uncertainty estimates. As a result, only deterministic geometric grasp filters (front-facing approach, non-vertical orientation, and jaw‚Äìobject intersection checks) are applied. Among the remaining grasps, the one with the highest CGNet confidence score is selected. This ablation isolates the effect of geometric filtering in the absence of uncertainty-aware reasoning.\nIV-B\n4\nDropout Selection (Uncertainty-Aware)\nThis strategy evaluates grasp feasibility across multiple MC-dropout completion samples and applies the geometric filters used in the No-Dropout strategy in addition to uncertainty-aware filtering and the LCB decision rule described in Subsection\nIII-C\n. Although repeated completion and grasp evaluation introduce additional computational cost, we apply standard engineering accelerations to ensure real-time feasibility on physical hardware; these affect runtime only and do not alter the underlying decision logic.\nPractical Considerations for Real-Time Deployment\nWhile jaw-object intersection checking contributes non-negligible cost, the dominant runtime overhead arises from repeated point cloud completion and grasp evaluation across MC dropout samples. In real-world deployment, we optimize the jaw-object intersection test using a hierarchical strategy:\n1.\nBounding box pre-filtering\n: Axis-aligned bounding boxes are constructed around each jaw line with padding\nœÑ\n\\tau\n, allowing rapid rejection of distant points.\n2.\nCandidate restriction\n: Point-to-line distance checks are performed only for points within the bounding boxes.\n3.\nVectorized computation\n: Remaining distance calculations are implemented using NumPy broadcasting, avoiding per-point Python loops.\nAdditional employed system-level optimizations:\n‚Ä¢\nParallel execution of MC dropout samples using thread pools.\n‚Ä¢\nKD-tree acceleration for nearest-neighbor queries during contact estimation.\n‚Ä¢\nReduced the friction cone discretization,\nN\nd\n‚Äã\ni\n‚Äã\nr\n‚àí\no\n‚Äã\np\n‚Äã\nt\nN_{dir-opt}\n, to lower the wrench computation cost.\nThese optimizations are not required in simulation, where execution time is less constrained, but are critical for achieving practical grasping rates on the physical robot.\nIV-C\nEvaluation Metrics\nWe evaluate performance using grasp success rate, defined as the fraction of successful grasps over all executed attempts. An attempt corresponds to a strawberry that is successfully detected and passes all filtering stages. A grasp is considered successful if the robot securely lifts the strawberry without slippage or collision. Additional qualitative failure case analysis is provided in Appendix\n-F\n. Grasp quality metrics based on contact geometry are used only internally for feasibility assessment rather than as evaluation criteria.\nTo isolate the effect of grasp evaluation, the grasp\nposition\nis fixed at the centroid of the completed point cloud to ensure inverse kinematics feasibility, while uncertainty is evaluated only in the predicted grasp\norientation\n. Consequently, the only difference between methods lies in how candidate grasps are evaluated and selected, allowing performance differences to be attributed directly to uncertainty-aware decision making rather than differences in perception or grasp synthesis.\nIV-D\nHyperparameters and Thresholds\nUnless otherwise stated, all hyperparameters are shared across methods and fixed for all experiments.\nCompletion and Grasp Generation\nWe use an MC dropout rate of\n0.1\n0.1\nwith\nK\n=\n20\nK=20\ncompletion samples per observation. For each completed shape,\nM\n=\n200\nM=200\ncandidate grasps are generated using CGNet.\nGeometric Constraints\nThe grasp approach direction constraint\nŒ∏\ndot\n=\n0.7\n\\theta_{\\text{dot}}=0.7\nenforces that grasps approach the fruit within approximately\n45\n‚àò\n45^{\\circ}\nof the camera-facing direction (\ncos\n‚Å°\n45\n‚àò\n‚âà\n0.707\n\\cos 45^{\\circ}\\approx 0.707\n), preventing risky approaches such as the rear or side that may collide with leaves. The vertical alignment constraint\nŒ∏\nvert\n=\n0.5\n\\theta_{\\text{vert}}=0.5\nlimits grasps to within\n60\n‚àò\n60^{\\circ}\nof the vertical axis\nùê≥\n=\n[\n0\n,\n0\n,\n1\n]\nT\n\\mathbf{z}=[0,0,1]^{T}\nto reduce fruit slippage and damage.\nGripper and Contact Parameters\nBased on the gripper specifications, the jaw width is set to\nw\n=\n0.04\n‚Äã\nm\nw=0.04\\,\\text{m}\n, and the jaw length is constrained to\nt\n‚àà\n[\n0\n,\n0.2\n]\n‚Äã\nm\nt\\in[0,0.2]\\,\\text{m}\n. A moderate friction coefficient\nŒº\n=\n0.5\n\\mu=0.5\nis used to reflect stable yet gentle grasps appropriate for delicate objects such as strawberries.\nTo account for depth sensing uncertainty, we use a conservative clearance margin of\nœÑ\n=\n5\n‚Äã\nmm\n\\tau=5\\,\\text{mm}\n. Prior work shows that RGB-D depth noise scales with distance, with standard deviations of approximately\n0.7\n0.7\n‚Äì\n0.8\n%\n0.8\\%\nof the measured depth\n[\n12\n]\n, corresponding to\n3\n3\n‚Äì\n6\n‚Äã\nmm\n6\\,\\text{mm}\nat the manipulation distances considered, motivating our choice of\nœÑ\n\\tau\n.\nThe friction cone at the contact points is discretized into\nN\nd\n‚Äã\ni\n‚Äã\nr\n=\n8\nN_{dir}=8\ndirections. In the optimized dropout variant, this is reduced to\nN\nd\n‚Äã\ni\n‚Äã\nr\n‚àí\no\n‚Äã\np\n‚Äã\nt\n=\n6\nN_{dir-opt}=6\nto improve computational efficiency without degrading performance.\nOcclusion-Dependent Confidence Scaling\nThe parameter\nŒ±\n‚àà\n{\n0.0\n,\n0.1\n,\n0.2\n,\n0.3\n,\n0.4\n}\n\\alpha\\in\\{0.0,0.1,0.2,0.3,0.4\\}\ncontrols the severity of synthetic leaf occlusion applied to the strawberry point clouds and corresponds to the occlusion levels described in Subsection\nIV-A\n3\n. To adapt the LCB to increasing occlusion severity, we adjust the confidence factor\nz\nz\nas a linear function of\nŒ±\n\\alpha\n,\nz\nŒ±\n‚àà\n{\n0.75\n,\n0.88\n,\n1.02\n,\n1.15\n,\n1.28\n}\n,\nz_{\\alpha}\\in\\{0.75,\\ 0.88,\\ 1.02,\\ 1.15,\\ 1.28\\},\nallowing stricter confidence requirements under increasing occlusion.\nUncertainty Threshold Selection\nDue to differences in sensor noise, depth quantization, and unmodeled physical effects, uncertainty distributions differ between simulation and the physical robot, making a single universal threshold impractical. We therefore select thresholds empirically for each domain. After preliminary evaluation, we set the global and local uncertainty thresholds to\nŒ¥\nglobal\n=\nŒ¥\nlocal\n=\n0.01\n\\delta_{\\text{global}}=\\delta_{\\text{local}}=0.01\nin simulation. For the physical robot experiments, we apply a stricter threshold of\nŒ¥\nglobal\n=\nŒ¥\nlocal\n=\n0.0037\n\\delta_{\\text{global}}=\\delta_{\\text{local}}=0.0037\nto account for increased sensing noise. These thresholds are fixed across all occlusion levels and trials.\nV\nResults and Discussion\nAs discussed in Section\nII\n, we do not perform direct quantitative comparison with prior uncertainty-aware grasping methods, as our approach reasons over learned shape ambiguity under severe occlusion and supports object-level abstention, resulting in a fundamentally different decision scope. Instead, we evaluate our method through controlled ablations designed to isolate the effects of individual pipeline components.\nV-A\nEffect of Occlusion on Uncertainty\n0\n6.94\n28.83\n63.12\n87.45\n0\n1\n1\n2\n2\n3\n3\n‚ãÖ\n10\n‚àí\n3\n\\cdot 10^{-3}\nOcclusion level (%)\nCompletion Uncertainty\nFigure 4:\nCompletion uncertainty under increasing occlusion.\nBars indicate mean uncertainty across MC-dropout samples and whiskers denote standard deviation.\nAs occlusion increases, the centroid of the partial point cloud can shift substantially by up to\n241.7\n%\n241.7\\%\n(Fig.\n1\n), demonstrating that partial observations alone do not provide reliable geometric cues for grasping. Fig.\n4\nfurther shows that completion uncertainty, measured as the mean standard deviation of MC-dropout shape completions, increases monotonically with occlusion, reflecting growing geometric ambiguity in the reconstructed shape. Under heavy occlusion, the Dropout model exhibits large variability across samples, indicating multiple incompatible shape hypotheses. To avoid overconfident grasp attempts under severe occlusion, we propagate completion uncertainty into grasp decision making and use an LCB-based attempt/abstain criterion.\nV-B\nPipeline Ablations\n0\n6.94\n28.83\n63.12\n87.45\n0\n0.2\n0.2\n0.4\n0.4\n0.6\n0.6\n0.8\n0.8\n1\n1\nCGNet (Partial)\nCGNet + Geometry (Partial)\nCentroid (Completed)\nOcclusion level (%)\nGrasp success rate\nFigure 5:\nSimulation-only ablations showing grasp success rate under increasing occlusion. Shape completion and geometric filtering improve robustness but remain insufficient under severe ambiguity. Values are reported as mean\n¬±\n\\pm\nstandard deviation over ten runs.\nAs summarized in Fig.\n5\n, CGNet applied directly to partial point clouds exhibits poor and inconsistent performance as occlusion increases (grey). This confirms that partial geometry alone provides insufficient information for reliable grasping in cluttered scenes.\nGeometric filtering further improves robustness by eliminating physically infeasible grasps, particularly at moderate occlusion levels. Nevertheless, geometric constraints degrade under severe occlusion since they rely only on the deterministic partial point cloud, indicating that geometric heuristics alone cannot resolve ambiguity arising from missing structure.\nIntroducing shape completion improves performance by recovering missing geometry, even when grasps are selected using simple heuristics such as centroid-based placement. However, completion alone is insufficient under heavy occlusion, as ambiguous reconstructions can still lead to overconfident and unstable grasps if uncertainty is not explicitly quantified and incorporated into the grasp decision process.\nV-C\nGrasp Success Under Occlusion\nTABLE III:\nGrasp success under varying occlusion levels in simulation and physical robot experiments. Reported values are mean\n¬±\n\\pm\nstandard deviation.\nOcclusion Percentage\nMethod\n0%\n6.94\n¬±\n4.60\n%\n6.94\\pm 4.60\\%\n28.83\n¬±\n12.24\n%\n28.83\\pm 12.24\\%\n63.12\n¬±\n15.86\n%\n63.12\\pm 15.86\\%\n87.45\n¬±\n13.86\n%\n87.45\\pm 13.86\\%\nReal Leaf (\n‚àº\n70\n%\n\\sim 70\\%\n)\nSimulation\nCGNet (Partial)\n0.342\n¬±\n0.087\n0.342\\pm 0.087\n0.348\n¬±\n0.110\n0.348\\pm 0.110\n0.417\n¬±\n0.233\n0.417\\pm 0.233\n0.380\n¬±\n0.092\n0.380\\pm 0.092\n0.337\n¬±\n0.093\n0.337\\pm 0.093\n‚Äì\nCGNet + Geometry (Partial)\n0.683\n¬±\n0.033\n0.683\\pm 0.033\n0.652\n¬±\n0.061\n0.652\\pm 0.061\n0.708\n¬±\n0.100\n0.708\\pm 0.100\n0.633\n¬±\n0.067\n0.633\\pm 0.067\n0.683\n¬±\n0.033\n0.683\\pm 0.033\n‚Äì\nCentroid (Completed)\n0.495\n¬±\n0.106\n0.495\\pm 0.106\n0.443\n¬±\n0.134\n0.443\\pm 0.134\n0.625\n¬±\n0.090\n0.625\\pm 0.090\n0.830\n¬±\n0.087\n0.830\\pm 0.087\n0.600\n¬±\n0.167\n0.600\\pm 0.167\n‚Äì\nBaseline\n0.580\n¬±\n0.227\n0.580\\pm 0.227\n0.585\n¬±\n0.263\n0.585\\pm 0.263\n0.647\n¬±\n0.197\n0.647\\pm 0.197\n0.690\n¬±\n0.247\n0.690\\pm 0.247\n0.760\n¬±\n0.107\n0.760\\pm 0.107\n‚Äì\nNo Dropout\n0.675\n¬±\n0.216\n0.675\\pm 0.216\n0.777\n¬±\n0.174\n0.777\\pm 0.174\n0.710\n¬±\n0.196\n0.710\\pm 0.196\n0.730\n¬±\n0.228\n0.730\\pm 0.228\n0.780\n¬±\n0.129\n0.780\\pm 0.129\n‚Äì\nDropout\n0.742\n¬±\n\\pm\n0.280\n1.000\n¬±\n\\pm\n0.000\n0.750\n¬±\n\\pm\n0.250\n0.850\n¬±\n\\pm\n0.213\n0.870\n¬±\n\\pm\n0.140\n‚Äì\nPhysical Robot\nBaseline\n0.500\n¬±\n0.293\n0.500\\pm 0.293\n0.683\n¬±\n0.291\n0.683\\pm 0.291\n0.617\n¬±\n0.332\n0.617\\pm 0.332\n0.683\n¬±\n0.186\n0.683\\pm 0.186\n0.600\n¬±\n0.255\n0.600\\pm 0.255\n0.300\n¬±\n0.400\n0.300\\pm 0.400\nNo Dropout\n0.500\n¬±\n0.274\n0.500\\pm 0.274\n0.733\n¬±\n0.389\n0.733\\pm 0.389\n0.600\n¬±\n0.374\n0.600\\pm 0.374\n0.467\n¬±\n0.400\n0.467\\pm 0.400\n0.483\n¬±\n0.170\n0.483\\pm 0.170\n0.367\n¬±\n0.371\n0.367\\pm 0.371\nDropout\n0.800\n¬±\n\\pm\n0.400\n1.000\n¬±\n\\pm\n0.000\n1.000\n¬±\n\\pm\n0.000\n0.950\n¬±\n\\pm\n0.100\n0.800\n¬±\n\\pm\n0.400\n0.517\n¬±\n\\pm\n0.410\nCompared to the three diagnostic ablations, the Baseline consistently achieves higher grasp success rates across occlusion levels, highlighting the importance of combining both shape completion and learned grasp synthesis. Methods that rely solely on partial observations (no completion model) or simple centroid-based grasping (no grasps generation model) fail under moderate to severe occlusion. This comparison demonstrates that recovering missing geometry and reasoning over learned grasp proposals are necessary prerequisites for robust grasping, even before accounting for uncertainty. The diagnostic ablations therefore motivate the Baseline as a meaningful reference point and establish that both completion and GraspNet are essential components of any effective pipeline under occlusion.\nThe No-Dropout method serves as an ablation that isolates the contribution of geometric filtering without uncertainty modeling. While it consistently outperforms the Baseline, a substantial gap remains relative to the full Dropout method. This gap indicates that geometric heuristics alone are insufficient under heavy occlusion. Instead, geometric filtering and uncertainty-aware selection play complementary roles: geometric constraints remove physically infeasible grasps, while uncertainty-based LCB filtering suppresses overconfident grasps arising from ambiguous perception. Robust performance under severe occlusion requires both components.\nDespite increased completion uncertainty under severe occlusion (Fig.\n4\n), the Dropout method consistently achieves higher grasp success rates than both Baseline and No-Dropout across all occlusion levels (Table\nIII\n). By avoiding grasp attempts on highly ambiguous strawberries, the method prevents unsafe interactions that would otherwise lead to failed grasps or fruit damage.\nVI\nLimitations and Future Works\nThe proposed pipeline remains slower than fully deterministic baselines despite practical optimizations, which may limit throughput in large-scale harvesting scenarios. In addition, the system relies on single-view RGB-D observations; under extreme occlusion, insufficient visible geometry can still lead to ambiguous completions. Grasp execution is evaluated using a fixed gripper model, which does not fully capture soft fruit compliance, friction variations, or dynamic effects.\nFuture work will focus on further reducing inference time, integrating active perception and next-best-view strategies to reduce uncertainty prior to grasping, and extending the approach to multi-view and multi-object harvesting. Additional strategies to further improve robustness include prioritizing the most visible or least uncertain fruits, whose removal could help clear occlusions, and actively manipulating leaves to reduce clutter.\nVII\nConclusion\nRobust robotic harvesting in agricultural environments requires reliable grasping under severe partial observability, where occlusions from leaves and neighboring fruit introduce substantial geometric uncertainty. In this work, we present an uncertainty-aware grasping pipeline for partially occluded strawberries that explicitly models uncertainty arising from learned shape completion and propagates it to grasp selection.\nBy combining transformer-based point cloud completion with MC dropout and a conservative LCB decision rule, the proposed method enables the robot to reason not only about grasp quality but also about confidence in that estimate. Extensive simulation and physical robot experiments show that explicitly accounting for completion uncertainty improves grasp robustness under heavy occlusion. By abstaining from grasp attempts when geometric uncertainty is high, the system reduces the risk of unstable grasps, collisions, and fruit damage.\nReferences\n[1]\nWoo-Jeong Baek, Christoph Pohl, Philipp Pelcz, Torsten Kr√∂ger, and Tamim Asfour.\nImproving humanoid grasp success rate based on uncertainty-aware metrics and sensitivity optimization.\nIn\n2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids)\n, pages 786‚Äì793, 2022.\n[2]\nOnur Bagoren, Marc Micatka, Katherine¬†A. Skinner, and Aaron Marburg.\nPugs: Perceptual uncertainty for grasp selection in underwater environments, 2025.\n[3]\nAngela Dai, Charles¬†Ruizhongtai Qi, and Matthias Nie√üner.\nShape completion using 3d-encoder-predictor cnns and shape synthesis, 2017.\n[4]\nNuno¬†Ferreira Duarte, Seyed¬†S. Mohammadi, Plinio Moreno, Alessio¬†Del Bue, and Jose Santos-Victor.\nMeasuring uncertainty in shape completion to improve grasp quality, 2025.\n[5]\nC.¬†Ferrari and J.¬†Canny.\nPlanning optimal grasps.\nIn\nProceedings 1992 IEEE International Conference on Robotics and Automation\n, pages 2290‚Äì2295 vol.3, 1992.\n[6]\nYarin Gal and Zoubin Ghahramani.\nDropout as a bayesian approximation: Representing model uncertainty in deep learning, 2016.\n[7]\nKaijen Hsiao, Leslie¬†Pack Kaelbling, and Tom√°s Lozano-P√©rez.\nRobust grasping under object pose uncertainty.\nAutonomous Robots\n, 31(2):253‚Äì268, 2011.\n[8]\nMatthias Humt, Dominik Winkelbauer, and Ulrich Hillenbrand.\nShape completion with prediction of uncertain regions, 2023.\n[9]\nGlenn Jocher, Ayush Chaurasia, and Jing Qiu.\nUltralytics yolov8, 2023.\n[10]\nHamidreza Kasaei and Mohammadreza Kasaei.\nMvgrasp: Real-time multi-view 3d object grasping in highly cluttered environments, 2022.\n[11]\nBen Kehoe, Dmitry Berenson, and Ken Goldberg.\nToward cloud-based grasping with uncertainty in shape: Estimating lower bounds on achieving force closure with zero-slip push grasps.\nIn\n2012 IEEE International Conference on Robotics and Automation\n, pages 576‚Äì583, 2012.\n[12]\nKourosh Khoshelham and Sander¬†Oude Elberink.\nAccuracy and resolution of kinect depth data for indoor mapping applications.\nSensors\n, 12(2):1437‚Äì1454, 2012.\n[13]\nJunggon Kim, Kunihiro Iwamoto, James Kuffner, Yasuhiro Ota, and Nancy Pollard.\nPhysically based grasp quality evaluation under pose uncertainty.\nRobotics, IEEE Transactions on\n, 29:3258‚Äì3263, 04 2012.\n[14]\nTengyu Liu, Zeyu Liu, Ziyuan Jiao, Yixin Zhu, and Song-Chun Zhu.\nSynthesizing diverse and physically stable grasps with arbitrary hand structures using differentiable force closure estimator.\nIEEE Robotics and Automation Letters\n, 7(1):470‚Äì477, January 2022.\n[15]\nJens Lundell, Francesco Verdoja, and Ville Kyrki.\nRobust grasp planning over uncertain shape completions.\nIn\n2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, page 1526‚Äì1532. IEEE, November 2019.\n[16]\nFederico Magistri, Rodrigo Marcuzzi, Elias Marks, Matteo Sodano, Jens Behley, and Cyrill Stachniss.\nEfficient and accurate transformer-based 3d shape completion and reconstruction of fruits for agricultural robots.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 8657‚Äì8663, 2024.\n[17]\nJeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan¬†Aparicio Ojea, and Ken Goldberg.\nDex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics, 2017.\n[18]\nV.-D. Nguyen.\nConstructing force-closure grasps.\nIn\nProceedings. 1986 IEEE International Conference on Robotics and Automation\n, volume¬†3, pages 1368‚Äì1373, 1986.\n[19]\nNikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R√§dle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan¬†Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Doll√°r, and Christoph Feichtenhofer.\nSam 2: Segment anything in images and videos, 2024.\n[20]\nAndrea Rosasco, Stefano Berti, Fabrizio Bottarel, Michele Colledanchise, and Lorenzo Natale.\nTowards confidence-guided shape completion for robotic applications, 2022.\n[21]\nMartin Sundermeyer, Arsalan Mousavian, Rudolph Triebel, and Dieter Fox.\nContact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes\n, 2021.\n[22]\nAndreas ten Pas, Marcus Gualtieri, Kate Saenko, and Robert Platt.\nGrasp pose detection in point clouds, 2017.\n[23]\nJun Wang, Ying Cui, Dongyan Guo, Junxia Li, Qingshan Liu, and Chunhua Shen.\nPointattn: You only need attention for point cloud completion, 2022.\n[24]\nWentao Yuan, Tejas Khot, David Held, Christoph Mertz, and Martial Hebert.\nPcn: Point completion network, 2019.\n-A\nAlgorithm\nThe proposed UNCLE-Grasp pipeline is summarized in Algorithm\n1\n.\nAlgorithm 1\nUncertainty-aware strawberry grasping pipeline\n1:\nInput:\n2:\nRGB-D frame\n(\nI\nr\n‚Äã\ng\n‚Äã\nb\n,\nI\nd\n)\n(I_{rgb},I_{d})\n, camera model\nŒ†\n\\Pi\n3:\nCompletion network\nPointAttN\n, grasp network\nCGNet\n4:\nMode\n‚àà\n{\nBaseline\n,\nNoDropout\n,\nDropout\n}\n\\in\\{\\textsc{Baseline},\\textsc{NoDropout},\\textsc{Dropout}\\}\n5:\nMC samples\nK\nK\n, thresholds\nŒ∏\ndot\n,\nŒ∏\nvert\n,\nœÑ\n\\theta_{\\text{dot}},\\theta_{\\text{vert}},\\tau\n6:\nUncertainty thresholds\n(\nŒ¥\nglobal\n,\nŒ¥\nlocal\n)\n(\\delta_{\\text{global}},\\delta_{\\text{local}})\n, confidence factor\nz\nŒ±\nz_{\\alpha}\n7:\nOutput:\nAttempt\nor\nAbstain\n8:\nPerception and Segmentation\n9:\nùí´\ns\n‚Äã\nc\n‚Äã\ne\n‚Äã\nn\n‚Äã\ne\n‚Üê\nCleanPointCloud\n‚Äã\n(\nI\nd\n)\n\\mathcal{P}_{scene}\\leftarrow\\textsc{CleanPointCloud}(I_{d})\n10:\n{\nùí´\n(\nj\n)\n}\nj\n=\n1\nJ\n‚Üê\nDetectSegmentProject\n‚Äã\n(\nYOLOv8\n,\nSAM2\n,\nŒ†\n,\nùí´\ns\n‚Äã\nc\n‚Äã\ne\n‚Äã\nn\n‚Äã\ne\n)\n\\{\\mathcal{P}^{(j)}\\}_{j=1}^{J}\\leftarrow\\textsc{DetectSegmentProject}(\\text{YOLOv8},\\text{SAM2},\\Pi,\\mathcal{P}_{scene})\n11:\nfor\nj\n=\n1\n,\n‚Ä¶\n,\nJ\nj=1,\\dots,J\ndo\n12:\nùí´\n‚Üê\nùí´\n(\nj\n)\n\\mathcal{P}\\leftarrow\\mathcal{P}^{(j)}\n13:\nShape Completion\n14:\nif\nmode\n‚â†\n\\neq\nDropout\nthen\n15:\nùí´\nc\n‚Äã\no\n‚Äã\nm\n‚Äã\np\n‚Äã\nl\n‚Äã\ne\n‚Äã\nt\n‚Äã\ne\n‚Äã\nd\n‚Üê\nPointAttN\n‚Äã\n(\nùí´\n)\n\\mathcal{P}_{completed}\\leftarrow\\textsc{PointAttN}(\\mathcal{P})\n16:\nelse\n17:\nGenerate\n{\nùí´\nc\n‚Äã\no\n‚Äã\nm\n‚Äã\np\n‚Äã\nl\n‚Äã\ne\n‚Äã\nt\n‚Äã\ne\n‚Äã\nd\n(\nk\n)\n}\nk\n=\n1\nK\n\\{\\mathcal{P}_{completed}^{(k)}\\}_{k=1}^{K}\nusing MC dropout\n18:\nend\nif\n19:\nGrasp Generation\n20:\nif\nmode\n‚â†\n\\neq\nDropout\nthen\n21:\n{\nùêÜ\ni\n,\ns\ni\n}\ni\n=\n1\nM\n‚Üê\nCGNet\n‚Äã\n(\nùí´\nc\n‚Äã\no\n‚Äã\nm\n‚Äã\np\n‚Äã\nl\n‚Äã\ne\n‚Äã\nt\n‚Äã\ne\n‚Äã\nd\n)\n\\{\\mathbf{G}_{i},s_{i}\\}_{i=1}^{M}\\leftarrow\\textsc{CGNet}(\\mathcal{P}_{completed})\n22:\nelse\n23:\nfor\nk\n=\n1\n,\n‚Ä¶\n,\nK\nk=1,\\dots,K\ndo\n24:\n{\nùêÜ\ni\n(\nk\n)\n,\ns\ni\n(\nk\n)\n}\ni\n=\n1\nM\n‚Üê\nCGNet\n‚Äã\n(\nùí´\nc\n‚Äã\no\n‚Äã\nm\n‚Äã\np\n‚Äã\nl\n‚Äã\ne\n‚Äã\nt\n‚Äã\ne\n‚Äã\nd\n(\nk\n)\n)\n\\{\\mathbf{G}_{i}^{(k)},s_{i}^{(k)}\\}_{i=1}^{M}\\leftarrow\\textsc{CGNet}(\\mathcal{P}_{completed}^{(k)})\n25:\nend\nfor\n26:\nend\nif\n27:\nDeterministic Baselines\n28:\nif\nmode =\nBaseline\nthen\n29:\nExecute grasp with highest\ns\ni\ns_{i}\n30:\nreturn\nAttempt\n31:\nend\nif\n32:\nif\nmode =\nNoDropout\nthen\n33:\n{\nùêÜ\nj\n,\ns\nj\n}\nj\n=\n1\nM\n‚Ä≤\n‚Üê\nGeomFilter\n‚Äã\n(\n{\nùêÜ\ni\n,\ns\ni\n}\ni\n=\n1\nM\n,\nùí´\nc\n‚Äã\no\n‚Äã\nm\n‚Äã\np\n‚Äã\nl\n‚Äã\ne\n‚Äã\nt\n‚Äã\ne\n‚Äã\nd\n)\n\\{\\mathbf{G}_{j},s_{j}\\}_{j=1}^{M^{\\prime}}\\leftarrow\\textsc{GeomFilter}(\\{\\mathbf{G}_{i},s_{i}\\}_{i=1}^{M},\\mathcal{P}_{completed})\n34:\nif\nM\n‚Ä≤\n=\n0\nM^{\\prime}=0\nthen\n35:\nreturn\nAbstain\n36:\nelse\n37:\nùêÜ\nselected\n‚Üê\narg\n‚Å°\nmax\nj\n‚àà\n{\n1\n,\n‚Ä¶\n,\nM\n‚Ä≤\n}\n‚Å°\ns\nj\n\\mathbf{G}_{\\text{selected}}\\leftarrow\\arg\\max_{j\\in\\{1,\\dots,M^{\\prime}\\}}s_{j}\n38:\nreturn\nAttempt\n39:\nend\nif\n40:\nend\nif\n41:\nUncertainty-Aware Evaluation (Dropout)\n42:\nif\nmode =\nDropout\nthen\n43:\nif\nGlobalUnc\n‚Äã\n(\n{\nùí´\nc\n‚Äã\no\n‚Äã\nm\n‚Äã\np\n‚Äã\nl\n‚Äã\ne\n‚Äã\nt\n‚Äã\ne\n‚Äã\nd\n(\nk\n)\n}\nk\n=\n1\nK\n)\n>\nŒ¥\nglobal\n\\textsc{GlobalUnc}(\\{\\mathcal{P}_{completed}^{(k)}\\}_{k=1}^{K})>\\delta_{\\text{global}}\nthen\n44:\nreturn\nAbstain\n45:\nend\nif\n46:\nfor\nk\n=\n1\n,\n‚Ä¶\n,\nK\nk=1,\\dots,K\ndo\n47:\n{\nùêÜ\nj\n(\nk\n)\n,\ns\nj\n(\nk\n)\n}\nj\n=\n1\nM\nk\n‚Ä≤\n‚Üê\nLocalUnc+GeomFilter\n‚Äã\n(\n{\nùêÜ\ni\n(\nk\n)\n,\ns\ni\n(\nk\n)\n}\ni\n=\n1\nM\nk\n,\nùí´\nc\n‚Äã\no\n‚Äã\nm\n‚Äã\np\n‚Äã\nl\n‚Äã\ne\n‚Äã\nt\n‚Äã\ne\n‚Äã\nd\n(\nk\n)\n)\n\\{\\mathbf{G}_{j}^{(k)},{s}_{j}^{(k)}\\}_{j=1}^{M^{\\prime}_{k}}\\leftarrow\\textsc{LocalUnc+GeomFilter}(\\{\\mathbf{G}_{i}^{(k)},s_{i}^{(k)}\\}_{i=1}^{M_{k}},\\mathcal{P}_{completed}^{(k)})\n‚ä≥\n\\triangleright\nM\nk\n‚Ä≤\n‚â§\nM\nk\nM^{\\prime}_{k}\\leq M_{k}\nremaining grasps after local uncertainty and geometric filtering\n48:\nif\nM\nk\n‚Ä≤\n=\n0\nM^{\\prime}_{k}=0\nthen\n49:\nœµ\nk\n‚Üê\n0\n\\epsilon_{k}\\leftarrow 0\n50:\nelse\n51:\nEstimate contacts for\n{\nùêÜ\nj\n(\nk\n)\n}\nj\n=\n1\nM\nk\n‚Ä≤\n\\{\\mathbf{G}_{j}^{(k)}\\}_{j=1}^{M^{\\prime}_{k}}\n52:\nœµ\nk\n‚Üê\nœµ\nFC\n‚Äã\n(\n‚ãÖ\n)\n\\epsilon_{k}\\leftarrow\\epsilon_{\\text{FC}}(\\cdot)\n53:\nend\nif\n54:\nend\nfor\n55:\nCompute\nœµ\n¬Ø\n,\nœÉ\nœµ\n\\bar{\\epsilon},\\sigma_{\\epsilon}\n56:\nLCB\n‚Üê\nœµ\n¬Ø\n‚àí\nz\nŒ±\n‚Äã\nœÉ\nœµ\n\\mathrm{LCB}\\leftarrow\\bar{\\epsilon}-z_{\\alpha}\\sigma_{\\epsilon}\n57:\nif\nLCB\n‚â§\n0\n\\mathrm{LCB}\\leq 0\nthen\n58:\nreturn\nAbstain\n59:\nelse\n60:\nreturn\nAttempt\n61:\nend\nif\n62:\nend\nif\n63:\nend\nfor\n64:\nreturn\nAbstain\n-B\nData Collection\nWe used a mixture of simulation and physical robot data to train our completion model. For simulation data, we utilized NVIDIA Isaac Sim to create a virtual strawberry field with realistic occlusions and simulated an Intel RealSense D435i RGB-D camera from the robot‚Äôs perspective to capture partial point clouds of strawberries. This setup also provided complete point clouds from the simulator, which served as the ground truth for training and evaluation. Since no large-scale real-world dataset of occluded/complete strawberry pairs exists, the complete shapes were taken directly from Isaac Sim as ground truth in simulation.\nFor physical robot data collection, we captured RGB-D data using an Intel RealSense D435i camera in our lab environment, designed to replicate an indoor greenhouse strawberry plantation. The lab setup included five hanging strawberries with varied arrangements to simulate realistic harvesting conditions. The captured RGB-D data was processed to generate partial point clouds, which were manually annotated by aligning them with an approximated 3D CAD model. Both the simulation and physical robot environments, including the camera perspectives and strawberry field layouts, are shown in Fig.\n3\n.\n-C\nSynthetic Leaf Occlusion Modeling\nLeaf Placement\nA synthetic leaf is attached to one of the four lateral sides of the strawberry. The attachment axis\ns\n‚àà\n{\nx\n,\ny\n}\ns\\in\\{x,y\\}\nand sign\nœÉ\n‚àà\n{\n‚àí\n1\n,\n+\n1\n}\n\\sigma\\in\\{-1,+1\\}\nare sampled uniformly at random. The leaf center\nùêú\nleaf\n\\mathbf{c}_{\\text{leaf}}\nis positioned at the corresponding boundary of the bounding box:\nùêú\nleaf\n=\n{\n(\nx\nmax\n,\ny\nc\n,\nz\nc\n)\n,\nif\n‚Äã\ns\n=\nx\n,\nœÉ\n=\n+\n1\n(\nx\nmin\n,\ny\nc\n,\nz\nc\n)\n,\nif\n‚Äã\ns\n=\nx\n,\nœÉ\n=\n‚àí\n1\n(\nx\nc\n,\ny\nmax\n,\nz\nc\n)\n,\nif\n‚Äã\ns\n=\ny\n,\nœÉ\n=\n+\n1\n(\nx\nc\n,\ny\nmin\n,\nz\nc\n)\n,\nif\n‚Äã\ns\n=\ny\n,\nœÉ\n=\n‚àí\n1\n,\n\\mathbf{c}_{\\text{leaf}}=\\begin{cases}(x_{\\max},y_{c},z_{c}),&\\text{if }s=x,\\ \\sigma=+1\\\\\n(x_{\\min},y_{c},z_{c}),&\\text{if }s=x,\\ \\sigma=-1\\\\\n(x_{c},y_{\\max},z_{c}),&\\text{if }s=y,\\ \\sigma=+1\\\\\n(x_{c},y_{\\min},z_{c}),&\\text{if }s=y,\\ \\sigma=-1\\end{cases},\n(16)\nwhere\n(\nx\nc\n,\ny\nc\n,\nz\nc\n)\n(x_{c},y_{c},z_{c})\ndenotes the bounding-box center.\nThe leaf normal\nùêß\n\\mathbf{n}\npoints inward toward the object center, and two orthonormal vectors\nùêö\n1\n\\mathbf{a}_{1}\nand\nùêö\n2\n\\mathbf{a}_{2}\nspan the leaf plane.\nLeaf Geometry\nThe leaf footprint is modeled as an ellipse lying in the plane spanned by\nùêö\n1\n\\mathbf{a}_{1}\nand\nùêö\n2\n\\mathbf{a}_{2}\n. We introduce a scalar occlusion parameter\nŒ±\n\\alpha\nthat controls the relative size of the synthetic leaf to indicate the occlusion severity.\nThe major and minor axes of the ellipse are defined as\na\n=\nŒ±\n‚ãÖ\nd\n,\nb\n=\nŒ±\nm\n‚Äã\na\n‚Äã\nx\n‚Äã\na\n.\na=\\alpha\\cdot d,\\qquad b=\\alpha_{max}a.\n(17)\nFor each point\nùê©\ni\n\\mathbf{p}_{i}\n, we compute its coordinates in the leaf reference frame:\nùê´\ni\n\\displaystyle\\mathbf{r}_{i}\n=\nùê©\ni\n‚àí\nùêú\nleaf\n,\n\\displaystyle=\\mathbf{p}_{i}-\\mathbf{c}_{\\text{leaf}},\n(18)\nu\ni\n\\displaystyle u_{i}\n=\nùê´\ni\n‚ãÖ\nùêö\n1\n,\nv\ni\n=\nùê´\ni\n‚ãÖ\nùêö\n2\n,\nd\ni\n=\nùê´\ni\n‚ãÖ\nùêß\n.\n\\displaystyle=\\mathbf{r}_{i}\\cdot\\mathbf{a}_{1},\\quad v_{i}=\\mathbf{r}_{i}\\cdot\\mathbf{a}_{2},\\quad d_{i}=\\mathbf{r}_{i}\\cdot\\mathbf{n}.\n(19)\nA point lies within the elliptical footprint if\n(\nu\ni\na\n)\n2\n+\n(\nv\ni\nb\n)\n2\n‚â§\n1\n.\n\\left(\\frac{u_{i}}{a}\\right)^{2}+\\left(\\frac{v_{i}}{b}\\right)^{2}\\leq 1.\n(20)\nLeaf Thickness\nTo model the finite thickness of real leaves, a point is considered occluded only if it also lies within a slab of thickness\nt\nleaf\nt_{\\text{leaf}}\nalong the normal direction:\n|\nd\ni\n|\n‚â§\nt\nleaf\n.\n|d_{i}|\\leq t_{\\text{leaf}}.\n(21)\nOcclusion Mask\nA point\nùê©\ni\n\\mathbf{p}_{i}\nis removed from the point cloud if it satisfies both of the following footprint and thickness constraints:\nùê©\ni\n‚Äã\noccluded\n‚áî\n(\nu\ni\na\n)\n2\n+\n(\nv\ni\nb\n)\n2\n‚â§\n1\n‚àß\n|\nd\ni\n|\n‚â§\nt\nleaf\n.\n\\mathbf{p}_{i}\\text{ occluded }\\iff\\left(\\frac{u_{i}}{a}\\right)^{2}+\\left(\\frac{v_{i}}{b}\\right)^{2}\\leq 1\\;\\land\\;|d_{i}|\\leq t_{\\text{leaf}}.\n(22)\nThe resulting occluded point cloud is given by\nùí´\nocc\n=\n{\nùê©\ni\n‚àà\nùí´\n‚à£\nùê©\ni\n‚Äã\nnot occluded\n}\n.\n\\mathcal{P}_{\\text{occ}}=\\{\\mathbf{p}_{i}\\in\\mathcal{P}\\mid\\mathbf{p}_{i}\\text{ not occluded}\\}.\n(23)\nResulting Occlusion Levels\nWhile\nŒ±\n\\alpha\ncontrols leaf geometry, the resulting fraction of removed points depends on object shape and placement, producing a range of realistic occlusion levels from fully visible fruit to extreme coverage. Increasing occlusion can substantially shift the partial point cloud centroid (Fig.\n1\n), motivating the use of shape completion rather than grasping directly from partial observations.\n-D\nTraining Details\nThe completion model was trained on 420 combined simulation and physical robot partial point cloud samples to enhance robustness and bridge the sim-to-real gap. All 67 validation and 50 test samples consist exclusively of physical robot data, ensuring evaluation under realistic sensing conditions.\nTraining used 1748 as a fixed seed, batch size of 16 for 400 epochs, with each point cloud uniformly sampled to 2,048 points. We employed the Adam optimizer with an initial learning rate of\n1\n√ó\n10\n‚àí\n4\n1\\times 10^{-4}\n, no weight decay, and applied a learning rate decay of 0.7 every 40 epochs.\nAt inference, the model predicts complete strawberry shapes directly from partial scans. Ground truth is not used for prediction but solely for evaluation (e.g., Chamfer Distance). The validation and test sets consist exclusively of physical robot data, ensuring that performance reflects realistic deployment scenarios. While physical robot annotations inevitably include minor manual imperfections, exposure to such noise during training improves the model‚Äôs robustness in practice.\nFor grasp pose generation, we employ the publicly available CGNet model in an off-the-shelf manner. At inference, CGNet takes the completed point cloud predicted by our shape completion network and outputs a set of candidate grasp orientations and approach directions.\n-E\nVision Modules\nWe trained a YOLOv8\n[\n9\n]\nmodel for strawberry detection using RGB images collected from both the simulation and physical robot environments. The training dataset was augmented with variations in lighting, occlusion, and strawberry orientations to improve generalization.\nAlthough SAM2 is computationally expensive, applying it only within YOLO-predicted bounding boxes keeps the pipeline practical, which is sufficient for mobile harvesting robots where precision outweighs high frame rate. For each detected strawberry, the 2D mask is projected onto the 3D point cloud to extract the fruit-specific point cloud,\nùí´\n\\mathcal{P}\n, which we call the segmented partial point cloud of each strawberry. In our implementation, YOLOv8 and SAM2 require an average of 0.30‚Äâs and 0.22‚Äâs per frame, respectively, resulting in a total perception time well within the operational requirements of mobile harvesting robots.\nThe detected strawberries are ordered by the Euclidean distance between their estimated centers and the robot end-effector, and processed sequentially from nearest to farthest. This prioritization favors reachable targets and improves robustness under time and computational constraints.\n-F\nFailure Case Analysis\nDespite the overall improvement in grasp success achieved by the proposed uncertainty-aware pipeline, failures still occur under certain conditions. Analyzing these failure cases provides insight into the remaining limitations of the system and highlights directions for future improvement.\n-F\n1\nSevere Occlusion Beyond Completion Capability.\nIn cases of extreme leaf occlusion, the visible point cloud may contain insufficient geometric cues for reliable shape completion. Although the dropout-based method frequently abstains from grasping in such scenarios, occasional failures occur when multiple completion hypotheses converge to similarly incorrect shapes, leading to overconfident but inaccurate grasp estimates.\n-F\n2\nKinematic and Reachability Constraints.\nSome failures arise from the physical limitations of the robotic arm rather than perception errors. Strawberries located near the workspace boundary or partially occluded by rigid structures may admit valid grasps in perception space that are infeasible under inverse kinematics constraints, resulting in missed or unstable grasps.\n-F\n3\nExecution-Level Slippage and Compliance.\nA subset of failures occurs during execution, where soft fruit compliance and unmodeled friction variations cause the strawberry to slip from the gripper despite a geometrically valid grasp. These failures are not fully captured by the geometric and force-closure metrics used in this work.\nOverall, the majority of observed failures correspond to cases with high geometric uncertainty or physical execution constraints. Importantly, the proposed dropout-based strategy significantly reduces failure frequency by abstaining from grasp attempts in high-uncertainty scenarios, prioritizing safety over aggressive execution.\n-G\nCentroid Shift Under Occlusion\nTo quantify the geometric bias introduced by partial occlusion, we conduct an ablation study measuring the shift in the estimated centroid of strawberry point clouds under increasing occlusion severity.\nFor each strawberry in the dataset (train/validation/test splits), we compute the centroid of the fully visible point cloud and compare it to the centroid obtained after applying synthetic leaf occlusion at each occlusion level described in Section\n-C\n. The centroid shift is defined as the Euclidean distance between the two centroids, normalized by the bounding-box diagonal of the fully visible point cloud.\nAcross the entire dataset, we observe an average centroid shift of\n1.86%\nwhen comparing the fully visible case to the most severe occlusion level. As occlusion increases, centroid estimates become increasingly biased toward the visible surface regions, consistent with the qualitative example shown in Fig.\n1\n.\nThis analysis highlights the fundamental limitation of centroid-based grasp localization under occlusion and motivates the use of shape completion and uncertainty-aware grasp selection. Even modest centroid shifts can result in grasp poses that are unreachable or unstable, particularly for small objects such as strawberries.\n-H\nInference Time\nTable\nIV\nreports inference time for each module on the physical robot. While the Dropout method incurs additional computational cost due to Monte Carlo completion and grasp evaluation, optimizing the LCB aggregation stage reduces its runtime by over two orders of magnitude, resulting in an overall\n4\n√ó\n4\\times\nspeedup compared to the unoptimized pipeline. Although uncertainty-aware grasping remains slower than deterministic baselines, this overhead represents a deliberate trade-off for increased robustness and safer behavior in highly occluded physical robot environments.\nTABLE IV:\nAverage inference time per module.\nMethod\nModule\nTime (s)\nShared\nYOLO detection (per frame)\n0.30\nSAM segmentation (per frame)\n0.22\nPoint cloud completion (per strawberry)\n0.79\nCGNet (per strawberry)\n1.57\nBaseline\nGrasp selection and filtering\n0.01\nEnd-to-end (per strawberry)\n2.48\nNo-Dropout\nFiltering and selection\n9.71\nEnd-to-end (per strawberry)\n11.86\nDropout\nMC completion (20 forward passes)\n15.86\nCGNet (20 forward passes)\n37.72\nLCB aggregation (original)\n208.17\nEnd-to-end (original, per strawberry)\n230\nLCB aggregation (optimized)\n1.21\nEnd-to-end (optimized, per strawberry)\n57.53",
    "preview_text": "Robotic strawberry harvesting is challenging under partial occlusion, where leaves induce significant geometric uncertainty and make grasp decisions based on a single deterministic shape estimate unreliable. From a single partial observation, multiple incompatible 3D completions may be plausible, causing grasps that appear feasible on one completion to fail on another. We propose an uncertainty-aware grasping pipeline for partially occluded strawberries that explicitly models completion uncertainty arising from both occlusion and learned shape reconstruction. Our approach uses point cloud completion with Monte Carlo dropout to sample multiple shape hypotheses, generates candidate grasps for each completion, and evaluates grasp feasibility using physically grounded force-closure-based metrics. Rather than selecting a grasp based on a single estimate, we aggregate feasibility across completions and apply a conservative lower confidence bound (LCB) criterion to decide whether a grasp should be attempted or safely abstained. We evaluate the proposed method in simulation and on a physical robot across increasing levels of synthetic and real leaf occlusion. Results show that uncertainty-aware decision making enables reliable abstention from high-risk grasp attempts under severe occlusion while maintaining robust grasp execution when geometric confidence is sufficient, outperforming deterministic baselines in both simulated and physical robot experiments.\n\nUNCLE-Grasp: Uncertainty-Aware Grasping of Leaf-Occluded Strawberries\nMalak Mansour,\nAli Abouzeid,\nZezhou Sun,\nQinbo Sun,\nDezhen Song,\nAbdalla Swikir\nDepartment of Robotics, Mohamed bin Zayed University of Artificial Intelligence\nEmails:\n{malak.mansour, ali.abouzeid, zezhou.sun, qinbo.sun, dezhen.song, abdalla.swikir}@mbzuai.ac.ae\nAbstract\nRobotic strawberry harvesting is challenging under partial occlusion, where leaves induce significant geometric uncertainty and make grasp decisions based on a single deterministic sha",
    "is_relevant": true,
    "relevance_score": 3.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "ËØ•ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÁöÑËçâËéìÊäìÂèñÊñπÊ≥ïÔºåÈÄöËøáÁÇπ‰∫ëË°•ÂÖ®ÂíåËíôÁâπÂç°Ê¥õdropoutÂ§ÑÁêÜÈÉ®ÂàÜÈÅÆÊå°‰∏ãÁöÑÂá†‰Ωï‰∏çÁ°ÆÂÆöÊÄßÔºå‰ΩÜÊú™Áõ¥Êé•Ê∂âÂèäÂº∫ÂåñÂ≠¶‰π†„ÄÅVLA„ÄÅÊâ©Êï£Ê®°Âûã„ÄÅFlow Matching„ÄÅVLMÊàñÂÖ®Ë∫´ÊéßÂà∂„ÄÇ",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T21:27:18Z",
    "created_at": "2026-01-27T15:53:15.376173",
    "updated_at": "2026-01-27T15:53:15.376179",
    "recommend": 0
}