{
    "id": "2601.10707v1",
    "title": "See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection",
    "authors": [
        "Amir Mallak",
        "Erfan Aasi",
        "Shiva Sreeram",
        "Tsun-Hsuan Wang",
        "Daniela Rus",
        "Alaa Maalouf"
    ],
    "abstract": "ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼ŒåŸºäºåŸºç¡€æ¨¡å‹æå–çš„åŒºå—å¯¹é½ç‰¹å¾è¿›è¡Œè®­ç»ƒçš„ç­–ç•¥ï¼Œåœ¨åˆ†å¸ƒå¤–æ³›åŒ–æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚æˆ‘ä»¬å‡è®¾ï¼Œç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ä½œç”¨ï¼Œæ¯ä¸ªåŒºå—ç‰¹å¾éƒ½éšå¼åœ°åµŒå…¥äº†æ¥è‡ªæ‰€æœ‰å…¶ä»–åŒºå—çš„ä¿¡æ¯ï¼Œåªæ˜¯ä»¥ä¸åŒçš„æ–¹å¼å’Œå¼ºåº¦å‘ˆç°ï¼Œè¿™ä½¿å¾—è¿™äº›æè¿°ç¬¦é«˜åº¦å†—ä½™ã€‚æˆ‘ä»¬é€šè¿‡ä¸»æˆåˆ†åˆ†æå’Œè·¨åŒºå—ç›¸ä¼¼æ€§é‡åŒ–äº†æ­¤ç±»ï¼ˆBLIP2ï¼‰ç‰¹å¾ä¸­çš„å†—ä½™ï¼š90%çš„æ–¹å·®ä»…ç”±17/64ä¸ªä¸»æˆåˆ†æ•è·ï¼Œä¸”å¼ºè·¨ä»¤ç‰Œç›¸å…³æ€§æ™®éå­˜åœ¨ã€‚åœ¨è¿™ç§é‡å ä¿¡æ¯ä¸Šè®­ç»ƒä¼šå¯¼è‡´ç­–ç•¥è¿‡åº¦æ‹Ÿåˆè™šå‡ç›¸å…³æ€§ï¼Œä»è€ŒæŸå®³åˆ†å¸ƒå¤–é²æ£’æ€§ã€‚æˆ‘ä»¬æå‡ºéšæœºåŒºå—é€‰æ‹©æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°æ›´é²æ£’ã€æ›´æ³›åŒ–ä¸”æ›´é«˜æ•ˆçš„ç­–ç•¥ã€‚å¯¹äºæ¯ä¸€å¸§ï¼ŒSPSéšæœºæ©ç ä¸€éƒ¨åˆ†åŒºå—æè¿°ç¬¦ï¼Œä¸å°†å…¶è¾“å…¥ç­–ç•¥æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒå‰©ä½™åŒºå—çš„ç©ºé—´å¸ƒå±€ã€‚å› æ­¤ï¼Œç­–ç•¥æ¥æ”¶åˆ°çš„æ˜¯åŒä¸€åœºæ™¯çš„ä¸åŒéšæœºä½†å®Œæ•´çš„è§†å›¾ï¼šæ¯ä¸ªéšæœºåŒºå—å­é›†éƒ½åƒæ˜¯ä¸–ç•Œçš„ä¸€ä¸ªä¸åŒä½†åˆç†çš„è¿è´¯æŠ•å½±ã€‚å› æ­¤ï¼Œç­–ç•¥å°†å…¶å†³ç­–å»ºç«‹åœ¨é‚£äº›å¯¹å“ªäº›ç‰¹å®šä»¤ç‰Œä¿ç•™ä¸å˜çš„ç‰¹å¾ä¸Šã€‚å¤§é‡å®éªŒè¯å®ï¼Œåœ¨æ‰€æœ‰åˆ†å¸ƒå¤–åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡ä¼˜äºç°æœ‰æœ€ä½³æŠ€æœ¯ï¼Œå¹³å‡æå‡6.2%ï¼Œåœ¨é—­ç¯ä»¿çœŸä¸­æœ€é«˜æå‡20.4%ï¼ŒåŒæ—¶é€Ÿåº¦æå‡2.4å€ã€‚æˆ‘ä»¬å¯¹æ©ç ç‡å’ŒåŒºå—ç‰¹å¾é‡ç»„è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œè®­ç»ƒå¹¶è¯„ä¼°äº†9ä¸ªç³»ç»Ÿï¼Œå…¶ä¸­8ä¸ªè¶…è¶Šäº†å…ˆå‰çš„æœ€ä½³æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŒä¸€å­¦ä¹ ç­–ç•¥æ— éœ€ä»»ä½•è°ƒæ•´å³å¯è¿ç§»åˆ°ç‰©ç†ç°å®ä¸–ç•Œçš„æ±½è½¦ä¸Šã€‚",
    "url": "https://arxiv.org/abs/2601.10707v1",
    "html_url": "https://arxiv.org/html/2601.10707v1",
    "html_content": "See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection\nAmir Mallak\n1\nErfan Aasi\n2\nShiva Sreeram\n2\nTsun-Hsuan Wang\n2\nDaniela Rus\n2\nAlaa Maalouf\n1,2\n1\nUniversity of Haifa\n2\nCSAIL, MIT\nCorrespondance: mallak002@gmail.com\nAbstract\nRecent advances in end-to-end autonomous driving show that policies trained on patchâ€‘aligned features extracted from foundation models generalize better to Outâ€‘Ofâ€‘Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant.\nWe quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity:\n90\n%\n90\\%\nof variance is captured by\n17\n/\n64\n17/64\nprincipal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness.\nWe present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient.\nFor every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches.\nThus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive.\nExtensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a\n6.2\n%\n6.2\\%\naverage improvement and up to\n20.4\n%\n20.4\\%\nin closedâ€‘loop simulations, while being\n2.4\nÃ—\n2.4\\times\nfaster.\nWe conduct ablations over masking rates and patchâ€“feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA.\nFinally, we show that the same learned policy transfers to a physical, real-world car without any tuning.\nFigure 1\n:\nStochastic Patch Selection (SPS) in a nutshell.\n(1) We use large vision-language models to extract patch-level features for input images. (2) These descriptors are often highly redundant and correlated. (3) SPS randomly masks a subset of descriptors, forcing the policy to learn based on different subsets and less correlation, improving efficiency (\n2.4\nÃ—\n\\mathbf{2.4\\times}\nspeedup), generalization (\n+\n6.2\n%\n\\mathbf{+6.2\\%}\nperformance), and enabling plug-and-play integration with downstream policies. In (4), we compare speed vs performance on different variants of SPS against SOTA.\n1\nIntroduction and background\nAs autonomousâ€‘driving technology matures, endâ€‘toâ€‘end approaches emerged as a leading strategy\n[\n1\n,\n2\n]\nembodying a transformative design philosophy: a single model handles everything, from perception to control. This approach (i) dispenses with many of the rigid assumptions imposed on individual submodules, and (ii) evaluates and optimizes performance against a single overarching objective, yielding a more coherent and effective system.\nFrom foundation models to driving polices.\nRecently,\nWang etÂ al.\n[\n3\n]\nshowed that extracting patchâ€‘aligned (multimodal) features from a foundation model (FM)\n[\n4\n,\n5\n,\n6\n]\n, for each input frame, and training a lightweight policy on those features, markedly improves outâ€‘ofâ€‘distribution (OOD) generalization.\nTo obtain patch features, they applied\nmasked\nattention at a chosen layer (specifically, at the input to BLIP2â€™s Q-Former), so the mask guides the attention to focus on a single image patch. Repeating this operation for every patch yields a descriptor per patch.\nSince the descriptors come from the output layer that interfaces with the text encoder, they live in a shared visionâ€“language space, enabling text-driven augmentation and robust policy debugging.\nPatch-wise features are redundant.\nWe posit that patch descriptors extracted from ViT backbones are intrinsically redundant and correlated.\nBefore introducing the maskedâ€‘attention layer, every patch embedding produced by the sub-ViT backbone has already attended to all other patches, as selfâ€‘attention mixes information across tokens; each â€œpatch featureâ€ is no longer a purely local descriptor, as it carries a weighted summary of the entire scene, although the information is focused on this patch.\nThis redundancy is detrimental when we hand the descriptors to the policy head: (1) Correlated inputs inflate the effective dimensionality of the feature space, making it harder for the policy network to disentangle the truly discriminative signals. (2) Overlapping information encourages overâ€‘fitting; the policy may memorize spurious correlations present in the duplicated context instead of learning robust, spatially grounded cues. Finally, (3) unnecessary redundancy wastes computation and memory, both during training and at inference time.\n1.1\nOur contributions.\nWe first empirically show that patch-features are redundant in Section\n4.1\nvia similarity overlays, patch-wise correlations, and PCA explained-variance curves. Building on these insights, we propose\nStochastic-Patch-Selection (SPS)\n, a lightweight, plug-and-play patch-masking strategy that improves both the efficiency and OOD generalization of policy learning; See Figure\n1\n. Our main contributions are as follows.\n(i)\nSPS:\nAt each frame, SPS randomly drops a fixed proportion of patch descriptors, forwards the untouched descriptors to the policy network, and crucially leaves their grid positions unchanged. Each unmasked subset forms a coherent yet distinct stochastic view of the scene, encouraging the policy to focus on stable/invariant, causally relevant cues instead of spurious correlations.\n(ii)\nRobustness and efficiency gains over SOTA.\nAcross all OOD scenarios, SPS improves closedâ€‘loop driving success by an average of\n6.2\n%\n6.2\\,\\%\n, and up to\n20.4\n%\n20.4\\,\\%\n, while being\n2.4\nÃ—\n2.4\\times\n.\n(iii)\nComprehensive ablations at scale.\nWe train and evaluate 9 autonomousâ€‘driving systems under varied masking ratios and patchâ€‘reorganization schemes, identifying the design choices that maximize robustness; all variants except one surpassed the previous SOTA.\n(iv)\nSimâ€‘toâ€‘Real generalization.\nWe deployed SPS on a full-scale autonomous vehicle, showing a seamless transfer from simulation to a real car without tuning.\nNotably, as SPS operates in a shared visionâ€‘language space, it inherits from\n[\n3\n]\nthe support for textâ€‘conditioned perturbations/augmentation in the patches embedding space, yielding an additional\n1.7\n%\n1.7\\,\\%\nperformance boost.\n2\nRelated work\nEnd-to-end autonomous driving.\nEarly work established that neural networks can map raw sensory inputs directly to low-level control\n[\n1\n,\n2\n]\n, with subsequent efforts exploring probabilistic objectives and uncertainty-aware control\n[\n7\n]\n, stability/attention regularization\n[\n8\n]\n, and safety-aware formulations using control barrier functions\n[\n9\n,\n10\n]\n.\nWhile promising, end-to-end approaches typically demand large, diverse real-world datasets that are costly and risky to collect at scale\n[\n11\n,\n12\n]\n.\nTo mitigate this, photorealistic simulators like CARLA, AirSim, and Drake\n[\n13\n,\n14\n,\n15\n]\n, and trace-driven interactive simulation such as\nVISTA\n[\n12\n]\n, have become standard for training and evaluation. A complementary line replaces raw pixels with intermediate visual abstractions (affordances, semantics, lane topology) to simplify control\n[\n16\n,\n17\n,\n18\n]\n. In this space, â€œDrive Anywhereâ€\n[\n3\n]\nintroduced a\nfoundation-model\n-based pipeline that extracts rich, multimodal patch-aligned descriptors and feeds them to a lightweight policy head, showing strong OOD generalization. Our work builds on this FM-based formulation and aims to patch features\nreduce redundancy\nat train/inference time to improve OOD robustness without modifying or fine-tuning the FM.\nFoundation models in robotics and vision.\nLarge pretrained vision language models are increasingly used as general-purpose perception and reasoning backbones, from language-conditioned manipulation and planning\n[\n19\n,\n20\n,\n21\n,\n22\n,\n23\n]\n, video summarizations\n[\n24\n]\n, and 3D/open-world scene understanding\n[\n25\n,\n26\n,\n27\n]\nto navigation and instruction following\n[\n28\n]\nand follow-and-detect pipelines\n[\n29\n,\n30\n,\n31\n,\n32\n,\n33\n]\n.\nTheir cross-modal capacity also enables generative interfaces that tie vision and language\n[\n34\n,\n35\n,\n36\n,\n37\n]\n.\nWithin autonomous driving, some prior work cautions that the temporal reasoning robustness of off-the-shelf VLMs is limited in settings that require strong temporal understanding\n[\n38\n]\n; therefore, deploying them for driving tasks out of the box is not straightforward. This motivates learning downstream driving policies on top of features extracted from pretrained VLMs, rather than relying on end-to-end VLM reasoning. works increasingly leverage language-grounded or explainable representations/features/descriptors for introspection, learning, and counterfactual analysis\n[\n39\n,\n40\n,\n41\n,\n42\n,\n43\n,\n28\n,\n3\n]\n. We follow this trend in spirit, but use only frozen FMs (BLIP/BLIP-2\n[\n5\n,\n4\n]\n, DINO\n[\n44\n,\n45\n]\n) and intervene solely at the patch-descriptor interface to a small policy headâ€”isolating our stochastic selection from FM training dynamics.\nPositioning and novelty.\nPrior end-to-end policies\n[\n1\n,\n2\n,\n7\n,\n8\n,\n9\n,\n10\n]\nand FM-based driving frameworks\n[\n3\n,\n28\n]\nhave not treated\nfeature redundancy\nin FM patch descriptors as a first-class lever for OOD robustness. Such patch embeddings from pretrained models can be obtained by: (i)\nregion-first\npipelines that segment the image (e.g., SAM/Mask2Former) and then pool features within each mask by an FM encoder (e.g., CLIP)\n[\n46\n,\n47\n,\n48\n,\n49\n,\n50\n,\n51\n]\n. These methods inherit sensitivity to segmentation quality and add extra compute stages. (ii)\npatch-aligned\nextraction directly from the backbone without external masks or fine-tuning\n[\n52\n,\n3\n,\n28\n]\n, preserving spatial layout at token resolution and avoiding segmentation dependencies.\nOur contribution is orthogonal and complementary: we show that even strong patch-aligned descriptors,\nare highly redundant\n. We therefore introduce\nStochastic Patch Selection (SPS)\n, a simple mechanism to handle such correlations, via a\npost-FM\ntoken-selection step that preserves spatial layout, reduces token count, and improves closed-loop OOD performance while reducing latency.\nSPS is architecture-agnostic, requires no FM fine-tuning. Evidence from PCA and inter-token-correlation analyses, ablations over masking/reorganization variants, and real-world transfer leads to a simple takeaway: for FM-derived patch features in driving,\nless can be more\n: carefully injected stochastic sparsity improves generalization without any overhead.\n3\nMethod\nWe first provide the preliminaries.\n3.1\nPreliminaries\nEnd-to-end driving as feature-based control.\nWe model an autonomous-driving policy as a control function\nÏ•\n\\phi\nthat maps a perception stream\nF\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n3\nF\\!\\in\\!\\mathbb{R}^{H\\times W\\times 3}\nof RGB frames to steering and throttle commands\nu\n=\nÏ•\nâ€‹\n(\nF\n)\nu=\\phi(F)\n. Rather than operating on raw pixels, we feed\nÏ•\n\\phi\na tensor of patch-aligned representations\nF\nâ€²\n=\nDesc\nâ€‹\n(\nF\n)\nâˆˆ\nâ„\nH\nâ€²\nÃ—\nW\nâ€²\nÃ—\nD\nF^{\\prime}=\\texttt{Desc}(F)\\!\\in\\!\\mathbb{R}^{H^{\\prime}\\!\\times\\!W^{\\prime}\\!\\times\\!D}\nproduced by a multimodal foundation model\nDesc\n, where\n(\nH\nâ€²\n,\nW\nâ€²\n)\n(H^{\\prime},W^{\\prime})\nis the patch grid and\nD\nD\nis the channel dimension, i.e.,\nu\n=\nÏ•\nâ€‹\n(\nF\nâ€²\n)\nu=\\phi\\bigl(F^{\\prime}\\bigr)\n.\nWe now recall the masked-attention mechanism of\n[\n3\n]\n, which enforces spatially selective mixing.\nMultimodal patch-wise feature extraction\n[\n3\n]\n.\nLet\nDesc\nbe a vision transformer of\nL\nL\nlayers and\nN\n=\nH\nâ€²\nâ€‹\nW\nâ€²\nN=H^{\\prime}W^{\\prime}\nbe the number of non-overlapping patches.\nDuring a forward pass of\nDesc\non\nF\nF\n, the\nâ„“\n\\ell\n-th self-attention layer yields the query, key, and value matrices denoted by\nQ\nDesc\nâ€‹\n(\nF\n)\nâ„“\n,\nK\nDesc\nâ€‹\n(\nF\n)\nâ„“\nâˆˆ\nâ„\nN\nÃ—\nD\nk\nQ^{\\ell}_{\\texttt{Desc}(F)},K^{\\ell}_{\\texttt{Desc}(F)}\\in\\mathbb{R}^{N\\times D_{k}}\nand\nV\nDesc\nâ€‹\n(\nF\n)\nâ„“\nâˆˆ\nâ„\nN\nÃ—\nD\nV^{\\ell}_{\\texttt{Desc}(F)}\\in\\mathbb{R}^{N\\times D}\n, respectively. To compute the feature of patch\nj\nâˆˆ\n[\nN\n]\nj\\!\\in\\![N]\n,\nWang etÂ al.\n[\n3\n]\nintroduced (1) an attention mask\nm\n(\nj\n)\nâˆˆ\n[\n0\n,\n1\n]\nN\nm^{(j)}\\in[0,1]^{N}\n, where\nm\ni\n(\nj\n)\n=\n1\nm^{(j)}_{i}=1\npreserves information from patch\ni\ni\nand\nm\ni\n(\nj\n)\n=\n0\nm^{(j)}_{i}=0\nexcludes it, and (2) a parameter\nr\n<\n0\nr<0\ncontroling the suppression strength: the larger\n|\nr\n|\n\\lvert r\\rvert\n, the stronger the masking effect. Forming the similarity matrix\nG\nDesc\nâ€‹\n(\nF\n)\nâ„“\n=\nQ\nDesc\nâ€‹\n(\nF\n)\nâ„“\nâ€‹\nK\nDesc\nâ€‹\n(\nF\n)\nâ„“\nâŠ¤\nG^{\\ell}_{\\texttt{Desc}(F)}=Q^{\\ell}_{\\texttt{Desc}(F)}{K^{\\ell}_{\\texttt{Desc}(F)}}^{\\top}\n, the attention focus on the information defined by\nm\n(\nj\n)\nm^{(j)}\nis:\nG\nDesc\nâ€‹\n(\nF\n)\nâ„“\n~\n=\n\\displaystyle\\tilde{G^{\\ell}_{\\texttt{Desc}(F)}}=\nG\nDesc\nâ€‹\n(\nF\n)\nâ„“\n+\n(\nğŸ\nâˆ’\nM\n(\nj\n)\n)\nâ€‹\nr\n,\n\\displaystyle G^{\\ell}_{\\texttt{Desc}(F)}+\\bigl(\\mathbf{1}-M^{(j)}\\bigr)r,\n(1)\nwhere\nM\n(\nj\n)\n=\n[\nm\n(\nj\n)\n,\nâ€¦\n,\nm\n(\nj\n)\n]\nâŠ¤\n.\nM^{(j)}=[m^{(j)},\\dots,m^{(j)}]^{\\top}.\nThis operation drives attention scores in\nG\nDesc\nâ€‹\n(\nF\n)\nâ„“\n~\n\\tilde{G^{\\ell}_{\\texttt{Desc}(F)}}\nfor patches with\nm\ni\nâ‰ˆ\n0\nm_{i}\\approx 0\ndown to\nr\nr\n, thus masking them in the following softmax. The term\n(\nğŸ\nâˆ’\nM\n(\nj\n)\n)\n(\\mathbf{1}-M^{(j)})\nadds 0 when the mask is 1 and\nr\nr\nwhen it is near 0. The masked attention weights\nSoftMax\nâ€‹\n(\nğ’¢\n~\n)\n\\texttt{{SoftMax}}(\\tilde{\\mathcal{G}})\nyield the desired descriptor through the remaining layers:\nF\nâ€²\nâ£\n(\nj\n)\n=\nDesc\nâ„“\nâ†’\nâ€‹\n(\nSoftMax\nâ€‹\n(\nğ’¢\n~\n)\nâ€‹\nV\nDesc\nâ€‹\n(\nF\n)\nâ„“\n)\n.\nF^{\\prime(j)}\\;=\\;\\texttt{Desc}^{\\ell\\rightarrow}\\!\\bigl(\\texttt{{SoftMax}}(\\tilde{\\mathcal{G}})V^{\\ell}_{\\texttt{Desc}(F)}\\bigr).\n(2)\nRepeating this procedure for all\nj\nj\nreconstructs\nF\nâ€²\nâˆˆ\nâ„\nH\nâ€²\nÃ—\nW\nâ€²\nÃ—\nD\nF^{\\prime}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times D}\n.\nMask design.\nFor a patch\nj\nj\n, we set weight\nm\ni\n(\nj\n)\n=\nf\nâ€‹\n(\nd\ni\nâ€‹\nj\n)\nm^{(j)}_{i}=f\\!\\bigl(d_{ij}\\bigr)\nwhere\nd\ni\nâ€‹\nj\nd_{ij}\nis the distance in the patch grid\nâˆ¥\n(\nx\ni\n,\ny\ni\n)\nâˆ’\n(\nx\nj\n,\ny\nj\n)\nâˆ¥\n2\n\\lVert(x_{i},y_{i})-(x_{j},y_{j})\\rVert_{2}\n, and\nf\nf\nas a hard cutoff\nğŸ\nâ€‹\n[\nd\ni\nâ€‹\nj\nâ‰¤\nÎ±\n]\n\\mathbf{1}[d_{ij}\\leq\\alpha]\n, or soft decays\n2\nâˆ’\nd\ni\nâ€‹\nj\n2^{-d_{ij}}\nand\n1\n/\nd\ni\nâ€‹\nj\n1/d_{ij}\n, giving flexible locality.\n3.2\nRedundancy as low-rank structure\nFigure 2\n:\nSPS algorithm vs Drive-Anywhere illustration\n. From left to Right. 1st: Input images are processed through a frozen foundation model to produce patch-level descriptors. 2nd: In\nDrive-Anywhere\n, the full tensor is forwarded unchanged to the policy. 3rd: In our approach, we introduce two patch selection strategies (uniform stochastic and matrix-based probability selection), followed by (4th) a restructuring phase: either masking unselected descriptors or removing them and adjusting positional encodings. Both versions preserve spatial semantics while significantly reducing runtime. SPS improves efficiency by a factor of\n2.4\nÃ—\n\\mathbf{2.4\\times}\nwhile also boosting generalization.\nLet\nF\nc\nâˆˆ\nâ„\nN\nÃ—\nD\nF_{c}\\in\\mathbb{R}^{N\\times D}\ndenote the matrix of patch descriptors obtained by applying\nDesc\nto\nF\nF\n, where\nN\n=\nH\nâ€²\nâ€‹\nW\nâ€²\nN=H^{\\prime}W^{\\prime}\nis the number of patches and\nD\nD\nis the dimension of each descriptor (\nF\nc\nF_{c}\nis the reshaped\nF\nâ€²\nF^{\\prime}\nto matrix form).\nLe\nF\n~\nc\n\\tilde{F}_{c}\nbe the mean-subtracted matrix, and\nÎ»\n1\nâ‰¥\nâ‹¯\nâ‰¥\nÎ»\nr\n>\n0\n\\lambda_{1}\\geq\\cdots\\geq\\lambda_{r}>0\nbe the eigenvalues of the sample covariance\nÎ£\n=\n1\nN\nâ€‹\nF\n~\nc\nâŠ¤\nâ€‹\nF\n~\nc\n\\Sigma=\\tfrac{1}{N}\\tilde{F}_{c}^{\\top}\\tilde{F}_{c}\n, with rank\nr\nâ‰¤\nd\nr\\leq d\n. The cumulative explained variance after the first\nm\nâ‰¤\nr\nm\\leq r\ncomponents is\nE\nâ€‹\n(\nm\n)\n:=\nâˆ‘\ni\n=\n1\nm\nÎ»\ni\nâˆ‘\ni\n=\n1\nr\nÎ»\ni\n.\n\\displaystyle E(m):=\\frac{\\sum_{i=1}^{m}\\lambda_{i}}{\\sum_{i=1}^{r}\\lambda_{i}}.\n(3)\nA rapidly saturating\nE\nâ€‹\n(\nm\n)\nE(m)\nfor\nm\nâ‰ª\nd\nm\\ll d\nindicates a low-rank structure, hence redundancy among the patch descriptors.\nIn Section\n4.1\nwe empirically show that BLIP2 features reach\n90\n%\n90\\%\nexplained variance with\n17\n17\nof\n64\n64\ncomponents (\nE\nâ€‹\n(\n17\n)\n>\n0.9\nE(17)>0.9\n). Even more, when using half of the patches with the highest\nâ„“\n2\n\\ell_{2}\nnorm, the first\n14\n14\nprincipal components explain\n90\n%\n90\\%\nof the variance (\nE\nâ€‹\n(\n14\n)\n>\n0.9\nE(14)>0.9\n).\nA proof sketch and implementation details appear in Appx.\nA.1\n.\n3.3\nStochastic patch selection (SPS)\nReducing descriptor redundancy.\nAs we saw,\ninformation about a scene is implicitly replicated across all tokens. The resulting redundancy\n(i)Â inflates the computation needed to extract\nevery\npatch feature,\n(ii)Â encourages the policy to rely on spurious,\ncorrelated cues that collapse under distribution shift, and\n(iii)Â  forces the policy to require every tiny detail of the full scene to provide a robust decision.\nOur goal is therefore to\nexpose the policy\n, at every input frame, to a fraction of the patch descriptors and withhold the others, via a token-level stochastic selection mechanism operating in the latent space of the foundation model, while maintaining the spatial layout of the remaining patches. Each random subset forms a different, yet still coherent, projection of the same scene, giving the policy a stream of stochastic but complete views. Consequently, the controller is trained to base its decisions on features that remain invariant to which specific tokens survive, yielding improved OOD robustness and lower compute, all while preserving spatial coherence and encouraging the use of robust, generalizable signals over exact token identity.\nLet\nN\n=\nH\nâ€²\nâ€‹\nW\nâ€²\nN=H^{\\prime}W^{\\prime}\ndenote the number of patch positions produced by the\nbackbone\nDesc\n(see Sec.\n3.1\n).\nFix a\nsampling rate\nrate\nâˆˆ\n(\n0\n,\n1\n]\n\\textsc{rate}\\in(0,1]\n.\nFor every input frame\nt\nt\nwe:\n1.\nSample a subset.\nUniformly at random, select exactly\nk\n=\nâŒˆ\nrate\nâ‹…\nN\nâŒ‰\nk=\\lceil\\textsc{rate}\\cdot N\\rceil\npatch indices,\ndenoted\nÎ©\nt\nâŠ†\n{\n1\n,\nâ€¦\n,\nN\n}\n\\Omega_{t}\\subseteq\\{1,\\dots,N\\}\nwith\n|\nÎ©\nt\n|\n=\nk\n|\\Omega_{t}|=k\n.\n2.\nCompute descriptors.\nRun the maskedâ€“attention extractor\n(Sec.\n3.1\n)\nonly\nfor patches\ni\nâˆˆ\nÎ©\nt\ni\\in\\Omega_{t}\n,\nobtaining for every\ni\ni\n, the descriptor\nF\ni\nâ€²\nâˆˆ\nâ„\nD\nF^{\\prime}_{i}\\in\\mathbb{R}^{D}\n.\n3.\nBuild the sparse tensor.\nAssemble\nF\n~\nâ€²\nâˆˆ\nâ„\nH\nâ€²\nÃ—\nW\nâ€²\nÃ—\nD\n\\tilde{F}^{\\prime}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times D}\n:\nF\n~\ni\nâ€²\n=\n{\nF\ni\nâ€²\n,\ni\nâˆˆ\nÎ©\nt\n,\nğŸ\n,\notherwise\n,\n\\tilde{F}^{\\prime}_{i}=\\begin{cases}F^{\\prime}_{i},&i\\in\\Omega_{t},\\\\\n\\mathbf{0},&\\text{otherwise},\\end{cases}\n(4)\nthereby\npreserving the original spatial layout\nso the policy head receives a tensor\nof fixed size.\nComputational footprint.\nThe expected fraction of evaluated descriptors is\nrate\n, so feature extraction time scales linearly with the keep-rate.\nFor example,\nrate\n=\n0.4\n\\textsc{rate}=0.4\ncuts the forward pass of the foundation model ViT\nL\n{}_{\\text{L}}\nbackbone by\nâˆ¼\n2.5\nÃ—\n\\sim\\!2.5\\times\nwithout\naltering the policy architecture. The variant which fix the positional embedding at the VIT, also improve the runtime of the policy.\nOverall.\nSPS turns a dense, patch-aligned representation into a\nstochastically sparse\n, yet spatially coherent, token sequence. By training the control policy on these variable but complete\nprojections of the scene, we (1) reduce compute, (2) discourage reliance on redundant correlations, and (3) drive consistent gains in OOD generalization; see Section\n4.3\n.\n3.4\nAblations and variants\nTo understand the design space, we evaluate two additional variants of the above procedure.\n(v1) Threshold masking.\nInstead of sampling exactly\nk\nk\nindices,\ndraw an i.i.d. vector\nR\nâˆ¼\nğ’°\nâ€‹\n(\n0\n,\n1\n)\nH\nâ€²\nâ€‹\nW\nâ€²\nR\\sim\\mathcal{U}(0,1)^{H^{\\prime}W^{\\prime}}\nand define\nÎ©\nt\n=\n{\ni\nâˆ£\nR\ni\nâ‰¤\nrate\n}\n.\n\\Omega_{t}=\\bigl\\{i\\mid R_{i}\\leq\\textsc{rate}\\bigr\\}.\n(5)\nEach patch is sampled independently with\nprobability\nrate\n, so the expected keep-rate is\nrate\nbut the actual count fluctuates.\nThis adds stochastic diversity at negligible cost.\n(v2) Position-adjusted sparse sequence.\nAfter choosing\nÎ©\nt\n\\Omega_{t}\n(either by fixed-count or threshold\nmasking),\nomit\nthe dropped tokens entirely and feed the policy\nonly the descriptors that were actually computed. Each retained\ndescriptor\nF\ni\nâ€²\nF^{\\prime}_{i}\nis augmented with its original positional\nembedding\np\ni\nâˆˆ\nâ„\nD\np_{i}\\!\\in\\!\\mathbb{R}^{D}\ndrawn from the ViTâ€™s full\npositional-embedding table, so the controller still knows\nwhere\neach token came from. This yields a variable-length input sequence\nthat is shorter by a factor of\nrate\nwithout inserting any zero\nvectors.\nFigure\n2\nillustrates our method alongside its variants.\n3.5\nSPS preserves scene semantics\nSubspace preservation under uniform row sampling.\nIf the variance of\nF\nc\nF_{c}\nconcentrates in a few components, the patch descriptors (rows of\nF\nc\nF_{c}\n) lie near a low-dimensional subspace. If the row coherence is low, uniformly sampling enough rows preserves this principal subspace with high probability, hence the policy still receives descriptors that span the same semantics. We now show it formally.\nLemma 1\n(SPS preserves the row-space under low rank and bounded coherence)\n.\nLet\nF\nc\nâˆˆ\nâ„\nN\nÃ—\nd\nF_{c}\\in\\mathbb{R}^{N\\times d}\nbe a centered data matrix with\nrank\nâ¡\n(\nF\nc\n)\n=\nr\n\\operatorname{rank}(F_{c})=r\n, and let its (thin) singular value decomposition be\nF\nc\n=\nU\nr\nâ€‹\nÎ£\nr\nâ€‹\nV\nr\nâŠ¤\n,\nF_{c}\\;=\\;U_{r}\\Sigma_{r}V_{r}^{\\top},\nwhere\nU\nr\nâˆˆ\nâ„\nN\nÃ—\nr\nU_{r}\\in\\mathbb{R}^{N\\times r}\nand\nV\nr\nâˆˆ\nâ„\nd\nÃ—\nr\nV_{r}\\in\\mathbb{R}^{d\\times r}\nhave orthonormal columns. The orthogonal projector onto the row space of\nF\nc\nF_{c}\nis\nÎ \nF\n=\nV\nr\nâ€‹\nV\nr\nâŠ¤\n,\n\\Pi_{F}\\;=\\;V_{r}V_{r}^{\\top},\nand the row-space coherence is defined as\nÎ¼\nâ‰œ\nN\nr\nâ€‹\nmax\ni\nâˆˆ\n[\nN\n]\nâ¡\nâ€–\ne\ni\nâŠ¤\nâ€‹\nU\nr\nâ€–\n2\n2\nâˆˆ\n[\n1\n,\nN\n/\nr\n]\n.\n\\mu\\;\\triangleq\\;\\frac{N}{r}\\,\\max_{i\\in[N]}\\|e_{i}^{\\top}U_{r}\\|_{2}^{2}\\;\\in\\;[1,\\,N/r].\nLet\nâ„\nâŠ‚\n[\nN\n]\n\\mathcal{I}\\subset[N]\nbe a uniformly random subset of\nm\nm\nindices without replacement, and let\nF\nâ„\nâˆˆ\nâ„\nm\nÃ—\nd\nF_{\\mathcal{I}}\\in\\mathbb{R}^{m\\times d}\nbe the corresponding submatrix of\nF\nc\nF_{c}\n. Let\nÎ \nF\nâ„\n\\Pi_{F_{\\mathcal{I}}}\ndenote the orthogonal projector onto the row space of\nF\nâ„\nF_{\\mathcal{I}}\n. Then there exists a constant\nC\n>\n0\nC>0\nsuch that, for any\nÎµ\n,\nÎ´\nâˆˆ\n(\n0\n,\n1\n)\n\\varepsilon,\\delta\\in(0,1)\n, if\nm\nâ‰¥\nC\nâ€‹\nÎ¼\nâ€‹\nr\nÎµ\n2\nâ€‹\nlog\nâ¡\n(\nr\nÎ´\n)\n,\nm\\;\\geq\\;C\\,\\frac{\\mu\\,r}{\\varepsilon^{2}}\\,\\log\\!\\Bigl(\\frac{r}{\\delta}\\Bigr),\n(6)\nthe following holds with probability at least\n1\nâˆ’\nÎ´\n1-\\delta\n:\nâ€–\nÎ \nF\nâˆ’\nÎ \nF\nâ„\nâ€–\n2\nâ‰¤\nÎµ\n,\n\\bigl\\|\\Pi_{F}-\\Pi_{F_{\\mathcal{I}}}\\bigr\\|_{2}\\;\\leq\\;\\varepsilon,\n(7)\ni.e., the principal\nr\nr\n-dimensional subspace of\nF\nc\nF_{c}\nis preserved. For the full proof, see section\nA.4\n.\n4\nExperimental results\nFigure 3\n:\n(a) Cumulative explained variance over principal components for all patches versus the top-\n128\n128\npatches selected by\nâ„“\n2\n\\ell_{2}\nnorm. The red line marks\n90\n%\n90\\%\n. Vertical markers indicate\n17\n17\nand\n14\n14\ncomponents for all and top-\n128\n128\n, respectively. (b) Patch-wise Pearson correlation matrix for one scenario. Strong off-diagonal correlation indicates widespread cross-patch redundancy. (c) Cosine-similarity overlays projected onto the image plane. Bright regions indicate patches whose descriptors are highly similar to the seed, visualizing global entanglement from self-attention.\nWe begin by empirically confirming that patch-wise features are redundant and highly correlated.\n4.1\nRedundancy analysis: qualitative and quantitative evidence\nPCA explained variance over feature dimensions.\nLet\nF\nâˆˆ\nâ„\nN\nÃ—\nd\nF\\in\\mathbb{R}^{N\\times d}\nbe the descriptor matrix. We visualize the cumulative explained variance\nE\nâ€‹\n(\nm\n)\nE(m)\n(as inÂ (\n3\n)) as a function of the number of components used\nm\nm\n. Fig.\n3\n(a) shows that\n90\n%\n90\\%\nof the variance is captured by\n17\n17\nof\n64\n64\nprincipal components. Repeating the analysis over the\n128\n128\npatches with the highest norm, yields\n90\n%\n90\\%\nwith\n14\n14\ncomponents, which confirms that redundancy persists even among the strongest tokens. The experiment was conducted on\n10000\n10000\nframes from different scenes, and averaged across all.\nPatch-wise correlation structure.\nWe compute the Pearson correlation between all patch descriptors within a frame to obtain an\nN\nÃ—\nN\nN\\times N\ncorrelation matrix. A representative heatmap appears in Fig.\n3\n(b), which shows extensive off-diagonal positive and negative correlation, indicating pervasive redundancy across spatial tokens.\nSimilarity overlays on the image plane exampe.\nFor a seed patch\ni\ni\nwith descriptor\nf\ni\nâˆˆ\nâ„\nd\nf_{i}\\in\\mathbb{R}^{d}\n, we compute cosine similarities\ns\nj\n=\nf\ni\nâ‹…\nf\nj\nâˆ¥\nf\ni\nâˆ¥\nâ€‹\nâˆ¥\nf\nj\nâˆ¥\ns_{j}=\\frac{f_{i}\\cdot f_{j}}{\\lVert f_{i}\\rVert\\lVert f_{j}\\rVert}\nfor all patches\nj\nj\n, reshape\ns\nâˆˆ\nâ„\nN\ns\\in\\mathbb{R}^{N}\nto the\nH\nâ€²\nÃ—\nW\nâ€²\nH^{\\prime}\\times W^{\\prime}\ngrid, upsample to the input resolution, and overlay it on the raw image with a heatmap. High intensity indicates semantic overlap. Examples for two seed locations are shown in Fig.\n3\n(c). Widespread high similarity far from the seed in multiple locations illustrates global entanglement and redundancy. See Appx.\nA.2\nfor the full\n16\nÃ—\n16\n16\\times 16\ngrid of overlays for all patches.\nThese qualitative and quantitative results support our hypothesis that patch descriptors live in a low-dimensional subspace and exhibit strong cross-patch redundancy, explaining why stochastic sub-sampling maintains scene semantics.\nFigure 4\n:\nLeft: Diverse OOD sample frames across varying seasons, weather, and lighting conditions. Right: Real-car deployment representative frames from the rural road (public park) and parking garage, captured from both the onboard camera and external view.\n4.2\nSPS Experimental settings\nWe conduct extensive closed-loop driving experiments both for in-distribution and OOD environments. Below, we detail the hardware, tasks, evaluation metrics, and training data.\nTask definition and evaluation protocol.\nAs in\n[\n3\n]\n, we target a general-purpose autonomous driving task requiring the vehicle to follow lanes and avoid obstacles. Failure is defined by three conditions: (i) crossing lane boundaries, (ii) collisions or dangerously close proximity to objects, and (iii) heading deviations exceeding 30\nâˆ˜\nfrom the lane direction. In simulation, we assess performance using a normalized success duration metric, measuring how long the vehicle drives without triggering any failure, averaged over 100 episodes of roughly 20 seconds each. In real-world testing, performance is quantified by counting safety driver interventions, following the same failure definitions. Evaluations are performed in a closed-loop manner unless otherwise specified.\nTraining data and learning framework.\nOur training data combines real-world driving logs with diverse simulated experiences generated using VISTA\n[\n12\n]\n, a simulator built upon approximately two hours of real driving data captured across varied environments, lighting, and weather conditions. The learning procedure adopts Guided Policy Learning\n[\n53\n,\n12\n]\n, leveraging privileged simulator signals to supervise image-based control policies. Control labels are derived using a PID controller for lane keeping and Control Barrier Functions (CBFs)\n[\n9\n]\nfor safety-aware obstacle avoidance.\nPolicies are trained using the Adam optimizer (learning rate\n10\nâˆ’\n3\n10^{-3}\n), an L2 loss objective, and a plateau scheduler with patience 10 and no decay.\nWe utilize BLIP-2\n[\n4\n]\nas the feature extractor to enable a fare comparison with SOTA\n[\n3\n]\n.\nVehicle platform and Hardware.\nThe real car experiments were conducted using a fully autonomous 2019 Lexus RX 450H equipped with high-performance computing and sensing hardware. The onboard system includes an NVIDIA RTX 4070 Ti GPU and an AMD Ryzen 7 3800X 8-core CPU. Visual input is captured via a BFS-PGE-23S3C-CS camera running at 30 frames per second, with a 130\nâˆ˜\nhorizontal field of view and a resolution of 960Ã—600 pixels.\nThe models were trained and evaluated on a university-managed cloud cluster using a total of four A100 GPUs (40GB each). Training took approximately four days.\n4.3\nOOD generalization\nTable 1\n:\nBenchmarking OOD generalization.\nâ€ \nIndicates car types different from training.\nID\n= in-distribution.\nOOD\n= out-of-distribution.\nSetting\nScenarios\nMethods\nUni-modal FMF\nMulti-modal FMF\nScene\nSeason\nWeather\nTime\nActor\nNo-FM\nI-ViT\nMF\nDA\nSPS (Ours)\nID\nRural\nSummer\nDry\nDay\nCar\n1.00\n1.00\n0.72\n1.00\n1.00\nOOD\nRural\nSpring\nDry\nDay\nCar\nâ€ \n0.84\n0.86\n0.42\n0.96\n0.98\nSummer\nDry\nNight\nCar\nâ€ \n0.30\n0.80\n0.35\n0.89\n0.92\nFall\nDry\nDay\nCar\nâ€ \n0.90\n0.95\n0.74\n0.91\n0.96\nWinter\nSnow\nDay\nCar\nâ€ \n0.14\n0.88\n0.42\n0.96\n0.99\nSpring\nDry\nDay\nAnimal\n0.85\n0.89\n0.39\n0.95\n0.99\nSummer\nDry\nNight\nAnimal\n0.29\n0.59\n0.39\n0.85\n0.86\nFall\nDry\nDay\nAnimal\n0.87\n0.95\n0.71\n0.88\n0.93\nWinter\nSnow\nDay\nAnimal\n0.15\n0.87\n0.45\n0.95\n0.99\nUrban\nSummer\nDry\nDay\nCar\nâ€ \n0.55\n0.77\n0.50\n0.62\n0.82\nSummer\nRain\nDay\nCar\nâ€ \n0.69\n0.81\n0.43\n0.81\n0.83\nSummer\nDry\nNight\nCar\nâ€ \n0.45\n0.81\n0.42\n0.78\n0.87\nSummer\nDry\nDay\nAnimal\n0.58\n0.80\n0.50\n0.64\n0.79\nSummer\nRain\nDay\nAnimal\n0.66\n0.83\n0.43\n0.78\n0.84\nSummer\nDry\nNight\nAnimal\n0.45\n0.86\n0.36\n0.81\n0.88\nAverage\n0.55\n0.83\n0.47\n0.84\n0.90\nOur increase (%)\nvs. other methods\nâ†‘\nğŸ‘ğŸ“\n%\n\\uparrow 35\\%\nâ†‘\nğŸ•\n%\n\\uparrow 7\\%\nâ†‘\nğŸ“ğŸ‘\n%\n\\uparrow 53\\%\nâ†‘\nğŸ”\n%\n\\uparrow 6\\%\nâ€“\nTo contextualize our improvements, we adapt the OOD generalization experiments and benchmarks reported in the\nDrive-Anywhere\n[\n3\n]\nframework.\nSpecifically, we train the model in rural environments during summer, under dry weather conditions and daytime lighting, with the presence of other vehicles on the road. We then evaluate its performance across diverse scenes, weather conditions, times of day, and in the presence of other dynamic actors.\nBaselines:\n(i) No Foundation Model (\nNo-FM\n)\n[\n8\n]\n,\n[\n12\n]\n, a baseline that trains a convolutional model without using foundation models (transformer-based variants performed similarly); (ii)\nMask-based Features (MF)\n[\n29\n,\n47\n]\n, which segment the input image by applying a universal segmentation model\n[\n46\n]\n, extracts a global feature vector for each region by applying a pretrained encoder\n[\n49\n]\non a crop bounding this region, and assign that vector uniformly to all pixels within the mask. (iii)\nInherent ViT Features (I-ViT)\n[\n52\n]\n, which extract per-patch features from the output of intermediate layers of a ViT model\n[\n6\n]\n, using the key, query, and value matrices as token-aligned visual descriptors, (iv) the current SOTA Drive Anywhere (DA)\n[\n3\n]\n, which creates per-patch descriptor via the masking strategy explained in Section\n3.1\n, and ours;\nSPS\nas defined in Section\n3.3\nwithout adding any variant from\n3.4\n, with a\nrate\n= 0.5.\nDiscussion.\nThe results are reported in Table\n1\n. First,\nMF\nunderperforms across both in-distribution and OOD settings, likely due to the applied masking model, which may miss and not segment part of the image\n[\n54\n]\n, and thus exclude relevant information from the feature tensor. In contrast,\nI-ViT\nand\nDA\noutperform\nNo-FM\n, underscoring the benefit of pretrained representations. However,\nI-ViT\nremains a uni-modal approach and does not incorporate language grounding. Finally, our proposed method,\nSPS\n, clearly outperforms all baselines, improving upon the previous SOTA (\nDA\n) by an average of 6% and up to 20% in specific OOD scenarios. Our improvement over each method is reported in the last row of the table. We provide examples in Fig.\n4\nfor representative frames from diverse OOD conditions.\nOther variants.\nWe evaluate additional variants of\nSPS\nyielding nine different models in total, incorporating different architectural modifications (e.g., different\nrate\n, position-adjusted, threshold masking). Figure\n5\nshows that 8 out of the 9 models outperform the strongest baseline in OOD settings, and Table\n2\nshows detailed results of all variants across each OOD scenario, demonstrating the robustness and flexibility of our approach; see Section\n4.5\nfor details.\nCross-backbone generalization (DINO).\nWe also applied SPS to a DINO backbone to test transfer beyond BLIP2. For brevity, we report in the appendix OOD scenarios that vary a lot from the in-distribution training set and are therefore especially challenging. On this subset, SPS improves DINO in\n6\n6\nof\n7\n7\nscenarios and ties in one, with an average absolute gain of\n+\n3.3\n%\n+3.3\\%\n; see Table\n5\nin the Appendix.\nTable 2\n:\nAblating all of our variants for OOD generalization.\nâ€ \nindicates car types different from training.\nID\nis in distribution.\nOOD\nis out-of-distribution.\nSetting\nScenarios\nMethods\nScene\nSeason\nWeather\nTime\nActor\nDA\nMSPPS-70%\nMSPPS-50%\nMSPPS-30%\nSPPS-70%\nSPPS-50%\nSPPS-30%\nSPS-70%\nSPS-50%\nSPS-30%\nID\nRural\nSummer\nDry\nDay\nCar\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nOOD\nRural\nSpring\nDry\nDay\nCar\nâ€ \n0.96\n0.96\n0.99\n0.97\n0.98\n0.98\n0.99\n0.99\n0.98\n0.94\nSummer\nDry\nNight\nCar\nâ€ \n0.89\n0.82\n0.89\n0.87\n0.87\n0.91\n0.84\n0.93\n0.92\n0.91\nFall\nDry\nDay\nCar\nâ€ \n0.91\n0.98\n0.94\n0.97\n0.95\n0.97\n0.97\n0.96\n0.96\n0.98\nWinter\nSnow\nDay\nCar\nâ€ \n0.96\n0.95\n0.98\n0.99\n0.99\n0.96\n0.99\n0.96\n0.99\n0.98\nSpring\nDry\nDay\nAnimal\n0.95\n0.95\n0.97\n0.97\n0.97\n0.98\n0.95\n0.95\n0.99\n0.95\nSummer\nDry\nNight\nAnimal\n0.85\n0.79\n0.84\n0.77\n0.86\n0.84\n0.80\n0.87\n0.86\n0.87\nFall\nDry\nDay\nAnimal\n0.88\n0.99\n0.91\n0.94\n0.93\n0.97\n0.92\n0.97\n0.93\n0.92\nWinter\nSnow\nDay\nAnimal\n0.95\n0.94\n0.97\n0.99\n0.99\n0.97\n0.95\n0.97\n0.99\n0.97\nUrban\nSummer\nDry\nDay\nCar\nâ€ \n0.62\n0.53\n0.70\n0.75\n0.79\n0.65\n0.77\n0.71\n0.82\n0.72\nSummer\nRain\nDay\nCar\nâ€ \n0.81\n0.65\n0.85\n0.91\n0.83\n0.84\n0.82\n0.82\n0.83\n0.86\nSummer\nDry\nNight\nCar\nâ€ \n0.78\n0.71\n0.89\n0.90\n0.88\n0.84\n0.86\n0.79\n0.87\n0.85\nSummer\nDry\nDay\nAnimal\n0.64\n0.50\n0.65\n0.72\n0.76\n0.64\n0.74\n0.73\n0.79\n0.70\nSummer\nRain\nDay\nAnimal\n0.78\n0.65\n0.86\n0.91\n0.87\n0.86\n0.79\n0.86\n0.84\n0.87\nSummer\nDry\nNight\nAnimal\n0.81\n0.71\n0.88\n0.90\n0.89\n0.85\n0.86\n0.85\n0.88\n0.90\nAverage\n84.2\n%\n84.2\\%\n79.5\n%\n79.5\\%\n88\n%\n88\\%\n89.7\n%\n89.7\\%\n89.7\n%\n89.7\\%\n87.6\n%\n87.6\\%\n87.5\n%\n87.5\\%\n88.3\n%\n88.3\\%\n90.4\n%\n90.4\\%\n88.7\n%\n88.7\\%\nOur increase (%)\nvs. other methods\nâ€“\nâ†“\n4.7\n%\n\\downarrow 4.7\\%\nâ†‘\n3.8\n%\n\\uparrow 3.8\\%\nâ†‘\n5.5\n%\n\\uparrow 5.5\\%\nâ†‘\n5.5\n%\n\\uparrow 5.5\\%\nâ†‘\n3.4\n%\n\\uparrow 3.4\\%\nâ†‘\n3.3\n%\n\\uparrow 3.3\\%\nâ†‘\n4.1\n%\n\\uparrow 4.1\\%\nâ†‘\n6.2\n%\n\\uparrow 6.2\\%\nâ†‘\n4.5\n%\n\\uparrow 4.5\\%\n4.4\nLatent space text-augmented fine-tuning\nWe adapt language-guided latent space augmentation to further improve the robustness of our best model,\nSPS-50%\n. The augmentation pipeline proceeds as follows: (i) We prompt an LLM to produce a concise list of driving-relevant textual features; visual concepts present in the training scenes, e.g.,\nTree\nor\nTruck\n; these are then identified, and will plausibly be replaced to foster OOD generalization.\n(ii) An LLM is queried to generate alternative concepts that are visually plausible and commonly encountered in driving scenarios (e.g., house instead of tree).\n(iii) In selected frames, patch-level descriptors associated with the target concepts are replaced in latent space with those derived from the suggested alternatives using the foundation modelâ€™s text encoder. This results in a form of data augmentation that preserves scene coherence while introducing semantic diversity.\nTable 3\n:\nLanguage-guided augmentation for OOD generalization.\nWe fine-tune the pretrained\nSPS-50%\nmodel using language-driven latent feature substitution based on LLM-suggested concepts.\nRSDDC\nRSDNC\nRFDDC\nRWSDC\nRSDDA\nâ€“\n+0.29%\n+1.85%\nâ€“\nâ€“\nRSDNA\nRFDDA\nRWSDA\nUSDDC\nUSRDC\n-0.21%\n+2.93\nâ€“\n-1.16\n+4.39%\nUSDNC\nUSDDA\nUSRDA\nUSDNA\nAll\n+0.36%\n+3.86%\n+3.28%\n+1.43%\n+1.7%\nWe apply this technique as a lightweight fine-tuning step to the pretrained\nSPS-50%\nmodel. The model is updated for a small number of iterations using augmented latent features.\nAs shown in Table\n3\n, this results in a further +1.7% accuracy gain across OOD scenarios, excluding cases where the base model already achieves near-perfect performance (\nâ‰¥\n\\geq\n98%), where meaningful further gains are unlikely. These results show that\nSPS\neffectively integrates semantic augmentations through an interpretable, text-driven process, enabling scalable generalization via concept-level latent edits.\n4.5\nAblations: efficiency and performance\nBeyond improving generalization, SPS offers substantial computational benefits during both training and inference. By selecting only a subset of patch features, we reduce the overall processing load without sacrificing semantic richness. We explore multiple variants leveraging the SPS mechanism.\nVariants.\nIn the base\nSPS\nvariant, unselected patch descriptors are replaced with zeros, preserving the feature tensorâ€™s original shape. In the more aggressive\nSPPS (Structured Patch Pruning with Selection)\n, unselected tokens are entirely removed; the selected tokens are then spatially reorganized and assigned position embeddings relative to their original location (Method V2 from Section\n3.4\n).\nA third variant,\nMSPPS (Matrix-based Structured Patch Pruning)\n, stochastically prunes patch tokens based on a per-patch probability mask, yielding dynamic, matrix-based sparsity. Notably, it also prunes unselected patches as in SPPS (the combination of V1 and V2 from section\n3.4\n).\nSpeed reported results.\nEach variant is evaluated across three selection rates: 70%, 50%, and 30%. Inference timing was measured over 100 independent runs per model component, with averages reported for the QFormer, vision projection and normalization, BLIP2 convolution, and the ViT policy model. While we train and benchmark nine model variants in total, only six representative models are shown in Table\n4\n, as the\nSPPS\nand\nMSPPS\nvariants are functionally similar in terms of runtime, with any differences being negligible in practice.\nThe\nSPS-50%\nmodel (our best model in terms of accuracy) achieves a\n2.43\nÃ—\n2.43\\times\nspeedup over the SOTA\nDrive-Anywhere\n.\nAt the more aggressive 30% selection rate,\nSPPS\nand\nSPS\nreach\n3.31\nÃ—\n3.31\\times\nand\n3.47\nÃ—\n3.47\\times\nspeedups, respectively. This shows the core trends: a consistent correlation between lower selection rates and improved runtime, highlighting that SPS not only improves generalization, but also enables substantial computational gains, making it attractive for real-world autonomous systems deployment.\nSpeed vs accuracy reported results.\nTo analyse the trade-off between efficiency and accuracy, in Figure\n1\n(4) we show a comparison, where each model is represented by a single node: The x-axis reflects runtime cost, defined as the inverse of the modelâ€™s speedup over the baseline -\n1\n/\nT\nFactor\n1/T_{\\text{Factor}}\n(lower is better), while the y-axis reports average closed-loop performance.\nNode color encodes a composite performance score, computed as accuracy divided by runtime (higher is better), highlighting models that strike the best balance between precision and efficiency.\nThe six reported models gain better accuracy while being faster than the drive anywhere model; the improvements of our variant in terms of accuracy vary from\n3.3\n%\n3.3\\%\nto\n6.2\n%\n6.2\\%\n, while the speed improved\n1.69\nÃ—\n1.69\\times\nto\n3.47\nÃ—\n3.47\\times\n.\nWhile our composite performance analysis (Fig.\n1\n(4)) shows that\nSPS-70%\nand\nSPPS-70%\nachieve the highest scores when accuracy and runtime are equally weighted, practical deployment considerations suggest a different weighting scheme.\nIn the context of real-world autonomous driving, reliability and generalization, particularly under OOD conditions, are of higher criticality than marginal gains in computational speed. The\nSPS-50%\nmodel consistently delivers the highest average accuracy across scenarios, and this performance margin is non-trivial in safety-critical systems. We therefore view\nSPS-50%\nas the strongest overall candidate for real-world deployment, offering the best tradeoff between robustness and efficiency.\nThe effect of\nrate\n.\nAs shown in Table\n2\n, most variants outperform the\nDrive-Anywhere\nbenchmark across OOD scenarios, with\nSPS-50%\nachieving the highest accuracy (+6.2%), highlighting the general effectiveness of stochastic patch selection.\nFigure\n5\nplots performance against selection rate for each variant, revealing distinct trends:\nSPS\npeaks at 50%, offering the strongest gain;\nSPPS\nperforms best at 70% and declines with more aggressive pruning; and\nMSPPS\nimproves as selection becomes stricter, peaking at 30%. These findings confirm that stochastic selection improves generalization across architectures, though the optimal ratio varies. Notably, moderate selection rates strike the best tradeoff between redundancy reduction and information retention in models where spatial structure is retained.\nTable 4\n:\nRuntime Efficiency of SPS.\nThe calculations were averaged across\n100\n100\nindependent runs.\nModel\nQformer\nVision Proj.&Norm\nFM Project\nPolicy\nTotal Efficiency\nQformer\nEfficiency\nVP&N\nEfficiency\nFMC\nEfficiency\nPolicy\nEfficiency\nPercentage\nFactor\nBenchmark\n2.17s\nâ€“\n871\nÎ¼\nâ€‹\ns\n\\mu s\nâ€“\n540\nÎ¼\nâ€‹\ns\n\\mu s\nâ€“\n17\nm\nâ€‹\ns\nms\nâ€“\n100\n%\n\\%\nx1\nSPS_\n70\n%\n70\\%\n1.56s\n39.10\n%\n39.10\\%\n839\nÎ¼\nâ€‹\ns\n\\mu s\n3.75\n%\n3.75\\%\n376\nÎ¼\nâ€‹\ns\n\\mu s\n43.67\n%\n43.67\\%\n20\nm\nâ€‹\ns\nms\n0.24\n%\n0.24\\%\n86.52\n%\n86.52\\%\nx1.87\nSPS_\n50\n%\n50\\%\n1.11s\n95.50\n%\n95.50\\%\n855\nÎ¼\nâ€‹\ns\n\\mu s\n1.88\n%\n1.88\\%\n370\nÎ¼\nâ€‹\ns\n\\mu s\n46.08\n%\n46.08\\%\n20\nm\nâ€‹\ns\nms\nâˆ’\n1.22\n%\n-1.22\\%\n143.45\n%\n143.45\\%\nx2.43\nSPS_\n30\n%\n30\\%\n0.71s\n205.63\n%\n205.63\\%\n842\nÎ¼\nâ€‹\ns\n\\mu s\n3.45\n%\n3.45\\%\n390\nÎ¼\nâ€‹\ns\n\\mu s\n38.41\n%\n38.41\\%\n20\nm\nâ€‹\ns\nms\n0.41\n%\n0.41\\%\n247.50\n%\n247.50\\%\nx3.47\nSPPS_\n70\n%\n70\\%\n1.56s\n39.10\n%\n39.10\\%\n846\nÎ¼\nâ€‹\ns\n\\mu s\n2.93\n%\n2.93\\%\n500\nÎ¼\nâ€‹\ns\n\\mu s\n8.06\n%\n8.06\\%\n14.3\nm\nâ€‹\ns\nms\n18.92\n%\n18.92\\%\n69.02\n%\n69.02\\%\nx1.69\nSPPS_\n50\n%\n50\\%\n1.11s\n95.50\n%\n95.50\\%\n837\nÎ¼\nâ€‹\ns\n\\mu s\n4.06\n%\n4.06\\%\n493\nÎ¼\nâ€‹\ns\n\\mu s\n9.41\n%\n9.41\\%\n14.5\nm\nâ€‹\ns\nms\n17.45\n%\n17.45\\%\n126.41\n%\n126.41\\%\nx2.26\nSPPS_\n30\n%\n30\\%\n0.71s\n205.63\n%\n205.63\\%\n841\nÎ¼\nâ€‹\ns\n\\mu s\n3.58\n%\n3.58\\%\n542\nÎ¼\nâ€‹\ns\n\\mu s\nâˆ’\n0.44\n%\n-0.44\\%\n13.9\nm\nâ€‹\ns\nms\n22.43\n%\n22.43\\%\n231.21\n%\n231.21\\%\nx3.31\nFigure 5\n:\nAccuracy as a function of selection rate across variants.\n4.6\nReal-world car deployment\nWe deployed the\nSPS-50%\nmodel on a full-scale autonomous vehicle (see supplementary video). The experiments took place during the spring season at two distinct locations: a rural road within a public park and an underground parking garage.\nIn the rural scenario, the vehicle successfully performed lane-following under natural daylight conditions, confirming the modelâ€™s ability to transfer from simulation to the real world. The underground scenario was a challenging OOD test case, featuring low-light conditions, the absence of clear lanes, and visually cluttered geometry, such as perpendicular parking lines, parked vehicles, and distractor objects. Despite never encountering such a configuration at training, the vehicle executed the correct motion.\nIn addition, it successfully avoided static obstacles, including cones and boxes, further demonstrating its robustness. Representative frames from both real-world scenarios, captured from onboard and external views, are shown in Figure\n4\n.\n5\nConclusion and future work\nThis work shows that patch-aligned features (extracted from visionâ€“language models) used for training driving policies are correlated and redundant. Motivated by this, it introduced SPS: a stochastic patchâ€“masking strategy that tackles the redundancy inevitably built into such features. By randomly suppressing a fraction of patch descriptors while preserving their spatial arrangement, SPS forces the policy to ground its decisions on features that are robust to which tokens survive. Across a diverse suite of closed-loop driving scenarios, covering weather, lighting and geographic domain shifts, SPS delivers a\n6.2%\naverage gain in OOD success rate, peaks at\n20.4%\nimprovement in the hardest scenarios, and speeds inference by\n2.4\nÃ—\n\\times\ncompared with the previous SOTA.\nThe same mechanism enables text-conditioned data augmentation in the latent patch space, yielding more\n1.7%\nboost without extra image synthesis. Ablations confirm that the benefit is robust to (reasonable) selection rates, selection methods, and feature re-ordering schemes: 8 out of the 9 trained systems surpassed the prior SOTA. We demonstrated that policies trained with SPS in simulation transfer to a real-world autonomous vehicle with\nno\nextra fine-tuning, highlighting the practical relevance of the approach.\nFuture work\nincludes sophisticated selection techniques (i) learning a state-dependent sampling policy that select the number of patches to allocate based on scenes, and (ii) going beyond uniform sampling by\ninspecting\nthe descriptors themselves, e.g., coreset selection, attention entropy, or mutual-information scores, to drop provably redundant patches, further sharpening both efficiency and robustness.\nAcknowledgments\nAlaa Maalouf acknowledges support from the Neubauer Family Foundation and from the MAOF Fellowship of the Council for Higher Education. This work is supported by Toyota Research Institute (TRI) and Capgemini Engineering. It, however, reflects solely the opinions and conclusions of its authors and not TRI or any other Toyota entity.\nReferences\nPomerleau [1988]\nD.Â A. Pomerleau, â€œAlvinn: An autonomous land vehicle in a neural network,â€\nAdvances in neural information processing systems\n, vol.Â 1, 1988.\nBojarski etÂ al. [2016]\nM.Â Bojarski, D.Â DelÂ Testa, D.Â Dworakowski, B.Â Firner, B.Â Flepp, P.Â Goyal, L.Â D. Jackel, M.Â Monfort, U.Â Muller, J.Â Zhang\netÂ al.\n, â€œEnd to end learning for self-driving cars,â€\narXiv preprint arXiv:1604.07316\n, 2016.\nWang etÂ al. [2024]\nT.-H. Wang, A.Â Maalouf, W.Â Xiao, Y.Â Ban, A.Â Amini, G.Â Rosman, S.Â Karaman, and D.Â Rus, â€œDrive anywhere: Generalizable end-to-end autonomous driving with multi-modal foundation models,â€ in\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, 2024, pp. 6687â€“6694.\nLi etÂ al. [2023]\nJ.Â Li, D.Â Li, S.Â Savarese, and S.Â Hoi, â€œBlip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,â€\narXiv preprint arXiv:2301.12597\n, 2023.\nLi etÂ al. [2022a]\nJ.Â Li, D.Â Li, C.Â Xiong, and S.Â Hoi, â€œBlip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,â€ in\nInternational Conference on Machine Learning\n.â€‚â€„â€ŠPMLR, 2022, pp. 12â€‰888â€“12â€‰900.\nZhang etÂ al. [2022]\nH.Â Zhang, F.Â Li, S.Â Liu, L.Â Zhang, H.Â Su, J.Â Zhu, L.Â M. Ni, and H.-Y. Shum, â€œDino: Detr with improved denoising anchor boxes for end-to-end object detection,â€ 2022.\nAmini etÂ al. [2019]\nA.Â Amini, G.Â Rosman, S.Â Karaman, and D.Â Rus, â€œVariational end-to-end navigation and localization,â€ in\n2019 International Conference on Robotics and Automation (ICRA)\n.â€‚â€„â€ŠIEEE, 2019, pp. 8958â€“8964.\nWang etÂ al. [2023]\nT.-H. Wang, W.Â Xiao, M.Â Chahine, A.Â Amini, R.Â Hasani, and D.Â Rus, â€œLearning stability attention in vision-based end-to-end driving policies,â€ in\nLearning for Dynamics and Control Conference\n.â€‚â€„â€ŠPMLR, 2023, pp. 1099â€“1111.\nXiao and Belta [2019]\nW.Â Xiao and C.Â Belta, â€œControl barrier functions for systems with high relative degree,â€ in\nProc. of 58th IEEE Conference on Decision and Control\n, Nice, France, 2019, pp. 474â€“479.\nXiao etÂ al. [2023]\nW.Â Xiao, T.-H. Wang, R.Â Hasani, M.Â Chahine, A.Â Amini, X.Â Li, and D.Â Rus, â€œBarriernet: Differentiable control barrier functions for learning of safe robot control,â€\nIEEE Transactions on Robotics\n, 2023.\nKendall etÂ al. [2019]\nA.Â Kendall, J.Â Hawke, D.Â Janz, P.Â Mazur, D.Â Reda, J.-M. Allen, V.-D. Lam, A.Â Bewley, and A.Â Shah, â€œLearning to drive in a day,â€ in\n2019 International Conference on Robotics and Automation (ICRA)\n.â€‚â€„â€ŠIEEE, 2019, pp. 8248â€“8254.\nAmini etÂ al. [2022]\nA.Â Amini, T.-H. Wang, I.Â Gilitschenski, W.Â Schwarting, Z.Â Liu, S.Â Han, S.Â Karaman, and D.Â Rus, â€œVista 2.0: An open, data-driven simulator for multimodal sensing and policy learning for autonomous vehicles,â€ in\n2022 International Conference on Robotics and Automation (ICRA)\n.â€‚â€„â€ŠIEEE, 2022, pp. 2419â€“2426.\nDosovitskiy etÂ al. [2017]\nA.Â Dosovitskiy, G.Â Ros, F.Â Codevilla, A.Â Lopez, and V.Â Koltun, â€œCarla: An open urban driving simulator,â€ in\nConference on robot learning\n.â€‚â€„â€ŠPMLR, 2017, pp. 1â€“16.\nShah etÂ al. [2018]\nS.Â Shah, D.Â Dey, C.Â Lovett, and A.Â Kapoor, â€œAirsim: High-fidelity visual and physical simulation for autonomous vehicles,â€ in\nField and Service Robotics: Results of the 11th International Conference\n.â€‚â€„â€ŠSpringer, 2018, pp. 621â€“635.\nTedrake etÂ al. [2019]\nR.Â Tedrake\netÂ al.\n, â€œDrake: Model-based design and verification for robotics,â€ 2019, software Package, available at\nhttps://drake.mit.edu/\n.\nMÃ¼ller etÂ al. [2018]\nM.Â MÃ¼ller, A.Â Dosovitskiy, B.Â Ghanem, and V.Â Koltun, â€œDriving policy transfer via modularity and abstraction,â€\narXiv preprint arXiv:1804.09364\n, 2018.\nToromanoff etÂ al. [2020]\nM.Â Toromanoff, E.Â Wirbel, and F.Â Moutarde, â€œEnd-to-end model-free reinforcement learning for urban driving using implicit affordances,â€ in\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, 2020, pp. 7153â€“7162.\nBehl etÂ al. [2020]\nA.Â Behl, K.Â Chitta, A.Â Prakash, E.Â Ohn-Bar, and A.Â Geiger, â€œLabel efficient visual abstractions for autonomous driving,â€ in\n2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n.â€‚â€„â€ŠIEEE, 2020, pp. 2338â€“2345.\nTellex etÂ al. [2020]\nS.Â Tellex, N.Â Gopalan, H.Â Kress-Gazit, and C.Â Matuszek, â€œRobots that use language,â€\nAnnual Review of Control, Robotics, and Autonomous Systems\n, vol.Â 3, pp. 25â€“55, 2020.\nBisk etÂ al. [2020]\nY.Â Bisk, A.Â Holtzman, J.Â Thomason, J.Â Andreas, Y.Â Bengio, J.Â Chai, M.Â Lapata, A.Â Lazaridou, J.Â May, A.Â Nisnevich\netÂ al.\n, â€œExperience grounds language,â€\narXiv preprint arXiv:2004.10151\n, 2020.\nAhn etÂ al. [2022]\nM.Â Ahn, A.Â Brohan, N.Â Brown, Y.Â Chebotar, O.Â Cortes, B.Â David, C.Â Finn, C.Â Fu, K.Â Gopalakrishnan, K.Â Hausman\netÂ al.\n, â€œDo as i can, not as i say: Grounding language in robotic affordances,â€\narXiv preprint arXiv:2204.01691\n, 2022.\nBrohan etÂ al. [2022]\nA.Â Brohan, N.Â Brown, J.Â Carbajal, Y.Â Chebotar, J.Â Dabis, C.Â Finn, K.Â Gopalakrishnan, K.Â Hausman, A.Â Herzog, J.Â Hsu\netÂ al.\n, â€œRt-1: Robotics transformer for real-world control at scale,â€\narXiv preprint arXiv:2212.06817\n, 2022.\nLi etÂ al. [2022b]\nS.Â Li, X.Â Puig, C.Â Paxton, Y.Â Du, C.Â Wang, L.Â Fan, T.Â Chen, D.-A. Huang, E.Â AkyÃ¼rek, A.Â Anandkumar\netÂ al.\n, â€œPre-trained language models for interactive decision-making,â€\nAdvances in Neural Information Processing Systems\n, vol.Â 35, pp. 31â€‰199â€“31â€‰212, 2022.\nBarbara and Maalouf [2025]\nM.Â Barbara and A.Â Maalouf, â€œPrompts to summaries: Zero-shot language-guided video summarization,â€\narXiv preprint arXiv:2506.10807\n, 2025.\nHuang etÂ al. [2023]\nC.Â Huang, O.Â Mees, A.Â Zeng, and W.Â Burgard, â€œAudio visual language maps for robot navigation,â€\narXiv preprint arXiv:2303.07522\n, 2023.\nDing etÂ al. [2023]\nR.Â Ding, J.Â Yang, C.Â Xue, W.Â Zhang, S.Â Bai, and X.Â Qi, â€œPla: Language-driven open-vocabulary 3d scene understanding,â€ in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2023, pp. 7010â€“7019.\nPeng etÂ al. [2023]\nS.Â Peng, K.Â Genova, C.Â Jiang, A.Â Tagliasacchi, M.Â Pollefeys, T.Â Funkhouser\netÂ al.\n, â€œOpenscene: 3d scene understanding with open vocabularies,â€ in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2023, pp. 815â€“824.\nChahine etÂ al. [2024]\nM.Â Chahine, A.Â Quach, A.Â Maalouf, T.-H. Wang, and D.Â Rus, â€œFlex: End-to-end text-instructed visual navigation with foundation models,â€ 2024. [Online]. Available:\nhttps://arxiv.org/abs/2410.13002\nMaalouf etÂ al. [2023]\nA.Â Maalouf, N.Â Jadhav, K.Â M. Jatavallabhula, M.Â Chahine, D.Â M. Vogt, R.Â J. Wood, A.Â Torralba, and D.Â Rus, â€œFollow anything: Open-set detection, tracking, and following in real-time,â€\narXiv preprint arXiv:2308.05737\n, 2023.\nLiu etÂ al. [2023]\nS.Â Liu, Z.Â Zeng, T.Â Ren, F.Â Li, H.Â Zhang, J.Â Yang, C.Â Li, J.Â Yang, H.Â Su, J.Â Zhu\netÂ al.\n, â€œGrounding dino: Marrying dino with grounded pre-training for open-set object detection,â€\narXiv preprint arXiv:2303.05499\n, 2023.\nGhiasi etÂ al. [2022]\nG.Â Ghiasi, X.Â Gu, Y.Â Cui, and T.-Y. Lin, â€œScaling open-vocabulary image segmentation with image-level labels,â€ in\nComputer Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XXXVI\n.â€‚â€„â€ŠSpringer, 2022, pp. 540â€“557.\nLi etÂ al. [2022c]\nB.Â Li, K.Â Q. Weinberger, S.Â Belongie, V.Â Koltun, and R.Â Ranftl, â€œLanguage-driven semantic segmentation,â€\narXiv preprint arXiv:2201.03546\n, 2022.\nChahine etÂ al. [2025]\nM.Â Chahine, W.Â Yang, A.Â Maalouf, J.Â Siriska, N.Â Jadhav, D.Â Vogt, S.Â Gil, R.Â Wood, and D.Â Rus, â€œDecentralized vision-based autonomous aerial wildlife monitoring,â€\narXiv preprint arXiv:2508.15038\n, 2025.\nRamesh etÂ al. [2021]\nA.Â Ramesh, M.Â Pavlov, G.Â Goh, S.Â Gray, C.Â Voss, A.Â Radford, M.Â Chen, and I.Â Sutskever, â€œZero-shot text-to-image generation,â€ in\nInternational Conference on Machine Learning\n.â€‚â€„â€ŠPMLR, 2021, pp. 8821â€“8831.\nCrowson etÂ al. [2022]\nK.Â Crowson, S.Â Biderman, D.Â Kornis, D.Â Stander, E.Â Hallahan, L.Â Castricato, and E.Â Raff, â€œVqgan-clip: Open domain image generation and editing with natural language guidance,â€ in\nEuropean Conference on Computer Vision\n.â€‚â€„â€ŠSpringer, 2022, pp. 88â€“105.\nPatashnik etÂ al. [2021]\nO.Â Patashnik, Z.Â Wu, E.Â Shechtman, D.Â Cohen-Or, and D.Â Lischinski, â€œStyleclip: Text-driven manipulation of stylegan imagery,â€ in\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, 2021, pp. 2085â€“2094.\nRamesh etÂ al. [2022]\nA.Â Ramesh, P.Â Dhariwal, A.Â Nichol, C.Â Chu, and M.Â Chen, â€œHierarchical text-conditional image generation with clip latents,â€\narXiv preprint arXiv:2204.06125\n, 2022.\nSreeram etÂ al. [2025]\nS.Â Sreeram, T.-H. Wang, A.Â Maalouf, G.Â Rosman, S.Â Karaman, and D.Â Rus, â€œProbing multimodal llms as world models for driving,â€\nIEEE Robotics and Automation Letters\n, vol.Â 10, no.Â 11, pp. 11â€‰403â€“11â€‰410, 2025.\nKim etÂ al. [2019]\nJ.Â Kim, T.Â Misu, Y.-T. Chen, A.Â Tawari, and J.Â Canny, â€œGrounding human-to-vehicle advice for self-driving vehicles,â€ in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n, Nov. 2019, pp. 10â€‰591â€“10â€‰599.\nOmeiza etÂ al. [2021]\nD.Â Omeiza, H.Â Webb, M.Â Jirotka, and L.Â Kunze, â€œExplanations in autonomous driving: A survey,â€\nIEEE Transactions on Intelligent Transportation Systems\n, vol.Â 23, no.Â 8, pp. 10â€‰142â€“10â€‰162, 2021.\nKuo etÂ al. [2022]\nY.-L. Kuo, X.Â Huang, A.Â Barbu, S.Â G. McGill, B.Â Katz, J.Â J. Leonard, and G.Â Rosman, â€œTrajectory prediction with linguistic representations,â€ in\n2022 International Conference on Robotics and Automation (ICRA)\n.â€‚â€„â€ŠIEEE, 2022, pp. 2868â€“2875.\nTan etÂ al. [2023]\nS.Â Tan, B.Â Ivanovic, X.Â Weng, M.Â Pavone, and P.Â Kraehenbuehl, â€œLanguage conditioned traffic generation,â€\narXiv preprint arXiv:2307.07947\n, 2023.\nZhong etÂ al. [2023]\nZ.Â Zhong, D.Â Rempe, Y.Â Chen, B.Â Ivanovic, Y.Â Cao, D.Â Xu, M.Â Pavone, and B.Â Ray, â€œLanguage-guided traffic simulation via scene-level diffusion,â€\narXiv preprint arXiv:2306.06344\n, 2023.\nCaron etÂ al. [2021]\nM.Â Caron, H.Â Touvron, I.Â Misra, H.Â JÃ©gou, J.Â Mairal, P.Â Bojanowski, and A.Â Joulin, â€œEmerging properties in self-supervised vision transformers,â€ in\nProceedings of the IEEE/CVF international conference on computer vision\n, 2021, pp. 9650â€“9660.\nOquab etÂ al. [2023]\nM.Â Oquab, T.Â Darcet, T.Â Moutakanni, H.Â Vo, M.Â Szafraniec, V.Â Khalidov, P.Â Fernandez, D.Â Haziza, F.Â Massa, A.Â El-Nouby\netÂ al.\n, â€œDinov2: Learning robust visual features without supervision,â€\narXiv preprint arXiv:2304.07193\n, 2023.\nKirillov etÂ al. [2023]\nA.Â Kirillov, E.Â Mintun, N.Â Ravi, H.Â Mao, C.Â Rolland, L.Â Gustafson, T.Â Xiao, S.Â Whitehead, A.Â C. Berg, W.-Y. Lo\netÂ al.\n, â€œSegment anything,â€\narXiv preprint arXiv:2304.02643\n, 2023.\nZhao etÂ al. [2023]\nX.Â Zhao, W.Â Ding, Y.Â An, Y.Â Du, T.Â Yu, M.Â Li, M.Â Tang, and J.Â Wang, â€œFast segment anything,â€ 2023.\nCheng etÂ al. [2022]\nB.Â Cheng, I.Â Misra, A.Â G. Schwing, A.Â Kirillov, and R.Â Girdhar, â€œMasked-attention mask transformer for universal image segmentation,â€ in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2022, pp. 1290â€“1299.\nRadford etÂ al. [2021]\nA.Â Radford, J.Â W. Kim, C.Â Hallacy, A.Â Ramesh, G.Â Goh, S.Â Agarwal, G.Â Sastry, A.Â Askell, P.Â Mishkin, J.Â Clark\netÂ al.\n, â€œLearning transferable visual models from natural language supervision,â€ in\nInternational conference on machine learning\n.â€‚â€„â€ŠPMLR, 2021, pp. 8748â€“8763.\nZhong etÂ al. [2022]\nY.Â Zhong, J.Â Yang, P.Â Zhang, C.Â Li, N.Â Codella, L.Â H. Li, L.Â Zhou, X.Â Dai, L.Â Yuan, Y.Â Li\netÂ al.\n, â€œRegionclip: Region-based language-image pretraining,â€ in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2022, pp. 16â€‰793â€“16â€‰803.\nJatavallabhula etÂ al. [2023]\nK.Â M. Jatavallabhula, A.Â Kuwajerwala, Q.Â Gu, M.Â Omama, T.Â Chen, A.Â Maalouf, S.Â Li, G.Â Iyer, S.Â Saryazdi, N.Â Keetha\netÂ al.\n, â€œConceptfusion: Open-set multimodal 3d mapping,â€\narXiv preprint arXiv:2302.07241\n, 2023.\nAmir etÂ al. [2021]\nS.Â Amir, Y.Â Gandelsman, S.Â Bagon, and T.Â Dekel, â€œDeep ViT features as dense visual descriptors,â€\narXiv preprint arXiv:2112.05814\n, 2021.\nLevine and Koltun [2013]\nS.Â Levine and V.Â Koltun, â€œGuided policy search,â€ in\nInternational conference on machine learning\n.â€‚â€„â€ŠPMLR, 2013, pp. 1â€“9.\nDai etÂ al. [2015]\nJ.Â Dai, K.Â He, and J.Â Sun, â€œConvolutional feature masking for joint object and stuff segmentation,â€ in\n2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n.â€‚â€„â€ŠIEEE, Jun. 2015.\nAppendix A\nAppendix\nA.1\nPCA and correlation analysis: definitions and rationale\nFeature covariance.\nLet\nF\nc\nF_{c}\nbe an\nN\nÃ—\nD\nN\\times D\nmatrix of the\nN\nN\ncomputed patch descriptors. Assume\nF\nc\nF_{c}\nis centered (otherwise do so). Then compute\nÎ£\n=\n1\nN\nâˆ’\n1\nâ€‹\nF\nc\nâŠ¤\nâ€‹\nF\nc\nâˆˆ\nâ„\nd\nÃ—\nd\n\\Sigma=\\frac{1}{N-1}F_{c}^{\\top}F_{c}\\in\\mathbb{R}^{d\\times d}\n. The total variance equals the trace of\nÎ£\n\\Sigma\n, which is the sum of eigenvalues. The cumulative explained variance curve reports\nâˆ‘\ni\n=\n1\nm\nÎ»\ni\n/\nâˆ‘\ni\n=\n1\nd\nÎ»\ni\n\\sum_{i=1}^{m}\\lambda_{i}\\big/\\sum_{i=1}^{d}\\lambda_{i}\n.\nTop-energy subset.\nLet\nf\nj\nf_{j}\ndenote the computed descriptor for patch\nj\nj\n. Compute\nâˆ¥\nf\ni\nâˆ¥\n2\n\\lVert f_{i}\\rVert_{2}\nfor each patch, sort, and keep the top\n128\n128\npatches. Recompute the PCA curve on this subset. Achieving\n90\n%\n90\\%\nvariance with about\n14\n14\ncomponents indicates redundancy even among the strongest tokens.\nWhy feature-space PCA supports patch masking.\nAlthough SPS masks patches rather than feature channels, a low-rank feature manifold implies that many patches project into the same few directions. Removing some rows of\nF\nF\npreserves the principal subspace with high probability, so semantics remain available from the retained tokens. One can also analyze patch-wise covariance\n1\nd\nâˆ’\n1\nâ€‹\nF\nc\nâ€‹\nF\nc\nâŠ¤\nâˆˆ\nâ„\nN\nÃ—\nN\n\\frac{1}{d-1}F_{c}F_{c}^{\\top}\\in\\mathbb{R}^{N\\times N}\n, which shows many correlated patches, consistent with the qualitative overlays in Appx.\nA.2\n.\nSecond-moment viewpoint.\nThe matrix\nF\nc\nâŠ¤\nâ€‹\nF\nc\nF_{c}^{\\top}F_{c}\nis the second-moment (Gram) matrix of descriptors.\nWhen we uniformly sample\nm\nm\nrows without replacement to form\nS\nâ€‹\nF\nc\nSF_{c}\n, the expected Gram matrix of the sampled descriptors satisfies\nğ”¼\nâ€‹\n[\n(\nS\nâ€‹\nF\nc\n)\nâŠ¤\nâ€‹\n(\nS\nâ€‹\nF\nc\n)\n]\n=\nm\nN\nâ€‹\nF\nc\nâŠ¤\nâ€‹\nF\nc\n\\mathbb{E}\\big[(SF_{c})^{\\top}(SF_{c})\\big]=\\tfrac{m}{N}\\,F_{c}^{\\top}F_{c}\n.\nThus a quadratic form induced by the sampled rows preserves the full quadratic form in expectation up to the known scale\nm\n/\nN\nm/N\n.\nA.2\nFull grid similarity overlays\nFigure 6\n:\nFull grid of similarity overlays for all\n16\nÃ—\n16\n16\\times 16\npatches. Each cell shows the raw image overlaid with the cosine similarity from the corresponding seed patch to all other patches.\nHow to compute it?\nGiven\nF\nc\nâˆˆ\nâ„\nN\nÃ—\nd\nF_{c}\\in\\mathbb{R}^{N\\times d}\nbe the matrix of patch features as before, and let\nf\nj\nf_{j}\ndenote the computed descriptor for patch\nj\nj\n. Choose a seed index/patch\ni\ni\nand define\ns\nj\n=\nf\ni\nâ‹…\nf\nj\nâˆ¥\nf\ni\nâˆ¥\nâ€‹\nâˆ¥\nf\nj\nâˆ¥\ns_{j}=\\frac{f_{i}\\cdot f_{j}}{\\lVert f_{i}\\rVert\\lVert f_{j}\\rVert}\nfor every patch\nj\nj\n. Set\ns\n=\n(\ns\n1\n,\nâ‹¯\n,\ns\nN\n)\ns=(s_{1},\\cdots,s_{N})\n, and reshape\ns\ns\nto form\nH\nâ€²\nÃ—\nW\nâ€²\nH^{\\prime}\\times W^{\\prime}\nmatrix, then upsample the matrix to the input resolution, and normalize to\n[\n0\n,\n1\n]\n[0,1]\nand overlay with the original input frame\nF\nF\n.\nInterpretation of Fig.\n6\n.\nBlue denotes low or negative cosine similarity and red denotes high positive similarity. For a given seed patch, only a sparse set of regions are strongly similar (red), while most are weakly or negatively related (blue). As the seed changes across the grid, the red regions shift to locations that share semantics with the new seed, illustrating globally entangled yet redundant representations.\nA.3\nDINO backbone results on Animal OOD scenarios\nWe also evaluate SPS with the DINO backbone (to show generalization across different setups/backbones) and observe consistent gains (Table\n5\n).\nTable 5\n:\nSPS with a DINO backbone on Animal OOD scenarios.\nThese actors were not present in the in-distribution training set. SPS improves DINO in\n6\n6\nof\n7\n7\nscenarios and ties in one.\nScenario\nDINO\nDINO + SPS\nGain\nRSpDDA\n0.89\n0.96\n+\n7\n%\n+7\\%\nRSuDNA\n0.59\n0.65\n+\n6\n%\n+6\\%\nRFDDA\n0.95\n0.97\n+\n2\n%\n+2\\%\nRWSDA\n0.87\n0.92\n+\n5\n%\n+5\\%\nUSuDDA\n0.80\n0.82\n+\n2\n%\n+2\\%\nUSuRDA\n0.83\n0.83\n0\n%\n0\\%\nUSuDNA\n0.86\n0.87\n+\n1\n%\n+1\\%\nAverage\n82.7\n%\n82.7\\%\n86\n%\n86\\%\n+3.3%\nA.4\nProof of Lemma\n1\nProof of Lemma\n1\n.\nStep 0: Thin SVD, projectors, and coherence.\nLet the thin SVD of the centered descriptor matrix be\nF\nc\n=\nU\nr\nâ€‹\nÎ£\nr\nâ€‹\nV\nr\nâŠ¤\nF_{c}=U_{r}\\Sigma_{r}V_{r}^{\\top}\n,\nwith\nU\nr\nâˆˆ\nâ„\nN\nÃ—\nr\nU_{r}\\in\\mathbb{R}^{N\\times r}\nand\nV\nr\nâˆˆ\nâ„\nd\nÃ—\nr\nV_{r}\\in\\mathbb{R}^{d\\times r}\nhaving orthonormal columns, and\nÎ£\nr\nâˆˆ\nâ„\nr\nÃ—\nr\n\\Sigma_{r}\\in\\mathbb{R}^{r\\times r}\ndiagonal with positive entries.\nThe projector onto the row space is\nÎ \nF\n=\nV\nr\nâ€‹\nV\nr\nâŠ¤\n\\Pi_{F}=V_{r}V_{r}^{\\top}\n.\nDefine the uniform row coherence\nÎ¼\nâ‰œ\nN\nr\nâ€‹\nmax\ni\nâˆˆ\n[\nN\n]\nâ¡\nâ€–\n(\nU\nr\n)\ni\n:\nâ€–\n2\n2\nâˆˆ\n[\n1\n,\nN\n/\nr\n]\n.\n\\mu\\triangleq\\frac{N}{r}\\,\\max_{i\\in[N]}\\|(U_{r})_{i:}\\|_{2}^{2}\\in[1,\\,N/r].\nFix tolerances\nÎµ\nâˆˆ\n(\n0\n,\n1\n)\n\\varepsilon\\in(0,1)\nand\nÎ´\nâˆˆ\n(\n0\n,\n1\n)\n\\delta\\in(0,1)\n.\nStep 1: Second-moment identity in expectation.\nLet\nS\nâˆˆ\n{\n0\n,\n1\n}\nm\nÃ—\nN\nS\\in\\{0,1\\}^{m\\times N}\nselect\nm\nm\nrows uniformly without replacement, and write\nS\nâ€‹\nF\nc\nSF_{c}\nfor the sampled submatrix.\nThen\nS\nâŠ¤\nâ€‹\nS\n=\ndiag\nâ€‹\n(\nc\n1\n,\nâ€¦\n,\nc\nN\n)\nS^{\\top}S=\\mathrm{diag}(c_{1},\\dots,c_{N})\nwith\nc\ni\nâˆˆ\n{\n0\n,\n1\n}\nc_{i}\\in\\{0,1\\}\nindicating whether row\ni\ni\nis selected, so\nğ”¼\nâ€‹\n[\nS\nâŠ¤\nâ€‹\nS\n]\n=\nm\nN\nâ€‹\nI\n\\mathbb{E}[S^{\\top}S]=\\tfrac{m}{N}I\nand\nğ”¼\nâ€‹\n[\n(\nS\nâ€‹\nF\nc\n)\nâŠ¤\nâ€‹\n(\nS\nâ€‹\nF\nc\n)\n]\n=\nF\nc\nâŠ¤\nâ€‹\nğ”¼\nâ€‹\n[\nS\nâŠ¤\nâ€‹\nS\n]\nâ€‹\nF\nc\n=\nm\nN\nâ€‹\nF\nc\nâŠ¤\nâ€‹\nF\nc\n.\n\\mathbb{E}\\big[(SF_{c})^{\\top}(SF_{c})\\big]=F_{c}^{\\top}\\mathbb{E}[S^{\\top}S]F_{c}=\\tfrac{m}{N}\\,F_{c}^{\\top}F_{c}.\n(8)\nHence the second-moment (Gram) matrix of sampled descriptors preserves the full one in expectation up to the factor\nm\n/\nN\nm/N\n.\nStep 2: Subspace embedding via uniform row sampling.\nBy matrix Chernoff concentration with bounded coherence (standard subspace-embedding results), if\nm\nâ‰¥\nC\nâ€‹\nÎ¼\nâ€‹\nr\nÎµ\n2\nâ€‹\nlog\nâ¡\n(\nr\nÎ´\n)\n,\nm\\;\\geq\\;C\\,\\frac{\\mu\\,r}{\\varepsilon^{2}}\\,\\log\\!\\Bigl(\\frac{r}{\\delta}\\Bigr),\nthen with probability at least\n1\nâˆ’\nÎ´\n1-\\delta\nwe have the spectral sandwich\n(\n1\nâˆ’\nÎµ\n)\nâ€‹\nI\nr\nâª¯\nN\nm\nâ€‹\nU\nr\nâŠ¤\nâ€‹\nS\nâŠ¤\nâ€‹\nS\nâ€‹\nU\nr\nâª¯\n(\n1\n+\nÎµ\n)\nâ€‹\nI\nr\n.\n(1-\\varepsilon)\\,I_{r}\\;\\preceq\\;\\frac{N}{m}\\,U_{r}^{\\top}S^{\\top}S\\,U_{r}\\;\\preceq\\;(1+\\varepsilon)\\,I_{r}.\n(9)\nIn particular,\nU\nr\nâŠ¤\nâ€‹\nS\nâŠ¤\nâ€‹\nS\nâ€‹\nU\nr\nU_{r}^{\\top}S^{\\top}S\\,U_{r}\nis positive definite, so\nS\nâ€‹\nU\nr\nSU_{r}\nhas full column rank\nr\nr\n.\nBy (\n9\n), all eigenvalues of\nN\nm\nâ€‹\nU\nr\nâŠ¤\nâ€‹\nS\nâŠ¤\nâ€‹\nS\nâ€‹\nU\nr\n\\tfrac{N}{m}\\,U_{r}^{\\top}S^{\\top}S\\,U_{r}\nlie in\n[\n1\nâˆ’\nÎµ\n,\n1\n+\nÎµ\n]\n[\\,1-\\varepsilon,\\,1+\\varepsilon\\,]\n,\nhence\nU\nr\nâŠ¤\nâ€‹\nS\nâŠ¤\nâ€‹\nS\nâ€‹\nU\nr\nâ‰»\n0\nU_{r}^{\\top}S^{\\top}S\\,U_{r}\\succ 0\n.\nPositive definiteness implies that for any\nx\nâ‰ \n0\nx\\neq 0\n,\nx\nâŠ¤\nâ€‹\nU\nr\nâŠ¤\nâ€‹\nS\nâŠ¤\nâ€‹\nS\nâ€‹\nU\nr\nâ€‹\nx\n>\n0\nx^{\\top}U_{r}^{\\top}S^{\\top}S\\,U_{r}x>0\n; therefore\nS\nâ€‹\nU\nr\nâ€‹\nx\nâ‰ \n0\nSU_{r}x\\neq 0\nand\nrank\nâ€‹\n(\nS\nâ€‹\nU\nr\n)\n=\nr\n\\mathrm{rank}(SU_{r})=r\n.\nThis means the sampled rows still span an\nr\nr\n-dimensional subspace in\nâ„\nN\n\\mathbb{R}^{N}\n.\nStep 3: Consequences for the row space.\nUsing\nF\nc\n=\nU\nr\nâ€‹\nÎ£\nr\nâ€‹\nV\nr\nâŠ¤\nF_{c}=U_{r}\\Sigma_{r}V_{r}^{\\top}\n,\n(\nS\nâ€‹\nF\nc\n)\nâŠ¤\nâ€‹\n(\nS\nâ€‹\nF\nc\n)\n\\displaystyle(SF_{c})^{\\top}(SF_{c})\n=\nV\nr\nâ€‹\nÎ£\nr\nâ€‹\n(\nU\nr\nâŠ¤\nâ€‹\nS\nâŠ¤\nâ€‹\nS\nâ€‹\nU\nr\n)\nâ€‹\nÎ£\nr\nâ€‹\nV\nr\nâŠ¤\n\\displaystyle=\\;V_{r}\\,\\Sigma_{r}\\,\\bigl(U_{r}^{\\top}S^{\\top}S\\,U_{r}\\bigr)\\,\\Sigma_{r}\\,V_{r}^{\\top}\n(10)\n=\nV\nr\nâ€‹\n[\nÎ£\nr\nâ€‹\n(\nU\nr\nâŠ¤\nâ€‹\nS\nâŠ¤\nâ€‹\nS\nâ€‹\nU\nr\n)\nâ€‹\nÎ£\nr\n]\nâ€‹\nV\nr\nâŠ¤\n.\n\\displaystyle=\\;V_{r}\\bigl[\\Sigma_{r}(U_{r}^{\\top}S^{\\top}S\\,U_{r})\\Sigma_{r}\\bigr]V_{r}^{\\top}.\nSince\nU\nr\nâŠ¤\nâ€‹\nS\nâŠ¤\nâ€‹\nS\nâ€‹\nU\nr\nU_{r}^{\\top}S^{\\top}S\\,U_{r}\nis\nr\nÃ—\nr\nr\\times r\npositive definite,\nthe nonzero eigenvectors of\n(\nS\nâ€‹\nF\nc\n)\nâŠ¤\nâ€‹\n(\nS\nâ€‹\nF\nc\n)\n(SF_{c})^{\\top}(SF_{c})\nare the columns of\nV\nr\nV_{r}\n, and the corresponding eigenvalues are those of\nÎ£\nr\nâ€‹\n(\nU\nr\nâŠ¤\nâ€‹\nS\nâŠ¤\nâ€‹\nS\nâ€‹\nU\nr\n)\nâ€‹\nÎ£\nr\n\\Sigma_{r}(U_{r}^{\\top}S^{\\top}S\\,U_{r})\\Sigma_{r}\n.\nThus\n(\nS\nâ€‹\nF\nc\n)\nâŠ¤\nâ€‹\n(\nS\nâ€‹\nF\nc\n)\n(SF_{c})^{\\top}(SF_{c})\nhas the same nonzero eigenvectors as\nV\nr\nV_{r}\n, whose columns span\nspan\nâ€‹\n(\nV\nr\n)\n\\mathrm{span}(V_{r})\n.\nConsequently, the range of\n(\nS\nâ€‹\nF\nc\n)\nâŠ¤\nâ€‹\n(\nS\nâ€‹\nF\nc\n)\n(SF_{c})^{\\top}(SF_{c})\nequals\nspan\nâ€‹\n(\nV\nr\n)\n\\mathrm{span}(V_{r})\n, so the row space of\nS\nâ€‹\nF\nc\nSF_{c}\ncoincides with that of\nF\nc\nF_{c}\n, and the orthogonal projectors satisfy\nÎ \nS\nâ€‹\nF\nc\n=\nÎ \nF\n\\Pi_{SF_{c}}=\\Pi_{F}\n.\nThis is stronger than the bound\nâ€–\nÎ \nF\nâˆ’\nÎ \nS\nâ€‹\nF\nc\nâ€–\n2\nâ‰¤\nÎµ\n\\|\\Pi_{F}-\\Pi_{SF_{c}}\\|_{2}\\leq\\varepsilon\n, which follows immediately.\nCombining Steps 1â€“3 proves the lemma under the stated sample complexity.\nâˆ\nRemark 1 (Lipschitz stability).\nLet\nÏ•\n\\phi\nbe a row-wise Lipschitz map with constant\nL\nL\n(for example, a per-token linear map followed by a 1-Lipschitz normalization). Let\nÏ•\n\\phi\nact independently on each row. Then\nâ€–\nÏ•\nâ€‹\n(\nS\nâ€‹\nF\nc\n)\nâˆ’\nÏ•\nâ€‹\n(\nF\nc\n)\nâ€–\nF\n2\n\\displaystyle\\|\\phi(SF_{c})-\\phi(F_{c})\\|_{F}^{2}\n=\nâˆ‘\ni\n=\n1\nN\nâ€–\nÏ•\nâ€‹\n(\n(\nS\nâ€‹\nF\nc\n)\ni\n:\n)\nâˆ’\nÏ•\nâ€‹\n(\n(\nF\nc\n)\ni\n:\n)\nâ€–\n2\n2\n\\displaystyle=\\sum_{i=1}^{N}\\|\\phi((SF_{c})_{i:})-\\phi((F_{c})_{i:})\\|_{2}^{2}\n(11)\nâ‰¤\nâˆ‘\ni\n=\n1\nN\nL\n2\nâ€‹\nâ€–\n(\nS\nâ€‹\nF\nc\n)\ni\n:\nâˆ’\n(\nF\nc\n)\ni\n:\nâ€–\n2\n2\n\\displaystyle\\leq\\sum_{i=1}^{N}L^{2}\\|(SF_{c})_{i:}-(F_{c})_{i:}\\|_{2}^{2}\n=\nL\n2\nâ€‹\nâ€–\nS\nâ€‹\nF\nc\nâˆ’\nF\nc\nâ€–\nF\n2\n,\n\\displaystyle=L^{2}\\|SF_{c}-F_{c}\\|_{F}^{2},\nso\nâ€–\nÏ•\nâ€‹\n(\nS\nâ€‹\nF\nc\n)\nâˆ’\nÏ•\nâ€‹\n(\nF\nc\n)\nâ€–\nF\nâ‰¤\nL\nâ€‹\nâ€–\nS\nâ€‹\nF\nc\nâˆ’\nF\nc\nâ€–\nF\n\\|\\phi(SF_{c})-\\phi(F_{c})\\|_{F}\\leq L\\,\\|SF_{c}-F_{c}\\|_{F}\n.\nStandard bounded-difference or matrix Bernstein arguments for sampling without replacement yield concentration of this deviation as\nm\nm\ngrows.\nRemark 2 (Unbiased quadratic forms and covariance).\nFor any per-token linear map\nÏ•\nâ€‹\n(\nX\n)\n=\nX\nâ€‹\nW\n\\phi(X)=XW\n,\nğ”¼\nâ€‹\n[\n(\n(\nS\nâ€‹\nF\nc\n)\nâ€‹\nW\n)\nâŠ¤\nâ€‹\n(\n(\nS\nâ€‹\nF\nc\n)\nâ€‹\nW\n)\n]\n=\nm\nN\nâ€‹\n(\nF\nc\nâ€‹\nW\n)\nâŠ¤\nâ€‹\n(\nF\nc\nâ€‹\nW\n)\n.\n\\mathbb{E}\\big[((SF_{c})W)^{\\top}((SF_{c})W)\\big]=\\tfrac{m}{N}\\,(F_{c}W)^{\\top}(F_{c}W).\n(12)\nThus quadratic objectives and covariances computed from the sampled rows preserve the full counterparts in expectation up to a known scale\nm\n/\nN\nm/N\n.\nOne can reweight by\nN\n/\nm\nN/m\n, or rely on normalization layers to absorb the scale.\nCovariance remark.\nLet\nÎ£\n^\nm\nâ‰œ\n1\nm\nâˆ’\n1\nâ€‹\n(\nS\nâ€‹\nF\nc\n)\nâŠ¤\nâ€‹\n(\nS\nâ€‹\nF\nc\n)\n\\widehat{\\Sigma}_{m}\\triangleq\\tfrac{1}{m-1}(SF_{c})^{\\top}(SF_{c})\nbe the usual unbiased sample covariance built from the sampled rows, and\nÎ£\n^\nN\nâ‰œ\n1\nN\nâˆ’\n1\nâ€‹\nF\nc\nâŠ¤\nâ€‹\nF\nc\n\\widehat{\\Sigma}_{N}\\triangleq\\tfrac{1}{N-1}F_{c}^{\\top}F_{c}\nthe full-sample covariance.\nThen\nğ”¼\nâ€‹\n[\nÎ£\n^\nm\n]\n\\displaystyle\\mathbb{E}\\big[\\widehat{\\Sigma}_{m}\\big]\n=\n1\nm\nâˆ’\n1\nâ€‹\nğ”¼\nâ€‹\n[\n(\nS\nâ€‹\nF\nc\n)\nâŠ¤\nâ€‹\n(\nS\nâ€‹\nF\nc\n)\n]\n=\nm\nN\nâ€‹\n(\nm\nâˆ’\n1\n)\nâ€‹\nF\nc\nâŠ¤\nâ€‹\nF\nc\n\\displaystyle=\\frac{1}{m-1}\\,\\mathbb{E}\\big[(SF_{c})^{\\top}(SF_{c})\\big]=\\frac{m}{N(m-1)}\\,F_{c}^{\\top}F_{c}\n(13)\n=\nN\nâˆ’\n1\nN\nâ‹…\nm\nm\nâˆ’\n1\nâ€‹\nÎ£\n^\nN\n.\n\\displaystyle=\\frac{N-1}{N}\\cdot\\frac{m}{m-1}\\;\\widehat{\\Sigma}_{N}.\nThe factor\nÎ±\nâ‰œ\nN\nâˆ’\n1\nN\nâ‹…\nm\nm\nâˆ’\n1\n\\alpha\\triangleq\\tfrac{N-1}{N}\\cdot\\tfrac{m}{m-1}\nis close to\n1\n1\nwhenever\nm\nm\nand\nN\nN\nare moderate to large.\nFor\nm\n=\nN\n/\n2\nm=N/2\n,\nÎ±\n=\nN\nâˆ’\n1\nN\nâ‹…\nN\nN\nâˆ’\n2\n=\nN\nâˆ’\n1\nN\nâˆ’\n2\n=\n1\n+\nO\nâ€‹\n(\n1\n/\nN\n)\n\\alpha=\\tfrac{N-1}{N}\\cdot\\tfrac{N}{N-2}=\\tfrac{N-1}{\\,N-2\\,}=1+O(1/N)\n.\nHence the covariance is preserved in expectation up to a negligible finite-sample correction.\nIn practice, either explicit reweighting (\nN\n/\nm\nN/m\n) or standard normalization layers absorb any global scale, and the subspace preservation of StepsÂ 2â€“3 is unaffected by scale.\nA.5\nSelf Attensin Causes the Patch Redundancy\nSelf-attention mixing.\nGiven a patch embedding matrix\nX\nâˆˆ\nâ„\nN\nÃ—\nD\nX\\in\\mathbb{R}^{N\\times D}\n, a transformer layer forms\nQ\n=\nX\nâ€‹\nW\nQ\nQ=XW^{Q}\n,\nK\n=\nX\nâ€‹\nW\nK\nK=XW^{K}\n, and\nV\n=\nX\nâ€‹\nW\nV\nV=XW^{V}\nwith learned projections.\nThe attention logits are\nG\n=\nQ\nâ€‹\nK\nâŠ¤\nG=QK^{\\top}\n, and the token update is\nY\n=\nSoftMax\n(\nG\n)\nV\nâˆˆ\nâ„\nN\nÃ—\nD\n.\nY\\;=\\;\\texttt{{SoftMax}}(G)\\,V\\quad\\in\\;\\mathbb{R}^{N\\times D}.\n(14)\nHence, each output token\nY\ni\nY_{i}\nis a convex combination of all value vectors\nV\nj\nV_{j}\nthat are weighted by content similarity through the softmax of\nQ\ni\nâ€‹\nK\nj\nâŠ¤\nQ_{i}K_{j}^{\\top}\n. This induces global mixing of information across tokens, so the descriptor at each spatial location carries scene-wide context.",
    "preview_text": "Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.\n\nSee Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection\nAmir Mallak\n1\nErfan Aasi\n2\nShiva Sreeram\n2\nTsun-Hs",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "VLA",
        "diffusion",
        "Flow Matching",
        "locomotion",
        "VLM",
        "whole body control"
    ],
    "one_line_summary": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºåŸºç¡€æ¨¡å‹çš„éšæœºè¡¥ä¸é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºæé«˜ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç­–ç•¥çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ï¼Œä½†ä¸å¼ºåŒ–å­¦ä¹ ã€VLAã€æ‰©æ•£æ¨¡å‹ã€Flow Matchingã€è¿åŠ¨æ§åˆ¶ã€VLMå’Œå…¨èº«æ§åˆ¶ç­‰å…³é”®è¯çš„ç›´æ¥ç›¸å…³æ€§æœ‰é™ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦\n\n**è®ºæ–‡æ ‡é¢˜ï¼š** ã€Šçœ‹å¾—æ›´å°‘ï¼Œå¼€å¾—æ›´å¥½ï¼šé€šè¿‡åŸºç¡€æ¨¡å‹éšæœºå—é€‰æ‹©å®ç°å¯æ³›åŒ–çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ã€‹\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\néšç€ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å‘å±•ï¼Œåˆ©ç”¨å¤§å‹åŸºç¡€æ¨¡å‹æå–**å—å¯¹é½ç‰¹å¾**æ¥è®­ç»ƒè½»é‡çº§ç­–ç•¥å·²æˆä¸ºæå‡æ¨¡å‹**åˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›**çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºè§†è§‰Transformerä¸­çš„**è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼Œæ¯ä¸ªå›¾åƒå—çš„ç‰¹å¾éƒ½éšå¼åœ°åŒ…å«äº†æ¥è‡ªå…¶ä»–æ‰€æœ‰å—çš„ä¿¡æ¯ï¼Œå¯¼è‡´è¿™äº›å—çº§ç‰¹å¾æè¿°ç¬¦ä¹‹é—´å­˜åœ¨**é«˜åº¦å†—ä½™å’Œå¼ºç›¸å…³æ€§**ã€‚è¿™ç§å†—ä½™ä¸ä»…æµªè´¹è®¡ç®—èµ„æºï¼Œæ›´ä¼šå¯¼è‡´ç­–ç•¥ç½‘ç»œå­¦ä¹ åˆ°è™šå‡çš„ç›¸å…³æ€§ï¼Œä»è€ŒæŸå®³å…¶åœ¨æœªè§è¿‡çš„åœºæ™¯ï¼ˆOODï¼‰ä¸­çš„é²æ£’æ€§ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡ºäº†**éšæœºå—é€‰æ‹©**æ–¹æ³•ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šåœ¨æ¯ä¸€å¸§å›¾åƒå¤„ç†æ—¶ï¼Œ**éšæœºä¸¢å¼ƒä¸€å®šæ¯”ä¾‹**çš„å—ç‰¹å¾æè¿°ç¬¦ï¼Œä»…å°†å‰©ä½™çš„ç‰¹å¾ï¼ˆä¿æŒå…¶åŸå§‹ç©ºé—´å¸ƒå±€ï¼‰è¾“å…¥ç»™ä¸‹æ¸¸çš„ç­–ç•¥ç½‘ç»œã€‚SPSé€šè¿‡å¼•å…¥è¿™ç§**éšæœºä½†å®Œæ•´çš„åœºæ™¯è§†å›¾**ï¼Œè¿«ä½¿ç­–ç•¥å­¦ä¹ åŸºäºé‚£äº›å¯¹å…·ä½“å“ªäº›å—è¢«ä¿ç•™**ä¸æ•æ„Ÿ**çš„ã€æ›´æœ¬è´¨çš„ç‰¹å¾ï¼Œä»è€Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•æ˜¯ä¸€ä¸ª**å³æ’å³ç”¨**çš„æ¨¡å—ï¼Œæ— éœ€å¾®è°ƒåŸºç¡€æ¨¡å‹ï¼Œå¹¶èƒ½æ˜¾è‘—å‡å°‘è®¡ç®—é‡ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nåœ¨å¹¿æ³›çš„é—­ç¯é©¾é©¶å®éªŒä¸­ï¼ŒSPSæ–¹æ³•åœ¨**æ‰€æœ‰OODåœºæ™¯**ä¸‹å‡è¶…è¶Šäº†ç°æœ‰æœ€ä½³æ–¹æ³•ï¼š\n*   **æ€§èƒ½æå‡**ï¼šå¹³å‡æˆåŠŸç‡æå‡ **6.2%**ï¼Œåœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸‹æå‡é«˜è¾¾ **20.4%**ã€‚\n*   **æ•ˆç‡æå‡**ï¼šæ¨ç†é€Ÿåº¦æå‡ **2.4å€**ã€‚\n*   **é²æ£’æ€§éªŒè¯**ï¼šå¯¹9ç§ä¸åŒå˜ä½“ï¼ˆä¸åŒæ©ç ç‡ã€é‡ç»„ç­–ç•¥ï¼‰çš„æ¶ˆèå®éªŒä¸­ï¼Œæœ‰8ç§è¶…è¶Šäº†å…ˆå‰çš„æœ€ä½³æ–¹æ³•ã€‚\n*   **ä»¿çœŸåˆ°ç°å®è¿ç§»**ï¼šåœ¨ä»¿çœŸä¸­è®­ç»ƒçš„SPSç­–ç•¥èƒ½å¤Ÿ**æ— ç¼è¿ç§»**åˆ°çœŸå®ä¸–ç•Œçš„ç‰©ç†è½¦è¾†ä¸Šï¼Œæ— éœ€ä»»ä½•è°ƒä¼˜ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\næœ¬ç ”ç©¶æ­ç¤ºäº†åŸºäºåŸºç¡€æ¨¡å‹çš„è‡ªåŠ¨é©¾é©¶ç‰¹å¾æå–ä¸­å­˜åœ¨çš„å†—ä½™é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚SPSä¸ä»…é€šè¿‡**ä¸»åŠ¨å¼•å…¥éšæœºæ€§**æ¥æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œè¿˜é€šè¿‡**å‡å°‘è®¡ç®—å†—ä½™**å®ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚å…¶â€œå³æ’å³ç”¨â€çš„ç‰¹æ€§ä½¿å…¶æ˜“äºé›†æˆåˆ°ç°æœ‰æ¡†æ¶ä¸­ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºæ›´é«˜æ•ˆã€æ›´å¯é çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†æ–°çš„æ€è·¯ï¼Œå¹¶è¯æ˜äº†åœ¨ç‰¹å¾å±‚é¢â€œå°‘å³æ˜¯å¤šâ€çš„è®¾è®¡å“²å­¦ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­ï¼ŒåŸºäºåŸºç¡€æ¨¡å‹æå–çš„â€œå—å¯¹é½ç‰¹å¾â€å­˜åœ¨é«˜åº¦å†—ä½™**çš„é—®é¢˜ã€‚è¿™ç§å†—ä½™å¯¼è‡´ç­–ç•¥ç½‘ç»œå®¹æ˜“è¿‡æ‹Ÿåˆåˆ°è™šå‡çš„ç›¸å…³æ€§ä¸Šï¼Œä»è€Œä¸¥é‡æŸå®³äº†æ¨¡å‹åœ¨**åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›**ã€‚\n\n### **æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼šéšæœºå—é€‰æ‹©**\nè®ºæ–‡æå‡ºäº† **Stochastic Patch Selection** æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ä¸ª**ç®€å•ã€é«˜æ•ˆä¸”å³æ’å³ç”¨**çš„è§£å†³æ–¹æ¡ˆã€‚\n\n**æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨æ¯ä¸€å¸§è¾“å…¥æ—¶ï¼Œ**éšæœºä¸¢å¼ƒï¼ˆæ©ç ï¼‰ä¸€éƒ¨åˆ†å›¾åƒå—çš„ç‰¹å¾æè¿°ç¬¦**ï¼Œåªå°†å‰©ä½™çš„ç‰¹å¾ï¼ˆä¿æŒå…¶åŸå§‹ç©ºé—´å¸ƒå±€ï¼‰è¾“å…¥ç»™ä¸‹æ¸¸çš„ç­–ç•¥ç½‘ç»œè¿›è¡Œè®­ç»ƒå’Œæ¨ç†ã€‚\n\n### **å¦‚ä½•è§£å†³**\nSPS é€šè¿‡ä¸€ç§â€œä»¥é€€ä¸ºè¿›â€çš„æœºåˆ¶ï¼Œä»ä¸‰ä¸ªå±‚é¢è§£å†³äº†é—®é¢˜ï¼š\n\n1.  **å¼ºåˆ¶å­¦ä¹ ä¸å˜æ€§**ï¼šé€šè¿‡ä¸ºåŒä¸€åœºæ™¯æä¾›ä¸åŒçš„ã€éšæœºçš„ä½†å®Œæ•´çš„â€œå­è§†å›¾â€ï¼Œè¿«ä½¿ç­–ç•¥ç½‘ç»œå¿…é¡»åŸºäºé‚£äº›**ä¸éšå…·ä½“å“ªäº›å—è¢«ä¿ç•™è€Œå˜åŒ–çš„ã€æ›´æœ¬è´¨çš„ç‰¹å¾**æ¥åšå†³ç­–ã€‚è¿™ç±»ä¼¼äºä¸€ç§å¼ºæ•°æ®å¢å¼ºï¼Œæå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚\n2.  **æ‰“ç ´å†—ä½™ä¸è™šå‡å…³è”**ï¼šéšæœºä¸¢å¼ƒé«˜åº¦ç›¸å…³çš„å—ç‰¹å¾ï¼Œåˆ‡æ–­äº†ç­–ç•¥ç½‘ç»œå¯èƒ½ä¾èµ–çš„è™šå‡ç»Ÿè®¡å…³è”ï¼Œé¼“åŠ±å…¶å­¦ä¹ çœŸæ­£å…·æœ‰å› æœå…³ç³»çš„ã€ç©ºé—´ä¸Š grounded çš„çº¿ç´¢ã€‚\n3.  **æå‡è®¡ç®—æ•ˆç‡**ï¼šç”±äºåªéœ€ä¸ºåŸºç¡€æ¨¡å‹ä¸­è¢«é€‰ä¸­çš„å—è®¡ç®—ç‰¹å¾ï¼Œ**æ˜¾è‘—å‡å°‘äº†å‰å‘ä¼ æ’­çš„è®¡ç®—é‡**ã€‚è®ºæ–‡ä¸­æœ€ä½³æ¨¡å‹å®ç°äº† **2.4å€** çš„åŠ é€Ÿã€‚\n\n### **æŠ€æœ¯å®ç°è¦ç‚¹**\n- **åŸºç¡€**ï¼šæ²¿ç”¨å…ˆå‰å·¥ä½œï¼ˆDrive Anywhereï¼‰çš„æ–¹æ³•ï¼Œä½¿ç”¨ BLIP-2 ç­‰åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡**æ©ç æ³¨æ„åŠ›**æœºåˆ¶æå–æ¯ä¸ªå›¾åƒå—çš„è§†è§‰-è¯­è¨€å¯¹é½ç‰¹å¾ã€‚\n- **å…³é”®æ“ä½œ**ï¼šåœ¨ç‰¹å¾æå–åã€ç­–ç•¥ç½‘ç»œå‰ï¼Œå¼•å…¥ SPS æ¨¡å—ã€‚å¯¹æ¯ä¸ªè®­ç»ƒ/æ¨ç†å¸§ï¼š\n    - ä»¥å›ºå®šæ¯”ä¾‹ï¼ˆå¦‚ 50%ï¼‰éšæœºé€‰æ‹©ä¸€æ‰¹å—ç´¢å¼•ã€‚\n    - ä»…è®¡ç®—è¿™äº›é€‰ä¸­å—çš„æè¿°ç¬¦ã€‚\n    - æ„å»ºä¸€ä¸ªä¸åŸå§‹ç©ºé—´ç½‘æ ¼åŒå°ºå¯¸çš„å¼ é‡ï¼Œé€‰ä¸­ä½ç½®å¡«å……è®¡ç®—å‡ºçš„ç‰¹å¾ï¼Œæœªé€‰ä¸­ä½ç½®å¡«å……é›¶ï¼ˆæˆ–ç›´æ¥ç§»é™¤å¹¶è°ƒæ•´ä½ç½®ç¼–ç ï¼‰ã€‚\n- **ç†è®ºæ”¯æ’‘**ï¼šè®ºæ–‡é€šè¿‡ PCA å’Œå—é—´ç›¸ä¼¼æ€§åˆ†æ**é‡åŒ–äº†ç‰¹å¾å†—ä½™**ï¼ˆä¾‹å¦‚ï¼Œ90%çš„æ–¹å·®ä»…ç”± 17/64 ä¸ªä¸»æˆåˆ†è§£é‡Šï¼‰ï¼Œå¹¶ä»å­ç©ºé—´ä¿æŒçš„è§’åº¦è¯æ˜äº†éšæœºé‡‡æ ·çš„åˆç†æ€§ã€‚\n- **å˜ä½“ä¸æ¶ˆè**ï¼šæ¢ç´¢äº†å›ºå®šæ•°é‡ vs. æ¦‚ç‡é˜ˆå€¼é‡‡æ ·ã€ä¿ç•™é›¶å€¼ vs. ç§»é™¤ä»¤ç‰Œç­‰ä¸åŒå˜ä½“ï¼Œå…±è®­ç»ƒäº†9ä¸ªç³»ç»Ÿï¼Œå…¶ä¸­8ä¸ªè¶…è¶Šäº†ä¹‹å‰çš„SOTAã€‚\n\n### **å®é™…ä»·å€¼**\n- **æ€§èƒ½æ˜¾è‘—æå‡**ï¼šåœ¨å¹¿æ³›çš„OODæµ‹è¯•åœºæ™¯ï¼ˆä¸åŒå­£èŠ‚ã€å¤©æ°”ã€å…‰ç…§ã€åŠ¨æ€ç‰©ä½“ï¼‰ä¸­ï¼Œå¹³å‡æ€§èƒ½æå‡ **6.2%**ï¼Œåœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­æå‡é«˜è¾¾ **20.4%**ã€‚\n- **æ•ˆç‡ä¸æ€§èƒ½å…¼å¾—**ï¼šåœ¨æå‡æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œè·å¾—äº† **2.4å€** çš„æ¨ç†åŠ é€Ÿï¼Œå®ç°äº†â€œæ—¢å¿«åˆå¥½â€ã€‚\n- **å¼ºå¤§çš„å¯è¿ç§»æ€§**ï¼šåœ¨æ¨¡æ‹Ÿå™¨ä¸­è®­ç»ƒçš„SPSç­–ç•¥ï¼Œ**æ— éœ€ä»»ä½•å¾®è°ƒ**å³å¯ç›´æ¥éƒ¨ç½²åˆ°çœŸå®çš„ç‰©ç†è½¦è¾†ä¸Šï¼ŒæˆåŠŸåº”å¯¹äº†ä¹¡æ‘é“è·¯å’Œåœ°ä¸‹åœè½¦åœºç­‰å¤æ‚åœºæ™¯ï¼Œè¯æ˜äº†å…¶å“è¶Šçš„ Sim-to-Real èƒ½åŠ›ã€‚\n- **æ–¹æ³•é€šç”¨**ï¼šSPS ä¸ä¾èµ–äºç‰¹å®šåŸºç¡€æ¨¡å‹ï¼ˆåœ¨ BLIP-2 å’Œ DINO ä¸Šå‡æœ‰æ•ˆï¼‰ï¼Œæ— éœ€å¾®è°ƒåŸºç¡€æ¨¡å‹ï¼Œæ˜¯ä¸€ç§è½»é‡çº§ã€å¯æ’æ‹”çš„é€šç”¨å¢å¼ºæŠ€æœ¯ã€‚\n\n**æ€»ç»“**ï¼šè¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ´å¯Ÿæ˜¯â€œå°‘å³æ˜¯å¤šâ€ã€‚å®ƒå‘ç°å¹¶å®è¯äº†ç°æœ‰æ–¹æ³•ä¸­ç‰¹å¾å†—ä½™çš„å±å®³ï¼Œå¹¶æå‡ºäº†ä¸€ç§æå…¶ç®€å•å´æœ‰æ•ˆçš„éšæœºä¸¢å¼ƒæ–¹æ¡ˆã€‚SPS é€šè¿‡å¼•å…¥å—æ§çš„éšæœºæ€§æ¥æå‡æ¨¡å‹çš„é²æ£’æ€§å’Œæ•ˆç‡ï¼Œä¸ºåŸºäºåŸºç¡€æ¨¡å‹çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æä¾›äº†ä¸€ç§æ–°çš„ã€å…·æœ‰å¼ºå¤§å®ç”¨ä»·å€¼çš„æŠ€æœ¯è·¯å¾„ã€‚",
            "timestamp": "2026-01-20T17:57:35.237037",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­åŸºäºåŸºç¡€æ¨¡å‹ç‰¹å¾æå–çš„å‘å±•è„‰ç»œåˆ†æ\n\næœ¬æ–‡ã€ŠSee Less, Drive Betterã€‹å»ºç«‹åœ¨è¿‘æœŸâ€œåŸºäºåŸºç¡€æ¨¡å‹çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶â€è¿™ä¸€æ–°å…´æ–¹å‘ä¹‹ä¸Šã€‚å…¶æ ¸å¿ƒå‘å±•è„‰ç»œå¯ä»¥æ¢³ç†ä¸ºä»¥ä¸‹å‡ ä¸ªå…³é”®é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½é’ˆå¯¹å‰ä¸€é˜¶æ®µçš„å±€é™æ€§è¿›è¡Œäº†æ”¹è¿›ï¼Œå¹¶åˆ†åŒ–å‡ºä¸åŒçš„æŠ€æœ¯å‘å±•æ–¹å‘ã€‚\n\n### å‘å±•è„‰ç»œæ€»è§ˆ\n\n```mermaid\ntimeline\n    title ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç‰¹å¾æå–å‘å±•è„‰ç»œ\n    section ä¼ ç»Ÿç«¯åˆ°ç«¯æ–¹æ³•\n        æ—©æœŸå·¥ä½œ : åŸå§‹åƒç´ åˆ°æ§åˆ¶<br>éœ€å¤§é‡çœŸå®æ•°æ®ï¼Œæ³›åŒ–æ€§å·®\n        æ”¹è¿›æ–¹å‘ : å¼•å…¥æ¦‚ç‡ç›®æ ‡ã€<br>æ³¨æ„åŠ›æœºåˆ¶ã€å®‰å…¨å‡½æ•°\n    section å¼•å…¥ä¸­é—´è¡¨å¾\n        è§†è§‰æŠ½è±¡ : ä½¿ç”¨è¯­ä¹‰ã€è½¦é“æ‹“æ‰‘ç­‰<br>ç®€åŒ–æ§åˆ¶ï¼Œä¾èµ–æ ‡æ³¨æˆ–æ¨¡å‹\n    section åŸºç¡€æ¨¡å‹æ—¶ä»£\n        Drive Anywhere : æå–patchçº§å¤šæ¨¡æ€ç‰¹å¾<br>æ˜¾è‘—æå‡OODæ³›åŒ–ï¼Œä½†ç‰¹å¾å†—ä½™\n        See Less, Drive Better : æå‡ºSPSï¼Œéšæœºæ©ç å†—ä½™patch<br>æå‡æ•ˆç‡ä¸æ³›åŒ–ï¼Œå®ç°â€œå°‘å³æ˜¯å¤šâ€\n```\n\n### é˜¶æ®µä¸€ï¼šä¼ ç»Ÿç«¯åˆ°ç«¯æ–¹æ³•ï¼ˆRaw Pixels to Controlï¼‰\n\n*   **ä»£è¡¨å·¥ä½œ**ï¼šALVINN (1988), NVIDIA PilotNet (2016) åŠåç»­åŸºäºå·ç§¯ç½‘ç»œï¼ˆCNNï¼‰æˆ–å¾ªç¯ç½‘ç»œï¼ˆRNNï¼‰çš„æ¨¡å‹ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šä½¿ç”¨å•ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç›´æ¥ä»åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰æ˜ å°„åˆ°æ§åˆ¶æŒ‡ä»¤ï¼ˆå¦‚æ–¹å‘ç›˜è½¬è§’ã€æ²¹é—¨ï¼‰ã€‚\n*   **æ”¹è¿›ä¸ä»·å€¼**ï¼š\n    *   **ç®€åŒ–ç³»ç»Ÿ**ï¼šæ‘’å¼ƒäº†ä¼ ç»Ÿæ¨¡å—åŒ–ï¼ˆæ„ŸçŸ¥-è§„åˆ’-æ§åˆ¶ï¼‰æµæ°´çº¿ä¸­ä¸¥æ ¼çš„å‡è®¾å’Œæ¥å£ï¼Œå®ç°äº†æ›´è¿è´¯çš„ä¼˜åŒ–ã€‚\n    *   **æ•°æ®é©±åŠ¨**ï¼šèƒ½å¤Ÿä»å¤§è§„æ¨¡æ•°æ®ä¸­å­¦ä¹ å¤æ‚çš„æ˜ å°„å…³ç³»ã€‚\n*   **ä¸»è¦å±€é™**ï¼š\n    *   **æ•°æ®éœ€æ±‚å¤§**ï¼šéœ€è¦å¤§é‡è¦†ç›–å„ç§åœºæ™¯çš„çœŸå®ä¸–ç•Œé©¾é©¶æ•°æ®ï¼Œæ”¶é›†æˆæœ¬é«˜ã€é£é™©å¤§ã€‚\n    *   **æ³›åŒ–èƒ½åŠ›å¼±**ï¼šå¯¹è®­ç»ƒæ•°æ®åˆ†å¸ƒï¼ˆIn-Distribution, IDï¼‰ä¹‹å¤–çš„åœºæ™¯ï¼ˆOut-Of-Distribution, OODï¼‰ï¼Œå¦‚æœªçŸ¥å¤©æ°”ã€å…‰ç…§ã€ç‰©ä½“ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚\n    *   **å¯è§£é‡Šæ€§å·®**ï¼šæ¨¡å‹å†³ç­–è¿‡ç¨‹åƒä¸€ä¸ªé»‘ç›’ã€‚\n\n### é˜¶æ®µäºŒï¼šå¼•å…¥ä¸­é—´è§†è§‰æŠ½è±¡ï¼ˆIntermediate Visual Abstractionsï¼‰\n\n*   **ä»£è¡¨å·¥ä½œ**ï¼šä½¿ç”¨è¯­ä¹‰åˆ†å‰²ã€è½¦é“çº¿æ£€æµ‹ã€ affordanceï¼ˆåŠŸèƒ½å¯è§æ€§ï¼‰ç­‰ä¸­é—´è¡¨å¾ä½œä¸ºç­–ç•¥ç½‘ç»œçš„è¾“å…¥ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šä¸ç›´æ¥ä½¿ç”¨åŸå§‹åƒç´ ï¼Œè€Œæ˜¯å…ˆé€šè¿‡ä¸€ä¸ªæ„ŸçŸ¥æ¨¡å—æå–é«˜çº§çš„ã€ä»»åŠ¡ç›¸å…³çš„è§†è§‰æŠ½è±¡ç‰¹å¾ï¼Œå†åŸºäºè¿™äº›ç‰¹å¾è¿›è¡Œæ§åˆ¶å†³ç­–ã€‚\n*   **æ”¹è¿›ä¸ä»·å€¼**ï¼š\n    *   **ç®€åŒ–å­¦ä¹ ä»»åŠ¡**ï¼šå°†å¤æ‚çš„åƒç´ åˆ°æ§åˆ¶æ˜ å°„ï¼Œåˆ†è§£ä¸ºâ€œæ„ŸçŸ¥æŠ½è±¡â€å’Œâ€œåŸºäºæŠ½è±¡çš„æ§åˆ¶â€ä¸¤ä¸ªç›¸å¯¹ç®€å•çš„å­é—®é¢˜ã€‚\n    *   **æå‡å¯è§£é‡Šæ€§**ï¼šå†³ç­–åŸºäºäººç±»å¯ç†è§£çš„ç‰¹å¾ï¼ˆå¦‚â€œå‰æ–¹æœ‰è½¦â€ã€â€œè½¦é“çº¿å¼¯æ›²â€ï¼‰ã€‚\n    *   **ä¸€å®šç¨‹åº¦æå‡æ³›åŒ–**ï¼šå¯¹åƒç´ çº§å˜åŒ–ï¼ˆå¦‚é¢œè‰²ã€çº¹ç†ï¼‰çš„é²æ£’æ€§å¢å¼ºã€‚\n*   **ä¸»è¦å±€é™**ï¼š\n    *   **ä¾èµ–ä¸Šæ¸¸æ¨¡å‹è´¨é‡**ï¼šä¸­é—´è¡¨å¾çš„å‡†ç¡®æ€§ä¸¥é‡ä¾èµ–äºè¯­ä¹‰åˆ†å‰²ã€æ£€æµ‹ç­‰ä¸Šæ¸¸æ¨¡å‹çš„æ€§èƒ½ï¼Œé”™è¯¯ä¼šä¼ æ’­åˆ°ä¸‹æ¸¸ã€‚\n    *   **ä¿¡æ¯æŸå¤±**ï¼šæŠ½è±¡è¿‡ç¨‹å¯èƒ½ä¸¢å¤±å¯¹é©¾é©¶å†³ç­–æœ‰ç”¨çš„ç»†èŠ‚ä¿¡æ¯ã€‚\n    *   **æ¨¡å—åŒ–é—ç•™é—®é¢˜**ï¼šæœ¬è´¨ä¸Šä»æ˜¯ä¸¤é˜¶æ®µç³»ç»Ÿï¼Œå¯èƒ½å¼•å…¥æ¬¡ä¼˜è§£ã€‚\n\n### é˜¶æ®µä¸‰ï¼šå¼•å…¥åŸºç¡€æ¨¡å‹ç‰¹å¾ï¼ˆFoundation Model Featuresï¼‰\n\nè¿™æ˜¯æœ¬æ–‡æ‰€ç«‹è¶³çš„ç›´æ¥å‰ä½œé˜¶æ®µï¼Œæ ¸å¿ƒæ˜¯åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºå¼ºå¤§çš„ç‰¹å¾æå–å™¨ã€‚\n\n#### æ–¹å‘ä¸€ï¼šåŸºäºåŒºåŸŸçš„ç‰¹å¾æå–ï¼ˆRegion-First Pipelinesï¼‰\n\n*   **ä»£è¡¨å·¥ä½œ**ï¼šä½¿ç”¨SAMã€Mask2Formerç­‰æ¨¡å‹å…ˆå¯¹å›¾åƒè¿›è¡Œåˆ†å‰²ï¼Œç„¶ååœ¨æ¯ä¸ªåŒºåŸŸå†…éƒ¨ä½¿ç”¨CLIPç­‰ç¼–ç å™¨æ± åŒ–ç‰¹å¾ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šåˆ©ç”¨åŸºç¡€æ¨¡å‹è·å¾—ç‰©ä½“çº§åˆ«çš„ã€å¼€æ”¾è¯æ±‡çš„è¯­ä¹‰ç‰¹å¾ã€‚\n*   **æ”¹è¿›**ï¼šæä¾›äº†ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œæ”¯æŒè¯­è¨€äº¤äº’å’Œè°ƒè¯•ã€‚\n*   **å±€é™ï¼ˆä¹Ÿæ˜¯æœ¬æ–‡çš„å¯¹æ¯”åŸºçº¿ï¼‰**ï¼š\n    *   **è®¡ç®—å¤æ‚**ï¼šéœ€è¦è¿è¡Œåˆ†å‰²å’Œç¼–ç ä¸¤å¥—æ¨¡å‹ã€‚\n    *   **ä¾èµ–åˆ†å‰²è´¨é‡**ï¼šåˆ†å‰²é”™è¯¯ä¼šå¯¼è‡´ç‰¹å¾é—æ¼æˆ–é”™è¯¯ã€‚\n    *   **ä¸¢å¤±ç©ºé—´å¸ƒå±€**ï¼šåŒºåŸŸç‰¹å¾é€šå¸¸è¢«æ± åŒ–ä¸ºä¸€ä¸ªå…¨å±€å‘é‡ï¼Œä¸¢å¤±äº†ç²¾ç¡®çš„ç©ºé—´å…³ç³»ã€‚\n\n#### æ–¹å‘äºŒï¼šåŸºäºå¯¹é½å›¾åƒå—çš„ç‰¹å¾æå–ï¼ˆPatch-Aligned Feature Extractionï¼‰â€”â€” **æœ¬æ–‡çš„åŸºçŸ³**\n\n*   **ä»£è¡¨å·¥ä½œ**ï¼š**Drive Anywhere (Wang et al., 2024)**ï¼Œ å³æœ¬æ–‡æ‰€æŒ‡çš„SOTAå’Œç›´æ¥å‰ä½œã€‚\n*   **æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°**ï¼š\n    1.  **ç‰¹å¾æå–æ–¹æ³•**ï¼šæå‡º **â€œæ©ç æ³¨æ„åŠ›â€** æœºåˆ¶ã€‚åœ¨BLIP2ç­‰VLMçš„ç‰¹å®šå±‚ï¼ˆå¦‚Q-Formerè¾“å…¥ï¼‰ï¼Œé€šè¿‡æ–½åŠ ä¸€ä¸ªç©ºé—´æ©ç ï¼Œå¼ºåˆ¶æ¨¡å‹ä»…å…³æ³¨å•ä¸ªå›¾åƒå—ï¼ˆpatchï¼‰åŠå…¶é‚»åŸŸï¼Œä»è€Œä¸º**æ¯ä¸ªå›¾åƒå—ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„ç‰¹å¾æè¿°ç¬¦**ã€‚è¿™äº›æè¿°ç¬¦ä½äºè§†è§‰-è¯­è¨€å…±äº«ç©ºé—´ã€‚\n    2.  **ç³»ç»Ÿæ¶æ„**ï¼šä½¿ç”¨**å†»ç»“çš„**åŸºç¡€æ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œä»…è®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„ç­–ç•¥ç½‘ç»œï¼ˆå¦‚ViTï¼‰åŸºäºè¿™äº›patchç‰¹å¾è¿›è¡Œé©¾é©¶å†³ç­–ã€‚\n*   **ç›¸æ¯”äºä¹‹å‰å·¥ä½œçš„é‡å¤§æ”¹è¿›**ï¼š\n    *   **æå¼ºçš„OODæ³›åŒ–**ï¼šåˆ©ç”¨åŸºç¡€æ¨¡å‹åœ¨è¶…å¤§è§„æ¨¡æ•°æ®ä¸Šå­¦åˆ°çš„é€šç”¨è§†è§‰-è¯­è¨€çŸ¥è¯†ï¼Œå¯¹æœªè§è¿‡çš„åœºæ™¯ã€å¤©æ°”ã€ç‰©ä½“è¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚\n    *   **ä¿ç•™ç©ºé—´ä¿¡æ¯**ï¼špatch-alignedç‰¹å¾ä¿æŒäº†åŸå§‹å›¾åƒçš„ç©ºé—´ç½‘æ ¼å¸ƒå±€ï¼Œä¸ºç­–ç•¥ç½‘ç»œæä¾›äº†ç»†ç²’åº¦çš„ç©ºé—´ä¸Šä¸‹æ–‡ã€‚\n    *   **æ”¯æŒè¯­è¨€äº¤äº’**ï¼šç‰¹å¾å­˜åœ¨äºå¤šæ¨¡æ€ç©ºé—´ï¼Œå¤©ç„¶æ”¯æŒåŸºäºæ–‡æœ¬çš„å¢å¼ºã€è°ƒè¯•å’ŒæŒ‡ä»¤ç†è§£ã€‚\n    *   **æ•ˆç‡ä¸å¯è¿ç§»æ€§**ï¼šå†»ç»“åŸºç¡€æ¨¡å‹ï¼Œåªéœ€è®­ç»ƒå°å‹ç­–ç•¥ç½‘ç»œï¼Œè®¡ç®—æˆæœ¬ç›¸å¯¹å¯æ§ï¼Œä¸”æ˜“äºè¿ç§»ã€‚\n*   **é—ç•™çš„æ ¸å¿ƒé—®é¢˜ï¼ˆæœ¬æ–‡çš„å‡ºå‘ç‚¹ï¼‰**ï¼š\n    *   **ç‰¹å¾å†—ä½™**ï¼šç”±äºViTä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¯ä¸ªpatchçš„ç‰¹å¾éƒ½éšå¼åŒ…å«äº†å…¨å±€ä¿¡æ¯ï¼Œå¯¼è‡´ä¸åŒpatchçš„ç‰¹å¾é«˜åº¦ç›¸å…³å’Œå†—ä½™ã€‚è¿™ä¼šä½¿ç­–ç•¥ç½‘ç»œè¿‡æ‹Ÿåˆäºè™šå‡çš„ç›¸å…³æ€§ï¼ŒæŸå®³é²æ£’æ€§ï¼Œå¹¶é€ æˆè®¡ç®—æµªè´¹ã€‚\n\n### é˜¶æ®µå››ï¼šé’ˆå¯¹ç‰¹å¾å†—ä½™çš„ä¼˜åŒ– â€”â€” æœ¬æ–‡å·¥ä½œã€ŠSee Less, Drive Betterã€‹\n\n*   **æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°**ï¼š**éšæœºå—é€‰æ‹©ï¼ˆStochastic Patch Selection, SPSï¼‰**\n*   **é’ˆå¯¹å‰ä½œï¼ˆDrive Anywhereï¼‰çš„æ”¹è¿›**ï¼š\n    1.  **é—®é¢˜é©±åŠ¨**ï¼šé¦–æ¬¡æ˜ç¡®æŒ‡å‡ºå¹¶é‡åŒ–äº†patch-alignedç‰¹å¾çš„å†—ä½™æ€§é—®é¢˜ï¼ˆé€šè¿‡PCAã€ç›¸å…³æ€§åˆ†æï¼‰ï¼Œå¹¶å°†å…¶è§†ä¸ºå½±å“OODé²æ£’æ€§çš„å…³é”®å› ç´ ã€‚\n    2.  **æ–¹æ³•åˆ›æ–°**ï¼šæå‡ºåœ¨ç‰¹å¾æå–åã€ç­–ç•¥ç½‘ç»œå‰ï¼Œ**éšæœºä¸¢å¼ƒä¸€éƒ¨åˆ†patchçš„ç‰¹å¾**ï¼ˆç”¨é›¶å¡«å……æˆ–ç›´æ¥ç§»é™¤ä½†ä¿ç•™ä½ç½®ç¼–ç ï¼‰ï¼ŒåŒæ—¶ä¿æŒå‰©ä½™ç‰¹å¾çš„ç©ºé—´å¸ƒå±€ä¸å˜ã€‚\n    3.  **è®­ç»ƒæœºåˆ¶**ï¼šé€šè¿‡ä¸ºæ¯ä¸€å¸§æä¾›ä¸åŒçš„ã€éšæœºçš„ä½†å®Œæ•´çš„åœºæ™¯â€œè§†å›¾â€ï¼Œ**å¼ºåˆ¶ç­–ç•¥ç½‘ç»œå­¦ä¹ åŸºäºé‚£äº›å¯¹å…·ä½“å“ªäº›tokenè¢«ä¿ç•™ä¸å˜çš„ã€å› æœç›¸å…³çš„ç‰¹å¾**ï¼Œä»è€ŒæŠ‘åˆ¶å¯¹å†—ä½™å’Œè™šå‡ç›¸å…³æ€§çš„ä¾èµ–ã€‚\n*   **å¸¦æ¥çš„å…¨é¢æå‡**ï¼š\n    *   **æ³›åŒ–æ€§ï¼ˆæ ¸å¿ƒè´¡çŒ®ï¼‰**ï¼šå¹³å‡OODæˆåŠŸç‡æå‡**6.2%**ï¼Œæœ€é«˜æå‡**20.4%**ã€‚åœ¨9ä¸ªå˜ä½“æ¨¡å‹ä¸­ï¼Œ8ä¸ªè¶…è¶Šäº†å‰SOTAã€‚\n    *   **æ•ˆç‡**ï¼šæ¨ç†é€Ÿåº¦æå‡ **2.4å€**ï¼ˆæœ€ä½³æ¨¡å‹SPS-50%ï¼‰ï¼Œå› ä¸ºåªéœ€è®¡ç®—éƒ¨åˆ†patchçš„ç‰¹å¾ã€‚\n    *   **ç†è®ºæ”¯æ’‘**ï¼šæä¾›äº†ä½ç§©å­ç©ºé—´å’Œéšæœºé‡‡æ ·ä¿ç•™è¯­ä¹‰çš„ç†è®ºåˆ†æï¼ˆLemma 1ï¼‰ï¼Œè¯æ˜â€œå°‘å³æ˜¯å¤šâ€çš„åˆç†æ€§ã€‚\n    *   **å…¼å®¹æ€§ä¸æ‰©å±•æ€§**ï¼š\n        *   ä¸æ–‡æœ¬é©±åŠ¨çš„æ½œåœ¨ç©ºé—´å¢å¼ºæ— ç¼ç»“åˆï¼Œå¸¦æ¥é¢å¤–**1.7%** æ€§èƒ½æå‡ã€‚\n        *   å¯è¿ç§»è‡³å…¶ä»–åŸºç¡€æ¨¡å‹ï¼ˆå¦‚DINOï¼‰ï¼Œå±•ç¤ºé€šç”¨æ€§ã€‚\n        *   æ— éœ€å¾®è°ƒåŸºç¡€æ¨¡å‹ï¼Œå³æ’å³ç”¨ã€‚\n    *   **ä»¿çœŸåˆ°ç°å®ï¼ˆSim-to-Realï¼‰**ï¼šæˆåŠŸéƒ¨ç½²åˆ°çœŸå®è½¦è¾†ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…OODåœºæ™¯ï¼ˆå¦‚åœ°ä¸‹è½¦åº“ï¼‰ä¸­çš„æœ‰æ•ˆæ€§ã€‚\n\n### æ€»ç»“ï¼šå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘\n\n1.  **è¾“å…¥è¡¨å¾çš„å‘å±•æ–¹å‘**ï¼š**åŸå§‹åƒç´  -> äººå·¥å®šä¹‰çš„ä¸­å±‚æŠ½è±¡ï¼ˆè¯­ä¹‰/è½¦é“ï¼‰ -> åŸºç¡€æ¨¡å‹é©±åŠ¨çš„é€šç”¨ç‰¹å¾ï¼ˆåŒºåŸŸ/å—å¯¹é½ï¼‰ -> å»å†—ä½™åŒ–çš„å—å¯¹é½ç‰¹å¾**ã€‚è¶‹åŠ¿æ˜¯è¿½æ±‚æ›´ä¸°å¯Œã€æ›´é€šç”¨ã€æ›´é«˜æ•ˆä¸”æ›´é²æ£’çš„ç‰¹å¾è¡¨ç¤ºã€‚\n2.  **æ³›åŒ–èƒ½åŠ›æå‡æ–¹å‘**ï¼š**ä¾èµ–å¤§é‡çœŸå®æ•°æ® -> åˆ©ç”¨ä»¿çœŸå™¨ -> åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯† -> åœ¨åŸºç¡€æ¨¡å‹ç‰¹å¾ä¸Šæ–½åŠ æ­£åˆ™åŒ–ï¼ˆå¦‚SPSï¼‰ä»¥æå‡OODé²æ£’æ€§**ã€‚æ ¸å¿ƒæ˜¯åˆ©ç”¨æ›´å¼ºå¤§çš„å…ˆéªŒå’Œæ›´æ™ºèƒ½çš„æ­£åˆ™åŒ–æ¥å…‹æœæ•°æ®åˆ†å¸ƒçš„å±€é™ã€‚\n3.  **ç³»ç»Ÿè®¾è®¡å“²å­¦æ–¹å‘**ï¼š**ç«¯åˆ°ç«¯é»‘ç®± -> æ¨¡å—åŒ–å¯è§£é‡Š -> åŸºäºå†»ç»“åŸºç¡€æ¨¡å‹çš„â€œç‰¹å¾æå–+è½»é‡ç­–ç•¥â€ -> åœ¨ç‰¹å¾å±‚é¢è¿›è¡Œå¹²é¢„å’Œä¼˜åŒ–çš„é«˜æ•ˆæ¶æ„**ã€‚åœ¨ä¿æŒç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ä¼˜åŠ¿çš„åŒæ—¶ï¼Œå¼•å…¥äº†å¯è§£é‡Šã€å¯å¹²é¢„çš„ä¸­é—´å±‚ï¼ˆåŸºç¡€æ¨¡å‹ç‰¹å¾ï¼‰ã€‚\n4.  **æ•ˆç‡ä¼˜åŒ–æ–¹å‘**ï¼š**è®¡ç®—å¯†é›†å‹å¤§æ¨¡å‹ -> è½»é‡çº§ç­–ç•¥ç½‘ç»œ -> åŠ¨æ€ç‰¹å¾é€‰æ‹©ï¼ˆSPSï¼‰**ã€‚",
            "timestamp": "2026-01-20T17:58:42.852902",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## å‰ä½œæ¢³ç†ä¸åˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œå…¶æ ¸å¿ƒå‰ä½œæ˜¯ **Wang et al. (2024) çš„ â€œDrive Anywhereâ€** å·¥ä½œã€‚è¯¥å·¥ä½œæ˜¯æœ¬æ–‡æ–¹æ³•ï¼ˆSPSï¼‰ç›´æ¥æ„å»ºå’Œå¯¹æ¯”çš„åŸºç¡€ã€‚æ­¤å¤–ï¼Œè®ºæ–‡åœ¨ç›¸å…³å·¥ä½œä¸­è¿˜æåˆ°äº†å…¶ä»–å‡ ç¯‡é‡è¦çš„å‰æœŸç ”ç©¶ï¼Œå…±åŒæ„æˆäº†è¯¥é¢†åŸŸçš„æŠ€æœ¯æ¼”è¿›è„‰ç»œã€‚\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†æ¢³ç†äº†ä¸»è¦å‰ä½œã€æœ¬æ–‡ï¼ˆSPSï¼‰ä¸å‰ä½œçš„åŒºåˆ«ä»¥åŠå…·ä½“çš„æŠ€æœ¯æ”¹è¿›ï¼š\n\n| è®ºæ–‡ / å·¥ä½œ | æ ¸å¿ƒæ–¹æ³• | ä¸æœ¬æ–‡ï¼ˆSPSï¼‰çš„ä¸»è¦åŒºåˆ« | æœ¬æ–‡ï¼ˆSPSï¼‰çš„å…·ä½“æ”¹è¿›ä¸åˆ›æ–° |\n| :--- | :--- | :--- | :--- |\n| **Wang et al. (2024) - â€œDrive Anywhereâ€** (ç›´æ¥å‰ä½œ) | 1. **åŸºç¡€æ¡†æ¶**ï¼šä½¿ç”¨å†»ç»“çš„å¤§å‹è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚BLIP-2ï¼‰æå–**å—å¯¹é½ï¼ˆpatch-alignedï¼‰çš„å¤šæ¨¡æ€ç‰¹å¾**ã€‚<br>2. **ç‰¹å¾æå–**ï¼šé‡‡ç”¨**æ©ç æ³¨æ„åŠ›ï¼ˆmasked attentionï¼‰**æœºåˆ¶ï¼Œåœ¨Q-Formerå±‚ä¸ºæ¯ä¸ªå›¾åƒå—ç”Ÿæˆä¸€ä¸ªèšç„¦äºè¯¥å—çš„æè¿°ç¬¦ã€‚<br>3. **ç­–ç•¥è®­ç»ƒ**ï¼šåœ¨è¿™äº›ä¸°å¯Œçš„ç‰¹å¾ä¸Šè®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„ç­–ç•¥ç½‘ç»œï¼Œå®ç°ç«¯åˆ°ç«¯é©¾é©¶ã€‚å…¶æ ¸å¿ƒè´¡çŒ®æ˜¯è¯æ˜äº†ä½¿ç”¨åŸºç¡€æ¨¡å‹ç‰¹å¾èƒ½æ˜¾è‘—æå‡**åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ³›åŒ–**èƒ½åŠ›ã€‚ | 1. **ç‰¹å¾å†—ä½™é—®é¢˜**ï¼šå‰ä½œæœªå¤„ç†åŸºç¡€æ¨¡å‹äº§ç”Ÿçš„å—ç‰¹å¾ä¹‹é—´å­˜åœ¨çš„**é«˜åº¦å†—ä½™å’Œç›¸å…³æ€§**é—®é¢˜ã€‚<br>2. **è®­ç»ƒæ•ˆç‡ä¸é²æ£’æ€§**ï¼šç›´æ¥ä½¿ç”¨å…¨éƒ¨å—ç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œå¯èƒ½å¯¼è‡´ç­–ç•¥è¿‡æ‹Ÿåˆäºè™šå‡ç›¸å…³æ€§ï¼Œä¸”è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚<br>3. **æ–¹æ³•ç„¦ç‚¹**ï¼šå‰ä½œèšç„¦äº**å¼•å…¥åŸºç¡€æ¨¡å‹ç‰¹å¾**çš„æœ‰æ•ˆæ€§ï¼Œè€Œæœ¬æ–‡èšç„¦äº**ä¼˜åŒ–è¿™äº›ç‰¹å¾çš„ä½¿ç”¨æ–¹å¼**ã€‚ | 1. **æ ¸å¿ƒåˆ›æ–°ï¼ˆSPSï¼‰**ï¼šæå‡ºäº†**éšæœºå—é€‰æ‹©**æœºåˆ¶ã€‚åœ¨æ¯ä¸€å¸§ï¼Œéšæœºä¸¢å¼ƒä¸€å®šæ¯”ä¾‹ï¼ˆå¦‚50%ï¼‰çš„å—æè¿°ç¬¦ï¼Œä»…å°†å‰©ä½™çš„æè¿°ç¬¦ï¼ˆä¿æŒå…¶ç©ºé—´å¸ƒå±€ï¼‰è¾“å…¥ç­–ç•¥ç½‘ç»œã€‚<br>2. **è§£å†³å†—ä½™**ï¼šè¿«ä½¿ç­–ç•¥å­¦ä¹ åŸºäº**ä¸åŒéšæœºå­é›†**çš„å†³ç­–ï¼Œä»è€Œå…³æ³¨**å¯¹å…·ä½“ä»¤ç‰Œå­˜ç•™ä¸å˜çš„ã€æ›´é²æ£’çš„ç‰¹å¾**ï¼Œå‡å°‘äº†è¿‡æ‹Ÿåˆã€‚<br>3. **æ€§èƒ½æå‡**ï¼šåœ¨OODåœºæ™¯ä¸‹ï¼Œå¹³å‡æˆåŠŸç‡æå‡**6.2%**ï¼Œæœ€é«˜æå‡è¾¾**20.4%**ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦åŠ å¿«**2.4å€**ã€‚<br>4. **ç†è®ºåˆ†æ**ï¼šé¦–æ¬¡é€šè¿‡PCAå’Œå—é—´ç›¸ä¼¼æ€§åˆ†æ**é‡åŒ–å¹¶è¯æ˜äº†**BLIP-2ç­‰æ¨¡å‹ç‰¹å¾çš„å†—ä½™æ€§ï¼ˆ90%æ–¹å·®ä»…ç”±17/64ä¸ªä¸»æˆåˆ†è§£é‡Šï¼‰ã€‚ |\n| **Amini et al. (2022) - â€œVISTA 2.0â€** (ä»¿çœŸä¸è®­ç»ƒæ¡†æ¶) | æå‡ºäº†ä¸€ä¸ª**å¼€æºã€æ•°æ®é©±åŠ¨çš„å¤šæ¨¡æ€ä»¿çœŸå™¨**ï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶ç­–ç•¥å­¦ä¹ ã€‚å®ƒåŸºäºçœŸå®é©¾é©¶æ—¥å¿—ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨¡æ‹Ÿç»éªŒï¼Œæ”¯æŒé—­ç¯è®­ç»ƒä¸è¯„ä¼°ã€‚ | æœ¬æ–‡å¹¶éç›´æ¥æ”¹è¿›ä»¿çœŸå™¨ï¼Œè€Œæ˜¯å°†SPSæ–¹æ³•**åº”ç”¨äº**ç”±VISTAç­‰ä»¿çœŸå™¨ç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚VISTAæ˜¯æœ¬æ–‡å®éªŒçš„åŸºç¡€å¹³å°ä¹‹ä¸€ã€‚ | **æ–¹æ³•é›†æˆ**ï¼šæœ¬æ–‡åœ¨VISTAä»¿çœŸæ¡†æ¶å†…è®­ç»ƒå’Œè¯„ä¼°SPSç­–ç•¥ï¼Œè¯æ˜äº†SPSåœ¨è¯¥å…ˆè¿›ä»¿çœŸç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡çš„è´¡çŒ®åœ¨äº**é©¾é©¶ç­–ç•¥æœ¬èº«çš„å­¦ä¹ æ–¹æ³•**ï¼Œè€Œéä»¿çœŸå¹³å°ã€‚ |\n| **æ—©æœŸç«¯åˆ°ç«¯æ–¹æ³•** (å¦‚Bojarski et al., 2016) | ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç›´æ¥å°†åŸå§‹åƒç´ æ˜ å°„åˆ°æ§åˆ¶æŒ‡ä»¤ï¼ˆè½¬å‘ã€æ²¹é—¨ï¼‰ã€‚ | 1. **è¡¨ç¤ºå±‚é¢**ï¼šæ—©æœŸæ–¹æ³•ä½¿ç”¨**åŸå§‹åƒç´ æˆ–CNNç‰¹å¾**ï¼Œè€Œæœ¬æ–‡åŠå‰ä½œä½¿ç”¨**åŸºç¡€æ¨¡å‹æå–çš„è¯­ä¹‰ä¸°å¯Œçš„å¤šæ¨¡æ€ç‰¹å¾**ã€‚<br>2. **æ³›åŒ–èƒ½åŠ›**ï¼šæ—©æœŸæ–¹æ³•ä¸¥é‡ä¾èµ–å¤§é‡çœŸå®æ•°æ®ï¼Œåœ¨OODåœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚ | **è¡¨ç¤ºå­¦ä¹ é£è·ƒ**ï¼šæœ¬æ–‡ç»§æ‰¿äº†â€œDrive Anywhereâ€çš„æ€è·¯ï¼Œä½†è¿›ä¸€æ­¥ä¼˜åŒ–äº†ç‰¹å¾è¡¨ç¤ºã€‚SPSé€šè¿‡åœ¨é«˜çº§è¯­ä¹‰ç‰¹å¾ç©ºé—´è¿›è¡ŒéšæœºåŒ–ï¼Œå®ç°äº†æ¯”æ—©æœŸåƒç´ çº§æ–¹æ³•**æ›´é«˜æ•ˆã€æ›´é²æ£’**çš„æ³›åŒ–ï¼Œå‡å°‘äº†å¯¹æµ·é‡çœŸå®é£é™©æ•°æ®çš„éœ€æ±‚ã€‚ |\n| **åŸºäºä¸­é—´è¡¨ç¤ºçš„é©¾é©¶** (å¦‚Toromanoff et al., 2020) | ä½¿ç”¨**è§†è§‰æŠ½è±¡**ï¼ˆå¦‚å¯é©¾é©¶åŒºåŸŸã€è¯­ä¹‰åˆ†å‰²ã€è½¦é“æ‹“æ‰‘ï¼‰ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œå†åŸºäºæ­¤è¿›è¡Œè§„åˆ’æ§åˆ¶ã€‚ç®€åŒ–äº†æ§åˆ¶é—®é¢˜ï¼Œä½†ä¾èµ–è¿™äº›æŠ½è±¡è¡¨ç¤ºçš„å‡†ç¡®æ€§å’Œæ³›åŒ–æ€§ã€‚ | æœ¬æ–‡å±äº**ç«¯åˆ°ç«¯ç‰¹å¾å­¦ä¹ **æµæ´¾ï¼Œä¸ä¾èµ–äººå·¥å®šä¹‰æˆ–å¤–éƒ¨æ¨¡å‹ç”Ÿæˆçš„ä¸­é—´æŠ½è±¡è¡¨ç¤ºã€‚ç‰¹å¾æ¥è‡ªé€šç”¨çš„åŸºç¡€æ¨¡å‹ï¼Œæ›´å…·æ™®é€‚æ€§ã€‚ | **ç‰¹å¾é€šç”¨æ€§ vs. æŠ½è±¡ç‰¹å¼‚æ€§**ï¼šSPSæ–¹æ³•ä¸ä¾èµ–äºç‰¹å®šä»»åŠ¡ï¼ˆå¦‚è½¦é“æ£€æµ‹ï¼‰çš„æŠ½è±¡æ¨¡å‹ï¼Œé¿å…äº†è¿™äº›æ¨¡å‹åœ¨OODåœºæ™¯ä¸‹å¯èƒ½å‡ºç°çš„æ•…éšœä¼ æ’­ã€‚å®ƒåˆ©ç”¨åŸºç¡€æ¨¡å‹çš„é€šç”¨è§†è§‰-è¯­è¨€çŸ¥è¯†ï¼Œå¹¶é€šè¿‡SPSå¢å¼ºå…¶é²æ£’æ€§ã€‚ |\n| **åŸºäºåˆ†å‰²æ©ç çš„ç‰¹å¾æ–¹æ³•** (å¦‚MFåŸºçº¿) | å…ˆä½¿ç”¨é€šç”¨åˆ†å‰²æ¨¡å‹ï¼ˆå¦‚SAMï¼‰åˆ†å‰²å›¾åƒï¼Œç„¶ååœ¨æ¯ä¸ªåŒºåŸŸå†…çš„è£å‰ªå›¾åƒä¸Šä½¿ç”¨é¢„è®­ç»ƒç¼–ç å™¨ï¼ˆå¦‚CLIPï¼‰æå–ä¸€ä¸ªå…¨å±€ç‰¹å¾å‘é‡ï¼Œå¹¶åˆ†é…ç»™è¯¥åŒºåŸŸæ‰€æœ‰åƒç´ ã€‚ | 1. **ç‰¹å¾ç”Ÿæˆæµç¨‹**ï¼šâ€œæ©ç ä¼˜å…ˆâ€ vs æœ¬æ–‡çš„â€œå—å¯¹é½â€ã€‚å‰è€…ä¾èµ–**å¤–éƒ¨åˆ†å‰²æ¨¡å‹**çš„è´¨é‡ï¼Œè‹¥åˆ†å‰²å¤±è´¥åˆ™ä¼šä¸¢å¤±ä¿¡æ¯ï¼›åè€…ç›´æ¥ä»åŸºç¡€æ¨¡å‹éª¨å¹²ç½‘ç»œæå–ï¼Œ**ä¿ç•™ç©ºé—´å¸ƒå±€ä¸”æ— é¢å¤–ä¾èµ–**ã€‚<br>2. **ç‰¹å¾ç²’åº¦**ï¼šå‰è€…ä¸ºæ¯ä¸ªåŒºåŸŸç”Ÿæˆä¸€ä¸ª**å•ä¸€ç‰¹å¾å‘é‡**ï¼›åè€…ä¸ºæ¯ä¸ªå›¾åƒå—ç”Ÿæˆä¸€ä¸ªæè¿°ç¬¦ï¼Œ**ç©ºé—´ç²’åº¦æ›´ç»†**ã€‚ | **æµç¨‹ç®€åŒ–ä¸é²æ£’æ€§**ï¼šSPSå»ºç«‹åœ¨â€œDrive Anywhereâ€çš„å—å¯¹é½æå–æµç¨‹ä¸Šï¼Œè¯¥æµç¨‹æœ¬èº«å°±æ›´ç®€æ´ã€æ›´ä¸æ˜“å—åˆ†å‰²é”™è¯¯å½±å“ã€‚SPSåœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥å¤„ç†ç‰¹å¾å†—ä½™ï¼Œä»è€Œåœ¨æ€§èƒ½ä¸Šå¤§å¹…è¶…è¶Šäº†MFåŸºçº¿ï¼ˆè§è¡¨1ï¼‰ã€‚ |\n\n## æ€»ç»“ï¼šæœ¬æ–‡ä¸å‰ä½œçš„æ ¸å¿ƒåŒºåˆ«ä¸æ”¹è¿›\n\n1.  **é—®é¢˜æ„è¯†ä¸åŒ**ï¼šå‰ä½œâ€œDrive Anywhereâ€**å‘ç°å¹¶éªŒè¯äº†**ä½¿ç”¨åŸºç¡€æ¨¡å‹å—ç‰¹å¾å¯¹OODæ³›åŒ–çš„å·¨å¤§ä»·å€¼ã€‚æœ¬æ–‡åˆ™**è¯Šæ–­å¹¶è§£å†³äº†**è¯¥æ–¹æ¡ˆä¸­ä¸€ä¸ªæœªè¢«é‡è§†çš„å†…åœ¨é—®é¢˜ï¼š**ç‰¹å¾å†—ä½™**ã€‚\n2.  **æŠ€æœ¯åˆ›æ–°ç‚¹ä¸åŒ**ï¼šå‰ä½œçš„å…³é”®åˆ›æ–°æ˜¯**æ©ç æ³¨æ„åŠ›æå–å—å¯¹é½ç‰¹å¾**çš„å·¥ç¨‹æ–¹æ³•ã€‚æœ¬æ–‡çš„å…³é”®åˆ›æ–°æ˜¯**éšæœºå—é€‰æ‹©ï¼ˆSPSï¼‰** è¿™ä¸€è®­ç»ƒ/æ¨ç†ç­–ç•¥ï¼Œå®ƒæ˜¯ä¸€ç§é’ˆå¯¹å·²æœ‰ç‰¹å¾çš„åå¤„ç†æ­£åˆ™åŒ–æ–¹æ³•ã€‚\n3.  **å…·ä½“æ”¹è¿›**ï¼š\n    *   **æ€§èƒ½**ï¼šåœ¨å®Œå…¨ç›¸åŒçš„åŸºå‡†ï¼ˆDrive Anywhereæ¡†æ¶ï¼‰å’ŒåŸºç¡€æ¨¡å‹ï¼ˆBLIP-2ï¼‰ä¸Šï¼ŒSPSå®ç°äº†**æ˜¾è‘—çš„OODæ€§èƒ½æå‡**ï¼ˆå¹³å‡+6.2%ï¼‰å’Œ**æ¨ç†åŠ é€Ÿ**ï¼ˆ2.4å€ï¼‰ã€‚\n    *   **é²æ£’æ€§**ï¼šé€šè¿‡è¿«ä½¿ç­–ç•¥ä¸ä¾èµ–äºä»»ä½•ç‰¹å®šçš„ç‰¹å¾å­é›†ï¼ŒSPSå¢å¼ºäº†ç­–ç•¥çš„**ä¸å˜æ€§å’Œé²æ£’æ€§**ã€‚\n    *   **å¯æ‰©å±•æ€§**ï¼šSPSè¢«è¯æ˜å¯å…¼å®¹ä¸åŒçš„åŸºç¡€æ¨¡å‹ï¼ˆå¦‚DINOï¼‰ï¼Œä¸”æ”¯æŒä¸æ–‡æœ¬é©±åŠ¨çš„æ½œåœ¨ç©ºé—´æ•°æ®å¢å¼ºç»“åˆï¼Œè·å¾—é¢å¤–æå‡ã€‚\n    *   **å®ç”¨æ€§**ï¼šæ•ˆç‡çš„æå‡å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶**æ— éœ€å¾®è°ƒå³å¯ä»ä»¿çœŸè¿ç§»åˆ°çœŸå®è½¦è¾†**ï¼Œå®ç”¨æ€§æ›´å¼ºã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡æ˜¯å¯¹å‰ä½œâ€œDrive Anywhereâ€æ¡†æ¶çš„ä¸€æ¬¡æ·±åˆ»ä¼˜åŒ–å’Œå¢å¼ºã€‚å®ƒæ²¡æœ‰æ”¹å˜åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„éšæœºåŒ–æŠ€å·§ï¼Œè§£å†³äº†è¯¥æ¡†æ¶å†…åœ¨çš„ç‰¹å¾å†—ä½™ç¼ºé™·ï¼Œä»è€Œåœ¨æ€§èƒ½ã€æ•ˆç‡å’Œé²æ£’æ€§ä¸Šå®ç°äº†å…¨é¢çš„è¶…è¶Šã€‚**",
            "timestamp": "2026-01-20T17:59:37.253344",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## è®ºæ–‡å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\næœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°æ˜¯**éšæœºè¡¥ä¸é€‰æ‹©ï¼ˆStochastic-Patch-Selection, SPSï¼‰**ï¼Œæ—¨åœ¨è§£å†³åŸºäºåŸºç¡€æ¨¡å‹æå–çš„è¡¥ä¸ç‰¹å¾å†—ä½™é—®é¢˜ï¼Œä»¥æå‡ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç­–ç•¥çš„æ³›åŒ–æ€§å’Œæ•ˆç‡ã€‚\n\n### å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è¿°\n\n| æ–¹æ³•åç§° | æ ¸å¿ƒæ€æƒ³ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…³é”®å‚æ•°ä¸å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. éšæœºè¡¥ä¸é€‰æ‹© (SPS)** | åœ¨æ¯ä¸€å¸§è¾“å…¥æ—¶ï¼Œ**éšæœºä¸¢å¼ƒ**ä¸€éƒ¨åˆ†è¡¥ä¸æè¿°ç¬¦ï¼Œä»…å°†å‰©ä½™çš„æè¿°ç¬¦ï¼ˆä¿æŒå…¶åŸå§‹ç©ºé—´å¸ƒå±€ï¼‰è¾“å…¥ç­–ç•¥ç½‘ç»œã€‚è¿™è¿«ä½¿ç­–ç•¥å­¦ä¹ åŸºäºä¸åŒã€éšæœºä½†å®Œæ•´çš„åœºæ™¯è§†å›¾è¿›è¡Œå†³ç­–ï¼Œä»è€Œå…³æ³¨**ä¸å˜ä¸”å› æœç›¸å…³**çš„çº¿ç´¢ï¼Œè€Œéè™šå‡ç›¸å…³æ€§ã€‚ | **1. å†—ä½™é‡åŒ– (ç†è®ºåŸºç¡€):** <br> - è®¾ä»åŸºç¡€æ¨¡å‹æå–çš„è¡¥ä¸ç‰¹å¾çŸ©é˜µä¸º \\( F_c \\in \\mathbb{R}^{N \\times D} \\)ï¼Œå…¶ä¸­ \\( N = H' \\times W' \\) ä¸ºè¡¥ä¸æ•°ï¼Œ\\( D \\) ä¸ºç‰¹å¾ç»´åº¦ã€‚<br> - é€šè¿‡**ä¸»æˆåˆ†åˆ†æ(PCA)**è®¡ç®—ç´¯è®¡è§£é‡Šæ–¹å·® \\( E(m) \\)ï¼š<br>   \\[ E(m) := \\frac{\\sum_{i=1}^{m} \\lambda_i}{\\sum_{i=1}^{r} \\lambda_i} \\]<br>   å…¶ä¸­ \\( \\lambda_i \\) ä¸ºåæ–¹å·®çŸ©é˜µ \\( \\Sigma = \\frac{1}{N} \\tilde{F}_c^\\top \\tilde{F}_c \\) çš„ç‰¹å¾å€¼ï¼Œ\\( r \\) ä¸ºçŸ©é˜µç§©ã€‚è®ºæ–‡å‘ç°BLIP2ç‰¹å¾ä»…éœ€ **17/64** ä¸ªä¸»æˆåˆ†å³å¯è§£é‡Š **90%** çš„æ–¹å·®ï¼Œè¯æ˜å…¶é«˜åº¦å†—ä½™ã€‚<br> - **è¡¥ä¸é—´ç›¸ä¼¼æ€§åˆ†æ**: è®¡ç®—æ‰€æœ‰è¡¥ä¸æè¿°ç¬¦é—´çš„Pearsonç›¸å…³ç³»æ•°çŸ©é˜µæˆ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œç»“æœæ˜¾ç¤ºå­˜åœ¨å¹¿æ³›çš„å¼ºç›¸å…³æ€§ï¼ˆè§å›¾3ï¼‰ã€‚<br><br>**2. SPSç®—æ³•æµç¨‹:** <br> å¯¹äºæ¯ä¸€å¸§ \\( t \\)ï¼š<br> a. **é‡‡æ ·å­é›†**: å›ºå®šé‡‡æ ·ç‡ \\( \\text{rate} \\in (0, 1] \\)ï¼Œéšæœºå‡åŒ€é€‰æ‹© \\( k = \\lceil \\text{rate} \\cdot N \\rceil \\) ä¸ªè¡¥ä¸ç´¢å¼•ï¼Œè®°ä¸º \\( \\Omega_t \\)ã€‚<br> b. **è®¡ç®—æè¿°ç¬¦**: ä»…å¯¹ \\( \\Omega_t \\) ä¸­çš„è¡¥ä¸è¿è¡Œ**æ©ç æ³¨æ„åŠ›æå–å™¨**ï¼ˆè§ä¸‹æ–‡â€œè¡¥ä¸å¯¹é½ç‰¹å¾æå–â€ï¼‰ã€‚<br> c. **æ„å»ºç¨€ç–å¼ é‡**: ç»„è£…è¾“å‡ºå¼ é‡ \\( \\tilde{F}' \\in \\mathbb{R}^{H' \\times W' \\times D} \\)ï¼š<br>    \\[ \\tilde{F}'_i = \\begin{cases} F'_i, & i \\in \\Omega_t, \\\\ \\mathbf{0}, & \\text{otherwise} \\end{cases} \\]<br>    å…³é”®ç‚¹ï¼š**ä¿ç•™åŸå§‹ç©ºé—´ç½‘æ ¼å¸ƒå±€**ï¼Œç”¨é›¶å‘é‡å¡«å……è¢«æ©ç çš„è¡¥ä¸ä½ç½®ã€‚ | **å…³é”®å‚æ•°:** <br> - **é‡‡æ ·ç‡ (rate)**: è®ºæ–‡ä¸»è¦å®éªŒä½¿ç”¨ **rate=0.5**ï¼ˆå³ä¿ç•™50%è¡¥ä¸ï¼‰ã€‚å¦æµ‹è¯•äº†70%å’Œ30%ã€‚<br> - **è¡¥ä¸æ•° \\( N \\)**: å–å†³äºåŸºç¡€æ¨¡å‹çš„ViTé…ç½®ï¼Œä¾‹å¦‚BLIP2å¯èƒ½äº§ç”Ÿ \\( 16 \\times 16 = 256 \\) ä¸ªè¡¥ä¸ã€‚<br> - **æ©ç æ³¨æ„åŠ›å‚æ•° \\( r \\)**: åœ¨ç‰¹å¾æå–é˜¶æ®µä½¿ç”¨ï¼Œæ§åˆ¶æ©ç å¼ºåº¦ï¼ˆæ¥è‡ªDrive-Anywhereæ–¹æ³•ï¼‰ï¼Œé€šå¸¸ \\( r < 0 \\)ï¼ˆå¦‚-100ï¼‰ã€‚<br><br>**å®ç°:** ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼ˆå¦‚BLIP2ï¼‰å’Œè½»é‡çº§ç­–ç•¥ç½‘ç»œä¹‹é—´çš„**å³æ’å³ç”¨**å±‚ã€‚æ— éœ€å¾®è°ƒåŸºç¡€æ¨¡å‹ã€‚ |\n| **2. è¡¥ä¸å¯¹é½ç‰¹å¾æå– (åŸºç¡€)** | é‡‡ç”¨ **Drive-Anywhere [3]** çš„æ–¹æ³•ï¼Œä»å†»ç»“çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚BLIP2ï¼‰ä¸­æå–æ¯ä¸ªå›¾åƒè¡¥ä¸çš„ç‹¬ç«‹æè¿°ç¬¦ã€‚æ ¸å¿ƒæ˜¯ä½¿ç”¨**æ©ç æ³¨æ„åŠ›**å¼ºåˆ¶æ¨¡å‹å…³æ³¨å•ä¸ªè¡¥ä¸ã€‚ | **æ•°å­¦åŸç†:** <br> 1. è®¾åŸºç¡€æ¨¡å‹ \\( \\text{Desc} \\) æœ‰ \\( L \\) å±‚ã€‚å¯¹äºè¾“å…¥å›¾åƒ \\( F \\)ï¼Œåœ¨ç¬¬ \\( \\ell \\) å±‚è®¡ç®—æŸ¥è¯¢ \\( Q \\)ã€é”® \\( K \\)ã€å€¼ \\( V \\) çŸ©é˜µã€‚<br> 2. ä¸ºæå–ç¬¬ \\( j \\) ä¸ªè¡¥ä¸çš„ç‰¹å¾ï¼Œå®šä¹‰ä¸€ä¸ª**äºŒè¿›åˆ¶æ©ç å‘é‡** \\( m^{(j)} \\in \\{0,1\\}^N \\)ï¼Œå…¶ä¸­ \\( m^{(j)}_i = 1 \\) è¡¨ç¤ºä¿ç•™æ¥è‡ªè¡¥ä¸ \\( i \\) çš„ä¿¡æ¯ã€‚<br> 3. ä¿®æ”¹æ³¨æ„åŠ›å¯¹æ•° \\( G^\\ell = Q^\\ell (K^\\ell)^\\top \\)ï¼ŒåŠ å…¥æ©ç ï¼š<br>    \\[ \\tilde{G^\\ell} = G^\\ell + (\\mathbf{1} - M^{(j)}) r \\]<br>    å…¶ä¸­ \\( M^{(j)} = [m^{(j)}, \\dots, m^{(j)}]^\\top \\)ï¼Œ\\( r \\) æ˜¯ä¸€ä¸ªå¤§çš„è´Ÿå€¼ï¼ˆå¦‚-100ï¼‰ã€‚è¿™å°†è¢«æ©ç ä½ç½®ï¼ˆ\\( m_i \\approx 0 \\)ï¼‰çš„æ³¨æ„åŠ›åˆ†æ•°æ¨å‘è´Ÿæ— ç©·ï¼Œä»è€Œåœ¨åç»­softmaxä¸­è¢«æŠ‘åˆ¶ã€‚<br> 4. è®¡ç®—æ©ç æ³¨æ„åŠ›è¾“å‡ºå¹¶ç»§ç»­å‰å‘ä¼ æ’­ï¼š<br>    \\[ F^{\\prime(j)} = \\text{Desc}^{\\ell \\rightarrow} \\bigl( \\text{SoftMax}(\\tilde{G}) V^\\ell \\bigr) \\]<br> 5. å¯¹æ‰€æœ‰ \\( j \\in [N] \\) é‡å¤æ­¤è¿‡ç¨‹ï¼Œå¾—åˆ°å®Œæ•´çš„è¡¥ä¸ç‰¹å¾å¼ é‡ \\( F' \\)ã€‚ | **å…³é”®å‚æ•°:** <br> - **æ©ç è®¾è®¡ \\( m^{(j)} \\)**: å®šä¹‰å›´ç»•ç›®æ ‡è¡¥ä¸ \\( j \\) çš„å±€éƒ¨åŒºåŸŸã€‚å¯ä»¥æ˜¯ç¡¬æˆªæ–­ \\( \\mathbf{1}[d_{ij} \\le \\alpha] \\)ï¼Œæˆ–è½¯è¡°å‡å¦‚ \\( 2^{-d_{ij}} \\)ï¼Œå…¶ä¸­ \\( d_{ij} \\) æ˜¯è¡¥ä¸ç½‘æ ¼ä¸­çš„æ¬§æ°è·ç¦»ã€‚<br> - **æ©ç å±‚ \\( \\ell \\)**: é€šå¸¸é€‰æ‹©åœ¨BLIP2çš„Q-Formerè¾“å…¥å±‚è¿›è¡Œæ©ç ã€‚<br> - **æŠ‘åˆ¶å¼ºåº¦ \\( r \\)**: è´Ÿå€¼ï¼Œç»å¯¹å€¼è¶Šå¤§æ©ç æ•ˆæœè¶Šå¼ºã€‚ |\n| **3. SPSå˜ä½“ (æ¶ˆèç ”ç©¶)** | æ¢ç´¢ä¸åŒçš„æ©ç å’Œé‡ç»„ç­–ç•¥ï¼Œä»¥ç†è§£è®¾è®¡ç©ºé—´å¹¶ä¼˜åŒ–æ€§èƒ½ã€‚ | **V1: é˜ˆå€¼æ©ç  (Threshold Masking)** <br> - æ¯ä¸ªè¡¥ä¸ \\( i \\) ä»¥æ¦‚ç‡ \\( \\text{rate} \\) è¢«ç‹¬ç«‹é€‰ä¸­ï¼š\\( \\Omega_t = \\{ i \\mid R_i \\le \\text{rate} \\} \\)ï¼Œå…¶ä¸­ \\( R_i \\sim \\mathcal{U}(0,1) \\)ã€‚<br> - ä¿ç•™çš„è¡¥ä¸æ•°é‡æ˜¯æ³¢åŠ¨çš„ï¼Œå¢åŠ äº†éšæœºå¤šæ ·æ€§ã€‚<br><br>**V2: ä½ç½®è°ƒæ•´çš„ç¨€ç–åºåˆ— (Position-Adjusted Sparse Sequence)** <br> - åœ¨é€‰ä¸­ \\( \\Omega_t \\) åï¼Œ**å®Œå…¨ç§»é™¤**è¢«ä¸¢å¼ƒçš„ä»¤ç‰Œï¼Œä¸ä¿ç•™é›¶å¡«å……ã€‚<br> - å°†é€‰ä¸­çš„æè¿°ç¬¦ \\( F'_i \\) ä¸å…¶åœ¨ViTåŸå§‹**ä½ç½®ç¼–ç è¡¨**ä¸­å¯¹åº”çš„ä½ç½®åµŒå…¥ \\( p_i \\) æ‹¼æ¥ï¼Œå½¢æˆä¸€ä¸ª**å¯å˜é•¿åº¦**çš„è¾“å…¥åºåˆ—ä¼ é€’ç»™ç­–ç•¥ç½‘ç»œã€‚<br> - è¿™è¿›ä¸€æ­¥å‡å°‘äº†è®¡ç®—é‡ã€‚ | **å˜ä½“å‘½å:** <br> - **SPS**: åŸºç¡€ç‰ˆæœ¬ï¼Œé›¶å¡«å……ã€‚<br> - **SPPS (Structured Patch Pruning with Selection)**: å¯¹åº”V2ï¼Œç§»é™¤ä»¤ç‰Œå¹¶é‡ç»„ã€‚<br> - **MSPPS (Matrix-based SPPS)**: ç»“åˆV1å’ŒV2ï¼ŒåŸºäºæ¦‚ç‡æ©ç è¿›è¡Œéšæœºå‰ªæå¹¶é‡ç»„ã€‚ |\n| **4. ç†è®ºä¿è¯: å­ç©ºé—´ä¿æŒå¼•ç†** | ä»ç†è®ºä¸Šè¯æ˜ï¼Œåœ¨ç‰¹å¾å†—ä½™ï¼ˆä½ç§©ï¼‰ä¸”è¡Œç›¸å¹²æ€§æœ‰ç•Œçš„æ¡ä»¶ä¸‹ï¼Œå‡åŒ€éšæœºé‡‡æ ·è¶³å¤Ÿå¤šçš„è¡¥ä¸è¡Œï¼Œèƒ½ä»¥é«˜æ¦‚ç‡ä¿ç•™åŸå§‹ç‰¹å¾çŸ©é˜µçš„**ä¸»è¡Œå­ç©ºé—´**ï¼Œä»è€Œä¿æŒåœºæ™¯è¯­ä¹‰ã€‚ | **å¼•ç†1 (ç®€åŒ–):** <br> è®¾ä¸­å¿ƒåŒ–ç‰¹å¾çŸ©é˜µ \\( F_c \\) çš„ç§©ä¸º \\( r \\)ï¼Œå…¶è¡Œç›¸å¹²æ€§ä¸º \\( \\mu \\)ã€‚éšæœºå‡åŒ€é‡‡æ · \\( m \\) è¡Œå¾—åˆ°å­çŸ©é˜µ \\( F_{\\mathcal{I}} \\)ã€‚å­˜åœ¨å¸¸æ•° \\( C > 0 \\)ï¼Œä½¿å¾—å¯¹äºä»»æ„ \\( \\varepsilon, \\delta \\in (0,1) \\)ï¼Œå¦‚æœ<br> \\[ m \\ge C \\frac{\\mu r}{\\varepsilon^2} \\log(\\frac{r}{\\delta}) \\]<br> åˆ™ä»¥è‡³å°‘ \\( 1-\\delta \\) çš„æ¦‚ç‡ï¼Œé‡‡æ ·å‰åè¡Œç©ºé—´æŠ•å½±ç®—å­çš„è°±èŒƒæ•°è¯¯å·®æ»¡è¶³ï¼š<br> \\[ \\| \\Pi_F - \\Pi_{F_{\\mathcal{I}}} \\|_2 \\le \\varepsilon \\]<br> è¿™æ„å‘³ç€é‡‡æ ·åçš„ç‰¹å¾ä»ç„¶å¼ æˆä¸åŸå§‹æ•°æ®è¿‘ä¼¼ç›¸åŒçš„è¯­ä¹‰å­ç©ºé—´ã€‚ | **å®é™…æ„ä¹‰:** <br> è¯¥å¼•ç†ä¸ºSPSçš„æœ‰æ•ˆæ€§æä¾›äº†æ•°å­¦åŸºç¡€ã€‚ç”±äºè¡¥ä¸ç‰¹å¾æœ¬è´¨ä¸Šæ˜¯ä½ç§©çš„ï¼ˆPCAè¯æ˜ï¼‰ï¼Œå› æ­¤å³ä½¿éšæœºä¸¢å¼ƒå¤§éƒ¨åˆ†è¡¥ä¸ï¼Œå‰©ä½™è¡¥ä¸æ‰€æºå¸¦çš„ä¿¡æ¯",
            "timestamp": "2026-01-20T18:00:41.321142",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡é‡‡ç”¨**é—­ç¯é©¾é©¶æˆåŠŸç‡**ä½œä¸ºæ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼Œå…·ä½“å®šä¹‰å’Œç»†èŠ‚å¦‚ä¸‹ï¼š\n\n| æŒ‡æ ‡åç§° | è®¡ç®—æ–¹å¼ | ç»†èŠ‚è¯´æ˜ |\n| :--- | :--- | :--- |\n| **å½’ä¸€åŒ–æˆåŠŸæŒç»­æ—¶é—´** | `æˆåŠŸé©¾é©¶æ—¶é—´ / æ€»æµ‹è¯•æ—¶é—´`ï¼Œåœ¨å¤šä¸ªæµ‹è¯•å›åˆä¸­å–å¹³å‡ã€‚ | 1. **å¤±è´¥æ¡ä»¶**ï¼šæ»¡è¶³ä»¥ä¸‹ä»»ä¸€å³åˆ¤å®šä¸ºå¤±è´¥ï¼š<br>   - **è½¦é“åç¦»**ï¼šè½¦è¾†è·¨è¶Šè½¦é“è¾¹ç•Œã€‚<br>   - **ç¢°æ’/å±é™©æ¥è¿‘**ï¼šä¸éšœç¢ç‰©å‘ç”Ÿç¢°æ’æˆ–è·ç¦»è¿‡è¿‘ã€‚<br>   - **èˆªå‘åå·®**ï¼šè½¦è¾†èˆªå‘ä¸è½¦é“æ–¹å‘åå·®è¶…è¿‡30Â°ã€‚<br>2. **æµ‹è¯•åè®®**ï¼šåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼Œè¿›è¡Œ**100ä¸ªå›åˆ**çš„æµ‹è¯•ï¼Œæ¯ä¸ªå›åˆçº¦20ç§’ï¼Œè®¡ç®—å¹³å‡æˆåŠŸç‡ã€‚ |\n| **å®‰å…¨å‘˜å¹²é¢„æ¬¡æ•°** | åœ¨å®è½¦æµ‹è¯•ä¸­ï¼Œç»Ÿè®¡å› è§¦å‘ä¸Šè¿°å¤±è´¥æ¡ä»¶è€Œéœ€è¦äººç±»å®‰å…¨å‘˜æ¥ç®¡çš„æ¬¡æ•°ã€‚ | ç”¨äºè¯„ä¼°æ¨¡å‹ä»ä»¿çœŸåˆ°ç°å®ä¸–ç•Œï¼ˆSim-to-Realï¼‰çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚å¹²é¢„è¶Šå°‘ï¼Œæ€§èƒ½è¶Šå¥½ã€‚ |\n\n**æŒ‡æ ‡ç‰¹ç‚¹**ï¼š\n- **ç»¼åˆæ€§**ï¼šè¯¥æŒ‡æ ‡èåˆäº†è·¯å¾„è·Ÿè¸ªï¼ˆè½¦é“ä¿æŒï¼‰ã€å®‰å…¨æ€§ï¼ˆé¿éšœï¼‰å’Œç¨³å®šæ€§ï¼ˆèˆªå‘æ§åˆ¶ï¼‰ç­‰å¤šä¸ªè‡ªåŠ¨é©¾é©¶æ ¸å¿ƒä»»åŠ¡ã€‚\n- **å®ç”¨æ€§**ï¼šç›´æ¥åæ˜ äº†ç­–ç•¥åœ¨**é—­ç¯ã€äº¤äº’å¼ç¯å¢ƒ**ä¸­çš„é•¿æœŸå¯é æ€§å’Œå®‰å…¨æ€§ï¼Œæ¯”å¼€ç¯æŒ‡æ ‡ï¼ˆå¦‚è½¨è¿¹è¯¯å·®ï¼‰æ›´å…·å®é™…æ„ä¹‰ã€‚\n- **å¯å¯¹æ¯”æ€§**ï¼šä¸åŸºçº¿æ–¹æ³•ï¼ˆDrive Anywhereç­‰ï¼‰ä½¿ç”¨ç›¸åŒçš„è¯„ä»·åè®®ï¼Œç¡®ä¿äº†æ¯”è¾ƒçš„å…¬å¹³æ€§ã€‚\n\n### äºŒã€ è®­ç»ƒä¸æµ‹è¯•æ•°æ®é›†\n\nè®ºæ–‡ä½¿ç”¨äº†**æ··åˆæ•°æ®æº**è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨**ç²¾å¿ƒè®¾è®¡çš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯**ä¸­è¿›è¡Œè¯„ä¼°ã€‚\n\n#### 1. è®­ç»ƒæ•°æ®\n| æ•°æ®æ¥æº | æè¿° | ç”¨é€”ä¸ç»†èŠ‚ |\n| :--- | :--- | :--- |\n| **VISTA 2.0 æ¨¡æ‹Ÿå™¨** | ä¸€ä¸ªåŸºäºçœŸå®é©¾é©¶æ—¥å¿—æ„å»ºçš„å…‰å­¦é€¼çœŸã€æ•°æ®é©±åŠ¨çš„æ¨¡æ‹Ÿå™¨ã€‚ | - **åŸºç¡€æ•°æ®**ï¼šçº¦**2å°æ—¶**çš„çœŸå®ä¸–ç•Œé©¾é©¶è®°å½•ï¼Œæ¶µç›–äº†å¤šç§ç¯å¢ƒã€å…‰ç…§å’Œå¤©æ°”ã€‚<br>- **æ•°æ®å¢å¼º**ï¼šåˆ©ç”¨æ¨¡æ‹Ÿå™¨çš„èƒ½åŠ›ï¼Œä»åŸå§‹æ—¥å¿—ç”Ÿæˆ**å¤šæ ·åŒ–çš„é©¾é©¶ä½“éªŒ**ï¼Œç”¨äºå¢åŠ è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒå¹¿åº¦ã€‚<br>- **ç›‘ç£ä¿¡å·**ï¼šé‡‡ç”¨**å¼•å¯¼ç­–ç•¥å­¦ä¹ ï¼ˆGuided Policy Learningï¼‰**ï¼Œåˆ©ç”¨æ¨¡æ‹Ÿå™¨æä¾›çš„ç‰¹æƒä¿¡æ¯ï¼ˆå¦‚ç²¾ç¡®çš„è½¦é“çº¿ã€éšœç¢ç‰©ä½ç½®ï¼‰é€šè¿‡PIDæ§åˆ¶å™¨å’ŒåŸºäºæ§åˆ¶å±éšœå‡½æ•°ï¼ˆCBFï¼‰çš„å®‰å…¨æ§åˆ¶å™¨æ¥ç”Ÿæˆæ§åˆ¶æ ‡ç­¾ï¼ˆè½¬å‘å’Œæ²¹é—¨ï¼‰ã€‚ |\n| **çœŸå®é©¾é©¶æ—¥å¿—** | å®é™…é‡‡é›†çš„è½¦è¾†ä¼ æ„Ÿå™¨æ•°æ®ã€‚ | ä½œä¸ºVISTAæ¨¡æ‹Ÿå™¨æ„å»ºçš„åŸºç¡€ï¼Œæä¾›äº†çœŸå®çš„é©¾é©¶è¡Œä¸ºå’Œç¯å¢ƒåˆ†å¸ƒã€‚ |\n\n#### 2. æµ‹è¯•åœºæ™¯ï¼ˆOODè¯„ä¼°åŸºå‡†ï¼‰\næµ‹è¯•æ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨**æœªè§è¿‡çš„ã€å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶**ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è®­ç»ƒç¯å¢ƒè®¾å®šä¸ºå›ºå®šçš„â€œ**ä¹¡æ‘é“è·¯ã€å¤å­£ã€å¹²ç‡¥ã€ç™½å¤©ã€æœ‰è½¦è¾†**â€ã€‚æµ‹è¯•åˆ™ç³»ç»Ÿæ€§åœ°æ”¹å˜ä»¥ä¸‹ä¸€ä¸ªæˆ–å¤šä¸ªç»´åº¦ï¼š\n\n| åœºæ™¯ç»´åº¦ | æµ‹è¯•å–å€¼ï¼ˆOODç¤ºä¾‹ï¼‰ | è¯´æ˜ |\n| :--- | :--- | :--- |\n| **åœºæ™¯ç±»å‹** | ä¹¡æ‘ã€**åŸå¸‚** | ä»è®­ç»ƒç¯å¢ƒï¼ˆä¹¡æ‘ï¼‰åˆ‡æ¢åˆ°ç»“æ„æ›´å¤æ‚ã€åŠ¨æ€ç‰©ä½“æ›´å¤šçš„åŸå¸‚ç¯å¢ƒã€‚ |\n| **å­£èŠ‚** | å¤å­£ã€**æ˜¥å­£ã€ç§‹å­£ã€å†¬å­£** | æµ‹è¯•æ¤è¢«ã€å…‰ç…§å’Œæ•´ä½“åœºæ™¯å¤–è§‚çš„å·¨å¤§å˜åŒ–ã€‚ |\n| **å¤©æ°”** | å¹²ç‡¥ã€**ä¸‹é›ªã€ä¸‹é›¨** | æµ‹è¯•é™é›ªã€é™é›¨ç­‰æç«¯å¤©æ°”å¯¹èƒ½è§åº¦å’Œè·¯é¢åå…‰çš„å½±å“ã€‚ |\n| **æ—¶é—´** | ç™½å¤©ã€**å¤œæ™š** | æµ‹è¯•ä½å…‰ç…§æ¡ä»¶ä¸‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚ |\n| **åŠ¨æ€å‚ä¸è€…** | è½¦è¾†ã€**åŠ¨ç‰©** | æµ‹è¯•å¯¹è®­ç»ƒä¸­æœªå‡ºç°è¿‡çš„éšœç¢ç‰©ç±»å‹ï¼ˆå¦‚é¹¿ï¼‰çš„è¯†åˆ«ä¸é¿è®©èƒ½åŠ›ã€‚ |\n| **è½¦è¾†ç±»å‹** | è®­ç»ƒé›†è½¦è¾†ã€**æœªè§è¿‡çš„è½¦è¾†ç±»å‹â€ ** | æµ‹è¯•å¯¹é™Œç”Ÿè½¦è¾†å¤–è§‚çš„æ³›åŒ–èƒ½åŠ›ã€‚ |\n\n**è¯„ä»·æµç¨‹ç»†èŠ‚**ï¼š\n1.  **å›ºå®šè®­ç»ƒåˆ†å¸ƒ**ï¼šæ‰€æœ‰æ–¹æ³•éƒ½åœ¨å®Œå…¨ç›¸åŒçš„â€œä¹¡æ‘-å¤å­£-å¹²ç‡¥-ç™½å¤©-è½¦è¾†â€æ¡ä»¶ä¸‹è®­ç»ƒã€‚\n2.  **ç³»ç»ŸåŒ–OODæµ‹è¯•**ï¼šå¦‚è¡¨1æ‰€ç¤ºï¼Œå…±è®¾è®¡äº†**13ä¸ª**ä¸åŒçš„OODæµ‹è¯•æ¡ä»¶ï¼Œè¦†ç›–äº†ä¸Šè¿°æ‰€æœ‰ç»´åº¦çš„ç»„åˆï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§ã€‚\n3.  **åŸºå‡†å¯¹æ¯”**ï¼šå°†æå‡ºçš„SPSæ–¹æ³•ä¸å¤šä¸ªå¼ºåŸºçº¿ï¼ˆNo-FM, I-ViT, MF, Drive Anywhereï¼‰åœ¨æ‰€æœ‰è¿™äº›OODåœºæ™¯ä¸‹è¿›è¡Œå¯¹æ¯”ï¼Œè®¡ç®—**å¹³å‡æˆåŠŸç‡**ä½œä¸ºæœ€ç»ˆæ€§èƒ½æ€»ç»“ã€‚\n\n### æ€»ç»“\nè¯¥è®ºæ–‡çš„è¯„ä»·ä½“ç³»è®¾è®¡ç²¾è‰¯ï¼š\n- **æŒ‡æ ‡å±‚é¢**ï¼Œé‡‡ç”¨è´´è¿‘å®é™…åº”ç”¨çš„é—­ç¯æˆåŠŸç‡ï¼Œå®šä¹‰ä¸¥æ ¼ä¸”åˆç†ã€‚\n- **æ•°æ®é›†å±‚é¢**ï¼Œåˆ©ç”¨æ¨¡æ‹Ÿå™¨é«˜æ•ˆç”Ÿæˆå¤šæ ·åŒ–æ•°æ®ï¼Œå¹¶è®¾è®¡äº†ä¸€å¥—ç³»ç»Ÿæ€§çš„ã€è¦†ç›–å¤šç»´åº¦åˆ†å¸ƒåç§»çš„OODæµ‹è¯•åŸºå‡†ï¼Œæœ‰åŠ›åœ°éªŒè¯äº†æ–¹æ³•åœ¨**æ³›åŒ–æ€§ã€é²æ£’æ€§**æ–¹é¢çš„æå‡ã€‚å…¶å®è½¦é›¶æ ·æœ¬éƒ¨ç½²å®éªŒè¿›ä¸€æ­¥è¯æ˜äº†è¯¥è¯„ä¼°ä½“ç³»çš„æœ‰æ•ˆæ€§å’Œæ–¹æ³•çš„å®ç”¨ä»·å€¼ã€‚",
            "timestamp": "2026-01-20T18:01:18.158111",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒä¸æ€§èƒ½åˆ†æ\n\n### æ•°æ®é›†ä¸å®éªŒè®¾ç½®\nè®ºæ–‡åœ¨**ä»¿çœŸç¯å¢ƒ**å’Œ**çœŸå®ä¸–ç•Œ**ä¸¤ä¸ªå±‚é¢è¿›è¡Œäº†å®éªŒï¼š\n1.  **ä»¿çœŸç¯å¢ƒ**ï¼šä¸»è¦ä½¿ç”¨ **VISTA 2.0** æ¨¡æ‹Ÿå™¨ã€‚è®­ç»ƒæ•°æ®ç»“åˆäº†çº¦2å°æ—¶çš„**çœŸå®ä¸–ç•Œé©¾é©¶æ—¥å¿—**ï¼Œå¹¶é€šè¿‡VISTAåœ¨å¤šç§ç¯å¢ƒã€å…‰ç…§å’Œå¤©æ°”æ¡ä»¶ä¸‹ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨¡æ‹Ÿç»éªŒã€‚\n2.  **çœŸå®ä¸–ç•Œéƒ¨ç½²**ï¼šåœ¨ä¸€è¾†é…å¤‡ä¼ æ„Ÿå™¨å’Œè®¡ç®—å¹³å°çš„ **2019æ¬¾é›·å…‹è¨æ–¯RX 450H** å®è½¦ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œåœ°ç‚¹åŒ…æ‹¬ä¹¡æ‘å…¬å›­é“è·¯å’Œåœ°ä¸‹åœè½¦åœºã€‚\n\n### ä¸»è¦è¯„ä¼°æŒ‡æ ‡\n1.  **æ ¸å¿ƒæŒ‡æ ‡**ï¼š**å½’ä¸€åŒ–æˆåŠŸæŒç»­æ—¶é—´**ã€‚åœ¨ä»¿çœŸä¸­ï¼Œè¡¡é‡è½¦è¾†åœ¨è§¦å‘å¤±è´¥æ¡ä»¶ï¼ˆå¦‚å‹çº¿ã€ç¢°æ’ã€èˆªå‘åå·®>30Â°ï¼‰å‰èƒ½å®‰å…¨è¡Œé©¶çš„æ—¶é—´ï¼Œåœ¨100ä¸ªçº¦20ç§’çš„ç‰‡æ®µä¸Šå–å¹³å‡ã€‚\n2.  **å®è½¦æŒ‡æ ‡**ï¼š**å®‰å…¨é©¾é©¶å‘˜å¹²é¢„æ¬¡æ•°**ï¼Œä¾æ®ä¸ä»¿çœŸç›¸åŒçš„å¤±è´¥æ¡ä»¶è¿›è¡Œç»Ÿè®¡ã€‚\n3.  **æ•ˆç‡æŒ‡æ ‡**ï¼šæ¨¡å‹**æ¨ç†é€Ÿåº¦çš„åŠ é€Ÿæ¯”**ï¼ˆä¸åŸºçº¿ç›¸æ¯”ï¼‰ã€‚\n4.  **åˆ†ææŒ‡æ ‡**ï¼šä½¿ç”¨**ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰è§£é‡Šæ–¹å·®**å’Œ**å—é—´ç›¸ä¼¼æ€§/ç›¸å…³æ€§**æ¥é‡åŒ–ç‰¹å¾å†—ä½™ã€‚\n\n### æ€§èƒ½æå‡ä¸å…³é”®ç»“æœ\nè®ºæ–‡æå‡ºçš„ **SPSï¼ˆStochastic Patch Selectionï¼‰æ–¹æ³•** åœ¨å¤šä¸ªç»´åº¦ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„SOTAæ–¹æ³•ï¼ˆDrive Anywhere, DAï¼‰ï¼š\n\n| æ¯”è¾ƒç»´åº¦ | æ€§èƒ½æå‡ | å…³é”®ç»†èŠ‚ |\n| :--- | :--- | :--- |\n| **OODæ³›åŒ–æ€§èƒ½** | **å¹³å‡æå‡ +6.2%** | åœ¨æ¶µç›–å­£èŠ‚ã€å¤©æ°”ã€å…‰ç…§ã€åœºæ™¯ï¼ˆåŸä¹¡ï¼‰ã€åŠ¨æ€å‚ä¸è€…ï¼ˆè½¦è¾†/åŠ¨ç‰©ï¼‰çš„å…¨éƒ¨OODåœºæ™¯ä¸­ã€‚ |\n| | **æœ€é«˜æå‡ +20.4%** | åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„OODåœºæ™¯ï¼ˆå¦‚åŸå¸‚å¤œé—´ï¼‰ä¸­å–å¾—ã€‚ |\n| **æ¨ç†æ•ˆç‡** | **åŠ é€Ÿ 2.4å€** | æœ€ä½³æ€§èƒ½æ¨¡å‹ **SPS-50%** ç›¸æ¯”åŸºçº¿DAçš„åŠ é€Ÿæ¯”ã€‚ |\n| | **æœ€é«˜åŠ é€Ÿ 3.47å€** | æ›´æ¿€è¿›çš„ **SPS-30%** å˜ä½“å®ç°çš„åŠ é€Ÿæ¯”ã€‚ |\n| **æ–¹æ³•é²æ£’æ€§** | **8/9 çš„å˜ä½“è¶…è¶ŠSOTA** | å¯¹æ©ç ç‡ã€é‡ç»„ç­–ç•¥è¿›è¡Œæ¶ˆèï¼Œè®­ç»ƒäº†9ä¸ªç³»ç»Ÿï¼Œå…¶ä¸­8ä¸ªéƒ½è¶…è¿‡äº†ä¹‹å‰çš„SOTAã€‚ |\n| **ä»¿çœŸåˆ°ç°å®è¿ç§»** | **é›¶è°ƒä¼˜ç›´æ¥éƒ¨ç½²** | åœ¨ä»¿çœŸè®­ç»ƒçš„ **SPS-50%** ç­–ç•¥å¯ç›´æ¥éƒ¨ç½²åˆ°å®è½¦ï¼Œåœ¨ä¹¡æ‘é“è·¯å’Œå¤æ‚åœ°ä¸‹åœè½¦åœºå‡è¡¨ç°æˆåŠŸã€‚ |\n| **è¾…åŠ©å¢å¼º** | **é¢å¤– +1.7% æå‡** | åœ¨SPSåŸºç¡€ä¸Šï¼Œä½¿ç”¨**æ½œåœ¨ç©ºé—´æ–‡æœ¬å¢å¼ºå¾®è°ƒ**ï¼ˆç”¨LLMå»ºè®®çš„æ¦‚å¿µæ›¿æ¢æè¿°ç¬¦ï¼‰å¸¦æ¥çš„è¿›ä¸€æ­¥æ€§èƒ½å¢ç›Šã€‚ |\n\n### æ ¸å¿ƒç»“è®º\nè®ºæ–‡é€šè¿‡ç³»ç»Ÿçš„å®éªŒè¡¨æ˜ï¼š\n- **æŠ€æœ¯åˆ›æ–°**ï¼š**SPS** é€šè¿‡éšæœºä¸¢å¼ƒå†—ä½™çš„å—æè¿°ç¬¦ï¼Œè¿«ä½¿ç­–ç•¥å­¦ä¹ å¯¹å…·ä½“å¹¸å­˜ä»¤ç‰Œä¸å˜çš„ã€æ›´å…·å› æœæ€§çš„ç‰¹å¾ï¼Œä»è€ŒåŒæ—¶æå‡äº†**æ³›åŒ–æ€§**å’Œ**æ•ˆç‡**ã€‚\n- **å®é™…ä»·å€¼**ï¼šè¯¥æ–¹æ³•ä¸ä»…åœ¨å„ç§åˆ†å¸ƒå¤–æŒ‘æˆ˜ä¸­æ˜¾è‘—æé«˜äº†é©¾é©¶æˆåŠŸç‡ï¼Œè¿˜å¤§å¹…é™ä½äº†è®¡ç®—å¼€é”€ï¼Œå¹¶å®ç°äº†**é›¶è°ƒä¼˜çš„ä»¿çœŸåˆ°ç°å®è¿ç§»**ï¼Œä¸ºé«˜æ•ˆã€é²æ£’çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
            "timestamp": "2026-01-20T18:01:42.930614",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ã€ŠSee Less, Drive Betterã€‹çš„ä¸»è¦å±€é™æ€§ä¸æœªæ¥æ”¹è¿›æ–¹å‘åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„æ·±å…¥åˆ†æï¼Œå…¶æå‡ºçš„**éšæœºå—é€‰æ‹©ï¼ˆSPSï¼‰**æ–¹æ³•è™½ç„¶åœ¨æå‡ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶çš„æ³›åŒ–æ€§ã€é²æ£’æ€§å’Œæ•ˆç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ä»å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œå¹¶æŒ‡æ˜äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚\n\n### ä¸»è¦å±€é™æ€§\n\n1.  **éšæœºé€‰æ‹©ç­–ç•¥çš„ç®€å•æ€§**\n    - **å½“å‰æ–¹æ³•**ï¼šSPSé‡‡ç”¨**å‡åŒ€éšæœº**çš„æ–¹å¼ä¸¢å¼ƒå—æè¿°ç¬¦ã€‚è™½ç„¶æœ‰æ•ˆï¼Œä½†è¿™æ˜¯ä¸€ç§**æ— ä¿¡æ¯ã€æ— çŠ¶æ€**çš„éšæœºè¿‡ç¨‹ï¼Œå¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„ã€‚\n    - **æ½œåœ¨é—®é¢˜**ï¼šåœ¨å¤æ‚æˆ–å±é™©åœºæ™¯ä¸­ï¼ˆå¦‚çªç„¶å‡ºç°çš„è¡Œäººã€æ¶åŠ£å¤©æ°”ï¼‰ï¼Œéšæœºä¸¢å¼ƒå…³é”®ä¿¡æ¯å—å¯èƒ½ä¼šæš‚æ—¶é™ä½æ„ŸçŸ¥èƒ½åŠ›ï¼Œå°½ç®¡è®­ç»ƒé¼“åŠ±äº†é²æ£’æ€§ï¼Œä½†åœ¨æç«¯è¾¹ç¼˜æƒ…å†µä¸‹å¯èƒ½å­˜åœ¨é£é™©ã€‚\n\n2.  **å¯¹åŸºç¡€æ¨¡å‹ç‰¹å¾çš„ä¾èµ–**\n    - **æ–¹æ³•å‰æ**ï¼šSPSçš„æœ‰æ•ˆæ€§å»ºç«‹åœ¨åŸºç¡€æ¨¡å‹ï¼ˆå¦‚BLIP-2ï¼‰æå–çš„å—ç‰¹å¾æœ¬èº«å…·æœ‰é«˜åº¦**å†—ä½™æ€§**çš„å‰æä¸‹ã€‚å¦‚æœæœªæ¥åŸºç¡€æ¨¡å‹çš„æ¶æ„æˆ–è®­ç»ƒç›®æ ‡å‘ç”Ÿæ”¹å˜ï¼Œå‡å°‘äº†è¿™ç§å†…åœ¨å†—ä½™ï¼ŒSPSçš„å¢ç›Šå¯èƒ½ä¼šå‡å¼±ã€‚\n    - **ç‰¹å¾æå–å¼€é”€**ï¼šå°½ç®¡SPSå‡å°‘äº†è¾“å…¥ç­–ç•¥ç½‘ç»œçš„ä»¤ç‰Œæ•°é‡ï¼ŒåŠ é€Ÿäº†åç»­è®¡ç®—ï¼Œä½†**ç‰¹å¾æå–è¿‡ç¨‹**ï¼ˆè¿è¡Œå¤§å‹åŸºç¡€æ¨¡å‹ï¼‰ä»ç„¶æ˜¯è®¡ç®—ç“¶é¢ˆã€‚SPSä»…éƒ¨åˆ†ç¼“è§£äº†æ­¤é—®é¢˜ã€‚\n\n3.  **è¶…å‚æ•°ï¼ˆä¸¢å¼ƒç‡ï¼‰çš„æ•æ„Ÿæ€§**\n    - **å®éªŒå‘ç°**ï¼šè®ºæ–‡çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œä¸åŒå˜ä½“ï¼ˆSPS, SPPS, MSPPSï¼‰çš„**æœ€ä¼˜ä¸¢å¼ƒç‡ï¼ˆrateï¼‰** ä¸åŒï¼ˆå¦‚SPSåœ¨50%æœ€ä½³ï¼ŒSPPSåœ¨70%æœ€ä½³ï¼‰ã€‚è¿™è¡¨æ˜æ€§èƒ½å¯¹ä¸¢å¼ƒç‡çš„é€‰æ‹©æœ‰ä¸€å®šæ•æ„Ÿæ€§ï¼Œéœ€è¦é’ˆå¯¹å…·ä½“æ¶æ„è¿›è¡Œè°ƒæ•´ï¼Œè€Œéä¸€ä¸ªæ™®é€‚çš„æœ€ä¼˜å€¼ã€‚\n\n4.  **ç†è®ºåˆ†æçš„ç†æƒ³åŒ–å‡è®¾**\n    - **ç†è®ºæ”¯æ’‘**ï¼šè®ºæ–‡ä¸­çš„å¼•ç†1ï¼ˆå…³äºå­ç©ºé—´ä¿æŒï¼‰ä¾èµ–äº**ä½ç§©å’Œæœ‰ç•Œç›¸å¹²æ€§**çš„å‡è®¾ã€‚è™½ç„¶å®éªŒæ•°æ®ï¼ˆPCAåˆ†æï¼‰æ”¯æŒä½ç§©æ€§ï¼Œä½†ç°å®ä¸–ç•Œæ•°æ®çš„å¤æ‚æ€§å¯èƒ½ä½¿è¿™äº›ç†è®ºä¿è¯åœ¨å®é™…ä¸­å¼±åŒ–ã€‚\n\n5.  **è¯„ä¼°ç¯å¢ƒçš„é™åˆ¶**\n    - **ä»¿çœŸä¸»å¯¼**ï¼šä¸»è¦è¯„ä¼°åœ¨CARLA/VISTAç­‰ä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œã€‚å°½ç®¡æœ‰æˆåŠŸçš„**ä»¿çœŸåˆ°ç°å®ï¼ˆSim-to-Realï¼‰** è¿ç§»æ¼”ç¤ºï¼Œä½†çœŸå®ä¸–ç•Œçš„é•¿å°¾åœºæ™¯ã€æç«¯å¤©æ°”å’Œå¤æ‚äº¤äº’çš„å¤šæ ·æ€§å’Œä¸å¯é¢„æµ‹æ€§è¿œé«˜äºå½“å‰æµ‹è¯•èŒƒå›´ã€‚\n\n### æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nè®ºæ–‡åœ¨ç»“è®ºéƒ¨åˆ†æ˜ç¡®æå‡ºäº†æœªæ¥å·¥ä½œæ–¹å‘ï¼Œä¸ä¸Šè¿°å±€é™æ€§ç›¸å‘¼åº”ï¼š\n\n1.  **æ›´æ™ºèƒ½çš„é€‰æ‹©ç­–ç•¥**\n    - **çŠ¶æ€ä¾èµ–çš„é‡‡æ ·**ï¼šå­¦ä¹ ä¸€ä¸ª**ä¾èµ–äºåœºæ™¯çŠ¶æ€**çš„é‡‡æ ·ç­–ç•¥ï¼ŒåŠ¨æ€å†³å®šæ¯ä¸ªå¸§éœ€è¦ä¿ç•™å¤šå°‘ä»¥åŠå“ªäº›å—ã€‚ä¾‹å¦‚ï¼Œåœ¨ç®€å•ç›´é“ä¸Šå¯ä»¥ä¸¢å¼ƒæ›´å¤šï¼Œåœ¨å¤æ‚äº¤å‰å£åˆ™ä¿ç•™æ›´å¤šå…³é”®ä¿¡æ¯ã€‚\n    - **åŸºäºä¿¡æ¯çš„é‡‡æ ·**ï¼šè¶…è¶Šéšæœºå‡åŒ€é‡‡æ ·ï¼Œé‡‡ç”¨**æ ¸å¿ƒé›†é€‰æ‹©ã€æ³¨æ„åŠ›ç†µæˆ–äº’ä¿¡æ¯è¯„åˆ†**ç­‰æ–¹æ³•ï¼Œä¸»åŠ¨ä¸¢å¼ƒé‚£äº›è¢«è¯æ˜ä¿¡æ¯é‡æœ€å°‘æˆ–å†—ä½™åº¦æœ€é«˜çš„å—ï¼Œä»è€Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æœ€å¤§åŒ–æ•ˆç‡ã€‚\n\n2.  **ä¸åŸºç¡€æ¨¡å‹çš„é«˜æ•ˆååŒ**\n    - **è½»é‡çº§ç‰¹å¾æå–**ï¼šæ¢ç´¢ä¸SPSæ€æƒ³ååŒçš„ã€æ›´è½»é‡çš„åŸºç¡€æ¨¡å‹æˆ–ç‰¹å¾æå–ç®¡é“ï¼Œä»æºå¤´é™ä½è®¡ç®—æˆæœ¬ã€‚\n    - **è‡ªé€‚åº”åŸºç¡€æ¨¡å‹**ï¼šç ”ç©¶å¦‚ä½•å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œ**å¾®å°çš„ã€é’ˆå¯¹æ€§çš„è°ƒæ•´**ï¼Œä½¿å…¶äº§ç”Ÿçš„ç‰¹å¾å†—ä½™åº¦æ›´ä½ï¼Œæˆ–ä¸ä¸‹æ¸¸é©¾é©¶ç­–ç•¥çš„å­¦ä¹ ç›®æ ‡æ›´å¯¹é½ï¼Œè€Œä¸æ˜¯å®Œå…¨å†»ç»“ã€‚\n\n3.  **æ¶æ„ä¸æ–¹æ³•çš„æ‰©å±•**\n    - **æ—¶åºå»ºæ¨¡é›†æˆ**ï¼šå½“å‰å·¥ä½œä¸»è¦å¤„ç†å•å¸§ã€‚æœªæ¥å¯ä»¥å°†SPSæ€æƒ³æ‰©å±•åˆ°**æ—¶åºæ¨¡å‹**ä¸­ï¼Œç ”ç©¶å¦‚ä½•åœ¨è¿ç»­å¸§é—´è¿›è¡Œéšæœºå—é€‰æ‹©ï¼Œä»¥å­¦ä¹ å¯¹åŠ¨æ€åœºæ™¯æ›´é²æ£’çš„æ—¶åºè¡¨ç¤ºã€‚\n    - **å¤šæ¨¡æ€èåˆå¢å¼º**ï¼šé™¤äº†è§†è§‰å—ç‰¹å¾ï¼Œæ¢ç´¢å¦‚ä½•å°†SPSçš„éšæœºé€‰æ‹©æ€æƒ³åº”ç”¨äº**å¤šæ¨¡æ€èåˆ**ï¼ˆå¦‚LiDARç‚¹äº‘ã€é›·è¾¾æ•°æ®ï¼‰ä¸­ï¼Œä»¥æå‡åœ¨ä¼ æ„Ÿå™¨å¤±æ•ˆæˆ–å™ªå£°æƒ…å†µä¸‹çš„é²æ£’æ€§ã€‚\n\n4.  **æ›´å…¨é¢çš„è¯„ä¼°ä¸ç†è®ºæ·±åŒ–**\n    - **å¤§è§„æ¨¡çœŸå®ä¸–ç•Œæµ‹è¯•**ï¼šåœ¨æ›´å¹¿æ³›ã€æ›´é•¿æœŸçš„çœŸå®é“è·¯æµ‹è¯•ä¸­éªŒè¯SPSçš„é²æ£’æ€§å’Œå®‰å…¨æ€§ï¼Œæ”¶é›†çœŸæ­£çš„é•¿å°¾æ¡ˆä¾‹ã€‚\n    - **æ›´åšå®çš„ç†è®ºæ¡†æ¶**ï¼šå‘å±•æ›´å®Œå–„çš„ç†è®ºï¼Œåˆ†æåœ¨**æ•°æ®åˆ†å¸ƒåç§»ã€å¯¹æŠ—æ€§æ‰°åŠ¨**ç­‰æ›´ä¸¥å³»æ¡ä»¶ä¸‹ï¼Œéšæœºå—é€‰æ‹©å¯¹ç­–ç•¥æ³›åŒ–èƒ½åŠ›çš„å½±å“è¾¹ç•Œã€‚\n\n**æ€»ç»“è€Œè¨€**ï¼Œè¯¥è®ºæ–‡çš„æ ¸å¿ƒå±€é™æ€§åœ¨äºå…¶â€œéšæœºä¸¢å¼ƒâ€æœºåˆ¶çš„ç®€å•æ€§ä»¥åŠå¯¹ç°æœ‰åŸºç¡€æ¨¡å‹å†—ä½™ç»“æ„çš„ä¾èµ–ã€‚æœªæ¥çš„æ”¹è¿›å°†èšç„¦äºå°†â€œéšæœºâ€å˜ä¸ºâ€œæ™ºèƒ½â€ï¼Œå¹¶æ¨åŠ¨æ•´ä¸ªæ„ŸçŸ¥-å†³ç­–ç®¡é“å‘æ›´é«˜æ•ˆã€æ›´é²æ£’çš„æ–¹å‘æ¼”è¿›ã€‚å…¶æå‡ºçš„â€œå°‘å³æ˜¯å¤šï¼ˆless can be moreï¼‰â€å“²å­¦ä¸ºåç»­ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¯Œæœ‰æ½œåŠ›çš„æ–¹å‘ã€‚",
            "timestamp": "2026-01-20T18:02:19.036869",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-15T18:58:33Z",
    "created_at": "2026-01-20T17:49:56.378338",
    "updated_at": "2026-01-20T17:49:56.378347",
    "recommend": 0
}