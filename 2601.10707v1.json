{
  "id": "2601.10707v1",
  "title": "See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection",
  "authors": [
    "Amir Mallak",
    "Erfan Aasi",
    "Shiva Sreeram",
    "Tsun-Hsuan Wang",
    "Daniela Rus",
    "Alaa Maalouf"
  ],
  "abstract": "Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.",
  "url": "https://arxiv.org/abs/2601.10707v1",
  "html_url": "https://arxiv.org/html/2601.10707v1",
  "html_content": "See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection\nAmir Mallak\n1\nErfan Aasi\n2\nShiva Sreeram\n2\nTsun-Hsuan Wang\n2\nDaniela Rus\n2\nAlaa Maalouf\n1,2\n1\nUniversity of Haifa\n2\nCSAIL, MIT\nCorrespondance: mallak002@gmail.com\nAbstract\nRecent advances in end-to-end autonomous driving show that policies trained on patch‚Äëaligned features extracted from foundation models generalize better to Out‚ÄëOf‚ÄëDistribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant.\nWe quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity:\n90\n%\n90\\%\nof variance is captured by\n17\n/\n64\n17/64\nprincipal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness.\nWe present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient.\nFor every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches.\nThus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive.\nExtensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a\n6.2\n%\n6.2\\%\naverage improvement and up to\n20.4\n%\n20.4\\%\nin closed‚Äëloop simulations, while being\n2.4\n√ó\n2.4\\times\nfaster.\nWe conduct ablations over masking rates and patch‚Äìfeature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA.\nFinally, we show that the same learned policy transfers to a physical, real-world car without any tuning.\nFigure 1\n:\nStochastic Patch Selection (SPS) in a nutshell.\n(1) We use large vision-language models to extract patch-level features for input images. (2) These descriptors are often highly redundant and correlated. (3) SPS randomly masks a subset of descriptors, forcing the policy to learn based on different subsets and less correlation, improving efficiency (\n2.4\n√ó\n\\mathbf{2.4\\times}\nspeedup), generalization (\n+\n6.2\n%\n\\mathbf{+6.2\\%}\nperformance), and enabling plug-and-play integration with downstream policies. In (4), we compare speed vs performance on different variants of SPS against SOTA.\n1\nIntroduction and background\nAs autonomous‚Äëdriving technology matures, end‚Äëto‚Äëend approaches emerged as a leading strategy\n[\n1\n,\n2\n]\nembodying a transformative design philosophy: a single model handles everything, from perception to control. This approach (i) dispenses with many of the rigid assumptions imposed on individual submodules, and (ii) evaluates and optimizes performance against a single overarching objective, yielding a more coherent and effective system.\nFrom foundation models to driving polices.\nRecently,\nWang et¬†al.\n[\n3\n]\nshowed that extracting patch‚Äëaligned (multimodal) features from a foundation model (FM)\n[\n4\n,\n5\n,\n6\n]\n, for each input frame, and training a lightweight policy on those features, markedly improves out‚Äëof‚Äëdistribution (OOD) generalization.\nTo obtain patch features, they applied\nmasked\nattention at a chosen layer (specifically, at the input to BLIP2‚Äôs Q-Former), so the mask guides the attention to focus on a single image patch. Repeating this operation for every patch yields a descriptor per patch.\nSince the descriptors come from the output layer that interfaces with the text encoder, they live in a shared vision‚Äìlanguage space, enabling text-driven augmentation and robust policy debugging.\nPatch-wise features are redundant.\nWe posit that patch descriptors extracted from ViT backbones are intrinsically redundant and correlated.\nBefore introducing the masked‚Äëattention layer, every patch embedding produced by the sub-ViT backbone has already attended to all other patches, as self‚Äëattention mixes information across tokens; each ‚Äúpatch feature‚Äù is no longer a purely local descriptor, as it carries a weighted summary of the entire scene, although the information is focused on this patch.\nThis redundancy is detrimental when we hand the descriptors to the policy head: (1) Correlated inputs inflate the effective dimensionality of the feature space, making it harder for the policy network to disentangle the truly discriminative signals. (2) Overlapping information encourages over‚Äëfitting; the policy may memorize spurious correlations present in the duplicated context instead of learning robust, spatially grounded cues. Finally, (3) unnecessary redundancy wastes computation and memory, both during training and at inference time.\n1.1\nOur contributions.\nWe first empirically show that patch-features are redundant in Section\n4.1\nvia similarity overlays, patch-wise correlations, and PCA explained-variance curves. Building on these insights, we propose\nStochastic-Patch-Selection (SPS)\n, a lightweight, plug-and-play patch-masking strategy that improves both the efficiency and OOD generalization of policy learning; See Figure\n1\n. Our main contributions are as follows.\n(i)\nSPS:\nAt each frame, SPS randomly drops a fixed proportion of patch descriptors, forwards the untouched descriptors to the policy network, and crucially leaves their grid positions unchanged. Each unmasked subset forms a coherent yet distinct stochastic view of the scene, encouraging the policy to focus on stable/invariant, causally relevant cues instead of spurious correlations.\n(ii)\nRobustness and efficiency gains over SOTA.\nAcross all OOD scenarios, SPS improves closed‚Äëloop driving success by an average of\n6.2\n%\n6.2\\,\\%\n, and up to\n20.4\n%\n20.4\\,\\%\n, while being\n2.4\n√ó\n2.4\\times\n.\n(iii)\nComprehensive ablations at scale.\nWe train and evaluate 9 autonomous‚Äëdriving systems under varied masking ratios and patch‚Äëreorganization schemes, identifying the design choices that maximize robustness; all variants except one surpassed the previous SOTA.\n(iv)\nSim‚Äëto‚ÄëReal generalization.\nWe deployed SPS on a full-scale autonomous vehicle, showing a seamless transfer from simulation to a real car without tuning.\nNotably, as SPS operates in a shared vision‚Äëlanguage space, it inherits from\n[\n3\n]\nthe support for text‚Äëconditioned perturbations/augmentation in the patches embedding space, yielding an additional\n1.7\n%\n1.7\\,\\%\nperformance boost.\n2\nRelated work\nEnd-to-end autonomous driving.\nEarly work established that neural networks can map raw sensory inputs directly to low-level control\n[\n1\n,\n2\n]\n, with subsequent efforts exploring probabilistic objectives and uncertainty-aware control\n[\n7\n]\n, stability/attention regularization\n[\n8\n]\n, and safety-aware formulations using control barrier functions\n[\n9\n,\n10\n]\n.\nWhile promising, end-to-end approaches typically demand large, diverse real-world datasets that are costly and risky to collect at scale\n[\n11\n,\n12\n]\n.\nTo mitigate this, photorealistic simulators like CARLA, AirSim, and Drake\n[\n13\n,\n14\n,\n15\n]\n, and trace-driven interactive simulation such as\nVISTA\n[\n12\n]\n, have become standard for training and evaluation. A complementary line replaces raw pixels with intermediate visual abstractions (affordances, semantics, lane topology) to simplify control\n[\n16\n,\n17\n,\n18\n]\n. In this space, ‚ÄúDrive Anywhere‚Äù\n[\n3\n]\nintroduced a\nfoundation-model\n-based pipeline that extracts rich, multimodal patch-aligned descriptors and feeds them to a lightweight policy head, showing strong OOD generalization. Our work builds on this FM-based formulation and aims to patch features\nreduce redundancy\nat train/inference time to improve OOD robustness without modifying or fine-tuning the FM.\nFoundation models in robotics and vision.\nLarge pretrained vision language models are increasingly used as general-purpose perception and reasoning backbones, from language-conditioned manipulation and planning\n[\n19\n,\n20\n,\n21\n,\n22\n,\n23\n]\n, video summarizations\n[\n24\n]\n, and 3D/open-world scene understanding\n[\n25\n,\n26\n,\n27\n]\nto navigation and instruction following\n[\n28\n]\nand follow-and-detect pipelines\n[\n29\n,\n30\n,\n31\n,\n32\n,\n33\n]\n.\nTheir cross-modal capacity also enables generative interfaces that tie vision and language\n[\n34\n,\n35\n,\n36\n,\n37\n]\n.\nWithin autonomous driving, some prior work cautions that the temporal reasoning robustness of off-the-shelf VLMs is limited in settings that require strong temporal understanding\n[\n38\n]\n; therefore, deploying them for driving tasks out of the box is not straightforward. This motivates learning downstream driving policies on top of features extracted from pretrained VLMs, rather than relying on end-to-end VLM reasoning. works increasingly leverage language-grounded or explainable representations/features/descriptors for introspection, learning, and counterfactual analysis\n[\n39\n,\n40\n,\n41\n,\n42\n,\n43\n,\n28\n,\n3\n]\n. We follow this trend in spirit, but use only frozen FMs (BLIP/BLIP-2\n[\n5\n,\n4\n]\n, DINO\n[\n44\n,\n45\n]\n) and intervene solely at the patch-descriptor interface to a small policy head‚Äîisolating our stochastic selection from FM training dynamics.\nPositioning and novelty.\nPrior end-to-end policies\n[\n1\n,\n2\n,\n7\n,\n8\n,\n9\n,\n10\n]\nand FM-based driving frameworks\n[\n3\n,\n28\n]\nhave not treated\nfeature redundancy\nin FM patch descriptors as a first-class lever for OOD robustness. Such patch embeddings from pretrained models can be obtained by: (i)\nregion-first\npipelines that segment the image (e.g., SAM/Mask2Former) and then pool features within each mask by an FM encoder (e.g., CLIP)\n[\n46\n,\n47\n,\n48\n,\n49\n,\n50\n,\n51\n]\n. These methods inherit sensitivity to segmentation quality and add extra compute stages. (ii)\npatch-aligned\nextraction directly from the backbone without external masks or fine-tuning\n[\n52\n,\n3\n,\n28\n]\n, preserving spatial layout at token resolution and avoiding segmentation dependencies.\nOur contribution is orthogonal and complementary: we show that even strong patch-aligned descriptors,\nare highly redundant\n. We therefore introduce\nStochastic Patch Selection (SPS)\n, a simple mechanism to handle such correlations, via a\npost-FM\ntoken-selection step that preserves spatial layout, reduces token count, and improves closed-loop OOD performance while reducing latency.\nSPS is architecture-agnostic, requires no FM fine-tuning. Evidence from PCA and inter-token-correlation analyses, ablations over masking/reorganization variants, and real-world transfer leads to a simple takeaway: for FM-derived patch features in driving,\nless can be more\n: carefully injected stochastic sparsity improves generalization without any overhead.\n3\nMethod\nWe first provide the preliminaries.\n3.1\nPreliminaries\nEnd-to-end driving as feature-based control.\nWe model an autonomous-driving policy as a control function\nœï\n\\phi\nthat maps a perception stream\nF\n‚àà\n‚Ñù\nH\n√ó\nW\n√ó\n3\nF\\!\\in\\!\\mathbb{R}^{H\\times W\\times 3}\nof RGB frames to steering and throttle commands\nu\n=\nœï\n‚Äã\n(\nF\n)\nu=\\phi(F)\n. Rather than operating on raw pixels, we feed\nœï\n\\phi\na tensor of patch-aligned representations\nF\n‚Ä≤\n=\nDesc\n‚Äã\n(\nF\n)\n‚àà\n‚Ñù\nH\n‚Ä≤\n√ó\nW\n‚Ä≤\n√ó\nD\nF^{\\prime}=\\texttt{Desc}(F)\\!\\in\\!\\mathbb{R}^{H^{\\prime}\\!\\times\\!W^{\\prime}\\!\\times\\!D}\nproduced by a multimodal foundation model\nDesc\n, where\n(\nH\n‚Ä≤\n,\nW\n‚Ä≤\n)\n(H^{\\prime},W^{\\prime})\nis the patch grid and\nD\nD\nis the channel dimension, i.e.,\nu\n=\nœï\n‚Äã\n(\nF\n‚Ä≤\n)\nu=\\phi\\bigl(F^{\\prime}\\bigr)\n.\nWe now recall the masked-attention mechanism of\n[\n3\n]\n, which enforces spatially selective mixing.\nMultimodal patch-wise feature extraction\n[\n3\n]\n.\nLet\nDesc\nbe a vision transformer of\nL\nL\nlayers and\nN\n=\nH\n‚Ä≤\n‚Äã\nW\n‚Ä≤\nN=H^{\\prime}W^{\\prime}\nbe the number of non-overlapping patches.\nDuring a forward pass of\nDesc\non\nF\nF\n, the\n‚Ñì\n\\ell\n-th self-attention layer yields the query, key, and value matrices denoted by\nQ\nDesc\n‚Äã\n(\nF\n)\n‚Ñì\n,\nK\nDesc\n‚Äã\n(\nF\n)\n‚Ñì\n‚àà\n‚Ñù\nN\n√ó\nD\nk\nQ^{\\ell}_{\\texttt{Desc}(F)},K^{\\ell}_{\\texttt{Desc}(F)}\\in\\mathbb{R}^{N\\times D_{k}}\nand\nV\nDesc\n‚Äã\n(\nF\n)\n‚Ñì\n‚àà\n‚Ñù\nN\n√ó\nD\nV^{\\ell}_{\\texttt{Desc}(F)}\\in\\mathbb{R}^{N\\times D}\n, respectively. To compute the feature of patch\nj\n‚àà\n[\nN\n]\nj\\!\\in\\![N]\n,\nWang et¬†al.\n[\n3\n]\nintroduced (1) an attention mask\nm\n(\nj\n)\n‚àà\n[\n0\n,\n1\n]\nN\nm^{(j)}\\in[0,1]^{N}\n, where\nm\ni\n(\nj\n)\n=\n1\nm^{(j)}_{i}=1\npreserves information from patch\ni\ni\nand\nm\ni\n(\nj\n)\n=\n0\nm^{(j)}_{i}=0\nexcludes it, and (2) a parameter\nr\n<\n0\nr<0\ncontroling the suppression strength: the larger\n|\nr\n|\n\\lvert r\\rvert\n, the stronger the masking effect. Forming the similarity matrix\nG\nDesc\n‚Äã\n(\nF\n)\n‚Ñì\n=\nQ\nDesc\n‚Äã\n(\nF\n)\n‚Ñì\n‚Äã\nK\nDesc\n‚Äã\n(\nF\n)\n‚Ñì\n‚ä§\nG^{\\ell}_{\\texttt{Desc}(F)}=Q^{\\ell}_{\\texttt{Desc}(F)}{K^{\\ell}_{\\texttt{Desc}(F)}}^{\\top}\n, the attention focus on the information defined by\nm\n(\nj\n)\nm^{(j)}\nis:\nG\nDesc\n‚Äã\n(\nF\n)\n‚Ñì\n~\n=\n\\displaystyle\\tilde{G^{\\ell}_{\\texttt{Desc}(F)}}=\nG\nDesc\n‚Äã\n(\nF\n)\n‚Ñì\n+\n(\nùüè\n‚àí\nM\n(\nj\n)\n)\n‚Äã\nr\n,\n\\displaystyle G^{\\ell}_{\\texttt{Desc}(F)}+\\bigl(\\mathbf{1}-M^{(j)}\\bigr)r,\n(1)\nwhere\nM\n(\nj\n)\n=\n[\nm\n(\nj\n)\n,\n‚Ä¶\n,\nm\n(\nj\n)\n]\n‚ä§\n.\nM^{(j)}=[m^{(j)},\\dots,m^{(j)}]^{\\top}.\nThis operation drives attention scores in\nG\nDesc\n‚Äã\n(\nF\n)\n‚Ñì\n~\n\\tilde{G^{\\ell}_{\\texttt{Desc}(F)}}\nfor patches with\nm\ni\n‚âà\n0\nm_{i}\\approx 0\ndown to\nr\nr\n, thus masking them in the following softmax. The term\n(\nùüè\n‚àí\nM\n(\nj\n)\n)\n(\\mathbf{1}-M^{(j)})\nadds 0 when the mask is 1 and\nr\nr\nwhen it is near 0. The masked attention weights\nSoftMax\n‚Äã\n(\nùí¢\n~\n)\n\\texttt{{SoftMax}}(\\tilde{\\mathcal{G}})\nyield the desired descriptor through the remaining layers:\nF\n‚Ä≤\n‚Å£\n(\nj\n)\n=\nDesc\n‚Ñì\n‚Üí\n‚Äã\n(\nSoftMax\n‚Äã\n(\nùí¢\n~\n)\n‚Äã\nV\nDesc\n‚Äã\n(\nF\n)\n‚Ñì\n)\n.\nF^{\\prime(j)}\\;=\\;\\texttt{Desc}^{\\ell\\rightarrow}\\!\\bigl(\\texttt{{SoftMax}}(\\tilde{\\mathcal{G}})V^{\\ell}_{\\texttt{Desc}(F)}\\bigr).\n(2)\nRepeating this procedure for all\nj\nj\nreconstructs\nF\n‚Ä≤\n‚àà\n‚Ñù\nH\n‚Ä≤\n√ó\nW\n‚Ä≤\n√ó\nD\nF^{\\prime}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times D}\n.\nMask design.\nFor a patch\nj\nj\n, we set weight\nm\ni\n(\nj\n)\n=\nf\n‚Äã\n(\nd\ni\n‚Äã\nj\n)\nm^{(j)}_{i}=f\\!\\bigl(d_{ij}\\bigr)\nwhere\nd\ni\n‚Äã\nj\nd_{ij}\nis the distance in the patch grid\n‚à•\n(\nx\ni\n,\ny\ni\n)\n‚àí\n(\nx\nj\n,\ny\nj\n)\n‚à•\n2\n\\lVert(x_{i},y_{i})-(x_{j},y_{j})\\rVert_{2}\n, and\nf\nf\nas a hard cutoff\nùüè\n‚Äã\n[\nd\ni\n‚Äã\nj\n‚â§\nŒ±\n]\n\\mathbf{1}[d_{ij}\\leq\\alpha]\n, or soft decays\n2\n‚àí\nd\ni\n‚Äã\nj\n2^{-d_{ij}}\nand\n1\n/\nd\ni\n‚Äã\nj\n1/d_{ij}\n, giving flexible locality.\n3.2\nRedundancy as low-rank structure\nFigure 2\n:\nSPS algorithm vs Drive-Anywhere illustration\n. From left to Right. 1st: Input images are processed through a frozen foundation model to produce patch-level descriptors. 2nd: In\nDrive-Anywhere\n, the full tensor is forwarded unchanged to the policy. 3rd: In our approach, we introduce two patch selection strategies (uniform stochastic and matrix-based probability selection), followed by (4th) a restructuring phase: either masking unselected descriptors or removing them and adjusting positional encodings. Both versions preserve spatial semantics while significantly reducing runtime. SPS improves efficiency by a factor of\n2.4\n√ó\n\\mathbf{2.4\\times}\nwhile also boosting generalization.\nLet\nF\nc\n‚àà\n‚Ñù\nN\n√ó\nD\nF_{c}\\in\\mathbb{R}^{N\\times D}\ndenote the matrix of patch descriptors obtained by applying\nDesc\nto\nF\nF\n, where\nN\n=\nH\n‚Ä≤\n‚Äã\nW\n‚Ä≤\nN=H^{\\prime}W^{\\prime}\nis the number of patches and\nD\nD\nis the dimension of each descriptor (\nF\nc\nF_{c}\nis the reshaped\nF\n‚Ä≤\nF^{\\prime}\nto matrix form).\nLe\nF\n~\nc\n\\tilde{F}_{c}\nbe the mean-subtracted matrix, and\nŒª\n1\n‚â•\n‚ãØ\n‚â•\nŒª\nr\n>\n0\n\\lambda_{1}\\geq\\cdots\\geq\\lambda_{r}>0\nbe the eigenvalues of the sample covariance\nŒ£\n=\n1\nN\n‚Äã\nF\n~\nc\n‚ä§\n‚Äã\nF\n~\nc\n\\Sigma=\\tfrac{1}{N}\\tilde{F}_{c}^{\\top}\\tilde{F}_{c}\n, with rank\nr\n‚â§\nd\nr\\leq d\n. The cumulative explained variance after the first\nm\n‚â§\nr\nm\\leq r\ncomponents is\nE\n‚Äã\n(\nm\n)\n:=\n‚àë\ni\n=\n1\nm\nŒª\ni\n‚àë\ni\n=\n1\nr\nŒª\ni\n.\n\\displaystyle E(m):=\\frac{\\sum_{i=1}^{m}\\lambda_{i}}{\\sum_{i=1}^{r}\\lambda_{i}}.\n(3)\nA rapidly saturating\nE\n‚Äã\n(\nm\n)\nE(m)\nfor\nm\n‚â™\nd\nm\\ll d\nindicates a low-rank structure, hence redundancy among the patch descriptors.\nIn Section\n4.1\nwe empirically show that BLIP2 features reach\n90\n%\n90\\%\nexplained variance with\n17\n17\nof\n64\n64\ncomponents (\nE\n‚Äã\n(\n17\n)\n>\n0.9\nE(17)>0.9\n). Even more, when using half of the patches with the highest\n‚Ñì\n2\n\\ell_{2}\nnorm, the first\n14\n14\nprincipal components explain\n90\n%\n90\\%\nof the variance (\nE\n‚Äã\n(\n14\n)\n>\n0.9\nE(14)>0.9\n).\nA proof sketch and implementation details appear in Appx.\nA.1\n.\n3.3\nStochastic patch selection (SPS)\nReducing descriptor redundancy.\nAs we saw,\ninformation about a scene is implicitly replicated across all tokens. The resulting redundancy\n(i)¬†inflates the computation needed to extract\nevery\npatch feature,\n(ii)¬†encourages the policy to rely on spurious,\ncorrelated cues that collapse under distribution shift, and\n(iii)¬† forces the policy to require every tiny detail of the full scene to provide a robust decision.\nOur goal is therefore to\nexpose the policy\n, at every input frame, to a fraction of the patch descriptors and withhold the others, via a token-level stochastic selection mechanism operating in the latent space of the foundation model, while maintaining the spatial layout of the remaining patches. Each random subset forms a different, yet still coherent, projection of the same scene, giving the policy a stream of stochastic but complete views. Consequently, the controller is trained to base its decisions on features that remain invariant to which specific tokens survive, yielding improved OOD robustness and lower compute, all while preserving spatial coherence and encouraging the use of robust, generalizable signals over exact token identity.\nLet\nN\n=\nH\n‚Ä≤\n‚Äã\nW\n‚Ä≤\nN=H^{\\prime}W^{\\prime}\ndenote the number of patch positions produced by the\nbackbone\nDesc\n(see Sec.\n3.1\n).\nFix a\nsampling rate\nrate\n‚àà\n(\n0\n,\n1\n]\n\\textsc{rate}\\in(0,1]\n.\nFor every input frame\nt\nt\nwe:\n1.\nSample a subset.\nUniformly at random, select exactly\nk\n=\n‚åà\nrate\n‚ãÖ\nN\n‚åâ\nk=\\lceil\\textsc{rate}\\cdot N\\rceil\npatch indices,\ndenoted\nŒ©\nt\n‚äÜ\n{\n1\n,\n‚Ä¶\n,\nN\n}\n\\Omega_{t}\\subseteq\\{1,\\dots,N\\}\nwith\n|\nŒ©\nt\n|\n=\nk\n|\\Omega_{t}|=k\n.\n2.\nCompute descriptors.\nRun the masked‚Äìattention extractor\n(Sec.\n3.1\n)\nonly\nfor patches\ni\n‚àà\nŒ©\nt\ni\\in\\Omega_{t}\n,\nobtaining for every\ni\ni\n, the descriptor\nF\ni\n‚Ä≤\n‚àà\n‚Ñù\nD\nF^{\\prime}_{i}\\in\\mathbb{R}^{D}\n.\n3.\nBuild the sparse tensor.\nAssemble\nF\n~\n‚Ä≤\n‚àà\n‚Ñù\nH\n‚Ä≤\n√ó\nW\n‚Ä≤\n√ó\nD\n\\tilde{F}^{\\prime}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times D}\n:\nF\n~\ni\n‚Ä≤\n=\n{\nF\ni\n‚Ä≤\n,\ni\n‚àà\nŒ©\nt\n,\nùüé\n,\notherwise\n,\n\\tilde{F}^{\\prime}_{i}=\\begin{cases}F^{\\prime}_{i},&i\\in\\Omega_{t},\\\\\n\\mathbf{0},&\\text{otherwise},\\end{cases}\n(4)\nthereby\npreserving the original spatial layout\nso the policy head receives a tensor\nof fixed size.\nComputational footprint.\nThe expected fraction of evaluated descriptors is\nrate\n, so feature extraction time scales linearly with the keep-rate.\nFor example,\nrate\n=\n0.4\n\\textsc{rate}=0.4\ncuts the forward pass of the foundation model ViT\nL\n{}_{\\text{L}}\nbackbone by\n‚àº\n2.5\n√ó\n\\sim\\!2.5\\times\nwithout\naltering the policy architecture. The variant which fix the positional embedding at the VIT, also improve the runtime of the policy.\nOverall.\nSPS turns a dense, patch-aligned representation into a\nstochastically sparse\n, yet spatially coherent, token sequence. By training the control policy on these variable but complete\nprojections of the scene, we (1) reduce compute, (2) discourage reliance on redundant correlations, and (3) drive consistent gains in OOD generalization; see Section\n4.3\n.\n3.4\nAblations and variants\nTo understand the design space, we evaluate two additional variants of the above procedure.\n(v1) Threshold masking.\nInstead of sampling exactly\nk\nk\nindices,\ndraw an i.i.d. vector\nR\n‚àº\nùí∞\n‚Äã\n(\n0\n,\n1\n)\nH\n‚Ä≤\n‚Äã\nW\n‚Ä≤\nR\\sim\\mathcal{U}(0,1)^{H^{\\prime}W^{\\prime}}\nand define\nŒ©\nt\n=\n{\ni\n‚à£\nR\ni\n‚â§\nrate\n}\n.\n\\Omega_{t}=\\bigl\\{i\\mid R_{i}\\leq\\textsc{rate}\\bigr\\}.\n(5)\nEach patch is sampled independently with\nprobability\nrate\n, so the expected keep-rate is\nrate\nbut the actual count fluctuates.\nThis adds stochastic diversity at negligible cost.\n(v2) Position-adjusted sparse sequence.\nAfter choosing\nŒ©\nt\n\\Omega_{t}\n(either by fixed-count or threshold\nmasking),\nomit\nthe dropped tokens entirely and feed the policy\nonly the descriptors that were actually computed. Each retained\ndescriptor\nF\ni\n‚Ä≤\nF^{\\prime}_{i}\nis augmented with its original positional\nembedding\np\ni\n‚àà\n‚Ñù\nD\np_{i}\\!\\in\\!\\mathbb{R}^{D}\ndrawn from the ViT‚Äôs full\npositional-embedding table, so the controller still knows\nwhere\neach token came from. This yields a variable-length input sequence\nthat is shorter by a factor of\nrate\nwithout inserting any zero\nvectors.\nFigure\n2\nillustrates our method alongside its variants.\n3.5\nSPS preserves scene semantics\nSubspace preservation under uniform row sampling.\nIf the variance of\nF\nc\nF_{c}\nconcentrates in a few components, the patch descriptors (rows of\nF\nc\nF_{c}\n) lie near a low-dimensional subspace. If the row coherence is low, uniformly sampling enough rows preserves this principal subspace with high probability, hence the policy still receives descriptors that span the same semantics. We now show it formally.\nLemma 1\n(SPS preserves the row-space under low rank and bounded coherence)\n.\nLet\nF\nc\n‚àà\n‚Ñù\nN\n√ó\nd\nF_{c}\\in\\mathbb{R}^{N\\times d}\nbe a centered data matrix with\nrank\n‚Å°\n(\nF\nc\n)\n=\nr\n\\operatorname{rank}(F_{c})=r\n, and let its (thin) singular value decomposition be\nF\nc\n=\nU\nr\n‚Äã\nŒ£\nr\n‚Äã\nV\nr\n‚ä§\n,\nF_{c}\\;=\\;U_{r}\\Sigma_{r}V_{r}^{\\top},\nwhere\nU\nr\n‚àà\n‚Ñù\nN\n√ó\nr\nU_{r}\\in\\mathbb{R}^{N\\times r}\nand\nV\nr\n‚àà\n‚Ñù\nd\n√ó\nr\nV_{r}\\in\\mathbb{R}^{d\\times r}\nhave orthonormal columns. The orthogonal projector onto the row space of\nF\nc\nF_{c}\nis\nŒ†\nF\n=\nV\nr\n‚Äã\nV\nr\n‚ä§\n,\n\\Pi_{F}\\;=\\;V_{r}V_{r}^{\\top},\nand the row-space coherence is defined as\nŒº\n‚âú\nN\nr\n‚Äã\nmax\ni\n‚àà\n[\nN\n]\n‚Å°\n‚Äñ\ne\ni\n‚ä§\n‚Äã\nU\nr\n‚Äñ\n2\n2\n‚àà\n[\n1\n,\nN\n/\nr\n]\n.\n\\mu\\;\\triangleq\\;\\frac{N}{r}\\,\\max_{i\\in[N]}\\|e_{i}^{\\top}U_{r}\\|_{2}^{2}\\;\\in\\;[1,\\,N/r].\nLet\n‚Ñê\n‚äÇ\n[\nN\n]\n\\mathcal{I}\\subset[N]\nbe a uniformly random subset of\nm\nm\nindices without replacement, and let\nF\n‚Ñê\n‚àà\n‚Ñù\nm\n√ó\nd\nF_{\\mathcal{I}}\\in\\mathbb{R}^{m\\times d}\nbe the corresponding submatrix of\nF\nc\nF_{c}\n. Let\nŒ†\nF\n‚Ñê\n\\Pi_{F_{\\mathcal{I}}}\ndenote the orthogonal projector onto the row space of\nF\n‚Ñê\nF_{\\mathcal{I}}\n. Then there exists a constant\nC\n>\n0\nC>0\nsuch that, for any\nŒµ\n,\nŒ¥\n‚àà\n(\n0\n,\n1\n)\n\\varepsilon,\\delta\\in(0,1)\n, if\nm\n‚â•\nC\n‚Äã\nŒº\n‚Äã\nr\nŒµ\n2\n‚Äã\nlog\n‚Å°\n(\nr\nŒ¥\n)\n,\nm\\;\\geq\\;C\\,\\frac{\\mu\\,r}{\\varepsilon^{2}}\\,\\log\\!\\Bigl(\\frac{r}{\\delta}\\Bigr),\n(6)\nthe following holds with probability at least\n1\n‚àí\nŒ¥\n1-\\delta\n:\n‚Äñ\nŒ†\nF\n‚àí\nŒ†\nF\n‚Ñê\n‚Äñ\n2\n‚â§\nŒµ\n,\n\\bigl\\|\\Pi_{F}-\\Pi_{F_{\\mathcal{I}}}\\bigr\\|_{2}\\;\\leq\\;\\varepsilon,\n(7)\ni.e., the principal\nr\nr\n-dimensional subspace of\nF\nc\nF_{c}\nis preserved. For the full proof, see section\nA.4\n.\n4\nExperimental results\nFigure 3\n:\n(a) Cumulative explained variance over principal components for all patches versus the top-\n128\n128\npatches selected by\n‚Ñì\n2\n\\ell_{2}\nnorm. The red line marks\n90\n%\n90\\%\n. Vertical markers indicate\n17\n17\nand\n14\n14\ncomponents for all and top-\n128\n128\n, respectively. (b) Patch-wise Pearson correlation matrix for one scenario. Strong off-diagonal correlation indicates widespread cross-patch redundancy. (c) Cosine-similarity overlays projected onto the image plane. Bright regions indicate patches whose descriptors are highly similar to the seed, visualizing global entanglement from self-attention.\nWe begin by empirically confirming that patch-wise features are redundant and highly correlated.\n4.1\nRedundancy analysis: qualitative and quantitative evidence\nPCA explained variance over feature dimensions.\nLet\nF\n‚àà\n‚Ñù\nN\n√ó\nd\nF\\in\\mathbb{R}^{N\\times d}\nbe the descriptor matrix. We visualize the cumulative explained variance\nE\n‚Äã\n(\nm\n)\nE(m)\n(as in¬†(\n3\n)) as a function of the number of components used\nm\nm\n. Fig.\n3\n(a) shows that\n90\n%\n90\\%\nof the variance is captured by\n17\n17\nof\n64\n64\nprincipal components. Repeating the analysis over the\n128\n128\npatches with the highest norm, yields\n90\n%\n90\\%\nwith\n14\n14\ncomponents, which confirms that redundancy persists even among the strongest tokens. The experiment was conducted on\n10000\n10000\nframes from different scenes, and averaged across all.\nPatch-wise correlation structure.\nWe compute the Pearson correlation between all patch descriptors within a frame to obtain an\nN\n√ó\nN\nN\\times N\ncorrelation matrix. A representative heatmap appears in Fig.\n3\n(b), which shows extensive off-diagonal positive and negative correlation, indicating pervasive redundancy across spatial tokens.\nSimilarity overlays on the image plane exampe.\nFor a seed patch\ni\ni\nwith descriptor\nf\ni\n‚àà\n‚Ñù\nd\nf_{i}\\in\\mathbb{R}^{d}\n, we compute cosine similarities\ns\nj\n=\nf\ni\n‚ãÖ\nf\nj\n‚à•\nf\ni\n‚à•\n‚Äã\n‚à•\nf\nj\n‚à•\ns_{j}=\\frac{f_{i}\\cdot f_{j}}{\\lVert f_{i}\\rVert\\lVert f_{j}\\rVert}\nfor all patches\nj\nj\n, reshape\ns\n‚àà\n‚Ñù\nN\ns\\in\\mathbb{R}^{N}\nto the\nH\n‚Ä≤\n√ó\nW\n‚Ä≤\nH^{\\prime}\\times W^{\\prime}\ngrid, upsample to the input resolution, and overlay it on the raw image with a heatmap. High intensity indicates semantic overlap. Examples for two seed locations are shown in Fig.\n3\n(c). Widespread high similarity far from the seed in multiple locations illustrates global entanglement and redundancy. See Appx.\nA.2\nfor the full\n16\n√ó\n16\n16\\times 16\ngrid of overlays for all patches.\nThese qualitative and quantitative results support our hypothesis that patch descriptors live in a low-dimensional subspace and exhibit strong cross-patch redundancy, explaining why stochastic sub-sampling maintains scene semantics.\nFigure 4\n:\nLeft: Diverse OOD sample frames across varying seasons, weather, and lighting conditions. Right: Real-car deployment representative frames from the rural road (public park) and parking garage, captured from both the onboard camera and external view.\n4.2\nSPS Experimental settings\nWe conduct extensive closed-loop driving experiments both for in-distribution and OOD environments. Below, we detail the hardware, tasks, evaluation metrics, and training data.\nTask definition and evaluation protocol.\nAs in\n[\n3\n]\n, we target a general-purpose autonomous driving task requiring the vehicle to follow lanes and avoid obstacles. Failure is defined by three conditions: (i) crossing lane boundaries, (ii) collisions or dangerously close proximity to objects, and (iii) heading deviations exceeding 30\n‚àò\nfrom the lane direction. In simulation, we assess performance using a normalized success duration metric, measuring how long the vehicle drives without triggering any failure, averaged over 100 episodes of roughly 20 seconds each. In real-world testing, performance is quantified by counting safety driver interventions, following the same failure definitions. Evaluations are performed in a closed-loop manner unless otherwise specified.\nTraining data and learning framework.\nOur training data combines real-world driving logs with diverse simulated experiences generated using VISTA\n[\n12\n]\n, a simulator built upon approximately two hours of real driving data captured across varied environments, lighting, and weather conditions. The learning procedure adopts Guided Policy Learning\n[\n53\n,\n12\n]\n, leveraging privileged simulator signals to supervise image-based control policies. Control labels are derived using a PID controller for lane keeping and Control Barrier Functions (CBFs)\n[\n9\n]\nfor safety-aware obstacle avoidance.\nPolicies are trained using the Adam optimizer (learning rate\n10\n‚àí\n3\n10^{-3}\n), an L2 loss objective, and a plateau scheduler with patience 10 and no decay.\nWe utilize BLIP-2\n[\n4\n]\nas the feature extractor to enable a fare comparison with SOTA\n[\n3\n]\n.\nVehicle platform and Hardware.\nThe real car experiments were conducted using a fully autonomous 2019 Lexus RX 450H equipped with high-performance computing and sensing hardware. The onboard system includes an NVIDIA RTX 4070 Ti GPU and an AMD Ryzen 7 3800X 8-core CPU. Visual input is captured via a BFS-PGE-23S3C-CS camera running at 30 frames per second, with a 130\n‚àò\nhorizontal field of view and a resolution of 960√ó600 pixels.\nThe models were trained and evaluated on a university-managed cloud cluster using a total of four A100 GPUs (40GB each). Training took approximately four days.\n4.3\nOOD generalization\nTable 1\n:\nBenchmarking OOD generalization.\n‚Ä†\nIndicates car types different from training.\nID\n= in-distribution.\nOOD\n= out-of-distribution.\nSetting\nScenarios\nMethods\nUni-modal FMF\nMulti-modal FMF\nScene\nSeason\nWeather\nTime\nActor\nNo-FM\nI-ViT\nMF\nDA\nSPS (Ours)\nID\nRural\nSummer\nDry\nDay\nCar\n1.00\n1.00\n0.72\n1.00\n1.00\nOOD\nRural\nSpring\nDry\nDay\nCar\n‚Ä†\n0.84\n0.86\n0.42\n0.96\n0.98\nSummer\nDry\nNight\nCar\n‚Ä†\n0.30\n0.80\n0.35\n0.89\n0.92\nFall\nDry\nDay\nCar\n‚Ä†\n0.90\n0.95\n0.74\n0.91\n0.96\nWinter\nSnow\nDay\nCar\n‚Ä†\n0.14\n0.88\n0.42\n0.96\n0.99\nSpring\nDry\nDay\nAnimal\n0.85\n0.89\n0.39\n0.95\n0.99\nSummer\nDry\nNight\nAnimal\n0.29\n0.59\n0.39\n0.85\n0.86\nFall\nDry\nDay\nAnimal\n0.87\n0.95\n0.71\n0.88\n0.93\nWinter\nSnow\nDay\nAnimal\n0.15\n0.87\n0.45\n0.95\n0.99\nUrban\nSummer\nDry\nDay\nCar\n‚Ä†\n0.55\n0.77\n0.50\n0.62\n0.82\nSummer\nRain\nDay\nCar\n‚Ä†\n0.69\n0.81\n0.43\n0.81\n0.83\nSummer\nDry\nNight\nCar\n‚Ä†\n0.45\n0.81\n0.42\n0.78\n0.87\nSummer\nDry\nDay\nAnimal\n0.58\n0.80\n0.50\n0.64\n0.79\nSummer\nRain\nDay\nAnimal\n0.66\n0.83\n0.43\n0.78\n0.84\nSummer\nDry\nNight\nAnimal\n0.45\n0.86\n0.36\n0.81\n0.88\nAverage\n0.55\n0.83\n0.47\n0.84\n0.90\nOur increase (%)\nvs. other methods\n‚Üë\nùüëùüì\n%\n\\uparrow 35\\%\n‚Üë\nùüï\n%\n\\uparrow 7\\%\n‚Üë\nùüìùüë\n%\n\\uparrow 53\\%\n‚Üë\nùüî\n%\n\\uparrow 6\\%\n‚Äì\nTo contextualize our improvements, we adapt the OOD generalization experiments and benchmarks reported in the\nDrive-Anywhere\n[\n3\n]\nframework.\nSpecifically, we train the model in rural environments during summer, under dry weather conditions and daytime lighting, with the presence of other vehicles on the road. We then evaluate its performance across diverse scenes, weather conditions, times of day, and in the presence of other dynamic actors.\nBaselines:\n(i) No Foundation Model (\nNo-FM\n)\n[\n8\n]\n,\n[\n12\n]\n, a baseline that trains a convolutional model without using foundation models (transformer-based variants performed similarly); (ii)\nMask-based Features (MF)\n[\n29\n,\n47\n]\n, which segment the input image by applying a universal segmentation model\n[\n46\n]\n, extracts a global feature vector for each region by applying a pretrained encoder\n[\n49\n]\non a crop bounding this region, and assign that vector uniformly to all pixels within the mask. (iii)\nInherent ViT Features (I-ViT)\n[\n52\n]\n, which extract per-patch features from the output of intermediate layers of a ViT model\n[\n6\n]\n, using the key, query, and value matrices as token-aligned visual descriptors, (iv) the current SOTA Drive Anywhere (DA)\n[\n3\n]\n, which creates per-patch descriptor via the masking strategy explained in Section\n3.1\n, and ours;\nSPS\nas defined in Section\n3.3\nwithout adding any variant from\n3.4\n, with a\nrate\n= 0.5.\nDiscussion.\nThe results are reported in Table\n1\n. First,\nMF\nunderperforms across both in-distribution and OOD settings, likely due to the applied masking model, which may miss and not segment part of the image\n[\n54\n]\n, and thus exclude relevant information from the feature tensor. In contrast,\nI-ViT\nand\nDA\noutperform\nNo-FM\n, underscoring the benefit of pretrained representations. However,\nI-ViT\nremains a uni-modal approach and does not incorporate language grounding. Finally, our proposed method,\nSPS\n, clearly outperforms all baselines, improving upon the previous SOTA (\nDA\n) by an average of 6% and up to 20% in specific OOD scenarios. Our improvement over each method is reported in the last row of the table. We provide examples in Fig.\n4\nfor representative frames from diverse OOD conditions.\nOther variants.\nWe evaluate additional variants of\nSPS\nyielding nine different models in total, incorporating different architectural modifications (e.g., different\nrate\n, position-adjusted, threshold masking). Figure\n5\nshows that 8 out of the 9 models outperform the strongest baseline in OOD settings, and Table\n2\nshows detailed results of all variants across each OOD scenario, demonstrating the robustness and flexibility of our approach; see Section\n4.5\nfor details.\nCross-backbone generalization (DINO).\nWe also applied SPS to a DINO backbone to test transfer beyond BLIP2. For brevity, we report in the appendix OOD scenarios that vary a lot from the in-distribution training set and are therefore especially challenging. On this subset, SPS improves DINO in\n6\n6\nof\n7\n7\nscenarios and ties in one, with an average absolute gain of\n+\n3.3\n%\n+3.3\\%\n; see Table\n5\nin the Appendix.\nTable 2\n:\nAblating all of our variants for OOD generalization.\n‚Ä†\nindicates car types different from training.\nID\nis in distribution.\nOOD\nis out-of-distribution.\nSetting\nScenarios\nMethods\nScene\nSeason\nWeather\nTime\nActor\nDA\nMSPPS-70%\nMSPPS-50%\nMSPPS-30%\nSPPS-70%\nSPPS-50%\nSPPS-30%\nSPS-70%\nSPS-50%\nSPS-30%\nID\nRural\nSummer\nDry\nDay\nCar\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nOOD\nRural\nSpring\nDry\nDay\nCar\n‚Ä†\n0.96\n0.96\n0.99\n0.97\n0.98\n0.98\n0.99\n0.99\n0.98\n0.94\nSummer\nDry\nNight\nCar\n‚Ä†\n0.89\n0.82\n0.89\n0.87\n0.87\n0.91\n0.84\n0.93\n0.92\n0.91\nFall\nDry\nDay\nCar\n‚Ä†\n0.91\n0.98\n0.94\n0.97\n0.95\n0.97\n0.97\n0.96\n0.96\n0.98\nWinter\nSnow\nDay\nCar\n‚Ä†\n0.96\n0.95\n0.98\n0.99\n0.99\n0.96\n0.99\n0.96\n0.99\n0.98\nSpring\nDry\nDay\nAnimal\n0.95\n0.95\n0.97\n0.97\n0.97\n0.98\n0.95\n0.95\n0.99\n0.95\nSummer\nDry\nNight\nAnimal\n0.85\n0.79\n0.84\n0.77\n0.86\n0.84\n0.80\n0.87\n0.86\n0.87\nFall\nDry\nDay\nAnimal\n0.88\n0.99\n0.91\n0.94\n0.93\n0.97\n0.92\n0.97\n0.93\n0.92\nWinter\nSnow\nDay\nAnimal\n0.95\n0.94\n0.97\n0.99\n0.99\n0.97\n0.95\n0.97\n0.99\n0.97\nUrban\nSummer\nDry\nDay\nCar\n‚Ä†\n0.62\n0.53\n0.70\n0.75\n0.79\n0.65\n0.77\n0.71\n0.82\n0.72\nSummer\nRain\nDay\nCar\n‚Ä†\n0.81\n0.65\n0.85\n0.91\n0.83\n0.84\n0.82\n0.82\n0.83\n0.86\nSummer\nDry\nNight\nCar\n‚Ä†\n0.78\n0.71\n0.89\n0.90\n0.88\n0.84\n0.86\n0.79\n0.87\n0.85\nSummer\nDry\nDay\nAnimal\n0.64\n0.50\n0.65\n0.72\n0.76\n0.64\n0.74\n0.73\n0.79\n0.70\nSummer\nRain\nDay\nAnimal\n0.78\n0.65\n0.86\n0.91\n0.87\n0.86\n0.79\n0.86\n0.84\n0.87\nSummer\nDry\nNight\nAnimal\n0.81\n0.71\n0.88\n0.90\n0.89\n0.85\n0.86\n0.85\n0.88\n0.90\nAverage\n84.2\n%\n84.2\\%\n79.5\n%\n79.5\\%\n88\n%\n88\\%\n89.7\n%\n89.7\\%\n89.7\n%\n89.7\\%\n87.6\n%\n87.6\\%\n87.5\n%\n87.5\\%\n88.3\n%\n88.3\\%\n90.4\n%\n90.4\\%\n88.7\n%\n88.7\\%\nOur increase (%)\nvs. other methods\n‚Äì\n‚Üì\n4.7\n%\n\\downarrow 4.7\\%\n‚Üë\n3.8\n%\n\\uparrow 3.8\\%\n‚Üë\n5.5\n%\n\\uparrow 5.5\\%\n‚Üë\n5.5\n%\n\\uparrow 5.5\\%\n‚Üë\n3.4\n%\n\\uparrow 3.4\\%\n‚Üë\n3.3\n%\n\\uparrow 3.3\\%\n‚Üë\n4.1\n%\n\\uparrow 4.1\\%\n‚Üë\n6.2\n%\n\\uparrow 6.2\\%\n‚Üë\n4.5\n%\n\\uparrow 4.5\\%\n4.4\nLatent space text-augmented fine-tuning\nWe adapt language-guided latent space augmentation to further improve the robustness of our best model,\nSPS-50%\n. The augmentation pipeline proceeds as follows: (i) We prompt an LLM to produce a concise list of driving-relevant textual features; visual concepts present in the training scenes, e.g.,\nTree\nor\nTruck\n; these are then identified, and will plausibly be replaced to foster OOD generalization.\n(ii) An LLM is queried to generate alternative concepts that are visually plausible and commonly encountered in driving scenarios (e.g., house instead of tree).\n(iii) In selected frames, patch-level descriptors associated with the target concepts are replaced in latent space with those derived from the suggested alternatives using the foundation model‚Äôs text encoder. This results in a form of data augmentation that preserves scene coherence while introducing semantic diversity.\nTable 3\n:\nLanguage-guided augmentation for OOD generalization.\nWe fine-tune the pretrained\nSPS-50%\nmodel using language-driven latent feature substitution based on LLM-suggested concepts.\nRSDDC\nRSDNC\nRFDDC\nRWSDC\nRSDDA\n‚Äì\n+0.29%\n+1.85%\n‚Äì\n‚Äì\nRSDNA\nRFDDA\nRWSDA\nUSDDC\nUSRDC\n-0.21%\n+2.93\n‚Äì\n-1.16\n+4.39%\nUSDNC\nUSDDA\nUSRDA\nUSDNA\nAll\n+0.36%\n+3.86%\n+3.28%\n+1.43%\n+1.7%\nWe apply this technique as a lightweight fine-tuning step to the pretrained\nSPS-50%\nmodel. The model is updated for a small number of iterations using augmented latent features.\nAs shown in Table\n3\n, this results in a further +1.7% accuracy gain across OOD scenarios, excluding cases where the base model already achieves near-perfect performance (\n‚â•\n\\geq\n98%), where meaningful further gains are unlikely. These results show that\nSPS\neffectively integrates semantic augmentations through an interpretable, text-driven process, enabling scalable generalization via concept-level latent edits.\n4.5\nAblations: efficiency and performance\nBeyond improving generalization, SPS offers substantial computational benefits during both training and inference. By selecting only a subset of patch features, we reduce the overall processing load without sacrificing semantic richness. We explore multiple variants leveraging the SPS mechanism.\nVariants.\nIn the base\nSPS\nvariant, unselected patch descriptors are replaced with zeros, preserving the feature tensor‚Äôs original shape. In the more aggressive\nSPPS (Structured Patch Pruning with Selection)\n, unselected tokens are entirely removed; the selected tokens are then spatially reorganized and assigned position embeddings relative to their original location (Method V2 from Section\n3.4\n).\nA third variant,\nMSPPS (Matrix-based Structured Patch Pruning)\n, stochastically prunes patch tokens based on a per-patch probability mask, yielding dynamic, matrix-based sparsity. Notably, it also prunes unselected patches as in SPPS (the combination of V1 and V2 from section\n3.4\n).\nSpeed reported results.\nEach variant is evaluated across three selection rates: 70%, 50%, and 30%. Inference timing was measured over 100 independent runs per model component, with averages reported for the QFormer, vision projection and normalization, BLIP2 convolution, and the ViT policy model. While we train and benchmark nine model variants in total, only six representative models are shown in Table\n4\n, as the\nSPPS\nand\nMSPPS\nvariants are functionally similar in terms of runtime, with any differences being negligible in practice.\nThe\nSPS-50%\nmodel (our best model in terms of accuracy) achieves a\n2.43\n√ó\n2.43\\times\nspeedup over the SOTA\nDrive-Anywhere\n.\nAt the more aggressive 30% selection rate,\nSPPS\nand\nSPS\nreach\n3.31\n√ó\n3.31\\times\nand\n3.47\n√ó\n3.47\\times\nspeedups, respectively. This shows the core trends: a consistent correlation between lower selection rates and improved runtime, highlighting that SPS not only improves generalization, but also enables substantial computational gains, making it attractive for real-world autonomous systems deployment.\nSpeed vs accuracy reported results.\nTo analyse the trade-off between efficiency and accuracy, in Figure\n1\n(4) we show a comparison, where each model is represented by a single node: The x-axis reflects runtime cost, defined as the inverse of the model‚Äôs speedup over the baseline -\n1\n/\nT\nFactor\n1/T_{\\text{Factor}}\n(lower is better), while the y-axis reports average closed-loop performance.\nNode color encodes a composite performance score, computed as accuracy divided by runtime (higher is better), highlighting models that strike the best balance between precision and efficiency.\nThe six reported models gain better accuracy while being faster than the drive anywhere model; the improvements of our variant in terms of accuracy vary from\n3.3\n%\n3.3\\%\nto\n6.2\n%\n6.2\\%\n, while the speed improved\n1.69\n√ó\n1.69\\times\nto\n3.47\n√ó\n3.47\\times\n.\nWhile our composite performance analysis (Fig.\n1\n(4)) shows that\nSPS-70%\nand\nSPPS-70%\nachieve the highest scores when accuracy and runtime are equally weighted, practical deployment considerations suggest a different weighting scheme.\nIn the context of real-world autonomous driving, reliability and generalization, particularly under OOD conditions, are of higher criticality than marginal gains in computational speed. The\nSPS-50%\nmodel consistently delivers the highest average accuracy across scenarios, and this performance margin is non-trivial in safety-critical systems. We therefore view\nSPS-50%\nas the strongest overall candidate for real-world deployment, offering the best tradeoff between robustness and efficiency.\nThe effect of\nrate\n.\nAs shown in Table\n2\n, most variants outperform the\nDrive-Anywhere\nbenchmark across OOD scenarios, with\nSPS-50%\nachieving the highest accuracy (+6.2%), highlighting the general effectiveness of stochastic patch selection.\nFigure\n5\nplots performance against selection rate for each variant, revealing distinct trends:\nSPS\npeaks at 50%, offering the strongest gain;\nSPPS\nperforms best at 70% and declines with more aggressive pruning; and\nMSPPS\nimproves as selection becomes stricter, peaking at 30%. These findings confirm that stochastic selection improves generalization across architectures, though the optimal ratio varies. Notably, moderate selection rates strike the best tradeoff between redundancy reduction and information retention in models where spatial structure is retained.\nTable 4\n:\nRuntime Efficiency of SPS.\nThe calculations were averaged across\n100\n100\nindependent runs.\nModel\nQformer\nVision Proj.&Norm\nFM Project\nPolicy\nTotal Efficiency\nQformer\nEfficiency\nVP&N\nEfficiency\nFMC\nEfficiency\nPolicy\nEfficiency\nPercentage\nFactor\nBenchmark\n2.17s\n‚Äì\n871\nŒº\n‚Äã\ns\n\\mu s\n‚Äì\n540\nŒº\n‚Äã\ns\n\\mu s\n‚Äì\n17\nm\n‚Äã\ns\nms\n‚Äì\n100\n%\n\\%\nx1\nSPS_\n70\n%\n70\\%\n1.56s\n39.10\n%\n39.10\\%\n839\nŒº\n‚Äã\ns\n\\mu s\n3.75\n%\n3.75\\%\n376\nŒº\n‚Äã\ns\n\\mu s\n43.67\n%\n43.67\\%\n20\nm\n‚Äã\ns\nms\n0.24\n%\n0.24\\%\n86.52\n%\n86.52\\%\nx1.87\nSPS_\n50\n%\n50\\%\n1.11s\n95.50\n%\n95.50\\%\n855\nŒº\n‚Äã\ns\n\\mu s\n1.88\n%\n1.88\\%\n370\nŒº\n‚Äã\ns\n\\mu s\n46.08\n%\n46.08\\%\n20\nm\n‚Äã\ns\nms\n‚àí\n1.22\n%\n-1.22\\%\n143.45\n%\n143.45\\%\nx2.43\nSPS_\n30\n%\n30\\%\n0.71s\n205.63\n%\n205.63\\%\n842\nŒº\n‚Äã\ns\n\\mu s\n3.45\n%\n3.45\\%\n390\nŒº\n‚Äã\ns\n\\mu s\n38.41\n%\n38.41\\%\n20\nm\n‚Äã\ns\nms\n0.41\n%\n0.41\\%\n247.50\n%\n247.50\\%\nx3.47\nSPPS_\n70\n%\n70\\%\n1.56s\n39.10\n%\n39.10\\%\n846\nŒº\n‚Äã\ns\n\\mu s\n2.93\n%\n2.93\\%\n500\nŒº\n‚Äã\ns\n\\mu s\n8.06\n%\n8.06\\%\n14.3\nm\n‚Äã\ns\nms\n18.92\n%\n18.92\\%\n69.02\n%\n69.02\\%\nx1.69\nSPPS_\n50\n%\n50\\%\n1.11s\n95.50\n%\n95.50\\%\n837\nŒº\n‚Äã\ns\n\\mu s\n4.06\n%\n4.06\\%\n493\nŒº\n‚Äã\ns\n\\mu s\n9.41\n%\n9.41\\%\n14.5\nm\n‚Äã\ns\nms\n17.45\n%\n17.45\\%\n126.41\n%\n126.41\\%\nx2.26\nSPPS_\n30\n%\n30\\%\n0.71s\n205.63\n%\n205.63\\%\n841\nŒº\n‚Äã\ns\n\\mu s\n3.58\n%\n3.58\\%\n542\nŒº\n‚Äã\ns\n\\mu s\n‚àí\n0.44\n%\n-0.44\\%\n13.9\nm\n‚Äã\ns\nms\n22.43\n%\n22.43\\%\n231.21\n%\n231.21\\%\nx3.31\nFigure 5\n:\nAccuracy as a function of selection rate across variants.\n4.6\nReal-world car deployment\nWe deployed the\nSPS-50%\nmodel on a full-scale autonomous vehicle (see supplementary video). The experiments took place during the spring season at two distinct locations: a rural road within a public park and an underground parking garage.\nIn the rural scenario, the vehicle successfully performed lane-following under natural daylight conditions, confirming the model‚Äôs ability to transfer from simulation to the real world. The underground scenario was a challenging OOD test case, featuring low-light conditions, the absence of clear lanes, and visually cluttered geometry, such as perpendicular parking lines, parked vehicles, and distractor objects. Despite never encountering such a configuration at training, the vehicle executed the correct motion.\nIn addition, it successfully avoided static obstacles, including cones and boxes, further demonstrating its robustness. Representative frames from both real-world scenarios, captured from onboard and external views, are shown in Figure\n4\n.\n5\nConclusion and future work\nThis work shows that patch-aligned features (extracted from vision‚Äìlanguage models) used for training driving policies are correlated and redundant. Motivated by this, it introduced SPS: a stochastic patch‚Äìmasking strategy that tackles the redundancy inevitably built into such features. By randomly suppressing a fraction of patch descriptors while preserving their spatial arrangement, SPS forces the policy to ground its decisions on features that are robust to which tokens survive. Across a diverse suite of closed-loop driving scenarios, covering weather, lighting and geographic domain shifts, SPS delivers a\n6.2%\naverage gain in OOD success rate, peaks at\n20.4%\nimprovement in the hardest scenarios, and speeds inference by\n2.4\n√ó\n\\times\ncompared with the previous SOTA.\nThe same mechanism enables text-conditioned data augmentation in the latent patch space, yielding more\n1.7%\nboost without extra image synthesis. Ablations confirm that the benefit is robust to (reasonable) selection rates, selection methods, and feature re-ordering schemes: 8 out of the 9 trained systems surpassed the prior SOTA. We demonstrated that policies trained with SPS in simulation transfer to a real-world autonomous vehicle with\nno\nextra fine-tuning, highlighting the practical relevance of the approach.\nFuture work\nincludes sophisticated selection techniques (i) learning a state-dependent sampling policy that select the number of patches to allocate based on scenes, and (ii) going beyond uniform sampling by\ninspecting\nthe descriptors themselves, e.g., coreset selection, attention entropy, or mutual-information scores, to drop provably redundant patches, further sharpening both efficiency and robustness.\nAcknowledgments\nAlaa Maalouf acknowledges support from the Neubauer Family Foundation and from the MAOF Fellowship of the Council for Higher Education. This work is supported by Toyota Research Institute (TRI) and Capgemini Engineering. It, however, reflects solely the opinions and conclusions of its authors and not TRI or any other Toyota entity.\nReferences\nPomerleau [1988]\nD.¬†A. Pomerleau, ‚ÄúAlvinn: An autonomous land vehicle in a neural network,‚Äù\nAdvances in neural information processing systems\n, vol.¬†1, 1988.\nBojarski et¬†al. [2016]\nM.¬†Bojarski, D.¬†Del¬†Testa, D.¬†Dworakowski, B.¬†Firner, B.¬†Flepp, P.¬†Goyal, L.¬†D. Jackel, M.¬†Monfort, U.¬†Muller, J.¬†Zhang\net¬†al.\n, ‚ÄúEnd to end learning for self-driving cars,‚Äù\narXiv preprint arXiv:1604.07316\n, 2016.\nWang et¬†al. [2024]\nT.-H. Wang, A.¬†Maalouf, W.¬†Xiao, Y.¬†Ban, A.¬†Amini, G.¬†Rosman, S.¬†Karaman, and D.¬†Rus, ‚ÄúDrive anywhere: Generalizable end-to-end autonomous driving with multi-modal foundation models,‚Äù in\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, 2024, pp. 6687‚Äì6694.\nLi et¬†al. [2023]\nJ.¬†Li, D.¬†Li, S.¬†Savarese, and S.¬†Hoi, ‚ÄúBlip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,‚Äù\narXiv preprint arXiv:2301.12597\n, 2023.\nLi et¬†al. [2022a]\nJ.¬†Li, D.¬†Li, C.¬†Xiong, and S.¬†Hoi, ‚ÄúBlip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,‚Äù in\nInternational Conference on Machine Learning\n.‚ÄÇ‚ÄÑ‚ÄäPMLR, 2022, pp. 12‚Äâ888‚Äì12‚Äâ900.\nZhang et¬†al. [2022]\nH.¬†Zhang, F.¬†Li, S.¬†Liu, L.¬†Zhang, H.¬†Su, J.¬†Zhu, L.¬†M. Ni, and H.-Y. Shum, ‚ÄúDino: Detr with improved denoising anchor boxes for end-to-end object detection,‚Äù 2022.\nAmini et¬†al. [2019]\nA.¬†Amini, G.¬†Rosman, S.¬†Karaman, and D.¬†Rus, ‚ÄúVariational end-to-end navigation and localization,‚Äù in\n2019 International Conference on Robotics and Automation (ICRA)\n.‚ÄÇ‚ÄÑ‚ÄäIEEE, 2019, pp. 8958‚Äì8964.\nWang et¬†al. [2023]\nT.-H. Wang, W.¬†Xiao, M.¬†Chahine, A.¬†Amini, R.¬†Hasani, and D.¬†Rus, ‚ÄúLearning stability attention in vision-based end-to-end driving policies,‚Äù in\nLearning for Dynamics and Control Conference\n.‚ÄÇ‚ÄÑ‚ÄäPMLR, 2023, pp. 1099‚Äì1111.\nXiao and Belta [2019]\nW.¬†Xiao and C.¬†Belta, ‚ÄúControl barrier functions for systems with high relative degree,‚Äù in\nProc. of 58th IEEE Conference on Decision and Control\n, Nice, France, 2019, pp. 474‚Äì479.\nXiao et¬†al. [2023]\nW.¬†Xiao, T.-H. Wang, R.¬†Hasani, M.¬†Chahine, A.¬†Amini, X.¬†Li, and D.¬†Rus, ‚ÄúBarriernet: Differentiable control barrier functions for learning of safe robot control,‚Äù\nIEEE Transactions on Robotics\n, 2023.\nKendall et¬†al. [2019]\nA.¬†Kendall, J.¬†Hawke, D.¬†Janz, P.¬†Mazur, D.¬†Reda, J.-M. Allen, V.-D. Lam, A.¬†Bewley, and A.¬†Shah, ‚ÄúLearning to drive in a day,‚Äù in\n2019 International Conference on Robotics and Automation (ICRA)\n.‚ÄÇ‚ÄÑ‚ÄäIEEE, 2019, pp. 8248‚Äì8254.\nAmini et¬†al. [2022]\nA.¬†Amini, T.-H. Wang, I.¬†Gilitschenski, W.¬†Schwarting, Z.¬†Liu, S.¬†Han, S.¬†Karaman, and D.¬†Rus, ‚ÄúVista 2.0: An open, data-driven simulator for multimodal sensing and policy learning for autonomous vehicles,‚Äù in\n2022 International Conference on Robotics and Automation (ICRA)\n.‚ÄÇ‚ÄÑ‚ÄäIEEE, 2022, pp. 2419‚Äì2426.\nDosovitskiy et¬†al. [2017]\nA.¬†Dosovitskiy, G.¬†Ros, F.¬†Codevilla, A.¬†Lopez, and V.¬†Koltun, ‚ÄúCarla: An open urban driving simulator,‚Äù in\nConference on robot learning\n.‚ÄÇ‚ÄÑ‚ÄäPMLR, 2017, pp. 1‚Äì16.\nShah et¬†al. [2018]\nS.¬†Shah, D.¬†Dey, C.¬†Lovett, and A.¬†Kapoor, ‚ÄúAirsim: High-fidelity visual and physical simulation for autonomous vehicles,‚Äù in\nField and Service Robotics: Results of the 11th International Conference\n.‚ÄÇ‚ÄÑ‚ÄäSpringer, 2018, pp. 621‚Äì635.\nTedrake et¬†al. [2019]\nR.¬†Tedrake\net¬†al.\n, ‚ÄúDrake: Model-based design and verification for robotics,‚Äù 2019, software Package, available at\nhttps://drake.mit.edu/\n.\nM√ºller et¬†al. [2018]\nM.¬†M√ºller, A.¬†Dosovitskiy, B.¬†Ghanem, and V.¬†Koltun, ‚ÄúDriving policy transfer via modularity and abstraction,‚Äù\narXiv preprint arXiv:1804.09364\n, 2018.\nToromanoff et¬†al. [2020]\nM.¬†Toromanoff, E.¬†Wirbel, and F.¬†Moutarde, ‚ÄúEnd-to-end model-free reinforcement learning for urban driving using implicit affordances,‚Äù in\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, 2020, pp. 7153‚Äì7162.\nBehl et¬†al. [2020]\nA.¬†Behl, K.¬†Chitta, A.¬†Prakash, E.¬†Ohn-Bar, and A.¬†Geiger, ‚ÄúLabel efficient visual abstractions for autonomous driving,‚Äù in\n2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n.‚ÄÇ‚ÄÑ‚ÄäIEEE, 2020, pp. 2338‚Äì2345.\nTellex et¬†al. [2020]\nS.¬†Tellex, N.¬†Gopalan, H.¬†Kress-Gazit, and C.¬†Matuszek, ‚ÄúRobots that use language,‚Äù\nAnnual Review of Control, Robotics, and Autonomous Systems\n, vol.¬†3, pp. 25‚Äì55, 2020.\nBisk et¬†al. [2020]\nY.¬†Bisk, A.¬†Holtzman, J.¬†Thomason, J.¬†Andreas, Y.¬†Bengio, J.¬†Chai, M.¬†Lapata, A.¬†Lazaridou, J.¬†May, A.¬†Nisnevich\net¬†al.\n, ‚ÄúExperience grounds language,‚Äù\narXiv preprint arXiv:2004.10151\n, 2020.\nAhn et¬†al. [2022]\nM.¬†Ahn, A.¬†Brohan, N.¬†Brown, Y.¬†Chebotar, O.¬†Cortes, B.¬†David, C.¬†Finn, C.¬†Fu, K.¬†Gopalakrishnan, K.¬†Hausman\net¬†al.\n, ‚ÄúDo as i can, not as i say: Grounding language in robotic affordances,‚Äù\narXiv preprint arXiv:2204.01691\n, 2022.\nBrohan et¬†al. [2022]\nA.¬†Brohan, N.¬†Brown, J.¬†Carbajal, Y.¬†Chebotar, J.¬†Dabis, C.¬†Finn, K.¬†Gopalakrishnan, K.¬†Hausman, A.¬†Herzog, J.¬†Hsu\net¬†al.\n, ‚ÄúRt-1: Robotics transformer for real-world control at scale,‚Äù\narXiv preprint arXiv:2212.06817\n, 2022.\nLi et¬†al. [2022b]\nS.¬†Li, X.¬†Puig, C.¬†Paxton, Y.¬†Du, C.¬†Wang, L.¬†Fan, T.¬†Chen, D.-A. Huang, E.¬†Aky√ºrek, A.¬†Anandkumar\net¬†al.\n, ‚ÄúPre-trained language models for interactive decision-making,‚Äù\nAdvances in Neural Information Processing Systems\n, vol.¬†35, pp. 31‚Äâ199‚Äì31‚Äâ212, 2022.\nBarbara and Maalouf [2025]\nM.¬†Barbara and A.¬†Maalouf, ‚ÄúPrompts to summaries: Zero-shot language-guided video summarization,‚Äù\narXiv preprint arXiv:2506.10807\n, 2025.\nHuang et¬†al. [2023]\nC.¬†Huang, O.¬†Mees, A.¬†Zeng, and W.¬†Burgard, ‚ÄúAudio visual language maps for robot navigation,‚Äù\narXiv preprint arXiv:2303.07522\n, 2023.\nDing et¬†al. [2023]\nR.¬†Ding, J.¬†Yang, C.¬†Xue, W.¬†Zhang, S.¬†Bai, and X.¬†Qi, ‚ÄúPla: Language-driven open-vocabulary 3d scene understanding,‚Äù in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2023, pp. 7010‚Äì7019.\nPeng et¬†al. [2023]\nS.¬†Peng, K.¬†Genova, C.¬†Jiang, A.¬†Tagliasacchi, M.¬†Pollefeys, T.¬†Funkhouser\net¬†al.\n, ‚ÄúOpenscene: 3d scene understanding with open vocabularies,‚Äù in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2023, pp. 815‚Äì824.\nChahine et¬†al. [2024]\nM.¬†Chahine, A.¬†Quach, A.¬†Maalouf, T.-H. Wang, and D.¬†Rus, ‚ÄúFlex: End-to-end text-instructed visual navigation with foundation models,‚Äù 2024. [Online]. Available:\nhttps://arxiv.org/abs/2410.13002\nMaalouf et¬†al. [2023]\nA.¬†Maalouf, N.¬†Jadhav, K.¬†M. Jatavallabhula, M.¬†Chahine, D.¬†M. Vogt, R.¬†J. Wood, A.¬†Torralba, and D.¬†Rus, ‚ÄúFollow anything: Open-set detection, tracking, and following in real-time,‚Äù\narXiv preprint arXiv:2308.05737\n, 2023.\nLiu et¬†al. [2023]\nS.¬†Liu, Z.¬†Zeng, T.¬†Ren, F.¬†Li, H.¬†Zhang, J.¬†Yang, C.¬†Li, J.¬†Yang, H.¬†Su, J.¬†Zhu\net¬†al.\n, ‚ÄúGrounding dino: Marrying dino with grounded pre-training for open-set object detection,‚Äù\narXiv preprint arXiv:2303.05499\n, 2023.\nGhiasi et¬†al. [2022]\nG.¬†Ghiasi, X.¬†Gu, Y.¬†Cui, and T.-Y. Lin, ‚ÄúScaling open-vocabulary image segmentation with image-level labels,‚Äù in\nComputer Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part XXXVI\n.‚ÄÇ‚ÄÑ‚ÄäSpringer, 2022, pp. 540‚Äì557.\nLi et¬†al. [2022c]\nB.¬†Li, K.¬†Q. Weinberger, S.¬†Belongie, V.¬†Koltun, and R.¬†Ranftl, ‚ÄúLanguage-driven semantic segmentation,‚Äù\narXiv preprint arXiv:2201.03546\n, 2022.\nChahine et¬†al. [2025]\nM.¬†Chahine, W.¬†Yang, A.¬†Maalouf, J.¬†Siriska, N.¬†Jadhav, D.¬†Vogt, S.¬†Gil, R.¬†Wood, and D.¬†Rus, ‚ÄúDecentralized vision-based autonomous aerial wildlife monitoring,‚Äù\narXiv preprint arXiv:2508.15038\n, 2025.\nRamesh et¬†al. [2021]\nA.¬†Ramesh, M.¬†Pavlov, G.¬†Goh, S.¬†Gray, C.¬†Voss, A.¬†Radford, M.¬†Chen, and I.¬†Sutskever, ‚ÄúZero-shot text-to-image generation,‚Äù in\nInternational Conference on Machine Learning\n.‚ÄÇ‚ÄÑ‚ÄäPMLR, 2021, pp. 8821‚Äì8831.\nCrowson et¬†al. [2022]\nK.¬†Crowson, S.¬†Biderman, D.¬†Kornis, D.¬†Stander, E.¬†Hallahan, L.¬†Castricato, and E.¬†Raff, ‚ÄúVqgan-clip: Open domain image generation and editing with natural language guidance,‚Äù in\nEuropean Conference on Computer Vision\n.‚ÄÇ‚ÄÑ‚ÄäSpringer, 2022, pp. 88‚Äì105.\nPatashnik et¬†al. [2021]\nO.¬†Patashnik, Z.¬†Wu, E.¬†Shechtman, D.¬†Cohen-Or, and D.¬†Lischinski, ‚ÄúStyleclip: Text-driven manipulation of stylegan imagery,‚Äù in\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, 2021, pp. 2085‚Äì2094.\nRamesh et¬†al. [2022]\nA.¬†Ramesh, P.¬†Dhariwal, A.¬†Nichol, C.¬†Chu, and M.¬†Chen, ‚ÄúHierarchical text-conditional image generation with clip latents,‚Äù\narXiv preprint arXiv:2204.06125\n, 2022.\nSreeram et¬†al. [2025]\nS.¬†Sreeram, T.-H. Wang, A.¬†Maalouf, G.¬†Rosman, S.¬†Karaman, and D.¬†Rus, ‚ÄúProbing multimodal llms as world models for driving,‚Äù\nIEEE Robotics and Automation Letters\n, vol.¬†10, no.¬†11, pp. 11‚Äâ403‚Äì11‚Äâ410, 2025.\nKim et¬†al. [2019]\nJ.¬†Kim, T.¬†Misu, Y.-T. Chen, A.¬†Tawari, and J.¬†Canny, ‚ÄúGrounding human-to-vehicle advice for self-driving vehicles,‚Äù in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n, Nov. 2019, pp. 10‚Äâ591‚Äì10‚Äâ599.\nOmeiza et¬†al. [2021]\nD.¬†Omeiza, H.¬†Webb, M.¬†Jirotka, and L.¬†Kunze, ‚ÄúExplanations in autonomous driving: A survey,‚Äù\nIEEE Transactions on Intelligent Transportation Systems\n, vol.¬†23, no.¬†8, pp. 10‚Äâ142‚Äì10‚Äâ162, 2021.\nKuo et¬†al. [2022]\nY.-L. Kuo, X.¬†Huang, A.¬†Barbu, S.¬†G. McGill, B.¬†Katz, J.¬†J. Leonard, and G.¬†Rosman, ‚ÄúTrajectory prediction with linguistic representations,‚Äù in\n2022 International Conference on Robotics and Automation (ICRA)\n.‚ÄÇ‚ÄÑ‚ÄäIEEE, 2022, pp. 2868‚Äì2875.\nTan et¬†al. [2023]\nS.¬†Tan, B.¬†Ivanovic, X.¬†Weng, M.¬†Pavone, and P.¬†Kraehenbuehl, ‚ÄúLanguage conditioned traffic generation,‚Äù\narXiv preprint arXiv:2307.07947\n, 2023.\nZhong et¬†al. [2023]\nZ.¬†Zhong, D.¬†Rempe, Y.¬†Chen, B.¬†Ivanovic, Y.¬†Cao, D.¬†Xu, M.¬†Pavone, and B.¬†Ray, ‚ÄúLanguage-guided traffic simulation via scene-level diffusion,‚Äù\narXiv preprint arXiv:2306.06344\n, 2023.\nCaron et¬†al. [2021]\nM.¬†Caron, H.¬†Touvron, I.¬†Misra, H.¬†J√©gou, J.¬†Mairal, P.¬†Bojanowski, and A.¬†Joulin, ‚ÄúEmerging properties in self-supervised vision transformers,‚Äù in\nProceedings of the IEEE/CVF international conference on computer vision\n, 2021, pp. 9650‚Äì9660.\nOquab et¬†al. [2023]\nM.¬†Oquab, T.¬†Darcet, T.¬†Moutakanni, H.¬†Vo, M.¬†Szafraniec, V.¬†Khalidov, P.¬†Fernandez, D.¬†Haziza, F.¬†Massa, A.¬†El-Nouby\net¬†al.\n, ‚ÄúDinov2: Learning robust visual features without supervision,‚Äù\narXiv preprint arXiv:2304.07193\n, 2023.\nKirillov et¬†al. [2023]\nA.¬†Kirillov, E.¬†Mintun, N.¬†Ravi, H.¬†Mao, C.¬†Rolland, L.¬†Gustafson, T.¬†Xiao, S.¬†Whitehead, A.¬†C. Berg, W.-Y. Lo\net¬†al.\n, ‚ÄúSegment anything,‚Äù\narXiv preprint arXiv:2304.02643\n, 2023.\nZhao et¬†al. [2023]\nX.¬†Zhao, W.¬†Ding, Y.¬†An, Y.¬†Du, T.¬†Yu, M.¬†Li, M.¬†Tang, and J.¬†Wang, ‚ÄúFast segment anything,‚Äù 2023.\nCheng et¬†al. [2022]\nB.¬†Cheng, I.¬†Misra, A.¬†G. Schwing, A.¬†Kirillov, and R.¬†Girdhar, ‚ÄúMasked-attention mask transformer for universal image segmentation,‚Äù in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2022, pp. 1290‚Äì1299.\nRadford et¬†al. [2021]\nA.¬†Radford, J.¬†W. Kim, C.¬†Hallacy, A.¬†Ramesh, G.¬†Goh, S.¬†Agarwal, G.¬†Sastry, A.¬†Askell, P.¬†Mishkin, J.¬†Clark\net¬†al.\n, ‚ÄúLearning transferable visual models from natural language supervision,‚Äù in\nInternational conference on machine learning\n.‚ÄÇ‚ÄÑ‚ÄäPMLR, 2021, pp. 8748‚Äì8763.\nZhong et¬†al. [2022]\nY.¬†Zhong, J.¬†Yang, P.¬†Zhang, C.¬†Li, N.¬†Codella, L.¬†H. Li, L.¬†Zhou, X.¬†Dai, L.¬†Yuan, Y.¬†Li\net¬†al.\n, ‚ÄúRegionclip: Region-based language-image pretraining,‚Äù in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2022, pp. 16‚Äâ793‚Äì16‚Äâ803.\nJatavallabhula et¬†al. [2023]\nK.¬†M. Jatavallabhula, A.¬†Kuwajerwala, Q.¬†Gu, M.¬†Omama, T.¬†Chen, A.¬†Maalouf, S.¬†Li, G.¬†Iyer, S.¬†Saryazdi, N.¬†Keetha\net¬†al.\n, ‚ÄúConceptfusion: Open-set multimodal 3d mapping,‚Äù\narXiv preprint arXiv:2302.07241\n, 2023.\nAmir et¬†al. [2021]\nS.¬†Amir, Y.¬†Gandelsman, S.¬†Bagon, and T.¬†Dekel, ‚ÄúDeep ViT features as dense visual descriptors,‚Äù\narXiv preprint arXiv:2112.05814\n, 2021.\nLevine and Koltun [2013]\nS.¬†Levine and V.¬†Koltun, ‚ÄúGuided policy search,‚Äù in\nInternational conference on machine learning\n.‚ÄÇ‚ÄÑ‚ÄäPMLR, 2013, pp. 1‚Äì9.\nDai et¬†al. [2015]\nJ.¬†Dai, K.¬†He, and J.¬†Sun, ‚ÄúConvolutional feature masking for joint object and stuff segmentation,‚Äù in\n2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n.‚ÄÇ‚ÄÑ‚ÄäIEEE, Jun. 2015.\nAppendix A\nAppendix\nA.1\nPCA and correlation analysis: definitions and rationale\nFeature covariance.\nLet\nF\nc\nF_{c}\nbe an\nN\n√ó\nD\nN\\times D\nmatrix of the\nN\nN\ncomputed patch descriptors. Assume\nF\nc\nF_{c}\nis centered (otherwise do so). Then compute\nŒ£\n=\n1\nN\n‚àí\n1\n‚Äã\nF\nc\n‚ä§\n‚Äã\nF\nc\n‚àà\n‚Ñù\nd\n√ó\nd\n\\Sigma=\\frac{1}{N-1}F_{c}^{\\top}F_{c}\\in\\mathbb{R}^{d\\times d}\n. The total variance equals the trace of\nŒ£\n\\Sigma\n, which is the sum of eigenvalues. The cumulative explained variance curve reports\n‚àë\ni\n=\n1\nm\nŒª\ni\n/\n‚àë\ni\n=\n1\nd\nŒª\ni\n\\sum_{i=1}^{m}\\lambda_{i}\\big/\\sum_{i=1}^{d}\\lambda_{i}\n.\nTop-energy subset.\nLet\nf\nj\nf_{j}\ndenote the computed descriptor for patch\nj\nj\n. Compute\n‚à•\nf\ni\n‚à•\n2\n\\lVert f_{i}\\rVert_{2}\nfor each patch, sort, and keep the top\n128\n128\npatches. Recompute the PCA curve on this subset. Achieving\n90\n%\n90\\%\nvariance with about\n14\n14\ncomponents indicates redundancy even among the strongest tokens.\nWhy feature-space PCA supports patch masking.\nAlthough SPS masks patches rather than feature channels, a low-rank feature manifold implies that many patches project into the same few directions. Removing some rows of\nF\nF\npreserves the principal subspace with high probability, so semantics remain available from the retained tokens. One can also analyze patch-wise covariance\n1\nd\n‚àí\n1\n‚Äã\nF\nc\n‚Äã\nF\nc\n‚ä§\n‚àà\n‚Ñù\nN\n√ó\nN\n\\frac{1}{d-1}F_{c}F_{c}^{\\top}\\in\\mathbb{R}^{N\\times N}\n, which shows many correlated patches, consistent with the qualitative overlays in Appx.\nA.2\n.\nSecond-moment viewpoint.\nThe matrix\nF\nc\n‚ä§\n‚Äã\nF\nc\nF_{c}^{\\top}F_{c}\nis the second-moment (Gram) matrix of descriptors.\nWhen we uniformly sample\nm\nm\nrows without replacement to form\nS\n‚Äã\nF\nc\nSF_{c}\n, the expected Gram matrix of the sampled descriptors satisfies\nùîº\n‚Äã\n[\n(\nS\n‚Äã\nF\nc\n)\n‚ä§\n‚Äã\n(\nS\n‚Äã\nF\nc\n)\n]\n=\nm\nN\n‚Äã\nF\nc\n‚ä§\n‚Äã\nF\nc\n\\mathbb{E}\\big[(SF_{c})^{\\top}(SF_{c})\\big]=\\tfrac{m}{N}\\,F_{c}^{\\top}F_{c}\n.\nThus a quadratic form induced by the sampled rows preserves the full quadratic form in expectation up to the known scale\nm\n/\nN\nm/N\n.\nA.2\nFull grid similarity overlays\nFigure 6\n:\nFull grid of similarity overlays for all\n16\n√ó\n16\n16\\times 16\npatches. Each cell shows the raw image overlaid with the cosine similarity from the corresponding seed patch to all other patches.\nHow to compute it?\nGiven\nF\nc\n‚àà\n‚Ñù\nN\n√ó\nd\nF_{c}\\in\\mathbb{R}^{N\\times d}\nbe the matrix of patch features as before, and let\nf\nj\nf_{j}\ndenote the computed descriptor for patch\nj\nj\n. Choose a seed index/patch\ni\ni\nand define\ns\nj\n=\nf\ni\n‚ãÖ\nf\nj\n‚à•\nf\ni\n‚à•\n‚Äã\n‚à•\nf\nj\n‚à•\ns_{j}=\\frac{f_{i}\\cdot f_{j}}{\\lVert f_{i}\\rVert\\lVert f_{j}\\rVert}\nfor every patch\nj\nj\n. Set\ns\n=\n(\ns\n1\n,\n‚ãØ\n,\ns\nN\n)\ns=(s_{1},\\cdots,s_{N})\n, and reshape\ns\ns\nto form\nH\n‚Ä≤\n√ó\nW\n‚Ä≤\nH^{\\prime}\\times W^{\\prime}\nmatrix, then upsample the matrix to the input resolution, and normalize to\n[\n0\n,\n1\n]\n[0,1]\nand overlay with the original input frame\nF\nF\n.\nInterpretation of Fig.\n6\n.\nBlue denotes low or negative cosine similarity and red denotes high positive similarity. For a given seed patch, only a sparse set of regions are strongly similar (red), while most are weakly or negatively related (blue). As the seed changes across the grid, the red regions shift to locations that share semantics with the new seed, illustrating globally entangled yet redundant representations.\nA.3\nDINO backbone results on Animal OOD scenarios\nWe also evaluate SPS with the DINO backbone (to show generalization across different setups/backbones) and observe consistent gains (Table\n5\n).\nTable 5\n:\nSPS with a DINO backbone on Animal OOD scenarios.\nThese actors were not present in the in-distribution training set. SPS improves DINO in\n6\n6\nof\n7\n7\nscenarios and ties in one.\nScenario\nDINO\nDINO + SPS\nGain\nRSpDDA\n0.89\n0.96\n+\n7\n%\n+7\\%\nRSuDNA\n0.59\n0.65\n+\n6\n%\n+6\\%\nRFDDA\n0.95\n0.97\n+\n2\n%\n+2\\%\nRWSDA\n0.87\n0.92\n+\n5\n%\n+5\\%\nUSuDDA\n0.80\n0.82\n+\n2\n%\n+2\\%\nUSuRDA\n0.83\n0.83\n0\n%\n0\\%\nUSuDNA\n0.86\n0.87\n+\n1\n%\n+1\\%\nAverage\n82.7\n%\n82.7\\%\n86\n%\n86\\%\n+3.3%\nA.4\nProof of Lemma\n1\nProof of Lemma\n1\n.\nStep 0: Thin SVD, projectors, and coherence.\nLet the thin SVD of the centered descriptor matrix be\nF\nc\n=\nU\nr\n‚Äã\nŒ£\nr\n‚Äã\nV\nr\n‚ä§\nF_{c}=U_{r}\\Sigma_{r}V_{r}^{\\top}\n,\nwith\nU\nr\n‚àà\n‚Ñù\nN\n√ó\nr\nU_{r}\\in\\mathbb{R}^{N\\times r}\nand\nV\nr\n‚àà\n‚Ñù\nd\n√ó\nr\nV_{r}\\in\\mathbb{R}^{d\\times r}\nhaving orthonormal columns, and\nŒ£\nr\n‚àà\n‚Ñù\nr\n√ó\nr\n\\Sigma_{r}\\in\\mathbb{R}^{r\\times r}\ndiagonal with positive entries.\nThe projector onto the row space is\nŒ†\nF\n=\nV\nr\n‚Äã\nV\nr\n‚ä§\n\\Pi_{F}=V_{r}V_{r}^{\\top}\n.\nDefine the uniform row coherence\nŒº\n‚âú\nN\nr\n‚Äã\nmax\ni\n‚àà\n[\nN\n]\n‚Å°\n‚Äñ\n(\nU\nr\n)\ni\n:\n‚Äñ\n2\n2\n‚àà\n[\n1\n,\nN\n/\nr\n]\n.\n\\mu\\triangleq\\frac{N}{r}\\,\\max_{i\\in[N]}\\|(U_{r})_{i:}\\|_{2}^{2}\\in[1,\\,N/r].\nFix tolerances\nŒµ\n‚àà\n(\n0\n,\n1\n)\n\\varepsilon\\in(0,1)\nand\nŒ¥\n‚àà\n(\n0\n,\n1\n)\n\\delta\\in(0,1)\n.\nStep 1: Second-moment identity in expectation.\nLet\nS\n‚àà\n{\n0\n,\n1\n}\nm\n√ó\nN\nS\\in\\{0,1\\}^{m\\times N}\nselect\nm\nm\nrows uniformly without replacement, and write\nS\n‚Äã\nF\nc\nSF_{c}\nfor the sampled submatrix.\nThen\nS\n‚ä§\n‚Äã\nS\n=\ndiag\n‚Äã\n(\nc\n1\n,\n‚Ä¶\n,\nc\nN\n)\nS^{\\top}S=\\mathrm{diag}(c_{1},\\dots,c_{N})\nwith\nc\ni\n‚àà\n{\n0\n,\n1\n}\nc_{i}\\in\\{0,1\\}\nindicating whether row\ni\ni\nis selected, so\nùîº\n‚Äã\n[\nS\n‚ä§\n‚Äã\nS\n]\n=\nm\nN\n‚Äã\nI\n\\mathbb{E}[S^{\\top}S]=\\tfrac{m}{N}I\nand\nùîº\n‚Äã\n[\n(\nS\n‚Äã\nF\nc\n)\n‚ä§\n‚Äã\n(\nS\n‚Äã\nF\nc\n)\n]\n=\nF\nc\n‚ä§\n‚Äã\nùîº\n‚Äã\n[\nS\n‚ä§\n‚Äã\nS\n]\n‚Äã\nF\nc\n=\nm\nN\n‚Äã\nF\nc\n‚ä§\n‚Äã\nF\nc\n.\n\\mathbb{E}\\big[(SF_{c})^{\\top}(SF_{c})\\big]=F_{c}^{\\top}\\mathbb{E}[S^{\\top}S]F_{c}=\\tfrac{m}{N}\\,F_{c}^{\\top}F_{c}.\n(8)\nHence the second-moment (Gram) matrix of sampled descriptors preserves the full one in expectation up to the factor\nm\n/\nN\nm/N\n.\nStep 2: Subspace embedding via uniform row sampling.\nBy matrix Chernoff concentration with bounded coherence (standard subspace-embedding results), if\nm\n‚â•\nC\n‚Äã\nŒº\n‚Äã\nr\nŒµ\n2\n‚Äã\nlog\n‚Å°\n(\nr\nŒ¥\n)\n,\nm\\;\\geq\\;C\\,\\frac{\\mu\\,r}{\\varepsilon^{2}}\\,\\log\\!\\Bigl(\\frac{r}{\\delta}\\Bigr),\nthen with probability at least\n1\n‚àí\nŒ¥\n1-\\delta\nwe have the spectral sandwich\n(\n1\n‚àí\nŒµ\n)\n‚Äã\nI\nr\n‚™Ø\nN\nm\n‚Äã\nU\nr\n‚ä§\n‚Äã\nS\n‚ä§\n‚Äã\nS\n‚Äã\nU\nr\n‚™Ø\n(\n1\n+\nŒµ\n)\n‚Äã\nI\nr\n.\n(1-\\varepsilon)\\,I_{r}\\;\\preceq\\;\\frac{N}{m}\\,U_{r}^{\\top}S^{\\top}S\\,U_{r}\\;\\preceq\\;(1+\\varepsilon)\\,I_{r}.\n(9)\nIn particular,\nU\nr\n‚ä§\n‚Äã\nS\n‚ä§\n‚Äã\nS\n‚Äã\nU\nr\nU_{r}^{\\top}S^{\\top}S\\,U_{r}\nis positive definite, so\nS\n‚Äã\nU\nr\nSU_{r}\nhas full column rank\nr\nr\n.\nBy (\n9\n), all eigenvalues of\nN\nm\n‚Äã\nU\nr\n‚ä§\n‚Äã\nS\n‚ä§\n‚Äã\nS\n‚Äã\nU\nr\n\\tfrac{N}{m}\\,U_{r}^{\\top}S^{\\top}S\\,U_{r}\nlie in\n[\n1\n‚àí\nŒµ\n,\n1\n+\nŒµ\n]\n[\\,1-\\varepsilon,\\,1+\\varepsilon\\,]\n,\nhence\nU\nr\n‚ä§\n‚Äã\nS\n‚ä§\n‚Äã\nS\n‚Äã\nU\nr\n‚âª\n0\nU_{r}^{\\top}S^{\\top}S\\,U_{r}\\succ 0\n.\nPositive definiteness implies that for any\nx\n‚â†\n0\nx\\neq 0\n,\nx\n‚ä§\n‚Äã\nU\nr\n‚ä§\n‚Äã\nS\n‚ä§\n‚Äã\nS\n‚Äã\nU\nr\n‚Äã\nx\n>\n0\nx^{\\top}U_{r}^{\\top}S^{\\top}S\\,U_{r}x>0\n; therefore\nS\n‚Äã\nU\nr\n‚Äã\nx\n‚â†\n0\nSU_{r}x\\neq 0\nand\nrank\n‚Äã\n(\nS\n‚Äã\nU\nr\n)\n=\nr\n\\mathrm{rank}(SU_{r})=r\n.\nThis means the sampled rows still span an\nr\nr\n-dimensional subspace in\n‚Ñù\nN\n\\mathbb{R}^{N}\n.\nStep 3: Consequences for the row space.\nUsing\nF\nc\n=\nU\nr\n‚Äã\nŒ£\nr\n‚Äã\nV\nr\n‚ä§\nF_{c}=U_{r}\\Sigma_{r}V_{r}^{\\top}\n,\n(\nS\n‚Äã\nF\nc\n)\n‚ä§\n‚Äã\n(\nS\n‚Äã\nF\nc\n)\n\\displaystyle(SF_{c})^{\\top}(SF_{c})\n=\nV\nr\n‚Äã\nŒ£\nr\n‚Äã\n(\nU\nr\n‚ä§\n‚Äã\nS\n‚ä§\n‚Äã\nS\n‚Äã\nU\nr\n)\n‚Äã\nŒ£\nr\n‚Äã\nV\nr\n‚ä§\n\\displaystyle=\\;V_{r}\\,\\Sigma_{r}\\,\\bigl(U_{r}^{\\top}S^{\\top}S\\,U_{r}\\bigr)\\,\\Sigma_{r}\\,V_{r}^{\\top}\n(10)\n=\nV\nr\n‚Äã\n[\nŒ£\nr\n‚Äã\n(\nU\nr\n‚ä§\n‚Äã\nS\n‚ä§\n‚Äã\nS\n‚Äã\nU\nr\n)\n‚Äã\nŒ£\nr\n]\n‚Äã\nV\nr\n‚ä§\n.\n\\displaystyle=\\;V_{r}\\bigl[\\Sigma_{r}(U_{r}^{\\top}S^{\\top}S\\,U_{r})\\Sigma_{r}\\bigr]V_{r}^{\\top}.\nSince\nU\nr\n‚ä§\n‚Äã\nS\n‚ä§\n‚Äã\nS\n‚Äã\nU\nr\nU_{r}^{\\top}S^{\\top}S\\,U_{r}\nis\nr\n√ó\nr\nr\\times r\npositive definite,\nthe nonzero eigenvectors of\n(\nS\n‚Äã\nF\nc\n)\n‚ä§\n‚Äã\n(\nS\n‚Äã\nF\nc\n)\n(SF_{c})^{\\top}(SF_{c})\nare the columns of\nV\nr\nV_{r}\n, and the corresponding eigenvalues are those of\nŒ£\nr\n‚Äã\n(\nU\nr\n‚ä§\n‚Äã\nS\n‚ä§\n‚Äã\nS\n‚Äã\nU\nr\n)\n‚Äã\nŒ£\nr\n\\Sigma_{r}(U_{r}^{\\top}S^{\\top}S\\,U_{r})\\Sigma_{r}\n.\nThus\n(\nS\n‚Äã\nF\nc\n)\n‚ä§\n‚Äã\n(\nS\n‚Äã\nF\nc\n)\n(SF_{c})^{\\top}(SF_{c})\nhas the same nonzero eigenvectors as\nV\nr\nV_{r}\n, whose columns span\nspan\n‚Äã\n(\nV\nr\n)\n\\mathrm{span}(V_{r})\n.\nConsequently, the range of\n(\nS\n‚Äã\nF\nc\n)\n‚ä§\n‚Äã\n(\nS\n‚Äã\nF\nc\n)\n(SF_{c})^{\\top}(SF_{c})\nequals\nspan\n‚Äã\n(\nV\nr\n)\n\\mathrm{span}(V_{r})\n, so the row space of\nS\n‚Äã\nF\nc\nSF_{c}\ncoincides with that of\nF\nc\nF_{c}\n, and the orthogonal projectors satisfy\nŒ†\nS\n‚Äã\nF\nc\n=\nŒ†\nF\n\\Pi_{SF_{c}}=\\Pi_{F}\n.\nThis is stronger than the bound\n‚Äñ\nŒ†\nF\n‚àí\nŒ†\nS\n‚Äã\nF\nc\n‚Äñ\n2\n‚â§\nŒµ\n\\|\\Pi_{F}-\\Pi_{SF_{c}}\\|_{2}\\leq\\varepsilon\n, which follows immediately.\nCombining Steps 1‚Äì3 proves the lemma under the stated sample complexity.\n‚àé\nRemark 1 (Lipschitz stability).\nLet\nœï\n\\phi\nbe a row-wise Lipschitz map with constant\nL\nL\n(for example, a per-token linear map followed by a 1-Lipschitz normalization). Let\nœï\n\\phi\nact independently on each row. Then\n‚Äñ\nœï\n‚Äã\n(\nS\n‚Äã\nF\nc\n)\n‚àí\nœï\n‚Äã\n(\nF\nc\n)\n‚Äñ\nF\n2\n\\displaystyle\\|\\phi(SF_{c})-\\phi(F_{c})\\|_{F}^{2}\n=\n‚àë\ni\n=\n1\nN\n‚Äñ\nœï\n‚Äã\n(\n(\nS\n‚Äã\nF\nc\n)\ni\n:\n)\n‚àí\nœï\n‚Äã\n(\n(\nF\nc\n)\ni\n:\n)\n‚Äñ\n2\n2\n\\displaystyle=\\sum_{i=1}^{N}\\|\\phi((SF_{c})_{i:})-\\phi((F_{c})_{i:})\\|_{2}^{2}\n(11)\n‚â§\n‚àë\ni\n=\n1\nN\nL\n2\n‚Äã\n‚Äñ\n(\nS\n‚Äã\nF\nc\n)\ni\n:\n‚àí\n(\nF\nc\n)\ni\n:\n‚Äñ\n2\n2\n\\displaystyle\\leq\\sum_{i=1}^{N}L^{2}\\|(SF_{c})_{i:}-(F_{c})_{i:}\\|_{2}^{2}\n=\nL\n2\n‚Äã\n‚Äñ\nS\n‚Äã\nF\nc\n‚àí\nF\nc\n‚Äñ\nF\n2\n,\n\\displaystyle=L^{2}\\|SF_{c}-F_{c}\\|_{F}^{2},\nso\n‚Äñ\nœï\n‚Äã\n(\nS\n‚Äã\nF\nc\n)\n‚àí\nœï\n‚Äã\n(\nF\nc\n)\n‚Äñ\nF\n‚â§\nL\n‚Äã\n‚Äñ\nS\n‚Äã\nF\nc\n‚àí\nF\nc\n‚Äñ\nF\n\\|\\phi(SF_{c})-\\phi(F_{c})\\|_{F}\\leq L\\,\\|SF_{c}-F_{c}\\|_{F}\n.\nStandard bounded-difference or matrix Bernstein arguments for sampling without replacement yield concentration of this deviation as\nm\nm\ngrows.\nRemark 2 (Unbiased quadratic forms and covariance).\nFor any per-token linear map\nœï\n‚Äã\n(\nX\n)\n=\nX\n‚Äã\nW\n\\phi(X)=XW\n,\nùîº\n‚Äã\n[\n(\n(\nS\n‚Äã\nF\nc\n)\n‚Äã\nW\n)\n‚ä§\n‚Äã\n(\n(\nS\n‚Äã\nF\nc\n)\n‚Äã\nW\n)\n]\n=\nm\nN\n‚Äã\n(\nF\nc\n‚Äã\nW\n)\n‚ä§\n‚Äã\n(\nF\nc\n‚Äã\nW\n)\n.\n\\mathbb{E}\\big[((SF_{c})W)^{\\top}((SF_{c})W)\\big]=\\tfrac{m}{N}\\,(F_{c}W)^{\\top}(F_{c}W).\n(12)\nThus quadratic objectives and covariances computed from the sampled rows preserve the full counterparts in expectation up to a known scale\nm\n/\nN\nm/N\n.\nOne can reweight by\nN\n/\nm\nN/m\n, or rely on normalization layers to absorb the scale.\nCovariance remark.\nLet\nŒ£\n^\nm\n‚âú\n1\nm\n‚àí\n1\n‚Äã\n(\nS\n‚Äã\nF\nc\n)\n‚ä§\n‚Äã\n(\nS\n‚Äã\nF\nc\n)\n\\widehat{\\Sigma}_{m}\\triangleq\\tfrac{1}{m-1}(SF_{c})^{\\top}(SF_{c})\nbe the usual unbiased sample covariance built from the sampled rows, and\nŒ£\n^\nN\n‚âú\n1\nN\n‚àí\n1\n‚Äã\nF\nc\n‚ä§\n‚Äã\nF\nc\n\\widehat{\\Sigma}_{N}\\triangleq\\tfrac{1}{N-1}F_{c}^{\\top}F_{c}\nthe full-sample covariance.\nThen\nùîº\n‚Äã\n[\nŒ£\n^\nm\n]\n\\displaystyle\\mathbb{E}\\big[\\widehat{\\Sigma}_{m}\\big]\n=\n1\nm\n‚àí\n1\n‚Äã\nùîº\n‚Äã\n[\n(\nS\n‚Äã\nF\nc\n)\n‚ä§\n‚Äã\n(\nS\n‚Äã\nF\nc\n)\n]\n=\nm\nN\n‚Äã\n(\nm\n‚àí\n1\n)\n‚Äã\nF\nc\n‚ä§\n‚Äã\nF\nc\n\\displaystyle=\\frac{1}{m-1}\\,\\mathbb{E}\\big[(SF_{c})^{\\top}(SF_{c})\\big]=\\frac{m}{N(m-1)}\\,F_{c}^{\\top}F_{c}\n(13)\n=\nN\n‚àí\n1\nN\n‚ãÖ\nm\nm\n‚àí\n1\n‚Äã\nŒ£\n^\nN\n.\n\\displaystyle=\\frac{N-1}{N}\\cdot\\frac{m}{m-1}\\;\\widehat{\\Sigma}_{N}.\nThe factor\nŒ±\n‚âú\nN\n‚àí\n1\nN\n‚ãÖ\nm\nm\n‚àí\n1\n\\alpha\\triangleq\\tfrac{N-1}{N}\\cdot\\tfrac{m}{m-1}\nis close to\n1\n1\nwhenever\nm\nm\nand\nN\nN\nare moderate to large.\nFor\nm\n=\nN\n/\n2\nm=N/2\n,\nŒ±\n=\nN\n‚àí\n1\nN\n‚ãÖ\nN\nN\n‚àí\n2\n=\nN\n‚àí\n1\nN\n‚àí\n2\n=\n1\n+\nO\n‚Äã\n(\n1\n/\nN\n)\n\\alpha=\\tfrac{N-1}{N}\\cdot\\tfrac{N}{N-2}=\\tfrac{N-1}{\\,N-2\\,}=1+O(1/N)\n.\nHence the covariance is preserved in expectation up to a negligible finite-sample correction.\nIn practice, either explicit reweighting (\nN\n/\nm\nN/m\n) or standard normalization layers absorb any global scale, and the subspace preservation of Steps¬†2‚Äì3 is unaffected by scale.\nA.5\nSelf Attensin Causes the Patch Redundancy\nSelf-attention mixing.\nGiven a patch embedding matrix\nX\n‚àà\n‚Ñù\nN\n√ó\nD\nX\\in\\mathbb{R}^{N\\times D}\n, a transformer layer forms\nQ\n=\nX\n‚Äã\nW\nQ\nQ=XW^{Q}\n,\nK\n=\nX\n‚Äã\nW\nK\nK=XW^{K}\n, and\nV\n=\nX\n‚Äã\nW\nV\nV=XW^{V}\nwith learned projections.\nThe attention logits are\nG\n=\nQ\n‚Äã\nK\n‚ä§\nG=QK^{\\top}\n, and the token update is\nY\n=\nSoftMax\n(\nG\n)\nV\n‚àà\n‚Ñù\nN\n√ó\nD\n.\nY\\;=\\;\\texttt{{SoftMax}}(G)\\,V\\quad\\in\\;\\mathbb{R}^{N\\times D}.\n(14)\nHence, each output token\nY\ni\nY_{i}\nis a convex combination of all value vectors\nV\nj\nV_{j}\nthat are weighted by content similarity through the softmax of\nQ\ni\n‚Äã\nK\nj\n‚ä§\nQ_{i}K_{j}^{\\top}\n. This induces global mixing of information across tokens, so the descriptor at each spatial location carries scene-wide context.",
  "preview_text": "Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.\n\nSee Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection\nAmir Mallak\n1\nErfan Aasi\n2\nShiva Sreeram\n2\nTsun-Hs",
  "is_relevant": true,
  "relevance_score": 6.0,
  "extracted_keywords": [
    "Reinforcement Learning",
    "VLA",
    "diffusion",
    "Flow Matching",
    "locomotion",
    "VLM",
    "whole body control"
  ],
  "one_line_summary": "ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂü∫Á°ÄÊ®°ÂûãÁöÑÈöèÊú∫Ë°•‰∏ÅÈÄâÊã©ÊñπÊ≥ïÔºåÁî®‰∫éÊèêÈ´òÁ´ØÂà∞Á´ØËá™Âä®È©æÈ©∂Á≠ñÁï•ÁöÑÊ≥õÂåñÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºå‰ΩÜ‰∏éÂº∫ÂåñÂ≠¶‰π†„ÄÅVLA„ÄÅÊâ©Êï£Ê®°Âûã„ÄÅFlow Matching„ÄÅËøêÂä®ÊéßÂà∂„ÄÅVLMÂíåÂÖ®Ë∫´ÊéßÂà∂Á≠âÂÖ≥ÈîÆËØçÁöÑÁõ¥Êé•Áõ∏ÂÖ≥ÊÄßÊúâÈôê„ÄÇ",
  "detailed_summary": "## ËÆ∫ÊñáÊëòË¶Å\n\n**ËÆ∫ÊñáÊ†áÈ¢òÔºö** „ÄäÁúãÂæóÊõ¥Â∞ëÔºåÂºÄÂæóÊõ¥Â•ΩÔºöÈÄöËøáÂü∫Á°ÄÊ®°ÂûãÈöèÊú∫ÂùóÈÄâÊã©ÂÆûÁé∞ÂèØÊ≥õÂåñÁöÑÁ´ØÂà∞Á´ØËá™Âä®È©æÈ©∂„Äã\n\n### 1. Á†îÁ©∂ËÉåÊôØÂíåÂä®Êú∫\nÈöèÁùÄÁ´ØÂà∞Á´ØËá™Âä®È©æÈ©∂ÊäÄÊúØÁöÑÂèëÂ±ïÔºåÂà©Áî®Â§ßÂûãÂü∫Á°ÄÊ®°ÂûãÊèêÂèñ**ÂùóÂØπÈΩêÁâπÂæÅ**Êù•ËÆ≠ÁªÉËΩªÈáèÁ∫ßÁ≠ñÁï•Â∑≤Êàê‰∏∫ÊèêÂçáÊ®°Âûã**ÂàÜÂ∏ÉÂ§ñÊ≥õÂåñËÉΩÂäõ**ÁöÑÊúâÊïàÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éËßÜËßâTransformer‰∏≠ÁöÑ**Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂**ÔºåÊØè‰∏™ÂõæÂÉèÂùóÁöÑÁâπÂæÅÈÉΩÈöêÂºèÂú∞ÂåÖÂê´‰∫ÜÊù•Ëá™ÂÖ∂‰ªñÊâÄÊúâÂùóÁöÑ‰ø°ÊÅØÔºåÂØºËá¥Ëøô‰∫õÂùóÁ∫ßÁâπÂæÅÊèèËø∞Á¨¶‰πãÈó¥Â≠òÂú®**È´òÂ∫¶ÂÜó‰ΩôÂíåÂº∫Áõ∏ÂÖ≥ÊÄß**„ÄÇËøôÁßçÂÜó‰Ωô‰∏ç‰ªÖÊµ™Ë¥πËÆ°ÁÆóËµÑÊ∫êÔºåÊõ¥‰ºöÂØºËá¥Á≠ñÁï•ÁΩëÁªúÂ≠¶‰π†Âà∞ËôöÂÅáÁöÑÁõ∏ÂÖ≥ÊÄßÔºå‰ªéËÄåÊçüÂÆ≥ÂÖ∂Âú®Êú™ËßÅËøáÁöÑÂú∫ÊôØÔºàOODÔºâ‰∏≠ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ\n\n### 2. Ê†∏ÂøÉÊñπÊ≥ïÂíåÊäÄÊúØÂàõÊñ∞\nÊú¨ÊñáÊèêÂá∫‰∫Ü**ÈöèÊú∫ÂùóÈÄâÊã©**ÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÔºöÂú®ÊØè‰∏ÄÂ∏ßÂõæÂÉèÂ§ÑÁêÜÊó∂Ôºå**ÈöèÊú∫‰∏¢ÂºÉ‰∏ÄÂÆöÊØî‰æã**ÁöÑÂùóÁâπÂæÅÊèèËø∞Á¨¶Ôºå‰ªÖÂ∞ÜÂâ©‰ΩôÁöÑÁâπÂæÅÔºà‰øùÊåÅÂÖ∂ÂéüÂßãÁ©∫Èó¥Â∏ÉÂ±ÄÔºâËæìÂÖ•Áªô‰∏ãÊ∏∏ÁöÑÁ≠ñÁï•ÁΩëÁªú„ÄÇSPSÈÄöËøáÂºïÂÖ•ËøôÁßç**ÈöèÊú∫‰ΩÜÂÆåÊï¥ÁöÑÂú∫ÊôØËßÜÂõæ**ÔºåËø´‰ΩøÁ≠ñÁï•Â≠¶‰π†Âü∫‰∫éÈÇ£‰∫õÂØπÂÖ∑‰ΩìÂì™‰∫õÂùóË¢´‰øùÁïô**‰∏çÊïèÊÑü**ÁöÑ„ÄÅÊõ¥Êú¨Ë¥®ÁöÑÁâπÂæÅÔºå‰ªéËÄåÊèêÂçáÊ≥õÂåñËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÊòØ‰∏Ä‰∏™**Âç≥ÊèíÂç≥Áî®**ÁöÑÊ®°ÂùóÔºåÊó†ÈúÄÂæÆË∞ÉÂü∫Á°ÄÊ®°ÂûãÔºåÂπ∂ËÉΩÊòæËëóÂáèÂ∞ëËÆ°ÁÆóÈáè„ÄÇ\n\n### 3. ‰∏ªË¶ÅÂÆûÈ™åÁªìÊûú\nÂú®ÂπøÊ≥õÁöÑÈó≠ÁéØÈ©æÈ©∂ÂÆûÈ™å‰∏≠ÔºåSPSÊñπÊ≥ïÂú®**ÊâÄÊúâOODÂú∫ÊôØ**‰∏ãÂùáË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÔºö\n*   **ÊÄßËÉΩÊèêÂçá**ÔºöÂπ≥ÂùáÊàêÂäüÁéáÊèêÂçá **6.2%**ÔºåÂú®ÊúÄÂÖ∑ÊåëÊàòÊÄßÁöÑÂú∫ÊôØ‰∏ãÊèêÂçáÈ´òËææ **20.4%**„ÄÇ\n*   **ÊïàÁéáÊèêÂçá**ÔºöÊé®ÁêÜÈÄüÂ∫¶ÊèêÂçá **2.4ÂÄç**„ÄÇ\n*   **È≤ÅÊ£íÊÄßÈ™åËØÅ**ÔºöÂØπ9Áßç‰∏çÂêåÂèò‰ΩìÔºà‰∏çÂêåÊé©Á†ÅÁéá„ÄÅÈáçÁªÑÁ≠ñÁï•ÔºâÁöÑÊ∂àËûçÂÆûÈ™å‰∏≠ÔºåÊúâ8ÁßçË∂ÖË∂ä‰∫ÜÂÖàÂâçÁöÑÊúÄ‰Ω≥ÊñπÊ≥ï„ÄÇ\n*   **‰ªøÁúüÂà∞Áé∞ÂÆûËøÅÁßª**ÔºöÂú®‰ªøÁúü‰∏≠ËÆ≠ÁªÉÁöÑSPSÁ≠ñÁï•ËÉΩÂ§ü**Êó†ÁºùËøÅÁßª**Âà∞ÁúüÂÆû‰∏ñÁïåÁöÑÁâ©ÁêÜËΩ¶ËæÜ‰∏äÔºåÊó†ÈúÄ‰ªª‰ΩïË∞É‰ºò„ÄÇ\n\n### 4. Á†îÁ©∂ÊÑè‰πâÂíå‰ª∑ÂÄº\nÊú¨Á†îÁ©∂Êè≠Á§∫‰∫ÜÂü∫‰∫éÂü∫Á°ÄÊ®°ÂûãÁöÑËá™Âä®È©æÈ©∂ÁâπÂæÅÊèêÂèñ‰∏≠Â≠òÂú®ÁöÑÂÜó‰ΩôÈóÆÈ¢òÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÈ´òÊïàÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇSPS‰∏ç‰ªÖÈÄöËøá**‰∏ªÂä®ÂºïÂÖ•ÈöèÊú∫ÊÄß**Êù•ÊèêÂçáÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄßÔºåËøòÈÄöËøá**ÂáèÂ∞ëËÆ°ÁÆóÂÜó‰Ωô**ÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊïàÁéáÊèêÂçá„ÄÇÂÖ∂‚ÄúÂç≥ÊèíÂç≥Áî®‚ÄùÁöÑÁâπÊÄß‰ΩøÂÖ∂Êòì‰∫éÈõÜÊàêÂà∞Áé∞ÊúâÊ°ÜÊû∂‰∏≠„ÄÇËøôÈ°πÂ∑•‰Ωú‰∏∫ÊûÑÂª∫Êõ¥È´òÊïà„ÄÅÊõ¥ÂèØÈù†ÁöÑÁ´ØÂà∞Á´ØËá™Âä®È©æÈ©∂Á≥ªÁªüÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑ØÔºåÂπ∂ËØÅÊòé‰∫ÜÂú®ÁâπÂæÅÂ±ÇÈù¢‚ÄúÂ∞ëÂç≥ÊòØÂ§ö‚ÄùÁöÑËÆæËÆ°Âì≤Â≠¶„ÄÇ",
  "qa_pairs": [
    {
      "question": "ËøôÁØáËÆ∫ÊñáÁöÑÊ†∏ÂøÉÂàõÊñ∞ÁÇπÊòØ‰ªÄ‰πàÔºå‰ªñÊÉ≥Ëß£ÂÜ≥‰ªÄ‰πàÈóÆÈ¢òÔºåÊÄé‰πàËß£ÂÜ≥ÁöÑÔºü",
      "answer": "## ËÆ∫ÊñáÊ†∏ÂøÉÂàÜÊûê\n\n### **Ê†∏ÂøÉÈóÆÈ¢ò**\nËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥**Á´ØÂà∞Á´ØËá™Âä®È©æÈ©∂‰∏≠ÔºåÂü∫‰∫éÂü∫Á°ÄÊ®°ÂûãÊèêÂèñÁöÑ‚ÄúÂùóÂØπÈΩêÁâπÂæÅ‚ÄùÂ≠òÂú®È´òÂ∫¶ÂÜó‰Ωô**ÁöÑÈóÆÈ¢ò„ÄÇËøôÁßçÂÜó‰ΩôÂØºËá¥Á≠ñÁï•ÁΩëÁªúÂÆπÊòìËøáÊãüÂêàÂà∞ËôöÂÅáÁöÑÁõ∏ÂÖ≥ÊÄß‰∏äÔºå‰ªéËÄå‰∏•ÈáçÊçüÂÆ≥‰∫ÜÊ®°ÂûãÂú®**ÂàÜÂ∏ÉÂ§ñÔºàOODÔºâÂú∫ÊôØ‰∏ãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ**„ÄÇ\n\n### **Ê†∏ÂøÉÂàõÊñ∞ÁÇπÔºöÈöèÊú∫ÂùóÈÄâÊã©**\nËÆ∫ÊñáÊèêÂá∫‰∫Ü **Stochastic Patch Selection** ÊñπÊ≥ïÔºåËøôÊòØ‰∏Ä‰∏™**ÁÆÄÂçï„ÄÅÈ´òÊïà‰∏îÂç≥ÊèíÂç≥Áî®**ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ\n\n**Ê†∏ÂøÉÊÄùÊÉ≥**ÔºöÂú®ÊØè‰∏ÄÂ∏ßËæìÂÖ•Êó∂Ôºå**ÈöèÊú∫‰∏¢ÂºÉÔºàÊé©Á†ÅÔºâ‰∏ÄÈÉ®ÂàÜÂõæÂÉèÂùóÁöÑÁâπÂæÅÊèèËø∞Á¨¶**ÔºåÂè™Â∞ÜÂâ©‰ΩôÁöÑÁâπÂæÅÔºà‰øùÊåÅÂÖ∂ÂéüÂßãÁ©∫Èó¥Â∏ÉÂ±ÄÔºâËæìÂÖ•Áªô‰∏ãÊ∏∏ÁöÑÁ≠ñÁï•ÁΩëÁªúËøõË°åËÆ≠ÁªÉÂíåÊé®ÁêÜ„ÄÇ\n\n### **Â¶Ç‰ΩïËß£ÂÜ≥**\nSPS ÈÄöËøá‰∏ÄÁßç‚Äú‰ª•ÈÄÄ‰∏∫Ëøõ‚ÄùÁöÑÊú∫Âà∂Ôºå‰ªé‰∏â‰∏™Â±ÇÈù¢Ëß£ÂÜ≥‰∫ÜÈóÆÈ¢òÔºö\n\n1.  **Âº∫Âà∂Â≠¶‰π†‰∏çÂèòÊÄß**ÔºöÈÄöËøá‰∏∫Âêå‰∏ÄÂú∫ÊôØÊèê‰æõ‰∏çÂêåÁöÑ„ÄÅÈöèÊú∫ÁöÑ‰ΩÜÂÆåÊï¥ÁöÑ‚ÄúÂ≠êËßÜÂõæ‚ÄùÔºåËø´‰ΩøÁ≠ñÁï•ÁΩëÁªúÂøÖÈ°ªÂü∫‰∫éÈÇ£‰∫õ**‰∏çÈöèÂÖ∑‰ΩìÂì™‰∫õÂùóË¢´‰øùÁïôËÄåÂèòÂåñÁöÑ„ÄÅÊõ¥Êú¨Ë¥®ÁöÑÁâπÂæÅ**Êù•ÂÅöÂÜ≥Á≠ñ„ÄÇËøôÁ±ª‰ºº‰∫é‰∏ÄÁßçÂº∫Êï∞ÊçÆÂ¢ûÂº∫ÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ\n2.  **ÊâìÁ†¥ÂÜó‰Ωô‰∏éËôöÂÅáÂÖ≥ËÅî**ÔºöÈöèÊú∫‰∏¢ÂºÉÈ´òÂ∫¶Áõ∏ÂÖ≥ÁöÑÂùóÁâπÂæÅÔºåÂàáÊñ≠‰∫ÜÁ≠ñÁï•ÁΩëÁªúÂèØËÉΩ‰æùËµñÁöÑËôöÂÅáÁªüËÆ°ÂÖ≥ËÅîÔºåÈºìÂä±ÂÖ∂Â≠¶‰π†ÁúüÊ≠£ÂÖ∑ÊúâÂõ†ÊûúÂÖ≥Á≥ªÁöÑ„ÄÅÁ©∫Èó¥‰∏ä grounded ÁöÑÁ∫øÁ¥¢„ÄÇ\n3.  **ÊèêÂçáËÆ°ÁÆóÊïàÁéá**ÔºöÁî±‰∫éÂè™ÈúÄ‰∏∫Âü∫Á°ÄÊ®°Âûã‰∏≠Ë¢´ÈÄâ‰∏≠ÁöÑÂùóËÆ°ÁÆóÁâπÂæÅÔºå**ÊòæËëóÂáèÂ∞ë‰∫ÜÂâçÂêë‰º†Êí≠ÁöÑËÆ°ÁÆóÈáè**„ÄÇËÆ∫Êñá‰∏≠ÊúÄ‰Ω≥Ê®°ÂûãÂÆûÁé∞‰∫Ü **2.4ÂÄç** ÁöÑÂä†ÈÄü„ÄÇ\n\n### **ÊäÄÊúØÂÆûÁé∞Ë¶ÅÁÇπ**\n- **Âü∫Á°Ä**ÔºöÊ≤øÁî®ÂÖàÂâçÂ∑•‰ΩúÔºàDrive AnywhereÔºâÁöÑÊñπÊ≥ïÔºå‰ΩøÁî® BLIP-2 Á≠âÂü∫Á°ÄÊ®°ÂûãÔºåÈÄöËøá**Êé©Á†ÅÊ≥®ÊÑèÂäõ**Êú∫Âà∂ÊèêÂèñÊØè‰∏™ÂõæÂÉèÂùóÁöÑËßÜËßâ-ËØ≠Ë®ÄÂØπÈΩêÁâπÂæÅ„ÄÇ\n- **ÂÖ≥ÈîÆÊìç‰Ωú**ÔºöÂú®ÁâπÂæÅÊèêÂèñÂêé„ÄÅÁ≠ñÁï•ÁΩëÁªúÂâçÔºåÂºïÂÖ• SPS Ê®°Âùó„ÄÇÂØπÊØè‰∏™ËÆ≠ÁªÉ/Êé®ÁêÜÂ∏ßÔºö\n    - ‰ª•Âõ∫ÂÆöÊØî‰æãÔºàÂ¶Ç 50%ÔºâÈöèÊú∫ÈÄâÊã©‰∏ÄÊâπÂùóÁ¥¢Âºï„ÄÇ\n    - ‰ªÖËÆ°ÁÆóËøô‰∫õÈÄâ‰∏≠ÂùóÁöÑÊèèËø∞Á¨¶„ÄÇ\n    - ÊûÑÂª∫‰∏Ä‰∏™‰∏éÂéüÂßãÁ©∫Èó¥ÁΩëÊ†ºÂêåÂ∞∫ÂØ∏ÁöÑÂº†ÈáèÔºåÈÄâ‰∏≠‰ΩçÁΩÆÂ°´ÂÖÖËÆ°ÁÆóÂá∫ÁöÑÁâπÂæÅÔºåÊú™ÈÄâ‰∏≠‰ΩçÁΩÆÂ°´ÂÖÖÈõ∂ÔºàÊàñÁõ¥Êé•ÁßªÈô§Âπ∂Ë∞ÉÊï¥‰ΩçÁΩÆÁºñÁ†ÅÔºâ„ÄÇ\n- **ÁêÜËÆ∫ÊîØÊíë**ÔºöËÆ∫ÊñáÈÄöËøá PCA ÂíåÂùóÈó¥Áõ∏‰ººÊÄßÂàÜÊûê**ÈáèÂåñ‰∫ÜÁâπÂæÅÂÜó‰Ωô**Ôºà‰æãÂ¶ÇÔºå90%ÁöÑÊñπÂ∑Æ‰ªÖÁî± 17/64 ‰∏™‰∏ªÊàêÂàÜËß£ÈáäÔºâÔºåÂπ∂‰ªéÂ≠êÁ©∫Èó¥‰øùÊåÅÁöÑËßíÂ∫¶ËØÅÊòé‰∫ÜÈöèÊú∫ÈááÊ†∑ÁöÑÂêàÁêÜÊÄß„ÄÇ\n- **Âèò‰Ωì‰∏éÊ∂àËûç**ÔºöÊé¢Á¥¢‰∫ÜÂõ∫ÂÆöÊï∞Èáè vs. Ê¶ÇÁéáÈòàÂÄºÈááÊ†∑„ÄÅ‰øùÁïôÈõ∂ÂÄº vs. ÁßªÈô§‰ª§ÁâåÁ≠â‰∏çÂêåÂèò‰ΩìÔºåÂÖ±ËÆ≠ÁªÉ‰∫Ü9‰∏™Á≥ªÁªüÔºåÂÖ∂‰∏≠8‰∏™Ë∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑSOTA„ÄÇ\n\n### **ÂÆûÈôÖ‰ª∑ÂÄº**\n- **ÊÄßËÉΩÊòæËëóÊèêÂçá**ÔºöÂú®ÂπøÊ≥õÁöÑOODÊµãËØïÂú∫ÊôØÔºà‰∏çÂêåÂ≠£ËäÇ„ÄÅÂ§©Ê∞î„ÄÅÂÖâÁÖß„ÄÅÂä®ÊÄÅÁâ©‰ΩìÔºâ‰∏≠ÔºåÂπ≥ÂùáÊÄßËÉΩÊèêÂçá **6.2%**ÔºåÂú®ÊúÄÂÖ∑ÊåëÊàòÊÄßÁöÑÂú∫ÊôØ‰∏≠ÊèêÂçáÈ´òËææ **20.4%**„ÄÇ\n- **ÊïàÁéá‰∏éÊÄßËÉΩÂÖºÂæó**ÔºöÂú®ÊèêÂçáÊ≥õÂåñËÉΩÂäõÁöÑÂêåÊó∂ÔºåËé∑Âæó‰∫Ü **2.4ÂÄç** ÁöÑÊé®ÁêÜÂä†ÈÄüÔºåÂÆûÁé∞‰∫Ü‚ÄúÊó¢Âø´ÂèàÂ•Ω‚Äù„ÄÇ\n- **Âº∫Â§ßÁöÑÂèØËøÅÁßªÊÄß**ÔºöÂú®Ê®°ÊãüÂô®‰∏≠ËÆ≠ÁªÉÁöÑSPSÁ≠ñÁï•Ôºå**Êó†ÈúÄ‰ªª‰ΩïÂæÆË∞É**Âç≥ÂèØÁõ¥Êé•ÈÉ®ÁΩ≤Âà∞ÁúüÂÆûÁöÑÁâ©ÁêÜËΩ¶ËæÜ‰∏äÔºåÊàêÂäüÂ∫îÂØπ‰∫Ü‰π°ÊùëÈÅìË∑ØÂíåÂú∞‰∏ãÂÅúËΩ¶Âú∫Á≠âÂ§çÊùÇÂú∫ÊôØÔºåËØÅÊòé‰∫ÜÂÖ∂ÂçìË∂äÁöÑ Sim-to-Real ËÉΩÂäõ„ÄÇ\n- **ÊñπÊ≥ïÈÄöÁî®**ÔºöSPS ‰∏ç‰æùËµñ‰∫éÁâπÂÆöÂü∫Á°ÄÊ®°ÂûãÔºàÂú® BLIP-2 Âíå DINO ‰∏äÂùáÊúâÊïàÔºâÔºåÊó†ÈúÄÂæÆË∞ÉÂü∫Á°ÄÊ®°ÂûãÔºåÊòØ‰∏ÄÁßçËΩªÈáèÁ∫ß„ÄÅÂèØÊèíÊãîÁöÑÈÄöÁî®Â¢ûÂº∫ÊäÄÊúØ„ÄÇ\n\n**ÊÄªÁªì**ÔºöËøôÁØáËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊ¥ûÂØüÊòØ‚ÄúÂ∞ëÂç≥ÊòØÂ§ö‚Äù„ÄÇÂÆÉÂèëÁé∞Âπ∂ÂÆûËØÅ‰∫ÜÁé∞ÊúâÊñπÊ≥ï‰∏≠ÁâπÂæÅÂÜó‰ΩôÁöÑÂç±ÂÆ≥ÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊûÅÂÖ∂ÁÆÄÂçïÂç¥ÊúâÊïàÁöÑÈöèÊú∫‰∏¢ÂºÉÊñπÊ°à„ÄÇSPS ÈÄöËøáÂºïÂÖ•ÂèóÊéßÁöÑÈöèÊú∫ÊÄßÊù•ÊèêÂçáÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊïàÁéáÔºå‰∏∫Âü∫‰∫éÂü∫Á°ÄÊ®°ÂûãÁöÑÁ´ØÂà∞Á´ØËá™Âä®È©æÈ©∂Êèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ„ÄÅÂÖ∑ÊúâÂº∫Â§ßÂÆûÁî®‰ª∑ÂÄºÁöÑÊäÄÊúØË∑ØÂæÑ„ÄÇ",
      "timestamp": "2026-01-20T17:57:35.237037",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "Âü∫‰∫é‰ªñÁöÑÂâç‰ΩúÔºåÊ¢≥ÁêÜËøô‰∏™ÊñπÂêëÁöÑÊï¥‰∏™ÂèëÂ±ïËÑâÁªúÔºåÊØè‰∏ÄÊ≠•Áõ∏ÊØî‰∫é‰πãÂâçÁöÑÂ∑•‰ΩúÈÉΩÊîπËøõ‰∫Ü‰ªÄ‰πàÔºåÁùÄÈáç‰∫éÂá†‰∏™‰∏çÂêåÁöÑÂèëÂ±ïÊñπÂêë„ÄÇ",
      "answer": "## Á´ØÂà∞Á´ØËá™Âä®È©æÈ©∂‰∏≠Âü∫‰∫éÂü∫Á°ÄÊ®°ÂûãÁâπÂæÅÊèêÂèñÁöÑÂèëÂ±ïËÑâÁªúÂàÜÊûê\n\nÊú¨Êñá„ÄäSee Less, Drive Better„ÄãÂª∫Á´ãÂú®ËøëÊúü‚ÄúÂü∫‰∫éÂü∫Á°ÄÊ®°ÂûãÁöÑÁ´ØÂà∞Á´ØËá™Âä®È©æÈ©∂‚ÄùËøô‰∏ÄÊñ∞ÂÖ¥ÊñπÂêë‰πã‰∏ä„ÄÇÂÖ∂Ê†∏ÂøÉÂèëÂ±ïËÑâÁªúÂèØ‰ª•Ê¢≥ÁêÜ‰∏∫‰ª•‰∏ãÂá†‰∏™ÂÖ≥ÈîÆÈò∂ÊÆµÔºåÊØè‰∏™Èò∂ÊÆµÈÉΩÈíàÂØπÂâç‰∏ÄÈò∂ÊÆµÁöÑÂ±ÄÈôêÊÄßËøõË°å‰∫ÜÊîπËøõÔºåÂπ∂ÂàÜÂåñÂá∫‰∏çÂêåÁöÑÊäÄÊúØÂèëÂ±ïÊñπÂêë„ÄÇ\n\n### ÂèëÂ±ïËÑâÁªúÊÄªËßà\n\n```mermaid\ntimeline\n    title Á´ØÂà∞Á´ØËá™Âä®È©æÈ©∂ÁâπÂæÅÊèêÂèñÂèëÂ±ïËÑâÁªú\n    section ‰º†ÁªüÁ´ØÂà∞Á´ØÊñπÊ≥ï\n        Êó©ÊúüÂ∑•‰Ωú : ÂéüÂßãÂÉèÁ¥†Âà∞ÊéßÂà∂<br>ÈúÄÂ§ßÈáèÁúüÂÆûÊï∞ÊçÆÔºåÊ≥õÂåñÊÄßÂ∑Æ\n        ÊîπËøõÊñπÂêë : ÂºïÂÖ•Ê¶ÇÁéáÁõÆÊ†á„ÄÅ<br>Ê≥®ÊÑèÂäõÊú∫Âà∂„ÄÅÂÆâÂÖ®ÂáΩÊï∞\n    section ÂºïÂÖ•‰∏≠Èó¥Ë°®ÂæÅ\n        ËßÜËßâÊäΩË±° : ‰ΩøÁî®ËØ≠‰πâ„ÄÅËΩ¶ÈÅìÊãìÊâëÁ≠â<br>ÁÆÄÂåñÊéßÂà∂Ôºå‰æùËµñÊ†áÊ≥®ÊàñÊ®°Âûã\n    section Âü∫Á°ÄÊ®°ÂûãÊó∂‰ª£\n        Drive Anywhere : ÊèêÂèñpatchÁ∫ßÂ§öÊ®°ÊÄÅÁâπÂæÅ<br>ÊòæËëóÊèêÂçáOODÊ≥õÂåñÔºå‰ΩÜÁâπÂæÅÂÜó‰Ωô\n        See Less, Drive Better : ÊèêÂá∫SPSÔºåÈöèÊú∫Êé©Á†ÅÂÜó‰Ωôpatch<br>ÊèêÂçáÊïàÁéá‰∏éÊ≥õÂåñÔºåÂÆûÁé∞‚ÄúÂ∞ëÂç≥ÊòØÂ§ö‚Äù\n```\n\n### Èò∂ÊÆµ‰∏ÄÔºö‰º†ÁªüÁ´ØÂà∞Á´ØÊñπÊ≥ïÔºàRaw Pixels to ControlÔºâ\n\n*   **‰ª£Ë°®Â∑•‰Ωú**ÔºöALVINN (1988), NVIDIA PilotNet (2016) ÂèäÂêéÁª≠Âü∫‰∫éÂç∑ÁßØÁΩëÁªúÔºàCNNÔºâÊàñÂæ™ÁéØÁΩëÁªúÔºàRNNÔºâÁöÑÊ®°Âûã„ÄÇ\n*   **Ê†∏ÂøÉÊÄùÊÉ≥**Ôºö‰ΩøÁî®Âçï‰∏™Á•ûÁªèÁΩëÁªúÊ®°ÂûãÔºåÁõ¥Êé•‰ªéÂéüÂßã‰º†ÊÑüÂô®Êï∞ÊçÆÔºàÂ¶ÇÂõæÂÉèÔºâÊò†Â∞ÑÂà∞ÊéßÂà∂Êåá‰ª§ÔºàÂ¶ÇÊñπÂêëÁõòËΩ¨Ëßí„ÄÅÊ≤πÈó®Ôºâ„ÄÇ\n*   **ÊîπËøõ‰∏é‰ª∑ÂÄº**Ôºö\n    *   **ÁÆÄÂåñÁ≥ªÁªü**ÔºöÊëíÂºÉ‰∫Ü‰º†ÁªüÊ®°ÂùóÂåñÔºàÊÑüÁü•-ËßÑÂàí-ÊéßÂà∂ÔºâÊµÅÊ∞¥Á∫ø‰∏≠‰∏•Ê†ºÁöÑÂÅáËÆæÂíåÊé•Âè£ÔºåÂÆûÁé∞‰∫ÜÊõ¥ËøûË¥ØÁöÑ‰ºòÂåñ„ÄÇ\n    *   **Êï∞ÊçÆÈ©±Âä®**ÔºöËÉΩÂ§ü‰ªéÂ§ßËßÑÊ®°Êï∞ÊçÆ‰∏≠Â≠¶‰π†Â§çÊùÇÁöÑÊò†Â∞ÑÂÖ≥Á≥ª„ÄÇ\n*   **‰∏ªË¶ÅÂ±ÄÈôê**Ôºö\n    *   **Êï∞ÊçÆÈúÄÊ±ÇÂ§ß**ÔºöÈúÄË¶ÅÂ§ßÈáèË¶ÜÁõñÂêÑÁßçÂú∫ÊôØÁöÑÁúüÂÆû‰∏ñÁïåÈ©æÈ©∂Êï∞ÊçÆÔºåÊî∂ÈõÜÊàêÊú¨È´ò„ÄÅÈ£éÈô©Â§ß„ÄÇ\n    *   **Ê≥õÂåñËÉΩÂäõÂº±**ÔºöÂØπËÆ≠ÁªÉÊï∞ÊçÆÂàÜÂ∏ÉÔºàIn-Distribution, IDÔºâ‰πãÂ§ñÁöÑÂú∫ÊôØÔºàOut-Of-Distribution, OODÔºâÔºåÂ¶ÇÊú™Áü•Â§©Ê∞î„ÄÅÂÖâÁÖß„ÄÅÁâ©‰ΩìÔºåÊÄßËÉΩÊÄ•Ââß‰∏ãÈôç„ÄÇ\n    *   **ÂèØËß£ÈáäÊÄßÂ∑Æ**ÔºöÊ®°ÂûãÂÜ≥Á≠ñËøáÁ®ãÂÉè‰∏Ä‰∏™ÈªëÁõí„ÄÇ\n\n### Èò∂ÊÆµ‰∫åÔºöÂºïÂÖ•‰∏≠Èó¥ËßÜËßâÊäΩË±°ÔºàIntermediate Visual AbstractionsÔºâ\n\n*   **‰ª£Ë°®Â∑•‰Ωú**Ôºö‰ΩøÁî®ËØ≠‰πâÂàÜÂâ≤„ÄÅËΩ¶ÈÅìÁ∫øÊ£ÄÊµã„ÄÅ affordanceÔºàÂäüËÉΩÂèØËßÅÊÄßÔºâÁ≠â‰∏≠Èó¥Ë°®ÂæÅ‰Ωú‰∏∫Á≠ñÁï•ÁΩëÁªúÁöÑËæìÂÖ•„ÄÇ\n*   **Ê†∏ÂøÉÊÄùÊÉ≥**Ôºö‰∏çÁõ¥Êé•‰ΩøÁî®ÂéüÂßãÂÉèÁ¥†ÔºåËÄåÊòØÂÖàÈÄöËøá‰∏Ä‰∏™ÊÑüÁü•Ê®°ÂùóÊèêÂèñÈ´òÁ∫ßÁöÑ„ÄÅ‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËßÜËßâÊäΩË±°ÁâπÂæÅÔºåÂÜçÂü∫‰∫éËøô‰∫õÁâπÂæÅËøõË°åÊéßÂà∂ÂÜ≥Á≠ñ„ÄÇ\n*   **ÊîπËøõ‰∏é‰ª∑ÂÄº**Ôºö\n    *   **ÁÆÄÂåñÂ≠¶‰π†‰ªªÂä°**ÔºöÂ∞ÜÂ§çÊùÇÁöÑÂÉèÁ¥†Âà∞ÊéßÂà∂Êò†Â∞ÑÔºåÂàÜËß£‰∏∫‚ÄúÊÑüÁü•ÊäΩË±°‚ÄùÂíå‚ÄúÂü∫‰∫éÊäΩË±°ÁöÑÊéßÂà∂‚Äù‰∏§‰∏™Áõ∏ÂØπÁÆÄÂçïÁöÑÂ≠êÈóÆÈ¢ò„ÄÇ\n    *   **ÊèêÂçáÂèØËß£ÈáäÊÄß**ÔºöÂÜ≥Á≠ñÂü∫‰∫é‰∫∫Á±ªÂèØÁêÜËß£ÁöÑÁâπÂæÅÔºàÂ¶Ç‚ÄúÂâçÊñπÊúâËΩ¶‚Äù„ÄÅ‚ÄúËΩ¶ÈÅìÁ∫øÂºØÊõ≤‚ÄùÔºâ„ÄÇ\n    *   **‰∏ÄÂÆöÁ®ãÂ∫¶ÊèêÂçáÊ≥õÂåñ**ÔºöÂØπÂÉèÁ¥†Á∫ßÂèòÂåñÔºàÂ¶ÇÈ¢úËâ≤„ÄÅÁ∫πÁêÜÔºâÁöÑÈ≤ÅÊ£íÊÄßÂ¢ûÂº∫„ÄÇ\n*   **‰∏ªË¶ÅÂ±ÄÈôê**Ôºö\n    *   **‰æùËµñ‰∏äÊ∏∏Ê®°ÂûãË¥®Èáè**Ôºö‰∏≠Èó¥Ë°®ÂæÅÁöÑÂáÜÁ°ÆÊÄß‰∏•Èáç‰æùËµñ‰∫éËØ≠‰πâÂàÜÂâ≤„ÄÅÊ£ÄÊµãÁ≠â‰∏äÊ∏∏Ê®°ÂûãÁöÑÊÄßËÉΩÔºåÈîôËØØ‰ºö‰º†Êí≠Âà∞‰∏ãÊ∏∏„ÄÇ\n    *   **‰ø°ÊÅØÊçüÂ§±**ÔºöÊäΩË±°ËøáÁ®ãÂèØËÉΩ‰∏¢Â§±ÂØπÈ©æÈ©∂ÂÜ≥Á≠ñÊúâÁî®ÁöÑÁªÜËäÇ‰ø°ÊÅØ„ÄÇ\n    *   **Ê®°ÂùóÂåñÈÅóÁïôÈóÆÈ¢ò**ÔºöÊú¨Ë¥®‰∏ä‰ªçÊòØ‰∏§Èò∂ÊÆµÁ≥ªÁªüÔºåÂèØËÉΩÂºïÂÖ•Ê¨°‰ºòËß£„ÄÇ\n\n### Èò∂ÊÆµ‰∏âÔºöÂºïÂÖ•Âü∫Á°ÄÊ®°ÂûãÁâπÂæÅÔºàFoundation Model FeaturesÔºâ\n\nËøôÊòØÊú¨ÊñáÊâÄÁ´ãË∂≥ÁöÑÁõ¥Êé•Ââç‰ΩúÈò∂ÊÆµÔºåÊ†∏ÂøÉÊòØÂà©Áî®Â§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÁöÑÂü∫Á°ÄÊ®°ÂûãÔºàVLMsÔºâ‰Ωú‰∏∫Âº∫Â§ßÁöÑÁâπÂæÅÊèêÂèñÂô®„ÄÇ\n\n#### ÊñπÂêë‰∏ÄÔºöÂü∫‰∫éÂå∫ÂüüÁöÑÁâπÂæÅÊèêÂèñÔºàRegion-First PipelinesÔºâ\n\n*   **‰ª£Ë°®Â∑•‰Ωú**Ôºö‰ΩøÁî®SAM„ÄÅMask2FormerÁ≠âÊ®°ÂûãÂÖàÂØπÂõæÂÉèËøõË°åÂàÜÂâ≤ÔºåÁÑ∂ÂêéÂú®ÊØè‰∏™Âå∫ÂüüÂÜÖÈÉ®‰ΩøÁî®CLIPÁ≠âÁºñÁ†ÅÂô®Ê±†ÂåñÁâπÂæÅ„ÄÇ\n*   **Ê†∏ÂøÉÊÄùÊÉ≥**ÔºöÂà©Áî®Âü∫Á°ÄÊ®°ÂûãËé∑ÂæóÁâ©‰ΩìÁ∫ßÂà´ÁöÑ„ÄÅÂºÄÊîæËØçÊ±áÁöÑËØ≠‰πâÁâπÂæÅ„ÄÇ\n*   **ÊîπËøõ**ÔºöÊèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑËØ≠‰πâ‰ø°ÊÅØÔºåÊîØÊåÅËØ≠Ë®Ä‰∫§‰∫íÂíåË∞ÉËØï„ÄÇ\n*   **Â±ÄÈôêÔºà‰πüÊòØÊú¨ÊñáÁöÑÂØπÊØîÂü∫Á∫øÔºâ**Ôºö\n    *   **ËÆ°ÁÆóÂ§çÊùÇ**ÔºöÈúÄË¶ÅËøêË°åÂàÜÂâ≤ÂíåÁºñÁ†Å‰∏§Â•óÊ®°Âûã„ÄÇ\n    *   **‰æùËµñÂàÜÂâ≤Ë¥®Èáè**ÔºöÂàÜÂâ≤ÈîôËØØ‰ºöÂØºËá¥ÁâπÂæÅÈÅóÊºèÊàñÈîôËØØ„ÄÇ\n    *   **‰∏¢Â§±Á©∫Èó¥Â∏ÉÂ±Ä**ÔºöÂå∫ÂüüÁâπÂæÅÈÄöÂ∏∏Ë¢´Ê±†Âåñ‰∏∫‰∏Ä‰∏™ÂÖ®Â±ÄÂêëÈáèÔºå‰∏¢Â§±‰∫ÜÁ≤æÁ°ÆÁöÑÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÇ\n\n#### ÊñπÂêë‰∫åÔºöÂü∫‰∫éÂØπÈΩêÂõæÂÉèÂùóÁöÑÁâπÂæÅÊèêÂèñÔºàPatch-Aligned Feature ExtractionÔºâ‚Äî‚Äî **Êú¨ÊñáÁöÑÂü∫Áü≥**\n\n*   **‰ª£Ë°®Â∑•‰Ωú**Ôºö**Drive Anywhere (Wang et al., 2024)**Ôºå Âç≥Êú¨ÊñáÊâÄÊåáÁöÑSOTAÂíåÁõ¥Êé•Ââç‰Ωú„ÄÇ\n*   **Ê†∏ÂøÉÊäÄÊúØÂàõÊñ∞**Ôºö\n    1.  **ÁâπÂæÅÊèêÂèñÊñπÊ≥ï**ÔºöÊèêÂá∫ **‚ÄúÊé©Á†ÅÊ≥®ÊÑèÂäõ‚Äù** Êú∫Âà∂„ÄÇÂú®BLIP2Á≠âVLMÁöÑÁâπÂÆöÂ±ÇÔºàÂ¶ÇQ-FormerËæìÂÖ•ÔºâÔºåÈÄöËøáÊñΩÂä†‰∏Ä‰∏™Á©∫Èó¥Êé©Á†ÅÔºåÂº∫Âà∂Ê®°Âûã‰ªÖÂÖ≥Ê≥®Âçï‰∏™ÂõæÂÉèÂùóÔºàpatchÔºâÂèäÂÖ∂ÈÇªÂüüÔºå‰ªéËÄå‰∏∫**ÊØè‰∏™ÂõæÂÉèÂùóÁîüÊàê‰∏Ä‰∏™Áã¨Á´ãÁöÑÁâπÂæÅÊèèËø∞Á¨¶**„ÄÇËøô‰∫õÊèèËø∞Á¨¶‰Ωç‰∫éËßÜËßâ-ËØ≠Ë®ÄÂÖ±‰∫´Á©∫Èó¥„ÄÇ\n    2.  **Á≥ªÁªüÊû∂ÊûÑ**Ôºö‰ΩøÁî®**ÂÜªÁªìÁöÑ**Âü∫Á°ÄÊ®°Âûã‰Ωú‰∏∫ÁâπÂæÅÊèêÂèñÂô®Ôºå‰ªÖËÆ≠ÁªÉ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÁ≠ñÁï•ÁΩëÁªúÔºàÂ¶ÇViTÔºâÂü∫‰∫éËøô‰∫õpatchÁâπÂæÅËøõË°åÈ©æÈ©∂ÂÜ≥Á≠ñ„ÄÇ\n*   **Áõ∏ÊØî‰∫é‰πãÂâçÂ∑•‰ΩúÁöÑÈáçÂ§ßÊîπËøõ**Ôºö\n    *   **ÊûÅÂº∫ÁöÑOODÊ≥õÂåñ**ÔºöÂà©Áî®Âü∫Á°ÄÊ®°ÂûãÂú®Ë∂ÖÂ§ßËßÑÊ®°Êï∞ÊçÆ‰∏äÂ≠¶Âà∞ÁöÑÈÄöÁî®ËßÜËßâ-ËØ≠Ë®ÄÁü•ËØÜÔºåÂØπÊú™ËßÅËøáÁöÑÂú∫ÊôØ„ÄÅÂ§©Ê∞î„ÄÅÁâ©‰ΩìË°®Áé∞Âá∫ÂçìË∂äÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ\n    *   **‰øùÁïôÁ©∫Èó¥‰ø°ÊÅØ**Ôºöpatch-alignedÁâπÂæÅ‰øùÊåÅ‰∫ÜÂéüÂßãÂõæÂÉèÁöÑÁ©∫Èó¥ÁΩëÊ†ºÂ∏ÉÂ±ÄÔºå‰∏∫Á≠ñÁï•ÁΩëÁªúÊèê‰æõ‰∫ÜÁªÜÁ≤íÂ∫¶ÁöÑÁ©∫Èó¥‰∏ä‰∏ãÊñá„ÄÇ\n    *   **ÊîØÊåÅËØ≠Ë®Ä‰∫§‰∫í**ÔºöÁâπÂæÅÂ≠òÂú®‰∫éÂ§öÊ®°ÊÄÅÁ©∫Èó¥ÔºåÂ§©ÁÑ∂ÊîØÊåÅÂü∫‰∫éÊñáÊú¨ÁöÑÂ¢ûÂº∫„ÄÅË∞ÉËØïÂíåÊåá‰ª§ÁêÜËß£„ÄÇ\n    *   **ÊïàÁéá‰∏éÂèØËøÅÁßªÊÄß**ÔºöÂÜªÁªìÂü∫Á°ÄÊ®°ÂûãÔºåÂè™ÈúÄËÆ≠ÁªÉÂ∞èÂûãÁ≠ñÁï•ÁΩëÁªúÔºåËÆ°ÁÆóÊàêÊú¨Áõ∏ÂØπÂèØÊéßÔºå‰∏îÊòì‰∫éËøÅÁßª„ÄÇ\n*   **ÈÅóÁïôÁöÑÊ†∏ÂøÉÈóÆÈ¢òÔºàÊú¨ÊñáÁöÑÂá∫ÂèëÁÇπÔºâ**Ôºö\n    *   **ÁâπÂæÅÂÜó‰Ωô**ÔºöÁî±‰∫éViT‰∏≠ÁöÑËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊØè‰∏™patchÁöÑÁâπÂæÅÈÉΩÈöêÂºèÂåÖÂê´‰∫ÜÂÖ®Â±Ä‰ø°ÊÅØÔºåÂØºËá¥‰∏çÂêåpatchÁöÑÁâπÂæÅÈ´òÂ∫¶Áõ∏ÂÖ≥ÂíåÂÜó‰Ωô„ÄÇËøô‰ºö‰ΩøÁ≠ñÁï•ÁΩëÁªúËøáÊãüÂêà‰∫éËôöÂÅáÁöÑÁõ∏ÂÖ≥ÊÄßÔºåÊçüÂÆ≥È≤ÅÊ£íÊÄßÔºåÂπ∂ÈÄ†ÊàêËÆ°ÁÆóÊµ™Ë¥π„ÄÇ\n\n### Èò∂ÊÆµÂõõÔºöÈíàÂØπÁâπÂæÅÂÜó‰ΩôÁöÑ‰ºòÂåñ ‚Äî‚Äî Êú¨ÊñáÂ∑•‰Ωú„ÄäSee Less, Drive Better„Äã\n\n*   **Ê†∏ÂøÉÊäÄÊúØÂàõÊñ∞**Ôºö**ÈöèÊú∫ÂùóÈÄâÊã©ÔºàStochastic Patch Selection, SPSÔºâ**\n*   **ÈíàÂØπÂâç‰ΩúÔºàDrive AnywhereÔºâÁöÑÊîπËøõ**Ôºö\n    1.  **ÈóÆÈ¢òÈ©±Âä®**ÔºöÈ¶ñÊ¨°ÊòéÁ°ÆÊåáÂá∫Âπ∂ÈáèÂåñ‰∫Üpatch-alignedÁâπÂæÅÁöÑÂÜó‰ΩôÊÄßÈóÆÈ¢òÔºàÈÄöËøáPCA„ÄÅÁõ∏ÂÖ≥ÊÄßÂàÜÊûêÔºâÔºåÂπ∂Â∞ÜÂÖ∂ËßÜ‰∏∫ÂΩ±ÂìçOODÈ≤ÅÊ£íÊÄßÁöÑÂÖ≥ÈîÆÂõ†Á¥†„ÄÇ\n    2.  **ÊñπÊ≥ïÂàõÊñ∞**ÔºöÊèêÂá∫Âú®ÁâπÂæÅÊèêÂèñÂêé„ÄÅÁ≠ñÁï•ÁΩëÁªúÂâçÔºå**ÈöèÊú∫‰∏¢ÂºÉ‰∏ÄÈÉ®ÂàÜpatchÁöÑÁâπÂæÅ**ÔºàÁî®Èõ∂Â°´ÂÖÖÊàñÁõ¥Êé•ÁßªÈô§‰ΩÜ‰øùÁïô‰ΩçÁΩÆÁºñÁ†ÅÔºâÔºåÂêåÊó∂‰øùÊåÅÂâ©‰ΩôÁâπÂæÅÁöÑÁ©∫Èó¥Â∏ÉÂ±Ä‰∏çÂèò„ÄÇ\n    3.  **ËÆ≠ÁªÉÊú∫Âà∂**ÔºöÈÄöËøá‰∏∫ÊØè‰∏ÄÂ∏ßÊèê‰æõ‰∏çÂêåÁöÑ„ÄÅÈöèÊú∫ÁöÑ‰ΩÜÂÆåÊï¥ÁöÑÂú∫ÊôØ‚ÄúËßÜÂõæ‚ÄùÔºå**Âº∫Âà∂Á≠ñÁï•ÁΩëÁªúÂ≠¶‰π†Âü∫‰∫éÈÇ£‰∫õÂØπÂÖ∑‰ΩìÂì™‰∫õtokenË¢´‰øùÁïô‰∏çÂèòÁöÑ„ÄÅÂõ†ÊûúÁõ∏ÂÖ≥ÁöÑÁâπÂæÅ**Ôºå‰ªéËÄåÊäëÂà∂ÂØπÂÜó‰ΩôÂíåËôöÂÅáÁõ∏ÂÖ≥ÊÄßÁöÑ‰æùËµñ„ÄÇ\n*   **Â∏¶Êù•ÁöÑÂÖ®Èù¢ÊèêÂçá**Ôºö\n    *   **Ê≥õÂåñÊÄßÔºàÊ†∏ÂøÉË¥°ÁåÆÔºâ**ÔºöÂπ≥ÂùáOODÊàêÂäüÁéáÊèêÂçá**6.2%**ÔºåÊúÄÈ´òÊèêÂçá**20.4%**„ÄÇÂú®9‰∏™Âèò‰ΩìÊ®°Âûã‰∏≠Ôºå8‰∏™Ë∂ÖË∂ä‰∫ÜÂâçSOTA„ÄÇ\n    *   **ÊïàÁéá**ÔºöÊé®ÁêÜÈÄüÂ∫¶ÊèêÂçá **2.4ÂÄç**ÔºàÊúÄ‰Ω≥Ê®°ÂûãSPS-50%ÔºâÔºåÂõ†‰∏∫Âè™ÈúÄËÆ°ÁÆóÈÉ®ÂàÜpatchÁöÑÁâπÂæÅ„ÄÇ\n    *   **ÁêÜËÆ∫ÊîØÊíë**ÔºöÊèê‰æõ‰∫Ü‰ΩéÁß©Â≠êÁ©∫Èó¥ÂíåÈöèÊú∫ÈááÊ†∑‰øùÁïôËØ≠‰πâÁöÑÁêÜËÆ∫ÂàÜÊûêÔºàLemma 1ÔºâÔºåËØÅÊòé‚ÄúÂ∞ëÂç≥ÊòØÂ§ö‚ÄùÁöÑÂêàÁêÜÊÄß„ÄÇ\n    *   **ÂÖºÂÆπÊÄß‰∏éÊâ©Â±ïÊÄß**Ôºö\n        *   ‰∏éÊñáÊú¨È©±Âä®ÁöÑÊΩúÂú®Á©∫Èó¥Â¢ûÂº∫Êó†ÁºùÁªìÂêàÔºåÂ∏¶Êù•È¢ùÂ§ñ**1.7%** ÊÄßËÉΩÊèêÂçá„ÄÇ\n        *   ÂèØËøÅÁßªËá≥ÂÖ∂‰ªñÂü∫Á°ÄÊ®°ÂûãÔºàÂ¶ÇDINOÔºâÔºåÂ±ïÁ§∫ÈÄöÁî®ÊÄß„ÄÇ\n        *   Êó†ÈúÄÂæÆË∞ÉÂü∫Á°ÄÊ®°ÂûãÔºåÂç≥ÊèíÂç≥Áî®„ÄÇ\n    *   **‰ªøÁúüÂà∞Áé∞ÂÆûÔºàSim-to-RealÔºâ**ÔºöÊàêÂäüÈÉ®ÁΩ≤Âà∞ÁúüÂÆûËΩ¶ËæÜÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÂÆûÈôÖOODÂú∫ÊôØÔºàÂ¶ÇÂú∞‰∏ãËΩ¶Â∫ìÔºâ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ\n\n### ÊÄªÁªìÔºöÂá†‰∏™‰∏çÂêåÁöÑÂèëÂ±ïÊñπÂêë\n\n1.  **ËæìÂÖ•Ë°®ÂæÅÁöÑÂèëÂ±ïÊñπÂêë**Ôºö**ÂéüÂßãÂÉèÁ¥† -> ‰∫∫Â∑•ÂÆö‰πâÁöÑ‰∏≠Â±ÇÊäΩË±°ÔºàËØ≠‰πâ/ËΩ¶ÈÅìÔºâ -> Âü∫Á°ÄÊ®°ÂûãÈ©±Âä®ÁöÑÈÄöÁî®ÁâπÂæÅÔºàÂå∫Âüü/ÂùóÂØπÈΩêÔºâ -> ÂéªÂÜó‰ΩôÂåñÁöÑÂùóÂØπÈΩêÁâπÂæÅ**„ÄÇË∂ãÂäøÊòØËøΩÊ±ÇÊõ¥‰∏∞ÂØå„ÄÅÊõ¥ÈÄöÁî®„ÄÅÊõ¥È´òÊïà‰∏îÊõ¥È≤ÅÊ£íÁöÑÁâπÂæÅË°®Á§∫„ÄÇ\n2.  **Ê≥õÂåñËÉΩÂäõÊèêÂçáÊñπÂêë**Ôºö**‰æùËµñÂ§ßÈáèÁúüÂÆûÊï∞ÊçÆ -> Âà©Áî®‰ªøÁúüÂô® -> Âà©Áî®Âü∫Á°ÄÊ®°ÂûãÁöÑÂÖàÈ™åÁü•ËØÜ -> Âú®Âü∫Á°ÄÊ®°ÂûãÁâπÂæÅ‰∏äÊñΩÂä†Ê≠£ÂàôÂåñÔºàÂ¶ÇSPSÔºâ‰ª•ÊèêÂçáOODÈ≤ÅÊ£íÊÄß**„ÄÇÊ†∏ÂøÉÊòØÂà©Áî®Êõ¥Âº∫Â§ßÁöÑÂÖàÈ™åÂíåÊõ¥Êô∫ËÉΩÁöÑÊ≠£ÂàôÂåñÊù•ÂÖãÊúçÊï∞ÊçÆÂàÜÂ∏ÉÁöÑÂ±ÄÈôê„ÄÇ\n3.  **Á≥ªÁªüËÆæËÆ°Âì≤Â≠¶ÊñπÂêë**Ôºö**Á´ØÂà∞Á´ØÈªëÁÆ± -> Ê®°ÂùóÂåñÂèØËß£Èáä -> Âü∫‰∫éÂÜªÁªìÂü∫Á°ÄÊ®°ÂûãÁöÑ‚ÄúÁâπÂæÅÊèêÂèñ+ËΩªÈáèÁ≠ñÁï•‚Äù -> Âú®ÁâπÂæÅÂ±ÇÈù¢ËøõË°åÂπ≤È¢ÑÂíå‰ºòÂåñÁöÑÈ´òÊïàÊû∂ÊûÑ**„ÄÇÂú®‰øùÊåÅÁ´ØÂà∞Á´ØËÅîÂêà‰ºòÂåñ‰ºòÂäøÁöÑÂêåÊó∂ÔºåÂºïÂÖ•‰∫ÜÂèØËß£Èáä„ÄÅÂèØÂπ≤È¢ÑÁöÑ‰∏≠Èó¥Â±ÇÔºàÂü∫Á°ÄÊ®°ÂûãÁâπÂæÅÔºâ„ÄÇ\n4.  **ÊïàÁéá‰ºòÂåñÊñπÂêë**Ôºö**ËÆ°ÁÆóÂØÜÈõÜÂûãÂ§ßÊ®°Âûã -> ËΩªÈáèÁ∫ßÁ≠ñÁï•ÁΩëÁªú -> Âä®ÊÄÅÁâπÂæÅÈÄâÊã©ÔºàSPSÔºâ**„ÄÇ",
      "timestamp": "2026-01-20T17:58:42.852902",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "‰ªñÁöÑÂâç‰ΩúÊúâÂì™‰∫õÔºü‰ΩøÁî®Ë°®Ê†º‰ªîÁªÜËÆ≤ËÆ≤‰ªñÁöÑÊØèÁØáÂâç‰ΩúÔºå‰ªñÂíåÂâç‰ΩúÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºå‰∏ªË¶ÅÊîπÂñÑÊòØ‰ªÄ‰πàÔºüÁùÄÈáç‰∫éÂÖ∑‰ΩìÁõ∏ÊØî‰∫é‰πãÂâçÊñáÁ´†ÁöÑÊîπÂä®",
      "answer": "## Ââç‰ΩúÊ¢≥ÁêÜ‰∏éÂàÜÊûê\n\nÊ†πÊçÆËÆ∫ÊñáÂÜÖÂÆπÔºåÂÖ∂Ê†∏ÂøÉÂâç‰ΩúÊòØ **Wang et al. (2024) ÁöÑ ‚ÄúDrive Anywhere‚Äù** Â∑•‰Ωú„ÄÇËØ•Â∑•‰ΩúÊòØÊú¨ÊñáÊñπÊ≥ïÔºàSPSÔºâÁõ¥Êé•ÊûÑÂª∫ÂíåÂØπÊØîÁöÑÂü∫Á°Ä„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáÂú®Áõ∏ÂÖ≥Â∑•‰Ωú‰∏≠ËøòÊèêÂà∞‰∫ÜÂÖ∂‰ªñÂá†ÁØáÈáçË¶ÅÁöÑÂâçÊúüÁ†îÁ©∂ÔºåÂÖ±ÂêåÊûÑÊàê‰∫ÜËØ•È¢ÜÂüüÁöÑÊäÄÊúØÊºîËøõËÑâÁªú„ÄÇ\n\n‰ª•‰∏ãË°®Ê†ºËØ¶ÁªÜÊ¢≥ÁêÜ‰∫Ü‰∏ªË¶ÅÂâç‰Ωú„ÄÅÊú¨ÊñáÔºàSPSÔºâ‰∏éÂâç‰ΩúÁöÑÂå∫Âà´‰ª•ÂèäÂÖ∑‰ΩìÁöÑÊäÄÊúØÊîπËøõÔºö\n\n| ËÆ∫Êñá / Â∑•‰Ωú | Ê†∏ÂøÉÊñπÊ≥ï | ‰∏éÊú¨ÊñáÔºàSPSÔºâÁöÑ‰∏ªË¶ÅÂå∫Âà´ | Êú¨ÊñáÔºàSPSÔºâÁöÑÂÖ∑‰ΩìÊîπËøõ‰∏éÂàõÊñ∞ |\n| :--- | :--- | :--- | :--- |\n| **Wang et al. (2024) - ‚ÄúDrive Anywhere‚Äù** (Áõ¥Êé•Ââç‰Ωú) | 1. **Âü∫Á°ÄÊ°ÜÊû∂**Ôºö‰ΩøÁî®ÂÜªÁªìÁöÑÂ§ßÂûãËßÜËßâ-ËØ≠Ë®ÄÂü∫Á°ÄÊ®°ÂûãÔºàÂ¶ÇBLIP-2ÔºâÊèêÂèñ**ÂùóÂØπÈΩêÔºàpatch-alignedÔºâÁöÑÂ§öÊ®°ÊÄÅÁâπÂæÅ**„ÄÇ<br>2. **ÁâπÂæÅÊèêÂèñ**ÔºöÈááÁî®**Êé©Á†ÅÊ≥®ÊÑèÂäõÔºàmasked attentionÔºâ**Êú∫Âà∂ÔºåÂú®Q-FormerÂ±Ç‰∏∫ÊØè‰∏™ÂõæÂÉèÂùóÁîüÊàê‰∏Ä‰∏™ËÅöÁÑ¶‰∫éËØ•ÂùóÁöÑÊèèËø∞Á¨¶„ÄÇ<br>3. **Á≠ñÁï•ËÆ≠ÁªÉ**ÔºöÂú®Ëøô‰∫õ‰∏∞ÂØåÁöÑÁâπÂæÅ‰∏äËÆ≠ÁªÉ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÁ≠ñÁï•ÁΩëÁªúÔºåÂÆûÁé∞Á´ØÂà∞Á´ØÈ©æÈ©∂„ÄÇÂÖ∂Ê†∏ÂøÉË¥°ÁåÆÊòØËØÅÊòé‰∫Ü‰ΩøÁî®Âü∫Á°ÄÊ®°ÂûãÁâπÂæÅËÉΩÊòæËëóÊèêÂçá**ÂàÜÂ∏ÉÂ§ñÔºàOODÔºâÊ≥õÂåñ**ËÉΩÂäõ„ÄÇ | 1. **ÁâπÂæÅÂÜó‰ΩôÈóÆÈ¢ò**ÔºöÂâç‰ΩúÊú™Â§ÑÁêÜÂü∫Á°ÄÊ®°Âûã‰∫ßÁîüÁöÑÂùóÁâπÂæÅ‰πãÈó¥Â≠òÂú®ÁöÑ**È´òÂ∫¶ÂÜó‰ΩôÂíåÁõ∏ÂÖ≥ÊÄß**ÈóÆÈ¢ò„ÄÇ<br>2. **ËÆ≠ÁªÉÊïàÁéá‰∏éÈ≤ÅÊ£íÊÄß**ÔºöÁõ¥Êé•‰ΩøÁî®ÂÖ®ÈÉ®ÂùóÁâπÂæÅËøõË°åËÆ≠ÁªÉÔºåÂèØËÉΩÂØºËá¥Á≠ñÁï•ËøáÊãüÂêà‰∫éËôöÂÅáÁõ∏ÂÖ≥ÊÄßÔºå‰∏îËÆ°ÁÆóÊàêÊú¨ËæÉÈ´ò„ÄÇ<br>3. **ÊñπÊ≥ïÁÑ¶ÁÇπ**ÔºöÂâç‰ΩúËÅöÁÑ¶‰∫é**ÂºïÂÖ•Âü∫Á°ÄÊ®°ÂûãÁâπÂæÅ**ÁöÑÊúâÊïàÊÄßÔºåËÄåÊú¨ÊñáËÅöÁÑ¶‰∫é**‰ºòÂåñËøô‰∫õÁâπÂæÅÁöÑ‰ΩøÁî®ÊñπÂºè**„ÄÇ | 1. **Ê†∏ÂøÉÂàõÊñ∞ÔºàSPSÔºâ**ÔºöÊèêÂá∫‰∫Ü**ÈöèÊú∫ÂùóÈÄâÊã©**Êú∫Âà∂„ÄÇÂú®ÊØè‰∏ÄÂ∏ßÔºåÈöèÊú∫‰∏¢ÂºÉ‰∏ÄÂÆöÊØî‰æãÔºàÂ¶Ç50%ÔºâÁöÑÂùóÊèèËø∞Á¨¶Ôºå‰ªÖÂ∞ÜÂâ©‰ΩôÁöÑÊèèËø∞Á¨¶Ôºà‰øùÊåÅÂÖ∂Á©∫Èó¥Â∏ÉÂ±ÄÔºâËæìÂÖ•Á≠ñÁï•ÁΩëÁªú„ÄÇ<br>2. **Ëß£ÂÜ≥ÂÜó‰Ωô**ÔºöËø´‰ΩøÁ≠ñÁï•Â≠¶‰π†Âü∫‰∫é**‰∏çÂêåÈöèÊú∫Â≠êÈõÜ**ÁöÑÂÜ≥Á≠ñÔºå‰ªéËÄåÂÖ≥Ê≥®**ÂØπÂÖ∑‰Ωì‰ª§ÁâåÂ≠òÁïô‰∏çÂèòÁöÑ„ÄÅÊõ¥È≤ÅÊ£íÁöÑÁâπÂæÅ**ÔºåÂáèÂ∞ë‰∫ÜËøáÊãüÂêà„ÄÇ<br>3. **ÊÄßËÉΩÊèêÂçá**ÔºöÂú®OODÂú∫ÊôØ‰∏ãÔºåÂπ≥ÂùáÊàêÂäüÁéáÊèêÂçá**6.2%**ÔºåÊúÄÈ´òÊèêÂçáËææ**20.4%**ÔºåÂêåÊó∂Êé®ÁêÜÈÄüÂ∫¶Âä†Âø´**2.4ÂÄç**„ÄÇ<br>4. **ÁêÜËÆ∫ÂàÜÊûê**ÔºöÈ¶ñÊ¨°ÈÄöËøáPCAÂíåÂùóÈó¥Áõ∏‰ººÊÄßÂàÜÊûê**ÈáèÂåñÂπ∂ËØÅÊòé‰∫Ü**BLIP-2Á≠âÊ®°ÂûãÁâπÂæÅÁöÑÂÜó‰ΩôÊÄßÔºà90%ÊñπÂ∑Æ‰ªÖÁî±17/64‰∏™‰∏ªÊàêÂàÜËß£ÈáäÔºâ„ÄÇ |\n| **Amini et al. (2022) - ‚ÄúVISTA 2.0‚Äù** (‰ªøÁúü‰∏éËÆ≠ÁªÉÊ°ÜÊû∂) | ÊèêÂá∫‰∫Ü‰∏Ä‰∏™**ÂºÄÊ∫ê„ÄÅÊï∞ÊçÆÈ©±Âä®ÁöÑÂ§öÊ®°ÊÄÅ‰ªøÁúüÂô®**ÔºåÁî®‰∫éËá™Âä®È©æÈ©∂Á≠ñÁï•Â≠¶‰π†„ÄÇÂÆÉÂü∫‰∫éÁúüÂÆûÈ©æÈ©∂Êó•ÂøóÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊ®°ÊãüÁªèÈ™åÔºåÊîØÊåÅÈó≠ÁéØËÆ≠ÁªÉ‰∏éËØÑ‰º∞„ÄÇ | Êú¨ÊñáÂπ∂ÈùûÁõ¥Êé•ÊîπËøõ‰ªøÁúüÂô®ÔºåËÄåÊòØÂ∞ÜSPSÊñπÊ≥ï**Â∫îÁî®‰∫é**Áî±VISTAÁ≠â‰ªøÁúüÂô®ÁîüÊàêÁöÑÊï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉÂíåËØÑ‰º∞„ÄÇVISTAÊòØÊú¨ÊñáÂÆûÈ™åÁöÑÂü∫Á°ÄÂπ≥Âè∞‰πã‰∏Ä„ÄÇ | **ÊñπÊ≥ïÈõÜÊàê**ÔºöÊú¨ÊñáÂú®VISTA‰ªøÁúüÊ°ÜÊû∂ÂÜÖËÆ≠ÁªÉÂíåËØÑ‰º∞SPSÁ≠ñÁï•ÔºåËØÅÊòé‰∫ÜSPSÂú®ËØ•ÂÖàËøõ‰ªøÁúüÁéØÂ¢É‰∏ãÁöÑÊúâÊïàÊÄß„ÄÇÊú¨ÊñáÁöÑË¥°ÁåÆÂú®‰∫é**È©æÈ©∂Á≠ñÁï•Êú¨Ë∫´ÁöÑÂ≠¶‰π†ÊñπÊ≥ï**ÔºåËÄåÈùû‰ªøÁúüÂπ≥Âè∞„ÄÇ |\n| **Êó©ÊúüÁ´ØÂà∞Á´ØÊñπÊ≥ï** (Â¶ÇBojarski et al., 2016) | ‰ΩøÁî®Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàCNNÔºâÁõ¥Êé•Â∞ÜÂéüÂßãÂÉèÁ¥†Êò†Â∞ÑÂà∞ÊéßÂà∂Êåá‰ª§ÔºàËΩ¨Âêë„ÄÅÊ≤πÈó®Ôºâ„ÄÇ | 1. **Ë°®Á§∫Â±ÇÈù¢**ÔºöÊó©ÊúüÊñπÊ≥ï‰ΩøÁî®**ÂéüÂßãÂÉèÁ¥†ÊàñCNNÁâπÂæÅ**ÔºåËÄåÊú¨ÊñáÂèäÂâç‰Ωú‰ΩøÁî®**Âü∫Á°ÄÊ®°ÂûãÊèêÂèñÁöÑËØ≠‰πâ‰∏∞ÂØåÁöÑÂ§öÊ®°ÊÄÅÁâπÂæÅ**„ÄÇ<br>2. **Ê≥õÂåñËÉΩÂäõ**ÔºöÊó©ÊúüÊñπÊ≥ï‰∏•Èáç‰æùËµñÂ§ßÈáèÁúüÂÆûÊï∞ÊçÆÔºåÂú®OODÂú∫ÊôØ‰∏ãÊ≥õÂåñËÉΩÂäõËæÉÂº±„ÄÇ | **Ë°®Á§∫Â≠¶‰π†È£ûË∑É**ÔºöÊú¨ÊñáÁªßÊâø‰∫Ü‚ÄúDrive Anywhere‚ÄùÁöÑÊÄùË∑ØÔºå‰ΩÜËøõ‰∏ÄÊ≠•‰ºòÂåñ‰∫ÜÁâπÂæÅË°®Á§∫„ÄÇSPSÈÄöËøáÂú®È´òÁ∫ßËØ≠‰πâÁâπÂæÅÁ©∫Èó¥ËøõË°åÈöèÊú∫ÂåñÔºåÂÆûÁé∞‰∫ÜÊØîÊó©ÊúüÂÉèÁ¥†Á∫ßÊñπÊ≥ï**Êõ¥È´òÊïà„ÄÅÊõ¥È≤ÅÊ£í**ÁöÑÊ≥õÂåñÔºåÂáèÂ∞ë‰∫ÜÂØπÊµ∑ÈáèÁúüÂÆûÈ£éÈô©Êï∞ÊçÆÁöÑÈúÄÊ±Ç„ÄÇ |\n| **Âü∫‰∫é‰∏≠Èó¥Ë°®Á§∫ÁöÑÈ©æÈ©∂** (Â¶ÇToromanoff et al., 2020) | ‰ΩøÁî®**ËßÜËßâÊäΩË±°**ÔºàÂ¶ÇÂèØÈ©æÈ©∂Âå∫Âüü„ÄÅËØ≠‰πâÂàÜÂâ≤„ÄÅËΩ¶ÈÅìÊãìÊâëÔºâ‰Ωú‰∏∫‰∏≠Èó¥Ë°®Á§∫ÔºåÂÜçÂü∫‰∫éÊ≠§ËøõË°åËßÑÂàíÊéßÂà∂„ÄÇÁÆÄÂåñ‰∫ÜÊéßÂà∂ÈóÆÈ¢òÔºå‰ΩÜ‰æùËµñËøô‰∫õÊäΩË±°Ë°®Á§∫ÁöÑÂáÜÁ°ÆÊÄßÂíåÊ≥õÂåñÊÄß„ÄÇ | Êú¨ÊñáÂ±û‰∫é**Á´ØÂà∞Á´ØÁâπÂæÅÂ≠¶‰π†**ÊµÅÊ¥æÔºå‰∏ç‰æùËµñ‰∫∫Â∑•ÂÆö‰πâÊàñÂ§ñÈÉ®Ê®°ÂûãÁîüÊàêÁöÑ‰∏≠Èó¥ÊäΩË±°Ë°®Á§∫„ÄÇÁâπÂæÅÊù•Ëá™ÈÄöÁî®ÁöÑÂü∫Á°ÄÊ®°ÂûãÔºåÊõ¥ÂÖ∑ÊôÆÈÄÇÊÄß„ÄÇ | **ÁâπÂæÅÈÄöÁî®ÊÄß vs. ÊäΩË±°ÁâπÂºÇÊÄß**ÔºöSPSÊñπÊ≥ï‰∏ç‰æùËµñ‰∫éÁâπÂÆö‰ªªÂä°ÔºàÂ¶ÇËΩ¶ÈÅìÊ£ÄÊµãÔºâÁöÑÊäΩË±°Ê®°ÂûãÔºåÈÅøÂÖç‰∫ÜËøô‰∫õÊ®°ÂûãÂú®OODÂú∫ÊôØ‰∏ãÂèØËÉΩÂá∫Áé∞ÁöÑÊïÖÈöú‰º†Êí≠„ÄÇÂÆÉÂà©Áî®Âü∫Á°ÄÊ®°ÂûãÁöÑÈÄöÁî®ËßÜËßâ-ËØ≠Ë®ÄÁü•ËØÜÔºåÂπ∂ÈÄöËøáSPSÂ¢ûÂº∫ÂÖ∂È≤ÅÊ£íÊÄß„ÄÇ |\n| **Âü∫‰∫éÂàÜÂâ≤Êé©Á†ÅÁöÑÁâπÂæÅÊñπÊ≥ï** (Â¶ÇMFÂü∫Á∫ø) | ÂÖà‰ΩøÁî®ÈÄöÁî®ÂàÜÂâ≤Ê®°ÂûãÔºàÂ¶ÇSAMÔºâÂàÜÂâ≤ÂõæÂÉèÔºåÁÑ∂ÂêéÂú®ÊØè‰∏™Âå∫ÂüüÂÜÖÁöÑË£ÅÂâ™ÂõæÂÉè‰∏ä‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁºñÁ†ÅÂô®ÔºàÂ¶ÇCLIPÔºâÊèêÂèñ‰∏Ä‰∏™ÂÖ®Â±ÄÁâπÂæÅÂêëÈáèÔºåÂπ∂ÂàÜÈÖçÁªôËØ•Âå∫ÂüüÊâÄÊúâÂÉèÁ¥†„ÄÇ | 1. **ÁâπÂæÅÁîüÊàêÊµÅÁ®ã**Ôºö‚ÄúÊé©Á†Å‰ºòÂÖà‚Äù vs Êú¨ÊñáÁöÑ‚ÄúÂùóÂØπÈΩê‚Äù„ÄÇÂâçËÄÖ‰æùËµñ**Â§ñÈÉ®ÂàÜÂâ≤Ê®°Âûã**ÁöÑË¥®ÈáèÔºåËã•ÂàÜÂâ≤Â§±Ë¥•Âàô‰ºö‰∏¢Â§±‰ø°ÊÅØÔºõÂêéËÄÖÁõ¥Êé•‰ªéÂü∫Á°ÄÊ®°ÂûãÈ™®Âπ≤ÁΩëÁªúÊèêÂèñÔºå**‰øùÁïôÁ©∫Èó¥Â∏ÉÂ±Ä‰∏îÊó†È¢ùÂ§ñ‰æùËµñ**„ÄÇ<br>2. **ÁâπÂæÅÁ≤íÂ∫¶**ÔºöÂâçËÄÖ‰∏∫ÊØè‰∏™Âå∫ÂüüÁîüÊàê‰∏Ä‰∏™**Âçï‰∏ÄÁâπÂæÅÂêëÈáè**ÔºõÂêéËÄÖ‰∏∫ÊØè‰∏™ÂõæÂÉèÂùóÁîüÊàê‰∏Ä‰∏™ÊèèËø∞Á¨¶Ôºå**Á©∫Èó¥Á≤íÂ∫¶Êõ¥ÁªÜ**„ÄÇ | **ÊµÅÁ®ãÁÆÄÂåñ‰∏éÈ≤ÅÊ£íÊÄß**ÔºöSPSÂª∫Á´ãÂú®‚ÄúDrive Anywhere‚ÄùÁöÑÂùóÂØπÈΩêÊèêÂèñÊµÅÁ®ã‰∏äÔºåËØ•ÊµÅÁ®ãÊú¨Ë∫´Â∞±Êõ¥ÁÆÄÊ¥Å„ÄÅÊõ¥‰∏çÊòìÂèóÂàÜÂâ≤ÈîôËØØÂΩ±Âìç„ÄÇSPSÂú®Ê≠§Âü∫Á°Ä‰∏äËøõ‰∏ÄÊ≠•Â§ÑÁêÜÁâπÂæÅÂÜó‰ΩôÔºå‰ªéËÄåÂú®ÊÄßËÉΩ‰∏äÂ§ßÂπÖË∂ÖË∂ä‰∫ÜMFÂü∫Á∫øÔºàËßÅË°®1Ôºâ„ÄÇ |\n\n## ÊÄªÁªìÔºöÊú¨Êñá‰∏éÂâç‰ΩúÁöÑÊ†∏ÂøÉÂå∫Âà´‰∏éÊîπËøõ\n\n1.  **ÈóÆÈ¢òÊÑèËØÜ‰∏çÂêå**ÔºöÂâç‰Ωú‚ÄúDrive Anywhere‚Äù**ÂèëÁé∞Âπ∂È™åËØÅ‰∫Ü**‰ΩøÁî®Âü∫Á°ÄÊ®°ÂûãÂùóÁâπÂæÅÂØπOODÊ≥õÂåñÁöÑÂ∑®Â§ß‰ª∑ÂÄº„ÄÇÊú¨ÊñáÂàô**ËØäÊñ≠Âπ∂Ëß£ÂÜ≥‰∫Ü**ËØ•ÊñπÊ°à‰∏≠‰∏Ä‰∏™Êú™Ë¢´ÈáçËßÜÁöÑÂÜÖÂú®ÈóÆÈ¢òÔºö**ÁâπÂæÅÂÜó‰Ωô**„ÄÇ\n2.  **ÊäÄÊúØÂàõÊñ∞ÁÇπ‰∏çÂêå**ÔºöÂâç‰ΩúÁöÑÂÖ≥ÈîÆÂàõÊñ∞ÊòØ**Êé©Á†ÅÊ≥®ÊÑèÂäõÊèêÂèñÂùóÂØπÈΩêÁâπÂæÅ**ÁöÑÂ∑•Á®ãÊñπÊ≥ï„ÄÇÊú¨ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞ÊòØ**ÈöèÊú∫ÂùóÈÄâÊã©ÔºàSPSÔºâ** Ëøô‰∏ÄËÆ≠ÁªÉ/Êé®ÁêÜÁ≠ñÁï•ÔºåÂÆÉÊòØ‰∏ÄÁßçÈíàÂØπÂ∑≤ÊúâÁâπÂæÅÁöÑÂêéÂ§ÑÁêÜÊ≠£ÂàôÂåñÊñπÊ≥ï„ÄÇ\n3.  **ÂÖ∑‰ΩìÊîπËøõ**Ôºö\n    *   **ÊÄßËÉΩ**ÔºöÂú®ÂÆåÂÖ®Áõ∏ÂêåÁöÑÂü∫ÂáÜÔºàDrive AnywhereÊ°ÜÊû∂ÔºâÂíåÂü∫Á°ÄÊ®°ÂûãÔºàBLIP-2Ôºâ‰∏äÔºåSPSÂÆûÁé∞‰∫Ü**ÊòæËëóÁöÑOODÊÄßËÉΩÊèêÂçá**ÔºàÂπ≥Âùá+6.2%ÔºâÂíå**Êé®ÁêÜÂä†ÈÄü**Ôºà2.4ÂÄçÔºâ„ÄÇ\n    *   **È≤ÅÊ£íÊÄß**ÔºöÈÄöËøáËø´‰ΩøÁ≠ñÁï•‰∏ç‰æùËµñ‰∫é‰ªª‰ΩïÁâπÂÆöÁöÑÁâπÂæÅÂ≠êÈõÜÔºåSPSÂ¢ûÂº∫‰∫ÜÁ≠ñÁï•ÁöÑ**‰∏çÂèòÊÄßÂíåÈ≤ÅÊ£íÊÄß**„ÄÇ\n    *   **ÂèØÊâ©Â±ïÊÄß**ÔºöSPSË¢´ËØÅÊòéÂèØÂÖºÂÆπ‰∏çÂêåÁöÑÂü∫Á°ÄÊ®°ÂûãÔºàÂ¶ÇDINOÔºâÔºå‰∏îÊîØÊåÅ‰∏éÊñáÊú¨È©±Âä®ÁöÑÊΩúÂú®Á©∫Èó¥Êï∞ÊçÆÂ¢ûÂº∫ÁªìÂêàÔºåËé∑ÂæóÈ¢ùÂ§ñÊèêÂçá„ÄÇ\n    *   **ÂÆûÁî®ÊÄß**ÔºöÊïàÁéáÁöÑÊèêÂçáÂíåÊõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰ΩøÂÖ∂**Êó†ÈúÄÂæÆË∞ÉÂç≥ÂèØ‰ªé‰ªøÁúüËøÅÁßªÂà∞ÁúüÂÆûËΩ¶ËæÜ**ÔºåÂÆûÁî®ÊÄßÊõ¥Âº∫„ÄÇ\n\n**ÊÄªËÄåË®Ä‰πãÔºåÊú¨ÊñáÊòØÂØπÂâç‰Ωú‚ÄúDrive Anywhere‚ÄùÊ°ÜÊû∂ÁöÑ‰∏ÄÊ¨°Ê∑±Âàª‰ºòÂåñÂíåÂ¢ûÂº∫„ÄÇÂÆÉÊ≤°ÊúâÊîπÂèòÂà©Áî®Âü∫Á°ÄÊ®°ÂûãÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÔºåËÄåÊòØÈÄöËøá‰∏Ä‰∏™ÁÆÄÂçïËÄåÊúâÊïàÁöÑÈöèÊú∫ÂåñÊäÄÂ∑ßÔºåËß£ÂÜ≥‰∫ÜËØ•Ê°ÜÊû∂ÂÜÖÂú®ÁöÑÁâπÂæÅÂÜó‰ΩôÁº∫Èô∑Ôºå‰ªéËÄåÂú®ÊÄßËÉΩ„ÄÅÊïàÁéáÂíåÈ≤ÅÊ£íÊÄß‰∏äÂÆûÁé∞‰∫ÜÂÖ®Èù¢ÁöÑË∂ÖË∂ä„ÄÇ**",
      "timestamp": "2026-01-20T17:59:37.253344",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "ËÆ∫ÊñáÊèêÂá∫‰∫ÜÂì™‰∫õÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïÔºåËØ∑ÂàóË°®Ê†ºÂÖ∑‰ΩìËØ¶ÁªÜËØ¥ÊòéÊäÄÊúØÁªÜËäÇÔºåÈúÄË¶ÅÂåÖÂê´ÂÖ∑‰ΩìÁöÑÊï∞Â≠¶ÂéüÁêÜÊé®ÂØºÔºå‰ª•ÂèäÂÖ∑‰ΩìÂèÇÊï∞„ÄÇ",
      "answer": "## ËÆ∫ÊñáÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïÂàÜÊûê\n\nÊú¨ÊñáÁöÑÊ†∏ÂøÉÂàõÊñ∞ÊòØ**ÈöèÊú∫Ë°•‰∏ÅÈÄâÊã©ÔºàStochastic-Patch-Selection, SPSÔºâ**ÔºåÊó®Âú®Ëß£ÂÜ≥Âü∫‰∫éÂü∫Á°ÄÊ®°ÂûãÊèêÂèñÁöÑË°•‰∏ÅÁâπÂæÅÂÜó‰ΩôÈóÆÈ¢òÔºå‰ª•ÊèêÂçáÁ´ØÂà∞Á´ØËá™Âä®È©æÈ©∂Á≠ñÁï•ÁöÑÊ≥õÂåñÊÄßÂíåÊïàÁéá„ÄÇ\n\n### ÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïËØ¶Ëø∞\n\n| ÊñπÊ≥ïÂêçÁß∞ | Ê†∏ÂøÉÊÄùÊÉ≥ | ÊäÄÊúØÁªÜËäÇ‰∏éÊï∞Â≠¶ÂéüÁêÜ | ÂÖ≥ÈîÆÂèÇÊï∞‰∏éÂÆûÁé∞ |\n| :--- | :--- | :--- | :--- |\n| **1. ÈöèÊú∫Ë°•‰∏ÅÈÄâÊã© (SPS)** | Âú®ÊØè‰∏ÄÂ∏ßËæìÂÖ•Êó∂Ôºå**ÈöèÊú∫‰∏¢ÂºÉ**‰∏ÄÈÉ®ÂàÜË°•‰∏ÅÊèèËø∞Á¨¶Ôºå‰ªÖÂ∞ÜÂâ©‰ΩôÁöÑÊèèËø∞Á¨¶Ôºà‰øùÊåÅÂÖ∂ÂéüÂßãÁ©∫Èó¥Â∏ÉÂ±ÄÔºâËæìÂÖ•Á≠ñÁï•ÁΩëÁªú„ÄÇËøôËø´‰ΩøÁ≠ñÁï•Â≠¶‰π†Âü∫‰∫é‰∏çÂêå„ÄÅÈöèÊú∫‰ΩÜÂÆåÊï¥ÁöÑÂú∫ÊôØËßÜÂõæËøõË°åÂÜ≥Á≠ñÔºå‰ªéËÄåÂÖ≥Ê≥®**‰∏çÂèò‰∏îÂõ†ÊûúÁõ∏ÂÖ≥**ÁöÑÁ∫øÁ¥¢ÔºåËÄåÈùûËôöÂÅáÁõ∏ÂÖ≥ÊÄß„ÄÇ | **1. ÂÜó‰ΩôÈáèÂåñ (ÁêÜËÆ∫Âü∫Á°Ä):** <br> - ËÆæ‰ªéÂü∫Á°ÄÊ®°ÂûãÊèêÂèñÁöÑË°•‰∏ÅÁâπÂæÅÁü©Èòµ‰∏∫ \\( F_c \\in \\mathbb{R}^{N \\times D} \\)ÔºåÂÖ∂‰∏≠ \\( N = H' \\times W' \\) ‰∏∫Ë°•‰∏ÅÊï∞Ôºå\\( D \\) ‰∏∫ÁâπÂæÅÁª¥Â∫¶„ÄÇ<br> - ÈÄöËøá**‰∏ªÊàêÂàÜÂàÜÊûê(PCA)**ËÆ°ÁÆóÁ¥ØËÆ°Ëß£ÈáäÊñπÂ∑Æ \\( E(m) \\)Ôºö<br>   \\[ E(m) := \\frac{\\sum_{i=1}^{m} \\lambda_i}{\\sum_{i=1}^{r} \\lambda_i} \\]<br>   ÂÖ∂‰∏≠ \\( \\lambda_i \\) ‰∏∫ÂçèÊñπÂ∑ÆÁü©Èòµ \\( \\Sigma = \\frac{1}{N} \\tilde{F}_c^\\top \\tilde{F}_c \\) ÁöÑÁâπÂæÅÂÄºÔºå\\( r \\) ‰∏∫Áü©ÈòµÁß©„ÄÇËÆ∫ÊñáÂèëÁé∞BLIP2ÁâπÂæÅ‰ªÖÈúÄ **17/64** ‰∏™‰∏ªÊàêÂàÜÂç≥ÂèØËß£Èáä **90%** ÁöÑÊñπÂ∑ÆÔºåËØÅÊòéÂÖ∂È´òÂ∫¶ÂÜó‰Ωô„ÄÇ<br> - **Ë°•‰∏ÅÈó¥Áõ∏‰ººÊÄßÂàÜÊûê**: ËÆ°ÁÆóÊâÄÊúâË°•‰∏ÅÊèèËø∞Á¨¶Èó¥ÁöÑPearsonÁõ∏ÂÖ≥Á≥ªÊï∞Áü©ÈòµÊàñ‰ΩôÂº¶Áõ∏‰ººÂ∫¶ÔºåÁªìÊûúÊòæÁ§∫Â≠òÂú®ÂπøÊ≥õÁöÑÂº∫Áõ∏ÂÖ≥ÊÄßÔºàËßÅÂõæ3Ôºâ„ÄÇ<br><br>**2. SPSÁÆóÊ≥ïÊµÅÁ®ã:** <br> ÂØπ‰∫éÊØè‰∏ÄÂ∏ß \\( t \\)Ôºö<br> a. **ÈááÊ†∑Â≠êÈõÜ**: Âõ∫ÂÆöÈááÊ†∑Áéá \\( \\text{rate} \\in (0, 1] \\)ÔºåÈöèÊú∫ÂùáÂåÄÈÄâÊã© \\( k = \\lceil \\text{rate} \\cdot N \\rceil \\) ‰∏™Ë°•‰∏ÅÁ¥¢ÂºïÔºåËÆ∞‰∏∫ \\( \\Omega_t \\)„ÄÇ<br> b. **ËÆ°ÁÆóÊèèËø∞Á¨¶**: ‰ªÖÂØπ \\( \\Omega_t \\) ‰∏≠ÁöÑË°•‰∏ÅËøêË°å**Êé©Á†ÅÊ≥®ÊÑèÂäõÊèêÂèñÂô®**ÔºàËßÅ‰∏ãÊñá‚ÄúË°•‰∏ÅÂØπÈΩêÁâπÂæÅÊèêÂèñ‚ÄùÔºâ„ÄÇ<br> c. **ÊûÑÂª∫Á®ÄÁñèÂº†Èáè**: ÁªÑË£ÖËæìÂá∫Âº†Èáè \\( \\tilde{F}' \\in \\mathbb{R}^{H' \\times W' \\times D} \\)Ôºö<br>    \\[ \\tilde{F}'_i = \\begin{cases} F'_i, & i \\in \\Omega_t, \\\\ \\mathbf{0}, & \\text{otherwise} \\end{cases} \\]<br>    ÂÖ≥ÈîÆÁÇπÔºö**‰øùÁïôÂéüÂßãÁ©∫Èó¥ÁΩëÊ†ºÂ∏ÉÂ±Ä**ÔºåÁî®Èõ∂ÂêëÈáèÂ°´ÂÖÖË¢´Êé©Á†ÅÁöÑË°•‰∏Å‰ΩçÁΩÆ„ÄÇ | **ÂÖ≥ÈîÆÂèÇÊï∞:** <br> - **ÈááÊ†∑Áéá (rate)**: ËÆ∫Êñá‰∏ªË¶ÅÂÆûÈ™å‰ΩøÁî® **rate=0.5**ÔºàÂç≥‰øùÁïô50%Ë°•‰∏ÅÔºâ„ÄÇÂè¶ÊµãËØï‰∫Ü70%Âíå30%„ÄÇ<br> - **Ë°•‰∏ÅÊï∞ \\( N \\)**: ÂèñÂÜ≥‰∫éÂü∫Á°ÄÊ®°ÂûãÁöÑViTÈÖçÁΩÆÔºå‰æãÂ¶ÇBLIP2ÂèØËÉΩ‰∫ßÁîü \\( 16 \\times 16 = 256 \\) ‰∏™Ë°•‰∏Å„ÄÇ<br> - **Êé©Á†ÅÊ≥®ÊÑèÂäõÂèÇÊï∞ \\( r \\)**: Âú®ÁâπÂæÅÊèêÂèñÈò∂ÊÆµ‰ΩøÁî®ÔºåÊéßÂà∂Êé©Á†ÅÂº∫Â∫¶ÔºàÊù•Ëá™Drive-AnywhereÊñπÊ≥ïÔºâÔºåÈÄöÂ∏∏ \\( r < 0 \\)ÔºàÂ¶Ç-100Ôºâ„ÄÇ<br><br>**ÂÆûÁé∞:** ‰Ωú‰∏∫Âü∫Á°ÄÊ®°ÂûãÔºàÂ¶ÇBLIP2ÔºâÂíåËΩªÈáèÁ∫ßÁ≠ñÁï•ÁΩëÁªú‰πãÈó¥ÁöÑ**Âç≥ÊèíÂç≥Áî®**Â±Ç„ÄÇÊó†ÈúÄÂæÆË∞ÉÂü∫Á°ÄÊ®°Âûã„ÄÇ |\n| **2. Ë°•‰∏ÅÂØπÈΩêÁâπÂæÅÊèêÂèñ (Âü∫Á°Ä)** | ÈááÁî® **Drive-Anywhere [3]** ÁöÑÊñπÊ≥ïÔºå‰ªéÂÜªÁªìÁöÑÂ§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÔºàÂ¶ÇBLIP2Ôºâ‰∏≠ÊèêÂèñÊØè‰∏™ÂõæÂÉèË°•‰∏ÅÁöÑÁã¨Á´ãÊèèËø∞Á¨¶„ÄÇÊ†∏ÂøÉÊòØ‰ΩøÁî®**Êé©Á†ÅÊ≥®ÊÑèÂäõ**Âº∫Âà∂Ê®°ÂûãÂÖ≥Ê≥®Âçï‰∏™Ë°•‰∏Å„ÄÇ | **Êï∞Â≠¶ÂéüÁêÜ:** <br> 1. ËÆæÂü∫Á°ÄÊ®°Âûã \\( \\text{Desc} \\) Êúâ \\( L \\) Â±Ç„ÄÇÂØπ‰∫éËæìÂÖ•ÂõæÂÉè \\( F \\)ÔºåÂú®Á¨¨ \\( \\ell \\) Â±ÇËÆ°ÁÆóÊü•ËØ¢ \\( Q \\)„ÄÅÈîÆ \\( K \\)„ÄÅÂÄº \\( V \\) Áü©Èòµ„ÄÇ<br> 2. ‰∏∫ÊèêÂèñÁ¨¨ \\( j \\) ‰∏™Ë°•‰∏ÅÁöÑÁâπÂæÅÔºåÂÆö‰πâ‰∏Ä‰∏™**‰∫åËøõÂà∂Êé©Á†ÅÂêëÈáè** \\( m^{(j)} \\in \\{0,1\\}^N \\)ÔºåÂÖ∂‰∏≠ \\( m^{(j)}_i = 1 \\) Ë°®Á§∫‰øùÁïôÊù•Ëá™Ë°•‰∏Å \\( i \\) ÁöÑ‰ø°ÊÅØ„ÄÇ<br> 3. ‰øÆÊîπÊ≥®ÊÑèÂäõÂØπÊï∞ \\( G^\\ell = Q^\\ell (K^\\ell)^\\top \\)ÔºåÂä†ÂÖ•Êé©Á†ÅÔºö<br>    \\[ \\tilde{G^\\ell} = G^\\ell + (\\mathbf{1} - M^{(j)}) r \\]<br>    ÂÖ∂‰∏≠ \\( M^{(j)} = [m^{(j)}, \\dots, m^{(j)}]^\\top \\)Ôºå\\( r \\) ÊòØ‰∏Ä‰∏™Â§ßÁöÑË¥üÂÄºÔºàÂ¶Ç-100Ôºâ„ÄÇËøôÂ∞ÜË¢´Êé©Á†Å‰ΩçÁΩÆÔºà\\( m_i \\approx 0 \\)ÔºâÁöÑÊ≥®ÊÑèÂäõÂàÜÊï∞Êé®ÂêëË¥üÊó†Á©∑Ôºå‰ªéËÄåÂú®ÂêéÁª≠softmax‰∏≠Ë¢´ÊäëÂà∂„ÄÇ<br> 4. ËÆ°ÁÆóÊé©Á†ÅÊ≥®ÊÑèÂäõËæìÂá∫Âπ∂ÁªßÁª≠ÂâçÂêë‰º†Êí≠Ôºö<br>    \\[ F^{\\prime(j)} = \\text{Desc}^{\\ell \\rightarrow} \\bigl( \\text{SoftMax}(\\tilde{G}) V^\\ell \\bigr) \\]<br> 5. ÂØπÊâÄÊúâ \\( j \\in [N] \\) ÈáçÂ§çÊ≠§ËøáÁ®ãÔºåÂæóÂà∞ÂÆåÊï¥ÁöÑË°•‰∏ÅÁâπÂæÅÂº†Èáè \\( F' \\)„ÄÇ | **ÂÖ≥ÈîÆÂèÇÊï∞:** <br> - **Êé©Á†ÅËÆæËÆ° \\( m^{(j)} \\)**: ÂÆö‰πâÂõ¥ÁªïÁõÆÊ†áË°•‰∏Å \\( j \\) ÁöÑÂ±ÄÈÉ®Âå∫Âüü„ÄÇÂèØ‰ª•ÊòØÁ°¨Êà™Êñ≠ \\( \\mathbf{1}[d_{ij} \\le \\alpha] \\)ÔºåÊàñËΩØË°∞ÂáèÂ¶Ç \\( 2^{-d_{ij}} \\)ÔºåÂÖ∂‰∏≠ \\( d_{ij} \\) ÊòØË°•‰∏ÅÁΩëÊ†º‰∏≠ÁöÑÊ¨ßÊ∞èË∑ùÁ¶ª„ÄÇ<br> - **Êé©Á†ÅÂ±Ç \\( \\ell \\)**: ÈÄöÂ∏∏ÈÄâÊã©Âú®BLIP2ÁöÑQ-FormerËæìÂÖ•Â±ÇËøõË°åÊé©Á†Å„ÄÇ<br> - **ÊäëÂà∂Âº∫Â∫¶ \\( r \\)**: Ë¥üÂÄºÔºåÁªùÂØπÂÄºË∂äÂ§ßÊé©Á†ÅÊïàÊûúË∂äÂº∫„ÄÇ |\n| **3. SPSÂèò‰Ωì (Ê∂àËûçÁ†îÁ©∂)** | Êé¢Á¥¢‰∏çÂêåÁöÑÊé©Á†ÅÂíåÈáçÁªÑÁ≠ñÁï•Ôºå‰ª•ÁêÜËß£ËÆæËÆ°Á©∫Èó¥Âπ∂‰ºòÂåñÊÄßËÉΩ„ÄÇ | **V1: ÈòàÂÄºÊé©Á†Å (Threshold Masking)** <br> - ÊØè‰∏™Ë°•‰∏Å \\( i \\) ‰ª•Ê¶ÇÁéá \\( \\text{rate} \\) Ë¢´Áã¨Á´ãÈÄâ‰∏≠Ôºö\\( \\Omega_t = \\{ i \\mid R_i \\le \\text{rate} \\} \\)ÔºåÂÖ∂‰∏≠ \\( R_i \\sim \\mathcal{U}(0,1) \\)„ÄÇ<br> - ‰øùÁïôÁöÑË°•‰∏ÅÊï∞ÈáèÊòØÊ≥¢Âä®ÁöÑÔºåÂ¢ûÂä†‰∫ÜÈöèÊú∫Â§öÊ†∑ÊÄß„ÄÇ<br><br>**V2: ‰ΩçÁΩÆË∞ÉÊï¥ÁöÑÁ®ÄÁñèÂ∫èÂàó (Position-Adjusted Sparse Sequence)** <br> - Âú®ÈÄâ‰∏≠ \\( \\Omega_t \\) ÂêéÔºå**ÂÆåÂÖ®ÁßªÈô§**Ë¢´‰∏¢ÂºÉÁöÑ‰ª§ÁâåÔºå‰∏ç‰øùÁïôÈõ∂Â°´ÂÖÖ„ÄÇ<br> - Â∞ÜÈÄâ‰∏≠ÁöÑÊèèËø∞Á¨¶ \\( F'_i \\) ‰∏éÂÖ∂Âú®ViTÂéüÂßã**‰ΩçÁΩÆÁºñÁ†ÅË°®**‰∏≠ÂØπÂ∫îÁöÑ‰ΩçÁΩÆÂµåÂÖ• \\( p_i \\) ÊãºÊé•ÔºåÂΩ¢Êàê‰∏Ä‰∏™**ÂèØÂèòÈïøÂ∫¶**ÁöÑËæìÂÖ•Â∫èÂàó‰º†ÈÄíÁªôÁ≠ñÁï•ÁΩëÁªú„ÄÇ<br> - ËøôËøõ‰∏ÄÊ≠•ÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÈáè„ÄÇ | **Âèò‰ΩìÂëΩÂêç:** <br> - **SPS**: Âü∫Á°ÄÁâàÊú¨ÔºåÈõ∂Â°´ÂÖÖ„ÄÇ<br> - **SPPS (Structured Patch Pruning with Selection)**: ÂØπÂ∫îV2ÔºåÁßªÈô§‰ª§ÁâåÂπ∂ÈáçÁªÑ„ÄÇ<br> - **MSPPS (Matrix-based SPPS)**: ÁªìÂêàV1ÂíåV2ÔºåÂü∫‰∫éÊ¶ÇÁéáÊé©Á†ÅËøõË°åÈöèÊú∫Ââ™ÊûùÂπ∂ÈáçÁªÑ„ÄÇ |\n| **4. ÁêÜËÆ∫‰øùËØÅ: Â≠êÁ©∫Èó¥‰øùÊåÅÂºïÁêÜ** | ‰ªéÁêÜËÆ∫‰∏äËØÅÊòéÔºåÂú®ÁâπÂæÅÂÜó‰ΩôÔºà‰ΩéÁß©Ôºâ‰∏îË°åÁõ∏Âπ≤ÊÄßÊúâÁïåÁöÑÊù°‰ª∂‰∏ãÔºåÂùáÂåÄÈöèÊú∫ÈááÊ†∑Ë∂≥Â§üÂ§öÁöÑË°•‰∏ÅË°åÔºåËÉΩ‰ª•È´òÊ¶ÇÁéá‰øùÁïôÂéüÂßãÁâπÂæÅÁü©ÈòµÁöÑ**‰∏ªË°åÂ≠êÁ©∫Èó¥**Ôºå‰ªéËÄå‰øùÊåÅÂú∫ÊôØËØ≠‰πâ„ÄÇ | **ÂºïÁêÜ1 (ÁÆÄÂåñ):** <br> ËÆæ‰∏≠ÂøÉÂåñÁâπÂæÅÁü©Èòµ \\( F_c \\) ÁöÑÁß©‰∏∫ \\( r \\)ÔºåÂÖ∂Ë°åÁõ∏Âπ≤ÊÄß‰∏∫ \\( \\mu \\)„ÄÇÈöèÊú∫ÂùáÂåÄÈááÊ†∑ \\( m \\) Ë°åÂæóÂà∞Â≠êÁü©Èòµ \\( F_{\\mathcal{I}} \\)„ÄÇÂ≠òÂú®Â∏∏Êï∞ \\( C > 0 \\)Ôºå‰ΩøÂæóÂØπ‰∫é‰ªªÊÑè \\( \\varepsilon, \\delta \\in (0,1) \\)ÔºåÂ¶ÇÊûú<br> \\[ m \\ge C \\frac{\\mu r}{\\varepsilon^2} \\log(\\frac{r}{\\delta}) \\]<br> Âàô‰ª•Ëá≥Â∞ë \\( 1-\\delta \\) ÁöÑÊ¶ÇÁéáÔºåÈááÊ†∑ÂâçÂêéË°åÁ©∫Èó¥ÊäïÂΩ±ÁÆóÂ≠êÁöÑË∞±ËåÉÊï∞ËØØÂ∑ÆÊª°Ë∂≥Ôºö<br> \\[ \\| \\Pi_F - \\Pi_{F_{\\mathcal{I}}} \\|_2 \\le \\varepsilon \\]<br> ËøôÊÑèÂë≥ÁùÄÈááÊ†∑ÂêéÁöÑÁâπÂæÅ‰ªçÁÑ∂Âº†Êàê‰∏éÂéüÂßãÊï∞ÊçÆËøë‰ººÁõ∏ÂêåÁöÑËØ≠‰πâÂ≠êÁ©∫Èó¥„ÄÇ | **ÂÆûÈôÖÊÑè‰πâ:** <br> ËØ•ÂºïÁêÜ‰∏∫SPSÁöÑÊúâÊïàÊÄßÊèê‰æõ‰∫ÜÊï∞Â≠¶Âü∫Á°Ä„ÄÇÁî±‰∫éË°•‰∏ÅÁâπÂæÅÊú¨Ë¥®‰∏äÊòØ‰ΩéÁß©ÁöÑÔºàPCAËØÅÊòéÔºâÔºåÂõ†Ê≠§Âç≥‰ΩøÈöèÊú∫‰∏¢ÂºÉÂ§ßÈÉ®ÂàÜË°•‰∏ÅÔºåÂâ©‰ΩôË°•‰∏ÅÊâÄÊê∫Â∏¶ÁöÑ‰ø°ÊÅØ",
      "timestamp": "2026-01-20T18:00:41.321142",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "‰ªñ‰ΩøÁî®‰∫ÜÂì™‰∫õËØÑ‰ª∑ÊåáÊ†á‰∏éÊï∞ÊçÆÈõÜÔºåÂàóË°®Ê†ºÂÖ∑‰ΩìËÆ≤ËÆ≤‰ªñÁöÑËØÑ‰ª∑ÊåáÊ†áÁöÑÁªÜËäÇ‰∏éÊï∞ÊçÆÈõÜÁöÑÁªÜËäÇ",
      "answer": "## ËÆ∫ÊñáËØÑ‰ª∑ÊåáÊ†á‰∏éÊï∞ÊçÆÈõÜÂàÜÊûê\n\n### ‰∏Ä„ÄÅ ‰∏ªË¶ÅËØÑ‰ª∑ÊåáÊ†á\n\nËÆ∫ÊñáÈááÁî®**Èó≠ÁéØÈ©æÈ©∂ÊàêÂäüÁéá**‰Ωú‰∏∫Ê†∏ÂøÉËØÑ‰ª∑ÊåáÊ†áÔºåÂÖ∑‰ΩìÂÆö‰πâÂíåÁªÜËäÇÂ¶Ç‰∏ãÔºö\n\n| ÊåáÊ†áÂêçÁß∞ | ËÆ°ÁÆóÊñπÂºè | ÁªÜËäÇËØ¥Êòé |\n| :--- | :--- | :--- |\n| **ÂΩí‰∏ÄÂåñÊàêÂäüÊåÅÁª≠Êó∂Èó¥** | `ÊàêÂäüÈ©æÈ©∂Êó∂Èó¥ / ÊÄªÊµãËØïÊó∂Èó¥`ÔºåÂú®Â§ö‰∏™ÊµãËØïÂõûÂêà‰∏≠ÂèñÂπ≥Âùá„ÄÇ | 1. **Â§±Ë¥•Êù°‰ª∂**ÔºöÊª°Ë∂≥‰ª•‰∏ã‰ªª‰∏ÄÂç≥Âà§ÂÆö‰∏∫Â§±Ë¥•Ôºö<br>   - **ËΩ¶ÈÅìÂÅèÁ¶ª**ÔºöËΩ¶ËæÜË∑®Ë∂äËΩ¶ÈÅìËæπÁïå„ÄÇ<br>   - **Á¢∞Êíû/Âç±Èô©Êé•Ëøë**Ôºö‰∏éÈöúÁ¢çÁâ©ÂèëÁîüÁ¢∞ÊíûÊàñË∑ùÁ¶ªËøáËøë„ÄÇ<br>   - **Ëà™ÂêëÂÅèÂ∑Æ**ÔºöËΩ¶ËæÜËà™Âêë‰∏éËΩ¶ÈÅìÊñπÂêëÂÅèÂ∑ÆË∂ÖËøá30¬∞„ÄÇ<br>2. **ÊµãËØïÂçèËÆÆ**ÔºöÂú®Ê®°ÊãüÁéØÂ¢É‰∏≠ÔºåËøõË°å**100‰∏™ÂõûÂêà**ÁöÑÊµãËØïÔºåÊØè‰∏™ÂõûÂêàÁ∫¶20ÁßíÔºåËÆ°ÁÆóÂπ≥ÂùáÊàêÂäüÁéá„ÄÇ |\n| **ÂÆâÂÖ®ÂëòÂπ≤È¢ÑÊ¨°Êï∞** | Âú®ÂÆûËΩ¶ÊµãËØï‰∏≠ÔºåÁªüËÆ°Âõ†Ëß¶Âèë‰∏äËø∞Â§±Ë¥•Êù°‰ª∂ËÄåÈúÄË¶Å‰∫∫Á±ªÂÆâÂÖ®ÂëòÊé•ÁÆ°ÁöÑÊ¨°Êï∞„ÄÇ | Áî®‰∫éËØÑ‰º∞Ê®°Âûã‰ªé‰ªøÁúüÂà∞Áé∞ÂÆû‰∏ñÁïåÔºàSim-to-RealÔºâÁöÑÈõ∂Ê†∑Êú¨ËøÅÁßªËÉΩÂäõ„ÄÇÂπ≤È¢ÑË∂äÂ∞ëÔºåÊÄßËÉΩË∂äÂ•Ω„ÄÇ |\n\n**ÊåáÊ†áÁâπÁÇπ**Ôºö\n- **ÁªºÂêàÊÄß**ÔºöËØ•ÊåáÊ†áËûçÂêà‰∫ÜË∑ØÂæÑË∑üË∏™ÔºàËΩ¶ÈÅì‰øùÊåÅÔºâ„ÄÅÂÆâÂÖ®ÊÄßÔºàÈÅøÈöúÔºâÂíåÁ®≥ÂÆöÊÄßÔºàËà™ÂêëÊéßÂà∂ÔºâÁ≠âÂ§ö‰∏™Ëá™Âä®È©æÈ©∂Ê†∏ÂøÉ‰ªªÂä°„ÄÇ\n- **ÂÆûÁî®ÊÄß**ÔºöÁõ¥Êé•ÂèçÊò†‰∫ÜÁ≠ñÁï•Âú®**Èó≠ÁéØ„ÄÅ‰∫§‰∫íÂºèÁéØÂ¢É**‰∏≠ÁöÑÈïøÊúüÂèØÈù†ÊÄßÂíåÂÆâÂÖ®ÊÄßÔºåÊØîÂºÄÁéØÊåáÊ†áÔºàÂ¶ÇËΩ®ËøπËØØÂ∑ÆÔºâÊõ¥ÂÖ∑ÂÆûÈôÖÊÑè‰πâ„ÄÇ\n- **ÂèØÂØπÊØîÊÄß**Ôºö‰∏éÂü∫Á∫øÊñπÊ≥ïÔºàDrive AnywhereÁ≠âÔºâ‰ΩøÁî®Áõ∏ÂêåÁöÑËØÑ‰ª∑ÂçèËÆÆÔºåÁ°Æ‰øù‰∫ÜÊØîËæÉÁöÑÂÖ¨Âπ≥ÊÄß„ÄÇ\n\n### ‰∫å„ÄÅ ËÆ≠ÁªÉ‰∏éÊµãËØïÊï∞ÊçÆÈõÜ\n\nËÆ∫Êñá‰ΩøÁî®‰∫Ü**Ê∑∑ÂêàÊï∞ÊçÆÊ∫ê**ËøõË°åËÆ≠ÁªÉÔºåÂπ∂Âú®**Á≤æÂøÉËÆæËÆ°ÁöÑÂàÜÂ∏ÉÂ§ñÔºàOODÔºâÂú∫ÊôØ**‰∏≠ËøõË°åËØÑ‰º∞„ÄÇ\n\n#### 1. ËÆ≠ÁªÉÊï∞ÊçÆ\n| Êï∞ÊçÆÊù•Ê∫ê | ÊèèËø∞ | Áî®ÈÄî‰∏éÁªÜËäÇ |\n| :--- | :--- | :--- |\n| **VISTA 2.0 Ê®°ÊãüÂô®** | ‰∏Ä‰∏™Âü∫‰∫éÁúüÂÆûÈ©æÈ©∂Êó•ÂøóÊûÑÂª∫ÁöÑÂÖâÂ≠¶ÈÄºÁúü„ÄÅÊï∞ÊçÆÈ©±Âä®ÁöÑÊ®°ÊãüÂô®„ÄÇ | - **Âü∫Á°ÄÊï∞ÊçÆ**ÔºöÁ∫¶**2Â∞èÊó∂**ÁöÑÁúüÂÆû‰∏ñÁïåÈ©æÈ©∂ËÆ∞ÂΩïÔºåÊ∂µÁõñ‰∫ÜÂ§öÁßçÁéØÂ¢É„ÄÅÂÖâÁÖßÂíåÂ§©Ê∞î„ÄÇ<br>- **Êï∞ÊçÆÂ¢ûÂº∫**ÔºöÂà©Áî®Ê®°ÊãüÂô®ÁöÑËÉΩÂäõÔºå‰ªéÂéüÂßãÊó•ÂøóÁîüÊàê**Â§öÊ†∑ÂåñÁöÑÈ©æÈ©∂‰ΩìÈ™å**ÔºåÁî®‰∫éÂ¢ûÂä†ËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂàÜÂ∏ÉÂπøÂ∫¶„ÄÇ<br>- **ÁõëÁù£‰ø°Âè∑**ÔºöÈááÁî®**ÂºïÂØºÁ≠ñÁï•Â≠¶‰π†ÔºàGuided Policy LearningÔºâ**ÔºåÂà©Áî®Ê®°ÊãüÂô®Êèê‰æõÁöÑÁâπÊùÉ‰ø°ÊÅØÔºàÂ¶ÇÁ≤æÁ°ÆÁöÑËΩ¶ÈÅìÁ∫ø„ÄÅÈöúÁ¢çÁâ©‰ΩçÁΩÆÔºâÈÄöËøáPIDÊéßÂà∂Âô®ÂíåÂü∫‰∫éÊéßÂà∂Â±èÈöúÂáΩÊï∞ÔºàCBFÔºâÁöÑÂÆâÂÖ®ÊéßÂà∂Âô®Êù•ÁîüÊàêÊéßÂà∂Ê†áÁ≠æÔºàËΩ¨ÂêëÂíåÊ≤πÈó®Ôºâ„ÄÇ |\n| **ÁúüÂÆûÈ©æÈ©∂Êó•Âøó** | ÂÆûÈôÖÈááÈõÜÁöÑËΩ¶ËæÜ‰º†ÊÑüÂô®Êï∞ÊçÆ„ÄÇ | ‰Ωú‰∏∫VISTAÊ®°ÊãüÂô®ÊûÑÂª∫ÁöÑÂü∫Á°ÄÔºåÊèê‰æõ‰∫ÜÁúüÂÆûÁöÑÈ©æÈ©∂Ë°å‰∏∫ÂíåÁéØÂ¢ÉÂàÜÂ∏É„ÄÇ |\n\n#### 2. ÊµãËØïÂú∫ÊôØÔºàOODËØÑ‰º∞Âü∫ÂáÜÔºâ\nÊµãËØïÊó®Âú®ËØÑ‰º∞Ê®°ÂûãÂú®**Êú™ËßÅËøáÁöÑ„ÄÅÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊù°‰ª∂**‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËÆ≠ÁªÉÁéØÂ¢ÉËÆæÂÆö‰∏∫Âõ∫ÂÆöÁöÑ‚Äú**‰π°ÊùëÈÅìË∑Ø„ÄÅÂ§èÂ≠£„ÄÅÂπ≤Áá•„ÄÅÁôΩÂ§©„ÄÅÊúâËΩ¶ËæÜ**‚Äù„ÄÇÊµãËØïÂàôÁ≥ªÁªüÊÄßÂú∞ÊîπÂèò‰ª•‰∏ã‰∏Ä‰∏™ÊàñÂ§ö‰∏™Áª¥Â∫¶Ôºö\n\n| Âú∫ÊôØÁª¥Â∫¶ | ÊµãËØïÂèñÂÄºÔºàOODÁ§∫‰æãÔºâ | ËØ¥Êòé |\n| :--- | :--- | :--- |\n| **Âú∫ÊôØÁ±ªÂûã** | ‰π°Êùë„ÄÅ**ÂüéÂ∏Ç** | ‰ªéËÆ≠ÁªÉÁéØÂ¢ÉÔºà‰π°ÊùëÔºâÂàáÊç¢Âà∞ÁªìÊûÑÊõ¥Â§çÊùÇ„ÄÅÂä®ÊÄÅÁâ©‰ΩìÊõ¥Â§öÁöÑÂüéÂ∏ÇÁéØÂ¢É„ÄÇ |\n| **Â≠£ËäÇ** | Â§èÂ≠£„ÄÅ**Êò•Â≠£„ÄÅÁßãÂ≠£„ÄÅÂÜ¨Â≠£** | ÊµãËØïÊ§çË¢´„ÄÅÂÖâÁÖßÂíåÊï¥‰ΩìÂú∫ÊôØÂ§ñËßÇÁöÑÂ∑®Â§ßÂèòÂåñ„ÄÇ |\n| **Â§©Ê∞î** | Âπ≤Áá•„ÄÅ**‰∏ãÈõ™„ÄÅ‰∏ãÈõ®** | ÊµãËØïÈôçÈõ™„ÄÅÈôçÈõ®Á≠âÊûÅÁ´ØÂ§©Ê∞îÂØπËÉΩËßÅÂ∫¶ÂíåË∑ØÈù¢ÂèçÂÖâÁöÑÂΩ±Âìç„ÄÇ |\n| **Êó∂Èó¥** | ÁôΩÂ§©„ÄÅ**Â§úÊôö** | ÊµãËØï‰ΩéÂÖâÁÖßÊù°‰ª∂‰∏ãÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇ |\n| **Âä®ÊÄÅÂèÇ‰∏éËÄÖ** | ËΩ¶ËæÜ„ÄÅ**Âä®Áâ©** | ÊµãËØïÂØπËÆ≠ÁªÉ‰∏≠Êú™Âá∫Áé∞ËøáÁöÑÈöúÁ¢çÁâ©Á±ªÂûãÔºàÂ¶ÇÈπøÔºâÁöÑËØÜÂà´‰∏éÈÅøËÆ©ËÉΩÂäõ„ÄÇ |\n| **ËΩ¶ËæÜÁ±ªÂûã** | ËÆ≠ÁªÉÈõÜËΩ¶ËæÜ„ÄÅ**Êú™ËßÅËøáÁöÑËΩ¶ËæÜÁ±ªÂûã‚Ä†** | ÊµãËØïÂØπÈôåÁîüËΩ¶ËæÜÂ§ñËßÇÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ |\n\n**ËØÑ‰ª∑ÊµÅÁ®ãÁªÜËäÇ**Ôºö\n1.  **Âõ∫ÂÆöËÆ≠ÁªÉÂàÜÂ∏É**ÔºöÊâÄÊúâÊñπÊ≥ïÈÉΩÂú®ÂÆåÂÖ®Áõ∏ÂêåÁöÑ‚Äú‰π°Êùë-Â§èÂ≠£-Âπ≤Áá•-ÁôΩÂ§©-ËΩ¶ËæÜ‚ÄùÊù°‰ª∂‰∏ãËÆ≠ÁªÉ„ÄÇ\n2.  **Á≥ªÁªüÂåñOODÊµãËØï**ÔºöÂ¶ÇË°®1ÊâÄÁ§∫ÔºåÂÖ±ËÆæËÆ°‰∫Ü**13‰∏™**‰∏çÂêåÁöÑOODÊµãËØïÊù°‰ª∂ÔºåË¶ÜÁõñ‰∫Ü‰∏äËø∞ÊâÄÊúâÁª¥Â∫¶ÁöÑÁªÑÂêàÔºåÁ°Æ‰øùËØÑ‰º∞ÁöÑÂÖ®Èù¢ÊÄß„ÄÇ\n3.  **Âü∫ÂáÜÂØπÊØî**ÔºöÂ∞ÜÊèêÂá∫ÁöÑSPSÊñπÊ≥ï‰∏éÂ§ö‰∏™Âº∫Âü∫Á∫øÔºàNo-FM, I-ViT, MF, Drive AnywhereÔºâÂú®ÊâÄÊúâËøô‰∫õOODÂú∫ÊôØ‰∏ãËøõË°åÂØπÊØîÔºåËÆ°ÁÆó**Âπ≥ÂùáÊàêÂäüÁéá**‰Ωú‰∏∫ÊúÄÁªàÊÄßËÉΩÊÄªÁªì„ÄÇ\n\n### ÊÄªÁªì\nËØ•ËÆ∫ÊñáÁöÑËØÑ‰ª∑‰ΩìÁ≥ªËÆæËÆ°Á≤æËâØÔºö\n- **ÊåáÊ†áÂ±ÇÈù¢**ÔºåÈááÁî®Ë¥¥ËøëÂÆûÈôÖÂ∫îÁî®ÁöÑÈó≠ÁéØÊàêÂäüÁéáÔºåÂÆö‰πâ‰∏•Ê†º‰∏îÂêàÁêÜ„ÄÇ\n- **Êï∞ÊçÆÈõÜÂ±ÇÈù¢**ÔºåÂà©Áî®Ê®°ÊãüÂô®È´òÊïàÁîüÊàêÂ§öÊ†∑ÂåñÊï∞ÊçÆÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏ÄÂ•óÁ≥ªÁªüÊÄßÁöÑ„ÄÅË¶ÜÁõñÂ§öÁª¥Â∫¶ÂàÜÂ∏ÉÂÅèÁßªÁöÑOODÊµãËØïÂü∫ÂáÜÔºåÊúâÂäõÂú∞È™åËØÅ‰∫ÜÊñπÊ≥ïÂú®**Ê≥õÂåñÊÄß„ÄÅÈ≤ÅÊ£íÊÄß**ÊñπÈù¢ÁöÑÊèêÂçá„ÄÇÂÖ∂ÂÆûËΩ¶Èõ∂Ê†∑Êú¨ÈÉ®ÁΩ≤ÂÆûÈ™åËøõ‰∏ÄÊ≠•ËØÅÊòé‰∫ÜËØ•ËØÑ‰º∞‰ΩìÁ≥ªÁöÑÊúâÊïàÊÄßÂíåÊñπÊ≥ïÁöÑÂÆûÁî®‰ª∑ÂÄº„ÄÇ",
      "timestamp": "2026-01-20T18:01:18.158111",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "ËÆ∫ÊñáÂú®Âì™‰∫õÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºü‰∏ªË¶ÅÁöÑËØÑ‰º∞ÊåáÊ†áÂíåÊÄßËÉΩÊèêÂçáÊòØÂ§öÂ∞ëÔºü",
      "answer": "## ËÆ∫ÊñáÂÆûÈ™å‰∏éÊÄßËÉΩÂàÜÊûê\n\n### Êï∞ÊçÆÈõÜ‰∏éÂÆûÈ™åËÆæÁΩÆ\nËÆ∫ÊñáÂú®**‰ªøÁúüÁéØÂ¢É**Âíå**ÁúüÂÆû‰∏ñÁïå**‰∏§‰∏™Â±ÇÈù¢ËøõË°å‰∫ÜÂÆûÈ™åÔºö\n1.  **‰ªøÁúüÁéØÂ¢É**Ôºö‰∏ªË¶Å‰ΩøÁî® **VISTA 2.0** Ê®°ÊãüÂô®„ÄÇËÆ≠ÁªÉÊï∞ÊçÆÁªìÂêà‰∫ÜÁ∫¶2Â∞èÊó∂ÁöÑ**ÁúüÂÆû‰∏ñÁïåÈ©æÈ©∂Êó•Âøó**ÔºåÂπ∂ÈÄöËøáVISTAÂú®Â§öÁßçÁéØÂ¢É„ÄÅÂÖâÁÖßÂíåÂ§©Ê∞îÊù°‰ª∂‰∏ãÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊ®°ÊãüÁªèÈ™å„ÄÇ\n2.  **ÁúüÂÆû‰∏ñÁïåÈÉ®ÁΩ≤**ÔºöÂú®‰∏ÄËæÜÈÖçÂ§á‰º†ÊÑüÂô®ÂíåËÆ°ÁÆóÂπ≥Âè∞ÁöÑ **2019Ê¨æÈõ∑ÂÖãËê®ÊñØRX 450H** ÂÆûËΩ¶‰∏äËøõË°åÊµãËØïÔºåÂú∞ÁÇπÂåÖÊã¨‰π°ÊùëÂÖ¨Âõ≠ÈÅìË∑ØÂíåÂú∞‰∏ãÂÅúËΩ¶Âú∫„ÄÇ\n\n### ‰∏ªË¶ÅËØÑ‰º∞ÊåáÊ†á\n1.  **Ê†∏ÂøÉÊåáÊ†á**Ôºö**ÂΩí‰∏ÄÂåñÊàêÂäüÊåÅÁª≠Êó∂Èó¥**„ÄÇÂú®‰ªøÁúü‰∏≠ÔºåË°°ÈáèËΩ¶ËæÜÂú®Ëß¶ÂèëÂ§±Ë¥•Êù°‰ª∂ÔºàÂ¶ÇÂéãÁ∫ø„ÄÅÁ¢∞Êíû„ÄÅËà™ÂêëÂÅèÂ∑Æ>30¬∞ÔºâÂâçËÉΩÂÆâÂÖ®Ë°åÈ©∂ÁöÑÊó∂Èó¥ÔºåÂú®100‰∏™Á∫¶20ÁßíÁöÑÁâáÊÆµ‰∏äÂèñÂπ≥Âùá„ÄÇ\n2.  **ÂÆûËΩ¶ÊåáÊ†á**Ôºö**ÂÆâÂÖ®È©æÈ©∂ÂëòÂπ≤È¢ÑÊ¨°Êï∞**Ôºå‰æùÊçÆ‰∏é‰ªøÁúüÁõ∏ÂêåÁöÑÂ§±Ë¥•Êù°‰ª∂ËøõË°åÁªüËÆ°„ÄÇ\n3.  **ÊïàÁéáÊåáÊ†á**ÔºöÊ®°Âûã**Êé®ÁêÜÈÄüÂ∫¶ÁöÑÂä†ÈÄüÊØî**Ôºà‰∏éÂü∫Á∫øÁõ∏ÊØîÔºâ„ÄÇ\n4.  **ÂàÜÊûêÊåáÊ†á**Ôºö‰ΩøÁî®**‰∏ªÊàêÂàÜÂàÜÊûêÔºàPCAÔºâËß£ÈáäÊñπÂ∑Æ**Âíå**ÂùóÈó¥Áõ∏‰ººÊÄß/Áõ∏ÂÖ≥ÊÄß**Êù•ÈáèÂåñÁâπÂæÅÂÜó‰Ωô„ÄÇ\n\n### ÊÄßËÉΩÊèêÂçá‰∏éÂÖ≥ÈîÆÁªìÊûú\nËÆ∫ÊñáÊèêÂá∫ÁöÑ **SPSÔºàStochastic Patch SelectionÔºâÊñπÊ≥ï** Âú®Â§ö‰∏™Áª¥Â∫¶‰∏äÊòæËëóË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑSOTAÊñπÊ≥ïÔºàDrive Anywhere, DAÔºâÔºö\n\n| ÊØîËæÉÁª¥Â∫¶ | ÊÄßËÉΩÊèêÂçá | ÂÖ≥ÈîÆÁªÜËäÇ |\n| :--- | :--- | :--- |\n| **OODÊ≥õÂåñÊÄßËÉΩ** | **Âπ≥ÂùáÊèêÂçá +6.2%** | Âú®Ê∂µÁõñÂ≠£ËäÇ„ÄÅÂ§©Ê∞î„ÄÅÂÖâÁÖß„ÄÅÂú∫ÊôØÔºàÂüé‰π°Ôºâ„ÄÅÂä®ÊÄÅÂèÇ‰∏éËÄÖÔºàËΩ¶ËæÜ/Âä®Áâ©ÔºâÁöÑÂÖ®ÈÉ®OODÂú∫ÊôØ‰∏≠„ÄÇ |\n| | **ÊúÄÈ´òÊèêÂçá +20.4%** | Âú®ÊúÄÂÖ∑ÊåëÊàòÊÄßÁöÑOODÂú∫ÊôØÔºàÂ¶ÇÂüéÂ∏ÇÂ§úÈó¥Ôºâ‰∏≠ÂèñÂæó„ÄÇ |\n| **Êé®ÁêÜÊïàÁéá** | **Âä†ÈÄü 2.4ÂÄç** | ÊúÄ‰Ω≥ÊÄßËÉΩÊ®°Âûã **SPS-50%** Áõ∏ÊØîÂü∫Á∫øDAÁöÑÂä†ÈÄüÊØî„ÄÇ |\n| | **ÊúÄÈ´òÂä†ÈÄü 3.47ÂÄç** | Êõ¥ÊøÄËøõÁöÑ **SPS-30%** Âèò‰ΩìÂÆûÁé∞ÁöÑÂä†ÈÄüÊØî„ÄÇ |\n| **ÊñπÊ≥ïÈ≤ÅÊ£íÊÄß** | **8/9 ÁöÑÂèò‰ΩìË∂ÖË∂äSOTA** | ÂØπÊé©Á†ÅÁéá„ÄÅÈáçÁªÑÁ≠ñÁï•ËøõË°åÊ∂àËûçÔºåËÆ≠ÁªÉ‰∫Ü9‰∏™Á≥ªÁªüÔºåÂÖ∂‰∏≠8‰∏™ÈÉΩË∂ÖËøá‰∫Ü‰πãÂâçÁöÑSOTA„ÄÇ |\n| **‰ªøÁúüÂà∞Áé∞ÂÆûËøÅÁßª** | **Èõ∂Ë∞É‰ºòÁõ¥Êé•ÈÉ®ÁΩ≤** | Âú®‰ªøÁúüËÆ≠ÁªÉÁöÑ **SPS-50%** Á≠ñÁï•ÂèØÁõ¥Êé•ÈÉ®ÁΩ≤Âà∞ÂÆûËΩ¶ÔºåÂú®‰π°ÊùëÈÅìË∑ØÂíåÂ§çÊùÇÂú∞‰∏ãÂÅúËΩ¶Âú∫ÂùáË°®Áé∞ÊàêÂäü„ÄÇ |\n| **ËæÖÂä©Â¢ûÂº∫** | **È¢ùÂ§ñ +1.7% ÊèêÂçá** | Âú®SPSÂü∫Á°Ä‰∏äÔºå‰ΩøÁî®**ÊΩúÂú®Á©∫Èó¥ÊñáÊú¨Â¢ûÂº∫ÂæÆË∞É**ÔºàÁî®LLMÂª∫ËÆÆÁöÑÊ¶ÇÂøµÊõøÊç¢ÊèèËø∞Á¨¶ÔºâÂ∏¶Êù•ÁöÑËøõ‰∏ÄÊ≠•ÊÄßËÉΩÂ¢ûÁõä„ÄÇ |\n\n### Ê†∏ÂøÉÁªìËÆ∫\nËÆ∫ÊñáÈÄöËøáÁ≥ªÁªüÁöÑÂÆûÈ™åË°®ÊòéÔºö\n- **ÊäÄÊúØÂàõÊñ∞**Ôºö**SPS** ÈÄöËøáÈöèÊú∫‰∏¢ÂºÉÂÜó‰ΩôÁöÑÂùóÊèèËø∞Á¨¶ÔºåËø´‰ΩøÁ≠ñÁï•Â≠¶‰π†ÂØπÂÖ∑‰ΩìÂπ∏Â≠ò‰ª§Áâå‰∏çÂèòÁöÑ„ÄÅÊõ¥ÂÖ∑Âõ†ÊûúÊÄßÁöÑÁâπÂæÅÔºå‰ªéËÄåÂêåÊó∂ÊèêÂçá‰∫Ü**Ê≥õÂåñÊÄß**Âíå**ÊïàÁéá**„ÄÇ\n- **ÂÆûÈôÖ‰ª∑ÂÄº**ÔºöËØ•ÊñπÊ≥ï‰∏ç‰ªÖÂú®ÂêÑÁßçÂàÜÂ∏ÉÂ§ñÊåëÊàò‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÈ©æÈ©∂ÊàêÂäüÁéáÔºåËøòÂ§ßÂπÖÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂºÄÈîÄÔºåÂπ∂ÂÆûÁé∞‰∫Ü**Èõ∂Ë∞É‰ºòÁöÑ‰ªøÁúüÂà∞Áé∞ÂÆûËøÅÁßª**Ôºå‰∏∫È´òÊïà„ÄÅÈ≤ÅÊ£íÁöÑÁ´ØÂà∞Á´ØËá™Âä®È©æÈ©∂Êèê‰æõ‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ",
      "timestamp": "2026-01-20T18:01:42.930614",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "ËÆ∫ÊñáÁöÑ‰∏ªË¶ÅÂ±ÄÈôêÊÄßÊúâÂì™‰∫õÔºüÊú™Êù•ÂèØËÉΩÁöÑÊîπËøõÊñπÂêëÊòØ‰ªÄ‰πàÔºü",
      "answer": "## ËÆ∫Êñá„ÄäSee Less, Drive Better„ÄãÁöÑ‰∏ªË¶ÅÂ±ÄÈôêÊÄß‰∏éÊú™Êù•ÊîπËøõÊñπÂêëÂàÜÊûê\n\nÂü∫‰∫éÂØπËÆ∫ÊñáÂÜÖÂÆπÁöÑÊ∑±ÂÖ•ÂàÜÊûêÔºåÂÖ∂ÊèêÂá∫ÁöÑ**ÈöèÊú∫ÂùóÈÄâÊã©ÔºàSPSÔºâ**ÊñπÊ≥ïËôΩÁÑ∂Âú®ÊèêÂçáÁ´ØÂà∞Á´ØËá™Âä®È©æÈ©∂ÁöÑÊ≥õÂåñÊÄß„ÄÅÈ≤ÅÊ£íÊÄßÂíåÊïàÁéáÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÊàêÊûúÔºå‰ΩÜ‰ªçÂ≠òÂú®‰∏Ä‰∫õÂ±ÄÈôêÊÄßÔºåÂπ∂ÊåáÊòé‰∫ÜÊú™Êù•ÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇ\n\n### ‰∏ªË¶ÅÂ±ÄÈôêÊÄß\n\n1.  **ÈöèÊú∫ÈÄâÊã©Á≠ñÁï•ÁöÑÁÆÄÂçïÊÄß**\n    - **ÂΩìÂâçÊñπÊ≥ï**ÔºöSPSÈááÁî®**ÂùáÂåÄÈöèÊú∫**ÁöÑÊñπÂºè‰∏¢ÂºÉÂùóÊèèËø∞Á¨¶„ÄÇËôΩÁÑ∂ÊúâÊïàÔºå‰ΩÜËøôÊòØ‰∏ÄÁßç**Êó†‰ø°ÊÅØ„ÄÅÊó†Áä∂ÊÄÅ**ÁöÑÈöèÊú∫ËøáÁ®ãÔºåÂèØËÉΩ‰∏çÊòØÊúÄ‰ºòÁöÑ„ÄÇ\n    - **ÊΩúÂú®ÈóÆÈ¢ò**ÔºöÂú®Â§çÊùÇÊàñÂç±Èô©Âú∫ÊôØ‰∏≠ÔºàÂ¶ÇÁ™ÅÁÑ∂Âá∫Áé∞ÁöÑË°å‰∫∫„ÄÅÊÅ∂Âä£Â§©Ê∞îÔºâÔºåÈöèÊú∫‰∏¢ÂºÉÂÖ≥ÈîÆ‰ø°ÊÅØÂùóÂèØËÉΩ‰ºöÊöÇÊó∂Èôç‰ΩéÊÑüÁü•ËÉΩÂäõÔºåÂ∞ΩÁÆ°ËÆ≠ÁªÉÈºìÂä±‰∫ÜÈ≤ÅÊ£íÊÄßÔºå‰ΩÜÂú®ÊûÅÁ´ØËæπÁºòÊÉÖÂÜµ‰∏ãÂèØËÉΩÂ≠òÂú®È£éÈô©„ÄÇ\n\n2.  **ÂØπÂü∫Á°ÄÊ®°ÂûãÁâπÂæÅÁöÑ‰æùËµñ**\n    - **ÊñπÊ≥ïÂâçÊèê**ÔºöSPSÁöÑÊúâÊïàÊÄßÂª∫Á´ãÂú®Âü∫Á°ÄÊ®°ÂûãÔºàÂ¶ÇBLIP-2ÔºâÊèêÂèñÁöÑÂùóÁâπÂæÅÊú¨Ë∫´ÂÖ∑ÊúâÈ´òÂ∫¶**ÂÜó‰ΩôÊÄß**ÁöÑÂâçÊèê‰∏ã„ÄÇÂ¶ÇÊûúÊú™Êù•Âü∫Á°ÄÊ®°ÂûãÁöÑÊû∂ÊûÑÊàñËÆ≠ÁªÉÁõÆÊ†áÂèëÁîüÊîπÂèòÔºåÂáèÂ∞ë‰∫ÜËøôÁßçÂÜÖÂú®ÂÜó‰ΩôÔºåSPSÁöÑÂ¢ûÁõäÂèØËÉΩ‰ºöÂáèÂº±„ÄÇ\n    - **ÁâπÂæÅÊèêÂèñÂºÄÈîÄ**ÔºöÂ∞ΩÁÆ°SPSÂáèÂ∞ë‰∫ÜËæìÂÖ•Á≠ñÁï•ÁΩëÁªúÁöÑ‰ª§ÁâåÊï∞ÈáèÔºåÂä†ÈÄü‰∫ÜÂêéÁª≠ËÆ°ÁÆóÔºå‰ΩÜ**ÁâπÂæÅÊèêÂèñËøáÁ®ã**ÔºàËøêË°åÂ§ßÂûãÂü∫Á°ÄÊ®°ÂûãÔºâ‰ªçÁÑ∂ÊòØËÆ°ÁÆóÁì∂È¢à„ÄÇSPS‰ªÖÈÉ®ÂàÜÁºìËß£‰∫ÜÊ≠§ÈóÆÈ¢ò„ÄÇ\n\n3.  **Ë∂ÖÂèÇÊï∞Ôºà‰∏¢ÂºÉÁéáÔºâÁöÑÊïèÊÑüÊÄß**\n    - **ÂÆûÈ™åÂèëÁé∞**ÔºöËÆ∫ÊñáÁöÑÊ∂àËûçÁ†îÁ©∂Ë°®ÊòéÔºå‰∏çÂêåÂèò‰ΩìÔºàSPS, SPPS, MSPPSÔºâÁöÑ**ÊúÄ‰ºò‰∏¢ÂºÉÁéáÔºàrateÔºâ** ‰∏çÂêåÔºàÂ¶ÇSPSÂú®50%ÊúÄ‰Ω≥ÔºåSPPSÂú®70%ÊúÄ‰Ω≥Ôºâ„ÄÇËøôË°®ÊòéÊÄßËÉΩÂØπ‰∏¢ÂºÉÁéáÁöÑÈÄâÊã©Êúâ‰∏ÄÂÆöÊïèÊÑüÊÄßÔºåÈúÄË¶ÅÈíàÂØπÂÖ∑‰ΩìÊû∂ÊûÑËøõË°åË∞ÉÊï¥ÔºåËÄåÈùû‰∏Ä‰∏™ÊôÆÈÄÇÁöÑÊúÄ‰ºòÂÄº„ÄÇ\n\n4.  **ÁêÜËÆ∫ÂàÜÊûêÁöÑÁêÜÊÉ≥ÂåñÂÅáËÆæ**\n    - **ÁêÜËÆ∫ÊîØÊíë**ÔºöËÆ∫Êñá‰∏≠ÁöÑÂºïÁêÜ1ÔºàÂÖ≥‰∫éÂ≠êÁ©∫Èó¥‰øùÊåÅÔºâ‰æùËµñ‰∫é**‰ΩéÁß©ÂíåÊúâÁïåÁõ∏Âπ≤ÊÄß**ÁöÑÂÅáËÆæ„ÄÇËôΩÁÑ∂ÂÆûÈ™åÊï∞ÊçÆÔºàPCAÂàÜÊûêÔºâÊîØÊåÅ‰ΩéÁß©ÊÄßÔºå‰ΩÜÁé∞ÂÆû‰∏ñÁïåÊï∞ÊçÆÁöÑÂ§çÊùÇÊÄßÂèØËÉΩ‰ΩøËøô‰∫õÁêÜËÆ∫‰øùËØÅÂú®ÂÆûÈôÖ‰∏≠Âº±Âåñ„ÄÇ\n\n5.  **ËØÑ‰º∞ÁéØÂ¢ÉÁöÑÈôêÂà∂**\n    - **‰ªøÁúü‰∏ªÂØº**Ôºö‰∏ªË¶ÅËØÑ‰º∞Âú®CARLA/VISTAÁ≠â‰ªøÁúüÁéØÂ¢É‰∏≠ËøõË°å„ÄÇÂ∞ΩÁÆ°ÊúâÊàêÂäüÁöÑ**‰ªøÁúüÂà∞Áé∞ÂÆûÔºàSim-to-RealÔºâ** ËøÅÁßªÊºîÁ§∫Ôºå‰ΩÜÁúüÂÆû‰∏ñÁïåÁöÑÈïøÂ∞æÂú∫ÊôØ„ÄÅÊûÅÁ´ØÂ§©Ê∞îÂíåÂ§çÊùÇ‰∫§‰∫íÁöÑÂ§öÊ†∑ÊÄßÂíå‰∏çÂèØÈ¢ÑÊµãÊÄßËøúÈ´ò‰∫éÂΩìÂâçÊµãËØïËåÉÂõ¥„ÄÇ\n\n### Êú™Êù•ÂèØËÉΩÁöÑÊîπËøõÊñπÂêë\n\nËÆ∫ÊñáÂú®ÁªìËÆ∫ÈÉ®ÂàÜÊòéÁ°ÆÊèêÂá∫‰∫ÜÊú™Êù•Â∑•‰ΩúÊñπÂêëÔºå‰∏é‰∏äËø∞Â±ÄÈôêÊÄßÁõ∏ÂëºÂ∫îÔºö\n\n1.  **Êõ¥Êô∫ËÉΩÁöÑÈÄâÊã©Á≠ñÁï•**\n    - **Áä∂ÊÄÅ‰æùËµñÁöÑÈááÊ†∑**ÔºöÂ≠¶‰π†‰∏Ä‰∏™**‰æùËµñ‰∫éÂú∫ÊôØÁä∂ÊÄÅ**ÁöÑÈááÊ†∑Á≠ñÁï•ÔºåÂä®ÊÄÅÂÜ≥ÂÆöÊØè‰∏™Â∏ßÈúÄË¶Å‰øùÁïôÂ§öÂ∞ë‰ª•ÂèäÂì™‰∫õÂùó„ÄÇ‰æãÂ¶ÇÔºåÂú®ÁÆÄÂçïÁõ¥ÈÅì‰∏äÂèØ‰ª•‰∏¢ÂºÉÊõ¥Â§öÔºåÂú®Â§çÊùÇ‰∫§ÂèâÂè£Âàô‰øùÁïôÊõ¥Â§öÂÖ≥ÈîÆ‰ø°ÊÅØ„ÄÇ\n    - **Âü∫‰∫é‰ø°ÊÅØÁöÑÈááÊ†∑**ÔºöË∂ÖË∂äÈöèÊú∫ÂùáÂåÄÈááÊ†∑ÔºåÈááÁî®**Ê†∏ÂøÉÈõÜÈÄâÊã©„ÄÅÊ≥®ÊÑèÂäõÁÜµÊàñ‰∫í‰ø°ÊÅØËØÑÂàÜ**Á≠âÊñπÊ≥ïÔºå‰∏ªÂä®‰∏¢ÂºÉÈÇ£‰∫õË¢´ËØÅÊòé‰ø°ÊÅØÈáèÊúÄÂ∞ëÊàñÂÜó‰ΩôÂ∫¶ÊúÄÈ´òÁöÑÂùóÔºå‰ªéËÄåÂú®‰øùÊåÅÊÄßËÉΩÁöÑÂêåÊó∂ÊúÄÂ§ßÂåñÊïàÁéá„ÄÇ\n\n2.  **‰∏éÂü∫Á°ÄÊ®°ÂûãÁöÑÈ´òÊïàÂçèÂêå**\n    - **ËΩªÈáèÁ∫ßÁâπÂæÅÊèêÂèñ**ÔºöÊé¢Á¥¢‰∏éSPSÊÄùÊÉ≥ÂçèÂêåÁöÑ„ÄÅÊõ¥ËΩªÈáèÁöÑÂü∫Á°ÄÊ®°ÂûãÊàñÁâπÂæÅÊèêÂèñÁÆ°ÈÅìÔºå‰ªéÊ∫êÂ§¥Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇ\n    - **Ëá™ÈÄÇÂ∫îÂü∫Á°ÄÊ®°Âûã**ÔºöÁ†îÁ©∂Â¶Ç‰ΩïÂØπÂü∫Á°ÄÊ®°ÂûãËøõË°å**ÂæÆÂ∞èÁöÑ„ÄÅÈíàÂØπÊÄßÁöÑË∞ÉÊï¥**Ôºå‰ΩøÂÖ∂‰∫ßÁîüÁöÑÁâπÂæÅÂÜó‰ΩôÂ∫¶Êõ¥‰ΩéÔºåÊàñ‰∏é‰∏ãÊ∏∏È©æÈ©∂Á≠ñÁï•ÁöÑÂ≠¶‰π†ÁõÆÊ†áÊõ¥ÂØπÈΩêÔºåËÄå‰∏çÊòØÂÆåÂÖ®ÂÜªÁªì„ÄÇ\n\n3.  **Êû∂ÊûÑ‰∏éÊñπÊ≥ïÁöÑÊâ©Â±ï**\n    - **Êó∂Â∫èÂª∫Ê®°ÈõÜÊàê**ÔºöÂΩìÂâçÂ∑•‰Ωú‰∏ªË¶ÅÂ§ÑÁêÜÂçïÂ∏ß„ÄÇÊú™Êù•ÂèØ‰ª•Â∞ÜSPSÊÄùÊÉ≥Êâ©Â±ïÂà∞**Êó∂Â∫èÊ®°Âûã**‰∏≠ÔºåÁ†îÁ©∂Â¶Ç‰ΩïÂú®ËøûÁª≠Â∏ßÈó¥ËøõË°åÈöèÊú∫ÂùóÈÄâÊã©Ôºå‰ª•Â≠¶‰π†ÂØπÂä®ÊÄÅÂú∫ÊôØÊõ¥È≤ÅÊ£íÁöÑÊó∂Â∫èË°®Á§∫„ÄÇ\n    - **Â§öÊ®°ÊÄÅËûçÂêàÂ¢ûÂº∫**ÔºöÈô§‰∫ÜËßÜËßâÂùóÁâπÂæÅÔºåÊé¢Á¥¢Â¶Ç‰ΩïÂ∞ÜSPSÁöÑÈöèÊú∫ÈÄâÊã©ÊÄùÊÉ≥Â∫îÁî®‰∫é**Â§öÊ®°ÊÄÅËûçÂêà**ÔºàÂ¶ÇLiDARÁÇπ‰∫ë„ÄÅÈõ∑ËææÊï∞ÊçÆÔºâ‰∏≠Ôºå‰ª•ÊèêÂçáÂú®‰º†ÊÑüÂô®Â§±ÊïàÊàñÂô™Â£∞ÊÉÖÂÜµ‰∏ãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ\n\n4.  **Êõ¥ÂÖ®Èù¢ÁöÑËØÑ‰º∞‰∏éÁêÜËÆ∫Ê∑±Âåñ**\n    - **Â§ßËßÑÊ®°ÁúüÂÆû‰∏ñÁïåÊµãËØï**ÔºöÂú®Êõ¥ÂπøÊ≥õ„ÄÅÊõ¥ÈïøÊúüÁöÑÁúüÂÆûÈÅìË∑ØÊµãËØï‰∏≠È™åËØÅSPSÁöÑÈ≤ÅÊ£íÊÄßÂíåÂÆâÂÖ®ÊÄßÔºåÊî∂ÈõÜÁúüÊ≠£ÁöÑÈïøÂ∞æÊ°à‰æã„ÄÇ\n    - **Êõ¥ÂùöÂÆûÁöÑÁêÜËÆ∫Ê°ÜÊû∂**ÔºöÂèëÂ±ïÊõ¥ÂÆåÂñÑÁöÑÁêÜËÆ∫ÔºåÂàÜÊûêÂú®**Êï∞ÊçÆÂàÜÂ∏ÉÂÅèÁßª„ÄÅÂØπÊäóÊÄßÊâ∞Âä®**Á≠âÊõ¥‰∏•Â≥ªÊù°‰ª∂‰∏ãÔºåÈöèÊú∫ÂùóÈÄâÊã©ÂØπÁ≠ñÁï•Ê≥õÂåñËÉΩÂäõÁöÑÂΩ±ÂìçËæπÁïå„ÄÇ\n\n**ÊÄªÁªìËÄåË®Ä**ÔºåËØ•ËÆ∫ÊñáÁöÑÊ†∏ÂøÉÂ±ÄÈôêÊÄßÂú®‰∫éÂÖ∂‚ÄúÈöèÊú∫‰∏¢ÂºÉ‚ÄùÊú∫Âà∂ÁöÑÁÆÄÂçïÊÄß‰ª•ÂèäÂØπÁé∞ÊúâÂü∫Á°ÄÊ®°ÂûãÂÜó‰ΩôÁªìÊûÑÁöÑ‰æùËµñ„ÄÇÊú™Êù•ÁöÑÊîπËøõÂ∞ÜËÅöÁÑ¶‰∫éÂ∞Ü‚ÄúÈöèÊú∫‚ÄùÂèò‰∏∫‚ÄúÊô∫ËÉΩ‚ÄùÔºåÂπ∂Êé®Âä®Êï¥‰∏™ÊÑüÁü•-ÂÜ≥Á≠ñÁÆ°ÈÅìÂêëÊõ¥È´òÊïà„ÄÅÊõ¥È≤ÅÊ£íÁöÑÊñπÂêëÊºîËøõ„ÄÇÂÖ∂ÊèêÂá∫ÁöÑ‚ÄúÂ∞ëÂç≥ÊòØÂ§öÔºàless can be moreÔºâ‚ÄùÂì≤Â≠¶‰∏∫ÂêéÁª≠Á†îÁ©∂Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÂØåÊúâÊΩúÂäõÁöÑÊñπÂêë„ÄÇ",
      "timestamp": "2026-01-20T18:02:19.036869",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    }
  ],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-15T18:58:33Z",
  "created_at": "2026-01-20T17:49:56.378338",
  "updated_at": "2026-01-20T17:49:56.378347"
}