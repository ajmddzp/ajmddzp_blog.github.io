{
    "id": "2601.06748v2",
    "title": "On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning",
    "authors": [
        "Changyu Liu",
        "Yiyang Liu",
        "Taowen Wang",
        "Qiao Zhuang",
        "James Chenhao Liang",
        "Wenhao Yang",
        "Renjing Xu",
        "Qifan Wang",
        "Dongfang Liu",
        "Cheng Han"
    ],
    "abstract": "è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹è¿‘æœŸå·²æˆä¸ºé€šç”¨æœºå™¨äººå­¦ä¹ çš„é‡è¦èŒƒå¼ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿå°†è§†è§‰è§‚å¯Ÿä¸è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„ä¸ºå¯æ‰§è¡Œçš„æœºå™¨äººåŠ¨ä½œã€‚å°½ç®¡è¿™ç±»æ¨¡å‹åº”ç”¨å¹¿æ³›ï¼Œä½†å…¶è®­ç»ƒä¸»è¦ä¾èµ–ç›‘ç£å¾®è°ƒæˆ–è®­ç»ƒé˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ ï¼Œéœ€è¦æ˜ç¡®çš„å¾®è°ƒé˜¶æ®µã€äººå·¥å¹²é¢„æˆ–å—æ§æ•°æ®æ”¶é›†è¿‡ç¨‹ã€‚å› æ­¤ï¼Œç°æœ‰æ–¹æ³•ä»éš¾ä»¥åº”å¯¹ä»¿çœŸæˆ–çœŸå®ä¸–ç•Œä¸­çš„å¤æ‚éƒ¨ç½²åœºæ™¯ï¼Œå³æœºå™¨äººå¿…é¡»å¯¹åŠ¨æ€å˜åŒ–çš„ç¯å¢ƒä½œå‡ºè‡ªä¸»çµæ´»å“åº”ã€‚ä¸ºçªç ´è¿™ä¸€å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶TT-VLAï¼Œè¯¥æ¡†æ¶æ”¯æŒåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®æ—¶è°ƒæ•´ç­–ç•¥ã€‚TT-VLAæ„å»ºäº†ä¸€ç§å¯†é›†å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨é€æ­¥ä»»åŠ¡è¿›åº¦ä¿¡å·åœ¨æµ‹è¯•é˜¶æ®µä¼˜åŒ–åŠ¨ä½œç­–ç•¥ï¼ŒåŒæ—¶ä¿ç•™ç›‘ç£å¾®è°ƒ/å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å…ˆéªŒçŸ¥è¯†ï¼Œä½¿å…¶æˆä¸ºå½“å‰VLAæ¨¡å‹çš„æœ‰æ•ˆè¡¥å……ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒçš„åŠ¨æ€æœªçŸ¥åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹æ•´ä½“é€‚åº”æ€§ã€ç¨³å®šæ€§å’Œä»»åŠ¡æˆåŠŸç‡ã€‚æˆ‘ä»¬ç›¸ä¿¡TT-VLAä¸ºæ„å»ºå…·å¤‡è‡ªè¿›åŒ–èƒ½åŠ›ã€å¯å®é™…éƒ¨ç½²çš„VLAæ¨¡å‹è¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ã€‚",
    "url": "https://arxiv.org/abs/2601.06748v2",
    "html_url": "https://arxiv.org/html/2601.06748v2",
    "html_content": "On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning\nChangyu Liu\n1\n,\nYiyang Liu\n1\n1\n1\nfootnotemark:\n1\n,\nTaowen Wang\n2\n,\nQiao Zhuang\n1\n,\nJames Chenhao Liang\n3\n,\nWenhao Yang\n4\n,\nRenjing Xu\n2\n,\nQifan Wang\n5\n,\nDongfang Liu\n6\n,\nCheng Han\n1\n1\nUniversity of Missouriâ€“Kansas City,\n2\nHong Kong University of Science and Technology (Guangzhou),\n3\nU. S. Naval Research Laboratory,\n4\nLamar University,\n5\nMeta AI,\n6\nRochester Institute of Technology\nEqual contribution\nCorresponding author\nAbstract\nVision-Language-Action (VLA) models have recently emerged as a powerful paradigm for general-purpose robot learning, enabling agents to map visual observations and natural-language instructions into executable robotic actions.\nThough popular, they are primarily trained via supervised fine-tuning or training-time\nreinforcement learning, requiring explicit fine-tuning phases, human interventions, or controlled data collection.\nConsequently, existing methods remain unsuitable for challenging simulated- or physical-world deployments, where robots must respond autonomously and flexibly to evolving environments.\nTo address this limitation, we introduce a Test-Time Reinforcement Learning for VLAs (TT-VLA), a framework that enables on-the-fly policy adaptation during inference. TT-VLA formulates a dense reward mechanism that leverages step-by-step task-progress signals to refine action policies during test time while preserving the SFT/RL-trained priors, making it an effective supplement to current VLA models.\nEmpirical results show that our approach enhances overall adaptability, stability, and task success in dynamic, previously unseen scenarios under simulated and real-world settings. We believe TT-VLA offers a principled step toward self-improving, deployment-ready VLAs.\nOn-the-Fly VLA Adaptation via Test-Time Reinforcement Learning\nChangyu Liu\n1\nâ€ \nâ€ \nthanks:\nEqual contribution\n,\nYiyang Liu\n1\n1\n1\nfootnotemark:\n1\n,\nTaowen Wang\n2\n,\nQiao Zhuang\n1\n,\nJames Chenhao Liang\n3\n,\nWenhao Yang\n4\n,\nRenjing Xu\n2\n,\nQifan Wang\n5\n,\nDongfang Liu\n6\n,\nCheng Han\n1\nâ€ \nâ€ \nthanks:\nCorresponding author\n1\nUniversity of Missouriâ€“Kansas City,\n2\nHong Kong University of Science and Technology (Guangzhou),\n3\nU. S. Naval Research Laboratory,\n4\nLamar University,\n5\nMeta AI,\n6\nRochester Institute of Technology\n1\nIntroduction\nThe ability to adapt to changing conditions is a fundamental requirement for intelligent agents operating in the real world. However, agents trained in fixed, well-defined environments frequently fail when confronted with the inherent real-world dynamic variability\nDulac-Arnold\net al.\n(\n2021\n); Tambe\net al.\n(\n1995\n); Kormushev\net al.\n(\n2013\n)\n. This gap between static training regimes and dynamic deployment environments remains a central challenge in robotics and embodied intelligence.\nFigure 1:\nTT-VLA supplements to SFT/RL-trained VLAs\nby continuously adapting policies to environment-derived rewards at test time, improving robustness to distributional shifts without retraining.\nCurrent Vision-Language-Action (VLA) models stand at the heart of this challenge. These VLAs integrate perception, language understanding, and control to translate visual observations and natural language instructions into executable actions, representing a significant step and performance gains\ntoward general-purpose embodied intelligence\nKim\net al.\n(\n2024\n); Brohan\net al.\n(\n2023\n); Zitkovich\net al.\n(\n2023\n); Wang\net al.\n(\n2025b\n); Kwok\net al.\n(\n2025\n)\n. Yet,\nmost VLAs remain bound to fixed policies, which are generally trained either through supervised fine-tuning (SFT) or through training-time reinforcement learning (RL) on curated datasets, explicit fine-tuning phases, and controlled environments.\nIn practice, these rigid policies limit VLAsâ€™ capacity\nfor challenging simulated-/physical-world applications, where\nrobots must adapt at test time as conditions evolve or distribution shifts\n.\nIn a broader perspective, current research in language or vision\nunderstanding has begun to explore\ntest-time training (TTT)\nZuo\net al.\n(\n2025\n); Li\net al.\n(\n2025c\n); Karmanov\net al.\n(\n2024\n); Hu\net al.\n(\n2024\n); Ma\net al.\n(\n2023\n)\nto update models directly on incoming test streams, often without ground-truth labels, underscoring a promising direction toward flexible model adjustments. These advances have emerged in other domains, leaving VLA test-time adaptation largely underexplored. We further find that current TTT cannot be directly applied to VLAs, as the multimodal nature brings substantial distributional shifts and is evolving (see Â§\n4.5\n).\nIn light of this view, we propose a\nT\nest-\nT\nime Reinforcement Learning framework for\nVLA\n(TT-VLA) that performs online inference-time policy fine-tuning efficiently without retraining cycles, preserving SFT/training-time RL priors while closing the loop with dense inference-time reward signals.\nHere, TTT provides accessibility for test-time adaptation, while RL complements it by addressing substantial variations in environmental conditions and underlying distributions.\nDifferent from traditional session-based RL (\ni.e\n.\n, SimpleVLA-RL), we creatively derive dense shaping signals from task-agnostic proxies to timely and effectively utilize the limited test-time information and optimize the VLA policy. These shaping signals operate independently across time frames, enabling stable, versatile, and continuous adjustments.\nOur design also bridges the prevailing offline-RL/VLA pipeline and the demands of real-world robotics, enabling continuous self-improvement under dynamic, previously unseen conditions. Extensive experiments conducted in both simulated and real-world robotic environments demonstrate that our method can naturally boost the performance of existing SFT-/RL-based approaches.\n2\nRelated Work\n2.1\nVLA Generalization & Adaptation\nCurrent VLAs\nBrohan\net al.\n(\n2023\n); Mees\net al.\n(\n2022\n); Pong\net al.\n(\n2020\n); Kwok\net al.\n(\n2025\n)\nare predominantly optimized via supervised fine-tuning on large, curated triplets\nZitkovich\net al.\n(\n2023\n); Kim\net al.\n(\n2024\n); Plaat\net al.\n(\n2024\n); Jiang\net al.\n(\n2025b\n); Kim\net al.\n(\n2025\n); Liu\net al.\n(\n2024a\n)\n, which yields strong performance in static, well-structured settings but leads to brittle behavior and limited robustness under distributional shifts due to the lack of adaptive learning mechanisms\nShenfeld\net al.\n(\n2025\n); Chen\net al.\n(\n2025a\n); Xu\net al.\n(\n2024\n); Chen\net al.\n(\n2025b\n); Wang\net al.\n(\n2025a\n)\n.\nTo address these limitations, recent studies have explored integrating VLAs with reinforcement learning. RL allows policies to actively interact with pre-collected environments or demonstrations, thereby enabling continual adaptation and performance improvement beyond static supervision\nHuang\net al.\n(\n2025\n); Zhang\net al.\n(\n2025\n); Mark\net al.\n(\n2024\n); Chen\net al.\n(\n2025c\n); Ye\net al.\n(\n2025\n); Li\net al.\n(\n2025a\n); Lu\net al.\n(\n2025\n); Jiang\net al.\n(\n2025a\n); Chen and Li (\n2025\n); Wu\net al.\n(\n2021\n); Guo\net al.\n(\n2025b\n)\n. By incorporating interaction-driven feedback, RL-augmented VLAs refine their behaviors to task-specific objectives\nHuang\net al.\n(\n2025\n); Zhang\net al.\n(\n2025\n); Mark\net al.\n(\n2024\n)\nand environmental variations\nChen\net al.\n(\n2025c\n); Ye\net al.\n(\n2025\n); Li\net al.\n(\n2025a\n); Lu\net al.\n(\n2025\n); Jiang\net al.\n(\n2025a\n); Chen and Li (\n2025\n); Wu\net al.\n(\n2021\n); Li\net al.\n(\n2025b\n); Guo\net al.\n(\n2025b\n)\n, demonstrating improved sample efficiency and generalization across unseen scenarios.\nDespite these advances, existing RL approaches still adhere to a training-deployment separation paradigm,\nwithout mechanism for inference-time adaptation once the model is deployed. This gap leaves VLAs vulnerable to unanticipated dynamics\nduring testing, where\nretraining is impractical due to time or data constraints.\n2.2\nTest-Time Training\nTest-time training (TTT) is originally proposed for adapting pre-trained models to a target domain using only unlabeled test data\nSun\net al.\n(\n2020\n); Hu\net al.\n(\n2025\n); Yoon\net al.\n(\n2024\n); Xiao\net al.\n(\n2025\n); Zhu\net al.\n(\n2024\n); Zuo\net al.\n(\n2025\n)\n. Unlike SFT\nJia\net al.\n(\n2022\n); Mosbach\net al.\n(\n2021\n); Han\net al.\n(\n2023\n); Wang\net al.\n(\n2025a\n); Liu\net al.\n(\n2025b\n,\n2024b\n); Hu\net al.\n(\n2022\n); Zeng\net al.\n(\n2024\n); Wang\net al.\n(\n2024b\n)\nor traditional RL\nSutton\net al.\n(\n1999a\n); Guo\net al.\n(\n2025a\n); Huang\net al.\n(\n2025\n); Zhang\net al.\n(\n2025\n); Mark\net al.\n(\n2024\n)\n,\nTTT leverages self-supervised objectives (\ne.g\n.\n, entropy minimization) to calibrate models during inference, thereby enabling effective domain adaptation in the absence of both human-curated labels and external feedback.\nIntuitively, TTT can be naturally extended to VLAs to enable efficient adaptation during inference. However, unlike relatively intuitive single-domain tasks (\ne.g\n.\n, vision, language), where pre-trained models exhibit high generalization capability\nBrown\net al.\n(\n2020\n); Wei\net al.\n(\n2022\n); Goyal\net al.\n(\n2023\n)\nand inter-task discrepancies are relatively minor\nHu\net al.\n(\n2025\n); Liu\net al.\n(\n2021\n); Zhao\net al.\n(\n2023\n); Han\net al.\n(\n2025\n)\n,\nrobotic tasks often entail substantial distributional shifts and evolving conditions across both visual and linguistic modalities\nPong\net al.\n(\n2020\n); Wang\net al.\n(\n2025a\n); Kim\net al.\n(\n2024\n); Liu\net al.\n(\n2024a\n)\n. Consequently,\nfixed, protocol-driven self-supervised objectives become inadequate (see Â§\n4.5\n) and overly generic.\nRecent work has explored reinforcement-based alternatives to the purely self-supervised test-time training objectives. In particular, EVOLVE-VLA\nBai\net al.\n(\n2025b\n)\nleverages task progress as a reward signal to optimize VLA policies during deployment. However, its reliance on GRPO-style optimization incurs substantial computational overhead, limiting its applicability to real-time robotic settings with strict latency constraints. We provide a more detailed discussion regarding the use of GRPO for TTT in VLAs in AppendixÂ Â§\nS7\n.\nTo address these limitations, we propose an RL-driven test-time learning framework that enables efficient online adaptation (see Â§\n3\n).\n3\nMethod\nIn this section, we introduce\nT\nest-\nT\nime Reinforcement Learning framework for\nVLA\n(TT-VLA), a novel\ntest-time training approach designed to enhance VLA performance through on-the-fly policy adaptation.\nIn Â§\n3.1\n, we first\nprovide background on Proximal Policy Optimization (PPO) and VLAs. We then present TT-VLA in Â§\n3.2\n.\nÂ§\n3.3\nfurther provides TT-VLAâ€™s theoretical analysis and insights. The overall framework is shown in Fig.\n2\n.\n3.1\nPreliminaries\nProblem Formulation.\nWe model robotic manipulation as a Partially Observable Markov Decision Process (POMDP)\nKaelbling\net al.\n(\n1998\n)\n, defined as:\nâ„³\n=\n(\nğ’®\n,\nğ’œ\n,\nğ’ª\n,\nâ„’\n)\n,\n\\mathcal{M}=(\\mathcal{S},\\mathcal{A},\\mathcal{O},\\mathcal{L}),\n(1)\nwhere\nğ’®\n\\mathcal{S}\ndenotes the state space of the robot and environment,\nğ’œ\n\\mathcal{A}\nis the action space,\nğ’ª\n\\mathcal{O}\nrepresents the multimodal observation space (\ne.g\n.\n, RGB and proprioception), and\nâ„’\n\\mathcal{L}\ndenotes the space of natural-language task instructions.\nAt the start of a task episode, the VLA policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nreceives an instruction\nl\nâˆˆ\nâ„’\nl\\in\\mathcal{L}\nand an initial observation\no\n0\no_{0}\n.\nThe goal of the VLA policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nis to generate a sequence of actions:\na\n0\n:\nT\nâˆ’\n1\nâˆ¼\nÏ€\nÎ¸\nâ€‹\n(\na\nt\nâˆ£\no\nt\nâˆ’\nH\n+\n1\n:\nt\n,\nl\n)\n,\na_{0:T-1}\\sim\\pi_{\\theta}(a_{t}\\mid o_{t-H+1:t},l),\n(2)\nwhere\no\nt\no_{t}\nand\na\nt\na_{t}\ndenote the observation and action at time\nt\nt\n,\nT\nT\nis the episode length, and\nH\nH\nis the number of past observations used as policy input.\nThe above formulation characterizes the standard VLA decision process. However, it inherently assumes a fixed, pre-trained policy. Real-world deployments, on the other hand, demand adaptability to dynamic environments.\nTherefore, under the test-time adaptation, our goal should now switch to adjusting the pretrained policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nonline during deployment flexibly\nwithout\nany access to training data, environment resets, or human intervention.\nProximal Policy Optimization (PPO)\n.\nPPO is an actorâ€“critic policy-gradient method that uses a clipped surrogate objective to constrain policy updates, ensuring stable optimization by keeping the updated policy within a trust region of the previous policy. Formally, the PPO objective is defined as:\nL\nPPO\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nt\nâ€‹\n[\nL\nt\nCLIP\nâ€‹\n(\nÎ¸\n)\nâˆ’\nc\n1\nâ€‹\nL\nt\nValue\nâ€‹\n(\nÎ¸\n)\n+\nc\n2\nâ€‹\nL\nt\nentropy\nâ€‹\n(\nÎ¸\n)\n]\n,\nL^{\\text{PPO}}(\\theta)=\\mathbb{E}_{t}\\left[L^{\\text{CLIP}}_{t}(\\theta)-c_{1}L^{\\text{Value}}_{t}(\\theta)+c_{2}L^{\\text{entropy}}_{t}(\\theta)\\right],\n(3)\nwhere\nL\nt\nCLIP\nâ€‹\n(\nÎ¸\n)\nL_{t}^{\\text{CLIP}}(\\theta)\nrepresents the clipped policy loss,\nL\nt\nValue\nâ€‹\n(\nÎ¸\n)\nL_{t}^{\\text{Value}}(\\theta)\ndenotes the value function loss,\nL\nt\nEntropy\nâ€‹\n(\nÎ¸\n)\nL_{t}^{\\text{Entropy}}(\\theta)\nis the entropy regularization term, and\nc\n1\nc_{1}\nand\nc\n2\nc_{2}\nare weighting coefficients.\nThe clipped policy objective is defined as:\nL\nt\nCLIP\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nt\nâ€‹\n[\nmin\nâ¡\n(\nr\nt\nâ€‹\n(\nÎ¸\n)\nâ€‹\nA\n^\nt\n,\nclip\nâ€‹\n(\nr\nt\nâ€‹\n(\nÎ¸\n)\n,\n1\nâˆ’\nÏµ\n,\n1\n+\nÏµ\n)\nâ€‹\nA\n^\nt\n)\n]\n,\nL^{\\text{CLIP}}_{t}(\\theta)=\\mathbb{E}_{t}\\left[\\min\\!\\left(r_{t}(\\theta)\\hat{A}_{t},\\,\\text{clip}\\!\\big(r_{t}(\\theta),1-\\epsilon,1+\\epsilon\\big)\\hat{A}_{t}\\right)\\right],\n(4)\nwhere\nr\nt\nâ€‹\n(\nÎ¸\n)\n=\nÏ€\nÎ¸\nâ€‹\n(\na\nt\nâˆ£\ns\nt\n)\nÏ€\nÎ¸\nold\nâ€‹\n(\na\nt\nâˆ£\ns\nt\n)\nr_{t}(\\theta)=\\frac{\\pi_{\\theta}(a_{t}\\mid s_{t})}{\\pi_{\\theta_{\\text{old}}}(a_{t}\\mid s_{t})}\nis the ratio between the new and old policy,\nÏµ\n\\epsilon\ncontrols the clipping range,\nA\n^\nt\n\\hat{A}_{t}\ndenotes the advantage estimate, and\nclip\nâ€‹\n(\nâ‹…\n)\n\\text{clip}(\\cdot)\ndenotes the clipping operation.\nPPO typically employs Generalized Advantage Estimation (GAE) to estimate\nA\n^\nt\n\\hat{A}_{t}\n,\ngiven by:\nA\n^\nt\n=\nâˆ‘\nl\n=\n0\nâˆ\n(\nÎ³\nâ€‹\nÎ»\n)\nl\nâ€‹\nÎ´\nt\n+\nl\n,\n\\hat{A}_{t}=\\sum_{l=0}^{\\infty}(\\gamma\\lambda)^{l}\\,\\delta_{t+l},\n(5)\nwhere\nÎ´\nt\n=\nr\nt\n+\nÎ³\nâ€‹\nV\nâ€‹\n(\ns\nt\n+\n1\n)\nâˆ’\nV\nâ€‹\n(\ns\nt\n)\n\\delta_{t}=r_{t}+\\gamma V(s_{t+1})-V(s_{t})\nis temporal-difference (TD) residual,\nÎ³\n\\gamma\nis the discount factor,\nÎ»\n\\lambda\nis the smoothing parameter, and\nV\nâ€‹\n(\ns\nt\n)\nV(s_{t})\nis\nthe estimated expected return from state\ns\nt\ns_{t}\n.\n3.2\nTT-VLA\nIn PPO, both the policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nand value function\nV\nÎ¸\nV_{\\theta}\nare trained jointly. However, in VLA test-time adaptation,\nlearning a reliable value function is generally infeasible due to two reasons:\nâ‘ \nLimited samples\n: Test-time adaptation relies on extremely limited interaction data, a single episode, which is insufficient for accurate return estimation.\nâ‘¡\nStrict time constraints\n: Test-time updates for VLA models must be performed online under strict latency constraints. To overcome these limitations, we develop a value-free PPO, which enables policy adaptation\nwithout\nan explicit value function.\nFigure 2:\nOverview of TT-VLA.\n(a) Overall pipeline.\nIn TT-VLA, a pretrained VLA policy receives an observation and instruction, executes actions in the environment, and receives dense, progress-based rewards computed by a progress estimator. These rewards are used to update the policy online via a value-free PPO objective, enabling continuous within-episode policy adaptation at test time (see Â§\n3.2\n).\n(b) Effectiveness.\nTT-VLA consistently improves the performance of diverse VLA backbones across unseen tasks, demonstrating robust generalization and adaptability under evolving conditions or distributional shifts (see Â§\n4.2\n-\n4.3\n).\nDense Progress-Based Reward.\nMost existing RL-based VLAs\nLi\net al.\n(\n2025a\n); Liu\net al.\n(\n2025a\n)\nrely on sparse terminal rewards, typically indicating binary task success or failure at the end of an episode. While such rewards are effective during offline training, where episodes can be replayed or reset, they are impractical for test-time adaptation:\nthe policy updates are delayed until task completion, preventing any mid-episode correction or online adjustment.\nConsequently, a robot operating with sparse rewards cannot refine its behavior during inference, leading to fragile and non-adaptive performance in dynamic environments.\nLet\np\nt\nâˆˆ\n[\n0\n,\n1\n]\np_{t}\\in[0,1]\ndenote task progress at time step\nt\nt\n. Intuitively,\np\nt\np_{t}\nshould increase when actions advance task completion\nand decrease when actions undo or reverse previously achieved progress. In a POMDP setting, we estimate progress directly from observations and language instructions as:\np\nt\n=\nÎ¦\nâ€‹\n(\no\n0\n:\nt\n+\n1\n,\nl\n)\n,\np_{t}=\\Phi(o_{0:t+1},l),\n(6)\nwhere\nÎ¦\n\\Phi\ndenotes a task progress predictor conditioned on the observation history and instruction\nl\nl\n. The per-step dense reward is then defined as the temporal difference in progress:\nr\nt\n=\np\nt\nâˆ’\np\nt\nâˆ’\n1\n.\nr_{t}=p_{t}-p_{t-1}.\n(7)\nWe instantiate\nÎ¦\n\\Phi\nusing the Vision-Language-Action-Critic model (VLAC)\nZhai\net al.\n(\n2025\n)\n, a pretrained multimodal model that serves as a scalar regressor for task progress estimation.\nThis progress-based reward exhibits three desirable properties. First, it requires no external supervision during inference, allowing fully autonomous adaptation at test time.\nSecond, it provides dense, step-wise feedback that facilitates continuous mid-episodic policy adaptation. Third, it encourages monotonic progress toward task completion while discouraging oscillatory or regressive behaviors.\nTraining Objective.\nUnder the test-time VLA setting, learning an accurate value function\nV\nÎ¸\nV_{\\theta}\nwithin a single episode is impractical.\nWe therefore adopt a value-free PPO variant that removes the value function learning and directly uses the per-step reward signal from Eq.\n7\nfor policy adaptation.\nStarting from Eq.\n3\n,\nwe remove auxiliary losses by setting\nc\n1\n=\n0\nc_{1}=0\nand\nc\n2\n=\n0\nc_{2}=0\n, discarding both value regression and entropy regularization term.\nWhile the entropy term encourages exploration during training, test-time adaptation prioritizes rapid fitting of the current task rather than broad exploration. As a result, our objective focuses solely on stable policy refinement, while preserving the clipped surrogate loss. Eq.\n3\nnow turns into:\nL\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nt\nâ€‹\n[\nL\nt\nCLIP\nâ€‹\n(\nÎ¸\n)\n]\n.\nL(\\theta)=\\mathbb{E}_{t}\\left[L^{\\text{CLIP}}_{t}(\\theta)\\right].\n(8)\nTo make the agent precisely capture the immediate value of the current action, we further redefine the advantage to depend only on the reward obtained from that action, rather than the exponentially weighted combination of TD residuals used in GAE (see Eq.\n5\n). In other words, we focus on how each individual action contributes to instantaneous progress rather than estimating long-term returns. To achieve this, we set\nÎ»\n=\n0\n\\lambda=0\nand\nÎ³\n=\n0\n\\gamma=0\n, collapsing GAE into a one-step formulation:\nA\n^\nt\n=\nÎ´\nt\n=\nr\nt\n.\n\\hat{A}_{t}=\\delta_{t}=r_{t}.\n(9)\nHere,\nÎ´\nt\n=\nr\nt\n\\delta_{t}=r_{t}\nsince we remove the value function, making the TD residual the immediate reward signal.\nThis simplification ensures that policy updates directly reflect the progress at each step, allowing the agent to adapt rapidly to changing conditions without relying on a value function.\nOverall Pipeline.\nAt the beginning of each episode, the pretrained VLA receives an initial observation\no\n0\no_{0}\nand instruction\nl\nl\n. The VLA policy\nÏ€\nÎ¸\n\\pi_{\\theta}\ngenerates the first action\na\n0\na_{0}\n. At each subsequent time step\nt\nt\n, the VLA receives the latest observation\no\nt\no_{t}\nand outputs an action\na\nt\na_{t}\n. After execution, the progress estimator\nÎ¦\n\\Phi\ncomputes the task progress\np\nt\np_{t}\n, and the corresponding dense reward\nr\nt\nr_{t}\n(see Eq.\n7\n). This reward is used to compute the policy loss via Eq.\n8\nin a value-free manner, and the policy parameters\nÎ¸\n\\theta\nare updated accordingly.\nThe updated policy\nÏ€\nÎ¸\nâ€²\n\\pi_{\\theta^{\\prime}}\nis then used to generate subsequent actions, allowing continuous refinement throughout the episode.\nThe pseudo code is shown in Appendix Algorithm\n1\n.\n3.3\nTheoretical Analysis of TT-VLA\nIn Â§\n3.2\n, we proposed TT-VLA using a progress-based dense reward and a value-free formulation for test-time adaptation. In this section, we provide a theoretical justification for these design choices.\nProposition 1\n(Vanishing learning signal under progress-difference reward)\n.\nLet the per-step reward be defined as the progress difference\nr\nt\n=\np\nt\nâˆ’\np\nt\nâˆ’\n1\nr_{t}=p_{t}-p_{t-1}\nwith\np\nt\nâˆˆ\n[\n0\n,\n1\n]\np_{t}\\in[0,1]\n.\nAssume that the value function represents the remaining progress,\nV\nâ€‹\n(\ns\nt\n)\n=\n1\nâˆ’\np\nt\nâˆ’\n1\nV(s_{t})=1-p_{t-1}\n, and the discount factor is\nÎ³\n=\n1\n\\gamma=1\n.\nThen the temporal-difference (TD) error vanishes for all\nt\nt\n, and consequently\nthe GAE\nA\n^\nt\n\\hat{A}_{t}\nis identically zero:\nÎ´\nt\n=\n0\n,\nA\n^\nt\n=\n0\n,\nâˆ€\nÎ»\nâˆˆ\n[\n0\n,\n1\n]\n.\n\\delta_{t}=0,\\quad\\hat{A}_{t}=0,\\qquad\\forall\\lambda\\in[0,1].\n(10)\nProof.\nSubstituting\nr\nt\n=\np\nt\nâˆ’\np\nt\nâˆ’\n1\nr_{t}=p_{t}-p_{t-1}\n,\nV\nâ€‹\n(\ns\nt\n)\n=\n1\nâˆ’\np\nt\nâˆ’\n1\nV(s_{t})=1-p_{t-1}\n, and\nÎ³\n=\n1\n\\gamma=1\ninto the TD residual yields\nÎ´\nt\n=\nr\nt\n+\nÎ³\nâ€‹\nV\nâ€‹\n(\ns\nt\n+\n1\n)\nâˆ’\nV\nâ€‹\n(\ns\nt\n)\n=\n(\np\nt\nâˆ’\np\nt\nâˆ’\n1\n)\n+\n(\n1\nâˆ’\np\nt\n)\nâˆ’\n(\n1\nâˆ’\np\nt\nâˆ’\n1\n)\n=\n0\n.\n\\begin{aligned} \\delta_{t}&=r_{t}+\\gamma V(s_{t+1})-V(s_{t})\\\\\n&=(p_{t}-p_{t-1})+(1-p_{t})-(1-p_{t-1})\\\\\n&=0.\\end{aligned}\n(11)\nBy Eq.\n5\n, GAE is a weighted sum of TD residuals, it follows that\nA\n^\nt\n=\n0\n\\hat{A}_{t}=0\n. Therefore, the policy gradient receives no\nlearning signal.\nâˆ\nCorollary 1\n(Negative TD bias when\nÎ³\n<\n1\n\\gamma<1\n)\n.\nLet\nr\nt\n=\np\nt\nâˆ’\np\nt\nâˆ’\n1\nr_{t}=p_{t}-p_{t-1}\nwith\np\nt\nâˆˆ\n[\n0\n,\n1\n]\np_{t}\\in[0,1]\nand\nV\nâ€‹\n(\ns\nt\n)\n=\n1\nâˆ’\np\nt\nâˆ’\n1\nV(s_{t})=1-p_{t-1}\n.\nIf\n0\n<\nÎ³\n<\n1\n0<\\gamma<1\n,\nthen TD residual\nÎ´\nt\n<\n0\n\\delta_{t}<0\n, introducing a systematic negative bias in advantage estimation.\nProof.\nSubstituting\nr\nt\n=\np\nt\nâˆ’\np\nt\nâˆ’\n1\nr_{t}=p_{t}-p_{t-1}\nand\nV\nâ€‹\n(\ns\nt\n)\n=\n1\nâˆ’\np\nt\nâˆ’\n1\nV(s_{t})=1-p_{t-1}\ninto the TD residual, we get\nÎ´\nt\n=\nr\nt\n+\nÎ³\nâ€‹\nV\nâ€‹\n(\ns\nt\n+\n1\n)\nâˆ’\nV\nâ€‹\n(\ns\nt\n)\n=\n(\np\nt\nâˆ’\np\nt\nâˆ’\n1\n)\n+\nÎ³\nâ€‹\n(\n1\nâˆ’\np\nt\n)\nâˆ’\n(\n1\nâˆ’\np\nt\nâˆ’\n1\n)\n=\n(\nÎ³\nâˆ’\n1\n)\nâ€‹\n(\n1\nâˆ’\np\nt\n)\n.\n\\begin{aligned} \\delta_{t}&=r_{t}+\\gamma V(s_{t+1})-V(s_{t})\\\\\n&=(p_{t}-p_{t-1})+\\gamma(1-p_{t})-(1-p_{t-1})\\\\\n&=(\\gamma-1)(1-p_{t}).\\end{aligned}\n(12)\nSince\n0\n<\nÎ³\n<\n1\n0<\\gamma<1\nand\n1\nâˆ’\np\nt\n>\n0\n1-p_{t}>0\n, this implies\nÎ³\nâˆ’\n1\n<\n0\n\\gamma-1<0\nand thus\nÎ´\nt\n<\n0\n\\delta_{t}<0\n.\nâˆ\nLemma 1\n(One-step collapse of GAE)\n.\nLet the reward be defined as the progress difference\nr\nt\n=\np\nt\nâˆ’\np\nt\nâˆ’\n1\n,\nr_{t}=p_{t}-p_{t-1},\n(13)\nand let the value function estimator be\nV\nâ€‹\n(\ns\n)\nV(s)\n.\nThen, for GAE:\nA\n^\nt\n=\nâˆ‘\nl\n=\n0\nâˆ\n(\nÎ³\nâ€‹\nÎ»\n)\nl\nâ€‹\nÎ´\nt\n+\nl\n,\n\\hat{A}_{t}=\\sum_{l=0}^{\\infty}(\\gamma\\lambda)^{l}\\,\\delta_{t+l},\n(14)\nÎ´\nt\n=\nr\nt\n+\nÎ³\nâ€‹\nV\nâ€‹\n(\ns\nt\n+\n1\n)\nâˆ’\nV\nâ€‹\n(\ns\nt\n)\n,\n\\delta_{t}=r_{t}+\\gamma V(s_{t+1})-V(s_{t}),\n(15)\nthe following statements hold:\n(a). If\nÎ»\n=\n0\n\\lambda=0\n, then\nA\nt\n=\nÎ´\nt\nA_{t}=\\delta_{t}\n.\n(b). If\nÎ³\n=\n0\n\\gamma=0\n, then\nA\nt\n=\nÎ´\nt\n=\nr\nt\nâˆ’\nV\nâ€‹\n(\ns\nt\n)\nA_{t}=\\delta_{t}=r_{t}-V(s_{t})\n. In part-\nicular, if\nV\nâ€‹\n(\ns\n)\nâ‰¡\n0\nV(s)\\equiv 0\n, then\nA\nt\n=\nr\nt\nA_{t}=r_{t}\n.\nThe proof is provided in AppendixÂ Â§\nS2\n.\n4\nExperiment\nIn this section, we present a comprehensive evaluation of our proposed method through a series of unseen robotic tasks. We detail the task setups, outline the implementation specifics, and compare our approach against baselines.\nMore experimental details are provided in Appendix Â§\nS3\n-\nS6\n.\nExecution\nVision\nModel\nObj. Pos.\nRobot Pose\nObj. Rep.\nAvg.\nTable\nTexture-w\nNoise-w\nTexture-s\nNoise-s\nAvg.\nNora\nHung\net al.\n(\n2025\n)\n23.75%\n10.83%\n7.50%\n14.03%\n39.72%\n32.50%\n36.67%\n19.58%\n22.92%\n29.92%\nNora + TT-VLA\n25.00%\n12.50%\n10.83%\n16.11%\n44.58%\n34.58%\n41.67%\n20.83%\n27.08%\n33.75%\nÎ”\n\\Delta\n+1.25%\n+1.67%\n+3.33%\n2.08%\n+6.66%\n+2.08%\n+5.00%\n+1.25%\n+4.16%\n+3.83%\nRelative Gain\nâ†‘\n\\uparrow\n5.26%\nâ†‘\n\\uparrow\n15.42%\nâ†‘\n\\uparrow\n44.40%\nâ†‘\n\\uparrow\n14.85%\nâ†‘\n\\uparrow\n17.56%\nâ†‘\n\\uparrow\n6.40%\nâ†‘\n\\uparrow\n13.64%\nâ†‘\n\\uparrow\n6.38%\nâ†‘\n\\uparrow\n18.15%\nâ†‘\n\\uparrow\n12.80%\nOpenVLA\nKim\net al.\n(\n2024\n)\n31.67%\n41.25%\n36.25%\n36.39%\n54.85%\n45.42%\n40.83%\n28.33%\n30.00%\n39.83%\nOpenVLA + TT-VLA\n34.58%\n42.08%\n42.92%\n39.83%\n57.08%\n47.08%\n42.92%\n31.25%\n31.33%\n41.93%\nÎ”\n\\Delta\n+2.92%\n+0.83%\n+6.67%\n+3.45%\n+2.50%\n+1.67%\n+2.08%\n+2.92%\n+1.33%\n+2.10%\nRelative Gain\nâ†‘\n\\uparrow\n9.21%\nâ†‘\n\\uparrow\n2.02%\nâ†‘\n\\uparrow\n18.40%\nâ†‘\n\\uparrow\n9.54%\nâ†‘\n\\uparrow\n4.58%\nâ†‘\n\\uparrow\n3.67%\nâ†‘\n\\uparrow\n5.10%\nâ†‘\n\\uparrow\n10.29%\nâ†‘\n\\uparrow\n4.43%\nâ†‘\n\\uparrow\n5.27%\nOpenVLA-RL\nLiu\net al.\n(\n2025a\n)\n82.08%\n81.25%\n81.25%\n81.53%\n87.08%\n85.00%\n85.83%\n64.17%\n69.17%\n78.25%\nOpenVLA-RL + TT-VLA\n84.17%\n82.08%\n86.25%\n84.17%\n90.00%\n86.25%\n85.83%\n65.83%\n72.08%\n80.00%\nÎ”\n\\Delta\n+2.09%\n+0.83%\n+5.00%\n+2.64%\n+2.92%\n+1.25%\n+0.00%\n+1.66%\n+2.91%\n+1.75%\nRelative Gain\nâ†‘\n\\uparrow\n2.54%\nâ†‘\n\\uparrow\n1.02%\nâ†‘\n\\uparrow\n6.15%\nâ†‘\n\\uparrow\n3.24%\nâ†‘\n\\uparrow\n2.08%\nâ†‘\n\\uparrow\n1.47%\nâ†‘\n\\uparrow\n0.00%\nâ†‘\n\\uparrow\n2.59%\nâ†‘\n\\uparrow\n4.21%\nâ†‘\n\\uparrow\n2.23%\nTraceVLA\nZheng\net al.\n(\n2025\n)\n55.00%\n18.75%\n7.08%\n26.94%\n71.67%\n67.08%\n67.08%\n45.83%\n45.83%\n59.50%\nTraceVLA + TT-VLA\n57.92%\n21.50%\n7.50%\n28.97%\n72.92%\n67.50%\n67.92%\n46.25%\n47.08%\n60.33%\nÎ”\n\\Delta\n+2.92 %\n+2.75%\n+0.42%\n+2.03%\n+1.25%\n+0.42%\n+0.84%\n+0.42%\n+1.25%\n+0.84%\nRelative Gain\nâ†‘\n\\uparrow\n5.31%\nâ†‘\n\\uparrow\n14.67%\nâ†‘\n\\uparrow\n5.93%\nâ†‘\n\\uparrow\n7.53%\nâ†‘\n\\uparrow\n1.47%\nâ†‘\n\\uparrow\n0.63%\nâ†‘\n\\uparrow\n1.25%\nâ†‘\n\\uparrow\n0.92%\nâ†‘\n\\uparrow\n2.73%\nâ†‘\n\\uparrow\n1.41%\nSemantics\nModel\nM-Obj. OOD\nInstruct\nDist Recep.\nRecep.\nM Recep.\nObject\nM-Obj. IND\nAvg.\nNora\nHung\net al.\n(\n2025\n)\n10.00%\n39.85%\n16.67%\n28.33%\n26.25%\n17.08%\n27.08%\n23.57%\nNora + TT-VLA\n11.25%\n42.50%\n18.75%\n30.83%\n30.00%\n18.33%\n27.08%\n25.53%\nÎ”\n\\Delta\n+1.25%\n+2.92%\n+2.08%\n+2.50%\n+3.75%\n+1.25%\n+0.00%\n+1.96%\nRelative Gain\nâ†‘\n\\uparrow\n12.50%\nâ†‘\n\\uparrow\n7.38%\nâ†‘\n\\uparrow\n12.48%\nâ†‘\n\\uparrow\n8.82%\nâ†‘\n\\uparrow\n14.29%\nâ†‘\n\\uparrow\n7.32%\nâ†‘\n\\uparrow\n0.00%\nâ†‘\n\\uparrow\n8.33%\nOpenVLA\nKim\net al.\n(\n2024\n)\n28.75%\n49.58%\n20.42%\n33.33%\n43.75%\n45.00%\n49.58%\n38.63%\nOpenVLA + TT-VLA\n32.05%\n50.17%\n29.58%\n37.50%\n45.00%\n46.25%\n50.00%\n41.51%\nÎ”\n\\Delta\n+3.30%\n+0.28%\n+9.17%\n+4.17%\n+1.25%\n+1.25%\n+0.42%\n+2.88%\nâ†‘\n\\uparrow\n11.48%\nâ†‘\n\\uparrow\n1.18%\nâ†‘\n\\uparrow\n44.90%\nâ†‘\n\\uparrow\n12.50%\nâ†‘\n\\uparrow\n2.86%\nâ†‘\n\\uparrow\n2.78%\nâ†‘\n\\uparrow\n0.85%\nâ†‘\n\\uparrow\n7.54%\nOpenVLA-RL\nLiu\net al.\n(\n2025a\n)\n62.50%\n86.67%\n60.00%\n79.58%\n80.42%\n77.50%\n77.50%\n74.88%\nOpenVLA-RL + TT-VLA\n65.00%\n90.00%\n61.25%\n79.58%\n80.83%\n78.33%\n80.00%\n76.43%\nÎ”\n\\Delta\n+2.50%\n+3.33%\n+1.25%\n+0.00%\n+0.41%\n+0.83%\n+2.50%\n+1.55%\nRelative Gain\nâ†‘\n\\uparrow\n4.00%\nâ†‘\n\\uparrow\n3.84%\nâ†‘\n\\uparrow\n2.08%\nâ†‘\n\\uparrow\n0.00%\nâ†‘\n\\uparrow\n0.51%\nâ†‘\n\\uparrow\n1.07%\nâ†‘\n\\uparrow\n3.23%\nâ†‘\n\\uparrow\n2.06%\nTraceVLA\nZheng\net al.\n(\n2025\n)\n22.50%\n59.17%\n27.92%\n47.50%\n55.83%\n45.00%\n57.92%\n45.12%\nTraceVLA + TT-VLA\n25.00%\n60.00%\n28.33%\n51.25%\n55.83%\n47.08%\n60.00%\n46.78%\nÎ”\n\\Delta\n+2.50%\n+0.83%\n+0.41%\n+3.75%\n+0.00%\n+2.08%\n+2.08%\n+1.66%\nRelative Gain\nâ†‘\n\\uparrow\n11.11%\nâ†‘\n\\uparrow\n1.40%\nâ†‘\n\\uparrow\n1.47%\nâ†‘\n\\uparrow\n7.89%\nâ†‘\n\\uparrow\n0.00%\nâ†‘\n\\uparrow\n4.62%\nâ†‘\n\\uparrow\n3.59%\nâ†‘\n\\uparrow\n3.69%\nTable 1:\nMain results on unseen simulation tasks.\nWe report success rates (%) across three generalization dimensions:\nExecution\n,\nVision\n, and\nSemantics\non 4 state-of-the-art open-source VLAs (\ni.e\n.\n, Nora, OpenVLA, OpenVLA-RL, and TraceVLA).\nÎ”\n\\Delta\ndenotes the absolute improvement, and\nâ†‘\n\\uparrow\nindicates relative gains. As shown in the table, across all baselines and task categories, TT-VLA consistently improves performance during test time, demonstrating substantial generalizability and constituting a novel angle for VLA adaptivity.\n4.1\nExperimental Setup\nEnvironment and Task Settings.\nAs stated in Â§\n2.1\n,\nTT-VLA aims to address the inherent vulnerability of current VLAs to unanticipated dynamics and domain shifts.\nTo evaluate this generalization capability on\nunseen tasks\n, we test TT-VLA on both simulated and real-world tasks.\nFor\nsimulation experiments\n(see Â§\n4.2\n),\nwe follow RL4VLAâ€™s\nLiu\net al.\n(\n2025a\n)\nsetup, focusing on a standard pick-and-place manipulation scenario. The agent receives an RGB observation and a natural-language instruction, and outputs a Cartesian end-effector delta along with a binary gripper command.\nSpecifically, as in\nLiu\net al.\n(\n2025a\n)\n, we evaluate generalization along three dimensions:\nExecution\n,\nVision\n, and\nSemantics\n.\nFor\nExecution\n, the initial poses of the robot, object, and receptacle are randomized, and an additional mid-episode object repositioning condition is introduced to evaluate robustness to dynamic variations during execution.\nFor\nVision\n, both foreground and background appearances are varied through dynamic textures, unseen table surfaces, and image-level noise.\nFor\nSemantics\n, unseen objects, receptacles, and instruction paraphrases are introduced, along with multi-object, multi-receptacle, and distractor-receptacle tasks designed to assess compositional and linguistic generalization. Detailed task specifications are provided in AppendixÂ Â§\nS3\n.\nAll simulation experiments are conducted in ManiSkill 3\nTao\net al.\n(\n2024\n)\nusing a WidowX-250S robotic arm.\nFor\nreal-world evaluation\n(see Â§\n4.3\n),\nwe study pick-and-place manipulation tasks on a Franka Research 3 platform. The agent similarly receives an RGB image and a task instruction, and outputs a Cartesian end-effector displacement together with a binary gripper command. We evaluate performance on nine unseen tasks designed to assess robustness to executional, visual, and semantic shifts.\nImplementation Details.\nFor simulation, each task is executed for 80 trials across three random seeds using a\n640\nÃ—\n480\n640\\times 480\nRGB image as input.\nFor real-world experiments, each task is evaluated over 10 trials under consistent experimental conditions, including fixed camera viewpoints with an image resolution of\n500\nÃ—\n480\n500\\times 480\n, controlled lighting, and static backgrounds.\nThe policy is fine-tuned using LoRA\nHu\net al.\n(\n2022\n)\nwith ranks\n{\n16\n,\n32\n}\n\\{16,32\\}\n. Learning rates are chosen from\n{\n1\nÃ—\n10\nâˆ’\n5\n,\n5\nÃ—\n10\nâˆ’\n5\n,\n1\nÃ—\n10\nâˆ’\n4\n}\n\\{1\\times 10^{-5},5\\times 10^{-5},1\\times 10^{-4}\\}\n, optimized with AdamW. A clip parameter\nÏµ\n\\epsilon\nof 0.2 is applied to enhance training stability.\nEach episode is executed with a fixed horizon of 160 steps.\n4.2\nSimulation Results\nBaselines.\nWe benchmark our proposed method against several state-of-the-art open-source VLA models, which span diverse architectural designs and training paradigms:\nâ€¢\nNora\nHung\net al.\n(\n2025\n)\nadopts Qwen-2.5-VL-3B\nBai\net al.\n(\n2025a\n)\nas its backbone and employs the FAST+ tokenizer\nPertsch\net al.\n(\n2025\n)\nto enable efficient action sequence generation. Following\nLiu\net al.\n(\n2025a\n)\nto ensure a strong initial policy, we further fine-tune Nora for 50k steps on a self-collected ManiSkill 3 dataset.\nâ€¢\nOpenVLA\nKim\net al.\n(\n2024\n)\nis one of the most widely used open-source VLA models. It is built on Llama-2-7B\nTouvron\net al.\n(\n2023\n)\n. Consistent with\nLiu\net al.\n(\n2025a\n)\n, we apply a warm-up fine-tuning phase of 10k steps prior to evaluation.\nâ€¢\nOpenVLA-RL\nLiu\net al.\n(\n2025a\n)\nextends OpenVLA via reinforcement learning during training, enabling further task-specific policy refinement beyond supervised pre-training.\nâ€¢\nTraceVLA\nZheng\net al.\n(\n2025\n)\nis designed to enhance spatio-temporal reasoning through visual trace prompting. By encoding stateâ€“action histories as visual prompts, it better captures long-horizon dependencies and improves manipulation performance in interactive environments.\nOverall Performance.\nAs shown in Table\n1\n, our proposed method\nconsistently improves\nthe performance of\nall\nrepresentative baseline models across a range of unseen tasks, demonstrating its broad applicability. For example, when applied to Nora, our method achieves gains on\n14 out of 15\ntasks, with relative improvements ranging from\n5.26% to 44.4%\n. The largest improvements are observed on Task Obj. Rep. (\n44.4\n%\n44.4\\%\n) and Task Noise-s (\n18.15\n%\n18.15\\%\n). Similarly, on OpenVLA, our method yields consistent performance improvements across nearly all tasks, including several large-margin gains (\ne.g\n.\n, 44.9% on Task Disturb Recep. and 18.4% on Task Obj. Rep.).\nOverall, while current methods generally focus on training-time sophisticated architectural improvements, we demonstrate that substantial generalizability across diverse baselines and unseen tasks can be achieved through a markedly more streamlined yet robust test-time adjustment. Given its capacity for dynamic adjustment based on unseen samples, our approach constitutes a significantly novel solution for VLA adaptivity\nKachaev\net al.\n(\n2025\n); Liu\net al.\n(\n2025a\n)\n.\nFigure 3:\nReal-world setup and evaluation.\nWe evaluate nine real-world pick-and-place tasks covering\nExecution\n,\nVision\n, and\nSemantics\ngeneralization, with three tasks per category. The results show that TT-VLA consistently improves performance over baseline VLA models in real-world settings.\n4.3\nReal-World Results\nWe use OpenVLA as the base policy, and evaluate TT-VLA on nine unseen real-world tasks (see Fig.\n3\n).\nAs seen, our method consistently improves performance over OpenVLA in real-world settings, demonstrating effective generalization beyond simulation and highlighting the practicality of test-time adaptation in real robotic deployments.\nWe further report\na representative case study â€œput banana on plateâ€ in Fig.\n5\n, where the robot grasps the banana and moves it toward the plate. During the original execution, the gripper temporarily deviates from\nthe target region and moves away from the plate, signifying a substantial risk of task failure.\nHowever, by leveraging the dense, progress-based reward of TT-VLA, the policy enables rapid detection of task regression and on-the-fly behavioral adjustments.\nThe immediate reward feedback allows the VLA policy to correct deviations and realign with the task objective, ultimately completing the placement successfully.\nThis example highlights the advantage of instantaneous, progress-aware rewards in enabling rapid recovery from execution errors.\nMore real-world qualitative examples are shown in AppendixÂ Â§\nS6\n.\nFigure 4:\nImpact of reward design\n.\nThe results show that our progress-based reward consistently outperforms the standard GAE across tasks and models.\nLearning\nExecution\nVision\nSemantics\nStep\nNora\nOpenVLA\nNora\nOpenVLA\nNora\nOpenVLA\n1\n10.83%\n40.42%\n40.00%\n54.12%\n15.42%\n43.33%\n4\n11.25%\n41.25%\n42.08%\n56.25%\n17.50%\n46.25%\n8\n12.50%\n42.08%\n44.58%\n57.08%\n18.33%\n46.25%\n16\n11.25%\n42.08%\n43.33%\n55.42%\n17.50%\n45.42%\nTable 2:\nImpact of learning step\n. An update interval of 8 steps yield the best performance.\n4.4\nDiagnostic Experiments\nWe conduct an ablation study on both Nora and OpenVLA using three representative unseen tasks.\nReward/Advantage Design.\nWe first analyze the effect of discounting in GAE (see Eq.\n5\n).\nSpecifically, we compare the standard GAE setting with nonzero discount factor and trace parameter (\ne.g\n.\n,\nÎ³\n>\n0\n\\gamma>0\n,\nÎ»\n>\n0\n\\lambda>0\n) against our variant in which both\nÎ³\n\\gamma\nand\nÎ»\n\\lambda\nare set to zero.\nBy eliminating discounting and trace accumulation, TT-VLA emphasizes how each individual action contributes to immediate progress rather than estimating long-term returns.\nEmpirically, focusing on immediate progress yields consistent improvements in performance. For example, as shown in Fig.\n4\n, on the\nVision\ntask with OpenVLA, our setting achieves a success rate of 57.08%, compared to 55.00% when using standard GAE. We attribute this performance gain to the fact that long-horizon returns can be unreliable in this setting, occasionally assigning overly optimistic advantage signals to ineffective actions. These results suggest that instantaneous feedback can be more effective than incorporating discounted future rewards during test time.\nTest-Time Training Steps.\nWe then explore the impact of model update frequency in TT-VLA by varying the update interval over different environment steps (\ni.e\n.\n, 1, 4, 8, and 16). The number of learning steps is designed to balance the trade-off between rapid adaptation to newly collected data and the overall stability of the optimization process. Table\n2\nshows that updating the model every 8 steps yields the best performance. More frequent updates (\ne.g\n.\n, 1 step)\ncan destabilize training and limit the effectiveness of each update. In contrast, less frequent updates (\ne.g\n.\n, 16 steps) delay policy improvement and reduce learning efficiency. These findings suggest that an intermediate update frequency achieves a balance between rapid policy adaptation and stable optimization. Additional details are provided in Appendix Â§\nS4\n.\nFigure 5:\nReal-world case study\nillustrates how TT-VLAâ€™s instantaneous reward feedback enables rapid recovery from trajectory errors during deployment.\n4.5\nDiscussions on VLA Test-Time Training\nAs stated in Â§\n2.2\n, TTT was originally proposed for LLMs.\nA natural question is:\nCan test-time training methods in LLMs be directly applied to VLA models?\nTo address this question, we examine two representative approaches for VLAs: a self-supervised test-time training method, TLM\nHu\net al.\n(\n2025\n)\n, and a test-time reinforcement learning method, TTRL\nHu\net al.\n(\n2025\n)\n. Unless otherwise specified, we follow the same experimental setup as the diagnostic experiments, using the same tasks and baseline models for evaluation.\nWe first consider TLM\nHu\net al.\n(\n2025\n)\nthat enables test-time adaptation by directly minimizing input perplexity without any external supervision.\nWhen applied to VLAs, TLM updates model parameters by optimizing the likelihood of test-time observation sequences.\nAs shown in Table\n3\n, this strategy results in lower performance gains than TT-VLA.\nThe reason is that,\nunlike pure language tasks, VLA tasks involve complex interactions between perceptions, instruction understanding, and actions. Updates driven solely by input likelihood may overly emphasize representational consistency rather than task-oriented decision making. As a result,\nself-supervised test-time objectives cannot readily translate to the VLA domain.\nWe further compare TT-VLA with TTRL\nHu\net al.\n(\n2025\n)\n, a recently proposed test-time reinforcement learning approach. TTRL performs test-time adaptation by sampling multiple candidate outputs for each input and constructing a consensus pseudo-label via majority voting\nShao\net al.\n(\n2024\n)\n. This pseudo-label is then used to construct rule-based rewards, where outputs that match/mismatch the pseudo-label receive positive/zero rewards.\nAs reported in Table\n3\n, TTRL underperforms our proposed TT-VLA,\nindicating that the consensus-based pseudo-label is less effective for VLA tasks.\nOne possible reason is that\nmajority voting does not reflect action quality, and reward signals derived from output agreement fail to provide\ntask-aligned learning signals, thereby limiting the effectiveness of VLA test-time updates. More details of TLM and TTRL are provided in Appendix Â§\nS5\n.\nExecution\nVision\nSemantics\nModel\nNora\nOpenVLA\nNora\nOpenVLA\nNora\nOpenVLA\nTLM\n11.25%\n40.42%\n41.25%\n52.50%\n16.67%\n42.9%\nTTRL\n10.42%\n40.83%\n39.58%\n51.42%\n16.25%\n41.76%\nOurs\n12.50%\n42.08%\n44.58%\n57.08%\n18.33%\n46.25%\nTable 3:\nComparison of TTT methods\n. We compare TT-VLA with TLM and TTRL (see Â§\n4.5\n). As seen, TT-VLA achieves superior performance, highlighting the importance of progress-based reward for effective test-test adaptation in VLAs.\n5\nConclusion\nWhile VLA models have gained significant popularity on closed-form benchmarks, this work focuses on the flexibility of applying these models in evolving environments via test-time reinforcement learning.\nExperimental results demonstrate that TT-VLA consistently enhances performance on unseen tasks across diverse simulated and real-world scenarios, as well as across various VLA backbones.\nWe believe that our framework provides pioneering and foundational contributions to VLA models to flexibly refine action policies\nunder dynamic, previously unseen test-time cases.\nAcknowledgments\nThe views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Naval Research Laboratory (NRL) or the U.S. Government.\nReferences\nA. Ahmadian, C. Cremer, M. GallÃ©, M. Fadaee, J. Kreutzer, O. Pietquin, A. ÃœstÃ¼n, and S. Hooker (2024)\nBack to basics: revisiting reinforce style optimization for learning from human feedback in llms\n.\narXiv preprint arXiv:2402.14740\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n.\nA. Ajay, Y. Du, A. Gupta, J. B. Tenenbaum, T. S. Jaakkola, and P. Agrawal (2023)\nIs conditional generative modeling all you need for decision making?\n.\nIn\nICLR\n,\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nS. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang,\net al.\n(2025a)\nQwen2. 5-vl technical report\n.\narXiv preprint arXiv:2502.13923\n.\nExternal Links:\nLink\nCited by:\n1st item\n.\nZ. Bai, C. Gao, and M. Z. Shou (2025b)\nEVOLVE-vla: test-time training from environment feedback for vision-language-action models\n.\narXiv preprint arXiv:2512.14666\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n,\nÂ§2.2\n.\nA. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. S. Ryoo, G. Salazar, P. R. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. T. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich (2023)\nRT-1: robotics transformer for real-world control at scale\n.\nIn\nRSS\n,\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n,\nÂ§1\n,\nÂ§2.1\n.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\net al.\n(2020)\nLanguage models are few-shot learners\n.\nIn\nNeurIPS\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nH. Chen, H. Tu, F. Wang, H. Liu, X. Tang, X. Du, Y. Zhou, and C. Xie (2025a)\nSft or rl? an early investigation into training r1-like reasoning large vision-language models\n.\narXiv preprint arXiv:2504.11468\n.\nExternal Links:\nLink\nCited by:\nÂ§2.1\n.\nY. Chen, S. Tian, S. Liu, Y. Zhou, H. Li, and D. Zhao (2025b)\nConrft: a reinforced fine-tuning method for vla models via consistency policy\n.\nIn\nRSS\n,\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n,\nÂ§2.1\n.\nY. Chen and X. Li (2025)\nRLRC: reinforcement learning-based recovery for compressed vision-language-action models\n.\narXiv preprint arXiv:2506.17639\n.\nExternal Links:\nLink\nCited by:\nÂ§2.1\n.\nZ. Chen, R. Niu, H. Kong, and Q. Wang (2025c)\nTGRPO: fine-tuning vision-language-action model via trajectory-wise group relative policy optimization\n.\narXiv preprint arXiv:2506.08440\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n,\nÂ§2.1\n.\nC. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song (2023)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nIn\nRSS\n,\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nY. Cui, C. Han, and D. Liu (2024)\nCollaborative multi-task learning for multi-object tracking and segmentation\n.\nJournal on Autonomous Transportation Systems\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nG. Dulac-Arnold, N. Levine, D. J. Mankowitz, J. Li, C. Paduraru, S. Gowal, and T. Hester (2021)\nChallenges of real-world reinforcement learning: definitions, benchmarks and analysis\n.\nMachine Learning\n.\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nS. Fei, S. Wang, L. Ji, A. Li, S. Zhang, L. Liu, J. Hou, J. Gong, X. Zhao, and X. Qiu (2025)\nSRPO: self-referential policy optimization for vision-language-action models\n.\narXiv preprint arXiv:2511.15605\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n.\nS. Goyal, A. Kumar, S. Garg, Z. Kolter, and A. Raghunathan (2023)\nFinetune like you pretrain: improved finetuning of zero-shot vision models\n.\nIn\nCVPR\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nD. Guo, D. Yang, H. Zhang, J. Song, P. Wang, Q. Zhu, R. Xu, R. Zhang, S. Ma, X. Bi,\net al.\n(2025a)\nDeepseek-r1 incentivizes reasoning in llms through reinforcement learning\n.\nNature\n.\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nH. Guo, F. Zeng, F. Zhu, J. Wang, X. Wang, J. Zhou, H. Zhao, W. Liu, S. Ma, X. Zhang,\net al.\n(2025b)\nA comprehensive survey on continual learning in generative models\n.\narXiv preprint arXiv:2506.13045\n.\nExternal Links:\nLink\nCited by:\nÂ§2.1\n.\nY. Guo, J. Zhang, X. Chen, X. Ji, Y. Wang, Y. Hu, and J. Chen (2025c)\nImproving vision-language-action model with online reinforcement learning\n.\narXiv preprint arXiv:2501.16664\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n.\nC. Han, Q. Wang, Y. Cui, Z. Cao, W. Wang, S. Qi, and D. Liu (2023)\nE\n2\nvpt: an effective and efficient approach for visual prompt tuning\n.\nIn\nICCV\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nJ. Han, J. Park, D. Han, and W. Hwang (2025)\nWhen test-time adaptation meets self-supervised models\n.\narXiv preprint arXiv:2506.23529\n.\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen,\net al.\n(2022)\nLora: low-rank adaptation of large language models.\n.\nIn\nICLR\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n,\nÂ§4.1\n.\nJ. Hu, Z. Zhang, G. Chen, X. Wen, C. Shuai, W. Luo, B. Xiao, Y. Li, and M. Tan (2025)\nTest-time learning for large language models\n.\narXiv preprint arXiv:2505.20633\n.\nExternal Links:\nLink\nCited by:\nÂ§2.2\n,\nÂ§2.2\n,\nÂ§4.5\n,\nÂ§4.5\n,\nÂ§4.5\n.\nX. Hu, K. Zhang, M. Sun, A. Chen, C. Kuo, and R. Nevatia (2024)\nBaFTA: backprop-free test-time adaptation for zero-shot vision-language models\n.\narXiv preprint arXiv:2406.11309\n.\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nD. Huang, Z. Fang, T. Zhang, Y. Li, L. Zhao, and C. Xia (2025)\nCO-rft: efficient fine-tuning of vision-language-action models through chunked offline reinforcement learning\n.\narXiv preprint arXiv:2508.02219\n.\nExternal Links:\nLink\nCited by:\nÂ§2.1\n,\nÂ§2.2\n.\nJ. Huang, S. Yong, X. Ma, X. Linghu, P. Li, Y. Wang, Q. Li, S. Zhu, B. Jia, and S. Huang (2023)\nAn embodied generalist agent in 3d world\n.\narXiv preprint arXiv:2311.12871\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nC. Hung, Q. Sun, P. Hong, A. Zadeh, C. Li, U. Tan, N. Majumder, S. Poria,\net al.\n(2025)\nNora: a small open-sourced generalist vision language action model for embodied tasks\n.\narXiv preprint arXiv:2504.19854\n.\nExternal Links:\nLink\nCited by:\n1st item\n,\nTable 1\n,\nTable 1\n.\nM. Janner, Y. Du, J. B. Tenenbaum, and S. Levine (2022)\nPlanning with diffusion for flexible behavior synthesis\n.\nIn\nICML\n,\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nM. Jia, L. Tang, B. Chen, C. Cardie, S. Belongie, B. Hariharan, and S. Lim (2022)\nVisual prompt tuning\n.\nIn\nECCV\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nA. Jiang, Y. Gao, Y. Wang, Z. Sun, S. Wang, Y. Heng, H. Sun, S. Tang, L. Zhu, J. Chai,\net al.\n(2025a)\nIRL-vla: training an vision-language-action policy via reward world model\n.\narXiv preprint arXiv:2508.06571\n.\nExternal Links:\nLink\nCited by:\nÂ§2.1\n.\nB. Jiang, S. Chen, Q. Zhang, W. Liu, and X. Wang (2025b)\nAlphadrive: unleashing the power of vlms in autonomous driving via reinforcement learning and reasoning\n.\narXiv preprint arXiv:2503.07608\n.\nExternal Links:\nLink\nCited by:\nÂ§2.1\n.\nY. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, Y. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu, and L. Fan (2022)\nVIMA: general robot manipulation with multimodal prompts\n.\narXiv preprint arXiv:2210.03094\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nN. Kachaev, M. Kolosov, D. Zelezetsky, A. K. Kovalev, and A. I. Panov (2025)\nDonâ€™t blind your vla: aligning visual representations for ood generalization\n.\narXiv preprint arXiv:2510.25616\n.\nExternal Links:\nLink\nCited by:\nÂ§4.2\n.\nL. P. Kaelbling, M. L. Littman, and A. R. Cassandra (1998)\nPlanning and acting in partially observable stochastic domains\n.\nArtificial intelligence\n.\nExternal Links:\nLink\nCited by:\nÂ§3.1\n.\nA. Karmanov, D. Guan, S. Lu, A. El Saddik, and E. Xing (2024)\nEfficient test-time adaptation of vision-language models\n.\nIn\nCVPR\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nD. Kim, S. Park, H. Jang, J. Shin, J. Kim, and Y. Seo (2025)\nRobot-r1: reinforcement learning for enhanced embodied reasoning in robotics\n.\narXiv preprint arXiv:2506.00070\n.\nExternal Links:\nLink\nCited by:\nÂ§2.1\n.\nM. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang, and C. Finn (2024)\nOpenVLA: an open-source vision-language-action model\n.\narXiv preprint arXiv:2406.09246\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n,\nÂ§1\n,\nÂ§2.1\n,\nÂ§2.2\n,\n2nd item\n,\nTable 1\n,\nTable 1\n.\nP. Kormushev, S. Calinon, and D. G. Caldwell (2013)\nReinforcement learning in robotics: applications and real-world challenges\n.\nRobotics\n.\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nJ. Kwok, C. Agia, R. Sinha, M. Foutter, S. Li, I. Stoica, A. Mirhoseini, and M. Pavone (2025)\nRoboMonkey: scaling test-time sampling and verification for vision-language-action models\n.\narXiv preprint arXiv:2506.17811\n.\nExternal Links:\nLink\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nH. Li, Y. Zuo, J. Yu, Y. Zhang, Z. Yang, K. Zhang, X. Zhu, Y. Zhang, T. Chen, G. Cui,\net al.\n(2025a)\nSimpleVLA-rl: scaling vla training via reinforcement learning\n.\narXiv preprint arXiv:2509.09674\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n,\nÂ§2.1\n,\nÂ§3.2\n.\nH. Li, P. Ding, R. Suo, Y. Wang, Z. Ge, D. Zang, K. Yu, M. Sun, H. Zhang, D. Wang,\net al.\n(2025b)\nVLA-rft: vision-language-action reinforcement fine-tuning with verified rewards in world simulators\n.\narXiv preprint arXiv:2510.00406\n.\nExternal Links:\nLink\nCited by:\nÂ§2.1\n.\nX. Li, M. Liu, H. Zhang, C. Yu, J. Xu, H. Wu, C. Cheang, Y. Jing, W. Zhang, H. Liu,\net al.\n(2023)\nVision-language foundation models as effective robot imitators\n.\narXiv preprint arXiv:2311.01378\n.\nCited by:\nÂ§S1.1\n.\nZ. Li, D. Zhang, M. Zhang, J. Zhang, Z. Liu, Y. Yao, H. Xu, J. Zheng, P. Wang, X. Chen,\net al.\n(2025c)\nFrom system 1 to system 2: a survey of reasoning large language models\n.\narXiv preprint arXiv:2502.17419\n.\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nZ. Liang, Y. Mu, M. Ding, F. Ni, M. Tomizuka, and P. Luo (2023)\nAdaptdiffuser: diffusion models as adaptive self-evolving planners\n.\narXiv preprint arXiv:2302.01877\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nJ. Liu, M. Liu, Z. Wang, P. An, X. Li, K. Zhou, S. Yang, R. Zhang, Y. Guo, and S. Zhang (2024a)\nRobomamba: efficient vision-language-action model for robotic reasoning and manipulation\n.\nIn\nNeurIPS\n,\nExternal Links:\nLink\nCited by:\nÂ§2.1\n,\nÂ§2.2\n.\nJ. Liu, F. Gao, B. Wei, X. Chen, Q. Liao, Y. Wu, C. Yu, and Y. Wang (2025a)\nWhat can rl bring to vla generalization? an empirical study\n.\narXiv preprint arXiv:2505.19789\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n,\nAppendix S10\n,\n3rd item\n,\nAppendix S3\n,\nÂ§3.2\n,\n1st item\n,\n2nd item\n,\n3rd item\n,\nÂ§4.1\n,\nÂ§4.2\n,\nTable 1\n,\nTable 1\n.\nY. Liu, J. C. Liang, R. Tang, Y. Lee, M. Rabbani, S. A. Dianat, R. Rao, L. Huang, D. Liu, Q. Wang, and C. Han (2025b)\nRe-imagining multimodal instruction tuning: a representation view\n.\nIn\nICLR\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nY. Liu, P. Kothari, B. van Delft, B. Bellot-Gurlet, T. Mordan, and A. Alahi (2021)\nTTT++: when does self-supervised test-time training fail or thrive?\n.\nIn\nNeurIPS\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nZ. Liu, J. Lyn, W. Zhu, X. Tian, and Y. Graham (2024b)\nALoRA: allocating low-rank adaptation for fine-tuning large language models\n.\nIn\nNAACL\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nG. Lu, W. Guo, C. Zhang, Y. Zhou, H. Jiang, Z. Gao, Y. Tang, and Z. Wang (2025)\nVla-rl: towards masterful and general robotic manipulation with scalable reinforcement learning\n.\narXiv preprint arXiv:2505.18719\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n,\nÂ§2.1\n.\nX. Ma, J. Zhang, S. Guo, and W. Xu (2023)\nSwapprompt: test-time prompt adaptation for vision-language models\n.\nIn\nNeurIPS\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nM. S. Mark, T. Gao, G. G. Sampaio, M. K. Srirama, A. Sharma, C. Finn, and A. Kumar (2024)\nPolicy agnostic rl: offline rl and online rl fine-tuning of any class and backbone\n.\narXiv preprint arXiv:2412.06685\n.\nExternal Links:\nLink\nCited by:\nÂ§2.1\n,\nÂ§2.2\n.\nO. Mees, L. Hermann, and W. Burgard (2022)\nWhat matters in language conditioned robotic imitation learning over unstructured data\n.\nIEEE Robotics and Automation Letters\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n,\nÂ§2.1\n.\nM. Mosbach, M. Andriushchenko, and D. Klakow (2021)\nOn the stability of fine-tuning bert: misconceptions, explanations, and strong baselines\n.\nIn\nICLR\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala (2019)\nPyTorch: an imperative style, high-performance deep learning library\n.\nIn\nNeurIPS\n,\nExternal Links:\nLink\nCited by:\nAppendix S8\n.\nK. Pertsch, K. Stachowicz, B. Ichter, D. Driess, S. Nair, Q. Vuong, O. Mees, C. Finn, and S. Levine (2025)\nFast: efficient action tokenization for vision-language-action models\n.\narXiv preprint arXiv:2501.09747\n.\nExternal Links:\nLink\nCited by:\n1st item\n.\nA. Plaat, A. Wong, S. Verberne, J. Broekens, N. van Stein, and T. BÃ¤ck (2024)\nReasoning with large language models, a survey\n.\nCoRR\n.\nExternal Links:\nLink\nCited by:\nÂ§2.1\n.\nV. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine (2020)\nSkew-fit: state-covering self-supervised reinforcement learning\n.\nIn\nICML\n,\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n,\nÂ§2.1\n,\nÂ§2.2\n.\nR. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn (2023)\nDirect preference optimization: your language model is secretly a reward model\n.\nIn\nNeurIPS\n,\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n.\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017)\nProximal policy optimization algorithms\n.\narXiv preprint arXiv:1707.06347\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu,\net al.\n(2024)\nDeepseekmath: pushing the limits of mathematical reasoning in open language models\n.\narXiv preprint arXiv:2402.03300\n.\nExternal Links:\nLink\nCited by:\nAppendix S7\n,\nÂ§4.5\n.\nI. Shenfeld, J. Pari, and P. Agrawal (2025)\nRLâ€™s razor: why online reinforcement learning forgets less\n.\narXiv preprint arXiv:2509.04259\n.\nExternal Links:\nLink\nCited by:\nÂ§2.1\n.\nJ. Shu, Z. Lin, and Y. Wang (2025)\nRFTF: reinforcement fine-tuning for embodied agents with temporal feedback\n.\narXiv preprint arXiv:2505.19767\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n.\nY. Sun, X. Wang, Z. Liu, J. Miller, A. Efros, and M. Hardt (2020)\nTest-time training with self-supervision for generalization under distribution shifts\n.\nIn\nICML\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nR. S. Sutton, A. G. Barto,\net al.\n(1999a)\nReinforcement learning\n.\nJournal of Cognitive Neuroscience\n.\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nR. S. Sutton, D. McAllester, S. Singh, and Y. Mansour (1999b)\nPolicy gradient methods for reinforcement learning with function approximation\n.\nIn\nNeurIPS\n,\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nM. Tambe, W. L. Johnson, R. M. Jones, F. Koss, J. E. Laird, P. S. Rosenbloom, and K. Schwamb (1995)\nIntelligent agents for interactive simulation environments\n.\nAI Magazine\n.\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nS. Tan, K. Dou, Y. Zhao, and P. KrÃ¤henbÃ¼hl (2025)\nInteractive post-training for vision-language-action models\n.\narXiv preprint arXiv:2505.17016\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n.\nS. Tao, F. Xiang, A. Shukla, Y. Qin, X. Hinrichsen, X. Yuan, C. Bao, X. Lin, Y. Liu, T. Chan,\net al.\n(2024)\nManiskill3: gpu parallelized robotics simulation and rendering for generalizable embodied ai\n.\narXiv preprint arXiv:2410.00425\n.\nExternal Links:\nLink\nCited by:\nÂ§4.1\n.\nO. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu, J. Luo, Y. L. Tan, L. Y. Chen, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh, C. Finn, and S. Levine (2024)\nOcto: an open-source generalist robot policy\n.\narXiv preprint arXiv:2405.12213\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale,\net al.\n(2023)\nLlama 2: open foundation and fine-tuned chat models\n.\narXiv preprint arXiv:2307.09288\n.\nExternal Links:\nLink\nCited by:\n2nd item\n.\nL. Trung, X. Zhang, Z. Jie, P. Sun, X. Jin, and H. Li (2024)\nReft: reasoning with reinforced fine-tuning\n.\nIn\nACL\n,\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n.\nT. Wang, Z. Fang, H. Xue, C. Zhang, M. Jin, W. Xu, D. Shu, S. Yang, Z. Wang, and D. Liu (2024a)\nLarge vision-language model security: a survey\n.\nIn\nFCS\n,\nCited by:\nÂ§S1.1\n.\nT. Wang, D. Liu, J. Chenhao Liang, W. Yang, Q. Wang, C. Han, J. Luo, and R. Tang (2025a)\nExploring the adversarial vulnerabilities of vision-language-action models in robotics\n.\nIn\nICCV\n,\nExternal Links:\nLink\nCited by:\nÂ§2.1\n,\nÂ§2.2\n,\nÂ§2.2\n.\nT. Wang, Y. Liu, J. C. Liang, Y. Cui, Y. Mao, S. Nie, J. Liu, F. Feng, Z. Xu, C. Han,\net al.\n(2024b)\nM\n2\npt: multimodal prompt tuning for zero-shot instruction learning\n.\nIn\nEMNLP\n,\nCited by:\nÂ§2.2\n.\nY. Wang, P. Ding, L. Li, C. Cui, Z. Ge, X. Tong, W. Song, H. Zhao, W. Zhao, P. Hou,\net al.\n(2025b)\nVLA-adapter: an effective paradigm for tiny-scale vision-language-action model\n.\narXiv preprint arXiv:2509.09372\n.\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nY. Wang, L. Yang, B. Li, Y. Tian, K. Shen, and M. Wang (2025c)\nRevolutionizing reinforcement learning framework for diffusion large language models\n.\narXiv preprint arXiv:2509.06949\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le (2022)\nFinetuned language models are zero-shot learners\n.\nIn\nICLR\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nJ. Wen, M. Zhu, J. Liu, Z. Liu, Y. Yang, L. Zhang, S. Zhang, Y. Zhu, and Y. Xu (2025a)\nDVLA: diffusion vision-language-action model with multimodal chain-of-thought\n.\narXiv preprint arXiv:2509.25681\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nJ. Wen, Y. Zhu, M. Zhu, Z. Tang, J. Li, Z. Zhou, X. Liu, C. Shen, Y. Peng, and F. Feng (2025b)\nDiffusionVLA: scaling robot foundation models via unified diffusion and autoregression\n.\nIn\nICML\n,\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nR. Wu, C. Guo, Y. Su, and K. Q. Weinberger (2021)\nOnline adaptation to label distribution shift\n.\nIn\nNeurIPS\n,\nExternal Links:\nLink\nCited by:\nÂ§2.1\n.\nZ. Xian and N. Gkanatsios (2023)\nChaineddiffuser: unifying trajectory diffusion and keypose prediction for robotic manipulation\n.\nIn\nPMLR\n,\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n.\nZ. Xiao, S. Yan, J. Hong, J. Cai, X. Jiang, Y. Hu, J. Shen, Q. Wang, and C. G. Snoek (2025)\nDynaprompt: dynamic test-time prompt tuning\n.\narXiv preprint arXiv:2501.16404\n.\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nC. Xu, Q. Li, J. Luo, and S. Levine (2024)\nRldg: robotic generalist policy distillation via reinforcement learning\n.\narXiv preprint arXiv:2412.09858\n.\nExternal Links:\nLink\nCited by:\nÂ§2.1\n.\nA. Ye, Z. Zhang, B. Wang, X. Wang, D. Zhang, and Z. Zhu (2025)\nVLA-r1: enhancing reasoning in vision-language-action models\n.\narXiv preprint arXiv:2510.01623\n.\nExternal Links:\nLink\nCited by:\nÂ§2.1\n.\nH. S. Yoon, E. Yoon, J. T. J. Tee, M. A. Hasegawa-Johnson, Y. Li, and C. D. Yoo (2024)\nC-tpt: calibrated test-time prompt tuning for vision-language models via text feature dispersion\n.\nIn\nICLR\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nR. Zeng, C. Han, Q. Wang, C. Wu, T. Geng, L. Huangg, Y. N. Wu, and D. Liu (2024)\nVisual fourier prompt tuning\n.\nIn\nNeurIPS\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nS. Zhai, Q. Zhang, T. Zhang, F. Huang, H. Zhang, M. Zhou, S. Zhang, L. Liu, S. Lin, and J. Pang (2025)\nA vision-language-action-critic model for robotic real-world reinforcement learning\n.\narXiv preprint arXiv:2509.15937\n.\nExternal Links:\nLink\nCited by:\nÂ§3.2\n.\nH. Zhang, Z. Zhuang, H. Zhao, P. Ding, H. Lu, and D. Wang (2025)\nReinboT: amplifying robot visual-language manipulation with reinforcement learning\n.\narXiv preprint arXiv:2505.07395\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n,\nÂ§2.1\n,\nÂ§2.2\n.\nZ. Zhang, K. Zheng, Z. Chen, J. Jang, Y. Li, S. Han, C. Wang, M. Ding, D. Fox, and H. Yao (2024)\nGrape: generalizing robot policy via preference alignment\n.\narXiv preprint arXiv:2411.19309\n.\nExternal Links:\nLink\nCited by:\nÂ§S1.2\n.\nH. Zhao, Y. Liu, A. Alahi, and T. Lin (2023)\nOn pitfalls of test-time adaptation\n.\nIn\nICML\n,\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nR. Zheng, Y. Liang, S. Huang, J. Gao, H. DaumÃ© III, A. Kolobov, F. Huang, and J. Yang (2025)\nTracevla: visual trace prompting enhances spatial-temporal awareness for generalist robotic policies\n.\nIn\nICLR\n,\nExternal Links:\nLink\nCited by:\n4th item\n,\nTable 1\n,\nTable 1\n.\nY. Zhu, G. Zhang, C. Xu, H. Shen, X. Chen, G. Wu, and L. Wang (2024)\nEfficient test-time prompt tuning for vision-language models\n.\narXiv preprint arXiv:2408.05775\n.\nExternal Links:\nLink\nCited by:\nÂ§2.2\n.\nB. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid, Q. Vuong, V. Vanhoucke, H. T. Tran, R. Soricut, A. Singh, J. Singh, P. Sermanet, P. R. Sanketi, G. Salazar, M. S. Ryoo, K. Reymann, K. Rao, K. Pertsch, I. Mordatch, H. Michalewski, Y. Lu, S. Levine, L. Lee, T. E. Lee, I. Leal, Y. Kuang, D. Kalashnikov, R. Julian, N. J. Joshi, A. Irpan, B. Ichter, J. Hsu, A. Herzog, K. Hausman, K. Gopalakrishnan, C. Fu, P. Florence, C. Finn, K. A. Dubey, D. Driess, T. Ding, K. M. Choromanski, X. Chen, Y. Chebotar, J. Carbajal, N. Brown, A. Brohan, M. G. Arenas, and K. Han (2023)\nRT-2: vision-language-action models transfer web knowledge to robotic control\n.\nIn\nCoRL\n,\nExternal Links:\nLink\nCited by:\nÂ§S1.1\n,\nÂ§1\n,\nÂ§2.1\n.\nY. Zuo, K. Zhang, L. Sheng, S. Qu, G. Cui, X. Zhu, H. Li, Y. Zhang, X. Long, E. Hua,\net al.\n(2025)\nTtrl: test-time reinforcement learning\n.\narXiv preprint arXiv:2504.16084\n.\nExternal Links:\nLink\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nSUMMARY OF THE APPENDIX\nThis appendix contains additional experimental results and discussions of our work, organized as:\nâ€¢\nÂ§\nS1\nprovides\nmore related works\non VLA models\nâ€¢\nÂ§\nS2\nprovides\nLemma proof\n.\nâ€¢\nÂ§\nS3\nincludes\nmore details on tasks\n.\nâ€¢\nÂ§\nS4\nsupplies additional information on\ndiagnostic experiments\n.\nâ€¢\nÂ§\nS5\nsupplies additional discussions on\nTest-Time Training\n.\nâ€¢\nÂ§\nS6\nprovides\nmore qualitative results\n.\nâ€¢\nÂ§\nS7\nadds discussions on the practicalness of using\nTest-Time GRPO in VLAs\n.\nâ€¢\nÂ§\nS8\nincludes the\nreproducibility statement and pseudo code\nof our method.\nâ€¢\nÂ§\nS9\nhighlights the\ntechnical contributions\nof our method.\nâ€¢\nÂ§\nS10\noffers a\nsummary of licenses and consent\n, and lists usage terms for all models and datasets.\nâ€¢\nÂ§\nS11\nincludes additional discussions on\nethics concerns\n.\nâ€¢\nÂ§\nS12\ndiscusses on\nfuture directions\n, highlighting potential areas for further research.\nâ€¢\nÂ§\nS13\nprovides an\nAI disclosure\n, and notes that AI assistance was limited to grammar checking.\nAppendix S1\nMore Related Works\nS1.1\nMore Discussions on VLA\nRecent studies\nBrohan\net al.\n(\n2023\n); Mees\net al.\n(\n2022\n); Pong\net al.\n(\n2020\n)\nhave advanced the potential of large-scale Vision Language Models (VLMs) as key enablers for generalist robots, demonstrating promising generalization across a variety of scenes\nZitkovich\net al.\n(\n2023\n); Jiang\net al.\n(\n2022\n); Team\net al.\n(\n2024\n); Huang\net al.\n(\n2023\n); Li\net al.\n(\n2023\n); Cui\net al.\n(\n2024\n); Wang\net al.\n(\n2024a\n)\n. They generally achieve action planning via two main branches: I. Discretization-based approaches\nKim\net al.\n(\n2024\n); Brohan\net al.\n(\n2023\n); Zitkovich\net al.\n(\n2023\n)\n, which discretize the action space into a small set of action tokens; and II. Diffusion-based approaches\nChi\net al.\n(\n2023\n); Xian and Gkanatsios (\n2023\n); Janner\net al.\n(\n2022\n); Liang\net al.\n(\n2023\n); Ajay\net al.\n(\n2023\n)\n, which integrate diffusion heads for action prediction.\nIn our study, we focus on and generalize discretization-based approaches. The reason is that most diffusion-head VLA models adopt a separate action decoder, typically a latent diffusion process that maps visual and instruction embeddings to an action embedding stream, followed by a shallow MLP to regress the robotâ€™s joint space\nWen\net al.\n(\n2025b\n)\n. This design renders reinforcement-learning (RL) optimization impractical (\ni.e\n.\n, also for diffusion large language model (DLLM) RL optimization\nWang\net al.\n(\n2025c\n)\n) for three technical reasons:\n(i) the resulting policy is implicit and does not expose a tractable per-step log-likelihood (\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\nâˆ£\ns\n)\n\\log\\pi_{\\theta}(a\\mid s)\n), precluding policy-gradient methods (\ne.g\n.\n, REINFORCE\nSutton\net al.\n(\n1999b\n)\n/PPO\nSchulman\net al.\n(\n2017\n)\n) and entropy regularization;\n(ii) action emission requires tens of denoising iterations per control step, creating an inner stochastic chain misaligned with environment time, which breaks step-wise credit assignment; and\n(iii) the diffusion noise-prediction objective is mismatched with return-based RL objectives, while the terminal MLP head is effectively deterministic, suppressing exploration.\nHowever, we notice a very recent paper dVLA\nWen\net al.\n(\n2025a\n)\ndecodes actions as a discrete, autoregressive token sequence conditioned on state/instruction, making the current RL attempts possible to apply to diffusion-based approaches. While the code is not publicly available for us to evaluate, we highlight that our method can be naturally applied to this line of research.\nS1.2\nRL Methods for VLA\nAs we mentioned in our study, recently, some efforts have attempted to apply RL to the VLA training stage, leaving the test-time adjustments underexplored. In light of this view, we aim to fill the last puzzle of on-the-fly policy adaptation. We list some research with high impact on the integration of RL on VLAs.\nGRAPE\nZhang\net al.\n(\n2024\n)\nuses Direct Preference Optimization (DPO)\nRafailov\net al.\n(\n2023\n)\nto train VLAs by integrating human preferences. ConRFT\nChen\net al.\n(\n2025b\n)\nintroduces Reinforced Fine-Tuning\nTrung\net al.\n(\n2024\n)\nto train VLAs in real-world environments, iteratively training VLAs\nthrough alternating RL and SFT rounds. ReinboT\nZhang\net al.\n(\n2025\n)\nrises dense reward design and optimized VLA through reward maximization. iRe-VLA\nGuo\net al.\n(\n2025c\n)\nproposed an iterative training framework that combines SFT and RL stages to address training instability and computational overhead.\nRIPT-VLA\nTan\net al.\n(\n2025\n)\nemploys REINFORCE Leave-One-Out (RLOO)\nAhmadian\net al.\n(\n2024\n)\nfor VLA training.\nLiu\net al.\n(\n2025a\n)\ninvestigates RLâ€™s impact on VLA generalization capabilities, demonstrating improvements\nover SFT in unseen environments, objects, and textures.\nVLA-RL\nLu\net al.\n(\n2025\n)\napplies the PPO; TGRPO and SimpleVLA-RL\nChen\net al.\n(\n2025c\n); Li\net al.\n(\n2025a\n)\nevaluate trajectories and optimize VLA with GRPO variants; RFTF\nShu\net al.\n(\n2025\n)\nuses value models to generate dense rewards in embodied scenarios for VLA online RL; and SRPO\nFei\net al.\n(\n2025\n)\nleverages a world model to generate progress-based dense rewards.\nThough promising, it is important to note that current RL-based approaches all operate during training, while real-world deployments inevitably involve evolving conditions and distributional shifts at test time, necessitating VLAs capable of adaptive adjustment in response.\nThe approach most relevant to our work is EVOLVE-VLA\nBai\net al.\n(\n2025b\n)\n, which utilizes task progress as a reward signal for reinforcement learning. However, we should notice that EVOLVE-VLA optimizes the policy using GRPO, which incurs substantial computational overhead and is therefore less suitable for real-time robotic deployment. This limitation becomes particularly pronounced in real-world robotic settings, where strict latency constraints are critical.\nAppendix S2\nLemma Proof\nIn this section, we provide the proof of Lemma\n1\n, which characterizes the relationship between GAE and the reward-only advantage used in TT-VLA. This result formally justifies our value-free test-time optimization objective.\nProof.\n(i) For (a): When\nÎ»\n=\n0\n\\lambda=0\n, the geometric weighting term\n(\nÎ³\nâ€‹\nÎ»\n)\nl\n(\\gamma\\lambda)^{l}\nvanishes for all\nl\n>\n0\nl>0\n, Utilizing (14), it yields\nA\nt\n=\nÎ´\nt\nA_{t}=\\delta_{t}\n.\n(ii) For (b): When\nÎ³\n=\n0\n\\gamma=0\n, (14) and (15) respectively yields\nA\nt\n=\nÎ´\nt\n,\nÎ´\nt\n=\nr\nt\nâˆ’\nV\nâ€‹\n(\ns\nt\n)\n,\nA_{t}=\\delta_{t},\\;\\delta_{t}=r_{t}-V(s_{t}),\n(16)\n(16) implies that when\nV\nâ€‹\n(\ns\n)\nâ‰¡\n0\nV(s)\\equiv 0\n, there holds\nA\nt\n=\nÎ´\nt\n=\nr\nt\n,\nA_{t}=\\delta_{t}=r_{t},\n(17)\nwhich completes the proof.\nâˆ\nAppendix S3\nTask Details\nFor simulation tasks, we follow\nLiu\net al.\n(\n2025a\n)\nto define three dimensions of generalization problems for unseen tasks, which are\nExecution\n,\nVision\n, and\nSemantics\n.\nThe training task setting: At the beginning of each episode, an object is sampled from the 16 training objects and a table appearance is sampled from the 16 training textures. The object and the receptacle (yellow plate) are placed on the table, with their positions uniformly randomized within a rectangular region. The language instruction follows the template â€œput\nO\nO\non\nR\nR\nâ€, where\nO\nO\nand\nR\nR\ndenote the object and receptacle names, respectively.\nExecution\nexplores changes in the initial positions of both the object, the receptacle, the robot initial pose, and mid-episode changes in the objectâ€™s position during task execution.\nâ€¢\nUnseen Object & Receptacle Position (Obj. Pos.): The object and the receptacle are placed on the table, with their positions randomized within a larger square region that surrounds the original rectangular area. All other settings follow the Training setting.\nâ€¢\nUnseen Robot Init Pose (Robot Pose): At the start of each episode, the initial poses of all robots are randomized instead of being fixed as in the Training setting. All other settings remain identical to the Training setting.\nâ€¢\nMid-Episode Object Reposition (Obj. Rep.): At the fifth timestep of each episode, the object is teleported to a new randomly sampled position on the table. All other settings remain identical to the Training setting.\nVision\nincludes both foreground and background changes, as well as image-level Dynamic Noise, applied with either weak or strong intensity.\nâ€¢\nUnseen Table (Table): The table appearance is sampled from 5 unseen appearance.\nâ€¢\nWeak Dynamic Texture (Texture-w): In addition to sampling an object and a table appearance, a texture is selected from the 16 available textures at the start of each episode. This texture is cropped and resized at each timestep differently, and overlaid onto the object, receptacle, and robot arm with a transparency factor of 0.3.\nâ€¢\nStrong Dynamic Texture (Texture-s): The settings matches the Weak Dynamic Texture setting, except that the image mixing transparency is increased to 0.5.\nâ€¢\nWeak Dynamic Noise (Noise-w): In addition to sampling an object and a table appearance, a texture is selected from the 16 available textures at the start of each episode. The texture is cropped and resized at each timestep differently and overlaid over the entire image with a transparency factor of 0.3.\nâ€¢\nStrong Dynamic Noise (Noise-s): The settings matches the Weak Dynamic Noise setting, except that the image mixing transparency is increased to 0.5\nSemantics\nconsiders previously unseen variations in Objects, Receptacles, and Instruction Phrasings.\nâ€¢\nUnseen Objects (Object): The object is sampled from 9 unseen objects, while all other settings follow the Training setting.\nâ€¢\nUnseen Receptacles (Recep.): In addition to sampling an object and a table appearance, a receptacle is selected from 16 unseen receptacles at the start of each episode, replacing the default training receptacle (yellow plate). All other settings follow the Training setting\nâ€¢\nUnseen Instruction Phrasing (Instruct): In addition to sampling an object and a table appearance, a language instruction template is selected from 16 unseen templates (Same as\nLiu\net al.\n(\n2025a\n)\n) at the start of each episode, replacing the default instruction (â€œput\nO\nO\non\nR\nR\nâ€). All other settings follow the Training setting.\nâ€¢\nSeen Multi-Object (M-Obj. (IND)): At the beginning of each episode, two distinct objects are sampled from the 16 training objects along with one of the 16 training table appearances. Both objects and the receptacle (yellow plate) are placed on the table, with their positions randomly initialized within a rectangular region.\nâ€¢\nUnseen Multi-Object (M-Obj. (OOD)): Two distinct objects are sampled from the nine unseen objects, with all other settings identical to the Seen Multi-Object settings.\nâ€¢\nDistractive Receptacle (Dist Recep.): In addition to sampling an object and a table appearance, a distractor receptacle is selected from 16 unseen receptacles at the start of each episode and placed on the table without being used in the task. All other settings follow the Training setting.\nâ€¢\nMulti-Receptacle (M Recep.): At the beginning of each episode, an object is sampled from the 16 training objects, two distinct receptacles are sampled from the 16 unseen receptacles, and a table appearance is selected from the 16 training textures. The object and both receptacles are placed on the table, with their positions randomly initialized within a rectangular region.\nFor real-world evaluation, we assess our method on nine unseen manipulation tasks designed to test generalization across execution, vision, and semantic dimensions. The execution tasks consist of are â€œput banana on plateâ€, â€œput lemon on plateâ€, â€œput apple on plateâ€ under different initial robot configurations, evaluating robustness to variations in starting states. The vision tasks use the same instructions but introduce different background appearances to assess visual generalization. The semantic tasks also follow the same instruction templates but involve an unseen plate at test time, evaluating the modelâ€™s ability to generalize to novel semantic contexts. The nine tasks are illustrated in Fig.\n3\n.\nAppendix S4\nAdditional Details on Diagnostic Experiments\nThis section provides additional implementation details for the diagnostic experiments discussed in Â§\n4.4\n. We conduct diagnostic experiments using Nora and OpenVLA. We evaluate one task from each dimension:\nexecution\n,\nvision\n, and\nsemantics\n. Specifically, we use Task Robot Pose (\nexecution\n), Task Table (\nvision\n), and Task Object (\nsemantics\n) for all ablations to ensure controlled and comparable evaluations across settings.\nFor Advantage Design, the standard GAE baseline uses a discount factor of\nÎ³\n=\n0.99\n\\gamma=0.99\nand a trace parameter of\nÎ»\n=\n0.95\n\\lambda=0.95\n, with a truncated horizon length of\nl\n=\n8\nl=8\nfor advantage estimation. In contrast, our method disables both discounting and trace accumulation by setting\nÎ³\n=\n0\n\\gamma=0\nand\nÎ»\n=\n0\n\\lambda=0\n, yielding a one-step, reward-only advantage.\nAppendix S5\nAdditional Details on Test-Time Training Discussions\nThis section provides implementation details for adapting TLM and TTRL to VLA models.\nFor TLM, we follow the original formulation and perform test-time adaptation by minimizing the perplexity of the instruction prompt. Concretely, given a task instruction, we optimize the model parameters to reduce the negative log-likelihood of the instruction tokens, without relying on external supervision or environment rewards. We set the loss weighting coefficient to\nÎ»\n=\n0.1\n\\lambda=0.1\nand use a threshold value of\n0\nfor triggering updates. The policy is updated every 8 environment steps. We apply LoRA to update the policy, using a rank of 32 and a learning rate of\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\n.\nFor TTRL, we adapt the consensus-based test-time reinforcement learning framework to the VLA setting. At each decision step, we sample multiple candidate action tokens from the model to construct a pseudo-label via majority voting. We set the voting group size to 8. The reward function is defined as a binary signal: a reward of 1 is assigned if the sampled action token matches the pseudo-label, and 0 otherwise. Policy updates are performed at every environment step to accommodate the step-wise nature of action execution in real-time settings. We employ LoRA to update the policy parameters, with a rank of 32 and a learning rate of\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\n.\nAppendix S6\nAdditional Real-world Qualitative Results\nThis section presents additional qualitative results from real-world scenarios, complementing results in Â§\n4.3\nand further demonstrating the effectiveness of TT-VLA. Fig.\nS1\npresents three real-world rollouts of the â€œput banana on plateâ€ task using TT-VLA. In the first episode, the robot initially grasps the banana but places it at an incorrect location. It then re-grasps the banana and successfully places it onto the plate. In the second episode, the robot grasps the banana and moves it to a position behind the plate; the policy subsequently corrects its direction and completes the placement. Similarly, in the third episode, the robot initially moves past to the right side of the plate before adjusting its motion to place the banana correctly. These qualitative results demonstrate TT-VLAâ€™s ability to recover from execution errors and handle real-world uncertainties without retraining or human intervention.\nFigure S1:\nAdditional real-world qualitative results.\nEach row shows a real-world episode of the â€œput banana on plateâ€ task, illustrating how TT-VLA adapts online to execution deviations and successfully completes the task using progress-based reward feedback.\nInput:\nPretrained VLA policy\nÏ€\nÎ¸\n\\pi_{\\theta}\n, frozen progress estimator\nÎ¦\nâ€‹\n(\no\n0\n:\nt\n,\nl\n)\n\\Phi(o_{0:t},l)\n,\nlanguage instruction\nl\nl\n, observation horizon\nH\nH\n, update interval\nK\nK\n,\nclipping threshold\nÎµ\n\\varepsilon\n, learning rate\nÎ·\n\\eta\nOutput:\nTask actions\n1:\nfor\neach episode\ndo\n2: â€Šâ€‚â€…Load pretrained VLA policy\nÏ€\nÎ¸\n\\pi_{\\theta}\n; progress\np\n0\nâ†\n0\np_{\\text{0}}\\leftarrow 0\n; buffer\nâ„¬\nâ†\nâˆ…\n\\mathcal{B}\\leftarrow\\emptyset\n; Environment resets to initial state\ns\n0\ns_{0}\n3:\nfor\neach time step\nt\n=\n0\n,\n1\n,\n2\n,\nâ€¦\n,\nT\nt=0,1,2,\\dots,T\ndo\n4:â€‚â€…â€‚â€…Sample\na\nt\nâˆ¼\nÏ€\nÎ¸\nâ€‹\n(\na\nt\nâˆ£\no\nt\nâˆ’\n1\n,\nl\n)\na_{t}\\sim\\pi_{\\theta}(a_{t}\\mid o_{t-1},l)\n, get\nlog\nâ¡\nÏ€\nÎ¸\nold\nâ€‹\n(\na\nt\nâˆ£\no\nt\n)\n\\log\\pi_{\\theta_{\\text{old}}}(a_{t}\\mid o_{t})\n, and execute\na\nt\na_{t}\n5: â€‚â€…â€‚â€…Get new oberservation\no\nt\n+\n1\no_{t+1}\n6: â€‚â€…â€‚â€…Compute\np\nt\nâ†\nÎ¦\nâ€‹\n(\no\n0\n:\nt\n,\nl\n)\np_{t}\\leftarrow\\Phi(o_{0:t},l)\nâŠ³\n\\triangleright\nEq.\n6\n7: â€‚â€…â€‚â€…Compute\nr\nt\nâ†\np\nt\nâˆ’\np\nt\nâˆ’\n1\nr_{t}\\leftarrow p_{t}-p_{t-1}\nâŠ³\n\\triangleright\nEq.\n7\n8: â€‚â€…â€‚â€…Store\n(\no\nt\n+\n1\n,\na\nt\n,\nr\nt\n,\nlog\nâ¡\nÏ€\nÎ¸\nold\nâ€‹\n(\na\nt\nâˆ£\no\nt\n)\n)\n(o_{t+1},a_{t},r_{t},\\log\\pi_{\\theta_{\\text{old}}}(a_{t}\\mid o_{t}))\nin\nâ„¬\n\\mathcal{B}\n9:\nif\nt\nmod\nK\n=\n0\nt\\bmod K=0\nthen\n10:\nfor\neach\n(\no\ni\n,\na\ni\n,\nr\ni\n,\nlog\nâ¡\nÏ€\nÎ¸\nold\nâ€‹\n(\na\ni\nâˆ£\no\ni\n)\n)\nâˆˆ\nâ„¬\n(o_{i},a_{i},r_{i},\\log\\pi_{\\theta_{\\text{old}}}(a_{i}\\mid o_{i}))\\in\\mathcal{B}\ndo\n11:â€‚â€…â€‚â€…â€‚â€…â€‚â€…Compute\nr\ni\n(\nÎ¸\n)\nâ†\nexp\n(\nlog\nÏ€\nÎ¸\n(\na\ni\nâˆ£\no\ni\n)\nr_{i}(\\theta)\\leftarrow\\exp(\\log\\pi_{\\theta}(a_{i}\\mid o_{i})\nâˆ’\nlog\nÏ€\nÎ¸\nold\n(\na\ni\nâˆ£\no\ni\n)\n)\n-\\log\\pi_{\\theta_{\\text{old}}}(a_{i}\\mid o_{i}))\n12:â€‚â€…â€‚â€…â€‚â€…â€‚â€…Compute\nL\ni\nâ†\nmin\n(\nr\ni\n(\nÎ¸\n)\nâ‹…\nr\ni\n,\nL_{i}\\leftarrow\\min(r_{i}(\\theta)\\cdot r_{i},\\,\nclip\n(\nr\ni\n(\nÎ¸\n)\n,\n1\nâˆ’\nÎµ\n,\n1\n+\nÎµ\n)\nâ‹…\nr\ni\n)\n\\text{clip}(r_{i}(\\theta),1-\\varepsilon,1+\\varepsilon)\\cdot r_{i})\nâŠ³\n\\triangleright\nEq.\n4\n13:â€‚â€…â€‚â€…â€‚â€…â€‚â€…Update policy parameters\nÎ¸\nâ†\nÎ¸\n+\nÎ·\nâ€‹\nâˆ‡\nÎ¸\nâ€‹\nâˆ‘\ni\nL\ni\n\\theta\\leftarrow\\theta+\\eta\\nabla_{\\theta}\\sum_{i}L_{i}\n14:â€‚â€…â€‚â€…Clear buffer\nâ„¬\n\\mathcal{B}\n15:\nend if\n16:\nend for\n[4pt]\nAlgorithmÂ 1\nTT-VLA Pipeline\nAppendix S7\nDiscussions on Using Test-Time GRPO in VLAs\nIn TT-VLA, we do not adopt Group Relative Policy Optimization (GRPO)\nShao\net al.\n(\n2024\n)\ndue to\ntwo practical constraints\nin test-time robotic deployment:\n1.\nGRPO relies on sampling multiple candidate trajectories or actions to update the policy, which introduces significant computational overhead and makes it inefficient for real-time test-time adaptation. Such sampling-based procedures are particularly unsuitable under test-time settings, where latency and responsiveness are critical.\n2.\nIn real-world robotic scenarios, sampled actions inevitably interact with the physical environment (\ne.g\n.\n, touching or moving objects). It is thus infeasible to reset the environment to a previous state after each interaction. These constraints make GRPO-style sampling-based optimization impractical for test-time adaptation in physical environments. In fact, that is the practical reason that we redefine the advantage to depend only on the reward obtained from the current action (see Â§\n9\n), as we want to prioritize rapid fitting of the current task rather than state accumulations.\nAppendix S8\nReproducibility\nTT-VLA is implemented in Pytorch\nPaszke\net al.\n(\n2019\n)\n. Experiments are conducted on NVIDIA RTX 6000 Ada GPUs.\nTo guarantee reproducibility, our full implementation shall be publicly released upon paper acceptance.\nWe provide the pseudo code of TT-VLA in Algorithm\n1\n.\nAppendix S9\nTechnical Contributions\nOur study presents three principal technical contributions:\nâ€¢\nWe introduce a test-time reinforcement learning framework for VLA models, enabling robots to adapt their policies on the fly during deployment without requiring retraining or environment resets. This capability directly addresses a key limitation of current VLA systems in real-world robotic settings, where conditions are dynamic and unpredictable.\nâ€¢\nTo cope with the severe data scarcity and latency constraints at inference time, we propose a dense, progress-based reward that provides stable and task-aligned learning signals at every step, allowing robots to refine their behavior during execution.\nâ€¢\nExtensive experiments in both simulated and real-world robotic environments demonstrate that our approach consistently improves the robustness and success rates of existing SFT- and RL-based VLA models, highlighting its practical value for real-world robotic deployment.\nAppendix S10\nAsset License and Consent\nAll models and datasets used in this work are publicly available.\nWe strictly comply with their original licenses and use them only for non-commercial academic research.\nThe contents of datasets do not represent our views or opinions.\nModels.\nWe utilize four open-source models:\nNora (MIT license),\nOpenVLA (MIT license),\nOpenVLA-RL (MIT license),\nTraceVLA (MIT license).\nAll licenses permit academic research use; detailed terms are available via the original model repositories.\nDatasets.\nAll simulation experiments were conducted in ManiSkill 3. The evaluated tasks are adopted from\nLiu\net al.\n(\n2025a\n)\n, and detailed task descriptions are provided in Â§\nS3\n.\nThe data (16400 demonstration trajectories) used to warm up the base models is collected following the same\nprocedure as in\nLiu\net al.\n(\n2025a\n)\n, and is generated automatically.\nConsent.\nOur study does not involve crowdsourcing or human subjects. All results are derived from publicly available models and datasets.\nAppendix S11\nEthics Concerns\nTest-time policy adaptation may increase the risk of unintended or unsafe behaviors, particularly in real-world robotic environments where erroneous actions can result in physical damage, equipment failure, or harm to surrounding objects and people. Because policy updates are performed online and are driven by interaction-derived feedback rather than explicit human supervision, unexpected environmental dynamics or imperfect reward signals may lead to behaviors that deviate from intended task objectives. To mitigate these risks, responsible deployment should incorporate safeguards such as constrained action spaces, explicit safety and termination constraints, and conservative update mechanisms. In addition, human oversight and monitoring remain essential, especially during deployment in safety-critical or unstructured environments, to ensure that adaptive behaviors remain aligned with task goals and safety requirements.\nAppendix S12\nFuture Direction\nAs discussed in Â§\nS1.1\n, owing to the architectural distinctions between discretization-based and diffusion-based approaches, our study primarily focuses on the former. Future work should naturally extend our method to diffusion-based formulations, as TT-VLA provides a generalizable solution.\nAnother promising direction is to utilize test-time adaptation (TTA) methods for effectively augmenting multimodal information.\nIt should be noted that these discussions on future direction present engineering\nopportunities rather than insurmountable barriers.\nAppendix S13\nAI Disclosure\nWe acknowledge the use of GPT-5 for grammar checking only. The model was employed to correct grammatical errors while ensuring the original meaning and intent of the text remained unchanged.",
    "preview_text": "Vision-Language-Action models have recently emerged as a powerful paradigm for general-purpose robot learning, enabling agents to map visual observations and natural-language instructions into executable robotic actions. Though popular, they are primarily trained via supervised fine-tuning or training-time reinforcement learning, requiring explicit fine-tuning phases, human interventions, or controlled data collection. Consequently, existing methods remain unsuitable for challenging simulated- or physical-world deployments, where robots must respond autonomously and flexibly to evolving environments. To address this limitation, we introduce a Test-Time Reinforcement Learning for VLAs (TT-VLA), a framework that enables on-the-fly policy adaptation during inference. TT-VLA formulates a dense reward mechanism that leverages step-by-step task-progress signals to refine action policies during test time while preserving the SFT/RL-trained priors, making it an effective supplement to current VLA models. Empirical results show that our approach enhances overall adaptability, stability, and task success in dynamic, previously unseen scenarios under simulated and real-world settings. We believe TT-VLA offers a principled step toward self-improving, deployment-ready VLAs.\n\nOn-the-Fly VLA Adaptation via Test-Time Reinforcement Learning\nChangyu Liu\n1\n,\nYiyang Liu\n1\n1\n1\nfootnotemark:\n1\n,\nTaowen Wang\n2\n,\nQiao Zhuang\n1\n,\nJames Chenhao Liang\n3\n,\nWenhao Yang\n4\n,\nRenjing Xu\n2\n,\nQifan Wang\n5\n,\nDongfang Liu\n6\n,\nCheng Han\n1\n1\nUniversity of Missouriâ€“Kansas City,\n2\nHong Kong University of Science and Technology (Guangzhou),\n3\nU. S. Naval Research Laboratory,\n4\nLamar University,\n5\nMeta AI,\n6\nRochester Institute of Technology\nEqual contribution\nCorresponding author\nAbstract\nVision-Language-Action (VLA) models have recently emerged as a powerful paradigm for general-purpose robot learning, enabling agents to map visual observations and natural-language instructions into executable robotic act",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "VLA",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ˆTT-VLAï¼‰ï¼Œç”¨äºåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œä»¥å¢å¼ºæœºå™¨äººåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œä»»åŠ¡æˆåŠŸç‡ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šåŸºäºæµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ çš„å³æ—¶è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹è‡ªé€‚åº”\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å·²æˆä¸ºé€šç”¨æœºå™¨äººå­¦ä¹ çš„æœ‰åŠ›èŒƒå¼ï¼Œä½†å…¶é€šå¸¸é€šè¿‡ç›‘ç£å¾®è°ƒæˆ–è®­ç»ƒæ—¶å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´ç­–ç•¥å›ºå®šï¼Œéš¾ä»¥åœ¨åŠ¨æ€ã€æœªçŸ¥çš„éƒ¨ç½²ç¯å¢ƒä¸­è‡ªä¸»é€‚åº”ã€‚ç°æœ‰æµ‹è¯•æ—¶è®­ç»ƒæ–¹æ³•ä¸»è¦é’ˆå¯¹å•æ¨¡æ€ä»»åŠ¡è®¾è®¡ï¼Œæ— æ³•ç›´æ¥åº”å¯¹VLAæ¨¡å‹é¢ä¸´çš„å¤šæ¨¡æ€åˆ†å¸ƒåç§»å’Œå®æ—¶æ€§æŒ‘æˆ˜ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§èƒ½å¤Ÿåœ¨æ¨ç†é˜¶æ®µå³æ—¶ã€é«˜æ•ˆåœ°è°ƒæ•´VLAç­–ç•¥çš„æ–¹æ³•ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡ºäº†**TT-VLA**æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š\n- **å¯†é›†çš„åŸºäºä»»åŠ¡è¿›åº¦çš„å¥–åŠ±æœºåˆ¶**ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„ä»»åŠ¡è¿›åº¦ä¼°è®¡å™¨ï¼Œæ ¹æ®è§‚å¯Ÿå†å²è®¡ç®—æ¯ä¸€æ­¥çš„è¿›åº¦å˜åŒ–ä½œä¸ºå³æ—¶ã€å¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œä¸ºç­–ç•¥æ›´æ–°æä¾›è¿ç»­åé¦ˆã€‚\n- **æ— ä»·å€¼å‡½æ•°çš„PPOä¼˜åŒ–**ï¼šé’ˆå¯¹æµ‹è¯•æ—¶æ•°æ®æœ‰é™ã€å»¶è¿Ÿè¦æ±‚é«˜çš„çº¦æŸï¼Œç§»é™¤äº†ä¼ ç»ŸPPOä¸­çš„ä»·å€¼å‡½æ•°å­¦ä¹ å’Œé•¿æœŸå›æŠ¥ä¼°è®¡ï¼Œå°†ä¼˜åŠ¿å‡½æ•°ç®€åŒ–ä¸ºå³æ—¶å¥–åŠ±ï¼Œå®ç°äº†å¿«é€Ÿã€ç¨³å®šçš„ç­–ç•¥åœ¨çº¿å¾®è°ƒã€‚\n- **å³æ—¶ç­–ç•¥é€‚åº”**ï¼šåœ¨å•ä¸ªä»»åŠ¡æ‰§è¡Œå‘¨æœŸå†…ï¼Œæ ¹æ®ç¯å¢ƒåé¦ˆæŒç»­æ›´æ–°ç­–ç•¥å‚æ•°ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€ç¯å¢ƒé‡ç½®æˆ–äººå·¥å¹²é¢„ï¼Œä¿ç•™äº†åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\nåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼š\n- **æ¨¡æ‹Ÿå®éªŒ**ï¼šåœ¨æ¶µç›–**æ‰§è¡Œ**ã€**è§†è§‰**ã€**è¯­ä¹‰**ä¸‰ä¸ªç»´åº¦çš„15ç§æœªè§ä»»åŠ¡ä¸Šï¼ŒTT-VLAä¸€è‡´æå‡äº†å››ç§ä¸»æµå¼€æºVLAæ¨¡å‹çš„æ€§èƒ½ï¼Œç›¸å¯¹å¢ç›Šæœ€é«˜è¾¾44.4%ã€‚\n- **çœŸå®ä¸–ç•Œå®éªŒ**ï¼šåœ¨Frankaæœºæ¢°è‡‚çš„ä¹ä¸ªæœªè§æ‹¾æ”¾ä»»åŠ¡ä¸­ï¼ŒTT-VLAæˆåŠŸæå‡äº†åŸºç¡€æ¨¡å‹çš„æˆåŠŸç‡ï¼Œå¹¶èƒ½å®æ—¶çº æ­£æ‰§è¡Œè¿‡ç¨‹ä¸­çš„è½¨è¿¹åå·®ã€‚\n- **æ¶ˆèç ”ç©¶**ï¼šéªŒè¯äº†æ‰€æå‡ºçš„å³æ—¶å¥–åŠ±å’Œæ— æŠ˜æ‰£ä¼˜åŠ¿è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜ç°æœ‰LLMçš„æµ‹è¯•æ—¶è®­ç»ƒæ–¹æ³•ç›´æ¥åº”ç”¨äºVLAæ—¶æ•ˆæœä¸ä½³ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\nTT-VLAä¸ºVLAæ¨¡å‹æä¾›äº†ä¸€ç§**è½»é‡çº§ã€å³æ’å³ç”¨çš„è‡ªé€‚åº”èƒ½åŠ›**ï¼Œå¡«è¡¥äº†è®­ç»ƒæ—¶ä¼˜åŒ–ä¸åŠ¨æ€éƒ¨ç½²éœ€æ±‚ä¹‹é—´çš„ç©ºç™½ã€‚å…¶æ„ä¹‰åœ¨äºï¼š\n- **æŠ€æœ¯è´¡çŒ®**ï¼šé¦–æ¬¡ç³»ç»Ÿåœ°å°†æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ å¼•å…¥VLAé¢†åŸŸï¼Œè®¾è®¡äº†é€‚ç”¨äºè¯¥åœºæ™¯çš„å¥–åŠ±å’Œä¼˜åŒ–èŒƒå¼ã€‚\n- **å®é™…ä»·å€¼**ï¼šä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨é¢å¯¹ç¯å¢ƒå˜åŒ–ã€åˆ†å¸ƒåç§»æ—¶è¿›è¡Œè‡ªæˆ‘æ”¹è¿›ï¼Œå¢å¼ºäº†VLAæ¨¡å‹åœ¨çœŸå®ã€å¼€æ”¾ä¸–ç•Œä¸­çš„é²æ£’æ€§å’Œå®ç”¨æ€§ï¼Œæ˜¯è¿ˆå‘â€œéƒ¨ç½²å³ç”¨â€å‹é€šç”¨æœºå™¨äººçš„å…³é”®ä¸€æ­¥ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šTT-VLA\n\n### **ä¸€ã€ æ ¸å¿ƒé—®é¢˜**\nå½“å‰ä¸»æµçš„**è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹**åœ¨éƒ¨ç½²æ—¶é¢ä¸´ä¸€ä¸ªæ ¹æœ¬æ€§ç¼ºé™·ï¼š**ç­–ç•¥åƒµåŒ–**ã€‚è¿™äº›æ¨¡å‹é€šå¸¸é€šè¿‡**ç›‘ç£å¾®è°ƒ**æˆ–**è®­ç»ƒæ—¶å¼ºåŒ–å­¦ä¹ **è¿›è¡Œä¼˜åŒ–ï¼Œä¸€æ—¦éƒ¨ç½²ï¼Œå…¶ç­–ç•¥ä¾¿å›ºå®šä¸å˜ã€‚ç„¶è€Œï¼ŒçœŸå®ä¸–ç•Œï¼ˆæ— è®ºæ˜¯ä»¿çœŸè¿˜æ˜¯ç‰©ç†ç¯å¢ƒï¼‰æ˜¯åŠ¨æ€ã€ä¸å¯é¢„æµ‹çš„ï¼Œå­˜åœ¨å¤§é‡**åˆ†å¸ƒåç§»**å’Œ**æœªè§æƒ…å†µ**ã€‚è¿™å¯¼è‡´ç°æœ‰VLAæ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­**é€‚åº”æ€§å·®ã€é²æ£’æ€§ä¸è¶³ã€ä»»åŠ¡æˆåŠŸç‡ä¸‹é™**ã€‚\n\n**ç®€è¨€ä¹‹ï¼Œè®ºæ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯ï¼šå¦‚ä½•è®©å·²éƒ¨ç½²çš„VLAæœºå™¨äººç­–ç•¥èƒ½å¤Ÿåœ¨æµ‹è¯•/æ‰§è¡Œæ—¶ï¼Œæ ¹æ®ç¯å¢ƒåé¦ˆè¿›è¡Œå®æ—¶ã€åœ¨çº¿çš„è‡ªæˆ‘è°ƒæ•´å’Œä¼˜åŒ–ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–äººå·¥å¹²é¢„ã€‚**\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº† **TT-VLA** æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†**æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ** å¼•å…¥VLAé¢†åŸŸï¼Œå®ç°äº† **â€œæ‰§è¡Œä¸­ç­–ç•¥è‡ªé€‚åº”â€** ã€‚å…·ä½“åˆ›æ–°ç‚¹å¦‚ä¸‹ï¼š\n\n1.  **èŒƒå¼åˆ›æ–°ï¼šé¦–æ¬¡ç³»ç»Ÿæ€§åœ°ä¸ºVLAæ¨¡å‹è®¾è®¡æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚**\n    *   æ‰“ç ´äº†ä¼ ç»Ÿâ€œè®­ç»ƒ-éƒ¨ç½²â€åˆ†ç¦»çš„èŒƒå¼ï¼Œå…è®¸æ¨¡å‹åœ¨**å•æ¬¡ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­**æŒç»­ä¼˜åŒ–ç­–ç•¥ã€‚\n    *   ä¸åŒäºè®­ç»ƒé˜¶æ®µçš„RLï¼ŒTT-VLAä¸“ä¸º**æ•°æ®æåº¦ç¨€ç¼ºï¼ˆå•æ¬¡episodeï¼‰ã€å»¶è¿Ÿè¦æ±‚ä¸¥æ ¼**çš„æµ‹è¯•æ—¶åœºæ™¯è®¾è®¡ã€‚\n\n2.  **æ–¹æ³•åˆ›æ–°ï¼šè®¾è®¡äº†é€‚ç”¨äºæµ‹è¯•æ—¶åœºæ™¯çš„å¯†é›†å¥–åŠ±ä¸æ— ä»·å€¼å‡½æ•°PPOä¼˜åŒ–ã€‚**\n    *   **å¯†é›†è¿›åº¦å¥–åŠ±**ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„**Vision-Language-Action-Critic** æ¨¡å‹ä½œä¸ºè¿›åº¦ä¼°è®¡å™¨ `Î¦`ï¼Œè®¡ç®—æ¯ä¸€æ­¥çš„ä»»åŠ¡è¿›åº¦ `p_t`ã€‚å¥–åŠ±å®šä¹‰ä¸ºè¿›åº¦å·®ï¼š`r_t = p_t - p_{t-1}`ã€‚\n        *   **ä¼˜ç‚¹**ï¼šæä¾›æ¯ä¸€æ­¥çš„å³æ—¶åé¦ˆï¼ˆéç¨€ç–ï¼‰ï¼Œé¼“åŠ±å•è°ƒè¿›å±•ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚\n    *   **æ— ä»·å€¼å‡½æ•°PPO**ï¼šç”±äºæµ‹è¯•æ—¶æ ·æœ¬æœ‰é™ï¼Œå­¦ä¹ å‡†ç¡®çš„ä»·å€¼å‡½æ•°ä¸å¯è¡Œã€‚å› æ­¤ï¼ŒTT-VLAå¯¹æ ‡å‡†PPOè¿›è¡Œäº†å…³é”®ç®€åŒ–ï¼š\n        *   ç§»é™¤ä»·å€¼å‡½æ•°æŸå¤±å’Œç†µæ­£åˆ™é¡¹ï¼ˆè®¾ `c1=0, c2=0`ï¼‰ã€‚\n        *   å°†å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ç®€åŒ–ä¸º**å³æ—¶å¥–åŠ±**ï¼ˆè®¾ `Î³=0, Î»=0`ï¼‰ï¼Œå³ `Ã‚_t = r_t`ã€‚\n        *   è¿™ä½¿å¾—ç­–ç•¥æ›´æ–°**ç›´æ¥ã€å¿«é€Ÿ**åœ°åæ˜ å½“å‰åŠ¨ä½œå¯¹ä»»åŠ¡è¿›åº¦çš„è´¡çŒ®ï¼Œé€‚åˆåœ¨çº¿å¿«é€Ÿé€‚åº”ã€‚\n\n3.  **ç†è®ºè´¡çŒ®ï¼šä¸ºä¸Šè¿°è®¾è®¡æä¾›äº†ç†è®ºä¾æ®ã€‚**\n    *   é€šè¿‡**å‘½é¢˜1**å’Œ**æ¨è®º1**è¯æ˜äº†ï¼Œå½“å¥–åŠ±æ˜¯è¿›åº¦å·®æ—¶ï¼Œè‹¥ä½¿ç”¨æŠ˜æ‰£çš„æœªæ¥å›æŠ¥ï¼ˆ`Î³>0`ï¼‰å’Œæ ‡å‡†ä»·å€¼å‡½æ•°ï¼Œä¼šå¯¼è‡´**ä¼˜åŠ¿ä¼°è®¡ä¿¡å·æ¶ˆå¤±æˆ–äº§ç”Ÿè´Ÿåå·®**ï¼Œä»è€Œè§£é‡Šäº†ä¸ºä½•å¿…é¡»é‡‡ç”¨æ— æŠ˜æ‰£çš„å³æ—¶å¥–åŠ±æ–¹æ¡ˆã€‚\n    *   **å¼•ç†1** åˆ™å½¢å¼åŒ–åœ°è¯æ˜äº†å½“ `Î³=0` ä¸” `Î»=0` æ—¶ï¼ŒGAEé€€åŒ–ä¸ºå³æ—¶å¥–åŠ±ï¼Œä¸ºâ€œæ— ä»·å€¼å‡½æ•°PPOâ€æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆæ¦‚è¿°**\nTT-VLAçš„è§£å†³æ–¹æ¡ˆæ˜¯ä¸€ä¸ªå®Œæ•´çš„åœ¨çº¿é€‚åº”ç®¡é“ï¼š\n\n1.  **åˆå§‹åŒ–**ï¼šåŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„VLAåŸºç¡€ç­–ç•¥ï¼ˆå¦‚OpenVLAã€Noraç­‰ï¼‰ã€‚\n2.  **äº¤äº’ä¸è¯„ä¼°**ï¼šåœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥ `t`ï¼š\n    *   ç­–ç•¥æ ¹æ®å½“å‰è§‚å¯Ÿ `o_t` å’ŒæŒ‡ä»¤ `l` ç”ŸæˆåŠ¨ä½œ `a_t` å¹¶æ‰§è¡Œã€‚\n    *   è¿›åº¦ä¼°è®¡å™¨ `Î¦` æ ¹æ®å†å²è§‚å¯Ÿå’ŒæŒ‡ä»¤ï¼Œè¯„ä¼°å½“å‰ä»»åŠ¡è¿›åº¦ `p_t`ã€‚\n    *   è®¡ç®—å¯†é›†å¥–åŠ± `r_t = p_t - p_{t-1}`ã€‚\n3.  **åœ¨çº¿ä¼˜åŒ–**ï¼šå®šæœŸï¼ˆå¦‚æ¯8æ­¥ï¼‰åˆ©ç”¨æ”¶é›†åˆ°çš„ `(çŠ¶æ€, åŠ¨ä½œ, å¥–åŠ±)` æ•°æ®ï¼Œé€šè¿‡ä¸Šè¿°**æ— ä»·å€¼å‡½æ•°PPOç›®æ ‡**å¯¹ç­–ç•¥å‚æ•°è¿›è¡Œæ¢¯åº¦æ›´æ–°ã€‚\n4.  **æŒç»­é€‚åº”**ï¼šæ›´æ–°åçš„ç­–ç•¥ç«‹å³ç”¨äºåç»­å†³ç­–ï¼Œä»è€Œåœ¨**å•ä¸ªä»»åŠ¡æ‰§è¡Œå‘¨æœŸå†…**å®ç°ç­–ç•¥çš„æŒç»­å¾®è°ƒã€‚\n\n### **å››ã€ å®é™…ä»·å€¼ä¸æ•ˆæœ**\n*   **é€šç”¨æ€§å¼º**ï¼šä½œä¸ºä¸€ä¸ªå³æ’å³ç”¨çš„æ¡†æ¶ï¼ŒTT-VLAæˆåŠŸæå‡äº†**å¤šç§**ä¸»æµå¼€æºVLAæ¨¡å‹ï¼ˆNora, OpenVLA, OpenVLA-RL, TraceVLAï¼‰åœ¨**æœªè§ä»»åŠ¡**ä¸Šçš„æ€§èƒ½ã€‚\n*   **å…¨é¢æ”¹è¿›**ï¼šåœ¨ä»¿çœŸå®éªŒä¸­ï¼Œåœ¨**æ‰§è¡Œ**ï¼ˆåŠ¨æ€å¹²æ‰°ï¼‰ã€**è§†è§‰**ï¼ˆçº¹ç†ã€å™ªå£°å˜åŒ–ï¼‰ã€**è¯­ä¹‰**ï¼ˆæ–°ç‰©ä½“ã€æ–°æŒ‡ä»¤ï¼‰ä¸‰ä¸ªç»´åº¦çš„åˆ†å¸ƒåç§»ä¸‹ï¼Œæ¨¡å‹æˆåŠŸç‡å‡è·å¾—**ä¸€è‡´æå‡**ï¼ˆç›¸å¯¹æå‡æœ€é«˜è¾¾44.4%ï¼‰ã€‚\n*   **ç°å®å¯è¡Œæ€§**ï¼šåœ¨çœŸå®æœºå™¨äººï¼ˆFranka Research 3ï¼‰ä¸Šçš„å®éªŒéªŒè¯äº†å…¶åœ¨å®é™…ç‰©ç†ä¸–ç•Œä¸­çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿåœ¨çº¿çº æ­£æ‰§è¡Œè½¨è¿¹åå·®ï¼ŒæˆåŠŸå®Œæˆä»»åŠ¡ã€‚\n*   **æ•ˆç‡é«˜**ï¼šç›¸æ¯”éœ€è¦é‡é‡‡æ ·æˆ–é‡æ¼”ç®—çš„æµ‹è¯•æ—¶æ–¹æ³•ï¼ˆå¦‚GRPOï¼‰ï¼ŒTT-VLAçš„è®¡ç®—å¼€é”€å°ï¼Œæ»¡è¶³å®æ—¶æœºå™¨äººéƒ¨ç½²çš„å»¶è¿Ÿè¦æ±‚ã€‚\n\n**æ€»ç»“**ï¼šTT-VLAçš„æ ¸å¿ƒåˆ›æ–°æ˜¯æå‡ºäº†ä¸€å¥—**ç†è®ºæ‰å®ã€è®¾è®¡ç²¾å·§ã€å®è·µæœ‰æ•ˆ**çš„æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ æœºåˆ¶ï¼Œä½¿VLAæœºå™¨äººä»â€œé™æ€æ‰§è¡Œè€…â€è½¬å˜ä¸ºâ€œåŠ¨æ€è‡ªé€‚è€…â€ï¼Œä¸ºè§£å†³æœºå™¨äººåœ¨åœ°åŒ–ã€ç»ˆèº«å­¦ä¹ ç­‰å…³é”®æŒ‘æˆ˜è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
            "timestamp": "2026-01-21T12:10:21.730327",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡ã€ŠOn-the-Fly VLA Adaptation via Test-Time Reinforcement Learningã€‹åŠç›¸å…³é¢†åŸŸå‘å±•è„‰ç»œåˆ†æ\n\næœ¬æ–‡æå‡ºçš„ **TT-VLA** æ˜¯**è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹**åœ¨**æµ‹è¯•æ—¶è‡ªé€‚åº”**æ–¹å‘ä¸Šçš„ä¸€ä¸ªé‡è¦è¿›å±•ã€‚è¦ç†è§£å…¶è´¡çŒ®ï¼Œéœ€è¦æ¢³ç†ä»**ç›‘ç£å¾®è°ƒ**åˆ°**è®­ç»ƒæ—¶å¼ºåŒ–å­¦ä¹ **ï¼Œå†åˆ°**æµ‹è¯•æ—¶è‡ªé€‚åº”**çš„æ•´ä¸ªå‘å±•è„‰ç»œã€‚å…¶æ ¸å¿ƒæ¼”è¿›é€»è¾‘æ˜¯ï¼š**è®©VLAæ¨¡å‹ä»é™æ€çš„ã€è®­ç»ƒåå›ºå®šçš„ç­–ç•¥ï¼Œé€æ­¥å‘å±•ä¸ºèƒ½åœ¨éƒ¨ç½²ä¸­åŠ¨æ€ã€è‡ªä¸»é€‚åº”ç¯å¢ƒå˜åŒ–çš„æ™ºèƒ½ä½“**ã€‚\n\nä»¥ä¸‹æ˜¯åŸºäºè¯¥è®ºæ–‡åŠå…¶å¼•ç”¨çš„ç›¸å…³å·¥ä½œçš„æ¢³ç†ï¼Œç€é‡äºå‡ ä¸ªå…³é”®å‘å±•æ–¹å‘ï¼š\n\n### å‘å±•è„‰ç»œæ€»è§ˆ\n1.  **åŸºç¡€èŒƒå¼å»ºç«‹ï¼šç›‘ç£å¾®è°ƒ**\n2.  **æ€§èƒ½æå‡ä¸äº¤äº’å­¦ä¹ ï¼šè®­ç»ƒæ—¶å¼ºåŒ–å­¦ä¹ **\n3.  **åº”å¯¹åŠ¨æ€ç¯å¢ƒçš„æ ¸å¿ƒæŒ‘æˆ˜ï¼šæµ‹è¯•æ—¶è‡ªé€‚åº”**\n    *   **æ–¹å‘Aï¼šå»¶ç»­è‡ªç›‘ç£çš„æµ‹è¯•æ—¶è®­ç»ƒ**\n    *   **æ–¹å‘Bï¼šå¼•å…¥å¼ºåŒ–å­¦ä¹ çš„æµ‹è¯•æ—¶è®­ç»ƒ (æœ¬æ–‡TT-VLAæ‰€å±æ–¹å‘)**\n    *   **æ–¹å‘Cï¼šåŸºäºç¾¤ä½“é‡‡æ ·çš„æµ‹è¯•æ—¶ä¼˜åŒ–**\n\n---\n\n### 1. åŸºç¡€èŒƒå¼å»ºç«‹ï¼šç›‘ç£å¾®è°ƒ\n*   **ä»£è¡¨å·¥ä½œ**ï¼šRT-1, RT-2, OpenVLA, Nora ç­‰ã€‚\n*   **æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨å¤§è§„æ¨¡ã€ç²¾å¿ƒç­–åˆ’çš„`(è§†è§‰è§‚å¯Ÿï¼Œè¯­è¨€æŒ‡ä»¤ï¼ŒåŠ¨ä½œ)`ä¸‰å…ƒç»„æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œè®©æ¨¡å‹å­¦ä¹ ä»è§‚å¯Ÿåˆ°åŠ¨ä½œçš„æ˜ å°„ã€‚\n*   **æ”¹è¿›ä¸ä»·å€¼**ï¼šè¯æ˜äº†å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶ä¸­çš„å¯è¡Œæ€§ï¼Œå®ç°äº†**å¼€ç®±å³ç”¨çš„æ³›åŒ–èƒ½åŠ›**ï¼Œèƒ½å¤„ç†å¤§é‡æœªè§è¿‡çš„ç‰©ä½“ã€åœºæ™¯å’Œç®€å•æŒ‡ä»¤ã€‚\n*   **å±€é™æ€§**ï¼š\n    *   **ç­–ç•¥åƒµåŒ–**ï¼šæ¨¡å‹ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œç­–ç•¥å³å›ºå®šã€‚\n    *   **åˆ†å¸ƒåç§»è„†å¼±**ï¼šåœ¨è®­ç»ƒæ•°æ®åˆ†å¸ƒä¹‹å¤–çš„åŠ¨æ€å˜åŒ–ï¼ˆå¦‚ç‰©ä½“æ„å¤–ç§»åŠ¨ã€èƒŒæ™¯å‰§å˜ã€æŒ‡ä»¤æ¨¡ç³Šï¼‰ä¸‹ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚\n    *   **ç¼ºä¹åœ¨çº¿å­¦ä¹ **ï¼šæ— æ³•ä»ä¸ç¯å¢ƒçš„å®æ—¶äº¤äº’ä¸­å­¦ä¹ å’Œçº æ­£é”™è¯¯ã€‚\n\n### 2. æ€§èƒ½æå‡ä¸äº¤äº’å­¦ä¹ ï¼šè®­ç»ƒæ—¶å¼ºåŒ–å­¦ä¹ \n*   **ä»£è¡¨å·¥ä½œ**ï¼šOpenVLA-RL, SimpleVLA-RL, TGRPO, RFTF, VLA-RL ç­‰ã€‚\n*   **æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨SFTæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œåœ¨ï¼ˆä»¿çœŸçš„ï¼‰è®­ç»ƒç¯å¢ƒä¸­å¼•å…¥å¼ºåŒ–å­¦ä¹ ã€‚åˆ©ç”¨ç¯å¢ƒåé¦ˆï¼ˆå¥–åŠ±ä¿¡å·ï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œè¶…è¶Šé™æ€æ¼”ç¤ºæ•°æ®çš„é™åˆ¶ã€‚\n*   **ç›¸æ¯”äºSFTçš„æ”¹è¿›**ï¼š\n    *   **ä¼˜åŒ–ç›®æ ‡æ›´ç›´æ¥**ï¼šä»æ¨¡ä»¿æ•°æ®å˜ä¸ºæœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼Œæ›´å¯¹é½ä»»åŠ¡æœ€ç»ˆç›®æ ‡ã€‚\n    *   **æ ·æœ¬æ•ˆç‡ä¸æ³›åŒ–æå‡**ï¼šé€šè¿‡äº¤äº’æ¢ç´¢ï¼Œèƒ½å‘ç°æ›´ä¼˜ç­–ç•¥ï¼Œå¯¹æœªè§è¿‡çš„ç¯å¢ƒå˜åŒ–è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼ˆè®ºæ–‡ä¸­OpenVLA-RLæ€§èƒ½æ˜¾è‘—é«˜äºOpenVLAï¼‰ã€‚\n    *   **èƒ½åˆ©ç”¨ç¨ å¯†å¥–åŠ±**ï¼šéƒ¨åˆ†å·¥ä½œè®¾è®¡äº†ä¸­é—´å¥–åŠ±ï¼Œæä¾›æ›´ä¸°å¯Œçš„å­¦ä¹ ä¿¡å·ã€‚\n*   **å±€é™æ€§**ï¼š\n    *   **è®­ç»ƒ-éƒ¨ç½²å‰²è£‚**ï¼šæ‰€æœ‰å­¦ä¹ è¿‡ç¨‹ä»åœ¨**è®­ç»ƒé˜¶æ®µ**å®Œæˆã€‚éƒ¨ç½²åï¼Œæ¨¡å‹ä¾ç„¶æ˜¯**é™æ€çš„**ã€‚\n    *   **æ— æ³•åº”å¯¹æµ‹è¯•æ—¶æ„å¤–**ï¼šå½“éƒ¨ç½²ç¯å¢ƒå‡ºç°è®­ç»ƒæ—¶æœªé¢„æ–™åˆ°çš„åˆ†å¸ƒåç§»æˆ–åŠ¨æ€å¹²æ‰°æ—¶ï¼Œæ¨¡å‹æ— æ³•è°ƒæ•´ã€‚\n\n### 3. åº”å¯¹åŠ¨æ€ç¯å¢ƒçš„æ ¸å¿ƒæŒ‘æˆ˜ï¼šæµ‹è¯•æ—¶è‡ªé€‚åº”\nè¿™æ˜¯æœ¬æ–‡èšç„¦çš„å‰æ²¿æ–¹å‘ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°â€œéƒ¨ç½²é™æ€åŒ–â€çš„æ ¹æœ¬ç¼ºé™·ã€‚ä¸»è¦åˆ†åŒ–ä¸ºå‡ ä¸ªå­æ–¹å‘ï¼š\n\n#### **æ–¹å‘Aï¼šå»¶ç»­è‡ªç›‘ç£çš„æµ‹è¯•æ—¶è®­ç»ƒ**\n*   **ä»£è¡¨å·¥ä½œ**ï¼šTLM ç­‰ï¼ˆåŸç”¨äºLLM/VLMï¼‰ã€‚\n*   **æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨æµ‹è¯•æ—¶ï¼Œåˆ©ç”¨æ— æ ‡ç­¾çš„æµ‹è¯•æ•°æ®æœ¬èº«ï¼Œé€šè¿‡è‡ªç›‘ç£ç›®æ ‡ï¼ˆå¦‚æœ€å°åŒ–è¾“å…¥å›°æƒ‘åº¦ï¼‰æ¥æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œå®ç°é¢†åŸŸè‡ªé€‚åº”ã€‚\n*   **å°è¯•åº”ç”¨äºVLAçš„ä¸è¶³**ï¼š\n    *   **ä¿¡å·ä¸ä»»åŠ¡è„±èŠ‚**ï¼šæ›´æ–°ç›®æ ‡æ˜¯è¡¨ç¤ºä¸€è‡´æ€§ï¼ˆå¦‚è¯­è¨€å›°æƒ‘åº¦ï¼‰ï¼Œè€Œé**å†³ç­–è´¨é‡**ã€‚åœ¨éœ€è¦æ„ŸçŸ¥ã€ç†è§£ã€å†³ç­–è”åŠ¨çš„VLAä»»åŠ¡ä¸­ï¼Œè¿™ç§ç›®æ ‡æ— æ³•æä¾›æœ‰æ•ˆçš„ä»»åŠ¡å¯¼å‘å­¦ä¹ ä¿¡å·ã€‚\n    *   **è®ºæ–‡éªŒè¯**ï¼šå¦‚è¡¨3æ‰€ç¤ºï¼ŒTLMåœ¨VLAä»»åŠ¡ä¸Šæå‡æœ‰é™ï¼Œç”šè‡³æ— æ•ˆã€‚\n\n#### **æ–¹å‘Bï¼šå¼•å…¥å¼ºåŒ–å­¦ä¹ çš„æµ‹è¯•æ—¶è®­ç»ƒ - æœ¬æ–‡TT-VLAçš„è´¡çŒ®**\n*   **å‰åºå·¥ä½œ**ï¼šEVOLVE-VLAã€‚\n*   **æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨**æµ‹è¯•/éƒ¨ç½²é˜¶æ®µ**ï¼Œå°†ä¸ç¯å¢ƒäº¤äº’äº§ç”Ÿçš„å¥–åŠ±ä¿¡å·ç”¨äºåœ¨çº¿ç­–ç•¥ä¼˜åŒ–ã€‚\n*   **EVOLVE-VLAçš„æ”¹è¿›ä¸å±€é™**ï¼š\n    *   **æ”¹è¿›**ï¼šé¦–æ¬¡å°†æµ‹è¯•æ—¶RLå¼•å…¥VLAï¼Œä½¿ç”¨ä»»åŠ¡è¿›åº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚\n    *   **å±€é™**ï¼šé‡‡ç”¨**GRPO**ä¼˜åŒ–ï¼Œéœ€è¦é‡‡æ ·å¤šä¸ªå€™é€‰è½¨è¿¹ï¼Œè®¡ç®—å¼€é”€å¤§ï¼Œä¸”**åœ¨ç‰©ç†ä¸–ç•Œä¸­æ— æ³•é‡ç½®ç¯å¢ƒä»¥è¿›è¡Œé‡‡æ ·**ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚\n*   **TT-VLAçš„å…³é”®æ”¹è¿›**ï¼š\n    1.  **ä»·å€¼å‡½æ•°å…å­¦ä¹ è®¾è®¡**ï¼šé’ˆå¯¹æµ‹è¯•æ—¶æ•°æ®æå°‘ã€éœ€å¿«é€Ÿæ›´æ–°çš„çº¦æŸï¼Œ**å»æ‰äº†å¯¹ä»·å€¼å‡½æ•°çš„å­¦ä¹ **ï¼Œé¿å…å…¶ä¼°è®¡ä¸å‡†å¸¦æ¥çš„å¹²æ‰°ã€‚\n    2.  **ç¨ å¯†ã€å³æ—¶çš„è¿›åº¦å¥–åŠ±**ï¼š\n        *   ä½¿ç”¨é¢„è®­ç»ƒçš„è¿›åº¦ä¼°è®¡å™¨ï¼Œæ¯ä¸€æ­¥éƒ½è®¡ç®—`r_t = p_t - p_{t-1}`ã€‚\n        *   **å³æ—¶æ€§**ï¼šæä¾›æ¯ä¸€æ­¥çš„åé¦ˆï¼Œæ”¯æŒ**ä¸­æ®µç­–ç•¥æ ¡æ­£**ï¼ˆå¦‚å›¾5æ¡ˆä¾‹ï¼‰ã€‚\n        *   **ç†è®ºä¿éšœ**ï¼šè®ºæ–‡é€šè¿‡å‘½é¢˜1è®ºè¯äº†åœ¨è¿›åº¦å¥–åŠ±ä¸‹ï¼Œä¼ ç»ŸGAEä¼˜åŠ¿ä¼°è®¡ä¼šå¤±æ•ˆæˆ–äº§ç”Ÿåå·®ï¼Œä»è€Œè¯æ˜äº†å…¶ç®€åŒ–è®¾è®¡ï¼ˆè®¾`Î³=0, Î»=0`ï¼Œä½¿`A_t = r_t`ï¼‰çš„åˆç†æ€§ã€‚\n    3.  **é«˜æ•ˆè½»é‡çš„PPOå˜ä½“**ï¼šåœ¨æ ‡å‡†PPOç›®æ ‡ä¸­ç§»é™¤ä»·å€¼æŸå¤±å’Œç†µæ­£åˆ™é¡¹ï¼Œä»…ä¿ç•™è£å‰ªçš„ç­–ç•¥æŸå¤±ï¼Œå¹¶ä½¿ç”¨å³æ—¶å¥–åŠ±ä½œä¸ºä¼˜åŠ¿ä¼°è®¡ï¼Œå®ç°**å¿«é€Ÿã€ç¨³å®šçš„åœ¨çº¿æ›´æ–°**ã€‚\n    4.  **å¹¿æ³›é€‚ç”¨æ€§**ï¼šä½œä¸ºä¸€ä¸ªâ€œæ’ä»¶â€æ¡†æ¶ï¼Œå¯æ— ç¼æå‡å¤šç§SFT/RLåŸºçº¿çš„æ€§èƒ½ï¼ˆå¦‚è¡¨1æ‰€ç¤ºï¼‰ã€‚\n\n#### **æ–¹å‘Cï¼šåŸºäºç¾¤ä½“é‡‡æ ·çš„æµ‹è¯•æ—¶ä¼˜åŒ–**\n*   **ä»£è¡¨å·¥ä½œ**ï¼šTTRLï¼ˆåŸç”¨äºLLMï¼‰ã€‚\n*   **æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨æµ‹è¯•æ—¶ï¼Œå¯¹æ¯ä¸ªè¾“å…¥é‡‡æ ·å¤šä¸ªå€™é€‰è¾“å‡ºï¼Œé€šè¿‡**å¤šæ•°æŠ•ç¥¨**äº§ç”Ÿä¼ªæ ‡ç­¾ï¼Œå°†åŒ¹é…ä¼ªæ ‡ç­¾çš„è¾“å‡ºè§†ä¸ºâ€œæ­£ç¡®â€å¹¶ç»™äºˆå¥–åŠ±ã€‚\n*   **å°è¯•åº”ç”¨äºVLAçš„ä¸è¶³**ï¼š\n    *   **å¥–åŠ±ä¸ä»»åŠ¡ç›®æ ‡è„±é’©**ï¼šå¥–åŠ±åŸºäºâ€œè¾“å‡ºä¸€è‡´æ€§â€ï¼Œè€Œéâ€œä»»åŠ¡è¿›åº¦â€æˆ–â€œæˆåŠŸâ€ã€‚ä¸€ä¸ªå…¨ä½“ä¸€è‡´ä½†é”™è¯¯çš„åŠ¨ä½œä¹Ÿä¼šè·å¾—é«˜å¥–åŠ±ã€‚\n    *   **ä¸é€‚ç”¨äºè¿ç»­æ§åˆ¶**ï¼šæŠ•ç¥¨æœºåˆ¶åœ¨ç¦»æ•£è¯­è¨€ç”Ÿæˆä¸­æœ‰æ•ˆï¼Œä½†åœ¨è¿ç»­æœºå™¨äººåŠ¨ä½œç©ºé—´ä¸­éš¾ä»¥å®šä¹‰å’Œè®¡ç®—â€œä¸€è‡´æ€§â€ã€‚\n    *   **è®ºæ–‡éªŒè¯**ï¼šå¦‚è¡¨3æ‰€ç¤ºï¼Œåœ¨VLAä»»åŠ¡ä¸Šæ•ˆæœä¸ä½³ã€‚\n\n### æ€»ç»“ä¸å¯¹æ¯”\n| å‘å±•é˜¶æ®µ | æ ¸å¿ƒæ€æƒ³ | å­¦ä¹ æ—¶æœº | è‡ªé€‚åº”èƒ½åŠ› | å…³é”®å±€é™ | TT-VLAçš„å®šä½ |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **ç›‘ç£å¾®è°ƒ** | æ¨¡ä»¿ä¸“å®¶æ•°æ® | è®­ç»ƒé˜¶æ®µ | æ—  | åˆ†å¸ƒåç§»ä¸‹è„†å¼± | å…¶å¢å¼ºå¯¹è±¡ |\n| **è®­ç»ƒæ—¶RL** | äº¤äº’æœ€å¤§åŒ–å¥–åŠ± | è®­ç»ƒé˜¶æ®µ | æ—  | éƒ¨ç½²åç­–ç•¥å›ºå®š | å…¶å¢å¼ºå¯¹è±¡ |\n| **æµ‹è¯•æ—¶è‡ªç›‘ç£** | æ— æ ‡ç­¾æµ‹è¯•æ•°æ®è‡ªå­¦ä¹  | æµ‹è¯•é˜¶æ®µ | æœ‰ï¼Œä½†å¼± | ä¿¡å·ä¸å†³ç­–ä»»åŠ¡è„±èŠ‚ | è¯æ˜å…¶ä¸é€‚ç”¨äºVLA |\n| **æµ‹è¯•æ—¶RL (EVOLVE)** | æµ‹è¯•æ—¶äº¤äº’å¥–åŠ±ä¼˜åŒ– | æµ‹è¯•é˜¶æ®µ | æœ‰ | è®¡ç®—å¼€é”€å¤§ï¼Œä¸å®æ—¶ | å‰åºå·¥ä½œï¼ŒTT-VLAè§£å†³äº†å…¶æ•ˆç‡é—®é¢˜ |\n| **æµ‹è¯•æ—¶é‡‡æ ·æŠ•ç¥¨** | æµ‹è¯•æ—¶è¾“å‡ºä¸€è‡´æ€§å¥–åŠ± | æµ‹è¯•é˜¶æ®µ | æœ‰ï¼Œä½†å¼± | å¥–åŠ±ä¸ä»»åŠ¡ç›®æ ‡è„±é’© | è¯æ˜å…¶ä¸é€‚ç”¨äºVLA |\n| **æµ‹è¯•æ—¶RL (TT-VLA)** | **é«˜æ•ˆã€å€¼å‡½æ•°å…ã€å³æ—¶è¿›åº¦å¥–åŠ±**ä¼˜åŒ– | **æµ‹è¯•é˜¶æ®µ** | **æœ‰ï¼Œä¸”å¼º** | éœ€é¢„è®­ç»ƒè¿›åº¦ä¼°è®¡å™¨ | **æœ¬æ–‡è´¡çŒ®ï¼Œå¹³è¡¡äº†æ•ˆç‡ã€å®æ—¶æ€§ä¸æœ‰æ•ˆæ€§** |\n\n**ç»“è®º**ï¼šTT-VLAä»£è¡¨äº†VLAæ¨¡å‹å‘**è‡ªä¸»ã€æŒç»­ã€åœ¨çº¿å­¦ä¹ **çš„æ™ºèƒ½ä½“æ¼”è¿›çš„å…³é”®ä¸€æ­¥ã€‚å®ƒæ²¡æœ‰è¿½æ±‚è®­ç»ƒé˜¶æ®µæ›´å¤æ‚çš„æ¶æ„æˆ–ç®—æ³•ï¼Œè€Œæ˜¯**å¼€åˆ›æ€§åœ°è§£å†³äº†â€œæœ€åä¸€å…¬é‡Œâ€é—®é¢˜â€”â€”åœ¨ä¸¥è‹›çš„æµ‹è¯•æ—¶çº¦æŸä¸‹ï¼Œå¦‚ä½•å®‰å…¨ã€é«˜æ•ˆåœ°åˆ©ç”¨ç¯å¢ƒåé¦ˆè¿›è¡Œç­–ç•¥è‡ªä¼˜åŒ–**ã€‚å…¶ç†è®ºåˆ†æã€è½»é‡è®¾è®¡å’Œå¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œä¸ºæ„å»ºçœŸæ­£èƒ½åº”å¯¹å¼€æ”¾ä¸–ç•ŒåŠ¨æ€å˜åŒ–çš„éƒ¨ç½²å°±ç»ªå‹æœºå™¨äººæä¾›äº† principled çš„è§£å†³æ–¹æ¡ˆã€‚æœªæ¥çš„æ–¹å‘å¯èƒ½åŒ…æ‹¬å°†å…¶æ‰©å±•åˆ°æ‰©æ•£å‹VLAæ¨¡å‹ï¼Œä»¥åŠæ¢ç´¢æ›´é²æ£’ã€æ›´å®‰å…¨çš„æµ‹è¯•æ—¶è‡ªé€‚åº”æœºåˆ¶ã€‚",
            "timestamp": "2026-01-21T12:11:24.977580",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œä½œè€…å›¢é˜Ÿï¼ˆç‰¹åˆ«æ˜¯é€šè®¯ä½œè€… Cheng Han åŠåˆä½œè€…ï¼‰åœ¨**è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹**å’Œ**æµ‹è¯•æ—¶é€‚åº”**é¢†åŸŸæœ‰ä¸€ç³»åˆ—ç›¸å…³çš„å‰æœŸå·¥ä½œã€‚ä»¥ä¸‹è¡¨æ ¼æ¢³ç†äº†å…¶æ ¸å¿ƒå‰ä½œï¼Œå¹¶é‡ç‚¹å¯¹æ¯”äº†æœ¬æ–‡ï¼ˆTT-VLAï¼‰ä¸è¿™äº›å‰ä½œåœ¨**æŠ€æœ¯æ–¹æ³•ã€æ ¸å¿ƒè´¡çŒ®å’Œåº”ç”¨åœºæ™¯**ä¸Šçš„å…·ä½“åŒºåˆ«ä¸æ”¹è¿›ã€‚\n\n| å‰ä½œè®ºæ–‡ / æŠ€æœ¯ | æ ¸å¿ƒæ–¹æ³•ä¸è´¡çŒ® | ä¸æœ¬æ–‡ (TT-VLA) çš„ä¸»è¦åŒºåˆ« | æœ¬æ–‡ (TT-VLA) çš„ä¸»è¦æ”¹å–„ä¸åˆ›æ–° |\n| :--- | :--- | :--- | :--- |\n| **OpenVLA** (Kim et al., 2024) åŠ **OpenVLA-RL** (Liu et al., 2025a) | **OpenVLA**: åŸºäºç›‘ç£å¾®è°ƒçš„å¼€æºVLAæ¨¡å‹ï¼Œåœ¨é™æ€ã€ç»“æ„åŒ–æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ã€‚<br>**OpenVLA-RL**: åœ¨OpenVLAåŸºç¡€ä¸Šï¼Œäº**è®­ç»ƒé˜¶æ®µ**å¼•å…¥å¼ºåŒ–å­¦ä¹ è¿›è¡Œç­–ç•¥å¾®è°ƒï¼Œæå‡äº†åœ¨æœªè§ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ | 1. **é€‚åº”é˜¶æ®µä¸åŒ**ï¼šå‰ä½œèšç„¦äº**è®­ç»ƒé˜¶æ®µ**çš„ä¼˜åŒ–ï¼ˆæ— è®ºæ˜¯SFTè¿˜æ˜¯è®­ç»ƒæ—¶RLï¼‰ã€‚<br>2. **ç­–ç•¥å›ºå®š**ï¼šæ¨¡å‹éƒ¨ç½²åï¼Œç­–ç•¥å‚æ•°å†»ç»“ï¼Œæ— æ³•æ ¹æ®æµ‹è¯•æ—¶ç¯å¢ƒåŠ¨æ€è°ƒæ•´ã€‚<br>3. **å¥–åŠ±ç¨€ç–**ï¼šRLè®­ç»ƒé€šå¸¸ä¾èµ–ç¨€ç–çš„å›åˆç»ˆå±€å¥–åŠ±ã€‚ | 1. **åˆ›æ–°é€‚åº”èŒƒå¼**ï¼šæå‡ºäº†**æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ **æ¡†æ¶ï¼Œé¦–æ¬¡å®ç°VLAæ¨¡å‹åœ¨**å•æ¬¡æ¨ç† episode å†…**çš„åœ¨çº¿ç­–ç•¥è‡ªé€‚åº”ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–ç¯å¢ƒé‡ç½®ã€‚<br>2. **åŠ¨æ€é€‚åº”æ€§**ï¼šè§£å†³äº†å‰ä½œåœ¨åŠ¨æ€ã€æ¼”åŒ–ç¯å¢ƒä¸­ç­–ç•¥è„†å¼±çš„æ ¹æœ¬é—®é¢˜ï¼Œä½¿æœºå™¨äººèƒ½åº”å¯¹éƒ¨ç½²æ—¶çš„åˆ†å¸ƒåç§»ã€‚ |\n| **E-VPT**, **MÂ²PT**, **Re-imagining Multimodal Instruction Tuning** (Han et al., 2023; Wang et al., 2024b; Liu et al., 2025b) | è¿™äº›å·¥ä½œé›†ä¸­äº**è§†è§‰-è¯­è¨€æ¨¡å‹**çš„**é«˜æ•ˆå¾®è°ƒ**ä¸**æç¤ºå­¦ä¹ **ï¼Œæ—¨åœ¨æå‡æ¨¡å‹åœ¨è·¨æ¨¡æ€ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡ã€‚ | 1. **é¢†åŸŸä¸åŒ**ï¼šä¸»è¦é’ˆå¯¹**è§†è§‰-è¯­è¨€ç†è§£**ä»»åŠ¡ï¼Œè€Œé**å…·èº«å†³ç­–**çš„VLAä»»åŠ¡ã€‚<br>2. **æ— åŠ¨ä½œäº¤äº’**ï¼šä¸æ¶‰åŠä¸ç¯å¢ƒçš„é—­ç¯äº¤äº’å’ŒåŠ¨ä½œç­–ç•¥çš„ä¼˜åŒ–ã€‚<br>3. **é€‚åº”ç›®æ ‡é™æ€**ï¼šé€‚åº”ç›®æ ‡å¤šä¸ºæå‡è¡¨å¾ä¸€è‡´æ€§æˆ–ä»»åŠ¡è¯†åˆ«å‡†ç¡®ç‡ï¼Œè€ŒéåŠ¨æ€ç¯å¢ƒä¸‹çš„ç­–ç•¥ç¨³å¥æ€§ã€‚ | 1. **ä»ç†è§£åˆ°è¡ŒåŠ¨**ï¼šå°†æµ‹è¯•æ—¶é€‚åº”çš„æ€æƒ³ä»â€œæ„ŸçŸ¥ä¸ç†è§£â€é¢†åŸŸæ‹“å±•åˆ°â€œæ„ŸçŸ¥-å†³ç­–-è¡ŒåŠ¨â€é—­ç¯çš„**æœºå™¨äººæ§åˆ¶**é¢†åŸŸã€‚<br>2. **å¼•å…¥äº¤äº’åé¦ˆ**ï¼šåˆ›æ–°æ€§åœ°åˆ©ç”¨ç¯å¢ƒäº¤äº’äº§ç”Ÿçš„**å¯†é›†è¿›åº¦å¥–åŠ±**ä½œä¸ºé€‚åº”ä¿¡å·ï¼Œå®ç°äº†**ä»»åŠ¡å¯¼å‘**çš„åœ¨çº¿ç­–ç•¥ä¼˜åŒ–ã€‚ |\n| **Exploring the Adversarial Vulnerabilities of VLAs** (Wang et al., 2025a) | ç ”ç©¶äº†VLAæ¨¡å‹åœ¨å¯¹æŠ—æ€§æ”»å‡»ä¸‹çš„å®‰å…¨æ€§é—®é¢˜ï¼Œæ­ç¤ºäº†å…¶åœ¨å¯¹æŠ—æ ·æœ¬ä¸‹çš„è„†å¼±æ€§ã€‚ | 1. **é—®é¢˜è§†è§’ä¸åŒ**ï¼šå…³æ³¨**å®‰å…¨æ€§**å’Œ**é²æ£’æ€§**ä¸­çš„**å¯¹æŠ—æ€§å¨èƒ**ã€‚<br>2. **è¢«åŠ¨é˜²å¾¡**ï¼šä¸»è¦åˆ†æè„†å¼±æ€§ï¼Œè€Œéæä¾›ä¸€ç§é€šç”¨çš„ã€ä¸»åŠ¨çš„é€‚åº”æ€§å¢å¼ºæœºåˆ¶ã€‚ | 1. **ä¸»åŠ¨é€‚åº”æœºåˆ¶**ï¼šTT-VLAæä¾›äº†ä¸€ç§**ä¸»åŠ¨çš„ã€é€šç”¨çš„**é€‚åº”æ€§å¢å¼ºæ–¹æ¡ˆï¼Œä¸ä»…èƒ½åº”å¯¹å¯¹æŠ—æ€§æ‰°åŠ¨ï¼Œæ›´èƒ½å¤„ç†å¹¿æ³›çš„**è‡ªç„¶åˆ†å¸ƒåç§»**ï¼ˆå¦‚ç‰©ä½“ä½ç½®å˜åŒ–ã€çº¹ç†å™ªå£°ã€è¯­ä¹‰å˜åŒ–ç­‰ï¼‰ã€‚<br>2. **ä»å®‰å…¨åˆ°é€šç”¨ç¨³å¥**ï¼šå°†ç ”ç©¶é‡ç‚¹ä»ç‰¹å®šæ”»å‡»ä¸‹çš„é˜²å¾¡ï¼Œè½¬å‘æ›´å¹¿æ³›çš„ã€é€‚ç”¨äºå¼€æ”¾åŠ¨æ€ç¯å¢ƒçš„**è‡ªæ”¹è¿›èƒ½åŠ›**ã€‚ |\n| **SimpleVLA-RL**, **TGRPO**, **VLA-RL** ç­‰ç›¸å…³è®­ç»ƒæ—¶RLå·¥ä½œ (Li et al., 2025a; Chen et al., 2025c; Lu et al., 2025) | è¿™äº›å·¥ä½œæ˜¯**è®­ç»ƒé˜¶æ®µRLåº”ç”¨äºVLA**çš„ä»£è¡¨ï¼Œé€šè¿‡è®¾è®¡å„ç§å¥–åŠ±å‡½æ•°ï¼ˆç¨€ç–/ç¨ å¯†ï¼‰å’Œä¼˜åŒ–ç®—æ³•ï¼ˆPPO, GRPOå˜ä½“ï¼‰æ¥æå‡ç­–ç•¥æ€§èƒ½ã€‚ | 1. **ä¸¥æ ¼çš„è®­ç»ƒ-éƒ¨ç½²åˆ†ç¦»**ï¼šæ‰€æœ‰ç­–ç•¥ä¼˜åŒ–å‡åœ¨**ç¦»çº¿è®­ç»ƒé˜¶æ®µ**å®Œæˆï¼Œéƒ¨ç½²åç­–ç•¥å›ºå®šã€‚<br>2. **è®¡ç®—å…è®¸å¤æ‚ä¼˜åŒ–**ï¼šè®­ç»ƒæ—¶å¯ä½¿ç”¨GRPOç­‰å¤šè½¨è¿¹é‡‡æ ·ã€ä»·å€¼å‡½æ•°ä¼°è®¡ç­‰å¤æ‚æŠ€æœ¯ï¼Œè®¡ç®—å¼€é”€å¤§ã€‚<br>3. **ä¾èµ–é¢„æ”¶é›†æ•°æ®æˆ–ä»¿çœŸé‡ç½®**ã€‚ | 1. **çªç ´é˜¶æ®µé™åˆ¶**ï¼šå°†RLçš„ä¼˜åŒ–èƒ½åŠ›**æ— ç¼å»¶ä¼¸è‡³éƒ¨ç½²é˜¶æ®µ**ï¼Œå®ç°äº†â€œç»ˆèº«å­¦ä¹ â€çš„é›å½¢ã€‚<br>2. **ä¸ºæµ‹è¯•æ—¶é‡èº«å®šåˆ¶**ï¼š<br>   - **ä»·å€¼å‡½æ•°å…å­¦ä¹ **ï¼šå› æµ‹è¯•æ—¶æ ·æœ¬æå°‘ã€å»¶è¿Ÿè¦æ±‚ä¸¥ï¼Œ**å»é™¤äº†ä»·å€¼å‡½æ•°**ï¼Œç›´æ¥ä½¿ç”¨å³æ—¶å¥–åŠ±ä½œä¸ºä¼˜åŠ¿ä¼°è®¡ã€‚<br>   - **å¥–åŠ±è®¾è®¡é©æ–°**ï¼šæå‡º**åŸºäºä»»åŠ¡è¿›åº¦çš„ç¨ å¯†å¥–åŠ±**ï¼Œæä¾›æ¯ä¸€æ­¥çš„å®šå‘å­¦ä¹ ä¿¡å·ï¼Œå…‹æœäº†ç¨€ç–å¥–åŠ±åœ¨æµ‹è¯•æ—¶æ— æ•ˆçš„é—®é¢˜ã€‚<br>   - **ç®€åŒ–ä¼˜åŒ–**ï¼šé‡‡ç”¨**å•æ­¥ã€æ— æŠ˜æ‰£**çš„ä¼˜åŠ¿ä¼°è®¡ï¼Œé¿å…é•¿æ—¶ç¨‹ä¼°è®¡ä¸å‡†ï¼Œå®ç°å¿«é€Ÿã€ç¨³å®šçš„åœ¨çº¿æ›´æ–°ã€‚<br>3. **è®¡ç®—é«˜æ•ˆ**ï¼šæ‘’å¼ƒäº†GRPOç­‰é‡‡æ ·å¯†é›†å‹æ–¹æ³•ï¼Œé€‚åº”**å®æ—¶æœºå™¨äººéƒ¨ç½²çš„ä¸¥æ ¼å»¶è¿Ÿçº¦æŸ**ã€‚ |\n| **EVOLVE-VLA** (Bai et al., 2025b) | ä¸æœ¬æ–‡æœ€ç›¸å…³çš„å·¥ä½œï¼Œä¹Ÿåˆ©ç”¨ä»»åŠ¡è¿›åº¦è¿›è¡ŒRLä¼˜åŒ–ï¼Œä½†å…¶åœ¨**æµ‹è¯•æ—¶**ä½¿ç”¨**GRPO**è¿›è¡Œç­–ç•¥æ›´æ–°ã€‚ | 1. **ä¼˜åŒ–ç®—æ³•ä¸åŒ**ï¼šä½¿ç”¨**GRPO**ï¼Œéœ€è¦ä¸ºæ¯ä¸ªçŠ¶æ€é‡‡æ ·å¤šä¸ªåŠ¨ä½œå€™é€‰è¿›è¡Œè¯„ä¼°ï¼Œè®¡ç®—å¼€é”€æå¤§ã€‚<br>2. **å®ç”¨æ€§å—é™**ï¼šå·¨å¤§çš„è®¡ç®—æˆæœ¬ä½¿å…¶éš¾ä»¥åº”ç”¨äº**å¯¹å»¶è¿Ÿæ•æ„Ÿçš„å®æ—¶ç‰©ç†æœºå™¨äººç³»ç»Ÿ**ã€‚ | 1. **ç®—æ³•æ•ˆç‡çªç ´**ï¼šæå‡ºäº†**å…ä»·å€¼å‡½æ•°çš„PPOå˜ä½“**ï¼Œæ›´æ–°ä»…éœ€å½“å‰ç­–ç•¥å’Œå³æ—¶å¥–åŠ±ï¼Œè®¡ç®—é‡æå°ï¼Œæ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚<br>2. **ç†è®ºæ”¯æ’‘**ï¼šé€šè¿‡**å‘½é¢˜1**å’Œ**æ¨è®º1**çš„ç†è®ºåˆ†æï¼Œæ­ç¤ºäº†åœ¨è¿›åº¦å·®åˆ†å¥–åŠ±ä¸‹ï¼Œä½¿ç”¨ä¼ ç»ŸGAEä¼šå¯¼è‡´å­¦ä¹ ä¿¡å·æ¶ˆå¤±æˆ–äº§ç”Ÿè´Ÿåå·®ï¼Œä»è€Œ**ä»ç†è®ºä¸Šè®ºè¯äº†å…¶ç®€åŒ–è®¾è®¡ï¼ˆÎ³=0, Î»=0ï¼‰çš„åˆç†æ€§**ã€‚ |\n| **TLM/TTRLç­‰LLMæµ‹è¯•æ—¶æ–¹æ³•** (Hu et al., 2025) | å°†æµ‹è¯•æ—¶è®­ç»ƒï¼ˆTTTï¼‰æˆ–æµ‹è¯•æ—¶RLï¼ˆTTRLï¼‰åº”ç”¨äº**å¤§è¯­è¨€æ¨¡å‹**ï¼Œé€šè¿‡æœ€å°åŒ–å›°æƒ‘åº¦æˆ–å¤šæ•°æŠ•ç¥¨ä¼ªæ ‡ç­¾è¿›è¡Œé€‚åº”ã€‚ | 1. **æ¨¡æ€ä¸ä»»åŠ¡ä¸åŒ¹é…**ï¼šLLMçš„æµ‹è¯•æ—¶æ–¹æ³•ä¾èµ–äºæ–‡æœ¬åºåˆ—çš„ä¼¼ç„¶æˆ–ä¸€è‡´æ€§ï¼Œæ— æ³•å¤„ç†VLAä¸­**è§†è§‰-åŠ¨ä½œ-ç¯å¢ƒåŠ¨æ€**çš„å¤æ‚è€¦åˆã€‚<br>2. **å¥–åŠ±ä¿¡å·æ— æ•ˆ**ï¼šåŸºäºå…±è¯†çš„ä¼ªæ ‡ç­¾å¥–åŠ±æ— æ³•åæ˜ VLAä»»åŠ¡ä¸­**åŠ¨ä½œçš„å®é™…è´¨é‡**å’Œ**ä»»åŠ¡è¿›åº¦**ã€‚ | 1. **è®¾è®¡é¢†åŸŸç‰¹å®šé€‚åº”æœºåˆ¶**ï¼šæŒ‡å‡ºç›´æ¥å°†LLMçš„TTTæ–¹æ³•è¿ç§»åˆ°VLAåŸŸæ˜¯æ— æ•ˆçš„ï¼ˆè§Â§4.5å®éªŒï¼‰ã€‚<br>2. **å¼•å…¥ç¯å¢ƒåé¦ˆä¿¡å·**ï¼šç”¨**ä¸ç¯å¢ƒçŠ¶æ€ç›´æ¥ç›¸å…³çš„ã€å¯é‡åŒ–çš„ä»»åŠ¡è¿›åº¦**ä½œä¸ºå¥–åŠ±æ ¹æºï¼Œç¡®ä¿äº†å­¦ä¹ ä¿¡å·ä¸ä»»åŠ¡ç›®æ ‡**å¼ºå¯¹é½**ï¼Œè¿™æ˜¯VLAæµ‹è¯•æ—¶é€‚åº”æˆåŠŸçš„å…³é”®ã€‚ |\n\n### **æ€»ç»“ï¼šæœ¬æ–‡ç›¸æ¯”äºå‰ä½œçš„æ ¸å¿ƒæ”¹åŠ¨ä¸æå‡**\n\n1.  **èŒƒå¼è½¬å˜**ï¼šä»â€œ**è®­ç»ƒæ—¶ä¼˜åŒ–ï¼Œéƒ¨ç½²æ—¶å›ºå®š**â€è½¬å˜ä¸ºâ€œ**éƒ¨ç½²æ—¶æŒç»­åœ¨çº¿è‡ªä¼˜åŒ–**â€ï¼Œè§£å†³äº†VLAæ¨¡å‹åº”å¯¹**çœŸå®ä¸–ç•ŒåŠ¨æ€æ€§**çš„æ ¸å¿ƒç“¶é¢ˆã€‚\n2.  **æ–¹æ³•åˆ›æ–°**ï¼š\n    - **å¥–åŠ±æœºåˆ¶**ï¼šè®¾è®¡äº†**åŸºäºä»»åŠ¡è¿›åº¦çš„ç¨ å¯†å¥–åŠ±**ï¼Œä¸ºæµ‹è¯•æ—¶çš„æ¯ä¸€æ­¥æä¾›å³æ—¶ã€å®šå‘çš„åé¦ˆã€‚\n    - **ä¼˜åŒ–ç®—æ³•**ï¼šæå‡ºäº†**å…ä»·å€¼å‡½æ•°çš„æµ‹è¯•æ—¶PPO**ï¼Œå°†ä¼˜åŠ¿ä¼°è®¡ç®€åŒ–ä¸ºå³æ—¶å¥–åŠ±ï¼Œå…¼é¡¾äº†ç¨³å®šæ€§ä¸æè‡´çš„è®¡ç®—æ•ˆç‡ï¼Œé€‚åˆå®æ—¶ç³»ç»Ÿã€‚\n    - **ç†è®ºéªŒè¯**ï¼šæä¾›äº†ç†è®ºåˆ†æï¼Œè¯æ˜ä¼ ç»ŸGAEåœ¨è¿›åº¦å¥–åŠ±ä¸‹çš„å¤±æ•ˆï¼Œä¸ºå…¶ç®€åŒ–è®¾è®¡å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚\n3.  **å®ç”¨ä»·å€¼**ï¼šé€šè¿‡å¤§é‡ä»¿çœŸå’Œ**çœŸå®æœºå™¨äººå®éªŒ**ï¼ŒéªŒè¯äº†æ¡†æ¶çš„æ™®é€‚æ€§ï¼ˆé€‚é…å¤šç§VLAéª¨å¹²æ¨¡å‹ï¼‰å’Œæœ‰æ•ˆæ€§ï¼ˆå…¨é¢æå‡æ‰§è¡Œã€è§†è§‰ã€è¯­ä¹‰ä¸‰ä¸ªç»´åº¦çš„æ³›åŒ–æ€§èƒ½ï¼‰ï¼Œæ ‡å¿—ç€å‘**å¯è‡ªæ”¹è¿›ã€éƒ¨ç½²å°±ç»ªçš„VLA**è¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ã€‚",
            "timestamp": "2026-01-21T12:12:26.967158",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨æ·±å…¥åˆ†æè¿™ç¯‡é¢˜ä¸ºã€ŠOn-the-Fly VLA Adaptation via Test-Time Reinforcement Learningã€‹çš„è®ºæ–‡ï¼Œå¹¶è¯¦ç»†é˜è¿°å…¶æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åˆ†æ\n\næœ¬æ–‡çš„æ ¸å¿ƒè´¡çŒ®æ˜¯æå‡ºäº† **TT-VLA** æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨åŠ¨æ€ã€æœªçŸ¥çš„éƒ¨ç½²ç¯å¢ƒä¸­ç­–ç•¥åƒµåŒ–ã€æ— æ³•åœ¨çº¿é€‚åº”çš„é—®é¢˜ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯**åœ¨æ¨ç†é˜¶æ®µï¼ˆæµ‹è¯•æ—¶ï¼‰**ï¼Œåˆ©ç”¨ç¯å¢ƒäº¤äº’äº§ç”Ÿçš„å¯†é›†å¥–åŠ±ä¿¡å·ï¼Œå¯¹é¢„è®­ç»ƒçš„VLAç­–ç•¥è¿›è¡Œ**åœ¨çº¿ã€å®æ—¶çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒ**ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–äººå·¥å¹²é¢„ã€‚\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†åˆ—å‡ºäº†è®ºæ–‡æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•åŠå…¶ç»†èŠ‚ï¼š\n\n| æŠ€æœ¯æ¨¡å— | æ ¸å¿ƒæ€æƒ³ä¸ç›®æ ‡ | å…·ä½“æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…³é”®å‚æ•°ä¸è®¾è®¡é€‰æ‹© |\n| :--- | :--- | :--- | :--- |\n| **1. å¯†é›†çš„ã€åŸºäºä»»åŠ¡è¿›åº¦çš„å¥–åŠ±æœºåˆ¶** | è§£å†³æµ‹è¯•æ—¶ç¨€ç–å¥–åŠ±ä¿¡å·ï¼ˆä»…ä»»åŠ¡æˆåŠŸ/å¤±è´¥ï¼‰æ— æ³•æä¾›**æ­¥è¿›å¼å­¦ä¹ ä¿¡å·**çš„é—®é¢˜ã€‚ç›®æ ‡æ˜¯æä¾›æ¯ä¸€æ­¥éƒ½èƒ½åæ˜ ä»»åŠ¡å®Œæˆåº¦çš„å³æ—¶åé¦ˆï¼Œä½¿ç­–ç•¥èƒ½åœ¨å•æ¬¡ä»»åŠ¡æ‰§è¡Œä¸­è¿›è¡Œä¸­æœŸä¿®æ­£ã€‚ | **1. è¿›åº¦ä¼°è®¡å™¨ï¼š** <br> ä½¿ç”¨é¢„è®­ç»ƒçš„Vision-Language-Action-Criticæ¨¡å‹ä½œä¸ºè¿›åº¦å‡½æ•° $\\Phi$ã€‚ <br> $p_t = \\Phi(o_{0:t+1}, l)$ <br> å…¶ä¸­ï¼Œ$p_t \\in [0,1]$ è¡¨ç¤ºåœ¨æ—¶é—´æ­¥ $t$ï¼Œç»™å®šè§‚æµ‹å†å² $o_{0:t+1}$ å’ŒæŒ‡ä»¤ $l$ ä¸‹çš„ä»»åŠ¡è¿›åº¦ä¼°è®¡å€¼ã€‚ <br><br> **2. å¯†é›†å¥–åŠ±è®¡ç®—ï¼š** <br> å°†æ¯ä¸€æ­¥çš„å¥–åŠ±å®šä¹‰ä¸º**è¿›åº¦å€¼çš„æ—¶é—´å·®åˆ†**ï¼Œé¼“åŠ±å•è°ƒè¿›å±•ï¼Œæƒ©ç½šå€’é€€è¡Œä¸ºã€‚ <br> $r_t = p_t - p_{t-1}$ <br> è¯¥è®¾è®¡ç¡®ä¿äº†å¥–åŠ±çš„**æ— ç›‘ç£æ€§**ï¼ˆæ— éœ€å¤–éƒ¨æ ‡æ³¨ï¼‰ã€**å¯†é›†æ€§**ï¼ˆæ¯æ­¥éƒ½æœ‰ï¼‰å’Œ**æ–¹å‘æ€§**ï¼ˆå¼•å¯¼ä»»åŠ¡å®Œæˆï¼‰ã€‚ | - **è¿›åº¦ä¼°è®¡å™¨ï¼š** é‡‡ç”¨é¢„è®­ç»ƒçš„VLACæ¨¡å‹ï¼Œåœ¨å®éªŒä¸­ä¿æŒå†»ç»“ã€‚ <br> - **å¥–åŠ±èŒƒå›´ï¼š** $r_t \\in [-1, 1]$ï¼Œå› ä¸º $p_t \\in [0,1]$ã€‚ |\n| **2. æ— ä»·å€¼å‡½æ•°çš„PPOå˜ä½“** | è§£å†³æµ‹è¯•æ—¶**æ ·æœ¬æåº¦æœ‰é™**ï¼ˆå•æ¬¡episodeï¼‰å’Œ**ä¸¥æ ¼å»¶è¿Ÿçº¦æŸ**ä¸‹ï¼Œå­¦ä¹ ä¸€ä¸ªå‡†ç¡®çš„ä»·å€¼å‡½æ•° $V(s)$ ä¸åˆ‡å®é™…çš„é—®é¢˜ã€‚ç›®æ ‡æ˜¯å®ç°é«˜æ•ˆã€ç¨³å®šçš„å•æ­¥ç­–ç•¥æ›´æ–°ã€‚ | **1. ç®€åŒ–PPOç›®æ ‡å‡½æ•°ï¼š** <br> ä»æ ‡å‡†PPOç›®æ ‡å‡ºå‘ï¼Œç§»é™¤ä»·å€¼å‡½æ•°æŸå¤±å’Œç†µæ­£åˆ™é¡¹ï¼Œä¸“æ³¨äºç­–ç•¥çš„ç¨³å®šä¼˜åŒ–ã€‚ <br> $L(\\theta) = \\mathbb{E}_t [L_t^{CLIP}(\\theta)]$ <br> å…¶ä¸­ $L_t^{CLIP}(\\theta) = \\mathbb{E}_t[\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$ï¼Œ $r_t(\\theta) = \\frac{\\pi_\\theta(a_t\\|s_t)}{\\pi_{\\theta_{old}}(a_t\\|s_t)}$ã€‚ <br><br> **2. ä¼˜åŠ¿ä¼°è®¡ç®€åŒ–ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰ï¼š** <br> **ç†è®ºä¾æ®ï¼ˆå‘½é¢˜1ä¸æ¨è®º1ï¼‰ï¼š** è®ºæ–‡è¯æ˜ï¼Œå½“å¥–åŠ± $r_t = p_t - p_{t-1}$ ä¸”ä»·å€¼å‡½æ•° $V(s_t)=1-p_{t-1}$ æ—¶ï¼Œè‹¥æŠ˜æ‰£å› å­ $\\gamma=1$ï¼Œåˆ™æ—¶åºå·®åˆ†è¯¯å·® $\\delta_t=0$ï¼Œå¯¼è‡´ä¼˜åŠ¿ä¿¡å·æ¶ˆå¤±ï¼›è‹¥ $\\gamma<1$ï¼Œåˆ™ä¼šäº§ç”Ÿç³»ç»Ÿæ€§çš„è´Ÿåå·®ã€‚ <br> **è§£å†³æ–¹æ¡ˆï¼š** æ‘’å¼ƒå¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼Œå°†ä¼˜åŠ¿å‡½æ•°ç›´æ¥å®šä¹‰ä¸º**å³æ—¶å¥–åŠ±**ï¼Œä¸“æ³¨äºå½“å‰åŠ¨ä½œå¯¹å³æ—¶è¿›å±•çš„è´¡çŒ®ã€‚ <br> è®¾ç½®GAEå‚æ•° $\\lambda=0$ å’Œ $\\gamma=0$ï¼Œå¾—åˆ°ï¼š <br> $\\hat{A}_t = \\delta_t = r_t$ <br> è¿™å®è´¨ä¸Šæ˜¯**Lemma 1**çš„ä¸€ä¸ªç‰¹ä¾‹ï¼šå½“ $\\gamma=0$ ä¸” $V(s) \\equiv 0$ æ—¶ï¼Œ$A_t = r_t$ã€‚ | - **æŠ˜æ‰£å› å­ $\\gamma$ï¼š** è®¾ç½®ä¸º **0**ã€‚ <br> - **GAEå‚æ•° $\\lambda$ï¼š** è®¾ç½®ä¸º **0**ã€‚ <br> - **è£å‰ªèŒƒå›´ $\\epsilon$ï¼š** é‡‡ç”¨PPOæ ‡å‡†å€¼ **0.2**ã€‚ <br> - **ä»·å€¼å‡½æ•°ç³»æ•° $c_1$ å’Œç†µç³»æ•° $c_2$ï¼š** å‡è®¾ç½®ä¸º **0**ã€‚ |\n| **3. æ•´ä½“åœ¨çº¿é€‚åº”æµç¨‹** | å°†ä¸Šè¿°ç»„ä»¶æ•´åˆä¸ºä¸€ä¸ªå¯åœ¨å•æ¬¡ä»»åŠ¡æ‰§è¡Œä¸­é—­ç¯è¿è¡Œçš„ç³»ç»Ÿï¼Œå®ç°â€œè¾¹æ‰§è¡Œã€è¾¹å­¦ä¹ ã€è¾¹æ”¹è¿›â€ã€‚ | **ç®—æ³•æµç¨‹ï¼ˆå¯¹åº”è®ºæ–‡å›¾2å’Œé™„å½•Algorithm 1ï¼‰ï¼š** <br> 1. **åˆå§‹åŒ–ï¼š** è½½å…¥é¢„è®­ç»ƒVLAç­–ç•¥ $\\pi_\\theta$ å’Œå†»ç»“çš„è¿›åº¦ä¼°è®¡å™¨ $\\Phi$ã€‚ <br> 2. **å¾ªç¯ï¼ˆæ¯ä¸€æ­¥ $t$ï¼‰ï¼š** <br>    a. è§‚æµ‹ $o_t$ï¼Œç­–ç•¥è¾“å‡ºåŠ¨ä½œ $a_t \\sim \\pi_\\theta(\\cdot\\|o_t, l)$ã€‚ <br>    b. æ‰§è¡Œ $a_t$ï¼Œè·å¾—æ–°è§‚æµ‹ $o_{t+1}$ã€‚ <br>    c. è¿›åº¦ä¼°è®¡å™¨è®¡ç®— $p_t = \\Phi(o_{0:t}, l)$ å’Œ $p_{t+1} = \\Phi(o_{0:t+1}, l)$ã€‚ <br>    d. è®¡ç®—å³æ—¶å¥–åŠ± $r_t = p_{t+1} - p_t$ã€‚ <br>    e. å°† $(o_t, a_t, r_t)$ å­˜å…¥ç¼“å†²åŒºã€‚ <br>    f. **æ¯éš”Kæ­¥è¿›è¡Œç­–ç•¥æ›´æ–°ï¼š** ä½¿ç”¨ç¼“å†²åŒºä¸­çš„æ•°æ®ï¼Œæ ¹æ®ä¸Šè¿°æ— ä»·å€¼å‡½æ•°PPOç›®æ ‡è®¡ç®—æ¢¯åº¦ï¼Œæ›´æ–°ç­–ç•¥å‚æ•° $\\theta$ã€‚ <br>    g. æ¸…ç©ºç¼“å†²åŒºï¼Œç»§ç»­ä¸‹ä¸€æ­¥ã€‚ | - **æ›´æ–°é—´éš” $K$ï¼š** å®éªŒå‘ç°æ¯ **8** æ­¥æ›´æ–°ä¸€æ¬¡æ•ˆæœæœ€ä½³ï¼Œå¹³è¡¡äº†é€‚åº”é€Ÿåº¦ä¸è®­ç»ƒç¨³å®šæ€§ã€‚ <br> - **ä¼˜åŒ–å™¨ï¼š** AdamWã€‚ <br> - **å­¦ä¹ ç‡ï¼š** ä» ${1\\times10^{-5}, 5\\times10^{-5}, 1\\times10^{-4}}$ ä¸­é€‰å–ã€‚ <br> - **æ¨¡å‹å¾®è°ƒæ–¹å¼ï¼š** é‡‡ç”¨ **LoRA**ï¼ˆä½ç§©é€‚åº”ï¼‰ï¼Œç§©ä¸º {16, 32}ï¼Œä»¥é«˜æ•ˆæ›´æ–°å¤§æ¨¡å‹å‚æ•°ã€‚ |\n\n## å®é™…ä»·å€¼ä¸æ„ä¹‰\n\n1.  **å¡«è¡¥äº†VLAéƒ¨ç½²è‡ªé€‚åº”èƒ½åŠ›çš„ç©ºç™½**ï¼šé¦–æ¬¡ç³»ç»Ÿæ€§åœ°å°†æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ åº”ç”¨äºVLAæ¨¡å‹ï¼Œä½¿å…¶èƒ½ä»éƒ¨ç½²ç¯å¢ƒçš„äº¤äº’ä¸­æŒç»­è‡ªæˆ‘æ”¹è¿›ï¼Œæ˜¯è¿ˆå‘â€œéƒ¨ç½²å³ç”¨â€ä¸”å…·å¤‡è‡ªé€‚åº”èƒ½åŠ›æœºå™¨äººçš„å…³é”®ä¸€æ­¥ã€‚\n2.  **è½»é‡çº§ã€é«˜æ•ˆçš„åœ¨çº¿é€‚åº”æ–¹æ¡ˆ**ï¼šé€šè¿‡**æ— ä»·å€¼å‡½æ•°ä¼˜åŒ–**å’Œ**åŸºäºå³æ—¶è¿›åº¦å¥–åŠ±**çš„è®¾è®¡ï¼Œæå¤§é™ä½äº†æµ‹è¯•æ—¶æ›´æ–°çš„è®¡ç®—å¼€é”€å’Œå¤æ‚åº¦ï¼Œä½¿å…¶èƒ½å¤Ÿæ»¡è¶³çœŸå®æœºå™¨äººåœºæ™¯çš„**å®æ—¶æ€§è¦æ±‚**ã€‚\n3.  **å¹¿æ³›çš„å…¼å®¹æ€§ä¸æå‡æ•ˆæœ**ï¼šè¯¥æ–¹æ³•ä½œä¸ºâ€œå³æ’å³ç”¨â€çš„è¡¥å……æ¨¡å—ï¼Œåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­ï¼Œå‡èƒ½ç¨³å®šæå‡å¤šç§ä¸»æµVLAåŸºåº§æ¨¡å‹ï¼ˆNora, OpenVLA, OpenVLA-RL, TraceVLAï¼‰åœ¨**æ‰§è¡Œã€è§†è§‰ã€è¯­ä¹‰**ä¸‰ä¸ªç»´åº¦çš„æ³›åŒ–æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ™®é€‚æ€§å’Œæœ‰æ•ˆæ€§ã€‚\n4.  **ç†è®ºæŒ‡å¯¼å®è·µ**ï¼šè®ºæ–‡ä¸ä»…æå‡ºäº†æ–¹æ³•ï¼Œè¿˜é€šè¿‡**å‘½é¢˜1ã€æ¨è®º1å’ŒLemma 1** ç­‰ç†è®ºåˆ†æï¼Œæ·±åˆ»æ­ç¤ºäº†åœ¨æµ‹è¯•æ—¶ã€åŸºäºè¿›åº¦å·®åˆ†çš„å¥–åŠ±è®¾å®šä¸‹ï¼Œä¼ ç»ŸGAEä¼˜åŠ¿ä¼°è®¡ä¼šå¤±æ•ˆæˆ–äº§ç”Ÿåå·®çš„æ ¹æœ¬åŸå› ï¼Œä»è€Œä¸ºå…¶ç‹¬ç‰¹çš„â€œå¥–åŠ±å³ä¼˜åŠ¿â€è®¾è®¡æä¾›äº†åšå®çš„ç†è®ºä¾æ®ã€‚\n\n**æ€»ç»“**ï¼šTT-VLAçš„æ ¸å¿ƒæŠ€æœ¯è´¡çŒ®åœ¨äº**ä¸€å¥—ä¸“ä¸ºæµ‹è¯•æ—¶VLAé€‚åº”è®¾è®¡çš„ã€ç†è®ºè‡ªæ´½çš„è½»é‡çº§åœ¨çº¿RLæ¡†æ¶**ã€‚å®ƒé€šè¿‡**å¯†é›†è¿›åº¦å¥–åŠ±**æä¾›å­¦ä¹ ä¿¡å·ï¼Œé€šè¿‡**æ— ä»·å€¼å‡½æ•°PPO**å®ç°é«˜æ•ˆæ›´æ–°ï¼Œæœ€ç»ˆä½¿æœºå™¨äººèƒ½åœ¨åŠ¨æ€æœªçŸ¥ç¯å¢ƒä¸­å®ç°å®æ—¶çš„ç­–ç•¥è‡ªä¼˜åŒ–ï¼Œå…·æœ‰é‡è¦çš„å­¦æœ¯åˆ›æ–°ä»·å€¼å’Œå®é™…åº”ç”¨æ½œåŠ›ã€‚",
            "timestamp": "2026-01-21T12:13:27.271173",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡\næœ¬æ–‡çš„æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡æ˜¯**ä»»åŠ¡æˆåŠŸç‡ï¼ˆSuccess Rate, %ï¼‰**ã€‚è¯¥æŒ‡æ ‡ç”¨äºè¡¡é‡æ™ºèƒ½ä½“åœ¨å•ä¸ªä»»åŠ¡è¯•éªŒä¸­ï¼Œæ˜¯å¦èƒ½å¤Ÿæ ¹æ®è¯­è¨€æŒ‡ä»¤æˆåŠŸå®ŒæˆæŒ‡å®šçš„æœºå™¨äººæ“ä½œï¼ˆå¦‚æŠ“å–å¹¶æ”¾ç½®ç‰©ä½“ï¼‰ã€‚\n\n**æŒ‡æ ‡ç»†èŠ‚ï¼š**\n- **å®šä¹‰**ï¼šåœ¨å•ä¸ªä»»åŠ¡è¯•éªŒï¼ˆepisodeï¼‰ç»“æŸæ—¶ï¼Œæ ¹æ®ä»»åŠ¡ç›®æ ‡ï¼ˆå¦‚ç‰©ä½“è¢«æˆåŠŸæ”¾å…¥æŒ‡å®šå®¹å™¨ï¼‰åˆ¤æ–­æˆåŠŸä¸å¦ï¼Œè®¡ç®—ä¸ºäºŒå€¼ç»“æœï¼ˆ1/0ï¼‰ã€‚æœ€ç»ˆæŠ¥å‘Šçš„æ˜¯åœ¨å¤šæ¬¡è¯•éªŒï¼ˆå¦‚80æ¬¡ï¼‰ä¸­çš„å¹³å‡æˆåŠŸç‡ã€‚\n- **åº”ç”¨åœºæ™¯**ï¼šè¯¥æŒ‡æ ‡è¢«åº”ç”¨äº**ä¸‰ä¸ªæ ¸å¿ƒçš„æ³›åŒ–ç»´åº¦**ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨æœªè§è¿‡çš„ã€åŠ¨æ€åœºæ™¯ä¸‹çš„é€‚åº”èƒ½åŠ›ã€‚å…·ä½“ç»´åº¦åŠåŒ…å«çš„ä»»åŠ¡å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æ³›åŒ–ç»´åº¦ | å…·ä½“ä»»åŠ¡ï¼ˆç¼©å†™ï¼‰ | ä»»åŠ¡æè¿°ä¸æŒ‘æˆ˜ |\n| :--- | :--- | :--- |\n| **æ‰§è¡Œ (Execution)** | Obj. Pos. | **ç‰©ä½“ä¸å®¹å™¨ä½ç½®æœªçŸ¥**ï¼šç‰©ä½“å’Œå®¹å™¨çš„åˆå§‹ä½ç½®åœ¨æ›´å¤§èŒƒå›´å†…éšæœºåŒ–ï¼Œæµ‹è¯•å¯¹ç©ºé—´å˜åŒ–çš„é²æ£’æ€§ã€‚ |\n| | Robot Pose | **æœºå™¨äººåˆå§‹å§¿æ€æœªçŸ¥**ï¼šæœºå™¨äººçš„åˆå§‹å…³èŠ‚å§¿æ€éšæœºåŒ–ï¼Œè€Œéå›ºå®šï¼Œæµ‹è¯•å¯¹è‡ªèº«çŠ¶æ€å˜åŒ–çš„é€‚åº”ã€‚ |\n| | Obj. Rep. | **ä»»åŠ¡ä¸­ç‰©ä½“è¢«é‡ç½®**ï¼šåœ¨ä»»åŠ¡æ‰§è¡Œåˆ°ç¬¬5æ­¥æ—¶ï¼Œç‰©ä½“è¢«ç¬ç§»åˆ°æ¡Œä¸Šå¦ä¸€ä¸ªéšæœºä½ç½®ï¼Œæµ‹è¯•å¯¹åŠ¨æ€å¹²æ‰°çš„å³æ—¶å“åº”èƒ½åŠ›ã€‚ |\n| **è§†è§‰ (Vision)** | Table | **æ¡Œé¢å¤–è§‚æœªçŸ¥**ï¼šä½¿ç”¨5ç§æœªè§è¿‡çš„æ¡Œé¢çº¹ç†ï¼Œæµ‹è¯•å¯¹èƒŒæ™¯å˜åŒ–çš„æ³›åŒ–ã€‚ |\n| | Texture-w/s | **åŠ¨æ€çº¹ç†ï¼ˆå¼±/å¼ºï¼‰**ï¼šåœ¨æ¯ä¸€æ—¶é—´æ­¥ï¼Œä»æœªè§è¿‡çš„çº¹ç†åº“ä¸­é€‰å–ä¸€å—çº¹ç†ï¼Œä»¥ä¸€å®šçš„é€æ˜åº¦ï¼ˆå¼±ï¼š0.3ï¼Œå¼ºï¼š0.5ï¼‰è¦†ç›–åˆ°ç‰©ä½“ã€å®¹å™¨å’Œæœºæ¢°è‡‚ä¸Šï¼Œæµ‹è¯•å¯¹å‰§çƒˆå‰æ™¯å¤–è§‚å˜åŒ–çš„é²æ£’æ€§ã€‚ |\n| | Noise-w/s | **åŠ¨æ€å™ªå£°ï¼ˆå¼±/å¼ºï¼‰**ï¼šä¸åŠ¨æ€çº¹ç†ç±»ä¼¼ï¼Œä½†å™ªå£°çº¹ç†è¦†ç›–æ•´ä¸ªå›¾åƒï¼Œæ¨¡æ‹Ÿä¼ æ„Ÿå™¨å™ªå£°æˆ–ç¯å¢ƒå¹²æ‰°ã€‚ |\n| **è¯­ä¹‰ (Semantics)** | Object | **æœªè§è¿‡çš„ç‰©ä½“**ï¼šä½¿ç”¨9ä¸ªè®­ç»ƒä¸­æœªå‡ºç°è¿‡çš„ç‰©ä½“ï¼Œæµ‹è¯•å¯¹æ–°é¢–ç‰©ä½“çš„æ³›åŒ–ã€‚ |\n| | Recep. | **æœªè§è¿‡çš„å®¹å™¨**ï¼šä½¿ç”¨16ä¸ªæœªè§è¿‡çš„å®¹å™¨ï¼ˆå¦‚ä¸åŒé¢œè‰²/å½¢çŠ¶çš„ç›˜å­ï¼‰æ›¿æ¢é»˜è®¤å®¹å™¨ï¼Œæµ‹è¯•å¯¹ç›®æ ‡è¯­ä¹‰å˜åŒ–çš„é€‚åº”ã€‚ |\n| | Instruct | **æœªè§è¿‡çš„æŒ‡ä»¤è¡¨è¿°**ï¼šä½¿ç”¨16ç§ä¸è®­ç»ƒæ¨¡æ¿ä¸åŒçš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼ˆå¦‚â€œPlace the O onto the Râ€ï¼‰ï¼Œæµ‹è¯•è¯­è¨€ç†è§£æ³›åŒ–ã€‚ |\n| | M-Obj. (IND/OOD) | **å¤šç‰©ä½“åœºæ™¯ï¼ˆå·²çŸ¥/æœªçŸ¥ï¼‰**ï¼šåœºæ™¯ä¸­å‡ºç°ä¸¤ä¸ªç‰©ä½“ï¼ŒæŒ‡ä»¤æŒ‡å®šå…¶ä¸­ä¸€ä¸ªã€‚æµ‹è¯•åœ¨å¹²æ‰°ç‰©å­˜åœ¨ä¸‹çš„ç›®æ ‡é€‰æ‹©å’Œç»„åˆæ¨ç†èƒ½åŠ›ã€‚INDæŒ‡ç‰©ä½“æ¥è‡ªè®­ç»ƒé›†ï¼ŒOODæ¥è‡ªæµ‹è¯•é›†ã€‚ |\n| | Dist Recep. | **å¹²æ‰°æ€§å®¹å™¨**ï¼šåœ¨åœºæ™¯ä¸­æ”¾ç½®ä¸€ä¸ªé¢å¤–çš„ã€æœªè¢«æŒ‡ä»¤æåŠçš„å®¹å™¨ä½œä¸ºå¹²æ‰°ï¼Œæµ‹è¯•å¯¹æ— å…³è¯­ä¹‰ä¿¡æ¯çš„è¿‡æ»¤èƒ½åŠ›ã€‚ |\n| | M Recep. | **å¤šå®¹å™¨åœºæ™¯**ï¼šåœºæ™¯ä¸­å‡ºç°ä¸¤ä¸ªå®¹å™¨ï¼ŒæŒ‡ä»¤æŒ‡å®šå…¶ä¸­ä¸€ä¸ªã€‚æµ‹è¯•åœ¨å¤šä¸ªæ½œåœ¨ç›®æ ‡ä¸­çš„ç²¾ç¡®å®šä½èƒ½åŠ›ã€‚ |\n\n### äºŒã€ ä½¿ç”¨çš„æ•°æ®é›†ä¸å®éªŒç¯å¢ƒ\næœ¬æ–‡æœªä½¿ç”¨ä¼ ç»Ÿçš„é™æ€æ•°æ®é›†è¿›è¡Œè®­ç»ƒæˆ–æµ‹è¯•ï¼Œè€Œæ˜¯ä¸»è¦åœ¨**æ¨¡æ‹Ÿä»¿çœŸç¯å¢ƒ**å’Œ**çœŸå®æœºå™¨äººå¹³å°**ä¸Šæ„å»ºåŠ¨æ€ä»»åŠ¡è¿›è¡Œè¯„ä¼°ã€‚\n\n**1. æ¨¡æ‹Ÿå®éªŒ (Simulation)**\n- **ç¯å¢ƒ**ï¼š**ManiSkill 3** (Tao et al., 2024)ã€‚è¿™æ˜¯ä¸€ä¸ªGPUå¹¶è¡ŒåŒ–çš„æœºå™¨äººä»¿çœŸä¸æ¸²æŸ“å¹³å°ï¼Œé€‚ç”¨äº embodied AI ç ”ç©¶ã€‚\n- **æœºå™¨äºº**ï¼šWidowX-250S æœºæ¢°è‡‚ã€‚\n- **ä»»åŠ¡è®¾ç½®**ï¼š\n    - **åŸºç¡€ä»»åŠ¡**ï¼šæ ‡å‡†çš„â€œæŠ“å–-æ”¾ç½®â€æ“ä½œã€‚æ™ºèƒ½ä½“æ¥æ”¶640Ã—480çš„RGBå›¾åƒå’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œè¾“å‡ºç¬›å¡å°”ç©ºé—´ä¸‹çš„æœ«ç«¯æ‰§è¡Œå™¨ä½ç§»å’ŒäºŒè¿›åˆ¶å¤¹çˆªæŒ‡ä»¤ã€‚\n    - **è®­ç»ƒè®¾ç½®**ï¼šç”¨äºâ€œé¢„çƒ­â€åŸºçº¿çš„æ•°æ®éµå¾ªRL4VLA (Liu et al., 2025a) çš„è®¾ç½®ï¼Œä½¿ç”¨16ä¸ªç‰©ä½“å’Œ16ç§æ¡Œé¢çº¹ç†ï¼Œåœ¨å›ºå®šæŒ‡ä»¤æ¨¡æ¿ä¸‹è‡ªåŠ¨æ”¶é›†äº†16,400æ¡æ¼”ç¤ºè½¨è¿¹ã€‚\n    - **æµ‹è¯•è®¾ç½®**ï¼šå¦‚ä¸Šè¡¨æ‰€è¿°ï¼Œåœ¨ä¸‰ä¸ªæ³›åŒ–ç»´åº¦ä¸Šæ„å»ºäº†15ç§å…·ä½“çš„â€œæœªè§è¿‡çš„ä»»åŠ¡â€å˜ä½“ã€‚æ¯ä¸ªä»»åŠ¡è¿›è¡Œ**80æ¬¡è¯•éªŒ**ï¼ˆä¸åŒéšæœºç§å­ï¼‰ï¼ŒæŠ¥å‘Šå¹³å‡æˆåŠŸç‡ã€‚\n\n**2. çœŸå®ä¸–ç•Œå®éªŒ (Real-World)**\n- **å¹³å°**ï¼šFranka Research 3 æœºæ¢°è‡‚ã€‚\n- **ä»»åŠ¡è®¾ç½®**ï¼š\n    - æ‰§è¡Œ9ç§æœªè§è¿‡çš„â€œæŠ“å–-æ”¾ç½®â€ä»»åŠ¡ï¼ŒåŒæ ·æ¶µç›–æ‰§è¡Œã€è§†è§‰ã€è¯­ä¹‰ä¸‰ä¸ªç»´åº¦ï¼ˆä¾‹å¦‚ï¼Œæ”¹å˜åˆå§‹é…ç½®ã€èƒŒæ™¯ã€ä½¿ç”¨æœªè§è¿‡çš„ç›˜å­ï¼‰ã€‚\n    - è¾“å…¥ä¸º500Ã—480çš„RGBå›¾åƒå’ŒæŒ‡ä»¤ï¼Œè¾“å‡ºä¸ºæœ«ç«¯ä½ç§»å’Œå¤¹çˆªæŒ‡ä»¤ã€‚\n    - æ¯ä¸ªä»»åŠ¡è¿›è¡Œ**10æ¬¡è¯•éªŒ**ï¼Œåœ¨å…‰ç…§ã€èƒŒæ™¯ã€è§†è§’å¯æ§çš„æ¡ä»¶ä¸‹è¿›è¡Œã€‚\n\n### ä¸‰ã€ åŸºçº¿æ¨¡å‹ (Benchmark Models)\nè¯„ä»·åœ¨å››ä¸ªå¼€æºVLAæ¨¡å‹ä¸Šè¿›è¡Œï¼Œä»£è¡¨äº†ä¸åŒçš„æ¶æ„å’Œè®­ç»ƒèŒƒå¼ï¼š\n1.  **Nora**ï¼šåŸºäºQwen2.5-VL-3Bï¼Œä½¿ç”¨FAST+åˆ†è¯å™¨ç”ŸæˆåŠ¨ä½œåºåˆ—ã€‚\n2.  **OpenVLA**ï¼šåŸºäºLlama-2-7Bï¼Œå¹¿æ³›ä½¿ç”¨çš„å¼€æºVLAæ¨¡å‹ã€‚\n3.  **OpenVLA-RL**ï¼šåœ¨OpenVLAåŸºç¡€ä¸Šï¼Œé€šè¿‡è®­ç»ƒé˜¶æ®µå¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥å¾®è°ƒã€‚\n4.  **TraceVLA**ï¼šé€šè¿‡è§†è§‰è½¨è¿¹æç¤ºå¢å¼ºæ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚\n\n**è¯„ä»·æ–¹å¼**ï¼šæŠ¥å‘Šæ¯ä¸ªåŸºçº¿æ¨¡å‹åœ¨15ä¸ªæ¨¡æ‹Ÿä»»åŠ¡ä¸Šçš„åŸå§‹æˆåŠŸç‡ï¼Œä»¥åŠå åŠ æœ¬æ–‡æå‡ºçš„ **TT-VLA** æ–¹æ³•åçš„æˆåŠŸç‡ï¼Œè®¡ç®—ç»å¯¹æå‡ï¼ˆÎ”ï¼‰å’Œç›¸å¯¹æå‡ï¼ˆâ†‘ï¼‰ï¼Œä»¥è¯æ˜æ–¹æ³•çš„æ™®é€‚æ€§å’Œæœ‰æ•ˆæ€§ã€‚\n\n### æ€»ç»“\næœ¬æ–‡çš„è¯„ä»·ä½“ç³»**ç´§å¯†å›´ç»•å…¶æ ¸å¿ƒåˆ›æ–°ç‚¹â€”â€”æµ‹è¯•æ—¶é€‚åº”èƒ½åŠ›**è€Œè®¾è®¡ï¼š\n- **æŒ‡æ ‡**ï¼šé‡‡ç”¨ç›´æ¥åæ˜ ä»»åŠ¡å®Œæˆåº¦çš„**æˆåŠŸç‡**ã€‚\n- **ä»»åŠ¡è®¾è®¡**ï¼šç²¾å¿ƒæ„å»ºäº†è¦†ç›–**æ‰§è¡Œã€è§†è§‰ã€è¯­ä¹‰**ä¸‰å¤§ç»´åº¦çš„ã€å…·æœ‰æŒ‘æˆ˜æ€§çš„**æœªè§è¿‡çš„åŠ¨æ€ä»»åŠ¡**ï¼Œè€Œéåœ¨é™æ€åŸºå‡†ä¸Šæµ‹è¯•ï¼Œè¿™æœ‰æ•ˆéªŒè¯äº†â€œon-the-fly adaptationâ€çš„å®é™…ä»·å€¼ã€‚\n- **å®éªŒè®¾ç½®**ï¼šæ¶µç›–**ä»¿çœŸä¸å®ç‰©**ï¼Œè¯æ˜äº†æ–¹æ³•çš„å¯è¡Œæ€§ä¸æ³›åŒ–æ€§ã€‚\n- **åŸºçº¿å¯¹æ¯”**ï¼šåœ¨å¤šä¸ªä¸»æµVLAæ¨¡å‹ä¸ŠéªŒè¯ï¼Œè¯æ˜äº†å…¶ä½œä¸º**å³æ’å³ç”¨å¢å¼ºæ¨¡å—**çš„å®ç”¨ä»·å€¼ã€‚",
            "timestamp": "2026-01-21T12:14:14.933532",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†ä¸ç¯å¢ƒ\nè®ºæ–‡åœ¨**ä»¿çœŸ**å’Œ**çœŸå®ä¸–ç•Œ**ä¸¤ç§ç¯å¢ƒä¸‹è¿›è¡Œäº†å…¨é¢å®éªŒï¼Œä»¥è¯„ä¼°TT-VLAæ¡†æ¶çš„æ³›åŒ–ä¸é€‚åº”èƒ½åŠ›ã€‚\n\n#### 1. ä»¿çœŸå®éªŒ\n- **ä»¿çœŸå¹³å°**ï¼šManiSkill 3ã€‚\n- **æœºå™¨äººå¹³å°**ï¼šWidowX-250Sæœºæ¢°è‡‚ã€‚\n- **ä»»åŠ¡æ ¸å¿ƒ**ï¼šæ ‡å‡†çš„â€œæ‹¾å–-æ”¾ç½®â€æ“ä½œä»»åŠ¡ã€‚\n- **æ³›åŒ–ç»´åº¦**ï¼šå®éªŒéµå¾ªRL4VLAçš„è®¾ç½®ï¼Œä»ä¸‰ä¸ªç»´åº¦ç³»ç»Ÿæ€§åœ°æµ‹è¯•æ¨¡å‹åœ¨**æœªè§ä»»åŠ¡**ä¸Šçš„è¡¨ç°ï¼š\n    - **æ‰§è¡Œæ³›åŒ–**ï¼šéšæœºåŒ–æœºå™¨äººã€ç‰©ä½“ã€å®¹å™¨çš„åˆå§‹ä½å§¿ï¼Œå¹¶å¼•å…¥**ä»»åŠ¡ä¸­é€”ç‰©ä½“é‡å®šä½**çš„åŠ¨æ€å¹²æ‰°ã€‚\n    - **è§†è§‰æ³›åŒ–**ï¼šæ”¹å˜å‰æ™¯å’ŒèƒŒæ™¯å¤–è§‚ï¼ŒåŒ…æ‹¬åŠ¨æ€çº¹ç†ã€æœªè§è¿‡çš„æ¡Œé¢æè´¨ã€å›¾åƒçº§å™ªå£°ï¼ˆåˆ†ä¸ºå¼±/å¼ºå¼ºåº¦ï¼‰ã€‚\n    - **è¯­ä¹‰æ³›åŒ–**ï¼šå¼•å…¥æœªè§è¿‡çš„ç‰©ä½“ã€å®¹å™¨ã€æŒ‡ä»¤è¡¨è¿°ï¼Œä»¥åŠåŒ…å«å¤šç‰©ä½“ã€å¤šå®¹å™¨å’Œå¹²æ‰°å®¹å™¨çš„ç»„åˆä»»åŠ¡ã€‚\n\n#### 2. çœŸå®ä¸–ç•Œå®éªŒ\n- **æœºå™¨äººå¹³å°**ï¼šFranka Research 3ã€‚\n- **ä»»åŠ¡è®¾ç½®**ï¼šè®¾è®¡äº†9ä¸ªæœªè§è¿‡çš„â€œæ‹¾å–-æ”¾ç½®â€ä»»åŠ¡ï¼ŒåŒæ ·è¦†ç›–**æ‰§è¡Œã€è§†è§‰ã€è¯­ä¹‰**ä¸‰ä¸ªæ³›åŒ–ç»´åº¦ï¼Œæ¯ä¸ªç»´åº¦3ä¸ªä»»åŠ¡ã€‚\n- **ç¯å¢ƒ**ï¼šå›ºå®šç›¸æœºè§†è§’ã€å¯æ§å…‰ç…§ã€é™æ€èƒŒæ™¯ã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\n- **æ ¸å¿ƒæŒ‡æ ‡**ï¼š**ä»»åŠ¡æˆåŠŸç‡**ã€‚\n- **è¯„ä¼°æ–¹å¼**ï¼š\n    - **ä»¿çœŸ**ï¼šæ¯ä¸ªä»»åŠ¡è¿›è¡Œ80æ¬¡è¯•éªŒï¼ŒæŠ¥å‘Šå¹³å‡æˆåŠŸç‡ã€‚\n    - **çœŸå®ä¸–ç•Œ**ï¼šæ¯ä¸ªä»»åŠ¡è¿›è¡Œ10æ¬¡è¯•éªŒã€‚\n- **å¯¹æ¯”åŸºå‡†**ï¼šè®ºæ–‡åœ¨å››ä¸ªå…ˆè¿›çš„**å¼€æºVLAæ¨¡å‹**ä¸Šåº”ç”¨TT-VLAï¼Œå¹¶å¯¹æ¯”å…¶åŸå§‹æ€§èƒ½ï¼š\n    1. **Nora** (åŸºäºQwen-2.5-VL)\n    2. **OpenVLA** (åŸºäºLlama-2-7B)\n    3. **OpenVLA-RL** (ç»è¿‡è®­ç»ƒæ—¶RLå¾®è°ƒçš„OpenVLA)\n    4. **TraceVLA** (å…·æœ‰è§†è§‰è½¨è¿¹æç¤ºçš„æ¨¡å‹)\n\n### ä¸‰ã€ ä¸»è¦æ€§èƒ½æå‡\nTT-VLAåœ¨æ‰€æœ‰åŸºå‡†æ¨¡å‹å’Œå‡ ä¹æ‰€æœ‰ä»»åŠ¡ç±»åˆ«ä¸Šéƒ½å¸¦æ¥äº†**ä¸€è‡´ä¸”æ˜¾è‘—çš„æ€§èƒ½æå‡**ã€‚\n\n#### å…³é”®æ•°æ®æ‘˜è¦ï¼ˆåŸºäºä»¿çœŸå®éªŒä¸»è¡¨ï¼‰ï¼š\n| æ¨¡å‹ | å¹³å‡æå‡ (ç»å¯¹Î”) | å…³é”®äº®ç‚¹ (æœ€å¤§ç›¸å¯¹å¢ç›Š) |\n| :--- | :--- | :--- |\n| **Nora** | **æ‰§è¡Œ**: +2.08% <br> **è§†è§‰**: +3.83% <br> **è¯­ä¹‰**: +1.96% | - **ç‰©ä½“é‡å®šä½ä»»åŠ¡**: **ç›¸å¯¹æå‡44.40%** (ä»7.50%åˆ°10.83%) <br> - **å¼ºå™ªå£°ä»»åŠ¡**: ç›¸å¯¹æå‡18.15% |\n| **OpenVLA** | **æ‰§è¡Œ**: +3.45% <br> **è§†è§‰**: +2.10% <br> **è¯­ä¹‰**: +2.88% | - **å¹²æ‰°å®¹å™¨ä»»åŠ¡**: **ç›¸å¯¹æå‡44.90%** (ä»20.42%åˆ°29.58%) <br> - **ç‰©ä½“é‡å®šä½ä»»åŠ¡**: ç›¸å¯¹æå‡18.40% |\n| **OpenVLA-RL** | **æ‰§è¡Œ**: +2.64% <br> **è§†è§‰**: +1.75% <br> **è¯­ä¹‰**: +1.55% | - åœ¨å·²ç»å¾ˆå¼ºçš„RLå¾®è°ƒæ¨¡å‹ä¸Šä»èƒ½è·å¾—**ç¨³å®šæå‡** (å¦‚æ‰§è¡Œä»»åŠ¡+2.64%) |\n| **TraceVLA** | **æ‰§è¡Œ**: +2.03% <br> **è§†è§‰**: +0.84% <br> **è¯­ä¹‰**: +1.66% | - **å¤šç‰©ä½“OODä»»åŠ¡**: ç›¸å¯¹æå‡11.11% <br> - **å¤šå®¹å™¨ä»»åŠ¡**: ç›¸å¯¹æå‡7.89% |\n\n#### æ ¸å¿ƒç»“è®ºï¼š\n1.  **æ™®é€‚æ€§**ï¼šTT-VLAèƒ½å¤Ÿä½œä¸º**å³æ’å³ç”¨çš„è¡¥å……æ¨¡å—**ï¼Œæœ‰æ•ˆæå‡**ä¸åŒç±»å‹ã€ä¸åŒè®­ç»ƒèŒƒå¼**çš„VLAåŸºåº§æ¨¡å‹åœ¨åŠ¨æ€æœªçŸ¥åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚\n2.  **å¯¹å¼±æ¨¡å‹æå‡æ›´å¤§**ï¼šåœ¨åˆå§‹æ€§èƒ½ç›¸å¯¹è¾ƒä½çš„æ¨¡å‹ï¼ˆå¦‚Noraï¼‰ä¸Šï¼Œ**ç›¸å¯¹æå‡å¹…åº¦å°¤ä¸ºæ˜¾è‘—**ï¼ˆéƒ¨åˆ†ä»»åŠ¡è¶…è¿‡40%ï¼‰ï¼Œè¯æ˜äº†æµ‹è¯•æ—¶é€‚åº”å¯¹äºå¼¥è¡¥æ¨¡å‹å…ˆå¤©ä¸è¶³çš„ä»·å€¼ã€‚\n3.  **å¯¹å¼ºæ¨¡å‹ä»æœ‰å¢ç›Š**ï¼šå³ä½¿åœ¨å·²ç»è¿‡RLè®­ç»ƒã€æ€§èƒ½å¾ˆé«˜çš„æ¨¡å‹ï¼ˆå¦‚OpenVLA-RLï¼‰ä¸Šï¼ŒTT-VLAä»èƒ½å¸¦æ¥**é¢å¤–çš„ç¨³å¥æ€§æå‡**ï¼Œè¯´æ˜å…¶é€‚åº”æœºåˆ¶ä¸è®­ç»ƒæ—¶ä¼˜åŒ–æ˜¯äº’è¡¥çš„ã€‚\n4.  **çœŸå®ä¸–ç•Œæœ‰æ•ˆ**ï¼šåœ¨çœŸå®æœºå™¨äººå®éªŒä¸­ï¼ŒTT-VLAåŒæ ·ç¨³å®šæå‡äº†OpenVLAåœ¨ä¹é¡¹æœªè§ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ï¼Œå¹¶é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†å…¶èƒ½**åœ¨çº¿çº æ­£æ‰§è¡Œè½¨è¿¹åå·®**ï¼ŒéªŒè¯äº†å…¶å®é™…éƒ¨ç½²ä»·å€¼ã€‚\n\n**æ€»ç»“**ï¼šè®ºæ–‡é€šè¿‡ç³»ç»Ÿæ€§çš„å®éªŒè¡¨æ˜ï¼ŒTT-VLAæ¡†æ¶æ— éœ€é‡æ–°è®­ç»ƒï¼Œä»…é€šè¿‡åœ¨æµ‹è¯•æ—¶åˆ©ç”¨å¯†é›†çš„è¿›åº¦å¥–åŠ±è¿›è¡Œç­–ç•¥å¾®è°ƒï¼Œå°±èƒ½æ˜¾è‘—ä¸”ç¨³å®šåœ°æå‡å„ç§VLAæ¨¡å‹åœ¨**æ‰§è¡Œã€è§†è§‰ã€è¯­ä¹‰**ä¸‰ä¸ªå…³é”®æ³›åŒ–ç»´åº¦ä¸Šçš„é€‚åº”èƒ½åŠ›å’Œä»»åŠ¡æˆåŠŸç‡ã€‚",
            "timestamp": "2026-01-21T12:14:52.221290",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„åˆ†æï¼ŒTT-VLA æ¡†æ¶è™½ç„¶åˆ›æ–°æ€§åœ°è§£å†³äº† VLA æ¨¡å‹åœ¨æµ‹è¯•æ—¶æ— æ³•è‡ªé€‚åº”çš„é—®é¢˜ï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹å‡ ä¸ªä¸»è¦å±€é™æ€§ï¼š\n\n- **æ¶æ„é€‚ç”¨èŒƒå›´å—é™**ï¼šè®ºæ–‡æ˜ç¡®æŒ‡å‡ºï¼Œå½“å‰æ–¹æ³•ä¸»è¦é’ˆå¯¹**åŸºäºç¦»æ•£åŒ–åŠ¨ä½œç©ºé—´**çš„ VLA æ¨¡å‹ï¼ˆå¦‚ OpenVLAã€Noraï¼‰ã€‚å¯¹äºå¦ä¸€å¤§ç±»**åŸºäºæ‰©æ•£æ¨¡å‹**çš„ VLAï¼ˆå¦‚ Diffusion Policyï¼‰ï¼Œç”±äºå…¶åŠ¨ä½œè§£ç è¿‡ç¨‹æ˜¯éšå¼çš„ã€å¤šæ­¥å»å™ªçš„ï¼Œä¸ PPO ç­‰ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ‰€éœ€çš„é€æ­¥å¯¹æ•°ä¼¼ç„¶è®¡ç®—ä¸å…¼å®¹ï¼Œå› æ­¤ TT-VLA æ— æ³•ç›´æ¥åº”ç”¨ã€‚è®ºæ–‡Â§S1.1 å°†æ­¤åˆ—ä¸ºæœªæ¥å·¥ä½œæ–¹å‘ã€‚\n\n- **è®¡ç®—ä¸å®æ—¶æ€§çº¦æŸ**ï¼šå°½ç®¡è®ºæ–‡é€šè¿‡**å€¼å‡½æ•°å…å­¦ä¹ **å’Œ**å•æ­¥ä¼˜åŠ¿ä¼°è®¡**ï¼ˆ`Î»=0, Î³=0`ï¼‰å¤§å¹…é™ä½äº†è®¡ç®—å¼€é”€ï¼Œä½¿å…¶ä¼˜äº EVOLVE-VLA ç­‰åŸºäº GRPO çš„æ–¹æ³•ï¼ˆÂ§S7ï¼‰ï¼Œä½†åœ¨**ä¸¥æ ¼å®æ—¶è¦æ±‚**çš„ç‰©ç†æœºå™¨äººåœºæ™¯ä¸­ï¼Œæ¯ä¸€æ­¥éƒ½è¿›è¡Œç­–ç•¥æ›´æ–°ï¼ˆå³ä½¿æ˜¯ä½ç§©LoRAæ›´æ–°ï¼‰ä»å¯èƒ½å¸¦æ¥ä¸å¯å¿½ç•¥çš„å»¶è¿Ÿã€‚è®ºæ–‡ä¸­é€šè¿‡é—´éš”8æ­¥æ›´æ–°ï¼ˆè¡¨2ï¼‰æ¥æƒè¡¡ï¼Œä½†è¿™å¯èƒ½ç‰ºç‰²äº†éƒ¨åˆ†å³æ—¶é€‚åº”æ€§ã€‚\n\n- **å¥–åŠ±ä¿¡å·çš„ä¾èµ–ä¸è´¨é‡**ï¼šTT-VLA çš„æ ¸å¿ƒä¾èµ–äºä¸€ä¸ª**é¢„è®­ç»ƒçš„ä»»åŠ¡è¿›åº¦ä¼°è®¡å™¨ `Î¦`**ï¼ˆæ–‡ä¸­ä½¿ç”¨ VLACï¼‰ã€‚è¿™ä¸ªæ¨¡å—çš„æ€§èƒ½ç›´æ¥å†³å®šäº†å¯†é›†å¥–åŠ± `r_t = p_t - p_{t-1}` çš„è´¨é‡ã€‚å¦‚æœ `Î¦` åœ¨æç«¯æœªè§è¿‡çš„åœºæ™¯ä¸­ä¼°è®¡ä¸å‡†ï¼Œæˆ–å¯¹ç»†å¾®è¿›å±•ä¸æ•æ„Ÿï¼Œåˆ™å¥–åŠ±ä¿¡å·å¯èƒ½å¤±æ•ˆï¼Œè¿›è€Œè¯¯å¯¼ç­–ç•¥æ›´æ–°ã€‚\n\n- **å®‰å…¨æ€§ä¸é²æ£’æ€§é£é™©**ï¼šè®ºæ–‡åœ¨é™„å½•Â§S11ä¸­æ‰¿è®¤ï¼Œ**æµ‹è¯•æ—¶åœ¨çº¿ç­–ç•¥æ›´æ–°**å¼•å…¥äº†æ–°çš„é£é™©ã€‚åœ¨åŠ¨æ€ã€ä¸ç¡®å®šçš„çœŸå®ä¸–ç•Œä¸­ï¼ŒåŸºäºä¸å®Œå…¨æˆ–å˜ˆæ‚çš„è¿›åº¦ä¿¡å·è¿›è¡Œç­–ç•¥è°ƒæ•´ï¼Œå¯èƒ½å¯¼è‡´æ„å¤–ç”šè‡³å±é™©çš„è¡Œä¸ºã€‚ç›®å‰æ¡†æ¶ç¼ºä¹å†…ç½®çš„å®‰å…¨çº¦æŸæˆ–é£é™©è§„é¿æœºåˆ¶ã€‚\n\n- **å•ä»»åŠ¡ã€å•å›åˆé€‚åº”**ï¼šæ–¹æ³•è®¾è®¡ä¸»è¦é’ˆå¯¹**å•ä¸ªä»»åŠ¡å›åˆå†…**çš„åœ¨çº¿é€‚åº”ã€‚å®ƒæ²¡æœ‰æ˜¾å¼æœºåˆ¶æ¥**è·¨å›åˆç§¯ç´¯å’Œè½¬ç§»çŸ¥è¯†**ã€‚å¦‚æœä¸€ä¸ªæœºå™¨äººåœ¨ä¸åŒæ—¥å­æ‰§è¡Œç›¸ä¼¼ä»»åŠ¡ï¼ŒTT-VLA æ¯æ¬¡éƒ½ä¼šä»å¤´å¼€å§‹é€‚åº”ï¼Œæ— æ³•å½¢æˆæŒç»­çš„ç»éªŒç§¯ç´¯å’Œè‡ªæˆ‘æå‡ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nç»“åˆè®ºæ–‡çš„è®¨è®ºï¼ˆç‰¹åˆ«æ˜¯Â§S12ï¼‰å’Œåˆ†æï¼Œæœªæ¥ç ”ç©¶å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹å‘æ·±å…¥ï¼š\n\n1.  **æ‰©å±•åˆ°æ‰©æ•£å‹ VLA æ¨¡å‹**ï¼š\n    - æ­£å¦‚è®ºæ–‡æ‰€å»ºè®®ï¼Œæœªæ¥å·¥ä½œåº”æ¢ç´¢å°†æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ é€‚é…åˆ°**æ‰©æ•£å‹ VLA**ã€‚è¿™å¯èƒ½æ¶‰åŠè®¾è®¡æ–°çš„ç­–ç•¥æ¢¯åº¦ä¼°è®¡æ–¹æ³•ï¼Œä»¥é€‚åº”æ‰©æ•£æ¨¡å‹çš„éšå¼ç­–ç•¥å’Œå»å™ªè¿‡ç¨‹ï¼Œæˆ–è€…åˆ©ç”¨åƒ dVLA é‚£æ ·å°†åŠ¨ä½œé‡æ–°è¡¨è¿°ä¸ºè‡ªå›å½’ç¦»æ•£åºåˆ—çš„æ¶æ„ã€‚\n\n2.  **æ›´é«˜æ•ˆã€æ›´å®‰å…¨çš„åœ¨çº¿å­¦ä¹ ç®—æ³•**ï¼š\n    - å¼€å‘è®¡ç®—å¼€é”€æ›´ä½çš„åœ¨çº¿ä¼˜åŒ–ç®—æ³•ï¼Œä¾‹å¦‚æ›´é«˜æ•ˆçš„äºŒé˜¶ä¼˜åŒ–è¿‘ä¼¼æˆ–åŠ¨æ€è°ƒæ•´æ›´æ–°é¢‘ç‡çš„æœºåˆ¶ã€‚\n    - é›†æˆ**å®‰å…¨å¼ºåŒ–å­¦ä¹ **æ€æƒ³ï¼Œåœ¨ç­–ç•¥æ›´æ–°ä¸­å¼•å…¥æˆæœ¬å‡½æ•°æˆ–å®‰å…¨çº¦æŸï¼Œé˜²æ­¢ç­–ç•¥åœ¨æ¢ç´¢ä¸­äº§ç”Ÿç ´åæ€§è¡Œä¸ºã€‚\n\n3.  **æ”¹è¿›å¥–åŠ±å¡‘é€ ä¸è¿›åº¦ä¼°è®¡**ï¼š\n    - ç ”ç©¶æ›´é²æ£’ã€æ›´é€šç”¨çš„**è¿›åº¦ä¼°è®¡æ–¹æ³•**ï¼Œä¾‹å¦‚ç»“åˆå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆVLMsï¼‰çš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›ï¼Œæˆ–å¼€å‘èƒ½åœ¨çº¿å°å¹…æ›´æ–°çš„è¿›åº¦ä¼°è®¡å™¨ã€‚\n    - æ¢ç´¢**æ··åˆå¥–åŠ±ä¿¡å·**ï¼Œç»“åˆç¨ å¯†è¿›åº¦å¥–åŠ±ä¸ç¨€ç–ä»»åŠ¡æˆåŠŸå¥–åŠ±ï¼Œæˆ–åœ¨å¥–åŠ±ä¸­å¼•å…¥å…³äºåŠ¨ä½œå¹³æ»‘åº¦ã€èƒ½è€—ç­‰è¾…åŠ©ç›®æ ‡ã€‚\n\n4.  **å®ç°è·¨ä»»åŠ¡ä¸è·¨å›åˆçš„æŒç»­å­¦ä¹ **ï¼š\n    - å°† TT-VLA ä¸**æŒç»­å­¦ä¹ **æˆ–**å…ƒå­¦ä¹ **æ¡†æ¶ç»“åˆã€‚è®©æœºå™¨äººä¸ä»…åœ¨ä¸€ä¸ªå›åˆå†…é€‚åº”ï¼Œè¿˜èƒ½å°†å¤šä¸ªå›åˆã€å¤šä¸ªç›¸å…³ä»»åŠ¡ä¸­å­¦åˆ°çš„è°ƒæ•´ç­–ç•¥**æŠ½è±¡å¹¶å­˜å‚¨**ï¼Œå½¢æˆå¯è¿ç§»çš„é€‚åº”å…ˆéªŒï¼Œå®ç°çœŸæ­£çš„â€œéƒ¨ç½²å³å­¦ä¹ â€ã€‚\n\n5.  **åœ¨æ›´å¤æ‚ã€é•¿è§†é‡ä»»åŠ¡ä¸­éªŒè¯**ï¼š\n    - å½“å‰å®éªŒä¸»è¦é›†ä¸­åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„**æŠ“æ”¾æ“ä½œ**ã€‚æœªæ¥éœ€è¦åœ¨æ›´å¤æ‚çš„ã€é•¿è§†é‡çš„ã€åŒ…å«æ›´å¤šå­ä»»åŠ¡çš„åœºæ™¯ï¼ˆå¦‚è£…é…ã€å¨æˆ¿æ“ä½œï¼‰ä¸­éªŒè¯ TT-VLA çš„æœ‰æ•ˆæ€§ï¼Œç ”ç©¶å…¶åœ¨å¤§çŠ¶æ€ç©ºé—´å’Œé•¿æœŸä¿¡ç”¨åˆ†é…é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚\n\n**æ€»ç»“**ï¼šTT-VLA ä¸ºè§£å†³ VLA æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­éƒ¨ç½²çš„å…³é”®éš¾é¢˜æä¾›äº†å¼€åˆ›æ€§çš„æ€è·¯ã€‚å…¶å±€é™æ€§ä¸»è¦é›†ä¸­äºæ¶æ„æ™®é€‚æ€§ã€è®¡ç®—æ•ˆç‡ã€å®‰å…¨ä¿è¯å’ŒçŸ¥è¯†ç§¯ç´¯æ–¹é¢ã€‚æœªæ¥çš„æ”¹è¿›å°†å›´ç»•**æ‰©å±•æ¨¡å‹å…¼å®¹æ€§ã€æå‡ç®—æ³•æ•ˆç‡ä¸å®‰å…¨æ€§ã€è®¾è®¡æ›´æ™ºèƒ½çš„å¥–åŠ±æœºåˆ¶ä»¥åŠå®ç°æŒç»­å­¦ä¹ èƒ½åŠ›**å±•å¼€ï¼Œä»è€Œæ¨åŠ¨è‡ªæ”¹è¿›ã€é«˜å¯é çš„å…·èº«æ™ºèƒ½ç³»ç»Ÿèµ°å‘ç°å®ã€‚",
            "timestamp": "2026-01-21T12:15:27.104456",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-11T01:51:30Z",
    "created_at": "2026-01-21T12:09:04.492524",
    "updated_at": "2026-01-21T12:09:04.492532",
    "recommend": 0
}