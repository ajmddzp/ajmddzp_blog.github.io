{
  "id": "2512.01773v1",
  "title": "IGen: Scalable Data Generation for Robot Learning from Open-World Images",
  "authors": [
    "Chenghao Gu",
    "Haolan Kang",
    "Junchao Lin",
    "Jinghe Wang",
    "Duo Wu",
    "Shuzhao Xie",
    "Fanding Huang",
    "Junchen Ge",
    "Ziyang Gong",
    "Letian Li",
    "Hongying Zheng",
    "Changwei Lv",
    "Zhi Wang"
  ],
  "abstract": "The rise of generalist robotic policies has created an exponential demand for large-scale training data. However, on-robot data collection is labor-intensive and often limited to specific environments. In contrast, open-world images capture a vast diversity of real-world scenes that naturally align with robotic manipulation tasks, offering a promising avenue for low-cost, large-scale robot data acquisition. Despite this potential, the lack of associated robot actions hinders the practical use of open-world images for robot learning, leaving this rich visual resource largely unexploited. To bridge this gap, we propose IGen, a framework that scalably generates realistic visual observations and executable actions from open-world images. IGen first converts unstructured 2D pixels into structured 3D scene representations suitable for scene understanding and manipulation. It then leverages the reasoning capabilities of vision-language models to transform scene-specific task instructions into high-level plans and generate low-level actions as SE(3) end-effector pose sequences. From these poses, it synthesizes dynamic scene evolution and renders temporally coherent visual observations. Experiments validate the high quality of visuomotor data generated by IGen, and show that policies trained solely on IGen-synthesized data achieve performance comparable to those trained on real-world data. This highlights the potential of IGen to support scalable data generation from open-world images for generalist robotic policy training.",
  "url": "https://arxiv.org/abs/2512.01773v1",
  "html_url": "https://arxiv.org/html/2512.01773v1",
  "html_content": "\\minted@def@optcl\nenvname-P envname#1\nIGen: Scalable Data Generation for Robot Learning from Open-World Images\nChenghao Gu\n1‚àó\nHaolan Kang\n2‚àó\nJunchao Lin\n3‚àó\nJinghe Wang\n1\nDuo Wu\n1\nShuzhao Xie\n1\nFanding Huang\n1\nJunchen Ge\n1\nZiyang Gong\n4\nLetian Li\n1\nHongying Zheng\n5\nChangwei Lv\n5\nZhi Wang\n1,‚Ä†\n1\nTsinghua University\n2\nHKU\n3\nBeijing University of Chemical Technology\n4\nShanghai Jiao Tong University\n5\nShenzhen University of Infomation Technology\nhttps://chenghaogu.github.io/IGen/\nAbstract\nThe rise of generalist robotic policies has created an exponential demand for large-scale training data. However, on-robot data collection is labor-intensive and often limited to specific environments. In contrast, open-world images capture a vast diversity of real-world scenes that naturally align with robotic manipulation tasks, offering a promising avenue for low-cost, large-scale robot data acquisition. Despite this potential, the lack of associated robot actions hinders the practical use of open-world images for robot learning, leaving this rich visual resource largely unexploited. To bridge this gap, we propose\nIGen\n, a framework that scalably generates realistic visual observations and executable actions from open-world images. IGen first converts unstructured 2D pixels into structured 3D scene representations suitable for scene understanding and manipulation. It then leverages the reasoning capabilities of vision-language models to transform scene-specific task instructions into high-level plans and generate low-level actions as\nS\n‚Äã\nE\n‚Äã\n(\n3\n)\nSE(3)\nend-effector pose sequences. From these poses, it synthesizes dynamic scene evolution and renders temporally coherent visual observations. Experiments validate the high quality of visuomotor data generated by IGen, and show that policies trained solely on IGen-synthesized data achieve performance comparable to those trained on real-world data. This highlights the potential of IGen to support scalable data generation from open-world images for generalist robotic policy training.\nFigure 1\n:\nWe propose\nIGen\n, a data generation framework that converts open-world images into grounded visuomotor data, enabling scalable data synthesis for robot learning. From a single image, IGen generates large-scale realistic observations and reliable actions. The policies trained solely on IGen-generated data can effectively generalize to real-world scenes and successfully perform manipulation tasks.\n‚Ä†\n‚Ä†\n* Equal contribution.\n‚Ä†\n\\dagger\nCorresponding author.\n1\nIntroduction\nVisuomotor policy learning\n[\nchi2023diffusion\n,\nze20243d\n,\nblack2024pi_0\n,\nbjorck2025gr00t\n,\nkim2024openvla\n]\nhas shown great promise in enabling robots to perform open-world manipulation, yet it typically requires large-scale paired visual-action data for effective learning. Unfortunately, collecting such data on real robots is labor-intensive and often limited to specific environments. The high cost and environment-specific nature of robotic data collection remain a fundamental bottleneck to the generalization of visuomotor policies across diverse real-world scenarios\n[\nmirchandani2024so\n]\n.\nIn contrast to on-robot data collection, open-world images can be acquired at extremely low cost and encompass a vast diversity of scenarios that naturally align with real-world robotic tasks. Harnessing such rich visual resources offers a promising path toward building scalable, general-purpose robotic policies\n[\nzhang2024vision\n,\nlaurenccon2024matters\n]\n. Indeed, the remarkable success of large vision‚Äìlanguage models (VLMs)\n[\nachiam2023gpt\n,\nbai2025qwen2\n,\nlu2024deepseek\n]\nhas demonstrated that open-world images provide a powerful foundation for training capable, generalist perception systems. However, robotic policy learning imposes unique demands: it requires not only semantically meaningful visual inputs but also physically grounded, executable action sequences\n[\nchi2023diffusion\n,\nzhao2023learning\n,\nfu2024mobile\n]\n. This mismatch prevents the effective utilization of open-world images for robot learning.\nTo address the complementary limitations of open-world images and robot-collected data, a growing line of work seeks to derive robot-relevant action representations from unstructured visual data, yet existing approaches remain limited in their ability to harness in-the-wild images for scalable robot learning. For instance, previous\nreal-to-sim-to-real\nmethods\n[\ntorne2024robot\n,\nli2024robogsim\n,\nqureshi2025splatsim\n,\nlou2025robo\n,\njia2025discoverse\n]\nrequire explicit reconstruction of physical workspaces to build simulation environments for data generation. This reliance on scene-specific acquisition prevents them from leveraging arbitrary open-world images, fundamentally limiting their scalability. Meanwhile, recent works\n[\nbruce2024genie\n,\nyang2023learning\n,\nzhou2024robodreamer\n,\nzhang2024combo\n,\nzhen2025tesseract\n,\njang2025dreamgen\n]\nleverage vision generative models to predict future visual observations of robot actions within real-world scenes. However, due to the limitations of existing video generation models, they cannot provide explicit robot actions and struggle to generate long-horizon or instruction-driven tasks of higher complexity.\nIn this work, we introduce\nIGen\n, a novel framework that takes robotic data synthesis the extra mile by enabling scalable, diverse, and grounded visuomotor data generation from unannotated open-world images with minimal effort. It takes in-the-wild images as the sole visual input and leverages task and motion planning to automatically generate robot behaviors at scale without any human annotation. IGen further synthesizes large-scale visual observations and action trajectories that serve as high-quality training data for effective robot learning.\nTo be more specific, IGen employs the following unified pipeline to generate visuomotor data from open-world images.\nFirst,\nrather than directly operating on unstructured 2D pixels\n, IGen leverages the strong visual capabilities of large vision models (LVM) to reconstruct scenes as 3D point clouds and spatial keypoints.\nNext, it utilizes the visual understanding capabilities of vision‚Äìlanguage models (VLM) to perform high-level task planning in 3D pixel space and transform motions into low-level control functions.\nMeanwhile, to generate robot‚Äìenvironment interactions, IGen uses the end-effector\nS\n‚Äã\nE\n‚Äã\n(\n3\n)\nSE(3)\ntrajectory to perform rigid-motion‚Äìbased synthesis of the scene point cloud sequence. Finally, frame-wise rendering is applied to the entire point cloud sequence, generating action-consistent visual observations of the target manipulation task.\nWe evaluate IGen along three dimensions: (1) visual fidelity, measured by the consistency between the generated visual observations and real-world perception;\n(2) action quality, quantified by the scores of instruction following and physics alignment of the generated action videos; and (3) policy transferability, assessed through the performance of visuomotor policies trained on generated data and evaluated on real-world tasks.\nExperiments reveal that IGen generates visually realistic images and physically reliable robotic behaviors that align with specific task instructions, providing high-quality data for effective robot learning. Notably, results demonstrate that policies trained solely on IGen-generated data‚Äìwithout any human-annotated demonstrations‚Äìcan outperform those trained on real-world trajectories.\nIn summary, our contributions are as follows:\n‚Ä¢\nWe propose an effective data generation framework that produces scalable visual-action datasets from open-world images, integrating cross-scene generalization, instruction diversity and long-horizon task applicability.\n‚Ä¢\nWe transform unstructured open-world images into actionable 3D scene representations that enable robotic task reasoning and motion planning, generating task-consistent behaviors aligned with the physical world. Furthermore, we introduce a simulation-free point cloud synthesis approach that produces realistic visual observations and supports large-scale robot experience generation.\n‚Ä¢\nWe demonstrate that robotic policies trained on IGen-generated data can successfully perform real-world manipulation tasks without any additional data collection.\nThis suggests the promising potential of IGen to establish real-world images as an effective source for robot policy training, paving the way for scalable, annotation-free robot learning.\n2\nRelated Work\n2.1\nVisuomotor Robot Learning\nRecent progress in visuomotor policy learning\n[\nchi2023diffusion\n,\nze20243d\n,\nzhao2023learning\n,\nfu2024mobile\n,\nzheng2022extraneousness\n,\nhansen2022pre\n]\nenables robots to perform real-world manipulation tasks through end-to-end imitation learning from paired visual and action demonstrations. Inspired by the development of large-scale vision‚Äìlanguage models (VLM)\n[\nachiam2023gpt\n,\nbai2025qwen2\n,\nlu2024deepseek\n]\n, recent efforts have focused on scaling up model size and data volume to build more generalist robotic models\n[\nteam2024octo\n,\nhuang2025otter\n,\nblack2024pi_0\n,\nintelligence2025pi_\n,\nbjorck2025gr00t\n,\nkim2024openvla\n,\nteam2025gemini\n,\nliu2025hybridvla\n,\nchen2025fast\n]\n. Meanwhile, many studies\n[\nhuang2023voxposer\n,\nhuang2024rekep\n,\npan2025omnimanip\n]\nleverage the planning capabilities of VLMs to enable open-world robotic manipulation. Existing datasets\n[\nteed2021droid\n,\no2024open\n,\nebert2021bridge\n,\nfang2023rh20t\n]\nhave collected large-scale demonstration data across diverse real-world scenes and task settings. Meanwhile, recent works\n[\nchi2024universal\n,\nzeng2025activeumi\n,\ncheng2024open\n,\nding2024bunny\n,\ngao2024efficient\n,\nmandlekar2023mimicgen\n,\njiang2025dexmimicgen\n,\nxue2025demogen\n,\nzhang2025doglove\n]\nfocus on improving the efficiency and scalability of robotic data collection.\nIn contrast to vision‚Äìlanguage models that benefit from abundant web-scale data, robotic learning lacks equivalent sources of large-scale, open-world demonstrations, posing a major bottleneck for developing generalist robot models\n[\nmirchandani2024so\n]\n. Open-world internet images provide a vast and diverse source of scenes for improving the generalization of robot learning. The core objective of our work is to enable scalable generation of robot-relevant data from such unstructured visual sources.\nFigure 2\n:\nOverview of IGen.\nGiven an open-world image and a task description, IGen first reconstructs the environment and objects as point clouds via Foundation Vision Models. After spatial keypoint extraction, VLM maps the task description to high-level plans and low-level control commands. During the robot‚Äôs execution in simulation, a virtual depth camera captures the motion point cloud sequences. The resulting end-effector pose trajectory is used to synthesize dynamic point-cloud sequences, which are then rendered frame-by-frame into visual observations of the manipulation. The final output consists of the generated robot actions and the visual observations.\n2.2\nReal-Sim-Real Data Generation\nThe Real-Sim-Real approach leverages real-world visual observations to build simulation workspaces for scalable policy learning and real-world transfer. Many recent works\n[\ndai2024automated\n,\nzook2025grs\n,\nye2025video2policy\n,\npatel2025real\n,\njiang2022ditto\n,\nwang2023gensim\n,\nli2024evaluating\n]\nemploy digital twin or digital cousin approaches to recreate objects and spatial layouts for building simulation environments for robot training. However, such methods often struggle to capture fine-grained scene details for realistic world modeling. Meanwhile, some approaches\n[\ntorne2024robot\n,\nduan2023ar2\n,\nli2024robogsim\n,\nqureshi2025splatsim\n,\nlou2025robo\n,\njia2025discoverse\n]\nleverage high-fidelity 3D reconstruction techniques to model real-world scenes from multi-view observations, which are instantiated as executable robot workspaces within simulation environments. These methods remain confined to task-specific environments and require extensive visual data collection for scene reconstruction.\nOpen-source internet images provide a rich and scalable data source that can cover most real-world scenarios. To leverage this advantage, RoLA\n[\nzhao2025robot\n]\nproposes an open-world, image-based framework for robot learning data generation. However, it relies on accurate physical property estimation for scene modeling and remains limited in modeling complex interactive manipulation tasks. IGen, on the other hand, leverages images as the primary data source without explicit physics-based reconstruction, offering a lightweight alternative to simulation-based pipelines.\n2.3\nExperience Synthesis from Unstructured Data\nInspired by the success of large-scale visual models, there is growing interest in leveraging vast amounts of unstructured internet data‚Äîsuch as images and videos‚Äîto provide scalable training sources for robot learning. Recent approaches\n[\nsingh2025hand\n,\nchen2024object\n,\nchen2025vidbot\n,\nlepert2025phantom\n,\nyuan2025hermes\n,\nzhu2024densematcher\n]\nsynthesize robot manipulation data from collections of human demonstration videos from the Internet or real-world recordings. However, such data are limited in scene diversity and inherently suffer from human bias. Many works\n[\nyang2023learning\n,\nko2023learning\n,\nzhen2025tesseract\n,\nzhou2024robodreamer\n,\njang2025dreamgen\n]\nbuild upon large-scale pre-trained video generation models to create visual observations for robotic manipulation. Yet, these generative methods often lack grounded robotic actions, struggle with complex and long-horizon tasks, and incur substantial computational costs. Images are the most abundant source of data on the internet, yet they inherently lack robot-relevant information. To this end, IGen aims to transform unstructured image data into grounded robotic experiences, providing a scalable source of data for robot learning.\n3\nMethodology\nIGen is a robotic data generation framework that synthesizes task-specific robot actions and visual observations from open-world images. The architecture consists of three main stages: (1)\nScene Reconstruction\n, converting the input image into a manipulable workspace for the robot; (2)\nAction Planning\n, reasoning over spatial keypoints to generate action trajectories; and (3)\nObservation Synthesis\n, composing and rendering point-cloud sequences to generate visual observations of the task.\n3.1\nFrom Pixels to Structured 3D Representations\nOpen-world images are unstructured and lack explicit robot-related actions. Hence, the key to enabling robotic learning from images lies in transforming raw pixels into structured representations that robots can effectively interpret and take actions upon. To this end, we adopt 3D point clouds as the modality, which offers a format better suited to visual observation and structured editing.\nTo begin with, we employ a versatile monocular geometric foundation model\n[\nhu2024metric3d\n]\nto estimate the depth and geometric structure of the scene. Then, we employ Segment Anything Model\n[\nkirillov2023segment\n]\nto obtain masks\n‚Ñ≥\n\\mathcal{M}\nof task-relevant objects. Following the keypoints representation in\n[\nhuang2024rekep\n]\n, we extract scene features using DINOv2\n[\noquab2023dinov2\n]\n.\nNext, we apply K-means clustering to both the feature embeddings and 3D coordinates, resulting in a set of\nK\nK\nspatial keypoints, denoted as\nùí¶\n=\n{\nk\nj\n‚àà\n‚Ñù\n3\n‚à£\nj\n=\n1\n,\n2\n,\n‚Ä¶\n,\nK\n}\n\\mathcal{K}=\\{\\,k_{j}\\in\\mathbb{R}^{3}\\ \\mid j=1,2,\\dots,K\\}\n, with their associated 3D coordinates.\nThen, the inpainting model\n[\nyu2023inpaint\n]\nis applied to the original image to remove the target manipulable object and reconstruct the background image\nùêà\nbg\n‚àà\n‚Ñù\nH\n√ó\nW\n√ó\n3\n\\mathbf{I}_{\\mathrm{bg}}\\in\\mathbb{R}^{H\\times W\\times 3}\n. Given the estimated depth map\nùêÉ\n‚àà\n‚Ñù\nH\n√ó\nW\n\\mathbf{D}\\in\\mathbb{R}^{H\\times W}\nand the camera intrinsic matrix\nùêä\n\\mathbf{K}\n, all pixels are lifted into 3D space as a dense 3D point cloud\nP\nbg\n‚àà\n‚Ñù\nH\n√ó\nW\n√ó\n6\nP_{\\mathrm{bg}}\\in\\mathbb{R}^{H\\times W\\times 6}\n. To address incomplete modeling of manipulated objects from monocular views, we employ a 3D generative reconstruction model\n[\nxiang2025structured\n]\nto their full 3D shape and appearance. After reconstructing the objects into dense point clouds, we re-position the completed objects to their original pose using 6D pose estimation\n[\nlee2025any6d\n]\n. Furthermore, to enable spatial domain randomization, we extract a set of feasible placement points from the supporting surface (e.g., the spatial points of the tabletop), which serve as candidate spatial poses for object generalization. Details of the reconstruction are provided in the appendix.\nFigure 3\n:\nQualitative comparison of robotic behavior generation using IGen.\nGiven a single captured image and a natural-language manipulation instruction, TesserAct\n[\nzhen2025tesseract\n]\n, Cosmos\n[\nagarwal2025cosmos\n]\n, and our IGen generate behavior observations. IGen produces more instruction-consistent and physically coherent object motions, closely matching the intended tasks. The green box represents action observations that adhere to physical laws and follow the task instructions, and the checkmark indicates task completion.\n3.2\nSpatial Planning for Behavior Generation\nSince open-world images lack action-related guidance for robots, we leverage the strong visual reasoning and planning capabilities of VLMs\n[\nachiam2023gpt\n,\nbai2025qwen2\n]\nto guide robotic behavior generation. We provide the image annotated with keypoints\nùí¶\n\\mathcal{K}\n, their 3D coordinates, and the task instruction as inputs to the VLM. The VLM decomposes the overall task into a set of sub-stages\nùíÆ\n=\n{\nS\ni\n}\ni\n=\n1\nN\n\\mathcal{S}=\\{S_{i}\\}_{i=1}^{N}\nthrough high-level task planning. For each sub-stage, the VLM model generates an action description associated with the keypoints.\nTo enable robotic action execution, we develop an easily programmable control language in Python based on the end-effector‚Äôs\nS\n‚Äã\nE\n‚Äã\n(\n3\n)\nSE(3)\npose, translating high-level task stages\nùíÆ\n\\mathcal{S}\ninto executable low-level control functions\n‚Ñ±\n=\n{\nf\ni\n}\ni\n=\n1\nN\n\\mathcal{F}=\\{f_{i}\\}_{i=1}^{N}\nwith VLM. For each task stage\nS\ni\nS_{i}\n, the function\nf\ni\nf_{i}\nis defined as a keypoint-conditioned solver that computes the reference end-effector pose from the spatial anchors in\nùí¶\ni\n\\mathcal{K}_{i}\n. During the pre-manipulation stage, the end-effector interacts precisely with the object based on a grasping prior model, where the end-effector pose is predicted by the grasp model\n[\nfang2020graspnet\n,\nmurali2025graspgen\n]\n, and the gripper state is determined by the point cloud width along the gripper‚Äôs principal axis. During manipulation, keypoints on the manipulated object are treated as movable points rigidly attached to the end-effector, while other keypoints serve as static scene anchors. The specific prompts used are detailed in the appendix.\nStarting from the initial robot state\nx\n0\nx_{0}\n, the motion planner produces a set of sub-goals\nùí≥\n=\n{\nx\n^\ni\n}\ni\n=\n1\nN\n\\mathcal{X}=\\{\\hat{x}_{i}\\}_{i=1}^{N}\nthat define the desired states for subsequent control execution. Each stage computes the next state as\nx\n^\ni\n=\nf\ni\n‚Äã\n(\nx\n^\ni\n‚àí\n1\n)\n\\hat{x}_{i}=f_{i}(\\hat{x}_{i-1})\n, where\nx\n^\n0\n=\nx\n0\n\\hat{x}_{0}=x_{0}\n. We use a motion planner to generate feasible trajectories, which are then executed in a simulation environment, producing an action sequence\nùíú\n=\n{\na\nt\n}\nt\n=\n1\nT\n\\mathcal{A}=\\{{a_{t}}\\}_{t=1}^{T}\nover\nT\nT\ntime steps, sampled at a fixed frame rate. Each action\na\nt\na_{t}\nincludes the robot‚Äôs end-effector pose and joint positions. Detailed simulation settings are provided in the appendix.\nTask\nPSNR\n‚Üë\n\\uparrow\nSSIM\n‚Üë\n\\uparrow\nFID\n‚Üì\n\\downarrow\nLPIPS\n1\n‚Üì\n\\downarrow\nLPIPS\n2\n‚Üì\n\\downarrow\nLPIPS\n3\n‚Üì\n\\downarrow\nReal-to-Sim\nIGen\nReal-to-Sim\nIGen\nReal-to-Sim\nIGen\nReal-to-Sim\nIGen\nReal-to-Sim\nIGen\nReal-to-Sim\nIGen\nPick Carrot on Plate\n18.1480\n\\cellcolor\n[HTML]F2F7F2\n28.2611\n0.6756\n\\cellcolor\n[HTML]F2F7F2\n0.8371\n0.0001\n\\cellcolor\n[HTML]F2F7F2\n0.0001\n0.2864\n\\cellcolor\n[HTML]F2F7F2\n0.0518\n0.3882\n\\cellcolor\n[HTML]F2F7F2\n0.1166\n0.2057\n\\cellcolor\n[HTML]F2F7F2\n0.0418\nPut Eggplant in Basket\n16.6910\n\\cellcolor\n[HTML]F2F7F2\n23.2821\n0.7706\n\\cellcolor\n[HTML]F2F7F2\n0.8350\n0.0001\n0.0009\n0.2458\n\\cellcolor\n[HTML]F2F7F2\n0.0825\n0.2658\n\\cellcolor\n[HTML]F2F7F2\n0.1242\n0.1682\n\\cellcolor\n[HTML]F2F7F2\n0.0547\nPut Spoon on Towel\n15.9262\n\\cellcolor\n[HTML]F2F7F2\n26.7524\n0.6041\n\\cellcolor\n[HTML]F2F7F2\n0.8621\n0.0061\n\\cellcolor\n[HTML]F2F7F2\n0.0008\n0.4275\n\\cellcolor\n[HTML]F2F7F2\n0.0621\n0.4879\n\\cellcolor\n[HTML]F2F7F2\n0.1129\n0.2980\n\\cellcolor\n[HTML]F2F7F2\n0.0433\nStack Cubes\n18.2944\n\\cellcolor\n[HTML]F2F7F2\n29.7206\n0.6828\n\\cellcolor\n[HTML]F2F7F2\n0.8747\n0.0030\n\\cellcolor\n[HTML]F2F7F2\n0.0003\n0.3345\n\\cellcolor\n[HTML]F2F7F2\n0.0558\n0.4419\n\\cellcolor\n[HTML]F2F7F2\n0.1021\n0.2491\n\\cellcolor\n[HTML]F2F7F2\n0.0386\nAverage\n17.2649\n\\cellcolor\n[HTML]E2ECE2\n27.0040\n0.6833\n\\cellcolor\n[HTML]E2ECE2\n0.8522\n0.0023\n\\cellcolor\n[HTML]E2ECE2\n0.0005\n0.3235\n\\cellcolor\n[HTML]E2ECE2\n0.0630\n0.3959\n\\cellcolor\n[HTML]E2ECE2\n0.1139\n0.2302\n\\cellcolor\n[HTML]E2ECE2\n0.0446\nTable 1\n:\nWe compare the visual similarity between the digital-twin scenes reconstructured by Real-to-Sim and those generated by IGen. Real-to-Sim refers to the method in Simpler\n[\nli2024evaluating\n]\nthat converts real-world scenes into simulated digital-twin scenes. We compute the LPIPS\n[\nwigner1990unreasonable\n]\nvariants using AlexNet\n[\nkrizhevsky2012imagenet\n]\n, VGGNet\n[\nsimonyan2014very\n]\n, and SqueezeNet\n[\niandola2016squeezenet\n]\n, denoted as LPIPS\n1\n, LPIPS\n2\n, and LPIPS\n3\n.\n‚Üë\n\\uparrow\n/\n‚Üì\n\\downarrow\nindicates higher‚Äâ/‚Äâlower is better.\nFigure 4\n:\nQuantitative Comparison of robotic behavior generated by IGen.\nPerformance is assessed on DreamGen Bench\n[\njang2025dreamgen\n]\nunder two criteria:\nInstruction Following\nand\nPhysics Alignment\n. Evaluations are conducted using GPT-4o\n[\nachiam2023gpt\n]\n, Qwen-3-VL-Plus\n[\nbai2025qwen2\n]\nand GLM-4.5V\n[\nzeng2025glm\n]\nas video assessment models. Each method generates 40 videos along with the prompts, and the reported metric represents the proportion of videos receiving a score of 1 from the evaluator.\nFigure 5\n:\nEvaluation of IGen‚Äôs computational efficiency.\nWe compare the video generation time and GPU memory consumption of IGen and baselines under identical input images and task instructions. The average computation time refers to the time required to generate one robot behavior video.\n3.3\nExperience Synthesis for Robot Learning\nTo obtain visual observations synchronized with embodied actions,\nwe propose a robotic experience synthesis framework based on real-time point cloud rendering.\nA complete point cloud observation of a manipulation task comprises the action point cloud sequence of the robot\nand the dynamic point cloud sequence of the scene.\nTo synthesize action point clouds, the simulated robot is placed at the planned pose\np\nrobot\np_{\\text{robot}}\nin simulation,\nand a virtual camera is positioned at\np\ncam\np_{\\text{cam}}\n,\naligned with the viewpoint used for generating the scene point clouds. From the action sequence\nùíú\n\\mathcal{A}\n, we obtain an end-effector pose trajectory\nùíØ\n=\n{\nùêì\nt\n}\nt\n=\n1\nT\n\\mathcal{T}=\\{\\mathbf{T}_{t}\\}_{t=1}^{T}\n, where\nùêì\nt\n‚àà\nS\n‚Äã\nE\n‚Äã\n(\n3\n)\n\\mathbf{T}_{t}\\in SE(3)\ndenotes the 6-DoF pose of the end-effector at time step\nt\nt\n,\nincluding both rotation and translation.\nAt each time step\nt\nt\n, the environment is rendered to produce synchronized RGB and depth frames, which are then back-projected through the virtual camera\nùêÇ\n\\mathbf{C}\nto construct the point cloud sequence of the robot‚Äôs motion\nùí´\nrobot\n=\n{\nP\nrobot\n,\nt\n}\nt\n=\n1\nT\n\\mathcal{P}_{\\text{robot}}=\\{P_{\\text{robot},t}\\}_{t=1}^{T}\n. Meanwhile, the background is modeled as a static point cloud sequence\nùí´\nbg\n\\mathcal{P}_{\\text{bg}}\n.\nWe perform dynamic interaction between the end-effector and the point clouds based on transformations of the end-effector‚Äôs pose. Assume the grasp is established at time\nt\ng\nt_{g}\n, with the object‚Äôs pose\nùêì\nobj\n,\nt\ng\n\\mathbf{T}_{\\text{obj},t_{g}}\nand the end-effector pose\nùêì\nt\ng\n\\mathbf{T}_{t_{g}}\n. For all time steps\nt\n‚àà\nùíØ\ngrasp\nt\\in\\mathcal{T}_{\\text{grasp}}\n, where\nùíØ\ngrasp\n\\mathcal{T}_{\\text{grasp}}\ndenotes the set of time indices during which the gripper remains closed on the object,\nthe world pose the object evolves by rigidly following the end-effector. The manipulated object, represented as a point cloud sequence\nùí´\nobj\n=\n{\nP\nobj\n,\nt\n}\nt\n=\n1\nT\n\\mathcal{P}_{\\text{obj}}=\\{P_{\\text{obj},t}\\}_{t=1}^{T}\n, undergoes rigid-body transformations induced by the end-effector poses. The transformation at time\nt\nt\ncan be expressed as:\nP\nobj\n,\nt\n=\n{\nP\nobj\n,\nt\nt\n‚àâ\nùíØ\ngrasp\n,\nùêì\nt\n‚Äã\n(\nùêì\nt\ng\n)\n‚àí\n1\n‚Äã\nùêì\nobj\n,\nt\ng\n‚Äã\nP\nobj\n,\nt\ng\nt\n‚àà\nùíØ\ngrasp\n.\nP_{\\text{obj},t}=\\begin{cases}P_{\\text{obj},t}&t\\notin\\mathcal{T}_{\\text{grasp}},\\\\[6.0pt]\n\\mathbf{T}_{t}\\,(\\mathbf{T}_{t_{g}})^{-1}\\,\\mathbf{T}_{\\text{obj},t_{g}}\\,P_{\\text{obj},t_{g}}&t\\in\\mathcal{T}_{\\text{grasp}}~.\\end{cases}\n(1)\nBy combining the static environment, the robot, and the manipulated object, we represent the complete task as a composite point cloud sequence:\nùí´\ntask\n=\nùí´\nbg\n‚à™\nùí´\nobj\n‚à™\nùí´\nrobot\n.\n\\mathcal{P}_{\\text{task}}=\\mathcal{P}_{\\text{bg}}\\cup\\mathcal{P}_{\\text{obj}}\\cup\\mathcal{P}_{\\text{robot}}.\n(2)\nBy rendering the point cloud sequence through the virtual camera\nùêÇ\n\\mathbf{C}\n,\nwe obtain the visual observations\nùí™\n\\mathcal{O}\n.\nThe temporally synchronized observations, together with the action sequence\nùíú\n\\mathcal{A}\n,\nconstitute the paired visual‚Äìaction data for robot learning.\n4\nExperiment\nIn this section, we aim to address the following three research questions:\n(1) Can\nIGen\ngenerate visually realistic data from real-world images across various scenes?\n(2) Can\nIGen\nefficiently generate robot actions that align with the environment and follow task instructions?\n(3) Can\nIGen\nsynthesize effective robotic training data directly from a single image, enabling policy training and real-world deployment without any human-teleoperated demonstrations?\nFigure 6\n:\nExperimental setup.\nStarting from a captured real-world scene image, IGen automatically generates 1,000 task demonstrations with spatial randomization. The resulting data are used to train a visuomotor policy, which is later deployed and evaluated in the real world.\n4.1\nScene Reconstruction Fidelity\nIn this section, we compare IGen with Real-to-Sim methods to measure the visual realism of the synthesized scenes. We conduct experiments on the Simpler dataset\n[\nli2024evaluating\n]\n, which provides diverse real-world robotic manipulation scenes. Based on the real-world captured images in Simpler, we employ IGen to reconstruct the corresponding 3D scenes for evaluation. Meanwhile, the Real-to-Sim method in Simpler reconstructs digital-twin scenes from real-world images.\nWe adopt multiple visual metrics, including PSNR, SSIM, FID, and LPIPS\n[\nwigner1990unreasonable\n]\n, to assess the visual consistency between the original real-world scenes and those reconstructed by the Real-to-Sim method in Simpler or IGen.\nAs shown in Tab.\n1\n, IGen achieves superior performance across multiple evaluation metrics. For instance, using LPIPS scores computed across multiple perception models to assess the perceptual discrepancy between generated scenes and the original images, we observe that IGen achieves up to a\n5.13\n√ó\n\\times\nimprovement in similarity compared with the Real-to-Sim baseline. These results indicate that the scenes generated by IGen remain highly consistent with the corresponding real-world images, demonstrating its capability to produce visually faithful and realistic observations that align closely with real environments, thus mitigating the sim-to-real gap.\nFigure 7\n:\nReal-world robot evaluation results.\nWe assess policy performance on real-world tasks, comparing task success rates under four different conditions: zero-shot, fine-tuned with 10 human teleoperation data, fine-tuned with 100 human teleoperation data, and fine-tuned with 1,000 IGen-synthesized data.\nFigure 8\n:\nEffectiveness of IGen for Robot Learning.\nWe demonstrate that policies trained on IGen-synthesized data outperform those trained on limited on-robot data. The blue trajectory represents actions that correctly follow the task instructions, while the red trajectory indicates deviations from the target action.\n4.2\nEvaluation of Behavior Generation\nThis section focuses on evaluating the behavioral accuracy and instruction alignment of the videos of robotic behaviors generated with IGen. We evaluate IGen across diverse visual sources and task instructions to assess the quality of robotic behaviors generated from a single image. For comparison, we include Cosmos-Predict2\n[\nagarwal2025cosmos\n,\njang2025dreamgen\n]\nand TesserAct\n[\nzhen2025tesseract\n]\nas baselines, both of which are capable of generating robot behaviors from a single image input. Cosmos-Predict2 employs the Cosmos-Predict2-2B-Video2World model, while TesserAct uses the video generation model fine-tuned from CogVideoX-5B-I2V\n[\nyang2024cogvideox\n]\n.\nQualitative Experiments.\nAs shown in Fig.\n3\n, we evaluate on unlabeled, open-world images without any task annotations or demonstrations. Each image is associated with a scene-specific instruction, which serves as a high-level command for IGen and as the language prompt for Cosmos-Predict2 and TesserAct. The results demonstrate that IGen can generate complete and coherent robotic behaviors that accurately follow the given task instructions, producing physically consistent motions and visual observations that align with real-world physics. In contrast, the baseline methods exhibit temporal discontinuities, geometric distortions, and inaccurate motion execution, failing to produce instruction-aligned behaviors.\nQuantitative Experiments.\nTo comprehensively evaluate the quality of robotic behaviors generated by IGen, we conduct experiments on the DreamGen Bench\n[\njang2025dreamgen\n]\n. Two criteria‚Äî\nInstruction Following\nand\nPhysics Alignment\n‚Äîare used to assess the quality of the generated robot-manipulation videos. We employ GPT-4o\n[\nachiam2023gpt\n]\n, Qwen-3-VL-Plus\n[\nbai2025qwen2\n]\n, and GLM-4.5V\n[\nzeng2025glm\n]\nas evaluation models, ensuring fair and consistent comparison under identical video resolution and frame-rate settings. As shown in Fig.\n5\n, IGen outperforms all compared models in both\nInstruction Following\nand\nPhysics Alignment\n. As shown in the results, IGen produces nearly\ntwice\nas many\nInstruction Following\nsuccesses as the baseline when evaluated by Qwen-3-VL-Plus. Furthermore, under the\nPhysics Alignment\nmetric, GLM-4.5V judges\n100%\nof IGen-generated videos to exhibit physically consistent dynamics.\nThese results indicate that IGen provides high-quality visual observations and physically accurate action sequences, which can effectively support robot learning in real-world environments.\nComputational Efficiency.\nFor large-scale model training, generating data at scale requires high computational efficiency. In this section, we evaluate IGen‚Äôs computational efficiency, focusing on the runtime performance and resource consumption. For open-world image inputs, we use IGen to generate 1,000 randomized samples per image and report the average per-sample generation time and GPU memory utilization. For comparison, we compute the average per-sample video generation time and GPU memory utilization for Cosmos-Predict2 and TesserAct. As illustrated in Fig.\n5\n, IGen demonstrates higher efficiency in data generation, requiring only 8.3 GB of GPU memory and approximately 18.6 seconds per sample. Under the same GPU memory conditions, IGen achieves data generation with approximately\n30\n√ó\n\\times\nand\n200\n√ó\n\\times\nhigher efficiency than TesserAct and Cosmos-Predict2, respectively.\nThese results highlight that IGen is significantly more computationally efficient and scalable for large-scale robotic data synthesis.\n4.3\nRobot Learning from Unstructured Images\nWe provide IGen with a single image and, without any on-robot data, automatically generate visuomotor data to train the robot model for real-world manipulation tasks.\nHardware Setup.\nAll real-world experiments are conducted on a Franka Research 3 robot arm. Perception is provided by a single Microsoft Kinect RGB-D camera mounted in front of the manipulator, from which we use only the RGB stream as visual input.\nRobot Policy.\nWe adopt\nœÄ\n0\n\\pi_{0}\n[\nblack2024pi_0\n]\nas the base vision-language-action model. We first verify that, without any fine-tuning,\nœÄ\n0\n\\pi_{0}\nachieves an almost zero success rate across our tasks, indicating limited zero-shot transfer capability in our experimental setting. Subsequently, the model is fine-tuned separately on teleoperation data and on IGen-generated data with LoRA\n[\nhu2022lora\n]\n, and evaluated on the same set of manipulation tasks for comparison.\nTasks and Evaluation.\nWe design diverse manipulation tasks to evaluate the effectiveness of IGen, covering two types: Placement (\n‚ÄúPlace the bottle‚Äù\n), Scene-Object Interaction (\n‚ÄúWater the flowers‚Äù\n). We assess the effect of data scaling by training the model with (a) 10 real-robot samples, (b) 100 real-robot samples, and (c) 1,000 IGen-generated samples, and evaluating them under the same conditions.\nData Generation.\nStarting from a single camera-view image, IGen applies spatial randomization to generate training data for real-world scenes. A manipulable tabletop workspace is designated within the robot‚Äôs reachable area, and 3D points within this region are randomly sampled from the scene point cloud as object placement positions. As shown in Fig.\n6\n, we generate 1,000 data samples with spatial randomization for each task.\nPerformance Analysis.\nAs shown in Fig.\n7\n, when directly deploying the\nœÄ\n0\n\\pi_{0}\nmodel without any fine-tuning samples, the success rate remains close to zero. With just a single image, IGen automatically generates 1,000 synthetic data samples for policy fine-tuning. On\nPlace Bottle\ntask, the fine-tuned model achieves a significant increase in task success rate, increasing\nfrom 0.0% to 75.0%\n.\nNotably, the policy trained on 1,000 IGen-generated data,\nwithout any on-robot data fine-tuning\n, surpasses the success rates of 8.3% with 10 on-robot data and 66.7% with 100 on-robot data. As shown in Fig.\n8\n, the policy fine-tuned on a small amount of real data exhibits deviations in action trajectory execution. In contrast, the policy fine-tuned on a large scale of IGen-generated data demonstrates more robust performance in executing task trajectories. The result suggest that IGen can serve as an effective and scalable alternative to human teleoperation for training robot policies. These findings highlight the potential to enable robots to learn from a wide range of open-world images, further enhancing its applicability for diverse real-world tasks. More details are provided in the appendix.\n5\nConclusion\nIn this work, we propose a novel data generation framework that transforms open-world images into high-quality visuomotor data for robot learning. Leveraging only unstructured open-world images, our approach enables the scalable generation of training data, eliminating the need for human effort while effectively improving robot policy performance. This work has the potential to alleviate the challenge of limited real-world robot data, offering a promising data-driven solution for the development of generalist robot policies.\nLimitations.\nOur method relies on the capabilities of foundational vision models, and its performance may be limited in images with visual defects. We look forward to stronger foundational models bringing improvements in the future.\nAppendix\nAppendix A\nSingle-Image Scene Reconstruction Details\nIn this section, we describe how IGen reconstructs the 3D scene from a single open-world RGB image to facilitate robot data generation, as illustrated in Fig.\n9\n.\nWe use Metric3Dv2\n[\nhu2024metric3d\n]\nto estimate the depth and convert image pixels into a point cloud.\nFor open-world images, the focal length is fixed to\n1000\n1000\n, while for specific camera types (e.g., iPhone or Microsoft Kinect Camera), we adopt their corresponding intrinsic parameters.\nThe resulting point cloud preserves the same spatial resolution and dimensions as the original RGB image.\nFor object-level reconstruction, we utilize the TRELLIS model\n[\nxiang2025structured\n]\nto perform monocular 3D reconstruction and convert the outputs into colored point clouds.\nThe input images to TRELLIS are pre-processed using segmentation masks obtained from SAM\n[\nkirillov2023segment\n]\n, ensuring that the reconstruction focuses on the target objects.\nFigure 9\n:\nSingle-Image Scene Reconstruction Pipeline.\nAppendix B\nSimulation Environment Details\nThis section describes the details of building the robotic manipulation platform in simulation. We adopt Isaac Sim as the simulation environment and deploy both the Franka Emika Panda and Franka Research 3 robotic arms within it. For motion planning, we utilize Curobo as the solver, which computes feasible trajectories given the target end-effector poses. We use the default illumination settings in the simulation environment.\nAs shown in Fig.\n10\n, we place a virtual depth camera at the origin\n[\n0\n,\n0\n,\n0\n]\n\\left[0,0,0\\right]\nof the simulation scene, oriented relative to the robot‚Äôs base frame.\nThe camera‚Äôs focal length is set to match that used in the depth estimation module.\nThe robotic arm is positioned at a predefined spatial coordinate\n[\nx\nr\n,\ny\nr\n,\nz\nr\n]\n\\left[x_{r},y_{r},z_{r}\\right]\nwithin the reconstructed point cloud space.\nDuring robot motion, the camera operates at a sampling rate of 30¬†fps, capturing synchronized RGB and depth frames for subsequent reconstruction of the robot‚Äôs dynamic point cloud sequences.\nFigure 10\n:\nRobot and Camera Placement in Simulation.\nIn simulation platforms such as IsaacSim, the virtual camera is placed at the position (0, 0, 0), while the robotic arm base is positioned at the corresponding point in the point cloud, denoted as\n(\nx\nr\n,\ny\nr\n,\nz\nr\n)\n(x_{r},y_{r},z_{r})\n. RGB and depth data are collected during the robotic arm‚Äôs motion.\nAppendix C\nManipulation Synthesis Details\nWe divide the point cloud sequence into three components: the background point cloud, the robot point cloud, and the object point cloud. Among them, the robot and object point clouds are dynamic, while the background point cloud remains static. We use GraspGen\n[\nmurali2025graspgen\n]\nfor grasp pose estimation.\nThe grasp width is inferred from the inter-point distance along the principal axis of the reconstructed object point cloud. At the grasping moment\nt\ng\nt_{g}\n, the end-effector pose is denoted as\nùêì\nt\ng\n\\mathbf{T}_{t_{g}}\nand the object pose as\nùêì\nobj\n,\nt\ng\n\\mathbf{T}_{\\text{obj},t_{g}}\n.\nDuring the subsequent manipulation at time\nt\nt\n, given the current end-effector pose\nùêì\nt\n\\mathbf{T}_{t}\n,\nthe object pose in the scene can be computed through rigid-body transformation as:\nùêì\nobj\n,\nt\n=\nùêì\nt\n‚Äã\nùêì\nt\ng\n‚àí\n1\n‚Äã\nùêì\nobj\n,\nt\ng\n.\n\\mathbf{T}_{\\text{obj},t}=\\mathbf{T}_{t}\\,\\mathbf{T}_{t_{g}}^{-1}\\,\\mathbf{T}_{\\text{obj},t_{g}}.\n(3)\nFigure 11\n:\nPoint Cloud Synthesis during Manipulation.\nAt time\nt\ng\nt_{g}\n, the object is grasped. The gripper width is calculated based on the point cloud, and the transformation of the object point cloud at time\nt\nt\nis computed according to the end-effector‚Äôs pose.\nAppendix D\nReal-World Experiment Details\nHardware Setup.\nAs shown in Fig.\n12\n, we set up a real-world evaluation environment using the Franka Research 3 robotic arm. A Microsoft Kinect camera is placed in front of the robot to provide RGB visual input. The robotic arm performs task operations on the tabletop.\nFigure 12\n:\nHardware Setup.\nOur experimental setup consists of a Franka Research 3 robotic arm, a tabletop workspace, and a global RGB camera.\nSpatial Randomization.\nFor real-world data collection, we sample random object positions within a\n40\n‚Äã\ncm\n√ó\n30\n‚Äã\ncm\n40~\\text{cm}\\times 30~\\text{cm}\ntabletop grid. In IGen, spatial randomization is performed based on the point cloud of the placement area (e.g., the tabletop surface). We define a\n200\n√ó\n150\n200\\times 150\npixel grid as the sampling region for randomization, ensuring that the spatial distribution closely matches that of the real-world setup. Regarding the spatial randomization of real-world data and IGen-generated data, see Fig.\n13\nand\n15\n.\nTask Evaluation.\nWe design diverse manipulation tasks involving complex interactions between objects and the surrounding scene.\nFor each task, we conduct 12 independent trials. Object initial positions are sampled on a\n30\n‚Äã\ncm\n√ó\n25\n‚Äã\ncm\n30~\\text{cm}\\times 25~\\text{cm}\ntabletop grid, with a spacing of\n7\n‚Äã\ncm\n7~\\text{cm}\nbetween adjacent positions. All models are evaluated using the same set of initial object positions.\nPolicy Learning.\nThis section describes the fine-tuning process of policy. We fine-tune\nœÄ\n0\n{\\pi_{0}}\n-base\n[\nblack2024pi_0\n]\nfor 30k training steps using LoRA\n[\nhu2022lora\n]\nwith a batch size of 8. The model takes as input a single\n224\n√ó\n224\n224\\times 224\nRGB image and the absolute joint positions as the state, and predicts a 10-step relative joint angle action chunk. Training is conducted on a single NVIDIA A40 GPU, requiring approximately 10.8 hours per training. The performance of the model in real-world deployment is shown in Fig.\n16\nand\n17\n.\nFigure 13\n:\nSpatial randomization of real-world data and IGen-generated data. The task is\nGrab the watering can and water the flowers.\nFigure 14\n:\nSpatial randomization of real-world data and IGen-generated data. The task is\nPick up the bottle and place it into the basket.\nFigure 15\n:\nSpatial randomization of real-world data and IGen-generated data. The task is\nUse the hammer to hit the cardboard box.\nFigure 16\n:\nReal-World Deployment of Policy trained with IGen-Generated Data.\nThe task instructions are as follows:\nGrab the watering can and water the flowers.\nPick up the bottle and place it into the basket.\nUse the hammer to hit the cardboard box.\nFigure 17\n:\nReal-World Deployment of Policy trained with IGen-Generated Data.\nThe task instructions are as follows:\nGrasp the toy and put it into the bin.\nUse the watering can on the cabinet to water the flowers.\nPour water from the plastic bottle into the container.",
  "preview_text": "The rise of generalist robotic policies has created an exponential demand for large-scale training data. However, on-robot data collection is labor-intensive and often limited to specific environments. In contrast, open-world images capture a vast diversity of real-world scenes that naturally align with robotic manipulation tasks, offering a promising avenue for low-cost, large-scale robot data acquisition. Despite this potential, the lack of associated robot actions hinders the practical use of open-world images for robot learning, leaving this rich visual resource largely unexploited. To bridge this gap, we propose IGen, a framework that scalably generates realistic visual observations and executable actions from open-world images. IGen first converts unstructured 2D pixels into structured 3D scene representations suitable for scene understanding and manipulation. It then leverages the reasoning capabilities of vision-language models to transform scene-specific task instructions into high-level plans and generate low-level actions as SE(3) end-effector pose sequences. From these poses, it synthesizes dynamic scene evolution and renders temporally coherent visual observations. Experiments validate the high quality of visuomotor data generated by IGen, and show that policies trained solely on IGen-synthesized data achieve performance comparable to those trained on real-world data. This highlights the potential of IGen to support scalable data generation from open-world images for generalist robotic policy training.\n\n\\minted@def@optcl\nenvname-P envname#1\nIGen: Scalable Data Generation for Robot Learning from Open-World Images\nChenghao Gu\n1‚àó\nHaolan Kang\n2‚àó\nJunchao Lin\n3‚àó\nJinghe Wang\n1\nDuo Wu\n1\nShuzhao Xie\n1\nFanding Huang\n1\nJunchen Ge\n1\nZiyang Gong\n4\nLetian Li\n1\nHongying Zheng\n5\nChangwei Lv\n5\nZhi Wang\n1,‚Ä†\n1\nTsinghua University\n2\nHKU\n3\nBeijing University of Chemical Technology\n4\nShanghai Jiao Tong University\n5\nShenzhen University of Infomation Technology\nhttps://chengha",
  "is_relevant": false,
  "relevance_score": 2.0,
  "extracted_keywords": [
    "scalable data generation",
    "robot learning",
    "open-world images",
    "visuomotor policy",
    "3D scene representation",
    "vision-language models",
    "SE(3) pose generation"
  ],
  "one_line_summary": "IGenÂà©Áî®ÂºÄÊîæ‰∏ñÁïåÂõæÂÉèÁîüÊàêÊú∫Âô®‰∫∫Â≠¶‰π†ÊâÄÈúÄÁöÑËßÜËßâ-ËøêÂä®Êï∞ÊçÆÔºåÈÄöËøáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÁîüÊàêÂä®‰ΩúÂ∫èÂàóÔºåÂÆûÁé∞‰ªéÈùôÊÄÅÂõæÂÉèÂà∞Âä®ÊÄÅÂú∫ÊôØÁöÑÂèØÊâ©Â±ïÊï∞ÊçÆÂêàÊàê„ÄÇ",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "2025-12-01T15:15:04Z",
  "created_at": "2026-01-09T09:59:22.491826",
  "updated_at": "2026-01-09T09:59:22.491840"
}