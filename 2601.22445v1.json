{
    "id": "2601.22445v1",
    "title": "High-Definition 5MP Stereo Vision Sensing for Robotics",
    "authors": [
        "Leaf Jiang",
        "Matthew Holzel",
        "Bernhard Kaplan",
        "Hsiou-Yuan Liu",
        "Sabyasachi Paul",
        "Karen Rankin",
        "Piotr Swierczynski"
    ],
    "abstract": "高分辨率（500万像素以上）立体视觉系统对于提升机器人能力至关重要，能够支持更远距离的操作并生成更密集、更精确的三维点云。然而，要充分发挥高角分辨率传感器的全部潜力，需要相应更高水平的校准精度和更快的处理速度——这些要求往往是传统方法无法满足的。本研究通过采用一种新颖先进的帧间校准与立体匹配方法处理500万像素相机图像，旨在同时实现高精度与高速度，从而填补了这一关键空白。此外，我们提出了一种新的实时性能评估方法，通过将实时生成的视差图与基于计算密集型立体匹配算法得出的地面真实视差图进行比较。重要的是，本研究证明，只有通过实施高精度校准，高像素相机才能生成高质量的点云。",
    "url": "https://arxiv.org/abs/2601.22445v1",
    "html_url": "https://arxiv.org/html/2601.22445v1",
    "html_content": "Abstract\nHigh-resolution (5MP+) stereo vision systems are essential for advancing robotic capabilities, enabling operation over longer ranges and generating significantly denser and accurate 3D point clouds. However, realizing the full potential of high-angular-resolution sensors requires a commensurately higher level of calibration accuracy and faster processing – requirements often unmet by conventional methods. This study addresses that critical gap by processing 5MP camera imagery using a novel, advanced frame-to-frame calibration and stereo matching methodology designed to achieve both high accuracy and speed. Furthermore, we introduce a new approach to evaluate real-time performance by comparing real-time disparity maps with ground-truth disparity maps derived from more computationally intensive stereo matching algorithms. Crucially, the research demonstrates that high-pixel-count cameras yield high-quality point clouds only through the implementation of high-accuracy calibration.\nkeywords:\nStereo Vision; Stereo cameras; 3D Sensing; 3D Camera; 3D Sensor, Camera Calibration, Calibration Accuracy, Robotic Vision, Advanced Calibration; Robotics\n\\pubvolume\n1\n\\issuenum\n1\n\\articlenumber\n0\n\\datereceived\n\\daterevised\n\\dateaccepted\n\\datepublished\n\\hreflink\nhttps://doi.org/\n\\Title\nHigh-Definition 5MP Stereo Vision Sensing for Robotics\n\\TitleCitation\nTitle\n\\Author\nLeaf Jiang\n1,\n*\n\\orcidA\n, Matthew Holzel\n2\n, Bernhard Kaplan\n2\n, Hsiou-Yuan Liu\n1\n, Sabyasachi Paul\n2\n, Karen Rankin\n1\n, and Piotr Swierczynski\n2\n\\orcidB\n\\AuthorNames\nLeaf Jiang, Matthew Holzel, Bernhard Kaplan, Hsiou-Yuan Liu, Sabyasachi Paul, Karen Rankin, and Piotr Swierczynski\n\\isAPAStyle\n\\AuthorCitation\nLastname, F., Lastname, F., & Lastname, F.\n\\isChicagoStyle\n\\AuthorCitation\nLastname, Firstname, Firstname Lastname, and Firstname Lastname.\n\\AuthorCitation\nLastname, F.; Lastname, F.; Lastname, F.\n\\corres\nCorrespondence: leaf@nodarsensor.com; Tel.: +01-855-218-6422\n1\nIntroduction\nHigh-fidelity three-dimensional (3D) environmental sensing is a fundamental requirement for advanced robotic and autonomous systems. Stereo vision, in particular, offers a passive, cost-effective, and scalable method for generating dense 3D representations, or point clouds, crucial for tasks like path planning, object manipulation, and long-range localization. Recent technological advancements have led to the widespread adoption of high-resolution sensors, such as 5-megapixel (5MP) stereo camera systems. These high-angular-resolution sensors are essential for next-generation robotic capabilities, offering the promise of operating at significantly longer ranges, integrating with higher-capacity payloads, and providing a dramatic increase in the density and accuracy of the resulting point clouds.\nThe necessity of high-resolution sensing extends across the operational spectrum, from macro-scale scene analysis to micro-scale manipulation. At long ranges, high resolution is critical for reliably detecting and identifying small objects, preventing them from being blended into the background (akin to a low-pass filtering effect) and ensuring high confidence in tracking and recognition. Conversely, at close range, the sub-millimeter precision afforded by these systems is crucial for fine-motor tasks, such as the robotic manipulation and inspection of delicate components, including specialized parts like dental implants or micro-assembly units. This dual requirement highlights the increasing demand for robust, high-performance stereo vision systems.\nThe transition to high-resolution sensors, however, introduces a critical dependency: the precision required for camera system calibration must increase proportionally to the sensor’s horizontal pixel count. Specifically, the ability to resolve fine details and accurately determine depth (range resolution) at increasing distances relies not just on the raw sensor capability but also on the sub-pixel accuracy of the entire stereo rig model. Conventional static stereo camera calibration methodologies\nBradski (\n2000\n)\n, involving a one-time calibration at the factory, which are typically sufficient for lower-resolution systems, often introduce residual errors that become dominant in 5MP and higher-resolution setups over time, especially for systems exposed to shock and vibration, such as walking robots or autonomous vehicles. These limiting errors prevent the realization of the theoretical point cloud quality that the hardware is capable of achieving, creating a significant performance gap in state-of-the-art robotic vision systems. Instead, dynamic calibration\nJiang et al. (\n2022\n)\n, which can be calibrated on natural scenes, is necessary to maintain the performance of high-definition stereo vision systems under changing environmental conditions and potentially harsh conditions.\nTo contextualize the commercial landscape driving this research, Table\n1\nsummarizes the characteristics of several modern commercial stereo camera products and their respective resolutions. With higher-resolution cameras and longer baselines (the distance between cameras), extrinsic camera calibration becomes more difficult due to greater angular tolerance and greater sensitivity to disturbances. For example, the bending angle (slope of deflection) of a cantilever beam (representing the stereo camera structure) with a point load applied at the free end is proportional to the\nsquare\nof the length of the beam. This means that a 1-m baseline stereo camera is 100 times more sensitive to external perturbative forces arising from shocks and vibrations than a 10-cm baseline stereo camera, necessitating faster, more accurate online extrinsic camera calibration.\nTable\n1\nhighlights a second trend towards outdoor robotics, where high-dynamic-range (HDR) operation is crucial, enabling the stereo camera to obtain depth estimates in challenging lighting conditions, such as deep shadows or when the sun is shining directly into the camera. Cameras that offer this crucial high dynamic range (typically, 120-150 dB for automotive CMOS) often employ a rolling shutter rather than the preferred global shutter. This rolling-shutter mechanism inherently introduces difficulties for standard stereo matching algorithms. However, this barrier is being overcome: a few manufacturers, such as NODAR, are now producing specialized stereo vision processing solutions that successfully work with rolling-shutter HDR cameras.\nThe “Min Light\" column in Table\n1\nis, more accurately, the absolute sensitivity threshold, which is the minimum number of photons needed to equal the noise level. The NODAR stereo camera, for example, has a nearly photon-counting sensitivity with an absolute sensitivity threshold of 1.5 photons, which allows this camera to operate at night with excellent sensitivity.\nTable 1:\nSummary of modern commercial stereo vision camera system resolutions.\n\\isPreprints\nProduct Model\nManufacturer\nResolution\nDynamic Range\nMin Light\nBaseline\nNDR-HDK-2.0-100\nNODAR\n5.4MP\n2880\n×\n1860\n2880\\times 1860\n123 dB\n1.5 photons\n100 cm\nEagle\nLeopard\nImaging\n5.1 MP\n2560\n×\n1984\n2560\\times 1984\n100 dB\n–\n15 cm\nZED X\nStereolabs\n2.3 MP\n1920\n×\n1200\n1920\\times 1200\n71.4 dB\n–\n12 cm\nBumblebee X\nTeledyne\nFLIR\n3.2 MP\n2048\n×\n1536\n2048\\times 1536\n71.62 dB\n4.64 photons\n24 cm\nD455\nRealsense\n0.92 MP\n1280\n×\n720\n1280\\times 720\n–\n–\n9.5 cm\nOAK 4 D\nLuxonis\n1 MP\n1280\n×\n800\n1280\\times 800\n68 dB\n–\n7.5 cm\nGemini 435Le\nOrbbec\n1 MP\n1280\n×\n800\n1280\\times 800\n–\n–\n9.5 cm\nThis study directly addresses the performance bottleneck imposed by insufficient calibration. We propose and validate a novel, advanced calibration methodology specifically engineered to meet the demanding accuracy requirements of 5MP stereo vision. By meticulously processing imagery from two 5MP cameras using this advanced methodology, we successfully demonstrate the realization of the system’s theoretical point cloud quality. A key finding of this work is the establishment of a novel scaling law: point cloud quality, defined as the range resolution, scales as the square-root of the number of camera pixels. Our results show that high pixel count alone is insufficient; high-quality point clouds are achieved only with this specialized advanced calibration. The remainder of this paper is structured as follows: Section 2 reviews theoretical framework. Section 3 details the materials and methods. Section 4 presents the experimental results. Finally, Section 5 provides the conclusion.\n2\nTheoretical Framework\nThis section discusses the online calibration algorithms and derives the equation that relates the point cloud quality to the image sensor resolution.\n2.1\nOnline Auto-Calibration Algorithm\nAuto-calibration of a stereo rig is discussed in\nHartley and Zisserman (\n2004\n)\n, but such approaches rely on keypoints, which are inaccurate in natural scenes with non-pointy, round structures. These algorithms cannot be computed quickly enough to compensate for perturbations that occur frame by frame (e.g., engine or road vibration). For example, the relative roll, pitch, and yaw between two cameras mounted 1.2 meters apart on a car driving on a highway are plotted in Fig.\n1\n, showing that the extrinsic camera parameters vary significantly from frame to frame. These extrinsic camera parameters are extracted from the images and do not ues any accelerometer or inertial sensor data, which makes the approach more straightforward to implement, as it only needs camera images.\nFigure 1:\nExtrinsic camera parameters change every frame for 1.2-m baseline stereo camera mounted on a car and driving on a highway. The relative roll, pitch, and yaw are shown in units of degrees.\nThe latest auto-calibration algorithms, such as\nJiang et al. (\n2022\n)\n, do not make keypoint assumptions and consider all pixels in the frame (not just a few select keypoints that are often clustered in small parts of the image with unique texture), which are sensitive and fast enough to reveal the high-frequency camera motion in Fig.\n1\n. In this paper, we use the NODAR auto-calibration algorithm, downloaded from\nNODAR Inc (\n2025\n)\n.\n2.2\nStereo Matcher\nSelecting a stereo matching algorithm involves a trade-off between computational efficiency (speed) and disparity map accuracy. This research investigates two distinct stereo matching approaches developed by NODAR, each optimized for a specific set of robotic application requirements. The computational benchmarks are shown in Table\n2\n. It is important to note that the algorithms operate at native resolution and are not downsampled, as downsampling can miss small objects or those far away (\n∼\n5\n−\n15\n\\sim 5-15\npixels).\nTable 2:\nComputational benchmarks for Hammerhead and GroundTruth stereo matching algorithms on popular embedded GPUs and a common gaming GPU. The reported resolutions are depth maps at their native resolution (not downsampled, as downsampling is a common first step for stereo matcher neural networks to increase frame rates). This allows measuring the distance to small objects since it is at the native resolution. The specification, Points Per Second (pps), is common for lidar systems.\n\\isPreprints\nHardware\nGPU\nHammerhead\nGroundTruth\nNvidia AGX Orin\n5.3 FP32 TFLOPS\n160M pps\n20 FPS @ 8MP\n30 FPS @ 5.4MP\n160 FPS @ 1MP\n10M pps\n1 FPS @ 8MP\n2 FPS @ 5.4MP\n10 FPS @ 1MP\nNvidia AGX Thor\n7.8 FP32 TFLOPS\n235M pps\n29 FPS @ 8MP\n44 FPS @ 5.4MP\n235 FPS @ 1MP\n15M pps\n1.8 FPS @ 8MP\n2.7 FPS @ 5.4MP\n15 FPS @ 1MP\nNvidia Thor-X-Super\n18.4 FP32 TFLOPS\n555M pps\n69 FPS @ 8MP\n103 FPS @ 5.4MP\n555 FPS @ 1MP\n35M pps\n4.3 FPS @ 8MP\n6.4 FPS @ 5.4MP\n35 FPS @ 1MP\nNvidia RTX 4090\n82.6 FP32 TFLOPS\n2494M pps\n312 FPS @ 8 MP\n462 FPS @ 5.4MP\n2494 FPS @ 1MP\n156M pps\n19.5 FPS @ 8MP\n28.9 FPS @ 5.4MP\n156 FPS @ 1MP\n2.2.1\nReal-Time Stereo Matching: The Hammerhead Algorithm\nThe NODAR Hammerhead algorithm\nNODAR Inc. (\n2025a\n)\nis implemented as a real-time stereo matcher. It is specifically optimized for applications that demand high throughput, such as those in autonomous robotic systems. This algorithm is capable of rapidly processing 5-megapixel (MP) images at their full native resolution, addressing the critical requirement for low-latency operation in dynamic environments.\n2.2.2\nHigh-Accuracy Stereo Matching: The GroundTruth Algorithm\nIn contrast, the NODAR GroundTruth algorithm\nNODAR Inc. (\n2025b\n)\nis used as an offline stereo matcher. The primary optimization objective of this algorithm is to achieve the highest possible disparity map accuracy, with computational efficiency a secondary consideration. This makes it particularly suitable for applications that are less constrained by real-time processing demands, such as generating precise training data, validating algorithmic performance, and establishing ground-truth references.\nThe inclusion of these two algorithms effectively spans the primary use-case spectrum encountered in robotics development: from offline training and validation requiring maximum accuracy (GroundTruth) to on-board product implementation necessitating a highly efficient, lightweight, and real-time solution (Hammerhead).\n2.3\nQuantifying Depth Uncertainty in Stereo Vision\nThe best theoretical depth resolution for a stereo camera, limited by the camera’s angular resolving power, is\nSzeliski (\n2010\n)\nΔ\n​\nz\nb\n​\ne\n​\ns\n​\nt\n=\nz\n2\nf\n​\nB\n​\nΔ\n​\nd\n=\nx\n2\nB\n​\nΔ\n​\nθ\n,\n\\Delta z_{best}=\\frac{z^{2}}{fB}\\Delta d=\\frac{x^{2}}{B}\\Delta\\theta,\n(1)\nwhere\nz\nz\nis the depth to the object,\nf\nf\nis the focal length (typically units of pixels),\nB\nB\nis the baseline width, and\nΔ\n​\nd\n\\Delta d\nis the disparity resolution (typically in units of pixels). The angular resolution of the stereo vision measurement is\nΔ\n​\nθ\n=\nΔ\n​\nd\n/\nf\n\\Delta\\theta=\\Delta d/f\n. Since the angular resolution of the camera is proportional to the square-root of the number of pixels,\nN\np\n​\ni\n​\nx\nN_{pix}\n, then the depth resolution can be rewritten as\nΔ\n​\nz\nb\n​\ne\n​\ns\n​\nt\n∝\nz\n2\nB\n​\nN\np\n​\ni\n​\nx\n.\n\\Delta z_{best}\\propto\\frac{z^{2}}{B}\\sqrt{N_{pix}}.\n(2)\nTherefore, the depth resolution improves with a longer baseline or a larger format camera. In this paper, we experimentally show that increasing\nN\np\n​\ni\n​\nx\nN_{pix}\nfrom 1.275 to 5.1 MP (a factor of 4) improves the range resolution (point cloud quality) by a factor of 2, thereby confirming the square-root functional form.\nThe standard methodology for quantifying depth uncertainty (\nΔ\n​\nz\n\\Delta z\n) in stereo vision typically involves calculating the root-mean-square error (RMSE) of the disparity measurements around a planar target with a highly textured surface.\n2.3.1\nLimitations of the Standard Approach\nWhile useful for baseline assessment, this conventional method often fails to capture the full spectrum of large, sporadic stereo-matching errors that significantly impact the perceived quality and robustness of depth data in real-world scenarios. The use of an artificially textured, planar target simplifies the matching problem, often leading to an overly optimistic assessment of algorithm performance.\n2.3.2\nProposed Methodology: GroundTruth Approach\nIn contrast, our approach utilizes real-world objects of interest, specifically human subjects and complex, non-planar surfaces with challenging geometry and reflectance characteristics, to derive a more representative and robust measure of depth quality.\nUsing the high-accuracy stereo matcher disparity map as the ground truth (NODAR GroundTruth), we can then directly compute the error of the actual disparity map generated by the real-time stereo matcher algorithm (NODAR Hammerhead) as a function of range bin. More specifically, the range error for a given depth bin,\nΔ\n​\nz\nn\n\\Delta z_{n}\nfrom\nz\n=\nz\nn\nz=z_{n}\nto\nz\nn\n+\n1\nz_{n+1}\n, is the RMS difference between Hammerhead,\nz\nh\n​\nh\nz_{hh}\n, and GroundTruth,\nz\ng\n​\nt\nz_{gt}\n, depth estimates for pixels that are in the depth bin.\nThis methodology provides a more meaningful evaluation compared to relying on an unrealistic, texture-rich calibration target that facilitates trivial matching.\n3\nMaterials and Methods\nThe stereo vision camera used in the experiments, shown in Figure\n2\n, is the Leopard Imaging Eagle (model LI-VB1940-GM2C-137H) with a wide 137-degree horizontal field of view\nLeopard Imaging (\n2024\n)\n. This system incorporates two 5.1-megapixel, 2560 (H) x 1984 (V), STMicroelectronics VB1940 automotive image sensors. Both sensors feature a global shutter and are of the RGB-IR type. The two cameras are separated by 15 cm and utilize Maxim GMSL2 serializers for data transmission.\n\\isPreprints\nFigure 2:\nThe 15-cm-baseline stereo vision camera used in this work.\nThe system setup consisted of a stereo camera linked to an Nvidia Jetson Orin AGX developer kit via a Leopard Imaging serializer board. The Orin AGX was loaded with NODAR Hammerhead software, version 2.8.0\nNODAR Inc (\n2025\n)\n. We utilized the NODAR Hammerhead camera driver for Leopard Imaging Eagle camera\nNODAR Inc (\n2025\n)\n. This driver captures the stereo camera feed via GStreamer wrapper and translates it into ZMQ packets for Hammerhead processing. The camera-to-camera timing jitter was measured by recording a stopwatch on a monitor and was found to be about 1 ms.\nThe system was deployed outdoors on a tripod to capture scenes with long-range depth (relative to the baseline distance or about 200x baseline length) and high dynamic range (i.e., bright and dark areas). During data collection, the following were recorded to disk: full-resolution color images from both left and right cameras, and corresponding depth maps. Some sky pixels were saturated, illustrating the performance of a realistic stereo camera system with suboptimal gain and exposure settings.\nIn post-processing, the images were resized to half the height and half the width with a Lanczos resampling\nImageMagick Development Team (\n2025\n)\n.\nA single outdoor data capture was processed four ways:\n1.\nGroundTruth with 5MP images\n2.\nGroundTruth with downsampled images, 1.3MP\n3.\nHammerhead with 5MP images\n4.\nHammerhead with downsampled images, 1.3MP\nThe scene contained grass, trees, and moving people.\n4\nResults\nA single video sequence of 82 frames at approximately 5 FPS was captured from the left and right cameras of a grassy area with people moving around. The stereo camera was mounted on a tripod at about 1 m height. The raw data used in the article, as well as the Google Colab notebook used to summarize the data, are available on a public Google Drive\nNODAR Inc (\n2025\n)\n.\nThe left rectified images for frames 10, 17, and 41 are shown in Fig.\n4\n, and were used for more detailed analysis. These frames were selected to capture the person in the pink jacket at near (0.71 m), intermediate (4.50 m), and far (20.93 m) ranges.\n\\isPreprints\n(a)\n(b)\n(c)\n\\isPreprints\nFigure 3:\nLeft rectified images: (\na\n) Frame 10. (\nb\n) Frame 17. (\nc\n) Frame 41.\nThe point clouds for frame 10 are shown in Fig.\n4\n. The GroundTruth point clouds have fewer stray points than the Hammerhead point clouds, which are mainly seen at the edges of objects, indicating some foreground-background mixing. Not surprisingly, the downsampled images (factor of 2 in rows and columns, or 1280 x 992 pixels) show a loss of point cloud quality and density. Videos of the point clouds can be found here\nJiang (\n2025\n)\n. The point clouds reveal a gentle downward slope in the grass surface, with about 1 meter of elevation change for every 20 meters of range. Monocular depth estimation networks often struggle to produce metrically accurate depth reconstructions, indicating the importance of direct measurements, such as triangulation (i.e., stereo vision).\n\\isPreprints\n(a)\n(b)\n(c)\n(d)\n\\isPreprints\nFigure 4:\nPoint cloud from frame 10 from the video: (\na\n) GroundTruth with 5MP images. (\nb\n) GroundTruth with 1.3MP images. (\nc\n) Hammerhead with 5MP images. (\nd\n) Hammerhead with 1.3MP images. Visualizations from NODAR Viewer\nNODAR Inc (\n2025a\n)\n. The grid has 1-m spacing.\nThe top view of the point cloud of the adult in the blue shirt at a depth of 3.64 m is shown in Fig.\n4\nfor frame 17. The top view (or Bird’s-eye view) is seldom shown in stereo vision publications because it reveals range errors, especially at the edges of objects and at longer ranges. However, even at a relatively long depth of 3.64 m (or 72.8x baseline and wide-FOV lenses), the point cloud quality is high because the larger-format 5MP imager enables more accurate angular measurements.\nThe point cloud top view (or Bird’s-eye view) of an adult wearing a blue shirt is presented in Fig.\n4\n(for frame 17), captured at a depth of 3.64 m. While the top view is often omitted in stereo vision literature – as it inherently exposes range errors, particularly at object boundaries and extended ranges – this visualization clearly demonstrates the system’s high point cloud quality. This superior performance, even at a relatively long range (3.64 m, corresponding to\n24.3\n×\nbaseline\n24.3\\times\\text{baseline}\nwith wide-FOV lenses), is attributed to the use of a larger-format 5MP imager, which facilitates more accurate angular measurements. It’s also worth noting that the 5MP point clouds reveal more points between objects or between legs than the lower-resolution 1.3MP depth maps.\n\\isPreprints\n(a)\n(b)\n(c)\n(d)\n\\isPreprints\nFigure 5:\nTop view of point cloud from frame 17 from the video centered on an adult in blue shirt: (\na\n) GroundTruth with 5MP images. (\nb\n) GroundTruth with 1.3MP images. (\nc\n) Hammerhead with 5MP images. (\nd\n) Hammerhead with 1.3MP images. Visualizations from NODAR Viewer\nNODAR Inc (\n2025a\n)\n. The grid has 1-m spacing.\nThe point cloud top view (or Bird’s-eye view) of a child wearing a pink jacket is presented in Fig.\n4\n(for frame 41), captured at a depth of 18.24 m, corresponding to\n121.6\n×\nbaseline\n121.6\\times\\text{baseline}\n. This frame corresponds to an extremely long range. At this extreme range, the higher-resolution 5MP point clouds reveal the true advantage to longer ranges: points are clustered closer together and less range spread from objects.\n\\isPreprints\n(a)\n(b)\n(c)\n(d)\n\\isPreprints\nFigure 6:\nTop view of point cloud from frame 41 from the video centered on a child in a pink jacket next to the tree on the left side of the image. The child is located in the center of the bird’s eye view map (pink points). (\na\n) GroundTruth with 5MP images. (\nb\n) GroundTruth with 1.3MP images. (\nc\n) Hammerhead with 5MP images. (\nd\n) Hammerhead with 1.3MP images. Visualizations from NODAR Viewer\nNODAR Inc (\n2025a\n)\n. The grid has 2.5-m spacing.\n5\nDiscussion\nThe point cloud performance of the real-time Hammerhead algorithm is evaluated for full and half-resolutions, and for frames 10, 17, and 41 (corresponding to people at close, medium, and far range) as follows:\n1.\nThe real-time (Hammerhead) and offline (GroundTruth) disparity maps are loaded.\n2.\nThe disparity maps are visualized side-by-side. See Fig.\n5\n(a).\n3.\nThe absolute difference between the two disparity maps is computed and plotted. See Fig.\n5\n(b).\n4.\nBinary masks based on range bins are computed. Each mask corresponds to the pixels at a given depth bin. The depth bins for this work were (0, 2), (2, 4), (4, 6), (6, 8), (8, 10), (10, 16), (16, 22), (22, 28), (28, 36), and (36, 42) meters. At close ranges, the bins were 2 meters wide up to 10 meters depth, and 6 meters wide up to 42 meters depth. The wider bins at longer ranges help account for the point spread.\n5.\nIterate through the binary masks, apply each mask to the absolute disparity difference map, and compute the median absolute disparity error (in meters). Plot the measured results against a theoretical depth error curve (square of depth,\nZ\n2\nZ^{2}\n). See Fig.\n8\n.\nThe source code for computing the processed results is available in a publicly accessible Google Colab notebook\nNODAR Inc (\n2025b\n)\n. The GroundTruth and Hammerhead disparity maps (e.g., Fig.\n5\n(a)) are very similar with larger differences at the edges of the object since Hammerhead’s algorithm slightly overestimates the size of objects by a few pixels (see Fig.\n5\n(b)).\n\\isPreprints\n(a)\n(b)\n\\isPreprints\nFigure 7:\nDisparity maps from frame 10 (full 5 MP resolution) from the video: (\na\n) GroundTruth algorithm (left). Hammerhead algorithm (right). (\nb\n) The absolute difference between GroundTruth and Hammerhead disparity maps.\nThe GroundTruth disparity map is masked into 2-m depth bins from\nZ\n=\n0\nZ=0\nto 10 meters and into 6-m depth bins from\nZ\n=\n10\nZ=10\nto 42 meters. Making the depth bins larger at greater ranges helps accumulate more statistics for estimating depth error, as the fraction of the image corresponding to longer ranges decreases. For example, the binary masks corresponding to the depth bins is shown in Fig.\n8\n, which shows a statistically significant number of pixels in each depth bin.\n\\isPreprints\nFigure 8:\nBinary masks corresponding to 10 different depth bins for frame 10.\nThe primary results of this study are presented in Fig.\n9\n, which illustrates the relationship between mean absolute error and depth. As expected, depth error increases with the object’s distance from the stereo sensor. To calculate this error, we determined the median absolute difference between the Hammerhead and Ground Truth disparity maps, subsequently converting the values from pixels to meters. The median was utilized rather than the mean to ensure the results remain robust against outliers.\nThe noise floor of the measurement (black dotted line with diamonds) is computed for frame 10, by taking the difference of the GroundTruth disparity map of full-resolution images (followed by downsampling) and the GroundTruth disparity map of half-resolution images. As expected, the measurement noise floor is below all other experimental data.\nThe theoretical depth errors (\nΔ\n​\nZ\n=\nZ\n2\n/\n(\nf\n​\nB\n)\n​\nΔ\n​\nd\n\\Delta Z=Z^{2}/(fB)\\Delta d\n) are plotted for full resolution images (\nf\n=\n1180\nf=1180\npixels and\nΔ\n​\nd\n=\n0.31\n\\Delta d=0.31\npixels, solid gray line), and half resolution images (\nf\n=\n590\nf=590\npixels and\nΔ\n​\nd\n=\n0.25\n\\Delta d=0.25\npixels, dashed gray line). The pixel error (\nΔ\n​\nd\n\\Delta d\n) was tuned to fit the measured data. As expected, the full-resolution depth maps offer better depth measurement accuracy than the half-resolution depth maps. For example, at a depth of\nZ\n=\n39\nZ=39\nmeters, the mean absolute errors are 2.6 and 4.3 meters, respectively.\nThe median absolute errors vs. depth for frames 10, 17, and 41 are shown with solid lines for full-resolution images (5 MP) and dashed lines for half-resolution images. In general, the error for full-resolution depth maps is better than that of half-resolution depth maps.\nThe mean absolute error of points is sub-meter for objects up to around 20-meter distance, which is a direct result of processing high-resolution 5 MP images in real-time.\n\\isPreprints\nFigure 9:\nThe mean absolute error (meters) vs. depth.\nTo verify these results, a more traditional approach for measuring the error was to use a flat board with a textured pattern. A plane was fit to the depth map over the region of interest (ROI) corresponding to the flat board, at ranges from 0.33 to 8.98 meters, in approximately half-meter increments. The root-mean-square error (RMSE) between the fitted plane and the depth map values was then computed over the 100 x 100 pixel ROI. The plot of RMSE vs. depth is shown in Fig.\n10\n. The corresponding error agrees quite well with Fig.\n9\n.\n\\isPreprints\nFigure 10:\nThe RMSE (meters) vs. depth.\n6\nConclusions\nAs stereo vision cameras increase in resolution, they are able to support detection at longer ranges due to inherently better angular measurements. Increasing the resolution of stereo cameras by a factor of 5 (e.g., 1 MP to 5 MP), increases the range of the sensor by a factor of 2.24. We presented a technique for quantifying the depth map quality based on the GroundTruth algorithm. As the angular resolution and baseline of the cameras increase, calibration becomes a necessary condition.\nUnder typical conditions, a wide-angle stereo camera with a 137-degree field of view and only 15-cm baseline is not expected to reliably detect objects at a distance of 20 m. However, the increased spatial resolution of our system enables effective perception at this range.\n\\informedconsent\nInformed consent was obtained from all subjects involved in the study.\n\\dataavailability\nThe original data presented in the study are openly available in\nJiang (\n2025\n)\n,\nNODAR Inc (\n2025\n)\n, and\nNODAR Inc (\n2025\n)\n.\nAcknowledgements.\nThe authors thank Leopard Imaging for providing the Eagle stereo vision camera used in the study.\n\\conflictsofinterest\nThe authors declare no conflicts of interest.\n\\isPreprints\n\\reftitle\nReferences\nReferences\nBradski (2000)\nBradski, G.\nThe OpenCV Library.\nDr. Dobb’s Journal of Software Tools\n2000\n.\nJiang et al. (2022)\nJiang, L.A.; Rosen, P.B.; Swierczynski, P.\nNon-rigid stereo vision camera system.\nUS 11,282,234, Mar 2022.\nHartley and Zisserman (2004)\nHartley, R.; Zisserman, A.\nMultiple View Geometry in Computer Vision\n, second ed.; Cambridge University Press: Cambridge, UK, 2004.\nhttps://doi.org/10.1017/cbo9780511811685\n.\nNODAR Inc (2025)\nNODAR Inc.\nHammerhead SDK.\nhttps://nodarsensor.notion.site/sdk\n, 2025.\n(Accessed on 3 December 2025).\nNODAR Inc. (2025a)\nNODAR Inc..\nHammerhead | Real-Time 3D Mapping for Autonomous Vehicles.\nhttps://www.nodarsensor.com/products/hammerhead\n, 2025.\n(Accessed on 2 November 2025).\nNODAR Inc. (2025b)\nNODAR Inc..\nNODAR Cloud | Transform 2D Video into High-Resolution, Long-Range 3D Ground Truth.\nhttps://www.nodarsensor.com/products/nodarcloud\n, 2025.\n(Accessed on 2 November 2025).\nSzeliski (2010)\nSzeliski, R.\nComputer Vision: Algorithms and Applications\n; Springer Science & Business Media: London, 2010.\nLeopard Imaging (2024)\nLeopard Imaging.\nLI-VB1940-GM2C-137H RGB-IR Active Stereo Camera Datasheet.\nDatasheet, 2024.\nVersion 0.4, Revised 10 Oct 2024. Accessed on 1 November 2025.\nNODAR Inc (2025)\nNODAR Inc.\nGitHub Repository for Hammerhead Camera Drivers for Leopard Iamging Cameras.\nhttps://github.com/nodarhub/camera-leopard-imaging-eagle\n, 2025.\nAccessed: 2025-12-03.\nImageMagick Development Team (2025)\nImageMagick Development Team.\nImageMagick: Command-line Tools for Image Manipulation\n.\nImageMagick Studio LLC, 2025.\nVersion 7.1.1 (Note: Year and version are placeholders for the current release).\nNODAR Inc (2025)\nNODAR Inc.\nRaw data for paper.\nhttps://drive.google.com/drive/folders/1TSxPOVk5OzSri3XGfGg8jENQVYOkDAb4?usp=sharing\nor\nhttps://tinyurl.com/5mp-stereo-vision-data\n, 2025.\nAccessed: 2025-12-03.\nJiang (2025)\nJiang, L.\nPoint cloud videos from 1.2 and 5MP stereo cameras with real-time and offline stereo matching algorithms\n2025\n.\nhttps://doi.org/10.6084/m9.figshare.30780125\n.\nNODAR Inc (2025a)\nNODAR Inc.\nNODAR Viewer.\nhttps://nodarsensor.notion.site/nodar-viewer\n, 2025.\nAccessed: 2025-12-03.\nNODAR Inc (2025b)\nNODAR Inc.\nGoogle Colab Notebook.\nhttps://colab.research.google.com/drive/1IdcVVVzwQeGBz_G3WA1e67yCZq4IRVwu?usp=sharing\nor\nhttps://tinyurl.com/5mp-stereo-vision-notebook\n, 2025.\nAccessed: 2025-12-18.\n\\isPreprints",
    "preview_text": "High-resolution (5MP+) stereo vision systems are essential for advancing robotic capabilities, enabling operation over longer ranges and generating significantly denser and accurate 3D point clouds. However, realizing the full potential of high-angular-resolution sensors requires a commensurately higher level of calibration accuracy and faster processing -- requirements often unmet by conventional methods. This study addresses that critical gap by processing 5MP camera imagery using a novel, advanced frame-to-frame calibration and stereo matching methodology designed to achieve both high accuracy and speed. Furthermore, we introduce a new approach to evaluate real-time performance by comparing real-time disparity maps with ground-truth disparity maps derived from more computationally intensive stereo matching algorithms. Crucially, the research demonstrates that high-pixel-count cameras yield high-quality point clouds only through the implementation of high-accuracy calibration.\n\nAbstract\nHigh-resolution (5MP+) stereo vision systems are essential for advancing robotic capabilities, enabling operation over longer ranges and generating significantly denser and accurate 3D point clouds. However, realizing the full potential of high-angular-resolution sensors requires a commensurately higher level of calibration accuracy and faster processing – requirements often unmet by conventional methods. This study addresses that critical gap by processing 5MP camera imagery using a novel, advanced frame-to-frame calibration and stereo matching methodology designed to achieve both high accuracy and speed. Furthermore, we introduce a new approach to evaluate real-time performance by comparing real-time disparity maps with ground-truth disparity maps derived from more computationally intensive stereo matching algorithms. Crucially, the research demonstrates that high-pixel-count cameras yield high-quality point clouds only through the implementation of high-accuracy calibration.\nkey",
    "is_relevant": false,
    "relevance_score": 0.0,
    "extracted_keywords": [],
    "one_line_summary": "该论文专注于高分辨率立体视觉系统的校准和匹配方法，与强化学习、VLA、机器人控制等关键词无直接关联。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T01:29:34Z",
    "created_at": "2026-02-03T15:53:04.248092",
    "updated_at": "2026-02-03T15:53:04.248098"
}