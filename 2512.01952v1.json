{
    "id": "2512.01952v1",
    "title": "GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment",
    "authors": [
        "Haoyang He",
        "Jay Patrikar",
        "Dong-Ki Kim",
        "Max Smith",
        "Daniel McGann",
        "Ali-akbar Agha-mohammadi",
        "Shayegan Omidshafiei",
        "Sebastian Scherer"
    ],
    "abstract": "è¿‘å¹´æ¥ï¼Œè§†é¢‘ä¸–ç•Œå»ºæ¨¡æ–¹é¢çš„è¿›å±•ä½¿å¾—å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿä»¥é«˜è§†è§‰ä¿çœŸåº¦æ¨¡æ‹Ÿå…·èº«ç¯å¢ƒï¼Œä¸ºé¢„æµ‹ã€è§„åˆ’å’Œæ§åˆ¶æä¾›äº†å¼ºæœ‰åŠ›çš„å…ˆéªŒä¿¡æ¯ã€‚ç„¶è€Œï¼Œå°½ç®¡è¿™äº›æ¨¡å‹å…·æœ‰é«˜åº¦çš„çœŸå®æ€§ï¼Œå®ƒä»¬å¾€å¾€ç¼ºä¹å‡ ä½•ä¸Šçš„ groundingï¼ˆå®ä½“å¯¹åº”ï¼‰ï¼Œé™åˆ¶äº†å…¶åœ¨éœ€è¦ç©ºé—´ä¸€è‡´æ€§ä¸é•¿æ—¶ç¨‹ç¨³å®šæ€§çš„å¯¼èˆªä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¸–ç•Œ grounding çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ˆRLWGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è‡ªç›‘ç£çš„åè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å‡ ä½•ä¸æ„ŸçŸ¥å¥–åŠ±å°†é¢„è®­ç»ƒçš„ä¸–ç•Œæ¨¡å‹ä¸ç‰©ç†ä¸Šå¯éªŒè¯çš„ç»“æ„å¯¹é½ã€‚ç±»ä¼¼äºè¯­è¨€æ¨¡å‹ä¸­åŸºäºå¯éªŒè¯åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ï¼ŒRLWG å¯ä»¥ç»“åˆå¤šç§è¡¡é‡å§¿æ€å¾ªç¯ä¸€è‡´æ€§ã€æ·±åº¦é‡æŠ•å½±è¯¯å·®ä»¥åŠæ—¶åºè¿è´¯æ€§çš„å¥–åŠ±ä¿¡å·ã€‚æˆ‘ä»¬åœ¨è¯¥æ¡†æ¶ä¸‹å®ç°äº† GrndCtrl æ–¹æ³•ï¼Œä¸€ç§åŸºäºåˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¥–åŠ±å¯¹é½å¼é€‚é…æŠ€æœ¯ï¼Œä½¿ä¸–ç•Œæ¨¡å‹èƒ½å¤Ÿåœ¨å…·èº«å¯¼èˆªä¸­ä¿æŒç¨³å®šçš„è½¨è¿¹ã€ä¸€è‡´çš„å‡ ä½•ç»“æ„ä»¥åŠå¯é çš„ rollout è¡¨ç°ã€‚ä¸å¤§è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒå¯¹é½ç±»ä¼¼ï¼ŒGrndCtrl åˆ©ç”¨å¯éªŒè¯çš„å¥–åŠ±ä¿¡å·å¼¥åˆäº†ç”Ÿæˆå¼é¢„è®­ç»ƒä¸ grounded è¡Œä¸ºä¹‹é—´çš„é¸¿æ²Ÿï¼Œåœ¨æˆ·å¤–ç¯å¢ƒä¸­ç›¸æ¯”ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œå±•ç°å‡ºæ›´ä¼˜çš„ç©ºé—´ä¸€è‡´æ€§ä¸å¯¼èˆªç¨³å®šæ€§ã€‚",
    "url": "https://arxiv.org/abs/2512.01952v1",
    "html_url": "https://arxiv.org/html/2512.01952v1",
    "html_content": "GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment\nhttps://rlwg-grndctrl.github.io/\nHaoyang He\n1,2\nJay Patrikar\n2\nDong-Ki Kim\n2\nMax Smith\n2\nDaniel McGann\n2\nAli-akbar Agha-mohammadi\n2\nShayegan Omidshafiei\n2\nSebastian Scherer\n1,2\nAbstract\nRecent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce\nReinforcement Learning with World Grounding (RLWG)\n, a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with\nGrndCtrl\n, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models,\nGrndCtrl\nleverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.\n1\nCarnegie Mellon University\n2\nFieldAI\nFigure 1\n:\nReinforcement Learning with World Grounding (RLWG)\naddresses geometric inconsistencies in pretrained video world models through self-supervised post-training with verifiable rewards. Instead of reconstruction losses, RLWG grounds models using geometric and perceptual rewards from frozen evaluators.\nGrndCtrl\ninstantiates RLWG using Group Relative Policy Optimization (GRPO), enabling physically consistent rollouts essential for reliable world generation.\n1\nIntroduction\nLarge-scale video world models have emerged as powerful priors for modeling perception and control for embodied agents\n[\nnvidia2025cosmosworldfoundationmodel\n,\nbar2025navigationworldmodels\n,\nassran2025vjepa2\n,\njin2025posepilotsteeringcamerapose\n,\nren2025gen3c\n]\n. By learning to predict future observations from past frames and actions, these models approximate the transition dynamics of the physical world, enabling simulation, planning, and policy evaluation. Operating in the pixel domain aligns them with real-world sensors and exploits the vast implicit supervision available in video, allowing unified modeling across domains such as manipulation, driving, and navigation. Yet despite their impressive generative fidelity, these models are often incentivized to capture the appearance of motion more than its structure. Their rollouts remain visually plausible but geometrically and temporally inconsistent: poses drift, depths wobble, and trajectories lose alignment over time. Even subtle deviations in inferred geometry accumulate into compounding spatial errors corrupting metric structure. These instabilities limit the use of current models for closed-loop tasks such as localization, mapping, and planning, where physically consistent representation is essential.\nWe define\nworld model grounding\nas aligning learned dynamics with physically verifiable spatial and temporal invariants, so that rollouts honor geometry and time in addition to reproducing surface appearance. Grounding shifts the objective of world modeling from visual plausibility to structural consistency, ensuring that the modelâ€™s internal dynamics respect the constraints of real motion and scene structure. To this end, we introduce\nReinforcement Learning with World Grounding (RLWG)\n, a self-supervised post-training framework that refines pretrained world models using verifiable geometric and perceptual rewards derived from model rollouts. RLWG extends the principle of Reinforcement Learning with Verifiable Rewards (RLVR) from language models\n[\nlambert2025tulu3pushingfrontiers\n]\nto the embodied domain, replacing text-based logical verification with geometric and temporal verification. In RLWG, a pretrained world model is treated as a policy that generates multiple candidate rollouts from the same context; each rollout is automatically scored using verifiable grounding rewards that quantify spatial and temporal coherence, such as pose cycle-consistency, depth reprojection agreement and action adherence. Unlike reconstruction losses that only penalize pixel error, these rewards measure physical correctness of the rollouts.\nTo optimize these verifiable rewards efficiently, we adopt Group Relative Policy Optimization (GRPO)\n[\nshao2024deepseekmathpushinglimitsmathematical\n]\nas our training mechanism, yielding our algorithm,\nGrndCtrl\n. For each context (and actions when available), the model generates a group of rollouts that are ranked by their grounding rewards; relative advantages are computed within the group, and the latent transition operator is updated using a clipped policy gradient objective regularized toward the pretrained model. This formulation preserves visual quality while progressively aligning the modelâ€™s dynamics with measurable structure in the real world. The process requires no human annotations or external simulators, operating entirely through self-supervised reinforcement grounded in the modelâ€™s own predictions. Conceptually,\nGrndCtrl\nextends the success of GRPO-based alignment in generative modeling to the geometric domain, grounding visual world models in verifiable 3D and temporal coherence.\nThis paradigm reframes the role of post-training in world modeling. Rather than optimizing for perceptual fidelity or next-frame likelihood, RLWG drives the model toward internal representations that are self-consistent and physically grounded. It establishes a structural analogue to the self-alignment processes that have improved reasoning in large language models: where RLVR grounds language in logic, RLWG grounds world models in geometry. The resulting models are self-grounded, spatially coherent, and dynamically stableâ€”capable not only of rendering the world vividly, but of representing it in actionable, physically consistent form. Through this lens, we move beyond visually coherent generation toward structurally consistent simulation, bridging the gap between generative video modeling and physical world understanding, and opening a path toward world models that can both imagine and inhabit the real world.\nThe main contributions of this work are:\n1.\nWe introduce\nReinforcement Learning with World Grounding (RLWG)\n, a self-supervised grounding framework using verifiable geometric and temporal rewards from frozen evaluators without labels or simulators.\n2.\nWe construct\nGrndCtrl\n, a method that extends GRPO to the RLWG regime by multi-reward alignment over stochastic rollouts optimizing Translation, Rotation, Depth Temporal Reprojection Inlier ratio, and perceptual quality with pretrained frozen evaluators.\n3.\nWe provide a comprehensive evaluation of\nGrndCtrl\nacross multiple datasets showing reduced pose error means and variances, with strong gains under counterfactual rollouts and generalization to unseen inputs.\n2\nRelated Work\n2.1\nControlling World Models\nRecent progress in large-scale video foundation models has transformed video prediction into controllable world simulation. Models such as Cosmos-Predict\n[\nnvidia2025cosmosworldfoundationmodel\n]\n, and V-JEPA\n[\nassran2025vjepa2\n]\nunify multi-modal conditioning for long-horizon prediction and control. These models achieve impressive simulation fidelity but still exhibit spatial drift, geometric misalignment, and temporal incoherence over extended rollouts, revealing limitations in geometric grounding. Architectural innovations like flow matching, conditional diffusion transformers, and masked latent prediction have improved realism but not physical consistency.\nControlling these models can be categorized into\naction-conditioned\nand\ncamera-conditioned\nparadigms. The first paradigm trains models to predict futures from discrete actions, and can be further defined by the nature of the action and observation frame. Some methods\n[\nzhou2025dinowmworldmodelspretrained\n,\nzhu2025unifiedworldmodelscoupling\n]\nassume a static camera observing an embodiment performing actions, while others\n[\nhafner2024masteringdiversedomainsworld\n,\nvalevski2025diffusionmodelsrealtimegame\n,\nbar2025navigationworldmodels\n,\nbai2025wholebodyconditionedegocentricvideo\n]\nassume a fixed camera on the embodiment and train models to predict ego-centric views. Jointly, they aim to enable model-predictive control, but face challenges with physical realism. The second paradigm explicitly decouples viewpoint from embodiment, allowing the model to generate future observations from arbitrary camera poses. Early works\n[\nwang2024motionctrlunifiedflexiblemotion\n,\nhe2025cameractrlenablingcameracontrol\n]\ninjected pose embeddings into diffusion models to achieve this control, but lacked explicit geometric alignment. Subsequent methods\n[\njin2025posepilotsteeringcamerapose\n,\nren2025gen3c\n]\nimproved consistency via self-supervised warping, 3D-informed point cloud conditioning, achieving more precise viewpoint control. Recent work\n[\nguo2025ctrlworldcontrollablegenerativeworld\n]\nbridges the two paradigms by jointly training on multiple viewpoints, and improved spatial-temporal consistency with pose-conditioned memory retrieval. Despite these advances, most approaches rely on supervised learning or one-step consistency objectives and remain open-loop, with no mechanism to evaluate or optimize physical correctness.\n2.2\nReward Learning for Post-Training\nReinforcement-based post-training has become central to aligning large generative models. In language systems, Reinforcement Learning from Human Feedback (RLHF)\n[\nouyang2022traininglanguagemodelsfollow\n]\nand Reinforcement Learning with Verifiable Rewards (RLVR)\n[\nlambert2025tulu3pushingfrontiers\n]\nreplace imitation with objective-driven alignment, while Group Relative Policy Optimization (GRPO)\n[\nshao2024deepseekmathpushinglimitsmathematical\n]\nstabilizes learning via stochastic rollouts comparative updates. Extensions to vision, such as DanceGRPO\n[\nxue2025dancegrpounleashinggrpovisual\n]\n, demonstrate that rewards on visual quality can fine-tune video diffusion models effectively.\nBuilding on this foundation, RLWG adapts RLVR to world modeling, optimizing pretrained world models using physically verifiable rewards including cycle-consistency, depth reprojection, and trajectory stability.\nGrndCtrl\ninstantiates RLWG as a multi-objective GRPO over grounded rewards. This process enforces geometric coherence without human supervision, significantly reducing long-horizon drift. Parallel advances in 3D perception emphasize similar constraints: VGGT\n[\nwang2025vggt\n]\nand MapAnything\n[\nkeetha2025mapanythinguniversalfeedforwardmetric\n]\npredict depth and camera pose for consistent scene reconstruction, while SpaTracker\n[\nxiao2025spatialtrackerv23dpointtracking\n]\nintegrates rigidity priors for robust 3D tracking. Together, these efforts point toward reward-informed geometry as a unifying principle, where physical correctness acts as an alignment signal bridging generative modeling, simulation, and control.\nFigure 2\n:\nGrndCtrl\nframework architecture. Given conditioning context\nc\n=\n(\nx\n0\n,\na\n0\n:\nT\nâˆ’\n1\n)\nc=(x_{0},a_{0:T-1})\n, the world model generates multiple stochastic rollouts\n{\nx\n^\n1\n:\nT\n(\ni\n)\n}\ni\n=\n1\nG\n\\{\\hat{x}^{(i)}_{1:T}\\}_{i=1}^{G}\n. Frozen evaluators compute verifiable rewards for each rollout. Relative advantages are calculated within each group, and GRPO updates model parameters using a clipped policy gradient objective regularized toward the pretrained model, favoring physically consistent rollouts.\n3\nGrndCtrl\n3.1\nProblem Definition\nWe consider a pretrained\nvideo world model\nW\nÎ¸\nW_{\\theta}\n, a policy parameterized by\nÎ¸\n\\theta\n, that predicts future observations conditioned on a visual history and optionally actions.\nLet\nx\n0\nx_{0}\ndenote the observed frame and\na\n0\n:\nT\nâˆ’\n1\n=\n(\na\n0\n,\nâ€¦\n,\na\nT\nâˆ’\n1\n)\na_{0:T-1}=(a_{0},\\ldots,a_{T-1})\nthe associated control inputs.\nThe model samples a rollout\nx\n^\n1\n:\nT\nâˆ¼\nW\nÎ¸\n(\nâ‹…\nâˆ£\nx\n0\n,\na\n0\n:\nT\nâˆ’\n1\n)\n\\hat{x}_{1:T}\\sim W_{\\theta}(\\cdot\\mid x_{0},a_{0:T-1})\n,\nwhere\na\nt\n=\n(\nR\nt\n,\nğ­\nt\n)\na_{t}=(R_{t},\\mathbf{t}_{t})\ncontrols translation\nğ­\nt\nâˆˆ\nâ„\n3\n\\mathbf{t}_{t}\\in\\mathbb{R}^{3}\nand rotation\nR\nt\nâˆˆ\nSO\nâ€‹\n(\n3\n)\nR_{t}\\in\\mathrm{SO}(3)\n. Our goal is to\npost-train\nW\nÎ¸\nW_{\\theta}\nusing self-supervised reinforcement learning to improve the\nspatial coherence\nand\nembodied reliability\nof its rollouts.\nTo obtain\nverifiable\nfeedback without supervision, we use a frozen\nfeed-forward 3D evaluator\nâ„°\n\\mathcal{E}\nthat provides relative pose estimates\n(\nÎ”\nâ€‹\nR\n1\n:\nT\n,\nÎ”\nâ€‹\nğ­\n1\n:\nT\n)\n(\\Delta R_{1:T},\\Delta\\mathbf{t}_{1:T})\nand per-frame depth maps\nD\nt\nD_{t}\n,\nwhere\nÎ”\nâ€‹\nR\nt\nâˆˆ\nSO\nâ€‹\n(\n3\n)\n\\Delta R_{t}\\in\\mathrm{SO}(3)\nand\nÎ”\nâ€‹\nğ­\nt\nâˆˆ\nâ„\n3\n\\Delta\\mathbf{t}_{t}\\in\\mathbb{R}^{3}\n. Additionally, we obtain feedback on the visual and motion quality of the overall video using a frozen\nfeed-forward video evaluator\nğ’±\n\\mathcal{V}\nthat provides overall visual quality scoring.\nOur objective is to optimize\nÎ¸\n\\theta\nsuch that sampled rollouts maximize a set of verifiable rewards\nU\nâ€‹\n(\nx\n^\n1\n:\nT\n)\nU(\\hat{x}_{1:T})\ncomprising translation (\nr\ntrans\nr_{\\text{trans}}\n), rotation (\nr\nrot\nr_{\\text{rot}}\n), depth temporal reprojection (\nr\ndtr\nr_{\\text{dtr}}\n), and video quality (\nr\nv\nr_{\\text{v}}\n).\nThese rewards are constructed from the 3D evaluator\nâ„°\n\\mathcal{E}\nand video evaluator\nğ’±\n\\mathcal{V}\n, and are defined in detail in Section\n3.2\n.\n3.2\nVerifiable Self-Supervised Rewards\nLet\nğ’¯\n\\mathcal{T}\ndenote the set of evaluated timesteps.\nEach reward term measures a distinct aspect of spatial and temporal consistency.\n(1) Translation Reward.\nWe compute the Euclidean deviation in translation:\nğ\nt\ntrans\n=\nÎ”\nâ€‹\nğ­\nt\nâˆ’\nğ­\nt\n.\n\\mathbf{e}^{\\text{trans}}_{t}=\\Delta\\mathbf{t}_{t}-\\mathbf{t}_{t}.\n(1)\nand define the sum of mean squared trajectory error and squared final error as translation reward:\nr\ntrans\n=\nâˆ’\n(\n1\n|\nğ’¯\n|\nâ€‹\nâˆ‘\nt\nâˆˆ\nğ’¯\nâ€–\nğ\nt\ntrans\nâ€–\n2\n2\n+\nâ€–\nğ\n|\nğ’¯\n|\ntrans\nâ€–\n2\n2\n)\n.\nr_{\\text{trans}}=-\\left(\\frac{1}{|\\mathcal{T}|}\\sum_{t\\in\\mathcal{T}}\\|\\mathbf{e}^{\\text{trans}}_{t}\\|^{2}_{2}+\\|\\mathbf{e}^{\\text{trans}}_{|\\mathcal{T}|}\\|^{2}_{2}\\right).\n(2)\nWhen metric scale is ambiguous, a normalization factor is applied from the evaluatorâ€™s scale estimate.\n(2) Rotation Reward.\nWe compute the minimum angular deviation between predicted and evaluator rotations:\nğ\nt\nrot\n=\narccos\nâ¡\n(\ntr\nâ¡\n(\nÎ”\nâ€‹\nR\nt\nâ‹…\nR\nt\nT\n)\nâˆ’\n1\n2\n)\n,\n\\mathbf{e}^{\\text{rot}}_{t}=\\arccos\\left(\\frac{\\operatorname{tr}\\left(\\Delta R_{t}\\cdot{R_{t}}^{T}\\right)-1}{2}\\right),\n(3)\nand define the axis-angle cumulative error as rotation reward:\nr\nrot\n=\nâˆ’\nâˆ‘\nt\nâˆˆ\nğ’¯\nğ\nt\nrot\n.\nr_{\\text{rot}}=-\\sum_{t\\in\\mathcal{T}}\\mathbf{e}^{\\text{rot}}_{t}.\n(4)\n(3) Depth Temporal Reprojection Reward.\nWe adopt the\ndepth inlier ratio\nfrom MapAnything\n[\nkeetha2025mapanythinguniversalfeedforwardmetric\n]\nevaluations as a verifiable geometric reward for depth temporal reprojection.\nFor each pixel\np\nâˆˆ\nÎ©\np\\!\\in\\!\\Omega\n, define the reprojected correspondence and expected depth via the evaluator geometry\n(\np\n^\n,\nd\nt\nâ†’\nt\n+\n1\nexp\nâ€‹\n(\np\n)\n)\n=\nÎ¦\nâ€‹\n(\np\n;\nD\nt\n,\nÎ”\nâ€‹\nR\nt\n,\nÎ”\nâ€‹\nğ­\nt\n,\nK\nt\n,\nK\nt\n+\n1\n)\n,\n(\\hat{p},\\,d_{t\\to t+1}^{\\exp}(p))\\;=\\;\\Phi\\!\\big(p;\\,D_{t},\\,\\Delta R_{t},\\,\\Delta\\mathbf{t}_{t},\\,K_{t},K_{t+1}\\big),\n(5)\nwhere\nÎ¦\n\\Phi\nback-projects\np\np\nusing\nD\nt\nD_{t}\n, applies\n(\nÎ”\nâ€‹\nR\nt\n,\nÎ”\nâ€‹\nğ­\nt\n)\n(\\Delta R_{t},\\Delta\\mathbf{t}_{t})\n, and projects into frame\nt\n+\n1\nt{+}1\n.\nThe per-pair depth inlier ratio (threshold\nÎ³\n=\n0.0103\n\\gamma{=}0.0103\n) is\nDTRI\nt\n(\nÎ³\n)\n=\n1\n|\nÎ©\n|\nâ€‹\nâˆ‘\np\nâˆˆ\nÎ©\nğŸ\nâ€‹\n[\n|\nD\nt\n+\n1\nâ€‹\n(\np\n^\n)\nâˆ’\nd\nt\nâ†’\nt\n+\n1\nexp\nâ€‹\n(\np\n)\n|\nd\nt\nâ†’\nt\n+\n1\nexp\nâ€‹\n(\np\n)\n<\nÎ³\n]\n.\n\\mathrm{DTRI}_{t}^{(\\gamma)}\\;=\\;\\frac{1}{|\\Omega|}\\sum_{p\\in\\Omega}\\mathbf{1}\\!\\left[\\frac{\\big|\\,D_{t+1}(\\hat{p})-d_{t\\to t+1}^{\\exp}(p)\\,\\big|}{d_{t\\to t+1}^{\\exp}(p)}\\;<\\;\\gamma\\right].\n(6)\nWe use the average inlier ratio as the reward:\nr\ndtr\n=\n1\n|\nğ’¯\n|\nâ€‹\nâˆ‘\nt\nâˆˆ\nğ’¯\nDTRI\nt\n(\nÎ³\n)\n.\nr_{\\text{dtr}}\\;=\\;\\frac{1}{|\\mathcal{T}|}\\sum_{t\\in\\mathcal{T}}\\mathrm{DTRI}_{t}^{(\\gamma)}.\n(7)\n(4) Video Quality Reward.\nWe use the frozen VideoAlign\n[\nliu2025improving\n]\nas evaluator\nğ’±\n\\mathcal{V}\n, which returns three sequence-level scores in\n[\n0\n,\n1\n]\n[0,1]\nâ€”visual quality\ns\nvis\ns_{\\text{vis}}\n, motion quality\ns\nmot\ns_{\\text{mot}}\n, and text alignment\ns\ntxt\ns_{\\text{txt}}\nâ€”for a rollout\nx\n^\n1\n:\nT\n\\hat{x}_{1:T}\n(optionally conditioned on a prompt\ny\ny\n):\n(\ns\nvis\n,\ns\nmot\n,\ns\ntxt\n)\n=\nğ’±\nâ€‹\n(\nx\n^\n1\n:\nT\n;\ny\n)\n.\n(s_{\\text{vis}},\\,s_{\\text{mot}},\\,s_{\\text{txt}})\\;=\\;\\mathcal{V}(\\hat{x}_{1:T};\\,y).\n(8)\nOur visual reward uses only visual and motion quality as a convex combination,\nr\nv\n=\nÎ±\nâ€‹\ns\nvis\n+\nÎ²\nâ€‹\ns\nmot\n,\nÎ±\n,\nÎ²\nâ‰¥\n0\n,\nÎ±\n+\nÎ²\n=\n1\n,\nr_{\\text{v}}\\;=\\;\\alpha\\,s_{\\text{vis}}+\\beta\\,s_{\\text{mot}},\\qquad\\alpha,\\beta\\geq 0,\\;\\alpha+\\beta=1,\n(9)\nwith\nÎ±\n=\nÎ²\n=\n1\n2\n\\alpha=\\beta=\\tfrac{1}{2}\nby default.\n3.3\nGRPO for RLWG Post-Training\nFor each\nconditioning context\nc\n=\n(\nx\n0\n,\na\n0\n:\nT\nâˆ’\n1\n)\nc=(x_{0},a_{0:T-1})\n, a group of\nG\nG\ncandidate rollouts\n{\nx\n^\n1\n:\nT\n(\ni\n)\n}\ni\n=\n1\nG\n\\{\\hat{x}^{(i)}_{1:T}\\}_{i=1}^{G}\nis sampled from\nW\nÎ¸\nW_{\\theta}\n.\nEach rollout is evaluated by every reward from the verifiable reward set\nU\nâ€‹\n(\nx\n^\n1\n:\nT\n)\nU(\\hat{x}_{1:T})\n, obtaining\n{\nr\ntrans\n(\ni\n)\n,\nr\nrot\n(\ni\n)\n,\nr\ndtr\n(\ni\n)\n,\nr\nv\n(\ni\n)\n}\n=\nU\nâ€‹\n(\nx\n^\n1\n:\nT\n(\ni\n)\n)\n\\{r^{(i)}_{\\text{trans}},r^{(i)}_{\\text{rot}},r^{(i)}_{\\text{dtr}},r^{(i)}_{\\text{v}}\\}=U(\\hat{x}^{(i)}_{1:T})\n. We compute the normalized reward for every verifiable reward\nr\n~\n(\ni\n)\n\\tilde{r}^{(i)}\n, and obtain a multi-objective normalized group advantage\nA\ni\nA_{i}\n:\nr\n~\n(\ni\n)\n=\nr\n(\ni\n)\nâˆ’\nmean\nâ€‹\n(\nr\n(\n1\n)\n,\nâ€¦\n,\nr\n(\nG\n)\n)\nstd\nâ€‹\n(\nr\n(\n1\n)\n,\nâ€¦\n,\nr\n(\nG\n)\n)\n,\nA\ni\n=\nr\n~\n(\ni\n)\nâˆ’\nmean\nâ€‹\n(\nr\n~\n(\n1\n)\n,\nâ€¦\n,\nr\n~\n(\nG\n)\n)\nstd\nâ€‹\n(\nr\n~\n(\n1\n)\n,\nâ€¦\n,\nr\n~\n(\nG\n)\n)\n,\n\\tilde{r}^{(i)}=\\tfrac{r^{(i)}-\\text{mean}(r^{(1)},\\ldots,r^{(G)})}{\\text{std}(r^{(1)},\\ldots,r^{(G)})},\\quad A_{i}=\\tfrac{\\tilde{r}^{(i)}-\\text{mean}(\\tilde{r}^{(1)},\\ldots,\\tilde{r}^{(G)})}{\\text{std}(\\tilde{r}^{(1)},\\ldots,\\tilde{r}^{(G)})},\n(10)\nThe GRPO objective optimizes\nÎ¸\n\\theta\nusing the clipped surrogate:\nJ\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nâ€‹\n[\n1\nG\nâ€‹\nâˆ‘\ni\n=\n1\nG\n1\nT\nâ€‹\nâˆ‘\nt\n=\n1\nT\nmin\nâ¡\n(\nÏ\nt\n,\ni\nâ€‹\nA\ni\n,\nclip\nâ€‹\n(\nÏ\nt\n,\ni\n,\n1\nâˆ’\nÏµ\n,\n1\n+\nÏµ\n)\nâ€‹\nA\ni\n)\n]\n,\nJ(\\theta)=\\mathbb{E}\\Big[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{T}\\sum_{t=1}^{T}\\min\\big(\\rho_{t,i}A_{i},\\;\\text{clip}(\\rho_{t,i},1-\\epsilon,1+\\epsilon)A_{i}\\big)\\Big],\n(11)\nwhere\nÏµ\n\\epsilon\nis the clip ratio,\nÏ\nt\n,\ni\n\\rho_{t,i}\nis the per-step likelihood ratio under the current and reference policies.\nThis formulation stabilizes policy optimization for continuous video generation tasks. An illustration of the detailed\nGrndCtrl\nframework is shown in Figure\n2\n. To form each group for GRPO in practice, we generate multiple stochastic rollouts.\nGroups are instantiated by sampling multiple candidate rollouts from the diffusion generator under controlled stochasticity. Diffusion sampling can be formulated as a reverse-time\nstochastic differential equation\n(SDE) or, equivalently in time-marginals, as a\nprobabilityâ€“flow ordinary differential equation\n(ODE)\n[\nsong2021scorebasedgenerativemodelingstochastic\n,\nalbergo2025stochasticinterpolantsunifyingframework\n]\n. Let\nx\nt\nâˆˆ\nâ„\nd\nx_{t}\\in\\mathbb{R}^{d}\ndenote the latent state at time\nt\nâˆˆ\n[\n0\n,\n1\n]\nt\\in[0,1]\n(noisiest at\nt\n=\n1\nt{=}1\n, data manifold at\nt\nâ†’\n0\nt{\\to}0\n). A standard varianceâ€“preserving forward SDE is\nd\nâ€‹\nx\nt\n=\nâˆ’\n1\n2\nâ€‹\nÎ²\nâ€‹\n(\nt\n)\nâ€‹\nx\nt\nâ€‹\nd\nâ€‹\nt\n+\nÎ²\nâ€‹\n(\nt\n)\nâ€‹\nd\nâ€‹\nw\nt\n,\n\\mathrm{d}x_{t}=-\\tfrac{1}{2}\\beta(t)x_{t}\\mathrm{d}t+\\sqrt{\\beta(t)}\\mathrm{d}w_{t},\n(12)\nwith\nw\nt\nw_{t}\nas a standard Brownian motion and\nÎ²\nâ€‹\n(\nt\n)\n>\n0\n\\beta(t){>}0\nas the noise rate. The corresponding\nreverse\nSDE is\nd\nâ€‹\nx\nt\n=\n(\nâˆ’\n1\n2\nâ€‹\nÎ²\nâ€‹\n(\nt\n)\nâ€‹\nx\nt\nâˆ’\nÎ²\nâ€‹\n(\nt\n)\nâ€‹\nâˆ‡\nx\nlog\nâ¡\np\nt\nâ€‹\n(\nx\nt\n)\n)\nâ€‹\nd\nâ€‹\nt\n+\nÎ²\nâ€‹\n(\nt\n)\nâ€‹\nd\nâ€‹\nw\nÂ¯\nt\n,\n\\mathrm{d}x_{t}=\\big(-\\tfrac{1}{2}\\beta(t)x_{t}-\\beta(t)\\nabla_{x}\\log p_{t}(x_{t})\\big)\\mathrm{d}t+\\sqrt{\\beta(t)}\\mathrm{d}\\bar{w}_{t},\n(13)\nwhich is stochastic due to the\nd\nâ€‹\nw\nÂ¯\nt\n\\mathrm{d}\\bar{w}_{t}\nterm and uses the learned score\nâˆ‡\nx\nlog\nâ¡\np\nt\n\\nabla_{x}\\log p_{t}\n, where\np\nt\np_{t}\nis the forward SDE solution distribution of\nx\nt\nx_{t}\n. The\nprobabilityâ€“flow ODE\n, which shares the same\np\nt\n{p_{t}}\nas (\n13\n) but is deterministic, is\nd\nâ€‹\nx\nt\nd\nâ€‹\nt\n=\nâˆ’\n1\n2\nâ€‹\nÎ²\nâ€‹\n(\nt\n)\nâ€‹\nx\nt\nâˆ’\n1\n2\nâ€‹\nÎ²\nâ€‹\n(\nt\n)\nâ€‹\nâˆ‡\nx\nlog\nâ¡\np\nt\nâ€‹\n(\nx\nt\n)\n.\n\\frac{\\mathrm{d}x_{t}}{\\mathrm{d}t}=-\\tfrac{1}{2}\\beta(t)x_{t}-\\tfrac{1}{2}\\beta(t)\\nabla_{x}\\log p_{t}(x_{t}).\n(14)\nIn practice, sampler stochasticity is controlled by\nÎ·\nâˆˆ\n[\n0\n,\n1\n]\n\\eta\\in[0,1]\n(ODE limit at\nÎ·\n=\n0\n\\eta{=}0\n; SDE-style sampling with per-step Gaussian perturbations at\nÎ·\n=\n1\n\\eta{=}1\n). For each context\nc\nc\n, we draw\nG\nG\nrollouts\n{\nx\n^\n1\n:\nT\n(\ni\n)\n}\ni\n=\n1\nG\n\\{\\hat{x}_{1:T}^{(i)}\\}_{i=1}^{G}\nwith identical conditioning and independent noise governed by\nÎ·\n\\eta\n, providing the within-group diversity required by GRPO.\n4\nExperimental Setup\n(a)\n(b)\nFigure 3\n:\nQualitative results: (a)\nGrndCtrl\nmitigates scene drift on counterfactual rollouts, maintaining spatial coherence where baseline diverges. (b)\nGrndCtrl\nsuccessfully follows directionally inverted actions, generating geometrically consistent rollouts where baseline fails.\n4.1\nDatasets\nWe train and evaluate our methods on three datasets spanning diverse embodiments and scenarios.\nCODa\n[\nzhang2023towards\n]\nis a campus navigation dataset collected on wheeled robots with pseudo-ground truth poses.\nSCAND\n[\nkarnan2022sociallycompliantnavigationdataset\n]\nis a social navigation dataset also collected on campus, featuring embodiments of both a wheeled robot and a quadruped.\nCityWalk\n[\nliu2025citywalker\n]\nis an egocentric urban navigation dataset of a person walking in crowded city streets, collected from in-the-wild YouTube city walking videos and reprocessed with MapAnything\n[\nkeetha2025mapanythinguniversalfeedforwardmetric\n]\nto obtain pose estimates. We subsample each dataset to 2k non-overlapping 13-frame sequences of pose-action pairs for training, ensuring temporal diversity and avoiding data leakage.\n4.2\nBaseline Model\nWe obtain the baseline pretrained world model\nW\nÎ¸\nW_{\\theta}\nvia supervised fine-tuning (SFT) on Cosmos-Predict2-2B-Video2World\n[\nnvidia2025cosmosworldfoundationmodel\n]\n, a diffusion model with a latent VAE backbone and temporal attention. We use a modified version of its action-conditioned video predictor post-training pipeline, adapting the action space from 7 to 6 degrees of freedom to match our navigation setting:\n(\nx\n,\ny\n,\nz\n,\nroll\n,\npitch\n,\nyaw\n)\n(x,y,z,\\mathrm{roll},\\mathrm{pitch},\\mathrm{yaw})\nas 6Ã—12 action embeddings. We initialize the action embeddings randomly while fine-tuning the entire DiT backbone for 20k steps with an effective batch size of 64. Training follows the EDM framework\n[\nkarras2022elucidatingdesignspacediffusionbased\n,\nkarras2024analyzingimprovingtrainingdynamics\n]\n, using the weighted expectation of denoising score matching loss over noise levels. We perform full SFT over all DiT backbone parameters keeping the visual encoder and decoder frozen, as our experiments showed that only full SFT or full-size LoRA yielded meaningful changes in motion dynamics. Additional implementation details are provided in Appendix.\n4.3\nGRPO Post-Training\nPost-training applies GRPO with self-supervised verifiable rewards as described in Sec.\n3.3\n. We use MapAnything\n[\nkeetha2025mapanythinguniversalfeedforwardmetric\n]\nas\nâ„°\n\\mathcal{E}\nto obtain rewards\nr\ntrans\nr_{\\text{trans}}\n,\nr\nrot\nr_{\\text{rot}}\n, and\nr\ndtr\nr_{\\text{dtr}}\n. We use VideoAlign\n[\nliu2025improving\n]\nas\nğ’±\n\\mathcal{V}\nto obtain the reward\nr\nv\nr_{\\text{v}}\n. We perform an ablation study of combinatorial rewards as defined in Sec.\n3.2\nto evaluate multi-objective GRPO. For each reward configuration, we train for 100 steps with an effective batch size of 8, generating\nG\n=\n8\nG=8\nstochastic rollouts\n{\nx\n^\n1\n:\nT\n(\ni\n)\n}\ni\n=\n1\nG\n\\{\\hat{x}^{(i)}_{1:T}\\}_{i=1}^{G}\nper conditioning context\nc\n=\n(\nx\n0\n,\na\n0\n:\nT\nâˆ’\n1\n)\nc=(x_{0},a_{0:T-1})\n. To obtain diverse rollouts, we use the same initial noise for reverse-SDE diffusion with the same starting frame\nx\n0\nx_{0}\nand action trajectory\na\n0\n:\nT\nâˆ’\n1\na_{0:T-1}\n, but inject stochastic Brownian noise at each diffusion step as described in Sec.\n3.3\n. We compute per-step likelihood ratios only over the first 60% of diffusion timesteps to focus training on the most relevant denoising steps to improve training stability. Additional training details are provided in Appendix.\n4.4\nEvaluation Regimes\nWe evaluate on three regimes that progressively test generalization capabilities:\nâ€¢\nSeen\n: Start frames and scene domains seen during SFT with matching action distributions. This regime tests in-distribution performance.\nâ€¢\nCounterfactual\n: Scenes seen during SFT but with counterfactual actions (e.g., mirrored or directionally inverted action sequences). This regime tests the modelâ€™s ability to extrapolate geometric structure to novel motion patterns.\nâ€¢\nUnseen\n: Both scenes and actions novel at test time, with scenes from different domains and action sequences with different motion characteristics. This regime tests full generalization to unseen scenarios.\n4.5\nMetrics\nWe report four metrics averaged across an evaluation set of 200 non-overlapping sequences for each regime: Translation error (\nT\n=\nâˆ’\nr\ntrans\n\\text{T}=-r_{\\text{trans}}\n) in meters, Rotation error (\nR\n=\nâˆ’\nr\nrot\n\\text{R}=-r_{\\text{rot}}\n) in radians, Video Quality (\nV\n=\nr\nv\n\\text{V}=r_{\\text{v}}\n) combining visual and motion quality scores, and Depth Temporal Reprojection Inlier Ratio (\nDTRI\n=\nr\ndtr\n\\text{DTRI}=r_{\\text{dtr}}\n) as a percentage. Lower values indicate better performance for T and R, while higher values are better for V and DTRI.\n5\nAnalysis and Discussion\nSeen\nCounterfactual\nUnseen\nMethod\nT\nâ†“\nT\\downarrow\nR\nâ†“\nR\\downarrow\nV\nâ†‘\nV\\uparrow\nDTRI\nâ†‘\n\\uparrow\nT\nâ†“\nT\\downarrow\nR\nâ†“\nR\\downarrow\nV\nâ†‘\nV\\uparrow\nDTRI\nâ†‘\n\\uparrow\nT\nâ†“\nT\\downarrow\nR\nâ†“\nR\\downarrow\nV\nâ†‘\nV\\uparrow\nDTRI\nâ†‘\n\\uparrow\nCODa\n[\nzhang2023towards\n]\nBaseline\n[\nnvidia2025cosmosworldfoundationmodel\n]\n57.8\n57.8\n1.77\n1.77\n7.40\n7.40\n38.9\n38.9\n71.5\n71.5\n1.55\n1.55\n7.41\n7.41\n39.1\n39.1\n56.9\n56.9\n1.71\n1.71\n7.40\n7.40\n38.3\n38.3\n+T+R\n46.4\n46.4\n1.44\n1.44\n7.32\n7.32\n38.4\n38.4\n50.5\n50.5\n1.53\n1.53\n7.34\n7.34\n38.7\n38.7\n54.3\n54.3\n1.75\n1.75\n7.36\n7.36\n39.3\n39.3\n+T+R+DTRI\n65.7\n65.7\n1.74\n1.74\n7.43\n7.43\n37.0\n37.0\n57.7\n57.7\n1.86\n1.86\n7.42\n7.42\n36.8\n36.8\n42.6\n42.6\n1.74\n1.74\n7.40\n7.40\n37.1\n37.1\n+T+R+DTRI+V\n39.9\n39.9\n1.27\n1.27\n7.35\n7.35\n37.5\n37.5\n40.7\n40.7\n1.42\n1.42\n7.34\n7.34\n37.4\n37.4\n31.0\n31.0\n1.53\n1.53\n7.37\n7.37\n38.0\n38.0\nSCAND\n[\nkarnan2022sociallycompliantnavigationdataset\n]\nBaseline\n[\nnvidia2025cosmosworldfoundationmodel\n]\n186.3\n186.3\n3.76\n3.76\n7.16\n7.16\n23.6\n23.6\n315.9\n315.9\n4.24\n4.24\n7.13\n7.13\n21.4\n21.4\n117.0\n117.0\n4.02\n4.02\n6.99\n6.99\n18.4\n18.4\n+T+R\n158.2\n158.2\n3.61\n3.61\n7.19\n7.19\n23.7\n23.7\n251.2\n251.2\n4.34\n4.34\n7.18\n7.18\n21.7\n21.7\n131.1\n131.1\n3.95\n3.95\n7.04\n7.04\n19.1\n19.1\n+T+R+DTRI\n157.9\n157.9\n3.65\n3.65\n7.10\n7.10\n22.1\n22.1\n288.6\n288.6\n4.45\n4.45\n7.17\n7.17\n20.1\n20.1\n118.6\n118.6\n4.07\n4.07\n7.03\n7.03\n17.9\n17.9\n+T+R+DTRI+V\n133.4\n133.4\n3.30\n3.30\n7.11\n7.11\n24.5\n24.5\n220.1\n220.1\n4.23\n4.23\n7.08\n7.08\n22.8\n22.8\n123.4\n123.4\n3.62\n3.62\n6.98\n6.98\n19.4\n19.4\nCityWalk\n[\nliu2025citywalker\n]\nBaseline\n[\nnvidia2025cosmosworldfoundationmodel\n]\n11.7\n11.7\n3.13\n3.13\n7.96\n7.96\n46.9\n46.9\n13.1\n13.1\n3.27\n3.27\n7.94\n7.94\n47.4\n47.4\n20.8\n20.8\n4.47\n4.47\n7.90\n7.90\n44.5\n44.5\n+T+R\n8.9\n8.9\n3.31\n3.31\n7.90\n7.90\n44.9\n44.9\n4.8\n4.8\n4.42\n4.42\n7.91\n7.91\n45.6\n45.6\n10.2\n10.2\n3.47\n3.47\n7.87\n7.87\n42.8\n42.8\n+T+R+DTRI\n8.4\n8.4\n3.36\n3.36\n7.84\n7.84\n43.5\n43.5\n4.7\n4.7\n4.40\n4.40\n7.83\n7.83\n44.1\n44.1\n10.9\n10.9\n3.68\n3.68\n7.79\n7.79\n41.4\n41.4\n+T+R+DTRI+V\n8.8\n8.8\n3.37\n3.37\n7.84\n7.84\n42.6\n42.6\n4.7\n4.7\n4.37\n4.37\n7.85\n7.85\n43.3\n43.3\n9.9\n9.9\n3.74\n3.74\n7.80\n7.80\n40.8\n40.8\nTable 1\n:\nQuantitative evaluation across three datasets (\nCODa\n,\nSCAND\n,\nCityWalk\n) and three regimes:\nSeen\n,\nCounterfactual\n, and\nUnseen\n. We compare baseline against progressive reward combinations (T+R, T+R+DTRI, T+R+DTRI+V).\nGrndCtrl\nachieves substantial improvements. Metrics: T (Translation Error, m), R (Rotation Error, rad), V (Video Quality), DTRI (Depth Temporal Reprojection Inliers).\n5.1\nImpact of\nGrndCtrl\non World Model Failures\nO1: Baselines show poor counterfactual performance but good generalization within familiar motion manifolds.\nTable\n1\nreports quantitative results on CODa, SCAND, and CityWalk datasets across three evaluation regimes. Across all datasets, the baseline performs well in Seen but degrades significantly in Counterfactual, indicating limited transfer to out-of-distribution action sequences. On CODa, baseline translation error increases by\n24\n%\n24\\%\nfrom Seen to Counterfactual, while on SCAND the degradation is more severe, increasing by\n70\n%\n70\\%\n. On CityWalk, the increase is more modest at\n12\n%\n12\\%\n. Unseen performance, however, remains comparable to Seen across datasets, suggesting that the pretrained model does generalize within familiar motion manifolds. This contrast in performance between Counterfactual and Unseen indicates a fundamental limitation in pretrained video world modelâ€™s ability to extrapolate geometric structure to counterfactual rollouts, a key property expected of grounded world simulators.\nO2: Translation and rotation rewards improve spatial alignment with largest gains under counterfactual motion.\nIntroducing translation and rotation rewards\nr\ntrans\nr_{\\text{trans}}\nand\nr\nrot\nr_{\\text{rot}}\n(\nT+R\n) improves spatial alignment across all datasets. On CODa, T+R decreases translation error by\n20\n%\n20\\%\nin Seen and by\n29\n%\n29\\%\nin Counterfactual relative to baseline, while rotation error improves by\n19\n%\n19\\%\nin Seen. On SCAND, the improvements are substantial: translation error decreases by\n15\n%\n15\\%\nin Seen and by\n20\n%\n20\\%\nin Counterfactual, with rotation error improving by\n4\n%\n4\\%\nin Seen. CityWalk shows the strongest counterfactual gains, with Counterfactual translation error dropping by\n63\n%\n63\\%\nrelative to baseline, while Seen improves by\n24\n%\n24\\%\n. These results demonstrate that explicit pose-based feedback enhances motion consistency and stabilizes rollouts\nx\n^\n1\n:\nT\n\\hat{x}_{1:T}\nacross diverse embodiments. Notably, the improvement in Counterfactual highlights that verifiable motion alignment encourages generalization to directionally inverted action sequences\na\n0\n:\nT\nâˆ’\n1\na_{0:T-1}\n, an essential feature for embodied reasoning.\nTable\n2\nfurther demonstrates that GRPO training systematically improves model reliability: the baseline model shows high variance across both translation and rotation errors, indicating unstable rollouts sensitive to diffusion noise. With\nGrndCtrl\ntraining, both mean errors and variance decrease substantially: at 200 iterations, translation error means reduce by\n77\n%\n77\\%\nrelative to baseline across experiments, with standard deviations reduced by\n75\n%\n75\\%\n. Rotation error also improves, with means reducing by 39% and standard deviations reduced by\n32\n%\n32\\%\n, achieving reliable and consistent rollouts across all evaluation regimes.\nSeen\nCounterfactual\nUnseen\nMethod\nT\nâ†“\nT\\downarrow\nR\nâ†“\nR\\downarrow\nT\nâ†“\nT\\downarrow\nR\nâ†“\nR\\downarrow\nT\nâ†“\nT\\downarrow\nR\nâ†“\nR\\downarrow\nBaseline\n[\nnvidia2025cosmosworldfoundationmodel\n]\n73.2\nÂ±\n243.7\n73.2\\pm 243.7\n2.38\nÂ±\n3.88\n2.38\\pm 3.88\n75.8\nÂ±\n253.9\n75.8\\pm 253.9\n2.38\nÂ±\n3.90\n2.38\\pm 3.90\n71.2\nÂ±\n251.2\n71.2\\pm 251.2\n2.88\nÂ±\n4.28\n2.88\\pm 4.28\nGrndCtrl\nT+R 100\n72.0\nÂ±\n283.6\n72.0\\pm 283.6\n1.95\nÂ±\n3.38\n1.95\\pm 3.38\n75.9\nÂ±\n311.7\n75.9\\pm 311.7\n1.85\nÂ±\n3.22\n1.85\\pm 3.22\n58.4\nÂ±\n231.1\n58.4\\pm 231.1\n2.57\nÂ±\n4.08\n2.57\\pm 4.08\nGrndCtrl\nT+R 150\n26.7\nÂ±\n101.5\n26.7\\pm 101.5\n1.54\nÂ±\n2.84\n1.54\\pm 2.84\n24.8\nÂ±\n99.1\n24.8\\pm 99.1\n1.53\nÂ±\n2.80\n1.53\\pm 2.80\n26.5\nÂ±\n104.3\n26.5\\pm 104.3\n2.08\nÂ±\n3.33\n2.08\\pm 3.33\nGrndCtrl\nT+R 200\n18.4\nÂ±\n68.1\n18.4\\pm 68.1\n1.40\nÂ±\n2.49\n1.40\\pm 2.49\n16.8\nÂ±\n63.0\n16.8\\pm 63.0\n1.36\nÂ±\n2.57\n1.36\\pm 2.57\n16.3\nÂ±\n56.5\n16.3\\pm 56.5\n1.97\nÂ±\n3.11\n1.97\\pm 3.11\nTable 2\n:\nReliability analysis showing error statistics (mean\nÂ±\n\\pm\nstandard deviation) across multiple stochastic rollouts for different GRPO iterations. Baseline exhibits high variance, while GRPO training progressively reduces both mean errors and variance, achieving consistent rollouts. Metrics: T (Translation Error, m), R (Rotation Error, rad).\nO3: Depth reward enforces local coherence but trades off global alignment.\nAdding the depth reprojection reward\nr\ndtr\nr_{\\text{dtr}}\n(\nT+R+DTRI\n) enforces local geometric coherence but introduces a trade-off in global rollout alignment. On CODa, compared to T+R, translation error increases by\n42\n%\n42\\%\nin Seen and by\n14\n%\n14\\%\nin Counterfactual, while Unseen benefits substantially with a\n22\n%\n22\\%\nimprovement. On SCAND, Seen shows minimal change, Counterfactual degrades by\n15\n%\n15\\%\n, while Unseen improves by\n10\n%\n10\\%\n. CityWalk shows a different pattern where DTRI maintains or slightly improves performance across most regimes. This suggests the depth reward enforces short-horizon consistency and local geometric smoothness, with benefits most apparent in unseen scenarios.\nO4: Full reward set produces most balanced and robust performance.\nIncorporating perceptual feedback through the full reward set (\nT+R+DTRI+V\n) produces the most balanced and robust performance across all datasets. On CODa, the full objective achieves translation error reductions of\n31\n%\n31\\%\n(Seen),\n43\n%\n43\\%\n(Counterfactual), and\n45\n%\n45\\%\n(Unseen) relative to baseline, with rotation error improving by\n28\n%\n28\\%\n(Seen),\n8\n%\n8\\%\n(Counterfactual), and\n11\n%\n11\\%\n(Unseen). On SCAND, translation error reduces by\n28\n%\n28\\%\n(Seen),\n30\n%\n30\\%\n(Counterfactual), and\n5\n%\n5\\%\n(Unseen), while CityWalk shows consistent improvements with Counterfactual achieving a\n64\n%\n64\\%\nreduction. The full objective recovers and extends the rollout accuracy of T+R while preserving local stability from DTRI. Perceptual alignment via video-based evaluators acts as a long-horizon stabilizer, promoting rollouts that are both physically consistent and visually coherent.\nFigure\n3\nshowcases qualitative comparisons of\nGrndCtrl\nrollouts\nx\n^\n1\n:\nT\n\\hat{x}_{1:T}\nagainst the baseline.\nGrndCtrl\nsignificantly mitigates the scene drift failures frequently observed when queried with mirrored input action sequences\na\n0\n:\nT\nâˆ’\n1\na_{0:T-1}\n, and improves rollout consistency.\n5.2\nTraining Insights and Stability\nI1: Pretraining bias requires full fine-tuning for meaningful motion adaptation.\nCosmos-Predict2â€™s pretraining on videos with mostly short clips and slight forward movements biases the model toward scene stability over dynamics. This pretraining bias explains why we observed negligible changes in motion behavior when using parameter-efficient methods like LoRA (except full-size variants), suggesting that pretrained world models may require substantial capacity updates to adapt to navigation-specific action distributions.\nI2: Early diffusion timesteps are most critical for GRPO training stability.\nGrndCtrl\ntraining is significantly more memory intensive than SFT, as it requires maintaining computation graphs over all diffusion steps to track per-timestep log-probabilities for likelihood ratio computation. Early timesteps are primarily responsible for content denoising while later timesteps refine details with exponentially smaller likelihood values, making the 60% timestep limitation both memory-efficient and beneficial for training stability by focusing on the most relevant denoising steps.\nI3:\nGrndCtrl\nrequires sufficient reward variance and careful checkpoint selection.\nMost critically,\nGrndCtrl\ntraining stability depends on having sufficient reward variance across stochastic rollouts.\nGrndCtrl\nfocuses optimization on the reward component with highest variance within each group, which is beneficial for addressing scene drift when some rollouts fail visual odometry while others succeed. However, when checkpoints are overfitted or undertrained, producing uniformly poor or uniformly good rollouts, the normalization step in advantage calculation can amplify subtle differences, leading to degraded training where the model learns arbitrary patterns that add noise to predictions. We mitigate this through KL regularization toward the pretrained model and careful checkpoint selection, ensuring the baseline checkpoint generates distinguishably good and bad rollouts before applying GRPO.\n6\nLimitations\nRLWG introduces a new paradigm for grounding world models using verifiable rewards, and our study focuses on establishing core principles rather than exhaustively scaling the approach. Our experiments use moderate budgets and limited datasets, but already demonstrate that meaningful grounding can be achieved efficiently. A practical constraint of\nGrndCtrl\nis its reliance on sufficient rollout variance for stable relative-policy optimization; understanding the variance dynamics of reward-aligned world model training remains an open direction. Additionally, while RLWG optimizes a multi-objective reward, we do not explore alternative weighting strategies or adaptive weighting schemes, which could further shape the trade-off between global alignment, local consistency, and visual fidelity. Investigating principled multi-reward weighting and larger-scale training holds promise for pushing RLWG toward increasingly stable, generalizable, and physically grounded world models.\n7\nConclusions\nOur work demonstrates that\nRLWG\nenables pretrained video world models to be effectively grounded in physical structure through self-supervised post-training, without requiring human labels or external simulators. Instantiated as\nGrndCtrl\n, the key insight is that verifiable geometric and perceptual rewards, when optimized via relative policy methods, systematically improve spatial coherence while preserving the visual quality that makes these models powerful priors. The most significant improvements emerge under counterfactual scenarios.\nGrndCtrl\nachieves substantial improvements under counterfactual rollouts, with up to\n64\n%\n64\\%\nreduction in translation error, demonstrating that reward-based alignment addresses a fundamental limitation of current world models: their tendency to prioritize visual plausibility over structural consistency. By explicitly optimizing for both,\nRLWG\nenables models that are not only visually coherent but also geometrically consistent, opening new possibilities for reliable long-horizon planning and control in real-world environments.\nAcknowledgements\nWe thank NVIDIA and the authors of Cosmos-Predict2\n[\nnvidia2025cosmosworldfoundationmodel\n]\nfor publicly releasing their code and checkpoints, upon which we build our post-training pipeline. We thank Nikhil Keetha and collaborators for MapAnything\n[\nkeetha2025mapanythinguniversalfeedforwardmetric\n]\n, and the authors of VideoAlign\n[\nliu2025improving\n]\n, for making their code and checkpoints publicly available, enabling our verifiable self-supervised rewards and evaluation metrics. We also thank Amir Bar and collaborators for releasing Navigation World Models\n[\nbar2025navigationworldmodels\n]\ncode and checkpoints and for kindly responding to our inquiry email. Finally, we thank Cherie Ho for early-stage discussions and Jay Karhade for discussions on geometric consistency.\n\\thetitle\nAppendix\nAppendix A\nQualitative Results\nWe include additional counterfactual generation results of\nGrndCtrl\nagainst baseline and ground truths in SCAND\n[\nkarnan2022sociallycompliantnavigationdataset\n]\n(Figure\n4\n) and CityWalk\n[\nliu2025citywalker\n]\n(Figure\n5\n), in addition to CODa\n[\nzhang2023towards\n]\n(Figure\n3\n). The counterfactual rollouts of\nGrndCtrl\nand baseline are expected to mirror the trajectory shown in ground truths in each scene. We observe\nGrndCtrl\nimproves trajectory-following and mitigates scene drift in counterfactual rollouts in all three datasets.\nFigure 4\n:\nCounterfactual generation example in SCAND. Baseline and\nGrndCtrl\nare conditioned on left-right mirrored actions, and the rollouts are expected to invert the movements of the ground truth. Baseline generation still follows similar left-forward movement as GT (trajectory-following failure), while\nGrndCtrl\ngeneration successfully follow inverted right-forward movement.\nFigure 5\n:\nCounterfactual generation example in CityWalk. Baseline and\nGrndCtrl\nare conditioned on left-right mirrored actions, and the rollouts are expected to invert the movements of the ground truth. Baseline generation still follows similar left-turn movement as GT (trajectory-following failure), while\nGrndCtrl\ngeneration successfully follow inverted right-turn movement.\nAppendix B\nComparisons with NWM\n[\nbar2025navigationworldmodels\n]\nWhile RLWG is a post-training framework agnostic to the baseline world model, and in\nGrndCtrl\nwe obtain the baseline with supervised fine-tuning, we acknowledge other potential candidates that perform similar tasks as an experimental baseline. This is exemplified by NWM\n[\nbar2025navigationworldmodels\n]\n, which operates a similar world models scenario in navigation tasks. However, significant differences in experimental settings between\nGrndCtrl\nand NWM results in infeasible quantitative comparisons:\nâ€¢\nNWM is jointly-trained on multiple datasets, including RECON\n[\nshah2021rapid\n]\nand HuRoN\n[\nhirose2023sacson\n]\n, which are collected using fish-eye cameras. This results in frequent twisted artifacts in rollouts conditioned on rectilinear images such as SCAND\n[\nkarnan2022sociallycompliantnavigationdataset\n]\n. This biases NWM to perform poorly with visual odometry metrics based on a\nfeed-forward 3D evaluator\nsuch as MapAnything\n[\nkeetha2025mapanythinguniversalfeedforwardmetric\n]\n.\nâ€¢\nGrndCtrl\nis conditioned on actions defined in 6-DOF space\n(\nx\n,\ny\n,\nz\n,\nroll\n,\npitch\n,\nyaw\n)\n(x,y,z,\\mathrm{roll},\\mathrm{pitch},\\mathrm{yaw})\n, whereas NWMâ€™s actions are simplified to 3-DOF space\n(\nx\n,\ny\n,\nyaw\n)\n(x,y,\\mathrm{yaw})\n.\nSince\nGrndCtrl\nadopts MapAnything as an evaluator to obtain rewards, we are unable to directly use NWM as our baseline pretrained world model\nW\nÎ¸\nW_{\\theta}\n. Nevertheless, we include qualitative samples of NWM counterfactual rollouts in Figure\n6\n. We used the same image-action sequences from Figures\n4\nin SCAND, the dataset shared by both our experiments and NWM. We used 3-DOF actions as conditions, then performed the same inversion for the rest of the trajectory in 3-DOF to generate counterfactual rollouts. We observe similar failures in scene drift and trajectory-following.\nFigure 6\n:\nCounterfactual generation example in SCAND. NWM and\nGrndCtrl\nare conditioned on left-right mirrored actions, and the rollouts are expected to invert the movements of the ground truth. NWM generation shows clear sign of scene drift in frames 1-3, ending in a more consistent but different scene than the conditional image.\nAppendix C\nFailure Modes\nWhile\nGrndCtrl\ndemonstrates meaningful grounding in world models, we identify three main failure modes.\nFirst, the lack of pixel-level supervision in our post-training results in gradual increase in pixel-level noises in our rollouts. While we mitigate this with KL-regularization towards the pretrained model, we still observe a gradual increase in visual noises as our post-training increases overall rewards with by improving trajectory-following or mitigating scene drift in rollouts. When unconstrained in post-training iterations, the added visual noise may eventually exceed the noise tolerance of the\nfeed-forward 3D evaluator\n, resulting in invalid rewards used for training, and the model rollouts gradually collapse to pure noises.\nSecond, when a set of rollouts produce similarly good or similarly bad results, our multi-objective normalized group advantage treats the slightly worse good results as negative samples, or the slightly better bad results as positive samples. This results in counterproductive learning in post-training, which causes training instability and gradual collapse to noises. We leave the investigation of rollout variance in world models as future work.\nThird, we occasionally observe reward-hacking behaviors, where rewards increase despite generating noises. Due to the black-box nature of the evaluators and our pipeline stochasticity, we mitigate this by retraining.\nAppendix D\nImplementation Details\nWe first describe supervised fine-tuning of the baseline world model, followed by GRPO post-training and the evaluator setup. Table\n3\nsummarizes the key hyperparameters used in both stages.\nD.1\nSupervised Fine-Tuning\nWe fine-tune the Cosmos-Predict2-2B Video2World backbone starting from the released 2B-720p-16fps checkpoint. The visual encoder and decoder of the latent VAE are kept frozen, and we update all parameters in the DiT backbone.\nBefore settling on this configuration, we experimented with several alternatives: (i) full supervised fine-tuning (all DiT parameters), (ii) LoRA with ranks\n16\n16\nand\n64\n64\n, (iii) a full-size LoRA configuration with\n2048\nÃ—\n2048\n2048\\times 2048\nhidden states, and (iv) training only the action embeddings. Due to Cosmos-Predict2â€™s large-scale pretraining on short clips with mostly mild forward motion, the model is strongly biased toward scene stability rather than rich trajectory dynamics. In practice, only full SFT and the full-size LoRA variant produced meaningful changes in motion behavior; lower-rank LoRA and action-only updates had negligible effect. For simplicity and robustness, we therefore adopt full SFT for all reported experiments.\nSupervision is applied in the latent space using an MSE loss with EDM regularization. During SFT, we perform single-step diffusion and backpropagate only through the DiT backbone, which keeps training computationally efficient. We train for\n20\n20\nk steps with Fully Sharded Data Parallel (FSDP) across eight A100 GPUs. All remaining optimization and diffusion hyperparameters follow the default action-conditioned Cosmos-Predict2 post-training configuration and are listed in Table\n3\n.\nD.2\nGRPO Post-Training\nGRPO post-training is substantially more GPU-memory intensive than SFT because it requires multiple full video rollouts\nx\n^\n1\n:\nT\n\\hat{x}_{1:T}\nper conditioning context while maintaining computation graphs over all diffusion steps to track per-timestep log-probabilities. To fit within memory constraints, we train with a batch size of\n1\n1\nper GPU on eight A100 GPUs and compensate by sampling\nG\n=\n8\nG=8\nstochastic rollouts for each context\nc\n=\n(\nx\n0\n,\na\n0\n:\nT\nâˆ’\n1\n)\nc=(x_{0},a_{0:T-1})\n, which provides sufficient diversity for group-relative advantage estimation.\nA single GRPO update accumulates gradients through the diffusion trajectory to compute per-step likelihood ratios. However, diffusion timesteps do not contribute equally: early steps primarily denoise the main scene content, whereas later steps refine fine-grained details and contribute exponentially smaller likelihood values. Computing likelihood ratios over all steps causes the cumulative log-likelihood to vanish and destabilizes training. In practice, we compute the per-step likelihood ratio only over the first\n60\n%\n60\\%\nof diffusion timesteps while still running the full sampler, which improves both numerical stability and memory efficiency.\nComponent\nHyperparameter\nValue\nOptimization\nSFT/GRPO\nOptimizer\nfused AdamW\nSFT/GRPO\nLearning rate\n1.0\nÃ—\n10\nâˆ’\n4\n1.0\\times 10^{-4}\nSFT/GRPO\nWeight decay\n0.1\n0.1\nSFT/GRPO\nBetas\n(\n0.9\n,\n0.99\n)\n(0.9,0.99)\nSFT/GRPO\nEpsilon\n1.0\nÃ—\n10\nâˆ’\n8\n1.0\\times 10^{-8}\nSFT/GRPO\nLR schedule\nconstant\nDiffusion and sampling\nSFT/GRPO\nPrecision\nbfloat16\nSFT\nClassifier-free guidance\n7.0\n7.0\nSFT/GRPO\nDiffusion timesteps\n35\n35\nSFT\nEDM loss scale\n10.0\n10.0\nSFT\nEDM\nÏƒ\ncond\n\\sigma_{\\text{cond}}\n1.0\nÃ—\n10\nâˆ’\n4\n1.0\\times 10^{-4}\nSFT\nEDM\nÏƒ\ndata\n\\sigma_{\\text{data}}\n1.0\n1.0\nSFT\nEDM high-\nÏƒ\n\\sigma\nratio\n0.0\n0.0\nGRPO\nClassifier-free guidance\n0.0\n0.0\nGRPO\nTimesteps backpropagated\n21\n21\nBatching and hardware\nSFT/GRPO\nGPUs\n8\nÃ—\n8\\times\nA100\nSFT\nBatch size / GPU\n8\n8\nSFT\nEffective batch size\n64\n64\nGRPO\nBatch size / GPU\n1\n1\nGRPO\nEffective batch size\n8\n8\nGRPO\nRollouts per context\nG\nG\n8\n8\nTable 3:\nKey hyperparameters for supervised fine-tuning (SFT) and GRPO post-training.\nD.3\nCounterfactual Actions\nWe define the action conditioning of our model as\n(\nx\n,\ny\n,\nz\n,\nroll\n,\npitch\n,\nyaw\n)\n(x,y,z,\\mathrm{roll},\\mathrm{pitch},\\mathrm{yaw})\n. In practice, we use the absolute poses with respect to the conditional imageâ€™s camera frame for each frame we generate. We follow the standard camera coordinate convention with\nx\nx\nright,\ny\ny\ndown, and\nz\nz\nforward.\nWe perform left-right mirroring of the ground truth actions to obtain the counterfactual actions. Mirroring a trajectory with respect to the image center corresponds to reflecting motion across the plane\nx\n=\n0\nx=0\nand flipping the viewing direction around the optical axis. For the translational part, this reflection is\n(\nx\n,\ny\n,\nz\n)\nâŠ¤\nâ†¦\n(\nâˆ’\nx\n,\ny\n,\nz\n)\nâŠ¤\n(x,y,z)^{\\top}\\;\\mapsto\\;(-x,y,z)^{\\top}\n, which inverts only the lateral component. For small angular magnitudes, the viewing direction can be linearized as\nğ\nâ€‹\n(\npitch\n,\nyaw\n)\nâ‰ˆ\n(\nyaw\n,\npitch\n,\n1\n)\nâŠ¤\n\\mathbf{d}(\\mathrm{pitch},\\mathrm{yaw})\\approx(\\mathrm{yaw},\\mathrm{pitch},1)^{\\top}\n, so mirroring around the optical axis sends\nğ\nâ†¦\n(\nâˆ’\nyaw\n,\nâˆ’\npitch\n,\n1\n)\nâŠ¤\n\\mathbf{d}\\mapsto(-\\mathrm{yaw},-\\mathrm{pitch},1)^{\\top}\n, corresponding to\n(\npitch\n,\nyaw\n)\nâ†¦\n(\nâˆ’\npitch\n,\nâˆ’\nyaw\n)\n(\\mathrm{pitch},\\mathrm{yaw})\\mapsto(-\\mathrm{pitch},-\\mathrm{yaw})\n. Combining these, a 6-DOF action\n(\nx\n,\ny\n,\nz\n,\nroll\n,\npitch\n,\nyaw\n)\n(x,y,z,\\mathrm{roll},\\mathrm{pitch},\\mathrm{yaw})\nhas the counterfactual action\n(\nâˆ’\nx\n,\ny\n,\nz\n,\nroll\n,\nâˆ’\npitch\n,\nâˆ’\nyaw\n)\n(-x,y,z,\\mathrm{roll},-\\mathrm{pitch},-\\mathrm{yaw})\n,\nwhich is the mirror of the original motion with respect to the conditioning imageâ€™s center axis.\nThis method of obtaining counterfactual actions ensure realistic movement of the embodiment as those in the datasets. When conditioning using arbitrary actions, the model rollouts from the pretrained model are more likely to have poorer quality.\nD.4\nEvaluators and Rewards\nMapAnything\n[\nkeetha2025mapanythinguniversalfeedforwardmetric\n]\nserves as our frozen 3D evaluator\nE\nE\n, providing relative pose estimates\n(\nÎ”\nâ€‹\nR\nt\n,\nÎ”\nâ€‹\nğ­\nt\n)\n(\\Delta R_{t},\\Delta\\mathbf{t}_{t})\nand per-frame depth maps\nD\nt\nD_{t}\nfor each rollout\nx\n^\n1\n:\nT\n\\hat{x}_{1:T}\n. VideoAlign\n[\nliu2025improving\n]\nserves as our frozen video evaluator\nV\nV\n, providing sequence-level visual quality, motion quality, and text alignment scores. We use only visual and motion quality scores (with equal weighting\nÎ±\n=\nÎ²\n=\n0.5\n\\alpha=\\beta=0.5\n) for the video quality reward\nr\nv\nr_{\\text{v}}\n, ignoring text alignment since our rollouts are not text-conditioned. When GPU memory is constrained, model parameters of the evaluators are temporarily offloaded to CPU, and re-loaded only when used.",
    "preview_text": "Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.\n\nGrndCtrl: Grounding World Models via Self-Supervised Reward Alignment\nhttps://rlwg-grndctrl.github.io/\nHaoyang He\n1,2\nJay Patrikar\n2\nDong-Ki Kim\n2\nMax Smith\n2\nDaniel McGann\n2\nAli-akbar Agha-mohammadi\n2\nShayegan Omidshafiei\n2\nSebastian Scherer\n1,2\nAbstract\nRecent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce\nReinforcement Learning with Wo",
    "is_relevant": null,
    "relevance_score": 0.0,
    "extracted_keywords": [],
    "one_line_summary": "",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T18:03:29Z",
    "created_at": "2026-01-08T10:08:12.788680",
    "updated_at": "2026-01-08T10:08:12.788689",
    "flag": true
}