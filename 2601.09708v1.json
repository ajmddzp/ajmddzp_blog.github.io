{
    "id": "2601.09708v1",
    "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
    "authors": [
        "Chi-Pin Huang",
        "Yunze Man",
        "Zhiding Yu",
        "Min-Hung Chen",
        "Jan Kautz",
        "Yu-Chiang Frank Wang",
        "Fu-En Yang"
    ],
    "abstract": "è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ä»»åŠ¡éœ€è¦åœ¨å¤æ‚è§†è§‰åœºæ™¯ä¸­è¿›è¡Œæ¨ç†ï¼Œå¹¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ‰§è¡Œé€‚åº”æ€§åŠ¨ä½œã€‚å°½ç®¡è¿‘æœŸå…³äºæ¨ç†å‹VLAçš„ç ”ç©¶è¡¨æ˜ï¼Œæ˜¾å¼çš„æ€ç»´é“¾ï¼ˆCoTï¼‰èƒ½å¤Ÿæå‡æ³›åŒ–èƒ½åŠ›ï¼Œä½†å†—é•¿çš„æ¨ç†è½¨è¿¹ä¼šå¯¼è‡´è¾ƒé«˜çš„æ¨ç†å»¶è¿Ÿã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºFast-ThinkActâ€”â€”ä¸€ç§é«˜æ•ˆæ¨ç†æ¡†æ¶ï¼Œé€šè¿‡å¯è¡¨è¾¾åŒ–çš„æ½œåœ¨æ¨ç†å®ç°ç´§å‡‘ä¸”é«˜æ€§èƒ½çš„è§„åˆ’ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»æ•™å¸ˆæ¨¡å‹ä¸­è’¸é¦å­¦ä¹ ï¼Œä»¥åå¥½å¼•å¯¼ç›®æ ‡é©±åŠ¨ï¼Œå­¦ä¹ åˆ©ç”¨æ½œåœ¨æ€ç»´é“¾è¿›è¡Œé«˜æ•ˆæ¨ç†ï¼Œä»è€Œå¯¹é½æ“ä½œè½¨è¿¹ï¼ŒåŒæ—¶è¿ç§»è¯­è¨€ä¸è§†è§‰è§„åˆ’èƒ½åŠ›ä»¥æ”¯æŒå…·èº«æ§åˆ¶ã€‚è¿™ä½¿å¾—æ¨ç†å¢å¼ºçš„ç­–ç•¥å­¦ä¹ èƒ½å¤Ÿæœ‰æ•ˆè¿æ¥ç´§å‡‘æ¨ç†ä¸åŠ¨ä½œæ‰§è¡Œã€‚åœ¨å¤šæ ·åŒ–çš„å…·èº«æ“ä½œä¸æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼ŒFast-ThinkActåœ¨ä¿æŒé«˜æ•ˆé•¿æ—¶ç¨‹è§„åˆ’ã€å°‘æ ·æœ¬é€‚åº”åŠæ•…éšœæ¢å¤èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†å¼ºåŠ²æ€§èƒ½ï¼Œå…¶æ¨ç†å»¶è¿Ÿè¾ƒå½“å‰æœ€å…ˆè¿›çš„æ¨ç†å‹VLAæ¨¡å‹é™ä½é«˜è¾¾89.3%ã€‚",
    "url": "https://arxiv.org/abs/2601.09708v1",
    "html_url": "https://arxiv.org/html/2601.09708v1",
    "html_content": "\\correspondingauthor\nX\nFast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning\nChi-Pin Huang\n1\nYunze Man\n2\nZhiding Yu\nMin-Hung Chen\nJan Kautz\nYu-Chiang Frank Wang\n1\nFu-En Yang\nNVIDIA\nAbstract\nVision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.\nLinks:\nProject Page\n\\abscontent\n1\nIntroduction\nFigure 1\n:\nOverview of Fast-ThinkAct.\nPrevious reasoning VLAs generate lengthy reasoning traces (\nâˆ¼\n\\sim\n250 tokens). Our approach learns compact continuous tokens (e.g., 6) (\nblue\n) and parallel spatial tokens (\ngreen\n) as internal reasoning. The bottom-right plot shows that we achieve\n9.3\nÃ—\n9.3\\times\nfaster inference than ThinkAct-7B\nhuang2025thinkact\n, while delivering improved performance on the SimplerEnv-Google benchmark.\nRecent large vision-language models (VLMs)\nliu2023visual\n;\ncomanici2025gemini\n;\nliu2024nvila\n;\nbai2025qwen2\n;\nshi2024eagle\n;\nli2025eagle\n;\nchen2025eagle\n;\nwang2025internvl3\n;\nxie2024show\nhave achieved remarkable capabilities in visual-language understanding across diverse multimodal tasks. To extend these capabilities to embodied-centric tasks, recent works leverage large-scale robot demonstrations\no2024open\nto develop Vision-Language-Action (VLA) foundation models\nbrohan2022rt\n;\nbrohan2023rt\n;\nteam2024octo\n;\nbjorck2025gr00t\n;\nli2025hamster\n;\nblack2024pi_0\n;\nyang2025magma\n;\nkim24openvla\n. These VLA tasks require agents to perceive complex visual scenes, reason over spatial and temporal contexts, and execute adaptive actions within dynamic environments, demanding robust long-horizon planning and contextual adaptation. However, as these VLA models primarily rely on supervised training from action data, they excel at basic skills (e.g., pick-and-place) but struggle to generalize beyond training distributions, such as long-horizon planning, self-correction from failures, and adaptation to novel scenarios, due to the impracticality of collecting exhaustive robot demonstrations.\nReasoning VLAs\nzawalski2024robotic\n;\nzhao2025cot\n;\nlee2025molmoact\n;\nwu2025you\n;\nqu2025eo\n;\nhuang2025thinkact\n;\nkim2025robot\naddress these limitations by incorporating intermediate thinking processes, improving generalization and task-solving capability. Supervised chain-of-thought (CoT) methods\nzawalski2024robotic\n;\nzhao2025cot\n;\nlee2025molmoact\n;\nqu2025eo\naddress this by learning from intermediate reasoning annotations. These approaches can be categorized into textual reasoning methods that leverage off-the-shelf LLMs and VLMs to generate pseudo CoT labels\nzawalski2024robotic\n, and visual reasoning methods that generate structured visual reasoning representations such as sub-goal images, image depth, and 2D visual traces\nzhao2025cot\n;\nlee2025molmoact\n. However, these supervised approaches require substantial reasoning annotations and remain limited by training data coverage. To address this, ThinkAct\nhuang2025thinkact\nemploys RL-based reasoning\nshao2024deepseekmath\nto generate long textual CoTs guided by action-aligned visual rewards. While these reasoning methods effectively improve task generalization and planning capabilities, they require generating lengthy chain-of-thought steps that introduce substantial reasoning latency, which hampers embodied applications with\nreal-time\nrequirements.\nIn embodied AI applications such as robotic manipulation and autonomous driving, agents must make rapid decisions at high frequencies (e.g., 1-15 Hz)\nguan2025efficient\n. However, generating lengthy reasoning traces can take several seconds per decision (e.g., 0.1 Hz)\nhuang2025thinkact\n;\nlee2025molmoact\n, creating a critical bottleneck that limits real-time performance\nguan2025efficient\n;\nyu2025survey\nand poses safety risks in time-critical scenarios\nwang2025alpamayo\n. To mitigate this efficiency bottleneck while preserving reasoning capabilities, very recent works\nchen2025training\n;\nyu2025survey\n;\nguan2025efficient\nhave explored approaches to reduce inference latency in embodied reasoning. For instance, ECoT-Lite\nchen2025training\nproposes reasoning dropout to accelerate inference, yet directly reducing textual reasoning length risks performance degradation due to critical information loss. How to preserve reasoning capability while enabling compact representations that properly capture essential spatial-temporal dynamics remains a crucial challenge for reasoning VLA models.\nIn this paper, we propose\nFast-ThinkAct\n, an efficient embodied reasoning framework for Vision-Language-Action tasks that achieves compact yet expressive planning through verbalizable latent reasoning. As depicted in Figure\n1\n, unlike prior reasoning VLAs that generate lengthy explicit textual CoT traces, we introduce reward-guided preference distillation with visual trajectory alignment to compress linguistic and visual planning into compact continuous latents that enable implicit internal reasoning. Our student VLM encodes reasoning into compact latents decodable by a verbalizer, enabling preference-based optimization that leverages RL-derived reward signals to distill high-quality reasoning patterns from a textual teacher VLM while suppressing low-quality ones. We further align trajectory latents between teacher and student to transfer visual planning capabilities essential for embodied control. Once trained, the student VLM enables reasoning-enhanced policy learning that bridges implicit multimodal planning with action execution, achieving significantly faster inference while outperforming existing reasoning VLAs.\nOur contributions can be summarized as follows:\nâ€¢\nWe propose\nFast-ThinkAct\n, an efficient reasoning framework that compresses reasoning into verbalizable latent thoughts while maintaining expressive planning abilities.\nâ€¢\nWe introduce preference-guided distillation with manipulation trajectory alignment that compresses linguistic and visual planning into compact continuous latents.\nâ€¢\nWe bridge high-level visual planning to low-level action execution through reasoning-enhanced policy learning guided by manipulation trajectory latents.\nâ€¢\nWe achieve up to 89.3% inference latency reduction over state-of-the-art reasoning VLAs while maintaining strong performance across diverse embodied benchmarks.\n2\nRelated Works\n2.1\nVision-Language-Action (VLA) Models\nFoundation VLAs.\nVision-Language-Action (VLA) models\nbrohan2022rt\n;\nbrohan2023rt\n;\nteam2024octo\n;\nbjorck2025gr00t\n;\nli2025hamster\n;\nblack2024pi_0\n;\nyang2025magma\n;\npertsch2025fast\n;\ndriess2025knowledge\n;\nbu2025agibot\n;\nteam2025gemini\n;\nwang2025vla\nhave recently emerged as a promising paradigm for embodied AI by training vision-language backbones on large-scale robot demonstrations. Works such as OpenVLA\nkim24openvla\nand\nÏ€\n0\n\\pi_{0}\nblack2024pi_0\nachieve language-conditioned manipulation through end-to-end policy learning, while Magma\nyang2025magma\nco-trains on heterogeneous human and robot data. HAMSTER\nli2025hamster\nand TraceVLA\nzheng2024tracevla\nfurther leverage 2D visual trajectories to boost spatial-action connections. Despite success on routine manipulation, these imitation-based approaches struggle with long-horizon planning and generalization to novel scenarios due to limited training data coverage.\nReasoning VLAs.\nTo overcome these limitations, recent works\nzawalski2024robotic\n;\nzhao2025cot\n;\nlee2025molmoact\n;\nwu2025you\n;\nqu2025eo\n;\nhuang2025thinkact\n;\nkim2025robot\n;\nyuan2025embodied\n;\nabdolmaleki2025gemini\nintegrate explicit reasoning mechanisms into VLA architectures. Supervised approaches\nzawalski2024robotic\n;\nzhao2025cot\n;\nlee2025molmoact\n;\nqu2025eo\nintroduce intermediate reasoning through chain-of-thought annotations. Embodied CoT\nzawalski2024robotic\nand Hi-Robot\nshi2025hi\nsynthesize reasoning labels via pretrained foundation models. To perform vision-centric reasoning\nman2025argus\n;\nsarch2025grounded\nbeyond pure text, CoT-VLA\nzhao2025cot\nemploys visual goal generation and MolmoAct\nlee2025molmoact\nstructures reasoning by spatial representations. Additionally, EO-1\nqu2025eo\nintroduces interleaved vision-language-action pre-training to bridge reasoning and interaction. Recent works\nyuan2025embodied\n;\nhuang2025thinkact\nalternatively leverage reinforcement fine-tuning to generate reasoning chains with designed rewards. Despite improved generalization, these reasoning VLAs suffer from high inference latency and inevitably introduce extraneous information that degrades action quality.\n2.2\nEfficient Reasoning\nTo address the inference latency of reasoning, recent LLM research explores various efficiency techniques\nlee2025vlsi\n;\ndai2025stable\n;\nyuan2025efficient\n;\nxiang2025just\n;\naggarwal2025l1\n;\nlee2025unified\n. For example, RL-based approaches\ndai2025stable\n;\nyuan2025efficient\n;\nxiang2025just\n;\naggarwal2025l1\nintroduce length penalties to encourage shorter reasoning chains, though such methods can suffer from training instability. Beyond length control, latent reasoning methods\nhao2024training\n;\nshen2025codi\n;\nzhang2025soft\n;\ncheng2024compressed\n;\nxu2025softcot\nenable reasoning in continuous spaces, such as Coconut\nhao2024training\nusing hidden states as continuous thoughts, CODI\nshen2025codi\ndistilling explicit CoT into continuous space via teacher-student alignment, and Soft Thinking\nzhang2025soft\ngenerating weighted concept tokens. However, these LLM techniques cannot directly transfer to VLA tasks due to the need for spatial-temporal understanding and bridging semantic reasoning with embodied control.\nRecently, ECoT-Lite\nchen2025training\nproposes reasoning dropout to accelerate embodied reasoning by skipping test-time reasoning traces. However, reasoning dropout can lead to inconsistent planning as it builds on supervised embodied CoT. Our proposed Fast-ThinkAct distills reasoning into compact latent representations that naturally encode multimodal information, enabling robust reasoning-enhanced policy learning.\n3\nMethod\nFigure 2\n:\nOverview of Fast-ThinkAct.\n(a) Given observation\no\nt\no_{t}\nand instruction\nl\nl\n, the Textual Teacher VLM\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\ngenerates explicit reasoning chains. The Latent Student VLM\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\ndistills these into compact latent tokens\nğ³\n\\mathbf{z}\nguided by reward preferences. Verbalizer LLM\nğ’±\nÏˆ\n\\mathcal{V}_{\\psi}\ndecodes latents to text for preference-based learning via\nâ„’\nverb\n\\mathcal{L}_{\\text{verb}}\n, while\nâ„’\ndistill\n\\mathcal{L}_{\\text{distill}}\ntransfers visual planning capability from teacher, and spatial tokens enable parallel visual trajectory prediction via\nâ„’\nans\n\\mathcal{L}_{\\text{ans}}\n, ensuring latents are verbalizable and grounded in visual planning. (b) Reasoning-Enhanced Policy Learning. The Action Model\nÏ€\nÏ•\n\\pi_{\\phi}\nis trained with\nâ„’\nIL\n\\mathcal{L}_{\\text{IL}}\nwhile freezing the latent student\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nand state encoder.\n3.1\nProblem Formulation\nWe first define the setting and notations. At each timestep\nt\nt\n, given a language instruction\nl\nl\n, the model observes a visual input\no\nt\no_{t}\nand generates an action chunk\na\nt\na_{t}\n, represented as a sequence of continuous robot control vectors (e.g., 7- or 14-DOF for single- or bimanual robots, respectively).\nTo address this problem, we propose Fast-ThinkAct, an efficient reasoning framework that bridges high-level planning with low-level action execution. Our approach employs a VLM\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nto perform reasoning in\ncontinuous latent space\n, integrated with an action model\nÏ€\nÏ•\n\\pi_{\\phi}\nfor executable action generation. Specifically,\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nprocesses observation-instruction pairs\n(\no\nt\n,\nl\n)\n(o_{t},l)\nthrough latent chain-of-thought (CoT) reasoning to produce a compact visual plan latent\nc\nt\nc_{t}\nthat encapsulates the intended trajectory in visual space (Sec.\n3.2\n). This\nc\nt\nc_{t}\nsubsequently guides\nÏ€\nÏ•\n\\pi_{\\phi}\nto predict executable actions\na\nt\na_{t}\n(Sec.\n3.3\n). By distilling reasoning into a continuous latent space rather than discrete text, Fast-ThinkAct achieves significantly improved inference efficiency while enhancing action performance through better preservation of spatial and visual information.\n3.2\nEfficient Embodied Reasoning\nTo enable efficient embodied reasoning that meets the real-time requirements of embodied AI tasks, we aim to compress long textual CoTs into a compact set of continuous latent representations. However, compressing reasoning traces into latents is challenging, as there is no direct supervision signal in the latent space to guide what reasoning patterns should be encoded.\n3.2.1\nVerbalizable Latent CoT by Reward Preferences\nTo address this challenge, we propose to perform distillation in natural language space by introducing a verbalizer LLM that decodes latents into verbalizable reasoning. This approach grounds latent learning in an interpretable textual form, ensuring that the learned latents faithfully preserve the underlying reasoning structure. Since reasoning traces generated by the teacher model\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\nexhibit varying quality, we adopt a preference-based learning framework that exploits reward signals from the teacherâ€™s GRPO training to guide the latent student\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\ntoward high-quality reasoning patterns while suppressing low-quality ones.\nSpecifically, we employ a teacher-student framework where a textual teacher model\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\nfirst learns explicit reasoning through GRPO\nshao2024deepseekmath\ntraining by maximizing:\nğ’¥\nGRPO\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nÏ„\nâˆ¼\nâ„±\nÎ¸\nT\nâ€‹\n[\nmin\nâ¡\n(\nr\nÎ¸\nâ€‹\n(\nÏ„\n)\nâ€‹\nA\nâ€‹\n(\nÏ„\n)\n,\nclip\nâ€‹\n(\nr\nÎ¸\nâ€‹\n(\nÏ„\n)\n,\n1\nâˆ’\nÏµ\n,\n1\n+\nÏµ\n)\nâ€‹\nA\nâ€‹\n(\nÏ„\n)\n)\n]\n,\n\\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}_{\\tau\\sim\\mathcal{F}^{T}_{\\theta}}\\Big[\\min\\big(r_{\\theta}(\\tau)A(\\tau),\\text{clip}(r_{\\theta}(\\tau),1-\\epsilon,1+\\epsilon)A(\\tau)\\big)\\Big],\n(1)\nwhere\nÏ„\n\\tau\ndenotes a reasoning trace and\nr\nÎ¸\nâ€‹\n(\nÏ„\n)\n=\nâ„±\nÎ¸\nT\nâ€‹\n(\nÏ„\n)\nâ„±\nold\nT\nâ€‹\n(\nÏ„\n)\nr_{\\theta}(\\tau)=\\frac{\\mathcal{F}^{T}_{\\theta}(\\tau)}{\\mathcal{F}^{T}_{\\text{old}}(\\tau)}\nis the probability ratio. The advantage function for group rewards\n{\nR\ni\n}\ni\nâˆˆ\nG\nâ€‹\n(\nÏ„\n)\n\\{R_{i}\\}_{i\\in G(\\tau)}\nis represented as:\nA\nâ€‹\n(\nÏ„\n)\n=\nR\nÏ„\nâˆ’\nmean\nâ€‹\n(\n{\nR\ni\n}\ni\nâˆˆ\nG\nâ€‹\n(\nÏ„\n)\n)\nstd\nâ€‹\n(\n{\nR\ni\n}\ni\nâˆˆ\nG\nâ€‹\n(\nÏ„\n)\n)\n.\nA(\\tau)=\\frac{R_{\\tau}-\\text{mean}(\\{R_{i}\\}_{i\\in G(\\tau)})}{\\text{std}(\\{R_{i}\\}_{i\\in G(\\tau)})}.\n(2)\nThis training process produces textual CoTs with varying quality, where the advantage function\nA\nâ€‹\n(\nÏ„\n)\nA(\\tau)\nnaturally serves as a quality indicator. To construct preference pairs for distillation, we select the highest and lowest advantage traces from each rollout group:\nÏ„\n+\n=\narg\nâ¡\nmax\nÏ„\nâˆˆ\nG\nâ¡\nA\nâ€‹\n(\nÏ„\n)\nâ€‹\nand\nâ€‹\nÏ„\nâˆ’\n=\narg\nâ¡\nmin\nÏ„\nâˆˆ\nG\nâ¡\nA\nâ€‹\n(\nÏ„\n)\n.\n\\displaystyle\\tau^{+}=\\arg\\max_{\\tau\\in G}A(\\tau)\\text{ and }\\tau^{-}=\\arg\\min_{\\tau\\in G}A(\\tau).\n(3)\nInstead of generating textual tokens, the student model\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nperforms latent reasoning by autoregressively generating\nM\nM\ncontinuous latent vectors\nğ³\n=\n{\nz\nm\n}\nm\n=\n1\nM\n\\mathbf{z}=\\{z_{m}\\}_{m=1}^{M}\nwith\nz\nm\nâˆˆ\nâ„\nd\nz_{m}\\in\\mathbb{R}^{d}\n, where\nd\nd\nis the hidden size. We then train the verbalizer LLM\nğ’±\nÏˆ\n\\mathcal{V}_{\\psi}\nto decode these latents\nğ³\n\\mathbf{z}\ninto natural language. The training objective encourages the verbalizer to assign a higher likelihood to decoding latents into high-quality reasoning\nÏ„\n+\n\\tau^{+}\nthan low-quality reasoning\nÏ„\nâˆ’\n\\tau^{-}\n. Inspired by DPO\nrafailov2023direct\n, we formulate this as an optimization guided by the reward preferences:\nâ„’\nverb\n=\nâˆ’\nğ”¼\nâ€‹\n[\nlog\nâ¡\nÏƒ\nâ€‹\n(\nÎ²\nâ€‹\n(\nlog\nâ¡\np\nÏˆ\nâ€‹\n(\nÏ„\n+\nâˆ£\nğ³\n)\np\nref\nâ€‹\n(\nÏ„\n+\n)\nâˆ’\nlog\nâ¡\np\nÏˆ\nâ€‹\n(\nÏ„\nâˆ’\nâˆ£\nğ³\n)\np\nref\nâ€‹\n(\nÏ„\nâˆ’\n)\n)\n)\n]\n,\n\\mathcal{L}_{\\text{verb}}=-\\mathbb{E}\\Big[\\log\\sigma\\Big(\\beta\\big(\\log\\tfrac{p_{\\psi}(\\tau^{+}\\mid\\mathbf{z})}{p_{\\text{ref}}(\\tau^{+})}-\\log\\tfrac{p_{\\psi}(\\tau^{-}\\mid\\mathbf{z})}{p_{\\text{ref}}(\\tau^{-})}\\big)\\Big)\\Big],\n(4)\nwhere\np\nref\np_{\\text{ref}}\nis the reference model (i.e.,\nğ’±\nÏˆ\n\\mathcal{V}_{\\psi}\nwithout latent conditioning),\nÏƒ\n\\sigma\nis the sigmoid function, and\nÎ²\n=\n0.1\n\\beta=0.1\ncontrols preference strength. This encourages the student VLM\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nto encode latents that the verbalizer decodes into high-quality reasoning while suppressing low-quality patterns.\n3.2.2\nAction-Aligned Visual Plan Distillation\nWhile the verbalizer loss (Eq.\n4\n) enables the student\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nto capture high-level reasoning patterns, it does not explicitly ensure that latent representations encode the visual planning capability crucial for embodied control. To address this, we introduce action-aligned visual plan distillation to transfer the teacher\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\nâ€™s spatial reasoning ability to the student\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\n.\nWe distill spatial reasoning from the teacher, which is trained with trajectory-level rewards (e.g., goal completion and trajectory alignment\nhuang2025thinkact\n) for grounded visual planning. We align the trajectory-level representations by minimizing the L2 distance between hidden states of the\n<answer>\ntoken that encodes the visual plan:\nâ„’\ndistill\n=\nâ€–\nh\nt\nT\nâˆ’\nh\nt\nâ€–\n2\n2\n,\n\\mathcal{L}_{\\text{distill}}=\\|h_{t}^{T}-h_{t}\\|_{2}^{2},\n(5)\nwhere\nh\nt\nT\nh_{t}^{T}\nand\nh\nt\nh_{t}\nare the hidden states from teacher (corresponding to\nÏ„\n+\n\\tau^{+}\n) and student, respectively.\nTo enable efficient parallel trajectory prediction, unlike the textual teacher that autoregressively generates verbose text sequences of waypoints\n{\np\nk\n}\nk\n=\n1\nK\n\\{p_{k}\\}_{k=1}^{K}\nwith\np\nk\nâˆˆ\n[\n0\n,\n1\n]\n2\np_{k}\\in[0,1]^{2}\n(tokenized into 60-70 tokens when\nK\n=\n5\nK=5\n), the student uses\nK\nK\nlearnable spatial tokens\n{\nğ¬\ni\n}\ni\n=\n1\nK\n\\{\\mathbf{s}_{i}\\}_{i=1}^{K}\nappended to the reasoning latent sequence, with each output hidden state simultaneously projected to a waypoint via an MLP. The total objective for training\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\ncombines all three components:\nâ„’\nstudent\n\\displaystyle\\mathcal{L}_{\\text{student}}\n=\nâ„’\nverb\n+\nâ„’\ndistill\n+\nâ„’\nans\n,\nwhere\n\\displaystyle=\\mathcal{L}_{\\text{verb}}+\\mathcal{L}_{\\text{distill}}+\\mathcal{L}_{\\text{ans}},\\quad\\text{where }\n(6)\nâ„’\nans\n\\displaystyle\\mathcal{L}_{\\text{ans}}\n=\nâˆ‘\ni\n=\n1\nK\nâ€–\np\ni\nâˆ’\np\n^\ni\nâ€–\n2\n2\n,\nwith\nâ€‹\np\ni\n=\nMLP\nâ€‹\n(\nh\nâ€²\nâ€‹\n(\nğ¬\ni\n)\n)\n,\n\\displaystyle=\\sum_{i=1}^{K}\\|p_{i}-\\hat{p}_{i}\\|_{2}^{2},\\text{ with }p_{i}=\\text{MLP}(h^{\\prime}(\\mathbf{s}_{i})),\nwhere\nh\nâ€²\nâ€‹\n(\nğ¬\ni\n)\nh^{\\prime}(\\mathbf{s}_{i})\ndenotes the output hidden state of the\ni\ni\n-th spatial token and\np\n^\ni\n\\hat{p}_{i}\nare ground-truth waypoints. Through this unified framework, the student model\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nperforms compact yet expressive latent reasoning and generates visual trajectory plans efficiently.\n3.3\nReasoning-Enhanced Policy Learning\nAfter the student VLM\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nperforms compact latent reasoning and generates visual trajectory planning through spatial tokens, we leverage these representations to guide a diffusion Transformer-based action model\nÏ€\nÏ•\n\\pi_{\\phi}\n(e.g., RDT\nliu2024rdt\n) for action prediction. To bridge the high-level visual planning with low-level action generation, we connect the visual latent planning\nc\nt\nc_{t}\nencoded in the key-value cache corresponding to the spatial tokens to the action model.\nSpecifically, we extract visual latent planning\nc\nt\nc_{t}\nfrom the KV cache of spatial tokens in earlier VLM layers (since\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nhas more layers than\nÏ€\nÏ•\n\\pi_{\\phi}\n) and concatenate with KV pairs from the action modelâ€™s state encoder. The action modelâ€™s cross-attention then attends to both the visual planning context and state observations. We post-train on action-annotated robot data by\nfreezing\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nand the state encoder while updating only\nÏ€\nÏ•\n\\pi_{\\phi}\nwith the imitation learning objective:\nâ„’\nIL\nâ€‹\n(\nÏ•\n)\n=\nâ„“\nâ€‹\n(\nÏ€\nÏ•\nâ€‹\n(\no\nt\n,\nl\n,\nc\nt\n)\n,\na\n^\nt\n)\n,\n\\mathcal{L}_{\\text{IL}}(\\phi)=\\ell\\left(\\pi_{\\phi}(o_{t},l,c_{t}),\\hat{a}_{t}\\right),\n(7)\nwhere\nâ„“\n\\ell\ndenotes the denoising objective for diffusion policy and\na\n^\nt\n\\hat{a}_{t}\nis the ground-truth action. Through this post-training, the action model effectively translates visual planning from compact latent reasoning into low-level robot actions.\nFigure 3\n:\nEvaluation of robot manipulation and reasoning efficiency.\n(a)-(e) Success rates on LIBERO\nliu2023libero\nand SimplerEnv\nli24simpler\nbenchmarks compared with state-of-the-art 7B reasoning VLAs. (f) Latency comparison across 3B and 7B reasoning VLAs. Our approach achieves up to 89.3% inference latency reduction while maintaining superior task success rates.\n3.4\nLearning Strategy and Inference\nTraining Strategy.\nWe initialize both teacher\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\nand student\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nfrom the same checkpoint obtained through SFT and CoT-SFT on a pre-trained VLM. The teacher is trained with GRPO using action-aligned rewards\nhuang2025thinkact\n, while the student is trained with\nâ„’\nstudent\n\\mathcal{L}_{\\text{student}}\nto compress reasoning into compact latents. We then connect the trained\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nwith action model\nÏ€\nÏ•\n\\pi_{\\phi}\n(initialized from\nliu2024rdt\n) by freezing\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nand the state encoder while updating the latent projector and\nÏ€\nÏ•\n\\pi_{\\phi}\nwith\nâ„’\nIL\n\\mathcal{L}_{\\text{IL}}\non large-scale robotic data. For target environment adaptation (e.g., LIBERO\nliu2023libero\n, RoboTwin2.0\nchen2025robotwin2\n), we fine-tune on environment-specific demonstrations.\nInference.\nThe\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nprocesses\n(\no\nt\n,\nl\n)\n(o_{t},l)\nby compact latent reasoning, generating visual trajectories via\nK\nK\nspatial tokens. The visual latent planning\nc\nt\nc_{t}\n, extracted from the spatial tokensâ€™ KV cache, conditions\nÏ€\nÏ•\n\\pi_{\\phi}\nto predict actions\na\nt\na_{t}\n. Inference requires only\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nand\nÏ€\nÏ•\n\\pi_{\\phi}\n; the verbalizer\nğ’±\nÏˆ\n\\mathcal{V}_{\\psi}\nis used solely during training and optionally for interpretability.\n4\nExperiment\n4.1\nExperimental Setup\nImplementation Details.\nWe use Qwen2.5-VL 3B\nbai2025qwen2\nas the VLM backbone. The SFT stage runs for 1 epoch with batch size 64 and learning rate\n1\nâ€‹\ne\nâˆ’\n5\n1\\text{e}{-5}\n, followed by CoT-SFT for 15K iterations with the same hyperparameters. For teacher-student training, both\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\nand\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nare initialized from the CoT-SFT checkpoint and trained for 4,500 iterations with batch size 128 and learning rate\n1\nâ€‹\ne\nâˆ’\n6\n1\\text{e}{-6}\n. The teacher is optimized with GRPO\nshao2024deepseekmath\nusing action-aligned visual rewards\nhuang2025thinkact\nand QA-style rewards (detailed in supplementary material). For the first 3,000 iterations of the student training, we train the verbalizer\nğ’±\nÏˆ\n\\mathcal{V}_{\\psi}\nwith standard language modeling loss, then switch to\nâ„’\nverb\n\\mathcal{L}_{\\text{verb}}\nfor the remaining 1,500 iterations. For reasoning-enhanced policy learning, we initialize\nÏ€\nÏ•\n\\pi_{\\phi}\nfrom DiT-Policy\nchi2023diffusion\npre-trained on OXE\no2024open\nfor SimplerEnv, and from RDT\nliu2024rdt\nfor LIBERO and RoboTwin2.0. A linear projection adapts the VLMâ€™s KV cache to the action model dimension (1,024 for DiT-Policy and 2,048 for RDT). Training runs for 20K iterations with batch size 256 and learning rate\n1\nâ€‹\ne\nâˆ’\n4\n1\\text{e}{-4}\n. All diffusion hyperparameters follow those of the respective action models. All experiments are conducted on 16 NVIDIA A100 GPUs with 80 GB memory.\nTraining Datasets and Evaluation Benchmarks.\nFor reasoning VLM training, we utilize single-arm visual trajectories labeled by\nlee2025molmoact\nand dual-arm visual trajectories from the AIST dataset\naist2025aist\n, along with QA tasks from PixMo\ndeitke2024molmo\n, RoboFAC\nlu2025robofac\n, RoboVQA\nsermanet2024robovqa\n, ShareRobot\nji2025robobrain\n, EgoPlan\nchen2023egoplan\n, and Video-R1\nfeng2025video\n. For reasoning-enhanced policy learning, we use action data from the OXE dataset\no2024open\n(following OpenVLA\nkim24openvla\n) when training with DiT-Policy, and augment with bimanual data from the static Aloha dataset\nshi2023waypoint\n;\nzhao2023learning\nwhen training with RDT.\nWe evaluate Fast-ThinkAct on four embodied reasoning benchmarks and three robot manipulation benchmarks. For embodied reasoning, we use EgoPlan-Bench2\nqiu2024egoplan2\n(accuracy on multiple-choice questions), RoboVQA\nsermanet2024robovqa\n(BLEU score\npapineni2002bleu\n), OpenEQA\nmajumdar2024openeqa\n, and RoboFAC\nlu2025robofac\n(both using LLM-based scoring). Notably, RoboVQA and RoboFAC contain videos captured from real robots. For robot manipulation, we evaluate on SimplerEnv\nli24simpler\n, which demonstrates strong correlation with real-world performance, LIBERO\nliu2023libero\ncovering diverse manipulation tasks including long-horizon scenarios, and RoboTwin2.0\nchen2025robotwin2\nfor complex bimanual manipulation. All robot manipulation tasks use task success rate as the metric. Additional details are provided in the supplementary material.\n4.2\nQuantitative Evaluation\nRobot Manipulation.\nTable 1\n:\nQuantitative evaluation on RoboTwin2.0\nchen2025robotwin2\n.\nE and H denote easy and hard settings (without/with domain randomization). Background colors indicate task length based on expert demonstrations:\nshort (80-100)\n,\nmedium (110-220)\n,\nlong (270-470)\nsteps.\nModel\nclick\nalarm\nclick\nbell\nturn\nswitch\nadjust\nbottle\nbeat\nblock\nhandover\nmic\nhandover\nblock\nhanging\nmug\nstack\nblocks two\nstack\nbowls three\nAverage\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nDP\nchi2023diffusion\n61\n5\n54\n0\n36\n1\n97\n0\n42\n0\n53\n0\n10\n0\n8\n0\n7\n0\n63\n0\n43.1\n0.6\nACT\nzhao2023learning\n32\n4\n58\n3\n5\n2\n97\n23\n56\n3\n85\n0\n42\n0\n7\n0\n25\n0\n48\n0\n45.5\n3.5\nÏ€\n0\n\\pi_{0}\nblack2024pi_0\n63\n11\n44\n3\n27\n23\n90\n56\n43\n21\n98\n13\n45\n8\n11\n3\n42\n1\n66\n24\n52.9\n16.3\nRDT\nliu2024rdt\n61\n12\n80\n9\n35\n15\n81\n75\n77\n37\n90\n31\n45\n14\n23\n16\n21\n2\n51\n17\n56.4\n22.8\nThinkAct\nhuang2025thinkact\n64\n13\n84\n11\n40\n19\n94\n70\n79\n33\n92\n40\n56\n15\n31\n18\n30\n5\n54\n23\n62.4\n24.7\nFast-ThinkAct\n70\n17\n82\n12\n37\n21\n92\n72\n82\n33\n99\n42\n65\n15\n30\n22\n45\n5\n55\n25\n65.7\n26.4\nTable 2\n:\nQuantitative evaluation on EgoPlan-Bench2\nqiu2024egoplan2\n, RoboVQA\nsermanet2024robovqa\n, and OpenEQA\nmajumdar2024openeqa\nbenchmarks for embodied reasoning.\nMethod\nEgoPlan-Bench2\nRoboVQA\nOpenEQA\nOverall\nDaily.\nWork.\nRec.\nHobbies\nAvg.\nB-1\nB-2\nB-3\nB-4\nB-Avg.\nScore\nAvg.\nGPT-4V\nachiam2023gpt\n36.7\n27.7\n33.9\n32.5\n32.6\n32.2\n26.5\n24.7\n23.9\n26.8\n49.6\n36.4\nGemini-2.5-Flash\ncomanici2025gemini\n44.2\n42.3\n43.2\n39.1\n42.4\n39.1\n31.6\n22.9\n22.1\n28.9\n45.3\n38.9\nInternVL2.5-2B\nchen2024expanding\n30.9\n27.8\n28.6\n33.1\n30.1\n36.6\n33.7\n31.0\n29.4\n32.7\n47.1\n36.6\nInternVL3-2B\nzhu2025internvl3\n36.9\n29.9\n35.6\n31.5\n33.4\n34.4\n33.9\n33.5\n33.3\n33.8\n48.8\n38.7\nNVILA-2B\nliu2024nvila\n34.6\n26.7\n33.3\n31.6\n31.4\n38.7\n34.3\n31.1\n29.2\n33.3\n47.0\n37.2\nQwen2.5-VL-3B\nbai2025qwen2\n29.0\n27.0\n30.2\n28.9\n28.5\n42.5\n36.3\n28.7\n31.8\n34.8\n43.4\n35.6\nMagma-8B\nyang2025magma\n32.1\n25.7\n34.4\n29.3\n29.8\n38.6\n31.5\n28.1\n26.7\n31.2\n49.1\n36.7\nRoboBrain2.0-3B\nteam2025robobrain2\n45.3\n37.6\n45.9\n39.7\n41.8\n54.4\n47.7\n43.1\n41.0\n46.5\n50.1\n46.1\nThinkAct-3B\nhuang2025thinkact\n46.6\n41.4\n45.9\n42.5\n44.0\n62.4\n57.3\n52.0\n49.6\n55.3\n48.9\n49.4\nFast-ThinkAct-3B\n50.3\n44.3\n46.4\n43.2\n46.4\n70.1\n63.0\n57.2\n53.0\n60.8\n51.2\n52.8\nWe evaluate Fast-ThinkAct on robotic manipulation using LIBERO\nliu2023libero\nand SimplerEnv\nli24simpler\nbenchmarks. LIBERO covers diverse subtasks, including Spatial, Object, Goal, and Long, while SimplerEnv provides a simulated benchmark with strong real-world correlation, featuring variations in lighting, object appearance, and camera viewpoints. As shown in Fig.\n3\n(a)-(e), Fast-ThinkAct consistently outperforms all baselines, achieving the highest success rates across all LIBERO subtasks and SimplerEnv-Google. This includes substantial improvements over foundation VLAs such as OpenVLA\nkim24openvla\n, and reasoning VLAs including CoT-VLA\nzhao2025cot\n, ThinkAct\nhuang2025thinkact\n, and MolmoAct\nlee2025molmoact\n. Moreover, as shown in Fig.\n3\n(f), our compact latent reasoning achieves 89.3% and 88.0% latency reduction compared to ThinkAct-7B\nhuang2025thinkact\nand MolmoAct-7B\nlee2025molmoact\nrespectively, and\n7\nÃ—\n7\\times\nfaster inference than ThinkAct-3B, demonstrating substantial efficiency gains without sacrificing performance.\nTo further validate Fast-ThinkAct on more complex scenarios, we evaluate on RoboTwin2.0\nchen2025robotwin2\n, a challenging bimanual manipulation benchmark requiring long-horizon planning. As shown in Tab.\n1\n, Fast-ThinkAct significantly outperforms previous VLAs including DP\nchi2023diffusion\n, ACT\nzhao2023learning\n,\nÏ€\n0\n\\pi_{0}\nblack2024pi_0\n, RDT\nliu2024rdt\n, and ThinkAct\nhuang2025thinkact\nacross both easy and hard settings. Compared to RDT, Fast-ThinkAct achieves 9.3% and 3.6% higher success rates on easy and hard settings, respectively. Against the reasoning VLA ThinkAct, it improves success by 3.3% and 1.7% while maintaining substantially higher efficiency, as shown in Fig.\n3\n(f). These results demonstrate that our compact reasoning design enables both superior accuracy and computational efficiency on complex bimanual manipulation tasks.\nEmbodied Reasoning.\nIn Tab.\n2\n, we evaluate the reasoning capabilities of Fast-ThinkAct in embodied scenarios across three benchmarks: EgoPlan-Bench2\nqiu2024egoplan2\n, RoboVQA\nsermanet2024robovqa\n, and OpenEQA\nmajumdar2024openeqa\n. These benchmarks assess multi-step planning in egocentric everyday scenarios, long-horizon reasoning for robotic manipulation tasks, and zero-shot understanding of embodied scenes in diverse environments, respectively. We observed that, Fast-ThinkAct surpasses all comparison methods, including two proprietary models (i.e., GPT-4V\nachiam2023gpt\nand Gemini-2.5-Flash\ncomanici2025gemini\n), exceeding the runner-up by 2.4% on EgoPlan-Bench2, 5.5 BLEU score on RoboVQA, and 1.1 points on OpenEQA. These results demonstrate that Fast-ThinkAct effectively handles complex planning sequences and extended reasoning horizons while generalizing to novel environments, showcasing robust capabilities for scene comprehension and multi-step task execution in embodied AI applications.\nFigure 4\n:\nVisualization of predicted visual trajectories and action execution results on long-horizon tasks.\nExamples from (a) SimplerEnv-Google, (b) LIBERO-Long, and (c) RoboTwin2.0-Hard with long (278) steps. Yellow traces indicate single-arm/left gripper trajectories; red traces indicate right gripper trajectories for bimanual tasks.\nFigure 5\n:\nFailure recovery capability on RoboFAC\nlu2025robofac\n.\nLeft: Qualitative examples (from both simulation and real robot) of corrective guidance for manipulation errors. Right: Quantitative evaluation on simulation (RoboFAC-Sim) and real-robot (RoboFAC-Real) settings.\n4.3\nAnalysis of Fast-ThinkAct\nReasoning Enables Long-Horizon Planning.\nFigure 6\n:\nFew-shot adaptation results on RoboTwin2.0 benchmark.\nWe use 10 demonstrations per task for fine-tuning.\nWe analyze Fast-ThinkActâ€™s capability on long-horizon tasks in Tab.\n1\nand Fig.\n4\n. We focus on long-horizon tasks (average length exceeding 270 steps) in RoboTwin2.0\nchen2025robotwin2\nthat require multi-step reasoning and extended planning horizons. As shown in Tab.\n1\n, Fast-ThinkAct achieves average scores of 48.8 and 16.8 on easy and hard settings of long-horizon tasks, respectively, surpassing RDT (35.0/12.3) and ThinkAct (42.8/15.3). Fig.\n4\nvisualizes predicted 2D visual traces and execution results on representative tasks from SimplerEnv-Google\nli24simpler\n, LIBERO-Long\nliu2023libero\n, and RoboTwin2.0\nchen2025robotwin2\n. For example, the LIBERO-Long task requires sequentially turning on the stove and placing a moka pot on it, while the RoboTwin2.0 handover task requires bimanual coordination to transfer a block between grippers. The visual traces successfully predict feasible solution paths, with their corresponding representations serving as visual planning guidance for successful execution. These results demonstrate that our compact latent reasoning effectively supports long-horizon planning in complex manipulation scenarios.\nReasoning Enables Failure Recovery.\nFigure 7\n:\nReasoning trace comparison on RoboVQA.\n(a) Teacherâ€™s textual reasoning. (b) Studentâ€™s verbalized latent reasoning.\nGreen\n: relevant content;\norange\n: less relevant content.\nA key advantage of reasoning-based VLAs\nhuang2025thinkact\n;\nabdolmaleki2025gemini\nis their ability to identify runtime failures and provide corrective guidance for recovery. To evaluate this capability, we conduct experiments on RoboFAC\nlu2025robofac\n, a benchmark specifically designed to assess failure identification and correction in embodied VLMs. As shown in Fig.\n5\n, Fast-ThinkAct substantially outperforms the second-best baseline RoboFAC-3B\nlu2025robofac\nby 10.9 points on the simulation split and 16.4 points on the real-world split. The qualitative examples demonstrate Fast-ThinkActâ€™s ability to reason over manipulation videos, identify failures, and propose recovery steps. For instance, in the right example where the target object drops mid-execution, Fast-ThinkAct generates a concrete recovery plan: first moving the arm backward to create space, then adjusting laterally to align with the target object, and finally lowering to the appropriate height for a secure grasp. These results demonstrate that our latent reasoning supports both fast task execution and crucial failure analysis capabilities essential for robust robotic manipulation.\nReasoning Enables Few-Shot Adaptation.\nTable 3\n:\nAblation study of training objectives and learning stages.\nNote that Fast-ThinkAct w/o\nâ„’\nverb\n,\nâ„’\ndistill\n\\mathcal{L}_{\\text{verb}},\\mathcal{L}_{\\text{distill}}\ndenotes the student VLM\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\ntrained without the corresponding loss components.\nMethod\nEgoPlan\nRoboVQA\nOpenEQA\nAverage\nFast-ThinkAct\n46.4\n60.8\n51.2\n52.8\nw/o\nâ„’\nverb\n\\mathcal{L}_{\\text{verb}}\n42.1\n53.8\n49.5\n48.5\nw/o\nâ„’\nverb\n,\nâ„’\ndistill\n\\mathcal{L}_{\\text{verb}},\\mathcal{L}_{\\text{distill}}\n41.6\n52.7\n48.9\n47.7\nTextual Teacher\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\n41.7\n58.2\n49.4\n49.8\nSFT + CoT-SFT\n40.0\n46.1\n48.8\n45.0\nSFT only\n40.5\n53.6\n45.3\n46.5\nTo assess how reasoning capability improves few-shot adaptation, we conduct few-shot experiments on the RoboTwin2.0 benchmark\nchen2025robotwin2\n, fine-tuning models using only 10 demonstrations per task. As illustrated in Fig.\n6\n, Fast-ThinkAct significantly enhances our adopted action model RDT\nliu2024rdt\nand outperforms the state-of-the-art VLAs, including\nÏ€\n0\n\\pi_{0}\nblack2024pi_0\nand ThinkAct\nhuang2025thinkact\non both medium and long-horizon tasks. Notably, our method achieves these gains while operating with significantly lower reasoning latency compared to ThinkAct, highlighting the advantage of efficient yet effective reasoning for few-shot action adaptation in complex robot manipulation scenarios.\nVisualization of Verbalizable Latent Reasoning.\nIn Fig.\n7\n, we compare the teacherâ€™s textual reasoning with the studentâ€™s verbalized latent reasoning on RoboVQA. While both capture task-relevant information (\ngreen\n), the teacher generates verbose outputs with less directly relevant content (\norange\n), whereas our student produces more concise and focused responses when verbalized. This demonstrates that our preference-guided distillation not only reduces computational cost but also distills concise reasoning patterns while filtering out redundant information.\nAblation Study.\nIn Tab.\n3\n, we ablate training stages and loss components. Starting from the full Fast-ThinkAct, removing\nâ„’\nverb\n\\mathcal{L}_{\\text{verb}}\ncauses performance drops as latent CoTs lack preference-based guidance to align with high-quality reasoning and suppress low-quality patterns. Further removing\nâ„’\ndistill\n\\mathcal{L}_{\\text{distill}}\nleads to additional decline, indicating that aligning trajectory-level representations is crucial for transferring visual planning capabilities. Comparing training strategies, CoT-SFT underperforms SFT on EgoPlan-Bench2 and RoboVQA but improves on OpenEQA, suggesting naÃ¯ve chain-of-thought supervision benefits open-ended QA but introduces verbosity that hinders structured reasoning tasks. Our preference-guided approach distills high-quality reasoning while maintaining efficiency. This validates the necessity of our proposed distillation framework and visual trajectory alignment. We provide additional ablation studies in the supplementary material.\n5\nConclusion\nWe presented\nFast-ThinkAct\n, an efficient reasoning framework for vision-language-action tasks that achieves compact yet expressive planning through verbalizable latent reasoning. By distilling lengthy textual reasoning into compact latent representations via preference-guided distillation and visual trajectory alignment, our approach bridges high-level embodied reasoning with low-level action execution through reasoning-enhanced policy learning. Extensive experiments across diverse robotic manipulation and embodied reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with significantly reduced inference latency while enabling effective long-horizon planning, few-shot adaptation, and failure recovery capabilities.\nLimitations and Future Works.\nAs our verbalizer\nğ’±\nÏˆ\n\\mathcal{V}_{\\psi}\nis built upon a pre-trained LLM, it inevitably inherits language model limitations, including hallucination, occasionally producing plausible but inaccurate descriptions. However, this does not affect action execution during inference, as the verbalizer serves only for interpretability while action prediction uses the grounded latent representations from visual plan distillation. To further improve the faithfulness of verbalized reasoning, we can consider incorporating grounding-aware objectives or hallucination suppression techniques in future work.\nAppendix A\nAdditional Experimental Setup\nA.1\nAlgorithm\nInput:\nCoT-SFT checkpoint\nâ„±\nÎ¸\n0\n\\mathcal{F}_{\\theta_{0}}\n, training data\nğ’Ÿ\n\\mathcal{D}\n, rollout size\nN\nN\n, latent reasoning steps\nM\nM\n, number of waypoints\nK\nK\n, total iterations\nT\ntotal\nT_{\\text{total}}\nOutput:\nTrained student model\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\n//\nInitialize models\nâ„±\nÎ¸\nT\nâ†\nâ„±\nÎ¸\n0\n\\mathcal{F}_{\\theta}^{T}\\leftarrow\\mathcal{F}_{\\theta_{0}}\n,\nâ„±\nÎ¸\nâ†\nâ„±\nÎ¸\n0\n\\mathcal{F}_{\\theta}\\leftarrow\\mathcal{F}_{\\theta_{0}}\n;\nInitialize verbalizer\nğ’±\nÏˆ\n\\mathcal{V}_{\\psi}\nfrom pre-trained LLM;\nt\nâ†\n0\nt\\leftarrow 0\n;\nwhile\nt\n<\nT\ntotal\nt<T_{\\text{total}}\ndo\nSample batch\n(\no\n,\nl\n,\np\n^\n)\n(o,l,\\hat{p})\nfrom\nğ’Ÿ\n\\mathcal{D}\n;\n//\nSuppose bs=1 for simplicity\n0.5em\n//\nTeacher GRPO training\nGenerate\nN\nN\nrollouts\n{\nÏ„\ni\n}\ni\n=\n1\nN\n\\{\\tau_{i}\\}_{i=1}^{N}\nfrom\nâ„±\nÎ¸\nT\nâ€‹\n(\no\n,\nl\n)\n\\mathcal{F}_{\\theta}^{T}(o,l)\n;\nCompute trajectory rewards\n{\nr\ni\n}\ni\n=\n1\nN\n\\{r_{i}\\}_{i=1}^{N}\n;\nCompute group-wise advantages\n{\nA\ni\n}\ni\n=\n1\nN\n\\{A_{i}\\}_{i=1}^{N}\n;\nUpdate\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\nwith\nğ’¥\nGRPO\n\\mathcal{J}_{\\text{GRPO}}\n(Eq. 1);\nÏ„\n+\nâ†\narg\nâ¡\nmax\ni\nâ¡\nA\ni\n\\tau^{+}\\leftarrow\\arg\\max_{i}A_{i}\n,\nÏ„\nâˆ’\nâ†\narg\nâ¡\nmin\ni\nâ¡\nA\ni\n\\tau^{-}\\leftarrow\\arg\\min_{i}A_{i}\n(Eq. 3) ;\n//\nFor student distillation\nh\nt\nT\nâ†\nh_{t}^{T}\\leftarrow\nhidden state of\nÏ„\n+\n\\tau^{+}\nat\n<answer>\ntoken from\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\n;\n//\nFor distillation loss\n//\nStudent latent distillation\nğ³\n=\n{\nz\nm\n}\nm\n=\n1\nM\nâ†\nâ„±\nÎ¸\nâ€‹\n(\no\n,\nl\n)\n\\mathbf{z}=\\{z_{m}\\}_{m=1}^{M}\\leftarrow\\mathcal{F}_{\\theta}(o,l)\n;\n//\nPerform auto-regressive latent reasoning\nCompute\nâ„’\nverb\n\\mathcal{L}_{\\text{verb}}\nwith\nğ³\n\\mathbf{z}\n,\nğ’±\nÏˆ\n\\mathcal{V}_{\\psi}\n,\nÏ„\n+\n\\tau^{+}\n,\nÏ„\nâˆ’\n\\tau^{-}\n(Eq. 4);\nForward\nK\nK\nspatial tokens from\nâ„±\nÎ¸\nâ€‹\n(\no\n,\nl\n,\nğ³\n)\n\\mathcal{F}_{\\theta}(o,l,\\mathbf{z})\nto obtain\nh\nt\nh_{t}\nand\n{\nh\nâ€²\nâ€‹\n(\nğ¬\ni\n)\n}\ni\n=\n1\nK\n\\{h^{\\prime}(\\mathbf{s}_{i})\\}_{i=1}^{K}\n;\nCompute\nâ„’\ndistill\n\\mathcal{L}_{\\text{distill}}\nwith\nh\nt\nT\nh_{t}^{T}\n,\nh\nt\nh_{t}\n(Eq. 5);\nCompute\nâ„’\nans\n\\mathcal{L}_{\\text{ans}}\nwith\n{\nh\nâ€²\nâ€‹\n(\nğ¬\ni\n)\n}\ni\n=\n1\nK\n\\{h^{\\prime}(\\mathbf{s}_{i})\\}_{i=1}^{K}\n,\np\n^\n\\hat{p}\n(Eq. 6);\nUpdate\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nwith\nâ„’\nstudent\n=\nâ„’\nverb\n+\nâ„’\ndistill\n+\nâ„’\nans\n\\mathcal{L}_{\\text{student}}=\\mathcal{L}_{\\text{verb}}+\\mathcal{L}_{\\text{distill}}+\\mathcal{L}_{\\text{ans}}\n;\nt\nâ†\nt\n+\n1\nt\\leftarrow t+1\n;\nend while\nreturn\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\n;\nAlgorithmÂ 1\nTraining Fast-ThinkAct (Sec. 3.2)\nAlgorithm\n1\npresents the complete training procedure corresponding to Sec. 3.2. It shows how we jointly optimize the teacher model with GRPO and distill its reasoning into the studentâ€™s compact latent representations.\nA.2\nImplementation Details\nOur implementation follows the setup described in Sec. 4.1 of the main paper. Here we provide additional details. The verbalizer\nğ’±\nÏˆ\n\\mathcal{V}_{\\psi}\nis initialized from a small LLM, Qwen3-0.6B, with cross-attention layers inserted at each layer to condition on latent CoTs\nğ³\n\\mathbf{z}\n. For the student model training, in the first 3,000 iterations, we replace verbalization loss\nâ„’\nverb\n\\mathcal{L}_{\\text{verb}}\nwith language modeling loss using\nÏ„\n+\n\\tau^{+}\nas ground truth to warm up\nğ’±\nÏˆ\n\\mathcal{V}_{\\psi}\nâ€™s alignment with the latent representations\nğ³\n\\mathbf{z}\n. We then freeze\nğ’±\nÏˆ\n\\mathcal{V}_{\\psi}\nand use the\nâ„’\nverb\n\\mathcal{L}_{\\text{verb}}\nfor the remaining 1,500 iterations. The student\nâ„±\nÎ¸\n\\mathcal{F}_{\\theta}\nis optimized throughout both phases. For waypoint prediction in Eq. 6, each\np\ni\nâˆˆ\nâ„\n6\np_{i}\\in\\mathbb{R}^{6}\nencodes coordinates in the format\n[\nx\nsingle\n,\ny\nsingle\n,\nx\nleft\n,\ny\nleft\n,\nx\nright\n,\ny\nright\n]\n[x_{\\text{single}},y_{\\text{single}},x_{\\text{left}},y_{\\text{left}},x_{\\text{right}},y_{\\text{right}}]\n, where the first two dimensions are for single-arm and the last four are for bimanual robot. For ground-truth\np\n^\nâ€‹\ni\n\\hat{p}i\n, we fill the corresponding dimensions based on robot type and mask out the unused dimensions when computing\nâ„’\nans\n\\mathcal{L}_{\\text{ans}}\n. For GRPO training, we follow the configuration of ThinkAct\nhuang2025thinkact\n, using rollout size\nN\n=\n5\nN=5\n. Following\nlee2025molmoact\n, we set the number of waypoints in trajectory to\nK\n=\n5\nK=5\n. We use\nM\n=\n6\nM=6\nlatent reasoning tokens, with ablation study provided in Fig.\n8\n.\nDuring reasoning-enhanced policy learning, for SimplerEnv\nli24simpler\nevaluation, to ensure fair comparison with previous works\nkim24openvla\n;\nlee2025molmoact\n, we initialize\nÏ€\nÏ•\n\\pi_{\\phi}\nfrom DiT-Policy\nchi2023diffusion\npre-trained on the same OXE dataset\no2024open\n;\nkim24openvla\nand conduct reasoning-enhanced policy learning (Sec. 3.3) using the same OXE data. For LIBERO\nliu2023libero\nand RoboTwin2.0\nchen2025robotwin2\nevaluations, we initialize\nÏ€\nÏ•\n\\pi_{\\phi}\nfrom RDT\nliu2024rdt\n, which has demonstrated strong performance on RoboTwin2.0, and conduct policy learning using OXE\no2024open\nand static ALOHA datasets\nshi2023waypoint\n;\nzhao2023learning\n. Our method further enhances RDTâ€™s manipulation capabilities on both benchmarks. The use of different action models also demonstrates that our approach is agnostic to the underlying action model choice.\nA.3\nTraining Data Details\nA.3.1\nDataset Sources\n2D Visual Trace of Manipulation Tasks.\nFor single-arm manipulation, we utilize 2D visual trajectories labeled by MolmoAct\nlee2025molmoact\nfrom the Open X-Embodiment (OXE) dataset\no2024open\n, comprising approximately 1.3M trajectories. For bimanual manipulation, we extract dual-arm visual trajectories from the AIST dataset\naist2025aist\n, resulting in approximately 92K trajectory samples. Specifically, we first use Molmo-72B\ndeitke2024molmo\nto detect left and right gripper positions (following\nlee2025molmoact\n) in the first frame, then apply CoTracker3\nkaraev2025cotracker3\nto track and parse the manipulation trajectories throughout the video sequences.\nRoboFAC\nlu2025robofac\n.\nRoboFAC is a robotic failure analysis dataset containing 9,440 erroneous manipulation trajectories across 16 tasks in both simulated and real-world environments. We utilize the training set with 64K QA pairs covering various failure types for developing failure identification and correction planning capabilities.\nRoboVQA\nsermanet2024robovqa\n.\nRoboVQA contains robot manipulation videos with QA tasks covering task understanding. The dataset includes approximately 5K long-horizon and 92K medium-horizon video sequences from diverse robotic platforms, resulting in total 798K QA pairs. Videos are annotated with multiple questions probing spatial reasoning, action prediction, and task comprehension.\nShareRobot\nji2025robobrain\n.\nShareRobot is a large-scale dataset collected by RoboBrain\nji2025robobrain\n, containing over 1M QA pairs covering task planning, object affordances, and manipulation strategies across diverse robot embodiments and scenes. The dataset features fine-grained annotations linking task descriptions to frame-level execution details, facilitating learning of transferable manipulation knowledge.\nEgoPlan-Bench\nchen2023egoplan\n.\nEgoPlan-Bench features egocentric videos of daily activities annotated with task planning information including goals, execution history, and current states. The dataset contains approximately 53K video-text pairs for training long-horizon planning and progress tracking capabilities from egocentric view.\nVideo-R1-CoT\nfeng2025video\n.\nVideo-R1 comprises 165K video question-answer pairs with chain-of-thought reasoning annotations generated by large-scale vision-language models. The dataset covers diverse reasoning domains including mathematical logic, spatial understanding, OCR, and visual analytics. All samples are quality-filtered to ensure annotation consistency and correctness.\nPixMo\ndeitke2024molmo\n.\nPixMo is a general-purpose vision-language dataset with diverse image captions and question-answer pairs. Following MolmoAct\nlee2025molmoact\n, we incorporate PixMo dataset to preserve general visual understanding and prevent catastrophic forgetting when training on embodied dataset. Specifically, we use approximately 726K samples from the\nask_model_anything\n,\ncap\n, and\ncap-qa\nsplits.\nA.3.2\nData Processing and Formatting\nSupervised Fine-Tuning (SFT).\nTo enhance foundational embodied knowledge, we perform supervised fine-tuning on approximately 4M samples combining 2D visual trajectories from MolmoAct\nlee2025molmoact\nand AIST\naist2025aist\n, along with QA data from PixMo\ndeitke2024molmo\n, RoboFAC\nlu2025robofac\n, RoboVQA\nsermanet2024robovqa\n, ShareRobot\nji2025robobrain\n, and EgoPlan\nchen2023egoplan\n. This stage enables the model to acquire basic visual understanding, task comprehension, and manipulation knowledge across diverse embodiments and scenarios.\nChain-of-Thought SFT (CoT-SFT).\nTo develop reasoning capabilities while preserving embodied understanding, we sample 5% from the SFT data (approximately 200K samples) and augment with 165K samples from Video-R1-CoT\nfeng2025video\n. For data with CoT annotations, we format prompts to elicit structured reasoning enclosed in\n<think>\ntags followed by answers in\n<answer>\ntags; for data without CoT annotations, we prompt for direct answers only. This enables the model to learn reasoning capabilities from CoT-annotated data and generalize them to embodied tasks.\nTeacher-Student Training.\nBuilding upon the CoT-SFT checkpoint, we curate a balanced training set by sampling approximately 5,000 instances from each dataset and data type, totaling nearly 50K samples. We adopt the prompt formatting strategy from CoT-SFT for both teacher GRPO training and student latent distillation. We train both the teacher with GRPO and the student with latent distillation (as detailed in Sec. 3.2) on this data, efficiently transferring high-quality reasoning patterns into compact latent representations.\nTable 4\n:\nQuantitative results with larger model size (7B or 8B) on embodied reasoning benchmarks.\nMethod\nEgoPlan-Bench2\nRoboVQA\nOpenEQA\nOverall\nDaily.\nWork.\nRec.\nHobbies\nAvg.\nB-1\nB-2\nB-3\nB-4\nB-Avg.\nScore\nAvg.\nInternVL2.5-8B\nchen2024expanding\n36.2\n28.7\n34.4\n35.4\n33.5\n40.5\n33.3\n29.6\n27.5\n32.7\n54.4\n40.2\nInternVL3-8B\nzhu2025internvl3\n38.5\n32.9\n36.1\n37.2\n36.2\n44.3\n36.5\n31.6\n28.9\n35.3\n55.5\n42.3\nNVILA-8B\nliu2024nvila\n35.8\n28.7\n37.2\n35.4\n33.7\n42.7\n39.7\n37.6\n36.1\n39.0\n54.0\n42.2\nQwen2.5-VL-7B\nbai2025qwen2\n31.4\n26.7\n29.5\n28.6\n29.1\n47.8\n41.2\n36.2\n33.7\n39.7\n50.8\n39.9\nMagma-8B\nyang2025magma\n32.1\n25.7\n34.4\n29.3\n29.8\n38.6\n31.5\n28.1\n26.7\n31.2\n49.1\n36.7\nRoboBrain2.0-7B\nteam2025robobrain2\n39.4\n27.0\n33.9\n32.2\n33.2\n44.9\n38.2\n34.7\n33.5\n37.8\n51.1\n40.7\nThinkAct-7B\nhuang2025thinkact\n50.1\n49.8\n44.8\n45.2\n48.2\n69.1\n61.8\n56.0\n52.4\n59.8\n56.2\n54.7\nFast-ThinkAct-7B\n51.3\n47.3\n41.5\n45.9\n47.5\n70.4\n63.3\n57.3\n53.2\n61.1\n59.0\n55.9\nTable 5\n:\nResults on LIBERO and SimplerEnv benchmarks with additional ThinkAct-3B comparison.\nMethod\nLIBERO\nSimplerEnv-Google\nLatency (\nâ†“\n\\downarrow\n)\nOpenVLA-7B\nkim24openvla\n76.5\n40.2\nN/A\nCoT-VLA-7B\nzhao2025cot\n83.9\nN/A\nN/A\nThinkAct-7B\nhuang2025thinkact\n84.4\n68.3\n7513\nMolmoAct-7B\nlee2025molmoact\n86.8\n64.9\n6723\nThinkAct-3B\nhuang2025thinkact\n83.1\n64.7\n5674\nFast-ThinkAct-3B\n89.7\n68.7\n805 (\nâ†“\n\\downarrow\n7.0\nÃ—\n\\times\n)\nTable 6\n:\nComparison with efficient textual reasoning methods.\nMethod\nEgoPlan-\nBench2\nRoboVQA\nOpenEQA\nAverage\nTextual Teacher\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\n41.7\n58.2\n49.4\n49.8\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\nInference w/o thinking\n42.7\n55.0\n41.7\n46.5\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\nInference w/ 6 textual tokens\n39.3\n53.0\n46.5\n46.3\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\nw/ RL Length-Penalty\narora2025training\n41.2\n57.5\n44.7\n47.8\nFast-ThinkAct-3B\n46.4\n60.8\n52.8\n53.3\nA.4\nEvaluation Setup\nA.4.1\nEmbodied Reasoning Benchmarks\nWe evaluate on three benchmarks assessing different aspects of embodied reasoning. EgoPlan-Bench2\nchen2023egoplan\ntests egocentric task planning across 24 daily-life scenarios with 1,321 multiple-choice questions, measuring accuracy in predicting next steps given task goals and progress history. RoboVQA\nsermanet2024robovqa\nevaluates visual reasoning in manipulation contexts through 1,893 free-form QA pairs from robot and human demonstrations, assessed via BLEU score. OpenEQA\nmajumdar2024openeqa\nassesses spatial and functional understanding through 1,600+ questions spanning 180+ real-world environments, evaluated using LLM-based scoring aligned with human preferences. These benchmarks comprehensively evaluate embodied reasoning capability across planning, manipulation, and spatial understanding.\nA.4.2\nRobotic Manipulation Benchmarks\nFigure 8\n:\nVisualization of predicted visual trajectories and action execution results on RoboTwin2.0.\nYellow traces indicate left gripper trajectories; red traces indicate right gripper trajectories for bimanual tasks.\nFigure 9\n:\nFailure identification and analysis capabilities on RoboFAC\nlu2025robofac\n.\nTop row shows identification of failure types and execution stages. Bottom row demonstrates failure root cause analysis.\nFigure 10\n:\nReasoning trace comparison on OpenEQA.\n(a) Teacherâ€™s textual reasoning. (b) Studentâ€™s verbalized latent reasoning.\nGreen\n: reasonable reasoning trace;\nred\n: incorrect trace.\nWe evaluate on three simulation benchmarks covering diverse manipulation scenarios. SimplerEnv\nli24simpler\nprovides manipulation tasks with strong sim-to-real correlation, featuring diverse visual variations in lighting, textures, backgrounds, and camera poses. Following MolmoAct\nlee2025molmoact\n, we evaluate on the Google Robot tasks using the standard protocol\nkim24openvla\n;\nlee2025molmoact\nof directly evaluating on SimplerEnv after training on OXE. LIBERO\nliu2023libero\ntargets different generalization challenges through four task suites: spatial layout variation (LIBERO-Spatial), object diversity (LIBERO-Object), goal variation (LIBERO-Goal), and long-horizon planning with mixed variations (LIBERO-Long). We evaluate each suite over 500 trials using 3 random seeds following prior works\nkim24openvla\n;\nlee2025molmoact\n. RoboTwin2.0\nchen2025robotwin2\nfeatures challenging bimanual manipulation with easy and hard difficulty settings, where the hard setting introduces domain randomization including clutter, lighting variations, diverse textures, and height changes. Following the original protocol, we train on 50 clean expert demonstrations per task and evaluate with 100 rollouts under both settings. We assess 10 tasks categorized into short, medium, and long horizons based on demonstration lengths.\nAppendix B\nAdditional Experiment Results\nB.1\nAdditional Quantitative Results\nResults of Larger Model Size.\nTo demonstrate the scalability of our approach, we apply Fast-ThinkAct to a larger backbone, Qwen2.5-VL-7B, and evaluate its performance on embodied reasoning benchmarks. As shown in Tab.\n4\n, Fast-ThinkAct consistently achieves strong performance across EgoPlan-Bench2\nqiu2024egoplan2\n, RoboVQA\nsermanet2024robovqa\n, and OpenEQA\nmajumdar2024openeqa\n, validating that our latent reasoning distillation method effectively scales to larger model backbones.\nPerformance Comparison with ThinkAct-3B.\nTab.\n5\npresents detailed numerical results corresponding to Fig. 3 with additional ThinkAct-3B results. At the same 3B model size, Fast-ThinkAct achieves notable performance gains (89.7 vs. 83.1 on LIBERO, 68.7 vs. 64.7 on SimplerEnv-Google) while dramatically improving efficiency with\n7\nÃ—\n\\times\nfaster inference (805ms vs. 5674ms).\nComparison with Efficient Reasoning Baselines.\nTable\n6\ncompares our method with efficient textual reasoning alternatives applied to the textual teacher\nâ„±\nÎ¸\nT\n\\mathcal{F}_{\\theta}^{T}\n. We evaluate three baselines: removing reasoning during inference entirely (0 tokens), constraining the teacher to generate only 6 textual tokens during inference, and applying RL training with a length penalty\narora2025training\nto encourage concise reasoning (\nâˆ¼\n\\sim\n50 tokens). These achieve 46.5, 46.3, and 47.8 respectively, all degrading from the teacherâ€™s 49.8. In contrast, Fast-ThinkAct uses only 6 latent tokens and achieves\n53.3\n, demonstrating superior efficiency and performance.\nB.2\nAdditional Qualitative Results\nQualitative Robot Execution.\nWe provide qualitative robot execution comparisons between the base action model RDT\nliu2024rdt\nand Fast-ThinkAct in the supplementary video\nFast-ThinkAct.mp4\n. Our method shows substantial improvements on challenging robotic execution tasks, where reasoning capabilities provide better spatial understanding and coordination for successful manipulation.\nBimanual Manipulation Results.\nIn Fig.\n8\n, we present visualized trajectories and execution results for\nhanging mug\nand\nhandover mic\ntasks under easy and hard settings in RoboTwin2.0\nchen2025robotwin2\n. The hard setting includes different backgrounds and distractor objects. These examples show successful bimanual coordination where predicted waypoints accurately guide both grippers through the manipulation sequence, demonstrating Fast-ThinkActâ€™s spatial reasoning ability across varied visual conditions.\nFailure Identification and Recovery.\nIn Fig.\n9\n, we demonstrate Fast-ThinkActâ€™s failure identification and analysis capabilities, complementing the recovery planning shown in the main paper. The top row shows that Fast-ThinkAct identifies failure types (e.g., position deviation) and execution stages (e.g., reaching for the cube). The bottom row illustrates root cause analysis, for instance, in the bottom-right example, the model correctly infers that the failure to push the cube with an L-shaped tool stems from an improper initial grasp. These results demonstrate Fast-ThinkActâ€™s comprehensive understanding of manipulation failures beyond recovery planning.\nVerbalized Latent Reasoning.\nFig.\n10\nvisualizes teacher textual reasoning and student verbalized reasoning. While the student generates compact and correct (\ngreen\n) reasoning, the teacherâ€™s lengthy output sometimes contains erroneous steps (\nred\n) that might degrade the performance.\nB.3\nAdditional Ablation Study and Analysis\nAdditional Ablation Results on Manipulation Benchmarks.\nTable\n8\nshows ablation results on LIBERO\nliu2023libero\n, SimplerEnv-Google\nli24simpler\n, and RoboTwin2.0\nchen2025robotwin2\n. Removing\nâ„’\nverb\n\\mathcal{L}_{\\text{verb}}\nor\nâ„’\ndistill\n\\mathcal{L}_{\\text{distill}}\nprogressively degrades performance, confirming their contributions. Our full model consistently outperforms the textual teacher and models without teacher-student training (CoT-SFT, SFT only), demonstrating the benefits of compact latent reasoning distillation.\nTable 7\n:\nAdditional ablation study of training objectives and learning stages on robot manipulation benchmarks.\nMethod\nLIBERO\nSimplerEnv\nGoogle\nRoboTwin2.0\nAverage\nFast-ThinkAct\n89.7\n68.7\n46.1\n68.2\nw/o\nâ„’\nverb\n\\mathcal{L}_{\\text{verb}}\n88.6\n67.3\n44.9\n66.9\nw/o\nâ„’\nverb\n,\nâ„’\ndistill\n\\mathcal{L}_{\\text{verb}},\\mathcal{L}_{\\text{distill}}\n86.3\n65.7\n42.6\n64.9\nTextual Teacher\n88.5\n67.3\n45.8\n67.2\nSFT + CoT-SFT\n87.2\n65.8\n43.3\n65.4\nSFT only\n86.9\n64.5\n42.8\n64.7\nTable 8\n:\nAblation of Latent Reasoning Steps\nM\nM\n.\nAblation Study on Action Model Conditioning.\nIn Sec. 3.3, we extract visual latent planning\nc\nt\nc_{t}\nfrom early-layer KV cache of spatial tokens to condition the action model. We compare this against using late-layer KV cache (last\nN\nN\nlayers, where\nN\nN\nis the action model depth) and directly using spatial tokensâ€™ output hidden states. Our approach achieves\n89.7\non LIBERO, outperforming late-layer KV at 88.3 and output hidden states at 87.1, demonstrating that early-layer representations better capture visual planning information for action prediction. Therefore, we adopt early-layer KV conditioning as our default configuration.\nAblation Study on Latent Reasoning Steps.\nIn Fig.\n8\n, we study the effect of latent reasoning steps\nM\nM\n. We observe that too few steps (\nM\n=\n1\nM=1\n) limit reasoning capacity, while excessive steps (\nM\n=\n30\n,\n100\nM=30,100\n) might introduce redundant or noisy information. Therefore, we adopt\nM\n=\n6\nM=6\n, which achieves optimal performance, as our default.\nReferences",
    "preview_text": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.\n\n\\correspondingauthor\nX\nFast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning\nChi-Pin Huang\n1\nYunze Man\n2\nZhiding Yu\nMin-Hung Chen\nJan Kautz\nYu-Chiang Frank Wang\n1\nFu-En Yang\nNVIDIA\nAbstract\nVision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories ",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "VLA",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "Fast-ThinkAct æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨ç†æ¡†æ¶ï¼Œé€šè¿‡å¯è¨€è¯­åŒ–çš„æ½œåœ¨è§„åˆ’å‡å°‘æ¨ç†å»¶è¿Ÿï¼Œæå‡å…·èº«æ§åˆ¶æ€§èƒ½ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šFast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å…·èº«æ™ºèƒ½ä»»åŠ¡ä¸­éœ€è¦å¤„ç†å¤æ‚çš„è§†è§‰åœºæ™¯å¹¶æ‰§è¡Œè‡ªé€‚åº”åŠ¨ä½œã€‚ç°æœ‰åŸºäºæ˜¾å¼æ€ç»´é“¾çš„æ¨ç†æ–¹æ³•è™½ç„¶æå‡äº†æ³›åŒ–èƒ½åŠ›ï¼Œä½†å†—é•¿çš„æ¨ç†è¿‡ç¨‹å¯¼è‡´**æ¨ç†å»¶è¿Ÿæé«˜**ï¼ˆä¾‹å¦‚æ¯ç§’0.1æ¬¡å†³ç­–ï¼‰ï¼Œæ— æ³•æ»¡è¶³æœºå™¨äººç­‰å®æ—¶åº”ç”¨çš„éœ€æ±‚ã€‚å¦‚ä½•åœ¨ä¿æŒå¼ºå¤§æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå¤§å¹…æå‡æ•ˆç‡ï¼Œæˆä¸ºå…³é”®æŒ‘æˆ˜ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡º **Fast-ThinkAct**ï¼Œä¸€ä¸ªé«˜æ•ˆçš„å…·èº«æ¨ç†æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å†—é•¿çš„æ–‡æœ¬æ€ç»´é“¾**å‹ç¼©ä¸ºå¯è¨€è¯­åŒ–çš„æ½œåœ¨è¡¨ç¤º**ã€‚\n- **å¯è¨€è¯­åŒ–æ½œåœ¨è§„åˆ’**ï¼šé‡‡ç”¨å¸ˆç”Ÿè’¸é¦æ¡†æ¶ã€‚æ•™å¸ˆæ¨¡å‹ï¼ˆæ–‡æœ¬VLMï¼‰ç”Ÿæˆæ˜¾å¼æ€ç»´é“¾ï¼›å­¦ç”Ÿæ¨¡å‹åˆ™å­¦ä¹ ç”Ÿæˆ**ç´§å‡‘çš„è¿ç»­æ½œåœ¨å‘é‡**æ¥ç¼–ç æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡ä¸€ä¸ª**è¨€è¯­åŒ–å™¨**ï¼Œè¿™äº›æ½œåœ¨å‘é‡å¯è¢«è§£ç å›æ–‡æœ¬ï¼Œç¡®ä¿äº†æ½œåœ¨ç©ºé—´çš„è¯­ä¹‰å¯è§£é‡Šæ€§ã€‚\n- **åå¥½å¼•å¯¼çš„è’¸é¦**ï¼šåˆ©ç”¨æ•™å¸ˆæ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„å¥–åŠ±ä¿¡å·æ„å»º**é«˜è´¨é‡ä¸ä½è´¨é‡æ¨ç†å¯¹**ï¼ŒæŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹çš„æ½œåœ¨è¡¨ç¤ºæ›´å€¾å‘äºç¼–ç é«˜è´¨é‡çš„æ¨ç†æ¨¡å¼ã€‚\n- **è§†è§‰è½¨è¿¹å¯¹é½**ï¼šé™¤äº†è¯­è¨€æ¨ç†ï¼Œè¿˜é€šè¿‡å¯¹é½å¸ˆç”Ÿæ¨¡å‹åœ¨è§†è§‰è½¨è¿¹è§„åˆ’ä¸Šçš„éšè—çŠ¶æ€ï¼Œå°†å…³é”®çš„**ç©ºé—´è§†è§‰è§„åˆ’èƒ½åŠ›**è’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚\n- **æ¨ç†å¢å¼ºçš„ç­–ç•¥å­¦ä¹ **ï¼šè®­ç»ƒå®Œæˆåï¼Œå†»ç»“å­¦ç”ŸVLMï¼Œå°†å…¶ç”Ÿæˆçš„ç´§å‡‘è§†è§‰è§„åˆ’æ½œåœ¨è¡¨ç¤ºä½œä¸ºæ¡ä»¶ï¼ŒæŒ‡å¯¼ä¸€ä¸ªæ‰©æ•£TransformeråŠ¨ä½œæ¨¡å‹ç”Ÿæˆä½å±‚æœºå™¨äººåŠ¨ä½œï¼Œä»è€Œæ¡¥æ¥é«˜å±‚è§„åˆ’ä¸åº•å±‚æ‰§è¡Œã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nåœ¨å¤šä¸ªå…·èº«æ“ä½œä¸æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFast-ThinkActå–å¾—äº†å“è¶Šçš„æ€§èƒ½ä¸æ•ˆç‡ï¼š\n- **æ€§èƒ½é¢†å…ˆ**ï¼šåœ¨LIBEROã€SimplerEnvã€RoboTwin2.0ç­‰æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šï¼ŒæˆåŠŸç‡è¶…è¶ŠOpenVLAã€ThinkActç­‰å…ˆè¿›æ¨¡å‹ã€‚åœ¨EgoPlanã€RoboVQAç­‰å…·èº«æ¨ç†åŸºå‡†ä¸Šï¼Œä¹Ÿè¶…è¿‡äº†GPT-4Vã€Geminiç­‰é€šç”¨å¤§æ¨¡å‹ã€‚\n- **æ•ˆç‡é£è·ƒ**ï¼šç›¸æ¯”ThinkAct-7Bï¼Œå®ç°äº†é«˜è¾¾**89.3%çš„æ¨ç†å»¶è¿Ÿé™ä½**ï¼Œæ¨ç†é€Ÿåº¦æå‡çº¦9.3å€ï¼ŒåŒæ—¶æ€§èƒ½æ›´ä¼˜ã€‚\n- **å…³é”®èƒ½åŠ›**ï¼šå®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨**é•¿æ—¶ç¨‹è§„åˆ’ã€å°æ ·æœ¬é€‚åº”ã€å¤±è´¥æ¢å¤**ç­‰æ–¹é¢å‡å…·å¤‡å¼ºå¤§èƒ½åŠ›ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\nFast-ThinkActé¦–æ¬¡ç³»ç»Ÿæ€§åœ°è§£å†³äº†æ¨ç†VLAæ¨¡å‹çš„æ•ˆç‡ç“¶é¢ˆé—®é¢˜ã€‚å…¶ä»·å€¼åœ¨äºï¼š\n- **æŠ€æœ¯è´¡çŒ®**ï¼šæå‡ºäº†â€œå¯è¨€è¯­åŒ–æ½œåœ¨æ¨ç†â€çš„æ–°èŒƒå¼ï¼Œé€šè¿‡åå¥½è’¸é¦ä¸è§†è§‰å¯¹é½ï¼Œåœ¨å‹ç¼©è¡¨ç¤ºçš„åŒæ—¶ä¿ç•™äº†ä¸°å¯Œçš„è¯­ä¹‰å’Œç©ºé—´ä¿¡æ¯ï¼Œä¸ºé«˜æ•ˆå¤šæ¨¡æ€æ¨ç†æä¾›äº†æ–°æ€è·¯ã€‚\n- **å®é™…åº”ç”¨**ï¼šå¤§å¹…æå‡çš„æ¨ç†é€Ÿåº¦ä½¿å¾—å¤æ‚çš„åœ¨çº¿è§„åˆ’ä¸å†³ç­–èƒ½å¤Ÿåº”ç”¨äºå¯¹å®æ—¶æ€§è¦æ±‚ä¸¥æ ¼çš„çœŸå®æœºå™¨äººåœºæ™¯ï¼Œæ¨åŠ¨äº†å…·èº«æ™ºèƒ½èµ°å‘å®ç”¨åŒ–ã€‚\n- **å¼€æºä¸å¯å¤ç°**ï¼šåŸºäºå¼€æºæ¨¡å‹æ„å»ºï¼Œä¸ºç¤¾åŒºæä¾›äº†é«˜æ•ˆVLAç ”ç©¶çš„æ–°åŸºçº¿ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šFast-ThinkAct\n\n### **ä¸€ã€ æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨å…·èº«æ¨ç†ä»»åŠ¡ä¸­çš„â€œæ•ˆç‡-æ€§èƒ½â€æƒè¡¡éš¾é¢˜**ã€‚å…·ä½“è€Œè¨€ï¼š\n- **é—®é¢˜**ï¼šç°æœ‰çš„â€œæ¨ç†å‹VLAâ€æ¨¡å‹ï¼ˆå¦‚ThinkActï¼‰é€šè¿‡ç”Ÿæˆæ˜¾å¼çš„ã€å†—é•¿çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¥æå‡ä»»åŠ¡æ³›åŒ–èƒ½åŠ›å’Œè§„åˆ’èƒ½åŠ›ï¼Œä½†è¿™å¯¼è‡´äº†**æé«˜çš„æ¨ç†å»¶è¿Ÿ**ï¼ˆä¾‹å¦‚æ¯ç§’ä»…èƒ½åšå‡º0.1æ¬¡å†³ç­–ï¼‰ï¼Œæ— æ³•æ»¡è¶³æœºå™¨äººç­‰**å®æ—¶æ€§è¦æ±‚é«˜**çš„å…·èº«AIåº”ç”¨éœ€æ±‚ã€‚\n- **çŸ›ç›¾**ï¼šç›´æ¥ç¼©çŸ­æ–‡æœ¬æ¨ç†é“¾ï¼ˆCoTï¼‰ä¼šä¸¢å¤±å…³é”®ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚å¦‚ä½•åœ¨ä¿æŒå¼ºå¤§æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°é«˜æ•ˆã€ç´§å‡‘çš„è§„åˆ’ï¼Œæ˜¯å½“å‰é¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº† **Fast-ThinkAct** æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº **â€œå¯è¨€è¯­åŒ–çš„æ½œåœ¨è§„åˆ’â€** ã€‚è¿™æ˜¯ä¸€ç§**èŒƒå¼è½¬æ¢**ï¼šä»ç”Ÿæˆå†—é•¿çš„æ˜¾å¼æ–‡æœ¬æ¨ç†é“¾ï¼Œè½¬å‘å­¦ä¹ ç´§å‡‘çš„ã€è¿ç»­çš„æ½œåœ¨è¡¨ç¤ºæ¥è¿›è¡Œå†…éƒ¨æ¨ç†ã€‚\n\n1.  **åˆ›æ–°æ–¹æ³•ï¼šå¯è¨€è¯­åŒ–çš„æ½œåœ¨æ¨ç†**\n    - **ä¼ ç»Ÿæ–¹æ³•**ï¼šVLM â†’ ç”Ÿæˆå†—é•¿æ–‡æœ¬CoTï¼ˆ~250ä¸ªtokenï¼‰â†’ æŒ‡å¯¼åŠ¨ä½œã€‚\n    - **Fast-ThinkAct**ï¼šVLM â†’ ç”Ÿæˆå°‘é‡**è¿ç»­æ½œåœ¨å‘é‡**ï¼ˆå¦‚6ä¸ªï¼‰ + **å¹¶è¡Œç©ºé—´token** â†’ æŒ‡å¯¼åŠ¨ä½œã€‚\n    - **å…³é”®**ï¼šè¿™äº›æ½œåœ¨å‘é‡å¯ä»¥é€šè¿‡ä¸€ä¸ªé¢å¤–çš„â€œè¨€è¯­åŒ–å™¨â€LLMè§£ç å›è‡ªç„¶è¯­è¨€ï¼Œä»è€Œä¿è¯äº†æ½œåœ¨ç©ºé—´çš„**å¯è§£é‡Šæ€§**ï¼Œå¹¶åœ¨è®­ç»ƒé˜¶æ®µåˆ©ç”¨è¯­è¨€åå¥½è¿›è¡Œå¯¹é½ã€‚\n\n2.  **åˆ›æ–°è®­ç»ƒæœºåˆ¶ï¼šåå¥½å¼•å¯¼çš„è’¸é¦ä¸è½¨è¿¹å¯¹é½**\n    ä¸ºäº†å°†æ•™å¸ˆæ¨¡å‹ï¼ˆç”Ÿæˆæ–‡æœ¬CoTï¼‰çš„èƒ½åŠ›å‹ç¼©åˆ°å­¦ç”Ÿæ¨¡å‹ï¼ˆç”Ÿæˆæ½œåœ¨å‘é‡ï¼‰ä¸­ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªå¤šç›®æ ‡è’¸é¦æ¡†æ¶ï¼š\n    - **åå¥½å¼•å¯¼çš„è¨€è¯­åŒ–æŸå¤±**ï¼šåˆ©ç”¨æ•™å¸ˆæ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆGRPOï¼‰è®­ç»ƒä¸­äº§ç”Ÿçš„**å¥–åŠ±ä¿¡å·**ï¼ŒåŒºåˆ†é«˜è´¨é‡ä¸ä½è´¨é‡çš„æ¨ç†é“¾ã€‚é€šè¿‡ç±»ä¼¼DPOçš„ç›®æ ‡ï¼Œè®­ç»ƒå­¦ç”Ÿæ¨¡å‹ç”Ÿæˆçš„æ½œåœ¨å‘é‡ï¼Œä½¿å…¶è¢«â€œè¨€è¯­åŒ–å™¨â€è§£ç æ—¶ï¼Œæ›´å€¾å‘äºå¯¹åº”é«˜è´¨é‡çš„æ¨ç†æ–‡æœ¬ã€‚\n    - **åŠ¨ä½œå¯¹é½çš„è§†è§‰è§„åˆ’è’¸é¦æŸå¤±**ï¼šç›´æ¥å¯¹é½æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹åœ¨å…³é”®è§†è§‰è§„åˆ’è¡¨å¾ï¼ˆå¦‚`<answer>` tokençš„éšè—çŠ¶æ€ï¼‰ä¸Šçš„è·ç¦»ï¼Œç¡®ä¿å­¦ç”Ÿæ¨¡å‹ç»§æ‰¿æ•™å¸ˆå¯¹**ç©ºé—´-æ—¶é—´åŠ¨æ€**çš„ç†è§£èƒ½åŠ›ï¼Œè¿™å¯¹äºå…·èº«æ§åˆ¶è‡³å…³é‡è¦ã€‚\n    - **å¹¶è¡Œç©ºé—´è½¨è¿¹é¢„æµ‹**ï¼šä½¿ç”¨ä¸€ç»„å¯å­¦ä¹ çš„ç©ºé—´tokenï¼Œå¹¶è¡Œé¢„æµ‹æœªæ¥è½¨è¿¹çš„å¤šä¸ªè·¯å¾„ç‚¹ï¼Œæ›¿ä»£äº†æ•™å¸ˆæ¨¡å‹è‡ªå›å½’ç”Ÿæˆæ–‡æœ¬è·¯å¾„ç‚¹çš„ä½æ•ˆæ–¹å¼ã€‚\n\n3.  **åˆ›æ–°æ¶æ„ï¼šæ¨ç†å¢å¼ºçš„ç­–ç•¥å­¦ä¹ **\n    - è®­ç»ƒå®Œæˆåï¼Œ**å†»ç»“**å·²å­¦ä¼šæ½œåœ¨æ¨ç†çš„å­¦ç”ŸVLMã€‚\n    - å°†å…¶ç”Ÿæˆçš„**è§†è§‰æ½œåœ¨è§„åˆ’**ï¼ˆä»ç©ºé—´tokençš„Key-Valueç¼“å­˜ä¸­æå–ï¼‰ä½œä¸º**é«˜çº§æ¡ä»¶**ï¼Œè¾“å…¥åˆ°æ‰©æ•£Transformerç­‰åŠ¨ä½œæ¨¡å‹ä¸­ï¼ŒæŒ‡å¯¼ä½å±‚åŠ¨ä½œçš„ç”Ÿæˆã€‚\n    - è¿™å®ç°äº†**é«˜å±‚ç´§å‡‘è§„åˆ’**ä¸**åº•å±‚åŠ¨ä½œæ‰§è¡Œ**çš„é«˜æ•ˆã€ç´§å¯†è€¦åˆã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆæ€»ç»“**\n**å¦‚ä½•è§£å†³æ•ˆç‡é—®é¢˜ï¼Ÿ**\n1.  **è¡¨ç¤ºå‹ç¼©**ï¼šå°†æ•°ç™¾ä¸ªæ–‡æœ¬tokençš„æ¨ç†é“¾å‹ç¼©ä¸ºå°‘æ•°å‡ ä¸ªè¿ç»­æ½œåœ¨å‘é‡å’Œå¹¶è¡Œç©ºé—´tokenï¼Œæå¤§å‡å°‘äº†è‡ªå›å½’ç”Ÿæˆçš„æ­¥æ•°ã€‚\n2.  **å¹¶è¡Œé¢„æµ‹**ï¼šå¯¹è§†è§‰è½¨è¿¹è·¯å¾„ç‚¹è¿›è¡Œå¹¶è¡Œé¢„æµ‹ï¼Œé¿å…äº†é€ç‚¹ç”Ÿæˆçš„åºåˆ—å»¶è¿Ÿã€‚\n3.  **è®­ç»ƒ-æ¨ç†è§£è€¦**ï¼šå¤æ‚çš„â€œè¨€è¯­åŒ–å™¨â€å’Œåå¥½å­¦ä¹ **ä»…åœ¨è®­ç»ƒé˜¶æ®µä½¿ç”¨**ï¼Œæ¨ç†æ—¶åªéœ€è½»é‡çš„å­¦ç”ŸVLMå’ŒåŠ¨ä½œæ¨¡å‹ï¼Œæµç¨‹ç®€æ´ã€‚\n\n**å¦‚ä½•ä¿è¯æ€§èƒ½ä¸é™åå‡ï¼Ÿ**\n1.  **é«˜è´¨é‡çŸ¥è¯†è’¸é¦**ï¼šé€šè¿‡åŸºäºå¥–åŠ±çš„åå¥½å­¦ä¹ ï¼Œè’¸é¦äº†æ•™å¸ˆæ¨¡å‹ä¸­æœ€æœ‰æ•ˆçš„æ¨ç†æ¨¡å¼ï¼Œè¿‡æ»¤äº†å†—é•¿æ–‡æœ¬ä¸­çš„å™ªå£°å’Œæ— å…³ä¿¡æ¯ã€‚\n2.  **å¤šæ¨¡æ€å¯¹é½**ï¼šé€šè¿‡è§†è§‰è§„åˆ’è’¸é¦æŸå¤±ï¼Œæ˜¾å¼åœ°ä¿ç•™äº†å¯¹äºå…·èº«æ§åˆ¶è‡³å…³é‡è¦çš„**ç©ºé—´è§†è§‰ä¿¡æ¯**ï¼Œè¿™æ˜¯çº¯æ–‡æœ¬å‹ç¼©å®¹æ˜“ä¸¢å¤±çš„ã€‚\n3.  **é«˜æ•ˆè¡¨å¾åˆ©ç”¨**ï¼šç´§å‡‘çš„æ½œåœ¨è¡¨å¾ä½œä¸ºå¼ºæ¡ä»¶ä¿¡å·ï¼Œæ›´ç›´æ¥ã€é«˜æ•ˆåœ°å¼•å¯¼åŠ¨ä½œæ¨¡å‹ï¼Œå¯èƒ½æ¯”å†—é•¿æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯æ›´æ˜“äºæ¨¡å‹åˆ©ç”¨ã€‚\n\n### **å››ã€ å®é™…ä»·å€¼ä¸æŠ€æœ¯è´¡çŒ®**\n- **æ€§èƒ½**ï¼šåœ¨LIBEROã€SimplerEnvã€RoboTwin2.0ç­‰å¤šä¸ªæœºå™¨äººæ“ä½œå’Œå…·èº«æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œ**æ€§èƒ½è¶…è¶Š**åŒ…æ‹¬ThinkActåœ¨å†…çš„æœ€å…ˆè¿›æ¨ç†VLAã€‚\n- **æ•ˆç‡**ï¼šå®ç°äº†**é«˜è¾¾89.3%çš„æ¨ç†å»¶è¿Ÿé™ä½**ï¼ˆç›¸æ¯”ThinkAct-7Bï¼‰ï¼Œæé€Ÿè¾¾**9.3å€**ï¼ŒåŒæ—¶æ¨¡å‹å‚æ•°é‡æ›´å°ï¼ˆ3B vs 7Bï¼‰ã€‚\n- **èƒ½åŠ›**ï¼šä¿æŒäº†å¼ºå¤§çš„**é•¿æ—¶ç¨‹è§„åˆ’**ã€**å°æ ·æœ¬é€‚åº”**å’Œ**æ•…éšœæ¢å¤**èƒ½åŠ›ï¼Œè¯æ˜äº†ç´§å‡‘æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚\n- **å¯å‘æ€§**ï¼šä¸º**å®æ—¶å…·èº«AI**ç³»ç»Ÿæä¾›äº†ä¸€æ¡å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ï¼Œå±•ç¤ºäº†é€šè¿‡â€œæ½œåœ¨è§„åˆ’â€æ¥å¹³è¡¡æ¨ç†æ·±åº¦ä¸è®¡ç®—æ•ˆç‡çš„å·¨å¤§æ½œåŠ›ã€‚å…¶â€œæ•™å¸ˆ-å­¦ç”Ÿâ€è’¸é¦æ¡†æ¶å’Œâ€œå¯è¨€è¯­åŒ–æ½œåœ¨ç©ºé—´â€çš„è®¾è®¡ï¼Œå¯¹å¤šæ¨¡æ€é«˜æ•ˆæ¨ç†é¢†åŸŸå…·æœ‰æ™®é€‚çš„å‚è€ƒæ„ä¹‰ã€‚\n\n```mermaid\ngraph TD\n    A[é—®é¢˜: æ¨ç†VLAå»¶è¿Ÿé«˜] --> B[æ ¸å¿ƒåˆ›æ–°: å¯è¨€è¯­åŒ–æ½œåœ¨è§„åˆ’];\n    B --> C{å…³é”®æ–¹æ³•};\n    C --> D[åå¥½å¼•å¯¼è’¸é¦];\n    C --> E[è§†è§‰è½¨è¿¹å¯¹é½];\n    C --> F[å¹¶è¡Œç©ºé—´é¢„æµ‹];\n    D & E & F --> G[è®­ç»ƒå‡ºé«˜æ•ˆå­¦ç”ŸVLM];\n    G --> H[æ¨ç†å¢å¼ºç­–ç•¥å­¦ä¹ ];\n    H --> I[ç»“æœ: é«˜æ€§èƒ½ + æä½å»¶è¿Ÿ];\n```",
            "timestamp": "2026-01-20T17:57:49.643942",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡åˆ†æï¼šFast-ThinkAct åŠå…¶æŠ€æœ¯è„‰ç»œæ¢³ç†\n\nåŸºäºè®ºæ–‡å†…å®¹ï¼ŒFast-ThinkAct æ˜¯ Vision-Language-Action (VLA) æ¨ç†æ¨¡å‹å‘å±•è„‰ç»œä¸­çš„ä¸€ä¸ªæœ€æ–°å·¥ä½œã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äº**é€šè¿‡å¯è¨€è¯­åŒ–çš„æ½œåœ¨è§„åˆ’ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå¤§å¹…é™ä½äº†æ¨ç†å»¶è¿Ÿ**ã€‚ä»¥ä¸‹å°†æ¢³ç†è¯¥æ–¹å‘çš„å‘å±•è„‰ç»œï¼Œé‡ç‚¹åˆ†ææ¯ä¸€æ­¥çš„æ”¹è¿›ä»¥åŠå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚\n\n### å‘å±•è„‰ç»œæ€»è§ˆ\n\nè¯¥é¢†åŸŸçš„å‘å±•å¯ä»¥æ¦‚æ‹¬ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š\n1.  **åŸºç¡€ VLA æ¨¡å‹**ï¼šé€šè¿‡æ¨¡ä»¿å­¦ä¹ ä»å¤§è§„æ¨¡æœºå™¨äººæ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ æŠ€èƒ½ï¼Œä½†æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚\n2.  **æ˜¾å¼æ¨ç† VLA**ï¼šå¼•å…¥é“¾å¼æ€ç»´ç­‰æ˜¾å¼æ¨ç†æœºåˆ¶ï¼Œæå‡æ³›åŒ–ä¸è§„åˆ’èƒ½åŠ›ï¼Œä½†å¸¦æ¥é«˜æ˜‚çš„æ¨ç†å»¶è¿Ÿã€‚\n3.  **é«˜æ•ˆæ¨ç† VLA**ï¼šæ—¨åœ¨å‹ç¼©æˆ–ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å®ç°å®æ—¶æˆ–è¿‘å®æ—¶å†³ç­–ã€‚**Fast-ThinkAct å±äºæ­¤é˜¶æ®µçš„æœ€æ–°å·¥ä½œ**ã€‚\n\n---\n\n### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ VLA æ¨¡å‹ (æ¨¡ä»¿å­¦ä¹ )\n\n*   **ä»£è¡¨å·¥ä½œ**ï¼šRT-1/2, Octo, OpenVLA, Ï€â‚€, Magma ç­‰ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šå°†è§†è§‰-è¯­è¨€æ¨¡å‹ (VLM) åœ¨å¤§è§„æ¨¡æœºå™¨äººåŠ¨ä½œæ•°æ®é›†ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„ç›‘ç£å¾®è°ƒï¼Œå­¦ä¹ è¯­è¨€æ¡ä»¶åŒ–çš„ç­–ç•¥ã€‚\n*   **æ”¹è¿›ä¸ä»·å€¼**ï¼š\n    *   **è¯æ˜äº†æ•°æ®é©±åŠ¨çš„å¯è¡Œæ€§**ï¼šå±•ç¤ºäº†åˆ©ç”¨äº’è”ç½‘è§„æ¨¡çš„æœºå™¨äººæ•°æ®è®­ç»ƒé€šç”¨ç­–ç•¥çš„æ½œåŠ›ã€‚\n    *   **å®ç°äº†åŸºç¡€æŠ€èƒ½**ï¼šåœ¨è®­ç»ƒåˆ†å¸ƒå†…çš„æ‹¾å–ã€æ”¾ç½®ç­‰ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚\n*   **å±€é™æ€§**ï¼š\n    *   **æ³›åŒ–èƒ½åŠ›å·®**ï¼šä¸¥é‡å—é™äºè®­ç»ƒæ•°æ®åˆ†å¸ƒï¼Œéš¾ä»¥åº”å¯¹é•¿è§†é‡è§„åˆ’ã€æ–°åœºæ™¯é€‚åº”ã€ä»å¤±è´¥ä¸­æ¢å¤ç­‰ä»»åŠ¡ã€‚\n    *   **ç¼ºä¹æ˜¾å¼æ¨ç†**ï¼šæ¨¡å‹æ˜¯â€œé»‘ç®±â€ï¼Œå†³ç­–è¿‡ç¨‹ä¸é€æ˜ï¼Œéš¾ä»¥è¿›è¡Œå¤æ‚çš„é—®é¢˜åˆ†è§£å’Œé€»è¾‘è§„åˆ’ã€‚\n\n### ç¬¬äºŒé˜¶æ®µï¼šæ˜¾å¼æ¨ç† VLA (æå‡æ³›åŒ–)\n\nä¸ºäº†å…‹æœåŸºç¡€ VLA çš„å±€é™æ€§ï¼Œç ”ç©¶è€…ä»¬å¼•å…¥äº†**æ˜¾å¼çš„ä¸­é—´æ¨ç†è¿‡ç¨‹**ã€‚è¿™ä¸ªæ–¹å‘åˆå¯åˆ†ä¸ºå‡ ä¸ªå­æ–¹å‘ï¼š\n\n#### å‘å±•æ–¹å‘ 1: åŸºäºç›‘ç£çš„é“¾å¼æ€ç»´ (Supervised CoT)\n*   **ä»£è¡¨å·¥ä½œ**ï¼šEmbodied CoT, CoT-VLA, MolmoAct, EO-1ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šåˆ©ç”¨æ ‡æ³¨çš„æˆ–ç”±å¤§æ¨¡å‹ç”Ÿæˆçš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼ˆæ–‡æœ¬æˆ–è§†è§‰å½¢å¼ï¼‰ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè®­ç»ƒæ¨¡å‹å…ˆâ€œæ€è€ƒâ€å†â€œè¡ŒåŠ¨â€ã€‚\n*   **æŠ€æœ¯æ”¹è¿›**ï¼š\n    *   **æ–‡æœ¬æ¨ç†**ï¼šä½¿ç”¨ç°æˆçš„ LLM/VLM ç”Ÿæˆä¼ª CoT æ ‡ç­¾ï¼Œå¼•å¯¼æ¨¡å‹è¿›è¡Œè¯­è¨€æ¨ç†ã€‚\n    *   **è§†è§‰æ¨ç†**ï¼šç”Ÿæˆç»“æ„åŒ–çš„è§†è§‰è¡¨ç¤ºï¼ˆå¦‚å­ç›®æ ‡å›¾åƒã€æ·±åº¦å›¾ã€2D è§†è§‰è½¨è¿¹ï¼‰ä½œä¸ºä¸­é—´è§„åˆ’ï¼Œæ›´å¥½åœ°è¿æ¥ç©ºé—´ç†è§£ä¸åŠ¨ä½œã€‚\n    *   **ä»·å€¼**ï¼šæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨é•¿è§†é‡ä»»åŠ¡ã€å°‘æ ·æœ¬é€‚åº”å’Œç»„åˆæ³›åŒ–ä¸Šçš„èƒ½åŠ›ï¼Œä½¿å†³ç­–è¿‡ç¨‹æ›´å¯è§£é‡Šã€‚\n*   **å±€é™æ€§**ï¼š\n    *   **ä¾èµ–æ ‡æ³¨**ï¼šéœ€è¦å¤§é‡é«˜è´¨é‡çš„æ¨ç†æ ‡æ³¨ï¼Œæ•°æ®è·å–æˆæœ¬é«˜ã€‚\n    *   **è¦†ç›–æœ‰é™**ï¼šæ€§èƒ½å—é™äºç›‘ç£æ•°æ®çš„è¦†ç›–èŒƒå›´ã€‚\n    *   **æ¨ç†å»¶è¿Ÿé«˜**ï¼šç”Ÿæˆå†—é•¿çš„æ–‡æœ¬æˆ–è§†è§‰æ¨ç†é“¾ï¼ˆ~250ä¸ªtokenï¼‰å¯¼è‡´æ¨ç†é€Ÿåº¦æ…¢ï¼ˆç§’çº§ï¼‰ï¼Œæ— æ³•æ»¡è¶³å®æ—¶æœºå™¨äººæ§åˆ¶çš„éœ€æ±‚ã€‚\n\n#### å‘å±•æ–¹å‘ 2: åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨ç† (RL-based Reasoning)\n*   **ä»£è¡¨å·¥ä½œ**ï¼š**ThinkAct** (Fast-ThinkAct çš„å‰ä½œ)ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šä¸ä¾èµ–äººå·¥æ ‡æ³¨ï¼Œè€Œæ˜¯ä½¿ç”¨å¼ºåŒ–å­¦ä¹  (RL)ï¼Œé€šè¿‡è®¾è®¡**ä¸åŠ¨ä½œå¯¹é½çš„è§†è§‰å¥–åŠ±**æ¥å¼•å¯¼æ¨¡å‹ç”Ÿæˆæœ‰ç›Šçš„é“¾å¼æ€ç»´ã€‚\n*   **æŠ€æœ¯æ”¹è¿›**ï¼š\n    *   **å…äºæ ‡æ³¨**ï¼šåˆ©ç”¨ RLï¼ˆå¦‚ GRPOï¼‰è‡ªåŠ¨æ¢ç´¢å’Œä¼˜åŒ–æ¨ç†è·¯å¾„ï¼Œæ‘†è„±äº†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚\n    *   **å¥–åŠ±é©±åŠ¨**ï¼šæ¨ç†è´¨é‡ç›´æ¥ç”±ä»»åŠ¡å®Œæˆåº¦ã€è½¨è¿¹å¯¹é½ç­‰ä¸‹æ¸¸æŒ‡æ ‡è¡¡é‡å’Œä¼˜åŒ–ï¼Œç¡®ä¿äº†æ¨ç†å¯¹æœ€ç»ˆåŠ¨ä½œçš„æœ‰æ•ˆæ€§ã€‚\n*   **å±€é™æ€§**ï¼š\n    *   **å»¶è¿Ÿé—®é¢˜åŠ å‰§**ï¼šRL è®­ç»ƒå€¾å‘äºç”Ÿæˆæ›´é•¿ã€æ›´è¯¦ç»†çš„æ¨ç†é“¾ä»¥æœ€å¤§åŒ–å¥–åŠ±ï¼Œä½¿å¾—æ¨ç†å»¶è¿Ÿé—®é¢˜æ¯”ç›‘ç£æ–¹æ³•æ›´çªå‡ºã€‚\n    *   **è®­ç»ƒä¸ç¨³å®š**ï¼šRL è®­ç»ƒæœ¬èº«å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¯èƒ½ä¸ç¨³å®šã€‚\n\n### ç¬¬ä¸‰é˜¶æ®µï¼šé«˜æ•ˆæ¨ç† VLA (å‹ç¼©ä¸åŠ é€Ÿ)\n\né’ˆå¯¹æ˜¾å¼æ¨ç† VLA çš„é«˜å»¶è¿Ÿç“¶é¢ˆï¼Œè¿‘æœŸå·¥ä½œå¼€å§‹æ¢ç´¢å¦‚ä½•**å‹ç¼©æ¨ç†è¿‡ç¨‹**ã€‚Fast-ThinkAct åœ¨æ­¤èƒŒæ™¯ä¸‹æå‡ºï¼Œå¹¶é’ˆå¯¹å‡ ä¸ªä¸åŒçš„é«˜æ•ˆåŒ–æ–¹å‘åšå‡ºäº†åˆ›æ–°ã€‚\n\n#### å‘å±•æ–¹å‘ 1: æ¨ç†é•¿åº¦æƒ©ç½š/ä¸¢å¼ƒ\n*   **ä»£è¡¨å·¥ä½œ**ï¼šECoT-Lite, ä»¥åŠä¸€äº› LLM ä¸­çš„ RL é•¿åº¦æƒ©ç½šæ–¹æ³•ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨è®­ç»ƒæˆ–æ¨ç†æ—¶ï¼Œå¼ºåˆ¶æˆ–é¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´çŸ­çš„æ¨ç†é“¾ã€‚ä¾‹å¦‚ï¼ŒECoT-Lite åœ¨æµ‹è¯•æ—¶éšæœºâ€œä¸¢å¼ƒâ€éƒ¨åˆ†æ¨ç†æ­¥éª¤ã€‚\n*   **æŠ€æœ¯æ”¹è¿›**ï¼šç›´æ¥å‡å°‘äº†éœ€è¦ç”Ÿæˆçš„ token æ•°é‡ï¼Œä»è€Œé™ä½å»¶è¿Ÿã€‚\n*   **å±€é™æ€§**ï¼š\n    *   **æ€§èƒ½ä¸‹é™é£é™©**ï¼šç®€å•ç¼©çŸ­æˆ–ä¸¢å¼ƒå¯èƒ½æŸå¤±å…³é”®æ¨ç†ä¿¡æ¯ï¼Œå¯¼è‡´è§„åˆ’ä¸ä¸€è‡´æˆ–æ€§èƒ½ä¸‹é™ã€‚\n    *   **éæœ¬è´¨å‹ç¼©**ï¼šæœªæ”¹å˜æ¨ç†çš„è¡¨ç¤ºå½¢å¼ï¼Œæ•ˆç‡æå‡æœ‰é™ã€‚\n\n#### å‘å±•æ–¹å‘ 2: æ½œåœ¨ç©ºé—´æ¨ç† (Latent Reasoning) - **Fast-ThinkAct çš„æ ¸å¿ƒè´¡çŒ®**\n*   **ä»£è¡¨å·¥ä½œ**ï¼š**Fast-ThinkAct**ï¼Œä»¥åŠ LLM é¢†åŸŸçš„ Coconut, CODI ç­‰ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼š**å°†å†—é•¿çš„ç¦»æ•£æ–‡æœ¬æ¨ç†ï¼Œè’¸é¦/å‹ç¼©æˆç´§å‡‘çš„è¿ç»­æ½œåœ¨å‘é‡**ã€‚æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œâ€œæ€è€ƒâ€ï¼Œè¿™äº›æ½œåœ¨å‘é‡è•´å«äº†è§„åˆ’ä¿¡æ¯ï¼Œå¹¶èƒ½è¢«ä¸€ä¸ªâ€œè¨€è¯­åŒ–å™¨â€è§£ç å›æ–‡æœ¬ä»¥ä¾›è§£é‡Šã€‚\n*   **æŠ€æœ¯æ”¹è¿› (Fast-ThinkAct çš„è´¡çŒ®)**ï¼š\n    1.  **å¯è¨€è¯­åŒ–çš„æ½œåœ¨è§„åˆ’**ï¼šæå‡ºâ€œå­¦ç”Ÿ-æ•™å¸ˆâ€è’¸é¦æ¡†æ¶ã€‚æ•™å¸ˆæ¨¡å‹ï¼ˆå¦‚ ThinkActï¼‰ç”Ÿæˆæ˜¾å¼æ–‡æœ¬ CoTï¼Œå­¦ç”Ÿæ¨¡å‹å­¦ä¹ ç”Ÿæˆæå°‘æ•°ï¼ˆå¦‚6ä¸ªï¼‰è¿ç»­æ½œåœ¨ tokenã€‚é€šè¿‡ä¸€ä¸ªå¯è®­ç»ƒçš„è¨€è¯­åŒ–å™¨ LLM å’ŒåŸºäºåå¥½çš„æŸå¤±å‡½æ•°ï¼Œç¡®ä¿è¿™äº›æ½œåœ¨ token èƒ½å¤Ÿå¯¹åº”åˆ°é«˜è´¨é‡çš„æ¨ç†æ¨¡å¼ã€‚\n    2.  **è§†è§‰è§„åˆ’å¯¹é½**ï¼šé™¤äº†è¯­è¨€æ¨ç†ï¼Œè¿˜é€šè¿‡å¯¹é½æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹çš„è½¨è¿¹çº§éšè—çŠ¶æ€ï¼Œå°†**è§†è§‰è§„åˆ’èƒ½åŠ›**è’¸é¦åˆ°æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œè¿™å¯¹äºå…·èº«æ§åˆ¶è‡³å…³é‡è¦ã€‚\n    3.  **å¹¶è¡Œç©ºé—´è½¨è¿¹é¢„æµ‹**ï¼šå¼•å…¥å¯å­¦ä¹ çš„ç©ºé—´ tokenï¼Œå¹¶è¡Œé¢„æµ‹æœªæ¥è·¯å¾„ç‚¹ï¼Œæ›¿ä»£äº†æ•™å¸ˆæ¨¡å‹è‡ªå›å½’ç”Ÿæˆæ–‡æœ¬è·¯å¾„ç‚¹çš„ä½æ•ˆæ–¹å¼ã€‚\n    4.  **æ¨ç†å¢å¼ºçš„ç­–ç•¥å­¦ä¹ **ï¼šå°†è®­ç»ƒå¥½çš„å­¦ç”Ÿ VLM ç”Ÿæˆçš„æ½œåœ¨è§†è§‰è§„åˆ’ï¼Œé€šè¿‡ KV ç¼“å­˜æ³¨å…¥åˆ°åŠ¨ä½œæ¨¡å‹ä¸­ï¼Œæ¡¥æ¥äº†é«˜å±‚è§„åˆ’ä¸åº•å±‚åŠ¨ä½œæ‰§è¡Œã€‚\n*   **ä»·å€¼ä¸æ”¹è¿›**ï¼š\n    *   **æè‡´æ•ˆç‡**ï¼šå®ç°äº† **~89.3% çš„æ¨ç†å»¶è¿Ÿé™ä½**ï¼Œæ¯”å…¶å‰ä½œ ThinkAct-7B å¿« 9.3 å€ã€‚\n    *   **ä¿æŒç”šè‡³æå‡æ€§èƒ½**ï¼šåœ¨å¤šä¸ªæœºå™¨äººæ“ä½œå’Œå…·èº«æ¨ç†åŸºå‡†ä¸Šè¾¾åˆ° SOTAï¼Œè¯æ˜äº†ç´§å‡‘æ½œåœ¨è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚\n    *   **ä¿ç•™å…³é”®èƒ½åŠ›**ï¼šæˆåŠŸä¿æŒäº†é•¿è§†é‡è§„åˆ’ã€å°‘æ ·æœ¬é€‚åº”å’Œå¤±è´¥æ¢å¤ç­‰ç”±æ¨ç†å¸¦æ¥çš„æ ¸å¿ƒä¼˜åŠ¿ã€‚\n*   **å±€é™æ€§**ï¼š\n    *   **è¨€è¯­åŒ–å™¨å¯èƒ½å¹»è§‰**ï¼šç”¨äºè§£é‡Šæ½œåœ¨å‘é‡çš„è¨€è¯­åŒ–å™¨å¯èƒ½äº§ç”Ÿä¸å‡†ç¡®çš„æè¿°ï¼Œä½†è¿™ä¸å½±å“åŠ¨ä½œæ‰§è¡Œã€‚\n    *   **æ¡†æ¶å¤æ‚æ€§**ï¼šæ¶‰åŠå¤šé˜¶æ®µè®­ç»ƒï¼ˆæ•™å¸ˆ RL è®­ç»ƒã€å­¦ç”Ÿè’¸é¦ã€ç­–ç•¥å­¦ä¹ ï¼‰ï¼Œæµç¨‹ç›¸å¯¹å¤æ‚ã€‚\n\n---\n\n### æ€»ç»“ï¼šä¸åŒå‘å±•æ–¹å‘çš„å¯¹æ¯”\n\n| å‘å±•æ–¹å‘ | æ ¸å¿ƒæ€è·¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ | ä»£è¡¨å·¥ä½œ |\n| :--- | :--- | :--- | :--- | :--- |\n| **ç›‘ç£ CoT** | å­¦ä¹ æ ‡æ³¨çš„æ¨ç†æ­¥éª¤ | æå‡æ³›åŒ–ï¼Œè¿‡ç¨‹å¯è§£é‡Š | ä¾èµ–æ ‡æ³¨ï¼Œå»¶è¿Ÿé«˜ | CoT-VLA, MolmoAct |\n| **RL CoT** | ç”¨å¥–åŠ±ä¼˜åŒ–æ¨ç†ç”Ÿæˆ | å…æ ‡æ³¨ï¼Œæ¨ç†ä¸ä»»åŠ¡å¼ºç›¸å…³ | å»¶è¿Ÿæé«˜ï¼Œè®­ç»ƒä¸ç¨³å®š | **ThinkAct** (å‰ä½œ) |\n| **é•¿åº¦æ§åˆ¶** | å¼ºåˆ¶ç”Ÿæˆæ›´çŸ­æ¨ç† | ç®€å•ï¼Œç›´æ¥æé€Ÿ | æ˜“æŸå¤±ä¿¡æ¯ï¼Œæ€§èƒ½ä¸‹é™ | ECoT-Lite |\n| **æ½œåœ¨æ¨ç†** | å°†æ¨ç†å‹ç¼©ä¸ºè¿ç»­å‘é‡ | **é«˜æ•ˆä¸æ€§èƒ½å…¼å¾—**ï¼Œä¿ç•™è§„åˆ’èƒ½åŠ› | æ¡†æ¶å¤æ‚ï¼Œè§£é‡Šä¾èµ–é¢å¤–æ¨¡å‹ | **Fast-ThinkAct** |\n\n**ç»“è®º**ï¼šFast-ThinkAct ç«™åœ¨å…¶å‰ä½œ ThinkActï¼ˆRL CoTï¼‰çš„è‚©è†€ä¸Šï¼Œæ•é”åœ°æŠ“ä½äº†â€œæ¨ç†å»¶è¿Ÿâ€è¿™ä¸€åˆ¶çº¦å…·èº« AI å®é™…åº”ç”¨çš„ç“¶é¢ˆã€‚å®ƒæ²¡æœ‰é€‰æ‹©ç®€å•çš„â€œç¼©çŸ­æ–‡æœ¬â€è·¯å¾„ï¼Œè€Œæ˜¯å¼€åˆ›æ€§åœ°è½¬å‘â€œ**æ½œåœ¨ç©ºé—´æ¨ç†**â€ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„åå¥½è’¸é¦å’Œè§†è§‰å¯¹é½æœºåˆ¶ï¼ŒæˆåŠŸåœ°å°†ä¸°å¯Œçš„è¯­è¨€å’Œè§†è§‰è§„åˆ’ä¿¡æ¯å‹ç¼©åˆ°æå°‘çš„æ½œåœ¨ token ä¸­ã€‚è¿™é¡¹å·¥ä½œæ ‡å¿—ç€ VLA æ¨ç†æ¨¡å‹ä»â€œè¿½æ±‚èƒ½åŠ›â€åˆ°â€œ**å…¼é¡¾èƒ½åŠ›ä¸æ•ˆç‡**â€çš„é‡è¦è½¬å˜ï¼Œä¸ºå®æ—¶ã€é«˜æ•ˆçš„å…·èº«æ™ºèƒ½ç³»ç»Ÿæä¾›äº†å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆã€‚æœªæ¥çš„å·¥ä½œå¯èƒ½ä¼šè¿›ä¸€æ­¥æ¢ç´¢æ›´ç®€å•çš„å•é˜¶æ®µè®­ç»ƒã€æ›´é²æ£’çš„æ½œåœ¨è¡¨ç¤ºå­¦ä¹ ï¼Œä»¥åŠå¦‚ä½•å½»åº•æ¶ˆé™¤å¯¹ç‹¬ç«‹è¨€è¯­åŒ–å™¨çš„ä¾èµ–ã€‚",
            "timestamp": "2026-01-20T17:58:55.371186",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## Fast-ThinkAct çš„å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼ŒFast-ThinkAct çš„æ ¸å¿ƒå‰ä½œæ˜¯ **ThinkAct** (Huang et al., 2025)ã€‚æ­¤å¤–ï¼Œè®ºæ–‡åœ¨â€œç›¸å…³å·¥ä½œâ€éƒ¨åˆ†è¯¦ç»†å›é¡¾äº†å…¶ä»–å‡ ç±»ç›¸å…³ç ”ç©¶ï¼ŒåŒ…æ‹¬åŸºç¡€VLAæ¨¡å‹å’Œæ¨ç†VLAæ¨¡å‹ã€‚Fast-ThinkAct ä¸»è¦æ˜¯åœ¨ ThinkAct çš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œæ—¨åœ¨è§£å†³å…¶æ¨ç†å»¶è¿Ÿé«˜çš„é—®é¢˜ã€‚\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†åˆ—å‡ºäº†å…¶æ ¸å¿ƒå‰ä½œï¼ˆThinkActï¼‰åŠå…¶ä»–å…³é”®ç›¸å…³å·¥ä½œçš„å¯¹æ¯”ï¼Œå¹¶é‡ç‚¹é˜è¿°äº† Fast-ThinkAct çš„å…·ä½“æ”¹è¿›ã€‚\n\n| å‰ä½œ/ç›¸å…³å·¥ä½œ | æ ¸å¿ƒæ–¹æ³• | ä¸»è¦è´¡çŒ®/ç‰¹ç‚¹ | ä¸ Fast-ThinkAct çš„åŒºåˆ«åŠä¸»è¦æ”¹å–„ |\n| :--- | :--- | :--- | :--- |\n| **ThinkAct** (Huang et al., 2025) | **åŸºäºRLçš„æ˜¾å¼æ–‡æœ¬æ¨ç†**ã€‚ä½¿ç”¨GRPOï¼ˆGroup Relative Policy Optimizationï¼‰è®­ç»ƒï¼Œç”Ÿæˆå†—é•¿çš„æ–‡æœ¬é“¾å¼æ¨ç†ï¼ˆCoTï¼‰è½¨è¿¹ï¼Œä»¥è§†è§‰å¥–åŠ±ï¼ˆå¦‚ç›®æ ‡å®Œæˆåº¦ã€è½¨è¿¹å¯¹é½ï¼‰ä¸ºæŒ‡å¯¼ã€‚ | é¦–æ¬¡å°†å¼ºåŒ–å­¦ä¹ ä¸æ¨ç†ç›¸ç»“åˆç”¨äºVLAä»»åŠ¡ï¼Œæ˜¾è‘—æå‡äº†é•¿è§†é‡è§„åˆ’ã€æ³›åŒ–èƒ½åŠ›å’Œå¤±è´¥æ¢å¤èƒ½åŠ›ã€‚ | **æ ¸å¿ƒåŒºåˆ«ï¼šæ¨ç†å½¢å¼ä¸æ•ˆç‡**ã€‚<br>1. **æ¨ç†å½¢å¼**ï¼šThinkAct ç”Ÿæˆ**æ˜¾å¼ã€å†—é•¿çš„æ–‡æœ¬CoT**ï¼ˆçº¦250ä¸ªtokenï¼‰ï¼Œè€Œ Fast-ThinkAct å­¦ä¹ **ç´§å‡‘ã€è¿ç»­çš„æ½œåœ¨è¡¨ç¤º**ï¼ˆå¦‚6ä¸ªæ½œåœ¨tokenï¼‰è¿›è¡Œå†…éƒ¨æ¨ç†ã€‚<br>2. **æ•ˆç‡æå‡**ï¼šFast-ThinkAct é€šè¿‡**å¯è¨€è¯­åŒ–çš„æ½œåœ¨è§„åˆ’**ï¼Œå®ç°äº†é«˜è¾¾ **89.3% çš„æ¨ç†å»¶è¿Ÿé™ä½**ï¼ˆç›¸æ¯”ThinkAct-7Bï¼‰ï¼ŒåŒæ—¶æ€§èƒ½è¿˜æœ‰æ‰€æå‡ã€‚<br>3. **è®­ç»ƒæœºåˆ¶**ï¼šThinkAct ä¾èµ–RLç›´æ¥ä¼˜åŒ–æ–‡æœ¬ç”Ÿæˆï¼›Fast-ThinkAct é‡‡ç”¨**åå¥½å¼•å¯¼çš„è’¸é¦**ï¼Œä»ä¸€ä¸ªç»è¿‡RLè®­ç»ƒçš„æ•™å¸ˆæ¨¡å‹ï¼ˆå³ThinkActé£æ ¼çš„æ¨¡å‹ï¼‰ä¸­ï¼Œå°†é«˜è´¨é‡æ¨ç†æ¨¡å¼æç‚¼åˆ°å­¦ç”Ÿçš„æ½œåœ¨ç©ºé—´ä¸­ã€‚ |\n| **ç›‘ç£å¼æ¨ç†VLA** (å¦‚ Embodied CoT, CoT-VLA, MolmoAct) | **åŸºäºæ ‡æ³¨çš„æ˜¾å¼æ¨ç†**ã€‚ä½¿ç”¨å¸¦æœ‰ä¸­é—´æ¨ç†æ­¥éª¤ï¼ˆæ–‡æœ¬æˆ–è§†è§‰å­ç›®æ ‡/è½¨è¿¹ï¼‰çš„æ ‡æ³¨æ•°æ®è¿›è¡Œç›‘ç£è®­ç»ƒã€‚ | é€šè¿‡å¼•å…¥æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œæ”¹å–„äº†åŸºç¡€VLAæ¨¡å‹åœ¨è§„åˆ’ã€æ³›åŒ–æ–¹é¢çš„ä¸è¶³ã€‚ | **æ ¸å¿ƒåŒºåˆ«ï¼šå¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ä¸æ¨ç†æ•ˆç‡**ã€‚<br>1. **æ•°æ®éœ€æ±‚**ï¼šç›‘ç£æ–¹æ³•éœ€è¦å¤§é‡äººå·¥æˆ–æ¨¡å‹ç”Ÿæˆçš„æ¨ç†æ ‡æ³¨ï¼Œæˆæœ¬é«˜ä¸”å—é™äºæ•°æ®è¦†ç›–èŒƒå›´ã€‚Fast-ThinkAct é€šè¿‡**ä»RLè®­ç»ƒçš„æ•™å¸ˆæ¨¡å‹ä¸­è¿›è¡Œè’¸é¦**ï¼Œå‡å°‘äº†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ç›´æ¥ä¾èµ–ã€‚<br>2. **æ¨ç†æ•ˆç‡**ï¼šè¿™äº›æ–¹æ³•åŒæ ·ç”Ÿæˆæ˜¾å¼æ¨ç†æ­¥éª¤ï¼Œå­˜åœ¨å»¶è¿Ÿé—®é¢˜ã€‚Fast-ThinkAct çš„**æ½œåœ¨æ¨ç†**ä»æ ¹æœ¬ä¸Šå‹ç¼©äº†æ¨ç†è¿‡ç¨‹ï¼Œæ•ˆç‡æ›´é«˜ã€‚<br>3. **ä¿¡æ¯ä¿çœŸåº¦**ï¼šç›´æ¥å‡å°‘æ–‡æœ¬æ¨ç†é•¿åº¦ï¼ˆå¦‚ECoT-Liteçš„æ¨ç†ä¸¢å¼ƒï¼‰å¯èƒ½å¯¼è‡´å…³é”®ä¿¡æ¯ä¸¢å¤±ã€‚Fast-ThinkAct é€šè¿‡**è§†è§‰è½¨è¿¹å¯¹é½æŸå¤±**ï¼Œç¡®ä¿æ½œåœ¨ç©ºé—´ä¿ç•™äº†å¯¹äºå…·èº«æ§åˆ¶è‡³å…³é‡è¦çš„**ç©ºé—´-æ—¶é—´åŠ¨æ€ä¿¡æ¯**ã€‚ |\n| **åŸºç¡€VLAæ¨¡å‹** (å¦‚ RT-2, Octo, OpenVLA) | **æ¨¡ä»¿å­¦ä¹ **ã€‚åœ¨å¤§è§„æ¨¡æœºå™¨äººæ¼”ç¤ºæ•°æ®ä¸Šè¿›è¡Œç«¯åˆ°ç«¯ç­–ç•¥è®­ç»ƒï¼Œç›´æ¥å­¦ä¹ ä»è§‚å¯Ÿåˆ°åŠ¨ä½œçš„æ˜ å°„ã€‚ | å»ºç«‹äº†å¼ºå¤§çš„VLAåŸºç¡€ï¼Œåœ¨è®­ç»ƒåˆ†å¸ƒå†…çš„æŠ€èƒ½ï¼ˆå¦‚æŠ“å–-æ”¾ç½®ï¼‰ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ | **æ ¸å¿ƒåŒºåˆ«ï¼šæ¨ç†ä¸æ³›åŒ–èƒ½åŠ›**ã€‚<br>1. **æ³›åŒ–èƒ½åŠ›**ï¼šåŸºç¡€VLAæ¨¡å‹ç¼ºä¹æ˜¾å¼æ¨ç†æœºåˆ¶ï¼Œåœ¨**é•¿è§†é‡è§„åˆ’ã€åº”å¯¹æ–°åœºæ™¯ã€å¤±è´¥æ¢å¤**æ–¹é¢èƒ½åŠ›æœ‰é™ã€‚Fast-ThinkAct æ˜ç¡®å¼•å…¥äº†**å†…éƒ¨æ¨ç†æ¨¡å—**ï¼Œæ˜¾è‘—å¢å¼ºäº†è¿™äº›èƒ½åŠ›ã€‚<br>2. **æ¶æ„**ï¼šåŸºç¡€VLAé€šå¸¸æ˜¯å•ä¸€çš„ç­–ç•¥ç½‘ç»œã€‚Fast-ThinkAct é‡‡ç”¨**ä¸¤é˜¶æ®µæ¶æ„**ï¼šä¸€ä¸ªè¿›è¡Œæ½œåœ¨æ¨ç†çš„VLMï¼Œåæ¥ä¸€ä¸ªä»¥æ¨ç†ç»“æœä¸ºæ¡ä»¶çš„åŠ¨ä½œæ¨¡å‹ï¼Œå®ç°äº†**é«˜å±‚è§„åˆ’ä¸åº•å±‚æ‰§è¡Œçš„è§£è€¦ä¸æ¡¥æ¥**ã€‚ |\n| **é«˜æ•ˆæ¨ç†çš„LLMæ–¹æ³•** (å¦‚ CODI, Soft Thinking) | **æ½œåœ¨ç©ºé—´æ¨ç†**ã€‚å°†é“¾å¼æ€ç»´æç‚¼åˆ°è¿ç»­ç©ºé—´ï¼ˆå¦‚éšè—çŠ¶æ€ã€æ¦‚å¿µtokenï¼‰ã€‚ | åœ¨çº¯è¯­è¨€æ¨¡å‹ä¸­æ¢ç´¢äº†å‹ç¼©æ¨ç†ã€æå‡æ•ˆç‡çš„æ–¹æ³•ã€‚ | **æ ¸å¿ƒåŒºåˆ«ï¼šæ¨¡æ€ä¸ä»»åŠ¡é€‚é…**ã€‚<br>1. **å¤šæ¨¡æ€éœ€æ±‚**ï¼šLLMçš„æ½œåœ¨æ¨ç†æ–¹æ³•æ— æ³•ç›´æ¥å¤„ç†**è§†è§‰ç©ºé—´ç†è§£å’Œæ—¶ç©ºåŠ¨æ€**ã€‚Fast-ThinkAct ä¸“é—¨è®¾è®¡äº†**åŠ¨ä½œå¯¹é½çš„è§†è§‰è§„åˆ’è’¸é¦**å’Œ**ç©ºé—´tokenå¹¶è¡Œé¢„æµ‹è½¨è¿¹**ï¼Œä½¿æ½œåœ¨è¡¨ç¤ºèƒ½å¤Ÿæ‰¿è½½è§†è§‰è§„åˆ’ä¿¡æ¯ã€‚<br>2. **ä¸åŠ¨ä½œæ‰§è¡Œçš„è¿æ¥**ï¼šFast-ThinkAct ä¸ä»…è¿›è¡Œæ¨ç†ï¼Œè¿˜é€šè¿‡**æ¨ç†å¢å¼ºçš„ç­–ç•¥å­¦ä¹ **ï¼Œå°†å­¦åˆ°çš„æ½œåœ¨è§„åˆ’æ˜ç¡®åœ°è¿æ¥åˆ°ä¸‹æ¸¸åŠ¨ä½œæ¨¡å‹çš„KVç¼“å­˜ä¸­ï¼ŒæŒ‡å¯¼åŠ¨ä½œç”Ÿæˆï¼Œè¿™æ˜¯çº¯æ–‡æœ¬æ¨¡å‹ä¸å…·å¤‡çš„ã€‚ |\n\n## Fast-ThinkAct ç›¸æ¯”äºå‰ä½œï¼ˆThinkActï¼‰çš„å…·ä½“æ”¹åŠ¨ä¸æ”¹å–„\n\n1.  **æ¨ç†è¡¨ç¤ºçš„æ ¹æœ¬æ€§å˜é©**ï¼š\n    *   **å‰ä½œ (ThinkAct)**ï¼šè¾“å‡ºä¸ºç¦»æ•£çš„ã€äººç±»å¯è¯»çš„**æ–‡æœ¬åºåˆ—**ä½œä¸ºæ¨ç†é“¾ã€‚ç”Ÿæˆè¿‡ç¨‹è‡ªå›å½’ï¼Œé€Ÿåº¦æ…¢ã€‚\n    *   **æœ¬æ–‡ (Fast-ThinkAct)**ï¼šè¾“å‡ºä¸º**ç´§å‡‘çš„è¿ç»­æ½œåœ¨å‘é‡** (`z`) å’Œ**å¹¶è¡Œçš„ç©ºé—´token** (`s`)ã€‚æ½œåœ¨å‘é‡ç¼–ç æŠ½è±¡æ¨ç†é€»è¾‘ï¼Œç©ºé—´tokenç›´æ¥æ˜ å°„ä¸ºè§†è§‰è½¨è¿¹è·¯å¾„ç‚¹ã€‚è¿™å®ç°äº†**æ¨ç†çš„éšå¼åŒ–å’Œå¹¶è¡ŒåŒ–**ï¼Œæ˜¯é€Ÿåº¦æå‡çš„æ ¸å¿ƒã€‚\n\n2.  **åˆ›æ–°çš„è®­ç»ƒæ¡†æ¶ï¼šåå¥½å¼•å¯¼è’¸é¦ä¸è§†è§‰å¯¹é½**ï¼š\n    *   **å‰ä½œ (ThinkAct)**ï¼šä½¿ç”¨GRPOç›´æ¥ä¼˜åŒ–æ–‡æœ¬ç”Ÿæˆç­–ç•¥ï¼Œå¥–åŠ±ä¿¡å·åŸºäºæœ€ç»ˆåŠ¨ä½œçš„æˆåŠŸä¸å¦ã€‚\n    *   **æœ¬æ–‡ (Fast-ThinkAct)**ï¼š\n        *   **æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶**ï¼šä½¿ç”¨ä¸€ä¸ªç»è¿‡GRPOè®­ç»ƒçš„**æ–‡æœ¬æ•™å¸ˆæ¨¡å‹**ï¼ˆå³ThinkActï¼‰æ¥ç”Ÿæˆè´¨é‡ä¸ä¸€çš„æ¨ç†è½¨è¿¹ã€‚\n        *   **åå¥½è’¸é¦** (`â„’_verb`)ï¼šå¼•å…¥ä¸€ä¸ª**è¨€è¯­åŒ–å™¨**ï¼Œå°†å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆçš„æ½œåœ¨å‘é‡è§£ç å›æ–‡æœ¬ã€‚åˆ©ç”¨æ•™å¸ˆæ¨¡å‹æ¨ç†è½¨è¿¹çš„ä¼˜åŠ¿å‡½æ•°æ„å»ºåå¥½å¯¹ï¼Œé€šè¿‡ç±»ä¼¼DPOçš„ç›®æ ‡ï¼Œé¼“åŠ±è¨€è¯­åŒ–å™¨å°†æ½œåœ¨å‘é‡è§£ç ä¸ºé«˜è´¨é‡æ¨ç†ï¼ŒæŠ‘åˆ¶ä½è´¨é‡æ¨ç†ã€‚è¿™ç¡®ä¿äº†**æ½œåœ¨ç©ºé—´ä¿ç•™äº†æœ‰æ•ˆçš„æ¨ç†æ¨¡å¼**ã€‚\n        *   **è§†è§‰è§„åˆ’è’¸é¦** (`â„’_distill`)ï¼šé™¤äº†è¯­è¨€æ¨ç†ï¼Œè¿˜é€šè¿‡å¯¹é½æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹åœ¨`<answer>` tokençš„éšè—çŠ¶æ€ï¼Œç›´æ¥**è’¸é¦æ•™å¸ˆçš„è§†è§‰è§„åˆ’èƒ½åŠ›**åˆ°å­¦ç”Ÿçš„æ½œåœ¨è¡¨ç¤ºä¸­ã€‚è¿™æ˜¯ä¿è¯æ½œåœ¨è¡¨ç¤ºå¯¹å…·èº«ä»»åŠ¡æœ‰æ•ˆçš„å…³é”®ã€‚\n        *   **è½¨è¿¹é¢„æµ‹** (`â„’_ans`)ï¼šå­¦ç”Ÿæ¨¡å‹ä½¿ç”¨å¹¶è¡Œçš„ç©ºé—´tokenç›´æ¥å›å½’2Dè§†è§‰è½¨è¿¹è·¯å¾„ç‚¹ï¼Œæ›¿ä»£äº†æ•™å¸ˆæ¨¡å‹è‡ªå›å½’ç”Ÿæˆæ–‡æœ¬æè¿°è·¯å¾„ç‚¹çš„æ–¹å¼ï¼Œæ•ˆç‡æ›´é«˜ã€‚\n\n3.  **æ¨ç†ä¸åŠ¨ä½œæ‰§è¡Œçš„é«˜æ•ˆæ¡¥æ¥**ï¼š\n    *   **å‰ä½œ (ThinkAct)**ï¼šæ¨ç†æ–‡æœ¬ä½œä¸ºæ¡ä»¶è¾“å…¥åˆ°ç­–ç•¥ç½‘ç»œä¸­ï¼Œè¿æ¥å¯èƒ½ä¸å¤Ÿç´§å¯†ã€‚\n    *   **æœ¬æ–‡ (Fast-ThinkAct)**ï¼šä»å­¦ç”ŸVLMçš„**ç©ºé—´tokençš„Key-Valueç¼“å­˜ï¼ˆæ—©æœŸå±‚ï¼‰ä¸­æå–è§†è§‰æ½œåœ¨è§„åˆ’** (`c_t`)ï¼Œå°†å…¶ä¸åŠ¨ä½œæ¨¡å‹çš„çŠ¶æ€ç¼–ç KVç¼“å­˜æ‹¼æ¥ã€‚åŠ¨ä½œæ¨¡å‹é€šè¿‡äº¤å‰æ³¨æ„åŠ›åŒæ—¶å…³æ³¨åŸå§‹è§‚å¯Ÿå’Œè¿™ä¸ª**å¯Œå«è§„åˆ’ä¿¡æ¯çš„æ½œåœ¨ä¸Šä¸‹æ–‡**ã€‚è¿™ç§åœ¨è¡¨ç¤ºå±‚é¢çš„èåˆæ›´ç›´æ¥ã€é«˜æ•ˆã€‚\n\n4.  **æ€§èƒ½ä¸æ•ˆç‡çš„æ˜¾è‘—æå‡**ï¼š\n    *   **æ•ˆç‡**ï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼Œç›¸æ¯”ThinkAct-7Bå®ç°**æœ€é«˜89.3%çš„å»¶è¿Ÿé™ä½**ï¼Œç›¸æ¯”ThinkAct-3Bå¿«**7å€**ã€‚\n    *   **æ€§èƒ½**ï¼šåœ¨LIBEROã€SimplerEnvã€RoboTwin2.0ç­‰æœºå™¨äººæ“ä½œåŸºå‡†ä¸Šï¼Œ**æˆåŠŸç‡è¾¾åˆ°æˆ–è¶…è¿‡ThinkAct**ã€‚åœ¨EgoPlan-Bench2ã€RoboVQAç­‰å…·èº«æ¨ç†åŸºå‡†ä¸Šï¼Œ**æ€§èƒ½æ˜¾è‘—è¶…è¿‡ThinkActåŠå…¶ä»–åŸºçº¿**ã€‚å®ç°äº†**â€œæ—¢å¿«åˆå¥½â€** çš„ç›®æ ‡ã€‚\n\n**æ€»ç»“**ï¼šFast-ThinkAct çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°† ThinkAct çš„**æ˜¾å¼ã€ç¼“æ…¢çš„æ–‡æœ¬æ¨ç†è¿‡ç¨‹**ï¼Œé€šè¿‡ä¸€å¥—ç²¾å¿ƒè®¾è®¡çš„**åå¥½å¼•å¯¼è’¸é¦ä¸è§†è§‰å¯¹é½æœºåˆ¶**ï¼Œå‹ç¼©å¹¶è½¬åŒ–ä¸º**éšå¼ã€é«˜æ•ˆçš„æ½œåœ¨ç©ºé—´è¡¨ç¤º**ã€‚å®ƒä¸ä»…å¤§å¹…æå‡äº†æ¨ç†é€Ÿåº¦ï¼Œè¿˜é€šè¿‡æ›´å¥½çš„ä¿¡æ¯ä¿ç•™ï¼ˆå°¤å…¶æ˜¯è§†è§‰è§„åˆ’ä¿¡æ¯ï¼‰å’Œä¸åŠ¨ä½œæ¨¡å‹æ›´ç´§å¯†çš„é›†æˆï¼Œç»´æŒç”šè‡³æå‡äº†åœ¨å¤æ‚å…·èº«ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¿™æ˜¯å¯¹å‰ä½œThinkActåœ¨**æ•ˆç‡ç“¶é¢ˆ**é—®é¢˜ä¸Šçš„ä¸€ä¸ªæ ¹æœ¬æ€§è§£å†³æ–¹æ¡ˆã€‚",
            "timestamp": "2026-01-20T17:59:57.248252",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## è®ºæ–‡å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\nè®ºæ–‡ã€ŠFast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planningã€‹çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§**é«˜æ•ˆã€å¯è¨€è¯­åŒ–çš„æ½œåœ¨è§„åˆ’æ¡†æ¶**ï¼Œå°†å†—é•¿çš„æ˜¾å¼æ€ç»´é“¾ï¼ˆCoTï¼‰å‹ç¼©ä¸ºç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºï¼Œä»è€Œåœ¨ä¿æŒå¼ºå¤§æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå¤§å¹…é™ä½æ¨ç†å»¶è¿Ÿã€‚å…¶å…³é”®æŠ€æœ¯æ–¹æ³•å¯å½’çº³ä¸ºä»¥ä¸‹å››ä¸ªç›¸äº’å…³è”çš„æ¨¡å—ï¼š\n\n| å…³é”®æŠ€æœ¯æ–¹æ³• | æ ¸å¿ƒç›®æ ‡ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°ä¸å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. åŸºäºå¥–åŠ±åå¥½çš„å¯è¨€è¯­åŒ–æ½œåœ¨æ€ç»´é“¾** | å°†æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„è´¨é‡ä¸ä¸€çš„å†—é•¿æ–‡æœ¬CoTï¼Œè’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹çš„ç´§å‡‘æ½œåœ¨ç©ºé—´ä¸­ï¼Œå¹¶ç¡®ä¿è¯¥æ½œåœ¨è¡¨ç¤ºå¯è¢«â€œè¨€è¯­åŒ–â€ä¸ºé«˜è´¨é‡æ¨ç†æ–‡æœ¬ã€‚ | **1.1 æ•™å¸ˆæ¨¡å‹è®­ç»ƒ (GRPO):** <br> æ•™å¸ˆè§†è§‰è¯­è¨€æ¨¡å‹ $\\mathcal{F}_{\\theta}^{T}$ é€šè¿‡åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥ç”Ÿæˆä¸åŠ¨ä½œå¥–åŠ±å¯¹é½çš„æ¨ç†è½¨è¿¹ $\\tau$ã€‚å…¶ç›®æ ‡å‡½æ•°ä¸ºï¼š <br> $\\mathcal{J}_{\\text{GRPO}}(\\theta) = \\mathbb{E}_{\\tau\\sim\\mathcal{F}^{T}_{\\theta}}[\\min(r_{\\theta}(\\tau)A(\\tau), \\text{clip}(r_{\\theta}(\\tau),1-\\epsilon,1+\\epsilon)A(\\tau))]$ <br> å…¶ä¸­ï¼Œ$r_{\\theta}(\\tau) = \\frac{\\mathcal{F}^{T}_{\\theta}(\\tau)}{\\mathcal{F}^{T}_{\\text{old}}(\\tau)}$ ä¸ºæ¦‚ç‡æ¯”ï¼Œ$A(\\tau)$ ä¸ºä¼˜åŠ¿å‡½æ•°ï¼Œè®¡ç®—åŸºäºç»„å†…å¥–åŠ± $\\{R_i\\}_{i\\in G(\\tau)}$ çš„æ ‡å‡†åŒ–å€¼ï¼š$A(\\tau) = \\frac{R_{\\tau} - \\text{mean}(\\{R_i\\})}{\\text{std}(\\{R_i\\})}$ã€‚ <br><br> **1.2 åå¥½å¯¹æ„å»ºä¸æ½œåœ¨è’¸é¦:** <br> ä»æ¯ç»„ rollout ä¸­æ ¹æ®ä¼˜åŠ¿å‡½æ•° $A(\\tau)$ é€‰æ‹©æœ€ä¼˜æ¨ç† $\\tau^+$ å’Œæœ€å·®æ¨ç† $\\tau^-$ æ„æˆåå¥½å¯¹ã€‚å­¦ç”Ÿæ¨¡å‹ $\\mathcal{F}_{\\theta}$ ä¸ç”Ÿæˆæ–‡æœ¬ï¼Œè€Œæ˜¯è‡ªå›å½’åœ°ç”Ÿæˆ **M ä¸ªè¿ç»­æ½œåœ¨å‘é‡** $\\mathbf{z} = \\{z_m\\}_{m=1}^{M}, z_m \\in \\mathbb{R}^d$ã€‚ <br><br> **1.3 å¯è¨€è¯­åŒ–æŸå¤± ($\\mathcal{L}_{\\text{verb}}$):** <br> å¼•å…¥ä¸€ä¸ª**è¨€è¯­åŒ–å™¨** LLM $\\mathcal{V}_{\\psi}$ï¼Œå…¶ä»»åŠ¡æ˜¯å°†æ½œåœ¨å‘é‡ $\\mathbf{z}$ è§£ç ä¸ºè‡ªç„¶è¯­è¨€ã€‚è®­ç»ƒç›®æ ‡å€Ÿé‰´ DPOï¼Œé¼“åŠ±è¨€è¯­åŒ–å™¨å°† $\\mathbf{z}$ è§£ç ä¸ºé«˜è´¨é‡æ¨ç† $\\tau^+$ çš„æ¦‚ç‡è¿œé«˜äºè§£ç ä¸ºä½è´¨é‡æ¨ç† $\\tau^-$ çš„æ¦‚ç‡ï¼š <br> $\\mathcal{L}_{\\text{verb}} = -\\mathbb{E}[\\log \\sigma(\\beta(\\log\\frac{p_{\\psi}(\\tau^+ \\mid \\mathbf{z})}{p_{\\text{ref}}(\\tau^+)} - \\log\\frac{p_{\\psi}(\\tau^- \\mid \\mathbf{z})}{p_{\\text{ref}}(\\tau^-)}))]$ <br> å…¶ä¸­ï¼Œ$p_{\\text{ref}}$ æ˜¯æ— æ½œåœ¨æ¡ä»¶çº¦æŸçš„å‚è€ƒæ¨¡å‹ï¼Œ$\\sigma$ ä¸º sigmoid å‡½æ•°ï¼Œ$\\beta$ æ§åˆ¶åå¥½å¼ºåº¦ã€‚ | - **éª¨å¹²æ¨¡å‹:** Qwen2.5-VL 3Bã€‚ <br> - **æ½œåœ¨å‘é‡ç»´åº¦ $d$:** ä¸æ¨¡å‹éšè—å±‚å¤§å°ä¸€è‡´ï¼ˆæœªæ˜ç¡®ç»™å‡ºï¼Œé€šå¸¸ä¸º1024æˆ–2048ï¼‰ã€‚ <br> - **æ½œåœ¨æ¨ç†æ­¥æ•° $M$:** **6**ï¼ˆç»æ¶ˆèå®éªŒç¡®å®šä¸ºæœ€ä¼˜ï¼‰ã€‚ <br> - **åå¥½å¼ºåº¦ $\\beta$:** **0.1**ã€‚ <br> - **è¨€è¯­åŒ–å™¨æ¨¡å‹:** åŸºäº Qwen3-0.6B åˆå§‹åŒ–ï¼Œå¹¶æ’å…¥è·¨æ³¨æ„åŠ›å±‚ä»¥æ¥æ”¶æ½œåœ¨å‘é‡ $\\mathbf{z}$ ä½œä¸ºæ¡ä»¶ã€‚ <br> - **GRPO Rollout å¤§å° $N$:** **5**ã€‚ |\n| **2. åŠ¨ä½œå¯¹é½çš„è§†è§‰è§„åˆ’è’¸é¦** | ç¡®ä¿å­¦ç”Ÿæ¨¡å‹çš„æ½œåœ¨è¡¨ç¤ºä¸ä»…ç¼–ç è¯­è¨€æ¨ç†é€»è¾‘ï¼Œè¿˜èƒ½æ•è·å¯¹å…·èº«æ§åˆ¶è‡³å…³é‡è¦çš„**ç©ºé—´è§†è§‰è§„åˆ’èƒ½åŠ›**ã€‚ | **2.1 è½¨è¿¹è¡¨ç¤ºå¯¹é½ ($\\mathcal{L}_{\\text{distill}}$):** <br> æœ€å°åŒ–æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹åœ¨ç¼–ç è§†è§‰è§„åˆ’ä¿¡æ¯çš„å…³é”®ä½ç½®ï¼ˆå¦‚ `<answer>` æ ‡è®°ï¼‰çš„éšè—çŠ¶æ€ä¹‹é—´çš„å·®å¼‚ï¼š <br> $\\mathcal{L}_{\\text{distill}} = \\| h_t^T - h_t \\|_2^2$ <br> å…¶ä¸­ï¼Œ$h_t^T$ å’Œ $h_t$ åˆ†åˆ«æ¥è‡ªæ•™å¸ˆï¼ˆå¯¹åº” $\\tau^+$ï¼‰å’Œå­¦ç”Ÿæ¨¡å‹ã€‚ <br><br> **2.2 å¹¶è¡Œç©ºé—´è½¨è¿¹é¢„æµ‹ ($\\mathcal{L}_{\\text{ans}}$):** <br> æ•™å¸ˆæ¨¡å‹ä»¥è‡ªå›å½’æ–¹å¼ç”Ÿæˆæè¿°è·¯å¾„ç‚¹ $\\{p_k\\}_{k=1}^K$ çš„é•¿æ–‡æœ¬åºåˆ—ï¼ˆ$K=5$ æ—¶çº¦60-70ä¸ªtokenï¼‰ã€‚ä¸ºæå‡æ•ˆç‡ï¼Œå­¦ç”Ÿæ¨¡å‹ä½¿ç”¨ **K ä¸ªå¯å­¦ä¹ çš„ç©ºé—´token** $\\{\\mathbf{s}_i\\}_{i=1}^K$ é™„åŠ åœ¨æ½œåœ¨åºåˆ—åï¼Œå¹¶é€šè¿‡ä¸€ä¸ª MLP å°†æ¯ä¸ªç©ºé—´tokençš„è¾“å‡ºéšè—çŠ¶æ€ $h'(\\mathbf{s}_i)$ å¹¶è¡ŒæŠ•å½±ä¸ºè·¯å¾„ç‚¹åæ ‡ $p_i$ã€‚æŸå¤±å‡½æ•°ä¸ºï¼š <br> $\\mathcal{L}_{\\text{ans}} = \\sum_{i=1}^{K} \\| p_i - \\hat{p}_i \\|_2^2$ï¼Œ å…¶ä¸­ $p_i = \\text{MLP}(h'(\\mathbf{s}_i))$ï¼Œ$\\hat{p}_i$ æ˜¯çœŸå®è·¯å¾„ç‚¹ã€‚ | - **è·¯å¾„ç‚¹æ•°é‡ $K$:** **5**ã€‚ <br> - **è·¯å¾„ç‚¹åæ ‡ $p_i$:** å¯¹äºå•è‡‚ä»»åŠ¡ä¸º $[x_{\\text{single}}, y_{\\text{single}}]$ï¼Œå¯¹äºåŒè‡‚ä»»åŠ¡ä¸º $[x_{\\text{left}}, y_{\\text{left}}, x_{\\text{right}}, y_{\\text{right}}]$ï¼Œç»Ÿä¸€ç”¨ $\\mathbb{R}^6$ å‘é‡è¡¨ç¤ºï¼Œæœªä½¿ç”¨çš„ç»´åº¦åœ¨è®¡ç®—æŸå¤±æ—¶è¢«æ©ç ã€‚ <br> - **å­¦ç”Ÿæ¨¡å‹æ€»æŸå¤±:** $\\mathcal{L}_{\\text{student}} = \\mathcal{L}_{\\text{verb}} + \\mathcal{L}_{\\text{distill}} + \\mathcal{L}_{\\text{ans}}$ã€‚ |\n| **3. æ¨ç†å¢å¼ºçš„ç­–ç•¥å­¦ä¹ ** | å°†é«˜å±‚ã€ç´§å‡‘çš„æ½œåœ¨è§†è§‰è§„åˆ’ä¸ä½å±‚åŠ¨ä½œæ‰§è¡Œæ¡¥æ¥èµ·æ¥ï¼Œä½¿ç­–ç•¥æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨æ¨ç†ç»“æœç”Ÿæˆç²¾ç¡®åŠ¨ä½œã€‚ | **3.1 è§†è§‰æ½œåœ¨è§„åˆ’æå–:** <br> è®­ç»ƒå®Œæˆåï¼Œå†»ç»“å­¦ç”Ÿ VLM $\\mathcal{F}_{\\theta}$ã€‚åœ¨æ¨ç†æ—¶ï¼Œä» $\\mathcal{F}_{\\theta}$ **è¾ƒæµ…å±‚**çš„ç©ºé—´tokençš„ Key-Value (KV) ç¼“å­˜ä¸­æå–è§†è§‰æ½œåœ¨è§„åˆ’è¡¨ç¤º $c_t$ã€‚è®ºæ–‡å‘ç°æµ…å±‚è¡¨ç¤ºæ¯”æ·±å±‚æˆ–æœ€ç»ˆéšè—çŠ¶æ€æ›´èƒ½æœ‰æ•ˆä¿ç•™è§„åˆ’ä¿¡æ¯ã€‚ <br><br> **3.2 ç­–ç•¥æ¨¡å‹æ¡ä»¶åŒ–ä¸è®­ç»ƒ:** <br> åŠ¨ä½œæ¨¡å‹ $\\pi_{\\phi}$ï¼ˆå¦‚åŸºäº Diffusion Transformer çš„ RDTï¼‰é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒæ—¶å…³æ³¨çŠ¶æ€ç¼–ç å™¨çš„ KV å¯¹å’Œæ¥è‡ª VLM çš„è§„åˆ’ä¸Šä¸‹æ–‡ $c_t$ã€‚ç­–ç•¥é€šè¿‡æ¨¡ä»¿å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼ŒæŸå¤±å‡½æ•°ä¸ºï¼š <br> $\\mathcal{L}_{\\text{IL}}(\\phi) = \\ell(\\pi_{\\phi}(o_t, l, c_t), \\hat{a}_t)$ <br> å…¶ä¸­ $\\ell$ æ˜¯æ‰©æ•£ç­–ç•¥çš„å»å™ªç›®æ ‡ï¼ˆå¦‚å‡æ–¹è¯¯å·®ï¼‰ï¼Œ$\\hat{a}_t$ æ˜¯çœŸå®åŠ¨ä½œã€‚è®­ç»ƒæ—¶ï¼Œ$\\mathcal{F}_{\\theta}$ å’ŒçŠ¶æ€ç¼–ç å™¨è¢«å†»ç»“ï¼Œä»…æ›´æ–° $\\pi_{\\phi}$ å’Œç”¨äºé€‚é… $c_t$ ç»´åº¦çš„çº¿æ€§æŠ•å½±å±‚ã€‚ | - **åŠ¨ä½œæ¨¡å‹:** åœ¨ SimplerEnv ä¸Šä½¿ç”¨ DiT-Policyï¼Œåœ¨ LIBERO å’Œ RoboTwin2.0 ä¸Šä½¿ç”¨ RDTã€‚ <br> - **è§„åˆ’ä¸Šä¸‹æ–‡ $c_t$ çš„èåˆ:** é€šè¿‡çº¿æ€§æŠ•å½±å°† VLM KV ç¼“å­˜ç»´åº¦é€‚é…åˆ°åŠ¨ä½œæ¨¡å‹æ³¨æ„åŠ›ç»´åº¦ï¼ˆDiT-Policyä¸º1024ï¼ŒRDTä¸º2048ï¼‰ã€‚ <br> - **è®­ç»ƒæ•°æ®:** å¤§è§„æ¨¡æœºå™¨äººåŠ¨ä½œæ•°æ®ï¼ˆå¦‚ OXE æ•°æ®é›†ï¼‰ï¼Œé’ˆå¯¹ç‰¹å®šç¯å¢ƒï¼ˆå¦‚ LIBEROï¼‰è¿›è¡Œå°‘é‡æ¼”ç¤ºçš„å¾®è°ƒã€‚ |\n| **4. æ•´ä½“è®­ç»ƒä¸æ¨ç†æµç¨‹** | æ•´åˆä»¥ä¸Šæ–¹æ³•ï¼Œå®ç°ä»æ•™å¸ˆæ¨¡å‹çŸ¥è¯†è’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹é«˜æ•ˆæ¨ç†çš„ç­–ç•¥å­¦ä¹ å…¨æµç¨‹ã€‚ | **è®­ç»ƒåˆ†ä¸‰ä¸ªé˜¶æ®µ:** <br> 1. **SFT + CoT-SFT:** åœ¨æ··åˆæ•°æ®é›†ä¸Šå¯¹åŸºç¡€ VLM è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œå­¦ä¹ åŸºæœ¬è§†è§‰ç†è§£å’Œé“¾å¼æ€ç»´ã€‚ <br> 2. **æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦:** æ•™å¸ˆç”¨ GRPO è®­ç»ƒï¼›å­¦ç”Ÿç”¨ $\\mathcal{L}_{\\text{student}}$ è®­ç»ƒï¼Œå°†æ¨ç†å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ã€‚è¨€è¯­åŒ–å™¨å‰3000æ­¥ç”¨è¯­è¨€å»ºæ¨¡æŸå¤±é¢„çƒ­ï¼Œå1500æ­¥ç”¨ $\\mathcal{L}_{\\text{verb}}$ è¿›è¡Œåå¥½å­¦ä¹ ã€‚ <br> 3. **ç­–ç•¥å­¦ä¹ :** å†»ç»“è®­ç»ƒå¥½çš„å­¦ç”Ÿ V",
            "timestamp": "2026-01-20T18:01:00.910467",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä½¿ç”¨äº†**ä¸¤å¤§ç±»è¯„ä»·æŒ‡æ ‡**ï¼š**æœºå™¨äººæ“ä½œæˆåŠŸç‡**å’Œ**å…·èº«æ¨ç†èƒ½åŠ›è¯„åˆ†**ã€‚å…·ä½“ç»†èŠ‚å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æŒ‡æ ‡ç±»åˆ« | å…·ä½“æŒ‡æ ‡ | é€‚ç”¨ä»»åŠ¡/åŸºå‡† | æŒ‡æ ‡ç»†èŠ‚ä¸è¯´æ˜ |\n| :--- | :--- | :--- | :--- |\n| **æœºå™¨äººæ“ä½œ** | **ä»»åŠ¡æˆåŠŸç‡** | SimplerEnv, LIBERO, RoboTwin2.0 | æ ¸å¿ƒæŒ‡æ ‡ã€‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡ï¼Œä»¥**æˆåŠŸå®Œæˆä»»åŠ¡çš„è¯•éªŒæ¬¡æ•°æ¯”ä¾‹**ä½œä¸ºè¡¡é‡æ ‡å‡†ã€‚ä¾‹å¦‚ï¼Œåœ¨LIBEROçš„æ¯ä¸ªå­ä»»åŠ¡å¥—ä»¶ä¸Šè¿›è¡Œ500æ¬¡è¯•éªŒï¼Œè®¡ç®—å¹³å‡æˆåŠŸç‡ã€‚ |\n| | | | **RoboTwin2.0** è¿›ä¸€æ­¥åŒºåˆ†äº†**ç®€å•**å’Œ**å›°éš¾**ä¸¤ç§è®¾ç½®ï¼ˆåè€…åŒ…å«é¢†åŸŸéšæœºåŒ–ï¼‰ï¼Œå¹¶ä¾æ®ä¸“å®¶æ¼”ç¤ºæ­¥éª¤æ•°å°†ä»»åŠ¡åˆ†ä¸º**çŸ­ã€ä¸­ã€é•¿**è§†é‡ä»»åŠ¡ã€‚ |\n| **å…·èº«æ¨ç†** | **å‡†ç¡®ç‡** | EgoPlan-Bench2 | è¯„ä¼°ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒè§†è§’çš„å¤šæ­¥ä»»åŠ¡è§„åˆ’èƒ½åŠ›ã€‚æ•°æ®é›†åŒ…å«1,321é“**å¤šé¡¹é€‰æ‹©é¢˜**ï¼Œæ¨¡å‹éœ€æ ¹æ®ä»»åŠ¡ç›®æ ‡å’Œè¿›åº¦å†å²é¢„æµ‹ä¸‹ä¸€æ­¥åŠ¨ä½œï¼Œä»¥**å›ç­”æ­£ç¡®çš„æ¯”ä¾‹**ä½œä¸ºå¾—åˆ†ã€‚ |\n| | **BLEUåˆ†æ•°** | RoboVQA | è¯„ä¼°å¯¹æœºå™¨äººæ“ä½œè§†é¢‘çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚æ•°æ®é›†åŒ…å«1,893ä¸ª**å¼€æ”¾å¼é—®ç­”å¯¹**ã€‚ä½¿ç”¨**BLEU-1åˆ°BLEU-4**ä»¥åŠ**BLEU-Avg**ï¼ˆå¹³å‡ï¼‰åˆ†æ•°æ¥è¡¡é‡æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆä¹‹é—´çš„**n-gramé‡å åº¦**ï¼Œåˆ†æ•°è¶Šé«˜è¯´æ˜è¯­ä¹‰åŒ¹é…è¶Šå¥½ã€‚ |\n| | **LLMè¯„åˆ†** | OpenEQA, RoboFAC | è¯„ä¼°å¯¹å¼€æ”¾ä¸–ç•Œåœºæ™¯çš„é›¶æ ·æœ¬ç†è§£å’Œå¤±è´¥åˆ†æèƒ½åŠ›ã€‚ä½¿ç”¨**å¤§å‹è¯­è¨€æ¨¡å‹**æ ¹æ®äººç±»åå¥½å¯¹é½çš„è¯„åˆ†æ ‡å‡†ï¼Œå¯¹æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆè¿›è¡Œæ‰“åˆ†ã€‚è¿™æ˜¯ä¸€ç§**è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•çš„è¯„ä¼°æ–¹å¼**ï¼Œç”¨äºè¡¡é‡ç­”æ¡ˆçš„åˆç†æ€§å’Œå‡†ç¡®æ€§ã€‚ |\n| **æ•ˆç‡** | **æ¨ç†å»¶è¿Ÿ** | æ‰€æœ‰ä»»åŠ¡ï¼ˆå¯¹æ¯”åˆ†æï¼‰ | å…³é”®åˆ›æ–°æŒ‡æ ‡ã€‚æµ‹é‡æ¨¡å‹ä»æ¥æ”¶è¾“å…¥ï¼ˆè§‚å¯Ÿå’ŒæŒ‡ä»¤ï¼‰åˆ°è¾“å‡ºåŠ¨ä½œæ‰€éœ€çš„**å•æ¬¡æ¨ç†æ—¶é—´**ã€‚è®ºæ–‡å¼ºè°ƒï¼ŒFast-ThinkActé€šè¿‡å‹ç¼©æ¨ç†è¿‡ç¨‹ï¼Œå®ç°äº†é«˜è¾¾**89.3%çš„å»¶è¿Ÿé™ä½**å’Œ**9.3å€çš„åŠ é€Ÿ**ã€‚ |\n\n### äºŒã€ æ•°æ®é›†\n\nè®ºæ–‡ä½¿ç”¨äº†**å¤šç§å¤§è§„æ¨¡ã€å¤šæ¨¡æ€æ•°æ®é›†**è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œæ¶µç›–è§†è§‰è½¨è¿¹ã€é—®ç­”å¯¹å’Œæœºå™¨äººæ¼”ç¤ºæ•°æ®ã€‚\n\n#### 1. è®­ç»ƒæ•°æ®é›†\nç”¨äºè®­ç»ƒæ¨ç†VLMå’Œç­–ç•¥æ¨¡å‹ï¼Œå…·ä½“å¦‚ä¸‹ï¼š\n\n| æ•°æ®é›†åç§° | ç±»å‹/å†…å®¹ | ç”¨é€”ä¸ç»†èŠ‚ |\n| :--- | :--- | :--- |\n| **MolmoAct (OXE)** | **å•è‡‚2Dè§†è§‰è½¨è¿¹** | ä»Open X-Embodiment (OXE)æ•°æ®é›†ä¸­æå–ï¼Œçº¦**130ä¸‡æ¡è½¨è¿¹**ã€‚ç”¨äºå­¦ä¹ å•è‡‚æ“ä½œçš„ç©ºé—´è§„åˆ’å’ŒåŠ¨ä½œå…³è”ã€‚ |\n| **AIST** | **åŒè‡‚2Dè§†è§‰è½¨è¿¹** | ä»AISTæ•°æ®é›†ä¸­æå–åŒæ‰‹æœºå™¨äººæ“ä½œè½¨è¿¹ï¼Œçº¦**9.2ä¸‡æ¡æ ·æœ¬**ã€‚ä½¿ç”¨CoTracker3è¿›è¡Œè½¨è¿¹è·Ÿè¸ªï¼Œç”¨äºå­¦ä¹ åŒè‡‚åè°ƒã€‚ |\n| **PixMo** | é€šç”¨è§†è§‰-è¯­è¨€é—®ç­” | çº¦72.6ä¸‡æ ·æœ¬ï¼Œç”¨äºä¿æŒæ¨¡å‹çš„**é€šç”¨è§†è§‰ç†è§£èƒ½åŠ›**ï¼Œé˜²æ­¢åœ¨å…·èº«æ•°æ®ä¸Šè®­ç»ƒæ—¶å‘ç”Ÿç¾éš¾æ€§é—å¿˜ã€‚ |\n| **RoboFAC** | æœºå™¨äººå¤±è´¥åˆ†æä¸çº æ­£QA | åŒ…å«9,440æ¡é”™è¯¯æ“ä½œè½¨è¿¹å’Œ6.4ä¸‡QAå¯¹ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹çš„**å¤±è´¥è¯†åˆ«ã€åŸå› åˆ†æå’Œæ¢å¤è§„åˆ’**èƒ½åŠ›ã€‚ |\n| **RoboVQA** | æœºå™¨äººæ“ä½œè§†é¢‘QA | çº¦79.8ä¸‡QAå¯¹ï¼Œæ¶µç›–ä»»åŠ¡ç†è§£ã€ç©ºé—´æ¨ç†å’ŒåŠ¨ä½œé¢„æµ‹ï¼Œç”¨äºæå‡é•¿è§†é‡æ¨ç†èƒ½åŠ›ã€‚ |\n| **ShareRobot** | å¤§è§„æ¨¡æœºå™¨äººä»»åŠ¡è§„åˆ’QA | è¶…è¿‡**100ä¸‡QAå¯¹**ï¼Œæ¶µç›–ä»»åŠ¡è§„åˆ’ã€ç‰©ä½“åŠŸèƒ½ã€æ“ä½œç­–ç•¥ï¼Œä¿ƒè¿›å¯è¿ç§»æ“ä½œçŸ¥è¯†çš„å­¦ä¹ ã€‚ |\n| **EgoPlan-Bench** | è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä»»åŠ¡è§„åˆ’ | çº¦5.3ä¸‡è§†é¢‘-æ–‡æœ¬å¯¹ï¼Œç”¨äºè®­ç»ƒä»ç¬¬ä¸€äººç§°è§†è§’è¿›è¡Œ**é•¿è§†é‡è§„åˆ’å’Œè¿›åº¦è·Ÿè¸ª**çš„èƒ½åŠ›ã€‚ |\n| **Video-R1-CoT** | å¸¦æ€ç»´é“¾æ³¨é‡Šçš„è§†é¢‘QA | 16.5ä¸‡æ ·æœ¬ï¼Œæ¶µç›–æ•°å­¦é€»è¾‘ã€ç©ºé—´ç†è§£ç­‰å¤æ‚æ¨ç†ï¼Œç”¨äºæ³¨å…¥**é“¾å¼æ€ç»´æ¨ç†èƒ½åŠ›**ã€‚ |\n| **OXE (åŠ¨ä½œæ•°æ®)** | æœºå™¨äººæ¼”ç¤ºåŠ¨ä½œæ•°æ® | ç”¨äº**æ¨ç†å¢å¼ºçš„ç­–ç•¥å­¦ä¹ **é˜¶æ®µï¼Œè®­ç»ƒåŠ¨ä½œæ¨¡å‹ç”Ÿæˆä½å±‚æ§åˆ¶æŒ‡ä»¤ã€‚ |\n| **é™æ€ALOHAæ•°æ®é›†** | åŒè‡‚æœºå™¨äººæ¼”ç¤ºæ•°æ® | ä¸OXEæ•°æ®ç»“åˆï¼Œç”¨äºè®­ç»ƒåœ¨LIBEROå’ŒRoboTwin2.0ä¸Šè¯„ä¼°çš„RDTåŠ¨ä½œæ¨¡å‹ã€‚ |\n\n**è®­ç»ƒæµç¨‹**ï¼š\n1.  **ç›‘ç£å¾®è°ƒ**ï¼šæ··åˆä¸Šè¿°å¤šç§æ•°æ®ï¼ˆçº¦400ä¸‡æ ·æœ¬ï¼‰ï¼Œå­¦ä¹ åŸºç¡€è§†è§‰ç†è§£å’Œæ“ä½œçŸ¥è¯†ã€‚\n2.  **æ€ç»´é“¾å¾®è°ƒ**ï¼šä»SFTæ•°æ®ä¸­é‡‡æ ·5%ï¼Œå¹¶åŠ å…¥Video-R1-CoTæ•°æ®ï¼Œå­¦ä¹ ç”Ÿæˆç»“æ„åŒ–æ¨ç†ã€‚\n3.  **å¸ˆç”Ÿè®­ç»ƒ**ï¼šåœ¨CoT-SFTæ£€æŸ¥ç‚¹åŸºç¡€ä¸Šï¼Œä½¿ç”¨å¹³è¡¡é‡‡æ ·çš„çº¦5ä¸‡æ ·æœ¬ï¼Œè¿›è¡ŒGRPOæ•™å¸ˆè®­ç»ƒå’Œæ½œåœ¨ç©ºé—´å­¦ç”Ÿè’¸é¦ã€‚\n\n#### 2. è¯„ä¼°åŸºå‡†æ•°æ®é›†\nç”¨äºå…¨é¢æµ‹è¯•æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå…·ä½“å¦‚ä¸‹ï¼š\n\n| åŸºå‡†åç§° | è¯„ä¼°é‡ç‚¹ | ä»»åŠ¡ç»†èŠ‚ä¸æŒ‡æ ‡ |\n| :--- | :--- | :--- |\n| **SimplerEnv-Google** | **æ“ä½œæ³›åŒ–ä¸ä»¿çœŸåˆ°çœŸå®å…³è”** | æ¨¡æ‹ŸGoogleæœºå™¨äººä»»åŠ¡ï¼ŒåŒ…å«å…‰ç…§ã€çº¹ç†ã€èƒŒæ™¯ã€ç›¸æœºè§†è§’çš„å¤šæ ·åŒ–ã€‚è¯„ä¼°**ä»»åŠ¡æˆåŠŸç‡**ï¼Œè¯¥åŸºå‡†ä¸çœŸå®ä¸–ç•Œæ€§èƒ½æœ‰å¼ºç›¸å…³æ€§ã€‚ |\n| **LIBERO** | **å¤šæ ·åŒ–æ“ä½œæ³›åŒ–** | åŒ…å«å››ä¸ªå­å¥—ä»¶ï¼š<br>â€¢ **Spatial**ï¼šç©ºé—´å¸ƒå±€å˜åŒ–ã€‚<br>â€¢ **Object**ï¼šç‰©ä½“å¤šæ ·æ€§ã€‚<br>â€¢ **Goal**ï¼šç›®æ ‡å˜åŒ–ã€‚<br>â€¢ **Long**ï¼šé•¿è§†é‡ã€æ··åˆå˜åŒ–çš„è§„åˆ’ä»»åŠ¡ã€‚<br>æ¯ä¸ªå¥—ä»¶è¿›è¡Œ500æ¬¡è¯•éªŒï¼Œè¯„ä¼°**ä»»åŠ¡æˆåŠŸç‡**ã€‚ |\n| **RoboTwin2.0** | **å¤æ‚åŒè‡‚æ“ä½œä¸é•¿è§†é‡è§„åˆ’** | å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒè‡‚æ“ä½œåŸºå‡†ã€‚æä¾›**ç®€å•**å’Œ**å›°éš¾**ï¼ˆå«é¢†åŸŸéšæœºåŒ–ï¼‰ä¸¤ç§è®¾ç½®ã€‚ä»»åŠ¡æ ¹æ®æ¼”ç¤ºé•¿åº¦åˆ†ä¸º**çŸ­ã€ä¸­ã€é•¿**è§†é‡ã€‚è¯„ä¼°**ä»»åŠ¡æˆåŠŸç‡**ã€‚ |\n| **EgoPlan-Bench2** | **è‡ªæˆ‘ä¸­å¿ƒé•¿è§†é‡ä»»åŠ¡è§„åˆ’** | åŒ…å«24ä¸ªæ—¥å¸¸ç”Ÿæ´»åœºæ™¯ï¼Œ1,321é“å¤šé¡¹é€‰æ‹©é¢˜ã€‚è¯„ä¼°æ¨¡å‹åœ¨ç»™å®šç›®æ ‡å’Œå†å²è¿›åº¦ä¸‹é¢„æµ‹ä¸‹ä¸€æ­¥çš„**å‡†ç¡®ç‡**ã€‚ |\n| **RoboVQA** | **æœºå™¨äººæ“ä½œè§†é¢‘çš„è§†è§‰æ¨ç†** | åŒ…å«æ¥è‡ªæœºå™¨äººå’Œäººç±»æ¼”ç¤ºçš„1,893ä¸ªè‡ªç”±å½¢å¼QAå¯¹ã€‚ä½¿ç”¨**BLEUåˆ†æ•°**è¯„ä¼°ç”Ÿæˆç­”æ¡ˆçš„è´¨é‡ã€‚ |\n| **OpenEQA** | **å¼€æ”¾ä¸–ç•Œåœºæ™¯çš„é›¶æ ·æœ¬ç©ºé—´ä¸åŠŸèƒ½ç†è§£** | åŒ…å«è¶…è¿‡1,600ä¸ªé—®é¢˜ï¼Œæ¶µç›–180å¤šä¸ªçœŸå®ä¸–ç•Œç¯å¢ƒã€‚ä½¿ç”¨**LLMåŸºäºè¯„åˆ†**è¯„ä¼°ç­”æ¡ˆçš„åˆç†æ€§å’Œå‡†ç¡®æ€§ã€‚ |\n| **RoboFAC** | **å¤±è´¥è¯†åˆ«ä¸çº æ­£èƒ½åŠ›** | ä¸“é—¨è¯„ä¼°å¯¹æ“ä½œå¤±è´¥çš„åˆ†æå’Œçº æ­£è§„åˆ’èƒ½åŠ›ã€‚åœ¨**ä»¿çœŸ**å’Œ**çœŸå®æœºå™¨äºº**ä¸¤ä¸ªåˆ†é›†ä¸Šï¼Œä½¿ç”¨**LLMè¯„åˆ†**è¿›è¡Œè¯„ä¼°ã€‚ |\n\n### æ€»ç»“\nFast-ThinkActè®ºæ–‡é€šè¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä½¿ç”¨äº†**è¦†ç›–æœºå™¨äººæ“ä½œæˆåŠŸç‡å’Œå¤šç»´åº¦å…·èº«æ¨ç†èƒ½åŠ›çš„ç»¼åˆè¯„ä»·ä½“ç³»**ï¼Œå¹¶åœ¨**å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®é›†**ä¸Šè¿›è¡Œå­¦ä¹ ï¼Œæœ€ç»ˆåœ¨**ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ‡å‡†åŸºå‡†**ä¸ŠéªŒè¯äº†å…¶æ–¹æ³•åœ¨**æ€§èƒ½ã€æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›**ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚å…¶è¯„ä»·æŒ‡æ ‡ä¸ä»…å…³æ³¨â€œåšå¾—å¯¹â€ï¼ˆæˆåŠŸç‡ã€å‡†ç¡®ç‡ï¼‰ï¼Œä¹Ÿå…³æ³¨â€œåšå¾—å¿«â€ï¼ˆæ¨ç†å»¶è¿Ÿï¼‰ï¼Œå¹¶æ·±å…¥æ£€éªŒäº†æ¨¡å‹çš„æ ¸å¿ƒä»·å€¼â€”â€”**é•¿è§†é‡è§„åˆ’ã€å°‘æ ·æœ¬é€‚åº”å’Œå¤±è´¥æ¢å¤**èƒ½åŠ›ã€‚",
            "timestamp": "2026-01-20T18:01:56.003666",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ä¸¤å¤§ç±»æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼š**æœºå™¨äººæ“ä½œæ•°æ®é›†**å’Œ**å…·èº«æ¨ç†æ•°æ®é›†**ã€‚\n\n#### 1. æœºå™¨äººæ“ä½œæ•°æ®é›†ï¼ˆç”¨äºè®­ç»ƒä¸è¯„ä¼°ç­–ç•¥ï¼‰\n- **è®­ç»ƒæ•°æ®**ï¼š\n    - **OXE (Open X-Embodiment)**ï¼šå¤§è§„æ¨¡æœºå™¨äººæ¼”ç¤ºæ•°æ®é›†ï¼Œç”¨äºåŸºç¡€ç­–ç•¥å­¦ä¹ ã€‚\n    - **é™æ€ALOHAæ•°æ®é›†**ï¼šç”¨äºå¢å¼ºåŒæ‰‹æœºå™¨äººæ“ä½œèƒ½åŠ›ã€‚\n    - **AISTæ•°æ®é›†**ï¼šæä¾›çº¦92Kæ¡åŒæ‰‹æœºå™¨äººè§†è§‰è½¨è¿¹ã€‚\n    - **MolmoActæ ‡æ³¨çš„2Dè§†è§‰è½¨è¿¹**ï¼šä»OXEä¸­æå–ï¼Œçº¦130ä¸‡æ¡å•è‡‚æ“ä½œè½¨è¿¹ã€‚\n- **è¯„ä¼°åŸºå‡†**ï¼š\n    - **LIBERO**ï¼šæ¶µç›–ç©ºé—´ã€ç‰©ä½“ã€ç›®æ ‡å’Œé•¿è§†é‡ä»»åŠ¡ï¼Œè¯„ä¼°å¤šæ ·åŒ–æ“ä½œå’Œæ³›åŒ–èƒ½åŠ›ã€‚\n    - **SimplerEnv-Google**ï¼šå…·æœ‰å¼ºä»¿çœŸåˆ°çœŸå®ä¸–ç•Œç›¸å…³æ€§çš„åŸºå‡†ï¼ŒåŒ…å«å…‰ç…§ã€å¤–è§‚å’Œè§†è§’å˜åŒ–ã€‚\n    - **RoboTwin2.0**ï¼šå…·æœ‰æŒ‘æˆ˜æ€§çš„**åŒæ‰‹æœºå™¨äººé•¿è§†é‡æ“ä½œ**åŸºå‡†ï¼ŒåŒ…å«ç®€å•å’Œå›°éš¾ï¼ˆå¸¦åŸŸéšæœºåŒ–ï¼‰è®¾ç½®ã€‚\n\n#### 2. å…·èº«æ¨ç†æ•°æ®é›†ï¼ˆç”¨äºè®­ç»ƒä¸è¯„ä¼°æ¨ç†èƒ½åŠ›ï¼‰\n- **è®­ç»ƒæ•°æ®**ï¼š\n    - **PixMo**ï¼šé€šç”¨è§†è§‰è¯­è¨€æ•°æ®é›†ï¼Œç”¨äºä¿æŒåŸºç¡€è§†è§‰ç†è§£èƒ½åŠ›ã€‚\n    - **RoboFAC**ï¼šæœºå™¨äººå¤±è´¥åˆ†ææ•°æ®é›†ï¼ŒåŒ…å«é”™è¯¯æ“ä½œè½¨è¿¹å’ŒQAå¯¹ã€‚\n    - **RoboVQA**ï¼šæœºå™¨äººæ“ä½œè§†é¢‘çš„é—®ç­”æ•°æ®é›†ï¼Œæ¶µç›–ä»»åŠ¡ç†è§£ã€‚\n    - **ShareRobot**ï¼šå¤§è§„æ¨¡æœºå™¨äººä»»åŠ¡è§„åˆ’ä¸æ“ä½œçŸ¥è¯†æ•°æ®é›†ã€‚\n    - **EgoPlan-Bench**ï¼šä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒè§†è§’çš„æ—¥å¸¸æ´»åŠ¨è§„åˆ’æ•°æ®é›†ã€‚\n    - **Video-R1-CoT**ï¼šå¸¦æœ‰æ€ç»´é“¾æ ‡æ³¨çš„è§†é¢‘é—®ç­”æ•°æ®é›†ã€‚\n- **è¯„ä¼°åŸºå‡†**ï¼š\n    - **EgoPlan-Bench2**ï¼šè¯„ä¼°ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒè§†è§’çš„å¤šæ­¥ä»»åŠ¡è§„åˆ’èƒ½åŠ›ï¼ˆå¤šé€‰é¢˜å‡†ç¡®ç‡ï¼‰ã€‚\n    - **RoboVQA**ï¼šè¯„ä¼°æœºå™¨äººæ“ä½œåœºæ™¯ä¸­çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼ˆBLEUåˆ†æ•°ï¼‰ã€‚\n    - **OpenEQA**ï¼šè¯„ä¼°å¯¹å¤šæ ·åŒ–çœŸå®ä¸–ç•Œç¯å¢ƒçš„é›¶æ ·æœ¬ç©ºé—´ä¸åŠŸèƒ½ç†è§£ï¼ˆLLMè¯„åˆ†ï¼‰ã€‚\n    - **RoboFAC**ï¼šä¸“é—¨è¯„ä¼°**å¤±è´¥è¯†åˆ«ä¸æ¢å¤**èƒ½åŠ›ã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\n1.  **æœºå™¨äººæ“ä½œä»»åŠ¡**ï¼š\n    - **æ ¸å¿ƒæŒ‡æ ‡**ï¼š**ä»»åŠ¡æˆåŠŸç‡**ã€‚\n    - **æ•ˆç‡æŒ‡æ ‡**ï¼š**æ¨ç†å»¶è¿Ÿ**ï¼ˆæ¯«ç§’ï¼‰ï¼Œå¯¹æ¯”åŸºå‡†æ¨¡å‹çš„åŠ é€Ÿå€æ•°ã€‚\n\n2.  **å…·èº«æ¨ç†ä»»åŠ¡**ï¼š\n    - **EgoPlan-Bench2**ï¼š**å‡†ç¡®ç‡**ã€‚\n    - **RoboVQA**ï¼š**BLEUåˆ†æ•°**ï¼ˆB-1, B-2, B-3, B-4åŠå¹³å‡ï¼‰ã€‚\n    - **OpenEQA & RoboFAC**ï¼šä½¿ç”¨**LLMè¿›è¡Œè¯„åˆ†**ï¼ˆä¸äººç±»åå¥½å¯¹é½ï¼‰ã€‚\n\n### ä¸‰ã€ å…³é”®æ€§èƒ½æå‡\nFast-ThinkActåœ¨**ä¿æŒç”šè‡³æå‡æ€§èƒ½**çš„åŒæ—¶ï¼Œå®ç°äº†**æ˜¾è‘—çš„æ•ˆç‡æå‡**ã€‚\n\n#### 1. æ€§èƒ½æå‡ï¼ˆæ•ˆæœï¼‰\n- **æœºå™¨äººæ“ä½œ**ï¼š\n    - åœ¨**LIBERO**åŸºå‡†ä¸Šï¼ŒFast-ThinkAct-3Bè¾¾åˆ°**89.7%** æˆåŠŸç‡ï¼Œä¼˜äºThinkAct-3Bçš„83.1%å’ŒMolmoAct-7Bçš„86.8%ã€‚\n    - åœ¨**SimplerEnv-Google**ä¸Šè¾¾åˆ°**68.7%** æˆåŠŸç‡ï¼Œä¼˜äºThinkAct-7Bçš„68.3%ã€‚\n    - åœ¨**RoboTwin2.0**å›°éš¾è®¾ç½®çš„é•¿è§†é‡ä»»åŠ¡ä¸Šï¼Œå¹³å‡æˆåŠŸç‡æ¯”ThinkActé«˜**1.7%**ï¼Œæ¯”åŸºç¡€RDTé«˜**3.6%**ã€‚\n- **å…·èº«æ¨ç†**ï¼š\n    - åœ¨**EgoPlan-Bench2**ä¸Šï¼ŒFast-ThinkAct-3Bå¹³å‡å‡†ç¡®ç‡**46.4%**ï¼Œä¼˜äºThinkAct-3Bçš„44.0%ã€‚\n    - åœ¨**RoboVQA**ä¸Šï¼Œå¹³å‡BLEUåˆ†æ•°è¾¾åˆ°**60.8**ï¼Œå¤§å¹…ä¼˜äºThinkAct-3Bçš„55.3ã€‚\n    - åœ¨**RoboFAC**çœŸå®ä¸–ç•Œåˆ†å‰²ä¸Šï¼Œè¯„åˆ†æ¯”æœ€ä½³åŸºçº¿ï¼ˆRoboFAC-3Bï¼‰é«˜**16.4åˆ†**ï¼Œå±•ç¤ºäº†å“è¶Šçš„å¤±è´¥æ¢å¤èƒ½åŠ›ã€‚\n\n#### 2. æ•ˆç‡æå‡ï¼ˆé€Ÿåº¦ï¼‰\n- **æ ¸å¿ƒä¼˜åŠ¿**ï¼šå®ç°äº†**æè‡´çš„æ¨ç†åŠ é€Ÿ**ã€‚\n    - ç›¸æ¯”ThinkAct-7Bï¼Œæ¨ç†å»¶è¿Ÿé™ä½ **89.3%**ã€‚\n    - ç›¸æ¯”ThinkAct-3Bï¼Œæ¨ç†é€Ÿåº¦å¿« **7å€**ï¼ˆ805ms vs. 5674msï¼‰ã€‚\n    - ç›¸æ¯”MolmoAct-7Bï¼Œæ¨ç†å»¶è¿Ÿé™ä½ **88.0%**ã€‚\n- **å…³é”®å¯¹æ¯”**ï¼šFast-ThinkActä»…ä½¿ç”¨**6ä¸ªæ½œåœ¨æ¨ç†ä»¤ç‰Œ**å’Œ**5ä¸ªç©ºé—´ä»¤ç‰Œ**è¿›è¡Œå¹¶è¡Œè½¨è¿¹é¢„æµ‹ï¼Œæ›¿ä»£äº†ThinkActéœ€è¦ç”Ÿæˆçš„çº¦**250ä¸ªæ–‡æœ¬ä»¤ç‰Œ**çš„å†—é•¿æ€ç»´é“¾ï¼Œè¿™æ˜¯æ•ˆç‡æå‡çš„æ ¹æœ¬åŸå› ã€‚\n\n#### 3. ç»¼åˆèƒ½åŠ›æå‡\n- **é•¿è§†é‡è§„åˆ’**ï¼šåœ¨RoboTwin2.0çš„é•¿æ­¥éª¤ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚\n- **å°‘æ ·æœ¬é€‚åº”**ï¼šä»…ç”¨10æ¡æ¼”ç¤ºè¿›è¡Œå¾®è°ƒåï¼Œåœ¨RoboTwin2.0ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–VLAæ¨¡å‹ã€‚\n- **å¤±è´¥æ¢å¤**ï¼šåœ¨RoboFACåŸºå‡†ä¸Šå±•ç°å‡ºå¼ºå¤§çš„é”™è¯¯è¯†åˆ«å’Œçº æ­£è§„åˆ’èƒ½åŠ›ã€‚\n\n**æ€»ç»“**ï¼šFast-ThinkActé€šè¿‡å°†æ˜¾å¼æ–‡æœ¬æ¨ç†â€œå‹ç¼©â€ä¸ºå¯è¨€è¯­åŒ–çš„æ½œåœ¨è¡¨ç¤ºï¼Œåœ¨å¤šä¸ªæœºå™¨äººæ“ä½œå’Œå…·èº«æ¨ç†åŸºå‡†ä¸Šå®ç°äº† **SOTAæˆ–æ¥è¿‘SOTAçš„æ€§èƒ½**ï¼ŒåŒæ—¶å°†æ¨ç†é€Ÿåº¦æå‡äº†ä¸€ä¸ªæ•°é‡çº§ï¼ŒæˆåŠŸè§£å†³äº†ç°æœ‰æ¨ç†VLAæ¨¡å‹**å»¶è¿Ÿé«˜**çš„æ ¸å¿ƒç“¶é¢ˆã€‚",
            "timestamp": "2026-01-20T18:02:35.621308",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼ŒFast-ThinkAct çš„ä¸»è¦å±€é™æ€§é›†ä¸­åœ¨**å¯è§£é‡Šæ€§ç»„ä»¶ï¼ˆVerbalizerï¼‰çš„å¯é æ€§**ä¸Šï¼š\n\n- **è¯­è¨€æ¨¡å‹çš„å›ºæœ‰é—®é¢˜**ï¼šè®ºæ–‡æ˜ç¡®æŒ‡å‡ºï¼Œç”¨äºå°†æ½œåœ¨æ¨ç†è§£ç ä¸ºå¯è¯»æ–‡æœ¬çš„ `Verbalizer` æ˜¯åŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„å»ºçš„ã€‚å› æ­¤ï¼Œå®ƒ**ä¸å¯é¿å…åœ°ç»§æ‰¿äº†LLMçš„å¸¸è§ç¼ºé™·**ï¼Œä¾‹å¦‚**å¹»è§‰**â€”â€”å³ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å®é™…ä¸å‡†ç¡®æˆ–ä¸æ½œåœ¨è¡¨ç¤ºä¸ç¬¦çš„æè¿°ã€‚\n- **å¯¹æ ¸å¿ƒåŠŸèƒ½æ— å½±å“**ï¼šå€¼å¾—å¼ºè°ƒçš„æ˜¯ï¼Œè¿™ä¸€å±€é™æ€§**å¹¶ä¸å½±å“æ¨¡å‹åœ¨æ¨ç†æ—¶çš„åŠ¨ä½œæ‰§è¡Œæ€§èƒ½**ã€‚å› ä¸º `Verbalizer` ä»…åœ¨è®­ç»ƒé˜¶æ®µç”¨äºæŒ‡å¯¼æ½œåœ¨è¡¨ç¤ºçš„å­¦ä¹ ï¼Œä»¥åŠåœ¨éœ€è¦æ—¶å¯æä¾›è§£é‡Šæ€§è¾“å‡ºã€‚åŠ¨ä½œé¢„æµ‹å®Œå…¨ä¾èµ–äºç»è¿‡è§†è§‰è§„åˆ’è’¸é¦å¾—åˆ°çš„ã€æ›´å¯é çš„**æ½œåœ¨è¡¨ç¤º**ï¼Œ`Verbalizer` å¹¶éæ¨ç†æµæ°´çº¿çš„å¿…éœ€éƒ¨åˆ†ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nè®ºæ–‡é’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œæå‡ºäº†æ˜ç¡®çš„æœªæ¥å·¥ä½œæ–¹å‘ï¼Œå¹¶éšå«äº†å…¶ä»–æ½œåœ¨ä¼˜åŒ–ç‚¹ï¼š\n\n1. **æå‡ Verbalizer çš„å¿ å®åº¦**ï¼š\n   - **å¼•å…¥ grounding-aware ç›®æ ‡**ï¼šåœ¨è®­ç»ƒ `Verbalizer` æ—¶ï¼Œå¯ä»¥å¢åŠ ä½¿å…¶è¾“å‡ºä¸è§†è§‰è§‚å¯Ÿæˆ–åŠ¨ä½œè½¨è¿¹æ›´ç´§å¯†å¯¹é½çš„çº¦æŸæˆ–æŸå¤±å‡½æ•°ã€‚\n   - **é‡‡ç”¨å¹»è§‰æŠ‘åˆ¶æŠ€æœ¯**ï¼šå€Ÿé‰´å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸçš„ç ”ç©¶ï¼Œåº”ç”¨ä¸“é—¨çš„æŠ€æœ¯æ¥å‡å°‘æˆ–æ£€æµ‹ `Verbalizer` è¾“å‡ºä¸­çš„ä¸å®ä¿¡æ¯ã€‚\n\n2. **éšå«çš„æ‰©å±•ä¸ä¼˜åŒ–æ–¹å‘**ï¼ˆåŸºäºæ–¹æ³•è®¾è®¡ï¼‰ï¼š\n   - **æ¨¡å‹æ¶æ„ä¸æ•ˆç‡çš„è¿›ä¸€æ­¥æ¢ç´¢**ï¼šå½“å‰æ–¹æ³•å°†æ¨ç†å‹ç¼©ä¸º6ä¸ªæ½œåœ¨tokenï¼Œæœªæ¥å¯ä»¥ç ”ç©¶**è‡ªé€‚åº”é•¿åº¦çš„æ½œåœ¨æ¨ç†**ï¼Œæˆ–æ¢ç´¢æ›´é«˜æ•ˆçš„ç¼–ç æ¶æ„ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶è¿›ä¸€æ­¥é™ä½å»¶è¿Ÿã€‚\n   - **è®­ç»ƒç­–ç•¥çš„å¢å¼º**ï¼šè®ºæ–‡ä½¿ç”¨äº†åå¥½è’¸é¦ï¼ˆDPOé£æ ¼ï¼‰å’Œè½¨è¿¹å¯¹é½ã€‚æœªæ¥å¯ä»¥æ¢ç´¢**æ›´å¤æ‚çš„å¥–åŠ±è®¾è®¡**æˆ–**å¤šä»»åŠ¡è”åˆä¼˜åŒ–**ï¼Œä»¥æ›´å¥½åœ°å¹³è¡¡æ¨ç†è´¨é‡ã€å‹ç¼©ç‡å’ŒåŠ¨ä½œæ€§èƒ½ã€‚\n   - **æ›´å¹¿æ³›çš„ä»»åŠ¡ä¸åœºæ™¯éªŒè¯**ï¼šå½“å‰å·¥ä½œåœ¨å¤šä¸ªä»¿çœŸåŸºå‡†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚æœªæ¥çš„é‡è¦æ–¹å‘æ˜¯åœ¨**çœŸå®æœºå™¨äººå¹³å°**å’Œ**æ›´å¤æ‚ã€åŠ¨æ€çš„çœŸå®ä¸–ç•Œåœºæ™¯**ä¸­æµ‹è¯•å…¶æ•ˆç‡ä¸é²æ£’æ€§ï¼Œè¿™å¯¹å®é™…éƒ¨ç½²è‡³å…³é‡è¦ã€‚\n   - **æ¶ˆé™¤å¯¹æ•™å¸ˆæ¨¡å‹çš„ä¾èµ–**ï¼šå½“å‰æ–¹æ³•ä¾èµ–äºä¸€ä¸ªç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ•™å¸ˆæ¨¡å‹æ¥æä¾›é«˜è´¨é‡çš„æ¨ç†è½¨è¿¹ã€‚æœªæ¥å¯ä»¥ç ”ç©¶**ç«¯åˆ°ç«¯çš„è®­ç»ƒæ¡†æ¶**æˆ–**è‡ªç›‘ç£æ–¹æ³•**ï¼Œä»¥å‡å°‘å¯¹æ˜‚è´µæ•™å¸ˆæ¨¡å‹è®­ç»ƒçš„ä¾èµ–ã€‚\n\n**æ€»ç»“**ï¼šFast-ThinkAct çš„æ ¸å¿ƒåˆ›æ–°â€”â€”å°†å†—é•¿æ¨ç†å‹ç¼©ä¸ºé«˜æ•ˆçš„æ½œåœ¨è¡¨ç¤ºâ€”â€”æœ¬èº«æ˜¯ç¨³å¥çš„ã€‚å…¶ä¸»è¦å±€é™åœ¨äºå¤–å›´çš„è§£é‡Šæ€§ç»„ä»¶ï¼Œè€Œè¿™å¯ä»¥é€šè¿‡ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹å¯¹é½æŠ€æœ¯è¿›è¡Œæ”¹è¿›ã€‚æœªæ¥çš„å·¥ä½œå°†å›´ç»•æå‡æ•´ä¸ªç³»ç»Ÿçš„å¿ å®åº¦ã€æ•ˆç‡åŠåœ¨æ›´å¤æ‚ç°å®åœºæ™¯ä¸­çš„é€‚ç”¨æ€§å±•å¼€ã€‚",
            "timestamp": "2026-01-20T18:02:57.293410",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-14T18:59:59Z",
    "created_at": "2026-01-20T17:49:51.130239",
    "updated_at": "2026-01-20T17:49:51.130245",
    "recommend": 0
}