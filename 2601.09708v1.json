{
  "id": "2601.09708v1",
  "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
  "authors": [
    "Chi-Pin Huang",
    "Yunze Man",
    "Zhiding Yu",
    "Min-Hung Chen",
    "Jan Kautz",
    "Yu-Chiang Frank Wang",
    "Fu-En Yang"
  ],
  "abstract": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.",
  "url": "https://arxiv.org/abs/2601.09708v1",
  "html_url": "https://arxiv.org/html/2601.09708v1",
  "html_content": "\\correspondingauthor\nX\nFast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning\nChi-Pin Huang\n1\nYunze Man\n2\nZhiding Yu\nMin-Hung Chen\nJan Kautz\nYu-Chiang Frank Wang\n1\nFu-En Yang\nNVIDIA\nAbstract\nVision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.\nLinks:\nProject Page\n\\abscontent\n1\nIntroduction\nFigure 1\n:\nOverview of Fast-ThinkAct.\nPrevious reasoning VLAs generate lengthy reasoning traces (\n‚àº\n\\sim\n250 tokens). Our approach learns compact continuous tokens (e.g., 6) (\nblue\n) and parallel spatial tokens (\ngreen\n) as internal reasoning. The bottom-right plot shows that we achieve\n9.3\n√ó\n9.3\\times\nfaster inference than ThinkAct-7B\nhuang2025thinkact\n, while delivering improved performance on the SimplerEnv-Google benchmark.\nRecent large vision-language models (VLMs)\nliu2023visual\n;\ncomanici2025gemini\n;\nliu2024nvila\n;\nbai2025qwen2\n;\nshi2024eagle\n;\nli2025eagle\n;\nchen2025eagle\n;\nwang2025internvl3\n;\nxie2024show\nhave achieved remarkable capabilities in visual-language understanding across diverse multimodal tasks. To extend these capabilities to embodied-centric tasks, recent works leverage large-scale robot demonstrations\no2024open\nto develop Vision-Language-Action (VLA) foundation models\nbrohan2022rt\n;\nbrohan2023rt\n;\nteam2024octo\n;\nbjorck2025gr00t\n;\nli2025hamster\n;\nblack2024pi_0\n;\nyang2025magma\n;\nkim24openvla\n. These VLA tasks require agents to perceive complex visual scenes, reason over spatial and temporal contexts, and execute adaptive actions within dynamic environments, demanding robust long-horizon planning and contextual adaptation. However, as these VLA models primarily rely on supervised training from action data, they excel at basic skills (e.g., pick-and-place) but struggle to generalize beyond training distributions, such as long-horizon planning, self-correction from failures, and adaptation to novel scenarios, due to the impracticality of collecting exhaustive robot demonstrations.\nReasoning VLAs\nzawalski2024robotic\n;\nzhao2025cot\n;\nlee2025molmoact\n;\nwu2025you\n;\nqu2025eo\n;\nhuang2025thinkact\n;\nkim2025robot\naddress these limitations by incorporating intermediate thinking processes, improving generalization and task-solving capability. Supervised chain-of-thought (CoT) methods\nzawalski2024robotic\n;\nzhao2025cot\n;\nlee2025molmoact\n;\nqu2025eo\naddress this by learning from intermediate reasoning annotations. These approaches can be categorized into textual reasoning methods that leverage off-the-shelf LLMs and VLMs to generate pseudo CoT labels\nzawalski2024robotic\n, and visual reasoning methods that generate structured visual reasoning representations such as sub-goal images, image depth, and 2D visual traces\nzhao2025cot\n;\nlee2025molmoact\n. However, these supervised approaches require substantial reasoning annotations and remain limited by training data coverage. To address this, ThinkAct\nhuang2025thinkact\nemploys RL-based reasoning\nshao2024deepseekmath\nto generate long textual CoTs guided by action-aligned visual rewards. While these reasoning methods effectively improve task generalization and planning capabilities, they require generating lengthy chain-of-thought steps that introduce substantial reasoning latency, which hampers embodied applications with\nreal-time\nrequirements.\nIn embodied AI applications such as robotic manipulation and autonomous driving, agents must make rapid decisions at high frequencies (e.g., 1-15 Hz)\nguan2025efficient\n. However, generating lengthy reasoning traces can take several seconds per decision (e.g., 0.1 Hz)\nhuang2025thinkact\n;\nlee2025molmoact\n, creating a critical bottleneck that limits real-time performance\nguan2025efficient\n;\nyu2025survey\nand poses safety risks in time-critical scenarios\nwang2025alpamayo\n. To mitigate this efficiency bottleneck while preserving reasoning capabilities, very recent works\nchen2025training\n;\nyu2025survey\n;\nguan2025efficient\nhave explored approaches to reduce inference latency in embodied reasoning. For instance, ECoT-Lite\nchen2025training\nproposes reasoning dropout to accelerate inference, yet directly reducing textual reasoning length risks performance degradation due to critical information loss. How to preserve reasoning capability while enabling compact representations that properly capture essential spatial-temporal dynamics remains a crucial challenge for reasoning VLA models.\nIn this paper, we propose\nFast-ThinkAct\n, an efficient embodied reasoning framework for Vision-Language-Action tasks that achieves compact yet expressive planning through verbalizable latent reasoning. As depicted in Figure\n1\n, unlike prior reasoning VLAs that generate lengthy explicit textual CoT traces, we introduce reward-guided preference distillation with visual trajectory alignment to compress linguistic and visual planning into compact continuous latents that enable implicit internal reasoning. Our student VLM encodes reasoning into compact latents decodable by a verbalizer, enabling preference-based optimization that leverages RL-derived reward signals to distill high-quality reasoning patterns from a textual teacher VLM while suppressing low-quality ones. We further align trajectory latents between teacher and student to transfer visual planning capabilities essential for embodied control. Once trained, the student VLM enables reasoning-enhanced policy learning that bridges implicit multimodal planning with action execution, achieving significantly faster inference while outperforming existing reasoning VLAs.\nOur contributions can be summarized as follows:\n‚Ä¢\nWe propose\nFast-ThinkAct\n, an efficient reasoning framework that compresses reasoning into verbalizable latent thoughts while maintaining expressive planning abilities.\n‚Ä¢\nWe introduce preference-guided distillation with manipulation trajectory alignment that compresses linguistic and visual planning into compact continuous latents.\n‚Ä¢\nWe bridge high-level visual planning to low-level action execution through reasoning-enhanced policy learning guided by manipulation trajectory latents.\n‚Ä¢\nWe achieve up to 89.3% inference latency reduction over state-of-the-art reasoning VLAs while maintaining strong performance across diverse embodied benchmarks.\n2\nRelated Works\n2.1\nVision-Language-Action (VLA) Models\nFoundation VLAs.\nVision-Language-Action (VLA) models\nbrohan2022rt\n;\nbrohan2023rt\n;\nteam2024octo\n;\nbjorck2025gr00t\n;\nli2025hamster\n;\nblack2024pi_0\n;\nyang2025magma\n;\npertsch2025fast\n;\ndriess2025knowledge\n;\nbu2025agibot\n;\nteam2025gemini\n;\nwang2025vla\nhave recently emerged as a promising paradigm for embodied AI by training vision-language backbones on large-scale robot demonstrations. Works such as OpenVLA\nkim24openvla\nand\nœÄ\n0\n\\pi_{0}\nblack2024pi_0\nachieve language-conditioned manipulation through end-to-end policy learning, while Magma\nyang2025magma\nco-trains on heterogeneous human and robot data. HAMSTER\nli2025hamster\nand TraceVLA\nzheng2024tracevla\nfurther leverage 2D visual trajectories to boost spatial-action connections. Despite success on routine manipulation, these imitation-based approaches struggle with long-horizon planning and generalization to novel scenarios due to limited training data coverage.\nReasoning VLAs.\nTo overcome these limitations, recent works\nzawalski2024robotic\n;\nzhao2025cot\n;\nlee2025molmoact\n;\nwu2025you\n;\nqu2025eo\n;\nhuang2025thinkact\n;\nkim2025robot\n;\nyuan2025embodied\n;\nabdolmaleki2025gemini\nintegrate explicit reasoning mechanisms into VLA architectures. Supervised approaches\nzawalski2024robotic\n;\nzhao2025cot\n;\nlee2025molmoact\n;\nqu2025eo\nintroduce intermediate reasoning through chain-of-thought annotations. Embodied CoT\nzawalski2024robotic\nand Hi-Robot\nshi2025hi\nsynthesize reasoning labels via pretrained foundation models. To perform vision-centric reasoning\nman2025argus\n;\nsarch2025grounded\nbeyond pure text, CoT-VLA\nzhao2025cot\nemploys visual goal generation and MolmoAct\nlee2025molmoact\nstructures reasoning by spatial representations. Additionally, EO-1\nqu2025eo\nintroduces interleaved vision-language-action pre-training to bridge reasoning and interaction. Recent works\nyuan2025embodied\n;\nhuang2025thinkact\nalternatively leverage reinforcement fine-tuning to generate reasoning chains with designed rewards. Despite improved generalization, these reasoning VLAs suffer from high inference latency and inevitably introduce extraneous information that degrades action quality.\n2.2\nEfficient Reasoning\nTo address the inference latency of reasoning, recent LLM research explores various efficiency techniques\nlee2025vlsi\n;\ndai2025stable\n;\nyuan2025efficient\n;\nxiang2025just\n;\naggarwal2025l1\n;\nlee2025unified\n. For example, RL-based approaches\ndai2025stable\n;\nyuan2025efficient\n;\nxiang2025just\n;\naggarwal2025l1\nintroduce length penalties to encourage shorter reasoning chains, though such methods can suffer from training instability. Beyond length control, latent reasoning methods\nhao2024training\n;\nshen2025codi\n;\nzhang2025soft\n;\ncheng2024compressed\n;\nxu2025softcot\nenable reasoning in continuous spaces, such as Coconut\nhao2024training\nusing hidden states as continuous thoughts, CODI\nshen2025codi\ndistilling explicit CoT into continuous space via teacher-student alignment, and Soft Thinking\nzhang2025soft\ngenerating weighted concept tokens. However, these LLM techniques cannot directly transfer to VLA tasks due to the need for spatial-temporal understanding and bridging semantic reasoning with embodied control.\nRecently, ECoT-Lite\nchen2025training\nproposes reasoning dropout to accelerate embodied reasoning by skipping test-time reasoning traces. However, reasoning dropout can lead to inconsistent planning as it builds on supervised embodied CoT. Our proposed Fast-ThinkAct distills reasoning into compact latent representations that naturally encode multimodal information, enabling robust reasoning-enhanced policy learning.\n3\nMethod\nFigure 2\n:\nOverview of Fast-ThinkAct.\n(a) Given observation\no\nt\no_{t}\nand instruction\nl\nl\n, the Textual Teacher VLM\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\ngenerates explicit reasoning chains. The Latent Student VLM\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\ndistills these into compact latent tokens\nùê≥\n\\mathbf{z}\nguided by reward preferences. Verbalizer LLM\nùí±\nœà\n\\mathcal{V}_{\\psi}\ndecodes latents to text for preference-based learning via\n‚Ñí\nverb\n\\mathcal{L}_{\\text{verb}}\n, while\n‚Ñí\ndistill\n\\mathcal{L}_{\\text{distill}}\ntransfers visual planning capability from teacher, and spatial tokens enable parallel visual trajectory prediction via\n‚Ñí\nans\n\\mathcal{L}_{\\text{ans}}\n, ensuring latents are verbalizable and grounded in visual planning. (b) Reasoning-Enhanced Policy Learning. The Action Model\nœÄ\nœï\n\\pi_{\\phi}\nis trained with\n‚Ñí\nIL\n\\mathcal{L}_{\\text{IL}}\nwhile freezing the latent student\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nand state encoder.\n3.1\nProblem Formulation\nWe first define the setting and notations. At each timestep\nt\nt\n, given a language instruction\nl\nl\n, the model observes a visual input\no\nt\no_{t}\nand generates an action chunk\na\nt\na_{t}\n, represented as a sequence of continuous robot control vectors (e.g., 7- or 14-DOF for single- or bimanual robots, respectively).\nTo address this problem, we propose Fast-ThinkAct, an efficient reasoning framework that bridges high-level planning with low-level action execution. Our approach employs a VLM\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nto perform reasoning in\ncontinuous latent space\n, integrated with an action model\nœÄ\nœï\n\\pi_{\\phi}\nfor executable action generation. Specifically,\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nprocesses observation-instruction pairs\n(\no\nt\n,\nl\n)\n(o_{t},l)\nthrough latent chain-of-thought (CoT) reasoning to produce a compact visual plan latent\nc\nt\nc_{t}\nthat encapsulates the intended trajectory in visual space (Sec.\n3.2\n). This\nc\nt\nc_{t}\nsubsequently guides\nœÄ\nœï\n\\pi_{\\phi}\nto predict executable actions\na\nt\na_{t}\n(Sec.\n3.3\n). By distilling reasoning into a continuous latent space rather than discrete text, Fast-ThinkAct achieves significantly improved inference efficiency while enhancing action performance through better preservation of spatial and visual information.\n3.2\nEfficient Embodied Reasoning\nTo enable efficient embodied reasoning that meets the real-time requirements of embodied AI tasks, we aim to compress long textual CoTs into a compact set of continuous latent representations. However, compressing reasoning traces into latents is challenging, as there is no direct supervision signal in the latent space to guide what reasoning patterns should be encoded.\n3.2.1\nVerbalizable Latent CoT by Reward Preferences\nTo address this challenge, we propose to perform distillation in natural language space by introducing a verbalizer LLM that decodes latents into verbalizable reasoning. This approach grounds latent learning in an interpretable textual form, ensuring that the learned latents faithfully preserve the underlying reasoning structure. Since reasoning traces generated by the teacher model\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\nexhibit varying quality, we adopt a preference-based learning framework that exploits reward signals from the teacher‚Äôs GRPO training to guide the latent student\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\ntoward high-quality reasoning patterns while suppressing low-quality ones.\nSpecifically, we employ a teacher-student framework where a textual teacher model\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\nfirst learns explicit reasoning through GRPO\nshao2024deepseekmath\ntraining by maximizing:\nùí•\nGRPO\n‚Äã\n(\nŒ∏\n)\n=\nùîº\nœÑ\n‚àº\n‚Ñ±\nŒ∏\nT\n‚Äã\n[\nmin\n‚Å°\n(\nr\nŒ∏\n‚Äã\n(\nœÑ\n)\n‚Äã\nA\n‚Äã\n(\nœÑ\n)\n,\nclip\n‚Äã\n(\nr\nŒ∏\n‚Äã\n(\nœÑ\n)\n,\n1\n‚àí\nœµ\n,\n1\n+\nœµ\n)\n‚Äã\nA\n‚Äã\n(\nœÑ\n)\n)\n]\n,\n\\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}_{\\tau\\sim\\mathcal{F}^{T}_{\\theta}}\\Big[\\min\\big(r_{\\theta}(\\tau)A(\\tau),\\text{clip}(r_{\\theta}(\\tau),1-\\epsilon,1+\\epsilon)A(\\tau)\\big)\\Big],\n(1)\nwhere\nœÑ\n\\tau\ndenotes a reasoning trace and\nr\nŒ∏\n‚Äã\n(\nœÑ\n)\n=\n‚Ñ±\nŒ∏\nT\n‚Äã\n(\nœÑ\n)\n‚Ñ±\nold\nT\n‚Äã\n(\nœÑ\n)\nr_{\\theta}(\\tau)=\\frac{\\mathcal{F}^{T}_{\\theta}(\\tau)}{\\mathcal{F}^{T}_{\\text{old}}(\\tau)}\nis the probability ratio. The advantage function for group rewards\n{\nR\ni\n}\ni\n‚àà\nG\n‚Äã\n(\nœÑ\n)\n\\{R_{i}\\}_{i\\in G(\\tau)}\nis represented as:\nA\n‚Äã\n(\nœÑ\n)\n=\nR\nœÑ\n‚àí\nmean\n‚Äã\n(\n{\nR\ni\n}\ni\n‚àà\nG\n‚Äã\n(\nœÑ\n)\n)\nstd\n‚Äã\n(\n{\nR\ni\n}\ni\n‚àà\nG\n‚Äã\n(\nœÑ\n)\n)\n.\nA(\\tau)=\\frac{R_{\\tau}-\\text{mean}(\\{R_{i}\\}_{i\\in G(\\tau)})}{\\text{std}(\\{R_{i}\\}_{i\\in G(\\tau)})}.\n(2)\nThis training process produces textual CoTs with varying quality, where the advantage function\nA\n‚Äã\n(\nœÑ\n)\nA(\\tau)\nnaturally serves as a quality indicator. To construct preference pairs for distillation, we select the highest and lowest advantage traces from each rollout group:\nœÑ\n+\n=\narg\n‚Å°\nmax\nœÑ\n‚àà\nG\n‚Å°\nA\n‚Äã\n(\nœÑ\n)\n‚Äã\nand\n‚Äã\nœÑ\n‚àí\n=\narg\n‚Å°\nmin\nœÑ\n‚àà\nG\n‚Å°\nA\n‚Äã\n(\nœÑ\n)\n.\n\\displaystyle\\tau^{+}=\\arg\\max_{\\tau\\in G}A(\\tau)\\text{ and }\\tau^{-}=\\arg\\min_{\\tau\\in G}A(\\tau).\n(3)\nInstead of generating textual tokens, the student model\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nperforms latent reasoning by autoregressively generating\nM\nM\ncontinuous latent vectors\nùê≥\n=\n{\nz\nm\n}\nm\n=\n1\nM\n\\mathbf{z}=\\{z_{m}\\}_{m=1}^{M}\nwith\nz\nm\n‚àà\n‚Ñù\nd\nz_{m}\\in\\mathbb{R}^{d}\n, where\nd\nd\nis the hidden size. We then train the verbalizer LLM\nùí±\nœà\n\\mathcal{V}_{\\psi}\nto decode these latents\nùê≥\n\\mathbf{z}\ninto natural language. The training objective encourages the verbalizer to assign a higher likelihood to decoding latents into high-quality reasoning\nœÑ\n+\n\\tau^{+}\nthan low-quality reasoning\nœÑ\n‚àí\n\\tau^{-}\n. Inspired by DPO\nrafailov2023direct\n, we formulate this as an optimization guided by the reward preferences:\n‚Ñí\nverb\n=\n‚àí\nùîº\n‚Äã\n[\nlog\n‚Å°\nœÉ\n‚Äã\n(\nŒ≤\n‚Äã\n(\nlog\n‚Å°\np\nœà\n‚Äã\n(\nœÑ\n+\n‚à£\nùê≥\n)\np\nref\n‚Äã\n(\nœÑ\n+\n)\n‚àí\nlog\n‚Å°\np\nœà\n‚Äã\n(\nœÑ\n‚àí\n‚à£\nùê≥\n)\np\nref\n‚Äã\n(\nœÑ\n‚àí\n)\n)\n)\n]\n,\n\\mathcal{L}_{\\text{verb}}=-\\mathbb{E}\\Big[\\log\\sigma\\Big(\\beta\\big(\\log\\tfrac{p_{\\psi}(\\tau^{+}\\mid\\mathbf{z})}{p_{\\text{ref}}(\\tau^{+})}-\\log\\tfrac{p_{\\psi}(\\tau^{-}\\mid\\mathbf{z})}{p_{\\text{ref}}(\\tau^{-})}\\big)\\Big)\\Big],\n(4)\nwhere\np\nref\np_{\\text{ref}}\nis the reference model (i.e.,\nùí±\nœà\n\\mathcal{V}_{\\psi}\nwithout latent conditioning),\nœÉ\n\\sigma\nis the sigmoid function, and\nŒ≤\n=\n0.1\n\\beta=0.1\ncontrols preference strength. This encourages the student VLM\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nto encode latents that the verbalizer decodes into high-quality reasoning while suppressing low-quality patterns.\n3.2.2\nAction-Aligned Visual Plan Distillation\nWhile the verbalizer loss (Eq.\n4\n) enables the student\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nto capture high-level reasoning patterns, it does not explicitly ensure that latent representations encode the visual planning capability crucial for embodied control. To address this, we introduce action-aligned visual plan distillation to transfer the teacher\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\n‚Äôs spatial reasoning ability to the student\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\n.\nWe distill spatial reasoning from the teacher, which is trained with trajectory-level rewards (e.g., goal completion and trajectory alignment\nhuang2025thinkact\n) for grounded visual planning. We align the trajectory-level representations by minimizing the L2 distance between hidden states of the\n<answer>\ntoken that encodes the visual plan:\n‚Ñí\ndistill\n=\n‚Äñ\nh\nt\nT\n‚àí\nh\nt\n‚Äñ\n2\n2\n,\n\\mathcal{L}_{\\text{distill}}=\\|h_{t}^{T}-h_{t}\\|_{2}^{2},\n(5)\nwhere\nh\nt\nT\nh_{t}^{T}\nand\nh\nt\nh_{t}\nare the hidden states from teacher (corresponding to\nœÑ\n+\n\\tau^{+}\n) and student, respectively.\nTo enable efficient parallel trajectory prediction, unlike the textual teacher that autoregressively generates verbose text sequences of waypoints\n{\np\nk\n}\nk\n=\n1\nK\n\\{p_{k}\\}_{k=1}^{K}\nwith\np\nk\n‚àà\n[\n0\n,\n1\n]\n2\np_{k}\\in[0,1]^{2}\n(tokenized into 60-70 tokens when\nK\n=\n5\nK=5\n), the student uses\nK\nK\nlearnable spatial tokens\n{\nùê¨\ni\n}\ni\n=\n1\nK\n\\{\\mathbf{s}_{i}\\}_{i=1}^{K}\nappended to the reasoning latent sequence, with each output hidden state simultaneously projected to a waypoint via an MLP. The total objective for training\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\ncombines all three components:\n‚Ñí\nstudent\n\\displaystyle\\mathcal{L}_{\\text{student}}\n=\n‚Ñí\nverb\n+\n‚Ñí\ndistill\n+\n‚Ñí\nans\n,\nwhere\n\\displaystyle=\\mathcal{L}_{\\text{verb}}+\\mathcal{L}_{\\text{distill}}+\\mathcal{L}_{\\text{ans}},\\quad\\text{where }\n(6)\n‚Ñí\nans\n\\displaystyle\\mathcal{L}_{\\text{ans}}\n=\n‚àë\ni\n=\n1\nK\n‚Äñ\np\ni\n‚àí\np\n^\ni\n‚Äñ\n2\n2\n,\nwith\n‚Äã\np\ni\n=\nMLP\n‚Äã\n(\nh\n‚Ä≤\n‚Äã\n(\nùê¨\ni\n)\n)\n,\n\\displaystyle=\\sum_{i=1}^{K}\\|p_{i}-\\hat{p}_{i}\\|_{2}^{2},\\text{ with }p_{i}=\\text{MLP}(h^{\\prime}(\\mathbf{s}_{i})),\nwhere\nh\n‚Ä≤\n‚Äã\n(\nùê¨\ni\n)\nh^{\\prime}(\\mathbf{s}_{i})\ndenotes the output hidden state of the\ni\ni\n-th spatial token and\np\n^\ni\n\\hat{p}_{i}\nare ground-truth waypoints. Through this unified framework, the student model\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nperforms compact yet expressive latent reasoning and generates visual trajectory plans efficiently.\n3.3\nReasoning-Enhanced Policy Learning\nAfter the student VLM\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nperforms compact latent reasoning and generates visual trajectory planning through spatial tokens, we leverage these representations to guide a diffusion Transformer-based action model\nœÄ\nœï\n\\pi_{\\phi}\n(e.g., RDT\nliu2024rdt\n) for action prediction. To bridge the high-level visual planning with low-level action generation, we connect the visual latent planning\nc\nt\nc_{t}\nencoded in the key-value cache corresponding to the spatial tokens to the action model.\nSpecifically, we extract visual latent planning\nc\nt\nc_{t}\nfrom the KV cache of spatial tokens in earlier VLM layers (since\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nhas more layers than\nœÄ\nœï\n\\pi_{\\phi}\n) and concatenate with KV pairs from the action model‚Äôs state encoder. The action model‚Äôs cross-attention then attends to both the visual planning context and state observations. We post-train on action-annotated robot data by\nfreezing\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nand the state encoder while updating only\nœÄ\nœï\n\\pi_{\\phi}\nwith the imitation learning objective:\n‚Ñí\nIL\n‚Äã\n(\nœï\n)\n=\n‚Ñì\n‚Äã\n(\nœÄ\nœï\n‚Äã\n(\no\nt\n,\nl\n,\nc\nt\n)\n,\na\n^\nt\n)\n,\n\\mathcal{L}_{\\text{IL}}(\\phi)=\\ell\\left(\\pi_{\\phi}(o_{t},l,c_{t}),\\hat{a}_{t}\\right),\n(7)\nwhere\n‚Ñì\n\\ell\ndenotes the denoising objective for diffusion policy and\na\n^\nt\n\\hat{a}_{t}\nis the ground-truth action. Through this post-training, the action model effectively translates visual planning from compact latent reasoning into low-level robot actions.\nFigure 3\n:\nEvaluation of robot manipulation and reasoning efficiency.\n(a)-(e) Success rates on LIBERO\nliu2023libero\nand SimplerEnv\nli24simpler\nbenchmarks compared with state-of-the-art 7B reasoning VLAs. (f) Latency comparison across 3B and 7B reasoning VLAs. Our approach achieves up to 89.3% inference latency reduction while maintaining superior task success rates.\n3.4\nLearning Strategy and Inference\nTraining Strategy.\nWe initialize both teacher\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\nand student\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nfrom the same checkpoint obtained through SFT and CoT-SFT on a pre-trained VLM. The teacher is trained with GRPO using action-aligned rewards\nhuang2025thinkact\n, while the student is trained with\n‚Ñí\nstudent\n\\mathcal{L}_{\\text{student}}\nto compress reasoning into compact latents. We then connect the trained\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nwith action model\nœÄ\nœï\n\\pi_{\\phi}\n(initialized from\nliu2024rdt\n) by freezing\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nand the state encoder while updating the latent projector and\nœÄ\nœï\n\\pi_{\\phi}\nwith\n‚Ñí\nIL\n\\mathcal{L}_{\\text{IL}}\non large-scale robotic data. For target environment adaptation (e.g., LIBERO\nliu2023libero\n, RoboTwin2.0\nchen2025robotwin2\n), we fine-tune on environment-specific demonstrations.\nInference.\nThe\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nprocesses\n(\no\nt\n,\nl\n)\n(o_{t},l)\nby compact latent reasoning, generating visual trajectories via\nK\nK\nspatial tokens. The visual latent planning\nc\nt\nc_{t}\n, extracted from the spatial tokens‚Äô KV cache, conditions\nœÄ\nœï\n\\pi_{\\phi}\nto predict actions\na\nt\na_{t}\n. Inference requires only\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nand\nœÄ\nœï\n\\pi_{\\phi}\n; the verbalizer\nùí±\nœà\n\\mathcal{V}_{\\psi}\nis used solely during training and optionally for interpretability.\n4\nExperiment\n4.1\nExperimental Setup\nImplementation Details.\nWe use Qwen2.5-VL 3B\nbai2025qwen2\nas the VLM backbone. The SFT stage runs for 1 epoch with batch size 64 and learning rate\n1\n‚Äã\ne\n‚àí\n5\n1\\text{e}{-5}\n, followed by CoT-SFT for 15K iterations with the same hyperparameters. For teacher-student training, both\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\nand\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nare initialized from the CoT-SFT checkpoint and trained for 4,500 iterations with batch size 128 and learning rate\n1\n‚Äã\ne\n‚àí\n6\n1\\text{e}{-6}\n. The teacher is optimized with GRPO\nshao2024deepseekmath\nusing action-aligned visual rewards\nhuang2025thinkact\nand QA-style rewards (detailed in supplementary material). For the first 3,000 iterations of the student training, we train the verbalizer\nùí±\nœà\n\\mathcal{V}_{\\psi}\nwith standard language modeling loss, then switch to\n‚Ñí\nverb\n\\mathcal{L}_{\\text{verb}}\nfor the remaining 1,500 iterations. For reasoning-enhanced policy learning, we initialize\nœÄ\nœï\n\\pi_{\\phi}\nfrom DiT-Policy\nchi2023diffusion\npre-trained on OXE\no2024open\nfor SimplerEnv, and from RDT\nliu2024rdt\nfor LIBERO and RoboTwin2.0. A linear projection adapts the VLM‚Äôs KV cache to the action model dimension (1,024 for DiT-Policy and 2,048 for RDT). Training runs for 20K iterations with batch size 256 and learning rate\n1\n‚Äã\ne\n‚àí\n4\n1\\text{e}{-4}\n. All diffusion hyperparameters follow those of the respective action models. All experiments are conducted on 16 NVIDIA A100 GPUs with 80 GB memory.\nTraining Datasets and Evaluation Benchmarks.\nFor reasoning VLM training, we utilize single-arm visual trajectories labeled by\nlee2025molmoact\nand dual-arm visual trajectories from the AIST dataset\naist2025aist\n, along with QA tasks from PixMo\ndeitke2024molmo\n, RoboFAC\nlu2025robofac\n, RoboVQA\nsermanet2024robovqa\n, ShareRobot\nji2025robobrain\n, EgoPlan\nchen2023egoplan\n, and Video-R1\nfeng2025video\n. For reasoning-enhanced policy learning, we use action data from the OXE dataset\no2024open\n(following OpenVLA\nkim24openvla\n) when training with DiT-Policy, and augment with bimanual data from the static Aloha dataset\nshi2023waypoint\n;\nzhao2023learning\nwhen training with RDT.\nWe evaluate Fast-ThinkAct on four embodied reasoning benchmarks and three robot manipulation benchmarks. For embodied reasoning, we use EgoPlan-Bench2\nqiu2024egoplan2\n(accuracy on multiple-choice questions), RoboVQA\nsermanet2024robovqa\n(BLEU score\npapineni2002bleu\n), OpenEQA\nmajumdar2024openeqa\n, and RoboFAC\nlu2025robofac\n(both using LLM-based scoring). Notably, RoboVQA and RoboFAC contain videos captured from real robots. For robot manipulation, we evaluate on SimplerEnv\nli24simpler\n, which demonstrates strong correlation with real-world performance, LIBERO\nliu2023libero\ncovering diverse manipulation tasks including long-horizon scenarios, and RoboTwin2.0\nchen2025robotwin2\nfor complex bimanual manipulation. All robot manipulation tasks use task success rate as the metric. Additional details are provided in the supplementary material.\n4.2\nQuantitative Evaluation\nRobot Manipulation.\nTable 1\n:\nQuantitative evaluation on RoboTwin2.0\nchen2025robotwin2\n.\nE and H denote easy and hard settings (without/with domain randomization). Background colors indicate task length based on expert demonstrations:\nshort (80-100)\n,\nmedium (110-220)\n,\nlong (270-470)\nsteps.\nModel\nclick\nalarm\nclick\nbell\nturn\nswitch\nadjust\nbottle\nbeat\nblock\nhandover\nmic\nhandover\nblock\nhanging\nmug\nstack\nblocks two\nstack\nbowls three\nAverage\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nE\nH\nDP\nchi2023diffusion\n61\n5\n54\n0\n36\n1\n97\n0\n42\n0\n53\n0\n10\n0\n8\n0\n7\n0\n63\n0\n43.1\n0.6\nACT\nzhao2023learning\n32\n4\n58\n3\n5\n2\n97\n23\n56\n3\n85\n0\n42\n0\n7\n0\n25\n0\n48\n0\n45.5\n3.5\nœÄ\n0\n\\pi_{0}\nblack2024pi_0\n63\n11\n44\n3\n27\n23\n90\n56\n43\n21\n98\n13\n45\n8\n11\n3\n42\n1\n66\n24\n52.9\n16.3\nRDT\nliu2024rdt\n61\n12\n80\n9\n35\n15\n81\n75\n77\n37\n90\n31\n45\n14\n23\n16\n21\n2\n51\n17\n56.4\n22.8\nThinkAct\nhuang2025thinkact\n64\n13\n84\n11\n40\n19\n94\n70\n79\n33\n92\n40\n56\n15\n31\n18\n30\n5\n54\n23\n62.4\n24.7\nFast-ThinkAct\n70\n17\n82\n12\n37\n21\n92\n72\n82\n33\n99\n42\n65\n15\n30\n22\n45\n5\n55\n25\n65.7\n26.4\nTable 2\n:\nQuantitative evaluation on EgoPlan-Bench2\nqiu2024egoplan2\n, RoboVQA\nsermanet2024robovqa\n, and OpenEQA\nmajumdar2024openeqa\nbenchmarks for embodied reasoning.\nMethod\nEgoPlan-Bench2\nRoboVQA\nOpenEQA\nOverall\nDaily.\nWork.\nRec.\nHobbies\nAvg.\nB-1\nB-2\nB-3\nB-4\nB-Avg.\nScore\nAvg.\nGPT-4V\nachiam2023gpt\n36.7\n27.7\n33.9\n32.5\n32.6\n32.2\n26.5\n24.7\n23.9\n26.8\n49.6\n36.4\nGemini-2.5-Flash\ncomanici2025gemini\n44.2\n42.3\n43.2\n39.1\n42.4\n39.1\n31.6\n22.9\n22.1\n28.9\n45.3\n38.9\nInternVL2.5-2B\nchen2024expanding\n30.9\n27.8\n28.6\n33.1\n30.1\n36.6\n33.7\n31.0\n29.4\n32.7\n47.1\n36.6\nInternVL3-2B\nzhu2025internvl3\n36.9\n29.9\n35.6\n31.5\n33.4\n34.4\n33.9\n33.5\n33.3\n33.8\n48.8\n38.7\nNVILA-2B\nliu2024nvila\n34.6\n26.7\n33.3\n31.6\n31.4\n38.7\n34.3\n31.1\n29.2\n33.3\n47.0\n37.2\nQwen2.5-VL-3B\nbai2025qwen2\n29.0\n27.0\n30.2\n28.9\n28.5\n42.5\n36.3\n28.7\n31.8\n34.8\n43.4\n35.6\nMagma-8B\nyang2025magma\n32.1\n25.7\n34.4\n29.3\n29.8\n38.6\n31.5\n28.1\n26.7\n31.2\n49.1\n36.7\nRoboBrain2.0-3B\nteam2025robobrain2\n45.3\n37.6\n45.9\n39.7\n41.8\n54.4\n47.7\n43.1\n41.0\n46.5\n50.1\n46.1\nThinkAct-3B\nhuang2025thinkact\n46.6\n41.4\n45.9\n42.5\n44.0\n62.4\n57.3\n52.0\n49.6\n55.3\n48.9\n49.4\nFast-ThinkAct-3B\n50.3\n44.3\n46.4\n43.2\n46.4\n70.1\n63.0\n57.2\n53.0\n60.8\n51.2\n52.8\nWe evaluate Fast-ThinkAct on robotic manipulation using LIBERO\nliu2023libero\nand SimplerEnv\nli24simpler\nbenchmarks. LIBERO covers diverse subtasks, including Spatial, Object, Goal, and Long, while SimplerEnv provides a simulated benchmark with strong real-world correlation, featuring variations in lighting, object appearance, and camera viewpoints. As shown in Fig.\n3\n(a)-(e), Fast-ThinkAct consistently outperforms all baselines, achieving the highest success rates across all LIBERO subtasks and SimplerEnv-Google. This includes substantial improvements over foundation VLAs such as OpenVLA\nkim24openvla\n, and reasoning VLAs including CoT-VLA\nzhao2025cot\n, ThinkAct\nhuang2025thinkact\n, and MolmoAct\nlee2025molmoact\n. Moreover, as shown in Fig.\n3\n(f), our compact latent reasoning achieves 89.3% and 88.0% latency reduction compared to ThinkAct-7B\nhuang2025thinkact\nand MolmoAct-7B\nlee2025molmoact\nrespectively, and\n7\n√ó\n7\\times\nfaster inference than ThinkAct-3B, demonstrating substantial efficiency gains without sacrificing performance.\nTo further validate Fast-ThinkAct on more complex scenarios, we evaluate on RoboTwin2.0\nchen2025robotwin2\n, a challenging bimanual manipulation benchmark requiring long-horizon planning. As shown in Tab.\n1\n, Fast-ThinkAct significantly outperforms previous VLAs including DP\nchi2023diffusion\n, ACT\nzhao2023learning\n,\nœÄ\n0\n\\pi_{0}\nblack2024pi_0\n, RDT\nliu2024rdt\n, and ThinkAct\nhuang2025thinkact\nacross both easy and hard settings. Compared to RDT, Fast-ThinkAct achieves 9.3% and 3.6% higher success rates on easy and hard settings, respectively. Against the reasoning VLA ThinkAct, it improves success by 3.3% and 1.7% while maintaining substantially higher efficiency, as shown in Fig.\n3\n(f). These results demonstrate that our compact reasoning design enables both superior accuracy and computational efficiency on complex bimanual manipulation tasks.\nEmbodied Reasoning.\nIn Tab.\n2\n, we evaluate the reasoning capabilities of Fast-ThinkAct in embodied scenarios across three benchmarks: EgoPlan-Bench2\nqiu2024egoplan2\n, RoboVQA\nsermanet2024robovqa\n, and OpenEQA\nmajumdar2024openeqa\n. These benchmarks assess multi-step planning in egocentric everyday scenarios, long-horizon reasoning for robotic manipulation tasks, and zero-shot understanding of embodied scenes in diverse environments, respectively. We observed that, Fast-ThinkAct surpasses all comparison methods, including two proprietary models (i.e., GPT-4V\nachiam2023gpt\nand Gemini-2.5-Flash\ncomanici2025gemini\n), exceeding the runner-up by 2.4% on EgoPlan-Bench2, 5.5 BLEU score on RoboVQA, and 1.1 points on OpenEQA. These results demonstrate that Fast-ThinkAct effectively handles complex planning sequences and extended reasoning horizons while generalizing to novel environments, showcasing robust capabilities for scene comprehension and multi-step task execution in embodied AI applications.\nFigure 4\n:\nVisualization of predicted visual trajectories and action execution results on long-horizon tasks.\nExamples from (a) SimplerEnv-Google, (b) LIBERO-Long, and (c) RoboTwin2.0-Hard with long (278) steps. Yellow traces indicate single-arm/left gripper trajectories; red traces indicate right gripper trajectories for bimanual tasks.\nFigure 5\n:\nFailure recovery capability on RoboFAC\nlu2025robofac\n.\nLeft: Qualitative examples (from both simulation and real robot) of corrective guidance for manipulation errors. Right: Quantitative evaluation on simulation (RoboFAC-Sim) and real-robot (RoboFAC-Real) settings.\n4.3\nAnalysis of Fast-ThinkAct\nReasoning Enables Long-Horizon Planning.\nFigure 6\n:\nFew-shot adaptation results on RoboTwin2.0 benchmark.\nWe use 10 demonstrations per task for fine-tuning.\nWe analyze Fast-ThinkAct‚Äôs capability on long-horizon tasks in Tab.\n1\nand Fig.\n4\n. We focus on long-horizon tasks (average length exceeding 270 steps) in RoboTwin2.0\nchen2025robotwin2\nthat require multi-step reasoning and extended planning horizons. As shown in Tab.\n1\n, Fast-ThinkAct achieves average scores of 48.8 and 16.8 on easy and hard settings of long-horizon tasks, respectively, surpassing RDT (35.0/12.3) and ThinkAct (42.8/15.3). Fig.\n4\nvisualizes predicted 2D visual traces and execution results on representative tasks from SimplerEnv-Google\nli24simpler\n, LIBERO-Long\nliu2023libero\n, and RoboTwin2.0\nchen2025robotwin2\n. For example, the LIBERO-Long task requires sequentially turning on the stove and placing a moka pot on it, while the RoboTwin2.0 handover task requires bimanual coordination to transfer a block between grippers. The visual traces successfully predict feasible solution paths, with their corresponding representations serving as visual planning guidance for successful execution. These results demonstrate that our compact latent reasoning effectively supports long-horizon planning in complex manipulation scenarios.\nReasoning Enables Failure Recovery.\nFigure 7\n:\nReasoning trace comparison on RoboVQA.\n(a) Teacher‚Äôs textual reasoning. (b) Student‚Äôs verbalized latent reasoning.\nGreen\n: relevant content;\norange\n: less relevant content.\nA key advantage of reasoning-based VLAs\nhuang2025thinkact\n;\nabdolmaleki2025gemini\nis their ability to identify runtime failures and provide corrective guidance for recovery. To evaluate this capability, we conduct experiments on RoboFAC\nlu2025robofac\n, a benchmark specifically designed to assess failure identification and correction in embodied VLMs. As shown in Fig.\n5\n, Fast-ThinkAct substantially outperforms the second-best baseline RoboFAC-3B\nlu2025robofac\nby 10.9 points on the simulation split and 16.4 points on the real-world split. The qualitative examples demonstrate Fast-ThinkAct‚Äôs ability to reason over manipulation videos, identify failures, and propose recovery steps. For instance, in the right example where the target object drops mid-execution, Fast-ThinkAct generates a concrete recovery plan: first moving the arm backward to create space, then adjusting laterally to align with the target object, and finally lowering to the appropriate height for a secure grasp. These results demonstrate that our latent reasoning supports both fast task execution and crucial failure analysis capabilities essential for robust robotic manipulation.\nReasoning Enables Few-Shot Adaptation.\nTable 3\n:\nAblation study of training objectives and learning stages.\nNote that Fast-ThinkAct w/o\n‚Ñí\nverb\n,\n‚Ñí\ndistill\n\\mathcal{L}_{\\text{verb}},\\mathcal{L}_{\\text{distill}}\ndenotes the student VLM\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\ntrained without the corresponding loss components.\nMethod\nEgoPlan\nRoboVQA\nOpenEQA\nAverage\nFast-ThinkAct\n46.4\n60.8\n51.2\n52.8\nw/o\n‚Ñí\nverb\n\\mathcal{L}_{\\text{verb}}\n42.1\n53.8\n49.5\n48.5\nw/o\n‚Ñí\nverb\n,\n‚Ñí\ndistill\n\\mathcal{L}_{\\text{verb}},\\mathcal{L}_{\\text{distill}}\n41.6\n52.7\n48.9\n47.7\nTextual Teacher\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\n41.7\n58.2\n49.4\n49.8\nSFT + CoT-SFT\n40.0\n46.1\n48.8\n45.0\nSFT only\n40.5\n53.6\n45.3\n46.5\nTo assess how reasoning capability improves few-shot adaptation, we conduct few-shot experiments on the RoboTwin2.0 benchmark\nchen2025robotwin2\n, fine-tuning models using only 10 demonstrations per task. As illustrated in Fig.\n6\n, Fast-ThinkAct significantly enhances our adopted action model RDT\nliu2024rdt\nand outperforms the state-of-the-art VLAs, including\nœÄ\n0\n\\pi_{0}\nblack2024pi_0\nand ThinkAct\nhuang2025thinkact\non both medium and long-horizon tasks. Notably, our method achieves these gains while operating with significantly lower reasoning latency compared to ThinkAct, highlighting the advantage of efficient yet effective reasoning for few-shot action adaptation in complex robot manipulation scenarios.\nVisualization of Verbalizable Latent Reasoning.\nIn Fig.\n7\n, we compare the teacher‚Äôs textual reasoning with the student‚Äôs verbalized latent reasoning on RoboVQA. While both capture task-relevant information (\ngreen\n), the teacher generates verbose outputs with less directly relevant content (\norange\n), whereas our student produces more concise and focused responses when verbalized. This demonstrates that our preference-guided distillation not only reduces computational cost but also distills concise reasoning patterns while filtering out redundant information.\nAblation Study.\nIn Tab.\n3\n, we ablate training stages and loss components. Starting from the full Fast-ThinkAct, removing\n‚Ñí\nverb\n\\mathcal{L}_{\\text{verb}}\ncauses performance drops as latent CoTs lack preference-based guidance to align with high-quality reasoning and suppress low-quality patterns. Further removing\n‚Ñí\ndistill\n\\mathcal{L}_{\\text{distill}}\nleads to additional decline, indicating that aligning trajectory-level representations is crucial for transferring visual planning capabilities. Comparing training strategies, CoT-SFT underperforms SFT on EgoPlan-Bench2 and RoboVQA but improves on OpenEQA, suggesting na√Øve chain-of-thought supervision benefits open-ended QA but introduces verbosity that hinders structured reasoning tasks. Our preference-guided approach distills high-quality reasoning while maintaining efficiency. This validates the necessity of our proposed distillation framework and visual trajectory alignment. We provide additional ablation studies in the supplementary material.\n5\nConclusion\nWe presented\nFast-ThinkAct\n, an efficient reasoning framework for vision-language-action tasks that achieves compact yet expressive planning through verbalizable latent reasoning. By distilling lengthy textual reasoning into compact latent representations via preference-guided distillation and visual trajectory alignment, our approach bridges high-level embodied reasoning with low-level action execution through reasoning-enhanced policy learning. Extensive experiments across diverse robotic manipulation and embodied reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with significantly reduced inference latency while enabling effective long-horizon planning, few-shot adaptation, and failure recovery capabilities.\nLimitations and Future Works.\nAs our verbalizer\nùí±\nœà\n\\mathcal{V}_{\\psi}\nis built upon a pre-trained LLM, it inevitably inherits language model limitations, including hallucination, occasionally producing plausible but inaccurate descriptions. However, this does not affect action execution during inference, as the verbalizer serves only for interpretability while action prediction uses the grounded latent representations from visual plan distillation. To further improve the faithfulness of verbalized reasoning, we can consider incorporating grounding-aware objectives or hallucination suppression techniques in future work.\nAppendix A\nAdditional Experimental Setup\nA.1\nAlgorithm\nInput:\nCoT-SFT checkpoint\n‚Ñ±\nŒ∏\n0\n\\mathcal{F}_{\\theta_{0}}\n, training data\nùíü\n\\mathcal{D}\n, rollout size\nN\nN\n, latent reasoning steps\nM\nM\n, number of waypoints\nK\nK\n, total iterations\nT\ntotal\nT_{\\text{total}}\nOutput:\nTrained student model\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\n//\nInitialize models\n‚Ñ±\nŒ∏\nT\n‚Üê\n‚Ñ±\nŒ∏\n0\n\\mathcal{F}_{\\theta}^{T}\\leftarrow\\mathcal{F}_{\\theta_{0}}\n,\n‚Ñ±\nŒ∏\n‚Üê\n‚Ñ±\nŒ∏\n0\n\\mathcal{F}_{\\theta}\\leftarrow\\mathcal{F}_{\\theta_{0}}\n;\nInitialize verbalizer\nùí±\nœà\n\\mathcal{V}_{\\psi}\nfrom pre-trained LLM;\nt\n‚Üê\n0\nt\\leftarrow 0\n;\nwhile\nt\n<\nT\ntotal\nt<T_{\\text{total}}\ndo\nSample batch\n(\no\n,\nl\n,\np\n^\n)\n(o,l,\\hat{p})\nfrom\nùíü\n\\mathcal{D}\n;\n//\nSuppose bs=1 for simplicity\n0.5em\n//\nTeacher GRPO training\nGenerate\nN\nN\nrollouts\n{\nœÑ\ni\n}\ni\n=\n1\nN\n\\{\\tau_{i}\\}_{i=1}^{N}\nfrom\n‚Ñ±\nŒ∏\nT\n‚Äã\n(\no\n,\nl\n)\n\\mathcal{F}_{\\theta}^{T}(o,l)\n;\nCompute trajectory rewards\n{\nr\ni\n}\ni\n=\n1\nN\n\\{r_{i}\\}_{i=1}^{N}\n;\nCompute group-wise advantages\n{\nA\ni\n}\ni\n=\n1\nN\n\\{A_{i}\\}_{i=1}^{N}\n;\nUpdate\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\nwith\nùí•\nGRPO\n\\mathcal{J}_{\\text{GRPO}}\n(Eq. 1);\nœÑ\n+\n‚Üê\narg\n‚Å°\nmax\ni\n‚Å°\nA\ni\n\\tau^{+}\\leftarrow\\arg\\max_{i}A_{i}\n,\nœÑ\n‚àí\n‚Üê\narg\n‚Å°\nmin\ni\n‚Å°\nA\ni\n\\tau^{-}\\leftarrow\\arg\\min_{i}A_{i}\n(Eq. 3) ;\n//\nFor student distillation\nh\nt\nT\n‚Üê\nh_{t}^{T}\\leftarrow\nhidden state of\nœÑ\n+\n\\tau^{+}\nat\n<answer>\ntoken from\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\n;\n//\nFor distillation loss\n//\nStudent latent distillation\nùê≥\n=\n{\nz\nm\n}\nm\n=\n1\nM\n‚Üê\n‚Ñ±\nŒ∏\n‚Äã\n(\no\n,\nl\n)\n\\mathbf{z}=\\{z_{m}\\}_{m=1}^{M}\\leftarrow\\mathcal{F}_{\\theta}(o,l)\n;\n//\nPerform auto-regressive latent reasoning\nCompute\n‚Ñí\nverb\n\\mathcal{L}_{\\text{verb}}\nwith\nùê≥\n\\mathbf{z}\n,\nùí±\nœà\n\\mathcal{V}_{\\psi}\n,\nœÑ\n+\n\\tau^{+}\n,\nœÑ\n‚àí\n\\tau^{-}\n(Eq. 4);\nForward\nK\nK\nspatial tokens from\n‚Ñ±\nŒ∏\n‚Äã\n(\no\n,\nl\n,\nùê≥\n)\n\\mathcal{F}_{\\theta}(o,l,\\mathbf{z})\nto obtain\nh\nt\nh_{t}\nand\n{\nh\n‚Ä≤\n‚Äã\n(\nùê¨\ni\n)\n}\ni\n=\n1\nK\n\\{h^{\\prime}(\\mathbf{s}_{i})\\}_{i=1}^{K}\n;\nCompute\n‚Ñí\ndistill\n\\mathcal{L}_{\\text{distill}}\nwith\nh\nt\nT\nh_{t}^{T}\n,\nh\nt\nh_{t}\n(Eq. 5);\nCompute\n‚Ñí\nans\n\\mathcal{L}_{\\text{ans}}\nwith\n{\nh\n‚Ä≤\n‚Äã\n(\nùê¨\ni\n)\n}\ni\n=\n1\nK\n\\{h^{\\prime}(\\mathbf{s}_{i})\\}_{i=1}^{K}\n,\np\n^\n\\hat{p}\n(Eq. 6);\nUpdate\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nwith\n‚Ñí\nstudent\n=\n‚Ñí\nverb\n+\n‚Ñí\ndistill\n+\n‚Ñí\nans\n\\mathcal{L}_{\\text{student}}=\\mathcal{L}_{\\text{verb}}+\\mathcal{L}_{\\text{distill}}+\\mathcal{L}_{\\text{ans}}\n;\nt\n‚Üê\nt\n+\n1\nt\\leftarrow t+1\n;\nend while\nreturn\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\n;\nAlgorithm¬†1\nTraining Fast-ThinkAct (Sec. 3.2)\nAlgorithm\n1\npresents the complete training procedure corresponding to Sec. 3.2. It shows how we jointly optimize the teacher model with GRPO and distill its reasoning into the student‚Äôs compact latent representations.\nA.2\nImplementation Details\nOur implementation follows the setup described in Sec. 4.1 of the main paper. Here we provide additional details. The verbalizer\nùí±\nœà\n\\mathcal{V}_{\\psi}\nis initialized from a small LLM, Qwen3-0.6B, with cross-attention layers inserted at each layer to condition on latent CoTs\nùê≥\n\\mathbf{z}\n. For the student model training, in the first 3,000 iterations, we replace verbalization loss\n‚Ñí\nverb\n\\mathcal{L}_{\\text{verb}}\nwith language modeling loss using\nœÑ\n+\n\\tau^{+}\nas ground truth to warm up\nùí±\nœà\n\\mathcal{V}_{\\psi}\n‚Äôs alignment with the latent representations\nùê≥\n\\mathbf{z}\n. We then freeze\nùí±\nœà\n\\mathcal{V}_{\\psi}\nand use the\n‚Ñí\nverb\n\\mathcal{L}_{\\text{verb}}\nfor the remaining 1,500 iterations. The student\n‚Ñ±\nŒ∏\n\\mathcal{F}_{\\theta}\nis optimized throughout both phases. For waypoint prediction in Eq. 6, each\np\ni\n‚àà\n‚Ñù\n6\np_{i}\\in\\mathbb{R}^{6}\nencodes coordinates in the format\n[\nx\nsingle\n,\ny\nsingle\n,\nx\nleft\n,\ny\nleft\n,\nx\nright\n,\ny\nright\n]\n[x_{\\text{single}},y_{\\text{single}},x_{\\text{left}},y_{\\text{left}},x_{\\text{right}},y_{\\text{right}}]\n, where the first two dimensions are for single-arm and the last four are for bimanual robot. For ground-truth\np\n^\n‚Äã\ni\n\\hat{p}i\n, we fill the corresponding dimensions based on robot type and mask out the unused dimensions when computing\n‚Ñí\nans\n\\mathcal{L}_{\\text{ans}}\n. For GRPO training, we follow the configuration of ThinkAct\nhuang2025thinkact\n, using rollout size\nN\n=\n5\nN=5\n. Following\nlee2025molmoact\n, we set the number of waypoints in trajectory to\nK\n=\n5\nK=5\n. We use\nM\n=\n6\nM=6\nlatent reasoning tokens, with ablation study provided in Fig.\n8\n.\nDuring reasoning-enhanced policy learning, for SimplerEnv\nli24simpler\nevaluation, to ensure fair comparison with previous works\nkim24openvla\n;\nlee2025molmoact\n, we initialize\nœÄ\nœï\n\\pi_{\\phi}\nfrom DiT-Policy\nchi2023diffusion\npre-trained on the same OXE dataset\no2024open\n;\nkim24openvla\nand conduct reasoning-enhanced policy learning (Sec. 3.3) using the same OXE data. For LIBERO\nliu2023libero\nand RoboTwin2.0\nchen2025robotwin2\nevaluations, we initialize\nœÄ\nœï\n\\pi_{\\phi}\nfrom RDT\nliu2024rdt\n, which has demonstrated strong performance on RoboTwin2.0, and conduct policy learning using OXE\no2024open\nand static ALOHA datasets\nshi2023waypoint\n;\nzhao2023learning\n. Our method further enhances RDT‚Äôs manipulation capabilities on both benchmarks. The use of different action models also demonstrates that our approach is agnostic to the underlying action model choice.\nA.3\nTraining Data Details\nA.3.1\nDataset Sources\n2D Visual Trace of Manipulation Tasks.\nFor single-arm manipulation, we utilize 2D visual trajectories labeled by MolmoAct\nlee2025molmoact\nfrom the Open X-Embodiment (OXE) dataset\no2024open\n, comprising approximately 1.3M trajectories. For bimanual manipulation, we extract dual-arm visual trajectories from the AIST dataset\naist2025aist\n, resulting in approximately 92K trajectory samples. Specifically, we first use Molmo-72B\ndeitke2024molmo\nto detect left and right gripper positions (following\nlee2025molmoact\n) in the first frame, then apply CoTracker3\nkaraev2025cotracker3\nto track and parse the manipulation trajectories throughout the video sequences.\nRoboFAC\nlu2025robofac\n.\nRoboFAC is a robotic failure analysis dataset containing 9,440 erroneous manipulation trajectories across 16 tasks in both simulated and real-world environments. We utilize the training set with 64K QA pairs covering various failure types for developing failure identification and correction planning capabilities.\nRoboVQA\nsermanet2024robovqa\n.\nRoboVQA contains robot manipulation videos with QA tasks covering task understanding. The dataset includes approximately 5K long-horizon and 92K medium-horizon video sequences from diverse robotic platforms, resulting in total 798K QA pairs. Videos are annotated with multiple questions probing spatial reasoning, action prediction, and task comprehension.\nShareRobot\nji2025robobrain\n.\nShareRobot is a large-scale dataset collected by RoboBrain\nji2025robobrain\n, containing over 1M QA pairs covering task planning, object affordances, and manipulation strategies across diverse robot embodiments and scenes. The dataset features fine-grained annotations linking task descriptions to frame-level execution details, facilitating learning of transferable manipulation knowledge.\nEgoPlan-Bench\nchen2023egoplan\n.\nEgoPlan-Bench features egocentric videos of daily activities annotated with task planning information including goals, execution history, and current states. The dataset contains approximately 53K video-text pairs for training long-horizon planning and progress tracking capabilities from egocentric view.\nVideo-R1-CoT\nfeng2025video\n.\nVideo-R1 comprises 165K video question-answer pairs with chain-of-thought reasoning annotations generated by large-scale vision-language models. The dataset covers diverse reasoning domains including mathematical logic, spatial understanding, OCR, and visual analytics. All samples are quality-filtered to ensure annotation consistency and correctness.\nPixMo\ndeitke2024molmo\n.\nPixMo is a general-purpose vision-language dataset with diverse image captions and question-answer pairs. Following MolmoAct\nlee2025molmoact\n, we incorporate PixMo dataset to preserve general visual understanding and prevent catastrophic forgetting when training on embodied dataset. Specifically, we use approximately 726K samples from the\nask_model_anything\n,\ncap\n, and\ncap-qa\nsplits.\nA.3.2\nData Processing and Formatting\nSupervised Fine-Tuning (SFT).\nTo enhance foundational embodied knowledge, we perform supervised fine-tuning on approximately 4M samples combining 2D visual trajectories from MolmoAct\nlee2025molmoact\nand AIST\naist2025aist\n, along with QA data from PixMo\ndeitke2024molmo\n, RoboFAC\nlu2025robofac\n, RoboVQA\nsermanet2024robovqa\n, ShareRobot\nji2025robobrain\n, and EgoPlan\nchen2023egoplan\n. This stage enables the model to acquire basic visual understanding, task comprehension, and manipulation knowledge across diverse embodiments and scenarios.\nChain-of-Thought SFT (CoT-SFT).\nTo develop reasoning capabilities while preserving embodied understanding, we sample 5% from the SFT data (approximately 200K samples) and augment with 165K samples from Video-R1-CoT\nfeng2025video\n. For data with CoT annotations, we format prompts to elicit structured reasoning enclosed in\n<think>\ntags followed by answers in\n<answer>\ntags; for data without CoT annotations, we prompt for direct answers only. This enables the model to learn reasoning capabilities from CoT-annotated data and generalize them to embodied tasks.\nTeacher-Student Training.\nBuilding upon the CoT-SFT checkpoint, we curate a balanced training set by sampling approximately 5,000 instances from each dataset and data type, totaling nearly 50K samples. We adopt the prompt formatting strategy from CoT-SFT for both teacher GRPO training and student latent distillation. We train both the teacher with GRPO and the student with latent distillation (as detailed in Sec. 3.2) on this data, efficiently transferring high-quality reasoning patterns into compact latent representations.\nTable 4\n:\nQuantitative results with larger model size (7B or 8B) on embodied reasoning benchmarks.\nMethod\nEgoPlan-Bench2\nRoboVQA\nOpenEQA\nOverall\nDaily.\nWork.\nRec.\nHobbies\nAvg.\nB-1\nB-2\nB-3\nB-4\nB-Avg.\nScore\nAvg.\nInternVL2.5-8B\nchen2024expanding\n36.2\n28.7\n34.4\n35.4\n33.5\n40.5\n33.3\n29.6\n27.5\n32.7\n54.4\n40.2\nInternVL3-8B\nzhu2025internvl3\n38.5\n32.9\n36.1\n37.2\n36.2\n44.3\n36.5\n31.6\n28.9\n35.3\n55.5\n42.3\nNVILA-8B\nliu2024nvila\n35.8\n28.7\n37.2\n35.4\n33.7\n42.7\n39.7\n37.6\n36.1\n39.0\n54.0\n42.2\nQwen2.5-VL-7B\nbai2025qwen2\n31.4\n26.7\n29.5\n28.6\n29.1\n47.8\n41.2\n36.2\n33.7\n39.7\n50.8\n39.9\nMagma-8B\nyang2025magma\n32.1\n25.7\n34.4\n29.3\n29.8\n38.6\n31.5\n28.1\n26.7\n31.2\n49.1\n36.7\nRoboBrain2.0-7B\nteam2025robobrain2\n39.4\n27.0\n33.9\n32.2\n33.2\n44.9\n38.2\n34.7\n33.5\n37.8\n51.1\n40.7\nThinkAct-7B\nhuang2025thinkact\n50.1\n49.8\n44.8\n45.2\n48.2\n69.1\n61.8\n56.0\n52.4\n59.8\n56.2\n54.7\nFast-ThinkAct-7B\n51.3\n47.3\n41.5\n45.9\n47.5\n70.4\n63.3\n57.3\n53.2\n61.1\n59.0\n55.9\nTable 5\n:\nResults on LIBERO and SimplerEnv benchmarks with additional ThinkAct-3B comparison.\nMethod\nLIBERO\nSimplerEnv-Google\nLatency (\n‚Üì\n\\downarrow\n)\nOpenVLA-7B\nkim24openvla\n76.5\n40.2\nN/A\nCoT-VLA-7B\nzhao2025cot\n83.9\nN/A\nN/A\nThinkAct-7B\nhuang2025thinkact\n84.4\n68.3\n7513\nMolmoAct-7B\nlee2025molmoact\n86.8\n64.9\n6723\nThinkAct-3B\nhuang2025thinkact\n83.1\n64.7\n5674\nFast-ThinkAct-3B\n89.7\n68.7\n805 (\n‚Üì\n\\downarrow\n7.0\n√ó\n\\times\n)\nTable 6\n:\nComparison with efficient textual reasoning methods.\nMethod\nEgoPlan-\nBench2\nRoboVQA\nOpenEQA\nAverage\nTextual Teacher\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\n41.7\n58.2\n49.4\n49.8\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\nInference w/o thinking\n42.7\n55.0\n41.7\n46.5\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\nInference w/ 6 textual tokens\n39.3\n53.0\n46.5\n46.3\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\nw/ RL Length-Penalty\narora2025training\n41.2\n57.5\n44.7\n47.8\nFast-ThinkAct-3B\n46.4\n60.8\n52.8\n53.3\nA.4\nEvaluation Setup\nA.4.1\nEmbodied Reasoning Benchmarks\nWe evaluate on three benchmarks assessing different aspects of embodied reasoning. EgoPlan-Bench2\nchen2023egoplan\ntests egocentric task planning across 24 daily-life scenarios with 1,321 multiple-choice questions, measuring accuracy in predicting next steps given task goals and progress history. RoboVQA\nsermanet2024robovqa\nevaluates visual reasoning in manipulation contexts through 1,893 free-form QA pairs from robot and human demonstrations, assessed via BLEU score. OpenEQA\nmajumdar2024openeqa\nassesses spatial and functional understanding through 1,600+ questions spanning 180+ real-world environments, evaluated using LLM-based scoring aligned with human preferences. These benchmarks comprehensively evaluate embodied reasoning capability across planning, manipulation, and spatial understanding.\nA.4.2\nRobotic Manipulation Benchmarks\nFigure 8\n:\nVisualization of predicted visual trajectories and action execution results on RoboTwin2.0.\nYellow traces indicate left gripper trajectories; red traces indicate right gripper trajectories for bimanual tasks.\nFigure 9\n:\nFailure identification and analysis capabilities on RoboFAC\nlu2025robofac\n.\nTop row shows identification of failure types and execution stages. Bottom row demonstrates failure root cause analysis.\nFigure 10\n:\nReasoning trace comparison on OpenEQA.\n(a) Teacher‚Äôs textual reasoning. (b) Student‚Äôs verbalized latent reasoning.\nGreen\n: reasonable reasoning trace;\nred\n: incorrect trace.\nWe evaluate on three simulation benchmarks covering diverse manipulation scenarios. SimplerEnv\nli24simpler\nprovides manipulation tasks with strong sim-to-real correlation, featuring diverse visual variations in lighting, textures, backgrounds, and camera poses. Following MolmoAct\nlee2025molmoact\n, we evaluate on the Google Robot tasks using the standard protocol\nkim24openvla\n;\nlee2025molmoact\nof directly evaluating on SimplerEnv after training on OXE. LIBERO\nliu2023libero\ntargets different generalization challenges through four task suites: spatial layout variation (LIBERO-Spatial), object diversity (LIBERO-Object), goal variation (LIBERO-Goal), and long-horizon planning with mixed variations (LIBERO-Long). We evaluate each suite over 500 trials using 3 random seeds following prior works\nkim24openvla\n;\nlee2025molmoact\n. RoboTwin2.0\nchen2025robotwin2\nfeatures challenging bimanual manipulation with easy and hard difficulty settings, where the hard setting introduces domain randomization including clutter, lighting variations, diverse textures, and height changes. Following the original protocol, we train on 50 clean expert demonstrations per task and evaluate with 100 rollouts under both settings. We assess 10 tasks categorized into short, medium, and long horizons based on demonstration lengths.\nAppendix B\nAdditional Experiment Results\nB.1\nAdditional Quantitative Results\nResults of Larger Model Size.\nTo demonstrate the scalability of our approach, we apply Fast-ThinkAct to a larger backbone, Qwen2.5-VL-7B, and evaluate its performance on embodied reasoning benchmarks. As shown in Tab.\n4\n, Fast-ThinkAct consistently achieves strong performance across EgoPlan-Bench2\nqiu2024egoplan2\n, RoboVQA\nsermanet2024robovqa\n, and OpenEQA\nmajumdar2024openeqa\n, validating that our latent reasoning distillation method effectively scales to larger model backbones.\nPerformance Comparison with ThinkAct-3B.\nTab.\n5\npresents detailed numerical results corresponding to Fig. 3 with additional ThinkAct-3B results. At the same 3B model size, Fast-ThinkAct achieves notable performance gains (89.7 vs. 83.1 on LIBERO, 68.7 vs. 64.7 on SimplerEnv-Google) while dramatically improving efficiency with\n7\n√ó\n\\times\nfaster inference (805ms vs. 5674ms).\nComparison with Efficient Reasoning Baselines.\nTable\n6\ncompares our method with efficient textual reasoning alternatives applied to the textual teacher\n‚Ñ±\nŒ∏\nT\n\\mathcal{F}_{\\theta}^{T}\n. We evaluate three baselines: removing reasoning during inference entirely (0 tokens), constraining the teacher to generate only 6 textual tokens during inference, and applying RL training with a length penalty\narora2025training\nto encourage concise reasoning (\n‚àº\n\\sim\n50 tokens). These achieve 46.5, 46.3, and 47.8 respectively, all degrading from the teacher‚Äôs 49.8. In contrast, Fast-ThinkAct uses only 6 latent tokens and achieves\n53.3\n, demonstrating superior efficiency and performance.\nB.2\nAdditional Qualitative Results\nQualitative Robot Execution.\nWe provide qualitative robot execution comparisons between the base action model RDT\nliu2024rdt\nand Fast-ThinkAct in the supplementary video\nFast-ThinkAct.mp4\n. Our method shows substantial improvements on challenging robotic execution tasks, where reasoning capabilities provide better spatial understanding and coordination for successful manipulation.\nBimanual Manipulation Results.\nIn Fig.\n8\n, we present visualized trajectories and execution results for\nhanging mug\nand\nhandover mic\ntasks under easy and hard settings in RoboTwin2.0\nchen2025robotwin2\n. The hard setting includes different backgrounds and distractor objects. These examples show successful bimanual coordination where predicted waypoints accurately guide both grippers through the manipulation sequence, demonstrating Fast-ThinkAct‚Äôs spatial reasoning ability across varied visual conditions.\nFailure Identification and Recovery.\nIn Fig.\n9\n, we demonstrate Fast-ThinkAct‚Äôs failure identification and analysis capabilities, complementing the recovery planning shown in the main paper. The top row shows that Fast-ThinkAct identifies failure types (e.g., position deviation) and execution stages (e.g., reaching for the cube). The bottom row illustrates root cause analysis, for instance, in the bottom-right example, the model correctly infers that the failure to push the cube with an L-shaped tool stems from an improper initial grasp. These results demonstrate Fast-ThinkAct‚Äôs comprehensive understanding of manipulation failures beyond recovery planning.\nVerbalized Latent Reasoning.\nFig.\n10\nvisualizes teacher textual reasoning and student verbalized reasoning. While the student generates compact and correct (\ngreen\n) reasoning, the teacher‚Äôs lengthy output sometimes contains erroneous steps (\nred\n) that might degrade the performance.\nB.3\nAdditional Ablation Study and Analysis\nAdditional Ablation Results on Manipulation Benchmarks.\nTable\n8\nshows ablation results on LIBERO\nliu2023libero\n, SimplerEnv-Google\nli24simpler\n, and RoboTwin2.0\nchen2025robotwin2\n. Removing\n‚Ñí\nverb\n\\mathcal{L}_{\\text{verb}}\nor\n‚Ñí\ndistill\n\\mathcal{L}_{\\text{distill}}\nprogressively degrades performance, confirming their contributions. Our full model consistently outperforms the textual teacher and models without teacher-student training (CoT-SFT, SFT only), demonstrating the benefits of compact latent reasoning distillation.\nTable 7\n:\nAdditional ablation study of training objectives and learning stages on robot manipulation benchmarks.\nMethod\nLIBERO\nSimplerEnv\nGoogle\nRoboTwin2.0\nAverage\nFast-ThinkAct\n89.7\n68.7\n46.1\n68.2\nw/o\n‚Ñí\nverb\n\\mathcal{L}_{\\text{verb}}\n88.6\n67.3\n44.9\n66.9\nw/o\n‚Ñí\nverb\n,\n‚Ñí\ndistill\n\\mathcal{L}_{\\text{verb}},\\mathcal{L}_{\\text{distill}}\n86.3\n65.7\n42.6\n64.9\nTextual Teacher\n88.5\n67.3\n45.8\n67.2\nSFT + CoT-SFT\n87.2\n65.8\n43.3\n65.4\nSFT only\n86.9\n64.5\n42.8\n64.7\nTable 8\n:\nAblation of Latent Reasoning Steps\nM\nM\n.\nAblation Study on Action Model Conditioning.\nIn Sec. 3.3, we extract visual latent planning\nc\nt\nc_{t}\nfrom early-layer KV cache of spatial tokens to condition the action model. We compare this against using late-layer KV cache (last\nN\nN\nlayers, where\nN\nN\nis the action model depth) and directly using spatial tokens‚Äô output hidden states. Our approach achieves\n89.7\non LIBERO, outperforming late-layer KV at 88.3 and output hidden states at 87.1, demonstrating that early-layer representations better capture visual planning information for action prediction. Therefore, we adopt early-layer KV conditioning as our default configuration.\nAblation Study on Latent Reasoning Steps.\nIn Fig.\n8\n, we study the effect of latent reasoning steps\nM\nM\n. We observe that too few steps (\nM\n=\n1\nM=1\n) limit reasoning capacity, while excessive steps (\nM\n=\n30\n,\n100\nM=30,100\n) might introduce redundant or noisy information. Therefore, we adopt\nM\n=\n6\nM=6\n, which achieves optimal performance, as our default.\nReferences",
  "preview_text": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.\n\n\\correspondingauthor\nX\nFast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning\nChi-Pin Huang\n1\nYunze Man\n2\nZhiding Yu\nMin-Hung Chen\nJan Kautz\nYu-Chiang Frank Wang\n1\nFu-En Yang\nNVIDIA\nAbstract\nVision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories ",
  "is_relevant": true,
  "relevance_score": 8.0,
  "extracted_keywords": [
    "Reinforcement Learning",
    "VLA",
    "locomotion",
    "whole body control"
  ],
  "one_line_summary": "Fast-ThinkAct ÊòØ‰∏Ä‰∏™È´òÊïàÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊé®ÁêÜÊ°ÜÊû∂ÔºåÈÄöËøáÂèØË®ÄËØ≠ÂåñÁöÑÊΩúÂú®ËßÑÂàíÂáèÂ∞ëÊé®ÁêÜÂª∂ËøüÔºåÊèêÂçáÂÖ∑Ë∫´ÊéßÂà∂ÊÄßËÉΩ„ÄÇ",
  "detailed_summary": "## ËÆ∫ÊñáÊëòË¶ÅÔºöFast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning\n\n### 1. Á†îÁ©∂ËÉåÊôØÂíåÂä®Êú∫\nËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂú®ÂÖ∑Ë∫´Êô∫ËÉΩ‰ªªÂä°‰∏≠ÈúÄË¶ÅÂ§ÑÁêÜÂ§çÊùÇÁöÑËßÜËßâÂú∫ÊôØÂπ∂ÊâßË°åËá™ÈÄÇÂ∫îÂä®‰Ωú„ÄÇÁé∞ÊúâÂü∫‰∫éÊòæÂºèÊÄùÁª¥ÈìæÁöÑÊé®ÁêÜÊñπÊ≥ïËôΩÁÑ∂ÊèêÂçá‰∫ÜÊ≥õÂåñËÉΩÂäõÔºå‰ΩÜÂÜóÈïøÁöÑÊé®ÁêÜËøáÁ®ãÂØºËá¥**Êé®ÁêÜÂª∂ËøüÊûÅÈ´ò**Ôºà‰æãÂ¶ÇÊØèÁßí0.1Ê¨°ÂÜ≥Á≠ñÔºâÔºåÊó†Ê≥ïÊª°Ë∂≥Êú∫Âô®‰∫∫Á≠âÂÆûÊó∂Â∫îÁî®ÁöÑÈúÄÊ±Ç„ÄÇÂ¶Ç‰ΩïÂú®‰øùÊåÅÂº∫Â§ßÊé®ÁêÜËÉΩÂäõÁöÑÂêåÊó∂ÔºåÂ§ßÂπÖÊèêÂçáÊïàÁéáÔºåÊàê‰∏∫ÂÖ≥ÈîÆÊåëÊàò„ÄÇ\n\n### 2. Ê†∏ÂøÉÊñπÊ≥ïÂíåÊäÄÊúØÂàõÊñ∞\nÊú¨ÊñáÊèêÂá∫ **Fast-ThinkAct**Ôºå‰∏Ä‰∏™È´òÊïàÁöÑÂÖ∑Ë∫´Êé®ÁêÜÊ°ÜÊû∂ÔºåÂÖ∂Ê†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂ∞ÜÂÜóÈïøÁöÑÊñáÊú¨ÊÄùÁª¥Èìæ**ÂéãÁº©‰∏∫ÂèØË®ÄËØ≠ÂåñÁöÑÊΩúÂú®Ë°®Á§∫**„ÄÇ\n- **ÂèØË®ÄËØ≠ÂåñÊΩúÂú®ËßÑÂàí**ÔºöÈááÁî®Â∏àÁîüËí∏È¶èÊ°ÜÊû∂„ÄÇÊïôÂ∏àÊ®°ÂûãÔºàÊñáÊú¨VLMÔºâÁîüÊàêÊòæÂºèÊÄùÁª¥ÈìæÔºõÂ≠¶ÁîüÊ®°ÂûãÂàôÂ≠¶‰π†ÁîüÊàê**Á¥ßÂáëÁöÑËøûÁª≠ÊΩúÂú®ÂêëÈáè**Êù•ÁºñÁ†ÅÊé®ÁêÜËøáÁ®ã„ÄÇÈÄöËøá‰∏Ä‰∏™**Ë®ÄËØ≠ÂåñÂô®**ÔºåËøô‰∫õÊΩúÂú®ÂêëÈáèÂèØË¢´Ëß£Á†ÅÂõûÊñáÊú¨ÔºåÁ°Æ‰øù‰∫ÜÊΩúÂú®Á©∫Èó¥ÁöÑËØ≠‰πâÂèØËß£ÈáäÊÄß„ÄÇ\n- **ÂÅèÂ•ΩÂºïÂØºÁöÑËí∏È¶è**ÔºöÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ‰∏≠ÁöÑÂ•ñÂä±‰ø°Âè∑ÊûÑÂª∫**È´òË¥®Èáè‰∏é‰ΩéË¥®ÈáèÊé®ÁêÜÂØπ**ÔºåÊåáÂØºÂ≠¶ÁîüÊ®°ÂûãÁöÑÊΩúÂú®Ë°®Á§∫Êõ¥ÂÄæÂêë‰∫éÁºñÁ†ÅÈ´òË¥®ÈáèÁöÑÊé®ÁêÜÊ®°Âºè„ÄÇ\n- **ËßÜËßâËΩ®ËøπÂØπÈΩê**ÔºöÈô§‰∫ÜËØ≠Ë®ÄÊé®ÁêÜÔºåËøòÈÄöËøáÂØπÈΩêÂ∏àÁîüÊ®°ÂûãÂú®ËßÜËßâËΩ®ËøπËßÑÂàí‰∏äÁöÑÈöêËóèÁä∂ÊÄÅÔºåÂ∞ÜÂÖ≥ÈîÆÁöÑ**Á©∫Èó¥ËßÜËßâËßÑÂàíËÉΩÂäõ**Ëí∏È¶èÂà∞Â≠¶ÁîüÊ®°Âûã‰∏≠„ÄÇ\n- **Êé®ÁêÜÂ¢ûÂº∫ÁöÑÁ≠ñÁï•Â≠¶‰π†**ÔºöËÆ≠ÁªÉÂÆåÊàêÂêéÔºåÂÜªÁªìÂ≠¶ÁîüVLMÔºåÂ∞ÜÂÖ∂ÁîüÊàêÁöÑÁ¥ßÂáëËßÜËßâËßÑÂàíÊΩúÂú®Ë°®Á§∫‰Ωú‰∏∫Êù°‰ª∂ÔºåÊåáÂØº‰∏Ä‰∏™Êâ©Êï£TransformerÂä®‰ΩúÊ®°ÂûãÁîüÊàê‰ΩéÂ±ÇÊú∫Âô®‰∫∫Âä®‰ΩúÔºå‰ªéËÄåÊ°•Êé•È´òÂ±ÇËßÑÂàí‰∏éÂ∫ïÂ±ÇÊâßË°å„ÄÇ\n\n### 3. ‰∏ªË¶ÅÂÆûÈ™åÁªìÊûú\nÂú®Â§ö‰∏™ÂÖ∑Ë∫´Êìç‰Ωú‰∏éÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÔºåFast-ThinkActÂèñÂæó‰∫ÜÂçìË∂äÁöÑÊÄßËÉΩ‰∏éÊïàÁéáÔºö\n- **ÊÄßËÉΩÈ¢ÜÂÖà**ÔºöÂú®LIBERO„ÄÅSimplerEnv„ÄÅRoboTwin2.0Á≠âÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏äÔºåÊàêÂäüÁéáË∂ÖË∂äOpenVLA„ÄÅThinkActÁ≠âÂÖàËøõÊ®°Âûã„ÄÇÂú®EgoPlan„ÄÅRoboVQAÁ≠âÂÖ∑Ë∫´Êé®ÁêÜÂü∫ÂáÜ‰∏äÔºå‰πüË∂ÖËøá‰∫ÜGPT-4V„ÄÅGeminiÁ≠âÈÄöÁî®Â§ßÊ®°Âûã„ÄÇ\n- **ÊïàÁéáÈ£ûË∑É**ÔºöÁõ∏ÊØîThinkAct-7BÔºåÂÆûÁé∞‰∫ÜÈ´òËææ**89.3%ÁöÑÊé®ÁêÜÂª∂ËøüÈôç‰Ωé**ÔºåÊé®ÁêÜÈÄüÂ∫¶ÊèêÂçáÁ∫¶9.3ÂÄçÔºåÂêåÊó∂ÊÄßËÉΩÊõ¥‰ºò„ÄÇ\n- **ÂÖ≥ÈîÆËÉΩÂäõ**ÔºöÂÆûÈ™åÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÂú®**ÈïøÊó∂Á®ãËßÑÂàí„ÄÅÂ∞èÊ†∑Êú¨ÈÄÇÂ∫î„ÄÅÂ§±Ë¥•ÊÅ¢Â§ç**Á≠âÊñπÈù¢ÂùáÂÖ∑Â§áÂº∫Â§ßËÉΩÂäõ„ÄÇ\n\n### 4. Á†îÁ©∂ÊÑè‰πâÂíå‰ª∑ÂÄº\nFast-ThinkActÈ¶ñÊ¨°Á≥ªÁªüÊÄßÂú∞Ëß£ÂÜ≥‰∫ÜÊé®ÁêÜVLAÊ®°ÂûãÁöÑÊïàÁéáÁì∂È¢àÈóÆÈ¢ò„ÄÇÂÖ∂‰ª∑ÂÄºÂú®‰∫éÔºö\n- **ÊäÄÊúØË¥°ÁåÆ**ÔºöÊèêÂá∫‰∫Ü‚ÄúÂèØË®ÄËØ≠ÂåñÊΩúÂú®Êé®ÁêÜ‚ÄùÁöÑÊñ∞ËåÉÂºèÔºåÈÄöËøáÂÅèÂ•ΩËí∏È¶è‰∏éËßÜËßâÂØπÈΩêÔºåÂú®ÂéãÁº©Ë°®Á§∫ÁöÑÂêåÊó∂‰øùÁïô‰∫Ü‰∏∞ÂØåÁöÑËØ≠‰πâÂíåÁ©∫Èó¥‰ø°ÊÅØÔºå‰∏∫È´òÊïàÂ§öÊ®°ÊÄÅÊé®ÁêÜÊèê‰æõ‰∫ÜÊñ∞ÊÄùË∑Ø„ÄÇ\n- **ÂÆûÈôÖÂ∫îÁî®**ÔºöÂ§ßÂπÖÊèêÂçáÁöÑÊé®ÁêÜÈÄüÂ∫¶‰ΩøÂæóÂ§çÊùÇÁöÑÂú®Á∫øËßÑÂàí‰∏éÂÜ≥Á≠ñËÉΩÂ§üÂ∫îÁî®‰∫éÂØπÂÆûÊó∂ÊÄßË¶ÅÊ±Ç‰∏•Ê†ºÁöÑÁúüÂÆûÊú∫Âô®‰∫∫Âú∫ÊôØÔºåÊé®Âä®‰∫ÜÂÖ∑Ë∫´Êô∫ËÉΩËµ∞ÂêëÂÆûÁî®Âåñ„ÄÇ\n- **ÂºÄÊ∫ê‰∏éÂèØÂ§çÁé∞**ÔºöÂü∫‰∫éÂºÄÊ∫êÊ®°ÂûãÊûÑÂª∫Ôºå‰∏∫Á§æÂå∫Êèê‰æõ‰∫ÜÈ´òÊïàVLAÁ†îÁ©∂ÁöÑÊñ∞Âü∫Á∫ø„ÄÇ",
  "qa_pairs": [
    {
      "question": "ËøôÁØáËÆ∫ÊñáÁöÑÊ†∏ÂøÉÂàõÊñ∞ÁÇπÊòØ‰ªÄ‰πàÔºå‰ªñÊÉ≥Ëß£ÂÜ≥‰ªÄ‰πàÈóÆÈ¢òÔºåÊÄé‰πàËß£ÂÜ≥ÁöÑÔºü",
      "answer": "## ËÆ∫ÊñáÊ†∏ÂøÉÂàÜÊûêÔºöFast-ThinkAct\n\n### **‰∏Ä„ÄÅ Ê†∏ÂøÉÈóÆÈ¢ò**\nËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥**ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®ÂÖ∑Ë∫´Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑ‚ÄúÊïàÁéá-ÊÄßËÉΩ‚ÄùÊùÉË°°ÈöæÈ¢ò**„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºö\n- **ÈóÆÈ¢ò**ÔºöÁé∞ÊúâÁöÑ‚ÄúÊé®ÁêÜÂûãVLA‚ÄùÊ®°ÂûãÔºàÂ¶ÇThinkActÔºâÈÄöËøáÁîüÊàêÊòæÂºèÁöÑ„ÄÅÂÜóÈïøÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâÊù•ÊèêÂçá‰ªªÂä°Ê≥õÂåñËÉΩÂäõÂíåËßÑÂàíËÉΩÂäõÔºå‰ΩÜËøôÂØºËá¥‰∫Ü**ÊûÅÈ´òÁöÑÊé®ÁêÜÂª∂Ëøü**Ôºà‰æãÂ¶ÇÊØèÁßí‰ªÖËÉΩÂÅöÂá∫0.1Ê¨°ÂÜ≥Á≠ñÔºâÔºåÊó†Ê≥ïÊª°Ë∂≥Êú∫Âô®‰∫∫Á≠â**ÂÆûÊó∂ÊÄßË¶ÅÊ±ÇÈ´ò**ÁöÑÂÖ∑Ë∫´AIÂ∫îÁî®ÈúÄÊ±Ç„ÄÇ\n- **ÁüõÁõæ**ÔºöÁõ¥Êé•Áº©Áü≠ÊñáÊú¨Êé®ÁêÜÈìæÔºàCoTÔºâ‰ºö‰∏¢Â§±ÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇÂ¶Ç‰ΩïÂú®‰øùÊåÅÂº∫Â§ßÊé®ÁêÜËÉΩÂäõÁöÑÂêåÊó∂ÔºåÂÆûÁé∞È´òÊïà„ÄÅÁ¥ßÂáëÁöÑËßÑÂàíÔºåÊòØÂΩìÂâçÈ¢ÜÂüüÁöÑÊ†∏ÂøÉÊåëÊàò„ÄÇ\n\n### **‰∫å„ÄÅ Ê†∏ÂøÉÂàõÊñ∞ÁÇπ**\nËÆ∫ÊñáÊèêÂá∫‰∫Ü **Fast-ThinkAct** Ê°ÜÊû∂ÔºåÂÖ∂Ê†∏ÂøÉÂàõÊñ∞Âú®‰∫é **‚ÄúÂèØË®ÄËØ≠ÂåñÁöÑÊΩúÂú®ËßÑÂàí‚Äù** „ÄÇËøôÊòØ‰∏ÄÁßç**ËåÉÂºèËΩ¨Êç¢**Ôºö‰ªéÁîüÊàêÂÜóÈïøÁöÑÊòæÂºèÊñáÊú¨Êé®ÁêÜÈìæÔºåËΩ¨ÂêëÂ≠¶‰π†Á¥ßÂáëÁöÑ„ÄÅËøûÁª≠ÁöÑÊΩúÂú®Ë°®Á§∫Êù•ËøõË°åÂÜÖÈÉ®Êé®ÁêÜ„ÄÇ\n\n1.  **ÂàõÊñ∞ÊñπÊ≥ïÔºöÂèØË®ÄËØ≠ÂåñÁöÑÊΩúÂú®Êé®ÁêÜ**\n    - **‰º†ÁªüÊñπÊ≥ï**ÔºöVLM ‚Üí ÁîüÊàêÂÜóÈïøÊñáÊú¨CoTÔºà~250‰∏™tokenÔºâ‚Üí ÊåáÂØºÂä®‰Ωú„ÄÇ\n    - **Fast-ThinkAct**ÔºöVLM ‚Üí ÁîüÊàêÂ∞ëÈáè**ËøûÁª≠ÊΩúÂú®ÂêëÈáè**ÔºàÂ¶Ç6‰∏™Ôºâ + **Âπ∂Ë°åÁ©∫Èó¥token** ‚Üí ÊåáÂØºÂä®‰Ωú„ÄÇ\n    - **ÂÖ≥ÈîÆ**ÔºöËøô‰∫õÊΩúÂú®ÂêëÈáèÂèØ‰ª•ÈÄöËøá‰∏Ä‰∏™È¢ùÂ§ñÁöÑ‚ÄúË®ÄËØ≠ÂåñÂô®‚ÄùLLMËß£Á†ÅÂõûËá™ÁÑ∂ËØ≠Ë®ÄÔºå‰ªéËÄå‰øùËØÅ‰∫ÜÊΩúÂú®Á©∫Èó¥ÁöÑ**ÂèØËß£ÈáäÊÄß**ÔºåÂπ∂Âú®ËÆ≠ÁªÉÈò∂ÊÆµÂà©Áî®ËØ≠Ë®ÄÂÅèÂ•ΩËøõË°åÂØπÈΩê„ÄÇ\n\n2.  **ÂàõÊñ∞ËÆ≠ÁªÉÊú∫Âà∂ÔºöÂÅèÂ•ΩÂºïÂØºÁöÑËí∏È¶è‰∏éËΩ®ËøπÂØπÈΩê**\n    ‰∏∫‰∫ÜÂ∞ÜÊïôÂ∏àÊ®°ÂûãÔºàÁîüÊàêÊñáÊú¨CoTÔºâÁöÑËÉΩÂäõÂéãÁº©Âà∞Â≠¶ÁîüÊ®°ÂûãÔºàÁîüÊàêÊΩúÂú®ÂêëÈáèÔºâ‰∏≠ÔºåËÆ∫ÊñáËÆæËÆ°‰∫Ü‰∏Ä‰∏™Â§öÁõÆÊ†áËí∏È¶èÊ°ÜÊû∂Ôºö\n    - **ÂÅèÂ•ΩÂºïÂØºÁöÑË®ÄËØ≠ÂåñÊçüÂ§±**ÔºöÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÂú®Âº∫ÂåñÂ≠¶‰π†ÔºàGRPOÔºâËÆ≠ÁªÉ‰∏≠‰∫ßÁîüÁöÑ**Â•ñÂä±‰ø°Âè∑**ÔºåÂå∫ÂàÜÈ´òË¥®Èáè‰∏é‰ΩéË¥®ÈáèÁöÑÊé®ÁêÜÈìæ„ÄÇÈÄöËøáÁ±ª‰ººDPOÁöÑÁõÆÊ†áÔºåËÆ≠ÁªÉÂ≠¶ÁîüÊ®°ÂûãÁîüÊàêÁöÑÊΩúÂú®ÂêëÈáèÔºå‰ΩøÂÖ∂Ë¢´‚ÄúË®ÄËØ≠ÂåñÂô®‚ÄùËß£Á†ÅÊó∂ÔºåÊõ¥ÂÄæÂêë‰∫éÂØπÂ∫îÈ´òË¥®ÈáèÁöÑÊé®ÁêÜÊñáÊú¨„ÄÇ\n    - **Âä®‰ΩúÂØπÈΩêÁöÑËßÜËßâËßÑÂàíËí∏È¶èÊçüÂ§±**ÔºöÁõ¥Êé•ÂØπÈΩêÊïôÂ∏àÂíåÂ≠¶ÁîüÊ®°ÂûãÂú®ÂÖ≥ÈîÆËßÜËßâËßÑÂàíË°®ÂæÅÔºàÂ¶Ç`<answer>` tokenÁöÑÈöêËóèÁä∂ÊÄÅÔºâ‰∏äÁöÑË∑ùÁ¶ªÔºåÁ°Æ‰øùÂ≠¶ÁîüÊ®°ÂûãÁªßÊâøÊïôÂ∏àÂØπ**Á©∫Èó¥-Êó∂Èó¥Âä®ÊÄÅ**ÁöÑÁêÜËß£ËÉΩÂäõÔºåËøôÂØπ‰∫éÂÖ∑Ë∫´ÊéßÂà∂Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ\n    - **Âπ∂Ë°åÁ©∫Èó¥ËΩ®ËøπÈ¢ÑÊµã**Ôºö‰ΩøÁî®‰∏ÄÁªÑÂèØÂ≠¶‰π†ÁöÑÁ©∫Èó¥tokenÔºåÂπ∂Ë°åÈ¢ÑÊµãÊú™Êù•ËΩ®ËøπÁöÑÂ§ö‰∏™Ë∑ØÂæÑÁÇπÔºåÊõø‰ª£‰∫ÜÊïôÂ∏àÊ®°ÂûãËá™ÂõûÂΩíÁîüÊàêÊñáÊú¨Ë∑ØÂæÑÁÇπÁöÑ‰ΩéÊïàÊñπÂºè„ÄÇ\n\n3.  **ÂàõÊñ∞Êû∂ÊûÑÔºöÊé®ÁêÜÂ¢ûÂº∫ÁöÑÁ≠ñÁï•Â≠¶‰π†**\n    - ËÆ≠ÁªÉÂÆåÊàêÂêéÔºå**ÂÜªÁªì**Â∑≤Â≠¶‰ºöÊΩúÂú®Êé®ÁêÜÁöÑÂ≠¶ÁîüVLM„ÄÇ\n    - Â∞ÜÂÖ∂ÁîüÊàêÁöÑ**ËßÜËßâÊΩúÂú®ËßÑÂàí**Ôºà‰ªéÁ©∫Èó¥tokenÁöÑKey-ValueÁºìÂ≠ò‰∏≠ÊèêÂèñÔºâ‰Ωú‰∏∫**È´òÁ∫ßÊù°‰ª∂**ÔºåËæìÂÖ•Âà∞Êâ©Êï£TransformerÁ≠âÂä®‰ΩúÊ®°Âûã‰∏≠ÔºåÊåáÂØº‰ΩéÂ±ÇÂä®‰ΩúÁöÑÁîüÊàê„ÄÇ\n    - ËøôÂÆûÁé∞‰∫Ü**È´òÂ±ÇÁ¥ßÂáëËßÑÂàí**‰∏é**Â∫ïÂ±ÇÂä®‰ΩúÊâßË°å**ÁöÑÈ´òÊïà„ÄÅÁ¥ßÂØÜËÄ¶Âêà„ÄÇ\n\n### **‰∏â„ÄÅ Ëß£ÂÜ≥ÊñπÊ°àÊÄªÁªì**\n**Â¶Ç‰ΩïËß£ÂÜ≥ÊïàÁéáÈóÆÈ¢òÔºü**\n1.  **Ë°®Á§∫ÂéãÁº©**ÔºöÂ∞ÜÊï∞Áôæ‰∏™ÊñáÊú¨tokenÁöÑÊé®ÁêÜÈìæÂéãÁº©‰∏∫Â∞ëÊï∞Âá†‰∏™ËøûÁª≠ÊΩúÂú®ÂêëÈáèÂíåÂπ∂Ë°åÁ©∫Èó¥tokenÔºåÊûÅÂ§ßÂáèÂ∞ë‰∫ÜËá™ÂõûÂΩíÁîüÊàêÁöÑÊ≠•Êï∞„ÄÇ\n2.  **Âπ∂Ë°åÈ¢ÑÊµã**ÔºöÂØπËßÜËßâËΩ®ËøπË∑ØÂæÑÁÇπËøõË°åÂπ∂Ë°åÈ¢ÑÊµãÔºåÈÅøÂÖç‰∫ÜÈÄêÁÇπÁîüÊàêÁöÑÂ∫èÂàóÂª∂Ëøü„ÄÇ\n3.  **ËÆ≠ÁªÉ-Êé®ÁêÜËß£ËÄ¶**ÔºöÂ§çÊùÇÁöÑ‚ÄúË®ÄËØ≠ÂåñÂô®‚ÄùÂíåÂÅèÂ•ΩÂ≠¶‰π†**‰ªÖÂú®ËÆ≠ÁªÉÈò∂ÊÆµ‰ΩøÁî®**ÔºåÊé®ÁêÜÊó∂Âè™ÈúÄËΩªÈáèÁöÑÂ≠¶ÁîüVLMÂíåÂä®‰ΩúÊ®°ÂûãÔºåÊµÅÁ®ãÁÆÄÊ¥Å„ÄÇ\n\n**Â¶Ç‰Ωï‰øùËØÅÊÄßËÉΩ‰∏çÈôçÂèçÂçáÔºü**\n1.  **È´òË¥®ÈáèÁü•ËØÜËí∏È¶è**ÔºöÈÄöËøáÂü∫‰∫éÂ•ñÂä±ÁöÑÂÅèÂ•ΩÂ≠¶‰π†ÔºåËí∏È¶è‰∫ÜÊïôÂ∏àÊ®°Âûã‰∏≠ÊúÄÊúâÊïàÁöÑÊé®ÁêÜÊ®°ÂºèÔºåËøáÊª§‰∫ÜÂÜóÈïøÊñáÊú¨‰∏≠ÁöÑÂô™Â£∞ÂíåÊó†ÂÖ≥‰ø°ÊÅØ„ÄÇ\n2.  **Â§öÊ®°ÊÄÅÂØπÈΩê**ÔºöÈÄöËøáËßÜËßâËßÑÂàíËí∏È¶èÊçüÂ§±ÔºåÊòæÂºèÂú∞‰øùÁïô‰∫ÜÂØπ‰∫éÂÖ∑Ë∫´ÊéßÂà∂Ëá≥ÂÖ≥ÈáçË¶ÅÁöÑ**Á©∫Èó¥ËßÜËßâ‰ø°ÊÅØ**ÔºåËøôÊòØÁ∫ØÊñáÊú¨ÂéãÁº©ÂÆπÊòì‰∏¢Â§±ÁöÑ„ÄÇ\n3.  **È´òÊïàË°®ÂæÅÂà©Áî®**ÔºöÁ¥ßÂáëÁöÑÊΩúÂú®Ë°®ÂæÅ‰Ωú‰∏∫Âº∫Êù°‰ª∂‰ø°Âè∑ÔºåÊõ¥Áõ¥Êé•„ÄÅÈ´òÊïàÂú∞ÂºïÂØºÂä®‰ΩúÊ®°ÂûãÔºåÂèØËÉΩÊØîÂÜóÈïøÊñáÊú¨ÁöÑËØ≠‰πâ‰ø°ÊÅØÊõ¥Êòì‰∫éÊ®°ÂûãÂà©Áî®„ÄÇ\n\n### **Âõõ„ÄÅ ÂÆûÈôÖ‰ª∑ÂÄº‰∏éÊäÄÊúØË¥°ÁåÆ**\n- **ÊÄßËÉΩ**ÔºöÂú®LIBERO„ÄÅSimplerEnv„ÄÅRoboTwin2.0Á≠âÂ§ö‰∏™Êú∫Âô®‰∫∫Êìç‰ΩúÂíåÂÖ∑Ë∫´Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ôºå**ÊÄßËÉΩË∂ÖË∂ä**ÂåÖÊã¨ThinkActÂú®ÂÜÖÁöÑÊúÄÂÖàËøõÊé®ÁêÜVLA„ÄÇ\n- **ÊïàÁéá**ÔºöÂÆûÁé∞‰∫Ü**È´òËææ89.3%ÁöÑÊé®ÁêÜÂª∂ËøüÈôç‰Ωé**ÔºàÁõ∏ÊØîThinkAct-7BÔºâÔºåÊèêÈÄüËææ**9.3ÂÄç**ÔºåÂêåÊó∂Ê®°ÂûãÂèÇÊï∞ÈáèÊõ¥Â∞èÔºà3B vs 7BÔºâ„ÄÇ\n- **ËÉΩÂäõ**Ôºö‰øùÊåÅ‰∫ÜÂº∫Â§ßÁöÑ**ÈïøÊó∂Á®ãËßÑÂàí**„ÄÅ**Â∞èÊ†∑Êú¨ÈÄÇÂ∫î**Âíå**ÊïÖÈöúÊÅ¢Â§ç**ËÉΩÂäõÔºåËØÅÊòé‰∫ÜÁ¥ßÂáëÊé®ÁêÜÁöÑÊúâÊïàÊÄß„ÄÇ\n- **ÂêØÂèëÊÄß**Ôºö‰∏∫**ÂÆûÊó∂ÂÖ∑Ë∫´AI**Á≥ªÁªüÊèê‰æõ‰∫Ü‰∏ÄÊù°ÂèØË°åÁöÑÊäÄÊúØË∑ØÂæÑÔºåÂ±ïÁ§∫‰∫ÜÈÄöËøá‚ÄúÊΩúÂú®ËßÑÂàí‚ÄùÊù•Âπ≥Ë°°Êé®ÁêÜÊ∑±Â∫¶‰∏éËÆ°ÁÆóÊïàÁéáÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇÂÖ∂‚ÄúÊïôÂ∏à-Â≠¶Áîü‚ÄùËí∏È¶èÊ°ÜÊû∂Âíå‚ÄúÂèØË®ÄËØ≠ÂåñÊΩúÂú®Á©∫Èó¥‚ÄùÁöÑËÆæËÆ°ÔºåÂØπÂ§öÊ®°ÊÄÅÈ´òÊïàÊé®ÁêÜÈ¢ÜÂüüÂÖ∑ÊúâÊôÆÈÄÇÁöÑÂèÇËÄÉÊÑè‰πâ„ÄÇ\n\n```mermaid\ngraph TD\n    A[ÈóÆÈ¢ò: Êé®ÁêÜVLAÂª∂ËøüÈ´ò] --> B[Ê†∏ÂøÉÂàõÊñ∞: ÂèØË®ÄËØ≠ÂåñÊΩúÂú®ËßÑÂàí];\n    B --> C{ÂÖ≥ÈîÆÊñπÊ≥ï};\n    C --> D[ÂÅèÂ•ΩÂºïÂØºËí∏È¶è];\n    C --> E[ËßÜËßâËΩ®ËøπÂØπÈΩê];\n    C --> F[Âπ∂Ë°åÁ©∫Èó¥È¢ÑÊµã];\n    D & E & F --> G[ËÆ≠ÁªÉÂá∫È´òÊïàÂ≠¶ÁîüVLM];\n    G --> H[Êé®ÁêÜÂ¢ûÂº∫Á≠ñÁï•Â≠¶‰π†];\n    H --> I[ÁªìÊûú: È´òÊÄßËÉΩ + ÊûÅ‰ΩéÂª∂Ëøü];\n```",
      "timestamp": "2026-01-20T17:57:49.643942",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "Âü∫‰∫é‰ªñÁöÑÂâç‰ΩúÔºåÊ¢≥ÁêÜËøô‰∏™ÊñπÂêëÁöÑÊï¥‰∏™ÂèëÂ±ïËÑâÁªúÔºåÊØè‰∏ÄÊ≠•Áõ∏ÊØî‰∫é‰πãÂâçÁöÑÂ∑•‰ΩúÈÉΩÊîπËøõ‰∫Ü‰ªÄ‰πàÔºåÁùÄÈáç‰∫éÂá†‰∏™‰∏çÂêåÁöÑÂèëÂ±ïÊñπÂêë„ÄÇ",
      "answer": "## ËÆ∫ÊñáÂàÜÊûêÔºöFast-ThinkAct ÂèäÂÖ∂ÊäÄÊúØËÑâÁªúÊ¢≥ÁêÜ\n\nÂü∫‰∫éËÆ∫ÊñáÂÜÖÂÆπÔºåFast-ThinkAct ÊòØ Vision-Language-Action (VLA) Êé®ÁêÜÊ®°ÂûãÂèëÂ±ïËÑâÁªú‰∏≠ÁöÑ‰∏Ä‰∏™ÊúÄÊñ∞Â∑•‰Ωú„ÄÇÂÖ∂Ê†∏ÂøÉË¥°ÁåÆÂú®‰∫é**ÈÄöËøáÂèØË®ÄËØ≠ÂåñÁöÑÊΩúÂú®ËßÑÂàíÔºåÂú®‰øùÊåÅÈ´òÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÂ§ßÂπÖÈôç‰Ωé‰∫ÜÊé®ÁêÜÂª∂Ëøü**„ÄÇ‰ª•‰∏ãÂ∞ÜÊ¢≥ÁêÜËØ•ÊñπÂêëÁöÑÂèëÂ±ïËÑâÁªúÔºåÈáçÁÇπÂàÜÊûêÊØè‰∏ÄÊ≠•ÁöÑÊîπËøõ‰ª•ÂèäÂá†‰∏™‰∏çÂêåÁöÑÂèëÂ±ïÊñπÂêë„ÄÇ\n\n### ÂèëÂ±ïËÑâÁªúÊÄªËßà\n\nËØ•È¢ÜÂüüÁöÑÂèëÂ±ïÂèØ‰ª•Ê¶ÇÊã¨‰∏∫‰∏â‰∏™Èò∂ÊÆµÔºö\n1.  **Âü∫Á°Ä VLA Ê®°Âûã**ÔºöÈÄöËøáÊ®°‰ªøÂ≠¶‰π†‰ªéÂ§ßËßÑÊ®°Êú∫Âô®‰∫∫ÊºîÁ§∫Êï∞ÊçÆ‰∏≠Â≠¶‰π†ÊäÄËÉΩÔºå‰ΩÜÊ≥õÂåñËÉΩÂäõÊúâÈôê„ÄÇ\n2.  **ÊòæÂºèÊé®ÁêÜ VLA**ÔºöÂºïÂÖ•ÈìæÂºèÊÄùÁª¥Á≠âÊòæÂºèÊé®ÁêÜÊú∫Âà∂ÔºåÊèêÂçáÊ≥õÂåñ‰∏éËßÑÂàíËÉΩÂäõÔºå‰ΩÜÂ∏¶Êù•È´òÊòÇÁöÑÊé®ÁêÜÂª∂Ëøü„ÄÇ\n3.  **È´òÊïàÊé®ÁêÜ VLA**ÔºöÊó®Âú®ÂéãÁº©Êàñ‰ºòÂåñÊé®ÁêÜËøáÁ®ãÔºåÂú®‰øùÊåÅÊÄßËÉΩÁöÑÂêåÊó∂ÂÆûÁé∞ÂÆûÊó∂ÊàñËøëÂÆûÊó∂ÂÜ≥Á≠ñ„ÄÇ**Fast-ThinkAct Â±û‰∫éÊ≠§Èò∂ÊÆµÁöÑÊúÄÊñ∞Â∑•‰Ωú**„ÄÇ\n\n---\n\n### Á¨¨‰∏ÄÈò∂ÊÆµÔºöÂü∫Á°Ä VLA Ê®°Âûã (Ê®°‰ªøÂ≠¶‰π†)\n\n*   **‰ª£Ë°®Â∑•‰Ωú**ÔºöRT-1/2, Octo, OpenVLA, œÄ‚ÇÄ, Magma Á≠â„ÄÇ\n*   **Ê†∏ÂøÉÊÄùÊÉ≥**ÔºöÂ∞ÜËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã (VLM) Âú®Â§ßËßÑÊ®°Êú∫Âô®‰∫∫Âä®‰ΩúÊï∞ÊçÆÈõÜ‰∏äËøõË°åÁ´ØÂà∞Á´ØÁöÑÁõëÁù£ÂæÆË∞ÉÔºåÂ≠¶‰π†ËØ≠Ë®ÄÊù°‰ª∂ÂåñÁöÑÁ≠ñÁï•„ÄÇ\n*   **ÊîπËøõ‰∏é‰ª∑ÂÄº**Ôºö\n    *   **ËØÅÊòé‰∫ÜÊï∞ÊçÆÈ©±Âä®ÁöÑÂèØË°åÊÄß**ÔºöÂ±ïÁ§∫‰∫ÜÂà©Áî®‰∫íËÅîÁΩëËßÑÊ®°ÁöÑÊú∫Âô®‰∫∫Êï∞ÊçÆËÆ≠ÁªÉÈÄöÁî®Á≠ñÁï•ÁöÑÊΩúÂäõ„ÄÇ\n    *   **ÂÆûÁé∞‰∫ÜÂü∫Á°ÄÊäÄËÉΩ**ÔºöÂú®ËÆ≠ÁªÉÂàÜÂ∏ÉÂÜÖÁöÑÊãæÂèñ„ÄÅÊîæÁΩÆÁ≠â‰ªªÂä°‰∏äË°®Áé∞ËâØÂ•Ω„ÄÇ\n*   **Â±ÄÈôêÊÄß**Ôºö\n    *   **Ê≥õÂåñËÉΩÂäõÂ∑Æ**Ôºö‰∏•ÈáçÂèóÈôê‰∫éËÆ≠ÁªÉÊï∞ÊçÆÂàÜÂ∏ÉÔºåÈöæ‰ª•Â∫îÂØπÈïøËßÜÈáéËßÑÂàí„ÄÅÊñ∞Âú∫ÊôØÈÄÇÂ∫î„ÄÅ‰ªéÂ§±Ë¥•‰∏≠ÊÅ¢Â§çÁ≠â‰ªªÂä°„ÄÇ\n    *   **Áº∫‰πèÊòæÂºèÊé®ÁêÜ**ÔºöÊ®°ÂûãÊòØ‚ÄúÈªëÁÆ±‚ÄùÔºåÂÜ≥Á≠ñËøáÁ®ã‰∏çÈÄèÊòéÔºåÈöæ‰ª•ËøõË°åÂ§çÊùÇÁöÑÈóÆÈ¢òÂàÜËß£ÂíåÈÄªËæëËßÑÂàí„ÄÇ\n\n### Á¨¨‰∫åÈò∂ÊÆµÔºöÊòæÂºèÊé®ÁêÜ VLA (ÊèêÂçáÊ≥õÂåñ)\n\n‰∏∫‰∫ÜÂÖãÊúçÂü∫Á°Ä VLA ÁöÑÂ±ÄÈôêÊÄßÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÂºïÂÖ•‰∫Ü**ÊòæÂºèÁöÑ‰∏≠Èó¥Êé®ÁêÜËøáÁ®ã**„ÄÇËøô‰∏™ÊñπÂêëÂèàÂèØÂàÜ‰∏∫Âá†‰∏™Â≠êÊñπÂêëÔºö\n\n#### ÂèëÂ±ïÊñπÂêë 1: Âü∫‰∫éÁõëÁù£ÁöÑÈìæÂºèÊÄùÁª¥ (Supervised CoT)\n*   **‰ª£Ë°®Â∑•‰Ωú**ÔºöEmbodied CoT, CoT-VLA, MolmoAct, EO-1„ÄÇ\n*   **Ê†∏ÂøÉÊÄùÊÉ≥**ÔºöÂà©Áî®Ê†áÊ≥®ÁöÑÊàñÁî±Â§ßÊ®°ÂûãÁîüÊàêÁöÑ‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÔºàÊñáÊú¨ÊàñËßÜËßâÂΩ¢ÂºèÔºâ‰Ωú‰∏∫ÁõëÁù£‰ø°Âè∑ÔºåËÆ≠ÁªÉÊ®°ÂûãÂÖà‚ÄúÊÄùËÄÉ‚ÄùÂÜç‚ÄúË°åÂä®‚Äù„ÄÇ\n*   **ÊäÄÊúØÊîπËøõ**Ôºö\n    *   **ÊñáÊú¨Êé®ÁêÜ**Ôºö‰ΩøÁî®Áé∞ÊàêÁöÑ LLM/VLM ÁîüÊàê‰º™ CoT Ê†áÁ≠æÔºåÂºïÂØºÊ®°ÂûãËøõË°åËØ≠Ë®ÄÊé®ÁêÜ„ÄÇ\n    *   **ËßÜËßâÊé®ÁêÜ**ÔºöÁîüÊàêÁªìÊûÑÂåñÁöÑËßÜËßâË°®Á§∫ÔºàÂ¶ÇÂ≠êÁõÆÊ†áÂõæÂÉè„ÄÅÊ∑±Â∫¶Âõæ„ÄÅ2D ËßÜËßâËΩ®ËøπÔºâ‰Ωú‰∏∫‰∏≠Èó¥ËßÑÂàíÔºåÊõ¥Â•ΩÂú∞ËøûÊé•Á©∫Èó¥ÁêÜËß£‰∏éÂä®‰Ωú„ÄÇ\n    *   **‰ª∑ÂÄº**ÔºöÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÂú®ÈïøËßÜÈáé‰ªªÂä°„ÄÅÂ∞ëÊ†∑Êú¨ÈÄÇÂ∫îÂíåÁªÑÂêàÊ≥õÂåñ‰∏äÁöÑËÉΩÂäõÔºå‰ΩøÂÜ≥Á≠ñËøáÁ®ãÊõ¥ÂèØËß£Èáä„ÄÇ\n*   **Â±ÄÈôêÊÄß**Ôºö\n    *   **‰æùËµñÊ†áÊ≥®**ÔºöÈúÄË¶ÅÂ§ßÈáèÈ´òË¥®ÈáèÁöÑÊé®ÁêÜÊ†áÊ≥®ÔºåÊï∞ÊçÆËé∑ÂèñÊàêÊú¨È´ò„ÄÇ\n    *   **Ë¶ÜÁõñÊúâÈôê**ÔºöÊÄßËÉΩÂèóÈôê‰∫éÁõëÁù£Êï∞ÊçÆÁöÑË¶ÜÁõñËåÉÂõ¥„ÄÇ\n    *   **Êé®ÁêÜÂª∂ËøüÈ´ò**ÔºöÁîüÊàêÂÜóÈïøÁöÑÊñáÊú¨ÊàñËßÜËßâÊé®ÁêÜÈìæÔºà~250‰∏™tokenÔºâÂØºËá¥Êé®ÁêÜÈÄüÂ∫¶ÊÖ¢ÔºàÁßíÁ∫ßÔºâÔºåÊó†Ê≥ïÊª°Ë∂≥ÂÆûÊó∂Êú∫Âô®‰∫∫ÊéßÂà∂ÁöÑÈúÄÊ±Ç„ÄÇ\n\n#### ÂèëÂ±ïÊñπÂêë 2: Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÊé®ÁêÜ (RL-based Reasoning)\n*   **‰ª£Ë°®Â∑•‰Ωú**Ôºö**ThinkAct** (Fast-ThinkAct ÁöÑÂâç‰Ωú)„ÄÇ\n*   **Ê†∏ÂøÉÊÄùÊÉ≥**Ôºö‰∏ç‰æùËµñ‰∫∫Â∑•Ê†áÊ≥®ÔºåËÄåÊòØ‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π† (RL)ÔºåÈÄöËøáËÆæËÆ°**‰∏éÂä®‰ΩúÂØπÈΩêÁöÑËßÜËßâÂ•ñÂä±**Êù•ÂºïÂØºÊ®°ÂûãÁîüÊàêÊúâÁõäÁöÑÈìæÂºèÊÄùÁª¥„ÄÇ\n*   **ÊäÄÊúØÊîπËøõ**Ôºö\n    *   **ÂÖç‰∫éÊ†áÊ≥®**ÔºöÂà©Áî® RLÔºàÂ¶Ç GRPOÔºâËá™Âä®Êé¢Á¥¢Âíå‰ºòÂåñÊé®ÁêÜË∑ØÂæÑÔºåÊëÜËÑ±‰∫ÜÂØπÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÁöÑ‰æùËµñ„ÄÇ\n    *   **Â•ñÂä±È©±Âä®**ÔºöÊé®ÁêÜË¥®ÈáèÁõ¥Êé•Áî±‰ªªÂä°ÂÆåÊàêÂ∫¶„ÄÅËΩ®ËøπÂØπÈΩêÁ≠â‰∏ãÊ∏∏ÊåáÊ†áË°°ÈáèÂíå‰ºòÂåñÔºåÁ°Æ‰øù‰∫ÜÊé®ÁêÜÂØπÊúÄÁªàÂä®‰ΩúÁöÑÊúâÊïàÊÄß„ÄÇ\n*   **Â±ÄÈôêÊÄß**Ôºö\n    *   **Âª∂ËøüÈóÆÈ¢òÂä†Ââß**ÔºöRL ËÆ≠ÁªÉÂÄæÂêë‰∫éÁîüÊàêÊõ¥Èïø„ÄÅÊõ¥ËØ¶ÁªÜÁöÑÊé®ÁêÜÈìæ‰ª•ÊúÄÂ§ßÂåñÂ•ñÂä±Ôºå‰ΩøÂæóÊé®ÁêÜÂª∂ËøüÈóÆÈ¢òÊØîÁõëÁù£ÊñπÊ≥ïÊõ¥Á™ÅÂá∫„ÄÇ\n    *   **ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆö**ÔºöRL ËÆ≠ÁªÉÊú¨Ë∫´ÂÖ∑ÊúâÊåëÊàòÊÄßÔºåÂèØËÉΩ‰∏çÁ®≥ÂÆö„ÄÇ\n\n### Á¨¨‰∏âÈò∂ÊÆµÔºöÈ´òÊïàÊé®ÁêÜ VLA (ÂéãÁº©‰∏éÂä†ÈÄü)\n\nÈíàÂØπÊòæÂºèÊé®ÁêÜ VLA ÁöÑÈ´òÂª∂ËøüÁì∂È¢àÔºåËøëÊúüÂ∑•‰ΩúÂºÄÂßãÊé¢Á¥¢Â¶Ç‰Ωï**ÂéãÁº©Êé®ÁêÜËøáÁ®ã**„ÄÇFast-ThinkAct Âú®Ê≠§ËÉåÊôØ‰∏ãÊèêÂá∫ÔºåÂπ∂ÈíàÂØπÂá†‰∏™‰∏çÂêåÁöÑÈ´òÊïàÂåñÊñπÂêëÂÅöÂá∫‰∫ÜÂàõÊñ∞„ÄÇ\n\n#### ÂèëÂ±ïÊñπÂêë 1: Êé®ÁêÜÈïøÂ∫¶ÊÉ©ÁΩö/‰∏¢ÂºÉ\n*   **‰ª£Ë°®Â∑•‰Ωú**ÔºöECoT-Lite, ‰ª•Âèä‰∏Ä‰∫õ LLM ‰∏≠ÁöÑ RL ÈïøÂ∫¶ÊÉ©ÁΩöÊñπÊ≥ï„ÄÇ\n*   **Ê†∏ÂøÉÊÄùÊÉ≥**ÔºöÂú®ËÆ≠ÁªÉÊàñÊé®ÁêÜÊó∂ÔºåÂº∫Âà∂ÊàñÈºìÂä±Ê®°ÂûãÁîüÊàêÊõ¥Áü≠ÁöÑÊé®ÁêÜÈìæ„ÄÇ‰æãÂ¶ÇÔºåECoT-Lite Âú®ÊµãËØïÊó∂ÈöèÊú∫‚Äú‰∏¢ÂºÉ‚ÄùÈÉ®ÂàÜÊé®ÁêÜÊ≠•È™§„ÄÇ\n*   **ÊäÄÊúØÊîπËøõ**ÔºöÁõ¥Êé•ÂáèÂ∞ë‰∫ÜÈúÄË¶ÅÁîüÊàêÁöÑ token Êï∞ÈáèÔºå‰ªéËÄåÈôç‰ΩéÂª∂Ëøü„ÄÇ\n*   **Â±ÄÈôêÊÄß**Ôºö\n    *   **ÊÄßËÉΩ‰∏ãÈôçÈ£éÈô©**ÔºöÁÆÄÂçïÁº©Áü≠Êàñ‰∏¢ÂºÉÂèØËÉΩÊçüÂ§±ÂÖ≥ÈîÆÊé®ÁêÜ‰ø°ÊÅØÔºåÂØºËá¥ËßÑÂàí‰∏ç‰∏ÄËá¥ÊàñÊÄßËÉΩ‰∏ãÈôç„ÄÇ\n    *   **ÈùûÊú¨Ë¥®ÂéãÁº©**ÔºöÊú™ÊîπÂèòÊé®ÁêÜÁöÑË°®Á§∫ÂΩ¢ÂºèÔºåÊïàÁéáÊèêÂçáÊúâÈôê„ÄÇ\n\n#### ÂèëÂ±ïÊñπÂêë 2: ÊΩúÂú®Á©∫Èó¥Êé®ÁêÜ (Latent Reasoning) - **Fast-ThinkAct ÁöÑÊ†∏ÂøÉË¥°ÁåÆ**\n*   **‰ª£Ë°®Â∑•‰Ωú**Ôºö**Fast-ThinkAct**Ôºå‰ª•Âèä LLM È¢ÜÂüüÁöÑ Coconut, CODI Á≠â„ÄÇ\n*   **Ê†∏ÂøÉÊÄùÊÉ≥**Ôºö**Â∞ÜÂÜóÈïøÁöÑÁ¶ªÊï£ÊñáÊú¨Êé®ÁêÜÔºåËí∏È¶è/ÂéãÁº©ÊàêÁ¥ßÂáëÁöÑËøûÁª≠ÊΩúÂú®ÂêëÈáè**„ÄÇÊ®°ÂûãÂú®ÊΩúÂú®Á©∫Èó¥ËøõË°å‚ÄúÊÄùËÄÉ‚ÄùÔºåËøô‰∫õÊΩúÂú®ÂêëÈáèËï¥Âê´‰∫ÜËßÑÂàí‰ø°ÊÅØÔºåÂπ∂ËÉΩË¢´‰∏Ä‰∏™‚ÄúË®ÄËØ≠ÂåñÂô®‚ÄùËß£Á†ÅÂõûÊñáÊú¨‰ª•‰æõËß£Èáä„ÄÇ\n*   **ÊäÄÊúØÊîπËøõ (Fast-ThinkAct ÁöÑË¥°ÁåÆ)**Ôºö\n    1.  **ÂèØË®ÄËØ≠ÂåñÁöÑÊΩúÂú®ËßÑÂàí**ÔºöÊèêÂá∫‚ÄúÂ≠¶Áîü-ÊïôÂ∏à‚ÄùËí∏È¶èÊ°ÜÊû∂„ÄÇÊïôÂ∏àÊ®°ÂûãÔºàÂ¶Ç ThinkActÔºâÁîüÊàêÊòæÂºèÊñáÊú¨ CoTÔºåÂ≠¶ÁîüÊ®°ÂûãÂ≠¶‰π†ÁîüÊàêÊûÅÂ∞ëÊï∞ÔºàÂ¶Ç6‰∏™ÔºâËøûÁª≠ÊΩúÂú® token„ÄÇÈÄöËøá‰∏Ä‰∏™ÂèØËÆ≠ÁªÉÁöÑË®ÄËØ≠ÂåñÂô® LLM ÂíåÂü∫‰∫éÂÅèÂ•ΩÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÁ°Æ‰øùËøô‰∫õÊΩúÂú® token ËÉΩÂ§üÂØπÂ∫îÂà∞È´òË¥®ÈáèÁöÑÊé®ÁêÜÊ®°Âºè„ÄÇ\n    2.  **ËßÜËßâËßÑÂàíÂØπÈΩê**ÔºöÈô§‰∫ÜËØ≠Ë®ÄÊé®ÁêÜÔºåËøòÈÄöËøáÂØπÈΩêÊïôÂ∏àÂíåÂ≠¶ÁîüÊ®°ÂûãÁöÑËΩ®ËøπÁ∫ßÈöêËóèÁä∂ÊÄÅÔºåÂ∞Ü**ËßÜËßâËßÑÂàíËÉΩÂäõ**Ëí∏È¶èÂà∞ÊΩúÂú®Ë°®Á§∫‰∏≠ÔºåËøôÂØπ‰∫éÂÖ∑Ë∫´ÊéßÂà∂Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ\n    3.  **Âπ∂Ë°åÁ©∫Èó¥ËΩ®ËøπÈ¢ÑÊµã**ÔºöÂºïÂÖ•ÂèØÂ≠¶‰π†ÁöÑÁ©∫Èó¥ tokenÔºåÂπ∂Ë°åÈ¢ÑÊµãÊú™Êù•Ë∑ØÂæÑÁÇπÔºåÊõø‰ª£‰∫ÜÊïôÂ∏àÊ®°ÂûãËá™ÂõûÂΩíÁîüÊàêÊñáÊú¨Ë∑ØÂæÑÁÇπÁöÑ‰ΩéÊïàÊñπÂºè„ÄÇ\n    4.  **Êé®ÁêÜÂ¢ûÂº∫ÁöÑÁ≠ñÁï•Â≠¶‰π†**ÔºöÂ∞ÜËÆ≠ÁªÉÂ•ΩÁöÑÂ≠¶Áîü VLM ÁîüÊàêÁöÑÊΩúÂú®ËßÜËßâËßÑÂàíÔºåÈÄöËøá KV ÁºìÂ≠òÊ≥®ÂÖ•Âà∞Âä®‰ΩúÊ®°Âûã‰∏≠ÔºåÊ°•Êé•‰∫ÜÈ´òÂ±ÇËßÑÂàí‰∏éÂ∫ïÂ±ÇÂä®‰ΩúÊâßË°å„ÄÇ\n*   **‰ª∑ÂÄº‰∏éÊîπËøõ**Ôºö\n    *   **ÊûÅËá¥ÊïàÁéá**ÔºöÂÆûÁé∞‰∫Ü **~89.3% ÁöÑÊé®ÁêÜÂª∂ËøüÈôç‰Ωé**ÔºåÊØîÂÖ∂Ââç‰Ωú ThinkAct-7B Âø´ 9.3 ÂÄç„ÄÇ\n    *   **‰øùÊåÅÁîöËá≥ÊèêÂçáÊÄßËÉΩ**ÔºöÂú®Â§ö‰∏™Êú∫Âô®‰∫∫Êìç‰ΩúÂíåÂÖ∑Ë∫´Êé®ÁêÜÂü∫ÂáÜ‰∏äËææÂà∞ SOTAÔºåËØÅÊòé‰∫ÜÁ¥ßÂáëÊΩúÂú®Ë°®Á§∫ÁöÑÊúâÊïàÊÄß„ÄÇ\n    *   **‰øùÁïôÂÖ≥ÈîÆËÉΩÂäõ**ÔºöÊàêÂäü‰øùÊåÅ‰∫ÜÈïøËßÜÈáéËßÑÂàí„ÄÅÂ∞ëÊ†∑Êú¨ÈÄÇÂ∫îÂíåÂ§±Ë¥•ÊÅ¢Â§çÁ≠âÁî±Êé®ÁêÜÂ∏¶Êù•ÁöÑÊ†∏ÂøÉ‰ºòÂäø„ÄÇ\n*   **Â±ÄÈôêÊÄß**Ôºö\n    *   **Ë®ÄËØ≠ÂåñÂô®ÂèØËÉΩÂπªËßâ**ÔºöÁî®‰∫éËß£ÈáäÊΩúÂú®ÂêëÈáèÁöÑË®ÄËØ≠ÂåñÂô®ÂèØËÉΩ‰∫ßÁîü‰∏çÂáÜÁ°ÆÁöÑÊèèËø∞Ôºå‰ΩÜËøô‰∏çÂΩ±ÂìçÂä®‰ΩúÊâßË°å„ÄÇ\n    *   **Ê°ÜÊû∂Â§çÊùÇÊÄß**ÔºöÊ∂âÂèäÂ§öÈò∂ÊÆµËÆ≠ÁªÉÔºàÊïôÂ∏à RL ËÆ≠ÁªÉ„ÄÅÂ≠¶ÁîüËí∏È¶è„ÄÅÁ≠ñÁï•Â≠¶‰π†ÔºâÔºåÊµÅÁ®ãÁõ∏ÂØπÂ§çÊùÇ„ÄÇ\n\n---\n\n### ÊÄªÁªìÔºö‰∏çÂêåÂèëÂ±ïÊñπÂêëÁöÑÂØπÊØî\n\n| ÂèëÂ±ïÊñπÂêë | Ê†∏ÂøÉÊÄùË∑Ø | ‰ºòÁÇπ | Áº∫ÁÇπ | ‰ª£Ë°®Â∑•‰Ωú |\n| :--- | :--- | :--- | :--- | :--- |\n| **ÁõëÁù£ CoT** | Â≠¶‰π†Ê†áÊ≥®ÁöÑÊé®ÁêÜÊ≠•È™§ | ÊèêÂçáÊ≥õÂåñÔºåËøáÁ®ãÂèØËß£Èáä | ‰æùËµñÊ†áÊ≥®ÔºåÂª∂ËøüÈ´ò | CoT-VLA, MolmoAct |\n| **RL CoT** | Áî®Â•ñÂä±‰ºòÂåñÊé®ÁêÜÁîüÊàê | ÂÖçÊ†áÊ≥®ÔºåÊé®ÁêÜ‰∏é‰ªªÂä°Âº∫Áõ∏ÂÖ≥ | Âª∂ËøüÊûÅÈ´òÔºåËÆ≠ÁªÉ‰∏çÁ®≥ÂÆö | **ThinkAct** (Ââç‰Ωú) |\n| **ÈïøÂ∫¶ÊéßÂà∂** | Âº∫Âà∂ÁîüÊàêÊõ¥Áü≠Êé®ÁêÜ | ÁÆÄÂçïÔºåÁõ¥Êé•ÊèêÈÄü | ÊòìÊçüÂ§±‰ø°ÊÅØÔºåÊÄßËÉΩ‰∏ãÈôç | ECoT-Lite |\n| **ÊΩúÂú®Êé®ÁêÜ** | Â∞ÜÊé®ÁêÜÂéãÁº©‰∏∫ËøûÁª≠ÂêëÈáè | **È´òÊïà‰∏éÊÄßËÉΩÂÖºÂæó**Ôºå‰øùÁïôËßÑÂàíËÉΩÂäõ | Ê°ÜÊû∂Â§çÊùÇÔºåËß£Èáä‰æùËµñÈ¢ùÂ§ñÊ®°Âûã | **Fast-ThinkAct** |\n\n**ÁªìËÆ∫**ÔºöFast-ThinkAct Á´ôÂú®ÂÖ∂Ââç‰Ωú ThinkActÔºàRL CoTÔºâÁöÑËÇ©ËÜÄ‰∏äÔºåÊïèÈîêÂú∞Êäì‰Ωè‰∫Ü‚ÄúÊé®ÁêÜÂª∂Ëøü‚ÄùËøô‰∏ÄÂà∂Á∫¶ÂÖ∑Ë∫´ AI ÂÆûÈôÖÂ∫îÁî®ÁöÑÁì∂È¢à„ÄÇÂÆÉÊ≤°ÊúâÈÄâÊã©ÁÆÄÂçïÁöÑ‚ÄúÁº©Áü≠ÊñáÊú¨‚ÄùË∑ØÂæÑÔºåËÄåÊòØÂºÄÂàõÊÄßÂú∞ËΩ¨Âêë‚Äú**ÊΩúÂú®Á©∫Èó¥Êé®ÁêÜ**‚ÄùÔºåÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÁöÑÂÅèÂ•ΩËí∏È¶èÂíåËßÜËßâÂØπÈΩêÊú∫Âà∂ÔºåÊàêÂäüÂú∞Â∞Ü‰∏∞ÂØåÁöÑËØ≠Ë®ÄÂíåËßÜËßâËßÑÂàí‰ø°ÊÅØÂéãÁº©Âà∞ÊûÅÂ∞ëÁöÑÊΩúÂú® token ‰∏≠„ÄÇËøôÈ°πÂ∑•‰ΩúÊ†áÂøóÁùÄ VLA Êé®ÁêÜÊ®°Âûã‰ªé‚ÄúËøΩÊ±ÇËÉΩÂäõ‚ÄùÂà∞‚Äú**ÂÖºÈ°æËÉΩÂäõ‰∏éÊïàÁéá**‚ÄùÁöÑÈáçË¶ÅËΩ¨ÂèòÔºå‰∏∫ÂÆûÊó∂„ÄÅÈ´òÊïàÁöÑÂÖ∑Ë∫´Êô∫ËÉΩÁ≥ªÁªüÊèê‰æõ‰∫ÜÂèØË°åÁöÑÊäÄÊúØÊñπÊ°à„ÄÇÊú™Êù•ÁöÑÂ∑•‰ΩúÂèØËÉΩ‰ºöËøõ‰∏ÄÊ≠•Êé¢Á¥¢Êõ¥ÁÆÄÂçïÁöÑÂçïÈò∂ÊÆµËÆ≠ÁªÉ„ÄÅÊõ¥È≤ÅÊ£íÁöÑÊΩúÂú®Ë°®Á§∫Â≠¶‰π†Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÂΩªÂ∫ïÊ∂àÈô§ÂØπÁã¨Á´ãË®ÄËØ≠ÂåñÂô®ÁöÑ‰æùËµñ„ÄÇ",
      "timestamp": "2026-01-20T17:58:55.371186",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "‰ªñÁöÑÂâç‰ΩúÊúâÂì™‰∫õÔºü‰ΩøÁî®Ë°®Ê†º‰ªîÁªÜËÆ≤ËÆ≤‰ªñÁöÑÊØèÁØáÂâç‰ΩúÔºå‰ªñÂíåÂâç‰ΩúÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºå‰∏ªË¶ÅÊîπÂñÑÊòØ‰ªÄ‰πàÔºüÁùÄÈáç‰∫éÂÖ∑‰ΩìÁõ∏ÊØî‰∫é‰πãÂâçÊñáÁ´†ÁöÑÊîπÂä®",
      "answer": "## Fast-ThinkAct ÁöÑÂâç‰ΩúÂàÜÊûê\n\nÊ†πÊçÆËÆ∫ÊñáÂÜÖÂÆπÔºåFast-ThinkAct ÁöÑÊ†∏ÂøÉÂâç‰ΩúÊòØ **ThinkAct** (Huang et al., 2025)„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáÂú®‚ÄúÁõ∏ÂÖ≥Â∑•‰Ωú‚ÄùÈÉ®ÂàÜËØ¶ÁªÜÂõûÈ°æ‰∫ÜÂÖ∂‰ªñÂá†Á±ªÁõ∏ÂÖ≥Á†îÁ©∂ÔºåÂåÖÊã¨Âü∫Á°ÄVLAÊ®°ÂûãÂíåÊé®ÁêÜVLAÊ®°Âûã„ÄÇFast-ThinkAct ‰∏ªË¶ÅÊòØÂú® ThinkAct ÁöÑÂü∫Á°Ä‰∏äËøõË°åÊîπËøõÔºåÊó®Âú®Ëß£ÂÜ≥ÂÖ∂Êé®ÁêÜÂª∂ËøüÈ´òÁöÑÈóÆÈ¢ò„ÄÇ\n\n‰ª•‰∏ãË°®Ê†ºËØ¶ÁªÜÂàóÂá∫‰∫ÜÂÖ∂Ê†∏ÂøÉÂâç‰ΩúÔºàThinkActÔºâÂèäÂÖ∂‰ªñÂÖ≥ÈîÆÁõ∏ÂÖ≥Â∑•‰ΩúÁöÑÂØπÊØîÔºåÂπ∂ÈáçÁÇπÈòêËø∞‰∫Ü Fast-ThinkAct ÁöÑÂÖ∑‰ΩìÊîπËøõ„ÄÇ\n\n| Ââç‰Ωú/Áõ∏ÂÖ≥Â∑•‰Ωú | Ê†∏ÂøÉÊñπÊ≥ï | ‰∏ªË¶ÅË¥°ÁåÆ/ÁâπÁÇπ | ‰∏é Fast-ThinkAct ÁöÑÂå∫Âà´Âèä‰∏ªË¶ÅÊîπÂñÑ |\n| :--- | :--- | :--- | :--- |\n| **ThinkAct** (Huang et al., 2025) | **Âü∫‰∫éRLÁöÑÊòæÂºèÊñáÊú¨Êé®ÁêÜ**„ÄÇ‰ΩøÁî®GRPOÔºàGroup Relative Policy OptimizationÔºâËÆ≠ÁªÉÔºåÁîüÊàêÂÜóÈïøÁöÑÊñáÊú¨ÈìæÂºèÊé®ÁêÜÔºàCoTÔºâËΩ®ËøπÔºå‰ª•ËßÜËßâÂ•ñÂä±ÔºàÂ¶ÇÁõÆÊ†áÂÆåÊàêÂ∫¶„ÄÅËΩ®ËøπÂØπÈΩêÔºâ‰∏∫ÊåáÂØº„ÄÇ | È¶ñÊ¨°Â∞ÜÂº∫ÂåñÂ≠¶‰π†‰∏éÊé®ÁêÜÁõ∏ÁªìÂêàÁî®‰∫éVLA‰ªªÂä°ÔºåÊòæËëóÊèêÂçá‰∫ÜÈïøËßÜÈáéËßÑÂàí„ÄÅÊ≥õÂåñËÉΩÂäõÂíåÂ§±Ë¥•ÊÅ¢Â§çËÉΩÂäõ„ÄÇ | **Ê†∏ÂøÉÂå∫Âà´ÔºöÊé®ÁêÜÂΩ¢Âºè‰∏éÊïàÁéá**„ÄÇ<br>1. **Êé®ÁêÜÂΩ¢Âºè**ÔºöThinkAct ÁîüÊàê**ÊòæÂºè„ÄÅÂÜóÈïøÁöÑÊñáÊú¨CoT**ÔºàÁ∫¶250‰∏™tokenÔºâÔºåËÄå Fast-ThinkAct Â≠¶‰π†**Á¥ßÂáë„ÄÅËøûÁª≠ÁöÑÊΩúÂú®Ë°®Á§∫**ÔºàÂ¶Ç6‰∏™ÊΩúÂú®tokenÔºâËøõË°åÂÜÖÈÉ®Êé®ÁêÜ„ÄÇ<br>2. **ÊïàÁéáÊèêÂçá**ÔºöFast-ThinkAct ÈÄöËøá**ÂèØË®ÄËØ≠ÂåñÁöÑÊΩúÂú®ËßÑÂàí**ÔºåÂÆûÁé∞‰∫ÜÈ´òËææ **89.3% ÁöÑÊé®ÁêÜÂª∂ËøüÈôç‰Ωé**ÔºàÁõ∏ÊØîThinkAct-7BÔºâÔºåÂêåÊó∂ÊÄßËÉΩËøòÊúâÊâÄÊèêÂçá„ÄÇ<br>3. **ËÆ≠ÁªÉÊú∫Âà∂**ÔºöThinkAct ‰æùËµñRLÁõ¥Êé•‰ºòÂåñÊñáÊú¨ÁîüÊàêÔºõFast-ThinkAct ÈááÁî®**ÂÅèÂ•ΩÂºïÂØºÁöÑËí∏È¶è**Ôºå‰ªé‰∏Ä‰∏™ÁªèËøáRLËÆ≠ÁªÉÁöÑÊïôÂ∏àÊ®°ÂûãÔºàÂç≥ThinkActÈ£éÊ†ºÁöÑÊ®°ÂûãÔºâ‰∏≠ÔºåÂ∞ÜÈ´òË¥®ÈáèÊé®ÁêÜÊ®°ÂºèÊèêÁÇºÂà∞Â≠¶ÁîüÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠„ÄÇ |\n| **ÁõëÁù£ÂºèÊé®ÁêÜVLA** (Â¶Ç Embodied CoT, CoT-VLA, MolmoAct) | **Âü∫‰∫éÊ†áÊ≥®ÁöÑÊòæÂºèÊé®ÁêÜ**„ÄÇ‰ΩøÁî®Â∏¶Êúâ‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÔºàÊñáÊú¨ÊàñËßÜËßâÂ≠êÁõÆÊ†á/ËΩ®ËøπÔºâÁöÑÊ†áÊ≥®Êï∞ÊçÆËøõË°åÁõëÁù£ËÆ≠ÁªÉ„ÄÇ | ÈÄöËøáÂºïÂÖ•ÊòéÁ°ÆÁöÑÊé®ÁêÜËøáÁ®ãÔºåÊîπÂñÑ‰∫ÜÂü∫Á°ÄVLAÊ®°ÂûãÂú®ËßÑÂàí„ÄÅÊ≥õÂåñÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇ | **Ê†∏ÂøÉÂå∫Âà´ÔºöÂØπÊ†áÊ≥®Êï∞ÊçÆÁöÑ‰æùËµñ‰∏éÊé®ÁêÜÊïàÁéá**„ÄÇ<br>1. **Êï∞ÊçÆÈúÄÊ±Ç**ÔºöÁõëÁù£ÊñπÊ≥ïÈúÄË¶ÅÂ§ßÈáè‰∫∫Â∑•ÊàñÊ®°ÂûãÁîüÊàêÁöÑÊé®ÁêÜÊ†áÊ≥®ÔºåÊàêÊú¨È´ò‰∏îÂèóÈôê‰∫éÊï∞ÊçÆË¶ÜÁõñËåÉÂõ¥„ÄÇFast-ThinkAct ÈÄöËøá**‰ªéRLËÆ≠ÁªÉÁöÑÊïôÂ∏àÊ®°Âûã‰∏≠ËøõË°åËí∏È¶è**ÔºåÂáèÂ∞ë‰∫ÜÂØπÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÁöÑÁõ¥Êé•‰æùËµñ„ÄÇ<br>2. **Êé®ÁêÜÊïàÁéá**ÔºöËøô‰∫õÊñπÊ≥ïÂêåÊ†∑ÁîüÊàêÊòæÂºèÊé®ÁêÜÊ≠•È™§ÔºåÂ≠òÂú®Âª∂ËøüÈóÆÈ¢ò„ÄÇFast-ThinkAct ÁöÑ**ÊΩúÂú®Êé®ÁêÜ**‰ªéÊ†πÊú¨‰∏äÂéãÁº©‰∫ÜÊé®ÁêÜËøáÁ®ãÔºåÊïàÁéáÊõ¥È´ò„ÄÇ<br>3. **‰ø°ÊÅØ‰øùÁúüÂ∫¶**ÔºöÁõ¥Êé•ÂáèÂ∞ëÊñáÊú¨Êé®ÁêÜÈïøÂ∫¶ÔºàÂ¶ÇECoT-LiteÁöÑÊé®ÁêÜ‰∏¢ÂºÉÔºâÂèØËÉΩÂØºËá¥ÂÖ≥ÈîÆ‰ø°ÊÅØ‰∏¢Â§±„ÄÇFast-ThinkAct ÈÄöËøá**ËßÜËßâËΩ®ËøπÂØπÈΩêÊçüÂ§±**ÔºåÁ°Æ‰øùÊΩúÂú®Á©∫Èó¥‰øùÁïô‰∫ÜÂØπ‰∫éÂÖ∑Ë∫´ÊéßÂà∂Ëá≥ÂÖ≥ÈáçË¶ÅÁöÑ**Á©∫Èó¥-Êó∂Èó¥Âä®ÊÄÅ‰ø°ÊÅØ**„ÄÇ |\n| **Âü∫Á°ÄVLAÊ®°Âûã** (Â¶Ç RT-2, Octo, OpenVLA) | **Ê®°‰ªøÂ≠¶‰π†**„ÄÇÂú®Â§ßËßÑÊ®°Êú∫Âô®‰∫∫ÊºîÁ§∫Êï∞ÊçÆ‰∏äËøõË°åÁ´ØÂà∞Á´ØÁ≠ñÁï•ËÆ≠ÁªÉÔºåÁõ¥Êé•Â≠¶‰π†‰ªéËßÇÂØüÂà∞Âä®‰ΩúÁöÑÊò†Â∞Ñ„ÄÇ | Âª∫Á´ã‰∫ÜÂº∫Â§ßÁöÑVLAÂü∫Á°ÄÔºåÂú®ËÆ≠ÁªÉÂàÜÂ∏ÉÂÜÖÁöÑÊäÄËÉΩÔºàÂ¶ÇÊäìÂèñ-ÊîæÁΩÆÔºâ‰∏äË°®Áé∞‰ºòÂºÇ„ÄÇ | **Ê†∏ÂøÉÂå∫Âà´ÔºöÊé®ÁêÜ‰∏éÊ≥õÂåñËÉΩÂäõ**„ÄÇ<br>1. **Ê≥õÂåñËÉΩÂäõ**ÔºöÂü∫Á°ÄVLAÊ®°ÂûãÁº∫‰πèÊòæÂºèÊé®ÁêÜÊú∫Âà∂ÔºåÂú®**ÈïøËßÜÈáéËßÑÂàí„ÄÅÂ∫îÂØπÊñ∞Âú∫ÊôØ„ÄÅÂ§±Ë¥•ÊÅ¢Â§ç**ÊñπÈù¢ËÉΩÂäõÊúâÈôê„ÄÇFast-ThinkAct ÊòéÁ°ÆÂºïÂÖ•‰∫Ü**ÂÜÖÈÉ®Êé®ÁêÜÊ®°Âùó**ÔºåÊòæËëóÂ¢ûÂº∫‰∫ÜËøô‰∫õËÉΩÂäõ„ÄÇ<br>2. **Êû∂ÊûÑ**ÔºöÂü∫Á°ÄVLAÈÄöÂ∏∏ÊòØÂçï‰∏ÄÁöÑÁ≠ñÁï•ÁΩëÁªú„ÄÇFast-ThinkAct ÈááÁî®**‰∏§Èò∂ÊÆµÊû∂ÊûÑ**Ôºö‰∏Ä‰∏™ËøõË°åÊΩúÂú®Êé®ÁêÜÁöÑVLMÔºåÂêéÊé•‰∏Ä‰∏™‰ª•Êé®ÁêÜÁªìÊûú‰∏∫Êù°‰ª∂ÁöÑÂä®‰ΩúÊ®°ÂûãÔºåÂÆûÁé∞‰∫Ü**È´òÂ±ÇËßÑÂàí‰∏éÂ∫ïÂ±ÇÊâßË°åÁöÑËß£ËÄ¶‰∏éÊ°•Êé•**„ÄÇ |\n| **È´òÊïàÊé®ÁêÜÁöÑLLMÊñπÊ≥ï** (Â¶Ç CODI, Soft Thinking) | **ÊΩúÂú®Á©∫Èó¥Êé®ÁêÜ**„ÄÇÂ∞ÜÈìæÂºèÊÄùÁª¥ÊèêÁÇºÂà∞ËøûÁª≠Á©∫Èó¥ÔºàÂ¶ÇÈöêËóèÁä∂ÊÄÅ„ÄÅÊ¶ÇÂøµtokenÔºâ„ÄÇ | Âú®Á∫ØËØ≠Ë®ÄÊ®°Âûã‰∏≠Êé¢Á¥¢‰∫ÜÂéãÁº©Êé®ÁêÜ„ÄÅÊèêÂçáÊïàÁéáÁöÑÊñπÊ≥ï„ÄÇ | **Ê†∏ÂøÉÂå∫Âà´ÔºöÊ®°ÊÄÅ‰∏é‰ªªÂä°ÈÄÇÈÖç**„ÄÇ<br>1. **Â§öÊ®°ÊÄÅÈúÄÊ±Ç**ÔºöLLMÁöÑÊΩúÂú®Êé®ÁêÜÊñπÊ≥ïÊó†Ê≥ïÁõ¥Êé•Â§ÑÁêÜ**ËßÜËßâÁ©∫Èó¥ÁêÜËß£ÂíåÊó∂Á©∫Âä®ÊÄÅ**„ÄÇFast-ThinkAct ‰∏ìÈó®ËÆæËÆ°‰∫Ü**Âä®‰ΩúÂØπÈΩêÁöÑËßÜËßâËßÑÂàíËí∏È¶è**Âíå**Á©∫Èó¥tokenÂπ∂Ë°åÈ¢ÑÊµãËΩ®Ëøπ**Ôºå‰ΩøÊΩúÂú®Ë°®Á§∫ËÉΩÂ§üÊâøËΩΩËßÜËßâËßÑÂàí‰ø°ÊÅØ„ÄÇ<br>2. **‰∏éÂä®‰ΩúÊâßË°åÁöÑËøûÊé•**ÔºöFast-ThinkAct ‰∏ç‰ªÖËøõË°åÊé®ÁêÜÔºåËøòÈÄöËøá**Êé®ÁêÜÂ¢ûÂº∫ÁöÑÁ≠ñÁï•Â≠¶‰π†**ÔºåÂ∞ÜÂ≠¶Âà∞ÁöÑÊΩúÂú®ËßÑÂàíÊòéÁ°ÆÂú∞ËøûÊé•Âà∞‰∏ãÊ∏∏Âä®‰ΩúÊ®°ÂûãÁöÑKVÁºìÂ≠ò‰∏≠ÔºåÊåáÂØºÂä®‰ΩúÁîüÊàêÔºåËøôÊòØÁ∫ØÊñáÊú¨Ê®°Âûã‰∏çÂÖ∑Â§áÁöÑ„ÄÇ |\n\n## Fast-ThinkAct Áõ∏ÊØî‰∫éÂâç‰ΩúÔºàThinkActÔºâÁöÑÂÖ∑‰ΩìÊîπÂä®‰∏éÊîπÂñÑ\n\n1.  **Êé®ÁêÜË°®Á§∫ÁöÑÊ†πÊú¨ÊÄßÂèòÈù©**Ôºö\n    *   **Ââç‰Ωú (ThinkAct)**ÔºöËæìÂá∫‰∏∫Á¶ªÊï£ÁöÑ„ÄÅ‰∫∫Á±ªÂèØËØªÁöÑ**ÊñáÊú¨Â∫èÂàó**‰Ωú‰∏∫Êé®ÁêÜÈìæ„ÄÇÁîüÊàêËøáÁ®ãËá™ÂõûÂΩíÔºåÈÄüÂ∫¶ÊÖ¢„ÄÇ\n    *   **Êú¨Êñá (Fast-ThinkAct)**ÔºöËæìÂá∫‰∏∫**Á¥ßÂáëÁöÑËøûÁª≠ÊΩúÂú®ÂêëÈáè** (`z`) Âíå**Âπ∂Ë°åÁöÑÁ©∫Èó¥token** (`s`)„ÄÇÊΩúÂú®ÂêëÈáèÁºñÁ†ÅÊäΩË±°Êé®ÁêÜÈÄªËæëÔºåÁ©∫Èó¥tokenÁõ¥Êé•Êò†Â∞Ñ‰∏∫ËßÜËßâËΩ®ËøπË∑ØÂæÑÁÇπ„ÄÇËøôÂÆûÁé∞‰∫Ü**Êé®ÁêÜÁöÑÈöêÂºèÂåñÂíåÂπ∂Ë°åÂåñ**ÔºåÊòØÈÄüÂ∫¶ÊèêÂçáÁöÑÊ†∏ÂøÉ„ÄÇ\n\n2.  **ÂàõÊñ∞ÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºöÂÅèÂ•ΩÂºïÂØºËí∏È¶è‰∏éËßÜËßâÂØπÈΩê**Ôºö\n    *   **Ââç‰Ωú (ThinkAct)**Ôºö‰ΩøÁî®GRPOÁõ¥Êé•‰ºòÂåñÊñáÊú¨ÁîüÊàêÁ≠ñÁï•ÔºåÂ•ñÂä±‰ø°Âè∑Âü∫‰∫éÊúÄÁªàÂä®‰ΩúÁöÑÊàêÂäü‰∏éÂê¶„ÄÇ\n    *   **Êú¨Êñá (Fast-ThinkAct)**Ôºö\n        *   **ÊïôÂ∏à-Â≠¶ÁîüÊ°ÜÊû∂**Ôºö‰ΩøÁî®‰∏Ä‰∏™ÁªèËøáGRPOËÆ≠ÁªÉÁöÑ**ÊñáÊú¨ÊïôÂ∏àÊ®°Âûã**ÔºàÂç≥ThinkActÔºâÊù•ÁîüÊàêË¥®Èáè‰∏ç‰∏ÄÁöÑÊé®ÁêÜËΩ®Ëøπ„ÄÇ\n        *   **ÂÅèÂ•ΩËí∏È¶è** (`‚Ñí_verb`)ÔºöÂºïÂÖ•‰∏Ä‰∏™**Ë®ÄËØ≠ÂåñÂô®**ÔºåÂ∞ÜÂ≠¶ÁîüÊ®°ÂûãÁîüÊàêÁöÑÊΩúÂú®ÂêëÈáèËß£Á†ÅÂõûÊñáÊú¨„ÄÇÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÊé®ÁêÜËΩ®ËøπÁöÑ‰ºòÂäøÂáΩÊï∞ÊûÑÂª∫ÂÅèÂ•ΩÂØπÔºåÈÄöËøáÁ±ª‰ººDPOÁöÑÁõÆÊ†áÔºåÈºìÂä±Ë®ÄËØ≠ÂåñÂô®Â∞ÜÊΩúÂú®ÂêëÈáèËß£Á†Å‰∏∫È´òË¥®ÈáèÊé®ÁêÜÔºåÊäëÂà∂‰ΩéË¥®ÈáèÊé®ÁêÜ„ÄÇËøôÁ°Æ‰øù‰∫Ü**ÊΩúÂú®Á©∫Èó¥‰øùÁïô‰∫ÜÊúâÊïàÁöÑÊé®ÁêÜÊ®°Âºè**„ÄÇ\n        *   **ËßÜËßâËßÑÂàíËí∏È¶è** (`‚Ñí_distill`)ÔºöÈô§‰∫ÜËØ≠Ë®ÄÊé®ÁêÜÔºåËøòÈÄöËøáÂØπÈΩêÊïôÂ∏àÂíåÂ≠¶ÁîüÊ®°ÂûãÂú®`<answer>` tokenÁöÑÈöêËóèÁä∂ÊÄÅÔºåÁõ¥Êé•**Ëí∏È¶èÊïôÂ∏àÁöÑËßÜËßâËßÑÂàíËÉΩÂäõ**Âà∞Â≠¶ÁîüÁöÑÊΩúÂú®Ë°®Á§∫‰∏≠„ÄÇËøôÊòØ‰øùËØÅÊΩúÂú®Ë°®Á§∫ÂØπÂÖ∑Ë∫´‰ªªÂä°ÊúâÊïàÁöÑÂÖ≥ÈîÆ„ÄÇ\n        *   **ËΩ®ËøπÈ¢ÑÊµã** (`‚Ñí_ans`)ÔºöÂ≠¶ÁîüÊ®°Âûã‰ΩøÁî®Âπ∂Ë°åÁöÑÁ©∫Èó¥tokenÁõ¥Êé•ÂõûÂΩí2DËßÜËßâËΩ®ËøπË∑ØÂæÑÁÇπÔºåÊõø‰ª£‰∫ÜÊïôÂ∏àÊ®°ÂûãËá™ÂõûÂΩíÁîüÊàêÊñáÊú¨ÊèèËø∞Ë∑ØÂæÑÁÇπÁöÑÊñπÂºèÔºåÊïàÁéáÊõ¥È´ò„ÄÇ\n\n3.  **Êé®ÁêÜ‰∏éÂä®‰ΩúÊâßË°åÁöÑÈ´òÊïàÊ°•Êé•**Ôºö\n    *   **Ââç‰Ωú (ThinkAct)**ÔºöÊé®ÁêÜÊñáÊú¨‰Ωú‰∏∫Êù°‰ª∂ËæìÂÖ•Âà∞Á≠ñÁï•ÁΩëÁªú‰∏≠ÔºåËøûÊé•ÂèØËÉΩ‰∏çÂ§üÁ¥ßÂØÜ„ÄÇ\n    *   **Êú¨Êñá (Fast-ThinkAct)**Ôºö‰ªéÂ≠¶ÁîüVLMÁöÑ**Á©∫Èó¥tokenÁöÑKey-ValueÁºìÂ≠òÔºàÊó©ÊúüÂ±ÇÔºâ‰∏≠ÊèêÂèñËßÜËßâÊΩúÂú®ËßÑÂàí** (`c_t`)ÔºåÂ∞ÜÂÖ∂‰∏éÂä®‰ΩúÊ®°ÂûãÁöÑÁä∂ÊÄÅÁºñÁ†ÅKVÁºìÂ≠òÊãºÊé•„ÄÇÂä®‰ΩúÊ®°ÂûãÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõÂêåÊó∂ÂÖ≥Ê≥®ÂéüÂßãËßÇÂØüÂíåËøô‰∏™**ÂØåÂê´ËßÑÂàí‰ø°ÊÅØÁöÑÊΩúÂú®‰∏ä‰∏ãÊñá**„ÄÇËøôÁßçÂú®Ë°®Á§∫Â±ÇÈù¢ÁöÑËûçÂêàÊõ¥Áõ¥Êé•„ÄÅÈ´òÊïà„ÄÇ\n\n4.  **ÊÄßËÉΩ‰∏éÊïàÁéáÁöÑÊòæËëóÊèêÂçá**Ôºö\n    *   **ÊïàÁéá**ÔºöÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏äÔºåÁõ∏ÊØîThinkAct-7BÂÆûÁé∞**ÊúÄÈ´ò89.3%ÁöÑÂª∂ËøüÈôç‰Ωé**ÔºåÁõ∏ÊØîThinkAct-3BÂø´**7ÂÄç**„ÄÇ\n    *   **ÊÄßËÉΩ**ÔºöÂú®LIBERO„ÄÅSimplerEnv„ÄÅRoboTwin2.0Á≠âÊú∫Âô®‰∫∫Êìç‰ΩúÂü∫ÂáÜ‰∏äÔºå**ÊàêÂäüÁéáËææÂà∞ÊàñË∂ÖËøáThinkAct**„ÄÇÂú®EgoPlan-Bench2„ÄÅRoboVQAÁ≠âÂÖ∑Ë∫´Êé®ÁêÜÂü∫ÂáÜ‰∏äÔºå**ÊÄßËÉΩÊòæËëóË∂ÖËøáThinkActÂèäÂÖ∂‰ªñÂü∫Á∫ø**„ÄÇÂÆûÁé∞‰∫Ü**‚ÄúÊó¢Âø´ÂèàÂ•Ω‚Äù** ÁöÑÁõÆÊ†á„ÄÇ\n\n**ÊÄªÁªì**ÔºöFast-ThinkAct ÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂ∞Ü ThinkAct ÁöÑ**ÊòæÂºè„ÄÅÁºìÊÖ¢ÁöÑÊñáÊú¨Êé®ÁêÜËøáÁ®ã**ÔºåÈÄöËøá‰∏ÄÂ•óÁ≤æÂøÉËÆæËÆ°ÁöÑ**ÂÅèÂ•ΩÂºïÂØºËí∏È¶è‰∏éËßÜËßâÂØπÈΩêÊú∫Âà∂**ÔºåÂéãÁº©Âπ∂ËΩ¨Âåñ‰∏∫**ÈöêÂºè„ÄÅÈ´òÊïàÁöÑÊΩúÂú®Á©∫Èó¥Ë°®Á§∫**„ÄÇÂÆÉ‰∏ç‰ªÖÂ§ßÂπÖÊèêÂçá‰∫ÜÊé®ÁêÜÈÄüÂ∫¶ÔºåËøòÈÄöËøáÊõ¥Â•ΩÁöÑ‰ø°ÊÅØ‰øùÁïôÔºàÂ∞§ÂÖ∂ÊòØËßÜËßâËßÑÂàí‰ø°ÊÅØÔºâÂíå‰∏éÂä®‰ΩúÊ®°ÂûãÊõ¥Á¥ßÂØÜÁöÑÈõÜÊàêÔºåÁª¥ÊåÅÁîöËá≥ÊèêÂçá‰∫ÜÂú®Â§çÊùÇÂÖ∑Ë∫´‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇËøôÊòØÂØπÂâç‰ΩúThinkActÂú®**ÊïàÁéáÁì∂È¢à**ÈóÆÈ¢ò‰∏äÁöÑ‰∏Ä‰∏™Ê†πÊú¨ÊÄßËß£ÂÜ≥ÊñπÊ°à„ÄÇ",
      "timestamp": "2026-01-20T17:59:57.248252",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "ËÆ∫ÊñáÊèêÂá∫‰∫ÜÂì™‰∫õÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïÔºåËØ∑ÂàóË°®Ê†ºÂÖ∑‰ΩìËØ¶ÁªÜËØ¥ÊòéÊäÄÊúØÁªÜËäÇÔºåÈúÄË¶ÅÂåÖÂê´ÂÖ∑‰ΩìÁöÑÊï∞Â≠¶ÂéüÁêÜÊé®ÂØºÔºå‰ª•ÂèäÂÖ∑‰ΩìÂèÇÊï∞„ÄÇ",
      "answer": "## ËÆ∫ÊñáÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïÂàÜÊûê\n\nËÆ∫Êñá„ÄäFast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning„ÄãÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∏ÄÁßç**È´òÊïà„ÄÅÂèØË®ÄËØ≠ÂåñÁöÑÊΩúÂú®ËßÑÂàíÊ°ÜÊû∂**ÔºåÂ∞ÜÂÜóÈïøÁöÑÊòæÂºèÊÄùÁª¥ÈìæÔºàCoTÔºâÂéãÁº©‰∏∫Á¥ßÂáëÁöÑÊΩúÂú®Ë°®Á§∫Ôºå‰ªéËÄåÂú®‰øùÊåÅÂº∫Â§ßÊé®ÁêÜËÉΩÂäõÁöÑÂêåÊó∂ÔºåÂ§ßÂπÖÈôç‰ΩéÊé®ÁêÜÂª∂Ëøü„ÄÇÂÖ∂ÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïÂèØÂΩíÁ∫≥‰∏∫‰ª•‰∏ãÂõõ‰∏™Áõ∏‰∫íÂÖ≥ËÅîÁöÑÊ®°ÂùóÔºö\n\n| ÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ï | Ê†∏ÂøÉÁõÆÊ†á | ÊäÄÊúØÁªÜËäÇ‰∏éÊï∞Â≠¶ÂéüÁêÜ | ÂÖ∑‰ΩìÂèÇÊï∞‰∏éÂÆûÁé∞ |\n| :--- | :--- | :--- | :--- |\n| **1. Âü∫‰∫éÂ•ñÂä±ÂÅèÂ•ΩÁöÑÂèØË®ÄËØ≠ÂåñÊΩúÂú®ÊÄùÁª¥Èìæ** | Â∞ÜÊïôÂ∏àÊ®°ÂûãÁîüÊàêÁöÑË¥®Èáè‰∏ç‰∏ÄÁöÑÂÜóÈïøÊñáÊú¨CoTÔºåËí∏È¶èÂà∞Â≠¶ÁîüÊ®°ÂûãÁöÑÁ¥ßÂáëÊΩúÂú®Á©∫Èó¥‰∏≠ÔºåÂπ∂Á°Æ‰øùËØ•ÊΩúÂú®Ë°®Á§∫ÂèØË¢´‚ÄúË®ÄËØ≠Âåñ‚Äù‰∏∫È´òË¥®ÈáèÊé®ÁêÜÊñáÊú¨„ÄÇ | **1.1 ÊïôÂ∏àÊ®°ÂûãËÆ≠ÁªÉ (GRPO):** <br> ÊïôÂ∏àËßÜËßâËØ≠Ë®ÄÊ®°Âûã $\\mathcal{F}_{\\theta}^{T}$ ÈÄöËøáÂàÜÁªÑÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàGRPOÔºâËøõË°åËÆ≠ÁªÉÔºå‰ª•ÁîüÊàê‰∏éÂä®‰ΩúÂ•ñÂä±ÂØπÈΩêÁöÑÊé®ÁêÜËΩ®Ëøπ $\\tau$„ÄÇÂÖ∂ÁõÆÊ†áÂáΩÊï∞‰∏∫Ôºö <br> $\\mathcal{J}_{\\text{GRPO}}(\\theta) = \\mathbb{E}_{\\tau\\sim\\mathcal{F}^{T}_{\\theta}}[\\min(r_{\\theta}(\\tau)A(\\tau), \\text{clip}(r_{\\theta}(\\tau),1-\\epsilon,1+\\epsilon)A(\\tau))]$ <br> ÂÖ∂‰∏≠Ôºå$r_{\\theta}(\\tau) = \\frac{\\mathcal{F}^{T}_{\\theta}(\\tau)}{\\mathcal{F}^{T}_{\\text{old}}(\\tau)}$ ‰∏∫Ê¶ÇÁéáÊØîÔºå$A(\\tau)$ ‰∏∫‰ºòÂäøÂáΩÊï∞ÔºåËÆ°ÁÆóÂü∫‰∫éÁªÑÂÜÖÂ•ñÂä± $\\{R_i\\}_{i\\in G(\\tau)}$ ÁöÑÊ†áÂáÜÂåñÂÄºÔºö$A(\\tau) = \\frac{R_{\\tau} - \\text{mean}(\\{R_i\\})}{\\text{std}(\\{R_i\\})}$„ÄÇ <br><br> **1.2 ÂÅèÂ•ΩÂØπÊûÑÂª∫‰∏éÊΩúÂú®Ëí∏È¶è:** <br> ‰ªéÊØèÁªÑ rollout ‰∏≠Ê†πÊçÆ‰ºòÂäøÂáΩÊï∞ $A(\\tau)$ ÈÄâÊã©ÊúÄ‰ºòÊé®ÁêÜ $\\tau^+$ ÂíåÊúÄÂ∑ÆÊé®ÁêÜ $\\tau^-$ ÊûÑÊàêÂÅèÂ•ΩÂØπ„ÄÇÂ≠¶ÁîüÊ®°Âûã $\\mathcal{F}_{\\theta}$ ‰∏çÁîüÊàêÊñáÊú¨ÔºåËÄåÊòØËá™ÂõûÂΩíÂú∞ÁîüÊàê **M ‰∏™ËøûÁª≠ÊΩúÂú®ÂêëÈáè** $\\mathbf{z} = \\{z_m\\}_{m=1}^{M}, z_m \\in \\mathbb{R}^d$„ÄÇ <br><br> **1.3 ÂèØË®ÄËØ≠ÂåñÊçüÂ§± ($\\mathcal{L}_{\\text{verb}}$):** <br> ÂºïÂÖ•‰∏Ä‰∏™**Ë®ÄËØ≠ÂåñÂô®** LLM $\\mathcal{V}_{\\psi}$ÔºåÂÖ∂‰ªªÂä°ÊòØÂ∞ÜÊΩúÂú®ÂêëÈáè $\\mathbf{z}$ Ëß£Á†Å‰∏∫Ëá™ÁÑ∂ËØ≠Ë®Ä„ÄÇËÆ≠ÁªÉÁõÆÊ†áÂÄüÈâ¥ DPOÔºåÈºìÂä±Ë®ÄËØ≠ÂåñÂô®Â∞Ü $\\mathbf{z}$ Ëß£Á†Å‰∏∫È´òË¥®ÈáèÊé®ÁêÜ $\\tau^+$ ÁöÑÊ¶ÇÁéáËøúÈ´ò‰∫éËß£Á†Å‰∏∫‰ΩéË¥®ÈáèÊé®ÁêÜ $\\tau^-$ ÁöÑÊ¶ÇÁéáÔºö <br> $\\mathcal{L}_{\\text{verb}} = -\\mathbb{E}[\\log \\sigma(\\beta(\\log\\frac{p_{\\psi}(\\tau^+ \\mid \\mathbf{z})}{p_{\\text{ref}}(\\tau^+)} - \\log\\frac{p_{\\psi}(\\tau^- \\mid \\mathbf{z})}{p_{\\text{ref}}(\\tau^-)}))]$ <br> ÂÖ∂‰∏≠Ôºå$p_{\\text{ref}}$ ÊòØÊó†ÊΩúÂú®Êù°‰ª∂Á∫¶ÊùüÁöÑÂèÇËÄÉÊ®°ÂûãÔºå$\\sigma$ ‰∏∫ sigmoid ÂáΩÊï∞Ôºå$\\beta$ ÊéßÂà∂ÂÅèÂ•ΩÂº∫Â∫¶„ÄÇ | - **È™®Âπ≤Ê®°Âûã:** Qwen2.5-VL 3B„ÄÇ <br> - **ÊΩúÂú®ÂêëÈáèÁª¥Â∫¶ $d$:** ‰∏éÊ®°ÂûãÈöêËóèÂ±ÇÂ§ßÂ∞è‰∏ÄËá¥ÔºàÊú™ÊòéÁ°ÆÁªôÂá∫ÔºåÈÄöÂ∏∏‰∏∫1024Êàñ2048Ôºâ„ÄÇ <br> - **ÊΩúÂú®Êé®ÁêÜÊ≠•Êï∞ $M$:** **6**ÔºàÁªèÊ∂àËûçÂÆûÈ™åÁ°ÆÂÆö‰∏∫ÊúÄ‰ºòÔºâ„ÄÇ <br> - **ÂÅèÂ•ΩÂº∫Â∫¶ $\\beta$:** **0.1**„ÄÇ <br> - **Ë®ÄËØ≠ÂåñÂô®Ê®°Âûã:** Âü∫‰∫é Qwen3-0.6B ÂàùÂßãÂåñÔºåÂπ∂ÊèíÂÖ•Ë∑®Ê≥®ÊÑèÂäõÂ±Ç‰ª•Êé•Êî∂ÊΩúÂú®ÂêëÈáè $\\mathbf{z}$ ‰Ωú‰∏∫Êù°‰ª∂„ÄÇ <br> - **GRPO Rollout Â§ßÂ∞è $N$:** **5**„ÄÇ |\n| **2. Âä®‰ΩúÂØπÈΩêÁöÑËßÜËßâËßÑÂàíËí∏È¶è** | Á°Æ‰øùÂ≠¶ÁîüÊ®°ÂûãÁöÑÊΩúÂú®Ë°®Á§∫‰∏ç‰ªÖÁºñÁ†ÅËØ≠Ë®ÄÊé®ÁêÜÈÄªËæëÔºåËøòËÉΩÊçïËé∑ÂØπÂÖ∑Ë∫´ÊéßÂà∂Ëá≥ÂÖ≥ÈáçË¶ÅÁöÑ**Á©∫Èó¥ËßÜËßâËßÑÂàíËÉΩÂäõ**„ÄÇ | **2.1 ËΩ®ËøπË°®Á§∫ÂØπÈΩê ($\\mathcal{L}_{\\text{distill}}$):** <br> ÊúÄÂ∞èÂåñÊïôÂ∏àÊ®°ÂûãÂíåÂ≠¶ÁîüÊ®°ÂûãÂú®ÁºñÁ†ÅËßÜËßâËßÑÂàí‰ø°ÊÅØÁöÑÂÖ≥ÈîÆ‰ΩçÁΩÆÔºàÂ¶Ç `<answer>` Ê†áËÆ∞ÔºâÁöÑÈöêËóèÁä∂ÊÄÅ‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºö <br> $\\mathcal{L}_{\\text{distill}} = \\| h_t^T - h_t \\|_2^2$ <br> ÂÖ∂‰∏≠Ôºå$h_t^T$ Âíå $h_t$ ÂàÜÂà´Êù•Ëá™ÊïôÂ∏àÔºàÂØπÂ∫î $\\tau^+$ÔºâÂíåÂ≠¶ÁîüÊ®°Âûã„ÄÇ <br><br> **2.2 Âπ∂Ë°åÁ©∫Èó¥ËΩ®ËøπÈ¢ÑÊµã ($\\mathcal{L}_{\\text{ans}}$):** <br> ÊïôÂ∏àÊ®°Âûã‰ª•Ëá™ÂõûÂΩíÊñπÂºèÁîüÊàêÊèèËø∞Ë∑ØÂæÑÁÇπ $\\{p_k\\}_{k=1}^K$ ÁöÑÈïøÊñáÊú¨Â∫èÂàóÔºà$K=5$ Êó∂Á∫¶60-70‰∏™tokenÔºâ„ÄÇ‰∏∫ÊèêÂçáÊïàÁéáÔºåÂ≠¶ÁîüÊ®°Âûã‰ΩøÁî® **K ‰∏™ÂèØÂ≠¶‰π†ÁöÑÁ©∫Èó¥token** $\\{\\mathbf{s}_i\\}_{i=1}^K$ ÈôÑÂä†Âú®ÊΩúÂú®Â∫èÂàóÂêéÔºåÂπ∂ÈÄöËøá‰∏Ä‰∏™ MLP Â∞ÜÊØè‰∏™Á©∫Èó¥tokenÁöÑËæìÂá∫ÈöêËóèÁä∂ÊÄÅ $h'(\\mathbf{s}_i)$ Âπ∂Ë°åÊäïÂΩ±‰∏∫Ë∑ØÂæÑÁÇπÂùêÊ†á $p_i$„ÄÇÊçüÂ§±ÂáΩÊï∞‰∏∫Ôºö <br> $\\mathcal{L}_{\\text{ans}} = \\sum_{i=1}^{K} \\| p_i - \\hat{p}_i \\|_2^2$Ôºå ÂÖ∂‰∏≠ $p_i = \\text{MLP}(h'(\\mathbf{s}_i))$Ôºå$\\hat{p}_i$ ÊòØÁúüÂÆûË∑ØÂæÑÁÇπ„ÄÇ | - **Ë∑ØÂæÑÁÇπÊï∞Èáè $K$:** **5**„ÄÇ <br> - **Ë∑ØÂæÑÁÇπÂùêÊ†á $p_i$:** ÂØπ‰∫éÂçïËáÇ‰ªªÂä°‰∏∫ $[x_{\\text{single}}, y_{\\text{single}}]$ÔºåÂØπ‰∫éÂèåËáÇ‰ªªÂä°‰∏∫ $[x_{\\text{left}}, y_{\\text{left}}, x_{\\text{right}}, y_{\\text{right}}]$ÔºåÁªü‰∏ÄÁî® $\\mathbb{R}^6$ ÂêëÈáèË°®Á§∫ÔºåÊú™‰ΩøÁî®ÁöÑÁª¥Â∫¶Âú®ËÆ°ÁÆóÊçüÂ§±Êó∂Ë¢´Êé©Á†Å„ÄÇ <br> - **Â≠¶ÁîüÊ®°ÂûãÊÄªÊçüÂ§±:** $\\mathcal{L}_{\\text{student}} = \\mathcal{L}_{\\text{verb}} + \\mathcal{L}_{\\text{distill}} + \\mathcal{L}_{\\text{ans}}$„ÄÇ |\n| **3. Êé®ÁêÜÂ¢ûÂº∫ÁöÑÁ≠ñÁï•Â≠¶‰π†** | Â∞ÜÈ´òÂ±Ç„ÄÅÁ¥ßÂáëÁöÑÊΩúÂú®ËßÜËßâËßÑÂàí‰∏é‰ΩéÂ±ÇÂä®‰ΩúÊâßË°åÊ°•Êé•Ëµ∑Êù•Ôºå‰ΩøÁ≠ñÁï•Ê®°ÂûãËÉΩÂ§üÂà©Áî®Êé®ÁêÜÁªìÊûúÁîüÊàêÁ≤æÁ°ÆÂä®‰Ωú„ÄÇ | **3.1 ËßÜËßâÊΩúÂú®ËßÑÂàíÊèêÂèñ:** <br> ËÆ≠ÁªÉÂÆåÊàêÂêéÔºåÂÜªÁªìÂ≠¶Áîü VLM $\\mathcal{F}_{\\theta}$„ÄÇÂú®Êé®ÁêÜÊó∂Ôºå‰ªé $\\mathcal{F}_{\\theta}$ **ËæÉÊµÖÂ±Ç**ÁöÑÁ©∫Èó¥tokenÁöÑ Key-Value (KV) ÁºìÂ≠ò‰∏≠ÊèêÂèñËßÜËßâÊΩúÂú®ËßÑÂàíË°®Á§∫ $c_t$„ÄÇËÆ∫ÊñáÂèëÁé∞ÊµÖÂ±ÇË°®Á§∫ÊØîÊ∑±Â±ÇÊàñÊúÄÁªàÈöêËóèÁä∂ÊÄÅÊõ¥ËÉΩÊúâÊïà‰øùÁïôËßÑÂàí‰ø°ÊÅØ„ÄÇ <br><br> **3.2 Á≠ñÁï•Ê®°ÂûãÊù°‰ª∂Âåñ‰∏éËÆ≠ÁªÉ:** <br> Âä®‰ΩúÊ®°Âûã $\\pi_{\\phi}$ÔºàÂ¶ÇÂü∫‰∫é Diffusion Transformer ÁöÑ RDTÔºâÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂêåÊó∂ÂÖ≥Ê≥®Áä∂ÊÄÅÁºñÁ†ÅÂô®ÁöÑ KV ÂØπÂíåÊù•Ëá™ VLM ÁöÑËßÑÂàí‰∏ä‰∏ãÊñá $c_t$„ÄÇÁ≠ñÁï•ÈÄöËøáÊ®°‰ªøÂ≠¶‰π†ËøõË°åÂæÆË∞ÉÔºåÊçüÂ§±ÂáΩÊï∞‰∏∫Ôºö <br> $\\mathcal{L}_{\\text{IL}}(\\phi) = \\ell(\\pi_{\\phi}(o_t, l, c_t), \\hat{a}_t)$ <br> ÂÖ∂‰∏≠ $\\ell$ ÊòØÊâ©Êï£Á≠ñÁï•ÁöÑÂéªÂô™ÁõÆÊ†áÔºàÂ¶ÇÂùáÊñπËØØÂ∑ÆÔºâÔºå$\\hat{a}_t$ ÊòØÁúüÂÆûÂä®‰Ωú„ÄÇËÆ≠ÁªÉÊó∂Ôºå$\\mathcal{F}_{\\theta}$ ÂíåÁä∂ÊÄÅÁºñÁ†ÅÂô®Ë¢´ÂÜªÁªìÔºå‰ªÖÊõ¥Êñ∞ $\\pi_{\\phi}$ ÂíåÁî®‰∫éÈÄÇÈÖç $c_t$ Áª¥Â∫¶ÁöÑÁ∫øÊÄßÊäïÂΩ±Â±Ç„ÄÇ | - **Âä®‰ΩúÊ®°Âûã:** Âú® SimplerEnv ‰∏ä‰ΩøÁî® DiT-PolicyÔºåÂú® LIBERO Âíå RoboTwin2.0 ‰∏ä‰ΩøÁî® RDT„ÄÇ <br> - **ËßÑÂàí‰∏ä‰∏ãÊñá $c_t$ ÁöÑËûçÂêà:** ÈÄöËøáÁ∫øÊÄßÊäïÂΩ±Â∞Ü VLM KV ÁºìÂ≠òÁª¥Â∫¶ÈÄÇÈÖçÂà∞Âä®‰ΩúÊ®°ÂûãÊ≥®ÊÑèÂäõÁª¥Â∫¶ÔºàDiT-Policy‰∏∫1024ÔºåRDT‰∏∫2048Ôºâ„ÄÇ <br> - **ËÆ≠ÁªÉÊï∞ÊçÆ:** Â§ßËßÑÊ®°Êú∫Âô®‰∫∫Âä®‰ΩúÊï∞ÊçÆÔºàÂ¶Ç OXE Êï∞ÊçÆÈõÜÔºâÔºåÈíàÂØπÁâπÂÆöÁéØÂ¢ÉÔºàÂ¶Ç LIBEROÔºâËøõË°åÂ∞ëÈáèÊºîÁ§∫ÁöÑÂæÆË∞É„ÄÇ |\n| **4. Êï¥‰ΩìËÆ≠ÁªÉ‰∏éÊé®ÁêÜÊµÅÁ®ã** | Êï¥Âêà‰ª•‰∏äÊñπÊ≥ïÔºåÂÆûÁé∞‰ªéÊïôÂ∏àÊ®°ÂûãÁü•ËØÜËí∏È¶èÂà∞Â≠¶ÁîüÊ®°ÂûãÈ´òÊïàÊé®ÁêÜÁöÑÁ≠ñÁï•Â≠¶‰π†ÂÖ®ÊµÅÁ®ã„ÄÇ | **ËÆ≠ÁªÉÂàÜ‰∏â‰∏™Èò∂ÊÆµ:** <br> 1. **SFT + CoT-SFT:** Âú®Ê∑∑ÂêàÊï∞ÊçÆÈõÜ‰∏äÂØπÂü∫Á°Ä VLM ËøõË°åÁõëÁù£ÂæÆË∞ÉÔºåÂ≠¶‰π†Âü∫Êú¨ËßÜËßâÁêÜËß£ÂíåÈìæÂºèÊÄùÁª¥„ÄÇ <br> 2. **ÊïôÂ∏à-Â≠¶ÁîüËí∏È¶è:** ÊïôÂ∏àÁî® GRPO ËÆ≠ÁªÉÔºõÂ≠¶ÁîüÁî® $\\mathcal{L}_{\\text{student}}$ ËÆ≠ÁªÉÔºåÂ∞ÜÊé®ÁêÜÂéãÁº©Âà∞ÊΩúÂú®Á©∫Èó¥„ÄÇË®ÄËØ≠ÂåñÂô®Ââç3000Ê≠•Áî®ËØ≠Ë®ÄÂª∫Ê®°ÊçüÂ§±È¢ÑÁÉ≠ÔºåÂêé1500Ê≠•Áî® $\\mathcal{L}_{\\text{verb}}$ ËøõË°åÂÅèÂ•ΩÂ≠¶‰π†„ÄÇ <br> 3. **Á≠ñÁï•Â≠¶‰π†:** ÂÜªÁªìËÆ≠ÁªÉÂ•ΩÁöÑÂ≠¶Áîü V",
      "timestamp": "2026-01-20T18:01:00.910467",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "‰ªñ‰ΩøÁî®‰∫ÜÂì™‰∫õËØÑ‰ª∑ÊåáÊ†á‰∏éÊï∞ÊçÆÈõÜÔºåÂàóË°®Ê†ºÂÖ∑‰ΩìËÆ≤ËÆ≤‰ªñÁöÑËØÑ‰ª∑ÊåáÊ†áÁöÑÁªÜËäÇ‰∏éÊï∞ÊçÆÈõÜÁöÑÁªÜËäÇ",
      "answer": "## ËÆ∫ÊñáËØÑ‰ª∑ÊåáÊ†á‰∏éÊï∞ÊçÆÈõÜÂàÜÊûê\n\n### ‰∏Ä„ÄÅ ËØÑ‰ª∑ÊåáÊ†á\n\nËÆ∫Êñá‰ΩøÁî®‰∫Ü**‰∏§Â§ßÁ±ªËØÑ‰ª∑ÊåáÊ†á**Ôºö**Êú∫Âô®‰∫∫Êìç‰ΩúÊàêÂäüÁéá**Âíå**ÂÖ∑Ë∫´Êé®ÁêÜËÉΩÂäõËØÑÂàÜ**„ÄÇÂÖ∑‰ΩìÁªÜËäÇÂ¶Ç‰∏ãË°®ÊâÄÁ§∫Ôºö\n\n| ÊåáÊ†áÁ±ªÂà´ | ÂÖ∑‰ΩìÊåáÊ†á | ÈÄÇÁî®‰ªªÂä°/Âü∫ÂáÜ | ÊåáÊ†áÁªÜËäÇ‰∏éËØ¥Êòé |\n| :--- | :--- | :--- | :--- |\n| **Êú∫Âô®‰∫∫Êìç‰Ωú** | **‰ªªÂä°ÊàêÂäüÁéá** | SimplerEnv, LIBERO, RoboTwin2.0 | Ê†∏ÂøÉÊåáÊ†á„ÄÇÂú®Ê®°ÊãüÁéØÂ¢É‰∏≠ÊâßË°å‰ªªÂä°Ôºå‰ª•**ÊàêÂäüÂÆåÊàê‰ªªÂä°ÁöÑËØïÈ™åÊ¨°Êï∞ÊØî‰æã**‰Ωú‰∏∫Ë°°ÈáèÊ†áÂáÜ„ÄÇ‰æãÂ¶ÇÔºåÂú®LIBEROÁöÑÊØè‰∏™Â≠ê‰ªªÂä°Â•ó‰ª∂‰∏äËøõË°å500Ê¨°ËØïÈ™åÔºåËÆ°ÁÆóÂπ≥ÂùáÊàêÂäüÁéá„ÄÇ |\n| | | | **RoboTwin2.0** Ëøõ‰∏ÄÊ≠•Âå∫ÂàÜ‰∫Ü**ÁÆÄÂçï**Âíå**Âõ∞Èöæ**‰∏§ÁßçËÆæÁΩÆÔºàÂêéËÄÖÂåÖÂê´È¢ÜÂüüÈöèÊú∫ÂåñÔºâÔºåÂπ∂‰æùÊçÆ‰∏ìÂÆ∂ÊºîÁ§∫Ê≠•È™§Êï∞Â∞Ü‰ªªÂä°ÂàÜ‰∏∫**Áü≠„ÄÅ‰∏≠„ÄÅÈïø**ËßÜÈáé‰ªªÂä°„ÄÇ |\n| **ÂÖ∑Ë∫´Êé®ÁêÜ** | **ÂáÜÁ°ÆÁéá** | EgoPlan-Bench2 | ËØÑ‰º∞‰ª•Ëá™Êàë‰∏∫‰∏≠ÂøÉËßÜËßíÁöÑÂ§öÊ≠•‰ªªÂä°ËßÑÂàíËÉΩÂäõ„ÄÇÊï∞ÊçÆÈõÜÂåÖÂê´1,321ÈÅì**Â§öÈ°πÈÄâÊã©È¢ò**ÔºåÊ®°ÂûãÈúÄÊ†πÊçÆ‰ªªÂä°ÁõÆÊ†áÂíåËøõÂ∫¶ÂéÜÂè≤È¢ÑÊµã‰∏ã‰∏ÄÊ≠•Âä®‰ΩúÔºå‰ª•**ÂõûÁ≠îÊ≠£Á°ÆÁöÑÊØî‰æã**‰Ωú‰∏∫ÂæóÂàÜ„ÄÇ |\n| | **BLEUÂàÜÊï∞** | RoboVQA | ËØÑ‰º∞ÂØπÊú∫Âô®‰∫∫Êìç‰ΩúËßÜÈ¢ëÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÊï∞ÊçÆÈõÜÂåÖÂê´1,893‰∏™**ÂºÄÊîæÂºèÈóÆÁ≠îÂØπ**„ÄÇ‰ΩøÁî®**BLEU-1Âà∞BLEU-4**‰ª•Âèä**BLEU-Avg**ÔºàÂπ≥ÂùáÔºâÂàÜÊï∞Êù•Ë°°ÈáèÊ®°ÂûãÁîüÊàêÁöÑÊñáÊú¨Á≠îÊ°à‰∏éÊ†áÂáÜÁ≠îÊ°à‰πãÈó¥ÁöÑ**n-gramÈáçÂè†Â∫¶**ÔºåÂàÜÊï∞Ë∂äÈ´òËØ¥ÊòéËØ≠‰πâÂåπÈÖçË∂äÂ•Ω„ÄÇ |\n| | **LLMËØÑÂàÜ** | OpenEQA, RoboFAC | ËØÑ‰º∞ÂØπÂºÄÊîæ‰∏ñÁïåÂú∫ÊôØÁöÑÈõ∂Ê†∑Êú¨ÁêÜËß£ÂíåÂ§±Ë¥•ÂàÜÊûêËÉΩÂäõ„ÄÇ‰ΩøÁî®**Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã**Ê†πÊçÆ‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÁöÑËØÑÂàÜÊ†áÂáÜÔºåÂØπÊ®°ÂûãÁîüÊàêÁöÑÁ≠îÊ°àËøõË°åÊâìÂàÜ„ÄÇËøôÊòØ‰∏ÄÁßç**Ëá™Âä®Âåñ„ÄÅÂèØÊâ©Â±ïÁöÑËØÑ‰º∞ÊñπÂºè**ÔºåÁî®‰∫éË°°ÈáèÁ≠îÊ°àÁöÑÂêàÁêÜÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ |\n| **ÊïàÁéá** | **Êé®ÁêÜÂª∂Ëøü** | ÊâÄÊúâ‰ªªÂä°ÔºàÂØπÊØîÂàÜÊûêÔºâ | ÂÖ≥ÈîÆÂàõÊñ∞ÊåáÊ†á„ÄÇÊµãÈáèÊ®°Âûã‰ªéÊé•Êî∂ËæìÂÖ•ÔºàËßÇÂØüÂíåÊåá‰ª§ÔºâÂà∞ËæìÂá∫Âä®‰ΩúÊâÄÈúÄÁöÑ**ÂçïÊ¨°Êé®ÁêÜÊó∂Èó¥**„ÄÇËÆ∫ÊñáÂº∫Ë∞ÉÔºåFast-ThinkActÈÄöËøáÂéãÁº©Êé®ÁêÜËøáÁ®ãÔºåÂÆûÁé∞‰∫ÜÈ´òËææ**89.3%ÁöÑÂª∂ËøüÈôç‰Ωé**Âíå**9.3ÂÄçÁöÑÂä†ÈÄü**„ÄÇ |\n\n### ‰∫å„ÄÅ Êï∞ÊçÆÈõÜ\n\nËÆ∫Êñá‰ΩøÁî®‰∫Ü**Â§öÁßçÂ§ßËßÑÊ®°„ÄÅÂ§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜ**ËøõË°åËÆ≠ÁªÉÂíåËØÑ‰º∞ÔºåÊ∂µÁõñËßÜËßâËΩ®Ëøπ„ÄÅÈóÆÁ≠îÂØπÂíåÊú∫Âô®‰∫∫ÊºîÁ§∫Êï∞ÊçÆ„ÄÇ\n\n#### 1. ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ\nÁî®‰∫éËÆ≠ÁªÉÊé®ÁêÜVLMÂíåÁ≠ñÁï•Ê®°ÂûãÔºåÂÖ∑‰ΩìÂ¶Ç‰∏ãÔºö\n\n| Êï∞ÊçÆÈõÜÂêçÁß∞ | Á±ªÂûã/ÂÜÖÂÆπ | Áî®ÈÄî‰∏éÁªÜËäÇ |\n| :--- | :--- | :--- |\n| **MolmoAct (OXE)** | **ÂçïËáÇ2DËßÜËßâËΩ®Ëøπ** | ‰ªéOpen X-Embodiment (OXE)Êï∞ÊçÆÈõÜ‰∏≠ÊèêÂèñÔºåÁ∫¶**130‰∏áÊù°ËΩ®Ëøπ**„ÄÇÁî®‰∫éÂ≠¶‰π†ÂçïËáÇÊìç‰ΩúÁöÑÁ©∫Èó¥ËßÑÂàíÂíåÂä®‰ΩúÂÖ≥ËÅî„ÄÇ |\n| **AIST** | **ÂèåËáÇ2DËßÜËßâËΩ®Ëøπ** | ‰ªéAISTÊï∞ÊçÆÈõÜ‰∏≠ÊèêÂèñÂèåÊâãÊú∫Âô®‰∫∫Êìç‰ΩúËΩ®ËøπÔºåÁ∫¶**9.2‰∏áÊù°Ê†∑Êú¨**„ÄÇ‰ΩøÁî®CoTracker3ËøõË°åËΩ®ËøπË∑üË∏™ÔºåÁî®‰∫éÂ≠¶‰π†ÂèåËáÇÂçèË∞É„ÄÇ |\n| **PixMo** | ÈÄöÁî®ËßÜËßâ-ËØ≠Ë®ÄÈóÆÁ≠î | Á∫¶72.6‰∏áÊ†∑Êú¨ÔºåÁî®‰∫é‰øùÊåÅÊ®°ÂûãÁöÑ**ÈÄöÁî®ËßÜËßâÁêÜËß£ËÉΩÂäõ**ÔºåÈò≤Ê≠¢Âú®ÂÖ∑Ë∫´Êï∞ÊçÆ‰∏äËÆ≠ÁªÉÊó∂ÂèëÁîüÁÅæÈöæÊÄßÈÅóÂøò„ÄÇ |\n| **RoboFAC** | Êú∫Âô®‰∫∫Â§±Ë¥•ÂàÜÊûê‰∏éÁ∫†Ê≠£QA | ÂåÖÂê´9,440Êù°ÈîôËØØÊìç‰ΩúËΩ®ËøπÂíå6.4‰∏áQAÂØπÔºåÁî®‰∫éËÆ≠ÁªÉÊ®°ÂûãÁöÑ**Â§±Ë¥•ËØÜÂà´„ÄÅÂéüÂõ†ÂàÜÊûêÂíåÊÅ¢Â§çËßÑÂàí**ËÉΩÂäõ„ÄÇ |\n| **RoboVQA** | Êú∫Âô®‰∫∫Êìç‰ΩúËßÜÈ¢ëQA | Á∫¶79.8‰∏áQAÂØπÔºåÊ∂µÁõñ‰ªªÂä°ÁêÜËß£„ÄÅÁ©∫Èó¥Êé®ÁêÜÂíåÂä®‰ΩúÈ¢ÑÊµãÔºåÁî®‰∫éÊèêÂçáÈïøËßÜÈáéÊé®ÁêÜËÉΩÂäõ„ÄÇ |\n| **ShareRobot** | Â§ßËßÑÊ®°Êú∫Âô®‰∫∫‰ªªÂä°ËßÑÂàíQA | Ë∂ÖËøá**100‰∏áQAÂØπ**ÔºåÊ∂µÁõñ‰ªªÂä°ËßÑÂàí„ÄÅÁâ©‰ΩìÂäüËÉΩ„ÄÅÊìç‰ΩúÁ≠ñÁï•Ôºå‰øÉËøõÂèØËøÅÁßªÊìç‰ΩúÁü•ËØÜÁöÑÂ≠¶‰π†„ÄÇ |\n| **EgoPlan-Bench** | Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ë‰ªªÂä°ËßÑÂàí | Á∫¶5.3‰∏áËßÜÈ¢ë-ÊñáÊú¨ÂØπÔºåÁî®‰∫éËÆ≠ÁªÉ‰ªéÁ¨¨‰∏Ä‰∫∫Áß∞ËßÜËßíËøõË°å**ÈïøËßÜÈáéËßÑÂàíÂíåËøõÂ∫¶Ë∑üË∏™**ÁöÑËÉΩÂäõ„ÄÇ |\n| **Video-R1-CoT** | Â∏¶ÊÄùÁª¥ÈìæÊ≥®ÈáäÁöÑËßÜÈ¢ëQA | 16.5‰∏áÊ†∑Êú¨ÔºåÊ∂µÁõñÊï∞Â≠¶ÈÄªËæë„ÄÅÁ©∫Èó¥ÁêÜËß£Á≠âÂ§çÊùÇÊé®ÁêÜÔºåÁî®‰∫éÊ≥®ÂÖ•**ÈìæÂºèÊÄùÁª¥Êé®ÁêÜËÉΩÂäõ**„ÄÇ |\n| **OXE (Âä®‰ΩúÊï∞ÊçÆ)** | Êú∫Âô®‰∫∫ÊºîÁ§∫Âä®‰ΩúÊï∞ÊçÆ | Áî®‰∫é**Êé®ÁêÜÂ¢ûÂº∫ÁöÑÁ≠ñÁï•Â≠¶‰π†**Èò∂ÊÆµÔºåËÆ≠ÁªÉÂä®‰ΩúÊ®°ÂûãÁîüÊàê‰ΩéÂ±ÇÊéßÂà∂Êåá‰ª§„ÄÇ |\n| **ÈùôÊÄÅALOHAÊï∞ÊçÆÈõÜ** | ÂèåËáÇÊú∫Âô®‰∫∫ÊºîÁ§∫Êï∞ÊçÆ | ‰∏éOXEÊï∞ÊçÆÁªìÂêàÔºåÁî®‰∫éËÆ≠ÁªÉÂú®LIBEROÂíåRoboTwin2.0‰∏äËØÑ‰º∞ÁöÑRDTÂä®‰ΩúÊ®°Âûã„ÄÇ |\n\n**ËÆ≠ÁªÉÊµÅÁ®ã**Ôºö\n1.  **ÁõëÁù£ÂæÆË∞É**ÔºöÊ∑∑Âêà‰∏äËø∞Â§öÁßçÊï∞ÊçÆÔºàÁ∫¶400‰∏áÊ†∑Êú¨ÔºâÔºåÂ≠¶‰π†Âü∫Á°ÄËßÜËßâÁêÜËß£ÂíåÊìç‰ΩúÁü•ËØÜ„ÄÇ\n2.  **ÊÄùÁª¥ÈìæÂæÆË∞É**Ôºö‰ªéSFTÊï∞ÊçÆ‰∏≠ÈááÊ†∑5%ÔºåÂπ∂Âä†ÂÖ•Video-R1-CoTÊï∞ÊçÆÔºåÂ≠¶‰π†ÁîüÊàêÁªìÊûÑÂåñÊé®ÁêÜ„ÄÇ\n3.  **Â∏àÁîüËÆ≠ÁªÉ**ÔºöÂú®CoT-SFTÊ£ÄÊü•ÁÇπÂü∫Á°Ä‰∏äÔºå‰ΩøÁî®Âπ≥Ë°°ÈááÊ†∑ÁöÑÁ∫¶5‰∏áÊ†∑Êú¨ÔºåËøõË°åGRPOÊïôÂ∏àËÆ≠ÁªÉÂíåÊΩúÂú®Á©∫Èó¥Â≠¶ÁîüËí∏È¶è„ÄÇ\n\n#### 2. ËØÑ‰º∞Âü∫ÂáÜÊï∞ÊçÆÈõÜ\nÁî®‰∫éÂÖ®Èù¢ÊµãËØïÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂÖ∑‰ΩìÂ¶Ç‰∏ãÔºö\n\n| Âü∫ÂáÜÂêçÁß∞ | ËØÑ‰º∞ÈáçÁÇπ | ‰ªªÂä°ÁªÜËäÇ‰∏éÊåáÊ†á |\n| :--- | :--- | :--- |\n| **SimplerEnv-Google** | **Êìç‰ΩúÊ≥õÂåñ‰∏é‰ªøÁúüÂà∞ÁúüÂÆûÂÖ≥ËÅî** | Ê®°ÊãüGoogleÊú∫Âô®‰∫∫‰ªªÂä°ÔºåÂåÖÂê´ÂÖâÁÖß„ÄÅÁ∫πÁêÜ„ÄÅËÉåÊôØ„ÄÅÁõ∏Êú∫ËßÜËßíÁöÑÂ§öÊ†∑Âåñ„ÄÇËØÑ‰º∞**‰ªªÂä°ÊàêÂäüÁéá**ÔºåËØ•Âü∫ÂáÜ‰∏éÁúüÂÆû‰∏ñÁïåÊÄßËÉΩÊúâÂº∫Áõ∏ÂÖ≥ÊÄß„ÄÇ |\n| **LIBERO** | **Â§öÊ†∑ÂåñÊìç‰ΩúÊ≥õÂåñ** | ÂåÖÂê´Âõõ‰∏™Â≠êÂ•ó‰ª∂Ôºö<br>‚Ä¢ **Spatial**ÔºöÁ©∫Èó¥Â∏ÉÂ±ÄÂèòÂåñ„ÄÇ<br>‚Ä¢ **Object**ÔºöÁâ©‰ΩìÂ§öÊ†∑ÊÄß„ÄÇ<br>‚Ä¢ **Goal**ÔºöÁõÆÊ†áÂèòÂåñ„ÄÇ<br>‚Ä¢ **Long**ÔºöÈïøËßÜÈáé„ÄÅÊ∑∑ÂêàÂèòÂåñÁöÑËßÑÂàí‰ªªÂä°„ÄÇ<br>ÊØè‰∏™Â•ó‰ª∂ËøõË°å500Ê¨°ËØïÈ™åÔºåËØÑ‰º∞**‰ªªÂä°ÊàêÂäüÁéá**„ÄÇ |\n| **RoboTwin2.0** | **Â§çÊùÇÂèåËáÇÊìç‰Ωú‰∏éÈïøËßÜÈáéËßÑÂàí** | ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂèåËáÇÊìç‰ΩúÂü∫ÂáÜ„ÄÇÊèê‰æõ**ÁÆÄÂçï**Âíå**Âõ∞Èöæ**ÔºàÂê´È¢ÜÂüüÈöèÊú∫ÂåñÔºâ‰∏§ÁßçËÆæÁΩÆ„ÄÇ‰ªªÂä°Ê†πÊçÆÊºîÁ§∫ÈïøÂ∫¶ÂàÜ‰∏∫**Áü≠„ÄÅ‰∏≠„ÄÅÈïø**ËßÜÈáé„ÄÇËØÑ‰º∞**‰ªªÂä°ÊàêÂäüÁéá**„ÄÇ |\n| **EgoPlan-Bench2** | **Ëá™Êàë‰∏≠ÂøÉÈïøËßÜÈáé‰ªªÂä°ËßÑÂàí** | ÂåÖÂê´24‰∏™Êó•Â∏∏ÁîüÊ¥ªÂú∫ÊôØÔºå1,321ÈÅìÂ§öÈ°πÈÄâÊã©È¢ò„ÄÇËØÑ‰º∞Ê®°ÂûãÂú®ÁªôÂÆöÁõÆÊ†áÂíåÂéÜÂè≤ËøõÂ∫¶‰∏ãÈ¢ÑÊµã‰∏ã‰∏ÄÊ≠•ÁöÑ**ÂáÜÁ°ÆÁéá**„ÄÇ |\n| **RoboVQA** | **Êú∫Âô®‰∫∫Êìç‰ΩúËßÜÈ¢ëÁöÑËßÜËßâÊé®ÁêÜ** | ÂåÖÂê´Êù•Ëá™Êú∫Âô®‰∫∫Âíå‰∫∫Á±ªÊºîÁ§∫ÁöÑ1,893‰∏™Ëá™Áî±ÂΩ¢ÂºèQAÂØπ„ÄÇ‰ΩøÁî®**BLEUÂàÜÊï∞**ËØÑ‰º∞ÁîüÊàêÁ≠îÊ°àÁöÑË¥®Èáè„ÄÇ |\n| **OpenEQA** | **ÂºÄÊîæ‰∏ñÁïåÂú∫ÊôØÁöÑÈõ∂Ê†∑Êú¨Á©∫Èó¥‰∏éÂäüËÉΩÁêÜËß£** | ÂåÖÂê´Ë∂ÖËøá1,600‰∏™ÈóÆÈ¢òÔºåÊ∂µÁõñ180Â§ö‰∏™ÁúüÂÆû‰∏ñÁïåÁéØÂ¢É„ÄÇ‰ΩøÁî®**LLMÂü∫‰∫éËØÑÂàÜ**ËØÑ‰º∞Á≠îÊ°àÁöÑÂêàÁêÜÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ |\n| **RoboFAC** | **Â§±Ë¥•ËØÜÂà´‰∏éÁ∫†Ê≠£ËÉΩÂäõ** | ‰∏ìÈó®ËØÑ‰º∞ÂØπÊìç‰ΩúÂ§±Ë¥•ÁöÑÂàÜÊûêÂíåÁ∫†Ê≠£ËßÑÂàíËÉΩÂäõ„ÄÇÂú®**‰ªøÁúü**Âíå**ÁúüÂÆûÊú∫Âô®‰∫∫**‰∏§‰∏™ÂàÜÈõÜ‰∏äÔºå‰ΩøÁî®**LLMËØÑÂàÜ**ËøõË°åËØÑ‰º∞„ÄÇ |\n\n### ÊÄªÁªì\nFast-ThinkActËÆ∫ÊñáÈÄöËøáÁ≤æÂøÉËÆæËÆ°Ôºå‰ΩøÁî®‰∫Ü**Ë¶ÜÁõñÊú∫Âô®‰∫∫Êìç‰ΩúÊàêÂäüÁéáÂíåÂ§öÁª¥Â∫¶ÂÖ∑Ë∫´Êé®ÁêÜËÉΩÂäõÁöÑÁªºÂêàËØÑ‰ª∑‰ΩìÁ≥ª**ÔºåÂπ∂Âú®**Â§ßËßÑÊ®°„ÄÅÂ§öÊ†∑ÂåñÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜ**‰∏äËøõË°åÂ≠¶‰π†ÔºåÊúÄÁªàÂú®**‰∏ÄÁ≥ªÂàóÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊ†áÂáÜÂü∫ÂáÜ**‰∏äÈ™åËØÅ‰∫ÜÂÖ∂ÊñπÊ≥ïÂú®**ÊÄßËÉΩ„ÄÅÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ**‰∏äÁöÑÊòæËëó‰ºòÂäø„ÄÇÂÖ∂ËØÑ‰ª∑ÊåáÊ†á‰∏ç‰ªÖÂÖ≥Ê≥®‚ÄúÂÅöÂæóÂØπ‚ÄùÔºàÊàêÂäüÁéá„ÄÅÂáÜÁ°ÆÁéáÔºâÔºå‰πüÂÖ≥Ê≥®‚ÄúÂÅöÂæóÂø´‚ÄùÔºàÊé®ÁêÜÂª∂ËøüÔºâÔºåÂπ∂Ê∑±ÂÖ•Ê£ÄÈ™å‰∫ÜÊ®°ÂûãÁöÑÊ†∏ÂøÉ‰ª∑ÂÄº‚Äî‚Äî**ÈïøËßÜÈáéËßÑÂàí„ÄÅÂ∞ëÊ†∑Êú¨ÈÄÇÂ∫îÂíåÂ§±Ë¥•ÊÅ¢Â§ç**ËÉΩÂäõ„ÄÇ",
      "timestamp": "2026-01-20T18:01:56.003666",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "ËÆ∫ÊñáÂú®Âì™‰∫õÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºü‰∏ªË¶ÅÁöÑËØÑ‰º∞ÊåáÊ†áÂíåÊÄßËÉΩÊèêÂçáÊòØÂ§öÂ∞ëÔºü",
      "answer": "## ËÆ∫ÊñáÂÆûÈ™åÊï∞ÊçÆÈõÜ„ÄÅËØÑ‰º∞ÊåáÊ†á‰∏éÊÄßËÉΩÊèêÂçáÂàÜÊûê\n\n### ‰∏Ä„ÄÅ ÂÆûÈ™åÊï∞ÊçÆÈõÜ\nËÆ∫ÊñáÂú®‰∏§Â§ßÁ±ªÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºö**Êú∫Âô®‰∫∫Êìç‰ΩúÊï∞ÊçÆÈõÜ**Âíå**ÂÖ∑Ë∫´Êé®ÁêÜÊï∞ÊçÆÈõÜ**„ÄÇ\n\n#### 1. Êú∫Âô®‰∫∫Êìç‰ΩúÊï∞ÊçÆÈõÜÔºàÁî®‰∫éËÆ≠ÁªÉ‰∏éËØÑ‰º∞Á≠ñÁï•Ôºâ\n- **ËÆ≠ÁªÉÊï∞ÊçÆ**Ôºö\n    - **OXE (Open X-Embodiment)**ÔºöÂ§ßËßÑÊ®°Êú∫Âô®‰∫∫ÊºîÁ§∫Êï∞ÊçÆÈõÜÔºåÁî®‰∫éÂü∫Á°ÄÁ≠ñÁï•Â≠¶‰π†„ÄÇ\n    - **ÈùôÊÄÅALOHAÊï∞ÊçÆÈõÜ**ÔºöÁî®‰∫éÂ¢ûÂº∫ÂèåÊâãÊú∫Âô®‰∫∫Êìç‰ΩúËÉΩÂäõ„ÄÇ\n    - **AISTÊï∞ÊçÆÈõÜ**ÔºöÊèê‰æõÁ∫¶92KÊù°ÂèåÊâãÊú∫Âô®‰∫∫ËßÜËßâËΩ®Ëøπ„ÄÇ\n    - **MolmoActÊ†áÊ≥®ÁöÑ2DËßÜËßâËΩ®Ëøπ**Ôºö‰ªéOXE‰∏≠ÊèêÂèñÔºåÁ∫¶130‰∏áÊù°ÂçïËáÇÊìç‰ΩúËΩ®Ëøπ„ÄÇ\n- **ËØÑ‰º∞Âü∫ÂáÜ**Ôºö\n    - **LIBERO**ÔºöÊ∂µÁõñÁ©∫Èó¥„ÄÅÁâ©‰Ωì„ÄÅÁõÆÊ†áÂíåÈïøËßÜÈáé‰ªªÂä°ÔºåËØÑ‰º∞Â§öÊ†∑ÂåñÊìç‰ΩúÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ\n    - **SimplerEnv-Google**ÔºöÂÖ∑ÊúâÂº∫‰ªøÁúüÂà∞ÁúüÂÆû‰∏ñÁïåÁõ∏ÂÖ≥ÊÄßÁöÑÂü∫ÂáÜÔºåÂåÖÂê´ÂÖâÁÖß„ÄÅÂ§ñËßÇÂíåËßÜËßíÂèòÂåñ„ÄÇ\n    - **RoboTwin2.0**ÔºöÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ**ÂèåÊâãÊú∫Âô®‰∫∫ÈïøËßÜÈáéÊìç‰Ωú**Âü∫ÂáÜÔºåÂåÖÂê´ÁÆÄÂçïÂíåÂõ∞ÈöæÔºàÂ∏¶ÂüüÈöèÊú∫ÂåñÔºâËÆæÁΩÆ„ÄÇ\n\n#### 2. ÂÖ∑Ë∫´Êé®ÁêÜÊï∞ÊçÆÈõÜÔºàÁî®‰∫éËÆ≠ÁªÉ‰∏éËØÑ‰º∞Êé®ÁêÜËÉΩÂäõÔºâ\n- **ËÆ≠ÁªÉÊï∞ÊçÆ**Ôºö\n    - **PixMo**ÔºöÈÄöÁî®ËßÜËßâËØ≠Ë®ÄÊï∞ÊçÆÈõÜÔºåÁî®‰∫é‰øùÊåÅÂü∫Á°ÄËßÜËßâÁêÜËß£ËÉΩÂäõ„ÄÇ\n    - **RoboFAC**ÔºöÊú∫Âô®‰∫∫Â§±Ë¥•ÂàÜÊûêÊï∞ÊçÆÈõÜÔºåÂåÖÂê´ÈîôËØØÊìç‰ΩúËΩ®ËøπÂíåQAÂØπ„ÄÇ\n    - **RoboVQA**ÔºöÊú∫Âô®‰∫∫Êìç‰ΩúËßÜÈ¢ëÁöÑÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºåÊ∂µÁõñ‰ªªÂä°ÁêÜËß£„ÄÇ\n    - **ShareRobot**ÔºöÂ§ßËßÑÊ®°Êú∫Âô®‰∫∫‰ªªÂä°ËßÑÂàí‰∏éÊìç‰ΩúÁü•ËØÜÊï∞ÊçÆÈõÜ„ÄÇ\n    - **EgoPlan-Bench**Ôºö‰ª•Ëá™Êàë‰∏∫‰∏≠ÂøÉËßÜËßíÁöÑÊó•Â∏∏Ê¥ªÂä®ËßÑÂàíÊï∞ÊçÆÈõÜ„ÄÇ\n    - **Video-R1-CoT**ÔºöÂ∏¶ÊúâÊÄùÁª¥ÈìæÊ†áÊ≥®ÁöÑËßÜÈ¢ëÈóÆÁ≠îÊï∞ÊçÆÈõÜ„ÄÇ\n- **ËØÑ‰º∞Âü∫ÂáÜ**Ôºö\n    - **EgoPlan-Bench2**ÔºöËØÑ‰º∞‰ª•Ëá™Êàë‰∏∫‰∏≠ÂøÉËßÜËßíÁöÑÂ§öÊ≠•‰ªªÂä°ËßÑÂàíËÉΩÂäõÔºàÂ§öÈÄâÈ¢òÂáÜÁ°ÆÁéáÔºâ„ÄÇ\n    - **RoboVQA**ÔºöËØÑ‰º∞Êú∫Âô®‰∫∫Êìç‰ΩúÂú∫ÊôØ‰∏≠ÁöÑËßÜËßâÊé®ÁêÜËÉΩÂäõÔºàBLEUÂàÜÊï∞Ôºâ„ÄÇ\n    - **OpenEQA**ÔºöËØÑ‰º∞ÂØπÂ§öÊ†∑ÂåñÁúüÂÆû‰∏ñÁïåÁéØÂ¢ÉÁöÑÈõ∂Ê†∑Êú¨Á©∫Èó¥‰∏éÂäüËÉΩÁêÜËß£ÔºàLLMËØÑÂàÜÔºâ„ÄÇ\n    - **RoboFAC**Ôºö‰∏ìÈó®ËØÑ‰º∞**Â§±Ë¥•ËØÜÂà´‰∏éÊÅ¢Â§ç**ËÉΩÂäõ„ÄÇ\n\n### ‰∫å„ÄÅ ‰∏ªË¶ÅËØÑ‰º∞ÊåáÊ†á\n1.  **Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°**Ôºö\n    - **Ê†∏ÂøÉÊåáÊ†á**Ôºö**‰ªªÂä°ÊàêÂäüÁéá**„ÄÇ\n    - **ÊïàÁéáÊåáÊ†á**Ôºö**Êé®ÁêÜÂª∂Ëøü**ÔºàÊØ´ÁßíÔºâÔºåÂØπÊØîÂü∫ÂáÜÊ®°ÂûãÁöÑÂä†ÈÄüÂÄçÊï∞„ÄÇ\n\n2.  **ÂÖ∑Ë∫´Êé®ÁêÜ‰ªªÂä°**Ôºö\n    - **EgoPlan-Bench2**Ôºö**ÂáÜÁ°ÆÁéá**„ÄÇ\n    - **RoboVQA**Ôºö**BLEUÂàÜÊï∞**ÔºàB-1, B-2, B-3, B-4ÂèäÂπ≥ÂùáÔºâ„ÄÇ\n    - **OpenEQA & RoboFAC**Ôºö‰ΩøÁî®**LLMËøõË°åËØÑÂàÜ**Ôºà‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÔºâ„ÄÇ\n\n### ‰∏â„ÄÅ ÂÖ≥ÈîÆÊÄßËÉΩÊèêÂçá\nFast-ThinkActÂú®**‰øùÊåÅÁîöËá≥ÊèêÂçáÊÄßËÉΩ**ÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫Ü**ÊòæËëóÁöÑÊïàÁéáÊèêÂçá**„ÄÇ\n\n#### 1. ÊÄßËÉΩÊèêÂçáÔºàÊïàÊûúÔºâ\n- **Êú∫Âô®‰∫∫Êìç‰Ωú**Ôºö\n    - Âú®**LIBERO**Âü∫ÂáÜ‰∏äÔºåFast-ThinkAct-3BËææÂà∞**89.7%** ÊàêÂäüÁéáÔºå‰ºò‰∫éThinkAct-3BÁöÑ83.1%ÂíåMolmoAct-7BÁöÑ86.8%„ÄÇ\n    - Âú®**SimplerEnv-Google**‰∏äËææÂà∞**68.7%** ÊàêÂäüÁéáÔºå‰ºò‰∫éThinkAct-7BÁöÑ68.3%„ÄÇ\n    - Âú®**RoboTwin2.0**Âõ∞ÈöæËÆæÁΩÆÁöÑÈïøËßÜÈáé‰ªªÂä°‰∏äÔºåÂπ≥ÂùáÊàêÂäüÁéáÊØîThinkActÈ´ò**1.7%**ÔºåÊØîÂü∫Á°ÄRDTÈ´ò**3.6%**„ÄÇ\n- **ÂÖ∑Ë∫´Êé®ÁêÜ**Ôºö\n    - Âú®**EgoPlan-Bench2**‰∏äÔºåFast-ThinkAct-3BÂπ≥ÂùáÂáÜÁ°ÆÁéá**46.4%**Ôºå‰ºò‰∫éThinkAct-3BÁöÑ44.0%„ÄÇ\n    - Âú®**RoboVQA**‰∏äÔºåÂπ≥ÂùáBLEUÂàÜÊï∞ËææÂà∞**60.8**ÔºåÂ§ßÂπÖ‰ºò‰∫éThinkAct-3BÁöÑ55.3„ÄÇ\n    - Âú®**RoboFAC**ÁúüÂÆû‰∏ñÁïåÂàÜÂâ≤‰∏äÔºåËØÑÂàÜÊØîÊúÄ‰Ω≥Âü∫Á∫øÔºàRoboFAC-3BÔºâÈ´ò**16.4ÂàÜ**ÔºåÂ±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÂ§±Ë¥•ÊÅ¢Â§çËÉΩÂäõ„ÄÇ\n\n#### 2. ÊïàÁéáÊèêÂçáÔºàÈÄüÂ∫¶Ôºâ\n- **Ê†∏ÂøÉ‰ºòÂäø**ÔºöÂÆûÁé∞‰∫Ü**ÊûÅËá¥ÁöÑÊé®ÁêÜÂä†ÈÄü**„ÄÇ\n    - Áõ∏ÊØîThinkAct-7BÔºåÊé®ÁêÜÂª∂ËøüÈôç‰Ωé **89.3%**„ÄÇ\n    - Áõ∏ÊØîThinkAct-3BÔºåÊé®ÁêÜÈÄüÂ∫¶Âø´ **7ÂÄç**Ôºà805ms vs. 5674msÔºâ„ÄÇ\n    - Áõ∏ÊØîMolmoAct-7BÔºåÊé®ÁêÜÂª∂ËøüÈôç‰Ωé **88.0%**„ÄÇ\n- **ÂÖ≥ÈîÆÂØπÊØî**ÔºöFast-ThinkAct‰ªÖ‰ΩøÁî®**6‰∏™ÊΩúÂú®Êé®ÁêÜ‰ª§Áâå**Âíå**5‰∏™Á©∫Èó¥‰ª§Áâå**ËøõË°åÂπ∂Ë°åËΩ®ËøπÈ¢ÑÊµãÔºåÊõø‰ª£‰∫ÜThinkActÈúÄË¶ÅÁîüÊàêÁöÑÁ∫¶**250‰∏™ÊñáÊú¨‰ª§Áâå**ÁöÑÂÜóÈïøÊÄùÁª¥ÈìæÔºåËøôÊòØÊïàÁéáÊèêÂçáÁöÑÊ†πÊú¨ÂéüÂõ†„ÄÇ\n\n#### 3. ÁªºÂêàËÉΩÂäõÊèêÂçá\n- **ÈïøËßÜÈáéËßÑÂàí**ÔºöÂú®RoboTwin2.0ÁöÑÈïøÊ≠•È™§‰ªªÂä°‰∏äË°®Áé∞‰ºò‰∫éÊâÄÊúâÂü∫Á∫ø„ÄÇ\n- **Â∞ëÊ†∑Êú¨ÈÄÇÂ∫î**Ôºö‰ªÖÁî®10Êù°ÊºîÁ§∫ËøõË°åÂæÆË∞ÉÂêéÔºåÂú®RoboTwin2.0‰∏äÊòæËëó‰ºò‰∫éÂÖ∂‰ªñVLAÊ®°Âûã„ÄÇ\n- **Â§±Ë¥•ÊÅ¢Â§ç**ÔºöÂú®RoboFACÂü∫ÂáÜ‰∏äÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈîôËØØËØÜÂà´ÂíåÁ∫†Ê≠£ËßÑÂàíËÉΩÂäõ„ÄÇ\n\n**ÊÄªÁªì**ÔºöFast-ThinkActÈÄöËøáÂ∞ÜÊòæÂºèÊñáÊú¨Êé®ÁêÜ‚ÄúÂéãÁº©‚Äù‰∏∫ÂèØË®ÄËØ≠ÂåñÁöÑÊΩúÂú®Ë°®Á§∫ÔºåÂú®Â§ö‰∏™Êú∫Âô®‰∫∫Êìç‰ΩúÂíåÂÖ∑Ë∫´Êé®ÁêÜÂü∫ÂáÜ‰∏äÂÆûÁé∞‰∫Ü **SOTAÊàñÊé•ËøëSOTAÁöÑÊÄßËÉΩ**ÔºåÂêåÊó∂Â∞ÜÊé®ÁêÜÈÄüÂ∫¶ÊèêÂçá‰∫Ü‰∏Ä‰∏™Êï∞ÈáèÁ∫ßÔºåÊàêÂäüËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊé®ÁêÜVLAÊ®°Âûã**Âª∂ËøüÈ´ò**ÁöÑÊ†∏ÂøÉÁì∂È¢à„ÄÇ",
      "timestamp": "2026-01-20T18:02:35.621308",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "ËÆ∫ÊñáÁöÑ‰∏ªË¶ÅÂ±ÄÈôêÊÄßÊúâÂì™‰∫õÔºüÊú™Êù•ÂèØËÉΩÁöÑÊîπËøõÊñπÂêëÊòØ‰ªÄ‰πàÔºü",
      "answer": "## ËÆ∫Êñá‰∏ªË¶ÅÂ±ÄÈôêÊÄßÂàÜÊûê\n\nÊ†πÊçÆËÆ∫ÊñáÂÜÖÂÆπÔºåFast-ThinkAct ÁöÑ‰∏ªË¶ÅÂ±ÄÈôêÊÄßÈõÜ‰∏≠Âú®**ÂèØËß£ÈáäÊÄßÁªÑ‰ª∂ÔºàVerbalizerÔºâÁöÑÂèØÈù†ÊÄß**‰∏äÔºö\n\n- **ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂõ∫ÊúâÈóÆÈ¢ò**ÔºöËÆ∫ÊñáÊòéÁ°ÆÊåáÂá∫ÔºåÁî®‰∫éÂ∞ÜÊΩúÂú®Êé®ÁêÜËß£Á†Å‰∏∫ÂèØËØªÊñáÊú¨ÁöÑ `Verbalizer` ÊòØÂü∫‰∫éÈ¢ÑËÆ≠ÁªÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊûÑÂª∫ÁöÑ„ÄÇÂõ†Ê≠§ÔºåÂÆÉ**‰∏çÂèØÈÅøÂÖçÂú∞ÁªßÊâø‰∫ÜLLMÁöÑÂ∏∏ËßÅÁº∫Èô∑**Ôºå‰æãÂ¶Ç**ÂπªËßâ**‚Äî‚ÄîÂç≥ÁîüÊàêÁúã‰ººÂêàÁêÜ‰ΩÜÂÆûÈôÖ‰∏çÂáÜÁ°ÆÊàñ‰∏éÊΩúÂú®Ë°®Á§∫‰∏çÁ¨¶ÁöÑÊèèËø∞„ÄÇ\n- **ÂØπÊ†∏ÂøÉÂäüËÉΩÊó†ÂΩ±Âìç**ÔºöÂÄºÂæóÂº∫Ë∞ÉÁöÑÊòØÔºåËøô‰∏ÄÂ±ÄÈôêÊÄß**Âπ∂‰∏çÂΩ±ÂìçÊ®°ÂûãÂú®Êé®ÁêÜÊó∂ÁöÑÂä®‰ΩúÊâßË°åÊÄßËÉΩ**„ÄÇÂõ†‰∏∫ `Verbalizer` ‰ªÖÂú®ËÆ≠ÁªÉÈò∂ÊÆµÁî®‰∫éÊåáÂØºÊΩúÂú®Ë°®Á§∫ÁöÑÂ≠¶‰π†Ôºå‰ª•ÂèäÂú®ÈúÄË¶ÅÊó∂ÂèØÊèê‰æõËß£ÈáäÊÄßËæìÂá∫„ÄÇÂä®‰ΩúÈ¢ÑÊµãÂÆåÂÖ®‰æùËµñ‰∫éÁªèËøáËßÜËßâËßÑÂàíËí∏È¶èÂæóÂà∞ÁöÑ„ÄÅÊõ¥ÂèØÈù†ÁöÑ**ÊΩúÂú®Ë°®Á§∫**Ôºå`Verbalizer` Âπ∂ÈùûÊé®ÁêÜÊµÅÊ∞¥Á∫øÁöÑÂøÖÈúÄÈÉ®ÂàÜ„ÄÇ\n\n## Êú™Êù•ÂèØËÉΩÁöÑÊîπËøõÊñπÂêë\n\nËÆ∫ÊñáÈíàÂØπ‰∏äËø∞Â±ÄÈôêÊÄßÔºåÊèêÂá∫‰∫ÜÊòéÁ°ÆÁöÑÊú™Êù•Â∑•‰ΩúÊñπÂêëÔºåÂπ∂ÈöêÂê´‰∫ÜÂÖ∂‰ªñÊΩúÂú®‰ºòÂåñÁÇπÔºö\n\n1. **ÊèêÂçá Verbalizer ÁöÑÂø†ÂÆûÂ∫¶**Ôºö\n   - **ÂºïÂÖ• grounding-aware ÁõÆÊ†á**ÔºöÂú®ËÆ≠ÁªÉ `Verbalizer` Êó∂ÔºåÂèØ‰ª•Â¢ûÂä†‰ΩøÂÖ∂ËæìÂá∫‰∏éËßÜËßâËßÇÂØüÊàñÂä®‰ΩúËΩ®ËøπÊõ¥Á¥ßÂØÜÂØπÈΩêÁöÑÁ∫¶ÊùüÊàñÊçüÂ§±ÂáΩÊï∞„ÄÇ\n   - **ÈááÁî®ÂπªËßâÊäëÂà∂ÊäÄÊúØ**ÔºöÂÄüÈâ¥Â§ßËØ≠Ë®ÄÊ®°ÂûãÈ¢ÜÂüüÁöÑÁ†îÁ©∂ÔºåÂ∫îÁî®‰∏ìÈó®ÁöÑÊäÄÊúØÊù•ÂáèÂ∞ëÊàñÊ£ÄÊµã `Verbalizer` ËæìÂá∫‰∏≠ÁöÑ‰∏çÂÆû‰ø°ÊÅØ„ÄÇ\n\n2. **ÈöêÂê´ÁöÑÊâ©Â±ï‰∏é‰ºòÂåñÊñπÂêë**ÔºàÂü∫‰∫éÊñπÊ≥ïËÆæËÆ°ÔºâÔºö\n   - **Ê®°ÂûãÊû∂ÊûÑ‰∏éÊïàÁéáÁöÑËøõ‰∏ÄÊ≠•Êé¢Á¥¢**ÔºöÂΩìÂâçÊñπÊ≥ïÂ∞ÜÊé®ÁêÜÂéãÁº©‰∏∫6‰∏™ÊΩúÂú®tokenÔºåÊú™Êù•ÂèØ‰ª•Á†îÁ©∂**Ëá™ÈÄÇÂ∫îÈïøÂ∫¶ÁöÑÊΩúÂú®Êé®ÁêÜ**ÔºåÊàñÊé¢Á¥¢Êõ¥È´òÊïàÁöÑÁºñÁ†ÅÊû∂ÊûÑÔºåÂú®‰øùÊåÅÊÄßËÉΩÁöÑÂêåÊó∂Ëøõ‰∏ÄÊ≠•Èôç‰ΩéÂª∂Ëøü„ÄÇ\n   - **ËÆ≠ÁªÉÁ≠ñÁï•ÁöÑÂ¢ûÂº∫**ÔºöËÆ∫Êñá‰ΩøÁî®‰∫ÜÂÅèÂ•ΩËí∏È¶èÔºàDPOÈ£éÊ†ºÔºâÂíåËΩ®ËøπÂØπÈΩê„ÄÇÊú™Êù•ÂèØ‰ª•Êé¢Á¥¢**Êõ¥Â§çÊùÇÁöÑÂ•ñÂä±ËÆæËÆ°**Êàñ**Â§ö‰ªªÂä°ËÅîÂêà‰ºòÂåñ**Ôºå‰ª•Êõ¥Â•ΩÂú∞Âπ≥Ë°°Êé®ÁêÜË¥®Èáè„ÄÅÂéãÁº©ÁéáÂíåÂä®‰ΩúÊÄßËÉΩ„ÄÇ\n   - **Êõ¥ÂπøÊ≥õÁöÑ‰ªªÂä°‰∏éÂú∫ÊôØÈ™åËØÅ**ÔºöÂΩìÂâçÂ∑•‰ΩúÂú®Â§ö‰∏™‰ªøÁúüÂü∫ÂáÜ‰∏äËøõË°å‰∫ÜÈ™åËØÅ„ÄÇÊú™Êù•ÁöÑÈáçË¶ÅÊñπÂêëÊòØÂú®**ÁúüÂÆûÊú∫Âô®‰∫∫Âπ≥Âè∞**Âíå**Êõ¥Â§çÊùÇ„ÄÅÂä®ÊÄÅÁöÑÁúüÂÆû‰∏ñÁïåÂú∫ÊôØ**‰∏≠ÊµãËØïÂÖ∂ÊïàÁéá‰∏éÈ≤ÅÊ£íÊÄßÔºåËøôÂØπÂÆûÈôÖÈÉ®ÁΩ≤Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ\n   - **Ê∂àÈô§ÂØπÊïôÂ∏àÊ®°ÂûãÁöÑ‰æùËµñ**ÔºöÂΩìÂâçÊñπÊ≥ï‰æùËµñ‰∫é‰∏Ä‰∏™ÁªèËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÁöÑÊïôÂ∏àÊ®°ÂûãÊù•Êèê‰æõÈ´òË¥®ÈáèÁöÑÊé®ÁêÜËΩ®Ëøπ„ÄÇÊú™Êù•ÂèØ‰ª•Á†îÁ©∂**Á´ØÂà∞Á´ØÁöÑËÆ≠ÁªÉÊ°ÜÊû∂**Êàñ**Ëá™ÁõëÁù£ÊñπÊ≥ï**Ôºå‰ª•ÂáèÂ∞ëÂØπÊòÇË¥µÊïôÂ∏àÊ®°ÂûãËÆ≠ÁªÉÁöÑ‰æùËµñ„ÄÇ\n\n**ÊÄªÁªì**ÔºöFast-ThinkAct ÁöÑÊ†∏ÂøÉÂàõÊñ∞‚Äî‚ÄîÂ∞ÜÂÜóÈïøÊé®ÁêÜÂéãÁº©‰∏∫È´òÊïàÁöÑÊΩúÂú®Ë°®Á§∫‚Äî‚ÄîÊú¨Ë∫´ÊòØÁ®≥ÂÅ•ÁöÑ„ÄÇÂÖ∂‰∏ªË¶ÅÂ±ÄÈôêÂú®‰∫éÂ§ñÂõ¥ÁöÑËß£ÈáäÊÄßÁªÑ‰ª∂ÔºåËÄåËøôÂèØ‰ª•ÈÄöËøáÁé∞ÊúâÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂØπÈΩêÊäÄÊúØËøõË°åÊîπËøõ„ÄÇÊú™Êù•ÁöÑÂ∑•‰ΩúÂ∞ÜÂõ¥ÁªïÊèêÂçáÊï¥‰∏™Á≥ªÁªüÁöÑÂø†ÂÆûÂ∫¶„ÄÅÊïàÁéáÂèäÂú®Êõ¥Â§çÊùÇÁé∞ÂÆûÂú∫ÊôØ‰∏≠ÁöÑÈÄÇÁî®ÊÄßÂ±ïÂºÄ„ÄÇ",
      "timestamp": "2026-01-20T18:02:57.293410",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    }
  ],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-14T18:59:59Z",
  "created_at": "2026-01-20T17:49:51.130239",
  "updated_at": "2026-01-20T17:49:51.130245"
}