{
  "id": "2601.09812v1",
  "title": "LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving",
  "authors": [
    "Carlo Sgaravatti",
    "Riccardo Pieroni",
    "Matteo Corno",
    "Sergio M. Savaresi",
    "Luca Magri",
    "Giacomo Boracchi"
  ],
  "abstract": "Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D.",
  "url": "https://arxiv.org/abs/2601.09812v1",
  "html_url": "https://arxiv.org/html/2601.09812v1",
  "html_content": "LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving\nâ€ \nâ€ \nthanks:\nThis paper has been accepted for publication in\nPattern Recognition\n, 2026.\nThe final version is available at\nhttps://doi.org/10.1016/j.patcog.2026.113046\n.\nCarlo Sgaravatti\ncarlo.sgaravatti@polimi.it\nRiccardo Pieroni\nriccardo.pieroni@polimi.it\nMatteo Corno\nmatteo.corno@polimi.it\nSergio M. Savaresi\nsergio.savaresi@polimi.it\nLuca Magri\nluca.magri@polimi.it\nGiacomo Boracchi\ngiacomo.boracchi@polimi.it\nAbstract\nAccurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging.\nWe propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network.\nOur solution combines two key principles: (i)\nlate fusion\n, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii)\ncascade fusion\n, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains.\nLCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from:\nhttps://github.com/CarloSgaravatti/LCF3D\n.\nkeywords:\nDeep Learning , 3D Object Detection , Multimodal , Autonomous Vehicles\nâ€ \nâ€ \njournal:\nPattern Recognition\n\\affiliation\n[label1]organization=DEIB- Dipartimento Elettronica, Informazione e Bioingegneria, Politecnico di Milano,\naddressline=Via Ponzio 34/5,\ncity=Milan,\npostcode=20133,\ncountry=Italy\n1\nIntroduction\nIn Autonomous Driving (AD), a key requirement for safe navigation is the accurate detection of small and distant objects, such as pedestrians, cyclists, and other vulnerable road users. This task is particularly challenging because autonomous vehicles must detect such objects in real time, often by combining multiple sensing modalities, each with its own limitations and latency constraints. To achieve this goal, Autonomous Vehicles (AVs) are typically equipped with complementary sensors such as LiDAR scanners and RGB cameras. LiDAR sensors provide accurate geometric information but return extremely sparse measurements for distant objects, making small targets difficult to detect and often underrepresented in AD datasets\n[\n4\n,\n1\n]\n. Conversely, RGB images offer dense semantic information that improves the recognition of small and distant objects\n[\n31\n]\n, but lack explicit depth cues, which limits precise 3D localization\n[\n20\n]\n.\nCombining both sensing modalities is crucial to achieve both accurate 3D localization and robust semantic understanding. However, effectively fusing LiDAR and RGB data without introducing substantial computational overheads that hinder real-time performance (LiDAR typically operates at 10-20 Hz) remains challenging.\nSeveral\nfusion\nstrategies have been proposed in AD to combine LiDAR and RGB information, differing in the stage where the two modalities are fused.\nEarly fusion\n[\n28\n,\n29\n]\ninjects RGB information directly into the point cloud before it is processed by the LiDAR-based detector. A particular case of this approach is\nCascade fusion\n[\n19\n]\n, which first detects 2D objects in RGB images and then generates 3D frustums from the corresponding regions to guide LiDAR processing. Although effective in reducing the 3D search space, these methods are computationally expensive, as the two modalities cannot be processed in parallel and each frustum requires separate inference, making them unsuitable for real-time applications.\nIntermediate fusion\napproaches\n[\n8\n,\n11\n]\nmerge the feature representations extracted from both modalities within a single end-to-end deep network. While they exploit rich cross-modal interactions, the joint training and feature alignment significantly increase their computational cost, preventing real-time operation.\nFinally,\nlate fusion\nmethods\n[\n17\n,\n14\n]\nprocess LiDAR and RGB data independently and combine their predictions to suppress LiDAR False Positives (FPs). The two branches can be processed in parallel, but objects missed by the LiDAR detector, i.e. False Negatives (FNs), cannot be recovered, thus small and distant targets often remain undetected\n[\n31\n]\n.\nBeyond computational challenges, a further critical issue when performing 3D object detection in real-world AD scenarios is\ndomain shift\n, which causes performance degradation when models are deployed in an environment different from the training one. Such shifts arise naturally in AD due to variations in sensor configurations, weather conditions, or geographic settings. For instance, LiDAR detectors are sensitive to changes in beam density, while RGB-based models depend on camera intrinsics and optical properties, often leading to reduced accuracy when transferred across setups\n[\n30\n]\n. Since collecting and annotating large multimodal datasets for every possible configuration is very expensive, models must be designed to generalise well across domains, which is preferred over relying on exhaustive re-training\n[\n32\n]\n.\nIn particular, we address domain generalization, aiming for models that preserve accuracy across different environments and sensor setups without requiring model fine-tuning and access to target-domain data. In our experiments, we assess domain generalization performances of LCF3D through variations in the sensing equipment, which is a practical proxy that reflects real-world changes such as hardware differences or weather conditions that commonly affect 3D object detection.\nWe propose\nLate Cascade Fusion 3D\n(LCF3D), a novel hybrid fusion framework that succesfully integrates principles from both late and cascade fusion paradigms, combining the outputs of a 2D RGB object detector and a 3D LiDAR-based object detector. In practice, LCF3D is designed to address two critical limitations of LiDAR-based 3D detection: the presence of FPs and the failure to detect small or distant objects due to Point Cloud sparsity. To this end, LCF3D introduces three key components: i) a\nBounding Box Matching\nmodule for filtering FPs and increasing the precision of the detector, ii) a\nDetection Recovery\nmodule for retrieving missed objects, increasing detection recall, and iii) a\nSemantic Fusion\nmodule for resolving label inconsistencies by favouring the semantic predictions of the RGB detector.\nMore in detail, in the Bounding Box Matching module, we project 3D bounding boxes from the LiDAR branch onto the image plane and match them with 2D detections by an optimization procedure based on Intersection over Union (IoU). LiDAR detections that do not correspond to any RGB detection are considered FPs and removed. Conversely, we consider 2D detections without a corresponding 3D bounding box as potential FNs from the LiDAR branch. Thus, in the Detection Recovery module, we backproject unmatched 2D boxes into 3D frustums where we apply an ad-hoc 3D localization model to recover the missing objects. The Semantic Fusion module finally resolves any label inconsistencies between detections from the two modalities, assigning the final label based on the class predicted by the RGB detector, which we consider more semantically reliable. Our framework, described in Section\n4\n, can work on AVs equipped with both monocular (Section\n4.1\n) and stereo images (Section\n4.2\n).\nMoreover, LCF3D includes lightweight post-processing modules that avoids significant computational overhead associated with early fusion methods, since it can conveniently run detection networks on the RGB and LiDAR data in parallel. Indeed, we exploit cascade fusion principles only for recovering missed objects from the LiDAR branch, while avoiding the substantial computational burden that characterizes traditional cascade-fusion methods that need to process all the RGB detections using frustums. Moreover, the architecture is independent from the underlying 2D and 3D detectors used, without requiring the joint training of the two detectors, enabling the use of off-the-shelf detectors (potentially trained on different single-modal datasets), resulting in great flexibility. Notably, our experiments demonstrates that LCF3D better generalizes to different domains as the 2D RGB Object Detector mitigates the impact of domain shifts due to changes in the LiDAR sensor.\nIn Section\n5\n, we test LCF3D on both KITTI\n[\n4\n]\nand nuScenes\n[\n1\n]\ndatasets, showing superior performance than LiDAR-based object detectors, especially on imbalanced classes and small objects such as\nPedestrians\nand\nCyclists\non KITTI and\nBicycles\nand\nMotorcycles\non nuScenes. Additionally, by testing models trained on KITTI and on nuScenes, and vice versa, we show that LCF3D generalizes better than other early and intermediate fusion approaches.\nOur main contributions can be summarized as follows:\n1.\nWe propose a novel hybrid LiDAR-RGB fusion method combining late and cascade fusion, that can work with both stereo images and monocular images.\n2.\nWe reduce LiDAR False Positive detections thanks to a novel Bounding Box Matching module applied to clusters of overlapping 3D bounding boxes referred to the same objects detected by the LiDAR branch.\n3.\nWe recover objects missed from LiDAR branch in our Detection Recovery module, which, in the stereo-view setting, leverages epipolar geometry principles to match pairs of 2D detections from different views.\n4.\nWe analyze the effect of domain shifts for 3D LiDAR detectors and study how fusion with 2D RGB detectors can help mitigate them.\nThis work extends our workshop paper\n[\n24\n]\n, by:\ni)\nhandling single-view RGB cameras,\nii)\nenhancing our Bounding Box Matching module by clustering bounding boxes to improve the matching of 3D detections with 2D detections,\niii)\nusing Instance Segmentation masks instead of simple 2D bounding boxes in the Detection Recovery module, to further reduce the computational overhead of cascade fusion by selecting fewer points in each frustum, and\niv)\npresenting an extended experimental validation to assess domain generalization performance.\n2\nRelated Work\n2.1\nMultimodal 3D Object Detection\nBased on how the RGB Images and LiDAR Point Clouds are fused, multimodal approaches can be divided into three categories\n[\n15\n]\n: early, intermediate and late fusion.\n2.1.1\nEarly Fusion\nIn Early Fusion approaches, RGB information is integrated into the point cloud before being processed by a LiDAR-based detector.\nMVX-Net\n[\n28\n]\nprojects 3D voxels onto images and concatenates the corresponding pixel features to voxels, while PointPainting\n[\n29\n]\nattaches semantic labels from image segmentation to each 3D point. These methods suffer from feature blurring, since identical pixel features are often assigned to multiple neighboring points. Other solutions, such as PVConvNet\n[\n10\n]\nand VirConv\n[\n33\n]\n, use depth completion to generate dense pseudo point clouds, but this dramatically increases computational cost, hindering real-time processing.\nA special case of Early Fusion is Cascade Fusion\n[\n31\n]\n, where 2D detections from RGB images define frustums that constrain the LiDAR search space\n[\n19\n]\n. Although this strategy improves localization, it is computationally expensive because each frustum requires separate 3D inference. Variants like Frustum PointPillars\n[\n16\n]\nmitigate this cost by processing all frustums jointly, but still struggle with multi-class detection and real-time constraints.\nIn our framework, we adopt a more efficient variant of cascade fusion, applying frustum-based processing only to RGB detections that are not matched with those from the LiDAR branch, thus significantly reducing the computational overhead.\n2.1.2\nIntermediate Fusion\nIntermediate Fusion solutions combine the features extracted from the single-modal backbones of 2D and 3D networks in an intermediate layer of an end-to-end trainable multimodal network. RoI-based fusion methods\n[\n2\n]\nfuse features at a Region of Interest (RoI) level, after finding an initial set of 3D proposals, e.g. from the Birdâ€™s Eye View (BEV). These solutions cannot capture cross-modality interactions in the early stages of the network\n[\n31\n]\n. Differently, recent solutions like BEVFusion\n[\n11\n]\nbuild a unified representation between LiDAR and RGB images on the BEV, which is more computationally efficient to process than voxels. However, this method is still computationally infeasible for real-time processing as it works at 8.6 FPS on nuScenes, while modern LiDAR sensors work at least at 10 Hz\n[\n20\n]\n.\n2.1.3\nLate Fusion\nLate fusion approaches employ two parallel Deep Learning branches for the two modalities, namely 3D Object Detection on the LiDAR branch and 2D Object Detection on the RGB branch, and combine their predictions in a fusion network. Typically, late fusion solutions aims at removing FP detections of the 3D LiDAR Object Detection network, by leveraging geometric and semantic consistency among detections from different modalities\n[\n17\n]\n, by adopting an additional 3D object detectors from RGB images\n[\n18\n]\n, or by projection of the 3D detections on the image plane\n[\n14\n]\n.\nUnfortunately, none of these methods address the recovery of objects missed from LiDAR detectors, which frequently occur for small, distant objects such as pedestrians and cyclists.\nOur method, in contrast, is designed to recover accurate 3D detections of all the objects returned by the 2D object detection networks.\n2.2\nDomain Adaptation and Generalization\nLiDAR-based detectors are highly sensitive to domain shifts, such as changes in point-cloud sparsity across different sensors or vehicle sizes across different geographic regions. Unsupervised Domain Adaptation (UDA) methods address domain shifts by adapting models from a labeled source domain to an unlabeled target one. Self-training approaches like ST3D\n[\n35\n]\nuse pseudo-labels for fine-tuning, but Zhang\net al.\n[\n37\n]\nshow that psuedo labels mainly transfer knowledge to the target domain while degrading performance on the source.\nIn this work, we instead pursue\nDomain Generalization\n(DG), which aims at training models that are robust across\nall\ndomains without relying on target-domain data. DG remains underexplored for multimodal detection, and Zhang\net al.\n[\n37\n]\nreport that multimodal models can generalize even worse than LiDAR-only ones. More recently, Hegde\net al.\n[\n5\n]\nproposed a supervised contrastive-learning framework to improve multimodal invariance, but it requires extensive cross-domain training.\nOur approach achieves domain generalization by an ad-hoc fusion procedure with 2D RGB detectors, which tend to generalize better across varying conditions.\n3\nProblem Formulation\nWe address a multi-modal multi-class 3D Object Detection problem, where our inputs are a set of\nK\n{K}\nmonocular or stereo images\nâ„\n{\\mathcal{I}}\nand a Point Cloud\nğ’«\n{\\mathcal{P}}\n. We assume all the sensors are synchronized, i.e.\nâ„\n{\\mathcal{I}}\nand\nğ’«\n{\\mathcal{P}}\nare acquired in the same time frame. More in detail,\nğ’«\n{\\mathcal{P}}\ncontains\nN\nN\npoints\nğ’«\n=\n{\np\n1\n,\np\n2\n,\nâ€¦\n,\np\nN\n}\n{\\mathcal{P}}=\\{p_{1},p_{2},...,p_{N}\\}\n, where\np\nj\n=\n(\nx\nj\n,\ny\nj\n,\nz\nj\n,\nr\nj\n)\nT\nâˆˆ\nâ„\n4\np_{j}=(x_{j},y_{j},z_{j},r_{j})^{T}\\in\\mathbb{R}^{4}\nand\n(\nx\nj\n,\ny\nj\n,\nz\nj\n)\n(x_{j},y_{j},z_{j})\nis the position of the point\np\nj\np_{j}\n, while\nr\nj\nr_{j}\nis the reflectance measured by LiDAR at that point.\nğ’«\n{\\mathcal{P}}\nis expressed in LiDAR coordinates, with\nT\nâˆˆ\nâ„\n4\nÃ—\n4\n{T}\\in\\mathbb{R}^{4\\times 4}\nbeing the known transformation matrix from LiDAR to camera coordinates, which are in the coordinate system of a reference camera in\nâ„\n{\\mathcal{I}}\n.\nA Multimodal 3D Object Detection solution process\nâ„\n{\\mathcal{I}}\nand\nğ’«\n{\\mathcal{P}}\nto return a set of 3D bounding boxes\nğ’Ÿ\n3\nâ€‹\nD\n\\mathcal{D}^{3D}\nsurrounding each object in the 3D space:\n(\nâ„\n,\nğ’«\n)\nâŸ¼\nğ’Ÿ\n3\nâ€‹\nD\n=\n{\n(\nğ\np\n,\ns\np\n,\nÎ»\np\n)\n|\nğ\np\nâˆˆ\nâ„\n7\n,\ns\np\nâˆˆ\n[\n0\n,\n1\n]\n,\nÎ»\np\nâˆˆ\nÎ›\n,\np\n=\n1\n,\nâ€¦\n,\nP\n}\n,\n({\\mathcal{I}},{\\mathcal{P}})\\longmapsto\\mathcal{D}^{3D}=\\{(\\mathbf{B}_{p},s_{p},{\\lambda}_{p})|\\mathbf{B}_{p}\\in\\mathbb{R}^{7},s_{p}\\in[0,1],{\\lambda}_{p}\\in{\\Lambda},p=1,\\dots,P\\},\n(1)\nwhere\nğ\np\n=\n(\nx\np\n,\ny\np\n,\nz\np\n,\nl\np\n,\nh\np\n,\nw\np\n,\nÎ¸\np\n)\nT\n\\mathbf{B}_{p}=(x_{p},y_{p},z_{p},l_{p},h_{p},w_{p},{\\theta}_{p})^{T}\ncontains the 3D coordinates\n(\nx\np\n,\ny\np\n,\nz\np\n)\n(x_{p},y_{p},z_{p})\nof the center of the object, the dimensions\n(\nl\np\n,\nh\np\n,\nw\np\n)\n(l_{p},h_{p},w_{p})\nof the bounding box, and the yaw angle\nÎ¸\np\nâˆˆ\n[\n0\n,\n2\nâ€‹\nÏ€\n]\n{\\theta}_{p}\\in[0,2\\pi]\n,\ns\np\nâˆˆ\n[\n0\n,\n1\n]\ns_{p}\\in[0,1]\ndenotes the detection confidence score,\nÎ»\np\n{\\lambda}_{p}\nis the estimated label from the set of classes\nÎ›\n{\\Lambda}\n, and\nP\nP\nis the number of detections.\nWe consider two possible RGB camera configurations, corresponding to two different setups for\nâ„\n{\\mathcal{I}}\n:\nSingle-View Cameras\n. In this setting, the AV is equipped with\nK\n{K}\ncameras without overlapping fields of view, and we denote the images as\nâ„\n=\n{\nI\n1\n,\nâ€¦\n,\nI\nK\n}\n{\\mathcal{I}}=\\{I_{1},...,I_{{K}}\\}\n,\nI\ni\nâˆˆ\nâ„\nW\ni\nÃ—\nH\ni\nÃ—\n3\nI_{i}\\in\\mathbb{R}^{W_{i}\\times H_{i}\\times 3}\n. We assume to know for each image\nI\ni\nI_{i}\nthe camera matrix\nP\ni\nâˆˆ\nâ„\n3\nÃ—\n4\n{P}_{i}\\in\\mathbb{R}^{3\\times 4}\n, which projects any 3D point in the coordinate system of the image plane.\nStereo-View Cameras\n. In this setting, the AV is equipped with\nK\n{K}\npairs of stereo-cameras, each providing two different views of the same scene. In this case,\nâ„\n{\\mathcal{I}}\nis a set of (left, right) stereo paired images:\nâ„\n=\n{\n(\nI\n1\nl\n,\nI\n1\nr\n)\n,\nâ€¦\n,\n(\nI\nK\nl\n,\nI\nK\nr\n)\n}\n{\\mathcal{I}}=\\{(I_{1}^{l},I_{1}^{r}),...,(I_{{K}}^{l},I_{{K}}^{r})\\}\n, where\nI\ni\nl\nâˆˆ\nâ„\nW\ni\nl\nÃ—\nH\ni\nl\nÃ—\n3\nI_{i}^{l}\\in\\mathbb{R}^{W_{i}^{l}\\times H_{i}^{l}\\times 3}\nand\nI\ni\nr\nâˆˆ\nâ„\nW\ni\nr\nÃ—\nH\ni\nr\nÃ—\n3\nI_{i}^{r}\\in\\mathbb{R}^{W_{i}^{r}\\times H_{i}^{r}\\times 3}\ncorrespond to the left (\nl\nl\n) and right (\nr\nr\n) cameras. Each image\nI\ni\nq\nI_{i}^{q}\n, with\nq\nâˆˆ\n{\nl\n,\nr\n}\nq\\in\\{l,r\\}\n, is acquired with its own camera matrix\nP\ni\nq\nâˆˆ\nâ„\n3\nÃ—\n4\n{P}_{i}^{q}\\in\\mathbb{R}^{3\\times 4}\n.\n4\nOur Method: LCF3D\nFigure 1\n:\nLCF3D consists of two parallel branches and three sequential steps. The RGB branch (a) produces 2D detections\nğ’Ÿ\n2\nâ€‹\nD\n\\mathcal{D}^{2D}\n, the LiDAR branch (b) generates 3D detections\nğ’Ÿ\n^\n3\nâ€‹\nD\n\\widehat{\\mathcal{D}}^{3D}\nfrom the point cloud\nğ’«\n{\\mathcal{P}}\n. In step (c), 3D detections are projected and matched with 2D ones (\nâ„³\n{\\mathcal{M}}\n). Unmatched RGB detections\nğ’°\n{\\mathcal{U}}\nare processed in step (d) by the Detection Recovery module, which uses Frustum Proposals and a Frustum Localizer to recover missed LiDAR detections (\nâ„›\n{\\mathcal{R}}\n). Step (e) employs Semantic Fusion to enforce consistency between LiDAR and RGB branches.\nAt a high level, LCF3D is composed of the 5 modules illustrated in Figure\n1\n: (a) RGB branch, (b) LiDAR branch, (c) Bounding Box Matching, (d) Detection Recovery and (e) Semantic Fusion. The RGB branch takes as input a set\nâ„\n{\\mathcal{I}}\nof single-view or stereo-view images and runs a 2D Object Detection model predicting a set of 2D bounding boxes\nğ’Ÿ\n2\nâ€‹\nD\n\\mathcal{D}^{2D}\n. In parallel, the LiDAR branch processes the input Point Cloud\nğ’«\n{\\mathcal{P}}\nto produce a preliminary set of 3D bounding boxes\nğ’Ÿ\n^\n3\nâ€‹\nD\n\\widehat{\\mathcal{D}}^{3D}\n. The Bounding Box Matching module performs late fusion to filter out FP detections from the LiDAR branch by matching detections from\nğ’Ÿ\n2\nâ€‹\nD\n\\mathcal{D}^{2D}\nand\nğ’Ÿ\n^\n3\nâ€‹\nD\n\\widehat{\\mathcal{D}}^{3D}\n, producing a set\nâ„³\n{\\mathcal{M}}\nof paired 3D and 2D bounding boxes. Instead, the Detection Recovery step processes all the RGB detections\nğ’°\n{\\mathcal{U}}\nthat are not matched in (c) to recover FNs from the LiDAR branch (b), producing as output a new set\nâ„›\n{\\mathcal{R}}\nof 3D detections associated with unmatched 2D bounding boxes. Finally, Semantic Fusion enforces consistency in predicted labels between the matched detections\nâ„³\n{\\mathcal{M}}\nin (c) and the new 3D detections\nâ„›\n{\\mathcal{R}}\nfrom (d). The principle underpinning LCF3D is that the RGB modality is better for finding small and distant objects\n[\n31\n]\n. Thus, we expect the RGB branch to have a higher recall than the LiDAR one for those objects, and rely on cascade fusion in the Detection Recovery for objects missed by the LiDAR branch. This higher recall does not necessarily come at the cost of lower precision: RGB images provide higher spatial resolution and richer appearance cues (texture, color, shape), which makes such objects easier to localize reliably than in sparse LiDAR point clouds\n[\n31\n]\n. Moreover, the Bounding Box Matching module exploits the fact that RGB detectors are expected to have a high precision, since RGB images possess richer semantics. Please note that our method is not constrained to a particular model in the LiDAR and RGB branches, enabling the usage of any state-of-the-art object detection models (discussed in Section\n5.4\n).\n4.1\nThe Single View case\nFot the sake of illustration, we will outline the method for the single-view setup. We will then discuss the extension to the stereo-view case in Section\n4.2\n. For the notation sake, we will also illustrate our method assuming a single -image is processed (\nK\n=\n1\n{K}=1\n); the formulation naturally extends to multiple, non overlapping, images (\nK\n>\n1\n{K}>1\n) by repeating all the steps for each input image.\n4.1.1\nRGB branch\nIn the RGB branch, a 2D Object Detection network process the input image to produce 2D bounding boxes. Any 2D Object Detection model can be used in this branch: single-stage object detectors are preferred when fast inference is needed, but they usually perform worse than two-stage detectors like Faster RCNN\n[\n23\n]\n. Since in the AD field objects can have very different depths, a Feature Pyramid Network (FPN) is usually employed as it produces features at different scales, which is useful to detect objects having very different 2D dimensions. Optionally, we can also adopt an instance segmentation network to predict masks within each box to further reduce the search space for Detection Recovery, as discussed in Section\n4.1.4\n.\nWe denote with\nğ’Ÿ\n2\nâ€‹\nD\n=\n{\n(\nğ›\nm\n,\ns\nm\n,\nÎ»\nm\n)\n}\nm\n=\n1\nM\n\\mathcal{D}^{2D}=\\{(\\mathbf{b}_{m},s_{m},{\\lambda}_{m})\\}_{m=1}^{M}\nthe set of 2D detections in the image\nI\nI\n, where\nğ›\nm\n\\mathbf{b}_{m}\nis a 2D bounding box described by 4 coordinates\n(\nx\nm\nâ€‹\ni\nâ€‹\nn\n,\ny\nm\nâ€‹\ni\nâ€‹\nn\n,\nx\nm\nâ€‹\na\nâ€‹\nx\n,\ny\nm\nâ€‹\na\nâ€‹\nx\n)\n(x_{min},y_{min},x_{max},y_{max})\n,\ns\nm\nâˆˆ\n[\n0\n,\n1\n]\ns_{m}\\in[0,1]\nis the confidence score,\nÎ»\nm\nâˆˆ\nÎ›\n{\\lambda}_{m}\\in{\\Lambda}\nis the semantic class and\nM\nM\nis the number of detections. Since we are interested in 3D bounding boxes, we combine\nğ’Ÿ\n2\nâ€‹\nD\n\\mathcal{D}^{2D}\nwith detections from a 3D LiDAR Object Detection network.\n4.1.2\nLiDAR branch\nThe LiDAR branch is a 3D Object Detection network processing Point Clouds\nğ’«\n{\\mathcal{P}}\n, which produces an initial set\nğ’Ÿ\n^\n3\nâ€‹\nD\n\\widehat{\\mathcal{D}}^{3D}\nof 3D bounding boxes, as in (\n1\n), that are further processed by other modules.\nPoint-based methods\n[\n26\n]\n, which extract features directly from LiDAR points, are typically too slow for real-time applications. Instead, we follow a Point Cloud discretization approach in either voxels or BEV. Specifically, voxel-based methods\n[\n13\n,\n12\n,\n21\n]\nencode points into voxels and use 3D CNNs for feature extraction, offering faster inference with comparable performance. BEV-based methods\n[\n7\n,\n40\n]\n, instead, project the Point Cloud into the Birdâ€™s Eye View (BEV) and use 2D CNNs, enabling faster inference at the cost of additional 2D discretization.\nDue to the sparsity and occlusions that affect the Point Cloud, detections in\nğ’Ÿ\n^\n3\nâ€‹\nD\n\\widehat{\\mathcal{D}}^{3D}\nmight either miss some relevant object in the scene. Therefore, we remove the Non Maximum Suppression (NMS) step in the LiDAR branch and use a low confidence score threshold in the LiDAR detector. These changes improve the recall of the LiDAR network at the cost of some 3D FPs. We compensate for these by late fusion with RGB detections in the Bounding Box Matching module.\n4.1.3\nBounding Box Matching\nThe goal of the Bounding Box Matching module is to remove FP detections from the LiDAR branch by we associating each LiDAR 3D detection in\nğ’Ÿ\n^\n3\nâ€‹\nD\n\\widehat{\\mathcal{D}}^{3D}\nto a 2D detection of\nğ’Ÿ\n2\nâ€‹\nD\n\\mathcal{D}^{2D}\n, as shown in Figure\n2\n. Projecting 3D detections in the image plane results in information loss (e.g., the depth or the 3D orientation of the bounding box), and the projected 3D detections might not match at best a 2D detection. Therefore, we need to be cautious before discarding a 3D detection yielding low IoU with 2D detections. To this purpose, we replace the NMS of the 3D network with an ad-hoc procedure where neighbouring 3D bounding boxes are first clustered and then matched at cluster-level to 2D detections. This safely discards 3D detections whose corresponding cluster does not overlap with any 2D detection. By keeping a high threshold for both the confidence score and NMS in the 2D Object Detection network, we ensure that clusters of 3D bounding boxes are matched only with relevant 2D detections.\nThe core components of the Bounding Box Matching module are illustrated in Figure\n3\n. When removing the NMS from the 3D Object Detection network, the LiDAR branch returns several 3D detections for a single 3D object (see Figure\n4\nleft). Thus, we first\ncluster\nthe 3D detections and then project all the 3D bounding boxes on the image plane, where we solve a\nmatching\nproblem to pair 3D clusters with 2D bounding boxes. Unmatched clusters are removed, while matched clusters undergo a\ncluster-wise NMS\nstep to retrieve the highest-confidence 3D bounding box within the cluster.\nFigure 2\n:\nComparison between the LiDAR branch output (left) and the Bbox Matching module output (right), which removes FPs. Only the highest-confidence bounding box per cluster is shown on the left.\nIn practice, rather than performing the clustering directly in 3D, we found it convenient to work on the BEV. We define a cluster of 3D bounding boxes as a subset of\nğ’Ÿ\n^\n3\nâ€‹\nD\n\\widehat{\\mathcal{D}}^{3D}\nhaving a high mutual IoU on the BEV. Clusters of 3D bounding boxes are hence identified as maximal cliques in a graph where the nodes are the 3D bounding boxes and an edges connect two bounding boxes when their 2D IoU on the BEV\nI\nâ€‹\no\nâ€‹\nU\nB\nâ€‹\nE\nâ€‹\nV\nIoU_{BEV}\nis higher than a threshold\nÏ„\nz\n\\tau_{z}\n.\nMore formally, denoting\nZ\n=\n{\nZ\nc\n|\nc\n=\n1\n,\nâ€¦\n,\nC\n}\nZ=\\{Z_{c}|c=1,\\dots,C\\}\nthe set of clusters of 3D detections, a cluster\nZ\nc\nZ_{c}\nis identified as:\nâˆ€\nğ\ni\n,\nğ\nj\nâˆˆ\nğ’Ÿ\n^\n3\nâ€‹\nD\n:\n(\nğ\ni\nâˆˆ\nZ\nc\nâˆ§\nğ\nj\nâˆˆ\nZ\nc\n)\nâ‡”\nI\nâ€‹\no\nâ€‹\nU\nB\nâ€‹\nE\nâ€‹\nV\nâ€‹\n(\nğ\ni\n,\nğ\nj\n)\n>\nÏ„\nz\n.\n\\forall\\mathbf{B}_{i},\\mathbf{B}_{j}\\in\\widehat{\\mathcal{D}}^{3D}:(\\mathbf{B}_{i}\\in Z_{c}\\land\\mathbf{B}_{j}\\in Z_{c})\\iff IoU_{BEV}(\\mathbf{B}_{i},\\mathbf{B}_{j})>\\tau_{z}.\n(2)\nFigure 3\n:\nHigh-level overview of the Bounding Box Matching for one image. First, 3D boxes are clustered in BEV, projected onto the image, and matched with 2D detections using IoU.\nFinally, each 3D cluster is matched with the 2D detections using an optimization problem based on the IoU and the bounding box with the highest score inside each matched cluster is selected (Cluster-wise NMS).\nFigure 4\n:\n(Left) Projection of all the 3D bounding box of a cluster\nZ\nc\nZ_{c}\nonto the image plane. (Right) Comparison with RGB 2D detections (blue): green boxes have IoU\n>\n0.5\n>0.5\n, red boxes are below the threshold. The maximum IoU in the cluster is used to resolve conflicts.\nThen, we assign a cluster\nZ\nc\nZ_{c}\nto a 2D detection\nğ›\nm\nâˆˆ\nğ’Ÿ\n2\nâ€‹\nD\n\\mathbf{b}_{m}\\in\\mathcal{D}^{2D}\nas follows. We first project the 8 corners\n{\nh\n1\n,\nâ€¦\n,\nh\n8\n}\n\\{h_{1},\\dots,h_{8}\\}\nof each 3D bounding box\nğ\nâˆˆ\nZ\nc\n\\mathbf{B}\\in Z_{c}\ninto the image plane using the projection matrix\nP\ni\n{P}_{i}\n. From the projected corners\nh\n~\nj\n=\nP\ni\nâ€‹\nT\nâ€‹\nh\nj\n\\widetilde{h}_{j}={P}_{i}{T}h_{j}\n, we extract the minimum axis-aligned bounding box\nğ›\ni\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n\\mathbf{b}^{proj}_{i}\nenclosing all of them. We define the 2D IoU between a cluster\nZ\nc\nZ_{c}\nand a 2D bounding box\nğ›\nm\nâˆˆ\nğ’Ÿ\n2\nâ€‹\nD\n\\mathbf{b}_{m}\\in\\mathcal{D}^{2D}\nas the maximum IoU of the projected bounding boxes of the cluster with\nğ›\nm\n\\mathbf{b}_{m}\n, as illustrated in Figure\n4\nright:\nI\nâ€‹\no\nâ€‹\nU\nâ€‹\n(\nZ\nc\n,\nğ›\nm\n)\n=\nmax\nğ\ni\nâˆˆ\nZ\nc\nâ¡\nI\nâ€‹\no\nâ€‹\nU\n2\nâ€‹\nd\nâ€‹\n(\nğ›\ni\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n,\nğ›\nm\n)\n.\nIoU(Z_{c},\\mathbf{b}_{m})=\\max_{\\mathbf{B}_{i}\\in Z_{c}}IoU_{2d}(\\mathbf{b}^{proj}_{i},\\mathbf{b}_{m}).\n(3)\nTo compute a matching\nâ„³\n~\n\\widetilde{{\\mathcal{M}}}\nbetween clusters in\nZ\nZ\nand bounding boxes in\nğ’Ÿ\n2\nâ€‹\nD\n\\mathcal{D}^{2D}\n, we solve the following optimization problem using the Jonker-Volgenant algorithm\n[\n6\n]\n:\nmax\nx\n\\max_{{x}}\ns.t.\nwhere\nx\nc\n,\nm\nâˆˆ\n{\n0\n,\n1\n}\n{x}_{c,m}\\in\\{0,1\\}\ndenotes if cluster\nZ\nc\nZ_{c}\nand 2D detection\nğ›\nm\n\\mathbf{b}_{m}\nare matched or not. The first two constraints ensure each cluster matches at most one RGB detection, and vice versa, while the last maximizes total assignments.\nWe keep in\nâ„³\n~\n\\widetilde{{\\mathcal{M}}}\nonly the matched clusters with an IoU higher than a threshold\nÏ„\nb\n\\tau_{b}\n, and for each matched cluster, we keep only the 3D bounding box with the highest confidence score (\ncluster-wise NMS\n), yielding the set of matches\nâ„³\n{\\mathcal{M}}\n. The set of unmatched RGB detections, denoted by\nğ’°\n{\\mathcal{U}}\n, will be processed in the following Detection Recovery module (Section\n4.1.4\n).\nAlternative strategies exist to match RGB and LiDAR detections, such as lifting 2D boxes to BEV\n[\n14\n]\nor using a 3D detector in the RGB branch\n[\n18\n]\n. These approaches generally underperform compared to image-plane matching\n[\n14\n]\n, and our experiments (Section\n5.7.3\n) confirm the benefit of postponing NMS until after 3Dâ€“2D matching.\n4.1.4\nDetection Recovery\nThe Detection Recovery module recovers FNs from the LiDAR branch, i.e. missed 3D objects, processing the set of unmatched 2D bounding boxes for each image\nğ’°\nâŠ†\nğ’Ÿ\n2\nâ€‹\nD\n{\\mathcal{U}}\\subseteq\\mathcal{D}^{2D}\n, where\nğ’°\nâŠ†\nğ’Ÿ\n2\nâ€‹\nD\n{\\mathcal{U}}\\subseteq\\mathcal{D}^{2D}\n. We recover the corresponding missed 3D detections by leveraging single-view geometry principles, returning a set\nâ„›\n{\\mathcal{R}}\nof pairs of RGB-LiDAR detections:\nâ„›\n:=\n{\n(\nğ\nj\n,\ns\nj\n3\nâ€‹\nd\n,\nÎ»\nj\n3\nâ€‹\nd\n,\nğ›\nj\n,\ns\nj\n2\nâ€‹\nd\n,\nÎ»\nj\n2\nâ€‹\nd\n)\n}\n,\n{\\mathcal{R}}:=\\{(\\mathbf{B}_{j},s_{j}^{3d},{\\lambda}_{j}^{3d},\\mathbf{b}_{j},s_{j}^{2d},{\\lambda}_{j}^{2d})\\},\n(5)\nwhere\n(\nğ\nj\n,\ns\nj\n3\nâ€‹\nd\n,\nÎ»\nj\n3\nâ€‹\nd\n)\n(\\mathbf{B}_{j},s_{j}^{3d},{\\lambda}_{j}^{3d})\nare the new 3D detections recovered and\n(\nğ›\nj\n,\ns\nj\n2\nâ€‹\nd\n,\nÎ»\nj\n2\nâ€‹\nd\n)\nâˆˆ\nğ’Ÿ\n2\nâ€‹\nD\n(\\mathbf{b}_{j},s_{j}^{2d},{\\lambda}_{j}^{2d})\\in\\mathcal{D}^{2D}\nis the corresponding 2D detection.\nFigure 5\n:\nIn the single-view scenario, the Detection Recovery module processes an unmatched 2D RGB bounding box\nğ›\nj\n\\mathbf{b}_{j}\n. A Frustum Proposal\nğ’«\nj\n{\\mathcal{P}}_{j}\nis generated by projecting the 3D points of the input Point Cloud\nğ’«\n{\\mathcal{P}}\ninto the image and selecting points within\nğ›\nj\n\\mathbf{b}_{j}\n. The Frustum Localizer then extracts a 3D bounding box\nğ\nj\n\\mathbf{B}_{j}\nfrom the frustum, which inherits the semantic label of the 2D detection.\nFigure\n5\nillustrates the Detection Recovery module. We extract from each unmatched 2D bounding box\nğ›\nj\nâˆˆ\nğ’°\n\\mathbf{b}_{j}\\in{\\mathcal{U}}\na Frustum Proposal\nğ’«\nj\n{\\mathcal{P}}_{j}\n, i.e. the set of 3D points contained into the 3D frustum of that 2D bounding box. This is illustrated in Figure\nLABEL:fig:frustums:bbox_frustum\n, where the frustums are in red and correspond to the set of 3D points that are projected inside the 2D bounding box in Figure\nLABEL:fig:frustums:masks\n. Note that, before computing the Frustum Proposal, we slightly increase the dimensions of the 2D bounding boxes by an enlargement factor\ne\ne\nfor both width and height, keeping the centers of the bounding boxes fixed. Moreover, we add to each point\np\nâˆˆ\nğ’«\nj\np\\in{\\mathcal{P}}_{j}\n, projected in the image as\np\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n=\n(\nx\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n,\ny\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n)\np^{proj}=(x^{proj},y^{proj})\n, the Gaussian mask proposed in Frustum PointPillars\n[\n16\n]\n, computed as:\nG\nâ€‹\n(\np\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n)\n=\nexp\nâ¡\n(\nâˆ’\n(\nx\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\nâˆ’\nx\n0\n)\n2\n2\nâ€‹\nw\n2\nâˆ’\n(\ny\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\nâˆ’\ny\n0\n)\n2\n2\nâ€‹\nh\n2\n)\n,\nG(p^{proj})=\\exp\\left(-\\frac{(x^{proj}-x_{0})^{2}}{2w^{2}}-\\frac{(y^{proj}-y_{0})^{2}}{2h^{2}}\\right),\n(6)\nwhere\n(\nx\n0\n,\ny\n0\n)\n(x_{0},y_{0})\nis the center of\nğ›\nj\n\\mathbf{b}_{j}\nand\n(\nw\n,\nh\n)\n(w,h)\nare the width and the height. We filter out Frustum Proposals that contain less than\np\nmin\n=\n10\np_{\\min}=10\npoints, which can happen for very distant objects.\nEach Frustum Proposal is then processed by the Frustum Localizer (Frustum PointNet\n[\n19\n]\n), a 3D Localization model that predicts a 3D bounding box from a single frustum. We then assign the semantic label of the 2D detection to the recovered 3D bounding box. We also shrink the confidence score by the IoU with the 2D detection to penalize inaccurate bounding boxes:\ns\nj\n3\nâ€‹\nd\n=\ns\nj\n2\nâ€‹\nd\nâ‹…\nI\nâ€‹\no\nâ€‹\nU\nâ€‹\n(\nğ›\nj\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n,\nğ›\nj\n)\n,\ns_{j}^{3d}=s_{j}^{2d}\\cdot IoU(\\mathbf{b}^{proj}_{j},\\mathbf{b}_{j}),\n(7)\nwhere\ns\nj\n2\nâ€‹\nd\ns_{j}^{2d}\nis the confidence score of the RGB detection, and\nğ›\nj\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n\\mathbf{b}^{proj}_{j}\nis the projection of the localized object by the Frustum Localizer in the image plane. We confirm the new detection if and only if the IoU between its projection and the corresponding 2D bounding box is higher than a threshold\nÏ„\nd\n\\tau_{d}\n.\nWe can also use instance segmentation in the RGB branch to generate frustum proposals that include only the 3D points projected inside the segmentation mask (Figure\nLABEL:fig:frustums:mask_frustum\n), which typically lie within the object of interest. In practice, we extract the frustum from the 2D bounding box and add an additional binary channel encoding the mask information for each point (Figure\nLABEL:fig:frustums:mask_frustum_one_hot\n). This strategy preserves relevant points in case of inconsistent masks and enriches the frustum representation with finer semantic cues.\n(a)\n(b)\n(c)\n(d)\nFigure 6\n:\n(a) Output of an instance segmentation network. (b) Frustum Proposal from a 2D bounding box (red points). (c) Frustum Proposal from a 2D instance mask. (d) Frustum Proposal from a 2D bounding box with mask channel (yellow = 0, red = 1).\n4.1.5\nSemantic Fusion\nFigure 7\n:\nLeft\n: a Cyclist wrongly classified as Pedestrian by the LiDAR branch.\nRight\n: the Cyclist is correctly classified by the RGB branch.\nThe Semantic Fusion module, described in Algorithm\n1\n, combines both the labels to enforce semantic consistency and the confidence scores of LiDAR and RGB detections. In fact, it may happen that 2D and 3D bounding boxes matched by the Bounding Box Matching module are associated to different semantic labels in\nÎ›\n{\\Lambda}\n(Figure\n7\n). The Semantic Fusion module takes as input the set\nğ’œ\n=\nâ„³\nâˆª\nâ„›\n{\\mathcal{A}}={\\mathcal{M}}\\cup{\\mathcal{R}}\n, including both matched (\nâ„³\n{\\mathcal{M}}\n) and recovered (\nâ„›\n{\\mathcal{R}}\n) detections. Since RGB images better capture semantic details, the RGB detector is expected to be better at defining object classes. Thus, we associate the labels estimated from the RGB branch to each matched LiDAR bounding box (Line 5), as in\n[\n14\n]\n. When the two predicted classes are different, we assign the label and the confidence of the RGB detector to the 3D detection (Line 9). Instead, when the predicted classes are equal, we follow the probabilistic ensemble framework in\n[\n3\n]\n, assuming conditional independence between modalities, and define the final detection confidence score\ns\nâ€²\ns^{\\prime}\nfor class\nÎ»\nâˆˆ\nÎ›\n\\lambda\\in{\\Lambda}\n:\ns\nâ€²\nâ€‹\n(\nÎ»\n)\nâˆ\ns\n3\nâ€‹\nd\nâ‹…\ns\n2\nâ€‹\nd\n/\np\nâ€‹\n(\nÎ»\n)\ns^{\\prime}(\\lambda)\\propto s^{3d}\\cdot s^{2d}\\mathbin{/}p(\\lambda)\n, where\np\nâ€‹\n(\nÎ»\n)\np(\\lambda)\nis the class prior, treated as a uniform in this work.\nAlgorithm 1\nSemantic Fusion\nInput\n: Matching detections\nğ’œ\n{\\mathcal{A}}\nOutput\n: Final detection output\nğ’Ÿ\n3\nâ€‹\nD\n\\mathcal{D}^{3D}\n1:\nfunction\nSemanticFusion\n(\nğ’œ\n{\\mathcal{A}}\n)\n2:\nğ’Ÿ\n3\nâ€‹\nD\nâ†\nâˆ…\n\\mathcal{D}^{3D}\\leftarrow\\emptyset\n3:\nfor\n(\nğ\nj\n,\ns\nj\n3\nâ€‹\nd\n,\nÎ»\nj\n3\nâ€‹\nd\n,\nğ›\nj\n,\ns\nj\n2\nâ€‹\nd\n,\nÎ»\nj\n2\nâ€‹\nd\n)\nâˆˆ\nğ’œ\n(\\mathbf{B}_{j},s_{j}^{3d},{\\lambda}_{j}^{3d},\\mathbf{b}_{j},s_{j}^{2d},{\\lambda}_{j}^{2d})\\in{\\mathcal{A}}\ndo\n4:\nÎ»\nj\nâ€²\nâ†\nÎ»\nj\n2\nâ€‹\nd\n{\\lambda}_{j}^{\\prime}\\leftarrow{\\lambda}_{j}^{2d}\n5:\nif\nÎ»\nj\n2\nâ€‹\nd\n=\nÎ»\nj\n3\nâ€‹\nd\n{\\lambda}_{j}^{2d}={\\lambda}_{j}^{3d}\nthen\n6:\ns\nj\nâ€²\nâ†\nProbabilisticEnsemble\nâ€‹\n(\ns\nj\n3\nâ€‹\nd\n,\nÎ»\nj\n3\nâ€‹\nd\n,\ns\n2\nâ€‹\nd\n,\nÎ»\nj\nâ€²\n)\ns_{j}^{\\prime}\\leftarrow\\textsc{ProbabilisticEnsemble}(s_{j}^{3d},{\\lambda}_{j}^{3d},s^{2d},{\\lambda}_{j}^{\\prime})\n7:\nelse\n8:\ns\nj\nâ€²\nâ†\ns\nj\n2\nâ€‹\nd\ns_{j}^{\\prime}\\leftarrow s_{j}^{2d}\n9:\nend\nif\n10:\nğ’Ÿ\n3\nâ€‹\nD\nâ†\nğ’Ÿ\n3\nâ€‹\nD\nâˆª\n(\nğ\nj\n,\ns\nj\nâ€²\n,\nÎ»\nj\nâ€²\n)\n\\mathcal{D}^{3D}\\leftarrow\\mathcal{D}^{3D}\\cup(\\mathbf{B}_{j},s_{j}^{\\prime},{\\lambda}_{j}^{\\prime})\n11:\nend\nfor\n12:\nreturn\nğ’Ÿ\n3\nâ€‹\nD\n\\mathcal{D}^{3D}\n13:\nend\nfunction\n4.2\nThe Stereo View case\nWe now revisit the single-view approach for the case in which the input consists of stereo images. In short, from a stereo pair we can leverage two sets of detections, one for the left view and one from the right view. Thus, we consider a 3D detection from the LiDAR branch as True Positive (TP) when it matches a 2D bounding box at least in one image. In the Detection Recovery, we leverage the epipolar geometry of the stereo pair to pair unmatched bounding boxes from the two views and intersect the frustums from the two matched bounding boxes to further reduce the 3D search space.\n4.2.1\nStereo RGB branch\nThe input of the stereo RGB branch is composed by stereo images (\n(\nI\nl\n,\nI\nr\n)\n)\n(I^{l},I^{r}))\n. Thus, the output is the pair of sets\n(\nğ’Ÿ\nl\n2\nâ€‹\nD\n,\nğ’Ÿ\nr\n2\nâ€‹\nD\n)\n(\\mathcal{D}_{l}^{2D},\\mathcal{D}_{r}^{2D})\n, where\nğ’Ÿ\nl\n2\nâ€‹\nD\n\\mathcal{D}_{l}^{2D}\nand\nğ’Ÿ\nr\n2\nâ€‹\nD\n\\mathcal{D}_{r}^{2D}\nare the set of bounding boxes in the left and right images, respectively, defined as in the single-view case.\n4.2.2\nStereo Bounding Box Matching\nIn the stereo view setting, the Bounding Box Matching module can leverage 2D bounding boxes from both left and right views. In particular, we repeat the previously described method for both the images\nI\nl\nI^{l}\nand\nI\nr\nI^{r}\nand we confirm the LiDAR detections when these are matched with 2D bounding boxes in any of the two images. Table\n1\nsummarizes the process for one pair of stereo images.\nTable 1\n:\nBounding box matching summary for\nK\n=\n1\n{K}=1\nstereo views\nLiDAR\nRGB left\nRGB right\nComment\nâœ—\nâœ—\nâœ—\nNo detection\nâœ—\nâœ—\nâœ“\nRGB right False Positive\nâœ—\nâœ“\nâœ—\nRGB left False Positive\nâœ—\nâœ“\nâœ“\nDetection Recovery\nâœ“\nâœ—\nâœ—\nLiDAR False Positive\nâœ“\nâœ—\nâœ“\nSemantic Fusion\nâœ“\nâœ“\nâœ—\nSemantic Fusion\nâœ“\nâœ“\nâœ“\nSemantic Fusion\n4.2.3\nStereo Detection Recovery\nRGB detections from both left and right image that are not matched with any 3D detection are fed to the stereo Detection Recovery module. At a high level, the stereo Detection Recovery module performs three steps. First, we match each bounding box\nğ›\nl\nâˆˆ\nğ’°\nl\n\\mathbf{b}_{l}\\in{\\mathcal{U}}_{{l}}\nin the left image with possibly one bounding box\nğ›\nr\nâˆˆ\nğ’°\nr\n\\mathbf{b}_{r}\\in{\\mathcal{U}}_{{r}}\nin the right view, exploiting two-view geometry constraints. Then, we extract Frustum Proposals from each pair of matched detections and execute the Frustrum Localizer on their intersection.\n(a)\n(b)\n(c)\nFigure 8\n:\nIllustration of the Frustum Proposals, obtained from the Detection Recovery module assignment procedure between two detections\n(\nğ›\nl\n,\nğ›\nr\n)\n(\\mathbf{b}_{l},\\mathbf{b}_{r})\nbelonging to stereo images\nI\nl\nI^{l}\nand\nI\nr\nI^{r}\n.\nTo match left and right bounding boxes, we design an epipolar assignment procedure. Given a bounding box\nğ›\nl\n\\mathbf{b}_{l}\ndetected on the left image, we compute the epipolar lines\n(\nl\n1\n,\nl\n2\n)\n(l_{1},l_{2})\ncorresponding to its top left and bottom right corners\n(\nc\n1\n,\nc\n2\n)\n(c_{1},c_{2})\non the right image as\nl\nk\n=\nF\nl\nâ€‹\nr\nâ€‹\nc\nk\n,\nk\nâˆˆ\n{\n1\n,\n2\n}\nl_{k}=F_{lr}c_{k},\\quad k\\in\\{1,2\\}\n, where\nF\nl\nâ€‹\nr\nF_{lr}\nis the fundamental matrix between the two images, computed as:\nF\nl\nâ€‹\nr\n=\nK\nl\nâˆ’\nT\nâ€‹\nE\nâ€‹\nK\nr\nâˆ’\n1\n=\nK\nl\nâˆ’\nT\nâ€‹\n[\nt\n]\nÃ—\nâ€‹\nR\nâ€‹\nK\nr\nâˆ’\n1\n,\nF_{lr}=K_{l}^{-T}EK_{r}^{-1}=K_{l}^{-T}[t]_{\\times}RK_{r}^{-1},\n(8)\nwhere\nK\nl\nK_{l}\nand\nK\nr\nK_{r}\nare the intrinsic matrices of the two cameras,\nR\nR\nand\nt\nt\nare the relative rotation and translation between the cameras, respectively.\nWhen the stereo pair is rectified as in Figure\nLABEL:fig:det_recovery:right_epipolar\n, the epipolar lines are horizontal. Ideally, the same corners\n(\nc\n1\nâ€²\n,\nc\n2\nâ€²\n)\n(c_{1}^{\\prime},c_{2}^{\\prime})\nof a bounding box\nğ›\nr\n\\mathbf{b}_{r}\ncorresponding to the same object in the right image should belong to these two epipolar lines. However, the predictions of the RGB branch may have small inconsistencies, as shown in Figures\nLABEL:fig:det_recovery:left_bboxes\nand\nLABEL:fig:det_recovery:right_epipolar\n, but still the corners\n(\nc\n1\nâ€²\n,\nc\n2\nâ€²\n)\n(c_{1}^{\\prime},c_{2}^{\\prime})\nare expected to be close to the epipolar lines\n(\nl\n1\n,\nl\n2\n)\n(l_{1},l_{2})\ndefined by the bounding box in the other image. This is illustrated for the cyclist in Figure\nLABEL:fig:det_recovery:right_epipolar\n.\nThis motivates our cost function\nd\nâ€‹\n(\nâ‹…\n,\nâ‹…\n)\nd(\\cdot,\\cdot)\nfor matching\nğ›\nl\n\\mathbf{b}_{l}\nand\nğ›\nr\n\\mathbf{b}_{r}\nas the sum of the Euclidean distances\nd\n~\nâ€‹\n(\nâ‹…\n,\nâ‹…\n)\n\\tilde{d}(\\cdot,\\cdot)\nbetween each corner of\nğ›\nr\n\\mathbf{b}_{r}\nand the epipolar lines of the corresponding corner of\nğ›\nl\n\\mathbf{b}_{l}\n:\nd\nâ€‹\n(\nğ›\nl\n,\nğ›\nr\n)\n=\nd\n~\nâ€‹\n(\nl\n1\n,\nc\n1\nâ€²\n)\n+\nd\n~\nâ€‹\n(\nl\n2\n,\nc\n2\nâ€²\n)\n.\nd(\\mathbf{b}_{l},\\mathbf{b}_{r})=\\tilde{d}(l_{1},c_{1}^{\\prime})+\\tilde{d}(l_{2},c_{2}^{\\prime}).\n(9)\nThe assignment problem is similar to (\n4\n), but here we minimize this distance instead of maximizing the IoU. As before, we solve the assignment problem using the Jonker-Volgenant algorithm, to get matches\nâ„³\n2\nâ€‹\nd\n\\mathcal{M}_{2d}\nas output.\nGiven the matched pairs of 2D detections in\nâ„³\n2\nâ€‹\nd\n\\mathcal{M}_{2d}\nfrom the stereo views, we extract 3D Frustum Proposals by back-projecting each detection in a frustum and intersecting the two frustums from both views (Figure\nLABEL:fig:det_recovery:frustum\n). The same Frustum Localizer used in the single-view setting produces a 3D bounding box from the Frustum Proposal. We then assign to the 3D bounding box the estimated label and the score of the most confident RGB detection and down-weight the confidence score by the IoU with the 2D detections as:\ns\n3\nâ€‹\nd\n=\ns\nR\nâ€‹\nG\nâ€‹\nB\nâ‹…\nI\nâ€‹\no\nâ€‹\nU\nâ€‹\n(\nğ›\nl\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n,\nğ›\nl\n)\nâ‹…\nI\nâ€‹\no\nâ€‹\nU\nâ€‹\n(\nğ›\nr\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n,\nğ›\nr\n)\n,\ns^{3d}=s_{RGB}\\cdot IoU(\\mathbf{b}^{proj}_{l},\\mathbf{b}_{l})\\cdot IoU(\\mathbf{b}^{proj}_{r},\\mathbf{b}_{r}),\n(10)\nwhere\ns\nR\nâ€‹\nG\nâ€‹\nB\ns_{RGB}\nis the confidence score of the most confident RGB detection, and\n(\nğ›\nl\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n,\nğ›\nr\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n)\n(\\mathbf{b}^{proj}_{l},\\mathbf{b}^{proj}_{r})\nare the projected 2D bounding boxes in the two image planes of the bounding box predicted by the Frustum Localizer. We then discard the recovered 3D detections that do not satisfy the following condition:\nmin\nâ¡\n(\nI\nâ€‹\no\nâ€‹\nU\nâ€‹\n(\nğ›\nl\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n,\nğ›\nl\n)\nâ‹…\nI\nâ€‹\no\nâ€‹\nU\nâ€‹\n(\nğ›\nr\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nj\n,\nğ›\nr\n)\n)\nâ‰¥\nÏ„\nd\n.\n\\min(IoU(\\mathbf{b}^{proj}_{l},\\mathbf{b}_{l})\\cdot IoU(\\mathbf{b}^{proj}_{r},\\mathbf{b}_{r}))\\geq\\tau_{d}.\n(11)\n4.2.4\nStereo Semantic Fusion\nThe Semantic Fusion module for the stereo-view setting is similar to the single-view one, with the difference that each LiDAR 3D detection can be associated with the labels from one or two 2D RGB bounding boxes when it is matched in both stereo images. When the number of matched RGB detections is one, this reduces to the same procedure as the single-view setting. Differently, when 2D detections from both images are matched, we take the label of the most confident-matched RGB detection to assign the label to the matched 3D bounding box, and fuse the scores as in Section\n4.1.5\n, but with an additional term for the second image:\ns\nâ€²\nâ€‹\n(\nÎ»\n)\nâˆ\ns\n3\nâ€‹\nd\nâ‹…\ns\nl\n2\nâ€‹\nd\nâ‹…\ns\nr\n2\nâ€‹\nd\n/\np\nâ€‹\n(\nÎ»\n)\ns^{\\prime}(\\lambda)\\propto s^{3d}\\cdot s_{l}^{2d}\\cdot s_{r}^{2d}\\mathbin{/}p(\\lambda)\n.\n5\nExperiments\nWe test LCF3D with KITTI\n[\n4\n]\nand nuScenes\n[\n1\n]\ndatasets, comparing our solution with state-of-the-art LiDAR-based and multimodal methods for 3D Object Detection. KITTI and nuScenes provide very different settings for both the LiDAR sensor and the type of RGB cameras employed, resulting in domain shifts when one dataset is used for training and the other for inference. We test the Domain Generalization performance of LCF3D in these settings.\n5.1\nDatasets\nThe\nKITTI\nobject detection dataset\n[\n4\n]\nincludes data from a 64-beam LiDAR and a stereo RGB camera pair (in the single-view setup, only the left camera is used). Following the protocol in\n[\n2\n]\n, we split the training set into 3712 training and 3769 validation samples, and adopt the official evaluation scheme with three difficulty levels: easy (fully visible, nearby objects), moderate (partially occluded or more distant), and hard (small or heavily occluded objects).\nThe\nnuScenes\ndataset\n[\n1\n]\nprovides a comprehensive sensor suite with one 32-beam LiDAR and six non-overlapping RGB cameras covering the full field of view. To train the RGB branch, we additionally use nuImages, a complementary dataset of 93k images sharing the same sensor setup and including instance-segmentation labels. We train 2D detection models on both nuScenes and nuImages, and instance segmentation models on nuImages only.\n5.2\nFigure of merits\nWe consider both KITTI and nuScenes metrics. For KITTI, we use the 3D Average Precision (AP) and we consider the Car, Pedestrian and Cyclist classes. For nuScenes, we use the per-class Average Precision (AP), the mean Average Precision (mAP) and the nuScenes Detection Score (NDS) using all the 10 classes. More details are in\n[\n1\n]\n. We measure our inference speed on an A100 GPU.\nTo assess Domain Generalization, we follow the approach by DG-BEV\n[\n30\n]\nto compute a variant of the original NDS. Indeed, the original NDS aggregates six metrics, including mAP, mATE, mASE, mAOE, mAVE and mAAE. As velocity and attributes are present only in nuScenes, we adopt the figure of merit\nN\nâ€‹\nD\nâ€‹\nS\nâˆ—\n^\nNDS^{\\hat{*}}\n, proposed by DG-BEV\n[\n30\n]\nto not involve mAVE and mAAE.\nN\nâ€‹\nD\nâ€‹\nS\nâˆ—\n^\nNDS^{\\hat{*}}\nis computed as:\nN\nD\nS\nâˆ—\n^\n=\n1\n6\n(\n3\nm\nA\nP\n+\nâˆ‘\nm\nâ€‹\nT\nâ€‹\nP\nâˆˆ\nğ•‹\nâ€‹\nâ„™\n(\n1\nâˆ’\nm\ni\nn\n(\n1\n,\nm\nT\nP\n)\n)\nNDS^{\\hat{*}}=\\frac{1}{6}(3mAP+\\sum_{mTP\\in\\mathbb{T}\\mathbb{P}}(1-min(1,mTP))\n(12)\nwhere\nğ•‹\nâ€‹\nâ„™\n=\n{\nm\nâ€‹\nA\nâ€‹\nT\nâ€‹\nE\n,\nm\nâ€‹\nA\nâ€‹\nS\nâ€‹\nE\n,\nm\nâ€‹\nA\nâ€‹\nO\nâ€‹\nE\n}\n\\mathbb{T}\\mathbb{P}=\\{mATE,mASE,mAOE\\}\n. While nuScenes provides annotations in the ring-view, KITTI is limited to the front-view. Thus, for a fair comparison, in these experiments, we limit the evaluation of nuScenes models only to the front-view, i.e., the field of view of the front camera.\n5.3\nCompetitors\nWe compare LCF3D against state-of-the-art multimodal models\n[\n19\n,\n8\n,\n17\n,\n33\n,\n39\n,\n9\n]\non the KITTI validation set. Results and inference speeds are taken from the corresponding official papers.\nTo test Domain Generalization, we compare LCF3D, configured with PointPillars\n[\n7\n]\nin the LiDAR branch and FasterRCNN\n[\n23\n]\nin the RGB branch, against a representative early fusion technique (MVXNet\n[\n28\n]\n) and an intermediate fusion solution (BEVFusion\n[\n11\n]\n). We do not include Domain Adaptation methods in the analysis, as they pursue a different objective, whereas our goal is to evaluate generalization to unseen domains without any adaptation.\n5.4\nImplementation Details\n5.4.1\nLCF3D configuration on KITTI\nLiDAR Detectors\n. We test the pre-trained PointPillars\n[\n7\n]\n, PV-RCNN\n[\n25\n]\nand PartA2\n[\n27\n]\nmodels by MMDetection3D and train SECOND\n[\n34\n]\non the KITTI training split for 80 epochs using standard augmentations (object noise, BEV random flip, and ground-truth sampling).\nRGB Detectors\n. A Faster R-CNN\n[\n23\n]\nwith ResNet101-FPN backbone taken from MMDetection is fine-tuned on the left KITTI images.\nFrustum Localizer\n.\nFrustum PointNet\n[\n19\n]\nis re-implemented within MMDetection3D and trained on frustums from 2D ground-truth boxes, with separate models for single- and stereo-view setups.\n5.4.2\nLCF3D configuration on nuScenes\nLiDAR Detectors\n. We use pre-trained models (PointPillars\n[\n7\n]\n, SSN\n[\n40\n]\nand CenterPoint\n[\n36\n]\n) on the nuScenes dataset from the MMDetection3D framework.\nRGB Detectors\n. For Object Detection, we train a Faster RCNN\n[\n23\n]\nmodel with ResNet50 as backbone and an FPN as the neck, and a DDQ-DETR\n[\n38\n]\nnetwork with Swin-L as backbone. For Instance Segmentation, we use DetectorRS\n[\n22\n]\n. All the models were trained using MMDetectionâ€™s framework for 12 epochs.\nFrustum Localizer\n. We train a Frustum Pointnet of single-view Frustum Proposals extracted from 2D bounding boxes obtained through the projection of the 3D ones in the images, as in\n5.4.1\n. We also train the Frustum Localizer on frustums extracted from 2D instance segmentation masks. We use the trained DetectorRS model to generate instance masks that we match with 3D ground truths using Bounding Box Matching.\n5.5\nResults on KITTI\nTable 2\n:\nComparison with single modal detectors (3D AP) on the KITTI val set. Blue denotes the\nbest overall performance\n, while green denotes the\nbest performance among the rows\nwith the same 3D detector on the LiDAR branch. Rows having an empty RGB branch and RGB setting denote the single-modal LiDAR solution.\nLiDAR branch\nRGB branch\nRGB setting\nCar\nA\nâ€‹\nP\n3\nâ€‹\nd\nâ†‘\nAP_{3d}\\uparrow\nPedestrian\nA\nâ€‹\nP\n3\nâ€‹\nd\nâ†‘\nAP_{3d}\\uparrow\nCyclist\nA\nâ€‹\nP\n3\nâ€‹\nd\nâ†‘\nAP_{3d}\\uparrow\nEasy\nMod.\nHard\nEasy\nMod.\nHard\nEasy\nMod.\nHard\nSECOND\n-\n-\n87.83\n78.46\n73.75\n59.12\n52.78\n47.41\n75.58\n61.73\n58.18\nSECOND\nFaster RCNN\nStereo\n88.41\n79.45\n74.35\n65.98\n59.73\n53.47\n85.24\n73.44\n69.23\nSECOND\nFaster RCNN\nSingle\n88.31\n79.04\n74.29\n66.33\n61.84\n55.80\n85.49\n74.24\n69.54\nPointPillars\n-\n-\n88.52\n79.29\n76.34\n57.27\n51.00\n46.44\n83.88\n62.77\n59.50\nPointPillars\nFaster RCNN\nStereo\n89.45\n80.29\n77.24\n70.38\n63.98\n58.76\n88.07\n73.88\n69.07\nPointPillars\nFaster RCNN\nSingle\n89.17\n80.21\n77.44\n69.81\n64.66\n59.90\n87.13\n75.74\n70.86\nPartA2\n-\n-\n92.45\n82.88\n80.64\n60.61\n53.59\n48.86\n90.45\n70.17\n65.52\nPartA2\nFaster RCNN\nStereo\n92.98\n83.80\n81.37\n72.44\n65.52\n58.98\n94.01\n79.39\n74.28\nPartA2\nFaster RCNN\nSingle\n92.75\n83.44\n81.16\n72.67\n67.35\n61.11\n93.64\n79.94\n74.99\nPV-RCNN\n-\n-\n91.82\n84.53\n82.42\n66.72\n59.27\n54.31\n90.36\n73.26\n69.36\nPV-RCNN\nFaster RCNN\nStereo\n92.93\n86.31\n83.34\n73.86\n68.44\n63.61\n91.01\n77.25\n72.01\nPV-RCNN\nFaster RCNN\nSingle\n92.81\n86.30\n83.63\n73.89\n68.60\n63.80\n91.88\n77.47\n72.44\nTable\n2\ncompares our performance with LiDAR-based 3D Object Detectors. We group the rows based on the specific 3D Detector used in the LiDAR branch and report the metrics of both single and stereo-view settings. LCF3D significantly outperforms single-modal detectors on highly imbalanced classes (Pedestrians and Cyclists) in all difficulty levels, especially when these are distant from the sensor (moderate and hard). In the easy case, stereo vision is more reliable, while single-view yields better results in the moderate and hard cases for both Pedestrians and Cyclists.\nIndeed, the stereo setup improves detection of nearby objects but is less effective for distant ones. The epipolar-based matching requires consistent 2D detections in both views, which benefits easy cases but limits Detection Recovery when objects are far or partially occluded, resulting in fewer and sparser frustum proposals than in the single-view setting.\nIn Table\n3\n, we collate the results of LCF3D with other multi-modal solutions on the KITTI validation set. VirConv\n[\n33\n]\nstill outperforms our method for Cars, but for Pedestrians and Cyclists we achieve state-of-the-art results by combining PV-RCNN on the LiDAR branch and Faster R-CNN on the RGB branch in the single-view setting (\nLCF3D-Single (FR + PV)\nin Table\n3\n). Moreover, using PointPillars, our approach ensure significantly lower computational times compared to other multi-modal solutions, while remaining competitive. Thus, although the comparison may not be based on the same hardware architectures, it remains indicative, as we found that our measured inference speeds of the PointPillars and PV-RCNN are in line with the official ones.\nTable 3\n:\nPerformance comparison with multi-modal solutions on the KITTI val set, using Faster RCNN (FR) in the RGB branch and PointPillars (PP) or PV-RCNN (PV) in the LiDAR branch. Best results are in\nbold\n, second best results are\nunderlined\n. Inference speed is taken from the original publications, when available.\nDetector\nSpeed\n(FPS\nâˆ—\n)\nCar\nA\nâ€‹\nP\n3\nâ€‹\nd\nâ†‘\nAP_{3d}\\uparrow\nPedestrian\nA\nâ€‹\nP\n3\nâ€‹\nd\nâ†‘\nAP_{3d}\\uparrow\nCyclist\nA\nâ€‹\nP\n3\nâ€‹\nd\nâ†‘\nAP_{3d}\\uparrow\nEasy\nMod.\nHard\nEasy\nMod.\nHard\nEasy\nMod.\nHard\nCLOCs-PVCas\n[\n17\n]\n-\n89.49\n79.31\n77.36\n62.88\n56.20\n50.10\n87.57\n67.92\n63.67\nFrustum PointNet\n[\n19\n]\n5.9\n83.76\n70.92\n63.65\n70.00\n61.32\n53.59\n77.15\n56.49\n53.37\nFrustum PointPillars\n[\n16\n]\n14.3\n88.90\n79.28\n78.07\n66.11\n61.89\n56.91\n87.54\n72.78\n66.07\nPointPainting\n[\n29\n]\n-\n88.38\n77.74\n76.76\n69.38\n61.67\n54.58\n85.21\n71.62\n66.98\nMVXNet\n[\n28\n]\n-\n88.48\n78.75\n74.34\n58.27\n55.51\n51.83\n79.15\n63.25\n60.56\nCAT-Det\n[\n39\n]\n10.2\n90.12\n81.46\n79.15\n74.08\n66.35\n58.92\n87.64\n72.82\n68.20\nVirConv-T\n[\n33\n]\n10.2\n94.98\n89.96\n88.13\n73.32\n66.93\n60.38\n90.04\n73.90\n69.06\nLoGoNet\n-\n92.04\n85.04\n84.31\n70.20\n63.72\n59.46\n91.74\n75.35\n72.42\nMLF-DET-V\n10.8\n89.70\n87.31\n79.34\n71.15\n68.50\n61.72\n86.05\n72.14\n65.42\nLCF3D-Single (FR + PP)\n30.4\n89.30\n80.03\n77.23\n69.81\n64.66\n59.90\n87.13\n75.74\n70.86\nLCF3D-Single (FR + PV)\n10.5\n92.44\n85.99\n83.54\n73.43\n68.18\n63.56\n89.83\n77.27\n72.17\nLCF3D-Stereo (FR + PV)\n10.1\n92.95\n86.09\n83.32\n73.87\n67.40\n62.67\n91.01\n77.25\n72.01\n5.6\nResults on nuScenes\nResults on the validation set of nuScenes are collected in Table\n4\n. The rundown of the experiment is similar to the single-view case of KITTI. The benefits of LCF3D are marginal for Cars. However, the improvements are very noticeable for imbalanced classes such as Bicycles and Motorcycles, as well as for classes associated with small objects like Traffic Cones and Barriers. The advantages are less evident for Pedestrians, as they do not constitute an imbalanced class, as confirmed by the strong performance of the LiDAR branch alone. However, for this category, our method still provides significant improvements over PointPillars and SSN. Interestingly, we do not observe improvements for CenterPoint on Pedestrians, suggesting that the RGB detectors we employed do not outperform the baseline of CenterPoint in this class.\nThe modular design of LCF3D makes it compatible with different RGB and LiDAR detectors without architectural modification. We verified this by combining various backbones (e.g., Faster R-CNN, DDQ-DETR, DetectorRS PointPillars, SSN, CenterPoint), and observed consistent improvements across all setups, as shown in Table\n4\n. Overall, these results confirm the generalization ability of LCF3D while showing that its performance naturally depends on the reliability of the underlying single-modal detectors.\nTable 4\n:\nResults on the nuScenes validation set. Con.V., Pedes. Motor. and TC are abbreviations for Construction Vehicles, Pedestrians, Motorcycles and Traffic Cones, respectively. Please note that DetectorRS (*) is trained only on nuImages, while Faster RCNN and DDQ are also trained on nuScenes. In green we report the best results among variants with the same LiDAR detector, in blue the best overall performance.\nLiDAR branch\nRGB branch\nmAP\nâ†‘\n\\uparrow\nNDS\nâ†‘\n\\uparrow\nCar\nâ†‘\n\\uparrow\nTruck\nâ†‘\n\\uparrow\nBus\nâ†‘\n\\uparrow\nTrailer\nâ†‘\n\\uparrow\nCon.V.\nâ†‘\n\\uparrow\nPedes.\nâ†‘\n\\uparrow\nMotor.\nâ†‘\n\\uparrow\nBicycle\nâ†‘\n\\uparrow\nTC\nâ†‘\n\\uparrow\nBarrier\nâ†‘\n\\uparrow\nPointPillars\n[\n7\n]\n-\n0.390\n0.526\n0.797\n0.354\n0.427\n0.256\n0.050\n0.682\n0.382\n0.105\n0.334\n0.515\nPointPillars\nFasterRCNN\n0.533\n0.588\n0.797\n0.430\n0.474\n0.235\n0.157\n0.812\n0.612\n0.512\n0.697\n0.606\nPointPillars\nDDQ\n0.570\n0.609\n0.813\n0.490\n0.551\n0.307\n0.198\n0.830\n0.655\n0.536\n0.701\n0.618\nPointPillars\nDetectorRS*\n0.533\n0.585\n0.796\n0.454\n0.454\n0.210\n0.153\n0.779\n0.615\n0.518\n0.626\n0.653\nSSN\n[\n40\n]\n-\n0.459\n0.577\n0.827\n0.518\n0.611\n0.314\n0.158\n0.666\n0.473\n0.219\n0.271\n0.536\nSSN\nFasterRCNN\n0.570\n0.627\n0.821\n0.532\n0.606\n0.243\n0.215\n0.813\n0.637\n0.552\n0.664\n0.621\nSSN\nDDQ\n0.607\n0.648\n0.837\n0.591\n0.668\n0.311\n0.251\n0.833\n0.682\n0.575\n0.676\n0.642\nSSN\nDetectorRS*\n0.560\n0.614\n0.822\n0.536\n0.625\n0.222\n0.198\n0.779\n0.629\n0.549\n0.576\n0.660\nCenterPoint\n[\n36\n]\n-\n0.554\n0.641\n0.845\n0.523\n0.666\n0.359\n0.156\n0.827\n0.529\n0.344\n0.638\n0.653\nCenterPoint\nFasterRCNN\n0.609\n0.661\n0.829\n0.542\n0.620\n0.335\n0.233\n0.847\n0.654\n0.591\n0.747\n0.689\nCenterPoint\nDDQ\n0.635\n0.674\n0.846\n0.592\n0.724\n0.382\n0.251\n0.865\n0.680\n0.594\n0.739\n0.675\nCenterPoint\nDetectorRS*\n0.609\n0.658\n0.832\n0.550\n0.663\n0.302\n0.215\n0.820\n0.655\n0.580\n0.730\n0.740\n5.7\nAblation Studies\nWe assess each LCF3D module, using its modularity to form selective baselines.\nUsing Bounding Box Matching alone produces matched 3D detections with classes predicted by the LiDAR branch. Adding Semantic Fusion replaces the class labels with those from the RGB branch and adjusts the detection scores. Semantic Fusion depends on matched detections from both modalities, thus cannot be tested alone.\nThe Detection Recovery only baseline represents a pure cascade-fusion setup, where 3D boxes are generated solely from frustum proposals built on all the RGB detections, as there is no LiDAR branch. This modular evaluation allows direct quantification of each componentâ€™s contribution.\n5.7.1\nModules Contribution\nTable 5\n:\nAblation studies on the KITTI val set (Single View).\nBbox Match.\nDet. Recovery\nSem. Fusion\nOverall\nA\nâ€‹\nP\n3\nâ€‹\nd\nâ†‘\nAP_{3d}\\uparrow\nSpeed\n(FPS)\nâ†‘\n\\uparrow\nEasy\nMod.\nHard\nâœ—\nâœ—\nâœ—\n76.56\n64.35\n60.77\n37.1\nâœ—\nâœ“\nâœ—\n68.95\n68.34\n64.34\n8.2\nâœ“\nâœ—\nâœ—\n81.09\n70.21\n65.12\n35.8\nâœ“\nâœ“\nâœ—\n80.97\n71.97\n67.83\n30.4\nâœ“\nâœ—\nâœ“\n81.52\n71.32\n66.95\n35.8\nâœ“\nâœ“\nâœ“\n82.08\n73.48\n69.33\n30.4\nWe evaluate the contribution of each component of our module on the KITTI validation set, using the single-modal detector PointPillars as a baseline.\nTable\n5\nreports the overall 3D AP on the KITTI validation set, aggregated w.r.t. the difficulty of the detections. The first line reports the performance of PointPillars, where we report the inference time of the RGB branch (the Faster RCNN) to have a fair comparison with the other lines. The other rows of the tables refer to computations that are added on top of the LiDAR and RGB branches. The Bbox Matching module, shown in the third row, significantly improves the metrics by reducing the FP detections, confirming our hypothesis that the precision of the RGB branch is higher due to the accurate semantic information contained in it.\nIt is worth noting that the RGB branch also generally exhibits a higher recall than the LiDAR branch, particularly for small or distant objects such as pedestrians, cyclists, and far-away vehicles.\nThe improvements observed in our ablation study (fourth row of Table\n5\n) when enabling Detection Recovery confirm that many unmatched RGB detections correspond to true positives missed by LiDAR, thereby substantiating the recall advantage of the RGB branch.\nIn the fifth row, we show that enabling it together with the Bounding Box Matching, we further enhance the performance of Bounding Box Matching alone, confirming that the RGB branch provides a more reliable semantic information. Finally, combining all modules yields, as expected, the best overall performance. Notably, by enabling Detection Recovery only (second line), we obtain worse performance than the LiDAR branch in the easy setting, but better performance in medium and hard settings. Additionally, as expected, Detection Recovery alone is not suitable for real-time applications.\n5.7.2\nComputational Complexity\nWe evaluate the inference speed of LCF3D on an A100 GPU to assess real-time applicability. As shown in Tables\n3\nand\n5\n, PointPillars runs at 62.5 FPS and Faster R-CNN at 37 FPS. Since RGB and LiDAR branches can operate in parallel\n[\n29\n]\n, the overall rate is determined by the slower branch. The additional cost introduced by our three fusion modules is minimal, allowing the system to meet real-time requirements given typical LiDAR sensor rates (10â€“20 FPS).\nTable\n6\nfurther reports computational costs: LCF3D achieves an excellent trade-off between efficiency and accuracy, with most of the extra time (\nâˆ¼\n\\sim\n5 ms) due to the Detection Recovery step, which depends on the number of unmatched RGB detections. Despite this, LCF3D remains faster and more memory-efficient than MVX-Net\n[\n28\n]\nand BEVFusion\n[\n11\n]\n, confirming that the late-cascade design yields significant accuracy gains with negligible computational penalty.\nTable 6\n:\nComputational complexity is evaluated on KITTI, comparing single-view LCF3D with MVXNet and BEVFusion. Final inference time considers parallel execution, taking the slower branch.\nModule\nSpeed (ms)\nGFLOPS\nGPU Memory (MB)\nLiDAR branch (PointPillars)\n16.04\n62.5\n458.17\nRGB branch (Faster RCNN)\n26.95\n320.38\n467.79\nBounding Box Matching\n0.88\n0.00\n268.72\nDetection Recovery\n5.06\n2.23\n405.57\nSemantic Fusion\n0.14\n0.00\n268.71\nLCF3D\n33.03\n385.11\n467.79\nMVXNet\n[\n28\n]\n96.76\n311.93\n922.54\nBEVFusion\n[\n11\n]\n70.37\n411.63\n1291.42\n5.7.3\nBounding Box Matching\nWe evaluate the impact of our improved Bounding Box Matching module relative to our previous work\n[\n24\n]\n. To isolate its effect, we disable the Detection Recovery module and compute 3D AP across multiple IoU thresholds on the KITTI dataset. Results in Figure\n9\nshow that the new matching strategy yields higher AP at lower IoU thresholds, as removing NMS increases the likelihood of retaining True Positive LiDAR detections even with slightly inaccurate boxes.\nSince late fusion cannot correct localization errors, the final bounding box quality depends on the LiDAR branch.\nSince in real-world scenarios it is often more important to detect an object, even with a slightly inaccurate bounding box, than to miss it entirely, our improved Bounding Box Matching allows us to retain True Positive objects with a low IoU with the ground truth.\nThanks to our Clustered Detections, we can preserve such objects without affecting the removal of actual FPs in the previous version of the Bounding Box Matching.\nFigure 9\n:\nPlots comparing the Bounding Box Matching module, by removing Detection Recovery, of LCF3D with our previous version in\n[\n24\n]\n, denoted with LCF3D*.\n5.7.4\n2D Object Detection vs Instance Segmentation\nTable 7\n:\nThe effect of instance segmentation masks in the Frustum Localizer performance (Cascade Fusion), using DetectorRS as RGB detector. The FPS are reported for the Detection Recovery module only.\nFrustum Proposals\nmAP\nâ†‘\n\\uparrow\nNDS\nâ†‘\n\\uparrow\nPedestr.\nâ†‘\n\\uparrow\nMotor.\nâ†‘\n\\uparrow\nBicycle\nâ†‘\n\\uparrow\nFPS\nâ†‘\n\\uparrow\nBbox\n0.265\n0.283\n0.447\n0.305\n0.347\n5.26\nBbox + Mask\n0.295\n0.306\n0.461\n0.365\n0.382\n4.54\nMask\n0.316\n0.333\n0.531\n0.379\n0.377\n6.67\nIn Table\n7\n, we compare the three approaches for extracting Frustum Proposals shown in Figure\n6\n. Namely, i)\nBbox\nextracts proposals from 2D bounding boxes, ii)\nMask\nuses instance segmentation masks, and iii)\nBbox+Mask\nadds the mask as an additional channel to the bounding box input. We report the performance of a vanilla cascade fusion setup where the LiDAR branch and Bounding Box Matching module are removed, i.e. all the RGB 2D detections are used as input for the Detection Recovery. We report global mAP, NDS, and per-class mAP for Pedestrians, Motorcycles, and Bicycles. Results show that adding the mask channel improves metrics of\nBbox\nbut increases computational cost. Finally,\nMask\nobtains the best performance as it selects only the points that project inside the instance mask, and has a lower computational overhead, as fewer points are kept in Frustum Proposals.\nThese results confirm that LiDAR branch and late fusion are necessary, as cascade fusion alone is insufficient for real-time performance, and LCF3D outperforms it using the same RGB detector.\n5.8\nDomain Generalization analysis\nTable 8\n:\nPerformance of our method under domain shifts. We report the\nN\nâ€‹\nD\nâ€‹\nS\nâˆ—\n^\nNDS^{\\hat{*}}\ndefined as in (\n12\n).\nTask\n(\nT\nâ€‹\nr\nâ€‹\na\nâ€‹\ni\nâ€‹\nn\nâ†’\nT\nâ€‹\ne\nâ€‹\ns\nâ€‹\nt\n)\n(Train\\rightarrow Test)\nModel\nN\nâ€‹\nD\nâ€‹\nS\nâˆ—\n^\nâ†‘\nNDS^{\\hat{*}}\\uparrow\nm\nâ€‹\nA\nâ€‹\nP\nâ†‘\nmAP\\uparrow\nm\nâ€‹\nA\nâ€‹\nT\nâ€‹\nE\nâ†“\nmATE\\downarrow\nm\nâ€‹\nA\nâ€‹\nS\nâ€‹\nE\nâ†“\nmASE\\downarrow\nm\nâ€‹\nA\nâ€‹\nO\nâ€‹\nE\nâ†“\nmAOE\\downarrow\nK\nâ€‹\ni\nâ€‹\nt\nâ€‹\nt\nâ€‹\ni\nâ†’\nK\nâ€‹\ni\nâ€‹\nt\nâ€‹\nt\nâ€‹\ni\nKitti\\rightarrow Kitti\nPointPillars\n[\n7\n]\n0.722\n0.696\n0.101\n0.214\n0.443\nMVXNet\n[\n28\n]\n0.714\n0.712\n0.111\n0.204\n0.537\nBEVFusion\n[\n11\n]\n0.804\n0.771\n0.099\n0.227\n0.161\nOurs\n0.821\n0.908\n0.101\n0.222\n0.475\nK\nâ€‹\ni\nâ€‹\nt\nâ€‹\nt\nâ€‹\ni\nâ†’\nn\nâ€‹\nu\nâ€‹\nS\nâ€‹\nc\nâ€‹\ne\nâ€‹\nn\nâ€‹\ne\nâ€‹\ns\nKitti\\rightarrow nuScenes\nPointPillars\n0.152\n0.059\n0.731\n0.739\n0.798\nMVXNet\n0.127\n0.025\n0.768\n0.752\n0.791\nBEVFusion\n0.243\n0.091\n0.408\n0.406\n1.232\nOurs\n0.234\n0.157\n0.516\n0.552\n1.067\nn\nâ€‹\nu\nâ€‹\nS\nâ€‹\nc\nâ€‹\ne\nâ€‹\nn\nâ€‹\ne\nâ€‹\ns\nâ†’\nn\nâ€‹\nu\nâ€‹\nS\nâ€‹\nc\nâ€‹\ne\nâ€‹\nn\nâ€‹\ne\nâ€‹\ns\nnuScenes\\rightarrow nuScenes\nPointPillars\n0.577\n0.427\n0.267\n0.245\n0.309\nMVXNet\n0.586\n0.452\n0.233\n0.251\n0.356\nBEVFusion\n0.685\n0.612\n0.181\n0.242\n0.306\nOurs\n0.539\n0.459\n0.305\n0.310\n0.528\nn\nâ€‹\nu\nâ€‹\nS\nâ€‹\nc\nâ€‹\ne\nâ€‹\nn\nâ€‹\ne\nâ€‹\ns\nâ†’\nK\nâ€‹\ni\nâ€‹\nt\nâ€‹\nt\nâ€‹\ni\nnuScenes\\rightarrow Kitti\nPointPillars\n0.455\n0.414\n0.189\n0.325\n1.048\nMVXNet\n0.449\n0.379\n0.178\n0.340\n0.926\nBEVFusion\n0.525\n0.474\n0.169\n0.359\n0.743\nOurs\n0.545\n0.613\n0.198\n0.371\n1.094\nFigure 10\n:\nThe 3D Average Precision, as the IoU threshold varies, under distribution shifts.\nWe evaluate the performance of LCF3D when tested on a different dataset from the one it was trained on. For models trained on nuScenes, sweeps are used to augment the Point Cloud and usually an additional channel, representing the timestamp difference w.r.t the current frame, is added to each point. Since KITTI lacks sweeps, to test nuScenes models on KITTI, we perform inference on a single frame and set this channel to zero in each 3D point. For MVXNet and BEVFusion, we use the open-source implementations from MMDetection3D. We train MVXNet on KITTI for 40 epochs by using the suggested parameters by the framework, and on nuScenes for 24 epochs using ResNet-50 as backbone for the image modality and SECOND as voxel encoder. For BEVFusion, we use the pre-trained nuScenes model by MMDetection3D, and train the KITTI model by pre-training the LiDAR backbone for 3D Object Detection and fine-tuning it together with the RGB modality.\nTable\n8\nshows nuScenes metrics. While BEVFusion outperforms across all metrics on nuScenes, under domain shift (e.g., nuScenes\nâ†’\n\\to\nKITTI) our method achieves higher mAP, despite BEVFusion having lower mATE, mASE, and mAOE for matched true positives.\nSince mAP reflects the ability to correctly detect objects (penalizing both FPs and FNs), this result indicates that our approach is more robust in maintaining correct detections under domain shift, even though it cannot improve the precision of the bounding boxes themselves. The reason is that our framework leverages pre-estimated LiDAR detections: the geometry of each bounding box is bounded by the LiDAR detectorâ€™s performance, but the Bounding Box Matching procedure reduces the number of spurious detections, leading to fewer FPs, and the Detection Recovery can find missed objects, leading to fewer FNs.\nThis robustness is important in real-time scenarios, where missing or hallucinating objects are more critical than slight inaccuracies in box geometry. Additionally, using a more generalizable LiDAR detector could directly improve mATE, mASE, and mAOE.\nFigure\n10\nshows the 3D AP w.r.t. IoU thresholds. On the KITTI dataset, LCF3D almost recovers the performance of PointPillars trained directly on KITTI, by fusing a PointPillars model trained on nuScenes with a 2D RGB detector (either DDQ or Faster R-CNN) trained on nuScenes and nuImages. However, for Cars, we are unable to surpass PointPillars. This can be explained by the 3D AP values of PointPillars trained on nuScenes: the 3D AP exceeds\n80\n%\n80\\%\nat an IoU threshold of 0.4, indicating that the model correctly detects many True Positives, albeit with inaccurate bounding boxes. Conversely, for models trained on KITTI and tested on nuScenes, a large performance gap remains even at lower IoUs, mainly due to the higher sparsity of nuScenes point clouds and the fact that KITTI models are trained on single-frame data without leveraging multiple sweeps. Please note that Cyclist performance is affected by differences in dataset definitions: KITTI does not annotate bicycles without riders, unlike nuScenes.\n5.9\nLimitations and Qualitative Results\nFigures\n11\nand\n12\nprovide qualitative examples on KITTI and nuScenes respectively, highlighting the complementary strengths of LiDAR and RGB. On KITTI, the RGB branch improves precision by suppressing LiDAR FPs and recovers distant or small objects missed by LiDAR, though recovery fails when both branches miss an object (second row). On nuScenes, sparser LiDAR point clouds cause more FNs, many of which are recovered by our method, but some frustums lack sufficient points for reliable 3D localization. Orientation estimation from frustums is also challenging with few points (top-right image).\nOverall, qualitative results show that LCF3D reduces FPs via late fusion and recovers FNs through cascade fusion, achieving strong cross-dataset performance, though some limitations remain. LCF3D requires both modalities; if one branch fails systematically, recovery is impossible. Additionally, recovery quality depends on LiDAR point density within frustums, with sparse scenes potentially causing inaccurate localization or orientation.\nFigure 11\n:\nQualitative results on KITTI show object classes by bounding box color: red for pedestrians, yellow for bicycles, and cyan for cars; orange circles indicate FPs, purple circles FNs.\nGB (Faster R-CNN) improves precision by removing LiDAR FPs and can recover missed objects (the car in the last frame) though recovery fails if both branches miss them (second row).\nFigure 12\n:\nQualitative results on the nuScenes dataset.\nSparse LiDAR point clouds cause more FNs for distant objects. Our method recovers many, but limited points hinder full recovery and orientation estimation.\n6\nConclusions\nWe have proposed a hybrid late-cascade fusion approach that exploits a 3D LiDAR detector, a 2D RGB detector and the geometrical constraints of the scene.\nOur solution increases the performance of single-modal detectors, especially for more challenging classes like Cyclists and Pedestrians and is completely independent of the underlying single-modal detectors, allowing flexible solutions including the usage of pre-trained state-of-the-art models.\nComputationally, LCF3D introduces minimal overhead and offers a strong balance of latency, memory, and accuracy compared to other multimodal approaches, making it suitable for real-world autonomous driving. Limitations include reliance on both modalities and sensitivity to sparse point clouds, which can affect 3D localization and orientation. Future work will explore more robust frustum-based localization and alternative recovery mechanisms using RGB data to compensate for missing LiDAR information.\nAcknowledgements\nThis paper is supported by FAIR (NextGenerationEU program, PNRR-PE-AI scheme, M4C2, Investment 1.3, Line on Artificial Intelligence) and by GEOPRIDE ID: 2022245ZYB, CUP: D53D23008370001 (PRIN 2022 M4.C2.1.1 Investment).\nModel training and testing were possible thanks to the HPC grant from by the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (ID:90254)\nReferences\n[1]\nH. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom\n(2019)\nNuScenes: a multimodal dataset for autonomous driving\n.\narXiv preprint arXiv:1903.11027\n.\nCited by:\nÂ§1\n,\nÂ§1\n,\nÂ§5.1\n,\nÂ§5.2\n,\nÂ§5\n.\n[2]\nX. Chen, H. Ma, J. Wan, B. Li, and T. Xia\n(2017)\nMulti-view 3d object detection network for autonomous driving\n.\nCited by:\nÂ§2.1.2\n,\nÂ§5.1\n.\n[3]\nY. Chen, J. Shi, Z. Ye, C. Mertz, D. Ramanan, and S. Kong\n(2022)\nMultimodal object detection via probabilistic ensembling\n.\nECCV\n.\nCited by:\nÂ§4.1.5\n.\n[4]\nA. Geiger, P. Lenz, and R. Urtasun\n(2012)\nAre we ready for autonomous driving? the kitti vision benchmark suite\n.\nCVPR\n(\n).\nCited by:\nÂ§1\n,\nÂ§1\n,\nÂ§5.1\n,\nÂ§5\n.\n[5]\nD. Hegde, S. Lohit, K. Peng, M. J. Jones, and V. M. Patel\n(2024)\nMultimodal 3d object detection on unseen domains\n.\nCited by:\nÂ§2.2\n.\n[6]\nR. Jonker and A. Volgenant\n(1987)\nA shortest augmenting path algorithm for dense and sparse linear assignment problems\n.\nComputing\n38\n(\n4\n),\npp.Â 325â€“340\n.\nCited by:\nÂ§4.1.3\n.\n[7]\nA. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom\n(2019)\nPointpillars: fast encoders for object detection from point clouds\n.\nCited by:\nÂ§4.1.2\n,\nÂ§5.3\n,\nÂ§5.4.1\n,\nÂ§5.4.2\n,\nTable 4\n,\nTable 8\n.\n[8]\nX. Li, T. Ma, Y. Hou, B. Shi, Y. Yang, Y. Liu, X. Wu, Q. Chen, Y. Li, Y. Qiao,\net al.\n(2023)\nLogonet: towards accurate 3d object detection with local-to-global cross-modal fusion\n.\nCited by:\nÂ§1\n,\nÂ§5.3\n.\n[9]\nZ. Lin, Y. Shen, S. Zhou, S. Chen, and N. Zheng\n(2023)\nMlf-det: multi-level fusion for cross-modal 3d object detection\n.\nCited by:\nÂ§5.3\n.\n[10]\nH. Liu, J. Du, Y. Zhang, H. Zhang, and J. Zeng\n(2024)\nPVConvNet: pixel-voxel sparse convolution for multimodal 3d object detection\n.\n149\n.\nExternal Links:\nISSN 0031-3203\nCited by:\nÂ§2.1.1\n.\n[11]\nZ. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. L. Rus, and S. Han\n(2023)\nBevfusion: multi-task multi-sensor fusion with unified birdâ€™s-eye view representation\n.\nCited by:\nÂ§1\n,\nÂ§2.1.2\n,\nÂ§5.3\n,\nÂ§5.7.2\n,\nTable 6\n,\nTable 8\n.\n[12]\nB. Lu, Y. Sun, Z. Yang, R. Song, H. Jiang, and Y. Liu\n(2024)\nHRNet: 3d object detection network for point cloud with hierarchical refinement\n.\n149\n.\nExternal Links:\nISSN 0031-3203\nCited by:\nÂ§4.1.2\n.\n[13]\nJ. Ma, Y. Huang, C. Qian, J. Kang, J. Liu, H. Zhang, and W. Hong\n(2024)\nLGNet: local and global point dependency network for 3d object detection\n.\n154\n.\nExternal Links:\nISSN 0031-3203\nCited by:\nÂ§4.1.2\n.\n[14]\nY. Ma, N. Peri, S. Wei, W. Hua, D. Ramanan, Y. Li, and S. Kong\n(2023)\nLong-tailed 3d detection via 2d late fusion\n.\narXiv preprint arXiv:2312.10986\n.\nCited by:\nÂ§1\n,\nÂ§2.1.3\n,\nÂ§4.1.3\n,\nÂ§4.1.5\n.\n[15]\nJ. Mao, S. Shi, X. Wang, and H. Li\n(2023)\n3D object detection for autonomous driving: a comprehensive survey\n.\nIJCV\n131\n(\n8\n).\nCited by:\nÂ§2.1\n.\n[16]\nA. Paigwar, D. Sierra-Gonzalez, O. Erkent, and C. Laugier\n(2021)\nFrustum-pointpillars: a multi-stage approach for 3d object detection using rgb camera and lidar\n.\nICCVW\n.\nCited by:\nÂ§2.1.1\n,\nÂ§4.1.4\n,\nTable 3\n.\n[17]\nS. Pang, D. Morris, and H. Radha\n(2020)\nCLOCs: camera-lidar object candidates fusion for 3d object detection\n.\nIROS\n.\nCited by:\nÂ§1\n,\nÂ§2.1.3\n,\nÂ§5.3\n,\nTable 3\n.\n[18]\nN. Peri, A. Dave, D. Ramanan, and S. Kong\n(2023)\nTowards long-tailed 3d detection\n.\nCoRL\n.\nCited by:\nÂ§2.1.3\n,\nÂ§4.1.3\n.\n[19]\nC. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas\n(2018)\nFrustum pointnets for 3d object detection from rgb-d data\n.\nCVPR\n.\nCited by:\nÂ§1\n,\nÂ§2.1.1\n,\nÂ§4.1.4\n,\nÂ§5.3\n,\nÂ§5.4.1\n,\nTable 3\n.\n[20]\nR. Qian, X. Lai, and X. Li\n(2022)\n3D object detection for autonomous driving: a survey\n.\nPattern Recognition\n130\n.\nCited by:\nÂ§1\n,\nÂ§2.1.2\n.\n[21]\nR. Qian, X. Lai, and X. Li\n(2022)\nBADet: boundary-aware 3d object detection from point clouds\n.\n125\n.\nCited by:\nÂ§4.1.2\n.\n[22]\nS. Qiao, L. Chen, and A. Yuille\n(2021)\nDetectors: detecting objects with recursive feature pyramid and switchable atrous convolution\n.\nCited by:\nÂ§5.4.2\n.\n[23]\nS. Ren, K. He, R. Girshick, and J. Sun\n(2015)\nFaster r-cnn: towards real-time object detection with region proposal networks\n.\nCited by:\nÂ§4.1.1\n,\nÂ§5.3\n,\nÂ§5.4.1\n,\nÂ§5.4.2\n.\n[24]\nC. Sgaravatti, R. Basla, R. Pieroni, M. Corno, S. M. Savaresi, L. Magri, and G. Boracchi\n(2025)\nA multimodal hybrid late-cascade fusion network forÂ enhanced 3d object detection\n.\nComputer Vision â€“ ECCV 2024 Workshops\n.\nCited by:\nÂ§1\n,\nFigure 9\n,\nFigure 9\n,\nÂ§5.7.3\n.\n[25]\nS. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li\n(2020)\nPv-rcnn: point-voxel feature set abstraction for 3d object detection\n.\nCited by:\nÂ§5.4.1\n.\n[26]\nS. Shi, X. Wang, and H. Li\n(2019)\nPointrcnn: 3d object proposal generation and detection from point cloud\n.\nCited by:\nÂ§4.1.2\n.\n[27]\nS. Shi, Z. Wang, J. Shi, X. Wang, and H. Li\n(2020)\nFrom points to parts: 3d object detection from point cloud with part-aware and part-aggregation network\n.\n43\n(\n8\n),\npp.Â 2647â€“2664\n.\nCited by:\nÂ§5.4.1\n.\n[28]\nV. A. Sindagi, Y. Zhou, and O. Tuzel\n(2019)\nMvx-net: multimodal voxelnet for 3d object detection\n.\nCited by:\nÂ§1\n,\nÂ§2.1.1\n,\nÂ§5.3\n,\nÂ§5.7.2\n,\nTable 3\n,\nTable 6\n,\nTable 8\n.\n[29]\nS. Vora, A. H. Lang, B. Helou, and O. Beijbom\n(2020)\nPointpainting: sequential fusion for 3d object detection\n.\nCVPR\n.\nCited by:\nÂ§1\n,\nÂ§2.1.1\n,\nÂ§5.7.2\n,\nTable 3\n.\n[30]\nS. Wang, X. Zhao, H. Xu, Z. Chen, D. Yu, J. Chang, Z. Yang, and F. Zhao\n(2023)\nTowards domain generalization for multi-view 3d object detection in bird-eye-view\n.\nCited by:\nÂ§1\n,\nÂ§5.2\n.\n[31]\nY. Wang, Q. Mao, H. Zhu, J. Deng, Y. Zhang, J. Ji, H. Li, and Y. Zhang\n(2023)\nMulti-modal 3d object detection in autonomous driving: a survey\n.\nIJCV\n131\n(\n8\n).\nCited by:\nÂ§1\n,\nÂ§1\n,\nÂ§2.1.1\n,\nÂ§2.1.2\n,\nÂ§4\n.\n[32]\nM. K. Wozniak, M. Hansson, M. Thiel, and P. Jensfelt\n(2024)\nUada3d: unsupervised adversarial domain adaptation for 3d object detection with sparse lidar and large domain gaps\n.\nCited by:\nÂ§1\n.\n[33]\nH. Wu, C. Wen, S. Shi, X. Li, and C. Wang\n(2023)\nVirtual sparse convolution for multimodal 3d object detection\n.\nCited by:\nÂ§2.1.1\n,\nÂ§5.3\n,\nÂ§5.5\n,\nTable 3\n.\n[34]\nY. Yan, Y. Mao, and B. Li\n(2018)\nSECOND: sparsely embedded convolutional detection\n.\n18\n(\n10\n).\nExternal Links:\nISSN 1424-8220\nCited by:\nÂ§5.4.1\n.\n[35]\nJ. Yang, S. Shi, Z. Wang, H. Li, and X. Qi\n(2021)\nSt3d: self-training for unsupervised domain adaptation on 3d object detection\n.\nCited by:\nÂ§2.2\n.\n[36]\nT. Yin, X. Zhou, and P. Krahenbuhl\n(2021)\nCenter-based 3d object detection and tracking\n.\nCited by:\nÂ§5.4.2\n,\nTable 4\n.\n[37]\nR. Zhang, J. Lee, X. Cai, and A. Prugel-Bennett\n(2024)\nRevisiting cross-domain problem for lidar-based 3d object detection\n.\nCited by:\nÂ§2.2\n.\n[38]\nS. Zhang, X. Wang, J. Wang, J. Pang, C. Lyu, W. Zhang, P. Luo, and K. Chen\n(2023)\nDense distinct query for end-to-end object detection\n.\nCited by:\nÂ§5.4.2\n.\n[39]\nY. Zhang, J. Chen, and D. Huang\n(2022)\nCat-det: contrastively augmented transformer for multi-modal 3d object detection\n.\nCited by:\nÂ§5.3\n,\nTable 3\n.\n[40]\nX. Zhu, Y. Ma, T. Wang, Y. Xu, J. Shi, and D. Lin\n(2020)\nSsn: shape signature networks for multi-class object detection from point clouds\n.\nCited by:\nÂ§4.1.2\n,\nÂ§5.4.2\n,\nTable 4\n.",
  "preview_text": "Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D.\n\nLCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving\nâ€ \nâ€ \nthanks:\nThis paper has been accepted for publication in\nPattern Recognition\n, 2026.\nThe final version is available at\nhttps://doi.org/10.1016/j.patcog.2026.113046\n.\nCarlo Sgaravatti\ncarlo.sgaravatti@polimi.it\nRiccardo Pieroni\nriccardo.pieroni@polimi.it\nMatteo Corno\nmatteo.corno@polimi.it\nSergio M. Savaresi\nsergio.savaresi@polimi.it\nLuca Magri\nluca.magri@polimi.it\nGiacomo Boracchi\ngiacomo.boracchi@polimi.it\nAbstract\nAccurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonom",
  "is_relevant": false,
  "relevance_score": 1.0,
  "extracted_keywords": [
    "3D object detection",
    "sensor fusion",
    "autonomous driving",
    "LiDAR",
    "RGB cameras",
    "late fusion",
    "cascade fusion"
  ],
  "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºè‡ªåŠ¨é©¾é©¶ä¸­3Dç‰©ä½“æ£€æµ‹çš„é²æ£’å®æ—¶ä¼ æ„Ÿå™¨èåˆæ¡†æ¶ï¼Œç»“åˆäº†RGBå›¾åƒå’ŒLiDARç‚¹äº‘æ•°æ®ï¼Œä½†ä¸å¼ºåŒ–å­¦ä¹ ã€VLAã€æ‰©æ•£æ¨¡å‹ã€Flow Matchingã€è¿åŠ¨æ§åˆ¶ã€VLMå’Œå…¨èº«æ§åˆ¶ç­‰å…³é”®è¯æ— å…³ã€‚",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-14T19:19:37Z",
  "created_at": "2026-01-20T17:49:51.642489",
  "updated_at": "2026-01-20T17:49:51.642497"
}