{
    "id": "2601.08828v1",
    "title": "Motion Attribution for Video Generation",
    "authors": [
        "Xindi Wu",
        "Despoina Paschalidou",
        "Jun Gao",
        "Antonio Torralba",
        "Laura Leal-TaixÃ©",
        "Olga Russakovsky",
        "Sanja Fidler",
        "Jonathan Lorraine"
    ],
    "abstract": "å°½ç®¡è§†é¢‘ç”Ÿæˆæ¨¡å‹å‘å±•è¿…é€Ÿï¼Œä½†æ•°æ®å¯¹è¿åŠ¨çš„å½±å“æœºåˆ¶ä»ä¸æ˜ç¡®ã€‚æˆ‘ä»¬æå‡ºMotiveï¼ˆè§†é¢‘ç”Ÿæˆè¿åŠ¨å½’å› æ¡†æ¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥è¿åŠ¨ä¸ºæ ¸å¿ƒã€åŸºäºæ¢¯åº¦çš„æ•°æ®å½’å› æ¡†æ¶ï¼Œèƒ½å¤Ÿé€‚é…ç°ä»£å¤§è§„æ¨¡é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ä¸æ¨¡å‹ã€‚é€šè¿‡è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å“ªäº›å¾®è°ƒç‰‡æ®µä¼šæ”¹å–„æˆ–æŸå®³æ—¶åºåŠ¨æ€æ•ˆæœã€‚Motiveé€šè¿‡è¿åŠ¨åŠ æƒæŸå¤±æ©ç å°†æ—¶åºåŠ¨æ€ä¸é™æ€å¤–è§‚åˆ†ç¦»ï¼Œå®ç°äº†é«˜æ•ˆå¯æ‰©å±•çš„ç‰¹å®šè¿åŠ¨å½±å“è®¡ç®—ã€‚åœ¨æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸­ï¼ŒMotiveèƒ½è¯†åˆ«å¯¹è¿åŠ¨æœ‰æ˜¾è‘—å½±å“çš„è§†é¢‘ç‰‡æ®µï¼Œå¹¶æŒ‡å¯¼æ•°æ®ç­›é€‰ä»¥æå‡æ—¶åºä¸€è‡´æ€§ä¸ç‰©ç†åˆç†æ€§ã€‚ä½¿ç”¨Motiveç­›é€‰çš„é«˜å½±å“åŠ›æ•°æ®åï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨VBenchè¯„æµ‹ä¸­åŒæ—¶æå‡äº†è¿åŠ¨å¹³æ»‘åº¦ä¸åŠ¨æ€è¡¨ç°åº¦ï¼Œç›¸æ¯”é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹è·å¾—74.1%çš„äººç±»åå¥½èƒœç‡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­è¿åŠ¨ï¼ˆè€Œéè§†è§‰å¤–è§‚ï¼‰è¿›è¡Œå½’å› çš„æ¡†æ¶ï¼Œå¹¶é¦–æ¬¡å°†å…¶åº”ç”¨äºå¾®è°ƒæ•°æ®ç­›é€‰ã€‚",
    "url": "https://arxiv.org/abs/2601.08828v1",
    "html_url": "https://arxiv.org/html/2601.08828v1",
    "html_content": "\\dosecttoc\nMotion Attribution for Video Generation\nXindi Wu\n12\nDespoina Paschalidou\n1\nJun Gao\n1\nAntonio Torralba\n3\nLaura Leal-TaixÃ©\n1\nOlga Russakovsky\n2\nSanja Fidler\n1\nJonathan Lorraine\n1\n1\nNVIDIA\n2\nPrinceton University\n3\nMIT CSAIL\nhttps://research.nvidia.com/labs/sil/projects/MOTIVE/\nAbstract\nDespite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present\nMotive\n(\nMOTI\non attribution for\nV\nideo g\nE\nneration\n), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models.\nWe use this to study which fine-tuning clips improve or degrade temporal dynamics.\nMotive\nisolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation.\nOn text-to-video models,\nMotive\nidentifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility.\nWith\nMotive\n-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model.\nTo our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.\n\\abscontent\n1\nIntroduction\nMotion is the defining element of videos. Unlike image generation, which produces a single frame, video generative models capture how objects move, interact, and obey physical constraints\n(\nwiedemer2025video\n;\nkang2024far\n)\n. Yet even with the rapid progress of video generative models, a fundamental question remains:\nWhich training clips influence the motion in generated videos?\nWhy it matters.\nDiffusion models are data-driven, and their progress has tracked the scaling of data and compute\n(\nsaharia2022photorealistic\n;\nnichol2021improved\n;\nho2022video\n;\npeebles2023scalable\n)\n.\nPrior work\n(\nblattmann2023stable\n;\nkaplan2020scaling\n;\nravishankar2025scaling\n)\nshows that training data shapes key generative properties, including visual quality\n(\nrombach2022high\n)\n, semantic fidelity\n(\nnamekata2024emerdiff\n)\n, and compositionality\n(\nwu2025compact\n;\nfavero2025compositional\n)\n.\nMotion is no exception.\nMotion\nrefers to temporal dynamics captured by optical flow, including trajectories, deformations, camera movement, and interactions. If generated motion reflects the data distribution that shaped the model, then attributing motion to influential training clips provides a direct lens on why a model moves the way it does and enables targeted data selection for desired dynamics.\nHigh-quality data often matters most in fine-tuning, where large pretraining corpora are inaccessible and carefully selected clips can have an outsized impact. Motion-specific attribution is therefore especially valuable in the fine-tuning regime, where the goal is to identify which clips most influence temporal coherence and physical plausibility.\nWhy existing methods are limited for motion.\nPrior diffusion data attribution focuses on images and explains static content. Extending these methods to videos naÃ¯vely collapses motion into appearance, missing the temporal structure that distinguishes videos from images. Three challenges drive this gap: (i) localizing motion so attribution focuses on dynamic regions rather than static backgrounds, (ii) scaling to sequences since gradients must integrate across time, and (iii) capturing temporal relations like velocity, acceleration, and trajectory coherence that single-frame attribution cannot measure. Addressing motion attribution requires methods that explicitly model temporal structure, rather than treating time as an additional spatial axis.\nOur method.\nWe introduce\nMotive\n, a motion attribution framework for video generation models that isolates motion-specific influence.\nMotive\ncomputes gradients with motion-aware masking. As a result, the attribution signal emphasizes dynamic regions rather than static appearance. Efficient approximations make the method practical for large, high-quality datasets and video generative models. Our scores trace generated motion back to training clips, enabling target curation and improving motion quality when used to guide fine-tuning.\nOur contributions are:\n1.\nProposing a scalable gradient-based attribution approach for video generation models that is computationally efficient, even at the scale of modern, high-quality data and large generative models (Â§\n3.2\n).\n2.\nAddressing a video-specific bias by correcting frame-length effects in gradient magnitudes, ensuring fair attribution across clips of different durations (Â§\n3.3\n).\n3.\nIntroducing an attribution that emphasizes temporal dynamics to trace which training clips most strongly influence motion quality (Â§\n3.4\n).\n4.\nShowing that we improve motion smoothness and dynamic degree on VBench and in human evaluation (Â§\n4\n), matching, or surpassing, full-dataset fine-tuning performance with only\n10\n%\n10\\%\nof the data, and outperforming motion-unaware attribution baselines (Tables\n1\nand\n2\n).\n2\nBackground\nWe provide notation in App. Â§\nA\nand extended related work in App. Â§\nB\n.\n2.1\nVideo Generation with Diffusion and Flow-Matching Models\nDiffusion and flow matching in latent space.\nLet\np\nğœ½\nâ€‹\n(\nğ¯\nâˆ£\nğœ\n)\np_{\\bm{\\theta}}(\\mathbf{v}\\mid\\mathbf{c})\nbe a conditional generator with parameters\nğœ½\n\\bm{\\theta}\n, where\nğ¯\nâˆˆ\nâ„\nF\nÃ—\nH\nÃ—\nW\nÃ—\n3\n\\mathbf{v}\\in\\mathbb{R}^{F\\times H\\times W\\times 3}\nis a clip of height\nH\nH\n, width\nW\nW\n, and\nF\nF\nframes, and\nğœ\n\\mathbf{c}\ndenotes conditioning such as text or other multimodal metadata (e.g., fps, depth, pose).\nWe operate in VAE latents:\nğ¡\n=\nE\nâ€‹\n(\nğ¯\n)\n\\mathbf{h}=E(\\mathbf{v})\nand train a denoiser or velocity field on noisy latents.\nA noise scheduler supplies time-dependent coefficients\n(\nÎ±\nt\n,\nÏƒ\nt\n)\n(\\alpha_{t},\\sigma_{t})\ncontrolling signal and noise scales, and the forward noising is:\nğ³\nâ€‹\n(\nt\n,\nÏµ\n)\n=\nÎ±\nt\nâ€‹\nğ¡\n+\nÏƒ\nt\nâ€‹\nÏµ\n,\nÏµ\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nğˆ\n)\n,\nt\nâˆˆ\n{\n1\n,\nâ€¦\n,\nT\n}\n.\n\\smash{\\mathbf{z}(t,\\bm{\\epsilon})\\;=\\;\\alpha_{t}\\,\\mathbf{h}\\;+\\;\\sigma_{t}\\,\\bm{\\epsilon},\\quad\\bm{\\epsilon}\\sim\\mathcal{N}(0,\\mathbf{I}),\\quad t\\in\\{1,\\ldots,T\\}.}\\vskip-2.75269pt\n(1)\nDenoising diffusion\n(\nho2020denoising\n)\ntrains a network\nÏµ\nğœ½\nâ€‹\n(\nğ³\n,\nğœ\n,\nt\n)\n\\bm{\\epsilon}_{\\bm{\\theta}}(\\mathbf{z},\\mathbf{c},t)\nto predict the injected noise:\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ¯\n,\nğœ\n)\n=\nğ”¼\nt\n,\nÏµ\nâ€‹\n[\nâ€–\nÏµ\nğœ½\nâ€‹\n(\nğ³\nâ€‹\n(\nt\n,\nÏµ\n)\n,\nğœ\n,\nt\n)\nâˆ’\nÏµ\nâ€–\n2\n2\n]\n.\n\\smash{\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{v},\\mathbf{c})\\;=\\;\\mathbb{E}_{t,\\bm{\\epsilon}}\\!\\left[\\left\\lVert\\bm{\\epsilon}_{\\bm{\\theta}}(\\mathbf{z}(t,\\bm{\\epsilon}),\\mathbf{c},t)-\\bm{\\epsilon}\\right\\rVert_{2}^{2}\\right].}\\vskip-2.75269pt\n(2)\nFlow matching\n(\nlipman2022flow\n;\nalbergo2023stochastic\n)\nlearns a time-dependent vector field\nğŸ\nğœ½\nâ€‹\n(\nğ³\nt\n,\nğœ\n,\nt\n)\n\\mathbf{f}_{\\bm{\\theta}}(\\mathbf{z}_{t},\\mathbf{c},t)\nthat matches the instantaneous velocity\nğ³\nË™\n=\nd\nd\nâ€‹\nt\nâ€‹\nğ³\n\\dot{\\mathbf{z}}=\\tfrac{\\textnormal{d}}{\\textnormal{d}t}\\mathbf{z}\ninduced by a chosen interpolant:\nâ„’\nflow\nâ€‹\n(\nğœ½\n;\nğ¯\n,\nğœ\n)\n=\nğ”¼\nt\n,\nÏµ\nâ€‹\n[\nâ€–\nğŸ\nğœ½\nâ€‹\n(\nğ³\nâ€‹\n(\nt\n,\nÏµ\n)\n,\nğœ\n,\nt\n)\nâˆ’\nğ³\nË™\nâ€‹\n(\nt\n,\nÏµ\n)\nâ€–\n2\n2\n]\n.\n\\smash{\\mathcal{L}_{\\textnormal{flow}}(\\bm{\\theta};\\mathbf{v},\\mathbf{c})\\;=\\;\\mathbb{E}_{t,\\bm{\\epsilon}}\\!\\left[\\left\\lVert\\mathbf{f}_{\\bm{\\theta}}(\\mathbf{z}(t,\\bm{\\epsilon}),\\mathbf{c},t)-\\dot{\\mathbf{z}}(t,\\bm{\\epsilon})\\right\\rVert_{2}^{2}\\right].}\\vskip-2.75269pt\n(3)\nBoth objectives train time-indexed predictors over the latent space by integrating over\nt\nt\nand\nÏµ\n\\bm{\\epsilon}\n, thus gradient-based methods like attribution share similar challenges.\nFrom images to video for generation.\nAdding a temporal axis materially changes modeling and training.\nGeneration must capture spatial appearance and temporal dynamics such as object and camera motion, deformations, and interactions.\nModern systems extend image backbones with temporal capacity, for example, 3D U-Nets or 2D U-Nets augmented with temporal attention, causal or sliding-window context, and factorized space-time blocks, often trained in a latent-video VAE that compresses frames while preserving temporal cues.\nTraining departs from images along several axes, which we address in Â§\n3\n:\n(i)\nCompute and storage.\nLonger sequences multiply the cost of sampling timesteps, noise draws, and frames, motivating fixed-timestep or small-subset estimators that reduce variance without prohibitive cost (Â§\n3.2\n).\n(ii)\nVariable horizon.\nClips vary in\nF\nF\nand frame rate (Â§\n3.3\n).\n(iii)\nTime-specific failure modes.\nTypical artifacts include inconsistent trajectories, temporal flicker, identity drift, and physically implausible dynamics despite sharp individual frames (Â§\n3.4\n).\nMotion representations in videos.\nWe denote our video as\nğ¯\n=\n[\nğŸ\nf\n]\nf\n=\n1\nF\n\\mathbf{v}=[\\mathbf{f}_{f}]_{f=1}^{F}\nwith\nğŸ\nf\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n3\n\\mathbf{f}_{f}\\in\\mathbb{R}^{H\\times W\\times 3}\nbeing the\nf\nf\n-th frame.\nWe represent motion via optical flow between consecutive frames:\nğ…\nf\n:\n{\n1\n,\nâ€¦\n,\nH\n}\nÃ—\n{\n1\n,\nâ€¦\n,\nW\n}\nâ†’\nâ„\n2\n\\mathbf{F}_{f}:\\{1,\\ldots,H\\}\\times\\{1,\\ldots,W\\}\\to\\mathbb{R}^{2}\n, where each flow vector in\nâ„\n2\n\\mathbb{R}^{2}\nencodes the horizontal displacement\nd\nâ€‹\nw\n\\mathrm{d}w\nand vertical displacement\nd\nâ€‹\nh\n\\mathrm{d}h\nof a pixel.\nThe motion magnitude is\nM\nf\nâ€‹\n(\nh\n,\nw\n)\n=\nâ€–\nğ…\nf\nâ€‹\n(\nh\n,\nw\n)\nâ€–\n2\nM_{\\!f}(h,w)=\\|\\mathbf{F}_{f}(h,w)\\|_{2}\n.\nThe\nM\nf\nM_{\\!f}\nover frames\nf\nf\nand pixels\nh\n,\nw\nh,w\nsummarizes the amount and spatial layout of motion in a clip, which we use to provide masks in our motion-weighted loss in Â§\n3\n.\n2.2\nData Attribution\nData attribution measures how individual training samples affect a modelâ€™s predictions\n(\nbae2024training\n)\n.\nA classic approach to data attribution is to use influence functions\n(\nkoh2017understanding\n)\n. Intuitively, the influence of a training sample measures: if we upweight this training example, how much would the modelâ€™s prediction on a test datum change?\nConsider a loss function\nâ„’\nâ€‹\n(\nğœ½\n;\nğ±\n)\n\\mathcal{L}(\\bm{\\theta};\\mathbf{x})\nand a test sample\nğ±\ntest\n\\mathbf{x}_{\\textnormal{test}}\n, the influence of a training point\nğ±\nn\n\\mathbf{x}_{n}\ncan be quantified as:\nI\nâ€‹\n(\nğ±\nn\n,\nğ±\ntest\n)\n=\nâˆ’\nâˆ‡\nğœ½\nâ„’\nâ€‹\n(\nğœ½\n;\nğ±\ntest\n)\nâŠ¤\nâ€‹\nğ‡\nğœ½\nâˆ’\n1\nâ€‹\nâˆ‡\nğœ½\nâ„’\nâ€‹\n(\nğœ½\n;\nğ±\nn\n)\n,\nğ‡\nğœ½\n=\n1\nN\nâ€‹\nâˆ‘\nn\n=\n1\nN\nâˆ‡\nğœ½\n2\nâ„’\nâ€‹\n(\nğœ½\n;\nğ±\nn\n)\n,\nI(\\mathbf{x}_{n},\\mathbf{x}_{\\textnormal{test}})\\penalty 10000\\ =\\penalty 10000\\ -\\nabla_{\\bm{\\theta}}\\mathcal{L}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}})^{\\!\\top}\\,\\mathbf{H}_{\\bm{\\theta}}^{-1}\\,\\nabla_{\\bm{\\theta}}\\mathcal{L}(\\bm{\\theta};\\mathbf{x}_{n}),\\quad\\mathbf{H}_{\\bm{\\theta}}=\\tfrac{1}{N}\\sum\\nolimits_{n=1}^{N}\\nabla_{\\bm{\\theta}}^{2}\\mathcal{L}(\\bm{\\theta};\\mathbf{x}_{n}),\n(4)\nwhere the inverse Hessian captures the curvature of the loss landscape, yet computing or storing it is infeasible at modern model and dataset scales. Thus, practical methods (e.g., TracIn\n(\npruthi2020estimating\n)\nand TRAK\n(\npark2023trak\n)\n) approximate influence via gradient inner products or gradient feature projections.\nAttribution in diffusion models.\nDiffusion training aggregates gradients over timesteps\nt\nt\nand noise draws\nÏµ\n\\bm{\\epsilon}\n, where gradient norms vary systematically with\nt\nt\n, producing a timestep bias where examples aligned with large-norm timesteps appear spuriously influential. Diffusion-ReTrac\n(\nxie2024data\n)\nreduces this bias by normalizing gradients and sub-sampling\nt\nt\nand\nÏµ\n\\bm{\\epsilon}\nfor influence. Let\nâ„’\ndiff\n\\mathcal{L}_{\\textnormal{diff}}\ndenote the diffusion loss, and with the sampled-timestep-and-noise set\nğ’¯\n\\mathcal{T}\n, we compute a cosine-style score for normalized test and train gradients:\nI\ndiff\nâ€‹\n(\nğ±\nn\n,\nğ±\ntest\n)\n=\n1\n|\nğ’¯\ntest\n|\nâ€‹\nâˆ‘\nt\n,\nÏµ\nâˆˆ\nğ’¯\ntest\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ±\ntest\n,\nt\n,\nÏµ\n)\nâ€–\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ±\ntest\n,\nt\n,\nÏµ\n)\nâ€–\nâŠ¤\nâ€‹\n1\n|\nğ’¯\nn\n|\nâ€‹\nâˆ‘\nt\n,\nÏµ\nâˆˆ\nğ’¯\nn\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ±\nn\n,\nt\n,\nÏµ\n)\nâ€–\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ±\nn\n,\nt\n,\nÏµ\n)\nâ€–\n.\nI_{\\textnormal{diff}}(\\mathbf{x}_{n},\\mathbf{x}_{\\textnormal{test}})\\!=\\!\\mathchoice{\\hbox to575.83pt{\\vbox to26pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 287.91437pt\\lower-10.00012pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-287.91437pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to575.83pt{\\vbox to26pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 287.91437pt\\lower-10.00012pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-287.91437pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to353.57pt{\\vbox to20pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 176.78328pt\\lower-7.00009pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-176.78328pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to275.18pt{\\vbox to17pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 137.59033pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-137.59033pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}\\,\\,\\,\\,\\mathchoice{\\hbox to472.99pt{\\vbox to26pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 236.49352pt\\lower-10.00012pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-236.49352pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to472.99pt{\\vbox to26pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 236.49352pt\\lower-10.00012pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-236.49352pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to299.63pt{\\vbox to20pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 149.81506pt\\lower-7.00009pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-149.81506pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to237.99pt{\\vbox to17pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 118.99416pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-118.99416pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}.\n(5)\nnormalized test gradients\nnormalized training gradients\naaaaaaaaa\nAveraging gradients over\n(\nt\n,\nÏµ\n)\n(t,\\bm{\\epsilon})\nstabilizes estimates, and normalization mitigates timestep-induced scale effects. Attribution quality is also sensitive to the measurement function used to score examples, such as denoising loss versus likelihood proxies\n(\nzheng2023intriguing\n)\n.\nWhy vanilla attribution is insufficient for videos.\nNaÃ¯vely applying gradient-based attribution to video diffusion risks treating appearance and motion alike, often overemphasizing low-level appearance matches (objects, textures, backgrounds) while overlooking dynamics\n(\npark2025concept\n;\ntulyakov2018mocogan\n)\n. Its cost grows with clip length, sampled timesteps, noise draws, and gradient dimensionality, making naÃ¯ve methods impractical at modern video scales. Because we aim to explain and improve motion, we need attribution that suppresses static appearance, emphasizes motion-specific signals, and remains efficient, motivating the motion-centric approach in Â§\n3\n. Motion is distributed across frames and temporal horizons and entangled with static cues, so influence cannot be assigned by considering frames independently.\n3\nMethod\nWe formalize the problem setup in Â§\n3.1\nand develop a practical framework for motion attribution in video diffusion models with four key components: scalable gradient computation (Â§\n3.2\n), frame-length bias fix (Â§\n3.3\n), motion-aware weighting (Â§\n3.4\n), and data selection for targeted fine-tuning (Â§\n3.5\n). We also provide a computational efficiency analysis (Â§\n3.6\n) demonstrating the scalability of our approach to billion-parameter models and large-scale video datasets.\n3.1\nProblem Formulation\nWe study data attribution for motion in the fine-tuning setting. Let\nğ’Ÿ\nft\n=\n{\n(\nğ¯\nn\n,\nğœ\nn\n)\n}\nn\n=\n1\nN\n\\mathcal{D}_{\\textnormal{ft}}=\\{(\\mathbf{v}_{n},\\mathbf{c}_{n})\\}_{n=1}^{N}\nbe the fine-tuning corpus. Given a query video\n(\nğ¯\n^\n,\nğœ\n^\n)\n(\\hat{\\mathbf{v}},\\hat{\\mathbf{c}})\n, we assign to each training clip\n(\nğ¯\nn\n,\nğœ\nn\n)\n(\\mathbf{v}_{n},\\mathbf{c}_{n})\na motion-aware influence score\nI\nâ€‹\n(\nğ¯\nn\n,\nğ¯\n^\n;\nğœ½\n)\nI(\\mathbf{v}_{n},\\hat{\\mathbf{v}};\\bm{\\theta})\nthat explains how it contributes to the dynamics observed in\nğ¯\n^\n\\hat{\\mathbf{v}}\n. The score should satisfy:\n(i)\npredictivity\n, rankings correlate with observed changes from fine-tuning on the most influential subsets;\n(ii)\nefficiency\n, scales to modern video generators, such as forgoing explicit Hessian inversion, expensive per-data integration, or prohibitive storage.\nTo do this, we augment the influence target defined in  Eq.\n5\nto be (a) lower variance for stable rankings with feasible levels of compute, (b) more scalable to store, and (c) motion-centric.\nFine-tuning Subset Selection.\nFor a budget\nK\nâ‰ª\nN\nK\\ll N\n, we get a motion-influential subset by ranking scores and taking the top-\nK\nK\nexamples. When aggregating across multiple query motions, we combine selections as described in Â§\n3\n. The resulting subsets serve as candidates for motion-centric fine-tuning.\n3.2\nScalable Gradient-based Attribution for Generative Models\nWe make attribution practical for modern, large, high-quality video datasets and models via inverse-Hessian approximations, lower-variance gradient-similarity estimators, low-cost single-sample estimators, and a Fastfood projection for tractable storage.\nApproximating the inverse-Hessian.\nComputing exact inverse-Hessian-vector products is infeasible for modern neural networks.\nWe estimate influence via gradient similarity, using an identity preconditioner for the inverse Hessian\n(\nkoh2017understanding\n;\npruthi2020estimating\n;\npark2023trak\n)\n.\nCommon randomness for stable rankings.\nTo reduce variance without changing the target, we evaluate train and test gradients under the same\n(\nt\n,\nÏµ\n)\n(t,\\bm{\\epsilon})\npairs and average over a small set\nğ’¯\n\\mathcal{T}\n(\nxie2024data\n;\nlin2024diffusion\n)\n. This paired averaging stabilizes rankings compared to independent draws:\nI\ndiff\n1\nâ€‹\n(\nğ±\nn\n,\nğ±\ntest\n)\n=\n1\n|\nğ’¯\n|\nâ€‹\nâˆ‘\nt\n,\nÏµ\nâˆˆ\nğ’¯\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ±\ntest\n,\nt\n,\nÏµ\n)\nâ€–\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ±\ntest\n,\nt\n,\nÏµ\n)\nâ€–\nâŠ¤\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ±\nn\n,\nt\n,\nÏµ\n)\nâ€–\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ±\nn\n,\nt\n,\nÏµ\n)\nâ€–\n.\nI_{\\textnormal{diff}}^{1}(\\mathbf{x}_{n},\\mathbf{x}_{\\textnormal{test}})=\\frac{1}{|\\mathcal{T}|}\\sum_{t,\\bm{\\epsilon}\\in\\mathcal{T}}\\mathchoice{\\hbox to341.66pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 170.82907pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-170.82907pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to341.66pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 170.82907pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-170.82907pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to223.18pt{\\vbox to19pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 111.5877pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-111.5877pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to182.23pt{\\vbox to17pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 91.11737pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-91.11737pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}\\hskip 10.34958pt\\mathchoice{\\hbox to294.53pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 147.2633pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-147.2633pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to294.53pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 147.2633pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-147.2633pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to199.12pt{\\vbox to19pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 99.56108pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-99.56108pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to166.39pt{\\vbox to17pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 83.19362pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-83.19362pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}.\n(6)\nnormalized test gradients\nnormalized training gradients\naaaaaaaaa\nSingle-sample variant for reduced compute.\nWe fix a single\nt\nfix\nt_{\\textnormal{fix}}\nand a shared draw\nÏµ\nfix\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nğˆ\n)\n\\bm{\\epsilon}_{\\textnormal{fix}}\\sim\\mathcal{N}(0,\\mathbf{I})\nfor all trainâ€“test pairs at the final checkpoint. Sharing\n(\nt\nfix\n,\nÏµ\nfix\n)\n(t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\nallows low enough variance for the low-cost single-sample estimator to maintain relative ordering\n(\nxie2024data\n;\nlin2024diffusion\n)\n. The estimator collapses to:\nI\ndiff\n2\nâ€‹\n(\nğ±\nn\n,\nğ±\ntest\n)\n=\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ±\ntest\n,\nt\nfix\n,\nÏµ\nfix\n)\nâ€–\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ±\ntest\n,\nt\nfix\n,\nÏµ\nfix\n)\nâ€–\nâŠ¤\nâ€‹\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ±\nn\n,\nt\nfix\n,\nÏµ\nfix\n)\nâ€–\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ±\nn\n,\nt\nfix\n,\nÏµ\nfix\n)\nâ€–\n.\nI_{\\textnormal{diff}}^{2}(\\mathbf{x}_{n},\\mathbf{x}_{\\textnormal{test}})=\\mathchoice{\\hbox to410.21pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 205.10716pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-205.10716pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to410.21pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 205.10716pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-205.10716pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to261.4pt{\\vbox to19pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 130.69882pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-130.69882pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to210.68pt{\\vbox to17pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 105.33957pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-105.33957pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}\\;\\;\\mathchoice{\\hbox to363.08pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 181.5414pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-181.5414pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to363.08pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 181.5414pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-181.5414pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to237.34pt{\\vbox to19pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 118.6722pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-118.6722pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to194.83pt{\\vbox to17pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 97.41582pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-97.41582pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}.\n(7)\nnormalized test gradient\nnormalized training gradient\naaaaaaaaa\nStructured projection for reduced storage.\nTo operate at model scale, we apply a Johnsonâ€“Lindenstrauss projection via Fastfood\n(\nle2014fastfood\n)\nand then normalize.\nLet\nğ\nâˆˆ\nâ„\nD\nâ€²\nÃ—\nD\nbe implemented as\nğ\n:=\n1\nÎ¾\nâ€‹\nD\nâ€²\nâ€‹\nğ’ğğ†\nâ€‹\nğš·\nâ€‹\nğğ\n,\n\\mathbf{P}\\in\\mathbb{R}^{D^{\\prime}\\times D}\\quad\\text{be implemented as}\\quad\\mathbf{P}:=\\frac{1}{\\xi\\sqrt{D^{\\prime}}}\\;\\mathbf{S}\\mathbf{Q}\\mathbf{G}\\bm{\\Pi}\\mathbf{Q}\\mathbf{B},\\vskip-11.0pt\n(8)\nwhere\nğ\n\\mathbf{Q}\nis the Walshâ€“Hadamard matrix,\nğ\n\\mathbf{B}\nis a diagonal Rademacher matrix,\nğš·\n\\bm{\\Pi}\nis a random permutation,\nğ†\n\\mathbf{G}\nis a diagonal Gaussian scaling, and\nğ’\n\\mathbf{S}\nis a diagonal rescaling, and\nÎ¾\n\\xi\nnormalizes the variance. The projected, normalized gradient is:\nğ \n~\nâ€‹\n(\nğœ½\n,\nğ±\n)\n:=\nğ\nâ€‹\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n,\nğ±\n,\nt\nfix\n,\nÏµ\nfix\n)\nâ€–\nğ\nâ€‹\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n,\nğ±\n,\nt\nfix\n,\nÏµ\nfix\n)\nâ€–\n.\n\\tilde{\\mathbf{g}}\\big(\\bm{\\theta},\\mathbf{x}\\big)\\penalty 10000\\ :=\\penalty 10000\\ \\frac{\\mathbf{P}\\,\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta},\\mathbf{x},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\|\\mathbf{P}\\,\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta},\\mathbf{x},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\|}.\n(9)\nThen the influence score is the compact cosine in\nâ„\nD\nâ€²\n\\mathbb{R}^{D^{\\prime}}\n:\nI\ndiff\n3\nâ€‹\n(\nğ±\nn\n,\nğ±\ntest\n)\n=\nğ \n~\nâ€‹\n(\nğœ½\n;\nğ±\ntest\n)\nâŠ¤\nâ€‹\nğ \n~\nâ€‹\n(\nğœ½\n;\nğ±\nn\n)\n.\nI_{\\textnormal{diff}}^{3}(\\mathbf{x}_{n},\\mathbf{x}_{\\textnormal{test}})\\penalty 10000\\ =\\penalty 10000\\ \\mathchoice{\\hbox to175.15pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 87.57513pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-87.57513pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to175.15pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 87.57513pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-87.57513pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to129.42pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 64.70833pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-64.70833pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to111.75pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 55.87692pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-55.87692pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}\\;\\;\\mathchoice{\\hbox to126.8pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 63.39989pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-63.39989pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to126.8pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 63.39989pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-63.39989pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to103.94pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 51.97021pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-51.97021pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to94.89pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 47.44493pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-47.44493pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}.\n(10)\nprojected, normalized test gradient\nprojected, normalized training gradient\nThis keeps compute\nğ’ª\nâ€‹\n(\nD\nâ€²\nâ€‹\nlog\nâ¡\nD\nâ€²\n)\n\\mathcal{O}(D^{\\prime}\\log D^{\\prime})\nfor projection and\nğ’ª\nâ€‹\n(\nD\nâ€²\n)\n\\mathcal{O}(D^{\\prime})\nper dot product, with storage\nğ’ª\nâ€‹\n(\n|\nğ’Ÿ\n|\nâ€‹\nD\nâ€²\n)\n\\mathcal{O}(|\\mathcal{D}|\\,D^{\\prime})\n, while staying close to the ranking behavior of full-gradient cosine similarity\n(\npark2023trak\n)\n.\nFigure 1\n:\nMotive\n.\nTop.\nMotion-gradient computation (Â§\n3.4\n) has three steps: (1) detect motion with AllTracker; (2) compute motion-magnitude patches; (3) apply loss-space motion masks to focus gradients on dynamic regions.\nBottom.\nOur method (Â§\n3.2\n) is made scalable via a single-sample variant with common randomness and a projection, computed for each pair of training and query data, aggregated (Â§\n3.5\n) for a final ranking, and eventually used to select fine-tuning subsets.\n3.3\nVideo-specific Frame-length Bias Fix\nRaw gradient magnitudes depend on the number of frames\nF\nF\nin the video\nğ¯\n\\mathbf{v}\n, thereby biasing scores toward longer videos.\nWe correct this by normalizing for frame count before the projectionâ€“normalization step:\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ¯\n,\nt\nfix\n,\nÏµ\nfix\n)\nâ†\n1\nF\nâ€‹\nâˆ‡\nğœ½\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ¯\n,\nt\nfix\n,\nÏµ\nfix\n)\n.\n\\smash{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{v},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\;\\leftarrow\\;\\frac{1}{F}\\,\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{v},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}}).}\n(11)\nWe still apply\nâ„“\n2\n\\ell_{2}\nnormalization in Eq.\n10\n, further stabilizing scales across examples. Together, single-timestep, common randomness, projection, and frame-length correction form a compact, scalable estimator that we use throughout.\nHowever, naÃ¯ve video-level attribution conflates appearance with motion, often ranking clips high just because they share backgrounds or objects, while offering little insight into dynamics.\n3.4\nMotion Attribution\nTo move beyond whole-video influence, we introduce motion attribution, which isolates the contribution of training data to temporal dynamics. Unlike video-level attribution, which treats each clip as a single unit and conflates appearance with motion, motion attribution reweights per-location gradients using motion masks, assigning influence via dynamic behavior rather than static content.\nMotion Masking Attribution.\nMotion is what distinguishes video diffusion from image diffusion.\nOur goal is to understand how training data shapes motion in video diffusion models.\nPrior work has emphasized architectural or algorithmic changes for motion modeling\n(\npeebles2023scalable\n;\nblattmann2023stable\n;\nguo2023animatediff\n)\n, many of the largest generative gains have instead come from scaling and curating massive video corpora, which in turn enable impressive motion synthesis results in video diffusion models\n(\nho2022video\n;\nwan2025\n;\ntan2024vidgen\n;\nyang2024cogvideox\n)\n. Yet we lack tools that quantify how specific training clips shape particular motion patterns. We address this by attributing motion back to data via motion-weighted gradients, which yields actionable signals for targeted data selection, artifact diagnosis, and selective fine-tuning.\nMotion Detection and Latent Space Mapping.\nGiven a video\nğ¯\nâˆˆ\nâ„\nF\nÃ—\nH\nÃ—\nW\nÃ—\n3\n\\mathbf{v}\\in\\mathbb{R}^{F\\times H\\times W\\times 3}\nwith\nF\nF\nframes of resolution\nH\nÃ—\nW\nH\\times W\n, we first encode it into the VAE latent space as\nğ¡\n=\nE\nâ€‹\n(\nğ¯\n)\nâˆˆ\nâ„\nF\nÃ—\nH\n/\ns\nÃ—\nW\n/\ns\nÃ—\nC\n\\mathbf{h}=E(\\mathbf{v})\\in\\mathbb{R}^{F\\times\\nicefrac{{H}}{{s}}\\times\\nicefrac{{W}}{{s}}\\times C}\n, with downsampling factor\ns\n=\n8\ns=8\nand\nC\n=\n16\nC=16\nfollowing the\nWan2.1\nbackbone used in our experiments.\nFor motion computation, we use AllTracker\n(\nharley2025alltracker\n)\nto extract motion information in pixel space:\nA\n=\nğ’œ\nâ€‹\n(\nğ¯\n)\nâˆˆ\nâ„\nF\nÃ—\nH\nÃ—\nW\nÃ—\n4\nA=\\mathcal{A}(\\mathbf{v})\\in\\mathbb{R}^{F\\times H\\times W\\times 4}\n, where the first two channels contain optical flow maps\nA\n:\n,\n:\n,\n:\n,\n0\n:\n2\nA_{:,:,:,0:2}\nindicating pixel displacement between frames, and the remaining channels\nA\n:\n,\n:\n,\n:\n,\n2\n:\n4\nA_{:,:,:,2:4}\nencode visibility and confidence scores. We extract displacement vectors at each pixel location as:\nğƒ\nf\nâ€‹\n(\nh\n,\nw\n)\n=\n(\nA\nf\n,\nh\n,\nw\n,\n0\n,\nA\nf\n,\nh\n,\nw\n,\n1\n)\n=\n(\nd\nâ€‹\nw\n,\nd\nâ€‹\nh\n)\n.\n\\smash{\\mathbf{D}_{\\!f}(h,w)=(A_{f,h,w,0},A_{f,h,w,1})=(\\mathrm{d}w,\\mathrm{d}h).}\\vskip-5.49698pt\n(12)\nWe then bilinearly downsample motion quantities from\n(\nH\n,\nW\n)\n(H,W)\nto the latent grid\n(\nH\ns\n,\nW\ns\n)\n\\big(\\tfrac{H}{s},\\tfrac{W}{s}\\big)\nso that our masking lives where gradients are computed.\nMotion-Weighted Gradient Computation.\nWe define the motion magnitude at each location as:\nM\nf\nâ€‹\n(\nh\n,\nw\n)\n=\nâ€–\nğƒ\nf\nâ€‹\n(\nh\n,\nw\n)\nâ€–\n2\nM_{\\!f}(h,w)\\!=\\!\\|\\mathbf{D}_{\\!f}(h,w)\\|_{2}\n.\nTo obtain comparable motion weights across frames and pixels, we minâ€“max normalize over all frames and pixels, ensuring values lie in\n[\n0\n,\n1\n]\n[0,1]\n:\nğ–\nâ€‹\n(\nf\n,\nh\n,\nw\n)\n=\nM\nf\nâ€‹\n(\nh\n,\nw\n)\nâˆ’\nmin\nf\nâ€²\n,\nh\nâ€²\n,\nw\nâ€²\nâ¡\nM\nf\nâ€²\nâ€‹\n(\nh\nâ€²\n,\nw\nâ€²\n)\nmax\nf\nâ€²\n,\nh\nâ€²\n,\nw\nâ€²\nâ¡\nM\nf\nâ€²\nâ€‹\n(\nh\nâ€²\n,\nw\nâ€²\n)\nâˆ’\nmin\nf\nâ€²\n,\nh\nâ€²\n,\nw\nâ€²\nâ¡\nM\nf\nâ€²\nâ€‹\n(\nh\nâ€²\n,\nw\nâ€²\n)\n+\nÎ¶\n,\n\\mathbf{W}(f,h,w)\\penalty 10000\\ =\\penalty 10000\\ \\frac{M_{\\!f}(h,w)-\\min_{f^{\\prime},h^{\\prime},w^{\\prime}}M_{\\!f^{\\prime}}(h^{\\prime},w^{\\prime})}{\\max_{f^{\\prime},h^{\\prime},w^{\\prime}}M_{\\!f^{\\prime}}(h^{\\prime},w^{\\prime})-\\min_{f^{\\prime},h^{\\prime},w^{\\prime}}M_{\\!f^{\\prime}}(h^{\\prime},w^{\\prime})+\\zeta},\\vskip-2.75269pt\n(13)\nwhere\nÎ¶\n=\n10\nâˆ’\n6\n\\zeta=10^{-6}\nensures a positive denominator.\nThis normalization mitigates bias from absolute motion scale, yielding weights that emphasize relative motion saliency rather than raw magnitude, following prior practice in video saliency detection\n(\nfang2013video\n)\n.\nLet\n(\nh\n~\n,\nw\n~\n)\n(\\tilde{h},\\tilde{w})\nindex the latent grid. We obtain latent-aligned weights by bilinear downsampling:\nğ–\n~\nâ€‹\n(\nf\n,\nh\n~\n,\nw\n~\n)\n=\nBilinear\nâ¡\n(\nğ–\nâ€‹\n(\nâ‹…\n,\nâ‹…\n,\nâ‹…\n)\n,\nF\n,\nH\ns\n,\nW\ns\n)\n.\n\\smash{\\tilde{\\mathbf{W}}(f,\\tilde{h},\\tilde{w})\\penalty 10000\\ =\\penalty 10000\\ \\operatorname{Bilinear}\\!\\left(\\mathbf{W}(\\cdot,\\cdot,\\cdot),F,\\penalty 10000\\ \\tfrac{H}{s},\\penalty 10000\\ \\tfrac{W}{s}\\right).}\n(14)\nWe compute per-location squared error at fixed\n(\nt\nfix\n,\nÏµ\nfix\n)\n(t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\nat each frame\nf\nf\nand â€œlatent pixelâ€\n(\nh\n~\n,\nw\n~\n)\n(\\tilde{h},\\tilde{w})\n:\nâ„’\n~\nğœ½\n,\nğ¯\n,\nğœ\nâ€‹\n(\nf\n,\nh\n~\n,\nw\n~\n)\n=\n(\n[\nÏµ\nğœ½\nâ€‹\n(\nğ³\nâ€‹\n(\nğ¯\n,\nt\nfix\n,\nÏµ\nfix\n)\n,\nt\nfix\n,\nğœ\n)\n]\nf\n,\nh\n~\n,\nw\n~\nâˆ’\n[\nÏµ\ntarget\nâ€‹\n(\nt\nfix\n,\nÏµ\nfix\n)\n]\nf\n,\nh\n~\n,\nw\n~\n)\n2\n,\n\\smash{\\tilde{\\mathcal{L}}_{\\bm{\\theta},\\mathbf{v},\\mathbf{c}}(f,\\tilde{h},\\tilde{w})\\penalty 10000\\ =\\penalty 10000\\ \\left(\\,[\\bm{\\epsilon}_{\\bm{\\theta}}(\\mathbf{z}(\\mathbf{v},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}}),t_{\\textnormal{fix}},\\mathbf{c})]_{f,\\tilde{h},\\tilde{w}}-[\\bm{\\epsilon}_{\\textnormal{target}}(t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})]_{f,\\tilde{h},\\tilde{w}}\\,\\right)^{2},}\\vskip-2.75269pt\n(15)\nand define the motion-weighted loss by averaging over frames and latent spatial locations:\nâ„’\nmot\nâ€‹\n(\nğœ½\n;\nğ¯\n,\nğœ\n)\n=\n1\nF\nğ¯\nâ€‹\nmean\nf\n,\nh\n~\n,\nw\n~\nâ¡\n[\nğ–\n~\nğ¯\n,\nğœ\nâ€‹\n(\nf\n,\nh\n~\n,\nw\n~\n)\nâ‹…\nâ„’\n~\nğœ½\n,\nğ¯\n,\nğœ\nâ€‹\n(\nf\n,\nh\n~\n,\nw\n~\n)\n]\n.\n\\smash{\\mathcal{L}_{\\textnormal{mot}}(\\bm{\\theta};\\mathbf{v},\\mathbf{c})\\penalty 10000\\ =\\penalty 10000\\ \\frac{1}{F_{\\mathbf{v}}}\\operatorname{mean}_{f,\\tilde{h},\\tilde{w}}\\left[\\tilde{\\mathbf{W}}_{\\mathbf{v},\\mathbf{c}}(f,\\tilde{h},\\tilde{w})\\cdot\\tilde{\\mathcal{L}}_{\\bm{\\theta},\\mathbf{v},\\mathbf{c}}(f,\\tilde{h},\\tilde{w})\\right].}\\vskip-2.75269pt\n(16)\nNotably, when\nğ–\n~\n\\tilde{\\mathbf{W}}\nis all ones, this recovers the standard objective with no motion emphasis. The\n1\n/\nF\nğ¯\n\\nicefrac{{1}}{{F_{\\mathbf{v}}}}\nfactor corrects for frame-length bias and\nF\nğ¯\nF_{\\mathbf{v}}\nsignifies how the number of frames may be video-dependent. The corresponding motion-weighted gradient for attribution is:\nI\nmot\nâ€‹\n(\nğ¯\nn\n,\nğ¯\n^\n)\n=\nğ \n~\nmot\nâ€‹\n(\nğœ½\n,\nğ¯\n^\n)\nâŠ¤\nâ€‹\nğ \n~\nmot\nâ€‹\n(\nğœ½\n,\nğ¯\nn\n)\n,\nwhere\nâ€‹\nğ \n~\nmot\nâ€‹\n(\nğœ½\n,\nğ¯\n)\n:=\nğğ \nmot\nâ€‹\n(\nğœ½\n,\nğ¯\n,\nt\nfix\n,\nÏµ\nfix\n)\nâ€–\nğğ \nmot\nâ€‹\n(\nğœ½\n,\nğ¯\n,\nt\nfix\n,\nÏµ\nfix\n)\nâ€–\nâ€‹\nand\nâ€‹\nğ \nmot\n:=\nâˆ‡\nğœ½\nâ„’\nmot\n.\n\\!\\!\\!\\!I_{\\textnormal{mot}}\\!(\\mathbf{v}_{n},\\!\\hat{\\mathbf{v}})\\!=\\!\\tilde{\\mathbf{g}}_{\\textnormal{mot}}\\!(\\bm{\\theta}\\!,\\!\\hat{\\mathbf{v}})^{\\!\\top}\\!\\tilde{\\mathbf{g}}_{\\textnormal{mot}}\\!(\\bm{\\theta}\\!,\\!\\mathbf{v}_{n}),\\textnormal{ where }\\tilde{\\mathbf{g}}_{\\textnormal{mot}}\\!(\\bm{\\theta}\\!,\\!\\mathbf{v})\\!:=\\!\\frac{\\mathbf{P}\\mathbf{g}_{\\textnormal{mot}}\\!(\\bm{\\theta}\\!,\\!\\mathbf{v}\\!,\\!t_{\\textnormal{fix}},\\!\\bm{\\epsilon}_{\\textnormal{fix}})}{\\|\\mathbf{P}\\mathbf{g}_{\\textnormal{mot}}\\!(\\bm{\\theta}\\!,\\!\\mathbf{v}\\!,\\!t_{\\textnormal{fix}},\\!\\bm{\\epsilon}_{\\textnormal{fix}})\\|}\\textnormal{ and }\\mathbf{g}_{\\textnormal{mot}}\\!:=\\!\\nabla_{\\!\\bm{\\theta}}\\mathcal{L}_{\\textnormal{mot}}.\\vskip-2.75269pt\n(17)\nLoss-space masking leaves forward noising and generation unchanged and reweights only attribution, avoiding interactions between motion weighting and noise injection. In contrast, our motion-aware attribution emphasizes dynamic regions and de-emphasizes static backgrounds, so rankings identify training clips that most strongly shape the modelâ€™s motion rather than appearance.\n3.5\nMost Influential Fine-tuning Subset Selection\nGoal.\nGiven a query clip\n(\nğ¯\n^\n,\nğœ\n^\n)\n(\\hat{\\mathbf{v}},\\hat{\\mathbf{c}})\n, we compute a motion-aware attribution\nvalue for each candidate fine-tuning example\n(\nğ¯\nn\n,\nğœ\nn\n)\nâˆˆ\nğ’Ÿ\nft\n(\\mathbf{v}_{n},\\mathbf{c}_{n})\\in\\mathcal{D}_{\\textnormal{ft}}\nusing:\nI\nmot\nâ€‹\n(\nğ¯\nn\n,\nğ¯\n^\n)\nI_{\\textnormal{mot}}\\!\\left(\\mathbf{v}_{n},\\,\\hat{\\mathbf{v}}\\right)\nfrom  Eq.\n17\n. Then, we construct a fine-tuning dataset\nğ’®\n\\mathcal{S}\nfor one or many query videos\nğ¯\n^\n\\hat{\\mathbf{v}}\n.\nSingle-query-point fine-tuning selection.\nFor a budget of\nK\nK\ndata points, we select the\nK\nK\nhighest-scoring examples.\nIn practice,\nK\nK\nis chosen as a percentile of the dataset size (e.g., top 1â€“10%), ensuring the subset scales consistently across datasets.\nMulti-query-point fine-tuning selection: aggregating attribution scores.\nFor\nQ\nQ\nqueries, we adopt the majority voting approach from ICONS\n(\nwu2024icons\n)\nand aggregate motion-aware influence scores across queries by percentile thresholding and voting. A sample receives a vote if the score is above the percentile cutoff\nÏ„\n\\tau\nfor that query.\nThe consensus score of a candidate\nğ¯\nn\n\\mathbf{v}_{n}\nis the total number of queries that vote for it. We then rank all training samples by\nMajVote\nâ¡\n(\nğ¯\nn\n)\n\\operatorname{MajVote}(\\mathbf{v}_{n})\nand select the top-\nK\nK\nto form the fine-tuning subset. This formulation emphasizes samples that are consistently influential across multiple queries, without requiring cross-query calibration of raw scores:\nMajVote\nn\n=\nâˆ‘\nq\n=\n1\nQ\nğ•€\nâ€‹\n[\nI\nmot\nâ€‹\n(\nğ¯\nn\n,\nğ¯\n^\nq\n)\n>\nÏ„\n]\n,\nğ’®\nvote\nâ€‹\n(\nK\n)\n=\n{\nğ¯\nn\n|\nğ¯\nn\nâ€‹\nin top-\nâ€‹\nK\nâ€‹\nby\nâ€‹\nMajVote\n}\n.\n\\operatorname{MajVote}_{n}\\!=\\!\\sum\\nolimits_{q=1}^{Q}\\!\\mathbb{I}\\!\\big[\\,I_{\\textnormal{mot}}(\\mathbf{v}_{n},\\hat{\\mathbf{v}}_{q})\\!>\\!\\tau\\,\\big],\\mathcal{S}_{\\textnormal{vote}}(K)\\!=\\!\\bigl\\{\\mathbf{v}_{n}|\\mathbf{v}_{n}\\text{ in top-}K\\text{ by }\\operatorname{MajVote}\\bigr\\}.\n(18)\n3.6\nComputational Efficiency Analysis\nGradient Compute.\nNaÃ¯vely averaging over timesteps and noise for every example costs\nğ’ª\nâ€‹\n(\n|\nğ’Ÿ\n|\nâ€‹\n|\nğ’¯\n|\nâ€‹\nB\n)\n\\mathcal{O}(|\\mathcal{D}|\\,|\\mathcal{T}|\\,B)\n, where\nB\nB\nis a single forward+backward cost and\n|\nğ’¯\n|\n|\\mathcal{T}|\nis the number of sampled\nt\n,\nÏµ\nt,\\bm{\\epsilon}\nper data.\nUsing a single sample reduces this to\nğ’ª\nâ€‹\n(\n|\nğ’Ÿ\n|\nâ€‹\nB\n)\n\\mathcal{O}(|\\mathcal{D}|\\,B)\n, which is key to keeping the cost reasonable for modern video datasets and models, while reusing a sample across data allows low enough variance for stable rankings. Projection adds\nğ’ª\nâ€‹\n(\nD\nâ€²\nâ€‹\nlog\nâ¡\nD\nâ€²\n)\n\\mathcal{O}(D^{\\prime}\\log D^{\\prime})\nper example using Fastfood\n(\nle2014fastfood\n)\n, negligible relative to a backward pass.\nGradient Storage.\nStoring full gradients is\nğ’ª\nâ€‹\n(\n|\nğ’Ÿ\n|\nâ€‹\nD\n)\n\\mathcal{O}(|\\mathcal{D}|\\,D)\n. We instead store only projected vectors,\nğ’ª\nâ€‹\n(\n|\nğ’Ÿ\n|\nâ€‹\nD\nâ€²\n)\n\\mathcal{O}(|\\mathcal{D}|\\,D^{\\prime})\n, plus the structured Fastfood state,\nğ’ª\nâ€‹\n(\nD\n)\n\\mathcal{O}(D)\n. Since\nD\nâ€²\nD^{\\prime}\nis typically orders of magnitude smaller than\nD\nD\n, this transformation makes storage tractable for billion-parameter models.\nData Ranking Compute.\nInfluence computation in  Eq.\n10\nis an inner product in\nâ„\nD\nâ€²\n\\mathbb{R}^{D^{\\prime}}\n, so evaluating all train examples against a query is\nğ’ª\nâ€‹\n(\n|\nğ’Ÿ\n|\nâ€‹\nD\nâ€²\n)\n\\mathcal{O}(|\\mathcal{D}|\\,D^{\\prime})\n, and sorting is\nğ’ª\nâ€‹\n(\n|\nğ’Ÿ\n|\nâ€‹\nlog\nâ¡\n|\nğ’Ÿ\n|\n)\n\\mathcal{O}(|\\mathcal{D}|\\log|\\mathcal{D}|)\n.\nAdditional Motion-Emphasis Compute.\nMotion-specific overhead primarily stems from AllTracker mask extraction with complexity\nğ’ª\nâ€‹\n(\n|\nğ’Ÿ\n|\nâ‹…\nH\nâ‹…\nW\nâ‹…\nF\n)\n\\mathcal{O}(|\\mathcal{D}|\\cdot H\\cdot W\\cdot F)\nfor clip length\nF\nF\nand frame resolution\nH\nÃ—\nW\nH\\times W\n. Masks are extracted once, cached, and negligible relative to gradient cost. We provide a detailed runtime breakdown in App.\nG.1\n.\n4\nExperiment\n4.1\nSetup\nFigure 2\n:\nMotion attribution examples.\nTop\n: Query clips showing float (\nleft\n) and roll (\nright\n) motions.\nMiddle\n:\nTop-ranked positive training samples identified by\nMotive\nwith high influence scores.\nBottom\n: Negative influence samples with minimal, camera-only motion, or cartoon-style content that conflict with target motions.\nFine-tuning Datasets.\nWe evaluate our motion attribution framework on two large-scale video datasets:\nVIDGEN-1M\n(\ntan2024vidgen\n)\nand\n4DNeX-10M\n(\nchen20254dnex\n)\n, both of which offer diverse motion patterns, rich temporal dynamics, and complex scenes. For our experiments, we use 10k videos from both datasets, which provide sufficient scale and diversity to thoroughly evaluate motion attribution methods across different temporal patterns and video generation scenarios.\nMotion Query Data.\nTo evaluate our motion attribution, we curate a set of query videos representing distinct motion patterns and scenarios. Our query dataset consists of videos spanning\n10\n10\nmotion categories, with a focus on object dynamics: compress, bounce, roll, explode, float, free fall, slide, spin, stretch, swing.\nFive videos, totaling\n50\n50\nqueries, represent each motion type. These videos are chosen for their clear, isolated motions, serving as a basis for evaluating attribution quality and downstream motion generation. Further details on query video curation are provided in App.\nF.2\n.\nModel & Baselines.\nAll experiments primarily use pretrained\nWan2.1-T2V-1.3B\n, a widely used open-source baseline with strong performance and feasible compute. We provide additional results on\nWan2.2-TI2V-5B\nin App.\nC\n. Our baselines:\nBase model\n(pretrained, no fine-tuning);\nFull fine-tuning\n(approximate upper bound using the complete dataset);\nRandom selection\n(uniform sampling);\nMotion magnitude\n(selects videos with the highest average motion magnitude);\nV-JEPA embeddings\n(selects most representative videos of motion patterns using self-supervised spatiotemporal V-JEPA\n(\nassran2025v\n)\nfeatures, capturing high-level motion semantics);\nand\nOurs w/o motion masking\n(influence of the entire video level without motion-specific masking).\nBenchmark.\nWe evaluate our attribution using VBench\n(\nhuang2024vbench\n)\nmetrics across six dimensions: subject consistency, background consistency, motion smoothness, dynamic degree, aesthetic quality, and imaging quality. Motion smoothness and dynamic degree are our primary targets for temporal dynamics, while other metrics ensure visual quality is maintained. We use custom evaluation prompts, following VBenchâ€™s descriptive style, designed for ten motion types (bounce, compress, explode, float, free fall, roll, slide, spin, stretch, swing), to assess our frameworkâ€™s effectiveness for specific physical motions, with five prompts per motion type.\nImplementation Details.\nWe finetune\nWan2.1-T2V-1.3B\nwith our\nMotive\n-selected high-quality video data following the official and DiffSynth-Studio implementation.\nDuring fine-tuning, we update only the DiT backbone while freezing the T5 text encoder and VAE. All models are trained at a resolution of\n480\nÃ—\n832\n480\\times 832\nwith a learning rate of\n1\nÃ—\n10\nâˆ’\n5\n1\\times 10^{-5}\n.\nSpecialist models are trained on single motion category selected data, while generalist models use aggregated selections (both with top\n10\n%\n10\\%\nselection from\nVIDGEN-1M\n(\ntan2024vidgen\n)\nor\n4DNeX\n(\nchen20254dnex\n)\nwith motion-weighted loss attribution). All training runs are conducted on\n4\n4\n-\n8\n8\nNVIDIA A100 GPUs.\nWith one A100 GPU, it takes approximately\n150\n150\nhours to compute the influence score of 10k samples. This process is highly parallelizable, on\n64\n64\nGPUs, taking\nâˆ¼\n2.3\n\\sim 2.3\nhours. The computed gradients can be reused for multiple selection queries, amortizing this one-time cost.\n4.2\nMain Results\nFigure 3\n:\nQualitative Comparisons.\nWe compare four motion scenarios (compress, spin, slide, free fall) across the base model, random selection, and our method. Our approach yields more realistic motion dynamics. Supplementary videos are included.\nMethod\nSubject Consist.\nBackground Consist.\nMotion Smooth.\nDynamic Degree\nAesthetic Quality\nImaging Quality\nBase\n95.3\n96.4\n96.3\n39.6\n45.3\n65.7\nFull fine-tuning\n95.9\n96.6\n96.3\n42.0\n45.0\n63.9\nRandom selection\n95.3\n96.6\n96.3\n41.3\n45.7\n65.1\nMotion magnitude\n95.6\n96.2\n95.7\n40.1\n45.1\n63.2\nV-JEPA embedding\n95.7\n96.0\n95.6\n41.6\n44.9\n62.7\nOurs w/o MM\n95.4\n96.1\n96.3\n43.8\n45.7\n63.2\nOurs (\nMotive\n)\n96.3\n96.1\n96.3\n47.6\n46.0\n64.6\nTable 1\n:\nVBench Evaluation.\nPerformance comparison on VBench\n(\nhuang2024vbench\n)\nacross different baselines (all values in\n%\n\\%\n, higher is better). All selection methods use\n10\n%\n10\\%\nof training data; our method uses majority vote aggregation (Â§\n3.5\n) across motion queries. MM: motion masking.\nHigh-influence selection and negative filtering.\nFig.\n2\nshows our motion-aware attribution ranks clips with clear, physically grounded dynamics and downranks those with little transferable motion. For rolling and floating, positives show continuous trajectories and smooth temporal evolution (turbulent water carrying objects; planetary rotation). Negatives are mostly static footage, camera-only motion, or cartoons whose simplified kinematics do not transfer. Our method promotes useful motions and filters data that dilutes temporal learning during fine-tuning, holding across categories and aligning with the following quantitative gains.\nQualitative improvements across motion types.\nFig.\n3\ncompares the base pretrained model, naÃ¯ve motion fine-tuning, and our motion-aware data selection for fine-tuning across four scenarios. Top: rubber-ball compression and coin spinning. Bottom: coffee mug sliding and red ball drop. Our method yields higher motion fidelity and temporal consistency than both baselines, especially for complex deformation, rotational dynamics, and physics-driven motion.\nQuantitative Results.\nWe evaluate our approach across different metrics using VBench\n(\nhuang2024vbench\n)\n, demonstrating consistent improvements in motion fidelity when fine-tuning with attribution-selected data compared to random sampling or naÃ¯ve approaches.\nAs shown in Tab.\n1\n,\nMotive\nachieves the highest dynamic degree score (\n47.6\n%\n47.6\\%\n), significantly outperforming random selection (\n41.3\n%\n41.3\\%\n) and whole video attribution (\n43.8\n%\n43.8\\%\n). Our method also excels in subject consistency (\n96.3\n%\n96.3\\%\n) and aesthetic quality (\n46.0\n%\n46.0\\%\n), while maintaining competitive motion smoothness (\n96.3\n%\n96.3\\%\n). Notably, using only\n10\n%\n10\\%\nof the training data, our approach surpasses the full fine-tuned model on dynamic degree (\n42.0\n%\n42.0\\%\n) and subject consistency (\n95.9\n%\n95.9\\%\n), demonstrating the superior empirical performance of motion-specific attribution for targeted fine-tuning. We further analyze the motion magnitude distribution of selected videos in App.\nD\n.\n4.3\nHuman Evaluation\nMethod\nWin (\n%\n\\%\n)\nTie (\n%\n\\%\n)\nLoss (\n%\n\\%\n)\nOurs vs. Base\n74.1\n74.1\n12.3\n12.3\n13.6\n13.6\nOurs vs. Random\n58.9\n58.9\n12.1\n12.1\n29.0\n29.0\nOurs vs. Full FT\n53.1\n53.1\n14.8\n14.8\n32.1\n32.1\nOurs vs. w/o MM\n46.9\n46.9\n20.0\n20.0\n33.1\n33.1\nTable 2:\nHuman evaluation.\nPairwise comparisons across\n50\n50\nvideos with\n17\n17\nparticipants (\n850\n850\ntotal). Win, tie, and loss rates show where our method is preferred, rated equal, or outperformed.\nAutomated scores can miss perceptual motion quality, so we run a human evaluation pairwise comparison protocol: participants view two generated videos and choose which shows better motion. We recruit\n17\n17\nannotators and evaluate\n10\n10\nmotion categories. For each category, we prepare\n5\n5\ntest cases and compare our method to baselines across three pairings, yielding a balanced set of judgments. Presentation order is randomized, and ties are allowed. We report win rate (fraction our method is preferred), tie rate, and overall preference. As shown in the table, annotators favor our attribution-guided selection:\n74.1\n%\n74.1\\%\nwin rate vs. the base model and\n53.1\n%\n53.1\\%\nvs. the full fine-tuned model, showing perceptually meaningful motion improvements.\n4.4\nAblations\nSingle-timestep attribution.\nUsing a single timestep avoids the cost of averaging across timesteps while closely matching the multi-timestep baseline. With a fixed\nt\n=\n751\nt\\!=\\!751\n(the midpoint of the\n1000\n1000\n-step denoising trajectory), we obtain\nÏ\n=\n66\n%\n\\rho\\!=\\!66\\%\nagreement with the ground truth computed using\n10\n10\nevenly-spaced timesteps sampled according to the flow matching schedule.\nUsing the same timestep for both training and testing is key to preserving relative rankings. High timesteps (early denoising) heavily corrupt inputs with noise, obscuring motion cues; low timesteps (late denoising) operate on nearly formed videos, where gradients reflect fine details rather than semantic structure.\nt\n=\n751\nt\\!=\\!751\n(mid-denoising) strikes a balance, delivering high correlation and substantial compute savings. Averaging multiple timesteps yields minimal ranking gains, and incorporating late-timestep gradients can bias rankings. A single fixed timestep is therefore sufficient for variance-reduced, scalable attribution.\nSpearman Correlation\nÏ\n\\rho\nProjection Dimension\nFigure 4\n:\nProjection dimension analysis.\nSpearman correlation between projected and full gradients shows rapid improvement with projection dimension, with\n512\n512\nproviding a strong trade-off between accuracy and efficiency.\nProjected Gradients Preserve Influence Rankings.\nComparing full gradients for attribution is infeasible at a billion-parameter scale. We reduce dimensionality with structured random projections that preserve influence geometry, ablating\nD\nâ€²\nâˆˆ\n{\n128\n,\nâ€¦\n,\n2048\n}\nD^{\\prime}\\!\\in\\!\\{128,\\dots,2048\\}\nagainst the full-gradient baseline. We assess ranking preservation via Spearman correlation with unprojected scores (Fig.\n4\n). Small projections preserve rankings poorly:\nD\nâ€²\n=\n128\nD^{\\prime}\\!=\\!128\nyields\nÏ\n=\n46.9\n%\n\\rho\\!=\\!46.9\\%\n. Preservation improves with size:\nD\nâ€²\n=\n512\nD^{\\prime}\\!=\\!512\nreaches\nÏ\n=\n74.7\n%\n\\rho\\!=\\!74.7\\%\n. Beyond that, gains are marginal while cost rises:\nD\nâ€²\n=\n1024\nD^{\\prime}\\!=\\!1024\n(\nÏ\n=\n75.7\n%\n\\rho\\!=\\!75.7\\%\n) and\nD\nâ€²\n=\n2048\nD^{\\prime}\\!=\\!2048\n(\nÏ\n=\n76.1\n%\n\\rho\\!=\\!76.1\\%\n). Thus,\nD\nâ€²\n=\n512\nD^{\\prime}\\!=\\!512\noffers the best trade-off, scaling to large models while maintaining quality.\nFrame-Length Normalization.\nAs in the Wan training protocol, we standardize all videos to 81 frames at 16 fps (satisfying the\n4\nâ€‹\nn\n+\n1\n4n{+}1\nconstraint) for fair attribution across clips of different raw lengths. Without standardization, gradient-based scores correlate strongly with video length rather than motion quality (\nÏ\n=\n78.0\n%\n\\rho\\!=\\!78.0\\%\n), leading to longer clips ranking higher regardless of dynamics. Standardizing frames reduces spurious length correlations by\n54.0\n%\n54.0\\%\nwhile preserving motion-based correlation, so rankings reflect motion rather than duration.\nAs in Fig.\n5\n, normalization clarifies motion-specific patterns. For floating queries with frame-length normalization (left), top-ranked samples consistently show wave dynamics, floating objects, and surfing, all matching the target motion. Without normalization (right), top samples lack coherent similarity because rankings are driven by clip length, harming motion-relevant training example identification.\nFigure 5\n:\nImpact of Frame-Length Normalization on Motion Attribution.\nComparison of top-ranked samples for floating motion query.\nLeft\n: With proper frame-length normalization, top samples consistently exhibit floating motion (waves, floating objects, surfing).\nRight\n: Without normalization, rankings are biased by video length, resulting in no coherent patterns among top samples.\n5\nConclusion\nWe address a central and underexplored question in video diffusion: which training clips influence the motion in generated videos? We propose\nMotive\n, which traces generated dynamics back to influential training clips by isolating motion-specific gradients. Unlike image-based attribution, our method directly targets temporal dynamics, revealing how coherence and physical plausibility emerge from data. We show motion learning is traceable to specific examples, providing a quantitative tool for diagnosing artifacts and enabling targeted data selection and curation, enabling more controllable and interpretable video diffusion models. As models scale, such data-level understanding will be essential for building robust and reliable generative systems.\nLimitations.\n(i) Motion saliency depends on the chosen tracker; severe occlusions or transparency can degrade masks. (ii) Camera-only motion and very subtle micro-motion remain challenging to separate without extra signals (e.g., camera pose). Further discussion is in App.\nG\n.\nAcknowledgements\nWe thank the following people (listed alphabetically by last name) for their helpful discussions, feedback, or participation in human studies: Allison Chen, Sanghyuk Chun, Amaya Dharmasiri, Xingyu Fu, Will Hwang, Yifeng Jiang, Amogh Joshi, Chen-Hsuan Lin, Huan Ling, Tiffany Ling, Shaowei Liu, Zhengyi Luo, Rafid Mahmood, Kaleb S. Newman, Julian Ost, Zeeshan Patel, Davis Rempe, Anya Tsvetkov, Esin Tureci, Sheng-Yu Wang, Tingwu Wang, Zian Wang, Hongyu Wen, Jon Williams, Donglai Xiang, Yilun Xu, William Yang, and Haotian Zhang.\nReferences\nAppendix A\nNotation\nTable 3\n:\nGlossary and notation.\nSymbol\nDescription\nAcronyms and Basic Notation\nVAE\nVariational Autoencoder\nDiT\nDiffusion Transformer backbone\nğˆ\n\\mathbf{I}\nIdentity matrix\nVideo Generation\np\nğœ½\nâ€‹\n(\nğ¯\nâˆ£\nğœ\n)\np_{\\bm{\\theta}}(\\mathbf{v}\\mid\\mathbf{c})\nConditional video generator with parameters\nğœ½\n\\bm{\\theta}\nğ¯\nâˆˆ\nâ„\nF\nÃ—\nH\nÃ—\nW\nÃ—\n3\n\\mathbf{v}\\in\\mathbb{R}^{F\\times H\\times W\\times 3}\nVideo clip with frames\nF\nF\n, height\nH\nH\n, width\nW\nW\nğœ\n\\mathbf{c}\nConditioning signal such as text or multimodal metadata\nğœ½\n\\bm{\\theta}\nTrainable model parameters\nf\nâˆˆ\n{\n1\n,\nâ€¦\n,\nF\n}\nf\\in\\{1,\\ldots,F\\}\nFrame index\nh\nâˆˆ\n{\n1\n,\nâ€¦\n,\nH\n}\nh\\in\\{1,\\ldots,H\\}\n,\nw\nâˆˆ\n{\n1\n,\nâ€¦\n,\nW\n}\nw\\in\\{1,\\ldots,W\\}\nSpatial indices for height and width respectively\nh\n~\n\\tilde{h}\n,\nw\n~\n\\tilde{w}\nLatent grid indices\nDatasets\nğ’Ÿ\n=\n{\n(\nğ¯\nn\n,\nğœ\nn\n)\n}\nn\n=\n1\nN\n\\mathcal{D}=\\{(\\mathbf{v}_{n},\\mathbf{c}_{n})\\}_{n=1}^{N}\nTraining corpus with size\nN\nN\nand index\nn\nn\nğ’Ÿ\nft\nâŠ†\nğ’Ÿ\n\\mathcal{D}_{\\textnormal{ft}}\\subseteq\\mathcal{D}\nFine-tuning dataset\nğ’®\nâŠ†\nğ’Ÿ\n\\mathcal{S}\\subseteq\\mathcal{D}\nSelected influential subset\nk\nâˆˆ\n{\n1\n,\nâ€¦\n,\nK\n}\nk\\in\\{1,\\ldots,K\\}\nThe selected subset size\nQ\nQ\nNumber of query data\nq\nâˆˆ\n{\n1\n,\nâ€¦\n,\nQ\n}\nq\\in\\{1,\\ldots,Q\\}\nQuery index\nğ±\n\\mathbf{x}\nGeneric input data pair\nğ±\ntest\n\\mathbf{x}_{\\textnormal{test}}\n,\nğ±\nn\n\\mathbf{x}_{n}\nTest/query pair and training pair\nğ¯\n^\n\\hat{\\mathbf{v}}\n,\nğœ\n^\n\\hat{\\mathbf{c}}\nQuery video and its conditioning\nLatent Space and Diffusion Components\nE\nE\nVAE encoder\nğ¡\n=\nE\nâ€‹\n(\nğ¯\n)\nâˆˆ\nâ„\nF\nÃ—\n(\nH\n/\ns\n)\nÃ—\n(\nW\n/\ns\n)\nÃ—\nC\n\\mathbf{h}=E(\\mathbf{v})\\in\\mathbb{R}^{F\\times(H/s)\\times(W/s)\\times C}\nLatent video with spatial factor\ns\ns\nand channels\nC\nC\nğ³\n\\mathbf{z}\nNoisy latent variable used in diffusion or flow matching\nÏµ\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nğˆ\n)\n\\bm{\\epsilon}\\sim\\mathcal{N}(0,\\mathbf{I})\nGaussian noise\nÏµ\nğœ½\nâ€‹\n(\nğ³\n,\nğœ\n,\nt\n)\n\\bm{\\epsilon}_{\\bm{\\theta}}(\\mathbf{z},\\mathbf{c},t)\nPredicted noise network in diffusion training\nğŸ\nğœ½\nâ€‹\n(\nğ³\n,\nğœ\n,\nt\n)\n\\mathbf{f}_{\\bm{\\theta}}(\\mathbf{z},\\mathbf{c},t)\nTime-indexed vector field in flow matching\nğ³\nË™\n\\dot{\\mathbf{z}}\nTime derivative of the latent trajectory\nÎ±\nt\n,\nÏƒ\nt\n\\alpha_{t},\\sigma_{t}\nScheduler signal and noise scales at timestep\nt\nt\nÏµ\ntarget\n\\bm{\\epsilon}_{\\textnormal{target}}\nTarget noise or velocity used for supervision\nt\nâˆˆ\n{\n1\n,\nâ€¦\n,\nT\n}\nt\\in\\{1,\\ldots,T\\}\nDiffusion or flow-matching timestep, with total timesteps\nT\nT\nt\nfix\nt_{\\textnormal{fix}}\n,\nÏµ\nfix\n\\bm{\\epsilon}_{\\textnormal{fix}}\nFixed timestep and shared noise draw used for low-variance gradients\nTable 4\n:\nGlossary and notation (continued).\nSymbol\nDescription\nAttribution and Influence\nI\nâ€‹\n(\nğ¯\nn\n,\nğ¯\n^\n;\nğœ½\n)\nI(\\mathbf{v}_{n},\\hat{\\mathbf{v}};\\bm{\\theta})\nInfluence score between a train clip and a query clip\nI\nmot\nâ€‹\n(\nğ¯\nn\n,\nğ¯\n^\n;\nğœ½\n)\nI_{\\textnormal{mot}}(\\mathbf{v}_{n},\\hat{\\mathbf{v}};\\bm{\\theta})\nMotion-aware influence score\nTopK\nâ¡\n(\nâ‹…\n)\n\\operatorname{TopK}(\\cdot)\nTop-\nK\nK\noperator for selecting highest scores\nMajVote\nâ¡\n(\nâ‹…\n)\n\\operatorname{MajVote}(\\cdot)\nMajority-vote aggregation across queries\nÏ„\n\\tau\nPercentile cutoff for voting\nÏ\n\\rho\nSpearman correlation coefficient\nLoss Functions\nâ„’\n\\mathcal{L}\nGeneric loss\nâ„’\ndiff\nâ€‹\n(\nğœ½\n;\nğ¯\n,\nğœ\n)\n,\nâ„’\nflow\nâ€‹\n(\nğœ½\n;\nğ¯\n,\nğœ\n)\n\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{v},\\mathbf{c}),\\mathcal{L}_{\\textnormal{flow}}(\\bm{\\theta};\\mathbf{v},\\mathbf{c})\nDiffusion and flow-matching objective\nâ„’\nmot\nâ€‹\n(\nğœ½\n;\nğ¯\n,\nğœ\n)\n\\mathcal{L}_{\\textnormal{mot}}(\\bm{\\theta};\\mathbf{v},\\mathbf{c})\nMotion-weighted objective used for attribution\nâ„’\n~\n\\tilde{\\mathcal{L}}\nPer-location squared error in latent space\nğ \n\\mathbf{g}\n,\nğ \n~\n\\tilde{\\mathbf{g}}\nGradient and its projected version\nğ \nmot\n\\mathbf{g}_{\\textnormal{mot}}\n,\nğ \n~\nmot\n\\tilde{\\mathbf{g}}_{\\textnormal{mot}}\nMotion-weighted gradient and its projection\nğ‡\nğœ½\n\\mathbf{H}_{\\bm{\\theta}}\nHessian with respect to\nğœ½\n\\bm{\\theta}\nMotion Representations\nğ’œ\nâ€‹\n(\nğ¯\n)\n=\nA\n\\mathcal{A}(\\mathbf{v})=A\nAllTracker motion extraction\nA\nâˆˆ\nâ„\nF\nÃ—\nH\nÃ—\nW\nÃ—\n4\nA\\in\\mathbb{R}^{F\\times H\\times W\\times 4}\nMotion tensor containing flow, visibility, and confidence\nğƒ\nf\nâ€‹\n(\nh\n,\nw\n)\n\\mathbf{D}_{\\!f}(h,w)\nDisplacement vector at frame\nf\nf\nand location\n(\nh\n,\nw\n)\n(h,w)\nM\nf\nâ€‹\n(\nh\n,\nw\n)\nM_{\\!f}(h,w)\nMotion magnitude at a location, computed from the displacement\nğ–\nâ€‹\n(\nf\n,\nh\n,\nw\n)\nâˆˆ\n[\n0\n,\n1\n]\n\\mathbf{W}(f,h,w)\\in[0,1]\nNormalized motion weights used to mask per-location losses\nÎ¶\n\\zeta\nSmall numerical bias for stability (e.g.,\n10\nâˆ’\n6\n10^{-6}\n)\nProjections and Computational Details\nD\nD\n,\nD\nâ€²\nD^{\\prime}\nFull and projected gradient dimensions\nğ\nâˆˆ\nâ„\nD\nâ€²\nÃ—\nD\n\\mathbf{P}\\in\\mathbb{R}^{D^{\\prime}\\times D}\nProjection matrix used for Fastfood-style JL projection\nÎ¾\n\\xi\nVariance normalization constant for projection\nğ’¯\n\\mathcal{T}\nSet of sampled\n(\nt\n,\nÏµ\n)\n(t,\\bm{\\epsilon})\npairs for gradient estimation\nB\nB\nUnit compute cost used in complexity accounting\nAppendix B\nExtended Related Work\nB.1\nData Attribution\nUnderstanding how individual training examples shape model behavior has been a long-standing goal.\nModern data attribution methods fall into two main groups\n[\nhammoudeh2024training\n]\n: retraining-based methods (e.g., leave-one-out\n[\ncook1982residuals\n,\njia2021scalability\n]\n, downsampling (also known as subsampling or counterfactual influence)\n[\nfeldman2020neural\n]\n, Shapley-value\n[\nwang2024data\n,\nwang2024rethinking\n]\n) and gradient-based methods (influence-function family, including Influence Functions\n[\nkoh2017understanding\n,\nlorraine2024jacnet\n]\n, TracIn\n[\npruthi2020estimating\n]\n, and TRAK\n[\npark2023trak\n]\n).\nInfluence functions provide a principled framework by approximating the effect of removing a training point.\nTracIn\n[\npruthi2020estimating\n]\nand TRAK\n[\npark2023trak\n]\nmake attribution feasible at scale. While effective for classification, these assume a mapping between training gradients and predictions, which becomes more complex in generative models.\nData attribution refers to methods that trace how individual training examples (or subsets) influence a modelâ€™s predictions or behavior. Formally, it assigns an attribution score to each training sample, estimating the extent to which that sample contributes (positively or negatively) to the modelâ€™s output on a given test query or behavior.\nInfluence data attribution is an example of nested optimization\n[\nvicol2022implicit\n,\nlorraine2024scalable\n]\nwith other examples including differentiable games\n[\nbalduzzi2018mechanics\n,\nlorraine2021lyapunov\n,\nlorraine2022complex\n]\nand hyperparameter optimization\n[\nraghu2021meta\n,\nlorraine2018stochastic\n,\nmehta2024improving\n,\nlorraine2020optimizing\n]\n.\nBefore diffusion models, attribution methods were applied to supervised learning tasks such as classification and regression, where influence functions\n[\nkoh2017understanding\n,\nlorraine2022task\n]\nand scalable approximations such as TracIn\n[\npruthi2020estimating\n]\n, TRAK\n[\npark2023trak\n]\n, and TDA\n[\nbae2024training\n]\nquantified the impact of training examples on downstream predictions.\nRecent work adapted data attribution to diffusion models\ngeorgiev2023journey\n,\nzheng2023intriguing\n,\nwang2025fast\n,\nwang2024data\n,\nlin2024diffusion\n,\nbrokman2024montrage\n,\nkwon2023datainf\n, where iterative denoising introduces timestep-dependent bias.\nmlodozeniec2024influence\npropose scalable approximations, while\nxie2024data\nidentify timestep-induced artifacts and normalization schemes. Concept-TRAK\n[\npark2025concept\n]\nextends attribution to concepts by reweighting gradients with concept-specific rewards, enabling attribution to semantic factors.\nwang2023evaluating\ninstead design a customization-based benchmark for text-to-image models, where models are fine-tuned on exemplar images with novel tokens and attribution is evaluated by whether it can recover the responsible exemplars.\nHowever, these are limited to image diffusion, which captures static appearance but not temporal dynamics.\nB.2\nMotion in Video Generation\nVideo diffusion extends image generation to time, requiring coherent motion across frames\n[\nho2022video\n,\nblattmann2023stable\n,\npeebles2023scalable\n,\nwan2025\n,\nagarwal2025cosmos\n]\n. A large body of work builds temporal structure via attention layers\n[\nwu2023tune\n]\n, control signals\n[\nchen2023control\n,\nzhang2023controllable\n]\n, feature correspondences\n[\ngeyer2023tokenflow\n,\nbao2023latentwarp\n,\nwang2024cove\n]\n, or consistency distillation\n[\nwang2023videolcm\n,\nzhou2024upscale\n]\n. Recent work has highlighted the challenge of decoupling motion from appearance in video diffusion transformers, where spatial and temporal information become entangled in the modelâ€™s representations\n[\nshi2025decouple\n]\n. However, understanding which training clips influence specific motion patterns in generated videos remains an open challenge.\nIn parallel, motion has long been studied using optical flow and correspondence, from classical formulations\n[\nhorn1981determining\n,\nlucas1981iterative\n]\nto modern approaches such as RAFT\n[\nteed2020raft\n]\n, which improve accuracy and generalization. These priors are often repurposed during generation to guide dynamics, but they do not explain which training examples shaped a modelâ€™s motion behavior. Our work addresses both gaps by introducing a motion-aware data attribution framework specifically designed for video diffusion. We use motion-weighted gradients that disentangle temporal dynamics from static appearance, enabling us to trace generated motion patterns back to the most influential training clips.\nAppendix C\nAdditional Experiments\nC.1\nResults on Different Video Generation Models\nWe further test our framework on additional video generation architectures beyond Wan2.1-T2V-1.3B. We have included the experiment results of\nMotive\non Wan2.2-TI2V-5B, which introduces a much larger parameter count (5B vs. 1.3B) and a new high-compression Wan2.2-VAE. The results in Tab.\n5\nshow that our approach works effectively across different model designs.\nModel\nMethod (\nâ†“\n\\downarrow\n) / Metric (\nâ†’\n\\rightarrow\n)\nSubject\nBackground\nMotion\nDynamic\nAesthetic\nImaging\nConsist.\nConsist.\nSmooth.\nDegree\nQuality\nQuality\nWan2.2-TI2V-5B\nBase\n94.9\n96.4\n97.5\n42.0\n44.4\n65.5\nFull fine-tuning\n95.3\n96.5\n97.5\n45.3\n44.8\n66.2\nRandom selection\n94.7\n96.2\n97.3\n41.6\n44.6\n65.2\nOurs w/o MM\n94.9\n96.5\n97.4\n43.8\n45.2\n64.8\nOurs (\nMotive\n)\n95.1\n96.6\n97.6\n48.3\n45.6\n65.5\nTable 5\n:\nVBench Evaluation on Additional Model.\nFollowing the same setting in Â§\n4\n, we extend the VBench\n[\nhuang2024vbench\n]\nevaluation to\nWan2.2-TI2V-5B\n, a larger-scale text-to-video model. Random selection and our\nMotive\nboth select\n10\n%\n10\\%\nof the training data, with our method using majority vote aggregation (Â§\n3.5\n) across all motion queries. Results demonstrate that\nMotive\ngeneralizes effectively to different models. MM: motion masking.\nAppendix D\nAnalysis\nNumber of Samples\nMean Motion Magnitude\n(a)\nMotion Distribution\n% of Selected Videos\nMotion Bin\n(b)\nDistribution Across Motion Bins\nFigure 6\n:\nMotive\nis not simply selecting â€œmotion-rich\" clips.\nOur influence scores are computed via gradients, and training videos are considered influential only when they directly improve the modelâ€™s ability to generate the target motion dynamics, not because they contain more motion overall.\nD.1\nMotion Distribution Analysis\nMotive\nis not simply selecting â€œmotion-rich\" clips:\nThe key distinction is that our influence scores are computed via gradients, and training videos are considered influential only when they directly allow the model to lower the loss, improving the modelâ€™s ability to generate the target motion dynamics, not because they contain more motion overall.\nTo empirically validate this, we further analyze the distribution of motion magnitudes in our selected data. We compute the mean motion magnitude for 10k videos in the\nVIDGEN\ndataset and compare the distributions of the top 10% (highest influence scores) and the bottom 10% (lowest influence scores).\nAs shown in Fig.\n6\n, the top 10% selected videos have a mean motion magnitude of 3.85, which is only 4.3% higher than the bottom 10% (3.69), despite representing opposite extremes of influence scores. The analysis also shows that within the moderate-motion range (bins 3, 4, and 5), the top 10% of positive-influence samples outnumber the bottom 10% of negative-influence samples. Yet, both groups also appear in low-motion bins (0-2) and high-motion bins (6-9).\nThis distribution pattern shows that high-influence videos selected by\nMotive\nspan the entire motion spectrum, not just high-motion regions. Many high-motion videos receive low influence scores, while numerous influential videos exhibit modest or even low motion magnitude. These findings show that our motion attribution approach captures training influence, focusing on motion rather than simply acting as a motion-saliency filter.\n(a)\n4DNEX Influence Heatmap\n(b)\nVIDGEN Influence Heatmap\nFigure 7\n:\nCross-motion influence overlap across datasets.\nHeatmaps showing the percentage overlap of top-\n100\n100\ninfluential training samples across motion categories for (a) 4DNEX and (b) VIDGEN datasets. Each cell\n(\ni\n,\nj\n)\n(i,j)\nshows the percentage of motion category\ni\ni\nâ€™s influential data (aggregated from\n5\n5\nqueries per category) that also appears in motion category\nj\nj\nâ€™s top-\n100\n100\ninfluential samples. The asymmetric nature of the matrices (e.g., bounceâ†’float\nâ‰ \n\\neq\nfloatâ†’bounce) arises because different motion categories have different numbers of unique influential videos, leading to directional overlap percentages. Consistent high-overlap pairs (e.g., bounce-float:\n44.4\n%\n44.4\\%\n/\n46.3\n%\n46.3\\%\n) and low-overlap pairs (e.g., free fall-stretch:\n12.8\n%\n12.8\\%\n/\n12.7\n%\n12.7\\%\n) across datasets validate that these influence patterns reflect fundamental aspects of motion representation in video generation models.\nD.2\nCross-Motion Influence Patterns\nTo analyze cross-motion influence patterns, we examine the percentage overlap of top-\n100\n100\ninfluential training data across different motion categories in both 4DNEX and VIDGEN datasets. As described in Â§\n4\n, we use\n5\n5\nquery samples to identify the top-100 most influential training videos, aggregating results across queries. As shown in Fig.\n7\n, both datasets exhibit remarkably similar patterns with mean overlaps of\n24.0\n%\n24.0\\%\nand\n24.3\n%\n24.3\\%\n, respectively, indicating moderate sharing of influential data across motion categories.\nBoth datasets consistently identify the same high-overlap pairs: bounce-float (\n44.4\n%\n44.4\\%\n/\n46.3\n%\n46.3\\%\n), compress-float (\n40.1\n%\n40.1\\%\n/\n34.0\n%\n34.0\\%\n), and compress-spin (\n36.9\n%\n36.9\\%\n/\n39.6\n%\n39.6\\%\n), suggesting these motions share fundamental characteristics that the model learns from similar training examples. Conversely, low-overlap pairs such as free fall-stretch (\n12.8\n%\n12.8\\%\n/\n12.7\n%\n12.7\\%\n) and float-slide (\n14.0\n%\n14.0\\%\n/\n10.9\n%\n10.9\\%\n) indicate more specialized influential data for mechanically dissimilar motions. The influence matrices are asymmetric because the number of unique influential samples shared across the\n5\n5\nquery samples differs across motion categories.\nThe similar cross-motion influence patterns observed across both the 4DNEX and VIDGEN datasets demonstrate that these relationships are generalizable across different video datasets and reflect dynamic similarity.\nAppendix E\nAdditional Method Details\nTracker-agnostic scope.\nWe treat the motion estimator as a pluggable source of saliency rather than a training dependency.\nGiven displacement magnitudes, we construct latent-space weights via bilinear mapping and normalization.\nOur implementation supports alternative estimators (such as dense optical flow or point tracking) with identical interfaces, enabling users to swap AllTracker without modifying the attribution code.\nModel-agnostic scope.\nOur attribution only requires per-example gradients under matched\n(\nt\nfix\n,\nÏµ\nfix\n)\n(t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\n, and therefore applies to both diffusion and flow-matching objectives.\nThe score reduces to a gradient inner product under a fixed preconditioner; the generator architecture affects gradient statistics but not the definition of influence.\nIn practice, replacing the denoiser or velocity field leaves the weighting and aggregation unchanged.\nAlgorithm Summary.\nFor completeness, Algorithm\n1\nsummarizes the full\nMotive\npipeline, detailing the computation of motion-weighted gradients, projection into low-dimensional space, and the subsequent influence-based ranking and selection of training clips.\nAlgorithm 1\nMotive\n: Motion-Aware Data Attribution Framework\n1:\nfine-tuning corpus\nğ’Ÿ\nft\n=\n{\n(\nğ¯\nn\n,\nğœ\nn\n)\n}\nn\n=\n1\nN\n\\mathcal{D}_{\\textnormal{ft}}=\\{(\\mathbf{v}_{n},\\mathbf{c}_{n})\\}_{n=1}^{N}\n, query video\n(\nğ¯\n^\n,\nğœ\n^\n)\n(\\hat{\\mathbf{v}},\\hat{\\mathbf{c}})\n, fixed\n(\nt\nfix\n,\nÏµ\nfix\n)\n(t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\n, projection matrix\nğ\n\\mathbf{P}\n2:\nMotion-aware influence scores\n{\nI\nmot\nâ€‹\n(\nğ¯\nn\n,\nğ¯\n^\n)\n}\nn\n=\n1\nN\n\\{I_{\\textnormal{mot}}(\\mathbf{v}_{n},\\hat{\\mathbf{v}})\\}_{n=1}^{N}\n3:\nfor\n(\nğ¯\nn\n,\nğœ\nn\n)\nâˆˆ\nğ’Ÿ\nft\n(\\mathbf{v}_{n},\\mathbf{c}_{n})\\in\\mathcal{D}_{\\textnormal{ft}}\ndo\n4:\nA\nn\nâ†\nAllTracker\nâ€‹\n(\nğ¯\nn\n)\nA_{n}\\leftarrow\\textsc{AllTracker}(\\mathbf{v}_{n})\nâŠ³\n\\triangleright\nExtract per-pixel flow displacements\nğƒ\nf\n\\mathbf{D}_{\\!f}\n(Eq.\n12\n)\n5:\nDownsample and normalize to latent-space motion mask\nğ–\nn\n\\mathbf{W}_{n}\n(Eqs.\n13\nâ€“\n15\n)\n6:\nEvaluate motion-weighted loss\nâ„’\nmot\nâ€‹\n(\nğœ½\n;\nğ¯\nn\n,\nğœ\nn\n)\n\\mathcal{L}_{\\textnormal{mot}}(\\bm{\\theta};\\mathbf{v}_{n},\\mathbf{c}_{n})\n(Eq.\n16\n)\n7:\nCompute motion gradient\nğ \nmot\nâ€‹\n(\nğœ½\n,\nğ¯\nn\n,\nt\nfix\n,\nÏµ\nfix\n)\n=\nâˆ‡\nğœ½\nâ„’\nmot\nâ€‹\n(\nğœ½\n;\nğ¯\nn\n,\nğœ\nn\n,\nt\nfix\n,\nÏµ\nfix\n)\n\\mathbf{g}_{\\textnormal{mot}}(\\bm{\\theta},\\mathbf{v}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})=\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{mot}}(\\bm{\\theta};\\mathbf{v}_{n},\\mathbf{c}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\n8:\nNormalize by frame length:\nğ \nmot\nâ†\nğ \nmot\n/\nF\n\\mathbf{g}_{\\textnormal{mot}}\\leftarrow\\mathbf{g}_{\\textnormal{mot}}/F\n9:\nProject motion gradient:\nğ \n~\nmot\nâ€‹\n(\nğœ½\n,\nğ¯\nn\n)\n:=\nğğ \nmot\nâ€‹\n(\nğœ½\n,\nğ¯\nn\n,\nt\nfix\n,\nÏµ\nfix\n)\nâ€–\nğğ \nmot\nâ€‹\n(\nğœ½\n,\nğ¯\nn\n,\nt\nfix\n,\nÏµ\nfix\n)\nâ€–\n\\tilde{\\mathbf{g}}_{\\textnormal{mot}}(\\bm{\\theta},\\mathbf{v}_{n}):=\\frac{\\mathbf{P}\\mathbf{g}_{\\textnormal{mot}}(\\bm{\\theta},\\mathbf{v}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\|\\mathbf{P}\\mathbf{g}_{\\textnormal{mot}}(\\bm{\\theta},\\mathbf{v}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\|}\n(Eq.\n17\n)\n10:\nend\nfor\n11:\nCompute query gradient:\nğ \n~\nmot\nâ€‹\n(\nğœ½\n,\nğ¯\n^\n)\n\\tilde{\\mathbf{g}}_{\\textnormal{mot}}(\\bm{\\theta},\\hat{\\mathbf{v}})\nusing the same procedure for\n(\nğ¯\n^\n,\nğœ\n^\n)\n(\\hat{\\mathbf{v}},\\hat{\\mathbf{c}})\n12:\nfor\nn\n=\n1\n,\nâ€¦\n,\nN\nn=1,\\dots,N\ndo\n13:\nI\nmot\nâ€‹\n(\nğ¯\nn\n,\nğ¯\n^\n)\n=\nğ \n~\nmot\nâ€‹\n(\nğœ½\n,\nğ¯\n^\n)\nâŠ¤\nâ€‹\nğ \n~\nmot\nâ€‹\n(\nğœ½\n,\nğ¯\nn\n)\nI_{\\textnormal{mot}}(\\mathbf{v}_{n},\\hat{\\mathbf{v}})=\\tilde{\\mathbf{g}}_{\\textnormal{mot}}(\\bm{\\theta},\\hat{\\mathbf{v}})^{\\top}\\tilde{\\mathbf{g}}_{\\textnormal{mot}}(\\bm{\\theta},\\mathbf{v}_{n})\n(Eq.\n17\n)\n14:\nend\nfor\n15:\nRank all training clips by\nI\nmot\nâ€‹\n(\nğ¯\nn\n,\nğ¯\n^\n)\nI_{\\textnormal{mot}}(\\mathbf{v}_{n},\\hat{\\mathbf{v}})\nand select top-\nK\nK\ninfluential samples using majority vote aggregation (Eq.\n18\n):\nğ’®\n=\nğ’®\nvote\nâ€‹\n(\nK\n)\n=\n{\nğ¯\nn\n|\nğ¯\nn\nâ€‹\nin top-\nâ€‹\nK\nâ€‹\nby\nâ€‹\nMajVote\n}\n\\mathcal{S}=\\mathcal{S}_{\\textnormal{vote}}(K)=\\bigl\\{\\mathbf{v}_{n}|\\mathbf{v}_{n}\\text{ in top-}K\\text{ by }\\operatorname{MajVote}\\bigr\\}\n16:\nreturn\nğ’®\n\\mathcal{S}\nAppendix F\nAdditional Experiment Details\nF.1\nHyperparameter Settings\nFor reproducibility, we document the hyperparameters used throughout attribution, subset selection, and fine-tuning. Where values were not explicitly tuned, we adopted defaults from DiffSynth-Studio and the official Wan repo.\nAttribution.\nMotion-aware influence estimation is computed at a single fixed timestep\nt\nfix\n=\n751\nt_{\\textnormal{fix}}=751\n, corresponding to scheduler index\n500\n500\nat the midpoint of the denoising trajectory, which strongly correlates with multi-timestep averaging.\nA shared Gaussian draw\nÏµ\nfix\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nğˆ\n)\n\\bm{\\epsilon}_{\\textnormal{fix}}\\sim\\mathcal{N}(0,\\mathbf{I})\nis used across all trainingâ€“query pairs to reduce stochastic variance. Gradients are projected from dimension\nD\n=\n1â€‰418â€‰996â€‰800\nD=$1\\,418\\,996\\,800$\nto\nD\nâ€²\n=\n512\nD^{\\prime}=512\nusing a Fastfood Johnsonâ€“Lindenstrauss projection\nğ\n\\mathbf{P}\nselected via the search in Fig.\n4\nto balance performance and storage. Motion weights\nğ–\n\\mathbf{W}\nare computed from AllTracker flow magnitudes\nM\nf\nM_{\\!f}\n, minâ€“max normalized to\n[\n0\n,\n1\n]\n[0,1]\nwith a small bias\nÎ¶\n=\n10\nâˆ’\n6\n\\zeta=10^{-6}\n.\nAll computations use bfloat16 precision for memory efficiency.\nSubset Selection & fine-tuning.\nFor any number of query points, we select top-\n10\n10\n% data of the datasets. We finetune the\nWan2.1-T2V-1.3B\nbackbone while freezing both the T5 text encoder\n[\nraffel2020exploring\n]\nand the VAE. The input resolution is fixed to\n480\nÃ—\n832\n480\\times 832\npixels. We use a learning rate of\n1\nÃ—\n10\nâˆ’\n5\n1\\times 10^{-5}\nand the AdamW optimizer\n[\nloshchilov2017decoupled\n]\nfollowing the DiffSynth-Studio defaults. We train the models for\n1\n1\nepoch, repeating the dataset\n50\n50\ntimes.\nEvaluation.\nThe test set consists of the same\n10\n10\nmotion categories, but with different visual appearances, compared with the query set. We provide the prompt samples below.\nExample F.1\n(\nSamples from Query Set)\n.\nWe illustrate representative prompts from our query set used to generate query videos with Veo-3.\nâ€¢\ncompress\n: â€œA slice of sandwich bread flattened by a flat metal plate, steady camera, soft studio lighting, plain backdrop; emphasize air pockets collapsing.â€\nâ€¢\nbounce\n: â€œA ping-pong ball bouncing on a white table, steady side camera, neutral light, seamless backdrop; emphasize consistent bounce height and timing.â€\nâ€¢\nroll\n: â€œA spool of thread rolling from left to right, close-up static camera, bright studio light; highlight axle rotation and smooth travel.â€\nâ€¢\nexplode\n: â€œA single balloon bursting into fragments, captured in high-speed slow motion with a fixed camera, bright even lighting, seamless background; emphasize outward debris and air release.â€\nâ€¢\nfloat\n: â€œA foam cube floating on the surface of water, static overhead camera, bright light, clean tank; emphasize buoyancy and slight rocking.â€\nExample F.2\n(\nSamples from Test Set)\n.\nWe illustrate representative prompts from our test set that our fine-tuned models use to generate test videos.\nâ€¢\ncompress\n: â€œA rubber ball being compressed under a flat press, filmed with a stationary camera. Bright, shadow-free lighting and a clean background emphasize the deformation as it flattens.â€\nâ€¢\nbounce\n: â€œA basketball bouncing vertically on a wooden court plank, unmoving camera, balanced indoor lighting, plain wall background; clearly show deformation at impact.â€\nâ€¢\nroll\n: â€œA bike tire rolling freely on a stand, static side camera, indoor neutral light; show uniform rotation without wobble.â€\nâ€¢\nexplode\n: â€œA fragile glass ornament breaking apart mid-air, fixed camera, bright controlled lighting, plain backdrop; capture shards and reflections crisply.â€\nâ€¢\nfloat\n: â€œA green leaf floating gently on perfectly still water in a transparent tank, fixed top-down camera, bright even lighting; emphasize surface tension ripples.â€\nF.2\nDetails on Motion Query Data\nA small, controlled set of query videos is constructed to isolate specific motion primitives while minimizing confounding factors (e.g., textured backgrounds, uncontrolled camera motion). Such clean and consistent clips are challenging to obtain from natural data sources. To address this, we synthesize the query set using Veo-3\n[\nveo3\n]\nand apply a strict post-generation screening for physical plausibility and generation realism. We target ten motion types:\ncompress, bounce, roll, explode, float, free fall, slide, spin, stretch, swing\n.\nFor each category, we retain\n5\n5\nquery samples, yielding a total of\n50\n50\nqueries. This scale provides adequate coverage of the motion taxonomy used in our evaluations while maintaining tractable attribution computation. We further provide a few examples of the generation prompts and the generated video query set in Fig.\n8\n.\nFigure 8\n:\nIllustration of motion query set.\nWe generate near-realistic video queries with Veo-3 across ten motion categories.\nEach category contains five query videos synthesized with controlled prompts and manually screened for clarity and physical plausibility.\nRationale for synthetic queries.\nThe query set is not used as training data; instead, it specifies targets for attribution and for multi-query aggregation. Synthetic generation offers controllability that is difficult to achieve at scale with web videos. This design yields near-realistic yet standardized stimuli aligned with our goal of probing motion-specific influence.\nAppendix G\nDiscussion\nG.1\nRuntime\nComponent\nComplexity\nRuntime\nNotes\nGradient computation\nğ’ª\nâ€‹\n(\nB\n)\n\\mathcal{O}(B)\nper sample\nQuery:\nâˆ¼\n54\n\\sim 54\nseconds\nTraining:\nâˆ¼\n150\n\\sim 150\nhours\n1\n1\nA100 GPU;\nSingle forward+backward pass;\ntraining is dominant cost but amortized over all queries;\nembarrassingly parallel (with\n64\n64\nGPUs,\nâˆ¼\n2.3\n\\sim 2.3\nhours)\nProjection\nğ’ª\nâ€‹\n(\n|\nğ’Ÿ\n|\nâ‹…\nD\nâ€²\nâ€‹\nlog\nâ¡\nD\nâ€²\n)\n\\mathcal{O}(|\\mathcal{D}|\\cdot D^{\\prime}\\log D^{\\prime})\nâˆ¼\n1.97\n\\sim 1.97\nseconds per sample\nD\nâ€²\n=\n512\nD^{\\prime}=512\nInfluence computation\nğ’ª\nâ€‹\n(\n|\nğ’Ÿ\n|\nâ‹…\nD\nâ€²\n)\n\\mathcal{O}(|\\mathcal{D}|\\cdot D^{\\prime})\nâˆ¼\n46\n\\sim 46\nmilliseconds per query\n1\n1\nquery\nÃ—\n\\times\n10k training samples\nMajority-vote aggregation\nğ’ª\nâ€‹\n(\n|\nğ’Ÿ\n|\nâ‹…\nq\n)\n\\mathcal{O}(|\\mathcal{D}|\\cdot q)\nâˆ¼\n139\n\\sim 139\nmilliseconds\n50\n50\nqueries\nÃ—\n\\times\n10k samples\nTable 6:\nRuntime Breakdown.\nDetailed computational complexity and runtime for each component of our motion attribution framework on 10k training samples with\nWan2.1-T2V-1.3B\nmodel.\nWe provide a detailed runtime breakdown from our experiments on 10k training samples with\nWan2.1-T2V-1.3B\nmodel to address scalability concerns. The key insight is that the dominant cost of our pipeline is computing per-training-sample gradients, which is performed once and then can be reused for all subsequent queries. Each training clipâ€™s gradient is projected into a compact\n512\n512\n-dimensional vector, and adding a new query requires only (i) a single backward pass to obtain its own projected\n512\n512\n-dimensional gradient and (ii) computing cosine similarity between the query vector and stored training vectors, which is exceptionally lightweight (on the order of seconds). Thus, the computational burden does not scale with the number of queries but only with the size of the training set.\nAs shown in Tab.\n6\n, the dominant cost is the one-time training data gradient computation (\nâˆ¼\n150\n\\sim 150\nhours on 1 A100), which is amortized across all queries. Once computed, adding a new query requires only\nâˆ¼\n54\n\\sim 54\nseconds (gradient computation) +\n46\n46\nms (influence computation) =\nâˆ¼\n\\sim\n54 seconds total. The training data gradient computation is embarrassingly parallel and can be reduced to\nâˆ¼\n2.3\n\\sim\\!$2.3$\nhours with\n64\n64\nGPUs.\nRuntime comparison with baselines.\nWe compare the computational cost of our method with the baseline approaches for processing 10k training samples on a single GPU (Table\n7\n). While our method requires more upfront computation than the baseline approaches, this cost is amortized across all queries, and the computed gradients can be reused for multiple selection queries, making it practical for large-scale data curation scenarios.\nMethod\nRandom\nMotion Magnitude\nOptical Flow\nV-JEPA\nOurs\nTotal for 10k (\n1\n1\nGPU)\n<\n1\n<1\nsecond\nâˆ¼\n5.5\n\\sim 5.5\nhours\nâˆ¼\n5.7\n\\sim 5.7\nhours\nâˆ¼\n3\n\\sim 3\nhours\nâˆ¼\n150\n\\sim 150\nhours\nTable 7\n:\nRuntime Comparison with Baselines.\nTotal computational time required for processing 10k training samples on a single GPU across different data selection methods.\nG.2\nLimitations\nGradient-based attribution is computationally expensive, requiring high upfront cost for per-sample gradient computation (see Â§\nG.1\n), though this cost is amortized across queries.\nOur analysis treats each video as a whole unit, thereby avoiding collapsing motion into frame-level appearance, but it risks overlooking the fact that only certain segments may carry motion-relevant information. Highly informative intervals can be diluted when averaged with static or redundant portions of the same clip. This suggests an open direction toward finer-grained attribution at the motion segment or motion event level, which could reveal more precise insights into how different phases of a trajectory shape motion learning. Another limitation is that our motion masks may overemphasize camera-only motion; we detect this by spatial uniformity of\nğ–\n\\mathbf{W}\nand down-weight such clips, but a full disentanglement of ego and object motion remains future work.\nAdditionally, our framework does not explicitly account for classifier-free guidance (CFG), which is widely used in practice to steer video generation but introduces discrepancies between training-time attribution and inference-time dynamics.\nAs a result, our influence estimates may not fully capture how guidance alters motion behavior.\nIn addition, while attribution-driven fine-tuning improves targeted motion quality, it may introduce trade-offs with base model capabilities.\nThis raises the need for future work on balancing targeted motion adaptation with the preservation of broader generative capabilities.\nG.3\nFuture Directions\nTracker-robust motion saliency.\nReplace or ensemble AllTracker with alternative estimators and use its confidence/visibility channels to weight masks.\nClosed-loop data curation.\nMove from one-shot ranking to active selection: iteratively attribute, finetune, and re-attribute, or replace simple majority voting with learned query weights.\nSafety and governance.\nUse negative-influence filtering to suppress undesirable or unsafe dynamics, document curator choices, and audit motion behaviors exposed by our framework.\nSophisticated finetuning.\nMove to more sophisticated finetuning setups, such as multi-student distillation\n[\nsong2024multi\n]\n.\nOther modalities.\nExtend our methodology to other modalities, including world models\n[\nzhu2025astra\n]\n, audio\n[\nevans2025stable\n,\nrichter2025score\n]\nor video+audio\n[\nwiedemer2025video\n]\n.\nSelf-generated video queries.\nUse model-generated videos as queries to trace problematic motion patterns (e.g., unrealistic physics) back to training data, enabling iterative diagnostics and targeted motion improvement.\nAppendix H\nVisualization\nH.1\nMotion Visualization\nTo provide intuition for the behavior of our motion-weighted loss, we visualize the motion magnitude as an overlay. We compute per-pixel motion magnitude using optical flow and apply motion-based weighting that preserves the appearance of dynamic regions while attenuating static backgrounds. This motion overlay directly illustrates the spatial weighting applied by our motion loss during training.\nFig.\n9\npresents representative frames from our dataset, comparing original frames with their corresponding motion overlays for seven distinct video samples. These visualizations show that the motion-weighted loss preferentially emphasizes dynamic content while down-weighting static scene elements.\nFigure 9\n:\nMotion overlay visualization.\nComparison of original frames and motion overlays for seven video samples across three time points (early, middle, late). The motion overlay demonstrates the spatial weighting of our motion loss: dynamic regions remain visible, while static backgrounds are attenuated to neutral gray.\nTakeaway:\nThis provides heuristic intuition into what information our motion attribution focuses on: the information in grayer regions, which lack motion, is down-weighted by our method.",
    "preview_text": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.\n\n\\dosecttoc\nMotion Attribution for Video Generation\nXindi Wu\n12\nDespoina Paschalidou\n1\nJun Gao\n1\nAntonio Torralba\n3\nLaura Leal-TaixÃ©\n1\nOlga Russakovsky\n2\nSanja Fidler\n1\nJonathan Lorraine\n1\n1\nNVIDIA\n2\nPrinceton University\n3\nMIT CSAIL\nhttps://research.nvidia.com/labs/sil/projects/MOTIVE/\nAbstract\nDespite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present\nMotive\n(\nMOTI\non attribution for\nV\nideo g\nE\nneration\n), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models.\nWe use this to study which fine-tuning clips improve or degrade temporal dynamics.\nMotive\nisolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation.\nOn text-to-video models,\nMotive\nidentifies clips that strongly affect",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "video generation",
        "motion attribution",
        "data attribution",
        "fine-tuning",
        "temporal dynamics"
    ],
    "one_line_summary": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºæ¢¯åº¦çš„è¿åŠ¨å½’å› æ¡†æ¶Motiveï¼Œç”¨äºåˆ†æè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­æ•°æ®å¯¹è¿åŠ¨çš„å½±å“ï¼Œå¹¶é€šè¿‡æ•°æ®ç­›é€‰æå‡è¿åŠ¨è´¨é‡ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šã€ŠMotion Attribution for Video Generationã€‹\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nå°½ç®¡è§†é¢‘ç”Ÿæˆæ¨¡å‹å‘å±•è¿…é€Ÿï¼Œä½†è®­ç»ƒæ•°æ®å¦‚ä½•å½±å“ç”Ÿæˆè§†é¢‘ä¸­çš„**è¿åŠ¨**ï¼ˆmotionï¼‰ä»ä¸æ˜ç¡®ã€‚ç°æœ‰æ•°æ®å½’å› æ–¹æ³•ä¸»è¦é’ˆå¯¹å›¾åƒï¼Œå…³æ³¨é™æ€å¤–è§‚ï¼Œæ— æ³•æœ‰æ•ˆè§£é‡Šè§†é¢‘ç‰¹æœ‰çš„æ—¶é—´åŠ¨æ€ã€‚ç†è§£å“ªäº›è®­ç»ƒç‰‡æ®µå¡‘é€ äº†æ¨¡å‹çš„è¿åŠ¨æ¨¡å¼ï¼Œå¯¹äºæå‡ç”Ÿæˆè§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ã€ç‰©ç†åˆç†æ€§å’Œå¯æ§æ€§è‡³å…³é‡è¦ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡ºäº† **Motive**ï¼Œé¦–ä¸ªé¢å‘è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ã€ä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„æ¢¯åº¦æ•°æ®å½’å› æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åŒ…æ‹¬ï¼š\n- **è¿åŠ¨æ„ŸçŸ¥å½’å› **ï¼šé€šè¿‡å…‰æµè®¡ç®—è¿åŠ¨æ˜¾è‘—æ€§æ©ç ï¼Œåœ¨æŸå¤±å‡½æ•°ä¸­å¯¹åŠ¨æ€åŒºåŸŸè¿›è¡ŒåŠ æƒï¼Œä»è€Œå°†**æ—¶é—´åŠ¨æ€**çš„å½’å› ä¸é™æ€å¤–è§‚è§£è€¦ã€‚\n- **é«˜æ•ˆå¯æ‰©å±•è®¾è®¡**ï¼šé‡‡ç”¨å›ºå®šæ—¶é—´æ­¥é‡‡æ ·ã€å…±äº«å™ªå£°å’ŒFastfoodéšæœºæŠ•å½±ç­‰æŠ€æœ¯ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å’Œå­˜å‚¨å¼€é”€ï¼Œä½¿å…¶èƒ½åº”ç”¨äºåäº¿å‚æ•°çº§åˆ«çš„å¤§å‹è§†é¢‘ç”Ÿæˆæ¨¡å‹å’Œæµ·é‡æ•°æ®é›†ã€‚\n- **è§†é¢‘ç‰¹å®šåå·®æ ¡æ­£**ï¼šä¿®æ­£äº†è§†é¢‘é•¿åº¦å¯¹æ¢¯åº¦å¹…å€¼çš„å½±å“ï¼Œç¡®ä¿å¯¹ä¸åŒæ—¶é•¿ç‰‡æ®µçš„å…¬å¹³å½’å› ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\nåœ¨æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼š\n- **æœ‰æ•ˆè¯†åˆ«**ï¼šMotiveèƒ½å‡†ç¡®è¯†åˆ«å‡ºå¯¹ç›®æ ‡è¿åŠ¨æ¨¡å¼æœ‰å¼ºå½±å“çš„è®­ç»ƒç‰‡æ®µã€‚\n- **æå‡æ€§èƒ½**ï¼šä½¿ç”¨Motiveç­›é€‰å‡ºçš„é«˜å½±å“åŠ›æ•°æ®ï¼ˆä»…å å…¨æ•°æ®é›†çš„10%ï¼‰è¿›è¡Œå¾®è°ƒï¼Œåœ¨VBenchè¯„æµ‹ä¸­æ˜¾è‘—æå‡äº†**è¿åŠ¨å¹³æ»‘åº¦**å’Œ**åŠ¨æ€ç¨‹åº¦**ã€‚\n- **äººç±»åå¥½**ï¼šå¾®è°ƒåçš„æ¨¡å‹åœ¨äººç±»è¯„ä¼°ä¸­ï¼Œç›¸è¾ƒäºé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹è·å¾—äº†**74.1%çš„èƒœç‡**ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\næœ¬ç ”ç©¶é¦–æ¬¡å®ç°äº†å¯¹è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­**è¿åŠ¨**ï¼ˆè€Œéå¤–è§‚ï¼‰çš„æ•°æ®å½’å› ï¼Œä¸ºç†è§£æ¨¡å‹åŠ¨åŠ›å­¦è¡Œä¸ºæä¾›äº†ç›´æ¥å·¥å…·ã€‚å…¶å®é™…ä»·å€¼åœ¨äºèƒ½å¤ŸæŒ‡å¯¼**æ•°æ®ç­›é€‰å’Œé’ˆå¯¹æ€§å¾®è°ƒ**ï¼Œä»è€Œé«˜æ•ˆæå‡ç”Ÿæˆè§†é¢‘çš„è¿åŠ¨è´¨é‡ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºæ›´å¯æ§ã€å¯è§£é‡Šä¸”è¿åŠ¨æ›´é€¼çœŸçš„è§†é¢‘ç”Ÿæˆç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡åˆ†æï¼šã€ŠMotion Attribution for Video Generationã€‹\n\n### **æ ¸å¿ƒé—®é¢˜**\nè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰çš„æ€§èƒ½æå‡ä¸¥é‡ä¾èµ–äºæµ·é‡è®­ç»ƒæ•°æ®ï¼Œä½†ä¸€ä¸ªæ ¹æœ¬æ€§é—®é¢˜å°šæœªè¢«å……åˆ†ç†è§£ï¼š**è®­ç»ƒæ•°æ®ä¸­çš„å“ªäº›è§†é¢‘ç‰‡æ®µï¼ˆclipsï¼‰å…·ä½“å½±å“äº†ç”Ÿæˆè§†é¢‘ä¸­çš„â€œè¿åŠ¨â€ï¼ˆmotionï¼‰è´¨é‡ï¼Ÿ** ç°æœ‰æ•°æ®å½’å› æ–¹æ³•ä¸»è¦é’ˆå¯¹å›¾åƒç”Ÿæˆï¼Œå…³æ³¨é™æ€å¤–è§‚ï¼ˆå¦‚ç‰©ä½“ã€çº¹ç†ï¼‰ï¼Œè€Œæ— æ³•æœ‰æ•ˆè¿½è¸ªè§†é¢‘ä¸­åŠ¨æ€æ—¶åºç»“æ„ï¼ˆå¦‚è½¨è¿¹ã€å½¢å˜ã€ç‰©ç†äº¤äº’ï¼‰çš„æ¥æºã€‚\n\n### **æ ¸å¿ƒåˆ›æ–°ç‚¹**\næœ¬æ–‡æå‡ºäº† **Motive** æ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ä¸ºè§†é¢‘ç”Ÿæˆæ¨¡å‹è®¾è®¡çš„ã€**ä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„æ•°æ®å½’å› æ–¹æ³•**ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å½’å› ç›®æ ‡ä»â€œå¤–è§‚â€è½¬å‘â€œè¿åŠ¨â€ï¼Œå¹¶å®ç°äº†åœ¨å¤§è§„æ¨¡é«˜è´¨é‡è§†é¢‘æ•°æ®é›†å’Œæ¨¡å‹ä¸Šçš„é«˜æ•ˆã€å¯æ‰©å±•è®¡ç®—ã€‚\n\n#### **1. æŠ€æœ¯åˆ›æ–°**\n- **è¿åŠ¨æ„ŸçŸ¥çš„æ¢¯åº¦å½’å› **ï¼šé€šè¿‡**è¿åŠ¨åŠ æƒçš„æŸå¤±æ©ç **ï¼Œåœ¨è®¡ç®—æ¢¯åº¦æ—¶å¼ºè°ƒåŠ¨æ€åŒºåŸŸï¼ŒæŠ‘åˆ¶é™æ€èƒŒæ™¯ï¼Œä»è€Œéš”ç¦»å‡ºè¿åŠ¨ç‰¹å®šçš„å½±å“ä¿¡å·ã€‚\n- **é’ˆå¯¹è§†é¢‘çš„å·¥ç¨‹ä¼˜åŒ–**ï¼š\n    - **å¯æ‰©å±•æ€§**ï¼šé‡‡ç”¨å•æ—¶é—´æ­¥/å™ªå£°é‡‡æ ·ã€å…¬å…±éšæœºæ€§ã€ä»¥åŠ**FastfoodéšæœºæŠ•å½±**ï¼Œå°†é«˜ç»´æ¢¯åº¦å‹ç¼©åˆ°ä½ç»´ç©ºé—´ï¼Œä½¿åäº¿å‚æ•°æ¨¡å‹çš„å½’å› è®¡ç®—å˜å¾—å¯è¡Œã€‚\n    - **å¸§é•¿åå·®ä¿®æ­£**ï¼šå¯¹æ¢¯åº¦è¿›è¡Œå¸§æ•°å½’ä¸€åŒ–ï¼Œç¡®ä¿ä¸åŒæ—¶é•¿è§†é¢‘ç‰‡æ®µçš„å½’å› å…¬å¹³æ€§ã€‚\n- **é¢å‘å¾®è°ƒçš„æ•°æ®ç­›é€‰**ï¼šåˆ©ç”¨å½’å› å¾—åˆ†ï¼Œä»æµ·é‡æ•°æ®ä¸­è‡ªåŠ¨ç­›é€‰å‡ºå¯¹ç›®æ ‡è¿åŠ¨æ¨¡å¼**å½±å“æœ€å¤§**çš„å°‘é‡ï¼ˆå¦‚10%ï¼‰é«˜è´¨é‡ç‰‡æ®µç”¨äºå¾®è°ƒï¼Œå®ç°é«˜æ•ˆçš„æ•°æ®ç­–å±•ã€‚\n\n#### **2. æ–¹æ³•è§£å†³è·¯å¾„**\n```\n1. é—®é¢˜å®šä¹‰ï¼šåœ¨å¾®è°ƒåœºæ™¯ä¸‹ï¼Œé‡åŒ–æ¯ä¸ªè®­ç»ƒè§†é¢‘ç‰‡æ®µ (ğ¯_n) å¯¹æŸ¥è¯¢è§†é¢‘ (ğ¯^) ä¸­è¿åŠ¨çš„å½±å“ã€‚\n2. è¿åŠ¨æå–ï¼šä½¿ç”¨å…‰æµä¼°è®¡å™¨ï¼ˆå¦‚AllTrackerï¼‰è®¡ç®—è®­ç»ƒå’ŒæŸ¥è¯¢è§†é¢‘çš„è¿åŠ¨å¹…åº¦å›¾ (M_f)ã€‚\n3. è¿åŠ¨åŠ æƒæŸå¤±ï¼šåœ¨æ½œåœ¨ç©ºé—´ï¼Œç”¨è¿åŠ¨å¹…åº¦å›¾å¯¹æ‰©æ•£æ¨¡å‹æŸå¤±è¿›è¡Œç©ºé—´åŠ æƒï¼Œå¾—åˆ°è¿åŠ¨ç‰¹å®šæŸå¤± â„’_motã€‚\n4. é«˜æ•ˆå½’å› è®¡ç®—ï¼š\n   a. è®¡ç®— â„’_mot å¯¹æ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚\n   b. ä½¿ç”¨å›ºå®šæ—¶é—´æ­¥å’Œå…±äº«å™ªå£°é‡‡æ ·ä»¥é™ä½æ–¹å·®ã€‚\n   c. é€šè¿‡FastfoodæŠ•å½±å°†é«˜ç»´æ¢¯åº¦å‹ç¼©ä¸ºä½ç»´ç‰¹å¾å‘é‡ã€‚\n   d. è®¡ç®—æŸ¥è¯¢è§†é¢‘ä¸æ‰€æœ‰è®­ç»ƒè§†é¢‘æŠ•å½±æ¢¯åº¦ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä½œä¸ºå½’å› å¾—åˆ† I_motã€‚\n5. æ•°æ®ç­›é€‰ï¼šæ ¹æ®å½’å› å¾—åˆ†æ’åï¼Œé€‰å–top-Kç‰‡æ®µç»„æˆå¾®è°ƒæ•°æ®é›†ã€‚\n```\n\n### **å®é™…ä»·å€¼ä¸éªŒè¯ç»“æœ**\n- **æå‡ç”Ÿæˆè§†é¢‘çš„è¿åŠ¨è´¨é‡**ï¼šä½¿ç”¨Motiveç­›é€‰å‡ºçš„æ•°æ®å¾®è°ƒæ¨¡å‹ï¼Œåœ¨**VBench**è¯„æµ‹ä¸­æ˜¾è‘—æå‡äº†**è¿åŠ¨å¹³æ»‘åº¦**å’Œ**åŠ¨æ€ç¨‹åº¦**ã€‚ä»…ä½¿ç”¨10%çš„æ•°æ®ï¼Œå…¶æ•ˆæœå³è¾¾åˆ°æˆ–è¶…è¿‡äº†ä½¿ç”¨å…¨éƒ¨æ•°æ®å¾®è°ƒçš„æ€§èƒ½ã€‚\n- **äººç±»åå¥½èƒœå‡º**ï¼šåœ¨äººç±»è¯„ä¼°ä¸­ï¼Œä¸é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼ŒMotiveå¾®è°ƒåçš„æ¨¡å‹è·å¾—äº† **74.1%** çš„åå¥½èƒœç‡ã€‚\n- **ä¸ºæ¨¡å‹ç†è§£å’Œæ•°æ®ç­–å±•æä¾›æ–°å·¥å…·**ï¼š\n    - **å¯è§£é‡Šæ€§**ï¼šå¯ä»¥è¿½æº¯ç”Ÿæˆè§†é¢‘ä¸­ä¸è‡ªç„¶è¿åŠ¨çš„æ¥æºï¼ˆå³å“ªäº›è®­ç»ƒæ•°æ®å¯¼è‡´äº†ä¸è‰¯è¿åŠ¨ï¼‰ã€‚\n    - **é«˜æ•ˆæ•°æ®æ¸…æ´—**ï¼šèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«å¹¶è¿‡æ»¤æ‰å¯¹è¿åŠ¨å­¦ä¹ æ— ç›Šç”šè‡³æœ‰å®³çš„æ•°æ®ï¼ˆå¦‚ä»…æœ‰æ‘„åƒæœºè¿åŠ¨ã€å¡é€šé£æ ¼ç‰‡æ®µï¼‰ã€‚\n    - **å®šå‘æ€§èƒ½æå‡**ï¼šå¯ä»¥é’ˆå¯¹ç‰¹å®šè¿åŠ¨ç±»å‹ï¼ˆå¦‚â€œæ¼‚æµ®â€ã€â€œæ—‹è½¬â€ï¼‰ç­›é€‰æ•°æ®ï¼Œè¿›è¡Œä¸“é¡¹å¾®è°ƒï¼Œä»è€Œé’ˆå¯¹æ€§æå‡æ¨¡å‹åœ¨è¯¥è¿åŠ¨ä¸Šçš„ç”Ÿæˆèƒ½åŠ›ã€‚\n\n### **æ€»ç»“**\n**Motive** çš„æ ¸å¿ƒè´¡çŒ®æ˜¯**é¦–æ¬¡å°†æ•°æ®å½’å› çš„ç„¦ç‚¹ä»å›¾åƒçš„å¤–è§‚å»¶ä¼¸åˆ°è§†é¢‘çš„è¿åŠ¨**ï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ—æŠ€æœ¯åˆ›æ–°ä½¿å…¶é€‚ç”¨äºç°ä»£å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚å®ƒä¸ä»…æ˜¯ä¸€ä¸ªåˆ†æå·¥å…·ï¼Œæ›´æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ•°æ®ç­–å±•å¼•æ“ï¼Œèƒ½å¤Ÿä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹å¼ï¼Œé«˜æ•ˆã€å®šå‘åœ°æå‡è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨**æ—¶åºåŠ¨æ€**è¿™ä¸€å…³é”®ç»´åº¦ä¸Šçš„è´¨é‡ä¸å¯æ§æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£æ•°æ®å¦‚ä½•å¡‘é€ ç”Ÿæˆæ¨¡å‹çš„â€œè¡Œä¸ºâ€å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚",
            "timestamp": "2026-01-20T17:51:02.976811",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡ã€ŠMotion Attribution for Video Generationã€‹åˆ†æ\n\n### æ ¸å¿ƒè´¡çŒ®\næœ¬æ–‡æå‡ºäº† **Motive**ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„**è¿åŠ¨å½’å› æ¡†æ¶**ã€‚å®ƒé€šè¿‡åŸºäºæ¢¯åº¦çš„æ•°æ®å½’å› æ–¹æ³•ï¼Œè¯†åˆ«å‡ºå¯¹ç”Ÿæˆè§†é¢‘ä¸­**è¿åŠ¨åŠ¨æ€**æœ‰é‡è¦å½±å“çš„è®­ç»ƒç‰‡æ®µï¼Œå¹¶åˆ©ç”¨è¿™äº›ä¿¡æ¯æŒ‡å¯¼æ•°æ®ç­›é€‰å’Œå¾®è°ƒï¼Œä»è€Œæ˜¾è‘—æå‡ç”Ÿæˆè§†é¢‘çš„**æ—¶é—´ä¸€è‡´æ€§å’Œç‰©ç†åˆç†æ€§**ã€‚\n\n### æŠ€æœ¯å‘å±•è„‰ç»œæ¢³ç†\n\nè¯¥ç ”ç©¶å»ºç«‹åœ¨æ•°æ®å½’å› å’Œè§†é¢‘ç”Ÿæˆä¸¤å¤§é¢†åŸŸçš„äº¤å‰ç‚¹ä¸Šã€‚å…¶å‘å±•è„‰ç»œå¯ä»¥æ¢³ç†ä¸ºä»¥ä¸‹å‡ ä¸ªå…³é”®é˜¶æ®µå’Œæ–¹å‘ï¼š\n\n#### æ–¹å‘ä¸€ï¼šæ•°æ®å½’å› æ–¹æ³•çš„æ¼”è¿›ï¼ˆä»é€šç”¨åˆ°æ‰©æ•£æ¨¡å‹ä¸“ç”¨ï¼‰\n\n1.  **ç»å…¸å½±å“å‡½æ•° (Influence Functions)**\n    *   **ä»£è¡¨å·¥ä½œ**: Koh & Liang (2017) *Understanding Black-box Predictions via Influence Functions*\n    *   **æ ¸å¿ƒæ€æƒ³**: é€šè¿‡è®¡ç®—Hessiané€†ä¸æ¢¯åº¦çš„ä¹˜ç§¯ï¼Œç†è®ºä¸Šé‡åŒ–å•ä¸ªè®­ç»ƒæ ·æœ¬å¯¹æ¨¡å‹åœ¨æŸä¸ªæµ‹è¯•æ ·æœ¬ä¸Šé¢„æµ‹çš„å½±å“ã€‚\n    *   **å±€é™æ€§**: è®¡ç®—å’Œå­˜å‚¨å®Œæ•´çš„Hessiané€†çŸ©é˜µå¯¹äºç°ä»£å¤§è§„æ¨¡ç¥ç»ç½‘ç»œæ˜¯**ä¸å¯è¡Œçš„**ã€‚\n\n2.  **å¯æ‰©å±•çš„æ¢¯åº¦è¿‘ä¼¼æ–¹æ³•**\n    *   **ä»£è¡¨å·¥ä½œ**: TracIn (Pruthi et al., 2020), TRAK (Park et al., 2023)\n    *   **æ”¹è¿›ç‚¹**: æ”¾å¼ƒäº†ç²¾ç¡®çš„Hessiané€†è®¡ç®—ï¼Œè½¬è€Œä½¿ç”¨**æ¢¯åº¦å†…ç§¯**æˆ–**æŠ•å½±æ¢¯åº¦ç‰¹å¾**æ¥è¿‘ä¼¼å½±å“åŠ›ã€‚è¿™å¤§å¤§é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä½¿å…¶èƒ½å¤Ÿåº”ç”¨äºå¤§è§„æ¨¡æ¨¡å‹å’Œæ•°æ®é›†ã€‚\n    *   **æ ¸å¿ƒå…¬å¼**: `I(x_n, x_test) â‰ˆ g(x_test)^T Â· g(x_n)`ï¼Œå…¶ä¸­ `g` æ˜¯æ¢¯åº¦å‘é‡ã€‚\n\n3.  **æ‰©æ•£æ¨¡å‹ä¸“ç”¨å½’å› **\n    *   **ä»£è¡¨å·¥ä½œ**: Diffusion-ReTrac (Xie et al., 2024)\n    *   **æ”¹è¿›ç‚¹**: å‘ç°äº†æ‰©æ•£æ¨¡å‹è®­ç»ƒä¸­å› åœ¨å¤šä¸ªæ—¶é—´æ­¥ `t` å’Œå™ªå£° `Îµ` ä¸Šé‡‡æ ·æ¢¯åº¦è€Œå¼•å…¥çš„**æ—¶é—´æ­¥åå·®**ã€‚é€šè¿‡**æ¢¯åº¦å½’ä¸€åŒ–**å’Œ**å…±äº«éšæœºæ€§é‡‡æ ·**ï¼Œç¨³å®šäº†å½’å› æ’åã€‚\n    *   **æ ¸å¿ƒå…¬å¼**: å¯¹æ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦åœ¨ `(t, Îµ)` ä¸Šå¹³å‡å¹¶å½’ä¸€åŒ–åè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ã€‚\n    *   **å±€é™æ€§**: è¿™äº›æ–¹æ³•ä¸»è¦é’ˆå¯¹**å›¾åƒç”Ÿæˆ**ï¼Œå…³æ³¨é™æ€å¤–è§‚ï¼ˆç‰©ä½“ã€çº¹ç†ï¼‰ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰è§†é¢‘ç‰¹æœ‰çš„**æ—¶é—´åŠ¨æ€ç»“æ„**ã€‚\n\n#### æ–¹å‘äºŒï¼šè§†é¢‘ç”Ÿæˆä¸­è¿åŠ¨å»ºæ¨¡çš„æ¼”è¿›\n\n1.  **æ¶æ„ä¸ç®—æ³•æ”¹è¿›**\n    *   **ä»£è¡¨å·¥ä½œ**: åœ¨å›¾åƒæ‰©æ•£ä¸»å¹²ä¸Šå¢åŠ æ—¶é—´æ³¨æ„åŠ›ã€å› æœå·ç§¯ã€3D U-Netç­‰ï¼ˆå¦‚Video Diffusion Models, Stable Video Diffusionï¼‰ã€‚\n    *   **æ ¸å¿ƒæ€æƒ³**: é€šè¿‡ä¿®æ”¹æ¨¡å‹æ¶æ„ï¼Œæ˜¾å¼åœ°å¢åŠ å¯¹æ—¶é—´ç»´åº¦çš„å»ºæ¨¡èƒ½åŠ›ã€‚\n    *   **å±€é™æ€§**: ä¾§é‡äºæ¨¡å‹æœ¬èº«çš„è®¾è®¡ï¼Œ**ç¼ºä¹å¯¹è®­ç»ƒæ•°æ®å¦‚ä½•å½±å“æœ€ç»ˆè¿åŠ¨è´¨é‡çš„ç†è§£**ã€‚\n\n2.  **æ•°æ®é©±åŠ¨çš„è´¨é‡æå‡**\n    *   **è§‚å¯Ÿ**: è§†é¢‘ç”Ÿæˆè´¨é‡çš„å·¨å¤§é£è·ƒå¾€å¾€ä¸**å¤§è§„æ¨¡ã€é«˜è´¨é‡è§†é¢‘æ•°æ®é›†çš„æ‰©å±•å’Œç­›é€‰**ç´§å¯†ç›¸å…³ã€‚\n    *   **é—®é¢˜**: å°½ç®¡æ•°æ®è‡³å…³é‡è¦ï¼Œä½†ç¤¾åŒºç¼ºä¹å·¥å…·æ¥é‡åŒ–â€œ**å“ªäº›å…·ä½“çš„è®­ç»ƒç‰‡æ®µå¡‘é€ äº†ç‰¹å®šçš„è¿åŠ¨æ¨¡å¼**â€ã€‚\n\n#### æ–¹å‘ä¸‰ï¼šæœ¬æ–‡å·¥ä½œâ€”â€”è¿åŠ¨ä¸­å¿ƒçš„æ•°æ®å½’å› ï¼ˆMotiveï¼‰\n\næœ¬æ–‡æ˜¯ä¸Šè¿°ä¸¤ä¸ªæ–¹å‘çš„äº¤æ±‡ä¸çªç ´ï¼Œä¸»è¦æ”¹è¿›å’Œåˆ›æ–°ç‚¹å¦‚ä¸‹ï¼š\n\n1.  **ä»â€œå¤–è§‚å½’å› â€åˆ°â€œè¿åŠ¨å½’å› â€çš„èŒƒå¼è½¬å˜**\n    *   **ä¹‹å‰å·¥ä½œçš„å±€é™**: å°†è§†é¢‘å½’å› ç®€å•åœ°è§†ä¸ºå¤šå¸§å›¾åƒå½’å› ï¼Œ**æ··æ·†äº†åŠ¨æ€è¿åŠ¨ä¸é™æ€å¤–è§‚**ï¼Œå¯¼è‡´å½’å› ç»“æœåå‘äºèƒŒæ™¯ã€ç‰©ä½“ç­‰é™æ€åŒ¹é…ã€‚\n    *   **Motiveçš„æ”¹è¿›**: é¦–æ¬¡æ˜ç¡®æå‡ºå¹¶å®ç°äº†**é’ˆå¯¹è¿åŠ¨åŠ¨æ€çš„å½’å› **ã€‚é€šè¿‡å¼•å…¥**è¿åŠ¨åŠ æƒçš„æŸå¤±æ©ç **ï¼Œåœ¨è®¡ç®—æ¢¯åº¦æ—¶å¼ºè°ƒåŠ¨æ€åŒºåŸŸï¼ŒæŠ‘åˆ¶é™æ€èƒŒæ™¯ï¼Œä½¿å½’å› ä¿¡å·èšç„¦äºæ—¶é—´å˜åŒ–ã€‚\n\n2.  **è§£å†³è§†é¢‘ç‰¹æœ‰çš„è®¡ç®—ä¸å…¬å¹³æ€§æŒ‘æˆ˜**\n    *   **è®¡ç®—å¯æ‰©å±•æ€§**:\n        *   **é—®é¢˜**: è§†é¢‘åºåˆ—é•¿ï¼Œæ¢¯åº¦è®¡ç®—æˆæœ¬éšå¸§æ•°ã€æ—¶é—´æ­¥ã€å™ªå£°é‡‡æ ·å‘ˆå€æ•°å¢é•¿ã€‚\n        *   **æ”¹è¿›**: é‡‡ç”¨**å•æ—¶é—´æ­¥/å•å™ªå£°é‡‡æ ·ä¼°è®¡å™¨** (`t_fix`, `Îµ_fix`)ï¼Œå¹¶åˆ©ç”¨**FastfoodéšæœºæŠ•å½±**å°†é«˜ç»´æ¢¯åº¦å‹ç¼©åˆ°ä½ç»´ç©ºé—´ (`D=512`)ï¼Œå®ç°äº†å¯¹åäº¿å‚æ•°æ¨¡å‹å’Œå¤§å‹æ•°æ®é›†çš„é«˜æ•ˆå¤„ç†ã€‚\n    *   **å¸§é•¿åå·®ä¿®æ­£**:\n        *   **é—®é¢˜**: åŸå§‹æ¢¯åº¦å¹…å€¼ä¸è§†é¢‘å¸§æ•° `F` ç›¸å…³ï¼Œå¯¼è‡´é•¿è§†é¢‘åœ¨å½’å› ä¸­å ä¼˜ã€‚\n        *   **æ”¹è¿›**: åœ¨æ¢¯åº¦è®¡ç®—ä¸­å¼•å…¥ `1/F` çš„å½’ä¸€åŒ–å› å­ï¼Œç¡®ä¿ä¸åŒæ—¶é•¿è§†é¢‘ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒã€‚\n\n3.  **æ„å»ºâ€œå½’å› -ç­›é€‰-æ”¹è¿›â€çš„é—­ç¯åº”ç”¨**\n    *   **ä¹‹å‰å·¥ä½œ**: å½’å› å¤šç”¨äºäº‹åè§£é‡Šæˆ–åˆ†æã€‚\n    *   **Motiveçš„æ”¹è¿›**: å°†è¿åŠ¨å½’å› ç›´æ¥åº”ç”¨äº**æ•°æ®ç­›é€‰**ï¼ŒæŒ‡å¯¼å¾®è°ƒã€‚\n        *   **æ–¹æ³•**: å¯¹ç›®æ ‡è¿åŠ¨ï¼ˆæŸ¥è¯¢è§†é¢‘ï¼‰è®¡ç®—æ‰€æœ‰è®­ç»ƒç‰‡æ®µçš„å½±å“åŠ›åˆ†æ•°ï¼Œé€‰å–Top-Ké«˜åˆ†æ ·æœ¬ä½œä¸ºå¾®è°ƒæ•°æ®é›†ã€‚\n        *   **å¤šæŸ¥è¯¢èšåˆ**: ä½¿ç”¨**å¤šæ•°æŠ•ç¥¨æ³•**ï¼Œç­›é€‰å‡ºå¯¹å¤šç§ç›®æ ‡è¿åŠ¨ consistently æœ‰å½±å“åŠ›çš„æ ·æœ¬ï¼Œæå‡æ³›åŒ–æ€§ã€‚\n    *   **å®é™…ä»·å€¼**: ä»…ä½¿ç”¨ **10%** çš„ç­›é€‰æ•°æ®å¾®è°ƒï¼Œåœ¨è¿åŠ¨å¹³æ»‘åº¦å’ŒåŠ¨æ€ç¨‹åº¦æŒ‡æ ‡ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº†ä½¿ç”¨å…¨éƒ¨æ•°æ®å¾®è°ƒçš„æ•ˆæœï¼Œ**äººç±»åå¥½èƒœç‡è¾¾74.1%**ã€‚è¿™ä¸ºé«˜æ•ˆã€å®šå‘åœ°æå‡è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è¿åŠ¨è´¨é‡æä¾›äº†å¯å®è·µçš„å·¥å…·ã€‚\n\n### æ€»ç»“ï¼šå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘\n\n1.  **å½’å› æ–¹æ³•æœ¬èº«çš„å¯æ‰©å±•æ€§ä¸ç²¾å‡†åŒ–**ï¼šä»ç†è®ºå½±å“å‡½æ•° -> å®ç”¨æ¢¯åº¦è¿‘ä¼¼ -> é’ˆå¯¹æ‰©æ•£æ¨¡å‹è®­ç»ƒåŠ¨æ€çš„ç¨³å®šåŒ– -> é’ˆå¯¹è§†é¢‘æ¨¡æ€çš„è¿åŠ¨ç‰¹å¼‚æ€§èšç„¦ã€‚\n2.  **ä»æ¨¡å‹æ¶æ„åˆ›æ–°åˆ°æ•°æ®ç†è§£æ·±åŒ–**ï¼šè§†é¢‘ç”Ÿæˆç¤¾åŒºä»ä¸“æ³¨äºè®¾è®¡æ›´å¥½çš„æ—¶ç©ºç½‘ç»œï¼Œå¼€å§‹è½¬å‘ç†è§£é©±åŠ¨æ¨¡å‹è¡¨ç°çš„æ ¹æœ¬â€”â€”è®­ç»ƒæ•°æ®ã€‚Motive æ˜¯è¿™ä¸€è½¬å‘ä¸­çš„å…³é”®å·¥å…·ã€‚\n3.  **ä»äº‹åè§£é‡Šåˆ°äº‹å‰å¹²é¢„çš„åº”ç”¨èŒƒå¼**ï¼šå½’å› æŠ€æœ¯ä»ä¸€ç§è¯Šæ–­ã€è§£é‡Šå·¥å…·ï¼Œå‘å±•ä¸ºä¸€ç§ä¸»åŠ¨çš„ã€ç”¨äºæ•°æ®æ²»ç†å’Œæ¨¡å‹ä¼˜åŒ–çš„å·¥ç¨‹å·¥å…·ã€‚è¿™ä¸ºæ„å»ºæ›´å¯æ§ã€æ›´å¯é çš„ç”Ÿæˆç³»ç»Ÿå¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚\n\n**ç»“è®º**ï¼šæœ¬æ–‡çš„ **Motive** æ¡†æ¶æ ‡å¿—ç€è§†é¢‘ç”Ÿæˆç ”ç©¶è¿›å…¥äº†ä¸€ä¸ªæ–°é˜¶æ®µï¼Œå³é€šè¿‡æ•°æ®å½’å› æŠ€æœ¯æ¥**é‡åŒ–å¹¶åˆ©ç”¨æ•°æ®ä¸è¿åŠ¨è´¨é‡ä¹‹é—´çš„å› æœå…³ç³»**ã€‚å®ƒä¸ä»…æ˜¯ä¸€ä¸ªåˆ†æå·¥å…·ï¼Œæ›´æ˜¯ä¸€ä¸ªèƒ½æœ‰æ•ˆæŒ‡å¯¼æ•°æ®ç­›é€‰ã€å®ç°é’ˆå¯¹æ€§æ¨¡å‹æ”¹è¿›çš„å®ç”¨ç³»ç»Ÿï¼Œå¯¹æ¨åŠ¨é«˜è´¨é‡ã€ç‰©ç†åˆç†çš„è§†é¢‘ç”Ÿæˆå…·æœ‰é‡è¦æ„ä¹‰ã€‚",
            "timestamp": "2026-01-20T17:51:52.617605",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡ã€ŠMotion Attribution for Video Generationã€‹çš„å‰ä½œåˆ†æ\n\næœ¬æ–‡ä½œè€…å›¢é˜Ÿï¼ˆç‰¹åˆ«æ˜¯ç¬¬ä¸€ä½œè€…Xindi Wuï¼‰åœ¨æ•°æ®å½’å› ï¼ˆData Attributionï¼‰å’Œè§†é¢‘ç”Ÿæˆé¢†åŸŸæœ‰ä¸€ç³»åˆ—ç›¸å…³çš„å‰æœŸå·¥ä½œã€‚ä»¥ä¸‹è¡¨æ ¼è¯¦ç»†æ¢³ç†äº†å…¶æ ¸å¿ƒå‰ä½œï¼Œå¹¶é‡ç‚¹å¯¹æ¯”äº†æœ¬æ–‡ï¼ˆMotiveï¼‰ä¸è¿™äº›å‰ä½œåœ¨**æŠ€æœ¯æ–¹æ³•ã€æ ¸å¿ƒè´¡çŒ®å’Œåº”ç”¨åœºæ™¯**ä¸Šçš„å…·ä½“åŒºåˆ«ä¸æ”¹è¿›ã€‚\n\n| å‰ä½œè®ºæ–‡æ ‡é¢˜ | å‘è¡¨å¹´ä»½/ä¼šè®® | æ ¸å¿ƒè´¡çŒ®ä¸æŠ€æœ¯æ–¹æ³• | ä¸æœ¬æ–‡ï¼ˆMotiveï¼‰çš„ä¸»è¦åŒºåˆ«ä¸æ”¹è¿› |\n| :--- | :--- | :--- | :--- |\n| **ICONS: Interpretable COncept-based Data SelectioN** | 2024 (ICLR) | æå‡ºäº†ä¸€ä¸ª**åŸºäºæ¦‚å¿µçš„æ•°æ®é€‰æ‹©æ¡†æ¶**ï¼Œç”¨äºå›¾åƒç”Ÿæˆæ¨¡å‹çš„å¾®è°ƒã€‚å®ƒé€šè¿‡**æ–‡æœ¬å¼•å¯¼çš„å¥–åŠ±æ¨¡å‹**è®¡ç®—è®­ç»ƒæ•°æ®å¯¹ç‰¹å®šâ€œæ¦‚å¿µâ€ï¼ˆå¦‚é£æ ¼ã€ç‰©ä½“ï¼‰çš„å½’å› åˆ†æ•°ï¼Œä»è€Œé€‰æ‹©æœ€å…·å½±å“åŠ›çš„æ•°æ®å­é›†è¿›è¡Œé«˜æ•ˆå¾®è°ƒã€‚ | **1. å½’å› ç›®æ ‡ä¸åŒï¼š** ICONSå½’å› äº**é™æ€çš„ã€è¯­ä¹‰å±‚é¢çš„â€œæ¦‚å¿µâ€**ï¼ˆå¦‚â€œæ¢µé«˜é£æ ¼â€ã€â€œæŸ¯åŸºçŠ¬â€ï¼‰ï¼Œè€ŒMotiveå½’å› äº**åŠ¨æ€çš„â€œè¿åŠ¨â€**ï¼ˆå¦‚æ»šåŠ¨ã€å¼¹è·³ï¼‰ã€‚<br>**2. æŠ€æœ¯è·¯å¾„ä¸åŒï¼š** ICONSä¾èµ–äº**å¤–éƒ¨å¥–åŠ±æ¨¡å‹**ï¼ˆå¦‚CLIPï¼‰æ¥è¯„ä¼°æ¦‚å¿µç›¸ä¼¼æ€§ï¼›Motiveåˆ™ä½¿ç”¨**æ¨¡å‹å†…éƒ¨çš„æ¢¯åº¦**ï¼ˆåŸºäºæ‰©æ•£æŸå¤±ï¼‰è¿›è¡Œè®¡ç®—ï¼Œæ— éœ€å¤–éƒ¨æ¨¡å‹ã€‚<br>**3. é¢†åŸŸä¸åŒï¼š** ICONSé’ˆå¯¹**å›¾åƒç”Ÿæˆ**ï¼›Motiveä¸“é—¨è§£å†³**è§†é¢‘ç”Ÿæˆ**ä¸­ç‰¹æœ‰çš„æ—¶åºåŠ¨æ€å½’å› é—®é¢˜ï¼Œå¼•å…¥äº†è¿åŠ¨æ©ç ã€å¸§é•¿æ ¡æ­£ç­‰è§†é¢‘ä¸“ç”¨æŠ€æœ¯ã€‚ |\n| **Compact Data Selection: Towards Efficient and Interpretable Data-Centric AI** | 2025 (é¢„å°æœ¬) | æ¢ç´¢äº†åœ¨**å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹**ä¸Šè¿›è¡Œ**é«˜æ•ˆæ•°æ®é€‰æ‹©**çš„é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é€‰æ‹©çš„è®¡ç®—æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚å®ƒå¯èƒ½æ¶‰åŠæ¢¯åº¦æŠ•å½±ã€é«˜æ•ˆç›¸ä¼¼åº¦è®¡ç®—ç­‰æŠ€æœ¯ã€‚ | **1. é—®é¢˜æ³›åŒ– vs. é—®é¢˜ç‰¹åŒ–ï¼š** ã€ŠCompact Data Selectionã€‹æ›´åå‘äºä¸€ä¸ª**é€šç”¨ã€é«˜æ•ˆçš„æ¡†æ¶**ï¼Œå¯èƒ½é€‚ç”¨äºå¤šç§ä»»åŠ¡ã€‚è€ŒMotiveæ˜¯**é«˜åº¦ç‰¹åŒ–**çš„ï¼Œä¸“é—¨è§£å†³â€œ**è§†é¢‘è¿åŠ¨å½’å› **â€è¿™ä¸€æœªè¢«æ¢ç´¢çš„å…·ä½“é—®é¢˜ã€‚<br>**2. æŠ€æœ¯åˆ›æ–°ç‚¹ä¸åŒï¼š** æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°åœ¨äº**â€œè¿åŠ¨æ„ŸçŸ¥â€**â€”â€”é€šè¿‡å…‰æµè®¡ç®—è¿åŠ¨æ©ç æ¥åŠ æƒæŸå¤±æ¢¯åº¦ï¼Œä»è€Œå°†å½’å› ä¿¡å·ä»é™æ€å¤–è§‚ä¸­åˆ†ç¦»å‡ºæ¥ã€‚è¿™æ˜¯å‰ä½œæ‰€æ²¡æœ‰çš„ã€é’ˆå¯¹è§†é¢‘æœ¬è´¨çš„åˆ›æ–°ã€‚<br>**3. éªŒè¯åœºæ™¯ä¸åŒï¼š** æœ¬æ–‡çš„éªŒè¯å®Œå…¨å›´ç»•**è§†é¢‘ç”Ÿæˆè´¨é‡**ï¼ˆç‰¹åˆ«æ˜¯è¿åŠ¨å¹³æ»‘åº¦ã€åŠ¨æ€ç¨‹åº¦ï¼‰å’Œäººç±»åå¥½å±•å¼€ï¼Œè€Œéé€šç”¨çš„é€‰æ‹©æ•ˆç‡æˆ–æ¦‚å¿µä¿çœŸåº¦ã€‚ |\n| **ï¼ˆç›¸å…³èƒŒæ™¯å·¥ä½œï¼‰Diffusion-ReTrac** | 2024 (Xie et al.) | å¹¶éæœ¬æ–‡ä½œè€…çš„å·¥ä½œï¼Œä½†æ˜¯æœ¬æ–‡æ–¹æ³•çš„é‡è¦**åŸºç¡€**ã€‚å®ƒæå‡ºäº†é’ˆå¯¹**å›¾åƒæ‰©æ•£æ¨¡å‹**çš„æ•°æ®å½’å› æ–¹æ³•ï¼Œè§£å†³äº†æ—¶é—´æ­¥ï¼ˆtimestepï¼‰åå·®é—®é¢˜ï¼Œé€šè¿‡æ¢¯åº¦å½’ä¸€åŒ–å’Œå­é‡‡æ ·æ¥ç¨³å®šå½’å› åˆ†æ•°ã€‚ | **1. ä»å›¾åƒåˆ°è§†é¢‘çš„å…³é”®æ‰©å±•ï¼š** Diffusion-ReTracæ˜¯**å›¾åƒå±‚é¢**çš„å½’å› ã€‚Motiveå°†å…¶æ‰©å±•è‡³**è§†é¢‘**ï¼Œå¹¶è§£å†³äº†ä¸‰ä¸ªæ–°æŒ‘æˆ˜ï¼š**(i) è¿åŠ¨å®šä½**ï¼ˆç”¨æ©ç èšç„¦åŠ¨æ€åŒºåŸŸï¼‰ã€**(ii) æ‰©å±•åˆ°åºåˆ—**ï¼ˆå¤„ç†å¤šå¸§æ¢¯åº¦ï¼‰ã€**(iii) æ•æ‰æ—¶åºå…³ç³»**ï¼ˆé€Ÿåº¦ã€åŠ é€Ÿåº¦ï¼‰ã€‚<br>**2. å¼•å…¥è¿åŠ¨ç‰¹å¼‚æ€§ï¼š** Diffusion-ReTracçš„å½’å› æ˜¯â€œæ•´ä½“â€çš„ï¼Œä¼šæ··æ·†å¤–è§‚å’Œè¿åŠ¨ã€‚Motiveé€šè¿‡**è¿åŠ¨åŠ æƒæŸå¤±æ©ç **ï¼Œä¸»åŠ¨æŠ‘åˆ¶é™æ€èƒŒæ™¯ï¼Œå¼ºè°ƒåŠ¨æ€åŒºåŸŸï¼Œå®ç°äº†**è¿åŠ¨ç‰¹å¼‚æ€§å½’å› **ã€‚<br>**3. å¤„ç†è§†é¢‘ç‰¹æœ‰åå·®ï¼š** Motiveé¢å¤–å¼•å…¥äº†**å¸§é•¿åå·®æ ¡æ­£**ï¼ˆå…¬å¼11ï¼‰ï¼Œç¡®ä¿ä¸åŒæ—¶é•¿è§†é¢‘çš„å½’å› å…¬å¹³æ€§ï¼Œè¿™æ˜¯å›¾åƒå½’å› ä¸­ä¸å­˜åœ¨çš„é—®é¢˜ã€‚ |\n| **ï¼ˆç›¸å…³èƒŒæ™¯å·¥ä½œï¼‰TRAK / TracIn** | 2023/2020 (Park et al. / Pruthi et al.) | ç»å…¸çš„ã€**é’ˆå¯¹åˆ¤åˆ«å¼æ¨¡å‹**ï¼ˆå¦‚åˆ†ç±»å™¨ï¼‰çš„å¯æ‰©å±•æ¢¯åº¦å½’å› æ–¹æ³•ã€‚å®ƒä»¬ä½¿ç”¨æ¢¯åº¦å†…ç§¯æˆ–æŠ•å½±æ¥è¿‘ä¼¼å½±å“å‡½æ•°ï¼Œé¿å…äº†è®¡ç®—é€†æµ·æ£®çŸ©é˜µçš„å·¨å¤§å¼€é”€ã€‚ | **1. ä»åˆ¤åˆ«å¼ä»»åŠ¡åˆ°ç”Ÿæˆå¼ä»»åŠ¡ï¼š** TRAK/TracInä¸»è¦ç”¨äºåˆ†ç±»/å›å½’ä»»åŠ¡ã€‚Motiveå°†å…¶èŒƒå¼**æˆåŠŸè¿ç§»åˆ°å¤§è§„æ¨¡ã€æ¡ä»¶å¼çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹**ï¼ˆæ‰©æ•£/æµåŒ¹é…æ¨¡å‹ï¼‰ã€‚<br>**2. é€‚åº”ç”Ÿæˆå¼è®­ç»ƒçš„åŠ¨æ€æ€§ï¼š** ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒæ¶‰åŠå¯¹æ—¶é—´æ­¥ `t` å’Œå™ªå£° `Ïµ` çš„ç§¯åˆ†ã€‚Motiveå€Ÿé‰´å¹¶ç®€åŒ–äº†Diffusion-ReTracçš„ç­–ç•¥ï¼ˆå¦‚å›ºå®š `t_fix`, `Ïµ_fix`ï¼‰ï¼Œä½¿ç»å…¸æ¢¯åº¦å½’å› æ³•èƒ½é€‚åº”æ‰©æ•£è®­ç»ƒçš„åŠ¨æ€æŸå¤±æ™¯è§‚ã€‚<br>**3. å¢åŠ é¢†åŸŸç‰¹å®šæ¨¡å—ï¼š** åœ¨ç»§æ‰¿äº†TRAKçš„**å¯æ‰©å±•æ¢¯åº¦æŠ•å½±**ï¼ˆFastfoodï¼‰ç­‰é«˜æ•ˆæŠ€å·§çš„åŸºç¡€ä¸Šï¼ŒMotive**æ–°å¢äº†è¿åŠ¨æ„ŸçŸ¥æ¨¡å—**ï¼Œä½¿å…¶ä¸å†æ˜¯ç®€å•çš„â€œè§†é¢‘ç‰ˆTRAKâ€ï¼Œè€Œæ˜¯å…·æœ‰è§†é¢‘è¿åŠ¨è§£é‡Šèƒ½åŠ›çš„ä¸“ç”¨å·¥å…·ã€‚ |\n\n## æ€»ç»“ï¼šæœ¬æ–‡ç›¸å¯¹äºå‰ä½œçš„æ ¸å¿ƒæ”¹è¿›\n\næœ¬æ–‡ã€ŠMotiveã€‹å¹¶éç®€å•åœ°å°†ç°æœ‰æ•°æ®å½’å› æ–¹æ³•åº”ç”¨åˆ°è§†é¢‘é¢†åŸŸï¼Œè€Œæ˜¯è¿›è¡Œäº†ä¸€ç³»åˆ—**æ ¹æœ¬æ€§çš„ã€é’ˆå¯¹è§†é¢‘ç”Ÿæˆç‰¹æ€§çš„åˆ›æ–°**ï¼š\n\n1.  **é¦–åˆ›â€œè¿åŠ¨å½’å› â€é—®é¢˜å®šä¹‰**ï¼šé¦–æ¬¡æå‡ºå¹¶ç³»ç»Ÿæ€§åœ°ç ”ç©¶â€œå“ªäº›è®­ç»ƒè§†é¢‘ç‰‡æ®µå½±å“äº†ç”Ÿæˆè§†é¢‘ä¸­çš„**è¿åŠ¨**â€ï¼Œè€Œéé™æ€å¤–è§‚æˆ–è¯­ä¹‰æ¦‚å¿µã€‚\n2.  **æ ¸å¿ƒæŠ€æœ¯ï¼šè¿åŠ¨åŠ æƒæ¢¯åº¦**ï¼šé€šè¿‡**å…‰æµï¼ˆAllTrackerï¼‰è®¡ç®—è¿åŠ¨å¹…åº¦å›¾ï¼Œå¹¶ä»¥æ­¤ä¸ºæ©ç å¯¹æ‰©æ•£æ¨¡å‹çš„æŸå¤±æ¢¯åº¦è¿›è¡Œç©ºé—´å’Œæ—¶é—´ä¸Šçš„é‡åŠ æƒ**ã€‚è¿™ä½¿å¾—å½’å› ä¿¡å·èšç„¦äºåŠ¨æ€åŒºåŸŸï¼Œå®ç°äº†è¿åŠ¨ä¸å¤–è§‚çš„è§£è€¦ã€‚\n3.  **è§£å†³è§†é¢‘ç‰¹æœ‰æŒ‘æˆ˜**ï¼š\n    - **æ•ˆç‡ä¸å¯æ‰©å±•æ€§**ï¼šç»“åˆå•æ ·æœ¬ä¼°è®¡ã€å…¬å…±éšæœºæ€§ã€FastfoodæŠ•å½±ï¼Œä½¿æ–¹æ³•èƒ½åº”ç”¨äºåäº¿å‚æ•°çº§åˆ«çš„å¤§æ¨¡å‹å’Œé«˜è´¨é‡è§†é¢‘æ•°æ®é›†ã€‚\n    - **å¸§é•¿åå·®æ ¡æ­£**ï¼šå¯¹æ¢¯åº¦è¿›è¡Œå¸§æ•°å½’ä¸€åŒ–ï¼Œé˜²æ­¢é•¿è§†é¢‘ç‰‡æ®µå› æ¢¯åº¦ç´¯ç§¯è€Œè·å¾—è™šå‡çš„é«˜å½’å› åˆ†æ•°ã€‚\n4.  **åº”ç”¨ä»·å€¼å¯¼å‘**ï¼šæ˜ç¡®å°†å½’å› ç»“æœç”¨äº**æŒ‡å¯¼å¾®è°ƒæ•°æ®ç­›é€‰**ã€‚å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨10%çš„ã€ç”±Motiveç­›é€‰å‡ºçš„é«˜å½±å“åŠ›æ•°æ®å¾®è°ƒæ¨¡å‹ï¼Œåœ¨è¿åŠ¨è´¨é‡ä¸Šèƒ½è¾¾åˆ°ç”šè‡³è¶…è¿‡ä½¿ç”¨å…¨éƒ¨æ•°æ®å¾®è°ƒçš„æ•ˆæœï¼Œå¹¶è·å¾—äº†74.1%çš„äººç±»åå¥½èƒœç‡ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡çš„æ ¸å¿ƒè¿›æ­¥åœ¨äºï¼šå°†æ•°æ®å½’å› çš„ç ”ç©¶è§†è§’ä»é™æ€çš„å›¾åƒã€è¯­ä¹‰æ¦‚å¿µï¼Œæ¨è¿›åˆ°äº†åŠ¨æ€çš„è§†é¢‘è¿åŠ¨é¢†åŸŸï¼Œå¹¶ä¸ºæ­¤è®¾è®¡äº†ä¸€å¥—å®Œæ•´ã€é«˜æ•ˆã€ç‰¹åŒ–çš„æŠ€æœ¯æ¡†æ¶ï¼ŒåŒæ—¶å±•ç¤ºäº†å…¶åœ¨æå‡è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿åŠ¨è´¨é‡æ–¹é¢çš„å·¨å¤§å®ç”¨ä»·å€¼ã€‚**",
            "timestamp": "2026-01-20T17:52:45.947536",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨æ·±å…¥åˆ†æè¿™ç¯‡é¢˜ä¸ºã€ŠMotion Attribution for Video Generationã€‹çš„è®ºæ–‡ï¼Œé‡ç‚¹é˜è¿°å…¶æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åˆ†æ\n\næœ¬æ–‡çš„æ ¸å¿ƒè´¡çŒ®æ˜¯æå‡ºäº† **Motive** æ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„**è¿åŠ¨å½’å› **æ–¹æ³•ã€‚å®ƒæ—¨åœ¨è§£å†³ä¸€ä¸ªå…³é”®é—®é¢˜ï¼š**å“ªäº›è®­ç»ƒè§†é¢‘ç‰‡æ®µå½±å“äº†ç”Ÿæˆè§†é¢‘ä¸­çš„è¿åŠ¨æ¨¡å¼ï¼Ÿ** ä¼ ç»Ÿçš„æ•°æ®å½’å› æ–¹æ³•ä¸»è¦å…³æ³¨é™æ€å›¾åƒçš„å¤–è§‚ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†è§†é¢‘ç‰¹æœ‰çš„æ—¶é—´åŠ¨æ€ç‰¹æ€§ã€‚\n\nMotive é€šè¿‡ä¸€ç³»åˆ—æŠ€æœ¯åˆ›æ–°ï¼Œå®ç°äº†å¯¹è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­â€œè¿åŠ¨â€å½±å“çš„ç²¾å‡†ã€é«˜æ•ˆè¿½æº¯ã€‚å…¶ä¸»è¦æŠ€æœ¯æ–¹æ³•å¯æ€»ç»“å¦‚ä¸‹ï¼š\n\n| å…³é”®æŠ€æœ¯æ–¹æ³• | æ ¸å¿ƒç›®æ ‡ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°/å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. åŸºäºæ¢¯åº¦çš„å¯æ‰©å±•å½’å› æ¡†æ¶** | ä½¿å½’å› è®¡ç®—èƒ½å¤Ÿé€‚åº”ç°ä»£å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹å’Œæ•°æ®é›†ï¼Œé¿å…è®¡ç®—æµ·æ£®çŸ©é˜µé€†ç­‰ä¸å¯è¡Œæ“ä½œã€‚ | **åŸç†**ï¼šé‡‡ç”¨**æ¢¯åº¦ç›¸ä¼¼åº¦**è¿‘ä¼¼ä¼ ç»Ÿå½±å“å‡½æ•°ã€‚å¯¹äºæ‰©æ•£æ¨¡å‹ï¼Œåœ¨ç‰¹å®šå™ªå£°æ°´å¹³ `t` å’Œå™ªå£°å®ä¾‹ `Ïµ` ä¸‹ï¼Œè®­ç»ƒæ ·æœ¬ `x_n` å¯¹æµ‹è¯•æ ·æœ¬ `x_test` çš„å½±å“åˆ†æ•°è¿‘ä¼¼ä¸ºå®ƒä»¬æŸå¤±å‡½æ•°æ¢¯åº¦çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼š<br> `I(x_n, x_test) â‰ˆ <âˆ‡Î¸L(Î¸; x_test), âˆ‡Î¸L(Î¸; x_n)>` <br> **æ¨å¯¼ä¸ä¼˜åŒ–**ï¼š<br> 1. **é€†æµ·æ£®çŸ©é˜µè¿‘ä¼¼**ï¼šä½¿ç”¨å•ä½çŸ©é˜µä½œä¸ºé¢„æ¡ä»¶å­ï¼Œé¿å…è®¡ç®— `H_Î¸^{-1}`ã€‚<br> 2. **å…±åŒéšæœºæ€§**ï¼šä¸ºæ‰€æœ‰ï¼ˆè®­ç»ƒï¼Œæµ‹è¯•ï¼‰å¯¹**å›ºå®š**åŒä¸€ä¸ª `(t_fix, Ïµ_fix)` å¯¹è¿›è¡Œæ¢¯åº¦è®¡ç®—ï¼Œæå¤§é™ä½æ–¹å·®ï¼Œç¨³å®šæ’åºã€‚<br> 3. **ç»“æ„åŒ–æŠ•å½±**ï¼šä½¿ç”¨ **Fastfood æŠ•å½±** å°†é«˜ç»´æ¢¯åº¦ (`D â‰ˆ 1.4B`) æŠ•å½±åˆ°ä½ç»´ç©ºé—´ (`D'`)ï¼Œæ˜¾è‘—é™ä½å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ã€‚æŠ•å½±çŸ©é˜µ `P` ç”±ä¸€ç³»åˆ—ç»“æ„åŒ–çŸ©é˜µï¼ˆå“ˆè¾¾ç›å¾·çŸ©é˜µ `Q`ã€å¯¹è§’ç¬¦å·çŸ©é˜µ `B`ã€éšæœºæ’åˆ—çŸ©é˜µ `Î `ã€å¯¹è§’é«˜æ–¯çŸ©é˜µ `G`ã€å¯¹è§’ç¼©æ”¾çŸ©é˜µ `S`ï¼‰ç»„åˆè€Œæˆï¼Œè®¡ç®—å¤æ‚åº¦ä¸º `O(D' log D')`ã€‚ | - `t_fix = 751` (å¯¹åº”1000æ­¥å»å™ªè½¨è¿¹çš„ä¸­ç‚¹)ã€‚<br>- `Ïµ_fix` ä» `N(0, I)` ä¸­é‡‡æ ·ä¸€æ¬¡å¹¶å…±äº«ã€‚<br>- æŠ•å½±ç»´åº¦ `D' = 512` (å®éªŒè¡¨æ˜æ­¤ç»´åº¦åœ¨æ•ˆç‡ä¸æ’åºä¿çœŸåº¦é—´å–å¾—æœ€ä½³å¹³è¡¡ï¼Œä¸å…¨æ¢¯åº¦æ’åºçš„Spearmanç›¸å…³ç³»æ•° Ï â‰ˆ 74.7%)ã€‚<br>- Fastfood æŠ•å½±å®ç°ä¸º `P = (1/(Î¾âˆšD')) * S Q G Î  Q B`ã€‚ |\n| **2. è§†é¢‘ç‰¹å®šçš„å¸§é•¿åå·®ä¿®æ­£** | æ¶ˆé™¤å› è§†é¢‘ç‰‡æ®µå¸§æ•° (`F`) ä¸åŒå¯¼è‡´çš„æ¢¯åº¦å¹…å€¼åå·®ï¼Œç¡®ä¿ä¸åŒæ—¶é•¿è§†é¢‘é—´çš„å…¬å¹³æ¯”è¾ƒã€‚ | **åŸç†**ï¼šåŸå§‹æ¢¯åº¦ `âˆ‡Î¸L` çš„èŒƒæ•°é€šå¸¸ä¸è§†é¢‘å¸§æ•° `F` æˆæ­£æ¯”ã€‚æ›´é•¿çš„è§†é¢‘ä¼šäº§ç”Ÿæ›´å¤§çš„æ¢¯åº¦ï¼Œä»è€Œåœ¨å½’å› åˆ†æ•°ä¸­å æ®ä¸å…¬å¹³çš„ä¼˜åŠ¿ã€‚<br> **æ–¹æ³•**ï¼šåœ¨è®¡ç®—æ¢¯åº¦åã€è¿›è¡ŒæŠ•å½±å’Œå½’ä¸€åŒ–ä¹‹å‰ï¼Œå¯¹æ¢¯åº¦è¿›è¡Œ**å¸§æ•°å½’ä¸€åŒ–**ï¼š<br> `âˆ‡Î¸L_corrected = (1/F) * âˆ‡Î¸L` <br> è¿™ç¡®ä¿äº†å½’å› åˆ†æ•°åæ˜ çš„æ˜¯**æ¯å¸§çš„å¹³å‡å½±å“**ï¼Œè€Œéè§†é¢‘çš„æ€»é•¿åº¦ã€‚ | å½’ä¸€åŒ–å› å­ä¸º `1/F`ï¼Œå…¶ä¸­ `F` æ˜¯æ¯ä¸ªè§†é¢‘ç‰‡æ®µçš„å®é™…å¸§æ•°ã€‚åœ¨å®éªŒä¸­ï¼Œæ‰€æœ‰è§†é¢‘è¢«é¢„å¤„ç†ä¸ºæ ‡å‡†åŒ–çš„81å¸§ï¼Œè¿›ä¸€æ­¥ç¡®ä¿äº†å…¬å¹³æ€§ã€‚ |\n| **3. è¿åŠ¨æ„ŸçŸ¥çš„åŠ æƒå½’å› ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰** | å°†å½’å› ä¿¡å·ä»**æ•´ä½“å¤–è§‚**èšç„¦åˆ°**æ—¶é—´åŠ¨æ€**ä¸Šï¼ŒçœŸæ­£å®ç°â€œè¿åŠ¨â€å½’å› ã€‚ | **åŸç†**ï¼šé€šè¿‡**è¿åŠ¨åŠ æƒæŸå¤±æ©ç **ï¼Œåœ¨è®¡ç®—æ¢¯åº¦æ—¶ï¼Œå¯¹è§†é¢‘ä¸­åŠ¨æ€åŒºåŸŸçš„æŸå¤±èµ‹äºˆæ›´é«˜æƒé‡ï¼Œå¯¹é™æ€èƒŒæ™¯åŒºåŸŸèµ‹äºˆæ›´ä½æƒé‡ã€‚<br> **å…·ä½“æ­¥éª¤**ï¼š<br> 1. **è¿åŠ¨æå–**ï¼šä½¿ç”¨ **AllTracker** ä¸ºè§†é¢‘ `v` è®¡ç®—æ¯å¸§æ¯ä¸ªåƒç´ çš„å…‰æµä½ç§»å‘é‡ `D_f(h,w)` å’Œè¿åŠ¨å¹…åº¦ `M_f(h,w) = ||D_f(h,w)||_2`ã€‚<br> 2. **æ©ç ç”Ÿæˆ**ï¼šå°†è¿åŠ¨å¹…åº¦å½’ä¸€åŒ–åˆ° `[0,1]` åŒºé—´ï¼Œå¾—åˆ°åƒç´ çº§æƒé‡ `W(f,h,w)`ã€‚ç„¶åé€šè¿‡åŒçº¿æ€§ä¸‹é‡‡æ ·å°†å…¶å¯¹é½åˆ°VAEæ½œåœ¨ç©ºé—´ç½‘æ ¼ `(H/s, W/s)`ï¼Œå¾—åˆ°æ½œåœ¨ç©ºé—´å¯¹é½çš„æƒé‡ `Å´(f, Ä¥, Åµ)`ã€‚<br> 3. **è¿åŠ¨åŠ æƒæŸå¤±**ï¼šå®šä¹‰åŸºäºè¿åŠ¨æƒé‡çš„é€ä½ç½®æŸå¤±ã€‚å¯¹äºå›ºå®šçš„ `(t_fix, Ïµ_fix)`ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä½ç½® `(f, Ä¥, Åµ)` çš„æŸå¤±ä¸ºé¢„æµ‹å™ªå£°ä¸ç›®æ ‡å™ªå£°çš„å¹³æ–¹è¯¯å·®ã€‚è¿åŠ¨åŠ æƒæŸå¤±æ˜¯è¿™äº›é€ä½ç½®æŸå¤±çš„åŠ æƒå¹³å‡ï¼š<br> `L_mot(Î¸; v, c) = (1/F) * mean_{f, Ä¥, Åµ} [ Å´(f, Ä¥, Åµ) * LÌƒ_{Î¸,v,c}(f, Ä¥, Åµ) ]` <br> 4. **è¿åŠ¨æ„ŸçŸ¥æ¢¯åº¦**ï¼šè®¡ç®—è¯¥è¿åŠ¨åŠ æƒæŸå¤±çš„æ¢¯åº¦ `g_mot = âˆ‡Î¸ L_mot`ï¼Œå¹¶ç”¨å…¶æ›¿ä»£åŸå§‹æ¢¯åº¦è¿›è¡Œåç»­çš„æŠ•å½±å’Œç›¸ä¼¼åº¦è®¡ç®—ã€‚ | - ä½¿ç”¨ **AllTracker** æå–å…‰æµï¼ˆ4é€šé“ï¼šdx, dy, å¯è§æ€§ï¼Œç½®ä¿¡åº¦ï¼‰ã€‚<br>- è¿åŠ¨å¹…åº¦å½’ä¸€åŒ–æ—¶æ·»åŠ å°å¸¸æ•° `Î¶ = 10^{-6}` é˜²æ­¢é™¤é›¶ã€‚<br>- VAEä¸‹é‡‡æ ·å› å­ `s = 8` (éµå¾ªWan2.1éª¨å¹²ç½‘ç»œ)ã€‚<br>- æŸå¤±åŠ æƒåœ¨**æ½œåœ¨ç©ºé—´**è¿›è¡Œï¼Œä¸æ¨¡å‹å‰å‘è¿‡ç¨‹è§£è€¦ï¼Œä»…å½±å“å½’å› è®¡ç®—ã€‚ |\n| **4. åŸºäºå½’å› çš„å¾®è°ƒæ•°æ®é€‰æ‹©** | åˆ©ç”¨è¿åŠ¨å½’å› åˆ†æ•°ï¼Œä»æµ·é‡æ•°æ®ä¸­æ™ºèƒ½ç­›é€‰å‡ºå¯¹ç›®æ ‡è¿åŠ¨æ¨¡å¼æœ€æœ‰å½±å“åŠ›çš„å­é›†è¿›è¡Œé«˜æ•ˆå¾®è°ƒã€‚ | **åŸç†**ï¼šå¯¹äºç»™å®šçš„ç›®æ ‡è¿åŠ¨æŸ¥è¯¢è§†é¢‘ `vÌ‚`ï¼Œè®¡ç®—æ•°æ®é›†ä¸­æ¯ä¸ªè®­ç»ƒæ ·æœ¬ `v_n` çš„è¿åŠ¨æ„ŸçŸ¥å½±å“åˆ†æ•° `I_mot(v_n, vÌ‚)`ã€‚<br> **é€‰æ‹©ç­–ç•¥**ï¼š<br> - **å•æŸ¥è¯¢**ï¼šç›´æ¥é€‰æ‹©åˆ†æ•°æœ€é«˜çš„ Top-K ä¸ªæ ·æœ¬ã€‚<br> - **å¤šæŸ¥è¯¢**ï¼ˆæ›´é²æ£’ï¼‰ï¼šé‡‡ç”¨**å¤šæ•°æŠ•ç¥¨æ³•**ã€‚å¯¹äºå¤šä¸ªæŸ¥è¯¢ `{vÌ‚_q}`ï¼Œæ¯ä¸ªè®­ç»ƒæ ·æœ¬ `v_n` è·å¾—ä¸€ä¸ªâ€œç¥¨æ•°â€ï¼Œå³å…¶åˆ†æ•°è¶…è¿‡æŸä¸ªç™¾åˆ†ä½é˜ˆå€¼ `Ï„` çš„æŸ¥è¯¢æ•°é‡ã€‚æœ€ç»ˆé€‰æ‹©å¾—ç¥¨æ•°æœ€é«˜çš„ Top-K ä¸ªæ ·æœ¬ï¼š<br> `MajVote_n = Î£_{q=1}^Q I[ I_mot(v_n, vÌ‚_q) > Ï„ ]` <br> `S_vote(K) = {v_n | v_n æ˜¯æŒ‰ MajVote æ’åçš„å‰ K å}` | - åœ¨ä¸»è¦å®éªŒä¸­ï¼Œ`K` è®¾ç½®ä¸ºæ•°æ®é›†çš„ **10%**ã€‚<br> - å¤šæ•°æŠ•ç¥¨æ³•é¿å…äº†ä¸åŒæŸ¥è¯¢é—´åˆ†æ•°å°ºåº¦çš„æ ¡å‡†é—®é¢˜ã€‚<br> - è¯¥æ–¹æ³•èƒ½å¤Ÿç­›é€‰å‡ºåœ¨å¤šç§ç›®æ ‡è¿åŠ¨ä¸Šéƒ½è¡¨ç°å‡ºé«˜å½±å“åŠ›çš„â€œé€šç”¨â€é«˜è´¨é‡è¿åŠ¨æ•°æ®ã€‚ |\n\n## å®é™…ä»·å€¼ä¸æ„ä¹‰\n\n1.  **å¯è§£é‡Šæ€§å·¥å…·**ï¼šé¦–æ¬¡ä¸ºè§†é¢‘ç”Ÿæˆæ¨¡å‹æä¾›äº†â€œè¿åŠ¨å±‚é¢â€çš„å¯è§£é‡Šæ€§å·¥å…·ï¼Œä½¿ç ”ç©¶è€…èƒ½å¤Ÿç†è§£æ¨¡å‹ä¸ºä½•ä»¥æŸç§æ–¹å¼è¿åŠ¨ï¼Œå¹¶è¯Šæ–­è¿åŠ¨ä¼ªå½±ï¼ˆå¦‚è½¨è¿¹ä¸ä¸€è‡´ã€ç‰©ç†ä¸åˆç†ï¼‰çš„æ¥æºã€‚\n2.  **é«˜æ•ˆæ•°æ®æ²»ç†**ï¼šæä¾›äº†ä¸€ç§æ•°æ®é©±åŠ¨çš„ã€ç›®æ ‡æ˜ç¡®çš„å¾®è°ƒæ•°æ®ç­›é€‰æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨ **10%** ç”± Motive ç­›é€‰çš„é«˜å½±å“åŠ›æ•°æ®å¾®è°ƒæ¨¡å‹ï¼Œåœ¨è¿åŠ¨å¹³æ»‘åº¦å’ŒåŠ¨æ€ç¨‹åº¦ç­‰æŒ‡æ ‡ä¸Šå³å¯è¾¾åˆ°æˆ–è¶…è¶Šä½¿ç”¨å…¨éƒ¨æ•°æ®å¾®è°ƒçš„æ•ˆæœï¼Œå¹¶åœ¨äººç±»è¯„ä¼°ä¸­è·å¾—äº† **74.1%** çš„åå¥½èƒœç‡ã€‚\n3.  **è®¡ç®—å¯è¡Œæ€§**ï¼šé€šè¿‡å•æ ·æœ¬ä¼°è®¡ã€å…±åŒéšæœºæ€§å’Œç»“æ„åŒ–æŠ•å½±ç­‰ä¼˜åŒ–ï¼Œä½¿å¾—å¯¹åäº¿å‚æ•°çº§åˆ«æ¨¡å‹å’Œæ•°ä¸‡è§„æ¨¡æ•°æ®é›†è¿›è¡Œå½’å› è®¡ç®—å˜å¾—å¯è¡Œï¼ˆè™½ä»æœ‰æˆæœ¬ï¼Œä½†å·²ä»â€œä¸å¯è¡Œâ€å˜ä¸ºâ€œå¯ç®¡ç†â€ï¼‰ã€‚\n4.  **åŸºç¡€ç ”ç©¶è´¡çŒ®**ï¼šéªŒè¯äº†â€œè¿åŠ¨å­¦ä¹ å¯è¿½æº¯è‡³å…·ä½“è®­ç»ƒæ ·æœ¬â€è¿™ä¸€å‡è®¾ï¼Œä¸ºä»æ•°æ®è§’åº¦ç†è§£å’Œæ§åˆ¶ç”Ÿæˆæ¨¡å‹çš„åŠ¨æ€è¡Œä¸ºå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚\n\n**æ€»ç»“**ï¼šMotive çš„æ ¸å¿ƒæŠ€æœ¯çªç ´åœ¨äºå°†ä¼ ç»Ÿçš„æ•°æ®å½’å› æ€æƒ³ï¼Œé€šè¿‡**è¿åŠ¨åŠ æƒæŸå¤±æ©ç **ã€**è§†é¢‘ç‰¹å®šåå·®ä¿®æ­£**å’Œ**å¯æ‰©å±•çš„æ¢¯åº¦è¿‘ä¼¼**ç­‰ä¸€ç³»åˆ—ç²¾å¿ƒè®¾è®¡ï¼ŒæˆåŠŸåœ°é€‚é…åˆ°äº†é«˜ç»´ã€æ—¶åºçš„è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå®ç°äº†ä»â€œå¤–è§‚å½’å› â€åˆ°â€œè¿åŠ¨å½’å› â€çš„èŒƒå¼è½¬å˜ï¼Œå…¼å…·é‡è¦çš„ç†è®ºæ„ä¹‰å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚",
            "timestamp": "2026-01-20T17:53:50.524798",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡ã€ŠMotion Attribution for Video Generationã€‹çš„è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä½¿ç”¨äº†**è‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•ï¼ˆVBenchï¼‰**å’Œ**äººå·¥è¯„ä¼°**ä¸¤ç§æ–¹å¼ï¼Œå…¨é¢è¡¡é‡æ¨¡å‹åœ¨è¿åŠ¨ç”Ÿæˆè´¨é‡ä¸Šçš„è¡¨ç°ã€‚\n\n#### 1. VBench è‡ªåŠ¨åŒ–è¯„ä¼°\nVBench æ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„è§†é¢‘ç”Ÿæˆè¯„ä¼°åŸºå‡†ã€‚è®ºæ–‡é‡ç‚¹å…³æ³¨äº†ä»¥ä¸‹å…­ä¸ªç»´åº¦ï¼š\n\n| æŒ‡æ ‡ | è‹±æ–‡å…¨ç§° | æ ¸å¿ƒå®šä¹‰ä¸æµ‹é‡ç»†èŠ‚ |\n| :--- | :--- | :--- |\n| **ä¸»ä½“ä¸€è‡´æ€§** | Subject Consistency | è¡¡é‡ç”Ÿæˆè§†é¢‘ä¸­ä¸»è¦ç‰©ä½“ï¼ˆå¦‚äººç‰©ã€åŠ¨ç‰©ï¼‰åœ¨æ—¶é—´ç»´åº¦ä¸Šçš„èº«ä»½ã€å½¢çŠ¶å’Œå¤–è§‚æ˜¯å¦ä¿æŒç¨³å®šï¼Œé¿å…â€œé—ªçƒâ€æˆ–â€œèº«ä»½æ¼‚ç§»â€ã€‚é€šå¸¸é€šè¿‡è·¨å¸§çš„ç‰¹å¾ç›¸ä¼¼åº¦è®¡ç®—ã€‚ |\n| **èƒŒæ™¯ä¸€è‡´æ€§** | Background Consistency | è¯„ä¼°è§†é¢‘é™æ€èƒŒæ™¯çš„ç¨³å®šæ€§ï¼Œé¿å…ä¸åˆç†çš„æŠ–åŠ¨ã€çªå˜æˆ–æ‰­æ›²ã€‚å…³æ³¨éè¿åŠ¨åŒºåŸŸçš„æ—¶åºè¿è´¯æ€§ã€‚ |\n| **è¿åŠ¨å¹³æ»‘åº¦** | Motion Smoothness | é‡åŒ–ç‰©ä½“æˆ–ç›¸æœºè¿åŠ¨è½¨è¿¹çš„æµç•…æ€§ã€‚é€šè¿‡åˆ†æå…‰æµæˆ–ç‰¹å¾ç‚¹çš„è½¨è¿¹ï¼Œæ£€æµ‹æ˜¯å¦å­˜åœ¨ç”Ÿç¡¬çš„è·³è·ƒã€å¡é¡¿æˆ–ä¸è‡ªç„¶çš„åŠ é€Ÿåº¦å˜åŒ–ã€‚ |\n| **åŠ¨æ€ç¨‹åº¦** | Dynamic Degree | **è®ºæ–‡çš„æ ¸å¿ƒå…³æ³¨æŒ‡æ ‡**ã€‚è¡¡é‡è§†é¢‘ä¸­â€œå‘ç”Ÿäº†å¤šå°‘è¿åŠ¨â€ã€‚é€šè¿‡è®¡ç®—æ•´ä¸ªè§†é¢‘åºåˆ—ä¸­å…‰æµå¹…å€¼çš„ç»Ÿè®¡é‡ï¼ˆå¦‚å‡å€¼ã€æ–¹å·®ï¼‰æ¥è¯„ä¼°è¿åŠ¨çš„ä¸°å¯Œæ€§å’Œå¼ºåº¦ã€‚é«˜åˆ†è¡¨ç¤ºè§†é¢‘å……æ»¡åŠ¨æ€å˜åŒ–ã€‚ |\n| **ç¾å­¦è´¨é‡** | Aesthetic Quality | è¯„ä¼°è§†é¢‘å¸§çš„è§†è§‰ç¾æ„Ÿï¼Œå¦‚æ„å›¾ã€è‰²å½©ã€å…‰ç…§ç­‰ã€‚é€šå¸¸ä½¿ç”¨é¢„è®­ç»ƒçš„ç¾å­¦è¯„åˆ†æ¨¡å‹å¯¹é‡‡æ ·å¸§è¿›è¡Œæ‰“åˆ†å¹¶å–å¹³å‡ã€‚ |\n| **æˆåƒè´¨é‡** | Imaging Quality | è¡¡é‡å•ä¸ªè§†é¢‘å¸§çš„æ¸…æ™°åº¦ã€é”åˆ©åº¦å’Œä½å™ªå£°ç¨‹åº¦ã€‚å¸¸ç”¨æŒ‡æ ‡åŒ…æ‹¬**FIDï¼ˆFrÃ©chet Inception Distanceï¼‰**æˆ–**CLIP Score**ï¼Œä¸é«˜è´¨é‡å‚è€ƒå›¾åƒåˆ†å¸ƒè¿›è¡Œæ¯”è¾ƒã€‚ |\n\n**è®ºæ–‡ä¾§é‡ç‚¹**ï¼šä½œè€…æ˜ç¡®æŒ‡å‡ºï¼Œ**è¿åŠ¨å¹³æ»‘åº¦ (Motion Smoothness)** å’Œ **åŠ¨æ€ç¨‹åº¦ (Dynamic Degree)** æ˜¯è¯„ä¼°å…¶**è¿åŠ¨å½’å› æ¡†æ¶ (Motive)** æˆæ•ˆçš„**ä¸»è¦ç›®æ ‡**ã€‚å…¶ä»–æŒ‡æ ‡ç”¨äºç¡®ä¿åœ¨æå‡è¿åŠ¨è´¨é‡çš„åŒæ—¶ï¼Œä¸æŸå®³åŸºæœ¬çš„è§†è§‰è´¨é‡å’Œä¸€è‡´æ€§ã€‚\n\n#### 2. äººå·¥è¯„ä¼° (Human Evaluation)\nç”±äºè‡ªåŠ¨åŒ–æŒ‡æ ‡å¯èƒ½æ— æ³•å®Œå…¨æ•æ‰äººç±»å¯¹è¿åŠ¨è´¨é‡çš„æ„ŸçŸ¥ï¼Œè®ºæ–‡è¿›è¡Œäº†ä¸¥è°¨çš„äººå·¥ pairwise æ¯”è¾ƒã€‚\n\n- **æ–¹æ³•**ï¼šè®©å‚ä¸è€…è§‚çœ‹åŒä¸€æç¤ºè¯ä¸‹ã€ç”±ä¸åŒæ–¹æ³•ç”Ÿæˆçš„ä¸¤æ®µè§†é¢‘ï¼Œé€‰æ‹©å“ªä¸€æ®µçš„**è¿åŠ¨æ›´ä¼˜**ï¼ˆå…è®¸å¹³å±€ï¼‰ã€‚\n- **è§„æ¨¡**ï¼š17 ä½æ ‡æ³¨å‘˜ï¼Œå¯¹ 10 ç±»è¿åŠ¨å„ 5 ä¸ªæµ‹è¯•æ¡ˆä¾‹ï¼ˆå…± 50 ä¸ªè§†é¢‘ï¼‰è¿›è¡Œå¤šè½®ä¸¤ä¸¤æ¯”è¾ƒï¼Œæ€»è®¡ 850 æ¬¡åˆ¤æ–­ã€‚\n- **æŠ¥å‘ŠæŒ‡æ ‡**ï¼š\n    - **èƒœç‡ (Win Rate)**ï¼šæœ¬æ–‡æ–¹æ³•è¢«åå¥½çš„æ¯”ä¾‹ã€‚\n    - **å¹³å±€ç‡ (Tie Rate)**ï¼šä¸¤è€…è¢«è®¤ä¸ºè´¨é‡ç›¸å½“çš„æ¯”ä¾‹ã€‚\n    - **è´Ÿç‡ (Loss Rate)**ï¼šåŸºçº¿æ–¹æ³•è¢«åå¥½çš„æ¯”ä¾‹ã€‚\n- **ç›®çš„**ï¼šæä¾›**æ„ŸçŸ¥å±‚é¢**çš„è¿åŠ¨è´¨é‡éªŒè¯ï¼Œæ˜¯è‡ªåŠ¨åŒ–æŒ‡æ ‡çš„é‡è¦è¡¥å……ã€‚\n\n### äºŒã€ ä½¿ç”¨çš„æ•°æ®é›†\n\nè®ºæ–‡åœ¨å¾®è°ƒï¼ˆfine-tuningï¼‰å’Œè¯„ä¼°é˜¶æ®µä½¿ç”¨äº†å¤šä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è§†é¢‘æ•°æ®é›†ã€‚\n\n#### 1. å¾®è°ƒæ•°æ®é›† (Fine-tuning Datasets)\nç”¨äºéªŒè¯ `Motive` æ¡†æ¶ç­›é€‰å‡ºçš„æ•°æ®å­é›†è¿›è¡Œå¾®è°ƒçš„æ•ˆæœã€‚\n\n| æ•°æ®é›† | è§„æ¨¡ï¼ˆè®ºæ–‡æ‰€ç”¨å­é›†ï¼‰ | ç‰¹ç‚¹ä¸ç»†èŠ‚ |\n| :--- | :--- | :--- |\n| **VIDGEN-1M** | 10k è§†é¢‘ | å¤§è§„æ¨¡æ–‡æœ¬-è§†é¢‘å¯¹æ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–çš„è¿åŠ¨æ¨¡å¼ã€å¤æ‚çš„åœºæ™¯å’Œä¸°å¯Œçš„æ—¶åºåŠ¨æ€ã€‚ç”¨äºéªŒè¯æ–¹æ³•åœ¨é€šç”¨è§†é¢‘ç”Ÿæˆæ•°æ®ä¸Šçš„æœ‰æ•ˆæ€§ã€‚ |\n| **4DNeX-10M** | 10k è§†é¢‘ | å¦ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è§†é¢‘æ•°æ®é›†ï¼ŒåŒæ ·æä¾›ä¸°å¯Œçš„æ—¶ç©ºåŠ¨æ€å’Œå¤æ‚åœºæ™¯ã€‚ç”¨äºæµ‹è¯•æ–¹æ³•çš„æ³›åŒ–æ€§å’Œåœ¨ä¸åŒæ•°æ®åˆ†å¸ƒä¸Šçš„è¡¨ç°ã€‚ |\n\n**ä½¿ç”¨æ–¹å¼**ï¼šè®ºæ–‡ä»è¿™ä¸¤ä¸ªæ•°æ®é›†ä¸­å„æŠ½å– 10k è§†é¢‘ä½œä¸ºå¾®è°ƒå€™é€‰æ± ã€‚`Motive` æ¡†æ¶çš„ç›®æ ‡æ˜¯ä»ä¸­ç­›é€‰å‡ºå¯¹ç›®æ ‡è¿åŠ¨æœ€æœ‰å½±å“åŠ›çš„å­é›†ï¼ˆä¾‹å¦‚ top 10%ï¼‰ç”¨äºå¾®è°ƒï¼Œå¹¶ä¸ä½¿ç”¨å…¨æ•°æ®é›†æˆ–éšæœºå­é›†å¾®è°ƒçš„æ•ˆæœè¿›è¡Œå¯¹æ¯”ã€‚\n\n#### 2. æŸ¥è¯¢æ•°æ®é›† (Query Datasets)\nç”¨äºé©±åŠ¨ `Motive` è¿›è¡Œå½’å› è®¡ç®—ï¼Œå³â€œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹å­¦ä¼šç”Ÿæˆå“ªç§è¿åŠ¨â€ã€‚\n\n- **æ„æˆ**ï¼šä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„**åˆæˆè§†é¢‘é›†**ï¼ŒåŒ…å« **10 ç±»æ˜ç¡®çš„ç‰©ä½“è¿åŠ¨**ï¼š\n    `compress`ï¼ˆå‹ç¼©ï¼‰, `bounce`ï¼ˆå¼¹è·³ï¼‰, `roll`ï¼ˆæ»šåŠ¨ï¼‰, `explode`ï¼ˆçˆ†ç‚¸ï¼‰, `float`ï¼ˆæ¼‚æµ®ï¼‰, `free fall`ï¼ˆè‡ªç”±è½ä½“ï¼‰, `slide`ï¼ˆæ»‘åŠ¨ï¼‰, `spin`ï¼ˆæ—‹è½¬ï¼‰, `stretch`ï¼ˆæ‹‰ä¼¸ï¼‰, `swing`ï¼ˆæ‘†åŠ¨ï¼‰ã€‚\n- **è§„æ¨¡**ï¼šæ¯ç±»è¿åŠ¨ 5 ä¸ªè§†é¢‘ï¼Œå…± **50 ä¸ªæŸ¥è¯¢è§†é¢‘**ã€‚\n- **ç”Ÿæˆä¸ç­›é€‰**ï¼šä½¿ç”¨ **Veo-3** æ¨¡å‹æ ¹æ®æè¿°æ€§æ–‡æœ¬æç¤ºç”Ÿæˆï¼Œå¹¶ç»è¿‡äººå·¥ç­›é€‰ï¼Œç¡®ä¿è¿åŠ¨æ¸…æ™°ã€ç‰©ç†åˆç†ã€èƒŒæ™¯å¹²å‡€ï¼Œä»¥**éš”ç¦»å‡ºç‰¹å®šçš„è¿åŠ¨åŸºå…ƒ**ã€‚\n- **ä½œç”¨**ï¼šè¿™äº›æŸ¥è¯¢è§†é¢‘ä¸ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œè€Œæ˜¯ä½œä¸ºå½’å› åˆ†æçš„â€œæµ‹è¯•ç‚¹â€æˆ–â€œç›®æ ‡â€ï¼Œç”¨äºè®¡ç®—è®­ç»ƒé›†ä¸­æ¯ä¸ªè§†é¢‘å¯¹è¯¥ç±»è¿åŠ¨çš„å½±å“åˆ†æ•°ã€‚\n\n#### 3. æµ‹è¯•æç¤ºé›† (Evaluation Prompts)\nç”¨äºåœ¨å¾®è°ƒå**å®šé‡è¯„ä¼°**æ¨¡å‹ç”Ÿæˆèƒ½åŠ›çš„æ–‡æœ¬æç¤ºé›†åˆã€‚\n\n- **æ„æˆ**ï¼šä¸æŸ¥è¯¢æ•°æ®é›†å¯¹åº”çš„ **10 ç±»è¿åŠ¨**ï¼Œä½†ä½¿ç”¨**ä¸åŒçš„ç‰©ä½“å’Œåœºæ™¯æè¿°**ï¼Œä»¥é¿å…è¿‡æ‹Ÿåˆã€‚\n- **è§„æ¨¡**ï¼šæ¯ç±»è¿åŠ¨ 5 ä¸ªæç¤ºè¯ï¼Œå…± **50 ä¸ªè¯„ä¼°æç¤º**ã€‚\n- **ç¤ºä¾‹**ï¼š\n    - æŸ¥è¯¢é›†æç¤ºï¼ˆç”¨äºç”ŸæˆæŸ¥è¯¢è§†é¢‘ï¼‰ï¼šâ€œä¸€ä¸ªä¹’ä¹“çƒåœ¨ç™½è‰²æ¡Œé¢ä¸Šå¼¹è·³...â€\n    - æµ‹è¯•é›†æç¤ºï¼ˆç”¨äºè¯„ä¼°æ¨¡å‹ï¼‰ï¼šâ€œä¸€ä¸ªç¯®çƒåœ¨æœ¨æ¿åœ°æ¿ä¸Šå‚ç›´å¼¹è·³...â€\n- **ä½œç”¨**ï¼šå°†è¿™äº›æç¤ºè¾“å…¥ç»™å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆè§†é¢‘ï¼Œç„¶åç”¨ VBench å¯¹ç”Ÿæˆçš„è§†é¢‘è¿›è¡Œè‡ªåŠ¨åŒ–è¯„åˆ†ï¼Œä»è€Œå®¢è§‚æ¯”è¾ƒä¸åŒå¾®è°ƒç­–ç•¥çš„ä¼˜åŠ£ã€‚\n\n### æ€»ç»“è¡¨æ ¼ï¼šè¯„ä¼°ä½“ç³»æ¦‚è§ˆ\n\n| è¯„ä¼°ç±»å‹ | å…·ä½“é¡¹ç›® | ç”¨é€”ä¸ç›®æ ‡ |\n| :--- | :--- | :--- |\n| **è‡ªåŠ¨åŒ–æŒ‡æ ‡ (VBench)** | ä¸»ä½“/èƒŒæ™¯ä¸€è‡´æ€§ã€è¿åŠ¨å¹³æ»‘åº¦ã€**åŠ¨æ€ç¨‹åº¦**ã€ç¾å­¦/æˆåƒè´¨é‡ | å®¢è§‚ã€å¯é‡å¤åœ°è¡¡é‡è§†é¢‘åœ¨å¤šç»´åº¦ä¸Šçš„è´¨é‡ï¼Œ**é‡ç‚¹å…³æ³¨è¿åŠ¨ç›¸å…³æŒ‡æ ‡**ã€‚ |\n| **äººå·¥è¯„ä¼°** | Pairwise æ¯”è¾ƒèƒœ/å¹³/è´Ÿç‡ | æä¾›äººç±»å¯¹è¿åŠ¨è´¨é‡çš„æ„ŸçŸ¥åˆ¤æ–­ï¼ŒéªŒè¯è‡ªåŠ¨åŒ–æŒ‡æ ‡çš„ç»“è®ºã€‚ |\n| **å¾®è°ƒæ•°æ®é›†** | VIDGEN-1M, 4DNeX-10M (å„10kå­é›†) | æä¾›å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å¾®è°ƒæ•°æ®æ± ï¼Œç”¨äºéªŒè¯æ•°æ®ç­›é€‰çš„æœ‰æ•ˆæ€§ã€‚ |\n| **æŸ¥è¯¢æ•°æ®é›†** | 10ç±»è¿åŠ¨ Ã— 5è§†é¢‘ (å…±50ä¸ªï¼Œç”±Veo-3åˆæˆ) | ä½œä¸ºå½’å› åˆ†æçš„ç›®æ ‡ï¼Œé©±åŠ¨ `Motive` æ¡†æ¶è¯†åˆ«å‡ºå¯¹ç‰¹å®šè¿åŠ¨æœ‰å½±å“åŠ›çš„è®­ç»ƒæ•°æ®ã€‚ |\n| **æµ‹è¯•æç¤ºé›†** | 10ç±»è¿åŠ¨ Ã— 5æç¤º (å…±50ä¸ªï¼Œä¸æŸ¥è¯¢é›†è¯­ä¹‰å¯¹åº”ä½†æè¿°ä¸åŒ) | ç”¨äºåœ¨æ¨¡å‹å¾®è°ƒåï¼Œç”Ÿæˆè§†é¢‘å¹¶è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œç¡®ä¿è¯„ä¼°çš„æ³›åŒ–æ€§ã€‚ |\n\n**æ ¸å¿ƒç»“è®º**ï¼šè®ºæ–‡æ„å»ºäº†ä¸€å¥—ä»**ç›®æ ‡å®šä¹‰ï¼ˆæŸ¥è¯¢è§†é¢‘ï¼‰** -> **æ•°æ®å½’å› ä¸ç­›é€‰ï¼ˆMotiveï¼‰** -> **æ¨¡å‹å¾®è°ƒ** -> **å¤šç»´åº¦è¯„ä¼°ï¼ˆè‡ªåŠ¨åŒ–+äººå·¥ï¼‰** çš„å®Œæ•´å®éªŒé—­ç¯ã€‚å…¶è¯„ä»·æŒ‡æ ‡ç´§å¯†å›´ç»• **â€œè¿åŠ¨è´¨é‡â€** è¿™ä¸€æ ¸å¿ƒï¼ŒåŒæ—¶å…¼é¡¾äº†è§†è§‰ä¿çœŸåº¦ï¼Œæœ‰åŠ›åœ°è¯æ˜äº† `Motive` æ¡†æ¶åœ¨ç†è§£å’Œæå‡è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿åŠ¨åŠ¨åŠ›å­¦æ–¹é¢çš„ä»·å€¼ã€‚",
            "timestamp": "2026-01-20T17:54:46.687323",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒä¸è¯„ä¼°åˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ä¸¤ä¸ªå¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼š\n\n- **VIDGEN-1M** (Tan et al., 2024)\n- **4DNeX-10M** (Chen et al., 2025)\n\nå®éªŒä¸­ï¼Œä»æ¯ä¸ªæ•°æ®é›†ä¸­æŠ½å–äº† **10k ä¸ªè§†é¢‘** ä½œä¸ºå¾®è°ƒæ•°æ®é›†ï¼Œä»¥ç¡®ä¿å®éªŒçš„è§„æ¨¡å’Œå¤šæ ·æ€§ã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nä½¿ç”¨ **VBench** (Huang et al., 2024) åŸºå‡†è¿›è¡Œé‡åŒ–è¯„ä¼°ï¼Œé‡ç‚¹å…³æ³¨ä»¥ä¸‹å…­ä¸ªç»´åº¦ï¼š\n\n1.  **ä¸»ä½“ä¸€è‡´æ€§** (Subject Consistency)\n2.  **èƒŒæ™¯ä¸€è‡´æ€§** (Background Consistency)\n3.  **è¿åŠ¨å¹³æ»‘åº¦** (Motion Smoothness)\n4.  **åŠ¨æ€ç¨‹åº¦** (Dynamic Degree) - **æ ¸å¿ƒæŒ‡æ ‡**\n5.  **ç¾å­¦è´¨é‡** (Aesthetic Quality)\n6.  **æˆåƒè´¨é‡** (Imaging Quality)\n\nå…¶ä¸­ï¼Œ**è¿åŠ¨å¹³æ»‘åº¦** å’Œ **åŠ¨æ€ç¨‹åº¦** æ˜¯è¯„ä¼°æ—¶é—´åŠ¨æ€ï¼ˆå³è¿åŠ¨è´¨é‡ï¼‰çš„ä¸»è¦ç›®æ ‡æŒ‡æ ‡ã€‚\n\n### ä¸‰ã€æ€§èƒ½æå‡ä¸ä¸»è¦ç»“æœ\n\n#### 1. å®šé‡ç»“æœ (VBench)\nåœ¨ä»…ä½¿ç”¨ **10%** çš„å¾®è°ƒæ•°æ®ï¼ˆç”± **Motive** æ–¹æ³•é€‰æ‹©ï¼‰çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€§èƒ½è¡¨ç°å¦‚ä¸‹ï¼ˆä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”ï¼‰ï¼š\n\n| æ–¹æ³• | åŠ¨æ€ç¨‹åº¦ (Dynamic Degree) | ä¸»ä½“ä¸€è‡´æ€§ | è¿åŠ¨å¹³æ»‘åº¦ | å¤‡æ³¨ |\n| :--- | :--- | :--- | :--- | :--- |\n| **åŸºç¡€æ¨¡å‹** (æœªå¾®è°ƒ) | 39.6% | 95.3% | 96.3% | åŸºå‡† |\n| **å…¨æ•°æ®é›†å¾®è°ƒ** | 42.0% | 95.9% | 96.3% | æ€§èƒ½ä¸Šé™å‚è€ƒ |\n| **éšæœºé€‰æ‹©** (10%æ•°æ®) | 41.3% | 95.3% | 96.3% | éšæœºåŸºçº¿ |\n| **Ours w/o è¿åŠ¨æ©ç ** | 43.8% | 95.4% | 96.3% | æ¶ˆèå®éªŒ |\n| **Ours (Motive)** | **47.6%** | **96.3%** | 96.3% | **æœ¬æ–‡æ–¹æ³•** |\n\n**å…³é”®ç»“è®º**ï¼š\n- **åŠ¨æ€ç¨‹åº¦**ï¼š**Motive** æ–¹æ³•ï¼ˆ47.6%ï¼‰æ˜¾è‘—ä¼˜äºéšæœºé€‰æ‹©ï¼ˆ41.3%ï¼‰å’Œå…¨æ•°æ®é›†å¾®è°ƒï¼ˆ42.0%ï¼‰ï¼Œ**ç›¸å¯¹åŸºç¡€æ¨¡å‹æå‡çº¦8ä¸ªç™¾åˆ†ç‚¹**ã€‚\n- **è¿åŠ¨è´¨é‡**ï¼šåœ¨ä¿æŒè¿åŠ¨å¹³æ»‘åº¦ä¸ä¸‹é™çš„å‰æä¸‹ï¼Œå¤§å¹…æå‡äº†è§†é¢‘çš„åŠ¨æ€è¡¨ç°ã€‚\n- **æ•°æ®æ•ˆç‡**ï¼šä»…ç”¨10%çš„æ•°æ®ï¼Œåœ¨æ ¸å¿ƒæŒ‡æ ‡ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº†ä½¿ç”¨100%æ•°æ®å¾®è°ƒçš„æ€§èƒ½ã€‚\n\n#### 2. äººå·¥è¯„ä¼°\nè¿›è¡Œäº†æˆå¯¹æ¯”è¾ƒçš„äººç±»åå¥½è¯„ä¼°ï¼ˆ50ä¸ªè§†é¢‘ï¼Œ17åå‚ä¸è€…ï¼‰ï¼š\n\n| å¯¹æ¯”ç»„ | **Motive è·èƒœç‡** | å¹³å±€ç‡ | å¤±è´¥ç‡ |\n| :--- | :--- | :--- | :--- |\n| vs. åŸºç¡€æ¨¡å‹ | **74.1%** | 12.3% | 13.6% |\n| vs. éšæœºé€‰æ‹© | **58.9%** | 12.1% | 29.0% |\n| vs. å…¨æ•°æ®é›†å¾®è°ƒ | **53.1%** | 14.8% | 32.1% |\n\n**ç»“è®º**ï¼šäººç±»è¯„ä¼°è€…æ˜¾è‘—åå¥½ç”± **Motive** ç­›é€‰æ•°æ®å¾®è°ƒåç”Ÿæˆçš„è§†é¢‘ï¼Œå°¤å…¶æ˜¯åœ¨è¿åŠ¨è´¨é‡æ–¹é¢ã€‚\n\n### å››ã€å®é™…ä»·å€¼æ€»ç»“\n- **æŠ€æœ¯åˆ›æ–°**ï¼šé¦–æ¬¡æå‡ºäº†é’ˆå¯¹è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„**è¿åŠ¨å½’å› æ¡†æ¶**ï¼Œèƒ½å¤Ÿè¿½æº¯ç”Ÿæˆè§†é¢‘ä¸­çš„åŠ¨æ€æ•ˆæœåˆ°å…·ä½“çš„è®­ç»ƒç‰‡æ®µã€‚\n- **æ€§èƒ½ä»·å€¼**ï¼šè¯æ˜äº†é€šè¿‡**è¿åŠ¨æ„ŸçŸ¥çš„æ•°æ®ç­›é€‰**è¿›è¡Œé’ˆå¯¹æ€§å¾®è°ƒï¼Œå¯ä»¥**ç”¨æå°‘çš„æ•°æ®ï¼ˆ10%ï¼‰å®ç°è¿åŠ¨è´¨é‡çš„æ˜¾è‘—æå‡**ï¼Œä¸ºé«˜è´¨é‡è§†é¢‘ç”Ÿæˆæä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®åˆ©ç”¨æ–¹æ¡ˆã€‚\n- **åº”ç”¨å‰æ™¯**ï¼šè¯¥æ¡†æ¶å¯ç”¨äºè¯Šæ–­æ¨¡å‹è¿åŠ¨ä¼ªå½±ã€æŒ‡å¯¼æ•°æ®æ¸…æ´—ä¸ curationï¼Œå¹¶æœ€ç»ˆå¸®åŠ©æ„å»ºæ›´å¯æ§ã€å¯è§£é‡Šçš„è§†é¢‘ç”Ÿæˆç³»ç»Ÿã€‚",
            "timestamp": "2026-01-20T17:55:15.877875",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ã€ŠMotion Attribution for Video Generationã€‹åˆ†æï¼šå±€é™æ€§ä¸æœªæ¥æ–¹å‘\n\nåŸºäºå¯¹è®ºæ–‡çš„æ·±å…¥åˆ†æï¼Œä»¥ä¸‹æ˜¯è¯¥ç ”ç©¶ï¼ˆMOTIVEæ¡†æ¶ï¼‰çš„ä¸»è¦å±€é™æ€§åŠæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘ã€‚\n\n### ä¸»è¦å±€é™æ€§\n\n1.  **è®¡ç®—æˆæœ¬é«˜æ˜‚**\n    - **æ ¸å¿ƒç“¶é¢ˆ**ï¼šåŸºäºæ¢¯åº¦çš„å½’å› æ–¹æ³•éœ€è¦ä¸º**æ¯ä¸ªè®­ç»ƒæ ·æœ¬**è®¡ç®—æ¢¯åº¦ï¼Œå¯¹äºç°ä»£å¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†å’Œåäº¿å‚æ•°æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€æ¬¡æ€§ä½†å·¨å¤§çš„å‰æœŸè®¡ç®—å¼€é”€ã€‚\n    - **å…·ä½“æ•°æ®**ï¼šåœ¨å•å¼ A100 GPUä¸Šï¼Œä¸º1ä¸‡ä¸ªè®­ç»ƒæ ·æœ¬è®¡ç®—æ¢¯åº¦çº¦éœ€150å°æ—¶ã€‚è™½ç„¶å¯å¹¶è¡ŒåŒ–ï¼ˆ64å¼ GPUçº¦2.3å°æ—¶ï¼‰ä¸”æˆæœ¬å¯åˆ†æ‘Šç»™åç»­å¤šä¸ªæŸ¥è¯¢ï¼Œä½†ä»æ„æˆå®é™…éƒ¨ç½²çš„éšœç¢ã€‚\n\n2.  **è§†é¢‘çº§å½’å› çš„ç²’åº¦é—®é¢˜**\n    - è®ºæ–‡å°†**æ•´ä¸ªè§†é¢‘ç‰‡æ®µ**ä½œä¸ºä¸€ä¸ªå•å…ƒè¿›è¡Œå½’å› ã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœä¸€ä¸ªé•¿è§†é¢‘ä¸­åªæœ‰ä¸€å°æ®µåŒ…å«ç›¸å…³è¿åŠ¨ï¼Œå…¶å½±å“åŠ›ä¼šè¢«é™æ€æˆ–æ— å…³çš„å¸§â€œç¨€é‡Šâ€ï¼Œå¯¼è‡´å½’å› ä¿¡å·ä¸å¤Ÿç²¾ç¡®ã€‚\n    - è¿™é™åˆ¶äº†æ–¹æ³•è¯†åˆ«**ç‰¹å®šè¿åŠ¨äº‹ä»¶æˆ–è½¨è¿¹ç‰‡æ®µ**çš„èƒ½åŠ›ã€‚\n\n3.  **è¿åŠ¨æ©ç çš„ä¾èµ–æ€§ä¸å±€é™æ€§**\n    - è¿åŠ¨æƒé‡ä¸¥é‡ä¾èµ–äºå¤–éƒ¨è¿åŠ¨ä¼°è®¡å™¨ï¼ˆå¦‚è®ºæ–‡ä½¿ç”¨çš„AllTrackerï¼‰ã€‚åœ¨**ä¸¥é‡é®æŒ¡ã€é€æ˜ç‰©ä½“æˆ–å¾®å°è¿åŠ¨**çš„æƒ…å†µä¸‹ï¼Œä¼°è®¡å™¨å¯èƒ½å¤±æ•ˆï¼Œå¯¼è‡´æ©ç ä¸å‡†ç¡®ã€‚\n    - å½“å‰æ–¹æ³•éš¾ä»¥å®Œå…¨**åŒºåˆ†æ‘„åƒæœºè¿åŠ¨ä¸ç‰©ä½“è‡ªèº«è¿åŠ¨**ã€‚è™½ç„¶è®ºæ–‡é€šè¿‡ç©ºé—´å‡åŒ€æ€§æ£€æµ‹æ¥é™æƒçº¯æ‘„åƒæœºè¿åŠ¨ç‰‡æ®µï¼Œä½†å½»åº•çš„è§£è€¦ä»æ˜¯æœªè§£å†³çš„é—®é¢˜ã€‚\n\n4.  **æœªè€ƒè™‘åˆ†ç±»å™¨æ— å…³å¼•å¯¼**\n    - è®ºæ–‡çš„å½’å› è®¡ç®—åŸºäºè®­ç»ƒç›®æ ‡ï¼ˆå»å™ªæŸå¤±ï¼‰ï¼Œä½†å®é™…è§†é¢‘ç”Ÿæˆä¸­å¹¿æ³›ä½¿ç”¨**åˆ†ç±»å™¨æ— å…³å¼•å¯¼**æ¥æå‡è´¨é‡ã€‚CFGä¼šæ”¹å˜æ¨ç†æ—¶çš„åŠ¨æ€è¡Œä¸ºï¼Œè€Œå½“å‰çš„å½’å› æ¡†æ¶å¹¶æœªå»ºæ¨¡è¿™ä¸€è¿‡ç¨‹ï¼Œå› æ­¤å½’å› ä¼°è®¡å¯èƒ½æ— æ³•å®Œå…¨åæ˜ CFGå½±å“ä¸‹çš„æœ€ç»ˆè¿åŠ¨è¡Œä¸ºã€‚\n\n5.  **ç›®æ ‡å¾®è°ƒå¯èƒ½å¸¦æ¥çš„èƒ½åŠ›æƒè¡¡**\n    - ä½¿ç”¨MOTIVEé€‰æ‹©çš„é«˜å½±å“åŠ›æ•°æ®è¿›è¡Œé’ˆå¯¹æ€§å¾®è°ƒï¼Œåœ¨æå‡**ç‰¹å®šè¿åŠ¨è´¨é‡**ï¼ˆå¦‚å¹³æ»‘åº¦ã€åŠ¨æ€ç¨‹åº¦ï¼‰çš„åŒæ—¶ï¼Œ**å¯èƒ½æŸå®³åŸºç¡€æ¨¡å‹åœ¨å…¶ä»–ä»»åŠ¡æˆ–è¿åŠ¨ç±»å‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›**ã€‚è®ºæ–‡æœªæ·±å…¥æ¢è®¨è¿™ç§æƒè¡¡ã€‚\n\n### æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\n1.  **æå‡è®¡ç®—ä¸å­˜å‚¨æ•ˆç‡**\n    - å¼€å‘æ›´é«˜æ•ˆçš„æ¢¯åº¦è¿‘ä¼¼æˆ–**ç¨€ç–åŒ–æ–¹æ³•**ï¼Œå‡å°‘æ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦è®¡ç®—æˆæœ¬ã€‚\n    - æ¢ç´¢**åˆ†å±‚æˆ–åˆ†å—å½’å› **ï¼Œå…ˆå¿«é€Ÿç­›é€‰å€™é€‰é›†ï¼Œå†å¯¹é«˜æ½œåŠ›æ ·æœ¬è¿›è¡Œç²¾ç»†è®¡ç®—ã€‚\n\n2.  **å®ç°æ›´ç»†ç²’åº¦çš„å½’å› **\n    - ä»è§†é¢‘çº§å½’å› æ¨è¿›åˆ°**ç‰‡æ®µçº§ï¼ˆsub-clipï¼‰ç”šè‡³å¸§é—´çº§å½’å› **ã€‚è¿™èƒ½æ›´ç²¾å‡†åœ°å®šä½å½±å“ç‰¹å®šè¿åŠ¨æ¨¡å¼çš„å…³é”®è®­ç»ƒæ—¶åˆ»ã€‚\n    - ç»“åˆ**è¿åŠ¨åˆ†å‰²æˆ–äº‹ä»¶æ£€æµ‹æŠ€æœ¯**ï¼Œè‡ªåŠ¨è¯†åˆ«è§†é¢‘ä¸­çš„è¿åŠ¨ç‰‡æ®µè¿›è¡Œç‹¬ç«‹åˆ†æã€‚\n\n3.  **å¢å¼ºè¿åŠ¨ä¼°è®¡çš„é²æ£’æ€§ä¸è§£è€¦èƒ½åŠ›**\n    - é‡‡ç”¨**å¤šè¿åŠ¨ä¼°è®¡å™¨é›†æˆ**ï¼Œæˆ–åˆ©ç”¨æ¨¡å‹çš„å†…éƒ¨è¡¨å¾æ¥ç”Ÿæˆè¿åŠ¨æ˜¾è‘—æ€§å›¾ï¼Œå‡å°‘å¯¹å¤–éƒ¨è¿½è¸ªå™¨çš„ä¾èµ–ã€‚\n    - é›†æˆ**æ‘„åƒæœºå§¿æ€ä¼°è®¡æˆ–å…‰æµåˆ†è§£æŠ€æœ¯**ï¼Œæ›´å¥½åœ°åŒºåˆ†åœºæ™¯è¿åŠ¨ï¼ˆego-motionï¼‰ä¸ç‰©ä½“è¿åŠ¨ï¼Œä½¿å½’å› æ›´çº¯ç²¹åœ°å…³æ³¨ç‰©ä½“åŠ¨åŠ›å­¦ã€‚\n\n4.  **æ„å»ºé—­ç¯ä¸ä¸»åŠ¨æ•°æ®ç®¡ç†æµç¨‹**\n    - å°†ä¸€æ¬¡æ€§å½’å› -é€‰æ‹©-å¾®è°ƒæ‰©å±•ä¸º**è¿­ä»£å¼ã€é—­ç¯çš„ä¸»åŠ¨å­¦ä¹ æ¡†æ¶**ï¼šå¾®è°ƒåï¼Œåœ¨æ–°æ¨¡å‹ä¸Šé‡æ–°è®¡ç®—å½’å› ï¼Œè¿›ä¸€æ­¥ç­›é€‰æ•°æ®ï¼ŒæŒç»­ä¼˜åŒ–ã€‚\n    - å¼€å‘**åŸºäºå­¦ä¹ çš„æŸ¥è¯¢æƒé‡åˆ†é…**ï¼Œæ›¿ä»£ç®€å•çš„å¤šæ•°æŠ•ç¥¨æ³•ï¼Œæ›´æ™ºèƒ½åœ°èšåˆå¤šæŸ¥è¯¢çš„å½’å› ç»“æœã€‚\n\n5.  **æ‰©å±•æ¡†æ¶çš„åº”ç”¨èŒƒå›´ä¸æ²»ç†èƒ½åŠ›**\n    - **å®‰å…¨ä¸æ²»ç†**ï¼šåˆ©ç”¨è´Ÿå½±å“åŠ›ï¼ˆæœ‰å®³æˆ–ä¸è‰¯è¿åŠ¨æ¨¡å¼ï¼‰è¯†åˆ«æŠ€æœ¯ï¼Œä»è®­ç»ƒé›†ä¸­è¿‡æ»¤æˆ–é™æƒé—®é¢˜æ•°æ®ï¼Œä¸ºç”Ÿæˆæ¨¡å‹çš„å®¡è®¡ä¸å®‰å…¨æä¾›å·¥å…·ã€‚\n    - **å¤šæ¨¡æ€æ‰©å±•**ï¼šå°†è¿åŠ¨å½’å› æ¡†æ¶æ‰©å±•åˆ°**ä¸–ç•Œæ¨¡å‹ã€éŸ³é¢‘ç”Ÿæˆæˆ–éŸ³è§†é¢‘è”åˆç”Ÿæˆ**ç­‰é¢†åŸŸï¼Œç†è§£æ•°æ®åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å½±å“ã€‚\n    - **è‡ªæˆ‘è¯Šæ–­**ï¼šä½¿ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„ã€å­˜åœ¨è¿åŠ¨ä¼ªå½±çš„è§†é¢‘ä½œä¸ºæŸ¥è¯¢ï¼Œåå‘è¿½è¸ªå¯¼è‡´è¿™äº›é—®é¢˜çš„è®­ç»ƒæ•°æ®ï¼Œå®ç°**è¿­ä»£å¼æ¨¡å‹è¯Šæ–­ä¸ä¿®å¤**ã€‚\n\n6.  **ç»“åˆæ›´å…ˆè¿›çš„å¾®è°ƒç­–ç•¥**\n    - æ¢ç´¢å°†MOTIVEé€‰æ‹©çš„æ•°æ®ç”¨äº**å¤šå­¦ç”Ÿè’¸é¦ã€å‚æ•°é«˜æ•ˆå¾®è°ƒ**ç­‰æ›´å¤æ‚çš„å¾®è°ƒè®¾ç½®ï¼Œåœ¨æå‡ç›®æ ‡è¿åŠ¨è´¨é‡çš„åŒæ—¶ï¼Œæ›´å¥½åœ°ä¿ç•™åŸºç¡€æ¨¡å‹çš„å¹¿æ³›èƒ½åŠ›ã€‚\n\n### æ€»ç»“\nMOTIVEæ¡†æ¶åœ¨**è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è¿åŠ¨æ•°æ®å½’å› **é¢†åŸŸåšå‡ºäº†å¼€åˆ›æ€§è´¡çŒ®ï¼Œä½†å…¶å±€é™æ€§ä¸»è¦é›†ä¸­åœ¨**è®¡ç®—å¯æ‰©å±•æ€§ã€å½’å› ç²’åº¦ã€è¿åŠ¨ä¼°è®¡ä¾èµ–æ€§**ä»¥åŠ**ä¸å®é™…ç”Ÿæˆæµç¨‹çš„å·®è·**ä¸Šã€‚æœªæ¥çš„å·¥ä½œæœ‰æœ›é€šè¿‡ç®—æ³•ä¼˜åŒ–ã€ç²’åº¦ç»†åŒ–ã€æµç¨‹é—­ç¯åŒ–ä»¥åŠåº”ç”¨æ‰©å±•ï¼Œä½¿æ•°æ®å½’å› æˆä¸ºæ„å»ºæ›´å¯æ§ã€å¯è§£é‡Šã€é«˜æ€§èƒ½è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ ¸å¿ƒå·¥å…·ã€‚",
            "timestamp": "2026-01-20T17:55:53.871072",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-13T18:59:09Z",
    "created_at": "2026-01-20T17:49:46.593195",
    "updated_at": "2026-01-20T17:49:46.593204"
}