{
  "id": "2601.08828v1",
  "title": "Motion Attribution for Video Generation",
  "authors": [
    "Xindi Wu",
    "Despoina Paschalidou",
    "Jun Gao",
    "Antonio Torralba",
    "Laura Leal-Taix茅",
    "Olga Russakovsky",
    "Sanja Fidler",
    "Jonathan Lorraine"
  ],
  "abstract": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
  "url": "https://arxiv.org/abs/2601.08828v1",
  "html_url": "https://arxiv.org/html/2601.08828v1",
  "html_content": "\\dosecttoc\nMotion Attribution for Video Generation\nXindi Wu\n12\nDespoina Paschalidou\n1\nJun Gao\n1\nAntonio Torralba\n3\nLaura Leal-Taix茅\n1\nOlga Russakovsky\n2\nSanja Fidler\n1\nJonathan Lorraine\n1\n1\nNVIDIA\n2\nPrinceton University\n3\nMIT CSAIL\nhttps://research.nvidia.com/labs/sil/projects/MOTIVE/\nAbstract\nDespite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present\nMotive\n(\nMOTI\non attribution for\nV\nideo g\nE\nneration\n), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models.\nWe use this to study which fine-tuning clips improve or degrade temporal dynamics.\nMotive\nisolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation.\nOn text-to-video models,\nMotive\nidentifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility.\nWith\nMotive\n-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model.\nTo our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.\n\\abscontent\n1\nIntroduction\nMotion is the defining element of videos. Unlike image generation, which produces a single frame, video generative models capture how objects move, interact, and obey physical constraints\n(\nwiedemer2025video\n;\nkang2024far\n)\n. Yet even with the rapid progress of video generative models, a fundamental question remains:\nWhich training clips influence the motion in generated videos?\nWhy it matters.\nDiffusion models are data-driven, and their progress has tracked the scaling of data and compute\n(\nsaharia2022photorealistic\n;\nnichol2021improved\n;\nho2022video\n;\npeebles2023scalable\n)\n.\nPrior work\n(\nblattmann2023stable\n;\nkaplan2020scaling\n;\nravishankar2025scaling\n)\nshows that training data shapes key generative properties, including visual quality\n(\nrombach2022high\n)\n, semantic fidelity\n(\nnamekata2024emerdiff\n)\n, and compositionality\n(\nwu2025compact\n;\nfavero2025compositional\n)\n.\nMotion is no exception.\nMotion\nrefers to temporal dynamics captured by optical flow, including trajectories, deformations, camera movement, and interactions. If generated motion reflects the data distribution that shaped the model, then attributing motion to influential training clips provides a direct lens on why a model moves the way it does and enables targeted data selection for desired dynamics.\nHigh-quality data often matters most in fine-tuning, where large pretraining corpora are inaccessible and carefully selected clips can have an outsized impact. Motion-specific attribution is therefore especially valuable in the fine-tuning regime, where the goal is to identify which clips most influence temporal coherence and physical plausibility.\nWhy existing methods are limited for motion.\nPrior diffusion data attribution focuses on images and explains static content. Extending these methods to videos na茂vely collapses motion into appearance, missing the temporal structure that distinguishes videos from images. Three challenges drive this gap: (i) localizing motion so attribution focuses on dynamic regions rather than static backgrounds, (ii) scaling to sequences since gradients must integrate across time, and (iii) capturing temporal relations like velocity, acceleration, and trajectory coherence that single-frame attribution cannot measure. Addressing motion attribution requires methods that explicitly model temporal structure, rather than treating time as an additional spatial axis.\nOur method.\nWe introduce\nMotive\n, a motion attribution framework for video generation models that isolates motion-specific influence.\nMotive\ncomputes gradients with motion-aware masking. As a result, the attribution signal emphasizes dynamic regions rather than static appearance. Efficient approximations make the method practical for large, high-quality datasets and video generative models. Our scores trace generated motion back to training clips, enabling target curation and improving motion quality when used to guide fine-tuning.\nOur contributions are:\n1.\nProposing a scalable gradient-based attribution approach for video generation models that is computationally efficient, even at the scale of modern, high-quality data and large generative models (搂\n3.2\n).\n2.\nAddressing a video-specific bias by correcting frame-length effects in gradient magnitudes, ensuring fair attribution across clips of different durations (搂\n3.3\n).\n3.\nIntroducing an attribution that emphasizes temporal dynamics to trace which training clips most strongly influence motion quality (搂\n3.4\n).\n4.\nShowing that we improve motion smoothness and dynamic degree on VBench and in human evaluation (搂\n4\n), matching, or surpassing, full-dataset fine-tuning performance with only\n10\n%\n10\\%\nof the data, and outperforming motion-unaware attribution baselines (Tables\n1\nand\n2\n).\n2\nBackground\nWe provide notation in App. 搂\nA\nand extended related work in App. 搂\nB\n.\n2.1\nVideo Generation with Diffusion and Flow-Matching Models\nDiffusion and flow matching in latent space.\nLet\np\n\n\n(\n\n\n\n)\np_{\\bm{\\theta}}(\\mathbf{v}\\mid\\mathbf{c})\nbe a conditional generator with parameters\n\n\\bm{\\theta}\n, where\n\n\n\nF\n\nH\n\nW\n\n3\n\\mathbf{v}\\in\\mathbb{R}^{F\\times H\\times W\\times 3}\nis a clip of height\nH\nH\n, width\nW\nW\n, and\nF\nF\nframes, and\n\n\\mathbf{c}\ndenotes conditioning such as text or other multimodal metadata (e.g., fps, depth, pose).\nWe operate in VAE latents:\n\n=\nE\n\n(\n\n)\n\\mathbf{h}=E(\\mathbf{v})\nand train a denoiser or velocity field on noisy latents.\nA noise scheduler supplies time-dependent coefficients\n(\n伪\nt\n,\n\nt\n)\n(\\alpha_{t},\\sigma_{t})\ncontrolling signal and noise scales, and the forward noising is:\n\n\n(\nt\n,\n系\n)\n=\n伪\nt\n\n\n+\n\nt\n\n系\n,\n系\n\n\n\n(\n0\n,\n\n)\n,\nt\n\n{\n1\n,\n\n,\nT\n}\n.\n\\smash{\\mathbf{z}(t,\\bm{\\epsilon})\\;=\\;\\alpha_{t}\\,\\mathbf{h}\\;+\\;\\sigma_{t}\\,\\bm{\\epsilon},\\quad\\bm{\\epsilon}\\sim\\mathcal{N}(0,\\mathbf{I}),\\quad t\\in\\{1,\\ldots,T\\}.}\\vskip-2.75269pt\n(1)\nDenoising diffusion\n(\nho2020denoising\n)\ntrains a network\n系\n\n\n(\n\n,\n\n,\nt\n)\n\\bm{\\epsilon}_{\\bm{\\theta}}(\\mathbf{z},\\mathbf{c},t)\nto predict the injected noise:\n\ndiff\n\n(\n\n;\n\n,\n\n)\n=\n\nt\n,\n系\n\n[\n\n系\n\n\n(\n\n\n(\nt\n,\n系\n)\n,\n\n,\nt\n)\n\n系\n\n2\n2\n]\n.\n\\smash{\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{v},\\mathbf{c})\\;=\\;\\mathbb{E}_{t,\\bm{\\epsilon}}\\!\\left[\\left\\lVert\\bm{\\epsilon}_{\\bm{\\theta}}(\\mathbf{z}(t,\\bm{\\epsilon}),\\mathbf{c},t)-\\bm{\\epsilon}\\right\\rVert_{2}^{2}\\right].}\\vskip-2.75269pt\n(2)\nFlow matching\n(\nlipman2022flow\n;\nalbergo2023stochastic\n)\nlearns a time-dependent vector field\n\n\n\n(\n\nt\n,\n\n,\nt\n)\n\\mathbf{f}_{\\bm{\\theta}}(\\mathbf{z}_{t},\\mathbf{c},t)\nthat matches the instantaneous velocity\n\n\n=\nd\nd\n\nt\n\n\n\\dot{\\mathbf{z}}=\\tfrac{\\textnormal{d}}{\\textnormal{d}t}\\mathbf{z}\ninduced by a chosen interpolant:\n\nflow\n\n(\n\n;\n\n,\n\n)\n=\n\nt\n,\n系\n\n[\n\n\n\n\n(\n\n\n(\nt\n,\n系\n)\n,\n\n,\nt\n)\n\n\n\n\n(\nt\n,\n系\n)\n\n2\n2\n]\n.\n\\smash{\\mathcal{L}_{\\textnormal{flow}}(\\bm{\\theta};\\mathbf{v},\\mathbf{c})\\;=\\;\\mathbb{E}_{t,\\bm{\\epsilon}}\\!\\left[\\left\\lVert\\mathbf{f}_{\\bm{\\theta}}(\\mathbf{z}(t,\\bm{\\epsilon}),\\mathbf{c},t)-\\dot{\\mathbf{z}}(t,\\bm{\\epsilon})\\right\\rVert_{2}^{2}\\right].}\\vskip-2.75269pt\n(3)\nBoth objectives train time-indexed predictors over the latent space by integrating over\nt\nt\nand\n系\n\\bm{\\epsilon}\n, thus gradient-based methods like attribution share similar challenges.\nFrom images to video for generation.\nAdding a temporal axis materially changes modeling and training.\nGeneration must capture spatial appearance and temporal dynamics such as object and camera motion, deformations, and interactions.\nModern systems extend image backbones with temporal capacity, for example, 3D U-Nets or 2D U-Nets augmented with temporal attention, causal or sliding-window context, and factorized space-time blocks, often trained in a latent-video VAE that compresses frames while preserving temporal cues.\nTraining departs from images along several axes, which we address in 搂\n3\n:\n(i)\nCompute and storage.\nLonger sequences multiply the cost of sampling timesteps, noise draws, and frames, motivating fixed-timestep or small-subset estimators that reduce variance without prohibitive cost (搂\n3.2\n).\n(ii)\nVariable horizon.\nClips vary in\nF\nF\nand frame rate (搂\n3.3\n).\n(iii)\nTime-specific failure modes.\nTypical artifacts include inconsistent trajectories, temporal flicker, identity drift, and physically implausible dynamics despite sharp individual frames (搂\n3.4\n).\nMotion representations in videos.\nWe denote our video as\n\n=\n[\n\nf\n]\nf\n=\n1\nF\n\\mathbf{v}=[\\mathbf{f}_{f}]_{f=1}^{F}\nwith\n\nf\n\n\nH\n\nW\n\n3\n\\mathbf{f}_{f}\\in\\mathbb{R}^{H\\times W\\times 3}\nbeing the\nf\nf\n-th frame.\nWe represent motion via optical flow between consecutive frames:\n\nf\n:\n{\n1\n,\n\n,\nH\n}\n\n{\n1\n,\n\n,\nW\n}\n\n\n2\n\\mathbf{F}_{f}:\\{1,\\ldots,H\\}\\times\\{1,\\ldots,W\\}\\to\\mathbb{R}^{2}\n, where each flow vector in\n\n2\n\\mathbb{R}^{2}\nencodes the horizontal displacement\nd\n\nw\n\\mathrm{d}w\nand vertical displacement\nd\n\nh\n\\mathrm{d}h\nof a pixel.\nThe motion magnitude is\nM\nf\n\n(\nh\n,\nw\n)\n=\n\n\nf\n\n(\nh\n,\nw\n)\n\n2\nM_{\\!f}(h,w)=\\|\\mathbf{F}_{f}(h,w)\\|_{2}\n.\nThe\nM\nf\nM_{\\!f}\nover frames\nf\nf\nand pixels\nh\n,\nw\nh,w\nsummarizes the amount and spatial layout of motion in a clip, which we use to provide masks in our motion-weighted loss in 搂\n3\n.\n2.2\nData Attribution\nData attribution measures how individual training samples affect a models predictions\n(\nbae2024training\n)\n.\nA classic approach to data attribution is to use influence functions\n(\nkoh2017understanding\n)\n. Intuitively, the influence of a training sample measures: if we upweight this training example, how much would the models prediction on a test datum change?\nConsider a loss function\n\n\n(\n\n;\n\n)\n\\mathcal{L}(\\bm{\\theta};\\mathbf{x})\nand a test sample\n\ntest\n\\mathbf{x}_{\\textnormal{test}}\n, the influence of a training point\n\nn\n\\mathbf{x}_{n}\ncan be quantified as:\nI\n\n(\n\nn\n,\n\ntest\n)\n=\n\n\n\n\n\n(\n\n;\n\ntest\n)\n\n\n\n\n\n1\n\n\n\n\n\n(\n\n;\n\nn\n)\n,\n\n\n=\n1\nN\n\n\nn\n=\n1\nN\n\n\n2\n\n\n(\n\n;\n\nn\n)\n,\nI(\\mathbf{x}_{n},\\mathbf{x}_{\\textnormal{test}})\\penalty 10000\\ =\\penalty 10000\\ -\\nabla_{\\bm{\\theta}}\\mathcal{L}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}})^{\\!\\top}\\,\\mathbf{H}_{\\bm{\\theta}}^{-1}\\,\\nabla_{\\bm{\\theta}}\\mathcal{L}(\\bm{\\theta};\\mathbf{x}_{n}),\\quad\\mathbf{H}_{\\bm{\\theta}}=\\tfrac{1}{N}\\sum\\nolimits_{n=1}^{N}\\nabla_{\\bm{\\theta}}^{2}\\mathcal{L}(\\bm{\\theta};\\mathbf{x}_{n}),\n(4)\nwhere the inverse Hessian captures the curvature of the loss landscape, yet computing or storing it is infeasible at modern model and dataset scales. Thus, practical methods (e.g., TracIn\n(\npruthi2020estimating\n)\nand TRAK\n(\npark2023trak\n)\n) approximate influence via gradient inner products or gradient feature projections.\nAttribution in diffusion models.\nDiffusion training aggregates gradients over timesteps\nt\nt\nand noise draws\n系\n\\bm{\\epsilon}\n, where gradient norms vary systematically with\nt\nt\n, producing a timestep bias where examples aligned with large-norm timesteps appear spuriously influential. Diffusion-ReTrac\n(\nxie2024data\n)\nreduces this bias by normalizing gradients and sub-sampling\nt\nt\nand\n系\n\\bm{\\epsilon}\nfor influence. Let\n\ndiff\n\\mathcal{L}_{\\textnormal{diff}}\ndenote the diffusion loss, and with the sampled-timestep-and-noise set\n\n\\mathcal{T}\n, we compute a cosine-style score for normalized test and train gradients:\nI\ndiff\n\n(\n\nn\n,\n\ntest\n)\n=\n1\n|\n\ntest\n|\n\n\nt\n,\n系\n\n\ntest\n\n\n\ndiff\n\n(\n\n;\n\ntest\n,\nt\n,\n系\n)\n\n\n\n\ndiff\n\n(\n\n;\n\ntest\n,\nt\n,\n系\n)\n\n\n\n1\n|\n\nn\n|\n\n\nt\n,\n系\n\n\nn\n\n\n\ndiff\n\n(\n\n;\n\nn\n,\nt\n,\n系\n)\n\n\n\n\ndiff\n\n(\n\n;\n\nn\n,\nt\n,\n系\n)\n\n.\nI_{\\textnormal{diff}}(\\mathbf{x}_{n},\\mathbf{x}_{\\textnormal{test}})\\!=\\!\\mathchoice{\\hbox to575.83pt{\\vbox to26pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 287.91437pt\\lower-10.00012pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-287.91437pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to575.83pt{\\vbox to26pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 287.91437pt\\lower-10.00012pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-287.91437pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to353.57pt{\\vbox to20pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 176.78328pt\\lower-7.00009pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-176.78328pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to275.18pt{\\vbox to17pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 137.59033pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-137.59033pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{\\textnormal{test}}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{\\textnormal{test}}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}\\,\\,\\,\\,\\mathchoice{\\hbox to472.99pt{\\vbox to26pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 236.49352pt\\lower-10.00012pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-236.49352pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to472.99pt{\\vbox to26pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 236.49352pt\\lower-10.00012pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-236.49352pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to299.63pt{\\vbox to20pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 149.81506pt\\lower-7.00009pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-149.81506pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to237.99pt{\\vbox to17pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 118.99416pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-118.99416pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{1}{|\\mathcal{T}_{n}|}\\!\\sum_{t\\!,\\bm{\\epsilon}\\in\\mathcal{T}_{n}}\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}.\n(5)\nnormalized test gradients\nnormalized training gradients\naaaaaaaaa\nAveraging gradients over\n(\nt\n,\n系\n)\n(t,\\bm{\\epsilon})\nstabilizes estimates, and normalization mitigates timestep-induced scale effects. Attribution quality is also sensitive to the measurement function used to score examples, such as denoising loss versus likelihood proxies\n(\nzheng2023intriguing\n)\n.\nWhy vanilla attribution is insufficient for videos.\nNa茂vely applying gradient-based attribution to video diffusion risks treating appearance and motion alike, often overemphasizing low-level appearance matches (objects, textures, backgrounds) while overlooking dynamics\n(\npark2025concept\n;\ntulyakov2018mocogan\n)\n. Its cost grows with clip length, sampled timesteps, noise draws, and gradient dimensionality, making na茂ve methods impractical at modern video scales. Because we aim to explain and improve motion, we need attribution that suppresses static appearance, emphasizes motion-specific signals, and remains efficient, motivating the motion-centric approach in 搂\n3\n. Motion is distributed across frames and temporal horizons and entangled with static cues, so influence cannot be assigned by considering frames independently.\n3\nMethod\nWe formalize the problem setup in 搂\n3.1\nand develop a practical framework for motion attribution in video diffusion models with four key components: scalable gradient computation (搂\n3.2\n), frame-length bias fix (搂\n3.3\n), motion-aware weighting (搂\n3.4\n), and data selection for targeted fine-tuning (搂\n3.5\n). We also provide a computational efficiency analysis (搂\n3.6\n) demonstrating the scalability of our approach to billion-parameter models and large-scale video datasets.\n3.1\nProblem Formulation\nWe study data attribution for motion in the fine-tuning setting. Let\n\nft\n=\n{\n(\n\nn\n,\n\nn\n)\n}\nn\n=\n1\nN\n\\mathcal{D}_{\\textnormal{ft}}=\\{(\\mathbf{v}_{n},\\mathbf{c}_{n})\\}_{n=1}^{N}\nbe the fine-tuning corpus. Given a query video\n(\n\n^\n,\n\n^\n)\n(\\hat{\\mathbf{v}},\\hat{\\mathbf{c}})\n, we assign to each training clip\n(\n\nn\n,\n\nn\n)\n(\\mathbf{v}_{n},\\mathbf{c}_{n})\na motion-aware influence score\nI\n\n(\n\nn\n,\n\n^\n;\n\n)\nI(\\mathbf{v}_{n},\\hat{\\mathbf{v}};\\bm{\\theta})\nthat explains how it contributes to the dynamics observed in\n\n^\n\\hat{\\mathbf{v}}\n. The score should satisfy:\n(i)\npredictivity\n, rankings correlate with observed changes from fine-tuning on the most influential subsets;\n(ii)\nefficiency\n, scales to modern video generators, such as forgoing explicit Hessian inversion, expensive per-data integration, or prohibitive storage.\nTo do this, we augment the influence target defined in  Eq.\n5\nto be (a) lower variance for stable rankings with feasible levels of compute, (b) more scalable to store, and (c) motion-centric.\nFine-tuning Subset Selection.\nFor a budget\nK\n\nN\nK\\ll N\n, we get a motion-influential subset by ranking scores and taking the top-\nK\nK\nexamples. When aggregating across multiple query motions, we combine selections as described in 搂\n3\n. The resulting subsets serve as candidates for motion-centric fine-tuning.\n3.2\nScalable Gradient-based Attribution for Generative Models\nWe make attribution practical for modern, large, high-quality video datasets and models via inverse-Hessian approximations, lower-variance gradient-similarity estimators, low-cost single-sample estimators, and a Fastfood projection for tractable storage.\nApproximating the inverse-Hessian.\nComputing exact inverse-Hessian-vector products is infeasible for modern neural networks.\nWe estimate influence via gradient similarity, using an identity preconditioner for the inverse Hessian\n(\nkoh2017understanding\n;\npruthi2020estimating\n;\npark2023trak\n)\n.\nCommon randomness for stable rankings.\nTo reduce variance without changing the target, we evaluate train and test gradients under the same\n(\nt\n,\n系\n)\n(t,\\bm{\\epsilon})\npairs and average over a small set\n\n\\mathcal{T}\n(\nxie2024data\n;\nlin2024diffusion\n)\n. This paired averaging stabilizes rankings compared to independent draws:\nI\ndiff\n1\n\n(\n\nn\n,\n\ntest\n)\n=\n1\n|\n\n|\n\n\nt\n,\n系\n\n\n\n\n\ndiff\n\n(\n\n;\n\ntest\n,\nt\n,\n系\n)\n\n\n\n\ndiff\n\n(\n\n;\n\ntest\n,\nt\n,\n系\n)\n\n\n\n\n\ndiff\n\n(\n\n;\n\nn\n,\nt\n,\n系\n)\n\n\n\n\ndiff\n\n(\n\n;\n\nn\n,\nt\n,\n系\n)\n\n.\nI_{\\textnormal{diff}}^{1}(\\mathbf{x}_{n},\\mathbf{x}_{\\textnormal{test}})=\\frac{1}{|\\mathcal{T}|}\\sum_{t,\\bm{\\epsilon}\\in\\mathcal{T}}\\mathchoice{\\hbox to341.66pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 170.82907pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-170.82907pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to341.66pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 170.82907pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-170.82907pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to223.18pt{\\vbox to19pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 111.5877pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-111.5877pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to182.23pt{\\vbox to17pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 91.11737pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-91.11737pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t,\\bm{\\epsilon})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}\\hskip 10.34958pt\\mathchoice{\\hbox to294.53pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 147.2633pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-147.2633pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to294.53pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 147.2633pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-147.2633pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to199.12pt{\\vbox to19pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 99.56108pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-99.56108pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to166.39pt{\\vbox to17pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 83.19362pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-83.19362pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t,\\bm{\\epsilon})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}.\n(6)\nnormalized test gradients\nnormalized training gradients\naaaaaaaaa\nSingle-sample variant for reduced compute.\nWe fix a single\nt\nfix\nt_{\\textnormal{fix}}\nand a shared draw\n系\nfix\n\n\n\n(\n0\n,\n\n)\n\\bm{\\epsilon}_{\\textnormal{fix}}\\sim\\mathcal{N}(0,\\mathbf{I})\nfor all traintest pairs at the final checkpoint. Sharing\n(\nt\nfix\n,\n系\nfix\n)\n(t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\nallows low enough variance for the low-cost single-sample estimator to maintain relative ordering\n(\nxie2024data\n;\nlin2024diffusion\n)\n. The estimator collapses to:\nI\ndiff\n2\n\n(\n\nn\n,\n\ntest\n)\n=\n\n\n\ndiff\n\n(\n\n;\n\ntest\n,\nt\nfix\n,\n系\nfix\n)\n\n\n\n\ndiff\n\n(\n\n;\n\ntest\n,\nt\nfix\n,\n系\nfix\n)\n\n\n\n\n\n\ndiff\n\n(\n\n;\n\nn\n,\nt\nfix\n,\n系\nfix\n)\n\n\n\n\ndiff\n\n(\n\n;\n\nn\n,\nt\nfix\n,\n系\nfix\n)\n\n.\nI_{\\textnormal{diff}}^{2}(\\mathbf{x}_{n},\\mathbf{x}_{\\textnormal{test}})=\\mathchoice{\\hbox to410.21pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 205.10716pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-205.10716pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to410.21pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 205.10716pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-205.10716pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to261.4pt{\\vbox to19pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 130.69882pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-130.69882pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to210.68pt{\\vbox to17pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 105.33957pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-105.33957pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}\\;\\;\\mathchoice{\\hbox to363.08pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 181.5414pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-181.5414pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to363.08pt{\\vbox to22pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 181.5414pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-181.5414pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to237.34pt{\\vbox to19pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 118.6722pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-118.6722pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to194.83pt{\\vbox to17pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 97.41582pt\\lower-6.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-97.41582pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\frac{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\big\\|\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{x}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\big\\|}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}.\n(7)\nnormalized test gradient\nnormalized training gradient\naaaaaaaaa\nStructured projection for reduced storage.\nTo operate at model scale, we apply a JohnsonLindenstrauss projection via Fastfood\n(\nle2014fastfood\n)\nand then normalize.\nLet\n\n\n\nD\n\n\nD\nbe implemented as\n\n:=\n1\n尉\n\nD\n\n\n\n\n\n\n\n,\n\\mathbf{P}\\in\\mathbb{R}^{D^{\\prime}\\times D}\\quad\\text{be implemented as}\\quad\\mathbf{P}:=\\frac{1}{\\xi\\sqrt{D^{\\prime}}}\\;\\mathbf{S}\\mathbf{Q}\\mathbf{G}\\bm{\\Pi}\\mathbf{Q}\\mathbf{B},\\vskip-11.0pt\n(8)\nwhere\n\n\\mathbf{Q}\nis the WalshHadamard matrix,\n\n\\mathbf{B}\nis a diagonal Rademacher matrix,\n\n\\bm{\\Pi}\nis a random permutation,\n\n\\mathbf{G}\nis a diagonal Gaussian scaling, and\n\n\\mathbf{S}\nis a diagonal rescaling, and\n尉\n\\xi\nnormalizes the variance. The projected, normalized gradient is:\n\n~\n\n(\n\n,\n\n)\n:=\n\n\n\n\n\ndiff\n\n(\n\n,\n\n,\nt\nfix\n,\n系\nfix\n)\n\n\n\n\n\n\ndiff\n\n(\n\n,\n\n,\nt\nfix\n,\n系\nfix\n)\n\n.\n\\tilde{\\mathbf{g}}\\big(\\bm{\\theta},\\mathbf{x}\\big)\\penalty 10000\\ :=\\penalty 10000\\ \\frac{\\mathbf{P}\\,\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta},\\mathbf{x},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\|\\mathbf{P}\\,\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta},\\mathbf{x},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\|}.\n(9)\nThen the influence score is the compact cosine in\n\nD\n\n\\mathbb{R}^{D^{\\prime}}\n:\nI\ndiff\n3\n\n(\n\nn\n,\n\ntest\n)\n=\n\n~\n\n(\n\n;\n\ntest\n)\n\n\n\n~\n\n(\n\n;\n\nn\n)\n.\nI_{\\textnormal{diff}}^{3}(\\mathbf{x}_{n},\\mathbf{x}_{\\textnormal{test}})\\penalty 10000\\ =\\penalty 10000\\ \\mathchoice{\\hbox to175.15pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 87.57513pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-87.57513pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to175.15pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 87.57513pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-87.57513pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to129.42pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 64.70833pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-64.70833pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to111.75pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 55.87692pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-55.87692pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{NavyBlue!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}{\\hbox{\\pagecolor{NavyBlue!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{\\textnormal{test}}\\big)^{\\!\\top}$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}\\;\\;\\mathchoice{\\hbox to126.8pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 63.39989pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-63.39989pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\displaystyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to126.8pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 63.39989pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-63.39989pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\textstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to103.94pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 51.97021pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-51.97021pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}{\\hbox to94.89pt{\\vbox to12pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 47.44493pt\\lower-3.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{}{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{}{{\n{}{}}}{\n{}{}}\n{{}{{}}}{{}{}}{}{{}{}}\n{\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-47.44493pt}{0.0pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{$\\scriptscriptstyle\\definecolor{currentcolor}{rgb}{0,0,0}\\mathchoice{\\hbox{\\pagecolor{OliveGreen!17}$\\displaystyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\textstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}{\\hbox{\\pagecolor{OliveGreen!17}$\\scriptscriptstyle\\mathstrut\\tilde{\\mathbf{g}}\\!\\big(\\bm{\\theta};\\,\\mathbf{x}_{n}\\big)$}}$}}\n}}\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n\\pgfsys@invoke{ }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{ }\\pgfsys@endscope\\hbox to0.0pt{}{{\n{}{}{}}}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}.\n(10)\nprojected, normalized test gradient\nprojected, normalized training gradient\nThis keeps compute\n\n\n(\nD\n\n\nlog\n\nD\n\n)\n\\mathcal{O}(D^{\\prime}\\log D^{\\prime})\nfor projection and\n\n\n(\nD\n\n)\n\\mathcal{O}(D^{\\prime})\nper dot product, with storage\n\n\n(\n|\n\n|\n\nD\n\n)\n\\mathcal{O}(|\\mathcal{D}|\\,D^{\\prime})\n, while staying close to the ranking behavior of full-gradient cosine similarity\n(\npark2023trak\n)\n.\nFigure 1\n:\nMotive\n.\nTop.\nMotion-gradient computation (搂\n3.4\n) has three steps: (1) detect motion with AllTracker; (2) compute motion-magnitude patches; (3) apply loss-space motion masks to focus gradients on dynamic regions.\nBottom.\nOur method (搂\n3.2\n) is made scalable via a single-sample variant with common randomness and a projection, computed for each pair of training and query data, aggregated (搂\n3.5\n) for a final ranking, and eventually used to select fine-tuning subsets.\n3.3\nVideo-specific Frame-length Bias Fix\nRaw gradient magnitudes depend on the number of frames\nF\nF\nin the video\n\n\\mathbf{v}\n, thereby biasing scores toward longer videos.\nWe correct this by normalizing for frame count before the projectionnormalization step:\n\n\n\ndiff\n\n(\n\n;\n\n,\nt\nfix\n,\n系\nfix\n)\n\n1\nF\n\n\n\n\ndiff\n\n(\n\n;\n\n,\nt\nfix\n,\n系\nfix\n)\n.\n\\smash{\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{v},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\;\\leftarrow\\;\\frac{1}{F}\\,\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\,\\mathbf{v},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}}).}\n(11)\nWe still apply\n\n2\n\\ell_{2}\nnormalization in Eq.\n10\n, further stabilizing scales across examples. Together, single-timestep, common randomness, projection, and frame-length correction form a compact, scalable estimator that we use throughout.\nHowever, na茂ve video-level attribution conflates appearance with motion, often ranking clips high just because they share backgrounds or objects, while offering little insight into dynamics.\n3.4\nMotion Attribution\nTo move beyond whole-video influence, we introduce motion attribution, which isolates the contribution of training data to temporal dynamics. Unlike video-level attribution, which treats each clip as a single unit and conflates appearance with motion, motion attribution reweights per-location gradients using motion masks, assigning influence via dynamic behavior rather than static content.\nMotion Masking Attribution.\nMotion is what distinguishes video diffusion from image diffusion.\nOur goal is to understand how training data shapes motion in video diffusion models.\nPrior work has emphasized architectural or algorithmic changes for motion modeling\n(\npeebles2023scalable\n;\nblattmann2023stable\n;\nguo2023animatediff\n)\n, many of the largest generative gains have instead come from scaling and curating massive video corpora, which in turn enable impressive motion synthesis results in video diffusion models\n(\nho2022video\n;\nwan2025\n;\ntan2024vidgen\n;\nyang2024cogvideox\n)\n. Yet we lack tools that quantify how specific training clips shape particular motion patterns. We address this by attributing motion back to data via motion-weighted gradients, which yields actionable signals for targeted data selection, artifact diagnosis, and selective fine-tuning.\nMotion Detection and Latent Space Mapping.\nGiven a video\n\n\n\nF\n\nH\n\nW\n\n3\n\\mathbf{v}\\in\\mathbb{R}^{F\\times H\\times W\\times 3}\nwith\nF\nF\nframes of resolution\nH\n\nW\nH\\times W\n, we first encode it into the VAE latent space as\n\n=\nE\n\n(\n\n)\n\n\nF\n\nH\n/\ns\n\nW\n/\ns\n\nC\n\\mathbf{h}=E(\\mathbf{v})\\in\\mathbb{R}^{F\\times\\nicefrac{{H}}{{s}}\\times\\nicefrac{{W}}{{s}}\\times C}\n, with downsampling factor\ns\n=\n8\ns=8\nand\nC\n=\n16\nC=16\nfollowing the\nWan2.1\nbackbone used in our experiments.\nFor motion computation, we use AllTracker\n(\nharley2025alltracker\n)\nto extract motion information in pixel space:\nA\n=\n\n\n(\n\n)\n\n\nF\n\nH\n\nW\n\n4\nA=\\mathcal{A}(\\mathbf{v})\\in\\mathbb{R}^{F\\times H\\times W\\times 4}\n, where the first two channels contain optical flow maps\nA\n:\n,\n:\n,\n:\n,\n0\n:\n2\nA_{:,:,:,0:2}\nindicating pixel displacement between frames, and the remaining channels\nA\n:\n,\n:\n,\n:\n,\n2\n:\n4\nA_{:,:,:,2:4}\nencode visibility and confidence scores. We extract displacement vectors at each pixel location as:\n\nf\n\n(\nh\n,\nw\n)\n=\n(\nA\nf\n,\nh\n,\nw\n,\n0\n,\nA\nf\n,\nh\n,\nw\n,\n1\n)\n=\n(\nd\n\nw\n,\nd\n\nh\n)\n.\n\\smash{\\mathbf{D}_{\\!f}(h,w)=(A_{f,h,w,0},A_{f,h,w,1})=(\\mathrm{d}w,\\mathrm{d}h).}\\vskip-5.49698pt\n(12)\nWe then bilinearly downsample motion quantities from\n(\nH\n,\nW\n)\n(H,W)\nto the latent grid\n(\nH\ns\n,\nW\ns\n)\n\\big(\\tfrac{H}{s},\\tfrac{W}{s}\\big)\nso that our masking lives where gradients are computed.\nMotion-Weighted Gradient Computation.\nWe define the motion magnitude at each location as:\nM\nf\n\n(\nh\n,\nw\n)\n=\n\n\nf\n\n(\nh\n,\nw\n)\n\n2\nM_{\\!f}(h,w)\\!=\\!\\|\\mathbf{D}_{\\!f}(h,w)\\|_{2}\n.\nTo obtain comparable motion weights across frames and pixels, we minmax normalize over all frames and pixels, ensuring values lie in\n[\n0\n,\n1\n]\n[0,1]\n:\n\n\n(\nf\n,\nh\n,\nw\n)\n=\nM\nf\n\n(\nh\n,\nw\n)\n\nmin\nf\n\n,\nh\n\n,\nw\n\n\nM\nf\n\n\n(\nh\n\n,\nw\n\n)\nmax\nf\n\n,\nh\n\n,\nw\n\n\nM\nf\n\n\n(\nh\n\n,\nw\n\n)\n\nmin\nf\n\n,\nh\n\n,\nw\n\n\nM\nf\n\n\n(\nh\n\n,\nw\n\n)\n+\n味\n,\n\\mathbf{W}(f,h,w)\\penalty 10000\\ =\\penalty 10000\\ \\frac{M_{\\!f}(h,w)-\\min_{f^{\\prime},h^{\\prime},w^{\\prime}}M_{\\!f^{\\prime}}(h^{\\prime},w^{\\prime})}{\\max_{f^{\\prime},h^{\\prime},w^{\\prime}}M_{\\!f^{\\prime}}(h^{\\prime},w^{\\prime})-\\min_{f^{\\prime},h^{\\prime},w^{\\prime}}M_{\\!f^{\\prime}}(h^{\\prime},w^{\\prime})+\\zeta},\\vskip-2.75269pt\n(13)\nwhere\n味\n=\n10\n\n6\n\\zeta=10^{-6}\nensures a positive denominator.\nThis normalization mitigates bias from absolute motion scale, yielding weights that emphasize relative motion saliency rather than raw magnitude, following prior practice in video saliency detection\n(\nfang2013video\n)\n.\nLet\n(\nh\n~\n,\nw\n~\n)\n(\\tilde{h},\\tilde{w})\nindex the latent grid. We obtain latent-aligned weights by bilinear downsampling:\n\n~\n\n(\nf\n,\nh\n~\n,\nw\n~\n)\n=\nBilinear\n\n(\n\n\n(\n\n,\n\n,\n\n)\n,\nF\n,\nH\ns\n,\nW\ns\n)\n.\n\\smash{\\tilde{\\mathbf{W}}(f,\\tilde{h},\\tilde{w})\\penalty 10000\\ =\\penalty 10000\\ \\operatorname{Bilinear}\\!\\left(\\mathbf{W}(\\cdot,\\cdot,\\cdot),F,\\penalty 10000\\ \\tfrac{H}{s},\\penalty 10000\\ \\tfrac{W}{s}\\right).}\n(14)\nWe compute per-location squared error at fixed\n(\nt\nfix\n,\n系\nfix\n)\n(t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\nat each frame\nf\nf\nand latent pixel\n(\nh\n~\n,\nw\n~\n)\n(\\tilde{h},\\tilde{w})\n:\n\n~\n\n,\n\n,\n\n\n(\nf\n,\nh\n~\n,\nw\n~\n)\n=\n(\n[\n系\n\n\n(\n\n\n(\n\n,\nt\nfix\n,\n系\nfix\n)\n,\nt\nfix\n,\n\n)\n]\nf\n,\nh\n~\n,\nw\n~\n\n[\n系\ntarget\n\n(\nt\nfix\n,\n系\nfix\n)\n]\nf\n,\nh\n~\n,\nw\n~\n)\n2\n,\n\\smash{\\tilde{\\mathcal{L}}_{\\bm{\\theta},\\mathbf{v},\\mathbf{c}}(f,\\tilde{h},\\tilde{w})\\penalty 10000\\ =\\penalty 10000\\ \\left(\\,[\\bm{\\epsilon}_{\\bm{\\theta}}(\\mathbf{z}(\\mathbf{v},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}}),t_{\\textnormal{fix}},\\mathbf{c})]_{f,\\tilde{h},\\tilde{w}}-[\\bm{\\epsilon}_{\\textnormal{target}}(t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})]_{f,\\tilde{h},\\tilde{w}}\\,\\right)^{2},}\\vskip-2.75269pt\n(15)\nand define the motion-weighted loss by averaging over frames and latent spatial locations:\n\nmot\n\n(\n\n;\n\n,\n\n)\n=\n1\nF\n\n\nmean\nf\n,\nh\n~\n,\nw\n~\n\n[\n\n~\n\n,\n\n\n(\nf\n,\nh\n~\n,\nw\n~\n)\n\n\n~\n\n,\n\n,\n\n\n(\nf\n,\nh\n~\n,\nw\n~\n)\n]\n.\n\\smash{\\mathcal{L}_{\\textnormal{mot}}(\\bm{\\theta};\\mathbf{v},\\mathbf{c})\\penalty 10000\\ =\\penalty 10000\\ \\frac{1}{F_{\\mathbf{v}}}\\operatorname{mean}_{f,\\tilde{h},\\tilde{w}}\\left[\\tilde{\\mathbf{W}}_{\\mathbf{v},\\mathbf{c}}(f,\\tilde{h},\\tilde{w})\\cdot\\tilde{\\mathcal{L}}_{\\bm{\\theta},\\mathbf{v},\\mathbf{c}}(f,\\tilde{h},\\tilde{w})\\right].}\\vskip-2.75269pt\n(16)\nNotably, when\n\n~\n\\tilde{\\mathbf{W}}\nis all ones, this recovers the standard objective with no motion emphasis. The\n1\n/\nF\n\n\\nicefrac{{1}}{{F_{\\mathbf{v}}}}\nfactor corrects for frame-length bias and\nF\n\nF_{\\mathbf{v}}\nsignifies how the number of frames may be video-dependent. The corresponding motion-weighted gradient for attribution is:\nI\nmot\n\n(\n\nn\n,\n\n^\n)\n=\n\n~\nmot\n\n(\n\n,\n\n^\n)\n\n\n\n~\nmot\n\n(\n\n,\n\nn\n)\n,\nwhere\n\n\n~\nmot\n\n(\n\n,\n\n)\n:=\n\nmot\n\n(\n\n,\n\n,\nt\nfix\n,\n系\nfix\n)\n\n\nmot\n\n(\n\n,\n\n,\nt\nfix\n,\n系\nfix\n)\n\n\nand\n\n\nmot\n:=\n\n\n\nmot\n.\n\\!\\!\\!\\!I_{\\textnormal{mot}}\\!(\\mathbf{v}_{n},\\!\\hat{\\mathbf{v}})\\!=\\!\\tilde{\\mathbf{g}}_{\\textnormal{mot}}\\!(\\bm{\\theta}\\!,\\!\\hat{\\mathbf{v}})^{\\!\\top}\\!\\tilde{\\mathbf{g}}_{\\textnormal{mot}}\\!(\\bm{\\theta}\\!,\\!\\mathbf{v}_{n}),\\textnormal{ where }\\tilde{\\mathbf{g}}_{\\textnormal{mot}}\\!(\\bm{\\theta}\\!,\\!\\mathbf{v})\\!:=\\!\\frac{\\mathbf{P}\\mathbf{g}_{\\textnormal{mot}}\\!(\\bm{\\theta}\\!,\\!\\mathbf{v}\\!,\\!t_{\\textnormal{fix}},\\!\\bm{\\epsilon}_{\\textnormal{fix}})}{\\|\\mathbf{P}\\mathbf{g}_{\\textnormal{mot}}\\!(\\bm{\\theta}\\!,\\!\\mathbf{v}\\!,\\!t_{\\textnormal{fix}},\\!\\bm{\\epsilon}_{\\textnormal{fix}})\\|}\\textnormal{ and }\\mathbf{g}_{\\textnormal{mot}}\\!:=\\!\\nabla_{\\!\\bm{\\theta}}\\mathcal{L}_{\\textnormal{mot}}.\\vskip-2.75269pt\n(17)\nLoss-space masking leaves forward noising and generation unchanged and reweights only attribution, avoiding interactions between motion weighting and noise injection. In contrast, our motion-aware attribution emphasizes dynamic regions and de-emphasizes static backgrounds, so rankings identify training clips that most strongly shape the models motion rather than appearance.\n3.5\nMost Influential Fine-tuning Subset Selection\nGoal.\nGiven a query clip\n(\n\n^\n,\n\n^\n)\n(\\hat{\\mathbf{v}},\\hat{\\mathbf{c}})\n, we compute a motion-aware attribution\nvalue for each candidate fine-tuning example\n(\n\nn\n,\n\nn\n)\n\n\nft\n(\\mathbf{v}_{n},\\mathbf{c}_{n})\\in\\mathcal{D}_{\\textnormal{ft}}\nusing:\nI\nmot\n\n(\n\nn\n,\n\n^\n)\nI_{\\textnormal{mot}}\\!\\left(\\mathbf{v}_{n},\\,\\hat{\\mathbf{v}}\\right)\nfrom  Eq.\n17\n. Then, we construct a fine-tuning dataset\n\n\\mathcal{S}\nfor one or many query videos\n\n^\n\\hat{\\mathbf{v}}\n.\nSingle-query-point fine-tuning selection.\nFor a budget of\nK\nK\ndata points, we select the\nK\nK\nhighest-scoring examples.\nIn practice,\nK\nK\nis chosen as a percentile of the dataset size (e.g., top 110%), ensuring the subset scales consistently across datasets.\nMulti-query-point fine-tuning selection: aggregating attribution scores.\nFor\nQ\nQ\nqueries, we adopt the majority voting approach from ICONS\n(\nwu2024icons\n)\nand aggregate motion-aware influence scores across queries by percentile thresholding and voting. A sample receives a vote if the score is above the percentile cutoff\n\n\\tau\nfor that query.\nThe consensus score of a candidate\n\nn\n\\mathbf{v}_{n}\nis the total number of queries that vote for it. We then rank all training samples by\nMajVote\n\n(\n\nn\n)\n\\operatorname{MajVote}(\\mathbf{v}_{n})\nand select the top-\nK\nK\nto form the fine-tuning subset. This formulation emphasizes samples that are consistently influential across multiple queries, without requiring cross-query calibration of raw scores:\nMajVote\nn\n=\n\nq\n=\n1\nQ\n\n\n[\nI\nmot\n\n(\n\nn\n,\n\n^\nq\n)\n>\n\n]\n,\n\nvote\n\n(\nK\n)\n=\n{\n\nn\n|\n\nn\n\nin top-\n\nK\n\nby\n\nMajVote\n}\n.\n\\operatorname{MajVote}_{n}\\!=\\!\\sum\\nolimits_{q=1}^{Q}\\!\\mathbb{I}\\!\\big[\\,I_{\\textnormal{mot}}(\\mathbf{v}_{n},\\hat{\\mathbf{v}}_{q})\\!>\\!\\tau\\,\\big],\\mathcal{S}_{\\textnormal{vote}}(K)\\!=\\!\\bigl\\{\\mathbf{v}_{n}|\\mathbf{v}_{n}\\text{ in top-}K\\text{ by }\\operatorname{MajVote}\\bigr\\}.\n(18)\n3.6\nComputational Efficiency Analysis\nGradient Compute.\nNa茂vely averaging over timesteps and noise for every example costs\n\n\n(\n|\n\n|\n\n|\n\n|\n\nB\n)\n\\mathcal{O}(|\\mathcal{D}|\\,|\\mathcal{T}|\\,B)\n, where\nB\nB\nis a single forward+backward cost and\n|\n\n|\n|\\mathcal{T}|\nis the number of sampled\nt\n,\n系\nt,\\bm{\\epsilon}\nper data.\nUsing a single sample reduces this to\n\n\n(\n|\n\n|\n\nB\n)\n\\mathcal{O}(|\\mathcal{D}|\\,B)\n, which is key to keeping the cost reasonable for modern video datasets and models, while reusing a sample across data allows low enough variance for stable rankings. Projection adds\n\n\n(\nD\n\n\nlog\n\nD\n\n)\n\\mathcal{O}(D^{\\prime}\\log D^{\\prime})\nper example using Fastfood\n(\nle2014fastfood\n)\n, negligible relative to a backward pass.\nGradient Storage.\nStoring full gradients is\n\n\n(\n|\n\n|\n\nD\n)\n\\mathcal{O}(|\\mathcal{D}|\\,D)\n. We instead store only projected vectors,\n\n\n(\n|\n\n|\n\nD\n\n)\n\\mathcal{O}(|\\mathcal{D}|\\,D^{\\prime})\n, plus the structured Fastfood state,\n\n\n(\nD\n)\n\\mathcal{O}(D)\n. Since\nD\n\nD^{\\prime}\nis typically orders of magnitude smaller than\nD\nD\n, this transformation makes storage tractable for billion-parameter models.\nData Ranking Compute.\nInfluence computation in  Eq.\n10\nis an inner product in\n\nD\n\n\\mathbb{R}^{D^{\\prime}}\n, so evaluating all train examples against a query is\n\n\n(\n|\n\n|\n\nD\n\n)\n\\mathcal{O}(|\\mathcal{D}|\\,D^{\\prime})\n, and sorting is\n\n\n(\n|\n\n|\n\nlog\n\n|\n\n|\n)\n\\mathcal{O}(|\\mathcal{D}|\\log|\\mathcal{D}|)\n.\nAdditional Motion-Emphasis Compute.\nMotion-specific overhead primarily stems from AllTracker mask extraction with complexity\n\n\n(\n|\n\n|\n\nH\n\nW\n\nF\n)\n\\mathcal{O}(|\\mathcal{D}|\\cdot H\\cdot W\\cdot F)\nfor clip length\nF\nF\nand frame resolution\nH\n\nW\nH\\times W\n. Masks are extracted once, cached, and negligible relative to gradient cost. We provide a detailed runtime breakdown in App.\nG.1\n.\n4\nExperiment\n4.1\nSetup\nFigure 2\n:\nMotion attribution examples.\nTop\n: Query clips showing float (\nleft\n) and roll (\nright\n) motions.\nMiddle\n:\nTop-ranked positive training samples identified by\nMotive\nwith high influence scores.\nBottom\n: Negative influence samples with minimal, camera-only motion, or cartoon-style content that conflict with target motions.\nFine-tuning Datasets.\nWe evaluate our motion attribution framework on two large-scale video datasets:\nVIDGEN-1M\n(\ntan2024vidgen\n)\nand\n4DNeX-10M\n(\nchen20254dnex\n)\n, both of which offer diverse motion patterns, rich temporal dynamics, and complex scenes. For our experiments, we use 10k videos from both datasets, which provide sufficient scale and diversity to thoroughly evaluate motion attribution methods across different temporal patterns and video generation scenarios.\nMotion Query Data.\nTo evaluate our motion attribution, we curate a set of query videos representing distinct motion patterns and scenarios. Our query dataset consists of videos spanning\n10\n10\nmotion categories, with a focus on object dynamics: compress, bounce, roll, explode, float, free fall, slide, spin, stretch, swing.\nFive videos, totaling\n50\n50\nqueries, represent each motion type. These videos are chosen for their clear, isolated motions, serving as a basis for evaluating attribution quality and downstream motion generation. Further details on query video curation are provided in App.\nF.2\n.\nModel & Baselines.\nAll experiments primarily use pretrained\nWan2.1-T2V-1.3B\n, a widely used open-source baseline with strong performance and feasible compute. We provide additional results on\nWan2.2-TI2V-5B\nin App.\nC\n. Our baselines:\nBase model\n(pretrained, no fine-tuning);\nFull fine-tuning\n(approximate upper bound using the complete dataset);\nRandom selection\n(uniform sampling);\nMotion magnitude\n(selects videos with the highest average motion magnitude);\nV-JEPA embeddings\n(selects most representative videos of motion patterns using self-supervised spatiotemporal V-JEPA\n(\nassran2025v\n)\nfeatures, capturing high-level motion semantics);\nand\nOurs w/o motion masking\n(influence of the entire video level without motion-specific masking).\nBenchmark.\nWe evaluate our attribution using VBench\n(\nhuang2024vbench\n)\nmetrics across six dimensions: subject consistency, background consistency, motion smoothness, dynamic degree, aesthetic quality, and imaging quality. Motion smoothness and dynamic degree are our primary targets for temporal dynamics, while other metrics ensure visual quality is maintained. We use custom evaluation prompts, following VBenchs descriptive style, designed for ten motion types (bounce, compress, explode, float, free fall, roll, slide, spin, stretch, swing), to assess our frameworks effectiveness for specific physical motions, with five prompts per motion type.\nImplementation Details.\nWe finetune\nWan2.1-T2V-1.3B\nwith our\nMotive\n-selected high-quality video data following the official and DiffSynth-Studio implementation.\nDuring fine-tuning, we update only the DiT backbone while freezing the T5 text encoder and VAE. All models are trained at a resolution of\n480\n\n832\n480\\times 832\nwith a learning rate of\n1\n\n10\n\n5\n1\\times 10^{-5}\n.\nSpecialist models are trained on single motion category selected data, while generalist models use aggregated selections (both with top\n10\n%\n10\\%\nselection from\nVIDGEN-1M\n(\ntan2024vidgen\n)\nor\n4DNeX\n(\nchen20254dnex\n)\nwith motion-weighted loss attribution). All training runs are conducted on\n4\n4\n-\n8\n8\nNVIDIA A100 GPUs.\nWith one A100 GPU, it takes approximately\n150\n150\nhours to compute the influence score of 10k samples. This process is highly parallelizable, on\n64\n64\nGPUs, taking\n\n2.3\n\\sim 2.3\nhours. The computed gradients can be reused for multiple selection queries, amortizing this one-time cost.\n4.2\nMain Results\nFigure 3\n:\nQualitative Comparisons.\nWe compare four motion scenarios (compress, spin, slide, free fall) across the base model, random selection, and our method. Our approach yields more realistic motion dynamics. Supplementary videos are included.\nMethod\nSubject Consist.\nBackground Consist.\nMotion Smooth.\nDynamic Degree\nAesthetic Quality\nImaging Quality\nBase\n95.3\n96.4\n96.3\n39.6\n45.3\n65.7\nFull fine-tuning\n95.9\n96.6\n96.3\n42.0\n45.0\n63.9\nRandom selection\n95.3\n96.6\n96.3\n41.3\n45.7\n65.1\nMotion magnitude\n95.6\n96.2\n95.7\n40.1\n45.1\n63.2\nV-JEPA embedding\n95.7\n96.0\n95.6\n41.6\n44.9\n62.7\nOurs w/o MM\n95.4\n96.1\n96.3\n43.8\n45.7\n63.2\nOurs (\nMotive\n)\n96.3\n96.1\n96.3\n47.6\n46.0\n64.6\nTable 1\n:\nVBench Evaluation.\nPerformance comparison on VBench\n(\nhuang2024vbench\n)\nacross different baselines (all values in\n%\n\\%\n, higher is better). All selection methods use\n10\n%\n10\\%\nof training data; our method uses majority vote aggregation (搂\n3.5\n) across motion queries. MM: motion masking.\nHigh-influence selection and negative filtering.\nFig.\n2\nshows our motion-aware attribution ranks clips with clear, physically grounded dynamics and downranks those with little transferable motion. For rolling and floating, positives show continuous trajectories and smooth temporal evolution (turbulent water carrying objects; planetary rotation). Negatives are mostly static footage, camera-only motion, or cartoons whose simplified kinematics do not transfer. Our method promotes useful motions and filters data that dilutes temporal learning during fine-tuning, holding across categories and aligning with the following quantitative gains.\nQualitative improvements across motion types.\nFig.\n3\ncompares the base pretrained model, na茂ve motion fine-tuning, and our motion-aware data selection for fine-tuning across four scenarios. Top: rubber-ball compression and coin spinning. Bottom: coffee mug sliding and red ball drop. Our method yields higher motion fidelity and temporal consistency than both baselines, especially for complex deformation, rotational dynamics, and physics-driven motion.\nQuantitative Results.\nWe evaluate our approach across different metrics using VBench\n(\nhuang2024vbench\n)\n, demonstrating consistent improvements in motion fidelity when fine-tuning with attribution-selected data compared to random sampling or na茂ve approaches.\nAs shown in Tab.\n1\n,\nMotive\nachieves the highest dynamic degree score (\n47.6\n%\n47.6\\%\n), significantly outperforming random selection (\n41.3\n%\n41.3\\%\n) and whole video attribution (\n43.8\n%\n43.8\\%\n). Our method also excels in subject consistency (\n96.3\n%\n96.3\\%\n) and aesthetic quality (\n46.0\n%\n46.0\\%\n), while maintaining competitive motion smoothness (\n96.3\n%\n96.3\\%\n). Notably, using only\n10\n%\n10\\%\nof the training data, our approach surpasses the full fine-tuned model on dynamic degree (\n42.0\n%\n42.0\\%\n) and subject consistency (\n95.9\n%\n95.9\\%\n), demonstrating the superior empirical performance of motion-specific attribution for targeted fine-tuning. We further analyze the motion magnitude distribution of selected videos in App.\nD\n.\n4.3\nHuman Evaluation\nMethod\nWin (\n%\n\\%\n)\nTie (\n%\n\\%\n)\nLoss (\n%\n\\%\n)\nOurs vs. Base\n74.1\n74.1\n12.3\n12.3\n13.6\n13.6\nOurs vs. Random\n58.9\n58.9\n12.1\n12.1\n29.0\n29.0\nOurs vs. Full FT\n53.1\n53.1\n14.8\n14.8\n32.1\n32.1\nOurs vs. w/o MM\n46.9\n46.9\n20.0\n20.0\n33.1\n33.1\nTable 2:\nHuman evaluation.\nPairwise comparisons across\n50\n50\nvideos with\n17\n17\nparticipants (\n850\n850\ntotal). Win, tie, and loss rates show where our method is preferred, rated equal, or outperformed.\nAutomated scores can miss perceptual motion quality, so we run a human evaluation pairwise comparison protocol: participants view two generated videos and choose which shows better motion. We recruit\n17\n17\nannotators and evaluate\n10\n10\nmotion categories. For each category, we prepare\n5\n5\ntest cases and compare our method to baselines across three pairings, yielding a balanced set of judgments. Presentation order is randomized, and ties are allowed. We report win rate (fraction our method is preferred), tie rate, and overall preference. As shown in the table, annotators favor our attribution-guided selection:\n74.1\n%\n74.1\\%\nwin rate vs. the base model and\n53.1\n%\n53.1\\%\nvs. the full fine-tuned model, showing perceptually meaningful motion improvements.\n4.4\nAblations\nSingle-timestep attribution.\nUsing a single timestep avoids the cost of averaging across timesteps while closely matching the multi-timestep baseline. With a fixed\nt\n=\n751\nt\\!=\\!751\n(the midpoint of the\n1000\n1000\n-step denoising trajectory), we obtain\n\n=\n66\n%\n\\rho\\!=\\!66\\%\nagreement with the ground truth computed using\n10\n10\nevenly-spaced timesteps sampled according to the flow matching schedule.\nUsing the same timestep for both training and testing is key to preserving relative rankings. High timesteps (early denoising) heavily corrupt inputs with noise, obscuring motion cues; low timesteps (late denoising) operate on nearly formed videos, where gradients reflect fine details rather than semantic structure.\nt\n=\n751\nt\\!=\\!751\n(mid-denoising) strikes a balance, delivering high correlation and substantial compute savings. Averaging multiple timesteps yields minimal ranking gains, and incorporating late-timestep gradients can bias rankings. A single fixed timestep is therefore sufficient for variance-reduced, scalable attribution.\nSpearman Correlation\n\n\\rho\nProjection Dimension\nFigure 4\n:\nProjection dimension analysis.\nSpearman correlation between projected and full gradients shows rapid improvement with projection dimension, with\n512\n512\nproviding a strong trade-off between accuracy and efficiency.\nProjected Gradients Preserve Influence Rankings.\nComparing full gradients for attribution is infeasible at a billion-parameter scale. We reduce dimensionality with structured random projections that preserve influence geometry, ablating\nD\n\n\n{\n128\n,\n\n,\n2048\n}\nD^{\\prime}\\!\\in\\!\\{128,\\dots,2048\\}\nagainst the full-gradient baseline. We assess ranking preservation via Spearman correlation with unprojected scores (Fig.\n4\n). Small projections preserve rankings poorly:\nD\n\n=\n128\nD^{\\prime}\\!=\\!128\nyields\n\n=\n46.9\n%\n\\rho\\!=\\!46.9\\%\n. Preservation improves with size:\nD\n\n=\n512\nD^{\\prime}\\!=\\!512\nreaches\n\n=\n74.7\n%\n\\rho\\!=\\!74.7\\%\n. Beyond that, gains are marginal while cost rises:\nD\n\n=\n1024\nD^{\\prime}\\!=\\!1024\n(\n\n=\n75.7\n%\n\\rho\\!=\\!75.7\\%\n) and\nD\n\n=\n2048\nD^{\\prime}\\!=\\!2048\n(\n\n=\n76.1\n%\n\\rho\\!=\\!76.1\\%\n). Thus,\nD\n\n=\n512\nD^{\\prime}\\!=\\!512\noffers the best trade-off, scaling to large models while maintaining quality.\nFrame-Length Normalization.\nAs in the Wan training protocol, we standardize all videos to 81 frames at 16 fps (satisfying the\n4\n\nn\n+\n1\n4n{+}1\nconstraint) for fair attribution across clips of different raw lengths. Without standardization, gradient-based scores correlate strongly with video length rather than motion quality (\n\n=\n78.0\n%\n\\rho\\!=\\!78.0\\%\n), leading to longer clips ranking higher regardless of dynamics. Standardizing frames reduces spurious length correlations by\n54.0\n%\n54.0\\%\nwhile preserving motion-based correlation, so rankings reflect motion rather than duration.\nAs in Fig.\n5\n, normalization clarifies motion-specific patterns. For floating queries with frame-length normalization (left), top-ranked samples consistently show wave dynamics, floating objects, and surfing, all matching the target motion. Without normalization (right), top samples lack coherent similarity because rankings are driven by clip length, harming motion-relevant training example identification.\nFigure 5\n:\nImpact of Frame-Length Normalization on Motion Attribution.\nComparison of top-ranked samples for floating motion query.\nLeft\n: With proper frame-length normalization, top samples consistently exhibit floating motion (waves, floating objects, surfing).\nRight\n: Without normalization, rankings are biased by video length, resulting in no coherent patterns among top samples.\n5\nConclusion\nWe address a central and underexplored question in video diffusion: which training clips influence the motion in generated videos? We propose\nMotive\n, which traces generated dynamics back to influential training clips by isolating motion-specific gradients. Unlike image-based attribution, our method directly targets temporal dynamics, revealing how coherence and physical plausibility emerge from data. We show motion learning is traceable to specific examples, providing a quantitative tool for diagnosing artifacts and enabling targeted data selection and curation, enabling more controllable and interpretable video diffusion models. As models scale, such data-level understanding will be essential for building robust and reliable generative systems.\nLimitations.\n(i) Motion saliency depends on the chosen tracker; severe occlusions or transparency can degrade masks. (ii) Camera-only motion and very subtle micro-motion remain challenging to separate without extra signals (e.g., camera pose). Further discussion is in App.\nG\n.\nAcknowledgements\nWe thank the following people (listed alphabetically by last name) for their helpful discussions, feedback, or participation in human studies: Allison Chen, Sanghyuk Chun, Amaya Dharmasiri, Xingyu Fu, Will Hwang, Yifeng Jiang, Amogh Joshi, Chen-Hsuan Lin, Huan Ling, Tiffany Ling, Shaowei Liu, Zhengyi Luo, Rafid Mahmood, Kaleb S. Newman, Julian Ost, Zeeshan Patel, Davis Rempe, Anya Tsvetkov, Esin Tureci, Sheng-Yu Wang, Tingwu Wang, Zian Wang, Hongyu Wen, Jon Williams, Donglai Xiang, Yilun Xu, William Yang, and Haotian Zhang.\nReferences\nAppendix A\nNotation\nTable 3\n:\nGlossary and notation.\nSymbol\nDescription\nAcronyms and Basic Notation\nVAE\nVariational Autoencoder\nDiT\nDiffusion Transformer backbone\n\n\\mathbf{I}\nIdentity matrix\nVideo Generation\np\n\n\n(\n\n\n\n)\np_{\\bm{\\theta}}(\\mathbf{v}\\mid\\mathbf{c})\nConditional video generator with parameters\n\n\\bm{\\theta}\n\n\n\nF\n\nH\n\nW\n\n3\n\\mathbf{v}\\in\\mathbb{R}^{F\\times H\\times W\\times 3}\nVideo clip with frames\nF\nF\n, height\nH\nH\n, width\nW\nW\n\n\\mathbf{c}\nConditioning signal such as text or multimodal metadata\n\n\\bm{\\theta}\nTrainable model parameters\nf\n\n{\n1\n,\n\n,\nF\n}\nf\\in\\{1,\\ldots,F\\}\nFrame index\nh\n\n{\n1\n,\n\n,\nH\n}\nh\\in\\{1,\\ldots,H\\}\n,\nw\n\n{\n1\n,\n\n,\nW\n}\nw\\in\\{1,\\ldots,W\\}\nSpatial indices for height and width respectively\nh\n~\n\\tilde{h}\n,\nw\n~\n\\tilde{w}\nLatent grid indices\nDatasets\n\n=\n{\n(\n\nn\n,\n\nn\n)\n}\nn\n=\n1\nN\n\\mathcal{D}=\\{(\\mathbf{v}_{n},\\mathbf{c}_{n})\\}_{n=1}^{N}\nTraining corpus with size\nN\nN\nand index\nn\nn\n\nft\n\n\n\\mathcal{D}_{\\textnormal{ft}}\\subseteq\\mathcal{D}\nFine-tuning dataset\n\n\n\n\\mathcal{S}\\subseteq\\mathcal{D}\nSelected influential subset\nk\n\n{\n1\n,\n\n,\nK\n}\nk\\in\\{1,\\ldots,K\\}\nThe selected subset size\nQ\nQ\nNumber of query data\nq\n\n{\n1\n,\n\n,\nQ\n}\nq\\in\\{1,\\ldots,Q\\}\nQuery index\n\n\\mathbf{x}\nGeneric input data pair\n\ntest\n\\mathbf{x}_{\\textnormal{test}}\n,\n\nn\n\\mathbf{x}_{n}\nTest/query pair and training pair\n\n^\n\\hat{\\mathbf{v}}\n,\n\n^\n\\hat{\\mathbf{c}}\nQuery video and its conditioning\nLatent Space and Diffusion Components\nE\nE\nVAE encoder\n\n=\nE\n\n(\n\n)\n\n\nF\n\n(\nH\n/\ns\n)\n\n(\nW\n/\ns\n)\n\nC\n\\mathbf{h}=E(\\mathbf{v})\\in\\mathbb{R}^{F\\times(H/s)\\times(W/s)\\times C}\nLatent video with spatial factor\ns\ns\nand channels\nC\nC\n\n\\mathbf{z}\nNoisy latent variable used in diffusion or flow matching\n系\n\n\n\n(\n0\n,\n\n)\n\\bm{\\epsilon}\\sim\\mathcal{N}(0,\\mathbf{I})\nGaussian noise\n系\n\n\n(\n\n,\n\n,\nt\n)\n\\bm{\\epsilon}_{\\bm{\\theta}}(\\mathbf{z},\\mathbf{c},t)\nPredicted noise network in diffusion training\n\n\n\n(\n\n,\n\n,\nt\n)\n\\mathbf{f}_{\\bm{\\theta}}(\\mathbf{z},\\mathbf{c},t)\nTime-indexed vector field in flow matching\n\n\n\\dot{\\mathbf{z}}\nTime derivative of the latent trajectory\n伪\nt\n,\n\nt\n\\alpha_{t},\\sigma_{t}\nScheduler signal and noise scales at timestep\nt\nt\n系\ntarget\n\\bm{\\epsilon}_{\\textnormal{target}}\nTarget noise or velocity used for supervision\nt\n\n{\n1\n,\n\n,\nT\n}\nt\\in\\{1,\\ldots,T\\}\nDiffusion or flow-matching timestep, with total timesteps\nT\nT\nt\nfix\nt_{\\textnormal{fix}}\n,\n系\nfix\n\\bm{\\epsilon}_{\\textnormal{fix}}\nFixed timestep and shared noise draw used for low-variance gradients\nTable 4\n:\nGlossary and notation (continued).\nSymbol\nDescription\nAttribution and Influence\nI\n\n(\n\nn\n,\n\n^\n;\n\n)\nI(\\mathbf{v}_{n},\\hat{\\mathbf{v}};\\bm{\\theta})\nInfluence score between a train clip and a query clip\nI\nmot\n\n(\n\nn\n,\n\n^\n;\n\n)\nI_{\\textnormal{mot}}(\\mathbf{v}_{n},\\hat{\\mathbf{v}};\\bm{\\theta})\nMotion-aware influence score\nTopK\n\n(\n\n)\n\\operatorname{TopK}(\\cdot)\nTop-\nK\nK\noperator for selecting highest scores\nMajVote\n\n(\n\n)\n\\operatorname{MajVote}(\\cdot)\nMajority-vote aggregation across queries\n\n\\tau\nPercentile cutoff for voting\n\n\\rho\nSpearman correlation coefficient\nLoss Functions\n\n\\mathcal{L}\nGeneric loss\n\ndiff\n\n(\n\n;\n\n,\n\n)\n,\n\nflow\n\n(\n\n;\n\n,\n\n)\n\\mathcal{L}_{\\textnormal{diff}}(\\bm{\\theta};\\mathbf{v},\\mathbf{c}),\\mathcal{L}_{\\textnormal{flow}}(\\bm{\\theta};\\mathbf{v},\\mathbf{c})\nDiffusion and flow-matching objective\n\nmot\n\n(\n\n;\n\n,\n\n)\n\\mathcal{L}_{\\textnormal{mot}}(\\bm{\\theta};\\mathbf{v},\\mathbf{c})\nMotion-weighted objective used for attribution\n\n~\n\\tilde{\\mathcal{L}}\nPer-location squared error in latent space\n\n\\mathbf{g}\n,\n\n~\n\\tilde{\\mathbf{g}}\nGradient and its projected version\n\nmot\n\\mathbf{g}_{\\textnormal{mot}}\n,\n\n~\nmot\n\\tilde{\\mathbf{g}}_{\\textnormal{mot}}\nMotion-weighted gradient and its projection\n\n\n\\mathbf{H}_{\\bm{\\theta}}\nHessian with respect to\n\n\\bm{\\theta}\nMotion Representations\n\n\n(\n\n)\n=\nA\n\\mathcal{A}(\\mathbf{v})=A\nAllTracker motion extraction\nA\n\n\nF\n\nH\n\nW\n\n4\nA\\in\\mathbb{R}^{F\\times H\\times W\\times 4}\nMotion tensor containing flow, visibility, and confidence\n\nf\n\n(\nh\n,\nw\n)\n\\mathbf{D}_{\\!f}(h,w)\nDisplacement vector at frame\nf\nf\nand location\n(\nh\n,\nw\n)\n(h,w)\nM\nf\n\n(\nh\n,\nw\n)\nM_{\\!f}(h,w)\nMotion magnitude at a location, computed from the displacement\n\n\n(\nf\n,\nh\n,\nw\n)\n\n[\n0\n,\n1\n]\n\\mathbf{W}(f,h,w)\\in[0,1]\nNormalized motion weights used to mask per-location losses\n味\n\\zeta\nSmall numerical bias for stability (e.g.,\n10\n\n6\n10^{-6}\n)\nProjections and Computational Details\nD\nD\n,\nD\n\nD^{\\prime}\nFull and projected gradient dimensions\n\n\n\nD\n\n\nD\n\\mathbf{P}\\in\\mathbb{R}^{D^{\\prime}\\times D}\nProjection matrix used for Fastfood-style JL projection\n尉\n\\xi\nVariance normalization constant for projection\n\n\\mathcal{T}\nSet of sampled\n(\nt\n,\n系\n)\n(t,\\bm{\\epsilon})\npairs for gradient estimation\nB\nB\nUnit compute cost used in complexity accounting\nAppendix B\nExtended Related Work\nB.1\nData Attribution\nUnderstanding how individual training examples shape model behavior has been a long-standing goal.\nModern data attribution methods fall into two main groups\n[\nhammoudeh2024training\n]\n: retraining-based methods (e.g., leave-one-out\n[\ncook1982residuals\n,\njia2021scalability\n]\n, downsampling (also known as subsampling or counterfactual influence)\n[\nfeldman2020neural\n]\n, Shapley-value\n[\nwang2024data\n,\nwang2024rethinking\n]\n) and gradient-based methods (influence-function family, including Influence Functions\n[\nkoh2017understanding\n,\nlorraine2024jacnet\n]\n, TracIn\n[\npruthi2020estimating\n]\n, and TRAK\n[\npark2023trak\n]\n).\nInfluence functions provide a principled framework by approximating the effect of removing a training point.\nTracIn\n[\npruthi2020estimating\n]\nand TRAK\n[\npark2023trak\n]\nmake attribution feasible at scale. While effective for classification, these assume a mapping between training gradients and predictions, which becomes more complex in generative models.\nData attribution refers to methods that trace how individual training examples (or subsets) influence a models predictions or behavior. Formally, it assigns an attribution score to each training sample, estimating the extent to which that sample contributes (positively or negatively) to the models output on a given test query or behavior.\nInfluence data attribution is an example of nested optimization\n[\nvicol2022implicit\n,\nlorraine2024scalable\n]\nwith other examples including differentiable games\n[\nbalduzzi2018mechanics\n,\nlorraine2021lyapunov\n,\nlorraine2022complex\n]\nand hyperparameter optimization\n[\nraghu2021meta\n,\nlorraine2018stochastic\n,\nmehta2024improving\n,\nlorraine2020optimizing\n]\n.\nBefore diffusion models, attribution methods were applied to supervised learning tasks such as classification and regression, where influence functions\n[\nkoh2017understanding\n,\nlorraine2022task\n]\nand scalable approximations such as TracIn\n[\npruthi2020estimating\n]\n, TRAK\n[\npark2023trak\n]\n, and TDA\n[\nbae2024training\n]\nquantified the impact of training examples on downstream predictions.\nRecent work adapted data attribution to diffusion models\ngeorgiev2023journey\n,\nzheng2023intriguing\n,\nwang2025fast\n,\nwang2024data\n,\nlin2024diffusion\n,\nbrokman2024montrage\n,\nkwon2023datainf\n, where iterative denoising introduces timestep-dependent bias.\nmlodozeniec2024influence\npropose scalable approximations, while\nxie2024data\nidentify timestep-induced artifacts and normalization schemes. Concept-TRAK\n[\npark2025concept\n]\nextends attribution to concepts by reweighting gradients with concept-specific rewards, enabling attribution to semantic factors.\nwang2023evaluating\ninstead design a customization-based benchmark for text-to-image models, where models are fine-tuned on exemplar images with novel tokens and attribution is evaluated by whether it can recover the responsible exemplars.\nHowever, these are limited to image diffusion, which captures static appearance but not temporal dynamics.\nB.2\nMotion in Video Generation\nVideo diffusion extends image generation to time, requiring coherent motion across frames\n[\nho2022video\n,\nblattmann2023stable\n,\npeebles2023scalable\n,\nwan2025\n,\nagarwal2025cosmos\n]\n. A large body of work builds temporal structure via attention layers\n[\nwu2023tune\n]\n, control signals\n[\nchen2023control\n,\nzhang2023controllable\n]\n, feature correspondences\n[\ngeyer2023tokenflow\n,\nbao2023latentwarp\n,\nwang2024cove\n]\n, or consistency distillation\n[\nwang2023videolcm\n,\nzhou2024upscale\n]\n. Recent work has highlighted the challenge of decoupling motion from appearance in video diffusion transformers, where spatial and temporal information become entangled in the models representations\n[\nshi2025decouple\n]\n. However, understanding which training clips influence specific motion patterns in generated videos remains an open challenge.\nIn parallel, motion has long been studied using optical flow and correspondence, from classical formulations\n[\nhorn1981determining\n,\nlucas1981iterative\n]\nto modern approaches such as RAFT\n[\nteed2020raft\n]\n, which improve accuracy and generalization. These priors are often repurposed during generation to guide dynamics, but they do not explain which training examples shaped a models motion behavior. Our work addresses both gaps by introducing a motion-aware data attribution framework specifically designed for video diffusion. We use motion-weighted gradients that disentangle temporal dynamics from static appearance, enabling us to trace generated motion patterns back to the most influential training clips.\nAppendix C\nAdditional Experiments\nC.1\nResults on Different Video Generation Models\nWe further test our framework on additional video generation architectures beyond Wan2.1-T2V-1.3B. We have included the experiment results of\nMotive\non Wan2.2-TI2V-5B, which introduces a much larger parameter count (5B vs. 1.3B) and a new high-compression Wan2.2-VAE. The results in Tab.\n5\nshow that our approach works effectively across different model designs.\nModel\nMethod (\n\n\\downarrow\n) / Metric (\n\n\\rightarrow\n)\nSubject\nBackground\nMotion\nDynamic\nAesthetic\nImaging\nConsist.\nConsist.\nSmooth.\nDegree\nQuality\nQuality\nWan2.2-TI2V-5B\nBase\n94.9\n96.4\n97.5\n42.0\n44.4\n65.5\nFull fine-tuning\n95.3\n96.5\n97.5\n45.3\n44.8\n66.2\nRandom selection\n94.7\n96.2\n97.3\n41.6\n44.6\n65.2\nOurs w/o MM\n94.9\n96.5\n97.4\n43.8\n45.2\n64.8\nOurs (\nMotive\n)\n95.1\n96.6\n97.6\n48.3\n45.6\n65.5\nTable 5\n:\nVBench Evaluation on Additional Model.\nFollowing the same setting in 搂\n4\n, we extend the VBench\n[\nhuang2024vbench\n]\nevaluation to\nWan2.2-TI2V-5B\n, a larger-scale text-to-video model. Random selection and our\nMotive\nboth select\n10\n%\n10\\%\nof the training data, with our method using majority vote aggregation (搂\n3.5\n) across all motion queries. Results demonstrate that\nMotive\ngeneralizes effectively to different models. MM: motion masking.\nAppendix D\nAnalysis\nNumber of Samples\nMean Motion Magnitude\n(a)\nMotion Distribution\n% of Selected Videos\nMotion Bin\n(b)\nDistribution Across Motion Bins\nFigure 6\n:\nMotive\nis not simply selecting motion-rich\" clips.\nOur influence scores are computed via gradients, and training videos are considered influential only when they directly improve the models ability to generate the target motion dynamics, not because they contain more motion overall.\nD.1\nMotion Distribution Analysis\nMotive\nis not simply selecting motion-rich\" clips:\nThe key distinction is that our influence scores are computed via gradients, and training videos are considered influential only when they directly allow the model to lower the loss, improving the models ability to generate the target motion dynamics, not because they contain more motion overall.\nTo empirically validate this, we further analyze the distribution of motion magnitudes in our selected data. We compute the mean motion magnitude for 10k videos in the\nVIDGEN\ndataset and compare the distributions of the top 10% (highest influence scores) and the bottom 10% (lowest influence scores).\nAs shown in Fig.\n6\n, the top 10% selected videos have a mean motion magnitude of 3.85, which is only 4.3% higher than the bottom 10% (3.69), despite representing opposite extremes of influence scores. The analysis also shows that within the moderate-motion range (bins 3, 4, and 5), the top 10% of positive-influence samples outnumber the bottom 10% of negative-influence samples. Yet, both groups also appear in low-motion bins (0-2) and high-motion bins (6-9).\nThis distribution pattern shows that high-influence videos selected by\nMotive\nspan the entire motion spectrum, not just high-motion regions. Many high-motion videos receive low influence scores, while numerous influential videos exhibit modest or even low motion magnitude. These findings show that our motion attribution approach captures training influence, focusing on motion rather than simply acting as a motion-saliency filter.\n(a)\n4DNEX Influence Heatmap\n(b)\nVIDGEN Influence Heatmap\nFigure 7\n:\nCross-motion influence overlap across datasets.\nHeatmaps showing the percentage overlap of top-\n100\n100\ninfluential training samples across motion categories for (a) 4DNEX and (b) VIDGEN datasets. Each cell\n(\ni\n,\nj\n)\n(i,j)\nshows the percentage of motion category\ni\ni\ns influential data (aggregated from\n5\n5\nqueries per category) that also appears in motion category\nj\nj\ns top-\n100\n100\ninfluential samples. The asymmetric nature of the matrices (e.g., bouncefloat\n\n\\neq\nfloatbounce) arises because different motion categories have different numbers of unique influential videos, leading to directional overlap percentages. Consistent high-overlap pairs (e.g., bounce-float:\n44.4\n%\n44.4\\%\n/\n46.3\n%\n46.3\\%\n) and low-overlap pairs (e.g., free fall-stretch:\n12.8\n%\n12.8\\%\n/\n12.7\n%\n12.7\\%\n) across datasets validate that these influence patterns reflect fundamental aspects of motion representation in video generation models.\nD.2\nCross-Motion Influence Patterns\nTo analyze cross-motion influence patterns, we examine the percentage overlap of top-\n100\n100\ninfluential training data across different motion categories in both 4DNEX and VIDGEN datasets. As described in 搂\n4\n, we use\n5\n5\nquery samples to identify the top-100 most influential training videos, aggregating results across queries. As shown in Fig.\n7\n, both datasets exhibit remarkably similar patterns with mean overlaps of\n24.0\n%\n24.0\\%\nand\n24.3\n%\n24.3\\%\n, respectively, indicating moderate sharing of influential data across motion categories.\nBoth datasets consistently identify the same high-overlap pairs: bounce-float (\n44.4\n%\n44.4\\%\n/\n46.3\n%\n46.3\\%\n), compress-float (\n40.1\n%\n40.1\\%\n/\n34.0\n%\n34.0\\%\n), and compress-spin (\n36.9\n%\n36.9\\%\n/\n39.6\n%\n39.6\\%\n), suggesting these motions share fundamental characteristics that the model learns from similar training examples. Conversely, low-overlap pairs such as free fall-stretch (\n12.8\n%\n12.8\\%\n/\n12.7\n%\n12.7\\%\n) and float-slide (\n14.0\n%\n14.0\\%\n/\n10.9\n%\n10.9\\%\n) indicate more specialized influential data for mechanically dissimilar motions. The influence matrices are asymmetric because the number of unique influential samples shared across the\n5\n5\nquery samples differs across motion categories.\nThe similar cross-motion influence patterns observed across both the 4DNEX and VIDGEN datasets demonstrate that these relationships are generalizable across different video datasets and reflect dynamic similarity.\nAppendix E\nAdditional Method Details\nTracker-agnostic scope.\nWe treat the motion estimator as a pluggable source of saliency rather than a training dependency.\nGiven displacement magnitudes, we construct latent-space weights via bilinear mapping and normalization.\nOur implementation supports alternative estimators (such as dense optical flow or point tracking) with identical interfaces, enabling users to swap AllTracker without modifying the attribution code.\nModel-agnostic scope.\nOur attribution only requires per-example gradients under matched\n(\nt\nfix\n,\n系\nfix\n)\n(t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\n, and therefore applies to both diffusion and flow-matching objectives.\nThe score reduces to a gradient inner product under a fixed preconditioner; the generator architecture affects gradient statistics but not the definition of influence.\nIn practice, replacing the denoiser or velocity field leaves the weighting and aggregation unchanged.\nAlgorithm Summary.\nFor completeness, Algorithm\n1\nsummarizes the full\nMotive\npipeline, detailing the computation of motion-weighted gradients, projection into low-dimensional space, and the subsequent influence-based ranking and selection of training clips.\nAlgorithm 1\nMotive\n: Motion-Aware Data Attribution Framework\n1:\nfine-tuning corpus\n\nft\n=\n{\n(\n\nn\n,\n\nn\n)\n}\nn\n=\n1\nN\n\\mathcal{D}_{\\textnormal{ft}}=\\{(\\mathbf{v}_{n},\\mathbf{c}_{n})\\}_{n=1}^{N}\n, query video\n(\n\n^\n,\n\n^\n)\n(\\hat{\\mathbf{v}},\\hat{\\mathbf{c}})\n, fixed\n(\nt\nfix\n,\n系\nfix\n)\n(t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\n, projection matrix\n\n\\mathbf{P}\n2:\nMotion-aware influence scores\n{\nI\nmot\n\n(\n\nn\n,\n\n^\n)\n}\nn\n=\n1\nN\n\\{I_{\\textnormal{mot}}(\\mathbf{v}_{n},\\hat{\\mathbf{v}})\\}_{n=1}^{N}\n3:\nfor\n(\n\nn\n,\n\nn\n)\n\n\nft\n(\\mathbf{v}_{n},\\mathbf{c}_{n})\\in\\mathcal{D}_{\\textnormal{ft}}\ndo\n4:\nA\nn\n\nAllTracker\n\n(\n\nn\n)\nA_{n}\\leftarrow\\textsc{AllTracker}(\\mathbf{v}_{n})\n\n\\triangleright\nExtract per-pixel flow displacements\n\nf\n\\mathbf{D}_{\\!f}\n(Eq.\n12\n)\n5:\nDownsample and normalize to latent-space motion mask\n\nn\n\\mathbf{W}_{n}\n(Eqs.\n13\n\n15\n)\n6:\nEvaluate motion-weighted loss\n\nmot\n\n(\n\n;\n\nn\n,\n\nn\n)\n\\mathcal{L}_{\\textnormal{mot}}(\\bm{\\theta};\\mathbf{v}_{n},\\mathbf{c}_{n})\n(Eq.\n16\n)\n7:\nCompute motion gradient\n\nmot\n\n(\n\n,\n\nn\n,\nt\nfix\n,\n系\nfix\n)\n=\n\n\n\nmot\n\n(\n\n;\n\nn\n,\n\nn\n,\nt\nfix\n,\n系\nfix\n)\n\\mathbf{g}_{\\textnormal{mot}}(\\bm{\\theta},\\mathbf{v}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})=\\nabla_{\\bm{\\theta}}\\mathcal{L}_{\\textnormal{mot}}(\\bm{\\theta};\\mathbf{v}_{n},\\mathbf{c}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\n8:\nNormalize by frame length:\n\nmot\n\n\nmot\n/\nF\n\\mathbf{g}_{\\textnormal{mot}}\\leftarrow\\mathbf{g}_{\\textnormal{mot}}/F\n9:\nProject motion gradient:\n\n~\nmot\n\n(\n\n,\n\nn\n)\n:=\n\nmot\n\n(\n\n,\n\nn\n,\nt\nfix\n,\n系\nfix\n)\n\n\nmot\n\n(\n\n,\n\nn\n,\nt\nfix\n,\n系\nfix\n)\n\n\\tilde{\\mathbf{g}}_{\\textnormal{mot}}(\\bm{\\theta},\\mathbf{v}_{n}):=\\frac{\\mathbf{P}\\mathbf{g}_{\\textnormal{mot}}(\\bm{\\theta},\\mathbf{v}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})}{\\|\\mathbf{P}\\mathbf{g}_{\\textnormal{mot}}(\\bm{\\theta},\\mathbf{v}_{n},t_{\\textnormal{fix}},\\bm{\\epsilon}_{\\textnormal{fix}})\\|}\n(Eq.\n17\n)\n10:\nend\nfor\n11:\nCompute query gradient:\n\n~\nmot\n\n(\n\n,\n\n^\n)\n\\tilde{\\mathbf{g}}_{\\textnormal{mot}}(\\bm{\\theta},\\hat{\\mathbf{v}})\nusing the same procedure for\n(\n\n^\n,\n\n^\n)\n(\\hat{\\mathbf{v}},\\hat{\\mathbf{c}})\n12:\nfor\nn\n=\n1\n,\n\n,\nN\nn=1,\\dots,N\ndo\n13:\nI\nmot\n\n(\n\nn\n,\n\n^\n)\n=\n\n~\nmot\n\n(\n\n,\n\n^\n)\n\n\n\n~\nmot\n\n(\n\n,\n\nn\n)\nI_{\\textnormal{mot}}(\\mathbf{v}_{n},\\hat{\\mathbf{v}})=\\tilde{\\mathbf{g}}_{\\textnormal{mot}}(\\bm{\\theta},\\hat{\\mathbf{v}})^{\\top}\\tilde{\\mathbf{g}}_{\\textnormal{mot}}(\\bm{\\theta},\\mathbf{v}_{n})\n(Eq.\n17\n)\n14:\nend\nfor\n15:\nRank all training clips by\nI\nmot\n\n(\n\nn\n,\n\n^\n)\nI_{\\textnormal{mot}}(\\mathbf{v}_{n},\\hat{\\mathbf{v}})\nand select top-\nK\nK\ninfluential samples using majority vote aggregation (Eq.\n18\n):\n\n=\n\nvote\n\n(\nK\n)\n=\n{\n\nn\n|\n\nn\n\nin top-\n\nK\n\nby\n\nMajVote\n}\n\\mathcal{S}=\\mathcal{S}_{\\textnormal{vote}}(K)=\\bigl\\{\\mathbf{v}_{n}|\\mathbf{v}_{n}\\text{ in top-}K\\text{ by }\\operatorname{MajVote}\\bigr\\}\n16:\nreturn\n\n\\mathcal{S}\nAppendix F\nAdditional Experiment Details\nF.1\nHyperparameter Settings\nFor reproducibility, we document the hyperparameters used throughout attribution, subset selection, and fine-tuning. Where values were not explicitly tuned, we adopted defaults from DiffSynth-Studio and the official Wan repo.\nAttribution.\nMotion-aware influence estimation is computed at a single fixed timestep\nt\nfix\n=\n751\nt_{\\textnormal{fix}}=751\n, corresponding to scheduler index\n500\n500\nat the midpoint of the denoising trajectory, which strongly correlates with multi-timestep averaging.\nA shared Gaussian draw\n系\nfix\n\n\n\n(\n0\n,\n\n)\n\\bm{\\epsilon}_{\\textnormal{fix}}\\sim\\mathcal{N}(0,\\mathbf{I})\nis used across all trainingquery pairs to reduce stochastic variance. Gradients are projected from dimension\nD\n=\n1418996800\nD=$1\\,418\\,996\\,800$\nto\nD\n\n=\n512\nD^{\\prime}=512\nusing a Fastfood JohnsonLindenstrauss projection\n\n\\mathbf{P}\nselected via the search in Fig.\n4\nto balance performance and storage. Motion weights\n\n\\mathbf{W}\nare computed from AllTracker flow magnitudes\nM\nf\nM_{\\!f}\n, minmax normalized to\n[\n0\n,\n1\n]\n[0,1]\nwith a small bias\n味\n=\n10\n\n6\n\\zeta=10^{-6}\n.\nAll computations use bfloat16 precision for memory efficiency.\nSubset Selection & fine-tuning.\nFor any number of query points, we select top-\n10\n10\n% data of the datasets. We finetune the\nWan2.1-T2V-1.3B\nbackbone while freezing both the T5 text encoder\n[\nraffel2020exploring\n]\nand the VAE. The input resolution is fixed to\n480\n\n832\n480\\times 832\npixels. We use a learning rate of\n1\n\n10\n\n5\n1\\times 10^{-5}\nand the AdamW optimizer\n[\nloshchilov2017decoupled\n]\nfollowing the DiffSynth-Studio defaults. We train the models for\n1\n1\nepoch, repeating the dataset\n50\n50\ntimes.\nEvaluation.\nThe test set consists of the same\n10\n10\nmotion categories, but with different visual appearances, compared with the query set. We provide the prompt samples below.\nExample F.1\n(\nSamples from Query Set)\n.\nWe illustrate representative prompts from our query set used to generate query videos with Veo-3.\n\ncompress\n: A slice of sandwich bread flattened by a flat metal plate, steady camera, soft studio lighting, plain backdrop; emphasize air pockets collapsing.\n\nbounce\n: A ping-pong ball bouncing on a white table, steady side camera, neutral light, seamless backdrop; emphasize consistent bounce height and timing.\n\nroll\n: A spool of thread rolling from left to right, close-up static camera, bright studio light; highlight axle rotation and smooth travel.\n\nexplode\n: A single balloon bursting into fragments, captured in high-speed slow motion with a fixed camera, bright even lighting, seamless background; emphasize outward debris and air release.\n\nfloat\n: A foam cube floating on the surface of water, static overhead camera, bright light, clean tank; emphasize buoyancy and slight rocking.\nExample F.2\n(\nSamples from Test Set)\n.\nWe illustrate representative prompts from our test set that our fine-tuned models use to generate test videos.\n\ncompress\n: A rubber ball being compressed under a flat press, filmed with a stationary camera. Bright, shadow-free lighting and a clean background emphasize the deformation as it flattens.\n\nbounce\n: A basketball bouncing vertically on a wooden court plank, unmoving camera, balanced indoor lighting, plain wall background; clearly show deformation at impact.\n\nroll\n: A bike tire rolling freely on a stand, static side camera, indoor neutral light; show uniform rotation without wobble.\n\nexplode\n: A fragile glass ornament breaking apart mid-air, fixed camera, bright controlled lighting, plain backdrop; capture shards and reflections crisply.\n\nfloat\n: A green leaf floating gently on perfectly still water in a transparent tank, fixed top-down camera, bright even lighting; emphasize surface tension ripples.\nF.2\nDetails on Motion Query Data\nA small, controlled set of query videos is constructed to isolate specific motion primitives while minimizing confounding factors (e.g., textured backgrounds, uncontrolled camera motion). Such clean and consistent clips are challenging to obtain from natural data sources. To address this, we synthesize the query set using Veo-3\n[\nveo3\n]\nand apply a strict post-generation screening for physical plausibility and generation realism. We target ten motion types:\ncompress, bounce, roll, explode, float, free fall, slide, spin, stretch, swing\n.\nFor each category, we retain\n5\n5\nquery samples, yielding a total of\n50\n50\nqueries. This scale provides adequate coverage of the motion taxonomy used in our evaluations while maintaining tractable attribution computation. We further provide a few examples of the generation prompts and the generated video query set in Fig.\n8\n.\nFigure 8\n:\nIllustration of motion query set.\nWe generate near-realistic video queries with Veo-3 across ten motion categories.\nEach category contains five query videos synthesized with controlled prompts and manually screened for clarity and physical plausibility.\nRationale for synthetic queries.\nThe query set is not used as training data; instead, it specifies targets for attribution and for multi-query aggregation. Synthetic generation offers controllability that is difficult to achieve at scale with web videos. This design yields near-realistic yet standardized stimuli aligned with our goal of probing motion-specific influence.\nAppendix G\nDiscussion\nG.1\nRuntime\nComponent\nComplexity\nRuntime\nNotes\nGradient computation\n\n\n(\nB\n)\n\\mathcal{O}(B)\nper sample\nQuery:\n\n54\n\\sim 54\nseconds\nTraining:\n\n150\n\\sim 150\nhours\n1\n1\nA100 GPU;\nSingle forward+backward pass;\ntraining is dominant cost but amortized over all queries;\nembarrassingly parallel (with\n64\n64\nGPUs,\n\n2.3\n\\sim 2.3\nhours)\nProjection\n\n\n(\n|\n\n|\n\nD\n\n\nlog\n\nD\n\n)\n\\mathcal{O}(|\\mathcal{D}|\\cdot D^{\\prime}\\log D^{\\prime})\n\n1.97\n\\sim 1.97\nseconds per sample\nD\n\n=\n512\nD^{\\prime}=512\nInfluence computation\n\n\n(\n|\n\n|\n\nD\n\n)\n\\mathcal{O}(|\\mathcal{D}|\\cdot D^{\\prime})\n\n46\n\\sim 46\nmilliseconds per query\n1\n1\nquery\n\n\\times\n10k training samples\nMajority-vote aggregation\n\n\n(\n|\n\n|\n\nq\n)\n\\mathcal{O}(|\\mathcal{D}|\\cdot q)\n\n139\n\\sim 139\nmilliseconds\n50\n50\nqueries\n\n\\times\n10k samples\nTable 6:\nRuntime Breakdown.\nDetailed computational complexity and runtime for each component of our motion attribution framework on 10k training samples with\nWan2.1-T2V-1.3B\nmodel.\nWe provide a detailed runtime breakdown from our experiments on 10k training samples with\nWan2.1-T2V-1.3B\nmodel to address scalability concerns. The key insight is that the dominant cost of our pipeline is computing per-training-sample gradients, which is performed once and then can be reused for all subsequent queries. Each training clips gradient is projected into a compact\n512\n512\n-dimensional vector, and adding a new query requires only (i) a single backward pass to obtain its own projected\n512\n512\n-dimensional gradient and (ii) computing cosine similarity between the query vector and stored training vectors, which is exceptionally lightweight (on the order of seconds). Thus, the computational burden does not scale with the number of queries but only with the size of the training set.\nAs shown in Tab.\n6\n, the dominant cost is the one-time training data gradient computation (\n\n150\n\\sim 150\nhours on 1 A100), which is amortized across all queries. Once computed, adding a new query requires only\n\n54\n\\sim 54\nseconds (gradient computation) +\n46\n46\nms (influence computation) =\n\n\\sim\n54 seconds total. The training data gradient computation is embarrassingly parallel and can be reduced to\n\n2.3\n\\sim\\!$2.3$\nhours with\n64\n64\nGPUs.\nRuntime comparison with baselines.\nWe compare the computational cost of our method with the baseline approaches for processing 10k training samples on a single GPU (Table\n7\n). While our method requires more upfront computation than the baseline approaches, this cost is amortized across all queries, and the computed gradients can be reused for multiple selection queries, making it practical for large-scale data curation scenarios.\nMethod\nRandom\nMotion Magnitude\nOptical Flow\nV-JEPA\nOurs\nTotal for 10k (\n1\n1\nGPU)\n<\n1\n<1\nsecond\n\n5.5\n\\sim 5.5\nhours\n\n5.7\n\\sim 5.7\nhours\n\n3\n\\sim 3\nhours\n\n150\n\\sim 150\nhours\nTable 7\n:\nRuntime Comparison with Baselines.\nTotal computational time required for processing 10k training samples on a single GPU across different data selection methods.\nG.2\nLimitations\nGradient-based attribution is computationally expensive, requiring high upfront cost for per-sample gradient computation (see 搂\nG.1\n), though this cost is amortized across queries.\nOur analysis treats each video as a whole unit, thereby avoiding collapsing motion into frame-level appearance, but it risks overlooking the fact that only certain segments may carry motion-relevant information. Highly informative intervals can be diluted when averaged with static or redundant portions of the same clip. This suggests an open direction toward finer-grained attribution at the motion segment or motion event level, which could reveal more precise insights into how different phases of a trajectory shape motion learning. Another limitation is that our motion masks may overemphasize camera-only motion; we detect this by spatial uniformity of\n\n\\mathbf{W}\nand down-weight such clips, but a full disentanglement of ego and object motion remains future work.\nAdditionally, our framework does not explicitly account for classifier-free guidance (CFG), which is widely used in practice to steer video generation but introduces discrepancies between training-time attribution and inference-time dynamics.\nAs a result, our influence estimates may not fully capture how guidance alters motion behavior.\nIn addition, while attribution-driven fine-tuning improves targeted motion quality, it may introduce trade-offs with base model capabilities.\nThis raises the need for future work on balancing targeted motion adaptation with the preservation of broader generative capabilities.\nG.3\nFuture Directions\nTracker-robust motion saliency.\nReplace or ensemble AllTracker with alternative estimators and use its confidence/visibility channels to weight masks.\nClosed-loop data curation.\nMove from one-shot ranking to active selection: iteratively attribute, finetune, and re-attribute, or replace simple majority voting with learned query weights.\nSafety and governance.\nUse negative-influence filtering to suppress undesirable or unsafe dynamics, document curator choices, and audit motion behaviors exposed by our framework.\nSophisticated finetuning.\nMove to more sophisticated finetuning setups, such as multi-student distillation\n[\nsong2024multi\n]\n.\nOther modalities.\nExtend our methodology to other modalities, including world models\n[\nzhu2025astra\n]\n, audio\n[\nevans2025stable\n,\nrichter2025score\n]\nor video+audio\n[\nwiedemer2025video\n]\n.\nSelf-generated video queries.\nUse model-generated videos as queries to trace problematic motion patterns (e.g., unrealistic physics) back to training data, enabling iterative diagnostics and targeted motion improvement.\nAppendix H\nVisualization\nH.1\nMotion Visualization\nTo provide intuition for the behavior of our motion-weighted loss, we visualize the motion magnitude as an overlay. We compute per-pixel motion magnitude using optical flow and apply motion-based weighting that preserves the appearance of dynamic regions while attenuating static backgrounds. This motion overlay directly illustrates the spatial weighting applied by our motion loss during training.\nFig.\n9\npresents representative frames from our dataset, comparing original frames with their corresponding motion overlays for seven distinct video samples. These visualizations show that the motion-weighted loss preferentially emphasizes dynamic content while down-weighting static scene elements.\nFigure 9\n:\nMotion overlay visualization.\nComparison of original frames and motion overlays for seven video samples across three time points (early, middle, late). The motion overlay demonstrates the spatial weighting of our motion loss: dynamic regions remain visible, while static backgrounds are attenuated to neutral gray.\nTakeaway:\nThis provides heuristic intuition into what information our motion attribution focuses on: the information in grayer regions, which lack motion, is down-weighted by our method.",
  "preview_text": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.\n\n\\dosecttoc\nMotion Attribution for Video Generation\nXindi Wu\n12\nDespoina Paschalidou\n1\nJun Gao\n1\nAntonio Torralba\n3\nLaura Leal-Taix茅\n1\nOlga Russakovsky\n2\nSanja Fidler\n1\nJonathan Lorraine\n1\n1\nNVIDIA\n2\nPrinceton University\n3\nMIT CSAIL\nhttps://research.nvidia.com/labs/sil/projects/MOTIVE/\nAbstract\nDespite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present\nMotive\n(\nMOTI\non attribution for\nV\nideo g\nE\nneration\n), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models.\nWe use this to study which fine-tuning clips improve or degrade temporal dynamics.\nMotive\nisolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation.\nOn text-to-video models,\nMotive\nidentifies clips that strongly affect",
  "is_relevant": true,
  "relevance_score": 6.0,
  "extracted_keywords": [
    "video generation",
    "motion attribution",
    "data attribution",
    "fine-tuning",
    "temporal dynamics"
  ],
  "one_line_summary": "杩绡璁烘轰涓涓轰姊搴杩ㄥ妗Motive锛ㄤ瑙棰妯″涓版瀵硅ㄧ褰卞锛骞堕杩版绛杩ㄨ川",
  "detailed_summary": "## 璁烘瑕锛Motion Attribution for Video Generation\n\n**1. 绌惰ㄦ**\n灏界¤棰妯″灞杩锛浣璁缁版濡浣褰卞瑙棰涓**杩**锛motion锛浠涓纭版版褰规涓昏瀵瑰惧锛虫敞澶瑙锛娉瑙ｉ瑙棰规堕村ㄦ瑙ｅ浜璁缁娈靛浜妯″杩ㄦā寮锛瀵逛瑙棰堕翠存с╃уфц冲抽瑕\n\n**2. 稿规**\n轰 **Motive**锛棣涓㈠瑙棰妯″浠ヨㄤ负涓蹇姊搴版褰妗躲舵稿板锛\n- **杩ㄦュ**锛杩娴璁＄杩ㄦ捐ф╃锛ㄦ澶卞芥颁腑瀵瑰ㄦ哄杩琛锛浠灏**堕村ㄦ**褰涓澶瑙瑙ｈ\n- **楂╁璁捐**锛ㄥ哄堕存ラ枫变韩澹板Fastfood烘褰辩锛捐浣浜璁＄瀛ㄥ锛浣垮惰藉ㄤ浜垮扮骇澶у瑙棰妯″娴烽版\n- **瑙棰瑰宸℃**锛淇姝ｄ瑙棰垮害瀵规搴骞肩褰卞锛纭淇瀵逛堕跨娈电骞冲\n\n**3. 涓昏瀹楠缁**\nㄦ拌棰妯″涓瀹楠琛ㄦ锛\n- **璇**锛Motive藉纭璇哄圭杩ㄦā寮寮哄奖璁缁娈点\n- **ц**锛浣跨Motive绛虹楂褰卞版锛浠ㄦ版10%锛杩琛寰璋锛VBench璇娴涓捐浜**杩ㄥ钩婊搴****ㄦ绋搴**\n- **浜虹被濂**锛寰璋妯″ㄤ汉绫昏浼颁腑锛歌浜棰璁缁虹妯″峰浜**74.1%**\n\n**4. 绌舵涔浠峰**\n绌堕娆″颁瀵硅棰妯″涓**杩**锛澶瑙锛版褰锛涓虹瑙ｆāㄥ瀛琛涓烘渚浜存ュ伐枫跺浠峰煎ㄤ藉瀵**版绛瀵规у井璋**锛浠楂瑙棰杩ㄨ川杩椤瑰伐浣涓烘寤烘村с瑙ｉ涓杩ㄦ撮肩瑙棰绯荤濂瀹浜虹",
  "qa_pairs": [
    {
      "question": "杩绡璁烘稿扮规浠涔锛浠宠В充涔棰锛涔瑙ｅ崇锛",
      "answer": "## 璁烘锛Motion Attribution for Video Generation\n\n### **稿棰**\n瑙棰妯″锛濡╂ｆā锛ц芥涓ラ渚璧浜娴烽璁缁版锛浣涓涓规ч棰灏琚瑙ｏ**璁缁版涓浜瑙棰娈碉clips锛蜂褰卞浜瑙棰涓杩ㄢ锛motion锛璐ㄩ锛** 版版褰规涓昏瀵瑰惧锛虫敞澶瑙锛濡╀绾圭锛锛娉杩借釜瑙棰涓ㄦ跺缁锛濡杞ㄨ抗褰㈠╃浜や锛ユ\n\n### **稿扮**\n轰 **Motive** 妗讹杩棣涓涓ㄤ负瑙棰妯″璁捐＄**浠ヨㄤ负涓蹇版褰规**舵稿板ㄤ灏褰浠澶瑙杞杩ㄢ锛骞跺颁ㄥぇ瑙妯￠璐ㄩ瑙棰版妯″涓楂╁璁＄\n\n#### **1. **\n- **杩ㄦョ姊搴褰**锛杩**杩ㄥ澶辨╃**锛ㄨ＄姊搴跺己璋ㄦ哄锛堕锛浠绂诲鸿ㄧ瑰褰卞淇″枫\n- **瀵硅棰宸ョ浼**锛\n    - **╁**锛ㄥ堕存/澹伴枫遍烘с浠ュ**Fastfood烘褰**锛灏楂缁存搴缂╁颁缁寸┖达浣垮浜垮版ā褰璁＄寰琛\n    - **甯ч垮宸淇姝**锛瀵规搴杩琛甯ф板涓锛纭淇涓堕胯棰娈电褰骞虫с\n- **㈠寰璋版绛**锛╃ㄥ寰锛浠娴烽版涓ㄧ哄圭杩ㄦā寮**褰卞澶**灏锛濡10%锛楂璐ㄩ娈电ㄤ寰璋锛瀹伴版绛灞\n\n#### **2. 规瑙ｅ宠矾寰**\n```\n1. 棰瀹涔锛ㄥ井璋烘涓锛姣涓璁缁瑙棰娈 (_n) 瀵规ヨ㈣棰 (^) 涓杩ㄧ褰卞\n2. 杩ㄦ锛浣跨ㄥ娴浼拌″锛濡AllTracker锛璁＄璁缁ヨ㈣棰杩ㄥ搴 (M_f)\n3. 杩ㄥ澶憋ㄦㄧ┖达ㄨㄥ搴惧规╂ｆā澶辫琛绌洪村锛寰拌ㄧ瑰澶 _mot\n4. 楂褰璁＄锛\n   a. 璁＄ _mot 瀵规ā扮姊搴\n   b. 浣跨ㄥ哄堕存ュ变韩澹伴蜂互浣瑰樊\n   c. 杩Fastfood褰卞楂缁存搴缂╀负浣缁寸瑰\n   d. 璁＄ヨ㈣棰涓璁缁瑙棰褰辨搴涔寸浣寮镐技搴锛浣涓哄寰 I_mot\n5. 版绛锛规褰寰锛top-K娈电寰璋版\n```\n\n### **瀹浠峰间楠璇缁**\n- **瑙棰杩ㄨ川**锛浣跨Motive绛虹版寰璋妯″锛**VBench**璇娴涓捐浜**杩ㄥ钩婊搴****ㄦ绋搴**浠浣跨10%版锛舵宠揪版瓒杩浜浣跨ㄥㄩㄦ版寰璋ц姐\n- **浜虹被濂借**锛ㄤ汉绫昏浼颁腑锛涓棰璁缁虹妯″告锛Motive寰璋妯″峰浜 **74.1%** 濂借\n- **涓烘ā瑙ｅ版绛灞渚板伐**锛\n    - **瑙ｉ**锛浠ヨ拷婧瑙棰涓涓惰ㄧユ锛冲浜璁缁版瀵艰翠涓杩锛\n    - **楂版娓娲**锛藉ㄨ骞惰婊ゆ瀵硅ㄥ涔虫瀹崇版锛濡浠鸿ㄣ￠椋肩娈碉\n    - **瀹ц芥**锛浠ラ瀵圭瑰杩ㄧ被锛濡婕娴杞锛绛版锛杩琛涓椤瑰井璋锛浠瀵规ф妯″ㄨヨㄤ藉\n\n### **荤**\n**Motive** 稿璐＄**棣娆″版褰逛惧澶瑙寤朵几拌棰杩**锛骞堕杩涓绯诲颁娇堕ㄤ颁唬澶ц妯¤棰妯″瀹涓浠涓涓宸ュ凤存涓涓寮哄ぇ版绛灞寮锛藉浠ユ版涓轰腑蹇瑰锛楂瀹版瑙棰妯″**跺ㄦ**杩涓抽缁村害涓璐ㄩ涓фс杩椤瑰伐浣涓虹瑙ｆ版濡浣濉妯″琛涓衡寮杈浜扮瑰",
      "timestamp": "2026-01-20T17:51:02.976811",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "轰浠浣锛姊崇杩涓瑰翠釜灞缁锛姣涓姝ョ告浜涔宸ヤ芥硅浜浠涔锛浜涓涓灞瑰",
      "answer": "## 璁烘Motion Attribution for Video Generation\n\n### 稿璐＄\n轰 **Motive**锛杩棣涓涓ㄩ瀵硅棰妯″**杩ㄥ妗**瀹杩轰姊搴版褰规锛璇哄圭瑙棰涓**杩ㄥㄦ**瑕褰卞璁缁娈碉骞跺╃ㄨ浜淇℃瀵兼版绛寰璋锛浠捐瑙棰**堕翠存у╃**\n\n### 灞缁姊崇\n\n璇ョ绌跺缓绔ㄦ版褰瑙棰涓ゅぇ棰浜ゅ逛跺灞缁浠ユ⒊涓轰互涓涓抽舵靛瑰锛\n\n#### 瑰涓锛版褰规婕杩锛浠ㄥ版╂ｆā涓锛\n\n1.  **缁稿奖芥 (Influence Functions)**\n    *   **浠ｈ〃宸ヤ**: Koh & Liang (2017) *Understanding Black-box Predictions via Influence Functions*\n    *   **稿**: 杩璁＄Hessian涓姊搴涔绉锛璁轰涓璁缁锋瀵规āㄦ涓娴璇锋涓棰娴褰卞\n    *   **灞**: 璁＄瀛ㄥ寸Hessian╅靛逛颁唬澶ц妯＄缁缃缁**涓琛**\n\n2.  **╁姊搴杩浼兼规**\n    *   **浠ｈ〃宸ヤ**: TracIn (Pruthi et al., 2020), TRAK (Park et al., 2023)\n    *   **硅**: 惧浜绮剧‘Hessian璁＄锛杞浣跨**姊搴绉****褰辨搴瑰**ヨ浼煎奖杩澶уぇ浣浜璁＄锛浣垮惰藉搴ㄤ澶ц妯℃ā版\n    *   **稿寮**: `I(x_n, x_test)  g(x_test)^T 路 g(x_n)`锛朵腑 `g` 姊搴\n\n3.  **╂ｆā涓ㄥ**\n    *   **浠ｈ〃宸ヤ**: Diffusion-ReTrac (Xie et al., 2024)\n    *   **硅**: 颁╂ｆā璁缁涓ㄥ涓堕存 `t` 澹 `蔚` 涓锋搴寮ョ**堕存ュ宸**杩**姊搴褰涓****变韩烘ч**锛绋冲浜褰\n    *   **稿寮**: 瀵规涓锋姊搴 `(t, 蔚)` 涓骞冲骞跺涓璁＄浣寮镐技搴\n    *   **灞**: 杩浜规涓昏瀵**惧**锛虫敞澶瑙锛╀绾圭锛锛娉瑙棰规**堕村ㄦ缁**\n\n#### 瑰浜锛瑙棰涓杩ㄥ缓妯＄婕杩\n\n1.  **舵涓绠娉硅**\n    *   **浠ｈ〃宸ヤ**: ㄥ惧╂ｄ富骞蹭澧堕存敞风Н3D U-Net绛锛濡Video Diffusion Models, Stable Video Diffusion锛\n    *   **稿**: 杩淇规ā舵锛惧板瀵规堕寸淮搴寤烘ā藉\n    *   **灞**: 渚ч浜妯″韬璁捐★**缂轰瀵硅缁版濡浣褰卞缁杩ㄨ川瑙**\n\n2.  **版椹卞ㄧ璐ㄩ**\n    *   **瑙瀵**: 瑙棰璐ㄩ宸ㄥぇ椋璺寰寰涓**澶ц妯°楂璐ㄩ瑙棰版╁绛**绱у稿炽\n    *   **棰**: 灏界℃版冲抽瑕锛浣绀惧虹己涔宸ュ锋ラ**浜蜂璁缁娈靛浜瑰杩ㄦā寮**\n\n#### 瑰涓锛宸ヤ杩ㄤ腑蹇版褰锛Motive锛\n\n涓杩颁袱涓瑰浜ゆ涓绐达涓昏硅扮瑰涓锛\n\n1.  **浠澶瑙褰扳杩ㄥ寮杞**\n    *   **涔宸ヤ灞**: 灏瑙棰褰绠拌涓哄甯у惧褰锛**娣锋浜ㄦ杩ㄤ澶瑙**锛瀵艰村缁浜╀绛归\n    *   **Motive硅**: 棣娆℃纭哄苟瀹颁**瀵硅ㄥㄦ褰**杩寮**杩ㄥ澶辨╃**锛ㄨ＄姊搴跺己璋ㄦ哄锛堕锛浣垮淇″疯浜堕村\n\n2.  **瑙ｅ宠棰规璁＄涓骞虫ф**\n    *   **璁＄╁**:\n        *   **棰**: 瑙棰搴匡姊搴璁＄甯ф般堕存ャ澹伴峰板裤\n        *   **硅**: **堕存/澹伴蜂及璁″** (`t_fix`, `蔚_fix`)锛骞跺╃**Fastfood烘褰**灏楂缁存搴缂╁颁缁寸┖ (`D=512`)锛瀹颁瀵瑰浜垮版ā澶у版楂澶\n    *   **甯ч垮宸淇姝**:\n        *   **棰**: 濮姊搴骞间瑙棰甯ф `F` 稿筹瀵艰撮胯棰ㄥ涓浼\n        *   **硅**: ㄦ搴璁＄涓寮 `1/F` 褰涓瀛锛纭淇涓堕胯棰涔寸骞虫杈\n\n3.  **寤衡褰-绛-硅搴**\n    *   **涔宸ヤ**: 褰澶ㄤ浜瑙ｉ\n    *   **Motive硅**: 灏杩ㄥ存ュㄤ**版绛**锛瀵煎井璋\n        *   **规**: 瀵圭杩锛ヨ㈣棰锛璁＄璁缁娈电褰卞帮Top-K楂锋浣涓哄井璋版\n        *   **澶ヨ㈣**: 浣跨**澶版绁ㄦ**锛绛哄瑰绉杩 consistently 褰卞锋锛娉с\n    *   **瀹浠峰**: 浠浣跨 **10%** 绛版寰璋锛ㄨㄥ钩婊搴ㄦ绋搴涓杈惧版瓒杩浜浣跨ㄥㄩㄦ版寰璋锛**浜虹被濂借杈74.1%**杩涓洪瀹版瑙棰妯″杩ㄨ川渚浜瀹璺电宸ュ枫\n\n### 荤锛涓涓灞瑰\n\n1.  **褰规韬╁т绮惧**锛浠璁哄奖芥 -> 瀹ㄦ搴杩浼 -> 瀵规╂ｆā璁缁ㄦ绋冲 -> 瀵硅棰妯℃杩ㄧ瑰ц\n2.  **浠妯″舵板版版瑙ｆ繁**锛瑙棰绀惧轰涓娉ㄤ璁捐℃村ソ剁┖缃缁锛寮濮杞瑙ｉ┍ㄦā琛ㄧ扮规璁缁版Motive 杩涓杞涓抽宸ュ枫\n3.  **浠浜瑙ｉ颁骞查搴ㄨ寮**锛褰浠涓绉璇瑙ｉ宸ュ凤灞涓轰绉涓诲ㄧㄤ版娌荤妯″浼宸ョ宸ュ枫杩涓烘寤烘村с村绯荤寮杈浜拌矾寰\n\n**缁璁**锛 **Motive** 妗舵蹇瑙棰绌惰ヤ涓涓伴舵碉抽杩版褰**骞跺╃ㄦ版涓杩ㄨ川涔寸崇郴**瀹涓浠涓涓宸ュ凤存涓涓芥瀵兼版绛瀹伴瀵规фā硅瀹ㄧ郴缁锛瀵规ㄥㄩ璐ㄩ╃瑙棰锋瑕涔",
      "timestamp": "2026-01-20T17:51:52.617605",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "浠浣浜锛浣跨ㄨ〃间缁璁茶蹭姣绡浣锛浠浣哄浠涔锛涓昏瑰浠涔锛浜蜂告浜涔绔瑰",
      "answer": "## 璁烘Motion Attribution for Video Generation浣\n\n浣㈤锛瑰绗涓浣Xindi Wu锛ㄦ版褰锛Data Attribution锛瑙棰棰涓绯诲稿崇宸ヤ浠ヤ琛ㄦ艰缁姊崇浜舵稿浣锛骞堕瑰规浜锛Motive锛涓杩浜浣**规稿璐＄搴ㄥ烘**涓蜂哄涓硅\n\n| 浣璁烘棰 | 琛ㄥ勾浠/浼璁 | 稿璐＄涓规 | 涓锛Motive锛涓昏哄涓硅 |\n| :--- | :--- | :--- | :--- |\n| **ICONS: Interpretable COncept-based Data SelectioN** | 2024 (ICLR) | 轰涓涓**轰姒蹇电版╂**锛ㄤ惧妯″寰璋瀹杩**寮瀵肩濂辨ā**璁＄璁缁版瀵圭瑰姒蹇碘锛濡椋笺╀锛褰帮浠╂峰奖版瀛杩琛楂寰璋 | **1. 褰涓锛** ICONS褰浜**璇涔灞㈢姒蹇碘**锛濡姊甸椋尖虹锛锛Motive褰浜**ㄦ杩ㄢ**锛濡婊ㄣ寮硅烦锛<br>**2. 璺寰涓锛** ICONS渚璧浜**澶ㄥ辨ā**锛濡CLIP锛ヨ浼版蹇电镐技эMotive浣跨**妯″ㄧ姊搴**锛轰╂ｆ澶憋杩琛璁＄锛澶ㄦā<br>**3. 棰涓锛** ICONS瀵**惧**锛Motive涓ㄨВ**瑙棰**涓规跺ㄦ褰棰锛寮ヤ杩ㄦ╃甯ч挎℃ｇ瑙棰涓ㄦ |\n| **Compact Data Selection: Towards Efficient and Interpretable Data-Centric AI** | 2025 (棰版) | ㈢储浜**澶ц妯￠璁缁妯″**涓杩琛**楂版**ㄦ讹ㄥㄦ楂╃璁＄瑙ｉс瀹芥姊搴褰便楂镐技搴璁＄绛 | **1. 棰娉 vs. 棰瑰锛** Compact Data Selection村浜涓涓**ㄣ楂妗**锛介ㄤ澶绉浠诲°Motive**楂搴瑰**锛涓ㄨВ斥**瑙棰杩ㄥ**杩涓琚㈢储蜂棰<br>**2. 扮逛锛** 稿板ㄤ**杩ㄦモ**杩娴璁＄杩ㄦ╃ュ澶辨搴锛浠灏褰淇″蜂澶瑙涓绂诲烘ャ杩浣娌℃瀵硅棰璐ㄧ般<br>**3. 楠璇烘涓锛** 楠璇瀹ㄥ寸**瑙棰璐ㄩ**锛瑰杩ㄥ钩婊搴ㄦ绋搴锛浜虹被濂藉寮锛ㄧ╂姒蹇典搴 |\n| **锛稿宠宸ヤ锛Diffusion-ReTrac** | 2024 (Xie et al.) | 骞堕浣宸ヤ锛浣规瑕**虹**瀹轰瀵**惧╂ｆā**版褰规锛瑙ｅ充堕存ワtimestep锛宸棰锛杩姊搴褰涓瀛锋ョǔ瀹褰般 | **1. 浠惧拌棰抽╁锛** Diffusion-ReTrac**惧灞**褰Motive灏舵╁**瑙棰**锛骞惰В充涓涓版锛**(i) 杩ㄥ浣**锛ㄦ╃ㄦ哄锛**(ii) ╁板**锛澶澶甯ф搴锛**(iii) 跺崇郴**锛搴搴锛<br>**2. 寮ヨㄧ瑰э** Diffusion-ReTrac褰翠锛浼娣锋澶瑙杩ㄣMotive杩**杩ㄥ澶辨╃**锛涓诲ㄦ堕锛寮鸿ㄦ哄锛瀹颁**杩ㄧ瑰у**<br>**3. 澶瑙棰规宸锛** Motive棰澶寮ヤ**甯ч垮宸℃**锛寮11锛锛纭淇涓堕胯棰褰骞虫э杩惧褰涓涓瀛ㄧ棰 |\n| **锛稿宠宸ヤ锛TRAK / TracIn** | 2023/2020 (Park et al. / Pruthi et al.) | 缁哥**瀵瑰ゅ寮妯″**锛濡绫诲锛╁姊搴褰规瀹浠浣跨ㄦ搴绉褰辨ヨ浼煎奖芥帮垮浜璁＄娴锋．╅电宸ㄥぇ寮 | **1. 浠ゅ寮浠诲″扮寮浠诲★** TRAK/TracIn涓昏ㄤ绫/褰浠诲°Motive灏惰寮**杩绉诲板ぇ瑙妯°′欢寮瑙棰妯″**锛╂/娴归妯″锛<br>**2. 搴寮璁缁ㄦэ** 妯″璁缁娑瀵规堕存 `t` 澹 `系` 绉Motive村苟绠浜Diffusion-ReTrac绛ワ濡哄 `t_fix`, `系_fix`锛锛浣跨告搴褰娉介搴╂ｈ缁ㄦ澶辨瑙<br>**3. 澧棰瑰妯″锛** ㄧ户夸TRAK**╁姊搴褰**锛Fastfood锛绛楂宸х虹涓锛Motive**板浜杩ㄦユā**锛浣垮朵绠瑙棰TRAK锛锋瑙棰杩ㄨВ藉涓ㄥ伐枫 |\n\n## 荤锛稿逛浣稿硅\n\nMotive骞堕绠板版版褰规搴ㄥ拌棰棰锛杩琛浜涓绯诲**规х瀵硅棰规х**锛\n\n1.  **棣杩ㄥ棰瀹涔**锛棣娆℃哄苟绯荤у扮绌垛浜璁缁瑙棰娈靛奖浜瑙棰涓**杩**锛澶瑙璇涔姒蹇点\n2.  **稿锛杩ㄥ姊搴**锛杩**娴锛AllTracker锛璁＄杩ㄥ搴撅骞朵互姝や负╃瀵规╂ｆā澶辨搴杩琛绌洪村堕翠**杩浣垮褰淇″疯浜ㄦ哄锛瀹颁杩ㄤ澶瑙瑙ｈ\n3.  **瑙ｅ宠棰规**锛\n    - **涓╁**锛缁锋浼拌°遍烘сFastfood褰憋浣挎规藉ㄤ浜垮扮骇澶фā楂璐ㄩ瑙棰版\n    - **甯ч垮宸℃**锛瀵规搴杩琛甯ф板涓锛叉㈤胯棰娈靛姊搴绱绉峰楂褰般\n4.  **搴ㄤ环煎煎**锛纭灏褰缁ㄤ**瀵煎井璋版绛**瀹楠琛ㄦ锛浠浣跨10%Motive绛虹楂褰卞版寰璋妯″锛ㄨㄨ川涓借揪扮宠杩浣跨ㄥㄩㄦ版寰璋锛骞惰峰浜74.1%浜虹被濂借\n\n**昏瑷涔锛稿杩姝ュㄤ锛灏版褰绌惰瑙浠惧璇涔姒蹇碉ㄨ颁ㄦ瑙棰杩ㄩ锛骞朵负姝よ捐′涓濂瀹淬楂瑰妗讹跺绀轰跺ㄦ瑙棰妯″杩ㄨ川归㈢宸ㄥぇ瀹ㄤ环笺**",
      "timestamp": "2026-01-20T17:52:45.947536",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "璁烘轰浜抽规锛璇峰琛ㄦ煎蜂璇缁璇存缁锛瑕蜂板ㄥ硷浠ュ蜂般",
      "answer": "濂界锛浣涓轰涓涓瀛璁烘╂锛灏涓烘ㄦ繁ュ杩绡棰涓恒Motion Attribution for Video Generation璁烘锛归杩板舵虹抽规\n\n## 璁烘稿板\n\n稿璐＄轰 **Motive** 妗讹杩棣涓涓ㄩ瀵硅棰妯″**杩ㄥ**规瀹ㄥㄨВ充涓抽棰锛**浜璁缁瑙棰娈靛奖浜瑙棰涓杩ㄦā寮锛** 浼缁版褰规涓昏虫敞惧澶瑙锛娉澶瑙棰规堕村ㄦ规с\n\nMotive 杩涓绯诲帮瀹颁瀵硅棰妯″涓杩ㄢ褰卞绮惧楂杩芥函朵富瑕规荤濡涓锛\n\n| 抽规 | 稿 | 缁涓板 | 蜂/瀹 |\n| :--- | :--- | :--- | :--- |\n| **1. 轰姊搴╁褰妗** | 浣垮璁＄藉搴颁唬澶ц妯¤棰妯″版锛垮璁＄娴锋．╅甸绛涓琛浣 | ****锛**姊搴镐技搴**杩浼间缁褰卞芥般瀵逛╂ｆā锛ㄧ瑰澹版按骞 `t` 澹板渚 `系` 涓锛璁缁锋 `x_n` 瀵规璇锋 `x_test` 褰卞拌浼间负瀹浠澶卞芥版搴浣寮镐技搴锛<br> `I(x_n, x_test)  <胃L(胃; x_test), 胃L(胃; x_n)>` <br> **ㄥ间浼**锛<br> 1. **娴锋．╅佃浼**锛浣跨ㄥ浣╅典涓洪′欢瀛锛垮璁＄ `H_胃^{-1}`<br> 2. **卞烘**锛涓烘锛璁缁锛娴璇锛瀵**哄**涓涓 `(t_fix, 系_fix)` 瀵硅琛姊搴璁＄锛澶ч浣瑰樊锛绋冲搴<br> 3. **缁褰**锛浣跨 **Fastfood 褰** 灏楂缁存搴 (`D  1.4B`) 褰卞颁缁寸┖ (`D'`)锛捐浣瀛ㄥ璁＄褰辩╅ `P` 变绯诲缁╅碉杈剧寰风╅ `Q`瀵硅绗风╅ `B`烘╅ ``瀵硅楂╅ `G`瀵硅缂╂剧╅ `S`锛缁锛璁＄澶搴涓 `O(D' log D')` | - `t_fix = 751` (瀵瑰1000姝ュ诲杞ㄨ抗涓)<br>- `系_fix` 浠 `N(0, I)` 涓蜂娆″苟变韩<br>- 褰辩淮搴 `D' = 512` (瀹楠琛ㄦ姝ょ淮搴ㄦ涓搴淇搴村寰浣冲钩琛★涓ㄦ搴搴Spearman稿崇郴   74.7%)<br>- Fastfood 褰卞颁负 `P = (1/(尉D')) * S Q G  Q B` |\n| **2. 瑙棰瑰甯ч垮宸淇姝** | 娑ゅ瑙棰娈靛抚 (`F`) 涓瀵艰寸姊搴骞煎宸锛纭淇涓堕胯棰寸骞虫杈 | ****锛濮姊搴 `胃L` 伴甯镐瑙棰甯ф `F` 姝ｆ撮跨瑙棰浼浜х村ぇ姊搴锛浠ㄥ颁腑涓骞崇浼裤<br> **规**锛ㄨ＄姊搴杩琛褰卞褰涓涔锛瀵规搴杩琛**甯ф板涓**锛<br> `胃L_corrected = (1/F) * 胃L` <br> 杩纭淇浜褰板**姣甯х骞冲褰卞**锛瑙棰婚垮害 | 褰涓瀛涓 `1/F`锛朵腑 `F` 姣涓瑙棰娈电瀹甯ф般ㄥ楠涓锛瑙棰琚棰澶涓烘81甯э杩涓姝ョ‘淇浜骞虫с |\n| **3. 杩ㄦョ褰锛稿帮** | 灏褰淇″蜂**翠澶瑙****堕村ㄦ**涓锛姝ｅ扳杩ㄢ褰 | ****锛杩**杩ㄥ澶辨╃**锛ㄨ＄姊搴讹瀵硅棰涓ㄦ哄澶辫浜撮锛瀵归哄璧浜翠<br> **蜂姝ラ**锛<br> 1. **杩ㄦ**锛浣跨 **AllTracker** 涓鸿棰 `v` 璁＄姣甯ф涓绱娴浣绉诲 `D_f(h,w)` 杩ㄥ搴 `M_f(h,w) = ||D_f(h,w)||_2`<br> 2. **╃**锛灏杩ㄥ搴褰涓 `[0,1]` 洪达寰板绱绾ф `W(f,h,w)`跺杩绾挎т峰跺归VAE娼ㄧ┖寸 `(H/s, W/s)`锛寰版ㄧ┖村归 `糯(f, 磨, 诺)`<br> 3. **杩ㄥ澶**锛瀹涔轰杩ㄦ浣缃澶便瀵逛哄 `(t_fix, 系_fix)`锛ㄦㄧ┖翠缃 `(f, 磨, 诺)` 澶变负棰娴澹颁澹扮骞虫硅宸杩ㄥ澶辨杩浜浣缃澶辩骞冲锛<br> `L_mot(胃; v, c) = (1/F) * mean_{f, 磨, 诺} [ 糯(f, 磨, 诺) * L_{胃,v,c}(f, 磨, 诺) ]` <br> 4. **杩ㄦユ搴**锛璁＄璇ヨㄥ澶辩姊搴 `g_mot = 胃 L_mot`锛骞剁ㄥ舵夸唬濮姊搴杩琛缁褰卞镐技搴璁＄ | - 浣跨 **AllTracker** 娴锛4锛dx, dy, 瑙э缃淇″害锛<br>- 杩ㄥ搴褰涓舵坊灏甯告 `味 = 10^{-6}` 叉㈤ら躲<br>- VAE涓峰瀛 `s = 8` (靛惊Wan2.1楠ㄥ共缃缁)<br>- 澶卞**娼ㄧ┖**杩琛锛涓妯″杩绋瑙ｈ锛浠褰卞褰璁＄ |\n| **4. 轰褰寰璋版** | ╃ㄨㄥ帮浠娴烽版涓鸿界哄圭杩ㄦā寮褰卞瀛杩琛楂寰璋 | ****锛瀵逛缁瀹杩ㄦヨ㈣棰 `v`锛璁＄版涓姣涓璁缁锋 `v_n` 杩ㄦュ奖 `I_mot(v_n, v)`<br> **╃**锛<br> - **ヨ**锛存ラ╁版楂 Top-K 涓锋<br> - **澶ヨ**锛撮妫锛锛**澶版绁ㄦ**瀵逛澶涓ヨ `{v_q}`锛姣涓璁缁锋 `v_n` 峰涓涓绁ㄦ扳锛冲跺拌杩涓惧浣 `` ヨ㈡伴缁╁绁ㄦ版楂 Top-K 涓锋锛<br> `MajVote_n = 危_{q=1}^Q I[ I_mot(v_n, v_q) >  ]` <br> `S_vote(K) = {v_n | v_n  MajVote  K }` | - ㄤ富瑕瀹楠涓锛`K` 璁剧疆涓烘版 **10%**<br> - 澶版绁ㄦ垮浜涓ヨ㈤村板昂搴″棰<br> - 璇ユ规藉绛哄ㄥ绉杩ㄤ借〃板洪褰卞ㄢ楂璐ㄩ杩ㄦ版 |\n\n## 瀹浠峰间涔\n\n1.  **瑙ｉу伐**锛棣娆′负瑙棰妯″渚浜杩ㄥ⑩瑙ｉу伐凤浣跨绌惰藉瑙ｆā涓轰浠ユ绉瑰杩锛骞惰杩ㄤ吉褰憋濡杞ㄨ抗涓涓淬╃涓锛ユ\n2.  **楂版娌荤**锛渚浜涓绉版椹卞ㄧ纭寰璋版绛规瀹楠琛ㄦ锛浠浣跨 **10%**  Motive 绛楂褰卞版寰璋妯″锛ㄨㄥ钩婊搴ㄦ绋搴绛涓冲杈惧版瓒瓒浣跨ㄥㄩㄦ版寰璋锛骞跺ㄤ汉绫昏浼颁腑峰浜 **74.1%** 濂借\n3.  **璁＄琛**锛杩锋浼拌°卞烘у缁褰辩浼锛浣垮瀵瑰浜垮扮骇妯″颁瑙妯℃版杩琛褰璁＄寰琛锛戒锛浣宸蹭涓琛涓衡绠＄锛\n4.  **虹绌惰础**锛楠璇浜杩ㄥ涔杩芥函冲蜂璁缁锋杩涓璁撅涓轰版瑙搴瑙ｅу剁妯″ㄦ琛涓哄杈浜伴寰\n\n**荤**锛Motive 稿绐村ㄤ灏浼缁版褰筹杩**杩ㄥ澶辨╃****瑙棰瑰宸淇姝****╁姊搴杩浼**绛涓绯诲绮惧璁捐★伴颁楂缁淬跺瑙棰浠诲′腑锛瀹颁浠澶瑙褰扳杩ㄥ寮杞锛煎烽瑕璁烘涔瀹搴ㄤ环笺",
      "timestamp": "2026-01-20T17:53:50.524798",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "浠浣跨ㄤ浜璇浠锋涓版锛琛ㄦ煎蜂璁茶蹭璇浠锋缁涓版缁",
      "answer": "## 璁烘Motion Attribution for Video Generation璇浠锋涓版\n\n### 涓 涓昏璇浠锋\n\n璁烘浣跨ㄤ**ㄥ哄娴璇锛VBench锛****浜哄伐璇浼**涓ょ瑰锛ㄩ㈣　妯″ㄨㄧ璐ㄩ涓琛ㄧ般\n\n#### 1. VBench ㄥ璇浼\nVBench 涓涓缁煎х瑙棰璇浼板哄璁烘瑰虫敞浜浠ヤ涓缁村害锛\n\n|  | 辨ㄧО | 稿瀹涔涓娴缁 |\n| :--- | :--- | :--- |\n| **涓讳涓存** | Subject Consistency | 琛￠瑙棰涓涓昏╀锛濡浜虹┿ㄧ╋ㄦ堕寸淮搴涓韬浠姐褰㈢跺澶瑙淇绋冲锛垮韬浠芥绉烩甯搁杩璺ㄥ抚瑰镐技搴璁＄ |\n| **涓存** | Background Consistency | 璇浼拌棰绋冲э垮涓ㄣ绐层虫敞杩ㄥ哄跺杩璐с |\n| **杩ㄥ钩婊搴** | Motion Smoothness | ╀告鸿ㄨ建杩圭娴с杩娴瑰圭杞ㄨ抗锛妫娴瀛ㄧ纭璺宠￠】涓剁搴 |\n| **ㄦ绋搴** | Dynamic Degree | **璁烘稿虫敞**琛￠瑙棰涓浜澶灏杩ㄢ杩璁＄翠釜瑙棰搴涓娴骞肩缁璁￠锛濡笺瑰樊锛ヨ浼拌ㄧ涓板у寮哄害楂琛ㄧず瑙棰婊″ㄦ |\n| **缇瀛璐ㄩ** | Aesthetic Quality | 璇浼拌棰甯х瑙瑙缇锛濡俱插僵х甯镐娇ㄩ璁缁缇瀛璇妯″瀵归峰抚杩琛骞跺骞冲 |\n| **璐ㄩ** | Imaging Quality | 琛￠涓瑙棰甯х娓板害╁害浣澹扮搴甯哥ㄦ**FID锛Fr茅chet Inception Distance锛****CLIP Score**锛涓楂璐ㄩ惧甯杩琛姣杈 |\n\n**璁烘渚ч**锛浣纭猴**杩ㄥ钩婊搴 (Motion Smoothness)**  **ㄦ绋搴 (Dynamic Degree)** 璇浼板**杩ㄥ妗 (Motive)** **涓昏**朵ㄤ纭淇ㄦ杩ㄨ川讹涓瀹冲烘瑙瑙璐ㄩ涓存с\n\n#### 2. 浜哄伐璇浼 (Human Evaluation)\n变ㄥ芥娉瀹ㄦ浜虹被瀵硅ㄨ川ワ璁烘杩琛浜涓ヨ皑浜哄伐 pairwise 姣杈\n\n- **规**锛璁╁涓瑙涓绀鸿涓变规涓ゆ佃棰锛╁涓娈电**杩ㄦ翠**锛璁稿钩灞锛\n- **瑙妯**锛17 浣娉ㄥ锛瀵 10 绫昏ㄥ 5 涓娴璇妗渚锛 50 涓瑙棰锛杩琛澶杞涓や袱姣杈锛昏 850 娆″ゆ\n- **ュ**锛\n    - ** (Win Rate)**锛规琚濂界姣渚\n    - **骞冲 (Tie Rate)**锛涓よ琚璁や负璐ㄩ稿姣渚\n    - **璐 (Loss Rate)**锛虹嚎规琚濂界姣渚\n- ****锛渚**ュ**杩ㄨ川楠璇锛ㄥ瑕琛ュ\n\n### 浜 浣跨ㄧ版\n\n璁烘ㄥ井璋锛fine-tuning锛璇浼伴舵典娇ㄤ澶涓澶ц妯°楂璐ㄩ瑙棰版\n\n#### 1. 寰璋版 (Fine-tuning Datasets)\nㄤ楠璇 `Motive` 妗剁虹版瀛杩琛寰璋\n\n| 版 | 瑙妯★璁烘ㄥ锛 | 圭逛缁 |\n| :--- | :--- | :--- |\n| **VIDGEN-1M** | 10k 瑙棰 | 澶ц妯℃-瑙棰瀵规版锛娑电澶峰杩ㄦā寮澶烘涓板跺ㄦㄤ楠璇规ㄩㄨ棰版涓с |\n| **4DNeX-10M** | 10k 瑙棰 | 涓涓澶ц妯°楂璐ㄩ瑙棰版锛锋渚涓板剁┖ㄦ澶烘ㄤ娴璇规娉уㄤ版甯涓琛ㄧ般 |\n\n**浣跨ㄦ瑰**锛璁烘浠杩涓や釜版涓藉 10k 瑙棰浣涓哄井璋姹`Motive` 妗剁浠涓绛哄圭杩ㄦ褰卞瀛锛渚濡 top 10%锛ㄤ寰璋锛骞朵浣跨ㄥㄦ版哄寰璋杩琛瀵规\n\n#### 2. ヨ㈡版 (Query Datasets)\nㄤ椹卞 `Motive` 杩琛褰璁＄锛斥浠甯妯″瀛浼绉杩ㄢ\n\n- ****锛涓涓绮惧绛**瑙棰**锛 **10 绫绘纭╀杩**锛\n    `compress`锛缂╋, `bounce`锛寮硅烦锛, `roll`锛婊锛, `explode`锛革, `float`锛婕娴锛, `free fall`锛辫戒锛, `slide`锛婊锛, `spin`锛杞锛, `stretch`锛浼革, `swing`锛锛\n- **瑙妯**锛姣绫昏 5 涓瑙棰锛 **50 涓ヨ㈣棰**\n- **涓绛**锛浣跨 **Veo-3** 妯″规杩版ф绀虹锛骞剁杩浜哄伐绛锛纭淇杩ㄦ般╃骞插锛浠**绂诲虹瑰杩ㄥ哄**\n- **浣**锛杩浜ヨ㈣棰涓浣涓鸿缁版锛浣涓哄娴璇光锛ㄤ璁＄璁缁涓姣涓瑙棰瀵硅ョ被杩ㄧ褰卞般\n\n#### 3. 娴璇绀洪 (Evaluation Prompts)\nㄤㄥ井璋**瀹璇浼**妯″藉绀洪\n\n- ****锛涓ヨ㈡版瀵瑰 **10 绫昏**锛浣浣跨**涓╀烘杩**锛浠ラ垮杩\n- **瑙妯**锛姣绫昏 5 涓绀鸿锛 **50 涓璇浼版绀**\n- **绀轰**锛\n    - ヨ㈤绀猴ㄤヨ㈣棰锛锛涓涓涔涔ㄧ借叉涓寮硅烦...\n    - 娴璇绀猴ㄤ璇浼版ā锛锛涓涓绡ㄦㄦ垮版夸村脊璺...\n- **浣**锛灏杩浜绀鸿ョ寰璋妯″瑙棰锛跺 VBench 瀵圭瑙棰杩琛ㄥ璇锛浠瀹㈣姣杈涓寰璋绛ョ浼ｃ\n\n### 荤琛ㄦ硷璇浼颁绯绘瑙\n\n| 璇浼扮被 | 蜂椤圭 | ㄩ涓 |\n| :--- | :--- | :--- |\n| **ㄥ (VBench)** | 涓讳/涓存с杩ㄥ钩婊搴**ㄦ绋搴**缇瀛/璐ㄩ | 瀹㈣澶拌　瑙棰ㄥ缁村害涓璐ㄩ锛**瑰虫敞杩ㄧ稿虫** |\n| **浜哄伐璇浼** | Pairwise 姣杈/骞/璐 | 渚浜虹被瀵硅ㄨ川ュゆ锛楠璇ㄥ缁璁恒 |\n| **寰璋版** | VIDGEN-1M, 4DNeX-10M (10k瀛) | 渚澶ц妯°楂璐ㄩ寰璋版姹锛ㄤ楠璇版绛с |\n| **ヨ㈡版** | 10绫昏  5瑙棰 (50涓锛Veo-3) | 浣涓哄锛椹卞 `Motive` 妗惰哄圭瑰杩ㄦ褰卞璁缁版 |\n| **娴璇绀洪** | 10绫昏  5绀 (50涓锛涓ヨ㈤璇涔瀵瑰浣杩颁) | ㄤㄦā寰璋锛瑙棰骞惰琛ㄥ璇浼帮纭淇璇浼扮娉с |\n\n**稿缁璁**锛璁烘寤轰涓濂浠**瀹涔锛ヨ㈣棰锛** -> **版褰涓绛锛Motive锛** -> **妯″寰璋** -> **澶缁村害璇浼帮ㄥ+浜哄伐锛** 瀹村楠惰浠锋绱у寸 **杩ㄨ川** 杩涓稿锛跺奸【浜瑙瑙淇搴锛拌浜 `Motive` 妗跺ㄧ瑙ｅ瑙棰妯″杩ㄥㄥ瀛归㈢浠峰笺",
      "timestamp": "2026-01-20T17:54:46.687323",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "璁烘ㄥ浜版涓杩琛浜瀹楠锛涓昏璇浼版ц芥澶灏锛",
      "answer": "## 璁烘瀹楠涓璇浼板\n\n### 涓瀹楠版\n璁烘ㄤ袱涓澶ц妯¤棰版涓杩琛浜瀹楠锛\n\n- **VIDGEN-1M** (Tan et al., 2024)\n- **4DNeX-10M** (Chen et al., 2025)\n\n瀹楠涓锛浠姣涓版涓藉浜 **10k 涓瑙棰** 浣涓哄井璋版锛浠ョ‘淇瀹楠瑙妯″澶锋с\n\n### 浜涓昏璇浼版\n浣跨 **VBench** (Huang et al., 2024) 哄杩琛璇浼帮瑰虫敞浠ヤ涓缁村害锛\n\n1.  **涓讳涓存** (Subject Consistency)\n2.  **涓存** (Background Consistency)\n3.  **杩ㄥ钩婊搴** (Motion Smoothness)\n4.  **ㄦ绋搴** (Dynamic Degree) - **稿**\n5.  **缇瀛璐ㄩ** (Aesthetic Quality)\n6.  **璐ㄩ** (Imaging Quality)\n\n朵腑锛**杩ㄥ钩婊搴**  **ㄦ绋搴** 璇浼版堕村ㄦ锛宠ㄨ川锛涓昏\n\n### 涓ц芥涓涓昏缁\n\n#### 1. 瀹缁 (VBench)\nㄤ浣跨 **10%** 寰璋版锛 **Motive** 规╋典锛妯″ц借〃板涓锛涓虹嚎妯″瀵规锛锛\n\n| 规 | ㄦ绋搴 (Dynamic Degree) | 涓讳涓存 | 杩ㄥ钩婊搴 | 澶娉 |\n| :--- | :--- | :--- | :--- | :--- |\n| **虹妯″** (寰璋) | 39.6% | 95.3% | 96.3% | 哄 |\n| **ㄦ版寰璋** | 42.0% | 95.9% | 96.3% | ц戒 |\n| **洪** (10%版) | 41.3% | 95.3% | 96.3% | 哄虹嚎 |\n| **Ours w/o 杩ㄦ╃** | 43.8% | 95.4% | 96.3% | 娑瀹楠 |\n| **Ours (Motive)** | **47.6%** | **96.3%** | 96.3% | **规** |\n\n**抽缁璁**锛\n- **ㄦ绋搴**锛**Motive** 规锛47.6%锛捐浼浜洪╋41.3%锛ㄦ版寰璋锛42.0%锛锛**稿瑰虹妯″绾8涓惧**\n- **杩ㄨ川**锛ㄤ杩ㄥ钩婊搴涓涓涓锛澶у浜瑙棰ㄦ琛ㄧ般\n- **版**锛浠10%版锛ㄦ稿涓杈惧版瓒杩浜浣跨100%版寰璋ц姐\n\n#### 2. 浜哄伐璇浼\n杩琛浜瀵规杈浜虹被濂借浼帮50涓瑙棰锛17涓锛锛\n\n| 瀵规缁 | **Motive 疯** | 骞冲 | 澶辫触 |\n| :--- | :--- | :--- | :--- |\n| vs. 虹妯″ | **74.1%** | 12.3% | 13.6% |\n| vs. 洪 | **58.9%** | 12.1% | 29.0% |\n| vs. ㄦ版寰璋 | **53.1%** | 14.8% | 32.1% |\n\n**缁璁**锛浜虹被璇浼拌捐濂界 **Motive** 绛版寰璋瑙棰锛灏ゅ舵ㄨㄨ川归\n\n### 瀹浠峰兼荤\n- ****锛棣娆℃轰瀵硅棰妯″**杩ㄥ妗**锛藉杩芥函瑙棰涓ㄦ板蜂璁缁娈点\n- **ц戒环**锛璇浜杩**杩ㄦョ版绛**杩琛瀵规у井璋锛浠**ㄦ灏版锛10%锛瀹拌ㄨ川捐**锛涓洪璐ㄩ瑙棰渚浜涓绉楂版╃ㄦ规\n- **搴ㄥ**锛璇ユ跺ㄤ璇妯″杩ㄤ吉褰便瀵兼版娓娲涓 curation锛骞舵缁甯╂寤烘村с瑙ｉ瑙棰绯荤",
      "timestamp": "2026-01-20T17:55:15.877875",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "璁烘涓昏灞ф浜锛ュ界硅瑰浠涔锛",
      "answer": "## 璁烘Motion Attribution for Video Generation锛灞тユ瑰\n\n轰瀵硅烘娣卞ュ锛浠ヤ璇ョ绌讹MOTIVE妗讹涓昏灞уュ界硅瑰\n\n### 涓昏灞\n\n1.  **璁＄楂**\n    - **稿堕**锛轰姊搴褰规瑕涓**姣涓璁缁锋**璁＄姊搴锛瀵逛颁唬澶ц妯¤棰版浜垮版ā锛杩涓娆℃т宸ㄥぇ璁＄寮\n    - **蜂版**锛ㄥ寮A100 GPU涓锛涓1涓涓璁缁锋璁＄姊搴绾150灏躲界跺骞惰锛64寮GPU绾2.3灏讹涓缁缁澶涓ヨ锛浣浠瀹ㄧ讲纰\n\n2.  **瑙棰绾у绮搴棰**\n    - 璁烘灏**翠釜瑙棰娈**浣涓轰涓杩琛褰杩崇锛濡涓涓胯棰涓涓灏娈靛稿宠锛跺奖浼琚崇甯р绋锛瀵艰村淇″蜂澶绮剧‘\n    - 杩朵规璇**瑰杩ㄤ浠舵杞ㄨ抗娈**藉\n\n3.  **杩ㄦ╃渚璧т灞**\n    - 杩ㄦ涓ラ渚璧浜澶ㄨㄤ及璁″锛濡璁烘浣跨ㄧAllTracker锛**涓ラ°╀寰灏杩**典锛浼拌″ㄥ藉け锛瀵艰存╃涓纭\n    - 褰规句互瀹**哄鸿ㄤ╀韬杩**界惰烘杩绌洪村ф娴ラ绾鸿ㄧ娈碉浣褰诲瑙ｈ浠瑙ｅ崇棰\n\n4.  **绫诲ㄦ冲瀵**\n    - 璁烘褰璁＄轰璁缁锛诲澶憋锛浣瀹瑙棰涓骞挎浣跨**绫诲ㄦ冲瀵**ユ璐ㄩCFG浼瑰ㄧ剁ㄦ琛涓猴褰褰妗跺苟寤烘ā杩涓杩绋锛姝ゅ浼拌″芥娉瀹ㄥCFG褰卞涓缁杩ㄨ涓恒\n\n5.  **寰璋藉甫ョ藉琛**\n    - 浣跨MOTIVE╃楂褰卞版杩琛瀵规у井璋锛ㄦ**瑰杩ㄨ川**锛濡骞虫搴ㄦ绋搴锛讹**芥瀹冲虹妯″ㄥ朵浠诲℃杩ㄧ被涓娉藉**璁烘娣卞ユ㈣ㄨ绉琛°\n\n### ュ界硅瑰\n\n1.  **璁＄涓瀛ㄦ**\n    - 寮撮姊搴杩浼兼**绋规**锛灏姣涓锋姊搴璁＄\n    - ㈢储**灞褰**锛蹇绛锛瀵归娼锋杩琛绮剧璁＄\n\n2.  **瀹版寸绮搴褰**\n    - 浠瑙棰绾уㄨ**娈电骇锛sub-clip锛冲抚寸骇褰**杩芥寸簿板浣褰卞瑰杩ㄦā寮抽璁缁跺汇\n    - 缁**杩ㄥ叉浜浠舵娴**锛ㄨ瑙棰涓杩ㄧ娈佃琛绔\n\n3.  **澧寮鸿ㄤ及璁＄椴妫т瑙ｈ藉**\n    - **澶杩ㄤ及璁″ㄩ**锛╃ㄦāㄨ〃寰ョ杩ㄦ捐у撅灏瀵瑰ㄨ拷韪ㄧ渚璧\n    - **哄Э浼拌℃娴瑙ｆ**锛村ソ板哄烘杩锛ego-motion锛涓╀杩锛浣垮寸函绮瑰板虫敞╀ㄥ瀛\n\n4.  **寤洪涓涓诲ㄦ版绠＄娴绋**\n    - 灏涓娆℃у--寰璋╁涓**杩浠ｅ涓诲ㄥ涔妗**锛寰璋锛ㄦ版ā涓拌＄褰锛杩涓姝ョ版锛缁浼\n    - 寮**轰瀛涔ヨ㈡**锛夸唬绠澶版绁ㄦ锛存鸿藉拌澶ヨ㈢褰缁\n\n5.  **╁妗剁搴ㄨ翠娌荤藉**\n    - **瀹ㄤ娌荤**锛╃ㄨ褰卞锛瀹虫涓杩ㄦā寮锛璇锛浠璁缁涓杩婊ゆ棰版锛涓虹妯″瀹¤′瀹ㄦ渚宸ュ枫\n    - **澶妯℃╁**锛灏杩ㄥ妗舵╁**涓妯″抽宠棰**绛棰锛瑙ｆ版ㄥ妯℃浠诲′腑褰卞\n    - **璇**锛浣跨ㄦā韬瀛ㄨㄤ吉褰辩瑙棰浣涓烘ヨ锛杩借釜瀵艰磋浜棰璁缁版锛瀹**杩浠ｅ妯″璇涓淇澶**\n\n6.  **缁村杩寰璋绛**\n    - ㈢储灏MOTIVE╃版ㄤ**澶瀛搁伴寰璋**绛村寰璋璁剧疆锛ㄦ杩ㄨ川讹村ソ颁虹妯″骞挎藉\n\n### 荤\nMOTIVE妗跺**瑙棰妯″杩ㄦ版褰**棰轰寮ц础锛浣跺т富瑕涓**璁＄╁с褰绮搴杩ㄤ及璁′璧**浠ュ**涓瀹娴绋宸璺**涓ョ宸ヤ杩绠娉浼绮搴缁娴绋浠ュ搴ㄦ╁锛浣挎版褰涓烘寤烘村с瑙ｉ楂ц借棰妯″稿宸ュ枫",
      "timestamp": "2026-01-20T17:55:53.871072",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    }
  ],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-13T18:59:09Z",
  "created_at": "2026-01-20T17:49:46.593195",
  "updated_at": "2026-01-20T17:49:46.593204"
}