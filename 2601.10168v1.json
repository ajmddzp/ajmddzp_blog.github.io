{
    "id": "2601.10168v1",
    "title": "RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation",
    "authors": [
        "Yue Chang",
        "Rufeng Chen",
        "Zhaofan Zhang",
        "Yi Chen",
        "Sihong Xie"
    ],
    "abstract": "å¼€æ”¾è¯æ±‡ä¸‰ç»´åœºæ™¯å›¾ç”ŸæˆæŠ€æœ¯é€šè¿‡åˆ©ç”¨ç»“æ„åŒ–è¯­ä¹‰è¡¨ç¤ºï¼Œå¯å¢å¼ºæœºå™¨äººæ“ä½œä¸å¯¼èˆªç­‰ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚ä¸‰ç»´åœºæ™¯å›¾åŸºäºåœºæ™¯å¤šè§†è§’å›¾åƒæ„å»ºï¼Œå…¶ä¸­å¯¹è±¡è¡¨ç¤ºä¸ºèŠ‚ç‚¹ï¼Œå…³ç³»è¡¨ç¤ºä¸ºè¾¹ã€‚ç„¶è€Œï¼Œç°æœ‰å¼€æ”¾è¯æ±‡ä¸‰ç»´åœºæ™¯å›¾ç”Ÿæˆæ–¹æ³•å­˜åœ¨ç‰©ä½“è¯†åˆ«å‡†ç¡®ç‡ä½ã€å¤„ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ï¼Œä¸»è¦å—é™äºè§†è§’çº¦æŸã€é®æŒ¡ç°è±¡åŠå†—ä½™è¡¨é¢å¯†åº¦ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºRAG-3DSGæ–¹æ³•ï¼šé€šè¿‡é‡æ‹æ‘„å¼•å¯¼çš„ä¸ç¡®å®šæ€§ä¼°è®¡é™ä½èšåˆå™ªå£°ï¼Œå¹¶å€ŸåŠ©å¯é çš„ä½ä¸ç¡®å®šæ€§å¯¹è±¡å®ç°ç‰©ä½“çº§æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºåŠ¨æ€é™é‡‡æ ·æ˜ å°„ç­–ç•¥ï¼Œé€šè¿‡è‡ªé€‚åº”ç²’åº¦åŠ é€Ÿè·¨å›¾åƒç‰©ä½“èšåˆã€‚åœ¨Replicaæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRAG-3DSGæ˜¾è‘—æå‡äº†ä¸‰ç»´åœºæ™¯å›¾ç”Ÿæˆä¸­çš„èŠ‚ç‚¹æè¿°å‡†ç¡®ç‡ï¼ŒåŒæ—¶å°†æ˜ å°„æ—¶é—´è¾ƒåŸå§‹ç‰ˆæœ¬ç¼©çŸ­ä¸‰åˆ†ä¹‹äºŒã€‚",
    "url": "https://arxiv.org/abs/2601.10168v1",
    "html_url": "https://arxiv.org/html/2601.10168v1",
    "html_content": "RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation\nYue Chang, Rufeng Chen, Zhaofan Zhang, Yi Chen, Sihong Xie\nAI Thrust, HKUST(GZ)\nAbstract\nOpen-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations.\nA 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges.\nHowever, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density.\nTo address these challenges, we propose\nRAG-3DSG\nto mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented GenerationÂ (RAG) via reliable low-uncertainty objects.\nFurthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity.\nExperiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.\n1\nINTRODUCTION\nCompact and expressive representation of complex and semantic-rich 3D scenes has long been a fundamental challenge in robotics, with direct impact on downstream tasks such as robot manipulation\n(Shridhar etÂ al.,\n2022\n; Rashid etÂ al.,\n2023\n)\nand navigation\n(Gadre etÂ al.,\n2022\n; Shah etÂ al.,\n2023\n)\n.\nA promising solution is 3D scene graphsÂ (3DSGs)\n(Armeni etÂ al.,\n2019\n; Gay etÂ al.,\n2018\n)\n, which encode a scene into a graph where nodes denote objects and edges capture their pairwise relationships.\nEarly efforts focus on building 3DSGs by detecting objects and relationships from a closed vocabulary\n(Hughes etÂ al.,\n2022\n; Rosinol etÂ al.,\n2021\n; Wu etÂ al.,\n2021\n)\n.\nWhile these methods perform well and are efficient in fixed environments, their reliance on a closed vocabulary restricts their generalization to unseen scenes in the real-world.\nTo mitigate this limitation, recent approaches\n(Gu etÂ al.,\n2024\n; Werby etÂ al.,\n2024\n; Koch etÂ al.,\n2024\n; Maggio etÂ al.,\n2024\n; Jatavallabhula etÂ al.,\n2023\n)\nleverage foundation models to provide open-vocabulary 3DSG generation, which produces more expressive representations for more diverse scenes.\nDespite this progress, open-vocabulary approaches largely adhere to the pre-defined one-way pipeline of per-image object-level information extraction followed by cross-image aggregation.\nHowever, as illustrated in the upper part of Figure\n1\n, constrained viewpoints, occlusions, and other poor imaging conditions can introduce significant level of noise to object-level information extraction and reduces the accuracy of cross-image aggregation.\nFor example, as shown in Figure\n1\n(b), although the object of interest is the table, the presence of an occluding vase leads to multiple crop captions being mistakenly recognized as vase.\nWhen aggregating multiple captions or embeddings across images, such misleading semantics can compromise the accuracy of the resulting 3DSGs. Such noise-induced inaccuracies in 3DSGs are unacceptable for downstream robotic tasks, particularly in safety-critical scenarios.\nFor example, if a medication bottle is incorrectly described as a beverage container, a service robot could deliver dangerous substances to humans.\nTherefore, it is crucial to assess the uncertainty in per-image object-level information and mitigate the noise accordingly.\nAt the same time, we observe that existing methods use object crops only and miss the comprehensive descriptors of the target object and also the broader contextual information, which provide valuable clues for foundation modelsâ€™ captioning.\nFigure 1:\nAn overview of the RAG-3DSG framework.\nThe upper part illustrates common challenges in multi-view 3D scene graph generation.\nOur pipeline addresses these issues through\n(a) Multi-view RGB-D frames are segmented and fused into a global object list with point clouds and semantic embeddings.\n(b) Re-shot images are used to select the best-view caption, which is compared with crop captions to estimate uncertainty; clustering is applied and the top-1 cluster is retained for fusion.\n(c) Low-uncertainty objects form a retrieval document, while high-uncertainty objects leverage retrieved context for caption refinement via a VLM.\n(d) Finally, spatial and semantic relationships among objects are inferred by an LLM to construct the 3D scene graph.\nTo address these challenges, we propose\nRAG-3DSG\n, a 3DSG generation framework that mitigates noise in aggregation through re-shot guided uncertainty estimation.\nIt treats the 3DSG under construction as a database, and use the principle of Retrieval-Augmented Generation (RAG) to leverage surrounding context to enhance the representation of objects with high uncertainty.\nSpecifically, we first adopt a dynamic downsample-mapping strategy to construct a global 3D object list with low computational cost (Figure\n1\na).\nInspired by the concept of optimal viewpoint\n(VÃ¡zquez etÂ al.,\n2001\n)\n, our method then renders the object point clouds reconstructed from multiple images to obtain the best-view re-shot images.\nNext, we perform uncertainty estimation by comparing object captions generated from these re-shot images with those from the crop images (Figure\n1\nb).\nLow-uncertainty objects are directly used to construct an object document, while high-uncertainty objects will trigger the retrieval of surrounding high certainty object documents on the 3DSG to guide caption refinement (Figure\n1\nc), iteratively improve the accuracy of 3DSG.\nExperiments on Replica\n(Straub etÂ al.,\n2019\n)\ndataset demonstrate that our method consistently outperforms existing baselines, achieving an average node precision of 0.82 (vs. 0.68 for ConceptGraphs\n(Gu etÂ al.,\n2024\n)\n) and edge precision of 0.91 (vs. 0.85 for ConceptGraphs-Detector\n(Gu etÂ al.,\n2024\n)\n), while maintaining more valid objects and edges.\nIn addition, our dynamic downsample-mapping strategy reduces the time of 3D object mapping by nearly two-thirds compared to the fixed downsample-mapping strategy in ConceptGraphs\n(Gu etÂ al.,\n2024\n)\n(from 6.65s/iter to 2.49s/iter when the base voxel size is set to 0.01 on Replica Room 0), demonstrating improved accuracy, robustness, and efficiency in 3DSG construction.\n2\nRELATED WORK\nScene Graph Generation (SGG)\nScene graphs were initially introduced in the 2D domain to extract objects and their relationships from images, providing a structured representation that supports a highly abstract understanding of scenes for intelligent agents\n(Sun etÂ al.,\n2023\n; Liu etÂ al.,\n2021\n; Yin etÂ al.,\n2018\n; Krishna etÂ al.,\n2017\n; Lu etÂ al.,\n2016\n; Johnson etÂ al.,\n2015\n)\n.\nA scene graph is typically composed of three fundamental elementsâ€”objects, attributes, and relationsâ€”and can be expressed as a set of visual relationship triplets in the form of <subject, relation, object>\n(Li etÂ al.,\n2024\n)\n.\nIn the field of 3D scene representations, several works\n(Wald etÂ al.,\n2020\n; Kim etÂ al.,\n2019\n; Armeni etÂ al.,\n2019\n)\nhave drawn inspiration from 2D scene graphs and extended the concept into the 3D domain.\nCompared with original 3D scene representations such as point clouds with per-point semantic vector, which are often overly dense and difficult to interpret, 3D scene graphs (3DSGs) provide a more compact and structured abstraction of the scene.\nBy organizing objects and their relationships into a graph representation, they enable more efficient reasoning and facilitate downstream tasks such as robotic navigation\n(Gadre etÂ al.,\n2022\n; Shah etÂ al.,\n2023\n)\nand scene understanding\n(Rana etÂ al.,\n2023\n; Agia etÂ al.,\n2022\n)\n.\nEarly efforts in 3D scene graph generationÂ (3DSGG) enabled the construction of real-time systems capable of dynamically building hierarchical 3D scene representations\n(Hughes etÂ al.,\n2022\n; Rosinol etÂ al.,\n2021\n; Wu etÂ al.,\n2021\n)\n.\nHowever, these methods were confined to the closed-vocabulary setting, which restricted their applicability to a limited range of tasks.\nMore recently, several works\n(Gu etÂ al.,\n2024\n; Werby etÂ al.,\n2024\n; Koch etÂ al.,\n2024\n; Maggio etÂ al.,\n2024\n; Jatavallabhula etÂ al.,\n2023\n)\nhave begun to explore open-vocabulary approaches for 3D scene graph generation.\nUsing Vision Language Models (VLMs) and Large Language Models (LLMs), these methods sacrifice part of the real-time capability but significantly expand the range of object categories and relations that can be recognized, thus broadening the applicability to a wider variety of downstream tasks.\nOpen-vocabulary 3DSGG\nRecent advances have extended 3DSGG to the open-vocabulary setting by leveraging VLMs and LLMs.\nThe existing approaches can be divided into two main paradigms.\nThe first paradigm follows a caption-first strategy, where objects in each image are independently described and later aggregated into scene-level semantics.\nIn practice, multiple masked views of the same object are captioned separately, and a LLM is then used to aggregate these descriptions into a final caption (e.g.,\nGu etÂ al. (\n2024\n)\n).\nThe second paradigm adopts an embedding-first strategy, where semantic embeddings of multiple masked views of the same object are first extracted and aggregated (through like weighted fusion) without explicit captioning.\nThe aggregated embeddings are then converted into captions or directly aligned with user queries in a joint visionâ€“language space using models such as CLIP\n(Werby etÂ al.,\n2024\n; Koch etÂ al.,\n2024\n; Jatavallabhula etÂ al.,\n2023\n)\n.\nIn addition, some works assign captions to object embeddings by matching them against an arbitrary vocabulary with CLIP, a strategy commonly referred to as open-set 3DSG\n(Maggio etÂ al.,\n2024\n)\n.\nDespite their differences, both paradigms share the same underlying pipeline of per-image information extraction followed by cross-image information aggregation.\nHowever, the information extraction process is often affected by factors such as constrained viewpoints and object occlusion, which introduce noise into the representations.\nAs a result, aggregated information may be inaccurate, leading to reduced precision in the node and edge captions of the 3DSGs.\nThis limitation cannot be simply resolved by adopting more powerful captioning models, as the noise originates from inherent challenges in multi-view perception such as occlusion and viewpoint constraints.\nTime Considerations in 3DSGG\nClosed-vocabulary 3D scene graph generation methods are typically lightweight and efficient, since they operate on a fixed set of semantic categories.\nThis efficiency enables some approaches to support online\n(Wu etÂ al.,\n2023\n)\nor even faster-than-real-time construction (e.g.,\nHou etÂ al. (\n2025\n)\nachieves 7 ms end-to-end latency).\nIn contrast, open-vocabulary scene graph generation relies on VLMs or LLMs to provide flexible semantics, which introduces significant runtime overhead.\nAs a result, existing open-vocabulary methods\n(Gu etÂ al.,\n2024\n; Koch etÂ al.,\n2024\n)\nare usually performed offline, making real-time deployment challenging.\nUnlike strictly open-vocabulary methods,\nMaggio etÂ al. (\n2024\n)\nadopts an open-set formulation, which relaxes category constraints without relying on heavy VLM/LLM inference.\nThis design makes real-time 3DSGG feasible, whereas open-vocabulary approaches typically remain offline.\nEven in the offline setting, construction time remains a critical bottleneck: in addition to the heavy time cost of vision-language reasoning, cross-image aggregation also incurs significant latency, further limiting the practicality of open-vocabulary 3DSGG.\n3\nMETHOD\nIn this section, we present our proposed framework RAG-3DSG for open-vocabulary 3D scene graph generation.\nThe overall pipeline is illustrated in Figure\n1\n.\nOur pipeline consists of three main stages:\n(1)\nCross-image Object Mapping\n(Section\n3.1\n),\nwhere we perform 2D segmentation, dynamic downsampling, and 3D object fusion to construct a global object list with object-level point clouds and semantic embeddings;\n(2)\nNode Caption Generation\n(Section\n3.2\n),\nwhere we first obtain initial captions, then perform re-shot guided uncertainty estimation, and finally refine high-uncertainty objects using object-level RAG;\n(3)\nEdge Caption Generation\n(Section\n3.3\n),\nwhere we use a structured LLM prompt to produce interpretable relationship captions.\n3.1\nCross-image Object Mapping\nGiven an RGB-D image sequence\nI\n=\n{\nI\n1\n,\nI\n2\n,\nâ€¦\n,\nI\nt\n}\nI=\\{I_{1},I_{2},\\dots,I_{t}\\}\n,\neach image is represented as\nI\ni\n=\nâŸ¨\nI\ni\nRGB\n,\nI\ni\nD\n,\nP\ni\nâŸ©\nI_{i}=\\langle I_{i}^{\\text{RGB}},I_{i}^{\\text{D}},P_{i}\\rangle\n,\nwhere\nI\ni\nRGB\nI_{i}^{\\text{RGB}}\n,\nI\ni\nD\nI_{i}^{D}\n, and\nP\ni\nP_{i}\ndenote the color image, depth image, and camera pose respectively.\nEach image\nI\ni\nI_{i}\nyields a set of detected objects\nO\ni\nlocal\n=\n{\no\ni\n,\n1\nlocal\n,\nâ€¦\n,\no\ni\n,\nM\nlocal\n}\nO_{i}^{\\text{local}}=\\{o^{\\text{local}}_{i,1},\\dots,o^{\\text{local}}_{i,M}\\}\nobtained from segmentation.\nOur objective is to construct a global object list\nO\nt\nglobal\n=\n{\no\nt\n,\n1\nglobal\n,\nâ€¦\n,\no\nt\n,\nN\nglobal\n}\nO_{t}^{\\text{global}}=\\{o^{\\text{global}}_{t,1},\\dots,o^{\\text{global}}_{t,N}\\}\nby incrementally fusing the per-image objects\nO\nt\nlocal\nO_{t}^{\\text{local}}\ninto the accumulated set\nO\nt\nâˆ’\n1\nglobal\nO_{t-1}^{\\text{global}}\n.\nFollowing\nGu etÂ al. (\n2024\n)\n, we adopt incremental object-level cross-image mapping and further introduce a dynamic downsample-mapping strategy to improve efficiency and effectiveness.\n3.1.1\n2D Segmentation\nFor\nt\nt\n-th input image\nI\nt\nRGB\nI_{t}^{\\text{RGB}}\n, we first extract local object-level information.\nA class-agnostic segmentation model\nSAM\nâ€‹\n(\nâ‹…\n)\n\\text{SAM}(\\cdot)\n(Kirillov etÂ al.,\n2023\n)\nproduces a set of 2D object masks\n{\nm\nt\n,\ni\n}\ni\n=\n1\nâ€‹\nâ€¦\nâ€‹\nM\n=\nSAM\nâ€‹\n(\nI\nt\nRGB\n)\n\\{m_{t,i}\\}_{i=1\\dots M}=\\text{SAM}(I_{t}^{\\text{RGB}})\n.\nNext, a visual semantic encoder\nCLIP\nâ€‹\n(\nâ‹…\n)\n\\text{CLIP}(\\cdot)\n(Radford etÂ al.,\n2021\n)\nis used to obtain semantic embeddings\n{\nf\nt\n,\ni\n}\ni\n=\n1\nâ€‹\nâ€¦\nâ€‹\nM\n=\nCLIP\nâ€‹\n(\nI\nt\nRGB\n,\n{\nm\nt\n,\ni\n}\ni\n=\n1\nâ€‹\nâ€¦\nâ€‹\nM\n)\n\\{f_{t,i}\\}_{i=1\\dots M}=\\text{CLIP}(I_{t}^{\\text{RGB}},\\{m_{t,i}\\}_{i=1\\dots M})\nfor each masked region.\nTo prepare for 3D information aggregation, the camera pose\nP\nt\nP_{t}\nis used to project each 2D mask into 3D space, producing a set of point clouds\n{\np\nt\n,\ni\n}\ni\n=\n1\nâ€‹\nâ€¦\nâ€‹\nM\n\\{p_{t,i}\\}_{i=1\\dots M}\n.\nWith these extracted information, we define the local object list of\nI\nt\nI_{t}\nas\nO\nt\nlocal\n=\n{\no\nt\n,\n1\nlocal\n,\nâ€¦\n,\no\nt\n,\nM\nlocal\n}\nO_{t}^{\\text{local}}=\\{o_{t,1}^{\\text{local}},\\dots,o_{t,M}^{\\text{local}}\\}\n, where\no\nt\n,\ni\nlocal\n=\nâŸ¨\nf\nt\n,\ni\nlocal\n,\np\nt\n,\ni\nlocal\nâŸ©\no_{t,i}^{\\text{local}}=\\langle f_{t,i}^{\\text{local}},p_{t,i}^{\\text{local}}\\rangle\n.\nThis local object list\nO\nt\nlocal\nO_{t}^{\\text{local}}\nserves as the basis for subsequent 3D object mapping and fusion.\n3.1.2\nDynamic Downsampling\nBefore performing object mapping for\no\nt\n,\ni\nlocal\no_{t,i}^{\\text{local}}\nwith point clouds\np\nt\n,\ni\np_{t,i}\nand semantic embeddings\nf\nt\n,\ni\nf_{t,i}\n, we downsample the point clouds to reduce computational cost.\nExisting approaches typically adopt a fixed voxel size\nÎ´\nsample\n\\delta^{\\text{sample}}\n, which is determined by the size of smaller objects within the scene.\nThis strategy has a clear drawback that large objects remain lack-sampled, resulting in unnecessarily dense point clouds.\nTo address this issue, we propose a dynamic downsampling strategy that adapts the voxel size according to the scale of each object.\nThis not only improves efficiency but also facilitates the subsequent re-shot guided uncertainty estimation (Section\n3.2.2\n) by ensuring that object pixels in re-shot images are dense enough to faithfully capture the underlying object.\nFormally, the voxel size\nÎ´\nt\n,\ni\nvoxel\n\\delta_{t,i}^{\\text{voxel}}\nfor the point cloud\np\nt\n,\ni\np_{t,i}\nof object\no\nt\n,\ni\nlocal\no_{t,i}^{\\text{local}}\nis defined as:\nÎ´\nt\n,\ni\nvoxel\n=\nÎ´\nsample\nâ‹…\nâ€–\nBbox\nâ€‹\n(\np\nt\n,\ni\n)\nâ€–\n2\n1\n/\n2\n,\n\\delta_{t,i}^{\\text{voxel}}=\\delta^{\\text{sample}}\\cdot\\|\\text{Bbox}(p_{t,i})\\|_{2}^{1/2},\n(1)\nwhere\nÎ´\nsample\n\\delta^{\\text{sample}}\ndenotes the fixed base voxel size, and\nâ€–\nBbox\nâ€‹\n(\np\nt\n,\ni\n)\nâ€–\n2\n\\|\\text{Bbox}(p_{t,i})\\|_{2}\ncorresponds to the Euclidean norm of the bounding box size vector, i.e., the diagonal length of the 3D bounding box, which reflects its overall spatial extent.\nFor simplicity, we continue to use\np\nt\n,\ni\np_{t,i}\nto denote the downsampled point cloud in the following sections.\nCompared with fixed voxel-size downsampling, our strategy yields denser point clouds\nfor smaller objects and sparser ones for larger objects within the scene,\nthereby simultaneously achieving both finer granularity and higher efficiency,\nwithout incurring the usual trade-off between the two.\nConcrete examples of dynamic voxel sizes for common indoor objects are provided in Appendix\nA.2\n.\n3.1.3\n3D Object Fusion\nIncremental object mapping and fusion begins after dynamic downsampling.\nWe use the local object list\nO\n1\nlocal\nO_{1}^{\\text{local}}\nin the first image to initialize the global object list as\nO\n1\nglobal\n=\nO\n1\nlocal\nO_{1}^{\\text{global}}=O_{1}^{\\text{local}}\n.\nLater, for each object\no\nt\n,\ni\nlocal\n=\nâŸ¨\nf\nt\n,\ni\nlocal\n,\np\nt\n,\ni\nlocal\nâŸ©\no_{t,i}^{\\text{local}}=\\langle f_{t,i}^{\\text{local}},p_{t,i}^{\\text{local}}\\rangle\nin the\nt\nt\n-th image input, we follow\nGu etÂ al. (\n2024\n)\nto construct a fusion similarity\nÎ¸\nâ€‹\n(\ni\n,\nj\n)\n\\theta(i,j)\nas follows,\nÎ¸\nâ€‹\n(\ni\n,\nj\n)\n=\nÎ¸\nsemantic\nâ€‹\n(\ni\n,\nj\n)\n+\nÎ¸\nspatial\nâ€‹\n(\ni\n,\nj\n)\n,\n\\theta(i,j)=\\theta_{\\text{semantic}}(i,j)+\\theta_{\\text{spatial}}(i,j),\n(2)\nÎ¸\nsemantic\nâ€‹\n(\ni\n,\nj\n)\n\\theta_{\\text{semantic}}(i,j)\nis the semantic similarity between\no\nt\n,\ni\nlocal\no_{t,i}^{\\text{local}}\nand\no\nt\nâˆ’\n1\n,\nj\nglobal\no_{t-1,j}^{\\text{global}}\nas follows,\nÎ¸\nsemantic\nâ€‹\n(\ni\n,\nj\n)\n=\n(\nf\nt\n,\ni\nlocal\n)\nT\nâ€‹\nf\nt\nâˆ’\n1\n,\nj\nglobal\n/\n2\n+\n1\n/\n2\n,\n\\theta_{\\text{semantic}}(i,j)=(f_{t,i}^{\\text{local}})^{T}f_{t-1,j}^{\\text{global}}/2+1/2,\n(3)\nand\nÎ¸\nspatial\nâ€‹\n(\ni\n,\nj\n)\n\\theta_{\\text{spatial}}(i,j)\nis the spatial similarity between\no\nt\n,\ni\nlocal\no_{t,i}^{\\text{local}}\nand\no\nt\nâˆ’\n1\n,\nj\nglobal\no_{t-1,j}^{\\text{global}}\nas follows,\nÎ¸\nspatial\nâ€‹\n(\ni\n,\nj\n)\n=\ndnnratio\nâ€‹\n(\np\nt\n,\ni\nlocal\n,\np\nt\nâˆ’\n1\n,\nj\nglobal\n)\n,\n\\theta_{\\text{spatial}}(i,j)=\\text{dnnratio}(p_{t,i}^{\\text{local}},p_{t-1,j}^{\\text{global}}),\n(4)\nwhere\ndnnratio\nâ€‹\n(\nâ‹…\n)\n\\text{dnnratio}(\\cdot)\nis the proposed dynamic nearest neighbor ratio, equal to the proportion of points in point cloud\np\nt\n,\ni\nlocal\np_{t,i}^{\\text{local}}\nthat have nearest neighbors in point cloud\np\nt\nâˆ’\n1\n,\nj\nglobal\np_{t-1,j}^{\\text{global}}\n, within a dynamic distance threshold\nÎ´\ni\n,\nj\nnnratio\n=\nÎ´\nsample\nâ€‹\n(\nâ€–\nBbox\nâ€‹\n(\np\nt\n,\ni\nlocal\n)\nâ€–\n2\n1\n/\n2\n+\nâ€–\nBbox\nâ€‹\n(\np\nt\nâˆ’\n1\n,\nj\nglobal\n)\nâ€–\n2\n1\n/\n2\n)\n/\n2\n\\delta_{i,j}^{\\text{nnratio}}=\\delta^{\\text{sample}}(\\|\\text{Bbox}(p_{t,i}^{\\text{local}})\\|_{2}^{1/2}+\\|\\text{Bbox}(p_{t-1,j}^{\\text{global}})\\|_{2}^{1/2})/2\n.\nBy calculating fusion similarity, each new local object is matched with a global object which has the highest similarity score.\nIf no match is found with a similarity higher than\nÎ´\nsim\n\\delta^{\\text{sim}}\n, the local object will be treated directly as a new global object.\nFor the two matching objects\no\nt\n,\ni\nlocal\no_{t,i}^{\\text{local}}\nand\no\nt\nâˆ’\n1\n,\nj\nglobal\no_{t-1,j}^{\\text{global}}\n, the fused object\no\nt\n,\nj\nglobal\n=\nâŸ¨\nf\nt\n,\nj\nglobal\n,\np\nt\n,\nj\nglobal\nâŸ©\no_{t,j}^{\\text{global}}=\\langle f_{t,j}^{\\text{global}},p_{t,j}^{\\text{global}}\\rangle\n.\nThe fused semantic embedding\nf\nt\n,\nj\nglobal\nf_{t,j}^{\\text{global}}\nis calculated as\nf\nt\n,\nj\nglobal\n=\n(\nn\nâ€‹\nf\nt\nâˆ’\n1\n,\nj\nglobal\n+\nf\nt\n,\ni\nlocal\n)\n/\n(\nn\n+\n1\n)\nf_{t,j}^{\\text{global}}=(nf_{t-1,j}^{\\text{global}}+f_{t,i}^{\\text{local}})/(n+1)\n,\nwhere\nn\nn\nrepresents the mapping times of\nf\nt\nâˆ’\n1\n,\nj\nglobal\nf_{t-1,j}^{\\text{global}}\n.\nThe fused point cloud is directly taken as the union as\np\nt\n,\nj\nglobal\n=\np\nt\nâˆ’\n1\n,\nj\nglobal\nâˆª\np\nt\n,\ni\nlocal\np_{t,j}^{\\text{global}}=p_{t-1,j}^{\\text{global}}\\cup p_{t,i}^{\\text{local}}\n.\nAfter\nt\nt\niterations, we construct global object list\nO\nt\nglobal\n=\n{\no\nt\n,\n1\nglobal\n,\nâ€¦\n,\no\nt\n,\nN\nglobal\n}\nO_{t}^{\\text{global}}=\\{o_{t,1}^{\\text{global}},\\dots,o_{t,N}^{\\text{global}}\\}\n, where\no\nt\n,\ni\nglobal\n=\nâŸ¨\nf\nt\n,\ni\nglobal\n,\np\nt\n,\ni\nglobal\nâŸ©\no_{t,i}^{\\text{global}}=\\langle f_{t,i}^{\\text{global}},p_{t,i}^{\\text{global}}\\rangle\n.\nThe detailed algorithm is provided in Appendix\nA.3\n.\n3.2\nNode Caption Generation\nGiven the global object list, our goal is to derive node captions from the information aggregated in Section\n3.1\n.\nObject masks are first fed into the Vision-Language Model (VLM) to obtain initial captions (Section\n3.2.1\n).\nWe then render multi-view reconstructed point clouds to produce best-view re-shot images and perform re-shot guided uncertainty estimation by comparing captions from re-shot and original images (Section\n3.2.2\n).\nLow-uncertainty objects directly form the object document, while high-uncertainty objects retrieve this document for caption refinement, yielding more accurate and robust 3D scene graphs (Section\n3.2.3\n).\n3.2.1\nInitial Caption Generation\nFor each object in the global object list, we maintain the top-\nk\nk\nviews with the highest segmentation confidence.\nObject-level crops from these top-\nk\nk\nviews are fed into a VLM\n(Hurst etÂ al.,\n2024\n)\nto obtain initial crop captions using the prompt â€œbriefly describe the central object in the image in a few words.â€\nThe initial crop captions for object\no\nt\n,\ni\nglobal\no_{t,i}^{\\text{global}}\nare denoted as\nc\nt\n,\ni\n=\n{\nc\n1\n,\nâ€¦\n,\nc\nk\n}\nc_{t,i}=\\{c_{1},\\dots,c_{k}\\}\n,\nwhich may be incorrect due to constrained viewpoints or occlusion as illustrated in the upper part of Figure\n1\n.\n3.2.2\nRe-shot Guided Uncertainty Estimation\nThe initial crop captions for object\no\nt\n,\ni\nglobal\no_{t,i}^{\\text{global}}\nmay be unreliable due to constrained viewpoints and severe object occlusion.\nRelying solely on these captions could propagate noise into the subsequent aggregation of the object-level information.\nTo mitigate this issue, we introduce a re-shot strategy that generates best-view re-shot images from object point clouds.\nUnlike multi-view crops from the original scene images, the reconstructed object-level point cloud\np\np\ncontains only the object of interest, free from occlusion and background clutter.\nSince the point cloud can be observed from arbitrary viewpoints, we can render a 2D image from a perspective that maximally represents the objectâ€™s geometry and appearance.\nThis ensures that the resulting re-shot captions capture the most informative object features.\nConceptually, this approach is analogous to the viewpoint entropy\n(VÃ¡zquez etÂ al.,\n2001\n)\nor best-view selection problem in computer vision, where the goal is to choose a view that maximizes information content.\nGiven an object point cloud\np\np\nwith a maintained average camera position\nv\navg\n{v}^{\\text{avg}}\nfrom the images used to construct the point cloud, our goal is to render an optimal 2D view that best represents the object.\nTo this end, we uniformly sample multiple candidate camera positions on a hemisphere centered at the object center\no\n{o}\n, and render the corresponding 2D re-shot images.\nTo select the most informative view, we define a view quality score for each candidate position\nc\ni\nc_{i}\nwith three complementary terms:\nS\nvis\n=\n|\np\nvisible\n|\n|\np\n|\n,\nS\nup\n=\n1\nâˆ’\n|\nv\ni\nâ‹…\ng\n|\n,\nS\nprior\n=\n1\n2\nâ€‹\n(\n1\n+\nv\ni\nâ‹…\nf\n)\n,\nS_{\\text{vis}}=\\frac{|p^{\\text{visible}}|}{|p|},\\quad S_{\\text{up}}=1-|{v}_{i}\\cdot{g}|,\\quad S_{\\text{prior}}=\\tfrac{1}{2}(1+{v}_{i}\\cdot{f}),\n(5)\nwhere\nS\nvis\nS_{\\text{vis}}\nmeasures the visible ratio of points under hidden-point removal,\nS\nup\nS_{\\text{up}}\nevaluates the alignment of the view direction\nv\ni\n=\no\nâˆ’\nc\ni\n{v}_{i}={o}-{c}_{i}\nwith the gravity vector\ng\n{g}\n,\nand\nS\nprior\nS_{\\text{prior}}\nenforces consistency with the prior direction\nf\n=\nv\navg\nâˆ’\no\n{f}={v}^{\\text{avg}}-{o}\n.\nThe overall score is then computed as\nS\nview\n=\n(\n1\nâˆ’\nÎ±\nâˆ’\nÎ²\n)\nâ€‹\nS\nvis\n+\nÎ±\nâ€‹\nS\nup\n+\nÎ²\nâ€‹\nS\nprior\nS_{\\text{view}}=(1-\\alpha-\\beta)S_{\\text{vis}}+\\alpha S_{\\text{up}}+\\beta S_{\\text{prior}}\n,\nwith\nÎ±\n\\alpha\nand\nÎ²\n\\beta\ncontrolling the trade-off between uprightness and prior alignment.\nBased on the view quality score, we select the candidate view with the highest\nS\nview\nS_{\\text{view}}\nand render the corresponding 2D re-shot image\nI\nreshot\nI^{\\text{reshot}}\n(see Appendix\nA.4\nfor examples).\nThe re-shot caption\nc\nreshot\nc^{\\text{reshot}}\nis then obtained from the VLM using the same prompt as in Section\n3.2.1\n.\nTo quantify uncertainty, we compute the cosine similarity between the CLIP embeddings of the re-shot caption and the initial crop captions:\n{\ns\n1\n,\nâ€¦\n,\ns\nk\n}\n=\n{\ncos\nâ¡\n(\nCLIP\nâ€‹\n(\nc\nreshot\n)\n,\nCLIP\nâ€‹\n(\nc\ni\n)\n)\n}\ni\n=\n1\nk\n.\n\\{s_{1},\\dots,s_{k}\\}=\\left\\{\\cos\\big(\\text{CLIP}(c^{\\text{reshot}}),\\text{CLIP}(c_{i})\\big)\\right\\}_{i=1}^{k}.\n(6)\nWe then perform clustering on the similarity scores and select the top-1 cluster\n{\nc\n1\n,\nâ€¦\n,\nc\nl\n}\n,\n{\ns\n1\n,\nâ€¦\n,\ns\nl\n}\n,\nl\nâ‰¤\nk\n\\{c_{1},\\dots,c_{l}\\},\\quad\\{s_{1},\\dots,s_{l}\\},\\quad l\\leq k\n,\nwhich is considered the most reliable subset of initial captions for further refinement.\nThe captions in this subset are aggregated into a single caption\nc\n^\n\\hat{c}\nusing a Large Language Model (LLM)\n(Achiam etÂ al.,\n2023\n)\nwith a designed prompt.\nThe corresponding similarity scores are averaged to obtain\ns\n^\n=\n1\nl\nâ€‹\nâˆ‘\ni\n=\n1\nl\ns\ni\n,\n\\hat{s}=\\frac{1}{l}\\sum_{i=1}^{l}s_{i},\nwhere a higher\ns\n^\n\\hat{s}\nindicates stronger agreement among the captions, and thus lower uncertainty in\nc\n^\n\\hat{c}\n.\n3.2.3\nObject-level RAG\nBased on the re-shot guided uncertainty estimation introduced in the previous section, we first rank all objects by their uncertainty scores\n1\nâˆ’\ns\n^\n1-\\hat{s}\n.\nWe additionally apply a prompt to the VLM\n(Hurst etÂ al.,\n2024\n)\nto filter out background objects via crops.\nThe top-\n50\n%\n50\\%\nlow-uncertainty objects are directly included in the object document for RAG, where each final caption\nc\nc\nis set to\nc\n^\n\\hat{c}\n.\nFor the remaining high-uncertainty objects, we perform refinement with the aid of contextual information.\nSpecifically, a 3D position-based retriever retrieves the nearest object in the document, whose caption\nc\nenv\nc^{\\text{env}}\nserves as augmented auxiliary context.\nIn addition, we construct a composite image by concatenating the re-shot image (providing global context) with the crop image that yields the highest similarity score.\nThis composite image, together with a text prompt containing\nc\nenv\nc^{\\text{env}}\n, is fed into a VLM\n(Hurst etÂ al.,\n2024\n)\nto generate the refined caption\nc\nc\n.\nThe prompt is designed as:\nâ€œThe picture is stitched from the point cloud image and the RGB image of the same indoor object. There is a\nc\nenv\nc^{\\text{env}}\nnear the object. Briefly describe the object in the picture.â€\nThrough the refinement process, we obtain a precise object list\nO\n=\n{\no\n1\n,\nâ€¦\n,\no\nN\n}\nO=\\{o_{1},\\dots,o_{N}\\}\n,\nwhere each object\no\ni\no_{i}\nis represented as\no\ni\n=\nâŸ¨\nf\ni\n,\np\ni\n,\nc\ni\nâŸ©\no_{i}=\\langle f_{i},p_{i},c_{i}\\rangle\n,\nconsisting of its semantic embedding\nf\ni\nf_{i}\n, point cloud\np\ni\np_{i}\n, and precise node caption\nc\ni\nc_{i}\n.\n3.3\nEdge Caption Generation\nFollowing\nGu etÂ al. (\n2024\n)\n, we estimate the spatial relationships among 3D objects to complete the scene graph.\nGiven the set of 3D objects\nO\n=\n{\no\n1\n,\nâ€¦\n,\no\nN\n}\nO=\\{o_{1},\\dots,o_{N}\\}\n, we first compute their potential connectivity.\nUnlike\nGu etÂ al. (\n2024\n)\n, which adopts a fixed NN-ratio threshold to build a dense graph and then prunes it with a minimum spanning tree (MST), we introduce a dynamic threshold similar to Equation\n4\n, consistent with our dynamic downsample-mapping strategy, thus adapting the edge construction to varying point cloud densities.\nFor relationship captioning,\nGu etÂ al. (\n2024\n)\nemploy a structured prompt that restricts the output to five predefined relation types and uses object captions and 3D locations as inputs.\nIn contrast, we extend this design by (i) expanding the relation space to eight categories (\nâ€œa on b,â€ â€œb on a,â€ â€œa in b,â€ â€œb in a,â€ â€œa part of b,â€ â€œb part of a,â€ â€œnear,â€ â€œnone of theseâ€\n), and (ii) providing few-shot in-context examples to guide the model.\nThis design ensures that the generated relationships are more expressive.\n3.4\nScene Graph Representation\nWith the refined object list and edge captions, we formally define the final 3D scene graph as\nğ’¢\n=\n(\nO\n,\nE\n)\n\\mathcal{G}=(O,E)\n,\nwhere\nO\n=\n{\no\n1\n,\nâ€¦\n,\no\nN\n}\nO=\\{o_{1},\\dots,o_{N}\\}\ndenotes the set of objects and\nE\n=\n{\ne\ni\nâ€‹\nj\n}\nE=\\{e_{ij}\\}\nthe set of edges.\nEach object\no\ni\no_{i}\nis represented as\no\ni\n=\nâŸ¨\nf\ni\n,\np\ni\n,\nc\ni\nâŸ©\no_{i}=\\langle f_{i},p_{i},c_{i}\\rangle\n,\nconsisting of its semantic embedding\nf\ni\nf_{i}\n, point cloud\np\ni\np_{i}\n, and precise caption\nc\ni\nc_{i}\n.\nEach edge\ne\ni\nâ€‹\nj\ne_{ij}\nis represented as\ne\ni\nâ€‹\nj\n=\nâŸ¨\no\ni\n,\no\nj\n,\nr\ni\nâ€‹\nj\nâŸ©\ne_{ij}=\\langle o_{i},o_{j},r_{ij}\\rangle\n,\nwhere\nr\ni\nâ€‹\nj\nr_{ij}\nis the discrete relation label selected from the predefined set of eight categories.\nThis formulation yields a complete and interpretable 3D scene graph with both node- and edge-level semantic annotations (Figure\n2\n).\nFigure 2:\nVisualization of our 3DSGs for Replica\n(Straub etÂ al.,\n2019\n)\nRoom 0 (left) and 1 (right).\nThe blue points represent the objects and the red lines indicate the relationships between them.\n4\nEXPERIMENTS\n4.1\nImplementation Details\nFor object segmentation, we use\nSAM\nâ€‹\n(\nâ‹…\n)\n\\text{SAM}(\\cdot)\nwith the pretrained checkpoint\nsam_vit_h_4b8939\n.\nFor encoding semantic embeddings, we use\nCLIP\nâ€‹\n(\nâ‹…\n)\n\\text{CLIP}(\\cdot)\nwith the\nViT-H-14\nbackbone pretrained on\nlaion2b_s32b_b79k\n.\nFor all purely text-based tasks, we employ\ngpt-4-0613\nas the LLM, while for vision-language tasks involving both images and text, we adopt\ngpt-4o-mini\nas the VLM.\nRegarding hyperparameters, we set the similarity threshold for object mapping to\nÎ´\nsim\n=\n0.45\n\\delta^{\\text{sim}}=0.45\n, and the base voxel size for dynamic downsampling to\nÎ´\nsample\n=\n0.01\n\\delta^{\\text{sample}}=0.01\nmeters.\nFor the scoring function in re-shot guided uncertainty estimation, we set\nÎ±\n=\nÎ²\n=\n0.2\n\\alpha=\\beta=0.2\n.\nOther non-critical hyperparameters and prompts will be released with our code in the accompanying GitHub repository.\n4.2\nScene Graph Construction Evaluation\nWe first evaluate the accuracy of the generated 3D scene graphs on Replica\n(Straub etÂ al.,\n2019\n)\ndataset in Table\n4.2\n.\nWe compare our method against two baselines: ConceptGraphs (CG) and its variant ConceptGraphs-Detector (CG-D)\n(Gu etÂ al.,\n2024\n)\n.\nThe open-vocabulary nature of our method makes automatic evaluation challenging.\nTherefore, following the protocol of ConceptGraphs\n(Gu etÂ al.,\n2024\n)\n, we resort to human evaluation.\nWe recruited knowledgeable university students as annotators and randomly shuffled the basic evaluation units before distribution.\nFor node evaluation, each unit consists of a point cloud, its mask images, and the predicted node caption, and annotators are asked to judge whether the caption is accurate.\nFor edge evaluation, each unit includes two such node units along with the predicted edge caption and the whole scene point clouds, and annotators are similarly asked to assess the correctness of the relationship description.\nFrom Table\n4.2\n, our method consistently outperforms both ConceptGraphs (CG) and ConceptGraphs-Detector (CG-D) across most evaluation metrics.\nIn terms of node precision, our method achieves an average score of\n0.82\n0.82\n, which is notably higher than CG (\n0.68\n0.68\n) and CG-D (\n0.58\n0.58\n), demonstrating the effectiveness of our re-shot guided uncertainty estimation in reducing noise during object caption aggregation.\nFor edge precision, our method also attains the highest average score (\n0.91\n0.91\n), surpassing CG (\n0.82\n0.82\n) and CG-D (\n0.85\n0.85\n), indicating that our structured prompt and refined relationship categories lead to more accurate and interpretable edge captions.\nIn addition, our method substantially reduces duplicate predictions while maintaining a higher number of valid objects and edges, further confirming its robustness.\nOverall, these results validate that our approach not only improves caption accuracy at both the node and edge levels but also enhances the reliability of the entire 3D scene graph construction pipeline.\nTable 1:\nPerformance comparison of 3D scene graph generation methods on Replica\n(Straub etÂ al.,\n2019\n)\ndataset.\nNode precision, edge precision, duplicate objects and object/edge counts are evaluated through human annotation across multiple indoor scenes (room0-office4).\nscene\nnode prec.\nvalid objects\nduplicates\nedge prec.\nvalid edges\nOurs\nCG\nCG-D\nOurs\nCG\nCG-D\nOurs\nCG\nCG-D\nOurs\nCG\nCG-D\nOurs\nCG\nCG-D\nroom0\n0.87\n0.77\n0.53\n61\n57\n60\n1\n4\n3\n0.93\n0.87\n0.88\n27\n15\n16\nroom1\n0.88\n0.73\n0.71\n51\n45\n42\n0\n5\n3\n0.97\n0.92\n0.91\n30\n12\n11\nroom2\n0.85\n0.63\n0.50\n47\n48\n50\n0\n3\n2\n0.94\n0.91\n0.92\n35\n11\n12\noffice0\n0.73\n0.61\n0.61\n48\n44\n41\n1\n1\n1\n0.93\n0.78\n0.82\n27\n9\n11\noffice1\n0.73\n0.64\n0.46\n44\n25\n24\n0\n1\n3\n0.93\n0.80\n0.86\n28\n5\n7\noffice2\n0.87\n0.77\n0.68\n67\n48\n44\n1\n3\n2\n0.88\n0.79\n0.86\n34\n14\n14\noffice3\n0.85\n0.69\n0.60\n65\n59\n57\n2\n4\n2\n0.84\n0.78\n0.77\n32\n9\n13\noffice4\n0.79\n0.61\n0.57\n53\n41\n46\n1\n5\n4\n0.86\n0.67\n0.80\n22\n3\n5\n\\rowcolor\ngray!20â€‚â€ŠAverage\n0.82\n0.68\n0.58\n-\n-\n-\n-\n-\n-\n0.91\n0.82\n0.85\n-\n-\n-\n4.3\nSemantic Segmentation Evaluation\nWe evaluate our dynamic downsample-mapping strategy on the closed-set Replica dataset\n(Straub etÂ al.,\n2019\n)\nfollowing the ground-truth construction and evaluation protocol of\nGu etÂ al. (\n2024\n); Jatavallabhula etÂ al. (\n2023\n)\n.\nConcretely, the ground-truth (GT) point clouds with per-point semantic labels are obtained as in ConceptGraph\nGu etÂ al. (\n2024\n)\n: SemanticNeRF\n(Zhi etÂ al.,\n2021\n)\nprovides rendered RGB-D frames and 2D semantic masks, the masks are converted to one-hot per-pixel embeddings and fused into 3D via GradSLAM\n(Krishna Murthy etÂ al.,\n2020\n)\n, yielding the reference GT point cloud with per-point semantic annotations.\nAfter performing the cross-image object mapping described in Section\n3.1\n, our method produces object-level point clouds with fused semantic embeddings.\nTo align predictions with GT categories, we map each GT object label to the predicted object whose fused semantic embedding has the highest cosine similarity with the CLIP text embedding of that GT label.\nFollowing the same protocol, for each point in the GT point cloud we compute its 1-NN in the predicted point cloud, compare the GT class with the predicted class of that 1-NN to build the confusion matrix, and compute the class-mean recall (mAcc) and the frequency-weighted mean intersection-overunion (f-mIOU).\nWe report our result combined with the results reported in\nMaggio etÂ al. (\n2024\n); Gu etÂ al. (\n2024\n)\nin Table\n2\n.\nOur method shares the same overall matching pipeline with ConceptGraph, but significantly reduces the computational cost.\nUnder the same base voxel size, our dynamic downsample-mapping strategy shortens the processing time by nearly two-thirds (e.g., from 6.65s/iter to 2.49s/iter when the voxel size is set to 0.01 in Replica\n(Straub etÂ al.,\n2019\n)\nRoom 0).\nMoreover, as shown in Table\n2\n, our method achieves comparable or even superior accuracy, reaching the best mAcc score (40.67) while maintaining a competitive f-mIoU (35.65).\nThis demonstrates that our approach not only accelerates the object mapping process but also preserves segmentation quality.\nTable 2:\nSemantic segmentation experiments on Replica\n(Straub etÂ al.,\n2019\n)\ndataset for object mapping and time evaluation. Baseline results are reported from\nMaggio etÂ al. (\n2024\n); Gu etÂ al. (\n2024\n)\n.\nmAcc denotes class-mean recall and f-mIOU denotes frequency-weighted mean intersection-over-union reported from\n(Jatavallabhula etÂ al.,\n2023\n)\n.\nMethod\nmAcc\nF-mIOU\nMaskCLIP\n4.53\n0.94\nMask2former + Global CLIP feat\n10.42\n13.11\nConceptFusion\n24.16\n31.31\nConceptFusion + SAM\n31.53\n38.70\nConceptGraphs\n40.63\n35.95\nConceptGraphs-Detector\n38.72\n35.82\nOpenMask3D\n39.54\n49.26\nClio-batch\n37.95\n36.98\nOurs\n40.67\n35.65\nFigure 3:\nAblation study quantifying the contribution of each pipeline component. Performance degradation is observed when removing re-shot guided uncertainty estimation (w/o Reshot), RAG (w/o RAG), concatenated re-shot images (w/o Concat), or using random retrieval (Random RAG), confirming the complementary roles of all proposed components.\n4.4\nAblation Study\nWe conduct ablation studies to quantify the contribution of each component in our pipeline.\nOur full model (\nOurs\n) consists of dynamic downsample-mapping & fusion, re-shot guided uncertainty estimation, and node-level RAG with concatenated re-shot image prompts.\nWe compare against several variants: (i) removing re-shot guided uncertainty estimation (\nw/o Reshot\n), (ii) removing RAG (\nw/o RAG\n), (iii) applying random retrieval for RAG (\nrandom-RAG\n), and (iv) removing the concatenated re-shot image prompts (\nw/o Concat\n).\nExperiments are conducted on Replica\n(Straub etÂ al.,\n2019\n)\ndataset, following a protocol similar to Section\n4.3\n.\nThe ground-truth (GT) point clouds with per-point semantic labels are obtained as in ConceptGraphs\n(Gu etÂ al.,\n2024\n)\n.\nDifferent from Section\n4.3\n, where fused semantic embeddings are directly compared with GT embeddings via cosine similarity,\nhere we leverage the final node captions of predicted objects as semantic representations and employ GPT-4o as a semantic assigner.\nConcretely, for each GT object label, GPT-4o is prompted to determine the most likely corresponding predicted node based on the node captions,\nthereby enabling a more faithful evaluation under the open-vocabulary setting.\nAfter establishing this assignment, we compute 1-NN matching between GT and predicted point clouds, construct the confusion matrix, and report quantitative results in Figure\n3\n.\nDetailed heatmaps for each individual scene are provided in Appendix\nA.5\n.\nAs shown in Figure\n3\n, removing any component leads to a noticeable performance drop, confirming their complementary roles.\nIn particular, removing re-shot guided uncertainty estimation (\nw/o Reshot\n) causes a drastic degradation in mF1 (14.66 vs. 30.78) and f-mIoU (35.56 vs. 54.26), underscoring its importance in filtering unreliable captions.\nEliminating RAG (\nw/o RAG\n) reduces both precision and overall accuracy, while replacing it with random retrieval (\nrandom-RAG\n) further deteriorates performance, highlighting the necessity of semantic-aware retrieval.\nRemoving the concatenated re-shot image (\nw/o Concat\n) also leads to lower recall and f-mIoU, suggesting that multi-view prompts alleviate viewpoint bias and enrich object descriptions.\nThese results collectively demonstrate that all three proposed components contribute significantly to the robustness and accuracy of our framework.\n5\nCONCLUSION\nIn this work, we propose a 3DSG generation method named RAG-3DSG for more accurate and robust 3DSGs.\nWe are the first to specifically address noise in cross-image information aggregation and incorporate an object-level RAG into 3DSGs for caption refinement.\nTo evaluate our approach, we conduct experiments on Replica\n(Straub etÂ al.,\n2019\n)\ndataset, which shows that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation.\nEthics Statement\nThis work focuses on developing methods for open-vocabulary 3D scene graph generation using public dataset (Replica\n(Straub etÂ al.,\n2019\n)\n).\nIt does not involve human subjects, personal data, or sensitive information.\nOur work is intended solely for advancing robotics and embodied AI research in safe and beneficial contexts, such as robot navigation and manipulation.\nWe encourage responsible use aligned with ethical research practices.\nReproducibility Statement\nWe have made extensive efforts to ensure reproducibility.\nAll implementation details, including pipline architectures, foundation models, and hyperparameters, are provided in the main text and appendix.\nThe datasets we used (Replica\n(Straub etÂ al.,\n2019\n)\n) are publicly available.\nOur code, along with evaluation scripts, will be released on GitHub to facilitate replication.\nWe also describe the evaluation procedure in detail, so that results can be reproduced independently.\nReferences\nAchiam etÂ al. (2023)\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, FlorenciaÂ Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, etÂ al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774\n, 2023.\nAgia etÂ al. (2022)\nChristopher Agia, KrishnaÂ Murthy Jatavallabhula, Mohamed Khodeir, Ondrej Miksik, Vibhav Vineet, Mustafa Mukadam, Liam Paull, and Florian Shkurti.\nTaskography: Evaluating robot task planning over large 3d scene graphs.\nIn\nConference on Robot Learning\n, pp.  46â€“58. PMLR, 2022.\nArmeni etÂ al. (2019)\nIro Armeni, Zhi-Yang He, JunYoung Gwak, AmirÂ R Zamir, Martin Fischer, Jitendra Malik, and Silvio Savarese.\n3d scene graph: A structure for unified semantics, 3d space, and camera.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n, pp.  5664â€“5673, 2019.\nGadre etÂ al. (2022)\nSamirÂ Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco, Ludwig Schmidt, and Shuran Song.\nClip on wheels: Zero-shot object navigation as object localization and exploration.\narXiv preprint arXiv:2203.10421\n, 3(4):7, 2022.\nGay etÂ al. (2018)\nPaul Gay, James Stuart, and Alessio DelÂ Bue.\nVisual graphs from motion (vgfm): Scene understanding with object geometry reasoning.\nIn\nAsian Conference on Computer Vision\n, pp.  330â€“346. Springer, 2018.\nGu etÂ al. (2024)\nQiao Gu, Ali Kuwajerwala, Sacha Morin, KrishnaÂ Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, etÂ al.\nConceptgraphs: Open-vocabulary 3d scene graphs for perception and planning.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pp.  5021â€“5028. IEEE, 2024.\nHou etÂ al. (2025)\nHao-Yu Hou, Chun-Yi Lee, Motoharu Sonogashira, and Yasutomo Kawanishi.\nFross: Faster-than-real-time online 3d semantic scene graph generation from rgb-d images.\nArXiv\n, abs/2507.19993, 2025.\nURL\nhttps://api.semanticscholar.org/CorpusID:280322834\n.\nHughes etÂ al. (2022)\nNathan Hughes, Yun Chang, and Luca Carlone.\nHydra: A real-time spatial perception system for 3d scene graph construction and optimization.\narXiv preprint arXiv:2201.13360\n, 2022.\nHurst etÂ al. (2024)\nAaron Hurst, Adam Lerer, AdamÂ P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJÂ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, etÂ al.\nGpt-4o system card.\narXiv preprint arXiv:2410.21276\n, 2024.\nJatavallabhula etÂ al. (2023)\nKrishnaÂ Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Alaa Maalouf, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, etÂ al.\nConceptfusion: Open-set multimodal 3d mapping.\narXiv preprint arXiv:2302.07241\n, 2023.\nJohnson etÂ al. (2015)\nJustin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and LiÂ Fei-Fei.\nImage retrieval using scene graphs.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n, pp.  3668â€“3678, 2015.\nKim etÂ al. (2019)\nUe-Hwan Kim, Jin-Man Park, Taek-Jin Song, and Jong-Hwan Kim.\n3-d scene graph: A sparse and semantic representation of physical environments for intelligent agents.\nIEEE transactions on cybernetics\n, 50(12):4921â€“4933, 2019.\nKirillov etÂ al. (2023)\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, AlexanderÂ C Berg, Wan-Yen Lo, etÂ al.\nSegment anything.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n, pp.  4015â€“4026, 2023.\nKoch etÂ al. (2024)\nSebastian Koch, Narunas Vaskevicius, Mirco Colosi, Pedro Hermosilla, and Timo Ropinski.\nOpen3dsg: Open-vocabulary 3d scene graphs from point clouds with queryable objects and open-set relationships.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pp.  14183â€“14193, 2024.\nKrishna etÂ al. (2017)\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, DavidÂ A Shamma, etÂ al.\nVisual genome: Connecting language and vision using crowdsourced dense image annotations.\nInternational journal of computer vision\n, 123(1):32â€“73, 2017.\nKrishna Murthy etÂ al. (2020)\nJatavallabhula Krishna Murthy, Soroush Saryazdi, Ganesh Iyer, and Liam Paull.\ngradslam: Dense slam meets automatic differentiation.\nIn\narXiv\n, 2020.\nLi etÂ al. (2024)\nHongsheng Li, Guangming Zhu, Liang Zhang, Youliang Jiang, Yixuan Dang, Haoran Hou, Peiyi Shen, Xia Zhao, Syed AfaqÂ Ali Shah, and Mohammed Bennamoun.\nScene graph generation: A comprehensive survey.\nNeurocomputing\n, 566:127052, 2024.\nLiu etÂ al. (2023)\nHaotian Liu, Chunyuan Li, Qingyang Wu, and YongÂ Jae Lee.\nVisual instruction tuning.\nAdvances in neural information processing systems\n, 36:34892â€“34916, 2023.\nLiu etÂ al. (2021)\nHengyue Liu, Ning Yan, Masood Mortazavi, and Bir Bhanu.\nFully convolutional scene graph generation.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pp.  11546â€“11556, 2021.\nLu etÂ al. (2016)\nCewu Lu, Ranjay Krishna, Michael Bernstein, and LiÂ Fei-Fei.\nVisual relationship detection with language priors.\nIn\nEuropean conference on computer vision\n, pp.  852â€“869. Springer, 2016.\nMaggio etÂ al. (2024)\nDominic Maggio, Yun Chang, Nathan Hughes, Matthew Trang, Dan Griffith, Carlyn Dougherty, Eric Cristofalo, Lukas Schmid, and Luca Carlone.\nClio: Real-time task-driven open-set 3d scene graphs.\nIEEE Robotics and Automation Letters\n, 2024.\nRadford etÂ al. (2021)\nAlec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, etÂ al.\nLearning transferable visual models from natural language supervision.\nIn\nInternational conference on machine learning\n, pp.  8748â€“8763. PmLR, 2021.\nRana etÂ al. (2023)\nKrishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf.\nSayplan: Grounding large language models using 3d scene graphs for scalable robot task planning.\narXiv preprint arXiv:2307.06135\n, 2023.\nRashid etÂ al. (2023)\nAdam Rashid, Satvik Sharma, ChungÂ Min Kim, Justin Kerr, LawrenceÂ Yunliang Chen, Angjoo Kanazawa, and Ken Goldberg.\nLanguage embedded radiance fields for zero-shot task-oriented grasping.\nIn\n7th Annual Conference on Robot Learning\n, 2023.\nRosinol etÂ al. (2021)\nAntoni Rosinol, Andrew Violette, Marcus Abate, Nathan Hughes, Yun Chang, Jingnan Shi, Arjun Gupta, and Luca Carlone.\nKimera: From slam to spatial perception with 3d dynamic scene graphs.\nThe International Journal of Robotics Research\n, 40(12-14):1510â€“1546, 2021.\nShah etÂ al. (2023)\nDhruv Shah, BÅ‚aÅ¼ej OsiÅ„ski, Sergey Levine, etÂ al.\nLm-nav: Robotic navigation with large pre-trained models of language, vision, and action.\nIn\nConference on robot learning\n, pp.  492â€“504. PMLR, 2023.\nShridhar etÂ al. (2022)\nMohit Shridhar, Lucas Manuelli, and Dieter Fox.\nCliport: What and where pathways for robotic manipulation.\nIn\nConference on robot learning\n, pp.  894â€“906. PMLR, 2022.\nStraub etÂ al. (2019)\nJulian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, JakobÂ J Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, etÂ al.\nThe replica dataset: A digital replica of indoor spaces.\narXiv preprint arXiv:1906.05797\n, 2019.\nSun etÂ al. (2023)\nShuzhou Sun, Shuaifeng Zhi, Janne HeikkilÃ¤, and LiÂ Liu.\nEvidential uncertainty and diversity guided active learning for scene graph generation.\nIn\nThe Eleventh International Conference on Learning Representations\n, 2023.\nURL\nhttps://openreview.net/forum?id=xI1ZTtVOtlz\n.\nVÃ¡zquez etÂ al. (2001)\nPere-Pau VÃ¡zquez, Miquel Feixas, Mateu Sbert, and Wolfgang Heidrich.\nViewpoint selection using viewpoint entropy.\nIn\nVMV\n, volumeÂ 1, pp.  273â€“280, 2001.\nWald etÂ al. (2020)\nJohanna Wald, Helisa Dhamo, Nassir Navab, and Federico Tombari.\nLearning 3d semantic scene graphs from 3d indoor reconstructions.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pp.  3961â€“3970, 2020.\nWerby etÂ al. (2024)\nAbdelrhman Werby, Chenguang Huang, Martin BÃ¼chner, Abhinav Valada, and Wolfram Burgard.\nHierarchical open-vocabulary 3d scene graphs for language-grounded robot navigation.\nIn\nFirst Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024\n, 2024.\nWu etÂ al. (2021)\nShun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, and Federico Tombari.\nScenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pp.  7515â€“7525, 2021.\nWu etÂ al. (2023)\nShun-Cheng Wu, Keisuke Tateno, Nassir Navab, and Federico Tombari.\nIncremental 3d semantic scene graph prediction from rgb sequences.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pp.  5064â€“5074, 2023.\nYin etÂ al. (2018)\nGuojun Yin, LuÂ Sheng, Bin Liu, Nenghai Yu, Xiaogang Wang, Jing Shao, and ChenÂ Change Loy.\nZoom-net: Mining deep feature interactions for visual relationship recognition.\nIn\nProceedings of the European conference on computer vision (ECCV)\n, pp.  322â€“338, 2018.\nZhi etÂ al. (2021)\nShuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and AndrewÂ J Davison.\nIn-place scene labelling and understanding with implicit scene representation.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, pp.  15838â€“15847, 2021.\nAppendix A\nAppendix\nA.1\nLarge Language Model Usage\nLarge Language Models are utilized in this work to aid and polish writing, specifically to improve clarity, grammatical accuracy, and overall readability of the manuscript.\nAll technical content, scientific contributions, and research findings remain entirely the work of the authors, who take full responsibility for the entire content of this paper.\nA.2\nDynamic Downsample Examples\nWe provide detailed examples of our dynamic downsampling strategy for indoor 3D pointclouds in Table\n3\n.\nThe voxel size for each object is computed based on its spatial extent using Equation\n1\nin Section\n3.1.2\n.\nA.3\nPseudo Code for Object Mapping\nFor clarity and reproducibility, we provide the pseudo code of our incremental 3D object mapping algorithm in\nAlgorithm\n1\n.\nThis algorithm describes the step-by-step procedure of updating global object list from local object lists of incoming frames,\nincluding point cloud integration and semantic refinement.\nIt serves as a concise summary of the mapping pipeline presented in the main paper.\nA.4\nRe-shot Images\nFigure\n4\nshows some examples of re-shot images automatically selected by our method.\nAs discussed in Section\n3.2.2\n, the initial image crops of an object may suffer from occlusions or constrained viewpoints,\nleading to unreliable captions and noisy semantics.\nTo address this issue, we propose a re-shot strategy that leverages object-level point clouds to render new views from arbitrary perspectives,\nensuring that the essential geometry and appearance of the object are faithfully captured.\nThis approach effectively eliminates occlusion and viewpoint constraints inherent in the original images.\nA.5\nAll Heatmaps of Ablation Study on Replica\nAs shown in Figure\n5\n, we present the heatmaps of semantic segmentation metrics across different scenes and ablation settings.\nA.6\nCase Study\nFigure\n6\ndemonstrates a concrete example of our re-shot guided uncertainty estimation when LLaVA\n(Liu etÂ al.,\n2023\n)\nis the VLM.\nMultiple crop images incorrectly identify the object asâ€œvaseâ€ due to occlusions and constrained viewing angles.\nIn contrast, the re-shot image (top-left) provides a correct caption from an optimal viewpoint selected from our algorithm, avoiding the viewpoint limitations and occlusions that plague the original crop images.\nThe low similarity scores flag these crop captions as unreliable, triggering our object-level RAG caption refinement process.\nFigure 4:\nExamples of re-shot images automatically selected by our method.\nFor each object category, we present the top-4 rendered views ranked by our re-shot scoring strategy.\nFrom top to bottom: armchair, vase, cushion, sofa, artwork, and TV stand.\nFigure 5:\nHeatmaps of semantic segmentation performance across scenes for different ablation settings.\nMetrics include mIoU, mRecall, mPrecision, mF1score, and frequency-weighted mIoU (fMiou).\nThe full pipeline serves as the baseline, while â€œw/o concatâ€, â€œw/o ragâ€, â€œrandom ragâ€, and â€œw/o reshotâ€ show the impact of removing or modifying individual components.\nFigure 6:\nCase study of re-shot guided uncertainty estimation using LLaVA\n(Liu etÂ al.,\n2023\n)\nas VLM.\nTop panel: Re-shot image with caption (top-left) and crop images from original viewpoints with their captions and similarity scores to the re-shot image.\nBottom panel: Complete scene reconstruction via GradSLAM\n(Krishna Murthy etÂ al.,\n2020\n)\nwith the target object highlighted in red bounding box.\nThe crop images consistently misidentify the object as â€œvaseâ€ due to occlusions and constrained viewpoints, resulting in low similarity scores that indicate high uncertainty.\nTable 3:\nExamples of dynamic voxel sizes for different objects with base voxel size\nÎ´\nsample\n=\n0.01\n\\delta^{\\text{sample}}=0.01\nm.\nObject Category\nTypical Size (LÃ—WÃ—H)\nDiagonal Length\nVoxel Size\nReduction Factor\nSmall Objects\nCoffee cup\n0.08\nÃ—\n0.08\nÃ—\n0.12\n0.08\\times 0.08\\times 0.12\n0.165 m\n0.004 m\n2.5Ã—\nSmartphone\n0.15\nÃ—\n0.07\nÃ—\n0.008\n0.15\\times 0.07\\times 0.008\n0.166 m\n0.004 m\n2.5Ã—\nComputer mouse\n0.12\nÃ—\n0.06\nÃ—\n0.04\n0.12\\times 0.06\\times 0.04\n0.14 m\n0.0037 m\n2.7Ã—\nMedium Objects\nMonitor\n0.55\nÃ—\n0.32\nÃ—\n0.05\n0.55\\times 0.32\\times 0.05\n0.638 m\n0.008 m\n1.25Ã—\nChair\n0.60\nÃ—\n0.55\nÃ—\n0.85\n0.60\\times 0.55\\times 0.85\n1.177 m\n0.011 m\n0.9Ã—\nDesk\n1.20\nÃ—\n0.60\nÃ—\n0.75\n1.20\\times 0.60\\times 0.75\n1.537 m\n0.012 m\n0.8Ã—\nLarge Objects\nSofa\n2.00\nÃ—\n0.90\nÃ—\n0.85\n2.00\\times 0.90\\times 0.85\n2.352 m\n0.015 m\n0.67Ã—\nDining table\n2.50\nÃ—\n1.20\nÃ—\n0.75\n2.50\\times 1.20\\times 0.75\n2.873 m\n0.017 m\n0.59Ã—\nBookshelf\n0.80\nÃ—\n0.30\nÃ—\n2.20\n0.80\\times 0.30\\times 2.20\n2.36 m\n0.015 m\n0.67Ã—\nExtra Large Objects\nWall\n10.0\nÃ—\n0.20\nÃ—\n3.0\n10.0\\times 0.20\\times 3.0\n10.44 m\n0.032 m\n0.03Ã—\nFloor\n10.0\nÃ—\n10.0\nÃ—\n0.05\n10.0\\times 10.0\\times 0.05\n10.00 m\n0.0316 m\n0.03Ã—\nAlgorithm 1\n3D Object Fusion with dynamic threshold\n0:\nLocal object list\nO\nt\nlocal\nO_{t}^{\\text{local}}\nfor frame\nt\nt\n, similarity threshold\nÎ´\nsim\n\\delta^{\\text{sim}}\n, sample threshold\nÎ´\nsample\n\\delta^{\\text{sample}}\n0:\nGlobal object list\nO\nt\nglobal\nO_{t}^{\\text{global}}\n1:\nInitialize global object list:\nO\n1\nglobal\n=\nO\n1\nlocal\nO_{1}^{\\text{global}}=O_{1}^{\\text{local}}\n2:\nfor\nt\n=\n2\nt=2\nto\nT\nT\ndo\n3:\nfor\neach local object\no\nt\n,\ni\nlocal\n=\n(\nf\nt\n,\ni\nlocal\n,\np\nt\n,\ni\nlocal\n)\nâˆˆ\nO\nt\nlocal\no_{t,i}^{\\text{local}}=(f_{t,i}^{\\text{local}},p_{t,i}^{\\text{local}})\\in O_{t}^{\\text{local}}\ndo\n4:\nbest_match\n=\nNone\n\\text{best\\_match}=\\text{None}\n5:\nmax_similarity\n=\n0\n\\text{max\\_similarity}=0\n6:\nfor\neach global object\no\nt\nâˆ’\n1\n,\nj\nglobal\n=\n(\nf\nt\nâˆ’\n1\n,\nj\nglobal\n,\np\nt\nâˆ’\n1\n,\nj\nglobal\n)\nâˆˆ\nO\nt\nâˆ’\n1\nglobal\no_{t-1,j}^{\\text{global}}=(f_{t-1,j}^{\\text{global}},p_{t-1,j}^{\\text{global}})\\in O_{t-1}^{\\text{global}}\ndo\n7:\n// Calculate semantic similarity\n8:\nÎ¸\nsemantic\nâ€‹\n(\ni\n,\nj\n)\n=\n(\nf\nt\n,\ni\nlocal\n)\nT\nâ€‹\nf\nt\nâˆ’\n1\n,\nj\nglobal\n/\n2\n+\n1\n/\n2\n\\theta_{\\text{semantic}}(i,j)=(f_{t,i}^{\\text{local}})^{T}f_{t-1,j}^{\\text{global}}/2+1/2\n9:\n// Calculate spatial similarity\n10:\nÎ´\ni\n,\nj\nnnratio\n=\nÎ´\nsample\nâ‹…\n(\nâ€–\nBbox\nâ€‹\n(\np\nt\n,\ni\nlocal\n)\nâ€–\n2\n1\n/\n2\n+\nâ€–\nBbox\nâ€‹\n(\np\nt\nâˆ’\n1\n,\nj\nglobal\n)\nâ€–\n2\n1\n/\n2\n)\n/\n2\n\\delta_{i,j}^{\\text{nnratio}}=\\delta^{\\text{sample}}\\cdot(\\|\\text{Bbox}(p_{t,i}^{\\text{local}})\\|_{2}^{1/2}+\\|\\text{Bbox}(p_{t-1,j}^{\\text{global}})\\|_{2}^{1/2})/2\n11:\nÎ¸\nspatial\nâ€‹\n(\ni\n,\nj\n)\n=\ndnnratio\nâ€‹\n(\np\nt\n,\ni\nlocal\n,\np\nt\nâˆ’\n1\n,\nj\nglobal\n)\n\\theta_{\\text{spatial}}(i,j)=\\text{dnnratio}(p_{t,i}^{\\text{local}},p_{t-1,j}^{\\text{global}})\n{Using threshold\nÎ´\ni\n,\nj\nnnratio\n\\delta_{i,j}^{\\text{nnratio}}\n}\n12:\n// Calculate fusion similarity\n13:\nÎ¸\nâ€‹\n(\ni\n,\nj\n)\n=\nÎ¸\nsemantic\nâ€‹\n(\ni\n,\nj\n)\n+\nÎ¸\nspatial\nâ€‹\n(\ni\n,\nj\n)\n\\theta(i,j)=\\theta_{\\text{semantic}}(i,j)+\\theta_{\\text{spatial}}(i,j)\n14:\nif\nÎ¸\nâ€‹\n(\ni\n,\nj\n)\n>\nmax_similarity\n\\theta(i,j)>\\text{max\\_similarity}\nthen\n15:\nmax_similarity\n=\nÎ¸\nâ€‹\n(\ni\n,\nj\n)\n\\text{max\\_similarity}=\\theta(i,j)\n16:\nbest_match\n=\nj\n\\text{best\\_match}=j\n17:\nend\nif\n18:\nend\nfor\n19:\nif\nmax_similarity\n>\nÎ´\nsim\n\\text{max\\_similarity}>\\delta^{\\text{sim}}\nand\nbest_match\nâ‰ \nNone\n\\text{best\\_match}\\neq\\text{None}\nthen\n20:\n// Fuse with matched global object\n21:\nj\n=\nbest_match\nj=\\text{best\\_match}\n22:\nn\n=\nmapping_times\nâ€‹\n(\nf\nt\nâˆ’\n1\n,\nj\nglobal\n)\nn=\\text{mapping\\_times}(f_{t-1,j}^{\\text{global}})\n23:\nf\nt\n,\nj\nglobal\n=\n(\nn\nâ‹…\nf\nt\nâˆ’\n1\n,\nj\nglobal\n+\nf\nt\n,\ni\nlocal\n)\n/\n(\nn\n+\n1\n)\nf_{t,j}^{\\text{global}}=(n\\cdot f_{t-1,j}^{\\text{global}}+f_{t,i}^{\\text{local}})/(n+1)\n24:\np\nt\n,\nj\nglobal\n=\np\nt\nâˆ’\n1\n,\nj\nglobal\nâˆª\np\nt\n,\ni\nlocal\np_{t,j}^{\\text{global}}=p_{t-1,j}^{\\text{global}}\\cup p_{t,i}^{\\text{local}}\n25:\no\nt\n,\nj\nglobal\n=\n(\nf\nt\n,\nj\nglobal\n,\np\nt\n,\nj\nglobal\n)\no_{t,j}^{\\text{global}}=(f_{t,j}^{\\text{global}},p_{t,j}^{\\text{global}})\n26:\nelse\n27:\n// Create new global object\n28:\nAdd\no\nt\n,\ni\nlocal\no_{t,i}^{\\text{local}}\nto\nO\nt\nglobal\nO_{t}^{\\text{global}}\nas a new global object\n29:\nend\nif\n30:\nend\nfor\n31:\nend\nfor\n32:\nreturn\nO\nt\nglobal\n=\n{\no\nt\n,\n1\nglobal\n,\nâ€¦\n,\no\nt\n,\nN\nglobal\n}\nO_{t}^{\\text{global}}=\\{o_{t,1}^{\\text{global}},\\ldots,o_{t,N}^{\\text{global}}\\}\nwhere\no\nt\n,\ni\nglobal\n=\n(\nf\nt\n,\ni\nglobal\n,\np\nt\n,\ni\nglobal\n)\no_{t,i}^{\\text{global}}=(f_{t,i}^{\\text{global}},p_{t,i}^{\\text{global}})",
    "preview_text": "Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.\n\nRAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation\nYue Chang, Rufeng Chen, Zhaofan Zhang, Yi Chen, Sihong Xie\nAI Thrust, HKUST(GZ)\nAbstract\nOpen-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations.\nA 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges.\nHowever, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density.\nTo address these challenges, we propose\nRAG-3DSG\nto mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented GenerationÂ (RAG) via reliable low-uncertainty objects.\nFurt",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "3D Scene Graph",
        "Retrieval-Augmented Generation",
        "object recognition",
        "robotics",
        "uncertainty estimation"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºRAG-3DSGæ–¹æ³•ï¼Œé€šè¿‡é‡æ‹å¼•å¯¼çš„ä¸ç¡®å®šæ€§ä¼°è®¡å’Œæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œæé«˜å¼€æ”¾è¯æ±‡3Dåœºæ™¯å›¾ç”Ÿæˆçš„å¯¹è±¡è¯†åˆ«å‡†ç¡®æ€§å’Œé€Ÿåº¦ï¼Œåº”ç”¨äºæœºå™¨äººæ“ä½œå’Œå¯¼èˆªä»»åŠ¡ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-15T08:15:01Z",
    "created_at": "2026-01-20T17:49:53.854793",
    "updated_at": "2026-01-20T17:49:53.854802",
    "recommend": 0
}