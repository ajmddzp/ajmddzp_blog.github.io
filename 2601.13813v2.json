{
    "id": "2601.13813v2",
    "title": "GuideTouch: An Obstacle Avoidance Device with Tactile Feedback for Visually Impaired",
    "authors": [
        "Timofei Kozlov",
        "Artem Trandofilov",
        "Georgii Gazaryan",
        "Issatay Tokmurziyev",
        "Miguel Altamirano Cabrera",
        "Dzmitry Tsetserukou"
    ],
    "abstract": "针对视障人士的安全导航仍是一项关键挑战，尤其在头部高度障碍物检测方面，传统助行设备往往难以应对。本文提出GuideTouch——一款紧凑、经济、独立的可穿戴自主避障设备。该系统集成两个垂直排列的飞行时间传感器实现三维环境感知，配备四个振动触觉执行器提供定向触觉反馈。通过部署于用户肩部与上胸部的四点式触觉反馈系统，可直观传达障碍物的距离与方向信息。为提升实际场景鲁棒性，设备创新采用离心式自清洁光学盖板机制，并配备跌落定位声光警报系统。我们通过22名参与者（男性17名，女性5名，年龄21-48岁，均值25.7，标准差6.1）完成触觉感知精度评估，统计分析证实不同振动模式间的感知准确率存在显著差异。系统展现出高识别准确率，单/双电机（主方向）模式平均达92.9%。此外，14名视障用户的初步实验验证了该交互界面的有效性，对主方向线索的识别准确率达93.75%。结果表明，GuideTouch能够实现直观的空间感知，有望显著提升视障用户在独立导航过程中的安全性、自信心与自主性。",
    "url": "https://arxiv.org/abs/2601.13813v2",
    "html_url": "https://arxiv.org/html/2601.13813v2",
    "html_content": "GuideTouch: An Obstacle Avoidance Device with Tactile Feedback for Visually Impaired\nTimofei Kozlov, Artem Trandofilov*, Georgii Gazaryan*,\nIssatay Tokmurziyev, Miguel Altamirano Cabrera, Dzmitry Tsetserukou\nSkolkovo Institute of Science and Technology (Skoltech)\nMoscow\nRussia\nTimofei.Kozlov, Artem.Trandofilov, Georgii.Gazaryan, Issatay.Tokmurziyev, m.altamirano, d.tsetserukou@skoltech.ru\nAbstract.\nSafe navigation for the visually impaired individuals remains a critical challenge, especially concerning head-level obstacles, which traditional mobility aids often fail to detect. We introduce GuideTouch, a compact, affordable, standalone wearable device designed for autonomous obstacle avoidance. The system integrates two vertically aligned Time-of-Flight (ToF) sensors, enabling three-dimensional environmental perception, and four vibrotactile actuators that provide directional haptic feedback. Proximity and direction information is communicated via an intuitive 4-point vibrotactile feedback system located across the user’s shoulders and upper chest. For real-world robustness, the device includes a unique centrifugal self-cleaning optical cover mechanism and a sound alarm system for location if the device is dropped. We evaluated the haptic perception accuracy across 22 participants (17 male and 5 female, aged 21–-48; mean 25.7,\n±\n\\pm\n6.1). Statistical analysis confirmed a significant difference between the perception accuracy of different patterns. The system demonstrated high recognition accuracy, achieving an average of 92.9% for single and double motor (primary directional) patterns. Furthermore, preliminary experiments with 14 visually impaired users validated this interface, showing a recognition accuracy of 93.75% for primary directional cues. The results demonstrate that GuideTouch enables intuitive spatial perception and could significantly improve the safety, confidence, and autonomy of users with visual impairments during independent navigation.\nAssistive navigation, Vibrotactile feedback, Obstacle detection, Time-of-Flight sensors, Visual Impairment, Haptic interface\n*These authors contributed equally to this work.\n†\n†\nccs:\nHuman-centered computing Accessibility technologies\n†\n†\nccs:\nHuman-centered computing Haptic devices\n†\n†\nccs:\nHuman-centered computing Ubiquitous and mobile computing systems and tools\n†\n†\nccs:\nHuman-centered computing Empirical studies in accessibility\nFigure 1\n.\nGuideTouch wearable obstacle avoidance system for visually impaired navigation. (a) Usage scenario with the device worn during outdoor navigation. (b) System architecture comprising a vibrotactile module, a sensor module, an auditory alert module, and a power supply.\nThis figure shows the architecture and use scenario of the GuideTouch - wearable obstacle avoidance device for the visually impaired. On the left, a user who is blind walks outdoors near an obstacle at head level, which cannot be reached by a white cane. The obstacle is detected by infrared ToF sensors on the GuideTouch device, worn on the shoulders, and the directional vibrotactile feedback given to the user by four embedded vibration motors in the scarf-like wearable allows the user to avoid the obstacle before physical contact. On the right (b), a hardware diagram breaks down the main functional blocks of the GuideTouch device. The Vibrotactile Feedback Block includes four vibration motors placed on the shoulders and upper chest for spatially distributed alerts. The Sensor Block consists of two ToF sensors that measure distance, an ESP32 microcontroller that performs the processing, and a self-cleaning optical cover mechanism to maintain sensor clarity in harsh outdoor conditions. The Sound Alarm System consists of a buzzer and a contact key to help locate the device if it falls off. Finally, the Power Supply Block consists of a rechargeable battery and an external power switch for user control. Each component is physically integrated into the wearable structure, as shown on the actual photo of the device on the right.\n1.\nIntroduction\nVisual impairment substantially reduces personal autonomy and mobility. According to the WHO, around 295 million people are visually impaired and 43 million are blind worldwide\n(World Health Organization,\n2023\n)\n. Navigation in dynamic environments remains difficult: more than 40% of blind individuals report head‑level injuries or falls every few months, regardless of mobility aids\n(Blanco et al\n.\n,\n2010\n)\n. Modern assistive navigation technologies increasingly combine real‑time obstacle detection with haptic feedback\n(Xu et al\n.\n,\n2023a\n; Joseph et al\n.\n,\n2023\n; Eagleman and Perrotta,\n2023\n)\n, which offers a silent and intuitive orientation channel compared to auditory cues that may be masked by environmental noise. Studies confirm that tactile feedback improves safety by providing immediate, directionally relevant information through wearable devices.\nExisting solutions\n(Katzschmann et al\n.\n,\n2018\n; Skulimowski et al\n.\n,\n2025\n; Leporini et al\n.\n,\n2022\n; Bazhenov et al\n.\n,\n2024\n)\nrange from deep‑learning smart glasses and VLM-based interaction systems\n(Tokmurziyev et al\n.\n,\n2025\n; Gao et al\n.\n,\n2025b\n; Khan et al\n.\n,\n2025\n)\nto wearable ToF systems, but all exhibit limitations. Computer Vision based models such as YOLO require substantial computational power\n(Said et al\n.\n,\n2023\n; Xu et al\n.\n,\n2023b\n)\n, making devices bulky or dependent on external processing. Smart-glasses based designs may be uncomfortable or culturally unsuitable, while camera systems degrade in low‑light conditions\n(Elmannai and Elleithy,\n2017\n)\n. Several solutions also assume replacing rather than complementing the white cane, which our user study indicates is highly undesirable.\nWe propose a wearable device that detects obstacles via infrared Time‑of‑Flight sensors and provides haptic feedback through four vibration motors positioned on the shoulders and upper chest\n(Park et al\n.\n,\n2025\n; Yeganeh et al\n.\n,\n2023\n)\n. The system includes automatic optical cover cleaning and a location alarm in case the device is dropped.\nIn Section\n2\n, the architecture of the system is described with subsections that describe each subsystem in more detail. In Section\n3\n, the experiment is conducted to validate the accuracy of haptic perception of several vibrotactile patterns.\nTo validate system requirements, we interviewed 28 visually impaired participants (17 male, 11 female, aged 16–-60). They identified three major categories of undetected obstacles: head‑level hazards (road signs, gates, horizontal bars), drop‑offs and ground‑level risks (open manholes, stairs), and thin objects (poles, bollards, fencing). Participants expressed strong interest in a hands‑free, wearable device capable of covering these blind spots—especially head‑level obstacles, which were reported as the most frequent and dangerous.\n2.\nSystem Architecture\nGuideTouch is an obstacle avoidance device based on ToF sensors, which produce a depth image of the area in front of the wearer. The device analyzes the image using an ESP32 microcontroller. The general scheme of the prototype is shown in Fig.\n1\n. The system consists of two boxes and a scarf that holds them together, with a total weight not exceeding 500 g, and an estimated cost of approximately 100 USD. Vibration motors, the alarm system, an electronic speed controller, and wiring are installed on the inside of the scarf. One of the boxes contains a battery and a general power switch, providing up to 12 hours of autonomous operation, or 4 hours while rotating the optical cover. The other box includes a microcontroller ESP32, two ToF sensors, and a self-cleaning optical cover mechanism, which consists of a brushless direct current (BLDC) motor and mechanical transmission. The switch for turning on the mechanism is located on the side of the second box.\n2.1.\nMechanics\nRaindrops and snowflakes pose a significant problem for wearable devices that rely on cameras or other light-based sensors. We investigated several approaches for cleaning the optical cover that is in front of the ToF sensors: an ultrasonic membrane, which breaks droplets into smaller particles and detaches them from the surface; vibration motors, which help droplets flow down more quickly; and a spinning mechanism that relies on centrifugal force, using a BLDC motor and a belt drive to rotate the optical cover (in the form of circular infrared transparent glass) at 3000 rpm.\nDuring our testing, the spinning mechanism demonstrated the best performance, effectively removing droplets of any size in 18 out of 20 cases and leaving no visible traces, whereas the other methods failed to clear all droplets or removed them too slowly. The noise level generated by this chosen cleaning system measured an average of 70 dB at head level, a sound level considered safe for continuous exposure exceeding the mechanism’s expected operating time\n(Fink,\n2017\n)\n.\n2.2.\nElectronics and Sensors\nFigure 2\n.\nCustom ToF sensor module. (a) PCB layout design, (b) Assembled board with dual VL53L5CX sensors mounted behind the optical lens.\nFigure shows the sensor hardware in the GuideTouch system.\nImage (a) shows the designed layout of a custom PCB, intended for connecting a ToF distance sensor. Image (b) is a close-up photo of the assembled sensor module embedded inside the GuideTouch device. Two stacked PCBs are attached: the first is aiming straight ahead, while the second one aims a bit down. The whole assembly is fixed behind the circular optical cover and enclosed in the mechanical self-cleaning module.\nGuideTouch uses two vertically aligned ToF multizone ranging sensors (VL53L5CX) on custom printed circuit boards (PCBs) to obtain distance data (Fig.\n2\n). Each sensor provides an 8x8 matrix of distances and has a\n65\n∘\n65^{\\circ}\ndiagonal square Field of View. The two sensors are positioned at a\n30\n∘\n30^{\\circ}\nangle relative to each other, achieving a combined vertical Field of View of\n90\n∘\n90^{\\circ}\n. This configuration allows the device, when worn by a user of average height (170 cm), to detect obstacles at knee level (30 cm) and head level (160 cm) from a distance of 50 cm. The size of a detectable obstacle can be calculated based on the sensor’s view angle and the number of measuring zones. The linear size\ns\ns\nof the zones can be calculated with the following formula:\n(1)\ns\n=\n2\n​\nd\n⋅\ntan\n⁡\n(\n1\n2\n⋅\nFoV\nN\n⋅\nπ\n180\n)\n,\ns=2d\\cdot\\tan\\left(\\frac{1}{2}\\cdot\\frac{\\text{FoV}}{N}\\cdot\\frac{\\pi}{180}\\right),\nwhere\nd\n{d}\nis the distance from the user to the obstacle,\nF\n​\no\n​\nV\n=\n60\n∘\n{FoV}=60^{\\circ}\nis the sensor’s view angle,\nN\n=\n8\n{N}=8\nis the number of zones on each side of the view square. Sensors can detect obstacles that occupy approximately 30% of a view zone’s area. At a distance of 1 m the device is able to detect obstacles with the linear size of 4 cm.\nThe Alarm system consists of a passive piezo buzzer with a timer module NE555 and an electrical contact key (clip). The clip is placed on a person’s collar or other part of clothing. If the device becomes dislodged, the clip closes the circuit, and the buzzer produces a high-pitched noise (chosen around 3–4 kHz) to notify the user of the device’s position.\n2.3.\nHaptic Feedback and Navigation\nFour vibration motors are embedded into the fabric of the scarf: two positioned above each enclosure, and two placed over the shoulders on either side of the neck. Each vibration motor corresponds to a specific direction. The general work process of the proposed device is visualized in Fig.\n3\n. The ESP32 microcontroller receives the distance data from the ToF sensors every 0.1 seconds. Then it processes these data, filters any noise using temporal outlier filtering, and analyzes the depth map for the presence of obstacles (Fig.\n4\n). The algorithm separates the pictures into several zones (e.g., left, right, top) and produces the signals for vibration motors. The device vibrates on the side closest to the obstacle, informing the user of the direction of the possible danger. If an obstacle covers multiple zones (e.g., an overhanging section) the device activates the set of motors corresponding to all dangerous directions.\nFigure 3\n.\nSystem operational workflow.\nThe figure shows the flow diagram of data in GuideTouch, where on the left, the environment is sensed using Time-of-Flight sensors that produce a raw matrix of distance measurements. Data is then sent to the ESP32 microcontroller, which has a custom algorithm to process this matrix. A control algorithm then uses this processed information to generate signals that turn on vibration motors, which are used to provide a user with vibrotactile feedback about the presence and direction of obstacles detected. The user receives feedback and, based on that, can adapt their movements.\nThe device functions as a human-oriented analog of a parking sensor system, providing proximity feedback through haptic cues. The device vibrates on the side that is closest to the obstacle ahead, thus informing the user of the direction of the possible danger. If the obstacle covers multiple zones (e.g., a sudden horizontal wall protrusion with an overhanging section, posing a serious risk of head impact), the device will vibrate with a set of vibration motors that correspond to all the dangerous directions.\nFigure 4\n.\nSensor data visialization methods. (a) 3D point cloud of obstacle geometry. (b) 2D heat map of distance measurements.\nThe figure consists of two subfigures showing the visualisation of data obtained from Time-of-Flight sensors. Subfigure (a) shows a 3D rendering of distance vectors produced by a Time-Of-Flight sensor. Blue rays are cast towards a nearby rectangular object, with distance points visualised as small spheres on the object surface. Subfigure (b) presents a heat map of the sensor readings. The grid is coloured according to the proximity of objects: red indicates close distances (near obstacles), while blue represents farther regions. The image on the heat map corresponds to a Depth Image of a rectangular object\n3.\nSystem Evaluation\nThe system evaluation was conducted in two phases: assessing the accuracy of perception of the haptic interface and performing preliminary validation with visually impaired end-users.\n3.1.\nParticipants\nTo evaluate the performance of the haptic feedback subsystem and to analyze the precision of vibrotactile pattern recognition, we conducted a series of experiments involving 22 participants (17 male and 5 female, aged 21 to 48; mean 25.7,\n±\n\\pm\n6.1) without previous experience on the system. The participants were divided into two equal-sized groups, A and B, where group A experienced all possible vibrotactile patterns with single, double, and triple motor combinations (15 patterns) and group B experienced only single and double motor vibration (10 patterns).\n3.2.\nProcedure\nAt the beginning of the experiment, the participant was invited to sit in front of a laptop and wear the GuideTouch device. Then a training session was conducted, in which the user became familiar with the vibrotactile patterns. After reading the initial instruction, the experiment coordinators turned on the GuideTouch device and motors started vibrating in a predetermined order. The experiment coordinators announced to the participant which motors were vibrating at that exact time. Group A experienced vibrotactile patterns that included 1, 2, and 3 motors. Group B experienced vibrotactile patterns that included only single and double motor patterns.\nAfter the initial stage of the experiment, vibrations stopped and the experiment coordinators read the next part of the instructions. The Python script was launched on the laptop, which displayed the experiment GUI. The participant was instructed to familiarize themselves with the GUI, after which they pressed the Start button and the Python script began sending vibration commands to the device, starting the vibrations. The order of the vibrotactile patterns was randomized by the script, each pattern occurring 5 times during the experiment. Each participant in Group A experienced\n5\n×\n15\n=\n75\n5\\times 15=75\nvibrotactile patterns and each participant in Group B experienced\n5\n×\n10\n=\n50\n5\\times 10=50\nvibrotactile patterns. Each vibration lasted for 3 seconds, after or during which the participant chose the button in the GUI that corresponded to the vibrotactile pattern currently being active. After the participant’s answer, the next vibration started until each pattern was repeated 5 times. The Python script recorded the vibration pattern that was sent to the device and executed as well as the participant’s perceived pattern which they chose in the GUI. The total number of measurements is 825 for Group A (15 patterns\n×\n\\times\n5 repetitions\n×\n\\times\n11 users) and 550 for Group B (10 patterns\n×\n\\times\n5 repetitions\n×\n\\times\n11 users).\n3.3.\nResults\nTo visualize the results, confusion matrices for each group were created (Table\n1\nand Table\n2\n). Each vibration motor was assigned a corresponding abbreviation. The bottom left motor is L1, the bottom right motor is R1, the upper left motor is L2 and the upper right motor is R2. For abbreviating patterns that consist of combinations of motors, the ‘+’ sign is used (e.g. the pattern consisting of the upper right and the bottom left motors is abbreviated as L1 + R2).\nTable 1\n.\nConfusion Matrix of Vibrotactile Patterns for Group A (11 participants), values in %\nTrue \\ Pred.\nL1\nL2\nR1\nR2\nL1+L2\nR1+R2\nL1+R1\nL2+R2\nL1+R2\nL2+R1\nL1+L2+R1\nL1+L2+R2\nL1+R1+R2\nL2+R1+R2\nL1+L2+R1+R2\nL1\n98\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nL2\n2\n93\n0\n0\n0\n0\n0\n0\n0\n0\n2\n2\n0\n2\n0\nR1\n0\n0\n96\n2\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\nR2\n0\n0\n0\n98\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\nL1+L2\n5\n9\n0\n0\n85\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nR1+R2\n0\n0\n4\n15\n0\n80\n0\n0\n0\n0\n0\n0\n2\n0\n0\nL1+R1\n2\n0\n0\n0\n0\n0\n96\n0\n0\n0\n2\n0\n0\n0\n0\nL2+R2\n0\n0\n0\n0\n0\n0\n0\n98\n0\n0\n0\n0\n0\n2\n0\nL1+R2\n0\n0\n0\n0\n0\n0\n2\n0\n73\n4\n0\n9\n7\n5\n0\nL2+R1\n0\n5\n0\n0\n0\n0\n0\n0\n5\n65\n11\n2\n0\n9\n2\nL1+L2+R1\n0\n0\n0\n0\n2\n0\n7\n2\n2\n5\n65\n5\n2\n0\n9\nL1+L2+R2\n0\n2\n0\n0\n0\n0\n2\n16\n2\n0\n4\n64\n0\n4\n7\nL1+R1+R2\n0\n0\n0\n0\n0\n2\n13\n0\n18\n0\n4\n5\n42\n11\n5\nL2+R1+R2\n0\n2\n0\n0\n0\n0\n0\n20\n0\n5\n2\n4\n9\n56\n2\nL1+L2+R1+R2\n0\n0\n0\n0\n0\n0\n2\n7\n0\n0\n13\n2\n2\n9\n65\nTable 2\n.\nConfusion Matrix of Vibrotactile Patterns For Group B (11 participants), values in %\nTrue \\ Pred.\nL1\nL2\nR1\nR2\nL1+L2\nR1+R2\nL1+R1\nL2+R2\nL1+R2\nR1+L2\nL1\n98\n2\n0\n0\n0\n0\n0\n0\n0\n0\nL2\n0\n95\n0\n4\n0\n0\n0\n2\n0\n0\nR1\n0\n0\n95\n4\n0\n0\n0\n2\n0\n0\nR2\n0\n0\n0\n98\n0\n0\n0\n0\n2\n0\nL1+L2\n4\n11\n0\n0\n84\n0\n0\n0\n2\n0\nR1+R2\n0\n0\n4\n11\n0\n80\n2\n2\n0\n2\nL1+R1\n2\n4\n0\n0\n0\n0\n93\n2\n0\n0\nL2+R2\n0\n0\n0\n2\n0\n0\n0\n98\n0\n0\nL1+R2\n0\n0\n0\n0\n0\n0\n0\n2\n98\n0\nR1+L2\n0\n5\n0\n2\n0\n0\n0\n2\n0\n91\nThe average accuracy for perceiving the vibrotactile patterns is 78.4% for Group A and 92.9% for Group B. The difference in average accuracy can be explained by the difference in the complexity of the patterns between two groups. This can be further observed in the Confusion Matrix for Group A. The accuracy for single and double motor patterns is higher than for 3 and 4 motor patterns, while the majority of errors in single and double motor patterns were caused by confusing them with 3 and 4 motor patterns.\nFor Group A (All Patterns), one-way repeated measurements ANOVA on vibration patterns revealed a statistically significant difference:\nF\n​\n(\n14\n,\n150\n)\n=\n7.5339\n,\nF(14,150)=7.5339,\np\n<\n0.000001\np<0.000001\n. Post-hoc analysis (Bonferroni and Tukey’s HSD) confirmed that the significant differences were concentrated between simpler patterns (1 and 2 motors) and more complex patterns (3 and 4 motors), indicating that increased spatial overlap reduces recognition accuracy.\nFor Group B (Simple Patterns), the one-way repeated measurements ANOVA did not show statistically significant difference\nF\n​\n(\n9\n,\n100\n)\n=\n1.227\n,\nF(9,100)=1.227,\np\n=\n0.287\np=0.287\n. However, standard one-way ANOVA on participants revealed a significant effect:\nF\n​\n(\n10\n,\n99\n)\n=\n3.8842\n,\nF(10,99)=3.8842,\np\n=\n0.0002\np=0.0002\n. Tukey’s HSD test detected statistically significant differences between two female participants and the rest. Notably, these two female participants had reported suboptimal contact with the device during the experiment, which they attributed to physiological discomfort, potentially affecting their recognition accuracy.\n3.4.\nPreliminary Experiments with Visually Impaired Participants\nWe conducted preliminary experiments with blind participants, including visits to several associations for the blind and visually impaired support centers. During these tests, we evaluated the ability of blind users (14 participants, 8 male and 6 female, aged 16–-60) to accurately identify vibrotactile patterns generated by our device. The results demonstrated an average recognition accuracy of 93.7%, indicating the effectiveness of our approach.\n4.\nConclusion and Future Work\nIn this paper, we introduced GuideTouch, a compact standalone wearable device designed to assist individuals with visual impairments in avoiding obstacles, particularly upper-body obstacles, which are a common cause of injury. The system combines the use of Time-of-Flight sensors for 3-D environmental perception with a 4-point tactile feedback system to convey spatial cues. The conducted experiments validated the utility of vibration motors as a practical way to convey directional information to the user. Statistical analysis showed a high average recognition accuracy (92.9%) for single and double motor patterns. Post-hoc analysis indicated a significant variance in the perception of vibrotactile patterns based on their complexity, and a significant difference in accuracy between some participants, suggesting a dependency on wearability and potentially physiological differences. Preliminary experiments with blind participants further demonstrated a high recognition accuracy (93.75%) for primary directional cues, confirming the effectiveness of the interface.\nWhile these findings validate the interface concept, they were obtained with first-time users and under static conditions. Based on the results of the experiments, we formulated the hypothesis that with training — or when tested by blind individuals who are more sensitive to tactile stimuli — the recognition accuracy of vibrotactile patterns may improve\n(Gao et al\n.\n,\n2025a\n; Shah et al\n.\n,\n2022\n; Cuppone et al\n.\n,\n2016\n; Stronks et al\n.\n,\n2017\n)\n. Future work will focus on a comprehensive evaluation of the system in dynamic scenarios with multiple-day training periods for participants to heighten their perception of vibrotactile stimuli. In the immediate future, we plan to integrate a higher-resolution ToF sensor, which would be implemented in a Computer Vision system\n(Kolb et al\n.\n,\n2010\n)\nto help navigating in the crowds. At the same time, we plan to start training sessions with the participants to familiarize them with the vibrations.\nAcknowledgements\nResearch reported in this publication was financially supported by the RSF grant No. 24-41-02039.\nReferences\n(1)\nBazhenov et al\n.\n(2024)\nA. Bazhenov, V. Berman, S. Satsevich, O. Shalopanova, M. Altamirano Cabrera, A. Lykov, and D. Tsetserukou. 2024.\nDogSurf: Quadruped Robot Capable of GRU-based Surface Recognition for Blind Person Navigation. In\nProc. IEEE/ACM Int. Conf. on Human-Robot Interaction\n(HRI 2024)\n. ACM, Boulder, Colorado, USA, 238–242.\ndoi:\n10.1145/3610978.3640606\nBlanco et al\n.\n(2010)\nK. Blanco, C. Sanchez, and M. Hu. 2010.\nMobility-Related Accidents Experienced by People with Visual Impairment\n.\nTechnical Report UCSC-SOE-10-24. University of California, Santa Cruz, Santa Cruz, CA, USA.\nCuppone et al\n.\n(2016)\nAnna Vera Cuppone, Valentina Squeri, Marianna Semprini, Lorenzo Masia, and Jürgen Konczak. 2016.\nRobot-Assisted Proprioceptive Training with Added Vibro-Tactile Feedback Enhances Somatosensory and Motor Performance.\nPLOS ONE\n11, 10, Article e0164511 (2016).\ndoi:\n10.1371/journal.pone.0164511\nEagleman and Perrotta (2023)\nDavid M. Eagleman and Michael V. Perrotta. 2023.\nThe future of sensory substitution, addition, and expansion via haptic devices.\nFrontiers in Human Neuroscience\n16, Article 1055546 (2023).\ndoi:\n10.3389/fnhum.2022.1055546\nElmannai and Elleithy (2017)\nWafa Elmannai and Khaled Elleithy. 2017.\nSensor-Based Assistive Devices for Visually-Impaired People: Current Status, Challenges, and Future Directions.\nSensors\n17, 3, Article 565 (2017).\ndoi:\n10.3390/s17030565\nFink (2017)\nDaniel J. Fink. 2017.\nWhat Is a Safe Noise Level for the Public?\nAmerican Journal of Public Health\n107, 1 (2017), 44–45.\ndoi:\n10.2105/AJPH.2016.303527\nGao et al\n.\n(2025a)\nYichen Gao, Menghan Hu, and Gang Luo. 2025a.\nGrating haptic perception through touchscreen: Sighted vs. Visually Impaired\n.\narXiv:2511.10026.\nRetrieved from https://arxiv.org/abs/2511.10026.\nGao et al\n.\n(2025b)\nYun Gao, Dan Wu, Jie Song, Xueyi Zhang, Bangbang Hou, Hengfa Liu, Junqi Liao, and Liang Zhou. 2025b.\nA Wearable Obstacle Avoidance Device for Visually Impaired Individuals with Cross-Modal Learning.\nNature Communications\n16, Article 2857 (2025).\ndoi:\n10.1038/s41467-025-58085-x\nJoseph et al\n.\n(2023)\nAnna M. Joseph, Azadeh Kian, and Rezaul Begg. 2023.\nState-of-the-Art Review on Wearable Obstacle Detection Systems Developed for Assistive Technologies and Footwear.\nSensors\n23, 5, Article 2802 (2023).\ndoi:\n10.3390/s23052802\nKatzschmann et al\n.\n(2018)\nRobert K. Katzschmann, Brandon Araki, and Daniela Rus. 2018.\nSafe Local Navigation for Visually Impaired Users With a Time-of-Flight and Haptic Feedback Device.\nIEEE Transactions on Neural Systems and Rehabilitation Engineering\n26, 3 (2018), 583–593.\ndoi:\n10.1109/TNSRE.2018.2800665\nKhan et al\n.\n(2025)\nMuhammad Haris Khan, Miguel Altamirano Cabrera, Dmitrii Iarchuk, Yara Mahmoud, Daria Trinitatova, Issatay Tokmurziyev, and Dzmitry Tsetserukou. 2025.\nHapticVLM: VLM-Driven Texture Recognition Aimed at Intelligent Haptic Interaction\n.\narXiv:2505.02569.\nRetrieved from https://arxiv.org/abs/2505.02569.\nKolb et al\n.\n(2010)\nAndreas Kolb, Erhardt Barth, Reinhard Koch, and Rasmus Larsen. 2010.\nTime-of-Flight Cameras in Computer Graphics.\nComputer Graphics Forum\n29, 1 (2010), 141–159.\ndoi:\n10.1111/j.1467-8659.2009.01583.x\nLeporini et al\n.\n(2022)\nBarbara Leporini, Michele Rosellini, and Nicola Forgione. 2022.\nHaptic Wearable System to Assist Visually-Impaired People in Obstacle Detection. In\nProc. of the 15th Int. Conf. on Pervasive Technologies Related to Assistive Environments\n(PETRA ’22)\n. ACM, New York, NY, USA, 515–519.\ndoi:\n10.1145/3529190.3529217\nPark et al\n.\n(2025)\nSeong Ung Park, Hyeong Kwon Lee, Hyeong Bo Kim, et al\n.\n2025.\nWearable interactive full-body motion tracking and haptic feedback network systems with deep learning.\nNature Communications\n16, Article 8604 (2025).\ndoi:\n10.1038/s41467-025-63644-3\nSaid et al\n.\n(2023)\nYahia Said, Mohamed Atri, Marwan Ali Albahar, Ahmed Ben Atitallah, and Yazan Ahmad Alsariera. 2023.\nObstacle Detection System for Navigation Assistance of Visually Impaired People Based on Deep Learning Techniques.\nSensors\n23, 11, Article 5262 (2023).\ndoi:\n10.3390/s23115262\nShah et al\n.\n(2022)\nVinay A. Shah, Nikolai Nikolaev, Mariya A. Hu, Alexander V. Kirillov, et al\n.\n2022.\nExtended training improves the accuracy and efficiency of goal-directed movements guided by vibro-tactile feedback.\nExperimental Brain Research\n241, 2 (2022), 479–493.\ndoi:\n10.1007/s00221-022-06533-1\nSkulimowski et al\n.\n(2025)\nPiotr Skulimowski, Paweł Strumiłło, and Szymon Trygar. 2025.\nHaptic and auditory cues: a study on independent navigation for visually impaired individuals.\nJournal on Multimodal User Interfaces\n19 (2025), 363–373.\ndoi:\n10.1007/s12193-025-00463-2\nStronks et al\n.\n(2017)\nH. Christiaan Stronks, Janine Walker, Daniel J. Parker, and Nick Barnes. 2017.\nTraining Improves Vibrotactile Spatial Acuity and Intensity Discrimination on the Lower Back Using Coin Motors.\nArtificial Organs\n41, 11 (2017), 1059–1070.\ndoi:\n10.1111/aor.12882\nTokmurziyev et al\n.\n(2025)\nIssatay Tokmurziyev, Miguel Altamirano Cabrera, Muhammad Haris Khan, Yara Mahmoud, Luis Moreno, and Dzmitry Tsetserukou. 2025.\nLLM-Glasses: GenAI-driven Glasses with Haptic Feedback for Navigation of Visually Impaired People\n.\narXiv:2503.16475.\nRetrieved from https://arxiv.org/abs/2503.16475.\nWorld Health Organization (2023)\nWorld Health Organization. 2023.\nBlindness and vision impairment\n.\nReport. World Health Organization (WHO), Geneva, Switzerland.\nXu et al\n.\n(2023a)\nPeijie Xu, Gerard A. Kennedy, Fei-Yi Zhao, Wen-Jing Zhang, and Ron Van Schyndel. 2023a.\nWearable Obstacle Avoidance Electronic Travel Aids for Blind and Visually Impaired Individuals: A Systematic Review.\nIEEE Access\n11 (2023), 66587–66613.\ndoi:\n10.1109/ACCESS.2023.3285396\nXu et al\n.\n(2023b)\nPeijie Xu, Andy Song, and Ke Wang. 2023b.\nIntelligent Head-Mounted Obstacle Avoidance Wearable for the Blind and Visually Impaired.\nSensors\n23, 23, Article 9598 (2023).\ndoi:\n10.3390/s23239598\nYeganeh et al\n.\n(2023)\nNashmin Yeganeh, Ivan Makarov, Árni Kristjánsson, and Runar Unnthórsson. 2023.\nDiscrimination Accuracy of Sequential Versus Simultaneous Vibrotactile Stimulation on the Forearm.\nApplied Sciences\n14, 1, Article 43 (2023).\ndoi:\n10.3390/app14010043",
    "preview_text": "Safe navigation for the visually impaired individuals remains a critical challenge, especially concerning head-level obstacles, which traditional mobility aids often fail to detect. We introduce GuideTouch, a compact, affordable, standalone wearable device designed for autonomous obstacle avoidance. The system integrates two vertically aligned Time-of-Flight (ToF) sensors, enabling three-dimensional environmental perception, and four vibrotactile actuators that provide directional haptic feedback. Proximity and direction information is communicated via an intuitive 4-point vibrotactile feedback system located across the user's shoulders and upper chest. For real-world robustness, the device includes a unique centrifugal self-cleaning optical cover mechanism and a sound alarm system for location if the device is dropped. We evaluated the haptic perception accuracy across 22 participants (17 male and 5 female, aged 21-48, mean 25.7, sd 6.1). Statistical analysis confirmed a significant difference between the perception accuracy of different patterns. The system demonstrated high recognition accuracy, achieving an average of 92.9% for single and double motor (primary directional) patterns. Furthermore, preliminary experiments with 14 visually impaired users validated this interface, showing a recognition accuracy of 93.75% for primary directional cues. The results demonstrate that GuideTouch enables intuitive spatial perception and could significantly improve the safety, confidence, and autonomy of users with visual impairments during independent navigation.\n\nGuideTouch: An Obstacle Avoidance Device with Tactile Feedback for Visually Impaired\nTimofei Kozlov, Artem Trandofilov*, Georgii Gazaryan*,\nIssatay Tokmurziyev, Miguel Altamirano Cabrera, Dzmitry Tsetserukou\nSkolkovo Institute of Science and Technology (Skoltech)\nMoscow\nRussia\nTimofei.Kozlov, Artem.Trandofilov, Georgii.Gazaryan, Issatay.Tokmurziyev, m.altamirano, d.tsetserukou@skoltech.ru\nAbstract.\nSafe navigation",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "obstacle avoidance",
        "tactile feedback",
        "wearable device",
        "visually impaired",
        "navigation"
    ],
    "one_line_summary": "这篇论文介绍了一种名为GuideTouch的触觉反馈避障设备，专为视障人士设计，通过ToF传感器和振动触觉执行器实现三维环境感知和方向提示。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T10:12:05Z",
    "created_at": "2026-01-27T15:53:10.476310",
    "updated_at": "2026-01-27T15:53:10.476316",
    "recommend": 0
}