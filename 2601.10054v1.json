{
    "id": "2601.10054v1",
    "title": "UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow",
    "authors": [
        "Nick Truong",
        "Pritam P. Karmokar",
        "William J. Beksi"
    ],
    "abstract": "Ê∞¥‰∏ãÊàêÂÉèÈù¢‰∏¥ÁöÑÊ†πÊú¨ÊåëÊàòÂú®‰∫éÊ≥¢ÈïøÁõ∏ÂÖ≥ÁöÑÂÖâË°∞Âáè„ÄÅÊÇ¨ÊµÆÈ¢óÁ≤íÁöÑÂº∫ÁÉàÊï£Â∞Ñ„ÄÅÊµëÊµäÂ∫¶ÂØºËá¥ÁöÑÊ®°Á≥ä‰ª•Âèä‰∏çÂùáÂåÄÁöÑÁÖßÊòé„ÄÇËøô‰∫õÊïàÂ∫î‰∏ç‰ªÖÊçüÂÆ≥‰∫ÜÊ†áÂáÜÁõ∏Êú∫ÁöÑÊàêÂÉèË¥®ÈáèÔºå‰πü‰ΩøÂæóËé∑ÂèñÁúüÂÆûËøêÂä®‰ø°ÊÅØÂá†‰πé‰∏çÂèØËÉΩ„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºå‰∫ã‰ª∂Áõ∏Êú∫ÂÖ∑Â§áÂæÆÁßíÁ∫ßÁöÑÊó∂Èó¥ÂàÜËæ®ÁéáÂíåÈ´òÂä®ÊÄÅËåÉÂõ¥ÁâπÊÄß„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÁº∫‰πèÂ∞ÜÁúüÂÆûÊ∞¥‰∏ãÂÖâÂ≠¶ÁâπÊÄß‰∏éÁ≤æÁ°ÆÂÖâÊµÅÊï∞ÊçÆÁõ∏ÁªìÂêàÁöÑÊï∞ÊçÆÈõÜÔºå‰∫ã‰ª∂Áõ∏Êú∫Âú®Ê∞¥‰∏ãÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®Á†îÁ©∂ËøõÂ±ïÊúâÈôê„ÄÇ‰∏∫Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨È¶ñÊ¨°Êé®Âá∫‰∫ÜÂü∫‰∫éÁâ©ÁêÜÂÖâÁ∫øËøΩË∏™RGBDÂ∫èÂàóÁîüÊàêÁöÑÂêàÊàêÊ∞¥‰∏ã‰∫ã‰ª∂ÂÖâÊµÅÂü∫ÂáÜÊï∞ÊçÆÈõÜ„ÄÇÈÄöËøáÂØπÊ∏≤ÊüìÁöÑÊ∞¥‰∏ãËßÜÈ¢ëÂ∫îÁî®Áé∞‰ª£ËßÜÈ¢ë-‰∫ã‰ª∂ËΩ¨Êç¢ÊµÅÁ®ãÔºåÊàë‰ª¨ÁîüÊàê‰∫ÜÂåÖÂê´ÂØÜÈõÜÁúüÂÆûÂÖâÊµÅ„ÄÅÊ∑±Â∫¶ÂèäÁõ∏Êú∫ËøêÂä®‰ø°ÊÅØÁöÑÈÄºÁúü‰∫ã‰ª∂Êï∞ÊçÆÊµÅ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÈÄöËøáÂü∫ÂáÜÊµãËØïÊØîËæÉ‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑÂü∫‰∫éÂ≠¶‰π†ÂíåÂü∫‰∫éÊ®°ÂûãÁöÑÂÖâÊµÅÈ¢ÑÊµãÊñπÊ≥ïÔºå‰ª•Êé¢Á©∂Ê∞¥‰∏ãÂÖâ‰º†ËæìÂ¶Ç‰ΩïÂΩ±Âìç‰∫ã‰ª∂ÂΩ¢ÊàêÂèäËøêÂä®‰º∞ËÆ°Á≤æÂ∫¶„ÄÇÊú¨Êï∞ÊçÆÈõÜ‰∏∫Êú™Êù•Ê∞¥‰∏ã‰∫ã‰ª∂ÊÑüÁü•ÁÆóÊ≥ïÁöÑÂºÄÂèë‰∏éËØÑ‰º∞Âª∫Á´ã‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜ„ÄÇÈ°πÁõÆÊ∫ê‰ª£Á†ÅÂèäÊï∞ÊçÆÈõÜÂ∑≤Âú®https://robotic-vision-lab.github.io/ueofÂÖ¨ÂºÄ„ÄÇ",
    "url": "https://arxiv.org/abs/2601.10054v1",
    "html_url": "https://arxiv.org/html/2601.10054v1",
    "html_content": "UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow\nNick Truong\n‚àó\n, Pritam P. Karmokar\n‚àó\n, and William J. Beksi\nThe University of Texas at Arlington\nArlington, TX, USA\n{nxt0706,pritam.karmokar}@mavs.uta.edu, william.beksi@uta.edu\n‚àó\nIndicates equal contribution.\nAbstract\nUnderwater imaging is fundamentally challenging due to wavelength-dependent\nlight attenuation, strong scattering from suspended particles,\nturbidity-induced blur, and non-uniform illumination. These effects impair\nstandard cameras and make ground-truth motion nearly impossible to obtain. On\nthe other hand, event cameras offer microsecond resolution and high dynamic\nrange. Nonetheless, progress on investigating event cameras for underwater\nenvironments has been limited due to the lack of datasets that pair realistic\nunderwater optics with accurate optical flow. To address this problem, we\nintroduce the first synthetic underwater benchmark dataset for event-based\noptical flow derived from physically-based ray-traced RGBD sequences. Using a\nmodern video-to-event pipeline applied to rendered underwater videos, we\nproduce realistic event data streams with dense ground-truth flow, depth, and\ncamera motion. Moreover, we benchmark state-of-the-art learning-based and\nmodel-based optical flow prediction methods to understand how underwater light\ntransport affects event formation and motion estimation accuracy. Our dataset\nestablishes a new baseline for future development and evaluation of underwater\nevent-based perception algorithms. The source code and dataset for this project\nare publicly available at\nhttps://robotic-vision-lab.github.io/ueof\n.\n¬©2026 IEEE. Personal use of this material is permitted.\nPermission from IEEE must be obtained for all other uses, in any current or future\nmedia, including reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to servers or\nlists, or reuse of any copyrighted component of this work in other works.\n1\nIntroduction\nUnmanned underwater vehicles (UUVs) require reliable perception systems for\ntasks such as localization\n[\n11\n]\n,\nmapping\n[\n22\n,\n57\n,\n46\n]\n,\nnavigation\n[\n1\n,\n2\n,\n45\n,\n53\n]\n,\ndocking\n[\n9\n,\n34\n,\n59\n]\n,\nexamination\n[\n43\n,\n4\n]\n,\ninspection\n[\n32\n,\n56\n]\n, object pose\nestimation\n[\n42\n,\n44\n]\n, and\nmanipulation\n[\n61\n,\n49\n]\n. Nevertheless, underwater\nvision-based perception remains extremely challenging due to the complex\nlight-medium interactions that cause rapid attenuation, wavelength-dependent\ncolor shifts, low contrast, and strong backscatter from suspended\nparticles\n[\n55\n,\n28\n,\n3\n]\n.\nThese effects degrade the performance of conventional cameras, especially\nduring fast motion or in low-light, turbid, or high-dynamic range (HDR)\nconditions.\nCompared to a frame-based camera, an event camera provides complementary\nadvantages in challenging underwater scenarios. The asynchronous, high temporal\nresolution output of an event camera offers robustness to motion blur and HDR\nlighting, making it well-suited for visual odometry, optical flow estimation,\nand simultaneous localization and mapping (SLAM) in these environments. Yet,\nadvancements in underwater event-based perception have been significantly\nconstrained by a critical lack of data. Current underwater event-based datasets\nrarely include accurate ground truth for motion and optical flow since light\ndetection and ranging and motion-capture systems do not function reliably\nunderwater. As a result, algorithm development and benchmarking have relied\npredominantly on terrestrial datasets or synthetic data.\nFigure 1\n:\nAn illustration of data and ground-truth modalities from the UEOF\ndataset. Our dataset assembles physically-realistic underwater RGB imagery,\nground-truth optical flow, camera ego-velocities, and temporally dense event\nstreams, enabling the benchmarking of multimodal event-based optical flow\nestimation algorithms.\nTo bridge this gap, we introduce a new underwater event-based optical flow\n(UEOF) dataset generated by applying high-quality video-to-event conversion to\nphysically-based ray-traced (PBRT) RGBD sequences, Fig.\n1\n. In\nparticular, we obtain pseudo-event streams that capture the true temporal\ndynamics of the scene, while preserving accurate ground-truth optical flow and\ndepth. This approach circumvents the limitations of both real-world data\ncollection and rasterization-based simulators. It provides the first benchmark\nthat combines physically accurate underwater rendering with temporally dense\nevent data for motion estimation. We summarize our contributions as follows.\n1.\nWe present a synthetic underwater dataset that couples PBRT rendering\nwith high temporal resolution pseudo-events through video-to-event\nconversion, enabling event-based evaluation under realistic underwater\nphysics.\n2.\nWe provide dense ground-truth optical flow, depth, and pose from PBRT\nsimulation, making this the first underwater dataset suitable for\nbenchmarking event-based optical flow algorithms.\n3.\nWe benchmark a wide range of event-based optical flow techniques,\nincluding state-of-the-art event-driven and multimodal contrast maximization\n(CM) approaches, revealing the failure modes and domain-gap challenges unique\nto underwater scenes.\n4.\nWe analyze how underwater optical phenomena (\ne.g\n.\n, attenuation,\nbackscatter, caustics, turbidity) affect event generation and event\nrepresentations, offering insight into the suitability of event cameras for\nunderwater perception.\nOur dataset establishes a foundation to study event-based motion estimation in\nunderwater environments and paves the way to realize robust multimodal\nperception systems for UUV autonomy.\nThe remainder of this paper is organized as follows. In\nSec.\n2\n, we review the landscape of existing underwater\ndatasets and event simulators, identifying the critical gaps in ground truth\navailability that motivates our work.\nSec.\n3\ndetails the proposed\ndata generation pipeline, describing the integration of high-fidelity ray\ntracing with event simulation to produce the UEOF dataset. We present a\ncomprehensive benchmarking of state-of-the-art supervised, unsupervised, and\nmodel-based algorithms along with a critical analysis of how underwater optical\nphenomena impacts estimation performance in\nSec.\n4\n. Finally, we\nconclude in\nSec.\n5\nand discuss future directions\nfor underwater event-based perception.\n2\nRelated Work\n2.1\nReal-World Underwater Event Datasets\nSeveral datasets contain real-world underwater event data, yet none capture\nground-truth optical flow due to the difficulty of obtaining accurate motion\nsupervision. For example, DAVIS-NUIUIED\n[\n5\n]\nfurnishes frame and\nevent data recorded under non-uniform illumination to support underwater image\nenhancement research, without motion or pose estimations.\nFLSea\n[\n47\n]\nprovides stereo RGB images and inertial\nmeasurement unit (IMU) data for underwater SLAM evaluation, sans optical flow\nor event annotations. The Aqua-Eye\n[\n35\n]\ndataset contains\nannotated DAVIS346 images, events, and fused event-frame representations for\ndetecting transparent marine organisms. UTNet\n[\n18\n]\nextends\nAqua-Eye by integrating ResNet50\n[\n20\n]\nwith submanifold sparse\nconvolutions for improved segmentation of transparent underwater objects.\nAlthough both datasets include events, they do not provide depth or optical\nflow ground truth.\nThe AquaticVision\n[\n46\n]\ndataset provides synchronized\nstereo DAVIS346 events, grayscale frames, and IMU data at high frequency, along\nwith 6-DoF motion-capture ground truth. The sequences cover clear water, HDR\nconditions, and varying turbidity. An analysis shows that event representations\nsuch as time surfaces perform well in clear water, but degrade significantly in\nturbid scenes, while event packet and voxel-grid representations offer improved\nrobustness. The dataset is designed for visual SLAM and benchmarking, yet it\nlacks optical flow annotations. EvtSlowTV\n[\n37\n]\ngenerates synthetic events from YouTube videos using video-to-event conversion,\nincluding underwater exploration footage, however it is not designed for\noptical flow or SLAM evaluations.\n(a) Low contrast\n(b) Turbidity\n(c) Caustics, marine snow\n(d) Haloing\nFigure 2\n:\nAn illustration of key underwater visual degradation modes and their\nimpact on event formation. RGB frames (top row) and the corresponding events\n(bottom row) are shown for a variety of challenging underwater scenarios.\n2.2\nSynthetic Underwater Scenes\nRasterization-based simulators and PBRT engines differ drastically in their\nability to model underwater light transport. For instance,\nGazebo\n[\n26\n]\n, UUV Simulator\n[\n39\n]\n, and\nStonefish\n[\n7\n,\n17\n]\nrely on\nshader-based approximations. They cannot simulate volumetric scattering,\nspectral attenuation, or photon transport, leading to unrealistic underwater\nimagery that is unsuitable for photometrically sensitive\ntasks\n[\n31\n,\n60\n]\n. MIMIR-UW\n[\n1\n]\nis\nbuilt atop Unreal Engine\n[\n13\n]\nand\nAirSim\n[\n52\n]\n. It provides synthetic images, segmentation\nmasks, and 6-DoF ground truth for SLAM and object inspection, but it does not\ninclude optical flow. Similarly, OysterSim\n[\n33\n]\nsimulates\noyster reef monitoring with underwater scattering and includes IMU, sonar, and\ncamera pose data, yet it does not have event-based optical flow data.\nRay-traced renderers such as Blender Cycles\n[\n8\n]\nand Unreal Engine\nsimulate photon transport, spectral absorption, and volumetric scattering.\nVAROS\n[\n62\n]\nyields ray-traced underwater RGB imagery with\ndepth and surface normals. LOFUE\n[\n12\n]\nextends this by\nadding\nground-truth optical flow\n, making it one of the most physically\nrealistic underwater datasets with optical-flow labels.\nPHISWID\n[\n23\n]\nsimulates underwater color degradation using\nabsorption, scattering, and marine snow applied to terrestrial RGBD sequences.\nAlthough physically-based renderers produce highly realistic underwater\nimagery, they do not natively generate event data. This motivates the use of\nvideo-to-event conversion software such as the\nv2e\n[\n14\n,\n21\n]\ntoolbox.\n2.3\nEvent Simulation\nExisting simulators such as ESIM\n[\n48\n]\n,\nMDR\n[\n36\n]\n, and BlinkSim\n[\n30\n]\nrely on\nrasterized images and cannot model underwater light physics.\neCARLA-scenes\n[\n40\n]\nadapts CARLA\n[\n10\n]\nfor event data with a focus on autonomous driving.\neStonefish-scenes\n[\n41\n]\nemploys the Stonefish simulator\nto generate synthetic optical flow and events, but it inherits the limitations\nof rasterization (\ni.e\n.\n, no volumetric scattering and simplified light\nattenuation). Blender Cycles is utilized by EREBUS\n[\n27\n]\nfor\nhigh-quality rendering. However, its event data is generated via the v2e\ntoolbox and does not target optical flow. No existing event simulator models\nunderwater light transport, no underwater event dataset provides ground-truth\noptical flow, and no ray-traced underwater dataset includes events, which\nbeckons the need for new data generation pipelines.\nTo the best of our\nknowledge, UEOF is the first dataset to leverage PBRT RGBD data to generate\ntemporally dense pseudo-events for evaluating event-based optical flow\nalgorithms.\n3\nUnderwater Event-Based Optical Flow Dataset\n3.1\nUnderwater Simulation\nFigure 3\n:\nThe UEOF data generation pipeline. First, Blender is utilized to render\nhigh-fidelity RGB frames and extract ground-truth data from the LOFUE and VAROS\ndatasets. Next, the RGB frames undergo event conversion via the v2e toolbox.\nFinally, the resulting event streams are processed by event-based optical flow\nmethods and the predicted flow is evaluated against the ground-truth data using\nstandard endpoint error metrics.\nRealistic underwater rendering requires modeling complex interactions between\nlight and a heterogeneous optical medium. Water contains dissolved organic\nmatter, temperature gradients, salinity variations, air bubbles, suspended\nparticles, and marine snow, all of which alter absorption and scattering in\nspace and time. Consequently, underwater image formation is nonlinear,\nwavelength-dependent, and strongly range-dependent, making it difficult to\nfaithfully simulate. Underwater light transport involves wavelength-dependent\nattenuation, and both forward and back scattering governed by complex phase\nfunctions\n[\n3\n]\n. Accurate simulation requires solving\nthe radiative transfer equation, typically via Monte Carlo photon simulation,\nwhich is computationally expensive and sensitive to optical\nparameters\n[\n51\n]\n.\nAs highlighted in\nFig.\n2\n, optical properties\nvary with depth, water type (coastal vs.¬†oceanic), turbidity, and environmental\nconditions\n[\n50\n]\n. Bubbles, turbulence, flickering\ncaustics, and marine snow introduce temporal inconsistencies that are difficult\nto capture using static rendering pipelines\n[\n58\n]\n. Natural\nlight attenuates rapidly with depth, producing low-light blue-green-shifted\nimagery\n[\n38\n]\n. Artificial illumination introduces strong\nbackscatter, halos, and veiling glare whose appearance depends on the geometry\nof the light sources and the particle field\n[\n23\n]\n.\nWavelength-dependent attenuation causes color shifts and contrast loss that\ncannot be captured by simple haze-style degradation\nmodels\n[\n28\n]\n. For many underwater scenes, the true\nreflectance or ‚Äúwater-free‚Äù reference appearance is unknown, complicating the\nvalidation of synthetic renderings\n[\n62\n]\n.\nReal-world underwater event datasets with dense ground-truth optical flow are\nnonexistent. Moreover, they rarely include accurate optical measurements,\ngeometry, or lighting conditions. This makes it difficult to parameterize\nsimulators and leads to significant domain gaps between synthetic and real\nimagery\n[\n29\n]\n. Furthermore, existing synthetic event\nsimulators are built on rasterization pipelines that do not model volumetric\nlight transport or scattering, limiting their applicability in the underwater\ndomain. These fundamental challenges limit the fidelity of underwater\nrendering engines, an issue that becomes even more restrictive when simulating\nevent data or generating training labels such as optical flow. We address these\nissues via a high-fidelity pseudo-event dataset. Concretely, the UEOF dataset\nprovides the following: (i) photometrically realistic RGB imagery rendered\nusing Blender Cycles, (ii) asynchronous event streams emulated with the v2e\ntoolbox, (iii) dense per-pixel optical flow ground truth, and (iv) camera\nvelocities.\n3.2\nSimulation Sources\nUEOF was built from two underwater ray-traced datasets that complement one\nanother in scene complexity, motion patterns, and rendering realism. The first\ndataset, VAROS, simulates UUV operations across infrastructure inspection and\nseafloor exploration scenarios. We divide the VAROS data into five distinct\nscenes where each one contains the following: (i) lossless RGB images, (ii)\nsurface normal vector images, (iii) metric depth maps, (iv) accurate 6-DoF\nground-truth poses, (v) synchronized IMU data, and (vi) depth-gauge data. The\nscenes comprise pipes, man-made structures, and the natural seabed with\nphysically correct illumination and scattering. Although optical flow is not\nprovided, all dynamic motion arises purely from camera ego-motion (\ni.e\n.\n, no\nindependently moving objects), enabling perfect geometric reconstruction.\nThe second dataset, LOFUE, features complex underwater scenes with dynamic\nobjects (\ne.g\n.\n, fish, vegetation, floating debris,\netc\n.\n), varied water turbidity,\nand both static and moving cameras. Specifically,\nscene1\nand\nscene5\nfeature a static camera, while\nscene2\n,\nscene3\n, and\nscene4\npresent a moving camera simulating\nrealistic vehicle motion. All imagery is rendered using Blender Cycles with\nphysically-based light propagation, including volumetric scattering and\nspectral attenuation. LOFUE provides RGB frames and dense optical flow ground\ntruth using the add-on VisionBlender\n[\n6\n]\n. Together\nVAROS and LOFUE offer complementary strengths. LOFUE includes dynamic-object\nflow, while VAROS yields precise geometric and inertial measurements. Both\ndatasets supply physically-grounded underwater imagery.\n3.3\nEvent Data Generation\nWe generated asynchronous event streams from the rendered RGB sequences using\nthe v2e toolbox, which models the behavior of a dynamic vision sensor (DVS).\nStandard v2e parameters are designed for terrestrial scenes and require\nadaptation for underwater imagery, where contrast is heavily degraded due to\nscattering and spectral attenuation. For each source sequence, the raw frames\nwere first converted to video at 30‚ÄâFPS and 10‚ÄâFPS from the LOFUE and VAROS\ndatasets, respectively. Then, the v2e software was used to perform slow-motion\ninterpolation such that no pixel translates more than 1‚Äâpx between adjacent\nframes, ensuring realistic event triggering under fast camera motion or\nlow-light visibility.\nTo increase DVS sensitivity in low-texture or heavily attenuated regions (\ne.g\n.\n,\nsandy seafloors), we lowered the positive/negative thresholds from the default\nvalues. This improves the event density in regions that would otherwise fail to\nproduce measurable illumination changes. Nonetheless, lower thresholds,\nŒ∏\nnominal\n\\theta_{\\text{nominal}}\n, risk false triggering in darker areas. We countered\nthis by reducing the threshold variation,\nœÉ\nŒ∏\n\\sigma_{\\theta}\n, effectively\nsuppressing fixed-pattern noise without blurring genuine edges. Events are\nexported via the HDF5 file structure as a sequence of\n(\nt\n,\nx\n,\ny\n,\np\n)\n(t,x,y,p)\ntuples,\nwhere\nt\nt\nis the timestamp,\n(\nx\n,\ny\n)\n(x,y)\nis the pixel location, and\np\np\nis the\npolarity of the fired event. An overview of the UEOF data generation pipeline\nis shown in\nFig.\n3\n.\n3.4\nGround-Truth Data Generation\nModalities\nGround Truth\nDataset\nSensor / Sim.\nRes (\nW\n√ó\nH\nW\\times H\n)\nDuration\nRGB\nEvents\nFlow\nVel\nReal-World Datasets\nAquaticVision\n[\n46\n]\nDAVIS346\n346\n√ó\n260\n346\\times 260\n13m06s\n-\n‚úì\n-\n‚úì\nOsloMet\n[\n49\n]\nDAVIS346\n346\n√ó\n260\n346\\times 260\n‚Äì\n-\n‚úì\n-\n‚úì\nAqua-eye\n[\n35\n]\nDAVIS346\n346\n√ó\n260\n346\\times 260\n‚Äì\n‚úì\n‚úì\n-\n-\nDAVIS-NUIUIED\n[\n5\n]\nDAVIS346\n346\n√ó\n260\n346\\times 260\n‚Äì\n‚úì\n‚úì\n-\n-\nSynthetic and Converted Datasets\nEvtSlowTV\n[\n37\n]\nYouTube\n‚Üí\n\\to\nESIM\nVar.\n9000m\n‚úì\n‚úì\n-\n-\neStonefish\n[\n41\n]\nStonefish\n1280\n√ó\n720\n1280\\times 720\n6m\n-\n‚úì\n‚úì\n-\nEREBUS\n[\n27\n]\nBlender\n‚Üí\n\\to\nV2E\n1920\n√ó\n1080\n1920\\times 1080\n‚Äì\n‚úì\n‚úì\n-\n-\nUEOF (Ours)\nBlender\n‚Üí\n\\to\nV2E\n1280\n√ó\n\\times\n720\n‚àó\n12m51s\n‚úì\n‚úì\n‚úì\n‚úì\nTable 1\n:\nA comparison of underwater event-based datasets. While several\ndatasets exist, UEOF is the first to provide ground-truth optical flow and\ncamera ego-velocity derived from PBRT simulation.\nRes\n: resolution.\nFlow\n: optical flow.\nVel\n: camera ego-velocity. ‚Äò‚Äì‚Äô: duration\nnot reported or available.\n‚àó\nIncludes subsets at\n960\n√ó\n540\n960\\times 540\nresolution.\nThe UEOF dataset contains dense per-pixel optical flow maps and 6-DoF camera\nvelocity ground truth for all sequences. The ground-truth optical flow is\nstored in the standard Middlebury format (\n.flo\n), aligned with each RGB\nframe. For the LOFUE sequences, we utilized the precomputed optical flow\nground truth provided by the authors. The VAROS sequences do not provide\noptical flow data. Therefore, we reconstructed dense optical flow via\ngeometric back-projection. Specifically, valid pixels\nùêÆ\nt\n\\mathbf{u}_{t}\nwere\nback-projected into 3D points\nùêè\nt\n=\nœÄ\n‚àí\n1\n‚Äã\n(\nùêÆ\nt\n,\nD\nt\n‚Äã\n(\nùêÆ\nt\n)\n)\n\\mathbf{P}_{t}=\\pi^{-1}(\\mathbf{u}_{t},D_{t}(\\mathbf{u}_{t}))\nusing the provided metric depth map\nD\nt\nD_{t}\nand intrinsic\nparameters. These points were then transformed to the subsequent frame via the\nrigid-body motion\nùêì\nt\n‚Üí\nt\n+\n1\n‚àà\nS\n‚Äã\nE\n‚Äã\n(\n3\n)\n\\mathbf{T}_{t\\rightarrow t+1}\\in SE(3)\nyielding the\ninduced flow\nŒî\n‚Äã\nùêÆ\n=\nœÄ\n‚Äã\n(\nùêì\nt\n‚Üí\nt\n+\n1\n‚Äã\nùêè\nt\n)\n‚àí\nùêÆ\nt\n\\Delta\\mathbf{u}=\\pi(\\mathbf{T}_{t\\rightarrow t+1}\\mathbf{P}_{t})-\\mathbf{u}_{t}\n. We also applied a geometric occlusion check by\ncomparing transformed points projected onto the target image plane, and their\ncomputed depth\nz\nt\n+\n1\nz_{t+1}\nagainst the target depth map\nD\nt\n+\n1\nD_{t+1}\n.\nCorrespondences were considered valid only if they fell within image bounds,\nsatisfied the near-plane constraint (\nz\nt\n+\n1\n>\n0.1\n‚Äã\nm\nz_{t+1}>0.1\\,\\text{m}\n), and passed the\nvisibility test (\nz\nt\n+\n1\n‚â§\nD\nt\n+\n1\n+\nœµ\nz_{t+1}\\leq D_{t+1}+\\epsilon\n). We set the tolerance\nœµ\n=\n0.01\n‚Äã\nm\n\\epsilon=0.01\\,\\text{m}\n. This filters out occluded regions where the\nprojected point lies behind the visible surface.\nThe 6-DoF linear (\nùêØ\n\\mathbf{v}\n) and angular (\nùùé\n\\boldsymbol{\\omega}\n)\nground-truth camera ego-velocities are provided for all the sequences in the\nUEOF dataset. These measurements are stored and compressed as\n.npz\narchives containing three primary keys:\nlin_vel\n,\nang_vel\n,\nand\ntimestamps\nin microseconds. For the LOFUE sequences, we utilized\nthe Blender Python API to directly extract the camera‚Äôs instantaneous\nrigid-body velocity vectors in the camera coordinate frame at each render step\n(30‚ÄâHz). For the VAROS sequences, we derived the velocities from the provided\nhigh-frequency (200‚ÄâHz) ground-truth camera poses. Then, we computed the\nbody-frame velocities using central finite differences on the position and\nquaternion parameters by converting quaternion derivatives to angular velocity\nvia the standard kinematic relationship\nùùé\n=\n2\n‚Äã\nùê™\n^\n‚àí\n1\n‚äó\nùê™\n^\nÀô\n\\boldsymbol{\\omega}=2\\hat{\\mathbf{q}}^{-1}\\otimes\\dot{\\hat{\\mathbf{q}}}\n, where\n‚äó\n\\otimes\ndenotes\nthe Hamilton product,\nùê™\n^\n\\hat{\\mathbf{q}}\nis the unit quaternion representing\ncamera orientation, and\nùê™\n^\nÀô\n\\dot{\\hat{\\mathbf{q}}}\nis its time derivative.\n3.5\nDataset Statistics\nUsing the proposed simulation pipeline, we generated 12 minutes and 51 seconds\nof data across 13,714 RGB frames. This results in a total of 4.94 billion\nevents across all scenes. In summary, the UEOF dataset exhibits the following\nkey characteristics.\n‚Ä¢\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\nHigh Resolution\n: As indicated in\nTab.\n1\n,\nthe dataset provides event streams in two spatial resolutions:\n960\n√ó\n540\n960\\times 540\nand\n1280\n√ó\n720\n1280\\times 720\n. Notably, the\n1280\n√ó\n720\n1280\\times 720\nresolution\ncoincides with the sensor specifications of the Prophesee EVK4 HD with the\nSony IMX636ES HD sensor, a widely used event camera. This alignment minimizes\nthe simulation-to-reality gap and establishes a realistic benchmark for\nevaluating event-based optical flow estimation methods at standard sensor\nresolutions.\n‚Ä¢\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\nTemporally Dense\n: The LOFUE scenes operate at 30‚ÄâFPS for RGB\nframes, with synchronized ground-truth velocities and optical flow evaluation\nprovided at 30‚ÄâHz. The VAROS scenes run at 10‚ÄâFPS for RGB frames\n(evaluations at 10‚ÄâHz). However, the camera ego-velocities are derived from\nhigh-precision logs recorded at 200‚ÄâHz. In total, the dataset provides\n13,704 evaluation intervals.\n‚Ä¢\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\nComprehensive Motion\n: As displayed in\nFig.\n4\n, the dataset exhibits a high dynamic range\nof motion, with a mean flow magnitude of\n6.1\n6.1\npx and a median of\n3.6\n3.6\npx.\nThe motion distribution is heavy-tailed. While a majority of pixels undergo\nmoderate displacement (with a 95th percentile of\n21.20\n21.20\npx), the dataset\nalso includes significant high-magnitude flow upwards of\n80\n80\npx.\nMethods\nscene1\nscene2\nscene3\nscene4\nscene5\nAEE (\n‚Üì\n\\downarrow\n)\nAEE (\n‚Üì\n\\downarrow\n)\nAEE (\n‚Üì\n\\downarrow\n)\nAEE (\n‚Üì\n\\downarrow\n)\nAEE (\n‚Üì\n\\downarrow\n)\nLB\nE-RAFT\n[\n16\n]\n8.67\n9.05\n10.52\n8.64\n16.32\nMotionPriorCMax\n[\n19\n]\n2.61\n1.54\n1.92\n1.49\n2.36\nMB\nEINCM\n[\n25\n]\n4.52\n2.36\n2.06\n1.01\n4.12\nMultiCM\n[\n54\n]\n5.10\n1.37\n2.42\n1.37\n2.27\nOPCM\n[\n24\n]\n‚Äì\n1.62\n2.14\n1.93\n‚Äì\nTable 2\n:\nThe average endpoint error (AEE) in pixels on the UEOF dataset\n(shallow-water environment) across five scenes (\nd\n‚Äã\nt\n=\n1\ndt=1\n).\nBold\nindicates best,\nunderline\nindicates second-best.\nMethods\nscene1\nscene2\nscene3\nscene4\nscene5\nAEE (\n‚Üì\n\\downarrow\n)\nAEE (\n‚Üì\n\\downarrow\n)\nAEE (\n‚Üì\n\\downarrow\n)\nAEE (\n‚Üì\n\\downarrow\n)\nAEE (\n‚Üì\n\\downarrow\n)\nLB\nE-RAFT\n[\n16\n]\n5.82\n10.11\n4.88\n7.64\n3.61\nMotionPriorCMax\n[\n19\n]\n6.93\n5.61\n6.24\n7.39\n5.25\nMB\nEINCM\n[\n25\n]\n3.94\n2.58\n3.37\n2.36\n2.56\nMultiCM\n[\n54\n]\n4.44\n2.39\n3.32\n2.05\n2.35\nTable 3\n:\nThe average end-point error (AEE) in pixels on the UEOF dataset\n(deep-water environment) across five scenes at\nd\n‚Äã\nt\n=\n1\ndt=1\n.\nBold\nindicates\nbest,\nunderline\nindicates second-best.\n(a)\nPixel displacement\n(b)\nMagnitude distribution\nFigure 4\n:\nOptical flow statistics for the UEOF dataset: (a) the joint\ndistribution of\n(\nu\n,\nv\n)\n(u,v)\npixel displacements highlight dense coverage of motion\ndirections; (b) the semi-log histogram shows a heavy-tailed distribution with a\nmean of 6.1‚Äâpx.\nMethods\nscene1\nscene2\nscene3\nA1PE (\n‚Üì\n\\downarrow\n)\nA2PE (\n‚Üì\n\\downarrow\n)\nA3PE (\n‚Üì\n\\downarrow\n)\nA5PE (\n‚Üì\n\\downarrow\n)\nA10PE (\n‚Üì\n\\downarrow\n)\nA20PE (\n‚Üì\n\\downarrow\n)\nA1PE (\n‚Üì\n\\downarrow\n)\nA2PE (\n‚Üì\n\\downarrow\n)\nA3PE (\n‚Üì\n\\downarrow\n)\nA5PE (\n‚Üì\n\\downarrow\n)\nA10PE (\n‚Üì\n\\downarrow\n)\nA20PE (\n‚Üì\n\\downarrow\n)\nA1PE (\n‚Üì\n\\downarrow\n)\nA2PE (\n‚Üì\n\\downarrow\n)\nA3PE (\n‚Üì\n\\downarrow\n)\nLB\nE-RAFT\n[\n16\n]\n59.24\n47.72\n42.30\n36.10\n26.52\n9.43\n97.9\n92.70\n85.10\n66.80\n23.50\n4.00\n90.64\n72.78\n57.19\nMotionPriorCMax\n[\n19\n]\n83.13\n30.53\n21.38\n12.62\n3.61\n1.20\n56.16\n18.28\n8.10\n3.25\n1.06\n0.17\n72.10\n24.20\n11.53\nMB\nEINCM\n[\n25\n]\n69.50\n63.22\n59.93\n50.28\n5.57\n0.01\n44.34\n26.67\n20.40\n13.66\n8.42\n7.01\n27.41\n18.74\n14.88\nMultiCM\n[\n54\n]\n75.11\n73.59\n71.19\n61.17\n5.54\n<\n<\n0.01\n41.96\n12.66\n6.91\n3.97\n1.33\n0.31\n59.56\n35.00\n23.31\nOPCM\n[\n24\n]\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n58.47\n25.14\n11.17\n3.33\n0.60\n<\n<\n0.01\n72.80\n33.45\n19.66\nscene4\nscene5\nscene3\n(contd.)\nA1PE (\n‚Üì\n\\downarrow\n)\nA2PE (\n‚Üì\n\\downarrow\n)\nA3PE (\n‚Üì\n\\downarrow\n)\nA5PE (\n‚Üì\n\\downarrow\n)\nA10PE (\n‚Üì\n\\downarrow\n)\nA20PE (\n‚Üì\n\\downarrow\n)\nA1PE (\n‚Üì\n\\downarrow\n)\nA2PE (\n‚Üì\n\\downarrow\n)\nA3PE (\n‚Üì\n\\downarrow\n)\nA5PE (\n‚Üì\n\\downarrow\n)\nA10PE (\n‚Üì\n\\downarrow\n)\nA20PE (\n‚Üì\n\\downarrow\n)\nA5PE (\n‚Üì\n\\downarrow\n)\nA10PE (\n‚Üì\n\\downarrow\n)\nA20PE (\n‚Üì\n\\downarrow\n)\nLB\nE-RAFT\n[\n16\n]\n96.26\n91.34\n85.62\n68.21\n19.78\n4.16\n43.85\n36.71\n33.70\n29.71\n23.52\n20.07\n42.11\n20.20\n10.23\nMotionPriorCMax\n[\n19\n]\n65.52\n15.67\n5.13\n2.25\n0.37\n0.02\n83.09\n20.60\n13.76\n0.98\n0.35\n0.24\n5.19\n1.47\n0.25\nMB\nEINCM\n[\n25\n]\n18.88\n8.92\n6.22\n3.79\n1.10\n0.30\n75.61\n66.22\n58.84\n35.92\n4.11\n0.88\n10.60\n6.20\n1.60\nMultiCM\n[\n54\n]\n41.39\n18.27\n9.23\n3.80\n0.58\n0.20\n69.33\n40.26\n23.54\n10.00\n1.50\n0.08\n12.52\n3.42\n0.54\nOPCM\n[\n24\n]\n80.46\n37.45\n12.75\n2.41\n0.25\n0.05\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n8.29\n1.17\n0.05\nTable 4\n:\nThe N-pixel error rates on the UEOF dataset (shallow-water\nenvironment) across five scenes at\nd\n‚Äã\nt\n=\n1\ndt=1\n.\nBold\nindicates best,\nunderline\nindicates second-best.\nMethods\nscene1\nscene2\nscene3\nA1PE (\n‚Üì\n\\downarrow\n)\nA2PE (\n‚Üì\n\\downarrow\n)\nA3PE (\n‚Üì\n\\downarrow\n)\nA5PE (\n‚Üì\n\\downarrow\n)\nA10PE (\n‚Üì\n\\downarrow\n)\nA20PE (\n‚Üì\n\\downarrow\n)\nA1PE (\n‚Üì\n\\downarrow\n)\nA2PE (\n‚Üì\n\\downarrow\n)\nA3PE (\n‚Üì\n\\downarrow\n)\nA5PE (\n‚Üì\n\\downarrow\n)\nA10PE (\n‚Üì\n\\downarrow\n)\nA20PE (\n‚Üì\n\\downarrow\n)\nA1PE (\n‚Üì\n\\downarrow\n)\nA2PE (\n‚Üì\n\\downarrow\n)\nA3PE (\n‚Üì\n\\downarrow\n)\nLB\nE-RAFT\n[\n16\n]\n72.43\n51.13\n38.96\n23.72\n8.63\n4.13\n98.37\n94.41\n89.66\n77.83\n42.58\n5.92\n59.61\n41.97\n31.88\nMotionPriorCMax\n[\n19\n]\n92.70\n81.71\n69.04\n47.35\n21.45\n5.65\n84.83\n70.28\n59.20\n42.94\n16.66\n2.20\n85.97\n70.72\n59.64\nMB\nEINCM\n[\n25\n]\n60.92\n47.67\n38.76\n25.58\n10.44\n2.95\n41.39\n30.39\n24.40\n16.72\n6.75\n1.07\n50.15\n37.40\n30.55\nMultiCM\n[\n54\n]\n60.38\n44.94\n36.86\n27.62\n13.81\n4.34\n47.33\n33.10\n24.67\n14.48\n4.52\n0.60\n48.66\n36.03\n29.19\nscene4\nscene5\nscene3\n(contd.)\nA1PE (\n‚Üì\n\\downarrow\n)\nA2PE (\n‚Üì\n\\downarrow\n)\nA3PE (\n‚Üì\n\\downarrow\n)\nA5PE (\n‚Üì\n\\downarrow\n)\nA10PE (\n‚Üì\n\\downarrow\n)\nA20PE (\n‚Üì\n\\downarrow\n)\nA1PE (\n‚Üì\n\\downarrow\n)\nA2PE (\n‚Üì\n\\downarrow\n)\nA3PE (\n‚Üì\n\\downarrow\n)\nA5PE (\n‚Üì\n\\downarrow\n)\nA10PE (\n‚Üì\n\\downarrow\n)\nA20PE (\n‚Üì\n\\downarrow\n)\nA5PE (\n‚Üì\n\\downarrow\n)\nA10PE (\n‚Üì\n\\downarrow\n)\nA20PE (\n‚Üì\n\\downarrow\n)\nLB\nE-RAFT\n[\n16\n]\n69.48\n53.99\n47.92\n40.25\n16.08\n4.06\n65.11\n45.71\n33.23\n20.13\n7.09\n1.34\n19.44\n7.96\n3.13\nMotionPriorCMax\n[\n19\n]\n87.87\n78.54\n72.07\n60.35\n26.36\n3.35\n85.94\n71.70\n59.70\n37.66\n13.29\n2.11\n42.71\n18.86\n4.94\nMB\nEINCM\n[\n25\n]\n37.46\n26.61\n20.38\n12.54\n4.97\n1.57\n47.20\n31.50\n23.80\n15.90\n6.45\n0.95\n23.10\n12.85\n3.25\nMultiCM\n[\n54\n]\n39.79\n25.98\n18.83\n11.68\n4.88\n0.54\n45.87\n30.02\n22.44\n14.06\n5.28\n0.51\n21.29\n10.73\n2.12\nTable 5\n:\nThe N-pixel error rates on the UEOF dataset (deep-water environment)\nacross five scenes at\nd\n‚Äã\nt\n=\n1\ndt=1\n.\nBold\nindicates best,\nunderline\nindicates second-best.\n4\nEvaluation\n4.1\nExperimental Setup\nWe conducted experiments on all scenes of the UEOF dataset. The event-based\noptical flow metrics include the average endpoint error (AEE) as well as\nA\n‚Äã\nN\n‚Äã\nPE\n\\text{A}N\\text{PE}\n, which represents the percentage of pixels with an\nendpoint error lower than\nN\nN\npixels for\nN\n‚àà\n{\n1\n,\n2\n,\n3\n,\n5\n,\n10\n,\n20\n}\nN\\in\\{1,2,3,5,10,20\\}\n. To\nensure a comprehensive evaluation, we benchmarked the UEOF dataset against\nrepresentative state-of-the-art approaches categorized into two primary\nparadigms: learning-based (LB) and model-based (MB) algorithms. The LB\ntechniques consist of supervised and unsupervised learning. Supervised\ntechniques, like E-RAFT\n[\n16\n]\n, leverage high-quality ground\ntruth for training. Conversely, unsupervised learning algorithms, such as\nMotionPriorCMax\n[\n19\n]\n, circumvent this by relying exclusively\non the raw event data itself. Conversely, the MB methods stray away from neural\nnetworks and instead adopt traditional nonlinear optimization with contrast\nmaximization objectives.\nTo make the UEOF dataset more impactful, we first extended the existing\ndataloaders of each evaluated technique. This involved adapting the algorithm‚Äôs\ninput to correctly handle our specified data formats. In the evaluations of the\nLB approaches, we utilized the checkpoints provided by each publication, which\nwere pretrained on the DSEC\n[\n15\n]\ndataset. The MB methods used\nidentical accumulated event window sizes for each respective scene. Otherwise,\nwe maintained the default configurations and parameters. The configuration\nfiles and modified source code used for the evaluation are available on our\nproject website.\n4.2\nResults\nThe quantitative results for shallow-water and deep-water environments are\npresented in\nTabs.\n2\nand\n4\nand\nTabs.\n3\nand\n5\n, respectively. We note that the MB\napproaches MultiCM\n[\n54\n]\nand EINCM\n[\n25\n]\nachieved the lowest error rates, with EINCM\n[\n25\n]\nreaching\na minimum AEE of 1.01‚Äâpx in\nscene4\n(shallow-water). Conversely, the\nLB model E-RAFT\n[\n16\n]\nexhibited the highest errors, peaking at\n16.32‚Äâpx in\nscene5\n(shallow-water). In the deep-water environment, we\nobserved a universal increase in error magnitudes across all techniques.\nShallow-Water Environment\nai\nscene1\nai\nscene2\nai\nscene3\nai\nscene4\nai\nscene5\nDeep-Water Environment\nai\nscene1\nai\nscene2\nai\nscene3\nai\nscene4\nai\nscene5\n(a) Frames\n(b) Events\n(c) GT Flow\n(d) MultiCM\n[\n54\n]\n(e) MotionPriorCM\n[\n19\n]\nFigure 5\n:\nQualitative event-based optical flow estimation results using the\nUEOF dataset. Columns (a-e) show the frames, events, ground-truth (GT) optical\nflow, and the predicted event-based optical flow using a representative MB\n(MultiCM\n[\n54\n]\n) and LB\n(MotionPriorCMax\n[\n19\n]\n) method. The top five and bottom\nfive rows correspond to the shallow-water and deep-water environment\nsequences, respectively. The optical flow direction and magnitude are encoded\naccording to the color legend shown below.\n4.3\nAnalysis\nOur experiments reveal a significant performance degradation for\nstate-of-the-art event-based optical flow methods. Specifically, the results\nindicate that the limitations are a compounding interaction wherein underwater\nphenomena expose and heighten flaws in current event-based processing\nalgorithms. Supervised approaches such as E-RAFT\n[\n16\n]\nexhibited high error rates when relying on pretrained weights from the DSEC\ndataset. We hypothesize that this is due to the fact that the terrestrial\ndatasets are dominated by high-frequency rigid edges (\ne.g\n.\n, buildings, vehicles,\netc\n.\n). Conversely, underwater scenes, particularly in turbid waters, are\ndefined by soft, low-frequency textures due to volumetric scattering. As a\nresult, the supervised networks suffered from severe feature distribution\nshifts.\nWe also noticed that refractive caustics created high-contrast light patterns\nthat traversed the seafloor independent of physical geometry. Consequently,\ncontrast maximization approaches that rely on a brightness constancy assumption\nrevealed a limitation in estimating optical flow from events that are not\ncaused by motion. Since caustics generate dense, high-frequency event streams,\nwe observed that this can cause CM objectives to falsely align and produce\nincorrect motion estimates. As shown in\nFig.\n5\n(shallow-water scenes 1, 2, 3, and 5), a performance gap appears in dynamic\nsequences where independent objects (\ne.g\n.\n, fish) traverse in different\ndirections. Methods such as MultiCM\n[\n54\n]\nfailed to resolve\nthese conflicting motions, while MotionPriorCMax\n[\n19\n]\nmaintained robustness.\nIn addition, the error rates across all models evaluated on the deep-water\nenvironment are notably higher when compared to the shallow-water environment.\nWe attribute this to the many adverse effects of the deep-water scenes, such as\nhaloing from UUV lighting, poor contrast due to attenuation, and a low-texture\nseafloor. In comparison, the shallow-water scenes contain strong and consistent\nsunlight, which can create textures on the seabed. This provides more, albeit\nnoisy, features for tracking. Finally, the evaluation results demonstrate that\nmultimodal approaches, specifically those integrating velocity data like\nOPCM\n[\n24\n]\n, vary in improvement on dynamic scenes over\ntheir non-multimodal counterparts.\n5\nConclusion and Future Work\nIn this paper, we introduced UEOF, the first synthetic underwater dataset to\nleverage PBRT simulation with temporally dense event streams, accurate\nground-truth optical flow, and ego-velocities. Our comprehensive benchmarking\nresults reveal that state-of-the-art approaches, whether LB or MB, struggle to\ngeneralize from the terrestrial to the underwater domain. Thus, the UEOF\ndataset establishes a necessary baseline for advanced research, fostering the\ndevelopment of robust algorithms capable of handling the large domain gaps\npresent in underwater environments. We anticipate that this benchmark dataset\nwill accelerate progress towards reliable multimodal event-based perception for\nthe next generation of UUVs. Our future work includes extending the UEOF\ndataset with additional data and ground-truth modalities, richer environmental\ndiversity, and physically-grounded event noise models.\nAcknowledgments\nThis material is based upon work supported by the Office of Naval Research\nunder award number N000142512349.\nReferences\n[1]\nO. √Ålvarez-Tu√±√≥n, H. Kanner, L. R. Marnet, H. X. Pham, J. le Fevre Sejersen, Y. Brodskiy, and E. Kayacan\n(2023)\nMIMIR-uw: a multipurpose synthetic dataset for underwater navigation and inspection\n.\nIn\nProceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp.¬†6141‚Äì6148\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n,\n¬ß2.2\n.\n[2]\nA. Amer, O. √Ålvarez-Tu√±√≥n, H. ƒ∞. Uƒüurlu, J. L. F. Sejersen, Y. Brodskiy, and E. Kayacan\n(2023)\nUNav-sim: a visually realistic underwater robotics simulator and synthetic data-generation framework\n.\nIn\nProceedings of the International Conference on Advanced Robotics\n,\npp.¬†570‚Äì576\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[3]\nL. A. Barbosa and A. L. Apolinario Jr\n(2025)\nFrom physically based to generative models: a survey on underwater image synthesis techniques\n.\nJournal of Imaging\n11\n(\n5\n),\npp.¬†161\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n,\n¬ß3.1\n.\n[4]\nJ. Betancourt, W. Coral, and J. Colorado\n(2020)\nAn integrated rov solution for underwater net-cage inspection in fish farms using computer vision\n.\nSN Applied Sciences\n2\n(\n12\n),\npp.¬†1946\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[5]\nX. Bi, P. Wang, T. Wu, F. Zha, and P. Xu\n(2022)\nNon-uniform illumination underwater image enhancement via events and frame fusion\n.\nApplied Optics\n61\n(\n29\n),\npp.¬†8826‚Äì8832\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.1\n,\nTable 1\n.\n[6]\nJ. Cartucho, S. Tukra, Y. Li, D. S. Elson, and S. Giannarou\n(2021)\nVisionBlender: a tool to efficiently generate computer vision datasets for robotic surgery\n.\nComputer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization\n9\n(\n4\n),\npp.¬†331‚Äì338\n.\nExternal Links:\nDocument\nCited by:\n¬ß3.2\n.\n[7]\nP. Cie≈õlak\n(2019)\nStonefish: an advanced open-source simulation tool designed for marine robotics, with a ros interface\n.\nIn\nProceedings of OCEANS\n,\npp.¬†1‚Äì6\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.2\n.\n[8]\nB. O. Community\n(2025)\nBlender - a 3d modeling and rendering package\n.\nExternal Links:\nLink\nCited by:\n¬ß2.2\n.\n[9]\nS. Cowen, S. Briest, and J. Dombrowski\n(1997)\nUnderwater docking of autonomous undersea vehicles using optical terminal guidance\n.\nIn\nProceedings of OCEANS\n,\nVol.\n2\n,\npp.¬†1143‚Äì1147\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[10]\nA. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun\n(2017)\nCARLA: an open urban driving simulator\n.\nIn\nProceedings of the Conference on Robot Learning\n,\npp.¬†1‚Äì16\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.3\n.\n[11]\nJ. Fan, X. Liu, Y. Ou, P. Zhang, C. Zhou, and Z. Hou\n(2025)\nUnderwater robot self-localization method using tightly coupled events, images, inertial, and acoustic fusion\n.\nIEEE Transactions on Industrial Electronics\n72\n(\n5\n),\npp.¬†5126‚Äì5135\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[12]\nA. Ferone, M. Lazzaro, V. M. Scarrica, A. Ciaramella, and A. Staiano\n(2023)\nA synthetic dataset for learning optical flow in underwater environment\n.\nIn\nApplications of Artificial Intelligence and Neural Systems to Data Science\n,\npp.¬†147‚Äì156\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.2\n.\n[13]\nE. Games\n(2025)\nUnreal engine\n.\nExternal Links:\nLink\nCited by:\n¬ß2.2\n.\n[14]\nD. Gehrig, M. Gehrig, J. Hidalgo-Carri√≥, and D. Scaramuzza\n(2019)\nVideo to events: bringing modern computer vision closer to event cameras\n.\nArXiv\nabs/1912.03095\n.\nExternal Links:\nLink\nCited by:\n¬ß2.2\n.\n[15]\nM. Gehrig, W. Aarents, D. Gehrig, and D. Scaramuzza\n(2021)\nDsec: a stereo event camera dataset for driving scenarios\n.\nIEEE Robotics and Automation Letters\n6\n(\n3\n),\npp.¬†4947‚Äì4954\n.\nCited by:\n¬ß4.1\n.\n[16]\nM. Gehrig, M. Millh√§usler, D. Gehrig, and D. Scaramuzza\n(2021)\nE-raft: dense optical flow from event cameras\n.\nIn\nProceedings of the International Conference on 3D Vision\n,\npp.¬†197‚Äì206\n.\nExternal Links:\nDocument\nCited by:\nTable 2\n,\nTable 3\n,\nTable 4\n,\nTable 4\n,\nTable 5\n,\nTable 5\n,\n¬ß4.1\n,\n¬ß4.2\n,\n¬ß4.3\n.\n[17]\nM. Grimaldi, P. Cie≈õlak, E. Ochoa, V. Bharti, H. Rajani, I. Carlucho, M. Koskinopoulou, Y. R. Petillot, and N. Gracias\n(2025)\nStonefish: supporting machine learning research in marine robotics\n.\nIn\nProceedings of the IEEE International Conference on Robotics and Automation\n,\npp.¬†1‚Äì7\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.2\n.\n[18]\nF. Guo, P. Ren, and C. Luo\n(2025)\nUTNet: event-rgb multimodal fusion model for underwater transparent organism detection\n.\nIntelligent Marine Technology and Systems\n3\n(\n1\n),\npp.¬†18\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.1\n.\n[19]\nF. Hamann, Z. Wang, I. Asmanis, K. Chaney, G. Gallego, and K. Daniilidis\n(2024)\nMotion-prior contrast maximization for dense continuous-time motion estimation\n.\nIn\nProceedings of the European Conference on Computer Vision\n,\npp.¬†18‚Äì37\n.\nExternal Links:\nDocument\nCited by:\nTable 2\n,\nTable 3\n,\nTable 4\n,\nTable 4\n,\nTable 5\n,\nTable 5\n,\nFigure 5\n,\nFigure 5\n,\nFigure 5\n,\n¬ß4.1\n,\n¬ß4.3\n.\n[20]\nK. He, X. Zhang, S. Ren, and J. Sun\n(2016)\nDeep residual learning for image recognition\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.¬†770‚Äì778\n.\nCited by:\n¬ß2.1\n.\n[21]\nY. Hu, S. Liu, and T. Delbruck\n(2021)\nV2e: from video frames to realistic dvs events\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.¬†1312‚Äì1321\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.2\n.\n[22]\nM. J. Islam, A. Q. Li, Y. A. Girdhar, and I. Rekleitis\n(2024)\nComputer vision applications in underwater robotics and oceanography\n.\nIn\nComputer Vision\n,\npp.¬†173‚Äì204\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[23]\nR. Kaneko, T. Ueda, H. Higashi, and Y. Tanaka\n(2024)\nPHISWID: physics-inspired underwater image dataset synthesized from rgb-d images\n.\narXiv preprint arXiv:2404.03998\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.2\n,\n¬ß3.1\n.\n[24]\nP. P. Karmokar and W. J. Beksi\n(2025)\nInertia-informed orientation priors for event-based optical flow estimation\n.\narXiv preprint arXiv:2511.12961\n.\nCited by:\nTable 2\n,\nTable 4\n,\nTable 4\n,\n¬ß4.3\n.\n[25]\nP. P. Karmokar, Q. H. Nguyen, and W. J. Beksi\n(2025)\nSecrets of edge-informed contrast maximization for event-based vision\n.\nIn\nProceedings of the Winter Conference on Applications of Computer Vision\n,\npp.¬†630‚Äì639\n.\nExternal Links:\nDocument\nCited by:\nTable 2\n,\nTable 3\n,\nTable 4\n,\nTable 4\n,\nTable 5\n,\nTable 5\n,\n¬ß4.2\n.\n[26]\nN. Koenig and A. Howard\n(2004)\nDesign and use paradigms for gazebo, an open-source multi-robot simulator\n.\nIn\nProceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp.¬†2149‚Äì2154\n.\nCited by:\n¬ß2.2\n.\n[27]\nH. Kyatham, A. Suresh, A. Palnitkar, and Y. Aloimonos\n(2025)\nEREBUS: end-to-end robust event based underwater simulation\n.\nIn\nProceedings of the IEEE International Conference on Robotics and Automation AQUA2SIM Workshop\n,\nCited by:\n¬ß2.3\n,\nTable 1\n.\n[28]\nS. Li, Z. Zhang, Q. Zhang, H. Yao, X. Li, J. Mi, and H. Wang\n(2024)\nBreakthrough underwater physical environment limitations on optical information representations: an overview and suggestions\n.\nJournal of Marine Science and Engineering\n12\n(\n7\n),\npp.¬†1055\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n,\n¬ß3.1\n.\n[29]\nS. Li, T. Liu, Q. Jiang, Y. Li, J. Guo, L. Jiao, Y. Guo, and Z. Ni\n(2025)\nRealistic simulation of underwater scene for image enhancement\n.\nIEEE Transactions on Geoscience and Remote Sensing\n63\n,\npp.¬†1‚Äì14\n.\nExternal Links:\nDocument\nCited by:\n¬ß3.1\n.\n[30]\nY. Li, Z. Huang, S. Chen, X. Shi, H. Li, H. Bao, Z. Cui, and G. Zhang\n(2023)\nBlinkflow: a dataset to push the limits of event-based optical flow estimation\n.\nIn\nProceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp.¬†3881‚Äì3888\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.3\n.\n[31]\nY. Liao, M. Shangguan, Z. Yang, Z. Lin, Y. Wang, and S. Li\n(2023)\nGPU-accelerated monte carlo simulation for a single-photon underwater lidar\n.\nRemote Sensing\n15\n(\n21\n),\npp.¬†5245\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.2\n.\n[32]\nP. Liljeb√§ck and R. Mills\n(2017)\nEelume: a flexible and subsea resident imr vehicle\n.\nIn\nProceedings of OCEANS\n,\npp.¬†1‚Äì4\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[33]\nX. Lin, N. Jha, M. Joshi, N. Karapetyan, Y. Aloimonos, and M. Yu\n(2022)\nOysterSim: underwater simulation for enhancing oyster reef monitoring\n.\nIn\nProceedings of OCEANS\n,\npp.¬†1‚Äì6\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.2\n.\n[34]\nS. Liu, M. Ozay, T. Okatani, H. Xu, K. Sun, and Y. Lin\n(2018)\nDetection and pose estimation for short-range vision-based underwater docking\n.\nIEEE Access\n7\n,\npp.¬†2720‚Äì2749\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[35]\nC. Luo, J. Wu, S. Sun, and P. Ren\n(2023)\nTransCODNet: underwater transparently camouflaged object detection via rgb and event frames collaboration\n.\nIEEE Robotics and Automation Letters\n9\n(\n2\n),\npp.¬†1444‚Äì1451\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.1\n,\nTable 1\n.\n[36]\nX. Luo, K. Luo, A. Luo, Z. Wang, P. Tan, and S. Liu\n(2023)\nLearning optical flow from event camera with rendered dataset\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.¬†9847‚Äì9857\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.3\n.\n[37]\nS. L. Macaulay, N. Kaygusuz, and S. Hadfield\n(2025)\nEvtSlowTV - a large and diverse dataset for event-based depth estimation\n.\narXiv preprint arXiv:2511.02953\n.\nCited by:\n¬ß2.1\n,\nTable 1\n.\n[38]\nR. Makam, D. Shankari T M, S. Patil, and S. Sundram\n(2024)\nOceanLens: an adaptive backscatter and edge correction using deep learning model for enhanced underwater imaging\n.\narXiv preprint arXiv:2411.13230\n.\nExternal Links:\nDocument\nCited by:\n¬ß3.1\n.\n[39]\nM. M. M. Manh√£es, S. A. Scherer, M. Voss, L. R. Douat, and T. Rauschenbach\n(2016)\nUUV simulator: a gazebo-based package for underwater intervention and multi-robot simulation\n.\nIn\nProceedings of OCEANS\n,\npp.¬†1‚Äì8\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.2\n.\n[40]\nJ. Mansour, H. Rajani, R. Garcia, and N. Gracias\n(2024)\nECARLA-scenes: a synthetically generated dataset for event-based optical flow prediction\n.\narXiv preprint arXiv:2412.09209\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.3\n.\n[41]\nJ. Mansour, S. Realpe, H. Rajani, M. Grimaldi, R. Garcia, and N. Gracias\n(2025)\nEStonefish-scenes: a synthetically generated dataset for underwater event-based optical flow prediction tasks\n.\narXiv preprint arXiv:2505.13309\n.\nCited by:\n¬ß2.3\n,\nTable 1\n.\n[42]\nA. Mohammed, J. Kvam, J. T Thielemann, K. H. Haugholt, and P. Risholm\n(2021)\n6D pose estimation for subsea intervention in turbid waters\n.\nElectronics\n10\n(\n19\n),\npp.¬†2369\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[43]\nS. Negahdaripour and P. Firoozfam\n(2007)\nAn rov stereovision system for ship-hull inspection\n.\nIEEE Journal of Oceanic Engineering\n31\n(\n3\n),\npp.¬†551‚Äì564\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[44]\nM. C. Nielsen, M. H. Leonhardsen, and I. Schj√∏lberg\n(2019)\nEvaluation of posenet for 6-dof underwater pose estimation\n.\nIn\nProceedings of OCEANS\n,\npp.¬†1‚Äì6\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[45]\nA. Novo, F. Lobon, H. Garcia de Marina, S. Romero, and F. Barranco\n(2024)\nNeuromorphic perception and navigation for mobile robots: a review\n.\nACM Computing Surveys\n56\n(\n10\n).\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[46]\nY. Peng, Y. Hong, Z. Hong, A. P. Chui, and J. Wu\n(2025)\nAquaticVision: benchmarking visual slam in underwater environment with events and frames\n.\narXiv preprint arXiv:2505.03448\n.\nCited by:\n¬ß1\n,\n¬ß2.1\n,\nTable 1\n.\n[47]\nY. Randall and T. Treibitz\n(2023)\nFLSea: underwater visual-inertial and stereo-vision forward-looking datasets\n.\narXiv preprint arXiv:2302.12772\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.1\n.\n[48]\nH. Rebecq, D. Gehrig, and D. Scaramuzza\n(2018)\nEsim: an open event camera simulator\n.\nIn\nProceedings of the Conference on Robot Learning\n,\npp.¬†969‚Äì982\n.\nCited by:\n¬ß2.3\n.\n[49]\nI. B. Saksvik, H. Weydahl, H. Teigland, A. Alcocer, and V. Hassani\n(2023)\nTowards an open-source benchmark for underwater object detection and pose estimation\n.\nIn\nProceedings of the IEEE Conference on Underwater Technology\n,\npp.¬†1‚Äì5\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n,\nTable 1\n.\n[50]\nP. Sch√∂ntag, D. Nakath, J. Fischer, R. R√∂ttgers, and K. K√∂ser\n(2025)\nOptical ocean recipes: creating realistic datasets to facilitate underwater vision research\n.\narXiv preprint arXiv:2509.20171\n.\nExternal Links:\nDocument\nCited by:\n¬ß3.1\n.\n[51]\nA. Sedlazeck and R. Koch\n(2011)\nSimulating deep sea underwater images using physical models for light attenuation, scattering, and refraction\n.\nIn\nProceedings of Vision, Modeling, and Visualization\n,\nExternal Links:\nDocument\nCited by:\n¬ß3.1\n.\n[52]\nS. Shah, D. Dey, C. Lovett, and A. Kapoor\n(2017)\nAirsim: high-fidelity visual and physical simulation for autonomous vehicles\n.\nIn\nProceedings of the International Conference on Field and Service Robotics\n,\npp.¬†621‚Äì635\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.2\n.\n[53]\nC. Sheikder, W. Zhang, X. Chen, F. Li, Y. Liu, Z. Zuo, X. He, and X. Tan\n(2025)\nMarine-inspired multimodal sensor fusion and neuromorphic processing for autonomous navigation in unstructured subaquatic environments\n.\nSensors\n25\n(\n21\n),\npp.¬†6627\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[54]\nS. Shiba, Y. Aoki, and G. Gallego\n(2022)\nSecrets of event-based optical flow\n.\nIn\nProceedings of the European Conference on Computer Vision\n,\npp.¬†628‚Äì645\n.\nExternal Links:\nDocument\nCited by:\nTable 2\n,\nTable 3\n,\nTable 4\n,\nTable 4\n,\nTable 5\n,\nTable 5\n,\nFigure 5\n,\nFigure 5\n,\nFigure 5\n,\n¬ß4.2\n,\n¬ß4.3\n.\n[55]\nA. Steiner\n(2013)\nUnderstanding the basics of underwater lighting\n.\nOcean News & Technology\n19\n(\n4\n),\npp.¬†10‚Äì12\n.\nCited by:\n¬ß1\n.\n[56]\nH. Teigland, V. Hassani, and M. T. M√∏ller\n(2020)\nOperator focused automation of rov operations\n.\nIn\nProceedings of the IEEE/OES Autonomous Underwater Vehicles Symposium\n,\npp.¬†1‚Äì7\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[57]\nX. Wang, X. Fan, Y. Liu, Y. Xin, and P. Shi\n(2025)\nEum-slam: an enhancing underwater monocular visual slam with deep learning-based optical flow estimation\n.\nIEEE Transactions on Instrumentation and Measurement\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[58]\nD. Yang, J. J. Leonard, and Y. Girdhar\n(2025)\nSeaSplat: representing underwater scenes with 3d gaussian splatting and a physically grounded image formation model\n.\nIn\nProceedings of the IEEE International Conference on Robotics and Automation\n,\npp.¬†7632‚Äì7638\n.\nExternal Links:\nDocument\nCited by:\n¬ß3.1\n.\n[59]\nA. M. Yazdani, K. Sammut, O. Yakimenko, and A. Lammas\n(2020)\nA survey of underwater docking guidance systems\n.\nRobotics and Autonomous Systems\n124\n,\npp.¬†103382\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[60]\nP. Yue, X. Wang, S. Xu, and Y. Li\n(2025)\nMonte-carlo based non-line-of-sight underwater wireless optical communication channel modeling and system performance analysis under turbulence\n.\narXiv preprint arXiv:2501.12859\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.2\n.\n[61]\nM. M. Zhang, W. Choi, J. Herman, D. Davis, C. Vogt, M. McCarrin, Y. Vijay, D. Dutia, W. Lew, S. Peters, and B. Bingham\n(2022)\nDAVE aquatic virtual environment: toward a general underwater robotics simulator\n.\nIn\nProceedings of the IEEE/OES Autonomous Underwater Vehicles Symposium\n,\npp.¬†1‚Äì8\n.\nExternal Links:\nDocument\nCited by:\n¬ß1\n.\n[62]\nP. G. O. Zwilgmeyer, M. Yip, A. L. Teigen, R. Mester, and A. Stahl\n(2021)\nThe varos synthetic underwater data set: towards realistic multi-sensor underwater data with ground truth\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.¬†3722‚Äì3730\n.\nExternal Links:\nDocument\nCited by:\n¬ß2.2\n,\n¬ß3.1\n.",
    "preview_text": "Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.\n\nUEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow\nNick Truong\n‚àó\n, Pritam P. Karmokar\n‚àó\n, and William J. Beksi\nThe University of Texas at Arlington\nArlington, TX, USA\n{nxt0706,pritam.karmokar}@mavs.uta.edu, william.beksi@uta.edu\n‚àó\nIndicates equal contribution.\nAbstract\nUnderwater imaging is fundamentally challenging due to wavelength-dependent\nlight attenuation, strong scattering from suspended particles,\nturbidity-induced blur, and non-uniform illumination. These effects impair\nstandard cameras and make ground-truth motion nearly impossible to obtain. On\nthe other hand, event cameras offer microsecond resolution and h",
    "is_relevant": false,
    "relevance_score": 0.0,
    "extracted_keywords": [
        "optical flow",
        "event cameras",
        "underwater imaging",
        "benchmark dataset",
        "synthetic data"
    ],
    "one_line_summary": "ËØ•ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Áî®‰∫éÊ∞¥‰∏ã‰∫ã‰ª∂Áõ∏Êú∫ÂÖâÊµÅ‰º∞ËÆ°ÁöÑÂêàÊàêÂü∫ÂáÜÊï∞ÊçÆÈõÜÔºå‰∏éÂº∫ÂåñÂ≠¶‰π†„ÄÅVLA„ÄÅÊâ©Êï£Ê®°ÂûãÁ≠âÂÖ≥ÈîÆËØçÊó†ÂÖ≥„ÄÇ",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-15T04:10:14Z",
    "created_at": "2026-01-20T17:49:53.245565",
    "updated_at": "2026-01-20T17:49:53.245573",
    "recommend": 0
}