{
    "id": "2601.22517v1",
    "title": "RoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxing",
    "authors": [
        "Kangning Yin",
        "Zhe Cao",
        "Wentao Dong",
        "Weishuai Zeng",
        "Tianyi Zhang",
        "Qiang Zhang",
        "Jingbo Wang",
        "Jiangmiao Pang",
        "Ming Zhou",
        "Weinan Zhang"
    ],
    "abstract": "åœ¨ä»¿äººæœºå™¨äººé¢†åŸŸå®ç°äººç±»æ°´å¹³çš„ç«äº‰æ™ºèƒ½ä¸èº«ä½“æ•æ·æ€§ä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨æ‹³å‡»è¿™ç±»æ¥è§¦å¯†é›†ä¸”é«˜åº¦åŠ¨æ€çš„ä»»åŠ¡ä¸­ã€‚è™½ç„¶å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸ºç­–ç•¥æ€§äº¤äº’æä¾›äº†ç†è®ºæ¡†æ¶ï¼Œä½†å…¶ç›´æ¥åº”ç”¨äºä»¿äººæœºå™¨äººæ§åˆ¶ä»å—é™äºé«˜ç»´æ¥è§¦åŠ¨åŠ›å­¦å’Œç¼ºä¹å¼ºç‰©ç†è¿åŠ¨å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬æå‡ºRoboStrikerâ€”â€”ä¸€ä¸ªåˆ†å±‚ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡å°†é«˜å±‚ç­–ç•¥æ¨ç†ä¸åº•å±‚ç‰©ç†æ‰§è¡Œè§£è€¦ï¼Œå®ç°äº†å®Œå…¨è‡ªä¸»çš„ä»¿äººæœºå™¨äººæ‹³å‡»ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡åŸºäºäººç±»åŠ¨ä½œæ•æ‰æ•°æ®è®­ç»ƒå•æ™ºèƒ½ä½“è¿åŠ¨è¿½è¸ªå™¨ï¼Œå­¦ä¹ å…¨é¢çš„æ‹³å‡»æŠ€èƒ½åº“ã€‚éšåå°†è¿™äº›æŠ€èƒ½æç‚¼ä¸ºç»“æ„åŒ–æ½œåœ¨æµå½¢ï¼Œå¹¶é€šè¿‡å°†é«˜æ–¯å‚æ•°åŒ–åˆ†å¸ƒæŠ•å½±åˆ°å•ä½è¶…çƒé¢è¿›è¡Œæ­£åˆ™åŒ–ã€‚è¿™ç§æ‹“æ‰‘çº¦æŸæœ‰æ•ˆå°†æ¢ç´¢èŒƒå›´é™åˆ¶åœ¨ç‰©ç†åˆç†è¿åŠ¨çš„å­ç©ºé—´å†…ã€‚æœ€åé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºæ½œåœ¨ç©ºé—´ç¥ç»è™šæ‹Ÿè‡ªæˆ‘åšå¼ˆç®—æ³•ï¼Œä½¿ç«äº‰æ™ºèƒ½ä½“é€šè¿‡åœ¨æ½œåœ¨åŠ¨ä½œç©ºé—´è€ŒéåŸå§‹è¿åŠ¨ç©ºé—´ä¸­è¿›è¡Œäº¤äº’æ¥å­¦ä¹ ç«äº‰ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†å¤šæ™ºèƒ½ä½“è®­ç»ƒçš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoboStrikeråœ¨ä»¿çœŸç¯å¢ƒä¸­å±•ç°å‡ºå“è¶Šçš„ç«æŠ€è¡¨ç°ï¼Œå¹¶å®ç°äº†ä»¿çœŸåˆ°ç°å®çš„è¿ç§»ã€‚é¡¹ç›®ç½‘ç«™è¯¦è§RoboStrikerã€‚",
    "url": "https://arxiv.org/abs/2601.22517v1",
    "html_url": "https://arxiv.org/html/2601.22517v1",
    "html_content": "RoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxing\nKangning Yin\nZhe Cao\nWentao Dong\nWeishuai Zeng\nTianyi Zhang\nQiang Zhang\nJingbo Wang\nJiangmiao Pang\nMing Zhou\nWeinan Zhang\nAbstract\nAchieving human-level competitive intelligence and physical agility in humanoid robots remains a major challenge, particularly in contact-rich and highly dynamic tasks such as boxing. While Multi-Agent Reinforcement Learning (MARL) offers a principled framework for strategic interaction, its direct application to humanoid control is hindered by high-dimensional contact dynamics and the absence of strong physical motion priors. We propose RoboStriker, a hierarchical three-stage framework that enables fully autonomous humanoid boxing by decoupling high-level strategic reasoning from low-level physical execution. The framework first learns a comprehensive repertoire of boxing skills by training a single-agent motion tracker on human motion capture data. These skills are subsequently distilled into a structured latent manifold, regularized by projecting the Gaussian-parameterized distribution onto a unit hypersphere. This topological constraint effectively confines exploration to the subspace of physically plausible motions. In the final stage, we introduce Latent-Space Neural Fictitious Self-Play (LS-NFSP), where competing agents learn competitive tactics by interacting within the latent action space rather than the raw motor space, significantly stabilizing multi-agent training. Experimental results demonstrate that RoboStriker achieves superior competitive performance in simulation and exhibits sim-to-real transfer. Our website is available at\nRoboStriker\n.\nMachine Learning, ICML\nFigure 1\n:\nReal-world clips of humanoid boxing using RoboStriker, showcasing agile, contact-rich punches and defenses under physical constraints.\n1\nIntroduction\nHumanoid robots serve as a critical frontier for embodied intelligence, with recent breakthroughs demonstrating exceptional capabilities in locomotion and agile maneuvers\n(Liao\net al.\n,\n2025\n; Yin\net al.\n,\n2025\n; Zeng\net al.\n,\n2025\n; Zhang\net al.\n,\n2025\n)\n. These successes, however, have largely focused on single-agent execution, where the environment is passive and predefined. Once other agents are introduced, as in competitive physical interactions such as boxing, the problem fundamentally shifts from motion-generation to strategic co-adaptation under physical constraints.\nThis shift naturally situates humanoid competition within the scope of Multi-Agent Reinforcement Learning (MARL).\nYet, unlike symbolic or abstract domains where MARL has achieved grandmaster-level success\n(Silver\net al.\n,\n2017\n; Vinyals\net al.\n,\n2019\n)\n, embodied humanoid competition requires reasoning through high-dimensional, contact-rich dynamics, where every strategic decision is physically bound by balance stability and actuation limits. At this frontier of multi-agent embodiment, we argue that there lie two intrinsic and domain-agnostic contradictions that characterize the competitive humanoid systems.\nPhysical Feasibility vs. Non-stationary Learning.\nHumanoid control operates in a high-dimensional continuous action space, which is subject to strict physical constraints, including contact stability, joint limits and underactuated dynamics. Effective strategy exploration, however, requires diverse and sometimes aggressive behaviors. As a result, such unconstrained policy exploration often leads to physically invalid motions and fails to keep balance\n(Peng\net al.\n,\n2018\n,\n2021\n)\n, while overly restrictive controllers collapse strategy learning. This further creates an embodied cold-start problem: agents cannot evolve competitive strategies without first maintaining a stable physical stance\n(Luo\net al.\n,\n2023\n)\n.\nStrategy Evolution vs. System Stability.\nMARL via self-play\n(Samuel,\n1959\n; Hernandez\net al.\n,\n2019\n)\nrelies on continuously evolving opponent policies, which introduces severe non-stationarity during training. While such dynamics are essential for discovering competitive strategies, humanoid systems are dynamically fragile: small distributional shifts in opponent behavior can destabilize balance and contact, leading to training collapse. This results in another tension between competitive strategy evolution and stable embodied interaction.\nExisting approaches address these challenges in an isolated manner. Game-theoretic MARL methods such as Neural Fictitious Self-Play (NFSP)\n(Heinrich and Silver,\n2016\n)\nare designed for abstract or weakly grounded environments, lacking inductive biases for physical feasibility. Conversely, embodied control frameworks such as Adversarial Motion Priors (AMP)\n(Peng\net al.\n,\n2021\n)\nand DeepMimic\n(Peng\net al.\n,\n2018\n)\nexcel at learning robust single-agent motor skills via imitation, but do not support strategic co-evolution or opponent-aware adaptation. As a result, neither line of work alone can resolve the dual contradictions inherent to embodied competitive tasks.\nIn this work, we argue that resolving these challenges requires a structured decoupling of physical control and strategic reasoning, coupled with a training mechanism that stabilizes competitive evolution. To this end, we propose a hierarchical framework that decomposes embodied MARL into three coupled layers: (1) physically grounded motion library, (2) structured latent motion space for strategy representation and (3) multi-agent strategy evolution over this latent space.\nConcretely, in the first layer, we establish a DeepMimic-based tracking policy\n(Peng\net al.\n,\n2018\n)\nto faithfully reproduce diverse human motion primitives, ensuring the agent masters a repertoire of physically feasible kinematic skills. In the second layer, we construct a bounded latent motion space that supports diverse motion generation and serves as the effective strategy space. In the third layer, we perform competitive strategy learning via NFSP, namely Latent Space NFSP (LS-NFSP), which enables latent multi-agent co-evolution within the bounded latent manifold.\nCrucially, the bounded latent motion space is not merely a representational choice, it defines a compact, physically safe strategy search space that allows LS-NFSP to operate without inducing motor-level instability. Meanwhile, AMP-based\n(Peng\net al.\n,\n2021\n)\ncurriculum warmup initializes the policy within this manifold, mitigating the competitive cold-start problem and substantially reducing the non-stationarity during fictitious self-play. Together, these mechanisms resolve the aforementioned contradictions of embodied MARL by reconciling physical stability with strategic diversity, and stabilizing strategy evolution under continuous competition.\nWe evaluate our framework on a humanoid boxing task using Unitree G1 robots with 29 degrees of freedom\n(Unitree,\n2024\n)\n, demonstrating that the proposed method achieves substantially improved performance, robustness, and convergence stability compared to existing baselines. Beyond this specific task, our framework provides a general recipe for embodied multi-agent competition, offering a principled pathway to scale MARL from abstract games to physically grounded robotic systems. Our contributions are summarized as follows:\nâ€¢\nTo the best of our knowledge, this work is the first to formally characterize the intrinsic contradictions in embodied MARL, specifically the trade-off between physical feasibility and non-stationary learning, and the conflict between strategy evolution and system stability, using humanoid competition as a prime instantiation.\nâ€¢\nWe propose a hierarchical framework that decouples high-level strategic reasoning from low-level physical execution, providing a stable pathway for evolving dynamic combat behaviors.\nâ€¢\nWe demonstrate the robustness of our framework through the emergence of tactical boxing in simulation and its zero-shot transfer to physical humanoid robots (\nFigureÂ 1\n).\nFigure 2\n:\nOverview of RoboStriker.\nStage I\npretrains a motion tracker to produce physically plausible humanoid behaviors;\nStage II\ncompresses these behaviors into a bounded latent space for high-level control;\nStage III(a)\nruns warm-up training on top of Stage II, then followed with\nStage III(b)\n, a NFSP over latent-space to solve the humanoid boxing task as a two-player zero-sum game.\n2\nRelated Work\nHumanoid Motion Synthesis and Control.\nThe generation of lifelike humanoid movement has evolved from purely kinematic trajectory synthesis to physics-aware control policies.\nKinematic Motion Synthesis historically relied on Inverse Kinematics (IK) and space-time optimization to retarget motion capture data onto robotic morphologies while satisfying joint and geometric constraints\n(Gleicher,\n1998\n; Yamane\net al.\n,\n2004\n)\n. While deep generative models have significantly improved the diversity of synthesized trajectories\n(Holden\net al.\n,\n2016\n; Zhang\net al.\n,\n2023\n; Petrovich\net al.\n,\n2023\n; Yin\net al.\n,\n2024\n)\n, these purely kinematic approaches lack dynamical awareness, often resulting in physical violations like \"foot sliding\" when transferred to simulators.\nPhysics-based control addresses the challenge of executing motions within a dynamical environment by accounting for gravity, inertia, and contact forces. Classical robot learning methods utilize Zero Moment Point (ZMP) stability and Model Predictive Control (MPC) to ensure bipedal balance\n(Kajita\net al.\n,\n2003\n; Kuindersma\net al.\n,\n2016\n)\n. Modern reinforcement learning (RL)\n(Sutton\net al.\n,\n1998\n)\nhas advanced this through tracking-based imitation\n(Peng\net al.\n,\n2018\n)\nand adversarial stylistic regularization\n(Peng\net al.\n,\n2021\n)\n. Crucially, the development of universal motion trackers\n(Chen\net al.\n,\n2025\n; Yin\net al.\n,\n2025\n; Zhang\net al.\n,\n2025\n)\nhas enabled the learning of unified policies capable of tracking a diverse repertoire of human motions. While these universal trackers provide a robust foundation for executing a vast library of skills, they primarily focus on high-fidelity imitation of pre-defined trajectories. In contrast, our work builds upon universal tracking but extends it to a reactive, multi-agent setting, where the controller must maintain physical stability while simultaneously adapting to the non-stationary perturbations of an opponent.\nSkill-based Latent Representation Learning.\nTo manage the complexity of humanoid action spaces, recent research has shifted toward learning low-level skills in latent manifolds. PULSE\n(Luo\net al.\n,\n2023\n)\nestablishes that distilling vast motion capture datasets into latent embeddings allows high-level policies to invoke complex behaviors via a simplified behavioral interface. Further refinements like CALM\n(Tessler\net al.\n,\n2023\n)\nhave improved the semantic diversity and controllability of these latent spaces. However, these manifolds are typically treated as unbounded Euclidean spaces where unconstrained exploration can still lead to out-of-distribution actions that cause physical collapse. We depart from this by imposing topological regularizationâ€”projecting the latent space onto a unit hypersphere\n(Peng\net al.\n,\n2022\n; Davidson\net al.\n,\n2018\n)\n. This geometric constraint ensures that the strategic search remains bounded within a manifold of physically plausible maneuvers, effectively isolating the high-level policy from the dynamical instabilities of the robotâ€™s joints.\nIn this work, we leverage this topologically constrained manifold to decouple strategic reasoning from dynamics control, enabling the efficient evolution of complex competitive behaviors that would be intractable in raw observation spaces\nMulti-Agent RL and Fictitious Self-Play.\nSelf-play has emerged as a principled paradigm for learning robust policies in competitive settings by framing training as an implicit game-solving process. Classical fictitious play\n(Brown,\n1951\n)\niteratively best-responds to the empirical distribution of past opponents and is known to converge to a Nash equilibrium in certain classes of games. Modern adaptations of fictitious self-play (FSP) integrate reinforcement learning with historical policy averaging, achieving strong empirical performance in large-scale imperfect-information games such as poker\n(Heinrich and Silver,\n2016\n)\n. Subsequent extensions have explored continuous control and function approximation, showing that approximate best-response dynamics can converge to approximate Nash equilibria under bounded strategy spaces and smoothness assumptions\n(Mertikopoulos and Zhou,\n2019\n)\n. Despite these advances, applying self-play directly in high-dimensional MARL often leads to cyclic strategic oscillations and training instabilities, particularly when policies operate in unconstrained continuous action spaces\n(Lanctot\net al.\n,\n2017\n)\n. Recent works have highlighted that constraining the effective strategy spaceâ€”through regularization, policy parameterization, or latent abstractionsâ€”can significantly improve convergence behavior and stability in competitive learning\n(Balduzzi and Racaniere,\n2018\n)\n. From a game-theoretic perspective, restricting policies to a compact, continuous strategy set ensures the existence of mixed-strategy Nash equilibria and supports convergence guarantees for no-regret and fictitious-play-style dynamics\n(Rosen,\n1965\n; Ramazi and Cao,\n2020\n)\n.\nOur work builds on this line of research by situating fictitious self-play within a bounded latent strategy manifold. By conducting the competitive learning process in a compact hyperspherical latent space rather than the raw motor command space, we effectively limit the strategic degrees of freedom available to each agent. This design mitigates non-stationarity arising from unbounded policy updates and aligns the training dynamics with theoretical convergence results for approximate Nash equilibria in continuous games with compact strategy sets.\n3\nMethod\nWe propose a hierarchical competitive learning framework that enables fully autonomous humanoid boxing through structured decision making in a continuous latent strategy space. The interaction between two humanoid agents is formalized as a two-player zero-sum Markov game, where each agent selects high-level motion intents rather than direct motor commands. A shared expert motion decoder executes these intents while preserving physical feasibility and human-like behaviors. Training proceeds through a three-stage pipeline: learning a robust low-level motion tracker from human demonstrations (Section\n3.2\n), distilling motion primitives into a bounded continuous latent space that serves as the strategic action space (Section\n3.3\n), and progressively introducing competition via behavioral warmup (Section\n3.4\n) and Latent-Space Neural Fictitious Self-Play (LS-NFSP) (Section\n3.5\n). This design decouples strategic reasoning from low-level control, stabilizes multi-agent learning and facilitates convergence toward approximate Nash equilibria in high-dimensional humanoid combat scenarios.\n3.1\nTwo-player Zero-sum Markov Games\nWe define a two-player zero-sum Markov Game as\nâ„³\n=\nâŸ¨\nâ„\n,\nğ’®\n,\n{\nğ’ª\n}\ni\nâˆˆ\nâ„\n,\n{\nğ’µ\n}\ni\nâˆˆ\nâ„\n,\nğ’«\n,\n{\nğ’°\n}\ni\nâˆˆ\nâ„\n,\nÎ³\nâŸ©\n\\mathcal{M}=\\langle\\mathcal{I},\\mathcal{S},\\{\\mathcal{O}\\}^{i\\in\\mathcal{I}},\\{\\mathcal{Z}\\}^{i\\in\\mathcal{I}},\\mathcal{P},\\{\\mathcal{U}\\}^{i\\in\\mathcal{I}},\\gamma\\rangle\n.\nâ„\n=\n{\n1\n,\n2\n}\n\\mathcal{I}=\\{1,2\\}\nthe set of players,\nğ’®\n\\mathcal{S}\nthe state space,\n{\nğ’ª\n}\ni\nâˆˆ\nâ„\n\\{\\mathcal{O}\\}^{i\\in\\mathcal{I}}\nthe set of player observation spaces. We assume the existence of a (possibly implicit) mapping\nâ„±\n:\nğ’ª\n1\nÃ—\nâ‹¯\nÃ—\nğ’ª\n|\nâ„\n|\nâ†’\nğ’®\n\\mathcal{F}:\\mathcal{O}^{1}\\times\\cdots\\times\\mathcal{O}^{|\\mathcal{I}|}\\rightarrow\\mathcal{S}\nthat relates joint observations to the underlying environment state.\n{\nğ’µ\n}\ni\nâˆˆ\nâ„\n\\{\\mathcal{Z}\\}^{i\\in\\mathcal{I}}\nthe set of player action spaces where\nğ’µ\ni\nâŠ‚\nâ„\nd\n\\mathcal{Z}^{i}\\subset\\mathbb{R}^{d}\nand\nd\nd\nthe dimension size,\nğ’«\n:\nğ’®\nÃ—\nğ’µ\n1\nÃ—\nâ‹¯\nÃ—\nğ’µ\n|\nâ„\n|\nâ†’\nÎ”\nâ€‹\n(\nğ’®\n)\n\\mathcal{P}:\\mathcal{S}\\times\\mathcal{Z}^{1}\\times\\cdots\\times\\mathcal{Z}^{|\\mathcal{I}|}\\rightarrow\\Delta(\\mathcal{S})\nthe transition function,\n{\nğ’°\n}\ni\nâˆˆ\nâ„\n\\{\\mathcal{U}\\}^{i\\in\\mathcal{I}}\nthe set of player utility functions and\nu\ni\nâˆˆ\nğ’°\ni\n:\nğ’®\nÃ—\nğ’µ\n1\nÃ—\nâ‹¯\nÃ—\nğ’µ\n|\nâ„\n|\nâ†’\nâ„\nu^{i}\\in\\mathcal{U}^{i}:\\mathcal{S}\\times\\mathcal{Z}^{1}\\times\\cdots\\times\\mathcal{Z}^{|\\mathcal{I}|}\\rightarrow\\mathbb{R}\n,\nÎ³\nâˆˆ\n[\n0\n,\n1\n)\n\\gamma\\in[0,1)\nthe discounted factor. Let each player\ni\nâˆˆ\nâ„\ni\\in\\mathcal{I}\nadopt a policy\nÏ€\nz\ni\n:\nğ’ª\ni\nâ†’\nÎ”\nâ€‹\n(\nğ’µ\ni\n)\n\\pi^{i}_{z}:\\mathcal{O}^{i}\\rightarrow\\Delta(\\mathcal{Z}^{i})\n, and the joint policy by\nğ…\nz\n=\n(\nÏ€\nz\n1\n,\nÏ€\nz\n2\n)\n\\boldsymbol{\\pi}_{z}=(\\pi^{1}_{z},\\pi^{2}_{z})\n. The expected return for player\ni\ni\nunder joint policy\nğ…\n\\boldsymbol{\\pi}\nfrom state\ns\nâˆˆ\nğ’®\ns\\in\\mathcal{S}\nsatisfies\nV\nğ…\nz\ni\nâ€‹\n(\ns\n)\n=\nğ”¼\nz\nt\ni\nâˆ¼\nÏ€\nz\ni\n(\nâ‹…\nâˆ£\no\nt\ni\n)\nâ€‹\n[\nâˆ‘\nt\n=\n0\nâˆ\nÎ³\nt\nâ€‹\nu\ni\nâ€‹\n(\ns\nt\n,\nz\nt\ni\n,\nz\nt\nâˆ’\ni\n)\n|\ns\n0\n=\ns\n]\n,\nV^{i}_{\\boldsymbol{\\pi}_{z}}(s)=\\mathbb{E}_{z^{i}_{t}\\sim\\pi^{i}_{z}(\\cdot\\mid o^{i}_{t})}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\,u^{i}(s_{t},z^{i}_{t},z^{-i}_{t})\\;\\bigg|\\;s_{0}=s\\right],\n(1)\nwhere\nâˆ’\ni\n-i\nindicates the players except for player\ni\ni\n.\nIn the zero-sum setting\nu\n1\n=\nâˆ’\nu\n2\nu^{1}=-u^{2}\n, the learning objective for each player\ni\ni\nis to find an optimal policy\nÏ€\nz\ni\n,\nâˆ—\n\\pi^{i,*}_{z}\nthat maximizes its expected return against the worst-case opponent policy\nÏ€\nz\nâˆ’\ni\n\\pi^{-i}_{z}\n, i.e., to solve the following maxâ€“min optimization:\nÏ€\nz\ni\n,\nâˆ—\nâˆˆ\narg\nâ¡\nmax\nÏ€\nz\ni\nâ¡\nmin\nÏ€\nz\nâˆ’\ni\nâ¡\nJ\ni\nâ€‹\n(\nğ…\nz\n)\n,\n\\pi^{i,*}_{z}\\in\\arg\\max_{\\pi^{i}_{z}}\\,\\min_{\\pi^{-i}_{z}}J^{i}(\\boldsymbol{\\pi}_{z}),\n(2)\nwhere\nJ\ni\nâ€‹\n(\nğ…\nz\n)\n:=\nğ”¼\ns\nâˆ¼\np\nâ€‹\n(\ns\n)\nâ€‹\n[\nV\nğ…\nz\ni\nâ€‹\n(\ns\n)\n]\nJ^{i}(\\boldsymbol{\\pi}_{z}):=\\mathbb{E}_{s\\sim p(s)}\\left[V^{i}_{\\boldsymbol{\\pi}_{z}}(s)\\right]\n.\nWhen all players simultaneously solve the above optimization up to approximation error, the resulting joint policy corresponds to an\nÏµ\n\\epsilon\n-Nash Equilibrium of the Markov game, as defined below.\nDefinition 3.1\n(\nÏµ\n\\epsilon\n-Nash Equilibrium\n(Aumann,\n1976\n)\n)\n.\nA joint policy\nğ…\nz\nâ‹†\n=\n(\nÏ€\nz\ni\n,\nâ‹†\n,\nÏ€\nz\nâˆ’\ni\n,\nâ‹†\n)\n\\boldsymbol{\\pi}_{z}^{\\star}=(\\pi_{z}^{i,\\star},\\pi_{z}^{-i,\\star})\nis said to be an\nÏµ\n\\epsilon\n-Nash Equilibrium\nof a two-player zero-sum Markov game\nâ„³\n\\mathcal{M}\nif no player can unilaterally improve its expected return by deviating from its policy, given that the opponent adheres to\nğ…\nz\nâ‹†\n\\boldsymbol{\\pi}_{z}^{\\star}\n. Formally, for all players\ni\nâˆˆ\nâ„\ni\\in\\mathcal{I}\nand any alternative policy\nÏ€\nz\ni\n\\pi_{z}^{i}\n, it holds that\nJ\ni\nâ€‹\n(\nÏ€\nz\ni\n,\nâ‹†\n,\nÏ€\nz\nâˆ’\ni\n,\nâ‹†\n)\nâ‰¥\nJ\ni\nâ€‹\n(\nÏ€\nz\ni\n,\nÏ€\nz\nâˆ’\ni\n,\nâ‹†\n)\nâˆ’\nÏµ\n.\nJ^{i}(\\pi_{z}^{i,\\star},\\pi_{z}^{-i,\\star})\\geq J^{i}(\\pi_{z}^{i},\\pi_{z}^{-i,\\star})-\\epsilon.\n(3)\nIn this work, we frame the boxing task that involves two humanoid robots as a zero-sum Markov game, where a player represents a robot.\nRather than acting directly in the high-dimensional motor command space, each player selects actions in a bounded continuous latent space\nğ’µ\ni\nâŠ‚\nâ„\nd\n\\mathcal{Z}^{i}\\subset\\mathbb{R}^{d}\n.\nAn action\nz\nt\ni\nâˆˆ\nğ’µ\ni\nz_{t}^{i}\\in\\mathcal{Z}^{i}\nrepresents a high-level motion latent distilled from a large motion corpus.\nIn practice, we constrain\nğ’µ\ni\n\\mathcal{Z}^{i}\nto be compact (specifically,\nğ’µ\ni\n=\nğ•Š\nd\nâˆ’\n1\n\\mathcal{Z}^{i}=\\mathbb{S}^{d-1}\n), which restricts exploration to a physically plausible manifold and simplifies strategic interactions in continuous control settings.\nWhile the preceding content formulate the proposed framework, its practical implementation requires additional design considerations. In the following, we provide a detailed description of a three-stage training pipeline that realizes a hierarchical decision-making process.\n3.2\nLearning Expertise Motion Tracker\nIn the first stage, we learn a robust low-level controller that can faithfully track diverse humanoid boxing motions and provide a stable motor foundation for higher-level strategic learning.\nThe tracker\nÏ€\ntrk\nâ€‹\n(\na\nt\nâˆ£\ns\nt\nself\n,\ns\nt\nref\n)\n\\pi_{\\mathrm{trk}}(a_{t}\\mid s_{t}^{\\mathrm{self}},s^{\\mathrm{ref}}_{t})\nis trained using human motion capture data and shared across all robots,\nwhere\ns\nt\nself\n=\n(\ns\nt\nprop\n,\ns\nt\npriv\n)\ns_{t}^{\\mathrm{self}}=(s_{t}^{\\mathrm{prop}},s_{t}^{\\mathrm{priv}})\n. Here,\ns\nt\nprop\ns_{t}^{\\mathrm{prop}}\ndenotes a robotâ€™s proprioceptive state at timestep\nt\nt\nand\ns\nt\npriv\ns_{t}^{\\mathrm{priv}}\nrepresents privileged observations that are inaccessible to the robotâ€™s onboard sensors.\ns\nt\nref\ns^{\\mathrm{ref}}_{t}\nspecifies a reference motion goal derived from human motion capture data, which is represented as a time-indexed sequence of full-body humanoid poses and velocities extracted from motion capture dataset\nğ’Ÿ\nmotion\n\\mathcal{D}_{\\mathrm{motion}}\n.\nEach\ns\nt\nref\ns^{\\mathrm{ref}}_{t}\nencodes the root position and orientation, joint angles, and corresponding joint velocities in a canonical humanoid kinematic tree.\nAll motions are temporally aligned and retargeted to the simulated humanoid morphology, enabling consistent tracking across diverse boxing behaviors.\nThe objective maximizes expected tracking rewards\nr\ntrk\nr_{\\mathrm{trk}}\nover reference trajectories\nÏ„\nref\n\\tau^{\\mathrm{ref}}\n, encouraging high-fidelity imitation while maintaining physical plausibility,\nÏ€\ntrk\nâˆ—\n=\narg\nâ¡\nmax\nÏ€\ntrk\nâ¡\nğ”¼\ns\nref\nâˆ¼\nÏ„\nref\n,\nÏ„\nref\nâˆ¼\nğ’Ÿ\nmotion\nâ€‹\n[\nJ\nÏ€\ntrk\nâ€‹\n(\ns\nref\n)\n]\n,\n\\displaystyle\\pi_{\\mathrm{trk}}^{*}=\\arg\\max_{\\pi_{\\mathrm{trk}}}\\mathbb{E}_{s^{\\mathrm{ref}}\\sim\\tau^{\\mathrm{ref}},\\tau^{\\mathrm{ref}}\\sim\\mathcal{D}_{\\mathrm{motion}}}\\left[J_{\\pi_{\\mathrm{trk}}}(s^{\\mathrm{ref}})\\right],\n(4)\nwhere\nâ€‹\nJ\nÏ€\ntrk\nâ€‹\n(\ns\nref\n)\n=\nğ”¼\na\nt\nâˆ¼\nÏ€\ntrk\nâ€‹\n[\nâˆ‘\nt\n=\n0\nT\nÎ³\nt\nâ€‹\nr\ntrk\nâ€‹\n(\ns\nt\nself\n,\na\nt\n;\ns\nref\n)\n]\n.\n\\displaystyle\\text{where }J_{\\pi_{\\mathrm{trk}}}(s^{\\mathrm{ref}})=\\mathbb{E}_{a_{t}\\sim\\pi_{\\mathrm{trk}}}\\left[\\sum_{t=0}^{T}\\gamma^{t}\\,r_{\\mathrm{trk}}(s_{t}^{\\mathrm{self}},a_{t};s^{\\mathrm{ref}})\\right].\nThe reward function\nr\ntrk\nr_{\\mathrm{trk}}\nquantifies the similarity between the simulated humanoid and the reference motion, comprising specific terms for pose alignment, velocity tracking, and control regularization. Details of reward design and data collection are provided in Appendix\nA\n.\n3.3\nEncoding Motion via Topological Latent Distillation\nTo enable learnable strategic control, we project the motion space into a continuous latent space\nğ’µ\n\\mathcal{Z}\n.\nThis is achieved via a teacher-student distillation framework\n(Ross\net al.\n,\n2011\n)\nconsisting of an encoder\nE\nÏ•\nE_{\\phi}\n, a decoder\nD\nÏˆ\nD_{\\psi}\n, and a state-conditioned latent prior\nP\nÎ¾\nP_{\\xi}\n.\nThe encoder maps observations\no\nt\n=\n(\ns\nt\nself\n,\ns\nt\nref\n)\no_{t}=(s_{t}^{\\mathrm{self}},s^{\\mathrm{ref}}_{t})\nto a distribution of latent codes\nz\nt\nz_{t}\n, i.e.,\nz\nt\nâˆ¼\nE\nÏ•\n(\nâ‹…\nâˆ£\no\nt\n)\n.\nz_{t}\\sim E_{\\phi}(\\cdot\\mid o_{t}).\n(5)\nIn our implementation, we model\nE\nÏ•\nâ€‹\n(\nz\nt\n)\nE_{\\phi}(z_{t})\nas diagonal Gaussian\nğ’©\nâ€‹\n(\nz\nt\nâˆ£\nÎ¼\nt\ne\n,\nÏƒ\nt\ne\n)\n\\mathcal{N}(z_{t}\\mid\\mu^{e}_{t},\\sigma^{e}_{t})\n. Then the decoder is trained to reconstructs the teacherâ€™s actions\na\nt\nâˆ¼\nÏ€\ntrk\n(\nâ‹…\nâˆ£\no\nt\n)\na_{t}\\sim\\pi_{\\mathrm{trk}}(\\cdot\\mid o_{t})\nconditioned on\n(\ns\nt\nprop\n,\nz\nt\n)\n(s^{\\mathrm{prop}}_{t},z_{t})\nas\nD\nÏˆ\nâ‹†\n=\narg\nâ¡\nmin\nD\nÏˆ\nâ¡\nâ€–\na\nt\nâˆ’\na\n^\nt\nâ€–\n,\nD^{\\star}_{\\psi}=\\arg\\min_{D_{\\psi}}\\parallel a_{t}-\\hat{a}_{t}\\parallel,\n(6)\nwhere\na\n^\nt\nâˆ¼\nD\nÏˆ\n(\nâ‹…\nâˆ£\ns\nt\nprop\n,\nz\nt\n)\n\\hat{a}_{t}\\sim D_{\\psi}(\\cdot\\mid s^{\\mathrm{prop}}_{t},z_{t})\n.\nAcknowledging the inherent state-dependency of motion generation, we explicitly learn a state-conditioned latent prior, denoted as\nP\nÎ¾\nâ€‹\n(\nz\nt\nâˆ£\ns\nt\nprop\n)\n=\nğ’©\nâ€‹\n(\nz\nt\nâˆ£\nÎ¼\nt\np\n,\nÏƒ\nt\np\n)\nP_{\\xi}(z_{t}\\mid s^{\\mathrm{prop}}_{t})=\\mathcal{N}(z_{t}\\mid\\mu^{p}_{t},\\sigma^{p}_{t})\n. This prior serves to model valid transitions from the current proprioceptive state and constrains the encoder\nE\nÏ•\nâ€‹\n(\nz\nt\nâˆ£\no\nt\n)\nE_{\\phi}(z_{t}\\mid o_{t})\nvia KL-regularization\n(Hershey and Olsen,\n2007\n)\n, thereby preventing posterior collapse, i.e.,\n(\nE\nÏ•\nâ‹†\n,\nP\nÎ¾\nâ‹†\n)\n=\narg\nmin\n(\nE\nÏ•\n,\nP\nÎ¾\n)\nD\nK\nâ€‹\nL\n(\nE\nÏ•\n(\nz\nt\nâˆ£\no\nt\n)\nâˆ¥\nP\nÎ¾\n(\nz\nt\nâˆ£\ns\nt\nprop\n)\n)\n.\n(E^{\\star}_{\\phi},P^{\\star}_{\\xi})=\\arg\\min_{(E_{\\phi},P_{\\xi})}D_{KL}\\left(E_{\\phi}(z_{t}\\mid o_{t})\\parallel P_{\\xi}(z_{t}\\mid s^{\\mathrm{prop}}_{t})\\right).\n(7)\nA key design choice is to enforce the latent space\nğ’µ\n\\mathcal{Z}\nto be bounded and continuous. To this end, rather than allowing unconstrained latent representations, we normalize latent codes onto a compact manifold, which restricts motion commands to a physically plausible set and prevents out-of-distribution execution by the decoder. Under this formulation, each latent action\nz\nâˆˆ\nğ’µ\nz\\in\\mathcal{Z}\nfully specifies an motion decision, and we therefore model\nğ’µ\n\\mathcal{Z}\nas the effective strategy space of the induced two-player game in Section\n3.1\n. In implementation, we let\nz\nz\nsatisfies the requirements with normalization as\nz\n^\n=\nNormalize\nâ€‹\n(\nz\n)\n=\nE\nÏ•\nâ€‹\n(\no\n)\nâ€–\nE\nÏ•\nâ€‹\n(\no\n)\nâ€–\n2\n,\n\\hat{z}=\\mathrm{Normalize}(z)=\\frac{E_{\\phi}(o)}{\\|E_{\\phi}(o)\\|_{2}},\n(8)\nthereby constraining the latent representation of\nz\nz\nto lie on the unit hypersphere. The total training objective is detailed in Appendix\nB\n.\n3.4\nBehavioral Warmup with Adversarial Priors\nConsidering direct competitive self-play from scratch is unstable due to the lack of basic tactical competence. We therefore introduce a behavioral warmup stage, where agents learn effective striking behaviors against a stationary opponent that strictly maintains a stable standing stance. A learnable residual policy\nÏ€\nÎ¸\n(\nâ‹…\nâˆ£\ns\nt\ngoal\n)\n\\pi_{\\theta}(\\cdot\\mid s^{\\mathrm{goal}}_{t})\nis defined as outputs residual latent commands\nÎ”\nâ€‹\nz\nt\n\\Delta z_{t}\nover a fixed behavioral prior\nP\nÎ¾\nâŸ‚\nP^{\\perp}_{\\xi}\nfrom the second stage, ensuring stable and human-like motions.\nThen, the complete form of\nÏ€\nz\n\\pi_{z}\nis\nÏ€\nz\n(\nâ‹…\nâˆ£\ns\nprop\n,\ns\ngoal\n)\n=\nÏ€\nÎ¸\n(\nâ‹…\nâˆ£\ns\ngoal\n)\nâŠ•\nP\nÎ¾\nâŸ‚\n(\nâ‹…\nâˆ£\ns\nt\nprop\n)\n,\n\\pi_{z}(\\cdot\\mid s^{\\mathrm{prop}},s^{\\mathrm{goal}})=\\pi_{\\theta}(\\cdot\\mid s^{\\mathrm{goal}})\\oplus P^{\\perp}_{\\xi}(\\cdot\\mid s^{\\mathrm{prop}}_{t}),\n(9)\nand the corresponding generation of\nz\nt\nz_{t}\nis formulated as\nz\nt\n=\nNormalize\nâ€‹\n(\nÎ”\nâ€‹\nz\nt\n+\nz\nt\np\n)\n,\nz_{t}=\\mathrm{Normalize}(\\Delta z_{t}+z^{p}_{t}),\n(10)\nwhere\nÎ”\nz\nt\nâˆ¼\nÏ€\nÎ¸\n(\nâ‹…\nâˆ£\ns\ngoal\n)\n\\Delta z_{t}\\sim\\pi_{\\theta}(\\cdot\\mid s^{\\mathrm{goal}})\nand\nz\nt\np\nâˆ¼\nP\nÎ¾\nâŸ‚\n(\nâ‹…\nâˆ£\ns\nprop\n)\nz^{p}_{t}\\sim P^{\\perp}_{\\xi}(\\cdot\\mid s^{\\mathrm{prop}})\n.\nTo prevent degeneration of motion quality, we regularize the warmup training with AMP\n(Peng\net al.\n,\n2021\n)\n. Specifically, a discriminator\nC\nC\nis trained to distinguish motion transitions generated by the agent operating via the frozen decoder\nD\nÏˆ\nD_{\\psi}\nfrom reference motion data, providing a style-consistency reward\nr\nstyle\nr_{\\mathrm{style}}\n. Thus, the learning objective is to maximize the expected hybrid return:\nÏ€\nz\nâ‹†\n=\narg\nâ¡\nmax\nÏ€\nz\nâ¡\nğ”¼\nÏ„\nâˆ¼\n(\nÏ€\nz\n,\nD\nÏˆ\n)\nâ€‹\n[\nâˆ‘\nt\n=\n0\nT\nÎ³\nt\nâ€‹\nR\nâ€‹\n(\ns\nt\n,\nz\nt\n)\n]\n,\n\\pi_{z}^{\\star}=\\arg\\max_{\\pi_{z}}\\mathbb{E}_{\\tau\\sim(\\pi_{z},D_{\\psi})}\\left[\\sum_{t=0}^{T}\\gamma^{t}R(s_{t},z_{t})\\right],\n(11)\nwhere\nR\nâ€‹\n(\ns\nt\n,\nz\nt\n)\n=\nw\ntask\nâ‹…\nr\ntask\nâ€‹\n(\ns\nt\n)\n+\nw\nstyle\nâ‹…\nr\nstyle\nâ€‹\n(\ns\nt\n,\ns\nt\n+\n1\n)\nR(s_{t},z_{t})=w_{\\mathrm{task}}\\cdot r_{\\mathrm{task}}(s_{t})+w_{\\mathrm{style}}\\cdot r_{\\mathrm{style}}(s_{t},s_{t+1})\n,\nÏ„\n\\tau\ndenotes the trajectory induced by the policy\nÏ€\nz\n\\pi_{z}\nand the frozen decoder\nD\nÏˆ\nD_{\\psi}\n.\nw\ntask\nw_{\\mathrm{task}}\nand\nw\nstyle\nw_{\\mathrm{style}}\nis the weighting coefficient. The detail of the task observation\ns\nt\ngoal\ns_{t}^{\\mathrm{goal}}\n, the task reward definition and the AMP practice is illustrated in the Appendix\nC\n.\n3.5\nLatent-Space Neural Fictitious Self-Play\nBuilding on the behavioral warmup stage, we adopt Latent-Space Neural Fictitious Self-Play (LS-NFSP) to enable competitive co-evolution. Our approach inherit the core principles of Neural Fictitious Self-Play (NFSP)\n(Heinrich and Silver,\n2016\n)\n, while implementing them over a structured latent action space\nğ’µ\n\\mathcal{Z}\nrather than the raw motor space. In LS-NFSP, each player is controlled by an independent LS-NFSP agent that learns through simultaneous self-play interactions with its opponents. An LS-NFSP agent interacts with its opponents and records its experience into two buffers, a reinforcement learning dataset\nğ’Ÿ\nR\nâ€‹\nL\n\\mathcal{D}_{RL}\nthat stores transitions and a supervised learning dataset\nğ’Ÿ\nS\nâ€‹\nL\n\\mathcal{D}_{SL}\nthat stores its own best-response behaviors. These two datasets are treated distinctly, corresponding to reinforcement learning and supervised learning objectives, respectively. For each agent, we train an RL policy\nÏ€\nz\nR\nâ€‹\nL\n\\pi^{RL}_{z}\nvia PPO\n(Schulman\net al.\n,\n2017\n)\non samples from\nğ’Ÿ\nR\nâ€‹\nL\n\\mathcal{D}_{RL}\n. In parallel, we train an average policy\nÏ€\nÂ¯\nz\n\\bar{\\pi}_{z}\nfrom\nğ’Ÿ\nS\nâ€‹\nL\n\\mathcal{D}_{SL}\nwith supervised learning, to imitate the agentâ€™s historical best-response behaviors. Concretely, over learning iterations\nk\n=\n1\n,\nâ€¦\n,\nK\nk=1,\\dots,K\n, the supervised dataset is incrementally updated as\nğ’Ÿ\nS\nâ€‹\nL\nk\n=\nğ’Ÿ\nS\nâ€‹\nL\nk\nâˆ’\n1\nâˆª\nğ’Ÿ\nR\nâ€‹\nL\nk\n,\nâˆ€\nk\n=\n1\n,\nâ€¦\n,\nK\n.\n\\mathcal{D}^{k}_{SL}=\\mathcal{D}^{k-1}_{SL}\\cup\\mathcal{D}^{k}_{RL},\\forall k=1,\\dots,K.\n(12)\nAction selection over latent space\nğ’µ\n\\mathcal{Z}\nfollows an mixture strategy with\nÎ·\nâˆˆ\n[\n0\n,\n1\n)\n\\eta\\in[0,1)\n.\nFormally, the mixed strategy is defined as\nÏƒ\n=\nÎ·\nâ‹…\nÏ€\nÂ¯\nz\n+\n(\n1\nâˆ’\nÎ·\n)\nâ‹…\nÏ€\nz\nR\nâ€‹\nL\n,\n\\sigma=\\eta\\cdot\\bar{\\pi}_{z}+(1-\\eta)\\cdot\\pi^{RL}_{z},\n(13)\nand latent actions are sampled as\nz\nt\nâˆ¼\nÏƒ\n(\nâ‹…\nâˆ£\ns\nt\nprop\n,\ns\nt\ngoal\n)\nz_{t}\\sim\\sigma(\\cdot\\mid s^{\\mathrm{prop}}_{t},s^{\\mathrm{goal}}_{t})\n. Under this formulation,\nÏ€\nz\nR\nâ€‹\nL\n\\pi^{RL}_{z}\nrepresents an approximate best response to the opponentsâ€™ mixed strategies\nÏƒ\nâˆ’\ni\n\\sigma^{-i}\n, while\nÏ€\nÂ¯\nz\n\\bar{\\pi}_{z}\napproximates the agentâ€™s long-run average strategy. We summarize the pseudo algorithm of LS-NFSP in Appendix\nD\n.\n3.6\nTheoretical Analysis\nTo analyze learning dynamics in our induced latent game, we first explicitly state a set of modeling assumptions that reflect both our hierarchical motion abstraction and the mathematical requirements for grounding equilibrium analysis in continuous strategy domains.\nAssumption 3.2\n(Bounded Continuous Latent Space)\n.\nEach agentâ€™s high-level policy\nÏ€\nz\n\\pi_{z}\nis built on latent space\nğ’µ\n\\mathcal{Z}\n, where\nğ’µ\nâŠ‚\nâ„\nd\n\\mathcal{Z}\\subset\\mathbb{R}^{d}\nis a compact continuous set. In our implementation,\nğ’µ\n\\mathcal{Z}\nis the unit hypersphere\nğ•Š\nd\nâˆ’\n1\n\\mathbb{S}^{d-1}\n, ensuring boundedness of the latent space.\nAssumption 3.3\n(Well-Defined and Bounded Payoffs)\n.\nThe interaction between agents induces an effective two-player zero-sum game with payoff functions\nu\ni\nâ€‹\n(\nz\ni\n,\nz\nâˆ’\ni\n)\nu_{i}(z_{i},z_{-i})\nfor arbitrary\ns\nâˆˆ\nğ’®\ns\\in\\mathcal{S}\n, where the expectation is taken over environment stochasticity and low-level policy execution. We assume the payoff functions are bounded and measurable over\nğ’®\nÃ—\nğ’µ\nÃ—\nğ’µ\n\\mathcal{S}\\times\\mathcal{Z}\\times\\mathcal{Z}\n.\nAssumption 3.4\n(Approximate Best-Response Updates)\n.\nAt each iteration, each agent updates its latent strategy by approximately best-responding to the empirical distribution of its opponentâ€™s past strategies. The approximation error introduced by finite-horizon rollouts and stochastic optimization is assumed to be bounded.\nAssumption\n3.2\nand\n3.3\nare motivated by Glicksbergâ€™s existence theorem\n(Glicksberg,\n1952\n)\n, which ensures that a mixed-strategy Nash equilibrium exists when each playerâ€™s strategy set is non-empty and compact, and the payoff functions are continuous. Beyond these structural conditions on the strategy space, the analysis of self-play dynamics further requires assumptions on the learning process itself. In our framework, high-level policies are optimized via NFSP, which alternates between computing approximate best responses and updating average strategies. We therefore abstract this optimization procedure as an approximate best-response dynamic in the induced latent game (Assumption\n3.4\n). Together, these assumptions establish a coherent analytical setting in which the latent game admits an equilibrium and we can derive approximate convergence results presented below.\nProposition 3.5\n.\nUnder Assumptions\n3.2\nto\n3.4\n, the induced latent interaction admits the standard regularity conditions under which FSP dynamics in continuous zero-sum game are known to approach approximate Nash Equilibrium. Consequently, LS-NFSP will converge to an\nÏµ\n\\epsilon\n-Nash Equilibrium of the induced game, up to bounded approximation error.\nProof.\nSee\nAppendixÂ E\n.\nâˆ\n4\nExperiment\nPolicy training is performed in Isaac Lab\n(NVIDIA\net al.\n,\n2025\n)\non the NVIDIA Omniverse platform, leveraging large-scale parallel simulation for efficient data collection. The simulated platform is the Unitree G1 humanoid\n(Unitree,\n2024\n)\n, featuring 29 degrees of freedom. All experiments are trained on a single NVIDIA RTX 4090 GPU. To improve robustness and encourage the emergence of transferable tactical behaviors, we apply domain randomization\n(Tobin\net al.\n,\n2017\n)\nto contact friction, link masses, and actuator gains. Our evaluation is designed to answer the following questions:\n(i) does the proposed method achieve superior converged performance compared to relevant baselines?\nand\n(ii) do the introduced componentsâ€”including the behavioral warm-up, and AMPâ€”contribute positively to overall performance?\nTable 1\n:\nWin Rate (%) of Cross-Playing. Each item the win rate of the row agent against the column agent, averaged over 20 bouts.\nAgent \\ Opp\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\n(8)\n(1)\nOurs LS-NFSP\n-\n68.52\n76.24\n82.41\n84.47\n95.38\n98.50\n100.0\n(2) Fictitious SP\n-\n-\n62.35\n71.84\n75.16\n92.11\n96.18\n99.42\n(3) Naive SP\n-\n-\n-\n68.45\n70.38\n88.61\n94.52\n98.53\n(4) LS-NFSP w/o AMP\n-\n-\n-\n-\n62.12\n82.14\n88.37\n95.18\n(5) PPO-Only\n-\n-\n-\n-\n-\n45.24\n77.86\n82.11\n(6) Static-Target Specialist\n-\n-\n-\n-\n-\n-\n60.33\n85.58\n(7) Naive SP w/o Warmup\n-\n-\n-\n-\n-\n-\n-\n94.17\n(8) 29Dof Action-Space SP\n-\n-\n-\n-\n-\n-\n-\n-\nTable 2\n:\nComparison between Action-Space (29-DOF) vs. Latent-Space (Ours) on Tactical and Physical proficiency.\nMetric\n29Dof Action-Space SP\nOurs LS-NFSP\n(a)Tactical Metrics\nOffensive Landing Rate (\nÎ·\nh\nâ€‹\ni\nâ€‹\nt\n\\eta_{hit}\n)\nâ†‘\n\\uparrow\n0.142\nÂ±\n0.05\n0.142\\pm 0.05\n0.685\nÂ±\n0.03\n\\mathbf{0.685\\pm 0.03}\nEngagement Rate (\nE\nâ€‹\nR\nER\n)\nâ†‘\n\\uparrow\n0.315\nÂ±\n0.08\n0.315\\pm 0.08\n0.824\nÂ±\n0.02\n\\mathbf{0.824\\pm 0.02}\n(b)Physical Metrics\nBase Stability (\nB\nâ€‹\nO\nâ€‹\nS\nBOS\n)\nâ†‘\n\\uparrow\n0.418\nÂ±\n0.12\n0.418\\pm 0.12\n0.942\nÂ±\n0.01\n\\mathbf{0.942\\pm 0.01}\nTorque Smoothness (\nT\nâ€‹\nS\nÏ„\nTS_{\\tau}\n)\nâ†“\n\\downarrow\n7.452\nÂ±\n1.211\n7.452\\pm 1.211\n0.930\nÂ±\n0.150\n\\mathbf{0.930\\pm 0.150}\n4.1\nEvaluation Metrics\nTo answer the above question, we propose some metrics to quantify the evaluation of LS-NFSP and baselines from three primary dimensions, which is motivated by some principles in professional boxing training and robust robotics control, as listed as follows.\nTactical Proficiency.\nThe tactical proficiency concentrates on evaluating the offensive precision and active engagement capabilities. Thus, we introduces two metrics as\nOffensive Landing Rate\n(\nÎ·\nhit\n\\eta_{\\mathrm{hit}}\n) and\nEngagement Rate\n(\nE\nâ€‹\nR\nER\n).\nÎ·\nhit\n\\eta_{\\mathrm{hit}}\nis identified by the proportion of contacts exceeding the force threshold to the total number of offensive attempts.\nE\nâ€‹\nR\nER\nacts as a rigorous measure of an agentâ€™s ability to maintain a viable combat posture, effectively penalizing tactical avoidance or non-confrontational movement patterns, the higher the better, and it is calculated as the proportion of the episode duration in which the agent concurrently satisfies two spatial conditions: (1) keep a distance within an effective striking range and (2) ensuring a facing alignment that exceeds a predefined threshold.\nPhysical Stability.\nPhysical stability is assessed through\nBase Orientation Stability\n(\nB\nâ€‹\nO\nâ€‹\nS\nBOS\n) and\nTorque Smoothness\n(\nT\nâ€‹\nS\nÏ„\nTS_{\\tau}\n). To ensure a positive correlation with control quality,\nB\nâ€‹\nO\nâ€‹\nS\nBOS\nis defined using an exponential kernels on the angular deviation:\nB\nâ€‹\nO\nâ€‹\nS\n=\nğ”¼\nâ€‹\n[\nexp\nâ¡\n(\nâˆ’\nâ€–\nğ \nbase\nâˆ’\nğ \nworld\nâ€–\n2\n)\n]\nBOS=\\mathbb{E}\\left[\\exp(-\\|\\mathbf{g}_{\\mathrm{base}}-\\mathbf{g}_{\\mathrm{world}}\\|^{2})\\right]\n, where\nğ \nbase\n\\mathbf{g}_{\\mathrm{base}}\nand\nğ \nworld\n\\mathbf{g}_{\\mathrm{world}}\nthe gravity vector in the robotâ€™s local frame and the global frame, respectively. A higher\nB\nâ€‹\nO\nâ€‹\nS\nBOS\nindicates a superior ability to maintain an upright posture under competitive perturbations.\nT\nâ€‹\nS\nÏ„\nTS_{\\tau}\nmeasures the magnitude of high-frequency oscillations in motor commands, calculated as the mean absolute change in joint torques across consecutive control steps:\nT\nâ€‹\nS\nÏ„\n=\nğ”¼\nâ€‹\n[\nâ€–\nÏ„\nt\nâˆ’\nÏ„\nt\nâˆ’\n1\nâ€–\n]\nTS_{\\tau}=\\mathbb{E}\\left[\\|\\tau_{t}-\\tau_{t-1}\\|\\right]\n. A lower\nT\nâ€‹\nS\nÏ„\nTS_{\\tau}\nvalue represents more hardware-friendly execution and reduced mechanical wear.\nStylistic Authenticity.\nThe Stylistic Authenticity of the emergent behaviors is evaluated via qualitative visual inspection. Instead of relying on a singular numerical score, we provide a comparative analysis of simulation snapshots to assess the human-likeness of the motions. This includes identifying key boxing maneuvers such as slips, counters, and rhythmic footwork, while ensuring the absence of unnatural joint configurations or physically unsustainable postures that often characterize non-constrained reinforcement learning policies.\nTable 3\n:\nTactical performance comparison focusing on Offensive Landing Rate and Engagement Rate. All metrics are evaluated against the Naive Self-Play (Latent). LS-NFSP outperforms all the baselines.\nMethods\nOffensive Landing Rate\nâ†‘\n\\uparrow\nEngagement Rate\nâ†‘\n\\uparrow\n(a) Comparison with Strategic Algorithms\nPPO-Only\n0.231\nÂ±\n0.03\n0.231\\pm 0.03\n0.495\nÂ±\n0.02\n0.495\\pm 0.02\nNaive SP\n0.350\nÂ±\n0.04\n0.350\\pm 0.04\n0.580\nÂ±\n0.05\n0.580\\pm 0.05\nFictitious SP\n0.420\nÂ±\n0.03\n0.420\\pm 0.03\n0.650\nÂ±\n0.04\n0.650\\pm 0.04\nOurs LS-NFSP\n0.685\nÂ±\n0.03\n\\mathbf{0.685\\pm 0.03}\n0.824\nÂ±\n0.02\n\\mathbf{0.824\\pm 0.02}\n(b) Impact of Latent Space\n29Dof Action-Space SP\n0.142\nÂ±\n0.05\n0.142\\pm 0.05\n0.315\nÂ±\n0.08\n0.315\\pm 0.08\nOurs LS-NFSP\n0.685\nÂ±\n0.03\n\\mathbf{0.685\\pm 0.03}\n0.824\nÂ±\n0.02\n\\mathbf{0.824\\pm 0.02}\n(c) Impact of Curriculum Design\nStatic-Target Specialist\n0.210\nÂ±\n0.04\n0.210\\pm 0.04\n0.450\nÂ±\n0.06\n0.450\\pm 0.06\nSP w/o Warmup\n0.050\nÂ±\n0.02\n0.050\\pm 0.02\n0.120\nÂ±\n0.05\n0.120\\pm 0.05\nOurs LS-NFSP\n0.685\nÂ±\n0.03\n\\mathbf{0.685\\pm 0.03}\n0.824\nÂ±\n0.02\n\\mathbf{0.824\\pm 0.02}\n(d) Impact of Stylistic Constraints\nLS-NFSP w/o AMP\n0.490\nÂ±\n0.03\n0.490\\pm 0.03\n0.720\nÂ±\n0.05\n0.720\\pm 0.05\nOurs LS-NFSP\n0.685\nÂ±\n0.03\n\\mathbf{0.685\\pm 0.03}\n0.824\nÂ±\n0.02\n\\mathbf{0.824\\pm 0.02}\n4.2\nCross-Playing Evaluation\nWe answer the first question by conducting cross-play between LS-NFSP and involved baselines described in\nAppendixÂ F\n.\nTableÂ 1\npresents the win rates over all the involved methods, which shows that LS-NFSP outperforms all the baselines. The most critical insight is the superiority of performing competitive self-play within a structured latent manifold. We notice that the LS-NFSP agent dominates 29Dof Action-Space SP agent with a 100.00% win rate and also performs superior tactical and physical proficiency (\nTableÂ 2\n), proving that decoupling balance maintenance from tactical exploration is a fundamental prerequisite for competitive learning in high-dimensional humanoid combat. And for the comparison between self-play methods with latent space, LS-NFSP also outperforms the other methods, in the result of Fictitious SP of 68.52% and Naive SP of 76.24%, confirming that the integration of NFSP effectively mitigates policy cycling to produce a more robust best response. The necessity of each training stage is further validated by the substantial performance gaps against Direct-SP of 98.50% and Static-Target Specialist of 95.38%, which fail to respectively overcome reward sparsity and reactive dynamic opponents. Additionally, the No-AMP variant exhibits reduced tactical efficiency of 82.41% win rate against Ours due to its non-standardized movement patterns. Finally, the significant win rate against the PPO-Only baseline of 84.47% underscores that competitive interaction is the indispensable driver of emergent combat behaviors.\n4.3\nAblation Study\nTo answer the second question, we conduct ablation study from two perspectives, i.e., whether the strategy prior, warm-up and AMP can benefit the policy training.\nFigure 3\n:\nt-SNE visualization of the 32-dimensional latent manifold, illustrating the structured semantic clustering of combat primitives and their topological relationships that facilitate stable, compositional behavioral transitions in the hierarchical LS-NFSP framework.\nStrategy Priors.\nTableÂ 3\n(a) underscores the necessity of both competitive strategy priors in achieving elite boxing proficiency. LS-NFSP demonstrates superior tactical performance by achieving\nÎ·\nhit\n=\n0.685\n\\eta_{\\mathrm{hit}}=0.685\nand\nE\nâ€‹\nR\n=\n0.824\nER=0.824\n, which significantly outperforms baselines.\nWarm-Up Training.\nAs shown in\nTableÂ 3\n(c), the necessity of leveraging warm-up is further evidenced by the collapse of the Direct-SP variant. Despite competitive access, it achieves a negligible\nÎ·\nhit\n=\n0.050\n\\eta_{\\mathrm{hit}}=0.050\n, proving its inability to overcome the inefficiency due to reward sparsity settings.\nAdversarial Motion Prior.\nWe further evaluate the impact of stylistic constraints rigorously as in\nTableÂ 3\n(d). While the variant achieves\nE\nâ€‹\nR\n=\n0.720\nER=0.720\n, its\nÎ·\nhit\n\\eta_{\\mathrm{hit}}\ndrops sharply to 0.490. This discrepancy reveals that, without AMP, the agent can approach the opponent but executes erratic strikes that difficult to contact opponent effectively, as\nFigureÂ 4\ndemonstrates that the policy trained without AMP lacks physical authenticity. Ultimately, these results prove that the synergy of NFSP-driven exploration and AMP-guided movement is essential for producing a controller that is both strategically dominant and physically authentic.\nFigure 4\n:\nMujoco visualization of the LS-NFSP and the one without AMP.\n4.4\nAnalysis of the Learned Latent Space\nTo investigate the underlying structure of the latent skills, we perform a joint t-SNE\n(Maaten and Hinton,\n2008\n)\nanalysis on the generated embeddings, as illustrated in\nFigureÂ 3\n. The visualization reveals a highly structured manifold with a distinct topological arrangement: there is a clear separation between the pure\nMove\n(purple) and pure\nStrike\n(pink) clusters. This demarcation indicates that the latent space successfully disentangles stable locomotion dynamics from high-momentum offensive maneuvers. Crucially,\nMove+Strike\n(blue) embeddings occupy the intermediate space, exhibiting significant overlap with both\nMove\nand\nStrike\nregions. This dual overlap suggests that the\nMove+Strike\nmode functions as a topological bridge, seamlessly connecting the two distinct primitives. Such a structure provides empirical evidence of the latent spaceâ€™s compositional nature, enabling the LS-NFSP framework to smoothly interpolate between navigation and combat. This allows the agent to initiate attacks dynamically while moving, without suffering from the discontinuity or instability typically associated with switching between disjoint control modes.\n5\nConclusion\nWe propose a hierarchical competitive framework for humanoid interaction that integrates latent-space control with Neural Fictitious Self-Play (LS-NFSP). By decoupling high-level tactical strategy from low-level balance control via a pre-trained latent manifold, our approach ensures physical stability while enabling the emergence of sophisticated boxing maneuvers like slip-and-counter tactics. Experimental results from an\n8\nÃ—\n8\n8\\times 8\ncross-play tournament, supported by qualitative snapshots and t-SNE analysis, demonstrate that LS-NFSP consistently outperforms baselines in both competitive win rate and stylistic authenticity. Ultimately, this work proves that the synergy of hierarchical latent representations and competitive self-play provides a robust and scalable solution for achieving tactical intelligence in high-dimensional humanoid robotics.\nReferences\nJ. P. Araujo, Y. Ze, P. Xu, J. Wu, and C. K. Liu (2025)\nRetargeting matters: general motion retargeting for humanoid motion tracking\n.\nExternal Links:\n2510.02252\n,\nLink\nCited by:\nÂ§A.1\n.\nR. J. Aumann (1976)\nAgreeing to disagree\n.\nThe Annals of Statistics\n4\n(\n6\n),\npp.Â 1236â€“1239\n.\nCited by:\nDefinition 3.1\n.\nD. Balduzzi and S. Racaniere (2018)\nThe mechanics of n-player differentiable games\n.\nIn\nICML\n,\nCited by:\nÂ§2\n.\nG. W. Brown (1951)\nIterative solution of games by fictitious play\n.\nAct. Anal. Prod Allocation\n13\n(\n1\n),\npp.Â 374\n.\nCited by:\nAppendix E\n,\n2nd item\n,\nÂ§2\n.\nZ. Chen, M. Ji, X. Cheng, X. Peng, X. B. Peng, and X. Wang (2025)\nGMT: general motion tracking for humanoid whole-body control\n.\narXiv preprint arXiv:2506.14770\n.\nCited by:\nÂ§2\n.\nT. R. Davidson, L. Falorsi, N. De Cao, T. Kipf, and J. M. Tomczak (2018)\nHyperspherical variational auto-encoders\n.\narXiv preprint arXiv:1804.00891\n.\nCited by:\nÂ§2\n.\nM. Gleicher (1998)\nRetargetting motion to new characters\n.\nIn\nProceedings of the 25th annual conference on Computer graphics and interactive techniques\n,\npp.Â 33â€“42\n.\nCited by:\nÂ§2\n.\nI. L. Glicksberg (1952)\nA further generalization of the kakutani fixed point theorem, with application to nash equilibrium points\n.\nProceedings of the American Mathematical Society\n3\n(\n1\n),\npp.Â 170â€“174\n.\nCited by:\nÂ§3.6\n.\nJ. Heinrich and D. Silver (2016)\nDeep reinforcement learning from self-play in imperfect-information games\n.\narXiv preprint arXiv:1603.01121\n.\nCited by:\nÂ§1\n,\nÂ§2\n,\nÂ§3.5\n.\nD. Hernandez, K. DenamganaÃ¯, Y. Gao, P. York, S. Devlin, S. Samothrakis, and J. A. Walker (2019)\nA generalized framework for self-play training\n.\nIn\n2019 IEEE Conference on Games (CoG)\n,\npp.Â 1â€“8\n.\nCited by:\n1st item\n,\nÂ§1\n.\nJ. R. Hershey and P. A. Olsen (2007)\nApproximating the kullback leibler divergence between gaussian mixture models\n.\nIn\n2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSPâ€™07\n,\nVol.\n4\n,\npp.Â IVâ€“317\n.\nCited by:\nÂ§3.3\n.\nD. Holden, J. Saito, and T. Komura (2016)\nA deep learning framework for character motion synthesis and editing\n.\nACM Transactions on Graphics (ToG)\n35\n(\n4\n),\npp.Â 1â€“11\n.\nCited by:\nÂ§2\n.\nS. Kajita, F. Kanehiro, K. Kaneko, K. Fujiwara, K. Harada, K. Yokoi, and H. Hirukawa (2003)\nBiped walking pattern generation by using preview control of zero-moment point\n.\nIn\n2003 IEEE international conference on robotics and automation (Cat. No. 03CH37422)\n,\nVol.\n2\n,\npp.Â 1620â€“1626\n.\nCited by:\nÂ§2\n.\nS. Kuindersma, R. Deits, M. Fallon, A. Valenzuela, H. Dai, F. Permenter, T. Koolen, P. Marion, and R. Tedrake (2016)\nOptimization-based locomotion planning, estimation, and control design for the atlas humanoid robot\n.\nAutonomous robots\n40\n(\n3\n),\npp.Â 429â€“455\n.\nCited by:\nÂ§2\n.\nM. Lanctot, V. Zambaldi, and A. Gruslys (2017)\nA unified game-theoretic approach to multiagent reinforcement learning\n.\nIn\nNeurIPS\n,\nCited by:\nÂ§2\n.\nQ. Liao, T. E. Truong, X. Huang, Y. Gao, G. Tevet, K. Sreenath, and C. K. Liu (2025)\nBeyondMimic: from motion tracking to versatile humanoid control via guided diffusion\n.\nExternal Links:\n2508.08241\n,\nLink\nCited by:\nÂ§1\n.\nZ. Luo, J. Cao, J. Merel, A. Winkler, J. Huang, K. Kitani, and W. Xu (2023)\nUniversal humanoid motion representations for physics-based control\n.\narXiv preprint arXiv:2310.04582\n.\nCited by:\nÂ§1\n,\nÂ§2\n.\nL. v. d. Maaten and G. Hinton (2008)\nVisualizing data using t-sne\n.\nJournal of machine learning research\n9\n(\nNov\n),\npp.Â 2579â€“2605\n.\nCited by:\nÂ§4.4\n.\nP. Mertikopoulos and Z. Zhou (2019)\nLearning in games with continuous action sets and unknown payoff functions\n.\nMathematical Programming\n173\n(\n1\n),\npp.Â 465â€“507\n.\nCited by:\nÂ§2\n.\nNVIDIA, :, M. Mittal, P. Roth, J. Tigue, A. Richard, O. Zhang, P. Du, A. Serrano-MuÃ±oz, X. Yao, R. ZurbrÃ¼gg, N. Rudin, L. Wawrzyniak, M. Rakhsha, A. Denzler, E. Heiden, A. Borovicka, O. Ahmed, I. Akinola, A. Anwar, M. T. Carlson, J. Y. Feng, A. Garg, R. Gasoto, L. Gulich, Y. Guo, M. Gussert, A. Hansen, M. Kulkarni, C. Li, W. Liu, V. Makoviychuk, G. Malczyk, H. Mazhar, M. Moghani, A. Murali, M. Noseworthy, A. Poddubny, N. Ratliff, W. Rehberg, C. Schwarke, R. Singh, J. L. Smith, B. Tang, R. Thaker, M. Trepte, K. V. Wyk, F. Yu, A. Millane, V. Ramasamy, R. Steiner, S. Subramanian, C. Volk, C. Chen, N. Jawale, A. V. Kuruttukulam, M. A. Lin, A. Mandlekar, K. Patzwaldt, J. Welsh, H. Zhao, F. Anes, J. Lafleche, N. MoÃ«nne-Loccoz, S. Park, R. Stepinski, D. V. Gelder, C. Amevor, J. Carius, J. Chang, A. H. Chen, P. de Heras Ciechomski, G. Daviet, M. Mohajerani, J. von Muralt, V. Reutskyy, M. Sauter, S. Schirm, E. L. Shi, P. Terdiman, K. Vilella, T. Widmer, G. Yeoman, T. Chen, S. Grizan, C. Li, L. Li, C. Smith, R. Wiltz, K. Alexis, Y. Chang, D. Chu, L. \". Fan, F. Farshidian, A. Handa, S. Huang, M. Hutter, Y. Narang, S. Pouya, S. Sheng, Y. Zhu, M. Macklin, A. Moravanszky, P. Reist, Y. Guo, D. Hoeller, and G. State (2025)\nIsaac lab: a gpu-accelerated simulation framework for multi-modal robot learning\n.\nExternal Links:\n2511.04831\n,\nLink\nCited by:\nÂ§A.2\n,\nAppendix F\n,\nÂ§4\n.\nX. B. Peng, P. Abbeel, S. Levine, and M. Van de Panne (2018)\nDeepmimic: example-guided deep reinforcement learning of physics-based character skills\n.\nACM Transactions On Graphics (TOG)\n37\n(\n4\n),\npp.Â 1â€“14\n.\nCited by:\nÂ§A.2\n,\nÂ§C.2\n,\nÂ§C.2\n,\nÂ§1\n,\nÂ§1\n,\nÂ§1\n,\nÂ§2\n.\nX. B. Peng, Y. Guo, L. Halper, S. Levine, and S. Fidler (2022)\nAse: large-scale reusable adversarial skill embeddings for physically simulated characters\n.\nACM Transactions On Graphics (TOG)\n41\n(\n4\n),\npp.Â 1â€“17\n.\nCited by:\nÂ§2\n.\nX. B. Peng, Z. Ma, P. Abbeel, S. Levine, and A. Kanazawa (2021)\nAmp: adversarial motion priors for stylized physics-based character control\n.\nACM Transactions on Graphics (ToG)\n40\n(\n4\n),\npp.Â 1â€“20\n.\nCited by:\n3rd item\n,\nÂ§1\n,\nÂ§1\n,\nÂ§1\n,\nÂ§2\n,\nÂ§3.4\n.\nM. Petrovich, M. J. Black, and G. Varol (2023)\nTmr: text-to-motion retrieval using contrastive 3d human motion synthesis\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.Â 9488â€“9497\n.\nCited by:\nÂ§2\n.\nP. Ramazi and M. Cao (2020)\nGlobal convergence for replicator dynamics of repeated snowdrift games\n.\nIEEE Transactions on Automatic Control\n66\n(\n1\n),\npp.Â 291â€“298\n.\nCited by:\nÂ§2\n.\nJ. B. Rosen (1965)\nExistence and uniqueness of equilibrium points for concave n-person games\n.\nEconometrica\n.\nCited by:\nÂ§2\n.\nS. Ross, G. J. Gordon, and J. A. Bagnell (2011)\nA reduction of imitation learning and structured prediction to no-regret online learning\n.\nExternal Links:\n1011.0686\n,\nLink\nCited by:\nÂ§3.3\n.\nA. L. Samuel (1959)\nSome studies in machine learning using the game of checkers\n.\nIBM Journal of research and development\n3\n(\n3\n),\npp.Â 210â€“229\n.\nCited by:\n1st item\n,\nÂ§1\n.\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017)\nProximal policy optimization algorithms\n.\nExternal Links:\n1707.06347\n,\nLink\nCited by:\nÂ§D.1\n,\n4th item\n,\nÂ§3.5\n.\nD. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton,\net al.\n(2017)\nMastering the game of go without human knowledge\n.\nnature\n550\n(\n7676\n),\npp.Â 354â€“359\n.\nCited by:\nÂ§1\n.\nR. S. Sutton, A. G. Barto,\net al.\n(1998)\nReinforcement learning: an introduction\n.\nVol.\n1\n,\nMIT press Cambridge\n.\nCited by:\nÂ§2\n.\nC. Tessler, Y. Kasten, Y. Guo, S. Mannor, G. Chechik, and X. B. Peng (2023)\nCalm: conditional adversarial latent models for directable virtual characters\n.\nIn\nACM SIGGRAPH 2023 Conference Proceedings\n,\npp.Â 1â€“9\n.\nCited by:\nÂ§2\n.\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel (2017)\nDomain randomization for transferring deep neural networks from simulation to the real world\n.\nExternal Links:\n1703.06907\n,\nLink\nCited by:\nÂ§4\n.\nUnitree (2024)\nUnitree g1 humanoid agent ai avatar\n.\nExternal Links:\nLink\nCited by:\nÂ§A.1\n,\nÂ§A.2\n,\nAppendix F\n,\nÂ§1\n,\nÂ§4\n.\nO. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev,\net al.\n(2019)\nGrandmaster level in starcraft ii using multi-agent reinforcement learning\n.\nnature\n575\n(\n7782\n),\npp.Â 350â€“354\n.\nCited by:\nÂ§1\n.\nXsens Technologies (2024)\nXsens mvn: inertial motion capture system\n.\nNote:\nAccessed: 2026-01-28\nExternal Links:\nLink\nCited by:\nÂ§A.1\n.\nK. Yamane, J. J. Kuffner, and J. K. Hodgins (2004)\nSynthesizing animations of human manipulation tasks\n.\nIn\nACM SIGGRAPH 2004 Papers\n,\npp.Â 532â€“539\n.\nCited by:\nÂ§2\n.\nK. Yin, W. Zeng, K. Fan, M. Dai, Z. Wang, Q. Zhang, Z. Tian, J. Wang, J. Pang, and W. Zhang (2025)\nUnitracker: learning universal whole-body motion tracker for humanoid robots\n.\narXiv preprint arXiv:2507.07356\n.\nCited by:\nÂ§1\n,\nÂ§2\n.\nK. Yin, S. Zou, Y. Ge, and Z. Tian (2024)\nTri-modal motion retrieval by learning a joint embedding space\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 1596â€“1605\n.\nCited by:\nÂ§2\n.\nW. Zeng, S. Lu, K. Yin, X. Niu, M. Dai, J. Wang, and J. Pang (2025)\nBehavior foundation model for humanoid robots\n.\nExternal Links:\n2509.13780\n,\nLink\nCited by:\nÂ§1\n.\nJ. Zhang, Y. Zhang, X. Cun, Y. Zhang, H. Zhao, H. Lu, X. Shen, and Y. Shan (2023)\nGenerating human motion from textual descriptions with discrete representations\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 14730â€“14740\n.\nCited by:\nÂ§2\n.\nZ. Zhang, J. Guo, C. Chen, J. Wang, C. Lin, Y. Lian, H. Xue, Z. Wang, M. Liu, J. Lyu,\net al.\n(2025)\nTrack any motions under any disturbances\n.\narXiv preprint arXiv:2509.13833\n.\nCited by:\nÂ§1\n,\nÂ§2\n.\nM. Zinkevich, M. Johanson, M. Bowling, and C. Piccione (2007)\nRegret minimization in games with incomplete information\n.\nAdvances in neural information processing systems\n20\n.\nCited by:\nAppendix E\n.\nAppendix A\nLearning an Expert Motion Tracker\nThis section supplements Section\n3.2\nwith implementation details for learning an expert motion tracker, including motion dataset collection and processing, as well as the reinforcement learning setup used to train the tracking policy.\nA.1\nMotion Dataset Collection and Processing\nWe collect a diverse set of human boxing motions recorded by professional boxers using an Xsens motion capture system\n(Xsens Technologies,\n2024\n)\n. The dataset consists of 46 original motion clips with a total duration of approximately 14 minutes, captured at a frequency of 50 Hz. These clips cover a wide range of boxing behaviors, including offensive strikes, defensive maneuvers, footwork, and transitional movements.\nTo increase data diversity and improve generalization, we augment the dataset through left-right mirroring, effectively doubling the number of available motion sequences.\nThe captured human motions are retargeted to the Unitree G1 humanoid\n(Unitree,\n2024\n)\nmorphology following the Generalized Motion Retargeting (GMR) framework\n(Araujo\net al.\n,\n2025\n)\n. Through this retargeting process, we obtain physically realizable reference trajectories dataset\nğ’Ÿ\nmotion\n\\mathcal{D}_{\\mathrm{motion}}\nthat preserve the stylistic characteristics and temporal structure of the original boxing motions.\nA.2\nReinforcement Learning Settings and Training Details\nState, Observation and Action Spaces.\nThe state of the tracking policy is defined as\ns\nt\nself\n=\n(\ns\nt\nprop\n,\ns\nt\npriv\n)\ns_{t}^{\\mathrm{self}}=(s_{t}^{\\mathrm{prop}},s_{t}^{\\mathrm{priv}})\n,\nwhere\ns\nt\nprop\ns_{t}^{\\mathrm{prop}}\ndenotes the proprioceptive state, including base angular velocities, relative joint positions, and joint velocities, and\ns\nt\npriv\ns_{t}^{\\mathrm{priv}}\ndenotes privileged information, including base linear velocities, global base position and orientation, body link poses, and contact states.\nThe observation\nğ¨\ntrk\nâˆˆ\nğ’ª\n\\mathbf{o}_{\\mathrm{trk}}\\in\\mathcal{O}\nconsists of three components:\n(i) the proprioceptive state\ns\nt\nprop\ns_{t}^{\\mathrm{prop}}\n;\n(ii) the reference motion state\ns\nt\nref\nâˆ¼\nÏ„\nref\ns_{t}^{\\mathrm{ref}}\\sim\\tau^{\\mathrm{ref}}\n, which provides target full-body poses and base velocities over the current and the next\nK\n=\n3\nK=3\ntimesteps; and\n(iii) the previous action\na\nt\nâˆ’\n1\na_{t-1}\nto promote temporal smoothness.\nThe action\na\nt\nâˆˆ\nğ’œ\na_{t}\\in\\mathcal{A}\ncorresponds to target joint positions for the 29-degree-of-freedom Unitree G1 humanoid\n(Unitree,\n2024\n)\n, which are tracked by a low-level PD controller.\nReward Function.\nThe tracking reward\nr\ntrk\nr_{\\mathrm{trk}}\nis designed to encourage accurate imitation of reference boxing motions and is defined as a weighted sum of exponential tracking terms:\nr\ntrk\nâ€‹\n(\ns\nt\nself\n,\na\nt\n;\ns\nt\nref\n)\n\\displaystyle r_{\\mathrm{trk}}(s_{t}^{\\mathrm{self}},a_{t};s_{t}^{\\mathrm{ref}})\n=\nw\np\nâ‹…\nr\np\nâ€‹\n(\ns\nt\nself\n,\ns\nt\nref\n)\n+\nw\no\nâ‹…\nr\no\nâ€‹\n(\ns\nt\nself\n,\ns\nt\nref\n)\n+\nw\nbp\nâ‹…\nr\nbp\nâ€‹\n(\ns\nt\nself\n,\ns\nt\nref\n)\n\\displaystyle=w_{\\mathrm{p}}\\cdot r_{\\mathrm{p}}(s^{\\mathrm{self}}_{t},s^{\\mathrm{ref}}_{t})+w_{\\mathrm{o}}\\cdot r_{\\mathrm{o}}(s^{\\mathrm{self}}_{t},s^{\\mathrm{ref}}_{t})+w_{\\mathrm{bp}}\\cdot r_{\\mathrm{bp}}(s^{\\mathrm{self}}_{t},s^{\\mathrm{ref}}_{t})\n+\nw\nbo\nâ‹…\nr\nbo\nâ€‹\n(\ns\nt\nself\n,\ns\nt\nref\n)\n+\nw\nlv\nâ‹…\nr\nlv\nâ€‹\n(\ns\nt\nself\n,\ns\nt\nref\n)\n+\nw\nav\nâ‹…\nr\nav\nâ€‹\n(\ns\nt\nself\n,\ns\nt\nref\n)\nâˆ’\nr\nreg\nâ€‹\n(\ns\nt\nself\n,\na\nt\n)\n,\n\\displaystyle+w_{\\mathrm{bo}}\\cdot r_{\\mathrm{bo}}(s^{\\mathrm{self}}_{t},s^{\\mathrm{ref}}_{t})+w_{\\mathrm{lv}}\\cdot r_{\\mathrm{lv}}(s^{\\mathrm{self}}_{t},s^{\\mathrm{ref}}_{t})+w_{\\mathrm{av}}\\cdot r_{\\mathrm{av}}(s^{\\mathrm{self}}_{t},s^{\\mathrm{ref}}_{t})-r_{\\mathrm{reg}}(s^{\\mathrm{self}}_{t},a_{t}),\nwhere\nğ’³\n=\np\n,\no\n,\nbp\n,\nbo\n,\nlv\n,\nav\n\\mathcal{X}={\\mathrm{p,o,bp,bo,lv,av}}\nand each term is defined as\nr\nx\n=\nexp\nâ¡\n(\nâˆ’\nâ€–\nerr\nx\nâ€–\n2\n/\nÏƒ\nx\n2\n)\n.\nr_{x}=\\exp(-\\|\\mathrm{err}_{x}\\|^{2}/\\sigma_{x}^{2}).\nHere,\nr\np\nr_{\\mathrm{p}}\nand\nr\no\nr_{\\mathrm{o}}\nmeasure global root position and orientation tracking errors,\nr\nbp\nr_{\\mathrm{bp}}\nand\nr\nbo\nr_{\\mathrm{bo}}\nmeasure relative body link pose errors, and\nr\nlv\nr_{\\mathrm{lv}}\nand\nr\nav\nr_{\\mathrm{av}}\npenalize deviations in linear and angular velocities. The regularization term\nr\nreg\nr_{\\mathrm{reg}}\npenalizes excessive action rate, joint limit violations, and undesired self-collisions or ground contacts. The weights\nw\nx\nw_{x}\nand scaling factors\nÏƒ\nx\n\\sigma_{x}\nare provided in Table\n4\n.\nTable 4\n:\nReward Terms, Weights and Scaling Factor for Motion Tracker.\nReward Terms\nWeights\nw\nw\nScaling Factor\nÏƒ\n\\sigma\nRoot Position\nr\np\nr_{\\mathrm{p}}\n0.5\n0.5\n0.3\n0.3\nRoot Orientation\nr\no\nr_{\\mathrm{o}}\n0.5\n0.5\n0.4\n0.4\nBody Position\nr\nbp\nr_{\\mathrm{bp}}\n1.0\n1.0\n0.3\n0.3\nBody Orientation\nr\nbo\nr_{\\mathrm{bo}}\n1.0\n1.0\n0.4\n0.4\nBody Linear Velocity\nr\nlv\nr_{\\mathrm{lv}}\n1.0\n1.0\n1.0\n1.0\nBody Angular Velocity\nr\nav\nr_{\\mathrm{av}}\n1.0\n1.0\n3.14\n3.14\nEarly Termination.\nTo accelerate training and prevent the policy from exploring unstable or physically unfeasible states, we follow\n(Peng\net al.\n,\n2018\n)\nto implement an early termination mechanism\nğ’¯\n\\mathcal{T}\n. An episode is terminated if any of the following conditions are met: 1).\nPose Collapse:\nThe vertical distance between the robotâ€™s base and its reference height falls below a threshold of\n0.25\nâ€‹\nm\n0.25\\,\\text{m}\n, or the base orientation deviation exceeds\n0.8\n0.8\nrad. 2).\nEnd-Effector Violation:\nThe vertical position of key end-effectors, specifically the ankles and wrists, deviates from the reference height by more than\n0.25\nâ€‹\nm\n0.25\\,\\text{m}\n. 3).\nTemporal Limit:\nThe maximum episode duration of\n10.0\n10.0\nseconds is reached.\nBy combining structured reward shaping, early termination, and massive-scale parallel simulation in Isaac Lab\n(NVIDIA\net al.\n,\n2025\n)\n, the low-level tracking policy\nÏ€\ntrk\n\\pi_{\\mathrm{trk}}\nlearns robust and physically consistent motor skills, which form the foundation for subsequent latent skill distillation.\nAppendix B\nEncoding Motion via Topological Latent Distillation\nThis section provides implementation details for the latent motion distillation framework introduced in Section\n3.3\nDistillation Architecture.\nWe adopt a studentâ€“teacher paradigm to distill the low-level tracking policy\nÏ€\ntrk\n\\pi_{\\mathrm{trk}}\ninto a compact latent interface. During training, the teacher policy is fixed, and a student model consisting of an encoder\nE\nÏ•\nE_{\\phi}\n, a decoder\nD\nÏˆ\nD_{\\psi}\n, and a state-conditioned prior network\nP\nÎ¾\nP_{\\xi}\nis jointly optimized.\nAt each timestep, the encoder maps privileged observations\no\nt\n=\n(\ns\nt\nself\n,\ns\nt\nref\n)\no_{t}=(s_{t}^{\\mathrm{self}},s_{t}^{\\mathrm{ref}})\nto a latent distribution\nq\nÏ•\nâ€‹\n(\nz\nt\nâˆ£\no\nt\n)\nq_{\\phi}(z_{t}\\mid o_{t})\n. A latent code\nz\nt\nz_{t}\nis sampled and provided to the decoder, which predicts target joint actions conditioned on the proprioceptive state.\nTraining Objective.\nThe student is trained to reconstruct the teacherâ€™s actions while learning a structured latent distribution. The reconstruction loss is defined as\nâ„’\nrec\n=\nğ”¼\ns\nt\nref\nâˆ¼\nÏ„\nref\n,\nÏ„\nref\nâˆ¼\nğ’Ÿ\nmotion\nâ€‹\n[\nâ€–\na\nt\nâˆ’\na\n^\nt\nâ€–\n2\n]\n,\n\\mathcal{L}_{\\mathrm{rec}}=\\mathbb{E}_{s^{\\mathrm{ref}}_{t}\\sim\\tau^{\\mathrm{ref}},\\tau^{\\mathrm{ref}}\\sim\\mathcal{D}_{\\mathrm{motion}}}\\left[\\|a_{t}-\\hat{a}_{t}\\|^{2}\\right],\nwhere\na\nt\nâˆ¼\nÏ€\ntrk\n(\nâ‹…\nâˆ£\no\nt\n)\na_{t}\\sim\\pi_{\\mathrm{trk}}(\\cdot\\mid o_{t})\n,\nz\nt\nâˆ¼\nE\nÏ•\n(\nâ‹…\nâˆ£\no\nt\n)\nz_{t}\\sim E_{\\phi}(\\cdot\\mid o_{t})\n,\na\n^\nt\nâˆ¼\nD\nÏˆ\nâ€‹\n(\ns\nt\nprop\n,\nz\nt\n)\n\\hat{a}_{t}\\sim D_{\\psi}(s_{t}^{\\mathrm{prop}},z_{t})\n. To regularize the latent space, we impose a KL divergence between the encoder posterior and the state-conditioned prior:\nâ„’\nprior\n=\nğ”¼\ns\nt\nref\nâˆ¼\nÏ„\nref\n,\nÏ„\nref\nâˆ¼\nğ’Ÿ\nmotion\n[\nD\nKL\n(\nE\nÏ•\n(\nz\nt\nâˆ£\no\nt\n)\nâˆ¥\nP\nÎ¾\n(\nz\nt\nâˆ£\ns\nt\nprop\n)\n)\n]\n.\n\\mathcal{L}_{\\mathrm{prior}}=\\mathbb{E}_{s^{\\mathrm{ref}}_{t}\\sim\\tau^{\\mathrm{ref}},\\tau^{\\mathrm{ref}}\\sim\\mathcal{D}_{\\mathrm{motion}}}\\left[D_{\\mathrm{KL}}\\left(E_{\\phi}(z_{t}\\mid o_{t})\\|P_{\\xi}(z_{t}\\mid s_{t}^{\\mathrm{prop}})\\right)\\right].\nThe prior loss ensures that the prior network captures the semantic structure of the motion primitives. During the competitive stage in self-play, this learned prior provides a critical reference for the high-level policy, where a KL penalty is applied to prevent the strategy from deviating into physically unstable regions of the action space.\nThe total loss is given by\nâ„’\ndistill\n=\nâ„’\nrec\n+\nÎ»\nprior\nâ€‹\nâ„’\nprior\n.\n\\mathcal{L}_{\\mathrm{distill}}=\\mathcal{L}_{\\mathrm{rec}}+\\lambda_{\\mathrm{prior}}\\mathcal{L}_{\\mathrm{prior}}.\nwhere\nÎ»\np\nâ€‹\nr\nâ€‹\ni\nâ€‹\no\nâ€‹\nr\n=\n0.001\n\\lambda_{prior}=0.001\n. This dual optimization ensures that the student not only recovers the motor control expertise of the teacher but also constructs a structured, prior-conditioned manifold ready for competitive co-evolution.\nAppendix C\nBehavioral Warmup with Adversarial Priors\nThis section gives implementation details for the behavioral wamup training. We also describe how Adversarial Motion Priors (AMP) are incorporated to regularize behaviors toward the motion-capture distribution and preserve human-like boxing style.\nC.1\nGoal-Oriented Observation for Autonomous Boxing\nDifferent from previous tracking-based task settings, the observation of the autonomous boxing task does not contain any explicit motion commands. Instead, the residual policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nrelies on goal-oriented spatial information to guide decision-making. The task observation\ns\ngoal\ns^{\\mathrm{goal}}\nconsists of two complementary components: (1) offensive target information and (2) defensive threat information.\nOffensive Target Observation.\nThe offensive target observation encodes the relative positions of the ego agentâ€™s left and right fists with respect to the opponentâ€™s torso, represented in the ego-centric coordinate frame. Let\nğ©\nl\nego\n,\nğ©\nr\nego\nâˆˆ\nâ„\n3\n\\mathbf{p}_{l}^{\\mathrm{ego}},\\mathbf{p}_{r}^{\\mathrm{ego}}\\in\\mathbb{R}^{3}\ndenote the world-frame positions of the ego agentâ€™s wrists,\nğ©\nt\nopp\nâˆˆ\nâ„\n3\n\\mathbf{p}_{t}^{\\mathrm{opp}}\\in\\mathbb{R}^{3}\nthe position of the opponentâ€™s torso, and\nğª\nego\n\\mathbf{q}^{\\mathrm{ego}}\nthe ego root orientation. The relative vectors are defined as\nğ¯\nl\noff\n=\nğ©\nl\nego\nâˆ’\nğ©\nt\nopp\n,\nğ¯\nr\noff\n=\nğ©\nr\nego\nâˆ’\nğ©\nt\nopp\n,\n\\mathbf{v}_{l}^{\\mathrm{off}}=\\mathbf{p}_{l}^{\\mathrm{ego}}-\\mathbf{p}_{t}^{\\mathrm{opp}},\\quad\\mathbf{v}_{r}^{\\mathrm{off}}=\\mathbf{p}_{r}^{\\mathrm{ego}}-\\mathbf{p}_{t}^{\\mathrm{opp}},\n(14)\nwhich are further transformed into the ego-centric frame via\nğ¯\n~\noff\n=\nğ‘\nâ€‹\n(\nğª\nego\n)\nâˆ’\n1\nâ€‹\nğ«\noff\n,\n\\tilde{\\mathbf{v}}^{\\mathrm{off}}=\\mathbf{R}(\\mathbf{q}^{\\mathrm{ego}})^{-1}\\mathbf{r}^{\\mathrm{off}},\n(15)\nwhere\nğ‘\nâ€‹\n(\nâ‹…\n)\n\\mathbf{R}(\\cdot)\ndenotes the rotation matrix induced by the quaternion.\nThis transformation removes global orientation dependency and ensures rotational invariance of the observation. As a result, the offensive component provides direct and physically interpretable geometric information for guiding accurate, distance-aware striking behaviors.\nDefensive Target Observation.\nThe defensive target observation characterizes the relative positions of the opponentâ€™s left and right fists with respect to the ego agentâ€™s torso, also expressed in the ego-centric frame. Let\nğ©\nl\nopp\n,\nğ©\nr\nopp\nâˆˆ\nâ„\n3\n\\mathbf{p}_{l}^{\\mathrm{opp}},\\mathbf{p}_{r}^{\\mathrm{opp}}\\in\\mathbb{R}^{3}\ndenote the opponentâ€™s wrist positions and\nğ©\nt\nego\nâˆˆ\nâ„\n3\n\\mathbf{p}_{t}^{\\mathrm{ego}}\\in\\mathbb{R}^{3}\nthe ego torso position. The defensive vectors are computed as\nğ¯\nl\ndef\n=\nğ©\nl\nopp\nâˆ’\nğ©\nt\nego\n,\nğ¯\nr\ndef\n=\nğ©\nr\nopp\nâˆ’\nğ©\nt\nego\n,\n\\mathbf{v}_{l}^{\\mathrm{def}}=\\mathbf{p}_{l}^{\\mathrm{opp}}-\\mathbf{p}_{t}^{\\mathrm{ego}},\\quad\\mathbf{v}_{r}^{\\mathrm{def}}=\\mathbf{p}_{r}^{\\mathrm{opp}}-\\mathbf{p}_{t}^{\\mathrm{ego}},\n(16)\nand rotated into the local frame in the same manner:\nğ¯\n~\ndef\n=\nğ‘\nâ€‹\n(\nğª\nego\n)\nâˆ’\n1\nâ€‹\nğ¯\ndef\n.\n\\tilde{\\mathbf{v}}^{\\mathrm{def}}=\\mathbf{R}(\\mathbf{q}^{\\mathrm{ego}})^{-1}\\mathbf{v}^{\\mathrm{def}}.\n(17)\nThis component enables the policy to perceive incoming attack trajectories independently of global pose, facilitating anticipatory defense, evasive maneuvers, and counter-attacking strategies.\nTogether, the offensive and defensive observations form a compact yet expressive goal representation\ns\ngoal\n=\n(\nğ¯\n~\nl\noff\n,\nğ¯\n~\nr\noff\n,\nğ¯\n~\nl\ndef\n,\nğ¯\n~\nr\ndef\n)\ns^{\\mathrm{goal}}=(\\tilde{\\mathbf{v}}^{\\mathrm{off}}_{l},\\tilde{\\mathbf{v}}^{\\mathrm{off}}_{r},\\tilde{\\mathbf{v}}^{\\mathrm{def}}_{l},\\tilde{\\mathbf{v}}^{\\mathrm{def}}_{r})\nthat supports coordinated attackâ€“defense behaviors in autonomous humanoid boxing.\nC.2\nAdversarial Motion Prior\nTo preserve human-like motion quality, we follow\n(Peng\net al.\n,\n2018\n)\nto use a discriminator\nC\nC\nto distinguish whether the state originates from mocap dataset\nğ’Ÿ\nmotion\n\\mathcal{D}_{\\mathrm{motion}}\nor the agent formed by the policy\nÏ€\nz\n\\pi_{z}\nand the frozen decoder\nD\nÏˆ\nD_{\\psi}\n. Specifically, we first construct the AMP observation\no\nt\ndisc\n{o_{t}}^{\\mathrm{disc}}\nto better capture the style of the mocap data. The observation\no\nt\ndisc\n{o_{t}}^{{\\mathrm{disc}}}\nincludes the local rotation and velocity of each joint, along with the angular velocities of the base. Observation transitions from the agent and those sampled from the mocap data are fed separately to the discriminator. We label the transitions from the mocap data as\nreal\nand those from the agent as\nfake\n. The goal of the discriminator is to classify these inputs correctly: when mocap transitions are provided, the output should approach 1, whereas transitions from the agent should produce outputs closer to -1. As a result, the objective function can be formulated as follows:\narg\nâ¡\nmin\nC\n\\displaystyle\\arg\\min_{C}\nğ”¼\nd\nğ’Ÿ\nmotion\nâ€‹\n(\no\nt\ndisc\n,\no\nt\n+\n1\ndisc\n)\nâ€‹\n[\n(\nC\nâ€‹\n(\no\nt\ndisc\n,\no\nt\n+\n1\ndisc\n)\nâˆ’\n1\n)\n2\n]\n\\displaystyle\\ \\mathbb{E}_{d^{\\mathcal{D}_{\\mathrm{motion}}}({o_{t}}^{\\mathrm{disc}},{o_{t+1}^{\\mathrm{disc}}})}\\left[\\left(C({o_{t}}^{\\mathrm{disc}},{o_{t+1}^{\\mathrm{disc}}})-1\\right)^{2}\\right]\n+\nğ”¼\nd\nÏ€\nz\n,\nD\nÏˆ\nâ€‹\n(\no\nt\ndisc\n,\no\nt\n+\n1\ndisc\n)\nâ€‹\n[\n(\nC\nâ€‹\n(\no\nt\ndisc\n,\no\nt\n+\n1\ndisc\n)\n+\n1\n)\n2\n]\n.\n\\displaystyle+\\mathbb{E}_{d^{\\pi_{z},D_{\\psi}}({o_{t}}^{\\mathrm{disc}},{o_{t+1}^{\\mathrm{disc}}})}\\left[\\left(C({o_{t}}^{\\mathrm{disc}},{o_{t+1}^{\\mathrm{disc}}})+1\\right)^{2}\\right].\nSince discriminators often suffer from mode collapse, we follow AMP\n(Peng\net al.\n,\n2018\n)\nby applying a gradient penalty to the mocap data transitions to ensure stability. This can be formulated as follows:\narg\nâ¡\nmin\nC\nâ¡\nğ”¼\nd\nğ’Ÿ\nmotion\nâ€‹\n(\no\nt\ndisc\n,\no\nt\n+\n1\ndisc\n)\nâ€‹\n[\nâ€–\nâ–½\nâ€‹\nC\nâ€‹\n(\no\nt\ndisc\n,\no\nt\n+\n1\ndisc\n)\nâ€–\n2\n]\n.\n\\arg\\min_{C}\\ \\mathbb{E}_{d^{\\mathcal{D}_{\\mathrm{motion}}}({o_{t}}^{\\mathrm{disc}},{o_{t+1}^{\\mathrm{disc}}})}\\left[\\left\\lVert\\triangledown C({o_{t}}^{\\mathrm{disc}},{o_{t+1}^{\\mathrm{disc}}})\\right\\rVert^{2}\\right].\nThe final objective function to train the discriminator is formulated as follows:\narg\nâ¡\nmin\nC\n\\displaystyle\\arg\\min_{C}\nğ”¼\nd\nğ’Ÿ\nmotion\nâ€‹\n(\no\nt\ndisc\n,\no\nt\n+\n1\ndisc\n)\nâ€‹\n[\n(\nC\nâ€‹\n(\no\nt\ndisc\n,\no\nt\n+\n1\ndisc\n)\nâˆ’\n1\n)\n2\n]\n\\displaystyle\\ \\mathbb{E}_{d^{\\mathcal{D}_{\\mathrm{motion}}}({o_{t}}^{\\mathrm{disc}},{o_{t+1}^{\\mathrm{disc}}})}\\left[\\left(C({o_{t}}^{\\mathrm{disc}},{o_{t+1}^{\\mathrm{disc}}})-1\\right)^{2}\\right]\n+\nğ”¼\nd\nÏ€\nz\n,\nD\nÏˆ\nâ€‹\n(\no\nt\ndisc\n,\no\nt\n+\n1\ndisc\n)\nâ€‹\n[\n(\nC\nâ€‹\n(\no\nt\ndisc\n,\no\nt\n+\n1\ndisc\n)\n+\n1\n)\n2\n]\n\\displaystyle+\\mathbb{E}_{d^{\\pi_{z},D_{\\psi}}({o_{t}}^{\\mathrm{disc}},{o_{t+1}^{\\mathrm{disc}}})}\\left[\\left(C({o_{t}}^{\\mathrm{disc}},{o_{t+1}^{\\mathrm{disc}}})+1\\right)^{2}\\right]\n+\nw\ng\nâ€‹\np\nâ€‹\nğ”¼\nd\nğ’Ÿ\nmotion\nâ€‹\n(\no\nt\ndisc\n,\no\nt\n+\n1\ndisc\n)\nâ€‹\n[\nâ€–\nâ–½\nâ€‹\nC\nâ€‹\n(\no\nt\ndisc\n,\no\nt\n+\n1\ndisc\n)\nâ€–\n2\n]\n,\n\\displaystyle+w_{gp}\\mathbb{E}_{d^{\\mathcal{D}_{\\mathrm{motion}}}({o_{t}}^{\\mathrm{disc}},{o_{t+1}^{\\mathrm{disc}}})}\\left[\\left\\lVert\\triangledown C({o_{t}}^{\\mathrm{disc}},{o_{t+1}^{\\mathrm{disc}}})\\right\\rVert^{2}\\right],\nC.3\nReward Formulation\nTo encourage effective engagement, active striking, and robust defensive behaviors in humanoid combat, we design a hierarchical and compositional reward function. The overall reward integrates geometric alignment, locomotion, contact-based interaction, and terminal dominance, enabling stable learning under self-play settings.\nOverall Objective.\nThe warm-up reward is defined as a weighted combination of task-oriented and style-oriented objectives:\nr\nwarmup\n=\nw\ntask\nâ€‹\nr\ntask\n+\nw\nstyle\nâ€‹\nr\nstyle\n,\nr_{\\mathrm{warmup}}=w_{\\mathrm{task}}r_{\\mathrm{task}}+w_{\\mathrm{style}}r_{\\mathrm{style}},\nwhere\nr\nstyle\nr_{\\mathrm{style}}\nencourages motion naturalness, and\nr\ntask\nr_{\\mathrm{task}}\nfocuses on combat effectiveness.\nTask-Level Decomposition.\nThe task reward is formulated as\nr\ntask\n=\n\\displaystyle r_{\\mathrm{task}}=\nw\nface\nâ€‹\nr\nface\n+\nw\nvel\nâ€‹\nr\nvel\n\\displaystyle\\;w_{\\mathrm{face}}r_{\\mathrm{face}}+w_{\\mathrm{vel}}r_{\\mathrm{vel}}\n+\nw\ndist\nâ€‹\nr\ndist\n+\nw\nhit\nâ€‹\nr\nhit\n\\displaystyle+w_{\\mathrm{dist}}r_{\\mathrm{dist}}+w_{\\mathrm{hit}}r_{\\mathrm{hit}}\nwhere each component captures a distinct aspect of humanoid combat behavior.\nHierarchical Design.\nThe reward structure follows a layered principle:\n(i) geometric alignment and distance shaping,\n(ii) velocity-oriented locomotion, and\n(iii) contact-based striking behaviors.\nThis hierarchy stabilizes optimization and mitigates reward exploitation.\nFacing Alignment Reward.\nWe encourage the agent to orient its torso toward the opponent target.\nLet\nğ©\ne\n\\mathbf{p}_{e}\nand\nğ©\nt\n\\mathbf{p}_{t}\ndenote the ego root position and the opponent target position. The horizontal target direction in the world frame is defined as\nğ\nw\n=\nÏ€\nx\nâ€‹\ny\nâ€‹\n(\nğ©\nt\nâˆ’\nğ©\ne\n)\nâ€–\nÏ€\nx\nâ€‹\ny\nâ€‹\n(\nğ©\nt\nâˆ’\nğ©\ne\n)\nâ€–\n,\n\\mathbf{d}_{w}=\\frac{\\pi_{xy}(\\mathbf{p}_{t}-\\mathbf{p}_{e})}{\\|\\pi_{xy}(\\mathbf{p}_{t}-\\mathbf{p}_{e})\\|},\nwhere\nÏ€\nx\nâ€‹\ny\nâ€‹\n(\nâ‹…\n)\n\\pi_{xy}(\\cdot)\nprojects a vector onto the horizontal plane.\nLet\nR\ne\nâˆˆ\nS\nâ€‹\nO\nâ€‹\n(\n3\n)\nR_{e}\\in SO(3)\nbe the rotation matrix corresponding to\nğª\ne\n\\mathbf{q}_{e}\n. The direction expressed in the ego local frame is\nğ\nl\n=\nR\ne\nâˆ’\n1\nâ€‹\nğ\nw\n=\n(\nd\nx\n,\nd\ny\n,\nd\nz\n)\nâŠ¤\n.\n\\mathbf{d}_{l}=R_{e}^{-1}\\mathbf{d}_{w}=(d_{x},d_{y},d_{z})^{\\top}.\nThe facing reward is defined as\nr\nface\n=\nexp\nâ¡\n(\nâˆ’\n1\nâˆ’\nd\nx\nÏƒ\nface\n)\n,\nr_{\\mathrm{face}}=\\exp\\!\\left(-\\frac{1-d_{x}}{\\sigma_{\\mathrm{face}}}\\right),\nwhere\nd\nx\nd_{x}\nrepresents the forward component of the target direction, and\nÏƒ\nface\n\\sigma_{\\mathrm{face}}\ncontrols the sharpness.\nVelocity Reward.\nWe reward locomotion along the line of sight to the opponent.\nLet\nğ¯\ne\n\\mathbf{v}_{e}\ndenote the ego root linear velocity and\nğ\n2\nâ€‹\nD\n\\mathbf{d}_{2D}\nthe normalized horizontal target direction:\nğ\n2\nâ€‹\nD\n=\n(\nğ©\nt\nâˆ’\nğ©\ne\n)\nx\nâ€‹\ny\nâ€–\n(\nğ©\nt\nâˆ’\nğ©\ne\n)\nx\nâ€‹\ny\nâ€–\n.\n\\mathbf{d}_{2D}=\\frac{(\\mathbf{p}_{t}-\\mathbf{p}_{e})_{xy}}{\\|(\\mathbf{p}_{t}-\\mathbf{p}_{e})_{xy}\\|}.\nThe projected velocity along the target direction is\nv\nâˆ¥\n=\nğ¯\ne\n,\nx\nâ€‹\ny\nâŠ¤\nâ€‹\nğ\n2\nâ€‹\nD\n.\nv_{\\parallel}=\\mathbf{v}_{e,xy}^{\\top}\\mathbf{d}_{2D}.\nGiven the desired approaching speed\nv\ntar\nv_{\\mathrm{tar}}\n, we define the velocity error\ne\nv\n=\nmax\nâ¡\n(\n0\n,\nv\ntar\nâˆ’\nv\nâˆ¥\n)\n,\ne_{v}=\\max(0,v_{\\mathrm{tar}}-v_{\\parallel}),\nand the reward\nr\nvel\n=\nğ•€\nâ€‹\n[\nv\nâˆ¥\n>\n0\n]\nâ€‹\nexp\nâ¡\n(\nâˆ’\ne\nv\n2\nÏƒ\nvel\n)\n,\nr_{\\mathrm{vel}}=\\mathbb{I}[v_{\\parallel}>0]\\exp\\!\\left(-\\frac{e_{v}^{2}}{\\sigma_{\\mathrm{vel}}}\\right),\nwhere\nÏƒ\nvel\n\\sigma_{\\mathrm{vel}}\ndetermines the scaler.\nDistance Reward.\nTo prevent passive exploitation, we introduce a velocity-gated distance reward term.\nLet\nğ¯\nw\ni\n\\mathbf{v}_{w_{i}}\nbe the velocity of wrist\ni\ni\nand\nğ®\n^\ni\n=\nğ©\nt\nâˆ’\nğ©\nw\ni\nâ€–\nğ©\nt\nâˆ’\nğ©\nw\ni\nâ€–\n+\nÏµ\n\\hat{\\mathbf{u}}_{i}=\\frac{\\mathbf{p}_{t}-\\mathbf{p}_{w_{i}}}{\\|\\mathbf{p}_{t}-\\mathbf{p}_{w_{i}}\\|+\\epsilon}\nbe the unit vector from wrist\ni\ni\nto target torso link.\nThe projected punching speed is\ns\ni\n=\nğ¯\nw\ni\nâ‹…\nğ®\n^\ni\n.\ns_{i}=\\mathbf{v}_{w_{i}}\\cdot\\hat{\\mathbf{u}}_{i}.\nA smooth gating function is defined as\ng\ni\n=\nÏƒ\nâ€‹\n(\nÎ±\nâ€‹\n(\ns\ni\nâˆ’\nv\nth\n)\n)\n,\ng_{i}=\\sigma\\big(\\alpha(s_{i}-v_{\\mathrm{th}})\\big),\nwhere\nv\nth\nv_{\\mathrm{th}}\nis the speed threshold and\nÎ±\n\\alpha\ncontrols steepness.\nThe active hitting reward is\nr\ndist\n=\n1\nH\nâ€‹\nâˆ‘\ni\nexp\nâ¡\n(\nâˆ’\nd\ni\nÏƒ\ndist\n)\nâ€‹\ng\ni\n,\nr_{\\mathrm{dist}}=\\frac{1}{H}\\sum_{i}\\exp\\!\\left(-\\frac{d_{i}}{\\sigma_{\\mathrm{dist}}}\\right)g_{i},\nwith\nd\ni\nd_{i}\nrepresents the distance from wrist\ni\ni\nto the opponent torso.\nHit Reward.\nIn adversarial humanoid control, naive contact-based rewards often lead to degenerate behaviors such as body pushing, leaning, or passive collision, rather than deliberate punching actions. To address this issue, we design an offensive punch hit reward that jointly considers kinematic intent, geometric alignment, and physical contact.\nLet\nğ¯\nw\ni\n\\mathbf{v}_{w_{i}}\nbe the velocity of wrist\ni\ni\nand\nğ¯\nt\n\\mathbf{v}_{t}\nbe the velocity of the torso link. We define the relative punching velocities as:\nğ¯\nw\ni\nrel\n=\nğ¯\nw\ni\nâˆ’\nğ¯\nt\n,\n\\mathbf{v}_{w_{i}}^{\\mathrm{rel}}=\\mathbf{v}_{w_{i}}-\\mathbf{v}_{t},\nfor\ni\n=\nl\ni=l\nor\nr\nr\n.\nThese quantities represent the local striking motion of each fist with respect to the robotâ€™s body.\nWhen the agent only translates forward, the relative velocities turns to 0.\nThe effective attack speed is obtained by projecting relative velocity onto the attack direction:\ns\ni\n=\nğ¯\nğ°\nğ¢\nrel\nâ‹…\nğ®\n^\ni\n.\ns_{i}=\\mathbf{v_{w_{i}}^{\\mathrm{rel}}}\\cdot\\hat{\\mathbf{u}}_{i}.\nLet\nğ…\nt\nopp\n\\mathbf{F}^{\\mathrm{opp}}_{t}\nbe the contact force on opponentâ€™s torso link,\nğ…\ni\nego\n\\mathbf{F}^{\\mathrm{ego}}_{i}\nbe the contact forces on left or right wrists.\nWe define binary contact indicators:\nC\nt\nopp\n=\nğ•€\nâ€‹\n[\nâˆ¥\nğ…\nt\nopp\n>\nâ€‹\nÏ„\nf\n]\n,\nC\ni\nego\n=\nğ•€\nâ€‹\n[\nğ…\ni\nego\n>\nÏ„\nf\n]\n,\nC^{\\mathrm{opp}}_{t}=\\mathbb{I}[\\|\\mathbf{F}^{\\mathrm{opp}}_{t}>\\tau_{f}],~C_{i}^{\\mathrm{ego}}=\\mathbb{I}[\\mathbf{F}_{i}^{\\mathrm{ego}}>\\tau_{f}],\nthe valid hit signals for any wrist is:\nH\ni\n=\nC\ni\nego\nâˆ§\nC\nt\nopp\nâˆ§\ns\ni\n>\nÏ„\nv\n,\nH_{i}=C_{i}^{\\mathrm{ego}}\\land C^{\\mathrm{opp}}_{t}\\land s_{i}>\\tau_{v},\nr\nhit\n=\nğ•€\nâ€‹\n[\nH\nl\nâˆ¨\nH\nr\n]\nr_{\\mathrm{hit}}=\\mathbb{I}[H_{l}\\lor H_{r}]\nThis hit reward is a high-level sparse reward granted for landing a clean, significant blow on the opponent, incentivizing decisive offensive actions beyond simple contact.\nStyle Reward.\nThe style reward\nr\nstyle\nr_{\\mathrm{style}}\nis derived from the discriminatorâ€™s output to anchor the agentâ€™s behavior within the reference motion distribution:\nr\nstyle\nâ€‹\n(\ns\nt\n,\ns\nt\n+\n1\n)\n=\nmax\nâ¡\n[\n0\n,\n1\nâˆ’\n0.25\nâ€‹\n(\nC\nâ€‹\n(\no\nt\ndisc\n,\no\nt\n+\n1\ndisc\n)\nâˆ’\n1\n)\n2\n]\n.\nr_{\\mathrm{style}}(s_{t},s_{t+1})=\\max[0,1-0.25\\left(C(o_{t}^{\\mathrm{disc}},o_{t+1}^{\\mathrm{disc}})-1\\right)^{2}].\nReward Scheduling.\nTo ensure a stable transition from imitation to task execution, we implement a dynamic weight schedule. During the initial warmup epochs, the agent is driven primarily by\nr\nstyle\nr_{\\mathrm{style}}\nto master stable bipedal locomotion and boxing postures. Subsequently, the task reward weight\nw\ntask\nw_{\\mathrm{task}}\nis phased in, allowing the agent to optimize strike precision while adhering to the geometric and stylistic constraints imposed by the learned manifold and the AMP prior.\nAppendix D\nLatent-Space Neural Fictitious Self-Play\nThis section includes the pseudo code in Algorithm\n1\nand the training details of LS-NFSP.\nAlgorithm 1\nLatent-Space Neural Fictitious Self-Play (LS-NFSP)\n0:\nTwo-player zero-sum game environment\nâ„³\n\\mathcal{M}\n, anticipatory parameter\nÎ·\nâˆˆ\n[\n0\n,\n1\n]\n\\eta\\in[0,1]\n, horizon\nT\nT\n0:\nBest-response (RL) policies\n{\nÏ€\nz\n,\nÎ¸\ni\nRL\n}\ni\nâˆˆ\n{\n1\n,\n2\n}\n\\{\\pi^{\\mathrm{RL}}_{z,\\theta_{i}}\\}_{i\\in\\{1,2\\}}\n, average policies\n{\nÏ€\nÂ¯\nz\n,\nÏ•\ni\n}\ni\nâˆˆ\n{\n1\n,\n2\n}\n\\{\\bar{\\pi}_{z,\\phi_{i}}\\}_{i\\in\\{1,2\\}}\n0:\nOn-policy rollout buffers\n{\nğ’Ÿ\nRL\ni\n}\ni\nâˆˆ\n{\n1\n,\n2\n}\n\\{\\mathcal{D}^{i}_{\\mathrm{RL}}\\}_{i\\in\\{1,2\\}}\n, reservoir buffers\n{\nğ’Ÿ\nSL\ni\n}\ni\nâˆˆ\n{\n1\n,\n2\n}\n\\{\\mathcal{D}^{i}_{\\mathrm{SL}}\\}_{i\\in\\{1,2\\}}\n1:\nfor\niteration\nk\n=\n1\n,\n2\n,\nâ€¦\nk=1,2,\\dots\ndo\n2:\nReset\nâ„³\n\\mathcal{M}\nand observe\no\n1\n1\n,\no\n1\n2\no_{1}^{1},o_{1}^{2}\n{\no\nt\ni\no_{t}^{i}\ncan include\n(\ns\nt\nprop\n,\ni\n,\ns\nt\ngoal\n,\ni\n)\n(s_{t}^{\\mathrm{prop},i},s_{t}^{\\mathrm{goal},i})\n}\n3:\nfor\nt\n=\n1\n,\n2\n,\nâ€¦\n,\nT\nt=1,2,\\dots,T\ndo\n4:\nfor all\nplayers\ni\nâˆˆ\n{\n1\n,\n2\n}\ni\\in\\{1,2\\}\ndo\n5:\nSample mode\nm\ni\nâˆ¼\nBernoulli\nâ€‹\n(\nÎ·\n)\nm_{i}\\sim\\mathrm{Bernoulli}(\\eta)\n{\nm\ni\n=\n1\nm_{i}{=}1\n: best-response;\nm\ni\n=\n0\nm_{i}{=}0\n: average}\n6:\nif\nm\ni\n=\n1\nm_{i}=1\nthen\n7:\nSample latent action\nz\nt\ni\nâˆ¼\nÏ€\nz\n,\nÎ¸\ni\nRL\n(\nâ‹…\nâˆ£\no\nt\ni\n)\nz_{t}^{i}\\sim\\pi^{\\mathrm{RL}}_{z,\\theta_{i}}(\\cdot\\mid o_{t}^{i})\n8:\nInsert\n(\no\nt\ni\n,\nz\nt\ni\n)\n(o_{t}^{i},z_{t}^{i})\ninto\nğ’Ÿ\nSL\ni\n\\mathcal{D}^{i}_{\\mathrm{SL}}\nvia reservoir sampling\n9:\nelse\n10:\nSample latent action\nz\nt\ni\nâˆ¼\nÏ€\nÂ¯\nz\n,\nÏ•\ni\n(\nâ‹…\nâˆ£\no\nt\ni\n)\nz_{t}^{i}\\sim\\bar{\\pi}_{z,\\phi_{i}}(\\cdot\\mid o_{t}^{i})\n11:\nend\nif\n12:\nend\nfor\n13:\nStep environment with joint latent action\n(\nz\nt\n1\n,\nz\nt\n2\n)\n(z_{t}^{1},z_{t}^{2})\n; receive rewards\n(\nr\nt\n1\n,\nr\nt\n2\n)\n(r_{t}^{1},r_{t}^{2})\nand next obs\n(\no\nt\n+\n1\n1\n,\no\nt\n+\n1\n2\n)\n(o_{t+1}^{1},o_{t+1}^{2})\n14:\nfor all\nplayers\ni\nâˆˆ\n{\n1\n,\n2\n}\ni\\in\\{1,2\\}\ndo\n15:\nAppend transition\n(\no\nt\ni\n,\nz\nt\ni\n,\nr\nt\ni\n,\no\nt\n+\n1\ni\n)\n(o_{t}^{i},z_{t}^{i},r_{t}^{i},o_{t+1}^{i})\nto\nğ’Ÿ\nRL\ni\n\\mathcal{D}^{i}_{\\mathrm{RL}}\n16:\nend\nfor\n17:\nend\nfor\n18:\nfor all\nplayers\ni\nâˆˆ\n{\n1\n,\n2\n}\ni\\in\\{1,2\\}\ndo\n19:\nUpdate\nÎ¸\ni\n\\theta_{i}\nwith PPO on\nğ’Ÿ\nRL\ni\n\\mathcal{D}^{i}_{\\mathrm{RL}}\n20:\nUpdate\nÏ•\ni\n\\phi_{i}\nby minimizing\nâ„’\nSL\n=\nğ”¼\n(\no\n,\nz\n)\nâˆ¼\nğ’Ÿ\nSL\ni\nâ€‹\n[\nâ€–\nÏ€\nÂ¯\nz\n,\nÏ•\ni\nâ€‹\n(\no\n)\nâˆ’\nz\nâ€–\n2\n]\n\\mathcal{L}_{\\mathrm{SL}}=\\mathbb{E}_{(o,z)\\sim\\mathcal{D}^{i}_{\\mathrm{SL}}}\\big[\\|\\bar{\\pi}_{z,\\phi_{i}}(o)-z\\|^{2}\\big]\n21:\nend\nfor\n22:\nend\nfor\n23:\nreturn\nAverage policies\n{\nÏ€\nÂ¯\nz\n,\nÏ•\ni\n}\ni\nâˆˆ\n{\n1\n,\n2\n}\n\\{\\bar{\\pi}_{z,\\phi_{i}}\\}_{i\\in\\{1,2\\}}\nD.1\nDual-Policy Architecture.\nTo implement fictitious play, each agent maintains a dual-policy system. This system includes a best response policy (\nÏ€\nz\nR\nâ€‹\nL\n\\pi_{z}^{RL}\n) and a average policy (\nÏ€\nÂ¯\nz\n\\bar{\\pi}_{z}\n). The best response policy (\nÏ€\nz\nR\nâ€‹\nL\n\\pi_{z}^{RL}\n) is an on-policy actor optimized via PPO\n(Schulman\net al.\n,\n2017\n)\nto exploit the opponentâ€™s current average strategy. The average policy (\nÏ€\nÂ¯\nz\n\\bar{\\pi}_{z}\n) is a supervised learning network that approximates the agentâ€™s historical distribution of best-response behaviors, acting as a stable strategic anchor.\nD.2\nReservoir-Based Strategy Buffer.\nTo ensure that the average policy represents a uniform sampling of the agentâ€™s behavioral history, we utilize a reservoir sampling mechanism. Each agent stores its latent-space trajectories\n(\ns\nt\nprop\n,\ns\nt\ngoal\n,\nz\nt\n)\n(s_{t}^{\\mathrm{prop}},s_{t}^{\\mathrm{goal}},z_{t})\nin a buffer\nğ’Ÿ\nS\nâ€‹\nL\n\\mathcal{D}_{SL}\nof capacity\nK\nK\n. For every new experience\nn\n>\nK\nn>K\n, the sample replaces a random existing entry with probability\nP\n=\nK\n/\nn\nP=K/n\n. This prevents the average strategy from being biased toward recent iterations and preserves tactical diversity.\nD.3\nReward Formulation.\nTransitioning from the single-agent warmup to a zero-sum game requires an expanded task reward\nr\nexpand\nr_{\\mathrm{expand}}\n, which incorporates competitive outcome conditions and defensive necessity. In addition to the basic striking and facing rewards, we introduce the following competitive terms:\nr\nexpand\n=\nw\nstr\nâ€‹\nr\nstr\nâˆ’\nw\ndef\nâ€‹\nr\ndef\n+\nw\nterm\nâ€‹\nr\nterm\n,\nr_{\\mathrm{expand}}=w_{\\mathrm{str}}r_{\\mathrm{str}}-w_{\\mathrm{def}}r_{\\mathrm{def}}+w_{\\mathrm{term}}r_{\\mathrm{term}},\nStrike Force Reward.\nWe encourage effective physical impacts using contact forces.\nLet\nğ…\nopp\n\\mathbf{F}^{\\mathrm{opp}}\nand\nğ…\nego\n\\mathbf{F}^{\\mathrm{ego}}\ndenote the net contact forces measured on the opponent and ego torso body, respectively. The reward is defined as\nr\nstr\n=\nâ€–\nğ…\nk\nopp\nâ€–\nâˆ’\nâ€–\nğ…\nk\nego\nâ€–\n,\nr_{\\mathrm{str}}=\\|\\mathbf{F}^{\\mathrm{opp}}_{k}\\|-\\|\\mathbf{F}^{\\mathrm{ego}}_{k}\\|,\nDefensive Penalty.\nTo penalize being hit by high-velocity strikes, we define\nÎ´\ni\n=\nğ•€\nâ€‹\n[\nf\ni\n>\nÏ„\nf\nâˆ§\ns\ni\n>\nÏ„\nv\n]\n,\n\\delta_{i}=\\mathbb{I}\\big[f_{i}>\\tau_{f}\\wedge s_{i}>\\tau_{v}\\big],\nr\ndef\n=\nğ•€\nâ€‹\n[\nÎ´\nl\nâˆ¨\nÎ´\nr\n]\n.\nr_{\\mathrm{def}}=\\mathbb{I}[\\delta_{l}\\vee\\delta_{r}].\nTerminal Outcome Reward.\nThe terminal reward reflects episode outcomes:\nr\nterm\n=\nğ•€\nâ€‹\n[\nh\nopp\n<\nh\nmin\n]\nâˆ’\nğ•€\nâ€‹\n[\nh\nego\n<\nh\nmin\n]\n.\nr_{\\mathrm{term}}=\\mathbb{I}[h_{\\mathrm{opp}}<h_{\\min}]-\\mathbb{I}[h_{\\mathrm{ego}}<h_{\\min}].\nSummary.\nThe total task reward in this competitive stage is:\nr\ncom\n=\nr\nwarmup\n+\nr\nexpand\nr_{\\mathrm{com}}=r_{\\mathrm{warmup}}+r_{\\mathrm{expand}}\nThe detailed reward terms, weights and hyperparameters are shown in Table\n5\n.\nTable 5\n:\nSummary of Boxing Reward Components in Competitive Stage.\nTerm\nFormula\nHyperparameters\nWeight\nDescription\nr\nface\nr_{\\mathrm{face}}\nexp\nâ¡\n(\nâˆ’\n1\nâˆ’\nd\nx\nÏƒ\nface\n)\n\\exp(-\\frac{1-d_{x}}{\\sigma_{\\mathrm{face}}})\nÏƒ\nface\n=\n0.5\n\\sigma_{\\mathrm{face}}=0.5\nw\nface\n=\n1.2\nw_{\\mathrm{face}}=1.2\nFacing alignment\nr\nvel\nr_{\\mathrm{vel}}\nğ•€\nâ€‹\n[\nv\nâˆ¥\n>\n0\n]\nâ€‹\nexp\nâ¡\n(\nâˆ’\ne\nv\n2\nÏƒ\nvel\n)\n\\mathbb{I}[v_{\\parallel}>0]\\exp(-\\frac{e_{v}^{2}}{\\sigma_{\\mathrm{vel}}})\nÏƒ\nvel\n=\n0.25\n\\sigma_{\\mathrm{vel}}=0.25\n,\nv\ntar\n=\n1.0\nv_{\\mathrm{tar}}=1.0\nw\nvel\n=\n0.5\nw_{\\mathrm{vel}}=0.5\nVelocity\nr\ndist\nr_{\\mathrm{dist}}\n1\nH\nâ€‹\nâˆ‘\ne\nâˆ’\nd\ni\n/\nÏƒ\nâ€‹\ng\ni\n\\frac{1}{H}\\sum e^{-d_{i}/\\sigma}g_{i}\nÏƒ\ndist\n=\n1.0\n\\sigma_{\\mathrm{dist}}=1.0\n,\nv\nth\n=\n0.8\nv_{\\mathrm{th}}=0.8\n,\nÎ±\n=\n10.0\n\\alpha=10.0\nw\ndist\n=\n1.5\nw_{\\mathrm{dist}}=1.5\nVelocity-gated approach\nr\nhit\nr_{\\mathrm{hit}}\nğ•€\nâ€‹\n[\nH\nl\nâˆ¨\nH\nr\n]\n\\mathbb{I}[H_{l}\\lor H_{r}]\nÏ„\nf\n=\n1.0\n,\nÏ„\nv\n=\n1.0\n\\tau_{f}=1.0,\\tau_{v}=1.0\nw\nhit\n=\n50\nw_{\\mathrm{hit}}=50\nOffensive hit\nr\ndef\nr_{\\mathrm{def}}\nğ•€\nâ€‹\n[\nÎ´\nl\nâˆ¨\nÎ´\nr\n]\n\\mathbb{I}[\\delta_{l}\\vee\\delta_{r}]\nÏ„\nf\n=\n1.0\n\\tau_{f}=1.0\n,\nÏ„\nv\n=\n1.0\n\\tau_{v}=1.0\nw\ndef\n=\n8.0\nw_{\\mathrm{def}}=8.0\nDefensive penalty\nr\nstr\nr_{\\mathrm{str}}\nâ€–\nğ…\nopp\nâ€–\nâˆ’\nâ€–\nğ…\nego\nâ€–\n\\|\\mathbf{F}^{\\mathrm{opp}}\\|-\\|\\mathbf{F}^{\\mathrm{ego}}\\|\nâ€“\nw\nstr\n=\n0.3\nw_{\\mathrm{str}}=0.3\nDelta striking force\nr\nterm\nr_{\\mathrm{term}}\nğ•€\nâ€‹\n[\nh\nopp\n<\nh\nmin\n]\nâˆ’\nğ•€\nâ€‹\n[\nh\nego\n<\nh\nmin\n]\n\\mathbb{I}[h_{\\mathrm{opp}}<h_{\\min}]-\\mathbb{I}[h_{\\mathrm{ego}}<h_{\\min}]\nh\nmin\n=\n0.4\nh_{\\min}=0.4\nw\nterm\n=\n0.3\nw_{\\mathrm{term}}=0.3\nTerminal outcome\nD.4\nTraining Details\nLatent Action Mixing.\nDuring interaction, agents select their behavior based on an exploration parameter\nÎ·\nâˆˆ\n[\n0\n,\n1\n]\n\\eta\\in[0,1]\n. The latent action\nz\nz\nis sampled according to a mixture strategy\nÏƒ\n=\nÎ·\nâ‹…\nÏ€\nÂ¯\nz\n+\n(\n1\nâˆ’\nÎ·\n)\nâ‹…\nÏ€\nz\nR\nâ€‹\nL\n\\sigma=\\eta\\cdot\\bar{\\pi}_{z}+(1-\\eta)\\cdot\\pi^{RL}_{z}\n, i.e.,\nz\nâˆ¼\n{\nÏ€\nz\nR\nâ€‹\nL\nâ€‹\n(\ns\n)\n,\nwith probability\nâ€‹\n1\nâˆ’\nÎ·\n,\nÏ€\nÂ¯\nz\nâ€‹\n(\ns\n)\n,\nwith probability\nâ€‹\nÎ·\n.\nz\\sim\\begin{cases}\\pi^{RL}_{z}(s),&\\text{with probability }1-\\eta,\\\\\n\\bar{\\pi}_{z}(s),&\\text{with probability }\\eta.\\end{cases}\nOptimization Objectives.\nThe best response policy is updated using the PPO objective to maximize the hybrid task and style reward. Simultaneously, the average policy\nÏ€\nÂ¯\nz\n\\bar{\\pi}_{z}\nis optimized by minimizing the mean squared error (MSE) relative to the stored best-response actions:\nâ„’\nS\nâ€‹\nL\n=\nğ”¼\n(\ns\nprop\n,\ns\ngoal\n,\nz\n)\nâˆ¼\nğ’Ÿ\nS\nâ€‹\nL\nâ€‹\n[\nâ€–\nÏ€\nÂ¯\nz\nâ€‹\n(\ns\nprop\n,\ns\ngoal\n)\nâˆ’\nz\nâ€–\n2\n]\n\\mathcal{L}_{SL}=\\mathbb{E}_{(s^{\\mathrm{prop}},s^{\\mathrm{goal}},z)\\sim\\mathcal{D}_{SL}}\\left[\\|\\bar{\\pi}_{z}(s^{\\mathrm{prop}},s^{\\mathrm{goal}})-z\\|^{2}\\right]\nThrough this competitive co-evolution, the agents develop sophisticated boxing behaviors, while the latent-space constraints ensure that all maneuvers remain within the bounds of physical feasibility and stylistic authenticity.\nAppendix E\nProof Sketch of Proposition\n3.5\nProof.\nLet\nğ’µ\n\\mathcal{Z}\nbe a compact metric space and let\nu\n:\nğ’µ\nÃ—\nğ’µ\nâ†’\nâ„\nu:\\mathcal{Z}\\times\\mathcal{Z}\\to\\mathbb{R}\nbe the (induced) bounded payoff for playerÂ 1, with playerÂ 2 receiving\nâˆ’\nu\n-u\n.\nDenote\n|\nu\nâ€‹\n(\nz\n1\n,\nz\n2\n)\n|\nâ‰¤\nU\n|u(z_{1},z_{2})|\\leq U\nfor all\n(\nz\n1\n,\nz\n2\n)\n(z_{1},z_{2})\n(Assumption\n3.3\n).\nWe consider mixed strategies as probability measures over\nğ’µ\n\\mathcal{Z}\n.\nFor distributions\nÎ¼\n,\nÎ½\nâˆˆ\nÎ”\nâ€‹\n(\nğ’µ\n)\n\\mu,\\nu\\in\\Delta(\\mathcal{Z})\n, define the bilinear extension\nu\nâ€‹\n(\nÎ¼\n,\nÎ½\n)\nâ‰œ\nğ”¼\nz\n1\nâˆ¼\nÎ¼\n,\nz\n2\nâˆ¼\nÎ½\nâ€‹\n[\nu\nâ€‹\n(\nz\n1\n,\nz\n2\n)\n]\n.\nu(\\mu,\\nu)\\triangleq\\mathbb{E}_{z_{1}\\sim\\mu,\\,z_{2}\\sim\\nu}\\big[u(z_{1},z_{2})\\big].\n(18)\nBecause\nu\nu\nis bounded and measurable,\nu\nâ€‹\n(\nÎ¼\n,\nÎ½\n)\nu(\\mu,\\nu)\nis well-defined.\nMoreover, since\nğ’µ\n\\mathcal{Z}\nis compact and\nu\nu\nis continuous, the zero-sum game admits a value\nV\nâ‰œ\nmax\nÎ¼\nâˆˆ\nÎ”\nâ€‹\n(\nğ’µ\n)\nâ¡\nmin\nÎ½\nâˆˆ\nÎ”\nâ€‹\n(\nğ’µ\n)\nâ¡\nu\nâ€‹\n(\nÎ¼\n,\nÎ½\n)\n=\nmin\nÎ½\nâˆˆ\nÎ”\nâ€‹\n(\nğ’µ\n)\nâ¡\nmax\nÎ¼\nâˆˆ\nÎ”\nâ€‹\n(\nğ’µ\n)\nâ¡\nu\nâ€‹\n(\nÎ¼\n,\nÎ½\n)\n,\nV\\triangleq\\max_{\\mu\\in\\Delta(\\mathcal{Z})}\\min_{\\nu\\in\\Delta(\\mathcal{Z})}u(\\mu,\\nu)=\\min_{\\nu\\in\\Delta(\\mathcal{Z})}\\max_{\\mu\\in\\Delta(\\mathcal{Z})}u(\\mu,\\nu),\n(19)\nand at least one Nash equilibrium\n(\nÎ¼\nâ‹†\n,\nÎ½\nâ‹†\n)\n(\\mu^{\\star},\\nu^{\\star})\nexists (standard minimax on compact domains).\nLet\n(\nz\n1\nt\n,\nz\n2\nt\n)\nt\n=\n1\nT\n(z_{1}^{t},z_{2}^{t})_{t=1}^{T}\nbe the sequence generated by the learning process. Define the empirical distributions (uniform averages)\nÎ¼\nÂ¯\nT\nâ‰œ\n1\nT\nâ€‹\nâˆ‘\nt\n=\n1\nT\nÎ´\nz\n1\nt\n,\nÎ½\nÂ¯\nT\nâ‰œ\n1\nT\nâ€‹\nâˆ‘\nt\n=\n1\nT\nÎ´\nz\n2\nt\n,\n\\bar{\\mu}_{T}\\triangleq\\frac{1}{T}\\sum_{t=1}^{T}\\delta_{z_{1}^{t}},\\qquad\\bar{\\nu}_{T}\\triangleq\\frac{1}{T}\\sum_{t=1}^{T}\\delta_{z_{2}^{t}},\n(20)\nwhere\nÎ´\nz\n\\delta_{z}\nis a point mass at\nz\nz\n. Let\nÎ½\nt\nâˆ’\n1\n\\nu_{t-1}\ndenote playerÂ 2â€™s empirical distribution up to time\nt\nâˆ’\n1\nt-1\n:\nÎ½\nt\nâˆ’\n1\n=\n1\nt\nâˆ’\n1\nâ€‹\nâˆ‘\ns\n=\n1\nt\nâˆ’\n1\nÎ´\nz\n2\ns\n\\nu_{t-1}=\\frac{1}{t-1}\\sum_{s=1}^{t-1}\\delta_{z_{2}^{s}}\n(similarly\nÎ¼\nt\nâˆ’\n1\n\\mu_{t-1}\n). Assumption\n3.4\nstates that the updates are approximate best responses: for some error sequences\n(\nÏµ\n1\nt\n,\nÏµ\n2\nt\n)\n(\\epsilon_{1}^{t},\\epsilon_{2}^{t})\nwith\nÏµ\ni\nt\nâ‰¥\n0\n\\epsilon_{i}^{t}\\geq 0\n,\nu\nâ€‹\n(\nz\n1\nt\n,\nÎ½\nt\nâˆ’\n1\n)\n\\displaystyle u(z_{1}^{t},\\nu_{t-1})\nâ‰¥\nmax\nz\n1\nâˆˆ\nğ’µ\nâ¡\nu\nâ€‹\n(\nz\n1\n,\nÎ½\nt\nâˆ’\n1\n)\nâˆ’\nÏµ\n1\nt\n,\n\\displaystyle\\geq\\max_{z_{1}\\in\\mathcal{Z}}u(z_{1},\\nu_{t-1})-\\epsilon_{1}^{t},\n(21)\nu\nâ€‹\n(\nÎ¼\nt\nâˆ’\n1\n,\nz\n2\nt\n)\n\\displaystyle u(\\mu_{t-1},z_{2}^{t})\nâ‰¤\nmin\nz\n2\nâˆˆ\nğ’µ\nâ¡\nu\nâ€‹\n(\nÎ¼\nt\nâˆ’\n1\n,\nz\n2\n)\n+\nÏµ\n2\nt\n.\n\\displaystyle\\leq\\min_{z_{2}\\in\\mathcal{Z}}u(\\mu_{t-1},z_{2})+\\epsilon_{2}^{t}.\n(22)\nEquivalently, playerÂ 2 approximately best-responds to\nminimize\nplayerÂ 1â€™s payoff.\nGiven the regrets against the realized opponent actions as\nR\n1\nâ€‹\n(\nT\n)\n\\displaystyle R_{1}(T)\nâ‰œ\nmax\nz\n1\nâˆˆ\nğ’µ\nâ€‹\nâˆ‘\nt\n=\n1\nT\nu\nâ€‹\n(\nz\n1\n,\nz\n2\nt\n)\nâˆ’\nâˆ‘\nt\n=\n1\nT\nu\nâ€‹\n(\nz\n1\nt\n,\nz\n2\nt\n)\n,\n\\displaystyle\\triangleq\\max_{z_{1}\\in\\mathcal{Z}}\\sum_{t=1}^{T}u(z_{1},z_{2}^{t})\\;-\\;\\sum_{t=1}^{T}u(z_{1}^{t},z_{2}^{t}),\nR\n2\nâ€‹\n(\nT\n)\n\\displaystyle R_{2}(T)\nâ‰œ\nâˆ‘\nt\n=\n1\nT\nu\nâ€‹\n(\nz\n1\nt\n,\nz\n2\nt\n)\nâˆ’\nmin\nz\n2\nâˆˆ\nğ’µ\nâ€‹\nâˆ‘\nt\n=\n1\nT\nu\nâ€‹\n(\nz\n1\nt\n,\nz\n2\n)\n.\n\\displaystyle\\triangleq\\sum_{t=1}^{T}u(z_{1}^{t},z_{2}^{t})\\;-\\;\\min_{z_{2}\\in\\mathcal{Z}}\\sum_{t=1}^{T}u(z_{1}^{t},z_{2}).\nThen, a standard zero-sum argument shows that for any sequences, the empirical distributions satisfy\nmax\nz\n1\nâˆˆ\nğ’µ\nâ¡\nu\nâ€‹\n(\nz\n1\n,\nÎ½\nÂ¯\nT\n)\nâˆ’\nu\nâ€‹\n(\nÎ¼\nÂ¯\nT\n,\nÎ½\nÂ¯\nT\n)\nâ‰¤\nR\n1\nâ€‹\n(\nT\n)\nT\n,\nu\nâ€‹\n(\nÎ¼\nÂ¯\nT\n,\nÎ½\nÂ¯\nT\n)\nâˆ’\nmin\nz\n2\nâˆˆ\nğ’µ\nâ¡\nu\nâ€‹\n(\nÎ¼\nÂ¯\nT\n,\nz\n2\n)\nâ‰¤\nR\n2\nâ€‹\n(\nT\n)\nT\n,\n\\max_{z_{1}\\in\\mathcal{Z}}u(z_{1},\\bar{\\nu}_{T})\\;-\\;u(\\bar{\\mu}_{T},\\bar{\\nu}_{T})\\;\\leq\\;\\frac{R_{1}(T)}{T},\\qquad u(\\bar{\\mu}_{T},\\bar{\\nu}_{T})\\;-\\;\\min_{z_{2}\\in\\mathcal{Z}}u(\\bar{\\mu}_{T},z_{2})\\;\\leq\\;\\frac{R_{2}(T)}{T},\n(23)\nbecause\nu\nâ€‹\n(\nz\n1\n,\nÎ½\nÂ¯\nT\n)\n=\n1\nT\nâ€‹\nâˆ‘\nt\n=\n1\nT\nu\nâ€‹\n(\nz\n1\n,\nz\n2\nt\n)\nu(z_{1},\\bar{\\nu}_{T})=\\frac{1}{T}\\sum_{t=1}^{T}u(z_{1},z_{2}^{t})\nand similarly for the other term.\nThus, if both average regrets are small,\n(\nÎ¼\nÂ¯\nT\n,\nÎ½\nÂ¯\nT\n)\n(\\bar{\\mu}_{T},\\bar{\\nu}_{T})\nis an approximate Nash Equilibrium, i.e.,\nmax\nz\n1\nâ¡\nu\nâ€‹\n(\nz\n1\n,\nÎ½\nÂ¯\nT\n)\nâˆ’\nu\nâ€‹\n(\nÎ¼\nÂ¯\nT\n,\nÎ½\nÂ¯\nT\n)\nâ‰¤\nÏµ\n,\nu\nâ€‹\n(\nÎ¼\nÂ¯\nT\n,\nÎ½\nÂ¯\nT\n)\nâˆ’\nmin\nz\n2\nâ¡\nu\nâ€‹\n(\nÎ¼\nÂ¯\nT\n,\nz\n2\n)\nâ‰¤\nÏµ\nâ‡’\n(\nÎ¼\nÂ¯\nT\n,\nÎ½\nÂ¯\nT\n)\nâ€‹\nis an\nÏµ\n-Nash Equilibrium.\n\\max_{z_{1}}u(z_{1},\\bar{\\nu}_{T})-u(\\bar{\\mu}_{T},\\bar{\\nu}_{T})\\leq\\epsilon,\\quad u(\\bar{\\mu}_{T},\\bar{\\nu}_{T})-\\min_{z_{2}}u(\\bar{\\mu}_{T},z_{2})\\leq\\epsilon\\Rightarrow\\ (\\bar{\\mu}_{T},\\bar{\\nu}_{T})\\text{ is an $\\epsilon$-Nash Equilibrium.}\n(24)\nThe dynamics (\n21\n)â€“(\n22\n) correspond to a follow-the-leader / best-response-to-average procedure. In zero-sum games with bounded payoffs, this yields sublinear regret up to logarithmic factors; informally, best-responding to the running average prevents persistent exploitation.\nConcretely, one can show that there exists a constant\nC\nC\ndepending only on the payoff range (e.g.,\nC\n=\n2\nâ€‹\nU\nC=2U\nsuffices for a crude bound)\n(Brown,\n1951\n; Zinkevich\net al.\n,\n2007\n)\nsuch that\nR\n1\nâ€‹\n(\nT\n)\nâ‰¤\nC\nâ€‹\nlog\nâ¡\nT\n+\nâˆ‘\nt\n=\n1\nT\nÏµ\n1\nt\n,\nR\n2\nâ€‹\n(\nT\n)\nâ‰¤\nC\nâ€‹\nlog\nâ¡\nT\n+\nâˆ‘\nt\n=\n1\nT\nÏµ\n2\nt\n.\nR_{1}(T)\\leq C\\log T+\\sum_{t=1}^{T}\\epsilon_{1}^{t},\\qquad R_{2}(T)\\leq C\\log T+\\sum_{t=1}^{T}\\epsilon_{2}^{t}.\n(25)\nCombining (\n23\n) and (\n25\n) gives\nmax\nz\n1\nâ¡\nu\nâ€‹\n(\nz\n1\n,\nÎ½\nÂ¯\nT\n)\nâˆ’\nu\nâ€‹\n(\nÎ¼\nÂ¯\nT\n,\nÎ½\nÂ¯\nT\n)\nâ‰¤\nC\nâ€‹\nlog\nâ¡\nT\nT\n+\n1\nT\nâ€‹\nâˆ‘\nt\n=\n1\nT\nÏµ\n1\nt\n,\n\\max_{z_{1}}u(z_{1},\\bar{\\nu}_{T})-u(\\bar{\\mu}_{T},\\bar{\\nu}_{T})\\leq\\frac{C\\log T}{T}+\\frac{1}{T}\\sum_{t=1}^{T}\\epsilon_{1}^{t},\n(26)\nand\nu\nâ€‹\n(\nÎ¼\nÂ¯\nT\n,\nÎ½\nÂ¯\nT\n)\nâˆ’\nmin\nz\n2\nâ¡\nu\nâ€‹\n(\nÎ¼\nÂ¯\nT\n,\nz\n2\n)\nâ‰¤\nC\nâ€‹\nlog\nâ¡\nT\nT\n+\n1\nT\nâ€‹\nâˆ‘\nt\n=\n1\nT\nÏµ\n2\nt\n.\nu(\\bar{\\mu}_{T},\\bar{\\nu}_{T})-\\min_{z_{2}}u(\\bar{\\mu}_{T},z_{2})\\leq\\frac{C\\log T}{T}+\\frac{1}{T}\\sum_{t=1}^{T}\\epsilon_{2}^{t}.\n(27)\nTherefore, letting\nÏµ\nT\nâ‰œ\nC\nâ€‹\nlog\nâ¡\nT\nT\n+\n1\nT\nâ€‹\nâˆ‘\nt\n=\n1\nT\n(\nÏµ\n1\nt\n+\nÏµ\n2\nt\n)\n,\n\\epsilon_{T}\\triangleq\\frac{C\\log T}{T}+\\frac{1}{T}\\sum_{t=1}^{T}(\\epsilon_{1}^{t}+\\epsilon_{2}^{t}),\n(28)\nwe obtain that\n(\nÎ¼\nÂ¯\nT\n,\nÎ½\nÂ¯\nT\n)\n(\\bar{\\mu}_{T},\\bar{\\nu}_{T})\nis an\nÏµ\nT\n\\epsilon_{T}\n-Nash equilibrium of the induced game.\nAs\nT\nâ†’\nâˆ\nT\\to\\infty\n, if the average approximation errors vanish (or remain bounded),\nÏµ\nT\nâ†’\n0\n\\epsilon_{T}\\to 0\n(or to the\naverage error floor), establishing the claimed convergence to an approximate Nash Equilibrium.\nâˆ\nAppendix F\nExperiment Details\nTraining Setup.\nWe implement our framework using Isaac Lab\n(NVIDIA\net al.\n,\n2025\n)\non the NVIDIA Omniverse platform, leveraging massive parallelization with 4,096 environments for large-scale training. The simulated Unitree G1 humanoid\n(Unitree,\n2024\n)\nutilizes 29 degrees of freedom, with the physics engine operating at 200 Hz and the control policy at 50 Hz. Strategic co-evolution is conducted over a 32-dimensional latent manifold.\nMulti-agent Training.\nIn the final self-play stage, the LS-NFSP is configured with an exploration parameter\nÎ·\n=\n0.1\n\\eta=0.1\nand a strategy reservoir capacity of\nK\n=\n10\n6\nK=10^{6}\nsamples per agent. The reward function balances performance and style with a task reward weight\nw\ntask\nw_{\\text{task}}\nof\n0.8\n0.8\nand an AMP style reward weight\nw\nstyle\nw_{\\text{style}}\nof\n0.2\n0.2\n, while the prior loss coefficient\nÎ»\nprior\n\\lambda_{\\text{prior}}\nis set to\n0.001\n0.001\n. Training is executed on a single NVIDIA RTX 4090 GPU, incorporating domain randomization across friction, mass, and actuator gains to ensure robust tactical emergence.\nTo evaluate the tactical progression and physical performance of the LS-NFSP framework, we define a comprehensive set of metrics across three primary dimensions: tactical proficiency, physical stability, and stylistic authenticity. Win Rate (\nW\nâ€‹\nR\nWR\n) is defined as the percentage of matches won over a series of 20 evaluation bouts, where a match is lost by the agent that first contacts the ground with any body part other than the feet, thereby triggering a fall-based termination. To quantify offensive precision, we define Hit Efficiency (\nÎ·\nh\nâ€‹\ni\nâ€‹\nt\n\\eta_{hit}\n) as the ratio of successful strikes, which is identified by the proportion of contacts exceeding the force threshold\nF\nt\nâ€‹\nh\n=\n10\nâ€‹\nN\nF_{th}=10N\nto the total number of offensive attempts. Furthermore, the Engagement Rate (\nE\nâ€‹\nR\nER\n) evaluates the strategic positioning and operational persistence of an agent. It is calculated as the proportion of the episode duration in which the agent concurrently satisfies two spatial conditions: maintaining a distance\nd\nt\nd_{t}\nwithin the effective striking range\n[\n0.5\n,\n1.2\n]\n[0.5,1.2]\nand ensuring a facing alignment\ncos\nâ¡\n(\nÎ¸\nt\n)\n\\cos(\\theta_{t})\nthat exceeds a predefined threshold\nÏ„\nf\nâ€‹\na\nâ€‹\nc\nâ€‹\ne\n=\n0.9\n\\tau_{face}=0.9\n. This metric provides a rigorous measure of an agentâ€™s ability to maintain a viable combat posture, effectively penalizing tactical avoidance or non-confrontational movement\npatterns.\nPhysical stability is assessed through Base Orientation Stability (\nğ’ª\nb\nâ€‹\na\nâ€‹\ns\nâ€‹\ne\n\\mathcal{O}_{base}\n) and Torque Smoothness (\nğ’®\nÏ„\n\\mathcal{S}_{\\tau}\n). To ensure a positive correlation with control quality,\nğ’ª\nb\nâ€‹\na\nâ€‹\ns\nâ€‹\ne\n\\mathcal{O}_{base}\nis defined using an exponential kernels on the angular deviation:\nğ’ª\nb\nâ€‹\na\nâ€‹\ns\nâ€‹\ne\n=\nğ”¼\nâ€‹\n[\nexp\nâ¡\n(\nâˆ’\nâ€–\nğ \nb\nâ€‹\na\nâ€‹\ns\nâ€‹\ne\nâˆ’\nğ \nw\nâ€‹\no\nâ€‹\nr\nâ€‹\nl\nâ€‹\nd\nâ€–\n2\n)\n]\n\\mathcal{O}_{base}=\\mathbb{E}[\\exp(-\\|\\mathbf{g}_{base}-\\mathbf{g}_{world}\\|^{2})]\n, where\nğ \nb\nâ€‹\na\nâ€‹\ns\nâ€‹\ne\n\\mathbf{g}_{base}\nand\nğ \nw\nâ€‹\no\nâ€‹\nr\nâ€‹\nl\nâ€‹\nd\n\\mathbf{g}_{world}\nrepresent the gravity vector in the robotâ€™s local frame and the global frame, respectively. A higher\nğ’ª\nb\nâ€‹\na\nâ€‹\ns\nâ€‹\ne\n\\mathcal{O}_{base}\nscore indicates a superior ability to maintain an upright posture under competitve perturbations. Torque Smoothness (\nğ’®\nÏ„\n\\mathcal{S}_{\\tau}\n) measures the magnitude of high-frequency oscillations in motor commands, calculated as the mean absolute change in joint torques across consecutive control steps:\nğ’®\nÏ„\n=\nğ”¼\nâ€‹\n[\nâ€–\nÏ„\nt\nâˆ’\nÏ„\nt\nâˆ’\n1\nâ€–\n]\n\\mathcal{S}_{\\tau}=\\mathbb{E}[\\|\\tau_{t}-\\tau_{t-1}\\|]\n. A lower\nğ’®\nÏ„\n\\mathcal{S}_{\\tau}\nvalue represents more hardware-friendly execution and reduced mechanical wear. Finally, the Stylistic Authenticity of the emergent behaviors is evaluated via qualitative visual inspection. Instead of relying on a singular numerical score, we provide a comparative analysis of simulation snapshots to assess the human-likeness of the motions. This includes identifying key boxing maneuvers such as slips, counters, and rhythmic footwork, while ensuring the absence of unnatural joint configurations or physically unsustainable postures that often characterize non-constrained reinforcement learning policies.\nF.1\nBaselines\nWe compare LS-NFSP against a set of baselines that isolate the contributions of (i) NFSP-style opponent averaging, (ii) the latent strategy manifold, (iii) curriculum warmup, and (iv) stylistic regularization.\nâ€¢\nNaive Self-Play (Naive SP).\nA standard self-play baseline\n(Samuel,\n1959\n; Hernandez\net al.\n,\n2019\n)\ntrained in the same latent action space as ours, where each agent always plays against the opponentâ€™s latest policy snapshot (no opponent averaging).\nâ€¢\nFictitious Self-Play (Fictitious SP).\nA latent-space self-play baseline inspired by fictitious play\n(Brown,\n1951\n)\n, where agents are trained by sampling opponents uniformly from a population of historical policies. Unlike LS-NFSP, it does not maintain an explicit learned average policy network.\nâ€¢\nLS-NFSP w/o AMP.\nAn ablation of our method where the Adversarial Motion Prior (AMP) style reward\n(Peng\net al.\n,\n2021\n)\nis removed during training, testing the importance of motion-style regularization for physically plausible and effective strikes.\nâ€¢\nPPO-Only.\nA non-competitive baseline trained with PPO\n(Schulman\net al.\n,\n2017\n)\nagainst a fixed opponent (no self-play). This tests whether competitive co-evolution is necessary to induce robust tactics.\nâ€¢\nStatic-Target Specialist.\nThe policy after the warmup stage (trained to strike a passive sandbag) deployed without further competitive training. This measures how much performance comes from single-agent skill acquisition alone.\nâ€¢\nSelf-Play w/o Warmup (SP w/o Warmup).\nA curriculum ablation where self-play starts from scratch without the sandbag warmup, testing learning stability under sparse and non-stationary competitive signals.\nâ€¢\n29-DoF Action-Space Self-Play (29Dof Action-Space SP).\nA baseline that performs self-play directly in the 29-DoF joint target space (i.e., without the learned latent manifold), highlighting the instability of competitive learning in raw motor command space.",
    "preview_text": "Achieving human-level competitive intelligence and physical agility in humanoid robots remains a major challenge, particularly in contact-rich and highly dynamic tasks such as boxing. While Multi-Agent Reinforcement Learning (MARL) offers a principled framework for strategic interaction, its direct application to humanoid control is hindered by high-dimensional contact dynamics and the absence of strong physical motion priors. We propose RoboStriker, a hierarchical three-stage framework that enables fully autonomous humanoid boxing by decoupling high-level strategic reasoning from low-level physical execution. The framework first learns a comprehensive repertoire of boxing skills by training a single-agent motion tracker on human motion capture data. These skills are subsequently distilled into a structured latent manifold, regularized by projecting the Gaussian-parameterized distribution onto a unit hypersphere. This topological constraint effectively confines exploration to the subspace of physically plausible motions. In the final stage, we introduce Latent-Space Neural Fictitious Self-Play (LS-NFSP), where competing agents learn competitive tactics by interacting within the latent action space rather than the raw motor space, significantly stabilizing multi-agent training. Experimental results demonstrate that RoboStriker achieves superior competitive performance in simulation and exhibits sim-to-real transfer. Our website is available at RoboStriker.\n\nRoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxing\nKangning Yin\nZhe Cao\nWentao Dong\nWeishuai Zeng\nTianyi Zhang\nQiang Zhang\nJingbo Wang\nJiangmiao Pang\nMing Zhou\nWeinan Zhang\nAbstract\nAchieving human-level competitive intelligence and physical agility in humanoid robots remains a major challenge, particularly in contact-rich and highly dynamic tasks such as boxing. While Multi-Agent Reinforcement Learning (MARL) offers a principled framework for strategic interaction, its direct application to ",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control",
        "Multi-Agent Reinforcement Learning",
        "fine tune",
        "world model"
    ],
    "one_line_summary": "RoboStriker æ˜¯ä¸€ä¸ªåŸºäºåˆ†å±‚å†³ç­–å’Œå¼ºåŒ–å­¦ä¹ çš„ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºå®ç°äººå½¢æœºå™¨äººè‡ªä¸»æ‹³å‡»ï¼Œé€šè¿‡æŠ€èƒ½å­¦ä¹ ã€æ½œåœ¨ç©ºé—´æ­£åˆ™åŒ–å’Œå¤šæ™ºèƒ½ä½“äº¤äº’æ¥æå‡ç«äº‰æ€§èƒ½å’Œç‰©ç†æ§åˆ¶ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šRoboStriker: ç”¨äºè‡ªä¸»äººå½¢æœºå™¨äººæ‹³å‡»çš„åˆ†å±‚å†³ç­–\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nåœ¨æ¥è§¦å¯†é›†ã€é«˜åº¦åŠ¨æ€çš„ä»»åŠ¡ï¼ˆå¦‚æ‹³å‡»ï¼‰ä¸­ï¼Œå®ç°äººå½¢æœºå™¨äººå…·å¤‡äººç±»æ°´å¹³çš„ç«äº‰æ™ºèƒ½å’Œèº«ä½“æ•æ·æ€§ä»æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ–¹æ³•ç›´æ¥åº”ç”¨äºäººå½¢æ§åˆ¶æ—¶ï¼Œä¼šé¢ä¸´é«˜ç»´æ¥è§¦åŠ¨åŠ›å­¦å’Œç¼ºä¹å¼ºç‰©ç†è¿åŠ¨å…ˆéªŒçš„é˜»ç¢ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³**ç‰©ç†å¯è¡Œæ€§ä¸éå¹³ç¨³å­¦ä¹ **ä»¥åŠ**ç­–ç•¥æ¼”åŒ–ä¸ç³»ç»Ÿç¨³å®šæ€§**ä¹‹é—´çš„å†…åœ¨çŸ›ç›¾ï¼Œä¸ºäººå½¢æœºå™¨äººç«æŠ€ä»»åŠ¡æä¾›ä¸€ä¸ªç¨³å®šã€å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡ºäº† **RoboStriker**ï¼Œä¸€ä¸ª**ä¸‰å±‚åˆ†å±‚æ¡†æ¶**ï¼Œå°†é«˜å±‚ç­–ç•¥æ¨ç†ä¸åº•å±‚ç‰©ç†æ‰§è¡Œè§£è€¦ï¼š\n- **ç¬¬ä¸€é˜¶æ®µï¼ˆæŠ€èƒ½åº“ï¼‰**ï¼šåŸºäºäººç±»åŠ¨ä½œæ•æ‰æ•°æ®ï¼Œè®­ç»ƒä¸€ä¸ªé€šç”¨çš„è¿åŠ¨è·Ÿè¸ªç­–ç•¥ï¼Œå­¦ä¹ ä¸€å¥—ç‰©ç†ä¸Šå¯è¡Œçš„æ‹³å‡»æŠ€èƒ½ã€‚\n- **ç¬¬äºŒé˜¶æ®µï¼ˆç»“æ„åŒ–æ½œç©ºé—´ï¼‰**ï¼šå°†æŠ€èƒ½æç‚¼åˆ°ä¸€ä¸ª**æœ‰ç•Œçš„è¿ç»­æ½œç©ºé—´**ï¼ˆå•ä½è¶…çƒé¢ï¼‰ï¼Œè¯¥ç©ºé—´ä½œä¸ºé«˜å±‚ç­–ç•¥çš„åŠ¨ä½œç©ºé—´ï¼Œå°†æ¢ç´¢é™åˆ¶åœ¨ç‰©ç†åˆç†çš„è¿åŠ¨å­ç©ºé—´å†…ã€‚\n- **ç¬¬ä¸‰é˜¶æ®µï¼ˆç«äº‰å­¦ä¹ ï¼‰**ï¼šæå‡º**æ½œç©ºé—´ç¥ç»è™šæ‹Ÿè‡ªåšå¼ˆï¼ˆLS-NFSPï¼‰**ï¼Œæ™ºèƒ½ä½“åœ¨æ½œç©ºé—´è€ŒéåŸå§‹ç”µæœºç©ºé—´ä¸­è¿›è¡Œç«äº‰æ€§äº¤äº’ï¼Œå¹¶ç»“åˆ**å¯¹æŠ—æ€§è¿åŠ¨å…ˆéªŒï¼ˆAMPï¼‰** è¿›è¡Œé£æ ¼æ­£åˆ™åŒ–ï¼Œä»¥åŠ**è¯¾ç¨‹çƒ­èº«**æ¥ç¨³å®šè®­ç»ƒã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\nåœ¨Unitree G1äººå½¢æœºå™¨äººï¼ˆ29è‡ªç”±åº¦ï¼‰çš„æ‹³å‡»ä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ï¼š\n- **èƒœç‡**ï¼šåœ¨8x8äº¤å‰å¯¹æˆ˜ä¸­ï¼ŒLS-NFSPä»¥æ˜¾è‘—ä¼˜åŠ¿å‡»è´¥æ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œå¯¹â€œ29ç»´åŠ¨ä½œç©ºé—´è‡ªåšå¼ˆâ€åŸºçº¿èƒœç‡è¾¾100%ï¼‰ã€‚\n- **æˆ˜æœ¯ä¸ç‰©ç†æŒ‡æ ‡**ï¼šåœ¨è¿›æ”»å‘½ä¸­ç‡ï¼ˆ0.685 vs. 0.142ï¼‰ã€äº¤æˆ˜ç‡ï¼ˆ0.824 vs. 0.315ï¼‰ã€åŸºåº§ç¨³å®šæ€§ï¼ˆ0.942 vs. 0.418ï¼‰ç­‰å…³é”®æŒ‡æ ‡ä¸Šå‡å¤§å¹…é¢†å…ˆã€‚\n- **æ¶ˆèç ”ç©¶**ï¼šéªŒè¯äº†æ½œç©ºé—´çº¦æŸã€AMPé£æ ¼æ­£åˆ™åŒ–å’Œè¯¾ç¨‹çƒ­èº«å¯¹æ€§èƒ½æå‡çš„å¿…è¦æ€§ã€‚\n- **ä»¿çœŸåˆ°ç°å®è¿ç§»**ï¼šæˆåŠŸå®ç°äº†ä»ä»¿çœŸåˆ°çœŸå®æœºå™¨äººçš„é›¶æ ·æœ¬è¿ç§»ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\næœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿé˜è¿°äº†å…·èº«å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„å†…åœ¨çŸ›ç›¾ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€šç”¨çš„åˆ†å±‚è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å°†**ç»“æ„åŒ–æ½œç©ºé—´è¡¨ç¤º**ä¸**åšå¼ˆè®ºè‡ªåšå¼ˆç®—æ³•**ç›¸ç»“åˆï¼ŒRoboStrikeræˆåŠŸåœ°åœ¨ä¿æŒç‰©ç†ç¨³å®šæ€§å’Œè¿åŠ¨æ‹ŸäººåŒ–çš„åŒæ—¶ï¼Œæ¼”åŒ–å‡ºäº†å¤æ‚çš„ç«äº‰æ€§æˆ˜æœ¯ï¼ˆå¦‚é—ªé¿åå‡»ï¼‰ã€‚è¿™é¡¹å·¥ä½œä¸ºå°†MARLä»æŠ½è±¡æ¸¸æˆæ‰©å±•åˆ°å¤æ‚çš„ç‰©ç†å…·èº«æœºå™¨äººç³»ç»Ÿæä¾›äº†ä¸€æ¡åŸåˆ™æ€§è·¯å¾„ï¼Œå¯¹å‘å±•å…·å¤‡é«˜çº§äº¤äº’ä¸ç«äº‰èƒ½åŠ›çš„é€šç”¨æœºå™¨äººå…·æœ‰é‡è¦æ„ä¹‰ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šRoboStriker\n\n### **ä¸€ã€ æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**å…·èº«å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆEmbodied MARLï¼‰** åœ¨**é«˜åŠ¨æ€ã€æ¥è§¦ä¸°å¯Œçš„ç«äº‰æ€§ä»»åŠ¡**ï¼ˆå¦‚äººå½¢æœºå™¨äººæ‹³å‡»ï¼‰ä¸­é¢ä¸´çš„ä¸¤ä¸ªæ ¹æœ¬æ€§çŸ›ç›¾ï¼š\n1.  **ç‰©ç†å¯è¡Œæ€§ä¸éå¹³ç¨³å­¦ä¹ çš„çŸ›ç›¾**ï¼šäººå½¢æœºå™¨äººæ§åˆ¶å—é™äºé«˜ç»´è¿ç»­åŠ¨ä½œç©ºé—´å’Œä¸¥æ ¼çš„ç‰©ç†çº¦æŸï¼ˆå¦‚å¹³è¡¡ã€å…³èŠ‚æé™ï¼‰ã€‚æœ‰æ•ˆçš„ç­–ç•¥æ¢ç´¢éœ€è¦å¤šæ ·ç”šè‡³æ¿€è¿›çš„è¡Œä¸ºï¼Œä½†è¿™ææ˜“å¯¼è‡´ç‰©ç†ä¸Šæ— æ•ˆçš„åŠ¨ä½œï¼ˆå¦‚æ‘”å€’ï¼‰ï¼Œè€Œè¿‡åº¦ä¿å®ˆçš„æ§åˆ¶å™¨åˆä¼šæ‰¼æ€ç­–ç•¥å­¦ä¹ ã€‚\n2.  **ç­–ç•¥æ¼”åŒ–ä¸ç³»ç»Ÿç¨³å®šæ€§çš„çŸ›ç›¾**ï¼šé€šè¿‡è‡ªæˆ‘å¯¹å¼ˆï¼ˆself-playï¼‰çš„MARLä¾èµ–äºå¯¹æ‰‹ç­–ç•¥çš„æŒç»­æ¼”åŒ–ï¼Œè¿™å¼•å…¥äº†ä¸¥é‡çš„éå¹³ç¨³æ€§ã€‚è™½ç„¶è¿™å¯¹å‘ç°ç«äº‰æ€§ç­–ç•¥è‡³å…³é‡è¦ï¼Œä½†äººå½¢ç³»ç»ŸåŠ¨æ€è„†å¼±ï¼Œå¯¹æ‰‹è¡Œä¸ºçš„å¾®å°åˆ†å¸ƒåç§»éƒ½å¯èƒ½å¯¼è‡´å¹³è¡¡å’Œæ¥è§¦å¤±ç¨³ï¼Œå¼•å‘è®­ç»ƒå´©æºƒã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸º **RoboStriker** çš„**åˆ†å±‚ä¸‰é˜¶æ®µæ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº**å°†é«˜å±‚ç­–ç•¥æ¨ç†ä¸åº•å±‚ç‰©ç†æ‰§è¡Œè§£è€¦**ï¼Œå¹¶é€šè¿‡ä¸€ä¸ª**ç»“æ„åŒ–çš„æ½œåœ¨åŠ¨ä½œç©ºé—´**æ¥æ¡¥æ¥äºŒè€…ã€‚\n\n1.  **åˆ†å±‚å†³ç­–æ¶æ„**ï¼š\n    - **é˜¶æ®µä¸€ï¼ˆæŠ€èƒ½åº“ï¼‰**ï¼šåŸºäºäººç±»åŠ¨ä½œæ•æ‰æ•°æ®ï¼Œè®­ç»ƒä¸€ä¸ªé€šç”¨çš„è¿åŠ¨è·Ÿè¸ªç­–ç•¥ï¼ˆ`DeepMimic`é£æ ¼ï¼‰ï¼ŒæŒæ¡ä¸€å¥—**ç‰©ç†ä¸Šå¯è¡Œçš„**æ‹³å‡»åŸºç¡€æŠ€èƒ½ï¼ˆå¦‚å‡ºæ‹³ã€ç§»åŠ¨ã€é˜²å®ˆï¼‰ã€‚\n    - **é˜¶æ®µäºŒï¼ˆç»“æ„åŒ–æ½œåœ¨ç©ºé—´ï¼‰**ï¼šå°†å­¦åˆ°çš„æŠ€èƒ½**è’¸é¦**åˆ°ä¸€ä¸ª**æœ‰ç•Œçš„è¿ç»­æ½œåœ¨ç©ºé—´**ã€‚å…³é”®åˆ›æ–°åœ¨äºå¯¹è¯¥ç©ºé—´æ–½åŠ **æ‹“æ‰‘æ­£åˆ™åŒ–**ï¼ˆæŠ•å½±åˆ°å•ä½è¶…çƒé¢ `ğ•Š^(d-1)`ï¼‰ï¼Œä»è€Œå°†ç­–ç•¥æ¢ç´¢é™åˆ¶åœ¨ç‰©ç†åˆç†çš„è¿åŠ¨å­ç©ºé—´å†…ã€‚\n    - **é˜¶æ®µä¸‰ï¼ˆç«äº‰æ€§å­¦ä¹ ï¼‰**ï¼šåœ¨æ½œåœ¨ç©ºé—´ä¸­å¼•å…¥ **â€œæ½œåœ¨ç©ºé—´ç¥ç»è™šæ‹Ÿè‡ªæˆ‘å¯¹å¼ˆâ€**ã€‚\n\n2.  **Latent-Space Neural Fictitious Self-Play (LS-NFSP)**ï¼š\n    - è¿™æ˜¯è®ºæ–‡çš„**æ ¸å¿ƒç®—æ³•åˆ›æ–°**ã€‚å®ƒå°†ç»å…¸çš„NFSPï¼ˆä¸€ç§ç”¨äºæ±‚è§£åšå¼ˆå‡è¡¡çš„ç®—æ³•ï¼‰**ç§»æ¤åˆ°ç»“æ„åŒ–æ½œåœ¨ç©ºé—´**ä¸­æ‰§è¡Œã€‚\n    - **ä¼˜åŠ¿**ï¼šæ™ºèƒ½ä½“åœ¨ç´§å‡‘ã€ç‰©ç†å®‰å…¨çš„æ½œåœ¨åŠ¨ä½œç©ºé—´ä¸­å­¦ä¹ ç«äº‰ç­–ç•¥ï¼Œè€ŒéåŸå§‹çš„é«˜ç»´ç”µæœºæŒ‡ä»¤ç©ºé—´ã€‚è¿™**æå¤§åœ°ç¨³å®šäº†å¤šæ™ºèƒ½ä½“è®­ç»ƒ**ï¼Œé¿å…äº†å› ç­–ç•¥å‰§çƒˆå˜åŒ–å¯¼è‡´çš„ç‰©ç†å¤±ç¨³ï¼Œå¹¶ä½¿å­¦ä¹ åŠ¨æ€æ›´ç¬¦åˆåšå¼ˆè®ºæ”¶æ•›ç†è®ºã€‚\n\n3.  **è¡Œä¸ºé¢„çƒ­ä¸å¯¹æŠ—æ€§è¿åŠ¨å…ˆéªŒ**ï¼š\n    - åœ¨è¿›å…¥å®Œå…¨ç«äº‰å­¦ä¹ å‰ï¼Œå¼•å…¥ä¸€ä¸ª**è¡Œä¸ºé¢„çƒ­é˜¶æ®µ**ï¼Œè®©æ™ºèƒ½ä½“å…ˆå­¦ä¹ æ”»å‡»é™æ­¢ç›®æ ‡çš„åŸºæœ¬èƒ½åŠ›ã€‚\n    - ä½¿ç”¨**å¯¹æŠ—æ€§è¿åŠ¨å…ˆéªŒ** æ¥æ­£åˆ™åŒ–æ™ºèƒ½ä½“çš„åŠ¨ä½œé£æ ¼ï¼Œç¡®ä¿å…¶è¡Œä¸ºåœ¨è¿½æ±‚æˆ˜æœ¯ç›®æ ‡çš„åŒæ—¶ï¼Œä¿æŒ**ç±»äººçš„ã€è‡ªç„¶çš„è¿åŠ¨å½¢æ€**ï¼Œé¿å…å‡ºç°æ€ªå¼‚ã€æ— æ•ˆçš„åŠ¨ä½œæ¨¡å¼ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆè·¯å¾„**\n**â€œç‰©ç†æ‰§è¡Œä¸ç­–ç•¥æ¨ç†è§£è€¦ + æ½œåœ¨ç©ºé—´çº¦æŸä¸‹çš„åšå¼ˆå­¦ä¹ â€**\n\n1.  **è§£å†³ç‰©ç†å¯è¡Œæ€§çŸ›ç›¾**ï¼š\n    - **è§£è€¦**ï¼šåº•å±‚ç”±é¢„è®­ç»ƒä¸”å›ºå®šçš„è¿åŠ¨è§£ç å™¨è´Ÿè´£ï¼Œå®ƒ**åªäº§ç”Ÿç‰©ç†å¯è¡Œçš„åŠ¨ä½œ**ã€‚\n    - **çº¦æŸ**ï¼šé«˜å±‚ç­–ç•¥åœ¨**æœ‰ç•Œçš„æ½œåœ¨ç©ºé—´**ä¸­è¾“å‡ºæŒ‡ä»¤ï¼Œè¿™ä¸ªç©ºé—´æœ¬èº«ç”±ç‰©ç†åˆç†çš„æŠ€èƒ½è’¸é¦è€Œæ¥ã€‚è¿™ç›¸å½“äºä¸ºç­–ç•¥æ¢ç´¢è®¾ç½®äº†â€œå®‰å…¨å›´æ â€ã€‚\n\n2.  **è§£å†³ç­–ç•¥ç¨³å®šæ€§çŸ›ç›¾**ï¼š\n    - **ç¨³å®šç©ºé—´**ï¼šLS-NFSPåœ¨**ç´§å‡‘çš„æ½œåœ¨ç­–ç•¥ç©ºé—´**ä¸­è¿è¡Œï¼Œç­–ç•¥æ›´æ–°å¼•èµ·çš„åˆ†å¸ƒå˜åŒ–å¹…åº¦å—åˆ°å¤©ç„¶é™åˆ¶ã€‚\n    - **ç¨³å®šç®—æ³•**ï¼šNFSPæœºåˆ¶é€šè¿‡ç»´æŠ¤å’Œæ··åˆ**å¹³å‡ç­–ç•¥** ä¸**æœ€ä¼˜ååº”ç­–ç•¥**ï¼Œå¹³æ»‘äº†è‡ªæˆ‘å¯¹å¼ˆä¸­çš„éå¹³ç¨³æ€§ï¼Œå¼•å¯¼å­¦ä¹ è¶‹å‘åšå¼ˆå‡è¡¡ï¼Œè€Œéç­–ç•¥å¾ªç¯æŒ¯è¡ã€‚\n\n### **å››ã€ å®é™…ä»·å€¼ä¸éªŒè¯**\n- **æ€§èƒ½éªŒè¯**ï¼šåœ¨ä»¿çœŸå’ŒUnitree G1å®ä½“æœºå™¨äººä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå®ç°äº†**ä»¿çœŸåˆ°ç°å®çš„é›¶æ ·æœ¬è¿ç§»**ã€‚å®éªŒæ˜¾ç¤ºï¼ŒRoboStrikeråœ¨äº¤å‰å¯¹å¼ˆä¸­èƒœç‡æ˜¾è‘—é«˜äºæ‰€æœ‰åŸºçº¿ï¼ˆè§è¡¨1ï¼‰ï¼Œå¹¶å±•ç°å‡ºæ›´é«˜çš„æ‰“å‡»å‘½ä¸­ç‡ï¼ˆ`Î·_hit = 0.685`ï¼‰å’Œèº«ä½“ç¨³å®šæ€§ï¼ˆ`BOS = 0.942`ï¼‰ã€‚\n- **é€šç”¨æ€§è´¡çŒ®**ï¼šä¸ºè§£å†³**å…·èº«å¤šæ™ºèƒ½ä½“ç«äº‰**è¿™ä¸€æ™®ééš¾é¢˜æä¾›äº†ä¸€ä¸ªé€šç”¨æ¡†æ¶æ¨¡æ¿ï¼š**æŠ€èƒ½æ¨¡ä»¿ â†’ æ½œåœ¨ç©ºé—´è’¸é¦ â†’ çº¦æŸç©ºé—´å†…çš„åšå¼ˆå­¦ä¹ **ã€‚\n- **æŠ€æœ¯è´¡çŒ®**ï¼šé¦–æ¬¡åœ¨å…·èº«MARLé¢†åŸŸå½¢å¼åŒ–é˜è¿°äº†ä¸Šè¿°ä¸¤å¤§çŸ›ç›¾ï¼Œå¹¶ç»™å‡ºäº†ä¸€ä¸ªç»“åˆ**æ¨¡ä»¿å­¦ä¹ ã€è¡¨å¾å­¦ä¹ ã€åšå¼ˆè®ºä¸å¼ºåŒ–å­¦ä¹ **çš„ç»¼åˆæ€§è§£å†³æ–¹æ¡ˆã€‚\n\n**æ€»ç»“**ï¼šRoboStrikerçš„æ ¸å¿ƒåˆ›æ–°ä¸æ˜¯å•ä¸€ç®—æ³•ï¼Œè€Œæ˜¯ä¸€ä¸ª**ç³»ç»Ÿæ€§çš„å·¥ç¨‹ä¸ç®—æ³•æ¡†æ¶**ã€‚å®ƒé€šè¿‡å¼•å…¥ä¸€ä¸ª**ç‰©ç†æ¥åœ°ã€æ‹“æ‰‘çº¦æŸçš„æ½œåœ¨åŠ¨ä½œç©ºé—´**ä½œä¸ºâ€œæˆ˜ç•¥æ²™ç›˜â€ï¼ŒæˆåŠŸåœ°å°†é€‚ç”¨äºæŠ½è±¡åšå¼ˆçš„å…ˆè¿›å¤šæ™ºèƒ½ä½“ç®—æ³•ï¼ˆNFSPï¼‰å®‰å…¨ã€æœ‰æ•ˆåœ°åº”ç”¨äºé«˜ç»´ã€è„†å¼±çš„ç‰©ç†äººå½¢æœºå™¨äººæ§åˆ¶é—®é¢˜ä¸­ï¼Œå®ç°äº†åŠ¨æ€ã€é²æ£’ä¸”å…·æˆ˜æœ¯æ€§çš„è‡ªä¸»æ‹³å‡»è¡Œä¸ºã€‚",
            "timestamp": "2026-02-03T15:54:35.034906",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡ã€ŠRoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxingã€‹æŠ€æœ¯è„‰ç»œä¸å‘å±•æ–¹å‘åˆ†æ\n\nåŸºäºè®ºæ–‡å†…å®¹åŠå…¶å¼•ç”¨çš„ç›¸å…³æ–‡çŒ®ï¼Œå¯ä»¥æ¢³ç†å‡º**äººå½¢æœºå™¨äººå…·èº«æ™ºèƒ½ä¸ç«æŠ€æ§åˆ¶**è¿™ä¸€æ–¹å‘çš„å‘å±•è„‰ç»œã€‚å…¶æ ¸å¿ƒæ¼”è¿›é€»è¾‘æ˜¯ï¼š**ä»å•æ™ºèƒ½ä½“ã€è¢«åŠ¨çš„è¿åŠ¨æ¨¡ä»¿ä¸æ§åˆ¶ï¼Œé€æ­¥è¿ˆå‘å¤šæ™ºèƒ½ä½“ã€ä¸»åŠ¨çš„ã€å…·å¤‡æˆ˜ç•¥äº¤äº’èƒ½åŠ›çš„å…·èº«ç«æŠ€**ã€‚æ¯ä¸€æ­¥çš„æ”¹è¿›éƒ½æ—¨åœ¨è§£å†³æ›´å¤æ‚ç¯å¢ƒä¸‹çš„æ–°çŸ›ç›¾ã€‚\n\nä»¥ä¸‹æ˜¯è¯¥æ–¹å‘çš„æ•´ä½“å‘å±•è„‰ç»œï¼ŒæŒ‰å‡ ä¸ªå…³é”®å‘å±•æ–¹å‘è¿›è¡Œæ¢³ç†ï¼š\n\n### å‘å±•æ–¹å‘ä¸€ï¼šä»è¿åŠ¨æ¨¡ä»¿åˆ°é€šç”¨è¿åŠ¨æ§åˆ¶ï¼ˆå•æ™ºèƒ½ä½“åŸºç¡€ï¼‰\n\nè¿™ä¸ªæ–¹å‘çš„æ ¸å¿ƒç›®æ ‡æ˜¯è®©æœºå™¨äººèƒ½å¤Ÿç¨³å®šã€é€¼çœŸåœ°æ‰§è¡Œå¤æ‚çš„äººç±»åŠ¨ä½œï¼Œä¸ºé«˜çº§ä»»åŠ¡å¥ å®šç‰©ç†åŸºç¡€ã€‚\n\n1.  **æ—©æœŸè¿åŠ¨åˆæˆä¸é‡å®šå‘ (Kinematic Motion Synthesis)**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šåŸºäºé€†å‘è¿åŠ¨å­¦(IK)å’Œæ—¶ç©ºä¼˜åŒ–çš„è¿åŠ¨æ•æ‰æ•°æ®é‡å®šå‘æ–¹æ³• (Gleicher, 1998; Yamane et al., 2004)ã€‚\n    *   **æ”¹è¿›ä¸å±€é™**ï¼šå®ç°äº†åŠ¨ä½œçš„å‡ ä½•å¤ç°ï¼Œä½†**ç¼ºä¹åŠ¨åŠ›å­¦è€ƒé‡**ï¼Œåœ¨ç‰©ç†ä»¿çœŸä¸­æ˜“å‡ºç°â€œè„šæ»‘â€ç­‰è¿åç‰©ç†è§„å¾‹çš„ç°è±¡ï¼Œæ— æ³•ç›´æ¥ç”¨äºç‰©ç†æ§åˆ¶ã€‚\n\n2.  **åŸºäºç‰©ç†çš„æ¨¡ä»¿å­¦ä¹  (Physics-based Imitation Learning)**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼š**DeepMimic** (Peng et al., 2018)ã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼šå¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨ç‰©ç†ä»¿çœŸç¯å¢ƒä¸­é€šè¿‡å¥–åŠ±å‡½æ•°ï¼ˆå¦‚å§¿æ€ã€é€Ÿåº¦è¯¯å·®ï¼‰**è·Ÿè¸ªå‚è€ƒè¿åŠ¨**ã€‚è§£å†³äº†è¿åŠ¨åœ¨ç‰©ç†ä¸–ç•Œä¸­çš„**ç¨³å®šæ€§é—®é¢˜**ï¼ˆå¦‚å¹³è¡¡ã€æ¥è§¦ï¼‰ï¼Œä½¿æ¨¡ä»¿çš„åŠ¨ä½œç‰©ç†å¯è¡Œã€‚\n    *   **å±€é™**ï¼šä¸»è¦é’ˆå¯¹**é¢„å®šä¹‰ã€å›ºå®šçš„è¿åŠ¨åºåˆ—**è¿›è¡Œé«˜ä¿çœŸæ¨¡ä»¿ï¼Œç¼ºä¹å¯¹ç¯å¢ƒçš„**ååº”æ€§å’Œé€‚åº”æ€§**ã€‚\n\n3.  **å¯¹æŠ—æ€§è¿åŠ¨å…ˆéªŒä¸é£æ ¼åŒ–æ§åˆ¶ (Adversarial Stylistic Control)**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼š**AMP** (Peng et al., 2021)ã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼šå¼•å…¥**åˆ¤åˆ«å™¨**æ¥åŒºåˆ†æœºå™¨äººè¿åŠ¨ä¸äººç±»è¿åŠ¨æ•°æ®é›†ï¼Œæä¾›**é£æ ¼å¥–åŠ±**ã€‚è¿™ä½¿å¾—ç­–ç•¥ä¸ä»…èƒ½å®Œæˆç›®æ ‡ä»»åŠ¡ï¼Œè¿˜èƒ½**ä¿æŒäººç±»åŠ¨ä½œçš„â€œé£æ ¼â€æˆ–â€œè‡ªç„¶åº¦â€**ï¼Œå¤§å¤§æå‡äº†åŠ¨ä½œçš„å¤šæ ·æ€§å’Œæ‹Ÿäººæ€§ã€‚\n    *   **å±€é™**ï¼šä»èšç„¦äº**å•æ™ºèƒ½ä½“**ä»»åŠ¡ï¼Œæ— æ³•å¤„ç†åŠ¨æ€å¯¹æ‰‹å¸¦æ¥çš„éå¹³ç¨³æ€§ã€‚\n\n4.  **é€šç”¨è¿åŠ¨è·Ÿè¸ªå™¨ (Universal Motion Trackers)**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šGMT (Chen et al., 2025), Unitracker (Yin et al., 2025) ç­‰ã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼šæ—¨åœ¨å­¦ä¹ ä¸€ä¸ª**ç»Ÿä¸€çš„ç­–ç•¥**ï¼Œèƒ½å¤Ÿè·Ÿè¸ªä¸€ä¸ª**åºå¤§ã€å¤šæ ·çš„äººç±»åŠ¨ä½œåº“**ä¸­çš„ä»»ä½•åŠ¨ä½œã€‚è¿™ä¸ºæœºå™¨äººæä¾›äº†ä¸°å¯Œçš„â€œæŠ€èƒ½åº“â€ï¼Œæ˜¯è¿ˆå‘é€šç”¨æ§åˆ¶çš„å…³é”®ä¸€æ­¥ã€‚\n    *   **RoboStrikerçš„å®šä½ä¸æ”¹è¿›**ï¼šæœ¬æ–‡çš„**ç¬¬ä¸€é˜¶æ®µï¼ˆè¿åŠ¨è·Ÿè¸ªå™¨ï¼‰** ç›´æ¥å»ºç«‹åœ¨æ­¤ç±»å·¥ä½œä¹‹ä¸Šã€‚ä½†å…¶ç›®æ ‡ä¸ä»…æ˜¯æ¨¡ä»¿ï¼Œæ›´æ˜¯ä¸ºåç»­çš„**å¤šæ™ºèƒ½ä½“ç«æŠ€**æä¾›ä¸€ä¸ªç¨³å®šã€å¯é çš„åº•å±‚æ‰§è¡Œå™¨ã€‚å®ƒå°†è¿™äº›é€šç”¨æŠ€èƒ½ä½œä¸º**é«˜å±‚æˆ˜ç•¥æ¼”åŒ–çš„ç‰©ç†åŸºç¡€**ã€‚\n\n### å‘å±•æ–¹å‘äºŒï¼šä»åŸå§‹åŠ¨ä½œç©ºé—´åˆ°ç»“æ„åŒ–æ½œç©ºé—´ï¼ˆæŠ€èƒ½æŠ½è±¡ä¸è¡¨ç¤ºï¼‰\n\nè¿™ä¸ªæ–¹å‘æ—¨åœ¨è§£å†³é«˜ç»´è¿ç»­åŠ¨ä½œç©ºé—´å¸¦æ¥çš„æ¢ç´¢å›°éš¾å’Œè®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œé€šè¿‡æŠ½è±¡æ¥ç®€åŒ–æ§åˆ¶ã€‚\n\n1.  **æŠ€èƒ½æ½œè¡¨ç¤ºå­¦ä¹  (Skill-based Latent Representation Learning)**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼š**PULSE** (Luo et al., 2023), **CALM** (Tessler et al., 2023)ã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼šå°†åºå¤§çš„è¿åŠ¨æ•°æ®é›†**è’¸é¦**åˆ°ä¸€ä¸ªä½ç»´çš„**è¿ç»­æ½œç©ºé—´**ä¸­ã€‚é«˜å±‚ç­–ç•¥åªéœ€åœ¨è¿™ä¸ªæ½œç©ºé—´ä¸­è¾“å‡ºæ½œå˜é‡ï¼Œå†ç”±è§£ç å™¨ç”Ÿæˆå¤æ‚çš„å…³èŠ‚åŠ¨ä½œã€‚è¿™**æå¤§åœ°ç®€åŒ–äº†é«˜å±‚ç­–ç•¥çš„æœç´¢ç©ºé—´**ï¼Œå®ç°äº†â€œæŠ€èƒ½â€çš„æ¨¡å—åŒ–è°ƒç”¨ã€‚\n    *   **å±€é™**ï¼šé€šå¸¸å°†æ½œç©ºé—´å»ºæ¨¡ä¸º**æ— ç•Œçš„æ¬§å‡ é‡Œå¾—ç©ºé—´**ï¼Œé«˜å±‚ç­–ç•¥çš„æ¢ç´¢ä»å¯èƒ½äº§ç”Ÿåˆ†å¸ƒå¤–çš„æ½œåŠ¨ä½œï¼Œå¯¼è‡´åº•å±‚æ§åˆ¶å™¨æ‰§è¡Œå‡ºç‰©ç†ä¸Šæ— æ•ˆç”šè‡³å´©æºƒçš„åŠ¨ä½œã€‚\n\n2.  **RoboStrikerçš„æ‹“æ‰‘æ½œç©ºé—´è’¸é¦**\n    *   **æ ¸å¿ƒæ”¹è¿›ï¼ˆç¬¬äºŒé˜¶æ®µï¼‰**ï¼š\n        *   **æœ‰ç•ŒåŒ–**ï¼šé€šè¿‡å°†é«˜æ–¯å‚æ•°åŒ–çš„æ½œåˆ†å¸ƒ**æŠ•å½±åˆ°å•ä½è¶…çƒé¢** (`Normalize(z) = z / ||z||_2`)ï¼Œå°†æ½œç©ºé—´çº¦æŸä¸ºä¸€ä¸ª**ç´§è‡´çš„æ‹“æ‰‘æµå½¢** (`ğ’µ = ğ•Š^{d-1}`)ã€‚\n        *   **ç†è®ºæ„ä¹‰**ï¼šè¿™ç¡®ä¿äº†ç­–ç•¥æœç´¢è¢«é™åˆ¶åœ¨**ç‰©ç†å¯ä¿¡çš„è¿åŠ¨å­ç©ºé—´**å†…ï¼Œä»æ ¹æœ¬ä¸Šé˜²æ­¢äº†é«˜å±‚ç­–ç•¥äº§ç”Ÿå¯¼è‡´ç‰©ç†ä¸ç¨³å®šçš„åŠ¨ä½œã€‚è¿™ä¸ºåç»­çš„å¤šæ™ºèƒ½ä½“åšå¼ˆæä¾›äº†**ç´§å‡‘ä¸”å®‰å…¨çš„ç­–ç•¥é›†**ã€‚\n        *   **å®é™…ä»·å€¼**ï¼šè¿™æ˜¯æœ¬æ–‡è§£å†³ **â€œç‰©ç†å¯è¡Œæ€§ vs. éå¹³ç¨³å­¦ä¹ â€** çŸ›ç›¾çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ã€‚å®ƒå°†æˆ˜ç•¥æ¢ç´¢ä¸åŠ¨åŠ›å­¦ä¸ç¨³å®šæ€§**è§£è€¦**ã€‚\n\n### å‘å±•æ–¹å‘ä¸‰ï¼šä»å•æ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“ç«æŠ€æˆ˜ç•¥ï¼ˆåšå¼ˆä¸ååŒæ¼”åŒ–ï¼‰\n\nè¿™ä¸ªæ–¹å‘å°†åšå¼ˆè®ºå’Œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ å¼•å…¥å…·èº«æ§åˆ¶ï¼Œä»¥äº§ç”Ÿå…·æœ‰æˆ˜ç•¥äº¤äº’èƒ½åŠ›çš„æ™ºèƒ½ä½“ã€‚\n\n1.  **æŠ½è±¡é¢†åŸŸçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸è‡ªæˆ‘åšå¼ˆ**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šå›´æ£‹AlphaGo (Silver et al., 2017)ã€æ˜Ÿé™…äº‰éœ¸II (Vinyals et al., 2019)ã€**ç¥ç»è™šæ‹Ÿè‡ªæˆ‘åšå¼ˆ (NFSP)** (Heinrich and Silver, 2016)ã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼šåœ¨ä¿¡æ¯å®Œå…¨æˆ–ä¸å®Œå…¨åšå¼ˆä¸­ï¼Œé€šè¿‡**è‡ªæˆ‘åšå¼ˆ**å’Œ**è™šæ‹Ÿåšå¼ˆ**ç­‰æœºåˆ¶ï¼Œè®©æ™ºèƒ½ä½“åœ¨ä¸å¯¹æ‰‹çš„æŒç»­å¯¹æŠ—ä¸­**ååŒè¿›åŒ–**ï¼Œæœ€ç»ˆè¾¾åˆ°çº³ä»€å‡è¡¡æˆ–æŒæ¡é«˜è¶…ç­–ç•¥ã€‚\n    *   **å±€é™**ï¼šè¿™äº›æ–¹æ³•åº”ç”¨äº**æŠ½è±¡æˆ–å¼±å…·èº«ç¯å¢ƒ**ï¼ˆå¦‚æ£‹ç›˜ã€æ¸¸æˆç•Œé¢ï¼‰ï¼Œå…¶åŠ¨ä½œç©ºé—´æ˜¯ç¦»æ•£æˆ–ä½ç»´è¿ç»­çš„ï¼Œ**ç¼ºä¹å¯¹é«˜ç»´ã€æ¥è§¦ä¸°å¯Œçš„ç‰©ç†åŠ¨åŠ›å­¦å’Œç‰©ç†å¯è¡Œæ€§çš„å½’çº³åç½®**ã€‚ç›´æ¥åº”ç”¨äºäººå½¢æœºå™¨äººä¼šå¯¼è‡´è®­ç»ƒå´©æºƒã€‚\n\n2.  **RoboStrikerçš„æ½œç©ºé—´ç¥ç»è™šæ‹Ÿè‡ªæˆ‘åšå¼ˆ (LS-NFSP)**\n    *   **æ ¸å¿ƒæ”¹è¿›ï¼ˆç¬¬ä¸‰é˜¶æ®µï¼‰**ï¼š\n        *   **ç©ºé—´è¿ç§»**ï¼šå°†NFSPçš„æ ¸å¿ƒæ€æƒ³ï¼ˆç»´æŠ¤æœ€ä½³å“åº”ç­–ç•¥å’Œå¹³å‡ç­–ç•¥ï¼‰**ä»åŸå§‹åŠ¨ä½œç©ºé—´è¿ç§»åˆ°ç¬¬äºŒé˜¶æ®µæ„å»ºçš„æœ‰ç•Œæ½œç©ºé—´**ä¸­è¿›è¡Œã€‚\n        *   **ç¨³å®šåŒ–æœºåˆ¶**ï¼š\n            *   **è¡Œä¸ºé¢„çƒ­**ï¼šåœ¨æ­£å¼å¯¹æŠ—å‰ï¼Œè®©æ™ºèƒ½ä½“å…ˆä¸é™æ­¢ç›®æ ‡ï¼ˆæ²™è¢‹ï¼‰è®­ç»ƒï¼Œå­¦ä¹ åŸºæœ¬çš„è¿›æ”»æŠ€èƒ½ï¼Œè§£å†³ **â€œç«äº‰å†·å¯åŠ¨â€** é—®é¢˜ã€‚\n            *   **AMPæ­£åˆ™åŒ–**ï¼šåœ¨é¢„çƒ­å’ŒLS-NFSPé˜¶æ®µæŒç»­ä½¿ç”¨AMPå¥–åŠ±ï¼Œç¡®ä¿æ¼”åŒ–å‡ºçš„æˆ˜æœ¯åŠ¨ä½œ**ä¿æŒäººç±»é£æ ¼å’Œç‰©ç†åˆç†æ€§**ã€‚\n        *   **ç†è®ºä¿éšœ**ï¼šç”±äºæ½œç©ºé—´æ˜¯**ç´§è‡´**çš„ï¼Œæ ¹æ®åšå¼ˆè®ºï¼ˆå¦‚Glicksbergå®šç†ï¼‰ï¼Œ**æ··åˆç­–ç•¥çº³ä»€å‡è¡¡çš„å­˜åœ¨æ€§å¾—åˆ°ä¿è¯**ã€‚è®ºæ–‡è¯æ˜äº†LS-NFSPèƒ½åœ¨è¯¥è¯±å¯¼åšå¼ˆä¸­æ”¶æ•›åˆ°ä¸€ä¸ª**è¿‘ä¼¼çº³ä»€å‡è¡¡**ã€‚\n        *   **å®é™…ä»·å€¼**ï¼šè¿™æ˜¯æœ¬æ–‡è§£å†³ **â€œæˆ˜ç•¥æ¼”åŒ– vs. ç³»ç»Ÿç¨³å®šæ€§â€** çŸ›ç›¾çš„æ ¸å¿ƒã€‚å®ƒåœ¨**ä¸ç ´åç‰©ç†ç¨³å®šæ€§çš„å‰æä¸‹**ï¼Œå…è®¸æˆ˜ç•¥åœ¨å®‰å…¨çš„æ½œç©ºé—´å†…å……åˆ†æ¢ç´¢å’Œå¯¹æŠ—ï¼Œä»è€Œæ¶Œç°å‡ºå¤æ‚çš„æ‹³å‡»æˆ˜æœ¯ï¼ˆå¦‚æ»‘æ­¥ã€åå‡»ï¼‰ã€‚\n\n### æ€»ç»“ï¼šRoboStrikerçš„é›†æˆåˆ›æ–°ä¸è´¡çŒ®\n\n| å‘å±•é˜¶æ®µ | æ ¸å¿ƒé—®é¢˜ | ä»£è¡¨æ€§æŠ€æœ¯ | RoboStrikerçš„æ”¹è¿›ä¸æ•´åˆ |\n| :--- | :--- | :--- | :--- |\n| **1. å•æ™ºèƒ½ä½“è¿åŠ¨æ§åˆ¶** | å¦‚ä½•ç”Ÿæˆç‰©ç†å¯è¡Œã€è‡ªç„¶çš„äººå½¢è¿åŠ¨ï¼Ÿ | DeepMimic (æ¨¡ä»¿), AMP (é£æ ¼åŒ–) | **ç»§æ‰¿å¹¶ä½œä¸ºåŸºçŸ³**ï¼šç¬¬ä¸€é˜¶æ®µè®­ç»ƒé€šç”¨è¿åŠ¨è·Ÿè¸ªå™¨ï¼Œæä¾›ç‰©ç†å¯è¡Œçš„æŠ€èƒ½åº“ã€‚ |\n| **2. æŠ€èƒ½æŠ½è±¡è¡¨ç¤º** | å¦‚ä½•ç®€åŒ–é«˜å±‚æ§åˆ¶ï¼Œé¿å…ç‰©ç†å¤±æ•ˆï¼Ÿ | PULSE, CALM (æ½œç©ºé—´è’¸é¦) | **å…³é”®æ”¹è¿›**ï¼šå¼•å…¥**æ‹“æ‰‘çº¦æŸï¼ˆè¶…çƒé¢ï¼‰**ï¼Œåˆ›å»º**æœ‰ç•Œã€å®‰å…¨çš„æ½œåŠ¨ä½œç©ºé—´**ï¼Œè§£è€¦æˆ˜ç•¥ä¸ç‰©ç†ã€‚ |\n| **3. å¤šæ™ºèƒ½ä½“æˆ˜ç•¥å­¦ä¹ ** | å¦‚ä½•åœ¨ç‰©ç†çº¦æŸä¸‹è¿›è¡Œæˆ˜ç•¥å¯¹æŠ—ä¸æ¼”åŒ–ï¼Ÿ | NFSP (åšå¼ˆè®ºMARL) | **æ ¸å¿ƒåˆ›æ–°**ï¼šæå‡º **LS-NFSP**ï¼Œåœ¨**æœ‰ç•Œæ½œç©ºé—´**ä¸­è¿›è¡Œè™šæ‹Ÿè‡ªæˆ‘åšå¼ˆï¼Œç»“åˆ**è¡Œä¸ºé¢„çƒ­**å’Œ**AMPæ­£åˆ™åŒ–**ï¼Œé¦–æ¬¡åœ¨**é«˜ç»´å…·èº«ç³»ç»Ÿ**ä¸­å®ç°äº†ç¨³å®šçš„æˆ˜ç•¥ååŒæ¼”åŒ–ã€‚ |\n\n**æœ€ç»ˆï¼ŒRoboStrikerçš„åˆ›æ–°ä»·å€¼åœ¨äºï¼š**\nå®ƒ**ä¸æ˜¯**å¯¹å•ä¸€æ–¹å‘çš„æ”¹è¿›ï¼Œè€Œæ˜¯**é¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯†åˆ«å¹¶è§£å†³äº†å…·èº«å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸¤å¤§å›ºæœ‰çŸ›ç›¾**ï¼Œå¹¶é€šè¿‡ä¸€ä¸ª**å±‚æ¬¡åŒ–æ¡†æ¶**å°†ä¸Šè¿°ä¸‰ä¸ªå‘å±•æ–¹å‘æœ‰æœºèåˆï¼š\n1.  **åº•å±‚**ï¼šåŸºäºæ¨¡ä»¿å­¦ä¹ çš„**ç‰©ç†å¯è¡Œæ€§**ä¿éšœã€‚\n2.  **ä¸­å±‚**ï¼šåŸºäºæ‹“æ‰‘æ½œç©ºé—´è’¸é¦çš„**å®‰å…¨ç­–ç•¥æŠ½è±¡**ã€‚\n3.  **é«˜å±‚**ï¼šåŸºäºLS-NFSPçš„**ç¨³å®šæˆ˜ç•¥æ¼”åŒ–**ã€‚\n\nè¿™ä¸ªæ¡†æ¶ä¸ºå°†MARLä»æŠ½è±¡æ¸¸æˆæˆåŠŸæ‰©å±•åˆ°å¤æ‚çš„ç‰©ç†å…·èº«ç³»ç»Ÿï¼ˆå¦‚ç«æŠ€ã€åä½œä½œä¸šï¼‰æä¾›äº†ä¸€æ¡**åŸåˆ™æ€§çš„æŠ€æœ¯è·¯å¾„**ï¼Œå…¶â€œä»¿çœŸåˆ°ç°å®â€çš„è¿ç§»ç»“æœ",
            "timestamp": "2026-02-03T15:55:48.017341",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œä½œè€…å›¢é˜Ÿåœ¨**äººå½¢æœºå™¨äººè¿åŠ¨æ§åˆ¶ä¸æ¨¡ä»¿å­¦ä¹ **é¢†åŸŸæœ‰ä¸€ç³»åˆ—ç›¸å…³çš„å‰æœŸå·¥ä½œã€‚ä»¥ä¸‹æ˜¯è¿™äº›å‰ä½œåŠå…¶ä¸æœ¬æ–‡ã€ŠRoboStrikerã€‹åŒºåˆ«çš„è¯¦ç»†åˆ†æã€‚\n\n### å‰ä½œæ¦‚è§ˆè¡¨\n\n| è®ºæ–‡ç®€ç§° / å¼•ç”¨ | æ ¸å¿ƒè´¡çŒ® | ä¸ã€ŠRoboStrikerã€‹çš„ä¸»è¦åŒºåˆ« | ã€ŠRoboStrikerã€‹çš„ä¸»è¦æ”¹è¿›ä¸åˆ›æ–° |\n| :--- | :--- | :--- | :--- |\n| **DeepMimic** (Peng et al., 2018) | **å•æŠ€èƒ½æ¨¡ä»¿å­¦ä¹ **ï¼šæå‡ºäº†åŸºäºç¤ºä¾‹çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå­¦ä¹ ç‰©ç†æ¨¡æ‹Ÿè§’è‰²çš„æŠ€èƒ½ã€‚é€šè¿‡å¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œä½¿ç­–ç•¥èƒ½é«˜ä¿çœŸåœ°è·Ÿè¸ªè¿åŠ¨æ•æ‰æ•°æ®ã€‚ | **1. ä»»åŠ¡èŒƒç•´**ï¼šä¸“æ³¨äº**å•æ™ºèƒ½ä½“ã€å•æŠ€èƒ½**çš„æ¨¡ä»¿ä¸è·Ÿè¸ªï¼ˆå¦‚åç©ºç¿»ã€è·‘æ­¥ï¼‰ã€‚<br>**2. æ— ç­–ç•¥æ€§**ï¼šç¯å¢ƒæ˜¯é™æ€æˆ–è¢«åŠ¨çš„ï¼Œä¸æ¶‰åŠä¸å…¶ä»–æ™ºèƒ½ä½“çš„**ç­–ç•¥æ€§äº¤äº’**æˆ–å¯¹æŠ—ã€‚<br>**3. åŠ¨ä½œç©ºé—´**ï¼šç›´æ¥åœ¨åŸå§‹çš„é«˜ç»´å…³èŠ‚åŠ¨ä½œç©ºé—´è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚ | **æ ¹æœ¬æ€§æ‰©å±•**ï¼šä»**å•æ™ºèƒ½ä½“æ¨¡ä»¿**è½¬å‘**å¤šæ™ºèƒ½ä½“ç«äº‰**ã€‚å°†DeepMimicä½œä¸º**åº•å±‚è¿åŠ¨è·Ÿè¸ªå™¨**ï¼ˆStage Iï¼‰ï¼Œä¸ºå…¶èµ‹äºˆäº†**é«˜å±‚ç­–ç•¥å†³ç­–**èƒ½åŠ›ï¼Œè§£å†³äº†â€œå¦‚ä½•æ‰“â€è€Œéâ€œå¦‚ä½•æ¨¡ä»¿ä¸€ä¸ªå›ºå®šåŠ¨ä½œâ€çš„é—®é¢˜ã€‚ |\n| **AMP** (Peng et al., 2021) | **å¯¹æŠ—æ€§è¿åŠ¨å…ˆéªŒ**ï¼šé€šè¿‡ä¸€ä¸ªåˆ¤åˆ«å™¨æ¥æä¾›é£æ ¼å¥–åŠ±ï¼Œä½¿ç­–ç•¥åœ¨å®Œæˆç›®æ ‡ä»»åŠ¡çš„åŒæ—¶ï¼Œç”Ÿæˆçš„åŠ¨ä½œé£æ ¼ä¸å‚è€ƒè¿åŠ¨æ•°æ®é›†åˆ†å¸ƒä¸€è‡´ï¼Œä»è€Œå­¦ä¹ åˆ°å¤šæ ·ã€è‡ªç„¶çš„è¿åŠ¨é£æ ¼ã€‚ | **1. å­¦ä¹ ç›®æ ‡**ï¼šä¸»è¦ç”¨äºæå‡**å•æ™ºèƒ½ä½“**è¿åŠ¨çš„**é£æ ¼å¤šæ ·æ€§å’Œè‡ªç„¶åº¦**ï¼Œä¾‹å¦‚å­¦ä¹ ä¸åŒèµ°è·¯é£æ ¼ã€‚<br>**2. éå¯¹æŠ—ç¯å¢ƒ**ï¼šå…¶â€œå¯¹æŠ—â€æŒ‡çš„æ˜¯ç”Ÿæˆå™¨ä¸åˆ¤åˆ«å™¨ä¹‹é—´çš„å¯¹æŠ—ï¼Œ**ä¸æ˜¯æ™ºèƒ½ä½“ä¹‹é—´çš„ç­–ç•¥å¯¹æŠ—**ã€‚ | **åŠŸèƒ½æ•´åˆä¸å†å®šä½**ï¼šå°†AMPä½œä¸º**è¯¾ç¨‹çƒ­èº«é˜¶æ®µï¼ˆStage IIIaï¼‰çš„è§„åˆ™å™¨**ï¼Œç”¨äºç¡®ä¿æ™ºèƒ½ä½“åœ¨åˆæ­¥å­¦ä¹ æ”»å‡»é™æ­¢ç›®æ ‡æ—¶ï¼ŒåŠ¨ä½œä¸åç¦»äººç±»æ‹³å‡»é£æ ¼ã€‚è¿™è§£å†³äº†ç«äº‰å­¦ä¹ ä¸­å¯èƒ½å‡ºç°çš„åŠ¨ä½œç•¸å½¢é—®é¢˜ï¼Œä¸ºåç»­ç¨³å®šçš„ç­–ç•¥åšå¼ˆæ‰“ä¸‹ç‰©ç†å’Œé£æ ¼åŸºç¡€ã€‚ |\n| **Unitracker** (Yin et al., 2025) | **é€šç”¨å…¨èº«è¿åŠ¨è·Ÿè¸ªå™¨**ï¼šå­¦ä¹ ä¸€ä¸ª**ç»Ÿä¸€ç­–ç•¥**ï¼Œèƒ½å¤Ÿè·Ÿè¸ªä¸€ä¸ªåºå¤§ä¸”å¤šæ ·çš„äººä½“è¿åŠ¨æ•°æ®åº“ä¸­çš„ä»»ä½•è¿åŠ¨ç‰‡æ®µï¼Œå®ç°äº†äººå½¢æœºå™¨äººçš„é€šç”¨è¿åŠ¨å¤ç°èƒ½åŠ›ã€‚ | **1. ååº”æ€§ vs. ä¸»åŠ¨æ€§**ï¼šä½œä¸ºä¸€ä¸ªâ€œé€šç”¨æ’­æ”¾å™¨â€ï¼Œå…¶æ ¸å¿ƒæ˜¯**é«˜ä¿çœŸåœ°ååº”å¼è·Ÿè¸ª**ç»™å®šçš„å‚è€ƒè¿åŠ¨ã€‚ç¼ºä¹**è‡ªä¸»å†³ç­–**èƒ½åŠ›ï¼Œä¸çŸ¥é“åœ¨ä½•æ—¶è¯¥æ‰§è¡Œä½•ç§è¿åŠ¨ã€‚<br>**2. è¾“å…¥ä¾èµ–**ï¼šä¸¥é‡ä¾èµ–å¤–éƒ¨æä¾›çš„`å‚è€ƒè¿åŠ¨çŠ¶æ€(s_t^ref)`ä½œä¸ºè¾“å…¥ã€‚ | **æ¶æ„å‡çº§ä¸è§£è€¦**ï¼šå°†Unitrackerç±»æŠ€æœ¯ä½œä¸º**åº•å±‚åŸºç¡€èƒ½åŠ›**å°è£…èµ·æ¥ã€‚åœ¨ã€ŠRoboStrikerã€‹çš„Stage Iä¸­è®­ç»ƒäº†ç±»ä¼¼çš„è¿åŠ¨è·Ÿè¸ªå™¨ï¼Œä½†åœ¨åç»­é˜¶æ®µï¼Œç”¨**æ½œç©ºé—´ç¼–ç å™¨**å–ä»£äº†å¯¹å¤–éƒ¨å‚è€ƒè¿åŠ¨çš„ä¾èµ–ã€‚é«˜å±‚ç­–ç•¥åœ¨æ½œç©ºé—´ä¸­å†³ç­–ï¼ˆè¾“å‡º`z_t`ï¼‰ï¼Œå†ç”±è§£ç å™¨ç”ŸæˆåŠ¨ä½œï¼Œå®ç°äº†**ç­–ç•¥ä¸æ‰§è¡Œçš„è§£è€¦**ã€‚ |\n| **GMT** (Chen et al., 2025) / **Track any motions** (Zhang et al., 2025) | **é€šç”¨è¿åŠ¨è·Ÿè¸ª**ï¼šä¸Unitrackerç±»ä¼¼ï¼Œæ—¨åœ¨å»ºç«‹æ›´å¼ºå¤§ã€æ›´é²æ£’çš„ç»Ÿä¸€è¿åŠ¨è·Ÿè¸ªç­–ç•¥ï¼Œå¤„ç†æ›´å¤æ‚çš„è¿åŠ¨å’Œç¯å¢ƒæ‰°åŠ¨ã€‚ | **å…±æ€§åŒºåˆ«**ï¼šè¿™äº›å·¥ä½œéƒ½å±äºâ€œ**è¿åŠ¨è·Ÿè¸ª**â€èŒƒå¼ï¼Œæ ¸å¿ƒæ˜¯**æ¨¡ä»¿çš„ä¿çœŸåº¦ä¸é²æ£’æ€§**ã€‚å®ƒä»¬ä¸ºæœºå™¨äººæä¾›äº†ä¸°å¯Œçš„â€œæŠ€èƒ½åº“â€ï¼Œä½†**æ²¡æœ‰å®šä¹‰å¦‚ä½•è‡ªä¸»ã€æ™ºèƒ½åœ°ç»„åˆå’Œè°ƒç”¨è¿™äº›æŠ€èƒ½**æ¥å®Œæˆä¸€ä¸ªç«äº‰æ€§ç›®æ ‡ã€‚ | **ä»â€œæŠ€èƒ½åº“â€åˆ°â€œç­–ç•¥å¼•æ“â€**ï¼šã€ŠRoboStrikerã€‹çš„åˆ›æ–°åœ¨äº**æ„å»ºäº†ä¸€ä¸ªè°ƒç”¨æŠ€èƒ½åº“çš„â€œç­–ç•¥å¼•æ“â€**ã€‚å®ƒä¸å†é—®â€œå¦‚ä½•åšå‡ºè¿™ä¸ªåŠ¨ä½œâ€ï¼Œè€Œæ˜¯é—®â€œ**ç°åœ¨åº”è¯¥åšä»€ä¹ˆåŠ¨ä½œæ‰èƒ½èµ¢å¾—æ¯”èµ›**â€ã€‚è¿™æ˜¯ä»â€œæ‰§è¡Œå±‚â€åˆ°â€œå†³ç­–å±‚â€çš„å…³é”®è·¨è¶Šã€‚ |\n| **PULSE** (Luo et al., 2023) / **CALM** (Tessler et al., 2023) | **æŠ€èƒ½æ½œè¡¨ç¤ºå­¦ä¹ **ï¼šå°†å¤§é‡è¿åŠ¨æ•°æ®æç‚¼æˆè¿ç»­çš„æ½œç©ºé—´è¡¨ç¤ºã€‚é«˜å±‚ç­–ç•¥é€šè¿‡åœ¨è¿™ä¸ªä½ç»´æ½œç©ºé—´ä¸­æ“ä½œï¼Œå¯ä»¥é¡ºç•…åœ°ç»„åˆå’Œç”Ÿæˆå¤æ‚çš„æŠ€èƒ½ã€‚ | **1. ç©ºé—´è¾¹ç•Œ**ï¼šè¿™äº›æ–¹æ³•å­¦ä¹ çš„æ½œç©ºé—´é€šå¸¸æ˜¯**æ— ç•Œçš„æ¬§å‡ é‡Œå¾—ç©ºé—´**ï¼Œé«˜å±‚ç­–ç•¥çš„æ¢ç´¢å¯èƒ½äº§ç”Ÿè¶…å‡ºåˆ†å¸ƒçš„åŠ¨ä½œï¼Œå¯¼è‡´ç‰©ç†æ¨¡æ‹Ÿå´©æºƒã€‚<br>**2. åº”ç”¨åœºæ™¯**ï¼šä¸»è¦ç”¨äº**å•æ™ºèƒ½ä½“**çš„ä»»åŠ¡ï¼ˆå¦‚å¯¼èˆªã€ç‰©ä½“äº¤äº’ï¼‰ï¼Œæœªæ¶‰åŠå¤šæ™ºèƒ½ä½“ç«äº‰ç¯å¢ƒä¸‹çš„ç­–ç•¥ç¨³å®šæ€§é—®é¢˜ã€‚ | **æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ä¸€ï¼šæ‹“æ‰‘çº¦æŸæ½œç©ºé—´**ï¼šåœ¨Stage IIä¸­ï¼Œé€šè¿‡å¯¹é«˜æ–¯å‚æ•°åŒ–åˆ†å¸ƒè¿›è¡Œ**æŠ•å½±å½’ä¸€åŒ–**ï¼Œå°†æ½œç©ºé—´çº¦æŸåœ¨**å•ä½è¶…çƒé¢**ä¸Šã€‚è¿™åˆ›é€ äº†ä¸€ä¸ª**ç´§è‡´ã€æœ‰ç•Œçš„ç­–ç•¥æœç´¢ç©ºé—´**ï¼Œä»æ ¹æœ¬ä¸Šå°†ç­–ç•¥æ¢ç´¢é™åˆ¶åœ¨ç‰©ç†å¯è¡Œçš„è¿åŠ¨å­ç©ºé—´å†…ï¼Œæ˜¯ç¨³å®šå¤šæ™ºèƒ½ä½“è®­ç»ƒçš„å…³é”®ã€‚ |\n| **NFSP** (Heinrich and Silver, 2016) | **ç¥ç»è™šæ‹Ÿè‡ªæˆ‘åšå¼ˆ**ï¼šä¸€ç§ç”¨äºä¸å®Œå…¨ä¿¡æ¯åšå¼ˆï¼ˆå¦‚å¾·å·æ‰‘å…‹ï¼‰çš„MARLç®—æ³•ã€‚å®ƒé€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆè®­ç»ƒæœ€ä¼˜ååº”ç­–ç•¥ï¼‰å’Œç›‘ç£å­¦ä¹ ï¼ˆè®­ç»ƒå¹³å‡ç­–ç•¥ï¼‰æ¥è¿‘ä¼¼è™šæ‹Ÿåšå¼ˆï¼Œæ”¶æ•›åˆ°çº³ä»€å‡è¡¡ã€‚ | **1. åŠ¨ä½œç©ºé—´**ï¼šåŸNFSPè®¾è®¡ç”¨äº**ç¦»æ•£çš„ã€ç¬¦å·åŒ–çš„åŠ¨ä½œç©ºé—´**ï¼ˆå¦‚æ‰‘å…‹ä¸­çš„åŠ æ³¨ã€è·Ÿæ³¨ï¼‰ã€‚<br>**2. é¢†åŸŸ**ï¼šåº”ç”¨äºæŠ½è±¡åšå¼ˆé¢†åŸŸï¼Œ**å®Œå…¨ç¼ºä¹å¯¹ç‰©ç†å¯è¡Œæ€§çš„è€ƒé‡**ã€‚ç›´æ¥å°†å…¶ç”¨äºé«˜ç»´è¿ç»­æ§åˆ¶ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œæ— æ•ˆåŠ¨ä½œã€‚ | **æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°äºŒï¼šæ½œç©ºé—´NFSP**ï¼šå°†ç»å…¸çš„NFSPç®—æ³•**ç§»æ¤åˆ°æœ‰ç•Œçš„è¿ç»­æ½œåŠ¨ä½œç©ºé—´**ä¸­ã€‚æ™ºèƒ½ä½“åœ¨æ½œç©ºé—´ä¸­è¿›è¡Œåšå¼ˆï¼ˆStage IIIbï¼‰ï¼Œè€ŒéåŸå§‹çš„29ç»´å…³èŠ‚ç©ºé—´ã€‚è¿™**æ˜¾è‘—é™ä½äº†ç­–ç•¥æœç´¢çš„ç»´åº¦ä¸éå¹³ç¨³æ€§**ï¼Œä½¿å¾—åšå¼ˆè®ºç®—æ³•èƒ½å¤Ÿé¦–æ¬¡ç¨³å®šåœ°åº”ç”¨äºé«˜ç»´äººå½¢æœºå™¨äººæ§åˆ¶é—®é¢˜ã€‚ |\n\n### æ€»ç»“ï¼šä¸»è¦æ”¹å–„ä¸æ ¸å¿ƒè´¡çŒ®\n\nã€ŠRoboStrikerã€‹å¹¶éç®€å•åœ°å°†å‰ä½œæŠ€æœ¯å †å ï¼Œè€Œæ˜¯è¿›è¡Œäº†ä¸€æ¬¡**ä½“ç³»åŒ–çš„æ¡†æ¶é‡æ„ä¸åˆ›æ–°é›†æˆ**ï¼Œä¸»è¦æ”¹å–„ä½“ç°åœ¨ï¼š\n\n1.  **é—®é¢˜èŒƒå¼æ ¹æœ¬æ€§è½¬å˜**ï¼šä»å‰ä½œçš„ **â€œè¿åŠ¨ç”Ÿæˆâ€** è½¬å‘æœ¬æ–‡çš„ **â€œç­–ç•¥æ€§å…±é€‚åº”â€** ã€‚è¿™æ˜¯ä»è¢«åŠ¨æ‰§è¡Œåˆ°ä¸»åŠ¨åšå¼ˆçš„è´¨å˜ã€‚\n2.  **å±‚çº§åŒ–æ¶æ„è§£è€¦**ï¼šåˆ›æ–°æ€§åœ°æå‡º**ä¸‰é˜¶æ®µæ¡†æ¶**ï¼Œæ¸…æ™°åœ°å°†é—®é¢˜åˆ†è§£ï¼š\n    - **Stage I (ç‰©ç†åŸºç¡€)**ï¼šåˆ©ç”¨å‰ä½œï¼ˆDeepMimic/Unitrackerï¼‰è§£å†³ **â€œç‰©ç†å¯è¡Œæ€§â€**ã€‚\n    - **Stage II (ç­–ç•¥æ¥å£)**ï¼šå¼•å…¥**æ‹“æ‰‘çº¦æŸæ½œç©ºé—´**ï¼Œå°†æŠ€èƒ½ç¼–ç ä¸ºé«˜å±‚çš„å®‰å…¨ã€ç´§å‡‘åŠ¨ä½œç©ºé—´ï¼Œè§£å†³äº† **â€œæ¢ç´¢ä¸ç¨³å®šâ€** çš„çŸ›ç›¾ã€‚\n    - **Stage III (ç­–ç•¥å­¦ä¹ )**ï¼šå¼•å…¥**LS-NFSP**ï¼Œåœ¨æ½œç©ºé—´å†…è¿›è¡Œåšå¼ˆï¼Œè§£å†³äº† **â€œç­–ç•¥è¿›åŒ–ä¸ç³»ç»Ÿç¨³å®šâ€** çš„çŸ›ç›¾ã€‚\n3.  **å…³é”®æŠ€æœ¯èåˆä¸æ”¹é€ **ï¼š\n    - å°† **AMP** ä»é£æ ¼å­¦ä¹ å·¥å…·æ”¹é€ ä¸º**è¯¾ç¨‹çƒ­èº«é˜¶æ®µçš„è§„åˆ™å™¨**ï¼Œé˜²æ­¢ç«äº‰å­¦ä¹ åˆæœŸåŠ¨ä½œå¤±çœŸã€‚\n    - å°† **NFSP** ä»ç¦»æ•£åšå¼ˆç®—æ³•æ”¹é€ ä¸ºé€‚ç”¨äºè¿ç»­ç‰©ç†æ§åˆ¶çš„ **LS-NFSP**ã€‚\n    - å°† **æ½œè¡¨ç¤ºå­¦ä¹ ** ä»æ— ç•Œç©ºé—´æ”¹è¿›ä¸º**æœ‰ç•Œè¶…çƒé¢ç©ºé—´**ï¼Œæä¾›äº†ç†è®ºæ”¶æ•›ä¿è¯å’Œè®­ç»ƒç¨³å®šæ€§ã€‚\n4.  **ç³»ç»Ÿæ€§è§£å†³ä¸¤å¤§çŸ›ç›¾**ï¼šè®ºæ–‡æ˜ç¡®æå‡ºäº† embodied MARL çš„ä¸¤ä¸ªæ ¸å¿ƒçŸ›ç›¾ï¼Œå¹¶é€šè¿‡ä¸Šè¿°æ¡†æ¶é€ä¸€ç ´è§£ï¼š\n    - **ç‰©ç†å¯è¡Œæ€§ vs. éå¹³ç¨³å­¦ä¹ **ï¼šé€šè¿‡**æœ‰ç•Œæ½œç©ºé—´**é™åˆ¶æ¢ç´¢èŒƒå›´æ¥è§£å†³ã€‚\n    - **ç­–ç•¥è¿›åŒ– vs. ç³»ç»Ÿç¨³å®š**ï¼šé€šè¿‡**LS-NFSPåœ¨æ½œç©ºé—´å†…åšå¼ˆ**ä»¥åŠ**AMPçƒ­èº«**æ¥ç¨³å®šè®­ç»ƒåŠ¨æ€ã€‚\n\n**ç»“è®º**ï¼šã€ŠRoboStrikerã€‹çš„æ ¸å¿ƒçªç ´åœ¨äºï¼Œå®ƒ**é¦–æ¬¡æˆåŠŸåœ°å°†åšå¼ˆè®ºå±‚é¢çš„å¤šæ™ºèƒ½ä½“ç­–ç•¥å­¦ä¹ ï¼Œä¸ç‰©ç†å±‚é¢çš„äººå½¢æœºå™¨äººæ•æ·æ§åˆ¶ï¼Œé€šè¿‡ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„å±‚çº§åŒ–æ½œç©ºé—´æ¥å£è¿æ¥èµ·æ¥**ã€‚æ¯ä¸€ç¯‡å‰ä½œéƒ½æ˜¯è¿™ä¸ªå®å¤§æ‹¼å›¾ä¸­çš„ä¸€å—ç –çŸ³ï¼Œè€Œæœ¬æ–‡åˆ™æ˜¯è®¾è®¡äº†å…¨æ–°çš„è“å›¾ï¼Œå°†è¿™äº›ç –çŸ³ç Œæˆäº†ä¸€ä¸ªèƒ½è¿›è¡Œè‡ªä¸»æ ¼æ–—çš„â€œæ™ºèƒ½ä½“â€ï¼Œå®ç°äº†ä»â€œæ¨¡ä»¿äººâ€åˆ°â€œåœ¨å¯¹æŠ—ä¸­åƒäººä¸€æ ·æ€è€ƒå¹¶è¡ŒåŠ¨â€çš„è·¨è¶Šã€‚",
            "timestamp": "2026-02-03T15:56:58.670281",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## è®ºæ–‡å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\næœ¬æ–‡ã€ŠRoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxingã€‹æå‡ºäº†ä¸€ç§è§£å†³å…·èº«å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­ç‰©ç†å¯è¡Œæ€§ä¸ç­–ç•¥æ¼”åŒ–çŸ›ç›¾çš„ä¸‰å±‚åˆ†å±‚æ¡†æ¶ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†é«˜å±‚ç­–ç•¥æ¨ç†ä¸åº•å±‚ç‰©ç†æ‰§è¡Œè§£è€¦ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªç»“æ„åŒ–çš„æ½œåœ¨ç©ºé—´æ¥ç¨³å®šç«äº‰æ€§å­¦ä¹ ã€‚å…³é”®æŠ€æœ¯æ–¹æ³•å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æ–¹æ³•é˜¶æ®µ | æ ¸å¿ƒç»„ä»¶ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°ä¸å®ç° |\n| :--- | :--- | :--- | :--- |\n| **ç¬¬ä¸€é˜¶æ®µï¼šä¸“å®¶è¿åŠ¨è¿½è¸ªå™¨å­¦ä¹ ** | **åŸºäºDeepMimicçš„è·Ÿè¸ªç­–ç•¥** `Ï€_trk` | **ç›®æ ‡**ï¼šä»äººç±»åŠ¨ä½œæ•æ‰æ•°æ®ä¸­å­¦ä¹ ä¸€ä¸ªé²æ£’çš„ä½å±‚æ§åˆ¶å™¨ï¼Œç”Ÿæˆç‰©ç†ä¸Šå¯è¡Œçš„äººå½¢è¿åŠ¨åº“ã€‚<br>**æ•°å­¦åŸç†**ï¼š<br>1. **çŠ¶æ€ä¸è§‚å¯Ÿ**ï¼šç­–ç•¥è¾“å…¥ä¸º `o_t = (s_t^self, s_t^ref)`ï¼Œå…¶ä¸­ `s_t^self` ä¸ºæœºå™¨äººæœ¬ä½“æ„ŸçŸ¥çŠ¶æ€ï¼ˆå…³èŠ‚ä½ç½®ã€é€Ÿåº¦ç­‰ï¼‰ï¼Œ`s_t^ref` ä¸ºä»åŠ¨æ•æ•°æ® `D_motion` ä¸­é‡‡æ ·çš„å‚è€ƒè¿åŠ¨ç›®æ ‡çŠ¶æ€ï¼ˆå§¿æ€ã€é€Ÿåº¦ï¼‰ã€‚<br>2. **å¥–åŠ±å‡½æ•°**ï¼šæœ€å¤§åŒ–è·Ÿè¸ªå¥–åŠ±çš„æœŸæœ›ã€‚å¥–åŠ± `r_trk` ä¸ºå¤šä¸ªæŒ‡æ•°å‹å¯¹é½è¯¯å·®çš„åŠ æƒå’Œï¼š<br>`r_trk = Î£_{xâˆˆX} w_x Â· exp(-â€–err_xâ€–Â² / Ïƒ_xÂ²) - r_reg`<br>å…¶ä¸­ `X = {p, o, bp, bo, lv, av}` åˆ†åˆ«ä»£è¡¨æ ¹ä½ç½®ã€æ ¹æœå‘ã€èº«ä½“éƒ¨ä½ä½ç½®ã€èº«ä½“éƒ¨ä½æœå‘ã€çº¿é€Ÿåº¦ã€è§’é€Ÿåº¦çš„è·Ÿè¸ªè¯¯å·®ã€‚`r_reg` ä¸ºæ­£åˆ™åŒ–é¡¹ï¼Œæƒ©ç½šåŠ¨ä½œå˜åŒ–ç‡ã€å…³èŠ‚é™ä½å’Œè‡ªç¢°æ’ã€‚<br>3. **ä¼˜åŒ–ç›®æ ‡**ï¼š<br>`Ï€_trk* = argmax_{Ï€_trk} E_{s^refâˆ¼Ï„^ref, Ï„^refâˆ¼D_motion}[J_{Ï€_trk}(s^ref)]`<br>`J_{Ï€_trk}(s^ref) = E_{a_tâˆ¼Ï€_trk}[Î£_{t=0}^T Î³^t r_trk(s_t^self, a_t; s^ref)]` | **æœºå™¨äººå¹³å°**ï¼šUnitree G1 (29è‡ªç”±åº¦)ã€‚<br>**åŠ¨æ•æ•°æ®**ï¼š46æ®µä¸“ä¸šæ‹³å‡»åŠ¨ä½œï¼Œæ€»è®¡çº¦14åˆ†é’Ÿï¼Œé¢‘ç‡50Hzï¼Œç»å·¦å³é•œåƒå¢å¼ºã€‚<br>**å¥–åŠ±æƒé‡ (w_x)** ä¸ **ç¼©æ”¾å› å­ (Ïƒ_x)**ï¼š<br>- æ ¹ä½ç½®/æœå‘: w=0.5, Ïƒ=0.3/0.4<br>- èº«ä½“ä½ç½®/æœå‘: w=1.0, Ïƒ=0.3/0.4<br>- çº¿é€Ÿåº¦/è§’é€Ÿåº¦: w=1.0, Ïƒ=1.0/3.14<br>**è®­ç»ƒ**ï¼šåœ¨Isaac Labä¸­ä½¿ç”¨4096ä¸ªå¹¶è¡Œç¯å¢ƒã€‚ |\n| **ç¬¬äºŒé˜¶æ®µï¼šæ‹“æ‰‘æ½œåœ¨è’¸é¦ç¼–ç ** | **æœ‰ç•Œæ½œåœ¨ç©ºé—´æ„å»º** (ç¼–ç å™¨ `E_Ï†`, è§£ç å™¨ `D_Ïˆ`, å…ˆéªŒç½‘ç»œ `P_Î¾`) | **ç›®æ ‡**ï¼šå°†ç¬¬ä¸€é˜¶æ®µå­¦åˆ°çš„ä¸°å¯Œè¿åŠ¨æŠ€èƒ½å‹ç¼©åˆ°ä¸€ä¸ªè¿ç»­ã€æœ‰ç•Œçš„æ½œåœ¨ç©ºé—´ `Z`ï¼Œä½œä¸ºé«˜å±‚ç­–ç•¥çš„åŠ¨ä½œç©ºé—´ã€‚<br>**æ•°å­¦åŸç†**ï¼š<br>1. **è’¸é¦æ¡†æ¶**ï¼šé‡‡ç”¨å¸ˆç”Ÿæ¶æ„ã€‚ç¼–ç å™¨å°†è§‚å¯Ÿ `o_t` æ˜ å°„ä¸ºæ½œåœ¨åˆ†å¸ƒ `z_t âˆ¼ E_Ï†(Â·|o_t) = N(Î¼_t^e, Ïƒ_t^e)`ã€‚è§£ç å™¨æ ¹æ®æœ¬ä½“çŠ¶æ€ `s_t^prop` å’Œæ½œåœ¨ç  `z_t` é‡å»ºæ•™å¸ˆåŠ¨ä½œï¼š`Ã¢_t âˆ¼ D_Ïˆ(Â·|s_t^prop, z_t)`ã€‚<br>2. **çŠ¶æ€æ¡ä»¶å…ˆéªŒ**ï¼šå­¦ä¹ ä¸€ä¸ªå…ˆéªŒç½‘ç»œ `P_Î¾(z_t|s_t^prop) = N(z_t|Î¼_t^p, Ïƒ_t^p)`ï¼Œå¹¶é€šè¿‡KLæ•£åº¦çº¦æŸç¼–ç å™¨åéªŒï¼Œé˜²æ­¢åéªŒåå¡Œï¼š<br>`(E_Ï†*, P_Î¾*) = argmin_{(E_Ï†, P_Î¾)} D_KL( E_Ï†(z_t|o_t) â€– P_Î¾(z_t|s_t^prop) )`<br>3. **æ‹“æ‰‘çº¦æŸï¼ˆå…³é”®åˆ›æ–°ï¼‰**ï¼šå¯¹é‡‡æ ·å‡ºçš„æ½œåœ¨ç è¿›è¡Œå½’ä¸€åŒ–ï¼Œå°†å…¶çº¦æŸåœ¨å•ä½è¶…çƒé¢ä¸Šï¼š<br>`áº‘ = Normalize(z) = E_Ï†(o) / â€–E_Ï†(o)â€–â‚‚`<br>è¿™ä½¿å¾—æ½œåœ¨ç©ºé—´ `Z = ğ•Š^{d-1}` æˆä¸ºä¸€ä¸ªç´§é›†ï¼Œç¡®ä¿äº†ç­–ç•¥æ¢ç´¢è¢«é™åˆ¶åœ¨ç‰©ç†åˆç†çš„è¿åŠ¨å­æµå½¢å†…ã€‚ | **æ½œåœ¨ç©ºé—´ç»´åº¦**ï¼š`d = 32`ã€‚<br>**æŸå¤±å‡½æ•°**ï¼š`â„’_distill = â„’_rec + Î»_prior Â· â„’_prior`ï¼Œå…¶ä¸­ `â„’_rec` ä¸ºåŠ¨ä½œé‡å»ºçš„MSEæŸå¤±ï¼Œ`â„’_prior` ä¸ºä¸Šè¿°KLæ•£åº¦æŸå¤±ã€‚<br>**å…ˆéªŒæŸå¤±æƒé‡**ï¼š`Î»_prior = 0.001`ã€‚ |\n| **ç¬¬ä¸‰é˜¶æ®µï¼šè¡Œä¸ºé¢„çƒ­ä¸æ½œåœ¨ç©ºé—´ç¥ç»è™šæ‹Ÿè‡ªåšå¼ˆ** | **3.1 è¡Œä¸ºé¢„çƒ­ (AMPæ­£åˆ™åŒ–)** | **ç›®æ ‡**ï¼šåœ¨è¿›å…¥æ¿€çƒˆå¯¹æŠ—å‰ï¼Œè®©æ™ºèƒ½ä½“å…ˆå­¦ä¹ é’ˆå¯¹é™æ­¢å¯¹æ‰‹çš„åŸºæœ¬æ”»å‡»æŠ€èƒ½ï¼Œè§£å†³â€œç«äº‰å†·å¯åŠ¨â€é—®é¢˜ã€‚<br>**æ•°å­¦åŸç†**ï¼š<br>1. **æ®‹å·®ç­–ç•¥**ï¼šé«˜å±‚ç­–ç•¥ `Ï€_z` è¾“å‡ºç›¸å¯¹äºå†»ç»“å…ˆéªŒ `P_Î¾^âŠ¥` çš„æ®‹å·®æ½œåœ¨å‘½ä»¤ `Î”z_t`ï¼š<br>`Ï€_z(Â·|s^prop, s^goal) = Ï€_Î¸(Â·|s^goal) âŠ• P_Î¾^âŠ¥(Â·|s^prop)`<br>`z_t = Normalize(Î”z_t + z_t^p)`, å…¶ä¸­ `z_t^p âˆ¼ P_Î¾^âŠ¥`ã€‚<br>2. **å¯¹æŠ—è¿åŠ¨å…ˆéªŒ**ï¼šå¼•å…¥åˆ¤åˆ«å™¨ `C` æ¥åŒºåˆ†æ™ºèƒ½ä½“äº§ç”Ÿçš„è¿åŠ¨è¿‡æ¸¡ `(o_t^disc, o_{t+1}^disc)` å’ŒåŠ¨æ•æ•°æ®ä¸­çš„è¿‡æ¸¡ï¼Œå¹¶ç»™å‡ºé£æ ¼å¥–åŠ± `r_style`ï¼Œç¡®ä¿åŠ¨ä½œè‡ªç„¶ã€ç±»äººã€‚<br>3. **æ··åˆå¥–åŠ±**ï¼šæ€»å¥–åŠ± `R(s_t, z_t) = w_task Â· r_task(s_t) + w_style Â· r_style(s_t, s_{t+1})`ã€‚`r_task` åŒ…å«é¢å‘å¥–åŠ±ã€é€Ÿåº¦å¥–åŠ±ã€è·ç¦»å¥–åŠ±å’Œå‘½ä¸­å¥–åŠ±ç­‰ã€‚ | **ä»»åŠ¡è§‚å¯Ÿ `s^goal`**ï¼šåŒ…å«è‡ªæˆ‘æ‹³å¤´åˆ°å¯¹æ‰‹èº¯å¹²çš„å‘é‡ï¼ˆè¿›æ”»ä¿¡æ¯ï¼‰å’Œå¯¹æ‰‹æ‹³å¤´åˆ°è‡ªæˆ‘èº¯å¹²çš„å‘é‡ï¼ˆé˜²å¾¡ä¿¡æ¯ï¼‰ï¼Œå‡è½¬æ¢åˆ°è‡ªæˆ‘åæ ‡ç³»ã€‚<br>**å¥–åŠ±æƒé‡**ï¼š`w_task` å’Œ `w_style` åŠ¨æ€è°ƒæ•´ï¼ŒåˆæœŸä»¥ `w_style` ä¸ºä¸»ï¼ŒåæœŸå¢åŠ  `w_task`ã€‚<br>**AMPåˆ¤åˆ«å™¨**ï¼šä½¿ç”¨å¸¦æ¢¯åº¦æƒ©ç½šçš„æœ€å°äºŒä¹˜GANè¿›è¡Œè®­ç»ƒã€‚ |\n| | **3.2 æ½œåœ¨ç©ºé—´ç¥ç»è™šæ‹Ÿè‡ªåšå¼ˆ** | **ç›®æ ‡**ï¼šåœ¨ç»“æ„åŒ–çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç«äº‰æ€§ååŒæ¼”åŒ–ï¼Œå­¦ä¹ é«˜çº§æˆ˜æœ¯ï¼Œå¹¶æ”¶æ•›åˆ°è¿‘ä¼¼çº³ä»€å‡è¡¡ã€‚<br>**æ•°å­¦åŸç†**ï¼š<br>1. **åŒç­–ç•¥æœºåˆ¶**ï¼šæ¯ä¸ªæ™ºèƒ½ä½“ç»´æŠ¤ä¸¤ä¸ªç­–ç•¥ï¼š<br>   - **æœ€ä½³å“åº”ç­–ç•¥ (RLç­–ç•¥) `Ï€_z^RL`**ï¼šä½¿ç”¨PPOä¼˜åŒ–ï¼Œæ—¨åœ¨é’ˆå¯¹å¯¹æ‰‹å½“å‰çš„å¹³å‡ç­–ç•¥è·å–æœ€å¤§æ”¶ç›Šã€‚<br>   - **å¹³å‡ç­–ç•¥ `Ï€Ì„_z`**ï¼šé€šè¿‡ç›‘ç£å­¦ä¹ æ‹Ÿåˆæ™ºèƒ½ä½“å†å²æœ€ä½³å“åº”è¡Œä¸ºï¼Œä½œä¸ºç¨³å®šçš„ç­–ç•¥é”šã€‚<br>2. **æ··åˆåŠ¨ä½œé€‰æ‹©**ï¼šæ¯ä¸€æ­¥ï¼Œä»¥æ¦‚ç‡ `Î·` é€‰æ‹©å¹³å‡ç­–ç•¥çš„åŠ¨ä½œï¼Œä»¥æ¦‚ç‡ `1-Î·` é€‰æ‹©æœ€ä½³å“åº”ç­–ç•¥çš„åŠ¨ä½œï¼š<br>`Ïƒ = Î· Â· Ï€Ì„_z + (1-Î·) Â· Ï€_z^RL`<br>`z_t âˆ¼ Ïƒ(Â·|s_t^prop, s_t^goal)`<br>3. **ç»éªŒå­˜å‚¨ä¸æ›´æ–°**ï¼š<br>   - **RLç¼“å†²åŒº `D_RL`**ï¼šå­˜å‚¨äº¤äº’ç»éªŒ `(o_t, z_t, r_t, o_{t+1})`ï¼Œç”¨äºæ›´æ–° `Ï€_z^RL`ã€‚<br>   - **SLç¼“å†²åŒº `D_SL`**ï¼šé‡‡ç”¨**è“„æ°´æ± é‡‡æ ·**å­˜å‚¨ `Ï€_z^RL` é€‰å‡ºçš„åŠ¨ä½œ `(o_t, z_t)`ï¼Œç¡®ä¿å†å²ç»éªŒå‡åŒ€è¦†ç›–ï¼Œç”¨äºé€šè¿‡æœ€å°åŒ–MSEæŸå¤±æ¥æ›´æ–° `Ï€Ì„_z`ï¼š`â„’_SL = E_{(o,z)âˆ¼D_SL}[â€–Ï€Ì„_z(o) - zâ€–Â²]`ã€‚<br>4. **ç†è®ºæ”¶æ•›æ€§**ï¼šåœ¨**å‡è®¾3.2-3.4**ï¼ˆæ½œåœ¨ç©ºé—´ç´§è‡´ã€æ”¶ç›Šå‡½æ•°æœ‰ç•Œã€æœ€ä½³å“åº”è¿‘ä¼¼è¯¯å·®æœ‰ç•Œï¼‰ä¸‹ï¼ŒåŸºäºè™šæ‹Ÿè‡ªåšå¼ˆçš„åŠ¨åŠ›å­¦èƒ½ä½¿ç»éªŒåˆ†å¸ƒ `(Î¼Ì„_T, Î½Ì„_T)` æ”¶æ•›åˆ°è¯±å¯¼é›¶å’Œåšå¼ˆçš„ä¸€ä¸ª `Ïµ`-çº³ä»€å‡è¡¡ï¼Œå…¶ä¸­ `Ïµ` éšè®­ç»ƒè½®æ¬¡ `T` å¢åŠ è€Œå‡å°ã€‚ | **æ¢ç´¢å‚æ•°**ï¼š`Î· = 0.1`",
            "timestamp": "2026-02-03T15:58:09.119218",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ æ•°æ®é›†ç»†èŠ‚\n\n| æ•°æ®é›†åç§° | æ¥æº/ç±»å‹ | è§„æ¨¡ä¸å†…å®¹ | å¤„ç†ä¸ç”¨é€” |\n| :--- | :--- | :--- | :--- |\n| **äººç±»æ‹³å‡»åŠ¨ä½œæ•æ‰æ•°æ®é›† (`ğ’Ÿ_motion`)** | ä¸“ä¸šæ‹³å‡»æ‰‹ä½¿ç”¨ **Xsens MVN** æƒ¯æ€§åŠ¨ä½œæ•æ‰ç³»ç»Ÿå½•åˆ¶ã€‚ | - **åŸå§‹æ•°æ®**: 46æ®µåŠ¨ä½œç‰‡æ®µï¼Œæ€»æ—¶é•¿çº¦14åˆ†é’Ÿï¼Œé‡‡æ ·é¢‘ç‡50Hzã€‚<br>- **å†…å®¹**: æ¶µç›–**è¿›æ”»**ï¼ˆå‡ºæ‹³ï¼‰ã€**é˜²å®ˆ**ï¼ˆæ ¼æŒ¡ã€é—ªé¿ï¼‰ã€**æ­¥æ³•**ï¼ˆç§»åŠ¨ï¼‰å’Œ**è¿‡æ¸¡åŠ¨ä½œ**ã€‚<br>- **æ•°æ®å¢å¼º**: é€šè¿‡å·¦å³é•œåƒï¼Œæ•°æ®é‡ç¿»å€ã€‚ | 1. **è¿åŠ¨é‡å®šå‘**: ä½¿ç”¨å¹¿ä¹‰è¿åŠ¨é‡å®šå‘æ¡†æ¶ï¼Œå°†äººä½“åŠ¨ä½œæ˜ å°„åˆ° **Unitree G1** äººå½¢æœºå™¨äººï¼ˆ29è‡ªç”±åº¦ï¼‰çš„å½¢æ€ä¸Šï¼Œç”Ÿæˆç‰©ç†ä¸Šå¯è¡Œçš„å‚è€ƒè½¨è¿¹ã€‚<br>2. **ç”¨é€”**: ç”¨äº**ç¬¬ä¸€é˜¶æ®µ**è®­ç»ƒä½å±‚è¿åŠ¨è·Ÿè¸ªå™¨ (`Ï€_trk`)ï¼Œä¸ºæœºå™¨äººæä¾›æ¨¡ä»¿å­¦ä¹ çš„ä¸“å®¶æ•°æ®ã€‚ |\n\n### äºŒã€ è¯„ä»·æŒ‡æ ‡ç»†èŠ‚\n\nè®ºæ–‡ä»**æˆ˜æœ¯ç†Ÿç»ƒåº¦**ã€**ç‰©ç†ç¨³å®šæ€§**å’Œ**é£æ ¼çœŸå®æ€§**ä¸‰ä¸ªç»´åº¦è®¾è®¡äº†é‡åŒ–è¯„ä¼°æŒ‡æ ‡ã€‚\n\n#### 1. æˆ˜æœ¯ç†Ÿç»ƒåº¦æŒ‡æ ‡\n\n| æŒ‡æ ‡åç§° | è®¡ç®—å…¬å¼/å®šä¹‰ | ç‰©ç†æ„ä¹‰ä¸è¯„ä¼°ç›®æ ‡ |\n| :--- | :--- | :--- |\n| **èƒœç‡** | `WR = (è·èƒœåœºæ¬¡ / æ€»è¯„ä¼°åœºæ¬¡) Ã— 100%` | **æ ¸å¿ƒç«äº‰æŒ‡æ ‡**ã€‚ä¸€åœºæ¯”èµ›å¤±è´¥åˆ¤å®šä¸ºï¼šæœºå™¨äººé™¤è„šéƒ¨å¤–çš„ä»»ä½•èº«ä½“éƒ¨ä½æ¥è§¦åœ°é¢ï¼ˆå³æ‘”å€’ï¼‰ã€‚è¯„ä¼°ç­–ç•¥çš„æ•´ä½“æœ‰æ•ˆæ€§ã€‚ |\n| **è¿›æ”»å‘½ä¸­ç‡** | `Î·_hit = (æœ‰æ•ˆå‘½ä¸­æ¬¡æ•°) / (æ€»è¿›æ”»å°è¯•æ¬¡æ•°)` | **è¯„ä¼°è¿›æ”»ç²¾åº¦**ã€‚<br>- **æœ‰æ•ˆå‘½ä¸­**: æ¥è§¦åŠ›è¶…è¿‡é˜ˆå€¼ (`F_th = 10N`) ä¸”è¢«åˆ¤å®šä¸ºæœ‰æ„çš„å‡ºæ‹³åŠ¨ä½œï¼ˆç»“åˆé€Ÿåº¦å’Œæ–¹å‘åˆ¤æ–­ï¼‰ã€‚<br>- **ç›®æ ‡**: å€¼è¶Šé«˜ï¼Œè¡¨æ˜ç­–ç•¥çš„æ‰“å‡»è¶Šç²¾å‡†ã€æœ‰æ•ˆã€‚ |\n| **äº¤æˆ˜ç‡** | `ER = (æ»¡è¶³äº¤æˆ˜æ¡ä»¶çš„æ—¶é—´) / (æ€»å›åˆæ—¶é—´)` | **è¯„ä¼°æˆ˜æœ¯å‚ä¸åº¦å’Œç«™ä½**ã€‚åœ¨æ—¶é—´æ­¥`t`ï¼Œéœ€**åŒæ—¶æ»¡è¶³**ä¸¤ä¸ªç©ºé—´æ¡ä»¶ï¼š<br>1. **è·ç¦»æ¡ä»¶**: ä¸å¯¹æ‰‹è·ç¦» `d_t âˆˆ [0.5m, 1.2m]`ï¼ˆæœ‰æ•ˆæ‰“å‡»èŒƒå›´ï¼‰ã€‚<br>2. **æœå‘æ¡ä»¶**: èº¯å¹²é¢å‘å¯¹æ‰‹çš„ä½™å¼¦å€¼ `cos(Î¸_t) > Ï„_face = 0.9`ã€‚<br>- **ç›®æ ‡**: å€¼è¶Šé«˜ï¼Œè¡¨æ˜æœºå™¨äººèƒ½æ›´æŒç»­åœ°ä¿æŒåœ¨æœ‰åˆ©çš„æ”»å‡»ä½ç½®ï¼Œé¿å…æ¶ˆæé¿æˆ˜ã€‚ |\n\n#### 2. ç‰©ç†ç¨³å®šæ€§æŒ‡æ ‡\n\n| æŒ‡æ ‡åç§° | è®¡ç®—å…¬å¼/å®šä¹‰ | ç‰©ç†æ„ä¹‰ä¸è¯„ä¼°ç›®æ ‡ |\n| :--- | :--- | :--- |\n| **åŸºåº§æœå‘ç¨³å®šæ€§** | `BOS = ğ”¼[exp(-â€–ğ _base - ğ _worldâ€–Â²)]` | **è¯„ä¼°å¹³è¡¡ä¿æŒèƒ½åŠ›**ã€‚<br>- `ğ _base`: æœºå™¨äººæœ¬ä½“åæ ‡ç³»ä¸­çš„é‡åŠ›å‘é‡ã€‚<br>- `ğ _world`: ä¸–ç•Œåæ ‡ç³»ä¸­çš„é‡åŠ›å‘é‡ã€‚<br>- **è®¡ç®—**: ä¸¤è€…åå·®çš„èŒƒæ•°å¹³æ–¹åå–è´ŸæŒ‡æ•°ï¼ŒæœŸæœ›å€¼ã€‚åå·®è¶Šå°ï¼ŒæŒ‡æ•°é¡¹è¶Šæ¥è¿‘1ã€‚<br>- **ç›®æ ‡**: å€¼è¶Šé«˜ï¼ˆè¶Šæ¥è¿‘1ï¼‰ï¼Œè¡¨æ˜æœºå™¨äººåœ¨å¯¹æŠ—å¹²æ‰°ä¸‹ä¿æŒç›´ç«‹å§¿æ€çš„èƒ½åŠ›è¶Šå¼ºã€‚ |\n| **æ‰­çŸ©å¹³æ»‘åº¦** | `TS_Ï„ = ğ”¼[â€–Ï„_t - Ï„_(t-1)â€–]` | **è¯„ä¼°æ§åˆ¶æŒ‡ä»¤çš„å¹³æ»‘æ€§ä¸ç¡¬ä»¶å‹å¥½æ€§**ã€‚<br>- `Ï„_t`: åœ¨æ—¶é—´æ­¥`t`çš„å…³èŠ‚æ‰­çŸ©å‘é‡ã€‚<br>- **è®¡ç®—**: ç›¸é‚»æ—¶é—´æ­¥æ‰­çŸ©å˜åŒ–çš„ç»å¯¹å€¼å‡å€¼ã€‚<br>- **ç›®æ ‡**: å€¼**è¶Šä½**è¶Šå¥½ï¼Œè¡¨æ˜ç”µæœºæŒ‡ä»¤å˜åŒ–å¹³ç¼“ï¼Œèƒ½å‡å°‘æœºæ¢°ç£¨æŸï¼Œæ›´åˆ©äºå®é™…æœºå™¨äººéƒ¨ç½²ã€‚ |\n\n#### 3. é£æ ¼çœŸå®æ€§æŒ‡æ ‡\n- **è¯„ä¼°æ–¹æ³•**: **å®šæ€§è§†è§‰æ£€æŸ¥**ï¼Œè€Œéå•ä¸€æ•°å€¼åˆ†æ•°ã€‚\n- **è¯„ä¼°å†…å®¹**: é€šè¿‡å¯¹æ¯”ä»¿çœŸæˆªå›¾ï¼Œåˆ¤æ–­ emergent è¡Œä¸ºçš„â€œæ‹ŸäººåŒ–â€ç¨‹åº¦ã€‚\n    - **è§‚å¯Ÿæ˜¯å¦å‡ºç°**ï¼š æ ‡å‡†çš„æ‹³å‡»åŠ¨ä½œï¼ˆå¦‚æ»‘æ­¥ã€åå‡»ã€ç»„åˆæ‹³ã€æœ‰èŠ‚å¥çš„æ­¥æ³•ï¼‰ã€‚\n    - **ç¡®ä¿ä¸å‡ºç°**ï¼š ä¸è‡ªç„¶çš„å…³èŠ‚é…ç½®ã€ç‰©ç†ä¸Šä¸å¯æŒç»­çš„å§¿åŠ¿ï¼ˆè¿™æ˜¯æ— çº¦æŸå¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å¸¸è§é—®é¢˜ï¼‰ã€‚\n\n### æ€»ç»“è¡¨æ ¼\n\n| è¯„ä¼°ç»´åº¦ | å…·ä½“æŒ‡æ ‡ | è®¡ç®—æ–¹å¼/åˆ¤å®šæ ‡å‡† | ç›®æ ‡ |\n| :--- | :--- | :--- | :--- |\n| **æˆ˜æœ¯ç†Ÿç»ƒåº¦** | èƒœç‡ | æ‘”å€’åˆ¤å®šå¤±è´¥ï¼Œç»Ÿè®¡20å›åˆèƒœç‡ | è¡¡é‡ç­–ç•¥æ•´ä½“ç«äº‰åŠ› |\n| | è¿›æ”»å‘½ä¸­ç‡ | æœ‰æ•ˆå‘½ä¸­æ¬¡æ•° / æ€»è¿›æ”»å°è¯•æ¬¡æ•° | è¡¡é‡æ‰“å‡»ç²¾åº¦ä¸æ•ˆç‡ |\n| | äº¤æˆ˜ç‡ | åŒæ—¶å¤„äºæœ‰æ•ˆè·ç¦»å’Œé¢å‘å¯¹æ‰‹çš„æ—¶é—´å æ¯” | è¡¡é‡æˆ˜æœ¯ä¸»åŠ¨æ€§ä¸ç«™ä½ä¼˜åŠ£ |\n| **ç‰©ç†ç¨³å®šæ€§** | åŸºåº§æœå‘ç¨³å®šæ€§ | `ğ”¼[exp(-â€–æœ¬åœ°é‡åŠ›-ä¸–ç•Œé‡åŠ›â€–Â²)]` | è¡¡é‡åŠ¨æ€å¹³è¡¡ä¿æŒèƒ½åŠ› |\n| | æ‰­çŸ©å¹³æ»‘åº¦ | `ğ”¼[â€–æ‰­çŸ©å˜åŒ–é‡â€–]` | è¡¡é‡æ§åˆ¶æŒ‡ä»¤çš„å¹³æ»‘æ€§ã€ç¡¬ä»¶å‹å¥½æ€§ |\n| **é£æ ¼çœŸå®æ€§** | è§†è§‰çœŸå®æ€§ | ä¸“å®¶å®šæ€§è¯„ä¼°åŠ¨ä½œçš„æ‹ŸäººåŒ–å’Œåˆè§„æ€§ | ç¡®ä¿è¡Œä¸ºç¬¦åˆäººç±»è¿åŠ¨å…ˆéªŒï¼Œæ˜“äºæ¥å— |\n\n**åˆ›æ–°ç‚¹ä¸ä»·å€¼**:\nè¿™äº›æŒ‡æ ‡**ç³»ç»ŸåŒ–åœ°**å°†æŠ½è±¡çš„â€œæ‹³å‡»æ°´å¹³â€åˆ†è§£ä¸ºå¯é‡åŒ–çš„æŠ€æœ¯ç»´åº¦ï¼Œä¸ä»…æœåŠ¡äºè®ºæ–‡ä¸­çš„æ¨ªå‘å¯¹æ¯”ï¼ˆå¦‚**Table 1, 2, 3**ï¼‰ï¼Œæ›´é‡è¦çš„æ˜¯ä¸º**å…·èº«å¤šæ™ºèƒ½ä½“ç«äº‰ä»»åŠ¡**æä¾›äº†ä¸€ä¸ªé€šç”¨çš„è¯„ä¼°æ¡†æ¶ã€‚å®ƒåŒæ—¶å…³æ³¨äº†**ä»»åŠ¡æ€§èƒ½**ï¼ˆèƒœç‡ã€å‘½ä¸­ç‡ï¼‰ã€**ç³»ç»Ÿé²æ£’æ€§**ï¼ˆç¨³å®šæ€§æŒ‡æ ‡ï¼‰å’Œ**è¡Œä¸ºè´¨é‡**ï¼ˆé£æ ¼çœŸå®æ€§ï¼‰ï¼Œå¯¹æ¨åŠ¨æ­¤ç±»ç ”ç©¶ä»ä»¿çœŸèµ°å‘ç°å®ï¼ˆsim-to-realï¼‰å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚æ•°æ®é›†è™½ç„¶è§„æ¨¡ä¸å¤§ï¼Œä½†é€šè¿‡ä¸“ä¸šé‡‡é›†å’Œé‡å®šå‘å¤„ç†ï¼Œä¸ºå­¦ä¹ **ç‰©ç†å¯è¡Œçš„åŸºç¡€è¿åŠ¨æŠ€èƒ½**æä¾›äº†é«˜è´¨é‡çš„å…ˆéªŒï¼Œæ˜¯åç»­åˆ†å±‚å­¦ä¹ æˆåŠŸçš„å…³é”®åŸºçŸ³ã€‚",
            "timestamp": "2026-02-03T15:58:57.482715",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†\nè®ºæ–‡çš„å®éªŒä¸»è¦åŸºäºä»¥ä¸‹ä¸¤ç±»æ•°æ®ï¼š\n\n1.  **äººç±»åŠ¨ä½œæ•æ‰æ•°æ®é›†**\n    *   **æ¥æº**ï¼šä½¿ç”¨XsensåŠ¨ä½œæ•æ‰ç³»ç»Ÿï¼Œç”±ä¸“ä¸šæ‹³å‡»æ‰‹å½•åˆ¶ã€‚\n    *   **è§„æ¨¡**ï¼šåŒ…å«46ä¸ªåŸå§‹åŠ¨ä½œç‰‡æ®µï¼Œæ€»æ—¶é•¿çº¦14åˆ†é’Ÿï¼Œé‡‡æ ·é¢‘ç‡ä¸º50Hzã€‚\n    *   **å†…å®¹**ï¼šæ¶µç›–äº†å¹¿æ³›çš„æ‹³å‡»è¡Œä¸ºï¼ŒåŒ…æ‹¬è¿›æ”»æ€§å‡»æ‰“ã€é˜²å¾¡æ€§åŠ¨ä½œã€æ­¥æ³•å’Œè¿‡æ¸¡æ€§åŠ¨ä½œã€‚\n    *   **æ•°æ®å¢å¼º**ï¼šé€šè¿‡å·¦å³é•œåƒå¤„ç†ï¼Œä½¿æ•°æ®é‡ç¿»å€ã€‚\n    *   **å¤„ç†**ï¼šä½¿ç”¨å¹¿ä¹‰è¿åŠ¨é‡å®šå‘ï¼ˆGMRï¼‰æ¡†æ¶ï¼Œå°†æ•æ‰åˆ°çš„äººç±»åŠ¨ä½œé‡å®šå‘åˆ°Unitree G1äººå½¢æœºå™¨äººçš„å½¢æ€ä¸Šï¼Œç”Ÿæˆç‰©ç†ä¸Šå¯å®ç°çš„å‚è€ƒè½¨è¿¹æ•°æ®é›† `ğ’Ÿ_motion`ã€‚\n\n2.  **ä»¿çœŸç¯å¢ƒä¸æœºå™¨äººå¹³å°**\n    *   **ä»¿çœŸå™¨**ï¼šåœ¨NVIDIA Omniverseå¹³å°çš„Isaac Labä¸­è¿›è¡Œå¤§è§„æ¨¡å¹¶è¡Œä»¿çœŸè®­ç»ƒã€‚\n    *   **æœºå™¨äººæ¨¡å‹**ï¼šä½¿ç”¨Unitree G1äººå½¢æœºå™¨äººï¼ˆ29ä¸ªè‡ªç”±åº¦ï¼‰ä½œä¸ºä»¿çœŸå’Œå®ç‰©éªŒè¯å¹³å°ã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡ä»ä¸‰ä¸ªç»´åº¦è®¾è®¡äº†é‡åŒ–è¯„ä¼°æŒ‡æ ‡ï¼š\n\n| ç»´åº¦ | æŒ‡æ ‡ | å®šä¹‰ä¸ç›®çš„ |\n| :--- | :--- | :--- |\n| **æˆ˜æœ¯ç†Ÿç»ƒåº¦** | **è¿›æ”»å‘½ä¸­ç‡ (Î·_hit) â†‘** | æˆåŠŸå‡»æ‰“ï¼ˆæ¥è§¦åŠ›è¶…è¿‡é˜ˆå€¼ï¼‰æ¬¡æ•°å æ€»è¿›æ”»å°è¯•æ¬¡æ•°çš„æ¯”ä¾‹ã€‚è¡¡é‡è¿›æ”»ç²¾åº¦ã€‚ |\n| | **äº¤æˆ˜ç‡ (ER) â†‘** | æ™ºèƒ½ä½“åŒæ—¶æ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶çš„æ—¶é•¿å æ¯”ï¼š1) ä¸å¯¹æ‰‹è·ç¦»åœ¨æœ‰æ•ˆå‡»æ‰“èŒƒå›´å†…ï¼›2) é¢å‘å¯¹æ‰‹çš„è§’åº¦è¶…è¿‡é˜ˆå€¼ã€‚è¡¡é‡ä¸»åŠ¨äº¤æˆ˜å’Œæˆ˜æœ¯ç«™ä½èƒ½åŠ›ã€‚ |\n| **ç‰©ç†ç¨³å®šæ€§** | **åŸºç¡€å§¿æ€ç¨³å®šæ€§ (BOS) â†‘** | `BOS = ğ”¼[exp(-â€–ğ _base - ğ _worldâ€–Â²)]`ï¼Œå…¶ä¸­ğ ä¸ºé‡åŠ›å‘é‡ã€‚è¡¡é‡åœ¨å¯¹æŠ—æ‰°åŠ¨ä¸‹ä¿æŒç›´ç«‹å§¿æ€çš„èƒ½åŠ›ï¼Œå€¼è¶Šé«˜è¶Šç¨³å®šã€‚ |\n| | **æ‰­çŸ©å¹³æ»‘åº¦ (TS_Ï„) â†“** | `TS_Ï„ = ğ”¼[â€–Ï„_t - Ï„_(t-1)â€–]`ï¼Œå³å…³èŠ‚æ‰­çŸ©åœ¨è¿ç»­æ§åˆ¶æ­¥é—´çš„å¹³å‡ç»å¯¹å˜åŒ–ã€‚è¡¡é‡æ§åˆ¶æŒ‡ä»¤çš„å¹³æ»‘åº¦ï¼Œå€¼è¶Šä½å¯¹ç¡¬ä»¶è¶Šå‹å¥½ï¼Œæœºæ¢°ç£¨æŸè¶Šå°ã€‚ |\n| **é£æ ¼çœŸå®æ€§** | **å®šæ€§è§†è§‰è¯„ä¼°** | é€šè¿‡ä»¿çœŸæˆªå›¾å¯¹æ¯”ï¼Œè¯„ä¼°åŠ¨ä½œçš„â€œäººç±»ç›¸ä¼¼åº¦â€ï¼ŒåŒ…æ‹¬è§‚å¯Ÿæ˜¯å¦å‡ºç°æ»‘æ­¥ã€åå‡»ã€èŠ‚å¥æ€§æ­¥æ³•ç­‰å…¸å‹æ‹³å‡»åŠ¨ä½œï¼Œä»¥åŠæ˜¯å¦é¿å…ä¸è‡ªç„¶çš„å…³èŠ‚é…ç½®ã€‚ |\n\n**èƒœè´Ÿåˆ¤å®š**ï¼šåœ¨ä¸€ç³»åˆ—è¯„ä¼°å¯¹å±€ä¸­ï¼Œ**èƒœç‡ (Win Rate)** æ˜¯æ ¸å¿ƒæŒ‡æ ‡ã€‚å½“æ™ºèƒ½ä½“é™¤è„šéƒ¨å¤–çš„ä»»ä½•èº«ä½“éƒ¨ä½æ¥è§¦åœ°é¢ï¼ˆå³æ‘”å€’ï¼‰æ—¶ï¼Œå³åˆ¤è´Ÿã€‚\n\n### ä¸‰ã€æ€§èƒ½æå‡ä¸ä¸»è¦ç»“æœ\nè®ºæ–‡é€šè¿‡äº¤å‰å¯¹å±€ï¼ˆCross-Playingï¼‰å’Œæ¶ˆèå®éªŒè¯æ˜äº†æ‰€ææ–¹æ³•ï¼ˆLS-NFSPï¼‰çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚\n\n1.  **æ•´ä½“èƒœç‡å¯¹æ¯” (è¡¨1)**\n    *   **LS-NFSP vs. æ‰€æœ‰åŸºçº¿æ–¹æ³•**ï¼šå‡å–å¾—å‹å€’æ€§èƒœåˆ©ï¼ˆèƒœç‡68.52% ~ 100%ï¼‰ã€‚\n    *   **æœ€å…³é”®å¯¹æ¯”**ï¼šLS-NFSP **100%** æˆ˜èƒœäº†åœ¨åŸå§‹29ç»´å…³èŠ‚ç©ºé—´è¿›è¡Œè‡ªåšå¼ˆçš„åŸºçº¿ï¼ˆ29Dof Action-Space SPï¼‰ï¼Œè¯æ˜äº†**åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œç­–ç•¥è§£è€¦çš„æ ¹æœ¬å¿…è¦æ€§**ã€‚\n    *   **ç­–ç•¥ç®—æ³•å¯¹æ¯”**ï¼šåœ¨æ½œåœ¨ç©ºé—´å†…ï¼ŒLS-NFSPèƒœäºFictitious SP (68.52%) å’Œ Naive SP (76.24%)ï¼Œè¯æ˜äº†**NFSPæœºåˆ¶èƒ½æœ‰æ•ˆç¼“è§£ç­–ç•¥å¾ªç¯ï¼Œäº§ç”Ÿæ›´ç¨³å¥çš„æœ€ä½³å“åº”**ã€‚\n\n2.  **æˆ˜æœ¯ä¸ç‰©ç†æŒ‡æ ‡å¯¹æ¯” (è¡¨2 & è¡¨3)**\n    *   **æˆ˜æœ¯æŒ‡æ ‡**ï¼šLS-NFSPçš„è¿›æ”»å‘½ä¸­ç‡ (**0.685**) å’Œäº¤æˆ˜ç‡ (**0.824**) è¿œé«˜äºæ‰€æœ‰åŸºçº¿ã€‚\n    *   **ç‰©ç†æŒ‡æ ‡**ï¼šLS-NFSPçš„åŸºç¡€å§¿æ€ç¨³å®šæ€§ (**0.942**) æ˜¾è‘—æ›´é«˜ï¼Œæ‰­çŸ©å¹³æ»‘åº¦ (**0.930**) æ˜¾è‘—æ›´ä½ï¼Œè¯æ˜äº†å…¶**åœ¨æ¿€çƒˆå¯¹æŠ—ä¸­ä¿æŒç‰©ç†ç¨³å®šçš„å“è¶Šèƒ½åŠ›**ã€‚\n\n3.  **æ¶ˆèå®éªŒçš„å…³é”®å‘ç°**\n    *   **æ½œåœ¨ç©ºé—´çš„ä½œç”¨ (è¡¨3-b)**ï¼šä¸åœ¨åŸå§‹åŠ¨ä½œç©ºé—´è®­ç»ƒç›¸æ¯”ï¼Œæ½œåœ¨ç©ºé—´å°†å‘½ä¸­ç‡å’Œäº¤æˆ˜ç‡æå‡äº†çº¦ **4-5å€**ã€‚\n    *   **çƒ­èº«è®­ç»ƒçš„ä½œç”¨ (è¡¨3-c)**ï¼šæ²¡æœ‰çƒ­èº«è®­ç»ƒï¼ˆSP w/o Warmupï¼‰çš„ç‰ˆæœ¬å‡ ä¹æ— æ³•å­¦ä¹ ï¼Œå‘½ä¸­ç‡ä»…ä¸º **0.050**ï¼Œè¯æ˜äº†è¯¾ç¨‹å¼çƒ­èº«å¯¹å…‹æœç¨€ç–å¥–åŠ±ã€å¯åŠ¨ç«äº‰æ€§å­¦ä¹ çš„å¿…è¦æ€§ã€‚\n    *   **å¯¹æŠ—è¿åŠ¨å…ˆéªŒçš„ä½œç”¨ (è¡¨3-d)**ï¼šç§»é™¤AMPï¼ˆLS-NFSP w/o AMPï¼‰ä¼šå¯¼è‡´å‘½ä¸­ç‡ä»0.685ä¸‹é™è‡³ **0.490**ï¼Œè¯´æ˜**é£æ ¼çº¦æŸå¯¹äºç”Ÿæˆæ—¢æœ‰æ•ˆåˆç¬¦åˆç‰©ç†è§„å¾‹çš„åŠ¨ä½œè‡³å…³é‡è¦**ã€‚\n\n### æ€»ç»“\n**RoboStriker** æ¡†æ¶é€šè¿‡åœ¨**äººç±»æ‹³å‡»åŠ¨ä½œæ•æ‰æ•°æ®é›†**ä¸Šé¢„è®­ç»ƒï¼Œå¹¶åœ¨**å¤§è§„æ¨¡ä»¿çœŸç¯å¢ƒ**ä¸­åˆ©ç”¨**æ½œåœ¨ç©ºé—´ç¥ç»è™šæ‹Ÿè‡ªåšå¼ˆï¼ˆLS-NFSPï¼‰** è¿›è¡Œç«äº‰æ€§ååŒè¿›åŒ–ï¼Œå®ç°äº†äººå½¢æœºå™¨äººè‡ªä¸»æ‹³å‡»ã€‚å…¶ä¸»è¦æ€§èƒ½æå‡ä½“ç°åœ¨ï¼š\n*   **ç«äº‰æ€§**ï¼šåœ¨8x8äº¤å‰å¯¹å±€ä¸­èƒœç‡å…¨é¢é¢†å…ˆã€‚\n*   **æˆ˜æœ¯æ€§**ï¼šè¿›æ”»å‘½ä¸­ç‡å’Œä¸»åŠ¨äº¤æˆ˜ç‡æŒ‡æ ‡å¤§å¹…è¶…è¶ŠåŸºçº¿ã€‚\n*   **ç¨³å®šæ€§ä¸çœŸå®æ€§**ï¼šåœ¨ä¿æŒæé«˜ç‰©ç†ç¨³å®šæ€§å’ŒåŠ¨ä½œå¹³æ»‘åº¦çš„åŒæ—¶ï¼Œç”Ÿæˆäº†ç¬¦åˆäººç±»é£æ ¼çš„æ‹³å‡»æˆ˜æœ¯ï¼ˆå¦‚æ»‘æ­¥ã€åå‡»ï¼‰ã€‚\nè¯¥å·¥ä½œä¸ºå°†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä»æŠ½è±¡æ¸¸æˆæˆåŠŸæ‰©å±•åˆ°é«˜ç»´ã€æ¥è§¦ä¸°å¯Œçš„å®ä½“æœºå™¨äººç«äº‰ä»»åŠ¡æä¾›äº†ä¸€ä¸ªè¡Œä¹‹æœ‰æ•ˆçš„èŒƒä¾‹ã€‚",
            "timestamp": "2026-02-03T15:59:43.424171",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹ã€ŠRoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxingã€‹è®ºæ–‡çš„æ·±å…¥åˆ†æï¼Œå…¶æ ¸å¿ƒæ–¹æ³•è™½ç„¶å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n### 1. **ä»»åŠ¡ä¸åŠ¨ä½œç©ºé—´çš„ç‰¹å®šæ€§**\n   - **å±€é™æ€§**ï¼šæ•´ä¸ªæ¡†æ¶ï¼ˆåŒ…æ‹¬è¿åŠ¨åº“ã€æ½œåœ¨ç©ºé—´è’¸é¦ã€LS-NFSPï¼‰æ˜¯å›´ç»•**æ‹³å‡»**è¿™ä¸€ç‰¹å®šæ¥è§¦å¼å¯¹æŠ—ä»»åŠ¡è®¾è®¡çš„ã€‚æ‰€å­¦ä¹ çš„æ½œåœ¨åŠ¨ä½œç©ºé—´ç¼–ç äº†æ‹³å‡»ç›¸å…³çš„è¿åŠ¨åŸºå…ƒï¼ˆå¦‚ç§»åŠ¨ã€å‡ºæ‹³ï¼‰ï¼Œå…¶æ³›åŒ–èƒ½åŠ›å°šæœªåœ¨å…¶ä»–ç±»å‹çš„åŠ¨æ€äº¤äº’ä»»åŠ¡ï¼ˆå¦‚æ‘”è·¤ã€è¶³çƒã€åä½œæ¬è¿ï¼‰ä¸­å¾—åˆ°éªŒè¯ã€‚\n   - **å½±å“**ï¼šæ–¹æ³•çš„é€šç”¨æ€§å­˜ç–‘ã€‚å°†å…¶è¿ç§»è‡³éœ€è¦ä¸åŒæŠ€èƒ½é›†ï¼ˆå¦‚æŠ•æ·ã€æŠ“å–ã€å¤æ‚åœ°å½¢å¯¼èˆªï¼‰çš„ä»»åŠ¡æ—¶ï¼Œå¯èƒ½éœ€è¦é‡æ–°æ”¶é›†æ•°æ®ã€è®¾è®¡æ½œåœ¨ç©ºé—´å¹¶è®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ã€‚\n\n### 2. **å¯¹é«˜è´¨é‡æ¼”ç¤ºæ•°æ®çš„ä¾èµ–**\n   - **å±€é™æ€§**ï¼šç¬¬ä¸€é˜¶æ®µï¼ˆè¿åŠ¨è·Ÿè¸ªå™¨ï¼‰å’Œé£æ ¼çº¦æŸï¼ˆAMPï¼‰ä¸¥é‡ä¾èµ–äº**ä¸“ä¸šæ‹³å‡»æ‰‹çš„é«˜è´¨é‡åŠ¨ä½œæ•æ‰æ•°æ®**ï¼ˆ46æ®µå‰ªè¾‘ï¼Œå…±14åˆ†é’Ÿï¼‰ã€‚è¿™æ„æˆäº†ä¸€ä¸ªæ€§èƒ½ç“¶é¢ˆå’Œå¯æ‰©å±•æ€§éšœç¢ã€‚\n   - **å½±å“**ï¼š\n     - æ•°æ®æ”¶é›†æˆæœ¬é«˜ï¼Œéœ€è¦ä¸“ä¸šè®¾å¤‡å’Œäººå‘˜ã€‚\n     - æ•°æ®çš„å¤šæ ·æ€§å’Œè´¨é‡ç›´æ¥å†³å®šäº†ç­–ç•¥çš„ä¸Šé™ã€‚å¦‚æœæ•°æ®ä¸­ç¼ºä¹æŸäº›é«˜çº§æˆ˜æœ¯ï¼ˆå¦‚å¤æ‚çš„ç»„åˆæ‹³ã€å‡åŠ¨ä½œï¼‰ï¼Œæ™ºèƒ½ä½“å¯èƒ½éš¾ä»¥é€šè¿‡è‡ªåšå¼ˆè‡ªå‘æ¼”åŒ–å‡ºè¿™äº›ç­–ç•¥ã€‚\n     - é™åˆ¶äº†æœºå™¨äººå½¢æ€çš„æ³›åŒ–ã€‚è®ºæ–‡ä¸­ä½¿ç”¨Unitree G1æœºå™¨äººï¼Œå…¶å½¢æ€ä¸äººç±»ç›¸ä¼¼ã€‚è‹¥æœºå™¨äººå½¢æ€å·®å¼‚å·¨å¤§ï¼ˆå¦‚ä¸åŒå…³èŠ‚æ•°ã€è¿æ†æ¯”ä¾‹ï¼‰ï¼ŒåŠ¨ä½œé‡å®šå‘å’Œè·Ÿè¸ªå°†å˜å¾—å¼‚å¸¸å›°éš¾ã€‚\n\n### 3. **æ½œåœ¨ç©ºé—´çš„è¡¨è¾¾èƒ½åŠ›ä¸ç“¶é¢ˆ**\n   - **å±€é™æ€§**ï¼šæ½œåœ¨ç©ºé—´ï¼ˆ32ç»´å•ä½è¶…çƒé¢ï¼‰æ˜¯**é™æ€çš„**ï¼Œåœ¨ç¬¬äºŒé˜¶æ®µè’¸é¦å®Œæˆåå³è¢«å†»ç»“ã€‚é«˜å±‚ç­–ç•¥ï¼ˆLS-NFSPï¼‰åªèƒ½åœ¨è¿™ä¸ªå›ºå®šçš„ç©ºé—´å†…è¿›è¡Œç»„åˆå’Œæ¢ç´¢ã€‚\n   - **å½±å“**ï¼š\n     - **æ— æ³•åˆ›é€ å…¨æ–°åŠ¨ä½œ**ï¼šç­–ç•¥æ¼”åŒ–è¢«é™åˆ¶åœ¨é¢„å®šä¹‰çš„è¿åŠ¨åŸºå…ƒç»„åˆä¹‹å†…ï¼Œæ— æ³•ç”Ÿæˆè®­ç»ƒæ•°æ®ä¸­å®Œå…¨ä¸å­˜åœ¨çš„ã€å…¨æ–°çš„ã€å¯èƒ½æ›´ä¼˜çš„è¿åŠ¨æ¨¡å¼ï¼ˆä¾‹å¦‚ï¼Œä¸€ç§ä¸ºæœºå™¨äººèº«ä½“ç»“æ„ä¼˜åŒ–çš„ç‰¹æ®Šé—ªé¿æŠ€å·§ï¼‰ã€‚\n     - **ç»„åˆçˆ†ç‚¸çš„éšå¿§**ï¼šè™½ç„¶t-SNEå¯è§†åŒ–æ˜¾ç¤ºäº†è‰¯å¥½çš„èšç±»ï¼Œä½†éšç€ä»»åŠ¡å¤æ‚åº¦æå‡ï¼Œæœ‰é™çš„æ½œåœ¨ç»´åº¦å¯èƒ½æˆä¸ºè¡¨è¾¾å¤æ‚ã€é•¿æ—¶åºæˆ˜æœ¯ç»„åˆçš„ç“¶é¢ˆã€‚\n\n### 4. **æ¨¡æ‹Ÿåˆ°ç°å®çš„å·®è·è™½è¢«æåŠä½†æœªæ·±å…¥è§£å†³**\n   - **å±€é™æ€§**ï¼šè®ºæ–‡æåˆ°äº†â€œsim-to-real transferâ€å¹¶å±•ç¤ºäº†çœŸå®ä¸–ç•Œç‰‡æ®µï¼ˆå›¾1ï¼‰ï¼Œä½†é™„å½•å’Œå®éªŒéƒ¨åˆ†**æœªè¯¦ç»†é˜è¿°**è½¬ç§»çš„å…·ä½“è¿‡ç¨‹ã€é‡åˆ°äº†å“ªäº›æŒ‘æˆ˜ã€ä»¥åŠçœŸå®ä¸–ç•Œæ€§èƒ½çš„å®šé‡è¯„ä¼°ï¼ˆå¦‚ä¸ä»¿çœŸä¸­æˆ˜æœ¯æŒ‡æ ‡çš„å¯¹æ¯”ï¼‰ã€‚\n   - **å½±å“**ï¼šè¿™ä½¿å¾—è¯„ä¼°å…¶åœ¨å®é™…æœºå™¨äººä¸Šçš„é²æ£’æ€§å’Œå®ç”¨æ€§å˜å¾—å›°éš¾ã€‚ä»¿çœŸä¸­çš„æ¥è§¦æ¨¡å‹ã€ä¼ æ„Ÿå™¨å™ªå£°ã€æ‰§è¡Œå™¨å»¶è¿Ÿä¸çœŸå®ä¸–ç•Œå­˜åœ¨å·®å¼‚ï¼Œè¿™äº›éƒ½å¯èƒ½ç ´ååœ¨æ½œåœ¨ç©ºé—´ä¸­ä¹ å¾—çš„ç²¾ç»†æˆ˜æœ¯ã€‚\n\n### 5. **å¤šæ™ºèƒ½ä½“åšå¼ˆçš„å¤æ‚æ€§ä¸å‡è¡¡è´¨é‡**\n   - **å±€é™æ€§**ï¼šLS-NFSPæ—¨åœ¨å¯»æ‰¾è¿‘ä¼¼çº³ä»€å‡è¡¡ï¼Œä½†ç†è®ºåˆ†æï¼ˆå‘½é¢˜3.5ï¼‰åŸºäºä¸€ç³»åˆ—ç†æƒ³åŒ–å‡è®¾ï¼ˆå¦‚ç´§å‡‘ç­–ç•¥é›†ã€æœ‰ç•Œæ”¶ç›Šã€è¿‘ä¼¼æœ€ä½³å“åº”ï¼‰ã€‚å®é™…è®­ç»ƒä¸­ï¼ŒPPOä¼˜åŒ–å™¨çš„è¿‘ä¼¼è¯¯å·®ã€å‡½æ•°é€¼è¿‘å™¨çš„èƒ½åŠ›ã€ä»¥åŠæœ‰é™çš„è®­ç»ƒæ—¶é•¿ï¼Œéƒ½æ„å‘³ç€æ‰¾åˆ°çš„å¯èƒ½æ˜¯**å±€éƒ¨å‡è¡¡**ï¼Œè€Œéå…¨å±€æœ€ä¼˜ã€‚\n   - **å½±å“**ï¼šç­–ç•¥å¯èƒ½é™·å…¥â€œ**å±€éƒ¨æˆ˜æœ¯å¾ªç¯**â€ï¼Œå³åŒæ–¹æ¼”åŒ–å‡ºä¸€å¥—ç›¸äº’å…‹åˆ¶ä½†å¹¶éæœ€ä¼˜çš„å›ºå®šæ‰“æ³•ï¼Œè€Œæ— æ³•å‘ç°æ›´é«˜çº§çš„å…¨å±€ç­–ç•¥ã€‚äº¤å‰å¯¹å¼ˆè¡¨æ ¼ï¼ˆè¡¨1ï¼‰æ˜¾ç¤ºäº†ä¼˜åŠ¿ï¼Œä½†æœªè¯æ˜å·²æ”¶æ•›è‡³æ— æ³•è¢«ä»»ä½•æ–°ç­–ç•¥å‡»è´¥çš„å‡è¡¡ç‚¹ã€‚\n\n### 6. **è®¡ç®—ä¸è®­ç»ƒæˆæœ¬**\n   - **å±€é™æ€§**ï¼šä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ˆè¿åŠ¨è·Ÿè¸ªã€æ½œåœ¨è’¸é¦ã€è‡ªåšå¼ˆï¼‰éœ€è¦**å¤§é‡çš„æ¨¡æ‹Ÿèµ„æº**ï¼ˆ4096ä¸ªå¹¶è¡Œç¯å¢ƒï¼‰å’Œè®­ç»ƒæ—¶é—´ã€‚æ¯ä¸ªé˜¶æ®µéƒ½å¯èƒ½éœ€è¦æ•°å¤©ç”šè‡³æ›´é•¿çš„GPUè®­ç»ƒã€‚\n   - **å½±å“**ï¼šè¿™é™åˆ¶äº†ç ”ç©¶çš„å¯é‡å¤æ€§å’Œå¿«é€Ÿè¿­ä»£èƒ½åŠ›ï¼Œä¹Ÿé˜»ç¢äº†åœ¨èµ„æºæœ‰é™åœºæ™¯ä¸‹çš„åº”ç”¨ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œæœªæ¥çš„ç ”ç©¶å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹å‘å±•å¼€ï¼š\n\n### 1. **æå‡æ¡†æ¶çš„é€šç”¨æ€§ä¸å¯æ‰©å±•æ€§**\n   - **æ–¹å‘**ï¼šå¼€å‘**ä»»åŠ¡æ— å…³çš„è¿åŠ¨åŸºå…ƒå­¦ä¹ **ã€‚æ„å»ºä¸€ä¸ªæ›´é€šç”¨çš„â€œäººä½“è¿åŠ¨åŸºç¡€æ¨¡å‹â€ï¼Œèƒ½å¤Ÿç¼–ç è¡Œèµ°ã€è·‘è·³ã€æ¨ã€æ‹‰ã€å‡»æ‰“ç­‰å¤šç§æŠ€èƒ½ï¼Œè€Œä¸ä»…é™äºæ‹³å‡»ã€‚é«˜å±‚ç­–ç•¥åˆ™æ ¹æ®å…·ä½“ä»»åŠ¡ï¼ˆå¯ä»¥æ˜¯è¶³çƒã€æ•‘æ´ç­‰ï¼‰åœ¨è¿™äº›é€šç”¨æŠ€èƒ½ä¸Šè¿›è¡Œç»„åˆä¸ä¼˜åŒ–ã€‚\n   - **æŠ€æœ¯è·¯å¾„**ï¼šä½¿ç”¨æ›´å¤§è§„æ¨¡ã€æ›´å¤šæ ·åŒ–çš„åŠ¨æ•æ•°æ®é›†ï¼Œç»“åˆ**æ‰©æ•£æ¨¡å‹**æˆ–**å¤§å‹è¯­è¨€æ¨¡å‹**è¿›è¡Œè¿åŠ¨ç”Ÿæˆå’Œè¯­ä¹‰æ§åˆ¶ï¼Œä½¿æ½œåœ¨ç©ºé—´å…·å¤‡æ›´å¼ºçš„å¯è§£é‡Šæ€§å’Œç»„åˆæ€§ã€‚\n\n### 2. **å‡å°‘å¯¹æ¼”ç¤ºæ•°æ®çš„ä¾èµ–**\n   - **æ–¹å‘**ï¼šæ¢ç´¢**æ— ç›‘ç£æˆ–å¼±ç›‘ç£çš„æŠ€èƒ½å‘ç°**ã€‚åœ¨ä»¿çœŸä¸­ï¼Œè®©æœºå™¨äººé€šè¿‡å†…åœ¨åŠ¨æœºï¼ˆå¦‚å¥½å¥‡å¿ƒã€ empowermentï¼‰è‡ªä¸»æ¢ç´¢ï¼Œå‘ç°æœ‰ç”¨çš„è¿åŠ¨æŠ€èƒ½å¹¶è‡ªåŠ¨å½¢æˆç»“æ„åŒ–æ½œåœ¨ç©ºé—´ï¼Œè€Œéå®Œå…¨ä¾èµ–äºäººç±»æ¼”ç¤ºã€‚\n   - **æŠ€æœ¯è·¯å¾„**ï¼šç»“åˆ**ä¸–ç•Œæ¨¡å‹**å’Œ**åˆ†å±‚å¼ºåŒ–å­¦ä¹ **ï¼Œåº•å±‚å­¦ä¹ å¯é‡å¤ä½¿ç”¨çš„æŠ€èƒ½ï¼Œé«˜å±‚å­¦ä¹ æŠ€èƒ½çš„ä½¿ç”¨ç­–ç•¥ã€‚å¯ä»¥ä»…ç”¨å°‘é‡æ¼”ç¤ºæ•°æ®å¯¹æŠ€èƒ½è¿›è¡Œâ€œé£æ ¼åŒ–â€å¾®è°ƒï¼Œè€Œéä»å¤´å­¦ä¹ æ‰€æœ‰æŠ€èƒ½ã€‚\n\n### 3. **å¢å¼ºæ½œåœ¨ç©ºé—´çš„åŠ¨æ€æ€§ä¸åˆ›é€ æ€§**\n   - **æ–¹å‘**ï¼šå¼•å…¥**å¯å­¦ä¹ çš„æ½œåœ¨ç©ºé—´è¿›åŒ–æœºåˆ¶**ã€‚å…è®¸åœ¨è‡ªåšå¼ˆè¿‡ç¨‹ä¸­ï¼Œä¸ä»…ç­–ç•¥åœ¨æ½œåœ¨ç©ºé—´å†…æ¼”åŒ–ï¼Œæ½œåœ¨ç©ºé—´æœ¬èº«çš„è¡¨ç¤ºï¼ˆå¦‚é€šè¿‡è¶…ç½‘ç»œï¼‰ä¹Ÿèƒ½æ ¹æ®æˆ˜æœ¯éœ€æ±‚è¿›è¡Œå¾®è°ƒæˆ–æ‰©å±•ï¼Œä»è€Œçªç ´é¢„å®šä¹‰æŠ€èƒ½é›†çš„é™åˆ¶ã€‚\n   - **æŠ€æœ¯è·¯å¾„**ï¼šé‡‡ç”¨**æ¡ä»¶ç”Ÿæˆæ¨¡å‹**ä½œä¸ºè§£ç å™¨ï¼Œå…¶æ½œåœ¨æ¡ä»¶å¯ä»¥æ˜¯ä¸€ä¸ªæ›´çµæ´»ã€å¯åˆ†å±‚çš„è¡¨ç¤ºã€‚æˆ–è€…ï¼Œå°†æŠ€èƒ½è§†ä¸ºå¯ç»„åˆçš„æ¨¡å—ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œåŠ¨æ€ç”ŸæˆæŠ€èƒ½å‚æ•°ã€‚\n\n### 4. **æ·±åŒ–ä»¿çœŸåˆ°ç°å®çš„ç ”ç©¶**\n   - **æ–¹å‘**ï¼šç³»ç»Ÿæ€§åœ°ç ”ç©¶å’Œé‡åŒ–**sim-to-realçš„è½¬ç§»æ€§èƒ½**ã€‚è®¾è®¡æ›´å…¨é¢çš„çœŸå®ä¸–ç•Œè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶å¼€å‘é’ˆå¯¹æ¥è§¦å¼å¯¹æŠ—ä»»åŠ¡çš„ä¸“ç”¨åŸŸéšæœºåŒ–ä¸åŸŸé€‚åº”æŠ€æœ¯ã€‚\n   - **æŠ€æœ¯è·¯å¾„**ï¼š\n     - åœ¨ä»¿çœŸä¸­å¼•å…¥æ›´çœŸå®çš„æ‰§è¡Œå™¨æ¨¡å‹ã€æ¥è§¦å™ªå£°å’ŒçŠ¶æ€ä¼°è®¡è¯¯å·®ã€‚\n     - ä½¿ç”¨**ç³»ç»Ÿè¾¨è¯†**å’Œ**åœ¨çº¿è‡ªé€‚åº”**æŠ€æœ¯ï¼Œè®©æœºå™¨äººåœ¨çœŸå®ç¯å¢ƒä¸­å¿«é€Ÿæ ¡å‡†å…¶æ¨¡å‹ã€‚\n     - é‡‡ç”¨**å…ƒå­¦ä¹ **æˆ–**ç¦»çº¿å¼ºåŒ–å­¦ä¹ **ï¼Œåˆ©ç”¨ä»¿çœŸä¸­æ”¶é›†çš„å¤§é‡æ•°æ®é¢„è®­ç»ƒä¸€ä¸ªé²æ£’çš„ç­–ç•¥ï¼Œä½¿å…¶èƒ½é€‚åº”çœŸå®ç¯å¢ƒçš„åˆ†å¸ƒåç§»ã€‚\n\n### 5. **æ”¹è¿›åšå¼ˆç†è®ºä¸å­¦ä¹ ç®—æ³•çš„ç»“åˆ**\n   - **æ–¹å‘**ï¼šæ¢ç´¢æ›´é«˜æ•ˆã€æ›´ç¨³å®šçš„**å¤§è§„æ¨¡è¿ç»­ç­–ç•¥ç©ºé—´åšå¼ˆæ±‚è§£ç®—æ³•**ã€‚ç ”ç©¶å¦‚ä½•é¿å…ç­–ç•¥å¾ªç¯ï¼ŒåŠ é€Ÿæ”¶æ•›åˆ°é«˜è´¨é‡å‡è¡¡ã€‚\n   - **æŠ€æœ¯è·¯å¾„**ï¼š\n     - ç»“åˆ**ç§ç¾¤åŸºè®­ç»ƒ**ä¸**åšå¼ˆè®ºæ­£åˆ™åŒ–**ï¼Œç»´æŒç­–ç•¥çš„å¤šæ ·æ€§ã€‚\n     - åˆ©ç”¨**å¯¹æ‰‹å»ºæ¨¡**æ¥é¢„æµ‹å¯¹æ–¹è¡ŒåŠ¨ï¼Œä»è€Œåˆ¶å®šæ›´ä¼˜çš„ååˆ¶ç­–ç•¥ã€‚\n     - ç ”ç©¶åœ¨éå¹³ç¨³ç¯å¢ƒä¸­å…·æœ‰**ç†è®ºæ”¶æ•›ä¿è¯**çš„æ–°ç®—æ³•ã€‚\n\n### 6. **æ¢ç´¢æ–°å…´æŠ€æœ¯èåˆ**\n   - **æ–¹å‘**ï¼šå°†**å¤§æ¨¡å‹**çš„è§„åˆ’ä¸æ¨ç†èƒ½åŠ›å¼•å…¥åˆ†å±‚å†³ç­–æ¡†æ¶ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨LLM/VLMè¿›è¡Œé«˜å±‚æˆ˜æœ¯è§„åˆ’ï¼ˆâ€œç°åœ¨åº”è¯¥ä½¯æ”»å·¦è·¯ç„¶åå³å‹¾æ‹³â€ï¼‰ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæ½œåœ¨ç©ºé—´ä¸­çš„ç›®æ ‡æˆ–çº¦æŸï¼Œå†ç”±åº•å±‚æ§åˆ¶å™¨æ‰§è¡Œã€‚\n   - **æŠ€æœ¯è·¯å¾„**ï¼šæ„å»ºä¸€ä¸ªâ€œ**å…·èº«æ™ºèƒ½ä½“**â€æ¡†æ¶ï¼Œå…¶ä¸­å¤§æ¨¡å‹æ‹…ä»»â€œæ•™ç»ƒâ€æˆ–â€œæˆ˜æœ¯æŒ‡æŒ¥å®˜â€ï¼Œè€Œå¼ºåŒ–å­¦ä¹ æ¨¡å—æ‹…ä»»â€œè¿åŠ¨å‘˜â€ï¼Œä¸¤è€…é€šè¿‡è‡ªç„¶è¯­è¨€æˆ–æ½œåœ¨ç©ºé—´è¿›è¡Œäº¤äº’ã€‚\n\n### 7. **æ‹“å±•åº”ç”¨åœºæ™¯ä¸è¯„ä¼°åŸºå‡†**\n   - **æ–¹å‘**ï¼šå°†RoboStrikeræ¡†æ¶åº”ç”¨äº**æ›´å¹¿æ³›çš„ç‰©ç†äººæœºäº¤äº’åœºæ™¯**ï¼Œå¹¶å»ºç«‹æ ‡å‡†åŒ–çš„è¯„ä¼°åŸºå‡†ã€‚\n   - **æŠ€æœ¯è·¯å¾„**ï¼š\n     - å®šä¹‰ä¸€ç³»åˆ—å…·æœ‰ä¸åŒç‰©ç†å¤æ‚åº¦ï¼ˆä»æ¨ç®±å­åˆ°æ ¼æ–—ï¼‰å’Œåšå¼ˆå¤æ‚åº¦ï¼ˆä»å®Œå…¨ä¿¡æ¯åˆ°ä¸å®Œå…¨ä¿¡æ¯ï¼‰çš„â€œ**æœºå™¨äººç«æŠ€åŸºå‡†**â€ã€‚\n     - å¼€æºä»¿çœŸç¯å¢ƒã€æœºå™¨äººæ¨¡å‹å’ŒåŸºçº¿ç®—æ³•ï¼Œä¿ƒè¿›ç¤¾åŒºå…±åŒæ¨è¿›è¯¥é¢†åŸŸå‘å±•ã€‚\n\n**æ€»ç»“**ï¼šRoboStrikerä¸ºè§£å†³å…·èº«å¤šæ™ºèƒ½ä½“ç«äº‰è¿™ä¸€æ ¸å¿ƒéš¾é¢˜æä¾›äº†ä¸€ä¸ªä¼˜é›…è€Œæœ‰æ•ˆçš„åˆ†å±‚è§£è€¦æ–¹æ¡ˆï¼Œå…¶ä»·å€¼åœ¨äº**æ–¹æ³•è®ºä¸Šçš„çªç ´**ã€‚æœªæ¥çš„å·¥ä½œåº”è‡´åŠ›äºå°†å…¶ä»ä¸€ä¸ªé’ˆå¯¹â€œæ‹³å‡»â€çš„**ç‰¹åŒ–è§£å†³æ–¹æ¡ˆ**ï¼Œæ¼”å˜ä¸ºä¸€ä¸ªé€‚ç”¨äºâ€œé€šç”¨åŠ¨æ€ç‰©ç†äº¤äº’â€çš„**åŸºç¡€æ¡†æ¶**ï¼ŒåŒæ—¶å…‹æœå…¶å¯¹é«˜è´¨é‡æ•°æ®çš„ä¾èµ–å’Œè®¡ç®—æˆæœ¬é«˜çš„æŒ‘æˆ˜ï¼Œæœ€ç»ˆå®ç°çœŸæ­£é€šç”¨ã€é«˜æ•ˆã€é²æ£’çš„å…·èº«ç«äº‰æ™ºèƒ½ã€‚",
            "timestamp": "2026-02-03T16:00:53.919189",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T03:51:58Z",
    "created_at": "2026-02-03T15:53:05.034645",
    "updated_at": "2026-02-03T15:53:05.034655"
}