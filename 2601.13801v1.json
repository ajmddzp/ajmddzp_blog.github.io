{
    "id": "2601.13801v1",
    "title": "HoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction",
    "authors": [
        "Yuhua Jin",
        "Nikita Kuzmin",
        "Georgii Demianchuk",
        "Mariya Lezina",
        "Fawad Mehboob",
        "Issatay Tokmurziyev",
        "Miguel Altamirano Cabrera",
        "Muhammad Ahsan Mustafa",
        "Dzmitry Tsetserukou"
    ],
    "abstract": "在人类活动空间运行的无人机，由于通信机制不足，常使其意图表达存在不确定性。我们提出HoverAI——一种具身化空中智能体，它将无人机机动性、独立于基础设施的可视化投影和实时对话式人工智能整合为统一平台。该系统配备MEMS激光投影仪、机载半刚性屏幕和RGB摄像头，通过视觉与语音感知用户，并借助口型同步的虚拟形象进行响应，其外观可根据用户人口统计特征自适应调整。系统采用多模态处理流程，结合语音活动检测、语音识别（Whisper）、基于大语言模型的意图分类、检索增强生成对话技术、个性化面部分析及语音合成（XTTS v2）。评估显示，该系统在指令识别（F1值：0.90）、人口特征估计（性别F1值：0.89，年龄平均绝对误差：5.14岁）和语音转写（词错误率：0.181）方面均表现出高精度。通过将空中机器人技术与自适应对话AI及自持式视觉输出相结合，HoverAI开创了一类具有空间感知与社会响应能力的具身智能体，为导引、辅助及人本交互等应用场景提供了新范式。",
    "url": "https://arxiv.org/abs/2601.13801v1",
    "html_url": "https://arxiv.org/html/2601.13801v1",
    "html_content": "HoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction\nYuhua Jin\nChinese University of Hong Kong, Shenzhen\nGuangdong\nChina\nyuhuajin@cuhk.edu.cn\n,\nNikita Kuzmin\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nNikita.Kuzmin@skoltech.ru\n,\nGeorgii Demianchuk\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nGeorgii.Demianchuk@skoltech.ru\n,\nMariya Lezina\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nMariya.Lezina@skoltech.ru\n,\nFawad Mehboob\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nFawad.Mehboob@skoltech.ru\n,\nIssatay Tokmurziyev\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nissatay.tokmurziyev@skoltech.ru\n,\nMiguel Altamirano Cabrera\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nm.altamirano@skoltech.ru\n,\nMuhammad Ahsan Mustafa\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nAhsan.Mustafa@skoltech.ru\nand\nDzmitry Tsetserukou\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nd.tsetserukou@skoltech.ru\nAbstract.\nDrones operating in human-occupied spaces suffer from insufficient communication mechanisms that create uncertainty about their intentions. We present HoverAI, an embodied aerial agent that integrates drone mobility, infrastructure-independent visual projection, and real-time conversational AI into a unified platform. Equipped with a MEMS laser projector, onboard semi-rigid screen, and RGB camera, HoverAI perceives users through vision and voice, responding via lip-synced avatars that adapt appearance to user demographics. The system employs a multimodal pipeline combining VAD, ASR (Whisper), LLM-based intent classification, RAG for dialogue, face analysis for personalization, and voice synthesis (XTTS v2). Evaluation demonstrates high accuracy in command recognition (F1: 0.90), demographic estimation (gender F1: 0.89, age MAE: 5.14 years), and speech transcription (WER: 0.181). By uniting aerial robotics with adaptive conversational AI and self-contained visual output, HoverAI introduces a new class of spatially-aware, socially responsive embodied agents for applications in guidance, assistance, and human-centered interaction.\nHuman-drone interaction, MEMS projection, Embodied AI agents, Aerial displays\n†\n†\nccs:\nHuman-centered computing Collaborative interaction\n†\n†\nccs:\nComputing methodologies Vision for robotics\n†\n†\nccs:\nComputer systems organization Robotic control\n†\n†\nccs:\nHuman-centered computing User interface design\nFigure 1\n.\nThe HoverAI interactive aerial interface: (a) The HoverAI drone featuring an RGB camera for user perception and a front-mounted semi-rigid projection screen for displaying the interactive avatar; (b) User interacting with HoverAI in an experimental environment, wearing headphones for audio input/output while the drone hovers and displays a projected avatar.\nThe figure illustrates the HoverAI system in two parts: (a) shows a drone with labeled components including a camera, projector, and projected frame; (b) depicts a user interacting with the HoverAI drone in an experimental environment, where an avatar is displayed on the drone’s screen, visually connecting the user and the drone in a shared physical space.\n1.\nIntroduction\nDigital interfaces are increasingly moving beyond fixed screens into physical environments, reshaping how people interact with computational systems. Embodied AI agents offer a promising paradigm for more natural and spatially-aware human-computer interaction\n(Fung et al\n.\n,\n2025\n; Shin et al\n.\n,\n2025\n)\n. As spatial computing evolves, such agents can make digital information accessible exactly where it is needed, enabling seamless integration of content into shared environments. However, challenges in mobility, perception, communication, and social presence must be addressed to fully realize this vision.\nTraditional interfaces such as smartphones and monitors confine users to fixed locations and require explicit attention, limiting their suitability for dynamic, multi-user settings. Augmented and virtual reality systems provide immersive experiences but often isolate users behind head-mounted displays and depend on controlled environments. These limitations motivate the development of mobile platforms that can interact with people directly within everyday physical spaces.\nGround robots are a common solution, yet their dependence on floor-based navigation restricts mobility. They must avoid obstacles, navigate through doorways, and cope with constantly changing layouts, making them unreliable in crowded or dynamic environments such as airports or museums. Drones, in contrast, provide unrestricted three-dimensional mobility: they can hover at eye level, bypass obstacles from above, and accompany users without obstructing pedestrian flow. These properties make drones attractive as interactive embodied agents. Recent work has demonstrated the feasibility of integrating advanced AI models onboard UAVs for real-time reasoning\n(Lykov et al\n.\n,\n2025\n)\n, but natural human-drone communication remains largely unresolved. Studies in human-drone interaction show that uncertainty about drone intentions and state significantly reduces trust and usability in public spaces\n(Lingam et al\n.\n,\n2025b\n,\na\n)\n.\nPrior efforts to create drone-based visual interfaces illustrate both the potential and limitations of current approaches. Large-scale drone light shows enable impressive aerial displays but are designed for passive viewing rather than close interaction\n(Dronisos,\n2025\n)\n. Systems such as BitDrones attached small displays to drones, yet these solutions are constrained by weight, power, and viewing-angle limitations\n(Gomes et al\n.\n,\n2016\n)\n. Projection-based approaches like LightAir and MaskBot enable expressive visuals, but rely on external surfaces or infrastructure, preventing fully mobile interaction\n(Matrosov et al\n.\n,\n2016\n; Cabrera et al\n.\n,\n2020\n)\n. Flying Light Specks achieved detailed visualizations using coordinated drones and fixed projectors, but similarly requires pre-installed equipment\n(Ghandeharizadeh,\n2022\n)\n. Recent work on expressive drone avatars explored social presence through digital faces\n(Bretin et al\n.\n,\n2025\n)\n, yet lacked real-time conversational intelligence and infrastructure-independent output. To date, no aerial system has combined onboard conversational AI, adaptive visual projection, and real-time social interaction in a single lightweight platform.\nWe present HoverAI, an interactive aerial agent that combines drone mobility, ultra-light MEMS laser projection, real-time speech understanding, and adaptive avatar expression in a self-contained system. Using an onboard RGB camera and projection screen, HoverAI perceives users and communicates via voice and projected imagery without relying on external infrastructure. A multimodal pipeline (VAD, ASR, lightweight LLMs, and RAG) enables contextual dialogue, while real-time face analysis personalizes a lip-synced avatar. This closed-loop design allows HoverAI to function as a socially present, spatially aware embodied agent capable of natural interaction in shared environments.\n2.\nSystem Architecture\nCurrently, no aerial system combines onboard conversational AI, environment-independent visual output, and real-time social adaptation in a single lightweight platform. HoverAI addresses this gap through a self-contained quadrotor that integrates flight, visual projection, environmental perception, and conversational interaction. As shown in Fig.\n2\n, the 1.2 kg platform comprises: an Orange Pi 5 single-board computer for real-time processing, a front-facing RGB camera (1080p, 30 fps), and a MEMS laser projector (720p, 30 fps, 85 g) paired with a semi-rigid projection film, enabling infrastructure-free visual display during flight.\nFigure 2\n.\nHoverAI quadcopter key hardware components.\nThis figure shows the key components of the drone: MEMS-based laser projector with a front-mounted flexible projection film, camera, and Orange Pi 5.\n2.1.\nHardware Architecture\nThe Laser Scanning Projection (LSP) module features a 2D MEMS scanning mirror delivering 720p resolution without the weight or power constraints of conventional displays. A semi-rigid polycarbonate film (0.3 mm, 40 g) suspended 15 cm in front of the projector remains stable under airflow and vibrations, projecting clear visuals without requiring external surfaces. The Orange Pi 5 manages flight control via a Speedybee F405V4 flight controller and communicates wirelessly with a ground station PC over WiFi (5 GHz,  50 ms latency). User speech is captured via a close-talking headphone microphone to maximize signal-to-noise ratio in indoor environments.\n2.2.\nInteraction Pipeline\nFigure 3\n.\nHoverAI pipeline with audio analysis, video-based face processing, and TTS-driven avatar output.\nThis figure shows the system architecture diagram showing data flow from user input to drone output. On the left, audio input goes through VAD and ASR to transcript, then to the command classifier. Video input goes to face analysis for age and gender. Command classifier sends either a command to drone action or a query to the RAG system. RAG returns an answer to TTS, which generates speech. Speech and face analysis data feed into the avatar display. Drone action controls physical movement. All components are connected with arrows indicating the direction of data flow.\nAs illustrated in Fig.\n3\n, HoverAI processes two parallel input streams. Audio is transmitted via WiFi to the ground station, where it undergoes spectral noise reduction, Voice Activity Detection (VAD)\n(Team,\n2024\n)\n, and Automatic Speech Recognition using Whisper-medium.en\n(Radford et al\n.\n,\n2023\n)\n(average WER: 0.181). Transcripts are classified by gemma:7b-instruct\n(DeepMind,\n2024\n)\ninto:\n•\nStructured commands\n(“follow,” “land,” “stay,” “explore face”) forwarded to the drone control module for immediate execution.\n•\nConversational queries\nrouted to a Retrieval-Augmented Generation (RAG) system with a domain-specific knowledge base (museum artefacts, FAQ), ensuring grounded responses and minimizing hallucination.\nGenerated responses are synthesized via XTTS v2 Text-to-Speech, which adapts voice characteristics (pitch, timbre) to match the selected avatar demographic\n(Casanova et al\n.\n,\n2024\n)\n.\nConcurrently, the video stream undergoes face analysis via InsightFace\n(InsightFace,\n2017\n)\n, estimating age and gender to select among four predefined avatars: young woman (\n<\n30\n<30\n), adult woman (\n≥\n30\n\\geq 30\n), young man (\n<\n30\n<30\n), and adult man (\n≥\n30\n\\geq 30\n). When no face is detected, a gender-neutral default avatar is displayed.\nThe synthesized speech plays through user’s headphones while a lip-synced avatar with subtitles renders on the drone screen at\n∼\n25\n\\sim 25\nfps (total pipeline latency: 800–1200 ms). This closed-loop architecture enables HoverAI to function as a socially present, embodied agent that adapts appearance, voice, and behavior to user input, supporting natural turn-taking and co-presence.\nThe modular design supports future extensions, including SLAM-based autonomous navigation, multi-drone swarm coordination for large-scale displays, and expanded knowledge bases with real-time web querying while maintaining response reliability.\n3.\nEvaluation\nTo validate HoverAI’s multimodal interaction capabilities, we conducted a benchmark evaluation measuring performance across speech recognition, intent classification, and demographic estimation.\n3.1.\nExperimental Setup\nWe recruited 12 participants (6 male, 6 female, aged 22-48) to interact with HoverAI in a controlled indoor laboratory environment (6\n×\n\\times\n6 m). Each participant completed a 5-minute interaction session involving:\n•\nSpeech tasks\n: 20 conversational queries (general knowledge, navigation) and 10 structured commands (“follow me,” “land,” “stay,” “explore face”)\n•\nVision tasks\n: Continuous face tracking during interaction for demographic estimation\nSpeech was captured via close-talking headphones in ambient noise conditions (45-50 dB). All sessions were recorded with informed consent. The RAG knowledge base contained 150 curated facts about robotics and museum artifacts.\n3.2.\nResults\nAs shown in Fig.\n4\n, HoverAI achieved strong performance across all modalities:\n•\nSpeech Transcription (WER: 0.181)\n: Whisper-medium.en demonstrated reliable ASR despite ambient noise, with most errors occurring on technical terminology.\n•\nCommand Recognition (F1: 0.90)\n: The gemma:7b classifier correctly distinguished commands from queries in 90% of cases, with confusion primarily between “stay” and conversational “wait” statements.\n•\nGender Estimation (F1: 0.89)\n: InsightFace achieved robust classification across lighting conditions and viewing angles (\n±\n\\pm\n30\n​\n°\n).\n•\nAge Estimation (MAE: 5.14 years)\n: While absolute age error averaged 5.14 years, binary classification (\n<\n30\n<30\nvs.\n≥\n30\n\\geq 30\n) for avatar selection achieved 91.7% accuracy, sufficient for demographic adaptation.\nFigure 4\n.\nPerformance metrics averaged across 12 participants: speech transcription (WER: 0.181), command recognition (F1: 0.90), gender classification (F1: 0.89), and age estimation (MAE: 5.14 years).\nBar chart showing four performance metrics: WER of 0.181, Command F1 of 0.9, Gender F1 of 0.89, and Age MAE of 5.14 years.\nEnd-to-end pipeline latency averaged 950 ms (\n±\n\\pm\n120 ms) from speech onset to avatar response, supporting natural conversational turn-taking. No system crashes occurred during 60 minutes of total interaction time.\n4.\nApplications and Use Cases\nHoverAI’s combination of mobility, conversational AI, and adaptive visual presence enables several application scenarios:\nMuseum and Educational Guidance\n: HoverAI can follow visitors through exhibitions, projecting contextual narratives, multilingual subtitles, or 3D reconstructions directly aligned with physical artefacts. Its spatial mobility enables optimal positioning for visibility, offering an alternative to static displays or audio guides for users with limited mobility.\nAssistive Communication\n: For users with motor disabilities or speech impairments, HoverAI can display speech-to-text output, mirror smartphone content, or serve as a visual proxy during remote calls. Its hands-free operation and human-scale presence are valuable in home healthcare and eldercare contexts where conventional devices may be inaccessible.\nPersonal Companion\n: HoverAI can provide ambient assistance in everyday environments, hovering at eye level to deliver contextual information or social engagement through natural conversation, creating a persistent visual presence without requiring touch or wearable devices.\n5.\nDiscussion\n5.1.\nLimitations\nThe system demonstrates robust technical performance, achieving high accuracy in speech recognition, command classification, and demographic estimation, and establishes a compelling foundation for socially responsive human-drone interaction.\nHowever, several technical and methodological limitations constrain the current prototype. Flight time is limited to\n∼\n12\n\\sim 12\nminutes by battery capacity, restricting deployment duration. Stable projection requires indoor operation with minimal wind (\n<\n<\n0.5 m/s) and controlled lighting; outdoor use degrades visibility. WiFi-based audio streaming limits operational range to\n∼\n15\n\\sim 15\nm. The semi-rigid screen occasionally exhibits vibration artefacts during aggressive maneuvers, requiring conservative flight profiles.\nCritically, our evaluation focused on technical performance metrics rather than user perception. While we demonstrated reliable speech recognition and demographic estimation, we did not assess whether the adaptive avatar actually reduces uncertainty or enhances social presence.\nFuture work should therefore include controlled user studies measuring perceived safety, trust, and clarity of drone intentions during interaction.\nQualitative feedback and comparative experiments with non-adaptive or non-visual drone interfaces would help determine how much the projected avatar contributes to usability and overall user experience.\nDeploying a drone that adapts its appearance based on users’ age and gender and displays a human-like avatar raises important ethical questions. Recording people’s faces and voices in public, especially without their consent, can violate privacy. Mistakes in estimating age or gender may lead to misrepresentation or bias, and the lifelike avatar might make people think the drone has intentions it does not actually have. To address these risks, future work should include clear indicators when recording is active, obtain user consent, handle data transparently, and comply with privacy laws.\nThe RAG system is limited to pre-defined knowledge bases (\n∼\n150\n\\sim 150\nfacts) and cannot retrieve real-time information or handle out-of-domain queries gracefully, occasionally producing generic responses.\n5.2.\nFuture Directions\nNear-term improvements include SLAM-based autonomous navigation for spatial positioning, extended battery life through optimized power management, and expanded knowledge bases with vetted external source integration.\nRobustness to real-world acoustic environments is another critical direction: our current evaluation was conducted in controlled indoor settings with moderate ambient noise, but practical deployments in museums, educational spaces, or urban outdoor areas often involve higher noise levels and overlapping speech. Future work should therefore evaluate and enhance the system’s speech perception pipeline under diverse acoustic conditions, potentially incorporating noise-robust ASR models to maintain performance.\nLonger-term research directions include multi-drone swarm coordination for large-scale collaborative displays, 3D volumetric visualization, and distributed perception across coordinated units. Investigating outdoor deployment with brighter projection and stabilized screen mechanisms would broaden applicability.\n6.\nConclusion\nWe presented HoverAI, an embodied aerial agent that integrates infrastructure-independent visual projection, real-time conversational AI, and demographic-adaptive avatar generation into a self-contained mobile platform. By combining MEMS laser projection with a semi-rigid screen, multimodal perception through vision and speech, and closed-loop interaction via LLM-based dialogue and face analysis, HoverAI demonstrates a new approach to spatially-aware human-drone interaction. Evaluation across 12 participants showed robust performance in speech recognition (WER: 0.181), command classification (F1: 0.90), and demographic estimation, establishing technical feasibility for applications in guidance, assistance, and companionship. HoverAI represents a step toward mobile, socially responsive interfaces that bring digital content into shared physical spaces in more human-centered ways.\nAcknowledgements\nResearch reported in this publication was financially supported by the RSF grant No. 24-41-02039.\nReferences\n(1)\nBretin et al\n.\n(2025)\nRobin Bretin, Mohamed Khamis, Emily Cross, and Mohammad Obaid. 2025.\nThe Role of Drone’s Digital Facial Emotions and Gaze in Shaping Individuals’ Social Proxemics and Interpretation.\nJ. Hum.-Robot Interact.\n14, 3, Article 48 (May 2025), 34 pages.\ndoi:\n10.1145/3714477\nCabrera et al\n.\n(2020)\nMiguel Altamirano Cabrera, Igor Usachev, Juan Heredia, Jonathan Tirado, Aleksey Fedoseev, and Dzmitry Tsetserukou. 2020.\nMaskbot: Real-time robotic projection mapping with head motion tracking.\nIn\nSIGGRAPH Asia 2020 Emerging Technologies\n. 1–2.\nCasanova et al\n.\n(2024)\nEdresson Casanova, Kelly Davis, Eren Golge, Gorkem Goknar, Iulian Gulea, Logan Hart, Aya Aljafari, Joshua Meyer, et al\n.\n2024.\nXtts: a massively multilingual zero-shot text-to-speech model\n.\narXiv:2406.04904\nRetrieved from https://arxiv.org/abs/2406.04904.\nDeepMind (2024)\nGoogle DeepMind. 2024.\ngemma:7b-instruct: Open model for Ollama.\nhttps://ollama.com/library/gemma:7b-instruct\n.\nAccessed: 2025-12-08.\nDronisos (2025)\nDronisos. 2025.\nDrone Light Shows indoor vs outdoor.\nhttps://www.dronisos.com/post/drone-light-shows-indoor-vs-outdoor\nAccessed: 2025-12-08.\nFung et al\n.\n(2025)\nPascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hongyu Gong, Hervé Jégou, et al\n.\n2025.\nEmbodied AI Agents: Modeling the World\n.\narXiv:2506.22355\nRetrieved from https://arxiv.org/abs/2506.22355.\nGhandeharizadeh (2022)\nShahram Ghandeharizadeh. 2022.\nDisplay of 3D Illuminations using Flying Light Specks. In\nProc. of the 30th ACM Int. Conf. on Multimedia\n. New York, NY, USA, 2996–3005.\ndoi:\n10.1145/3503161.3548250\nGomes et al\n.\n(2016)\nAntonio Gomes, Calvin Rubens, Sean Braley, and Roel Vertegaal. 2016.\nBitDrones: Towards Using 3D Nanocopter Displays as Interactive Self-Levitating Programmable Matter. In\nProc. of the 2016 CHI Conf. on Human Factors in Computing Systems\n(CHI ’16)\n. 770–780.\ndoi:\n10.1145/2858036.2858519\nInsightFace (2017)\nInsightFace. 2017.\nInsightFace: State-of-the-art 2D & 3D Face Analysis Project.\nhttps://github.com/deepinsight/insightface\n.\nAccessed: 2025-12-08.\nLingam et al\n.\n(2025b)\nShiva Lingam, Rutger Verstegen, Sebastiaan Petermeijer, and Marieke Martens. 2025b.\nHuman Interactions With Delivery Drones in Public Spaces: Design Recommendations From Recipient and Bystander Perspectives.\ndoi:\n10.13140/RG.2.2.16544.70405\nLingam et al\n.\n(2025a)\nShiva Nischal Lingam, Mervyn Franssen, Sebastiaan M Petermeijer, and Marieke Martens. 2025a.\nChallenges and future directions for human-drone interaction research: an expert perspective.\nInternational Journal of Human–Computer Interaction\n41, 12 (2025), 7905–7921.\nLykov et al\n.\n(2025)\nArtem Lykov, Valerii Serpiva, Muhammad Haris Khan, Oleg Sautenkov, Artyom Myshlyaev, Grik Tadevosyan, Yasheerah Yaqoot, and Dzmitry Tsetserukou. 2025.\nCognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs\n.\narXiv:2503.01378.\nRetrieved from https://arxiv.org/abs/2503.01378.\nMatrosov et al\n.\n(2016)\nMikhail Matrosov, Olga Volkova, and Dzmitry Tsetserukou. 2016.\nLightAir: a novel system for tangible communication with quadcopters using foot gestures and projected image. In\nACM SIGGRAPH 2016 Emerging Technologies\n(Anaheim, California)\n(SIGGRAPH ’16)\n. Association for Computing Machinery, New York, NY, USA, Article 16, 2 pages.\ndoi:\n10.1145/2929464.2932429\nRadford et al\n.\n(2023)\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023.\nRobust speech recognition via large-scale weak supervision. In\nInt. conf. on machine learning\n. PMLR, 28492–28518.\nShin et al\n.\n(2025)\nJihun Shin, Hyeonjin Kim, Eunseong Lee, Donghwan Shin, Kwang Bin Lee, Taehei Kim, Hyeshim Kim, Joonsik An, and Sung-Hee Lee. 2025.\nSituated Embodied XR Agents via Spatial Reasoning and Prompting . In\n2025 IEEE Int. Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)\n. IEEE Computer Society, Los Alamitos, CA, USA, 933–934.\ndoi:\n10.1109/ISMAR-Adjunct68609.2025.00255\nTeam (2024)\nSilero Team. 2024.\nSilero VAD: pre-trained enterprise-grade Voice Activity Detector (VAD), Number Detector and Language Classifier.\nhttps://github.com/snakers4/silero-vad\n.",
    "preview_text": "Drones operating in human-occupied spaces suffer from insufficient communication mechanisms that create uncertainty about their intentions. We present HoverAI, an embodied aerial agent that integrates drone mobility, infrastructure-independent visual projection, and real-time conversational AI into a unified platform. Equipped with a MEMS laser projector, onboard semi-rigid screen, and RGB camera, HoverAI perceives users through vision and voice, responding via lip-synced avatars that adapt appearance to user demographics. The system employs a multimodal pipeline combining VAD, ASR (Whisper), LLM-based intent classification, RAG for dialogue, face analysis for personalization, and voice synthesis (XTTS v2). Evaluation demonstrates high accuracy in command recognition (F1: 0.90), demographic estimation (gender F1: 0.89, age MAE: 5.14 years), and speech transcription (WER: 0.181). By uniting aerial robotics with adaptive conversational AI and self-contained visual output, HoverAI introduces a new class of spatially-aware, socially responsive embodied agents for applications in guidance, assistance, and human-centered interaction.\n\nHoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction\nYuhua Jin\nChinese University of Hong Kong, Shenzhen\nGuangdong\nChina\nyuhuajin@cuhk.edu.cn\n,\nNikita Kuzmin\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nNikita.Kuzmin@skoltech.ru\n,\nGeorgii Demianchuk\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nGeorgii.Demianchuk@skoltech.ru\n,\nMariya Lezina\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nMariya.Lezina@skoltech.ru\n,\nFawad Mehboob\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nFawad.Mehboob@skoltech.ru\n,\nIssatay Tokmurziyev\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nissatay.tokmurziyev@skoltech.ru\n,\nMiguel Altamirano Cabrera\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nm.altamirano@skoltech.ru\n,\nMuhammad Ahsan Mustafa\nSkolkovo Institute of Science ",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "embodied agent",
        "aerial robotics",
        "human-drone interaction",
        "multimodal pipeline",
        "conversational AI",
        "visual projection",
        "personalization"
    ],
    "one_line_summary": "HoverAI是一个集成了无人机移动性、视觉投影和实时对话AI的空中代理系统，用于自然的人机交互，但未涉及强化学习、扩散模型或全身控制等指定技术。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T09:59:49Z",
    "created_at": "2026-01-27T15:53:10.103761",
    "updated_at": "2026-01-27T15:53:10.103767"
}