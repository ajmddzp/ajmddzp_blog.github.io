{
    "id": "2601.10455v1",
    "title": "SurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability",
    "authors": [
        "Ruochen Li",
        "Kun Yuan",
        "Yufei Xia",
        "Yue Zhou",
        "Qingyu Lu",
        "Weihang Li",
        "Youxiang Zhu",
        "Nassir Navab"
    ],
    "abstract": "手术规划融合了视觉感知、长程推理与程序性知识，然而当前评估方案能否在安全关键场景中可靠衡量视觉语言模型（VLMs）的性能仍不明确。基于目标导向的手术规划视角，我们通过阶段目标可满足性来定义规划正确性，即规划有效性由专家定义的手术规则判定。基于此定义，我们构建了一个多中心元评估基准，包含有效程序变体及存在顺序与内容错误的无效规划。通过该基准，我们发现序列相似度指标系统性误判规划质量：既对有效规划施加不当惩罚，又无法识别无效规划。因此，我们采用基于规则的目标可满足性指标作为高精度元评估参照，在渐进约束条件下评估视频大语言模型，揭示了感知错误与约束不足导致的推理失效现象。结构化知识能持续提升模型表现，而纯语义引导具有不可靠性，仅在与结构化约束结合时才对更大规模模型产生增益。",
    "url": "https://arxiv.org/abs/2601.10455v1",
    "html_url": "https://arxiv.org/html/2601.10455v1",
    "html_content": "SurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability\nRuochen Li\n1\nKun Yuan\n1,2\n1\n1\nfootnotemark:\n1\nYufei Xia\n3\nYue Zhou\n1\nQingyu Lu\n4\nWeihang Li\n1\nYouxiang Zhu\n5\nNassir Navab\n1\n2\n2\nfootnotemark:\n2\n1\nTechnical University of Munich, Germany\n2\nUniversity of Strasbourg, France\n3\nUniversity of Glasgow, United Kingdom\n4\nNanyang Technological University, Singapore\n5\nUniversity of Massachusetts Boston, USA\nEqual contribution.\nJoint supervision.\nAbstract\nSurgical planning integrates visual perception, long-horizon reasoning, and procedural knowledge, yet it remains unclear whether current evaluation protocols reliably assess vision-language models (VLMs) in safety-critical settings.\nMotivated by a goal-oriented view of surgical planning, we define planning correctness via phase-goal satisfiability, where plan validity is determined by expert-defined surgical rules. Based on this definition, we introduce a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans containing order and content errors.\nUsing this benchmark, we show that sequence similarity metrics systematically misjudge planning quality, penalizing valid plans while failing to identify invalid ones. We therefore adopt a rule-based goal-satisfiability metric as a high-precision meta-evaluation reference to assess Video-LLMs under progressively constrained settings, revealing failures due to perception errors and under-constrained reasoning. Structural knowledge consistently improves performance, whereas semantic guidance alone is unreliable and benefits larger models only when combined with structural constraints.\nSurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability\nRuochen Li\n1\n†\n†\nthanks:\nEqual contribution.\nKun Yuan\n1,2\n1\n1\nfootnotemark:\n1\nYufei Xia\n3\nYue Zhou\n1\nQingyu Lu\n4\nWeihang Li\n1\nYouxiang Zhu\n5\n†\n†\nthanks:\nJoint supervision.\nNassir Navab\n1\n2\n2\nfootnotemark:\n2\n1\nTechnical University of Munich, Germany\n2\nUniversity of Strasbourg, France\n3\nUniversity of Glasgow, United Kingdom\n4\nNanyang Technological University, Singapore\n5\nUniversity of Massachusetts Boston, USA\n1\nIntroduction\nVision-language models (VLMs) have become powerful foundation models capable of reasoning over visual content through natural language\n(Zhang\net al.\n,\n2025a\n)\n. In the video domain, they have progressed beyond low-level perception toward long-horizon temporal reasoning\n(Grauman\net al.\n,\n2022\n; Bai\net al.\n,\n2025\n)\n, enabling action recognition\n(Fan and Zheng,\n2024\n)\n, anticipation\n(Lin\net al.\n,\n2024\n)\n, and task-oriented planning\n(Li\net al.\n,\n2025a\n; Zhao\net al.\n,\n2023\n; Li\net al.\n,\n2025b\n)\n, and supporting online assistance in everyday scenarios. As these capabilities mature, VLMs are increasingly considered for deployment in high-stakes settings, where errors carry significant consequences. One such domain is surgery, where intelligent intra-operative surgical planning\n(Boels\net al.\n,\n2025b\n,\na\n; Xu\net al.\n,\n2025b\n)\npredicts future actions conditioned on the current procedural state and the goal, which is highly desirable but subject to strict requirements on safety and reliability issues.\nDespite recent progress in surgical planning models, it remains unclear whether existing evaluation protocols reliably measure clinically valid planning behavior. Most prior work evaluates planning outputs by comparing predicted step or phase sequences to a single reference trajectory using surface sequence similarity metrics\n(Ding\net al.\n,\n2025\n; Xu\net al.\n,\n2025a\n)\nsuch as edit distance or relative order accuracy. These metrics equate planning correctness with resemblance to one observed execution, potentially rewarding unsafe ordering errors while penalizing clinically plausible alternatives. This mismatch raises concerns about whether current evaluations meaningfully reflect planning capability in safety-critical surgical settings.\nTo address this, motivated by a goal-oriented, multi-step view of surgical planning, we define planning correctness via phase-goal satisfiability, with plan validity determined by expert-defined surgical rules encoding hierarchical phase–step relations and procedural constraints. Building on this definition, we introduce a multicentric meta-evaluation benchmark grounded in MultiBypass140\n(Lavanchy\net al.\n,\n2024\n)\n, comprising clinically valid procedural variations and systematically constructed invalid plans with order and content errors. This benchmark enables meta-evaluation of planning metrics by assessing whether their validity judgments align with goal satisfiability rather than surface-level resemblance.\nFigure 1:\nComparison of sequence similarity metrics and rule-based checker metric.\nTop\n: Surgical procedures follow a hierarchical structure: each phase (e.g., P5) contains mandatory core steps (blue) and permissible generic steps (green).\nBottom Left\n: Sequence similarity metrics compare predictions to a fixed reference, causing false negatives for valid clinical variations (Prediction A) and false positives for prohibited orderings (Prediction B).\nBottom Right\n: The rule-based checker correctly distinguishes valid from invalid plans using surgical rules.(Section\n3.1\n)\nUsing this benchmark, we show that widely used sequence similarity metrics are fundamentally misaligned with goal-satisfiability, and that LLM-as-a-judge baselines often capture semantic omissions but fail to enforce strict procedural dependencies.\nLacking a reliable alternative for planning correctness, we then adopt a rule-based checker metric as a high-precision meta-evaluation reference to assess Video-LLMs under progressively informative planning settings, from end-to-end video planning to explicit state and injected knowledge. This reveals failure modes driven by visual misrecognition and under-constrained reasoning, and shows that explicit procedural structure yields consistent gains, whereas semantic knowledge benefits depend strongly on model capacity.\nThese insights highlight the need for more robust and clinically grounded evaluation protocols to support the development of reliable surgical planning systems in high-stakes settings. Contributions:\n•\nWe define planning correctness via goal-satisfiability under expert-defined surgical rules.\n•\nWe introduce a meta-evaluation benchmark with valid procedural variations and invalid plans.\n•\nWe benchmark traditional metrics and LLM-based judges under a meta-evaluation framework, revealing systematic evaluation misalignment.\n•\nWe introduce a rule-based checker metric and use it to evaluate Video-LLMs across progressively constrained planning tasks.\n2\nRethinking Surgical Planning: Task Formulation and Prior Work\n2.1\nPrevious Work: Flat Prediction under Strong Structural Constraints\nTask formulation mismatch (flat vs. hierarchical).\nPrior work in surgical planning predominantly formulates the problem as flat prediction, including next-action classification\n(Xu\net al.\n,\n2025b\n,\na\n)\n, phase transition forecasting\n(Boels\net al.\n,\n2025a\n; Chen\net al.\n,\n2025\n)\n. These formulations assume a single temporal granularity and reduce planning to local transitions between adjacent actions or phases.\nHowever, real surgical procedures are inherently hierarchical\n(Biagini\net al.\n,\n2025\n; Lavanchy\net al.\n,\n2024\n; Lalys and Jannin,\n2014\n)\n. Phases correspond to strategic sub-goals, steps instantiate tactical plans, and actions serve as execution primitives. Planning cognition operates not at isolated action transitions, but in organizing multi-step structures toward phase-level objectives. Flat formulations obscure this hierarchy and fail to capture operative procedural logic.\nObjective mismatch (transition-based vs. goal-oriented).\nExisting approaches largely focus on predicting what comes next, treating planning as a sequence of independent transition decisions. In contrast, surgical decision-making is fundamentally goal-oriented. Surgeons do not decide on isolated actions; instead, they engage in top-down reasoning. A surgeon first establishes a strategic phase level objective (e.g., jejunal separation) , which then constrains and motivates a sequence of coordinated steps (e.g., mesenteric opening, jejunal transection) planned over a long temporal horizon.\nAs a result, transition-centric models lack the semantic grounding needed to explain why a step is performed and whether a sequence meaningfully contributes to procedural completion. For instance, a dissection action may be appropriate during the dissection of Calot’s triangle, but would be questionable if it occurs during the abdominal closure phase\n(Lavanchy\net al.\n,\n2025\n)\n.\nPath multiplicity ignored (single trajectory vs. multiple valid plans).\nClinical reality admits multiple valid step-level paths to achieve the same phase-level goal, driven by surgeon preference, patient anatomy, intraoperative findings, and institutional style. In practice, even the same surgical team may legitimately reorder phases; for instance, in Roux-en-Y gastric bypass\n(Lavanchy\net al.\n,\n2024\n)\n, performing omentum division before gastric pouch creation can improve operative exposure. Such variations are clinically equivalent as they preserve essential dependencies (e.g., performing anastomosis before its integrity test).\nPrior work, however, typically treats the observed sequence as a unique ground truth, implicitly equating deviations with errors.\nThis one-to-one assumption contradicts surgical practice, where procedural correctness is defined by dependency preservation rather than exact sequence identity. Modeling surgical planning as deterministic single-path prediction therefore systematically penalizes valid variations and conflates alternative plans with incorrect ones, despite strong structural constraints.\n2.2\nEvaluation Mismatch: From Sequence Similarity to Goal Satisfiability\nGiven that surgical planning is goal-oriented, and admits multiple valid paths, a key issue lies in how it is evaluated. Prior work predominantly relies on sequence match metrics and their variants, including edit distance, Jaccard similarity, and relative order accuracy. Despite different formulations, these metrics share a common design: they measure surface-level similarity between a predicted sequence and a single reference trajectory.\nThis evaluation paradigm implicitly assumes a unique correct execution path, equates deviation with error, and treats order similarity as a proxy for procedural validity. As a result, planning is assessed by agreement with an observed rollout rather than by whether a sequence can meaningfully accomplish the intended surgical objective.\nThese assumptions contradict surgical reality. Because multiple step-level paths may validly satisfy the same phase-level goal, sequence similarity metrics produce false negatives by penalizing legitimate alternatives, and false positives by rewarding superficially similar but dependency-violating plans (Figure\n1\n).\nTherefore, procedural correctness should be defined not by sequence identity, but by whether a plan plausibly satisfies the surgical goal under essential procedural dependencies.\nThis task-evaluation mismatch motivates the need for a goal-satisfiability-based perspective and leads to our meta-evaluation framework.\nFigure 2:\nMeta-evaluation and Evaluation Pipelines for Surgical Planning.\nLeft: Rule-based benchmark defining goal-satisfiability via hierarchical phase-step relations and procedural constraints (dependencies and prohibitive orderings), separating valid and invalid step sequences. Right: Progressive evaluation of Video-LLMs, from end-to-end planning to planning with ground-truth steps and injected knowledge.\n3\nMeta-Evaluation: A Goal-Satisfiability Benchmark for Surgical Planning\nThis section introduces a meta-evaluation benchmark to test whether existing planning metrics correctly assess goal-satisfiability in surgical planning.\n3.1\nFormalizing Goal-Satisfiability with Surgical Rules\nOur benchmark is grounded in the MultiBypass140 dataset\n(Lavanchy\net al.\n,\n2024\n)\n, a multicentric collection of 140 laparoscopic Roux-en-Y gastric bypass procedures with untrimmed videos and surgeon-annotated hierarchical labels spanning 11 phases and 45 fine-grained steps.\nWe formalize goal-satisfiability via a set of expert-defined surgical rules that determine whether a step sequence can plausibly complete a target phase. The rules encode clinically essential constraints derived from phase semantics and procedural dependencies and are intended as a high-precision reference for meta-evaluation. Rather than enumerating all valid surgical variations, the rules conservatively distinguish sequences that are\ndefinitively invalid\nfrom those that are\nplausibly goal-satisfiable\nwithin a well-defined scope.\nRule Components.\nAn example of phase P5 (anastomosis test).\n•\nRequired Steps Set\ndefines mandatory steps to achieve the phase goal: S22 (gastric tube placement), S23 (jejunal clamping), S24 (dye injection), and S25 (visual assessment).\n•\nAllowed Steps Set\nenumerates additional steps that may plausibly occur without violating the phase objective, such as S3 (retractor placement), S39 (hemostasis), or S40 (irrigation aspiration).\n•\nProcedural Dependencies\nenforce critical ordering constraints among core steps, such as, if S24 occurs, the first occurrence of S23 must strictly precede it, and S25 must occur after S24.\n•\nProhibitive Orderings\nfurther restricts when ancillary steps may appear. Actions such as S39 (hemostasis) or S40 (irrigation aspiration), if present, are permitted only after clamping has begun. Crucially, once the last S25 is completed, core test steps are prohibited from reappearing, explicitly marking completion of the phase.\nWe construct a comprehensive rule specification that encodes all constraints described above, including 50 expert-defined procedural dependencies and prohibitions, hierarchical phase-step relations, and phase-specific allowed and required steps from the MultiBypass140 annotation protocol. A step sequence is labeled as valid if it satisfies all rules defined for the target phase; any violation of rules renders it invalid. These rules provide a high-precision reference labeling of goal-satisfiability within the scope of the benchmark, serving as the basis for evaluating whether different metrics capture the intended notion of procedural validity. They are not intended to represent an exhaustive clinical ground truth, but rather a conservative reference that reliably identifies definitively invalid plans.\n3.2\nMeta-Evaluation Dataset Construction\nBuilding on these surgical rules, we construct a meta-evaluation dataset from MultiBypass140 designed to probe metric behavior under a multi-path formulation.\nWe curate a test set of step-phase pairs, categorized by expert surgeons (Figure\n2\n):\n•\nCorrect Sequences\n(\nN\n=\n191\nN=191\n): Clinically valid paths that satisfy all rules and achieve the phase goal, allowing legitimate variations in step order and ancillary maneuvers (e.g., hemostasis).\n•\nIncorrect Sequences\n(\nN\n=\n199\nN=199\n): Paths that violate one or more rules and fail to achieve the phase goal, further sub-classified into\nOrder Errors\n(OE; violations of procedural dependencies),\nContent Errors\n(CE; missing core steps or inclusion of any step outside the allowed set of the corresponding phase), and\nBoth\n(BE).\nTraditional Metrics\n⋆\n\\star\nLLM-based Judges\nSubset\nNED\nJIS\nROA\nRule\nGemini3\nGPT 5.2\nClaude 4.5\nHuluMed\nValid\n18.8\n18.8\n40.3\n40.3\n93.2\n93.2\n100.0\n100.0\n99.5\n99.5\n97.4\n97.4\n61.8\n61.8\n99.0\n99.0\nOE\n87.3\n87.3\n46.5\n46.5\n11.3\n11.3\n100.0\n100.0\n11.3\n11.3\n18.3\n18.3\n80.3\n80.3\n8.5\n8.5\nCE\n86.8\n86.8\n85.3\n85.3\n17.6\n17.6\n100.0\n100.0\n85.3\n85.3\n97.1\n97.1\n97.1\n97.1\n58.8\n58.8\nBE\n96.7\n96.7\n85.0\n85.0\n20.0\n20.0\n100.0\n100.0\n98.3\n98.3\n100.0\n100.0\n100.0\n100.0\n75.0\n75.0\nInvalid\n89.9\n89.9\n71.4\n71.4\n17.1\n17.1\n100.0\n100.0\n63.8\n63.8\n69.8\n69.8\n92.0\n92.0\n45.7\n45.7\nTable 1:\nAccuracy (%) comparison of traditional similarity metrics and LLM-based judges\nacross valid and erroneous subsets.\nNED: Normalized Edit Distance.\nJIS: Jaccard Index on Sequences.\nROA: Relative Order Accuracy.\nOE: order error.\nCE: content error.\nBE: both error.\n3.3\nMeta-Evaluation Protocol\nWe define a unified protocol to evaluate planning metrics under a goal-satisfiability formulation.\nInput.\nThe input to a metric is a candidate step sequence and its associated target phase. For metrics that require a reference sequence, we construct a canonical reference using the phase-specific core steps defined by the surgical rules, ordered according to standard procedural dependencies.\nOutput.\nEach metric produces a binary judgment indicating whether the candidate sequence is considered valid (goal-satisfiable) or invalid for completing the target phase. For continuous-valued metrics, scores are thresholded at 0.7 to obtain a binary decision.\nComparison and Reporting.\nThis protocol evaluates whether a metric’s decision boundary aligns with the rule-based definition of goal-satisfiability, rather than surface similarity to a single reference sequence. Performance is reported as binary classification accuracy, optionally stratified by sequence category (Valid, OE, CE, BE) to analyze metric sensitivity to different failure modes.\n4\nBenchmarking Planning Metrics under Goal-Satisfiability Meta-Evaluation\nUnder the proposed meta-evaluation protocol, planning metrics are evaluated by their ability to classify step sequences as goal-satisfiable or invalid.\n4.1\nMetrics under Comparison.\nSequence Similarity Metrics\nWe evaluate a representative set of surface-level metrics, including Normalized Edit Distance (NED)\n(Marzal and Vidal,\n2002\n)\n, Jaccard Index on Sequences (JIS)\n(Broder,\n1997\n)\n, and Relative Order Accuracy (ROA)\nKendall (\n1938\n)\n, using their standard formulations. These metrics, together with their commonly used variants, have been widely adopted to measure sequence similarity and relative order agreement in prior surgical workflow analysis.\nRule-based Checker Metric.\nWe include an expert-defined rule-based checker derived from the surgical rules in Section\n3.1\n, which labels sequences as valid or invalid; since the meta-evaluation dataset strictly follows these rules, the checker serves as an upper bound on performance.\nLLM Judge.\nWe evaluate several LLM-based judges that assess plan plausibility using injected phase-step relationships and descriptions, rather than the explicit rules in Section\n3.1\n. These judges output both a binary validity decision and a textual explanation, providing a more flexible and potentially scalable alternative to the rule-based checker.\n4.2\nResults and Analysis\nSurface-level similarity metrics exhibit a systematic bias.\nNED and JIS achieve high accuracy on invalid sequences but perform poorly on valid plans (Table\n1\n), misclassifying the majority of clinically correct variations. This confirms a similarity trap: deviations from a single reference trajectory are penalized regardless of whether the phase goal is satisfied. As a result, these metrics conflate procedural diversity and flexibility with error.\nROA is permissive but unsafe.\nROA achieves high accuracy on valid sequences but fails catastrophically on order errors (Table\n1\n). By measuring only relative pairwise order, it overlooks repetitions and critical misplacements that render a procedure infeasible, thereby rewarding sequences that violate essential temporal and causal constraints.\nRule-based evaluation provides a high-precision reference.\nThe expert-defined rule checker metric achieves perfect accuracy across all subsets within its defined scope, as it is directly derived from the same rules used to construct the dataset; we therefore treat it as a high-precision upper bound. However, this approach requires substantial expert effort and is highly task-specific, limiting its practicality and scalability to broader procedures or settings.\nLLM-based Judges: Semantics over Structure.\nTable\n1\nindicates that LLM-based evaluators perform well on content errors, indicating strong semantic understanding of phase goals and missing steps. They utilize internal medical knowledge to recognize that\nGastric Pouch Creation\ncannot be completed if the\nstapling\nstep is missing.\nHowever, most models struggle with order errors, frequently approving sequences that violate critical procedural dependencies. For phase P5, S24 is placed before S23, which would allow the dye to escape downstream before the test segment is sealed, thereby invalidating the anastomotic leak test; nevertheless, GPT judged the sequence as correct, reasoning that all required instruments for the test were present. This failure highlights a systematic tendency of LLMs to prioritize semantic completeness over strict procedural ordering, leading to errors when correct execution depends on temporal or causal constraints rather than the mere presence of actions.\nMoreover, different models exhibit distinct biases: HuluMed tends to over-accept plausibly complete plans, while Claude over-reject them; yet none consistently enforce dependency-level correctness.\nOverall, existing planning metrics fail to reliably assess goal-satisfiability in realistic multi-path settings: rule-based evaluation is precise but unscalable, while LLM-based evaluators are flexible yet unable to infer strict procedural constraints.\n5\nEvaluation of Video-LLMs for Goal-Oriented Surgical Planning\n5.1\nExperimental Setup\nDataset.\nExperiments are conducted on MultiBypass140. To focus on logical planning rather than temporal boundary detection, we segment each video into 5,032 discrete step-level clips based on expert annotations. Each clip preserves its full temporal context (up to ten minutes), ensuring that the model has access to the complete visual evidence of the ongoing maneuver.\nModels.\nWe evaluate VideoLLaMA3-7B\n(Zhang\net al.\n,\n2025b\n)\n, LLaVA-NeXT-Video-7B\n(Li\net al.\n,\n2024\n)\n, Qwen2.5-VL (7B/32B)\n(Bai\net al.\n,\n2025\n)\n, and medical models Hulu-Med (7B/32B)\n(Jiang\net al.\n,\n2025\n)\nand Lingshu (7B/32B)\n(Xu\net al.\n,\n2025c\n)\n. All models are evaluated zero-shot (temperature = 0, max output = 2048 tokens).\n5.2\nProgressive Task Formulation\nTo disentangle visual perception from procedural reasoning, we define progressively constrained planning tasks. (Figure\n2\n)\nReal-World Setting: End-to-End Planning.\nTask 1\n: Models are given raw surgical video clips, the current phase label, the history of completed steps, and a set of candidate step labels. They must infer the current procedural state from visual evidence and generate a plausible future step sequence. This setting mimics a real-world intraoperative assistant that must simultaneously ground its understanding in visual evidence (What is happening now?) and extrapolate future actions.\nControlled Setting: Planning with Explicit State.\nTask 2\n: To isolate planning from perception, models are additionally provided with the current step identity, while retaining video input. This removes ambiguity about the procedural state and allows us to directly assess the model’s ability to plan future steps from a fixed starting point.\nKnowledge Injection for Surgical Planning.\nTask 3\n: We further investigate how external medical expertise modulates planning quality by injecting three forms of knowledge into the Task 2 setup:\n3.1\nStructural knowledge: phase-step hierarchy.\n3.2\nSemantic knowledge: expert-written natural language descriptions of the phases and steps.\n3.3\nCombined knowledge: both knowledge.\n5.3\nPlanning Output\nEach model produces a unified structured output following a fixed JSON schema:\n•\nRemaining steps to complete the current phase\n•\nNext phase (as a phase name)\n•\nReasonable step sequence for the next phase\n•\nExplanation (brief justification)\nThis output supports two planning tasks under a unified evaluation protocol:\ncurrent-phase completion planning\n, evaluated on the combined plan of completed steps, the current step, and predicted remaining steps;\nnext-phase planning\n, evaluated on the generated step sequence for the predicted next phase.\n5.4\nEvaluation\nWe evaluate VLM surgical planning using\ngoal-satisfiability accuracy\n. In the absence of a reliable alternative metric (Section\n4.2\n), we use an expert-defined rule-based checker metric (Section\n4.1\n) to determine whether each generated step sequence constitutes a plausible path for completing the target phase. Accuracy is computed as the fraction of sequences judged valid.\nFor Task 1, we additionally report\ncurrent step recognition\n, defined as exact matching between the predicted and ground-truth current step, to separate step recognition from planning quality.\nWe do not evaluate exact next-phase prediction. Surgical planning admits multiple valid phase transitions and does not assume a single canonical next phase.\n5.5\nResults and Analysis\nTask\nMetric\nHulu-7B\nHulu-32B\nQwen-7B\nQwen-32B\nDAMO\nLingshu-7B\nLingshu-32B\nLLaVA\nReal-world\nStepAcc\n23.5%\n34.0%\n21.7%\n31.4%\n14.4%\n26.1%\n39.4%\n2.8%\nCurrent\n10.3%\n23.8%\n2.9%\n20.9%\n8.8%\n7.4%\n31.1%\n1.3%\nNext\n2.9%\n45.6%\n2.4%\n31.8%\n1.4%\n1.0%\n46.5\n%\n0.1%\nControlled\nCurrent\n27.9%\n40.2%\n13.7%\n48.6%\n22.5%\n22.3%\n51.1%\n20.5%\nNext\n2.7%\n31.0%\n2.9%\n29.4%\n1.5%\n4.5%\n40.7%\n0.1%\nTable 2:\nGoal-satisfiability accuracy (%) across models.\nTask 1 (real-world) evaluates current step recognition (StepAcc) and downstream planning, while Task 2 (controlled) focuses on phase-aware planning.\nCurrent and Next denote current-phase completion and next-phase planning accuracy.\nBest results are shown in bold.\nWe analyze the results of Section\n5.2\nby progressively isolating the roles of visual perception, planning logic, and medical knowledge in goal-oriented surgical planning.\n5.5.1\nThe Perception-Reasoning Gap in End-to-End Planning\nTask 1 reflects the real-world setting where models must infer procedural state directly from video. As shown in Table\n2\n, even the strongest model in our comparison (Lingshu-32B) achieves only 39.4% step recognition accuracy, resulting in substantial downstream planning errors. A clinician-led analysis reveals two dominant perception failure modes.\nConfusing exploration with procedural steps.\nModels often fail to distinguish long, repetitive exploratory video segments (e.g., tissue retraction to locate target organs or vessels) from well-defined surgical steps.\nFor example, prolonged tissue exploration before definitive vessel exposure is often misclassified as a subsequent surgical step. As a result, models may prematurely conclude that a phase objective has been met, producing implausible plans that omit essential preparatory actions.\nFailing to recognize repeated steps.\nModels also struggle when the same step appears multiple times within a phase. During gastric pouch creation phase, steps such as horizontal stapling, retrogastric dissection, vertical stapling, and hemostasis may repeat multiple times. Models often misinterpret such repetition as phase progression or completion, leading to incorrect phase status estimation and subsequently flawed planning.\nThese perception errors propagate directly to planning. For most small models, current-phase completion accuracy remains below 25%, while for larger models it is consistently lower than next-phase planning accuracy. This cascading failure indicates that step recognition errors directly undermine current-phase completion judgments. Importantly, this should not be interpreted as limited reasoning ability, as larger models perform well on next-phase planning. Rather, the results expose a dominant perception bottleneck: without reliable video-procedure alignment, even strong language backbones fail to support coherent surgical planning, indicating that end-to-end planning from surgical video is constrained primarily by video understanding rather than higher-level reasoning.\n5.5.2\nThe Reasoning Bottleneck: Under-Constrained Planning Space\nTask 2 removes perceptual ambiguity by providing the current step explicitly. Although performance improves relative to Task 1, planning quickly saturates: current-phase completion remains below 52%, and next-phase planning accuracy falls below 5% for small models.\nSemantically plausible but procedurally invalid plans.\nWithout explicit procedural guidance, models default to semantic proximity rather than procedural logic. As a result, generated plans often omit critical steps or assemble loosely related actions that fail to collectively achieve the phase goal.\nFor example, sequences may mix steps from the omentum division phase (e.g., omentum exposure and omental transection) with unrelated steps from gastrojejunal anastomosis, including biliary limb measurement or jejunal opening.\nThis indicates that, although Video-LLMs encode general surgical knowledge, particularly in medical VLMs such as Lingshu and HuluMed, their planning space remains under-constrained. Without explicitly encoded procedural constraints, models fail to produce stable, executable step-level plans, leading to performance saturation even when perceptual uncertainty is removed.\n5.5.3\nKnowledge Injection: Medical Guidance for Planning\nTask\nMetric\nHulu-7B\nHulu-32B\nQwen-7B\nQwen-32B\nDAMO\nLingshu-7B\nLingshu-32B\nLLaVA\nStructural\nCurrent\n42.8%\n66.0%\n63.0%\n73.0%\n43.0%\n46.1%\n71.1%\n23.4%\nNext\n51.2%\n49.2%\n64.3%\n68.7%\n38.0%\n62.6%\n70.5%\n22.9%\nDescription\nCurrent\n26.8%\n46.5%\n14.0%\n44.8%\n26.9%\n18.5%\n49.7%\n14.1%\nNext\n7.7%\n26.7%\n11.5%\n56.7%\n5.5%\n0.9%\n52.8%\n0.1%\nCombined\nCurrent\n36.2%\n67.6%\n55.0%\n69.1%\n38.6%\n34.6%\n70.3%\n19.7%\nNext\n41.7%\n76.5%\n60.8%\n89.2%\n53.8%\n56.5%\n82.1%\n21.3%\nTable 3:\nGoal-satisfiability accuracy (%) of Task 3 under different knowledge injection settings. Current and Next denote current-phase completion and next-phase planning accuracy. Best results are shown in bold.\nTask 3 introduces explicit medical knowledge to constrain planning, revealing how different forms of knowledge affect model behavior.\nStructural Knowledge constrains planning effectively.\nFor most models, structural knowledge (Task 3.1) is the most effective intervention. By explicitly specifying the phase-step hierarchy, procedural structure sharply narrows the space of admissible plans. This leads to large and consistent gains over Task 2 in both current-phase completion and next-phase planning (Table\n3\n), particularly for 7B-scale models. Generated plans exhibit fewer cross-phase intrusions and more reliably satisfy phase-level requirements under goal-satisfiability evaluation.\nFor example, when planning the gastrojejunal anastomosis phase, unrelated steps such as Petersen space exposure or biliary limb opening are more consistently excluded, which were frequently misincorporated in earlier tasks.\nSimilarly, during jejunojejunal anastomosis planning, extraneous steps such as mesenteric defect exposure or mesenteric defect closure are largely eliminated.\nThese results show that concise structural constraints directly align model generation with phase-level goals.\nSemantic descriptions alone cannot enforce procedural correctness.\nIn contrast, semantic descriptions without explicit structure (Task 3.2), as well as the combined setting (Task 3.3), underperform structural guidance alone for 7B-scale models, especially for next-phase planning(Table\n3\n). Although models often express correct surgical intent, they frequently omit critical execution steps needed to complete the phase goal. Rich semantic prompts tend to promote narrative plausibility rather than enforce discrete procedural requirements.\nFor instance, after Petersen space closure, models may correctly identify jejunojejunal anastomosis as the next phase, but still fail to include essential steps such as biliary limb opening or alimentary limb measurement. Under long-context prompts, semantic information is often diluted, leading to shallow or generic plans. Models may also lose global procedural coherence, treating jejunojejunal anastomosis as the final phase and appending steps from later phases (e.g., mesenteric defect closure or cleaning and coagulation) in an unstructured manner.\nThese failures show that semantic guidance alone insufficiently constrains the planning space.\n5.5.4\nModel Capacity Determines Knowledge Integration.\nAcross all settings, larger models outperform their 7B-scale counterparts, reflecting stronger reasoning capacity and more reliable use of long-context inputs. Structural knowledge (Task 3.1) provides a stable benefit across model sizes, consistently improving planning performance relative to Task 2.\nSmall models struggle to combine multiple guidance.\nFor 7B-scale models, combining semantic and structural knowledge (Task 3.3) consistently performs worse than structural constraints alone, particularly for next-phase planning (Table\n3\n). This suggests that limited-capacity models have difficulty integrating heterogeneous information sources, leading to weaker adherence to procedural constraints.\nLarger models better exploit combined knowledge.\nFor 32B-scale models, Task 3.3 achieves the best next-phase planning performance (Table\n3\n). This indicates that sufficient capacity enables models to leverage semantic descriptions to refine intent-level reasoning while still relying on structural constraints to maintain procedural validity. Notably, structural knowledge alone remains highly competitive even at 32B-scale, suggesting that explicit procedural structure benefits goal-oriented surgical planning across model capacities.\nOverall, these results suggest that model capacity governs the ability to integrate multiple forms of knowledge. Semantic enrichment becomes beneficial primarily when sufficient capacity is available to reconcile heterogeneous guidance without compromising procedural validity.\n6\nConclusion\nThis work shows that prevailing formulations and evaluations of surgical planning are misaligned with clinical reality, where planning is hierarchical, goal-oriented, and admits multiple valid execution paths. We introduce a goal-satisfiability-based meta-evaluation benchmark grounded in expert-defined procedural rules to test whether planning metrics align with this setting. Under this benchmark, widely used sequence similarity metrics reject most valid plans, while LLM-based judges, despite strong semantic understanding, frequently fail to enforce critical procedural dependencies. Through progressive evaluation of Video-LLMs, we further show that end-to-end planning is limited by perception bottlenecks and that reasoning remains under-constrained without explicit procedural structure. Injecting structural knowledge provides the most consistent gains, whereas semantic descriptions alone are insufficient and their combination is effective only at larger model scales. Together, these findings motivate a shift from single-trajectory similarity toward goal-satisfiability evaluation as a foundation for developing and interpreting clinically aligned surgical planning models.\nLimitations\nThis study has several limitations that suggest directions for future work.\nFirst, the expert-defined rule-based evaluator relies on manually constructed procedural rules derived from surgical principles and dataset-specific annotation protocols. While this approach provides high precision and interpretability within scope, it does not readily scale to new procedures, institutions, or surgical domains. Automated or semi-automated construction of procedural rules, potentially with LLMs assisting clinicians in formalizing surgical knowledge, remains a challenge.\nSecond, our evaluation treats goal-satisfiability as a binary criterion. Although appropriate for determining whether a plan can plausibly complete a surgical phase, this formulation does not capture finer-grained aspects of planning quality, such as efficiency, redundancy, or preferences among multiple valid procedural paths. Future work could explore more nuanced, goal-aware metrics without reverting to single-path assumptions.\nThird, our experiments are conducted on a limited set of surgical datasets with rich hierarchical annotations. At present, few publicly available datasets provide phase–step hierarchies with sufficient granularity and consistency to support goal-oriented planning analysis. Extending this framework to additional procedures will require broader annotation efforts or alternative forms of weak or implicit supervision.\nReferences\nS. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang,\net al.\n(2025)\nQwen2. 5-vl technical report\n.\narXiv preprint arXiv:2502.13923\n.\nExternal Links:\nLink\nCited by:\n§1\n,\n§5.1\n.\nD. Biagini, N. Navab, and A. Farshad (2025)\nHierasurg: hierarchy-aware diffusion model for surgical video generation\n.\nIn\nInternational Conference on Medical Image Computing and Computer-Assisted Intervention\n,\npp. 310–319\n.\nCited by:\n§2.1\n.\nM. Boels, Y. Liu, P. Dasgupta, A. Granados, and S. Ourselin (2025a)\nSWAG: long-term surgical workflow prediction with generative-based anticipation\n.\nInternational Journal of Computer Assisted Radiology and Surgery\n,\npp. 1–11\n.\nCited by:\n§1\n,\n§2.1\n.\nM. Boels, H. Robertshaw, T. C. Booth, P. Dasgupta, A. Granados, and S. Ourselin (2025b)\nDARIL: when imitation learning outperforms reinforcement learning in surgical action planning\n.\narXiv preprint arXiv:2507.05011\n.\nExternal Links:\nLink\nCited by:\n§1\n.\nA. Z. Broder (1997)\nOn the resemblance and containment of documents\n.\nIn\nProceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)\n,\npp. 21–29\n.\nCited by:\n§4.1\n.\nZ. Chen, X. Luo, J. Wu, L. Bai, Z. Lei, H. Ren, S. Ourselin, and H. Liu (2025)\nSurgplan++: universal surgical phase localization network for online and offline inference\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp. 12782–12788\n.\nCited by:\n§2.1\n.\nJ. Ding, Y. Zhang, Y. Shang, Y. Zhang, Z. Zong, J. Feng, Y. Yuan, H. Su, N. Li, N. Sukiennik,\net al.\n(2025)\nUnderstanding world or predicting future? a comprehensive survey of world models\n.\nACM Computing Surveys\n58\n(\n3\n),\npp. 1–38\n.\nCited by:\n§1\n.\nJ. Fan and P. Zheng (2024)\nA vision-language-guided robotic action planning approach for ambiguity mitigation in human–robot collaborative manufacturing\n.\nJournal of Manufacturing Systems\n74\n,\npp. 1009–1018\n.\nCited by:\n§1\n.\nK. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu,\net al.\n(2022)\nEgo4d: around the world in 3,000 hours of egocentric video\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp. 18995–19012\n.\nCited by:\n§1\n.\nS. Jiang, Y. Wang, S. Song, T. Hu, C. Zhou, B. Pu, Y. Zhang, Z. Yang, Y. Feng, J. T. Zhou,\net al.\n(2025)\nHulu-med: a transparent generalist model towards holistic medical vision-language understanding\n.\narXiv preprint arXiv:2510.08668\n.\nExternal Links:\nLink\nCited by:\n§5.1\n.\nM. G. Kendall (1938)\nA new measure of rank correlation\n.\nBiometrika\n30\n(\n1-2\n),\npp. 81–93\n.\nCited by:\n§4.1\n.\nF. Lalys and P. Jannin (2014)\nSurgical process modelling: a review\n.\nInternational journal of computer assisted radiology and surgery\n9\n(\n3\n),\npp. 495–511\n.\nCited by:\n§2.1\n.\nJ. L. Lavanchy, D. Alapatt, L. Sestini, M. Kraljević, P. C. Nett, D. Mutter, B. P. Müller-Stich, and N. Padoy (2025)\nAnalyzing the impact of surgical technique on intraoperative adverse events in laparoscopic roux-en-y gastric bypass surgery by video-based assessment\n.\nSurgical Endoscopy\n39\n(\n3\n),\npp. 2026–2036\n.\nCited by:\n§2.1\n.\nJ. L. Lavanchy, S. Ramesh, D. Dall’Alba, C. Gonzalez, P. Fiorini, B. P. Müller-Stich, P. C. Nett, J. Marescaux, D. Mutter, and N. Padoy (2024)\nChallenges in multi-centric generalization: phase and step recognition in Roux-en-Y gastric bypass surgery\n.\nInternational Journal of Computer Assisted Radiology and Surgery\n19\n(\n11\n),\npp. 2249–2257\n.\nCited by:\n§1\n,\n§2.1\n,\n§2.1\n,\n§3.1\n.\nF. Li, R. Zhang, H. Zhang, Y. Zhang, B. Li, W. Li, Z. Ma, and C. Li (2024)\nLlava-next-interleave: tackling multi-image, video, and 3d in large multimodal models\n.\narXiv preprint arXiv:2407.07895\n.\nExternal Links:\nLink\nCited by:\n§5.1\n.\nZ. Li, Y. Xie, R. Shao, G. Chen, W. Guan, D. Jiang, and L. Nie (2025a)\nOptimus-3: towards generalist multimodal minecraft agents with scalable task experts\n.\narXiv preprint arXiv:2506.10357\n.\nExternal Links:\nLink\nCited by:\n§1\n.\nZ. Li, Y. Hu, and W. Wang (2025b)\nEncouraging good processes without the need for good answers: reinforcement learning for llm agent planning\n.\nIn\nProceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track\n,\npp. 1654–1666\n.\nCited by:\n§1\n.\nB. Lin, Y. Ye, B. Zhu, J. Cui, M. Ning, P. Jin, and L. Yuan (2024)\nVideo-llava: learning united visual representation by alignment before projection\n.\nIn\nProceedings of the 2024 conference on empirical methods in natural language processing\n,\npp. 5971–5984\n.\nCited by:\n§1\n.\nA. Marzal and E. Vidal (2002)\nComputation of normalized edit distance and applications\n.\nIEEE transactions on pattern analysis and machine intelligence\n15\n(\n9\n),\npp. 926–932\n.\nCited by:\n§4.1\n.\nM. Xu, Z. Huang, D. Imans, Y. Ye, X. Zhang, and Q. Dou (2025a)\nSAP-bench: benchmarking multimodal large language models in surgical action planning\n.\narXiv preprint arXiv:2506.07196\n.\nExternal Links:\nLink\nCited by:\n§1\n,\n§2.1\n.\nM. Xu, Z. Huang, J. Zhang, X. Zhang, and Q. Dou (2025b)\nSurgical action planning with large language models\n.\nIn\nProceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)\n,\npp. 563–572\n.\nCited by:\n§1\n,\n§2.1\n.\nW. Xu, H. P. Chan, L. Li, M. Aljunied, R. Yuan, J. Wang, C. Xiao, G. Chen, C. Liu, Z. Li,\net al.\n(2025c)\nLingshu: a generalist foundation model for unified multimodal medical understanding and reasoning\n.\narXiv preprint arXiv:2506.07044\n.\nExternal Links:\nLink\nCited by:\n§5.1\n.\nB. Zhang, K. Li, Z. Cheng, Z. Hu, Y. Yuan, G. Chen, S. Leng, Y. Jiang, H. Zhang, X. Li,\net al.\n(2025a)\nVideollama 3: frontier multimodal foundation models for image and video understanding\n.\narXiv preprint arXiv:2501.13106\n.\nExternal Links:\nLink\nCited by:\n§1\n.\nB. Zhang, K. Li, Z. Cheng, Z. Hu, Y. Yuan, G. Chen, S. Leng, Y. Jiang, H. Zhang, X. Li,\net al.\n(2025b)\nVideollama 3: frontier multimodal foundation models for image and video understanding\n.\narXiv preprint arXiv:2501.13106\n.\nExternal Links:\nLink\nCited by:\n§5.1\n.\nQ. Zhao, S. Wang, C. Zhang, C. Fu, M. Q. Do, N. Agarwal, K. Lee, and C. Sun (2023)\nAntgpt: can large language models help long-term action anticipation from videos?\n.\narXiv preprint arXiv:2307.16368\n.\nExternal Links:\nLink\nCited by:\n§1\n.",
    "preview_text": "Surgical planning integrates visual perception, long-horizon reasoning, and procedural knowledge, yet it remains unclear whether current evaluation protocols reliably assess vision-language models (VLMs) in safety-critical settings. Motivated by a goal-oriented view of surgical planning, we define planning correctness via phase-goal satisfiability, where plan validity is determined by expert-defined surgical rules. Based on this definition, we introduce a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans containing order and content errors. Using this benchmark, we show that sequence similarity metrics systematically misjudge planning quality, penalizing valid plans while failing to identify invalid ones. We therefore adopt a rule-based goal-satisfiability metric as a high-precision meta-evaluation reference to assess Video-LLMs under progressively constrained settings, revealing failures due to perception errors and under-constrained reasoning. Structural knowledge consistently improves performance, whereas semantic guidance alone is unreliable and benefits larger models only when combined with structural constraints.\n\nSurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability\nRuochen Li\n1\nKun Yuan\n1,2\n1\n1\nfootnotemark:\n1\nYufei Xia\n3\nYue Zhou\n1\nQingyu Lu\n4\nWeihang Li\n1\nYouxiang Zhu\n5\nNassir Navab\n1\n2\n2\nfootnotemark:\n2\n1\nTechnical University of Munich, Germany\n2\nUniversity of Strasbourg, France\n3\nUniversity of Glasgow, United Kingdom\n4\nNanyang Technological University, Singapore\n5\nUniversity of Massachusetts Boston, USA\nEqual contribution.\nJoint supervision.\nAbstract\nSurgical planning integrates visual perception, long-horizon reasoning, and procedural knowledge, yet it remains unclear whether current evaluation protocols reliably assess vision-language models (VLMs) in safety-critical settings.\nMotivated by a goal-oriented view of surgical planning, we define planning correctness via phase-goal satisfiability, ",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "VLM"
    ],
    "one_line_summary": "该论文提出基于目标满足性的手术规划评估方法，重点关注视觉语言模型（VLMs）在安全关键场景下的评估问题。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-15T14:47:26Z",
    "created_at": "2026-01-20T17:49:55.935694",
    "updated_at": "2026-01-20T17:49:55.935705",
    "recommend": 0
}