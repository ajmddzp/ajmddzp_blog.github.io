{
    "id": "2601.08520v1",
    "title": "Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps",
    "authors": [
        "Krzysztof Zielinski",
        "Dominik Belter"
    ],
    "abstract": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå…³é”®å¸§çš„æ–°å‹å»ºå›¾ç³»ç»Ÿã€‚è¯¥æ–¹æ³•åˆ©ç”¨RGB-Dä¼ æ„Ÿå™¨æ•°æ®æ›´æ–°å±€éƒ¨æ­£æ€åˆ†å¸ƒå˜æ¢åœ°å›¾ï¼ˆNDTï¼‰ã€‚NDTå•å…ƒå­˜å‚¨åœ¨äºŒç»´è§†è§’ç›¸å…³ç»“æ„ä¸­ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨RGB-Dç›¸æœºçš„ç‰¹æ€§ä¸ä¸ç¡®å®šæ€§æ¨¡å‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªç„¶åœ°ä»¥æ›´é«˜ç²¾åº¦å‘ˆç°é è¿‘ç›¸æœºåŸç‚¹çš„ç‰©ä½“ã€‚å±€éƒ¨åœ°å›¾å­˜å‚¨äºä½å§¿å›¾ä¸­ï¼Œå¯åœ¨é—­ç¯æ£€æµ‹åä¿®æ­£å…¨å±€åœ°å›¾ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§èåˆä¸è¿‡æ»¤å±€éƒ¨åœ°å›¾ä»¥è·å–ç¯å¢ƒå…¨å±€åœ°å›¾çš„å¤„ç†æµç¨‹ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æœ¬æ–¹æ³•ä¸OctomapåŠNDT-OMè¿›è¡Œå¯¹æ¯”ï¼Œå¹¶å±•ç¤ºäº†æ‰€æå»ºå›¾æ–¹æ³•çš„åº”ç”¨å®ä¾‹ã€‚",
    "url": "https://arxiv.org/abs/2601.08520v1",
    "html_url": "https://arxiv.org/html/2601.08520v1",
    "html_content": "Keyframe-based Dense Mapping with the Graph\nof View-Dependent Local Maps\nKrzysztof ZieliÅ„ski\n1\n, Dominik Belter\n1\n*This work was supported by the National Centre for Research and Development (NCBR) through project LIDER/33/0176/L-8/16/NCBR/2017.\n1\nInstitute of Control, Robotics and Information Engineering, Poznan University of Technology, Poznan, Poland\ndominik.belter@put.poznan.pl\nAbstract\nIn this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.\nÂ©2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nI\nIntroduction\nAutonomous robots use maps to model the environment. The model of the environment allows collision detection, motion planning, and localization. The map can also store information about the objects\n[\n1\n]\nand geometrical primitives\n[\n2\n]\nwhich can be found in the workspace of the robot. Feature-based SLAM systems like\n[\n3\n]\nstore a sparse set of point features in the map. The point features are later used to re-localize the camera by matching features from the current camera image\n[\n3\n]\n. In contrast, dense mapping methods like Octomap\n[\n4\n]\nor Normal Distribution Transform Occupancy Map (NDT-OM)\n[\n5\n]\nare focused on the global reconstruction of the objects and obstacles in the environment for collision detection and motion planning. In this research, we propose a new dense mapping method which is based on data structures used mainly by the sparse localization methods like ORB-SLAM2\n[\n3\n]\n.\nFigure 1:\nLocal map of the environment: normal distribution transforms (ellipsoids) projected on the image plane.\nThe most popular methods to 3D dense mapping are Octomap\n[\n4\n]\nand NDT-OM\n[\n5\n]\n. Both methods build a global map that divides the space into cells (voxels) and determines their occupancy. The size of the cells has to be determined in advance. It means that we can represent objects in the map with the given accuracy. The accuracy of the map depends on the tasks and itâ€™s different during grasping objects and planning the collision-free motion of the robot in the building. Also, decreasing the size of the cell increases significantly the number of cells. Moreover, the regular division of the space does not take into account the properties of the sensors (uncertainty model) during the update of the map and we lose precise information about the shape of the objects which are close to the robot. Finally, the sensor data is integrated into the global map taking into account current information about the pose of the robot. Whenever the robot re-localizes itself it corrects current and previous robotâ€™s poses\n[\n3\n]\n. However, with the global map, the dense measurements cannot be corrected because they are already integrated into the dense map.\nIn this paper, we propose a dense mapping method for static environments which is inspired by recent advances in robot localization and ORB-SLAM method. Instead of building a global map of the environment, we create a set of local maps (Fig.\n1\n) which are related to the unique views from the camera (keyframes). We integrate depth measurements from the RGB-D sensor in the camera frame which allows representing objects closer to the camera with higher precision than distant objects. Despite the constant resolution in the local frame, we can represent the same regions of the 3D space with various precision and we donâ€™t lose information about details of the objects. Finally, we can generate a global map of the environment and correct the map after loop closure.\nFigure 2:\nBlock diagram of the view-dependent keyframe-based mapping method\nI-A\nRelated Work\nThe first implementations of the dense maps in robotics are 2.5D elevation maps\n[\n6\n,\n7\n]\n. The terrain surface is obtained using the locus method\n[\n7\n]\n. Moreover, data from multiple sensors are used to localize the robot using terrain matching. Recently, we presented that the 2.5D elevation map can be estimated using Kalman filtering to improve the accuracy of the map\n[\n8\n]\n. Fankhauser et al. suggest that the robot should be in the center of the local map which moves with the robot\n[\n9\n]\n. This approach is justified by a growing uncertainty of the robotâ€™s position and decreasing the accuracy of objects which are far from the sensor. To tackle the problem of objects that are above the robot (such as tunnels) and avoid the complexity of 3D maps, extended multi-level elevation maps have been proposed by Pfaff et al.\n[\n10\n]\n.\nOne of the most popular approaches to 3D mapping of the environment in robotics is Octomap\n[\n4\n]\n. Octomap stores occupied, free, and unknown spaces in the octree data structure. It is a memory-efficient method and guarantees logarithmic access time to each cell. Another algorithm that is based on Octree, updates corresponding parts of triangle-mesh\n[\n11\n]\n. This approach directly deals with the problem of representing the scene at multiple scales. Another approach to 3D mapping called multi-volume occupancy grid (MVOG) which explicitly stores information about the obstacles and free space reduces memory usage\n[\n12\n]\n. Recently, dense 3D reconstruction and pose estimation\n[\n13\n]\nbecame computationally efficient. Also, multi-resolution maps consisting of surfels can be processed on-line using CPU only which is important for the most robotic applications\n[\n14\n]\n.\nPlagemann et al.\n[\n15\n]\nsuggested a new mapping approach as a regression problem using the Gaussian process. This terrain model has the advantage of predicting elevations at unseen locations more reliably than alternative approaches. Later, a similar approach was suggested by Saarinen et al.\n[\n5\n]\n. The proposed Normal Distribution Transform combines the advantages of both representations â€“ compactness of NDT-maps and robustness of occupancy maps. We also utilize this approach (NDT) to update 3D local maps.\nLocal maps (submap) are popular for the 2D models of the environment. This idea is implemented in the Google Cartographer SLAM\n[\n16\n]\n. The submaps are represented by probability grids connected by edges in the graph and later used for loop-closure. A similar graph-based approach was presented by Strom et al.\n[\n17\n]\nby rasterizing a dynamic occupancy grid. In 3D space, local maps and the concept of loop-closure was used in\n[\n18\n]\n. In contrast, ElasticFusion uses a dense surfel-based model obtained by dense frame-to-frame camera tracking without graph optimization\n[\n19\n]\n. Also, Ho et al. use Virtual Occupancy Grid Maps to correct global map after loop closure detection\n[\n20\n]\n. In this paper, we show the advantages of building 3D local maps and storing them in the pose-based graph. In\n[\n21\n]\n, a similar approach is utilized but we use view-dependent representation as a local map which is memory efficient and better represents the uncertainty of the perception system.\nI-B\nApproach and Contribution\nIn this paper, we use a 2D view-dependent approach to generate a dense model of the environment. As a result, the robot has a set of keyframes stored in the pose graph. Each keyframe contains information about the local model of the environment. The dense model is based on the NDT-OM but instead of dividing 3D space into voxels with the given size, we use the image plane to obtain 3D occupancy ellipsoids (normal distribution transforms). The discretization is performed on the image plane (Fig.\n1\n).\nThe proposed structure of the map allows for correction of the global map after loop closure which is presented in our previous work\n[\n18\n]\n. In this research, we show that the proposed view-dependent model of the environment is more efficient than the 3D model (NDT-OM or Octomap) because it adapts the size of the cells of the map to the distance between the sensor (RGB-D camera) and the objects in the environment. Thus, the objects which are closer to the robot are represented in the map with higher precision than distant objects. Finally, the graph of local view-dependent dense maps allows restoring the global model of the environment.\nII\nView-dependent Mapping\nII-A\nGlobal graph-based map\nThe structure of the map and update procedure are presented in Fig.\n2\n. The global map consists of a set of local maps (keyframes)\nk\ni\nk_{i}\n. The local maps are organized in a graph-like structure. The pose of each local map w.r.t. first keyframe is represented by the\nSE3\ntransformation\nğŠ\ni\n{\\bf K}_{i}\n. We also add edges that represent co-visibility between keyframes\nÎ´\ni\n,\nj\n\\delta_{\\rm i,j}\n. The structure is inspired by the covisibility graph in the ORB-SLAM2\n[\n3\n]\n. The weight of the edge represents (covisibility\nÎ´\n\\delta\n) the number of re-observed 2D features in the keyframes normalized by the total number of 2D features.\nThe update procedure of the map is presented in Fig.\n2\n. We provide the pair of the RGB-D images to the input of the system to update the map. We assume that the camera pose is known and obtained from the independent localization system. First, we check the covisibility between the current RGB frame and the current keyframe\nk\ni\nk_{i}\n. If the covisiblility\nÎ´\n\\delta\nis above the threshold we update the current keyframe\nk\ni\nk_{i}\n. In other cases, we iterate over all keyframes in the map to detect loop closure. If the covisibility\nÎ´\n\\delta\nis above the threshold and we re-observe the scene which is stored in the map we update the keyframe which exists in the map\nk\ni\nâˆ’\nN\nk_{\\rm i-N}\n. We create a new keyframe if the loop closure detection procedure does not recognize the current scene.\nII-B\nLocal maps - keyframes\nOpposed to other methods where 3D points are updated in 3D space, this method allows updating the data on a two-dimensional grid. First of all, we introduce a keyframe that consists of RGB-D pair of images, the 3D pose of the keyframe, a set of point features detected on the RGB image and a 2D container with ellipsoids (Fig.\n1\n). Each ellipsoid is described by the 3D covariance matrix, 3D position (\n[\nx\n,\ny\n,\nz\n]\nT\n[x,y,z]^{T}\n), and color. The resolution of the container depends on the number of pixels used to update the ellipsoids in a single cell. In the experiments presented in the article, the size of each cell is set to 5\nÃ—\n\\times\n5Â px. Each 2D cell and position of the corresponding ellipsoid is updated using the RGB-D images and a color point cloud re-projected on the keyframe. The size of the 2D container is set to cover 1280\nÃ—\n\\times\n960Â px image. The RGB-D frame related to the keyframe is located in the center of the 2D container. Additionally, we show that the keyframe can contain information about objects detected on the RGB-D images.\nII-C\nUpdate of the local map\nThe color point cloud obtained from the RGB-D images and transformed to the pose of the considered keyframe is used to update the local map. Then, we use NDT update methodology\n[\n5\n]\nto obtain the position, color, and covariance matrix describing an ellipsoid. First, we compute a transformation matrix\nğ“\n{\\bf T}\nbetween the keyframe and current camera pose\nğ“\n=\nğŠ\n0\nâˆ’\n1\nâ‹…\nğŠ\ni\n,\n{\\bf T}={\\bf K}_{0}^{-1}\\cdot{\\bf K}_{i},\n(1)\nwhere\nğŠ\n0\n{\\bf K}_{0}\nis the global pose of the keyframe, and\nğŠ\ni\n{\\bf K}_{i}\nis the global pose of the\ni\ni\n-th RGB-D frame from the camera. We use the obtained transformation\nğ“\n{\\bf T}\nto compute position of points\nP\n=\n{\nğ©\n1\n,\nâ€¦\n,\nğ©\nN\n}\nP=\\{{\\bf{p}}_{1},...,{\\bf{p}}_{N}\\}\nin the keyframe coordinate system\nğŠ\n0\n{\\bf K}_{0}\nğ©\n0\n=\nğ“\nâ‹…\nğ©\nn\n,\n{\\bf p}^{0}={\\bf T}\\cdot{\\bf p}_{n},\n(2)\nwhere\nğ©\n0\n{\\bf p}^{0}\nis the position of the\nn\nn\n-th point in the keyframe coordinate system\nK\n0\n{K}_{0}\nand\nğ©\nn\n{\\bf p}_{n}\nis the position of the point in the current camera frame.\nLater, an inverse model of the camera is used to compute the position of the considered point on the 2D container related to the keyframe. Then, points are grouped according to the\nu\nu\nand\nv\nv\ncoordinates. Each group is used to update the ellipsoid related to cell\nc\nu\n,\nv\nc_{u,v}\nin the 2D keyframe container. We use NDT update method\n[\n5\n]\nto update position of the ellipsoid\nğ±\nu\n,\nv\n{\\bf x}_{u,v}\nand the covariance matrix\nğšº\nu\n,\nv\n{\\bf\\Sigma}_{u,v}\n. The\nN\nN\nnew points added to the cell and current mean value\nğ±\nt\n{\\bf x}_{t}\ncomputed from total\nM\nM\npoints are used to update the mean position of the ellipsoid\nğ±\nt\n+\n1\n{\\bf x}_{t+1}\n:\nğ±\nt\n+\n1\n=\nğ±\nt\nâ€‹\nM\n+\nâˆ‘\nn\n=\n1\nN\nğ©\nn\nM\n+\nN\n.\n{\\bf x}_{t+1}=\\frac{{\\bf x}_{t}M+\\sum_{n=1}^{N}{\\bf p}_{n}}{M+N}.\n(3)\nThe update procedure is repeated whenever a new point cloud from the sensor is provided. The iterative procedure requires the computation of the sum\nğ’”\nM\n=\nâˆ‘\nm\n=\n1\nM\nğ’‘\nm\n\\boldsymbol{s}_{M}=\\sum_{m=1}^{M}\\boldsymbol{p}_{m}\nand the total number of points M used to update the considered cell (ellipsoid). We update the covariance matrix using:\nğšº\nt\n+\n1\n=\nğšº\nt\n+\n1\n+\nâˆ‘\nn\n=\n1\nN\n(\nğ©\nn\nâˆ’\nğ¬\nN\nN\n)\nâ‹…\n(\nğ©\nn\nâˆ’\nğ¬\nN\nN\n)\nâ€²\n+\nM\nN\nâ€‹\n(\nM\n+\nN\n)\nâ€‹\n(\nN\nM\nâ€‹\nğ¬\nM\nâˆ’\nğ¬\nN\n)\nâ€‹\n(\nN\nM\nâ€‹\nğ¬\nM\nâˆ’\nğ¬\nN\n)\nâ€²\n,\n{\\bf\\Sigma}_{t+1}={\\bf\\Sigma}_{t+1}+\\sum_{n=1}^{N}\\left({\\bf p}_{n}-\\frac{{\\bf s}_{N}}{N}\\right)\\cdot\\left({\\bf p}_{n}-\\frac{{\\bf s}_{N}}{N}\\right)^{\\prime}+\\\\\n\\frac{M}{N(M+N)}\\left(\\frac{N}{M}{\\bf s}_{M}-{\\bf s}_{N}\\right)\\left(\\frac{N}{M}{\\bf s}_{M}-{\\bf s}_{N}\\right)^{\\prime},\n(4)\nwhere\nğ¬\nN\n=\nâˆ‘\nn\n=\n1\nN\nğ©\nn\n{\\bf s}_{N}=\\sum_{n=1}^{N}{\\bf p}_{n}\nand N is the size of point cloud used to update the voxel. To update the color of the voxel, we use:\nğœ\nt\n+\n1\n\\displaystyle{\\bf c}_{t+1}\n=\n\\displaystyle=\nM\nâ‹…\nğœ\nt\n+\nğ¬\nc\nN\nM\n+\nN\n,\n\\displaystyle\\frac{M\\cdot{\\bf c}_{t}+{\\bf s}_{c_{N}}}{M+N},\n(5)\nwhere\nğ¬\nc\nN\n=\nâˆ‘\nn\n=\n1\nN\nğœ\nn\n{\\bf s}_{c_{N}}=\\sum_{n=1}^{N}{\\bf c}_{n}\nis the sum of colors for points P used to update voxel,\nğ’„\nt\n\\boldsymbol{c}_{t}\nis the current mean color of the voxel.\nFigure 3:\nExample local map generated using, consecutively, 1, 2 and 10 frames, obtained for the\nfreiburg1_desk\nsequence from the TUM dataset\n[\n22\n]\n: 2D container with ellipsoids (a,b,c) and the enlarged region of the local map (d,e,f)\nAfter each cell update, the total number of points used to update single ellipsoid is increased\nM\n=\nM\n+\nN\nM=M+N\n. In Fig.\n3\nwe show the set of ellipsoids stored in the 2D container after 1, 2, and 10 update iterations. In the first iteration, the ellipsoids represent the real shape of the objects. The larger size of the ellipsoids naturally represents the larger uncertainty of the measurements. In the following iterations the shape described by the ellipsoids becomes more smooth (compare enlarged region in Fig.\n3\nafter 1 and 10 iterations).\nII-D\nEllipsoids filtering\nTo remove incorrect ellipsoids that appear on the edges of the objects we use the uncertainty model of the RGB-D sensor. Because the ellipsoids computed using NDT updating procedure are closely related to the uncertainty model of the RGB-D sensor\n[\n23\n]\nwe compare directly the model with the obtained ellipsoids. The filtering procedure is based on rejecting ellipsoids that are too long in 3D space according to the uncertainty characteristic of the RGB-D sensor. Ellipsoids with center points close to the camera origin should have a much smaller magnitude than those further away according to the sensor model. For results obtained from various datasets, we use the same generic uncertainty model of the Kinect sensor\n[\n24\n]\n.\nII-E\nMerging local maps\nTo compute the global map of the environment we have to compute the position of each ellipsoid from the local map in the global coordinate frame. To this end, we iterate over each local map\nk\ni\nk_{i}\nand compute the global pose of each ellipsoid. With the given set of ellipsoids, we can easily obtain the global point cloud and global Octomap by sampling from the set of ellipsoids\n[\n18\n]\n. However, in contrast to our previous approach where local maps have the same resolution in the 3D space, we store local maps with different 3D accuracy. Thus, ellipsoids from different local maps which describe the same region of the 3D space have various size. We propose a procedure that merges ellipsoids from various viewpoints, to merge the information from the local maps.\na\nb\nc\nd\nFigure 4:\nMerging two local maps (a,b): maps in the common coordinate frame (c) and the obtained set of ellipsoids (d).\nFirstly, we merge ellipsoids from neighboring local maps. The example of merging two neighboring local maps is presented in Fig.\n4\n. The local maps which are connected with the edges in the covisibility graph cover the same region in the 3D space. In Fig.\n4\nc we show how the local maps overlap. We merge ellipsoids that are close to each other in the 3D space. However, clustering in the 3D space is computationally expensive and time-consuming. Instead, we divide the local maps into the groups of neighboring maps. Then, we re-project all ellipsoids from neighboring maps to the map in the center of the group using inverse pinhole camera model\n[\n23\n]\n:\n[\nu\nv\nd\n]\n\\displaystyle\\begin{bmatrix}u\\\\\nv\\\\\nd\\\\\n\\end{bmatrix}\n=\n[\nx\nâ‹…\nf\nx\nz\n+\nx\nc\ny\nâ‹…\nf\ny\nz\n+\ny\nc\nz\n]\n,\n\\displaystyle=\\begin{bmatrix}\\frac{x\\cdot f_{x}}{z}+x_{c}\\\\\n\\frac{y\\cdot f_{y}}{z}+y_{c}\\\\\nz\\\\\n\\end{bmatrix},\n(6)\nwhere\n[\nu\n,\nv\n,\nd\n]\nT\n[u,v,d]^{T}\nis the position of the ellipsoid on the new image plane,\n[\nx\n,\ny\n,\nz\n]\nT\n[x,y,z]^{T}\nis the position of the ellipsoid in the camera frame,\nx\nc\nx_{c}\n,\ny\nc\ny_{c}\ndefine the position of the optical axis on image plane,\nf\nx\nf_{x}\nand\nf\ny\nf_{y}\nare focal lengths of the camera. Then, ellipsoids which are located in the same 2D cell\nc\nu\n,\nv\nc_{u,v}\nare clustered using mean shift clustering. We do not cluster if the distance between ellipsoids is larger than the threshold (we set the threshold value to 0.25Â m in all experiments presented in the paper). For all ellipsoids in the same cluster, we compute the mean position, covariance matrix, and the color. The results of the clustering are presented in Fig.\n4\nd. By applying the proposed procedure we reduce significantly the number of ellipsoids and smooth the global map.\na\nb\nFigure 5:\nOccluding ellipsoids removal procedure which allows keeping ellipsoids which represent objects with the highest precision: before (a) and after filtering (b)\nSecondly, we have to deal with the various resolution of the local maps in the 3D space. The same object can be observed from various viewpoints and included in multiple local maps. Each local map produces ellipsoids with various sizes in the 3D space. If the object is observed with various precision, our goal is to keep the most precise model. To this end, we remove ellipsoids which occlude other ellipsoids that have smaller uncertainty. We use the procedure which is similar to the procedure used to merge ellipsoids. Again, we create groups of local maps which are neighbors in the covisibility graphs but also we add local maps in which Euclidean distance and the angle between camera axes are below the threshold. We check if each ellipsoid from the local map is occluded by other ellipsoids for the given viewpoint. We can perform this operation efficiently because we apply the computationally efficient forward and inverse camera model and we store information about 2D coordinates of each ellipsoid on the image. For the map with 100 local maps, the filtering procedure takes less than 5Â s. Example results of the occluding removal procedure are presented in Fig.\n5\n.\nIII\nResults\nFigure 6:\nLocal map obtained for the\nfreiburg2_desk\nsequence from the TUM dataset\n[\n22\n]\n. The enlarged regions of the map are in the red and blue frames.\nWe performed experiments on various datasets to show the properties of the proposed method\n1\n1\n1\nshort video from experiments is available at\nhttps://youtu.be/uNEZbMYmRq4\n. In the first experiment, we present the example keyframe (local map) of the\nfreiburg3_desk\nsequence from the TUM dataset\n[\n22\n]\n. In Fig.\n6\nwe show the set of ellipsoids stored in the 2D container. We also enlarge the selected regions of the local map to show the properties of the map. The ellipsoids which are closer to the camera are much smaller than the distant ones. On the other hand, the ellipsoids located on the edges of the objects (a cup, coke, and mouse in Fig.\n6\n) are elongated due to measurements (depth values) along the\nz\nz\naxis of the camera. Fig.\n6\nalso show that the local map contains information about the object from a single viewpoint. The orientation and position of the camera do not change significantly which allows us to integrate 3D measurements on the 2D plane.\nFigure 7:\nGlobal map obtained for the\nfr3_long\nsequence from the TUM dataset\n[\n22\n]\n. The enlarged regions of the map are in the red and blue frames.\nIn the second experiment, we show the global map obtained for the\nfr3_long\nsequence from the TUM dataset\n[\n22\n]\n. The results are presented in Fig.\n7\n. We show the global set of ellipsoids. We applied the proposed methods of merging local maps and removing occluding ellipsoids. Finally, the global map contains 358258 ellipsoids which is less than the number of pixels in a single RGB image. In Fig.\n7\nwe also show the enlarged regions of the global map. Even though the global map is built using local maps with various 3D precision we can integrate all measurements and restore detailed information about objects.\nTABLE I:\nParameters of the maps obtained on the\nfr3_long_office\n,\nfreiburg1_room\n, and\nfr2_desk\nsequences from the TUM dataset. We compare the total number of ellipsoids in the local maps\nÎ£\ne\n\\Sigma_{e}\n, ellipsoids after clustering and filtering\nÎ£\ne\nâ€²\n\\Sigma_{e}^{\\prime}\n, and time required to generate global map\nt\ng\nt_{g}\nsequence\nfr3_long\nfr1_room\nfr2_desk\nsubmaps no.\n161\n88\n99\nÎ£\ne\n\\Sigma_{e}\n892232\n405137\n390435\nÎ£\ne\nâ€²\n\\Sigma_{e}^{\\prime}\n358258\n189434\n90719\nt\ng\nt_{g}\n[s]\n14.2\n4.8\n3.9\nWe summarize the properties of the global maps obtained on the three sequences from the TUM dataset in Tab.\nI\nobtained on the computer with i5-8250U CPU. The global map presented in Fig.\n7\ncontains initially 892232 ellipsoids. After clustering and filtering, this number is reduced more than two times. A similar reduction rate is obtained for other sequences. The maximal computation time for the global map is 14.2Â s which is obtained for the graph with 161 local maps. This time depends also on the total number of the ellipsoids stored in the local maps. For the smaller maps (\nfr2_desk\n) the global map is produced in less than 4Â s.\nTABLE II:\nComparison of OctoMap, NDT-OM and VD where\nd\nd\n- voxel size [m],\nt\nu\nt_{u}\n- update time [\nm\nâ€‹\ns\nms\n],\nÎ£\nv\n\\Sigma_{v}\n- no. of updated voxels/3D ellipsoids, 1 - OctoMap, 2 - NDT-OM, 3 - VD.\nseq.\nfr3_long\nfr1_room\nfr2_desk\n1\n2\n3\n1\n2\n3\n1\n2\n3\nd\nâ€‹\n[\nm\n]\nd[m]\n0.1\n0.1\n-\n0.1\n0.1\n-\n0.1\n0.1\n-\nt\nu\nâ€‹\n[\nm\nâ€‹\ns\n]\nt_{u}[ms]\n381\n2275\n1706\n203\n2057\n1823\n263\n2182\n1693\nÎ£\nv\n\\Sigma_{v}\n73404\n2255\n358258\n46967\n637\n189434\n74527\n1998\n90719\na\nb\nc\nd\nFigure 8:\nGlobal maps obtained for the\nfreiburg1_room\nsequence from the TUM dataset\n[\n22\n]\n: example RGB image (a), Octomap (b), NDT-OM (c) and our approach (d)\nIn the next experiment, we compare visually three different mapping methods: OctoMap\n[\n4\n]\n, NDT-OM, and view-dependent approach. The used sequence here is\nfreiburg1_room\nfrom the TUM dataset\n[\n22\n]\n. In Fig.\n8\na, we show the experimental set. The obtained environment models are presented in Fig.\n8\nb-d for the Octomap, NDT-OM and our approach, respectively. The Octomap and NDT-OM are generated for the voxel size equal to 0.1Â m. Further reduction of the voxel size significantly increases the number of updated voxels and computation time. However, the shape of the objects is better represented by NDT-OM than Octomap. Unfortunately, this also result in higher computational cost (Tab.\nII\n). Our approach, presented in\nFigureÂ 8\nd, provides a 3D map with varying resolution. The details of the objects included in the map are much better visible. In Tab.\nII\nwe compare the parameters of the global maps obtained using Octomap, NDT-OM and the proposed approach.\nTABLE III:\nComparison of OctoMap and proposed VD approach in the reconstruction task.\nOctomap\nVD\n0.1Â m\n0.05Â m\n0.02Â m\n3\nÃ—\n\\times\n3Â px\n9\nÃ—\n\\times\n9Â px\n15\nÃ—\n\\times\n15Â px\nR\nâ€‹\nM\nâ€‹\nS\nâ€‹\nE\nâ€‹\n[\nm\nâ€‹\nm\n]\nRMSE[mm]\n80.0\n72.8\n48.1\n9.7\n9.9\n12.5\nÎ£\nv\n\\Sigma_{v}\n20967\n115441\n1015243\n615206\n63605\n27081\nThe comparison between OctoMap\n[\n4\n]\nand the proposed method in the reconstruction task is presented in Tab.\nIII\n. The model of the environment is obtained on the ICL-NUIM\nliving_room\ndataset. We compare the obtained model with the reference mesh model. To this end, we create a point cloud from the OctoMap and 3D ellipsoids by taking the centers of these geometric structures. As seen in Tab\nIII\n, when the size of the voxel is decreased, we improve the model of the environment but the number of voxels significantly increases (to 1015243 when the voxel size is 0.02Â m). The RMSE for the OctoMap with 0.02Â m voxel size is relatively high (48.1 mm). Further decreasing the voxel size increase significantly the memory consumption. The reconstruction error is much smaller when the proposed view-dependent model is used. Even when the cell size is set to 15\nÃ—\n\\times\n15Â px the RMSE is 12.5Â mm and the number of ellipsoids is 27081 only.\nIV\nConclusions and Future Work\nIn this article, we present the mapping method which is based on the set of view-dependent local maps connected by the edges in the pose graph. The local maps allow integrating measurements from the RGB-D frames which are close to the pose of the keyframe. The obtained local model represents precisely objects which are close to the camera pose. The distant objects are represented in the local map with lower accuracy. This approach is closely related to the uncertainty model of the 3D sensors. With the proposed approach we do not lose information about objects which are smaller than the given threshold like the voxel size in the Octomap and NDT-OM mapping methods. Moreover, we can correct the global map when the loop closure is detected\n[\n18\n]\n.\nIn this research, we also compare the proposed approach with the Octomap and NDT-OM. We generate a global map to show the properties and capabilities of the method to represent the environment with various resolutions. However, we are focused on the application of local maps. The local representation of the environment is sufficient to plan the motion of the arm of the mobile manipulation robot. The local map can be also used to detect objects and localize the robot by matching the current data from the sensor to the set of local maps (keyframes) like it is implemented in the ORB-SLAM2\n[\n3\n]\n. In the future, we are going to integrate ORB-SLAM2 with our keyframe-based dense mapping method and share directly data structures between the localization system and dense mapping method. We are also going to work on the simultaneous integration of the local maps in the global map and camera pose estimation.\nReferences\n[1]\nD. Galvez-Lopez, M. Salas, J.D. Tardos, J.M.M. Montiel, Real-time monocular object SLAM, Robotics and Autonomous Systems, Vol.Â 75, pp.Â 435â€“449, 2016\n[2]\nJ. Wietrzykowski, P. SkrzypczyÅ„ski, PlaneLoc: Probabilistic global localization in 3-D using local planar features, Robotics and Autonomous Systems, Vol.Â 113, pp.Â 160â€“173, 2019\n[3]\nR. Mur-Artal, J.D TardÃ³s, ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras, IEEE Transactions on Robotics, Vol.Â 33(5), pp.Â 1255â€“1262, 2017\n[4]\nA. Hornung, K. Wurm, M. Bennewitz, C. Stachniss, W. Burgard, OctoMap: An efficient probabilistic 3D mapping framework based on octrees, Autonomous Robots, Vol. 34(3), pp.Â 189â€“206, 2013\n[5]\nJ. Saarinen, H. Andreasson, T. Stoyanov, A.J. Lilienthal, 3D normal distributions transform occupancy maps: An efficient representation for mapping in dynamic environments, International Journal of Robotics Research, Vol. 32(14), pp.Â 1627â€“1644, 2013\n[6]\nM. Hebert, C. Caillas, E. Krotkov, I. Kweon, Terrain mapping for\na roving planetary explorer, Proceedings of the IEEE International Conference on Robotics and Automation, Scottsdale, USA, pp.Â 997â€“1002, 1989\n[7]\nI. Kweon, T. Kanade, High-resolution terrain map from multiple sensor data, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 14(2), pp.Â 278â€“292, 1992\n[8]\nD. Belter, P. Labecki, P. Fankhauser, R. Siegwart, RGB-D terrain perception and dense mapping for legged robots, International Journal of Applied Mathematics and Computer Science, Vol. 26(1), pp.Â 81â€“97, 2016\n[9]\nP. Fankhauser, M. Bloesch, C. Gehring, M. Hutter, R. Siegwart, Robot- centric elevation mapping with uncertainty estimates, International Conference on Climbing and Walking Robots (CLAWAR), PoznaÅ„, Poland, pp.Â 433â€“440, 2014\n[10]\nP. Pfaff, R. Triebel, W. Burgard, An efficient extension to elevation maps for outdoor terrain mapping and loop closing, International Journal of Robotics Research, Vol. 26(2), pp.Â 217â€“230, 2007\n[11]\nF. Steinbruecker and J. Sturm and D. Cremers, Volumetric 3D Mapping in Real-Time on a CPU, Proceedings of the IEEE International Conference on Robotics and Automation, Hongkong, China, pp.Â 2021â€“2028, 2014\n[12]\nI. Dryanovski, W. Morris, J. Xiao, Multi-volume occupancy grids: An efficient probabilistic 3D mapping model for micro aerial vehicles, Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, Taipei, Taiwan, pp.Â 1553â€“1559, 2010\n[13]\nA. Dai, M. NieÃŸner, M. ZollhÃ¶fer, S. Izadi, C. Theobalt, BundleFusion: Real-Time Globally Consistent 3D Reconstruction Using On-the-Fly Surface Reintegration, ACM Transactions on Graphics, Vol.Â 36(3), 2017\n[14]\nJ. StÃ¼ckler, S. Behnke, Multi-Resolution Surfel Maps for Efficient Dense 3D Modeling and Tracking, Journal of Visual Communication and Image Representation, Vol.Â 25(1), pp.Â 137â€“147, 2014\n[15]\nC. Plagemann, S. Mischke, S. Prentice, K. Kersting, N. Roy, W. Burgard, Learning predictive terrain models for legged robot locomotion, Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, Nice, France, pp.Â 3545â€“3552, 2008\n[16]\nW. Hess, D. Kohler, H. Rapp, D. Andor, Real-Time Loop Closure in 2D LIDAR SLAM, 2016 IEEE International Conference on Robotics and Automation (ICRA), pp.Â 1271â€“1278, 2016\n[17]\nJ. Strom and E. Olson, Occupancy grid rasterization in large environments for teams of robots, Proceedings of the IEEE International Conference on Intelligent Robots and Systems, pp.Â 4271â€“4276, 2011\n[18]\nD. Belter, K. Piaskowski and R. Staszak, Keyframe-based Local Normal Distribution Transform Occupancy Maps for Environment Mapping, 2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA), Turin, 2018, pp.Â 706â€“712.\n[19]\nT. Whelan, S. Leutenegger, R.F. Salas-Moreno, B. Glocker, A. J. Davison, ElasticFusion: Dense SLAM Without A Pose Graph, Robotics: Science and Systems, 2015\n[20]\nB.-J.Ho, P. Sodhi, P. Teixeira, M. Hsiao, T. Kusnur, M. Kaess, Virtual Occupancy Grid Map for Submap-based Pose Graph SLAM and Planning in 3D Environments, IEEE/RSJ International Conference on Intelligent Robots and Systems, pp.Â 2175â€“2182, 2018\n[21]\nM. Meilland and A. I. Comport, On unifying key-frame and voxel-based dense visual SLAM at large scales, IEEE/RSJ International Conference on Intelligent Robots and Systems, Tokyo, pp.Â 3677â€“3683, 2013\n[22]\nJ. Sturm, N. Engelhard, F. Endres, W. Burgard, D. Cremers, A Benchmark for the Evaluation of RGB-D SLAM Systems, Proc. of the International Conference on Intelligent Robot Systems, pp.Â 573â€“580, 2012\n[23]\nD. Belter, M. Nowicki, P. SkrzypczyÅ„ski, Modeling spatial uncertainty of point features in feature-based RGB-D SLAM, Machine Vision and Applications, Vol. 29(5), pp. 827-844, 2018\n[24]\nG. Halmetschlager-Funek, M. Suchi, M. Kampel, M. Vincze, An Empirical Evaluation of Ten Depth Cameras, IEEE Robotics and Automation Magazine, Vol.Â 26(1), pp.Â 67â€“77, 2018\n[25]\nJ. Pan, S. Chitta, D. Manocha, FCL: A general purpose\nlibrary for collision and proximity queries, IEEE International Conference on Robotics and Automation, pp. 3859â€“3866, 2012",
    "preview_text": "In this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.\n\nKeyframe-based Dense Mapping with the Graph\nof View-Dependent Local Maps\nKrzysztof ZieliÅ„ski\n1\n, Dominik Belter\n1\n*This work was supported by the National Centre for Research and Development (NCBR) through project LIDER/33/0176/L-8/16/NCBR/2017.\n1\nInstitute of Control, Robotics and Information Engineering, Poznan University of Technology, Poznan, Poland\ndominik.belter@put.poznan.pl\nAbstract\nIn this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.\nÂ©2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other",
    "is_relevant": false,
    "relevance_score": 0.0,
    "extracted_keywords": [
        "keyframe-based mapping",
        "NDT",
        "RGB-D sensor",
        "pose graph",
        "loop closure",
        "global map"
    ],
    "one_line_summary": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå…³é”®å¸§çš„å¯†é›†åœ°å›¾æ„å»ºæ–¹æ³•ï¼Œä½¿ç”¨RGB-Dä¼ æ„Ÿå™¨æ›´æ–°å±€éƒ¨NDTåœ°å›¾ï¼Œå¹¶é€šè¿‡å§¿æ€å›¾ä¼˜åŒ–å…¨å±€åœ°å›¾ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-13T13:02:22Z",
    "created_at": "2026-01-20T17:49:44.787813",
    "updated_at": "2026-01-20T17:49:44.787831"
}