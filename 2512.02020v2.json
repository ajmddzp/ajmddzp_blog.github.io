{
  "id": "2512.02020v2",
  "title": "EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI",
  "authors": [
    "Jianlei Chang",
    "Ruofeng Mei",
    "Wei Ke",
    "Xiangyu Xu"
  ],
  "abstract": "Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.",
  "url": "https://arxiv.org/abs/2512.02020v2",
  "html_url": "https://arxiv.org/html/2512.02020v2",
  "html_content": "\\contribution\n[*]Equal Contribution\n\\contribution\n[‚Ä†]Corresponding author\nEfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI\nJianlei Chang\nRuofeng Mei\nWei Ke\nXiangyu Xu\nXi‚Äôan Jiaotong University\nAbstract\nGenerative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks.\nHowever, existing generative policies often struggle with\ndata inefficiency\n, requiring large-scale demonstrations, and\nsampling inefficiency\n, incurring slow action generation during inference.\nWe introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning.\nTo enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands.\nTo accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories.\nAcross a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.\n\\metadata\n[Code]\nhttps://github.com/chang-jl/EfficientFlow\n\\metadata\n[Website]\nhttps://efficientflow.github.io\n1\nIntroduction\nLearning robotic policies from data using generative models has emerged as a powerful and flexible paradigm in embodied AI, particularly with the recent success of diffusion-based approaches\n(Chi et al.,\n2023\n; Ze et al.,\n2024\n)\n.\nThese models have demonstrated strong performance in visuomotor control by learning complex action distributions conditioned on high-dimensional observations.\nHowever, two key limitations remain: low data efficiency, requiring large amounts of training data, and low sampling efficiency, incurring high computational cost at inference due to the iterative sampling process.\nRecent works have sought to address the data efficiency issue by incorporating equivariance into diffusion models for policy learning\n(Wang et al.,\n2024\n)\n.\nBy leveraging the inherent symmetries of the environment (e.g., 2D rotation), these methods introduce strong inductive biases that enable policies to generalize across symmetric configurations. Nevertheless, as they are still built upon diffusion models, which typically require hundreds of iterative denoising steps to generate a single action\n(Sohl-Dickstein et al.,\n2015\n; Ho et al.,\n2020\n)\n, they remain impractical for real-time robotic control.\nTo overcome this limitation, we turn to Flow Matching\n(Lipman et al.,\n2023\n)\n, a recent class of generative models that learns a continuous trajectory from a simple prior distribution to the data distribution using an ordinary differential equation (ODE) defined by a velocity field.\nCompared to diffusion models, flow-based approaches offer better numerical stability and faster inference, making them highly appealing for efficient embodied AI.\nWe present EfficientFlow, a new policy learning framework that unifies equivariant learning and flow-based generative modeling.\nWe first investigate how to incorporate equivariance into flow-based policy models and theoretically show that, under an isotropic Gaussian prior and an equivariant velocity field network, the conditional action distribution induced by flow matching remains equivariant with respect to input observation transformations (see Figure\n1\n(a)).\nThis property allows policies to generalize across symmetric configurations of the environment without additional supervision or data augmentation.\nTo further improve the action sampling efficiency, we introduce a regularization technique that penalizes the acceleration of the generation flow trajectory, i.e., the second-order temporal derivative, which encourages a smoother and more stable action sampling process.\nHowever, computing acceleration requires consecutive points along the marginal flow trajectories, which are unavailable in the standard flow matching framework.\nTo address this challenge, we propose a novel surrogate objective called Flow Acceleration Upper Bound (FABO).\nFABO provides a practical and effective approximation of the acceleration penalty using only conditional flow trajectories available during training, enabling much faster flow policies with lower computational costs.\nThe proposed EfficientFlow combines the best of both worlds: it achieves fast inference speed thanks to the flow-based architecture and smoothed sampling trajectory, and maintains high performance by leveraging equivariance.\nAs illustrated in Figure\n1\n(b), EfficientFlow compares favorably against existing methods in both inference speed and task success rates.\nOur primary contributions are as follows:\n‚Ä¢\nWe formulate a flow-based policy learning framework, EfficientFlow, that achieves equivariance to geometric transformations, allowing the model to generalize across symmetric states and significantly improve data efficiency. We provide a theoretical analysis showing that equivariance is preserved in the flow framework when using an isotropic prior and an equivariant velocity field conditioned on visual observations.\n‚Ä¢\nTo promote sampling speed, we propose a second-order regularization objective that penalizes flow acceleration. Since direct acceleration computation requires access to neighboring marginal samples that are unavailable, we introduce a novel surrogate loss called FABO, enabling effective training.\n‚Ä¢\nWe provide comprehensive evaluations of EfficientFlow on 12 robotic manipulation tasks in the MimicGen\n(Mandlekar et al.,\n2023\n)\nbenchmark, showing that EfficientFlow achieves favorable success rates with high inference speeds (19.9 to 56.1 times faster than EquiDiff\n(Wang et al.,\n2024\n)\n).\nFigure 1\n:\nWe propose EfficientFlow (a) to effectively combine equivariance with Flow Policy and introduce an acceleration regularization to achieve high-quality, fast action generation. As shown in (b), EfficientFlow compares favorably against baseline policy learning approaches in both success rate and inference speed. Results are from MimicGen with 100 training demonstrations.\n2\nRelated Work\n2.1\nEquivariance in Robot Manipulation\nApplying equivariance to robot manipulation is a highly promising research direction, and multiple studies have demonstrated that it can significantly enhance the data efficiency of robot policy learning\n(Wang et al.,\n2022b\n; Jia et al.,\n2023\n; Wang et al.,\n2022c\n; Simeonov et al.,\n2023\n; Pan et al.,\n2023\n; Huang et al.,\n2023\n; Liu et al.,\n2023a\n; Kim et al.,\n2023\n; Nguyen et al.,\n2023\n; Yang et al.,\n2024a\n)\n. Early work used SE(3) open-loop or SE(2) closed-loop for control, validated the effectiveness of equivariant models in on-robot learning\n(Zhu & Wang,\n2022\n; Wang et al.,\n2022a\n; Zhu et al.,\n2023\n)\n, and achieved pick-and-place tasks based on few-shot demonstrations\n(Huang et al.,\n2022\n; Simeonov et al.,\n2022\n; Ryu et al.,\n2023\n; Huang et al.,\n2024\n)\n.\nBuilding on this foundation, EquiDiff\n(Wang et al.,\n2024\n)\nextend the research to the SE(3) closed-loop action space, substantially improving the efficiency of imitation learning by integrating symmetry with diffusion policies. However, the DDPM architecture employed by EquiDiff requires a multi-step denoising process, resulting in slow inference speeds. In contrast, the EfficientFlow model marks a significant breakthrough in inference efficiency, attaining higher success rates than EquiDiff with only a minimal number of inference steps.\n2.2\nFlow Policy\nFlow Matching\n(Lipman et al.,\n2023\n)\nrepresents a novel class of generative models grounded in optimal transport theory. Its objective is to learn a vector field of a probability path, which is more efficient than diffusion paths, offering faster training and sampling, alongside better generalization capabilities. Compared to diffusion models, Flow Matching significantly reduces the number of inference steps, a critical factor for real-world robotic operations, thereby substantially broadening the applicability of such models. The work Flow Policy\n(Zhang et al.,\n2025\n)\nintroduced conditional Consistent Flow Matching\n(Yang et al.,\n2024b\n)\nto robotic manipulation. Conditioned on observed 3D point clouds, Flow Policy utilizes Consistency Flow Matching to directly define straight-line flows from different temporal states to the same action space, concurrently constraining their velocity values. It approximates trajectories from noise to robot actions by normalizing the self-consistency of the velocity field within the action space, thereby enhancing inference efficiency. MP1\n(Sheng et al.,\n2025\n)\nleverages Mean Flow\n(Geng et al.,\n2025\n)\nto shrink policy learning to a single state-action step, while a lightweight Dispersive Loss repels state embeddings. This combination steadies the flow field and delivers millisecond inference that outpaces DP3 and Flow Policy. Currently, many VLA (Vision-Language-Action) models\n(Black et al.,\n2024\n; Gao et al.,\n2025\n; Bjorck et al.,\n2025\n; Reuss et al.,\n2025\n)\nare utilizing flow matching policies and have achieved good results.\n3\nMethod\n3.1\nPreliminaries\n3.1.1\nFlow Matching\nThe core idea of Flow Matching\n(Lipman et al.,\n2023\n)\nis to learn the vector field of an ODE that smoothly transforms samples\nx\n0\nx_{0}\nfrom a simple prior distribution\np\n0\np_{0}\n(e.g., Gaussian noise) to samples\nx\n1\nx_{1}\nfrom a target data distribution\np\n1\np_{1}\n.\nSpecifically, let\n{\np\nt\n}\nt\n‚àà\n[\n0\n,\n1\n]\n\\{p_{t}\\}_{t\\in[0,1]}\nbe a time-evolving family of probability distributions satisfying the boundary conditions\np\nt\n=\n0\n=\np\n0\np_{t=0}=p_{0}\nand\np\nt\n=\n1\n=\np\n1\np_{t=1}=p_{1}\n. This path induces an underlying ground-truth instantaneous velocity field\nu\ngt\n‚Äã\n(\nt\n,\nx\n)\nu^{\\text{gt}}(t,x)\n.\nFlow Matching aims to learn a vector field\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\n)\nu_{\\theta}(t,x)\nparameterized by\nŒ∏\n\\theta\n, such that trajectories\nx\nt\nx_{t}\ndefined by the following ODE:\n{\nd\n‚Äã\nx\nt\nd\n‚Äã\nt\n=\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\nt\n)\nx\n0\n‚àº\np\n0\n\\begin{cases}\\frac{dx_{t}}{dt}=u_{\\theta}(t,x_{t})\\\\\nx_{0}\\sim p_{0}\\end{cases}\n(1)\ncan effectively transport the prior distribution\np\n0\np_{0}\nto the target distribution\np\n1\np_{1}\n.\nIdeally, the learned vector field\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\n)\nu_{\\theta}(t,x)\nshould approximate the true vector field\nu\ngt\n‚Äã\n(\nt\n,\nx\n)\nu^{\\text{gt}}(t,x)\n.\nThus, a natural learning objective for Flow Matching is:\nL\nFM\n=\nùîº\nt\n,\nx\nt\n‚Äã\n[\n‚Äñ\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\nt\n)\n‚àí\nu\ngt\n‚Äã\n(\nt\n,\nx\nt\n)\n‚Äñ\n2\n2\n]\n,\nL_{\\text{FM}}=\\mathbb{E}_{t,x_{t}}\\left[\\left\\|u_{\\theta}(t,x_{t})-u^{\\text{gt}}(t,x_{t})\\right\\|_{2}^{2}\\right],\n(2)\nwhere\nx\nt\n‚àº\np\nt\nx_{t}\\sim p_{t}\n.\nAs\nu\ngt\n‚Äã\n(\nt\n,\nx\nt\n)\nu^{\\text{gt}}(t,x_{t})\nis generally intractable in practice, Conditional Flow Matching (CFM)\n(Lipman et al.,\n2023\n)\nproposes to learn\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\nt\n)\nu_{\\theta}(t,x_{t})\nby regressing against a conditional vector field\nu\n‚Äã\n(\nt\n,\nx\nt\n|\nx\n1\n)\nu(t,x_{t}|x_{1})\n, using samples from a conditional probability path\np\nt\n‚Äã\n(\nx\n|\nx\n1\n)\np_{t}(x|x_{1})\n.\nThe corresponding objective is:\nL\nCFM\n=\nùîº\nt\n,\nx\n1\n,\nx\nt\n[\n‚à•\nu\nŒ∏\n(\nt\n,\nx\nt\n)\n‚àí\nu\n(\nt\n,\nx\nt\n|\nx\n1\n)\n‚à•\n2\n2\n]\n,\nL_{\\text{CFM}}=\\mathbb{E}_{t,x_{1},x_{t}}\\left[\\left\\|u_{\\theta}(t,x_{t})-u(t,x_{t}|x_{1})\\right\\|_{2}^{2}\\right],\n(3)\nwhere\nt\n‚àº\nU\n‚Äã\n(\n0\n,\n1\n)\n,\nx\n1\n‚àº\np\n1\n‚Äã\n(\nx\n)\nt\\sim U(0,1),x_{1}\\sim p_{1}(x)\n, and\nx\nt\n‚àº\np\nt\n‚Äã\n(\nx\n|\nx\n1\n)\nx_{t}\\sim p_{t}(x|x_{1})\n.\n3.1.2\nEquivariance\nEquivariance is a desirable property in many learning systems, especially when modeling structured data influenced by known symmetries\n(Cesa et al.,\n2022\n)\n.\nA function\nf\nf\nis said to be equivariant with respect to a transformation group\nG\nG\nif it commutes with the actions of the group.\nFormally, this is expressed as:\nf\n‚Äã\n(\nœÅ\nx\n‚Äã\n(\ng\n)\n‚Äã\nx\n)\n=\nœÅ\ny\n‚Äã\n(\ng\n)\n‚Äã\nf\n‚Äã\n(\nx\n)\n,\n‚àÄ\ng\n‚àà\nG\n,\nf(\\rho_{x}(g)x)=\\rho_{y}(g)f(x),\\quad\\forall g\\in G,\n(4)\nwhere\nœÅ\nx\n\\rho_{x}\nand\nœÅ\ny\n\\rho_{y}\ndenote group representations that describe how the group acts on the input space and output space, respectively. This equation ensures that applying a group transformation to the input and then evaluating the function (Eq.\n4\nleft) yields the same result as first applying the function and then transforming the output (Eq.\n4\nright).\nIn this work, we focus on learning equivariant policies for robot arm control, where the input\nx\nx\nrepresents the robot arm action in the task space.\nSince robotic manipulation tasks often exhibit rotational symmetry\n(Wang et al.,\n2024\n,\n2022b\n)\n, we study the action of the rotation group\nSO\n‚Äã\n(\n2\n)\n\\mathrm{SO}(2)\nand its finite cyclic subgroup\nC\nu\n‚äÇ\nSO\n‚Äã\n(\n2\n)\nC_{u}\\subset\\text{SO}(2)\n, which models discrete rotational symmetries (e.g., rotations by\n2\n‚Äã\nœÄ\nu\n\\frac{2\\pi}{u}\nradians).\nWe consider the following standard representations:\n1) the trivial representation\nœÅ\n0\n\\rho_{0}\n, which maps every group element\ng\n‚àà\nG\ng\\in G\nto the identity transformation. This is typically used when the function output should remain invariant under the group action.\n2) the standard irreducible representation\nœÅ\n1\n\\rho_{1}\n, which describes the canonical action of\nSO\n‚Äã\n(\n2\n)\n\\mathrm{SO}(2)\nor\nC\nu\nC_{u}\non the 2D plane, defined as\nœÅ\n1\n‚Äã\n(\ng\n)\n=\n[\ncos\n‚Å°\n(\ng\n)\n‚àí\nsin\n‚Å°\n(\ng\n)\nsin\n‚Å°\n(\ng\n)\ncos\n‚Å°\n(\ng\n)\n]\n,\n\\rho_{1}(g)=\\begin{bmatrix}\\cos(g)&-\\sin(g)\\\\\n\\sin(g)&\\cos(g)\\end{bmatrix},\nwhere we slightly abuse the notation to use\ng\ng\nto denote both a group element and its corresponding rotation angle.\nBy designing policy networks that are equivariant under these group actions, we aim to incorporate inductive biases that reflect the underlying symmetries of the robot‚Äôs action space. This not only improves sample efficiency but also enhances generalization across real task configurations.\nFigure 2\n:\nOverview of EfficientFlow. At each decision step, the policy utilizes the most recent two observation steps\no\nas input. This information is processed by the equivariant Flow Matching network to generate five candidate action trajectories. The trajectory that exhibits the minimum Euclidean distance to the previously predicted trajectory is then selected for execution, ensuring a smooth and coherent action sequence.\n3.2\nEquivariant Flow Policy\nGenerative models for policy learning have received significant attention in recent years. Given an observation\no\no\n, such models can predict a conditional distribution\np\nX\n1\n|\nO\n=\no\np_{X_{1}|O=o}\n, and generate robot actions by sampling\nx\n‚àº\nX\n1\n|\nO\n=\no\nx\\sim X_{1}\\big|_{O=o}\n, where\nX\n1\nX_{1}\nrepresents the random variable for the action to be executed by the robot arm under the condition that\nO\n=\no\nO=o\n.\nBoth\no\no\nand\nx\nx\ncan span multiple time steps:\no\n=\n[\no\nœÑ\n‚àí\n(\nm\n‚àí\n1\n)\n,\n‚ãØ\n,\no\nœÑ\n‚àí\n1\n,\no\nœÑ\n]\no=[o^{\\tau-(m-1)},\\cdots,o^{\\tau-1},o^{\\tau}]\n,\nx\n=\n[\nx\nœÑ\n,\nx\nœÑ\n+\n1\n,\n‚ãØ\n,\nx\nœÑ\n+\n(\nn\n‚àí\n1\n)\n]\nx=[x^{\\tau},x^{\\tau+1},\\cdots,x^{\\tau+(n-1)}]\n, where\nm\nm\nis the number of historical observations, and\nn\nn\nis the number of future action steps.\nThe observation\no\nœÑ\no^{\\tau}\nincludes both the image and the robot state at robot time\nœÑ\n\\tau\n.\nA desirable property for such models is equivariance: when the input\no\no\nis transformed by an element\ng\n‚àà\nG\ng\\in G\nof a symmetry group (e.g., a rotation), the conditional distribution of the output action should transform accordingly. In other words, symmetry in the observation space should induce symmetry in the action space:\nX\n1\n|\nO\n=\ng\n‚Äã\no\n=\nd\ng\n‚Äã\n(\nX\n1\n|\nO\n=\no\n)\n,\n\\displaystyle X_{1}\\big|_{O=go}\\stackrel{{\\scriptstyle d}}{{=}}g\\left(X_{1}\\big|_{O=o}\\right),\n(5)\nwhere\n=\nd\n\\stackrel{{\\scriptstyle d}}{{=}}\ndenotes that the two random variables have the same distribution. We leave the group representation\nœÅ\n‚Äã\n(\ng\n)\n\\rho(g)\nimplicit here and directly use\ng\ng\nfor brevity.\n3.2.1\nHow to Make Flow Policy Equivariant?\nThe main contribution of this work is to demonstrate that the desired property in Eq.\n5\ncan be achieved within the Flow Matching framework by:\n1.\nusing an isotropic distribution for\np\n0\np_{0}\nin Eq.\n1\n, e.g., Gaussian noise\nX\n0\n‚àº\nùí©\n‚Äã\n(\n0\n,\nI\n)\nX_{0}\\sim\\mathcal{N}(0,I)\n;\n2.\nusing an equivariant network\nu\nŒ∏\nu_{\\theta}\nfor the velocity field such that:\nu\nŒ∏\n‚Äã\n(\nt\n,\ng\n‚Äã\nx\n|\ng\n‚Äã\no\n)\n=\ng\n‚Äã\n(\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\n|\no\n)\n)\n,\n‚àÄ\ng\n‚àà\nG\n.\n\\displaystyle u_{\\theta}(t,gx|go)=g\\left(u_{\\theta}(t,x|o)\\right),\\quad\\forall g\\in G.\n(6)\nImportantly, we do\nnot\nimpose the strong assumption that the expert policy in the training data be equivariant, which is in sharp contrast with\n(Wang et al.,\n2024\n)\n.\nTheorem 1\n.\nLet\nG\nG\nbe a transformation group acting on both the observation space and the action space. Suppose the initial distribution\np\n0\np_{0}\nis isotropic, i.e.,\np\n0\n‚Äã\n(\ng\n‚Äã\nx\n)\n=\np\n0\n‚Äã\n(\nx\n)\np_{0}(gx)=p_{0}(x)\nfor all\ng\n‚àà\nG\ng\\in G\n, and the velocity network\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\n|\no\n)\nu_{\\theta}(t,x|o)\nis equivariant as in Eq.\n6\n.\nThen the induced conditional distribution at time\nt\nt\n, given by the flow ODE Eq.\n1\n, satisfies\nX\nt\n|\nO\n=\ng\n‚Äã\no\n=\nd\ng\n‚Äã\n(\nX\nt\n|\nO\n=\no\n)\n,\nt\n‚àà\n[\n0\n,\n1\n]\n\\displaystyle X_{t}\\big|_{O=go}\\stackrel{{\\scriptstyle d}}{{=}}g\\left(X_{t}\\big|_{O=o}\\right),\\quad t\\in[0,1]\n(7)\ni.e., the output distribution is equivariant under the group action.\nThe special case\nt\n=\n1\nt=1\nof Eq.\n7\ngives us the desired property in Eq.\n5\n.\nAn intuitive visualization of this result is provided in Figure\n1\n(a).\nFrom a discrete-time perspective, consider starting from a randomly sampled initial action\nx\n0\n‚àº\np\n0\nx_{0}\\sim p_{0}\n. After a small time step\nŒî\n‚Äã\nt\n\\Delta t\n, the action evolves under the velocity field to reach\nx\nŒî\n‚Äã\nt\n=\nx\n0\n+\nŒî\n‚Äã\nt\n‚ãÖ\nu\nŒ∏\n‚Äã\n(\n0\n,\nx\n0\n|\no\n)\nx_{\\Delta t}=x_{0}+\\Delta t\\cdot u_{\\theta}(0,x_{0}|o)\n.\nThis corresponds to the green curve in Figure\n1\n(a).\nNow, consider a rotated scenario where the initial action is transformed to\nx\n^\n0\n=\ng\n‚Äã\nx\n0\n\\hat{x}_{0}=gx_{0}\n, and the observation is rotated accordingly to\ng\n‚Äã\no\ngo\n. The updated action becomes\nx\n^\nŒî\n‚Äã\nt\n=\n\\displaystyle\\hat{x}_{\\Delta t}=\nx\n^\n0\n+\nŒî\n‚Äã\nt\n‚ãÖ\nu\nŒ∏\n‚Äã\n(\n0\n,\nx\n^\n0\n|\ng\n‚Äã\no\n)\n\\displaystyle\\ \\hat{x}_{0}+\\Delta t\\cdot u_{\\theta}(0,\\hat{x}_{0}|go)\n(one-step update)\n=\n\\displaystyle=\ng\n‚Äã\nx\n0\n+\nŒî\n‚Äã\nt\n‚ãÖ\nu\nŒ∏\n‚Äã\n(\n0\n,\ng\n‚Äã\nx\n0\n|\ng\n‚Äã\no\n)\n\\displaystyle\\ gx_{0}+\\Delta t\\cdot u_{\\theta}(0,gx_{0}|go)\n(since\nx\n^\n0\n=\ng\n‚Äã\nx\n0\n\\hat{x}_{0}=gx_{0}\n)\n=\n\\displaystyle=\ng\n‚Äã\nx\n0\n+\nŒî\n‚Äã\nt\n‚ãÖ\ng\n‚Äã\nu\nŒ∏\n‚Äã\n(\n0\n,\nx\n0\n|\no\n)\n\\displaystyle\\ gx_{0}+\\Delta t\\cdot gu_{\\theta}(0,x_{0}|o)\n(Eq.\n6\n)\n=\n\\displaystyle=\ng\n‚Äã\nx\nŒî\n‚Äã\nt\n.\n\\displaystyle\\ gx_{\\Delta t}.\nThis corresponds to the blue curve in Figure\n1\n(a), showing that the evolution of the rotated action\nx\n^\nt\n\\hat{x}_{t}\nunder the rotated observation\ng\n‚Äã\no\ngo\naligns with the rotated evolution of the original action\nx\nt\nx_{t}\n.\nBy repeating this process over the entire flow trajectory, we conclude that\nx\n^\n1\n=\ng\n‚Äã\nx\n1\n\\hat{x}_{1}=gx_{1}\n(see Figure\n1\n(a)).\nSince\np\n0\np_{0}\nis isotropic,\nx\n0\nx_{0}\nand\nx\n^\n0\n\\hat{x}_{0}\nhave the same probability density.\nGiven that the flow deterministically transports\nx\n0\nx_{0}\nto\nx\n1\nx_{1}\nand\nx\n^\n0\n\\hat{x}_{0}\nto\nx\n^\n1\n\\hat{x}_{1}\n, it follows that\nx\n1\nx_{1}\nand\nx\n^\n1\n\\hat{x}_{1}\nshare the same density.\nThis implies that the resulting distribution of\nx\n1\nx_{1}\nand\nx\n^\n1\n\\hat{x}_{1}\nrespects the desired equivariance with respect to\ng\ng\n.\nWe emphasize that this is only an intuitive explanation; a rigorous proof is provided in Appendix\nA\n.\nSince standard Flow Matching uses a Gaussian distribution as\np\n0\np_{0}\nby default, the isotropy condition in Theorem\n1\nis automatically satisfied.\nAs a result, making Flow Policy equivariant reduces to designing an equivariant network\nu\nŒ∏\nu_{\\theta}\n.\n3.2.2\nDesign of the Equivariant\nu\nŒ∏\nu_{\\theta}\nTo implement the equivariant policy network\nu\nŒ∏\nu_{\\theta}\n, we leverage the\nescnn\nlibrary\n(Cesa et al.,\n2022\n)\n, which supports constructing neural networks that are equivariant to symmetry groups (planar rotations modeled by SO(2) in our case).\nA critical step in using\nescnn\nis specifying how each output component transforms under group actions, which requires carefully choosing representations that respect the underlying task symmetries.\nIn our setting, the policy outputs an absolute 6-DoF end-effector pose with 3D rotation and 3D translation, along with a scalar gripper width to control a robot arm.\nTo represent the 3D rotation, we adopt the 6D continuous representation\n(Zhou et al.,\n2019\n)\nthat encodes the first two rows of a\n3\n√ó\n3\n3\\times 3\nrotation matrix, corresponding to the\nx\nx\nand\ny\ny\naxes.\nThis 6D representation can be seen as three 2D vectors in the\nx\nx\n-\ny\ny\nplane, which transform under SO(2) according to the irreducible representation\nœÅ\n1\n\\rho_{1}\n. Therefore, the 3D rotation component corresponds to\nœÅ\n1\n3\n\\rho_{1}^{3}\n.\nFor 3D translation, the\nx\nx\nand\ny\ny\ncomponents transform as a 2D vector under SO(2), again corresponding to\nœÅ\n1\n\\rho_{1}\n, while the\nz\nz\ncomponent remains invariant and is modeled as\nœÅ\n0\n\\rho_{0}\n.\nThe scalar gripper width is also invariant under planar rotation, corresponding to another\nœÅ\n0\n\\rho_{0}\n.\nCombining these components, the action vector at robot time\nœÑ\n\\tau\n, denoted by\nx\nœÑ\nx^{\\tau}\n, is a 10D vector comprising a 6D rotation representation (first 6 dimensions), a 3D translation vector (next 3 dimensions), and a scalar gripper width (final dimension). The corresponding equivariant representation of the action output is:\ng\n‚Äã\nx\nœÑ\n=\n(\nœÅ\n1\n3\n‚äï\n(\nœÅ\n1\n‚äï\nœÅ\n0\n)\n‚äï\nœÅ\n0\n)\n‚Äã\n(\ng\n)\n‚Äã\nx\nœÑ\n.\ngx^{\\tau}=(\\rho_{1}^{3}\\oplus(\\rho_{1}\\oplus\\rho_{0})\\oplus\\rho_{0})(g)x^{\\tau}.\nThis representation enables\nu\nŒ∏\nu_{\\theta}\nto produce actions that respect the SO(2) symmetry of the task, ensuring consistent behavior under planar rotations of the scene.\n3.2.3\nNetwork Architecture\nAs introduced above, the input of\nu\nŒ∏\nu_{\\theta}\nis flow time\nt\nt\n, action sequence\nx\nt\nx_{t}\n, and observation\no\no\n.\nWe set the equivariant group as a finite cyclic subgroup\nC\nu\n‚àà\nSO\n‚Äã\n(\n2\n)\nC_{u}\\in\\text{SO}(2)\n, and\nu\nu\nis the order of the group.\nWe first use an equivariant observation encoder to map observation\no\no\nto embeddings\ne\no\n‚àà\n‚Ñù\nu\n√ó\nd\no\ne_{o}\\in\\mathbb{R}^{u\\times d_{o}}\nand use an equivariant action encoder to map action sequence\nx\nt\nx_{t}\nto embeddings\ne\nx\n‚àà\n‚Ñù\nu\n√ó\nd\nx\ne_{x}\\in\\mathbb{R}^{u\\times d_{x}}\n, where\nd\no\nd_{o}\nand\nd\nx\nd_{x}\nare the feature dimensions associated with each group element.\nThe encoded embeddings\ne\no\n,\ne\nx\ne_{o},e_{x}\n, along with the timestep\nt\nt\n, are fed into a core equivariant neural network. This network, together with the observation and action encoders, parameterizes the conditional vector field\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\nt\n,\no\n)\nu_{\\theta}(t,x_{t},o)\n. As all components are designed to be equivariant, the entire mapping process from raw inputs to the predicted vector field strictly adheres to\nC\nu\nC_{u}\nsymmetry.\n3.2.4\nTemporal Consistency\nWhen generating action sequences, adjacent segments are predicted independently. As a result, the policy may switch between different behavioral modes, leading to inconsistencies in long-term execution.\nTo address this, we adopt a temporal overlapping strategy similar to\n(Chi et al.,\n2023\n)\n: only the first\nn\n1\nn_{1}\nsteps of each predicted sequence are executed, while the remaining\nn\n‚àí\nn\n1\nn-n_{1}\nsteps overlap with the subsequent prediction starting from time\nœÑ\n+\nn\n1\n\\tau+n_{1}\n.\nLong-term consistency can be achieved by generating neighboring action sequences with similar overlap.\nTo this end, we employ a batched trajectory selection and periodic reset strategy, inspired by IMLE Policy\n(Rana et al.,\n2025\n)\n, which balances multi-modal expressivity with temporal coherence.\nDuring inference, we sample\nm\nm\ninitial noise vectors\n{\nx\n0\n,\ni\n}\ni\n=\n1\nm\n\\{x_{0,i}\\}_{i=1}^{m}\nfrom a Gaussian distribution and evolve each through our model to generate\nm\nm\ncandidate action trajectories\n{\nx\n1\n,\ni\n}\ni\n=\n1\nm\n\\{x_{1,i}\\}_{i=1}^{m}\n. We then select the trajectory whose overlapping segment best matches the previous trajectory in the Euclidean sense:\narg\n‚Å°\nmin\ni\n‚àà\n1\n,\n‚Ä¶\n,\nm\n‚Å°\nd\n‚Äã\n(\n[\nx\npre\nœÑ\n+\nn\n1\n,\n‚Ä¶\n,\nx\npre\nœÑ\n+\nn\n]\n,\n[\nx\n1\n,\ni\nœÑ\n+\nn\n1\n,\n‚Ä¶\n,\nx\n1\n,\ni\nœÑ\n+\nn\n]\n)\n,\n\\displaystyle\\arg\\min_{i\\in{1,\\dots,m}}d\\left([x_{\\text{pre}}^{\\tau+n_{1}},\\dots,x_{\\text{pre}}^{\\tau+n}],[x_{1,i}^{\\tau+n_{1}},\\dots,x_{1,i}^{\\tau+n}]\\right),\nwhere we assume the current robot time is\nœÑ\n+\nn\n1\n\\tau+n_{1}\n, and\nx\npre\nx_{\\text{pre}}\ndenotes the previous action sequence predicted at time\nœÑ\n\\tau\n, where the steps\nx\npre\nœÑ\n,\n‚Ä¶\n,\nx\npre\nœÑ\n+\nn\n1\n‚àí\n1\nx_{\\text{pre}}^{\\tau},\\dots,x_{\\text{pre}}^{\\tau+n_{1}-1}\nhave already been executed.\nTo preserve the model‚Äôs ability to explore diverse behaviors, we introduce periodic resets: every 10 prediction cycles, we randomly select one trajectory from the batch for execution, instead of the one that minimizes the overlap distance.\nThis approach improves temporal consistency while maintaining multi-modality, and the batched design ensures minimal overhead in inference time due to parallelization.\n3.3\nAcceleration Regularization\nIn our experiments, we observe that flow-based policies trained solely with the conditional flow matching objective (Eq.\n3\n) tend to perform poorly when the number of function evaluations (NFE) is low. This suggests that the learned flow fields are overly curved, requiring more integration steps for accurate trajectory generation.\nTo address this, we propose an acceleration regularization term that encourages smoother, low-curvature flow trajectories. The underlying intuition is that smoother motion corresponds to smaller second-order derivatives (accelerations) of the trajectory\nx\nt\nx_{t}\n. In the extreme case of zero acceleration, the trajectory becomes a straight line.\nWe augment the training objective as follows:\nùîº\n[\n‚à•\nu\nŒ∏\n(\nt\n,\nx\nt\n)\n‚àí\nu\n(\nt\n,\nx\nt\n‚à£\nx\n1\n)\n‚à•\n2\n2\n]\n‚èü\nData Term\n+\nŒª\n‚Äã\nùîº\n‚Äã\n[\n‚Äñ\nd\n2\n‚Äã\nx\nt\nd\n‚Äã\nt\n2\n‚Äñ\n2\n2\n]\n‚èü\nAcceleration Penalty\n,\n\\underbrace{\\mathbb{E}\\left[\\left\\|u_{\\theta}(t,x_{t})-u(t,x_{t}\\mid x_{1})\\right\\|_{2}^{2}\\right]}_{\\text{Data Term}}+\\lambda\\!\\!\\!\\underbrace{\\mathbb{E}\\left[\\left\\|\\frac{\\mathrm{d}^{2}x_{t}}{\\mathrm{d}t^{2}}\\right\\|_{2}^{2}\\right]}_{\\text{Acceleration Penalty}},\n(8)\nwhere\nŒª\n\\lambda\ncontrols the trade-off between fidelity to the target velocity field and trajectory smoothness. In practice, we use a time-dependent weighting\nŒª\n‚Äã\n(\nt\n)\n=\n(\n1\n‚àí\nt\n)\n2\n\\lambda(t)=(1-t)^{2}\n, which encourages smoother flow at earlier timesteps and prioritizes accuracy as\nt\n‚Üí\n1\nt\\to 1\n.\nAccording to Eq.\n1\n, the second derivative term can be rewritten as:\nùîº\n‚Äã\n[\n‚Äñ\nd\n2\n‚Äã\nx\nt\nd\n‚Äã\nt\n2\n‚Äñ\n2\n2\n]\n‚âà\n1\n(\nŒî\n‚Äã\nt\n)\n2\n‚Äã\nùîº\n‚Äã\n‚Äñ\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\nt\n)\n‚àí\nu\nŒ∏\n‚Äã\n(\nt\n+\nŒî\n‚Äã\nt\n,\nx\nt\n+\nŒî\n‚Äã\nt\n)\n‚Äñ\n2\n2\n,\n\\displaystyle\\mathbb{E}\\left[\\left\\|\\frac{\\mathrm{d}^{2}x_{t}}{\\mathrm{d}t^{2}}\\right\\|_{2}^{2}\\right]\\approx\\frac{1}{(\\Delta t)^{2}}\\mathbb{E}\\left\\|u_{\\theta}(t,x_{t})-u_{\\theta}(t+\\Delta t,x_{t+\\Delta t})\\right\\|_{2}^{2},\nwhich, however, cannot be directly evaluated, because\nx\nt\nx_{t}\nand\nx\nt\n+\nŒî\n‚Äã\nt\nx_{t+\\Delta t}\nlie on the same underlying marginal trajectory that is unknown.\nTo overcome this, we introduce a practical surrogate regularization, which we call the Flow Acceleration Upper Bound (FABO):\nFABO\n=\nùîº\n‚Äã\n‚Äñ\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\n~\nt\n)\n‚àí\nu\nŒ∏\n‚Äã\n(\nt\n+\nŒî\n‚Äã\nt\n,\nx\n~\nt\n+\nŒî\n‚Äã\nt\n)\n‚Äñ\n2\n2\n‚â•\nùîº\n‚Äã\n‚Äñ\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\nt\n)\n‚àí\nu\nŒ∏\n‚Äã\n(\nt\n+\nŒî\n‚Äã\nt\n,\nx\nt\n+\nŒî\n‚Äã\nt\n)\n‚Äñ\n2\n2\n,\n\\displaystyle\\text{FABO}=\\mathbb{E}\\left\\|u_{\\theta}(t,\\tilde{x}_{t})-u_{\\theta}(t+\\Delta t,\\tilde{x}_{t+\\Delta t})\\right\\|_{2}^{2}\\geq\\mathbb{E}\\left\\|u_{\\theta}(t,x_{t})-u_{\\theta}(t+\\Delta t,x_{t+\\Delta t})\\right\\|_{2}^{2},\n(9)\nwhen\nŒî\n‚Äã\nt\n\\Delta t\nis small.\nNotably,\nx\n~\nt\n\\tilde{x}_{t}\nand\nx\n~\nt\n+\nŒî\n‚Äã\nt\n\\tilde{x}_{t+\\Delta t}\nare sampled from the same conditional trajectory at time\nt\nt\nand\nt\n+\nŒî\n‚Äã\nt\nt+\\Delta t\n, which are easy to draw and require no knowledge of the marginal trajectory.\nIn essence, FABO minimizes an upper bound on the true acceleration penalty, serving as a tractable and effective proxy. A formal proof for Eq.\n9\nis provided in Appendix\nB\n.\nMethod\nObs\nNFE\nStack D1\nSquare D2\nThreading D2\nStack Three D1\nCoffee D2\n3 Pc. Asm. D2\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\nOurs\nRGB\n1\n94\n100\n100\n21\n21\n45\n67\n67\n31\n36\n36\n49\n49\n48\n48\n73\n73\n92\n92\n65\n65\n81\n79\n79\n11\n11\n35\n35\n60\n60\n3\n88\n88\n100\n100\n20\n20\n45\n71\n31\n43\n53\n53\n49\n49\n76\n76\n94\n94\n66\n66\n80\n80\n84\n11\n11\n38\n38\n69\n69\n5\n87\n87\n100\n100\n22\n22\n43\n43\n71\n31\n41\n41\n58\n58\n50\n50\n79\n93\n93\n67\n79\n79\n83\n83\n11\n11\n42\n71\nEquiDiff\nRGB\n100\n93\n93\n100\n100\n25\n41.3\n41.3\n60\n60\n22\n22\n40\n40\n59\n55\n77\n77\n96\n60\n60\n79\n79\n76\n76\n15\n39\n39\n69\n69\nDP-C\nRGB\n100\n76\n76\n97\n97\n100\n8\n8\n19\n19\n46\n46\n17\n17\n35\n35\n59\n38\n38\n72\n72\n94\n94\n44\n44\n66\n66\n79\n79\n4\n4\n6\n6\n30\n30\nDP-T\nRGB\n100\n51\n51\n83\n83\n99\n99\n5\n5\n11\n11\n45\n45\n11\n11\n18\n18\n41\n41\n17\n17\n41\n41\n84\n84\n47\n47\n61\n61\n75\n75\n1\n1\n4\n4\n43\n43\nDP3\nPCD\n10\n69\n69\n87\n87\n99\n99\n7\n7\n6\n6\n19\n19\n12\n12\n23\n23\n40\n40\n7\n7\n23\n23\n65\n65\n34\n34\n45\n45\n69\n69\n0\n1\n1\n3\n3\nACT\nRGB\n1\n35\n35\n73\n73\n96\n96\n6\n6\n18\n18\n49\n49\n10\n10\n21\n21\n35\n35\n6\n6\n37\n37\n78\n78\n19\n19\n33\n33\n64\n64\n0\n3\n3\n24\n24\nMethod\nObs\nNFE\nHammer Cln. D1\nMug Cln. D1\nKitchen D1\nPick Place D0\nNut Asmn.D0\nCoffee Pre. D1\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\nOurs\nRGB\n1\n75\n75\n77\n77\n50\n65\n65\n67\n67\n66\n66\n78\n78\n81\n81\n37\n37\n50\n50\n67\n67\n59\n59\n83\n83\n94\n94\n75\n75\n74\n74\n70\n70\n3\n72\n72\n75\n84\n84\n50\n65\n65\n70\n73\n81\n81\n81\n81\n41\n41\n62\n62\n86\n86\n61\n61\n86\n86\n96\n96\n81\n81\n81\n81\n89\n5\n74\n74\n75\n83\n83\n50\n68\n70\n73\n81\n81\n83\n83\n41\n41\n66\n66\n87\n87\n62\n62\n87\n98\n83\n82\n82\n87\n87\nEquiDiff\nRGB\n100\n65\n65\n63\n63\n77\n77\n50\n64\n64\n67\n67\n67\n67\n77\n77\n81\n81\n42\n74\n92\n74\n85\n85\n94\n94\n77\n77\n83\n85\n85\nDP-C\nRGB\n100\n52\n52\n59\n59\n73\n73\n43\n43\n59\n59\n65\n65\n67\n67\n85\n87\n87\n35\n35\n65\n65\n83\n83\n55\n55\n68\n68\n83\n83\n65\n65\n62\n62\n58\n58\nDP-T\nRGB\n100\n48\n48\n60\n60\n76\n76\n30\n30\n43\n43\n63\n63\n54\n54\n75\n75\n81\n81\n15\n15\n37\n37\n50\n50\n31\n31\n32\n32\n46\n46\n38\n38\n51\n51\n76\n76\nDP3\nPCD\n10\n54\n54\n71\n71\n87\n21\n21\n33\n33\n53\n53\n45\n45\n71\n71\n91\n12\n12\n15\n15\n34\n34\n16\n16\n24\n24\n58\n58\n10\n10\n22\n22\n63\n63\nACT\nRGB\n1\n38\n38\n54\n54\n71\n71\n23\n23\n31\n31\n56\n56\n37\n37\n61\n61\n87\n87\n7\n7\n17\n17\n50\n50\n42\n42\n64\n64\n84\n84\n32\n32\n46\n46\n65\n65\nTable 1\n:\nComparison against SOTA. We report the success rates of 12 MimicGen\n(Mandlekar et al.,\n2023\n)\ntasks using 100, 200, and 1000 demonstrations, respectively. Results averaged over three seeds. The results of the baseline methods are directly cited from EquiDiff\n(Wang et al.,\n2024\n)\n.\n4\nExperiments\n4.1\nImplementation Details\nWe evaluate EfficientFlow on 12 tasks from the MimicGen benchmark\n(Mandlekar et al.,\n2023\n)\n.\nThese tasks span a wide range of difficulties, time horizons, and object arrangements, providing a comprehensive testbed for assessing policy performance across diverse robotic manipulation scenarios.\nNotably, the agent view camera in MimicGen is not positioned orthogonally to the workspace but rather provides an agent-centric perspective. While the resulting image rotations may not perfectly align with the true object rotations, potentially impacting the performance of equivariant networks, this setup more closely mirrors real-world scenarios where state information is acquired from non-ideal viewpoints and thus offers a more rigorous test of policy generalization.\nOur work aims for effective and efficient robot control in embodied AI.\nThus, we compare EfficientFlow against strong baselines in this field, including EquiDiff\n(Wang et al.,\n2024\n)\n, ACT\n(Zhao et al.,\n2023\n)\n, and two variants of Diffusion Policy\n(Chi et al.,\n2023\n)\n: the CNN-based DP-C and the Transformer-based DP-T.\nFor fair comparison, all baseline methods use the same input as EfficientFlow: RGB images from both the agent-view and wrist-mounted cameras.\nIn addition to the RGB-based methods, we further compare with DP3\n(Ze et al.,\n2024\n)\n, which utilizes 3D point cloud information.\nAll policies employ absolute pose estimation for control.\nWe evaluate EfficientFlow with 1, 3, and 5 NFE, while for baseline methods, we adhere to their original configurations.\n4.2\nQuantitative Comparison\n4.2.1\nSampling Efficiency\nEfficientFlow demonstrates notable advantages in inference speed and sampling efficiency.\nAs shown in Table\n2\n,\nit achieves an average inference time of only 12.22 ms under a 1-NFE setting, offering an approximately 56.1\n√ó\n\\times\nspeedup over EquiDiff.\nEven with a more computationally demanding 5-NFE inference, EfficientFlow remains approximately 19.9\n√ó\n\\times\nfaster than EquiDiff on average.\nThis significant gain in efficiency is critical for real-time robotic control, where rapid response is essential, and enables inference frequencies of up to 81.8 Hz with single-step EfficientFlow.\n4.2.2\nData Efficiency\nAs shown in Table\n1\n,\nunder the data-limited setting of only 100 demonstrations,\nEfficientFlow not only enables significantly faster inference but also achieves a strong policy success rate.\nAcross the 12 test tasks, EfficientFlow outperforms EquiDiff in 7 of them.\nFor the remaining 5 tasks, except the Nut Assembly D0 task, the performance gap between EfficientFlow and EquiDiff is within 5 percentage points.\nThese results indicate that EfficientFlow can match or even surpass the performance of the SOTA baseline method while drastically reducing inference latency.\nWhen trained with 200 demonstrations, EfficientFlow achieves 98.4% of the success rate of the DP-C method trained with 1000 demonstrations, while surpassing the average success rates of DP-T, DP3, and ACT.\nThis remarkable performance underscores its exceptional data efficiency and strong generalization under limited supervision.\nMethod\nNFE\nRuntime (ms)\nAverage Success Rate (%)\n100\n200\n1000\nOurs\n1\n12.22\n52.61\n66.18\n75.25\n3\n22.59\n53.49\n69.33\n81.36\n5\n34.45\n54.18\n70.26\n81.99\nEquiDiff\n100\n685.92\n53.77\n68.59\n79.69\nDP-C\n100\n542.96\n42.00\n57.75\n71.42\nDP-T\n100\n497.53\n29.00\n43.00\n64.92\nDP3\n10\n53.83\n23.92\n35.08\n56.75\nACT\n1\n12.51\n21.33\n38.17\n63.25\nTable 2:\nAverage success rates and inference time of EfficientFlow and baselines across 12 MimicGen tasks.\nMethod\nSk\nSq\nTh\nS3\nCf\n3P\nHm\nMu\nKi\nPP\nNu\nCP\nAvg\nOurs\n20\n60\n20\n40\n30\n40\n10\n30\n30\n40\n40\n20\n31.7\nEquiDiff\n70\n80\n40\n50\n40\n70\n50\n40\n30\n60\n50\n40\n51.7\nNoAcc\n30\n40\n20\n60\n30\n60\n20\n10\n30\n50\n30\n40\n35.0\nTable 3:\nMinimum training epochs required for EfficientFlow and EquiDiff to reach 50‚Äâ% of their final maximum success rate (MimicGen, 100 demonstrations, evaluated every 10 epochs with a fixed seed).\nMethod\nSk\nSq\nTh\nS3\nCf\n3P\nHm\nMu\nKi\nPP\nNu\nCP\nAvg\nOurs\n94\n21\n31\n48\n65\n11\n75\n50\n66\n37\n59\n75\n52.6\nNoAcc\n88\n16\n24\n44\n56\n10\n56\n28\n42\n16.5\n28\n62\n39.3\nNonEqui\n88\n12\n12\n14\n52\n0\n66\n38\n54\n17.5\n43\n56\n37.7\nEquiCFM\n88\n8\n22\n44\n60\n8\n54\n36\n66\n19\n40\n40\n40.4\nEquiMF\n96\n22\n26\n34\n50\n8\n58\n50\n62\n26\n51\n72\n46.3\nTable 4:\nAblation study on 12 MimicGen tasks using 100 demonstrations.\n4.2.3\nMore Analysis of the Performance\nAs shown in Table\n2\n, the average success rates across all tasks further highlight the advantage of EfficientFlow.\nFirst, the average performance of EfficientFlow exhibits an upward trend as the NFE increases; with sufficient data, multi-step inference can better capture the conditional action distribution to achieve higher success rates.\nMore importantly, across all dataset sizes, EfficientFlow consistently exceeds the average success rate of EquiDiff\n(Wang et al.,\n2024\n)\nwhile requiring dramatically fewer inference steps.\nWe attribute the advantage of EfficientFlow to two key design choices: the strong inductive biases introduced by the equivariant architecture, and the acceleration regularization that stabilizes the action sampling trajectories.\nThese factors enable the model to more efficiently learn key task structures and robust dynamics representations from limited demonstrations.\n4.2.4\nLearning Efficiency\nTo quantitatively evaluate the learning efficiency of EfficientFlow, we measure the minimum number of training epochs required to reach 50% of the final peak success rate.\nAs shown in Table\n3\n, both EfficientFlow and its variant without acceleration regularization (NoAcc) require substantially fewer training epochs than EquiDiff to reach 50% of their maximum success rate.\nNotably, in the Hammer Cleanup D1 task, EfficientFlow requires only one-fifth of the epochs needed by EquiDiff.\nThese results demonstrate the improved learning efficiency and stronger optimization dynamics of our equivariant flow-based framework.\nThe acceleration constraint further improves convergence speed, as evidenced by the faster learning of EfficientFlow compared to NoAcc.\nThese results indicate that EfficientFlow can extract essential policy information from demonstrations more rapidly, which reflects its superior learning dynamics,\nenabling it to reach target performance levels significantly faster than baseline methods.\n4.3\nAblation Study\nTo disentangle the contributions of the equivariant architecture and the acceleration regularization, we conduct comprehensive ablation studies across 12 MimicGen tasks using 100 demonstrations in Table\n4\n.\nWe evaluate four key variants:\n1) NoAcc, which removes the acceleration term and is trained solely with the Conditional Flow Matching loss (\nL\nCFM\nL_{\\text{CFM}}\nin Eq.\n3\n);\n2) NoEqui, which discards the equivariant architecture in favor of a non-equivariant backbone similar to Diffusion Policy\n(Chi et al.,\n2023\n)\nwhile retaining the acceleration constraint;\n3) EquiCFM, which combines the equivariant network with Consistency Flow Matching\n(Yang et al.,\n2024b\n)\n; and\n4) EquiMF, which integrates the equivariant network with MeanFlow\n(Geng et al.,\n2025\n)\n.\nThe results reveal two clear trends.\nFirst, both our proposed components, including equivariance and acceleration regularization, substantially and independently improve performance. Removing either one leads to a consistent drop in success rate, demonstrating their complementary roles: the equivariant structure provides strong inductive biases for learning symmetric behaviors, while the acceleration term stabilizes sampling trajectory learning.\nSecond, to further investigate the benefit of our acceleration regularization, we compare EfficientFlow against other efficient one-step flow matching variants.\nBy replacing our formulation with Consistency Flow Matching\n(Yang et al.,\n2024b\n)\nand MeanFlow\n(Geng et al.,\n2025\n)\nwhile keeping the equivariant architecture fixed, we observe that EfficientFlow achieves higher overall success rates across tasks. This suggests that our acceleration-regularized formulation not only stabilizes training but also leads to more accurate and robust policy generation.\n5\nConclusion\nWe introduce EfficientFlow, a theory-grounded generative policy learning framework that effectively balances inference speed and data efficiency. By leveraging equivariant flow matching and acceleration regularization, this work provides a principled approach for learning robust visuomotor policies while ensuring strong generalization and efficient learning.\nThe ultra-fast inference and strong data efficiency of EfficientFlow highlight its potential as a practical and high-performance solution for real-world embodied AI systems.\n6\nAcknowledgments\nThis research was supported by the National Natural Science Foundation of China (62302385).\nAppendix\nIn this appendix, we first present the proof of Theorem 1 of the main paper in Section\nA\n, followed by the proof of the Flow Acceleration Upper Bound in Section\nB\n. In Section\nC\n, we analyze the error term generated by FABO.\nWe complement the main results in our paper with standard deviation in Section\nD\n.\nAdditional details about the simulation environment and algorithm implementation are provided in Sections\nE\nand\nF\n, respectively.\nAppendix A\nProof of Theorem 1\nTheorem 1\n.\nLet\nG\nG\nbe a transformation group acting on both the observation space and the action space. Suppose the initial distribution\np\n0\np_{0}\nis isotropic, i.e.,\np\n0\n‚Äã\n(\ng\n‚Äã\nx\n)\n=\np\n0\n‚Äã\n(\nx\n)\np_{0}(gx)=p_{0}(x)\nfor all\ng\n‚àà\nG\ng\\in G\n, and the velocity network\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\n|\no\n)\nu_{\\theta}(t,x|o)\nis equivariant, i.e.,\nu\nŒ∏\n‚Äã\n(\nt\n,\ng\n‚Äã\nx\n|\ng\n‚Äã\no\n)\n=\ng\n‚Äã\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\n|\no\n)\nu_{\\theta}(t,gx|go)=gu_{\\theta}(t,x|o)\nfor all\ng\n‚àà\nG\ng\\in G\n.\nThen the induced conditional distribution at time\nt\nt\n,\nX\nt\n|\nO\n=\no\nX_{t}|_{O=o}\n, given by the flow ODE\nd\n‚Äã\nx\nt\nd\n‚Äã\nt\n=\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\nt\n|\no\n)\n,\nx\n0\n‚àº\np\n0\n\\frac{dx_{t}}{dt}=u_{\\theta}(t,x_{t}|o),x_{0}\\sim p_{0}\n, satisfies\nX\nt\n|\nO\n=\ng\n‚Äã\no\n=\nd\ng\n‚Äã\n(\nX\nt\n|\nO\n=\no\n)\n,\nt\n‚àà\n[\n0\n,\n1\n]\n\\displaystyle X_{t}\\big|_{O=go}\\stackrel{{\\scriptstyle d}}{{=}}g\\left(X_{t}\\big|_{O=o}\\right),\\quad t\\in[0,1]\n(10)\ni.e., the output distribution is equivariant under the group action.\nProof.\nLet\nŒ¶\nt\n‚Äã\n(\nx\n0\n|\no\n)\n\\Phi_{t}(x_{0}|o)\nbe the solution of the ODE at time\nt\nt\nwith the initial value\nx\n0\nx_{0}\n, conditioned on\no\no\n, so\nx\nt\n=\nŒ¶\nt\n‚Äã\n(\nx\n0\n|\no\n)\nx_{t}=\\Phi_{t}(x_{0}|o)\n. We first show\nŒ¶\nt\n\\Phi_{t}\nis equivariant.\nLet\nx\n^\nt\n=\ng\n‚Äã\nŒ¶\nt\n‚Äã\n(\nx\n0\n|\no\n)\n\\hat{x}_{t}=g\\Phi_{t}(x_{0}|o)\n. Its initial condition is\nx\n^\n0\n=\ng\n‚Äã\nŒ¶\n0\n‚Äã\n(\nx\n0\n|\no\n)\n=\ng\n‚Äã\nx\n0\n\\hat{x}_{0}=g\\Phi_{0}(x_{0}|o)=gx_{0}\n. Its dynamics are:\nd\nd\n‚Äã\nt\n‚Äã\nx\n^\nt\n=\ng\n‚Äã\nd\nd\n‚Äã\nt\n‚Äã\nŒ¶\nt\n‚Äã\n(\nx\n0\n|\no\n)\n=\ng\n‚Äã\nu\nŒ∏\n‚Äã\n(\nt\n,\nŒ¶\nt\n|\no\n)\n=\nu\nŒ∏\n‚Äã\n(\nt\n,\ng\n‚Äã\nŒ¶\nt\n|\ng\n‚Äã\no\n)\n=\nu\nŒ∏\n‚Äã\n(\nt\n,\nx\n^\nt\n|\ng\n‚Äã\no\n)\n.\n\\frac{d}{dt}\\hat{x}_{t}=g\\frac{d}{dt}\\Phi_{t}(x_{0}|o)=gu_{\\theta}(t,\\Phi_{t}|o)=u_{\\theta}(t,g\\Phi_{t}|go)=u_{\\theta}(t,\\hat{x}_{t}|go).\n(11)\nSince\nx\n^\nt\n\\hat{x}_{t}\nand\nŒ¶\nt\n‚Äã\n(\ng\n‚Äã\nx\n0\n|\ng\n‚Äã\no\n)\n\\Phi_{t}(gx_{0}|go)\nshare the same initial condition and ODE dynamics, by uniqueness\n(Perko,\n2013\n)\n, we have\ng\n‚Äã\nŒ¶\nt\n‚Äã\n(\nx\n0\n|\no\n)\n=\nŒ¶\nt\n‚Äã\n(\ng\n‚Äã\nx\n0\n|\ng\n‚Äã\no\n)\ng\\Phi_{t}(x_{0}|o)=\\Phi_{t}(gx_{0}|go)\n.\nUsing this equivariance, we can establish the main result.\nBy definition and the equivariance property, we have the following identity for the random variables:\ng\n‚Äã\n(\nX\nt\n|\nO\n=\no\n)\n=\ng\n‚Äã\nŒ¶\nt\n‚Äã\n(\nX\n0\n|\no\n)\n=\nŒ¶\nt\n‚Äã\n(\ng\n‚Äã\nX\n0\n|\ng\n‚Äã\no\n)\ng(X_{t}|_{O=o})=g\\Phi_{t}(X_{0}|o)=\\Phi_{t}(gX_{0}|go)\n(12)\nThis first equality holds by definition of the flow, and the second is the random variable identity derived from the deterministic equivariance of the flow shown above.\nNow, since the initial distribution\np\n0\np_{0}\nis isotropic, i.e.,\nX\n0\n=\nd\ng\n‚Äã\nX\n0\nX_{0}\\stackrel{{\\scriptstyle d}}{{=}}gX_{0}\n, applying the same deterministic function\nŒ¶\nt\n(\n‚ãÖ\n|\ng\no\n)\n\\Phi_{t}(\\cdot|go)\nto both sides preserves the distributional equality. Therefore,\nŒ¶\nt\n‚Äã\n(\ng\n‚Äã\nX\n0\n|\ng\n‚Äã\no\n)\n=\nd\nŒ¶\nt\n‚Äã\n(\nX\n0\n|\ng\n‚Äã\no\n)\n\\Phi_{t}(gX_{0}|go)\\stackrel{{\\scriptstyle d}}{{=}}\\Phi_{t}(X_{0}|go)\n(13)\nCombining (\n12\n) and (\n13\n), we have a chain of equalities:\ng\n‚Äã\n(\nX\nt\n|\nO\n=\no\n)\n=\nŒ¶\nt\n‚Äã\n(\ng\n‚Äã\nX\n0\n|\ng\n‚Äã\no\n)\n=\nd\nŒ¶\nt\n‚Äã\n(\nX\n0\n|\ng\n‚Äã\no\n)\ng(X_{t}|_{O=o})=\\Phi_{t}(gX_{0}|go)\\stackrel{{\\scriptstyle d}}{{=}}\\Phi_{t}(X_{0}|go)\n(14)\nBy definition, the random variable\nŒ¶\nt\n‚Äã\n(\nX\n0\n|\ng\n‚Äã\no\n)\n\\Phi_{t}(X_{0}|go)\nis the same as\nX\nt\n|\nO\n=\ng\n‚Äã\no\nX_{t}|_{O=go}\n. Thus, we can conclude:\ng\n‚Äã\n(\nX\nt\n|\nO\n=\no\n)\n=\nd\nX\nt\n|\nO\n=\ng\n‚Äã\no\ng(X_{t}|_{O=o})\\stackrel{{\\scriptstyle d}}{{=}}X_{t}|_{O=go}\n(15)\nThis completes the proof.\n‚àé\nAppendix B\nProof of Flow Acceleration Upper Bound (FABO)\nTheorem 2\n.\nAssume that\nu\n‚Äã\n(\nt\n,\nx\n)\nu(t,x)\nis twice continuously differentiable with bounded second derivatives.\nFor any\nx\nt\n‚àº\np\nt\nx_{t}\\sim p_{t}\n, we are interested in two trajectories passing through it:\nthe optimal marginal trajectory denoted as\nx\nt\nx_{t}\nand the linear conditional trajectory denoted as\nx\n~\nt\n=\n(\n1\n‚àí\nt\n)\n‚Äã\nx\n~\n0\n+\nt\n‚Äã\nx\n~\n1\n\\tilde{x}_{t}=(1-t)\\tilde{x}_{0}+t\\tilde{x}_{1}\n.\nThen, when\nŒî\n‚Äã\nt\n\\Delta t\nis small enough,\nùîº\n‚Äã\n‚Äñ\nu\n‚Äã\n(\nt\n,\nx\nt\n)\n‚àí\nu\n‚Äã\n(\nt\n+\nŒî\n‚Äã\nt\n,\nx\nt\n+\nŒî\n‚Äã\nt\n)\n‚Äñ\n2\n2\n‚â§\nùîº\n‚Äã\n‚Äñ\nu\n‚Äã\n(\nt\n,\nx\n~\nt\n)\n‚àí\nu\n‚Äã\n(\nt\n+\nŒî\n‚Äã\nt\n,\nx\n~\nt\n+\nŒî\n‚Äã\nt\n)\n‚Äñ\n2\n2\n.\n\\mathbb{E}\\|u(t,x_{t})-u(t+\\Delta t,x_{t+\\Delta t})\\|_{2}^{2}\\leq\\mathbb{E}\\|u(t,\\tilde{x}_{t})-u(t+\\Delta t,\\tilde{x}_{t+\\Delta t})\\|_{2}^{2}.\n(16)\nProof.\nSince two trajectories intersect at the same state at time\nt\nt\n, we have\nx\nt\n=\nx\n~\nt\nx_{t}=\\tilde{x}_{t}\n.\nùîº\n‚Äã\n‚Äñ\nu\n‚Äã\n(\nt\n,\nx\nt\n)\n‚àí\nu\n‚Äã\n(\nt\n+\nŒî\n‚Äã\nt\n,\nx\nt\n+\nŒî\n‚Äã\nt\n)\n‚Äñ\n2\n2\n\\displaystyle\\mathbb{E}\\|u(t,x_{t})-u(t+\\Delta t,x_{t+\\Delta t})\\|_{2}^{2}\n=\n\\displaystyle=\nùîº\n‚Äã\n‚Äñ\nu\n‚Äã\n(\nt\n,\nx\n~\nt\n)\n‚àí\nu\n‚Äã\n(\nt\n,\nx\n~\nt\n)\n‚àí\n‚àÇ\nu\n‚àÇ\nt\n‚Äã\nŒî\n‚Äã\nt\n‚àí\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\nu\nopt\n‚Äã\n(\nt\n,\nx\n~\nt\n)\n‚Äã\nŒî\n‚Äã\nt\n+\no\n‚Äã\n(\nŒî\n‚Äã\nt\n)\n‚Äñ\n2\n2\n\\displaystyle\\mathbb{E}\\|u(t,\\tilde{x}_{t})-u(t,\\tilde{x}_{t})-\\frac{\\partial{u}}{\\partial t}\\Delta t-\\frac{\\partial{u}}{\\partial x}u^{\\text{opt}}(t,\\tilde{x}_{t})\\Delta t+o(\\Delta t)\\|_{2}^{2}\n(Taylor Expansion)\n=\n\\displaystyle=\nùîº\n‚Äã\n‚Äñ\n‚àÇ\nu\n‚àÇ\nt\n‚Äã\nŒî\n‚Äã\nt\n+\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\nu\nopt\n‚Äã\n(\nt\n,\nx\n~\nt\n)\n‚Äã\nŒî\n‚Äã\nt\n+\no\n‚Äã\n(\nŒî\n‚Äã\nt\n)\n‚Äñ\n2\n2\n\\displaystyle\\mathbb{E}\\|\\frac{\\partial{u}}{\\partial t}\\Delta t+\\frac{\\partial{u}}{\\partial x}u^{\\text{opt}}(t,\\tilde{x}_{t})\\Delta t+o(\\Delta t)\\|_{2}^{2}\n=\n\\displaystyle=\nùîº\n‚à•\n‚àÇ\nu\n‚àÇ\nt\nŒî\nt\n+\n‚àÇ\nu\n‚àÇ\nx\nùîº\n[\nx\n~\n1\n‚àí\nx\n~\n0\n|\nx\n~\nt\n]\nŒî\nt\n+\no\n(\nŒî\nt\n)\n‚à•\n2\n2\n\\displaystyle\\mathbb{E}\\|\\frac{\\partial{u}}{\\partial t}\\Delta t+\\frac{\\partial{u}}{\\partial x}\\mathbb{E}[\\tilde{x}_{1}-\\tilde{x}_{0}|\\tilde{x}_{t}]\\Delta t+o(\\Delta t)\\|_{2}^{2}\n(Eq.2 of\n(Liu et al.,\n2023b\n)\n)\n=\n\\displaystyle=\nùîº\n[\n‚à•\n‚àÇ\nu\n‚àÇ\nt\nŒî\nt\n‚à•\n2\n2\n+\n2\n(\n‚àÇ\nu\n‚àÇ\nt\n)\nT\n‚àÇ\nu\n‚àÇ\nx\nùîº\n[\nx\n~\n1\n‚àí\nx\n~\n0\n|\nx\n~\nt\n]\nŒî\nt\n2\n+\n‚à•\n‚àÇ\nu\n‚àÇ\nx\nùîº\n[\nx\n~\n1\n‚àí\nx\n~\n0\n|\nx\n~\nt\n]\nŒî\nt\n‚à•\n2\n2\n]\n+\no\n(\nŒî\nt\n2\n)\n\\displaystyle\\mathbb{E}\\Biggl[\\|\\frac{\\partial{u}}{\\partial t}\\Delta t\\|_{2}^{2}+2(\\frac{\\partial{u}}{\\partial t})^{T}\\frac{\\partial{u}}{\\partial x}\\mathbb{E}[\\tilde{x}_{1}-\\tilde{x}_{0}|\\tilde{x}_{t}]\\Delta t^{2}+\\|\\frac{\\partial{u}}{\\partial x}\\mathbb{E}[\\tilde{x}_{1}-\\tilde{x}_{0}|\\tilde{x}_{t}]\\Delta t\\|_{2}^{2}\\Biggr]+o(\\Delta t^{2})\n=\n\\displaystyle=\nùîº\n[\nùîº\n[\n‚à•\n‚àÇ\nu\n‚àÇ\nt\nŒî\nt\n‚à•\n2\n2\n+\n2\n(\n‚àÇ\nu\n‚àÇ\nt\n)\nT\n‚àÇ\nu\n‚àÇ\nx\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\nŒî\nt\n2\n+\n‚à•\n‚àÇ\nu\n‚àÇ\nx\nùîº\n[\nx\n~\n1\n‚àí\nx\n~\n0\n|\nx\n~\nt\n]\nŒî\nt\n‚à•\n2\n2\n|\nx\n~\nt\n]\n]\n+\no\n(\nŒî\nt\n2\n)\n\\displaystyle\\mathbb{E}\\Biggl[\\mathbb{E}\\Biggl[\\|\\frac{\\partial{u}}{\\partial t}\\Delta t\\|_{2}^{2}+2(\\frac{\\partial{u}}{\\partial t})^{T}\\frac{\\partial{u}}{\\partial x}(\\tilde{x}_{1}-\\tilde{x}_{0})\\Delta t^{2}+\\|\\frac{\\partial{u}}{\\partial x}\\mathbb{E}[\\tilde{x}_{1}-\\tilde{x}_{0}|\\tilde{x}_{t}]\\Delta t\\|_{2}^{2}\\bigg|\\tilde{x}_{t}\\Biggr]\\Biggr]+o(\\Delta t^{2})\n=\n\\displaystyle=\nùîº\n[\n‚à•\n‚àÇ\nu\n‚àÇ\nt\nŒî\nt\n‚à•\n2\n2\n+\n2\n(\n‚àÇ\nu\n‚àÇ\nt\n)\nT\n‚àÇ\nu\n‚àÇ\nx\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\nŒî\nt\n2\n+\n‚à•\n‚àÇ\nu\n‚àÇ\nx\nùîº\n[\nx\n~\n1\n‚àí\nx\n~\n0\n|\nx\n~\nt\n]\nŒî\nt\n‚à•\n2\n2\n]\n+\no\n(\nŒî\nt\n2\n)\n\\displaystyle\\mathbb{E}\\Biggl[\\|\\frac{\\partial{u}}{\\partial t}\\Delta t\\|_{2}^{2}+2(\\frac{\\partial{u}}{\\partial t})^{T}\\frac{\\partial{u}}{\\partial x}(\\tilde{x}_{1}-\\tilde{x}_{0})\\Delta t^{2}+\\|\\frac{\\partial{u}}{\\partial x}\\mathbb{E}[\\tilde{x}_{1}-\\tilde{x}_{0}|\\tilde{x}_{t}]\\Delta t\\|_{2}^{2}\\Biggr]+o(\\Delta t^{2})\n(Total Expectation)\n=\n\\displaystyle=\nùîº\n‚Äã\n[\n‚Äñ\n‚àÇ\nu\n‚àÇ\nt\n‚Äã\nŒî\n‚Äã\nt\n‚Äñ\n2\n2\n+\n2\n‚Äã\n(\n‚àÇ\nu\n‚àÇ\nt\n)\nT\n‚Äã\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\n‚Äã\nŒî\n‚Äã\nt\n2\n+\n‚Äñ\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\n‚Äã\nŒî\n‚Äã\nt\n‚Äñ\n2\n2\n]\n+\no\n‚Äã\n(\nŒî\n‚Äã\nt\n2\n)\n\\displaystyle\\mathbb{E}\\Biggl[\\|\\frac{\\partial{u}}{\\partial t}\\Delta t\\|_{2}^{2}+2(\\frac{\\partial{u}}{\\partial t})^{T}\\frac{\\partial{u}}{\\partial x}(\\tilde{x}_{1}-\\tilde{x}_{0})\\Delta t^{2}+\\|\\frac{\\partial{u}}{\\partial x}(\\tilde{x}_{1}-\\tilde{x}_{0})\\Delta t\\|_{2}^{2}\\Biggr]+o(\\Delta t^{2})\n‚àí\nùîº\n[\n‚à•\n‚àÇ\nu\n‚àÇ\nx\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\nŒî\nt\n‚à•\n2\n2\n‚àí\n‚à•\n‚àÇ\nu\n‚àÇ\nx\nùîº\n[\nx\n~\n1\n‚àí\nx\n~\n0\n|\nx\n~\nt\n]\nŒî\nt\n‚à•\n2\n2\n]\n\\displaystyle-\\mathbb{E}\\Biggl[\\|\\frac{\\partial{u}}{\\partial x}(\\tilde{x}_{1}-\\tilde{x}_{0})\\Delta t\\|_{2}^{2}-\\|\\frac{\\partial{u}}{\\partial x}\\mathbb{E}[\\tilde{x}_{1}-\\tilde{x}_{0}|\\tilde{x}_{t}]\\Delta t\\|_{2}^{2}\\Biggr]\n=\n\\displaystyle=\nùîº\n‚Äã\n[\n‚Äñ\n‚àÇ\nu\n‚àÇ\nt\n‚Äã\nŒî\n‚Äã\nt\n‚Äñ\n2\n2\n+\n2\n‚Äã\n(\n‚àÇ\nu\n‚àÇ\nt\n)\nT\n‚Äã\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\n‚Äã\nŒî\n‚Äã\nt\n2\n+\n‚Äñ\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\n‚Äã\nŒî\n‚Äã\nt\n‚Äñ\n2\n2\n]\n+\no\n‚Äã\n(\nŒî\n‚Äã\nt\n2\n)\n\\displaystyle\\mathbb{E}\\Biggl[\\|\\frac{\\partial{u}}{\\partial t}\\Delta t\\|_{2}^{2}+2(\\frac{\\partial{u}}{\\partial t})^{T}\\frac{\\partial{u}}{\\partial x}(\\tilde{x}_{1}-\\tilde{x}_{0})\\Delta t^{2}+\\|\\frac{\\partial{u}}{\\partial x}(\\tilde{x}_{1}-\\tilde{x}_{0})\\Delta t\\|_{2}^{2}\\Biggr]+o(\\Delta t^{2})\n‚àí\nùîº\n‚Äã\n[\ntr\n‚Äã\n[\nVar\n‚Äã\n[\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\n‚Äã\nŒî\n‚Äã\nt\n|\nx\n~\nt\n]\n]\n]\n\\displaystyle-\\mathbb{E}\\Biggl[\\mathrm{tr}\\Biggl[\\mathrm{Var}\\Biggl[\\frac{\\partial{u}}{\\partial x}(\\tilde{x}_{1}-\\tilde{x}_{0})\\Delta t\\Bigg|\\tilde{x}_{t}\\Biggr]\\Biggr]\\Biggr]\n=\n\\displaystyle=\nùîº\n‚Äã\n‚Äñ\n‚àÇ\nu\n‚àÇ\nt\n‚Äã\nŒî\n‚Äã\nt\n+\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\n‚Äã\nŒî\n‚Äã\nt\n+\no\n‚Äã\n(\nŒî\n‚Äã\nt\n)\n‚Äñ\n2\n2\n‚àí\nùîº\n‚Äã\n[\ntr\n‚Äã\n[\nVar\n‚Äã\n[\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\n‚Äã\nŒî\n‚Äã\nt\n|\nx\n~\nt\n]\n]\n]\n\\displaystyle\\mathbb{E}\\|\\frac{\\partial{u}}{\\partial t}\\Delta t+\\frac{\\partial{u}}{\\partial x}(\\tilde{x}_{1}-\\tilde{x}_{0})\\Delta t+o(\\Delta t)\\|_{2}^{2}-\\mathbb{E}\\Biggl[\\mathrm{tr}\\Biggl[\\mathrm{Var}\\Biggl[\\frac{\\partial{u}}{\\partial x}(\\tilde{x}_{1}-\\tilde{x}_{0})\\Delta t\\Bigg|\\tilde{x}_{t}\\Biggr]\\Biggr]\\Biggr]\n(17)\n=\n\\displaystyle=\nùîº\n‚Äã\n‚Äñ\nu\n‚Äã\n(\nt\n,\nx\n~\nt\n)\n‚àí\nu\n‚Äã\n(\nt\n+\nŒî\n‚Äã\nt\n,\nx\n~\nt\n+\nŒî\n‚Äã\nt\n)\n‚Äñ\n2\n2\n+\no\n‚Äã\n(\nŒî\n‚Äã\nt\n2\n)\n‚àí\nùîº\n‚Äã\n[\ntr\n‚Äã\n[\nVar\n‚Äã\n[\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\n|\nx\n~\nt\n]\n]\n]\n‚Äã\n(\nŒî\n‚Äã\nt\n2\n)\n.\n\\displaystyle\\mathbb{E}\\|u(t,\\tilde{x}_{t})-u(t+\\Delta t,\\tilde{x}_{t+\\Delta t})\\|_{2}^{2}+o(\\Delta t^{2})-\\mathbb{E}\\Biggl[\\mathrm{tr}\\Biggl[\\mathrm{Var}\\Biggl[\\frac{\\partial{u}}{\\partial x}(\\tilde{x}_{1}-\\tilde{x}_{0})\\Bigg|\\tilde{x}_{t}\\Biggr]\\Biggr]\\Biggr](\\Delta t^{2}).\n(18)\nWhen\nŒî\n‚Äã\nt\n\\Delta t\nis small enough, we have\nùîº\n‚Äã\n‚Äñ\nu\n‚Äã\n(\nt\n,\nx\nt\n)\n‚àí\nu\n‚Äã\n(\nt\n+\nŒî\n‚Äã\nt\n,\nx\nt\n+\nŒî\n‚Äã\nt\n)\n‚Äñ\n2\n2\n‚â§\nùîº\n‚Äã\n‚Äñ\nu\n‚Äã\n(\nt\n,\nx\n~\nt\n)\n‚àí\nu\n‚Äã\n(\nt\n+\nŒî\n‚Äã\nt\n,\nx\n~\nt\n+\nŒî\n‚Äã\nt\n)\n‚Äñ\n2\n2\n\\mathbb{E}\\|u(t,x_{t})-u(t+\\Delta t,x_{t+\\Delta t})\\|_{2}^{2}\\leq\\mathbb{E}\\|u(t,\\tilde{x}_{t})-u(t+\\Delta t,\\tilde{x}_{t+\\Delta t})\\|_{2}^{2}\n(19)\n‚àé\nAppendix C\nAnalysis of the Error Term Generated by FABO\nLemma 1\n.\nLet\nA\nA\nand\nB\nB\nbe two\nn\n√ó\nn\nn\\times n\npositive semidefinite (PSD) matrices (\nA\n,\nB\n‚™∞\n0\nA,B\\succeq 0\n). Let\nŒª\nmin\n‚Äã\n(\nB\n)\n\\lambda_{\\min}(B)\nand\nŒª\nmax\n‚Äã\n(\nB\n)\n\\lambda_{\\max}(B)\ndenote the minimum and maximum eigenvalues of\nB\nB\n, respectively. Then, the trace of their product is bounded as follows:\nŒª\nmin\n‚Äã\n(\nB\n)\n‚ãÖ\ntr\n‚Å°\n(\nA\n)\n‚â§\ntr\n‚Å°\n(\nA\n‚Äã\nB\n)\n‚â§\nŒª\nmax\n‚Äã\n(\nB\n)\n‚ãÖ\ntr\n‚Å°\n(\nA\n)\n\\lambda_{\\min}(B)\\cdot\\operatorname{tr}(A)\\leq\\operatorname{tr}(AB)\\leq\\lambda_{\\max}(B)\\cdot\\operatorname{tr}(A)\nProof.\nLet the spectral decomposition of\nA\n‚™∞\n0\nA\\succeq 0\nbe\nA\n=\nŒª\n1\n‚Äã\nŒæ\n1\n‚Äã\nŒæ\n1\nT\n+\n‚ãØ\n+\nŒª\nn\n‚Äã\nŒæ\nn\n‚Äã\nŒæ\nn\nT\n,\n‚Äñ\nŒæ\ni\n‚Äñ\n2\n2\n=\n1\n,\n‚àÄ\n1\n‚â§\ni\n‚â§\nn\nA=\\lambda_{1}\\xi_{1}\\xi_{1}^{T}+\\cdots+\\lambda_{n}\\xi_{n}\\xi_{n}^{T},\\|\\xi_{i}\\|_{2}^{2}=1,\\forall 1\\leq i\\leq n\nThen\ntr\n‚Äã\n(\nA\n‚Äã\nB\n)\n=\n‚àë\ni\n=\n1\nn\nŒª\ni\n‚Äã\ntr\n‚Äã\n(\nŒæ\ni\n‚Äã\nŒæ\ni\nT\n‚Äã\nB\n)\n=\n‚àë\ni\n=\n1\nn\nŒª\ni\n‚Äã\nŒæ\ni\nT\n‚Äã\nB\n‚Äã\nŒæ\ni\nŒæ\ni\nT\n‚Äã\nŒæ\ni\n‚àà\n[\nŒª\nmin\n‚Äã\n(\nB\n)\n‚Äã\n‚àë\ni\n=\n1\nn\nŒª\ni\n,\nŒª\nmax\n‚Äã\n(\nB\n)\n‚Äã\n‚àë\ni\n=\n1\nn\nŒª\ni\n]\n\\mathrm{tr}(AB)=\\sum_{i=1}^{n}\\lambda_{i}\\mathrm{tr}(\\xi_{i}\\xi_{i}^{T}B)=\\sum_{i=1}^{n}\\lambda_{i}\\frac{\\xi_{i}^{T}B\\xi_{i}}{\\xi_{i}^{T}\\xi_{i}}\\in[\\lambda_{\\min}(B)\\sum_{i=1}^{n}\\lambda_{i},\\lambda_{\\max}(B)\\sum_{i=1}^{n}\\lambda_{i}]\nSo\nŒª\nmin\n‚Äã\n(\nB\n)\n‚ãÖ\ntr\n‚Å°\n(\nA\n)\n‚â§\ntr\n‚Å°\n(\nA\n‚Äã\nB\n)\n‚â§\nŒª\nmax\n‚Äã\n(\nB\n)\n‚ãÖ\ntr\n‚Å°\n(\nA\n)\n\\lambda_{\\min}(B)\\cdot\\operatorname{tr}(A)\\leq\\operatorname{tr}(AB)\\leq\\lambda_{\\max}(B)\\cdot\\operatorname{tr}(A)\n‚àé\nTheorem 3\n.\nWe assume that\n‚àÄ\na\n‚àà\n‚Ñù\nn\n,\n‚Äñ\na\n‚Äñ\n2\n2\n=\n1\n\\forall a\\in\\mathbb{R}^{n},\\|a\\|_{2}^{2}=1\n,\n0\n<\nŒº\n1\n‚â§\nVar\n‚Äã\n[\na\nT\n‚Äã\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\n|\nx\n~\nt\n]\n‚â§\nŒº\n2\n0<\\mu_{1}\\leq\\mathrm{Var}[a^{T}(\\tilde{x}_{1}-\\tilde{x}_{0})|\\tilde{x}_{t}]\\leq\\mu_{2}\n. This assumption, holding for all\nx\n~\nt\n\\tilde{x}_{t}\n, ensures the variance of any linear projection of the\nx\n~\n1\n‚àí\nx\n~\n0\n\\tilde{x}_{1}-\\tilde{x}_{0}\nis uniformly bounded: it remains non-deterministic even given\nx\n~\nt\n\\tilde{x}_{t}\nand finite. Then we have:\nŒº\n1\n‚Äã\nùîº\n‚Äã\n‚Äñ\n‚àÇ\nu\n‚àÇ\nx\n‚Äñ\nF\n2\n‚â§\nùîº\n‚Äã\n[\ntr\n‚Äã\n[\nVar\n‚Äã\n[\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\n|\nx\n~\nt\n]\n]\n]\n‚â§\nŒº\n2\n‚Äã\nùîº\n‚Äã\n‚Äñ\n‚àÇ\nu\n‚àÇ\nx\n‚Äñ\nF\n2\n\\mu_{1}\\mathbb{E}\\|\\frac{\\partial u}{\\partial x}\\|_{F}^{2}\\leq\\mathbb{E}\\Biggl[\\mathrm{tr}\\Biggl[\\mathrm{Var}\\Biggl[\\frac{\\partial{u}}{\\partial x}(\\tilde{x}_{1}-\\tilde{x}_{0})\\Bigg|\\tilde{x}_{t}\\Biggr]\\Biggr]\\Biggr]\\leq\\mu_{2}\\mathbb{E}\\|\\frac{\\partial u}{\\partial x}\\|_{F}^{2}\nProof.\nùîº\n‚Äã\n[\ntr\n‚Äã\n[\nVar\n‚Äã\n[\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\n(\nx\n~\n1\n‚àí\nx\n~\n0\n)\n|\nx\n~\nt\n]\n]\n]\n\\displaystyle\\mathbb{E}\\Biggl[\\mathrm{tr}\\Biggl[\\mathrm{Var}\\Biggl[\\frac{\\partial{u}}{\\partial x}(\\tilde{x}_{1}-\\tilde{x}_{0})\\Bigg|\\tilde{x}_{t}\\Biggr]\\Biggr]\\Biggr]\n(20)\n=\n\\displaystyle=\nùîº\n‚Äã\n[\ntr\n‚Äã\n[\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\nVar\n‚Äã\n[\nx\n~\n1\n‚àí\nx\n~\n0\n|\nx\n~\nt\n]\n‚Äã\n[\n‚àÇ\nu\n‚àÇ\nx\n]\nT\n]\n]\n\\displaystyle\\mathbb{E}\\Biggl[\\mathrm{tr}\\Biggl[\\frac{\\partial{u}}{\\partial x}\\text{Var}[\\tilde{x}_{1}-\\tilde{x}_{0}|\\tilde{x}_{t}][\\frac{\\partial{u}}{\\partial x}]^{T}\\Biggr]\\Biggr]\n(21)\n=\n\\displaystyle=\nùîº\n‚Äã\n[\ntr\n‚Äã\n[\n[\n‚àÇ\nu\n‚àÇ\nx\n]\nT\n‚Äã\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\nVar\n‚Äã\n[\nx\n~\n1\n‚àí\nx\n~\n0\n|\nx\n~\nt\n]\n]\n]\n\\displaystyle\\mathbb{E}\\Biggl[\\mathrm{tr}\\Biggl[[\\frac{\\partial{u}}{\\partial x}]^{T}\\frac{\\partial{u}}{\\partial x}\\text{Var}[\\tilde{x}_{1}-\\tilde{x}_{0}|\\tilde{x}_{t}]\\Biggr]\\Biggr]\n(22)\nUnder the assumption,\nVar\n‚Äã\n[\nx\n~\n1\n‚àí\nx\n~\n0\n|\nx\n~\nt\n]\n\\text{Var}[\\tilde{x}_{1}-\\tilde{x}_{0}|\\tilde{x}_{t}]\nis a positive definite matrix whose eigenvalues lie in the interval\n[\nŒº\n1\n,\nŒº\n2\n]\n[\\mu_{1},\\mu_{2}]\n. Meanwhile,\n[\n‚àÇ\nu\n‚àÇ\nx\n]\nT\n‚Äã\n‚àÇ\nu\n‚àÇ\nx\n[\\frac{\\partial{u}}{\\partial x}]^{T}\\frac{\\partial{u}}{\\partial x}\nis also positive semidefinite (PSD). Using Lemma\n1\n,\nùîº\n‚Äã\n[\nŒª\nmin\n‚Äã\ntr\n‚Äã\n[\n[\n‚àÇ\nu\n‚àÇ\nx\n]\nT\n‚Äã\n‚àÇ\nu\n‚àÇ\nx\n]\n]\n‚â§\nùîº\n‚Äã\n[\ntr\n‚Äã\n[\n[\n‚àÇ\nu\n‚àÇ\nx\n]\nT\n‚Äã\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\nVar\n‚Äã\n[\nx\n~\n1\n‚àí\nx\n~\n0\n|\nx\n~\nt\n]\n]\n]\n‚â§\nùîº\n‚Äã\n[\nŒª\nmax\n‚Äã\ntr\n‚Äã\n[\n[\n‚àÇ\nu\n‚àÇ\nx\n]\nT\n‚Äã\n‚àÇ\nu\n‚àÇ\nx\n]\n]\n\\mathbb{E}\\Biggl[\\lambda_{\\min}\\mathrm{tr}\\Biggl[[\\frac{\\partial{u}}{\\partial x}]^{T}\\frac{\\partial{u}}{\\partial x}\\Biggr]\\Biggr]\\leq\\mathbb{E}\\Biggl[\\mathrm{tr}\\Biggl[[\\frac{\\partial{u}}{\\partial x}]^{T}\\frac{\\partial{u}}{\\partial x}\\text{Var}[\\tilde{x}_{1}-\\tilde{x}_{0}|\\tilde{x}_{t}]\\Biggr]\\Biggr]\\leq\\mathbb{E}\\Biggl[\\lambda_{\\max}\\mathrm{tr}\\Biggl[[\\frac{\\partial{u}}{\\partial x}]^{T}\\frac{\\partial{u}}{\\partial x}\\Biggr]\\Biggr]\nSince\nùîº\n‚Äã\n[\ntr\n‚Äã\n[\n[\n‚àÇ\nu\n‚àÇ\nx\n]\nT\n‚Äã\n‚àÇ\nu\n‚àÇ\nx\n]\n]\n=\nùîº\n‚Äã\n‚Äñ\n‚àÇ\nu\n‚àÇ\nx\n‚Äñ\nF\n2\n\\mathbb{E}\\Biggl[\\mathrm{tr}\\Biggl[[\\frac{\\partial{u}}{\\partial x}]^{T}\\frac{\\partial{u}}{\\partial x}\\Biggr]\\Biggr]=\\mathbb{E}\\|\\frac{\\partial u}{\\partial x}\\|_{F}^{2}\nand\nŒº\n1\n‚â§\nŒª\nmin\n‚â§\nŒª\nmax\n‚â§\nŒº\n2\n,\n\\mu_{1}\\leq\\lambda_{\\min}\\leq\\lambda_{\\max}\\leq\\mu_{2},\nwe have\nŒº\n1\n‚Äã\nùîº\n‚Äã\n‚Äñ\n‚àÇ\nu\n‚àÇ\nx\n‚Äñ\nF\n2\n‚â§\nùîº\n‚Äã\n[\ntr\n‚Äã\n[\n[\n‚àÇ\nu\n‚àÇ\nx\n]\nT\n‚Äã\n‚àÇ\nu\n‚àÇ\nx\n‚Äã\nVar\n‚Äã\n[\nx\n~\n1\n‚àí\nx\n~\n0\n|\nx\n~\nt\n]\n]\n]\n‚â§\nŒº\n2\n‚Äã\nùîº\n‚Äã\n‚Äñ\n‚àÇ\nu\n‚àÇ\nx\n‚Äñ\nF\n2\n\\mu_{1}\\mathbb{E}\\|\\frac{\\partial u}{\\partial x}\\|_{F}^{2}\\leq\\mathbb{E}\\Biggl[\\mathrm{tr}\\Biggl[[\\frac{\\partial{u}}{\\partial x}]^{T}\\frac{\\partial{u}}{\\partial x}\\text{Var}[\\tilde{x}_{1}-\\tilde{x}_{0}|\\tilde{x}_{t}]\\Biggr]\\Biggr]\\leq\\mu_{2}\\mathbb{E}\\|\\frac{\\partial u}{\\partial x}\\|_{F}^{2}\n‚àé\nAppendix D\nStandard Deviation of Evaluation Results\nThe main evaluation experiments are repeated with three random seeds.\nDue to space limitations, only the average results are reported in the main paper, while the corresponding standard deviations are provided in Table\n5\n.\nStack D1\nStack Three D1\nThreading D2\nSquare D2\nMethod\nObs\nNFE\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\nOurs\nRGB\n1\n94.0\n¬±\n\\pm\n1.6\n100.0\n¬±\n\\pm\n0.0\n100.0\n¬±\n\\pm\n0.0\n48.0\n¬±\n\\pm\n0.0\n73.3\n¬±\n\\pm\n5.0\n92.0\n¬±\n\\pm\n0.0\n31.3\n¬±\n\\pm\n1.9\n36.0\n¬±\n\\pm\n1.6\n48.7\n¬±\n\\pm\n2.5\n20.7\n¬±\n\\pm\n1.9\n44.7\n¬±\n\\pm\n1.9\n67.3\n¬±\n\\pm\n0.9\n3\n88.0\n¬±\n\\pm\n1.6\n100.0\n¬±\n\\pm\n0.0\n100.0\n¬±\n\\pm\n0.0\n49.3\n¬±\n\\pm\n3.4\n76.0\n¬±\n\\pm\n4.3\n94.0\n¬±\n\\pm\n0.0\n30.7\n¬±\n\\pm\n5.2\n42.7\n¬±\n\\pm\n2.5\n53.3\n¬±\n\\pm\n2.5\n20.0\n¬±\n\\pm\n1.6\n45.3\n¬±\n\\pm\n2.5\n70.7\n¬±\n\\pm\n0.9\n5\n86.7\n¬±\n\\pm\n2.5\n100.0\n¬±\n\\pm\n0.0\n100.0\n¬±\n\\pm\n0.0\n50.0\n¬±\n\\pm\n2.8\n78.7\n¬±\n\\pm\n5.0\n93.3\n¬±\n\\pm\n1.9\n30.7\n¬±\n\\pm\n1.9\n41.3\n¬±\n\\pm\n1.9\n58.0\n¬±\n\\pm\n2.8\n22.0\n¬±\n\\pm\n1.6\n43.3\n¬±\n\\pm\n1.9\n71.3\n¬±\n\\pm\n0.9\nEquiDiff\nRGB\n100\n93.3\n¬±\n\\pm\n0.7\n100.0\n¬±\n\\pm\n0.0\n100.0\n¬±\n\\pm\n0.0\n54.7\n¬±\n\\pm\n5.2\n77.3\n¬±\n\\pm\n1.8\n96.0\n¬±\n\\pm\n1.2\n22.0\n¬±\n\\pm\n1.2\n40.0\n¬±\n\\pm\n1.2\n59.3\n¬±\n\\pm\n1.8\n25.3\n¬±\n\\pm\n8.7\n41.3\n¬±\n\\pm\n9.8\n60.0\n¬±\n\\pm\n4.2\nDP-C\nRGB\n100\n76.0\n¬±\n\\pm\n4.0\n97.3\n¬±\n\\pm\n0.7\n100.0\n¬±\n\\pm\n0.0\n38.0\n¬±\n\\pm\n0.0\n72.0\n¬±\n\\pm\n2.0\n94.0\n¬±\n\\pm\n1.2\n17.3\n¬±\n\\pm\n1.8\n35.3\n¬±\n\\pm\n1.3\n58.7\n¬±\n\\pm\n0.7\n8.0\n¬±\n\\pm\n1.2\n19.3\n¬±\n\\pm\n5.3\n46.0\n¬±\n\\pm\n7.2\nDP-T\nRGB\n100\n51.3\n¬±\n\\pm\n1.8\n82.7\n¬±\n\\pm\n0.7\n98.7\n¬±\n\\pm\n0.7\n16.7\n¬±\n\\pm\n0.7\n41.3\n¬±\n\\pm\n2.9\n84.0\n¬±\n\\pm\n1.2\n10.7\n¬±\n\\pm\n0.7\n18.0\n¬±\n\\pm\n1.2\n40.7\n¬±\n\\pm\n0.7\n4.7\n¬±\n\\pm\n1.8\n11.3\n¬±\n\\pm\n2.4\n44.7\n¬±\n\\pm\n4.7\nDP3\nPCD\n10\n69.3\n¬±\n\\pm\n3.7\n86.7\n¬±\n\\pm\n4.7\n99.3\n¬±\n\\pm\n0.7\n7.3\n¬±\n\\pm\n0.7\n22.7\n¬±\n\\pm\n3.7\n65.3\n¬±\n\\pm\n1.8\n12.0\n¬±\n\\pm\n3.1\n23.3\n¬±\n\\pm\n3.3\n40.0\n¬±\n\\pm\n2.0\n6.7\n¬±\n\\pm\n0.7\n6.0\n¬±\n\\pm\n0.0\n19.3\n¬±\n\\pm\n3.3\nACT\nRGB\n1\n34.7\n¬±\n\\pm\n0.7\n72.7\n¬±\n\\pm\n7.7\n96.0\n¬±\n\\pm\n1.2\n6.0\n¬±\n\\pm\n2.3\n36.7\n¬±\n\\pm\n2.7\n78.0\n¬±\n\\pm\n1.2\n10.0\n¬±\n\\pm\n1.2\n20.7\n¬±\n\\pm\n2.9\n35.3\n¬±\n\\pm\n2.4\n6.0\n¬±\n\\pm\n0.0\n18.0\n¬±\n\\pm\n1.2\n49.3\n¬±\n\\pm\n4.7\nCoffee D2\nThree Pc. Assembly D2\nHammer Cleanup D1\nMug Cleanup D1\nMethod\nObs\nNFE\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\nOurs\nRGB\n1\n64.7\n¬±\n\\pm\n5.2\n80.7\n¬±\n\\pm\n0.9\n78.7\n¬±\n\\pm\n0.9\n10.7\n¬±\n\\pm\n2.5\n35.3\n¬±\n\\pm\n4.1\n60.0\n¬±\n\\pm\n1.6\n74.7\n¬±\n\\pm\n1.9\n75.3\n¬±\n\\pm\n3.4\n77.3\n¬±\n\\pm\n7.5\n50.0\n¬±\n\\pm\n1.6\n64.7\n¬±\n\\pm\n1.9\n66.7\n¬±\n\\pm\n0.9\n3\n66.0\n¬±\n\\pm\n0.0\n80.0\n¬±\n\\pm\n1.6\n84.0\n¬±\n\\pm\n1.6\n10.7\n¬±\n\\pm\n1.9\n38.0\n¬±\n\\pm\n4.3\n69.3\n¬±\n\\pm\n3.4\n72.0\n¬±\n\\pm\n4.3\n75.3\n¬±\n\\pm\n3.4\n84.0\n¬±\n\\pm\n2.8\n50.0\n¬±\n\\pm\n4.3\n64.7\n¬±\n\\pm\n0.9\n70.0\n¬±\n\\pm\n1.6\n5\n67.3\n¬±\n\\pm\n1.9\n79.3\n¬±\n\\pm\n1.9\n82.7\n¬±\n\\pm\n0.9\n10.7\n¬±\n\\pm\n0.9\n42.0\n¬±\n\\pm\n3.3\n70.7\n¬±\n\\pm\n2.5\n74.0\n¬±\n\\pm\n4.3\n74.7\n¬±\n\\pm\n3.8\n82.7\n¬±\n\\pm\n5.7\n50.0\n¬±\n\\pm\n3.3\n68.0\n¬±\n\\pm\n1.6\n70.0\n¬±\n\\pm\n2.8\nEquiDiff\nRGB\n100\n60.0\n¬±\n\\pm\n2.0\n79.3\n¬±\n\\pm\n1.3\n76.0\n¬±\n\\pm\n2.0\n15.3\n¬±\n\\pm\n1.8\n39.3\n¬±\n\\pm\n1.8\n69.3\n¬±\n\\pm\n3.5\n65.3\n¬±\n\\pm\n0.7\n63.3\n¬±\n\\pm\n4.4\n76.7\n¬±\n\\pm\n0.7\n49.3\n¬±\n\\pm\n0.7\n64.0\n¬±\n\\pm\n1.2\n66.7\n¬±\n\\pm\n0.7\nDP-C\nRGB\n100\n44.0\n¬±\n\\pm\n1.2\n66.0\n¬±\n\\pm\n2.3\n78.7\n¬±\n\\pm\n0.7\n4.0\n¬±\n\\pm\n0.0\n6.0\n¬±\n\\pm\n1.2\n30.0\n¬±\n\\pm\n1.2\n52.0\n¬±\n\\pm\n1.2\n58.7\n¬±\n\\pm\n1.3\n73.3\n¬±\n\\pm\n2.4\n42.7\n¬±\n\\pm\n0.7\n58.7\n¬±\n\\pm\n1.3\n65.3\n¬±\n\\pm\n2.4\nDP-T\nRGB\n100\n47.3\n¬±\n\\pm\n0.7\n60.7\n¬±\n\\pm\n1.8\n74.7\n¬±\n\\pm\n2.7\n0.7\n¬±\n\\pm\n0.7\n4.0\n¬±\n\\pm\n0.0\n42.7\n¬±\n\\pm\n1.3\n48.0\n¬±\n\\pm\n1.2\n60.0\n¬±\n\\pm\n1.2\n76.0\n¬±\n\\pm\n1.2\n30.0\n¬±\n\\pm\n1.2\n42.7\n¬±\n\\pm\n2.9\n63.3\n¬±\n\\pm\n0.7\nDP3\nPCD\n10\n34.0\n¬±\n\\pm\n4.0\n45.3\n¬±\n\\pm\n4.1\n68.7\n¬±\n\\pm\n2.4\n0.0\n¬±\n\\pm\n0.0\n0.7\n¬±\n\\pm\n0.7\n3.3\n¬±\n\\pm\n0.7\n54.0\n¬±\n\\pm\n3.1\n70.7\n¬±\n\\pm\n4.1\n86.7\n¬±\n\\pm\n0.7\n21.3\n¬±\n\\pm\n2.7\n32.7\n¬±\n\\pm\n1.8\n52.7\n¬±\n\\pm\n4.4\nACT\nRGB\n1\n19.3\n¬±\n\\pm\n2.4\n33.3\n¬±\n\\pm\n2.4\n64.0\n¬±\n\\pm\n2.3\n0.0\n¬±\n\\pm\n0.0\n3.3\n¬±\n\\pm\n0.7\n24.0\n¬±\n\\pm\n3.1\n38.0\n¬±\n\\pm\n4.2\n54.0\n¬±\n\\pm\n1.2\n70.7\n¬±\n\\pm\n1.3\n23.3\n¬±\n\\pm\n0.7\n31.3\n¬±\n\\pm\n1.3\n56.0\n¬±\n\\pm\n2.0\nKitchen D1\nPick Place D0\nNut Assembly D0\nCoffee Preparation D1\nMethod\nObs\nNFE\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\n100\n200\n1000\nOurs\nRGB\n1\n65.7\n¬±\n\\pm\n2.1\n78.0\n¬±\n\\pm\n2.8\n80.7\n¬±\n\\pm\n4.1\n37.3\n¬±\n\\pm\n2.8\n49.5\n¬±\n\\pm\n3.1\n67.3\n¬±\n\\pm\n0.2\n59.0\n¬±\n\\pm\n1.4\n82.7\n¬±\n\\pm\n2.6\n94.3\n¬±\n\\pm\n0.5\n75.3\n¬±\n\\pm\n1.9\n74.0\n¬±\n\\pm\n3.3\n70.0\n¬±\n\\pm\n2.8\n3\n72.7\n¬±\n\\pm\n1.9\n80.7\n¬±\n\\pm\n0.9\n80.7\n¬±\n\\pm\n0.9\n40.8\n¬±\n\\pm\n2.7\n61.7\n¬±\n\\pm\n3.3\n85.7\n¬±\n\\pm\n1.6\n61.0\n¬±\n\\pm\n1.6\n86.3\n¬±\n\\pm\n0.5\n96.0\n¬±\n\\pm\n0.0\n80.7\n¬±\n\\pm\n0.9\n81.3\n¬±\n\\pm\n2.5\n88.7\n¬±\n\\pm\n1.9\n5\n72.7\n¬±\n\\pm\n1.9\n81.3\n¬±\n\\pm\n1.9\n83.3\n¬±\n\\pm\n2.5\n41.2\n¬±\n\\pm\n0.5\n65.5\n¬±\n\\pm\n2.1\n86.8\n¬±\n\\pm\n3.1\n62.3\n¬±\n\\pm\n1.7\n87.0\n¬±\n\\pm\n0.8\n97.7\n¬±\n\\pm\n1.7\n82.7\n¬±\n\\pm\n1.9\n82.0\n¬±\n\\pm\n1.6\n87.3\n¬±\n\\pm\n2.5\nEquiDiff\nRGB\n100\n67.3\n¬±\n\\pm\n0.7\n76.7\n¬±\n\\pm\n3.3\n81.3\n¬±\n\\pm\n0.7\n41.7\n¬±\n\\pm\n3.2\n74.2\n¬±\n\\pm\n3.2\n92.0\n¬±\n\\pm\n1.2\n74.0\n¬±\n\\pm\n1.2\n85.0\n¬±\n\\pm\n1.5\n93.7\n¬±\n\\pm\n0.9\n76.7\n¬±\n\\pm\n0.7\n82.7\n¬±\n\\pm\n0.7\n85.3\n¬±\n\\pm\n0.7\nDP-C\nRGB\n100\n66.7\n¬±\n\\pm\n2.4\n84.7\n¬±\n\\pm\n0.7\n86.7\n¬±\n\\pm\n1.8\n35.3\n¬±\n\\pm\n2.2\n65.0\n¬±\n\\pm\n2.8\n82.7\n¬±\n\\pm\n0.6\n54.7\n¬±\n\\pm\n2.3\n68.0\n¬±\n\\pm\n2.6\n83.0\n¬±\n\\pm\n1.5\n65.3\n¬±\n\\pm\n0.7\n62.0\n¬±\n\\pm\n4.2\n58.0\n¬±\n\\pm\n3.1\nDP-T\nRGB\n100\n54.0\n¬±\n\\pm\n2.3\n75.3\n¬±\n\\pm\n0.7\n81.3\n¬±\n\\pm\n2.4\n14.7\n¬±\n\\pm\n1.5\n36.5\n¬±\n\\pm\n1.3\n50.0\n¬±\n\\pm\n6.0\n30.7\n¬±\n\\pm\n5.0\n32.3\n¬±\n\\pm\n5.2\n45.7\n¬±\n\\pm\n5.9\n38.0\n¬±\n\\pm\n2.0\n51.3\n¬±\n\\pm\n1.8\n76.0\n¬±\n\\pm\n6.0\nDP3\nPCD\n10\n44.7\n¬±\n\\pm\n1.8\n71.3\n¬±\n\\pm\n2.4\n91.3\n¬±\n\\pm\n2.4\n11.7\n¬±\n\\pm\n0.9\n15.0\n¬±\n\\pm\n1.7\n34.0\n¬±\n\\pm\n0.0\n15.7\n¬±\n\\pm\n1.3\n23.7\n¬±\n\\pm\n3.4\n57.7\n¬±\n\\pm\n1.9\n10.0\n¬±\n\\pm\n2.3\n22.0\n¬±\n\\pm\n5.3\n63.3\n¬±\n\\pm\n4.1\nACT\nRGB\n1\n37.3\n¬±\n\\pm\n3.5\n60.7\n¬±\n\\pm\n3.5\n87.3\n¬±\n\\pm\n3.5\n7.2\n¬±\n\\pm\n0.9\n17.2\n¬±\n\\pm\n1.1\n50.0\n¬±\n\\pm\n2.9\n42.3\n¬±\n\\pm\n2.9\n63.7\n¬±\n\\pm\n3.5\n84.3\n¬±\n\\pm\n0.9\n32.0\n¬±\n\\pm\n2.0\n46.0\n¬±\n\\pm\n3.1\n64.7\n¬±\n\\pm\n2.4\nTable 5:\nThe performance of our EfficientFlow compared with the baselines in MimicGen. We experiment with 100, 200, and 1000 demos in each environment and report the maximum\ntask success rate among 50 evaluations throughout training. Results averaged over three seeds.\n¬±\n\\pm\nindicates standard deviation.\nAppendix E\nSimulation Environments\nFigure\n3\npresents agent-view observations from the 12 manipulation tasks within the MimicGen\n(Mandlekar et al.,\n2023\n)\nsimulation environment. As illustrated, these tasks vary significantly in complexity and the number of objects involved. For clarity in our analysis, these tasks can be broadly categorized as follows:\n1.\nBasic Tasks (Stack, Stack Three): This category comprises a set of box stacking tasks primarily designed to evaluate the fundamental precision of the robot‚Äôs motion control.\n2.\nContact-Rich Tasks (Square, Threading, Coffee, Three Piece Assembly, Hammer Cleanup, Mug Cleanup): This group includes tasks that necessitate behaviors with substantial physical contact, such as insertions or drawer articulations. These tasks assess the robot‚Äôs capability for fine-grained manipulation and its adaptability to uncertainties arising from physical interactions.ƒÉ\n3.\nLong-Horizon Tasks (Nut Assembly, Kitchen, Pick Place, Coffee Preparation): These tasks require the sequential execution of multiple distinct behaviors, thereby testing the stability of the robot‚Äôs long-duration movements and its comprehensive ability to perform error recovery when necessary.\nIn our experiments, both agent-view and eye-in-hand image observations were captured at a resolution of\n84\n√ó\n84\n84\\times 84\npixels with 3 color channels (RGB). Point cloud observations consisted of 1024 points, with each point represented by 6 features (XYZ coordinates and RGB color).\nFigure 3\n:\nEnvironment diagrams depicting the MimicGen\n(Mandlekar et al.,\n2023\n)\nsimulation experiments. The image sequences for each task, presented from left to right, illustrate the progression from the initial state to the final completion of the respective tasks.\nFigure 4\n:\nThe reset distributions for each task in MimicGen\n(Mandlekar et al.,\n2023\n)\nsimulation experiments.\nAppendix F\nImplementation Details\nF.1\nNetwork Architecture\nFor the network architecture, an equivariant ResNet-18 is employed to encode the agent-view images, yielding an output dimensionality of\n256\n√ó\n8\n256\\times 8\n. Concurrently, images from the hand camera are processed by a standard non-equivariant ResNet-18, resulting in a 256-dimensional feature vector. These visual features, in conjunction with proprioceptive robot state information, are subsequently fused and compressed via an equivariant layer, producing a combined embedding of\n256\n√ó\n8\n256\\times 8\ndimensions.\nFollowing this, a time step\nt\n‚àà\n[\n0\n,\n1\n]\nt\\in[0,1]\nis randomly initialized, along with an initial noise action sequence\nx\n0\nx_{0}\nsampled from a prior distribution. The intermediate action state\nx\nt\nx_{t}\nat time step\nt\nt\nis obtained through linear interpolation between\nx\n0\nx_{0}\nand the target action sequence\nx\n1\nx_{1}\n. This\nx\nt\nx_{t}\nis then encoded by an equivariant action encoder into a\n64\n√ó\n8\n64\\times 8\ndimensional action embedding.\nThe aforementioned embeddings serve as conditioning inputs for a 1D-UNet. This network, featuring hidden layer dimensions of\n[\n512\n,\n1024\n,\n2048\n]\n[512,1024,2048]\n, predicts a\n64\n√ó\n8\n64\\times 8\ndimensional vector. Finally, this vector is equivariantly decoded to generate the velocity prediction\nu\nŒ∏\nu_{\\theta}\n. The terminal action trajectory is then computed using the Euler method.\nF.2\nTraining Details\nWe train our models with the AdamW\n(Loshchilov & Hutter,\n2019\n)\noptimizer with a learning rate of\n10\n‚àí\n4\n10^{-4}\nand weight decay of\n10\n‚àí\n6\n10^{-6}\n(the learning rate in Coffee Preparation D1, Pick Place D0, and Hammer Cleanup D1 tasks is 0.001). We use a cosine learning rate scheduler with 500 warm-up steps. We conducted training on two types of graphics cards, 4090 and A100. The batch size we used is 80. For different tasks in MimicGen, the training on 4090 requires 23 to 82 hours, respectively. During training, the model receives the two most recent historical observations at each step. A single model output consists of an action sequence spanning 16 time steps, of which the\n[\n1\n,\n8\n]\n[1,8]\nsteps are executed. The total number of training steps was kept consistent across experiments with varying numbers of demonstrations.\nFor each different number of demos (100, 200, 1000), we maintain roughly the same number of training steps by training for 50000/n epochs, where n is the number of demos. Evaluations are conducted every 1000/n epochs (50 evaluations in total).\nFor baselines\n(Wang et al.,\n2024\n; Chi et al.,\n2023\n; Ze et al.,\n2024\n)\n, we adopted the hyperparameter configurations reported in their original publication, except that we use the same action sequence\nlength (16 for training and 8 for evaluation) in DP3\n(Ze et al.,\n2024\n)\nas\n(Wang et al.,\n2024\n; Chi et al.,\n2023\n)\nand our method. For the ACT\n(Zhao et al.,\n2023\n)\n, we follow the hyperparameters provided in the prior work, except that we use a chunk size of 10, a KL weight of 10, a batch size of 64 with a learning rate of\n5\n√ó\n10\n‚àí\n5\n5\\times 10^{-5}\n, and no temporal aggregation, following the tuning tips provided by the authors.\nF.3\nEvaluation Strategy\nDuring the evaluation phase, the model similarly processes the two most recent observations. To enhance temporal consistency, five independent initial noise action sequences are randomly sampled and processed in parallel by the network. These sequences undergo an iterative inference process for 1, 3, or 5 steps (NFE=1, 3, or 5), resulting in five distinct candidate action trajectories.\nTo ensure smooth action transitions, the Euclidean distance is computed between each newly generated candidate trajectory and the terminal segment of the previously predicted trajectory. Specifically, this involves comparing the last 7 steps of the previous trajectory with the initial 1-8 steps of the current candidate trajectory. The candidate trajectory exhibiting the smallest Euclidean distance is then selected for execution. Furthermore, to encourage exploration, approximately every 10 predictions, a trajectory is chosen randomly from the candidates instead of always selecting the one with the smoothest transition.\nAppendix G\nAdditional Analysis\nG.1\nHyperparameter Sensitivity Analysis for\nŒª\n\\lambda\nThe FABO module is critical for the model‚Äôs performance. Given that its influence is modulated by the hyperparameter\nŒª\n\\lambda\n, we performed a sensitivity analysis on the formulation of\nŒª\n\\lambda\nfor the Mug Cleanup D1 task, which is particularly sensitive to this component. The formulation adopted in this work is\nŒª\n=\n(\n1\n‚àí\nt\n)\n2\n\\lambda=(1-t)^{2}\n. We tested several alternative formulations to validate this choice in Table\n6\n.\nThe result reveals two key insights. First, the time-varying characteristic is essential, as replacing it with a constant schedule degrades the success rate from 50% to 42.0%. Second, the model exhibits significant robustness to the scale of this formulation: multiplying the original schedule by factors of 0.5, 1, or 2 yields comparable results. This insensitivity suggests that precise calibration of the magnitude is unnecessary, effectively easing the hyperparameter tuning overhead.\nHyperparameter Setting for\nŒª\n\\lambda\nMean Success Rate (%)\nTime-Varying Formulations\n0.5\n‚Äã\n(\n1\n‚àí\nt\n)\n2\n0.5(1-t)^{2}\n48.0\n¬±\n1.6\n48.0\\pm 1.6\n(\n1\n‚àí\nt\n)\n2\n(1-t)^{2}\n50.0\n¬±\n1.6\n50.0\\pm 1.6\n2\n‚Äã\n(\n1\n‚àí\nt\n)\n2\n2(1-t)^{2}\n51.3\n¬±\n3.8\n51.3\\pm 3.8\nConstant Formulations\n0.5\n0.5\n42.0\n¬±\n2.8\n42.0\\pm 2.8\nTable 6\n:\nSensitivity analysis of the hyperparameter\nŒª\n\\lambda\non the Mug Cleanup D1 task. The maximum task success rate among 50 evaluations throughout training and the standard deviations are reported.\nG.2\nAnalysis of Trajectory Quality\nTo quantify trajectory smoothness, we measured the rate of change in velocity at 500 sampled timesteps on the Stack D1 task. EfficientFlow exhibits a mean velocity change of 0.103 (std: 0.088), representing a significant reduction of 24.3% compared to the NoAcc baseline (mean: 0.136, std: 0.133).\nAppendix H\nMulti-Modal Extensions and Generalization\nH.1\nMulti-Modal Performance in MimicGen\nTo further investigate the adaptability and potential of our core architecture, we extend EfficientFlow to incorporate 3D geometric information through a voxel-based representation.\nWe implemented a voxel-based variant of EfficientFlow and compared it against our original RGB-based model from the main paper. For context, we also include results from a strong point-cloud-based method, Flowpolicy\n(Zhang et al.,\n2025\n)\n, and its baseline, DP3\n(Ze et al.,\n2024\n)\n. The evaluation was conducted in the MimicGen environment on five tasks (Stack D1, Threading D2, Square D2, Stack Three D1, and Three Pc. Asse. D2), with each model trained on 100 demonstrations. We report the mean of maximum success rates over three random seeds in Table\n7\n.\nThe Voxel-based EfficientFlow achieves superior performance by leveraging richer spatial perception and explicit 3D geometry. This demonstrates that our strategy effectively generalizes across different input modalities. However, the acquisition overhead of 3D data‚Äîranging from sensor cost to real-time processing‚Äîposes a barrier to real-world deployment. Consequently, while Voxels offer peak performance, our RGB variant remains a vital solution for scenarios where simplicity and hardware accessibility are prioritized.\nMethod\nStack\nThreading\nSquare\nStack 3\n3Pc. Asm.\nAverage\nDP3\n69.3\n¬±\n\\pm\n3.7\n12.0\n¬±\n\\pm\n3.1\n6.7\n¬±\n\\pm\n0.7\n7.3\n¬±\n\\pm\n0.7\n0.0\n¬±\n\\pm\n0.0\n19.1\nFlowPolicy\n72.0\n¬±\n\\pm\n7.1\n13.3\n¬±\n\\pm\n1.9\n6.0\n¬±\n\\pm\n1.6\n10.0\n¬±\n\\pm\n1.6\n0.0\n¬±\n\\pm\n0.0\n20.3\nOurs(RGB)\n94.0\n¬±\n\\pm\n1.6\n31.3\n¬±\n\\pm\n1.9\n20.7\n¬±\n\\pm\n1.9\n48.0\n¬±\n\\pm\n0.0\n10.7\n¬±\n\\pm\n2.5\n41.0\nOurs(Voxel)\n93.3\n¬±\n\\pm\n0.9\n41.3\n¬±\n\\pm\n0.9\n33.3\n¬±\n\\pm\n0.9\n67.3\n¬±\n\\pm\n6.2\n20.0\n¬±\n\\pm\n3.3\n51.0\nTable 7\n:\nMulti-modal performance comparison in the MimicGen environment. We report the mean success rates (%) and standard deviations over three random seeds. Our EfficientFlow framework, in both RGB and Voxel configurations, shows superior performance.\nH.2\nRobomimic Experiment\nTo further validate the generalization capabilities of our method beyond the training environment, we conducted experiments on the Robomimic\n(Mandlekar et al.,\n2021\n)\nbenchmark. We trained EfficientFlow and Diffusion Policy\n(Chi et al.,\n2023\n)\nusing only 20 expert demonstrations for four proficient-human (ph) single-arm tasks. Other hyperparameters mirror those used in our MimicGen experiment.\nDue to the limited randomness in the initial state distributions of these tasks, the data efficiency gains stemming from equivariance are less pronounced compared to the MimicGen experiments.\nNevertheless, EfficientFlow consistently outperforms the baseline across the majority of tasks, securing a superior average success rate.\nMethod\nTool hang\nCan\nLift\nSquare\nAverage\nDP-C\n15.3\n¬±\n\\pm\n2.5\n67.3\n¬±\n\\pm\n5.0\n100.0\n¬±\n\\pm\n0.0\n42.7\n¬±\n\\pm\n4.7\n56.3\nOurs\n16.7\n¬±\n\\pm\n5.7\n90.7\n¬±\n\\pm\n0.9\n100.0\n¬±\n\\pm\n0.0\n44.0\n¬±\n\\pm\n5.9\n62.9\nTable 8\n:\nPerformance comparison on Robomimic tasks. We report the maximum\ntask success rate among 50 evaluations throughout training and standard deviations over three random seeds.\nReferences\nBjorck et al. (2025)\nJohan Bjorck, Fernando Casta√±eda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al.\nGr00t n1: An open foundation model for generalist humanoid robots.\narXiv preprint arXiv:2503.14734\n, 2025.\nBlack et al. (2024)\nKevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al.\nœÄ\n0\n\\pi_{0}\n: A vision-language-action flow model for general robot control.\narXiv preprint arXiv:2410.24164\n, 2024.\nCesa et al. (2022)\nGabriele Cesa, Leon Lang, and Maurice Weiler.\nA program to build e (n)-equivariant steerable cnns.\nIn\nInternational Conference on Learning Representations (ICLR)\n, 2022.\nChi et al. (2023)\nCheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song.\nDiffusion policy: Visuomotor policy learning via action diffusion.\nThe International Journal of Robotics Research\n, 42, 2023.\nGao et al. (2025)\nDechen Gao, Boqi Zhao, Andrew Lee, Ian Chuang, Hanchu Zhou, Hang Wang, Zhe Zhao, Junshan Zhang, and Iman Soltani.\nVita: Vision-to-action flow matching policy.\narXiv preprint arXiv:2507.13231\n, 2025.\nGeng et al. (2025)\nZhengyang Geng, Mingyang Deng, Xingjian Bai, J Zico Kolter, and Kaiming He.\nMean flows for one-step generative modeling.\nIn\nAdvances in Neural Information Processing Systems (NeurIPS)\n, 2025.\nHo et al. (2020)\nJonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising diffusion probabilistic models.\nIn\nAdvances in Neural Information Processing Systems (NeurIPS)\n, 2020.\nHuang et al. (2022)\nHaojie Huang, Dian Wang, Robin Walters, and Robert Platt.\nEquivariant transporter network.\nIn\nProceedings of Robotics: Science and Systems (RSS)\n, 2022.\nHuang et al. (2023)\nHaojie Huang, Dian Wang, Xupeng Zhu, Robin Walters, and Robert Platt.\nEdge grasp network: A graph-based se (3)-invariant approach to grasp detection.\nIn\nProceedings of the IEEE International Conference on Robotics and Automation (ICRA)\n, 2023.\nHuang et al. (2024)\nHaojie Huang, Owen Howell, Xupeng Zhu, Dian Wang, Robin Walters, and Robert Platt.\nFourier transporter: Bi-equivariant robotic manipulation in 3d.\nIn\nInternational Conference on Learning Representations (ICLR)\n, 2024.\nJia et al. (2023)\nMingxi Jia, Dian Wang, Guanang Su, David Klee, Xupeng Zhu, Robin Walters, and Robert Platt.\nSeil: Simulation-augmented equivariant imitation learning.\nIn\nProceedings of the IEEE International Conference on Robotics and Automation (ICRA)\n, 2023.\nKim et al. (2023)\nSeungyeon Kim, Byeongdo Lim, Yonghyeon Lee, and Frank C Park.\nSe (2)-equivariant pushing dynamics models for tabletop object manipulations.\nIn\nConference on Robot Learning (CoRL)\n, 2023.\nLipman et al. (2023)\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.\nFlow matching for generative modeling.\nIn\nInternational Conference on Learning Representations (ICLR)\n, 2023.\nLiu et al. (2023a)\nShiqi Liu, Mengdi Xu, Peide Huang, Xilun Zhang, Yongkang Liu, Kentaro Oguchi, and Ding Zhao.\nContinual vision-based reinforcement learning with group symmetries.\nIn\nConference on Robot Learning (CoRL)\n, 2023a.\nLiu et al. (2023b)\nXingchao Liu, Chengyue Gong, and Qiang Liu.\nFlow straight and fast: Learning to generate and transfer data with rectified flow.\nIn\nInternational Conference on Learning Representations (ICLR)\n, 2023b.\nLoshchilov & Hutter (2019)\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\nIn\nInternational Conference on Learning Representations (ICLR)\n, 2019.\nMandlekar et al. (2021)\nAjay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Mart√≠n-Mart√≠n.\nWhat matters in learning from offline human demonstrations for robot manipulation.\nIn\nConference on Robot Learning (CoRL)\n, 2021.\nMandlekar et al. (2023)\nAjay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and Dieter Fox.\nMimicgen: A data generation system for scalable robot learning using human demonstrations.\nIn\nConference on Robot Learning (CoRL)\n, 2023.\nNguyen et al. (2023)\nHai Huu Nguyen, Andrea Baisero, David Klee, Dian Wang, Robert Platt, and Christopher Amato.\nEquivariant reinforcement learning under partial observability.\nIn\nConference on Robot Learning (CoRL)\n, 2023.\nPan et al. (2023)\nChuer Pan, Brian Okorn, Harry Zhang, Ben Eisner, and David Held.\nTax-pose: Task-specific cross-pose estimation for robot manipulation.\nIn\nConference on Robot Learning (CoRL)\n, 2023.\nPerko (2013)\nLawrence Perko.\nDifferential equations and dynamical systems\n, volume 7.\nSpringer Science & Business Media, 2013.\nRana et al. (2025)\nKrishan Rana, Robert Lee, David Pershouse, and Niko Suenderhauf.\nImle policy: Fast and sample efficient visuomotor policy learning via implicit maximum likelihood estimation.\nIn\nProceedings of Robotics: Science and Systems (RSS)\n, 2025.\nReuss et al. (2025)\nMoritz Reuss, Hongyi Zhou, Marcel R√ºhle, √ñmer Erdin√ß Yaƒümurlu, Fabian Otto, and Rudolf Lioutikov.\nFlower: Democratizing generalist robot policies with efficient vision-language-action flow policies.\nIn\nConference on Robot Learning (CoRL)\n, 2025.\nRyu et al. (2023)\nHyunwoo Ryu, Hong-in Lee, Jeong-Hoon Lee, and Jongeun Choi.\nEquivariant descriptor fields: Se (3)-equivariant energy-based models for end-to-end visual robotic manipulation learning.\nIn\nInternational Conference on Learning Representations (ICLR)\n, 2023.\nSheng et al. (2025)\nJuyi Sheng, Ziyi Wang, Peiming Li, and Mengyuan Liu.\nMp1: Mean flow tames policy learning in 1-step for robotic manipulation.\narXiv preprint arXiv:2507.10543\n, 2025.\nSimeonov et al. (2022)\nAnthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B Tenenbaum, Alberto Rodriguez, Pulkit Agrawal, and Vincent Sitzmann.\nNeural descriptor fields: Se (3)-equivariant object representations for manipulation.\nIn\nProceedings of the IEEE International Conference on Robotics and Automation (ICRA)\n, 2022.\nSimeonov et al. (2023)\nAnthony Simeonov, Yilun Du, Yen-Chen Lin, Alberto Rodriguez Garcia, Leslie Pack Kaelbling, Tom√°s Lozano-P√©rez, and Pulkit Agrawal.\nSe (3)-equivariant relational rearrangement with neural descriptor fields.\nIn\nConference on Robot Learning (CoRL)\n, 2023.\nSohl-Dickstein et al. (2015)\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.\nDeep unsupervised learning using nonequilibrium thermodynamics.\nIn\nInternational Conference on Machine Learning (ICML)\n, 2015.\nWang et al. (2022a)\nDian Wang, Mingxi Jia, Xupeng Zhu, Robin Walters, and Robert Platt.\nOn-robot learning with equivariant models.\nIn\nConference on Robot Learning (CoRL)\n, 2022a.\nWang et al. (2022b)\nDian Wang, Robin Walters, and Robert Platt.\nSo(2)-equivariant reinforcement learning.\nIn\nInternational Conference on Learning Representations (ICLR)\n, 2022b.\nWang et al. (2022c)\nDian Wang, Robin Walters, Xupeng Zhu, and Robert Platt.\nEquivariant\nq\nq\nlearning in spatial action spaces.\nIn\nConference on Robot Learning (CoRL)\n, 2022c.\nWang et al. (2024)\nDian Wang, Stephen Hart, David Surovik, Tarik Kelestemur, Haojie Huang, Haibo Zhao, Mark Yeatman, Jiuguang Wang, Robin Walters, and Robert Platt.\nEquivariant diffusion policy.\nIn\nConference on Robot Learning (CoRL)\n, 2024.\nYang et al. (2024a)\nJingyun Yang, Ziang Cao, Congyue Deng, Rika Antonova, Shuran Song, and Jeannette Bohg.\nEquibot: Sim (3)-equivariant diffusion policy for generalizable and data efficient learning.\nIn\nConference on Robot Learning (CoRL)\n, 2024a.\nYang et al. (2024b)\nLing Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, and Bin Cui.\nConsistency flow matching: Defining straight flows with velocity consistency.\nIn\nInternational Conference on Machine Learning (ICML)\n, 2024b.\nZe et al. (2024)\nYanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu.\n3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations.\nIn\nProceedings of Robotics: Science and Systems (RSS)\n, 2024.\nZhang et al. (2025)\nQinglun Zhang, Zhen Liu, Haoqiang Fan, Guanghui Liu, Bing Zeng, and Shuaicheng Liu.\nFlowpolicy: Enabling fast and robust 3d flow-based policy via consistency flow matching for robot manipulation.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence (AAAI)\n, 2025.\nZhao et al. (2023)\nTony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn.\nLearning fine-grained bimanual manipulation with low-cost hardware.\nIn\nProceedings of Robotics: Science and Systems (RSS)\n, 2023.\nZhou et al. (2019)\nYi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li.\nOn the continuity of rotation representations in neural networks.\nIn\nProceedings of the IEEE/CVF Conference on computer vision and pattern recognition (CVPR)\n, 2019.\nZhu & Wang (2022)\nXupeng Zhu and Dian Wang.\nSample efficient grasp learning using equivariant models.\nIn\nProceedings of Robotics: Science and Systems (RSS)\n, 2022.\nZhu et al. (2023)\nXupeng Zhu, Dian Wang, Guanang Su, Ondrej Biza, Robin Walters, and Robert Platt.\nOn robot grasp learning using equivariant models.\nAutonomous Robots\n, 47, 2023.",
  "preview_text": "Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.\n\n\\contribution\n[*]Equal Contribution\n\\contribution\n[‚Ä†]Corresponding author\nEfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI\nJianlei Chang\nRuofeng Mei\nWei Ke\nXiangyu Xu\nXi‚Äôan Jiaotong University\nAbstract\nGenerative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks.\nHowever, existing generative policies often struggle with\ndata inefficiency\n, requiring large-scale demonstrations, and\nsampling inefficiency\n, incurring slow action generation during inference.\nWe introduce EfficientFlow, a unified framework for effi",
  "is_relevant": null,
  "relevance_score": 0.0,
  "extracted_keywords": [],
  "one_line_summary": "",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "2025-12-01T18:59:59Z",
  "created_at": "2026-01-08T10:08:15.294801",
  "updated_at": "2026-01-08T10:08:15.294811"
}