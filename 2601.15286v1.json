{
    "id": "2601.15286v1",
    "title": "Iterative Refinement Improves Compositional Image Generation",
    "authors": [
        "Shantanu Jaiswal",
        "Mihir Prabhudesai",
        "Nikash Bhardwaj",
        "Zheyang Qin",
        "Amir Zadeh",
        "Chuan Li",
        "Katerina Fragkiadaki",
        "Deepak Pathak"
    ],
    "abstract": "文本到图像（T2I）模型已取得显著进展，但在处理需要同时涉及多个对象、关系和属性的复杂提示时仍面临挑战。现有的推理时策略，如使用验证器进行并行采样或单纯增加去噪步骤，虽能提升提示对齐效果，但对于需要满足多重约束的丰富组合场景仍显不足。受大语言模型中思维链推理成功的启发，我们提出一种迭代式测试时策略：T2I模型在视觉语言模型作为循环评判者的反馈引导下，通过多步骤逐步优化生成结果。该方法简洁易行，无需外部工具或先验知识，可灵活应用于各类图像生成器和视觉语言模型。实验表明，该方法在多个基准测试中持续提升图像生成效果：相较于计算量匹配的并行采样，在ConceptMix（k=7）上的全正确率提升16.9%，在T2I-CompBench（3D空间类别）上提升13.8%，在Visual Jenga场景解构任务上提升12.5%。除量化提升外，迭代优化通过将复杂提示分解为序列化修正，生成更忠实于提示的图像——在人工评估中，58.7%的参与者更青睐本方法，而选择并行基线方法的比例为41.3%。这些发现共同表明，迭代式自我纠正是组合图像生成领域具有广泛适用性的核心原则。完整结果与可视化案例详见https://iterative-img-gen.github.io/。",
    "url": "https://arxiv.org/abs/2601.15286v1",
    "html_url": "https://arxiv.org/html/2601.15286v1",
    "html_content": "Iterative Refinement Improves Compositional Image Generation\nShantanu Jaiswal\n1\nMihir Prabhudesai\n1\nNikash Bhardwaj\n1\nZheyang Qin\n1\nAmir Zadeh\n2\nChuan Li\n2\nKaterina Fragkiadaki\n1\nDeepak Pathak\n1\n1\nCarnegie Mellon University\n2\nLambda AI\nAbstract\nText-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a\n16.9\n%\n16.9\\%\nimprovement in all-correct rate on ConceptMix (k=7), a\n13.8\n%\n13.8\\%\nimprovement on T2I-CompBench (3D-Spatial category) and a\n12.5\n%\n12.5\\%\nimprovement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation.\nhttps://iterative-img-gen.github.io/\nFigure 1\n:\nIterative refinement during inference time enables high fidelity generation of complex prompts on which traditional inference-time scaling strategies such as parallel sampling can fail to generate a fully accurate image even at high num. of samples as shown above.\n1\nIntroduction\nLarge language models (LLMs) have achieved remarkable progress in recent years, as a result of simply scaling test-time compute\n[\n30\n,\n4\n,\n26\n]\n. A particularly influential development has been the use of chain-of-thought (CoT) prompting, where models are instructed to “think step by step”\n[\n30\n,\n15\n]\n. Despite its simplicity, this strategy enables models to exhibit sophisticated behaviors such as self-correction, error checking, and iterative refinement, ultimately leading to significant gains on reasoning-intensive benchmarks. These behaviors highlight the potential of LLMs not only as static predictors but as systems that can actively refine their outputs through structured intermediate reasoning.\nThe success of CoT reasoning in LLMs is closely tied to their pre-training data. During training, LLMs are exposed to large volumes of text that naturally contain traces of human step-by-step reasoning – mathematical derivations, logical arguments, and instructional writing. This supervision on the internet implicitly provides the prior that chain-of-thought prompting later exploits, enabling the model to perform multi-step reasoning. By contrast, text-to-image (T2I) models are trained on large-scale datasets of image–caption pairs that lack such structured reasoning traces. As a result, these models do not inherently develop capabilities like self-correction or iterative refinement, instead rely on one-shot generation strategies that limit their robustness in complex settings.\nFigure 2\n:\nOur iterative inference-time strategy achieves strong benefits over computation-matched parallel inference time scaling on multiple state-of-art image generation models.\nIn this work, we investigate how can we enable self-correction in T2I models. Our central idea is to leverage complementary modules that together mimic the iterative reasoning process observed in LLMs. Concretely, our framework integrates four components: (i) a text-to-image (T2I) model to generate an initial image, (ii) a vision-language model (VLM) critic to propose corrections by comparing the generated image with target prompt, (iii) an image editor to apply suggested edits, and (iv) a verifier to evaluate alignment between final image and target prompt. This pipeline allows the model to iteratively refine its outputs rather than relying solely on a single forward pass.\nWe compare our approach against the widely adopted strategy of parallel sampling\n[\n20\n,\n38\n]\n, where multiple images are generated independently and the best one is selected using a verifier. While parallel sampling increases diversity, it does not fundamentally change the underlying generation process, nor does it allow the model to revise or build upon earlier outputs. As a result, it struggles with complex compositional prompts. For example, consider a prompt requiring dozens of concept bindings: if the model’s attention heads cannot jointly resolve all bindings in a single forward pass, the pass@k will remain near zero regardless of how many samples are drawn.\nIn contrast, our approach explicitly reuses intermediate generations and progressively improves them through guided corrections. This factorization allows the model to handle only a subset of bindings at each step, compounding previously resolved components over time. Such sequential, step-by-step refinement—analogous to chain-of-thought reasoning— is crucial for reliably generating highly compositional images.\nFigure\n1\n, highlights the capability of our approach to generate complex compositional prompts. Given the caption on top, parallel sampling simply is unable to build on top of the previous steps thus being unsuccessful even after 4 passes through the generative model. In contrast, iterative refinement successfully generates the final image, while using the same amount of compute. Quantitatively in Figure\n2\n, we demonstrate that this leads to consistent performance improvements: our approach achieves a 16.9% higher all-correct rate on ConceptMix\n[\n34\n]\n(for concept binding=7) and a 13.8% gain on T2I-Bench 3D Spatial category\n[\n13\n]\nrelative to compute-matched parallel sampling.\nAn alternate family of methods—such as GenArtist\n[\n28\n]\nand CompAgent\n[\n29\n]\n—also performs sequential sampling by building on top of previous generations. However, these approaches rely on a large toolbox of auxiliary modules (e.g., layout-to-image models, bounding-box detectors, dragging tools, and object-removal systems). Because these toolchains evolve at different rates and often lag behind foundation model capabilities, the overall pipeline becomes brittle: errors from individual tools can accumulate rather than help the generation process for complex prompts. Other methods such as RPG\n[\n35\n]\nsimilarly show gains via increased test-time compute, but still depend on complex region-wise priors and bespoke pipelines not readily applicable to black-box foundation models.\nIn contrast, with recent advances in VLMs and modern image-editing models, we find that many of these specialized tool-based pipelines are no longer necessary for effective test-time scaling. Across all benchmarks, simply combining a strong VLM critic feedback generator with a standard image-editing model is sufficient to achieve state-of-the-art compositional image generation – without relying on heavy tool stacks or model-specific training and engineering pipelines. As shown in Figure\n5\n, methods such as GenArtist and RPG under-perform substantially in highly compositional settings, whereas our approach delivers a consistent\n∼\n9\n+\n%\n\\sim 9{+}\\%\npoint improvement under matched compute. Further, our framework naturally extends to the recent Visual Jenga scene decomposition task\n[\n2\n]\nas detailed in sec.\n4.3\n.\nOur findings further suggest that self-correction—long recognized as a key ingredient in LLM reasoning—also serves as a powerful inductive principle for generative vision models. Introducing a simple and general refinement pipeline enables behaviors traditionally associated with language models to naturally transfer into image generation, yielding tangible performance gains. More broadly, this work points to the promise of designing generative systems that not only produce outputs but also critique and improve upon them, moving towards a more unified view of reasoning across modalities.\n2\nRelated Work\nText-to-Image Inference-Time Strategies.\nRecent advances in text-to-image (T2I) generation have demonstrated impressive capabilities in producing high-quality and diverse images from natural language prompts\n[\n10\n,\n11\n,\n1\n]\n. However, complex prompts with multiple objects, relations, and fine-grained attributes remain challenging. Inference-time strategies such as classifier-free guidance\n[\n12\n]\n, parallel sampling\n[\n8\n,\n5\n]\n, and grounding-based methods\n[\n18\n,\n19\n]\nimprove prompt fidelity but often fail to scale to richly compositional prompts. Iterative refinement methods, including SDEdit\n[\n22\n]\n, InstructPix2Pix\n[\n3\n]\n, and IterComp\n[\n39\n]\nattempt to progressively improve image alignment with prompts by using multiple generation steps and feedback mechanisms. Human-preference-guided evaluation and optimization, as in\n[\n17\n,\n33\n,\n14\n]\n, further highlight the importance of incorporating adaptive guidance at inference time. T2I models\n[\n24\n,\n32\n,\n25\n,\n16\n]\nand compositional methods such as IterComp\n[\n39\n]\n, RPG\n[\n35\n]\n, GenArtist\n[\n28\n]\n, PARM\n[\n37\n]\n, LLM Diffusion\n[\n19\n]\nand CompAgent\n[\n29\n]\nare related to our method, but either make use of tool-calling, regional generation priors or reinforcement learning objectives to improve compositionality. In contrast, our method is a training free method with simply a VLM-critic utilized in loop with an image generation and editing model, and empirically demonstrates stronger performance benefits across different T2I model families.\nChain-of-Thought Reasoning in Large Language Models.\nChain-of-thought (CoT) prompting has been shown to elicit multi-step reasoning and improve performance on complex language tasks\n[\n30\n,\n27\n,\n36\n]\n. Iterative and self-refinement approaches\n[\n21\n]\nfurther demonstrate that models benefit from decomposing a problem into sequential reasoning steps with feedback loops. Drawing inspiration from these strategies, our method applies a similar iterative reasoning paradigm to T2I generation: the critic functions analogously to a CoT process, first evaluating candidate generations and then issuing targeted refinement prompts, enabling high-fidelity compositional image synthesis.\n3\nMethod\nFigure 3\n:\nGiven a complex text prompt\nX\np\nX_{p}\n, a generator\nG\nG\nproduces an initial image\nI\n0\nI_{0}\n. A test-time verifier\nV\nV\nand critic\nC\nC\n, conditioned on\nX\np\nX_{p}\n, output an action–sub-prompt pair\n(\na\nt\n,\np\nt\n)\n(a_{t},p_{t})\n. The previous image\nI\nt\n−\n1\nI_{t-1}\nand sub-prompt\np\nt\np_{t}\nare fed to an editor\nE\nE\nto yield the next image\nI\nt\nI_{t}\n. This process repeats under an inference-time budget\nB\nB\n, allocated as maximum\nT\nT\niterative rounds over\nM\nM\nparallel streams, until a\nSTOP\naction is emitted or\nB\nB\n(\n=\nT\n×\nM\n=T\\times M\n) is exhausted.\nGiven a complex text prompt\nP\nP\n, our goal is to generate an image\nI\nI\nthat faithfully captures all entailed entities and compositions. We adopt an\niterative inference-time refinement\nscheme in which a generator progressively improves its outputs under critic guidance, subject to an inference-time computational budget\nB\nB\nthat we allocate as\nT\nT\nrefinement rounds across\nM\nM\nparallel streams.\nSetup.\nLet\nG\nG\ndenote a text-to-image generator,\nE\nE\na image-to-image editor,\nV\nV\na verifier that scores alignment between a candidate image and prompt\nP\nP\n, and\nC\nC\na critic that outputs (i) a refinement sub-prompt\np\nt\np_{t}\nto guide subsequent updates, and (ii) an\naction\na\nt\n∈\n{\nSTOP\n,\nBACKTRACK\n,\nRESTART\n,\nCONTINUE\n}\na_{t}\\in\\{\\texttt{STOP},\\texttt{BACKTRACK},\\texttt{RESTART},\\texttt{CONTINUE}\\}\nindicating how the refinement should be applied. We assume an inference-time budget\nB\nB\n, allocated into\nT\nT\nrefinement rounds over\nM\nM\nparallel streams (i.e.,\nB\n=\nT\n×\nM\nB=T\\times M\nunit refinement operations). This parameterization exposes a controllable depth–breadth trade-off, where each unit corresponds to a single call to the text-to-image generator\nG\nG\nor the image-to-image editor\nE\nE\n. The action space of the critic\nC\nC\nis as follows:\n•\nSTOP\n: terminate the process upon completion satisfaction and return the current image.\n•\nBACKTRACK\n: revert to the previous generation\nI\nt\n−\n1\nI_{t-1}\nand refine it using the new sub-prompt\np\nt\np_{t}\n.\n•\nRESTART\n: discard the current trajectory and regenerate from scratch conditioned on\nP\nP\nand new sub-prompt\np\nt\np_{t}\n.\n•\nCONTINUE\n: refine the current best candidate\nI\nt\n∗\nI_{t}^{\\ast}\ndirectly using new sub-prompt\np\nt\np_{t}\n.\nUnlike standard single-shot or naive parallel sampling, our method unfolds over\nT\nT\nrefinement rounds and\nM\nM\nparallel streams under a fixed budget\nB\nB\n, where intermediate generations are evaluated, critiqued, and selectively improved.\nIterative refinement over parallel streams with critic feedback.\nAt\nt\n=\n0\nt=0\n, each parallel stream\nm\nm\ninitializes a candidate\nI\n0\nm\n←\nG\n​\n(\nP\n)\nI_{0}^{m}\\leftarrow G(P)\n, where\nP\nP\nis the user image prompt. At iteration\nt\nt\n, the verifier scores\ns\nt\nm\n←\nV\n​\n(\nI\nt\nm\n,\nP\n)\ns_{t}^{m}\\leftarrow V(I_{t}^{m},P)\nand the critic proposes\n(\na\nt\nm\n,\np\nt\nm\n)\n←\nC\n​\n(\nI\nt\nm\n,\nP\n)\n(a_{t}^{m},p_{t}^{m})\\leftarrow C(I_{t}^{m},P)\n. Depending on\na\nt\nm\na_{t}^{m}\n, we either stop, backtrack to\nI\nt\n−\n1\nm\nI_{t-1}^{m}\n, restart from\nG\n​\n(\nP\n,\np\nt\nm\n)\nG(P,p_{t}^{m})\n, or continue editing\nI\nt\nm\nI_{t}^{m}\nvia\nE\nE\n. The process terminates when a\nSTOP\naction is emitted for all streams or when the budget\nB\nB\n(parameterized by\nT\nT\nand\nM\nM\n) is exhausted.\nThis procedure enables T2I models to decompose complex compositional prompts into a sequence of refinement steps, akin to chain-of-thought reasoning in LLMs. The verifier ensures consistent prompt alignment, while critic provides targeted feedback to correct systematic errors.\nNote that the verifier\nV\nV\nis\nnot\nan oracle or benchmark evaluator; rather, it is a lightweight VLM used solely to provide automatic test-time guidance and improvement signals during refinement. We outline the pseudo-code of our iterative refinement technique in Algorithm Block\n1\n.\nAlgorithm 1\nIterative Image Refinement over Parallel Streams with Critic Feedback\n1:\nInput:\nPrompt\nP\nP\n, generator\nG\nG\n, non-oracle test-time verifier\nV\nV\n, critic\nC\nC\n, parallel streams\nM\nM\n, max iterative rounds\nT\nT\n; inference-time budget\nB\nB\nallocated as\nT\n×\nM\nT\\times M\nunit updates\n2:\nInitialize\n{\nI\n0\nm\n}\nm\n=\n1\nM\n←\n{\nG\n​\n(\nP\n)\n}\nm\n=\n1\nM\n\\{I_{0}^{m}\\}_{m=1}^{M}\\leftarrow\\{G(P)\\}_{m=1}^{M}\n3:\nfor\nt\n=\n1\nt=1\nto\nT\nT\ndo\n4:\nfor\nm\n=\n1\nm=1\nto\nM\nM\nin parallel\ndo\n5:\nScore candidate\ns\nt\nm\n←\nV\n​\n(\nI\nt\nm\n,\nP\n)\ns_{t}^{m}\\leftarrow V(I_{t}^{m},P)\n6:\nCritic outputs\n(\na\nt\nm\n,\np\nt\nm\n)\n←\nC\n​\n(\nI\nt\nm\n,\nP\n)\n(a_{t}^{m},p_{t}^{m})\\leftarrow C(I_{t}^{m},P)\n7:\nif\na\nt\nm\n=\na_{t}^{m}=\nSTOP\nthen\n8:\nMark stream\nm\nm\nas complete\n9:\nelse\nif\na\nt\nm\n=\na_{t}^{m}=\nBACKTRACK\nthen\n10:\nRevert to\nI\nt\n−\n1\nm\nI_{t-1}^{m}\nand refine:\nI\nt\n+\n1\nm\n←\nE\n​\n(\nI\nt\n−\n1\nm\n,\np\nt\nm\n)\nI_{t+1}^{m}\\leftarrow E(I_{t-1}^{m},p_{t}^{m})\n11:\nelse\nif\na\nt\nm\n=\na_{t}^{m}=\nRESTART\nthen\n12:\nReset\nI\nt\n+\n1\nm\n←\nG\n​\n(\nP\n,\np\nt\nm\n)\nI_{t+1}^{m}\\leftarrow G(P,p_{t}^{m})\n13:\nelse\nif\na\nt\nm\n=\na_{t}^{m}=\nCONTINUE\nthen\n14:\nUpdate\nI\nt\n+\n1\nm\n←\nE\n​\n(\nI\nt\nm\n,\np\nt\nm\n)\nI_{t+1}^{m}\\leftarrow E(I_{t}^{m},p_{t}^{m})\n15:\nend\nif\n16:\nend\nfor\n17:\nSelect best across all streams:\nI\nt\n∗\n←\narg\n⁡\nmax\nm\n⁡\ns\nt\nm\nI_{t}^{\\ast}\\leftarrow\\arg\\max_{m}s_{t}^{m}\n18:\nif\nall streams stopped\nthen\n19:\nreturn\nI\nt\n∗\nI_{t}^{\\ast}\n20:\nend\nif\n21:\nend\nfor\n22:\nreturn\nI\nT\n∗\nI_{T}^{\\ast}\n4\nExperiments\nConceptMix full solve rate (%)\nT2I-CompBench VLLM (GPT4o) score (1 to 100)\nModel\nk=1\nk=2\nk=3\nk=4\nk=5\nk=6\nk=7\nSpatial\n3DSpat\nNumer\nShape\nColor\nTexture\nNon-Spat\nCmplex\nQwen Parallel\n92.8\n82.5\n74.3\n69.2\n60.1\n51.2\n49.6\n82.3\n63.1\n87.0\n87.2\n92.6\n96.2\n92.8\n93.4\nQwen Iter (ours)\n96.1\n91.4\n87.0\n82.1\n79.6\n67.4\n64.3\n87.4\n77.3\n91.1\n91.2\n92.4\n95.1\n94.8\n94.8\nQwen Iter.+Par. (ours)\n96.5\n91.7\n87.4\n82.2\n78.9\n71.8\n66.5\n89.4\n76.9\n93.3\n90.1\n92.6\n95.8\n94.7\n95.0\n+3.7\n+9.2\n+13.1\n+13.0\n+18.8\n+20.6\n+16.9\n+7.1\n+13.8\n+6.3\n+2.9\n+0.0\n-0.4\n+1.9\n+1.6\nNano-Banana Parallel\n93.8\n88.8\n86.6\n78.4\n65.8\n61.7\n55.4\n84.7\n81.2\n84.3\n88.5\n89.8\n95.0\n96.8\n91.0\nNano-Banana Iter (ours)\n94.1\n90.4\n87.2\n81.3\n73.5\n64.6\n63.6\n90.6\n87.8\n93.9\n89.9\n89.7\n95.1\n95.8\n94.7\nNano-Banana Iter.+Par. (ours)\n93.8\n91.0\n87.5\n82.8\n71.4\n69.8\n63.7\n91.1\n89.1\n94.1\n88.8\n92.1\n94.8\n96.7\n94.5\n+0.0\n+2.2\n+0.9\n+4.4\n+5.6\n+8.1\n+8.3\n+6.6\n+7.9\n+9.8\n+0.3\n+2.3\n-0.2\n-0.1\n+3.5\nGPT-Image Parallel\n94.2\n89.2\n88.1\n76.7\n71.0\n69.5\n51.3\n87.5\n83.9\n88.6\n88.5\n91.6\n92.5\n95.3\n92.9\nGPT-Image Iterative (ours)\n96.0\n91.4\n90.6\n85.4\n72.0\n69.6\n58.9\n89.6\n90.0\n92.7\n92.1\n91.9\n92.0\n95.5\n93.0\nGPT-Image Iter.+Par. (ours)\n97.7\n94.2\n91.1\n84.6\n79.5\n76.8\n61.9\n91.0\n89.6\n93.2\n90.9\n91.1\n92.3\n95.3\n93.1\n+3.5\n+5.0\n+3.0\n+7.9\n+8.5\n+7.3\n+10.6\n+3.5\n+5.7\n+4.6\n+2.4\n-0.5\n-0.2\n+0.0\n+0.2\nTable 1\n:\nPerformance comparison of parallel sampling, iterative refinement, and combined strategies across three state-of-the-art text-to-image models on ConceptMix\n[\n34\n]\nand T2I-CompBench\n[\n13\n]\n. Our iterative approach (Iter.) and combined iterative+parallel strategy (Iter.+Par.) consistently outperform traditional parallel-sampling baselines, with gains most pronounced on complex compositional tasks (ConceptMix k=4–7) and precise spatial and numeric reasoning (T2I-CompBench spatial, 3D spatial, and numeracy categories).\nWe conduct experiments with three state-of-the-art text-to-image model families – Qwen-Image\n[\n32\n]\n, Gemini 2.5 Flash Image (NanoBanana), and GPT-Image-1. Qwen-Image being the open-sourced among the three. We evaluate models across three prominent compositional generation benchmarks: ConceptMix\n[\n34\n]\n, T2I-CompBench\n[\n13\n]\n, and TIIF-Bench\n[\n31\n]\n. ConceptMix measures a model’s ability to bind multiple concept categories (objects, textures, colors, shapes, styles, relations, etc.) under increasing compositional complexity, ranging from one to seven concept combinations. T2I-CompBench evaluates open-world compositionality, including attribute binding, object–object relationships, numeracy, and multi-object reasoning. TIIF-Bench focuses on fine-grained instruction following across diverse scenarios such as 3D perspective, logical negation, precise text rendering, and 2D spatial relations. We further evaluate our method on the Visual Jenga scene decomposition benchmark which tests a model’s ability to progressively remove objects from a scene in a physically plausible manner.\nFor all benchmarks, we follow their respective evaluation protocols and use a strong multimodal language model (MMLM) (Gemini-2.5-Pro or GPT-4o depending on dataset original specification) to assess prompt–image consistency. For ConceptMix and TIIF-Bench, question–answer prompts are provided to the evaluator MMLM along with the generated image which outputs binary/yes no answers. For T2I-CompBench, we adopt their MMLM-based scoring protocol (using GPT-4V), which outputs a continuous alignment score between 1 and 100. Importantly, our in-the-loop critic and verifier is a\nweaker MMLM\ndifferent from the final benchmark evaluator. For primary experiments, we use Gemini-2.5-Flash as the in-loop verifier and critic. Further implementation details and baselines are provided in appendix.\n4.1\nCompositional Image Generation\nWe first evaluate on ConceptMix and T2I-CompBench. For each model (Qwen-Image, Gemini, GPT-Image), we run two variants of our method – fully iterative (\nIter\n) and iterative+parallel (\nIter-Par\n) – under a matched inference-time compute budget\nB\nB\n. For Qwen-Image, we set\nB\n=\n16\nB{=}16\nfor Conceptmix and\nB\n=\n8\nB{=}8\nfor T2I-Bench (given reduced prompt complexity). Accordingly, for\nIter-Par\n, we consider 2 parallel steps and\nB\n/\n2\nB/2\niterative steps. As detailed in Section\n4.4\n, we find this configuration to be an optimal trade-off under a fixed compute budget. As a strong budget-matched\nparallel-only\nbaseline (\nParallel\n), we generate\nB\nB\nimages in parallel with different random seeds. We use the same VLM (Gemini-2.5-Flash) as in-loop verifier in both iterative and parallel steps to select the best image (from final set of images). This image is then passed to the benchmark-specific evaluator. Note, for GPT-Image and Gemini, we set\nB\n=\n12\nB{=}12\n(for Conceptmix) and\nB\n=\n8\nB{=}8\n(for T2I), and evaluate on a randomly sampled reduced subset of prompts for each category due to their higher inference cost and closed-source nature.\nConceptMix\n: As shown in Table\n1\n, both\nIter\nand\nIter-Par\nconsistently outperform the parallel-only baseline, with the largest gains on complex compositions (ConceptMix\nk=4–7\n). For Qwen-Image, we observe improvements of 18.8%, 21.6%, and 16.9% at binding complexities\nk\n=\n5\n,\n6\n,\n7\nk=5,6,7\n, respectively. We also see significant gains for Nano-Banana and GPT-Image, with improvements of 8.3% and 10.6% at\nk\n=\n7\nk=7\n, respectively. Notably, improvements are also present for smaller binding complexities such as\nk\n=\n1\n,\n2\n,\n3\nk=1,2,3\n, indicating effectiveness even at simpler compositions. Further, in Fig.\n4\n, we show mean accuracy across ConceptMix categories for Qwen-Image; the largest improvements are observed in Spatial, Style, Shape and Size categories. In comparison, Object and Color categories do not show strong improvements. This is possibly as the model has strong capabilities for these categories and does not perform poorly for them even at high concept bindings.\nFigure 4\n:\nPer-category level improvement for ConceptMix with Qwen-Image. As can be seen the largest improvement for iterative refinement comes from Spatial, Size, Style and Shape categories.\nModel\nOverall\nBasic Following\nAdvanced Following\nDesigner\nAvg\nAttribute\nRelation\nReasoning\nAvg\nAttr+Rel\nAttr+Reas\nRel+Reas\nStyle\nText\nReal World\nFLUX.1 [dev]\n[\n16\n]\n71.1\n83.1\n87.1\n87.3\n75.0\n65.8\n67.1\n73.8\n69.1\n66.7\n43.8\n70.7\nSD 3\n[\n7\n]\n67.5\n78.3\n83.3\n82.1\n71.1\n61.5\n61.1\n68.8\n51.0\n66.7\n59.8\n63.2\nJanus-Pro-7B\n[\n6\n]\n66.5\n79.3\n79.3\n78.3\n80.3\n59.7\n66.1\n70.5\n67.2\n60.0\n28.8\n65.8\nMidJourney v7\n[\n23\n]\n68.7\n77.4\n77.6\n82.1\n72.6\n64.7\n67.2\n81.2\n60.7\n83.3\n24.8\n68.8\nSeedream 3.0\n[\n9\n]\n86.0\n87.1\n90.5\n89.9\n80.9\n79.2\n79.8\n77.2\n75.6\n100.0\n97.2\n83.2\nQwen-Parallel\n[\n32\n]\n85.2\n85.2\n89.7\n88.3\n77.7\n80.6\n81.9\n79.6\n77.8\n89.7\n93.7\n90.4\nQwen-Iter\n85.4\n85.0\n92.0\n80.5\n82.3\n81.3\n80.8\n80.1\n80.2\n86.2\n97.6\n88.4\nQwen-Iter+Par\n87.4\n88.1\n90.5\n88.1\n85.4\n81.5\n81.4\n82.0\n80.5\n90.0\n97.7\n92.0\nTable 2\n:\nPerformance comparison across prominent open source text-to-image models on TIFF\n[\n31\n]\nbenchmark (short descriptions only; full long description results in suppl.). Qwen-Iter+Par achieves state-of-art and is especially beneficial on Basic Reasoning scenario as well as Attr+Reas, Rel+Reas and Text-writing categories. Compute-matched Qwen-Parallel has overall poorer performance to iterative variants.\nFigure 5\n:\nComparison against existing test-time scaling methods. As can be seen methods such as GenArtist\n[\n28\n]\nand RPG\n[\n35\n]\nare difficult to scale due to their reliance on tools or regional priors.\nT2I-CompBench\n: As shown in Table\n1\n, we observe strong gains across multiple categories of T2I-CompBench. For Qwen-Image, improvements of 7.1%, 13.8%, and 6.3% are achieved on Spatial, 3D-spatial, and Numeracy, respectively, suggesting that iterative refinement particularly helps in generation of images that entail precise spatial and numeric reasoning. We observe significant gains for Nano-Banana and GPT-Image as well, with Nano-Banana improving by 6.7%, 7.9%, and 9.8% on the same set of categories respectively. However, on categories such as Color, Non-Spatial and Texture we do not find strong benefits. This could be as the model is already strong in these categories and thus iterative refinement may not be required.\nComparison with Compositional Methods.\nWe also compare the application of our iterative refinement method with prominent compositional generation frameworks such as IterComp\n[\n39\n]\n, RPG\n[\n35\n]\n, and GenArtist\n[\n28\n]\n. As shown in Fig.\n5\n, while most methods have similar performance at initial binding complexities, our method shows stronger gains as the number of concepts increase. This is potentially due to our method’s usage of a simple general VLM-critic and editor in loop to iteratively critique and refine the image without usage of task-specific tools such as object detectors, super resolution generation, layout planners, etc that may introduce intermediate artifacts and compound errors over longer concept lengths. For GenArtist, we maintain the same base model Qwen-Image, while for RPG, we use the recent IterComp implementation\n[\n39\n]\nwhich uses stable diffusion as the base model. Additional details and baseline discussions are provided in the supplemental.\nFigure 6\n:\nEach row shows a different prompt and the corresponding sequence of critic-guided refinement steps. The critic issues actions (Continue, Restart, Backtrack) and targeted sub-prompts, allowing the generator to progressively correct errors—e.g., hiding the mouse behind the key, placing the carrot inside the metallic bee in a cubist style, and adjusting the flamingo’s pose. The final images satisfy all compositional constraints with high fidelity. See appendix for more examples and\nfailure cases\n(figs.\n11\nand\n12\n).\nFigure 7\n:\nHuman preference rate. As can be seen our method is preferred over the parallel only baseline by the human evaluators.\nTIIF-Bench:\nIn Table\n2\n, we report results on the TIIF-Bench benchmark which evaluates basic and advanced instruction-following capabilities of image generation models. We set Qwen-Iter+Par achieves state-of-the-art results among open-source methods, including a 5.0% improvement over Qwen-Parallel on basic reasoning prompts, 2.7% on advanced Relation+Reasoning, and 4.0% on text rendering. These results underscore the generality of our method and its applicability across diverse compositional text-to-image scenarios requiring varied reasoning skills.\n4.2\nQualitative analysis and human evaluation\nHere, we qualitatively analyze intermediate generations alongside the critic’s refinement prompts and actions. In Fig.\n6\n, we show three examples covering different refinement patterns. In the first example (prompt:\n“A mouse hidden by a key”\n), the initial image incorrectly depicts the mouse holding the key. The critic selects Continue and proposes a refinement prompt to hide the mouse behind the large key so only whiskers and eyes are visible. The next edit still leaves the mouse only partially hidden, so the critic again selects Continue and emphasizes that the mouse should be almost entirely obscured behind the metal key. The subsequent image improves but remains unconvincing, prompting another Continue action with a refinement to ensure the key dominates the composition and fully obscures the mouse. The final image convincingly depicts a mouse hidden by a key.\nIn the second example (prompt:\n“a tiny red carrot positioned inside a huge metallic bee, depicted in a cubist style”\n), the initial image lacks the cubist style. The critic chooses Restart (Fresh Start), yielding a more cubist rendering but with the carrot outside the bee. The critic then selects Continue and refines the prompt to place the carrot inside the bee while reinforcing the cubist style. The next image improves on both criteria, and a final Continue instructs a highly abstract, cubist oil-painting treatment. The resulting image convincingly shows a tiny red carrot inside a metallic bee in a cubist style.\nFinally, in the third example (prompt: “a lively pink flamingo spreading its wings as it dances in a rainforest”), the initial image incorrectly shows a flamingo grazing in a grass field. The critic selects Continue and refines the prompt to set the scene in a rainforest; the next image places the flamingo there. The critic then proposes to make the flamingo more animated, but this yields an overly “animated” stylization rather than a lively pose. The critic Backtracks to the previous image and refines the prompt to make the flamingo lively—explicitly asking it to spread its wings and dance in the rainforest. The resulting image shows a wing-spread, dancing flamingo in a rainforest, closely matching the prompt.\nHuman Evaluation:\nWe conducted a user study on 150 randomly sampled prompts from ConceptMix and T2I-CompBench. For each prompt, three raters answered a set of questions about the generated image, related to the correctness of each concept, attribute and relation mentioned in the prompt. We also collected preference judgments by presenting two images side-by-side along with the text prompt and asking raters to select their preferred image. A sample UI and additional details are provided in the supplemental. As shown in Fig.\n7\n, our method is preferred 58.7% of the time versus 41.3% for the parallel-only baseline. Inter-annotator agreement among humans is 85.3%. The average agreement between humans and the language model for the same set of images is 83.4%, indicating that the language model based evaluator (MMLM) is sufficiently reliable.\nFigure 8\n:\nQualitative analysis on Visual Jenga\n[\n2\n]\nscene decomposition task. The\ntop row\nshows how the VLM critic is able to identify errors (such as shadow residual artifact in first case or incorrect book removal in second case) and issue corrective feedback that the proposer incorporates for next step instruction. The\nbottom row\nshows an example where three rounds of feedback are required to generate correct removal with critic able to identify even subtle changes such as change of person appearance in background (see iteration 2).\n4.3\nVisual Jenga Scene Decomposition\nHere, we extend our iterative refinement framework to the recent Visual Jenga\n[\n2\n]\nfull scene decomposition task, where the goal is to progressively remove objects from a cluttered scene in a\nphysically plausible\nsequence while maintaining correctness at every intermediate generation. Starting from an initial scene image, the system selects the next object to remove, produces a\nremoval phrase\n(e.g., “remove the red mug from the table”), and generates the next scene representation with that object removed.\nExtending our iterative refinement setup.\nAt each step, given the current scene, a VLM proposer suggests the next object and corresponding removal phrase, which is passed to the editor to generate the next scene. We then feed\nboth\nthe previous and next images to a VLM critic that checks (1) the specified object was removed correctly, and (2) no other violations occurred (hallucinations, artifacts, identity drift, or physically implausible changes). If a violation is detected, the critic returns structured feedback that is fed back into the proposer to generate a more precise removal phrase or to choose an alternate next object, and the step is retried. This continues until the step is verified by the critic or the per-step compute budget is exhausted (in which case the highest scoring candidate is selected). We report results using the recent GPT-Image-1.5.\nWe compare against a budget-matched\nparallel sampling\nbaseline that generates 4 candidates per removal step and uses the same VLM critic as verifier to select the best candidate. We evaluate on the full decomposition subset consisting of 56 unique scenes, and report the\nfull solve rate\n: a scene is counted as solved only if the method completes the entire decomposition sequence with all intermediate steps satisfactory to human evaluation. As shown in Table\n3\n, applying iterative feedback through VLM critic feedback improves full decomposition solve rate from 64.29% (with parallel sampling) to 76.79%.\nIn Fig.\n8\n, we illustrate how the VLM critic identifies errors in candidate removals and how the proposer incorporates this feedback to correct the removal phrases. In the top row first case, removing only the small wooden ladder leaves behind its shadow; the VLM critic identifies this in its feedback, and in the next iteration, the proposer outputs prompt to remove both\nthe ladder and the shadow it casts\n, producing a correct intermediate scene generation. The second case in the top row similarly shows the critic detecting incorrect removal of the top red book (with the count of books unchanged) and providing precise corrective feedback. The bottom case shows how three rounds of feedback progressively refine the prompt until the\nfrontmost glass\nis correctly deemed by critic to be successfully removed.\nOverall, these results further underscore the benefit of iterative refinement in providing corrective feedback to improve intermediate generations, which is not captured in compute-matched parallel sampling thereby leading to lower performance in scene decomposition as well.\nGPT-Parallel\nGPT-Iter (ours)\nFull solve rate (%) (\n↑\n\\uparrow\n)\n64.29\n76.79\nTable 3\n:\nResults on Visual Jenga full scene decomposition. Iterative refinement significantly improves full solve rate over compute-matched parallel sampling.\nI\nI\nP\nP\nI\n×\nP\nI\\times P\nCMix(k\n{\n5\n,\n6\n,\n7\n}\n\\{5,6,7\\}\n)\nT2I-Avg\n1\n1\n1\n32.3\n79.8\n1\n2\n2\n35.6\n82.3\n2\n1\n2\n37.2\n82.6\n1\n4\n4\n41.1\n84.9\n2\n2\n4\n41.3\n84.7\n4\n1\n4\n48.4\n86.4\n1\n8\n8\n44.8\n86.5\n2\n4\n8\n43.9\n87.4\n4\n2\n8\n57.6\n90.2\n8\n1\n8\n60.0\n89.9\n1\n16\n16\n52.1\n87.9\n2\n8\n16\n53.4\n89.0\n4\n4\n16\n66.3\n91.7\n8\n2\n16\n69.6\n92.6\n16\n1\n16\n69.2\n92.1\nTable 4\n:\nAverage accuracy\nof configurations sorted by total compute;\nI\nI\n= iterative steps,\nP\nP\n= parallel steps. A higher proportion of iterative compute w.r.t. to parallel compute consistently leads to better results across different computation budgets.\n4.4\nAblations\nTrade-offs between iterative and parallel compute.\nWe analyze the trade-offs between iterative and parallel computation under different inference-time compute budgets. Given a total budget\nB\nB\n, we study the allocation between iterative steps\nI\nI\nand parallel steps\nP\nP\n(where\nI\n×\nP\n=\nB\nI\\times P=B\n). As shown in Table\n4\n, we evaluate\n(\nI\n,\nP\n)\n(I,P)\nconfigurations for\nB\n∈\n{\n1\n,\n2\n,\n4\n,\n8\n,\n16\n}\nB\\in\\{1,2,4,8,16\\}\n. Due to the high computational cost entailed, we conduct this analysis on the open-source Qwen-Image model, using a randomly sampled reduced subset of prompts from ConceptMix (binding lengths\nk\n∈\n{\n5\n,\n6\n,\n7\n}\nk\\in\\{5,6,7\\}\n) and T2I-CompBench (complex, 3D spatial, numeracy, spatial, color, texture). Across larger budgets (\nB\n≥\n4\nB\\geq 4\n), iterative allocation yields higher accuracy than parallel. For\nB\n=\n4\nB{=}4\n, purely iterative (\nI\n=\n4\n,\nP\n=\n1\nI{=}4,P{=}1\n) achieves 48.4% on ConceptMix and 86.4% on T2I-CompBench, compared to 41.1% and 84.9% for purely parallel (\nI\n=\n1\n,\nP\n=\n4\nI{=}1,P{=}4\n). Similarly, fully-iterative (\nI\n=\nB\n,\nP\n=\n1\nI{=}B,P{=}1\n) outperforms fully-parallel (\nI\n=\n1\n,\nP\n=\nB\nI{=}1,P{=}B\n) on ConceptMix by 15.2% and 17.1% at\nB\n=\n8\nB{=}8\nand\nB\n=\n16\nB{=}16\n, respectively.\nAt\nB\n=\n16\nB{=}16\n, the best allocation is\nI\n=\n8\n,\nP\n=\n2\nI{=}8,P{=}2\n, achieving 69.6% on ConceptMix and 92.6% on T2I-CompBench, compared to\nI\n=\n16\n,\nP\n=\n1\nI{=}16,P{=}1\n(69.2% and 92.1%). This indicates that, at higher budgets, a mixed strategy – primarily iterative refinement with a small amount of parallel sampling – can outperform purely iterative or purely parallel approaches. We believe this could be due to diminishing returns beyond a certain number of iterative refinements. Instead of doing unnecessary refinement steps, allocating a portion of compute to parallel candidates improves exploration and prevents over-refinement. Note that even within mixed allocations, skewing the budget toward iteration rather than parallelism performs best (Table\n4\n). As shown in Fig.\n9\n, settings with higher\nI\nI\n(green and red lines denoting\nI\n=\n4\nI{=}4\nand\nI\n=\n8\nI{=}8\n, respectively; purple dot denoting\nI\n=\n16\nI{=}16\n) consistently achieve higher solve rates than settings with lower\nI\nI\n(blue and orange lines denoting\nI\n=\n1\nI{=}1\nand\nI\n=\n2\nI{=}2\n) at the same budget\nB\nB\n. Overall, these results indicate that the combination of iterative refinement and parallel sampling is an optimal setting for compositional image generation.\nFigure 9\n:\nComparison of iterative and parallel compute allocations.\nGiven a budget of\nB\n=\n16\nB=16\n, mixed allocations of 8 iterative with 2 parallel generally outperform purely parallel or purely iterative strategies.\nChoice of VLM critic model and action space\nHere, we analyze the impact of the backbone VLM critic model and the action space of the critic model. As shown in table.\n5\n, we analyzed usage of Gemini-Pro, GPT-5 and Qwen3-VL-32B-Instruct models as the critic models for our method on a subset of randomly sampled Conceptmix prompts. As shown, with Gemini-Pro as the critic model, we achieve\n∼\n\\sim\n4% performance improvement than our default used Gemini-2.5-Flash model. However, using the small open-source Qwen3-VL-32B shows\n∼\n\\sim\n3% performance degradation indicating that recent improvements in the foundation model capabilities are critical to our improvement. In Table\n6\n, we analyze the impact of the action space of the critic model. As shown, we achieve the best accuracy with the full action space, with both actions of\n‘Fresh Start’\nand\n‘Backtrack’\nbeing beneficial.\nCritic VLM\nSolve rate (%).\nGemini-2.5-Flash\n69.7\nGemini-Pro\n74.0\nGPT-5\n72.3\nQwen3-VL-32B\n66.3\nTable 5\n:\nImpact of choice of critic VLM on performance.\nAction Space\nSolve rate (%).\nFull action space\n69.7\nw/o Backtrack\n68.0\nw/o Fresh Start\n67.7\nw/o Backtrack & Fresh Start\n67.3\nTable 6\n:\nImpact of action space components on performance.\n5\nConclusion\nWe introduced iterative refinement as a simple but broadly applicable inference-time strategy to improve compositional image generation capabilities of text-to-image (T2I) models. Our approach combines an image generation and editing model with a vision-language model (VLM) critic in the loop that progressively enables refinement of generated outputs. We show our method achieves strong performance benefits over traditional inference-time scaling methods such as parallel sampling on prominent T2I models including Nano-Banana, Qwen-Image and GPT-One. Our framework achieves state-of-art performances across compositional image generation benchmarks including Conceptmix, T2I-CompBench and TIFF as well as the Visual Jenga scene decomposition task. We further perform qualitative analysis to illustrate how our method works, besides human evaluation to concretely verify our method beyond benchmarks. We also conduct ablations studying tradeoffs between iterative and parallel compute allocation, and the impact of choice of VLM critic model and its action space.\nReferences\n[1]\nY. Bengio, Y. LeCun,\net al.\n(2007)\nScaling learning algorithms towards ai\n.\nLarge-scale kernel machines\n34\n(\n5\n),\npp. 1–41\n.\nCited by:\n§2\n.\n[2]\nA. Bhattad, K. Preechakul, and A. A. Efros\n(2025)\nVisual jenga: discovering object dependencies via counterfactual inpainting\n.\narXiv preprint arXiv:2503.21770\n.\nCited by:\n§1\n,\nFigure 8\n,\nFigure 8\n,\n§4.3\n.\n[3]\nT. Brooks, A. Holynski, and A. A. Efros\n(2023)\nInstructPix2Pix: learning to follow image editing instructions\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\n§2\n.\n[4]\nB. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V. Le, C. Ré, and A. Mirhoseini\n(2024)\nLarge language monkeys: scaling inference compute with repeated sampling\n.\narXiv preprint arXiv:2407.21787\n.\nCited by:\n§1\n.\n[5]\nH. Chefer, Y. Alaluf, Y. Vinker, L. Wolf, and D. Cohen-Or\n(2023)\nAttend-and-excite: attention-based semantic guidance for text-to-image diffusion models\n.\nACM transactions on Graphics (TOG)\n42\n(\n4\n),\npp. 1–10\n.\nCited by:\n§2\n.\n[6]\nX. Chen, Z. Wu, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, and C. Ruan\n(2025)\nJanus-pro: unified multimodal understanding and generation with data and model scaling\n.\narXiv preprint arXiv:2501.17811\n.\nCited by:\nTable 2\n.\n[7]\nP. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel,\net al.\n(2024)\nScaling rectified flow transformers for high-resolution image synthesis\n.\nIn\nForty-first international conference on machine learning\n,\nCited by:\nTable 2\n.\n[8]\nW. Feng, W. Zhu, T. Fu, V. Jampani, A. Akula, X. He, S. Basu, X. E. Wang, and W. Y. Wang\n(2023)\nLayoutgpt: compositional visual planning and generation with large language models\n.\nAdvances in Neural Information Processing Systems\n36\n,\npp. 18225–18250\n.\nCited by:\n§2\n.\n[9]\nY. Gao, L. Gong, Q. Guo, X. Hou, Z. Lai, F. Li, L. Li, X. Lian, C. Liao, L. Liu,\net al.\n(2025)\nSeedream 3.0 technical report\n.\narXiv preprint arXiv:2504.11346\n.\nCited by:\nTable 2\n.\n[10]\nI. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio\n(2016)\nDeep learning\n.\nVol.\n1\n,\nMIT press Cambridge\n.\nCited by:\n§2\n.\n[11]\nG. E. Hinton, S. Osindero, and Y. Teh\n(2006)\nA fast learning algorithm for deep belief nets\n.\nNeural computation\n18\n(\n7\n),\npp. 1527–1554\n.\nCited by:\n§2\n.\n[12]\nJ. Ho and T. Salimans\n(2022)\nClassifier-free diffusion guidance\n.\narXiv preprint arXiv:2207.12598\n.\nCited by:\n§2\n.\n[13]\nK. Huang, K. Sun, E. Xie, Z. Li, and X. Liu\n(2023)\nT2i-compbench: a comprehensive benchmark for open-world compositional text-to-image generation\n.\nAdvances in Neural Information Processing Systems\n36\n,\npp. 78723–78747\n.\nCited by:\n§1\n,\nTable 1\n,\nTable 1\n,\n§4\n.\n[14]\nY. Kirstain, A. Polyak, U. Singer, S. Matiana, J. Penna, and O. Levy\n(2023)\nPick-a-pic: an open dataset of user preferences for text-to-image generation\n.\nAdvances in neural information processing systems\n36\n,\npp. 36652–36663\n.\nCited by:\n§2\n.\n[15]\nT. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa\n(2022)\nLarge language models are zero-shot reasoners\n.\nAdvances in neural information processing systems\n35\n,\npp. 22199–22213\n.\nCited by:\n§1\n.\n[16]\nB. Labs\n(2024)\nFLUX\n.\nNote:\nhttps://github.com/black-forest-labs/flux\nCited by:\n§2\n,\nTable 2\n.\n[17]\nK. Lee, H. Liu, M. Ryu, O. Watkins, Y. Du, C. Boutilier, P. Abbeel, M. Ghavamzadeh, and S. S. Gu\n(2023)\nAligning text-to-image models using human feedback\n.\narXiv preprint arXiv:2302.12192\n.\nCited by:\n§2\n.\n[18]\nY. Li, H. Liu, Q. Wu, F. Mu, J. Yang, J. Gao, C. Li, and Y. J. Lee\n(2023)\nGligen: open-set grounded text-to-image generation\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp. 22511–22521\n.\nCited by:\n§2\n.\n[19]\nL. Lian, B. Li, A. Yala, and T. Darrell\n(2023)\nLlm-grounded diffusion: enhancing prompt understanding of text-to-image diffusion models with large language models\n.\narXiv preprint arXiv:2305.13655\n.\nCited by:\n§2\n.\n[20]\nN. Ma, S. Tong, H. Jia, H. Hu, Y. Su, M. Zhang, X. Yang, Y. Li, T. Jaakkola, X. Jia,\net al.\n(2025)\nInference-time scaling for diffusion models beyond scaling denoising steps\n.\narXiv preprint arXiv:2501.09732\n.\nCited by:\n§1\n.\n[21]\nA. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang,\net al.\n(2023)\nSelf-refine: iterative refinement with self-feedback\n.\nAdvances in Neural Information Processing Systems\n36\n,\npp. 46534–46594\n.\nCited by:\n§2\n.\n[22]\nC. Meng, Y. He, Y. Song, J. Song, J. Wu, J. Zhu, and S. Ermon\n(2022)\nSDEdit: guided image synthesis and editing with stochastic differential equations\n.\nIn\nInternational Conference on Learning Representations (ICLR)\n,\nCited by:\n§2\n,\n§7\n.\n[23]\nMidjourney\n(2025)\nMidjourney v7\n.\nNote:\nhttps://github.com/midjourney\nCited by:\nTable 2\n.\n[24]\nOpenAI\n(2023-09)\nDALL·e 3\n.\nNote:\nhttps://openai.com/research/dall-e-3\nCited by:\n§2\n.\n[25]\nOpenAI\n(2025)\nGPT-image-1\n.\nNote:\nhttps://openai.com/index/introducing-4o-image-generation/\nCited by:\n§2\n.\n[26]\nC. Snell, J. Lee, K. Xu, and A. Kumar\n(2024)\nScaling llm test-time compute optimally can be more effective than scaling model parameters\n.\narXiv preprint arXiv:2408.03314\n.\nCited by:\n§1\n.\n[27]\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou\n(2022)\nSelf-consistency improves chain of thought reasoning in language models\n.\narXiv preprint arXiv:2203.11171\n.\nCited by:\n§2\n.\n[28]\nZ. Wang, A. Li, Z. Li, and X. Liu\n(2024)\nGenartist: multimodal llm as an agent for unified image generation and editing\n.\nAdvances in Neural Information Processing Systems\n37\n,\npp. 128374–128395\n.\nCited by:\n§1\n,\n§2\n,\nFigure 5\n,\nFigure 5\n,\n§4.1\n,\n§7\n.\n[29]\nZ. Wang, E. Xie, A. Li, Z. Wang, X. Liu, and Z. Li\n(2024)\nDivide and conquer: language models can plan and self-correct for compositional text-to-image generation\n.\narXiv preprint arXiv:2401.15688\n.\nCited by:\n§1\n,\n§2\n.\n[30]\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou,\net al.\n(2022)\nChain-of-thought prompting elicits reasoning in large language models\n.\nAdvances in neural information processing systems\n35\n,\npp. 24824–24837\n.\nCited by:\n§1\n,\n§2\n.\n[31]\nX. Wei, J. Zhang, Z. Wang, H. Wei, Z. Guo, and L. Zhang\n(2025)\nTIIF-bench: how does your t2i model follow your instructions?\n.\narXiv preprint arXiv:2506.02161\n.\nCited by:\nTable 2\n,\nTable 2\n,\n§4\n.\n[32]\nC. Wu, J. Li, J. Zhou, J. Lin, K. Gao, K. Yan, S. Yin, S. Bai, X. Xu, Y. Chen,\net al.\n(2025)\nQwen-image technical report\n.\narXiv preprint arXiv:2508.02324\n.\nCited by:\n§2\n,\nTable 2\n,\n§4\n,\n§7.1\n.\n[33]\nX. Wu, K. Sun, F. Zhu, R. Zhao, and H. Li\n(2023)\nHuman preference score: better aligning text-to-image models with human preference\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp. 2096–2105\n.\nCited by:\n§2\n.\n[34]\nX. Wu, D. Yu, Y. Huang, O. Russakovsky, and S. Arora\n(2024)\nConceptmix: a compositional image generation benchmark with controllable difficulty\n.\nAdvances in Neural Information Processing Systems\n37\n,\npp. 86004–86047\n.\nCited by:\n§1\n,\nTable 1\n,\nTable 1\n,\n§4\n,\n§7.1\n.\n[35]\nL. Yang, Z. Yu, C. Meng, M. Xu, S. Ermon, and B. Cui\n(2024)\nMastering text-to-image diffusion: recaptioning, planning, and generating with multimodal llms\n.\nIn\nForty-first International Conference on Machine Learning\n,\nCited by:\n§1\n,\n§2\n,\nFigure 5\n,\nFigure 5\n,\n§4.1\n.\n[36]\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan\n(2023)\nTree of thoughts: deliberate problem solving with large language models\n.\nAdvances in neural information processing systems\n36\n,\npp. 11809–11822\n.\nCited by:\n§2\n.\n[37]\nR. Zhang, C. Tong, Z. Zhao, Z. Guo, H. Zhang, M. Zhang, J. Liu, P. Gao, and H. Li\n(2025)\nLet’s verify and reinforce image generation step by step\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp. 28662–28672\n.\nCited by:\n§2\n.\n[38]\nX. Zhang, H. Lin, H. Ye, J. Zou, J. Ma, Y. Liang, and Y. Du\n(2025)\nInference-time scaling of diffusion models through classical search\n.\narXiv preprint arXiv:2505.23614\n.\nCited by:\n§1\n.\n[39]\nX. Zhang, L. Yang, G. Li, Y. Cai, J. Xie, Y. Tang, Y. Yang, M. Wang, and B. Cui\n(2024)\nItercomp: iterative composition-aware feedback learning from model gallery for text-to-image generation\n.\narXiv preprint arXiv:2410.07171\n.\nCited by:\n§2\n,\n§4.1\n,\n§7\n.\nAppendix\n6\nFurther qualitative examples.\nWe visualize further samples in the attached interactive HTML webpage for different prompt types showcasing our method (Iterative Refinement) compared to the baseline (best chosen sample out of 16 parallel samples) for 3 model families – Qwen-Image, GPT and NanoBanana.\nLimitations and selected failure modes\nOur qualitative analysis reveals two primary failure modes:\n(i) Incorrect VLM reasoning:\nWhen the VLM critic or verifier produces faulty reasoning, it generates an incorrect verification signal. This can cause genuine errors in the generated image to go undetected, or conversely, lead to unnecessary refinements of correct images.\n(ii) Inability of editor to make prompted changes:\nWe also observed cases where the editor was unable to make the desired changes to the image, even though the prompt was clear. This is likely due to the complexity of the image and the limitations of the image editing model. The prompt “The image features a heart-shaped giraffe, a tiny pink screwdriver, and a huge robot. The screwdriver is positioned at the bottom of the robot, touching it.” is an example of a prompt where the editor was unable to make the desired changes.\nFigure 10\n:\nExample of model correcting ‘glass-shaped’ chicken over steps (see full example in visualization html page).\nFigure 11\n:\nExample of a failure case due to verifier not detecting giraffe is not heart shaped (see full example in visualization html page).\nFigure 12\n:\nExample of a failure case where editor is not able to add carrot in front of pine tree.\n7\nFurther experiment details\nWe provide further experiment details here regarding models used and how they were accessed. We will open source our codebase for reproducability.\nImage Generation Models:\n•\nQwen-Image\nand\nQwen-Image-Edit\n: Weights obtained from the Hugging Face model hub.\n•\nGPT-Image-One\n: Run on ’auto’ inference setting (for cost reasons) with official closed-source OpenAI API using model id:\ngpt-image-one\n.\n•\nNanoBanana\n: Run on default inference setting with Google GenAI API using model id:\ngemini-2.5-flash-image\n.\nVerifier and Critic Models:\n•\nGemini-2.5-Flash-Lite\n: Used for our primary experiments as the in-loop critic and verifier with Google GenAI API using model id:\ngemini-2.5-flash-lite\n.\n•\nGemini-2.5-Pro\n: Used for ablation studies and as final evaluation verifier for ConceptMix.\n•\nGPT-4o\n: Used for T2I-CompBench and TIIF-Bench as official VLM evaluator (following benchmark protocol).\n•\nGPT-5-Mini\n: Used for ablation on ’auto’ thinking complexity (for cost reasons) with official closed-source OpenAI API using model id:\ngpt-5\n.\n•\nQwen3-VL-32B-Instruct\n: Used for ablation studies with weights obtained from official Hugging Face model hub.\nWe provide system and user prompts provided to the VLM critic at each turn below.\nUser Prompt (Provided to critic after each iteration)\nFull complex prompt:\nIn an abstract ink style image, a corgi stands near a large tree. Nearby, there are three tiny hills with a metallic texture.\nYour previous step prompts were:\n•\nStep 1: In an abstract ink style image, a corgi stands near a large tree. Nearby, there are three tiny hills with a metallic texture.\n•\nStep 2: Change the texture of the three hills in the foreground to be shiny and metallic.\nThe most recently generated image had the following verifier scores:\n•\nDoes the image contain corgi?: 1\n•\nDoes the image contain hills?: 1\n•\nDoes the hill have a metallic texture?: 1\n•\nIs the style of the image abstract?: 0\n•\nIs the style of the image ink?: 1\n•\nIs the hill tiny in size?: 1\n•\nIs the number of hills exactly 3?: 1\n•\nCumulative mean binary score: 0.857\nThe maximum number of editing steps is\n4\n.\nThis is\nstep 3\nof image editing and you will have\n1 step left\nto complete the task.\nDecide the next step prompt accordingly.\nSystem Prompt for Critic\nYou are a helpful assistant that given a complex image generation prompt and previously generated image along with verifier scores, generates the best next step prompt for an image editing model.\nThe idea is to generate the image over multiple editing and refinement steps, so the next step prompt should either edit the previous image to improve it or add new elements to the image. Some suggested guidelines are:\n•\nCheck if previous step worked correctly\n•\nIdentify any important missing element from full prompt\n•\nCheck if there is space for new elements to be added in the current frame. If not, then prompt model to zoom out and make space first.\n•\nIn case of errors, prompt model to fix them or delete the incorrect element.\nYou have to choose from the following actions:\n1.\nCONTINUE: Continue editing the most recently generated image to improve it with your proposed prompt.\n2.\nBACKTRACK: Backtrack to image before the most recently generated image, and edit that image with your proposed prompt.\n3.\nFRESH_START: Start entirely from scratch with your proposed prompt due to major unfixable errors over steps.\n4.\nSTOP: Stop the editing process due to completion of the task\nYou will be provided following inputs:\n•\nThe full complex prompt\n•\nYour previously proposed step prompts\n•\nThe most recently generated image (which is attached for your reference) along with verifier scores (sometimes verifier can be wrong for attribute counts questions)\nYou have to output two things:\n1.\nThe action to be taken\n2.\nThe next step prompt that will be given to the image editor or generator\nThe maximum number of editing steps is 4. This is step 1 of image editing and you will have 3 steps left to complete the task. Decide the next step prompt accordingly.\nOutput your response in the following format:\nAction: [action to be taken]\nPrompt: [next step prompt]\nFurther compositional generation baselines.\nWe compare against prominent methods including IterComp\n[\n39\n]\n, RPG, and GenArtist\n[\n28\n]\n. For IterComp and RPG, we use their official codebases and checkpoints from\nhttps://github.com/YangLing0818/IterComp\nand\nhttps://github.com/YangLing0818/RPG-DiffusionMaster\n, respectively. For GenArtist, we use code from\nhttps://github.com/zhenyuw16/GenArtist\nand initialize the image generator and editor with Qwen-Image and Qwen-Edit for fair comparison, which we empirically found to perform better than their original Stable Diffusion\n[\n22\n]\nmodels. We adopt all other tools as specified in their original codebase.\n7.1\nHuman evaluation details\nTo conduct our human evaluation for the images, we first selected a random subset of 150 images from ConceptMix\n[\n34\n]\n. From the\nk\n=\n5\n,\n6\n,\n7\nk{=}5,6,7\ngroups we selected 45, 50, and 55 images, respectively. We then generated images for these prompts using the Qwen\n[\n32\n]\nmodel with inference-time budget of 16 computational steps under two settings: (i) baseline best parallely sampled image, and (ii) our iterative method’s best refined image. We defined each image’s score as the number of evaluation questions the model answered ”yes” to out of the total number (5, 6, or 7) of evaluation questions for that image. This produced 150 pairs of images, each with an associated score. We randomly shuffled these pairs and partitioned them into 25 blocks of 6 image-pair tasks. For each block, three participants evaluated every image using that image’s ConceptMix yes/no evaluation questions. Within each pair, the images generated by our iterative method and by the parallel sampling method were shown in random order. After answering the yes/no questions for each image, participants were also asked, given the prompt, which of the two images they preferred. Examples of our form UI can be seen in Figure\n13\nand Figure\n14\n.\nIn total, we collected responses from 75 unique participants (3 participants per block), providing the data used in our human evaluation analysis.\nFigure 13\n:\nUser interface for the pairwise preference question showing two generated images side-by-side along with the corresponding text prompt.\nFigure 14\n:\nUser interface for the yes/no evaluation questions used to assess whether each concept, attribute, and relation in the prompt is correctly depicted in the generated image.\nWe computed several key metrics from this human evaluation study. As seen in Table\n7\n, Human evaluators achieved a mean net solve rate of 91.0% and a mean perfect solve rate of 55.7% across all participants. For human–model agreement, images generated with 8 iterative refinement steps showed higher agreement with human evaluators (88.9% net agreement, 30.6% perfect agreement) compared to the highest-rated image out of 8 images generated in parallel with a single iterative step each (82.1% net agreement, 18.7% perfect agreement). In Table\n8\n, when comparing the two generation methods directly, the net agreement for images produced with 8 iterative steps is slightly higher than for images generated in parallel with a single step (86.1% vs. 84.8% net agreement), and our method’s images achieve higher perfect agreement rates as well (37.3% vs. 33.3%). This preference is most pronounced for\nk\n=\n6\nk{=}6\nprompts (66.7% preference for our method) and somewhat weaker for\nk\n=\n5\nk{=}5\n(57.3%) and\nk\n=\n7\nk{=}7\n(54.7%) prompts.\nParticipant\nNet Solve Rate\nImages Perfect\nPerfect Solve Rate\nParticipant 1\n0.905\n165/300\n0.550\nParticipant 2\n0.912\n169/300\n0.563\nParticipant 3\n0.912\n167/300\n0.557\nMean\n0.910\n0.557\nTable 7\n:\nHuman evaluator solve rates across all participants. Net solve rate represents the percentage of questions marked as ‘correct’ by evaluator for given images, while perfect solve rate represents the percentage of images where all questions were answered correctly.\nImage Type\nAgreed Questions\nTotal Questions\nPercentage\nPerfect Agreements\nPara.\n50\n150\n33.3%\nIter.\n56\n150\n37.3%\nNet Preferences by\nk\nk\nCategory\nk\nk\nCategory\nPara.\nIterative\nIter. vs Para.\nk\n=\n5\nk{=}5\n42.7%\n57.3%\n+14.6%\nk\n=\n6\nk{=}6\n47.3%\n64.7%\n+17.4%\nk\n=\n7\nk{=}7\n45.3%\n54.7%\n+9.4%\nTable 8\n:\nHuman-iteration agreement analysis comparing images generated with 1 iterative step vs. images generated with 8 iterative steps. Perfect agreements represent images where all human evaluators agreed on all questions. Net agreements represent total agreed questions across all evaluators. Preferences show the percentage of cases where each method was preferred by humans for different complexity levels (\nk\nk\n), and the rightmost column reports the improvement of 8 iterative steps over 1 iterative step.\n7.2\nAdditional experiment specifications\nWe show cumulative results over different models for ConceptMix\nk\n=\n1\nk=1\nto\nk\n=\n7\nk=7\nin Figure\n15\n. As illustrated, our iterative refinement approach consistently outperforms the parallel-only baseline across all three models (Qwen-Image, Nano-Banana, and GPT-Image) and across all prompt complexity levels. The performance benefits are most pronounced for Qwen-Image, where we observe substantial improvements particularly at higher binding complexities (\nk\n=\n5\nk=5\nthrough\nk\n=\n7\nk=7\n), with gains exceeding 15% in solve rate. These results demonstrate the robustness and generalizability of our method across different model architectures and compositional difficulty levels.\nWe show further results of our compute-budget allocation experiments for T2I Bench in Figure\n16\n. As shown, higher iterative compute is linked to higher T2I-Avg score, and mixed allocations of 8 iterative to 2 parallel generally outperform purely parallel or purely iterative strategies. This follows a similar pattern to conceptmix results reported in original paper.For these experiments, we used a subset of 30 prompts for each T2I-Bench category for compute reasons.\nFigure 15\n:\nPerformance of experimented models on ConceptMix k=1 to k=7 comparison for different models. As shown, our method consistently improves over the baseline across models and prompt complexities.\nFigure 16\n:\nComparison of iterative and parallel compute allocations on T2I-Bench.\nGiven a budget of\nB\n=\n16\nB=16\n, mixed allocations of 8 iterative to 2 parallel generally outperform purely parallel or purely iterative strategies.",
    "preview_text": "Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/\n\nIterative Refinement Improves Compositional Image Generation\nShantanu Jaiswal\n1\nMihir Prabhudesai\n1\nNikash Bhardwaj\n1\nZheyang Qin\n1\nAmir Zadeh\n2\nChuan Li\n2\nKaterina Fragkiadaki\n1\nDeepak Pathak\n1\n1\nCarnegie Mellon University\n2\nLambda AI\nAbstract\nText-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "VLM",
        "diffusion"
    ],
    "one_line_summary": "该论文提出了一种基于迭代反馈的文本到图像生成方法，利用视觉语言模型作为批评者来改进复杂提示下的图像生成质量。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T18:59:40Z",
    "created_at": "2026-01-27T15:53:25.500190",
    "updated_at": "2026-01-27T15:53:25.500197"
}