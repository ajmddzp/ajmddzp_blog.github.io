{
    "id": "2601.07723v1",
    "title": "FMAC: a Fair Fiducial Marker Accuracy Comparison Software",
    "authors": [
        "Guillaume J. Laurent",
        "Patrick Sandoz"
    ],
    "abstract": "本文提出了一种基于基准标记进行姿态估计精度公平比较的方法。该方法依托大规模高保真合成图像集，实现对六自由度空间的深度探索。通过空间低差异采样技术，可绘制36组自由度组合的相关性图谱，从而检验各自由度与姿态误差之间的关联。图像渲染采用基于物理的射线追踪代码实现，该代码专为直接调用任意相机的标准校准系数而开发，能够精确复现图像畸变、散焦与衍射模糊效应。此外，系统对锐利边缘实施亚像素采样处理，以提升渲染图像的保真度。在阐述渲染算法及其实验验证后，本文进一步提出姿态精度评估方法，并将其应用于经典标记分析，揭示了各类标记在姿态估计中的优势与局限。相关代码已在GitHub开源发布。",
    "url": "https://arxiv.org/abs/2601.07723v1",
    "html_url": "https://arxiv.org/html/2601.07723v1",
    "html_content": "FMAC: a Fair Fiducial Marker Accuracy Comparison Software\nGuillaume J. Laurent and Patrick Sandoz\n(\nUniversité Marie et Louis Pasteur, CNRS, SupMicroTech-ENSMM\nFEMTO-ST Institute, Besançon, F-25000, France.\n)\nAbstract\nThis paper presents a method for carrying fair comparisons of the accuracy of pose estimation using fiducial markers. These comparisons rely on large sets of high-fidelity synthetic images enabling deep exploration of the 6 degrees of freedom. A low-discrepancy sampling of the space allows to check the correlations between each degree of freedom and the pose errors by plotting the 36 pairs of combinations. The images are rendered using a physically based ray tracing code that has been specifically developed to use the standard calibration coefficients of any camera directly. The software reproduces image distortions, defocus and diffraction blur. Furthermore, sub-pixel sampling is applied to sharp edges to enhance the fidelity of the rendered image. After introducing the rendering algorithm and its experimental validation, the paper proposes a method for evaluating the pose accuracy. This method is applied to well-known markers, revealing their strengths and weaknesses for pose estimation. The code is open source and available on GitHub.\n1\nIntroduction\nFiducial markers are patterns designed to be easily detected and localized within the field of view of an imaging system. Originally designed as simple circular or cross shapes, they now usually embeds a small code to be identified individually like tags. Fiducial markers have found numerous applications beyond augmented reality such as robot navigation and tracking in various fields (indoor robotics, construction robotics, drone docking, space, etc)\n[\n3\n]\n,\ncamera and robot calibration\n[\n4\n,\n9\n,\n32\n]\n,\nmotion capture for virtual reality glasses\n[\n7\n]\n,\nrobot offline programming\n[\n18\n]\n,\nimage-guided surgery\n[\n30\n,\n20\n]\n, structural analysis\n[\n19\n,\n8\n]\n,\nand even ethology\n[\n22\n]\n.\nDozens of patterns have been proposed with different characteristics and merits. The metrics for appraising fiducial markers are numerous. The robustness of the detection is generally the most addressed one by testing its resistance to occlusion, defocus and motion blur, distortion, uneven illumination\n[\n28\n,\n26\n]\n.\nThis is usually done by estimating the rate of no detection, positive false, inter-marker confusion on image datasets. The range of detection, including the minimal size and the maximal orientation, is also an important feature to evaluate. The computational cost is of interest in case of embedded applications and fast tracking.\nWhen the markers are used as reference measuring systems, for example in construction and image-guided surgery, the accuracy of the pose estimation must be evaluated.\nThe accuracy is usually expressed in terms of bias (difference between the expectation of the test results and the reference values). In the case of pose estimation, the bias has six components (three translations and three rotations) and depends on a lot of factors (marker size, camera used to record the images, distance between the camera and the maker, etc.).\nMany papers investigate the evaluation of the pose accuracy that can be expected using a marker and a camera. The more convenient way is to track the marker position with a motion capture system but the resolution is then limited to a dozen of millimeters\n[\n15\n,\n16\n]\n. Some papers report using industrial robots as reference system\n[\n33\n]\n,\n[\n14\n]\n. However, industrial robots are precise but not really accurate. Using precision stages\n[\n35\n]\nand precision robots\n[\n2\n]\ncan improve results but these are still not metrology systems. The best solutions are to use a coordinate measuring machine or a laser tracker like in\n[\n17\n]\n, both being metrology systems. However, these are expensive and time-consuming to implement, thus limiting the number of poses that can be evaluated. Furthermore, it is challenging to reproduce the same conditions in different laboratories to conduct fair comparisons between markers.\n(a)\nAprilTag\n[\n23\n,\n29\n]\n(b)\nSTag\n[\n5\n]\n(c)\nTopotag\n[\n34\n]\n(d)\nArUco\n[\n10\n]\nFigure 1:\nSynthetic images with different fiducial markers at the same poses (overlay of 21 images).\nAn alternative is to use synthetic images. Synthetic images have several advantages over experimental images. The ground truth can be generated without any errors nor perturbations that enables fair comparison between different markers by reproducing exactly the same poses and conditions. A large amount of data can be generated allowing an homogeneous sampling of the six degrees of freedom. However, the disadvantages are that synthetic images may be insufficiently realistic to reflect actual imaging systems. This is notably the case when using rasterization-based rendering, as used by robotic simulators like Gazebo\n[\n21\n,\n27\n]\n. Realistic images can be obtained using ray tracing. Ray tracing is based of the physics of light propagation and is able of rendering a variety of optical effects, such as shadows, optical and chromatic aberrations, scattering and caustics, but also the sampling of digital sensors. Several studies of fiducial markers use ray tracing as ground truth for accuracy evaluation\n[\n6\n,\n31\n,\n25\n,\n23\n]\n.\nIn this paper, we present a physically-based ray tracing code, FMAC, that was specifically designed to render images of fiducial markers. While ray tracing software, such as Blender, aims to render photorealistic images, FMAC focuses on rendering sharp edges accurately and on modeling defocus and diffraction blur. FMAC applies sub-pixel sampling to reproduce pixel gradients near the edges with a high fidelity. In addition, the code is built upon OpenCV and directly uses intrinsic camera matrix and distortion coefficients, as obtained after calibration, along with physical characteristics of the image sensor (resolution, pixel pitch, bit depth) and settings of the lens (f-number, focus distance). FMAC can thus generate synthetic images of any fiducial marker at any poses as illustrated in Figure\n1\n.\nThe purpose of this paper if to fulfill two needs. First, it provides a method to carry fair comparisons of the accuracy of several markers. Notably, FMAC allows a low-discrepancy sampling of the space to finely evaluate the performances along each degree of freedom and the resulting pose errors by plotting the 36 pairs of combinations. The second goal is to provide a tool for use in routine testing after camera calibration, to determine the level of accuracy that can be expected for any camera-marker couple. The code is open source and can be downloaded from GitHub at\nhttps://github.com/vernierlib/fmac\n.\n2\nPhysically Based Rendering\nPhysically based rendering uses the principles of physics to model the interaction between light, optics and matter\n[\n24\n]\n. The aim of physically based engines is usually to render photorealistic images. The objective is to produce an image that is indistinguishable from a photograph to the naked eye. In this paper, however, the goal is slightly different. All the fiducial marker detectors use the edges and the corners of a marker to compute its pose in the camera frame. The accuracy of the pose estimation depends directly on the position and shape of the edges and corners in the image. Small details, imperceptible to the human eye, must thus be rendered with a high fidelity and without aliasing to ensure reliable accuracy evaluation.\nThe proposed pipeline focuses on the rendering of sharp edges. Based on the thin lens model, it renders defocus aberrations with additional distortion corrections to reproduce geometric aberrations. Diffraction blur, which is very important at the pixel level, is also carefully modeled. The synthetic camera sensor is defined by its resolution, pixel pitch, bit depth and principal point. Finally, a local over-sampling is applied near edges to avoid aliasing. Table\n1\ngathers the constitutive parameters of the proposed rendering pipeline.\nTable 1:\nSynthetic camera parameters with the two examples used for validation.\nModel\nDescription\nNotation\nLogitech HD C270\nCanon EOS Rebel XS\nThin lens\nFocal length\nf\nf\n4.47 mm\n26.49 mm\nPrincipal point\nc\nx\nc_{x}\n,\nc\ny\nc_{y}\n319.5 px, 239.5 px\n1407.5 px, 939.5 px\nF-number\nN\nN\n2.8\n4.5\nFocus distance\nz\nf\nz_{f}\n150 mm\n700mm\nDistortion\nRadial coefficients\nk\n1\nk_{1}\n,\nk\n2\nk_{2}\n,\nk\n3\nk_{3}\n,\nk\n4\nk_{4}\n,\nk\n5\nk_{5}\n,\nk\n6\nk_{6}\n-0.286, 0.057, 0.112, 0, 0, 0\n-0.125, 3.855, -40.371, 0, 0, 0\nPrism coefficients\ns\n1\ns_{1}\n,\ns\n2\ns_{2}\n,\ns\n3\ns_{3}\n,\ns\n4\ns_{4}\n0, 0, 0, 0\n0, 0, 0, 0\nTangential coefficients\np\n1\np_{1}\n,\np\n2\np_{2}\n0, 0\n0, 0\nSensor\nResolution (image width and height)\nw\nw\n,\nh\nh\n640x480\n2816x1880\nPixel pitch\nδ\n\\delta\n8.3 µm\n5.7 µm\nBit depth\n8\n,\n10\n,\n12\n8,10,12\n8\n12\nDiffraction\nWavelength of the light\nλ\n\\lambda\n650 nm\n650 nm\n2.1\nThin lens model\nMost computer vision applications rely on the pinhole camera model, whose parameters are the focal length\nf\nf\nand the principal point\n(\nc\nx\n,\nc\ny\n)\n(c_{x},c_{y})\n. Additional distortion coefficients are used to represent geometric aberrations. However, the pinhole camera model only considers rays passing through a single point to reach the sensor, which is insufficient for reproducing defocus aberration. Real cameras have lens systems that focus light through a finite-sized aperture onto the sensor. The goal was thus to develop a code that uses the same camera parameters as those in standard libraries such as OpenCV, while extending the pinhole projection to the thin lens model.\nUnder the thin lens approximation model, incident rays parallel to the optical axis pass at the focal point at a distance\nf\nf\nbehind the lens. Any ray passing through the center of the lens is not deviated. The Gaussian lens equation relates the distances from the object to the lens\nz\nf\nz_{f}\nand from the lens to its sharp image\nz\ns\nz_{s}\n:\n1\nz\nf\n+\n1\nz\ns\n=\n1\nf\n\\displaystyle\\frac{1}{z_{f}}+\\frac{1}{z_{s}}=\\frac{1}{f}\n(1)\nThe focus distance\nz\nf\nz_{f}\ncan be adjusted on camera lenses and must be known in order to reproduce defocus aberration. If an object point does not lie at this distance, it is imaged as a disk on the sensor rather than as a single point.\nThis disk is called the circle of confusion. Its diameter depends of the object point distance\nz\nz\nfollowing:\nd\nc\n=\n|\nd\n⋅\nf\n⋅\n(\nz\n−\nz\nf\n)\nz\n⋅\n(\nf\n+\nz\nf\n)\n|\n\\displaystyle d_{c}=\\left|\\frac{d\\cdot f\\cdot(z-z_{f})}{z\\cdot(f+z_{f})}\\right|\n(2)\nwhere\nd\nd\nstands for the entrance pupil diameter (effective aperture), related to the f-number\nN\nN\nof the lens and to the focal length\nf\nf\nby:\nN\n=\nf\nd\n\\displaystyle N=\\frac{f}{d}\n(3)\nAs long as the circle of confusion remains smaller than the pixel pitch of the sensor, an object point will effectively appear to be in focus.\nFMAC uses this value to adjust the pixel sampling around the marker edges (see below).\n2.2\nDistortion model\nActual camera lenses are not thin lenses and produce geometric aberrations, primarily radial distortion and slight tangential distortion. The code uses the distortion model implemented in OpenCV, comprising six radial coefficients\nk\n1\nk_{1}\n,\nk\n2\nk_{2}\n,\nk\n3\nk_{3}\n,\nk\n4\nk_{4}\n,\nk\n5\nk_{5}\n, and\nk\n6\nk_{6}\n, two tangential coefficients\np\n1\np_{1}\nand\np\n2\np_{2}\n, and four thin prism coefficients\ns\n1\ns_{1}\n,\ns\n2\ns_{2}\n,\ns\n3\ns_{3}\n, and\ns\n4\ns_{4}\n. This model is defined by:\n[\nu\n′\nv\n′\n]\n=\n[\nu\n​\n1\n+\nk\n1\n​\nr\n2\n+\nk\n2\n​\nr\n4\n+\nk\n3\n​\nr\n6\n1\n+\nk\n4\n​\nr\n2\n+\nk\n5\n​\nr\n4\n+\nk\n6\n​\nr\n6\n+\n2\n​\np\n1\n​\nu\n​\nv\n+\np\n2\n​\n(\nr\n2\n+\n2\n​\nu\n2\n)\n+\ns\n1\n​\nr\n2\n+\ns\n2\n​\nr\n4\nv\n​\n1\n+\nk\n1\n​\nr\n2\n+\nk\n2\n​\nr\n4\n+\nk\n3\n​\nr\n6\n1\n+\nk\n4\n​\nr\n2\n+\nk\n5\n​\nr\n4\n+\nk\n6\n​\nr\n6\n+\np\n1\n​\n(\nr\n2\n+\n2\n​\nv\n2\n)\n+\n2\n​\np\n2\n​\nu\n​\nv\n+\ns\n3\n​\nr\n2\n+\ns\n4\n​\nr\n4\n]\n\\displaystyle\\begin{bmatrix}u^{\\prime}\\\\\nv^{\\prime}\\end{bmatrix}=\\begin{bmatrix}u\\frac{1+k_{1}r^{2}+k_{2}r^{4}+k_{3}r^{6}}{1+k_{4}r^{2}+k_{5}r^{4}+k_{6}r^{6}}+2p_{1}uv+p_{2}(r^{2}+2u^{2})+s_{1}r^{2}+s_{2}r^{4}\\\\\nv\\frac{1+k_{1}r^{2}+k_{2}r^{4}+k_{3}r^{6}}{1+k_{4}r^{2}+k_{5}r^{4}+k_{6}r^{6}}+p_{1}(r^{2}+2v^{2})+2p_{2}uv+s_{3}r^{2}+s_{4}r^{4}\\\\\n\\end{bmatrix}\n(4)\nwhere\nr\nr\nis radial distance from the optical center,\nr\n2\n=\nu\n2\n+\nv\n2\nr^{2}=u^{2}+v^{2}\n.\nRegular camera calibration procedures can be used to identify all distortion coefficients, as well as the focal length and the principal point.\n2.3\nRay tracing and sampling\nIn ray tracing, an image is created by tracing the inverse path of a light ray from every sensor pixel through the camera lens until it intersects with an object. Multiple reflections can then be used to calculate the pixel intensity. The code assumes that the marker is perfectly illuminated without any specularity. Consequently, the visible color of the marker is simply the pixel value of its bitmap image.\nThe main difficulty in rendering a fiducial marker with a high fidelity originates from the aliasing of its edges. Most of fiducial markers are black-and-white binary bitmap images. This means that their edges are very sharp, creating abrupt transitions from black to white and vice versa. Rendering these high spatial frequencies therefore requires a high density of rays per pixel. However, the time taken to render an image is directly proportional to the number of computed rays.\nTo avoid aliasing without the computational expense of increasing the number of rays everywhere, adaptive sampling is used. First, a first coarse image is rendered with a single ray per pixel. The aim is to identify the regions of the image where over-sampling is required. Then, in a second step, pixels near edges are rendered again with numerous rays.\nTo achieve the best results, the sampled points within a pixel must also have good spatial coverage without introducing bias. Several low-discrepancy samplers have been proposed, such as the Halton sampler and the Sobol sequence, the latter being faster to calculate. FMAC uses the open-source implementation of the Sobol sequence written by Leonhard Gruenschloss\n1\n1\n1\nAvailable at:\nhttps://github.com/lgruen/sobol\n, that is based on the direction numbers\n[\n13\n]\n. The resulting spatial sampling of a pixel is illustrated in Figure\n2\na.\nSimilarly, rendering defocus aberrations requires tracing many rays from each pixel in order to adequately sample the lens aperture. To this aim, a concentric mapping of the lens is used, as illustrated in Figure\n2\nb.\nAt the initial coarse rendering step, the size of the circle of confusion is used to determine the regions of the image where the lens must be sampled.\nTo determine the number of rays to be cast in the regions identified above, the code uses the quantization limit of pixel intensity. In a digital video camera, sensors are often 8-bit (i.e., 256 levels of gray), or 12-bit for professional cameras. Due to this quantization, it is unnecessary to cast more rays than the bit depth. To be sure to obtain the best results, the maximum number of samples per pixel is then the bit depth of the camera sensor, for example 256 rays per pixel for a 8-bit camera.\n(a)\nPixel sampling with the Sobol sequence.\n(b)\nConcentric mapping of the lens.\nFigure 2:\nIllustration of pixels and lens sampling.\n2.4\nDiffraction blur\nDiffraction blur is often imperceptible in photorealistic renderings. At best, it is rendered using Gaussian filtering with a small kernel. However at the pixel level, it plays a significant role in the profile of a sharp edge.\nThe continuous diffraction pattern of a circular aperture corresponds to a circular shape named Airy disk, defined as follows:\ng\n​\n(\nu\n,\nv\n)\n=\na\n​\n[\n2\n​\nJ\n1\n​\n(\nπ\n​\nr\nr\na\n/\nr\nz\n)\nπ\n​\nr\nr\na\n/\nr\nz\n]\n2\n\\displaystyle g(u,v)=a\\left[\\frac{2\\,J_{1}(\\frac{\\pi r}{r_{a}/r_{z}})}{\\frac{\\pi r}{r_{a}/r_{z}}}\\right]^{2}\n(5)\nwhere\na\na\nis the amplitude of the Airy function,\nJ\n1\nJ_{1}\nis the first order Bessel function of the first kind,\nr\nr\nis the radial distance from the center of the kernel,\nr\na\nr_{a}\nthe radius of the Airy disk (radius of the first zero) and\nr\nz\n=\n1.2196698912665045\nr_{z}=1.2196698912665045\n.\nFor an optical lens, the radius of Airy disk\nr\na\nr_{a}\nis approximately\n2\n​\nr\nz\n​\nλ\n/\nd\n2\\,r_{z}\\lambda/d\n, where\nλ\n\\lambda\nis the wavelength of the light and\nd\nd\nis the diameter of\nthe aperture.\nAs this radius is of the same order of magnitude than the pixel size, the Airy pattern cannot be used directly as a discrete convolution kernel.\nTo obtain a discrete diffraction kernel, the Airy pattern must first be convolved with a rectangular function\nΠ\n\\Pi\nwhich is the size of a pixel. The resulting pattern is discretized at the pixel pitch\nδ\n\\delta\n:\nK\n​\n(\nu\n,\nv\n)\n=\n(\ng\n∗\nΠ\n)\n​\n(\nδ\n​\nu\n,\nδ\n​\nv\n)\n\\displaystyle K(u,v)=(g*\\Pi)(\\delta u,\\delta v)\n(6)\nThe diffraction blur is added after the ray tracing phase by convoluting the image with the kernel\nK\nK\nthat has been pre-calculated knowing the f-number and the wavelength of the light.\n2.5\nGamma correction and pixel quantification\nThe penultimate step in the pipeline involves applying gamma correction to the image intensity. The Rec. 709 transfer function\n[\n1\n]\nis used universally in almost all digital cameras:\nI\n′\n​\n(\nx\n,\ny\n)\n=\n{\n4.5\n​\nI\n​\n(\nx\n,\ny\n)\nif\n​\nI\n​\n(\nx\n,\ny\n)\n≤\n0.018\n1.099\n​\nI\n​\n(\nx\n,\ny\n)\n0.45\n−\n0.099\nelse\n\\displaystyle I^{\\prime}(x,y)=\\begin{cases}4.5I(x,y)&\\text{if }I(x,y)\\leq 0.018\\\\\n1.099I(x,y)^{0.45}-0.099&\\text{else}\\end{cases}\n(7)\nFinally, the corrected intensity is quantified according to the bit depth of the camera sensor (usually 8 bits or 12 bits).\nFigure 3:\nOverlay of actual and synthetic images from the OpenCV calibration dataset. The pixels colored in red indicate that the actual intensity is over the synthetic intensity. The pixels colored in cyan show the opposite errors. There is no difference between images if the pixels are white, black, or gray.\n(a)\nActual image\n(b)\nRendered image\nFigure 4:\nComparison of defocus aberrations on the Matlab’s calibration dataset. The bottom zooms show details that are in focus. The top zooms correspond to points that are beyond the depth of field limit.\n3\nRendering Validation\nThe quality of the rendered synthetic images was tested using two existing image sets. The first set comes from the OpenCV calibration examples. This set comprises 13 images of a chessboard captured with an 8-bit, 640 x 480 webcam such as the Logitech HD Webcam C270. The second set of images is part of MATLAB’s calibration examples. This set contains nine images of a chessboard taken with a Canon EOS Digital Rebel XS with an 18–55 mm zoom lens. This camera produces high-quality 12-bit images with a resolution of 2816 x 1880. Information on the imaging conditions have been found in the EXIF data of the files or guessed on the image itself. The complete characteristics of both cameras are presented in Table\n1\n.\nThe first image set was useful for verifying the projection model of the code. As the angle of view is large, the distortions are significant, enabling the thin lens and distortion models to be checked. The diffraction is relatively high due to the small size of the webcam’s pupil. For the same reason, the depth of field is large, resulting in almost no defocus blur on the chessboard. Superimposing the rendered images on the actual images shows very good concordance, as illustrated in Figure\n3\n.\nThe second image set was recorded with a higher f-number, resulting in significant defocus blur on some images. Figure\n4\nshows details of the actual and synthetic images of a chessboard. The bottom of both images is in focus, whereas the top is beyond the depth of field limit. The snapshot of the two corners shows a very similar gradient in both cases\n4\nFiducial Markers Accuracy Comparison\nThe evaluation of fiducial marker accuracy is fraught with several challenges. In experimental systems, the ground truth is inherently prone to uncertainty. Furthermore, it is difficult to reproduce identical testing conditions for multiple markers. Additionally, the process of capturing a large volume of images can be time-consuming. This section proposes a method for efficiently sampling the six degrees of freedom of the space using synthetic images. In order to enable fair comparison between various, large sets of synthetic images are generated for each marker but at identical poses. Then, the 36 combinations between each degree of freedom and each pose error are presented to assess potential correlations and trends of four markers.\n4.1\nSix degrees of freedom sampling\nThe pose of a fiducial marker in the camera frame is described by at minimum six values, three translation distances and three rotation angles (seven values in case of using quaternions). Sampling a six dimensional space requires a lot of points. Moreover, naive sampling approach such as regular grids can introduce a bias in the results by favouring certain frequencies.\nIn order to sample efficiently the space with minimal bias, we used the Halton sequence\n[\n11\n]\n. The Halton sequence is known for its low-discrepancy, the idea that the points are distributed more evenly across the space, minimizing clustering and gaps. Halton’s method generates points based on prime number bases (e.g., 2, 3, 5), which provides better coverage of the multidimensional space than purely random sampling. Like for Sobol sequences, the proposed code uses the open-source implementation of the Halton sequence written by Leonhard Gruenschloss\n2\n2\n2\nAvailable at:\nhttps://github.com/lgruen/halton\nthat supports both Faure-permutations and random digit permutations.\nWe generated a cloud of 10 000 images using the Logitech HD Webcam C270 parameters. The X and Y samples are restricted by the field of view taking the depth into account.\nThe sampling of depth ranges from 500 to 1 500 mm, ensuring that all the markers are in focus. The roll and pitch angle ranges are limited to -45 to 45 degrees. The yaw angle range is 360 degrees.\n4.2\nPose estimation analysis\nThis section compares the pose estimation errors of four well-known markers: ArUco\n[\n10\n]\n, AprilTag\n[\n23\n,\n29\n]\n, STag\n[\n5\n]\n, and TopoTag\n[\n34\n]\n. These markers have been chosen because each relies on a different way to achieve pose estimation. ArUco only uses the four corners of its outer square. AprilTag fits lines to outer edges gradient. STag uses the inner circular border to refine the estimated homography. TopoTag pose estimation takes advantage of all the inner marker vertices to increase the resolution.\nEach type of marker has been rendered in exactly the same poses, and its own detection code has been used to estimate the pose and compute the error against the ground truth. The size of the virtual markers are all 50x50 mm.\nFigure 5:\nErrors between the estimated poses of an ArUco marker and the actual pose for the six degrees of freedom. The images are rendered using the Logitech HD Webcam C270 parameters. The horizontal dashed lines represent the mean of the errors and the dotted lines the mean plus or minus the standard deviation.\nArUco\nThe first marker tested is the ArUco, as implemented in version 4.13.0 of OpenCV.\nFigure\n5\nshows the 36 pairs of combinations between errors and values for the six degrees of freedom in the form of an intercorrelation graph matrix.\nThis method allows to see the level of error for each value, as well as possible correlations with position/orientation in space.\nThroughout the whole measurement volume, the errors of pose estimation have a standard deviation of 5.4 mm in X and 3.8 mm in Y. These errors increase almost linearly as one moves away from the optical center. Errors in Z are more significant, reaching a standard deviation of 14.7 mm. They are always positive, which indicates that the detector overestimates the distance. The Z mean error is 24.2 mm and could be used to compensate the systematic overestimation. Rotation errors are fairly stable with a standard deviation in the order of a tens of degrees for all angles. The standard deviation is relatively large compared to the dispersion of the points because a small number of pose cause large errors, which can be explained by the detection ambiguity (less than 5 cases per thousand poses).\nA cyclical phenomenon appears clearly on the yaw angle with a period of approximately 90 degrees. This periodic error could indicate numerical errors in a trigonometric calculus.\nAll errors increase significantly as the marker moves away from the camera and becomes smaller in the image. Overall, the pose estimation is much more accurate between 500 and 900 mm in depth. However, the rate of ArUco detection is 100% on the whole image set.\nAprilTag\nFigure\n6\nshows the same tests, this time using an AprilTag marker. Detection was carried using the AprilTag 3.4.5 release, which can be found on GitHub\n3\n3\n3\nhttps://github.com/AprilRobotics/apriltag\n. 99.74% of the markers are detected. A very few of small and angled markers are not detected. The errors in all degrees of freedom are significantly smaller. As with ArUco, errors in X an Y are linearly correlated with the distance from the optical axis. All the errors are also correlated with the depth, albeit to a lesser extent. AprilTag is particularly good at estimating the rotation angles.\nAs with ArUco, the depth Z is always overestimated. The tests also revealed a systematic error in X and Y : the mean error is 0.62 mm in both directions. This error is equivalent to half a pixel, so we suspect that there is an unwanted constant offset in the code that should be removed in latter releases.\nFigure 6:\nErrors between the estimated poses of an ApriTag marker and the actual pose for the six degrees of freedom. The images are rendered using the Logitech HD Webcam C270 parameters. The horizontal dashed lines represent the mean of the errors and the dotted lines the mean plus or minus the standard deviation.\nSTag\nTo evaluate the STag marker pose estimation, we used the Stoiber’s fork of the original source code\n4\n4\n4\nhttps://github.com/manfredstoiber/stag\n. Figure\n7\nshows the detection results for the same set of poses.\nSTag achieves a similar error level to AprilTag for the X, Y, and Z translations, but the standard deviation of angular errors is significantly higher than for ArUco and AprilTag. Unlike ArUco and AprilTag, STag does not systematically overestimate depth. However, the errors appear to be distributed around two levels, suggesting that the method uses two different estimation modes.\nThe error in Z exhibits a periodic effect depending on the yaw angle. The detection rate of STag markers drops to 84.27%. Markers smaller than 40 pixels or close to the image borders are poorly detected.\nFigure 7:\nErrors between the estimated poses of a STag marker and the actual pose for the six degrees of freedom. The images are rendered using the Logitech HD Webcam C270 parameters. The horizontal dashed lines represent the mean of the errors and the dotted lines the mean plus or minus the standard deviation.\nTopoTag\nFinally, we tested the pose estimation of TopoTag using the binary code released on Github\n5\n5\n5\nhttps://github.com/herohuyongtao/topotag\n(TopoTag is not open-source unlike the three other markers).\nFor the same pose set, the detection rate is only 57.65%. TopoTags are correctly detected if their X and Y dimensions are greater than 50 pixels in the image corresponding to a distance of approximately 1000 mm. Beyond this depth, the detection rate drops dramatically, as can be seen in Figure\n8\n.\nOn the other hand, when the marker is detected, the positioning errors are very low for all degrees of freedom. These errors remain remarkably constant throughout the space. We only notice a slight variation in X and Y errors when the yaw angle changes.\nFigure 8:\nErrors between the estimated poses of a TopoTag marker and the actual pose for the six degrees of freedom. The images are rendered using the Logitech HD Webcam C270 parameters. The horizontal dashed lines represent the mean of the errors and the dotted lines the mean plus or minus the standard deviation.\n4.3\nAccuracy comparison\nFigure 9:\nComparison of the accuracy of four fiducial marker on the subset of 5625 poses that have been detected by all methods. The images are rendered using the Logitech HD Webcam C270 parameters.\nAccording to the analysis, TopoTag appears to be the most accurate. However, a direct comparison would not be fair, since TopoTag markers are detected only for closest poses. For this reason, we calculate the accuracy with the only 5625 poses that were detected by the four methods. Moreover, the accuracy is defined as the closeness of agreement between the average value obtained from a large series of test results and an accepted reference value\n[\n12\n]\n. Then, the accuracy corresponds to the average\nabsolute\nerror and not to the standard deviation of the error as presented in previous figures.\nAs our process is deterministic, one test per pose is sufficient.\nFor instance, the positional accuracy for X is given by:\nA\nx\n=\n1\nm\n​\n∑\nk\n=\n1\nm\n|\nx\ni\n−\nx\ni\n∗\n|\n\\displaystyle A_{x}=\\frac{1}{m}\\sum_{k=1}^{m}|x_{i}-x_{i}^{*}|\n(8)\nwhere\nm\nm\nis the number of poses,\nx\ni\nx_{i}\nthe abscissa estimation of the ith pose and\nx\ni\n∗\nx_{i}^{*}\nits true value.\nFigure\n9\npresents the comparison of the accuracy of the four markers for the six degrees of freedom. It clearly shows that TopoTag is the most accurate method for estimating the three translations. The mean absolute errors are approximately 0.1 mm in the X and Y directions, and 0.7 mm in the Z direction. Nevertheless, AprilTag is superior for estimating the three angles with typical errors of around 0.1 degrees. It is important to note that TopoTag detection only works well at depths ranging from 500 to 1000 mm whereas AprilTag can detect smaller markers up to 1500 mm.\n5\nConclusion\nThe methodology presented in this article aims to compare the accuracy of marker pose estimations using high-fidelity synthetic images.\nIt allows to study the potential correlations between pose errors and the six degrees of freedom of the space. The graph matrices showing the 36 possible combinations are particularly useful for verifying the correct working of the detection algorithms.\nThis article focuses on accuracy estimation, but other marker metrics could be evaluated with the proposed FMAC software. For example, by using a camera with a smaller f-number, it would be possible to test robustness to defocus blur. Other disturbances such as occlusion, uneven illumination and low image dynamics could also be integrated into a standardized testing process to objectively qualify the performances of each type of marker.\nReferences\n[1]\nI. T. U. (ITU)\n(1990)\nRecommendation ITU-R BT.709: basic parameter values for the hdtv standard for the studio and for international programme exchange\n.\nGeneva\n.\nCited by:\n§2.5\n.\n[2]\nB. Ahmad, P. Sandoz, and G. J. Laurent\n(2024)\n6-dof motion capture with nanometric resolutions over millimetric ranges using a pseudo-periodic encoded pattern\n.\nIEEE Transactions on Instrumentation and Measurement\n.\nCited by:\n§1\n.\n[3]\nM. Alghamdi, A. Al-Marakeby, and S. Abdel-Mageid\n(2025-06)\nMobile Robot Navigation Based on Artificial Markers: A Systematic Mapping Study\n.\nJournal of Robotics and Mechatronics\n37\n(\n3\n),\npp. 762–778\n.\nExternal Links:\nISSN 1883-8049, 0915-3942\n,\nDocument\nCited by:\n§1\n.\n[4]\nB. Atcheson, F. Heide, and W. Heidrich\n(2010)\nCaltag: High precision fiducial markers for camera calibration.\n.\nIn\nInt. Symposium on Vision, Modeling, and Visualization\n,\nVol.\n10\n,\npp. 41–48\n.\nCited by:\n§1\n.\n[5]\nB. Benligiray, C. Topal, and C. Akinlar\n(2019-09)\nSTag: A stable fiducial marker system\n.\nImage and Vision Computing\n89\n,\npp. 158–169\n.\nExternal Links:\nISSN 02628856\n,\nDocument\nCited by:\n1(b)\n,\n1(b)\n,\n§4.2\n.\n[6]\nT. M. Bocco and A. Rizzo\n(2021)\nHigh accuracy Pose Estimation with Computer Vision\n.\nMaster’s Thesis\nPolitecnico di Torino\n.\nCited by:\n§1\n.\n[7]\nA. Borrego, J. Latorre, R. Llorens, M. Alcañiz, and E. Noé\n(2016-12)\nFeasibility of a walking virtual reality system for rehabilitation: objective and subjective parameters\n.\nJournal of NeuroEngineering and Rehabilitation\n13\n(\n1\n),\npp. 68\n.\nExternal Links:\nISSN 1743-0003\n,\nDocument\nCited by:\n§1\n.\n[8]\nG. Čepon, D. Ocepek, M. Kodrič, M. Demšar, T. Bregar, and M. Boltežar\n(2024)\nImpact-pose estimation using aruco markers in structural dynamics\n.\nExperimental Techniques\n48\n(\n2\n),\npp. 369–380\n.\nCited by:\n§1\n.\n[9]\nA. Filion, A. Joubair, A. S. Tahan, and I. A. Bonev\n(2018-02)\nRobot calibration using a portable photogrammetry system\n.\nRobotics and Computer-Integrated Manufacturing\n49\n,\npp. 77–87\n.\nExternal Links:\nISSN 07365845\n,\nDocument\nCited by:\n§1\n.\n[10]\nS. Garrido-Jurado, R. Muñoz-Salinas, F.J. Madrid-Cuevas, and M.J. Marín-Jiménez\n(2014-06)\nAutomatic generation and detection of highly reliable fiducial markers under occlusion\n.\nPattern Recognition\n47\n(\n6\n),\npp. 2280–2292\n.\nExternal Links:\nISSN 00313203\n,\nDocument\nCited by:\n1(d)\n,\n1(d)\n,\n§4.2\n.\n[11]\nL. Grünschloß, M. Raab, and A. Keller\n(2012)\nEnumerating Quasi-Monte Carlo Point Sequences in Elementary Intervals\n.\nIn\nMonte Carlo and Quasi-Monte Carlo Methods 2010\n,\nL. Plaskota and H. Woźniakowski (Eds.)\n,\nVol.\n23\n,\npp. 399–408\n.\nExternal Links:\nDocument\n,\nISBN 978-3-642-27439-8 978-3-642-27440-4\nCited by:\n§4.1\n.\n[12]\nISO 5725-1:1994\n(1994)\nAccuracy (trueness and precision) of measurement methods and results\n.\nStandard\nTechnical Report\nISO 5725-1:1994\n,\nInternational Organization for Standardization\n.\nCited by:\n§4.3\n.\n[13]\nS. Joe and F. Y. Kuo\n(2008)\nConstructing sobol sequences with better two-dimensional projections\n.\nSIAM Journal on Scientific Computing\n30\n(\n5\n),\npp. 2635–2654\n.\nCited by:\n§2.3\n.\n[14]\nL. B. Junior, G. S. Berger, A. O. Júnior, J. Braun, M. A. Wehrmeister, M. F. Pinto, and J. Lima\n(2024)\nA Comparison of Fiducial Markers Pose Estimation for UAVs Indoor Precision Landing\n.\nIn\nOptimization, Learning Algorithms and Applications\n,\nA. I. Pereira, A. Mendes, F. P. Fernandes, M. F. Pacheco, J. P. Coelho, and J. Lima (Eds.)\n,\nVol.\n1981\n,\npp. 18–33\n.\nExternal Links:\nDocument\n,\nISBN 978-3-031-53024-1 978-3-031-53025-8\nCited by:\n§1\n.\n[15]\nD. Jurado-Rodriguez, R. Munoz-Salinas, S. Garrido-Jurado, and R. Medina-Carnicer\n(2021)\nDesign, Detection, and Tracking of Customized Fiducial Markers\n.\nIEEE Access\n9\n,\npp. 140066–140078\n.\nExternal Links:\nISSN 2169-3536\n,\nDocument\nCited by:\n§1\n.\n[16]\nM. Kalaitzakis, B. Cain, S. Carroll, A. Ambrosi, C. Whitehead, and N. Vitzilaios\n(2021)\nFiducial Markers for Pose Estimation: Overview, Applications and Experimental Comparison of the ARTag, AprilTag, ArUco and STag Markers\n.\nJournal of Intelligent & Robotic Systems\n101\n(\n4\n),\npp. 71\n.\nExternal Links:\nISSN 0921-0296, 1573-0409\n,\nDocument\nCited by:\n§1\n.\n[17]\nO. Kedilioglu, T. M. Bocco, M. Landesberger, A. Rizzo, and J. Franke\n(2021)\nArUcoE: enhanced aruco marker\n.\nIn\n2021 21st International Conference on Control, Automation and Systems (ICCAS)\n,\npp. 878–881\n.\nCited by:\n§1\n.\n[18]\nT. Le, Q. Tran, X. Nguyen, and C. Lin\n(2022-02)\nSolpen: An Accurate 6-DOF Positioning Tool for Vision-Guided Robotics\n.\nElectronics\n11\n(\n4\n).\nExternal Links:\nISSN 2079-9292\n,\nDocument\nCited by:\n§1\n.\n[19]\nZ. Lei, X. Chen, X. Chen, and L. Chai\n(2020-04)\nRadial Coverage Strength for Optimization of Multi-Camera Deployment\n.\narXiv\n.\nExternal Links:\n2004.00787\n,\nDocument\nCited by:\n§1\n.\n[20]\nZ. Lin, C. Lei, and L. Yang\n(2023-12)\nModern Image-Guided Surgery: A Narrative Review of Medical Image Processing and Visualization\n.\nSensors\n23\n(\n24\n),\npp. 9872\n.\nExternal Links:\nISSN 1424-8220\n,\nDocument\nCited by:\n§1\n.\n[21]\nA. López-Cerón and J. M. Cañas\n(2022-09)\nAccuracy analysis of marker-based 3D visual localization\n.\nIn\nXXXVII Jornadas de Automática\n,\nMadrid\n,\npp. 1124–1131\n.\nExternal Links:\nDocument\n,\nISBN 978-84-9749-808-1\nCited by:\n§1\n.\n[22]\nD. P. Mersch, A. Crespi, and L. Keller\n(2013-05)\nTracking Individuals Shows Spatial Fidelity Is a Key Regulator of Ant Social Organization\n.\nScience\n340\n(\n6136\n),\npp. 1090–1093\n.\nExternal Links:\nISSN 0036-8075, 1095-9203\n,\nDocument\nCited by:\n§1\n.\n[23]\nE. Olson\n(2011-05)\nAprilTag: A robust and flexible visual fiducial system\n.\nIn\n2011 IEEE International Conference on Robotics and Automation\n,\nShanghai, China\n,\npp. 3400–3407\n.\nExternal Links:\nDocument\n,\nISBN 978-1-61284-386-5\nCited by:\n1(a)\n,\n1(a)\n,\n§1\n,\n§4.2\n.\n[24]\nM. Pharr, W. Jakob, and G. Humphreys\n(2023)\nPhysically based rendering: from theory to implementation\n.\nMIT Press\n.\nExternal Links:\nLink\nCited by:\n§2\n.\n[25]\nD. D. W. Rijlaarsdam, M. Zwick, and J.M. (. Kuiper\n(2022-08)\nA novel encoding element for robust pose estimation using planar fiducials\n.\nFrontiers in Robotics and AI\n9\n,\npp. 838128\n.\nExternal Links:\nISSN 2296-9144\n,\nDocument\nCited by:\n§1\n.\n[26]\nF. J. Romero-Ramirez, R. Muñoz-Salinas, and R. Medina-Carnicer\n(2021-03)\nTracking fiducial markers with discriminative correlation filters\n.\nImage and Vision Computing\n107\n,\npp. 104094\n.\nExternal Links:\nISSN 02628856\n,\nDocument\nCited by:\n§1\n.\n[27]\nK. Shabalina, A. Sagitov, L. Sabirova, H. Li, and E. Magid\n(2020)\nARTag, AprilTag and CALTag Fiducial Systems Comparison in a Presence of Partial Rotation: Manual and Automated Approaches\n.\nIn\nInformatics in Control, Automation and Robotics\n,\nO. Gusikhin and K. Madani (Eds.)\n,\nVol.\n495\n,\npp. 536–558\n.\nExternal Links:\nDocument\n,\nISBN 978-3-030-11291-2 978-3-030-11292-9\nCited by:\n§1\n.\n[28]\nT. Tsoy, R. Safin, R. Sultanov, S. K. Saha, and E. Magid\n(2022)\nRecommended Criteria for Qualitative Comparison of Fiducial Markers Performance\n.\nIn\nIEEE International Siberian Conference on Control and Communications\n,\nTomsk, Russian Federation\n,\npp. 1–5\n.\nExternal Links:\nDocument\n,\nISBN 978-1-66547-628-7\nCited by:\n§1\n.\n[29]\nJ. Wang and E. Olson\n(2016-10)\nAprilTag 2: Efficient and robust fiducial detection\n.\nIn\n2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nDaejeon, South Korea\n,\npp. 4193–4198\n.\nExternal Links:\nDocument\n,\nISBN 978-1-5090-3762-9\nCited by:\n1(a)\n,\n1(a)\n,\n§4.2\n.\n[30]\nB. Wu, L. Wang, X. Liu, L. Wang, and K. Xu\n(2021-10)\nClosed-Loop Pose Control and Automated Suturing of Continuum Surgical Manipulators With Customized Wrist Markers Under Stereo Vision\n.\nIEEE Robotics and Automation Letters\n6\n(\n4\n),\npp. 7137–7144\n.\nExternal Links:\nISSN 2377-3766, 2377-3774\n,\nDocument\nCited by:\n§1\n.\n[31]\nP. Wu, Y. Tsai, and S. Chien\n(2014-09)\nStable pose tracking from a planar target with an analytical motion model in real-time applications\n.\nIn\nIEEE 16th International Workshop on Multimedia Signal Processing\n,\nJakarta, Indonesia\n,\npp. 1–6\n.\nExternal Links:\nDocument\n,\nISBN 978-1-4799-5896-2\nCited by:\n§1\n.\n[32]\nY. Yin, D. Gao, K. Deng, and Y. Lu\n(2024-10)\nVision-based autonomous robots calibration for large-size workspace using ArUco map and single camera systems\n.\nPrecision Engineering\n90\n,\npp. 191–204\n.\nExternal Links:\nISSN 01416359\n,\nDocument\nCited by:\n§1\n.\n[33]\nG. Yu, Y. Hu, and J. Dai\n(2021-09)\nTopoTag: A Robust and Scalable Topological Fiducial Marker System\n.\nIEEE Transactions on Visualization and Computer Graphics\n27\n(\n9\n),\npp. 3769–3780\n.\nExternal Links:\n1908.01450\n,\nISSN 1077-2626, 1941-0506, 2160-9306\n,\nDocument\nCited by:\n§1\n.\n[34]\nG. Yu, Y. Hu, and J. Dai\n(2021-09)\nTopoTag: A Robust and Scalable Topological Fiducial Marker System\n.\nIEEE Transactions on Visualization and Computer Graphics\n27\n(\n9\n),\npp. 3769–3780\n.\nExternal Links:\n1908.01450\n,\nISSN 1077-2626, 1941-0506, 2160-9306\n,\nDocument\nCited by:\n1(c)\n,\n1(c)\n,\n§4.2\n.\n[35]\nZ. Zhang, Y. Hu, G. Yu, and J. Dai\n(2022)\nDeepTag: A General Framework for Fiducial Marker Design and Detection\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n,\npp. 1–1\n.\nExternal Links:\nISSN 0162-8828, 2160-9292, 1939-3539\n,\nDocument\nCited by:\n§1\n.",
    "preview_text": "This paper presents a method for carrying fair comparisons of the accuracy of pose estimation using fiducial markers. These comparisons rely on large sets of high-fidelity synthetic images enabling deep exploration of the 6 degrees of freedom. A low-discrepancy sampling of the space allows to check the correlations between each degree of freedom and the pose errors by plotting the 36 pairs of combinations. The images are rendered using a physically based ray tracing code that has been specifically developed to use the standard calibration coefficients of any camera directly. The software reproduces image distortions, defocus and diffraction blur. Furthermore, sub-pixel sampling is applied to sharp edges to enhance the fidelity of the rendered image. After introducing the rendering algorithm and its experimental validation, the paper proposes a method for evaluating the pose accuracy. This method is applied to well-known markers, revealing their strengths and weaknesses for pose estimation. The code is open source and available on GitHub.\n\nFMAC: a Fair Fiducial Marker Accuracy Comparison Software\nGuillaume J. Laurent and Patrick Sandoz\n(\nUniversité Marie et Louis Pasteur, CNRS, SupMicroTech-ENSMM\nFEMTO-ST Institute, Besançon, F-25000, France.\n)\nAbstract\nThis paper presents a method for carrying fair comparisons of the accuracy of pose estimation using fiducial markers. These comparisons rely on large sets of high-fidelity synthetic images enabling deep exploration of the 6 degrees of freedom. A low-discrepancy sampling of the space allows to check the correlations between each degree of freedom and the pose errors by plotting the 36 pairs of combinations. The images are rendered using a physically based ray tracing code that has been specifically developed to use the standard calibration coefficients of any camera directly. The software reproduces image distortions, defocus and diffraction blur. Furthermore, sub-pixel sampling is applied to sharp edges to enhance the",
    "is_relevant": false,
    "relevance_score": 0.0,
    "extracted_keywords": [
        "fiducial markers",
        "pose estimation",
        "synthetic images",
        "ray tracing",
        "accuracy comparison"
    ],
    "one_line_summary": "这篇论文介绍了一种用于公平比较基准标记姿态估计精度的软件，基于高保真合成图像和物理渲染技术。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T16:55:26Z",
    "created_at": "2026-01-21T12:09:11.689204",
    "updated_at": "2026-01-21T12:09:11.689212"
}