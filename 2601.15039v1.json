{
    "id": "2601.15039v1",
    "title": "CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes",
    "authors": [
        "Jiyao Zhang",
        "Zhiyuan Ma",
        "Tianhao Wu",
        "Zeyuan Chen",
        "Hao Dong"
    ],
    "abstract": "åœ¨æ‚ä¹±ç¯å¢ƒä¸­è¿›è¡Œçµå·§æŠ“å–é¢ä¸´ç€å·¨å¤§æŒ‘æˆ˜ï¼Œè¿™æºäºçµå·§æ‰‹çš„é«˜è‡ªç”±åº¦ã€é®æŒ¡ç°è±¡ä»¥åŠå¤šæ ·ç‰©ä½“å‡ ä½•å½¢çŠ¶ä¸å¤æ‚å¸ƒå±€å¯èƒ½å¼•å‘çš„ç¢°æ’ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºCADGraspâ€”â€”ä¸€ç§åŸºäºå•è§†è§’ç‚¹äº‘è¾“å…¥çš„ä¸¤é˜¶æ®µé€šç”¨çµå·§æŠ“å–ç®—æ³•ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é¢„æµ‹ç¨€ç–IBSï¼ˆä¸€ç§è§£è€¦åœºæ™¯ã€æ„ŸçŸ¥æ¥è§¦ä¸ç¢°æ’çš„è¡¨å¾ï¼‰ä½œä¸ºä¼˜åŒ–ç›®æ ‡ã€‚ç¨€ç–IBSç´§å‡‘åœ°ç¼–ç äº†çµå·§æ‰‹ä¸åœºæ™¯é—´çš„å‡ ä½•åŠæ¥è§¦å…³ç³»ï¼Œèƒ½å¤Ÿå®ç°ç¨³å®šä¸”æ— ç¢°æ’çš„çµå·§æŠ“å–å§¿æ€ä¼˜åŒ–ã€‚ä¸ºæå‡è¿™ä¸€é«˜ç»´è¡¨å¾çš„é¢„æµ‹ç²¾åº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…·æœ‰ä½“ç´ çº§æ¡ä»¶å¼•å¯¼å’ŒåŠ›é—­åˆåˆ†æ•°ç­›é€‰åŠŸèƒ½çš„å ç”¨æ‰©æ•£æ¨¡å‹ã€‚ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬åŸºäºç¨€ç–IBSå¼€å‘äº†å¤šç§èƒ½é‡å‡½æ•°ä¸æ’åºç­–ç•¥è¿›è¡Œä¼˜åŒ–ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„çµå·§æŠ“å–å§¿æ€ã€‚åœ¨ä»¿çœŸç¯å¢ƒä¸çœŸå®åœºæ™¯ä¸­çš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶èƒ½å¤Ÿåœ¨ä¿æŒé«˜æŠ“å–æˆåŠŸç‡çš„åŒæ—¶ï¼Œæœ‰æ•ˆå‡å°‘å„ç±»ç‰©ä½“å’Œå¤æ‚åœºæ™¯ä¸­çš„ç¢°æ’é£é™©ã€‚",
    "url": "https://arxiv.org/abs/2601.15039v1",
    "html_url": "https://arxiv.org/html/2601.15039v1",
    "html_content": "CA\nD\nGrasp\n: Learning\nC\nontact and\nC\nollision\nA\nware General\nD\nexterous\nGrasp\ning in Cluttered Scenes\nJiyao Zhang\n1,2 *\n, Zhiyuan Ma\n1,2 *\n, Tianhao Wu\n1,2\n, Zeyuan Chen\n1,2\n, Hao Dong\n1,2\nâ€ \n\\dagger\n1\nCenter on Frontiers of Computing Studies, School of Computer Science, Peking University\n2\nNational Key Laboratory for Multimedia Information Processing,\nSchool of Computer Science, Peking University\njiyaozhang@stu.pku.edu.cn\nAbstract\nDexterous grasping in cluttered environments presents substantial challenges due to the high degrees of freedom of dexterous hands, occlusion, and potential collisions arising from diverse object geometries and complex layouts. To address these challenges, we propose\nCA\nD\nGrasp\n, a two-stage algorithm for general dexterous grasping using single-view point cloud inputs. In the first stage, we predict sparse IBS, a scene-decoupled, contact- and collision-aware representation, as the optimization target. Sparse IBS compactly encodes the geometric and contact relationships between the dexterous hand and the scene, enabling stable and collision-free dexterous grasp pose optimization. To enhance the prediction of this high-dimensional representation, we introduce an occupancy-diffusion model with voxel-level conditional guidance and force closure score filtering. In the second stage, we develop several energy functions and ranking strategies for optimization based on sparse IBS to generate high-quality dexterous grasp poses. Extensive experiments in both simulated and real-world settings validate the effectiveness of our approach, demonstrating its capability to mitigate collisions while maintaining a high grasp success rate across diverse objects and complex scenes. More details and videos are available at\nhttps://cadgrasp.github.io/\n.\nFigure 1:\nWe propose\nCA\nD\nGrasp\n, which learns a contact- and collision-aware intermediate representation as a constraint, and further obtains the dexterous grasp pose with an optimization method to achieve single-view dexterous hand grasping in cluttered scenes.\n{NoHyper}\nâ€ \nâ€ \n*: equal contribution,\nâ€ \n\\dagger\n: corresponding author\n1\nIntroduction\nDexterous grasping in cluttered scenes is a critical step toward enabling a dexterous hand to autonomously perform diverse tasks in real-world environments. Compared to single-object dexterous grasping\nXu\net al.\n(\n2024\n); Lu\net al.\n(\n2024\n); Zhong\net al.\n(\n2025\n); Qin\net al.\n(\n2022\n); Wan\net al.\n(\n2023\n); Huang\net al.\n(\n2024\n); Singh\net al.\n(\n2024\n)\n, the diverse and complex layouts in cluttered scenes\nFang\net al.\n(\n2020\n,\n2023\n); Li\net al.\n(\n2022\n); Zhang\net al.\n(\n2024a\n); Chen\net al.\n(\n2025\n)\nintroduce additional challenges beyond generalizing to various objects. The stacking of objects leads to occlusion, resulting in partial observations without full object geometry. Moreover, the restricted graspability caused by stacking requires more precise grasp poses while avoiding collisions with surrounding objects to prevent unintended outcomes, such as target object displacement due to contact between the dexterous hand and nearby objects.\nCurrent methods focus on constructing large-scale synthetic datasets to capture the distribution of potential cluttered scenes\nLi\net al.\n(\n2022\n); Zhang\net al.\n(\n2024a\n)\n. Based on these datasets, existing approaches typically first filter scene points with high graspability. Conditioned on these points, they employ either regression-based\nLi\net al.\n(\n2022\n)\nor generative-based models\nZhang\net al.\n(\n2024a\n)\nto predict grasp poses. However, directly mapping partial point cloud observations to grasp poses is challenging to generalize due to the non-linearity of the mapping from 3D point cloud space to pose space and the sensitivity of physical constraints to small variations in hand pose\nLi\net al.\n(\n2023a\n)\n. Current methods\nJiang\net al.\n(\n2021\n); Xu\net al.\n(\n2023\n); Li\net al.\n(\n2023b\n,\na\n)\nadopt a two-stage framework that combines contact map prediction and optimization to enhance generalization. However, these methods are primarily designed for single-object grasping, assume access to complete object geometry for optimization, which is not available in real-world cluttered scenes due to partial observation.\nTo tackle this problem, we propose predicting a contact- and collision-aware intermediate representation to serve as the optimization target as shown in Â Figure\n1\n. The proposed representation, sparse IBS, is the interaction bisector surface (IBS)\nKim\net al.\n(\n2006\n)\nbetween the scene and the dexterous hand, incorporating compact contact indicators. This representation is decoupled from the scene, eliminating the requirement for complete scene geometry, making it well-suited for cluttered scenes. Additionally, sparse IBS captures both geometric and contact information between the scene and the dexterous hand, making it effective for optimizing stable and collision-free dexterous grasp poses. To efficiently generate such a high-dimensional representation, we propose an occupancy-diffusion model with voxel-level conditional guidance and force closure score filtering. To further obtain stable and collision-free dexterous grasp poses, we design a set of energy functions tailored to sparse IBS.\nIn our experiments, we conduct extensive evaluations in simulation environments featuring 670 diverse cluttered scenes containing over 1300 objects. Comparative results demonstrate the effectiveness of our framework, while ablation studies further validate the effectiveness of our design. Our analysis highlights the stability and collision-awareness of the generated grasp poses. Additionally, we evaluate our method against other baselines in real-world settings to validate its practicality.\nIn summary, our contribution is as follows:\nâ€¢\nWe propose a two-stage framework consisting of scene-decoupled, contact- and collision-aware intermediate representation prediction and constrained grasp pose optimization for general dexterous grasping in cluttered scenes.\nâ€¢\nWe propose an occupancy-diffusion model with voxel-level conditional guidance and force closure score filtering to enhance representation prediction, along with several energy functions and ranking strategies to improve final grasp pose optimization.\nâ€¢\nWe conduct comprehensive simulation and real-world experiments to demonstrate the effectiveness of our method.\n2\nRelated Work\n2.1\nOne-stage Dexterous Grasp Pose Prediction\nOne-stage dexterous grasp pose generation\nXu\net al.\n(\n2024\n); Lu\net al.\n(\n2024\n); Zhong\net al.\n(\n2025\n); Huang\net al.\n(\n2023\n); Schmidt\net al.\n(\n2018\n); Liu\net al.\n(\n2019\n)\naims to train an end-to-end model to predict grasp poses. Regression-based methods\nSchmidt\net al.\n(\n2018\n); Liu\net al.\n(\n2019\n); Chen\net al.\n(\n2022\n)\nassume a one-to-one mapping between the object or scene and the grasp pose, which is limited in capturing the multi-modal dexterous grasp pose distribution due to the high degree of freedom. Generative-based methods\nHuang\net al.\n(\n2023\n); Lu\net al.\n(\n2024\n); Zhang\net al.\n(\n2023\n,\n2024b\n); Wu\net al.\n(\n2023\n)\ncan model complex distributions, making them more suitable for dexterous grasp pose generation. Current approaches leverage physical constraints\nZhong\net al.\n(\n2025\n); Lu\net al.\n(\n2024\n)\n, such as contact and penetration, to enhance grasp pose quality. However, end-to-end methods struggle with generalization due to the non-linearity between the observation space and pose space, as well as the sensitivity of physical constraints to small errors in grasp poses\nLi\net al.\n(\n2023a\n)\n. This challenge becomes more pronounced in cluttered environments, where the diversity of objects and layouts necessitates extremely large-scale grasp pose datasets\nLi\net al.\n(\n2022\n); Zhang\net al.\n(\n2024a\n)\n, limiting generalization. In contrast, we propose a two-stage framework that first predicts an intermediate representation in the observation space, followed by optimization based on this representation to enhance generalization.\n2.2\nTwo-stage Dexterous Grasp Pose Prediction\nTwo-stage dexterous grasp pose generation\nLi\net al.\n(\n2023b\n); Jiang\net al.\n(\n2021\n); Patzelt\net al.\n(\n2021\n); Lundell\net al.\n(\n2021a\n); Wei\net al.\n(\n2022\n); Mayer\net al.\n(\n2022\n)\ndecomposes grasp pose prediction into two stages to mitigate the challenges of direct mapping. Typically, the first stage predicts either the grasp pose\nPatzelt\net al.\n(\n2021\n); Lundell\net al.\n(\n2021a\n); Wei\net al.\n(\n2022\n); Lundell\net al.\n(\n2021b\n); Weng\net al.\n(\n2024\n)\nor an intermediate representation\nMayer\net al.\n(\n2022\n); Li\net al.\n(\n2023b\n); Jiang\net al.\n(\n2021\n)\n, followed by optimization based on physical constraints between the object and the dexterous hand. However, these methods are specifically designed for single-object grasping and assume prior knowledge of complete object geometries, which is often unavailable in cluttered scenes due to object stacking\nZhang\net al.\n(\n2024a\n); Li\net al.\n(\n2022\n)\n, even with multi-view cameras. This limitation restricts the applicability of two-stage methods in cluttered environments. In contrast, we propose a sparse IBS representation that serves as an intermediate representation, enabling the application of a two-stage framework in cluttered scenes.\n2.3\nHand-Object Representation\nHand-object representation can be primarily utilized in two ways. The first is to serve as the observation\nLiu\net al.\n(\n2023\n); She\net al.\n(\n2022\n); Mayer\net al.\n(\n2022\n); She\net al.\n(\n2024\n)\n, compressing the observation feature space and reducing the complexity of geometric feature learning, thereby enhancing grasp pose generation\nMayer\net al.\n(\n2022\n)\nor grasp policy learning\nLiu\net al.\n(\n2023\n); She\net al.\n(\n2022\n)\n. The second is to act as an intermediate representation for two-stage dexterous grasp pose generation methods\nJiang\net al.\n(\n2021\n); Li\net al.\n(\n2023b\n)\n.\nThe most common intermediate representation for two-stage grasp pose generation is the contact map\nLi\net al.\n(\n2023b\n); Xu\net al.\n(\n2023\n); Li\net al.\n(\n2023a\n)\n, which computes the distance between each point on the object and the dexterous hand. However, this representation requires complete object geometry, which is not accessible in cluttered scenes. The most relevant to our work is Interaction Bisector Surface (IBS)\nShe\net al.\n(\n2022\n)\n, which computes a surface between the scene and the dexterous hand along with various spatial and contact information. However, such representation has only been used as an observation representation\nShe\net al.\n(\n2022\n,\n2024\n)\n.\nIn contrast, we propose using IBS as an intermediate representation for grasp pose optimization. To reduce the difficulty of predicting such representation, we design a more compact sparse IBS representation and develop a specialized module to predict sparse IBS along with optimization strategies tailored to our sparse IBS representation.\n3\nMethod\nFigure 2:\nOverview of\nCA\nD\nGrasp\n, a two-stage framework for dexterous grasping in cluttered scenes.\n(I)\nConditional IBS Generation: A diffusion model is trained to model the conditional probability distribution\np\nâ€‹\n(\nâ„\n|\nğ’«\n,\nğ“\n)\np(\\mathcal{I}|\\mathcal{P},\\mathbf{T})\n.\n(II)\nGrasp Pose Optimization: We optimize the grasp poses\nğ’¢\n\\mathcal{G}\nwith predicted sparse IBS\nâ„\n^\n\\hat{\\mathcal{I}}\nas constraints.\nProblem Formulation.\nIn this work, we consider the problem of generating a set of grasp poses for a dexterous hand to grasp objects in a cluttered environment. We define a grasp pose of a dexterous hand as a tuple\nğ \n=\n{\nğ“\n,\nğ‰\n}\n\\mathbf{g}=\\{\\mathbf{T},\\mathbf{J}\\}\n, where\nğ“\nâˆˆ\nSE\nâ€‹\n(\n3\n)\n\\mathbf{T}\\in\\text{SE}(3)\nindicates the wrist pose,\nğ‰\nâˆˆ\nâ„\nn\n\\mathbf{J}\\in\\mathbb{R}^{n}\nis the joint configuration of the hand, and\nn\nn\nis the degree of freedom (DoF) of the hand. Given a single view pointcloud\nğ’«\nâˆˆ\nâ„\nN\nÃ—\n3\n\\mathcal{P}\\in\\mathbb{R}^{N\\times 3}\nof a cluttered scene, we estimate the grasp poses\nğ’¢\n=\n{\nğ \ni\n}\ni\n=\n1\n|\nğ’¢\n|\n\\mathcal{G}=\\{\\mathbf{g}_{i}\\}_{i=1}^{|\\mathcal{G}|}\nthat are stable and collision-free for dexterous grasping.\nOverview.\nThe overview of our method is shown inÂ Figure\n2\n. We propose a two-stage framework for dexterous grasping in cluttered scenes, called\nCA\nD\nGrasp\n. We use the sparse IBS\nâ„\n\\mathcal{I}\nthat is aware of contact and collisions as an intermediate representation between the two stages, which can efficiently encode the geometric relationship between the dexterous hand and the scene corresponding to the successful grasp poses (Â Section\n3.1\n). In the first stage, we model the conditional probability distribution\np\nâ€‹\n(\nâ„\n|\nğ’«\n,\nğ“\n)\np(\\mathcal{I}|\\mathcal{P},\\mathbf{T})\nby a diffusion model, where\nâ„\n\\mathcal{I}\nis the sparse IBS,\nğ’«\n\\mathcal{P}\nis the single-view observed scene point cloud, and\nğ“\n\\mathbf{T}\nis the wrist pose (Â Section\n3.2\n). In the second stage, with the predicted sparse IBS\nâ„\n^\n\\hat{\\mathcal{I}}\nas constraints, we get dexterous grasp poses\nğ’¢\n\\mathcal{G}\nvia an optimization algorithm (Â Section\n3.3\n).\n3.1\nContact and Collision Aware IBS for Dexterous Grasping.\nIBS\nKim\net al.\n(\n2006\n)\nis the Voronoi diagram between two close 3D geometric objects. Inspired by the efficiency of IBS in describing the spatial relationship between 3D objects and its successful application in dexterous hand manipulation\nShe\net al.\n(\n2022\n,\n2024\n)\n, we adapt IBS to represent the geometric relationship between the dexterous hand and the environment as well as the object when a successful grasping state is achieved. We generate the sparse IBS\nâ„\n\\mathcal{I}\nin a simulator for training, as shown inÂ Figure\n3\n. Specifically, for each dexterous grasp pose\nğ \n\\mathbf{g}\n, we define a canonical space centered at the grasp seed point\np\ns\np_{s}\nwith the rotation\nğ‘\n\\mathbf{R}\nof the wrist pose\nğ“\n=\n[\nğ‘\n|\nğ­\n]\n\\mathbf{T}=[\\mathbf{R}|\\mathbf{t}]\nas the direction. The sparse IBS\nâ„\nâˆˆ\nâ„\nn\nÃ—\nn\nÃ—\nn\nÃ—\n3\n\\mathcal{I}\\in\\mathbb{R}^{n\\times n\\times n\\times 3}\nis defined with a resolution of\nn\nn\n, where the last three dimensions are:\nâ€¢\nOccupancy of the IBS surface, which is 1 if the voxel in\nâ„\n\\mathcal{I}\nin IBS surface, otherwise it is\nâˆ’\n1\n-1\n.\nâ€¢\nOccupancy of contact points between the thumb and the target object, which is 1 if the voxel in\nâ„\n\\mathcal{I}\nis a contact point between the thumb and the object, otherwise it is\nâˆ’\n1\n-1\n.\nâ€¢\nOccupancy of contact points between other fingers and the object, which is 1 if the voxel in\nâ„\n\\mathcal{I}\nis a contact point between other fingers and the object, otherwise it is\nâˆ’\n1\n-1\n.\nThe sparse IBS\nâ„\n\\mathcal{I}\nencodes the geometric relationship between the hand corresponding to a successful grasp pose\nğ \n\\mathbf{g}\nand the environment, which includes not only the configuration of the hand but also the contact relationship between fingers and objects, and specifies a safety bound to ensure collision-free with environments.\nFigure 3:\nCreation of the sparse IBS for dexterous grasping.\nGiven a cluttered scene, we first generate the grasp pose\nğ \n\\mathbf{g}\nusing an optimization algorithm. Then, we canonicalize and crop the scene point cloud\nğ’«\n\\mathcal{P}\nto obtain the canonicalized point cloud\nğ’«\nâˆ—\n\\mathcal{P}^{*}\n. Finally, we compute the sparse IBS\nâ„\n\\mathcal{I}\nbased on the canonicalized point cloud.\n3.2\nConditional IBS Generation.\nIn this section, our goal is to generate the intermediate representation IBS to provide constraints for the subsequent grasp pose optimization stage. Specifically, as shown inÂ Figure\n2\n, given a single-view observed point cloud\nğ’«\n\\mathcal{P}\n, we first predict the wrist pose\nğ“\n\\mathbf{T}\nof the dexterous hand. Then, given\nğ“\n\\mathbf{T}\nand\nğ’«\n\\mathcal{P}\n, we generate multiple IBS candidates\n{\nâ„\ni\n}\ni\n=\n1\nn\n\\{\\mathcal{I}_{i}\\}_{i=1}^{n}\n, where\nn\nn\nis the number of candidates. Finally, we rank the IBS candidates and select the optimal one\nâ„\n^\n\\hat{\\mathcal{I}}\nas the final intermediate representation.\nWrist Pose Estimation.\nWe use the same structure as DexGraspNet2.0\nZhang\net al.\n(\n2024a\n)\n. First, given the scene point cloud\nğ’«\nâˆˆ\nâ„\nN\nÃ—\n3\n\\mathcal{P}\\in\\mathbb{R}^{N\\times 3}\n, where\nN\nN\nis the number of points, we extract point-wise features\nâ„±\n=\n{\nf\ni\n}\ni\n=\n1\nN\n\\mathcal{F}=\\{f_{i}\\}_{i=1}^{N}\nusing ResUNet14\nChoy\net al.\n(\n2019\n)\nand predict point-wise graspness\nğ’®\n=\n{\ns\ni\n}\ni\n=\n1\nN\n\\mathcal{S}=\\{s_{i}\\}_{i=1}^{N}\n. Finally, after ranking and FPS sampling, we obtain the final set of grasp seed points\n{\np\ns\ni\n}\ni\n=\n1\nM\n\\{p_{s}^{i}\\}_{i=1}^{M}\n, where\nM\nM\nis the number of sampled points. For each grasp seed point\np\ns\np_{s}\n, we condition on the corresponding point feature\nf\nf\n, and use a denoising diffusion model\nZheng\net al.\n(\n2023\n)\nto directly model the joint probability distribution\np\nâ€‹\n(\nğ“\n|\nf\n)\np(\\mathbf{T}|f)\nin Euclidean space and obtain the denoised wrist pose\nğ“\n\\mathbf{T}\nwith reverse ODE process.\nFigure 4:\nIBS generation.\nWe train a conditional occupancy-diffusion model to model the conditional probability distribution\np\nâ€‹\n(\nâ„\n|\nğ’«\nâˆ—\n)\np(\\mathcal{I}|\\mathcal{P}^{*})\n, where\nğ’«\nâˆ—\n\\mathcal{P}^{*}\nis the canonicalized and voxelized point cloud. The voxel-level alignment provides hierarchical conditions during the denoising process.\nIBS Candidates Generation.\nAs shown inÂ Figure\n4\n, given the selected grasp seed point\np\ns\np_{s}\nand the corresponding wrist pose\nğ“\n\\mathbf{T}\n, we define a canonical space with\np\ns\np_{s}\nas the coordinate origin and the rotation of\nğ“\n\\mathbf{T}\nas the direction. We canonicalize and voxelize the original observed point cloud\nğ’«\n\\mathcal{P}\nto obtain\nğ’«\nâˆ—\nâˆˆ\nâ„\nn\nÃ—\nn\nÃ—\nn\nÃ—\n1\n\\mathcal{P}^{*}\\in\\mathbb{R}^{n\\times n\\times n\\times 1}\n, where\nn\nn\nis the voxelization resolution. The above operations simplify the modeling of the probability distribution\np\nâ€‹\n(\nâ„\n|\nğ’«\n,\nğ“\n)\np(\\mathcal{I}|\\mathcal{P},\\mathbf{T})\nto modeling\np\nâ€‹\n(\nâ„\n|\nğ’«\nâˆ—\n)\np(\\mathcal{I}|\\mathcal{P}^{*})\n, making the feature space more compact and reducing the training difficulty of the network. Specifically, inspired by the dominance of denoising diffusion models in 3D generation\nLuo and Hu (\n2021\n); Zheng\net al.\n(\n2023\n)\n, we model\np\nâ€‹\n(\nâ„\n|\nğ’«\nâˆ—\n)\np(\\mathcal{I}|\\mathcal{P}^{*})\nbased on an occupancy-diffusion model\nZheng\net al.\n(\n2023\n)\n. The\nâ„\nâˆˆ\nâ„\nn\nÃ—\nn\nÃ—\nn\nÃ—\n3\n\\mathcal{I}\\in\\mathbb{R}^{n\\times n\\times n\\times 3}\nhas the same resolution as\nğ’«\nâˆ—\n\\mathcal{P}^{*}\n, and this voxel-level alignment provides hierarchical conditions during the generation of IBS, improving both the generation quality and efficiency. As shown inÂ Figure\n4\n, for both occupancy network\nÎ©\no\n\\Omega_{o}\nand point cloud network\nÎ©\np\n\\Omega_{p}\n, we use 3D UNet as the backbone network. UNet has 4 levels:\n40\n3\n40^{3}\n,\n20\n3\n20^{3}\n,\n10\n3\n10^{3}\n,\n5\n3\n5^{3}\n, with feature dimensions of\n32\n32\n,\n64\n64\n,\n128\n128\n, and\n256\n256\nrespectively at each level. We achieve voxel-level condition guidance by concatenating the point cloud features at corresponding levels to the occupancy features. The training of the diffusion network uses the following loss:\nâ„’\nâ„\n0\n=\nğ”¼\nÏµ\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nI\n)\n,\nt\nâˆ¼\nğ’°\nâ€‹\n(\n0\n,\n1\n)\nâ€‹\nâ€–\nÎ©\no\nâ€‹\n(\nâ„\nt\n,\nt\n,\nÎ©\np\nâ€‹\n(\nğ’«\nâˆ—\n)\n)\nâˆ’\nâ„\n0\nâ€–\n2\n2\n\\mathcal{L}_{\\mathcal{I}_{0}}=\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,I),t\\sim\\mathcal{U}(0,1)}\\left\\|\\Omega_{o}(\\mathcal{I}_{t},t,\\Omega_{p}(\\mathcal{P}^{*}))-\\mathcal{I}_{0}\\right\\|_{2}^{2}\n(1)\nwhere\nÏµ\n\\epsilon\nand\nâ„\n0\n\\mathcal{I}_{0}\nare the data sample and\nâ„\nt\n\\mathcal{I}_{t}\nis the noisy sample at time step\nt\nt\n.\nğ’©\n\\mathcal{N}\nis the Gaussian distribution, and\nğ’°\n\\mathcal{U}\nis the uniform distribution.\nIBS Ranking.\nConsidering that the sampling process has a certain probability of sampling in low-density regions, we sample multiple IBS candidates\n{\nâ„\n^\ni\n}\ni\n=\n1\nm\n\\{\\hat{\\mathcal{I}}_{i}\\}_{i=1}^{m}\nfrom the estimated distribution\np\nâ€‹\n(\nâ„\n|\nğ’«\nâˆ—\n)\np(\\mathcal{I}|\\mathcal{P}^{*})\n, where\nm\nm\nis the number of samples. Furthermore, we calculate the force closure score\n{\nğ’¬\nâ„\n^\ni\n}\ni\n=\n1\nm\n\\{\\mathcal{Q}_{\\hat{\\mathcal{I}}_{i}}\\}_{i=1}^{m}\nfor candidates based on the contact points and directions of the thumb and other fingers, and obtain a ranked sequence of IBS candidates\nâ„\n^\nÏƒ\n0\nâ‰»\nâ„\n^\nÏƒ\n1\nâ‰»\nâ‹¯\nâ‰»\nâ„\n^\nÏƒ\nm\n\\hat{\\mathcal{I}}_{\\sigma_{0}}\\succ\\hat{\\mathcal{I}}_{\\sigma_{1}}\\succ\\cdots\\succ\\hat{\\mathcal{I}}_{\\sigma_{m}}\n, where:\nâ„\n^\nÏƒ\ni\nâ‰»\nâ„\n^\nÏƒ\nj\nâ‡”\nğ’¬\nâ„\n^\nÏƒ\ni\n>\nğ’¬\nâ„\n^\nÏƒ\nj\n\\hat{\\mathcal{I}}_{\\sigma_{i}}\\succ\\hat{\\mathcal{I}}_{\\sigma_{j}}\\iff\\mathcal{Q}_{\\hat{\\mathcal{I}}_{\\sigma_{i}}}>\\mathcal{Q}_{\\hat{\\mathcal{I}}_{\\sigma_{j}}}\n(2)\nFinally, we select the top-ranked IBS as the final intermediate representation\nâ„\n^\n\\hat{\\mathcal{I}}\n.\n3.3\nGrasp Pose Optimization with IBS Constraints.\nGrasp Pose Optimization.\nGiven the predicted IBS\nâ„\n^\n\\hat{\\mathcal{I}}\n, we generate the dexterous grasp poses\nğ’¢\n\\mathcal{G}\nthrough an optimization algorithm. Specifically, we use a gradient-based optimization algorithm to minimize the energy function\nğ„\n\\mathbf{E}\n, which consists of four parts: 1) joint limits energy\nğ„\nj\n\\mathbf{E}_{j}\n, 2) self-penetration energy\nğ„\ns\nâ€‹\np\n\\mathbf{E}_{sp}\n, 3) contact energy\nğ„\nd\n\\mathbf{E}_{d}\n, and 4) collision energy\nğ„\np\n\\mathbf{E}_{p}\n. We obtain the dexterous grasp poses\nğ’¢\n\\mathcal{G}\nby minimizing the energy function. Specifically:\nğ„\nj\n\\mathbf{E}_{j}\nis used to limit the joint angles within a preset range, defined as:\nğ„\nj\n=\n1\nd\nâ€‹\nâˆ‘\ni\n=\n1\nd\n(\nmax\nâ¡\n(\nÎ¸\ni\nâˆ’\nÎ¸\ni\nmax\n,\n0\n)\n+\nmax\nâ¡\n(\nÎ¸\ni\nmin\nâˆ’\nÎ¸\ni\n,\n0\n)\n)\n\\mathbf{E}_{j}=\\frac{1}{d}\\sum_{i=1}^{d}\\left(\\max(\\theta_{i}-\\theta_{i}^{\\max},0)+\\max(\\theta_{i}^{\\min}-\\theta_{i},0)\\right)\n(3)\nwhere\nÎ¸\ni\n\\theta_{i}\nis the angle of the\ni\ni\n-th joint,\nÎ¸\ni\nmax\n\\theta_{i}^{\\max}\nand\nÎ¸\ni\nmin\n\\theta_{i}^{\\min}\nare the maximum and minimum angles of the\ni\ni\n-th joint, respectively, and\nd\nd\nis the number of joints.\nğ„\ns\nâ€‹\np\n\\mathbf{E}_{sp}\nis used to limit the self-penetration of the hand, defined as:\nğ„\ns\nâ€‹\np\n=\n1\n|\nğ’«\nh\n|\n2\nâ€‹\nâˆ‘\np\nâˆˆ\nğ’«\nh\nâˆ‘\nq\nâˆˆ\nğ’«\nh\n[\np\nâ‰ \nq\n]\nâ€‹\nmax\nâ¡\n(\nÎ´\nâˆ’\nd\nâ€‹\n(\np\n,\nq\n)\n,\n0\n)\n\\mathbf{E}_{sp}=\\frac{1}{|\\mathcal{P}_{h}|^{2}}\\sum_{p\\in\\mathcal{P}_{h}}\\sum_{q\\in\\mathcal{P}_{h}}[p\\neq q]\\max(\\delta-d(p,q),0)\n(4)\nwhere\nğ’«\nh\n\\mathcal{P}_{h}\nis the set of points on the hand,\nd\nâ€‹\n(\nâ‹…\n)\nd(\\cdot)\ncalculates the Euclidean distance between two points, and\nÎ´\n\\delta\nis the safety distance for self-penetration of the hand.\nğ„\np\n\\mathbf{E}_{p}\nis used to constrain the contact relationship between fingers and objects, defined as:\nğ„\np\n=\n1\n|\nğ’«\nh\n|\nâ€‹\nâˆ‘\np\nâˆˆ\nğ’«\nh\nmax\nâ¡\n(\n0\n,\nâˆ’\n(\np\nâˆ’\np\nâˆ—\nâ€–\np\nâˆ’\np\nâˆ—\nâ€–\nâ‹…\nğ§\n)\n)\n\\mathbf{E}_{p}=\\frac{1}{|\\mathcal{P}_{h}|}\\sum_{p\\in\\mathcal{P}_{h}}\\max\\left(0,-\\left(\\frac{p-p^{*}}{\\|p-p^{*}\\|}\\cdot\\mathbf{n}\\right)\\right)\n(5)\nwhere\nğ’«\nh\n\\mathcal{P}_{h}\nis the set of points on the hand,\np\np\nis the point on the hand,\np\nâˆ—\np^{*}\nis the nearest point in the IBS point set\nğ’«\nâ„\n\\mathcal{P}_{\\mathcal{I}}\ncorresponding to\np\np\n, and\nğ§\n\\mathbf{n}\nis the normal vector of the estimated IBS surface at point\np\nâˆ—\np^{*}\n.\nğ„\nd\n\\mathbf{E}_{d}\nis used to constrain the optimization of the hand in the safety space without collision with the environment, defined as:\nğ„\nd\n=\nÎ±\n1\n|\nğ’«\nt\nâˆ—\n|\nâ€‹\nâˆ‘\np\nâˆ—\nâˆˆ\nğ’«\nt\nâˆ—\nmin\np\nâˆˆ\nğ’«\nt\nâ¡\nâ€–\np\nâˆ—\nâˆ’\np\nâ€–\n2\n+\nÎ±\n2\n|\nğ’«\no\nâˆ—\n|\nâ€‹\nâˆ‘\np\nâˆ—\nâˆˆ\nğ’«\no\nâˆ—\nmin\np\nâˆˆ\nğ’«\no\nâ¡\nâ€–\np\nâˆ—\nâˆ’\np\nâ€–\n2\n+\nÎ±\n3\n|\nğ’«\nt\n|\n+\n|\nğ’«\no\n|\nâ€‹\nâˆ‘\np\nâˆˆ\nğ’«\nt\n,\nğ’«\no\nmin\np\nâˆ—\nâˆˆ\nğ’«\nt\nâˆ—\n,\nğ’«\no\nâˆ—\nâ¡\nâ€–\np\nâˆ—\nâˆ’\np\nâ€–\n2\n\\mathbf{E}_{d}=\\frac{\\alpha_{1}}{|\\mathcal{P}_{t}^{*}|}\\sum_{p^{*}\\in\\mathcal{P}_{t}^{*}}\\min_{p\\in\\mathcal{P}_{t}}\\|p^{*}-p\\|^{2}+\\frac{\\alpha_{2}}{|\\mathcal{P}_{o}^{*}|}\\sum_{p^{*}\\in\\mathcal{P}_{o}^{*}}\\min_{p\\in\\mathcal{P}_{o}}\\|p^{*}-p\\|^{2}+\\frac{\\alpha_{3}}{|\\mathcal{P}_{t}|+|\\mathcal{P}_{o}|}\\sum_{p\\in{\\mathcal{P}_{t},\\mathcal{P}_{o}}}\\min_{p^{*}\\in{\\mathcal{P}_{t}^{*},\\mathcal{P}_{o}^{*}}}\\|p^{*}-p\\|^{2}\n(6)\nwhere\nğ’«\nt\nâˆ—\n\\mathcal{P}_{t}^{*}\nand\nğ’«\no\nâˆ—\n\\mathcal{P}_{o}^{*}\nare the sets of points on the IBS that are in contact with the thumb and other fingers, respectively,\nğ’«\nt\n\\mathcal{P}_{t}\nand\nğ’«\no\n\\mathcal{P}_{o}\nare the sets of points on the thumb and other fingers, respectively.\nÎ±\n1\n\\alpha_{1}\n,\nÎ±\n2\n\\alpha_{2}\n, and\nÎ±\n3\n\\alpha_{3}\nare hyperparameters used to balance the weights between different energy terms.\nOverall, the final energy function is:\nğ„\n=\nÎ»\n1\nâ€‹\nğ„\nj\n+\nÎ»\n2\nâ€‹\nğ„\nsp\n+\nÎ»\n3\nâ€‹\nğ„\np\n+\nÎ»\n4\nâ€‹\nğ„\nd\n\\mathbf{E}=\\lambda_{1}\\mathbf{E}_{j}+\\lambda_{2}\\mathbf{E}_{\\text{sp}}+\\lambda_{3}\\mathbf{E}_{p}+\\lambda_{4}\\mathbf{E}_{d}\n(7)\nwhere\nÎ»\n1\n\\lambda_{1}\n,\nÎ»\n2\n\\lambda_{2}\n,\nÎ»\n3\n\\lambda_{3}\n, and\nÎ»\n4\n\\lambda_{4}\nare the weights of each energy term.\nGrasp Pose Ranking.\nGiven the inherent uncertainties in the optimization process, we simultaneously optimize multiple grasp configurations\nğ’¢\n\\mathcal{G}\nbased on a predicted sparse IBS\nâ„\n^\n\\hat{\\mathcal{I}}\n. We record the optimization residuals\n{\nğ„\nğ \ni\n}\ni\n=\n1\nk\n\\{\\mathbf{E}_{\\mathbf{g}_{i}}\\}_{i=1}^{k}\n, where\nk\nk\ndenotes the number of optimization trials. The grasp configurations\nğ’¢\n\\mathcal{G}\nare then ranked according to their residuals, and the configuration with the minimal residual is selected as the optimal grasp pose\nğ \n\\mathbf{g}\n.\n4\nExperiments\n4.1\nExperimental Setup\nDatasets.\nWe use the same object datasets and simulation environments as DexGraspNet 2.0\nZhang\net al.\n(\n2024a\n)\n. The object datasets consist of 60 training objects from GraspNet1Billion\n(Fang\net al.\n,\n2020\n)\nand 1259 testing objects from GraspNet1Billion and ShapeNet\n(Chang\net al.\n,\n2015\n)\n. We sample 100 scenes from the 7600 training scenes of DexGraspNet2\nZhang\net al.\n(\n2024a\n)\nfor the training of the IBS generation module. And we use the full 670 scenes from the testing set of DexGraspNet2\nZhang\net al.\n(\n2024a\n)\nfor the testing. The testing scenes are categorized into three density levels: loose, random, and dense.\nBaselines.\nWe compare our method with the following dexterous grasp pose prediction methods.\nâ€¢\nDexGraspNet2.0\nZhang\net al.\n(\n2024a\n)\n:\nAn end-to-end diffusion-based pipeline for grasp pose generation in cluttered scenes, directly mapping 3D point cloud observations to grasp poses.\nâ€¢\nHGC-Net\nLi\net al.\n(\n2022\n)\n:\nA regression-based model for direct pose prediction in cluttered scenes.\nâ€¢\nISAGrasp\nChen\net al.\n:\nThis baseline is originally designed for single-object grasping using an end-to-end regression-based model. Following the adaptation strategy in DexGraspNet2.0\nZhang\net al.\n(\n2024a\n)\n, we extend ISAGrasp to cluttered scenes.\nâ€¢\nGraspTTA\nJiang\net al.\n(\n2021\n)\n:\nA two-stage, single-object grasping framework that needs complete point clouds for the second-stage optimization, which is impractical in cluttered scenes; we therefore adapt it for cluttered scenes as DexGraspNet2.0 does and remove the optimization stage.\nThis two-stage framework is originally designed for single-object grasping and requires complete point clouds for second-stage optimization, which is not feasible in cluttered scenes. Therefore, following DexGraspNet2.0\nZhang\net al.\n(\n2024a\n)\n, we adapt GraspTTA for cluttered scenes and omit the optimization stage.\nEvaluation in Simulation.\nWe report the\nSuccess Rate\n, following the same evaluation protocol as DexGraspNet2.0\nZhang\net al.\n(\n2024a\n)\n. In the simulation, we evaluate from two perspectives:\nâ€¢\nObject and Scene Generalization.\nWe evaluate the performance of our method following the same protocol as DexGraspNet2.0\nZhang\net al.\n(\n2024a\n)\n.\nâ€¢\nCross-Embodiment Generalization.\nDifferent from other baselines, our method can directly zero-shot generalize to unseen embodiments, thanks to the proposed universal intermediate representation. We evaluate our method using an unseen dexterous hand (Allegro) in a zero-shot manner.\nReal-world Setup.\nAs shown inÂ Figure\n5\n, we conduct real-world dexterous grasping experiments in clutter scenes, using a Flexiv Rizon-4 robot arm equipped with a Leap Hand\nShaw\net al.\nas the end-effector.\nFigure 5:\nReal-world experiment setup.\nA third-person view RealSense D415 camera is employed for perception. We selected 30 objects with diverse shapes, sizes, and materials, as depicted inÂ Figure\n6\n. Following the simulation experiment setup, we also assess grasping under varying levels of clutter density, covering the entire object dataset across 5 cluttered scenes with 4 to 8 objects per scene, as illustrated inÂ Figure\n6\n. For each scene, the policy continues grasping until two consecutive failures occur. For each grasp execution, we follow the same sequence as in the simulation: pre-grasp, grasp, squeeze fingers, and lift. We report the\nSuccess Rate\nas the number of successfully grasped objects divided by the total number of attempts.\nFigure 6:\nReal-world object datasets and evaluated cluttered scenes.\nThe left image shows the objects used in real-world testing. The right image shows the layouts of objects in five different test scenes, with 4 to 8 objects per scene.\nMethod\nRatio\nGraspNet-1Billion\nShapeNet\nDense\nRandom\nLoose\nDense\nRandom\nLoose\nHGC-Net\nâ€ \nLi\net al.\n(\n2022\n)\n1\n46.0\n37.8\n26.7\n46.4\n44.8\n30.4\nGraspTTA\nâ€ \nJiang\net al.\n(\n2021\n)\n1\n62.5\n54.1\n42.8\n56.6\n57.8\n46.4\nISAGrasp\nâ€ \nChen\net al.\n1\n63.4\n60.7\n51.4\n64.0\n56.3\n52.7\nDexGraspNet2.0\nZhang\net al.\n(\n2024a\n)\n1/1000\n83.3\n79.5\n73.9\n81.5\n77.1\n73.5\nOurs\n1/1000\n86.5\n85.5\n80.1\n79.3\n77.7\n75.7\nOurs (Allegro)\n1/1000\n77.6\n75.0\n74.9\n75.7\n76.6\n73.0\nTable 1:\nComparison results.\nâ€ \nindicates the results are from\nZhang\net al.\n(\n2024a\n)\n.\nRatio\nrefers to the ratio of the number of grasps for training compared to the whole dataset. Each\nDense\nscene contains 8-11 objects, and each\nRandom\nscene contains 1-10 objects, obtained by deleting objects from Dense scenes, and each\nLoose\nscene contains 1-2 objects.\nAllegro\nis the result evaluated with the Allegro, and others use the Leap hand for evaluation. We report only our method, trained on Leap Hand and tested on Allegro Hand, since other methods do not demonstrate cross-hand generalization.\n4.2\nSimulation Results\nComparison with Baselines.\nAs shown in Table\n1\n, the regression-based methods HGC-Net and ISAGrasp struggle with the complex dexterous grasp pose distribution. The generative-based method GraspTTA performs even worse, likely due to the absence of a second-stage optimization step. DexGraspNet2.0 achieves a higher success rate by leveraging the diffusion model. However, its direct end-to-end mapping is susceptible to challenges associated with non-linear mapping and sensitivity to physical constraints. In contrast, our two-stage approach demonstrates superior performance. Furthermore, to evaluate robustness, we report success rates with standard deviations over 20 random seeds. The consistently small deviations indicate stable performance across initializations.\nCross-Embodiment Generalization.\nSince our proposed sparse IBS representation is embodiment-agnostic, we directly use the same generated sparse IBS that was used to evaluate the Leap Hand to optimize the Allegro Hand grasp pose. As shown in Table\n1\n, the Allegro Hand also achieves comparable results to the Leap Hand, demonstrating the universality of our proposed representation.\n4.3\nReal-world Results\nAs shown in Table\n2\n, our method achieves an average grasp success rate of 93.3%, significantly surpassing the baseline at 83.9%. While the two methods exhibit comparable performance on medium-scale objects, our approach demonstrates markedly greater robustness for small and flat objects, in line with the simulation analyses reported in Table\n7\n. This improvement can be attributed to the effectiveness of the IBS. By integrating IBS into the optimization, our method explicitly avoids collisions with the table and surrounding objects, yielding safer grasps, which is particularly critical in densely cluttered scenes. The observed real-world performance underscores the practical applicability of our approach for deployment in real robotic systems.\nMethod\nScene #1\nScene #2\nScene #3\nScene #4\nScene #5\nOverall\nDexGraspNet2.0\nZhang\net al.\n(\n2024a\n)\n100.0% (4/4)\n50.0% (2/4)\n71.4% 5/7)\n100.0% (7/7)\n88.9% (8/9)\n83.9% (26/31)\nCADGrasp (Ours)\n100.0% (4/4)\n100.0% (5/5)\n85.7% (6/7)\n100% (7/7)\n88.9% (8/9)\n93.8% (30/32)\nTable 2:\nReal-world results.\nWe evaluated 30 objects across 5 real-world cluttered scenes. In each scene, the policy continues attempting to grasp until two consecutive failures occur. The number to the left of â€™/â€™ indicates the number of successful grasps, while the number to the right indicates the total number of grasp attempts.\nModule\nStage\nTime (s)\nWrist Pose Estimation\n1\n1.38\nIBS Generation\n1\n1.39\nIBS Ranking\n1\n0.71\nGrasp Opt. & Rk.\n2\n3.03\nTotal\n-\n6.51\nTable 3:\nRuntime breakdown.\n4.4\nComputational Efficiency\nWe assess the inference efficiency of our method on a single NVIDIA RTX 4090 GPU by averaging 50 independent runs to suppress stochastic fluctuations in the sampling-based optimization. A detailed breakdown of the runtime is reported in Table\n3\n. The end-to-end time to generate a single grasp is 6.51 s on average, measured without any task-specific engineering optimizations. While not real-time, our method delivers a substantial latency advantage over existing state-of-the-art two-stage approaches (see Table\n4\n). We anticipate that real-time feasibility can be pushed closer to real-time by adopting faster samplers\nSalimans and Ho (\n2022\n); Song\net al.\n(\n2023\n)\nand by increasing parallelism in the optimization stage.\nMethod\nIntermediate Rep.\nPartial Obs.\nCluttered\nRuntime (s)\nGraspTTA\nContact Map\nÃ—\n\\times\nÃ—\n\\times\n43.23\nUniGrasp\n3 Contact Points\nÃ—\n\\times\nÃ—\n\\times\n9.33\nGenDexGrasp\nContact Map\nÃ—\n\\times\nÃ—\n\\times\n16.42\nCADGrasp (Ours)\nSparse IBS\nâœ“\nâœ“\n6.51\nTable 4:\nRuntime and capability comparison with representative two-stage methods.\nOur approach is faster and enables dexterous grasping under partial observations in cluttered scenes, both of which are essential for real-world deployment.\n4.5\nAblation Study.\nWe conduct comprehensive ablation studies on the dense scenes of GraspNet-1Billion test set to validate our design choices.\nâ€¢\nModule Interaction.\nWe first analyze the interaction effects between key modules in Table\n5\n. The results show that removing any component leads to a performance drop, with the full model achieving the best success rate 86.5%. Notably, decomposing the contact representation (â€˜Decomposeâ€™) for the thumb and other fingers provides a substantial improvement (e.g., 86.5% vs. 56.1% without it). This confirms the critical role of the thumb in dexterous grasping\nFeix\net al.\n(\n2015\n)\nand validates our design to model it separately. Both the IBS and Grasp Pose Ranking modules are also proven effective.\nIBS Ranking\nGrasp Pose Ranking\nDecompose\nSuccess Rate (%)\nÃ—\n\\times\nÃ—\n\\times\nâœ“\n73.1\nÃ—\n\\times\nâœ“\nÃ—\n\\times\n53.8\nÃ—\n\\times\nâœ“\nâœ“\n83.9\nâœ“\nÃ—\n\\times\nÃ—\n\\times\n26.9\nâœ“\nÃ—\n\\times\nâœ“\n75.7\nâœ“\nâœ“\nÃ—\n\\times\n56.1\nâœ“\nâœ“\nâœ“\n86.5\nTable 5:\nAblation study.\nThe interaction effects between key design elements in the dense-scene subset from the GraspNet-1Billon test set.\nâ€¢\nVoxel Resolution.\nNext, we study the effect of voxel resolution in Table\n6\n. We select a voxel size of 5mm to balance accuracy and efficiency. Finer resolutions (2.5mm) offer only marginal gains while significantly increasing memory usage, whereas coarser resolutions (10mm) lead to a clear performance drop.\nâ€¢\nObject Size.\nFinally, we analyze the performance on objects of different sizes in Table\n7\n. The results indicate that the success rate for small objects is indeed lower. However, combined with the findings in Table\n6\n, we attribute this to the inherently higher precision required for grasping small objects, rather than insufficient voxel resolution.\nVoxel Size (mm)\nMemory (GB)\nSR (%)\n2.5\n2.12\n81.2\n5\n0.84\n81.0\n10\n0.36\n72.4\nTable 6:\nAblation on voxel resolution.\nBolded items denote selected hyperparameters, balancing computational efficiency and performance.\nVolume Range (mÂ³)\nSR (%)\nProp. (%)\n(0, 0.00025)\n77.0\n33.0\n[0.00025, 0.0005)\n78.8\n36.9\n[0.0005, 0.001)\n82.6\n22.3\n[0.001, 0.0015)\n82.3\n6.0\n[0.0015,\n+\nâˆ\n+\\infty\n)\n91.5\n1.9\nTable 7:\nSuccess rate vs. object volume.\n5\nConclusion\nIn this paper, we enhance general dexterous grasp pose prediction in cluttered scenes by proposing a two-stage framework. The first stage predicts our proposed compact, scene-decoupled, contact- and collision-aware intermediate representation, which serves as the target for the second-stage optimization. To ensure the quality of the predicted representation, we introduce an occupancy-diffusion model with voxel-level conditional guidance and force closure filtering. To generate stable and collision-free grasp poses, we further propose several energy functions and ranking strategies for pose optimization. Comprehensive simulation and real-world experiments demonstrate the effectiveness of our method.\nLimitations.\nAlthough our method demonstrates better generalization, it primarily struggles with small objects, which could be addressed by incorporating more small objects during training. Additionally, the current second-stage optimization is time-consuming due to the use of the DDPM\nHo\net al.\n(\n2020\n)\nfor sampling IBS. This can be further optimized by adopting the DDIM\nSong\net al.\n.\nAcknowledgments and Disclosure of Funding\nThis work is supported by the National Natural Science Foundation of China - General Program (Project ID: 62376006), National Youth Talent Support Program (Project ID: 8200800081).\nReferences\nA. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,\net al.\n(2015)\nShapenet: an information-rich 3d model repository\n.\narXiv preprint arXiv:1512.03012\n.\nCited by:\nÂ§4.1\n.\n[2]\nQ. Chen, K. Van Wyk, Y. Chao, W. Yang, A. Mousavian, A. Gupta, and D. Fox\nLearning robust real-world dexterous grasping policies via implicit shape augmentation\n.\nIn\n6th Annual Conference on Robot Learning\n,\nCited by:\n3rd item\n,\nÂ§4.1\n.\nZ. Chen, Q. Yan, Y. Chen, T. Wu, J. Zhang, Z. Ding, J. Li, Y. Yang, and H. Dong (2025)\nClutterDexGrasp: a sim-to-real system for general dexterous grasping in cluttered scenes\n.\narXiv preprint arXiv:2506.14317\n.\nCited by:\nÂ§1\n.\nZ. Q. Chen, K. Van Wyk, Y. Chao, W. Yang, A. Mousavian, A. Gupta, and D. Fox (2022)\nDextransfer: real world multi-fingered dexterous grasping with minimal human demonstrations\n.\narXiv preprint arXiv:2209.14284\n.\nCited by:\nÂ§2.1\n.\nC. Choy, J. Gwak, and S. Savarese (2019)\n4d spatio-temporal convnets: minkowski convolutional neural networks\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 3075â€“3084\n.\nCited by:\nÂ§3.2\n.\nH. Fang, C. Wang, H. Fang, M. Gou, J. Liu, H. Yan, W. Liu, Y. Xie, and C. Lu (2023)\nAnygrasp: robust and efficient grasp perception in spatial and temporal domains\n.\nIEEE Transactions on Robotics\n39\n(\n5\n),\npp.Â 3929â€“3945\n.\nCited by:\nÂ§1\n.\nH. Fang, C. Wang, M. Gou, and C. Lu (2020)\nGraspnet-1billion: a large-scale benchmark for general object grasping\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 11444â€“11453\n.\nCited by:\nÂ§1\n,\nÂ§4.1\n.\nT. Feix, J. Romero, H. Schmiedmayer, A. M. Dollar, and D. Kragic (2015)\nThe grasp taxonomy of human grasp types\n.\nIEEE Transactions on human-machine systems\n46\n(\n1\n),\npp.Â 66â€“77\n.\nCited by:\n1st item\n.\nJ. Ho, A. Jain, and P. Abbeel (2020)\nDenoising diffusion probabilistic models\n.\nAdvances in neural information processing systems\n33\n,\npp.Â 6840â€“6851\n.\nCited by:\nÂ§5\n.\nS. Huang, Z. Wang, P. Li, B. Jia, T. Liu, Y. Zhu, W. Liang, and S. Zhu (2023)\nDiffusion-based generation, optimization, and planning in 3d scenes\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 16750â€“16761\n.\nCited by:\nÂ§2.1\n.\nZ. Huang, H. Yuan, Y. Fu, and Z. Lu (2024)\nEfficient residual learning with mixture-of-experts for universal dexterous grasping\n.\narXiv preprint arXiv:2410.02475\n.\nCited by:\nÂ§1\n.\nH. Jiang, S. Liu, J. Wang, and X. Wang (2021)\nHand-object contact consistency reasoning for human grasps generation\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp.Â 11107â€“11116\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n,\nÂ§2.3\n,\n4th item\n,\nÂ§4.1\n.\nC. Kim, C. Won, Y. Cho, D. Kim, S. Lee, J. Bhak, and D. Kim (2006)\nInteraction interfaces in proteins via the voronoi diagram of atoms\n.\nComputer-Aided Design\n38\n(\n11\n),\npp.Â 1192â€“1204\n.\nCited by:\nÂ§1\n,\nÂ§3.1\n.\nH. Li, X. Lin, Y. Zhou, X. Li, Y. Huo, J. Chen, and Q. Ye (2023a)\nContact2Grasp: 3d grasp synthesis via hand-object contact constraint\n.\nIn\nProceedings of the Thirty-Second International Joint Conference on Artificial Intelligence\n,\npp.Â 1053â€“1061\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nÂ§2.3\n.\nP. Li, T. Liu, Y. Li, Y. Geng, Y. Zhu, Y. Yang, and S. Huang (2023b)\nGendexgrasp: generalizable dexterous grasping\n.\nIn\n2023 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 8068â€“8074\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n,\nÂ§2.3\n.\nY. Li, W. Wei, D. Li, P. Wang, W. Li, and J. Zhong (2022)\nHGC-net: deep anthropomorphic hand grasping in clutter\n.\nIn\n2022 International Conference on Robotics and Automation (ICRA)\n,\npp.Â 714â€“720\n.\nCited by:\nÂ§1\n,\nÂ§1\n,\nÂ§2.1\n,\nÂ§2.2\n,\n2nd item\n,\nÂ§4.1\n.\nM. Liu, Z. Pan, K. Xu, K. Ganguly, and D. Manocha (2019)\nGenerating grasp poses for a high-dof gripper using neural networks\n.\nIn\n2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 1518â€“1525\n.\nCited by:\nÂ§2.1\n.\nQ. Liu, Y. Cui, Q. Ye, Z. Sun, H. Li, G. Li, L. Shao, and J. Chen (2023)\nDexrepnet: learning dexterous robotic grasping network with geometric and spatial hand-object representations\n.\nIn\n2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 3153â€“3160\n.\nCited by:\nÂ§2.3\n.\nJ. Lu, H. Kang, H. Li, B. Liu, Y. Yang, Q. Huang, and G. Hua (2024)\nUgg: unified generative grasping\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 414â€“433\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nJ. Lundell, E. Corona, T. N. Le, F. Verdoja, P. Weinzaepfel, G. Rogez, F. Moreno-Noguer, and V. Kyrki (2021a)\nMulti-fingan: generative coarse-to-fine sampling of multi-finger grasps\n.\nIn\n2021 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 4495â€“4501\n.\nCited by:\nÂ§2.2\n.\nJ. Lundell, F. Verdoja, and V. Kyrki (2021b)\nDdgc: generative deep dexterous grasping in clutter\n.\nIEEE Robotics and Automation Letters\n6\n(\n4\n),\npp.Â 6899â€“6906\n.\nCited by:\nÂ§2.2\n.\nS. Luo and W. Hu (2021)\nDiffusion probabilistic models for 3d point cloud generation\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 2837â€“2845\n.\nCited by:\nÂ§3.2\n.\nV. Mayer, Q. Feng, J. Deng, Y. Shi, Z. Chen, and A. Knoll (2022)\nFFHNet: generating multi-fingered robotic grasps for unknown objects in real-time\n.\nIn\n2022 International Conference on Robotics and Automation (ICRA)\n,\npp.Â 762â€“769\n.\nCited by:\nÂ§2.2\n,\nÂ§2.3\n.\nF. Patzelt, R. Haschke, and H. Ritter (2021)\nConditional stylegan for grasp generation\n.\nIn\n2021 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 4481â€“4487\n.\nCited by:\nÂ§2.2\n.\nY. Qin, B. Huang, Z. Yin, H. Su, and X. Wang (2022)\nDexPoint: generalizable point cloud reinforcement learning for sim-to-real dexterous manipulation\n.\nConference on Robot Learning (CoRL)\n.\nCited by:\nÂ§1\n.\nT. Salimans and J. Ho (2022)\nProgressive distillation for fast sampling of diffusion models\n.\nExternal Links:\n2202.00512\n,\nLink\nCited by:\nÂ§4.4\n.\nP. Schmidt, N. Vahrenkamp, M. WÃ¤chter, and T. Asfour (2018)\nGrasping of unknown objects using deep convolutional neural networks based on depth images\n.\nIn\n2018 IEEE international conference on robotics and automation (ICRA)\n,\npp.Â 6831â€“6838\n.\nCited by:\nÂ§2.1\n.\n[28]\nK. Shaw, A. Agarwal, and D. Pathak\nLEAP hand: low-cost, efficient, and anthropomorphic hand for robot learning\n.\nCited by:\nÂ§4.1\n.\nQ. She, R. Hu, J. Xu, M. Liu, K. Xu, and H. Huang (2022)\nLearning high-dof reaching-and-grasping via dynamic representation of gripper-object interaction\n.\nACM Transactions on Graphics (TOG)\n41\n(\n4\n),\npp.Â 1â€“14\n.\nCited by:\nÂ§2.3\n,\nÂ§3.1\n.\nQ. She, S. Zhang, Y. Ye, R. Hu, and K. Xu (2024)\nLearning cross-hand policies of high-dof reaching and grasping\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 269â€“285\n.\nCited by:\nÂ§2.3\n,\nÂ§3.1\n.\nR. Singh, A. Allshire, A. Handa, N. Ratliff, and K. Van Wyk (2024)\nDextrAH-rgb: visuomotor policies to grasp anything with dexterous hands\n.\narXiv preprint arXiv:2412.01791\n.\nCited by:\nÂ§1\n.\n[32]\nJ. Song, C. Meng, and S. Ermon\nDenoising diffusion implicit models\n.\nIn\nInternational Conference on Learning Representations\n,\nCited by:\nÂ§5\n.\nY. Song, P. Dhariwal, M. Chen, and I. Sutskever (2023)\nConsistency models\n.\nCited by:\nÂ§4.4\n.\nW. Wan, H. Geng, Y. Liu, Z. Shan, Y. Yang, L. Yi, and H. Wang (2023)\nUnidexgrasp++: improving dexterous grasping policy learning via geometry-aware curriculum and iterative generalist-specialist learning\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.Â 3891â€“3902\n.\nCited by:\nÂ§1\n.\nW. Wei, D. Li, P. Wang, Y. Li, W. Li, Y. Luo, and J. Zhong (2022)\nDVGG: deep variational grasp generation for dextrous manipulation\n.\nIEEE Robotics and Automation Letters\n7\n(\n2\n),\npp.Â 1659â€“1666\n.\nCited by:\nÂ§2.2\n.\nZ. Weng, H. Lu, D. Kragic, and J. Lundell (2024)\nDexdiffuser: generating dexterous grasps with diffusion models\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\nÂ§2.2\n.\nT. Wu, M. Wu, J. Zhang, Y. Gan, and H. Dong (2023)\nLearning score-based grasping primitive for human-assisting dexterous grasping\n.\nAdvances in Neural Information Processing Systems\n36\n,\npp.Â 22132â€“22150\n.\nCited by:\nÂ§2.1\n.\nG. Xu, Y. Wei, D. Zheng, X. Wu, and W. Zheng (2024)\nDexterous grasp transformer\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 17933â€“17942\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nY. Xu, W. Wan, J. Zhang, H. Liu, Z. Shan, H. Shen, R. Wang, H. Geng, Y. Weng, J. Chen,\net al.\n(2023)\nUnidexgrasp: universal robotic dexterous grasping via learning diverse proposal generation and goal-conditioned policy\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 4737â€“4746\n.\nCited by:\nÂ§1\n,\nÂ§2.3\n.\nJ. Zhang, H. Liu, D. Li, X. Yu, H. Geng, Y. Ding, J. Chen, and H. Wang (2024a)\nDexGraspNet 2.0: learning generative dexterous grasping in large-scale synthetic cluttered scenes\n.\nIn\n8th Annual Conference on Robot Learning\n,\nCited by:\nÂ§1\n,\nÂ§1\n,\nÂ§2.1\n,\nÂ§2.2\n,\nÂ§B.2\n,\nÂ§3.2\n,\n1st item\n,\n3rd item\n,\n4th item\n,\n1st item\n,\nÂ§4.1\n,\nÂ§4.1\n,\nÂ§4.1\n,\nÂ§4.3\n,\nTable 1\n.\nJ. Zhang, W. Huang, B. Peng, M. Wu, F. Hu, Z. Chen, B. Zhao, and H. Dong (2024b)\nOmni6dpose: a benchmark and model for universal 6d object pose estimation and tracking\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 199â€“216\n.\nCited by:\nÂ§2.1\n.\nJ. Zhang, M. Wu, and H. Dong (2023)\nGenerative category-level object pose estimation via diffusion models\n.\nAdvances in Neural Information Processing Systems\n36\n,\npp.Â 54627â€“54644\n.\nCited by:\nÂ§2.1\n.\nX. Zheng, H. Pan, P. Wang, X. Tong, Y. Liu, and H. Shum (2023)\nLocally attentional sdf diffusion for controllable 3d shape generation\n.\nACM Transactions on Graphics (ToG)\n42\n(\n4\n),\npp.Â 1â€“13\n.\nCited by:\nÂ§A.2\n,\nÂ§3.2\n,\nÂ§3.2\n.\nY. Zhong, Q. Jiang, J. Yu, and Y. Ma (2025)\nDexGrasp anything: towards universal robotic dexterous grasping with physics awareness\n.\narXiv preprint arXiv:2503.08257\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nAppendix\nIn this appendix, we first describe the implementation details in Section\nA\n, followed by additional experimental results in Section\nB\n.\nA\nImplementation Details\nA.1\nTraining Details\nWe train our model on 8 NVIDIA RTX 4090 GPUs with a batch size of 64. We use the AdamW optimizer with a learning rate of\n6\nâ€‹\ne\nâˆ’\n5\n6e^{-5}\nand trained the model for 130 epochs. The training procedure takes about 2 days.\nA.2\nHyperparameters\nThe hyperparameters employed in our experiments are detailed in Table\n8\n. The size of IBS volume is configured to\n0.2\nâ€‹\nm\nÃ—\n0.2\nâ€‹\nm\nÃ—\n0.2\nâ€‹\nm\n0.2\\,\\text{m}\\times 0.2\\,\\text{m}\\times 0.2\\,\\text{m}\n, adequately encompassing the interaction space between the dexterous hand and the object, while remaining sufficiently compact to focus on the critical local grasping region. The resolution of the IBS volume is set to\n40\nÃ—\n40\nÃ—\n40\n40\\times 40\\times 40\n, striking a balance between computational efficiency and the accuracy of the IBS surface representation. Both the IBS sampling and grasp pose optimization processes are executed concurrently, with the number of IBS candidates and grasp poses each limited to 5. The weights in the contact energy\nğ„\nd\n\\mathbf{E}_{d}\nare meticulously adjusted to balance the contacts between the object and the thumb, as well as the other fingers, whereas the weights in the overall energy\nğ„\n\\mathbf{E}\nare calibrated to harmonize the various energy terms. The hyperparameter for denoising timesteps is adopted from\n[\n43\n]\n.\nHyperparameter\nValue\nIBS Volume Size\n0.2\nâ€‹\nm\nÃ—\n0.2\nâ€‹\nm\nÃ—\n0.2\nâ€‹\nm\n0.2m\\times 0.2m\\times 0.2m\nIBS Resolution (\nn\nn\n)\n40\nÃ—\n40\nÃ—\n40\n40\\times 40\\times 40\nNumber of IBS Candidates (\nm\nm\n)\n5\nNumber of Grasp Poses (\nk\nk\n)\n5\nWeights in Contact Energy\nğ„\nd\n\\mathbf{E}_{d}\n(\nÎ±\n1\n,\nÎ±\n2\n,\nÎ±\n3\n\\alpha_{1},\\alpha_{2},\\alpha_{3}\n)\n80, 100, 2\nWeights in Overall Energy\nğ„\n\\mathbf{E}\n(\nÎ»\n1\n,\nÎ»\n2\n,\nÎ»\n3\n,\nÎ»\n4\n\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4}\n)\n5, 1, 1000, 1\nDenoising Timesteps\n50\nTable 8:\nHyperparameters used in our experiments.\nB\nMore Results\nIn this section, we provide additional results of our method to demonstrate the effectiveness, robustness, scalability, and generalization ability of our method.\nB.1\nQualitative Results\nIn this section, we present additional qualitative evaluations of our proposed method. Figure\n7\nshowcases the perception results of our method in both simulated and real-world environments. The results clearly demonstrate that our approach successfully discerns the sparse IBS volume pertinent to a success grasp pose from a single-view point cloud in cluttered settings. It effectively differentiates the contact regions of the thumb and other fingers with the object. Furthermore, the second-stage optimization, guided by Sparse IBS constraints, adaptly refines collision-free and plausible grasp poses. Notably, the adoption of point cloud representation minimizes the sim-to-real gap, ensuring that our method can generalize to real-world settings without additional training, thereby achieving robust dexterous grasping capabilities.\nFigure 7:\nQualitative results of our method in simulated (upper panel) and real-world environments (lower panel). From left to right: the initial single-view point cloud input, the sparse IBS prediction from the initial stage, and the optimized grasp pose from the subsequent stage. The purple areas indicate thumb contact, red areas denote contact by other fingers, and green areas represent non-contact regions on the IBS surface.\nB.2\nGrasp Diversity Analysis\nWe analyze the distribution of joint configurations for all predicted grasp poses by\nCA\nD\nGrasp\nand DexGraspNet2.0\n[\n40\n]\nwithin the GraspNet-1B loose scenarios\n[\n40\n]\n. This analysis aimed to compare the diversity of grasp poses generated by the two methods. Taking the thumb as an example, Â Figure\n8\nillustrates that our method can generate diverse grasps with significantly higher dexterity compared to DexGraspNet2.0. This highlights our approachâ€™s capability to produce a wider range of effective grasp configurations, enhancing its applicability in complex scenarios.\nFigure 8:\nGrasp diversity analysis of\nCA\nD\nGrasp\nand DexGraspNet2.0 within the GraspNet-1B loose scenarios. The histogram illustrates the distribution of joint configurations for the thumb.\nFigure 9:\nPenetration depth analysis of predicted grasp poses by\nCA\nD\nGrasp\nand DexGraspNet2.0 within the GraspNet-1B loose scenarios. The histogram illustrates the distribution of maximal penetration depths.\nB.3\nGrasp Quality Analysis\nIn evaluating grasp quality, the proximity and penetration between the predicted grasp pose and the object serve as crucial indicators. We analyzed the maximal penetration depth (in cm) of all predicted grasp poses by\nCA\nD\nGrasp\nand DexGraspNet2.0 within the GraspNet-1B loose scenarios. This metric is defined as the maximal penetration depth from the object point cloud to the hand meshes. As illustrated in Â Figure\n9\n, our method demonstrates a concentration of penetration depths near the objectâ€™s surface (penetration depth = 0), attributed to the constraints imposed by the IBS Surface and contact points. This further underscores the superiority of our approach in achieving precise and effective grasping.",
    "preview_text": "Dexterous grasping in cluttered environments presents substantial challenges due to the high degrees of freedom of dexterous hands, occlusion, and potential collisions arising from diverse object geometries and complex layouts. To address these challenges, we propose CADGrasp, a two-stage algorithm for general dexterous grasping using single-view point cloud inputs. In the first stage, we predict sparse IBS, a scene-decoupled, contact- and collision-aware representation, as the optimization target. Sparse IBS compactly encodes the geometric and contact relationships between the dexterous hand and the scene, enabling stable and collision-free dexterous grasp pose optimization. To enhance the prediction of this high-dimensional representation, we introduce an occupancy-diffusion model with voxel-level conditional guidance and force closure score filtering. In the second stage, we develop several energy functions and ranking strategies for optimization based on sparse IBS to generate high-quality dexterous grasp poses. Extensive experiments in both simulated and real-world settings validate the effectiveness of our approach, demonstrating its capability to mitigate collisions while maintaining a high grasp success rate across diverse objects and complex scenes.\n\nCA\nD\nGrasp\n: Learning\nC\nontact and\nC\nollision\nA\nware General\nD\nexterous\nGrasp\ning in Cluttered Scenes\nJiyao Zhang\n1,2 *\n, Zhiyuan Ma\n1,2 *\n, Tianhao Wu\n1,2\n, Zeyuan Chen\n1,2\n, Hao Dong\n1,2\nâ€ \n\\dagger\n1\nCenter on Frontiers of Computing Studies, School of Computer Science, Peking University\n2\nNational Key Laboratory for Multimedia Information Processing,\nSchool of Computer Science, Peking University\njiyaozhang@stu.pku.edu.cn\nAbstract\nDexterous grasping in cluttered environments presents substantial challenges due to the high degrees of freedom of dexterous hands, occlusion, and potential collisions arising from diverse object geometries and complex layouts. To address these challenges, we propose\nCA\nD\nGrasp\n, a tw",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "diffusion",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹å’Œç¨€ç–IBSè¡¨ç¤ºçš„ä¸¤é˜¶æ®µç®—æ³•ï¼Œç”¨äºåœ¨æ‚ä¹±åœºæ™¯ä¸­å®ç°æ¥è§¦å’Œç¢°æ’æ„ŸçŸ¥çš„çµå·§æŠ“å–ã€‚",
    "detailed_summary": "## CADGrasp è®ºæ–‡æ‘˜è¦\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nåœ¨æ‚ä¹±åœºæ™¯ä¸­è¿›è¡Œçµå·§æŠ“å–æ˜¯æœºå™¨äººè‡ªä¸»æ‰§è¡Œä»»åŠ¡çš„å…³é”®ï¼Œä½†é¢ä¸´ç€çµå·§æ‰‹é«˜è‡ªç”±åº¦ã€ç‰©ä½“é®æŒ¡ä»¥åŠå¤æ‚å¸ƒå±€å¯¼è‡´ç¢°æ’ç­‰å·¨å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç›´æ¥é¢„æµ‹æŠ“å–å§¿æ€ï¼Œéš¾ä»¥æ³›åŒ–ï¼Œæˆ–ä¾èµ–å®Œæ•´ç‰©ä½“å‡ ä½•ä¿¡æ¯è¿›è¡Œä¼˜åŒ–ï¼Œè¿™åœ¨éƒ¨åˆ†è§‚æµ‹çš„æ‚ä¹±åœºæ™¯ä¸­ä¸ç°å®ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³**å•è§†è§’ç‚¹äº‘è¾“å…¥ä¸‹ï¼Œæ‚ä¹±åœºæ™¯ä¸­é€šç”¨ã€ç¨³å®šä¸”æ— ç¢°æ’çš„çµå·§æŠ“å–**é—®é¢˜ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡º **CADGrasp**ï¼Œä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼š\n- **ç¬¬ä¸€é˜¶æ®µï¼šé¢„æµ‹ä¸­é—´è¡¨ç¤º**ã€‚æå‡ºä¸€ç§**ç¨€ç–äº¤äº’å¹³åˆ†é¢**ä½œä¸ºä¸åœºæ™¯è§£è€¦ã€æ„ŸçŸ¥æ¥è§¦å’Œç¢°æ’çš„ä¸­é—´è¡¨ç¤ºã€‚å®ƒç´§å‡‘åœ°ç¼–ç äº†æ‰‹ä¸åœºæ™¯çš„å‡ ä½•å’Œæ¥è§¦å…³ç³»ã€‚ä¸ºäº†é«˜æ•ˆç”Ÿæˆæ­¤é«˜ç»´è¡¨ç¤ºï¼Œè®¾è®¡äº†ä¸€ä¸ª**ä½“ç´ çº§æ¡ä»¶å¼•å¯¼çš„å ç”¨æ‰©æ•£æ¨¡å‹**ï¼Œå¹¶å¼•å…¥åŠ›é—­åˆåˆ†æ•°è¿›è¡Œå€™é€‰æ’åºã€‚\n- **ç¬¬äºŒé˜¶æ®µï¼šçº¦æŸä¼˜åŒ–æŠ“å–å§¿æ€**ã€‚åŸºäºé¢„æµ‹çš„ç¨€ç–IBSï¼Œè®¾è®¡äº†ä¸€ç³»åˆ—èƒ½é‡å‡½æ•°ï¼ˆå…³èŠ‚é™åˆ¶ã€è‡ªç©¿é€ã€æ¥è§¦ã€ç¢°æ’ï¼‰å’Œæ’åºç­–ç•¥ï¼Œé€šè¿‡ä¼˜åŒ–ç®—æ³•ç”Ÿæˆé«˜è´¨é‡ã€æ— ç¢°æ’çš„çµå·§æŠ“å–å§¿æ€ã€‚\n\n**æ ¸å¿ƒåˆ›æ–°**åœ¨äºåˆ©ç”¨**ç¨€ç–IBS**è¿™ä¸€é€šç”¨ã€åœºæ™¯è§£è€¦çš„ä¸­é—´è¡¨ç¤ºï¼Œå°†å¤æ‚çš„å§¿æ€é¢„æµ‹é—®é¢˜åˆ†è§£ä¸ºæ›´æ˜“å­¦ä¹ çš„è¡¨ç¤ºé¢„æµ‹å’ŒåŸºäºç‰©ç†çš„ä¼˜åŒ–ï¼Œä»è€Œæå‡äº†æ³›åŒ–èƒ½åŠ›å’Œå¯¹ç¢°æ’çš„æ„ŸçŸ¥ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\n- **ä»¿çœŸå®éªŒ**ï¼šåœ¨åŒ…å«670ä¸ªæ‚ä¹±åœºæ™¯ã€1300å¤šä¸ªç‰©ä½“çš„æµ‹è¯•é›†ä¸Šï¼ŒCADGraspå–å¾—äº†**æœ€é«˜æˆåŠŸç‡**ï¼ˆä¾‹å¦‚ï¼Œåœ¨å¯†é›†åœºæ™¯ä¸‹è¾¾åˆ°86.5%ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºDexGraspNet2.0ç­‰åŸºçº¿æ–¹æ³•ã€‚\n- **è·¨æœ¬ä½“æ³›åŒ–**ï¼šå¾—ç›Šäºç¨€ç–IBSçš„é€šç”¨æ€§ï¼Œæ¨¡å‹åœ¨æœªè®­ç»ƒè¿‡çš„Allegroçµå·§æ‰‹ä¸Šè¿›è¡Œé›¶æ ·æœ¬æµ‹è¯•ï¼Œä»èƒ½å–å¾—ä¸Leapæ‰‹ç›¸å½“çš„æˆåŠŸç‡ï¼ˆçº¦75%ï¼‰ã€‚\n- **å®ç‰©å®éªŒ**ï¼šåœ¨çœŸå®æœºå™¨äººå¹³å°ä¸Šï¼ˆFlexivæœºæ¢°è‡‚+Leapæ‰‹ï¼‰ï¼Œå¯¹30ä¸ªä¸åŒç‰©ä½“åœ¨5ä¸ªæ‚ä¹±åœºæ™¯ä¸­è¿›è¡Œæµ‹è¯•ï¼Œå¹³å‡æŠ“å–æˆåŠŸç‡è¾¾åˆ°**93.8%**ï¼Œè¯æ˜äº†æ–¹æ³•çš„å®ç”¨æ€§ã€‚\n- **æ•ˆç‡**ï¼šå¹³å‡ç”Ÿæˆä¸€ä¸ªæŠ“å–è€—æ—¶çº¦6.51ç§’ï¼Œä¼˜äºåŒç±»ä¸¤é˜¶æ®µæ–¹æ³•ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\nCADGraspé€šè¿‡å¼•å…¥**æ¥è§¦ä¸ç¢°æ’æ„ŸçŸ¥çš„ä¸­é—´è¡¨ç¤º**å’Œ**ä¸¤é˜¶æ®µä¼˜åŒ–æ¡†æ¶**ï¼Œæœ‰æ•ˆè§£å†³äº†æ‚ä¹±åœºæ™¯ä¸­çµå·§æŠ“å–çš„æ³›åŒ–ã€ç¨³å®šæ€§å’Œé¿ç¢°éš¾é¢˜ã€‚å…¶æ–¹æ³•ä¸ä¾èµ–äºå®Œæ•´åœºæ™¯å‡ ä½•ï¼Œå¯¹å•è§†è§’è§‚æµ‹æ›´å…·é²æ£’æ€§ã€‚ç¨€ç–IBSè¡¨ç¤ºçš„é€šç”¨æ€§ä½¿å…¶å…·å¤‡**è·¨ä¸åŒçµå·§æ‰‹é›¶æ ·æœ¬æ³›åŒ–çš„æ½œåŠ›**ï¼Œä¸ºå®é™…æœºå™¨äººéƒ¨ç½²æä¾›äº†æ›´é€šç”¨ã€å¯é çš„çµå·§æŠ“å–è§£å†³æ–¹æ¡ˆã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## CADGrasp è®ºæ–‡åˆ†æ\n\n### **æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**æ‚ä¹±åœºæ™¯ä¸­çµå·§æ‰‹æŠ“å–**çš„å…³é”®æŒ‘æˆ˜ï¼š\n1.  **é«˜è‡ªç”±åº¦**ï¼šçµå·§æ‰‹ï¼ˆå¦‚Leap Handï¼‰å…³èŠ‚å¤šï¼Œå§¿æ€ç©ºé—´å·¨å¤§ã€‚\n2.  **é®æŒ¡ä¸éƒ¨åˆ†è§‚æµ‹**ï¼šç‰©ä½“å †å å¯¼è‡´å•è§†è§’ç‚¹äº‘æ— æ³•æä¾›å®Œæ•´å‡ ä½•ä¿¡æ¯ã€‚\n3.  **ç¢°æ’é£é™©**ï¼šå¤æ‚å¸ƒå±€ä¸­ï¼Œæ‰‹éƒ¨ææ˜“ä¸ç›®æ ‡ç‰©ä½“åŠå‘¨å›´ç¯å¢ƒå‘ç”Ÿç¢°æ’ï¼Œå¯¼è‡´æŠ“å–å¤±è´¥æˆ–åœºæ™¯æ‰°åŠ¨ã€‚\n\nç°æœ‰æ–¹æ³•ï¼ˆå¦‚ç«¯åˆ°ç«¯å›å½’æˆ–ç”Ÿæˆæ¨¡å‹ï¼‰ç›´æ¥æ˜ å°„ç‚¹äº‘åˆ°æŠ“å–å§¿æ€ï¼Œç”±äº**è§‚æµ‹ç©ºé—´ä¸å§¿æ€ç©ºé—´ä¹‹é—´çš„éçº¿æ€§æ˜ å°„**ä»¥åŠ**ç‰©ç†çº¦æŸå¯¹å¾®å°è¯¯å·®çš„æ•æ„Ÿæ€§**ï¼Œåœ¨æ‚ä¹±åœºæ™¯ä¸­æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚ä¼ ç»Ÿçš„ä¸¤é˜¶æ®µæ–¹æ³•åˆä¾èµ–å®Œæ•´çš„ç‰©ä½“å‡ ä½•ä¿¡æ¯è¿›è¡Œä¼˜åŒ–ï¼Œè¿™åœ¨éƒ¨åˆ†è§‚æµ‹çš„æ‚ä¹±åœºæ™¯ä¸­ä¸é€‚ç”¨ã€‚\n\n### **æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸º **CADGrasp** çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸€ç§**åœºæ™¯è§£è€¦ã€æ¥è§¦ä¸ç¢°æ’æ„ŸçŸ¥çš„ä¸­é—´è¡¨ç¤º**ï¼Œå¹¶å›´ç»•å…¶æ„å»ºäº†å®Œæ•´çš„é¢„æµ‹ä¸ä¼˜åŒ–æµç¨‹ã€‚\n\n#### **1. åˆ›æ–°çš„ä¸­é—´è¡¨ç¤ºï¼šç¨€ç–äº¤äº’ç­‰åˆ†é¢**\n- **æ˜¯ä»€ä¹ˆ**ï¼š**ç¨€ç–IBS** æ˜¯çµå·§æ‰‹ä¸åœºæ™¯ä¹‹é—´**äº¤äº’ç­‰åˆ†é¢**çš„ç´§å‡‘è¡¨ç¤ºã€‚å®ƒåœ¨ä»¥æŠ“å–ç§å­ç‚¹ä¸ºä¸­å¿ƒçš„è§„èŒƒç©ºé—´å†…ï¼Œä»¥ä½“ç´ ç½‘æ ¼å½¢å¼ç¼–ç ä¸‰ç±»ä¿¡æ¯ï¼š\n    1.  IBSè¡¨é¢å æ®ï¼ˆå‡ ä½•å…³ç³»ï¼‰ã€‚\n    2.  æ‹‡æŒ‡ä¸ç›®æ ‡ç‰©ä½“çš„æ¥è§¦ç‚¹ã€‚\n    3.  å…¶ä»–æ‰‹æŒ‡ä¸ç›®æ ‡ç‰©ä½“çš„æ¥è§¦ç‚¹ã€‚\n- **ä¸ºä»€ä¹ˆåˆ›æ–°**ï¼š\n    - **åœºæ™¯è§£è€¦**ï¼šè¯¥è¡¨ç¤ºåŸºäºè§„èŒƒåŒ–çš„å±€éƒ¨ç©ºé—´ï¼Œ**ä¸ä¾èµ–äºå®Œæ•´çš„åœºæ™¯å‡ ä½•**ï¼Œå®Œç¾é€‚é…å•è§†è§’éƒ¨åˆ†è§‚æµ‹çš„æ‚ä¹±åœºæ™¯ã€‚\n    - **ç‰©ç†ä¿¡æ¯ä¸°å¯Œ**ï¼šåŒæ—¶ç¼–ç äº†**å‡ ä½•ä¸´è¿‘æ€§**ï¼ˆIBSè¡¨é¢å®šä¹‰äº†å®‰å…¨è¾¹ç•Œï¼‰å’Œ**å…³é”®æ¥è§¦ä¿¡æ¯**ï¼ˆåŒºåˆ†æ‹‡æŒ‡/å…¶ä»–æ‰‹æŒ‡ï¼‰ï¼Œä¸ºåç»­ä¼˜åŒ–æä¾›äº†æ˜ç¡®ã€å…¨é¢çš„ç‰©ç†çº¦æŸç›®æ ‡ã€‚\n\n#### **2. åˆ›æ–°çš„ä¸¤é˜¶æ®µæ¡†æ¶**\n**ç¬¬ä¸€é˜¶æ®µï¼šæ¡ä»¶åŒ–IBSç”Ÿæˆ**\n- **æ–¹æ³•**ï¼šä½¿ç”¨ä¸€ä¸ª**å ç”¨æ‰©æ•£æ¨¡å‹**æ¥å­¦ä¹ ç»™å®šè§„èŒƒåŒ–åœºæ™¯ç‚¹äº‘æ—¶ï¼Œç¨€ç–IBSçš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ `p(â„|ğ’«*)`ã€‚\n- **æŠ€æœ¯åˆ›æ–°**ï¼š\n    - **ä½“ç´ çº§æ¡ä»¶å¼•å¯¼**ï¼šå°†ç‚¹äº‘ç‰¹å¾ä¸æ‰©æ•£æ¨¡å‹çš„UNetç‰¹å¾åœ¨å¯¹åº”å±‚çº§æ‹¼æ¥ï¼Œå®ç°ç»†ç²’åº¦æ¡ä»¶ç”Ÿæˆï¼Œæå‡é¢„æµ‹è´¨é‡ã€‚\n    - **åŠ›é—­åˆåˆ†æ•°è¿‡æ»¤**ï¼šå¯¹æ‰©æ•£æ¨¡å‹é‡‡æ ·ç”Ÿæˆçš„å¤šä¸ªIBSå€™é€‰ï¼Œæ ¹æ®å…¶æ¥è§¦ç‚¹è®¡ç®—åŠ›é—­åˆåˆ†æ•°è¿›è¡Œæ’åºï¼Œé€‰æ‹©æœ€ä¼˜çš„ä½œä¸ºä¸­é—´è¡¨ç¤ºã€‚\n\n**ç¬¬äºŒé˜¶æ®µï¼šåŸºäºIBSçº¦æŸçš„æŠ“å–å§¿æ€ä¼˜åŒ–**\n- **æ–¹æ³•**ï¼šä»¥é¢„æµ‹çš„ç¨€ç–IBS `â„Ì‚` ä¸ºçº¦æŸç›®æ ‡ï¼Œé€šè¿‡æœ€å°åŒ–ä¸€ä¸ªç»„åˆèƒ½é‡å‡½æ•°æ¥ä¼˜åŒ–çµå·§æ‰‹çš„å…³èŠ‚å§¿æ€ã€‚\n- **æŠ€æœ¯åˆ›æ–°**ï¼š\n    - **é‡èº«å®šåˆ¶çš„èƒ½é‡å‡½æ•°**ï¼šè®¾è®¡äº†å››é¡¹èƒ½é‡é¡¹ï¼Œå‡ä¸IBSè¡¨ç¤ºç´§å¯†å…³è”ï¼š\n        - `ğ„_j`ï¼šå…³èŠ‚é™ä½ã€‚\n        - `ğ„_sp`ï¼šæ‰‹éƒ¨è‡ªç©¿é€æƒ©ç½šã€‚\n        - `ğ„_p`ï¼š**ç¢°æ’èƒ½é‡**ã€‚çº¦æŸæ‰‹éƒ¨ç‚¹äº‘ä½äºIBSè¡¨é¢â€œæ­£ç¡®â€çš„ä¸€ä¾§ï¼ˆå³ç‰©ä½“å¤–éƒ¨ï¼‰ï¼Œåˆ©ç”¨IBSæ³•å‘é‡ç¡®ä¿æ— ç¢°æ’ã€‚\n        - `ğ„_d`ï¼š**æ¥è§¦èƒ½é‡**ã€‚å¸å¼•æ‰‹æŒ‡ï¼ˆç‰¹åˆ«æ˜¯æ‹‡æŒ‡å’Œå…¶ä»–æ‰‹æŒ‡åˆ†åˆ«å¤„ç†ï¼‰å‘IBSä¸­é¢„æµ‹çš„å¯¹åº”æ¥è§¦ç‚¹é è¿‘ï¼Œå®ç°ç¨³å®šæ¥è§¦ã€‚\n    - **ä¼˜åŒ–ç»“æœæ’åº**ï¼šè¿›è¡Œå¤šæ¬¡ä¼˜åŒ–å°è¯•ï¼Œæ ¹æ®æœ€ç»ˆèƒ½é‡æ®‹å·®å¯¹ç”Ÿæˆçš„æŠ“å–å§¿æ€è¿›è¡Œæ’åºï¼Œé€‰æ‹©æœ€ä¼˜è§£ã€‚\n\n### **è§£å†³æ–¹æ¡ˆæ€»ç»“**\nè®ºæ–‡é€šè¿‡ **â€œé¢„æµ‹ä¸­é—´è¡¨ç¤º -> åŸºäºè¡¨ç¤ºä¼˜åŒ–â€** çš„èŒƒå¼ï¼Œå°†å¤æ‚çš„æŠ“å–å§¿æ€ç”Ÿæˆé—®é¢˜åˆ†è§£ï¼š\n1.  **é™ä½æ˜ å°„éš¾åº¦**ï¼šç¬¬ä¸€é˜¶æ®µåœ¨è§‚æµ‹ç©ºé—´ï¼ˆç‚¹äº‘ï¼‰åˆ°è¡¨ç¤ºç©ºé—´ï¼ˆIBSï¼‰ä¹‹é—´å­¦ä¹ æ˜ å°„ï¼Œæ¯”ç›´æ¥æ˜ å°„åˆ°é«˜ç»´å§¿æ€ç©ºé—´æ›´ç®€å•ã€æ›´ç¨³å®šã€‚\n2.  **æ³¨å…¥ç‰©ç†çº¦æŸ**ï¼šç¨€ç–IBSæœ¬èº«è•´å«äº†æ¥è§¦ä¸ç¢°æ’çš„ç‰©ç†ä¿¡æ¯ã€‚ç¬¬äºŒé˜¶æ®µä¼˜åŒ–å°†è¿™äº›ä¿¡æ¯è½¬åŒ–ä¸ºæ˜ç¡®çš„èƒ½é‡é¡¹ï¼Œ**æ˜¾å¼åœ°**å¼•å¯¼ç”Ÿæˆ**ç¨³å®šä¸”æ— ç¢°æ’**çš„æŠ“å–å§¿æ€ã€‚\n3.  **æå‡æ³›åŒ–èƒ½åŠ›**ï¼šä¸­é—´è¡¨ç¤ºçš„åœºæ™¯è§£è€¦ç‰¹æ€§ä½¿å…¶å¯¹é®æŒ¡é²æ£’ï¼›ä¸¤é˜¶æ®µè®¾è®¡å‡å°‘äº†å¯¹æµ·é‡æŠ“å–å§¿æ€æ•°æ®åˆ†å¸ƒçš„ä¾èµ–ï¼Œæé«˜äº†å¯¹æœªè§ç‰©ä½“å’Œåœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¿˜è¯æ˜äº†è¯¥è¡¨ç¤ºæ”¯æŒ**é›¶æ ·æœ¬è·¨çµå·§æ‰‹æ³›åŒ–**ï¼ˆå¦‚ä»Leap Handåˆ°Allegro Handï¼‰ã€‚\n\n### **å®é™…ä»·å€¼**\n- **æ€§èƒ½æå‡**ï¼šåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­ï¼ŒCADGraspåœ¨æ‚ä¹±åœºæ™¯ä¸‹çš„æŠ“å–æˆåŠŸç‡æ˜¾è‘—ä¼˜äºç°æœ‰ç«¯åˆ°ç«¯åŸºçº¿ï¼ˆå¦‚DexGraspNet2.0ï¼‰ã€‚\n- **å®‰å…¨æ€§å¢å¼º**ï¼šé€šè¿‡IBSæ˜¾å¼å»ºæ¨¡ç¢°æ’è¾¹ç•Œï¼Œç”Ÿæˆçš„æŠ“å–å§¿æ€èƒ½æœ‰æ•ˆé¿å…ä¸åœºæ™¯ä¸­å…¶ä»–ç‰©ä½“åŠæ¡Œé¢çš„ç¢°æ’ï¼Œè¿™å¯¹äºå®é™…æœºå™¨äººéƒ¨ç½²è‡³å…³é‡è¦ã€‚\n- **é€šç”¨æ€§**ï¼šæ¡†æ¶ä¸ä¾èµ–äºç‰¹å®šæ‰‹éƒ¨æ¨¡å‹ï¼Œä¸­é—´è¡¨ç¤ºçš„é€šç”¨æ€§ä¸ºä¸åŒçµå·§æ‰‹ä¹‹é—´çš„æŠ“å–ç­–ç•¥è¿ç§»æä¾›äº†å¯èƒ½ã€‚\n- **å®ç”¨æ€§**ï¼šå°½ç®¡éå®æ—¶ï¼ˆ~6.5ç§’/æŠ“å–ï¼‰ï¼Œä½†å…¶æ¨ç†é€Ÿåº¦å·²ä¼˜äºè®¸å¤šç°æœ‰ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œä¸”é€šè¿‡å·¥ç¨‹ä¼˜åŒ–æœ‰è¿›ä¸€æ­¥æå‡æ½œåŠ›ã€‚",
            "timestamp": "2026-01-27T16:01:48.476301",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡åˆ†æï¼šCADGraspåŠå…¶åœ¨çµå·§æŠ“å–é¢†åŸŸçš„å‘å±•è„‰ç»œ\n\nåŸºäºå¯¹è®ºæ–‡ã€ŠCADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenesã€‹çš„åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥æ¢³ç†å‡ºçµå·§æ‰‹åœ¨æ‚ä¹±åœºæ™¯ä¸­æŠ“å–è¿™ä¸€æ–¹å‘çš„å‘å±•è„‰ç»œã€‚å…¶æ ¸å¿ƒæŒ‘æˆ˜åœ¨äº**é«˜è‡ªç”±åº¦ã€é®æŒ¡å’Œç¢°æ’**ã€‚å‘å±•ä¸»çº¿å¯ä»¥æ¸…æ™°åœ°åˆ†ä¸ºå‡ ä¸ªé˜¶æ®µå’Œä¸åŒçš„æŠ€æœ¯æ–¹å‘ã€‚\n\n### å‘å±•è„‰ç»œæ€»è§ˆ\n\nè¯¥é¢†åŸŸçš„å‘å±•å‘ˆç°å‡ºä»**ç®€å•åˆ°å¤æ‚**ï¼ˆå•ç‰©ä½“ -> æ‚ä¹±åœºæ™¯ï¼‰ã€ä»**ç›´æ¥æ˜ å°„åˆ°åˆ†é˜¶æ®µä¼˜åŒ–**ã€ä»**ä¾èµ–å®Œæ•´å‡ ä½•åˆ°é€‚åº”éƒ¨åˆ†è§‚æµ‹**çš„æ¼”è¿›è¶‹åŠ¿ã€‚CADGraspæ˜¯è¿™ä¸€è„‰ç»œä¸‹çš„æœ€æ–°è¿›å±•ï¼Œå®ƒç»¼åˆäº†å¤šä¸ªæ–¹å‘çš„ä¼˜ç‚¹ã€‚\n\nä»¥ä¸‹æ˜¯åŸºäºè®ºæ–‡å†…å®¹æ¢³ç†çš„å…³é”®å‘å±•é˜¶æ®µå’Œæ”¹è¿›ï¼š\n\n### æ–¹å‘ä¸€ï¼šä»å•ç‰©ä½“æŠ“å–åˆ°æ‚ä¹±åœºæ™¯æŠ“å–\n\nè¿™æ˜¯åº”ç”¨åœºæ™¯çš„æ ¹æœ¬æ€§æ‹“å±•ï¼Œå¸¦æ¥äº†å…¨æ–°çš„æŒ‘æˆ˜ã€‚\n\n*   **æ—©æœŸå·¥ä½œï¼ˆå•ç‰©ä½“æŠ“å–ï¼‰:**\n    *   **ä»£è¡¨æ–¹æ³•:** GraspTTA, GenDexGrasp, Contact2Grasp ç­‰ã€‚\n    *   **ç‰¹ç‚¹ä¸å±€é™:** è¿™äº›æ–¹æ³•é€šå¸¸å‡è®¾å¯ä»¥è·å¾—**å®Œæ•´çš„ç‰©ä½“å‡ ä½•æ¨¡å‹**ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ¥è§¦å›¾é¢„æµ‹å’ŒæŠ“å–ä½å§¿ä¼˜åŒ–ã€‚å®ƒä»¬çš„èƒ½é‡å‡½æ•°å’Œä¼˜åŒ–ç­–ç•¥ä¸¥é‡ä¾èµ–å®Œæ•´çš„ç‰©ä½“è¡¨é¢ä¿¡æ¯ã€‚**åœ¨æ‚ä¹±åœºæ™¯ä¸­ï¼Œç”±äºç‰©ä½“å †å å’Œé®æŒ¡ï¼Œæ— æ³•è·å¾—å®Œæ•´è§‚æµ‹ï¼Œå› æ­¤è¿™äº›æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨ã€‚**\n\n*   **æ”¹è¿›å·¥ä½œï¼ˆé€‚åº”æ‚ä¹±åœºæ™¯ï¼‰:**\n    *   **ä»£è¡¨æ–¹æ³•:** DexGraspNet 2.0, HGC-Netã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›:** å¼€å§‹ä½¿ç”¨**å•è§†è§’ç‚¹äº‘**ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ„å»ºå¤§è§„æ¨¡åˆæˆæ•°æ®é›†æ¥æ¨¡æ‹Ÿæ‚ä¹±åœºæ™¯çš„åˆ†å¸ƒã€‚\n    *   **æŠ€æœ¯è·¯å¾„:** ä¸»è¦é‡‡ç”¨**ç«¯åˆ°ç«¯**çš„èŒƒå¼ï¼Œç›´æ¥ä»è§‚æµ‹ç‚¹äº‘å›å½’æˆ–ç”ŸæˆæŠ“å–ä½å§¿ã€‚\n    *   **é—ç•™é—®é¢˜:** ç«¯åˆ°ç«¯æ˜ å°„ç”±äºä»è§‚æµ‹ç©ºé—´åˆ°é«˜ç»´ä½å§¿ç©ºé—´çš„**éçº¿æ€§**ä»¥åŠæŠ“å–å¯¹ç‰©ç†çº¦æŸçš„**æ•æ„Ÿæ€§**ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ç”Ÿæˆçš„ç»“æœåœ¨ç¢°æ’é¿å…å’Œç¨³å®šæ€§ä¸Šä»æœ‰ä¸è¶³ã€‚\n\n*   **CADGraspçš„è´¡çŒ®:**\n    *   **å…³é”®åˆ›æ–°:** æå‡ºäº†ä¸€ä¸ª**ä¸åœºæ™¯è§£è€¦çš„ä¸­é—´è¡¨ç¤º**â€”â€”ç¨€ç–IBSã€‚\n    *   **å¦‚ä½•è§£å†³é—ç•™é—®é¢˜:**\n        1.  **è§£è€¦æ€§:** ç¨€ç–IBSçš„ç”Ÿæˆä¸ä¾èµ–äºå®Œæ•´çš„åœºæ™¯å‡ ä½•ï¼Œåªä¸æ‰‹å’Œç‰©ä½“çš„**å±€éƒ¨äº¤äº’åŒºåŸŸ**ç›¸å…³ï¼Œå®Œç¾é€‚é…å•è§†è§’éƒ¨åˆ†è§‚æµ‹ã€‚\n        2.  **ä¿¡æ¯ä¸°å¯Œæ€§:** è¯¥è¡¨ç¤ºåŒæ—¶ç¼–ç äº†**å‡ ä½•å…³ç³»**ï¼ˆIBSè¡¨é¢ï¼Œå®šä¹‰å®‰å…¨è¾¹ç•Œï¼‰ã€**æ¥è§¦ä¿¡æ¯**ï¼ˆæ‹‡æŒ‡/å…¶ä»–æ‰‹æŒ‡æ¥è§¦ç‚¹ï¼‰ï¼Œå¤©ç„¶åœ°å¼•å¯¼ç”Ÿæˆ**ç¨³å®šä¸”æ— ç¢°æ’**çš„æŠ“å–ã€‚\n        3.  **ä¸¤é˜¶æ®µæ¡†æ¶:** å°†å›°éš¾çš„â€œè§‚æµ‹->ä½å§¿â€æ˜ å°„ï¼Œåˆ†è§£ä¸ºâ€œè§‚æµ‹->ä¸­é—´è¡¨ç¤ºâ€å’Œâ€œè¡¨ç¤º->ä½å§¿ä¼˜åŒ–â€ä¸¤ä¸ªç›¸å¯¹æ›´å®¹æ˜“çš„å­é—®é¢˜ï¼Œæå‡äº†æ³›åŒ–èƒ½åŠ›ã€‚\n\n### æ–¹å‘äºŒï¼šä»ç«¯åˆ°ç«¯é¢„æµ‹åˆ°ä¸¤é˜¶æ®µä¼˜åŒ–\n\nè¿™æ˜¯æ–¹æ³•å­¦æ¡†æ¶çš„æ¼”è¿›ï¼Œæ—¨åœ¨æå‡æŠ“å–çš„è´¨é‡å’Œæ³›åŒ–æ€§ã€‚\n\n*   **ç«¯åˆ°ç«¯æ–¹æ³•:**\n    *   **ä»£è¡¨æ–¹æ³•:** å›å½’æ–¹æ³•ï¼ˆHGC-Netï¼‰ï¼Œç”Ÿæˆæ–¹æ³•ï¼ˆDexGraspNet 2.0, UGGï¼‰ã€‚\n    *   **ç‰¹ç‚¹:** è¿½æ±‚ç®€æ´ï¼Œä¸€æ­¥è¾“å‡ºæŠ“å–ä½å§¿ã€‚ç”Ÿæˆå¼æ–¹æ³•ï¼ˆå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹ï¼‰èƒ½æ›´å¥½åœ°å»ºæ¨¡æŠ“å–ä½å§¿çš„å¤šæ¨¡æ€åˆ†å¸ƒã€‚\n    *   **å±€é™:** å¦‚å‰è¿°ï¼Œå¯¹æ•°æ®é›†è§„æ¨¡å’Œè´¨é‡ä¾èµ–æé«˜ï¼Œä¸”éš¾ä»¥ç²¾ç¡®æ»¡è¶³å¤æ‚çš„ç‰©ç†çº¦æŸï¼ˆå¦‚ç²¾ç¡®æ¥è§¦ã€é›¶ç¢°æ’ï¼‰ï¼Œå¯¼è‡´åœ¨ä»¿çœŸä¸­è¡¨ç°å°šå¯ï¼Œä½†**sim-to-real**å·®è·å¤§ï¼Œå®é™…æˆåŠŸç‡å—é™ã€‚\n\n*   **ä¸¤é˜¶æ®µæ–¹æ³•ï¼ˆæ—©æœŸï¼Œé’ˆå¯¹å•ç‰©ä½“ï¼‰:**\n    *   **ä»£è¡¨æ–¹æ³•:** GraspTTA, GenDexGraspã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›:** ç¬¬ä¸€é˜¶æ®µé¢„æµ‹**æ¥è§¦å›¾**ç­‰ä¸­é—´è¡¨ç¤ºï¼Œç¬¬äºŒé˜¶æ®µåŸºäº**å®Œæ•´çš„ç‰©ä½“ç½‘æ ¼**è¿›è¡Œç‰©ç†ä¼˜åŒ–ã€‚è¿™æ˜¾è‘—æå‡äº†æŠ“å–çš„ç‰©ç†åˆç†æ€§å’Œè´¨é‡ã€‚\n    *   **å±€é™:** ä¸¥é‡ä¾èµ–**å®Œæ•´ç‰©ä½“å‡ ä½•**ï¼Œæ— æ³•ç”¨äºæ‚ä¹±åœºæ™¯ã€‚\n\n*   **CADGraspçš„è´¡çŒ®:**\n    *   **å…³é”®åˆ›æ–°:** å°†ä¸¤é˜¶æ®µæ¡†æ¶**æˆåŠŸå¼•å…¥æ‚ä¹±åœºæ™¯**ã€‚\n    *   **å¦‚ä½•å®ç°:**\n        1.  **è®¾è®¡äº†é€‚ç”¨äºæ‚ä¹±åœºæ™¯çš„ä¸­é—´è¡¨ç¤ºï¼ˆç¨€ç–IBSï¼‰**ï¼Œæ›¿ä»£äº†éœ€è¦å®Œæ•´å‡ ä½•çš„æ¥è§¦å›¾ã€‚\n        2.  **å¼€å‘äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é¢„æµ‹å™¨**ï¼Œç”¨äºä»éƒ¨åˆ†ç‚¹äº‘ç”Ÿæˆé«˜è´¨é‡çš„ç¨€ç–IBSã€‚\n        3.  **è®¾è®¡äº†ä¸€å¥—é’ˆå¯¹ç¨€ç–IBSçš„ä¸“ç”¨èƒ½é‡å‡½æ•°**ï¼ˆ`E_p`ç”¨äºæ¥è§¦çº¦æŸï¼Œ`E_d`ç”¨äºè·ç¦»çº¦æŸï¼‰ï¼Œå°†IBSè•´å«çš„å‡ ä½•å’Œæ¥è§¦ä¿¡æ¯è½¬åŒ–ä¸ºå¯ä¼˜åŒ–çš„ç›®æ ‡ï¼Œä»è€Œå¾—åˆ°é«˜è´¨é‡æŠ“å–ä½å§¿ã€‚\n\n### æ–¹å‘ä¸‰ï¼šä¸­é—´è¡¨ç¤ºçš„æ¼”è¿›\n\nè¿™æ˜¯æå‡ä¸¤é˜¶æ®µæ–¹æ³•æ•ˆèƒ½çš„æ ¸å¿ƒã€‚\n\n*   **æ¥è§¦å›¾:**\n    *   **ç”¨é€”:** æè¿°æ‰‹ä¸Šæ¯ä¸ªç‚¹ä¸ç‰©ä½“è¡¨é¢çš„è·ç¦»ï¼Œæ˜¯å•ç‰©ä½“æŠ“å–ä¸­å¸¸ç”¨çš„ä¸­é—´è¡¨ç¤ºã€‚\n    *   **å±€é™:** è®¡ç®—å’Œä¼˜åŒ–éƒ½éœ€è¦å®Œæ•´çš„ç‰©ä½“è¡¨é¢ï¼Œ**æ— æ³•å¤„ç†é®æŒ¡**ã€‚\n\n*   **äº¤äº’å¹³åˆ†æ›²é¢ï¼ˆIBSï¼‰ä½œä¸ºè§‚æµ‹è¡¨ç¤º:**\n    *   **ä»£è¡¨å·¥ä½œ:** She et al. ç­‰äººçš„å·¥ä½œã€‚\n    *   **ç”¨é€”:** å°†IBSç”¨ä½œ**è§‚æµ‹çš„å‹ç¼©è¡¨ç¤º**ï¼Œè¾“å…¥ç»™ç­–ç•¥ç½‘ç»œï¼Œä»¥é™ä½å­¦ä¹ å¤æ‚åº¦ã€‚\n    *   **å±€é™:** å°šæœªä½œä¸º**ä¼˜åŒ–ç›®æ ‡**ä½¿ç”¨ã€‚\n\n*   **CADGraspçš„è´¡çŒ®:**\n    *   **å…³é”®åˆ›æ–°:** é¦–æ¬¡å°†IBSæ”¹é€ å¹¶ç”¨ä½œ**æŠ“å–ä½å§¿ä¼˜åŒ–çš„ä¸­é—´è¡¨ç¤ºå’Œç›®æ ‡**ã€‚\n    *   **å…·ä½“æ”¹é€ :**\n        1.  **ç¨€ç–åŒ–ä¸ç»“æ„åŒ–:** å°†IBSä½“ç´ åŒ–ï¼Œå¹¶åœ¨æ¯ä¸ªä½“ç´ ä¸­ç¼–ç ä¸‰ç±»ä¿¡æ¯ï¼šIBSè¡¨é¢ã€æ‹‡æŒ‡æ¥è§¦ç‚¹ã€å…¶ä»–æ‰‹æŒ‡æ¥è§¦ç‚¹ã€‚è¿™ä½¿å¾—è¡¨ç¤ºæ›´ç´§å‡‘ä¸”ä¿¡æ¯æ˜ç¡®ã€‚\n        2.  **è§£è€¦ä¸è§„èŒƒåŒ–:** ä»¥æŠ“å–ç§å­ç‚¹å’Œæ‰‹è…•æœå‘å®šä¹‰è§„èŒƒç©ºé—´ï¼Œä½¿è¡¨ç¤ºä¸åœºæ™¯å…¨å±€å¸ƒå±€è§£è€¦ï¼Œåªå…³æ³¨å±€éƒ¨äº¤äº’ï¼Œå¢å¼ºäº†æ³›åŒ–æ€§ã€‚\n        3.  **é¢„æµ‹ä¸ä¼˜åŒ–é—­ç¯:** ä¸ä»…é¢„æµ‹å®ƒï¼Œè¿˜ä¸“é—¨è®¾è®¡äº†ä¸ä¹‹åŒ¹é…çš„ä¼˜åŒ–èƒ½é‡å‡½æ•°ï¼Œå½¢æˆäº†å®Œæ•´çš„â€œé¢„æµ‹-ä¼˜åŒ–â€å›è·¯ã€‚\n\n### æ–¹å‘å››ï¼šè·¨çµå·§æ‰‹æ³›åŒ–\n\nè¿™æ˜¯ä¸€ä¸ªå…·æœ‰é‡è¦å®ç”¨ä»·å€¼çš„æ–¹å‘ã€‚\n\n*   **å…ˆå‰å·¥ä½œçš„å±€é™:** ç»å¤§å¤šæ•°æ–¹æ³•éƒ½æ˜¯**æ‰‹å‹ç‰¹å®š**çš„ã€‚ä¸ºä¸€ç§çµå·§æ‰‹ï¼ˆå¦‚Leap Handï¼‰è®­ç»ƒçš„ç­–ç•¥æˆ–ç”Ÿæˆå™¨ï¼Œæ— æ³•ç›´æ¥ç”¨äºå¦ä¸€ç§æ‰‹ï¼ˆå¦‚Allegro Handï¼‰ã€‚éœ€è¦é‡æ–°æ”¶é›†æ•°æ®ã€è®­ç»ƒæ¨¡å‹ï¼Œæˆæœ¬é«˜æ˜‚ã€‚\n\n*   **CADGraspçš„è´¡çŒ®:**\n    *   **å…³é”®åˆ›æ–°:** åˆ©ç”¨ä¸­é—´è¡¨ç¤ºçš„** embodiment-agnostic **ï¼ˆä¸å…·ä½“æ‰‹å‹æ— å…³ï¼‰ç‰¹æ€§ï¼Œå®ç°äº†é›¶æ ·æœ¬è·¨æ‰‹å‹æ³›åŒ–ã€‚\n    *   **å¦‚ä½•å®ç°:** ç¨€ç–IBSæè¿°çš„æ˜¯â€œæ‰‹ä¸ç‰©ä½“/åœºæ™¯ä¹‹é—´åº”æœ‰çš„äº¤äº’çŠ¶æ€â€ï¼Œè€Œä¸é™å®šæ˜¯å“ªåªæ‰‹ã€‚åªè¦ç¬¬äºŒé˜¶æ®µä¼˜åŒ–æ—¶ï¼Œå°†èƒ½é‡å‡½æ•°ä¸­çš„æ‰‹éƒ¨ç‚¹äº‘ `P_h` æ›¿æ¢ä¸ºæ–°æ‰‹çš„æ¨¡å‹ï¼Œå°±èƒ½ç”¨**åŒä¸€ä¸ªé¢„æµ‹çš„ç¨€ç–IBS**æ¥ä¼˜åŒ–å‡ºæ–°æ‰‹çš„æŠ“å–ä½å§¿ã€‚\n    *   **å®éªŒéªŒè¯:** è®ºæ–‡å±•ç¤ºäº†åœ¨Leap Handä¸Šè®­ç»ƒï¼Œæ— éœ€å¾®è°ƒå³å¯ä¸ºAllegro Handç”ŸæˆæˆåŠŸæŠ“å–ï¼Œè¯æ˜äº†è¯¥è¡¨ç¤ºçš„é€šç”¨æ€§ã€‚\n\n### æ€»ç»“\n\nCADGraspçš„å·¥ä½œæ˜¯ç«™åœ¨å‰ä½œè‚©è†€ä¸Šçš„ç³»ç»Ÿæ€§åˆ›æ–°ï¼š\n1.  **åœºæ™¯ä¸Š**ï¼Œç»§æ‰¿äº†DexGraspNet 2.0å¯¹æ‚ä¹±åœºæ™¯çš„å…³æ³¨ã€‚\n2.  **æ¡†æ¶ä¸Š**ï¼Œé‡‡ç”¨äº†GraspTTAç­‰ä¸¤é˜¶æ®µä¼˜åŒ–çš„æ€æƒ³ï¼Œä½†**é©å‘½æ€§åœ°æ›¿æ¢äº†æ ¸å¿ƒä¸­é—´è¡¨ç¤º**ï¼Œä½¿å…¶é€‚åº”éƒ¨åˆ†è§‚æµ‹ã€‚\n3.  **è¡¨ç¤ºä¸Š**ï¼Œå€Ÿé‰´äº†IBSçš„æ¦‚å¿µï¼Œä½†å°†å…¶ä»è§‚æµ‹å·¥å…·æå‡ä¸º**ä¼˜åŒ–ç›®æ ‡**ï¼Œå¹¶è¿›è¡Œäº†é¢å‘æŠ“å–ä»»åŠ¡çš„ç»“æ„åŒ–è®¾è®¡ã€‚\n4.  **æ³›åŒ–ä¸Š**ï¼Œåˆ©ç”¨ä¸­é—´è¡¨ç¤ºçš„æŠ½è±¡æ€§ï¼Œå¼€è¾Ÿäº†**é›¶æ ·æœ¬è·¨æ‰‹å‹æ³›åŒ–**çš„æ–°èƒ½åŠ›ã€‚\n\nå› æ­¤ï¼ŒCADGraspçš„å‘å±•è„‰ç»œå¯ä»¥æ¦‚æ‹¬ä¸ºï¼š**ä¸ºäº†è§£å†³ç«¯åˆ°ç«¯æ–¹æ³•åœ¨æ‚ä¹±åœºæ™¯ä¸­æ³›åŒ–ä¸è¶³å’Œç‰©ç†çº¦æŸéš¾ä»¥æ»¡è¶³çš„é—®é¢˜ï¼Œå®ƒåˆ›é€ æ€§åœ°å¼•å…¥äº†ä¸€ä¸ªä¸åœºæ™¯è§£è€¦ã€å¯Œå«ç‰©ç†ä¿¡æ¯çš„ä¸­é—´è¡¨ç¤ºï¼ˆç¨€ç–IBSï¼‰ï¼Œä»è€Œå°†æˆç†Ÿçš„ä¸¤é˜¶æ®µä¼˜åŒ–æ¡†æ¶æˆåŠŸåº”ç”¨äºæ‚ä¹±åœºæ™¯ï¼Œå¹¶æ„å¤–åœ°è·å¾—äº†è·¨æ‰‹å‹æ³›åŒ–çš„å¼ºå¤§èƒ½åŠ›ã€‚** è¿™æ ‡å¿—ç€è¯¥é¢†åŸŸä»è¿½æ±‚ç›´æ¥ç”Ÿæˆï¼Œè½¬å‘è¿½æ±‚**å¯è§£é‡Šã€å¯çº¦æŸã€å¯æ³›åŒ–**çš„æŠ“å–ç”Ÿæˆã€‚",
            "timestamp": "2026-01-27T16:02:55.772037",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## CADGrasp çš„å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼ŒCADGrasp çš„ç ”ç©¶å›¢é˜Ÿï¼ˆç‰¹åˆ«æ˜¯ä½œè€… Zhang ç­‰äººï¼‰åœ¨çµå·§æŠ“å–é¢†åŸŸæœ‰ä¸€ç³»åˆ—é‡è¦çš„å‰æœŸå·¥ä½œã€‚è¿™äº›å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ„å»ºå¤§è§„æ¨¡åˆæˆæ•°æ®é›†å’Œå¼€å‘ç«¯åˆ°ç«¯çš„ç”Ÿæˆæ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯å…¶æ ¸å¿ƒå‰ä½œçš„è¯¦ç»†æ¢³ç†ï¼š\n\n### å‰ä½œæ€»ç»“è¡¨æ ¼\n\n| è®ºæ–‡åç§° | å‘è¡¨å¹´ä»½/ä¼šè®® | æ ¸å¿ƒè´¡çŒ® | ä¸ CADGrasp çš„å…³é”®åŒºåˆ« | CADGrasp çš„ä¸»è¦æ”¹å–„ |\n| :--- | :--- | :--- | :--- | :--- |\n| **DexGraspNet 2.0** <br> (Zhang et al., 2024a) | 2024 (CoRL) | 1. **å¤§è§„æ¨¡åˆæˆæ•°æ®é›†**ï¼šä¸ºæ‚ä¹±åœºæ™¯ä¸­çš„çµå·§æŠ“å–æ„å»ºäº†åŒ…å«7600ä¸ªè®­ç»ƒåœºæ™¯å’Œ670ä¸ªæµ‹è¯•åœºæ™¯çš„æ•°æ®é›†ã€‚<br>2. **ç«¯åˆ°ç«¯æ‰©æ•£æ¨¡å‹**ï¼šç›´æ¥ä½¿ç”¨æ‰©æ•£æ¨¡å‹ä»å•è§†è§’ç‚¹äº‘å›å½’æŠ“å–å§¿æ€ï¼ˆæ‰‹è…•ä½å§¿+å…³èŠ‚è§’ï¼‰ã€‚<br>3. **ç”Ÿæˆå¼æ–¹æ³•**ï¼šèƒ½å¤Ÿå»ºæ¨¡æŠ“å–å§¿æ€çš„å¤šæ¨¡æ€åˆ†å¸ƒã€‚ | 1. **å•é˜¶æ®µã€ç«¯åˆ°ç«¯**ï¼šç›´æ¥æ˜ å°„ç‚¹äº‘åˆ°æŠ“å–å§¿æ€ï¼Œæ²¡æœ‰ä¸­é—´è¡¨ç¤ºæˆ–ä¼˜åŒ–é˜¶æ®µã€‚<br>2. **ä¾èµ–å®Œæ•´æ•°æ®åˆ†å¸ƒ**ï¼šæ³›åŒ–èƒ½åŠ›å—é™äºå¤§è§„æ¨¡æ•°æ®ï¼Œå¯¹ç‰©ç†çº¦æŸï¼ˆå¦‚æ¥è§¦ã€ç¢°æ’ï¼‰çš„å°è¯¯å·®æ•æ„Ÿã€‚<br>3. **æœªæ˜¾å¼å¤„ç†ç¢°æ’**ï¼šåœ¨å¯†é›†æ‚ä¹±åœºæ™¯ä¸­ï¼Œç¢°æ’é¿å…èƒ½åŠ›æœ‰é™ã€‚ | 1. **å¼•å…¥ä¸¤é˜¶æ®µæ¡†æ¶**ï¼šå°†å›°éš¾çš„éçº¿æ€§æ˜ å°„é—®é¢˜åˆ†è§£ä¸º**ä¸­é—´è¡¨ç¤ºé¢„æµ‹**å’Œ**åŸºäºçº¦æŸçš„ä¼˜åŒ–**ä¸¤ä¸ªæ›´æ˜“å¤„ç†çš„é˜¶æ®µï¼Œæå‡äº†æ³›åŒ–èƒ½åŠ›ã€‚<br>2. **æå‡ºåœºæ™¯è§£è€¦çš„ä¸­é—´è¡¨ç¤ºï¼ˆç¨€ç–IBSï¼‰**ï¼šä¸å†éœ€è¦å®Œæ•´ç‰©ä½“å‡ ä½•ï¼Œé€‚ç”¨äºéƒ¨åˆ†è§‚æµ‹çš„æ‚ä¹±åœºæ™¯ï¼Œå¹¶æ˜¾å¼ç¼–ç äº†æ¥è§¦å’Œç¢°æ’ä¿¡æ¯ã€‚<br>3. **æ˜¾å¼ä¼˜åŒ–ç¢°æ’é¿å…**ï¼šé€šè¿‡åŸºäºç¨€ç–IBSè®¾è®¡çš„èƒ½é‡å‡½æ•°ï¼ˆå¦‚ç¢°æ’èƒ½é‡ `E_p`ï¼‰ï¼Œåœ¨ä¼˜åŒ–é˜¶æ®µä¸»åŠ¨é¿å…ä¸åœºæ™¯ä¸­å…¶ä»–ç‰©ä½“çš„ç¢°æ’ã€‚ |\n| **ï¼ˆæ›´æ—©çš„ï¼‰å¤§è§„æ¨¡æ•°æ®é›†æ„å»ºå·¥ä½œ** <br> (Li et al., 2022; Zhang et al., 2024a çš„å¥ åŸº) | 2022 (ICRA) ç­‰ | 1. **å¼€åˆ›æ€§æ•°æ®**ï¼šè¯æ˜äº†åœ¨æ‚ä¹±åœºæ™¯ä¸­è®­ç»ƒçµå·§æŠ“å–æ¨¡å‹éœ€è¦æ„å»ºè¶…å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„åˆæˆæ•°æ®é›†ï¼ˆå¦‚HGC-Netï¼‰ã€‚<br>2. **å›å½’æˆ–ç”ŸæˆåŸºçº¿**ï¼šä¸ºåç»­ç ”ç©¶ï¼ˆå¦‚DexGraspNet 2.0ï¼‰æä¾›äº†æ•°æ®åŸºç¡€å’ŒåŸºçº¿æ–¹æ³•ã€‚ | 1. **æ–¹æ³•å±€é™**ï¼šå›å½’æ–¹æ³•éš¾ä»¥æ•æ‰é«˜è‡ªç”±åº¦æŠ“å–çš„å¤šæ¨¡æ€åˆ†å¸ƒï¼›æ—©æœŸç”Ÿæˆæ–¹æ³•æ•ˆæœä¸ä½³ã€‚<br>2. **åœºæ™¯è€¦åˆ**ï¼šæ¨¡å‹å­¦ä¹ ä¸ç‰¹å®šåœºæ™¯å¸ƒå±€ç´§å¯†è€¦åˆï¼Œæ³›åŒ–åˆ°æ–°ç‰©ä½“å’Œæ–°å¸ƒå±€å›°éš¾ã€‚ | 1. **æ–¹æ³•åˆ›æ–°**ï¼šCADGrasp æ²¡æœ‰åœç•™åœ¨â€œç”¨æ›´å¤§æ•°æ®è®­ç»ƒæ›´å¥½æ¨¡å‹â€çš„è·¯å¾„ä¸Šï¼Œè€Œæ˜¯é€šè¿‡**ç®—æ³•æ¡†æ¶åˆ›æ–°**ï¼ˆä¸¤é˜¶æ®µ+ä¸­é—´è¡¨ç¤ºï¼‰æ¥ä»æ ¹æœ¬ä¸Šæå‡åœ¨æ‚ä¹±åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œå®‰å…¨æ€§ã€‚<br>2. **è¡¨ç¤ºåˆ›æ–°**ï¼šæå‡ºçš„ç¨€ç–IBSæ˜¯ä¸€ç§**ä¸åœºæ™¯è§£è€¦**çš„è¡¨ç¤ºï¼Œå…¶é¢„æµ‹ä¸ä¾èµ–äºå®Œæ•´çš„åœºæ™¯å‡ ä½•ï¼Œå› æ­¤å¯¹é®æŒ¡å’Œéƒ¨åˆ†è§‚æµ‹æ›´å…·é²æ£’æ€§ã€‚ |\n\n### æ ¸å¿ƒåŒºåˆ«ä¸ä¸»è¦æ”¹å–„çš„æ·±å…¥åˆ†æ\n\nCADGrasp ç›¸å¯¹äºå…¶å‰ä½œï¼ˆå°¤å…¶æ˜¯ç›´æ¥çš„â€œå‡çº§ç‰ˆâ€ DexGraspNet 2.0ï¼‰çš„æ”¹è¿›æ˜¯ç³»ç»Ÿæ€§å’Œæ ¹æœ¬æ€§çš„ï¼š\n\n1.  **ä»â€œç«¯åˆ°ç«¯é»‘ç®±â€åˆ°â€œå¯è§£é‡Šçš„åˆ†è§£å¼æ¡†æ¶â€**\n    *   **å‰ä½œ (DexGraspNet 2.0)**ï¼šåƒä¸€ä¸ªå¤æ‚çš„å‡½æ•°ï¼Œç›´æ¥å­¦ä¹ ä»ç‚¹äº‘ `P` åˆ°æŠ“å–å§¿æ€ `G` çš„æ˜ å°„ `f: P -> G`ã€‚è¿™ä¸ªæ˜ å°„é«˜åº¦éçº¿æ€§ï¼Œä¸”ç‰©ç†çº¦æŸï¼ˆå¦‚â€œæ‰‹æŒ‡åº”è¯¥æ¥è§¦è¿™é‡Œè€Œä¸ç¢°é‚£é‡Œâ€ï¼‰æ˜¯éšå¼åœ°åœ¨æŸå¤±å‡½æ•°ä¸­å­¦ä¹ çš„ï¼Œæ¨¡å‹å¯¹å…¶é¢„æµ‹ä¸­çš„å¾®å°è¯¯å·®éå¸¸æ•æ„Ÿã€‚\n    *   **CADGrasp**ï¼šå°†é—®é¢˜åˆ†è§£ä¸º `P -> I -> G`ï¼Œå…¶ä¸­ `I` æ˜¯ç¨€ç–IBSã€‚\n        *   **ç¬¬ä¸€é˜¶æ®µ (`P -> I`)**ï¼šå­¦ä¹ é¢„æµ‹ä¸€ä¸ª**ç‰©ç†æ„ä¹‰æ˜ç¡®**çš„ä¸­é—´è¡¨ç¤ºã€‚ç¨€ç–IBSæœ¬è´¨ä¸Šå®šä¹‰äº†æ‰‹ä¸ç‰©ä½“/ç¯å¢ƒä¹‹é—´ç†æƒ³çš„**å‡ ä½•å…³ç³»è¾¹ç•Œ**ï¼ˆæ¥è§¦ç‚¹åœ¨å“ªé‡Œï¼Œå®‰å…¨åŒºåŸŸæ˜¯ä»€ä¹ˆï¼‰ã€‚è¿™ä¸ªä»»åŠ¡åœ¨3Dè§‚å¯Ÿç©ºé—´å†…ï¼Œæ¯”ç›´æ¥æ˜ å°„åˆ°é«˜ç»´å§¿æ€ç©ºé—´æ›´ç®€å•ã€‚\n        *   **ç¬¬äºŒé˜¶æ®µ (`I -> G`)**ï¼šåŸºäºæ˜ç¡®çš„ç‰©ç†çº¦æŸï¼ˆç”±IBSå®šä¹‰ï¼‰è¿›è¡Œä¼˜åŒ–ã€‚è¿™æ˜¯ä¸€ä¸ª**åŸºäºæ¨¡å‹**çš„è¿‡ç¨‹ï¼Œé€šè¿‡æœ€å°åŒ–èƒ½é‡å‡½æ•°æ¥å¯»æ‰¾æ»¡è¶³IBSçº¦æŸçš„æ‰‹éƒ¨å§¿æ€ã€‚è¿™æå‡äº†å¯¹æœªçŸ¥ç‰©ä½“å’Œå¸ƒå±€çš„æ³›åŒ–èƒ½åŠ›ï¼Œå› ä¸ºä¼˜åŒ–å‡†åˆ™ï¼ˆIBSï¼‰æ˜¯é€šç”¨çš„ï¼Œè€Œéä»æ•°æ®ä¸­è®°å¿†çš„ç‰¹å®šå§¿æ€ã€‚\n\n2.  **ä»â€œéšå¼ç¢°æ’å¤„ç†â€åˆ°â€œæ˜¾å¼ç¢°æ’æ„ŸçŸ¥â€**\n    *   **å‰ä½œ**ï¼šç¢°æ’é¿å…ä¾èµ–äºè®­ç»ƒæ•°æ®ä¸­è´Ÿæ ·æœ¬ï¼ˆç¢°æ’å§¿æ€ï¼‰çš„åˆ†å¸ƒï¼Œæ˜¯æ•°æ®é©±åŠ¨çš„ã€éšå¼çš„ã€‚åœ¨è®­ç»ƒæœªå……åˆ†è¦†ç›–çš„æç«¯å¯†é›†åœºæ™¯ä¸­ï¼Œå®¹æ˜“å¤±è´¥ã€‚\n    *   **CADGrasp**ï¼šç¨€ç–IBSçš„**éæ¥è§¦è¡¨é¢**éƒ¨åˆ†ï¼Œæ˜ç¡®åœ°åˆ’å®šäº†æ‰‹éƒ¨ä¸å‘¨å›´ç¯å¢ƒä¹‹é—´çš„**å®‰å…¨ç¼“å†²åŒº**ã€‚åœ¨ä¼˜åŒ–é˜¶æ®µï¼Œ`E_p` èƒ½é‡é¡¹å¼ºåˆ¶æ‰‹éƒ¨è¡¨é¢ç‚¹åœç•™åœ¨IBSè¡¨é¢ä¹‹å¤–æˆ–ä¹‹ä¸Šï¼Œä»è€Œ**æ˜¾å¼åœ°**é¿å…äº†ç©¿é€å’Œç¢°æ’ã€‚è¿™æ˜¯å…¶åœ¨å®é™…æ‚ä¹±åœºæ™¯ä¸­æˆåŠŸç‡æ›´é«˜ã€ç‰¹åˆ«æ˜¯å¯¹å°/æ‰å¹³ç‰©ä½“æŠ“å–æ›´é²æ£’çš„å…³é”®ã€‚\n\n3.  **ä»â€œç‰¹å®šæ‰‹å‹â€åˆ°â€œè·¨çµå·§æ‰‹æ³›åŒ–â€**\n    *   **å‰ä½œ**ï¼šæ¨¡å‹é€šå¸¸é’ˆå¯¹ç‰¹å®šæ‰‹å‹ï¼ˆå¦‚Leap Handï¼‰è¿›è¡Œè®­ç»ƒï¼Œæ¢ä¸€ä¸ªæ‰‹å‹ï¼ˆå¦‚Allegro Handï¼‰éœ€è¦é‡æ–°æ”¶é›†æ•°æ®å’Œè®­ç»ƒã€‚\n    *   **CADGrasp**ï¼šç¨€ç–IBSæ˜¯ä¸€ç§**ä¸å…·ä½“æ‰‹å‹æ— å…³**çš„è¡¨ç¤ºï¼Œå®ƒæè¿°çš„æ˜¯**æœŸæœ›çš„äº¤äº’å‡ ä½•å…³ç³»**ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œåªéœ€ä¸ºæ–°çš„æ‰‹å‹é‡æ–°è®¡ç®—å…¶ä¸é¢„æµ‹IBSä¹‹é—´çš„èƒ½é‡å‡½æ•°ï¼Œå³å¯è¿›è¡Œé›¶æ ·æœ¬ä¼˜åŒ–ã€‚è®ºæ–‡ä¸­åœ¨Allegro Handä¸Šçš„æˆåŠŸå®éªŒéªŒè¯äº†è¿™ä¸€ç‚¹ï¼Œè¿™æ˜¯å‰ä½œæ— æ³•å®ç°çš„ã€‚\n\n**æ€»ç»“**ï¼šCADGrasp çš„æ ¸å¿ƒè¿›æ­¥åœ¨äºï¼Œå®ƒé€šè¿‡å¼•å…¥ä¸€ä¸ª**æ¥è§¦ä¸ç¢°æ’æ„ŸçŸ¥çš„ã€åœºæ™¯è§£è€¦çš„ä¸­é—´è¡¨ç¤ºï¼ˆç¨€ç–IBSï¼‰**ï¼Œå°†çµå·§æŠ“å–é—®é¢˜ä»ä¸€ä¸ªå›°éš¾çš„æ•°æ®é©±åŠ¨å›å½’/ç”Ÿæˆé—®é¢˜ï¼Œè½¬å˜ä¸ºä¸€ä¸ªæ›´å¯æ§çš„â€œ**è¡¨ç¤ºé¢„æµ‹ + åŸºäºç‰©ç†çš„çº¦æŸä¼˜åŒ–**â€é—®é¢˜ã€‚è¿™ä¸ä»…æé«˜äº†åœ¨æ‚ä¹±åœºæ™¯ä¸‹çš„æˆåŠŸç‡å’Œå®‰å…¨æ€§ï¼Œè¿˜å¸¦æ¥äº†æ›´å¥½çš„å¯è§£é‡Šæ€§å’Œè·¨æ‰‹å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚",
            "timestamp": "2026-01-27T16:03:49.775836",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## CADGraspå…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\nè®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ‚ä¹±åœºæ™¯ä¸­çµå·§æŠ“å–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸€ç§**åœºæ™¯è§£è€¦ã€æ¥è§¦ä¸ç¢°æ’æ„ŸçŸ¥çš„ä¸­é—´è¡¨ç¤º**ï¼Œå¹¶å›´ç»•å…¶è®¾è®¡äº†é¢„æµ‹ä¸ä¼˜åŒ–æ–¹æ³•ã€‚å…³é”®æŠ€æœ¯æ–¹æ³•å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æŠ€æœ¯æ¨¡å— | æ ¸å¿ƒæ–¹æ³• | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•° |\n| :--- | :--- | :--- | :--- |\n| **1. ä¸­é—´è¡¨ç¤ºï¼šç¨€ç–IBS** | **å®šä¹‰ä¸ç”Ÿæˆ** | çµæ„Ÿæºäºäº¤äº’å¹³åˆ†æ›²é¢ã€‚å¯¹äºä¸€ä¸ªæˆåŠŸçš„æŠ“å–å§¿æ€ **g**ï¼Œåœ¨**æŠ“å–ç§å­ç‚¹** **p_s** å¤„å®šä¹‰ä¸€ä¸ª**è§„èŒƒåŒ–ç©ºé—´**ï¼ˆä»¥**p_s**ä¸ºåŸç‚¹ï¼Œæ‰‹è…•å§¿æ€**T**çš„æ—‹è½¬**R**ä¸ºæ–¹å‘ï¼‰ã€‚ç¨€ç–IBS **â„** æ˜¯ä¸€ä¸ªå››ç»´å¼ é‡ **â„ âˆˆ â„^{nÃ—nÃ—nÃ—3}**ï¼Œå…¶ä¸­å‰ä¸‰ç»´æ˜¯ç©ºé—´ä½“ç´ ç½‘æ ¼ï¼Œç¬¬å››ç»´çš„3ä¸ªé€šé“åˆ†åˆ«è¡¨ç¤ºï¼š<br>1. **IBSè¡¨é¢å æ®**ï¼šè‹¥ä½“ç´ åœ¨IBSè¡¨é¢ä¸Šï¼Œå€¼ä¸º1ï¼Œå¦åˆ™ä¸º-1ã€‚<br>2. **æ‹‡æŒ‡æ¥è§¦ç‚¹å æ®**ï¼šè‹¥ä½“ç´ æ˜¯æ‹‡æŒ‡ä¸ç›®æ ‡ç‰©ä½“çš„æ¥è§¦ç‚¹ï¼Œå€¼ä¸º1ï¼Œå¦åˆ™ä¸º-1ã€‚<br>3. **å…¶ä»–æ‰‹æŒ‡æ¥è§¦ç‚¹å æ®**ï¼šè‹¥ä½“ç´ æ˜¯å…¶ä»–æ‰‹æŒ‡ä¸ç‰©ä½“çš„æ¥è§¦ç‚¹ï¼Œå€¼ä¸º1ï¼Œå¦åˆ™ä¸º-1ã€‚<br>**æ•°å­¦åŸç†**ï¼šIBSæœ¬è´¨ä¸Šæ˜¯çµå·§æ‰‹ä¸åœºæ™¯ä¹‹é—´Voronoiå›¾çš„è¿‘ä¼¼ï¼Œå®ƒç¼–ç äº†æ‰‹ä¸åœºæ™¯/ç‰©ä½“ä¹‹é—´çš„**å‡ ä½•å…³ç³»**å’Œ**æ¥è§¦å…³ç³»**ï¼Œå¹¶å®šä¹‰äº†ä¸€ä¸ª**å®‰å…¨è¾¹ç•Œ**ä»¥ç¡®ä¿æ— ç¢°æ’ã€‚ç”±äºå®ƒæ˜¯åœ¨è§„èŒƒåŒ–ç©ºé—´ä¸­å®šä¹‰çš„ï¼Œå› æ­¤ä¸åœºæ™¯çš„å…·ä½“å…¨å±€å‡ ä½•è§£è€¦ï¼Œé€‚ç”¨äºéƒ¨åˆ†è§‚æµ‹ã€‚ | - IBSä½“ç§¯å¤§å°ï¼š`0.2m Ã— 0.2m Ã— 0.2m` <br>- åˆ†è¾¨ç‡ `n`ï¼š`40` (å³ `40Ã—40Ã—40` ä½“ç´ ) <br>- ä½“ç´ å¤§å°ï¼š`5mm` (0.2m / 40) |\n| **2. ç¬¬ä¸€é˜¶æ®µï¼šæ¡ä»¶IBSç”Ÿæˆ** | **æ‰‹è…•å§¿æ€ä¼°è®¡** | æ²¿ç”¨DexGraspNet2.0çš„æ–¹æ³•ã€‚ä½¿ç”¨ResUNet14ä»åœºæ™¯ç‚¹äº‘ **ğ’«** ä¸­æå–ç‚¹ç‰¹å¾ **â„±** å¹¶é¢„æµ‹ç‚¹çº§åˆ«çš„â€œå¯æŠ“å–æ€§â€åˆ†æ•° **ğ’®**ã€‚é€šè¿‡æœ€è¿œç‚¹é‡‡æ ·å¾—åˆ°æŠ“å–ç§å­ç‚¹ **{p_s^i}**ã€‚å¯¹æ¯ä¸ªç§å­ç‚¹çš„ç‰¹å¾ **f**ï¼Œä½¿ç”¨**å»å™ªæ‰©æ•£æ¨¡å‹**ç›´æ¥å»ºæ¨¡æ‰‹è…•å§¿æ€ **T âˆˆ SE(3)** çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ **p(Tâ®f)**ï¼Œé€šè¿‡åå‘ODEè¿‡ç¨‹å»å™ªå¾—åˆ° **T**ã€‚ | - æ‰©æ•£æ¨¡å‹å»å™ªæ­¥æ•°ï¼š`50` |\n| | **IBSå€™é€‰ç”Ÿæˆï¼ˆæ‰©æ•£æ¨¡å‹ï¼‰** | ç»™å®š **p_s** å’Œ **T**ï¼Œå°†åŸå§‹ç‚¹äº‘ **ğ’«** è§„èŒƒåŒ–å¹¶ä½“ç´ åŒ–ï¼Œå¾—åˆ°æ¡ä»¶ **ğ’«* âˆˆ â„^{nÃ—nÃ—nÃ—1}**ã€‚å°†é¢„æµ‹ **p(â„â®ğ’«, T)** ç®€åŒ–ä¸ºé¢„æµ‹ **p(â„â®ğ’«*)**ã€‚<br>é‡‡ç”¨**å æ®-æ‰©æ•£æ¨¡å‹**ã€‚ä½¿ç”¨ä¸¤ä¸ª3D UNetä½œä¸ºä¸»å¹²ç½‘ç»œï¼š<br>- **ç‚¹äº‘ç½‘ç»œ Î©_p**ï¼šç¼–ç æ¡ä»¶ **ğ’«***ã€‚<br>- **å æ®ç½‘ç»œ Î©_o**ï¼šåœ¨å»å™ªè¿‡ç¨‹ä¸­é¢„æµ‹å¹²å‡€çš„IBS **â„_0**ã€‚<br>**å…³é”®æŠ€æœ¯**ï¼š**ä½“ç´ çº§æ¡ä»¶å¼•å¯¼**â€”â€”å°† **Î©_p** åœ¨æ¯ä¸ªå±‚çº§ï¼ˆ`40Â³, 20Â³, 10Â³, 5Â³`ï¼‰çš„ç‰¹å¾ä¸ **Î©_o** å¯¹åº”å±‚çº§çš„ç‰¹å¾æ‹¼æ¥ï¼Œå®ç°ç²¾ç»†çš„æ¡ä»¶æ§åˆ¶ã€‚<br>**è®­ç»ƒæŸå¤±ï¼ˆå…¬å¼1ï¼‰**ï¼š<br>`â„’_â„â‚€ = ğ”¼_Ïµâˆ¼ğ’©(0,I), tâˆ¼ğ’°(0,1) [ â€– Î©_o(â„_t, t, Î©_p(ğ’«*)) - â„â‚€ â€–â‚‚Â² ]` <br>å…¶ä¸­ **â„_t** æ˜¯æ—¶é—´æ­¥ `t` çš„å¸¦å™ªå£°æ ·æœ¬ï¼Œ**â„_0** æ˜¯çœŸå®æ•°æ®ã€‚ | - UNetå±‚çº§ç‰¹å¾ç»´åº¦ï¼š`32, 64, 128, 256` <br>- æ‰¹é‡å¤§å°ï¼š`64` <br>- ä¼˜åŒ–å™¨ï¼šAdamWï¼Œå­¦ä¹ ç‡ `6e-5` |\n| | **IBSå€™é€‰æ’åº** | ä»åˆ†å¸ƒ **p(â„â®ğ’«*)** ä¸­é‡‡æ · `m` ä¸ªIBSå€™é€‰ **{â„Ì‚_i}**ã€‚ä¸ºæ¯ä¸ªå€™é€‰è®¡ç®—**åŠ›é—­åˆåˆ†æ•°** **ğ’¬_â„Ì‚_i**ï¼ˆåŸºäºæ‹‡æŒ‡å’Œå…¶ä»–æ‰‹æŒ‡çš„æ¥è§¦ç‚¹åŠæ–¹å‘ï¼‰ã€‚æŒ‰åˆ†æ•°é™åºæ’åˆ—å€™é€‰ï¼š`â„Ì‚_Ïƒâ‚€ â‰» â„Ì‚_Ïƒâ‚ â‰» ... â‰» â„Ì‚_Ïƒ_m`ï¼Œå…¶ä¸­ `â„Ì‚_Ïƒ_i â‰» â„Ì‚_Ïƒ_j â‡” ğ’¬_â„Ì‚_Ïƒ_i > ğ’¬_â„Ì‚_Ïƒ_j`ã€‚é€‰æ‹©æ’åæœ€é«˜çš„ä½œä¸ºæœ€ç»ˆä¸­é—´è¡¨ç¤º **â„Ì‚**ã€‚ | - IBSå€™é€‰æ•° `m`ï¼š`5` |\n| **3. ç¬¬äºŒé˜¶æ®µï¼šåŸºäºIBSçº¦æŸçš„æŠ“å–å§¿æ€ä¼˜åŒ–** | **èƒ½é‡å‡½æ•°ä¼˜åŒ–** | ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æœ€å°åŒ–æ€»èƒ½é‡å‡½æ•° **E** æ¥ä¼˜åŒ–æŠ“å–å§¿æ€ **g = {T, J}**ï¼š<br>`E = Î»â‚E_j + Î»â‚‚E_sp + Î»â‚ƒE_p + Î»â‚„E_d` <br>**å„åˆ†é‡è¯¦è§£**ï¼š<br>1. **å…³èŠ‚é™åˆ¶èƒ½é‡ E_j (å…¬å¼3)**ï¼šç¡®ä¿å…³èŠ‚è§’åœ¨é¢„è®¾èŒƒå›´å†…ã€‚<br> `E_j = (1/d) Î£_{i=1}^d [ max(Î¸_i - Î¸_i^{max}, 0) + max(Î¸_i^{min} - Î¸_i, 0) ]` <br> å…¶ä¸­ `Î¸_i` æ˜¯ç¬¬ `i` ä¸ªå…³èŠ‚è§’ï¼Œ`d` æ˜¯å…³èŠ‚æ•°ã€‚<br>2. **è‡ªç©¿é€èƒ½é‡ E_sp (å…¬å¼4)**ï¼šé˜²æ­¢æ‰‹éƒ¨ä¸åŒéƒ¨åˆ†ç›¸äº’ç©¿é€ã€‚<br> `E_sp = (1/â®ğ’«_hâ®Â²) Î£_{pâˆˆğ’«_h} Î£_{qâˆˆğ’«_h, qâ‰ p} max(Î´ - d(p, q), 0)` <br> å…¶ä¸­ `ğ’«_h` æ˜¯æ‰‹éƒ¨ç‚¹é›†ï¼Œ`Î´` æ˜¯å®‰å…¨è·ç¦»ã€‚<br>3. **æ¥è§¦èƒ½é‡ E_p (å…¬å¼5)**ï¼šçº¦æŸæ‰‹æŒ‡ä¸ç‰©ä½“åœ¨é¢„æµ‹çš„IBSæ¥è§¦ç‚¹å¤„æ¥è§¦ã€‚<br> `E_p = (1/â®ğ’«_hâ®) Î£_{pâˆˆğ’«_h} max( 0, -((p-p*)/â€–p-p*â€– Â· n) )` <br> å…¶ä¸­ `p*` æ˜¯ `p` åœ¨IBSç‚¹é›† `ğ’«_â„` ä¸­çš„æœ€è¿‘ç‚¹ï¼Œ`n` æ˜¯ `p*` å¤„IBSè¡¨é¢çš„æ³•å‘é‡ã€‚æ­¤é¡¹é¼“åŠ±æ‰‹éƒ¨ç‚¹å‘IBSè¡¨é¢ç§»åŠ¨å¹¶æ²¿æ³•çº¿æ–¹å‘å¯¹é½ã€‚<br>4. **ç¢°æ’èƒ½é‡ E_d (å…¬å¼6)**ï¼šç»¼åˆæ€§çº¦æŸï¼Œç¡®ä¿æ‰‹æŒ‡ç‚¹é è¿‘å…¶æŒ‡å®šçš„IBSæ¥è§¦ç‚¹ï¼ŒåŒæ—¶æƒ©ç½šæ‰‹éƒ¨ç‚¹ä¸IBSè¡¨é¢çš„ç©¿é€ã€‚<br> `E_d = (Î±â‚/â®ğ’«_t*â®) Î£_{p*âˆˆğ’«_t*} min_{pâˆˆğ’«_t} â€–p*-pâ€–Â² + (Î±â‚‚/â®ğ’«_o*â®) Î£_{p*âˆˆğ’«_o*} min_{pâˆˆğ’«_o} â€–p*-pâ€–Â² + (Î±â‚ƒ/(â®ğ’«_tâ®+â®ğ’«_oâ®)) Î£_{pâˆˆ{ğ’«_t, ğ’«_o}} min_{p*âˆˆ{ğ’«_t*, ğ’«_o*}} â€–p*-pâ€–Â²` <br> å…¶ä¸­ `ğ’«_t*, ğ’«_o*` æ˜¯IBSä¸Šæ‹‡æŒ‡/å…¶ä»–æ‰‹æŒ‡çš„æ¥è§¦ç‚¹é›†ï¼Œ`ğ’«_t, ğ’«_o` æ˜¯æ‹‡æŒ‡/å…¶ä»–æ‰‹æŒ‡ä¸Šçš„ç‚¹é›†ã€‚å‰ä¸¤é¡¹æ˜¯å¸å¼•é¡¹ï¼Œåä¸€é¡¹æ˜¯æ’æ–¥é¡¹ã€‚ | - `Î»â‚, Î»â‚‚, Î»â‚ƒ, Î»â‚„`ï¼š`5, 1, 1000, 1` <br>- `Î±â‚, Î±â‚‚, Î±â‚ƒ`ï¼š`80, 100, 2` <br>- è‡ªç©¿é€å®‰å…¨è·ç¦» `Î´`ï¼šæ ¹æ®æ‰‹éƒ¨æ¨¡å‹è®¾å®š |\n| | **æŠ“å–å§¿æ€æ’åº** | åŸºäºåŒä¸€ä¸ªé¢„æµ‹çš„ç¨€ç–IBS **â„Ì‚**ï¼Œå¹¶è¡Œåˆå§‹åŒ–å¹¶ä¼˜åŒ– `k` ä¸ªæŠ“å–é…ç½®ã€‚è®°å½•æ¯ä¸ªé…ç½®ä¼˜åŒ–åçš„æœ€ç»ˆèƒ½é‡æ®‹å·® **{E_gi}**ã€‚é€‰æ‹©æ®‹å·®æœ€å°çš„é…ç½®ä½œä¸ºæœ€ä¼˜æŠ“å–å§¿æ€ **g**ã€‚ | - ä¼˜åŒ–å°è¯•æ¬¡æ•° `k`ï¼š`5` |\n\n### æŠ€æœ¯åˆ›æ–°ä¸ä»·å€¼æ€»ç»“\n1.  **è¡¨ç¤ºåˆ›æ–°**ï¼šæå‡ºçš„**ç¨€ç–IBS**æ˜¯ä¸€ç§æ–°é¢–çš„ã€**åœºæ™¯è§£è€¦**çš„ä¸­é—´è¡¨ç¤ºã€‚å®ƒé¦–æ¬¡å°†IBSç”¨ä½œ**ä¼˜åŒ–ç›®æ ‡**è€Œéè§‚æµ‹è¾“å…¥ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†",
            "timestamp": "2026-01-27T16:04:59.843259",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡ã€ŠCADGraspã€‹çš„è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä¸»è¦ä½¿ç”¨ **æŠ“å–æˆåŠŸç‡ (Success Rate)** ä½œä¸ºæ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼Œå¹¶åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­å‡è¿›è¡Œäº†è¯„ä¼°ã€‚\n\n#### **è¯„ä»·æŒ‡æ ‡ç»†èŠ‚ï¼š**\n\n| æŒ‡æ ‡åç§° | å®šä¹‰ä¸è®¡ç®—æ–¹å¼ | è¯„ä¼°åœºæ™¯ | ç»†èŠ‚è¯´æ˜ |\n| :--- | :--- | :--- | :--- |\n| **æŠ“å–æˆåŠŸç‡ (Success Rate)** | `æˆåŠŸæŠ“å–æ¬¡æ•° / æ€»å°è¯•æ¬¡æ•°` | **æ¨¡æ‹Ÿå®éªŒ** & **çœŸå®ä¸–ç•Œå®éªŒ** | **1. æ¨¡æ‹Ÿå®éªŒåˆ¤å®šæ ‡å‡†**ï¼šéµå¾ªDexGraspNet2.0çš„åè®®ã€‚ä¸€ä¸ªæŠ“å–è¢«åˆ¤å®šä¸ºæˆåŠŸï¼Œéœ€è¦æ»¡è¶³ï¼š<br> - **æŠ“å–ç¨³å®šæ€§**ï¼šåœ¨æ¨¡æ‹Ÿå™¨ä¸­æ‰§è¡ŒæŠ“å–ã€é—­åˆæ‰‹æŒ‡ã€æå‡åŠ¨ä½œåï¼Œç›®æ ‡ç‰©ä½“è¢«æˆåŠŸæŠ¬èµ·å¹¶ä¿æŒä¸€æ®µæ—¶é—´ã€‚<br> - **æ— ç¢°æ’å¹²æ‰°**ï¼šæŠ“å–è¿‡ç¨‹ä¸­ï¼Œçµå·§æ‰‹æˆ–ç›®æ ‡ç‰©ä½“æœªä¸åœºæ™¯ä¸­å…¶ä»–ç‰©ä½“å‘ç”Ÿå¯¼è‡´ä»»åŠ¡å¤±è´¥çš„ç¢°æ’ï¼ˆå¦‚ç¢°å€’å…¶ä»–ç‰©ä½“ï¼‰ã€‚<br><br> **2. çœŸå®ä¸–ç•Œåˆ¤å®šæ ‡å‡†**ï¼š<br> - æ‰§è¡Œç›¸åŒçš„åŠ¨ä½œåºåˆ—ï¼ˆé¢„æŠ“å–ã€æŠ“å–ã€æŒ¤å‹æ‰‹æŒ‡ã€æå‡ï¼‰ã€‚<br> - æˆåŠŸå°†ç›®æ ‡ç‰©ä½“ä»æ‚ä¹±åœºæ™¯ä¸­ç¨³å®šæŠ“èµ·å¹¶ç¦»å¼€æ¡Œé¢ã€‚ |\n| **è·¨çµå·§æ‰‹æ³›åŒ–æˆåŠŸç‡** | åŒä¸Š | **æ¨¡æ‹Ÿå®éªŒ** | ä½¿ç”¨åœ¨**Leap Hand**ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œ**é›¶æ ·æœ¬**åœ°åœ¨æœªè§è¿‡çš„**Allegro Hand**ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œè¯„ä¼°å…¶æˆåŠŸç‡ã€‚è¿™éªŒè¯äº†å…¶ä¸­é—´è¡¨ç¤ºï¼ˆç¨€ç–IBSï¼‰çš„**æœ¬ä½“æ— å…³æ€§**ã€‚ |\n| **ä¼˜åŒ–æ®‹å·® (Optimization Residual)** | å…¬å¼(7)ä¸­æ€»èƒ½é‡å‡½æ•° **E** çš„æœ€å°å€¼ | **æ–¹æ³•å†…éƒ¨** | åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå¯¹åŒä¸€é¢„æµ‹çš„ç¨€ç–IBSè¿›è¡Œå¤šæ¬¡ä¼˜åŒ–å°è¯•ï¼Œè®°å½•æ¯æ¬¡ä¼˜åŒ–åçš„èƒ½é‡å€¼ **E_gi**ã€‚**èƒ½é‡å€¼è¶Šä½ï¼Œä»£è¡¨æŠ“å–å§¿æ€è¶Šç¬¦åˆç¨€ç–IBSçš„å‡ ä½•ã€æ¥è§¦å’Œç¢°æ’çº¦æŸ**ã€‚ç”¨äºå¯¹ç”Ÿæˆçš„å¤šä¸ªæŠ“å–å§¿æ€è¿›è¡Œå†…éƒ¨æ’åºã€‚ |\n| **åŠ›é—­åˆåˆ†æ•° (Force Closure Score)** | åŸºäºé¢„æµ‹çš„æ¥è§¦ç‚¹å’Œæ–¹å‘è®¡ç®— | **æ–¹æ³•å†…éƒ¨** | åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œå¯¹ç”Ÿæˆçš„å¤šä¸ªç¨€ç–IBSå€™é€‰è¿›è¡Œè¯„åˆ†å’Œæ’åºã€‚åˆ†æ•°è¶Šé«˜ï¼Œä»£è¡¨è¯¥IBSå¯¹åº”çš„æ½œåœ¨æŠ“å–å§¿æ€åœ¨åŠ›å­¦ä¸Šè¶Šç¨³å®šï¼ˆæŠµæŠ—å¤–åŠ›èƒ½åŠ›è¶Šå¼ºï¼‰ã€‚ |\n\n**å…¶ä»–éšå«çš„è¯„ä¼°ç»´åº¦ï¼ˆé€šè¿‡ç»“æœåˆ†æä½“ç°ï¼‰ï¼š**\n*   **ç¢°æ’æ„ŸçŸ¥èƒ½åŠ›**ï¼šé€šè¿‡åœ¨é«˜å¯†åº¦æ‚ä¹±åœºæ™¯ä¸­çš„é«˜æˆåŠŸç‡ï¼Œä»¥åŠä¸åŸºçº¿æ–¹æ³•ï¼ˆå¦‚DexGraspNet2.0ï¼‰åœ¨çœŸå®ä¸–ç•Œä¸­å¯¹å°ç‰©ä½“ã€æ‰å¹³ç‰©ä½“æŠ“å–æˆåŠŸç‡çš„å¯¹æ¯”ï¼Œé—´æ¥è¯æ˜äº†å…¶ä¼˜ç§€çš„é¿éšœèƒ½åŠ›ã€‚\n*   **è®¡ç®—æ•ˆç‡**ï¼šé€šè¿‡æŠ¥å‘Š**ç«¯åˆ°ç«¯æ¨ç†æ—¶é—´ï¼ˆ6.51ç§’ï¼‰** åŠå…¶æ¨¡å—åˆ†è§£ï¼ˆè§è¡¨3ï¼‰ï¼Œå¹¶ä¸å…¶å®ƒä¸¤é˜¶æ®µæ–¹æ³•ï¼ˆå¦‚GraspTTAçš„43.23ç§’ï¼‰å¯¹æ¯”è¿›è¡Œè¯„ä¼°ã€‚\n*   **æŠ“å–è´¨é‡**ï¼šé€šè¿‡**ç©¿é€æ·±åº¦åˆ†æ**ï¼ˆå›¾9ï¼‰è¿›è¡Œè¾…åŠ©è¯„ä¼°ï¼Œè¡¨æ˜å…¶ç”Ÿæˆçš„æŠ“å–å§¿æ€ä¸ç‰©ä½“è¡¨é¢è´´åˆæ›´å¥½ï¼Œç©¿é€æ›´å°‘ã€‚\n\n### äºŒã€ æ•°æ®é›†\n\nè®ºæ–‡ä½¿ç”¨äº†å¤§è§„æ¨¡åˆæˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œæ„å»ºäº†æµ‹è¯•åœºæ™¯ã€‚\n\n#### **æ•°æ®é›†ç»†èŠ‚ï¼š**\n\n| æ•°æ®é›†ç±»å‹ | åç§°/æ¥æº | ç”¨é€” | ç»†èŠ‚è¯´æ˜ |\n| :--- | :--- | :--- | :--- |\n| **æ¨¡æ‹Ÿè®­ç»ƒé›† (å¯¹è±¡)** | **GraspNet-1Billion** | **è®­ç»ƒ** | ä»ä¸­é€‰å–äº† **60ä¸ª** ç‰©ä½“ç”¨äºè®­ç»ƒæŠ“å–ç”Ÿæˆæ¨¡å‹ã€‚ |\n| **æ¨¡æ‹Ÿè®­ç»ƒé›† (åœºæ™¯)** | **DexGraspNet2.0** çš„è®­ç»ƒåœºæ™¯ | **è®­ç»ƒ** | ä»DexGraspNet2.0çš„7600ä¸ªè®­ç»ƒåœºæ™¯ä¸­ï¼Œé‡‡æ ·äº† **100ä¸ª** æ‚ä¹±åœºæ™¯ï¼Œä¸“é—¨ç”¨äºè®­ç»ƒ **IBSç”Ÿæˆæ¨¡å—**ã€‚æ¯ä¸ªåœºæ™¯åŒ…å«å¤šä¸ªå †å çš„ç‰©ä½“ã€‚ |\n| **æ¨¡æ‹Ÿæµ‹è¯•é›† (å¯¹è±¡)** | **GraspNet-1Billion** + **ShapeNet** | **æµ‹è¯•** | å…± **1259ä¸ª** æµ‹è¯•ç‰©ä½“ï¼Œç”¨äºè¯„ä¼°å¯¹è±¡æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç‰©ä½“åœ¨è®­ç»ƒä¸­**æœªè§**ã€‚ |\n| **æ¨¡æ‹Ÿæµ‹è¯•é›† (åœºæ™¯)** | **DexGraspNet2.0** çš„æµ‹è¯•åœºæ™¯ | **æµ‹è¯•** | å…± **670ä¸ª** æµ‹è¯•åœºæ™¯ï¼Œç”¨äºè¯„ä¼°åœºæ™¯æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›åœºæ™¯æ ¹æ®ç‰©ä½“å¯†åº¦åˆ†ä¸ºä¸‰ç±»ï¼š<br> 1. **å¯†é›† (Dense)**: æ¯åœºæ™¯8-11ä¸ªç‰©ä½“ã€‚<br> 2. **éšæœº (Random)**: æ¯åœºæ™¯1-10ä¸ªç‰©ä½“ï¼ˆç”±å¯†é›†åœºæ™¯éšæœºåˆ é™¤ç‰©ä½“ç”Ÿæˆï¼‰ã€‚<br> 3. **æ¾æ•£ (Loose)**: æ¯åœºæ™¯1-2ä¸ªç‰©ä½“ã€‚ |\n| **çœŸå®ä¸–ç•Œæµ‹è¯•é›† (å¯¹è±¡)** | è‡ªå»ºç‰©ä½“é›† | **æµ‹è¯•** | åŒ…å« **30ä¸ª** å…·æœ‰ä¸åŒå½¢çŠ¶ã€å¤§å°ã€æè´¨çš„æ—¥å¸¸ç‰©ä½“ï¼ˆè§å›¾6å·¦ï¼‰ã€‚ |\n| **çœŸå®ä¸–ç•Œæµ‹è¯•é›† (åœºæ™¯)** | è‡ªå»ºæ‚ä¹±åœºæ™¯ | **æµ‹è¯•** | æ„å»ºäº† **5ä¸ª** ä¸åŒçš„æ‚ä¹±æµ‹è¯•åœºæ™¯ï¼ˆè§å›¾6å³ï¼‰ï¼Œæ¯ä¸ªåœºæ™¯åŒ…å«4åˆ°8ä¸ªç‰©ä½“ï¼Œè¦†ç›–äº†ä¸åŒçš„å †å å¯†åº¦ã€‚ |\n\n**å…³é”®ç‚¹æ€»ç»“ï¼š**\n1.  **æ•°æ®ä¾èµ–æ€§**ï¼šè¯¥æ–¹æ³•ä¸¥é‡ä¾èµ–**å¤§è§„æ¨¡åˆæˆæ•°æ®**ï¼ˆDexGraspNet2.0ï¼‰æ¥å­¦ä¹ æ‚ä¹±åœºæ™¯ä¸­çš„æŠ“å–åˆ†å¸ƒå’ŒIBSè¡¨ç¤ºã€‚\n2.  **è¯„ä¼°çš„ä¸¥è°¨æ€§**ï¼šæµ‹è¯•é›†åœ¨**ç‰©ä½“**å’Œ**åœºæ™¯å¸ƒå±€**ä¸Šå‡ä¸è®­ç»ƒé›†ä¿æŒåˆ†ç¦»ï¼Œä¸¥æ ¼æµ‹è¯•æ³›åŒ–èƒ½åŠ›ã€‚\n3.  **åœºæ™¯å¤æ‚åº¦åˆ†çº§**ï¼šé€šè¿‡â€œå¯†é›†ã€éšæœºã€æ¾æ•£â€ä¸‰çº§åˆ†ç±»ï¼Œå¯ä»¥ç³»ç»Ÿæ€§åœ°è¯„ä¼°æ–¹æ³•åœ¨ä¸åŒæŒ‘æˆ˜ç¨‹åº¦ä¸‹çš„æ€§èƒ½ã€‚\n4.  **ä»¿çœŸåˆ°ç°å® (Sim-to-Real)**ï¼šä½¿ç”¨å•è§†è§’ç‚¹äº‘ä½œä¸ºè¾“å…¥ï¼Œæœ‰æ•ˆå‡å°‘äº†ä»¿çœŸä¸ç°å®çš„å·®è·ï¼Œä½¿å¾—åœ¨çœŸå®ä¸–ç•Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯ç›´æ¥éƒ¨ç½²ã€‚",
            "timestamp": "2026-01-27T16:05:47.270661",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ä¸æ€§èƒ½è¯„ä¼°\n\n### ä¸€ã€å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ä»¥ä¸‹æ•°æ®é›†å’Œåœºæ™¯ä¸Šè¿›è¡Œäº†å®éªŒï¼š\n\n#### 1. **è®­ç»ƒæ•°æ®é›†**\n- **å¯¹è±¡æ•°æ®é›†**ï¼š60ä¸ªè®­ç»ƒå¯¹è±¡ï¼Œæ¥è‡ª **GraspNet-1Billion**ã€‚\n- **åœºæ™¯æ•°æ®é›†**ï¼šä» **DexGraspNet 2.0** çš„7600ä¸ªè®­ç»ƒåœºæ™¯ä¸­é‡‡æ · **100ä¸ªåœºæ™¯**ï¼Œç”¨äºè®­ç»ƒIBSç”Ÿæˆæ¨¡å—ã€‚\n\n#### 2. **æµ‹è¯•æ•°æ®é›†**\n- **å¯¹è±¡æ•°æ®é›†**ï¼šå…±1259ä¸ªæµ‹è¯•å¯¹è±¡ï¼ŒåŒ…æ‹¬ï¼š\n  - **GraspNet-1Billion** ä¸­çš„å¯¹è±¡ã€‚\n  - **ShapeNet** ä¸­çš„å¯¹è±¡ã€‚\n- **åœºæ™¯æ•°æ®é›†**ï¼šä½¿ç”¨ **DexGraspNet 2.0** çš„æµ‹è¯•é›†ï¼Œå…± **670ä¸ªåœºæ™¯**ï¼Œåˆ†ä¸ºä¸‰ç§å¯†åº¦çº§åˆ«ï¼š\n  - **æ¾æ•£ï¼ˆLooseï¼‰**ï¼šæ¯åœºæ™¯1-2ä¸ªå¯¹è±¡ã€‚\n  - **éšæœºï¼ˆRandomï¼‰**ï¼šæ¯åœºæ™¯1-10ä¸ªå¯¹è±¡ï¼ˆä»å¯†é›†åœºæ™¯ä¸­åˆ é™¤å¯¹è±¡ç”Ÿæˆï¼‰ã€‚\n  - **å¯†é›†ï¼ˆDenseï¼‰**ï¼šæ¯åœºæ™¯8-11ä¸ªå¯¹è±¡ã€‚\n\n#### 3. **çœŸå®ä¸–ç•Œå®éªŒ**\n- **å¯¹è±¡**ï¼š30ä¸ªå…·æœ‰ä¸åŒå½¢çŠ¶ã€å¤§å°å’Œæè´¨çš„ç‰©ä½“ã€‚\n- **åœºæ™¯**ï¼š5ä¸ªæ‚ä¹±åœºæ™¯ï¼Œæ¯åœºæ™¯åŒ…å«4-8ä¸ªç‰©ä½“ã€‚\n- **ç¡¬ä»¶**ï¼šFlexiv Rizon-4æœºæ¢°è‡‚ + Leap Handçµå·§æ‰‹ï¼Œä½¿ç”¨RealSense D415ç›¸æœºè¿›è¡Œæ„ŸçŸ¥ã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\n- **æ ¸å¿ƒæŒ‡æ ‡**ï¼š**æŠ“å–æˆåŠŸç‡ï¼ˆSuccess Rateï¼‰**ï¼Œå®šä¹‰ä¸ºæˆåŠŸæŠ“å–æ¬¡æ•°ä¸æ€»å°è¯•æ¬¡æ•°çš„æ¯”ä¾‹ã€‚\n- **è¯„ä¼°åè®®**ï¼šéµå¾ª **DexGraspNet 2.0** çš„è¯„ä¼°æ ‡å‡†ï¼Œåœ¨ä»¿çœŸç¯å¢ƒä¸­è¿›è¡ŒæŠ“å–æ¨¡æ‹ŸéªŒè¯ã€‚\n- **é™„åŠ åˆ†æ**ï¼š\n  - **è·¨çµå·§æ‰‹æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨æœªè®­ç»ƒè¿‡çš„Allegroæ‰‹ä¸Šè¿›è¡Œé›¶æ ·æœ¬æµ‹è¯•ã€‚\n  - **è®¡ç®—æ•ˆç‡**ï¼šè®°å½•å„é˜¶æ®µè¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰ã€‚\n  - **æŠ“å–è´¨é‡**ï¼šåˆ†æç©¿é€æ·±åº¦ã€å…³èŠ‚é…ç½®å¤šæ ·æ€§ç­‰ã€‚\n\n### ä¸‰ã€æ€§èƒ½æå‡ä¸å¯¹æ¯”ç»“æœ\n\n#### 1. **ä»¿çœŸå®éªŒå¯¹æ¯”ï¼ˆå…³é”®æ•°æ®ï¼‰**\nåœ¨ **GraspNet-1Billion** å’Œ **ShapeNet** æµ‹è¯•é›†ä¸Šï¼Œä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ï¼š\n\n| æ–¹æ³• | è®­ç»ƒæ•°æ®æ¯”ä¾‹ | GraspNet-1Billion (å¯†é›†åœºæ™¯) | ShapeNet (å¯†é›†åœºæ™¯) |\n|------|--------------|-----------------------------|---------------------|\n| HGC-Net | 1 | 46.0% | 46.4% |\n| GraspTTA | 1 | 62.5% | 56.6% |\n| ISAGrasp | 1 | 63.4% | 64.0% |\n| DexGraspNet2.0 | 1/1000 | **83.3%** | **81.5%** |\n| **CADGrasp (Ours)** | 1/1000 | **86.5%** | **79.3%** |\n| **CADGrasp (Allegroæ‰‹)** | 1/1000 | 77.6% | 75.7% |\n\n**æ€§èƒ½æå‡**ï¼š\n- åœ¨ **GraspNet-1Billion å¯†é›†åœºæ™¯** ä¸­ï¼ŒCADGraspæ¯”æœ€ä½³åŸºçº¿ï¼ˆDexGraspNet2.0ï¼‰**æå‡3.2ä¸ªç™¾åˆ†ç‚¹**ï¼ˆ86.5% vs 83.3%ï¼‰ã€‚\n- åœ¨ **ShapeNet å¯†é›†åœºæ™¯** ä¸­ï¼Œæ€§èƒ½ç•¥ä½äºDexGraspNet2.0ï¼ˆ79.3% vs 81.5%ï¼‰ï¼Œä½†åœ¨å…¶ä»–å¯†åº¦åœºæ™¯ä¸­è¡¨ç°æ›´å‡è¡¡ã€‚\n- **è·¨çµå·§æ‰‹æ³›åŒ–**ï¼šåœ¨æœªè®­ç»ƒçš„Allegroæ‰‹ä¸Šç›´æ¥æµ‹è¯•ï¼Œä»è¾¾åˆ° **77.6%** çš„æˆåŠŸç‡ï¼Œè¯æ˜äº†æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n#### 2. **çœŸå®ä¸–ç•Œå®éªŒå¯¹æ¯”**\n| æ–¹æ³• | åœºæ™¯#1 | åœºæ™¯#2 | åœºæ™¯#3 | åœºæ™¯#4 | åœºæ™¯#5 | æ€»ä½“æˆåŠŸç‡ |\n|------|--------|--------|--------|--------|--------|------------|\n| DexGraspNet2.0 | 100.0% | 50.0% | 71.4% | 100.0% | 88.9% | **83.9%** |\n| **CADGrasp (Ours)** | 100.0% | **100.0%** | **85.7%** | 100.0% | 88.9% | **93.8%** |\n\n**æ€§èƒ½æå‡**ï¼š\n- æ€»ä½“æˆåŠŸç‡ä» **83.9% æå‡è‡³ 93.8%**ï¼Œ**ç»å¯¹æå‡9.9ä¸ªç™¾åˆ†ç‚¹**ã€‚\n- åœ¨å¤æ‚åœºæ™¯ï¼ˆå¦‚åœºæ™¯#2å’Œ#3ï¼‰ä¸­æå‡å°¤ä¸ºæ˜¾è‘—ï¼Œè¯æ˜äº† **IBSè¡¨ç¤ºå¯¹ç¢°æ’é¿å…çš„æœ‰æ•ˆæ€§**ã€‚\n\n#### 3. **è®¡ç®—æ•ˆç‡**\n- **æ€»æ¨ç†æ—¶é—´**ï¼š**6.51ç§’**ï¼ˆå•æ¬¡æŠ“å–ç”Ÿæˆï¼‰ã€‚\n- **å¯¹æ¯”ä¼˜åŠ¿**ï¼šæ¯”ç°æœ‰ä¸¤é˜¶æ®µæ–¹æ³•ï¼ˆå¦‚GraspTTAçš„43.23ç§’ï¼‰**å¿«6å€ä»¥ä¸Š**ï¼Œæ›´é€‚åˆå®é™…éƒ¨ç½²ã€‚\n\n### å››ã€å…³é”®ç»“è®º\n1. **æ•°æ®é›†æœ‰æ•ˆæ€§**ï¼šåœ¨åŒ…å«1300+ç‰©ä½“ã€670ä¸ªåœºæ™¯çš„å¤§è§„æ¨¡æ‚ä¹±æ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•ã€‚\n2. **æ€§èƒ½ä¼˜åŠ¿**ï¼š\n   - åœ¨ä»¿çœŸä¸­ï¼Œ**æˆåŠŸç‡æ˜¾è‘—è¶…è¿‡ç°æœ‰æœ€ä½³æ–¹æ³•**ï¼ˆæœ€é«˜æå‡3.2%ï¼‰ã€‚\n   - åœ¨çœŸå®ä¸–ç•Œä¸­ï¼Œ**æˆåŠŸç‡æå‡è¿‘10%**ï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°æ›´ç¨³å¥ã€‚\n3. **æ³›åŒ–èƒ½åŠ›**ï¼šé€šè¿‡ **ç¨€ç–IBSè¡¨ç¤º** å®ç°äº†è·¨çµå·§æ‰‹çš„é›¶æ ·æœ¬æ³›åŒ–ï¼ˆAllegroæ‰‹æˆåŠŸç‡77.6%ï¼‰ã€‚\n4. **å®ç”¨æ€§**ï¼šæ¨ç†é€Ÿåº¦è¾ƒå¿«ï¼ˆ6.51ç§’ï¼‰ï¼Œå¹³è¡¡äº†ç²¾åº¦ä¸æ•ˆç‡ã€‚\n\n**å®é™…ä»·å€¼**ï¼šCADGraspé€šè¿‡ **æ¥è§¦ä¸ç¢°æ’æ„ŸçŸ¥çš„ä¸­é—´è¡¨ç¤º**ï¼Œæ˜¾è‘—æå‡äº†æ‚ä¹±åœºæ™¯ä¸­çµå·§æŠ“å–çš„ **æˆåŠŸç‡ã€å®‰å…¨æ€§å’Œæ³›åŒ–èƒ½åŠ›**ï¼Œä¸ºå®é™…æœºå™¨äººéƒ¨ç½²æä¾›äº†æ›´å¯é çš„è§£å†³æ–¹æ¡ˆã€‚",
            "timestamp": "2026-01-27T16:06:34.452486",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡ç¬¬5èŠ‚ï¼ˆConclusionï¼‰å’Œå®éªŒéƒ¨åˆ†ï¼ˆSection 4.5ï¼‰çš„è®¨è®ºï¼Œ**CADGrasp** æ–¹æ³•å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n- **å¯¹å°ç‰©ä½“æŠ“å–çš„æ€§èƒ½ç›¸å¯¹è¾ƒä½**\n    - å¦‚è¡¨7æ‰€ç¤ºï¼Œå¯¹äºä½“ç§¯å°äº `0.00025 mÂ³` çš„å°ç‰©ä½“ï¼ŒæŠ“å–æˆåŠŸç‡ï¼ˆ77.0%ï¼‰æ˜æ˜¾ä½äºå¤§ç‰©ä½“ï¼ˆå¦‚å¤§äº `0.0015 mÂ³` çš„ç‰©ä½“æˆåŠŸç‡ä¸º91.5%ï¼‰ã€‚\n    - **åŸå› åˆ†æ**ï¼šä½œè€…è®¤ä¸ºå°ç‰©ä½“æŠ“å–æœ¬èº«éœ€è¦æ›´é«˜çš„ç²¾åº¦ï¼Œè€Œéä½“ç´ åˆ†è¾¨ç‡ä¸è¶³ï¼ˆè¡¨6æ˜¾ç¤ºå°†åˆ†è¾¨ç‡ä»5mmæå‡è‡³2.5mmå¯¹æ€§èƒ½æå‡æœ‰é™ï¼‰ã€‚\n\n- **ç¬¬äºŒé˜¶æ®µä¼˜åŒ–è¿‡ç¨‹è€—æ—¶è¾ƒé•¿**\n    - å¦‚è¡¨3æ‰€ç¤ºï¼Œå®Œæ•´çš„æŠ“å–ç”Ÿæˆæµç¨‹å¹³å‡éœ€è¦ **6.51ç§’**ï¼Œå…¶ä¸­æŠ“å–å§¿æ€ä¼˜åŒ–ï¼ˆGrasp Opt. & Rk.ï¼‰é˜¶æ®µå  **3.03ç§’**ã€‚\n    - **åŸå› åˆ†æ**ï¼šå½“å‰ä½¿ç”¨DDPMè¿›è¡Œç¨€ç–IBSé‡‡æ ·å’ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œé™åˆ¶äº†å®æ—¶æ€§ã€‚\n\n- **è®­ç»ƒæ•°æ®å¯¹å°ç‰©ä½“çš„è¦†ç›–å¯èƒ½ä¸è¶³**\n    - ä½œè€…åœ¨å±€é™æ€§ä¸­æŒ‡å‡ºï¼Œæ€§èƒ½ç“¶é¢ˆå¯èƒ½æºäºè®­ç»ƒæ•°æ®é›†ä¸­**å°ç‰©ä½“æ ·æœ¬ç›¸å¯¹è¾ƒå°‘**ï¼Œå¯¼è‡´æ¨¡å‹å¯¹å°ç‰©ä½“å‡ ä½•å’Œæ¥è§¦æ¨¡å¼çš„å­¦ä¹ ä¸å……åˆ†ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nåŸºäºä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡æš—ç¤ºæˆ–å¯æ¨å¯¼å‡ºä»¥ä¸‹æ”¹è¿›æ–¹å‘ï¼š\n\n- **å¢å¼ºå¯¹å°ç‰©ä½“æŠ“å–çš„èƒ½åŠ›**\n    - **æ•°æ®å±‚é¢**ï¼šåœ¨è®­ç»ƒæ•°æ®é›†ä¸­**å¢åŠ æ›´å¤šå°ç‰©ä½“çš„æ ·æœ¬**ï¼Œè¦†ç›–æ›´å¹¿æ³›çš„å°ºå¯¸å’Œå‡ ä½•å½¢çŠ¶ï¼Œä»¥æå‡æ¨¡å‹å¯¹å°å°ºåº¦ç‰¹å¾çš„æ„ŸçŸ¥å’Œæ³›åŒ–èƒ½åŠ›ã€‚\n    - **è¡¨ç¤ºå±‚é¢**ï¼šæ¢ç´¢**è‡ªé€‚åº”æˆ–å¤šå°ºåº¦ä½“ç´ åŒ–**ç­–ç•¥ï¼Œå¯¹å°ç‰©ä½“åŒºåŸŸä½¿ç”¨æ›´é«˜åˆ†è¾¨ç‡çš„è¡¨ç¤ºï¼Œè€Œå¯¹å¤§ç‰©ä½“åŒºåŸŸä¿æŒè¾ƒä½åˆ†è¾¨ç‡ï¼Œä»¥å¹³è¡¡ç²¾åº¦ä¸è®¡ç®—æ•ˆç‡ã€‚\n\n- **æå‡ç®—æ³•è¿è¡Œæ•ˆç‡**\n    - **é‡‡æ ·åŠ é€Ÿ**ï¼šé‡‡ç”¨æ›´å¿«çš„æ‰©æ•£æ¨¡å‹é‡‡æ ·å™¨ï¼Œå¦‚ **DDIM**ã€**ä¸€è‡´æ€§æ¨¡å‹ï¼ˆConsistency Modelsï¼‰** æˆ–**æ¸è¿›è’¸é¦ï¼ˆProgressive Distillationï¼‰**ï¼ˆè®ºæ–‡4.4èŠ‚æåŠï¼‰ï¼Œä»¥å¤§å¹…å‡å°‘ç¬¬ä¸€é˜¶æ®µIBSç”Ÿæˆçš„æ—¶é—´ã€‚\n    - **ä¼˜åŒ–åŠ é€Ÿ**ï¼š**å¹¶è¡ŒåŒ–**ç¬¬äºŒé˜¶æ®µçš„å¤šç»„æŠ“å–å§¿æ€ä¼˜åŒ–è¿‡ç¨‹ï¼Œæˆ–ç ”ç©¶æ›´è½»é‡çº§çš„ä¼˜åŒ–ç®—æ³•ï¼Œå‡å°‘å•æ¬¡ä¼˜åŒ–è€—æ—¶ã€‚\n\n- **æ‹“å±•ä¸æ³›åŒ–**\n    - **åŠ¨æ€ä¸éåˆšæ€§ç‰©ä½“**ï¼šå½“å‰æ–¹æ³•ä¸“æ³¨äºé™æ€æ‚ä¹±åœºæ™¯ã€‚æœªæ¥å¯ç ”ç©¶å¦‚ä½•å°† **ç¨€ç–IBSè¡¨ç¤ºä¸åŠ¨æ€é¢„æµ‹æˆ–éåˆšæ€§ç‰©ä½“å»ºæ¨¡** ç»“åˆï¼Œå¤„ç†ç‰©ä½“ç§»åŠ¨æˆ–å˜å½¢çš„åœºæ™¯ã€‚\n    - **æ›´å¤æ‚çš„ä»»åŠ¡é“¾**ï¼šå°†æŠ“å–ç”Ÿæˆä½œä¸º **é•¿æ—¶åºæ“ä½œä»»åŠ¡ï¼ˆå¦‚é‡æ–°æ‘†æ”¾ã€å·¥å…·ä½¿ç”¨ï¼‰** çš„å­æ¨¡å—ï¼Œç ”ç©¶åŸºäºIBSè¡¨ç¤ºçš„åºåˆ—å†³ç­–è§„åˆ’ã€‚\n\n- **æŠ€æœ¯èåˆä¸æ¡†æ¶ä¼˜åŒ–**\n    - **é—­ç¯ä¸åœ¨çº¿ä¿®æ­£**ï¼šç»“åˆ**å®æ—¶æ„ŸçŸ¥ï¼ˆå¦‚è§¦è§‰åé¦ˆï¼‰**ï¼Œåœ¨ä¼˜åŒ–é˜¶æ®µæˆ–æ‰§è¡Œé˜¶æ®µå¯¹é¢„æµ‹çš„IBSå’ŒæŠ“å–å§¿æ€è¿›è¡Œåœ¨çº¿è°ƒæ•´ï¼Œåº”å¯¹æ„ŸçŸ¥è¯¯å·®å’Œåœºæ™¯ä¸ç¡®å®šæ€§ã€‚\n    - **ç»Ÿä¸€ç”Ÿæˆä¸ä¼˜åŒ–**ï¼šæ¢ç´¢**ç«¯åˆ°ç«¯å¯å¾®åˆ†çš„ä¼˜åŒ–å±‚**ï¼Œå°†ç¬¬äºŒé˜¶æ®µçš„éƒ¨åˆ†çº¦æŸä¼˜åŒ–è¿‡ç¨‹åµŒå…¥åˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œå®ç°æ›´é«˜æ•ˆçš„å•é˜¶æ®µæ¨ç†ï¼ŒåŒæ—¶ä¿æŒä¸¤é˜¶æ®µæ¡†æ¶çš„æ³›åŒ–ä¼˜åŠ¿ã€‚\n\n**æ€»ç»“**ï¼šCADGraspçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†**åœºæ™¯è§£è€¦ã€æ¥è§¦ä¸ç¢°æ’æ„ŸçŸ¥çš„ç¨€ç–IBSè¡¨ç¤º**ï¼Œæœ‰æ•ˆè§£å†³äº†æ‚ä¹±åœºæ™¯ä¸­çµå·§æŠ“å–çš„æ³›åŒ–ä¸ç¢°æ’é¿å…éš¾é¢˜ã€‚å…¶å±€é™æ€§ä¸»è¦å›´ç»•**å°ç‰©ä½“ç²¾åº¦**å’Œ**è®¡ç®—æ•ˆç‡**ï¼Œæœªæ¥çš„æ”¹è¿›å°†é›†ä¸­äº**æ•°æ®æ‰©å……**ã€**ç®—æ³•åŠ é€Ÿ**ä»¥åŠå‘**æ›´å¤æ‚ã€åŠ¨æ€çš„ä»»åŠ¡åœºæ™¯**æ‹“å±•ã€‚",
            "timestamp": "2026-01-27T16:07:04.877167",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T14:43:29Z",
    "created_at": "2026-01-27T15:53:22.103045",
    "updated_at": "2026-01-27T15:53:22.103052"
}