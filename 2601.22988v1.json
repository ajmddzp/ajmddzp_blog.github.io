{
    "id": "2601.22988v1",
    "title": "Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation",
    "authors": [
        "Di Zhang",
        "Weicheng Duan",
        "Dasen Gu",
        "Hongye Lu",
        "Hai Zhang",
        "Hang Yu",
        "Junqiao Zhao",
        "Guang Chen"
    ],
    "abstract": "ç°å®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œéœ€è¦å…·å¤‡å¼ºå¤§ç©ºé—´åœºæ™¯ç†è§£èƒ½åŠ›ä¸”èƒ½é€‚åº”ä¸åŒæ‘„åƒæœºè§†è§’çš„è§†è§‰è¿åŠ¨ç­–ç•¥ã€‚è™½ç„¶ä¸‰ç»´æ„ŸçŸ¥è§†è§‰è¡¨å¾çš„æœ€æ–°è¿›å±•å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä»å­˜åœ¨è‹¥å¹²å…³é”®å±€é™ï¼šæ¨ç†æ—¶ä¾èµ–å¤šè§†è§’è§‚æµ‹ï¼Œè¿™åœ¨å•è§†è§’å—é™åœºæ™¯ä¸­ä¸åˆ‡å®é™…ï¼›åœºæ™¯å»ºæ¨¡ä¸å®Œæ•´ï¼Œæ— æ³•æ•æ‰ç²¾ç¡®æ“ä½œæ‰€éœ€çš„å…¨æ™¯ä¸ç»†ç²’åº¦å‡ ä½•ç»“æ„ï¼›ç¼ºä¹æœ‰æ•ˆçš„ç­–ç•¥è®­ç»ƒæ–¹æ³•æ¥ä¿æŒå¹¶åˆ©ç”¨å·²è·å–çš„ä¸‰ç»´çŸ¥è¯†ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºMethodNameâ€”â€”ä¸€ä¸ªé¢å‘è§†è§’é€šç”¨åŒ–æœºå™¨äººæ“ä½œçš„ç»Ÿä¸€è¡¨å¾-ç­–ç•¥å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ–¹æ³•å¼•å…¥å•è§†è§’ä¸‰ç»´é¢„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡ç‚¹äº‘é‡å»ºä¸å‰é¦ˆé«˜æ–¯æº…å°„æŠ€æœ¯ï¼Œåœ¨å¤šè§†è§’ç›‘ç£ä¸‹å­¦ä¹ å…¨æ™¯å‡ ä½•è¡¨å¾ã€‚åœ¨ç­–ç•¥å­¦ä¹ é˜¶æ®µï¼ŒMethodNameæ‰§è¡Œå¤šæ­¥è’¸é¦ä»¥ä¿æŒé¢„è®­ç»ƒçš„å‡ ä½•ç†è§£ï¼Œå¹¶å°†å…¶æœ‰æ•ˆè¿ç§»è‡³æ“ä½œæŠ€èƒ½ä¸­ã€‚æˆ‘ä»¬åœ¨12é¡¹RLBenchä»»åŠ¡ä¸Šè¿›è¡Œå®éªŒï¼Œè¯¥æ–¹æ³•ä»¥å¹³å‡æˆåŠŸç‡12.7%çš„ä¼˜åŠ¿è¶…è¶Šå…ˆå‰æœ€ä¼˜æ–¹æ³•ã€‚åœ¨å…­é¡¹ä»£è¡¨æ€§ä»»åŠ¡ä¸­çš„è¿›ä¸€æ­¥è¯„ä¼°æ˜¾ç¤ºå‡ºå¼ºå¤§çš„é›¶æ ·æœ¬è§†è§’æ³›åŒ–èƒ½åŠ›ï¼šåœ¨ä¸­ç­‰å’Œå¤§å¹…è§†è§’åç§»ä¸‹ï¼ŒæˆåŠŸç‡ä»…åˆ†åˆ«ä¸‹é™22.0%å’Œ29.7%ï¼Œè€Œæœ€ä¼˜å¯¹æ¯”æ–¹æ³•çš„é™å¹…é«˜è¾¾41.6%å’Œ51.5%ã€‚",
    "url": "https://arxiv.org/abs/2601.22988v1",
    "html_url": "https://arxiv.org/html/2601.22988v1",
    "html_content": "Learning Geometrically-Grounded 3D Visual Representations\nfor View-Generalizable Robotic Manipulation\nDi Zhang\n1âˆ—\nWeicheng Duan\n1âˆ—\nDasen Gu\n1\nHongye Lu\n1\nHai Zhang\n2\nHang Yu\n1\nJunqiao Zhao\n1â€ \nGuang Chen\n1\n1\nTongji University\n2\nThe University of Hong Kong\nAbstract\nâˆ—\nEqual contribution. Contact\n{2331922, 2252109}@tongji.edu.cn\nâ€ \nCorresponding author. Contact\nzhaojunqiao@tongji.edu.cn\nProject Website:\ngem3d-project.github.io/gem3d.github.io/\nReal-world robotic manipulation demands visuomotor policies capable of robust spatial scene understanding and strong generalization across diverse camera viewpoints.\nWhile recent advances in 3D-aware visual representations have shown promise, they still suffer from several key limitations:\n(i) reliance on multi-view observations during inference, which is impractical in single-view restricted scenarios;\n(ii) incomplete scene modeling that fails to capture holistic and fine-grained geometric structures essential for precise manipulation; and\n(iii) lack of effective policy training strategies to retain and exploit the acquired 3D knowledge.\nTo address these challenges, we present\nGEM3D\n(\nGe\no\nm\netrically-Grounded\n3D\nManipulation), a unified representation-policy learning framework for view-generalizable robotic manipulation.\nGEM3D\nintroduces a single-view 3D pretraining paradigm that leverages point cloud reconstruction and feed-forward gaussian splatting under multi-view supervision to learn holistic geometric representations.\nDuring policy learning,\nGEM3D\nperforms multi-step distillation to preserve the pretrained geometric understanding and effectively transfer it to manipulation skills.\nWe conduct experiments on 12 RLBench tasks, where our approach outperforms the previous state-of-the-art (SOTA) method by\n12.7%\nin average success rate.\nFurther evaluation on six representative tasks demonstrates the strong zero-shot view generalization of our approach, with the success rate drops by only\n22.0%\nand\n29.7%\nunder moderate and large viewpoint shifts, respectively, whereas the SOTA method suffers larger decreases of\n41.6%\nand\n51.5%\n.\n(a)\nGEM3D\nlearns fine-grained 3D representations that enable more accurate novel-view rendering and point cloud reconstruction.\n(b)\nGEM3D\nexhibits strong robustness to viewpoint shifts, maintaining stable visuomotor performance during inference.\nFigure 1\n:\nWe present\nGEM3D\n, a unified rerepresentation-policy learning framework for view-generalizable robotic manipulation.\n1\nIntroduction\nLearning end-to-end visuomotor policies enables robots to perceive their surroundings and act upon visual understanding\n[\n3\n,\n18\n,\n4\n,\n12\n,\n45\n,\n9\n,\n11\n]\n.\nHowever, real-world environments are often spatially complex and partially occluded, with varying camera viewpoints.\nTherefore, learning spatially grounded and view-invariant visual representations is essential for developing robust visuomotor policies.\nMost existing approaches\n[\n41\n,\n19\n,\n10\n]\nemploy 2D visual encoders to compress images into latent vectors that capture task-relevant states.\nHowever, such 2D features lack explicit 3D structural awareness, limiting their effectiveness in spatially intricate manipulation tasks.\nRecent studies have attempted to address this limitation by leveraging multi-view inputs to learn 3D-aware visual representations\n[\n7\n,\n28\n,\n32\n]\n.\nWhile promising, such methods are often impractical for real-world deployment due to hardware constraints and inference overhead.\nMore recent efforts\n[\n6\n,\n37\n,\n23\n,\n43\n,\n44\n]\nintroduce single-view 3D perception by incorporating auxiliary reconstruction objectives such as Neural Radiance Fields (NeRFs)\n[\n24\n]\nor Gaussian Splatting\n[\n17\n]\n.\nAlthough these methods enhance 3D understanding, their scene modeling remains coarse, often failing to recover fine-grained geometric structures that are crucial for precise manipulation (as shown in\nFig.\nËœ\n1\n).\nMoreover, the resulting visual encoders tend to overfit to specific training viewpoints, leading to degraded generalization in unseen configurations during inference.\nTo learn more expressive 3D manipulation representations with high-fidelity scene embeddings and strong generalization across varying camera viewpoints, we propose\nGEM3D\n(\nGe\no\nm\netrically-Grounded\n3D\nManipulation), a unified representation-policy learning framework that leverages geometrically-grounded 3D representation pretraining on multi-view data and distills the acquired geometric knowledge into a single-view visual policy for downstream robotic manipulation tasks.\nGEM3D\nadopts a three-stage pretraining paradigm to learn expressive 3D scene embeddings:\n(i)\nFrom single-view RGB-D observations, the model encodes the input into a voxel-based\nDense Volumetric Feature\n.\n(ii)\nThis volumetric representation is then used to predict skeleton seed points, which are progressively refined into complete point structures via a coarse-to-fine Snowflake process\n[\n38\n]\n, enabling holistic geometric understanding.\n(iii)\nGuided by the refined points, the\nDense Volumetric Feature\nis resampled to predict Gaussian primitives for novel-view rendering under multi-view supervision, capturing fine-grained appearance and texture details.\nThrough this pretraining paradigm,\nGEM3D\nlearns to infer the complete geometry and appearance of a scene from single-view inputs, thereby achieving robust scene understanding and strong view-generalization.\nBeyond the generalizable\nDense Volumetric Feature\n, developing a robust policy that fully retains the scene understanding capability acquired during pretraining is equally crucial.\nTo this end, we propose a multi-step distillation-based visuomotor policy that can integrate with any pretrained representations, featuring an implicit dynamics-aware design that enhances perception-action consistency.\nRather than directly fine-tuning the pretrained encoder,\nGEM3D\nsoftly guides the policyâ€™s visual encoder training by aligning its latent features with the pretrained representations, effectively transferring the acquired geometric understanding to manipulation skills.\nTo evaluate the effectiveness of\nGEM3D\n, we conduct comprehensive experiments on 12 RLBench manipulation tasks\n[\n16\n]\n.\nGEM3D\nachieves a\n12.7%\nimprovement in average success rate over the previous state-of-the-art (SOTA) method ManiGaussian\n[\n23\n]\n.\nIt also demonstrates strong zero-shot view generalization, with success rates dropping by only\n22.03%\nand\n29.67%\nunder moderate and large viewpoint shifts, compared to the SOTA methodâ€™s\n41.62%\nand\n51.52%\ndeclines.\nIn addition, integrating our distillation-based policy with ManiGaussan-Pretrained representation backbones yields consistent performance gains, indicating its potential for diversified extension to a broader range of backbone architectures.\nAblation studies further verify the necessity of each component in our framework.\nFigure 2\n:\nOverview of\nGEM3D\n.\nGEM3D\ncomprises two key components:\n(1)\nGEM3D Pretraining\n, which learns holistic 3D representations through auxiliary scene reconstruction tasks; and\n(2)\nGEM3D Policy\n, which distills the pretrained 3D visual representations into a visuomotor policy for view-generalizable manipulation.\n2\nRelated Work\n2.1\n3D Representation Learning\nRecent advances in visuomotor policy learning have explored diverse representation forms, ranging from 2D image features\n[\n3\n,\n30\n]\nto 3D structures such as voxel grids\n[\n33\n]\nand point clouds\n[\n2\n,\n8\n,\n44\n]\n.\nAmong these, novel view synthesis (NVS) has recently gained attention as an auxiliary representation learning objective, as it enables models to infer unseen viewpoints from limited observationsâ€”reflecting a holistic understanding of 3D geometry and spatial consistency\n[\n24\n,\n17\n,\n27\n,\n22\n,\n31\n]\n.\nHowever, achieving accurate and generalizable reconstruction remains challenging for existing NVS paradigms.\nClassical methods such as NeRFs\n[\n24\n]\nand Gaussian Splatting\n[\n17\n]\noften struggle to generalize beyond the training distribution, leading to degraded rendering quality in novel scenes or viewpoints.\nMore recent feed-forward Gaussian Splatting approachesâ€”ImageSplatter\n[\n34\n]\n, VoxSplat\n[\n29\n]\n, and PixelSplat\n[\n1\n]\nâ€”improve generalization but remain unstable in dynamic environments, exhibiting noticeable degradation in reconstructed geometry and texture (see\nFig.\nËœ\n1\n).\nThese limitations highlight the need for a NVS framework that achieves both high-fidelity reconstruction and robust generalization across diverse robotic scenes.\n2.2\nVisuomotor Policy Learning\nTo effectively leverage the benefits of auxiliary representation learning tasks, recent visuomotor policy research can be categorized into two major training paradigms:\n(i)\nPretrain-Finetune\n.\nThese methods leverage visual encoders that are pretrained on large-scale 2D or 3D perception datasets and subsequently finetuned for downstream control tasks\n[\n39\n,\n36\n,\n40\n,\n42\n,\n28\n]\n.\nHowever, domain discrepancies between pretraining and downstream control tasks can lead to suboptimal transfer, and even degrade policy learning, as observed in\n[\n13\n,\n14\n]\n.\n(ii)\nJoint-Training\n.\nThis paradigm jointly optimizes visuomotor policies with auxiliary representation objectives, such as NVS\n[\n43\n,\n23\n]\n.\nYet, the optimization tends to be imbalanced: the representation gains little from auxiliary tasks, while the policy overfits to training viewpoints and generalizes poorly to unseen camera poses.\nIn contrast to prior works, we propose a multi-step distillation framework that softly guides the policy encoder training with pretrained 3D representations, effectively preserving geometric knowledge acquired during pretraining.\n3\nMethod\nOur goal is to learn a voxel-based 3D visual representation through pretraining and subsequently distill the acquired 3D knowledge into a visuomotor policy for view-generalizable robotic manipulation.\nIn the following, we outline the overall framework of\nGEM3D\n.\nSec.\nËœ\n3.1\ndetails the pretraining pipeline and\nSec.\nËœ\n3.2\npresents the policy learning strategy.\nAn overview is shown in\nFig.\nËœ\n2\n.\n3.1\nGeometrically-Grounded Pretraining\nFigure 3\n:\nGEM3D\nPretraining Pipeline.\n(a)\nencoding single-view RGB-D observations into volumetric features,\n(b)\nprogressively reconstructing scene geometry in a coarse-to-fine Snowflake\n[\n38\n]\nmanner, and\n(c)\nlearning fine-grained texture details through Gaussian-splatting-based novel view rendering.\nGEM3D\nâ€™s pretraining pipeline consists of three main components:\n(i) a\n3D Feature Extraction\nmodule that encodes single-view RGB-D observations into\ndense volumetric features\n;\n(ii) a\nPoint Cloud Reconstruction\nobjective that learns holistic 3D scene geometry; and\n(iii) a\nFeed-forward 3D Gaussian Splatting\nmodule that captures fine-grained texture through multi-view supervision.\nThe detailed pipeline is illustrated in\nFig.\nËœ\n3\n.\n3.1.1\n3D Feature Extraction\nAs shown in\nFig.\nËœ\n3\na, the feature extraction module takes a single-view RGB-D input\no\nt\n=\n{\nâ„\nt\n,\nğ’Ÿ\nt\n,\nğ’¦\nt\n}\no^{t}=\\{\\mathcal{I}^{t},\\mathcal{D}^{t},\\mathcal{K}^{t}\\}\nand processes it as follows:\nâ€¢\nThe depth map\nğ’Ÿ\nt\n\\mathcal{D}^{t}\nis back-projected using the camera parameters\nğ’¦\nt\n\\mathcal{K}^{t}\n, followed by cropping and farthest point sampling to obtain the point cloud\nğ’«\n\\mathcal{P}\n.\nâ€¢\nThe RGB image\nâ„\nt\n\\mathcal{I}^{t}\nis encoded by a pretrained DINOv2\n[\n25\n]\nmodel to extract pixel-wise 2D features\nâ„±\n2D\n\\mathcal{F}_{\\text{2D}}\n, which are then projected onto\nğ’«\n\\mathcal{P}\nto enrich the point cloud with visual semantics.\nâ€¢\nThe resulting semantically enriched point cloud is voxelized into occupancy and feature volumes, which are subsequently fused through a 3D U-Net\n[\n5\n]\nto produce the\nDense Volumetric Feature\nâ„±\nâˆˆ\nâ„\nD\n3\nÃ—\n128\n\\mathcal{F}\\in\\mathbb{R}^{D^{3}\\times 128}\n, where\nD\nD\ndenotes the voxel resolution, and\n128\n128\ncorresponds to the feature dimension.\nThis process encodes the single-view RGB-D observation into a dense voxel-based 3D representation\nâ„±\n\\mathcal{F}\nthat jointly captures geometric structure and visual semantics.\n3.1.2\nCoarse-to-Fine Point Cloud Reconstruction\nTo enable the\nholistic geometric understanding\nof the volumetric feature\nâ„±\n\\mathcal{F}\n, we introduce a coarse-to-fine reconstruction module that first predicts sparse seed points and then progressively refines them into a dense point cloud (\nFig.\nËœ\n3\nb).\nSeed Points Generation.\nTo capture the key structural points necessary for reconstructing the full scene, we initialize a\nlearnable voxel query set\nğ’¬\nâˆˆ\nâ„\nd\n3\nÃ—\n128\n\\mathcal{Q}\\in\\mathbb{R}^{d^{3}\\times 128}\n(with\nd\nâ‰ª\nD\nd\\ll D\nto reduce computational overhead) and apply a two-stage attention mechanism to aggregate multi-scale spatial features from the volumetric representation\nâ„±\n\\mathcal{F}\n:\nâ€¢\nCoarse Cross-Attention\n:\nWe first downsample\nâ„±\n\\mathcal{F}\nwith average pooling to obtain a low-resolution feature\nâ„±\ndown\nâˆˆ\nâ„\nd\n3\nÃ—\n128\n\\mathcal{F}_{\\text{down}}\\in\\mathbb{R}^{d^{3}\\times 128}\n, which only preserves the overall spatial structure of the scene.\nAnd then we perform cross-attention between\nğ’¬\n\\mathcal{Q}\nand\nâ„±\ndown\n\\mathcal{F}_{\\text{down}}\nto obtain\nproposal queries\nğ’¬\np\n\\mathcal{Q}_{p}\nthat capture coarse spatial relationships.\nâ€¢\n3D Deformable Cross-Attention\n:\nTo further enrich\nğ’¬\np\n\\mathcal{Q}_{p}\nwith fine-grained geometric cues, we apply a 3D deformable cross-attention module between\nğ’¬\np\n\\mathcal{Q}_{p}\nand\nâ„±\n\\mathcal{F}\n, following\n[\n46\n,\n20\n]\n.\nThis enables efficient aggregation of local geometric details from\nâ„±\n\\mathcal{F}\nto the query features. Implementation details are provided in AppendixÂ A.\nThis process yields a set of\nseed tokens\n, which are decoded by a shallow MLP to produce seed point coordinates\nğ’«\n^\n0\nâˆˆ\nâ„\nd\n3\nÃ—\n3\n\\hat{\\mathcal{P}}_{0}\\in\\mathbb{R}^{d^{3}\\times 3}\n.\nPoint Cloud Recovery.\nStarting from the seed points\nğ’«\n^\n0\n\\hat{\\mathcal{P}}_{0}\n, we follow the SnowflakeNet\n[\n38\n]\nand employ a\nSnowflake Point Deconvolution\n(SPD) block to progressively refine the point set.\nAt each stage\ni\ni\n:\nâ€¢\nThe parent points\nğ’«\n^\ni\nâˆˆ\nâ„\nN\ni\nÃ—\n3\n\\hat{\\mathcal{P}}_{i}\\in\\mathbb{R}^{N_{i}\\times 3}\n(\nN\ni\nN_{i}\ndenotes the point number at stage\ni\ni\n) query the volumetric feature\nâ„±\n\\mathcal{F}\nvia trilinear interpolation at their coordinates to obtain point features\nâ„±\ni\nâˆˆ\nâ„\nN\ni\nÃ—\n128\n\\mathcal{F}_{i}\\in\\mathbb{R}^{N_{i}\\times 128}\n.\nâ€¢\nGiven\nğ’«\n^\ni\n\\hat{\\mathcal{P}}_{i}\n,\nâ„±\ni\n\\mathcal{F}_{i}\nand current context features\nâ„±\ni\nc\n\\mathcal{F}^{\\text{c}}_{i}\n(\nthe intermediate output of the previous SPD stage, omitted for the first iteration)\nas input, the SPD block upsamples each point by a factor of\nr\nr\n, producing\nr\nr\ndisplacement vectors for each parent point\nÎ”\nâ€‹\nğ’«\ni\nâˆˆ\nâ„\nr\nÃ—\nN\ni\nÃ—\n3\n\\Delta\\mathcal{P}_{i}\\in\\mathbb{R}^{r\\times N_{i}\\times 3}\nand outputs the updated context features\nâ„±\ni\n+\n1\nc\n\\mathcal{F}^{\\text{c}}_{i+1}\n.\nâ€¢\nEach parent point is duplicated\nr\nr\ntimes and displaced by the predicted offsets\nÎ”\nâ€‹\nğ’«\ni\n\\Delta\\mathcal{P}_{i}\nto form the refined child points\nğ’«\n^\ni\n+\n1\nâˆˆ\nâ„\nN\ni\n+\n1\nÃ—\n3\n\\hat{\\mathcal{P}}_{i+1}\\in\\mathbb{R}^{N_{i+1}\\times 3}\n, where\nN\ni\n+\n1\n=\nr\nÃ—\nN\ni\nN_{i+1}=r\\times N_{i}\n.\nTo supervise reconstruction with holistic scene geometry, we fuse multi-view ground-truth point clouds into a complete set\nğ’«\nfull\n\\mathcal{P}_{\\text{full}}\n.\nAt each refinement stage\ni\ni\n, we use farthest point sampling to sample a subset\nğ’«\ni\nâŠ‚\nğ’«\nfull\n\\mathcal{P}_{i}\\subset\\mathcal{P}_{\\text{full}}\nwith a size comparable to that of the predicted set\nğ’«\n^\ni\n\\hat{\\mathcal{P}}_{i}\n, and supervise the refinement using the\nChamfer distance\nL\n2\nL_{2}\n:\nâ„’\nrec\n=\nâˆ‘\ni\n=\n0\n3\n(\n\\displaystyle\\mathcal{L}_{\\text{rec}}=\\sum_{i=0}^{3}\\Bigg(\n1\n|\nğ’«\n^\ni\n|\nâ€‹\nâˆ‘\nğ©\n^\nâˆˆ\nğ’«\n^\ni\nmin\nğ©\nâˆˆ\nğ’«\ni\nâ¡\nâ€–\nğ©\n^\nâˆ’\nğ©\nâ€–\n2\n2\n\\displaystyle\\frac{1}{|\\hat{\\mathcal{P}}_{i}|}\\sum_{\\hat{\\mathbf{p}}\\in\\hat{\\mathcal{P}}_{i}}\\min_{\\mathbf{p}\\in\\mathcal{P}_{i}}\\|\\hat{\\mathbf{p}}-\\mathbf{p}\\|_{2}^{2}\n(1)\n+\n1\n|\nğ’«\ni\n|\nâˆ‘\nğ©\nâˆˆ\nğ’«\ni\nmin\nğ©\n^\nâˆˆ\nğ’«\n^\ni\nâˆ¥\nğ©\nâˆ’\nğ©\n^\nâˆ¥\n2\n2\n)\n\\displaystyle+\\frac{1}{|\\mathcal{P}_{i}|}\\sum_{\\mathbf{p}\\in\\mathcal{P}_{i}}\\min_{\\hat{\\mathbf{p}}\\in\\hat{\\mathcal{P}}_{i}}\\|\\mathbf{p}-\\hat{\\mathbf{p}}\\|_{2}^{2}\\Bigg)\n3.1.3\nFeed-forward 3D Gaussian Splatting\nBased on the reconstructed dense point cloud\nğ’«\n^\n\\hat{\\mathcal{P}}\n,\nwe further integrate a feed-forward 3D Gaussian Splatting pipeline that performs novel-view rendering and\ncaptures fine-grained texture details\nunder multi-view supervision. (\nFig.\nËœ\n3\n(c))\nThe Gaussian centers\nÎ¼\n\\mu\nare directly given by the refined points\nğ’«\n^\nâˆˆ\nâ„\nN\np\nÃ—\n3\n\\hat{\\mathcal{P}}\\in\\mathbb{R}^{N_{p}\\times 3}\n.\nAt these locations, we sample Gaussian features\nâ„±\nğ’¢\nâˆˆ\nâ„\nN\np\nÃ—\n128\n\\mathcal{F}_{\\mathcal{G}}\\in\\mathbb{R}^{N_{p}\\times 128}\nfrom the dense volumetric field\nâ„±\n\\mathcal{F}\nvia trilinear interpolation, and feed them into a ResNetFC network to regress the remaining non-positional Gaussian parametersâ€”including color, opacity, rotation and scale:\n{\nc\n,\nÏƒ\n,\nr\n,\ns\n}\n=\nResNetFC\nâ€‹\n(\nâ„±\nğ’¢\n)\n\\{c,\\sigma,r,s\\}=\\text{ResNetFC}(\\mathcal{F}_{\\mathcal{G}})\n(2)\nThe resulting 3D Gaussian parameters\nğ’¢\n=\n[\nÎ¼\n,\nc\n,\nÏƒ\n,\nr\n,\ns\n]\n\\mathcal{G}=[\\mu,c,\\sigma,r,s]\nare employed in a differentiable renderer to synthesize novel-view images\nâ„\n^\nv\nt\n\\hat{\\mathcal{I}}^{t}_{v}\n, which are supervised by the corresponding multi-view RGB observations\nâ„\nv\nt\n\\mathcal{I}^{t}_{v}\nusing a\nfocal loss\n[\n21\n]\n, which places greater emphasis on dynamic and visually ambiguous regions:\nâ„’\nrend\n=\nFocalLoss\nâ€‹\n(\nâ„\n^\nv\nt\n,\nâ„\nv\nt\n)\n.\n\\mathcal{L}_{\\text{rend}}=\\text{FocalLoss}(\\hat{\\mathcal{I}}_{v}^{t},\\mathcal{I}_{v}^{t}).\n(3)\n3.2\nDistillation-based Policy Learning\nInstead of directly finetuning the pretrained visual encoder obtained from\nSec.\nËœ\n3.1.1\n, which may disrupt the learned geometric representations.\nAs shown in\nFig.\nËœ\n4\n, we initialize a separate policy encoder to process the single-view RGB-D observation\no\nt\no^{t}\ninto latent tokens\nğ±\nt\n\\mathbf{x}^{t}\n.\nMeanwhile, the frozen\nPretrained 3D Feature Extractor\nproduces the corresponding\ndense volumetric feature\nâ„±\nt\n\\mathcal{F}^{t}\n, which is subsequently patchified into reference tokens\nğ±\n~\nt\n\\tilde{\\mathbf{x}}^{t}\n.\nA cosine-similarity-based distillation loss is then applied to align the latent and reference tokens (as shown in\nFig.\nËœ\n4\n).\nFigure 4\n:\nGEM3D Policy\n. A multi-step distillation-based policy learning framework.\nSince we adopt end-effector position control, the robot action\na\nt\na_{t}\nis implicitly embedded within the next latent state\nğ±\nt\n+\n1\n\\mathbf{x}^{t+1}\n.\nTo equip the policy with dynamic understanding, we introduce an implicit latent dynamics model that predicts\nğ±\nt\n+\n1\n\\mathbf{x}^{t+1}\nfrom the current latent state\nğ±\nt\n\\mathbf{x}^{t}\n, together with proprioceptive and language embeddings.\nThe action\na\nt\na_{t}\nis then decoded from\nğ±\nt\n+\n1\n\\mathbf{x}^{t+1}\nthrough an action head.\nThis design features two benefits: (i) improve the decision explainability by modeling temporal transitions in the latent space; and (ii) further enables multi-step latent distillation to regularize the policyâ€™s temporal consistency, formulated as:\nâ„’\ndistill\n=\nâ„’\ncos\nâ€‹\n(\nğ±\nt\n,\nğ±\n~\nt\n)\n+\nâ„’\ncos\nâ€‹\n(\nğ±\nt\n+\n1\n,\nğ±\n~\nt\n+\n1\n)\n\\mathcal{L}_{\\text{distill}}=\\mathcal{L}_{\\text{cos}}(\\mathbf{x}^{t},\\tilde{\\mathbf{x}}^{t})+\\mathcal{L}_{\\text{cos}}(\\mathbf{x}^{t+1},\\tilde{\\mathbf{x}}^{t+1})\n(4)\nWe train the policy via imitation learning using expert actions as supervision. The overall policy training objective is formulated as:\nâ„’\npolicy\n=\nâ€–\nğš\nt\nâˆ’\nğš\nt\nâˆ—\nâ€–\n2\n2\n+\nÎ»\ndistill\nâ€‹\nâ„’\ndistill\n,\n\\mathcal{L}_{\\text{policy}}=\\|\\mathbf{a}_{t}-\\mathbf{a}_{t}^{*}\\|_{2}^{2}+\\lambda_{\\text{distill}}\\mathcal{L}_{\\text{distill}},\n(5)\n4\nExperiments\n4.1\nExperimental Setup\nEvaluation setup.\nWe evaluate our method on RLBench\n[\n16\n]\n, covering 12 tasks across 9 scenes with varying task instructions.\nThese tasks span diverse manipulation challenges and require both fine-grained control and strong spatial reasoning, as illustrated in\nFig.\nËœ\n5(a)\n.\n(a)\nRLBench task examples.\n(b)\nView-generalization evaluation settings.\nFigure 5\n:\nExamples of our evaluation setup.\nDuring pretraining, multi-view observations are captured from 8 uniformly distributed surrounding cameras, with 50 trajectories per scene (50\nÃ—\n\\times\n9 = 450 episodes in total).\nFor policy learning, each task is trained using 20 expert demonstrations collected from a fixed front-view camera.\nTable 1:\nComparison of\nGEM3D\nwith state-of-the-art methods on all 12 RLBench tasks (SR%).\nBold\nnumbers denote the best performance, and\nunderlined\nnumbers denote the second best.\nMethods\nAvg.\nSR (%)\nâ†‘\n\\uparrow\nAvg.\nRank\nâ†“\n\\downarrow\nClose\nJar\nOpen\nDrawer\nMeat off\nGrill\nTurn\nTap\nWater\nPlants\nPerAct\n21.9\nÂ±\n\\pm\n5.3\n4.58\n19.0\nÂ±\n\\pm\n4.6\n38.7\nÂ±\n\\pm\n13.7\n28.3\nÂ±\n\\pm\n4.7\n43.7\nÂ±\n\\pm\n4.7\n8.3\nÂ±\n\\pm\n2.3\nGNFactor\n29.2\nÂ±\n\\pm\n18.4\n3.33\n32.0\nÂ±\n\\pm\n16.8\n51.7\nÂ±\n\\pm\n13.5\n32.0\nÂ±\n\\pm\n10.5\n40.7\nÂ±\n\\pm\n10.7\n22.0\nÂ±\n\\pm\n4.4\nManiGaussian\n31.5\nÂ±\n\\pm\n8.5\n3.08\n27.3\nÂ±\n\\pm\n16.4\n57.3\nÂ±\n\\pm\n1.5\n30.0\nÂ±\n\\pm\n11.3\n44.0\nÂ±\n\\pm\n10.5\n19.3\nÂ±\n\\pm\n9.0\nManiGaussian\n(w Distill)\n33.3\nÂ±\n\\pm\n10.4\n3.00\n31.0\nÂ±\n\\pm\n12.1\n54.3\nÂ±\n\\pm\n12.5\n35.3\nÂ±\n\\pm\n9.8\n42.7\nÂ±\n\\pm\n14.6\n16.3\nÂ±\n\\pm\n8.7\nGEM3D\n(Ours)\n44.2\nÂ±\n\\pm\n6.2\n1.00\n44.7\nÂ±\n\\pm\n3.2\n65.7\nÂ±\n\\pm\n4.5\n44.3\nÂ±\n\\pm\n3.1\n54.0\nÂ±\n\\pm\n7.2\n32.7\nÂ±\n\\pm\n3.5\nMethods\nPhone on\nBase\nPut Money\nin Safe\nPlace\nWine\nStack\nBlocks\nOpen\nJar\nMeat on\nGrill\nTake Money\nout Safe\nPerAct\n51.3\nÂ±\n\\pm\n11.9\n10.7\nÂ±\n\\pm\n0.6\n3.0\nÂ±\n\\pm\n1.0\n4.0\nÂ±\n\\pm\n0.0\n26.7\nÂ±\n\\pm\n9.1\n16.0\nÂ±\n\\pm\n7.9\n13.7\nÂ±\n\\pm\n2.9\nGNFactor\n62.0\nÂ±\n\\pm\n5.6\n11.0\nÂ±\n\\pm\n10.5\n5.7\nÂ±\n\\pm\n2.1\n3.7\nÂ±\n\\pm\n1.2\n35.7\nÂ±\n\\pm\n6.4\n35.0\nÂ±\n\\pm\n6.1\n19.0\nÂ±\n\\pm\n4.0\nManiGaussian\n66.3\nÂ±\n\\pm\n12.5\n5.7\nÂ±\n\\pm\n3.5\n8.0\nÂ±\n\\pm\n2.6\n2.0\nÂ±\n\\pm\n1.7\n38.3\nÂ±\n\\pm\n9.0\n30.7\nÂ±\n\\pm\n12.9\n49.3\nÂ±\n\\pm\n10.5\nManiGaussian\n(w Distill)\n64.3\nÂ±\n\\pm\n16.2\n20.3\nÂ±\n\\pm\n9.2\n5.3\nÂ±\n\\pm\n3.5\n5.0\nÂ±\n\\pm\n2.6\n33.7\nÂ±\n\\pm\n9.5\n48.7\nÂ±\n\\pm\n2.9\n43.0\nÂ±\n\\pm\n23.4\nGEM3D\n(Ours)\n67.3\nÂ±\n\\pm\n1.5\n29.7\nÂ±\n\\pm\n2.9\n11.7\nÂ±\n\\pm\n4.0\n7.7\nÂ±\n\\pm\n2.9\n40.0\nÂ±\n\\pm\n5.6\n61.0\nÂ±\n\\pm\n22.1\n72.0\nÂ±\n\\pm\n13.7\nEvaluation metrics.\nWe evaluate policy performance using the task success rate (SR), defined as\n1\n1\nfor success and\n0\nfor failure per episode, without partial credit.\nFor fair comparison, each model is trained with three fixed random seeds (0, 1, 2). To mitigate overfitting, checkpoints at both 80% and the final training step are evaluated, and the better result per seed is selected. Each seed is tested on 100 different settings per task, including 178 variations in language, object position, and color. We report the mean and standard deviation of SR across all seeds.\nTo assess view generalization, we perform zero-shot evaluations under three levels of camera viewpoint shifts (as shown in\nFig.\nËœ\n5(b)\n):\nâ€¢\nNo Shift:\nSame viewpoint as training.\nâ€¢\nModerate Shift:\nCamera relocated within\n30\nâˆ˜\n30^{\\circ}\naround the robot base.\nâ€¢\nLarge Shift:\nSignificant viewpoint variations between\n60\nâˆ˜\n60^{\\circ}\nand\n90\nâˆ˜\n90^{\\circ}\n.\nBaselines.\nWe adopt\nPerceiverIO\n[\n15\n]\nas our policy backbone for its simplicity and compatibility with discrete token-based input-output representations.\nFor fair comparison, we consider prior methods that also employ the PerceiverIO architecture as their policy backbone, including:\n(i)\nPerAct\n[\n33\n]\n, which directly utilizes voxel-based 3D representations;\n(ii)\nGNFActor\n[\n43\n]\n, which incorporates NeRFs as an auxiliary representation learning objective; and\n(iii)\nManiGaussian\n[\n23\n]\n, which leverages 3D Gaussian Splatting for representation learning and jointly trains it with the policy.\nTo further assess our distillation-based policy learning framework, we pair the ManiGaussian representation module with our training strategy, denoted as (iv)\nManiGaussian (w Distill)\n, and additionally compare its pretraining quality against\nGEM3D\n.\nImplementation details.\nWe apply SE(3) augmentations to the input point cloud following prior works\n[\n33\n,\n35\n]\n.\nGEM3D\nis pretrained for 100K iterations on two NVIDIA RTX3090 GPUs, taking approximately four to five days.\nEach policy is then trained for 12K iterations on a single RTX3090 GPU, requiring about three to four hours per task.\nAdditional implementation and experimental setup details are provided in the Appendix B.\n4.2\nExperimental Results\n4.2.1\nComparison with State-of-the-Art Methods\nWe compare\nGEM3D\nwith baseline methods on 12 RLBench tasks, with each model trained independently per task.\nThe results are shown in\nTab.\nËœ\n1\n,\nGEM3D\nachieves the highest overall performance, attaining an average success rate of\n44.2%\nand an absolute gain of\n12.7%\nover\nManiGaussian\n, the previous State-of-the-Art (SOTA) method.\nFurthermore,\nTab.\nËœ\n2\nreports the zero-shot success rates under varying degrees of viewpoint shifts on six representative tasks, together with the relative performance drop compared to the training view.\nGEM3D\nexhibits only\n22.03%\nand\n29.67%\ndeclines under\nmoderate\nand\nlarge\nshifts, respectivelyâ€”significantly lower than the\n41.62%\nand\n51.52%\ndrops observed in\nManiGaussian\n.\nUnder large viewpoint shifts, our method achieves up to\n6\nÃ—\n\\times\nhigher success rates than\nPerAct\n(see\nFig.\nËœ\n1\n).\nNotably, although the distilled variant\nManiGaussian (w distill)\nshows slightly improved robustness over its original version, its performance remains far below that of\nGEM3D\n.\nThis suggests that while our distillation-based policy training helps mitigate performance degradation under unseen viewpoint shifts, holistic 3D representation learning through stronger pretraining is the key to achieving view-generalizable visuomotor policies.\nTable 2\n:\nResult of zero-shot view generalization on six RLBench tasks under two view-shift settings (SR%).\nBold\nnumbers denote the best performance, and\nâ†“\n\\downarrow\nindicates the relative performance drop compared to the training view.\nMethods\nClose\nJar\nMeat off\nGrill\nTurn\nTap\nPhone on\nBase\nOpen\nJar\nMeat on\nGrill\nAvg.\nModerate\nShift\nPerAct\n7.7\nÂ±\n\\pm\n9.9\n(59.65%\nâ†“\n\\downarrow\n)\n19.7\nÂ±\n\\pm\n7.2\n(30.56%\nâ†“\n\\downarrow\n)\n14.0\nÂ±\n\\pm\n7.2\n(67.94%\nâ†“\n\\downarrow\n)\n0.3\nÂ±\n\\pm\n0.6\n(99.35%\nâ†“\n\\downarrow\n)\n2.0\nÂ±\n\\pm\n1.7\n(92.50%\nâ†“\n\\downarrow\n)\n12.3\nÂ±\n\\pm\n9.6\n(22.92%\nâ†“\n\\downarrow\n)\n9.3\nÂ±\n\\pm\n7.4\n(69.80%\nâ†“\n\\downarrow\n)\nManiGaussian\n15.3\nÂ±\n\\pm\n15.9\n(43.91%\nâ†“\n\\downarrow\n)\n17.0\nÂ±\n\\pm\n12.1\n(43.33%\nâ†“\n\\downarrow\n)\n32.3\nÂ±\n\\pm\n2.3\n(26.52%\nâ†“\n\\downarrow\n)\n30.7\nÂ±\n\\pm\n18.6\n(53.76%\nâ†“\n\\downarrow\n)\n18.3\nÂ±\n\\pm\n2.5\n(52.18%\nâ†“\n\\downarrow\n)\n24.3\nÂ±\n\\pm\n10.1\n(20.67%\nâ†“\n\\downarrow\n)\n23.0\nÂ±\n\\pm\n7.3\n(41.62%\nâ†“\n\\downarrow\n)\nManiGaussian\n(w Distill)\n20.3\nÂ±\n\\pm\n10.3\n(34.41%\nâ†“\n\\downarrow\n)\n23.7\nÂ±\n\\pm\n2.3\n(33.01%\nâ†“\n\\downarrow\n)\n34.0\nÂ±\n\\pm\n9.8\n(20.32%\nâ†“\n\\downarrow\n)\n40.0\nÂ±\n\\pm\n22.6\n(37.82%\nâ†“\n\\downarrow\n)\n21.7\nÂ±\n\\pm\n3.2\n(35.64%\nâ†“\n\\downarrow\n)\n42.3\nÂ±\n\\pm\n7.0\n(13.02%\nâ†“\n\\downarrow\n)\n30.3\nÂ±\n\\pm\n9.7\n(28.87%\nâ†“\n\\downarrow\n)\nGEM3D\n(ours)\n33.7\nÂ±\n\\pm\n8.5\n(24.62%\nâ†“\n\\downarrow\n)\n33.0\nÂ±\n\\pm\n5.0\n(25.56%\nâ†“\n\\downarrow\n)\n47.4\nÂ±\n\\pm\n8.4\n(14.88%\nâ†“\n\\downarrow\n)\n48.3\nÂ±\n\\pm\n13.6\n(28.22%\nâ†“\n\\downarrow\n)\n34.3\nÂ±\n\\pm\n4.0\n(14.18%\nâ†“\n\\downarrow\n)\n47.0\nÂ±\n\\pm\n14.8\n(22.95%\nâ†“\n\\downarrow\n)\n40.7\nÂ±\n\\pm\n7.7\n(22.03%\nâ†“\n\\downarrow\n)\nLarge\nShift\nPerAct\n2.8\nÂ±\n\\pm\n3.4\n(85.11%\nâ†“\n\\downarrow\n)\n15.7\nÂ±\n\\pm\n3.1\n(44.69%\nâ†“\n\\downarrow\n)\n9.7\nÂ±\n\\pm\n4.3\n(77.86%\nâ†“\n\\downarrow\n)\n1.0\nÂ±\n\\pm\n2.0\n(98.05%\nâ†“\n\\downarrow\n)\n3.3\nÂ±\n\\pm\n4.1\n(87.51%\nâ†“\n\\downarrow\n)\n4.8\nÂ±\n\\pm\n3.8\n(69.81%\nâ†“\n\\downarrow\n)\n6.2\nÂ±\n\\pm\n5.5\n(79.87%\nâ†“\n\\downarrow\n)\nManiGaussian\n10.3\nÂ±\n\\pm\n14.2\n(62.20%\nâ†“\n\\downarrow\n)\n16.3\nÂ±\n\\pm\n8.4\n(45.57%\nâ†“\n\\downarrow\n)\n27.0\nÂ±\n\\pm\n7.0\n(38.64%\nâ†“\n\\downarrow\n)\n23.7\nÂ±\n\\pm\n16.0\n(64.32%\nâ†“\n\\downarrow\n)\n21.5\nÂ±\n\\pm\n6.0\n(43.91%\nâ†“\n\\downarrow\n)\n16.0\nÂ±\n\\pm\n6.2\n(47.83%\nâ†“\n\\downarrow\n)\n19.1\nÂ±\n\\pm\n6.1\n(51.52%\nâ†“\n\\downarrow\n)\nManiGaussian\n(w Distill)\n21.5\nÂ±\n\\pm\n9.2\n(45.16%\nâ†“\n\\downarrow\n)\n16.3\nÂ±\n\\pm\n6.9\n(67.92%\nâ†“\n\\downarrow\n)\n32.0\nÂ±\n\\pm\n8.6\n(29.69%\nâ†“\n\\downarrow\n)\n33.7\nÂ±\n\\pm\n13.1\n(41.97%\nâ†“\n\\downarrow\n)\n23.0\nÂ±\n\\pm\n12.0\n(62.38%\nâ†“\n\\downarrow\n)\n28.2\nÂ±\n\\pm\n8.0\n(53.42%\nâ†“\n\\downarrow\n)\n25.8\nÂ±\n\\pm\n6.6\n(39.43%\nâ†“\n\\downarrow\n)\nGEM3D\n(ours)\n30.2\nÂ±\n\\pm\n7.1\n(32.46%\nâ†“\n\\downarrow\n)\n26.0\nÂ±\n\\pm\n5.8\n(41.35%\nâ†“\n\\downarrow\n)\n45.3\nÂ±\n\\pm\n6.3\n(19.05%\nâ†“\n\\downarrow\n)\n39.0\nÂ±\n\\pm\n13.2\n(42.08%\nâ†“\n\\downarrow\n)\n35.8\nÂ±\n\\pm\n7.7\n(10.42%\nâ†“\n\\downarrow\n)\n40.3\nÂ±\n\\pm\n6.8\n(33.88%\nâ†“\n\\downarrow\n)\n36.1\nÂ±\n\\pm\n7.1\n(29.67%\nâ†“\n\\downarrow\n)\n4.2.2\nMulti-task Evaluation\nTo further validate the language-following ability and task-level generalization of\nGEM3D\n, we train a unified model using a mixture of four RLBench task datasets.\nThe multi-task average success rates and their relative decreases compared to the single-task setting are reported in\nTab.\nËœ\n3\n.\nWhile the original PerAct baseline exhibits a substantial drop on the train view and even shows policy collapse under shifted viewpoints, our policy shows only a modest decline, with the largest decrease being just 6.9% compared to its single-task performance.\nThis demonstrates that our geometrically grounded representation also facilitates clearer task distinction and more reliable instruction following.\nFurther explanations of task selection and per-task results are provided in AppendixÂ C.\nTable 3\n:\nMulti-task results on four RLBench tasks (SR%).\nBold\nnumbers denote the best performance, and\nâ†“\n\\downarrow\nindicates the relative performance drop compared to the single-task training setting.\nMethods\nTrain View\nModerate Shift\nLarge Shift\nPerAct\n26.1\nÂ±\n\\pm\n17.5\n(24.1%\nâ†“\n\\downarrow\n)\n6.4\nÂ±\n\\pm\n4.5\n(11.1%\nâ†“\n\\downarrow\n)\n5.6\nÂ±\n\\pm\n3.6\n(16.1%\nâ†‘\n\\uparrow\n)\nManiGaussian\n(w Distill)\n47.1\nÂ±\n\\pm\n5.8\n(0.4%\nâ†“\n\\downarrow\n)\n30.7\nÂ±\n\\pm\n8.1\n(3.8%\nâ†“\n\\downarrow\n)\n26.5\nÂ±\n\\pm\n7.7\n(10.7%\nâ†“\n\\downarrow\n)\nGEM3D\n(ours)\n52.9\nÂ±\n\\pm\n6.4\n(1.7%\nâ†‘\n\\uparrow\n)\n41.2\nÂ±\n\\pm\n9.6\n(6.9%\nâ†“\n\\downarrow\n)\n36.4\nÂ±\n\\pm\n9.0\n(3.7%\nâ†“\n\\downarrow\n)\n4.2.3\nQualitative Analysis of Scene Reconstruction\nHigh-fidelity scene reconstruction serves as a strong indicator of a modelâ€™s grasp of 3D geometry and appearance. As illustrated in\nFig.\nËœ\n1\n, our method reconstructs sharper structures and produces more consistent novel views, while\nManiGaussian\nsuffers from blurred textures and geometric distortions. These results highlight that our geometrically grounded 3D representations yield a more holistic and viewpoint-robust understanding of the scene.\nTable 4\n:\nComparison of scene reconstruction quality.\nLarger PSNR and SSIM indicate better image fidelity, while lower Chamfer\nL\n2\nL_{2}\nindicates higher point cloud geometric accuracy.\nMethods\nTasks\nPSNR\nâ†‘\n\\uparrow\nSSIM\nâ†‘\n\\uparrow\nChamfer\nL\n2\nL_{2}\nâ†“\n\\downarrow\nManiGaussian\nOpen Drawer\n16.24\n0.3926\n0.04035\nTurn Tap\n15.84\n0.3741\n0.05148\nWater Plants\n16.07\n0.3846\n0.04352\nAvg\n16.05\n0.3838\n0.04511\nGEM3D\n(ours)\nOpen Drawer\n23.54\n0.8259\n0.01541\nTurn Tap\n22.92\n0.8123\n0.01708\nWater Plants\n22.93\n0.8025\n0.01613\nAvg\n23.13\n0.8136\n0.01621\nFurthermore, we report quantitative metrics for the three scenes in\nFig.\nËœ\n1\n(see\nTab.\nËœ\n4\n), including\nPSNR\nand\nSSIM\nfor image quality and\nChamfer L2\nfor geometric accuracy.\nGEM3D Pretraining\nconsistently outperforms the pretraining method of\nManiGaussian\n, achieving average improvements of +7.08 dB in PSNR, +0.4298 in SSIM, and â€“0.0289 in Chamfer\nL\n2\nL_{2}\n, demonstrating its stronger ability to capture both visual fidelity and geometric consistency.\n4.3\nAblations\n4.3.1\nAblation on GEM3D Pretraining\nIn our\nGEM3D Pretraining\nmodule, we highlight three essential designs that jointly enhance geometric reconstruction and texture rendering:\nâ€¢\nDeformable cross-attention (DCA)\nensures efficient and precise seed generation while preserving spatial integrity.\nâ€¢\nSnowflake-style coarse-to-fine reconstruction\nexpands and refines point sets over three upsampling stages, yielding progressively denser geometric structures.\nâ€¢\nFocal loss\nemphasizes supervision on dynamic and ambiguous regions, enhancing robustness to motion-induced appearance and geometry changes.\nWe demonstrate the importance of these components through qualitative ablations (see\nFig.\nËœ\n6\n).\n(i) In the\nw/o DCA\nvariant, deformable cross-attention is replaced with standard cross-attention between dense and downsampled volumetric features, leading to the loss of fine spatial cues.\n(ii) The\nw/o snowflake\nvariant adopts a single-step reconstruction, preventing the model from progressively refining geometric details.\n(iii) The\nw/o focal loss\nsetting replaces focal loss with a plain MSE objective for Gaussian rendering, reducing supervisory emphasis on dynamic or ambiguous regions.\nAcross all cases, removing any individual component results in clear degradationâ€”robot arms collapse, complex plant structures fail to reconstruct, and texture consistency deterioratesâ€”highlighting the essential role each module plays in the overall design.\nFigure 6\n:\nQualitative ablation on\nGEM3D Pretraining\ndesigns\n4.3.2\nAblation on the Number of Pretraining Viewpoints\nIn our experiments, we use multi-view observations from eight surrounding cameras for pretraining. While effective, this configuration is often impractical in real-world deployment. To evaluate the dependence on view count, we vary the number of pretraining views and conduct multi-task training on\nPhone on Base\n,\nMeat on Grill\n, and\nTurn Tap\n.\nTab.\nËœ\n5\nsummarizes the average success rates under different view settings, including a â€œ0-viewâ€ setting where the\nFeed-forward Gaussian Splatting Rendering\nmodule is disabled.\nTable 5\n:\nAblations on pretraining viewpoint numbers (SR%).\nModules\nTrain View\nModerate Shift\nLarge Shift\nTrain View\nNumbers\n8\n58.8\nÂ±\n\\pm\n6.0\n45.9\nÂ±\n\\pm\n8.4\n39.6\nÂ±\n\\pm\n8.5\n4\n54.6\nÂ±\n\\pm\n4.9\n42.4\nÂ±\n\\pm\n6.6\n37.3\nÂ±\n\\pm\n8.8\n2\n54.9\nÂ±\n\\pm\n5.1\n40.3\nÂ±\n\\pm\n5.1\n33.6\nÂ±\n\\pm\n6.8\nw/o gaussian\nsplatting\n47.2\nÂ±\n\\pm\n7.4\n33.4\nÂ±\n\\pm\n6.1\n30.8\nÂ±\n\\pm\n5.8\nWe observe that\nGEM3D Pretraining\nis largely insensitive to the number of available views: reducing the pretraining setup from eight to two cameras leads to only marginal declines in success rates across all viewpoint conditions. In contrast, removing the Gaussian Splatting module yields a pronounced performance drop\nof nearly 10%. This indicates that Gaussian Splatting not only captures fine-grained appearance details but also enforces spatial consistency across viewpoints, enabling strong performance even under limited multi-view supervision.\n4.3.3\nAblation on GEM3D Policy\nIn this section, we perform an ablation study on the effectiveness of the\nGEM3D Policy\narchitecture. Under the same experimental setup as Sec.\n4.3.2\n, we compare two variants:\nâ€¢\nSingle-step Distillation\n: Representation alignment is applied only at the input token level, with the\nt\n+\n1\nt{+}1\nlatent distillation step removed from the multi-step pipeline.\nâ€¢\nPretrain-Finetune\n: A conventional baseline where the pretrained encoder is directly fine-tuned on downstream tasks without distillation.\nTab.\nËœ\n6\nreports the average success rates across three tasks.\nThe results indicate that incorporating distillation at the\nt\n+\n1\nt{+}1\nstep improves temporal consistency in the latent dynamics, leading to noticeably stronger policy performance.\nIn contrast, the conventional pretrain-finetune paradigm suffers from representation drift caused by the domain gap between pretraining and downstream tasks\n[\n13\n,\n26\n]\n, resulting in substantially reduced manipulation success.\nTable 6\n:\nAblations on\nGEM3D\npolicy training (SR%).\nMethods\nTrain View\nModerate Shift\nLarge Shift\nPretrain-Finetune\n32.2\nÂ±\n\\pm\n11.1\n21.1\nÂ±\n\\pm\n6.7\n15.1\nÂ±\n\\pm\n6.8\nSingle-step Distillation\n54.6\nÂ±\n\\pm\n7.6\n41.7\nÂ±\n\\pm\n9.0\n37.7\nÂ±\n\\pm\n8.7\nGEM3D\n(ours)\n58.0\nÂ±\n\\pm\n6.0\n45.9\nÂ±\n\\pm\n8.4\n39.6\nÂ±\n\\pm\n8.5\n5\nConclusion\nWe introduce\nGEM3D\n, a unified representation-policy learning framework that substantially improves view-generalizable robotic manipulation. Extensive experiments demonstrate the effectiveness of our approach.\nWe expect this study to motivate future efforts in two directions:\n(i) advancing large-scale 3D pretraining with broader object collections, with the goal of learning more universal and transferable representations, and\n(ii) integrating distillation-based policy learning with other types of pretrained representations to learn various forms of manipulation skills.\nReferences\n[1]\nD. Charatan, S. L. Li, A. Tagliasacchi, and V. Sitzmann\n(2024)\nPixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 19457â€“19467\n.\nCited by:\nÂ§2.1\n.\n[2]\nS. Chen, R. Garcia, C. Schmid, and I. Laptev\n(2023)\nPolarnet: 3d point clouds for language-guided robotic manipulation\n.\narXiv preprint arXiv:2309.15596\n.\nCited by:\nÂ§2.1\n.\n[3]\nC. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song\n(2024-03)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\narXiv\n.\nExternal Links:\n2303.04137\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[4]\nC. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake, and S. Song\n(2024-03)\nUniversal manipulation interface: in-the-wild robot teaching without in-the-wild robots\n.\narXiv\n.\nExternal Links:\n2402.10329\n,\nDocument\nCited by:\nÂ§1\n.\n[5]\nÃ–. Ã‡iÃ§ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger\n(2016)\n3D u-net: learning dense volumetric segmentation from sparse annotation\n.\nIn\nInternational conference on medical image computing and computer-assisted intervention\n,\npp.Â 424â€“432\n.\nCited by:\n3rd item\n.\n[6]\nD. Driess, I. Schubert, P. Florence, Y. Li, and M. Toussaint\nReinforcement learning with neural radiance fields\n.\nCited by:\nÂ§1\n.\n[7]\nA. Goyal, J. Xu, Y. Guo, V. Blukis, Y. Chao, and D. Fox\n(2023-06)\nRVT: robotic view transformer for 3d object manipulation\n.\narXiv\n.\nExternal Links:\n2306.14896\n,\nDocument\nCited by:\nÂ§1\n.\n[8]\nP. Guhur, S. Chen, R. G. Pinel, M. Tapaswi, I. Laptev, and C. Schmid\n(2023)\nInstruction-driven history-aware policies for robotic manipulations\n.\nIn\nConference on Robot Learning\n,\npp.Â 175â€“187\n.\nCited by:\nÂ§2.1\n.\n[9]\nD. Ha and J. Schmidhuber\n(2018-03)\nWorld models\n.\nExternal Links:\n1803.10122\n,\nDocument\nCited by:\nÂ§1\n.\n[10]\nD. Hafner, T. Lillicrap, and M. Norouzi\n(2020)\nDREAM to control: learning behaviors by latent imagination\n.\nCited by:\nÂ§1\n.\n[11]\nD. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba\n(2020-10)\nMastering atari with discrete world models\n.\nIn\nInternational Conference on Learning Representations\n,\nCited by:\nÂ§1\n.\n[12]\nN. Hansen, H. Su, and X. Wang\n(2024-03)\nTD-mpc2: scalable, robust world models for continuous control\n.\narXiv\n.\nExternal Links:\n2310.16828\nCited by:\nÂ§1\n.\n[13]\nN. Hansen, Z. Yuan, Y. Ze, T. Mu, A. Rajeswaran, H. Su, H. Xu, and X. Wang\n(2022)\nOn pre-training for visuo-motor control: revisiting a learning-from-scratch baseline\n.\narXiv preprint arXiv:2212.05749\n.\nCited by:\nÂ§2.2\n,\nÂ§4.3.3\n.\n[14]\nC. Hou, Y. Ze, Y. Fu, Z. Gao, S. Hu, Y. Yu, S. Zhang, and H. Xu\n(2025)\n4D visual pre-training for robot learning\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.Â 8451â€“8461\n.\nCited by:\nÂ§2.2\n.\n[15]\nA. Jaegle, S. Borgeaud, J. Alayrac, C. Doersch, C. Ionescu, D. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer,\net al.\n(2021)\nPerceiver io: a general architecture for structured inputs & outputs\n.\narXiv preprint arXiv:2107.14795\n.\nCited by:\nÂ§4.1\n.\n[16]\nS. James, Z. Ma, D. R. Arrojo, and A. J. Davison\n(2020)\nRlbench: the robot learning benchmark & learning environment\n.\nIEEE Robotics and Automation Letters\n5\n(\n2\n),\npp.Â 3019â€“3026\n.\nCited by:\nÂ§1\n,\nÂ§4.1\n.\n[17]\nB. Kerbl, G. Kopanas, T. LeimkÃ¼hler, and G. Drettakis\n(2023-08)\n3D gaussian splatting for real-time radiance field rendering\n.\narXiv\n.\nExternal Links:\n2308.04079\n,\nDocument\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nÂ§2.1\n.\n[18]\nM. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang, and C. Finn\n(2024-09)\nOpenVLA: an open-source vision-language-action model\n.\narXiv\n.\nExternal Links:\n2406.09246\nCited by:\nÂ§1\n.\n[19]\nM. Laskin, A. Srinivas, and P. Abbeel\n(2020-11)\nCURL: contrastive unsupervised representations for reinforcement learning\n.\nIn\nProceedings of the 37th International Conference on Machine Learning\n,\npp.Â 5639â€“5650\n.\nExternal Links:\nISSN 2640-3498\nCited by:\nÂ§1\n.\n[20]\nY. Li, Z. Yu, C. Choy, C. Xiao, J. M. Alvarez, S. Fidler, C. Feng, and A. Anandkumar\n(2023)\nVoxformer: sparse voxel transformer for camera-based 3d semantic scene completion\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 9087â€“9098\n.\nCited by:\n2nd item\n.\n[21]\nT. Lin, P. Goyal, R. Girshick, K. He, and P. DollÃ¡r\n(2017)\nFocal loss for dense object detection\n.\nIn\nProceedings of the IEEE international conference on computer vision\n,\npp.Â 2980â€“2988\n.\nCited by:\nÂ§3.1.3\n.\n[22]\nR. Liu, R. Wu, B. Van Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick\n(2023)\nZero-1-to-3: zero-shot one image to 3d object\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp.Â 9298â€“9309\n.\nCited by:\nÂ§2.1\n.\n[23]\nG. Lu, S. Zhang, Z. Wang, C. Liu, J. Lu, and Y. Tang\n(2024-07)\nManiGaussian: dynamic gaussian splatting for multi-task robotic manipulation\n.\narXiv\n.\nExternal Links:\n2403.08321\n,\nDocument\nCited by:\nÂ§1\n,\nÂ§1\n,\nÂ§2.2\n,\nÂ§4.1\n.\n[24]\nB. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng\n(2022-01)\nNeRF: representing scenes as neural radiance fields for view synthesis\n.\nCommunications of the ACM\n65\n(\n1\n),\npp.Â 99â€“106\n.\nExternal Links:\nISSN 0001-0782, 1557-7317\n,\nDocument\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nÂ§2.1\n.\n[25]\nM. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby,\net al.\n(2023)\nDinov2: learning robust visual features without supervision\n.\narXiv preprint arXiv:2304.07193\n.\nCited by:\n2nd item\n.\n[26]\nY. Pang, E. H. F. Tay, L. Yuan, and Z. Chen\n(2023)\nMasked autoencoders for 3d point cloud self-supervised learning\n.\nWorld Scientific Annual Review of Artificial Intelligence\n1\n,\npp.Â 2440001\n.\nCited by:\nÂ§4.3.3\n.\n[27]\nB. Poole, A. Jain, J. T. Barron, and B. Mildenhall\n(2022)\nDreamfusion: text-to-3d using 2d diffusion\n.\narXiv preprint arXiv:2209.14988\n.\nCited by:\nÂ§2.1\n.\n[28]\nS. Qian, K. Mo, V. Blukis, D. F. Fouhey, D. Fox, and A. Goyal\n(2025-03)\n3D-mvp: 3d multiview pretraining for robotic manipulation\n.\narXiv\n.\nExternal Links:\n2406.18158\n,\nDocument\nCited by:\nÂ§1\n,\nÂ§2.2\n.\n[29]\nX. Ren, Y. Lu, H. Liang, Z. Wu, H. Ling, M. Chen, S. Fidler, F. Williams, and J. Huang\n(2024)\nScube: instant large-scale scene reconstruction using voxsplats\n.\nAdvances in Neural Information Processing Systems\n37\n,\npp.Â 97670â€“97698\n.\nCited by:\nÂ§2.1\n.\n[30]\nR. Shah and V. Kumar\n(2021)\nRrl: resnet as representation for reinforcement learning\n.\narXiv preprint arXiv:2107.03380\n.\nCited by:\nÂ§2.1\n.\n[31]\nR. Shi, H. Chen, Z. Zhang, M. Liu, C. Xu, X. Wei, L. Chen, C. Zeng, and H. Su\n(2023)\nZero123++: a single image to consistent multi-view diffusion base model\n.\narXiv preprint arXiv:2310.15110\n.\nCited by:\nÂ§2.1\n.\n[32]\nM. Shridhar, L. Manuelli, and D. Fox\n(2022-11)\nPerceiver-actor: a multi-task transformer for robotic manipulation\n.\narXiv\n.\nExternal Links:\n2209.05451\n,\nDocument\nCited by:\nÂ§1\n.\n[33]\nM. Shridhar, L. Manuelli, and D. Fox\n(2023)\nPerceiver-actor: a multi-task transformer for robotic manipulation\n.\nIn\nConference on Robot Learning\n,\npp.Â 785â€“799\n.\nCited by:\nÂ§2.1\n,\nÂ§4.1\n,\nÂ§4.1\n.\n[34]\nS. Szymanowicz, C. Rupprecht, and A. Vedaldi\n(2024)\nSplatter image: ultra-fast single-view 3d reconstruction\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 10208â€“10217\n.\nCited by:\nÂ§2.1\n.\n[35]\nJ. Tian, L. Wang, S. Zhou, S. Wang, J. Li, H. Sun, and W. Tang\n(2025)\nPDFactor: learning tri-perspective view policy diffusion field for multi-task robotic manipulation\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 15757â€“15767\n.\nCited by:\nÂ§4.1\n.\n[36]\nC. Wang, X. Luo, K. Ross, and D. Li\n(2022)\nVrl3: a data-driven framework for visual deep reinforcement learning\n.\nAdvances in Neural Information Processing Systems\n35\n,\npp.Â 32974â€“32988\n.\nCited by:\nÂ§2.2\n.\n[37]\nJ. Wang, Q. Zhang, J. Sun, J. Cao, G. Han, W. Zhao, W. Zhang, Y. Shao, Y. Guo, and R. Xu\n(2024-08)\nReinforcement learning with generalizable gaussian splatting\n.\narXiv\n.\nExternal Links:\n2404.07950\n,\nDocument\nCited by:\nÂ§1\n.\n[38]\nP. Xiang, X. Wen, Y. Liu, Y. Cao, P. Wan, W. Zheng, and Z. Han\n(2021)\nSnowflakenet: point cloud completion by snowflake point deconvolution with skip-transformer\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp.Â 5499â€“5509\n.\nCited by:\nÂ§1\n,\nFigure 3\n,\nFigure 3\n,\nÂ§3.1.2\n.\n[39]\nT. Xiao, I. Radosavovic, T. Darrell, and J. Malik\n(2022)\nMasked visual pre-training for motor control\n.\narXiv preprint arXiv:2203.06173\n.\nCited by:\nÂ§2.2\n.\n[40]\nG. Yan, Y. Wu, and X. Wang\n(2024)\nDnact: diffusion guided multi-task 3d policy learning\n.\narXiv preprint arXiv:2403.04115\n.\nCited by:\nÂ§2.2\n.\n[41]\nD. Yarats, A. Zhang, I. Kostrikov, B. Amos, J. Pineau, and R. Fergus\n(2020-07)\nImproving sample efficiency in model-free reinforcement learning from images\n.\narXiv\n.\nExternal Links:\n1910.01741\nCited by:\nÂ§1\n.\n[42]\nY. Ze, N. Hansen, Y. Chen, M. Jain, and X. Wang\n(2023)\nVisual reinforcement learning with self-supervised 3d representations\n.\nIEEE Robotics and Automation Letters\n8\n(\n5\n),\npp.Â 2890â€“2897\n.\nCited by:\nÂ§2.2\n.\n[43]\nY. Ze, G. Yan, Y. Wu, A. Macaluso, Y. Ge, J. Ye, N. Hansen, L. E. Li, and X. Wang\n(2023-12)\nGNFactor: multi-task real robot learning with generalizable neural feature fields\n.\nIn\nProceedings of The 7th Conference on Robot Learning\n,\npp.Â 284â€“301\n.\nExternal Links:\nISSN 2640-3498\nCited by:\nÂ§1\n,\nÂ§2.2\n,\nÂ§4.1\n.\n[44]\nY. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, and H. Xu\n(2024-06)\n3D diffusion policy: generalizable visuomotor policy learning via simple 3d representations\n.\narXiv\n.\nExternal Links:\n2403.03954\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[45]\nD. Zhang, B. Lv, H. Zhang, F. Yang, J. Zhao, H. Yu, C. Huang, H. Zhou, C. Ye, and C. Jiang\n(2024-09)\nFocus on what matters: separated models for visual-based rl generalization\n.\narXiv\n.\nExternal Links:\n2410.10834\n,\nDocument\nCited by:\nÂ§1\n.\n[46]\nX. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai\n(2020)\nDeformable detr: deformable transformers for end-to-end object detection\n.\narXiv preprint arXiv:2010.04159\n.\nCited by:\n2nd item\n.\n\\thetitle\nSupplementary Material\nA\nMethod Details\nThis section provides a detailed explanation of the implementation of\nGEM3D Pretraining\n(see Figure 3). We also describe the training strategy and recommended hyperparameters used in the paper.\nA.1\nImplementation Details of\n3D Feature Extractor\nGiven a single-view observation\no\nt\n=\n{\nâ„\nt\n,\nğ’Ÿ\nt\n,\nğ’¦\nt\n}\no^{t}=\\{\\mathcal{I}^{t},\\;\\mathcal{D}^{t},\\;\\mathcal{K}^{t}\\}\nat timestep\nt\nt\n. The camera parameters\nğ’¦\nt\n\\mathcal{K}^{t}\nconsist of two parts-the extrinsic matrix:\nM\nextr\n=\n[\nR\nT\nO\nI\n]\n,\nM_{\\text{extr}}=\\begin{bmatrix}R&T\\\\\nO&I\\end{bmatrix},\nand the intrinsic matrix:\nM\nintr\n=\n[\nf\nx\n0\nc\nx\n0\nf\ny\nc\ny\n0\n0\n1\n]\nM_{\\text{intr}}=\\begin{bmatrix}f_{x}&0&c_{x}\\\\\n0&f_{y}&c_{y}\\\\\n0&0&1\\end{bmatrix}\n.\nThe depth map\nğ’Ÿ\nt\n\\mathcal{D}^{t}\nis back-projected to 3D in two steps.\nFirst, each pixel\n(\nu\n,\nv\n)\n(u,v)\nis lifted into the camera coordinate system using the intrinsics\nM\nintr\nM_{\\text{intr}}\n:\n[\nx\nc\ny\nc\nz\nc\n]\n=\n[\n(\nu\nâˆ’\nc\nx\n)\nâ€‹\nğ’Ÿ\nt\nâ€‹\n(\nu\n,\nv\n)\nf\nx\n(\nv\nâˆ’\nc\ny\n)\nâ€‹\nğ’Ÿ\nt\nâ€‹\n(\nu\n,\nv\n)\nf\ny\nğ’Ÿ\nt\nâ€‹\n(\nu\n,\nv\n)\n]\n.\n\\begin{bmatrix}x_{c}\\\\[2.0pt]\ny_{c}\\\\[2.0pt]\nz_{c}\\end{bmatrix}=\\begin{bmatrix}\\frac{(u-c_{x})\\,\\mathcal{D}^{t}(u,v)}{f_{x}}\\\\[8.0pt]\n\\frac{(v-c_{y})\\,\\mathcal{D}^{t}(u,v)}{f_{y}}\\\\[8.0pt]\n\\mathcal{D}^{t}(u,v)\\end{bmatrix}.\n(1)\nThen, the obtained 3D point is transformed into the world coordinate system using the extrinsics\nM\nextr\nM_{\\text{extr}}\n:\n[\nx\ny\nz\n]\n=\nM\nextr\nâ‹…\n[\nx\nc\ny\nc\nz\nc\n]\n.\n\\begin{bmatrix}x\\\\[2.0pt]\ny\\\\[2.0pt]\nz\\end{bmatrix}=M_{\\text{extr}}\\cdot\\begin{bmatrix}x_{c}\\\\[2.0pt]\ny_{c}\\\\[2.0pt]\nz_{c}\\end{bmatrix}.\n(2)\nPoint clouds from the depth map typically contain a large amount of redundant observations (e.g., background, floor, etc.). Therefore, we first\ncrop\nthe raw point cloud and apply\nfarthest point sampling (FPS)\nto regulate its spatial density and unify the number of points (\nNote:\nthe purpose of downsampling the point cloud to a unified number is to facilitate multi-batch parallel training\n). The processed point cloud is denoted as\nğ’«\n\\mathcal{P}\n.\nHowever, point cloud representation is inherently unstructured, making global reconstruction and compeletion of occlusion part difficult to perform. To alleviate this issue, we introduce a dense volumetric intermediate representation. Specifically, each point in\nğ’«\n\\mathcal{P}\nis assigned color information by associating it with the corresponding RGB pixel and the feature map extracted from the DinoV2. These two 3D point cloud representations are then jointly voxelized to yield volumetric observation, namingly\n3D Occupancy Map\nand\n3D Feature Map\n(see\nFig.\nËœ\n1\n).\nFigure 1\n:\nVolumetric observation of the Single-view input\no\nt\no^{t}\n. The left is the\n3D Occupancy Map\nof size\n(\nd\n3\n,\n10\n)\n(d^{3},10)\n, and the right is the\n3D Feature Map\nof size\n(\nd\n3\n,\n384\n)\n(d^{3},384)\n.\nFeature extraction is conducted in the voxelized space using a\nfusion-style 3D U-Net\n, which combines the\n3D Occupancy Map\nand the\n3D Feature Map\nto produce a spatially expanded dense volumetric feature\nâ„±\n\\mathcal{F}\nof size\n(\nD\n3\n,\n128\n)\n(D^{3},128)\n.\nThe overall procedure can be summarized by the following pseudocode:\nInput:\nâ„\nt\n\\mathcal{I}^{t}\n,\nğ’Ÿ\nt\n\\mathcal{D}^{t}\n,\nğ’¦\nt\n=\n{\nM\nextr\n,\nM\nintr\n}\n\\mathcal{K}^{t}=\\{M_{\\text{extr}},\\,M_{\\text{intr}}\\}\nOutput:\nDense volumetric feature\nâ„±\n\\mathcal{F}\nâ„±\n2\nâ€‹\nD\nâ†\nDinoV2\nâ€‹\n(\nâ„\nt\n)\n\\mathcal{F}_{2D}\\leftarrow\\texttt{DinoV2}(\\mathcal{I}^{t})\n// (\nH\nÃ—\nW\n,\n384\nH\\times W,384\n)\nP\nraw\nâ†\nPixel2Point\nâ€‹\n(\nğ’Ÿ\nt\n,\nğ’¦\nt\n)\nP_{\\text{raw}}\\leftarrow\\textsc{Pixel2Point}(\\mathcal{D}^{t},\\mathcal{K}^{t})\n// Eq.\n1\n, Eq.\n2\nP\ncrop\nâ†\nCrop\nâ€‹\n(\nP\nraw\n)\nP_{\\text{crop}}\\leftarrow\\textsc{Crop}(P_{\\text{raw}})\nP\nâ†\nFPS\nâ€‹\n(\nP\ncrop\n,\nN\n)\nP\\leftarrow\\textsc{FPS}(P_{\\text{crop}},N)\n(\nV\nocc\n,\nV\nfeat\n)\nâ†\nVoxelize\nâ€‹\n(\nP\n,\nâ„\nt\n,\nâ„±\n2\nâ€‹\nD\n)\n(V_{\\text{occ}},V_{\\text{feat}})\\leftarrow\\textsc{Voxelize}(P,\\mathcal{I}^{t},\\mathcal{F}_{2D})\n//\n(\nD\n3\n,\n10\n)\n(D^{3},10)\n,\n(\nD\n3\n,\n384\n)\n(D^{3},384)\nâ„±\nâ†\n3D UNet\nâ€‹\n(\nV\nocc\n,\nV\nfeat\n)\n\\mathcal{F}\\leftarrow\\texttt{3D UNet}(V_{\\text{occ}},V_{\\text{feat}})\n//\n(\nD\n3\n,\n128\n)\n(D^{3},128)\nreturn\nâ„±\n\\mathcal{F}\nAlgorithmÂ 1\n3D Feature Extractor\nA.2\nImplementation Details of\n3D Deformable Attention\nIn our\ncoarse-to-fine point cloud reconstruction\nmodule, we adopt a\n3D Deformable Cross-Attention (DCA)\nmechanism operating over voxel proposal queries.\nSpecifically, the input\nx\nx\ncorresponds to the\nproposal voxel queries\nğ’¬\np\nâˆˆ\nâ„\nd\n3\nÃ—\nC\n\\mathcal{Q}_{p}\\in\\mathbb{R}^{d^{3}\\times C}\nfrom the preceding stage, where\nd\n3\nd^{3}\nis the number of voxel tokens and\nC\nC\nis the channel dimension (here\nC\n=\n128\nC=128\n).\nThe optional context features are provided by the dense volumetric feature\nâ„±\n\\mathcal{F}\nextracted from the fusion-style 3D U-Net.\nFor each voxel query token\nğª\np\nâˆˆ\nğ’¬\np\n\\mathbf{q}_{p}\\in\\mathcal{Q}_{p}\n, the module predicts a set of learnable offsets\n{\nÎ”\nâ€‹\nğ©\nk\nâ€‹\n(\nğª\np\n)\n}\nk\n=\n1\nN\np\n\\{\\Delta\\mathbf{p}_{k}(\\mathbf{q}_{p})\\}_{k=1}^{N_{p}}\naround a reference voxel position\nğ©\n0\n\\mathbf{p}_{0}\n, forming a\nvoxel-wise sampling grid\nv\ngrid\nv_{\\text{grid}}\n.\nKeys and values are then sampled from\nâ„±\n\\mathcal{F}\nat these locations using\nF.grid_sample\n, and attention weights\nA\nk\nâ€‹\n(\nğª\np\n)\nA_{k}(\\mathbf{q}_{p})\nare computed to aggregate local geometric information.\nThe refined token feature, referred to as the\nseed token\n, is obtained as:\nDCA\nâ€‹\n(\nğª\np\n,\nâ„±\n)\n=\n1\nN\np\nâ€‹\nâˆ‘\nk\n=\n1\nN\np\nA\nk\nâ€‹\n(\nğª\np\n)\nâ€‹\nâ„±\nâ€‹\n(\nğ©\n0\n+\nÎ”\nâ€‹\nğ©\nk\nâ€‹\n(\nğª\np\n)\n)\n.\n\\text{DCA}(\\mathbf{q}_{p},\\mathcal{F})=\\frac{1}{N_{p}}\\sum_{k=1}^{N_{p}}A_{k}(\\mathbf{q}_{p})\\,\\mathcal{F}(\\mathbf{p}_{0}+\\Delta\\mathbf{p}_{k}(\\mathbf{q}_{p})).\n(3)\nTo enhance 3D geometric awareness, a\ncontinuous positional bias (CPB)\nis integrated.\nFor each query voxel token\nğª\np\n\\mathbf{q}_{p}\nand its sampled locations\nğ©\n0\n+\nÎ”\nâ€‹\nğ©\nk\nâ€‹\n(\nğª\np\n)\n\\mathbf{p}_{0}+\\Delta\\mathbf{p}_{k}(\\mathbf{q}_{p})\n, CPB produces a learnable bias\nb\np\n,\nk\n=\nCPB\nâ€‹\n(\nğª\np\n,\nğ©\n0\n+\nÎ”\nâ€‹\nğ©\nk\nâ€‹\n(\nğª\np\n)\n)\n,\nb_{p,k}=\\text{CPB}\\big(\\mathbf{q}_{p},\\mathbf{p}_{0}+\\Delta\\mathbf{p}_{k}(\\mathbf{q}_{p})\\big),\n(4)\nwhich is directly added to the raw attention logits before normalization:\nA\n~\nk\nâ€‹\n(\nğª\np\n)\n=\nsoftmax\nâ€‹\n(\nA\nk\nâ€‹\n(\nğª\np\n)\n+\nb\np\n,\nk\n)\n.\n\\tilde{A}_{k}(\\mathbf{q}_{p})=\\text{softmax}\\big(A_{k}(\\mathbf{q}_{p})+b_{p,k}\\big).\n(5)\nTogether with the learned offsets, this allows DCA to selectively aggregate features from spatially relevant voxels, efficiently refining each voxel token with precise local geometric context.\nIn essence, CPB provides a learnable bias that modulates attention weights based on relative 3D positions.\nCompared to standard full-grid 3D cross-attention, which has complexity\nO\nâ€‹\n(\nN\np\nâ‹…\nD\n3\nâ‹…\nC\n)\nO(N_{p}\\cdot D^{3}\\cdot C)\nfor\nN\np\nN_{p}\nqueries and\nD\n3\nD^{3}\nvoxels, DCA reduces both memory and computation to\nO\nâ€‹\n(\nN\np\n2\nâ‹…\nC\n)\nO(N_{p}^{2}\\cdot C)\nby attending only to\nN\np\nâ‰ª\nD\n3\nN_{p}\\ll D^{3}\nsampled locations per token while preserving fine-grained local details.\nBelow is the implementation procedure of 3D DCA.\nInput:\nVoxel proposal queries\nğ’¬\np\nâˆˆ\nâ„\nd\n3\nÃ—\nC\n\\mathcal{Q}_{p}\\in\\mathbb{R}^{d^{3}\\times C}\n, dense volumetric feature\nâ„±\n\\mathcal{F}\n, number of offsets\nN\np\nN_{p}\nOutput:\nRefined voxel tokens\nğ’¬\np\nrefined\n\\mathcal{Q}_{p}^{\\text{refined}}\n{\nÎ”\nâ€‹\nğ©\nk\nâ€‹\n(\nğª\np\n)\n}\nk\n=\n1\nN\np\nâ†\nOffsetPredictor\nâ€‹\n(\nğ’¬\np\n)\n\\{\\Delta\\mathbf{p}_{k}(\\mathbf{q}_{p})\\}_{k=1}^{N_{p}}\\leftarrow\\textsc{OffsetPredictor}(\\mathcal{Q}_{p})\n// Learnable offsets per voxel token\nğ©\nk\nâ†\nğ©\n0\n+\nÎ”\nâ€‹\nğ©\nk\nâ€‹\n(\nğª\np\n)\n\\mathbf{p}_{k}\\leftarrow\\mathbf{p}_{0}+\\Delta\\mathbf{p}_{k}(\\mathbf{q}_{p})\n// Sampling locations in 3D voxel space\nK\n,\nV\nâ†\nF.grid_sample\nâ€‹\n(\nâ„±\n,\nğ©\nk\n)\nK,V\\leftarrow\\texttt{F.grid\\_sample}(\\mathcal{F},\\mathbf{p}_{k})\n// Extract voxel features at sampled offsets\nA\nk\nâ€‹\n(\nğª\np\n)\nâ†\nSoftmax\nâ€‹\n(\nâŸ¨\nğª\np\n,\nK\nâŸ©\n+\nCPB\nâ€‹\n(\nğª\np\n,\nğ©\nk\n)\n)\nA_{k}(\\mathbf{q}_{p})\\leftarrow\\textsc{Softmax}(\\langle\\mathbf{q}_{p},K\\rangle+\\textsc{CPB}(\\mathbf{q}_{p},\\mathbf{p}_{k}))\n// Continuous positional bias modulates attention\nğ’¬\np\nrefined\nâ†\n1\nN\np\nâ€‹\nâˆ‘\nk\n=\n1\nN\np\nA\nk\nâ€‹\n(\nğª\np\n)\nâ€‹\nV\nk\n\\mathcal{Q}_{p}^{\\text{refined}}\\leftarrow\\frac{1}{N_{p}}\\sum_{k=1}^{N_{p}}A_{k}(\\mathbf{q}_{p})\\,V_{k}\n// Seed tokens capturing local geometry\nreturn\nğ’¬\np\nrefined\n\\mathcal{Q}_{p}^{\\text{refined}}\nAlgorithmÂ 2\n3D Deformable Cross-Attention\nDesign Highlights of 3D DCA:\nâ€¢\nAdaptive offsets enable attention to focus on locally informative voxels rather than uniform grids.\nâ€¢\nDense voxel features are queried via the voxel-wise sampling grid using\nF.grid_sample\n, enabling efficient aggregation of local geometric information from the volumetric feature field to each proposal voxel query.\nâ€¢\nContinuous positional bias (CPB) encodes relative 3D positions, improving spatial reasoning and guiding attention toward geometrically relevant locations.\nA.3\nTraning Details\nA.3.1\nTraning Details for\nGEM3D Pretraining\nTraining on robotic scenes with extremely large variance is inherently challenging. In the ablation study (Fig.Â 6 in the main paper), we have visually demonstrated the\nGEM3D Pretraining\nmodule that influence reconstruction and rendering quality.\nBut in practice, we additionally adopt several training techniques that are admittedly heuristic but not novel enough to be presented as core contributions in the main paper. However, these strategies are essential for stable reconstruction. In particular, in order to enable\nGEM3D Pretraining\nto perform feed-forward Gaussian splatting, we employ a series of optimization strategies that are substantially different from traditional 3D Gaussian Splatting. We summarize our key practical insights as follows:\nâ€¢\nStage-wise decoupled reconstruction and rendering.\nPrevious works have pointed out that the quality of Gaussian splatting reconstruction heavily depends on the initialization of point positions. Conventional 3D Gaussian Splatting often relies on Gaussian point cloud initialized from COLMAP. Similarly, during training, the rendering stage (c) strongly depends on the reconstruction quality of stage (b), especially when both modules share the same feature representation\nâ„±\n\\mathcal{F}\nto simultaneously solve two related but distinct tasks: geometry reconstruction and novel view synthesis.\nIf novel view rendering is enforced too early while the reconstruction quality is still poor, the gradients from stage (c) may negatively interfere with the learning of\nâ„±\n\\mathcal{F}\n, leading to unstable convergence and even complete collapse of the reconstructed scene. Therefore, during the first\nÎ´\n\\delta\ntraining iterations\n, we exclusively optimize the reconstruction objective of stage (b) and completely disable the rendering module (c). Only after reliable geometric and appearance-consistent point clouds can be recovered from\nâ„±\n\\mathcal{F}\ndo we enable stage (c) and allow gradients from novel-view rendering to be back-propagated. This strategy provides a stable geometric prior for subsequent Gaussian splatting.\nâ€¢\nMulti-step iterative reconstruction per scene.\nA fundamental difference between feed-forward Gaussian splatting and traditional optimization-based splatting lies in the need for strong cross-scene generalization. In the early training phase, directly switching to a new scene after a single gradient update often leads to reconstruction failure due to large scene variance.\nTo mitigate this issue, we perform\nk\nconsecutive optimization steps\non the same scene before switching to the next one. As training progresses and the model becomes more stable, we gradually decrease\nk\nk\nuntil it eventually becomes\n1\n1\n, corresponding to standard single-step updates per scene.\nA.3.2\nTraning Strategy for\nGEM3D Policy\nDuring policy learning, imitation is performed from human demonstrations.\nHowever, raw trajectories often contain redundant intermediate states (e.g., gripper approaching motions).\nWe therefore select a sparse set of keyframes that correspond to interaction-critical states.\nGiven a demonstration trajectory\nğ’Ÿ\n=\n{\no\nt\n}\nt\n=\n1\nT\n,\n\\mathcal{D}=\\{o_{t}\\}_{t=1}^{T},\nwhere\ng\nt\nâˆˆ\n{\n0\n,\n1\n}\ng_{t}\\in\\{0,1\\}\ndenotes the gripper open/close state, a timestep\nt\nt\nis selected as a keyframe if\nt\nâˆˆ\nğ’¦\nâŸº\n(\ng\nt\nâ‰ \ng\nt\nâˆ’\n1\n)\nâˆ¨\n(\nt\n=\nT\n)\nâˆ¨\nStopped\nâ€‹\n(\no\nt\n)\n,\nt\\in\\mathcal{K}\\;\\;\\Longleftrightarrow\\;\\;(g_{t}\\neq g_{t-1})\\;\\vee\\;(t=T)\\;\\vee\\;\\textsc{Stopped}(o_{t}),\n(6)\nwhere\nStopped\nâ€‹\n(\nâ‹…\n)\n\\textsc{Stopped}(\\cdot)\nindicates near-static motion under a small velocity threshold.\nRedundant adjacent keyframes are further removed.\nThis strategy significantly shortens the demonstration sequence while preserving states that are critical for object interaction.\nA.4\nParamters Details\nTable 1\n:\nKey Hyperparameters of\nGEM3D Pretraining\nParameters\nValue\ntrain.batch_size\n1\ntrain.learning_rate\n1.0\nÃ—\n10\nâˆ’\n4\n1.0\\times 10^{-4}\ntrain.optimizer\ntorch.optim.AdamW\ntrain.steps\n100K\ntrain.num_views\n8\ntrain.\nÎ´\n\\delta\n40K\ntrain.\nk\nk\n[4, 3, 2, 1]\ntrain.demos\n50\nnetwork.\nD\nD\n100\nnetwork.\nd\nd\n[7,7,7]\nnetwork.fps_sample_num\n512\nnetwork.scene_bounds\n[-0.375,-0.5,0.6,1.0,0.5,1.6]\nTable 2\n:\nKey Hyperparameters of\nGEM3D Policy\nParameters\nValue\ntrain.batch_size\n1\ntrain.learning_rate\n1.0\nÃ—\n10\nâˆ’\n4\n1.0\\times 10^{-4}\ntrain.optimizer\ntorch.optim.AdamW\ntrain.single_task_steps\n12K\ntrain.camera\n[front]\ntrain.demos\n20\nnetwork.scene_bounds\n[-0.375,-0.5,0.6,1.0,0.5,1.6]\nnetwork.num_tokens\n8000\nnetwork.num_latents\n2048\nnetwork.\nÎ»\nd\nâ€‹\ni\nâ€‹\ns\nâ€‹\nt\nâ€‹\ni\nâ€‹\nl\nâ€‹\nl\n\\lambda_{d}istill\n[1,0.5,0.3,0.2,0.1,0.05]\nB\nTasks Description\nWe selected 12 tasks from RLBench, which together contain 178 variations. See Fig.\n2\nand Table\n3\nfor detail description.\nFigure 2\n:\nRLBench tasks\n. 12 tasks of 9 scenes are selected.\nNotably, among these 12 tasks,\nclose jar\nand\nopen jar\n,\nmeat off grill\nand\nmeat on grill\n,\nput money in safe\nand\ntake money out safe\nbelong to the same scene but involve different action trajectories. Thus, these 12 tasks only cover 9 distinct scenes, and during pretraining, we only collected tasks from just 9 tasks to cover all of the scenes. This task design aims to evaluate the generalization capability of\nGEM3D\n.\nSpecifically, the trajectories for\nopen jar\n,\nmeat on grill\n, and\ntake money out safe\nwere not seen during pretraining. Nevertheless, Table 1 in the main paper demonstrates that\nGEM3D\nstill improves performance on these extrapolated tasks. Table 2 in main paper further shows that even for tasks with different trajectories within the same scene, our method enhances viewpoint generalization on extrapolated tasks. This indicates that although the pretrained model has not encountered new viewpoints in these tasks, single-view expert demonstrations during policy learning are sufficient to achieve significant improvements in novel viewpoints â€”it highlights that the pretrained representation effectively captures scene-level geometric features, enabling new skill adaptation and continual learning in real-world monocular system deployments.\nTable 3\n:\nTask Descriptions.\nThe 12 highly challenging tasks include a variety of random variations in color and layout, as well as multiple language instruction variations.\nTask\nType\nVariations\nInstruction Template\nClose Jar\ncolor\n20\nâ€œclose the _ jarâ€\nOpen Drawer\nplacement\n3\nâ€œopen the _ drawerâ€\nMeat off Grill\nsize\n2\nâ€œtake the _ off the grillâ€\nTurn Tap\ncategory\n2\nâ€œturn _ tapâ€\nWater Plants\nplacement\n1\nâ€œwater plantâ€\nPhone on Base\nplacement\n1\nâ€œput the phone on the baseâ€\nPut Money in Safe\nplacement\n3\nâ€œput the money away in the safe on the _ shelfâ€\nPlace Wine\nplacement\n1\nâ€œstack wine bottleâ€\nStack Blocks\ncolor, count\n60\nâ€œstack _ _ blocksâ€\nOpen Jar\ncolor\n80\nâ€œopen the _ jarâ€\nMeat on Grill\ncategory\n2\nâ€œput the _ on the grillâ€\nTake Money out Safe\nplacement\n3\nâ€œtake the money out of the _ shelf and place it on the table.â€\nC\nSupplementary of Experiments\nC.1\nAdditional Details of Manipulation Tasks\nTable 2 in the main paper reports results on six representative tasks. We further evaluate two additional heavily occluded tasks,\nopen drawer\nand\ntake money out safe\n, in Table\n4\n. We observe that performance drops more noticeably under viewpoint changes for these tasks, especially under the large-shift setting, where execution almost completely fails. This is because, under large shifts, one side of the scene is nearly entirely occluded (as shown in\nFig.\nËœ\n3\n), leading to severe inconsistencies in visual representations across viewpoints, which the model cannot effectively resolve, ultimately causing execution collapse. It shows that\nGEM3D\nstill relies on the visibility of essential contact details during manipulation.\nFigure 3\n:\nopen drawer\nand\ntake money out safe\ntasks in large view shift\n. Both of the tasks suffer the severe occulusion, resulting drastic success rate drop.\nTable 4\n:\nResult of zero-shot view generalization on eight RLBench tasks under two view-shift settings (SR%).\nBold\nnumbers denote the best performance, and\nâ†“\n\\downarrow\nindicates the relative performance drop compared to the training view.\nMethods\nClose\nJar\nMeat off\nGrill\nTurn\nTap\nPhone on\nBase\nOpen\nJar\nMeat on\nGrill\nOpen\nDrawer\nTake Money\nout Safe\nAvg.\nTrain\nView\nPerAct\n19.0\nÂ±\n\\pm\n4.6\n28.3\nÂ±\n\\pm\n4.7\n43.7\nÂ±\n\\pm\n4.7\n51.3\nÂ±\n\\pm\n11.9\n26.7\nÂ±\n\\pm\n9.1\n16.0\nÂ±\n\\pm\n7.9\n38.7\nÂ±\n\\pm\n13.7\n13.7\nÂ±\n\\pm\n2.9\n29.7\nÂ±\n\\pm\n13.7\nManiGaussian\n27.3\nÂ±\n\\pm\n16.4\n30.0\nÂ±\n\\pm\n11.3\n44.0\nÂ±\n\\pm\n10.5\n66.3\nÂ±\n\\pm\n12.5\n38.3\nÂ±\n\\pm\n9.0\n50.7\nÂ±\n\\pm\n12.9\n57.3\nÂ±\n\\pm\n1.5\n49.3\nÂ±\n\\pm\n10.5\n42.9\nÂ±\n\\pm\n14.0\nManiGaussian\n(w Distill)\n31.0\nÂ±\n\\pm\n12.1\n35.3\nÂ±\n\\pm\n9.8\n42.7\nÂ±\n\\pm\n14.6\n64.3\nÂ±\n\\pm\n16.2\n33.7\nÂ±\n\\pm\n9.5\n48.7\nÂ±\n\\pm\n2.9\n54.3\nÂ±\n\\pm\n12.5\n43.0\nÂ±\n\\pm\n23.4\n44.1\nÂ±\n\\pm\n11.3\nGEM3D\n(ours)\n44.7\nÂ±\n\\pm\n3.2\n44.3\nÂ±\n\\pm\n3.1\n56.0\nÂ±\n\\pm\n7.2\n67.3\nÂ±\n\\pm\n1.5\n40.0\nÂ±\n\\pm\n5.6\n61.0\nÂ±\n\\pm\n22.1\n65.7\nÂ±\n\\pm\n4.5\n72.0\nÂ±\n\\pm\n13.7\n56.4\nÂ±\n\\pm\n12.1\nModerate\nShift\nPerAct\n7.7\nÂ±\n\\pm\n9.9\n(59.65%\nâ†“\n\\downarrow\n)\n19.7\nÂ±\n\\pm\n7.2\n(30.56%\nâ†“\n\\downarrow\n)\n14.0\nÂ±\n\\pm\n7.2\n(67.94%\nâ†“\n\\downarrow\n)\n0.3\nÂ±\n\\pm\n0.6\n(99.35%\nâ†“\n\\downarrow\n)\n2.0\nÂ±\n\\pm\n1.7\n(92.50%\nâ†“\n\\downarrow\n)\n12.3\nÂ±\n\\pm\n9.6\n(22.92%\nâ†“\n\\downarrow\n)\n19.7\nÂ±\n\\pm\n7.2\n(49.14%\nâ†“\n\\downarrow\n)\n3.3\nÂ±\n\\pm\n3.2\n(75.61%\nâ†“\n\\downarrow\n)\n9.9\nÂ±\n\\pm\n7.7\n(62.21%\nâ†“\n\\downarrow\n)\nManiGaussian\n15.3\nÂ±\n\\pm\n15.9\n(43.91%\nâ†“\n\\downarrow\n)\n17.0\nÂ±\n\\pm\n12.1\n(43.33%\nâ†“\n\\downarrow\n)\n32.3\nÂ±\n\\pm\n2.3\n(26.52%\nâ†“\n\\downarrow\n)\n30.7\nÂ±\n\\pm\n18.6\n(53.76%\nâ†“\n\\downarrow\n)\n18.3\nÂ±\n\\pm\n2.5\n(52.18%\nâ†“\n\\downarrow\n)\n24.3\nÂ±\n\\pm\n10.1\n(20.67%\nâ†“\n\\downarrow\n)\n30.3\nÂ±\n\\pm\n18.0\n(47.09%\nâ†“\n\\downarrow\n)\n12.0\nÂ±\n\\pm\n4.4\n(75.67%\nâ†“\n\\downarrow\n)\n22.5\nÂ±\n\\pm\n7.9\n(45.39%\nâ†“\n\\downarrow\n)\nManiGaussian\n(w Distill)\n20.3\nÂ±\n\\pm\n10.3\n(34.41%\nâ†“\n\\downarrow\n)\n23.7\nÂ±\n\\pm\n2.3\n(33.01%\nâ†“\n\\downarrow\n)\n34.0\nÂ±\n\\pm\n9.8\n(20.32%\nâ†“\n\\downarrow\n)\n40.0\nÂ±\n\\pm\n22.6\n(37.82%\nâ†“\n\\downarrow\n)\n21.7\nÂ±\n\\pm\n3.2\n(35.64%\nâ†“\n\\downarrow\n)\n42.3\nÂ±\n\\pm\n7.0\n(13.02%\nâ†“\n\\downarrow\n)\n31.7\nÂ±\n\\pm\n4.5\n(41.71%\nâ†“\n\\downarrow\n)\n21.0\nÂ±\n\\pm\n14.8\n(51.16%\nâ†“\n\\downarrow\n)\n29.3\nÂ±\n\\pm\n8.9\n(33.39%\nâ†“\n\\downarrow\n)\nGEM3D\n(ours)\n33.7\nÂ±\n\\pm\n8.5\n(24.62%\nâ†“\n\\downarrow\n)\n33.0\nÂ±\n\\pm\n5.0\n(25.56%\nâ†“\n\\downarrow\n)\n47.7\nÂ±\n\\pm\n8.4\n(14.88%\nâ†“\n\\downarrow\n)\n48.3\nÂ±\n\\pm\n13.6\n(28.22%\nâ†“\n\\downarrow\n)\n34.3\nÂ±\n\\pm\n4.0\n(14.18%\nâ†“\n\\downarrow\n)\n47.0\nÂ±\n\\pm\n14.8\n(22.95%\nâ†“\n\\downarrow\n)\n42.7\nÂ±\n\\pm\n9.5\n(35.02%\nâ†“\n\\downarrow\n)\n41.7\nÂ±\n\\pm\n13.6\n(42.13%\nâ†“\n\\downarrow\n)\n41.0\nÂ±\n\\pm\n6.5\n(25.95%\nâ†“\n\\downarrow\n)\nLarge\nShift\nPerAct\n2.8\nÂ±\n\\pm\n3.4\n(85.11%\nâ†“\n\\downarrow\n)\n15.7\nÂ±\n\\pm\n3.1\n(44.69%\nâ†“\n\\downarrow\n)\n9.7\nÂ±\n\\pm\n4.3\n(77.86%\nâ†“\n\\downarrow\n)\n1.0\nÂ±\n\\pm\n2.0\n(98.05%\nâ†“\n\\downarrow\n)\n3.3\nÂ±\n\\pm\n4.1\n(87.51%\nâ†“\n\\downarrow\n)\n4.8\nÂ±\n\\pm\n3.8\n(69.81%\nâ†“\n\\downarrow\n)\n1.5\nÂ±\n\\pm\n1.9\n(96.12%\nâ†“\n\\downarrow\n)\n1.2\nÂ±\n\\pm\n2.4\n(91.44%\nâ†“\n\\downarrow\n)\n5.0\nÂ±\n\\pm\n5.2\n(81.32%\nâ†“\n\\downarrow\n)\nManiGaussian\n10.3\nÂ±\n\\pm\n14.2\n(62.20%\nâ†“\n\\downarrow\n)\n16.3\nÂ±\n\\pm\n8.4\n(45.57%\nâ†“\n\\downarrow\n)\n27.0\nÂ±\n\\pm\n7.0\n(38.64%\nâ†“\n\\downarrow\n)\n23.7\nÂ±\n\\pm\n16.0\n(64.32%\nâ†“\n\\downarrow\n)\n21.5\nÂ±\n\\pm\n6.9\n(43.91%\nâ†“\n\\downarrow\n)\n16.0\nÂ±\n\\pm\n6.2\n(47.83%\nâ†“\n\\downarrow\n)\n0.0\nÂ±\n\\pm\n0.0\n(100.00%\nâ†“\n\\downarrow\n)\n1.3\nÂ±\n\\pm\n1.8\n(97.30%\nâ†“\n\\downarrow\n)\n14.5\nÂ±\n\\pm\n10.0\n(62.47%\nâ†“\n\\downarrow\n)\nManiGaussian\n(w Distill)\n21.5\nÂ±\n\\pm\n9.2\n(45.16%\nâ†“\n\\downarrow\n)\n16.3\nÂ±\n\\pm\n6.9\n(67.92%\nâ†“\n\\downarrow\n)\n32.0\nÂ±\n\\pm\n8.6\n(29.69%\nâ†“\n\\downarrow\n)\n33.7\nÂ±\n\\pm\n13.1\n(41.97%\nâ†“\n\\downarrow\n)\n23.2\nÂ±\n\\pm\n12.0\n(62.38%\nâ†“\n\\downarrow\n)\n28.2\nÂ±\n\\pm\n8.0\n(53.42%\nâ†“\n\\downarrow\n)\n0.7\nÂ±\n\\pm\n1.2\n(98.77%\nâ†“\n\\downarrow\n)\n5.3\nÂ±\n\\pm\n7.4\n(87.60%\nâ†“\n\\downarrow\n)\n20.1\nÂ±\n\\pm\n12.0\n(60.86%\nâ†“\n\\downarrow\n)\nGEM3D\n(ours)\n30.2\nÂ±\n\\pm\n7.1\n(32.46%\nâ†“\n\\downarrow\n)\n26.0\nÂ±\n\\pm\n5.8\n(41.35%\nâ†“\n\\downarrow\n)\n45.3\nÂ±\n\\pm\n6.3\n(19.05%\nâ†“\n\\downarrow\n)\n39.0\nÂ±\n\\pm\n13.2\n(42.08%\nâ†“\n\\downarrow\n)\n35.8\nÂ±\n\\pm\n7.7\n(10.42%\nâ†“\n\\downarrow\n)\n40.3\nÂ±\n\\pm\n6.8\n(33.88%\nâ†“\n\\downarrow\n)\n11.0\nÂ±\n\\pm\n15.0\n(83.25%\nâ†“\n\\downarrow\n)\n8.2\nÂ±\n\\pm\n10.0\n(88.56%\nâ†“\n\\downarrow\n)\n29.5\nÂ±\n\\pm\n13.7\n(43.88%\nâ†“\n\\downarrow\n)\nAdditionally, we provide a more detailed analysis of the multi-task setting reported in TableÂ 3 of the main paper. Table\n5\nreports the per-task success rates under multi-task training, together with the relative performance changes compared to the single-task setting.\nTable 5\n:\nResult of Multi-task view generalization on RLBench tasks.\nWe report the success rate (mean\nÂ±\n\\pm\nstd %) in the Multi-task setting.\nBold\nnumbers denote the best performance.\nThe percentage in parentheses indicates the relative performance change compared to the Single-task baseline (\nMulti\nâˆ’\nSingle\nSingle\n\\frac{\\text{Multi}-\\text{Single}}{\\text{Single}}\n).\nMethods\nOpen\nJar\nMeat on\nGrill\nTurn\nTap\nPhone on\nBase\nAvg.\nModerate\nShift\nPerAct\n4.3\nÂ±\n\\pm\n4.0\n(115.00%\nâ†‘\n\\uparrow\n)\n2.0\nÂ±\n\\pm\n2.6\n(83.74%\nâ†“\n\\downarrow\n)\n15.7\nÂ±\n\\pm\n5.0\n(12.14%\nâ†‘\n\\uparrow\n)\n3.7\nÂ±\n\\pm\n6.4\n(1133.33%\nâ†‘\n\\uparrow\n)\n6.4\nÂ±\n\\pm\n4.5\n(11.11%\nâ†“\n\\downarrow\n)\nManiGaussian\n(w Distill)\n21.0\nÂ±\n\\pm\n7.9\n(3.23%\nâ†“\n\\downarrow\n)\n28.7\nÂ±\n\\pm\n10.1\n(32.15%\nâ†“\n\\downarrow\n)\n44.3\nÂ±\n\\pm\n9.1\n(30.29%\nâ†‘\n\\uparrow\n)\n28.7\nÂ±\n\\pm\n5.1\n(28.25%\nâ†“\n\\downarrow\n)\n30.7\nÂ±\n\\pm\n8.1\n(11.01%\nâ†“\n\\downarrow\n)\nGEM3D\n(ours)\n27.0\nÂ±\n\\pm\n13.0\n(21.28%\nâ†“\n\\downarrow\n)\n38.7\nÂ±\n\\pm\n14.5\n(17.66%\nâ†“\n\\downarrow\n)\n59.0\nÂ±\n\\pm\n4.6\n(23.69%\nâ†‘\n\\uparrow\n)\n40.0\nÂ±\n\\pm\n6.2\n(17.18%\nâ†“\n\\downarrow\n)\n41.2\nÂ±\n\\pm\n9.6\n(7.00%\nâ†“\n\\downarrow\n)\nLarge\nShift\nPerAct\n5.3\nÂ±\n\\pm\n5.2\n(60.61%\nâ†‘\n\\uparrow\n)\n1.2\nÂ±\n\\pm\n1.9\n(75.00%\nâ†“\n\\downarrow\n)\n14.5\nÂ±\n\\pm\n5.4\n(49.48%\nâ†‘\n\\uparrow\n)\n1.5\nÂ±\n\\pm\n1.8\n(50.00%\nâ†‘\n\\uparrow\n)\n5.6\nÂ±\n\\pm\n3.6\n(19.15%\nâ†‘\n\\uparrow\n)\nManiGaussian\n(w Distill)\n22.2\nÂ±\n\\pm\n7.2\n(16.23%\nâ†“\n\\downarrow\n)\n26.3\nÂ±\n\\pm\n9.3\n(6.05%\nâ†‘\n\\uparrow\n)\n36.0\nÂ±\n\\pm\n10.2\n(20.00%\nâ†‘\n\\uparrow\n)\n21.3\nÂ±\n\\pm\n5.7\n(42.90%\nâ†“\n\\downarrow\n)\n26.5\nÂ±\n\\pm\n8.1\n(10.77%\nâ†“\n\\downarrow\n)\nGEM3D\n(ours)\n27.0\nÂ±\n\\pm\n10.7\n(9.40%\nâ†“\n\\downarrow\n)\n31.3\nÂ±\n\\pm\n7.4\n(15.86%\nâ†“\n\\downarrow\n)\n47.0\nÂ±\n\\pm\n12.6\n(3.75%\nâ†‘\n\\uparrow\n)\n40.3\nÂ±\n\\pm\n5.4\n(3.33%\nâ†‘\n\\uparrow\n)\n36.4\nÂ±\n\\pm\n9.0\n(3.70%\nâ†“\n\\downarrow\n)\nC.2\nMore Rendering Results\nIn the main paper, we present qualitative visualizations on the tasks of\nopen drawer\n,\nturn tap\n, and\nwater plants\n, and quantitatively evaluate the PSNR and SSIM metrics for novel view synthesis. Specifically, for all keyframe sequences, we synthesize novel views from 18 uniformly distributed virtual cameras under 8 supervised viewpoints. In this section, we provide more demonstrations of novel view synthesis results. (See\nFig.\nËœ\n4\n,\nFig.\nËœ\n5\n)\nFigure 4\n:\nRendering Results (Part 1/2).\nFigure 5\n:\nRendering Results (Part 2/2).",
    "preview_text": "Real-world robotic manipulation demands visuomotor policies capable of robust spatial scene understanding and strong generalization across diverse camera viewpoints. While recent advances in 3D-aware visual representations have shown promise, they still suffer from several key limitations, including reliance on multi-view observations during inference which is impractical in single-view restricted scenarios, incomplete scene modeling that fails to capture holistic and fine-grained geometric structures essential for precise manipulation, and lack of effective policy training strategies to retain and exploit the acquired 3D knowledge. To address these challenges, we present MethodName, a unified representation-policy learning framework for view-generalizable robotic manipulation. MethodName introduces a single-view 3D pretraining paradigm that leverages point cloud reconstruction and feed-forward gaussian splatting under multi-view supervision to learn holistic geometric representations. During policy learning, MethodName performs multi-step distillation to preserve the pretrained geometric understanding and effectively transfer it to manipulation skills. We conduct experiments on 12 RLBench tasks, where our approach outperforms the previous state-of-the-art method by 12.7% in average success rate. Further evaluation on six representative tasks demonstrates strong zero-shot view generalization, with success rate drops of only 22.0% and 29.7% under moderate and large viewpoint shifts respectively, whereas the state-of-the-art method suffers larger decreases of 41.6% and 51.5%.\n\nLearning Geometrically-Grounded 3D Visual Representations\nfor View-Generalizable Robotic Manipulation\nDi Zhang\n1âˆ—\nWeicheng Duan\n1âˆ—\nDasen Gu\n1\nHongye Lu\n1\nHai Zhang\n2\nHang Yu\n1\nJunqiao Zhao\n1â€ \nGuang Chen\n1\n1\nTongji University\n2\nThe University of Hong Kong\nAbstract\nâˆ—\nEqual contribution. Contact\n{2331922, 2252109}@tongji.edu.cn\nâ€ \nCorresponding author. Contact\nzhaojunqiao@tongji.edu.cn\nProject Websi",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "VLA",
        "locomotion",
        "whole body control",
        "Vision-Language-Action Model",
        "VLA for Robotics",
        "Flow Matching",
        "Diffusion",
        "Inference Acceleration",
        "Edge Deployment",
        "fine tune",
        "offline Reinforcement Learning",
        "constrained reinforcement learning",
        "safe reinforcement learning",
        "goal conditioned reinforcement learning",
        "world model"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºæœºå™¨äººæ“ä½œçš„å‡ ä½•åŸºç¡€3Dè§†è§‰è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œä¸“æ³¨äºå•è§†å›¾æ¨ç†å’Œè§†å›¾æ³›åŒ–ï¼Œä½†ä¸å¼ºåŒ–å­¦ä¹ ã€VLAç­‰å…³é”®è¯çš„ç›´æ¥å…³è”è¾ƒå¼±ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šGEM3Dâ€”â€”ç”¨äºè§†è§’æ³›åŒ–æœºå™¨äººæ“ä½œçš„å‡ ä½•åŸºç¡€3Dè§†è§‰è¡¨å¾å­¦ä¹ \n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nç°å®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œè¦æ±‚è§†è§‰è¿åŠ¨ç­–ç•¥å…·å¤‡å¼ºå¤§çš„ç©ºé—´åœºæ™¯ç†è§£å’Œè·¨ä¸åŒç›¸æœºè§†è§’çš„æ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰åŸºäº2Dç‰¹å¾çš„ç­–ç•¥ç¼ºä¹æ˜ç¡®çš„3Dç»“æ„æ„ŸçŸ¥ï¼Œè€Œç°æœ‰çš„3Dæ„ŸçŸ¥æ–¹æ³•ï¼ˆå¦‚NeRFã€é«˜æ–¯æ³¼æº…ï¼‰é€šå¸¸å­˜åœ¨**ä¾èµ–å¤šè§†è§’æ¨ç†ã€åœºæ™¯å»ºæ¨¡ç²—ç³™ã€ç¼ºä¹æœ‰æ•ˆç­–ç•¥è®­ç»ƒç­–ç•¥**ç­‰å±€é™ï¼Œéš¾ä»¥åœ¨å•è§†è§’å—é™çš„å®é™…åœºæ™¯ä¸­å®ç°ç²¾ç¡®ã€é²æ£’çš„æ“ä½œã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡ºäº† **GEM3D**ï¼Œä¸€ä¸ªç»Ÿä¸€çš„è¡¨å¾-ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š\n- **å‡ ä½•åŸºç¡€çš„å•è§†è§’é¢„è®­ç»ƒèŒƒå¼**ï¼šé€šè¿‡**ç‚¹äº‘é‡å»º**å’Œ**å‰é¦ˆé«˜æ–¯æ³¼æº…æ¸²æŸ“**ï¼Œåœ¨**å¤šè§†è§’ç›‘ç£**ä¸‹å­¦ä¹ **æ•´ä½“å‡ ä½•è¡¨å¾**ã€‚å…·ä½“é‡‡ç”¨**ç²—åˆ°ç»†çš„â€œé›ªèŠ±â€ç‚¹äº‘é‡å»º**å’Œ**3Då¯å˜å½¢äº¤å‰æ³¨æ„åŠ›**ï¼Œä»¥ä»å•è§†è§’RGB-Dè¾“å…¥ä¸­æ¢å¤ç²¾ç»†çš„å‡ ä½•ä¸çº¹ç†ç»†èŠ‚ã€‚\n- **å¤šæ­¥è’¸é¦ç­–ç•¥å­¦ä¹ **ï¼šæå‡ºä¸€ç§**è’¸é¦å¼è§†è§‰è¿åŠ¨ç­–ç•¥**ï¼Œé€šè¿‡**ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±**å°†é¢„è®­ç»ƒè·å¾—çš„3Då‡ ä½•çŸ¥è¯†**è½¯å¯¹é½**åˆ°ç­–ç•¥ç¼–ç å™¨çš„æ½œåœ¨ç‰¹å¾ä¸­ï¼Œè€Œéç›´æ¥å¾®è°ƒï¼Œæœ‰æ•ˆé¿å…äº†è¡¨å¾æ¼‚ç§»ï¼Œå¹¶å¼•å…¥äº†**éšå¼æ½œåœ¨åŠ¨åŠ›å­¦æ¨¡å‹**ä»¥å¢å¼ºæ„ŸçŸ¥-åŠ¨ä½œä¸€è‡´æ€§ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nåœ¨12ä¸ªRLBenchä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼š\n- **æ€§èƒ½é¢†å…ˆ**ï¼šGEM3Dçš„å¹³å‡æˆåŠŸç‡æ¯”ä¹‹å‰çš„SOTAæ–¹æ³•ï¼ˆManiGaussianï¼‰**é«˜å‡º12.7%**ã€‚\n- **è§†è§’æ³›åŒ–èƒ½åŠ›å¼º**ï¼šåœ¨**ä¸­ç­‰**å’Œ**å¤§**è§†è§’åç§»ä¸‹è¿›è¡Œé›¶æ ·æœ¬æµ‹è¯•ï¼ŒGEM3Dçš„æˆåŠŸç‡ä»…åˆ†åˆ«ä¸‹é™**22.0%** å’Œ **29.7%**ï¼Œè¿œä¼˜äºSOTAæ–¹æ³•çš„**41.6%** å’Œ **51.5%** çš„ä¸‹é™ã€‚\n- **é‡å»ºè´¨é‡é«˜**ï¼šåœ¨åœºæ™¯é‡å»ºçš„å®šé‡æŒ‡æ ‡ï¼ˆPSNR, SSIM, Chamferè·ç¦»ï¼‰ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œè¯æ˜äº†å…¶å­¦ä¹ åˆ°çš„3Dè¡¨å¾å…·æœ‰æ›´é«˜çš„ä¿çœŸåº¦å’Œå‡ ä½•ä¸€è‡´æ€§ã€‚\n- **æ¶ˆèå®éªŒ**éªŒè¯äº†æ¡†æ¶ä¸­å„ä¸ªç»„ä»¶ï¼ˆå¦‚å¯å˜å½¢æ³¨æ„åŠ›ã€é›ªèŠ±é‡å»ºã€ç„¦ç‚¹æŸå¤±ã€å¤šæ­¥è’¸é¦ï¼‰çš„å¿…è¦æ€§ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\nGEM3Dä¸º**è§†è§’æ³›åŒ–çš„æœºå™¨äººæ“ä½œ**æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å…¶ä»·å€¼åœ¨äºï¼š\n- **æŠ€æœ¯è´¡çŒ®**ï¼šé¦–æ¬¡å°†**å‡ ä½•åŸºç¡€çš„å•è§†è§’3Dé‡å»º**ä¸**è’¸é¦å¼ç­–ç•¥å­¦ä¹ **ç´§å¯†ç»“åˆï¼Œåœ¨ä¿æŒæ¨ç†æ•ˆç‡ï¼ˆå•è§†è§’ï¼‰çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†ç­–ç•¥çš„ç©ºé—´ç†è§£èƒ½åŠ›å’Œè·¨è§†è§’é²æ£’æ€§ã€‚\n- **å®é™…æ„ä¹‰**ï¼šè¯¥æ–¹æ³•é™ä½äº†å¯¹éƒ¨ç½²æ—¶å¤šç›¸æœºç³»ç»Ÿçš„ä¾èµ–ï¼Œæ›´è´´è¿‘å®é™…æœºå™¨äººåº”ç”¨åœºæ™¯ï¼Œä¸ºåœ¨å¤æ‚ã€åŠ¨æ€ç¯å¢ƒä¸­å®ç°é²æ£’çš„æœºå™¨äººæ“ä½œé“ºå¹³äº†é“è·¯ã€‚\n- **å¯å‘æ–¹å‘**ï¼šå·¥ä½œä¸ºæœªæ¥**å¤§è§„æ¨¡3Dé¢„è®­ç»ƒ**å’Œ**ä¸å…¶ä»–é¢„è®­ç»ƒè¡¨å¾ç»“åˆçš„è’¸é¦ç­–ç•¥å­¦ä¹ **æŒ‡æ˜äº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**æœºå™¨äººè§†è§‰è¿åŠ¨ç­–ç•¥åœ¨çœŸå®ä¸–ç•Œéƒ¨ç½²ä¸­çš„è§†è§’æ³›åŒ–éš¾é¢˜**ã€‚å…·ä½“è€Œè¨€ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨ä¸‰ä¸ªå…³é”®å±€é™ï¼š\n1.  **æ¨ç†æ—¶ä¾èµ–å¤šè§†è§’è§‚æµ‹**ï¼šä¸é€‚ç”¨äºå•ç›®ç›¸æœºç­‰å—é™åœºæ™¯ã€‚\n2.  **åœºæ™¯å»ºæ¨¡ä¸å®Œæ•´**ï¼šæ— æ³•æ•è·å¯¹ç²¾ç¡®æ“ä½œè‡³å…³é‡è¦çš„**æ•´ä½“å’Œç»†ç²’åº¦å‡ ä½•ç»“æ„**ã€‚\n3.  **ç­–ç•¥è®­ç»ƒç­–ç•¥ä½æ•ˆ**ï¼šç¼ºä¹æœ‰æ•ˆæ–¹æ³•å°†é¢„è®­ç»ƒè·å¾—çš„3DçŸ¥è¯†ä¿ç•™å¹¶è¿ç§»åˆ°æ“æ§æŠ€èƒ½ä¸­ã€‚\n\n### **æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº† **GEM3D**ï¼ˆGeometrically-Grounded 3D Manipulationï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ª**ç»Ÿä¸€çš„è¡¨å¾-ç­–ç•¥å­¦ä¹ æ¡†æ¶**ï¼Œå…¶åˆ›æ–°æ€§ä½“ç°åœ¨ä»¥ä¸‹ç´§å¯†å…³è”çš„ä¸¤ä¸ªå±‚é¢ï¼š\n\n#### **1. å‡ ä½•æ¥åœ°çš„3Dè¡¨å¾é¢„è®­ç»ƒèŒƒå¼**\nè¿™æ˜¯GEM3Dçš„åŸºçŸ³ï¼Œæ—¨åœ¨ä»**å•è§†è§’RGB-Dè¾“å…¥**å­¦ä¹ **å®Œæ•´ã€ç²¾ç»†çš„3Dåœºæ™¯ç†è§£**ã€‚å…¶åˆ›æ–°æµç¨‹å¦‚ä¸‹ï¼š\n- **è¾“å…¥ä¸ç‰¹å¾æå–**ï¼šå°†å•è§†è§’RGB-Då›¾åƒé€šè¿‡DINOv2å’Œ3D U-Netç¼–ç ä¸º**å¯†é›†ä½“ç´ ç‰¹å¾**ï¼Œèåˆäº†å‡ ä½•ä¸è¯­ä¹‰ä¿¡æ¯ã€‚\n- **æ ¸å¿ƒä¸‰æ­¥é¢„è®­ç»ƒ**ï¼š\n    1.  **æ•´ä½“å‡ ä½•é‡å»º**ï¼šé‡‡ç”¨**ç”±ç²—åˆ°ç»†çš„â€œé›ªèŠ±â€ç‚¹äº‘é‡å»º**ã€‚å…ˆé€šè¿‡å¯å­¦ä¹ ä½“ç´ æŸ¥è¯¢å’Œ**3Då¯å˜å½¢äº¤å‰æ³¨æ„åŠ›**ç”Ÿæˆç¨€ç–ç§å­ç‚¹ï¼Œå†é€šè¿‡Snowflake Point Deconvolutioné€æ­¥ä¸Šé‡‡æ ·ã€ç»†åŒ–ï¼Œç”Ÿæˆç¨ å¯†ç‚¹äº‘ã€‚ä½¿ç”¨å€’è§’è·ç¦»ç›‘ç£ï¼Œç¡®ä¿æ•è·**æ•´ä½“åœºæ™¯ç»“æ„**ã€‚\n    2.  **ç»†ç²’åº¦å¤–è§‚å»ºæ¨¡**ï¼šåŸºäºé‡å»ºçš„ç‚¹äº‘ï¼Œé€šè¿‡**å‰é¦ˆå¼3Dé«˜æ–¯æ³¼æº…**è¿›è¡Œæ–°è§†è§’æ¸²æŸ“ã€‚ä»ä½“ç´ ç‰¹å¾ä¸­é‡‡æ ·é«˜æ–¯å‚æ•°ï¼Œåœ¨**å¤šè§†è§’ç›‘ç£ä¸‹**ï¼ˆä½¿ç”¨**Focal Loss**å¼ºè°ƒåŠ¨æ€å’Œæ¨¡ç³ŠåŒºåŸŸï¼‰å­¦ä¹ ç»†ç²’åº¦çš„çº¹ç†å’Œå¤–è§‚ã€‚\n    3.  **è¡¨å¾è¾“å‡º**ï¼šæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªèƒ½å¤Ÿä»å•è§†å›¾æ¨æ–­åœºæ™¯å®Œæ•´å‡ ä½•å’Œå¤–è§‚çš„**é€šç”¨ã€è§†è§’ä¸å˜çš„å¯†é›†ä½“ç´ ç‰¹å¾**ã€‚\n\n#### **2. åŸºäºå¤šæ­¥è’¸é¦çš„ç­–ç•¥å­¦ä¹ æœºåˆ¶**\nä¸ºäº†è§£å†³é¢„è®­ç»ƒ-å¾®è°ƒä¸­çš„è¡¨å¾æ¼‚ç§»é—®é¢˜ï¼ŒGEM3Dæ²¡æœ‰ç›´æ¥å¾®è°ƒé¢„è®­ç»ƒç¼–ç å™¨ï¼Œè€Œæ˜¯æå‡ºäº†ä¸€ä¸ª**è’¸é¦å¼çš„ç­–ç•¥å­¦ä¹ æ¡†æ¶**ï¼š\n- **æ¶æ„**ï¼šåˆå§‹åŒ–ä¸€ä¸ªç‹¬ç«‹çš„ç­–ç•¥ç¼–ç å™¨å¤„ç†è§‚æµ‹ï¼ŒåŒæ—¶å†»ç»“çš„é¢„è®­ç»ƒç‰¹å¾æå–å™¨æä¾›â€œå‚è€ƒâ€ç‰¹å¾ã€‚\n- **å¤šæ­¥çŸ¥è¯†è’¸é¦**ï¼šä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±ï¼Œåœ¨**å½“å‰æ—¶åˆ»ï¼ˆtï¼‰å’Œä¸‹ä¸€æ—¶åˆ»ï¼ˆt+1ï¼‰çš„æ½œåœ¨çŠ¶æ€**ä¸Šï¼Œå¯¹é½ç­–ç•¥ç¼–ç å™¨çš„ç‰¹å¾ä¸é¢„è®­ç»ƒçš„ç‰¹å¾ã€‚è¿™å®ç°äº†ï¼š\n    - **è½¯æ€§å¼•å¯¼**ï¼šå°†é¢„è®­ç»ƒè·å¾—çš„å‡ ä½•çŸ¥è¯†æœ‰æ•ˆè¿ç§»åˆ°ç­–ç•¥ä¸­ã€‚\n    - **æ—¶åºä¸€è‡´æ€§**ï¼šé€šè¿‡ `t+1` æ­¥çš„è’¸é¦ï¼Œä½¿ç­–ç•¥éšå¼å»ºæ¨¡æ½œåœ¨åŠ¨åŠ›å­¦ï¼Œæå‡æ„ŸçŸ¥-è¡ŒåŠ¨ä¸€è‡´æ€§ã€‚\n- **è®­ç»ƒç›®æ ‡**ï¼šç­–ç•¥æ€»æŸå¤± = æ¨¡ä»¿å­¦ä¹ æŸå¤±ï¼ˆä¸“å®¶åŠ¨ä½œç›‘ç£ï¼‰ + è’¸é¦æŸå¤±ã€‚\n\n### **è§£å†³æ–¹æ¡ˆæ€»ç»“**\nGEM3Dé€šè¿‡ **â€œå‡ ä½•æ¥åœ°çš„é¢„è®­ç»ƒâ€ + â€œå¤šæ­¥è’¸é¦çš„ç­–ç•¥å­¦ä¹ â€** è¿™ä¸€ç»„åˆæ‹³ï¼Œç³»ç»Ÿæ€§åœ°è§£å†³äº†å¼€ç¯‡æå‡ºçš„ä¸‰ä¸ªé—®é¢˜ï¼š\n1.  **è§£å†³å•è§†å›¾æ¨ç†**ï¼šé¢„è®­ç»ƒå’Œç­–ç•¥æ¨ç†éƒ½ä»…éœ€**å•è§†è§’RGB-Dè¾“å…¥**ã€‚\n2.  **è§£å†³ä¸å®Œæ•´å»ºæ¨¡**ï¼šé€šè¿‡**ç‚¹äº‘é‡å»ºï¼ˆæ•´ä½“å‡ ä½•ï¼‰+ é«˜æ–¯æ³¼æº…æ¸²æŸ“ï¼ˆç»†ç²’åº¦å¤–è§‚ï¼‰** çš„å¤šä»»åŠ¡é¢„è®­ç»ƒï¼Œè·å¾— holistic ä¸”ç²¾ç»†çš„3Dè¡¨å¾ã€‚\n3.  **è§£å†³çŸ¥è¯†è¿ç§»**ï¼šé€šè¿‡**å¤šæ­¥è’¸é¦**ç­–ç•¥ï¼Œè€Œéç›´æ¥å¾®è°ƒï¼Œæœ‰æ•ˆä¿ç•™å¹¶åˆ©ç”¨äº†é¢„è®­ç»ƒä¸­çš„3Då‡ ä½•çŸ¥è¯†ï¼Œé¿å…äº†è¡¨å¾æ¼‚ç§»ã€‚\n\n### **å®é™…ä»·å€¼ä¸æ•ˆæœ**\n- **æ€§èƒ½æå‡**ï¼šåœ¨12ä¸ªRLBenchä»»åŠ¡ä¸Šï¼Œå¹³å‡æˆåŠŸç‡æ¯”ä¹‹å‰SOTAï¼ˆManiGaussianï¼‰**æå‡12.7%**ã€‚\n- **å¼ºå¤§çš„è§†è§’æ³›åŒ–**ï¼šåœ¨é›¶æ ·æœ¬è§†è§’åç§»æµ‹è¯•ä¸­ï¼Œæ€§èƒ½ä¸‹é™è¿œå°äºåŸºçº¿ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤§è§†è§’åç§»ä¸‹ï¼ŒGEM3DæˆåŠŸç‡ä»…ä¸‹é™29.7%ï¼Œè€ŒSOTAæ–¹æ³•ä¸‹é™51.5%ã€‚\n- **è¡¨å¾è´¨é‡é«˜**ï¼šå®šé‡æŒ‡æ ‡ï¼ˆPSNR, SSIM, Chamferè·ç¦»ï¼‰å’Œå®šæ€§ç»“æœå‡æ˜¾ç¤ºï¼Œå…¶åœºæ™¯é‡å»ºå’Œæ–°è§†è§’æ¸²æŸ“è´¨é‡æ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚\n- **æ¡†æ¶é€šç”¨æ€§**ï¼šè’¸é¦ç­–ç•¥å¯é€‚é…å…¶ä»–é¢„è®­ç»ƒè¡¨å¾ï¼ˆå¦‚ManiGaussianï¼‰ï¼Œå¸¦æ¥ä¸€è‡´æ€§èƒ½æå‡ï¼Œæ˜¾ç¤ºäº†å…¶æ‰©å±•æ½œåŠ›ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼ŒGEM3Dçš„æ ¸å¿ƒè´¡çŒ®åœ¨äºè®¾è®¡äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé¦–å…ˆé€šè¿‡ä¸€ç§æ–°é¢–çš„ã€ç»“åˆäº†æ˜¾å¼å‡ ä½•é‡å»ºä¸éšå¼ç¥ç»æ¸²æŸ“çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œä»å¤šè§†è§’æ•°æ®ä¸­è’¸é¦å‡ºå¼ºå¤§çš„å•è§†å›¾3Dç†è§£èƒ½åŠ›ï¼›ç„¶åé€šè¿‡ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„è’¸é¦ç­–ç•¥ï¼Œå°†è¿™ç§å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ç¨³å¥åœ°æ³¨å…¥åˆ°è¿åŠ¨ç­–ç•¥ä¸­ï¼Œä»è€Œå®ç°äº†åœ¨è§†è§’å˜åŒ–ä¸‹é«˜åº¦æ³›åŒ–ä¸”ç²¾ç¡®çš„æœºå™¨äººæ“ä½œã€‚**",
            "timestamp": "2026-02-03T15:54:32.852093",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## æœºå™¨äººè§†è§‰è¿åŠ¨ç­–ç•¥ä¸­3Dè¡¨å¾å­¦ä¹ çš„å‘å±•è„‰ç»œåˆ†æ\n\næœ¬æ–‡æå‡ºçš„ **GEM3D** æ˜¯â€œåŸºäºå‡ ä½•çš„3Dè§†è§‰è¡¨å¾å­¦ä¹ ç”¨äºè§†è§’æ³›åŒ–æœºå™¨äººæ“ä½œâ€è¿™ä¸€ç ”ç©¶æ–¹å‘çš„æœ€æ–°è¿›å±•ã€‚è¯¥æ–¹å‘çš„æ ¸å¿ƒç›®æ ‡æ˜¯**ä»è§†è§‰è¾“å…¥ä¸­å­¦ä¹ å¯¹3Dç©ºé—´å…·æœ‰å¼ºç†è§£èƒ½åŠ›ã€ä¸”å¯¹ç›¸æœºè§†è§’å˜åŒ–å…·æœ‰é²æ£’æ€§çš„è¡¨å¾ï¼Œä»¥æå‡æœºå™¨äººæ“ä½œç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›**ã€‚\n\nå…¶å‘å±•è„‰ç»œå¯ä»¥æ¸…æ™°åœ°åˆ’åˆ†ä¸ºå‡ ä¸ªé˜¶æ®µå’Œä¸åŒçš„æŠ€æœ¯è·¯çº¿ï¼Œæ¯ä¸€æ­¥éƒ½åœ¨è§£å†³å‰åºå·¥ä½œçš„å…³é”®ç“¶é¢ˆã€‚\n\n### å‘å±•è„‰ç»œæ€»è§ˆ\n\n```mermaid\ntimeline\n    title 3Dè§†è§‰è¡¨å¾ç”¨äºæœºå™¨äººæ“ä½œçš„å‘å±•è„‰ç»œ\n    section 2Dè¡¨å¾ä¸»å¯¼æœŸ\n        æ—©æœŸå·¥ä½œ : ä½¿ç”¨2D CNNç¼–ç å›¾åƒ<br>é—®é¢˜: ç¼ºä¹3Dç©ºé—´æ„è¯†\n    section å¤šè§†å›¾3Dæ„ŸçŸ¥å°è¯•\n        RVTç­‰å·¥ä½œ : å¼•å…¥å¤šè§†å›¾è¾“å…¥<br>é—®é¢˜: æ¨ç†æ—¶ä¾èµ–å¤šè§†å›¾ï¼Œä¸å®ç”¨\n    section å•è§†å›¾3Dé‡å»ºè¾…åŠ©\n        GNFactor, ManiGaussian : å•è§†å›¾è¾“å…¥ï¼Œ<br>NeRF/GSè¾…åŠ©ä»»åŠ¡\n        : é—®é¢˜: åœºæ™¯å»ºæ¨¡ç²—ç³™ï¼Œ<br>å¯¹è®­ç»ƒè§†è§’è¿‡æ‹Ÿåˆ\n    section å‡ ä½•åŸºç¡€çš„å•è§†å›¾3Dè¡¨å¾ (æœ¬æ–‡)\n        GEM3D : å•è§†å›¾é¢„è®­ç»ƒ + å¤šæ­¥è’¸é¦\n        : åˆ›æ–°: æ•´ä½“å‡ ä½•ç†è§£ï¼Œ<br>ç»†ç²’åº¦é‡å»ºï¼Œå¼ºè§†è§’æ³›åŒ–\n```\n\n### è¯¦ç»†å‘å±•é˜¶æ®µä¸æŠ€æœ¯æ¼”è¿›\n\n#### 1. ç¬¬ä¸€é˜¶æ®µï¼š2Dè§†è§‰è¡¨å¾ä¸»å¯¼\n*   **ä»£è¡¨å·¥ä½œ**ï¼šåŸºäº2D CNNçš„è§†è§‰ç¼–ç å™¨ï¼ˆå¦‚CURLã€DrQç³»åˆ—ï¼‰ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šå°†RGBå›¾åƒé€šè¿‡2Då·ç§¯ç¥ç»ç½‘ç»œå‹ç¼©ä¸ºä»»åŠ¡ç›¸å…³çš„æ½œå‘é‡ï¼Œç›´æ¥ç”¨äºç­–ç•¥å­¦ä¹ ã€‚\n*   **æ”¹è¿›ä¸å±€é™**ï¼š\n    *   **ä¼˜ç‚¹**ï¼šæ¶æ„ç®€å•ï¼Œè®¡ç®—é«˜æ•ˆã€‚\n    *   **å…³é”®å±€é™**ï¼š**ç¼ºä¹æ˜¾å¼çš„3Dç»“æ„æ„è¯†**ã€‚2Dç‰¹å¾å¯¹ç‰©ä½“é®æŒ¡ã€é€è§†å˜å½¢å’Œç›¸æœºè§†è§’å˜åŒ–éå¸¸æ•æ„Ÿï¼Œåœ¨éœ€è¦ç²¾ç¡®ç©ºé—´æ¨ç†çš„æ“ä½œä»»åŠ¡ï¼ˆå¦‚å †å ã€æ’å…¥ï¼‰ä¸Šæ€§èƒ½ç“¶é¢ˆæ˜æ˜¾ã€‚\n\n#### 2. ç¬¬äºŒé˜¶æ®µï¼šå¼•å…¥å¤šè§†å›¾è¾“å…¥ä»¥è·å–3Dæ„ŸçŸ¥\n*   **ä»£è¡¨å·¥ä½œ**ï¼š**RVT**, **3D-MVP**ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨**è®­ç»ƒå’Œæ¨ç†æ—¶éƒ½ä½¿ç”¨å¤šä¸ªå›ºå®šè§†è§’çš„å›¾åƒä½œä¸ºè¾“å…¥**ï¼Œé€šè¿‡Transformerç­‰æ¶æ„èåˆå¤šè§†å›¾ä¿¡æ¯ï¼Œéšå¼åœ°å­¦ä¹ 3Dåœºæ™¯ç†è§£ã€‚\n*   **æ”¹è¿›**ï¼šç›¸æ¯”2Dæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†åœ¨è®­ç»ƒè§†è§’ä¸‹çš„ç©ºé—´ä»»åŠ¡æ€§èƒ½ï¼Œå› ä¸ºå¤šè§†å›¾æä¾›äº†æ›´ä¸°å¯Œçš„å‡ ä½•çº¿ç´¢ã€‚\n*   **å…³é”®å±€é™**ï¼š\n    *   **æ¨ç†æ—¶ä¾èµ–å¤šè§†å›¾**ï¼šåœ¨å®é™…æœºå™¨äººéƒ¨ç½²ä¸­ï¼Œé…ç½®å¤šä¸ªåŒæ­¥ç›¸æœºå¾€å¾€ä¸ç°å®ï¼Œå¢åŠ äº†ç¡¬ä»¶æˆæœ¬å’Œç³»ç»Ÿå¤æ‚æ€§ã€‚\n    *   **å¹¶éçœŸæ­£çš„å•è§†å›¾æ³›åŒ–**ï¼šæ¨¡å‹æ€§èƒ½ä¸¥é‡ä¾èµ–äºåœ¨è®­ç»ƒæ—¶è§è¿‡çš„ç‰¹å®šç›¸æœºé…ç½®ã€‚\n\n#### 3. ç¬¬ä¸‰é˜¶æ®µï¼šå•è§†å›¾è¾“å…¥ + 3Dé‡å»ºè¾…åŠ©ä»»åŠ¡\næ­¤é˜¶æ®µæ˜¯å½“å‰ç ”ç©¶çš„ä¸»æµï¼Œæ—¨åœ¨ç”¨**å•è§†å›¾è¾“å…¥**è·å¾—3Dæ„ŸçŸ¥èƒ½åŠ›ã€‚ä¸»è¦åˆ†ä¸ºä¸¤ä¸ªæŠ€æœ¯å‘å±•æ–¹å‘ï¼š\n\n##### å‘å±•æ–¹å‘Aï¼šåŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„è¾…åŠ©é‡å»º\n*   **ä»£è¡¨å·¥ä½œ**ï¼š**GNFactor**ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨ç­–ç•¥ç½‘ç»œæ—è¾¹ï¼Œå¼•å…¥ä¸€ä¸ªNeRFåˆ†æ”¯ï¼Œå…¶ç›®æ ‡æ˜¯ä»å½“å‰è§†è§’é‡å»ºåœºæ™¯æˆ–åˆæˆæ–°è§†è§’ã€‚é€šè¿‡å…±äº«è§†è§‰ç¼–ç å™¨ï¼Œå¸Œæœ›NeRFçš„3Dé‡å»ºç›®æ ‡èƒ½è¿«ä½¿ç¼–ç å™¨å­¦ä¹ åˆ°å‡ ä½•æœ‰æ„ä¹‰çš„ç‰¹å¾ã€‚\n*   **æ”¹è¿›**ï¼šé¦–æ¬¡åœ¨å•è§†å›¾è®¾å®šä¸‹ï¼Œé€šè¿‡å¯å¾®æ¸²æŸ“çš„è¾…åŠ©ä»»åŠ¡å¼•å…¥3Då½’çº³åç½®ã€‚\n*   **å…³é”®å±€é™**ï¼š\n    *   **é‡å»ºè´¨é‡ä¸æ³›åŒ–èƒ½åŠ›æœ‰é™**ï¼šNeRFä¼˜åŒ–è¿‡ç¨‹ç¼“æ…¢ï¼Œä¸”éš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„åœºæ™¯æˆ–å¤§å¹…è§†è§’å˜åŒ–ï¼Œå¯¼è‡´å­¦åˆ°çš„è¡¨å¾ä¸å¤Ÿé²æ£’ã€‚\n    *   **è®­ç»ƒä¸å¹³è¡¡**ï¼šç­–ç•¥ç›®æ ‡å¾€å¾€ä¸»å¯¼ä¼˜åŒ–ï¼Œä½¿å¾—è¾…åŠ©ä»»åŠ¡å¯¹è¡¨å¾å­¦ä¹ çš„è´¡çŒ®æœ‰é™ã€‚\n\n##### å‘å±•æ–¹å‘Bï¼šåŸºäº3Dé«˜æ–¯æ³¼æº…ï¼ˆ3D Gaussian Splatting, 3D-GSï¼‰çš„è¾…åŠ©é‡å»º\n*   **ä»£è¡¨å·¥ä½œ**ï¼š**ManiGaussian**ï¼ˆæœ¬æ–‡çš„ç›´æ¥å‰ä½œï¼‰ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šç”¨æ›´é«˜æ•ˆã€æ¸²æŸ“è´¨é‡æ›´é«˜çš„**3D-GS**æ›¿ä»£NeRFä½œä¸ºè¾…åŠ©é‡å»ºå·¥å…·ã€‚åŒæ ·é‡‡ç”¨è”åˆè®­ç»ƒï¼ˆJoint-Trainingï¼‰èŒƒå¼ï¼Œå³ç­–ç•¥å’ŒGSé‡å»ºå™¨ä¸€èµ·ç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚\n*   **æ”¹è¿›**ï¼š\n    *   **æ¸²æŸ“é€Ÿåº¦ä¸è´¨é‡**ï¼š3D-GSç›¸æ¯”NeRFå…·æœ‰å®æ—¶æ¸²æŸ“å’Œé«˜ä¿çœŸåº¦çš„ä¼˜åŠ¿ã€‚\n    *   **è¡¨å¾èƒ½åŠ›**ï¼šç†è®ºä¸Šèƒ½è·å¾—æ¯”NeRFæ›´ç²¾ç»†çš„åœºæ™¯è¡¨å¾ã€‚\n*   **å…³é”®å±€é™ï¼ˆæœ¬æ–‡æŒ‡å‡ºå¹¶ç€åŠ›è§£å†³ï¼‰**ï¼š\n    1.  **åœºæ™¯å»ºæ¨¡ç²—ç³™**ï¼šManiGaussianç­‰æ–¹æ³•çš„3Dé«˜æ–¯åˆå§‹åŒ–é€šå¸¸ä¾èµ–äºä¸å®Œæ•´çš„ç‚¹äº‘æˆ–ç²—ç³™ä¼°è®¡ï¼Œå¯¼è‡´é‡å»ºçš„å‡ ä½•ç»“æ„**ä¸å¤Ÿå®Œæ•´å’Œç²¾ç»†**ï¼Œä¸¢å¤±å¯¹ç²¾ç¡®æ“ä½œè‡³å…³é‡è¦çš„ç»†èŠ‚ã€‚\n    2.  **è§†è§’è¿‡æ‹Ÿåˆ**ï¼šåœ¨è”åˆè®­ç»ƒä¸­ï¼Œç­–ç•¥ä¼šè¿‡åº¦é€‚åº”è®­ç»ƒè§†è§’ï¼Œå½“æµ‹è¯•è§†è§’å˜åŒ–æ—¶æ€§èƒ½**æ€¥å‰§ä¸‹é™**ã€‚è¿™è¡¨æ˜è¾…åŠ©ä»»åŠ¡æœªèƒ½æˆåŠŸåœ°å°†é€šç”¨çš„3DçŸ¥è¯†è¿ç§»ç»™ç­–ç•¥ã€‚\n    3.  **ç¼ºä¹æœ‰æ•ˆçš„çŸ¥è¯†è¿ç§»æœºåˆ¶**ï¼šç®€å•çš„è”åˆè®­ç»ƒæˆ–é¢„è®­ç»ƒ-å¾®è°ƒï¼ˆPretrain-Finetuneï¼‰èŒƒå¼å­˜åœ¨**ä¼˜åŒ–å†²çª**æˆ–**è¡¨å¾æ¼‚ç§»**é—®é¢˜ï¼Œæ— æ³•å°†é¢„è®­ç»ƒä¸­è·å¾—çš„3DçŸ¥è¯†æœ‰æ•ˆä¿ç•™åˆ°ç­–ç•¥ä¸­ã€‚\n\n#### 4. ç¬¬å››é˜¶æ®µï¼ˆæœ¬æ–‡GEM3Dï¼‰ï¼šå‡ ä½•åŸºç¡€çš„å•è§†å›¾3Dè¡¨å¾ä¸è’¸é¦\nGEM3Dé’ˆå¯¹ä¸Šè¿°æ‰€æœ‰å±€é™ï¼Œæå‡ºäº†ä¸€ä¸ª**ç»Ÿä¸€çš„è¡¨å¾-ç­–ç•¥å­¦ä¹ æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°ä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼š\n\n##### æ”¹è¿›1ï¼š**â€œå‡ ä½•åŸºç¡€â€çš„å•è§†å›¾3Dé¢„è®­ç»ƒèŒƒå¼**\n*   **ç›®æ ‡**ï¼šä»å•è§†å›¾RGB-Dè¾“å…¥ï¼Œå­¦ä¹ **æ•´ä½“ä¸”ç»†ç²’åº¦**çš„3Dåœºæ™¯è¡¨å¾ã€‚\n*   **å…³é”®æŠ€æœ¯åˆ›æ–°**ï¼š\n    *   **ä¸‰é˜¶æ®µæµæ°´çº¿**ï¼š\n        1.  **3Dç‰¹å¾æå–**ï¼šå°†RGB-Dè½¬æ¢ä¸ºç¨ å¯†ä½“ç´ ç‰¹å¾ã€‚\n        2.  **ä»ç²—åˆ°ç»†çš„ç‚¹äº‘é‡å»º**ï¼šå¼•å…¥**å¯å­¦ä¹ ä½“ç´ æŸ¥è¯¢**å’Œ**3Då¯å˜å½¢äº¤å‰æ³¨æ„åŠ›**æ¥ç”Ÿæˆâ€œç§å­ç‚¹â€ï¼Œç„¶åé€šè¿‡**Snowflakeç‚¹åå·ç§¯**è¿›è¡Œæ¸è¿›å¼ç»†åŒ–ã€‚è¿™ç¡®ä¿äº†**æ•´ä½“å‡ ä½•ç»“æ„**çš„æ¢å¤ã€‚\n        3.  **å‰é¦ˆ3Dé«˜æ–¯æ³¼æº…æ¸²æŸ“**ï¼šåŸºäºé‡å»ºçš„ç²¾ç»†ç‚¹äº‘ï¼Œé‡‡æ ·ç‰¹å¾æ¥å›å½’é«˜æ–¯å‚æ•°ï¼Œè¿›è¡Œæ–°è§†è§’æ¸²æŸ“ã€‚ä½¿ç”¨**Focal Loss**åŠ å¼ºåŠ¨æ€/æ¨¡ç³ŠåŒºåŸŸçš„ç›‘ç£ã€‚\n*   **ç›¸æ¯”äºå‰ä½œçš„æ”¹è¿›**ï¼š\n    *   vs. ManiGaussianï¼šé€šè¿‡**æ˜¾å¼çš„ã€æ¸è¿›å¼çš„å‡ ä½•é‡å»º**ï¼Œè·å¾—äº†æ›´**å®Œæ•´ã€æ›´ç²¾ç»†**çš„3Dåœºæ™¯è¡¨ç¤ºï¼ˆPSNRæå‡7.08dBï¼ŒChamferè·ç¦»é™ä½0.0289ï¼‰ï¼Œä¸ºæ“ä½œæä¾›äº†æ›´å¯é çš„å‡ ä½•åŸºç¡€ã€‚\n    *   vs. æ‰€æœ‰å‰ä½œï¼šé¦–æ¬¡ç³»ç»Ÿæ€§åœ°é›†æˆäº†**ä½“ç´ ç‰¹å¾ã€ç‚¹äº‘é‡å»ºã€é«˜æ–¯æ¸²æŸ“**ï¼Œå½¢æˆäº†ä¸€ä¸ªå±‚æ¬¡åŒ–çš„ã€å‡ ä½•é©±åŠ¨çš„è¡¨å¾å­¦ä¹ æµç¨‹ã€‚\n\n##### æ”¹è¿›2ï¼š**å¤šæ­¥è’¸é¦çš„ç­–ç•¥å­¦ä¹ æ¡†æ¶**\n*   **ç›®æ ‡**ï¼šå°†é¢„è®­ç»ƒä¸­å­¦åˆ°çš„å¼ºå¤§3Då‡ ä½•çŸ¥è¯†ï¼Œ**æœ‰æ•ˆä¸”ç¨³å®šåœ°**è¿ç§»åˆ°è§†è§‰è¿åŠ¨ç­–ç•¥ä¸­ã€‚\n*   **å…³é”®æŠ€æœ¯åˆ›æ–°**ï¼š\n    *   **è’¸é¦è€Œéå¾®è°ƒ**ï¼š**å†»ç»“**é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨ï¼Œå°†å…¶è¾“å‡ºä½œä¸ºâ€œå‚è€ƒä»¤ç‰Œâ€ã€‚ç­–ç•¥æ‹¥æœ‰ç‹¬ç«‹çš„ç¼–ç å™¨ï¼Œå…¶è¾“å‡ºçš„â€œæ½œä»¤ç‰Œâ€éœ€ä¸å‚è€ƒä»¤ç‰Œåœ¨ç‰¹å¾ç©ºé—´è¿›è¡Œ**ä½™å¼¦ç›¸ä¼¼åº¦å¯¹é½**ã€‚\n    *   **å¤šæ­¥è’¸é¦**ï¼šä¸ä»…å¯¹é½å½“å‰æ—¶åˆ»ï¼ˆtï¼‰çš„ç‰¹å¾ï¼Œè¿˜å¯¹é½**é¢„æµ‹çš„ä¸‹ä¸€ä¸ªæ—¶åˆ»ï¼ˆt+1ï¼‰** çš„æ½œçŠ¶æ€ç‰¹å¾ã€‚è¿™å¼•å…¥äº†**éšå¼çš„åŠ¨æ€æ„ŸçŸ¥**ï¼Œå¢å¼ºäº†ç­–ç•¥çš„æ—¶åºä¸€è‡´æ€§ã€‚\n*   **ç›¸æ¯”äºå‰ä½œçš„æ”¹è¿›**ï¼š\n    *   vs. **è”åˆè®­ç»ƒï¼ˆManiGaussianï¼‰**ï¼šè§£å†³äº†ä¼˜åŒ–å†²çªé—®é¢˜ï¼Œé˜²æ­¢ç­–ç•¥å¯¹è®­ç»ƒè§†è§’è¿‡æ‹Ÿåˆï¼Œä»è€Œå®ç°äº†**å“è¶Šçš„è§†è§’æ³›åŒ–èƒ½åŠ›**ï¼ˆåœ¨è¾ƒå¤§è§†è§’å˜åŒ–ä¸‹ï¼Œæ€§èƒ½ä¸‹é™æ¯”ManiGaussianä½çº¦22%ï¼‰ã€‚\n    *   vs. **é¢„è®­ç»ƒ-å¾®è°ƒ**ï¼šé¿å…äº†å› ä¸‹æ¸¸ä»»åŠ¡å·®å¼‚å¯¼è‡´çš„**è¡¨å¾æ¼‚ç§»**é—®é¢˜ï¼Œè½¯æ€§å¯¹é½çš„æ–¹å¼æ›´å¥½åœ°ä¿ç•™äº†é¢„è®­ç»ƒçŸ¥è¯†ã€‚\n    *   **é€šç”¨æ€§**ï¼šè¯¥è’¸é¦æ¡†æ¶è¢«è¯æ˜å¯ä»¥é€‚é…åˆ°å…¶ä»–é¢„è®­ç»ƒéª¨å¹²ï¼ˆå¦‚ManiGaussiançš„ç¼–ç å™¨ï¼‰ï¼Œå¸¦æ¥ä¸€è‡´æ€§èƒ½æå‡ï¼Œæ˜¾ç¤ºäº†å…¶ä½œä¸º**é€šç”¨ç­–ç•¥å­¦ä¹ æ¨¡å—**çš„æ½œåŠ›ã€‚\n\n### æ€»ç»“ä¸æœªæ¥æ–¹å‘\n\n**GEM3D** é€šè¿‡å°† **â€œå‡ ä½•åŸºç¡€çš„3Dé¢„è®­ç»ƒâ€** ä¸ **â€œå¤šæ­¥çŸ¥è¯†è’¸é¦â€** ç›¸ç»“åˆï¼Œä»£è¡¨äº†å½“å‰è¯¥æ–¹å‘ä¸Šçš„æœ€æ–°è¿›å±•ã€‚å®ƒç³»ç»Ÿåœ°è§£å†³äº†ä¹‹å‰å·¥ä½œåœ¨**å‡ ä½•å®Œæ•´æ€§ã€ç»†ç²’åº¦é‡å»ºã€è§†è§’æ³›åŒ–ä»¥åŠçŸ¥è¯†è¿ç§»æ•ˆç‡**æ–¹é¢çš„æ ¸å¿ƒç¼ºé™·ã€‚\n\næœ¬æ–‡ä¹ŸæŒ‡å‡ºäº†æœªæ¥çš„å‘å±•æ–¹å‘ï¼š\n1.  **å¤§è§„æ¨¡3Dé¢„è®­ç»ƒ**ï¼šåœ¨æ›´å¹¿æ³›çš„å¯¹è±¡å’Œåœºæ™¯æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ **é€šç”¨æ€§æ›´å¼ºã€å¯è¿ç§»æ€§æ›´é«˜**çš„3Dè§†è§‰è¡¨å¾ã€‚\n2.  **è’¸é¦æ¡†æ¶çš„æ‰©å±•**ï¼šå°†æœ¬æ–‡æå‡ºçš„è’¸é¦ç­–ç•¥å­¦ä¹ æ¡†æ¶ä¸**å…¶ä»–ç±»å‹çš„é¢„è®­ç»ƒè¡¨å¾**ï¼ˆå¦‚æ¥è‡ªå¤§è§„æ¨¡è§†é¢‘æˆ–å¤šæ¨¡æ€æ¨¡å‹ï¼‰ç»“åˆï¼Œä»¥å­¦ä¹ æ›´å¤šæ ·åŒ–çš„æ“ä½œæŠ€èƒ½ã€‚\n\nè¿™ä¸ªå‘å±•è„‰ç»œæ¸…æ™°åœ°å±•ç¤ºäº†ä»**2Dæ„ŸçŸ¥**åˆ°**3Dæ„ŸçŸ¥**ï¼Œä»**",
            "timestamp": "2026-02-03T15:55:46.961931",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## GEM3D çš„å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼ŒGEM3D çš„ç ”ç©¶å›¢é˜Ÿï¼ˆä¸»è¦æ¥è‡ªåŒæµå¤§å­¦å’Œé¦™æ¸¯å¤§å­¦ï¼‰åœ¨ç›¸å…³é¢†åŸŸæœ‰æŒç»­çš„ç ”ç©¶ç§¯ç´¯ã€‚è®ºæ–‡ä¸­æ˜ç¡®å¼•ç”¨äº†å›¢é˜Ÿä¹‹å‰çš„å·¥ä½œï¼Œå¹¶ä¸å…¶ä»–è¿‘æœŸæ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚ä»¥ä¸‹è¡¨æ ¼è¯¦ç»†æ¢³ç†äº†å…¶ç›´æ¥ç›¸å…³çš„â€œå‰ä½œâ€ï¼ˆå³å›¢é˜Ÿå…ˆå‰å‘è¡¨çš„å·¥ä½œï¼‰ä»¥åŠä½œä¸ºä¸»è¦å¯¹æ¯”åŸºçº¿çš„æœ€æ–°SOTAæ–¹æ³•ï¼Œå¹¶é‡ç‚¹åˆ†æGEM3Dä¸è¿™äº›å·¥ä½œçš„åŒºåˆ«ä¸æ”¹è¿›ã€‚\n\n### ä¸»è¦ç›¸å…³å‰ä½œä¸å¯¹æ¯”æ–¹æ³•\n\n| è®ºæ–‡/æ–¹æ³• | å‘è¡¨å¹´ä»½/å‡ºå¤„ | æ ¸å¿ƒæ€æƒ³ | ä¸ GEM3D çš„ä¸»è¦åŒºåˆ« | GEM3D çš„ä¸»è¦æ”¹å–„ä¸åˆ›æ–°ç‚¹ |\n| :--- | :--- | :--- | :--- | :--- |\n| **PerAct** <br> (Perceiver-Actor) | CoRL 2023 <br> (Shridhar et al.) | ä½¿ç”¨ä½“ç´ åŒ–ï¼ˆvoxelï¼‰çš„3Dè¡¨ç¤ºå’ŒPerceiverIOæ¶æ„ï¼Œç›´æ¥ä»3Dè§‚å¯Ÿå­¦ä¹ ç«¯åˆ°ç«¯æ“ä½œç­–ç•¥ã€‚ | 1. **è¡¨ç¤ºå­¦ä¹ **ï¼šPerActç›´æ¥ä½¿ç”¨åŸå§‹ç‚¹äº‘ä½“ç´ åŒ–åçš„ç‰¹å¾ï¼Œæ²¡æœ‰ä¸“é—¨çš„3Då‡ ä½•é¢„è®­ç»ƒã€‚<br>2. **è§†è§’æ³›åŒ–**ï¼šå…¶3Dè¡¨ç¤ºå¯¹è®­ç»ƒè§†è§’è¿‡æ‹Ÿåˆï¼Œåœ¨è§†è§’å˜åŒ–æ—¶æ€§èƒ½ä¸‹é™ä¸¥é‡ã€‚ | 1. **å¼•å…¥å‡ ä½•é¢„è®­ç»ƒ**ï¼šGEM3Då¢åŠ äº†**å•è§†å›¾3Då‡ ä½•é¢„è®­ç»ƒ**é˜¶æ®µï¼Œé€šè¿‡ç‚¹äº‘é‡å»ºå’Œå‰é¦ˆé«˜æ–¯æº…å°„å­¦ä¹ **æ•´ä½“ä¸”ç»†ç²’åº¦çš„3Dåœºæ™¯è¡¨ç¤º**ï¼Œè€ŒPerActæ²¡æœ‰æ­¤è¿‡ç¨‹ã€‚<br>2. **æ›´å¼ºçš„å‡ ä½•åŸºç¡€**ï¼šé¢„è®­ç»ƒä½¿æ¨¡å‹å¯¹åœºæ™¯çš„å‡ ä½•ç»“æ„å’Œå¤–è§‚æœ‰æ›´æ·±ç†è§£ï¼Œä»è€Œåœ¨**é›¶æ ·æœ¬è§†è§’æ³›åŒ–**ä¸Šè¡¨ç°è¿œä¼˜äºPerActï¼ˆå¤§è§†è§’åç§»ä¸‹æˆåŠŸç‡ä¸‹é™ä»…29.7% vs. PerActçš„79.9%ï¼‰ã€‚ |\n| **GNFactor** | CoRL 2023 <br> (Ze et al.) | å°†**ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰** ä½œä¸ºè¾…åŠ©è¡¨ç¤ºå­¦ä¹ ç›®æ ‡ï¼Œä¸ç­–ç•¥è¿›è¡Œ**è”åˆè®­ç»ƒ**ï¼Œä»¥æå‡3Dæ„ŸçŸ¥ã€‚ | 1. **3Dè¡¨ç¤ºæŠ€æœ¯**ï¼šä½¿ç”¨NeRFè¿›è¡Œæ–°è§†è§’åˆæˆï¼Œå…¶ä¼˜åŒ–é€Ÿåº¦æ…¢ï¼Œä¸”é‡å»ºè´¨é‡åœ¨åŠ¨æ€åœºæ™¯ä¸­ä¸ç¨³å®šã€‚<br>2. **è®­ç»ƒèŒƒå¼**ï¼šé‡‡ç”¨è”åˆè®­ç»ƒï¼Œè¡¨ç¤ºå­¦ä¹ å’Œç­–ç•¥å­¦ä¹ å¯èƒ½ç›¸äº’å¹²æ‰°ã€‚ | 1. **æ›´å…ˆè¿›çš„3Dè¡¨ç¤º**ï¼šGEM3Dé‡‡ç”¨**å‰é¦ˆ3Dé«˜æ–¯æº…å°„ï¼ˆGaussian Splattingï¼‰**ï¼Œç›¸æ¯”NeRFï¼Œ**æ¸²æŸ“é€Ÿåº¦æ›´å¿«**ï¼Œä¸”é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„é¢„è®­ç»ƒèƒ½è·å¾—**æ›´é«˜ä¿çœŸåº¦çš„å‡ ä½•å’Œçº¹ç†**ï¼ˆPSNRå¹³å‡æå‡7.08 dBï¼‰ã€‚<br>2. **è§£è€¦çš„è®­ç»ƒæ¡†æ¶**ï¼šé‡‡ç”¨**é¢„è®­ç»ƒ+è’¸é¦**çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé¿å…äº†è”åˆè®­ç»ƒçš„ä¸ç¨³å®šæ€§ï¼Œæ›´å¥½åœ°ä¿ç•™äº†é¢„è®­ç»ƒä¸­å­¦åˆ°çš„å‡ ä½•çŸ¥è¯†ã€‚ |\n| **ManiGaussian** | arXiv 2024 <br> (Lu et al.) | å°†**åŠ¨æ€3Dé«˜æ–¯æº…å°„**ä½œä¸ºè¡¨ç¤ºå­¦ä¹ ç›®æ ‡ï¼Œå¹¶ä¸è§†è§‰è¿åŠ¨ç­–ç•¥è¿›è¡Œ**è”åˆè®­ç»ƒ**ï¼Œæ˜¯GEM3Dä¹‹å‰çš„SOTAæ–¹æ³•ã€‚ | 1. **åœºæ™¯å»ºæ¨¡å®Œæ•´æ€§**ï¼šManiGaussiançš„è”åˆè®­ç»ƒå¯¼è‡´åœºæ™¯å»ºæ¨¡**ç²—ç³™**ï¼Œæ— æ³•æ¢å¤å¯¹ç²¾ç»†æ“ä½œè‡³å…³é‡è¦çš„ç»†ç²’åº¦å‡ ä½•ç»“æ„ï¼ˆè§å›¾1å¯¹æ¯”ï¼‰ã€‚<br>2. **çŸ¥è¯†ä¿ç•™**ï¼šè”åˆè®­ç»ƒä¸­ï¼Œç­–ç•¥å­¦ä¹ ä¼šå¹²æ‰°3Dè¡¨ç¤ºçš„å­¦ä¹ ï¼Œå¯¼è‡´å­¦åˆ°çš„è¡¨ç¤ºå¯¹è®­ç»ƒè§†è§’è¿‡æ‹Ÿåˆï¼Œæ³›åŒ–èƒ½åŠ›å¼±ã€‚<br>3. **è®­ç»ƒæ•ˆç‡ä¸ç¨³å®šæ€§**ï¼šè”åˆè®­ç»ƒéœ€è¦å¹³è¡¡å¤šä¸ªç›®æ ‡ï¼Œä¼˜åŒ–æ›´å…·æŒ‘æˆ˜æ€§ã€‚ | 1. **ç»Ÿä¸€çš„é¢„è®­ç»ƒ-ç­–ç•¥æ¡†æ¶**ï¼šGEM3Dçš„æ ¸å¿ƒåˆ›æ–°æ˜¯æå‡ºäº†**è§£è€¦ä½†ç»Ÿä¸€**çš„æ¡†æ¶ï¼šå…ˆé€šè¿‡å¤šè§†å›¾ç›‘ç£è¿›è¡Œ**å‡ ä½•åŸºç¡€é¢„è®­ç»ƒ**ï¼Œå†é€šè¿‡**å¤šæ­¥è’¸é¦**å°†3DçŸ¥è¯†è¿ç§»åˆ°ç­–ç•¥ç½‘ç»œã€‚è¿™ç¡®ä¿äº†å­¦åˆ°çš„3Dè¡¨ç¤ºæ˜¯**å®Œæ•´ä¸”è§†è§’ä¸å˜çš„**ã€‚<br>2. **åˆ›æ–°çš„é¢„è®­ç»ƒç»„ä»¶**ï¼š<br>   - **ç²—åˆ°ç»†çš„ç‚¹äº‘é‡å»º**ï¼šä½¿ç”¨Snowflakeç‚¹åå·ç§¯ï¼Œä»ç¨€ç–ç§å­ç‚¹é€æ­¥ç»†åŒ–å‡ºå®Œæ•´ç‚¹äº‘ï¼Œå®ç°**æ•´ä½“å‡ ä½•ç†è§£**ã€‚<br>   - **3Då¯å˜å½¢äº¤å‰æ³¨æ„åŠ›**ï¼šé«˜æ•ˆèšåˆå±€éƒ¨å‡ ä½•ç»†èŠ‚åˆ°æŸ¥è¯¢ç‰¹å¾ï¼Œæå‡ç§å­ç‚¹ç”Ÿæˆè´¨é‡ã€‚<br>   - **ç„¦ç‚¹æŸå¤±ï¼ˆFocal Lossï¼‰**ï¼šåœ¨æ¸²æŸ“ä¸­åŠ å¼ºå¯¹åŠ¨æ€å’Œæ¨¡ç³ŠåŒºåŸŸçš„ç›‘ç£ï¼Œæå‡å¯¹è¿åŠ¨ç‰©ä½“çš„é‡å»ºé²æ£’æ€§ã€‚<br>3. **è’¸é¦ç­–ç•¥è®¾è®¡**ï¼šæå‡º**å¤šæ­¥æ½œåœ¨è’¸é¦**ï¼Œä¸ä»…å¯¹é½å½“å‰çŠ¶æ€çš„æ½œåœ¨ç‰¹å¾ï¼Œè¿˜å¯¹é½é¢„æµ‹çš„ä¸‹ä¸€çŠ¶æ€ç‰¹å¾ï¼Œå¢å¼ºäº†ç­–ç•¥çš„**æ—¶åºä¸€è‡´æ€§**å’ŒåŠ¨æ€ç†è§£èƒ½åŠ›ã€‚é¿å…äº†ç›´æ¥å¾®è°ƒå¯¼è‡´çš„**è¡¨ç¤ºæ¼‚ç§»**é—®é¢˜ã€‚ |\n| **Zhang et al. (2024)** <br> â€œFocus on what matters...â€ | arXiv 2024 <br> (æœ¬æ–‡ä½œè€…ä¹‹ä¸€) | ä¸“æ³¨äºè§†è§‰RLæ³›åŒ–ï¼Œæå‡ºäº†åˆ†ç¦»æ¨¡å‹æ¥å¤„ç†ä¸åŒå±‚é¢çš„è§†è§‰å˜åŒ–ã€‚ | 1. **é—®é¢˜ç„¦ç‚¹**ï¼šè¯¥å·¥ä½œä¸»è¦è§£å†³**è§†è§‰å¤–è§‚**ï¼ˆå¦‚çº¹ç†ã€å…‰ç…§ï¼‰çš„æ³›åŒ–ï¼Œè€Œé**å‡ ä½•ä¸è§†è§’**çš„æ³›åŒ–ã€‚<br>2. **è¡¨ç¤ºå½¢å¼**ï¼šæœªå¼ºè°ƒ3Då‡ ä½•åŸºç¡€çš„æ˜¾å¼å­¦ä¹ ã€‚ | 1. **æ ¸å¿ƒé—®é¢˜è½¬ç§»**ï¼šGEM3Då°†ç„¦ç‚¹ä»**2Då¤–è§‚æ³›åŒ–**è½¬å‘**3Då‡ ä½•ä¸è§†è§’æ³›åŒ–**ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´æ ¹æœ¬ä¸”å¯¹æœºå™¨äººæ“ä½œè‡³å…³é‡è¦çš„æŒ‘æˆ˜ã€‚<br>2. **æŠ€æœ¯è·¯å¾„ä¸åŒ**ï¼šGEM3Dæ„å»ºäº†å…¨æ–°çš„ã€åŸºäºå‡ ä½•é‡å»ºå’Œæ¸²æŸ“çš„3Dè¡¨ç¤ºå­¦ä¹ æµæ°´çº¿ï¼Œä¸ä¹‹å‰å·¥ä½œçš„æŠ€æœ¯æ ˆæœ‰æ˜¾è‘—åŒºåˆ«ã€‚ |\n\n### æ€»ç»“ï¼šGEM3D ç›¸å¯¹äºå‰ä½œçš„çªç ´\n\n1.  **ä»â€œè”åˆè®­ç»ƒâ€åˆ°â€œé¢„è®­ç»ƒ-è’¸é¦â€çš„èŒƒå¼è½¬å˜**ï¼šé’ˆå¯¹ManiGaussianç­‰è”åˆè®­ç»ƒæ–¹æ³•å­˜åœ¨çš„è¡¨ç¤ºç²—ç³™å’Œè¿‡æ‹Ÿåˆé—®é¢˜ï¼ŒGEM3Dåˆ›æ–°æ€§åœ°é‡‡ç”¨äº†**åˆ†é˜¶æ®µå­¦ä¹ **ã€‚å…ˆé€šè¿‡å¤§é‡å¤šè§†å›¾æ•°æ®ï¼ˆä»…éœ€é‡‡é›†ï¼Œæ— éœ€åŠ¨ä½œæ ‡ç­¾ï¼‰å­¦ä¹ ä¸€ä¸ªå¼ºå¤§çš„ã€è§†è§’ä¸å˜çš„3Dåœºæ™¯è¡¨ç¤ºå™¨ï¼Œå†é€šè¿‡**çŸ¥è¯†è’¸é¦**å°†å…¶è½»é‡åŒ–åœ°è¿ç§»åˆ°å•è§†å›¾ç­–ç•¥ä¸­ã€‚è¿™æ—¢ä¿ç•™äº†ä¸°å¯Œçš„3Då…ˆéªŒï¼Œåˆä¿è¯äº†ç­–ç•¥æ¨ç†æ—¶çš„æ•ˆç‡ã€‚\n\n2.  **ä»â€œç²—ç³™3Dâ€åˆ°â€œå‡ ä½•åŸºç¡€â€çš„è¡¨ç¤ºå‡çº§**ï¼šç›¸æ¯”PerActçš„ç®€å•ä½“ç´ ç‰¹å¾å’ŒManiGaussian/NeRFçš„ä¸ç¨³å®šé‡å»ºï¼ŒGEM3Dçš„é¢„è®­ç»ƒæ¨¡å—é€šè¿‡**ç‚¹äº‘é‡å»º + é«˜æ–¯æº…å°„æ¸²æŸ“**çš„åŒé‡ç›‘ç£ï¼Œå­¦ä¹ åˆ°äº†** holisticï¼ˆæ•´ä½“ï¼‰ä¸” fine-grainedï¼ˆç»†ç²’åº¦ï¼‰** çš„å‡ ä½•è¡¨ç¤ºã€‚è¿™ä½“ç°åœ¨æ›´é«˜çš„æ–°è§†è§’æ¸²æŸ“è´¨é‡ï¼ˆPSNR/SSIMï¼‰å’Œæ›´ç²¾ç¡®çš„ç‚¹äº‘é‡å»ºï¼ˆChamferè·ç¦»æ›´ä½ï¼‰ä¸Šï¼Œç›´æ¥æ”¯æ’‘äº†æ›´ç²¾ç¡®çš„æœºæ¢°æ“ä½œã€‚\n\n3.  **é’ˆå¯¹è§†è§’æ³›åŒ–çš„ç³»ç»Ÿæ€§è®¾è®¡**ï¼š\n    - **é¢„è®­ç»ƒé˜¶æ®µ**ï¼šåˆ©ç”¨å¤šè§†å›¾ç›‘ç£æœ¬è´¨ä¸Šæ˜¯å¼ºåˆ¶æ¨¡å‹ä»å•è§†å›¾æ¨ç†å‡ºå®Œæ•´3Dåœºæ™¯ï¼Œä»è€Œå†…åœ¨åœ°å­¦ä¹ äº†è§†è§’ä¸å˜æ€§ã€‚\n    - **ç­–ç•¥é˜¶æ®µ**ï¼šå¤šæ­¥è’¸é¦æŸå¤±ï¼ˆ`L_distill`ï¼‰ç¡®ä¿äº†ç­–ç•¥ç½‘ç»œç¼–ç çš„è§†è§‰ç‰¹å¾ä¸é¢„è®­ç»ƒçš„å‡ ä½•ç‰¹å¾ç©ºé—´å¯¹é½ï¼Œå³ä½¿è¾“å…¥è§†è§’å˜åŒ–ï¼Œå…¶è¯­ä¹‰å’Œå‡ ä½•å«ä¹‰ä¹Ÿä¿æŒä¸€è‡´ã€‚\n    - **ç»“æœ**ï¼šåœ¨ä¸­ç­‰å’Œå¤§è§†è§’åç§»ä¸‹çš„é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½ä¸‹é™**è¿œå°äº**æ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼ˆä¸‹é™ç‡22.0%/29.7% vs. ManiGaussiançš„41.6%/51.5%ï¼‰ï¼Œè¯æ˜äº†å…¶æ”¹è¿›çš„æœ‰æ•ˆæ€§ã€‚\n\n4.  **æ¨¡å—åŒ–ä¸å¯æ‰©å±•æ€§**ï¼šè®ºæ–‡é€šè¿‡å°†ManiGaussiançš„è¡¨ç¤ºæ¨¡å—ä¸GEM3Dçš„è’¸é¦ç­–ç•¥ç»“åˆï¼ˆå³â€œManiGaussian (w Distill)â€ï¼‰ï¼ŒéªŒè¯äº†å…¶è’¸é¦ç­–ç•¥çš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿæå‡å…¶ä»–3Dè¡¨ç¤ºéª¨å¹²ç½‘ç»œçš„æ€§èƒ½ã€‚è¿™ä¸ºæœªæ¥é›†æˆæ›´å¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼ŒGEM3Dçš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæ„å»ºäº†ä¸€ä¸ªæ—¨åœ¨ä»æ ¹æœ¬ä¸Šè§£å†³æœºå™¨äººæ“ä½œä¸­è§†è§’æ³›åŒ–éš¾é¢˜çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡å¼•å…¥ä¸€ä¸ªæ·±åº¦å‡ ä½•é¢„è®­ç»ƒé˜¶æ®µå’Œä¸€ç§ä¿å®ˆçš„çŸ¥è¯†è¿ç§»ï¼ˆè’¸é¦ï¼‰ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†3Dè§†è§‰è¡¨ç¤ºçš„å®Œæ•´æ€§ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œåœ¨å¤šä¸ªå¤æ‚æ“ä½œä»»åŠ¡ä¸Šå®ç°äº†æ€§èƒ½çš„é£è·ƒã€‚**",
            "timestamp": "2026-02-03T15:56:51.499187",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨è¯¦ç»†è§£æè®ºæ–‡ã€ŠLearning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulationã€‹æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ç‚¹\n\nè¯¥è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®æ˜¯æå‡ºäº†ä¸€ä¸ªåä¸º **GEM3D** çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä¸­è§†è§‰ç­–ç•¥å¯¹**å•è§†å›¾è¾“å…¥**å’Œ**è§†è§’å˜åŒ–**çš„æ³›åŒ–éš¾é¢˜ã€‚å…¶åˆ›æ–°æ€§ä¸»è¦ä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼š\n1.  **ä¸€ç§æ–°é¢–çš„ã€å‡ ä½•åŸºç¡€æ‰å®çš„å•è§†å›¾3Dè¡¨ç¤ºé¢„è®­ç»ƒèŒƒå¼**ï¼Œèƒ½å¤Ÿä»å•è§†å›¾RGB-Dè¾“å…¥ä¸­å­¦ä¹ å®Œæ•´ã€ç»†ç²’åº¦çš„3Dåœºæ™¯è¡¨ç¤ºã€‚\n2.  **ä¸€ç§å¤šæ­¥è’¸é¦çš„ç­–ç•¥å­¦ä¹ æ–¹æ³•**ï¼Œèƒ½å¤Ÿå°†é¢„è®­ç»ƒä¸­å­¦åˆ°çš„3Då‡ ä½•çŸ¥è¯†æœ‰æ•ˆåœ°è¿ç§»åˆ°ä¸‹æ¸¸æ“ä½œç­–ç•¥ä¸­ï¼Œé¿å…è¡¨ç¤ºæ¼‚ç§»ã€‚\n\nä»¥ä¸‹è¡¨æ ¼å°†è¯¦ç»†æ‹†è§£è¿™äº›å…³é”®æŠ€æœ¯æ–¹æ³•åŠå…¶ç»†èŠ‚ã€‚\n\n### å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è¿°\n\n| æ¨¡å—åç§° | æ ¸å¿ƒç›®æ ‡ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…³é”®å‚æ•°ä¸è®¾è®¡ |\n| :--- | :--- | :--- | :--- |\n| **1. GEM3D é¢„è®­ç»ƒ (Geometrically-Grounded Pretraining)** | ä»**å•è§†å›¾RGB-D**è¾“å…¥å­¦ä¹ ä¸€ä¸ª**å¯†é›†ä½“ç´ ç‰¹å¾**ï¼Œè¯¥ç‰¹å¾èƒ½ç¼–ç åœºæ™¯çš„å®Œæ•´å‡ ä½•ä¸å¤–è§‚ï¼Œæ”¯æŒ**ç‚¹äº‘é‡å»º**å’Œ**æ–°è§†è§’æ¸²æŸ“**ã€‚ | **æ•´ä½“æµç¨‹**ï¼šå•è§†å›¾RGB-D â†’ 3Dç‰¹å¾æå– â†’ ç‚¹äº‘é‡å»º â†’ å‰é¦ˆå¼3Dé«˜æ–¯æ³¼æº…æ¸²æŸ“ã€‚è¿™æ˜¯ä¸€ä¸ª**ä¸‰é˜¶æ®µ**çš„é¢„è®­ç»ƒèŒƒå¼ã€‚ | **è¾“å…¥**ï¼šå•è§†å›¾RGBå›¾åƒ `â„^t`ï¼Œæ·±åº¦å›¾ `ğ’Ÿ^t`ï¼Œç›¸æœºå‚æ•° `ğ’¦^t`ã€‚ **è¾“å‡º**ï¼šå¯†é›†ä½“ç´ ç‰¹å¾ `â„± âˆˆ â„^(D^3Ã—128)`ã€‚ |\n| **1.1 3Dç‰¹å¾æå– (3D Feature Extraction)** | å°†å•è§†å›¾è§‚æµ‹ç¼–ç ä¸ºç¨ å¯†çš„ã€å¯Œå«è¯­ä¹‰çš„3Dä½“ç´ ç‰¹å¾åœºã€‚ | **1. ç‚¹äº‘ç”Ÿæˆ**ï¼šæ·±åº¦å›¾é€šè¿‡ç›¸æœºå‚æ•°åæŠ•å½±åˆ°3Dä¸–ç•Œåæ ‡ï¼Œå…¬å¼ä¸º `[x, y, z]^T = M_extr Â· [((u-c_x)Â·ğ’Ÿ)/f_x, ((v-c_y)Â·ğ’Ÿ)/f_y, ğ’Ÿ]^T`ã€‚éšåè¿›è¡Œè£å‰ªå’Œæœ€è¿œç‚¹é‡‡æ ·(FPS)ã€‚ <br> **2. ç‰¹å¾æŠ•å½±**ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„DINOv2æå–RGBå›¾åƒçš„2Dç‰¹å¾ `â„±_2D`ï¼Œå¹¶å°†å…¶æŠ•å½±åˆ°3Dç‚¹äº‘ä¸Šï¼Œèµ‹äºˆç‚¹äº‘è¯­ä¹‰ä¿¡æ¯ã€‚ <br> **3. ä½“ç´ åŒ–ä¸èåˆ**ï¼šå°†å¸¦æœ‰é¢œè‰²å’Œè¯­ä¹‰ç‰¹å¾çš„ç‚¹äº‘ä½“ç´ åŒ–ä¸º**3Då æ®å›¾**å’Œ**3Dç‰¹å¾å›¾**ï¼Œé€šè¿‡ä¸€ä¸ª**3D U-Net**è¿›è¡Œèåˆï¼Œè¾“å‡ºå¯†é›†ä½“ç´ ç‰¹å¾ `â„±`ã€‚ | **ä½“ç´ åˆ†è¾¨ç‡**ï¼š`D = 100` (ç‰¹å¾å›¾å°ºå¯¸)ï¼Œ`d = 7` (æŸ¥è¯¢ä½“ç´ å°ºå¯¸)ã€‚ <br> **ç‰¹å¾ç»´åº¦**ï¼š128ã€‚ <br> **FPSé‡‡æ ·ç‚¹æ•°**ï¼š512ã€‚ <br> **åœºæ™¯è¾¹ç•Œ**ï¼š`[-0.375, -0.5, 0.6, 1.0, 0.5, 1.6]`ã€‚ |\n| **1.2 ç²—åˆ°ç»†çš„ç‚¹äº‘é‡å»º (Coarse-to-Fine Point Cloud Reconstruction)** | ä»å¯†é›†ä½“ç´ ç‰¹å¾ `â„±` ä¸­é‡å»ºå‡º**å®Œæ•´ã€é«˜ä¿çœŸ**çš„åœºæ™¯ç‚¹äº‘ï¼Œå®ç°**æ•´ä½“å‡ ä½•ç†è§£**ã€‚ | **1. ç§å­ç‚¹ç”Ÿæˆ**ï¼š <br>   - **å¯å­¦ä¹ ä½“ç´ æŸ¥è¯¢é›†**ï¼š`ğ’¬ âˆˆ â„^(d^3Ã—128)`ã€‚ <br>   - **ç²—ç²’åº¦äº¤å‰æ³¨æ„åŠ›**ï¼šå°† `â„±` ä¸‹é‡‡æ ·ä¸º `â„±_down`ï¼Œä¸ `ğ’¬` åšæ³¨æ„åŠ›ï¼Œå¾—åˆ°ææ¡ˆæŸ¥è¯¢ `ğ’¬_p`ã€‚ <br>   - **3Då¯å˜å½¢äº¤å‰æ³¨æ„åŠ›(3D DCA)**ï¼š`ğ’¬_p` å¯¹ `â„±` è¿›è¡Œå¯å˜å½¢æ³¨æ„åŠ›ï¼Œé«˜æ•ˆèšåˆå±€éƒ¨å‡ ä½•ç»†èŠ‚ï¼Œç”Ÿæˆ**ç§å­ä»¤ç‰Œ**ï¼Œè§£ç ä¸ºç§å­ç‚¹åæ ‡ `ğ’«Ì‚_0`ã€‚ <br> **2. ç‚¹äº‘æ¢å¤ (Snowflakeè¿‡ç¨‹)**ï¼šä»ç§å­ç‚¹ `ğ’«Ì‚_i` å¼€å§‹ï¼Œé€šè¿‡**é›ªèŠ±ç‚¹åå·ç§¯(SPD)**å—é€æ­¥ä¸Šé‡‡æ ·ç»†åŒ–ã€‚æ¯ä¸ªçˆ¶ç‚¹é€šè¿‡æŸ¥è¯¢ `â„±` è·å¾—ç‰¹å¾ï¼Œé¢„æµ‹ `r` ä¸ªä½ç§»å‘é‡ `Î”ğ’«_i`ï¼Œç”Ÿæˆå­ç‚¹ `ğ’«Ì‚_(i+1)`ã€‚ <br> **3. æŸå¤±å‡½æ•°**ï¼šä½¿ç”¨**å€’è§’è·ç¦»(Chamfer Distance)** `L_2` ç›‘ç£æ¯ä¸€é˜¶æ®µçš„ç‚¹äº‘é‡å»ºï¼š <br> `â„’_rec = Î£_i [ (1/|ğ’«Ì‚_i|) Î£_(pÌ‚âˆˆğ’«Ì‚_i) min_(pâˆˆğ’«_i) â€–pÌ‚ - pâ€–_2^2 + (1/|ğ’«_i|) Î£_(pâˆˆğ’«_i) min_(pÌ‚âˆˆğ’«Ì‚_i) â€–p - pÌ‚â€–_2^2 ]` | **DCAåç§»ç‚¹æ•°**ï¼š`N_p` (è®ºæ–‡æœªæ˜ç¡®ï¼Œé€šå¸¸ä¸º4æˆ–8)ã€‚ <br> **SPDä¸Šé‡‡æ ·ç‡**ï¼š`r` (è®ºæ–‡æœªæ˜ç¡®ï¼Œé€šå¸¸ä¸º2)ã€‚ <br> **é‡å»ºé˜¶æ®µæ•°**ï¼š`i = 0, 1, 2, 3` (å…±4é˜¶æ®µï¼ŒåŒ…æ‹¬ç§å­ç‚¹é˜¶æ®µ)ã€‚ |\n| **1.3 å‰é¦ˆå¼3Dé«˜æ–¯æ³¼æº… (Feed-forward 3D Gaussian Splatting)** | åŸºäºé‡å»ºçš„ç‚¹äº‘ï¼Œå­¦ä¹ åœºæ™¯çš„**ç»†ç²’åº¦çº¹ç†å’Œå¤–è§‚**ï¼Œé€šè¿‡æ–°è§†è§’æ¸²æŸ“ä»»åŠ¡è¿›è¡Œç›‘ç£ã€‚ | **1. é«˜æ–¯å‚æ•°é¢„æµ‹**ï¼šå°†é‡å»ºçš„å¯†é›†ç‚¹äº‘ `ğ’«Ì‚` ä½œä¸º3Dé«˜æ–¯çš„ä¸­å¿ƒ `Î¼`ã€‚åœ¨è¿™äº›ä½ç½®ï¼Œä» `â„±` ä¸­é€šè¿‡ä¸‰çº¿æ€§æ’å€¼é‡‡æ ·å¾—åˆ°é«˜æ–¯ç‰¹å¾ `â„±_ğ’¢`ï¼Œè¾“å…¥ä¸€ä¸ª**ResNetFC**ç½‘ç»œï¼Œå›å½’é¢œè‰² `c`ã€ä¸é€æ˜åº¦ `Ïƒ`ã€æ—‹è½¬ `r`ã€å°ºåº¦ `s`ï¼š`{c, Ïƒ, r, s} = ResNetFC(â„±_ğ’¢)`ã€‚ <br> **2. å¯å¾®åˆ†æ¸²æŸ“ä¸ç›‘ç£**ï¼šä½¿ç”¨3Dé«˜æ–¯æ³¼æº…çš„å¯å¾®åˆ†æ¸²æŸ“å™¨ï¼Œä»ä»»æ„è§†è§’æ¸²æŸ“å›¾åƒ `â„Ì‚_v^t`ã€‚ä½¿ç”¨**Focal Loss**ï¼ˆè€ŒéMSEï¼‰è¿›è¡Œç›‘ç£ï¼Œè¯¥æŸå¤±å¯¹åŠ¨æ€å’Œè§†è§‰æ¨¡ç³ŠåŒºåŸŸç»™äºˆæ›´é«˜æƒé‡ï¼š`â„’_rend = FocalLoss(â„Ì‚_v^t, â„_v^t)`ã€‚ | **é«˜æ–¯ç‰¹å¾ç»´åº¦**ï¼š128ã€‚ <br> **ç›‘ç£è§†è§’æ•°**ï¼šé¢„è®­ç»ƒæ—¶ä½¿ç”¨8ä¸ªç¯ç»•ç›¸æœºã€‚ <br> **æŸå¤±å‡½æ•°**ï¼šFocal Loss (æºè‡ªç›®æ ‡æ£€æµ‹ï¼Œè§£å†³éš¾æ˜“æ ·æœ¬ä¸å¹³è¡¡)ã€‚ |\n| **2. GEM3D ç­–ç•¥å­¦ä¹  (Distillation-based Policy Learning)** | å°†é¢„è®­ç»ƒä¸­å­¦åˆ°çš„å¼ºå¤§3Då‡ ä½•çŸ¥è¯†**è’¸é¦**åˆ°è§†è§‰è¿åŠ¨ç­–ç•¥ä¸­ï¼Œä½¿å…¶å…·å¤‡**è§†è§’æ³›åŒ–èƒ½åŠ›**ï¼ŒåŒæ—¶é¿å…ç›´æ¥å¾®è°ƒå¯¼è‡´çš„è¡¨ç¤ºç ´åã€‚ | **1. å¤šæ­¥ç‰¹å¾è’¸é¦**ï¼šç­–ç•¥ç½‘ç»œæœ‰ä¸€ä¸ªç‹¬ç«‹çš„è§†è§‰ç¼–ç å™¨ï¼Œå°†è§‚æµ‹ `o^t` ç¼–ç ä¸ºæ½œåœ¨ä»¤ç‰Œ `x^t`ã€‚åŒæ—¶ï¼Œ**å†»ç»“çš„**é¢„è®­ç»ƒç‰¹å¾æå–å™¨ç”Ÿæˆå‚è€ƒç‰¹å¾ `â„±^t`ï¼Œå¹¶å°†å…¶åˆ‡å—ä¸ºå‚è€ƒä»¤ç‰Œ `xÌƒ^t`ã€‚ä½¿ç”¨**ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±**å¯¹é½å½“å‰å¸§å’Œ**ä¸‹ä¸€å¸§**çš„æ½œåœ¨ç‰¹å¾ä¸å‚è€ƒç‰¹å¾ï¼š <br> `â„’_distill = â„’_cos(x^t, xÌƒ^t) + â„’_cos(x^(t+1), xÌƒ^(t+1))` <br> **2. éšå¼åŠ¨åŠ›å­¦æ¨¡å‹**ï¼šç­–ç•¥é‡‡ç”¨æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®æ§åˆ¶ï¼ŒåŠ¨ä½œ `a_t` è¢«éšå¼åœ°åµŒå…¥åœ¨ä¸‹ä¸€æ½œåœ¨çŠ¶æ€ `x^(t+1)` ä¸­ã€‚ç­–ç•¥ç½‘ç»œåŒ…å«ä¸€ä¸ªéšå¼æ½œåœ¨åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹ `x^(t+1)`ã€‚è¿™æé«˜äº†å†³ç­–çš„å¯è§£é‡Šæ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚ <br> **3. æ€»ä½“ç­–ç•¥æŸå¤±**ï¼šç»“åˆæ¨¡ä»¿å­¦ä¹ çš„è¡Œä¸ºå…‹éš†æŸå¤±å’Œè’¸é¦æŸå¤±ï¼š <br> `â„’_policy = â€–a_t - a_t*â€–_2^2 + Î»_distill Â· â„’_distill` | **ç­–ç•¥ä¸»å¹²**ï¼šPerceiverIOã€‚ <br> **æ½œåœ¨ä»¤ç‰Œæ•°**ï¼š2048ã€‚ <br> **è’¸é¦æƒé‡**ï¼š`Î»_distill` é‡‡ç”¨é€€ç«ç­–ç•¥ `[1, 0.5, 0.3, 0.2, 0.1, 0.05]`ã€‚ <br> **è®­ç»ƒæ•°æ®**ï¼šæ¯ä¸ªä»»åŠ¡20æ¡ä¸“å®¶ç¤ºæ•™è½¨è¿¹ï¼ˆä»…**å‰è§†**å•è§†è§’ï¼‰ã€‚ |\n| **3. å…³é”®è®­ç»ƒæŠ€å·§ (éæ ¸å¿ƒåˆ›æ–°ä½†è‡³å…³é‡è¦)** | ç¡®ä¿é¢„è®­ç»ƒå’Œç­–ç•¥å­¦ä¹ çš„ç¨³å®šä¸é«˜æ•ˆã€‚ | **1. åˆ†é˜¶æ®µè§£è€¦è®­ç»ƒ**ï¼šé¢„è®­ç»ƒæ—¶ï¼Œå…ˆå•ç‹¬è®­ç»ƒç‚¹äº‘é‡å»ºæ¨¡å—ï¼ˆçº¦40Kæ­¥ï¼‰ï¼Œå¾…å‡ ä½•ç¨³å®šåï¼Œå†å¯ç”¨é«˜æ–¯æ¸²æŸ“æ¨¡å—ï¼Œé˜²æ­¢æ¸²æŸ“æ¢¯åº¦å¹²æ‰°å‡ ä½•å­¦ä¹ ã€‚ <br> **2. å¤šæ­¥è¿­ä»£é‡å»º**ï¼šè®­ç»ƒåˆæœŸï¼Œå¯¹åŒä¸€",
            "timestamp": "2026-02-03T15:58:01.933168",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä½¿ç”¨äº†ä»¥ä¸‹æ ¸å¿ƒæŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œä¸»è¦é›†ä¸­åœ¨**ä»»åŠ¡æˆåŠŸç‡**å’Œ**åœºæ™¯é‡å»ºè´¨é‡**ä¸¤ä¸ªæ–¹é¢ã€‚\n\n| æŒ‡æ ‡ç±»åˆ« | å…·ä½“æŒ‡æ ‡ | è®¡ç®—æ–¹å¼ä¸ç»†èŠ‚ | è¯„ä¼°ç›®çš„ |\n| :--- | :--- | :--- | :--- |\n| **ç­–ç•¥æ€§èƒ½** | **ä»»åŠ¡æˆåŠŸç‡** | - **å®šä¹‰**ï¼šæ¯ä¸ªæµ‹è¯•å›åˆï¼ˆepisodeï¼‰çš„æˆåŠŸç‡ä¸º1ï¼ˆæˆåŠŸï¼‰æˆ–0ï¼ˆå¤±è´¥ï¼‰ï¼Œ**ä¸è®¾éƒ¨åˆ†å¾—åˆ†**ã€‚<br>- **è®¡ç®—**ï¼šåœ¨å¤šä¸ªéšæœºç§å­ï¼ˆ0, 1, 2ï¼‰ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œæ¯ä¸ªç§å­åœ¨æ¯é¡¹ä»»åŠ¡ä¸Šè¿›è¡Œ100æ¬¡ä¸åŒé…ç½®çš„æµ‹è¯•ï¼ˆåŒ…å«è¯­è¨€ã€ç‰©ä½“ä½ç½®ã€é¢œè‰²ç­‰178ç§å˜ä½“ï¼‰ã€‚æŠ¥å‘Šæ‰€æœ‰ç§å­æˆåŠŸç‡çš„**å‡å€¼ä¸æ ‡å‡†å·®**ã€‚<br>- **é˜²è¿‡æ‹Ÿåˆ**ï¼šé€‰å–è®­ç»ƒæ­¥æ•°80%å¤„å’Œæœ€ç»ˆçš„ä¸¤ä¸ªæ£€æŸ¥ç‚¹è¿›è¡Œè¯„ä¼°ï¼Œæ¯ä¸ªç§å­å–æœ€ä½³ç»“æœã€‚ | è¡¡é‡æ¨¡å‹åœ¨**è®­ç»ƒè§†è§’**ä¸‹å®ŒæˆæŒ‡å®šæœºå™¨äººæ“ä½œä»»åŠ¡çš„æ ¸å¿ƒèƒ½åŠ›ã€‚ |\n| | **é›¶æ ·æœ¬è§†è§’æ³›åŒ–æˆåŠŸç‡** | - **å®šä¹‰**ï¼šåœ¨**æœªç»è®­ç»ƒ**çš„æ–°æ‘„åƒæœºè§†è§’ä¸‹æµ‹è¯•ä»»åŠ¡æˆåŠŸç‡ã€‚<br>- **è§†è§’åç§»è®¾ç½®**ï¼šåˆ†ä¸ºä¸‰ä¸ªç­‰çº§ï¼š<br>  &nbsp;&nbsp;1. **æ— åç§»**ï¼šä¸è®­ç»ƒè§†è§’ç›¸åŒã€‚<br>  &nbsp;&nbsp;2. **ä¸­åº¦åç§»**ï¼šæ‘„åƒæœºåœ¨æœºå™¨äººåŸºåº§å‘¨å›´ **30Â°** èŒƒå›´å†…ç§»åŠ¨ã€‚<br>  &nbsp;&nbsp;3. **å¤§å¹…åç§»**ï¼šæ‘„åƒæœºè§†è§’åœ¨ **60Â° è‡³ 90Â°** ä¹‹é—´å¤§å¹…å˜åŒ–ã€‚<br>- **æŠ¥å‘Š**ï¼šé™¤äº†ç»å¯¹æˆåŠŸç‡ï¼Œè¿˜æŠ¥å‘Šç›¸å¯¹äºè®­ç»ƒè§†è§’çš„**ç›¸å¯¹æ€§èƒ½ä¸‹é™ç™¾åˆ†æ¯”**ï¼ˆâ†“ï¼‰ã€‚ | è¯„ä¼°æ¨¡å‹å¯¹**æœªçŸ¥æ‘„åƒæœºè§†è§’**çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œè¿™æ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹ã€‚ |\n| **é‡å»ºè´¨é‡** | **å³°å€¼ä¿¡å™ªæ¯” (PSNR)** | - **è®¡ç®—**ï¼šæ¯”è¾ƒ**åˆæˆçš„æ–°è§†è§’å›¾åƒ**ä¸çœŸå®å¤šè§†è§’RGBå›¾åƒä¹‹é—´çš„åƒç´ çº§å·®å¼‚ã€‚**å€¼è¶Šé«˜ï¼Œå›¾åƒä¿çœŸåº¦è¶Šå¥½**ã€‚ | é‡åŒ–æ¨¡å‹é€šè¿‡å‰é¦ˆé«˜æ–¯æ³¼æº…è¿›è¡Œ**æ–°è§†è§’æ¸²æŸ“**çš„**å›¾åƒè´¨é‡**ã€‚ |\n| | **ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•° (SSIM)** | - **è®¡ç®—**ï¼šè¯„ä¼°åˆæˆå›¾åƒä¸çœŸå®å›¾åƒåœ¨äº®åº¦ã€å¯¹æ¯”åº¦å’Œç»“æ„ä¸Šçš„ç›¸ä¼¼æ€§ã€‚**å€¼è¶Šæ¥è¿‘1ï¼Œæ„ŸçŸ¥è´¨é‡è¶Šå¥½**ã€‚ | ä»äººç±»è§†è§‰æ„ŸçŸ¥è§’åº¦è¯„ä¼°**æ¸²æŸ“å›¾åƒçš„è§†è§‰çœŸå®æ„Ÿ**ã€‚ |\n| | **å€’è§’è·ç¦» (Chamfer Lâ‚‚)** | - **è®¡ç®—**ï¼šè¡¡é‡é¢„æµ‹çš„ç‚¹äº‘ä¸çœŸå®å®Œæ•´ç‚¹äº‘ä¹‹é—´çš„å‡ ä½•å·®å¼‚ã€‚è®¡ç®—ä¸¤ä¸ªç‚¹é›†é—´æœ€è¿‘ç‚¹è·ç¦»çš„å¹³æ–¹å’Œå‡å€¼ã€‚**å€¼è¶Šä½ï¼Œå‡ ä½•ç²¾åº¦è¶Šé«˜**ã€‚<br>- **çœŸå®ç‚¹äº‘**ï¼šé€šè¿‡èåˆå¤šè§†è§’è§‚æµ‹å¾—åˆ°çš„å®Œæ•´åœºæ™¯ç‚¹äº‘ `P_full`ã€‚ | é‡åŒ–æ¨¡å‹**å•è§†è§’ç‚¹äº‘é‡å»º**çš„**å‡ ä½•å‡†ç¡®æ€§**ï¼Œåæ˜ å¯¹åœºæ™¯ä¸‰ç»´ç»“æ„çš„ç†è§£æ·±åº¦ã€‚ |\n\n### äºŒã€ ä½¿ç”¨çš„æ•°æ®é›†ä¸å®éªŒå¹³å°\n\nè®ºæ–‡ä¸»è¦åœ¨ **RLBench** ä»¿çœŸåŸºå‡†ä¸Šè¿›è¡Œå®éªŒã€‚\n\n| æ–¹é¢ | å…·ä½“ç»†èŠ‚ |\n| :--- | :--- |\n| **å¹³å°** | **RLBench**ï¼šä¸€ä¸ªåŸºäºPyBulletçš„æœºå™¨äººå­¦ä¹ åŸºå‡†æµ‹è¯•ä¸ç¯å¢ƒã€‚ |\n| **ä»»åŠ¡èŒƒå›´** | - **æ•°é‡**ï¼šä»RLBenchä¸­é€‰å–äº† **12ä¸ª** æ“ä½œä»»åŠ¡ï¼Œè¦†ç›– **9ä¸ª** ä¸åŒçš„åœºæ™¯ã€‚<br>- **å¤šæ ·æ€§**ï¼šä»»åŠ¡æ¶µç›–**å¼€æŠ½å±‰ã€æ‹§æ°´é¾™å¤´ã€å †å ç§¯æœ¨ã€å¼€å…³ç½å­ã€æ”¾ç½®æ‰‹æœºã€çƒ¤è‚‰**ç­‰å¤šç§éœ€è¦**ç²¾ç»†ç©ºé—´æ¨ç†**çš„æ“ä½œã€‚<br>- **è¯­è¨€æŒ‡ä»¤**ï¼šæ¯ä¸ªä»»åŠ¡åŒ…å«å¤šç§è¯­è¨€æŒ‡ä»¤å˜ä½“ï¼ˆå…±178ç§ï¼‰ï¼Œç”¨äºè¯„ä¼°æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚ |\n| **æ•°æ®æ”¶é›† (é¢„è®­ç»ƒ)** | - **è§†è§’**ï¼šä½¿ç”¨ **8ä¸ª** å‡åŒ€åˆ†å¸ƒåœ¨æœºå™¨äººå‘¨å›´çš„è™šæ‹Ÿæ‘„åƒæœºæ”¶é›†**å¤šè§†è§’è§‚æµ‹æ•°æ®**ã€‚<br>- **æ•°æ®é‡**ï¼šåœ¨æ¯ä¸ªåœºæ™¯ï¼ˆå…±9ä¸ªï¼‰æ”¶é›† **50æ¡è½¨è¿¹**ï¼Œæ€»è®¡ **450æ¡è½¨è¿¹**ã€‚<br>- **ç›®çš„**ï¼šç”¨äºè®­ç»ƒ **GEM3D** çš„**å‡ ä½•æ„ŸçŸ¥é¢„è®­ç»ƒæ¨¡å—**ï¼Œå­¦ä¹ ä»å•è§†å›¾RGB-Dè¾“å…¥é‡å»ºå®Œæ•´åœºæ™¯å‡ ä½•ä¸å¤–è§‚ã€‚ |\n| **æ•°æ®æ”¶é›† (ç­–ç•¥å­¦ä¹ )** | - **è§†è§’**ï¼šä½¿ç”¨**å›ºå®šçš„å‰è§†æ‘„åƒæœº**æ”¶é›†æ•°æ®ã€‚<br>- **æ•°æ®é‡**ï¼šæ¯ä¸ªä»»åŠ¡ä½¿ç”¨ **20æ¡ä¸“å®¶æ¼”ç¤ºè½¨è¿¹**è¿›è¡Œæ¨¡ä»¿å­¦ä¹ ã€‚<br>- **ç›®çš„**ï¼šç”¨äºè®­ç»ƒä¸‹æ¸¸çš„**è§†è§‰è¿åŠ¨ç­–ç•¥**ã€‚ |\n| **è¯„ä¼°è®¾ç½®** | - **å•ä»»åŠ¡è¯„ä¼°**ï¼š12ä¸ªä»»åŠ¡**åˆ†åˆ«ç‹¬ç«‹è®­ç»ƒå’Œè¯„ä¼°**ï¼ŒæŠ¥å‘Šå¹³å‡æˆåŠŸç‡ã€‚<br>- **å¤šä»»åŠ¡è¯„ä¼°**ï¼šå°†4ä¸ªä»»åŠ¡çš„æ•°æ®æ··åˆï¼Œè®­ç»ƒä¸€ä¸ª**ç»Ÿä¸€æ¨¡å‹**ï¼Œè¯„ä¼°å…¶å¤šä»»åŠ¡å­¦ä¹ å’ŒæŒ‡ä»¤åŒºåˆ†èƒ½åŠ›ã€‚<br>- **æ³›åŒ–æµ‹è¯•**ï¼šåœ¨æµ‹è¯•æ—¶å¼•å…¥ä¸Šè¿°çš„**è§†è§’åç§»**ï¼Œè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°ã€‚ |\n\n### æ€»ç»“ä¸ä»·å€¼\n\n- **æŒ‡æ ‡è®¾è®¡çš„é’ˆå¯¹æ€§**ï¼šè¯„ä»·ä½“ç³»ç´§å¯†å›´ç»•è®ºæ–‡ç›®æ ‡â€”â€”**æå‡è§†è§’æ³›åŒ–èƒ½åŠ›**ã€‚ä¸ä»…çœ‹æœ€ç»ˆä»»åŠ¡æˆåŠŸç‡ï¼Œæ›´å…³é”®çš„æ˜¯è¡¡é‡åœ¨**è§†è§’å‰§çƒˆå˜åŒ–ä¸‹çš„æ€§èƒ½ä¿æŒåº¦**ï¼ˆç›¸å¯¹ä¸‹é™ç‡ï¼‰ï¼Œå¹¶è¾…ä»¥**ä¸‰ç»´é‡å»ºè´¨é‡æŒ‡æ ‡**ï¼ˆPSNR, SSIM, Chamferï¼‰æ¥éªŒè¯æ‰€å­¦è¡¨å¾çš„å‡ ä½•å®Œå¤‡æ€§ã€‚\n- **æ•°æ®é›†çš„åˆç†æ€§**ï¼šRLBenchæ˜¯æœºå™¨äººæ“ä½œé¢†åŸŸçš„å¸¸ç”¨åŸºå‡†ï¼Œä»»åŠ¡å¤æ‚ä¸”å¤šæ ·ï¼Œèƒ½æœ‰æ•ˆæ£€éªŒæ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚**é¢„è®­ç»ƒä½¿ç”¨å¤šè§†è§’æ•°æ®ã€ç­–ç•¥è®­ç»ƒä½¿ç”¨å•è§†è§’æ•°æ®**çš„è®¾ç½®ï¼Œæ¨¡æ‹Ÿäº†ç°å®ä¸–ç•Œä¸­â€œåˆ©ç”¨ç¦»çº¿å¤šè§†è§’æ•°æ®å­¦ä¹ é€šç”¨è¡¨å¾ï¼Œåœ¨çº¿éƒ¨ç½²æ—¶ä»…ä¾èµ–å•ç›®ç›¸æœºâ€çš„å®ç”¨èŒƒå¼ã€‚\n- **å®é™…ä»·å€¼ä½“ç°**ï¼šè¿™å¥—è¯„ä¼°æ–¹æ¡ˆè¡¨æ˜ï¼Œ**GEM3D** çš„æˆåŠŸä¸ä»…åœ¨äºä»»åŠ¡å®Œæˆç‡æå‡ï¼ˆå¹³å‡+12.7%ï¼‰ï¼Œæ›´åœ¨äºå…¶**å¼ºå¤§çš„å®ç”¨æ€§**â€”â€”åœ¨æ‘„åƒæœºè§†è§’å‘ç”Ÿä¸­ç­‰åˆ°å¤§å¹…å˜åŒ–æ—¶ï¼Œæ€§èƒ½ä¸‹é™è¿œå°äºç°æœ‰æ–¹æ³•ï¼ˆä»…ä¸‹é™22.0%/29.7% vs SOTAçš„41.6%/51.5%ï¼‰ï¼Œè¿™å¯¹å…¶åœ¨çœŸå®ç¯å¢ƒï¼ˆæ‘„åƒæœºä½ç½®ä¸å›ºå®šã€å­˜åœ¨é®æŒ¡ï¼‰ä¸­çš„é²æ£’éƒ¨ç½²è‡³å…³é‡è¦ã€‚é«˜è´¨é‡çš„é‡å»ºç»“æœä¹Ÿé—´æ¥è¯æ˜äº†å…¶å­¦ä¹ åˆ°çš„3Dè¡¨å¾å…·æœ‰ç²¾ç¡®çš„å‡ ä½•åŸºç¡€ï¼Œè€Œéä»…ä»…æ‹Ÿåˆå¤–è§‚ã€‚",
            "timestamp": "2026-02-03T15:58:49.824172",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## å®éªŒæ•°æ®é›†ä¸è¯„ä¼°ç»“æœ\n\n### æ•°æ®é›†\nè®ºæ–‡åœ¨ **RLBench** æœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚å…·ä½“åŒ…æ‹¬ï¼š\n- **12ä¸ªä»»åŠ¡**ï¼Œè¦†ç›–9ä¸ªä¸åŒçš„åœºæ™¯ï¼ˆéƒ¨åˆ†åœºæ™¯åŒ…å«å¤šä¸ªä»»åŠ¡å˜ä½“ï¼‰ã€‚\n- ä»»åŠ¡ç¤ºä¾‹ï¼š`Close Jar`ã€`Open Drawer`ã€`Meat off Grill`ã€`Turn Tap`ã€`Water Plants`ã€`Phone on Base`ã€`Put Money in Safe`ã€`Place Wine`ã€`Stack Blocks`ã€`Open Jar`ã€`Meat on Grill`ã€`Take Money out Safe`ã€‚\n- æ¯ä¸ªä»»åŠ¡åŒ…å«**178ç§å˜ä½“**ï¼ˆè¯­è¨€æŒ‡ä»¤ã€ç‰©ä½“ä½ç½®ã€é¢œè‰²ç­‰éšæœºå˜åŒ–ï¼‰ã€‚\n- é¢„è®­ç»ƒä½¿ç”¨**å¤šè§†è§’æ•°æ®**ï¼ˆ8ä¸ªå‡åŒ€åˆ†å¸ƒçš„ç¯ç»•æ‘„åƒå¤´ï¼‰ï¼Œå…±450ä¸ªè½¨è¿¹ã€‚\n- ç­–ç•¥å­¦ä¹ ä½¿ç”¨**å•è§†è§’ä¸“å®¶æ¼”ç¤º**ï¼ˆå›ºå®šå‰è§†æ‘„åƒå¤´ï¼‰ï¼Œæ¯ä¸ªä»»åŠ¡20æ¡æ¼”ç¤ºã€‚\n\n### ä¸»è¦è¯„ä¼°æŒ‡æ ‡\n1. **ä»»åŠ¡æˆåŠŸç‡ï¼ˆSuccess Rate, SRï¼‰**ï¼š\n   - å®šä¹‰ä¸ºæ¯ä¸ªæµ‹è¯•å›åˆçš„æˆåŠŸï¼ˆ1ï¼‰æˆ–å¤±è´¥ï¼ˆ0ï¼‰ï¼Œæ— éƒ¨åˆ†å¥–åŠ±ã€‚\n   - æ¯ä¸ªæ¨¡å‹ä½¿ç”¨3ä¸ªéšæœºç§å­è®­ç»ƒï¼Œæ¯ä¸ªä»»åŠ¡æµ‹è¯•100æ¬¡ï¼ŒæŠ¥å‘Šå¹³å‡æˆåŠŸç‡åŠæ ‡å‡†å·®ã€‚\n\n2. **é›¶æ ·æœ¬è§†è§’æ³›åŒ–èƒ½åŠ›**ï¼š\n   - åœ¨ä¸‰ç§è§†è§’åç§»ä¸‹è¯„ä¼°ï¼š\n     - **æ— åç§»**ï¼šä¸è®­ç»ƒè§†è§’ç›¸åŒã€‚\n     - **ä¸­ç­‰åç§»**ï¼šç›¸æœºåœ¨æœºå™¨äººåŸºåº§å‘¨å›´30Â°å†…ç§»åŠ¨ã€‚\n     - **å¤§åç§»**ï¼šç›¸æœºåœ¨60Â°è‡³90Â°èŒƒå›´å†…å¤§å¹…ç§»åŠ¨ã€‚\n   - æŠ¥å‘ŠæˆåŠŸç‡åŠç›¸å¯¹äºè®­ç»ƒè§†è§’çš„**æ€§èƒ½ä¸‹é™ç™¾åˆ†æ¯”**ã€‚\n\n### æ€§èƒ½æå‡\n#### 1. æ•´ä½“æ€§èƒ½ï¼ˆ12ä¸ªä»»åŠ¡å¹³å‡ï¼‰\n- **GEM3D** å¹³å‡æˆåŠŸç‡ä¸º **44.2%**ã€‚\n- ç›¸æ¯”ä¹‹å‰çš„æœ€ä½³æ–¹æ³• **ManiGaussian**ï¼ˆ31.5%ï¼‰ï¼Œç»å¯¹æå‡ **12.7%**ã€‚\n\n#### 2. è§†è§’æ³›åŒ–èƒ½åŠ›ï¼ˆ6ä¸ªä»£è¡¨æ€§ä»»åŠ¡ï¼‰\n- **ä¸­ç­‰è§†è§’åç§»**ï¼š\n  - GEM3D æˆåŠŸç‡ä¸‹é™ä»… **22.0%**ã€‚\n  - ManiGaussian ä¸‹é™ **41.6%**ï¼ŒGEM3D ç›¸å¯¹æ›´ç¨³å¥ã€‚\n- **å¤§è§†è§’åç§»**ï¼š\n  - GEM3D æˆåŠŸç‡ä¸‹é™ **29.7%**ã€‚\n  - ManiGaussian ä¸‹é™ **51.5%**ï¼ŒGEM3D æ³›åŒ–èƒ½åŠ›æ˜¾è‘—æ›´å¼ºã€‚\n\n#### 3. åœºæ™¯é‡å»ºè´¨é‡\n- **å®šé‡æŒ‡æ ‡**ï¼ˆå¹³å‡ï¼‰ï¼š\n  - **PSNR**ï¼šGEM3Dï¼ˆ23.13 dBï¼‰ vs. ManiGaussianï¼ˆ16.05 dBï¼‰ï¼Œæå‡ **+7.08 dB**ã€‚\n  - **SSIM**ï¼šGEM3Dï¼ˆ0.8136ï¼‰ vs. ManiGaussianï¼ˆ0.3838ï¼‰ï¼Œæå‡ **+0.4298**ã€‚\n  - **Chamfer L2è·ç¦»**ï¼šGEM3Dï¼ˆ0.01621ï¼‰ vs. ManiGaussianï¼ˆ0.04511ï¼‰ï¼Œé™ä½ **-0.0289**ï¼ˆå‡ ä½•æ›´ç²¾ç¡®ï¼‰ã€‚\n\n#### 4. å¤šä»»åŠ¡è¯„ä¼°\n- åœ¨4ä¸ªä»»åŠ¡çš„æ··åˆæ•°æ®é›†ä¸Šè®­ç»ƒç»Ÿä¸€æ¨¡å‹ï¼š\n  - GEM3D åœ¨è®­ç»ƒè§†è§’ä¸‹å¹³å‡æˆåŠŸç‡ä¸º **52.9%**ï¼Œç›¸æ¯”å•ä»»åŠ¡è®¾ç½®ä»…ä¸‹é™ **1.7%**ï¼ˆè¡¨ç°ç¨³å®šï¼‰ã€‚\n  - åŸºçº¿æ–¹æ³• PerAct åœ¨è§†è§’åç§»ä¸‹å‡ºç°ç­–ç•¥å´©æºƒï¼Œè€Œ GEM3D ä¸‹é™å¹…åº¦è¾ƒå°ã€‚\n\n### å…³é”®ç»“è®º\n- **GEM3D åœ¨æ“ä½œæˆåŠŸç‡å’Œè§†è§’æ³›åŒ–æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•**ã€‚\n- å…¶æ ¸å¿ƒä¼˜åŠ¿æºäºï¼š\n  - **å‡ ä½•åŸºç¡€çš„ä¸‰ç»´è¡¨ç¤ºé¢„è®­ç»ƒ**ï¼ˆç‚¹äº‘é‡å»º + é«˜æ–¯æ³¼æº…æ¸²æŸ“ï¼‰ã€‚\n  - **å¤šæ­¥è’¸é¦ç­–ç•¥å­¦ä¹ **ï¼Œæœ‰æ•ˆä¿ç•™ä¸‰ç»´çŸ¥è¯†å¹¶è¿ç§»åˆ°æ“ä½œæŠ€èƒ½ã€‚\n- å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨**å•è§†è§’è¾“å…¥**ä¸‹ä»èƒ½å®ç°**å¼ºè§†è§’ä¸å˜æ€§**ï¼Œé€‚ç”¨äºå®é™…æœºå™¨äººéƒ¨ç½²åœºæ™¯ã€‚",
            "timestamp": "2026-02-03T15:59:20.717109",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„åˆ†æï¼Œ**GEM3D** æ¡†æ¶è™½ç„¶å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹å‡ ä¸ªå…³é”®å±€é™æ€§ï¼š\n\n### 1. **å¯¹ä¸¥é‡é®æŒ¡åœºæ™¯çš„é²æ£’æ€§ä¸è¶³**\n- **é—®é¢˜æè¿°**ï¼šåœ¨è§†è§’å‘ç”Ÿ**å¤§èŒƒå›´åç§»**ï¼ˆå¦‚ `60Â°-90Â°`ï¼‰æ—¶ï¼Œå¦‚æœå…³é”®æ“ä½œåŒºåŸŸï¼ˆå¦‚æŠ½å±‰å†…éƒ¨ã€ä¿é™©ç®±å†…éƒ¨ï¼‰è¢«ä¸¥é‡é®æŒ¡ï¼Œæ¨¡å‹çš„æ€§èƒ½ä¼šå‡ºç°**æ˜¾è‘—ä¸‹é™**ï¼ˆå¦‚ `Open Drawer` å’Œ `Take Money out Safe` ä»»åŠ¡åœ¨ä¸¥é‡é®æŒ¡ä¸‹æˆåŠŸç‡æ¥è¿‘å´©æºƒï¼‰ã€‚è¿™è¡¨æ˜æ¨¡å‹ä»ä¾èµ–äº**å¯è§çš„æ¥è§¦ç»†èŠ‚**æ¥è¿›è¡Œæ“ä½œã€‚\n- **è¯æ®**ï¼šè¡¥å……å®éªŒï¼ˆé™„å½•è¡¨4ï¼‰æ˜¾ç¤ºï¼Œåœ¨ä¸¥é‡é®æŒ¡çš„â€œå¤§åç§»â€è®¾ç½®ä¸‹ï¼ŒæŸäº›ä»»åŠ¡çš„æ€§èƒ½ä¸‹é™è¶…è¿‡ `80%`ã€‚\n\n### 2. **é¢„è®­ç»ƒå¯¹å¤šè§†è§’æ•°æ®çš„ä¾èµ–**\n- **é—®é¢˜æè¿°**ï¼šå°½ç®¡è®ºæ–‡è¯æ˜ä½¿ç”¨ `2` ä¸ªè§†è§’è¿›è¡Œé¢„è®­ç»ƒä»èƒ½å–å¾—ä¸é”™æ•ˆæœï¼Œä½†æ•´ä¸ªæ¡†æ¶çš„èµ·ç‚¹ä»æ˜¯**å¤šè§†è§’ç›‘ç£**ã€‚åœ¨çœŸå®ä¸–ç•Œä¸­ï¼Œè·å–å¯†é›†ã€æ ¡å‡†çš„å¤šè§†è§’æ•°æ®ï¼ˆå°¤å…¶æ˜¯åŠ¨æ€æœºå™¨äººåœºæ™¯ï¼‰æˆæœ¬é«˜æ˜‚ä¸”ä¸åˆ‡å®é™…ã€‚\n- **è¯æ®**ï¼šé¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨äº† `8` ä¸ªç¯ç»•æ‘„åƒå¤´çš„æ•°æ®è¿›è¡Œç›‘ç£ã€‚è™½ç„¶æ¶ˆèå®éªŒè¡¨æ˜å‡å°‘è§†è§’æ•°å½±å“ä¸å¤§ï¼Œä½†å®Œå…¨**é›¶è§†è§’**ï¼ˆå³ä¸ä½¿ç”¨é«˜æ–¯æº…å°„æ¸²æŸ“æ¨¡å—ï¼‰ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™è¿‘ `10%`ã€‚\n\n### 3. **è®¡ç®—å¼€é”€ä¸å®æ—¶æ€§æŒ‘æˆ˜**\n- **é—®é¢˜æè¿°**ï¼šæ¡†æ¶åŒ…å«**ä¸‰é˜¶æ®µé¢„è®­ç»ƒ**ï¼ˆä½“ç´ ç‰¹å¾æå–ã€ç‚¹äº‘é‡å»ºã€é«˜æ–¯æº…å°„æ¸²æŸ“ï¼‰å’Œ**å¤šæ­¥è’¸é¦ç­–ç•¥**ï¼Œæ•´ä½“æµç¨‹å¤æ‚ã€‚é¢„è®­ç»ƒéœ€ `4-5` å¤©ï¼ˆ2å—GPUï¼‰ï¼Œç­–ç•¥è®­ç»ƒéœ€ `3-4` å°æ—¶/ä»»åŠ¡ã€‚è¿™é™åˆ¶äº†å…¶åœ¨éœ€è¦å¿«é€Ÿéƒ¨ç½²æˆ–è®¡ç®—èµ„æºå—é™åœºæ™¯ä¸­çš„åº”ç”¨ã€‚\n- **è¯æ®**ï¼šæ–¹æ³•éƒ¨åˆ†è¯¦ç»†æè¿°äº†å¤æ‚çš„æµæ°´çº¿ï¼Œå®éªŒè®¾ç½®ä¹Ÿæ˜ç¡®äº†è®­ç»ƒæ—¶é—´æˆæœ¬ã€‚\n\n### 4. **æ³›åŒ–èƒ½åŠ›ä»å—é™äºè®­ç»ƒåˆ†å¸ƒ**\n- **é—®é¢˜æè¿°**ï¼š\n    - **åœºæ™¯æ³›åŒ–**ï¼šæ‰€æœ‰å®éªŒå‡åœ¨ **RLBench ä»¿çœŸç¯å¢ƒ**ä¸­è¿›è¡Œï¼Œå…¶ç‰©ä½“ã€çº¹ç†å’Œç‰©ç†ç‰¹æ€§ä¸çœŸå®ä¸–ç•Œå­˜åœ¨å·®è·ã€‚\n    - **ä»»åŠ¡æ³›åŒ–**ï¼šè™½ç„¶è¿›è¡Œäº†å¤šä»»åŠ¡è¯„ä¼°ï¼Œä½†ä»»åŠ¡å‡æ¥è‡ª `9` ä¸ªç›¸ä¼¼åœºæ™¯ã€‚æ¨¡å‹å¯¹äº**å…¨æ–°å‡ ä½•å½¢çŠ¶ã€å…¨æ–°ç‰©ä½“ç±»åˆ«æˆ–å…¨æ–°ä»»åŠ¡æŒ‡ä»¤**çš„æ³›åŒ–èƒ½åŠ›å°šæœªéªŒè¯ã€‚\n- **è¯æ®**ï¼šå®éªŒä»…é™äº `12` ä¸ª RLBench ä»»åŠ¡ï¼Œä¸”å¤šä»»åŠ¡è®­ç»ƒä»…æ··åˆäº† `4` ä¸ªä»»åŠ¡çš„æ•°æ®ã€‚\n\n### 5. **è¡¨å¾çš„â€œé»‘ç®±â€ç‰¹æ€§**\n- **é—®é¢˜æè¿°**ï¼šå­¦ä¹ åˆ°çš„**å¯†é›†ä½“ç´ ç‰¹å¾**å’Œè’¸é¦åçš„ç­–ç•¥è¡¨å¾ç¼ºä¹æ˜ç¡®çš„ã€äººç±»å¯è§£é‡Šçš„å‡ ä½•æˆ–è¯­ä¹‰ç»“æ„ã€‚è¿™ç»™**é”™è¯¯è¯Šæ–­ã€å®‰å…¨æ€§éªŒè¯å’Œæ¨¡å‹è°ƒè¯•**å¸¦æ¥äº†å›°éš¾ã€‚\n- **è¯æ®**ï¼šæ¡†æ¶ä¾èµ–äºé€šè¿‡æŸå¤±å‡½æ•°ï¼ˆChamferè·ç¦»ã€Focal Lossã€è’¸é¦æŸå¤±ï¼‰éšå¼åœ°å­¦ä¹ å‡ ä½•ä¸€è‡´æ€§ï¼Œè€Œéæ˜¾å¼åœ°é‡å»ºè¯­ä¹‰åˆ†å‰²æˆ–ç‰©ä½“å§¿æ€ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nè®ºæ–‡åœ¨ç»“è®ºå’Œå®éªŒåˆ†æä¸­å·²æš—ç¤ºäº†ä»¥ä¸‹æœªæ¥æ–¹å‘ï¼š\n\n### 1. **å¼€å‘å¯¹é®æŒ¡æ›´é²æ£’çš„è¡¨å¾ä¸ç­–ç•¥**\n- **æ–¹å‘**ï¼šèåˆ**ä¸»åŠ¨æ„ŸçŸ¥**ï¼ˆå¦‚ä¸»åŠ¨ç§»åŠ¨æ‘„åƒå¤´ï¼‰æˆ–**ç‰©ç†å¸¸è¯†**ï¼ˆå¦‚ç‰©ä½“ä¸å¯ç©¿é€æ€§ï¼‰æ¥æ¨ç†è¢«é®æŒ¡éƒ¨åˆ†çš„å‡ ä½•å½¢çŠ¶ã€‚\n- **æ–¹æ³•**ï¼šç»“åˆ**åœºæ™¯å®Œæˆ**æˆ–**4DåŠ¨æ€åœºæ™¯å»ºæ¨¡**æŠ€æœ¯ï¼Œä»å•è§†è§’æˆ–ç¨€ç–è§†è§’ä¸­é¢„æµ‹è¢«é®æŒ¡éƒ¨åˆ†çš„æŒç»­çŠ¶æ€ã€‚\n\n### 2. **è¿ˆå‘å¤§è§„æ¨¡3Dé¢„è®­ç»ƒä¸æ›´å°‘ç›‘ç£**\n- **æ–¹å‘**ï¼šæ„å»º**å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„3Dç‰©ä½“ä¸åœºæ™¯æ•°æ®é›†**ï¼Œè¿›è¡Œé¢„è®­ç»ƒä»¥å­¦ä¹ æ›´é€šç”¨çš„3Då…ˆéªŒã€‚\n- **æ–¹æ³•**ï¼š\n    - åˆ©ç”¨äº’è”ç½‘è§„æ¨¡çš„**å•ç›®è§†é¢‘**æˆ–**æ— åºå›¾åƒé›†**è¿›è¡Œè‡ªç›‘ç£3Dè¡¨å¾å­¦ä¹ ã€‚\n    - æ¢ç´¢**é›¶æ ·æœ¬/å°‘æ ·æœ¬**çš„è§†è§’æ³›åŒ–æ–¹æ³•ï¼Œå‡å°‘å¯¹å¯†é›†å¤šè§†è§’ç›‘ç£çš„ä¾èµ–ã€‚\n\n### 3. **æå‡è®¡ç®—æ•ˆç‡ä¸éƒ¨ç½²å¯è¡Œæ€§**\n- **æ–¹å‘**ï¼šç®€åŒ–æ¡†æ¶ï¼Œä½¿å…¶æ›´é€‚åˆ**å®æ—¶æ¨ç†**å’Œ**è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²**ã€‚\n- **æ–¹æ³•**ï¼š\n    - è®¾è®¡æ›´è½»é‡çš„**3Dç‰¹å¾ç¼–ç å™¨**ï¼ˆå¦‚ç¨€ç–ä½“ç´ ã€ç¥ç»éšå«åœºï¼‰ã€‚\n    - ç ”ç©¶**ä¸€æ¬¡æ€§**æˆ–**å…ƒå­¦ä¹ **ç­–ç•¥ï¼ŒåŠ é€Ÿå¯¹æ–°ä»»åŠ¡å’Œæ–°è§†è§’çš„é€‚åº”ã€‚\n\n### 4. **å¢å¼ºè·¨åœºæ™¯ä¸è·¨å½¢æ€çš„æ³›åŒ–èƒ½åŠ›**\n- **æ–¹å‘**ï¼šæµ‹è¯•å¹¶æå‡æ¨¡å‹åœ¨**çœŸå®æœºå™¨äººå¹³å°ã€å¤æ‚å¼€æ”¾ç¯å¢ƒåŠå¤šæ ·åŒ–ç‰©ä½“**ä¸Šçš„æ€§èƒ½ã€‚\n- **æ–¹æ³•**ï¼š\n    - åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯å’Œå¾®è°ƒã€‚\n    - ç»“åˆ**è¯­è¨€-è§†è§‰å¤§æ¨¡å‹**çš„è¯­ä¹‰å…ˆéªŒï¼Œå®ç°åŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„é›¶æ ·æœ¬æ“ä½œæ³›åŒ–ã€‚\n\n### 5. **æé«˜ç³»ç»Ÿçš„å¯è§£é‡Šæ€§ä¸å¯é æ€§**\n- **æ–¹å‘**ï¼šä½¿å­¦ä¹ åˆ°çš„3Dè¡¨å¾å’Œå†³ç­–è¿‡ç¨‹æ›´åŠ **é€æ˜å’Œå¯è§£é‡Š**ã€‚\n- **æ–¹æ³•**ï¼š\n    - åœ¨é¢„è®­ç»ƒæˆ–è’¸é¦ä¸­å¼•å…¥**æ˜¾å¼å‡ ä½•çº¦æŸ**ï¼ˆå¦‚å¯¹ç§°æ€§ã€æ”¯æ’‘å…³ç³»ï¼‰ã€‚\n    - è¾“å‡º**ä¸­é—´å¯è§†åŒ–ç»“æœ**ï¼ˆå¦‚é¢„æµ‹çš„æŠ“å–ç‚¹ã€ç¢°æ’åŒºåŸŸï¼‰ï¼Œä¾¿äºäººç±»ç›‘ç£å’Œè°ƒè¯•ã€‚\n\n### 6. **æ‰©å±•è’¸é¦æ¡†æ¶çš„é€‚ç”¨æ€§**\n- **æ–¹å‘**ï¼šå°†è®ºæ–‡æå‡ºçš„**å¤šæ­¥è’¸é¦ç­–ç•¥**ä¸æ›´å¤šç±»å‹çš„é¢„è®­ç»ƒè¡¨å¾ï¼ˆå¦‚æ¥è‡ªå¤§è§„æ¨¡è§†é¢‘çš„4Dè¡¨å¾ã€ç‰©ç†ä»¿çœŸæ¨¡å‹ï¼‰ç»“åˆï¼Œå­¦ä¹ æ›´ä¸°å¯Œçš„æ“ä½œæŠ€èƒ½ã€‚\n- **æ–¹æ³•**ï¼šå°†GEM3D Policyä½œä¸ºä¸€ä¸ª**é€šç”¨é€‚é…å™¨**ï¼Œç ”ç©¶å…¶ä¸ä¸åŒä¸Šæ¸¸è¡¨å¾æ¨¡å‹çš„å…¼å®¹æ€§å’Œæå‡æ•ˆæœã€‚\n\n---\n**æ€»ç»“**ï¼šGEM3D åœ¨**è§†è§’æ³›åŒ–çš„æœºå™¨äººæ“ä½œ**ä¸Šè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œä½†å…¶å±€é™æ€§æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶åœ¨**æ•°æ®æ•ˆç‡ã€è®¡ç®—æ•ˆç‡ã€æ³›åŒ–é²æ£’æ€§å’Œç³»ç»Ÿå¯è§£é‡Šæ€§**ç­‰å…³é”®ç»´åº¦ä¸Šçš„æ”¹è¿›ç©ºé—´ã€‚è¿™äº›æ–¹å‘å°†æ¨åŠ¨è§†è§‰è¿åŠ¨ç­–ç•¥å‘æ›´å®ç”¨ã€æ›´é€šç”¨çš„ç°å®ä¸–ç•Œéƒ¨ç½²è¿ˆè¿›ã€‚",
            "timestamp": "2026-02-03T16:00:08.378805",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T13:53:53Z",
    "created_at": "2026-02-03T15:53:09.552539",
    "updated_at": "2026-02-03T15:53:09.552547"
}