{
    "id": "2601.06806v1",
    "title": "SpatialNav: Leveraging Spatial Scene Graphs for Zero-Shot Vision-and-Language Navigation",
    "authors": [
        "Jiwen Zhang",
        "Zejun Li",
        "Siyuan Wang",
        "Xiangyu Shi",
        "Zhongyu Wei",
        "Qi Wu"
    ],
    "abstract": "尽管基于学习的视觉语言导航智能体能够从大规模训练数据中隐式学习空间知识，但零样本视觉语言导航智能体缺乏这一过程，主要依赖局部观测进行导航，导致探索效率低下且存在显著性能差距。为解决该问题，我们提出一种允许智能体在执行任务前充分探索环境的零样本视觉语言导航框架。在此基础上，我们构建了空间场景图来显式捕获已探索环境中的全局空间结构与语义信息。基于该空间场景图，我们提出SpatialNav零样本视觉语言导航智能体，其整合了以智能体为中心的空间地图、罗盘对齐的视觉表征以及远程目标定位策略，以实现高效导航。在离散与连续环境中的综合实验表明，SpatialNav显著优于现有零样本智能体，并明显缩小了与最先进基于学习方法的性能差距。这些结果凸显了全局空间表征对泛化性导航的重要性。",
    "url": "https://arxiv.org/abs/2601.06806v1",
    "html_url": "https://arxiv.org/html/2601.06806v1",
    "html_content": "SpatialNav: Leveraging Spatial Scene Graphs for Zero-Shot Vision-and-Language Navigation\nJiwen Zhang\nZejun Li\nSiyuan Wang\nXiangyu Shi\nZhongyu Wei\nQi Wu\nAbstract\nAlthough learning-based vision-and-language navigation (VLN) agents can learn spatial knowledge implicitly from large-scale training data, zero-shot VLN agents lack this process, relying primarily on local observations for navigation, which leads to inefficient exploration and a significant performance gap. To deal with the problem, we consider a zero-shot VLN setting that agents are allowed to fully explore the environment before task execution. Then, we construct the\nSpatial Scene Graph (SSG)\nto explicitly capture global spatial structure and semantics in the explored environment. Based on the SSG, we introduce\nSpatialNav\n, a zero-shot VLN agent that integrates an agent-centric spatial map, a compass-aligned visual representation, and a remote object localization strategy for efficient navigation. Comprehensive experiments in both discrete and continuous environments demonstrate that SpatialNav significantly outperforms existing zero-shot agents and clearly narrows the gap with state-of-the-art learning-based methods. Such results highlight the importance of global spatial representations for generalizable navigation.\n1\nIntroduction\nVision-and-Language Navigation (VLN)\n(Anderson et al.,\n2018\n; Krantz et al.,\n2020\n)\nis a fundamental problem in the embodied AI community, which requires an agent to follow natural language instructions to navigate in complex real-world environments. Early works in VLN mainly focused on supervised learning by designing dedicated model architectures\n(Hong et al.,\n2021\n; Qiao et al.,\n2022\n; Chen et al.,\n2022\n)\nor introducing better training strategies\n(Wang et al.,\n2019\n; Zhang et al.,\n2021\n)\n. More recently, the success of Large Language Models (LLMs) has inspired the adoption of multimodal large language models (MLLMs) to construct zero-shot VLN agents, such as NavGPT\n(Zhou et al.,\n2024b\n)\nand Open-Nav\n(Qiao et al.,\n2025\n)\n. Although these zero-shot agents demonstrate good generalization capabilities without task-specific training, there is still a significant performance gap compared to learning-based methods.\nFigure 1\n:\nIllustration of local v.s. global perception during navigation.\nWhen the instruction mentions a\nbedroom\n, agent maybe confused by local perception if multiple bedrooms are present and plausible. In contrast, global spatial information enables the agent to disambiguate these options and make more accurate actions.\nThe key factor contributing to this disparity lies in how zero-shot agents perceive the environment. Unlike learning-based VLN agents which can implicitly acquire the regularities about room layouts and functional co-occurrences (e.g. kitchens often connect to dining areas, and closets are usually adjacent to bedrooms) through large-scale pre-training\n(Chen et al.,\n2021b\n; Wang et al.,\n2023a\n; Zhou et al.,\n2024a\n)\n, zero-shot agents typically rely on constrained local observations for decision making without access to such spatial priors\n(Zhou et al.,\n2024b\n; Chen et al.,\n2024\n; Shi et al.,\n2025\n; He et al.,\n2025\n)\n. Therefore, navigation decisions are often short-sighted, leading to inefficient exploration. Moreover, as shown in Figure\n1\n, under such a local perception mechanism, navigation becomes particularly challenging when multiple instruction-consistent actions co-exist, as the agent lacks sufficient global information to disambiguate among them. However, how to enable zero-shot agents to access and use global spatial information for effective navigation remains a fundamental challenge.\nTo resolve the challenge, we propose a new zero-shot VLN setting that allows agents to fully explore the environment before task execution. This setting is grounded in practical considerations, since in real-world household applications such as vacuum cleaners, robots are deployed in bounded indoor environments and rarely switch scenes.\nWhile it departs from the original definition of VLN that restricts agents to purely online local perception, this setting reflects realistic deployment scenarios and opens up new opportunities for utilizing global spatial information.\nUnder this setting, we propose to equip zero-shot VLN agents with global perception ability by constructing a\nSpatial Scene Graph (SSG)\nthat explicitly captures global spatial layouts and semantics in the environment.\nSince environment reconstruction through pre-exploration using off-the-shelf SLAM systems\n(Liu et al.,\n2025\n)\nis feasible, we design an automatic pipeline to organize the reconstructed point cloud into a hierarchical graph structure. This pipeline involves segmenting floors and rooms, annotating room categories and detecting objects with labels. Our spatial scene graph serves as a compact knowledge base of the environment, enabling the agent to conduct long-horizon reasoning beyond local observations.\nTo effectively apply the spatial scene graph for navigation, we design\nSpatialNav\n, a zero-shot VLN agent with three novel components. (1) Since a spatial scene graph typically contains far more information than required for a specific navigation task, we propose to construct\nan agent-centric spatial map\nby querying the SSG with the current location of the agent and retrieving the task-relevant spatial layout within a bounded region. (2) We propose to organize the panoramic observations into\na compass-like visual representation\n, ensuring consistent orientation with the spatial map. (3) To support future-aware decision making, we further introduce\nremote object localization\n, which queries the SSG to retrieve the objects around candidate navigable locations, enabling the agent to reason about future observations beyond its current field of view. Extensive experiments in both discrete and continuous environments validate the superiority of SpatialNav, which achieves a success rate of 57.7%, 49.6%, 64.0%, 32.4% on val-unseen splits of R2R, REVERIE, R2R-CE, RxR-CE, respectively, surpassing all previous zero-shot agents and even several supervised-learning methods.\nIn conclusion, our contributions are:\n•\nWe introduce a novel zero-shot VLN setting that allows agents to pre-explore the environment, and construct the\nSpatial Scene Graph (SSG)\nto efficiently encode global spatial structure and semantic information from pre-exploration.\n•\nWe design\nSpatialNav\n, a zero-shot VLN agent that leverages the spatial scene graph through agent-centric spatial map, compass-like visual representation, and remote object localization.\n•\nWe empirically prove that global spatial information can be effectively utilized by different agents and across different environments. Our results indicate that zero-shot VLN agents equipped with spatial scene graphs can achieve comparable performance with SOTA learning-based agents.\nFigure 2\n:\nOverview of the spatial scene graph construction.\nGiven the point cloud input, we employ a four-stage annotation pipeline, including floor segmentation via height-based clustering, room segmentation within each floor using geometric heuristics, room classification based on visual observations and object detection from room-level point clouds. The definitions of node entities in the spatial scene graph are illustrated on the right.\n2\nRelated Works\nVision-Language Navigation (VLN).\nVLN\n(Anderson et al.,\n2018\n; Krantz et al.,\n2020\n; Ku et al.,\n2020\n)\nis one of fundamental challenges in embodied AI.\nEarly studies predominantly focused on discrete environments\n(Qi et al.,\n2020\n; Jain et al.,\n2019\n; Zhu et al.,\n2021\n)\n, where the action space is abstracted as a predefined navigation graph. For better adaptation to realistic open-world scenarios,\n(Krantz et al.,\n2020\n; Hong et al.,\n2022\n)\nhas extended VLN to continuous environments\n(Savva et al.,\n2019\n)\nthat allow low-level control. Under both settings, supervised training\n(Tan et al.,\n2019\n; Hao et al.,\n2020\n; Zhu et al.,\n2020\n; Zhang et al.,\n2021\n; Chen et al.,\n2021b\n; Wang et al.,\n2023a\n; Zhang et al.,\n2025a\n; Du et al.,\n2024\n; Zhang et al.,\n2025c\n)\non annotated datasets is the mainstream paradigm for advancing the performance, especially when combined with MLLMs\n(Zhang et al.,\n2024\n; Wei et al.,\n2025\n; Zheng et al.,\n2025\n)\n.\nHowever, these learning-based VLN agents have significant reliance on large-scale, domain-specific training, limiting their generalization to unseen environments.\nZero-Shot VLN Agents.\nRecent works have investigated zero-shot VLN agents\n(Long et al.,\n2024\n; Qiao et al.,\n2025\n; Chen et al.,\n2025\n)\nthat leverage (M)LLMs for navigation. These methods typically rely on complex prompting strategies, such as action-aware reasoning\n(Zhou et al.,\n2024b\n; Chen et al.,\n2023\n)\n, deliberative planning\n(Long et al.,\n2025\n)\n, progress estimation\n(He et al.,\n2025\n)\nand mistake reflection\n(Shi et al.,\n2025\n)\n.\nDespite these advances, these agents still lag behind the learning-based agents by a clear margin. Therefore, we propose Spatial-Nav, a strong zero-shot VLN agent that achieves competitive performance with SOTA supervised learning-based agents.\nEnvironment Exploration in VLN.\nPrevious learning-based agents have incorporated environment exploration as a useful tool to refine the navigation policy, either by self-supervised learning on successful exploration trajectories\n(Wang et al.,\n2019\n)\nor by training a topological map planner\n(Chen et al.,\n2021a\n)\n. Recently, VL-KnG\n(Mdfaa et al.,\n2025\n)\nemploys pre-explored videos to construct object-centric knowledge graphs for goal identification, proving that exploration can compensate for the absence of learned spatial priors. However, such pre-exploration settings have not been systematically validated for zero-shot VLN agents. Although VLN-Zero\n(Bhatt et al.,\n2025\n)\ntakes a step in this direction by building a symbolic scene graph after exploration, its exploration remains limited as it focuses only on symbolic constraints, overlooking the spatial layouts and semantics that are critical for VLN.\nTo mitigate this gap, we propose to fully explore the environment and to build spatial scene graphs that integrate global layouts with room and object semantics, enabling efficient and generalizable navigation.\nFigure 3\n:\nThe framework of SpatialNav agent.\nBased on its current position, SpatialNav queries the SSG to construct an agent-centric spatial map\nand to retrieve the object semantic descriptions around navigable places.\nPanoramic visual observations are organized into a single compass-like image. These information, combined with trajectory history and language instructions, constitutes the context of SpatialNav for predicting the next action.\n3\nMethods\nProblem setup.\nFollowing real-world applications such as robotic vacuum cleaners, we adopt a VLN setting where an agent is allowed to pre-explore the environment before executing the tasks. Since dense or sparse point clouds can be efficiently obtained using existing SLAM systems\n(Liu et al.,\n2025\n; Murai et al.,\n2025\n; Maggio et al.,\n2025\n)\nby taking RGB videos, we make the practical assumption that the 3D point cloud of the environment is available after exploration.\nGiven the reconstructed 3D structure provided by Matterport3D dataset\n(Chang et al.,\n2017\n)\n, we focus on two key objectives:\nFirstly, to enable spatial-enhanced navigation, we endow the raw 3D point cloud with structured spatial annotations (e.g., floor–room–object hierarchy) and semantic labels (e.g., room types and object categories), constructing a spatial scene graph that can be reused across different episodes.\nSecondly, we propose a SpatialNav agent to efficiently utilize the resulting spatial scene graph.\n3.1\nSpatial Scene Graph (SSG)\nWe construct the spatial scene graph by supplementing structural and semantic annotations on the raw point cloud, as demonstrated in Figure\n2\n. The process is divided into four stages: (1)\nFloor Segmentation\n: Following\n(Werby et al.,\n2024\n)\n, we segment the floors by calculating a height histogram of all points, applying DBSCAN and selecting the highest-ranking peaks as the floors. (2)\nRoom Segmentation\n: Then, we perform room-level segmentation within each floor. We adopt the geometric heuristic-based method proposed by\n(Bobkov et al.,\n2017\n)\n, which formulates room segmentation as a partitioning problem of enclosed regions. However, we find this approach relies heavily on the presence of strong geometric boundaries, such as walls, to delineate room regions. As a result, its performance degrades when multiple functional areas share a continuous open space. To resolve such problems, we refine the room segmentation results by manually verifying the regions with areas larger than 20 square meters. (3)\nRoom Classification\n: To assign semantic labels to each segmented room, we collect the images or video frames captured within each room during the pre-exploration phase and prompt GPT-5\n(OpenAI,\n2025\n)\nto classify the room according to a predefined room category list. (4)\nObject Detection\n: To detect objects within each room, we fine-tune SpatialLM\n(Mao et al.,\n2025\n)\n, a model that consumes the segmented room point clouds to predict the object bounding boxes with labels, on the training scans of Matterport3D\n(Chang et al.,\n2017\n)\n.\nFinally, we organize floors, rooms, and objects into a hierarchical spatial scene graph, where nodes correspond to entities (listed on the right side of Figure\n2\n) and edges represent the containment relations between different levels of the hierarchy.\n3.2\nSpatialNav Agent\nTypically, VLN agents make navigation decisions based on local visual observations, the instruction, and navigation history, predicting the next action at each step\n(Zhou et al.,\n2024b\n; Shi et al.,\n2025\n; He et al.,\n2025\n)\n. However, such a local perception field constrains the agent’s performance, as it lacks awareness of the spatial layout and semantic cues of the future. Equipped with a spatial scene graph, we resolve the limitations by designing an MLLM-based VLN agent, namely\nSpatialNav\n. SpatialNav distinguishes itself with previous (M)LLM-based agents from three distinct perspectives: (1) an agent-centric spatial map that captures the layout of surrounding rooms to support coarse-grained spatial reasoning, (2) a compass-like visual observation schema that aligns egocentric images with the orientation of the spatial map, and (3) a remote object localization strategy, which retrieves object-level semantic information around navigable places from the spatial scene graph to guide the fine-grained grounding. The framework of SpatialNav is summarized in Figure\n3\n.\nAgent-centric Spatial Map.\nPrevious LLM-based VLN agents are mostly restricted to a local perception range within about 3 meters away from their current position\n(Zhou et al.,\n2024b\n; Chen et al.,\n2024\n; Qiao et al.,\n2025\n)\n. Different from them, we enlarge the perception range by querying the spatial scene graph to generate an agent-centric spatial map. Specifically, given the position of the agent, we determine the floor level based on its Z-axis coordinate, and then identify the room it resides in according to its X-Y coordinates within that floor. After localizing the agent, we define an agent-centric spatial receptive field with a radius of approximately 7 meters within the same floor. The agent position and nearby rooms are then projected onto a top-down spatial map, with the agent heading always aligned to the upward direction. By offering a compact and structured representation of the scene, the spatial map allows the agent to reason about its immediate spatial context without being overwhelmed by irrelevant information.\nCompass-like Visual Observation.\nPrior zero-shot VLN agents, such as SpatialGPT\n(Jiang & Wang,\n2025\n)\nand Smartway\n(Shi et al.,\n2025\n)\n, usually process panoramic observations by capturing multiple images and feeding them sequentially into the MLLM for action prediction. However, this strategy results in high input token overhead. In contrast, we discretize the panorama into eight directional views (from 0° to 360°, turning right 45° each) with a field of view set as 90° to avoid adjusting elevations. Instead of feeding the visual observations of eight views sequentially, we organize them into a single image with a compass-style representation. As shown in the lower part of Figure\n3\n, this compass image is arranged as a 3×3 grid, where the eight views are placed along the perimeter of the grid in clockwise order. The center of this image is occupied by a compass that explicitly encodes the agent relative orientation with respect to these views. This schema ensures consistency between the egocentric observations with the spatial map and reduces the cost of visual inputs.\nRemote Object Localization.\nTo further help with the future planning, we propose to provide the agent with the awareness of “which objects will be observed if I took that direction”. Specifically, for each candidate navigable place obtained either from the navigation graph in discrete environments or via a waypoint predictor in continuous environments, we query the spatial scene graph to retrieve the object semantics within a local neighborhood of that place. The retrieved information includes object categories and their distances from that navigable place is then compressed as a concise textual description, added into the context of SpatialNav.\nBy incorporating these components together, SpatialNav enables long-horizon, goal-aware decision making that goes beyond local perception.\nTable 1:\nComparison with SOTAs in discrete environments on the R2R and REVERIE Val-Unseen splits.\nThe\nbest\nand the\nsecond best\nresults are denoted by\nbold\nand\nunderline\n.\n†\nmeans the spatial annotations are ground truth.\nSettings\nMethods\nR2R\nREVERIE\nTL\nNE(↓)\nOSR(↑)\nSR(↑)\nSPL(↑)\nOSR(↑)\nSR(↑)\nSPL(↑)\nSupervised\nLearning\nNavCoT\n(\nLin et al.\n,\n2024\n)\n9.95\n6.36\n48\n40\n37\n14.2\n9.2\n7.2\nPREVALENT\n(\nHao et al.\n,\n2020\n)\n10.19\n4.71\n-\n58\n53\n–\n–\n–\nVLN-BERT\n(\nHong et al.\n,\n2021\n)\n12.01\n3.93\n69\n63\n57\n27.7\n25.5\n21.1\nHAMT\n(\nChen et al.\n,\n2021b\n)\n11.46\n2.29\n73\n66\n61\n36.8\n33.0\n30.2\nDUET\n(\nChen et al.\n,\n2022\n)\n13.94\n3.31\n81\n72\n60\n51.1\n47.0\n33.7\nDUET+ScaleVLN\n(\nWang et al.\n,\n2023a\n)\n14.09\n2.09\n88\n81\n70\n63.9\n57.0\n41.8\nZero-Shot\nNavGPT\n(\nZhou et al.\n,\n2024b\n)\n11.45\n6.46\n42\n34\n29\n28.3\n19.2\n14.6\nMapGPT\n(\nChen et al.\n,\n2024\n)\n–\n5.63\n57.6\n43.7\n34.8\n36.8\n31.6\n20.3\nMC-GPT\n(\nZhan et al.\n,\n2024\n)\n–\n5.42\n68.8\n32.1\n–\n30.3\n19.4\n9.7\nSpatialGPT\n(\nJiang & Wang\n,\n2025\n)\n–\n5.56\n70.8\n48.4\n36.1\n–\n–\n–\nSpatialNav (ours)\n13.8\n4.54\n68.2\n57.7\n47.8\n58.1\n49.6\n34.6\nSpatialNav\n†\n(ours)\n13.8\n4.40\n70.8\n59.3\n48.0\n57.8\n50.4\n33.7\nTable 2:\nComparison in continuous environments on the R2R-CE and RxR-CE Val-Unseen splits.\nThe\nbest\nand the\nsecond best\nresults are denoted by\nbold\nand\nunderline\n.\n†\nmeans the spatial annotations are ground truth.\nSettings\nMethods\nR2R-CE\nRxR-CE\nNE(↓)\nOSR(↑)\nSR(↑)\nSPL(↑)\nnDTW(↑)\nNE(↓)\nSR(↑)\nSPL(↑)\nnDTW(↑)\nSupervised\nLearning\nMapNav\n(\nZhang et al.\n,\n2025b\n)\n4.93\n53.0\n39.7\n37.2\n–\n7.62\n32.6\n27.7\n43.5\nVLN-BERT\n(\nHong et al.\n,\n2021\n)\n5.74\n53.0\n44.0\n39.0\n–\n8.98\n27.1\n22.7\n46.7\nGridMM\n(\nWang et al.\n,\n2023b\n)\n5.11\n61.0\n49.0\n41.0\n–\n–\n–\n–\n–\nETPNav\n(\nAn et al.\n,\n2024\n)\n4.71\n65.0\n57.0\n49.0\n–\n5.64\n54.8\n44.9\n61.9\nHNR\n(\nWang et al.\n,\n2024\n)\n4.42\n67.0\n61.0\n51.0\n–\n5.51\n56.4\n46.7\n63.6\nNavFoM\n(\nZhang et al.\n,\n2025a\n)\n4.61\n72.1\n61.7\n55.3\n–\n4.74\n64.4\n56.2\n65.8\nEfficient-VLN\n(\nZheng et al.\n,\n2025\n)\n4.18\n73.7\n64.2\n55.9\n–\n3.88\n67.0\n54.3\n68.4\nZero-Shot\nOpenNav\n(\nQiao et al.\n,\n2025\n)\n6.70\n23.0\n19.0\n16.1\n45.8\n–\n–\n–\n–\nCA-Nav\n(\nChen et al.\n,\n2025\n)\n7.58\n48.0\n25.3\n10.8\n–\n10.37\n19.0\n6.0\n–\nSmartway\n(\nShi et al.\n,\n2025\n)\n7.01\n51.0\n29.0\n22.5\n–\n–\n–\n–\n–\nSTRIDER\n(\nHe et al.\n,\n2025\n)\n6.91\n39.0\n35.0\n30.3\n51.8\n11.19\n21.2\n9.6\n30.1\nVLN-Zero\n(\nBhatt et al.\n,\n2025\n)\n5.97\n51.6\n42.4\n26.3\n–\n9.13\n30.8\n19.0\n–\nSpatialNav (ours)\n5.15\n66.0\n64.0\n51.1\n65.4\n7.64\n32.4\n24.6\n55.0\nSpatialNav\n†\n(ours)\n4.21\n73.0\n68.0\n53.4\n69.3\n7.34\n39.0\n28.4\n56.0\n4\nExperiments\n4.1\nExperimental Setup\nDatasets and Simulators\nWe evaluate our approach in discrete and continuous environments. For discrete environment, we use the R2R\n(Anderson et al.,\n2018\n)\nand REVERIE\n(Qi et al.,\n2020\n)\ndatasets with Matterport3D simulator\n(Chang et al.,\n2017\n)\n. The validation unseen split for R2R and REVERIE contains 783 and 1328 trajectories spanning 11 scans. For continuous environment, we utilize the R2R-CE\n(Krantz et al.,\n2020\n)\nand RxR-CE\n(Ku et al.,\n2020\n)\ndatasets with Habitat v0.3.2 simulator\n(Savva et al.,\n2019\n)\n. Following previous zero-shot VLN agents in continuous environments\n(Qiao et al.,\n2025\n; Shi et al.,\n2025\n; He et al.,\n2025\n)\n, we randomly sampled a subset of 100 and 200 trajectories from the validation unseen split of R2R-CE and RxR-CE, respectively.\nEvaluation Metrics\nIn our experiments, we report the widely used standard VLN evaluation metrics\n(Hong et al.,\n2021\n; Wang et al.,\n2023a\n; Zhang et al.,\n2025a\n; Qiao et al.,\n2025\n)\n, including trajectory length (TL), navigation error (NE), success rate (SR), oracle success rate (OSR), success weighted by path length (SPL) and normalized Dynamic Time Warping (nDTW). An episode is considered successful if the agent stops within 3 meters of the goal in both discrete and continuous environments.\nBaselines\nIn discrete environment, we compare against six supervised learning-based agents and four zero-shot agents, where ScaleVLN\n(Wang et al.,\n2023a\n)\nand SpatialGPT\n(Jiang & Wang,\n2025\n)\nachieve the state-of-the-art performance.\nIn continuous environment, we compare against seven supervised learning-based agents and five zero-shot agents, where Efficient-VLN\n(Zheng et al.,\n2025\n)\nand VLN-Zero\n(Bhatt et al.,\n2025\n)\nachieve the state-of-the-art performance within each group.\nTable 3:\nComparison with baselines enhanced with spatial map.\n“SMap” is short for agent-centric spatial map. To ensure environmental diversity, we use the 56 scans on the validation splits of R2R and REVERIE, based on which we sample 267 and 230 trajectories, respectively. For R2R-CE, we use the randomly sampled 100 trajectories.\nMethod\nR2R-Val-Sampled\nREVERIE-Val-Sampled\nOSR(↑)\nSR(↑)\nSPL(↑)\nOSR(↑)\nSR(↑)\nSPL(↑)\nSMap Only\n61.8\n40.8\n31.7\n59.5\n33.2\n17.7\nNavGPT\n53.2\n43.5\n34.7\n37.1\n31.5\n23.4\nNavGPT + SMap\n63.7\n52.1\n42.7\n57.8\n47.4\n34.1\nSpatialNav\n68.9\n60.3\n50.1\n56.5\n50.9\n34.3\n(a)\nResults on discrete environments.\nMethod\nR2R-CE Val-Unseen\nOSR(↑)\nSR(↑)\nSPL(↑)\nSmartWay\n60.0\n51.0\n41.0\nSmartWay + SMap\n68.0\n62.0\n50.3\nSpatialNav\n66.0\n64.0\n51.1\n(b)\nResults on continuous environments.\nFigure 4\n:\nComparison between different MLLM backbones.\nWe report the SR, OSR and SPL.\nImplementation Details\nAs we have stated in Section\n3.2\n, we divide the panoramic observations into eight directions, representing the front, front-right, right, rear-right, rear, rear-left, left and front-left. The 256\n×\n\\times\n256 images taken at each view are organized into a single 1024\n×\n\\times\n1024 compass-like image. For the spatial map, we set its width and height as 1024, with gird size set as 0.015m, leading to a perception radius of 7.68m. For remote object localization, we retrieve the objects within the same room of the navigable places and group them together based on the predicted object categories. Note that in continuous environment, we utilize the waypoint predictor proposed by\n(Shi et al.,\n2025\n)\nto obtain the navigable places. we use GPT-5.1\n(OpenAI,\n2025\n)\nas the MLLM backbone.\n4.2\nMain Results\nWe compare SpatialNav with other state-of-the-art VLN agents under two different settings: supervised learning and zero-shot. The results in discrete and continuous environments are summarized in Table\n1\nand Table\n2\n, respectively. Note that we also report the performance of a variant of our method, namely SpatialNav\n†\n, where the spatial annotations about room segmentations, room types and object detections are replaced with ground-truth annotations provided by the Matterport3D dataset\n(Chang et al.,\n2017\n)\n, serving as an upper-bound reference.\nIn discrete environments, SpatialNav consistently outperforms existing zero-shot baselines on both R2R and REVERIE val-unseen splits. Compared with the strongest zero-shot baseline, SpatialGPT, SpatialNav achieves an absolute gain of +9.3% in SR and +11.7% in SPL on R2R val-unseen, demonstrating stronger navigation effectiveness and path efficiency. Although SpatialNav navigates in a zero-shot manner, its performance is comparable with several supervised learning-based agents, such as PREVALENT and VLN-BERT on the R2R dataset and DUET on the REVERIE dataset.\nIn continuous environments which pose additional challenges due to low-level control, SpatialNav significantly outperforms prior zero-shot methods on both R2R-CE and RxR-CE. In particular, SpatialNav significantly outperforms VLN-Zero, an approach closely related to us since it also leverages a form of global memory constructed through pre-exploration. While VLN-Zero only maintains a symbolic scene graph that lacks explicit semantic structure, SpatialNav builds a spatial scene graph that jointly models global layout and detailed semantics. As a result, SpatialNav yields absolute gains of +21.6% SR and +24.8% SPL on R2R-CE. Performance gains are also observed on the more challenging RxR-CE dataset, indicating that spatial knowledge can is beneficial for VLN.\nMoreover, SpatialNav\n†\nfurther improves the performance across four datasets in both discrete and continuous environments, demonstrating that the quality of spatial annotations plays a critical role in navigation performance. This observation encourages future work on more accurate and scalable automatic spatial annotation methods for VLN.\n4.3\nFurther Analysis\nIn this section, we analyze the soundness of SpatialNav by addressing the following two questions:\nQ1: Does explicit spatial knowledge serve as an effective signal for VLN?\nFirstly, in discrete environments where navigable viewpoints are stored as navigation graphs and are easily accessible, we design a spatial map only (SMap Only) baseline that navigates solely on the agent-centric spatial map, the navigation instruction, and the set of navigable locations, without using any visual or textual observations of the panorama.\nAs shown in Table\n3(a)\n, this baseline achieves good performance, validating that spatial maps alone contain sufficient spatial information to support navigation decisions. Then, we evaluate whether the spatial map provides consistent benefits when incorporated into existing VLN agents. Specifically, we augment NavGPT\n(Zhou et al.,\n2024b\n)\nin discrete environments and SmartWay\n(Shi et al.,\n2025\n)\nin continuous environments with the spatial map as additional input, while keeping their original mechanisms unchanged. Results in Table\n3\ndemonstrate that explicit spatial information constitutes an effective and generalizable signal for VLN across different agents and environment settings.\nFurthermore, we evaluate two alternative multimodal large language models (MLLMs) to explore whether the effectiveness depends on the choice of MLLM backbones. We select another strong closed-source model, Gemini-2.5-Pro\n(Comanici et al.,\n2025\n)\n, and a representative open-source model Qwen3-VL-Plus\n(Bai et al.,\n2025\n)\n. Results are summarized in Figure\n4\n. Both GPT-5.1 and Gemini-2.5-Pro exhibit clear gains when augmented with spatial maps, whereas Qwen3-VL-Plus-based NavGPT slightly degrades when augmented with the spatial map. We observe that the output pattern of Qwen3-VL-Plus is highly repetitive, suggesting that it may be constrained by prior fine-tuning on VLN-style data. For instance, thinking patterns such as “\nThought: I have reached/moved/entered/exited\n…” account for approximately 30% of the outputs, while patterns like “\nThought: The instruction requires\n…” appear in about 20% of the cases. Despite these differences, SpatialNav consistently achieves the best performance across all evaluated backbones, indicating that our method is largely backbone-agnostic.\nQ2: How do different components of SpatialNav affect the navigation?\nWe conduct a progressive analysis by incrementally enriching a minimal baseline along the dimensions of density and quality of spatial information, together with the modality of panoramic observation. We evaluate on 50 trajectories on 5 validation scans with the average recall and precision of room segmentation exceeding 50% to ensure a reasonable quality. We implement the minimal baseline as a text-based agent that utilizes the descriptions of each viewpoint from\n(Zhou et al.,\n2024b\n)\nwith no spatial information.\nAs shown in Table\n4\n, applying the spatial map improves the performance.\nHowever, adding remote object semantics under the text-only setting leads to performance degradation, particularly with ground-truth annotations. This is caused by semantic ambiguity between objects described in textual panoramic observations and those retrieved from future locations, which hinders accurate termination decisions and results in longer trajectories and higher navigation error. Such ambiguity is more severe for ground truth as it contains many small objects that cannot be predicted by SpatialLM. Importantly, this issue is alleviated when replacing the textual panoramic observations with our compass-style visual observations. We find that visual grounding helps to align current perceptions with future spatial cues. This indicates richer spatial semantics are most effective when combined with visual observations. Besides, we can also observe that ground-truth annotations consistently outperform model-predicted ones, while predicted annotations still provide clear benefits. Overall, these results validate the design of SpatialNav.\n4.4\nAblation Studies\nMoreover, we conduct experiments on the sampled subset of R2R validation splits spanning 56 scans to validate the hyper-parameter settings of SpatialNav.\nTable 4:\nImpact of the density and quality of spatial information on sampled subset of R2R validation scans.\n“G” and “P” indicate that the spatial annotations are ground truth and model-predicted, respectively.\nPano\nObs\nSpatial\nMap\nRemote\nObjects\nMetrics\nG\nP\nG\nP\nTL\nNE(↓)\nSR(↑)\nOSR(↑)\nSPL(↑)\ntext\n19.1\n5.97\n46\n60\n37.0\ntext\n✓\n\\checkmark\n17.1\n4.61\n66\n76\n53.2\ntext\n✓\n\\checkmark\n16.8\n4.25\n56\n70\n47.9\ntext\n✓\n\\checkmark\n✓\n\\checkmark\n18.4\n6.51\n60\n72\n48.7\ntext\n✓\n\\checkmark\n✓\n\\checkmark\n18.3\n5.37\n56\n70\n47.9\nvisual\n✓\n\\checkmark\n✓\n\\checkmark\n14.9\n4.34\n72\n74\n58.8\nvisual\n✓\n\\checkmark\n✓\n\\checkmark\n15.9\n4.92\n62\n70\n50.1\nEfficiency of compass-like visual observation.\nAs illustrated in Table\n5\n, among the compass-style representations, the 1024\n×\n\\times\n1024 resolution performs better. While the sequential input of eight directional views achieves the best overall performance, this observation format consumes over 1700 visual tokens per step, leading to high latency and cost. In contrast, our compass-style image representation requires only about 640 visual tokens at 1024\n×\n\\times\n1024 resolution, and can achieve navigation accuracy close to the sequential setting. Therefore, considering the efficiency, we adopt it as the default visual representation in SpatialNav.\nPerception range of spatial map.\nTable\n6\ndemonstrates that the perception radius has a clear trade-off between the spatial information involved and the decision efficiency. A small radius of 3.84m, which is very close to the local perception range without using a spatial map, provides little extra information and therefore leads to limited performance improvement. Expanding the radius to 7.68m yields the best balance, achieving the strongest SR, SPL and nDTW. In contrast, an overly large radius of 11.52m introduces more distant, instruction-irrelevant rooms and navigable structures, which can dilute the agent attention. The SR slightly decreases despite a higher OSR. Therefore, we select 7.68m as the perception radius as it provides adequate spatial context without overwhelming redundancy.\n5\nConclusions\nIn this work, we propose to pre-explore the environment to construct a spatial scene graph about global spatial information. We then introduce SpatialNav, a zero-shot agent that effectively uses spatial scene graphs for navigation. Extensive experiments demonstrate that our method generalizes across environments and agents, enabling zero-shot performance comparable to learning-based methods.\nLimitations\nWhile SpatialNav achieves robust zero-shot navigation performance, it still has several limitations. Firstly, how the spatial scene graphs can be integrated into supervised learning-based agents remains unexplored. Since most existing learning-based VLN agents are trained without access to top-down map-structured spatial representations, such inputs may not be compatible with these methods. We leave it as future work to collect additional VLN training data with spatial-map annotations and redesign the training paradigm to expose agents to this form of spatial information.\nTable 5:\nAblation on the visual format of panoramic observations.\n“seq” denotes that sequential images, whereas “cps” means a single compass-like image.\nFmt\n#Img\nImg Size\nOSR(↑)\nSR(↑)\nSPL(↑)\nnDTW(↑)\ncps\n1\n1536\n×\n\\times\n1536\n68.8\n58.1\n45.8\n55.1\ncps\n1\n1024\n×\n\\times\n1024\n68.9\n60.3\n50.1\n59.7\ncps\n1\n512\n×\n\\times\n512\n69.7\n59.9\n46.1\n55.3\nseq\n8\n256\n×\n\\times\n256\n70.0\n62.5\n54.6\n63.3\nTable 6:\nAblation on the perception radius of agent-centric spatial map.\nRadius\nOSR(↑)\nSR(↑)\nSPL(↑)\nnDTW(↑)\n11.52m\n71.5\n56.9\n47.0\n58.43\n7.68m\n68.9\n60.3\n50.1\n59.7\n3.84m\n62.2\n50.9\n42.9\n55.33\nSecondly, we admit that constructing a full spatial scene graph introduces additional computation overhead. Although modern SLAM systems can efficiently generate dense or sparse point clouds, we assume that these point clouds are available and do not evaluate the robustness and possible failure cases of the reconstruction process itself. Given that the reconstruction of 3D point cloud through pre-exploration can be computationally and operationally expensive, we consider fully evaluating and optimizing this pipeline as an important direction for future work.\nFurthermore, room-level segmentation remains challenging in open or ambiguously bounded spaces. Existing automated methods may produce inaccurate segmentation results, requiring extra manual correction. However, we find that this limitation is consistent with the situation in real-world robotic systems, where users are allowed to intervene after building the house map to refine the room layout.\nReferences\nAn et al. (2024)\nAn, D., Wang, H., Wang, W., Wang, Z., Huang, Y., He, K., and Wang, L.\nEtpnav: Evolving topological planning for vision-language navigation in continuous environments.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n, 2024.\nAnderson et al. (2018)\nAnderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sünderhauf, N., Reid, I., Gould, S., and Van Den Hengel, A.\nVision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n, pp.  3674–3683, 2018.\nBai et al. (2025)\nBai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K.\nQwen3-vl technical report, 2025.\nURL\nhttps://arxiv.org/abs/2511.21631\n.\nBhatt et al. (2025)\nBhatt, N. P., Yang, Y., Siva, R., Samineni, P., Milan, D., Wang, Z., and Topcu, U.\nVln-zero: Rapid exploration and cache-enabled neurosymbolic vision-language planning for zero-shot transfer in robot navigation.\narXiv preprint arXiv:2509.18592\n, 2025.\nBobkov et al. (2017)\nBobkov, D., Kiechle, M., Hilsenbeck, S., and Steinbach, E.\nRoom segmentation in 3d point clouds using anisotropic potential fields.\nIn\n2017 IEEE International Conference on Multimedia and Expo (ICME)\n, pp.  727–732. IEEE, 2017.\nChang et al. (2017)\nChang, A., Dai, A., Funkhouser, T., Halber, M., Niebner, M., Savva, M., Song, S., Zeng, A., and Zhang, Y.\nMatterport3d: Learning from rgb-d data in indoor environments.\nIn\n2017 International Conference on 3D Vision (3DV)\n, pp.  667–676. IEEE, 2017.\nChen et al. (2024)\nChen, J., Lin, B., Xu, R., Chai, Z., Liang, X., and Wong, K.-Y.\nMapgpt: Map-guided prompting with adaptive path planning for vision-and-language navigation.\nIn\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\n, pp.  9796–9810, 2024.\nChen et al. (2021a)\nChen, K., Chen, J. K., Chuang, J., Vázquez, M., and Savarese, S.\nTopological planning with transformers for vision-and-language navigation.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pp.  11276–11286, 2021a.\nChen et al. (2025)\nChen, K., An, D., Huang, Y., Xu, R., Su, Y., Ling, Y., Reid, I., and Wang, L.\nConstraint-aware zero-shot vision-language navigation in continuous environments.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n, 2025.\nChen et al. (2023)\nChen, P., Sun, X., Zhi, H., Zeng, R., Li, T. H., Liu, G., Tan, M., and Gan, C.\nA2nav: Action-aware zero-shot robot navigation by exploiting vision-and-language ability of foundation models.\narXiv preprint arXiv:2308.07997\n, 2023.\nChen et al. (2021b)\nChen, S., Guhur, P.-L., Schmid, C., and Laptev, I.\nHistory aware multimodal transformer for vision-and-language navigation.\nAdvances in neural information processing systems\n, 34:5834–5847, 2021b.\nChen et al. (2022)\nChen, S., Guhur, P.-L., Tapaswi, M., Schmid, C., and Laptev, I.\nThink global, act local: Dual-scale graph transformer for vision-and-language navigation.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pp.  16537–16547, 2022.\nComanici et al. (2025)\nComanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al.\nGemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.\narXiv preprint arXiv:2507.06261\n, 2025.\nDu et al. (2024)\nDu, M., Wu, B., Zhang, J., Fan, Z., Li, Z., Luo, R., Huang, X.-J., and Wei, Z.\nDelan: Dual-level alignment for vision-and-language navigation by cross-modal contrastive learning.\nIn\nProceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)\n, pp.  4605–4616, 2024.\nHao et al. (2020)\nHao, W., Li, C., Li, X., Carin, L., and Gao, J.\nTowards learning a generic agent for vision-and-language navigation via pre-training.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pp.  13137–13146, 2020.\nHe et al. (2025)\nHe, D., Gao, X., Li, H., Han, J., and Zhang, D.\nStrider: Navigation via instruction-aligned structural decision space optimization.\narXiv preprint arXiv:2511.00033\n, 2025.\nHong et al. (2021)\nHong, Y., Wu, Q., Qi, Y., Rodriguez-Opazo, C., and Gould, S.\nVln bert: A recurrent vision-and-language bert for navigation.\nIn\nProceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition\n, pp.  1643–1653, 2021.\nHong et al. (2022)\nHong, Y., Wang, Z., Wu, Q., and Gould, S.\nBridging the gap between learning in discrete and continuous environments for vision-and-language navigation.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pp.  15439–15449, 2022.\nJain et al. (2019)\nJain, V., Magalhaes, G., Ku, A., Vaswani, A., Ie, E., and Baldridge, J.\nStay on the path: Instruction fidelity in vision-and-language navigation.\nIn\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics\n, pp.  1862–1872, 2019.\nJiang & Wang (2025)\nJiang, Z. and Wang, X.\nSpatialgpt: Zero-shot vision-and-language navigation via spatial cot over structured spatial memory.\nIn\nProceedings of the 33rd ACM International Conference on Advances in Geographic Information Systems\n, pp.  423–435, 2025.\nKrantz et al. (2020)\nKrantz, J., Wijmans, E., Majumdar, A., Batra, D., and Lee, S.\nBeyond the nav-graph: Vision-and-language navigation in continuous environments.\nIn\nEuropean Conference on Computer Vision\n, pp.  104–120. Springer, 2020.\nKu et al. (2020)\nKu, A., Anderson, P., Patel, R., Ie, E., and Baldridge, J.\nRoom-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding.\nIn\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\n, pp.  4392–4412, 2020.\nLin et al. (2024)\nLin, B., Nie, Y., Wei, Z., Chen, J., Ma, S., Han, J., Xu, H., Chang, X., and Liang, X.\nNavcot: Boosting llm-based vision-and-language navigation via learning disentangled reasoning.\narXiv preprint arXiv:2403.07376\n, 2024.\nLiu et al. (2025)\nLiu, Y., Dong, S., Wang, S., Yin, Y., Yang, Y., Fan, Q., and Chen, B.\nSlam3r: Real-time dense scene reconstruction from monocular rgb videos.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pp.  16651–16662, 2025.\nLong et al. (2024)\nLong, Y., Li, X., Cai, W., and Dong, H.\nDiscuss before moving: Visual language navigation via multi-expert discussions.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pp.  17380–17387. IEEE, 2024.\nLong et al. (2025)\nLong, Y., Cai, W., Wang, H., Zhan, G., and Dong, H.\nInstructnav: Zero-shot system for generic instruction navigation in unexplored environment.\nIn\nConference on Robot Learning\n, pp.  2049–2060. PMLR, 2025.\nMaggio et al. (2025)\nMaggio, D., Lim, H., and Carlone, L.\nVggt-slam: Dense rgb slam optimized on the sl (4) manifold.\narXiv preprint arXiv:2505.12549\n, 2025.\nMao et al. (2025)\nMao, Y., Zhong, J., Fang, C., Zheng, J., Tang, R., Zhu, H., Tan, P., and Zhou, Z.\nSpatiallm: Training large language models for structured indoor modeling.\narXiv preprint arXiv:2506.07491\n, 2025.\nMdfaa et al. (2025)\nMdfaa, M. A., Lukina, S., Akhtyamov, T., Nigmatzyanov, A., Nalberskii, D., Zagoruyko, S., and Ferrer, G.\nVl-kng: Visual scene understanding for navigation goal identification using spatiotemporal knowledge graphs.\narXiv preprint arXiv:2510.01483\n, 2025.\nMurai et al. (2025)\nMurai, R., Dexheimer, E., and Davison, A. J.\nMast3r-slam: Real-time dense slam with 3d reconstruction priors.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pp.  16695–16705, 2025.\nOpenAI (2025)\nOpenAI.\nGpt-5 system card.\nhttps://cdn.openai.com/gpt-5-system-card.pdf\n, 2025.\nQi et al. (2020)\nQi, Y., Wu, Q., Anderson, P., Wang, X., Wang, W. Y., Shen, C., and Hengel, A. v. d.\nReverie: Remote embodied visual referring expression in real indoor environments.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pp.  9982–9991, 2020.\nQiao et al. (2022)\nQiao, Y., Qi, Y., Hong, Y., Yu, Z., Wang, P., and Wu, Q.\nHop: History-and-order aware pre-training for vision-and-language navigation.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pp.  15418–15427, 2022.\nQiao et al. (2025)\nQiao, Y., Lyu, W., Wang, H., Wang, Z., Li, Z., Zhang, Y., Tan, M., and Wu, Q.\nOpen-nav: Exploring zero-shot vision-and-language navigation in continuous environment with open-source llms.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n, pp.  6710–6717. IEEE, 2025.\nSavva et al. (2019)\nSavva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., Straub, J., Liu, J., Koltun, V., Malik, J., et al.\nHabitat: A platform for embodied ai research.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n, pp.  9339–9347, 2019.\nShi et al. (2025)\nShi, X., Li, Z., Lyu, W., Xia, J., Dayoub, F., Qiao, Y., and Wu, Q.\nSmartway: Enhanced waypoint prediction and backtracking for zero-shot vision-and-language navigation.\narXiv preprint arXiv:2503.10069\n, 2025.\nTan et al. (2019)\nTan, H., Yu, L., and Bansal, M.\nLearning to navigate unseen environments: Back translation with environmental dropout.\nIn\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)\n, pp.  2610–2621, 2019.\nWang et al. (2019)\nWang, X., Huang, Q., Celikyilmaz, A., Gao, J., Shen, D., Wang, Y.-F., Wang, W. Y., and Zhang, L.\nReinforced cross-modal matching and self-supervised imitation learning for vision-language navigation.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pp.  6629–6638, 2019.\nWang et al. (2023a)\nWang, Z., Li, J., Hong, Y., Wang, Y., Wu, Q., Bansal, M., Gould, S., Tan, H., and Qiao, Y.\nScaling data generation in vision-and-language navigation.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n, pp.  12009–12020, 2023a.\nWang et al. (2023b)\nWang, Z., Li, X., Yang, J., Liu, Y., and Jiang, S.\nGridmm: Grid memory map for vision-and-language navigation.\nIn\nProceedings of the IEEE/CVF International conference on computer vision\n, pp.  15625–15636, 2023b.\nWang et al. (2024)\nWang, Z., Li, X., Yang, J., Liu, Y., Hu, J., Jiang, M., and Jiang, S.\nLookahead exploration with neural radiance representation for continuous vision-language navigation.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pp.  13753–13762, 2024.\nWei et al. (2025)\nWei, M., Wan, C., Yu, X., Wang, T., Yang, Y., Mao, X., Zhu, C., Cai, W., Wang, H., Chen, Y., et al.\nStreamvln: Streaming vision-and-language navigation via slowfast context modeling.\narXiv preprint arXiv:2507.05240\n, 2025.\nWerby et al. (2024)\nWerby, A., Huang, C., Büchner, M., Valada, A., and Burgard, W.\nHierarchical open-vocabulary 3d scene graphs for language-grounded robot navigation.\nIn\nFirst Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024\n, 2024.\nZhan et al. (2024)\nZhan, Z., Yu, L., Yu, S., and Tan, G.\nMc-gpt: Empowering vision-and-language navigation with memory map and reasoning chains.\narXiv preprint arXiv:2405.10620\n, 2024.\nZhang et al. (2021)\nZhang, J., Fan, J., Peng, J., et al.\nCurriculum learning for vision-and-language navigation.\nAdvances in Neural Information Processing Systems\n, 34:13328–13339, 2021.\nZhang et al. (2024)\nZhang, J., Wang, K., Xu, R., Zhou, G., Hong, Y., Fang, X., Wu, Q., Zhang, Z., and Wang, H.\nNavid: Video-based vlm plans the next step for vision-and-language navigation.\narXiv preprint arXiv:2402.15852\n, 2024.\nZhang et al. (2025a)\nZhang, J., Li, A., Qi, Y., Li, M., Liu, J., Wang, S., Liu, H., Zhou, G., Wu, Y., Li, X., et al.\nEmbodied navigation foundation model.\narXiv preprint arXiv:2509.12129\n, 2025a.\nZhang et al. (2025b)\nZhang, L., Hao, X., Xu, Q., Zhang, Q., Zhang, X., Wang, P., Zhang, J., Wang, Z., Zhang, S., and Xu, R.\nMapNav: A novel memory representation via annotated semantic maps for VLM-based vision-and-language navigation.\nIn Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.),\nProceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\n, pp.  13032–13056, Vienna, Austria, July 2025b. Association for Computational Linguistics.\nISBN 979-8-89176-251-0.\nZhang et al. (2025c)\nZhang, Z., Zhu, W., Pan, H., Wang, X., Xu, R., Sun, X., and Zheng, F.\nActivevln: Towards active exploration via multi-turn rl in vision-and-language navigation.\narXiv preprint arXiv:2509.12618\n, 2025c.\nZheng et al. (2025)\nZheng, D., Huang, S., Li, Y., and Wang, L.\nEfficient-vln: A training-efficient vision-language navigation model.\narXiv preprint arXiv:2512.10310\n, 2025.\nZhou et al. (2024a)\nZhou, G., Hong, Y., Wang, Z., Wang, X. E., and Wu, Q.\nNavgpt-2: Unleashing navigational reasoning capability for large vision-language models.\nIn\nEuropean Conference on Computer Vision\n, pp.  260–278. Springer, 2024a.\nZhou et al. (2024b)\nZhou, G., Hong, Y., and Wu, Q.\nNavgpt: Explicit reasoning in vision-and-language navigation with large language models.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n, volume 38, pp.  7641–7649, 2024b.\nZhu et al. (2020)\nZhu, F., Zhu, Y., Chang, X., and Liang, X.\nVision-language navigation with self-supervised auxiliary reasoning tasks.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pp.  10012–10022, 2020.\nZhu et al. (2021)\nZhu, F., Liang, X., Zhu, Y., Yu, Q., Chang, X., and Liang, X.\nSoon: Scenario oriented object navigation with graph-based exploration.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pp.  12689–12699, 2021.",
    "preview_text": "Although learning-based vision-and-language navigation (VLN) agents can learn spatial knowledge implicitly from large-scale training data, zero-shot VLN agents lack this process, relying primarily on local observations for navigation, which leads to inefficient exploration and a significant performance gap. To deal with the problem, we consider a zero-shot VLN setting that agents are allowed to fully explore the environment before task execution. Then, we construct the Spatial Scene Graph (SSG) to explicitly capture global spatial structure and semantics in the explored environment. Based on the SSG, we introduce SpatialNav, a zero-shot VLN agent that integrates an agent-centric spatial map, a compass-aligned visual representation, and a remote object localization strategy for efficient navigation. Comprehensive experiments in both discrete and continuous environments demonstrate that SpatialNav significantly outperforms existing zero-shot agents and clearly narrows the gap with state-of-the-art learning-based methods. Such results highlight the importance of global spatial representations for generalizable navigation.\n\nSpatialNav: Leveraging Spatial Scene Graphs for Zero-Shot Vision-and-Language Navigation\nJiwen Zhang\nZejun Li\nSiyuan Wang\nXiangyu Shi\nZhongyu Wei\nQi Wu\nAbstract\nAlthough learning-based vision-and-language navigation (VLN) agents can learn spatial knowledge implicitly from large-scale training data, zero-shot VLN agents lack this process, relying primarily on local observations for navigation, which leads to inefficient exploration and a significant performance gap. To deal with the problem, we consider a zero-shot VLN setting that agents are allowed to fully explore the environment before task execution. Then, we construct the\nSpatial Scene Graph (SSG)\nto explicitly capture global spatial structure and semantics in the explored environment. Based on the SSG, we introduce\nSpatialNav\n, a zero-shot VLN agent that integrates an agent-centric spatial map,",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "VLA",
        "VLM",
        "locomotion"
    ],
    "one_line_summary": "这篇论文提出了一种基于空间场景图的零样本视觉语言导航方法，但与强化学习、扩散模型、流匹配和全身控制等技术无关。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-11T08:39:19Z",
    "created_at": "2026-01-21T12:09:04.709581",
    "updated_at": "2026-01-21T12:09:04.709590",
    "recommend": 0
}