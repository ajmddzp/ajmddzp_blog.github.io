{
    "id": "2601.22965v1",
    "title": "Self-Imitated Diffusion Policy for Efficient and Robust Visual Navigation",
    "authors": [
        "Runhua Zhang",
        "Junyi Hou",
        "Changxu Cheng",
        "Qiyi Chen",
        "Tao Wang",
        "Wuyue Zhao"
    ],
    "abstract": "æ‰©æ•£ç­–ç•¥åœ¨è§†è§‰å¯¼èˆªé¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå…¶é€šè¿‡æ•æ‰å¤šæ ·åŒ–çš„å¤šæ¨¡æ€è½¨è¿¹åˆ†å¸ƒå®ç°é«˜æ•ˆè§„åˆ’ã€‚ç„¶è€Œï¼Œå¤šæ•°æ‰©æ•£ç­–ç•¥ä¾èµ–çš„æ ‡å‡†æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸¸ä¼šç»§æ‰¿ä¸“å®¶ç¤ºèŒƒæ•°æ®çš„æ¬¡ä¼˜æ€§ä¸å†—ä½™æ€§ï¼Œå¯¼è‡´æ¨ç†é˜¶æ®µéœ€è¦ä¾èµ–è¾…åŠ©é€‰æ‹©å™¨è¿›è¡Œå¤§é‡é‡‡æ ·ä¸åç­›é€‰ï¼Œå½¢æˆè®¡ç®—å¯†é›†å‹çš„\"ç”Ÿæˆ-å†è¿‡æ»¤\"æµç¨‹ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºè‡ªæ¨¡ä»¿æ‰©æ•£ç­–ç•¥â€”â€”ä¸€ç§é€šè¿‡é€‰æ‹©æ€§æ¨¡ä»¿ä»è‡ªèº«é‡‡æ ·è½¨è¿¹æ¥å­¦ä¹ ä¼˜åŒ–è§„åˆ’çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥å¥–åŠ±å¼•å¯¼çš„è‡ªæ¨¡ä»¿æœºåˆ¶ï¼Œä¿ƒä½¿ç­–ç•¥æŒç»­ç”Ÿæˆé«˜è´¨é‡è½¨è¿¹è€Œéè´¨é‡ä¸ç¨³å®šçš„è¾“å‡ºï¼Œä»è€Œå‡å°‘å¯¹å¤§é‡é‡‡æ ·ä¸åè¿‡æ»¤çš„ä¾èµ–ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨å¥–åŠ±é©±åŠ¨çš„è¯¾ç¨‹å­¦ä¹ èŒƒå¼æ¥ç¼“è§£æ•°æ®åˆ©ç”¨æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå¹¶ç»“åˆç›®æ ‡æ— å…³çš„è½¨è¿¹å¢å¼ºæ¢ç´¢æœºåˆ¶ä»¥æå‡è§„åˆ’é²æ£’æ€§ã€‚åœ¨ç»¼åˆæ€§ä»¿çœŸåŸºå‡†æµ‹è¯•ä¸­çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè‡ªæ¨¡ä»¿æ‰©æ•£ç­–ç•¥æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¤šæœºå™¨äººå¹³å°çš„å®ç‰©å®éªŒè¿›ä¸€æ­¥éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚åœ¨Jetson Orin Nanoå¹³å°ä¸Šï¼Œè‡ªæ¨¡ä»¿æ‰©æ•£ç­–ç•¥çš„æ¨ç†é€Ÿåº¦è¾¾åˆ°åŸºçº¿NavDPçš„2.5å€ï¼ˆ110æ¯«ç§’å¯¹273æ¯«ç§’ï¼‰ï¼Œå®ç°äº†é«˜æ•ˆçš„å®æ—¶éƒ¨ç½²ã€‚",
    "url": "https://arxiv.org/abs/2601.22965v1",
    "html_url": "https://arxiv.org/html/2601.22965v1",
    "html_content": "Self-Imitated Diffusion Policy for Efficient and Robust\nVisual Navigation\nRunhua Zhang\n1,2âˆ—\n,\nJunyi Hou\n1,2âˆ—\n,\nChangxu Cheng\n1â€ \n,\nQiyi Chen\n1\n,\nTao Wang\n1\n,\nWuyue Zhao\n1\n1\nUni-Ubi\n2\nZhejiang University\nAbstract\nDiffusion policies (DP) have demonstrated significant potential in visual navigation by capturing diverse multi-modal trajectory distributions.\nHowever, standard imitation learning (IL), which most DP methods rely on for training, often inherits sub-optimality and redundancy from expert demonstrations, thereby necessitating a computationally intensive â€œgenerate-then-filterâ€ pipeline that relies on auxiliary selectors during inference.\nTo address these challenges, we propose\nSelf-Imitated Diffusion Policy (SIDP)\n, a novel framework that learns improved planning by selectively imitating a set of trajectories sampled from itself.\nSpecifically, SIDP introduces a reward-guided self-imitation mechanism that encourages the policy to consistently produce high-quality trajectories efficiently, rather than outputs of inconsistent quality, thereby reducing reliance on extensive sampling and post-filtering.\nDuring training, we employ a reward-driven curriculum learning paradigm to mitigate inefficient data utility\n, and goal-agnostic exploration for trajectory augmentation to improve planning robustness.\nExtensive evaluations on a comprehensive simulation benchmark show that SIDP significantly outperforms previous methods, with real-world experiments confirming its effectiveness across multiple robotic platforms.\nOn Jetson Orin Nano, SIDP delivers a 2.5\nÃ—\n\\times\nfaster inference than the baseline NavDP,\ni.e.\n, 110ms VS 273ms, enabling efficient real-time deployment.\n*\n*\nfootnotetext:\nCo-first authors.\nâ€ \nCorresponding author: ccx0127@gmail.com\nI\nINTRODUCTION\nDiffusion-based path planning has recently emerged as a powerful paradigm for visual navigation\n[\n4\n,\n30\n]\n. By modeling entire trajectories through iterative denoising, these policies ensure spatiotemporal consistency compared to sequential action prediction\n[\n10\n]\n. The generative nature of diffusion allows the policy to capture multi-modal trajectory distributions, making it well suited to complex environments with multiple feasible routes to a goal.\nA common strategy to train diffusion policies is imitation learning (IL) using expert demonstrations. However, IL introduces two critical bottlenecks.\nFirst, robustness is compromised by the limited coverage of expert datasets, causing policies to struggle under distributional shifts and fail in novel scenarios\n[\n23\n,\n28\n]\n.\nSecond, inference efficiency is affected.\nStandard IL attempts to imitate all given trajectories, including a diverse set of suboptimal ones, which leads to increased sampling variance and heterogeneous trajectory quality.\nConsequently, inference typically relies on a â€œgenerate-then-filterâ€ pipeline with auxiliary trajectory selectors\n[\n4\n,\n39\n,\n3\n,\n15\n]\nto select among numerous sampled candidates, which leads to increased latency, especially on resource-constrained devices.\nThese challenges for classic imitation-based diffusion policy are illustrated in\nFig.\n1\n(a).\nFigure 1\n:\nComparison between imitation and self-imitation based diffusion policy.\n(a) The classic imitation-based diffusion policy tends to generate trajectories with high variability, including unsafe, non-shortest or goal-unreachable routes, therefore relies on extensive sampling with post-filtering to obtain a final plan.\n(b) SIDP achieves distribution concentration via reward-guided self-imitation, enabling robust and efficient path planning without auxiliary selectors.\n(c) Validation across two robot platforms in real-world environments.\nIn this work, we propose the\nSelf-Imitated Diffusion Policy\n(SIDP), a framework designed for efficient and robust path planning in visual navigation.\nSIDP employs a reward-guided self-imitation scheme\n[\n20\n]\n, in which target trajectories are self-generated by the policy rather than sourced from external demonstrations.\nBy reweighting these trajectories based on their rewards, the framework prioritizes learning from high-quality experiences.\nThis training design facilitates iterative policy refinement by leveraging diverse navigation data and environmental feedback, thereby empowering the policy to develop\nrobust\nbehaviors that generalize beyond the inherent biases of fixed expert datasets.\nBeyond improving generalization, SIDP circumvents the complexity of RL-tuned diffusion\n[\n32\n,\n24\n]\nby eliminating the need for backpropagation through the entire sequence of denoising steps, a practice known as Backpropagation Through Time (BPTT) that is often computationally prohibitive and numerically unstable\n[\n36\n]\n.\nBy reformulating this challenging optimization into an iterative imitation learning objective, SIDP achieves stable policy gradients efficiently.\nFurthermore, this self-imitation paradigm fundamentally enhances navigation\nefficiency\nby fostering distributional concentration.\nOur policy preserves inter-modal diversity and promoting intra-modal convergence since suboptimal and redundant trajectories are progressively pruned during self-imitation learning.\nThis inherent refinement obviates the need for dense sampling and post-filtering, thus eliminating the reliance on an external selector and resulting in a more streamlined, end-to-end inference process.\nBesides, deterministic sampling with fewer denoising steps\n[\n29\n]\nis thus made feasible, which can further speed up the inference.\nUnder the SIDP framework, we devise two complementary learning strategies to augment the training process.\nFirst, we incorporate goal-agnostic exploration to enable autonomous navigation capabilities and diversify the trajectory pool, which also provides a critical regularization effect for point-goal navigation.\nSecond, a reward-driven curriculum learning scheme is employed to prioritize scenarios with high learning potential, effectively addressing the issue of inefficient data utility by dynamically selecting the most constructive training samples throughout the training process.\nThrough extensive experiments, SIDP achieves state-of-the-art performance on the InternVLA-N1 S1 Benchmark\n[\n4\n]\n, outperforming existing methods in both Success Rate (SR) and Success weighted by Path Length (SPL).\nWhile the recent baseline method exhibits a significant performance degradation when transitioning from synthetic training scenes to complex, high-fidelity test environments, SIDP effectively mitigates this distribution shift.\nSpecifically, SIDP surpasses NavDP by approximately 10 points on the InternScene-Commercial setting, underscoring its superior generalization capabilities and robustness in unseen, unstructured environments.\nFurthermore, SIDP boosts the inference efficiency of the baseline to 2.5\nÃ—\n\\times\non the Jetson Orin Nano without compromising navigation performance, reducing the latency from 273â€‰ms to 110â€‰ms.\nSuccessful deployment across two distinct robotic platforms confirms that SIDP provides a practical solution for real-world visual navigation.\nThe key contributions are summarized as follows:\nâ€¢\nWe propose a Self-Imitated Diffusion Policy that enhances robustness through high-quality self-generated experiences. By fostering distributional concentration, SIDP eliminates the need for dense sampling and post-filtering, streamlining the planning process.\nâ€¢\nWe introduce goal-agnostic exploration and reward-driven curriculum learning, which effectively diversifies trajectory pool and mitigates inefficient data utility\n, thus regularizing point-goal navigation and promoting convergence.\nâ€¢\nSIDP achieves state-of-the-art performance on high-fidelity benchmarks in SR and SPL. Besides, it achieves a 2.5\nÃ—\n\\times\ninference speedup on the edge devices, Jetson Orin Nano, demonstrating its practicality for real-world robotics.\nII\nRELATED WORK\nII-A\nPath Planning in Visual Navigation\nPath planning in visual navigation can be categorized into mapping-based and learning-based approaches.\nMapping-based methods\n[\n19\n,\n5\n]\nrely on visual Simultaneous Localization And Mapping(vSLAM) for map construction and pose estimation, followed by graph-search or sampling-based planners such as Dijkstra, A*, or RRT* to compute feasible paths.\nWhile effective in structured static environments, these methods are constrained by the need for precise mapping and heuristic planning algorithms, reducing robustness in dynamic or perceptually degraded scenarios.\nIn contrast, learning-based approaches avoid explicit mapping by either training policies in simulation or directly imitating expert demonstrations.\nPrevious reinforcement learning (RL) methods\n[\n33\n,\n37\n,\n7\n]\nexploit reward signals derived from environmental priors to train navigation policies, but incur high computational cost and transfer poorly to real-world domains.\nImitation learning (IL) offers better data efficiency\n[\n2\n,\n8\n,\n27\n,\n13\n,\n14\n,\n34\n]\n.\nHowever, IL-based planners are constrained by the scarcity and fidelity of expert demonstrations, frequently suffering from distribution shift.\nMoreover, they lack robust mechanisms to suppress compounding errors, which leads to drift during long-horizon navigation.\nInteractive IL frameworks\n[\n26\n,\n11\n]\nmitigate such issues by iteratively refining expert datasets.\nII-B\nPlanning Methods Based on Diffusion Policy\nDiffusion models have recently gained attention in sequential decision-making for their ability to capture multimodal distributions via iterative denoising compared to other autoregressive\n[\n2\n,\n8\n,\n27\n,\n13\n]\nand latent-variable models\n[\n14\n,\n34\n]\n.\nDiffuser\n[\n10\n]\nreformulates reinforcement learning as conditional trajectory generation, while Decision Diffuser\n[\n1\n]\nextends this by conditioning on returns and constraints.\nLater works further explore extensions to long-horizon tasks through hierarchical or compositional formulations\n[\n16\n,\n18\n]\n.\nThese advances have motivated the application of diffusion policies to path planning.\nNoMaD\n[\n30\n]\ndirectly generate trajectories from observations and integrate goal masking for goal-conditioned and undirected exploration. NavDP\n[\n4\n]\nleverages privileged information in simulation for path planning in indoor scenes.\nSome works integrate diffusion into classical planning pipelines to improve collision avoidance and planning efficiency\n[\n6\n,\n38\n]\n, or employ guided sampling strategies to enforce safety and encourage exploration\n[\n39\n,\n25\n,\n17\n,\n35\n,\n40\n,\n12\n]\n.\nThese approaches demonstrate that diffusion can flexibly model diverse candidate trajectories while incorporating task-specific constraints or guidance.\nHowever, they often require an auxiliary rule-based or learned selector\n[\n4\n,\n15\n,\n3\n]\nto assess and filter trajectories generated through diffusion sampling, in order to ensure feasibility and safety.\nSuch dependencies increase architectural complexity, posing bottlenecks to both computational efficiency and performance during real-world deployment.\nThese limitations underscore the need for paradigms that intrinsically streamline trajectory generation, enabling robust, end-to-end planning with both theoretical elegance and practical efficiency.\nFigure 2\n:\nOverview of the Self-Imitated Diffusion Policy (SIDP). The framework generates candidate trajectories using the current policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nand filters them via a reward-based sorting gate. High-reward samples are then used to compute the weighted denoising loss\nâ„’\nSIDP\n\\mathcal{L}_{\\text{SIDP}}\n, updating the policy parameters to iteratively align with the optimal distribution.\nIII\nMETHOD\nIII-A\nProblem Formulation\nWe address visual navigation, where an agent generates collision-free optimal trajectories to a specified goal using only egocentric RGB-D observations.\nThe input state is:\ns\nt\n=\n(\nI\nt\nRGB\n,\nI\nt\nD\n,\ng\nt\n)\ns_{t}=(I_{t}^{\\mathrm{RGB}},I_{t}^{\\mathrm{D}},g_{t})\n(1)\nwhere the RGB image\nI\nt\nRGB\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n3\nI_{t}^{\\mathrm{RGB}}\\in\\mathbb{R}^{H\\times W\\times 3}\nand depth map\nI\nt\nD\nâˆˆ\nâ„\nH\nÃ—\nW\nI_{t}^{\\mathrm{D}}\\in\\mathbb{R}^{H\\times W}\nare from the camera observation, and\npoint goal\ng\nt\nâˆˆ\nâ„\n3\ng_{t}\\in\\mathbb{R}^{3}\nare expressed relative to the cameraâ€™s reference frame.\nThe policy predicts an action sequence of fixed\nH\nH\nrelative waypoint displacements:\na\nt\n=\n(\nÎ”\nâ€‹\np\nt\n1\n,\nâ€¦\n,\nÎ”\nâ€‹\np\nt\nH\n)\nâˆˆ\nâ„\nH\nÃ—\n3\n=\nÏ€\nÎ¸\nâ€‹\n(\ns\nt\n)\n.\na_{t}=(\\Delta p_{t}^{1},\\dots,\\Delta p_{t}^{H})\\in\\mathbb{R}^{H\\times 3}=\\pi_{\\theta}(s_{t}).\n(2)\nTo formally evaluate trajectories, we define a quality metric\nr\nâ€‹\n(\ns\nt\n,\na\nt\n)\nr(s_{t},a_{t})\nthat rewards safety (collision-free navigation) and efficiency (optimal path toward the goal).\nThe ideal navigation policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nseeks to maximize the expected trajectory quality metric across the state distribution\nğ’Ÿ\n\\mathcal{D}\n:\nmax\nÎ¸\nâ¡\nğ”¼\ns\nt\nâˆ¼\nğ’Ÿ\n,\na\nt\nâˆ¼\nÏ€\nÎ¸\n(\nâ‹…\nâˆ£\ns\nt\n)\nâ€‹\nr\nâ€‹\n(\ns\nt\n,\na\nt\n)\n.\n\\max_{\\theta}\\mathbb{E}_{s_{t}\\sim\\mathcal{D},\\;a_{t}\\sim\\pi_{\\theta}(\\cdot\\mid s_{t})}r(s_{t},a_{t}).\n(3)\nIII-B\nSelf-Imitated Diffusion Policy\nTABLE I:\nReward Components\n.\nComponent\nFormulation\nSafety\nCollision\nr\ncol\n=\nâˆ’\nÎ»\ncol\nâ‹…\nğ•€\nâ€‹\n(\ncollision\n)\nr_{\\text{col}}=-\\lambda_{\\text{col}}\\cdot\\mathbb{I}(\\text{collision})\nEfficiency\nStep Cost\nr\nstep\n=\nâˆ’\nÎ»\nstep\nâ‹…\n(\nL\npath\n/\nd\ninit\n)\nr_{\\text{step}}=-\\lambda_{\\text{step}}\\cdot(L_{\\text{path}}/d_{\\text{init}})\nProgress\nr\nprog\n=\nÎ»\nprog\nâ‹…\nÎ”\nâ€‹\nd\ngeo\nr_{\\text{prog}}=\\lambda_{\\text{prog}}\\cdot\\Delta d_{\\text{geo}}\nDocking\nr\ndock\n=\nÎ»\ndock\nâ‹…\nÏˆ\nâ€‹\n(\nd\nt\n)\nâ‹…\nğ•€\nâ€‹\n(\nd\nt\n<\nÎ´\nfine\n)\nr_{\\text{dock}}=\\lambda_{\\text{dock}}\\cdot\\psi(d_{t})\\cdot\\mathbb{I}(d_{t}<\\delta_{\\text{fine}})\nTotal\nr\n=\nr\ncol\n+\nr\nstep\n+\nr\nprog\n+\nr\ndock\nr=r_{\\text{col}}+r_{\\text{step}}+r_{\\text{prog}}+r_{\\text{dock}}\nSettings:\nÎ»\ncol\n=\n10\n,\nÎ»\nstep\n=\n0.5\n,\nÎ»\nprog\n=\n5\n,\nÎ»\ndock\n=\n10\n\\lambda_{\\text{col}}{=}10,\\lambda_{\\text{step}}{=}0.5,\\lambda_{\\text{prog}}{=}5,\\lambda_{\\text{dock}}{=}10\n.\nDefinitions:\nğ•€\nâ€‹\n(\nâ‹…\n)\n\\mathbb{I}(\\cdot)\nis the indicator function.\nÎ”\nâ€‹\nd\ngeo\n\\Delta d_{\\text{geo}}\nis geodesic distance reduction.\nL\npath\nL_{\\text{path}}\n: trajectory length;\nd\ninit\n,\nd\nt\nd_{\\text{init}},d_{t}\n: start and terminal distances to goal.\nDocking:\nÏˆ\nâ€‹\n(\nd\n)\n=\nexp\nâ¡\n(\nâˆ’\n5\nâ€‹\n(\nd\n/\nd\ninit\n)\n2\n)\n\\psi(d)=\\exp(-5(d/d_{\\text{init}})^{2})\nactivates within the fine-tuning region\nÎ´\nfine\n=\n0.5\nâ€‹\nm\n\\delta_{\\text{fine}}{=}0.5\\text{m}\n.\nWe solve the problem defined in Eq.Â (\n3\n) from the distribution matching perspective,\ni.e.\n, to update the policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nto fit a theoretically optimal trajectory distribution\np\nâˆ—\nâ€‹\n(\na\n|\ns\nt\n)\np^{*}(a|s_{t})\n.\nIn practice, this is evolved and implemented by a simple yet effective self-imitation process.\nIII-B\n1\nDistribution Matching\nMaximizing the expected reward reduces to matching\nÏ€\nÎ¸\n\\pi_{\\theta}\nwith the optimal distribution\np\nâˆ—\np^{*}\n, which is achieved by minimizing the KL-divergence\nD\nKL\nâ€‹\n(\np\nâˆ—\nâˆ¥\nÏ€\nÎ¸\n)\nD_{\\text{KL}}(p^{*}\\|\\pi_{\\theta})\n.\nThis can be further simplified to maximizing the expected log-likelihood under\np\nâˆ—\np^{*}\n:\narg\nâ¡\nmin\nÎ¸\nâ¡\nD\nKL\nâ€‹\n(\np\nâˆ—\nâˆ¥\nÏ€\nÎ¸\n)\n\\displaystyle\\arg\\min_{\\theta}D_{\\text{KL}}(p^{*}\\|\\pi_{\\theta})\n=\narg\nâ¡\nmin\nÎ¸\nâ¡\nğ”¼\na\nâˆ¼\np\nâˆ—\nâ€‹\n[\nlog\nâ¡\np\nâˆ—\nâ€‹\n(\na\n|\ns\nt\n)\nâˆ’\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\n|\ns\nt\n)\n]\n\\displaystyle=\\arg\\min_{\\theta}\\mathbb{E}_{a\\sim p^{*}}\\big[\\log p^{*}(a|s_{t})-\\log\\pi_{\\theta}(a|s_{t})\\big]\n=\narg\nâ¡\nmax\nÎ¸\nâ¡\nğ”¼\na\nâˆ¼\np\nâˆ—\nâ€‹\n[\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\n|\ns\nt\n)\n]\n.\n\\displaystyle=\\arg\\max_{\\theta}\\mathbb{E}_{a\\sim p^{*}}[\\log\\pi_{\\theta}(a|s_{t})].\n(4)\nSince direct sampling from the theoretical target\np\nâˆ—\np^{*}\nis intractable, we leverage the current policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nas a proposal distribution to generate\nN\nN\ncandidate actions\nğ’œ\nt\n=\n{\na\nt\n(\ni\n)\n}\ni\n=\n1\nN\nâˆ¼\nÏ€\nÎ¸\n(\nâ‹…\n|\ns\nt\n)\n\\mathcal{A}_{t}=\\{a_{t}^{(i)}\\}_{i=1}^{N}\\sim\\pi_{\\theta}(\\cdot|s_{t})\nvia importance sampling (\nFig.\n2\n, Phase 1).\nThus the objective is converted as:\nmax\nÎ¸\nâ€‹\nâˆ‘\ni\n=\n1\nk\nw\nâ€‹\n(\na\nt\n(\ni\n)\n)\nâ€‹\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\nt\n(\ni\n)\n|\ns\nt\n)\n.\n\\max_{\\theta}\\sum_{i=1}^{k}w(a_{t}^{(i)})\\log\\pi_{\\theta}(a_{t}^{(i)}|s_{t}).\n(5)\nwhere\nw\nâ€‹\n(\na\nt\n(\ni\n)\n)\nw(a_{t}^{(i)})\nis the importance weight of the\ni\ni\n-th candidate.\nIII-B\n2\nImportance Weights\nAccording to REPS\n[\n22\n]\n, the optimal distribution that maximizes the expected reward, subject to a KL-divergence trust-region constraint relative to the current policy, is analytically given by:\np\nâˆ—\nâ€‹\n(\na\n|\ns\nt\n)\nâˆ\nÏ€\nÎ¸\nâ€‹\n(\na\n|\ns\nt\n)\nâ€‹\nexp\nâ¡\n(\nr\nâ€‹\n(\ns\nt\n,\na\n)\nÏ„\n)\n,\np^{*}(a|s_{t})\\propto\\pi_{\\theta}(a|s_{t})\\exp\\left(\\frac{r(s_{t},a)}{\\tau}\\right),\n(6)\nwhere\nÏ„\n\\tau\nis the temperature parameter that controls the constraint strength,\nÏ€\nÎ¸\n\\pi_{\\theta}\ndenotes the reference (current) policy.\nGenerally, the importance weights\n[\n21\n]\nassigned to the candidates are the likelihood ratio\nw\n=\np\nâˆ—\n/\nÏ€\nÎ¸\nw=p^{*}/\\pi_{\\theta}\n.\nCombining with\nEq.\n6\n, we have\nw\nw\nrelying solely on the trajectory quality\nr\nâ€‹\n(\ns\nt\n,\na\n)\nr(s_{t},a)\n.\nOur reward setting are detailed in\nTab.\nI\n.\nTo mitigate the high variance caused by the exponential nature of Boltzmann distributions, we apply a top-\nk\nk\ntruncation strategy that retains only the\nk\nk\nhighest-reward candidates to cover the dominant modes of\np\nâˆ—\np^{*}\n, as shown in\nFig.\n2\nphase-2.\nThe rewards are normalized to have importance weights:\nw\nâ€‹\n(\na\nt\n(\ni\n)\n)\n=\nexp\nâ¡\n(\nr\nâ€‹\n(\ns\nt\n,\na\nt\n(\ni\n)\n)\n/\nÏ„\n)\nâˆ‘\nj\n=\n1\nk\nexp\nâ¡\n(\nr\nâ€‹\n(\ns\nt\n,\na\nt\n(\nj\n)\n)\n/\nÏ„\n)\n\\displaystyle\\boxed{w(a_{t}^{(i)})=\\frac{\\exp(r(s_{t},a_{t}^{(i)})/\\tau)}{\\sum_{j=1}^{k}\\exp(r(s_{t},a_{t}^{(j)})/\\tau)}}\n(7)\nIII-B\n3\nSelf-Imitated Diffusion Policy Loss\nInspired by the derivation in DDPM\n[\n9\n]\n, maximizing the weighted log-likelihood is reduced to minimizing the reward-weighted denoising loss, which is exactly our SIDP objective:\nâ„’\nSIDP\n=\nâˆ‘\ni\n=\n1\nk\nw\nâ€‹\n(\na\nt\n(\ni\n)\n)\nâ€‹\nğ”¼\nt\nd\nâ€‹\nn\n,\nÏµ\nâ€‹\n[\nâ€–\nÏµ\nâˆ’\nÏµ\nÎ¸\nâ€‹\n(\na\nt\n(\ni\n)\n,\nt\nd\nâ€‹\nn\n,\nt\nd\nâ€‹\nn\n,\ns\nt\n)\nâ€–\n2\n2\n]\n\\boxed{\\mathcal{L}_{\\text{SIDP}}=\\sum_{i=1}^{k}w(a_{t}^{(i)})\\,\\mathbb{E}_{t_{dn},\\epsilon}\\Big[\\bigl\\|\\epsilon-\\epsilon_{\\theta}(a_{t}^{(i),t_{dn}},t_{dn},s_{t})\\bigr\\|_{2}^{2}\\Big]}\n(8)\nwhere\nt\nd\nâ€‹\nn\nt_{dn}\nis the diffusion timestep and\nÏµ\n\\epsilon\nis the noise.\nAt this point, the complete SIDP framework is fully established. Refer to\nAlgorithm\n1\nfor a summary view.\nBy integrating reward-guided self-imitation with the representational power of diffusion models, SIDP forms a coherent pipeline for iterative policy refinement.\nAlgorithm 1\nSelf-Imitated Diffusion Policy (SIDP)\n1:\nInput:\nPolicy network\nÏ€\nÎ¸\n\\pi_{\\theta}\n, candidate count\nN\nN\n, truncation\nk\nk\n, temperature\nÏ„\n\\tau\n2:\nInitialize:\nEnvironment\nâ„°\n\\mathcal{E}\n3:\nwhile\nNOT converged\ndo\n4:\n// Phase 1: Online Sampling\n5:\nObserve current state\ns\nt\ns_{t}\nfrom environment\nâ„°\n\\mathcal{E}\n6:\nGenerate\nN\nN\ncandidate actions\nğ’œ\nt\n\\mathcal{A}_{t}\nusing\nÏ€\nÎ¸\n\\pi_{\\theta}\n.\n7:\n// Environment: Reward Evaluation\n8:\nfor\neach candidate action\na\nt\n(\ni\n)\nâˆˆ\nğ’œ\nt\na_{t}^{(i)}\\in\\mathcal{A}_{t}\ndo\n9:\nEvaluate action quality:\nR\ni\n=\nr\nâ€‹\n(\ns\nt\n,\na\nt\n(\ni\n)\n)\nR_{i}=r(s_{t},a_{t}^{(i)})\n10:\nend\nfor\n11:\n// Phase 2: Filtering â€“ Importance weighting\n12:\nSelect top-\nk\nk\nactions with highest\nR\ni\nR_{i}\n:\nğ’œ\nt\ntop-\nâ€‹\nk\n\\mathcal{A}_{t}^{\\text{top-}k}\n13:\nCompute normalized importance weights (Eq.\n7\n).\n14:\n// Phase 3: Parameter Update (On-Policy)\n15:\nSample diffusion step\nt\nt\nand noise\nÏµ\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nI\n)\n\\epsilon\\sim\\mathcal{N}(0,I)\n16:\nCompute weighted denoising loss (refer to Eq.\n8\n).\n17:\nUpdate policy parameters using\nâˆ‡\nÎ¸\nâ„’\nSIDP\n\\nabla_{\\theta}\\mathcal{L}_{\\text{SIDP}}\n18:\nend\nwhile\n19:\nOutput:\nOptimized policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nIII-C\nComplementary Learning Strategies\nTo further augment the capabilities of the SIDP framework,\nwe devise two synergistic strategies: goal-agnostic exploration and reward-driven curriculum learning.\nIII-C\n1\nGoal-agnostic Exploration\nWe augment the training process with an extra\ngoal-agnostic\nsetting, to activate the exploration ability\n[\n30\n]\nand regularize the point-goal navigation learning.\nAt each environment reset, we first sample auxiliary goal points following a uniform angular and distance distribution within the range of\nâˆ’\n60\nâˆ¼\n60\nâˆ˜\n-60\\sim 60^{\\circ}\nand 3\nâˆ¼\n\\sim\n5â€‰m.\nThen feasible secure paths towards these goals are generated by the current policy\nÏ€\nÎ¸\n\\pi_{\\theta}\n.\nDuring the training forward pass, the point goal\ng\nt\ng_{t}\nin the input state\nEq.\n1\nis replaced with a special goal-agnostic embedding, while the importance weights in\nEq.\n5\nare set to be uniform.\nThis strategy facilitates goal-agnostic exploration by decoupling policy behaviors from specific objectives.\nMeanwhile, the sampled trajectories exhibit increased diversity, preventing the policy from overfitting to a narrow trajectory distribution.\nIII-C\n2\nReward-Driven Curriculum Learning\nIn the process of self-imitation generation, certain scenarios may be overly complex for the current policy.\nTrajectories generated in these scenarios are often suboptimal and can inject training noise that impairs stable policy convergence.\nFurthermore, it is crucial to prevent the model from overfitting to a narrow distribution of scenarios, which frequently leads to sub-optimal local minima.\nTo address these issues, we design a dynamic curriculum that prioritizes scenarios with high learning potential.\nFor each scenario\ns\ni\ns_{i}\n, we assess its current learning potential using two metrics derived from sampled trajectories: the maximum trajectory reward\nR\nmax\nâ€‹\n(\ns\ni\n)\nR_{\\text{max}}(s_{i})\nand the reward range\nR\nrange\nâ€‹\n(\ns\ni\n)\nR_{\\text{range}}(s_{i})\n.\nA scenario is selected for training if it satisfies\nR\nmax\nâ€‹\n(\ns\ni\n)\nâ‰¥\nÏ„\nmax\nR_{\\text{max}}(s_{i})\\geq\\tau_{\\text{max}}\nand\nR\nrange\nâ€‹\n(\ns\ni\n)\nâ‰¥\nÏ„\nrange\nR_{\\text{range}}(s_{i})\\geq\\tau_{\\text{range}}\n,\nwhere\nÏ„\nmax\n\\tau_{\\text{max}}\nguarantees learning feasibility, while\nÏ„\nrange\n\\tau_{\\text{range}}\npromotes informative gradient variance to facilitate policy discrimination.\nIII-D\nEfficient Inference of SIDP\nAscribed to our learning framework, SIDP generates concentrated and robust trajectory distributions.\nConsequently, it bypasses the conventional â€œgenerate-then-filterâ€ paradigm that introduces non-negligible computational overhead during inference.\nInstead of relying on an auxiliary selector to filter multiple candidates, SIDP directly yields a viable trajectory through the denoising process, significantly streamlining the inference pipeline.\nAnother advantage is that SIDP enables the use of DDIM\n[\n29\n]\nfor deterministic sampling.\nCompared to the stochastic DDPM\n[\n9\n]\n, DDIM reduces actual inference steps significantly, thus further improving planning efficiency.\nTo be clear, previous planners\n[\n4\n,\n30\n]\nrely on sample stochasticity via DDPM to explore a broader range of trajectories, compensating for less refined policies.\nTABLE II:\nQuantitative comparison on the InternVLA-N1 S1 benchmark.\nWe report Success Rate (SR, %) and Success weighted by Path Length (SPL, %) across different scene types and difficulty levels. mSR and mSPL denote the mean metrics averaged over all evaluated scenes.\nMethod\nInternScenes\nClutteredEnv\nmSR\n(\nâ†‘\n\\uparrow\n)\nmSPL\n(\nâ†‘\n\\uparrow\n)\nCommercial\nHome\nEasy\nHard\nSR\nSPL\nSR\nSPL\nSR\nSPL\nSR\nSPL\niPlanner\n53.02\n51.44\n39.16\n37.27\n89.80\n88.70\n80.69\n79.28\n59.14\n57.57\nViPlanner\n64.43\n62.90\n43.61\n42.07\n82.08\n81.98\n67.24\n67.07\n60.90\n59.83\nNavDP\n71.25\n68.89\n57.38\n55.08\n93.37\n91.44\n88.71\n86.31\n73.22\n70.95\nSIDP (Ours)\n81.19\n73.36\n63.17\n56.48\n94.36\n89.86\n91.58\n86.78\n79.11\n72.72\nIV\nExperiments\nIV-A\nEvaluation Setup\nIV-A\n1\nBenchmarks\nFor comparative evaluation, we conducted experiments on the\nInternVLA-N1 S1\nbenchmark\n[\n4\n]\nleveraging the high-fidelity Isaac Sim simulator.\nThis benchmark focuses on closed-loop sequential navigation (real-time), requiring the agent to reach goals through continuous, multi-step navigation.\nThe benchmark comprises 60 diverse scenes, including 40 from\nInternScenes\n(equally divided into home and commercial) and 20 from\nClutteredEnv\n(including 10 easy and 10 hard).\nFor each scene, 100 start-goal pairs were randomly sampled in unoccupied spaces. Initial orientations were pre-calculated via path planning to ensure collision-free initialization.\nFor the ablation studies, we curated a compact yet representative benchmark derived from the\nInternData-N1\ndataset\n[\n4\n]\n, encompassing 5 scenes and 500 total trials.\nIn contrast to the real-time interaction of the\nInternVLA-N1 S1\nbenchmark, this setup focuses on one-shot trajectory generation (single-step), evaluating the policyâ€™s ability to plan an entire path in a single inference.\nTo ensure evaluation consistency, we utilized fixed starting positions paired with randomly sampled goals for each scene.\nIV-A\n2\nEvaluation Metrics\nTo facilitate a comprehensive and rigorous evaluation, we report the following standard metrics for the main navigation task:\nâ€¢\nSuccess Rate (SR, %):\nThe percentage of episodes where the agent successfully reaches within 0.5â€‰m of the target.\nâ€¢\nSuccess weighted by Path Length (SPL, %):\nA metric evaluating both success and path efficiency, defined as:\nSPL\n=\n1\nN\nâ€‹\nâˆ‘\ni\n=\n1\nN\nS\ni\nâ‹…\nL\ni\nmax\nâ¡\n(\nP\ni\n,\nL\ni\n)\n,\n\\text{SPL}=\\frac{1}{N}\\sum_{i=1}^{N}S_{i}\\cdot\\frac{L_{i}}{\\max(P_{i},L_{i})},\n(9)\nwhere\nS\ni\nS_{i}\nis the binary success indicator,\nL\ni\nL_{i}\nis the geodesic shortest path length, and\nP\ni\nP_{i}\nis the actual path length taken by the agent.\nFurthermore, to analyze safety and exploration quality in the ablation studies on our custom benchmark, we introduce three additional metrics:\nâ€¢\nCollision Rate (CR, %):\nThe proportion of episodes in which the agent encounters at least one collision, serving as a key indicator of navigation safety.\nâ€¢\nDistance To Goal (DTG, m):\nThe average Euclidean distance between the agentâ€™s final position and the goal.\nâ€¢\nExploration Area (EA, m\n2\n):\nThe non-redundant union of\n16\n16\nsampled trajectories (each modeled as a\n0.2\nâ€‹\nm\n0.2\\text{m}\n-wide swept area), quantifying goal-agnostic exploration diversity.\nIV-B\nImplementation Details\nIV-B\n1\nTraining Scenario Construction\nTo facilitate efficient self-imitation learning without the computational overhead of full physics simulation, we construct an interactive environment based on the\nInternData-N1\ndataset\n[\n4\n]\n.\nSpecifically, we sample the agentâ€™s initial poses from the expert trajectories provided in the dataset, employing temporal subsampling to enhance state diversity.\nSubsequently, point goals of varying difficulty are generated using the sceneâ€™s global Euclidean Signed Distance Field (ESDF), which also ensures collision-free initialization.\nFurthermore, collision detection is approximated by directly querying ESDF values along predicted paths, enabling the agent to rapidly evaluate trajectory safety against complex scene geometries during the iterative training process. The detailed reward formulation is presented in\nTab.\nI\n.\nIV-B\n2\nNetwork Architecture and Training Details\nWe utilize the diffusion-based network, NavDP\n[\n4\n]\n, as the initialization for our policy network, while omitting the auxiliary critic network typically required for trajectory selection.\nThe model is optimized using the AdamW optimizer with a learning rate of\n2\nÃ—\n10\nâˆ’\n5\n2\\times 10^{-5}\n. We employ an effective batch size of 128 (\n64\nÃ—\n2\n64\\times 2\n), distributed across two NVIDIA A100 GPUs.\nThe entire training process spans approximately 35 hours.\nIV-C\nComparisons with State-of-the-art Methods\nWe compare our method with several learning-based visual navigation approaches on key metrics of success and efficiency.\nViPlanner and iPlanner are planning-based baselines that rely on learned perception modules coupled with analytical planners.\nNavDP shares a similar diffusion-based architecture to ours but is trained purely via imitation learning without self-imitation, serving as our primary baseline for assessing both robustness and efficiency.\nIV-C\n1\nNavigation Performance Analysis.\nQuantitative results are summarized in Table\nII\n. Our proposed SIDP achieves the highest SR across all evaluated scenarios and demonstrates superior overall performance in SPL, as evidenced by the highest mean metrics (mSR and mSPL)\nNotably, the performance advantage of SIDP is most pronounced in the challenging\nInternScenes\nenvironments.\nIn the\nCommercial\nand\nHome\nscenarios, which feature intricate furniture layouts and narrow traversable regions, SIDP outperforms NavDP by substantial margins of 9.94% and 5.79% in Success Rate, respectively.\nThis demonstrates that the self-imitation mechanism effectively refines the diffusion policy, enabling it to handle complex geometric constraints where imitation learning often struggles.\nIn the randomized\nClutteredEnv\nenvironments, the performance gap between SIDP and NavDP narrows.\nWe attribute this to a performance saturation effect: the baseline NavDP is already highly competent in these simpler obstacle avoidance tasks (achieving\n>\n93\n%\n>93\\%\nSR in Easy settings).\nHowever, even in these scenarios, SIDP maintains a consistent edge, validating its generalizability and robustness.\nIV-C\n2\nComputational Efficiency.\nBeyond success rates, we evaluate the efficiency of our approach for real-world deployment. We measure the path inference latency against NavDP on an NVIDIA Jetson Orin Nano Super development kit (8GB RAM), a representative hardware configuration for modern mobile robots.\nAs shown in Table\nIII\n, over 100 trials on this device, SIDP with 10-step DDPM sampling achieved an average inference time of 132â€‰ms, nearly 2\nÃ—\n\\times\nfaster than NavDP(273â€‰ms).\nUtilizing DDIM sampling further optimizes the inference pipeline. Notably, when reducing the denoising steps to 5, the inference time decreases to 110ms while the SR even marginally improves to 0.674. Even with a highly compressed 3-step configuration, SIDP maintains a competitive SR of 0.655 with a latency of only 99ms. These results identify the 5-step DDIM configuration as the optimal balance, providing the highest success rate with significant computational efficiency.\nTABLE III\n:\nInference time comparison between different models and scheduler.\nMethod\nScheduler\nDenoising Steps\nTime (ms)\nSR(\nâ†‘\n\\uparrow\n)\nNavDP\nDDPM\n10\n273 (1\nÃ—\n\\times\n)\n0.549\nSIDP\nDDPM\n10\n132 (2.07\nÃ—\n\\times\n)\n0.670\nSIDP\nDDIM\n10\n131 (2.08\nÃ—\n\\times\n)\n0.670\nSIDP\nDDIM\n5\n110 (2.48\nÃ—\n\\times\n)\n0.674\nSIDP\nDDIM\n3\n99\n(2.76\nÃ—\n\\times\n)\n0.655\nThis efficiency gain stems from our self-imitation framework, which generates a concentrated and reliable trajectory distribution, eliminating the need for stochastic and computationally expensive trajectory sampling and filtering seen in traditional diffusion planners.\nIV-D\nAblation Studies and Analysis\nTo evaluate the contribution of each key component in our framework, we conduct a series of ablation experiments corresponding to the strategies introduced in Section\nIII\n:\n(1) the impact of the proposed\nReward-Guided Self-Imitation\nmechanism (Section\nIII-B\n) on training stability and convergence;\n(2) the influence of the\nGoal-agnostic Exploration\nstrategy (Section\nIII-C\n) designed to diversify the training data; and\n(3) the effectiveness of the\nReward-Driven Curriculum Learning\nscheme(Section\nIII-C\n) in prioritizing high-reward training scenarios.\nIV-D\n1\nReward-Guided Self-Imitation\nFigure 3\n:\nLearning curves of SIDP under different temperature coefficients during training. The curves are smoothed using a Gaussian filter (\nÏƒ\n=\n10\n\\sigma=10\n), and the shaded region indicates the rolling-window standard deviation, capturing performance fluctuations caused by environment variations and randomly sampled navigation goals.\nTo validate the advantage of our exponential weighting strategy, we compare it with another linear weighting baseline, denoted as\nLin. Weig\n, where importance weights are assigned linearly based on the reward for all positive trajectories, instead of using the Softmax transformation.\nAs illustrated in\nFigs.\n3\nand\n4\n, Lin. Weig lags far behind the exponential function with\nÏ„\n=\n1.0\n\\tau=1.0\nin overall performance, which may be caused by its lack of sufficient selection pressure to distinguish trajectories.\nWe also investigate the temperature coefficient\nÏ„\n\\tau\n, which modulates this selection sharpness.\nIn\nFigs.\n3\nand\n4\n,\nÏ„\n=\n1.0\n\\tau=1.0\nyields optimal performance.\nExtreme values prove detrimental: low\nÏ„\n\\tau\nleads to instability by over-fitting to a few samples, while high\nÏ„\n\\tau\nflattens the action importance distribution.\nFigure 4\n:\nAblation study of the reward-guided self-imitation mechanism and the Softmax temperature coefficient\nÏ„\n\\tau\n.\nIV-D\n2\nGoal-agnostic Exploration\nTo evaluate the impact of goal-agnostic trajectory augmentation on the point-goal navigation and goal-agnostic exploration, we compare models trained with and without\ngoal-agnostic\nsetting. Note that in the goal-agnostic exploration, no explicit target is provided thus only the CR and EA metrics are reported.\nAs shown in Fig\n5\n, in the point-goal navigation tasks, incorporating the goal-agnostic trajectory augmentation during training leads to trajectories that are both safer and closer to the target, suggesting that goal-agnostic trajectory augmentation acts as a form of regularization that improves the overall navigation performance.\nIn the goal-agnostic exploration tasks, the model trained with goal-agnostic settings achieves a lower CR and a higher EA, indicating its ability to generate safe and diverse trajectories for exploration.\nNotably, this suggests that the goal-agnostic setting enables the model to produce consistently high-quality trajectories while preserving the inherent multi-modal distribution characteristic of diffusion policies.\nFigure 5\n:\nAblation study of goal-agnostic training under different evaluation settings.\nIV-D\n3\nReward-Driven Curriculum Learning\nWe investigate the effect of the reward-driven curriculum learning.\nAs shown in\nFig.\n6\n, the policy trained with curriculum learning achieves higher SR, SPL and lower CR, DTG.\nThe improved SR and reduced CR suggest that the policy benefits from prioritizing feasible scenarios as governed by\nR\nmax\nR_{\\text{max}}\n, thereby mitigating the destabilizing influence of failed trajectories. Simultaneously, the gains in SPL and DTG align with our objective of leveraging\nR\nrange\nR_{\\text{range}}\nto provide discriminative signals, which encourages the model to optimize for navigational efficiency rather than mere task completion.\nFigure 6\n:\nAblation study of reward-driven curriculum learning on policy performance.\nFigure 7\n:\nQualitative results of visual end-to-end navigation.\nTop: Diverse robot platforms (wheeled and quadruped) and their respective POV trajectories. Bottom: Third-person perspective of a navigation trial, showing the static environment (left) and the generated collision-free trajectory (right) visualized via motion blur.\nIV-E\nReal-world Deployment\nFig.\n7\ndemonstrates the robust real-world performance of our method on Unitree Go2 and Agilex T-rex platforms.\nWe utilize a stereo camera setup to acquire synchronized left and right RGB images, which are subsequently processed by BANet\n[\n31\n]\nto estimate the corresponding high-density depth maps.\nOur SIDP framework processes RGB-D inputs alongside human-specified goal points, enabling empirical validation across diverse environments such as offices and hallways.\nThe time-lapse trajectory (bottom right) demonstrates a continuous, collision-free path, highlighting the sim-to-real adaptability and stable navigation of our framework in complex scenes.\nV\nConclusion\nWe introduced the Self-Imitated Diffusion Policy (SIDP) framework, which combines self-imitation learning with diffusion models to enable efficient and robust path planning for visual navigation. By continuously learning from self-generated experiences, SIDP overcomes the suboptimal and incomplete nature of expert datasets, training policies that generate higher-performing trajectories, which also reduces the need for complex trajectory selection, lowering computational overhead and improving efficiency. The framework also incorporates two key strategies: goal-agnostic exploration to enhance regularization, and reward-driven curriculum learning to enhance data utility, further optimizing the policyâ€™s performance.\nExperimental results demonstrate that SIDP outperforms existing methods across various metrics, including success rates(SR), success weighted by path Length(SPL). Additionally, SIDP achieves a 2.5\nÃ—\n\\times\nspeedup in inference latency on embedded platforms, proving its suitability for real-time robotic deployment.\nIn conclusion, SIDP provides a robust and efficient solution for visual navigation, bridging the gap between generative models and practical robotic applications.\nAcknowledgment\nThis research was supported by the Key Scientific Research Program of Hangzhou Municipal Bureau of Science and Technology grants 2025SZD1A01.\nReferences\n[1]\nA. Ajay, Y. Du, A. Gupta, J. Tenenbaum, T. Jaakkola, and P. Agrawal\n(2023-07-10)\nIs conditional generative modeling all you need for decision-making?\n.\narXiv\n.\nExternal Links:\nLink\n,\n2211.15657 [cs]\nCited by:\nÂ§\nII-B\n.\n[2]\nP. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. SÃ¼nderhauf, I. Reid, S. Gould, and A. v. d. Hengel\n(2018-04-05)\nVision-and-language navigation: interpreting visually-grounded navigation instructions in real environments\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n1711.07280 [cs]\nCited by:\nÂ§\nII-A\n,\nÂ§\nII-B\n.\n[3]\nA. Bar, G. Zhou, D. Tran, T. Darrell, and Y. LeCun\n(2025)\nNavigation world models\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 15791â€“15801\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[4]\nW. Cai, J. Peng, Y. Yang, Y. Zhang, M. Wei, H. Wang, Y. Chen, T. Wang, and J. Pang\n(2025-05-15)\nNavDP: learning sim-to-real navigation diffusion policy with privileged information guidance\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2505.08712 [cs]\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nIII-D\n,\nÂ§\nIV-A\n1\n,\nÂ§\nIV-A\n1\n,\nÂ§\nIV-B\n1\n,\nÂ§\nIV-B\n2\n.\n[5]\nC. Campos, R. Elvira, J. J. G. RodrÃ­guez, J. M. M. Montiel, and J. D. TardÃ³s\n(2021-12)\nORB-SLAM3: an accurate open-source library for visual, visual-inertial and multi-map SLAM\n.\n37\n(\n6\n),\npp.Â 1874â€“1890\n.\nExternal Links:\nISSN 1552-3098, 1941-0468\n,\nLink\n,\nDocument\n,\n2007.11898 [cs]\nCited by:\nÂ§\nII-A\n.\n[6]\nJ. Carvalho, A. T. Le, M. Baierl, D. Koert, and J. Peters\n(2024-03-26)\nMotion planning diffusion: learning and planning of robot motions with diffusion models\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2308.01557 [cs]\nCited by:\nÂ§\nII-B\n.\n[7]\nS. S. Desai and S. Lee\n(2021-01)\nAuxiliary tasks for efficient learning of point-goal navigation\n.\nIn\n2021 IEEE Winter Conference on Applications of Computer Vision (WACV)\n,\npp.Â 717â€“725\n.\nExternal Links:\nISBN 978-1-6654-0477-8\n,\nLink\n,\nDocument\nCited by:\nÂ§\nII-A\n.\n[8]\nD. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, and T. Darrell\n(2018-10-27)\nSpeaker-follower models for vision-and-language navigation\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n1806.02724 [cs]\nCited by:\nÂ§\nII-A\n,\nÂ§\nII-B\n.\n[9]\nJ. Ho, A. Jain, and P. Abbeel\n(2020)\nDenoising diffusion probabilistic models\n.\nAdvances in neural information processing systems\n33\n,\npp.Â 6840â€“6851\n.\nCited by:\nÂ§\nIII-B\n3\n,\nÂ§\nIII-D\n.\n[10]\nM. Janner, Y. Du, J. B. Tenenbaum, and S. Levine\n(2022-12-21)\nPlanning with diffusion for flexible behavior synthesis\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2205.09991 [cs]\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[11]\nK. Kim, K. Shin, M. W. Lee, M. Lee, M. Lee, and B. Zhang\n(2024)\nVisual hindsight self-imitation learning for interactive navigation\n.\n12\n,\npp.Â 83796â€“83809\n.\nExternal Links:\nISSN 2169-3536\n,\nLink\n,\nDocument\n,\n2312.03446 [cs]\nCited by:\nÂ§\nII-A\n.\n[12]\nK. Kondo, A. Tagliabue, X. Cai, C. Tewari, O. Garcia, M. Espitia-Alvarez, and J. P. How\n(2024-05-02)\nCGD: constraint-guided diffusion policies for UAV trajectory planning\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2405.01758 [cs]\nCited by:\nÂ§\nII-B\n.\n[13]\nJ. Krantz, A. Gokaslan, D. Batra, S. Lee, and O. Maksymets\n(2021-10-05)\nWaypoint models for instruction-guided navigation in continuous environments\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2110.02207 [cs]\nCited by:\nÂ§\nII-A\n,\nÂ§\nII-B\n.\n[14]\nY. Li, J. Song, and S. Ermon\n(2017-11-14)\nInfoGAIL: interpretable imitation learning from visual demonstrations\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n1703.08840 [cs]\nCited by:\nÂ§\nII-A\n,\nÂ§\nII-B\n.\n[15]\nZ. Li, W. Yao, Z. Wang, X. Sun, J. Chen, N. Chang, M. Shen, Z. Wu, S. Lan, and J. M. Alvarez\n(2025-06-07)\nGeneralized trajectory scoring for end-to-end multimodal planning\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2506.06664 [cs]\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[16]\nZ. Liang, Y. Mu, H. Ma, M. Tomizuka, M. Ding, and P. Luo\n(2024-03-28)\nSkillDiffuser: interpretable hierarchical planning via skill abstractions in diffusion-based task execution\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2312.11598 [cs]\nCited by:\nÂ§\nII-B\n.\n[17]\nB. Liao, S. Chen, H. Yin, B. Jiang, C. Wang, S. Yan, X. Zhang, X. Li, Y. Zhang, Q. Zhang, and X. Wang\n(2025-04-10)\nDiffusionDrive: truncated diffusion model for end-to-end autonomous driving\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2411.15139 [cs]\nCited by:\nÂ§\nII-B\n.\n[18]\nY. Luo, U. A. Mishra, Y. Du, and D. Xu\n(2025-05-05)\nGenerative trajectory stitching through diffusion composition\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2503.05153 [cs]\nCited by:\nÂ§\nII-B\n.\n[19]\nR. Mur-Artal and J. D. Tardos\n(2017-10)\nORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-d cameras\n.\n33\n(\n5\n),\npp.Â 1255â€“1262\n.\nExternal Links:\nISSN 1552-3098, 1941-0468\n,\nLink\n,\nDocument\n,\n1610.06475 [cs]\nCited by:\nÂ§\nII-A\n.\n[20]\nJ. Oh, Y. Guo, S. Singh, and H. Lee\n(2018-06)\nSelf-Imitation Learning\n.\narXiv\n.\nNote:\narXiv:1806.05635 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§I\n.\n[21]\nA. B. Owen\n(2013)\nMonte carlo theory, methods and examples\n.\nStanford University\n.\nNote:\nAvailable online at\n\\url\nhttps://statweb.stanford.edu/Â owen/mc/\nCited by:\nÂ§\nIII-B\n2\n.\n[22]\nJ. Peters, K. Mulling, and Y. Altun\n(2010)\nRelative entropy policy search\n.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n,\nVol.\n24\n,\npp.Â 1607â€“1612\n.\nCited by:\nÂ§\nIII-B\n2\n.\n[23]\nA. Popov, A. Degirmenci, D. Wehr, S. Hegde, R. Oldja, A. Kamenev, B. Douillard, D. NistÃ©r, U. Muller, R. Bhargava, S. Birchfield, and N. Smolyanskiy\n(2025-05)\nMitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models\n.\narXiv\n.\nNote:\narXiv:2409.16663 [cs]Comment: 8 pages, 6 figures, updated in March 2025, original published in September 2024, for ICRA 2025 submission, for associated video file, see https://youtu.be/7m3bXzlVQvU\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§I\n.\n[24]\nA. Z. Ren, J. Lidard, L. L. Ankile, A. Simeonov, P. Agrawal, A. Majumdar, B. Burchfiel, H. Dai, and M. Simchowitz\n(2024)\nDiffusion policy policy optimization\n.\narXiv preprint arXiv:2409.00588\n.\nCited by:\nÂ§I\n.\n[25]\nH. Ren, Y. Zeng, Z. Bi, Z. Wan, J. Huang, and H. Cheng\n(2025-04-14)\nPrior does matter: visual navigation via denoising diffusion bridge models\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2504.10041 [cs]\nCited by:\nÂ§\nII-B\n.\n[26]\nS. Ross, G. Gordon, and D. Bagnell\n(2011-06-14)\nA reduction of imitation learning and structured prediction to no-regret online learning\n.\nIn\nProceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics\n,\npp.Â 627â€“635\n.\nNote:\nISSN: 1938-7228\nExternal Links:\nLink\nCited by:\nÂ§\nII-A\n.\n[27]\nD. Shah, B. Eysenbach, G. Kahn, N. Rhinehart, and S. Levine\n(2021-05-30)\nViNG: learning open-world navigation with visual goals\n.\nIn\n2021 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 13215â€“13222\n.\nExternal Links:\nLink\n,\nDocument\n,\n2012.09812 [cs]\nCited by:\nÂ§\nII-A\n,\nÂ§\nII-B\n.\n[28]\nH. Shi, X. Deng, Z. Li, G. Chen, Y. Wang, and L. Nie\n(2025-08-13)\nDAgger diffusion navigation: DAgger boosted diffusion policy for vision-language navigation\n.\narXiv\n.\nNote:\nversion: 1\nExternal Links:\nLink\n,\nDocument\n,\n2508.09444 [cs]\nCited by:\nÂ§I\n.\n[29]\nJ. Song, C. Meng, and S. Ermon\n(2020)\nDenoising diffusion implicit models\n.\narXiv preprint arXiv:2010.02502\n.\nCited by:\nÂ§I\n,\nÂ§\nIII-D\n.\n[30]\nA. Sridhar, D. Shah, C. Glossop, and S. Levine\n(2024-05-13)\nNoMaD: goal masked diffusion policies for navigation and exploration\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 63â€“70\n.\nExternal Links:\nISBN 979-8-3503-8457-4\n,\nLink\n,\nDocument\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nIII-C\n1\n,\nÂ§\nIII-D\n.\n[31]\nF. Tsai, Y. Peng, C. Tsai, Y. Lin, and C. Lin\n(2022)\nBANet: a blur-aware attention network for dynamic scene deblurring\n.\nIEEE Transactions on Image Processing\n31\n,\npp.Â 6789â€“6799\n.\nCited by:\nÂ§\nIV-E\n.\n[32]\nZ. Wang, J. J. Hunt, and M. Zhou\n(2022)\nDiffusion policies as an expressive policy class for offline reinforcement learning\n.\narXiv preprint arXiv:2208.06193\n.\nCited by:\nÂ§I\n.\n[33]\nE. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra\n(2020-01-20)\nDD-PPO: learning near-perfect PointGoal navigators from 2.5 billion frames\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n1911.00357 [cs]\nCited by:\nÂ§\nII-A\n.\n[34]\nQ. Wu, X. Gong, K. Xu, D. Manocha, J. Dong, and J. Wang\n(2022-05-09)\nTowards target-driven visual navigation in indoor scenes via generative imitation learning\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2009.14509 [cs]\nCited by:\nÂ§\nII-A\n,\nÂ§\nII-B\n.\n[35]\nB. Yang, H. Su, N. Gkanatsios, T. Ke, A. Jain, J. Schneider, and K. Fragkiadaki\n(2024-07-16)\nDiffusion-ES: gradient-free planning with diffusion for autonomous driving and zero-shot instruction following\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2402.06559 [cs]\nCited by:\nÂ§\nII-B\n.\n[36]\nN. Yang, J. Gao, F. Gao, Y. Wu, and C. Yu\n(2025)\nFine-tuning diffusion policies with backpropagation through diffusion timesteps\n.\nExternal Links:\n2505.10482\n,\nLink\nCited by:\nÂ§I\n.\n[37]\nJ. Ye, D. Batra, E. Wijmans, and A. Das\n(2020-11-04)\nAuxiliary tasks speed up learning PointGoal navigation\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2007.04561 [cs]\nCited by:\nÂ§\nII-A\n.\n[38]\nW. Yu, J. Peng, H. Yang, J. Zhang, Y. Duan, J. Ji, and Y. Zhang\n(2024-10)\nLDP: a local diffusion planner for efficient robot navigation and collision avoidance\n.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 5466â€“5472\n.\nNote:\nISSN: 2153-0866\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§\nII-B\n.\n[39]\nY. Zeng, H. Ren, S. Wang, J. Huang, and H. Cheng\n(2025-04-14)\nNaviDiffusor: cost-guided diffusion model for visual navigation\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2504.10003 [cs]\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[40]\nY. Zheng, R. Liang, K. Zheng, J. Zheng, L. Mao, J. Li, W. Gu, R. Ai, S. E. Li, X. Zhan, and J. Liu\n(2025-02-09)\nDiffusion-based planning for autonomous driving with flexible guidance\n.\narXiv\n.\nExternal Links:\nLink\n,\nDocument\n,\n2501.15564 [cs]\nCited by:\nÂ§\nII-B\n.",
    "preview_text": "Diffusion policies (DP) have demonstrated significant potential in visual navigation by capturing diverse multi-modal trajectory distributions. However, standard imitation learning (IL), which most DP methods rely on for training, often inherits sub-optimality and redundancy from expert demonstrations, thereby necessitating a computationally intensive \"generate-then-filter\" pipeline that relies on auxiliary selectors during inference. To address these challenges, we propose Self-Imitated Diffusion Policy (SIDP), a novel framework that learns improved planning by selectively imitating a set of trajectories sampled from itself. Specifically, SIDP introduces a reward-guided self-imitation mechanism that encourages the policy to consistently produce high-quality trajectories efficiently, rather than outputs of inconsistent quality, thereby reducing reliance on extensive sampling and post-filtering. During training, we employ a reward-driven curriculum learning paradigm to mitigate inefficient data utility, and goal-agnostic exploration for trajectory augmentation to improve planning robustness. Extensive evaluations on a comprehensive simulation benchmark show that SIDP significantly outperforms previous methods, with real-world experiments confirming its effectiveness across multiple robotic platforms. On Jetson Orin Nano, SIDP delivers a 2.5$\\times$ faster inference than the baseline NavDP, i.e., 110ms VS 273ms, enabling efficient real-time deployment.\n\nSelf-Imitated Diffusion Policy for Efficient and Robust\nVisual Navigation\nRunhua Zhang\n1,2âˆ—\n,\nJunyi Hou\n1,2âˆ—\n,\nChangxu Cheng\n1â€ \n,\nQiyi Chen\n1\n,\nTao Wang\n1\n,\nWuyue Zhao\n1\n1\nUni-Ubi\n2\nZhejiang University\nAbstract\nDiffusion policies (DP) have demonstrated significant potential in visual navigation by capturing diverse multi-modal trajectory distributions.\nHowever, standard imitation learning (IL), which most DP methods rely on for training, often inherits sub-optimality and redundancy from expert demonstrations, thereby n",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "Diffusion",
        "Inference Acceleration",
        "Edge Deployment",
        "fine tune",
        "goal conditioned reinforcement learning",
        "world model"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªæ¨¡ä»¿æ‰©æ•£ç­–ç•¥ï¼Œé€šè¿‡å¥–åŠ±å¼•å¯¼çš„è‡ªæˆ‘æ¨¡ä»¿æœºåˆ¶å’Œè¯¾ç¨‹å­¦ä¹ ï¼Œæé«˜äº†è§†è§‰å¯¼èˆªçš„æ•ˆç‡å’Œé²æ£’æ€§ï¼Œå¹¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°äº†å¿«é€Ÿæ¨ç†ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šSelf-Imitated Diffusion Policy for Efficient and Robust Visual Navigation\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nåŸºäºæ‰©æ•£æ¨¡å‹çš„ç­–ç•¥åœ¨è§†è§‰å¯¼èˆªä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå› å…¶èƒ½æ•æ‰å¤šæ¨¡æ€è½¨è¿¹åˆ†å¸ƒã€‚ç„¶è€Œï¼Œä¸»æµæ–¹æ³•ä¾èµ–æ¨¡ä»¿å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œè¿™ä½¿å…¶ç»§æ‰¿äº†ä¸“å®¶æ¼”ç¤ºæ•°æ®ä¸­çš„æ¬¡ä¼˜æ€§å’Œå†—ä½™æ€§ã€‚å› æ­¤ï¼Œåœ¨æ¨ç†æ—¶é€šå¸¸éœ€è¦ä¸€ä¸ªè®¡ç®—å¯†é›†çš„â€œç”Ÿæˆ-åè¿‡æ»¤â€æµç¨‹ï¼Œä¾èµ–è¾…åŠ©é€‰æ‹©å™¨æ¥ç­›é€‰å¤šä¸ªå€™é€‰è½¨è¿¹ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼Œå°¤å…¶åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæå‡ºä¸€ç§èƒ½å¤Ÿ**é«˜æ•ˆã€é²æ£’**åœ°ç”Ÿæˆé«˜è´¨é‡è½¨è¿¹çš„æ–°æ¡†æ¶ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡ºäº†**è‡ªæ¨¡ä»¿æ‰©æ•£ç­–ç•¥**æ¡†æ¶ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š\n- **å¥–åŠ±å¼•å¯¼çš„è‡ªæ¨¡ä»¿æœºåˆ¶**ï¼šç­–ç•¥ä»è‡ªèº«é‡‡æ ·ç”Ÿæˆä¸€æ‰¹å€™é€‰è½¨è¿¹ï¼Œå¹¶æ ¹æ®è½¨è¿¹è´¨é‡ï¼ˆå¥–åŠ±ï¼‰è¿›è¡ŒåŠ æƒï¼Œä¼˜å…ˆå­¦ä¹ é«˜è´¨é‡çš„ç»éªŒã€‚è¿™é¿å…äº†å¯¹å¤–éƒ¨æ¬¡ä¼˜ä¸“å®¶æ•°æ®çš„ä¾èµ–ï¼Œå¹¶ä¿ƒä½¿ç­–ç•¥çš„è½¨è¿¹åˆ†å¸ƒ**å‘é«˜è´¨é‡åŒºåŸŸé›†ä¸­**ã€‚\n- **äº’è¡¥å­¦ä¹ ç­–ç•¥**ï¼š\n    - **ç›®æ ‡æ— å…³æ¢ç´¢**ï¼šåœ¨è®­ç»ƒä¸­å¼•å…¥æ— æ˜ç¡®ç›®æ ‡çš„æ¢ç´¢ä»»åŠ¡ï¼Œä»¥å¢å¼ºç­–ç•¥çš„æ¢ç´¢èƒ½åŠ›å’Œè½¨è¿¹å¤šæ ·æ€§ï¼Œèµ·åˆ°æ­£åˆ™åŒ–ä½œç”¨ã€‚\n    - **å¥–åŠ±é©±åŠ¨çš„è¯¾ç¨‹å­¦ä¹ **ï¼šåŠ¨æ€é€‰æ‹©å…·æœ‰é«˜å­¦ä¹ æ½œåŠ›çš„è®­ç»ƒåœºæ™¯ï¼Œæå‡æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼Œä¿ƒè¿›ç¨³å®šæ”¶æ•›ã€‚\n- **é«˜æ•ˆæ¨ç†**ï¼šç”±äºç­–ç•¥è¾“å‡ºçš„è½¨è¿¹è´¨é‡é«˜ä¸”é›†ä¸­ï¼ŒSIDP**æ‘’å¼ƒäº†ä¼ ç»Ÿçš„â€œç”Ÿæˆ-åè¿‡æ»¤â€æµç¨‹å’Œå¤–éƒ¨é€‰æ‹©å™¨**ï¼Œå¯ç›´æ¥è¾“å‡ºå¯è¡Œè·¯å¾„ã€‚åŒæ—¶ï¼Œæ”¯æŒä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·ï¼ˆå¦‚DDIMï¼‰å¹¶å‡å°‘å»å™ªæ­¥æ•°ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nåœ¨é«˜ä¿çœŸä»¿çœŸåŸºå‡†InternVLA-N1 S1ä¸Šçš„å®éªŒè¡¨æ˜ï¼š\n- **å¯¼èˆªæ€§èƒ½é¢†å…ˆ**ï¼šåœ¨æˆåŠŸç‡ï¼ˆSRï¼‰å’Œè·¯å¾„é•¿åº¦åŠ æƒæˆåŠŸç‡ï¼ˆSPLï¼‰ä¸Šå‡è¶…è¶ŠåŸºçº¿æ–¹æ³•ï¼ˆå¦‚NavDPï¼‰ã€‚åœ¨å¤æ‚çš„å•†ä¸šå’Œå®¶åº­åœºæ™¯ä¸­ï¼Œä¼˜åŠ¿å°¤ä¸ºæ˜æ˜¾ï¼ˆSRæå‡çº¦10%ï¼‰ï¼Œè¯æ˜äº†å…¶å“è¶Šçš„æ³›åŒ–ä¸é²æ£’æ€§ã€‚\n- **æ¨ç†æ•ˆç‡æ˜¾è‘—æå‡**ï¼šåœ¨Jetson Orin Nanoè¾¹ç¼˜è®¾å¤‡ä¸Šï¼ŒSIDPçš„æ¨ç†å»¶è¿Ÿä»…ä¸º110æ¯«ç§’ï¼ˆä½¿ç”¨5æ­¥DDIMï¼‰ï¼Œæ¯”åŸºçº¿NavDPï¼ˆ273æ¯«ç§’ï¼‰**å¿«2.5å€**ï¼Œä¸”æ€§èƒ½æœªå—æŸã€‚\n- **æ¶ˆèå®éªŒéªŒè¯**ï¼šå¥–åŠ±å¼•å¯¼çš„è‡ªæ¨¡ä»¿ã€ç›®æ ‡æ— å…³æ¢ç´¢å’Œè¯¾ç¨‹å­¦ä¹ ä¸‰ä¸ªç»„ä»¶å‡è¢«è¯æ˜å¯¹æå‡æ€§èƒ½è‡³å…³é‡è¦ã€‚\n- **å®ç‰©éƒ¨ç½²æˆåŠŸ**ï¼šåœ¨è½®å¼å’Œå››è¶³æœºå™¨äººå¹³å°ä¸Šçš„çœŸå®ä¸–ç•Œå®éªŒéªŒè¯äº†å…¶ä»ä»¿çœŸåˆ°ç°å®çš„è¿ç§»èƒ½åŠ›ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\nSIDPä¸ºè§†è§‰å¯¼èˆªæä¾›äº†ä¸€ç§**é«˜æ•ˆä¸”é²æ£’**çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆã€‚å®ƒé€šè¿‡è‡ªæ¨¡ä»¿å­¦ä¹ æœºåˆ¶ï¼Œå·§å¦™åœ°ç»•è¿‡äº†ä¼ ç»Ÿæ‰©æ•£ç­–ç•¥å¯¹å¤§é‡é‡‡æ ·å’Œåå¤„ç†çš„ä¾èµ–ï¼Œ**åœ¨ä¿æŒç”Ÿæˆæ¨¡å‹å¤šæ ·æ€§çš„åŒæ—¶ï¼Œå®ç°äº†åˆ†å¸ƒé›†ä¸­å’Œæ¨ç†æµç¨‹çš„ç®€åŒ–**ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æ˜¾è‘—æå‡äº†å¯¼èˆªæ€§èƒ½ï¼Œæ›´é‡è¦çš„æ˜¯å¤§å¹…é™ä½äº†è®¡ç®—å¼€é”€ï¼Œä½¿å¾—é«˜æ€§èƒ½çš„æ‰©æ•£è§„åˆ’ç­–ç•¥èƒ½å¤Ÿ**å®æ—¶éƒ¨ç½²åœ¨èµ„æºå—é™çš„æœºå™¨äººå¹³å°ä¸Š**ï¼Œæ¨åŠ¨äº†ç”Ÿæˆå¼æ¨¡å‹åœ¨å®ç”¨æœºå™¨äººç³»ç»Ÿä¸­çš„è½åœ°ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **ä¸€ã€ è®ºæ–‡æƒ³è§£å†³çš„æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†è§‰å¯¼èˆªç­–ç•¥**åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå­˜åœ¨çš„ä¸¤ä¸ªå…³é”®ç“¶é¢ˆï¼š\n1.  **é²æ£’æ€§ä¸è¶³**ï¼šä¼ ç»Ÿçš„æ¨¡ä»¿å­¦ä¹ ä¾èµ–äºæœ‰é™çš„ä¸“å®¶æ¼”ç¤ºæ•°æ®é›†ï¼Œå¯¼è‡´ç­–ç•¥åœ¨é¢å¯¹åˆ†å¸ƒå¤–åœºæ™¯ï¼ˆå¦‚æ–°ç¯å¢ƒã€å¤æ‚å¸ƒå±€ï¼‰æ—¶æ³›åŒ–èƒ½åŠ›å·®ï¼Œå®¹æ˜“å¤±è´¥ã€‚\n2.  **æ¨ç†æ•ˆç‡ä½ä¸‹**ï¼šæ ‡å‡†æ¨¡ä»¿å­¦ä¹ ä¼šæ¨¡ä»¿æ‰€æœ‰ä¸“å®¶è½¨è¿¹ï¼ˆåŒ…æ‹¬æ¬¡ä¼˜å’Œå†—ä½™çš„ï¼‰ï¼Œå¯¼è‡´ç­–ç•¥ç”Ÿæˆçš„è½¨è¿¹è´¨é‡å‚å·®ä¸é½ã€‚ä¸ºäº†è·å¾—å¯è¡Œè·¯å¾„ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ **â€œå…ˆç”Ÿæˆ-åè¿‡æ»¤â€** çš„æ¨ç†æµç¨‹ï¼Œå³ç”Ÿæˆå¤§é‡å€™é€‰è½¨è¿¹ï¼Œå†é€šè¿‡ä¸€ä¸ªé¢å¤–çš„é€‰æ‹©å™¨è¿›è¡Œç­›é€‰ã€‚è¿™å¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—å¼€é”€ï¼Œå°¤å…¶åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œéš¾ä»¥å®ç°å®æ—¶éƒ¨ç½²ã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼šè‡ªæ¨¡ä»¿æ‰©æ•£ç­–ç•¥**\nè®ºæ–‡æå‡ºäº† **è‡ªæ¨¡ä»¿æ‰©æ•£ç­–ç•¥** æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº**ç”¨â€œè‡ªæˆ‘æ¨¡ä»¿â€å–ä»£â€œä¸“å®¶æ¨¡ä»¿â€**ï¼Œä»æ ¹æœ¬ä¸Šé‡å¡‘äº†ç­–ç•¥çš„å­¦ä¹ æ–¹å¼ã€‚\n\n**æ ¸å¿ƒæœºåˆ¶ï¼šå¥–åŠ±å¼•å¯¼çš„è‡ªæ¨¡ä»¿**\n- **ä¼ ç»Ÿæ–¹æ³•**ï¼šä»å›ºå®šçš„ä¸“å®¶æ•°æ®é›†ä¸­å­¦ä¹ ã€‚\n- **SIDPæ–¹æ³•**ï¼šç­–ç•¥ä»**è‡ªèº«ç”Ÿæˆ**çš„è½¨è¿¹ä¸­å­¦ä¹ ã€‚åœ¨è®­ç»ƒæ—¶ï¼Œç­–ç•¥ä¼šç”Ÿæˆä¸€æ‰¹å€™é€‰è½¨è¿¹ï¼Œç„¶åæ ¹æ®ä¸€ä¸ª**å¥–åŠ±å‡½æ•°**ï¼ˆè¯„ä¼°è½¨è¿¹çš„å®‰å…¨æ€§ã€æ•ˆç‡ç­‰ï¼Œè§è¡¨Iï¼‰å¯¹è¿™äº›è½¨è¿¹è¿›è¡Œæ’åºå’ŒåŠ æƒï¼Œæœ€åç­–ç•¥æ¨¡ä»¿é‚£äº›é«˜å¥–åŠ±çš„è½¨è¿¹ã€‚\n- **æ•°å­¦æœ¬è´¨**ï¼šé€šè¿‡é‡è¦æ€§é‡‡æ ·å’ŒåŸºäºå¥–åŠ±çš„æƒé‡åˆ†é…ï¼Œå°†ç­–ç•¥ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ª**åŠ æƒå»å™ªæŸå¤±**çš„æœ€å°åŒ–é—®é¢˜ï¼Œä½¿ç­–ç•¥åˆ†å¸ƒé€æ¸å‘é«˜è´¨é‡è½¨è¿¹åˆ†å¸ƒå¯¹é½ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆçš„ä¸‰å¤§æ”¯æŸ±**\nSIDPé€šè¿‡ä¸€ä¸ªå®Œæ•´çš„æ¡†æ¶æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå…·ä½“åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªæ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼š\n\n1.  **å¥–åŠ±å¼•å¯¼çš„è‡ªæ¨¡ä»¿å­¦ä¹ æœºåˆ¶**\n    - **å¦‚ä½•å·¥ä½œ**ï¼šç­–ç•¥`Ï€_Î¸`ç”ŸæˆNæ¡å€™é€‰è½¨è¿¹ â†’ ç”¨å¥–åŠ±å‡½æ•°`r(s, a)`è¯„ä¼°æ¯æ¡è½¨è¿¹ â†’ å¯¹å¥–åŠ±è¿›è¡ŒSoftmaxå½’ä¸€åŒ–å¾—åˆ°é‡è¦æ€§æƒé‡`w(a)` â†’ ç”¨åŠ æƒæŸå¤±`â„’_SIDP`ï¼ˆå…¬å¼8ï¼‰æ›´æ–°ç­–ç•¥ï¼Œä½¿å…¶æ›´å€¾å‘äºç”Ÿæˆé«˜å¥–åŠ±è½¨è¿¹ã€‚\n    - **è§£å†³ä»€ä¹ˆé—®é¢˜**ï¼š\n        - **æå‡é²æ£’æ€§**ï¼šé€šè¿‡æŒç»­ä»è‡ªèº«çš„é«˜è´¨é‡æˆåŠŸç»éªŒä¸­å­¦ä¹ ï¼Œç­–ç•¥èƒ½ä¸æ–­è‡ªæˆ‘æ”¹è¿›ï¼Œè¶…è¶Šå›ºå®šä¸“å®¶æ•°æ®é›†çš„åå·®ï¼Œé€‚åº”æ›´å¹¿æ³›çš„ç¯å¢ƒã€‚\n        - **æå‡æ•ˆç‡**ï¼šè¯¥è¿‡ç¨‹ä¿ƒä½¿ç­–ç•¥çš„è½¨è¿¹åˆ†å¸ƒå‘ç”Ÿ **â€œåˆ†å¸ƒé›†ä¸­â€** â€”â€” å³å‡å°‘æ¬¡ä¼˜æ¨¡å¼ï¼ŒåŒæ—¶ä¿ç•™é€šå¾€ç›®æ ‡çš„å¤šæ¡ä¼˜è´¨è·¯å¾„ã€‚è¿™ä½¿å¾—æ¨ç†æ—¶**æ— éœ€**ç”Ÿæˆå¤§é‡æ ·æœ¬å†è¿›è¡Œè¿‡æ»¤ï¼Œ**ç›´æ¥è¾“å‡º**çš„è½¨è¿¹è´¨é‡å°±å¾ˆé«˜ã€‚\n\n2.  **ç›®æ ‡æ— å…³çš„æ¢ç´¢**\n    - **å¦‚ä½•å·¥ä½œ**ï¼šåœ¨è®­ç»ƒä¸­ï¼Œéšæœºç”Ÿæˆä¸€äº›è¾…åŠ©ç›®æ ‡ç‚¹ï¼Œè®©ç­–ç•¥åœ¨ä¸æŒ‡å®šå…·ä½“ç›®æ ‡çš„æƒ…å†µä¸‹è¿›è¡Œæ¢ç´¢ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„è½¨è¿¹ã€‚\n    - **è§£å†³ä»€ä¹ˆé—®é¢˜**ï¼š\n        - **å¢å¼ºæ•°æ®å¤šæ ·æ€§**ï¼šæ‰©å……è®­ç»ƒè½¨è¿¹æ± ï¼Œé˜²æ­¢ç­–ç•¥è¿‡æ‹Ÿåˆåˆ°ç‹­çª„çš„è·¯å¾„åˆ†å¸ƒã€‚\n        - **èµ·åˆ°æ­£åˆ™åŒ–ä½œç”¨**ï¼šæå‡ç­–ç•¥åœ¨æŒ‡å®šç›®æ ‡å¯¼èˆªä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œå®‰å…¨æ€§ã€‚\n\n3.  **å¥–åŠ±é©±åŠ¨çš„è¯¾ç¨‹å­¦ä¹ **\n    - **å¦‚ä½•å·¥ä½œ**ï¼šåŠ¨æ€é€‰æ‹©è®­ç»ƒåœºæ™¯ã€‚åªé€‰æ‹©é‚£äº›å½“å‰ç­–ç•¥èƒ½è·å¾—ä¸€å®šå¥–åŠ±ï¼ˆ`R_max`ï¼‰ä¸”å¥–åŠ±æ–¹å·®è¶³å¤Ÿå¤§ï¼ˆ`R_range`ï¼‰çš„åœºæ™¯è¿›è¡Œè®­ç»ƒã€‚\n    - **è§£å†³ä»€ä¹ˆé—®é¢˜**ï¼š\n        - **æé«˜æ•°æ®åˆ©ç”¨æ•ˆç‡**ï¼šé¿å…åœ¨è¿‡äºç®€å•ï¼ˆæ— å­¦ä¹ ä»·å€¼ï¼‰æˆ–è¿‡äºå›°éš¾ï¼ˆå…¨æ˜¯å¤±è´¥ç»éªŒï¼‰çš„åœºæ™¯ä¸Šæµªè´¹è®¡ç®—èµ„æºã€‚\n        - **ä¿ƒè¿›ç¨³å®šæ”¶æ•›**ï¼šä¸ºç­–ç•¥æä¾›éš¾åº¦æ¸å¢ã€ä¿¡æ¯é‡ä¸°å¯Œçš„å­¦ä¹ æ ·æœ¬ï¼Œå¼•å¯¼å…¶æ›´ç¨³å®šåœ°ä¼˜åŒ–ã€‚\n\n### **å››ã€ å¸¦æ¥çš„å®é™…ä»·å€¼ä¸æ•ˆæœ**\n- **æ€§èƒ½æå‡**ï¼šåœ¨InternVLA-N1 S1åŸºå‡†æµ‹è¯•ä¸­ï¼Œ**SR**å’Œ**SPL**æŒ‡æ ‡å…¨é¢è¶…è¶ŠåŸºçº¿æ–¹æ³•ï¼ˆå¦‚NavDPï¼‰ï¼Œå°¤å…¶åœ¨å¤æ‚çš„å•†ä¸šå’Œå®¶åº­åœºæ™¯ä¸­ä¼˜åŠ¿æ˜¾è‘—ï¼ˆSRæå‡çº¦10%ï¼‰ã€‚\n- **æ•ˆç‡é£è·ƒ**ï¼šç”±äºæ¨ç†æ—¶æ‘’å¼ƒäº†é¢å¤–çš„é€‰æ‹©å™¨ï¼Œå¹¶å¾—ç›Šäºåˆ†å¸ƒé›†ä¸­ï¼Œå¯ä»¥ä½¿ç”¨æ›´é«˜æ•ˆçš„**DDIMé‡‡æ ·å™¨**å’Œæ›´å°‘çš„å»å™ªæ­¥æ•°ã€‚åœ¨Jetson Orin Nanoè¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œå®ç°äº†**2.5å€çš„æ¨ç†åŠ é€Ÿ**ï¼ˆä»273msé™è‡³110msï¼‰ï¼Œä¸ºå®æ—¶æœºå™¨äººéƒ¨ç½²é“ºå¹³é“è·¯ã€‚\n- **é²æ£’æ€§éªŒè¯**ï¼šåœ¨ä»¿çœŸåˆ°å®ç‰©çš„è¿ç§»ä¸­ï¼Œåœ¨è½®å¼å’Œå››è¶³æœºå™¨äººå¹³å°ä¸Šå‡å±•ç¤ºäº†æœ‰æ•ˆçš„ã€æ— ç¢°æ’çš„è§†è§‰å¯¼èˆªèƒ½åŠ›ã€‚\n\n**æ€»ç»“**ï¼šSIDPçš„åˆ›æ–°åœ¨äºå°†æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ä¸**åœ¨çº¿ã€è‡ªçœã€å¥–åŠ±é©±åŠ¨**çš„å­¦ä¹ èŒƒå¼ç›¸ç»“åˆã€‚å®ƒä¸å†è¢«åŠ¨æ¨¡ä»¿ä¸“å®¶ï¼Œè€Œæ˜¯ä¸»åŠ¨æ¢ç´¢ç¯å¢ƒå¹¶ä»è‡ªèº«çš„æœ€ä½³å®è·µä¸­å­¦ä¹ ï¼Œä»è€ŒåŒæ—¶æ”»å…‹äº†è§†è§‰å¯¼èˆªä¸­**é²æ£’æ€§**ä¸**å®æ—¶æ€§**ä¸¤å¤§éš¾é¢˜ï¼Œä¸ºå®é™…æœºå™¨äººåº”ç”¨æä¾›äº†ä¸€ä¸ªé«˜æ•ˆã€å¯é çš„ç«¯åˆ°ç«¯è·¯å¾„è§„åˆ’è§£å†³æ–¹æ¡ˆã€‚",
            "timestamp": "2026-02-03T15:54:31.979974",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è§†è§‰å¯¼èˆªä¸­æ‰©æ•£ç­–ç•¥çš„å‘å±•è„‰ç»œåˆ†æ\n\nåŸºäºè®ºæ–‡ã€ŠSelf-Imitated Diffusion Policy for Efficient and Robust Visual Navigationã€‹åŠå…¶å¼•ç”¨çš„ç›¸å…³å·¥ä½œï¼Œå¯ä»¥æ¢³ç†å‡ºè§†è§‰å¯¼èˆªä¸­æ‰©æ•£ç­–ç•¥ï¼ˆDiffusion Policyï¼‰è¿™ä¸€æ–¹å‘çš„å‘å±•è„‰ç»œã€‚å…¶æ¼”è¿›ä¸»è¦æ²¿ç€**ä¸¤ä¸ªæ ¸å¿ƒé©±åŠ¨åŠ›**å±•å¼€ï¼š**1) æå‡è§„åˆ’æ€§èƒ½ï¼ˆæˆåŠŸç‡ã€é²æ£’æ€§ï¼‰** å’Œ **2) æå‡ç³»ç»Ÿæ•ˆç‡ï¼ˆæ¨ç†é€Ÿåº¦ã€éƒ¨ç½²ä¾¿æ·æ€§ï¼‰**ã€‚æ¯ä¸€æ­¥çš„æ”¹è¿›éƒ½æ—¨åœ¨è§£å†³å‰åºå·¥ä½œçš„å…³é”®ç“¶é¢ˆã€‚\n\nä»¥ä¸‹æ˜¯è¯¥æ–¹å‘çš„ä¸»è¦å‘å±•é˜¶æ®µã€ä»£è¡¨æ€§å·¥ä½œåŠå…¶æ ¸å¿ƒæ”¹è¿›ï¼š\n\n### é˜¶æ®µä¸€ï¼šå¥ åŸºä¸å¼•å…¥â€”â€”å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºåºåˆ—å†³ç­–\n*   **ä»£è¡¨å·¥ä½œ**ï¼š**Diffuser** (Janner et al., 2022), **Decision Diffuser** (Ajay et al., 2023)\n*   **æ ¸å¿ƒæ”¹è¿›**ï¼š\n    *   **ä»è‡ªå›å½’æ¨¡å‹åˆ°ç”Ÿæˆå¼å»ºæ¨¡**ï¼šä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚åŸºäºRNN/Transformerçš„è‡ªå›å½’æ¨¡å‹ï¼‰é€æ­¥é¢„æµ‹åŠ¨ä½œï¼Œå®¹æ˜“äº§ç”Ÿè¯¯å·®ç´¯ç§¯ã€‚æ‰©æ•£æ¨¡å‹é¦–æ¬¡è¢«ç”¨äº**ç›´æ¥ç”Ÿæˆå®Œæ•´çš„è½¨è¿¹åºåˆ—**ï¼Œé€šè¿‡è¿­ä»£å»å™ªè¿‡ç¨‹ä¿è¯äº†è½¨è¿¹çš„æ—¶ç©ºä¸€è‡´æ€§ã€‚\n    *   **æ•è·å¤šæ¨¡æ€åˆ†å¸ƒ**ï¼šæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿè‡ªç„¶åœ°å»ºæ¨¡å¤æ‚ç¯å¢ƒä¸‹é€šå¾€ç›®æ ‡çš„**å¤šç§å¯è¡Œè·¯å¾„åˆ†å¸ƒ**ï¼Œä¸ºè§„åˆ’æä¾›äº†ä¸°å¯Œçš„å€™é€‰æ–¹æ¡ˆã€‚\n    *   **çµæ´»çš„æ¡ä»¶æ§åˆ¶**ï¼šé€šè¿‡å°†å›æŠ¥ï¼ˆrewardï¼‰æˆ–çº¦æŸä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆé«˜æ€§èƒ½æˆ–å®‰å…¨çš„è½¨è¿¹ã€‚\n\n### é˜¶æ®µäºŒï¼šåº”ç”¨äºè§†è§‰å¯¼èˆªâ€”â€”è§£å†³å…·ä½“ä»»åŠ¡æŒ‘æˆ˜\n*   **ä»£è¡¨å·¥ä½œ**ï¼š**NoMaD** (Sridhar et al., 2024), **NavDP** (Cai et al., 2025)\n*   **æ ¸å¿ƒæ”¹è¿›**ï¼š\n    *   **ä¸è§†è§‰æ„ŸçŸ¥ç«¯åˆ°ç«¯ç»“åˆ**ï¼šå°†æ‰©æ•£ç­–ç•¥ç›´æ¥åº”ç”¨äºä»¥RGB-Då›¾åƒä¸ºè¾“å…¥çš„è§†è§‰å¯¼èˆªä»»åŠ¡ï¼Œå®ç°äº†ä»æ„ŸçŸ¥åˆ°è§„åˆ’çš„ç«¯åˆ°ç«¯å­¦ä¹ ã€‚\n    *   **åˆ©ç”¨ç‰¹æƒä¿¡æ¯**ï¼šä»¥NavDPä¸ºä¾‹ï¼Œåœ¨ä»¿çœŸè®­ç»ƒä¸­åˆ©ç”¨**ç‰¹æƒä¿¡æ¯**ï¼ˆå¦‚å…¨å±€åœ°å›¾ã€ç²¾ç¡®ä½ç½®ï¼‰æ¥å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œç”Ÿæˆæ›´åˆç†çš„è½¨è¿¹ï¼Œä»¥è§£å†³ä»ä»¿çœŸåˆ°ç°å®ï¼ˆSim-to-Realï¼‰çš„è¿ç§»éš¾é¢˜ã€‚\n    *   **æ¢ç´¢ä¸è§„åˆ’çš„ç»Ÿä¸€**ï¼šå¦‚NoMaDé€šè¿‡ç›®æ ‡æ©ç ï¼ˆgoal maskingï¼‰æŠ€æœ¯ï¼Œä½¿åŒä¸€ä¸ªæ¨¡å‹æ—¢èƒ½è¿›è¡Œç›®æ ‡å¯¼å‘çš„å¯¼èˆªï¼Œä¹Ÿèƒ½è¿›è¡Œæ— ç›®æ ‡æ¢ç´¢ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚\n\n### é˜¶æ®µä¸‰ï¼šä¼˜åŒ–æ¨ç†æµç¨‹â€”â€”åº”å¯¹â€œç”Ÿæˆ-åè¿‡æ»¤â€ç“¶é¢ˆ\n*   **é—®é¢˜å‡¸æ˜¾**ï¼šå‰æœŸæ‰©æ•£ç­–ç•¥åœ¨æ¨ç†æ—¶ï¼Œä¸ºäº†ä»å¤šæ¨¡æ€åˆ†å¸ƒä¸­é€‰å‡ºä¸€ä¸ªé«˜è´¨é‡è½¨è¿¹ï¼Œæ™®éé‡‡ç”¨ **â€œç”Ÿæˆ-åè¿‡æ»¤â€** èŒƒå¼ã€‚å³å…ˆé€šè¿‡éšæœºé‡‡æ ·ç”Ÿæˆå¤§é‡ï¼ˆå¦‚16ä¸ªï¼‰è½¨è¿¹å€™é€‰ï¼Œå†ä¾èµ–ä¸€ä¸ª**é¢å¤–çš„é€‰æ‹©å™¨**ï¼ˆauxiliary selectorï¼Œå¯ä»¥æ˜¯åŸºäºè§„åˆ™çš„æˆ–å­¦ä¹ çš„ï¼‰æ¥ç­›é€‰æœ€ä¼˜è½¨è¿¹ã€‚\n*   **å¸¦æ¥çš„æŒ‘æˆ˜**ï¼š\n    1.  **è®¡ç®—å¼€é”€å¤§**ï¼šå¤šæ¬¡é‡‡æ ·å’Œé¢å¤–çš„é€‰æ‹©ç½‘ç»œæ˜¾è‘—å¢åŠ äº†æ¨ç†å»¶è¿Ÿï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚æœºå™¨äººåµŒå…¥å¼å¹³å°ï¼‰ä¸Šå®æ—¶è¿è¡Œã€‚\n    2.  **æ¶æ„å¤æ‚**ï¼šéœ€è¦è®­ç»ƒå’Œç»´æŠ¤ç­–ç•¥ç½‘ç»œå’Œé€‰æ‹©å™¨ç½‘ç»œä¸¤å¥—ç³»ç»Ÿã€‚\n    3.  **è´¨é‡ä¸ç¨³å®š**ï¼šç­–ç•¥æœ¬èº«å¯èƒ½ç”Ÿæˆå¤§é‡ä½è´¨é‡ï¼ˆç¢°æ’ã€ç»•è¿œï¼‰çš„è½¨è¿¹ï¼Œå®Œå…¨ä¾èµ–åç½®è¿‡æ»¤æ¥ä¿è¯å®‰å…¨ä¸æ•ˆç‡ã€‚\n\n### é˜¶æ®µå››ï¼šæ ¹æœ¬æ€§èŒƒå¼åˆ›æ–°â€”â€”ä»â€œæ¨¡ä»¿ä¸“å®¶â€åˆ°â€œè‡ªæˆ‘è¿›åŒ–â€\nè¿™æ˜¯æœ¬æ–‡**SIDP**å·¥ä½œæ‰€å¤„çš„é˜¶æ®µï¼Œå®ƒä»£è¡¨äº†å½“å‰ä¸€ä¸ªé‡è¦çš„å‰æ²¿å‘å±•æ–¹å‘ã€‚\n\n*   **ä»£è¡¨å·¥ä½œ**ï¼š**Self-Imitated Diffusion Policy (SIDP)** (æœ¬æ–‡)\n*   **æ ¸å¿ƒæ”¹è¿›ï¼ˆé’ˆå¯¹å‰è¿°æ‰€æœ‰ç“¶é¢ˆï¼‰**ï¼š\n    *   **è®­ç»ƒèŒƒå¼é©å‘½ï¼šä»æ¨¡ä»¿å­¦ä¹ åˆ°è‡ªæ¨¡ä»¿å­¦ä¹ **\n        *   **ä¹‹å‰**ï¼šNavDPç­‰ä¾èµ–**æ¨¡ä»¿å­¦ä¹ **ï¼Œå…¶æ€§èƒ½ä¸Šé™å—é™äºä¸“å®¶æ•°æ®é›†çš„è´¨é‡ã€è¦†ç›–åº¦å’Œå¯èƒ½å­˜åœ¨çš„æ¬¡ä¼˜æ€§ã€‚\n        *   **SIDP**ï¼šæå‡º**å¥–åŠ±å¼•å¯¼çš„è‡ªæ¨¡ä»¿**æœºåˆ¶ã€‚ç­–ç•¥ä¸å†æ¨¡ä»¿å›ºå®šæ•°æ®é›†ï¼Œè€Œæ˜¯ä»**è‡ªèº«å½“å‰ç­–ç•¥é‡‡æ ·**çš„è½¨è¿¹ä¸­ï¼Œæ ¹æ®å¥–åŠ±å‡½æ•°ç­›é€‰å‡ºé«˜è´¨é‡éƒ¨åˆ†è¿›è¡Œæ¨¡ä»¿ã€‚è¿™å½¢æˆäº†ä¸€ä¸ª**è‡ªæˆ‘è¿­ä»£ä¼˜åŒ–**çš„é—­ç¯ï¼Œä½¿ç­–ç•¥èƒ½ä¸æ–­è¶…è¶Šåˆå§‹æ•°æ®ï¼Œå­¦ä¹ åˆ°æ›´ä¼˜çš„è¡Œä¸ºã€‚\n    *   **å®ç°åˆ†å¸ƒé›†ä¸­ï¼Œå½»åº•æ‘’å¼ƒé€‰æ‹©å™¨**\n        *   **ä¹‹å‰**ï¼šç­–ç•¥è¾“å‡ºåˆ†å¸ƒå¹¿æ³›ä¸”è´¨é‡ä¸å‡ï¼Œå¿…é¡»åè¿‡æ»¤ã€‚\n        *   **SIDP**ï¼šé€šè¿‡æŒç»­æ¨¡ä»¿è‡ªèº«çš„é«˜å¥–åŠ±è½¨è¿¹ï¼Œç­–ç•¥çš„è½¨è¿¹è¾“å‡ºåˆ†å¸ƒä¼šè‡ªç„¶åœ°å‘**é«˜è´¨é‡åŒºåŸŸé›†ä¸­**ï¼ˆä¿æŒæ¨¡æ€é—´å¤šæ ·æ€§ï¼Œä½†æå‡æ¨¡æ€å†…è´¨é‡ï¼‰ã€‚å› æ­¤ï¼Œåœ¨æ¨ç†æ—¶**åªéœ€åšä¸€æ¬¡ç¡®å®šæ€§é‡‡æ ·**å³å¯å¾—åˆ°å¯é è½¨è¿¹ï¼Œç§»é™¤äº†æ•´ä¸ªé€‰æ‹©å™¨æ¨¡å—ï¼Œå®ç°äº†çœŸæ­£çš„ç«¯åˆ°ç«¯è§„åˆ’ã€‚\n    *   **å¤§å¹…æå‡æ¨ç†æ•ˆç‡**\n        *   ç”±äºåˆ†å¸ƒé›†ä¸­ä¸”è´¨é‡é«˜ï¼ŒSIDPå¯ä»¥å®‰å…¨åœ°ä½¿ç”¨**DDIM**ç­‰ç¡®å®šæ€§é‡‡æ ·ç®—æ³•ï¼Œå¹¶**å¤§å¹…å‡å°‘å»å™ªæ­¥æ•°**ï¼ˆä»10æ­¥å‡è‡³5æ­¥ç”šè‡³3æ­¥ï¼‰ã€‚è¿™æ˜¯å…¶èƒ½åœ¨Jetson Orin Nanoä¸Šå®ç°**2.5å€åŠ é€Ÿ**ï¼ˆ110ms vs 273msï¼‰çš„æ ¹æœ¬åŸå› ã€‚\n    *   **å¢å¼ºé²æ£’æ€§ä¸æ³›åŒ–æ€§**\n        *   é€šè¿‡**ç›®æ ‡æ— å…³æ¢ç´¢**ç­–ç•¥ï¼Œè®©ç­–ç•¥åœ¨è®­ç»ƒä¸­å­¦ä¹ ä¸ä¾èµ–ç‰¹å®šç›®æ ‡çš„æ¢ç´¢è¡Œä¸ºï¼Œå¢åŠ äº†è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ï¼Œèµ·åˆ°äº†æ­£åˆ™åŒ–ä½œç”¨ï¼Œæå‡äº†åœ¨é™Œç”Ÿç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚\n        *   é€šè¿‡**å¥–åŠ±é©±åŠ¨çš„è¯¾ç¨‹å­¦ä¹ **ï¼ŒåŠ¨æ€é€‰æ‹©å¯¹å½“å‰ç­–ç•¥å­¦ä¹ æ½œåŠ›æœ€å¤§çš„è®­ç»ƒåœºæ™¯ï¼Œæå‡äº†æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼ŒåŠ é€Ÿäº†æ”¶æ•›å¹¶é¿å…äº†å±€éƒ¨æœ€ä¼˜ã€‚\n\n### å…¶ä»–å¹¶è¡Œå‘å±•æ–¹å‘\nè®ºæ–‡ä¸­ä¹Ÿæåˆ°äº†å…¶ä»–ä¸€äº›å¹¶è¡Œçš„æŠ€æœ¯æ¼”è¿›è·¯å¾„ï¼š\n\n1.  **ä¸ç»å…¸è§„åˆ’å™¨èåˆ**ï¼š\n    *   **ä»£è¡¨**ï¼šLDP (Yu et al., 2024), Motion Planning Diffusion (Carvalho et al., 2024)\n    *   **æ”¹è¿›**ï¼šå°†æ‰©æ•£æ¨¡å‹ä½œä¸ºå±€éƒ¨è§„åˆ’å™¨æˆ–è½¨è¿¹ä¼˜åŒ–å™¨ï¼ŒåµŒå…¥åˆ°åŸºäºåœ°å›¾çš„ç»å…¸è§„åˆ’æ¡†æ¶ï¼ˆå¦‚A*ã€RRT*ï¼‰ä¸­ã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¤„ç†å¤æ‚çº¦æŸå’Œç”Ÿæˆå¹³æ»‘è½¨è¿¹çš„èƒ½åŠ›ï¼Œå¼¥è¡¥ä¼ ç»Ÿé‡‡æ ·æˆ–æœç´¢æ–¹æ³•åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„ä¸è¶³ã€‚\n\n2.  **é‡‡æ ·è¿‡ç¨‹å¼•å¯¼**ï¼š\n    *   **ä»£è¡¨**ï¼šNaviDiffusor (Zeng et al., 2025), CGD (Kondo et al., 2024)\n    *   **æ”¹è¿›**ï¼šåœ¨æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·ï¼ˆå»å™ªï¼‰è¿‡ç¨‹ä¸­ï¼Œå®æ—¶æ³¨å…¥åŸºäºæˆæœ¬å‡½æ•°ã€å®‰å…¨çº¦æŸæˆ–æ¢ç´¢æ¿€åŠ±çš„**æ¢¯åº¦å¼•å¯¼**ï¼Œä»è€Œåœ¨ä¸é‡æ–°è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå¯¹ç”Ÿæˆçš„è½¨è¿¹è¿›è¡Œå®æ—¶å¾®è°ƒï¼Œä½¿å…¶æ»¡è¶³ç‰¹å®šä»»åŠ¡éœ€æ±‚ã€‚\n\n### æ€»ç»“è„‰ç»œå›¾\n```mermaid\ngraph TD\n    A[å¥ åŸº: æ‰©æ•£æ¨¡å‹ç”¨äºå†³ç­–<br>Diffuser/Decision Diffuser] --> B[åº”ç”¨: è§†è§‰å¯¼èˆªç«¯åˆ°ç«¯åŒ–<br>NoMaD/NavDP]ï¼›\n    B --> C[é—®é¢˜å‡¸æ˜¾: â€œç”Ÿæˆ-åè¿‡æ»¤â€èŒƒå¼<br>æ¨ç†æ…¢ã€éœ€é€‰æ‹©å™¨]ï¼›\n    C --> D1[èŒƒå¼åˆ›æ–°: è‡ªæ¨¡ä»¿å­¦ä¹ <br>**SIDP (æœ¬æ–‡)**]ï¼›\n    C --> D2[è·¯å¾„èåˆ: ä¸ç»å…¸è§„åˆ’ç»“åˆ<br>LDP]ï¼›\n    C --> D3[è¿‡ç¨‹ä¼˜åŒ–: å¼•å¯¼é‡‡æ ·<br>NaviDiffusor]ï¼›\n\n    D1 --> E1[æ ¸å¿ƒè´¡çŒ®: <br>1. è‡ªæˆ‘è¿­ä»£ä¼˜åŒ–ï¼Œè¶…è¶Šä¸“å®¶æ•°æ®<br>2. åˆ†å¸ƒé›†ä¸­ï¼Œ**å®ç°å•æ¬¡é‡‡æ ·**<br>3. **æ¨ç†æ•ˆç‡å¤§å¹…æå‡**<br>4. é²æ£’æ€§ä¸æ³›åŒ–æ€§å¢å¼º]ï¼›\n    D2 --> E2[æ ¸å¿ƒè´¡çŒ®: <br>ç»“åˆæ‰©æ•£çš„çµæ´»æ€§ä¸<br>ç»å…¸è§„åˆ’çš„å…¨å±€æ€§/å¯é æ€§]ï¼›\n    D3 --> E3[æ ¸å¿ƒè´¡çŒ®: <br>åœ¨æ¨ç†æ—¶åŠ¨æ€è°ƒæ•´è½¨è¿¹ï¼Œ<br>æ»¡è¶³å³æ—¶çº¦æŸ]ï¼›\n```\n\n**ç»“è®º**ï¼šè§†è§‰å¯¼èˆªä¸­çš„æ‰©æ•£ç­–ç•¥ç ”ç©¶ï¼Œæ­£ä»æœ€åˆçš„â€œè¯æ˜æ¦‚å¿µå¯è¡Œâ€ï¼Œå¿«é€Ÿæ¼”è¿›åˆ°â€œè§£å†³å®é™…éƒ¨ç½²éš¾é¢˜â€ã€‚**SIDP**ä»£è¡¨äº†ä¸€ç§æ ¹æœ¬æ€§çš„æ€è·¯è½¬å˜â€”â€”é€šè¿‡æ”¹å˜è®­ç»ƒèŒƒå¼ï¼ˆè‡ªæ¨¡ä»¿ï¼‰æ¥ç›´æ¥ä¼˜åŒ–ç­–ç•¥çš„å†…åœ¨è´¨é‡ï¼Œä»è€Œä¸€åŠ³æ°¸é€¸åœ°è§£å†³äº†æ¨ç†æ•ˆç‡çš„ç“¶é¢ˆã€‚ä¸æ­¤åŒæ—¶ï¼Œä¸å…¶ä»–é¢†åŸŸï¼ˆç»å…¸è§„åˆ’ã€çº¦æŸä¼˜åŒ–ï¼‰çš„äº¤å‰èåˆï¼Œä¹Ÿä¸ºæ‰©æ•£è§„åˆ’å™¨æä¾›äº†æ›´å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œæ›´å¼ºçš„å®ç”¨æ€§ã€‚æœªæ¥çš„å‘å±•å¾ˆå¯èƒ½ç»§ç»­å›´ç»•**æ•ˆç‡ä¸é²æ£’æ€§çš„æè‡´å¹³è¡¡**ã€**æ›´å°‘çš„æ•°æ®ä¾èµ–**ä»¥åŠ**åœ¨å¤æ‚ç‰©ç†ä¸–ç•Œä¸­çš„å¯é éƒ¨ç½²**å±•å¼€ã€‚",
            "timestamp": "2026-02-03T15:55:37.701234",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œä½œè€…å›¢é˜Ÿï¼ˆç‰¹åˆ«æ˜¯é€šè®¯ä½œè€… Changxu Chengï¼‰åŠå…¶åˆä½œè€…åœ¨è§†è§‰å¯¼èˆªä¸æ‰©æ•£ç­–ç•¥é¢†åŸŸæœ‰ä¸€ç³»åˆ—ç›¸å…³ç ”ç©¶ã€‚ä»¥ä¸‹è¡¨æ ¼æ¢³ç†äº†å…¶æ ¸å¿ƒå‰ä½œï¼Œå¹¶é‡ç‚¹å¯¹æ¯”äº†æœ¬æ–‡ï¼ˆSIDPï¼‰ä¸å®ƒä»¬çš„åŒºåˆ«å’Œæ”¹è¿›ã€‚\n\n| å‰ä½œ/ç›¸å…³è®ºæ–‡ | æ ¸å¿ƒæ–¹æ³•/è´¡çŒ® | ä¸æœ¬æ–‡ï¼ˆSIDPï¼‰çš„ä¸»è¦åŒºåˆ« | æœ¬æ–‡ï¼ˆSIDPï¼‰çš„ä¸»è¦æ”¹è¿›ä¸åˆ›æ–° |\n| :--- | :--- | :--- | :--- |\n| **NavDP [4]** (Cai et al., 2025) | **åŸºäºæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰çš„æ‰©æ•£å¯¼èˆªç­–ç•¥**ã€‚åˆ©ç”¨ä»¿çœŸä¸­çš„ç‰¹æƒä¿¡æ¯ï¼ˆå¦‚å…¨å±€åœ°å›¾ã€ESDFï¼‰æ¥å¼•å¯¼æ‰©æ•£æ¨¡å‹è¿›è¡Œè·¯å¾„è§„åˆ’ã€‚è®­ç»ƒä¾èµ–äºä¸“å®¶æ¼”ç¤ºï¼Œæ¨ç†æ—¶é‡‡ç”¨ **â€œç”Ÿæˆ-åè¿‡æ»¤â€** èŒƒå¼ï¼Œéœ€è¦ä¸€ä¸ªé¢å¤–çš„é€‰æ‹©å™¨ï¼ˆå¦‚criticç½‘ç»œï¼‰ä»å¤šä¸ªé‡‡æ ·è½¨è¿¹ä¸­ç­›é€‰æœ€ä½³è·¯å¾„ã€‚ | 1. **è®­ç»ƒèŒƒå¼**ï¼šNavDPå®Œå…¨ä¾èµ–**å¤–éƒ¨ä¸“å®¶æ¼”ç¤º**è¿›è¡Œæ¨¡ä»¿å­¦ä¹ ã€‚<br>2. **æ¨ç†æµç¨‹**ï¼šéœ€è¦**å¯†é›†é‡‡æ ·**ï¼ˆå¦‚å¤šæ¬¡è¿è¡Œæ‰©æ•£è¿‡ç¨‹ï¼‰å’Œ**åå¤„ç†é€‰æ‹©å™¨**ï¼Œæµç¨‹å¤æ‚ã€å»¶è¿Ÿé«˜ã€‚<br>3. **ç­–ç•¥è´¨é‡**ï¼šç»§æ‰¿äº†ä¸“å®¶æ•°æ®ä¸­çš„æ¬¡ä¼˜æ€§å’Œå†—ä½™ï¼Œè½¨è¿¹è´¨é‡ä¸ä¸€è‡´ï¼Œæ³›åŒ–èƒ½åŠ›å—é™äºæ•°æ®é›†åˆ†å¸ƒã€‚ | 1. **è®­ç»ƒèŒƒå¼é©æ–°**ï¼šæå‡º**å¥–åŠ±å¼•å¯¼çš„è‡ªæ¨¡ä»¿ï¼ˆSelf-Imitationï¼‰**ã€‚ç­–ç•¥ä»**è‡ªèº«ç”Ÿæˆ**çš„è½¨è¿¹ä¸­å­¦ä¹ ï¼Œè€Œéå›ºå®šä¸“å®¶æ•°æ®ã€‚é€šè¿‡å¥–åŠ±å¯¹è½¨è¿¹é‡åŠ æƒï¼Œä¼˜å…ˆå­¦ä¹ é«˜è´¨é‡ç»éªŒï¼Œå®ç°äº†**ç­–ç•¥çš„è¿­ä»£è‡ªæˆ‘ç²¾ç‚¼**ã€‚<br>2. **æ¨ç†æµç¨‹ç®€åŒ–**ï¼šç”±äºè‡ªæ¨¡ä»¿ä¿ƒä½¿ç­–ç•¥è¾“å‡º**åˆ†å¸ƒé›†ä¸­**çš„é«˜è´¨é‡è½¨è¿¹ï¼Œ**æ— éœ€**é¢å¤–çš„é€‰æ‹©å™¨å’Œå¯†é›†é‡‡æ ·ã€‚æ”¯æŒä½¿ç”¨æ›´é«˜æ•ˆçš„DDIMé‡‡æ ·å™¨ï¼Œå®ç°äº†**ç«¯åˆ°ç«¯çš„å•æ¬¡æ¨ç†**ã€‚<br>3. **æ€§èƒ½ä¸æ•ˆç‡**ï¼šåœ¨å¤æ‚åœºæ™¯ï¼ˆå¦‚Commercialï¼‰ä¸Š**æ³›åŒ–èƒ½åŠ›æ˜¾è‘—æ›´å¼º**ï¼ˆSRæå‡çº¦10%ï¼‰ã€‚åœ¨Jetson Orin Nanoä¸Šæ¨ç†é€Ÿåº¦**æå‡2.5å€**ï¼ˆ110ms vs 273msï¼‰ã€‚ |\n| **NoMaD [30]** (Sridhar et al., 2024) | **ç›®æ ‡æ©ç æ‰©æ•£ç­–ç•¥**ã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œ**æ— ç›®æ ‡æ¢ç´¢**å’Œ**ç›®æ ‡æ¡ä»¶å¯¼èˆª**ã€‚é€šè¿‡æ©ç ç›®æ ‡ä¿¡æ¯æ¥è®­ç»ƒä¸€ä¸ªé€šç”¨çš„è½¨è¿¹ç”Ÿæˆå™¨ï¼Œå…·å¤‡è¾ƒå¼ºçš„æ¢ç´¢èƒ½åŠ›ã€‚ | 1. **æ ¸å¿ƒç›®æ ‡**ï¼šNoMaDä¾§é‡äº**æ¢ç´¢ï¼ˆexplorationï¼‰** ä¸å¯¼èˆªçš„é€šç”¨è¡¨ç¤ºå­¦ä¹ ã€‚<br>2. **è®­ç»ƒæ–¹å¼**ï¼šè™½ä¹Ÿæ¶‰åŠä»ç­–ç•¥è‡ªèº«é‡‡æ ·ï¼Œä½†å…¶è®­ç»ƒæ›´ä¾§é‡äºé‡å»ºæŸå¤±å’Œç›®æ ‡ä»»åŠ¡ï¼ˆå¦‚ç›®æ ‡åˆ°è¾¾ï¼‰çš„æ¡ä»¶ç”Ÿæˆï¼Œ**æœªæ˜ç¡®å¼•å…¥å¥–åŠ±ä¿¡å·è¿›è¡Œè½¨è¿¹è´¨é‡çš„è¿­ä»£ç­›é€‰å’Œä¼˜åŒ–**ã€‚<br>3. **æ•ˆç‡é—®é¢˜**ï¼šåŒæ ·é¢ä¸´æ‰©æ•£æ¨¡å‹æ¨ç†æ—¶é‡‡æ ·æ•ˆç‡çš„é—®é¢˜ï¼Œæœªä¸“é—¨é’ˆå¯¹**å®æ—¶éƒ¨ç½²çš„å»¶è¿Ÿ**è¿›è¡Œä¼˜åŒ–ã€‚ | 1. **ä¼˜åŒ–ç›®æ ‡æ˜ç¡®**ï¼šSIDPæ˜ç¡®ä»¥**æœ€å¤§åŒ–è½¨è¿¹å¥–åŠ±**ï¼ˆå®‰å…¨ã€é«˜æ•ˆï¼‰ä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œé€šè¿‡**åˆ†å¸ƒåŒ¹é…**çš„æ•°å­¦æ¡†æ¶å°†å¼ºåŒ–å­¦ä¹ ç›®æ ‡è½¬åŒ–ä¸ºç¨³å®šçš„è‡ªæ¨¡ä»¿å­¦ä¹ ã€‚<br>2. **ä¸“ä¸ºé«˜æ•ˆé²æ£’å¯¼èˆªè®¾è®¡**ï¼šå¼•å…¥äº†**ç›®æ ‡æ— å…³æ¢ç´¢**å’Œ**å¥–åŠ±é©±åŠ¨çš„è¯¾ç¨‹å­¦ä¹ **ä½œä¸º**äº’è¡¥ç­–ç•¥**ï¼Œä¸“é—¨ç”¨äº**æ­£åˆ™åŒ–**ç‚¹ç›®æ ‡å¯¼èˆªå’Œæå‡æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼Œè¿™æ˜¯NoMaDæ‰€ä¸å…·å¤‡çš„ã€‚<br>3. **éƒ¨ç½²å¯¼å‘ä¼˜åŒ–**ï¼šé€šè¿‡åˆ†å¸ƒé›†ä¸­ï¼Œä½¿å¾—**å°‘æ­¥æ•°ç¡®å®šæ€§é‡‡æ ·ï¼ˆDDIMï¼‰å˜å¾—å¯è¡Œä¸”é«˜æ•ˆ**ï¼Œç›´æ¥é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²çš„å»¶è¿Ÿç“¶é¢ˆè¿›è¡Œä¼˜åŒ–ã€‚ |\n| **ç»å…¸æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰ä¸åŸºäºRLçš„æ‰©æ•£æ–¹æ³•** (å¦‚ [32, 24]) | - **ILæ–¹æ³•**ï¼šç›´æ¥æ¨¡ä»¿ä¸“å®¶è½¨è¿¹ï¼Œæ˜“å—æ•°æ®åˆ†å¸ƒåç§»å½±å“ã€‚<br>- **RLè°ƒä¼˜æ‰©æ•£æ–¹æ³•**ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç›´æ¥ä¼˜åŒ–æ‰©æ•£ç­–ç•¥çš„å¥–åŠ±ã€‚é€šå¸¸éœ€è¦**é€šè¿‡æ—¶é—´åå‘ä¼ æ’­ï¼ˆBPTTï¼‰** åœ¨æ•´ä¸ªå»å™ªåºåˆ—ä¸Šè¿›è¡Œåå‘ä¼ æ’­ï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”æ•°å€¼ä¸ç¨³å®šã€‚ | 1. **æ•°æ®ä¾èµ–**ï¼šç»å…¸ILå—é™äºä¸“å®¶æ•°æ®è´¨é‡å’Œè¦†ç›–åº¦ã€‚<br>2. **è®­ç»ƒå¤æ‚åº¦**ï¼šRLè°ƒä¼˜æ‰©æ•£æ–¹æ³•è®­ç»ƒéå¸¸å¤æ‚ã€è®¡ç®—ä»£ä»·å¤§ã€‚ | 1. **æ‘†è„±æ•°æ®ä¾èµ–**ï¼šé€šè¿‡è‡ªæ¨¡ä»¿ä»**åœ¨çº¿äº¤äº’**ä¸­å­¦ä¹ ï¼Œèƒ½è¶…è¶Šå›ºå®šä¸“å®¶æ•°æ®é›†çš„åå·®ï¼Œé€‚åº”æ–°åœºæ™¯ã€‚<br>2. **è®­ç»ƒç¨³å®šé«˜æ•ˆ**ï¼šå°†å¤æ‚çš„RLä¼˜åŒ–é—®é¢˜**é‡æ„ä¸ºè¿­ä»£çš„è‡ªæ¨¡ä»¿å­¦ä¹ ç›®æ ‡**ï¼ˆå…¬å¼5, 8ï¼‰ï¼Œé¿å…äº†ç¹ççš„BPTTï¼Œå®ç°äº†**ç¨³å®šçš„ç­–ç•¥æ¢¯åº¦æ›´æ–°**ï¼Œå…¼é¡¾äº†RLçš„ä¼˜åŒ–ç›®æ ‡å’ŒILçš„è®­ç»ƒç¨³å®šæ€§ã€‚ |\n\n## æ€»ç»“ï¼šæœ¬æ–‡ç›¸å¯¹äºå‰ä½œçš„æ ¸å¿ƒæ”¹åŠ¨ä¸æ”¹å–„\n\n1.  **è®­ç»ƒæœºåˆ¶çš„æ ¹æœ¬æ€§æ”¹å˜**ï¼šä» **â€œæ¨¡ä»¿ä»–äººï¼ˆä¸“å®¶ï¼‰â€** è½¬å˜ä¸º **â€œæ¨¡ä»¿è‡ªæˆ‘ï¼ˆæœ€ä¼˜ç»éªŒï¼‰â€**ã€‚è¿™æ˜¯æœ€æ ¸å¿ƒçš„èŒƒå¼è½¬å˜ã€‚é€šè¿‡å¥–åŠ±å‡½æ•°å¯¹è‡ªèº«ç”Ÿæˆçš„è½¨è¿¹è¿›è¡Œé‡è¦æ€§åŠ æƒï¼ˆå…¬å¼7ï¼‰ï¼Œä½¿ç­–ç•¥æŒç»­å‘é«˜æ€§èƒ½åŒºåŸŸè¿›åŒ–ï¼Œè§£å†³äº†ä¸“å®¶æ•°æ®æ¬¡ä¼˜å’Œè¦†ç›–ä¸è¶³çš„é—®é¢˜ã€‚\n\n2.  **æ¨ç†æµç¨‹çš„æ˜¾è‘—ç®€åŒ–**ï¼š**ç§»é™¤äº†â€œç”Ÿæˆ-åè¿‡æ»¤â€ç®¡é“åŠå…¶ä¾èµ–çš„è¾…åŠ©é€‰æ‹©å™¨**ã€‚ç”±äºè‡ªæ¨¡ä»¿å­¦ä¹ ä½¿ç­–ç•¥è¾“å‡ºçš„è½¨è¿¹åˆ†å¸ƒæ›´é›†ä¸­ã€è´¨é‡æ›´é«˜ä¸”ä¸€è‡´ï¼Œå› æ­¤æ— éœ€åœ¨æ¨ç†æ—¶ç”Ÿæˆå¤§é‡å€™é€‰è½¨è¿¹å†è¿›è¡Œç­›é€‰ã€‚è¿™ç›´æ¥å¸¦æ¥äº†**å»¶è¿Ÿçš„å¤§å¹…é™ä½**å’Œéƒ¨ç½²å¤æ‚æ€§çš„å‡å°‘ã€‚\n\n3.  **å¼•å…¥ç³»ç»ŸåŒ–çš„è®­ç»ƒå¢å¼ºç­–ç•¥**ï¼š\n    *   **ç›®æ ‡æ— å…³æ¢ç´¢**ï¼šåœ¨è®­ç»ƒä¸­æ··åˆæ— æŒ‡å®šç›®æ ‡çš„æ¢ç´¢ä»»åŠ¡ï¼Œå¢åŠ äº†è½¨è¿¹æ•°æ®çš„å¤šæ ·æ€§ï¼Œèµ·åˆ°äº†æ­£åˆ™åŒ–ä½œç”¨ï¼Œæå‡äº†ç­–ç•¥åœ¨ç‚¹ç›®æ ‡å¯¼èˆªä¸­çš„**å®‰å…¨æ€§å’Œæ³›åŒ–èƒ½åŠ›**ï¼ˆå›¾5ï¼‰ã€‚\n    *   **å¥–åŠ±é©±åŠ¨è¯¾ç¨‹å­¦ä¹ **ï¼šåŠ¨æ€é€‰æ‹©å…·æœ‰é«˜å­¦ä¹ æ½œåŠ›ï¼ˆé«˜æœ€å¤§å¥–åŠ±å’Œé«˜å¥–åŠ±æ–¹å·®ï¼‰çš„åœºæ™¯è¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼ŒåŠ é€Ÿäº†æ”¶æ•›ï¼Œå¹¶é¿å…äº†æ¨¡å‹é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼ˆå›¾6ï¼‰ã€‚\n\n4.  **å®ç°äº†æ€§èƒ½ä¸æ•ˆç‡çš„ååŒæå‡**ï¼š\n    *   **æ›´å¼ºé²æ£’æ€§**ï¼šåœ¨é€¼çœŸã€å¤æ‚çš„ä»¿çœŸåœºæ™¯ï¼ˆInternScenesï¼‰ä¸­ï¼ŒæˆåŠŸç‡ï¼ˆSRï¼‰ç›¸æ¯”æœ€å¼ºåŸºçº¿NavDPæœ‰æ˜¾è‘—æå‡ï¼ˆ~10%ï¼‰ï¼Œè¯æ˜äº†å…¶æ›´å¥½çš„**åˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›**ã€‚\n    *   **æ›´é«˜æ•ˆç‡**ï¼šå¾—ç›Šäºåˆ†å¸ƒé›†ä¸­ï¼Œå¯ä»¥é‡‡ç”¨DDIMç­‰å¿«é€Ÿé‡‡æ ·å™¨å¹¶å‡å°‘å»å™ªæ­¥æ•°ï¼ˆå¦‚5æ­¥ï¼‰ï¼Œåœ¨åµŒå…¥å¼è®¾å¤‡ä¸Šå®ç°äº†**2.5å€çš„æ¨ç†åŠ é€Ÿ**ï¼Œä¸”æ€§èƒ½ä¸é™åå‡ï¼ŒçœŸæ­£æ»¡è¶³äº†**å®æ—¶æœºå™¨äººéƒ¨ç½²**çš„éœ€æ±‚ã€‚\n\n**æ€»è€Œè¨€ä¹‹**ï¼Œæœ¬æ–‡ï¼ˆSIDPï¼‰å¹¶éå¯¹å‰ä½œNavDPç­‰æ¨¡å‹çš„ç®€å•æ¶æ„ä¿®æ”¹ï¼Œè€Œæ˜¯æå‡ºäº†ä¸€å¥—**ä»è®­ç»ƒåˆ°æ¨ç†çš„å®Œæ•´æ–°æ¡†æ¶**ã€‚å®ƒå·§å¦™åœ°å°†å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–ç›®æ ‡ä¸æ¨¡ä»¿å­¦ä¹ çš„ç¨³å®šè®­ç»ƒç›¸ç»“åˆï¼Œå¹¶é€šè¿‡è‡ªæ¨¡ä»¿æœºåˆ¶å†…ç”Ÿåœ°æå‡äº†ç­–ç•¥çš„è´¨é‡å’Œä¸€è‡´æ€§ï¼Œä»è€Œä¸€ä¸¾è§£å†³äº†ä¼ ç»Ÿæ‰©æ•£å¯¼èˆªç­–ç•¥åœ¨**æ³›åŒ–æ€§ã€æ¨ç†å»¶è¿Ÿå’Œéƒ¨ç½²å¤æ‚æ€§**æ–¹é¢çš„å…³é”®ç“¶é¢ˆã€‚",
            "timestamp": "2026-02-03T15:56:32.461463",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨è¯¦ç»†åˆ†æã€ŠSelf-Imitated Diffusion Policy for Efficient and Robust Visual Navigationã€‹è¿™ç¯‡è®ºæ–‡æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæ–¹æ³•åˆ†æ\n\næœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæå‡ºäº† **è‡ªæ¨¡ä»¿æ‰©æ•£ç­–ç•¥ï¼ˆSelf-Imitated Diffusion Policy, SIDPï¼‰** æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰è®­ç»ƒæ‰©æ•£ç­–ç•¥æ—¶å­˜åœ¨çš„**æ¬¡ä¼˜æ€§ã€å†—ä½™æ€§**ä»¥åŠç”±æ­¤å¯¼è‡´çš„**æ¨ç†æ•ˆç‡ä½ä¸‹**é—®é¢˜ã€‚å…¶å…³é”®æŠ€æœ¯æ–¹æ³•å¯å½’çº³ä¸ºä»¥ä¸‹ä¸‰ä¸ªç›¸äº’å…³è”çš„éƒ¨åˆ†ï¼š\n\n### 1. å¥–åŠ±å¼•å¯¼çš„è‡ªæ¨¡ä»¿æœºåˆ¶ (Reward-Guided Self-Imitation)\n\nè¿™æ˜¯SIDPæ¡†æ¶çš„æ ¸å¿ƒï¼Œæ—¨åœ¨é€šè¿‡**è‡ªæˆ‘ç”Ÿæˆå¹¶ç­›é€‰é«˜è´¨é‡è½¨è¿¹**æ¥è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼Œæ›¿ä»£å¯¹å›ºå®šä¸“å®¶æ•°æ®çš„æ¨¡ä»¿ã€‚\n\n| æŠ€æœ¯ç»„ä»¶ | å…·ä½“ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…³é”®å‚æ•°ä¸è®¾ç½® |\n| :--- | :--- | :--- |\n| **é—®é¢˜é‡è¿°ä¸ºåˆ†å¸ƒåŒ¹é…** | å°†ç­–ç•¥ä¼˜åŒ–ç›®æ ‡ï¼ˆæœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ï¼‰è½¬åŒ–ä¸ºä½¿ç­–ç•¥åˆ†å¸ƒ \\(\\pi_\\theta(a\\|s_t)\\) é€¼è¿‘æœ€ä¼˜è½¨è¿¹åˆ†å¸ƒ \\(p^*(a\\|s_t)\\) çš„é—®é¢˜ã€‚é€šè¿‡æœ€å°åŒ–KLæ•£åº¦ï¼š<br> \\(\\arg\\min_\\theta D_{KL}(p^* \\| \\pi_\\theta) = \\arg\\max_\\theta \\mathbb{E}_{a \\sim p^*} [\\log \\pi_\\theta(a\\|s_t)]\\) <br> ç”±äº \\(p^*\\) æœªçŸ¥ï¼Œä½¿ç”¨å½“å‰ç­–ç•¥ \\(\\pi_\\theta\\) ä½œä¸ºæè®®åˆ†å¸ƒè¿›è¡Œé‡è¦æ€§é‡‡æ ·ã€‚ | - |\n| **é‡è¦æ€§æƒé‡æ¨å¯¼** | åŸºäº**ç›¸å¯¹ç†µç­–ç•¥æœç´¢ï¼ˆREPSï¼‰**æ¡†æ¶ï¼Œåœ¨KLæ•£åº¦ä¿¡ä»»åŸŸçº¦æŸä¸‹ï¼Œæœ€å¤§åŒ–æœŸæœ›å¥–åŠ±çš„æœ€ä¼˜åˆ†å¸ƒå…·æœ‰è§£æå½¢å¼ï¼š<br> \\(p^*(a\\|s_t) \\propto \\pi_\\theta(a\\|s_t) \\exp\\left(\\frac{r(s_t, a)}{\\tau}\\right)\\) <br> å…¶ä¸­ \\(\\tau\\) æ˜¯æ¸©åº¦å‚æ•°ã€‚é‡è¦æ€§æƒé‡ä¸ºä¼¼ç„¶æ¯” \\(w = p^* / \\pi_\\theta\\)ï¼Œå› æ­¤ï¼š<br> \\(w(a) \\propto \\exp\\left(\\frac{r(s_t, a)}{\\tau}\\right)\\) | **æ¸©åº¦å‚æ•° \\(\\tau\\)**ï¼šæ§åˆ¶é€‰æ‹©â€œé”åº¦â€ã€‚è®ºæ–‡é€šè¿‡å®éªŒç¡®å®š \\(\\tau = 1.0\\) æ—¶æ•ˆæœæœ€ä½³ã€‚ |\n| **Top-kæˆªæ–­ä¸æƒé‡å½’ä¸€åŒ–** | ä¸ºé™ä½æŒ‡æ•°æƒé‡å¸¦æ¥çš„é«˜æ–¹å·®ï¼Œä»…ä¿ç•™å¥–åŠ±æœ€é«˜çš„ \\(k\\) ä¸ªå€™é€‰è½¨è¿¹ï¼Œå¹¶å¯¹å®ƒä»¬çš„æƒé‡è¿›è¡ŒSoftmaxå½’ä¸€åŒ–ï¼Œå¾—åˆ°æœ€ç»ˆç”¨äºè®­ç»ƒçš„é‡è¦æ€§æƒé‡ï¼š<br> \\(w(a_t^{(i)}) = \\frac{\\exp(r(s_t, a_t^{(i)}) / \\tau)}{\\sum_{j=1}^k \\exp(r(s_t, a_t^{(j)}) / \\tau)}\\) | **å€™é€‰æ•° \\(N\\)**ï¼šæ¯æ¬¡ä»ç­–ç•¥ä¸­é‡‡æ ·çš„è½¨è¿¹æ€»æ•°ã€‚<br> **æˆªæ–­æ•° \\(k\\)**ï¼šä¿ç•™ç”¨äºè®­ç»ƒçš„é«˜è´¨é‡è½¨è¿¹æ•°ã€‚ |\n| **è‡ªæ¨¡ä»¿æ‰©æ•£æŸå¤±å‡½æ•°** | å°†æœ€å¤§åŒ–åŠ æƒå¯¹æ•°ä¼¼ç„¶çš„ç›®æ ‡ï¼Œè½¬åŒ–ä¸ºæœ€å°åŒ–**å¥–åŠ±åŠ æƒçš„å»å™ªæŸå¤±**ã€‚è¿™æ˜¯SIDPçš„æ ¸å¿ƒè®­ç»ƒç›®æ ‡ï¼š<br> \\(\\mathcal{L}_{\\text{SIDP}} = \\sum_{i=1}^k w(a_t^{(i)}) \\mathbb{E}_{t_{dn}, \\epsilon} \\left[ \\| \\epsilon - \\epsilon_\\theta(a_t^{(i), t_{dn}}, t_{dn}, s_t) \\|_2^2 \\right]\\) <br> å…¶ä¸­ \\(t_{dn}\\) æ˜¯æ‰©æ•£æ—¶é—´æ­¥ï¼Œ\\(\\epsilon\\) æ˜¯æ·»åŠ çš„å™ªå£°ï¼Œ\\(\\epsilon_\\theta\\) æ˜¯å»å™ªç½‘ç»œã€‚è¯¥æŸå¤±ä½¿ç­–ç•¥æ›´å€¾å‘äºç”Ÿæˆé«˜å¥–åŠ±è½¨è¿¹ã€‚ | æŸå¤±å‡½æ•°å½¢å¼ç»§æ‰¿è‡ªDDPMï¼Œä½†å¼•å…¥äº†æ ·æœ¬æƒé‡ \\(w\\)ã€‚ |\n| **å¥–åŠ±å‡½æ•°è®¾è®¡** | å¥–åŠ± \\(r(s_t, a_t)\\) æ˜¯å¼•å¯¼è‡ªæ¨¡ä»¿çš„å…³é”®ä¿¡å·ï¼Œç”±å››ä¸ªéƒ¨åˆ†ç»„æˆï¼ˆè¯¦è§è®ºæ–‡è¡¨Iï¼‰ï¼š<br> 1. **å®‰å…¨å¥–åŠ± \\(r_{\\text{col}}\\)**ï¼šå‘ç”Ÿç¢°æ’æ—¶ç»™äºˆå¤§é¢æƒ©ç½šã€‚<br> 2. **æ­¥é•¿æˆæœ¬ \\(r_{\\text{step}}\\)**ï¼šæƒ©ç½šè·¯å¾„é•¿åº¦ï¼Œé¼“åŠ±é«˜æ•ˆã€‚<br> 3. **è¿›åº¦å¥–åŠ± \\(r_{\\text{prog}}\\)**ï¼šå¥–åŠ±å‘ç›®æ ‡é è¿‘çš„å‡ ä½•è·ç¦»å‡å°‘é‡ã€‚<br> 4. **åœé å¥–åŠ± \\(r_{\\text{dock}}\\)**ï¼šåœ¨æ¥è¿‘ç›®æ ‡æ—¶ï¼ˆ<0.5mï¼‰æä¾›ç»†ç²’åº¦å¼•å¯¼ã€‚ | \\(\\lambda_{\\text{col}}=10, \\lambda_{\\text{step}}=0.5, \\lambda_{\\text{prog}}=5, \\lambda_{\\text{dock}}=10\\) <br> ç²¾ç»†åœé åŒºåŸŸ \\(\\delta_{\\text{fine}} = 0.5m\\) |\n\n### 2. ç›®æ ‡æ— å…³æ¢ç´¢ (Goal-Agnostic Exploration)\n\næ­¤ç­–ç•¥ç”¨äº**å¢å¼ºæ•°æ®å¤šæ ·æ€§**å¹¶**æ­£åˆ™åŒ–**ç‚¹ç›®æ ‡å¯¼èˆªä»»åŠ¡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚\n\n| æŠ€æœ¯ç»„ä»¶ | å…·ä½“ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…³é”®å‚æ•°ä¸è®¾ç½® |\n| :--- | :--- | :--- |\n| **æ•°æ®ç”Ÿæˆæµç¨‹** | 1. **é‡‡æ ·è¾…åŠ©ç›®æ ‡ç‚¹**ï¼šåœ¨æ™ºèƒ½ä½“åˆå§‹ä½ç½®å‘¨å›´ï¼ŒæŒ‰å‡åŒ€åˆ†å¸ƒé‡‡æ ·è§’åº¦ï¼ˆ-60Â° ~ 60Â°ï¼‰å’Œè·ç¦»ï¼ˆ3m ~ 5mï¼‰ç”Ÿæˆä¸´æ—¶ç›®æ ‡ã€‚<br>2. **ç­–ç•¥ç”Ÿæˆè½¨è¿¹**ï¼šä½¿ç”¨å½“å‰ç­–ç•¥ \\(\\pi_\\theta\\)ï¼Œå°†è¾“å…¥çŠ¶æ€ \\(s_t\\) ä¸­çš„çœŸå®ç›®æ ‡ \\(g_t\\) æ›¿æ¢ä¸ºä¸€ä¸ªç‰¹æ®Šçš„â€œç›®æ ‡æ— å…³â€åµŒå…¥ï¼Œç”Ÿæˆé€šå¾€ä¸´æ—¶ç›®æ ‡çš„è½¨è¿¹ã€‚ | è§’åº¦èŒƒå›´ï¼š[-60Â°, 60Â°]ï¼›è·ç¦»èŒƒå›´ï¼š[3m, 5m]ã€‚ |\n| **è®­ç»ƒé›†æˆ** | åœ¨è®­ç»ƒå‰å‘ä¼ æ’­ä¸­ï¼Œè¿™äº›è½¨è¿¹ä¸ç‚¹ç›®æ ‡å¯¼èˆªè½¨è¿¹æ··åˆã€‚å¯¹äºç›®æ ‡æ— å…³è½¨è¿¹ï¼Œåœ¨è®¡ç®—è‡ªæ¨¡ä»¿æŸå¤±æ—¶ï¼Œ**å°†å…¶é‡è¦æ€§æƒé‡ \\(w\\) è®¾ç½®ä¸ºå‡åŒ€åˆ†å¸ƒ**ï¼ˆå³æ‰€æœ‰æ ·æœ¬å¹³ç­‰å¯¹å¾…ï¼‰ã€‚è¿™ä½¿å¾—ç­–ç•¥å­¦ä¹ å®‰å…¨ã€åˆç†çš„è¿åŠ¨æ¨¡å¼ï¼Œè€Œä¸å—ç‰¹å®šç›®æ ‡ç‚¹çš„æŸç¼šã€‚ | ç›®æ ‡æ— å…³åµŒå…¥ï¼šä¸€ä¸ªå›ºå®šçš„ç‰¹æ®Šå‘é‡ï¼Œç”¨äºæ›¿æ¢è¾“å…¥ä¸­çš„ \\(g_t\\)ã€‚ |\n\n### 3. å¥–åŠ±é©±åŠ¨çš„è¯¾ç¨‹å­¦ä¹  (Reward-Driven Curriculum Learning)\n\næ­¤ç­–ç•¥ç”¨äº**åŠ¨æ€é€‰æ‹©è®­ç»ƒæ ·æœ¬**ï¼Œæé«˜æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼Œä¿ƒè¿›ç¨³å®šæ”¶æ•›ã€‚\n\n| æŠ€æœ¯ç»„ä»¶ | å…·ä½“ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…³é”®å‚æ•°ä¸è®¾ç½® |\n| :--- | :--- | :--- |\n| **åœºæ™¯æ½œåŠ›è¯„ä¼°** | å¯¹æ¯ä¸ªè®­ç»ƒåœºæ™¯ \\(s_i\\)ï¼Œé€šè¿‡å½“å‰ç­–ç•¥é‡‡æ ·ä¸€æ‰¹è½¨è¿¹ï¼Œè®¡ç®—ä¸¤ä¸ªæŒ‡æ ‡ï¼š<br>1. **æœ€å¤§å¥–åŠ± \\(R_{\\text{max}}(s_i)\\)**ï¼šè¯¥åœºæ™¯ä¸‹èƒ½è·å¾—çš„æœ€ä½³è¡¨ç°ï¼Œåæ˜ **å­¦ä¹ å¯è¡Œæ€§**ã€‚<br>2. **å¥–åŠ±èŒƒå›´ \\(R_{\\text{range}}(s_i)\\)**ï¼šæœ€é«˜å¥–åŠ±ä¸æœ€ä½å¥–åŠ±ä¹‹å·®ï¼Œåæ˜ **æ¢¯åº¦ä¿¡æ¯çš„ä¸°å¯Œåº¦**ï¼ˆåŒºåˆ†åº¦ï¼‰ã€‚ | - |\n| **åŠ¨æ€é€‰æ‹©æ¡ä»¶** | ä»…å½“åœºæ™¯åŒæ—¶æ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶æ—¶ï¼Œæ‰è¢«é€‰å…¥å½“å‰è®­ç»ƒæ‰¹æ¬¡ï¼š<br> \\(R_{\\text{max}}(s_i) \\geq \\tau_{\\text{max}}\\) **ä¸”** \\(R_{\\text{range}}(s_i) \\geq \\tau_{\\text{range}}\\) <br> - **é«˜ \\(\\tau_{\\text{max}}\\)** ç¡®ä¿ç­–ç•¥ä¸ä¼šåœ¨è¿‡äºå›°éš¾ã€æ€»æ˜¯å¤±è´¥çš„åœºæ™¯ä¸Šæµªè´¹æ—¶é—´ã€‚<br> - **é«˜ \\(\\tau_{\\text{range}}\\)** ç¡®ä¿åœºæ™¯èƒ½æä¾›å…·æœ‰åŒºåˆ†åº¦çš„æ ·æœ¬ï¼Œä¿ƒè¿›ç­–ç•¥ä¼˜åŒ–æ•ˆç‡è€Œéä»…ä»…å®Œæˆä»»åŠ¡ã€‚ | é˜ˆå€¼ \\(\\tau_{\\text{max}}\\) å’Œ \\(\\tau_{\\text{range}}\\) åœ¨è®ºæ–‡ä¸­æœªç»™å‡ºå…·ä½“æ•°å€¼ï¼Œéœ€æ ¹æ®å®é™…è®­ç»ƒåŠ¨æ€è°ƒæ•´æˆ–è®¾å®šã€‚ |\n\n## æŠ€æœ¯åˆ›æ–°ä¸å®é™…ä»·å€¼æ€»ç»“\n\n1.  **æ ¹æœ¬æ€§èŒƒå¼è½¬å˜**ï¼šä»â€œæ¨¡ä»¿å›ºå®šçš„ä¸“å®¶æ•°æ®â€è½¬å˜ä¸ºâ€œæ¨¡ä»¿è‡ªæˆ‘ç”Ÿæˆçš„é«˜è´¨é‡ç»éªŒâ€ï¼Œè§£å†³äº†ä¸“å®¶æ•°æ®æ¬¡ä¼˜ã€è¦†ç›–æœ‰é™çš„æ ¹æœ¬ç“¶é¢ˆã€‚\n2.  **å®ç°åˆ†å¸ƒé›†ä¸­**ï¼šé€šè¿‡å¥–åŠ±åŠ æƒï¼Œç­–ç•¥åˆ†å¸ƒé€æ¸å‘é«˜å¥–åŠ±åŒºåŸŸé›†ä¸­ï¼Œå‡å°‘äº†ç”Ÿæˆè½¨è¿¹çš„æ–¹å·®å’Œæ¬¡ä¼˜æ¨¡æ€ï¼Œä»è€Œ**åœ¨æ¨ç†æ—¶æ— éœ€è¿›è¡Œâ€œç”Ÿæˆ-åè¿‡æ»¤â€**ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„é«˜æ•ˆè§„åˆ’ã€‚\n3.  **ç¨³å®šé«˜æ•ˆçš„ä¼˜åŒ–**ï¼šå°†å¤æ‚çš„ã€éœ€è¦è´¯ç©¿æ‰©æ•£æ—¶é—´æ­¥åå‘ä¼ æ’­ï¼ˆBPTTï¼‰çš„å¼ºåŒ–å­¦ä¹ å¼ä¼˜åŒ–ï¼Œè½¬åŒ–ä¸ºç¨³å®šçš„åŠ æƒæ¨¡ä»¿å­¦ä¹ é—®é¢˜ï¼Œå¤§å¤§é™ä½äº†è®­ç»ƒéš¾åº¦å’Œè®¡ç®—æˆæœ¬ã€‚\n4.  **æ˜¾è‘—çš„æ€§èƒ½ä¸æ•ˆç‡æå‡**ï¼š\n    *   **æ€§èƒ½**ï¼šåœ¨å¤æ‚åœºæ™¯ï¼ˆå¦‚Commercialï¼‰ä¸Šç›¸æ¯”åŸºçº¿ï¼ˆNavDPï¼‰æœ‰è¿‘10ä¸ªç™¾åˆ†ç‚¹çš„æˆåŠŸç‡æå‡ï¼Œè¯æ˜äº†å…¶å“è¶Šçš„æ³›åŒ–ä¸é²æ£’æ€§ã€‚\n    *   **æ•ˆç‡**ï¼šç”±äºåˆ†å¸ƒé›†ä¸­ï¼Œå¯ä»¥ä½¿ç”¨æ›´å°‘çš„å»å™ªæ­¥æ•°ï¼ˆå¦‚DDIM 5æ­¥ï¼‰è¿›è¡Œç¡®å®šæ€§é‡‡æ ·ï¼Œåœ¨Jetson Orin Nanoä¸Šå®ç°**2.5å€**çš„æ¨ç†åŠ é€Ÿï¼ˆ110ms vs 273msï¼‰ï¼Œæ»¡è¶³äº†å®æ—¶æœºå™¨äººéƒ¨ç½²çš„ä¸¥è‹›è¦æ±‚ã€‚\n\nè¿™äº›æ–¹æ³•å…±åŒæ„æˆäº†ä¸€ä¸ªå®Œæ•´ã€é«˜æ•ˆä¸”é²æ£’çš„è§†è§‰å¯¼èˆªç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œå…¼å…·ç†è®ºåˆ›æ–°å’Œæé«˜çš„å®é™…åº”ç”¨ä»·å€¼ã€‚",
            "timestamp": "2026-02-03T15:57:40.604191",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä½¿ç”¨äº†**å¯¼èˆªæ€§èƒ½**å’Œ**æ•ˆç‡/å®‰å…¨æ€§**ä¸¤å¤§ç±»æŒ‡æ ‡ï¼Œå…·ä½“å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| ç±»åˆ« | æŒ‡æ ‡åç§° | ç¼©å†™ | è®¡ç®—å…¬å¼/å®šä¹‰ | è¯„ä¼°ç›®çš„ |\n| :--- | :--- | :--- | :--- | :--- |\n| **å¯¼èˆªæ€§èƒ½** | æˆåŠŸç‡ | SR | `SR = (æˆåŠŸæ¬¡æ•° / æ€»å°è¯•æ¬¡æ•°) * 100%` | è¯„ä¼°å¯¼èˆªä»»åŠ¡çš„åŸºæœ¬å®Œæˆèƒ½åŠ›ã€‚æˆåŠŸå®šä¹‰ä¸ºæ™ºèƒ½ä½“æœ€ç»ˆä½ç½®ä¸ç›®æ ‡ç‚¹çš„æ¬§æ°è·ç¦» < 0.5ç±³ã€‚ |\n| | è·¯å¾„é•¿åº¦åŠ æƒæˆåŠŸç‡ | SPL | `SPL = (1/N) * Î£ [S_i * (L_i / max(P_i, L_i))]` | **ç»¼åˆè¯„ä»·æŒ‡æ ‡**ï¼ŒåŒæ—¶è¡¡é‡**æˆåŠŸç‡å’Œè·¯å¾„æ•ˆç‡**ã€‚`S_i`ä¸ºæˆåŠŸä¸å¦ï¼ˆ0/1ï¼‰ï¼Œ`L_i`ä¸ºæœ€çŸ­è·¯å¾„ï¼ˆæµ‹åœ°çº¿ï¼‰é•¿åº¦ï¼Œ`P_i`ä¸ºå®é™…è·¯å¾„é•¿åº¦ã€‚å€¼è¶Šé«˜ï¼Œè¯´æ˜å¯¼èˆªæ—¢æˆåŠŸåˆé«˜æ•ˆã€‚ |\n| **æ•ˆç‡ä¸å®‰å…¨æ€§** | ç¢°æ’ç‡ | CR | `CR = (å‘ç”Ÿç¢°æ’çš„å°è¯•æ¬¡æ•° / æ€»å°è¯•æ¬¡æ•°) * 100%` | è¯„ä¼°å¯¼èˆªè¿‡ç¨‹ä¸­çš„**å®‰å…¨æ€§**ã€‚ç¢°æ’æ¬¡æ•°è¶Šå°‘ï¼Œç­–ç•¥è¶Šå®‰å…¨ã€‚ |\n| | ç»ˆç‚¹è·ç¦» | DTG | æ™ºèƒ½ä½“æœ€ç»ˆä½ç½®ä¸ç›®æ ‡ç‚¹ä¹‹é—´çš„å¹³å‡**æ¬§æ°è·ç¦»**ï¼ˆå•ä½ï¼šç±³ï¼‰ã€‚ | è¯„ä¼°å¯¼èˆªçš„**ç²¾ç¡®åº¦**ã€‚è·ç¦»è¶Šå°ï¼Œè¯´æ˜ç­–ç•¥è¶Šèƒ½ç²¾ç¡®æŠµè¾¾ç›®æ ‡ã€‚ |\n| | æ¢ç´¢é¢ç§¯ | EA | å°†16æ¡é‡‡æ ·è½¨è¿¹ï¼ˆæ¯æ¡è§†ä¸ºå®½0.2ç±³çš„æ‰«æ åŒºåŸŸï¼‰å–å¹¶é›†åçš„æ€»é¢ç§¯ï¼ˆå•ä½ï¼šå¹³æ–¹ç±³ï¼‰ã€‚ | è¯„ä¼°åœ¨**æ— ç›®æ ‡ï¼ˆGoal-agnosticï¼‰æ¢ç´¢**ä»»åŠ¡ä¸­ç”Ÿæˆè½¨è¿¹çš„**å¤šæ ·æ€§**ã€‚é¢ç§¯è¶Šå¤§ï¼Œæ¢ç´¢èŒƒå›´è¶Šå¹¿ã€‚ |\n| **è®¡ç®—æ•ˆç‡** | æ¨ç†æ—¶é—´ | Latency | åœ¨è¾¹ç¼˜è®¾å¤‡ï¼ˆJetson Orin Nanoï¼‰ä¸Šå®Œæˆå•æ¬¡è½¨è¿¹è§„åˆ’æ‰€éœ€çš„å¹³å‡æ—¶é—´ï¼ˆå•ä½ï¼šæ¯«ç§’ï¼‰ã€‚ | è¯„ä¼°ç®—æ³•çš„**å®æ—¶æ€§**å’Œ**éƒ¨ç½²å¯è¡Œæ€§**ã€‚æ—¶é—´è¶ŠçŸ­ï¼Œè¶Šé€‚åˆèµ„æºå—é™çš„æœºå™¨äººå¹³å°ã€‚ |\n\n**æŒ‡æ ‡ä½¿ç”¨åœºæ™¯æ€»ç»“ï¼š**\n- **ä¸»å®éªŒï¼ˆIV-Cï¼‰**ï¼šä½¿ç”¨ **SR** å’Œ **SPL** åœ¨æ ‡å‡†åŸºå‡†ä¸Šä¸å…¶ä»–SOTAæ–¹æ³•è¿›è¡Œç»¼åˆæ€§èƒ½å¯¹æ¯”ã€‚\n- **æ¶ˆèå®éªŒï¼ˆIV-Dï¼‰**ï¼šé™¤SRå’ŒSPLå¤–ï¼Œå¼•å…¥ **CRã€DTGã€EA** æ¥æ·±å…¥åˆ†æå„ç»„ä»¶ï¼ˆå¦‚ç›®æ ‡æ— å…³æ¢ç´¢ï¼‰å¯¹å®‰å…¨æ€§ã€ç²¾ç¡®åº¦å’Œæ¢ç´¢å¤šæ ·æ€§çš„å…·ä½“å½±å“ã€‚\n- **æ•ˆç‡éªŒè¯ï¼ˆIV-C2, Table IIIï¼‰**ï¼šä¸“é—¨æµ‹é‡**æ¨ç†æ—¶é—´**ï¼Œå¹¶ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ï¼Œè¯æ˜å…¶æ•ˆç‡ä¼˜åŠ¿ã€‚\n\n### äºŒã€ ä½¿ç”¨çš„æ•°æ®é›†ä¸åŸºå‡†\n\nè®ºæ–‡ä¸»è¦åœ¨ä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œå…·ä½“ä½¿ç”¨çš„æ•°æ®é›†å’ŒåŸºå‡†å¦‚ä¸‹ï¼š\n\n| åç§° | ç±»å‹ | æ¥æº/æè¿° | ç”¨é€” | å…³é”®ç»†èŠ‚ |\n| :--- | :--- | :--- | :--- | :--- |\n| **InternData-N1** | ä»¿çœŸæ•°æ®é›† | å¼•è‡ªNavDP[4]ã€‚åŒ…å«é«˜è´¨é‡çš„ä»¿çœŸåœºæ™¯ã€ä¸“å®¶è½¨è¿¹å’Œç‰¹æƒä¿¡æ¯ï¼ˆå¦‚å…¨å±€ESDFï¼‰ã€‚ | **1. è®­ç»ƒç¯å¢ƒæ„å»º**ï¼šç”¨äºé«˜æ•ˆçš„è‡ªæ¨¡ä»¿å­¦ä¹ ã€‚é€šè¿‡ä»ä¸“å®¶è½¨è¿¹ä¸­é‡‡æ ·åˆå§‹ä½å§¿ï¼Œå¹¶åˆ©ç”¨ESDFç”Ÿæˆä¸åŒéš¾åº¦çš„ç›®æ ‡ç‚¹ï¼Œæ„å»ºäº¤äº’å¼è®­ç»ƒç¯å¢ƒã€‚<br>**2. æ¶ˆèç ”ç©¶**ï¼šä»ä¸­é€‰å–5ä¸ªåœºæ™¯ã€500ä¸ªè¯•éªŒï¼Œç”¨äºå¯æ§çš„æ¶ˆèåˆ†æã€‚ | - æä¾›**æ¬§å‡ é‡Œå¾—ç¬¦å·è·ç¦»åœºï¼ˆESDFï¼‰**ï¼Œç”¨äºå¿«é€Ÿç¢°æ’æ£€æµ‹å’Œç¡®ä¿åˆå§‹çŠ¶æ€æ— ç¢°æ’ã€‚<br>- æ”¯æŒ**å•æ­¥ï¼ˆone-shotï¼‰è½¨è¿¹ç”Ÿæˆ**è¯„ä¼°ï¼Œä¸ä¸»åŸºå‡†çš„å®æ—¶äº¤äº’è¯„ä¼°å½¢æˆäº’è¡¥ã€‚ |\n| **InternVLA-N1 S1 Benchmark** | ç»¼åˆä»¿çœŸåŸºå‡† | å¼•è‡ªNavDP[4]ã€‚åŸºäºé«˜ä¿çœŸIsaac Simæ¨¡æ‹Ÿå™¨æ„å»ºã€‚ | **ä¸»æ€§èƒ½è¯„ä¼°**ï¼šç”¨äºåœ¨**é—­ç¯ã€å®æ—¶ã€å¤šæ­¥å¯¼èˆª**ä»»åŠ¡ä¸­ï¼Œä¸ç°æœ‰SOTAæ–¹æ³•è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚ | - **åœºæ™¯æ„æˆ**ï¼šæ€»è®¡60ä¸ªå¤šæ ·åŒ–åœºæ™¯ã€‚<br>  - **InternScenes (40ä¸ª)**ï¼šæ¨¡æ‹ŸçœŸå®å®¤å†…ç¯å¢ƒï¼Œå‡åˆ†ä¸º`Commercial`ï¼ˆå•†ä¸šåœºæ™¯ï¼‰å’Œ`Home`ï¼ˆå®¶åº­åœºæ™¯ï¼‰ã€‚<br>  - **ClutteredEnv (20ä¸ª)**ï¼šéšæœºéšœç¢ç‰©ç¯å¢ƒï¼Œå‡åˆ†ä¸º`Easy`ï¼ˆç®€å•ï¼‰å’Œ`Hard`ï¼ˆå›°éš¾ï¼‰ã€‚<br>- **è¯•éªŒè®¾ç½®**ï¼šæ¯ä¸ªåœºæ™¯éšæœºé‡‡æ ·100ä¸ªèµ·ç‚¹-ç›®æ ‡ç‚¹å¯¹ï¼Œç¡®ä¿åˆå§‹æ–¹å‘æ— ç¢°æ’ã€‚<br>- **è¯„ä¼°æ¨¡å¼**ï¼š**å®æ—¶äº¤äº’ï¼ˆreal-timeï¼‰**ï¼Œè¦æ±‚æ™ºèƒ½ä½“é€šè¿‡è¿ç»­çš„å¤šæ­¥å†³ç­–æŠµè¾¾ç›®æ ‡ã€‚ |\n\n**æ•°æ®é›†/åŸºå‡†ä½¿ç”¨é€»è¾‘æ€»ç»“ï¼š**\n1.  **è®­ç»ƒ**ï¼šåˆ©ç”¨ **InternData-N1** æ„å»ºè½»é‡ã€é«˜æ•ˆçš„è®­ç»ƒç¯å¢ƒï¼Œé¿å…å…¨ç‰©ç†ä»¿çœŸçš„é«˜æ˜‚è®¡ç®—æˆæœ¬ã€‚\n2.  **è¯„ä¼°**ï¼š\n    - **æ ¸å¿ƒæ€§èƒ½å¯¹æ¯”**ï¼šåœ¨æ ‡å‡†çš„ **InternVLA-N1 S1 Benchmark** ä¸Šè¿›è¡Œï¼Œç»“æœå…·æœ‰å¯æ¯”æ€§å’Œè¯´æœåŠ›ã€‚\n    - **æ·±å…¥åˆ†æ**ï¼šä» **InternData-N1** æ´¾ç”Ÿå®šåˆ¶åŒ–åŸºå‡†è¿›è¡Œæ¶ˆèå®éªŒï¼Œä»¥æ·±å…¥å‰–æå„ä¸ªæ¨¡å—çš„ä½œç”¨ã€‚\n3.  **ç°å®éªŒè¯**ï¼šåœ¨**çœŸå®æœºå™¨äººï¼ˆUnitree Go2, Agilex T-rexï¼‰** å’Œ**çœŸå®åœºæ™¯ï¼ˆåŠå…¬å®¤ã€èµ°å»Šï¼‰** ä¸­è¿›è¡Œéƒ¨ç½²æµ‹è¯•ï¼Œä½¿ç”¨ç«‹ä½“ç›¸æœºå’ŒBANet[31]ä¼°è®¡æ·±åº¦å›¾ï¼ŒéªŒè¯å…¶ä»ä»¿çœŸåˆ°ç°å®çš„è¿ç§»èƒ½åŠ›ã€‚\n\n**åˆ›æ–°ç‚¹ä¸ä»·å€¼ä½“ç°**ï¼š\n- **è¯„ä»·å…¨é¢æ€§**ï¼šä¸ä»…å…³æ³¨æœ€ç»ˆæˆåŠŸç‡ï¼ˆSRï¼‰ï¼Œæ›´é€šè¿‡**SPL**å¼ºè°ƒè·¯å¾„æ•ˆç‡ï¼Œé€šè¿‡**CRã€DTG**å…³æ³¨å®‰å…¨ä¸ç²¾ç¡®åº¦ï¼Œé€šè¿‡**EA**é‡åŒ–æ¢ç´¢èƒ½åŠ›ï¼Œé€šè¿‡**æ¨ç†æ—¶é—´**è¯æ˜éƒ¨ç½²å¯è¡Œæ€§ï¼Œå½¢æˆäº†å¤šç»´åº¦çš„è¯„ä¼°ä½“ç³»ã€‚\n- **åŸºå‡†æŒ‘æˆ˜æ€§**ï¼šç‰¹æ„åœ¨åŒ…å«å¤æ‚å®¶å…·å¸ƒå±€ã€ç‹­çª„é€šé“çš„`InternScenes`ä¸Šå±•ç¤ºæ˜¾è‘—æ€§èƒ½æå‡ï¼Œå‡¸æ˜¾äº†SIDPåœ¨**å¤æ‚ã€æœªè§è¿‡çš„éç»“æ„åŒ–ç¯å¢ƒ**ä¸­è¶…è¶Šä¼ ç»Ÿæ¨¡ä»¿å­¦ä¹ çš„**å¼ºæ³›åŒ–ä¸é²æ£’æ€§**ã€‚\n- **æ•ˆç‡å¯¼å‘**ï¼šæ‰€æœ‰æ•ˆç‡æŒ‡æ ‡ï¼ˆæ¨ç†æ—¶é—´ã€SPLï¼‰çš„ä¼˜åŒ–ï¼Œéƒ½ç›´æŒ‡**ç°å®ä¸–ç•Œæœºå™¨äººéƒ¨ç½²çš„æ ¸å¿ƒç“¶é¢ˆ**ï¼Œä½¿ç ”ç©¶ä¸ä»…åœç•™åœ¨ä»¿çœŸæ€§èƒ½æå‡ï¼Œæ›´å…·å¤‡äº†æ˜ç¡®çš„å·¥ç¨‹å®ç”¨ä»·å€¼ã€‚",
            "timestamp": "2026-02-03T15:58:30.572480",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## å®éªŒæ•°æ®é›†ä¸è¯„ä¼°ç»“æœåˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ä¸¤ä¸ªä¸»è¦çš„æ•°æ®é›†/åŸºå‡†ä¸Šè¿›è¡Œäº†å®éªŒï¼š\n\n1.  **InternVLA-N1 S1 Benchmark** (ç”¨äºä¸»è¦æ€§èƒ½å¯¹æ¯”)\n    *   **æ¥æº**ï¼š åŸºäºé«˜ä¿çœŸIsaac Simæ¨¡æ‹Ÿå™¨æ„å»ºã€‚\n    *   **ç‰¹ç‚¹**ï¼š ä¸“æ³¨äº**é—­ç¯ã€å®æ—¶ã€å¤šæ­¥**å¯¼èˆªä»»åŠ¡ã€‚\n    *   **æ„æˆ**ï¼š\n        *   **InternScenes** (40ä¸ªåœºæ™¯)ï¼š åˆ†ä¸º20ä¸ª**å®¶åº­**åœºæ™¯å’Œ20ä¸ª**å•†ä¸š**åœºæ™¯ã€‚\n        *   **ClutteredEnv** (20ä¸ªåœºæ™¯)ï¼š åˆ†ä¸º10ä¸ª**ç®€å•**åœºæ™¯å’Œ10ä¸ª**å›°éš¾**åœºæ™¯ã€‚\n    *   **ä»»åŠ¡è®¾ç½®**ï¼š æ¯ä¸ªåœºæ™¯éšæœºé‡‡æ ·100ä¸ªèµ·ç‚¹-ç›®æ ‡ç‚¹å¯¹ï¼Œå…±è®¡6000ä¸ªå¯¼èˆªè¯•éªŒã€‚\n\n2.  **è‡ªå®šä¹‰åŸºå‡†** (ç”¨äºæ¶ˆèç ”ç©¶)\n    *   **æ¥æº**ï¼š ä»**InternData-N1**æ•°æ®é›†ä¸­é€‰å–ã€‚\n    *   **ç‰¹ç‚¹**ï¼š ä¸“æ³¨äº**å•æ­¥ã€ä¸€æ¬¡æ€§**è½¨è¿¹ç”Ÿæˆè¯„ä¼°ã€‚\n    *   **æ„æˆ**ï¼š 5ä¸ªä»£è¡¨æ€§åœºæ™¯ï¼Œå…±500ä¸ªè¯•éªŒã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨äº†ä»¥ä¸‹æ ‡å‡†æŒ‡æ ‡è¿›è¡Œç»¼åˆè¯„ä¼°ï¼š\n\n| æŒ‡æ ‡ | å…¨ç§° | å®šä¹‰ä¸ç›®çš„ |\n| :--- | :--- | :--- |\n| **SR** | æˆåŠŸç‡ | æ™ºèƒ½ä½“æˆåŠŸæŠµè¾¾ç›®æ ‡ç‚¹ï¼ˆè·ç¦»<0.5ç±³ï¼‰çš„è¯•éªŒæ¯”ä¾‹ã€‚è¡¡é‡ä»»åŠ¡å®Œæˆèƒ½åŠ›ã€‚ |\n| **SPL** | è·¯å¾„é•¿åº¦åŠ æƒæˆåŠŸç‡ | `SPL = (1/N) * Î£ [S_i * (L_i / max(P_i, L_i))]`ã€‚åŒæ—¶è¡¡é‡**æˆåŠŸç‡å’Œè·¯å¾„æ•ˆç‡**ï¼Œæ˜¯æ ¸å¿ƒç»¼åˆæŒ‡æ ‡ã€‚ |\n| **CR** | ç¢°æ’ç‡ | è¯•éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿè‡³å°‘ä¸€æ¬¡ç¢°æ’çš„æ¯”ä¾‹ã€‚è¡¡é‡**å®‰å…¨æ€§**ã€‚ |\n| **DTG** | ç»ˆç‚¹è·ç›®æ ‡è·ç¦» | æ™ºèƒ½ä½“æœ€ç»ˆä½ç½®ä¸ç›®æ ‡ç‚¹çš„å¹³å‡æ¬§æ°è·ç¦»ã€‚è¡¡é‡å¯¼èˆª**ç²¾ç¡®åº¦**ã€‚ |\n| **EA** | æ¢ç´¢é¢ç§¯ | å¤šæ¡é‡‡æ ·è½¨è¿¹æ‰€è¦†ç›–çš„éå†—ä½™åŒºåŸŸé¢ç§¯ã€‚è¡¡é‡**æ¢ç´¢å¤šæ ·æ€§**ï¼ˆç”¨äºæ¶ˆèç ”ç©¶ï¼‰ã€‚ |\n\n### ä¸‰ã€ä¸»è¦æ€§èƒ½æå‡\nè®ºæ–‡æå‡ºçš„**SIDP**æ–¹æ³•åœ¨å¤šä¸ªç»´åº¦ä¸Šæ˜¾è‘—è¶…è¶Šäº†åŸºçº¿æ–¹æ³•ï¼ˆç‰¹åˆ«æ˜¯ä¸»è¦å¯¹æ¯”åŸºçº¿ **NavDP**ï¼‰ã€‚\n\n#### 1. å¯¼èˆªæ€§èƒ½ (SR & SPL)\nåœ¨**InternVLA-N1 S1 Benchmark**ä¸Šçš„ç»¼åˆç»“æœï¼š\n\n*   **æ•´ä½“æ€§èƒ½ (mSR/mSPL)**ï¼š\n    *   **SIDP**: **79.11%** / **72.72%**\n    *   **NavDP (åŸºçº¿)**: 73.22% / 70.95%\n    *   **æå‡**: **+5.89% (SR)** / **+1.77% (SPL)**\n\n*   **å…³é”®åœºæ™¯æ€§èƒ½**ï¼š\n    *   **å•†ä¸šåœºæ™¯ (Commercial)**:\n        *   SIDP SR: **81.19%** vs. NavDP: 71.25% â†’ **æå‡ +9.94%**\n    *   **å®¶åº­åœºæ™¯ (Home)**:\n        *   SIDP SR: **63.17%** vs. NavDP: 57.38% â†’ **æå‡ +5.79%**\n    *   **ç»“è®º**ï¼š SIDPåœ¨**å¤æ‚ã€ç»“æ„åŒ–ç¯å¢ƒ**ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œä¼˜åŠ¿æœ€ä¸ºæ˜æ˜¾ã€‚\n\n#### 2. æ¨ç†æ•ˆç‡\nåœ¨è¾¹ç¼˜è®¾å¤‡**NVIDIA Jetson Orin Nano**ä¸Šçš„å®æµ‹ç»“æœï¼š\n\n| æ–¹æ³• | é‡‡æ ·å™¨ | å»å™ªæ­¥æ•° | æ¨ç†æ—¶é—´ | ç›¸å¯¹åŠ é€Ÿ | SR |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **NavDP (åŸºçº¿)** | DDPM | 10 | 273 ms | 1.0Ã— | 0.549 |\n| **SIDP** | DDPM | 10 | 132 ms | **2.07Ã—** | 0.670 |\n| **SIDP** | DDIM | 5 | **110 ms** | **2.48Ã—** | **0.674** |\n| **SIDP** | DDIM | 3 | 99 ms | 2.76Ã— | 0.655 |\n\n*   **æ ¸å¿ƒç»“è®º**ï¼š\n    1.  **æ˜¾è‘—åŠ é€Ÿ**ï¼š SIDPå®ç°äº†**æœ€é«˜2.5å€**çš„æ¨ç†åŠ é€Ÿã€‚\n    2.  **æ€§èƒ½ä¸é™åå‡**ï¼š åœ¨åŠ é€Ÿçš„åŒæ—¶ï¼ŒSRæŒ‡æ ‡ä»åŸºçº¿çš„0.549æå‡è‡³**0.674**ã€‚\n    3.  **é«˜æ•ˆé…ç½®**ï¼š **5æ­¥DDIM**è¢«ç¡®å®šä¸ºæœ€ä½³å¹³è¡¡ç‚¹ï¼Œåœ¨110mså†…å®ç°äº†æœ€é«˜çš„æˆåŠŸç‡ã€‚\n\n#### 3. å®é™…ä»·å€¼æ€»ç»“\n*   **æŠ€æœ¯åˆ›æ–°ä»·å€¼**ï¼š SIDPé€šè¿‡**å¥–åŠ±å¼•å¯¼çš„è‡ªæ¨¡ä»¿**æœºåˆ¶ï¼Œå®ç°äº†ç­–ç•¥çš„è‡ªæˆ‘è¿­ä»£ä¼˜åŒ–ï¼Œé¿å…äº†ä¼ ç»Ÿæ¨¡ä»¿å­¦ä¹ å¯¹ä¸å®Œç¾ä¸“å®¶æ•°æ®çš„ä¾èµ–ï¼Œä»¥åŠâ€œç”Ÿæˆ-åè¿‡æ»¤â€ç®¡é“å¸¦æ¥çš„è®¡ç®—å¼€é”€ã€‚\n*   **å®é™…åº”ç”¨ä»·å€¼**ï¼š\n    *   **æ›´å¼ºé²æ£’æ€§**ï¼š åœ¨æœªè§è¿‡çš„å¤æ‚åœºæ™¯ä¸­ï¼ˆå¦‚å•†ä¸šç¯å¢ƒï¼‰æ€§èƒ½ä¸‹é™æ›´å°‘ï¼Œæ³›åŒ–èƒ½åŠ›æ›´å¼ºã€‚\n    *   **æ›´é«˜å®æ—¶æ€§**ï¼š åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¡ç®—å¹³å°ä¸Šå®ç°äº†**æ¯«ç§’çº§**çš„é«˜æ•ˆæ¨ç†ï¼Œæ»¡è¶³çœŸå®æœºå™¨äººå®æ—¶éƒ¨ç½²éœ€æ±‚ã€‚\n    *   **ç«¯åˆ°ç«¯ç®€åŒ–**ï¼š æ‘’å¼ƒäº†å¤–éƒ¨è½¨è¿¹é€‰æ‹©å™¨ï¼Œä½¿è§„åˆ’æµç¨‹æ›´åŠ ç®€æ´å¯é ã€‚",
            "timestamp": "2026-02-03T15:59:12.248065",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹ã€ŠSelf-Imitated Diffusion Policy for Efficient and Robust Visual Navigationã€‹å…¨æ–‡çš„æ·±å…¥åˆ†æï¼Œæœ¬æ–‡æå‡ºçš„SIDPæ¡†æ¶è™½ç„¶å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n- **ä»¿çœŸåˆ°ç°å®çš„æ³›åŒ–èƒ½åŠ›ä»éœ€è¿›ä¸€æ­¥éªŒè¯**ï¼šå°½ç®¡è®ºæ–‡åœ¨çœŸå®æœºå™¨äººå¹³å°ï¼ˆUnitree Go2, Agilex T-rexï¼‰ä¸Šè¿›è¡Œäº†éƒ¨ç½²æµ‹è¯•ï¼Œå¹¶å±•ç¤ºäº†åœ¨åŠå…¬å®¤ã€èµ°å»Šç­‰ç¯å¢ƒä¸­çš„æˆåŠŸæ¡ˆä¾‹ï¼Œä½†è¿™äº›æµ‹è¯•åœºæ™¯ç›¸å¯¹æœ‰é™ã€‚è®ºæ–‡æœªç³»ç»Ÿæ€§åœ°é‡åŒ–è¯„ä¼°åœ¨**æç«¯å…‰ç…§å˜åŒ–ã€åŠ¨æ€éšœç¢ç‰©ï¼ˆå¦‚ç§»åŠ¨çš„è¡Œäººï¼‰ã€ä¸¥é‡è§†è§‰é€€åŒ–ï¼ˆå¦‚çƒŸé›¾ã€é•œé¢åå°„ï¼‰æˆ–å…¨æ–°æœªçŸ¥å»ºç­‘ç»“æ„**ä¸‹çš„æ€§èƒ½ã€‚å½“å‰çš„â€œsim-to-realâ€è¿ç§»ä¸»è¦ä¾èµ–äºåœ¨é«˜è´¨é‡ä»¿çœŸæ•°æ®é›†ï¼ˆInternData-N1ï¼‰ä¸Šçš„è®­ç»ƒå’Œæ·±åº¦ä¼°è®¡ç½‘ç»œï¼ˆBANetï¼‰çš„è¾…åŠ©ï¼Œå…¶é¢å¯¹çœŸå®ä¸–ç•Œæ— é™é•¿å°¾åˆ†å¸ƒé—®é¢˜çš„é²æ£’æ€§å°šæœªå¾—åˆ°å……åˆ†è¯æ˜ã€‚\n\n- **å¥–åŠ±å‡½æ•°çš„è®¾è®¡ä¾èµ–å…ˆéªŒçŸ¥è¯†ä¸æ‰‹å·¥è°ƒæ•´**ï¼šSIDPçš„æ ¸å¿ƒä¾èµ–äºä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼ˆè§è¡¨Iï¼‰ï¼Œè¯¥å‡½æ•°åŒ…å«äº†ç¢°æ’æƒ©ç½šã€æ­¥é•¿æˆæœ¬ã€è¿›åº¦å¥–åŠ±å’Œåœé å¥–åŠ±ç­‰å¤šä¸ªæ‰‹å·¥åŠ æƒï¼ˆ`Î»_col=10, Î»_step=0.5, ...`ï¼‰çš„ç»„ä»¶ã€‚è¿™ç§è®¾è®¡å­˜åœ¨ä¸¤ä¸ªé—®é¢˜ï¼š\n    1. **ä¸»è§‚æ€§ä¸åœºæ™¯ä¾èµ–æ€§**ï¼šå¥–åŠ±æƒé‡éœ€è¦æ ¹æ®ä»»åŠ¡å’Œåœºæ™¯è¿›è¡Œè°ƒä¼˜ï¼Œå¯èƒ½æ— æ³•æ³›åŒ–åˆ°æ‰€æœ‰å¯¼èˆªä»»åŠ¡ï¼ˆå¦‚éœ€è¦ä¸åŒå®‰å…¨/æ•ˆç‡æƒè¡¡çš„åœºåˆï¼‰ã€‚\n    2. **ç¨€ç–å¥–åŠ±ä¸‹çš„æ¢ç´¢é—®é¢˜**ï¼šå¯¹äºéå¸¸é•¿è§†è·æˆ–ç»“æ„å¤æ‚çš„å¯¼èˆªä»»åŠ¡ï¼ŒåŸºäºæœ€ç»ˆç»“æœçš„ç¨€ç–å¥–åŠ±å¯èƒ½ä½¿â€œè‡ªæˆ‘æ¨¡ä»¿â€æœºåˆ¶åœ¨åˆæœŸéš¾ä»¥é‡‡æ ·åˆ°é«˜è´¨é‡è½¨è¿¹ï¼Œä»è€Œå½±å“å­¦ä¹ æ•ˆç‡ã€‚è®ºæ–‡ä¸­é€šè¿‡â€œç›®æ ‡æ— å…³æ¢ç´¢â€éƒ¨åˆ†ç¼“è§£äº†æ­¤é—®é¢˜ï¼Œä½†æœªä»æ ¹æœ¬ä¸Šè§£å†³ã€‚\n\n- **è®­ç»ƒè¿‡ç¨‹è®¡ç®—æˆæœ¬ä¾ç„¶è¾ƒé«˜**ï¼šè™½ç„¶SIDPå¤§å¹…æå‡äº†**æ¨ç†æ•ˆç‡**ï¼Œä½†å…¶**è®­ç»ƒè¿‡ç¨‹**ä»éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚è®ºæ–‡æåˆ°è®­ç»ƒè€—æ—¶çº¦35å°æ—¶ï¼Œä½¿ç”¨äº†ä¸¤å—NVIDIA A100 GPUã€‚è¿™ä¸»è¦æºäºå…¶â€œåœ¨çº¿é‡‡æ ·-è¯„ä¼°-æ›´æ–°â€çš„å¾ªç¯éœ€è¦ä¸ç¯å¢ƒè¿›è¡Œå¤§é‡äº¤äº’æ¥ç”Ÿæˆå€™é€‰è½¨è¿¹å¹¶è®¡ç®—å¥–åŠ±ã€‚å¯¹äºéœ€è¦å¿«é€Ÿè¿­ä»£æˆ–æ•°æ®æœ‰é™çš„ç°å®åº”ç”¨ï¼Œè¿™ç§è®­ç»ƒå¼€é”€å¯èƒ½æ˜¯ä¸€ä¸ªç“¶é¢ˆã€‚\n\n- **ç­–ç•¥çš„è¡¨è¾¾èƒ½åŠ›ä¸å¤æ‚å¤šæ¨¡æ€åœºæ™¯çš„å¹³è¡¡**ï¼šSIDPé€šè¿‡è‡ªæˆ‘æ¨¡ä»¿æœºåˆ¶ä¿ƒä½¿è½¨è¿¹åˆ†å¸ƒâ€œ**ç±»å†…æ”¶æ•›**â€ï¼Œä»¥æå‡æ¨ç†æ•ˆç‡ã€‚è™½ç„¶è®ºæ–‡å¼ºè°ƒä¿ç•™äº†â€œ**ç±»é—´å¤šæ ·æ€§**â€ï¼Œä½†è¿™ç§å¯¹é«˜è´¨é‡è½¨è¿¹çš„èšç„¦åœ¨æç«¯å¤šæ¨¡æ€åœºæ™¯ä¸‹ï¼ˆå¦‚å­˜åœ¨å¤šä¸ªåŒç­‰æœ€ä¼˜ä½†æˆªç„¶ä¸åŒçš„è·¯å¾„ï¼‰å¯èƒ½ä¼š**è¿‡æ—©åœ°æŠ‘åˆ¶æ¢ç´¢**ï¼Œå¯¼è‡´ç­–ç•¥æ”¶æ•›åˆ°æŸä¸ªå±€éƒ¨æœ€ä¼˜æ¨¡å¼ï¼Œè€Œæ— æ³•å‘ç°å…¶ä»–åŒæ ·æœ‰æ•ˆçš„è·¯å¾„ã€‚è®ºæ–‡çš„è¯„ä¼°ä¸»è¦å…³æ³¨æˆåŠŸç‡å’Œè·¯å¾„é•¿åº¦ï¼Œå¯¹â€œå‘ç°æ‰€æœ‰å¯è¡Œè§£â€çš„èƒ½åŠ›è¯„ä¼°ä¸è¶³ã€‚\n\n- **å¯¹ç²¾ç¡®æ·±åº¦ä¿¡æ¯çš„ä¾èµ–**ï¼šæ¨¡å‹è¾“å…¥çŠ¶æ€ `s_t` åŒ…å«RGB-Då›¾åƒï¼Œå…¶ä¸­æ·±åº¦å›¾ `I_t^D` åœ¨çœŸå®ä¸–ç•Œä¸­ç”±ç«‹ä½“è§†è§‰å’ŒBANetç½‘ç»œä¼°è®¡å¾—åˆ°ã€‚è¿™æ„å‘³ç€ç³»ç»Ÿçš„æ€§èƒ½ä¸æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ç´§å¯†è€¦åˆã€‚åœ¨ç¼ºä¹çº¹ç†ã€é€æ˜æˆ–åå°„è¡¨é¢ç­‰æ·±åº¦ä¼°è®¡å®¹æ˜“å¤±æ•ˆçš„åœºæ™¯ï¼Œå¯¼èˆªæ€§èƒ½å¯èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚è®ºæ–‡æœªæ¢è®¨åœ¨ä»…æœ‰RGBè¾“å…¥æˆ–æ·±åº¦ä¿¡æ¯ä¸¥é‡å™ªå£°æƒ…å†µä¸‹çš„æ€§èƒ½ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œæœªæ¥çš„ç ”ç©¶å·¥ä½œå¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹å‘å±•å¼€ï¼š\n\n- **å¢å¼ºä»¿çœŸåˆ°ç°å®çš„è¿ç§»ä¸æ³›åŒ–èƒ½åŠ›**ï¼š\n    - å¼€å‘æ›´**å¤šæ ·åŒ–ã€å¯¹æŠ—æ€§æ›´å¼ºçš„ä»¿çœŸåŸºå‡†**ï¼Œæ¶µç›–æ›´æç«¯çš„è§†è§‰æ¡ä»¶ã€åŠ¨æ€å…ƒç´ å’Œè¯­ä¹‰å˜åŒ–ã€‚\n    - å¼•å…¥**é¢†åŸŸéšæœºåŒ–** æˆ– **é¢†åŸŸè‡ªé€‚åº”æŠ€æœ¯**ï¼Œåœ¨è®­ç»ƒé˜¶æ®µä¸»åŠ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å˜å¼‚ï¼Œä½¿ç­–ç•¥å¯¹æ„ŸçŸ¥ä¸ç¡®å®šæ€§æ›´åŠ é²æ£’ã€‚\n    - ç»“åˆ**åœ¨çº¿è‡ªé€‚åº”æœºåˆ¶**ï¼Œè®©æœºå™¨äººåœ¨éƒ¨ç½²åèƒ½åˆ©ç”¨å°‘é‡çœŸå®äº¤äº’æ•°æ®æŒç»­å¾®è°ƒç­–ç•¥ã€‚\n\n- **ä¼˜åŒ–å¥–åŠ±è®¾è®¡ä¸å­¦ä¹ èŒƒå¼**ï¼š\n    - æ¢ç´¢**é€†å¼ºåŒ–å­¦ä¹ **æˆ–**åŸºäºèƒ½é‡çš„æ¨¡å‹**ï¼Œä»ä¸“å®¶æ•°æ®æˆ–æˆåŠŸè½¨è¿¹ä¸­**è‡ªåŠ¨å­¦ä¹ å¥–åŠ±å‡½æ•°**ï¼Œå‡å°‘å¯¹æ‰‹å·¥è®¾è®¡çš„ä¾èµ–ã€‚\n    - ç ”ç©¶**åˆ†å±‚å¼ºåŒ–å­¦ä¹ **ä¸æ‰©æ•£æ¨¡å‹çš„ç»“åˆï¼Œå°†é•¿è§†è·ä»»åŠ¡åˆ†è§£ä¸ºå­ç›®æ ‡ï¼Œç¼“è§£ç¨€ç–å¥–åŠ±é—®é¢˜ã€‚\n    - å°è¯•å°†**è¯­è¨€æŒ‡ä»¤**æˆ–**è¯­ä¹‰åœ°å›¾**ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œå®ç°æ›´æ³›åŒ–ã€æ›´æ˜“æŒ‡å®šçš„ä»»åŠ¡ç›®æ ‡ï¼Œè¶…è¶Šç®€å•çš„ç‚¹ç›®æ ‡å¯¼èˆªã€‚\n\n- **æå‡è®­ç»ƒä¸é‡‡æ ·æ•ˆç‡**ï¼š\n    - è®¾è®¡**æ›´é«˜æ•ˆçš„ç¦»çº¿è‡ªæˆ‘æ¨¡ä»¿ç®—æ³•**ï¼Œä¾‹å¦‚æ„å»ºä¸€ä¸ªä¸æ–­æ‰©å¤§çš„**é«˜è´¨é‡è½¨è¿¹å›æ”¾ç¼“å†²åŒº**ï¼Œå‡å°‘æ¯æ¬¡è¿­ä»£æ‰€éœ€çš„åœ¨çº¿é‡‡æ ·æ•°é‡ã€‚\n    - æ¢ç´¢**æ¨¡å‹é¢„æµ‹æ§åˆ¶**æ¡†æ¶ï¼Œå°†SIDPä½œä¸ºçŸ­è§†è·è§„åˆ’å™¨è¿›è¡Œæ»šåŠ¨ä¼˜åŒ–ï¼Œç»“åˆè½»é‡çº§çš„ä¸–ç•Œæ¨¡å‹è¿›è¡Œè½¨è¿¹æ¨æ¼”ï¼Œè¿›ä¸€æ­¥å‡å°‘å¯¹å¤§è§„æ¨¡é‡‡æ ·çš„ä¾èµ–ã€‚\n\n- **å¹³è¡¡æ•ˆç‡ä¸å¤šæ¨¡æ€è¡¨è¾¾èƒ½åŠ›**ï¼š\n    - åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥**å¯æ§çš„å¤šæ ·æ€§æ­£åˆ™é¡¹**ï¼Œæ˜ç¡®é¼“åŠ±ç­–ç•¥åœ¨ä¿æŒé«˜è´¨é‡çš„åŒæ—¶è¦†ç›–ä¸åŒçš„è¡Œä¸ºæ¨¡å¼ã€‚\n    - ç ”ç©¶**æ¡ä»¶ç”Ÿæˆæœºåˆ¶**ï¼Œä¾‹å¦‚é€šè¿‡æ½œåœ¨å˜é‡æˆ–æŠ€èƒ½ç¼–ç ï¼Œå…è®¸ç”¨æˆ·åœ¨æ¨ç†æ—¶æ˜¾å¼æŒ‡å®šæˆ–é€‰æ‹©æœŸæœ›çš„è·¯å¾„æ¨¡å¼ï¼ˆå¦‚â€œæ¿€è¿›â€æˆ–â€œä¿å®ˆâ€ï¼‰ã€‚\n\n- **é™ä½å¯¹å®Œç¾æ„ŸçŸ¥çš„ä¾èµ–**ï¼š\n    - å¼€å‘**åŸºäºçº¯RGBæˆ–ç¨€ç–æ·±åº¦**è¾“å…¥çš„SIDPå˜ä½“ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹æˆ–å‡ ä½•å…ˆéªŒæ¥å¼¥è¡¥æ·±åº¦ä¿¡æ¯çš„ç¼ºå¤±ã€‚\n    - å°†**ä¸ç¡®å®šæ€§ä¼°è®¡**æ¨¡å—é›†æˆåˆ°ç­–ç•¥ä¸­ï¼Œä½¿è§„åˆ’è¿‡ç¨‹èƒ½å¤Ÿæ„ŸçŸ¥å¹¶å¤„ç†æ„ŸçŸ¥ç½®ä¿¡åº¦ï¼Œåœ¨ä¸ç¡®å®šåŒºåŸŸé‡‡å–æ›´ä¿å®ˆçš„ç­–ç•¥ã€‚\n\n- **æ‹“å±•ä»»åŠ¡èŒƒå›´ä¸æ™ºèƒ½æ°´å¹³**ï¼š\n    - å°†SIDPæ¡†æ¶åº”ç”¨äº**å…·èº«æ™ºèƒ½**ä¸­æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚**ç‰©ä½“æ“ä½œå¯¼èˆª**ã€**å¤šä»»åŠ¡è¿ç»­å¯¼èˆª**æˆ–**äººæœºåä½œå¯¼èˆª**ã€‚\n    - æ¢ç´¢ä¸**å¤§è¯­è¨€æ¨¡å‹**æˆ–**è§†è§‰-è¯­è¨€æ¨¡å‹**çš„æ·±åº¦èåˆï¼Œå®ç°åŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„å¼€æ”¾ä¸–ç•Œå¯¼èˆªä¸æ¨ç†ã€‚\n\n**æ€»ç»“**ï¼šSIDPåœ¨æå‡æ‰©æ•£ç­–ç•¥çš„æ¨ç†æ•ˆç‡å’Œè®­ç»ƒé²æ£’æ€§æ–¹é¢è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œä½†å…¶åœ¨**æç«¯æ³›åŒ–æ€§ã€å¥–åŠ±è®¾è®¡è‡ªåŠ¨åŒ–ã€è®­ç»ƒæˆæœ¬ä»¥åŠå¤æ‚å¤šæ¨¡æ€å¹³è¡¡**ç­‰æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœªæ¥çš„å·¥ä½œåº”è‡´åŠ›äºæ„å»º**æ›´è‡ªé€‚åº”ã€æ›´é«˜æ•ˆã€å¯¹æ„ŸçŸ¥ç¼ºé™·æ›´é²æ£’**ï¼Œä¸”èƒ½å¤„ç†**æ›´å¼€æ”¾ã€æ›´å¤æ‚ä»»åŠ¡**çš„ä¸‹ä¸€ä»£è§†è§‰å¯¼èˆªç³»ç»Ÿã€‚",
            "timestamp": "2026-02-03T16:00:02.622497",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T13:27:59Z",
    "created_at": "2026-02-03T15:53:08.611719",
    "updated_at": "2026-02-03T15:53:08.611727"
}