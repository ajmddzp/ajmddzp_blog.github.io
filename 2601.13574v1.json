{
    "id": "2601.13574v1",
    "title": "Highly Deformable Proprioceptive Membrane for Real-Time 3D Shape Reconstruction",
    "authors": [
        "Guanyu Xu",
        "Jiaqi Wang",
        "Dezhong Tong",
        "Xiaonan Huang"
    ],
    "abstract": "é‡å»ºç‰©ä½“è¡¨é¢çš„ä¸‰ç»´å‡ ä½•å½¢æ€å¯¹æœºå™¨äººæ„ŸçŸ¥è‡³å…³é‡è¦ï¼Œä½†åŸºäºè§†è§‰çš„æ–¹æ³•åœ¨ä½å…‰ç…§æˆ–é®æŒ¡æ¡ä»¶ä¸‹é€šå¸¸ä¸å¯é ã€‚è¿™ä¸€å±€é™æ€§ä¿ƒä½¿æˆ‘ä»¬è®¾è®¡ä¸€ç§æœ¬ä½“æ„ŸçŸ¥è–„è†œï¼Œå®ƒèƒ½è´´åˆç›®æ ‡è¡¨é¢ï¼Œå¹¶é€šè¿‡é‡æ„è‡ªèº«å½¢å˜æ¥æ¨æ–­ä¸‰ç»´å‡ ä½•å½¢æ€ã€‚ä¼ ç»Ÿçš„å½¢çŠ¶æ„ŸçŸ¥è–„è†œé€šå¸¸ä¾èµ–ç”µé˜»ã€ç”µå®¹æˆ–ç£æ•æœºåˆ¶ï¼Œä½†è¿™äº›æ–¹æ³•å¸¸é¢ä¸´ç»“æ„å¤æ‚ã€å¤§å°ºåº¦å½¢å˜æ—¶é¡ºåº”æ€§æœ‰é™ä»¥åŠæ˜“å—ç”µç£å¹²æ‰°ç­‰æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºå…‰æ³¢å¯¼ä¼ æ„Ÿçš„æŸ”è½¯ã€æŸ”æ€§ä¸”å¯æ‹‰ä¼¸çš„æœ¬ä½“æ„ŸçŸ¥ç¡…èƒ¶è–„è†œã€‚è¯¥è–„è†œä¼ æ„Ÿå™¨é›†æˆè¾¹ç¼˜å®‰è£…çš„LEDä¸ä¸­å¿ƒåˆ†å¸ƒçš„å…‰ç”µäºŒæç®¡ï¼Œé€šè¿‡åµŒå…¥å¤šå±‚å¼¹æ€§å¤åˆææ–™çš„æ¶²æ€é‡‘å±è¿¹çº¿ç›¸äº’è¿æ¥ã€‚é€šè¿‡æ•°æ®é©±åŠ¨æ¨¡å‹è§£ç ä¸°å¯Œçš„å½¢å˜ç›¸å…³å…‰å¼ºä¿¡å·ï¼Œå¯å°†è–„è†œå‡ ä½•å½¢æ€é‡å»ºä¸ºä¸‰ç»´ç‚¹äº‘ã€‚åœ¨å®šåˆ¶çš„140æ¯«ç±³æ–¹å½¢è–„è†œä¸Šï¼Œç³»ç»Ÿä»¥90èµ«å…¹é¢‘ç‡å®æ—¶é‡å»ºå¤§å°ºåº¦é¢å¤–å˜å½¢ï¼Œé€šè¿‡å€’è§’è·ç¦»æµ‹é‡å¹³å‡é‡å»ºè¯¯å·®ä¸º1.3æ¯«ç±³ï¼Œåœ¨å‡¹é™·æ·±åº¦è¾¾25æ¯«ç±³æ—¶ä»ä¿æŒç²¾åº¦ã€‚è¯¥æ¡†æ¶ä¸ºå¯å˜å½¢æœºå™¨äººç³»ç»Ÿçš„å…¨å±€å½¢çŠ¶æ„ŸçŸ¥æä¾›äº†å¯æ‰©å±•ã€é²æ£’ä¸”ä½å‰–é¢çš„è§£å†³æ–¹æ¡ˆã€‚",
    "url": "https://arxiv.org/abs/2601.13574v1",
    "html_url": "https://arxiv.org/html/2601.13574v1",
    "html_content": "Highly Deformable Proprioceptive Membrane for Real-Time 3D Shape Reconstruction\nAbstract\nReconstructing the three-dimensional (3D) geometry of object surfaces is essential for robot perception, yet vision-based approaches are generally unreliable under low illumination or occlusion. This limitation motivates the design of a proprioceptive membrane that conforms to the surface of interest and infers 3D geometry by reconstructing its own deformation. Conventional shape-aware membranes typically rely on resistive, capacitive, or magneto-sensitive mechanisms. However, these methods often encounter challenges such as structural complexity, limited compliance during large-scale deformation, and susceptibility to electromagnetic interference. This work presents a soft, flexible, and stretchable proprioceptive silicone membrane based on optical waveguide sensing. The membrane sensor integrates edge-mounted LEDs and centrally distributed photodiodes (PDs), interconnected via liquid-metal traces embedded within a multilayer elastomeric composite. Rich deformation-dependent light intensity signals are decoded by a data-driven model to recover the membrane geometry as a 3D point cloud. On a customized 140 mm square membrane, real-time reconstruction of large-scale out-of-plane deformation is achieved at 90 Hz with an average reconstruction error of 1.3 mm, measured by Chamfer distance, while maintaining accuracy for indentations up to 25 mm. The proposed framework provides a scalable, robust, and low-profile solution for global shape perception in deformable robotic systems.\nkeywords:\nshape sensing, optical waveguide, 3D shape reconstruction, liquid metal, soft robotics\nGuanyu Xu\nJiaqi Wang\nDezhong Tong\nXiaonan Huang*\n{affiliations}\nHybrid Dynamic Robotics Lab, Robotics Department, University of Michigan, Ann Arbor, MI\n48109, USA\nEmail Address: xiaonanh@umich.edu\n1\nIntroduction\nPerceiving the geometry of large-scale 3D surfaces is a fundamental capability in robotics, as it provides rich information for both physical-interaction reasoning and internal state estimation\n[\n48\n]\n. In robot manipulation, reconstructing the surface geometry of external objects supports tasks such as object pose estimation\n[\n35\n,\n25\n]\nand grasp planning\n[\n2\n,\n44\n]\n. In soft robotics, integrating shape-sensing capabilities into soft robotic actuators or soft robots provides proprioceptive feedback for state estimation and closed-loop control\n[\n14\n,\n20\n,\n39\n]\n. In addition, shape sensing can provide direct information about the properties of objects in contact\n[\n45\n,\n46\n]\n, as well as enable inference of force distribution or contact locations\n[\n8\n,\n27\n]\n. Beyond robotics, shape sensing can also be integrated into wearable devices for healthcare monitoring\n[\n3\n,\n26\n]\n. While vision-based methods remain powerful and widely used for general 3D surface reconstruction\n[\n50\n,\n36\n,\n37\n,\n19\n]\n, they typically require proper illumination and unobstructed views\n[\n32\n]\n, conditions that can be difficult to guarantee in many aforementioned application scenarios. An alternative approach is to use a conformable, thin membrane that closely adheres to the surface of interest, thereby capturing its geometry through the membraneâ€™s own deformation. These considerations motivate\nproprioceptive shape sensing\n, in which a thin membrane is sensorized to reconstruct its deformation directly from internal measurements.\nMultiple methods have been explored to sensorize thin membranes for proprioceptive 3D shape reconstruction. Early work\n[\n24\n,\n12\n]\nassembled multiple rigid modules into flexible, mesh-like structures, in which each module embedded an inertial measurement unit (IMU) to estimate local orientation. However, such designs are structurally complex and noncompliant, limiting their applicability. Subsequently, researchers developed resistive\n[\n28\n,\n31\n]\n, capacitive\n[\n13\n]\n, and magnetic\n[\n43\n]\nsensing mechanisms that typically measure the spatial distribution of local strain and recover the global geometry. Similarly, Shah et al. introduced a stretchable shape-sensing (S3) sheet\n[\n33\n]\nthat integrates discrete IMUs and capacitive sensors into a single stretchable film. While these approaches leverage soft, stretchable substrates, they often lack robustness because their sensing mechanisms are inherently susceptible to electromagnetic interference (EMI) and signal cross-talk\n[\n34\n]\n. In contrast, optical waveguide-based approaches\n[\n41\n]\nhave recently attracted increasing attention, as they transmit deformation information via optical signals that are intrinsically immune to EMI and often enable relatively simple fabrication processes.\nAcross prior work, optical waveguide sensing strategies broadly fall into two regimes: wavelength-based sensing, in which deformation shifts a characteristic wavelength, and intensity-based sensing, in which deformation modulates the transmitted light intensity. Wavelength-based waveguide sensing typically relies on structures that reflect or transmit a characteristic wavelength that shifts in response to deformations. Fiber Bragg gratings (FBGs) are a representative example: axial strain modulates the Bragg wavelength, enabling high-resolution deformation measurements\n[\n11\n]\n, which have been explored for a variety of shape sensing applications\n[\n21\n,\n40\n,\n9\n,\n23\n]\n. However, because this approach often requires embedding optical fibers into the membrane, the resulting composite becomes less compliant and stretchable, thereby constraining large-scale surface deformation. In addition, wavelength-based sensing generally requires specialized and bulky hardware for spectral analysis, which precludes its use in fully untethered designs.\nIn contrast, intensity-based sensing is more commonly adopted, as it can be implemented using simple light emitters (e.g., LEDs) and detectors (e.g., photodiodes) that readily integrate into soft structures. In the canonical one-dimensional (1D) configuration\n[\n18\n,\n16\n,\n49\n,\n10\n,\n17\n]\n, an LED is installed at one end of the waveguide, and a photodiode at the other. Deformationâ€“bending, stretching, or twistingâ€“can be inferred by comparing the detected intensity to a baseline. By integrating single or multiple waveguides into soft pneumatic actuators, researchers have demonstrated reliable estimation of multi-degree-of-freedom (multi-DOF) motion, leveraging the mechanical compatibility between soft waveguides and pneumatic bodies. Beyond 1D waveguide structures, intensity-based sensing has also been explored for\nsurface\nshape reconstruction using distributed optoelectronic layouts\n[\n22\n]\n. In these approaches, multiple LEDs and photodiodes are sparsely embedded within a two-dimensional waveguide structure to generate a deformation-dependent signal set, which is then mapped to global geometry using data-driven models (e.g., neural network). While these studies establish the feasibility of waveguide-based shape reconstruction, most focus on bending-dominated deformation.\nFigure 1:\nDesign of highly deformable optical waveguide sensor for surface shape reconstruction.\n(A).\nPrototype system of the sensorized optical waveguide membrane, together with an example reconstruction: top view and cross sections, compare the ground-truth shape with the reconstructed point cloud.\n(B).\nOverview of the sensing principle: edge-mounted LEDs inject light into the transparent core of the waveguide, and embedded photodiodes measure the deformation-induced change in light intensity.\n(C).\nâ„“\n\\ell\nLEDs are strobed one at a time, and\np\np\nphotodiodes are sampled simultaneously for each LED. A full scan of all LEDs forms a\np\nÃ—\nâ„“\np\\times\\ell\nmeasurement matrix, which is then fed into a neural network to reconstruct the surface shape point cloud.\n(D).\nPhotograph of the sensor under representative deformations (bend, twist, and stretch), which shows the potential of operation under highly-deformed scenarios.\nOur work builds on intensity-based shape sensing and extends it toward\nhighly stretchable\nmembranes for proprioceptive shape reconstruction. As shown in Figure\n1\nA-B, we present a deformable optical waveguide sheet embedded with edge-mounted LEDs and centrally distributed PDs, capable of reconstructing global deformation directly from internal measurements. When the membrane deforms, changes in light transport modulate the intensity distribution across the membrane, which is detected by the PDs. By sequentially activating the LEDs and recording the corresponding light intensities at the PDs, we obtain a rich set of deformation-dependent signals. To map these signals to the global shape of the sensor, we adopt a data-driven neural network-based reconstruction approach that captures the complex, nonlinear relationship between optical transmission patterns and large-scale deformation (Figure\n1\nC). On a\n140\nÃ—\n140\n140\\times 140\nmm sensor prototype, we demonstrate that our learned shape reconstruction model achieves an average reconstruction error of\nâ‰ˆ\n\\approx\n1.3 mm, measured by the Chamfer distance, and remains accurate under out-of-plane indentations up to 25 mm. The reconstruction model runs at 5-7 ms per frame on a host PC with an NVIDIA T1200 Laptop GPU, yielding an end-to-end update rate of about 90 Hz. We also demonstrate that the membrane remains functional under large deformations, including bending, twisting, and stretching (Figure\n1\nD), endowing the design with the capability to reconstruct complex deformation modes.\nThe main contributions of this work are as follows:\n1.\nWe propose a soft, highly deformable proprioceptive membrane that is capable of reconstructing large deformations of its own shape from internal measurements.\n2.\nWe demonstrate that dense surface deformations can be reconstructed from sparse embedded optical measurements using a deep learning-based approach.\n3.\nWe develop a scalable method for embedding functional electronics into stretchable structures using flexible printed circuit board (PCB) connectors and liquid metal interconnects.\n2\nMethod\n2.1\nDesign and Fabrication of Membrane Sensor\nThis section describes the design and fabrication of our highly deformable optical waveguide membrane, together with the electronics and readout strategy used to acquire deformation-dependent signals. We first present the mechanical architecture of the waveguide stack, which confines light within a compliant transparent core while rejecting external illumination through reflective and light-shielding layers. Then, we describe the layer-by-layer fabrication process and the integration of stretchable electronics, including liquid metal-based interconnects, flexible breakout interfaces, and edge-mounted optoelectronic components that preserve membrane stretchability. Finally, we present the electrical system and time-division multiplexed readout scheme that rapidly scans LEDs and samples all photodiodes to form a high-dimensional measurement vector for downstream shape reconstruction.\n2.1.1\nMechanical System Design\nThe waveguide structure is designed to confine light propagation within its transparent core while reducing sensitivity to external illumination. As shown in Figure\n2\nA, the device adopts a five-layer stack: a transparent elastomer core is sandwiched between two reflective (white) layers, while two outer cladding (black) layers provide light shielding. The reflective layers enhance internal reflection at the core interfaces and reduce optical loss to the outer cladding, whereas the black cladding attenuates incident ambient light. The core is made of Ecoflex 00-45 Near Clear (Smooth-On, Inc.), a silicone rubber that exhibits high optical transmittance while remaining compliant under large deformations\n[\n38\n]\n. The reflective layers are fabricated from a 3:1 mixture of 00-45 and Print-On White Silicon Ink (Raw Material Suppliers) while the cladding layers consist of a 10:1 mixture of 00-45 and ELASTOSIL LR 3162 (Wacker Chemie AG). These formulations are selected empirically through iterative prototyping to achieve high reflectance in the reflective layers and high optical attenuation in the cladding, thereby improving internal light confinement and external light rejection.\nLight-emitting diodes (LEDs) and photodiodes (PDs) are embedded within the transparent layer. To maintain stretchability, electrical connections between components are implemented using oxidized gallium-indium alloy (OGaIn)\n[\n42\n]\npatterned on a stretchable substrate (VHB4905 double-sided tape, 3M). Each bare optoelectronic component interfaces with the OGaIn traces via a small flexible PCB breakout (5 mm footprint), which provides strain relief and a mechanically robust electrical connection. For LEDs, the flexible PCB is folded by 90\nâˆ˜\nto orient the emitter vertically toward the waveguide, thereby improving light injection efficiency. The sensor connects to the data acquisition board through a flexible flat cable (FFC, 40-pin, 1 mm pitch). To ensure adequate spacing and reduce the risk of shorting between adjacent conductors, the number of effective connectors is limited to 20. Photodiodes require per-channel wiring (two additional wires per PD), whereas the addressable LED chain shares common bus lines. Consequently, connector availability primarily constrains the number of photodiodes rather than the number of LEDs.\n2.1.2\nElectrical System Design\nElectrical components are carefully selected to achieve a high sampling rate while remaining compatible with the multiplexing-based data readout scheme. We use SK9822-EC20 addressable LEDs (SPI interface, up to 30 MHz) connected in a daisy chain for simplified wiring and high-speed multiplexing. Light-intensity signals are acquired using ADPD2211ACPZ-R7 photodiodes and digitized by an analog-to-digital converter (ADC). A 24-bit ADC (AD7175-8, 24-bit, 25 kSPS) is used to resolve small intensity variations at photodiodes far from the active LED, where optical propagation loss results in significantly lower signal levels than those at nearby channels. A microcontroller board (NUCLEO-L432KC, STMicroelectronics) coordinates LED updates, ADC sampling, and data transmission to the host PC.\nSensor readout is based on time-division multiplexing. At each illumination step, a single LED is activated, and the responses of all photodiode channels are sampled simultaneously. Each LED is activated for approximately 180\nÎ¼\nâ€‹\ns\n\\mathrm{\\mu s}\n, which is the minimum duration required to sample all photodiode channels once. This procedure then advances to the next LED until the entire LED array is scanned, yielding one measurement for each LED-PD pair. These measurements are concatenated into a single high-dimensional measurement vector. This scheme maximally decouples the optical signals from different LEDs, providing a richer set of independent measurements that benefits the reconstruction model. The complete measurement data are transmitted to the host PC via UART with DMA at a baud rate of 2 Mbit/s. The electrical schematic is shown in Figure\n2\nB.\nFigure 2:\nHardware architecture and data acquisition system.\n(A).\nExploded CAD view of the optical waveguide sensor showing internal layer stack, photodiode and LED array, stretchable OGaIn interconnects, and flexible flat cable interface. Flexible PCB connectors are designed to fold upward to mount LEDs vertically in the waveguide.\n(B).\nPhotograph of the sensor prototype with the electrical wiring overlaid. An STM32 microcontroller sequences the LEDs and coordinates photodiode sampling via the external ADC, producing a complete measurement frame by concatenating responses across all LED-photodiode pairs.\n2.1.3\nSensor Fabrication\nThe sensor fabrication process is illustrated in Figure\n3\n. The sensor is fabricated layer by layer in a 3D-printed square mold (depth: 6 mm; width: 140 mm). First, the bottom black cladding layer is spin-coated at 200 rpm and partially cured at 60\nC\nâˆ˜\n{}^{\\circ}\\mathrm{C}\nfor 15 min (Figure\n3\nA). Next, a reflective white layer is spin-coated on top of the black layer and fully cured at 25\nC\nâˆ˜\n{}^{\\circ}\\mathrm{C}\nfor 240 min (Figure\n3\nB). Stretchable electronics are prepared on a VHB substrate. A sheet of VHB 4905 is cut to\n135\nÃ—\n135\n135\\times 135\nmm and placed on top of the cured white layer (Figure\n3\nC). The VHB sheet is intentionally undersized to leave a margin for an Ecoflex 00-45 perimeter seal. Because adhesion between VHB and silicone is relatively weak, a full-size VHB sheet is prone to delamination during peeling. Sealing the perimeter with elastomer improves inter-layer integrity. The release liner on one side of the VHB tape serves as a mask for applying OGaIn traces. It is first laser-cut to define the trace pattern and then peeled to expose the adhesive regions designated for the traces (Figure\n3\nD). OGaIn is sequentially painted onto the mask to fill the patterned regions (Figure\n3\nE). After removing the mask, the patterned OGaIn traces remain on the VHB substrate (Figure\n3\nF). Electrical continuity is verified using a multimeter before the flexible flat cable (FFC) and optoelectronic components are placed at desired locations (Figure\n3\nG). No soldering is used; instead, the components are directly adhered to the VHB substrate, which provides mechanical fixation via its pressure-sensitive adhesive. After circuit integration, the transparent core is poured into the mold and partially cured (Figure\n3\nH). The top reflective white layer and top black cladding layers are then cast and cured using the same formulations as the bottom layers (Figure\n3\nI-J). After full curing, the membrane is demolded to obtain the final sensor.\nFigure 3:\nManufacturing process of the optical waveguide sensor.\n(A).\nSpin coat the bottom cladding (black) layer in a 3D-printed mold.\n(B).\nSpin coat the bottom reflective (white) layer on top of the black layer.\n(C).\nPlace a piece of VHB-4905 sheet on the cured white layer and seal the side with elastomer (Ecoflex 00-45).\n(D).\nCut circuit traces on the sticker paper of the VHB using a laser cutter.\n(E).\nPaint liquid metal (OGaIn) on the sticker paper to fill the traces.\n(F).\nPeel the sticker paper and leave the OGaIn traces on the VHB.\n(G).\nPlace LEDs and PDs flexible PCB connectors on the circuit and test the circuit.\n(H).\nCast the transparent core of the waveguide.\n(I).\nCast the top reflective layer.\n(J).\nCast the top cladding layer, and demold the entire sensor after it is fully cured.\n2.2\nData-driven Model for Shape Reconstruction\nTo accurately reconstruct the three-dimensional morphology of the sensor from high-dimensional optical signals, we propose a two-stage data-driven framework. The approach first establishes a low-dimensional latent space of feasible sensor deformations using a point-cloud autoencoder, effectively embedding geometric constraints into the model. Subsequently, a multi-layer perceptron (MLP) is employed to map the optical signals acquired from PDs to this learned latent space. This hierarchical strategy decouples the complex geometric reconstruction task from the cross-modal feature mapping, ensuring both computation efficiency and high reconstruction fidelity.\n2.2.1\nData Representation\nThe raw sensor data at each acquisition frame is structured as a measurement matrix\nğ‘¿\nâˆˆ\nâ„\np\nÃ—\nâ„“\n\\boldsymbol{X}\\in\\mathbb{R}^{p\\times\\ell}\n, obtained by sequentially scanning\nâ„“\n\\ell\nLEDs and sampling the responses of\np\np\nphotodiodes. For compatibility with deep learning architectures,\nğ‘¿\n\\boldsymbol{X}\nis vectorized into a feature vector\nğ’—\nâˆˆ\nâ„\np\nâ€‹\nâ„“\n\\boldsymbol{v}\\in\\mathbb{R}^{p\\ell}\n. While several representations of 3D surfaces exist, including polygon meshes\n[\n15\n]\nand volumetric occupancy grids\n[\n4\n]\n, we adopt a\npoint cloud\nrepresentation\nS\n=\n{\n(\nx\ni\n,\ny\ni\n,\nz\ni\n)\n}\ni\n=\n1\nM\nâˆˆ\nâ„\nM\nÃ—\n3\nS=\\left\\{(x_{i},y_{i},z_{i})\\right\\}_{i=1}^{M}\\in\\mathbb{R}^{M\\times 3}\nfor the sensor geometry\n[\n7\n]\n. This representation facilitates direct correspondence with depth-camera ground truth data and avoids the topological constraints often associated with fixed mesh structures. The learning objective is thus defined as identifying a mapping\nf\n:\nğ’—\nâ†¦\nS\npr\nf:\\boldsymbol{v}\\mapsto S_{\\mathrm{pr}}\nthat minimizes the discrepancy between the predicted surface\nS\npr\nS_{\\mathrm{pr}}\nand ground truth\nS\ngt\nS_{\\mathrm{gt}}\nwithin the dataset\nğ’Ÿ\n=\n{\n(\nğ’—\n(\ni\n)\n,\nS\ngt\n(\ni\n)\n)\n}\ni\n=\n1\nN\n\\mathcal{D}=\\{(\\boldsymbol{v}^{(i)},S_{\\mathrm{gt}}^{(i)})\\}_{i=1}^{N}\n.\nTo mitigate environmental noise and hardware-induced variance, we apply two preprocessing steps. First, a per-frame no-light offset is measured by turning off all LEDs once per frame and subtracting this offset from subsequent readings. Second, to address quantization noise in the acquisition circuitry, the raw ADC codes are denoised by discarding the seven least significant bits. This threshold is determined based on the photodiodeâ€™s intrinsic signal-to-noise ratio and the ADCâ€™s quantization error, effectively isolating meaningful physical signals from the underlying electronic noise floor. Finally, the feature vector\nğ’—\n\\boldsymbol{v}\nis channel-wise normalized using the training set statistics to ensure numerical stability during gradient descent. For computational efficiency, the ground-truth point cloud is downsampled with stride\ns\ns\nto\nM\ngt\nM_{\\mathrm{gt}}\npoints.\n2.2.2\nNetwork Architecture\n\\threesubsection\nStage 1: point-cloud autoencoder\nThe first stage focuses on learning a compact latent representation of the sensorâ€™s deformation space. As shown in Figure\n4\nA, given a ground-truth point cloud\nS\ngt\nâˆˆ\nâ„\nM\ngt\nÃ—\n3\nS_{\\mathrm{gt}}\\in\\mathbb{R}^{M_{\\mathrm{gt}}\\times 3}\n, an encoder\nE\nâ€‹\n(\nâ‹…\n)\nE\\left(\\cdot\\right)\nbased on PointNet\n[\n30\n]\narchitecture maps the input to a latent vector\nğ’›\nâˆˆ\nâ„\nL\n\\boldsymbol{z}\\in\\mathbb{R}^{L}\n. A decoder\nD\nâ€‹\n(\nâ‹…\n)\nD\\left(\\cdot\\right)\ncomposed of stacked transposed convolution layers then reconstructs a point cloud\nS\npr\n=\nD\nâ€‹\n(\nğ’›\n)\nâˆˆ\nâ„\nM\npr\nÃ—\n3\nS_{\\mathrm{pr}}=D(\\boldsymbol{z})\\in\\mathbb{R}^{M_{\\mathrm{pr}}\\times 3}\n. PointNet performs a point-wise operation on an unordered point set, thereby effectively extracting features of the point cloud. The autoencoder pre-training stage provides a shape prior that constrains reconstructions to lie on a learned manifold of feasible deformation.\n\\threesubsection\nStage 2: PD-to-latent regression\nOnce the latent representation of surface geometry is established, the autoencoder weights are frozen to preserve the learned shape prior. An MLP\nh\nâ€‹\n(\nâ‹…\n)\nh(\\cdot)\nis then trained to map the PD feature vector\nv\nv\nto the latent space, such that\nh\nâ€‹\n(\nğ’—\n)\n=\nğ’›\nh(\\boldsymbol{v})=\\boldsymbol{z}\n(Figure\n4\nB). During inference, the predicted latent vector is passed through the pre-trained decoder to obtain the reconstructed shape,\nS\npr\n=\nD\nâ€‹\n(\nğ’›\n)\n=\nD\nâ€‹\n(\nh\nâ€‹\n(\nğ’—\n)\n)\nS_{\\mathrm{pr}}=D\\left(\\boldsymbol{z}\\right)=D\\left(h\\left(\\boldsymbol{v}\\right)\\right)\n. This decomposition reduces the dimensionality of the regression target, transforming a high-dimensional coordinate prediction problem into a more tractable latent-space inference problem.\nFigure 4:\nData-driven model for surface shape reconstruction\n(A).\nStage 1: Autoencoder pre-training. A point-cloud autoencoder is trained using ground-truth surface point clouds. The encoder compresses the point cloud (shape:\n(\nM\ngt\n,\n3\n)\n(M_{\\mathrm{gt}},3)\n) into a latent vector (shape:\n(\nL\n,\n)\n(L,)\n), and the decoder reconstructs the point cloud (shape:\n(\nM\npr\n,\n3\n)\n(M_{\\mathrm{pr}},3)\n) from the latent space.\n(B).\nOptical signal to shape inference. Normalized photodiode measurements (shape:\n(\n150\n,\n0\n)\n(150,0)\n) are mapped to a latent vector using a multi-layer perceptron, and the pre-trained point-cloud decoder generates the predicted point cloud (shape:\n(\nM\npr\n,\n3\n)\n(M_{\\mathrm{pr}},3)\n).\n2.2.3\nTraining Objective and Evaluation Metrics\n\\threesubsection\nAutoencoder training\nThe autoencoder is supervised using the Chamfer distance (\nd\nCD\nd_{\\mathrm{CD}}\n), a commonly adopted metric that quantifies the bidirectional similarity between the predicted and ground-truth point clouds\n[\n7\n,\n1\n,\n47\n]\n.\nd\nCD\nâ€‹\n(\nS\ngt\n,\nS\npr\n)\n=\nâˆ‘\nğ’™\nâˆˆ\nS\npr\nmin\nğ’š\nâˆˆ\nS\ngt\nâ¡\nâ€–\nğ’™\nâˆ’\nğ’š\nâ€–\n2\n+\nâˆ‘\nğ’š\nâˆˆ\nS\ngt\nmin\nğ’™\nâˆˆ\nS\npr\nâ¡\nâ€–\nğ’™\nâˆ’\nğ’š\nâ€–\n2\n.\nd_{\\mathrm{CD}}(S_{\\mathrm{gt}},S_{\\mathrm{pr}})=\\sum_{\\boldsymbol{x}\\in S_{\\mathrm{pr}}}\\min_{\\boldsymbol{y}\\in S_{\\mathrm{gt}}}\\|\\boldsymbol{x}-\\boldsymbol{y}\\|^{2}+\\sum_{\\boldsymbol{y}\\in S_{\\mathrm{gt}}}\\min_{\\boldsymbol{x}\\in S_{\\mathrm{pr}}}\\|\\boldsymbol{x}-\\boldsymbol{y}\\|^{2}.\n(1)\nThe autoencoder minimizes\nd\nCD\nd_{\\mathrm{CD}}\nover the training set. The network is implemented in PyTorch\n[\n29\n]\nand trained for up to 100 epochs with a batch size of 64, using early stopping based on the validation loss. We employ the\nAdam\noptimizer with an initial learning rate of\n10\nâˆ’\n3\n10^{-3}\nand apply a\nReduceLROnPlateau\nscheduler (factor=0.2, patience=3) driven by the validation loss. These hyperparameters are selected empirically through iterative experiments to avoid local minima while maintaining reasonable convergence speed (typically within 20 epochs). The final model is selected from the epoch that achieves the lowest validation loss.\n\\threesubsection\nPD-to-latent regression training\nFor each training sample, the target latent vector is first computed as\nğ’›\n=\nE\nâ€‹\n(\nS\ngt\n)\n\\boldsymbol{z}=E\\left(S_{\\mathrm{gt}}\\right)\n. The MLP is then trained to minimize the mean squared error (MSE) between the predicted and target latent vectors.\nâ„’\nmse\nâ€‹\n(\nh\nâ€‹\n(\nğ’—\n)\n,\nğ’›\n)\n=\nâ€–\nh\nâ€‹\n(\nğ’—\n)\nâˆ’\nğ’›\nâ€–\n2\n\\mathcal{L}_{\\mathrm{mse}}\\left(h(\\boldsymbol{v}),\\boldsymbol{z}\\right)=\\|h(\\boldsymbol{v})-\\boldsymbol{z}\\|^{2}\n(2)\nThe MLP is trained under the same data partitioning and batch size as the autoencoder. We use stochastic gradient descent (SGD) with a momentum of 0.9 and an initial learning rate of\n10\nâˆ’\n3\n10^{-3}\n. A\nCosineAnnealingLR\nscheduler with\nT\nmax\n=\n100\nT_{\\max}=100\nis applied to enable gradual learning rate decay, facilitating fine-tuning of the regression mapping.\n\\threesubsection\nEvaluation Metrics\nReconstruction performance is evaluated on a held-out test dataset using the symmetric Chamfer distance (\nd\nCD\nd_{\\mathrm{CD}}\n). In addition to this global metric, we assess local reconstruction accuracy using a spatial error map computed as the nearest-neighbor (NN) distance from each predicted point to the ground-truth point set. This visualization reveals spatial regions where reconstruction errors are most pronounced.\n3\nResults\nWe evaluate the performance of the membrane sensor through a series of experiments conducted on a\n140\nâ€‹\nm\nâ€‹\nm\nÃ—\n140\nâ€‹\nm\nâ€‹\nm\n140\\mathrm{mm}\\times 140\\mathrm{mm}\nprototype equipped with\np\n=\n5\np=5\nPDs and\nâ„“\n=\n30\n\\ell=30\nLEDs. First, we characterize the sensorâ€™s sensitivity and repeatability using a gravity-loaded bending experiment to validate the underlying sensing principle. We then demonstrate high-fidelity 3D shape reconstruction under large out-of-plane deformations using a deep learning framework, establishing the sensorâ€™s capability to identify the geometry of objects in contact. Finally, we perform a feature ablation study using Shapley additive global importance (SAGE) to identify the redundancy among optoelectronic components and to provide design guidelines for future implementations. Collectively, these experiments demonstrate that the proposed sensor can resolve complex geometries with high fidelity while maintaining functionality under large-scale deformation.\n3.1\nSensor Characterization\nWe first conduct a gravity-loaded bending experiment to verify that the optical signals respond systematically to global deformation. As shown in Figure\n5\nA, the setup involves clamping one side of the sensor while allowing the remainder to bend under gravity. Different curvatures are generated by varying the wall angle\nÎ¸\n\\theta\nacross a range from\n0\nâˆ˜\n0^{\\circ}\nto\n150\nâˆ˜\n150^{\\circ}\nin\n30\nâˆ˜\n30^{\\circ}\nincrements. We compared two representative LED-PD pairs: one with the connecting line parallel to the bending direction, and another with it perpendicular.\nThe results, illustrated in Figure\n5\nB, show that PD readings decrease most significantly when bending occurs along the LED-PD line, whereas the pair oriented transverse to the bending direction remain largely stable. This directional sensitivity indicates that the information content of the optical signals depends on the\ndiversity of path orientations\nthrough the waveguide. Consequently, the layoutâ€”edge-mounted LEDs and centrally distributed PDsâ€”is validated as it generates a dense set of multi-directional paths capable of capturing large-scale, complex deformations. Moreover, the sensor exhibits high sensitivity and repeatability: for a fixed geometry, the PD readings remain constant within a narrow error band (\nÂ±\n0.35\n%\n\\pm 0.35\\%\nfull ADC scale). The systematic changes observed as\nÎ¸\n\\theta\nvaries indicate that global deformation predictably modulates internal light transport, providing a robust foundation for learning-based shape reconstruction.\nFigure 5:\nCharacterization of the waveguide sensor.\n(A).\nGravity-loaded bending setup: Left: wall angle controls the sensor curvature; Right: top-view layout of LEDs and photodiodes (PDs) with the bending direction indicated.\n(B).\nA comparison of PD responses versus wall angle between two representative LED-PD pairs. The pair aligned with the bending direction (LEDâ€‰Bâ€“PD3, green) shows a strong intensity change, while the transverse pair (LEDâ€‰Aâ€“PD3, pink) remains largely stable.\n(C).\nSingle feature (LED/PD) importance evaluation based on SAGE values and the relation with physical locations.\n(D).\nReconstruction performance under progressive feature inclusion. Top: PDs are added one-by-one with all LEDs enabled; Bottom: LEDs are added one-by-one with all PDs enabled. Curves compare different inclusion orders (Natural, SAGE-ranked, and reverse SAGE).\n(E).\nSample reconstruction error shown as a heatmap of nearest neighbor (NN) distance of representative reduced models with different numbers of LEDs.\n3.2\nIndentation Reconstruction\nTo evaluate the sensor in a realistic application context, we conduct an indentation reconstruction experiment. In this setup, all four sides of the membrane are clamped, as illustrated in Figure\n1\nA. The sensor remains highly stretchable while undergoing large out-of-plane deformations induced by indenting the membrane at its center. The shape reconstruction network is evaluated under this configuration.\n3.2.1\nDataset Generation\nTo generate diverse surface geometries, we manually press objects of various shapesâ€”including spheres, cylinders, cubes, triangular prisms, and U-shaped objectsâ€”into the membrane at different locations, orientations, and indentation depths (Figure\n7\nA). The resulting deformation magnitude, defined as\nÎ”\nâ€‹\nz\n=\nz\nmax\nâˆ’\nz\nmin\n\\Delta z=z_{\\max}-z_{\\min}\n, ranges from 0 to 25 mm, with a mean of 14.64 mm and a standard deviation of 4.49 mm.\nFor each deformation frame, we record the membrane surface geometry\nS\ngt\nS_{\\mathrm{gt}}\nalong with the photodiode measurement vector\nğ’—\nâˆˆ\nâ„\n150\n\\boldsymbol{v}\\in\\mathbb{R}^{150}\n. The sensor acquisition system operates at 90 Hz, while an Intel RealSense D435 depth camera captures ground-truth point clouds at 30 Hz. These two data streams are synchronized via software alignment: every depth frame is paired with the most recently timestamped optical measurement. The preprocessing steps described in Section\n2.2.1\nare applied to the raw PD readings prior to recording. In total, 343,189 paired samples are collected and partitioned into training and validation sets. A separate test set comprising 34,320 paired samples is collected to evaluate the modelâ€™s performance.\n3.2.2\nModel Setting\nA hyperparameter sweep was conducted to determine the optimal point-cloud resolution, characterized by the number of points in the point cloud\nM\nM\n, and latent dimension\nL\nL\n. While\nM\ngt\nM_{\\mathrm{gt}}\nand\nM\npr\nM_{\\mathrm{pr}}\nare generally independent, our experiments suggest that minimizing the discrepancy between\nM\ngt\nM_{\\mathrm{gt}}\nand\nM\npr\nM_{\\mathrm{pr}}\nby appropriately tuning the downsampling stride\ns\ns\nimproves reconstruction stability. We therefore evaluate\nM\npr\nM_{\\mathrm{pr}}\nat values of 1024, 2048, and 4096, corresponding to\nM\ngt\nM_{\\mathrm{gt}}\nvalues of 1444, 2116, and 5929, respectively, while varying the latent dimension\nL\nL\nfrom 128 to 1024. These numbers are chosen to maintain a reasonable computational cost. For each hyperparameter configuration, we train both the autoencoder and the PD-to-latent MLP as described in Section\n2.2.3\n, and evaluate reconstruction performance on the held-out test set using the symmetric Chamfer distance. Figure\n6\nA summarizes the results of a hyperparameter sweep: increasing the point-cloud resolution generally improves reconstruction accuracy, whereas the latent dimension has a comparatively minor effect. Based on these effects, we select the configuration with the best reconstruction performance (\nL\n=\n256\nL=256\n,\nM\npr\n=\n4096\nM_{\\mathrm{pr}}=4096\n, and\nM\npr\n=\n5929\nM_{\\mathrm{pr}}=5929\n) as the default model and use it in all subsequent evaluations.\nFigure 6:\nReconstruction accuracy summary:\n(A).\nHyperparameter sweep result showing the average chamfer distance (mean\nÂ±\n\\pm\nstd) of each\nL\n,\nM\nL,M\ncombination. The best configuration (\nL\n=\n256\n,\nM\n=\n4096\nL=256,M=4096\n) is selected as the default model for later analyses.\n(B).\nHistogram of per-sample Chamfer distance on the full test set.\n(C).\nViolin plot showing how reconstruction accuracy (in terms of Chamfer distance) evolves across different deformation ranges\nÎ”\nâ€‹\nz\n=\nz\nmax\nâˆ’\nz\nmin\n\\Delta z=z_{\\max}-z_{\\min}\n.\n3.2.3\nReconstruction Performance\nThe model achieves an average Chamfer distance (cd) of 1.307 mm on the held-out test set, with a median of 1.195 mm and a standard deviation of 0.396 mm. The error distribution, shown in Figure\n6\nB, is right-skewed, indicating that most reconstruction errors cluster between 1.0 and 1.5 mm. To assess robustness under large deformation, we analyze reconstruction performance across different deformation scales (Figure\n6\nC and Table\n1\n). The result shows that the sensor maintains stable accuracy even as\nÎ”\nâ€‹\nz\n=\nz\nmax\nâˆ’\nz\nmin\n\\Delta z=z_{\\max}-z_{\\min}\napproaches 25 mm, suggesting that the waveguide preserves signal integrity under high-strain conditions.\nÎ”\nâ€‹\nz\n\\Delta z\nBin (mm)\nCount\nMin\nQ1\nMedian\nQ3\nMax\nMean\nStd\n[\n0.00\n,\n10.00\n)\n[0.00,10.00)\n6060\n0.98\n1.17\n1.36\n1.76\n7.79\n1.61\n0.71\n[\n10.00\n,\n12.50\n)\n[10.00,12.50)\n4052\n0.98\n1.10\n1.21\n1.43\n7.30\n1.33\n0.37\n[\n12.50\n,\n15.00\n)\n[12.50,15.00)\n6516\n0.95\n1.08\n1.18\n1.37\n3.64\n1.27\n0.27\n[\n15.00\n,\n17.50\n)\n[15.00,17.50)\n8388\n0.95\n1.08\n1.16\n1.29\n5.65\n1.22\n0.20\n[\n17.50\n,\n20.00\n)\n[17.50,20.00)\n5895\n0.98\n1.10\n1.17\n1.27\n2.83\n1.21\n0.15\n[\n20.00\n,\n25.00\n)\n[20.00,25.00)\n3275\n0.99\n1.12\n1.18\n1.27\n2.29\n1.21\n0.14\n[\n25.00\n,\n+\nâˆ\n)\n[25.00,+\\infty)\n134\n1.02\n1.18\n1.27\n1.36\n1.70\n1.28\n0.15\nTable 1:\nPer-bin Chamfer distance statistics.\nThe sensorâ€™s capabilities are further illustrated in Figure\n7\n, which shows its performance in recognizing the geometry of different objects in contact, a representative scenario in tactile sensing. Figure\n7\nB compares ground-truth and reconstructed 3D shapes by overlaying the contours, demonstrating the sensorâ€™s ability to distinguish sharp edges and smooth curves. In addition, Figure\n7\nC provides a spatial error map based on the nearest neighbor (NN) distance and annotates the maximum NN error (nnd_max) for each scenario. These maps reveal that reconstruction errors are distributed relatively uniformly across the surface, with slightly increased errors in regions of high curvature, confirming the sensorâ€™s ability to capture complex global geometries.\nFigure 7:\nSample shape reconstruction results under representative indenters.\n(A).\nground-truth surface deformation during pressing, visualized as height contours (orange) overlaid on the sensor.\n(B).\nreconstructed 3D surface (top) with a planar contour comparison (bottom). The chamfer distance (cd) of each sample is labeled.\n(C).\nThe reconstruction error heat map and the maximum nearest neighbor distance (nnd_max) of each sample.\n3.3\nFeature Importance and Sensor Optimization\nWe further conduct an ablation study to understand the contributions of individual PDs and LEDs to the reconstruction network and to provide design guidance for future sensor iterations. The number of LEDs and photodiodes embedded within the waveguide is critical for obtaining a\nsufficient\nset of independent measurements of light intensity to reconstruct the global geometry. However, increasing the number of components also increases the fabrication complexity and limits data readout speed. Therefore, identifying the minimum number of LEDs and PDs that provide sufficient information for accurate shape reconstruction is essential for the efficient design of the membrane sensor.\n3.3.1\nFeature Importance via SAGE\nTo quantitatively evaluate the contribution of each optoelectronic component to reconstruction performance, we adopt the Shapley additive global importance (SAGE) value\n[\n6\n,\n5\n]\n, an\nadditive\n, model-agnostic metric that attributes the predictive power to individual input feature (or group of features). We compute the importance of each LED and PD in the trained PD-to-latent MLP\nh\nâ€‹\n(\nâ‹…\n)\nh(\\cdot)\nusing the group SAGE value evaluated over predefined feature sets. Specifically, the\ni\ni\n-th LED group is defined as\nğ’¢\nL\n(\ni\n)\n=\n{\np\nâ€‹\n(\ni\nâˆ’\n1\n)\n+\nj\n|\nj\n=\n1\n,\nâ€¦\n,\np\n}\n\\mathcal{G}_{L}^{(i)}=\\{p(i-1)+j|j=1,\\dots,p\\}\n, corresponding to the\np\np\nphotodiode readings collected when LED\ni\ni\nis activated. Similarly, the\nj\nj\n-th PD group is defined as\nğ’¢\nP\n(\nj\n)\n=\n{\np\nâ€‹\n(\ni\nâˆ’\n1\n)\n+\nj\n|\ni\n=\n1\n,\nâ€¦\n,\nâ„“\n}\n\\mathcal{G}_{P}^{(j)}=\\{p(i-1)+j|i=1,\\dots,\\ell\\}\n, which represents the reading of PD\nj\nj\nacross all\nâ„“\n\\ell\nLEDs. These groupings correspond physically to adding or removing a single LED or PD from the system. Let\nğ‘½\n=\n[\nV\n1\n,\nâ€¦\n,\nV\np\nâ€‹\nâ„“\n]\nâŠ¤\n\\boldsymbol{V}=[V_{1},\\dots,V_{p\\ell}]^{\\top}\ndenote the random vector of input features and let\nğ’\nâ‰œ\nE\nâ€‹\n(\nS\ngt\n)\n\\boldsymbol{Z}\\triangleq E(S_{\\mathrm{gt}})\ndenote the random vector of the latent code obtained by encoding the ground-truth surface\nS\ngt\nS_{\\mathrm{gt}}\n. We define the index set of all features as\nğ’©\n=\n{\n1\n,\nâ€¦\n,\np\nâ€‹\nâ„“\n}\n\\mathcal{N}=\\{1,\\dots,p\\ell\\}\n. For any subset\nğ’®\nâŠ†\nğ’©\n\\mathcal{S}\\subseteq\\mathcal{N}\n, the corresponding subset of features is denoted by\nğ‘½\nğ’®\nâ‰œ\n{\nV\ni\n|\ni\nâˆˆ\nğ’®\n}\n\\boldsymbol{V}_{\\mathcal{S}}\\triangleq\\{V_{i}|i\\in\\mathcal{S}\\}\n. The predictive power of a feature subset indexed by\nğ’®\n\\mathcal{S}\nis defined as\nu\nâ€‹\n(\nğ’®\n)\n=\nğ”¼\nâ€‹\n[\nâ„’\nmse\nâ€‹\n(\nh\nâˆ…\nâ€‹\n(\nv\nâˆ…\n)\n,\nZ\n)\n]\nâˆ’\nğ”¼\nâ€‹\n[\nâ„’\nmse\nâ€‹\n(\nh\nğ’®\nâ€‹\n(\nv\nğ’®\n)\n,\nZ\n)\n]\nu\\left(\\mathcal{S}\\right)=\\mathbb{E}\\left[\\mathcal{L}_{\\mathrm{mse}}\\left(h_{\\varnothing}\\left(v_{\\varnothing}\\right),Z\\right)\\right]-\\mathbb{E}\\left[\\mathcal{L}_{\\mathrm{mse}}\\left(h_{\\mathcal{S}}\\left(v_{\\mathcal{S}}\\right),Z\\right)\\right]\n(3)\nwhere\nh\nğ’®\nâ€‹\n(\nv\nğ’®\n)\n=\nğ”¼\nâ€‹\n[\nh\nâ€‹\n(\nV\n)\n|\nV\nğ’®\n=\nv\nğ’®\n]\nh_{\\mathcal{S}}\\left(v_{\\mathcal{S}}\\right)=\\mathbb{E}\\left[h(V)|V_{\\mathcal{S}}=v_{\\mathcal{S}}\\right]\ndenotes the reduced model obtained by marginalizing the excluded features\nV\nğ’©\n\\\nğ’®\nV_{\\mathcal{N}\\backslash\\mathcal{S}}\n. The SAGE value associated with a specific LED or PD group\nğ’¢\n\\mathcal{G}\nis then defined as\nÎ¦\nğ’¢\n=\n1\n|\nğ’©\n|\nâ€‹\nâˆ‘\nğ’®\nâŠ†\nğ’©\n\\\nğ’¢\n(\n|\nğ’©\n|\nâˆ’\n1\n|\nğ’®\n|\n)\nâˆ’\n1\nâ€‹\n[\nu\nâ€‹\n(\nğ’®\nâˆª\nğ’¢\n)\nâˆ’\nu\nâ€‹\n(\nğ’®\n)\n]\n\\Phi_{\\mathcal{G}}=\\frac{1}{|\\mathcal{N}|}\\sum_{\\mathcal{S}\\subseteq\\mathcal{N}\\backslash\\mathcal{G}}\\binom{|\\mathcal{N}|-1}{|\\mathcal{S}|}^{-1}\\left[u(\\mathcal{S}\\cup\\mathcal{G})-u(\\mathcal{S})\\right]\n(4)\n3.3.2\nGuidance on Sensor Layout\nUsing group SAGE value, we quantify the importance of each LED and PD and relate these scores to their physical locations within the sensor (Figure\n5\nC). Notably, PD group\nğ’¢\nP\n3\n\\mathcal{G}_{P}^{3}\n, which is located near the center of the membrane, exhibits the lowest importance value, and the LED groups\nğ’¢\nL\n15\n\\mathcal{G}_{L}^{15}\nto\nğ’¢\nL\n30\n\\mathcal{G}_{L}^{30}\ngenerally exhibit lower importance. We hypothesize that these reduced importance scores arise from the baseline light intensity measured by the photodiode: when the PDâ€™s baseline intensity is low, either due to large LED-PD distance or voltage drops along the LED power traces, small deformation-induced changes become harder to resolve relative to noise, thereby reducing the informativeness of those channels. This observation suggests that PD placement can be optimized to maintain baseline intensity within a favorable operating range, ensuring sufficient sensitivity for each measurement channel.\nWe further leverage this importance analysis to study how model performance scales with the number of optoelectronic components via a progressive feature-inclusion experiment. Specifically, we select\nK\nK\nPD or LED groups\nğ’¢\n(\ni\n)\n\\mathcal{G}^{(i)}\nto form a reduced feature index set\nğ’®\n=\nâ‹ƒ\ni\n=\n1\nK\nğ’¢\n(\ni\n)\n\\mathcal{S}=\\bigcup_{i=1}^{K}\\mathcal{G}^{(i)}\n, retrain the reduced MLP\nh\nğ’®\nh_{\\mathcal{S}}\n, and evaluate the reconstruction error of the composed model\nf\nğ’®\n=\nD\nâˆ˜\nh\nğ’®\nf_{\\mathcal{S}}=D\\circ h_{\\mathcal{S}}\nusing the average Chamfer distance on the test set. Feature groups are added in three orders: SAGE (most informative first), reverse SAGE (least informative first), and natural (numerical order). The results presented in Figure\n5\nD show that importance-aware inclusion yields a clear advantage at small\nK\nK\nâ€™s: selecting the most informative groups first generally achieves lower error than including the least informative groups. However, the marginal benefit of adding more optoelectronic components decreases as indicated by the plateau in the curve. The nearest-neighbor (NN) distance error maps shown in Figure\n5\nE also indicate that the regions of large reconstruction error shrink significantly as the most informative channels are incorporated, with diminishing improvements for large values of\nK\nK\n. Together, these results suggest that the optical signal becomes redundant after a moderate number of channels are added (i.e., 15 LEDs), providing a practical upper bound for future design. The LED count can be reduced substantially with minimal loss in accuracy, while PDs should be placed strategically to maintain high information-density measurements.\n4\nConclusion\nWe present a soft, highly deformable optical waveguide membrane sensor that enables proprioceptive reconstruction of large, global surface deformations from sparse embedded light-intensity measurements. The proposed hardware integrates a multilayer elastomeric waveguide stack with embedded LEDs and photodiodes, interconnected via liquid-metal traces and flexible PCB connectors, allowing the membrane to sustain large strains while maintaining robust electrical and optical functionality. An efficient time-division multiplexing scheme generates high-dimensional deformation-dependent signals, which are mapped to 3D surface geometry using a two-stage neural network architecture consisting of a point-cloud autoencoder and a photodiode-to-latent regressor. Experiments conducted on a\n140\nâ€‹\nm\nâ€‹\nm\nÃ—\n140\nâ€‹\nm\nâ€‹\nm\n140\\mathrm{mm}\\times 140\\mathrm{mm}\nprototype validate repeatable optical responses under controlled bending and demonstrate accurate shape reconstruction on a large-scale dataset, achieving a test-set Chamfer distance of 1.307 mm. Moreover, grouped SAGE analysis and progressive feature-inclusion studies confirm that the proposed\n30\nÃ—\n5\n30\\times 5\nLED/PD prototype provides sufficient, well-distributed information for accurate reconstruction of out-of-plane membrane deformations, supporting the effectiveness of our sensing architecture. Together, these results suggest that waveguide-based sensing, combined with learning-based decoding, offers a practical route toward thin, scalable, and EMI-robust shape sensors for soft robotic systems.\nWhile the proposed waveguide membrane sensor achieves accurate reconstructions on held-out test data, the current LED/PD layout is primarily guided by empirical design intuition. Section\n3.3\npartially addresses this limitation by quantifying group-level feature importance using SAGE and validating the ranking through progressive feature inclusion. However, a systematic method for determining globally optimal LED/PD placements under fabrication and wiring constraints remains an open problem. Given the strong reconstruction performance of our current prototype, we prioritize demonstrating the feasibility of waveguide-based global shape sensing, while leaving principled layout optimization for future work. In addition, the paired dataset is collected under a four-sided clamped configuration with predominantly out-of-plane deformation, a limitation imposed by the constraints of depth-camera-based ground-truth acquisition. Although the proposed learning-based reconstruction framework is agnostic to surface geometry and does not assume a specific deformation mode, the current experimental evaluation does not fully validate generalization to richer deformation families, such as stretching and twisting.\nIn future work, we plan to develop a scalable optical simulation framework for highly deformable waveguide membranes and leverage it to generate paired datasets spanning a broader range of deformation modes. Physics-grounded simulation, combined with real-data calibration, has the potential to substantially reduce the cost of collecting large training datasets and to enable systematic evaluation across deformation regimes that are difficult to capture experimentally. With richer data, the proposed point-cloud-based reconstruction architecture may naturally extend to stretching, twisting, and other complex deformation fields encountered in soft robotic applications. Furthermore, such a simulation framework could provide a rapid experimental sandbox for systematic optimization of LED and PD layouts, further improving sensor performance and scalability.\nReferences\n[1]\nP. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas\n(2018)\nLearning representations and generative models for 3d point clouds\n.\nIn\nInternational conference on machine learning\n,\npp.Â 40â€“49\n.\nCited by:\nÂ§2.2.3\n.\n[2]\nS. I. Ansary, A. Mishra, S. Deb, and A. K. Deb\n(2025)\nA framework for robotic grasping of 3d objects in a tabletop environment\n.\nMultimedia Tools and Applications\n84\n(\n22\n),\npp.Â 25865â€“25894\n.\nCited by:\nÂ§1\n.\n[3]\nM. D. Bartlett, E. J. Markvicka, and C. Majidi\n(2016)\nRapid fabrication of soft, multilayered electronics for wearable biomonitoring\n.\nAdvanced Functional Materials\n26\n(\n46\n),\npp.Â 8496â€“8504\n.\nCited by:\nÂ§1\n.\n[4]\nC. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese\n(2016)\n3d-r2n2: a unified approach for single and multi-view 3d object reconstruction\n.\nIn\nEuropean conference on computer vision\n,\npp.Â 628â€“644\n.\nCited by:\nÂ§2.2.1\n.\n[5]\nI. Covert, S. Lundberg, and S. Lee\n(2021)\nExplaining by removing: a unified framework for model explanation\n.\nJournal of Machine Learning Research\n22\n(\n209\n),\npp.Â 1â€“90\n.\nCited by:\nÂ§3.3.1\n.\n[6]\nI. Covert, S. M. Lundberg, and S. Lee\n(2020)\nUnderstanding global feature contributions with additive importance measures\n.\nAdvances in neural information processing systems\n33\n,\npp.Â 17212â€“17223\n.\nCited by:\nÂ§3.3.1\n.\n[7]\nH. Fan, H. Su, and L. J. Guibas\n(2017)\nA point set generation network for 3d object reconstruction from a single image\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 605â€“613\n.\nCited by:\nÂ§2.2.1\n,\nÂ§2.2.3\n.\n[8]\nB. Fang, J. Zhao, N. Liu, Y. Sun, S. Zhang, F. Sun, J. Shan, and Y. Yang\n(2025)\nForce measurement technology of vision-based tactile sensor\n.\nAdvanced Intelligent Systems\n7\n(\n1\n),\npp.Â 2400290\n.\nCited by:\nÂ§1\n.\n[9]\nJ. Guo, K. Zhao, B. Zhou, W. Ning, K. Jiang, C. Yang, L. Kong, and Q. Dai\n(2019)\nWearable and skin-mountable fiber-optic strain sensors interrogated by a free-running, dual-comb fiber laser\n.\nAdvanced Optical Materials\n7\n(\n12\n),\npp.Â 1900086\n.\nCited by:\nÂ§1\n.\n[10]\nA. Heiden, D. Preninger, L. Lehner, M. Baumgartner, M. Drack, E. Woritzka, D. Schiller, R. Gerstmayr, F. Hartmann, and M. Kaltenbrunner\n(2022)\n3D printing of resilient biogels for omnidirectional and exteroceptive soft actuators\n.\nScience Robotics\n7\n(\n63\n),\npp.Â eabk2119\n.\nCited by:\nÂ§1\n.\n[11]\nJ. Heo, J. Chung, and J. Lee\n(2006)\nTactile sensor arrays using fiber bragg grating sensors\n.\nSensors and Actuators A: Physical\n126\n(\n2\n),\npp.Â 312â€“327\n.\nCited by:\nÂ§1\n.\n[12]\nT. Hoshi and H. Shinoda\n(2008)\n3D shape measuring sheet utilizing gravitational and geomagnetic fields\n.\nIn\n2008 SICE Annual Conference\n,\npp.Â 915â€“920\n.\nCited by:\nÂ§1\n.\n[13]\nD. Hu, H. Dong, Z. Liu, Z. Chen, F. Giorgio-Serchi, and Y. Yang\n(2024)\nTouch and deformation perception of soft manipulators with capacitive e-skins and deep learning\n.\nIEEE Sensors Journal\n.\nCited by:\nÂ§1\n.\n[14]\nD. Hu, F. Giorgio-Serchi, S. Zhang, and Y. Yang\n(2023)\nStretchable e-skin and transformer enable high-resolution morphological reconstruction for soft robots\n.\nnature machine intelligence\n5\n(\n3\n),\npp.Â 261â€“272\n.\nCited by:\nÂ§1\n.\n[15]\nQ. Huang, H. Wang, and V. Koltun\n(2015)\nSingle-view reconstruction via joint analysis of image and shape collections.\n.\nACM Trans. Graph.\n34\n(\n4\n),\npp.Â 87â€“1\n.\nCited by:\nÂ§2.2.1\n.\n[16]\nT. Kim, S. Lee, T. Hong, G. Shin, T. Kim, and Y. Park\n(2020)\nHeterogeneous sensing in a multifunctional soft sensor for human-robot interfaces\n.\nScience robotics\n5\n(\n49\n),\npp.Â eabc6878\n.\nCited by:\nÂ§1\n.\n[17]\nH. Krauss and K. Takemura\n(2022)\nStretchable optical waveguide sensor capable of two-degree-of-freedom strain sensing mediated by a semidivided optical core\n.\nIEEE/ASME Transactions on Mechatronics\n27\n(\n4\n),\npp.Â 2151â€“2157\n.\nCited by:\nÂ§1\n.\n[18]\nH. Krauss and K. Takemura\n(2024)\nEnhanced model-free dynamic state estimation for a soft robot finger using an embedded optical waveguide sensor\n.\nIEEE Robotics and Automation Letters\n9\n(\n7\n),\npp.Â 6123â€“6129\n.\nCited by:\nÂ§1\n.\n[19]\nH. Liang, J. Cao, V. Goel, G. Qian, S. Korolev, D. Terzopoulos, K. N. Plataniotis, S. Tulyakov, and J. Ren\n(2025)\nWonderland: navigating 3d scenes from a single image\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 798â€“810\n.\nCited by:\nÂ§1\n.\n[20]\nL. Liu, X. Huang, X. Zhang, B. Zhang, H. Xu, V. M. Trivedi, K. Liu, Z. Shaikh, and H. Zhao\n(2025)\nModel-based 3d shape reconstruction of soft robots via distributed strain sensing\n.\nSoft Robotics\n.\nCited by:\nÂ§1\n.\n[21]\nT. L. T. Lun, K. Wang, J. D. Ho, K. Lee, K. Y. Sze, and K. Kwok\n(2019)\nReal-time surface shape sensing for soft and flexible structures using fiber bragg gratings\n.\nIEEE Robotics and Automation Letters\n4\n(\n2\n),\npp.Â 1454â€“1461\n.\nCited by:\nÂ§1\n.\n[22]\nC. Mak, Y. Li, K. Wang, M. Wu, J. D. Ho, Q. Dou, K. Sze, K. Althoefer, and K. Kwok\n(2024)\nIntelligent shape decoding of a soft optical waveguide sensor\n.\nAdvanced Intelligent Systems\n6\n(\n2\n),\npp.Â 2300082\n.\nCited by:\nÂ§1\n.\n[23]\nL. Massari, E. Schena, C. Massaroni, P. Saccomandi, A. Menciassi, E. Sinibaldi, and C. M. Oddo\n(2020)\nA machine-learning-based approach to solve both contact location and force in soft material tactile sensors\n.\nSoft robotics\n7\n(\n4\n),\npp.Â 409â€“420\n.\nCited by:\nÂ§1\n.\n[24]\nP. Mittendorfer and G. Cheng\n(2012)\n3D surface reconstruction for robotic body parts with artificial skins\n.\nIn\n2012 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp.Â 4505â€“4510\n.\nCited by:\nÂ§1\n.\n[25]\nP. K. Murali, B. Porr, and M. Kaboli\n(2025)\nShared visuo-tactile interactive perception for robust object pose estimation\n.\nThe International Journal of Robotics Research\n44\n(\n7\n),\npp.Â 1186â€“1216\n.\nCited by:\nÂ§1\n.\n[26]\nA. Padmanabha, A. Agarwal, C. Li, A. Williams, D. K. Patel, S. Chopkar, A. Wilson, A. Ozkan, W. Yuan, S. Choudhary,\net al.\n(2025)\nIn vivo skin 3-d surface reconstruction and wrinkle depth estimation using handheld high resolution tactile sensing\n.\nAdvanced Healthcare Materials\n,\npp.Â e04402\n.\nCited by:\nÂ§1\n.\n[27]\nK. Park, H. Yuk, M. Yang, J. Cho, H. Lee, and J. Kim\n(2022)\nA biomimetic elastomeric robot skin using electrical impedance and acoustic tomography for tactile sensing\n.\nScience Robotics\n7\n(\n67\n),\npp.Â eabm7187\n.\nCited by:\nÂ§1\n.\n[28]\nY. Park, B. Chen, and R. J. Wood\n(2012)\nDesign and fabrication of soft artificial skin using embedded microchannels and liquid conductors\n.\nIEEE Sensors journal\n12\n(\n8\n),\npp.Â 2711â€“2718\n.\nCited by:\nÂ§1\n.\n[29]\nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,\net al.\n(2019)\nPytorch: an imperative style, high-performance deep learning library\n.\nAdvances in neural information processing systems\n32\n.\nCited by:\nÂ§2.2.3\n.\n[30]\nC. R. Qi, H. Su, K. Mo, and L. J. Guibas\n(2017)\nPointnet: deep learning on point sets for 3d classification and segmentation\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 652â€“660\n.\nCited by:\nÂ§2.2.2\n.\n[31]\nC. Rendl, D. Kim, S. Fanello, P. Parzer, C. Rhemann, J. Taylor, M. Zirkl, G. Scheipl, T. RothlÃ¤nder, M. Haller,\net al.\n(2014)\nFlexSense: a transparent self-sensing deformable surface\n.\nIn\nProceedings of the 27th annual ACM symposium on User interface software and technology\n,\npp.Â 129â€“138\n.\nCited by:\nÂ§1\n.\n[32]\nM. F. Senussi, M. Abdalla, M. S. Kasem, M. Mahmoud, B. Yagoub, and H. Kang\n(2025)\nA comprehensive review on light field occlusion removal: trends, challenges, and future directions\n.\nIEEE Access\n.\nCited by:\nÂ§1\n.\n[33]\nD. Shah, S. J. Woodman, L. Sanchez-Botero, S. Liu, and R. Kramer-Bottiglio\n(2023)\nStretchable shape-sensing sheets\n.\nAdvanced intelligent systems\n5\n(\n12\n),\npp.Â 2300343\n.\nCited by:\nÂ§1\n.\n[34]\nB. Shih, D. Shah, J. Li, T. G. Thuruthel, Y. Park, F. Iida, Z. Bao, R. Kramer-Bottiglio, and M. T. Tolley\n(2020)\nElectronic skins and machine learning for intelligent soft robots\n.\nScience Robotics\n5\n(\n41\n),\npp.Â eaaz9239\n.\nCited by:\nÂ§1\n.\n[35]\nS. Suresh, H. Qi, T. Wu, T. Fan, L. Pineda, M. Lambeta, J. Malik, M. Kalakrishnan, R. Calandra, M. Kaess,\net al.\n(2024)\nNeuralFeels with neural fields: visuotactile perception for in-hand manipulation\n.\nScience Robotics\n9\n(\n96\n),\npp.Â eadl0628\n.\nCited by:\nÂ§1\n.\n[36]\nS. Szymanowicz, E. Insafutdinov, C. Zheng, D. Campbell, J. F. Henriques, C. Rupprecht, and A. Vedaldi\n(2025)\nFlash3d: feed-forward generalisable 3d scene reconstruction from a single image\n.\nIn\n2025 International Conference on 3D Vision (3DV)\n,\npp.Â 670â€“681\n.\nCited by:\nÂ§1\n.\n[37]\nT. Tu, M. Li, C. H. Lin, Y. Cheng, M. Sun, and M. Yang\n(2025)\nDreamo: articulated 3d reconstruction from a single casual video\n.\nIn\n2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)\n,\npp.Â 2269â€“2279\n.\nCited by:\nÂ§1\n.\n[38]\nJ. Vaicekauskaite, P. Mazurek, S. Vudayagiri, and A. L. Skov\n(2020)\nMapping the mechanical and electrical properties of commercial silicone elastomer formulations for stretchable transducers\n.\nJournal of Materials Chemistry C\n8\n(\n4\n),\npp.Â 1273â€“1279\n.\nCited by:\nÂ§2.1.1\n.\n[39]\nI. Van Meerbeek, C. De Sa, and R. Shepherd\n(2018)\nSoft optoelectronic sensory foams with proprioception\n.\nScience Robotics\n3\n(\n24\n),\npp.Â eaau2489\n.\nCited by:\nÂ§1\n.\n[40]\nK. Wang, C. Mak, J. D. Ho, Z. Liu, K. Sze, K. K. Wong, K. Althoefer, Y. Liu, T. Fukuda, and K. Kwok\n(2021)\nLarge-scale surface shape sensing with learning-based computational mechanics\n.\nAdvanced Intelligent Systems\n3\n(\n11\n),\npp.Â 2100089\n.\nCited by:\nÂ§1\n.\n[41]\nX. Wang, Z. Li, and L. Su\n(2024)\nSoft optical waveguides for biomedical applications, wearable devices, and soft robotics: a review\n.\nAdvanced Intelligent Systems\n6\n(\n1\n),\npp.Â 2300482\n.\nCited by:\nÂ§1\n.\n[42]\nS. J. Woodman, D. S. Shah, M. Landesberg, A. Agrawala, and R. Kramer-Bottiglio\n(2024)\nStretchable arduinos embedded in soft robots\n.\nScience Robotics\n9\n(\n94\n),\npp.Â eadn6844\n.\nCited by:\nÂ§2.1.1\n.\n[43]\nY. Yan, Z. Hu, Z. Yang, W. Yuan, C. Song, J. Pan, and Y. Shen\n(2021)\nSoft magnetic skin for super-resolution tactile sensing with force self-decoupling\n.\nScience Robotics\n6\n(\n51\n),\npp.Â eabc8801\n.\nCited by:\nÂ§1\n.\n[44]\nZ. Yin and Y. Li\n(2022)\nOverview of robotic grasp detection from 2d to 3d\n.\nCognitive Robotics\n2\n,\npp.Â 73â€“82\n.\nCited by:\nÂ§1\n.\n[45]\nL. Yu and D. Liu\n(2025)\nRecent progress in tactile sensing and machine learning for texture perception in humanoid robotics\n.\nInterdisciplinary Materials\n4\n(\n2\n),\npp.Â 235â€“248\n.\nCited by:\nÂ§1\n.\n[46]\nW. Yuan, S. Dong, and E. H. Adelson\n(2017)\nGelsight: high-resolution robot tactile sensors for estimating geometry and force\n.\nSensors\n17\n(\n12\n),\npp.Â 2762\n.\nCited by:\nÂ§1\n.\n[47]\nR. Zhang, U. Yoo, Y. Li, A. Argawal, and W. Yuan\n(2025)\nPneuGelSight: soft robotic vision-based proprioception and tactile sensing\n.\nThe International Journal of Robotics Research\n,\npp.Â 02783649251378153\n.\nCited by:\nÂ§2.2.3\n.\n[48]\nS. Zhang, Y. Yang, Y. Sun, N. Liu, F. Sun, and B. Fang\n(2025)\nArtificial skin based on visuo-tactile sensing for 3d shape reconstruction: material, method, and evaluation\n.\nAdvanced Functional Materials\n35\n(\n1\n),\npp.Â 2411686\n.\nCited by:\nÂ§1\n.\n[49]\nH. Zhao, K. Oâ€™brien, S. Li, and R. F. Shepherd\n(2016)\nOptoelectronically innervated soft prosthetic hand via stretchable optical waveguides\n.\nScience robotics\n1\n(\n1\n),\npp.Â eaai7529\n.\nCited by:\nÂ§1\n.\n[50]\nY. Zhou, D. Ye, H. Zhang, X. Xu, H. Sun, Y. Xu, X. Liu, and Y. Zhou\n(2025)\nRecurrent diffusion for 3d point cloud generation from a single image\n.\nIEEE Transactions on Image Processing\n.\nCited by:\nÂ§1\n.",
    "preview_text": "Reconstructing the three-dimensional (3D) geometry of object surfaces is essential for robot perception, yet vision-based approaches are generally unreliable under low illumination or occlusion. This limitation motivates the design of a proprioceptive membrane that conforms to the surface of interest and infers 3D geometry by reconstructing its own deformation. Conventional shape-aware membranes typically rely on resistive, capacitive, or magneto-sensitive mechanisms. However, these methods often encounter challenges such as structural complexity, limited compliance during large-scale deformation, and susceptibility to electromagnetic interference. This work presents a soft, flexible, and stretchable proprioceptive silicone membrane based on optical waveguide sensing. The membrane sensor integrates edge-mounted LEDs and centrally distributed photodiodes (PDs), interconnected via liquid-metal traces embedded within a multilayer elastomeric composite. Rich deformation-dependent light intensity signals are decoded by a data-driven model to recover the membrane geometry as a 3D point cloud. On a customized 140 mm square membrane, real-time reconstruction of large-scale out-of-plane deformation is achieved at 90 Hz with an average reconstruction error of 1.3 mm, measured by Chamfer distance, while maintaining accuracy for indentations up to 25 mm. The proposed framework provides a scalable, robust, and low-profile solution for global shape perception in deformable robotic systems.\n\nHighly Deformable Proprioceptive Membrane for Real-Time 3D Shape Reconstruction\nAbstract\nReconstructing the three-dimensional (3D) geometry of object surfaces is essential for robot perception, yet vision-based approaches are generally unreliable under low illumination or occlusion. This limitation motivates the design of a proprioceptive membrane that conforms to the surface of interest and infers 3D geometry by reconstructing its own deformation. Conventional shape-aware membranes typically ",
    "is_relevant": false,
    "relevance_score": 0.0,
    "extracted_keywords": [
        "proprioceptive membrane",
        "3D shape reconstruction",
        "optical waveguide sensing",
        "soft robotics",
        "deformation sensing"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå…‰å­¦æ³¢å¯¼ä¼ æ„Ÿçš„æŸ”è½¯å¯æ‹‰ä¼¸è†œï¼Œç”¨äºå®æ—¶ä¸‰ç»´å½¢çŠ¶é‡å»ºï¼Œä¸å¼ºåŒ–å­¦ä¹ ã€æ‰©æ•£æ¨¡å‹ç­‰å…³é”®è¯æ— å…³ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T03:59:21Z",
    "created_at": "2026-01-27T15:53:07.865687",
    "updated_at": "2026-01-27T15:53:07.865694"
}