{
    "id": "2602.00401v1",
    "title": "ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control",
    "authors": [
        "Jean Pierre Sleiman",
        "He Li",
        "Alphonsus Adu-Bredu",
        "Robin Deits",
        "Arun Kumar",
        "Kevin Bergamin",
        "Mohak Bhardwaj",
        "Scott Biddlestone",
        "Nicola Burger",
        "Matthew A. Estrada",
        "Francesco Iacobelli",
        "Twan Koolen",
        "Alexander Lambert",
        "Erica Lin",
        "M. Eva Mungai",
        "Zach Nobles",
        "Shane Rozen-Levy",
        "Yuyao Shi",
        "Jiashun Wang",
        "Jakob Welner",
        "Fangzhou Yu",
        "Mike Zhang",
        "Alfred Rizzi",
        "Jessica Hodgins",
        "Sylvain Bertrand",
        "Yeuhi Abe",
        "Scott Kuindersma",
        "Farbod Farshidian"
    ],
    "abstract": "åœ¨äººå½¢æœºå™¨äººä¸Šå®ç°ç¨³å¥ã€ç±»äººçš„å…¨èº«æ§åˆ¶ï¼Œä»¥å®Œæˆæ•æ·ä¸”æ¥è§¦ä¸°å¯Œçš„åŠ¨ä½œï¼Œä»æ˜¯ä¸€é¡¹æ ¸å¿ƒæŒ‘æˆ˜ï¼Œè¿™éœ€è¦å¤§é‡é’ˆå¯¹ç‰¹å®šæŠ€èƒ½çš„å·¥ç¨‹è®¾è®¡å’Œè„†å¼±çš„æ§åˆ¶å™¨è°ƒä¼˜è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ZESTï¼ˆé›¶æ ·æœ¬å®ä½“æŠ€èƒ½è¿ç§»ï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç®€åŒ–çš„è¿åŠ¨æ¨¡ä»¿æ¡†æ¶ï¼Œé€šè¿‡ä»å¤šæ ·åŒ–æ¥æºâ€”â€”é«˜ç²¾åº¦åŠ¨ä½œæ•æ‰ã€å™ªå£°å•ç›®è§†é¢‘ä»¥åŠæ— ç‰©ç†çº¦æŸçš„åŠ¨ç”»â€”â€”è¿›è¡Œå¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒç­–ç•¥ï¼Œå¹¶å®ç°é›¶æ ·æœ¬ç¡¬ä»¶éƒ¨ç½²ã€‚ZESTèƒ½å¤Ÿè·¨è¡Œä¸ºå’Œå¹³å°æ³›åŒ–ï¼ŒåŒæ—¶é¿å…äº†æ¥è§¦æ ‡ç­¾ã€å‚è€ƒæˆ–è§‚æµ‹çª—å£ã€çŠ¶æ€ä¼°è®¡å™¨ä»¥åŠå¤æ‚çš„å¥–åŠ±å¡‘é€ ã€‚å…¶è®­ç»ƒæµç¨‹ç»“åˆäº†è‡ªé€‚åº”é‡‡æ ·ï¼ˆä¸“æ³¨äºè®­ç»ƒå›°éš¾è¿åŠ¨ç‰‡æ®µï¼‰å’ŒåŸºäºæ¨¡å‹çš„è¾…åŠ©åŠ›çŸ©è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ ï¼Œå…±åŒå®ç°äº†åŠ¨æ€ã€é•¿æ—¶ç¨‹çš„å¤æ‚åŠ¨ä½œã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ç§ä»è¿‘ä¼¼è§£æç”µæ¢å€¼ä¸­é€‰æ‹©é—­ç¯é©±åŠ¨å™¨çš„å…³èŠ‚çº§å¢ç›Šçš„æ–¹æ³•ï¼Œå¹¶æ”¹è¿›äº†é©±åŠ¨å™¨æ¨¡å‹ã€‚ZESTå®Œå…¨åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­é€šè¿‡é€‚åº¦é¢†åŸŸéšæœºåŒ–è¿›è¡Œè®­ç»ƒï¼Œå±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ³¢å£«é¡¿åŠ¨åŠ›çš„Atlasäººå½¢æœºå™¨äººä¸Šï¼ŒZESTä»åŠ¨ä½œæ•æ‰æ•°æ®ä¸­å­¦ä¹ äº†åŠ¨æ€ã€å¤šæ¥è§¦æŠ€èƒ½ï¼ˆå¦‚åŒåŒå‰è¿›ã€éœ¹é›³èˆï¼‰ã€‚å®ƒèƒ½å¤Ÿå°†å¯Œæœ‰è¡¨ç°åŠ›çš„èˆè¹ˆå’Œåœºæ™¯äº¤äº’æŠ€èƒ½ï¼ˆå¦‚æ”€çˆ¬ç®±å­ï¼‰ç›´æ¥ä»è§†é¢‘è¿ç§»åˆ°Atlaså’ŒUnitree G1æœºå™¨äººä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜èƒ½è·¨å½¢æ€æ‰©å±•åˆ°Spotå››è¶³æœºå™¨äººï¼Œé€šè¿‡åŠ¨ç”»æ•°æ®å®ç°è¿ç»­åç©ºç¿»ç­‰é«˜éš¾åº¦ç‰¹æŠ€ã€‚è¿™äº›æˆæœå…±åŒè¯æ˜äº†ZESTåœ¨å¼‚æ„æ•°æ®æºå’Œä¸åŒæœºå™¨äººå½¢æ€é—´å®ç°ç¨³å¥é›¶æ ·æœ¬éƒ¨ç½²çš„èƒ½åŠ›ï¼Œç¡®ç«‹äº†å…¶ä½œä¸ºç”Ÿç‰©è¿åŠ¨ä¸æœºå™¨äººè¿åŠ¨ä¹‹é—´å¯æ‰©å±•æ¥å£çš„åœ°ä½ã€‚",
    "url": "https://arxiv.org/abs/2602.00401v1",
    "html_url": "https://arxiv.org/html/2602.00401v1",
    "html_content": "ZEST: Zero-shot Embodied Skill Transfer for\nAthletic Robot Control\nJean-Pierre Sleiman\nHe Li\nAlphonsus Adu-Bredu\nRobin Deits\nArun Kumar\nKevin Bergamin\nMohak Bhardwaj\nScott Biddlestone\nNicola Burger\nMatthew A. Estrada\nFrancesco Iacobelli\nTwan Koolen\nAlexander Lambert\nErica Lin\nM. Eva Mungai\nZach Nobles\nShane Rozen-Levy\nYuyao Shi\nJiashun Wang\nJakob Welner\nFangzhou Yu\nMike Zhang\nAlfred Rizzi\nJessica Hodgins\nSylvain Bertrand\nYeuhi Abe\nScott Kuindersma\nFarbod Farshidian\nAbstract\nAchieving robust, human-like whole-body control on humanoid robots for agile, contact-rich behaviors remains a central challenge, demanding heavy per-skill engineering and a brittle process of tuning controllers.\nWe introduce\nZEST\n(Zero-shot Embodied Skill Transfer), a streamlined motion-imitation framework that trains policies via reinforcement learning from diverse sourcesâ€”high-fidelity motion capture, noisy monocular video, and non-physics-constrained animationâ€”and deploys them to hardware zero-shot.\nZEST\ngeneralizes across behaviors and platforms while avoiding contact labels, reference or observation windows, state estimators, and extensive reward shaping.\nIts training pipeline combines adaptive sampling, which focuses training on difficult motion segments, and an automatic curriculum using a model-based assistive wrench, together enabling dynamic, long-horizon maneuvers.\nWe further provide a procedure for selecting joint-level gains from approximate analytical armature values for closed-chain actuators, along with a refined model of actuators.\nTrained entirely in simulation with moderate domain randomization,\nZEST\ndemonstrates remarkable generality.\nOn Boston Dynamicsâ€™ Atlas humanoid,\nZEST\nlearns dynamic, multi-contact skills (e.g., army crawl, breakdancing) from motion capture. It transfers expressive dance and scene-interaction skills, such as box-climbing, directly from videos to Atlas and the Unitree G1\n1\n1\n1\nThe work on Boston Dynamicsâ€™ Atlas and Spot involved all authors, while authors affiliated with the RAI Institute completed the work on the Unitree G1 robot.\n. Furthermore, it extends across morphologies to the Spot quadruped, enabling acrobatics, such as a continuous backflip, through animation.\nTogether, these results demonstrate robust zero-shot deployment across heterogeneous data sources and embodiments, establishing\nZEST\nas a scalable interface between biological movements and their robotic counterparts.\nâ€ \nâ€ \njournal:\nol\nINTRODUCTION\nThe design of humanoid robots is motivated by a simple fact: our environments are built around the shape, scale, and motion capabilities of the human body. A robot with a similar form can, in principle, operate within these environments without requiring extensive modifications, working alongside people to perform the same everyday tasks. However, to function effectively in such settings, it must first be capable of executing the kinds of motions that humans can perform. In this context, success is measured less by proficiency in a single, task-specific behavior and more by the generality of performing a broad repertoire of coordinated, whole-body movements and skills with human-like fluidity. However, realizing human-level physical intelligence on humanoids presents significant challenges: their high degrees of freedom, intermittent and multi-contact interactions, and inevitable modeling mismatches demand control strategies that can capture the richness of human movement while withstanding real-world uncertainty.\nHere, we address this grand challenge with\nZEST\n(Zero-shot Embodied Skill Transfer), a unified framework for learning physical intelligence.\nZEST\ntranslates the diversity and expressiveness of human motion from heterogeneous data sources into a robust control policy. This approach enables a legged robot to execute a vast spectrum of skills zero-shot, without any per-task fine-tuning, circumventing the need for complex behavior discovery from scratch by leveraging human motion datasets as a source of general-purpose skills.\nOver the past decade, a popular approach for enabling diverse behaviors on legged robots has been to generate whole-body motion trajectories offline and then track them with a model-based controller. A prime example is Boston Dynamicsâ€™ Atlas humanoid, which has demonstrated impressive parkour and dance routines by leveraging references derived from keyframe animation. These motions are executed using a two-layer control architecture composed of a Model Predictive Control (MPC) online planner and an optimization-based whole-body controller\n[\n1\n,\n2\n,\n3\n,\n4\n,\n5\n]\n.\nThe research community has widely adopted the same model-based\nplan offline, track online\nparadigm across a broad range of tasks for legged platforms. This includes humanoid robots using handrails to climb stairs\n[\n6\n]\n, quadrupeds showing acrobatic behaviors\n[\n7\n]\n, retargeting motions from animals to robots with different morphologies\n[\n8\n]\n, and quadrupedal mobile manipulators tackling multi-contact tasks\n[\n9\n]\n.\nAcross these efforts, the core principle remains the same: use offline trajectory optimizationâ€”often in a multi-contact settingâ€”to discover nontrivial, dynamically feasible behaviors that are difficult to design manually, and track these motions online with a general-purpose stabilizing controller.\nThis approach offers versatility and interpretability as a well-tuned controller can track a wide range of physically consistent motions without requiring skill-specific adjustments.\nHowever, the resulting controllers tend to have stronger requirements on environment modeling, estimation, and reference fidelity, making them more difficult to deploy in scenarios involving high uncertainty or complex contact.\nRecently, Reinforcement Learning (RL) has emerged as a powerful approach for control synthesis, providing exceptional agility and robustness. By learning to manage complex contact dynamics implicitly, RL policies can determine when and where to make contact in real-time, eliminating the need for predefined contact schedules and simplifying control design. Moreover, the ability to train at scale in high-fidelity simulators and transfer policies to hardware zero-shot\n[\n10\n,\n11\n]\nhas accelerated progress in data-driven control\n[\n12\n,\n13\n]\n. This success has led to state-of-the-art performance in legged locomotion, including robust navigation over challenging terrains in the wild\n[\n14\n,\n15\n,\n16\n]\n, high-speed running\n[\n17\n,\n18\n]\n, and extreme parkour\n[\n19\n,\n20\n,\n21\n]\n.\nBeyond locomotion, RL has also proven effective for dexterous manipulation tasks\n[\n22\n,\n23\n,\n24\n]\n, and whole-body mobile manipulation\n[\n25\n,\n26\n,\n27\n,\n28\n]\n. For instance, it has enabled various applications in whole-body mobile manipulation, such as a quadruped to play badminton\n[\n25\n]\n, a humanoid robot to perform forceful tasks such as carrying heavy loads, pulling carts, and opening spring-loaded doors\n[\n26\n]\n, and a quadrupedal manipulator to dynamically push and reorient large boxes\n[\n27\n]\n. Nonetheless, these\ntabula rasa\nRL techniques remain sample-inefficient and highly sensitive to reward design; without careful shaping and regularization, learned policies can exploit the reward structure and exhibit unnatural or overly aggressive behaviors.\nTo mitigate the limitations of\ntabula rasa\nRL, an effective strategy, pioneered in computer graphics, is to use motion data as a prior to regularize policy learning. This data-guided recipe guides controllers toward natural movements, reducing the need for extensive reward engineering. A seminal example is\nDeepMimic\n[\n29\n]\n, which trained polices to imitate a collection of Motion Capture (MoCap) clips while relying on a single, consistent reward structure. This approach was later extended to enable user-steerable control with a motion matcher\n[\n30\n]\n, achieve higher-fidelity tracking of complex motions\n[\n31\n]\n, and scale imitation to massive motion libraries\n[\n32\n]\n.\nAdversarial imitation learning replaces direct imitation rewards with those inferred indirectly by matching the distribution of demonstrations. In Adversarial Motion Priors\n[\n33\n]\n, a discriminator trained to differentiate between state transitions from the dataset and those from policy rollouts provides a style reward that encourages natural motions. Subsequent variants introduce hierarchical or latent-skill formulations to mitigate mode collapse and improve reusability and steerability\n[\n34\n,\n35\n]\n.\nMore recent work closes the loop by coupling kinematic motion generators with physics-based controllers. For instance, text-driven diffusion models now generate plans for multi-task sequences\n[\n36\n]\n, while other hierarchical approaches tackle agile terrain traversal by iteratively augmenting motion datasets\n[\n37\n]\n. This paradigm has also been extended to complex human-object interactions through methods like multi-teacher policy distillation\n[\n38\n]\n.\nThese works from computer graphics highlight a key theme: dataâ€”whether as explicit reference clips, adversarial priors, or learned skill embeddingsâ€”acts as a powerful regularizer that simplifies reward design, improves motion quality, and scales controllers to broad behavior repertoires.\nFigure 1\n:\nHardware deployment of\nZEST\nacross diverse data sources and robot morphologies.\nIn order of appearance from top left to bottom right, the figure illustrates the following behaviors. From MoCap: Crawl on all fours (Atlas), roll on all fours (Atlas), jog (Atlas), breakdance (Atlas), forward roll (Atlas), cartwheel (G1), table-tennis (G1), cartwheel (atlas), army crawl (Atlas). From ViCap: Dance snippet A (Atlas), jump onto box (G1), climb up/down box (G1), ballet (G1), dance snippet C, soccer kick (Atlas). From Animation: handstand invert (Atlas), handstand balance (Spot), continuous backflip (Spot), barrel roll (Spot).\nDespite strong results in simulation, deploying a unified tracking policy on hardware remains nontrivial. The sim-to-real gapâ€”stemming from inevitable modeling discrepanciesâ€”is worsened by contact-rich behaviors and operations near the robotâ€™s actuation limits: many dynamic or acrobatic motions become infeasible once real torque, speed, and safety constraints are enforced. In contrast, such limits are often ignored in computer graphics settings, allowing for aggressive exploration during training, which in turn facilitates learning. Policies must also operate under partial observability (e.g., missing global pose or root linear velocity), typically requiring either a state-estimation stack or a history-conditioned policy, both of which add complexity during training and deployment.\nNevertheless, a growing body of work has begun to bridge this gap by leveraging real-to-sim strategies and various data sources. In\n[\n39\n]\n, Peng et al. enable quadruped robots to perform diverse gaits by imitating real animal MoCap data. Grandia et al.\n[\n40\n]\npresent an RL-based policy that executes expressive, real-time stage performances driven by animation engines.\nVideoMimic\n[\n41\n]\nutilizes everyday videos to train a whole-body policy to execute contextual skills through a real-to-sim-to-real pipeline.\nASAP\n[\n42\n]\nlearns to perform agile whole-body skills by pre-training motion-tracking policies from human videos, collecting trajectories on hardware, and learning a â€œdelta-actionâ€ model that reduces the sim-to-real gap.\nKungfuBot\n[\n43\n]\ntrains a policy for Unitree G1 to perform dynamic motions, such as acrobatic dance and kung-fu strikes, through multi-step motion processing (filtering, correction, retargeting with physical constraints).\nHuB\n[\n44\n]\npushed the limits of the same hardware to achieve extreme balance in challenging one-legged stance tasks.\nUsing reference trajectories from model-based planners is another strategy for imitation-based RL.\nMethods like\nOpt-Mimic\n[\n45\n]\nand\nDTC\n[\n46\n]\nexemplify this approach by training policies to mimic optimized plans, thereby combining the precision of trajectory optimization with the robustness of an RL tracking controller.\nSimilarly, a two-stage RL framework can leverage a single optimal trajectory as initial guidance and then fine-tune purely for task completion, yielding robust locomotion policies\n[\n47\n]\n.\nFor multi-contact loco-manipulation, Sleiman et al.\n[\n48\n]\nutilize a single offline demonstration to train policies for tasks like traversing spring-loaded doors and manipulating heavy dishwashers. The resulting policy generalizes beyond the reference, learning to handle object variability and discover novel recovery maneuvers.\nAnother line of work focuses on human-driven and generalist control, grounded in large-scale motion tracking and teacherâ€“student distillation.\nOmniH2O\n[\n49\n]\ntreats kinematic pose as a universal control interface and learns an autonomous whole-body policy by imitating a privileged teacher trained on large-scale retargeted/augmented datasets.\nHOVER\n[\n50\n]\ndistills a motion imitation teacher into a student policy while applying proprioception and command masking with mode-specific and sparsity-based masks, resulting in a unified multi-mode command space for a whole-body tracking policy. Most recently,\nGMT\n[\n51\n]\ncombines an adaptive sampling strategy (biasing sampling towards harder-to-learn motions) with a Mixture-of-Experts teacher that enhances expressiveness and generalizability. Student policies distilled from this teacher track a broad spectrum of motions achieving state-of-the-art real-world performance.\nDespite these advancements, a significant obstacle to scalability and robustness persists: the dependence on intricate or multi-stage pipelines that are often composed of handcrafted components. These complexities can include auxiliary shaping reward terms, contact labeling, multi-stage training, access to non-proprioceptive signals or to histories of proprioception, and per-behavior retuning.\nIn this work, we present\nZEST\n, a unified, generic, and minimal recipe that trains policies in a single stage from heterogeneous motion data sources, including MoCap, Video-Captured (ViCap) motions, and keyframe animations. By design, this approach eliminates complex, domain-specific techniques such as contact labeling, extensive reward shaping, future-reference windows, observation histories, and state estimators, while avoiding multi-stage training pipelines. The resulting policy, implemented with a simple feedforward network, robustly tracks diverse motion references using a single set of hyperparameters for each robot and deploys zero-shot to hardware.\nWe apply\nZEST\nto Boston Dynamicsâ€™ electrically-actuated Atlas humanoid robot (approximately 1.8â€‰m and 100â€‰kg, comparable to an average adult human). The exact formulation and training recipe are applicable across robots with different sizes, weights, and embodiments, from the full-scale Atlas humanoid to the smaller Unitree G1 humanoid and the Spot quadruped. This versatility demonstrates a level of generality that represents a significant advance over specialized controllers tightly coupled to a single robot or behavior.\nConcurrent work reports closely related motion-imitation pipelines and transfer results on the G1 platform\n[\n52\n,\n53\n,\n54\n]\n. In this work, we evaluate a single, unified framework with a systematic, low-tuning training recipe across multiple robot platforms and multiple reference sources. This evaluation yields, to our knowledge, the first dynamic, multi-contact behaviors (e.g., army crawl, breakdance) on a full-size humanoid (Atlas) and the first video-to-behavior transfer of agile, contact-rich skills, such as box-climbing and dancing, to a physical humanoid (Unitree G1). Related work on ViCap-based sim-to-real transfer such as\nVideoMimic\n[\n41\n]\nprimarily focuses on locomotion-style behaviors, whereas our video-derived demonstrations emphasize complex dynamic multi-contact skills with intermittent whole-body contact events and sustained whole-body contact interactions.\nOur policies utilize the next-step reference and current proprioceptive signals as observations, with only the previous action as the history. They output residual joint targets, which are then added to the reference and sent to a joint-level PD (Proportional-Derivative) controller.\nTo ground our simulation in reality, the PD gains are tuned using effective motor armature values derived from an analytical model that approximates closed-chain kinematics in actuators such as knees and ankles. For Spot, we incorporated more accurate models of its power system and actuators in simulation.\nTo handle long-horizon clips and scale beyond single skills, trajectories are split into fixed-duration bins, with difficulty level estimated via an Exponential Moving Average (EMA) of failure scores. A categorical sampler biases toward harder bins, facilitating long-horizon and multi-trajectory learning without catastrophic forgetting.\nTo help stabilize training and avoid long convergence times for highly dynamic behaviorsâ€”especially those involving large base rotationsâ€”a virtual assistive wrench is computed in a model-based fashion and applied directly to the robotâ€™s base. Its magnitude is adapted based on the per-bin difficulty levels and eventually decays to zero as tracking performance improves, eliminating the need for a handcrafted curriculum. Training is performed entirely in simulation with moderate domain randomization (e.g., impulsive pushes, sensor noise, and variations in friction coefficients and masses).\nThe result of this framework is a pipeline that demonstrates remarkable robustness from data to deployment. It effectively processes reference motions from diverse sources, successfully handling high-fidelity MoCap data, noisy ViCap data with artifacts like pose jitter and foot-skidding, and kinematically clean but often physically infeasible animation data. The frameworkâ€™s ability to retarget motions from all three sources highlights its resilience to a wide range of data imperfections. This entire training process results in policies that are deployed to the hardware zero-shot via an automated pipeline.\nFigure 2\n:\nSummary of the\nZEST\nframework and its hardware results.\nRESULTS\nMovie 1 summarizes the methodology and results of the presented work. In this section, we outline our evaluation procedure and present our empirical findings. Unless otherwise indicated, all presented results are obtained on hardware. Each experiment uses a specialized policy trained solely on the target skill. This specialization avoids the extended training required for a purely multi-skill model to sample the target reference as often as in the single-skill setting and, in practice, delivers stronger and more consistent tracking performance within a fixed time budget. Therefore, we do not deploy a multi-skill policy on hardware, but only use it to perform simulation-based analyses. Each policy was trained for approximately 10 hours (about 7k iterations) on a single NVIDIA L4 GPU and validated across multiple independent hardware trials with consistent success. Evaluations span multiple embodimentsâ€”Boston Dynamicsâ€™ electrically actuated Atlas humanoid, Unitreeâ€™s G1 humanoid, and Boston Dynamicsâ€™ Spot quadrupedâ€”and are organized by reference source: MoCap, ViCap, and keyframe animation. Figure\n1\nshows representative frame snapshots across all skills, per-skill clips are provided in Movie 1, and the corresponding reference motions are included in Movies S1â€“S3. Table\n1\nlists all motions, and for each, marks whether it involves multiple body contacts with the ground (i.e., apart from the robotâ€™s feet), and reports tracking metrics for joint angles, base orientation, and base angular velocity. The base orientation is represented by its roll and pitch components. We exclude yaw, as it is not directly observable from the on-board IMU and is subject to significant drift. Next, we evaluate a multi-skill policy in simulation and use it to justify our key design choices via simulation-based ablation studies. Finally, we include a simulation-based comparison with a state-of-the-art whole-body MPC baseline developed at Boston Dynamics.\nMoCap-Derived Human-Like Skills on Hardware\nMoCap references enable a broad range of behaviors on Atlas (Table\n1\n; Movie S1). As shown in Movie S1, we deploy\nwalk\n,\njog\n,\nforward roll\n,\nroll on all fours\n,\ncrawl on all fours\n,\narmy crawl\n,\ncartwheel\n, and a short\nbreakdance\non hardware, and we also evaluate a MoCap\ntable-tennis\nand a\ncartwheel\nmotion on Unitreeâ€™s G1 using the same formulation (with actuator parameters adapted to that platform).\nWe highlight salient aspects of these MoCap-driven behaviors. For\nwalk\nand\njog\n, footâ€“ground interactions are straightforward, yet several natural-gait hallmarks emerge: a clear heelâ€“toe rollover rather than flat-foot placement; near full-knee extension despite proximity to joint limits; and graceful, quiet transitionsâ€”both within gait cycles and when decelerating from a fast jog to stanceâ€”without stomping, chatter, or audible impacts. These features are extremely challenging to induce through pure reward shaping alone. We attribute their emergence to high-fidelity MoCap references that preserve human nuance, actuator models tuned to be responsive without excessive stiffness, and regularization that discourages impulsive transients.\nThe same formulation also accommodates complex multi-contact behaviors without prescribing contact schedules or constraining base height or orientation to upright configurations. All motions begin from a standing configuration and return to standing at completion.\nForward roll\nthrows the body forward from stance, makes initial forearm contact, and completes a full forward roll with back contact.\nRoll on all fours\nand\ncrawl on all fours\nemploy intermittent heelâ€“toe, knee, and wrist contacts; the former executes a lateral roll, while the latter advances forward under low ground clearance.\nArmy crawl\nmaintains near-continuous armâ€“legâ€“torso contact with deliberate sliding to translate forward.\nCartwheel\nalternates handâ€“foot supports and passes through transient hand-supported inversions that require accurate hand placement and high angular-rate control. The most challenging case is the\nbreakdance\nsequence, which combines extended bodyâ€“ground contact phases with rapid leg sweeps and includes segments operated near joint limits where a legâ€”with its knee fully extendedâ€”threads beneath the opposite leg, inducing unavoidable self-contact.\nTable 1\n:\nEvaluation across motions, grouped by reference source.\nA skill is labeled as\nmulti-contact\nif any body part other than the feet interacts with the ground. Metrics are reported as\nmean absolute error (MAE), mean angular distance (MAD), and mean L2 error (ML2)\nover successful trials; lower is better. MAE\n(\nğ’’\n)\n(\\bm{q})\nis for joint-position error; MAD\n(\nR\n)\n(R)\nis for base orientation error; ML2\n(\nğ\n)\n(\\bm{\\omega})\nis for base angular velocity errors, respectively. We also include data for the maximum errors in joint position and orientation, denoted as max\n(\nğ’’\n)\n(\\bm{q})\nand max\n(\nR\n)\n(R)\nrespectively.\nMotion\nRobot\nMulti-Contact\nSkill (âœ—/âœ“)\nDuration\n(s)\nMAE\n(\nğ’’\n)\n(\\bm{q})\n(rad)\nMAD\n(\nR\n)\n(R)\n(rad)\nML2\n(\nğ\n)\n(\\bm{\\omega})\n(rad/s)\nmax\n(\nğ’’\n)\n(\\bm{q})\n(rad)\nmax\n(\nR\n)\n(R)\n(rad)\nMoCap-Derived Motions\nWalk\nAtlas\nâœ—\n8.47\n8.47\n0.056â€‰848\n0.056\\,848\n0.030â€‰011\n0.030\\,011\n0.364â€‰151\n0.364\\,151\n0.486â€‰572\n0.486\\,572\n0.078â€‰402\n0.078\\,402\nJog\nAtlas\nâœ—\n5.89\n5.89\n0.041â€‰299\n0.041\\,299\n0.059â€‰295\n0.059\\,295\n0.699â€‰386\n0.699\\,386\n0.642â€‰535\n0.642\\,535\n0.165â€‰264\n0.165\\,264\nForward roll\nAtlas\nâœ“\n7.11\n7.11\n0.061â€‰586\n0.061\\,586\n0.137â€‰010\n0.137\\,010\n1.654â€‰138\n1.654\\,138\n0.907â€‰468\n0.907\\,468\n0.527â€‰964\n0.527\\,964\nRoll on all fours\nAtlas\nâœ“\n7.80\n7.80\n0.063â€‰823\n0.063\\,823\n0.080â€‰306\n0.080\\,306\n1.604â€‰383\n1.604\\,383\n0.642â€‰819\n0.642\\,819\n0.202â€‰891\n0.202\\,891\nCrawl on all fours\nAtlas\nâœ“\n14.74\n14.74\n0.055â€‰822\n0.055\\,822\n0.096â€‰739\n0.096\\,739\n0.921â€‰553\n0.921\\,553\n0.764â€‰109\n0.764\\,109\n0.239â€‰705\n0.239\\,705\nArmy crawl\nAtlas\nâœ“\n16.66\n16.66\n0.103â€‰255\n0.103\\,255\n0.075â€‰445\n0.075\\,445\n1.010â€‰727\n1.010\\,727\n0.638â€‰430\n0.638\\,430\n0.276â€‰292\n0.276\\,292\nCartwheel\nAtlas\nâœ“\n5.98\n5.98\n0.083â€‰514\n0.083\\,514\n0.064â€‰348\n0.064\\,348\n1.733â€‰776\n1.733\\,776\n0.672â€‰884\n0.672\\,884\n0.234â€‰598\n0.234\\,598\nBreakdance\nAtlas\nâœ“\n6.56\n6.56\n0.079â€‰027\n0.079\\,027\n0.133â€‰372\n0.133\\,372\n1.911â€‰055\n1.911\\,055\n0.825â€‰025\n0.825\\,025\n0.364â€‰705\n0.364\\,705\nCartwheel\nG1\nâœ“\n8.0\n8.0\n0.078\n0.078\n0.331\n0.331\n0.886\n0.886\n0.660\n0.660\n0.892\n0.892\nTable-tennis\nG1\nâœ—\n29.98\n29.98\n0.108\n0.108\n0.402\n0.402\n1.827\n1.827\n2.483\n2.483\n0.637\n0.637\nViCap-Derived Motions\nSoccer kick\nAtlas\nâœ—\n6.00\n6.00\n0.049â€‰252\n0.049\\,252\n0.050â€‰408\n0.050\\,408\n1.253â€‰406\n1.253\\,406\n0.653â€‰306\n0.653\\,306\n0.140â€‰620\n0.140\\,620\nDance snippet A\nAtlas\nâœ—\n12.70\n12.70\n0.050â€‰629\n0.050\\,629\n0.038â€‰604\n0.038\\,604\n0.414â€‰272\n0.414\\,272\n0.769â€‰049\n0.769\\,049\n0.079â€‰681\n0.079\\,681\nDance snippet B\nAtlas\nâœ—\n9.40\n9.40\n0.054â€‰249\n0.054\\,249\n0.065â€‰766\n0.065\\,766\n1.463â€‰322\n1.463\\,322\n0.761â€‰656\n0.761\\,656\n0.238â€‰778\n0.238\\,778\nDance snippet C\nAtlas\nâœ—\n11.59\n11.59\n0.054â€‰567\n0.054\\,567\n0.046â€‰293\n0.046\\,293\n1.248â€‰154\n1.248\\,154\n0.759â€‰036\n0.759\\,036\n0.125â€‰329\n0.125\\,329\nBallet sequence\nG1\nâœ—\n7.00\n7.00\n0.061\n0.061\n0.186\n0.186\n1.395\n1.395\n1.101\n1.101\n0.588\n0.588\nJump onto box\nG1\nâœ—\n5.00\n5.00\n0.041\n0.041\n0.259\n0.259\n0.936\n0.936\n0.883\n0.883\n0.914\n0.914\nClimb up box\nG1\nâœ“\n9.18\n9.18\n0.073\n0.073\n0.385\n0.385\n0.855\n0.855\n1.103\n1.103\n0.798\n0.798\nClimb down box\nG1\nâœ“\n10.62\n10.62\n0.108\n0.108\n0.394\n0.394\n0.836\n0.836\n0.957\n0.957\n0.867\n0.867\nAnimation-Derived Motions\nHandstand invert\nAtlas\nâœ“\n5.18\n5.18\n0.073â€‰904\n0.073\\,904\n0.082â€‰358\n0.082\\,358\n0.465â€‰313\n0.465\\,313\n0.949â€‰880\n0.949\\,880\n0.449â€‰680\n0.449\\,680\nHandstand balance\nSpot\nâœ—\n4.30\n4.30\n0.075â€‰372â€‰607â€‰231â€‰681â€‰1\n0.075\\,372\\,607\\,231\\,681\\,1\n0.063â€‰911\n0.063\\,911\n0.408â€‰573\n0.408\\,573\n0.531â€‰845\n0.531\\,845\n0.235â€‰228\n0.235\\,228\nContinuous backflip\nSpot\nâœ—\n6.00\n6.00\n0.192â€‰162\n0.192\\,162\n0.149â€‰614\n0.149\\,614\n0.732â€‰049\n0.732\\,049\n2.381â€‰236\n2.381\\,236\n0.528â€‰300\n0.528\\,300\nBarrel roll*\nSpot\nâœ—\n2.45\n2.45\n0.145â€‰794\n0.145\\,794\n0.839â€‰707\n0.839\\,707\n*\n1.450â€‰854\n1.450\\,854\n1.067â€‰999\n1.067\\,999\n1.570â€‰796\n1.570\\,796\nHappy dog\nSpot\nâœ—\n12.00\n12.00\n0.106â€‰686â€‰689â€‰268â€‰162â€‰51\n0.106\\,686\\,689\\,268\\,162\\,51\n0.069â€‰333\n0.069\\,333\n1.008â€‰193\n1.008\\,193\n1.180â€‰046\n1.180\\,046\n0.348â€‰501\n0.348\\,501\n*\nDuring barrel roll, the Spot IMU saturated mid-flight; therefore, IMU data were excluded from that segment.\nThese motions are difficult precisely because they invoke contacts beyond the feet (hands, knees, forearms, torso, back), placing them outside the design envelope of standard whole-body control pipelines. For model-based frameworks, the intricacy of modeling contact phenomenaâ€”high-impulse events, stickâ€“slip transitions, and extended bodyâ€“ground interactions over complex geometriesâ€”poses a major obstacle to designing stabilizing controllers for multi-contact behaviors. Accordingly, these methods typically predefine the active end-effectors and avoid motions with whole-body contacts. Even for RL policies trained in simulation, these regimes magnify the sim-to-real gap, as small mismatches in contact, friction, compliance, and geometry compound over extended contact-rich phases. Consistent with Table\n1\n, simpler foot-only gaits (\nwalk\n,\njog\n) exhibit lower tracking errors across\nMAE\nâ€‹\n(\nğ’’\n)\n\\mathrm{MAE}(\\bm{q})\nand\nMAD\nâ€‹\n(\nR\n)\n\\mathrm{MAD}(R)\n, whereas more dynamic, multi-contact skills (\nforward roll\n,\nroll on all fours\n,\ncrawl on all fours\n,\narmy crawl\n,\ncartwheel\n,\nbreakdance\n) show higher orientation and angular-velocity discrepancies.\nFinally, we also imitate a\ncartwheel\nmotion and a\ntable-tennis\non Unitreeâ€™s G1. While our G1 evaluations are predominantly ViCap-derived motions (next section), this result provides an additional check of humanoid-to-humanoid cross-morphology transfer while holding the formulation constant across varying data sources. The G1â€™s\nCartwheel\nreference, although approximately 2s longer than the version for Atlas, demonstrates a comparable motion quality. However, it has a higher orientation error, which we attribute to differences in the quality of the IMUs between the two platforms. Another challenging long-horizon task is the\ntable-tennis\n, which requires coordinating upper-body swings with dynamic base maneuvers. Its extended duration makes it difficult to learn, and our adaptive sampling strategy was crucial in achieving a successful outcome for this behavior.\nViCap-Derived Human-Like Skills on Hardware\nOur experiments with ViCap references highlight robustness to noisy, video-reconstructed motions while maintaining fluid, temporally coherent motion over long sequences (Movie S2). We employ video demonstrations captured in the wild with a handheld phone camera, yielding a fast and practical data generation pipeline:\nrecord\nâ†’\n\\rightarrow\nreconstruct\nâ†’\n\\rightarrow\nretarget\nâ†’\n\\rightarrow\ntrain\nâ†’\n\\rightarrow\nexecute\n, where\nreconstruct\ndenotes 3D pose estimation with temporal smoothing. Converting a recorded video into a retargeted reference motion takes minutes, so in practice, we can capture a new clip in the morning, train during the day, and run it on hardware that evening.\nOn Atlas, we execute a\nsoccer kick\nmotion and three\ndance\nsnippets (Aâ€“C). These sequences include extended single-foot support phases and multiple consecutive hops, requiring sustained balance with precise foot placement and timing. Despite visible artifacts in the references (pose jitter and foot skidding; Movie S2), we still achieve zero-shot sim-to-real transfer while preserving expressivity, without heavy motion post-processing (e.g., physics-aware trajectory optimization for high-fidelity dynamic retargeting); as reported in Table\n1\n, tracking errors remain surprisingly low (e.g.,\nMAE\nâ€‹\n(\nğ’’\n)\n\\mathrm{MAE}(\\bm{q})\nand\nMAD\nâ€‹\n(\nR\n)\n\\mathrm{MAD}(R)\nnear\n0.05\n0.05\nrad). We attribute this to a reward formulation that does not force tight adherence to the reference root trajectory: the policy is encouraged to track the global motion in a relaxed fashion, rejecting obvious artifacts while still solving the task. Moreover, our MDP and rewards do not require contact labels, so the policy is agnostic to noisy contact timings/schedules extracted from the ViCap reference.\nWe further demonstrate cross-embodiment transfer on Unitreeâ€™s G1 with an athletic\nballet sequence\nfeaturing a sustained aerial phase with both legs abducted in mid-air and extended single-foot support near the edge of stability, stressing balance, orientation regulation, and precise foot placement. We also evaluate scene-interactive\nbox\nskills on G1:\njump onto box\n,\nclimb up box\n, and\nclimb down box\n. These experiments involve whole-body contacts and push the system near its operational limits.\nInstead of providing the policy with the boxâ€™s precise location through perception or MoCap, we test its ability to execute these skills robustly using a minimalist, proprioceptive-only interface. To achieve this, we train the policy with slight randomizations in the initial relative pose between the robot and the box. At test time, the policy reliably handles this variability without additional sensing.\nFurthermore, these scene-interactive skills reflect a combination of challenges, including (i) compounded modeling mismatches arising from multi-contact interactions, (ii) discrepancies between the simulated box and the physical platform (notably compliance and friction), (iii) lack of boxâ€“robot relative-positioning feedback, and (iv) artifacts in the reconstructed references. Despite these factors, hardware executions are completed reliably, and the characteristic style of the source motions remains evident. As highlighted in Table\n1\n, errors remain reasonable overall and are comparable to those observed for the more challenging behaviors presented in the MoCap section. To further characterize repeatability and robustness on these challenging behaviors, we conduct additional variability testing. In Movie S6, we execute box climb up and box climb down five times consecutively, achieving five out of five successful runs under conditions kept as close as possible to the nominal training setup (i.e., the box height set to its nominal value of 0.75â€‰m and the robot initialized near the nominal pose with respect to the box). In Movie S7, we stress-test the policyâ€™s robustness by varying the initial robotâ€“box relative pose around the nominal configuration. Since these policies are inherently invariant to lateral offsets relative to the box center, we randomize the robotâ€™s initial offset in\nx\nx\nand yaw while keeping\ny\ny\nclose to the nominal value. All trials initialized within the training distribution succeed (\nÂ±\n10\n\\pm 10\ncm in\nx\nx\nand\nÂ±\n0.3\n\\pm 0.3\nrad in yaw), as highlighted in Figure S5. Beyond this range, we observe occasional successes, while failures occur for initial states outside the training distribution. We additionally test robustness to mass variability by attaching a 2â€‰kg payload to the torso and still obtain successful executions for both box climb up and box climb down. Finally, we vary the box height beyond the randomized training range. Although training uses a nominal height of 0.75â€‰m with randomization over\n[\n0.70\n,\n0.80\n]\n[0.70,0.80]\nm, box climb up executes successfully at 0.55â€‰m, while box climb down succeeds at both 0.55â€‰m and 0.95â€‰m.\nKeyframe-Animated Skills for Humanoids and Quadrupeds on Hardware\nKeyframe animation broadens our motion library beyond natural human movement. For humanoids such as Atlas, it enables us to exploit robot-specific degrees of freedom not represented in human motion capture, such as continuous joints in the arms, legs, or spine. These joints enable configurations that human anatomy cannot achieve. For instance, we achieve a\nhandstand invert\nmotion by rotating the robotâ€™s backâ€“yaw joint while maintaining stable hand support (Movie S3).\nKeyframe animation is equally important for platforms that lack a human form, where collecting human-like demonstrations is inherently difficult. To assess generality under a larger morphology gap, we apply the same framework on Boston Dynamicsâ€™ Spot, where we demonstrate a\nhandstand balance\n(forelegs), a\ncontinuous backflip\n, a\nbarrel roll\n, and a playful\nhappy dog\nmotion.\nAlthough animation references are kinematically clean and temporally coherent, several sequences are dynamically aggressive or partially infeasible for the physical system. In these cases, the learned policy intentionally relaxes adherence to the referenceâ€”especially for base orientation and angular-rate profilesâ€”to remain feasible, which increases reported means while preserving overall execution quality. This pattern is visible in Table\n1\nwhen contrasting\nMAE\nâ€‹\n(\nğ’’\n)\n\\mathrm{MAE}(\\bm{q})\nand\nMAD\nâ€‹\n(\nR\n)\n\\mathrm{MAD}(R)\nwith the corresponding maxima: for Atlas\nhandstand invert\n, the means are moderate while\nmax\nâ¡\n(\nğ’’\n)\n\\max(\\bm{q})\nand\nmax\nâ¡\n(\nR\n)\n\\max(R)\nspike during transient inversions; on Spot,\nbackflip\nand\nbarrel roll\nshow larger orientation and angular-velocity errors (see\nMAD\nâ€‹\n(\nR\n)\n\\mathrm{MAD}(R)\nand\nML2\nâ€‹\n(\nğ\n)\n\\mathrm{ML2}(\\bm{\\omega})\n) and high maxima, reflecting brief deviations from the reference for the sake of feasibility and stability.\nNotably, the policyâ€™s robustness is highlighted on the Spot robot for the\nbarrel roll\nmotion, where it completes the motion even after the on-board IMU sensor saturates mid-maneuver.\n(a)\nWrench scaling.\n(b)\nFailure levels.\n(c)\nTrajectory-bin counts.\n(d)\nSampling probabilities.\n(e)\nAblation after 10 h training (\nâ‰ˆ\n\\approx\n7k iterations).\n(f)\nAblation after 20 h training (\nâ‰ˆ\n\\approx\n14k iterations).\nFigure 3\n:\nSimulation-based evaluation and ablation studies.\n(\na\nâ€“\nd\n) A mid-training snapshot of the signals driving our adaptive curriculum: (\na\n) assistive wrench scaling, which is modulated by failure, (\nb\n) per-bin failure levels, (\nc\n) bin visitation counts, and (\nd\n) sampling probabilities. Higher failure rates increase the sampling probability, and over time, visitation counts rise where failures are frequent.\n(\ne\nâ€“\nf\n) Success rates for the baseline and ablations after 10 h and 20 h of training under complete domain randomization. Bars show mean success; whiskers show minâ€“max; internal lines mark median; dots denote p10; numbers under lower whiskers report the minimum. The plots show that the assistive curriculum and adaptive sampling are critical for performance and sample efficiency. Removing privileged information from the critic or using absolute actions significantly degrades robustness, while adding observation/reference windows hinders learning.\nSimulation-Based Evaluation and Ablation Studies\nWe train a single multi-skill policy for Atlas over 15 motions with diverse lengths and difficulties drawn from MoCap, ViCap, and keyframe animations, ranging from simple gaits (walking, squatting) to multi-contact and acrobatic sequences (army crawl, breakdancing, cartwheel-to-backflip). To inspect how the method operates during learning, Figure\n3\n(aâ€“d) shows a representative mid-training snapshot of our adaptive sampling signals: failure levels per trajectory bin, the resulting sampling probabilities, the number of times each bin has been visited, and the assistive-wrench scaling applied when starting from that segment. Higher failure rates increase sampling probability and, over time, visitation counts rise where failures are frequent. The easiest bin is the animated squat (trajectory index 8), while the cartwheel-to-backflip sequence (trajectory index 13) is the hardest; the sampler thus allocates more training to the latter and less to the former, while modulating the assistive wrench accordingly.\nEvaluation proceeds by initializing each episode at the start of a randomly sampled reference trajectory and rolling it to completion. For each reference, we run 10000 rollouts under full domain randomization, sampling external pushes, observation noise, robot link masses, friction coefficients, and perturbations of the initial state. We quantify performance and robustness via ablations presented in Figure\n3\n(eâ€“f), which report success rates after 10â€‰h (\nâ‰ˆ\n\\approx\n7k iterations) and 20â€‰h (\nâ‰ˆ\n\\approx\n14k iterations) of training. At 10â€‰h, our method achieves the highest mean success and markedly better lower-tail performance across behaviors. By 20â€‰h, several ablations narrow the mean gap, yet the baseline remains more robust, with higher p10 and minimum success values that lie closer to the mean.\nWe ablate, one at a time, curriculum, adaptive sampling, reference and observation windows, action space, and actorâ€“critic observation design. Removing curriculum still converges, but its 20â€‰h performance roughly matches our method at 10â€‰h, indicating that a minimal variant without curriculum is feasible but less sample-efficient, particularly on motions with large base-orientation variability (e.g., trajectory index 13). Removing adaptive sampling decreases performance because the policy undersamples hard bins. The heatmaps support this mechanism, as sampling probabilities track failure levels and concentrate updates where failure is high, which the ablation cannot reproduce.\nTo better contextualize the ablations over the wrench-assist curriculum and the adaptive sampling strategy, we summarize the roles played by each component. The assistive wrench primarily serves as an exploration and stabilization aid for highly dynamic motions (e.g., the cartwheel-to-backflip sequence) that frequently terminate early in training; in these regimes, it reduces catastrophic failures, stabilizes learning, and typically improves sample efficiency when reset-based exploration (i.e., Reference State Initialization) alone is insufficient. For simpler motions with fewer early failures, we find the wrench can often be reduced or removed with limited impact. Adaptive sampling is most beneficial when training on long-horizon trajectories or large motion libraries, and when the goal is to track each phase of each motion well rather than optimizing average performance. Without adaptive sampling, training can under-emphasize rare or difficult trajectory segments, leading to persistent weaknesses on those segments and degraded lower-tail performance.\nUsing longer reference or observation windowsâ€”either 20 steps of history or 20 steps of future references (0.4â€‰s each)â€”increases input dimensionality and redundancy, complicates credit assignment, and slows or prevents convergence under the same budget. This negative result may be specific to our hyperparameters and architecture; sequence-to-sequence models or different tuning could yield different conclusions, but we did not pursue such changes to keep the setup minimal and focused. We provide a finer-grained sweep over future-reference horizons and observation-history lengths (5, 10, 15, and 20 steps) in the Supplementary Materials (Figure S3), which reveals a gradual degradation in performance as the window length increases.\nMatching actor and critic inputs by stripping privileged observations from the critic degrades value estimation. The critic benefits from privileged signals such as the full root state, contact forces, key-body positions, and the assistive-wrench signal, which reflects curriculum-induced dynamics. Without these, value estimates become noisier, leading to weaker policy updates. Finally, absolute actions (as opposed to residual actions) yield the worst performance. Because episodes are initialized on the reference throughout training, small policy outputsâ€”which typically occur during early stages of trainingâ€”can induce large PD tracking errors and hence large joint torques, destabilizing rollouts. In addition, absolute actions impose a larger exploration burden: the policy must synthesize the full joint commands, whereas residual actions leverage the feedforward reference and learn only the required offsets, leading to faster and more stable learning.\nAdditional details on the reference motions used for ablation are provided in the Supplementary Materials (Table S1), along with a video showcasing evaluation rollouts from the multi-skill policy (Movie S4). We further report simulation-based robustness sweeps across domain-randomization factors in the Supplementary Materials (Figure S4).\nTable 2\n:\nSimulation-based comparison of MPC vs. a multi-skill RL policy.\nThe RL policy is trained on all Atlas motions in Table\n1\n, but evaluations are performed over a subset of these motions. A dash (â€“) indicates failure to execute the behavior.\nMotion\nMAE\nâ€‹\n(\nğ’’\n)\n\\mathrm{MAE}(\\bm{q})\n(rad)\nMAD\nâ€‹\n(\nR\n)\n\\mathrm{MAD}(R)\n(rad)\nMPC\nRL\nMPC\nRL\nWalk\n0.046â€‰695\n0.046\\,695\n0.055â€‰393\n0.055\\,393\n0.020â€‰235\n0.020\\,235\n0.027â€‰978\n0.027\\,978\nJog\n0.117â€‰239\n0.117\\,239\n0.075â€‰721\n0.075\\,721\n0.109â€‰940\n0.109\\,940\n0.062â€‰128\n0.062\\,128\nHandstand invert\nNew A\n0.146â€‰578\n0.146\\,578\nNew A\n0.124â€‰962\n0.124\\,962\nCartwheel\n0.237â€‰148\n0.237\\,148\n0.088â€‰191\n0.088\\,191\n0.059â€‰717\n0.059\\,717\n0.062â€‰788\n0.062\\,788\nRoll on all fours\nNew A\n0.086â€‰242\n0.086\\,242\nNew A\n0.091â€‰448\n0.091\\,448\nDance snippet B\nNew A\n0.096â€‰435\n0.096\\,435\nNew A\n0.150â€‰253\n0.150\\,253\nSimulation-Based Benchmarking Against MPC\nWe compare a multi-skill RL policy trained jointly on all Atlas motions presented in Table\n1\nagainst a whole-body MPC baseline developed at Boston Dynamics, which is representative of state-of-the-art model-based controllers\n[\n2\n]\n. Evaluations use a subset of motions drawn from all three reference sources (MoCap, ViCap, and animation). For MPC, we extract contact schedules from each reference using a simple heuristic that considers the distance between the foot and the ground, as well as the footâ€™s velocity. This heuristic is relatively straightforward for animated sequences (e.g.,\nhandstand invert\n), but it becomes more error-prone for MoCap and particularly ViCap motions, where unrealistic foot sliding and noisy contact timings lead to mislabelled contacts and necessitate careful manual tuning. The MPC stack natively supports interaction with feet and hands, but not contacts beyond those end-effectors. As a result, motions that involve knee, torso, or forearm contacts (e.g.,\narmy crawl\n,\nforward roll\n,\ncrawl on all fours\n) fall outside its default capability and were excluded from the MPC comparison. In contrast, the learned policy does not require explicit contact labeling, is more tolerant of model mismatch and contact-timing errors, and readily accommodates interactions with arbitrary body parts.\nFor a quantitative comparison, Table\n2\nreports\nMAE\nâ€‹\n(\nğ’’\n)\n\\mathrm{MAE}(\\bm{q})\nand\nMAD\nâ€‹\n(\nR\n)\n\\mathrm{MAD}(R)\nfor MPC and RL.\nEmpirically, for\nwalk\n, MPC and RL perform nearly the same: the contact schedule is clean and easy to label, and the trajectory never pushes the robot beyond its operational bounds, which suits MPC. On the other hand, RL outperforms MPC on the more dynamic behaviors, namely\ncartwheel\nand\njog\n. In particular, for\njog\n, we observe MPC degradation tied to inconsistent contact annotation at the start of the reference despite extensive manual refinements. Several sequences fail under MPC (reported as â€œâ€“â€ in the MPC columns):\ndance B\n(inaccurate contact sequence and sliding),\nhandstand invert\n(controller saturation on arm torque under a point-contact constraint), and\nroll on all fours\n(difficulty regulating extended toe-contact phases without knee collisions). The videos of the references augmented with contact labels, as well as rollouts from MPC and the RL policy, are provided in Movie S5.\nDISCUSSION\nFigure 4\n:\nOverview of\nZEST\n, which consists of three main stages.\n(1) Reference data:\nA diverse set of motions from MoCap, ViCap, and keyframe animation is processed; MoCap/ViCap references are kinematically retargeted to the target robot.\n(2) Training setup and MDP formulation:\nIn simulation, the policy is trained using only on-robot signals and the next target state from the reference, while a separate critic receives privileged information (e.g., true base velocity, contact forces) to accelerate learning. To handle long-horizon clips and scale beyond single skills, an adaptive sampling scheme is proposed: trajectories are segmented into fixed-duration bins; a per-bin difficulty level metric is updated via an EMA of failure scores; and a categorical sampler biases reset-state selection toward harder bins while avoiding catastrophic forgetting of easier behaviors. A model-based virtual assistive wrench is applied at the base to stabilize training for highly dynamic behaviors; the current binâ€™s difficulty level modulates its magnitude and is automatically annealed to zero as tracking improves.\n(3) Zero-shot deployment:\nThe trained policy is deployed directly to the physical robot without any fine-tuning.\nZEST\nbridges motion datasets and robust whole-body control on legged robots. In doing so, it delivers three core contributions. First, it establishes a generic and streamlined motion-imitation recipe that trains RL policies in a single stage from heterogeneous reference sources with various imperfections (high-fidelity MoCap, noisy ViCap, and non-physics-constrained animation) and deploys it on hardware zero-shot.\nZEST\navoids complexities such as contact labels, reference or observation windows, state-estimators, extensive reward shaping, and multi-stage training. Second, our work achieves a new state of the art in demonstrated hardware capabilities across multiple embodiments. On Atlas, we demonstrate, to our knowledge, the first dynamic multi-contact behaviors on a full-size humanoid. Concurrently, on the Unitree G1, we present the first physical motion transfer of highly dynamic skills, such as dancing and box climbing, directly from video. Third, while focused on Atlas, the same policy structure and training recipe carry over to distinct morphologies such as Boston Dynamicsâ€™ Spot and Unitreeâ€™s G1. Compared to a state-of-the-art whole-body MPC baseline, which prescribes contacts only at predefined end-effectors, the learned controller eliminates the need for contact schedules and motion-specific costs, is more robust to modeling and contact-timing mismatches, and accommodates interactions with arbitrary body parts (e.g., knees, torso, forearms).\nOn the other hand, several limitations remain and motivate our next steps. First, we do not evaluate the generalization of our multi-skill policy to motions outside the training set â€“ unseen skills that are close to, but not identical with, the training distribution. Such tests would probe whether the policy is able to learn transferable control primitives as opposed to purely overfitting to specific references. We defer this generalization study to future work. Second, the current formulation is proprioceptive and assumes flat and non-slippery terrain; explicit perception of uneven or compliant environments is left for future work. Finally, sim-to-real hinges on reasonable modeling; while we provide a practical procedure involving PLA modeling and a principled selection of armature-dependent PD gains, fully automated system identification remains an open problem. Particularly challenging are complex phenomena that are difficult to capture with first-principles models, and are likely to necessitate data-driven methodologies.\nFurthermore, we outline several directions that extend the scope of this work. First, we will pursue general tracking of previously unseen motions via compact motion embeddings or with zero-/few-shot adaptations, paired with continual-learning strategies to expand libraries without catastrophic forgetting. In parallel, we plan to move beyond full-reference control toward partially conditioned interfaces. One approach is to train a teacher to track a large corpus with full references and then distill the knowledge to a student that accepts sparse, human-interpretable inputs. These inputs could include keyframes, short pose snippets, object and scene keyframes, or language commands, following the masked-conditioning paradigm explored in the literature on physics-based character control\n[\n55\n]\n. Additionally, we will explore high-level generative planners that map user commands to motion plans, which a general-purpose RL-based tracker can execute, thereby closing the loop between behavior synthesis and control, as demonstrated in recent character-control systems\n[\n36\n,\n37\n]\n.\nMATERIALS AND METHODS\nWe now present the technical details of\nZEST\n. This section details its architecture, as illustrated in Figure\n4\n, with a primary focus on the goal-conditioned MDP formulation. We describe the core components of this framework, including modeling, curation of the reference motion dataset, the training setup, and the policy deployment process on hardware.\nSystems and Modeling\nWe evaluated\nZEST\nusing three robots with different morphologies: the full-scale Boston Dynamics Atlas humanoid (\n30\n30\nDoF,\n1.8\nâ€‹\nm\n1.8\\mathrm{m}\n,\n100\nâ€‹\nk\nâ€‹\ng\n100\\mathrm{kg}\n), the smaller Unitree G1 humanoid (\n29\n29\nDoF,\n1.2\nâ€‹\nm\n1.2\\mathrm{m}\n,\n35\nâ€‹\nk\nâ€‹\ng\n35\\mathrm{kg}\n), and the Boston Dynamics Spot quadruped (\n12\n12\nDoF,\n33\nâ€‹\nk\nâ€‹\ng\n33\\mathrm{kg}\n). Our RL approach uses the Isaac Lab simulator\n[\n56\n]\n. However, a primary challenge for humanoids is to efficiently model the Parallel-Linkage Actuator (PLA), which is used in ankles, knees, and waists. While a crucial design for dynamic performance, simulating their closed-chain mechanisms results in stiff dynamics that are computationally expensive to calculate. To resolve the trade-off between simulation fidelity and computational efficiency, we developed a series of progressive approximations for the PLAs. A brief overview of these models is provided below, with the approach illustrated in Figure\n5\n. Detailed mathematical derivations are available in the Supplementary Materials (Section S2).\n1.\nLocally Projected Model (Massless-Links Approx.)\n: Our first approximation is motivated by the lightweight design of the PLAâ€™s support links. We assume these links are massless while retaining the inertia of the motor armatures and the main kinematic chain of PLA. This yields a simulator-compatible model where the effective motor armature is configuration-dependent.\n2.\nDynamic Armature Model (Diagonal Approx.)\n: To handle coupled PLAs (e.g., a humanoidâ€™s ankles), which produce non-diagonal armature matrices that standard simulators cannot handle, we use a Jacobi approximation. This method approximates the off-diagonal dynamic effects, introducing only a minor, transient error.\n3.\nNominal Armature Model (Fixed-Configuration Approx.)\n: Finally, to eliminate the computational overhead of updating armatures at every timestep (\nâ‰ˆ\n\\approx\n20% slowdown), we compute the armature values once at a single, nominal configuration and fix them. This provides a computationally cheap model and a principled basis for designing the robotâ€™s PD controller gains.\nWhile the Spot model proposed in\n[\n57\n]\nwas sufficient for generating most behaviors, it proved insufficient for the more dynamic triple backflip behavior. To bridge the sim-to-real gap, we developed a more detailed actuator model that incorporates the robotâ€™s power-limiting algorithm and simulates nonlinear effects, such as motor magnet saturation and torque losses resulting from transmission inefficiencies and friction. Static parameters of the model (e.g., rotor inertia, friction) were obtained from manufacturer specifications. The positive and negative work efficiencies were identified by optimizing the model against real-world data logged from the robot and randomly selected during training within ranges. See Supplementary Materials (Section S3) for derivations.\nFigure 5\n:\nProgressive simplification of proposed Parallel-Linkage Actuator models.\nIt illustrates our modeling approach using a representative humanoid leg, where a motor on the thigh actuates the knee via a four-bar linkage. We begin with\n(1) the Exact Model\n, which fully resolves the closed-loop dynamics. We then introduce a series of progressively simplified models.\n(2) The Locally Projected Model\nassumes massless support links.\n(3) The Dynamic/Nominal Armature Model\nuses a Jacobi approximation for coupled joints, and finally, the most efficient model calculates armature values at a single fixed configuration.\nReference Motion Pipeline\nOur framework learns from a diverse library of reference motions, derived from MoCap, ViCap, and keyframe animation. All human-centric data sourced through MoCap or ViCap is mapped to the target robotâ€™s skeleton through kinematic retargeting. Crucially, we assume no contact labeling for any data source, relying solely on kinematic information. The following subsections provide a brief overview of our data sources and kinematic retargeting pipeline; comprehensive implementation details are deferred to the Supplementary Materials (Section S4).\nData Sources\nMoCap data, captured using Xsens\n[\n58\n]\nand Vicon\n[\n59\n]\nsystems, serves as our highest-fidelity source for natural human movements, requiring only minimal post-processing. To leverage the vast amount of motion available in casual videos, our ViCap pipeline reconstructs 3D human motion from a single camera. This process combines\nMegaSaM\n[\n60\n]\nfor robust camera motion and scene estimation with\nTRAM\n[\n61\n]\nfor human pose estimation, a pairing that we found significantly reduces artifacts, such as pose jitter and foot-skidding. Finally, we use Keyframe Animation to create motions that are not humanly possible or are intended for non-humanoid morphologies, such as the Spot quadruped.\nKinematic Retargeting\nTo transfer MoCap and ViCap data to a target robot, we solve a spacetime optimization\n[\n62\n]\n. The objective function comprises several weighted costs: tracking the root position and orientation of the source motion, aligning corresponding bones via alignment frames, and regularizing the output by penalizing joint velocities. During this process, we also jointly optimize for a uniform scale and time resampling of the source data to ensure any ballistic motions are physically consistent with real-world gravity. The optimization is subject to the robotâ€™s forward kinematics and non-penetration constraints with itself and the ground.\nTraining Setup and MDP Formulation\nWe formulate\nZEST\nas a goal-conditioned MDP,\nâ„³\n=\n(\nğ’®\n,\nğ’œ\n,\nğ’¢\n,\nP\n,\nr\n,\nÎ³\n)\n\\mathcal{M}=(\\mathcal{S},\\mathcal{A},\\mathcal{G},P,r,\\gamma)\n. The state\ns\nt\nâˆˆ\nğ’®\ns_{t}\\in\\mathcal{S}\nrepresents the robot and environment states, and\na\nt\nâˆˆ\nğ’œ\na_{t}\\in\\mathcal{A}\nis the continuous action corresponding to motor action. At time\nt\nt\n, the agent receives the observation,\no\nt\n=\nÏˆ\nâ€‹\n(\ns\nt\n,\ng\nt\n)\no_{t}=\\psi(s_{t},g_{t})\nand the target,\ng\nt\nâˆˆ\nğ’¢\ng_{t}\\in\\mathcal{G}\n, selects actions\na\nt\nâˆ¼\nÏ€\nÎ¸\n(\nâ‹…\n|\no\nt\n,\ng\nt\n)\na_{t}\\!\\sim\\!\\pi_{\\theta}(\\,\\cdot\\,|\\,o_{t},g_{t})\n, obtains a reward\nr\nt\n=\nr\nâ€‹\n(\ns\nt\n,\na\nt\n,\ng\nt\n)\nr_{t}=r(s_{t},a_{t},g_{t})\n, and the system evolves to a new state\ns\nt\n+\n1\nâˆ¼\nP\n(\nâ‹…\n|\ns\nt\n,\na\nt\n)\ns_{t+1}\\!\\sim\\!P(\\,\\cdot\\,|\\,s_{t},a_{t})\n. The goal variable\ng\nt\ng_{t}\nencodes the reference target to be tracked and can be instantiated in several forms: (i) a time-indexed reference snippet\ns\n^\nt\n:\nt\n+\nH\n\\hat{s}_{t:t+H}\nwith\nH\nâ‰¥\n0\nH\\!\\geq\\!0\n(e.g., a single step or a short window); (ii) a latent embedding\nz\nt\n=\nf\nâ€‹\n(\ns\n^\nt\n:\nt\n+\nH\n)\nz_{t}=f(\\hat{s}_{t:t+H})\nof such a snippet; or (iii) a phase variable\nÏ•\nt\nâˆˆ\n[\n0\n,\n1\n]\n\\phi_{t}\\in[0,1]\nplus a trajectory identifier in the case of multiple reference motions. The policy parameters\nÎ¸\n\\theta\nare optimized to maximize the expected discounted return over a horizon\nT\nT\nJ\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nâ€‹\n[\nâˆ‘\nt\n=\n0\nT\nâˆ’\n1\nÎ³\nt\nâ€‹\nr\nâ€‹\n(\ns\nt\n,\na\nt\n,\ng\nt\n)\n]\n,\nJ(\\theta)\\;=\\;\\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1}\\gamma^{t}\\,r(s_{t},a_{t},g_{t})\\right],\nwhere the expectation is over rollouts induced by\nP\nP\nand\nÏ€\nÎ¸\n\\pi_{\\theta}\n, randomized initial states, and the goal distribution.\nTraining is conducted in Isaac Lab using Proximal Policy Optimization (PPO)\n[\n63\n]\nas the RL algorithm. To stabilize optimization and improve convergence, all observations are normalized using an empirical running-average scheme. The actor and critic networks are modeled as Multi-Layer Perceptrons (MLPs) with three hidden layers and ELU activations.\nTo accelerate training while preserving the hardware deployability, we utilize an asymmetric actor-critic architecture\n[\n64\n]\n. The critic is trained using extra, privileged information available only in simulations, which enables it to estimate the value function more accurately. In contrast, the policy is limited to the observations available during hardware deployment, ensuring it can be transferred without any state estimator. This approach prevents lag and bias from estimator fusion, sidesteps the fragile contact and terrain assumptions often required by state estimators, and eliminates the need for added complexity that is not captured during training. With the learning algorithm established, we now define the specifics of the MDP.\nActor and Critic Observations\nThe Actor, otherwise known as policy, receives proprioception information along with target references\nğ¨\nt\n=\n(\nğ¨\nprop\n,\nğ¨\nref\n)\n,\n\\mathbf{o}_{t}=(\\mathbf{o}_{\\text{prop}},\\ \\mathbf{o}_{\\text{ref}}),\nthe proprioceptive part\nğ¨\nprop\n=\n(\nğ\nI\nâ€‹\nT\nT\n,\nğ’ˆ\nI\nT\n,\nğ’’\nj\n,\nğ’’\nË™\nj\n,\nğ’‚\nt\nâˆ’\n1\n)\n{\\mathbf{o}_{\\text{prop}}=\\big({}_{T}\\bm{\\omega}_{IT},\\ {}_{T}\\bm{g}_{I},\\ \\bm{q}_{j},\\ \\dot{\\bm{q}}_{j},\\ \\bm{a}_{t-1}\\big)}\ncontains only instantaneous onboard measurements. We intentionally exclude quantities that require a state estimator, such as global base pose or linear velocity. Here,\nğ\nI\nâ€‹\nT\nT\n{}_{T}\\bm{\\omega}_{IT}\nis the absolute angular velocity of the torso-attached IMU frame\n{\nğ’¯\n}\n\\{\\mathcal{T}\\}\n,\nğ’ˆ\nI\nT\n{}_{T}\\bm{g}_{I}\nis the normalized gravity vector expressed in\n{\nğ’¯\n}\n\\{\\mathcal{T}\\}\n, and\n(\nğ’’\nj\n,\nğ’’\nË™\nj\n,\nğ’‚\nt\nâˆ’\n1\n)\n(\\bm{q}_{j},\\dot{\\bm{q}}_{j},\\bm{a}_{t-1})\ndenote joint positions, joint velocities, and the previous action. We hypothesize that including\nğ’‚\nt\nâˆ’\n1\n\\bm{a}_{t-1}\nis sufficient to infer essential information, such as contact states and base linear velocity, under nominal terrain conditions without resorting to long observation histories; it also provides the minimal temporal context needed to enforce action smoothness.\nThe reference observations\nğ¨\nref\n=\n(\nğ’“\n^\nI\nâ€‹\nB\nz\nI\n,\nğ’—\n^\nI\nâ€‹\nB\nB\n,\nğ\n^\nI\nâ€‹\nB\nB\n,\nğ’ˆ\n^\nI\nB\n,\nğ’’\n^\nj\n)\n{\\mathbf{o}_{\\text{ref}}=\\big({}_{I}\\hat{\\bm{r}}^{z}_{IB},\\ {}_{B}\\hat{\\bm{v}}_{IB},\\ {}_{B}\\hat{\\bm{\\omega}}_{IB},\\ {}_{B}\\hat{\\bm{g}}_{I},\\ \\hat{\\bm{q}}_{j}\\big)}\nencode the next target state extracted from the demonstration:\nğ’“\n^\nI\nâ€‹\nB\nz\nI\n{}_{I}\\hat{\\bm{r}}^{z}_{IB}\nis the base height;\nğ’—\n^\nI\nâ€‹\nB\nB\n{}_{B}\\hat{\\bm{v}}_{IB}\nand\nğ\n^\nI\nâ€‹\nB\nB\n{}_{B}\\hat{\\bm{\\omega}}_{IB}\nare the absolute linear and angular base velocities expressed in the base frame\n{\nâ„¬\n}\n\\{\\mathcal{B}\\}\n;\nğ’ˆ\n^\nI\nB\n{}_{B}\\hat{\\bm{g}}_{I}\nis the gravity direction in\n{\nâ„¬\n}\n\\{\\mathcal{B}\\}\n; and\nğ’’\n^\nj\n\\hat{\\bm{q}}_{j}\nare the reference joint positions. We deliberately avoid future-reference windows beyond this immediate next target as our ablations showed no benefit for performance or convergence speed.\nThe Critic receives the policy observations in addition to extra signals referred to as privileged information. This privileged input includes the base linear velocity, base height, end-effector positions, velocities, and contact forces, as well as curriculum-related signals. Additional details are provided in the Supplementary (Table S3).\nActions and Joint-Level Control\nThe policy outputs residual actions\nğ’‚\nt\n\\bm{a}_{t}\n, which are added to the reference joint positions before sending them as position targets to the joint-level PD controllers:\nğ’’\nj\ncmd\n=\nğ’’\n^\nj\n+\nğšº\nâ€‹\nğ’‚\nt\n,\n\\bm{q}^{\\mathrm{cmd}}_{j}=\\hat{\\bm{q}}_{j}+\\bm{\\Sigma}\\,\\bm{a}_{t},\nwhere\nğšº\n\\bm{\\Sigma}\nis a positive-definite diagonal matrix of per-DoF action scales. These scales set the amplitude of residual corrections and exploration on each axis and reflect whether additional feedforward torque is needed for reference tracking: actuators that require more assistance (e.g., hips and knees) receive larger scales, whereas those adequately handled by the PD control loop alone (e.g., the robotâ€™s head and wrists) use smaller scales. The PD gains are chosen by modeling each joint as an independent second-order system:\nI\nâ€‹\nÎ¸\nâ€²â€²\nâ€‹\n(\nt\n)\n+\nK\nd\nâ€‹\nÎ¸\nâ€²\nâ€‹\n(\nt\n)\n+\nK\np\nâ€‹\nÎ¸\nâ€‹\n(\nt\n)\n=\n0\n,\nI\\,\\theta^{\\prime\\prime}(t)+K_{d}\\,\\theta^{\\prime}(t)+K_{p}\\,\\theta(t)=0,\nwhere\nI\nI\nis the nominal armature value (refer to the Modeling section) about that axis. The gains\nK\np\nK_{p}\nand\nK\nd\nK_{d}\nare heuristically tuned to achieve a critically damped response with a desired natural frequency\nÏ‰\nn\n>\n0\n\\omega_{n}>0\n,\nK\np\n=\nI\nâ€‹\nÏ‰\nn\n2\n,\nK\nd\n=\n2\nâ€‹\nI\nâ€‹\nÏ‰\nn\n.\nK_{p}=I\\,\\omega_{n}^{2},\\qquad K_{d}=2\\,I\\,\\omega_{n}.\nIn practice, we set\nÏ‰\nn\n\\omega_{n}\nto balance between responsiveness and robustness. It is set high enough for fast tracking, but not so high that the loop becomes brittle during deployment or that simulation requires very small time steps to resolve stiff dynamics with fast transients, which would in turn increase training times. The same PD gains are used in simulation and on hardware to minimize sim-to-real mismatch.\nReward Terms\nThe total reward combines three main components: a tracking reward, a regularization reward, and a survival reward,\nr\ntotal\n=\nr\ntrack\n+\nr\nreg\n+\nr\nsurvival\n.\nr_{\\text{total}}\\;=\\;r_{\\text{track}}+r_{\\text{reg}}+r_{\\text{survival}}.\n(1)\nAll terms are deliberately kept generic: nothing is task- or motion-specific, and the reward setup does not rely on reference contact labels from the demonstrations, making it broadly applicable across behaviors. The tracking reward measures how well the policy follows the reference motion. It can be expressed in different forms, such as a quadratic penalty or a decaying exponential kernel; in our case, we adopt the latter, which yields a dense, bounded shaping signal that emphasizes accuracy near the reference and varies smoothly with the tracking error:\nr\ntrack\n=\nâˆ‘\ni\nc\nt\ni\nâ€‹\nexp\nâ¡\n(\nâˆ’\nÎº\nâ€‹\nâ€–\nğ\ni\nâ€–\n2\nÏƒ\ni\n2\n)\n,\nr_{\\text{track}}\\;=\\;\\sum_{i}c_{t_{i}}\\exp\\!\\Bigl(-\\,\\kappa\\frac{\\|\\mathbf{e}_{i}\\|^{2}}{\\sigma_{i}^{2}}\\Bigr),\n(2)\nwhere\nc\nt\ni\nc_{t_{i}}\nis the reward weight for term\ni\ni\n, and\nğ\ni\n\\mathbf{e}_{i}\ndenotes the error between the current and reference values for quantities such as base linear velocity, base angular velocity, base pose, joint positions, and keybody poses. The regularization component, denoted\nr\nreg\nr_{\\text{reg}}\n, aggregates penalties that encourage smooth and physically feasible behavior. Concretely, we penalize rapid changes in actions across time steps, large joint accelerations, and violations of joint position and torque limits. The survival reward is a constant positive term granted at each step,\nr\nsurvival\n=\nc\nsurvival\nr_{\\text{survival}}=c_{\\text{survival}}\n, which discourages premature termination by rewarding longer episodes. Full functional forms and hyperparameters for all reward terms are provided in the Supplementary Materials (Table S4).\nEarly Terminations\nTo steer exploration toward promising states and avoid wasting samples on highly undesirable trajectories, we trigger early termination under conditions such as contact forces exceeding a predefined threshold or a large deviation of the agent from the reference.\nDomain Randomization\nTo ensure robust zero-shot sim-to-real transfer, we employ domain randomization to prevent the policy from overfitting to the nominal simulation model. Our domain randomization strategy includes injecting Gaussian noise into observations, applying random impulsive pushes to the robot at random times, varying surface friction coefficient, and slightly randomizing link masses around their nominal values. We used these randomizations carefully, as excessive variability can degrade performance by making policies overly conservative or aggressive. The details of the domain randomizationâ€™s hyperparameters can be found in the Supplementary (Table S5).\nAdaptive Reference State Initialization\nTo initialize our environment during training, we first adopt the Reference State Initialization (RSI) strategy introduced in\n[\n29\n]\n. At each reset, a random phase\nÏ•\ninit\nâˆ¼\nğ’°\nâ€‹\n(\n0\n,\n1\n)\n\\phi_{\\text{init}}\\!\\sim\\!\\mathcal{U}(0,1)\nis sampled. Then the environment is initialized at the corresponding reference state\ns\n^\nâ€‹\n(\nÏ•\ninit\n)\n\\hat{s}(\\phi_{\\text{init}})\n. RSI exposes the policy to relevant states along the trajectory from the outset, decouples episode length from demonstration length, and improves sample efficiency. Additionally, when a rollout reaches the end of a demonstration, we insert a brief dwell and then reset by timeout (not failure), which further increases the number of informative transitions.\nWhile uniform RSI is generally effective, with long-horizon trajectories or large sets of trajectories, it can be inefficient because it blindly samples phases, oversampling already-mastered regions and undersampling difficult segments. To bias sampling toward where learning is most needed, we use an adaptive RSI scheme: all demonstrations are partitioned into fixed\nN\nN\n-step bins, and each bin\nb\nb\nmaintains a failure level\nf\nb\nf_{b}\nderived from recent tracking performance (based on a similarity score metric) and updated with an EMA filter. At reset, a bin index\nk\nk\nis drawn from a categorical distribution whose logits are proportional to\n{\nf\nb\n}\n\\{f_{b}\\}\n, and a phase is then sampled uniformly within the selected bin to obtain\nÏ•\ninit\n\\phi_{\\text{init}}\n; a small floor probability is retained for every bin to prevent forgetting previously acquired skills. This procedure increases sampling density on difficult motions and difficult phases while preserving global coverage; additional implementation details are provided in the Supplementary (Section S5).\nAssistive Wrench Automatic Curriculum\nWhile the framework presented so far is sufficient to track most motions in our library, skills with highly dynamic base rotations and large angular rates (e.g., handstands, cartwheels, backflips) tend to trigger immediate early terminations under RSI alone and converge significantly slower compared to simpler behaviors such as walking and running. To address this, we employ an automatic assistive-wrench curriculum, serving as a continuation strategy that gradually increases task difficulty: a virtual spatial wrench is applied at the base and annealed as tracking improves, analogous to a gymnastics coach providing diminishing support to their student. Similar assistive-force curricula have been explored in previous related works\n[\n65\n,\n66\n,\n67\n,\n68\n]\n. The assistive wrench is computed in a model-based fashion using a PD term on the base pose tracking error, together with a feedforward component that compensates nominal torso dynamics. To keep assistance partially supportive while still compelling the agent to learn the skill for itself, we scale the assistive wrench by a gain\nÎ²\nâˆˆ\n(\n0\n,\nÎ²\nmax\n)\n{\\beta\\!\\in\\!(0,\\beta_{\\text{max}})}\nwith\nÎ²\nmax\n<\n1.0\n{\\beta_{\\text{max}}<1.0}\n. The annealing schedule is driven automatically by the per-bin failure levels introduced in the adaptive RSI procedure: bins with higher failure receive stronger assistance initially and are relaxed more aggressively as tracking improves. Assistance decays quickly and ultimately vanishes once a target tracking performance is reached. Additional details are provided in the Supplementary (Section S6).\nHardware Deployment Pipeline\nAn important yet often overlooked aspect of embodied intelligence is the hardware deployment pipeline. Since our research involves frequent testing on physical robots, we created an efficient, automated, framework- and hardware-agnostic pipeline to ensure safe and consistent releases across multiple platforms. This pipeline follows a three-stage workflow. First, during training, policy checkpoints are exported in ONNX format and logged to a registry, along with detailed metadata that captures all relevant configurations. For validation, these artifacts are retrieved, and the metadata is used to automatically configure the controller in our\nevaluation simulator\n[\n69\n]\n, which mirrors the hardwareâ€™s software stack. Once a policy performs as expected, it is deployed to the physical robot using the exact same automated process, guaranteeing consistency and eliminating manual setup errors. It is worth noting that while some deployment workflow is necessary for hardware testing, the learned policy is not tied to this specific pipeline; we include it primarily to accelerate iteration and reduce incidental sources of error during sim-to-sim evaluation and hardware testing.\nAcknowledgments\nWe would like to thank Merritt Moore for the MoCap data collection. Moreover, we note that while all ideas, methods, and results are original to the authors, AI-based tools were used solely to refine the clarity and readability of the manuscript text.\nAuthor contributions:\nThe\nCore Contributors\ndesigned and implemented the ZEST pipeline; trained most of the policies, and oversaw the hardware experiments on robots. They also contributed to the writing of the manuscript. The\nAdditional Contributors\nassisted with engineering tasks to support the project, including deployment code on hardware, data collection and curation, as well as supporting hardware experiments. The\nProject Leads\nset the overarching vision by defining the core scientific goals. They provided technical guidance and coordinated the teamâ€™s efforts. Finally, they helped shape the overall narrative of the paper.\nCompeting interests:\nThe authors declare that they have no competing interests.\nData and materials availability:\nAll data needed to evaluate the conclusions in the paper are present in the paper or the Supplementary Materials.\nSupplementary materials\nSection S1.\nNomenclature\nSection S2.\nImplementation Details: Actuator Modeling\nSection S3.\nImplementation Details: Spot Modeling\nSection S4.\nImplementation Details: Motion Dataset\nSection S5.\nImplementation Details: Adaptive RSI Sampling\nSection S6.\nImplementation Details: Assistive Wrench Curriculum\nFigure S1.\nIllustration of Main Chain and Support Chain\nFigure S2.\nAtlas Ankle Torque Limits Profile\nFigure S3.\nEffect of Reference and Observation Windows on Success.\nFigure S4.\nRobustness to Model Uncertainty and Disturbances.\nFigure S5.\nRobustness Maps from Varied Initial Conditions.\nTable S1.\nLibrary of Reference Motions for Multi-Skill Policy\nTable S2.\nNormalized MSE of Simplified PLA Models.\nTable S3.\nObservation Terms Summary\nTable S4.\nReward Terms Summary\nTable S5.\nDomain Randomization Summary\nTable S6.\nMDP Hyperparameters\nTable S7.\nPPO Hyperparameters\nOther Supplementary Material for this manuscript includes:\nMovie S1.\nRetargeted MoCap-derived References\nMovie S2.\nRetargeted ViCap-derived References\nMovie S3.\nKeyframe Animation References\nMovie S4.\nSimulation-based Multi-Skill Policy Evaluation\nMovie S5.\nSimulation-based Benchmarking against MPC\nMovie S6.\nBox-Climbing: Repeatability Testing on Hardware.\nMovie S7.\nBox-Climbing: Robustness Testing on Hardware.\nMovie S8.\nImpact of PLA Modeling on Sim-to-Real Transfer.\nReferences\nKuindersma et al. [2016]\nScott Kuindersma, Robin Deits, Maurice F. Fallon, Andres Valenzuela, Hongkai Dai, Frank Permenter, Twan Koolen, Pat Marion, and Russ Tedrake.\nOptimization-based locomotion planning, estimation, and control design for the atlas humanoid robot.\nAuton. Robots\n, 40(3):429â€“455, 2016.\n10.1007/s10514-015-9479-3\n.\nBoston Dynamics [2024a]\nBoston Dynamics.\nPicking up momentum.\nBoston Dynamics Blog, 2024a.\nURL\nhttps://bostondynamics.com/blog/picking-up-momentum/\n.\nBlog post on Model Predictive Control for Atlas; accessed 2025-08-12.\nKuindersma [2020]\nScott Kuindersma.\nRecent progress on atlas, the worldâ€™s most dynamic humanoid robot.\nYouTube video, 2020.\nURL\nhttps://www.youtube.com/watch?v=EGABAx52GKI\n.\nBoston Dynamics [2023]\nBoston Dynamics.\nAtlas gets a grip.\nYouTube video, 2023.\nURL\nhttps://www.youtube.com/watch?v=-e1_QhJ1EhQ\n.\nBoston Dynamics [2024b]\nBoston Dynamics.\nHd atlas manipulates | boston dynamics.\nYouTube video, 2024b.\nURL\nhttps://www.youtube.com/watch?v=LeeiN9smjjY\n.\nKudruss et al. [2015]\nManuel Kudruss, Maximilien Naveau, Olivier Stasse, Nicolas Mansard, Christian Kirches, Philippe SouÃ¨res, and Katja D. Mombaur.\nOptimal control for whole-body motion generation using center-of-mass dynamics for predefined multi-contact configurations.\nIn\nIEEE-RAS Int. Conf. Humanoid Robots (Humanoids)\n, pages 684â€“689, 2015.\n10.1109/HUMANOIDS.2015.7363428\n.\nLi and Wensing [2025]\nHe Li and Patrick M. Wensing.\nCafe-mpc: A cascaded-fidelity model predictive control framework with tuning-free whole-body control.\nIEEE Trans. Robotics\n, 41:837â€“856, 2025.\n10.1109/TRO.2024.3504132\n.\nGrandia et al. [2023]\nRuben Grandia, Farbod Farshidian, Espen Knoop, Christian Schumacher, Marco Hutter, and Moritz BÃ¤cher.\nDOC: Differentiable optimal control for retargeting motions onto legged robots.\nACM Trans. Graph.\n, 42(4):96:1â€“96:14, 2023.\n10.1145/3592454\n.\nSleiman et al. [2023]\nJean-Pierre Sleiman, Farbod Farshidian, and Marco Hutter.\nVersatile multi-contact planning and control for legged loco-manipulation.\nScience Robotics\n, 8(81):eadg5014, 2023.\n10.1126/scirobotics.adg5014\n.\nRudin et al. [2021]\nNikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter.\nLearning to walk in minutes using massively parallel deep reinforcement learning.\nIn\nConference on Robot Learning (CoRL)\n, volume 164 of\nProceedings of Machine Learning Research\n, pages 91â€“100. PMLR, 2021.\nZhao et al. [2020]\nWenshuai Zhao, Jorge PeÃ±a Queralta, and Tomi Westerlund.\nSim-to-real transfer in deep reinforcement learning for robotics: A survey.\nIn\nIEEE Symposium Series on Computational Intelligence (SSCI)\n, pages 737â€“744, 2020.\n10.1109/SSCI47803.2020.9308468\n.\nHa et al. [2025]\nSehoon Ha, Joonho Lee, Michiel van de Panne, Zhaoming Xie, Wenhao Yu, and Majid Khadiv.\nLearning-based legged locomotion: State of the art and future perspectives.\nInt. J. Robot. Res.\n, 44(8):1396â€“1427, 2025.\n10.1177/02783649241312698\n.\nGu et al. [2025]\nZhaoyuan Gu, Junheng Li, Wenlan Shen, Wenhao Yu, Zhaoming Xie, Stephen McCrory, Xianyi Cheng, Abdulaziz Shamsah, Robert Griffin, C Karen Liu, et al.\nHumanoid locomotion and manipulation: Current progress and challenges in control, planning, and learning.\nIEEE/ASME Trans. Mechatronics\n, 2025.\nLee et al. [2020]\nJoonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter.\nLearning quadrupedal locomotion over challenging terrain.\nScience Robotics\n, 5(47):eabc5986, 2020.\n10.1126/scirobotics.abc5986\n.\nMiki et al. [2022]\nTakahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter.\nLearning robust perceptive locomotion for quadrupedal robots in the wild.\nScience Robotics\n, 7(62):eabk2822, 2022.\n10.1126/scirobotics.abk2822\n.\nSun et al. [2025]\nWandong Sun, Baoshi Cao, Long Chen, Yongbo Su, Yang Liu, Zongwu Xie, and Hong Liu.\nLearning perceptive humanoid locomotion over challenging terrain.\narXiv preprint arXiv:2503.00692\n, 2025.\nMiller et al. [2025a]\nA.J. Miller, Fangzhou Yu, Michael Brauckmann, and Farbod Farshidian.\nHigh-performance reinforcement learning on spot: Optimizing simulation parameters with distributional measures.\nIn\nIEEE Int. Conf. on Robotics and Automation (ICRA)\n, pages 9981â€“9988, 2025a.\n10.1109/ICRA55743.2025.11128575\n.\nBellegarda et al. [2022]\nGuillaume Bellegarda, Yiyu Chen, Zhuochen Liu, and Quan Nguyen.\nRobust high-speed running for quadruped robots via deep reinforcement learning.\nIn\n2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, pages 10364â€“10370, 2022.\n10.1109/IROS47612.2022.9982132\n.\nCheng et al. [2024]\nXuxin Cheng, Kexin Shi, Ananye Agarwal, and Deepak Pathak.\nExtreme parkour with legged robots.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 11443â€“11450, 2024.\n10.1109/ICRA57147.2024.10610200\n.\nHoeller et al. [2024]\nDavid Hoeller, Nikita Rudin, Dhionis V Sako, and Marco Hutter.\nAnymal parkour: Learning agile navigation for quadrupedal robots.\nScience Robotics\n, 9(88):eadi7566, 2024.\n10.1126/scirobotics.adi7566\n.\nRudin et al. [2025]\nNikita Rudin, Junzhe He, Joshua Aurand, and Marco Hutter.\nParkour in the wild: Learning a general and extensible agile locomotion policy using multi-expert distillation and rl fine-tuning.\narXiv preprint arXiv:2505.11164\n, 2025.\nOpenAI et al. [2020]\nOpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, RafaÅ‚ JÃ³zefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Jack Hessel, Rishabh Agarwal, Mikael Henaff, Lerrel Pinto, Pieter Abbeel, and Wojciech Zaremba.\nLearning dexterous in-hand manipulation.\nThe International Journal of Robotics Research\n, 39(1):3â€“20, 2020.\n10.1177/0278364919887447\n.\nHanda et al. [2023]\nAnkur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, Yashraj Narang, Jean-Francois Lafleche, Dieter Fox, and Gavriel State.\nDeXtreme: Transfer of agile in-hand manipulation from simulation to reality.\nIn\n2023 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 5962â€“5969, 2023.\n10.1109/ICRA48891.2023.10160216\n.\nLin et al. [2025]\nToru Lin, Kartik Sachdev, Linxi Fan, Jitendra Malik, and Yuke Zhu.\nSim-to-real reinforcement learning for vision-based dexterous manipulation on humanoids.\narXiv preprint arXiv:2502.20396\n, 2025.\nMa et al. [2025]\nYuntao Ma, Andrei Cramariuc, Farbod Farshidian, and Marco Hutter.\nLearning coordinated badminton skills for legged manipulators.\nScience Robotics\n, 10(102):eadu3922, 2025.\n10.1126/scirobotics.adu3922\n.\nZhang et al. [2025a]\nYuanhang Zhang, Yifu Yuan, Prajwal Gurunath, Tairan He, Shayegan Omidshafiei, Ali-akbar Agha-mohammadi, Marcell Vazquez-Chanlatte, Liam Pedersen, and Guanya Shi.\nFalcon: Learning force-adaptive humanoid loco-manipulation.\narXiv preprint arXiv:2505.06776\n, 2025a.\nDadiotis et al. [2025]\nIoannis Dadiotis, Mayank Mittal, Nikos Tsagarakis, and Marco Hutter.\nDynamic object goal pushing with mobile manipulators through model-free constrained reinforcement learning.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 13363â€“13369, 2025.\n10.1109/ICRA55743.2025.11128166\n.\nSchwarke et al. [2023]\nClemens Schwarke, Victor Klemm, Matthijs van der Boon, Marko Bjelonic, and Marco Hutter.\nCuriosity-driven learning of joint locomotion and manipulation tasks.\nIn\nConference on Robot Learning (CoRL)\n, volume 229 of\nProceedings of Machine Learning Research\n, pages 2594â€“2610. PMLR, 2023.\nPeng et al. [2018]\nXue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne.\nDeepMimic: Example-guided deep reinforcement learning of physics-based character skills.\nACM Trans. Graph.\n, 37(4):143:1â€“143:14, 2018.\n10.1145/3197517.3201311\n.\nBergamin et al. [2019]\nKevin Bergamin, Simon Clavet, Daniel Holden, and James Richard Forbes.\nDReCon: Data-driven responsive control of physics-based characters.\nACM Trans. Graph.\n, 38(6):206:1â€“206:11, 2019.\n10.1145/3355089.3356536\n.\nYuan and Kitani [2020]\nYe Yuan and Kris M. Kitani.\nResidual force control for agile human behavior imitation and extended motion synthesis.\nIn Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors,\nAdvances in Neural Information Processing Systems 33\n, pages 21763â€“21774, 2020.\n10.5555/3495724.3497550\n.\nLuo et al. [2023]\nZhengyi Luo, Jinkun Cao, Alexander W. Winkler, Kris M. Kitani, and Weipeng Xu.\nPerpetual humanoid control for real-time simulated avatars.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)\n, pages 10895â€“10904, 2023.\n10.1109/ICCV51070.2023.01000\n.\nPeng et al. [2021]\nXue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa.\nAMP: Adversarial motion priors for stylized physics-based character control.\nACM Trans. Graph.\n, 40(4):144:1â€“144:20, 2021.\n10.1145/3450626.3459670\n.\nPeng et al. [2022]\nXue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler.\nASE: Large-scale reusable adversarial skill embeddings for physically simulated characters.\nACM Trans. Graph.\n, 41(4):94:1â€“94:17, 2022.\n10.1145/3528223.3530110\n.\nTessler et al. [2023]\nChen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, and Xue Bin Peng.\nCALM: Conditional adversarial latent models for directable virtual characters.\nIn\nACM SIGGRAPH 2023 Conference Proceedings\n, 2023.\n10.1145/3588432.3591541\n.\nTevet et al. [2025]\nGuy Tevet, Yoni Kasten, Chen Tessler, Moshe Gabel, Amit H. Bermano, and Daniel Cohen-Or.\nCLoSD: Closing the loop between simulation and diffusion for multi-task character control.\nIn\nInternational Conference on Learning Representations (ICLR)\n, 2025.\nXu et al. [2025a]\nMichael Xu, Yi Shi, KangKang Yin, and Xue Bin Peng.\nParc: Physics-based augmentation with reinforcement learning for character controllers.\nIn\nSIGGRAPH 2025 Conference Papers\n, 2025a.\n10.1145/3721238.3730616\n.\nXu et al. [2025b]\nZhengyi Xu, Hung Yu Ling, Yu-Xiong Wang, and Liang-Yan Gui.\nIntermimic: Towards universal whole-body control for physics-based human-object interactions.\nIn\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)\n, 2025b.\nPeng et al. [2020]\nXue Bin Peng, Erwin Coumans, Tingnan Zhang, Tsang-Wei Edward Lee, Jie Tan, and Sergey Levine.\nLearning agile robotic locomotion skills by imitating animals.\nIn\nRobotics: Science and Systems (RSS)\n, 2020.\nGrandia et al. [2024]\nRuben Grandia, Espen Knoop, Michael A. Hopkins, Georg Wiedebach, Jared Bishop, Steven Pickles, David MÃ¼ller, and Moritz BÃ¤cher.\nDesign and control of a bipedal robotic character.\nIn\nRobotics: Science and Systems (RSS)\n, 2024.\nAllshire et al. [2025]\nArthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, and Angjoo Kanazawa.\nVisual imitation enables contextual humanoid control.\narXiv preprint arXiv:2505.03729\n, 2025.\nHe et al. [2025a]\nTairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobh, Chaoyi Pan, Zeji Yi, Guannan Qu, Kris M. Kitani, Jessica Hodgins, Linxi Jim Fan, Yuke Zhu, Changliu Liu, and Guanya Shi.\nASAP: Aligning simulation and real-world physics for learning agile humanoid whole-body skills.\nIn\nRobotics: Science and Systems (RSS)\n, 2025a.\nto appear.\nXie et al. [2025]\nWeiji Xie, Jinrui Han, Jiakun Zheng, Huanyu Li, Xinzhe Liu, Jiyuan Shi, Weinan Zhang, Chenjia Bai, and Xuelong Li.\nKungfubot: Physics-based humanoid whole-body control for learning highly-dynamic skills.\narXiv preprint arXiv:2506.12851\n, 2025.\nZhang et al. [2025b]\nTong Zhang, Boyuan Zheng, Ruiqian Nai, Yingdong Hu, Yen-Jen Wang, Geng Chen, Fanqi Lin, Jiongye Li, Chuye Hong, Koushil Sreenath, et al.\nHub: Learning extreme humanoid balance.\narXiv preprint arXiv:2505.07294\n, 2025b.\nFuchioka et al. [2023]\nYukiyasu Fuchioka, Tetsuya Ogata, and Shuuji Kajita.\nOpt-Mimic: Imitation of optimized trajectories for dynamic quadruped behaviors.\nIn\nIEEE Int. Conf. on Robotics and Automation (ICRA)\n, pages 6726â€“6732, 2023.\n10.1109/ICRA48891.2023.10161217\n.\nJenelten et al. [2024]\nFabian Jenelten, Junzhe He, Farbod Farshidian, and Marco Hutter.\nDtc: Deep tracking control.\nSci. Robotics\n, 9(86):eadh5401, 2024.\nBogdanovic et al. [2022]\nMiroslav Bogdanovic, Majid Khadiv, and Ludovic Righetti.\nModel-free reinforcement learning for robust locomotion using demonstrations from trajectory optimization.\nFront. Robot. AI\n, 9:854212, 2022.\n10.3389/frobt.2022.854212\n.\nSleiman et al. [2024]\nJean-Pierre Sleiman, Mayank K. Mittal, and Marco Hutter.\nGuided reinforcement learning for robust multi-contact loco-manipulation.\nIn\nConference on Robot Learning (CoRL)\n, volume 270 of\nProceedings of Machine Learning Research\n, pages 2210â€“2247. PMLR, 2024.\nHe et al. [2024]\nTairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris M. Kitani, Changliu Liu, and Guanya Shi.\nOmnih2o: Universal and dexterous human-to-humanoid whole-body teleoperation and learning.\nIn\nConference on Robot Learning (CoRL)\n, volume 270 of\nProceedings of Machine Learning Research\n, pages 1516â€“1540. PMLR, 2024.\nHe et al. [2025b]\nTairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia Xu, Zhenyu Jiang, Jan Kautz, Changliu Liu, Guanya Shi, Xiaolong Wang, Linxi Jim Fan, and Yuke Zhu.\nHOVER: Versatile neural whole-body controller for humanoid robots.\nIn\nIEEE Int. Conf. on Robotics and Automation (ICRA)\n, 2025b.\nto appear.\nChen et al. [2025]\nZixuan Chen, Mazeyu Ji, Xuxin Cheng, Xuanbin Peng, Xue Bin Peng, and Xiaolong Wang.\nGmt: General motion tracking for humanoid whole-body control.\narXiv preprint arXiv:2506.14770\n, 2025.\nLiao et al. [2025]\nQiayuan Liao, Takara E. Truong, Xiaoyu Huang, Guy Tevet, Koushil Sreenath, and C. Karen Liu.\nBeyondmimic: From motion tracking to versatile humanoid control via guided diffusion.\nCoRR\n, abs/2508.08241, 2025.\n10.48550/ARXIV.2508.08241\n.\nYang et al. [2025]\nLujie Yang, Xiaoyu Huang, Zhen Wu, Angjoo Kanazawa, Pieter Abbeel, Carmelo Sferrazza, C. Karen Liu, Rocky Duan, and Guanya Shi.\nOmniretarget: Interaction-preserving data generation for humanoid whole-body loco-manipulation and scene interaction.\nCoRR\n, abs/2509.26633, 2025.\n10.48550/ARXIV.2509.26633\n.\nLuo et al. [2025]\nZhengyi Luo, Ye Yuan, Tingwu Wang, Chenran Li, Sirui Chen, Fernando CastaÃ±eda, Zi-Ang Cao, Jiefeng Li, David Minor, Qingwei Ben, Xingye Da, Runyu Ding, Cyrus Hogg, Lina Song, Edy Lim, Eugene Jeong, Tairan He, Haoru Xue, Wenli Xiao, Zi Wang, Simon Yuen, Jan Kautz, Yan Chang, Umar Iqbal, Linxi Jim Fan, and Yuke Zhu.\nSONIC: supersizing motion tracking for natural humanoid whole-body control.\nCoRR\n, abs/2511.07820, 2025.\n10.48550/ARXIV.2511.07820\n.\nTessler et al. [2024]\nChen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik, and Xue Bin Peng.\nMaskedmimic: Unified physics-based character control through masked motion inpainting.\nACM Trans. Graph. (Proc. SIGGRAPH Asia)\n, 43(6), 2024.\n10.1145/3687951\n.\nMittal et al. [2025]\nMayank Mittal, Pascal Roth, James Tigue, Antoine Richard, Octi Zhang, Peter Du, Antonio Serrano-MuÃ±oz, Xinjie Yao, RenÃ© ZurbrÃ¼gg, Nikita Rudin, et al.\nIsaac lab: A GPU-accelerated simulation framework for multi-modal robot learning.\narXiv preprint arXiv:2511.04831\n, 2025.\nMiller et al. [2025b]\nAJ Miller, Fangzhou Yu, Michael Brauckmann, and Farbod Farshidian.\nHigh-performance reinforcement learning on spot: Optimizing simulation parameters with distributional measures.\narXiv preprint arXiv:2504.17857\n, 2025b.\nInc. [2025]\nMovella Inc.\nXsens link motion capture system.\nhttps://www.movella.com/motion-capture/xsens-mvn-link\n, 2025.\nInertial motion capture hardware and software system.\nLtd. [2025]\nVicon Motion Systems Ltd.\nVicon motion capture system.\nhttps://www.vicon.com/\n, 2025.\nOptical motion capture hardware and software system.\nLi et al. [2025]\nZhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely.\nMegasam: Accurate, fast and robust structure and motion from casual dynamic videos.\nIn\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)\n, pages 10486â€“10496, 2025.\nWang et al. [2024]\nYufu Wang, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis.\nTram: Global trajectory and motion of 3d humans from in-the-wild videos.\nIn\nEuropean Conference on Computer Vision\n, pages 467â€“487. Springer, 2024.\nGleicher [1998]\nMichael Gleicher.\nRetargeting motion to new characters.\nIn\nProc. of ACM SIGGRAPH 98\n, Annual Conference Series, pages 33â€“42. ACM SIGGRAPH, jul 1998.\nhttp://dx.doi.org/10.1145/280814.280820\n.\nSchulman et al. [2017]\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\nProximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347\n, 2017.\nPinto et al. [2017]\nLerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel.\nAsymmetric actor critic for image-based robot learning.\narXiv preprint arXiv:1710.06542\n, 2017.\nKarpathy and van de Panne [2012]\nAndrej Karpathy and Michiel van de Panne.\nCurriculum learning for motor skills.\nIn\nAdvances in Artificial Intelligence (Canadian AIÂ 2012)\n, volume 7310 of\nLecture Notes in Computer Science\n, pages 325â€“330. Springer, 2012.\n10.1007/978-3-642-30353-1_31\n.\nYu et al. [2018]\nWenhao Yu, Greg Turk, and C. Karen Liu.\nLearning symmetric and low-energy locomotion.\nACM Trans. Graph.\n, 37(4):144:1â€“144:12, 2018.\n10.1145/3197517.3201397\n.\nHuang et al. [2025]\nTao Huang, Junli Ren, Huayi Wang, Zirui Wang, Qingwei Ben, Muning Wen, Xiao Chen, Jianan Li, and Jiangmiao Pang.\nLearning humanoid standing-up control across diverse postures.\nIn\nRobotics: Science and Systems (RSS)\n, 2025.\nto appear.\nCao et al. [2025]\nZhanxiang Cao, Yang Zhang, Buqing Nie, Huangxuan Lin, Haoyang Li, and Yue Gao.\nLearning motion skills with adaptive assistive curriculum force in humanoid robots.\narXiv preprint arXiv:2506.23125\n, 2025.\nTodorov et al. [2012]\nEmanuel Todorov, Tom Erez, and Yuval Tassa.\nMuJoCo: A physics engine for model-based control.\nIn\n2012 IEEE/RSJ international conference on intelligent robots and systems\n, pages 5026â€“5033. IEEE, 2012.\nSupplementary Material\nSection S1. Nomenclature\nI\nI\nInertial (world) frame\nB\nB\nBase frame\nT\nT\nTorso frame\nj\nj\nJoint index\nk\nâ€‹\nb\nkb\nKeybodies\nğ’¥\n\\mathcal{J}\nSet of joint indices\nn\nj\nn_{j}\nNumber of joints\nn\nk\nâ€‹\nb\nn_{kb}\nNumber of keybodies\nn\na\nn_{a}\nNumber of actions\nğª\n\\mathbf{q}\nJoint positions vector\nğª\nË™\n\\dot{\\mathbf{q}}\nJoint velocities vector\nğª\nÂ¨\n\\ddot{\\mathbf{q}}\nJoint accelerations vector\nğ‰\n\\bm{\\tau}\nJoint torques vector\nğŒ\n\\mathbf{M}\nMass matrix\nğ¡\n\\mathbf{h}\nNonlinear forces vector\nğ‰\n\\mathbf{J}\nJacobian matrix\nğ…\n\\mathbf{F}\nForce vector\nğ–\n\\mathbf{W}\nWrench vector\nğˆ\n\\mathbf{I}\n(Base) inertia matrix\nâ„³\n\\mathcal{M}\nMarkov Decision Process\ns\nt\ns_{t}\nState at time\nt\nt\na\nt\na_{t}\nAction at time\nt\nt\no\nt\no_{t}\nObservation at time\nt\nt\nÏ€\nÎ¸\n\\pi_{\\theta}\nPolicy parameterized by\nÎ¸\n\\theta\nÎ¸\n\\theta\nPolicy parameters\nT\nT\nFinite horizon length\nN\nN\nNumber of reference trajectories\ni\nâˆˆ\n{\n1\n,\nâ€¦\n,\nN\n}\ni\\in\\{1,\\dots,N\\}\nTrajectory index\nD\ni\nD_{i}\nContinuous duration of trajectory\ni\ni\nL\ni\nL_{i}\nDiscrete length of trajectory\ni\ni\nÎ”\n\\Delta\nTemporal bin width\nB\nB\nTotal number of bins\nb\nâˆˆ\n{\n0\n,\nâ€¦\n,\nB\nâˆ’\n1\n}\nb\\in\\{0,\\dots,B{-}1\\}\nBin index\nM\ni\n,\nb\nM_{i,b}\nValidity mask for trajectory\ni\ni\nin bin\nb\nb\nÎ©\n\\Omega\nFeasible set of valid trajectoryâ€“bin pairs\n|\nÎ©\n|\n|\\Omega|\nCardinality of feasible set of bins\nf\ni\n,\nb\nf_{i,b}\nFailure level for trajectory\ni\ni\nin bin\nb\nb\nÎ±\n\\alpha\nEMA smoothing factor\nÏ„\n\\tau\nSampling temperature\nÏ„\nbase\n\\tau_{\\text{base}}\nBase temperature\nÎµ\n\\varepsilon\nUniform floor weight (per-bin lower bound\nâ‰¥\nÎµ\n/\n|\nÎ©\n|\n\\geq\\varepsilon/|\\Omega|\n)\nt\ninit\nt_{\\text{init}}\nInitial time for episode\nÏ•\n\\phi\nNormalized phase\nâˆˆ\n[\n0\n,\n1\n]\n\\in[0,1]\nÏ•\ninit\n\\phi_{\\text{init}}\nInitial phase for episode reset\nâ„“\ni\n,\nb\n\\ell_{i,b}\nLogits for sampling\np\ni\n,\nb\np_{i,b}\nSampling probabilities\nÎ²\ni\n,\nb\n\\beta_{i,b}\nAssistance scale for trajectory\ni\ni\nin bin\nb\nb\nÎ²\nmax\n\\beta_{\\max}\nMaximum assistance scale\nÎ²\ne\n\\beta_{e}\nEnvironment-specific assistance scale\nS\n^\ni\n,\nb\n\\hat{S}_{i,b}\nSmoothed similarity metric\nÎ·\n\\eta\nTarget similarity threshold\nğ…\nb\n\\mathbf{F}_{b}\nAssistive force vector\nğŒ\nb\n\\mathbf{M}_{b}\nAssistive moment vector\nğ°\ne\n\\mathbf{w}_{e}\nApplied assistive wrench\nk\np\nv\n,\nk\nd\nv\nk_{p}^{v},\\,k_{d}^{v}\nVirtual force PD gains\nk\np\nÏ‰\n,\nk\nd\nÏ‰\nk_{p}^{\\omega},\\,k_{d}^{\\omega}\nVirtual torque PD gains\nğ«\nb\n,\ncom\n\\mathbf{r}_{\\mathrm{b,com}}\nCoM position w.r.t. base\ns\nk\ns_{k}\nInstantaneous similarity metric at step\nk\nk\ns\nÂ¯\ne\n\\bar{s}_{e}\nLength-normalized similarity metric for episode\ne\ne\nL\nreal\nL_{\\mathrm{real}}\nRealized episode length\nL\nmax\nL_{\\max}\nMaximum possible episode length\nL\nÂ¯\nepisode\n\\bar{L}_{\\mathrm{episode}}\nEpisode length limit\nr\ntotal\nr_{\\text{total}}\nTotal reward\nr\ntrack\nr_{\\text{track}}\nTracking reward component\nr\nreg\nr_{\\text{reg}}\nRegularization reward component\nr\nsurvival\nr_{\\text{survival}}\nSurvival reward component\nc\nt\ni\nc_{t_{i}}\nTracking reward weight for term\ni\ni\nğ\ni\n\\mathbf{e}_{i}\nError vector for tracking term\ni\ni\nÏƒ\n\\sigma\nStandard deviation for tracking rewards\nÎº\n\\kappa\nStiffness parameter for tracking rewards\nd\nâ€‹\nt\ndt\nControl timestep\nğ©\n,\nğ¯\n\\mathbf{p},\\,\\mathbf{v}\nPosition and linear velocity\nğš½\n,\nğ\n\\bm{\\Phi},\\,\\bm{\\omega}\nOrientation and angular velocity\nğ \n\\mathbf{g}\nGravity vector\nğ«\nX\nâ€‹\nY\n\\mathbf{r}_{XY}\nPosition of frame\nY\nY\nw.r.t. frame\nX\nX\nğ¯\nX\nâ€‹\nY\n\\mathbf{v}_{XY}\nVelocity of frame\nY\nY\nw.r.t. frame\nX\nX\nğ\nX\nâ€‹\nY\n\\bm{\\omega}_{XY}\nAngular velocity of frame\nY\nY\nw.r.t. frame\nX\nX\n(\nâ‹…\n)\nX\n{}_{X}(\\cdot)\nQuantity expressed in frame\nX\nX\n(\nâ‹…\n)\n^\n\\hat{(\\cdot)}\nReference (desired) value\n(\nâ‹…\n)\nâˆ—\n(\\cdot)^{*}\nReference (desired) value (alternative notation)\nğ”¼\nâ€‹\n[\nâ‹…\n]\n\\mathbb{E}[\\cdot]\nExpectation operator\nâˆ¥\nâ‹…\nâˆ¥\n\\|\\cdot\\|\nEuclidean norm\nexp\nâ¡\n(\nâ‹…\n)\n\\exp(\\cdot)\nExponential function\nclip\nâ€‹\n(\nâ‹…\n)\n\\mathrm{clip}(\\cdot)\nClipping function\nğ’°\nâ€‹\n(\na\n,\nb\n)\n\\mathcal{U}(a,b)\nUniform distribution between\na\na\nand\nb\nb\nğ’©\nâ€‹\n(\nÎ¼\n,\nÏƒ\n2\n)\n\\mathcal{N}(\\mu,\\sigma^{2})\nNormal distribution\nğŸ\nâ€‹\n{\nâ‹…\n}\n\\mathbf{1}\\{\\cdot\\}\nIndicator function\nâŒˆ\nâ‹…\nâŒ‰\n\\lceil\\cdot\\rceil\nCeiling function\nâŠŸ\n\\boxminus\nLie-group difference on\nS\nâ€‹\nO\nâ€‹\n(\n3\n)\nSO(3)\nÃ—\n\\times\nCross product operator\nSection S2. Implementation Details: Actuator Modeling\nThe PLA systems in humanoid robots are essential hardware components that help reduce limb inertia by positioning the actuators closer to the body, which enhances dynamic performance. Nonetheless, enforcing their loop-closure constraints leads to large internal forces, making the simulation of dynamics stiff and substantially increasing the computational cost of the iterative Temporal Gauss-Seidel solver used for numerical integration. Since an entirely accurate model is too slow for large-scale RL training, we tackle this trade-off between fidelity and efficiency by developing a series of progressive approximations to model the PLAs.\nThis section details the formulation of our progressive PLA approximation models and provides an experimental validation of their accuracy. It concludes the section by showcasing the importance of considering configuration-dependent torque limits of the PLA actuators.\nProjected Model\nHere, we formulate an analytically exact\nProjected Model\nto circumvent the numerical constraint enforcement used in standard simulators. We begin by deriving the general equation of motion for a class of systems that contain a closed-kinematic chain. An instance of such a mechanism is the four-bar linkage, illustrated in Figure 4, where the primary kinematic chain consists of the floating base, thigh, shank, and their connecting joints. The PLAâ€™s actuation is provided by a motor mounted on the thigh, which drives the knee joint through a four-bar linkage. Our objective is to project the dynamics of the PLAâ€™s support links onto the main kinematic chain. To analyze the dynamics of PLA, we conceptually decompose its closed-loop structure into two overlapping open kinematic chains: a\nmain chain\n,\nâ„³\n\\mathcal{M}\n, and a\nsupport chain\n,\nğ’®\n\\mathcal{S}\n, as illustrated in Figure.\nS1\n.\nâ€¢\nThe\nmain chain\n,\nâ„³\n\\mathcal{M}\n, is comprised of the robotâ€™s primary kinematic tree, the PLAâ€™s parent link, and its unactuated branch.\nâ€¢\nThe\nsupport chain\n,\nğ’®\n\\mathcal{S}\n, consists of the PLAâ€™s support links, which also originate from the same parent link.\nA central assumption of this decomposition is that while the parent link is fully modeled with its physical mass in chain\nâ„³\n\\mathcal{M}\n, it is considered massless in the support chain,\nğ’®\n\\mathcal{S}\n, to avoid double-counting inertial properties. We will further assume that the velocity of the PLA joints in the support kinematic chain, including the actuated joints\nğª\ni\n\\mathbf{q}_{i}\nand dependent joints,\nğª\nd\n\\mathbf{q}_{d}\n, can be calculated based on the velocity of the PLA joints in the main kinematic chain,\nğª\no\n\\mathbf{q}_{o}\n,\n[\nğª\nË™\nd\nğª\nË™\ni\n]\n=\n[\nğšª\nd\nâ€‹\n(\nğª\no\n)\nğšª\ni\nâ€‹\n(\nğª\no\n)\n]\nâ€‹\nğª\nË™\no\n,\n\\begin{bmatrix}\\dot{\\mathbf{q}}_{d}\\\\\n\\dot{\\mathbf{q}}_{i}\\end{bmatrix}=\\begin{bmatrix}\\bm{\\Gamma}_{d}(\\mathbf{q}_{o})\\\\\n\\bm{\\Gamma}_{i}(\\mathbf{q}_{o})\\end{bmatrix}\\dot{\\mathbf{q}}_{o},\n(3)\nwhere\nğšª\nd\n\\bm{\\Gamma}_{d}\nand\nğšª\ni\n\\bm{\\Gamma}_{i}\ndenote the kinematic mappings from the main chain joint velocities to the dependent and actuated joints, respectively.\nTo simplify our derivation, we focus on a scenario where the parent joints are unactuated and the entire system is free from external forces, with actuation occurring only through the PLA. While extending this analysis to a more general case is straightforward, we will not cover it here to keep the discussion concise. The dynamics of the main chain and auxiliary chain can then be expressed as follows:\nğŒ\nâ„³\n\\displaystyle\\mathbf{M}_{\\mathcal{M}}\n[\nğª\nÂ¨\np\nğª\nÂ¨\no\n]\n+\nğ¡\nâ„³\n=\nğ‰\nâ„³\nâŠ¤\nâ€‹\nğ…\n\\displaystyle\\begin{bmatrix}\\ddot{\\mathbf{q}}_{p}\\\\\n\\ddot{\\mathbf{q}}_{o}\\end{bmatrix}+\\mathbf{h}_{\\mathcal{M}}=\\mathbf{J}_{\\mathcal{M}}^{\\top}\\mathbf{F}\n(4)\nğŒ\nğ’®\n\\displaystyle\\mathbf{M}_{\\mathcal{S}}\n[\nğª\nÂ¨\np\nğª\nÂ¨\nd\nğª\nÂ¨\ni\n]\n+\nğ¡\nğ’®\n=\n[\n0\n0\nğ‰\ni\n]\nâˆ’\nğ‰\nğ’®\nâŠ¤\nâ€‹\nğ…\n,\n\\displaystyle\\begin{bmatrix}\\ddot{\\mathbf{q}}_{p}\\\\\n\\ddot{\\mathbf{q}}_{d}\\\\\n\\ddot{\\mathbf{q}}_{i}\\end{bmatrix}+\\mathbf{h}_{\\mathcal{S}}=\\begin{bmatrix}0\\\\\n0\\\\\n\\bm{\\tau}_{i}\\end{bmatrix}-\\mathbf{J}_{\\mathcal{S}}^{\\top}\\mathbf{F},\n(5)\nwhere\nğŒ\nâ„³\n\\mathbf{M}_{\\mathcal{M}}\nand\nğ¡\nâ„³\n\\mathbf{h}_{\\mathcal{M}}\nare the mass matrix and the nonlinear forces of the main chain, and\nğŒ\nğ’®\n\\mathbf{M}_{\\mathcal{S}}\n,\nğ¡\nğ’®\n\\mathbf{h}_{\\mathcal{S}}\ndenote those of the support chain. The variable\nğª\np\n\\mathbf{q}_{p}\ncorresponds to the parent joints, and\nğ‰\ni\n\\bm{\\tau}_{i}\nrepresents the torque applied at the PLAâ€™s actuated joint.\nğ…\n\\mathbf{F}\nis the generalized interaction force exchanged between the main and support chains at their connection point, and\nğ‰\nâ„³\n\\mathbf{J}_{\\mathcal{M}}\nand\nğ‰\nğ’®\n\\mathbf{J}_{\\mathcal{S}}\nare the corresponding Jacobians at that point. Given that the velocity of the point should be equal, we have\nğ‰\nğ’®\nâ€‹\n[\nğª\nË™\np\nğª\nË™\nd\nğª\nË™\ni\n]\n=\nğ‰\nâ„³\nâ€‹\n[\nğª\nË™\np\nğª\nË™\no\n]\n.\n\\mathbf{J}_{\\mathcal{S}}\\begin{bmatrix}\\dot{\\mathbf{q}}_{p}\\\\\n\\dot{\\mathbf{q}}_{d}\\\\\n\\dot{\\mathbf{q}}_{i}\\end{bmatrix}=\\mathbf{J}_{\\mathcal{M}}\\begin{bmatrix}\\dot{\\mathbf{q}}_{p}\\\\\n\\dot{\\mathbf{q}}_{o}\\end{bmatrix}.\nApplying the mapping in\n3\nyields the corresponding relationship between the Jacobians\nğ‰\nâ„³\n=\nğ‰\nğ’®\nâ€‹\n[\nğˆ\n0\n0\nğšª\nd\nâ€‹\n(\nğª\no\n)\n0\nğšª\ni\nâ€‹\n(\nğª\no\n)\n]\nâŸ\nğ†\nâ€‹\n(\nğª\no\n)\n.\n\\mathbf{J}_{\\mathcal{M}}=\\mathbf{J}_{\\mathcal{S}}\\underbrace{\\begin{bmatrix}\\mathbf{I}&0\\\\\n0&\\bm{\\Gamma}_{d}(\\mathbf{q}_{o})\\\\\n0&\\bm{\\Gamma}_{i}(\\mathbf{q}_{o})\\end{bmatrix}}_{\\mathbf{G}(\\mathbf{q}_{o})}.\n(6)\nBased on\n5\n,\n4\n, and\n6\n, the dynamics of the\nProjected\nModel can be derived as\n(\nğŒ\nâ„³\n+\nğ†\nâŠ¤\nâ€‹\nğŒ\nğ’®\nâ€‹\nğ†\n)\nâ€‹\n[\nğª\nÂ¨\np\nğª\nÂ¨\no\n]\n+\nğ¡\nâ„³\n+\nğ†\nâŠ¤\nâ€‹\nğ¡\nğ’®\n+\nğ†\nâŠ¤\nâ€‹\nğŒ\nğ’®\nâ€‹\nğ†\nË™\nâ€‹\n[\n0\nğª\nË™\no\n]\n=\n[\n0\nğ‰\no\n]\n,\n\\big(\\mathbf{M}_{\\mathcal{M}}+\\mathbf{G}^{\\top}\\mathbf{M}_{\\mathcal{S}}\\mathbf{G}\\big)\\begin{bmatrix}\\ddot{\\mathbf{q}}_{p}\\\\\n\\ddot{\\mathbf{q}}_{o}\\end{bmatrix}+\\mathbf{h}_{\\mathcal{M}}+\\mathbf{G}^{\\top}\\mathbf{h}_{\\mathcal{S}}+\\mathbf{G}^{\\top}\\mathbf{M}_{\\mathcal{S}}\\dot{\\mathbf{G}}\\begin{bmatrix}0\\\\\n\\dot{\\mathbf{q}}_{o}\\end{bmatrix}=\\begin{bmatrix}0\\\\\n\\bm{\\tau}_{o}\\end{bmatrix},\n(7)\nwhere\nğ‰\no\n\\bm{\\tau}_{o}\ndenotes the output torque of PLA. The transmission Jacobian\nğšª\ni\n\\bm{\\Gamma}_{i}\ndefines the relationship between the input torque and the output torque of the PLA, expressed as:\nğ‰\no\n=\nğšª\ni\nâŠ¤\nâ€‹\n(\nğª\no\n)\nâ€‹\nğ‰\ni\n.\n\\bm{\\tau}_{o}=\\bm{\\Gamma}_{i}^{\\top}(\\mathbf{q}_{o})\\,\\bm{\\tau}_{i}.\n(8)\nAlthough this model is precise, its governing equations do not adhere to the standard rigid body structure required by off-the-shelf physics engines. This structural incompatibility prevents its direct application, leading us to create subsequent approximate models that are tailored to be compatible with simulators.\nLocally Projected Model: Massless-Links Approximation\nOur first approximation is motivated by the lightweight design of the support links in the parallel kinematic chain of PLAs. We assume that these support links are massless while still accounting for the full inertia of the motor armatures and the main kinematic chain of PLA. By treating the support links as massless and considering the Coriolis forces,\nğ¡\nğ’®\n\\mathbf{h}_{\\mathcal{S}}\n, to be negligible, we can derive:\n(\nğŒ\nâ„³\n+\n[\n0\n0\n0\nğŒ\no\n]\n)\nâ€‹\n[\nğª\nÂ¨\np\nğª\nÂ¨\no\n]\n+\nğ¡\nâ„³\n+\n[\n0\nğ¡\n0\n]\n=\n[\n0\nğ‰\no\n]\n,\n\\Big(\\mathbf{M}_{\\mathcal{M}}+\\begin{bmatrix}0&0\\\\\n0&\\mathbf{M}_{o}\\end{bmatrix}\\Big)\\begin{bmatrix}\\ddot{\\mathbf{q}}_{p}\\\\\n\\ddot{\\mathbf{q}}_{o}\\end{bmatrix}+\\mathbf{h}_{\\mathcal{M}}+\\begin{bmatrix}0\\\\\n\\mathbf{h}_{0}\\end{bmatrix}=\\begin{bmatrix}0\\\\\n\\bm{\\tau}_{o}\\end{bmatrix},\n(9)\nwhere\nğŒ\no\nâ€‹\n(\nğª\no\n)\n=\nğšª\ni\nâŠ¤\nâ€‹\nğˆ\ni\nâ€‹\nğšª\ni\n\\mathbf{M}_{o}(\\mathbf{q}_{o})=\\bm{\\Gamma}_{i}^{\\top}\\mathbf{I}_{i}\\bm{\\Gamma}_{i}\nis the locally projected mass matrix, under the assumption that\nğŒ\nğ’®\n=\nDiag\nâ€‹\n(\n0\n,\nğˆ\ni\n)\n\\mathbf{M}_{\\mathcal{S}}=\\text{Diag}(0,\\mathbf{I}_{i})\nis diagonal and only encodes the PLA actuator armature values, with\nğˆ\ni\n\\mathbf{I}_{i}\nrepresenting the motor armatures of the PLA. The term\nğ¡\n0\nâ€‹\n(\nğª\no\n,\nğª\nË™\no\n)\n=\nğšª\ni\nâŠ¤\nâ€‹\nğˆ\ni\nâ€‹\nğšª\nË™\ni\nâ€‹\nğª\nË™\no\n\\mathbf{h}_{0}(\\mathbf{q}_{o},\\dot{\\mathbf{q}}_{o})=\\bm{\\Gamma}_{i}^{\\top}\\mathbf{I}_{i}\\dot{\\bm{\\Gamma}}_{i}\\dot{\\mathbf{q}}_{o}\nrepresents the vector of nonlinear forces arising from the PLA dynamics, which acts locally on the PLAâ€™s output joints.\nThis model results in a valid rigid body model that is compatible with standard simulators. In this model, the armature values (inertia terms added to the diagonal of the mass matrix) depend on the configuration. In a simulator like Isaac Lab, this can be implemented by updating the armature values at each simulation step using a lookup table.\nDynamic Armature Model: Diagonal Approximation\nAlthough the\nLocally Projected Model\nis local, meaning that the dynamics of the PLA actuators only affect themselves and are solely a function of the PLAâ€™s output joint, it should be noted that\nğŒ\no\n\\mathbf{M}_{o}\nis not necessarily diagonal even though\nğˆ\nğ’¾\n\\mathbf{I}_{\\mathcal{i}}\nis diagonal. Since standard simulators cannot accommodate off-diagonal terms, this model faces limitations when applied to coupled PLAs, such as those found in the ankles of Atlas and G1, where two actuators simultaneously drive the ankle pitch and roll joints. To resolve this, we propose our second approximation, which we refer to as the\nDynamic Armature Model\n. To derive this model, we decompose\nğŒ\no\n\\mathbf{M}_{o}\ninto two components:\nD\no\nD_{o}\n, which is a diagonal matrix, and\nO\no\nO_{o}\n, which represents the off-diagonal elements. We then define the model as follows:\n(\nğŒ\nâ„³\n+\n[\n0\n0\n0\nD\no\n]\n)\nâ€‹\n[\nğª\nÂ¨\np\nğª\nÂ¨\no\n]\n+\nğ¡\nâ„³\n+\n[\n0\nğ¡\n0\n+\nO\no\nâ€‹\nğª\nÂ¨\no\n]\n=\n[\n0\nğ‰\no\n]\n.\n\\Big(\\mathbf{M}_{\\mathcal{M}}+\\begin{bmatrix}0&0\\\\\n0&D_{o}\\end{bmatrix}\\Big)\\begin{bmatrix}\\ddot{\\mathbf{q}}_{p}\\\\\n\\ddot{\\mathbf{q}}_{o}\\end{bmatrix}+\\mathbf{h}_{\\mathcal{M}}+\\begin{bmatrix}0\\\\\n\\mathbf{h}_{0}+O_{o}\\ddot{\\mathbf{q}}_{o}\\end{bmatrix}=\\begin{bmatrix}0\\\\\n\\bm{\\tau}_{o}\\end{bmatrix}.\n(10)\nIn the next step, instead of using\nğª\nÂ¨\no\n\\ddot{\\mathbf{q}}_{o}\n, we will use the previous joint acceleration of the PLA, denoted as\nğª\nÂ¨\no\nâ€²\n\\ddot{\\mathbf{q}}_{o}^{{}^{\\prime}}\n.\n(\nğŒ\nâ„³\n+\n[\n0\n0\n0\nD\no\n]\n)\nâ€‹\n[\nğª\nÂ¨\np\nğª\nÂ¨\no\n]\n+\nğ¡\nâ„³\n+\n[\n0\nğ¡\n0\n+\nO\no\nâ€‹\nğª\nÂ¨\no\nâ€²\n]\n=\n[\n0\nğ‰\no\n]\n.\n\\Big(\\mathbf{M}_{\\mathcal{M}}+\\begin{bmatrix}0&0\\\\\n0&D_{o}\\end{bmatrix}\\Big)\\begin{bmatrix}\\ddot{\\mathbf{q}}_{p}\\\\\n\\ddot{\\mathbf{q}}_{o}\\end{bmatrix}+\\mathbf{h}_{\\mathcal{M}}+\\begin{bmatrix}0\\\\\n\\mathbf{h}_{0}+O_{o}\\ddot{\\mathbf{q}}_{o}^{{}^{\\prime}}\\end{bmatrix}=\\begin{bmatrix}0\\\\\n\\bm{\\tau}_{o}\\end{bmatrix}.\n(11)\nThe\nDynamic Armature Model\nprovides a Jacobi approximation of the\nLocally Projected Model\n, representing the PLA dynamics as two components: a configuration-dependent diagonal armature and a fictitious torque. This fictitious torque, which accounts for the off-diagonal dynamic effects, is calculated using the PLA joint acceleration from the previous timestep. This approximation introduces only a transient error, provided the systemâ€™s mass matrix is diagonally dominant. We have empirically verified that this condition holds for the Atlas ankle by sweeping its whole workspace.\nNominal Armature Model: Fixed-Configuration Approximation\nThus far, our approximations have produced a compatible model with minimal error. However, the suggested configuration-dependent armature matrix creates a practical bottleneck. Updating armature values at each simulation step incurs a significant computational overhead (approximately 20% in Isaac Lab). To address this issue, we introduce our final and strongest approximation, the\nNominal Armature Model\n. This model computes the joint armatures once at a single, well-chosen nominal configuration and then fixes these values throughout training, denoted by\nD\nÂ¯\no\n\\bar{D}_{o}\nand\nO\nÂ¯\no\n\\bar{O}_{o}\n. The dynamics of the\nNominal Armature Model\nare as follows:\n(\nğŒ\nâ„³\n+\n[\n0\n0\n0\nD\nÂ¯\no\n]\n)\nâ€‹\n[\nğª\nÂ¨\np\nğª\nÂ¨\no\n]\n+\nğ¡\nâ„³\n+\n[\n0\nğ¡\n0\n+\nO\nÂ¯\no\nâ€‹\nğª\nÂ¨\no\nâ€²\n]\n=\n[\n0\nğ‰\no\n]\n,\n\\Big(\\mathbf{M}_{\\mathcal{M}}+\\begin{bmatrix}0&0\\\\\n0&\\bar{D}_{o}\\end{bmatrix}\\Big)\\begin{bmatrix}\\ddot{\\mathbf{q}}_{p}\\\\\n\\ddot{\\mathbf{q}}_{o}\\end{bmatrix}+\\mathbf{h}_{\\mathcal{M}}+\\begin{bmatrix}0\\\\\n\\mathbf{h}_{0}+\\bar{O}_{o}\\ddot{\\mathbf{q}}_{o}^{{}^{\\prime}}\\end{bmatrix}=\\begin{bmatrix}0\\\\\n\\bm{\\tau}_{o}\\end{bmatrix},\n(12)\nThis model enables adding the projected armature (of the motors on the closed kinematic chain) to the main kinematic tree when training is initialized. Crucially, this single set of nominal armature values serves a dual purpose: it simplifies the simulation dynamics and it provides a principled basis for designing the fixed PD gains of our low-level controller.\nPLA Models Evaluation\nTo quantify the accuracy of our approximations, we compare them to the actual model. In addition to the models mentioned above, we also evaluate a baseline known as the\nSimplest Model\n, which excludes all fictitious torque from the\nNominal Armature\nmodel.\nThe tests are conducted using the Atlas ankle, which is commanded to follow a 5 Hz sinusoidal trajectory in both pitch and roll, with an amplitude of half the joint position limit. To measure performance, we calculate the joint acceleration of each approximate model and determine the error relative to the actual model. This error is normalized at each timestep, and we report the final Mean Squared Error (MSE) for each model in Table\nS2\n.\nThe\nLocally Projected\n,\nDynamic Armature\nand\nNominal Armature\nmodels all demonstrate behaviors that closely match those of the exact model, indicating that the assumption of massless support links and the Jacobi approximation are valid. Although the\nSimplest Model\nshows relatively larger errors, these errors primarily manifest as a shift in the motion, and the resulting motion is quite similar to that of the actual model. Given that the motion of the\nSimplest Model\nis qualitatively similar, its simplicity makes it an attractive option, especially since the RL policy could learn to compensate for these model discrepancies. As a result, in some of the training runs, we opted to use this model.\nImpact of PLA Modeling on Sim-To-Real Transfer\nTo better isolate the effect of PLA modeling on sim-to-real transfer, we performed a controlled comparison that holds the remainder of the pipeline fixed while varying only the armature model, which is also used to compute the joint-level PD gains. Specifically, we compare our\nnominal armature model\nfor the Unitree G1 robot, which accounts for the parallel linkages at the waist and ankle, against an\nunprojected\nvariant that ignores these linkages. For both models, we use the same PD gain design recipe and hyperparameters (i.e., the same natural frequency and damping ratio), and we keep all other training components identical. In simulation, the two setups exhibit similar training curves and comparable success during rollout evaluation. On hardware, however, the difference becomes clear: across three representative behaviorsâ€”namely the cartwheel motion, the ping-pong sequence, and the box climb downâ€”the policy using the nominal armature model transfers reliably, whereas the policy trained with unprojected armatures consistently fails during execution. Qualitative comparisons are provided in Movie S8.\nPLA Torque Limits\nThe equation (\n8\n) defines the relationship between the input torque and the output torque of the PLA. This mapping is dependent on the configuration, meaning that the output torque limits are not simply box limits, even though the input space does have box limits. An approximation approach is to use box limits. However, this can lead to either overly conservative or overly optimistic results. Figure\nS2\nillustrates the configuration-dependent torque limits on the Atlas ankle at specific joint roll and pitch angles, compared to the fixed boundary approximation. We sweep the ankle pitchâ€“roll joint space over its position limits. At each pitchâ€“roll joint pair, we calculate the transmission Jacobian\nğšª\ni\n\\bm{\\Gamma}_{i}\n(eq.(\n3\n)) and apply this linear transformation to the box limit in the motor space, resulting in a sequence of parallelograms. As shown, the fixed limits significantly underestimate the possible torque output.\nSection S3. Implementation Details: Spot Modeling\nTo bridge the sim-to-real gap for the more challenging continuous backflip behavior, we incorporated a more accurate model of Spotâ€™s power system and actuators in simulation.\nFirst, we modeled Spotâ€™s power system. Spot employs a power-limiting algorithm that sums the requested mechanical power and resistive losses across all actuators and selectively saturates the commanded torque to maintain the total system power within defined limits. We replicated this algorithm in simulation so that policies would experience the same constraints during training\n2\n2\n2\nFurther details of this algorithm cannot be provided, as they involve proprietary industrial information.\n. Second, we modeled the motor and transmission efficiencies. At high currents, the motor constant of an electric motor decreases. We modeled this magnet saturation effect using the following equation:\nÏ„\no\nâ€‹\nu\nâ€‹\nt\n=\nÏ„\ni\nâ€‹\nn\n1\n+\nk\nâ€‹\n|\nÏ„\ni\nâ€‹\nn\n|\n,\n\\tau_{out}=\\frac{\\tau_{in}}{1+k|\\tau_{in}|},\nwith\nÏ„\no\nâ€‹\nu\nâ€‹\nt\n\\tau_{out}\ndenoting output torque,\nÏ„\ni\nâ€‹\nn\n\\tau_{in}\ninput torque, and\nk\nk\na derating constant. Torque losses from transmission inefficiencies and friction were modeled using the following equation:\nÏ„\no\nâ€‹\nu\nâ€‹\nt\n=\nÎ·\nâ€‹\n(\nÏ„\ni\nâ€‹\nn\nâˆ’\nI\nâ€‹\nÎ±\n)\nâˆ’\nK\nc\nâ€‹\ntanh\nâ¡\n(\ns\nâ€‹\nÏ‰\n)\nâˆ’\nK\nv\nâ€‹\nÏ‰\n,\n\\tau_{out}=\\eta(\\tau_{in}-I\\alpha)-K_{c}\\tanh{(s\\omega)}-K_{v}\\omega,\nwhere\nI\nI\nis the rotor inertia,\nÎ±\n\\alpha\nthe joint acceleration,\nK\nc\nK_{c}\nthe Coulomb friction constant,\ns\ns\na smoothing factor,\nÏ‰\n\\omega\nthe joint velocity,\nK\nv\nK_{v}\nthe viscous friction constant, and\nÎ·\n\\eta\nthe transmission efficiency. Two values of\nÎ·\n\\eta\nare used, depending on whether the motor performs positive or negative work. The complete actuator model is thus expressed as:\nÏ„\no\nâ€‹\nu\nâ€‹\nt\n=\nÎ·\nâ€‹\n(\nÏ„\ni\nâ€‹\nn\n1\n+\nk\nâ€‹\n|\nÏ„\ni\nâ€‹\nn\n|\nâˆ’\nI\nâ€‹\nÎ±\n)\nâˆ’\nK\nc\nâ€‹\ntanh\nâ¡\n(\ns\nâ€‹\nÏ‰\n)\nâˆ’\nK\nv\nâ€‹\nÏ‰\n.\n\\tau_{out}=\\eta\\Big(\\frac{\\tau_{in}}{1+k|\\tau_{in}|}-I\\alpha\\Big)-K_{c}\\tanh{(s\\omega)}-K_{v}\\omega.\nThe actuator model output was low-pass filtered for simulation stability. Parameters\nk\nk\n,\nI\nI\n,\nK\nc\nK_{c}\n,\ns\ns\n, and\nK\nv\nK_{v}\nwere sourced directly from the manufacturerâ€™s specifications. To define the randomization range for\nÎ·\n\\eta\n, we recorded joint torques during dynamic maneuvers (such as a continuous backflip) and estimated the parameters through optimization. The minimum and maximum values set the domain randomization ranges for each actuator type.\nSection S4. Implementation Details: Motion Dataset\nIn this section, we briefly present the three data sources used in this project.\nMoCap Data\nMoCap was used to gather high-quality motion references of a human using a combination of Xsens and Vicon systems. Human skeletons from different data sources can vary in animation bone layout, naming, and proportions due to differences in software conventions and variations in morphology among actors. Retargeting is performed in two stages to convert these into robot trajectories.\nIn the first stage, animation of skeletons is exported to\n.bvh\nformat and retargeted to a target skeleton with human-like kinematics and spherical joints, but proportions similar to the target robot. Alignment frames are placed on the source skeleton bones corresponding to the head, torso, upper arms, lower arms, hands, pelvis, knees, and feet. Optimization is used to minimize spacetime constraint violations between those frames and their counterparts on the target skeleton by solving for the target skeletonâ€™s joint poses at all timesteps. We also jointly optimize a uniform scaling of the source data, resampling time inversely with the scale to preserve a gravitational constant of\n9.81\nâ€‹\nm/s\n2\n9.81\\,\\text{m/s}^{2}\nin ballistic motion. The second stage applies kinematic retargeting, mapping the resulting motions to the target robot.\nViCap Data\nTo leverage the vast amount of motion data available in online videos, we use a custom 3D motion extraction pipeline to reconstruct global human motion, including full-body poses and joint angles, from monocular video recordings (e.g., from a cellphone). While this process yields detailed, high-fidelity motion, the resulting data is prone to more artifacts than MoCap. The process consists of two main stages. In the first stage, we utilize\nMegaSaM\nto recover the camera trajectory in the world frame, as well as the underlying scene structure and the global metric scale. In the second stage, we estimate human motion as an\nSMPL\nmodel in the camera frame using\nTRAM\n, and then map these poses to world coordinates using the transformation derived from\nMegaSaM\n. While the original\nTRAM\nimplementation integrates these two steps, we found that substituting the first stage with\nMegaSaM\nsignificantly reduces artifacts such as character misorientation, sliding, and floating. Finally, the extracted\nSMPL\nmotion is retargeted to either the Atlas or G1 through kinematic retargeting. Before this process, the\nSMPL\nskeleton is uniformly scaled to match the target robot, with the scaling factor chosen by matching the\nSMPL\nthigh bone length to that of the robot.\nKeyframe Animation\nWe utilize keyframe animation for two primary purposes: first, to create movements that are not achievable by humans, such as utilizing the continuous rotation joint of the robot; and second, to generate motions for non-humanoid morphologies, like the Spot quadruped, where collecting real-world animal data is challenging. As these motions are authored with the target robotâ€™s kinematics in mind, they typically do not require any retargeting.\nSection S5. Implementation Details: Adaptive RSI Sampler\nGiven a library containing\nN\nN\nreference trajectories indexed by\ni\nâˆˆ\n{\n1\n,\nâ€¦\n,\nN\n}\ni\\!\\in\\!\\{1,\\dots,N\\}\nwith continuous durations\nD\ni\nD_{i}\nand discrete lengths\nL\ni\nL_{i}\n. We fix a temporal bin width\nÎ”\n>\n0\n\\Delta{>}0\nand partition time into bins\nb\nâˆˆ\n{\n0\n,\nâ€¦\n,\nB\nâˆ’\n1\n}\nb\\!\\in\\!\\{0,\\dots,B{-}1\\}\nwhere\nB\n=\nâŒˆ\nmax\ni\nâ¡\nD\ni\nÎ”\nâŒ‰\n.\nB\\;=\\;\\left\\lceil\\max_{i}\\frac{D_{i}}{\\Delta}\\right\\rceil.\nA validity mask\nM\ni\n,\nb\n=\n1\nâ€‹\n{\nb\nâ‹…\nÎ”\n<\nD\ni\n}\nM_{i,b}\\;=\\;\\mathbf{1}\\!\\left\\{\\,b\\cdot\\Delta<D_{i}\\,\\right\\}\nmarks bins that intersect the support of trajectory\ni\ni\n. We define the feasible set\nÎ©\n=\n{\n(\ni\n,\nb\n)\n:\nM\ni\n,\nb\n=\n1\n}\n{\\Omega\\!=\\!\\{(i,b)\\,:\\,M_{i,b}\\!=\\!1\\}}\nand maintain a per-bin\nfailure level\nmatrix\nf\nâˆˆ\nâ„\nN\nÃ—\nB\nf\\in\\mathbb{R}^{N\\times B}\non\nÎ©\n\\Omega\n. For numerical convenience, we set\nf\ni\n,\nb\n=\nâˆ’\nâˆ\nf_{i,b}\\!=\\!{-}\\infty\nfor\n(\ni\n,\nb\n)\nâˆ‰\nÎ©\n(i,b)\\!\\notin\\!\\Omega\nso that invalid bins receive zero probability under a softmax. For each rollout\ne\ne\nthat\nstarts\nin bin\n(\ni\n,\nb\n)\n(i,b)\n, we compute a length-normalized similarity metric that captures overall tracking performance\ns\nÂ¯\ne\n=\n1\nL\nmax\nâ€‹\nâˆ‘\nk\n=\n1\nL\nreal\ns\nk\n,\ns\nk\nâˆˆ\n[\n0\n,\n1\n]\n,\n\\bar{s}_{e}\\;=\\;\\frac{1}{L_{\\max}}\\,\\sum_{k=1}^{L_{\\mathrm{real}}}s_{k},\\qquad s_{k}\\in[0,1],\nwhere\ns\nk\ns_{k}\nis given by the joint tracking reward.\nL\nreal\nL_{\\mathrm{real}}\nis the realized episode length and\nL\nmax\n=\nmin\nâ¡\n(\nL\nÂ¯\nepisode\n,\nL\ni\n)\nL_{\\max}=\\min(\\bar{L}_{\\text{episode}},L_{i})\nis the total number of steps that could have been realized when early terminations are not triggered (so early terminations are automatically penalized by missing terms). We then update the failure level with an exponential moving average (EMA):\nf\ni\n,\nb\nâ†\n(\n1\nâˆ’\nÎ±\n)\nâ€‹\nf\ni\n,\nb\n+\nÎ±\nâ€‹\n(\n1\nâˆ’\ns\nÂ¯\ne\n)\n,\nÎ±\nâˆˆ\n(\n0\n,\n1\n)\n.\nf_{i,b}\\;\\leftarrow\\;(1-\\alpha)\\,f_{i,b}\\;+\\;\\alpha\\,\\bigl(1-\\bar{s}_{e}\\bigr),\\qquad\\alpha\\in(0,1).\nTo sample the next initialization bin, we form logits on valid bins,\nâ„“\ni\n,\nb\n=\nf\ni\n,\nb\nÏ„\n,\nÏ„\n>\n0\n,\n\\ell_{i,b}\\;=\\;\\frac{f_{i,b}}{\\tau},\\qquad\\tau>0,\nand draw from the floor-smoothed categorical distribution\np\ni\n,\nb\n=\n(\n1\nâˆ’\nÎµ\n)\nâ€‹\ne\nâ„“\ni\n,\nb\nâˆ‘\n(\nu\n,\nv\n)\nâˆˆ\nÎ©\ne\nâ„“\nu\n,\nv\n+\nÎµ\nâ€‹\n1\n|\nÎ©\n|\n,\nÎµ\nâˆˆ\n(\n0\n,\n1\n)\n,\np_{i,b}\\;=\\;(1-\\varepsilon)\\,\\frac{e^{\\ell_{i,b}}}{\\sum_{(u,v)\\in\\Omega}e^{\\ell_{u,v}}}\\;+\\;\\varepsilon\\,\\frac{1}{|\\Omega|},\\qquad\\varepsilon\\in(0,1),\nwith\np\ni\n,\nb\n=\n0\np_{i,b}\\!=\\!0\nfor\n(\ni\n,\nb\n)\nâˆ‰\nÎ©\n(i,b)\\!\\notin\\!\\Omega\n. The temperature\nÏ„\n\\tau\ncontrols concentration around hard bins; the uniform floor\nÎµ\n\\varepsilon\npreserves global coverage and mitigates forgetting. Finally, given\n(\ni\n,\nb\n)\nâˆ¼\np\n(i,b)\\!\\sim\\!p\n, we sample a start time for trajectory\ni\ni\nt\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\nâˆ¼\nğ’°\nâ€‹\n(\nb\nâ‹…\nÎ”\n,\nmin\nâ¡\n{\n(\nb\n+\n1\n)\nâ€‹\nÎ”\n,\nD\ni\n}\n)\n,\nt_{init}\\;\\sim\\;\\mathcal{U}\\!\\bigl(b\\cdot\\Delta,\\;\\min\\{(b{+}1)\\Delta,\\,D_{i}\\}\\bigr),\nand convert to a normalized phase\nÏ•\n=\nt\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\n/\nD\ni\nâˆˆ\n[\n0\n,\n1\n]\n\\phi\\!=\\!t_{init}/D_{i}\\in[0,1]\n.\nSection S6. Implementation Details: Assistive-Wrench Curriculum\nWe convert per-bin failure levels\nf\ni\n,\nb\nf_{i,b}\nback to a smoothed similarity metric\nS\n^\ni\n,\nb\n=\n1\nâˆ’\nf\ni\n,\nb\n\\hat{S}_{i,b}\\!=\\!1-f_{i,b}\non\nÎ©\n\\Omega\nand map them to a non-negative assistance scale via a monotone schedule that vanishes at a target similarity\nÎ·\n\\eta\n:\nÎ²\ni\n,\nb\n=\nclip\nâ€‹\n(\n1\nâˆ’\nS\n^\ni\n,\nb\nÎ·\n,\n0\n,\nÎ²\nmax\n)\n,\nÎ·\n>\n0\n,\nÎ²\nmax\nâˆˆ\n[\n0\n,\n1\n]\n\\beta_{i,b}\\;=\\;\\mathrm{clip}\\!\\Bigl(1-\\tfrac{\\hat{S}_{i,b}}{\\eta},\\;0,\\;\\beta_{\\max}\\Bigr),\\qquad\\eta>0,\\;\\beta_{\\max}\\!\\in[0,1]\\!\nAt initialization, we set the environmentâ€™s assistive gain based on its sampled trajectory-bin pair\n(\ni\n,\nb\n)\n(i,b)\n(i.e.,\nÎ²\ne\n=\nÎ²\ni\n,\nb\n\\beta_{e}=\\beta_{i,b}\n) and we hold\nÎ²\ne\n\\beta_{e}\nconstant during that episode.\nVirtual wrench computation.\nLet\n(\nğ©\n,\nğ¯\n,\nğš½\n,\nğ\n)\n(\\mathbf{p},\\mathbf{v},\\bm{\\Phi},\\bm{\\omega})\ndenote the base position, linear velocity, orientation, and angular velocity; a hat on top of each variable denotes its corresponding reference. We compute a nominal spatial wrench acting on the robotâ€™s torso as follows:\nğ…\nb\n\\displaystyle\\mathbf{F}_{b}\\;\n=\nM\nâ€‹\n(\nğ¯\nË™\n^\n+\nk\np\nv\nâ€‹\n(\nğ©\n^\nâˆ’\nğ©\n)\n+\nk\nd\nv\nâ€‹\n(\nğ¯\n^\nâˆ’\nğ¯\n)\nâˆ’\nğ \n)\n,\n\\displaystyle=\\;M\\!\\left(\\hat{\\dot{\\mathbf{v}}}\\;+\\;k_{p}^{v}\\,(\\hat{\\mathbf{p}}-\\mathbf{p})\\;+\\;k_{d}^{v}\\,(\\hat{\\mathbf{v}}-\\mathbf{v})\\;-\\;\\mathbf{g}\\right),\n(13a)\nğŒ\nb\n\\displaystyle\\mathbf{M}_{b}\\;\n=\nğˆ\nâ€‹\nğ\nË™\n^\n+\nk\np\nÏ‰\nâ€‹\nğˆ\nâ€‹\n(\nğš½\n^\nâŠŸ\nğš½\n)\n+\nk\nd\nÏ‰\nâ€‹\nğˆ\nâ€‹\n(\nğ\n^\nâˆ’\nğ\n)\n+\nğ\nÃ—\n(\nğˆ\nâ€‹\nğ\n)\nâˆ’\nğ«\nb\n,\ncom\nÃ—\nM\nâ€‹\nğ \n,\n\\displaystyle=\\;\\mathbf{I}\\,\\hat{\\dot{\\bm{\\omega}}}\\;+\\;k_{p}^{\\omega}\\,\\mathbf{I}\\,\\bigl(\\hat{\\bm{\\Phi}}\\boxminus\\bm{\\Phi}\\bigr)\\;+\\;k_{d}^{\\omega}\\,\\mathbf{I}\\,(\\hat{\\bm{\\omega}}-\\bm{\\omega})\\;+\\;\\bm{\\omega}\\times(\\mathbf{I}\\bm{\\omega})\\;-\\;\\mathbf{r}_{\\mathrm{b,com}}\\times M\\mathbf{g},\n(13b)\nwhere\nM\nM\nand\nğˆ\n\\mathbf{I}\nare the whole-body mass and nominal base inertia at a default configuration,\nğ \n\\mathbf{g}\nis gravity,\nğ«\nb\n,\ncom\n\\mathbf{r}_{\\mathrm{b,com}}\nis the position of the whole-body CoM with respect to the base, and\nâŠŸ\n\\boxminus\ndenotes the Lie-group difference on\nS\nâ€‹\nO\nâ€‹\n(\n3\n)\nSO(3)\n(implemented via the rotation-log map for quaternions). The applied assistive wrench is\nğ°\ne\n=\nÎ²\ne\nâ€‹\n[\nğ…\nb\nğŒ\nb\n]\n,\n\\mathbf{w}_{e}\\;=\\;\\beta_{e}\\begin{bmatrix}\\mathbf{F}_{b}\\\\\n\\mathbf{M}_{b}\\end{bmatrix},\nexpressed in the world frame and applied at the torso link. The cap\nÎ²\nmax\n<\n1\n\\beta_{\\max}{<}1\nkeeps assistance partial to encourage meaningful exploration and avoid overfitting to the modified physics.\nCoupling to the adaptive RSI sampler.\nBecause\nw\ni\n,\nb\nw_{i,b}\nis a deterministic function of\nS\n^\ni\n,\nb\n\\hat{S}_{i,b}\n, bins with persistent errors (large\nf\ni\n,\nb\nf_{i,b}\n, small\nS\n^\ni\n,\nb\n\\hat{S}_{i,b}\n) receive stronger assistance initially. As tracking improves, the EMA drives\nf\ni\n,\nb\nf_{i,b}\ndown, which in turn reduces\nw\ni\n,\nb\nw_{i,b}\nand eventually clamps it to zero once\nS\n^\ni\n,\nb\nâ‰¥\nÎ·\n{\\hat{S}_{i,b}\\!\\geq\\!\\eta}\n. Simultaneously, the adaptive RSI scheme increases the sampling frequency of challenging bins until their failure levels subside while preserving coverage via the samplerâ€™s floor probability, yielding a simple, performance-coupled continuation scheme that is guaranteed to decay assistance to zero at the desired similarity threshold.\nFigure S1\n:\nIllustration of main chain and support chain. Dashed links are assumed to be massless.\nFigure S2\n:\nConfiguration-dependent torque limit versus box torque limit for Atlas ankle.\n(a)\nFuture references.\nBaseline uses 1 future reference.\n(b)\nObservation history.\nBaseline uses 0 history.\nFigure S3\n:\nEffect of future references and observation history on success.\nWe report success statistics as a function of (left) the number of future reference steps\nand (right) the observation history length. Curves show the mean success rate, the 10th percentile (p10),\nand the minimum across multiple trajectories. Overall, increasing the future-reference horizon tends to\nreduce performance, with degraded lower-tail behavior (p10/min) at longer horizons. Increasing observation\nhistory is even more detrimental: performance drops more sharply, especially in the lower tail, as history\nlength grows, indicating that stacking past observations is harmful in this setting.\n(a)\nPushes.\n(b)\nMass.\n(c)\nFriction.\n(d)\nObservation noise.\n(e)\nInitial state.\n(f)\nActuator delay.\nFigure S4\n:\nRobustness to model uncertainty and disturbances.\nWe evaluate a single policy under controlled robustness stress tests and report normalized success statistics\n(mean, p10, and minimum over multiple trajectories), divided by the nominal (no-randomization) performance so that\nthe baseline at randomization level 0 equals 1.0. Unless noted otherwise, the policy was trained with\nrandomization level 1, and levels 2â€“3 extrapolate beyond the training distribution.\nActuator delay\nhas the strongest effect in this suite; importantly, delay randomization was\nnot\nincluded\nduring training, yet performance remains acceptable for modest delays (randomization levels 1â€“3, corresponding to\nuniformly sampled delays of 0â€“1, 0â€“2, and 0â€“3 control steps), suggesting that other training randomizations\n(e.g., observation/action noise and dynamics randomization) provide indirect robustness to latency.\nAmong the randomizations used during training,\ninitial-state perturbations\nhave the largest impact, producing\nthe steepest drop in both mean and lower-tail performance. We attribute this primarily to the fact that aggressive\ninitial-state perturbations can spawn the robot in self-colliding configurations (e.g., armâ€“torso collisions) that are\neffectively irreversible under the controller, leading to a higher probability of early failure.\nPush disturbances\nand\nfriction variation\nexhibit the next strongest degradation, particularly in the tail\n(p10/min), reflecting reduced robustness under aggressive external forces and contact parameter shifts.\nIn contrast,\nmass variation\nand\nobservation noise\nhave comparatively mild effects over the tested range,\nsuggesting the learned controller is largely invariant to these sources.\n(a)\nBox climb up.\n(b)\nBox climb down.\nFigure S5\n:\nRobustness maps for box-climbing policies from varied initial conditions (top-down view).\nEach trial is drawn at its initial planar offset\n(\nÎ”\nâ€‹\nx\n,\nÎ”\nâ€‹\ny\n)\n(\\Delta x,\\Delta y)\nrelative to the box, with heading indicated by a short arrow oriented by the initial yaw\nÎ”\nâ€‹\nÏˆ\n\\Delta\\psi\n; green indicates success and red indicates failure.\nAxes follow the manuscript convention:\nx\nx\npoints upward and\ny\ny\npoints left.\nThe box footprint is shown as a light rectangle (80â€‰cm along\nx\nx\nand 120â€‰cm along\ny\ny\n); in this zoomed view, only the portion of the footprint within the plotted window is visible.\nThe dashed black square indicates the training randomization range\nÎ”\nâ€‹\nx\n,\nÎ”\nâ€‹\ny\nâˆˆ\n[\nâˆ’\n10\n,\n10\n]\n\\Delta x,\\Delta y\\in[-10,10]\ncm, and the dashed blue arc indicates yaw randomization\nÎ”\nâ€‹\nÏˆ\nâˆˆ\n[\nâˆ’\n0.3\n,\n0.3\n]\n\\Delta\\psi\\in[-0.3,0.3]\nrad.\nLeft\n: nominal robot start is defined at\n(\n0\n,\n0\n)\n(0,0)\nwith the box center located 60â€‰cm ahead along\n+\nx\n+x\n.\nRight\n: nominal robot start is aligned with the box center at\n(\n0\n,\n0\n)\n(0,0)\n. For both box climb up and box climb down, all trials initialized within the training distribution succeed. We also observe successes outside this range (mainly for box climb down), while failures occur only for initial conditions beyond the training distribution.\nIt is important to note that for visualization purposes, the\ny\ny\n-positions were scattered by a small offset within\nÂ±\n8\n\\pm 8\ncm to reduce overlap (policy is inherently invariant to\ny\ny\n).\nTable S1:\nLibrary of reference motions used for multi-skill policy evaluation and ablation studies.\nTrajectory Index\nMotion / Description\nData Source\nDuration (s)\n0\nArmy Crawl\nMoCap\n16.633â€‰333â€‰206â€‰176â€‰758\n16.633\\,333\\,206\\,176\\,758\n1\nDance\nViCap\n9.333â€‰240â€‰509â€‰033â€‰203\n9.333\\,240\\,509\\,033\\,203\n2\nStylish Walk\nViCap\n11.533â€‰218â€‰383â€‰789â€‰062\n11.533\\,218\\,383\\,789\\,062\n3\nSoccer Kick\nViCap\n6.633â€‰267â€‰402â€‰648â€‰926\n6.633\\,267\\,402\\,648\\,926\n4\nBreakdance\nMoCap\n8.300â€‰000â€‰190â€‰734â€‰863\n8.300\\,000\\,190\\,734\\,863\n5\nCartwheel\nMoCap\n5.966â€‰666â€‰698â€‰455â€‰810â€‰5\n5.966\\,666\\,698\\,455\\,810\\,5\n6\nCrouch Walk\nMoCap\n8.633â€‰333â€‰206â€‰176â€‰758\n8.633\\,333\\,206\\,176\\,758\n7\nCrawl on all Fours\nMoCap\n15.050â€‰000â€‰190â€‰734â€‰863\n15.050\\,000\\,190\\,734\\,863\n8\nDeep Squat\nAnimation\n2.991â€‰666â€‰555â€‰404â€‰663\n2.991\\,666\\,555\\,404\\,663\n9\nAnimated Walk\nAnimation\n7.483â€‰333â€‰110â€‰809â€‰326\n7.483\\,333\\,110\\,809\\,326\n10\nKneeling\nMoCap\n8.383â€‰333â€‰206â€‰176â€‰758\n8.383\\,333\\,206\\,176\\,758\n11\nRun\nMoCap\n6.474â€‰999â€‰904â€‰632â€‰568\n6.474\\,999\\,904\\,632\\,568\n12\nLightsaber Routine\nAnimation\n9.399â€‰999â€‰618â€‰530â€‰273\n9.399\\,999\\,618\\,530\\,273\n13\nCartwheelâ€“Backflip\nMoCap\n5.866â€‰666â€‰793â€‰823â€‰242\n5.866\\,666\\,793\\,823\\,242\n14\nRoll on all Fours\nMoCap\n8.800â€‰000â€‰190â€‰734â€‰863\n8.800\\,000\\,190\\,734\\,863\nTable S2\n:\nNormalized mean-squared errors of simplified PLA models. Errors are calculated as the differences of the joint accelerations between simplified models and the exact model, and are normalized over the joint accelerations of the exact model.\nAnkle Pitch\nAnkle Roll\nLocally Projected Model\n9.527\ne\nâˆ’\n4\ne^{-4}\n3.418\ne\nâˆ’\n4\ne^{-4}\nDynamic Armature Model\n1.296\ne\nâˆ’\n3\ne^{-3}\n7.902\ne\nâˆ’\n4\ne^{-4}\nNominal Armature Model\n1.220\ne\nâˆ’\n3\ne^{-3}\n9.391\ne\nâˆ’\n4\ne^{-4}\nSimplest Model\n0.0776\n0.8673\nTable S3\n:\nObservation Terms Summary. We do not perform any scaling or clipping on individual observation terms. All noise models are zero-mean Gaussian and additive in nature. Privileged information is used by the critic only.\nTerm Name\nDefinition\nNoise\nDim.\nProprioceptive Observations\nTorso angular velocity\nğ\nI\nâ€‹\nT\nT\n{}_{T}\\bm{\\omega}_{IT}\nğ’©\nâ€‹\n(\n0\n,\n0.10\n2\n)\n\\mathcal{N}(0,\\,0.10^{2})\n3\n3\nGravity vector in torso frame\nğ’ˆ\nI\nT\n{}_{T}\\bm{g}_{I}\nğ’©\nâ€‹\n(\n0\n,\n0.015\n2\n)\n\\mathcal{N}(0,\\,0.015^{2})\n3\n3\nJoint positions\nğ’’\nj\n\\bm{q}_{j}\nğ’©\nâ€‹\n(\n0\n,\n0.005\n2\n)\n\\mathcal{N}(0,\\,0.005^{2})\nn\nj\nn_{j}\nJoint velocities\nğ’’\nË™\nj\n\\dot{\\bm{q}}_{j}\nğ’©\nâ€‹\n(\n0\n,\n0.25\n2\n)\n\\mathcal{N}(0,\\,0.25^{2})\nn\nj\nn_{j}\nPrevious action\nğ’‚\nt\nâˆ’\n1\n\\bm{a}_{t-1}\nâˆ’\n-\nn\na\nn_{a}\nReference Observations\nBase height\nğ’“\n^\nI\nâ€‹\nB\nz\nI\n{}_{I}\\hat{\\bm{r}}^{z}_{IB}\nâ€“\n1\n1\nBase linear velocity\nğ’—\n^\nI\nâ€‹\nB\nB\n{}_{B}\\hat{\\bm{v}}_{IB}\nâ€“\n3\n3\nBase angular velocity\nğ\n^\nI\nâ€‹\nB\nB\n{}_{B}\\hat{\\bm{\\omega}}_{IB}\nâ€“\n3\n3\nGravity vector in base frame\nğ’ˆ\n^\nI\nB\n{}_{B}\\hat{\\bm{g}}_{I}\nâ€“\n3\n3\nJoint positions\nğ’’\n^\nj\n\\hat{\\bm{q}}_{j}\nâ€“\nn\nj\nn_{j}\nPrivileged Information (critic only)\nBase linear velocity\nğ’—\nI\nâ€‹\nB\nB\n{}_{B}\\bm{v}_{IB}\nâ€“\n3\n3\nBase height\nğ’“\nI\nâ€‹\nB\nz\nI\n{}_{I}\\bm{r}^{z}_{IB}\nâ€“\n1\n1\nBase contact force\nğ’‡\ncontact\nB\n{}_{B}\\bm{f}_{\\text{contact}}\nâ€“\n3\n3\nKeybodies contact forces\n{\nğ’‡\ni\n}\ni\n=\n1\nn\nk\nâ€‹\nb\n\\{\\bm{f}_{i}\\}_{i=1}^{n_{kb}}\nâ€“\n3\nÃ—\nn\nk\nâ€‹\nb\n3\\times n_{kb}\nKeybody positions w.r.t. base\nğ’“\nB\nâ€‹\nK\nB\n{}_{B}\\bm{r}_{BK}\nâ€“\n3\nÃ—\nn\nk\nâ€‹\nb\n3\\times n_{kb}\nKeybody linear velocities\nğ’—\nI\nâ€‹\nK\nB\n{}_{B}\\bm{v}_{IK}\nâ€“\n3\nÃ—\nn\nk\nâ€‹\nb\n3\\times n_{kb}\nFictitious force (assistive wrench)\nğ’‡\nassist\n\\bm{f}_{\\text{assist}}\nâ€“\n3\n3\nFictitious torque (assistive wrench)\nğ‰\nassist\n\\bm{\\tau}_{\\text{assist}}\nâ€“\n3\n3\nWrench scale\nÎ²\n\\beta\nâ€“\n1\n1\nTracking rewards\nğ’“\ntrack\n\\bm{r}_{\\text{track}}\nâ€“\n7\n7\nTask phase\nÏ•\n\\phi\nâ€“\n1\n1\nTable S4\n:\nReward Terms Summary. The environment scales the reward weights with the time-step\nd\nâ€‹\nt\ndt\n. For brevity, the time index\nt\nt\nis omitted unless needed. Keybody positions and orientations are expressed w.r.t. the base frame. A common stiffness parameter of\nÎº\n=\n1\n/\n4\n\\kappa=1/4\nis set for all tracking reward terms.\nTerm Name\nDefinition\nWeight\nÏƒ\ni\n\\sigma_{i}\nTracking Reward\nBase position tracking\nexp\nâ¡\n(\nâˆ’\nÎº\nâ€‹\nâ€–\nğ«\nI\nâ€‹\nB\nâˆ’\nğ«\nI\nâ€‹\nB\nâˆ—\nâ€–\n2\n/\nÏƒ\n1\n2\n)\n\\exp\\!\\bigl(-\\kappa\\,\\|\\mathbf{r}_{IB}-\\mathbf{r}^{*}_{IB}\\|^{2}/\\sigma_{1}^{2}\\bigr)\n1\n1\n0.4\n0.4\nBase orientation\nexp\nâ¡\n(\nâˆ’\nÎº\nâ€‹\nâ€–\nğš½\nI\nâ€‹\nB\nâŠŸ\nğš½\nI\nâ€‹\nB\nâˆ—\nâ€–\n2\n/\nÏƒ\n2\n2\n)\n\\exp\\!\\bigl(-\\kappa\\,\\|\\bm{\\Phi}_{IB}\\boxminus\\bm{\\Phi}^{*}_{IB}\\|^{2}/\\sigma_{2}^{2}\\bigr)\n1\n1\n0.5\n0.5\nBase angular velocity\nexp\nâ¡\n(\nâˆ’\nÎº\nâ€‹\nâ€–\nğ\nI\nâ€‹\nB\nâˆ’\nğ\nI\nâ€‹\nB\nâˆ—\nâ€–\n2\n/\nÏƒ\n3\n2\n)\n\\exp\\!\\bigl(-\\kappa\\,\\|\\bm{\\omega}_{IB}-\\bm{\\omega}^{*}_{IB}\\|^{2}/\\sigma_{3}^{2}\\bigr)\n1\n1\n1.5\n1.5\nBase linear velocity\nexp\nâ¡\n(\nâˆ’\nÎº\nâ€‹\nâ€–\nğ¯\nI\nâ€‹\nB\nâˆ’\nğ¯\nI\nâ€‹\nB\nâˆ—\nâ€–\n2\n/\nÏƒ\n4\n2\n)\n\\exp\\!\\bigl(-\\kappa\\,\\|\\mathbf{v}_{IB}-\\mathbf{v}^{*}_{IB}\\|^{2}/\\sigma_{4}^{2}\\bigr)\n1\n1\n0.6\n0.6\nJoint position\nexp\nâ¡\n(\nâˆ’\nÎº\nâ€‹\nâ€–\nğ’’\nj\nâˆ’\nğ’’\nj\nâˆ—\nâ€–\n2\n/\nÏƒ\n5\n2\n)\n\\exp\\!\\bigl(-\\kappa\\,\\|\\bm{q}_{j}-\\bm{q}^{*}_{j}\\|^{2}/\\sigma_{5}^{2}\\bigr)\n1\n1\n0.3\nâ‹…\nn\nj\n0.3\\cdot\\sqrt{n_{j}}\nKeybodies position\nexp\nâ¡\n(\nâˆ’\nÎº\nâ€‹\nâ€–\nğ’“\nk\nâ€‹\nb\nâˆ’\nğ’“\nk\nâ€‹\nb\nâˆ—\nâ€–\n2\n/\nÏƒ\n6\n2\n)\n\\exp\\!\\bigl(-\\kappa\\,\\|\\bm{r}_{kb}-\\bm{r}^{*}_{kb}\\|^{2}/\\sigma_{6}^{2}\\bigr)\n1\n1\n0.2\nâ‹…\nn\nk\nâ€‹\nb\n0.2\\cdot\\sqrt{n_{kb}}\nKeybodies orientation\nexp\nâ¡\n(\nâˆ’\nÎº\nâ€‹\nâ€–\nğš½\nk\nâ€‹\nb\nâŠŸ\nğš½\nk\nâ€‹\nb\nâˆ—\nâ€–\n2\n/\nÏƒ\n7\n2\n)\n\\exp\\!\\bigl(-\\kappa\\,\\|\\bm{\\Phi}_{kb}\\boxminus\\bm{\\Phi}^{*}_{kb}\\|^{2}/\\sigma_{7}^{2}\\bigr)\n1\n1\n0.4\nâ‹…\nn\nk\nâ€‹\nb\n0.4\\cdot\\sqrt{n_{kb}}\nRegularization Penalty\nAction smoothness\nâˆ’\nâ€–\nğ’‚\nt\nâˆ’\nğ’‚\nt\nâˆ’\n1\nâ€–\n-\\|\\bm{a}_{t}-\\bm{a}_{t-1}\\|\n0.15\n0.15\nâ€“\nJoint acceleration\nâˆ’\nâ€–\nğ’’\nÂ¨\nj\nâ€–\n-\\|\\ddot{\\bm{q}}_{j}\\|\n1\nÃ—\n10\nâˆ’\n5\n1\\times 10^{-5}\nâ€“\nJoint position limit\nâˆ’\nâˆ‘\ni\nâˆˆ\nğ’¥\n[\nmax\nâ¡\n(\n0\n,\nq\ni\nmin\nâˆ’\nq\ni\n)\n+\nmax\nâ¡\n(\n0\n,\nq\ni\nâˆ’\nq\ni\nmax\n)\n]\n-\\displaystyle\\sum_{i\\in\\mathcal{J}}\\!\\Bigl[\\max(0,q^{\\min}_{i}-q_{i})+\\max(0,q_{i}-q^{\\max}_{i})\\Bigr]\n1.0\n1.0\nâ€“\nJoint torque limit\nâˆ’\nâˆ‘\ni\nâˆˆ\nğ’¥\n[\nmax\nâ¡\n(\n0\n,\nÏ„\ni\nmin\nâˆ’\nÏ„\ni\n)\n+\nmax\nâ¡\n(\n0\n,\nÏ„\ni\nâˆ’\nÏ„\ni\nmax\n)\n]\n-\\displaystyle\\sum_{i\\in\\mathcal{J}}\\!\\Bigl[\\max(0,\\tau^{\\min}_{i}-\\tau_{i})+\\max(0,\\tau_{i}-\\tau^{\\max}_{i})\\Bigr]\n0.1\n0.1\nâ€“\nSurvival Reward\nSurvival\n1\n1\n1\n1\nâ€“\nTable S5\n:\nDomain randomization terms including dynamics randomization and external perturbations.\nTerm\nValue\nStatic friction\nğ’°\nâ€‹\n(\n0.6\n,\n1.0\n)\n\\mathcal{U}(0.6,\\;1.0)\nDynamic friction\nğ’°\nâ€‹\n(\n0.5\n,\n0.9\n)\n\\mathcal{U}(0.5,\\;0.9)\nRestitution\nğ’°\nâ€‹\n(\n0.0\n,\n0.2\n)\n\\mathcal{U}(0.0,\\;0.2)\nLink masses\nğ’°\n(\n0.9\n,\n1.1\n)\nÃ—\n\\mathcal{U}(0.9,\\;1.1)\\,\\times\ndefault masses\nExternal disturbance (impulsive push applied to base)\nInterval\n=\nğ’°\nâ€‹\n(\n0\ns\n,\n10\ns\n)\n=\\mathcal{U}($0\\text{\\,}\\mathrm{s}$,\\;$10\\text{\\,}\\mathrm{s}$)\n,\nv\nx\nâ€‹\ny\n=\n0.5\nm\ns\nâˆ’\n1\nv_{xy}=$0.5\\text{\\,}\\mathrm{m}\\text{\\,}{\\mathrm{s}}^{-1}$\nTable S6\n:\nMDP Hyperparameters.\nHyperparameter\nValue\nEpisode length\n(\nL\nÂ¯\nepisode\n)\n(\\bar{L}_{\\mathrm{episode}})\n10\ns\n10\\text{\\,}\\mathrm{s}\nSimulation time-step\n(\nd\nâ€‹\nt\n)\n(dt)\n0.004\ns\n0.004\\text{\\,}\\mathrm{s}\nControl decimation\n5\nAction scale (per joint)\n{\n0.05\n,\n0.10\n,\n0.20\n}\n\\{0.05,\\;0.10,\\;0.20\\}\nBin width\n(\nÎ”\n)\n(\\Delta)\nmin\nâ¡\n(\n4.0\ns\n,\nmin\ni\nâ¡\nD\ni\n)\n\\min\\!\\bigl($4.0\\text{\\,}\\mathrm{s}$,\\,\\min_{i}D_{i}\\bigr)\nEMA alpha\n(\nÎ±\n)\n(\\alpha)\n0.005\nBase temperature\n(\nÏ„\nbase\n)\n(\\tau_{\\text{base}})\n1.0\nAdaptive sampling temperature\n(\nÏ„\n)\n(\\tau)\nÏ„\nbase\nlog\nâ¡\n(\n1\n+\n|\nÎ©\n|\n)\n\\displaystyle\\frac{\\tau_{\\text{base}}}{\\log\\!\\bigl(1+\\lvert\\Omega\\rvert\\bigr)}\nUniform floor weight\n(\nÎµ\n)\n(\\varepsilon)\n0.15\nMax wrench scale\n(\nÎ²\nmax\n)\n(\\beta_{\\max})\n0.60\nSimilarity threshold\n(\nÎ·\n)\n(\\eta)\n0.80\nVirtual torque PD gains\n(\nk\np\nÏ‰\n,\nk\nd\nÏ‰\n)\n(k_{p}^{\\omega},\\,k_{d}^{\\omega})\n(\n200.0\n,\n10.0\n)\n(200.0,\\,10.0)\nVirtual force PD gains\n(\nk\np\nv\n,\nk\nd\nv\n)\n(k_{p}^{v},\\,k_{d}^{v})\n(\n0.0\n,\n10.0\n)\n(0.0,\\,10.0)\nTable S7\n:\nPPO Hyperparameters\nHyperparameter\nValue\nActor Network\nMLP(512, 256, 128) with\nELU\nactivation\nCritic Network\nMLP(512, 512, 256) with\nELU\nactivation\nEmpirical Normalization\nTrue\nLearning Rate (start of training)\n1e-3\nLearning Rate Schedule\nâ€œadaptive\" (based on KL-divergence)\nDiscount Factor\n0.99\nGAE Discount Factor\n0.95\nDesired KL-divergence\n0.01\nClip Range\n0.2\nEntropy Coefficient\n0.001\nValue Function Loss Coefficient\n0.5\nNumber of Epochs\n5\nNumber of Environments\n4096\nBatch Size\n245,760\n(\n4096\nÃ—\n24\n)\n(4096\\times 24)\nMini-Batch Size\n61,440\n(\n4096\nÃ—\n6\n)\n(4096\\times 6)",
    "preview_text": "Achieving robust, human-like whole-body control on humanoid robots for agile, contact-rich behaviors remains a central challenge, demanding heavy per-skill engineering and a brittle process of tuning controllers. We introduce ZEST (Zero-shot Embodied Skill Transfer), a streamlined motion-imitation framework that trains policies via reinforcement learning from diverse sources -- high-fidelity motion capture, noisy monocular video, and non-physics-constrained animation -- and deploys them to hardware zero-shot. ZEST generalizes across behaviors and platforms while avoiding contact labels, reference or observation windows, state estimators, and extensive reward shaping. Its training pipeline combines adaptive sampling, which focuses training on difficult motion segments, and an automatic curriculum using a model-based assistive wrench, together enabling dynamic, long-horizon maneuvers. We further provide a procedure for selecting joint-level gains from approximate analytical armature values for closed-chain actuators, along with a refined model of actuators. Trained entirely in simulation with moderate domain randomization, ZEST demonstrates remarkable generality. On Boston Dynamics' Atlas humanoid, ZEST learns dynamic, multi-contact skills (e.g., army crawl, breakdancing) from motion capture. It transfers expressive dance and scene-interaction skills, such as box-climbing, directly from videos to Atlas and the Unitree G1. Furthermore, it extends across morphologies to the Spot quadruped, enabling acrobatics, such as a continuous backflip, through animation. Together, these results demonstrate robust zero-shot deployment across heterogeneous data sources and embodiments, establishing ZEST as a scalable interface between biological movements and their robotic counterparts.\n\nZEST: Zero-shot Embodied Skill Transfer for\nAthletic Robot Control\nJean-Pierre Sleiman\nHe Li\nAlphonsus Adu-Bredu\nRobin Deits\nArun Kumar\nKevin Bergamin\nMohak Bhardwaj\nScott Biddlestone\nNicola Burger\nM",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control",
        "fine tune",
        "offline Reinforcement Learning",
        "goal conditioned reinforcement learning",
        "world model"
    ],
    "one_line_summary": "ZEST æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ä»å¤šæ ·åŒ–è¿åŠ¨æ•°æ®ï¼ˆå¦‚åŠ¨ä½œæ•æ‰ã€è§†é¢‘ã€åŠ¨ç”»ï¼‰è®­ç»ƒç­–ç•¥ï¼Œå®ç°é›¶æ ·æœ¬è¿ç§»åˆ°æœºå™¨äººç¡¬ä»¶ä¸Šçš„è¿åŠ¨æ¨¡ä»¿æ¡†æ¶ï¼Œä¸“æ³¨äºå…¨èº«æ§åˆ¶å’ŒåŠ¨æ€æŠ€èƒ½ã€‚",
    "detailed_summary": "## ZESTï¼šç”¨äºä»¿äººæœºå™¨äººè¿åŠ¨æ§åˆ¶çš„é›¶æ ·æœ¬å…·èº«æŠ€èƒ½è¿ç§»\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nå®ç°ä»¿äººæœºå™¨äººç¨³å¥ã€ç±»äººçš„å…¨èº«æ§åˆ¶ï¼Œä»¥æ‰§è¡Œæ•æ·ã€æ¥è§¦ä¸°å¯Œçš„åŠ¨æ€è¡Œä¸ºï¼Œä¸€ç›´æ˜¯æœºå™¨äººå­¦é¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºé’ˆå¯¹ç‰¹å®šæŠ€èƒ½çš„é‡åº¦å·¥ç¨‹è®¾è®¡å’Œè„†å¼±çš„æ§åˆ¶å™¨è°ƒä¼˜è¿‡ç¨‹ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨¡ä»¿å­¦ä¹ å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤æ‚çš„å¤šé˜¶æ®µè®­ç»ƒæµç¨‹ã€ç²¾å¯†çš„å¥–åŠ±å‡½æ•°è®¾è®¡æˆ–å¯¹æ¥è§¦æ—¶åºçš„ç²¾ç¡®æ ‡æ³¨ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡ºäº† **ZEST**ï¼Œä¸€ä¸ªç»Ÿä¸€ã€ç²¾ç®€çš„è¿åŠ¨æ¨¡ä»¿æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åœ¨äºï¼š\n- **ç»Ÿä¸€æ•°æ®æ¥å£**ï¼šèƒ½å¤Ÿç›´æ¥ä»å¼‚æ„æ•°æ®æºï¼ˆé«˜ç²¾åº¦åŠ¨ä½œæ•æ‰ã€å«å™ªå£°çš„å•ç›®è§†é¢‘ã€éç‰©ç†çº¦æŸçš„åŠ¨ç”»ï¼‰è®­ç»ƒç­–ç•¥ï¼Œæ— éœ€æ¥è§¦æ ‡ç­¾ã€å‚è€ƒ/è§‚æµ‹çª—å£ã€çŠ¶æ€ä¼°è®¡å™¨æˆ–å¤æ‚çš„å¥–åŠ±å¡‘å½¢ã€‚\n- **é«˜æ•ˆè®­ç»ƒæœºåˆ¶**ï¼šç»“åˆäº†**è‡ªé€‚åº”é‡‡æ ·**ï¼ˆèšç„¦äºéš¾å­¦çš„è¿åŠ¨ç‰‡æ®µï¼‰å’Œ**åŸºäºæ¨¡å‹çš„è¾…åŠ©åŠ›çŸ©è‡ªåŠ¨è¯¾ç¨‹**ï¼Œæœ‰æ•ˆè§£å†³äº†é•¿æ—¶ç¨‹ã€é«˜åŠ¨æ€åŠ¨ä½œçš„å­¦ä¹ éš¾é¢˜ã€‚\n- **ç²¾ç¡®çš„å»ºæ¨¡ä¸éƒ¨ç½²**ï¼šæå‡ºäº†ä»è¿‘ä¼¼è§£æçš„ç”µæœºæƒ¯é‡å€¼æ¨å¯¼å…³èŠ‚çº§PDå¢ç›Šçš„æ–¹æ³•ï¼Œå¹¶æ”¹è¿›äº†æ‰§è¡Œå™¨æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯å¹¶è”è¿æ†æ‰§è¡Œå™¨ï¼‰ï¼Œä»¥ç¼©å°ä»¿çœŸä¸ç°å®çš„å·®è·ã€‚\n- **æç®€ç­–ç•¥æ¶æ„**ï¼šç­–ç•¥ä»…ä½¿ç”¨å½“å‰æœ¬ä½“æ„Ÿè§‰å’Œä¸‹ä¸€æ—¶åˆ»çš„å‚è€ƒçŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºæ®‹å·®å…³èŠ‚ç›®æ ‡ï¼Œé€šè¿‡ç®€å•çš„å‰é¦ˆç½‘ç»œå®ç°ï¼Œç¡®ä¿äº†é›¶æ ·æœ¬ç¡¬ä»¶éƒ¨ç½²çš„å¯è¡Œæ€§ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nZEST åœ¨å¤šä¸ªæœºå™¨äººå¹³å°ä¸Šå±•ç¤ºäº†å“è¶Šçš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ï¼š\n- **åœ¨æ³¢å£«é¡¿åŠ¨åŠ› Atlas æœºå™¨äººä¸Š**ï¼šæˆåŠŸæ‰§è¡Œäº†ä»åŠ¨ä½œæ•æ‰æ•°æ®å­¦ä¹ çš„åŠ¨æ€å¤šæ¥è§¦æŠ€èƒ½ï¼Œå¦‚**å†›é˜Ÿçˆ¬è¡Œã€éœ¹é›³èˆã€å‰æ»šç¿»ã€ä¾§æ‰‹ç¿»**ã€‚\n- **åœ¨ Unitree G1 æœºå™¨äººä¸Š**ï¼šé¦–æ¬¡å®ç°äº†ä»**å•ç›®è§†é¢‘**åˆ°ç¡¬ä»¶çš„ç›´æ¥æŠ€èƒ½è¿ç§»ï¼ŒåŒ…æ‹¬**èˆè¹ˆã€è·³ä¸Š/çˆ¬ä¸‹ç®±å­**ç­‰å¤æ‚çš„åœºæ™¯äº¤äº’æŠ€èƒ½ã€‚\n- **åœ¨æ³¢å£«é¡¿åŠ¨åŠ› Spot å››è¶³æœºå™¨äººä¸Š**ï¼šåˆ©ç”¨åŠ¨ç”»æ•°æ®å®ç°äº†**è¿ç»­åç©ºç¿»ã€æ¡¶æ»š**ç­‰æ‚æŠ€åŠ¨ä½œã€‚\nå®éªŒè¡¨æ˜ï¼ŒZEST åœ¨å¤šç§æŠ€èƒ½ä¸Šå–å¾—äº†è¾ƒä½çš„å…³èŠ‚å’Œå§¿æ€è·Ÿè¸ªè¯¯å·®ï¼Œå¹¶ä¸”åœ¨ä»¿çœŸä¸­é€šè¿‡æ¶ˆèå®éªŒéªŒè¯äº†å…¶è‡ªé€‚åº”è¯¾ç¨‹å’Œé‡‡æ ·æœºåˆ¶çš„å…³é”®ä½œç”¨ã€‚ä¸å…ˆè¿›çš„æ¨¡å‹é¢„æµ‹æ§åˆ¶åŸºçº¿ç›¸æ¯”ï¼ŒZEST æ— éœ€é¢„å®šä¹‰æ¥è§¦åºåˆ—ï¼Œå¯¹æ¨¡å‹å¤±é…å’Œæ¥è§¦æ—¶åºè¯¯å·®æ›´å…·é²æ£’æ€§ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\nZEST å»ºç«‹äº†ä¸€ä¸ª**å¯æ‰©å±•çš„ã€é€šç”¨çš„æ¥å£**ï¼Œèƒ½å¤Ÿå°†ä¸°å¯Œçš„ç”Ÿç‰©è¿åŠ¨æ•°æ®ç›´æ¥è½¬åŒ–ä¸ºé²æ£’çš„æœºå™¨äººæ§åˆ¶ç­–ç•¥ã€‚å…¶ä»·å€¼ä½“ç°åœ¨ï¼š\n- **æŠ€æœ¯çªç ´**ï¼šé¦–æ¬¡åœ¨è¶³å°ºä»¿äººæœºå™¨äººä¸Šå®ç°äº†åŠ¨æ€å¤šæ¥è§¦æŠ€èƒ½çš„é›¶æ ·æœ¬éƒ¨ç½²ï¼Œå¹¶é¦–æ¬¡å°†è§†é¢‘ç›´æ¥ç”¨äºä»¿äººæœºå™¨äººçš„åŠ¨æ€æŠ€èƒ½è¿ç§»ã€‚\n- **æ–¹æ³•å­¦è´¡çŒ®**ï¼šæä¾›äº†ä¸€ç§ç²¾ç®€ã€é«˜æ•ˆçš„æ¨¡ä»¿å­¦ä¹ èŒƒå¼ï¼Œæ˜¾è‘—é™ä½äº†å¯¹æ•°æ®è´¨é‡ã€å·¥ç¨‹è°ƒä¼˜å’Œå¤æ‚æµç¨‹çš„ä¾èµ–ã€‚\n- **åº”ç”¨å‰æ™¯**ï¼šä¸ºæœºå™¨äººå¿«é€Ÿå­¦ä¹ å¤šæ ·åŒ–ã€äººæ€§åŒ–çš„è¿åŠ¨æŠ€èƒ½å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œæ¨åŠ¨äº†å…·èº«æ™ºèƒ½å‘æ›´é€šç”¨ã€æ›´å®ç”¨çš„æ–¹å‘å‘å±•ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šZEST\n\n### **ä¸€ã€ æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³ä¸€ä¸ªæœºå™¨äººæ§åˆ¶é¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜ï¼š**å¦‚ä½•è®©ä»¿äººæœºå™¨äººï¼ˆå¦‚Atlasï¼‰å’Œè¶³å¼æœºå™¨äººï¼ˆå¦‚Spotï¼‰ä»¥ç±»äººçš„æµç•…æ€§å’Œé²æ£’æ€§ï¼Œæ‰§è¡Œå¤æ‚ã€åŠ¨æ€ã€å¤šæ¥è§¦çš„å…¨èº«è¿åŠ¨æŠ€èƒ½**ã€‚\n\nä¼ ç»Ÿæ–¹æ³•å­˜åœ¨ä»¥ä¸‹ç—›ç‚¹ï¼š\n1.  **æ¨¡å‹ä¾èµ–æ€§å¼º**ï¼šåŸºäºæ¨¡å‹çš„æ§åˆ¶å™¨ï¼ˆå¦‚MPCï¼‰éœ€è¦ç²¾ç¡®çš„ç¯å¢ƒå»ºæ¨¡ã€é¢„å®šä¹‰çš„æ¥è§¦åºåˆ—å’Œå¤æ‚çš„è½¨è¿¹ä¼˜åŒ–ï¼Œéš¾ä»¥å¤„ç†é«˜ä¸ç¡®å®šæ€§æˆ–å¤æ‚æ¥è§¦åœºæ™¯ã€‚\n2.  **å·¥ç¨‹æˆæœ¬é«˜**ï¼šä¸ºæ¯ä¸ªæ–°æŠ€èƒ½è®¾è®¡æ§åˆ¶å™¨å’Œå¥–åŠ±å‡½æ•°éœ€è¦å¤§é‡çš„äººå·¥è°ƒä¼˜ï¼Œè¿‡ç¨‹è„†å¼±ä¸”éš¾ä»¥è§„æ¨¡åŒ–ã€‚\n3.  **æ•°æ®æºåˆ©ç”¨å—é™**ï¼šéš¾ä»¥ç›´æ¥åˆ©ç”¨å¤šæ ·åŒ–çš„è¿åŠ¨æ•°æ®æºï¼ˆå¦‚åŠ¨æ•ã€è§†é¢‘ã€åŠ¨ç”»ï¼‰æ¥ç”Ÿæˆé²æ£’çš„æ§åˆ¶ç­–ç•¥ï¼Œå°¤å…¶æ˜¯å­˜åœ¨å™ªå£°æˆ–ç‰©ç†ä¸å¯è¡Œæ€§çš„æ•°æ®ã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nZEST æå‡ºäº†ä¸€ç§**ç»Ÿä¸€ã€ç®€åŒ–ã€å¯æ‰©å±•çš„é›¶æ ·æœ¬è¿åŠ¨æ¨¡ä»¿æ¡†æ¶**ï¼Œå…¶åˆ›æ–°æ€§ä½“ç°åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š\n\n1.  **ç»Ÿä¸€çš„å¼‚æ„æ•°æ®æºå¤„ç†**ï¼š\n    - **é¦–åˆ›æ€§**ï¼šé¦–æ¬¡åœ¨**å…¨å°ºå¯¸ä»¿äººæœºå™¨äººï¼ˆAtlasï¼‰** ä¸Šå®ç°äº†ä»**åŠ¨æ•æ•°æ®**åˆ°åŠ¨æ€å¤šæ¥è§¦æŠ€èƒ½ï¼ˆå¦‚å†›é˜Ÿçˆ¬è¡Œã€éœ¹é›³èˆï¼‰çš„é›¶æ ·æœ¬è¿ç§»ã€‚\n    - **çªç ´æ€§**ï¼šé¦–æ¬¡å®ç°äº†ä»**å•ç›®è§†é¢‘**åˆ°ç‰©ç†ä»¿äººæœºå™¨äººï¼ˆUnitree G1ï¼‰çš„**åŠ¨æ€ã€æ¥è§¦ä¸°å¯Œçš„æŠ€èƒ½ï¼ˆå¦‚è·³èˆã€ç®±å­æ”€çˆ¬ï¼‰** çš„é›¶æ ·æœ¬è¿ç§»ã€‚\n    - **è·¨å½¢æ€æ³›åŒ–**ï¼šåŒä¸€æ¡†æ¶å¯å¤„ç†åŠ¨æ•ã€è§†é¢‘æ•æ‰å’Œå…³é”®å¸§åŠ¨ç”»æ•°æ®ï¼Œå¹¶èƒ½å°†æŠ€èƒ½è¿ç§»åˆ°å½¢æ€è¿¥å¼‚çš„æœºå™¨äººä¸Šï¼ˆäººå½¢Atlas/G1 åˆ°å››è¶³Spotï¼‰ã€‚\n\n2.  **æç®€åŒ–çš„è®­ç»ƒé…æ–¹**ï¼š\n    - **æ¶ˆé™¤å¤æ‚ç»„ä»¶**ï¼š**åˆ»æ„é¿å…ä½¿ç”¨**æ¥è§¦æ ‡ç­¾ã€æœªæ¥å‚è€ƒçª—å£ã€é•¿è§‚æµ‹å†å²ã€çŠ¶æ€ä¼°è®¡å™¨ã€å¤æ‚çš„å¤šé˜¶æ®µè®­ç»ƒæµç¨‹å’Œå¤§é‡çš„å¥–åŠ±å‡½æ•°å·¥ç¨‹ã€‚\n    - **æœ€å°åŒ–è§‚æµ‹**ï¼šç­–ç•¥ä»…ä½¿ç”¨**å½“å‰æœ¬ä½“æ„ŸçŸ¥ä¿¡å·**å’Œ**ä¸‹ä¸€æ—¶åˆ»çš„å‚è€ƒçŠ¶æ€**ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºå…³èŠ‚ä½ç½®çš„æ®‹å·®ä¿®æ­£ã€‚è¿™ä½¿å¾—ç­–ç•¥æå…¶ç®€æ´ï¼Œæ˜“äºéƒ¨ç½²ã€‚\n\n3.  **å…³é”®è®­ç»ƒæŠ€æœ¯åˆ›æ–°**ï¼š\n    - **è‡ªé€‚åº”å‚è€ƒçŠ¶æ€åˆå§‹åŒ–**ï¼šå°†é•¿æ—¶ç¨‹è½¨è¿¹åˆ†æ®µï¼Œæ ¹æ®å„æ®µçš„â€œå¤±è´¥æ°´å¹³â€åŠ¨æ€è°ƒæ•´é‡‡æ ·æ¦‚ç‡ï¼Œé›†ä¸­èµ„æºå­¦ä¹ å›°éš¾ç‰‡æ®µï¼Œé¿å…ç¾éš¾æ€§é—å¿˜ã€‚\n    - **è‡ªåŠ¨è¾…åŠ©åŠ›çŸ©è¯¾ç¨‹**ï¼šå¯¹äºé«˜åŠ¨æ€è¡Œä¸ºï¼ˆå¦‚ç©ºç¿»ï¼‰ï¼Œåœ¨è®­ç»ƒåˆæœŸå‘æœºå™¨äººåŸºåº§æ–½åŠ ä¸€ä¸ªæ¨¡å‹è®¡ç®—çš„è™šæ‹Ÿè¾…åŠ©åŠ›çŸ©ï¼Œå¹¶éšæ€§èƒ½æå‡è‡ªåŠ¨è¡°å‡è‡³é›¶ã€‚è¿™ç¨³å®šäº†è®­ç»ƒï¼ŒåŠ é€Ÿäº†æ”¶æ•›ã€‚\n    - **æ”¹è¿›çš„å»ºæ¨¡ä¸å¢ç›Šæ•´å®š**ï¼šæå‡ºäº†å¹¶è”è¿æ†æ‰§è¡Œå™¨çš„æ¸è¿›ç®€åŒ–æ¨¡å‹ï¼Œå¹¶åŸºäºè¿‘ä¼¼çš„ç”µæ¢æƒ¯é‡å€¼ï¼Œä¸ºé—­ç¯é“¾æ‰§è¡Œå™¨æä¾›äº†**ä»åˆ†ææ¨¡å‹æ¨å¯¼å…³èŠ‚çº§PDå¢ç›Š**çš„æµç¨‹ï¼Œå‡å°‘äº†sim-to-realçš„å·®è·ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆæ¦‚è¿°**\nZEST çš„è§£å†³æ–¹æ¡ˆæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æµæ°´çº¿ï¼Œå…¶æ ¸å¿ƒæµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ˆåŸºäºè®ºæ–‡å›¾4ï¼‰ï¼š\n\n```mermaid\ngraph TD\n    A[å¼‚æ„è¿åŠ¨æ•°æ®] --> B[è¿åŠ¨é‡å®šå‘];\n    B --> C[è®­ç»ƒé˜¶æ®µ];\n    subgraph C [è®­ç»ƒé˜¶æ®µ - åœ¨ä»¿çœŸä¸­è¿›è¡Œ]\n        C1[è‡ªé€‚åº”é‡‡æ ·] --> C2[RLç­–ç•¥è®­ç»ƒ];\n        C3[è‡ªåŠ¨è¾…åŠ©åŠ›çŸ©è¯¾ç¨‹] --> C2;\n        C4[åŸŸéšæœºåŒ–] --> C2;\n    end\n    C2 --> D[é›¶æ ·æœ¬éƒ¨ç½²];\n    D --> E[ç‰©ç†æœºå™¨äººç¡¬ä»¶];\n    \n    A1[åŠ¨æ•æ•°æ®] --> A;\n    A2[è§†é¢‘æ•°æ®] --> A;\n    A3[å…³é”®å¸§åŠ¨ç”»] --> A;\n    \n    E1[Atlasäººå½¢æœºå™¨äºº] --> E;\n    E2[Unitree G1äººå½¢æœºå™¨äºº] --> E;\n    E3[Spotå››è¶³æœºå™¨äºº] --> E;\n```\n\n1.  **æ•°æ®å‡†å¤‡**ï¼šä»åŠ¨æ•ã€è§†é¢‘ã€åŠ¨ç”»ä¸­è·å–å‚è€ƒè¿åŠ¨ï¼Œé€šè¿‡è¿åŠ¨å­¦é‡å®šå‘æ˜ å°„åˆ°ç›®æ ‡æœºå™¨äººéª¨æ¶ã€‚**ä¸ä¾èµ–ä»»ä½•æ¥è§¦æ ‡ç­¾**ã€‚\n2.  **è®­ç»ƒæ¡†æ¶**ï¼š\n    - **MDPè®¾è®¡**ï¼šç­–ç•¥è§‚æµ‹ä»…ä¸ºå½“å‰æœ¬ä½“æ„Ÿï¼ˆå…³èŠ‚ä½ç½®/é€Ÿåº¦ã€IMUæ•°æ®ã€ä¸Šä¸€åŠ¨ä½œï¼‰å’Œä¸‹ä¸€å‚è€ƒçŠ¶æ€ã€‚å¥–åŠ±å‡½æ•°é€šç”¨ï¼Œä»…åŒ…å«è·Ÿè¸ªå¥–åŠ±ã€æ­£åˆ™åŒ–å¥–åŠ±å’Œç”Ÿå­˜å¥–åŠ±ã€‚\n    - **è®­ç»ƒæŠ€å·§**ï¼šç»“åˆ**è‡ªé€‚åº”é‡‡æ ·**å’Œ**è‡ªåŠ¨è¾…åŠ©åŠ›çŸ©è¯¾ç¨‹**ï¼Œåœ¨Isaac Labä»¿çœŸä¸­ä½¿ç”¨PPOç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œå¹¶æ–½åŠ é€‚åº¦çš„åŸŸéšæœºåŒ–ï¼ˆå¤–åŠ›æ‰°åŠ¨ã€è´¨é‡/æ‘©æ“¦å˜åŒ–ã€ä¼ æ„Ÿå™¨å™ªå£°ï¼‰ã€‚\n    - **ä¸å¯¹ç§°è¯„è®ºå®¶**ï¼šè¯„è®ºå®¶ç½‘ç»œä½¿ç”¨ç‰¹æƒä¿¡æ¯ï¼ˆå¦‚çœŸå®åŸºåº§é€Ÿåº¦ã€æ¥è§¦åŠ›ï¼‰åŠ é€Ÿè®­ç»ƒï¼Œä½†ç­–ç•¥ç½‘ç»œä¸ä½¿ç”¨ï¼Œç¡®ä¿å¯ç›´æ¥éƒ¨ç½²ã€‚\n3.  **é›¶æ ·æœ¬éƒ¨ç½²**ï¼šè®­ç»ƒå®Œæˆåï¼Œç­–ç•¥è¢«è½¬æ¢ä¸ºONNXæ ¼å¼ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµæ°´çº¿ç›´æ¥éƒ¨ç½²åˆ°ç‰©ç†æœºå™¨äººï¼Œ**æ— éœ€ä»»ä½•å¾®è°ƒ**ã€‚ä½å±‚æ§åˆ¶é‡‡ç”¨åŸºäºæ¨å¯¼å‡ºçš„PDå¢ç›Šçš„å…³èŠ‚çº§PDæ§åˆ¶å™¨ã€‚\n\n### **å››ã€ å®é™…ä»·å€¼ä¸æ„ä¹‰**\n1.  **æŠ€æœ¯æ°‘ä¸»åŒ–**ï¼šæä¾›äº†ä¸€ç§**æ ‡å‡†åŒ–ã€ä½é—¨æ§›**çš„è¿åŠ¨æŠ€èƒ½è¿ç§»æµç¨‹ã€‚ç‰¹åˆ«æ˜¯è§†é¢‘åˆ°æŠ€èƒ½çš„ç®¡é“ï¼Œä½¿å¾—æ™®é€šäººç”¨æ‰‹æœºå½•åˆ¶è§†é¢‘å³å¯ä¸ºæœºå™¨äººç”Ÿæˆæ–°æŠ€èƒ½ï¼Œæå¤§åœ°é™ä½äº†æ•°æ®è·å–æˆæœ¬ã€‚\n2.  **æå‡æœºå™¨äººé€šç”¨æ€§**ï¼šä½¿å•ä¸ªæœºå™¨äººèƒ½å¤Ÿæ‰§è¡Œä»æ—¥å¸¸æ­¥æ€åˆ°é«˜éš¾åº¦ç‰¹æŠ€çš„**å¹¿æ³›æŠ€èƒ½åº“**ï¼Œå‘å®ç°çœŸæ­£çš„â€œé€šç”¨ç‰©ç†æ™ºèƒ½â€è¿ˆå‡ºé‡è¦ä¸€æ­¥ã€‚\n3.  **è·¨å¹³å°é€šç”¨æ¡†æ¶**ï¼šè¯æ˜äº†åŒä¸€å¥—ç®—æ³•æ¡†æ¶å¯ä»¥é€‚ç”¨äºä¸åŒå°ºå¯¸ã€é‡é‡å’Œå½¢æ€çš„æœºå™¨äººï¼Œå‡å°‘äº†ä¸ºä¸åŒæœºå™¨äººå¼€å‘ä¸“å±æ§åˆ¶å™¨çš„éœ€æ±‚ã€‚\n4.  **ä¸ºé«˜çº§äº¤äº’å¥ å®šåŸºç¡€**ï¼šè¯¥æ¡†æ¶å¯ä½œä¸ºåº•å±‚â€œè¿åŠ¨è·Ÿè¸ªå™¨â€ï¼Œæœªæ¥å¯ä¸é«˜å±‚è§„åˆ’å™¨ï¼ˆå¦‚åŸºäºæ‰©æ•£æ¨¡å‹æˆ–è¯­è¨€çš„è§„åˆ’å™¨ï¼‰ç»“åˆï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººè¡Œä¸ºåˆæˆã€‚\n\n**æ€»ç»“**ï¼šZEST çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºé€šè¿‡ä¸€ä¸ª**ç²¾å¿ƒè®¾è®¡ã€æå¤§ç®€åŒ–**çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œ**æ‰“é€šäº†ä»å¤šæ ·ã€ä¸å®Œç¾çš„è¿åŠ¨æ•°æ®åˆ°é²æ£’ã€é›¶æ ·æœ¬ç¡¬ä»¶éƒ¨ç½²çš„é€šé“**ï¼Œåœ¨å¤šä¸ªæœºå™¨äººå¹³å°ä¸Šå®ç°äº†å‰æ‰€æœªæœ‰çš„åŠ¨æ€æŠ€èƒ½è¡¨ç°ï¼Œä¸ºæœºå™¨äººå­¦ä¹ äººç±»èˆ¬çš„è¿åŠ¨æ™ºèƒ½æä¾›äº†ä¸€ä¸ªå¼ºå¤§ä¸”å¯æ‰©å±•çš„è“å›¾ã€‚",
            "timestamp": "2026-02-03T16:02:28.863263",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## ZESTè®ºæ–‡åˆ†æï¼šåŸºäºå‰ä½œçš„æŠ€èƒ½è¿ç§»å‘å±•è„‰ç»œæ¢³ç†\n\næœ¬æ–‡æå‡ºçš„**ZEST (Zero-shot Embodied Skill Transfer)** æ¡†æ¶ï¼Œæ˜¯æœºå™¨äººè¿åŠ¨æ¨¡ä»¿ä¸æ§åˆ¶é¢†åŸŸé•¿æœŸå‘å±•çš„ä¸€ä¸ªé›†å¤§æˆè€…å’Œé‡è¦çªç ´ã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯å°†å¤šæ ·åŒ–çš„è¿åŠ¨æ•°æ®ï¼ˆåŠ¨æ•ã€è§†é¢‘ã€åŠ¨ç”»ï¼‰è½¬åŒ–ä¸ºå¯åœ¨æœºå™¨äººä¸Š**é›¶æ ·æœ¬éƒ¨ç½²**çš„é²æ£’æ§åˆ¶ç­–ç•¥ï¼Œå¹¶è·¨è¶Šä¸åŒçš„æœºå™¨äººå½¢æ€ã€‚\n\nä¸ºäº†æ·±å…¥ç†è§£ZESTçš„åˆ›æ–°ï¼Œæˆ‘ä»¬éœ€æ¢³ç†å…¶æŠ€æœ¯è„‰ç»œï¼Œå®ƒä¸»è¦å»ºç«‹åœ¨**ä¸‰ä¸ªå¹¶è¡Œä¸”ç›¸äº’äº¤ç»‡çš„å‘å±•æ–¹å‘**ä¹‹ä¸Šï¼š\n\n### æ–¹å‘ä¸€ï¼šä»â€œç¦»çº¿è§„åˆ’ï¼Œåœ¨çº¿è·Ÿè¸ªâ€åˆ°â€œæ•°æ®é©±åŠ¨çš„ç«¯åˆ°ç«¯æ§åˆ¶â€\n\nè¿™ä¸ªæ–¹å‘å…³æ³¨**æ§åˆ¶èŒƒå¼çš„æ ¹æœ¬è½¬å˜**ã€‚\n\n1.  **ä¼ ç»Ÿæ¨¡å‹åŸºèŒƒå¼ (Model-based â€œPlan offline, track onlineâ€)**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šæ³¢å£«é¡¿åŠ¨åŠ›Atlasçš„è·‘é…·/èˆè¹ˆï¼ˆåŸºäºMPCå’Œå…¨èº«ä¼˜åŒ–æ§åˆ¶å™¨ï¼‰ã€ä½¿ç”¨æ‰¶æ‰‹çˆ¬æ¥¼æ¢¯çš„äººå½¢æœºå™¨äººã€å±•ç¤ºç‰¹æŠ€çš„å››è¶³æœºå™¨äººç­‰ã€‚\n    *   **æ ¸å¿ƒ**ï¼šé€šè¿‡ç¦»çº¿è½¨è¿¹ä¼˜åŒ–ï¼ˆå¸¸ä¸ºå¤šæ¥è§¦ï¼‰ç”ŸæˆåŠ¨æ€å¯è¡Œçš„è¡Œä¸ºï¼Œå†ç”±é€šç”¨çš„æ¨¡å‹åŸºæ§åˆ¶å™¨ï¼ˆå¦‚MPCï¼‰åœ¨çº¿è·Ÿè¸ªã€‚\n    *   **ä¼˜ç‚¹**ï¼šé€šç”¨æ€§å¼ºã€å¯è§£é‡Šæ€§å¥½ã€‚\n    *   **å±€é™**ï¼š**ä¸¥é‡ä¾èµ–ç²¾ç¡®çš„ç¯å¢ƒå»ºæ¨¡ã€çŠ¶æ€ä¼°è®¡å’Œé«˜è´¨é‡çš„å‚è€ƒè½¨è¿¹**ã€‚å¯¹äºæ¥è§¦å¤æ‚ã€ä¸ç¡®å®šæ€§é«˜çš„åœºæ™¯ï¼ˆå¦‚å…¨èº«æ¥è§¦ã€æ»‘åŠ¨ï¼‰ï¼Œè®¾è®¡ç¨³å®šæ§åˆ¶å™¨æå…¶å›°éš¾ã€‚\n\n2.  **å¼ºåŒ–å­¦ä¹ èŒƒå¼ (Reinforcement Learning)**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šåœ¨å¤æ‚åœ°å½¢ä¸Šé²æ£’å¯¼èˆªçš„å››è¶³æœºå™¨äººã€é«˜é€Ÿå¥”è·‘ã€æé™è·‘é…·ã€çµå·§æ“ä½œç­‰ã€‚\n    *   **æ”¹è¿›**ï¼šRLé€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œ**éšå¼åœ°å­¦ä¹ ç®¡ç†å¤æ‚çš„æ¥è§¦åŠ¨åŠ›å­¦**ï¼Œæ— éœ€é¢„å®šä¹‰æ¥è§¦æ—¶åºï¼Œç®€åŒ–äº†æ§åˆ¶è®¾è®¡ã€‚åˆ©ç”¨é«˜ä¿çœŸä»¿çœŸå’Œé›¶æ ·æœ¬è¿ç§»æŠ€æœ¯ï¼Œå®ç°äº†å“è¶Šçš„æ•æ·æ€§å’Œé²æ£’æ€§ã€‚\n    *   **å±€é™**ï¼š**æ ·æœ¬æ•ˆç‡ä½ã€å¯¹å¥–åŠ±å‡½æ•°è®¾è®¡é«˜åº¦æ•æ„Ÿ**ï¼ˆâ€œç™½æ¿â€RLï¼‰ã€‚ä¸å½“çš„å¥–åŠ±è®¾è®¡ä¼šå¯¼è‡´ç­–ç•¥åˆ©ç”¨æ¼æ´ï¼Œäº§ç”Ÿä¸è‡ªç„¶æˆ–è¿‡äºæ¿€è¿›çš„è¡Œä¸ºã€‚\n\n3.  **æ•°æ®å¼•å¯¼çš„RLèŒƒå¼ (Data-guided RL)**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼š`DeepMimic`ï¼ˆç”¨åŠ¨æ•æ•°æ®æ­£åˆ™åŒ–ç­–ç•¥ï¼‰ã€`AMP`ï¼ˆå¯¹æŠ—æ€§è¿åŠ¨å…ˆéªŒï¼‰ã€`CALM`ï¼ˆå¯å¼•å¯¼çš„æ½œåœ¨æŠ€èƒ½æ¨¡å‹ï¼‰ã€‚\n    *   **æ”¹è¿›**ï¼šå¼•å…¥**è¿åŠ¨æ•°æ®ä½œä¸ºå…ˆéªŒ**æ¥æ­£åˆ™åŒ–ç­–ç•¥å­¦ä¹ ï¼Œå¼•å¯¼æ§åˆ¶å™¨äº§ç”Ÿæ›´è‡ªç„¶çš„è¿åŠ¨ï¼Œ**å¤§å¹…å‡å°‘äº†å¯¹å¤æ‚å¥–åŠ±å·¥ç¨‹çš„éœ€æ±‚**ã€‚æ•°æ®ï¼ˆæ— è®ºæ˜¯æ˜¾å¼å‚è€ƒç‰‡æ®µè¿˜æ˜¯å¯¹æŠ—æ€§å…ˆéªŒï¼‰æˆä¸ºç®€åŒ–è®¾è®¡ã€æå‡è¿åŠ¨è´¨é‡å’Œæ‰©å±•è¡Œä¸ºåº“çš„å¼ºå¤§å·¥å…·ã€‚\n    *   **å±€é™**ï¼šè¿™äº›å·¥ä½œä¸»è¦åœ¨**ä»¿çœŸç¯å¢ƒ**ä¸­å–å¾—çªç ´ï¼Œå¿½ç•¥äº†æ‰§è¡Œå™¨çš„çœŸå®ç‰©ç†æé™ï¼ˆæ‰­çŸ©ã€é€Ÿåº¦ï¼‰ï¼Œä¸”é€šå¸¸éœ€è¦å…¨å±€çŠ¶æ€æˆ–å†å²è§‚æµ‹ï¼Œå¢åŠ äº†éƒ¨ç½²å¤æ‚åº¦ã€‚\n\n4.  **ZESTçš„è´¡çŒ® (æœ¬å·¥ä½œ)**\n    *   **æ”¹è¿›ä¸æ•´åˆ**ï¼šZESTç»§æ‰¿äº†æ•°æ®å¼•å¯¼RLçš„æ€æƒ³ï¼Œä½†**å°†å…¶æ¨å‘äº†ä¸€ä¸ªæå…¶ç²¾ç®€å’Œç»Ÿä¸€çš„ç¡¬ä»¶éƒ¨ç½²æ¡†æ¶**ã€‚\n        *   **ç»Ÿä¸€å¤„ç†å¼‚æ„æ•°æ®**ï¼šé¦–æ¬¡åœ¨**å•ä¸€æ¡†æ¶**å†…ï¼ŒæˆåŠŸå¤„ç†äº†é«˜ä¿çœŸåŠ¨æ•ã€å¸¦å™ªå£°çš„å•ç›®è§†é¢‘é‡å»ºã€ä»¥åŠéç‰©ç†çº¦æŸçš„åŠ¨ç”»æ•°æ®ï¼Œå¹¶å®ç°äº†é›¶æ ·æœ¬ç¡¬ä»¶è¿ç§»ã€‚\n        *   **æ¶ˆé™¤å¤æ‚ç»„ä»¶**ï¼š**åˆ»æ„é¿å…äº†**æ¥è§¦æ ‡ç­¾ã€é•¿æ—¶å‚è€ƒ/è§‚æµ‹çª—å£ã€å¤–éƒ¨çŠ¶æ€ä¼°è®¡å™¨ã€å¤æ‚çš„å¤šé˜¶æ®µè®­ç»ƒæµç¨‹å’Œå¤§é‡çš„å¥–åŠ±å¡‘å½¢ã€‚\n        *   **å…³æ³¨çœŸå®ç‰©ç†**ï¼šé€šè¿‡æ”¹è¿›çš„**æ‰§è¡Œå™¨å»ºæ¨¡**ï¼ˆç‰¹åˆ«æ˜¯å¹¶è”è¿æ†æœºæ„PLAï¼‰å’ŒåŸºäºæ¨¡å‹æ¨å¯¼çš„**PDå¢ç›Šè°ƒå‚**ï¼Œä½¿ä»¿çœŸæ›´è´´è¿‘çœŸå®ç¡¬ä»¶ï¼Œè¿™æ˜¯é›¶æ ·æœ¬è¿ç§»æˆåŠŸçš„å…³é”®åŸºç¡€ã€‚\n\n### æ–¹å‘äºŒï¼šä»â€œä»¿çœŸæŠ€èƒ½â€åˆ°â€œé›¶æ ·æœ¬çš„ç‰©ç†æœºå™¨äººæŠ€èƒ½â€\n\nè¿™ä¸ªæ–¹å‘å…³æ³¨**è·¨è¶Šä»¿çœŸåˆ°ç°å®é¸¿æ²Ÿ (Sim-to-Real Gap)** çš„å®è·µã€‚\n\n1.  **æ—©æœŸä»¿çœŸåˆ°ç°å®çš„å°è¯•**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼š`VideoMimic`ï¼ˆåˆ©ç”¨æ—¥å¸¸è§†é¢‘è®­ç»ƒå…¨èº«ç­–ç•¥ï¼‰ã€`ASAP`ï¼ˆé€šè¿‡â€œçœŸå®-ä»¿çœŸ-çœŸå®â€ç®¡é“å’Œå¢é‡åŠ¨ä½œæ¨¡å‹å‡å°é¸¿æ²Ÿï¼‰ã€`KungfuBot`ï¼ˆé€šè¿‡å¤šæ­¥è¿åŠ¨å¤„ç†å®ç°åŠ¨æ€è¿åŠ¨ï¼‰ã€‚\n    *   **æ”¹è¿›**ï¼šå¼€å§‹åˆ©ç”¨**æ›´æ˜“è·å–çš„æ•°æ®æºï¼ˆå¦‚è§†é¢‘ï¼‰**ï¼Œå¹¶è®¾è®¡ä¸“é—¨çš„æµç¨‹ï¼ˆå¦‚è¿åŠ¨åå¤„ç†ã€å¢é‡å­¦ä¹ ï¼‰æ¥åº”å¯¹æ•°æ®å™ªå£°å’Œæ¨¡å‹ä¸åŒ¹é…ã€‚\n    *   **å±€é™**ï¼š**æµç¨‹å¤æ‚ã€å¤šé˜¶æ®µã€ä¸”å¸¸é’ˆå¯¹ç‰¹å®šæœºå™¨äººæˆ–è¡Œä¸º**ã€‚ä¾èµ–äºæ‰‹å·¥ç»„ä»¶ï¼Œå¦‚æ¥è§¦æ ‡æ³¨ã€ç‰¹å®šçš„å¥–åŠ±é¡¹ï¼Œå¯æ‰©å±•æ€§å’Œé€šç”¨æ€§å—é™ã€‚\n\n2.  **åŸºäºä¼˜åŒ–è½¨è¿¹çš„æ¨¡ä»¿**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼š`Opt-Mimic`ã€`DTC`ï¼ˆè®­ç»ƒç­–ç•¥æ¥æ¨¡ä»¿ä¼˜åŒ–åçš„è§„åˆ’è½¨è¿¹ï¼‰ã€‚\n    *   **æ”¹è¿›**ï¼šç»“åˆäº†è½¨è¿¹ä¼˜åŒ–çš„**ç²¾ç¡®æ€§**å’ŒRLè·Ÿè¸ªæ§åˆ¶å™¨çš„**é²æ£’æ€§**ã€‚\n    *   **å±€é™**ï¼šä»ç„¶éœ€è¦**ç”Ÿæˆé«˜è´¨é‡çš„ä¼˜åŒ–è½¨è¿¹ä½œä¸ºå‚è€ƒ**ï¼Œè¿™æœ¬èº«å¯èƒ½å°±æ˜¯ä¸€ä¸ªè®¡ç®—å¯†é›†å‹æˆ–éœ€è¦é¢†åŸŸçŸ¥è¯†çš„è¿‡ç¨‹ã€‚\n\n3.  **å¤§è§„æ¨¡æ¨¡ä»¿ä¸å¸ˆç”Ÿè’¸é¦**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼š`OmniH2O`ï¼ˆå°†è¿åŠ¨å­¦å§¿æ€ä½œä¸ºé€šç”¨æ¥å£ï¼‰ã€`HOVER`ï¼ˆé€šè¿‡æ©ç å‘½ä»¤ç©ºé—´è’¸é¦æ•™å¸ˆç­–ç•¥ï¼‰ã€`GMT`ï¼ˆè‡ªé€‚åº”é‡‡æ · + æ··åˆä¸“å®¶æ•™å¸ˆï¼‰ã€‚\n    *   **æ”¹è¿›**ï¼šé€šè¿‡**åœ¨å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„è¿åŠ¨æ•°æ®é›†ä¸Šé¢„è®­ç»ƒâ€œæ•™å¸ˆâ€ç­–ç•¥**ï¼Œç„¶åè’¸é¦åˆ°ä»…ä½¿ç”¨æœ¬ä½“æ„ŸçŸ¥çš„â€œå­¦ç”Ÿâ€ç­–ç•¥ï¼Œå®ç°äº†**é«˜æ€§èƒ½ã€å¤šæ¨¡æ€çš„ç»Ÿä¸€æ§åˆ¶**ã€‚\n    *   **å±€é™**ï¼šè™½ç„¶æ€§èƒ½å¼ºå¤§ï¼Œä½†**ç®¡é“é€šå¸¸æ¶‰åŠå¤šé˜¶æ®µè®­ç»ƒå’Œå¤æ‚çš„ç½‘ç»œæ¶æ„**ï¼ˆå¦‚æ··åˆä¸“å®¶ï¼‰ï¼Œä¸”å¯¹ç‰¹å®šå¹³å°çš„æ•°æ®é›†ä¾èµ–è¾ƒå¼ºã€‚\n\n4.  **ZESTçš„è´¡çŒ® (æœ¬å·¥ä½œ)**\n    *   **æ”¹è¿›ä¸æ•´åˆ**ï¼šZESTè¿½æ±‚**æç®€ä¸é€šç”¨**ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå¤§å¹…ç®€åŒ–äº†æµç¨‹ã€‚\n        *   **å•é˜¶æ®µã€æœ€å°åŒ–é…æ–¹**ï¼šä½¿ç”¨ç®€å•çš„**å‰é¦ˆç½‘ç»œ**ï¼Œåœ¨**å•ä¸€é˜¶æ®µ**å†…ä»å¼‚æ„æ•°æ®è®­ç»ƒç­–ç•¥ï¼Œ**è¶…å‚æ•°é›†åœ¨æ¯ç±»æœºå™¨äººä¸Šå›ºå®š**ã€‚\n        *   **åˆ›æ–°çš„è®­ç»ƒæŠ€æœ¯**ï¼š\n            *   **è‡ªé€‚åº”å‚è€ƒçŠ¶æ€åˆå§‹åŒ–**ï¼šæ ¹æ®å¤±è´¥ç‡åç½®é‡‡æ ·ï¼Œä¸“æ³¨äºéš¾å­¦çš„è¿åŠ¨ç‰‡æ®µï¼Œè§£å†³äº†é•¿æ—¶ç¨‹ã€å¤šæŠ€èƒ½å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚\n            *   **è‡ªåŠ¨è¾…åŠ©åŠ›çŸ©è¯¾ç¨‹**ï¼šå¯¹é«˜åŠ¨æ€è¡Œä¸ºï¼ˆå¦‚ç©ºç¿»ï¼‰æ–½åŠ åŸºäºæ¨¡å‹çš„è™šæ‹Ÿè¾…åŠ©åŠ›çŸ©ï¼Œå¹¶éšæ€§èƒ½æå‡è‡ªåŠ¨è¡°å‡ï¼Œç¨³å®šäº†è®­ç»ƒï¼Œæ— éœ€æ‰‹å·¥è®¾è®¡è¯¾ç¨‹ã€‚\n        *   **è·¨å¹³å°éªŒè¯**ï¼š**åŒä¸€å¥—æ–¹æ³•**åœ¨**å…¨å°ºå¯¸äººå½¢Atlas**ã€**å°å‹äººå½¢Unitree G1**å’Œ**å››è¶³Spot**ä¸Šå‡éªŒè¯æˆåŠŸï¼Œå±•ç¤ºäº†å‰æ‰€æœªæœ‰çš„**è·¨å½¢æ€é€šç”¨æ€§**ã€‚ç‰¹åˆ«æ˜¯åœ¨Atlasä¸Šå®ç°äº†**é¦–ä¸ªåŠ¨æ€å¤šæ¥è§¦è¡Œä¸º**ï¼ˆå¦‚é™†å†›çˆ¬è¡Œã€éœ¹é›³èˆï¼‰ï¼Œåœ¨G1ä¸Šå®ç°äº†**é¦–ä¸ªä»è§†é¢‘åˆ°å¤æ‚åŠ¨æ€æŠ€èƒ½**ï¼ˆå¦‚è·³èˆã€çˆ¬ç®±ï¼‰çš„è¿ç§»ã€‚\n\n### æ–¹å‘ä¸‰ï¼šä»â€œç‰¹å®šå½¢æ€æ§åˆ¶â€åˆ°â€œé€šç”¨è¿åŠ¨è¡¨ç¤ºä¸æ¥å£â€\n\nè¿™ä¸ªæ–¹å‘å…³æ³¨**æ§åˆ¶ç­–ç•¥ä¸æœºå™¨äººå½¢æ€ã€ä»»åŠ¡æŒ‡ä»¤ä¹‹é—´çš„è§£è€¦**ã€‚\n\n1.  **å½¢æ€ç‰¹å®šçš„æ§åˆ¶å™¨**\n    *   **æ—©æœŸå¤§å¤šæ•°å·¥ä½œ**ï¼šæ§åˆ¶å™¨å’Œç­–ç•¥é€šå¸¸ä¸º**ç‰¹å®šæœºå™¨äººå¹³å°ï¼ˆç”šè‡³ç‰¹å®šè¡Œä¸ºï¼‰** é‡èº«å®šåˆ¶ï¼Œè°ƒæ•´å‚æ•°å’Œå¥–åŠ±ç»“æ„æ— æ³•ç›´æ¥è¿ç§»ã€‚\n\n2.  **è¿åŠ¨é‡å®šå‘ä¸é€šç”¨æ¥å£**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼š`DOC`ï¼ˆå°†è¿åŠ¨é‡å®šå‘åˆ°è…¿å¼æœºå™¨äººï¼‰ã€`OmniH2O`ï¼ˆå§¿æ€ä½œä¸ºæ¥å£ï¼‰ã€`MaskedMimic`ï¼ˆæ©ç è¿åŠ¨ä¿®å¤ä½œä¸ºé€šç”¨æ§åˆ¶æ¥å£ï¼‰ã€‚\n    *   **æ”¹è¿›**ï¼šæ¢ç´¢å°†**è¿åŠ¨å­¦ä¿¡æ¯**ä½œä¸ºä¸€ç§ä¸åº•å±‚æ§åˆ¶å™¨è§£è€¦çš„**é€šç”¨æ¥å£**ã€‚å…è®¸é«˜çº§è§„åˆ’æˆ–ç”¨æˆ·æŒ‡ä»¤ï¼ˆå¦‚å…³é”®å¸§ã€è¯­è¨€ï¼‰ç”Ÿæˆè¿åŠ¨ç›®æ ‡ï¼Œç”±åº•å±‚é€šç”¨è·Ÿè¸ªå™¨æ‰§è¡Œã€‚\n    *   **å±€é™**ï¼šå¦‚ä½•è®¾è®¡ä¸€ä¸ª**æ—¢é€šç”¨åˆé«˜æ€§èƒ½ã€ä¸”èƒ½å¤„ç†å¤æ‚ç‰©ç†äº¤äº’**çš„åº•å±‚è·Ÿè¸ªå™¨ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚\n\n3.  **ZESTçš„è´¡çŒ® (æœ¬å·¥ä½œ)**\n    *   **æ”¹è¿›ä¸æ•´åˆ**ï¼šZESTæœ¬èº«æ˜¯ä¸€ä¸ªå¼ºå¤§çš„**åº•å±‚é€šç”¨è¿åŠ¨è·Ÿè¸ªå™¨**ï¼Œä¸ºæ„å»ºæ›´é«˜çº§çš„é€šç”¨æ¥å£å¥ å®šäº†åŸºç¡€ã€‚\n        *   **ä½œä¸ºç†æƒ³çš„â€œæ‰§è¡Œå±‚â€**ï¼šZESTè¯æ˜äº†ï¼Œä¸€ä¸ªè®­ç»ƒè‰¯å¥½çš„ç­–ç•¥å¯ä»¥**ä»…å‡­ä¸‹ä¸€æ—¶åˆ»çš„å‚è€ƒçŠ¶æ€å’Œå½“å‰æœ¬ä½“æ„ŸçŸ¥**ï¼Œé²æ£’åœ°è·Ÿè¸ªæ¥è‡ªä»»ä½•æ¥æºçš„å¤æ‚è¿åŠ¨ã€‚è¿™ä½¿å…¶éå¸¸é€‚åˆä½œä¸º**åˆ†å±‚æ¶æ„ä¸­çš„åº•å±‚æ‰§è¡Œå™¨**ã€‚\n        *   **ä¸ºæœªæ¥æ¥å£é“ºè·¯**ï¼šè®ºæ–‡åœ¨è®¨è®ºä¸­æŒ‡å‡ºï¼Œæœªæ¥æ–¹å‘åŒ…æ‹¬å°†ZESTä¸**ç´§å‡‘è¿åŠ¨åµŒå…¥**ã€**ç¨€ç–äººç±»å¯è§£é‡Šè¾“å…¥**ï¼ˆå…³é”®å¸§ã€è¯­è¨€æŒ‡ä»¤ï¼‰ä»¥åŠ**é«˜çº§ç”Ÿæˆå¼è§„åˆ’å™¨**ç»“åˆã€‚ZESTçš„æˆåŠŸæ„å‘³ç€ï¼Œåªè¦é«˜çº§æ¨¡å—èƒ½äº§ç”Ÿåˆç†çš„è¿åŠ¨å‚è€ƒï¼Œå®ƒå°±èƒ½å¯é åœ°æ‰§è¡Œï¼Œä»è€Œ**é—­ç¯è¡Œä¸ºåˆæˆä¸æ§åˆ¶**ã€‚\n\n### æ€»ç»“ä¸å‘å±•è„‰ç»œå›¾\n\n```\nå‘å±•ä¸»çº¿ï¼šæ§åˆ¶èŒƒå¼æ¼”è¿› + Sim-to-Realå®è·µ + æ¥å£é€šç”¨åŒ–\n|\n|-- [èŒƒå¼] æ¨¡å‹åŸºç¦»çº¿è§„åˆ’+åœ¨çº¿è·Ÿè¸ª (Atlasç»å…¸æ§åˆ¶)\n|   |-- ä¼˜ç‚¹ï¼šå¯è§£é‡Šï¼Œé€šç”¨\n|   `-- ç“¶é¢ˆï¼šä¾èµ–ç²¾ç¡®æ¨¡å‹ï¼Œéš¾ä»¥å¤„ç†å¤æ‚æ¥è§¦/ä¸ç¡®å®šæ€§\n|\n|-- [èŒƒå¼] æ•°æ®é©±åŠ¨RL (å››è¶³è·‘é…·ã€çµå·§æ‰‹",
            "timestamp": "2026-02-03T16:03:43.936843",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## ZEST è®ºæ–‡åˆ†æï¼šä¸å‰ä½œçš„å¯¹æ¯”åŠä¸»è¦æ”¹è¿›\n\nZEST å»ºç«‹åœ¨ä½œè€…å›¢é˜ŸåŠé¢†åŸŸå†…ä¸€ç³»åˆ—å‰æœŸå·¥ä½œçš„åŸºç¡€ä¸Šï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº**æ•´åˆã€ç®€åŒ–å¹¶æ³›åŒ–**äº†ä»å¼‚æ„æ•°æ®æºåˆ°æœºå™¨äººç¡¬ä»¶é›¶æ ·æœ¬éƒ¨ç½²çš„è¿åŠ¨æ¨¡ä»¿æµç¨‹ã€‚ä»¥ä¸‹è¡¨æ ¼è¯¦ç»†æ¢³ç†äº†å…¶å…³é”®å‰ä½œï¼Œå¹¶é‡ç‚¹å¯¹æ¯”äº† ZEST çš„å…·ä½“æ”¹è¿›ã€‚\n\n### å‰ä½œæ¢³ç†ä¸å¯¹æ¯”åˆ†æ\n\n| å‰ä½œåç§° / ç±»åˆ« | æ ¸å¿ƒæ–¹æ³• / è´¡çŒ® | ä¸ ZEST çš„ä¸»è¦åŒºåˆ« | ZEST çš„ä¸»è¦æ”¹è¿› / å…·ä½“æ”¹åŠ¨ |\n| :--- | :--- | :--- | :--- |\n| **DeepMimic (Peng et al., 2018)** | **å¼€åˆ›æ€§å·¥ä½œ**ï¼šåœ¨è®¡ç®—æœºå›¾å½¢å­¦ä¸­ï¼Œä½¿ç”¨ MoCap æ•°æ®ä½œä¸ºå…ˆéªŒï¼Œé€šè¿‡å•ä¸€å¥–åŠ±ç»“æ„è®­ç»ƒç­–ç•¥æ¨¡ä»¿è¿åŠ¨ç‰‡æ®µï¼Œå¼•å¯¼ç­–ç•¥äº§ç”Ÿè‡ªç„¶è¿åŠ¨ã€‚ | 1. **é¢†åŸŸ**ï¼šä¸“æ³¨äºæ¨¡æ‹Ÿè§’è‰²æ§åˆ¶ï¼Œæœªæ¶‰åŠçœŸå®æœºå™¨äººéƒ¨ç½²ã€‚<br>2. **æ•°æ®æº**ï¼šä»…ä½¿ç”¨é«˜è´¨é‡ MoCapã€‚<br>3. **è®­ç»ƒ**ï¼šé‡‡ç”¨å‡åŒ€çš„å‚è€ƒçŠ¶æ€åˆå§‹åŒ–ï¼ˆRSIï¼‰ï¼Œæœªé’ˆå¯¹é•¿åºåˆ—æˆ–éš¾æ ·æœ¬ä¼˜åŒ–ã€‚ | 1. **è·¨é¢†åŸŸåˆ°çœŸå®æœºå™¨äºº**ï¼šå°†æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ç³»ç»Ÿæ€§åœ°åº”ç”¨äº Atlasã€G1ã€Spot ç­‰å¤šæ¬¾çœŸå®æœºå™¨äººï¼Œå¹¶å®ç°**é›¶æ ·æœ¬ç¡¬ä»¶éƒ¨ç½²**ã€‚<br>2. **æ•°æ®æºæ³›åŒ–**ï¼šå¤„ç† **MoCapã€ViCapï¼ˆå•ç›®è§†é¢‘ï¼‰ã€åŠ¨ç”»** ä¸‰ç§å¼‚æ„ä¸”è´¨é‡ä¸ä¸€çš„æ•°æ®æºï¼Œè¯æ˜äº†æ¡†æ¶å¯¹æ•°æ®å™ªå£°å’Œç‰©ç†ä¸å¯è¡Œæ€§çš„é²æ£’æ€§ã€‚<br>3. **è‡ªé€‚åº”é‡‡æ ·**ï¼šå¼•å…¥**åŸºäºåˆ†æ®µçš„å¤±è´¥æ°´å¹³è¯„ä¼°å’Œåˆ†ç±»é‡‡æ ·å™¨**ï¼Œä¸»åŠ¨å°†è®­ç»ƒé‡å¿ƒé›†ä¸­åœ¨éš¾ä»¥å­¦ä¹ çš„è¿åŠ¨ç‰‡æ®µä¸Šï¼Œè§£å†³äº†é•¿æ—¶ç¨‹æŠ€èƒ½å­¦ä¹ ä¸­çš„æ ·æœ¬æ•ˆç‡é—®é¢˜ã€‚ |\n| **AMP / ASE (Peng et al., 2021, 2022)** | **å¯¹æŠ—æ¨¡ä»¿å­¦ä¹ **ï¼šç”¨åˆ¤åˆ«å™¨æ›¿ä»£ç›´æ¥æ¨¡ä»¿å¥–åŠ±ï¼Œé€šè¿‡åŒ¹é…ç¤ºèŒƒæ•°æ®çš„çŠ¶æ€è½¬ç§»åˆ†å¸ƒæ¥é¼“åŠ±è‡ªç„¶è¿åŠ¨é£æ ¼ã€‚å¼•å…¥äº†æŠ€èƒ½åµŒå…¥ä»¥æé«˜å¯é‡ç”¨æ€§å’Œå¯æ§æ€§ã€‚ | 1. **å¥–åŠ±æœºåˆ¶**ï¼šä¾èµ–å¯¹æŠ—è®­ç»ƒï¼Œéœ€è¦è®­ç»ƒé¢å¤–çš„åˆ¤åˆ«å™¨ç½‘ç»œï¼Œå¢åŠ äº†å¤æ‚åº¦ã€‚<br>2. **éƒ¨ç½²ç„¦ç‚¹**ï¼šä¸»è¦ç›®æ ‡æ˜¯åœ¨æ¨¡æ‹Ÿä¸­ç”Ÿæˆå¤šæ ·ã€é£æ ¼åŒ–çš„è¿åŠ¨ï¼Œå¯¹ sim-to-real çš„æŒ‘æˆ˜ï¼ˆå¦‚æ‰§è¡Œå™¨æé™ã€çŠ¶æ€ä¼°è®¡ï¼‰è€ƒè™‘è¾ƒå°‘ã€‚ | 1. **ç®€åŒ–çš„å¥–åŠ±è®¾è®¡**ï¼šæ‘’å¼ƒäº†å¯¹æŠ—æ€§å¥–åŠ±ï¼Œå›å½’ä½¿ç”¨**å•ä¸€çš„ã€é€šç”¨çš„ã€åŸºäºæŒ‡æ•°çš„è·Ÿè¸ªå¥–åŠ±å‡½æ•°**ã€‚è¯¥å¥–åŠ±ä¸ä¾èµ–æ¥è§¦æ ‡ç­¾ï¼Œé¿å…äº†å¯¹æŠ—è®­ç»ƒçš„ä¸ç¨³å®šæ€§å’Œæ¨¡å¼å´©æºƒé—®é¢˜ï¼Œä½¿è®­ç»ƒæµç¨‹æ›´ç¨³å®šã€æ˜“å¤ç°ã€‚<br>2. **é¢å‘ç¡¬ä»¶çš„è®¾è®¡**ï¼šç­–ç•¥è§‚æµ‹**ä»…åŒ…å«æœ¬ä½“æ„ŸçŸ¥å’Œä¸‹ä¸€æ—¶åˆ»å‚è€ƒ**ï¼Œæ— éœ€çŠ¶æ€ä¼°è®¡å™¨æˆ–è§‚æµ‹å†å²çª—å£ã€‚åŠ¨ä½œè¾“å‡ºä¸º**æ®‹å·®å…³èŠ‚ç›®æ ‡**ï¼Œé™ä½äº†å­¦ä¹ éš¾åº¦ï¼Œå¹¶åˆ©ç”¨è°ƒè°çš„ PD æ§åˆ¶å™¨ä¿è¯åŸºç¡€ç¨³å®šæ€§ã€‚ |\n| **VideoMimic (Allshire et al., 2025)** | **è§†é¢‘åˆ°è¡Œä¸º**ï¼šé€šè¿‡â€œçœŸå®-æ¨¡æ‹Ÿ-çœŸå®â€æµç¨‹ï¼Œåˆ©ç”¨æ—¥å¸¸è§†é¢‘è®­ç»ƒå…¨èº«ç­–ç•¥æ‰§è¡Œä¸Šä¸‹æ–‡æŠ€èƒ½ã€‚ | 1. **æŠ€èƒ½å¤æ‚åº¦**ï¼šä¸»è¦å…³æ³¨**ç±» locomotion çš„è¡Œä¸º**ï¼ˆå¦‚èµ°è·¯ã€æ¬ä¸œè¥¿ï¼‰ã€‚<br>2. **æµç¨‹**ï¼šå¯èƒ½æ¶‰åŠå¤šé˜¶æ®µæˆ–ç‰¹å®šäºä»»åŠ¡çš„ç»„ä»¶ã€‚ | 1. **æŠ€èƒ½å¤æ‚åº¦æå‡**ï¼šå®ç°äº†ä»è§†é¢‘åˆ°**åŠ¨æ€ã€å¤šæ¥è§¦ã€åœºæ™¯äº¤äº’æŠ€èƒ½**çš„é›¶æ ·æœ¬è½¬ç§»ï¼Œä¾‹å¦‚ **G1 æœºå™¨äººçš„ç›’å­æ”€çˆ¬**ã€‚è¿™æ¶‰åŠé—´æ­‡æ€§å…¨èº«æ¥è§¦å’Œä¸ç¯å¢ƒçš„å¼ºäº¤äº’ï¼Œè¿œæ¯”å•çº¯ locomotion å¤æ‚ã€‚<br>2. **æµç¨‹ç»Ÿä¸€ä¸ç®€åŒ–**ï¼šæå‡ºäº†ä¸€ä¸ª**ç»Ÿä¸€çš„ã€å•é˜¶æ®µçš„è®­ç»ƒæµç¨‹**ï¼Œé¿å…äº† VideoMimic å¯èƒ½éœ€è¦çš„å¤æ‚å¤šé˜¶æ®µ pipeline æˆ–é’ˆå¯¹è§†é¢‘æ•°æ®çš„ç‰¹æ®Šå¤„ç†æ¨¡å—ã€‚ |\n| **ASAP (He et al., 2025a)** | **æ¨¡æ‹Ÿä¸ç‰©ç†å¯¹é½**ï¼šé€šè¿‡ä»äººç±»è§†é¢‘é¢„è®­ç»ƒè¿åŠ¨è·Ÿè¸ªç­–ç•¥ï¼Œåœ¨ç¡¬ä»¶ä¸Šæ”¶é›†è½¨è¿¹ï¼Œå¹¶å­¦ä¹ ä¸€ä¸ªâ€œå¢é‡åŠ¨ä½œâ€æ¨¡å‹æ¥å‡å° sim-to-real å·®è·ã€‚ | 1. **æ–¹æ³•**ï¼šé‡‡ç”¨**ä¸¤é˜¶æ®µ**æ–¹æ³•ï¼šå…ˆæ¨¡ä»¿ï¼Œå†é€šè¿‡çœŸå®æ•°æ®å¾®è°ƒä¸€ä¸ªåŠ¨åŠ›å­¦æ¨¡å‹ã€‚<br>2. **ä¾èµ–**ï¼šéœ€è¦åœ¨å®é™…æœºå™¨äººä¸Šæ”¶é›†æ•°æ®æ¥å¼¥åˆå·®è·ã€‚ | 1. **ç«¯åˆ°ç«¯é›¶æ ·æœ¬**ï¼š**å®Œå…¨åœ¨ä»¿çœŸä¸­è®­ç»ƒ**ï¼Œä»…ä½¿ç”¨é€‚åº¦çš„åŸŸéšæœºåŒ–ï¼Œç„¶åç›´æ¥é›¶æ ·æœ¬éƒ¨ç½²ã€‚æ— éœ€åœ¨çœŸå®æœºå™¨äººä¸Šæ”¶é›†ä»»ä½•äº¤äº’æ•°æ®æˆ–è®­ç»ƒé¢å¤–çš„è¡¥å¿æ¨¡å‹ã€‚<br>2. **è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ **ï¼šå¼•å…¥äº†**åŸºäºæ¨¡å‹çš„è¾…åŠ©åŠ›çŸ©è‡ªåŠ¨è¯¾ç¨‹**ã€‚è¯¥è™šæ‹ŸåŠ›çŸ©ä½œç”¨äºæœºå™¨äººèº¯å¹²ï¼Œå…¶å¤§å°æ ¹æ®åˆ†æ®µéš¾åº¦è‡ªåŠ¨è°ƒæ•´å¹¶è¡°å‡ï¼Œç¨³å®šäº†é«˜åŠ¨æ€åŠ¨ä½œï¼ˆå¦‚ç©ºç¿»ï¼‰çš„è®­ç»ƒï¼Œæ›¿ä»£äº†æ‰‹å·¥è®¾è®¡çš„è¯¾ç¨‹ã€‚ |\n| **GMT (Chen et al., 2025)** | **é€šç”¨è¿åŠ¨è·Ÿè¸ª**ï¼šç»“åˆè‡ªé€‚åº”é‡‡æ ·å’Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ•™å¸ˆæ¨¡å‹ï¼Œæç‚¼å‡ºèƒ½è·Ÿè¸ªå¹¿æ³›è¿åŠ¨çš„å­¦ç”Ÿç­–ç•¥ã€‚ | 1. **æ¶æ„**ï¼šé‡‡ç”¨**æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦**æ¡†æ¶ï¼Œç»“æ„ç›¸å¯¹å¤æ‚ã€‚<br>2. **æ³›åŒ–**ï¼šä¸“æ³¨äºåœ¨å·²è§è¿åŠ¨ä¸Šçš„è·Ÿè¸ªæ€§èƒ½ã€‚ | 1. **æç®€æ¶æ„**ï¼šç­–ç•¥æ˜¯ä¸€ä¸ªç®€å•çš„**å‰é¦ˆç¥ç»ç½‘ç»œ**ï¼Œè¾“å…¥ä»…ä¸ºå½“å‰æœ¬ä½“æ„ŸçŸ¥å’Œä¸‹ä¸€å‚è€ƒçŠ¶æ€ã€‚å»é™¤äº†å¤æ‚çš„ MoE æ•™å¸ˆã€è’¸é¦è¿‡ç¨‹å’Œå†å²è§‚æµ‹çª—å£ã€‚<br>2. **å»ºæ¨¡æ”¹è¿›**ï¼šæä¾›äº†é’ˆå¯¹**å¹¶è”è¿æ†æ‰§è¡Œå™¨ï¼ˆPLAï¼‰** çš„æ¸è¿›å¼ç®€åŒ–å»ºæ¨¡æ–¹æ³•ï¼Œå¹¶ä»è¿‘ä¼¼è§£æçš„è‡‚å€¼ä¸­æ¨å¯¼å‡ºå…³èŠ‚çº§ PD å¢ç›Šï¼Œè¿™æ˜¾è‘—æå‡äº†ä»¿çœŸå»ºæ¨¡ç²¾åº¦ï¼Œæ˜¯é›¶æ ·æœ¬è½¬ç§»æˆåŠŸçš„å…³é”®åŸºç¡€ã€‚ |\n| **Opt-Mimic / DTC (Fuchioka et al., 2023; Jenelten et al., 2024)** | **æ¨¡ä»¿ä¼˜åŒ–è½¨è¿¹**ï¼šè®­ç»ƒç­–ç•¥æ¥æ¨¡ä»¿åŸºäºæ¨¡å‹ä¼˜åŒ–å™¨ç”Ÿæˆçš„è½¨è¿¹ï¼Œç»“åˆäº†ä¼˜åŒ–çš„ç²¾ç¡®æ€§å’ŒRLçš„é²æ£’æ€§ã€‚ | 1. **å‚è€ƒæ¥æº**ï¼šä¾èµ–**æ¨¡å‹ä¼˜åŒ–å™¨**ç”Ÿæˆå‚è€ƒè½¨è¿¹ï¼Œè¿™æœ¬èº«æ˜¯ä¸€ä¸ªè®¡ç®—å¯†é›†å‹ä¸”éœ€è¦æ¥è§¦è§„åˆ’çš„è¿‡ç¨‹ã€‚<br>2. **çµæ´»æ€§**ï¼šå‚è€ƒè½¨è¿¹æ˜¯ç¡®å®šæ€§çš„ï¼Œç­–ç•¥å¯¹æ–°æƒ…å†µçš„æ³›åŒ–èƒ½åŠ›å¯èƒ½å—é™ã€‚ | 1. **æ•°æ®æºæ— å…³**ï¼šç›´æ¥ä»**äººç±»æˆ–åŠ¨ç”»æ•°æ®**å­¦ä¹ ï¼Œå®Œå…¨ç»•è¿‡å¤æ‚çš„è½¨è¿¹ä¼˜åŒ–å’Œæ¥è§¦è§„åˆ’æ­¥éª¤ã€‚è¿™å¤§å¤§é™ä½äº†è·å–å¤šæ ·åŒ–æŠ€èƒ½æ•°æ®çš„é—¨æ§›ã€‚<br>2. **å…é™¤æ¥è§¦æ ‡ç­¾**ï¼šZEST çš„å¥–åŠ±å‡½æ•°ä¸ä¾èµ–äºå‚è€ƒè¿åŠ¨ä¸­çš„ä»»ä½•æ¥è§¦æ ‡ç­¾ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªç„¶åœ°å¤„ç†æ‰‹ã€è†ã€èº¯å¹²ç­‰å¤šä½“æ¥è§¦ï¼Œè€ŒåŸºäºä¼˜åŒ–çš„æ–¹æ³•é€šå¸¸éœ€è¦é¢„å®šä¹‰æ¥è§¦åºåˆ—ã€‚ |\n\n### æ€»ç»“ï¼šZEST çš„æ ¸å¿ƒåˆ›æ–°ä¸æ”¹è¿›\n\nZEST å¹¶éä»é›¶å¼€å§‹å‘æ˜æ–°æŠ€æœ¯ï¼Œè€Œæ˜¯é€šè¿‡**æˆ˜ç•¥æ€§æ•´åˆä¸ç³»ç»Ÿæ€§ç®€åŒ–**ï¼Œå®ç°äº†æ€§èƒ½ä¸é€šç”¨æ€§çš„é£è·ƒï¼š\n\n1.  **ç»Ÿä¸€çš„å¼‚æ„æ•°æ®æ¥å£**ï¼šé¦–æ¬¡åœ¨**åŒä¸€æ¡†æ¶å†…**æ— ç¼å¤„ç†é«˜ä¿çœŸ MoCapã€å¸¦å™ªå£°çš„ ViCap å’Œç‰©ç†ä¸å¯è¡Œçš„åŠ¨ç”»æ•°æ®ï¼Œå¹¶å°†å®ƒä»¬é›¶æ ·æœ¬è½¬ç§»åˆ°**ä¸åŒå½¢æ€**ï¼ˆäººå½¢ Atlas/G1ã€å››è¶³ Spotï¼‰çš„æœºå™¨äººä¸Šã€‚\n2.  **æç®€ä¸”é²æ£’çš„è®­ç»ƒé…æ–¹**ï¼š\n    - **ç§»é™¤å¤æ‚ç»„ä»¶**ï¼šæ˜ç¡®é¿å…äº†æ¥è§¦æ ‡ç­¾ã€æœªæ¥å‚è€ƒçª—å£ã€è§‚æµ‹å†å²ã€çŠ¶æ€ä¼°è®¡å™¨ã€å¤šé˜¶æ®µè®­ç»ƒå’Œç²¾ç»†çš„å¥–åŠ±å¡‘å½¢ã€‚\n    - **æ ¸å¿ƒå¢å¼ºæŠ€æœ¯**ï¼šä¿ç•™äº†ä¸¤ä¸ªé«˜æ•ˆç»„ä»¶â€”â€”**è‡ªé€‚åº”åˆ†æ®µé‡‡æ ·**å’Œ**è‡ªåŠ¨è¾…åŠ©åŠ›çŸ©è¯¾ç¨‹**â€”â€”å¹¶ä½¿å…¶è‡ªåŠ¨åŒ–ã€é€šç”¨åŒ–ã€‚\n3.  **é¢å‘ç¡¬ä»¶çš„å»ºæ¨¡ä¸è®¾è®¡**ï¼š\n    - **æ”¹è¿›çš„ PLA å»ºæ¨¡**ï¼šæä¾›äº†ä»ç²¾ç¡®åˆ°å®ç”¨çš„æ¸è¿›å¼æ‰§è¡Œå™¨æ¨¡å‹ï¼Œä¸ºä»¿çœŸä¿çœŸåº¦æ‰“ä¸‹åŸºç¡€ã€‚\n    - **åŸºäºæ¨¡å‹çš„å¢ç›Šè°ƒè°**ï¼šä»æ‰§è¡Œå™¨è‡‚å€¼æ¨å¯¼ PD å¢ç›Šï¼Œå‡å°‘äº† sim-to-real çš„æ§åˆ¶å™¨å‚æ•°å·®è·ã€‚\n    - **æœ€å°åŒ–è§‚æµ‹**ï¼šç­–ç•¥ä»…éœ€å½“å‰æœ¬ä½“æ„Ÿå’Œä¸‹ä¸€ç›®æ ‡ï¼Œæœ€å¤§åŒ–éƒ¨ç½²å¯è¡Œæ€§å’Œé²æ£’æ€§ã€‚\n4.  **å®è¯èƒ½åŠ›çš„çªç ´**ï¼š\n    - åœ¨ **Atlas** ä¸Šå®ç°äº†é¦–ä¸ª**åŠ¨æ€å¤šæ¥è§¦æŠ€èƒ½**ï¼ˆå¦‚é™†å†›çˆ¬è¡Œã€éœ¹é›³èˆï¼‰ã€‚\n    - åœ¨ **G1** ä¸Šå®ç°äº†é¦–ä¸ªä»**è§†é¢‘åˆ°å¤æ‚åœºæ™¯äº¤äº’æŠ€èƒ½**ï¼ˆå¦‚ç›’å­æ”€çˆ¬ï¼‰çš„é›¶æ ·æœ¬è½¬ç§»ã€‚\n    - åœ¨ **Spot** ä¸Šå®ç°äº†ä»åŠ¨ç”»åˆ°é«˜åŠ¨æ€æŠ€èƒ½ï¼ˆå¦‚è¿ç»­åç©ºç¿»ï¼‰çš„è·¨å½¢æ€è½¬ç§»ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼ŒZEST çš„æ ¸å¿ƒä»·å€¼åœ¨äºå°†è¿åŠ¨æ¨¡ä»¿ä»ä¸€é¡¹éœ€è¦å¤§é‡é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€é’ˆå¯¹ç‰¹å®šæœºå™¨äººå’Œæ•°æ®æºè¿›è¡Œç²¾å¿ƒè°ƒä¼˜çš„â€œæ‰‹è‰ºâ€ï¼Œè½¬å˜ä¸ºä¸€ä¸ªæ ‡å‡†åŒ–ã€å¯æ‰©å±•ã€é«˜æ€§èƒ½çš„â€œæµæ°´çº¿â€ï¼Œæå¤§åœ°é™ä½äº†å°†ç”Ÿç‰©è¿åŠ¨è½¬åŒ–ä¸ºæœºå™¨äººé²æ£’æŠ€èƒ½çš„å£å’ã€‚**",
            "timestamp": "2026-02-03T16:04:51.881533",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨æ·±å…¥åˆ†æè®ºæ–‡ã€ŠZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Controlã€‹æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ç‚¹åˆ†æ\n\nZEST çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºäº†ä¸€å¥—**ç»Ÿä¸€ã€ç²¾ç®€ã€å¯æ‰©å±•**çš„è¿åŠ¨æ¨¡ä»¿æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å¤šç§å¼‚æ„æ•°æ®æºï¼ˆåŠ¨æ•ã€è§†é¢‘ã€åŠ¨ç”»ï¼‰å­¦ä¹ ç­–ç•¥ï¼Œå¹¶å®ç°**é›¶æ ·æœ¬**ï¼ˆzero-shotï¼‰åˆ°çœŸå®æœºå™¨äººçš„éƒ¨ç½²ã€‚å…¶æŠ€æœ¯æ–¹æ³•æ—¨åœ¨é¿å…ä¼ ç»Ÿæ–¹æ³•ä¸­çš„å¤æ‚ç»„ä»¶ï¼Œå¦‚æ¥è§¦æ ‡ç­¾ã€å¤æ‚çš„å¥–åŠ±å‡½æ•°è®¾è®¡ã€å¤šé˜¶æ®µè®­ç»ƒã€çŠ¶æ€ä¼°è®¡å™¨ç­‰ã€‚\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†è¯´æ˜äº†å…¶å…³é”®æŠ€æœ¯æ–¹æ³•ã€æ•°å­¦åŸç†åŠå…·ä½“å‚æ•°ï¼š\n\n| æŠ€æœ¯æ¨¡å— | æ ¸å¿ƒæ€æƒ³ä¸ç›®æ ‡ | å…³é”®æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°/å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. ç»Ÿä¸€çš„MDPä¸å¥–åŠ±å‡½æ•°è®¾è®¡** | **ç®€åŒ–è®¾è®¡**ï¼šé¿å…ä»»åŠ¡ç‰¹å®šçš„å¥–åŠ±å·¥ç¨‹å’Œæ¥è§¦æ ‡ç­¾ï¼Œä½¿ç”¨é€šç”¨ã€å¯†é›†çš„å¥–åŠ±ä¿¡å·å¼•å¯¼ç­–ç•¥å­¦ä¹ è‡ªç„¶è¿åŠ¨ã€‚ | **å¥–åŠ±å‡½æ•°æ„æˆ**ï¼š `r_total = r_track + r_reg + r_survival` <br> **1. è·Ÿè¸ªå¥–åŠ± (r_track)**ï¼š é‡‡ç”¨æŒ‡æ•°è¡°å‡æ ¸å‡½æ•°ï¼Œé¼“åŠ±çŠ¶æ€ä¸å‚è€ƒçŠ¶æ€æ¥è¿‘ã€‚å¯¹äºæ¯ä¸ªè·Ÿè¸ªé¡¹ `i` (å¦‚å…³èŠ‚ä½ç½®ã€åŸºåº§å§¿æ€ç­‰)ï¼š <br> `r_track_i = c_t_i * exp( -Îº * ||e_i||^2 / Ïƒ_i^2 )` <br> å…¶ä¸­ `e_i` æ˜¯å½“å‰çŠ¶æ€ä¸å‚è€ƒçŠ¶æ€çš„è¯¯å·®ï¼Œ`c_t_i` æ˜¯æƒé‡ï¼Œ`Îº` æ˜¯åˆšåº¦å‚æ•°ï¼ˆå›ºå®šä¸º 1/4ï¼‰ï¼Œ`Ïƒ_i` æ˜¯å½’ä¸€åŒ–å°ºåº¦ã€‚ <br> **2. æ­£åˆ™åŒ–å¥–åŠ± (r_reg)**ï¼š æƒ©ç½šé¡¹åŒ…æ‹¬åŠ¨ä½œå¹³æ»‘åº¦ (`-||a_t - a_{t-1}||`)ã€å…³èŠ‚åŠ é€Ÿåº¦ã€å…³èŠ‚ä½ç½®/æ‰­çŸ©è¶Šç•Œã€‚ <br> **3. ç”Ÿå­˜å¥–åŠ± (r_survival)**ï¼š æ¯æ­¥ç»™äºˆæ’å®šæ­£å¥–åŠ± (`c_survival = 1`)ï¼Œé¼“åŠ±å»¶é•¿å›åˆã€‚ | **è·Ÿè¸ªé¡¹**ï¼šåŸºåº§ä½ç½®ã€æœå‘ã€çº¿é€Ÿåº¦ã€è§’é€Ÿåº¦ã€å…³èŠ‚ä½ç½®ã€å…³é”®èº«ä½“éƒ¨ä½ä½ç½®å’Œæœå‘ã€‚ <br> **æƒé‡ `c_t_i`**ï¼š å„é¡¹å‡ä¸º 1ã€‚ <br> **å°ºåº¦ `Ïƒ_i`**ï¼š ä¾‹å¦‚ï¼ŒåŸºåº§ä½ç½® 0.4 radï¼Œå…³èŠ‚ä½ç½® `0.3 * sqrt(n_j)`ï¼Œå…³é”®èº«ä½“éƒ¨ä½ä½ç½® `0.2 * sqrt(n_kb)`ã€‚ <br> **æ­£åˆ™åŒ–æƒé‡**ï¼š åŠ¨ä½œå¹³æ»‘åº¦ 0.15ï¼Œå…³èŠ‚åŠ é€Ÿåº¦ 1e-5ï¼Œå…³èŠ‚ä½ç½®é™ä½ 1.0ï¼Œå…³èŠ‚æ‰­çŸ©é™ä½ 0.1ã€‚ |\n| **2. ç²¾ç®€çš„è§‚å¯Ÿä¸åŠ¨ä½œç©ºé—´** | **æœ€å°åŒ–è¾“å…¥**ï¼šç­–ç•¥ä»…ä½¿ç”¨å½“å‰æœ¬ä½“æ„ŸçŸ¥å’Œ**ä¸‹ä¸€æ—¶åˆ»**çš„å‚è€ƒçŠ¶æ€ï¼Œé¿å…è§‚å¯Ÿå†å²çª—å£å’Œæœªæ¥å‚è€ƒçª—å£ï¼Œç®€åŒ–ç­–ç•¥å¹¶æå‡å¯éƒ¨ç½²æ€§ã€‚ | **ç­–ç•¥è§‚å¯Ÿ `o_t = (o_prop, o_ref)`**ï¼š <br> **â€¢ æœ¬ä½“æ„ŸçŸ¥ `o_prop`**ï¼š èº¯å¹²IMUè§’é€Ÿåº¦ `_TÏ‰_IT`ã€é‡åŠ›å‘é‡åœ¨èº¯å¹²ç³»çš„è¡¨ç¤º `_Tg_I`ã€å…³èŠ‚ä½ç½® `q_j`ã€å…³èŠ‚é€Ÿåº¦ `qÌ‡_j`ã€ä¸Šä¸€æ—¶åˆ»åŠ¨ä½œ `a_{t-1}`ã€‚ <br> **â€¢ å‚è€ƒè§‚å¯Ÿ `o_ref`**ï¼š ä¸‹ä¸€ç›®æ ‡æ—¶åˆ»çš„åŸºåº§é«˜åº¦ `_Iğ‘ŸÌ‚_IB^z`ã€åŸºåº§çº¿é€Ÿåº¦ï¼ˆåŸºåº§ç³»ï¼‰`_Bğ‘£Ì‚_IB`ã€åŸºåº§è§’é€Ÿåº¦ï¼ˆåŸºåº§ç³»ï¼‰`_BÏ‰Ì‚_IB`ã€é‡åŠ›å‘é‡ï¼ˆåŸºåº§ç³»ï¼‰`_BgÌ‚_I`ã€å‚è€ƒå…³èŠ‚ä½ç½® `qÌ‚_j`ã€‚ <br> **åŠ¨ä½œç©ºé—´**ï¼š ç­–ç•¥è¾“å‡º**æ®‹å·®åŠ¨ä½œ** `a_t`ï¼Œä¸å‚è€ƒå…³èŠ‚ä½ç½®ç›¸åŠ åä½œä¸ºPDæ§åˆ¶å™¨çš„ç›®æ ‡ï¼š <br> `q_j_cmd = qÌ‚_j + Î£ * a_t` <br> å…¶ä¸­ `Î£` æ˜¯å¯¹è§’åŠ¨ä½œç¼©æ”¾çŸ©é˜µï¼Œæ ¹æ®ä¸åŒå…³èŠ‚çš„è·Ÿè¸ªéš¾åº¦è®¾ç½®ä¸åŒå°ºåº¦ã€‚ | **æ— è§‚å¯Ÿ/å‚è€ƒå†å²çª—å£**ï¼š è®ºæ–‡æ¶ˆèå®éªŒè¡¨æ˜ï¼Œå¢åŠ çª—å£ï¼ˆå¦‚20æ­¥ï¼Œ0.4ç§’ï¼‰ä¼šæŸå®³æ€§èƒ½ã€‚ <br> **åŠ¨ä½œç¼©æ”¾ `Î£`**ï¼š å¯¹éœ€è¦æ›´å¤šè¾…åŠ©çš„å…³èŠ‚ï¼ˆå¦‚é«‹ã€è†ï¼‰ä½¿ç”¨è¾ƒå¤§å°ºåº¦ï¼ˆ0.10, 0.20ï¼‰ï¼Œå¯¹æ˜“äºè·Ÿè¸ªçš„å…³èŠ‚ï¼ˆå¤´ã€è…•ï¼‰ä½¿ç”¨è¾ƒå°å°ºåº¦ï¼ˆ0.05ï¼‰ã€‚ <br> **PDå¢ç›Šè®¾è®¡**ï¼š åŸºäºäºŒé˜¶ç³»ç»Ÿæ¨¡å‹ï¼Œ`K_p = I * Ï‰_n^2`, `K_d = 2 * I * Ï‰_n`ã€‚å…¶ä¸­ `I` ä¸ºå…³èŠ‚æœ‰æ•ˆæƒ¯é‡ï¼ˆæ¥è‡ªPLAæ¨¡å‹ï¼‰ï¼Œ`Ï‰_n` ä¸ºæœŸæœ›è‡ªç„¶é¢‘ç‡ï¼Œåœ¨å“åº”é€Ÿåº¦å’Œé²æ£’æ€§é—´æƒè¡¡ã€‚ |\n| **3. è‡ªé€‚åº”å‚è€ƒçŠ¶æ€åˆå§‹åŒ–ä¸è¯¾ç¨‹å­¦ä¹ ** | **é«˜æ•ˆå­¦ä¹ **ï¼šè§£å†³é•¿æ—¶ç¨‹ã€å¤šæŠ€èƒ½è®­ç»ƒä¸­æ ·æœ¬æ•ˆç‡ä½å’Œç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚é€šè¿‡**éš¾åº¦æ„ŸçŸ¥é‡‡æ ·**å’Œ**æ¨¡å‹è¾…åŠ©åŠ›çŸ©è¯¾ç¨‹**ï¼Œèšç„¦äºéš¾å­¦çš„è¿åŠ¨ç‰‡æ®µã€‚ | **â€¢ è‡ªé€‚åº”RSIé‡‡æ ·**ï¼š <br> 1. **åˆ†æ¡¶**ï¼š å°†æ‰€æœ‰å‚è€ƒè½¨è¿¹æŒ‰å›ºå®šæ—¶é•¿ `Î”` åˆ†æ®µä¸ºæ¡¶ï¼ˆbinï¼‰ã€‚ <br> 2. **è®¡ç®—å¤±è´¥æ°´å¹³**ï¼š æ¯ä¸ªæ¡¶ `b` ç»´æŠ¤ä¸€ä¸ªå¤±è´¥æ°´å¹³ `f_b`ï¼Œé€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°ï¼š`f_b â† (1-Î±)*f_b + Î±*(1 - sÌ„_e)`ã€‚å…¶ä¸­ `sÌ„_e` æ˜¯åŸºäºè·Ÿè¸ªå¥–åŠ±çš„å›åˆå½’ä¸€åŒ–ç›¸ä¼¼åº¦åˆ†æ•°ã€‚ <br> 3. **åŸºäºéš¾åº¦çš„é‡‡æ ·**ï¼š é‡‡æ ·æ¦‚ç‡ `p_b` ä¸ `exp(f_b / Ï„)` æˆæ­£æ¯”ï¼Œå¹¶åŠ å…¥å‡åŒ€åœ°æ¿æ¦‚ç‡ `Îµ` ä»¥é¿å…é—å¿˜ï¼š`p_b âˆ (1-Îµ)*exp(f_b/Ï„) + Îµ/|Î©|`ã€‚ <br> **â€¢ è¾…åŠ©åŠ›çŸ©è‡ªåŠ¨è¯¾ç¨‹**ï¼š <br> 1. **è®¡ç®—è™šæ‹Ÿè¾…åŠ©åŠ›çŸ©**ï¼š åŸºäºæ¨¡å‹è®¡ç®—ä½œç”¨äºåŸºåº§çš„è™šæ‹Ÿç©ºé—´åŠ›çŸ© `[F_b; M_b]`ï¼Œç”¨äºè·Ÿè¸ªå‚è€ƒçš„åŸºåº§å§¿æ€å’Œè§’é€Ÿåº¦ï¼ˆå…¬å¼13a, 13bï¼‰ã€‚ <br> 2. **éš¾åº¦è°ƒåˆ¶ä¸è¡°å‡**ï¼š è¾…åŠ©åŠ›çŸ©ç¼©æ”¾ç³»æ•° `Î²_b = clip(1 - Åœ_b/Î·, 0, Î²_max)`ï¼Œå…¶ä¸­ `Åœ_b = 1 - f_b` ä¸ºå¹³æ»‘ç›¸ä¼¼åº¦ã€‚å½“æ¡¶çš„è·Ÿè¸ªæ€§èƒ½æå‡ï¼ˆ`Åœ_b` å¢åŠ ï¼‰ï¼Œ`Î²_b` è¡°å‡è‡³0ã€‚ | **åˆ†æ¡¶å®½åº¦ `Î”`**ï¼š `min(4.0s, min_i(D_i))`ã€‚ <br> **EMAå¹³æ»‘å› å­ `Î±`**ï¼š 0.005ã€‚ <br> **é‡‡æ ·æ¸©åº¦ `Ï„`**ï¼š `Ï„_base / log(1+|Î©|)`ï¼Œ`Ï„_base=1.0`ã€‚ <br> **åœ°æ¿æ¦‚ç‡ `Îµ`**ï¼š 0.15ã€‚ <br> **æœ€å¤§è¾…åŠ©ç³»æ•° `Î²_max`**ï¼š 0.60ã€‚ <br> **ç›¸ä¼¼åº¦é˜ˆå€¼ `Î·`**ï¼š 0.80ã€‚ <br> **è™šæ‹ŸPDå¢ç›Š**ï¼š `(k_p^Ï‰, k_d^Ï‰) = (200.0, 10.0)`, `(k_p^v, k_d^v) = (0.0, 10.0)`ã€‚ |\n| **4. é«˜ä¿çœŸå¹¶è”è¿æ†æ‰§è¡Œå™¨å»ºæ¨¡** | **ç¼©å°ä»¿çœŸ-ç°å®å·®è·**ï¼šä¸ºåŒ…å«å°é—­è¿åŠ¨é“¾ï¼ˆå¦‚è†ã€è¸å…³èŠ‚PLAï¼‰çš„æœºå™¨äººæä¾›ç²¾ç¡®ä¸”é«˜æ•ˆçš„ä»¿çœŸæ¨¡å‹ï¼Œå¹¶ä¸ºPDå¢ç›Šè°ƒå‚æä¾›ç†è®ºä¾æ®ã€‚ | **æ ¸å¿ƒé—®é¢˜**ï¼š PLAçš„å°é—­è¿åŠ¨é“¾å¯¼è‡´ä»¿çœŸè®¡ç®—æ˜‚è´µï¼Œä¸”å…¶æœ‰æ•ˆæƒ¯é‡å’Œæ‰­çŸ©é™ä½éšæ„å‹å˜åŒ–ã€‚ <br> **æ¸è¿›å¼å»ºæ¨¡æ–¹æ³•**ï¼ˆä»¥å››è¿æ†ä¸ºä¾‹ï¼‰ï¼š <br> 1. **æŠ•å½±æ¨¡å‹**ï¼š å°†æ”¯æ’‘é“¾åŠ¨åŠ›å­¦æŠ•å½±åˆ°ä¸»è¿åŠ¨é“¾ã€‚å‡è®¾æ”¯æ’‘é“¾è´¨é‡ä¸ºé›¶ï¼Œä»…ä¿ç•™ç”µæœºè½¬å­æƒ¯é‡ã€‚æ¨å¯¼å‡ºç­‰æ•ˆæƒ¯é‡çŸ©é˜µ `M_o(q_o) = Î“_i(q_o)^T * I_i * Î“_i(q_o)`ï¼Œå…¶ä¸­ `Î“_i` ä¸ºä¼ åŠ¨é›…å¯æ¯”çŸ©é˜µï¼Œ`I_i` ä¸ºç”µæœºè½¬å­æƒ¯é‡ã€‚ <br> 2. **åŠ¨æ€/åä¹‰æƒ¯é‡æ¨¡å‹**ï¼š ä¸ºå…¼å®¹ä»¿çœŸå™¨ï¼ˆä»…æ”¯æŒå¯¹è§’æƒ¯é‡ï¼‰ï¼Œå¯¹ `M_o` è¿›è¡Œé›…å¯æ¯”è¿‘ä¼¼ï¼Œåˆ†è§£ä¸ºå¯¹è§’éƒ¨åˆ† `D_o` å’Œå‰©ä½™çš„éå¯¹è§’éƒ¨åˆ† `O_o`ã€‚éå¯¹è§’éƒ¨åˆ†ä½œä¸ºè™šæ„æ‰­çŸ©æ–½åŠ ï¼š`Ï„_fictitious â‰ˆ O_o * qÌˆ_o'`ï¼ˆä½¿ç”¨ä¸Šä¸€æ—¶åˆ»åŠ é€Ÿåº¦ï¼‰ã€‚åä¹‰æ¨¡å‹åˆ™åœ¨å•ä¸€æ ‡ç§°æ„å‹ä¸‹è®¡ç®—å¹¶å›ºå®š `DÌ„_o` å’Œ `OÌ„_o`ã€‚ <br> 3. **æ‰­çŸ©é™ä½å»ºæ¨¡**ï¼š è¾“å‡ºæ‰­çŸ©é™ä½ `Ï„_o_lim` ç”±ç”µæœºè¾“å…¥æ‰­çŸ©é™ä½ `Ï„_i_lim` é€šè¿‡ä¼ åŠ¨é›…å¯æ¯”æ˜ å°„ï¼š`Ï„_o_lim = Î“_i(q_o)^T * Ï„_i_lim`ï¼Œæ˜¯æ„å‹ç›¸å…³çš„å¹³è¡Œå››è¾¹å½¢ï¼Œè€Œéå›ºå®šè¾¹ç•Œã€‚ | **æ¨¡å‹é€‰æ‹©**",
            "timestamp": "2026-02-03T16:06:03.278185",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## ZESTè®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡åœ¨ç¡¬ä»¶å®éªŒå’Œä»¿çœŸå®éªŒä¸­ä½¿ç”¨äº†å¤šç§é‡åŒ–æŒ‡æ ‡æ¥è¯„ä¼°ç­–ç•¥çš„è·Ÿè¸ªæ€§èƒ½å’Œé²æ£’æ€§ã€‚å…·ä½“æŒ‡æ ‡å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æŒ‡æ ‡åç§° | ç¬¦å·è¡¨ç¤º | è®¡ç®—æ–¹å¼ | ç‰©ç†æ„ä¹‰ | è¯„ä¼°åœºæ™¯ |\n| :--- | :--- | :--- | :--- | :--- |\n| **å…³èŠ‚ä½ç½®å¹³å‡ç»å¯¹è¯¯å·®** | MAE(ğ’’) | $\\frac{1}{T} \\sum_{t=1}^{T} \\|\\bm{q}_t - \\hat{\\bm{q}}_t\\|_1$ | è¡¡é‡ç­–ç•¥è¾“å‡ºå…³èŠ‚è§’åº¦ä¸å‚è€ƒè½¨è¿¹å…³èŠ‚è§’åº¦çš„å¹³å‡åå·®ã€‚å€¼è¶Šå°ï¼Œå…³èŠ‚è·Ÿè¸ªè¶Šç²¾ç¡®ã€‚ | ç¡¬ä»¶å®éªŒ (è¡¨1) |\n| **åŸºåº§å§¿æ€å¹³å‡è§’è·ç¦»** | MAD(R) | $\\frac{1}{T} \\sum_{t=1}^{T} \\text{dist}(\\bm{R}_t, \\hat{\\bm{R}}_t)$ï¼Œå…¶ä¸­distä¸ºæ—‹è½¬çŸ©é˜µé—´çš„æµ‹åœ°è·ç¦»ï¼ˆå¦‚è½´è§’è¡¨ç¤ºçš„æ¨¡é•¿ï¼‰ã€‚ | è¡¡é‡æœºå™¨äººåŸºåº§ï¼ˆèº¯å¹²ï¼‰çš„å®é™…æœå‘ä¸å‚è€ƒæœå‘ä¹‹é—´çš„å¹³å‡è§’åº¦è¯¯å·®ã€‚æ’é™¤åèˆªè§’ï¼ˆyawï¼‰ï¼Œå› å…¶åœ¨IMUä¸Šå­˜åœ¨æ˜¾è‘—æ¼‚ç§»ã€‚ | ç¡¬ä»¶å®éªŒ (è¡¨1) |\n| **åŸºåº§è§’é€Ÿåº¦å¹³å‡L2è¯¯å·®** | ML2(ğ) | $\\frac{1}{T} \\sum_{t=1}^{T} \\|\\bm{\\omega}_t - \\hat{\\bm{\\omega}}_t\\|_2$ | è¡¡é‡åŸºåº§å®é™…è§’é€Ÿåº¦ä¸å‚è€ƒè§’é€Ÿåº¦ä¹‹é—´çš„å¹³å‡æ¬§æ°è·ç¦»è¯¯å·®ã€‚ | ç¡¬ä»¶å®éªŒ (è¡¨1) |\n| **å…³èŠ‚ä½ç½®æœ€å¤§è¯¯å·®** | max(ğ’’) | $\\max_{t} \\|\\bm{q}_t - \\hat{\\bm{q}}_t\\|_\\infty$ | æ•´ä¸ªè¿åŠ¨è¿‡ç¨‹ä¸­ï¼Œå…³èŠ‚è§’åº¦åç¦»å‚è€ƒå€¼çš„æœ€å¤§ç¬æ—¶è¯¯å·®ã€‚åæ˜ æç«¯æƒ…å†µä¸‹çš„è·Ÿè¸ªæ€§èƒ½ã€‚ | ç¡¬ä»¶å®éªŒ (è¡¨1) |\n| **åŸºåº§å§¿æ€æœ€å¤§è¯¯å·®** | max(R) | $\\max_{t} \\text{dist}(\\bm{R}_t, \\hat{\\bm{R}}_t)$ | æ•´ä¸ªè¿åŠ¨è¿‡ç¨‹ä¸­ï¼ŒåŸºåº§å§¿æ€åç¦»å‚è€ƒå€¼çš„æœ€å¤§ç¬æ—¶è§’åº¦è¯¯å·®ã€‚ | ç¡¬ä»¶å®éªŒ (è¡¨1) |\n| **æˆåŠŸç‡** | Success Rate | åœ¨å¤§é‡éšæœºåŒ– rollout ä¸­ï¼ŒæˆåŠŸå®Œæˆæ•´ä¸ªå‚è€ƒè½¨è¿¹çš„æ¯”ç‡ã€‚æˆåŠŸå®šä¹‰ä¸ºæœªè§¦å‘æ—©æœŸç»ˆæ­¢æ¡ä»¶ï¼ˆå¦‚æ‘”å€’ã€ä¸¥é‡åç¦»ï¼‰ã€‚ | ç»¼åˆè¡¡é‡ç­–ç•¥åœ¨å­˜åœ¨å¹²æ‰°å’Œæ¨¡å‹ä¸ç¡®å®šæ€§ä¸‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ | ä»¿çœŸæ¶ˆèå®éªŒ (å›¾3e-f) |\n| **æ€§èƒ½åˆ†ä½æ•° (p10, æœ€å°å€¼)** | p10, Min | åœ¨å¤šæ¬¡å®éªŒæˆ–ä¸åŒéšæœºç§å­ä¸‹ï¼ŒæˆåŠŸç‡çš„ç¬¬10ä¸ªç™¾åˆ†ä½æ•°å’Œæœ€å°å€¼ã€‚ | è¯„ä¼°ç­–ç•¥çš„**æœ€å·®æƒ…å†µæ€§èƒ½**å’Œ**é²æ£’æ€§ä¸‹é™**ï¼Œæ¯”å¹³å‡æˆåŠŸç‡æ›´èƒ½åæ˜ å¯é æ€§ã€‚ | ä»¿çœŸæ¶ˆèå®éªŒ (å›¾3e-f) |\n\n**æŒ‡æ ‡ä½¿ç”¨ç»†èŠ‚ï¼š**\n1.  **ç¡¬ä»¶å®éªŒï¼ˆè¡¨1ï¼‰**ï¼šå¯¹æ¯ä¸ªæŠ€èƒ½è¿›è¡Œå¤šæ¬¡ç‹¬ç«‹è¯•éªŒï¼ŒæŠ¥å‘Šä¸Šè¿°è¯¯å·®æŒ‡æ ‡çš„å‡å€¼ã€‚åŒæ—¶æ ‡æ³¨æŠ€èƒ½æ˜¯å¦ä¸º**å¤šæ¥è§¦**ï¼ˆå³é™¤è„šå¤–ï¼Œæ‰‹ã€è†ã€èº¯å¹²ç­‰éƒ¨ä½æ˜¯å¦ä¸åœ°é¢æ¥è§¦ï¼‰ã€‚\n2.  **ä»¿çœŸå®éªŒ**ï¼š\n    *   **æ¶ˆèç ”ç©¶ï¼ˆå›¾3ï¼‰**ï¼šä¸»è¦ä½¿ç”¨**æˆåŠŸç‡**ï¼Œå¹¶è¾…ä»¥**p10**å’Œ**æœ€å°å€¼**ï¼Œä»¥è¯æ˜è‡ªé€‚åº”é‡‡æ ·å’Œè¾…åŠ©åŠ›çŸ©è¯¾ç¨‹å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚\n    *   **ä¸MPCå¯¹æ¯”ï¼ˆè¡¨2ï¼‰**ï¼šä½¿ç”¨ä¸ç¡¬ä»¶å®éªŒç›¸åŒçš„ **MAE(ğ’’)** å’Œ **MAD(R)**ï¼Œåœ¨ä»¿çœŸç¯å¢ƒä¸­å…¬å¹³æ¯”è¾ƒRLç­–ç•¥ä¸åŸºäºæ¨¡å‹çš„MPCæ§åˆ¶å™¨çš„è·Ÿè¸ªç²¾åº¦ã€‚\n3.  **æŒ‡æ ‡é€‰æ‹©é€»è¾‘**ï¼šé€‰æ‹©è¿™äº›æŒ‡æ ‡æ˜¯å› ä¸ºå®ƒä»¬ç›´æ¥åæ˜ äº†è¿åŠ¨æ¨¡ä»¿çš„æ ¸å¿ƒç›®æ ‡â€”â€”**ç²¾ç¡®è·Ÿè¸ªå‚è€ƒè¿åŠ¨**ã€‚åŒæ—¶ï¼Œé€šè¿‡åˆ†æå¤šæ¥è§¦æŠ€èƒ½ä¸çº¯è¶³å¼æŠ€èƒ½ä¹‹é—´çš„è¯¯å·®å·®å¼‚ï¼ˆä¾‹å¦‚ï¼Œå‰æ»šç¿»çš„MAD(R)è¯¯å·®è¿œå¤§äºæ­¥è¡Œï¼‰ï¼Œå¯ä»¥éªŒè¯æ¡†æ¶å¤„ç†å¤æ‚æ¥è§¦åŠ¨æ€çš„èƒ½åŠ›ã€‚\n\n### äºŒã€ æ•°æ®é›†ï¼ˆå‚è€ƒè¿åŠ¨æ¥æºï¼‰\n\nZESTæ¡†æ¶çš„æ ¸å¿ƒä¼˜åŠ¿ä¹‹ä¸€æ˜¯èƒ½å¤Ÿä»**å¼‚æ„æ•°æ®æº**ä¸­å­¦ä¹ ã€‚è®ºæ–‡ä½¿ç”¨äº†ä¸‰ç±»æ•°æ®æºï¼Œå…¶ç»†èŠ‚å’Œå¤„ç†æµç¨‹å¦‚ä¸‹ï¼š\n\n| æ•°æ®æºç±»å‹ | ç®€ç§° | è·å–æ–¹å¼ | ç‰¹ç‚¹ä¸æŒ‘æˆ˜ | è®ºæ–‡ä¸­åº”ç”¨çš„æŠ€èƒ½ç¤ºä¾‹ (æœºå™¨äººå¹³å°) | åå¤„ç†å…³é”®æ­¥éª¤ |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **è¿åŠ¨æ•æ‰æ•°æ®** | MoCap | ä½¿ç”¨Xsensï¼ˆæƒ¯æ€§ï¼‰å’ŒViconï¼ˆå…‰å­¦ï¼‰ç³»ç»Ÿé‡‡é›†çœŸäººè¿åŠ¨ã€‚ | **é«˜ä¿çœŸã€è‡ªç„¶**ï¼ŒåŒ…å«ä¸°å¯Œçš„äººç±»è¿åŠ¨ç»†å¾®ç‰¹å¾ã€‚æ˜¯è´¨é‡æœ€é«˜çš„æ•°æ®æºã€‚ | é™†å†›çˆ¬è¡Œã€æ»šåœ°ã€éœ¹é›³èˆã€å‰æ»šç¿»ã€ä¾§æ‰‹ç¿»ï¼ˆAtlasï¼‰ï¼›ä¹’ä¹“çƒã€ä¾§æ‰‹ç¿»ï¼ˆG1ï¼‰ | 1. **è¿åŠ¨é‡å®šå‘**ï¼šé€šè¿‡æ—¶ç©ºä¼˜åŒ–ï¼Œå°†æºéª¨éª¼è¿åŠ¨æ˜ å°„åˆ°ç›®æ ‡æœºå™¨äººéª¨éª¼ï¼Œä¼˜åŒ–ç›®æ ‡åŒ…æ‹¬è·Ÿè¸ªæ ¹è½¨è¿¹ã€å¯¹é½éª¨éª¼ã€æ­£åˆ™åŒ–å…³èŠ‚é€Ÿåº¦ã€‚<br>2. **ç‰©ç†ä¸€è‡´æ€§è°ƒæ•´**ï¼šè”åˆä¼˜åŒ–ç»Ÿä¸€ç¼©æ”¾å› å­å’Œæ—¶é—´é‡é‡‡æ ·ï¼Œç¡®ä¿å¼¹é“è¿åŠ¨ç¬¦åˆçœŸå®é‡åŠ›ã€‚ |\n| **è§†é¢‘æ•æ‰æ•°æ®** | ViCap | ä½¿ç”¨æ‰‹æœºç­‰å•ç›®æ‘„åƒå¤´å½•åˆ¶è§†é¢‘ï¼Œé€šè¿‡è®¡ç®—æœºè§†è§‰ç®—æ³•é‡å»º3Däººä½“è¿åŠ¨ã€‚ | **ä¾¿æ·ã€æ•°æ®é‡å¤§**ï¼Œä½†**å™ªå£°å¤§**ï¼Œå­˜åœ¨å§¿æ€æŠ–åŠ¨ã€è„šéƒ¨æ»‘åŠ¨ç­‰ä¼ªå½±ã€‚ | èˆè¹ˆç‰‡æ®µã€è¶³çƒè¸¢ï¼ˆAtlasï¼‰ï¼›èŠ­è•¾åºåˆ—ã€è·³ä¸Š/çˆ¬ä¸Š/çˆ¬ä¸‹ç®±å­ï¼ˆG1ï¼‰ | 1. **è¿åŠ¨é‡å»º**ï¼šç»“åˆ **MegaSaM**ï¼ˆæ¢å¤ç›¸æœºè½¨è¿¹ä¸åœºæ™¯ï¼‰å’Œ **TRAM**ï¼ˆä¼°è®¡SMPLäººä½“å§¿æ€ï¼‰ï¼Œæ˜¾è‘—å‡å°‘äº†è§’è‰²æ¼‚ç§»å’Œæ»‘åŠ¨ä¼ªå½±ã€‚<br>2. **ç¼©æ”¾ä¸é‡å®šå‘**ï¼šå°†SMPLæ¨¡å‹ç»Ÿä¸€ç¼©æ”¾åˆ°æœºå™¨äººå°ºå¯¸ï¼Œç„¶åè¿›è¡Œä¸MoCapç±»ä¼¼çš„è¿åŠ¨é‡å®šå‘ã€‚**ä¸ä¾èµ–æ¥è§¦æ ‡ç­¾**ã€‚ |\n| **å…³é”®å¸§åŠ¨ç”»æ•°æ®** | Animation | äººå·¥åˆ¶ä½œæˆ–ç®—æ³•ç”Ÿæˆï¼Œé’ˆå¯¹ç›®æ ‡æœºå™¨äººå½¢æ€è®¾è®¡ã€‚ | **è¿åŠ¨å­¦å¹²å‡€ã€æ—¶åºè¿è´¯**ï¼Œä½†å¯èƒ½**åŠ¨æ€ä¸å¯è¡Œ**ï¼ˆè¶…å‡ºæœºå™¨äººç‰©ç†æé™ï¼‰ã€‚å¯ç”¨äºåˆ›é€ éäººç±»è¿åŠ¨ã€‚ | æ‰‹å€’ç«‹ç¿»è½¬ï¼ˆAtlasï¼‰ï¼›æ‰‹å€’ç«‹å¹³è¡¡ã€è¿ç»­åç©ºç¿»ã€æ¡¶æ»šã€å¿«ä¹å°ç‹—ï¼ˆSpotï¼‰ | **é€šå¸¸æ— éœ€é‡å®šå‘**ï¼Œå› ä¸ºåŠ¨ç”»æ˜¯ç›´æ¥é’ˆå¯¹ç›®æ ‡æœºå™¨äººéª¨æ¶åˆ¶ä½œçš„ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºè®©ç­–ç•¥å­¦ä¼šæ”¾æ¾è·Ÿè¸ªä¸å¯è¡Œçš„éƒ¨åˆ†ï¼ˆå¦‚è¿‡å¿«çš„è§’é€Ÿåº¦ï¼‰ï¼Œä»¥ä¿æŒç¨³å®šæ€§ã€‚ |\n\n**æ•°æ®é›†ä½¿ç”¨ç»†èŠ‚ä¸æµç¨‹ï¼š**\n1.  **ç»Ÿä¸€å¤„ç†æµç¨‹**ï¼šæ— è®ºæ•°æ®æºå¦‚ä½•ï¼Œæœ€ç»ˆéƒ½è½¬åŒ–ä¸ºæœºå™¨äººå…³èŠ‚ç©ºé—´çš„æ—¶é—´åºåˆ—å‚è€ƒè½¨è¿¹ `{ğ’’Ì‚_j(t), ğ’—Ì‚_IB(t), ğÌ‚_IB(t), ...}`ã€‚\n2.  **æ ¸å¿ƒç‰¹ç‚¹**ï¼šZEST**ä¸ä¾èµ–ä»»ä½•æ¥è§¦æ ‡ç­¾**ã€‚ç­–ç•¥å¿…é¡»ä»è¿åŠ¨å­¦æ•°æ®ä¸­è‡ªè¡Œæ¨æ–­æ¥è§¦æ—¶åºå’Œæ¨¡å¼ï¼Œè¿™å¤§å¤§ç®€åŒ–äº†æ•°æ®å‡†å¤‡æµç¨‹å¹¶å¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ã€‚\n3.  **æ•°æ®åˆ°éƒ¨ç½²çš„å¿«é€Ÿé—­ç¯**ï¼šç‰¹åˆ«æ˜¯ViCapæµç¨‹ï¼Œå®ç°äº†â€œ**ä¸Šåˆå½•åˆ¶ -> ç™½å¤©è®­ç»ƒ -> æ™šä¸Šç¡¬ä»¶æ‰§è¡Œ**â€çš„å¿«é€Ÿè¿­ä»£ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶çš„å®ç”¨æ€§å’Œé«˜æ•ˆæ€§ã€‚\n4.  **å¤šå¹³å°é€‚ç”¨æ€§**ï¼šåŒä¸€å¥—æ•°æ®å¤„ç†å’Œè®­ç»ƒæ–¹æ³•é€‚ç”¨äºAtlasã€G1å’ŒSpotè¿™ä¸‰ä¸ªå½¢æ€ã€å°ºå¯¸ã€é‡é‡è¿¥å¼‚çš„æœºå™¨äººï¼Œè¯æ˜äº†æ–¹æ³•çš„é€šç”¨æ€§ã€‚\n\n### æ€»ç»“\nZESTè®ºæ–‡é€šè¿‡ä¸€å¥—**ç²¾å¿ƒè®¾è®¡çš„å®šé‡æŒ‡æ ‡**ï¼ˆèšç„¦è·Ÿè¸ªè¯¯å·®å’Œé²æ£’æ€§ï¼‰å’Œ**å¤šå…ƒåŒ–çš„å‚è€ƒè¿åŠ¨æ•°æ®é›†**ï¼ˆè¦†ç›–é«˜ä¿çœŸã€å™ªå£°å’Œè™šæ‹Ÿæ•°æ®ï¼‰ï¼Œå…¨é¢ã€å®¢è§‚åœ°è¯„ä¼°äº†å…¶æ¡†æ¶çš„æ€§èƒ½ã€‚å®éªŒç»“æœä¸ä»…å±•ç¤ºäº†åœ¨å„é¡¹æŒ‡æ ‡ä¸Šçš„ä¼˜å¼‚è¡¨ç°ï¼Œæ›´é‡è¦çš„æ˜¯éªŒè¯äº†å…¶**å¤„ç†å¼‚æ„ã€ä¸å®Œç¾æ•°æ®å¹¶å®ç°é›¶æ¬¡æ¨¡æ‹Ÿåˆ°çœŸå®è½¬ç§»**çš„æ ¸å¿ƒåˆ›æ–°ç‚¹ã€‚è¡¨æ ¼ä¸­å±•ç¤ºçš„å…·ä½“æŠ€èƒ½å’Œè¯¯å·®å€¼ä¸ºè¿™äº›ç»“è®ºæä¾›äº†åšå®çš„å®è¯æ”¯æŒã€‚",
            "timestamp": "2026-02-03T16:07:05.110346",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨**ä¸‰ç§å¼‚æ„æ•°æ®æº**ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œæ¶µç›–äº†ä»é«˜ç²¾åº¦åˆ°æ—¥å¸¸è§†é¢‘çš„å¤šç§è¿åŠ¨æ•°æ®ï¼š\n\n1.  **è¿åŠ¨æ•æ‰æ•°æ®**\n    *   **æ¥æº**ï¼šä½¿ç”¨ Xsens å’Œ Vicon ç³»ç»Ÿé‡‡é›†ã€‚\n    *   **ç‰¹ç‚¹**ï¼šé«˜ä¿çœŸåº¦ã€è‡ªç„¶çš„äººç±»è¿åŠ¨æ•°æ®ï¼Œä»…éœ€æœ€å°åŒ–åå¤„ç†ã€‚\n    *   **ç¤ºä¾‹æŠ€èƒ½**ï¼šè¡Œèµ°ã€æ…¢è·‘ã€å‰æ»šç¿»ã€å››è¶³æ»šåŠ¨ã€å››è¶³çˆ¬è¡Œã€åŒåŒå‰è¿›ã€ä¾§æ‰‹ç¿»ã€éœ¹é›³èˆç‰‡æ®µã€ä¹’ä¹“çƒæŒ¥æ‹ã€‚\n\n2.  **è§†é¢‘æ•æ‰æ•°æ®**\n    *   **æ¥æº**ï¼šä½¿ç”¨æ‰‹æŒæ‰‹æœºç­‰å•ç›®æ‘„åƒå¤´åœ¨è‡ªç„¶åœºæ™¯ä¸‹å½•åˆ¶ï¼Œé€šè¿‡ `MegaSaM`ï¼ˆç›¸æœºä¸åœºæ™¯ä¼°è®¡ï¼‰å’Œ `TRAM`ï¼ˆäººä½“å§¿æ€ä¼°è®¡ï¼‰è¿›è¡Œ3Dè¿åŠ¨é‡å»ºã€‚\n    *   **ç‰¹ç‚¹**ï¼šæ•°æ®æ˜“å¾—ï¼Œä½†å­˜åœ¨å§¿æ€æŠ–åŠ¨ã€è„šéƒ¨æ»‘åŠ¨ç­‰å™ªå£°å’Œä¼ªå½±ã€‚\n    *   **ç¤ºä¾‹æŠ€èƒ½**ï¼šè¶³çƒè¸¢çƒã€èˆè¹ˆç‰‡æ®µï¼ˆA-Cï¼‰ã€èŠ­è•¾åºåˆ—ã€è·³ä¸Š/çˆ¬ä¸Š/çˆ¬ä¸‹ç®±å­ã€‚\n\n3.  **å…³é”®å¸§åŠ¨ç”»æ•°æ®**\n    *   **æ¥æº**ï¼šäººå·¥åˆ¶ä½œã€‚\n    *   **ç‰¹ç‚¹**ï¼šè¿åŠ¨å­¦ä¸Šå¹²å‡€ã€æ—¶åºè¿è´¯ï¼Œä½†å¯èƒ½ä¸ç¬¦åˆç‰©ç†çº¦æŸï¼ˆå¦‚åŠ¨åŠ›å­¦ä¸å¯è¡Œï¼‰ã€‚\n    *   **ç”¨é€”**ï¼šç”Ÿæˆäººç±»æ— æ³•å®Œæˆçš„åŠ¨ä½œï¼ˆå¦‚åˆ©ç”¨æœºå™¨äººè¿ç»­å…³èŠ‚ï¼‰æˆ–ä¸ºéäººå½¢å½¢æ€ï¼ˆå¦‚å››è¶³æœºå™¨äººï¼‰è®¾è®¡åŠ¨ä½œã€‚\n    *   **ç¤ºä¾‹æŠ€èƒ½**ï¼šå€’ç«‹ç¿»è½¬ã€å€’ç«‹å¹³è¡¡ã€è¿ç»­åç©ºç¿»ã€æ¡¶æ»šã€â€œå¿«ä¹å°ç‹—â€åŠ¨ä½œã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨äº†ä¸€å¥—**å®šé‡ä¸å®šæ€§ç›¸ç»“åˆ**çš„è¯„ä¼°ä½“ç³»ï¼Œæ ¸å¿ƒå®šé‡æŒ‡æ ‡å¦‚ä¸‹ï¼š\n\n| æŒ‡æ ‡ | å…¨ç§° | å«ä¹‰ | è¯„ä¼°å¯¹è±¡ |\n| :--- | :--- | :--- | :--- |\n| **MAE(ğ’’)** | å¹³å‡ç»å¯¹è¯¯å·® | å…³èŠ‚ä½ç½®è·Ÿè¸ªè¯¯å·®ï¼ˆå¼§åº¦ï¼‰ | è¿åŠ¨è·Ÿè¸ªç²¾åº¦ |\n| **MAD(R)** | å¹³å‡è§’è·ç¦» | åŸºåº§ï¼ˆèº¯å¹²ï¼‰æ–¹å‘è·Ÿè¸ªè¯¯å·®ï¼ˆå¼§åº¦ï¼‰ | èº«ä½“å§¿æ€ç¨³å®šæ€§ |\n| **ML2(ğ)** | å¹³å‡L2è¯¯å·® | åŸºåº§è§’é€Ÿåº¦è·Ÿè¸ªè¯¯å·® (rad/s) | åŠ¨æ€è¿åŠ¨å¹³æ»‘æ€§ |\n| **max(ğ’’)** | æœ€å¤§å…³èŠ‚ä½ç½®è¯¯å·® | å•æ¬¡è¯•éªŒä¸­å…³èŠ‚ä½ç½®çš„æœ€å¤§åå·®ï¼ˆå¼§åº¦ï¼‰ | æç«¯æƒ…å†µä¸‹çš„è·Ÿè¸ªé²æ£’æ€§ |\n| **max(R)** | æœ€å¤§æ–¹å‘è¯¯å·® | å•æ¬¡è¯•éªŒä¸­åŸºåº§æ–¹å‘çš„æœ€å¤§åå·®ï¼ˆå¼§åº¦ï¼‰ | æç«¯å§¿æ€ä¸‹çš„ç¨³å®šæ€§ |\n\n**æ³¨**ï¼š\n*   æ‰€æœ‰æŒ‡æ ‡å‡åœ¨**ç¡¬ä»¶æˆåŠŸè¯•éªŒ**ä¸Šè®¡ç®—ï¼Œ**å€¼è¶Šä½è¡¨ç¤ºæ€§èƒ½è¶Šå¥½**ã€‚\n*   åŸºåº§æ–¹å‘è¯¯å·®ä»…è®¡ç®—æ¨ªæ»šå’Œä¿¯ä»°ï¼Œæ’é™¤äº†å› IMUæ¼‚ç§»è€Œéš¾ä»¥ç›´æ¥è§‚æµ‹çš„åèˆªè§’ã€‚\n*   è®ºæ–‡è¿˜é€šè¿‡**æˆåŠŸç‡**ï¼ˆåœ¨éšæœºåŒ–ä»¿çœŸä¸­å®Œæˆæ•´ä¸ªåŠ¨ä½œåºåˆ—çš„æ¯”ç‡ï¼‰è¿›è¡Œäº†å¤§è§„æ¨¡çš„ä»¿çœŸæ¶ˆèå®éªŒã€‚\n\n### ä¸‰ã€ æ€§èƒ½æå‡ä¸æ ¸å¿ƒæˆæœ\nZESTæ¡†æ¶çš„æ€§èƒ½æå‡ä¸»è¦ä½“ç°åœ¨å…¶**æ³›åŒ–èƒ½åŠ›ã€é²æ£’æ€§å’Œç®€åŒ–æµç¨‹**ä¸Šï¼Œè€Œéé’ˆå¯¹å•ä¸€æŒ‡æ ‡çš„ç»å¯¹æå‡ã€‚å…¶ä¸»è¦æˆæœå¯¹æ¯”å¦‚ä¸‹ï¼š\n\n| å¯¹æ¯”ç»´åº¦ | ZEST çš„è¡¨ç° | ä¼ ç»Ÿæˆ–åŸºçº¿æ–¹æ³•çš„å±€é™ |\n| :--- | :--- | :--- |\n| **æ•°æ®æºæ³›åŒ–** | **ç»Ÿä¸€æ¡†æ¶**å¤„ç†MoCapã€ViCapã€Animationä¸‰ç§è´¨é‡ä¸ä¸€çš„æ•°æ®ï¼Œå‡èƒ½å®ç°é›¶æ ·æœ¬ç¡¬ä»¶éƒ¨ç½²ã€‚ | é€šå¸¸éœ€è¦ä¸ºä¸åŒæ•°æ®æºè®¾è®¡ç‰¹å®šç®¡é“ï¼Œå¯¹é«˜å™ªå£°è§†é¢‘æ•°æ®é²æ£’æ€§å·®ã€‚ |\n| **è·¨å¹³å°æ³›åŒ–** | åŒä¸€å¥—æ–¹æ³•å’Œè¶…å‚æ•°åº”ç”¨äº**Atlasäººå½¢æœºå™¨äººã€Unitree G1äººå½¢æœºå™¨äººã€Spotå››è¶³æœºå™¨äºº**ã€‚ | æ§åˆ¶å™¨é€šå¸¸ä¸ç‰¹å®šæœºå™¨äººå¹³å°æˆ–å½¢æ€ç´§å¯†è€¦åˆï¼Œè¿ç§»æˆæœ¬é«˜ã€‚ |\n| **æŠ€èƒ½å¤æ‚åº¦** | å®ç°äº†**å…¨å°ºå¯¸äººå½¢æœºå™¨äººï¼ˆAtlasï¼‰ä¸Šé¦–æ¬¡åŠ¨æ€å¤šæ¥è§¦æŠ€èƒ½**ï¼ˆå¦‚åŒåŒå‰è¿›ã€éœ¹é›³èˆï¼‰ï¼Œä»¥åŠ**ä»è§†é¢‘åˆ°äººå½¢æœºå™¨äººï¼ˆG1ï¼‰çš„é¦–æ¬¡åŠ¨æ€æŠ€èƒ½è¿ç§»**ï¼ˆå¦‚è·³èˆã€çˆ¬ç®±ï¼‰ã€‚ | ä¼ ç»Ÿæ¨¡å‹é¢„æµ‹æ§åˆ¶ç­‰æ–¹æ³•éš¾ä»¥å¤„ç†è†ã€è‚˜ã€èº¯å¹²ç­‰å¤šç‚¹æ¥è§¦ï¼Œä¾èµ–ç²¾ç¡®çš„æ¥è§¦æ—¶åºæ ‡æ³¨ï¼Œå¯¹æ¨¡å‹è¯¯å·®æ•æ„Ÿã€‚ |\n| **æµç¨‹ç®€åŒ–** | **é¿å…ä½¿ç”¨**ï¼šæ¥è§¦æ ‡ç­¾ã€å†—é•¿çš„å‚è€ƒ/è§‚æµ‹çª—å£ã€å¤–éƒ¨çŠ¶æ€ä¼°è®¡å™¨ã€å¤æ‚çš„å¥–åŠ±å‡½æ•°å·¥ç¨‹ã€å¤šé˜¶æ®µè®­ç»ƒã€‚**ç­–ç•¥ç½‘ç»œä»…ä¸ºç®€å•çš„å‰é¦ˆMLP**ã€‚ | ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–ä¸Šè¿°ä¸€ä¸ªæˆ–å¤šä¸ªå¤æ‚ç»„ä»¶ï¼Œå¢åŠ äº†è°ƒè¯•å’Œéƒ¨ç½²çš„å¤æ‚æ€§ã€‚ |\n| **ä»¿çœŸåˆ°ç°å®è½¬ç§»** | åœ¨**ä¸­ç­‰ç¨‹åº¦çš„åŸŸéšæœºåŒ–**ä¸‹è®­ç»ƒï¼Œç­–ç•¥èƒ½å¤Ÿ**é›¶æ ·æœ¬**éƒ¨ç½²åˆ°ç‰©ç†ç¡¬ä»¶ï¼Œè¡¨ç°å‡ºå¯¹æ¨¡å‹è¯¯å·®å’Œæ‰°åŠ¨çš„é²æ£’æ€§ã€‚ | å¯¹äºæ¥è§¦ä¸°å¯Œã€æ¥è¿‘é©±åŠ¨æé™çš„åŠ¨æ€æŠ€èƒ½ï¼Œä»¿çœŸåˆ°ç°å®çš„å·®è·é€šå¸¸å¾ˆå¤§ï¼Œéœ€è¦é¢å¤–çš„å¾®è°ƒæˆ–è‡ªé€‚åº”ç­–ç•¥ã€‚ |\n| **ä¸MPCåŸºçº¿å¯¹æ¯”** | åœ¨ä»¿çœŸå¯¹æ¯”ä¸­ï¼Œå¯¹äºåŠ¨æ€è¡Œä¸ºï¼ˆå¦‚ä¾§æ‰‹ç¿»ã€æ…¢è·‘ï¼‰ï¼ŒZESTçš„RLç­–ç•¥åœ¨å…³èŠ‚è·Ÿè¸ªè¯¯å·®ä¸Šè¡¨ç°**ä¼˜äºæˆ–æŒå¹³**MPCã€‚MPCåœ¨éœ€è¦è†/èº¯å¹²æ¥è§¦æˆ–æ¥è§¦æ—¶åºä¸æ¸…æ™°çš„åŠ¨ä½œä¸Š**å¤±è´¥**ã€‚ | MPCè™½ç„¶å¯¹è¡Œèµ°ç­‰ç®€å•ã€æ¥è§¦æ¸…æ™°çš„åŠ¨ä½œè¡¨ç°è‰¯å¥½ï¼Œä½†å…¶æ€§èƒ½ä¸¥é‡ä¾èµ–ç²¾ç¡®çš„æ¥è§¦æ—¶åºæ ‡æ³¨å’Œå»ºæ¨¡ï¼Œæ— æ³•å¤„ç†ä»»æ„èº«ä½“éƒ¨ä½çš„æ¥è§¦ã€‚ |\n\n**å…³é”®æ€§èƒ½æ•°æ®ç¤ºä¾‹ï¼ˆæ¥è‡ªè®ºæ–‡è¡¨1ï¼‰**:\n*   **Atlas è¡Œèµ°**: MAE(ğ’’) = 0.057 rad, MAD(R) = 0.030 radã€‚\n*   **Atlas åŒåŒå‰è¿›**: MAE(ğ’’) = 0.103 rad, MAD(R) = 0.075 rad ï¼ˆå°½ç®¡æ˜¯å¤šæ¥è§¦å¤æ‚æŠ€èƒ½ï¼Œè¯¯å·®ä»åœ¨å¯æ§èŒƒå›´ï¼‰ã€‚\n*   **G1 çˆ¬ç®±**: MAE(ğ’’) = 0.073-0.108 radï¼Œ åœ¨åˆå§‹ä½ç½®ã€ç®±å­é«˜åº¦ã€æœºå™¨äººè´¨é‡å˜åŒ–ä¸‹ä»èƒ½æˆåŠŸæ‰§è¡Œï¼Œå±•ç¤ºäº†**å“è¶Šçš„é²æ£’æ€§**ã€‚\n\n### æ€»ç»“\nZESTçš„æ ¸å¿ƒæ€§èƒ½æå‡åœ¨äº**å»ºç«‹äº†ä¸€ä¸ªæå…¶ç®€åŒ–å´å¼‚å¸¸å¼ºå¤§çš„é€šç”¨ç®¡é“**ï¼Œèƒ½å¤Ÿå°†å¤šç§ä¸å®Œç¾çš„è¿åŠ¨æ•°æ®æºï¼Œç›´æ¥è½¬åŒ–ä¸ºå¯åœ¨å¤šç§æœºå™¨äººå¹³å°ä¸Šé›¶æ ·æœ¬æ‰§è¡Œçš„ã€é²æ£’çš„å…¨èº«æ§åˆ¶ç­–ç•¥ã€‚å®ƒçªç ´äº†ä¼ ç»Ÿæ–¹æ³•åœ¨**å¤„ç†å¤æ‚å¤šæ¥è§¦æŠ€èƒ½**å’Œ**ä¾èµ–ç¹çæ‰‹å·¥æ¨¡å—**æ–¹é¢çš„ç“¶é¢ˆï¼Œä¸ºå®ç°æ•æ·ã€æ‹Ÿäººçš„æœºå™¨äººæŠ€èƒ½åº“æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
            "timestamp": "2026-02-03T16:07:56.240372",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡â€œDISCUSSIONâ€å’Œâ€œMATERIALS AND METHODSâ€ç­‰ç« èŠ‚çš„è®ºè¿°ï¼ŒZESTæ¡†æ¶ç›®å‰å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n### 1. **æ³›åŒ–èƒ½åŠ›æœªç»éªŒè¯**\n   - **é—®é¢˜**ï¼šè®ºæ–‡æ˜ç¡®æŒ‡å‡ºï¼Œ**æœªè¯„ä¼°å¤šæŠ€èƒ½ç­–ç•¥å¯¹è®­ç»ƒé›†å¤–æ–°åŠ¨ä½œçš„æ³›åŒ–èƒ½åŠ›**ã€‚å½“å‰ç­–ç•¥æ˜¯é’ˆå¯¹ç‰¹å®šæŠ€èƒ½è¿›è¡Œä¸“é—¨è®­ç»ƒçš„ï¼ˆæ¯ä¸ªå®éªŒä½¿ç”¨ä¸€ä¸ªä¸“é—¨ç­–ç•¥ï¼‰ã€‚è™½ç„¶è®­ç»ƒäº†ä¸€ä¸ªå¤šæŠ€èƒ½ç­–ç•¥ç”¨äºä»¿çœŸåˆ†æï¼Œä½†å¹¶æœªåœ¨ç¡¬ä»¶ä¸Šéƒ¨ç½²ï¼Œä¹Ÿæœªæµ‹è¯•å…¶èƒ½å¦æ‰§è¡Œä¸è®­ç»ƒåŠ¨ä½œç›¸ä¼¼ä½†ä¸åŒçš„æ–°æŠ€èƒ½ã€‚\n   - **å½±å“**ï¼šè¿™å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šç­–ç•¥æ˜¯å­¦ä¹ åˆ°äº†å¯è¿ç§»çš„é€šç”¨æ§åˆ¶åŸºå…ƒï¼Œè¿˜æ˜¯ä»…ä»…è¿‡æ‹Ÿåˆåˆ°äº†å…·ä½“çš„å‚è€ƒè½¨è¿¹ã€‚è¿™é™åˆ¶äº†å…¶ä½œä¸ºâ€œé€šç”¨æ¥å£â€çš„å®£ç§°ã€‚\n\n### 2. **æ„ŸçŸ¥ä¸ç¯å¢ƒäº¤äº’çš„å±€é™æ€§**\n   - **å‡è®¾**ï¼šå½“å‰æ¡†æ¶æ˜¯**çº¯æœ¬ä½“æ„ŸçŸ¥çš„**ï¼Œå‡è®¾åœ°é¢å¹³å¦ä¸”ä¸æ»‘ã€‚\n   - **é—®é¢˜**ï¼šç¼ºä¹å¯¹**ä¸å¹³å¦ã€æŸ”è½¯æˆ–åŠ¨æ€ç¯å¢ƒ**çš„æ˜¾å¼æ„ŸçŸ¥èƒ½åŠ›ã€‚å¯¹äºéœ€è¦åœ¨å¤æ‚ã€éç»“æ„åŒ–ç¯å¢ƒä¸­è¿›è¡Œåœºæ™¯äº¤äº’çš„ä»»åŠ¡ï¼ˆå¦‚è®ºæ–‡ä¸­å±•ç¤ºçš„çˆ¬ç®±å­ï¼‰ï¼Œç­–ç•¥ä¾èµ–äºè®­ç»ƒæ—¶çš„éšæœºåŒ–æ¥åº”å¯¹åˆå§‹ä½å§¿å˜åŒ–ï¼Œè€Œéå®æ—¶çš„ç¯å¢ƒæ„ŸçŸ¥ã€‚\n   - **å½±å“**ï¼šè¿™é™åˆ¶äº†æœºå™¨äººåœ¨çœŸå®ä¸–ç•Œå¤æ‚ç¯å¢ƒï¼ˆå¦‚é‡å¤–ã€æœ‰éšœç¢ç‰©çš„å®¤å†…ï¼‰ä¸­çš„å®é™…åº”ç”¨ã€‚\n\n### 3. **ä»¿çœŸåˆ°ç°å®çš„ä¾èµ–æ€§ä¸ç³»ç»Ÿè¾¨è¯†æŒ‘æˆ˜**\n   - **é—®é¢˜**ï¼šé›¶æ ·æœ¬è¿ç§»çš„æˆåŠŸ**é«˜åº¦ä¾èµ–äºåˆç†çš„ä»¿çœŸå»ºæ¨¡**ã€‚è®ºæ–‡è™½ç„¶æå‡ºäº†å¹¶è”è¿æ†é©±åŠ¨å™¨ï¼ˆPLAï¼‰çš„æ¸è¿›ç®€åŒ–æ¨¡å‹å’ŒåŸºäºæ ‡ç§°ç”µæ¢å€¼é€‰æ‹©PDå¢ç›Šçš„å®ç”¨æµç¨‹ï¼Œä½†æŒ‡å‡º**å®Œå…¨è‡ªåŠ¨åŒ–çš„ç³»ç»Ÿè¾¨è¯†ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜**ã€‚\n   - **å…·ä½“æŒ‘æˆ˜**ï¼šéš¾ä»¥ç”¨ç¬¬ä¸€æ€§åŸç†æ¨¡å‹æ•è·çš„**å¤æ‚ç‰©ç†ç°è±¡**ï¼ˆå¦‚å¤æ‚çš„æ‘©æ“¦ã€æ¥è§¦æŸ”é¡ºæ€§ã€æ‰§è¡Œå™¨éçº¿æ€§åŠ¨åŠ›å­¦ï¼‰å¯èƒ½ä»éœ€æ•°æ®é©±åŠ¨çš„æ–¹æ³•æ¥è§£å†³ã€‚ä¸å‡†ç¡®çš„å»ºæ¨¡ä¼šæ‰©å¤§â€œä»¿çœŸåˆ°ç°å®â€çš„å·®è·ã€‚\n\n### 4. **ç­–ç•¥çš„ä¸“ä¸šåŒ–ä¸æ‰©å±•æ€§çŸ›ç›¾**\n   - **é—®é¢˜**ï¼šä¸ºäº†åœ¨å›ºå®šæ—¶é—´é¢„ç®—å†…è·å¾—æœ€ä½³è·Ÿè¸ªæ€§èƒ½ï¼Œç¡¬ä»¶éƒ¨ç½²é‡‡ç”¨äº†**ä¸ºæ¯ä¸ªç›®æ ‡æŠ€èƒ½è®­ç»ƒä¸“ç”¨ç­–ç•¥**çš„æ–¹æ¡ˆï¼Œè€Œéä½¿ç”¨å•ä¸€çš„å¤šæŠ€èƒ½é€šç”¨ç­–ç•¥ã€‚\n   - **å½±å“**ï¼šè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¸â€œç»Ÿä¸€ã€é€šç”¨â€çš„ç›®æ ‡ç›¸æ‚–ï¼Œå¢åŠ äº†ç®¡ç†å¤§é‡æŠ€èƒ½ç­–ç•¥çš„å¤æ‚æ€§ï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•åˆ©ç”¨æŠ€èƒ½é—´çš„å…±æ€§ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nè®ºæ–‡åœ¨â€œDISCUSSIONâ€æœ«å°¾æ˜ç¡®è§„åˆ’äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä¸»è¦åŒ…æ‹¬ï¼š\n\n### 1. **æå‡æ³›åŒ–ä¸æ‰©å±•æ€§**\n   - **ç›®æ ‡**ï¼šå®ç°å¯¹**æœªè§åŠ¨ä½œçš„é€šç”¨è·Ÿè¸ª**ã€‚\n   - **æ–¹æ³•**ï¼š\n     - ä½¿ç”¨**ç´§å‡‘çš„è¿åŠ¨åµŒå…¥**æˆ–**é›¶æ ·æœ¬/å°‘æ ·æœ¬è‡ªé€‚åº”**æŠ€æœ¯ã€‚\n     - ç»“åˆ**æŒç»­å­¦ä¹ ç­–ç•¥**ï¼Œä»¥ä¾¿åœ¨ä¸å‘ç”Ÿç¾éš¾æ€§é—å¿˜çš„æƒ…å†µä¸‹æ‰©å±•åŠ¨ä½œåº“ã€‚\n\n### 2. **å¼€å‘æ›´é«˜çº§ã€æ›´çµæ´»çš„äº¤äº’æ¥å£**\n   - **ç›®æ ‡**ï¼šè¶…è¶Šå®Œå…¨å‚è€ƒè½¨è¿¹æ§åˆ¶ï¼Œè½¬å‘**éƒ¨åˆ†æ¡ä»¶åŒ–çš„æ¥å£**ã€‚\n   - **æ–¹æ³•**ï¼š\n     - **å¸ˆç”Ÿè’¸é¦**ï¼šå…ˆè®­ç»ƒä¸€ä¸ªèƒ½è·Ÿè¸ªå¤§å‹åŠ¨ä½œåº“çš„â€œæ•™å¸ˆâ€ç­–ç•¥ï¼ˆä½¿ç”¨å®Œæ•´å‚è€ƒï¼‰ï¼Œç„¶åå°†çŸ¥è¯†è’¸é¦åˆ°ä¸€ä¸ªæ¥å—**ç¨€ç–ã€äººç±»å¯è§£é‡Šè¾“å…¥**çš„â€œå­¦ç”Ÿâ€ç­–ç•¥ä¸­ã€‚\n     - **è¾“å…¥å½¢å¼**ï¼šå…³é”®å¸§ã€çŸ­å§¿åŠ¿ç‰‡æ®µã€ç‰©ä½“å’Œåœºæ™¯å…³é”®å¸§ï¼Œæˆ–**è¯­è¨€æŒ‡ä»¤**ã€‚è¿™å€Ÿé‰´äº†ç‰©ç†è§’è‰²æ§åˆ¶ä¸­çš„**æ©ç æ¡ä»¶èŒƒå¼**ã€‚\n\n### 3. **é›†æˆé«˜å±‚ç”Ÿæˆå¼è§„åˆ’å™¨**\n   - **ç›®æ ‡**ï¼šåœ¨è¡Œä¸ºåˆæˆä¸æ§åˆ¶ä¹‹é—´å½¢æˆé—­ç¯ã€‚\n   - **æ–¹æ³•**ï¼šæ¢ç´¢èƒ½å¤Ÿå°†ç”¨æˆ·å‘½ä»¤æ˜ å°„ä¸ºè¿åŠ¨è®¡åˆ’çš„**é«˜å±‚ç”Ÿæˆå¼è§„åˆ’å™¨**ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰ï¼Œç„¶åç”±é€šç”¨çš„ã€åŸºäºRLçš„è·Ÿè¸ªå™¨æ¥æ‰§è¡Œã€‚è¿™å°†å®ç°æ›´è‡ªä¸»ã€æ›´çµæ´»çš„æœºå™¨äººè¡Œä¸ºç”Ÿæˆã€‚\n\n### 4. **å¢å¼ºç¯å¢ƒæ„ŸçŸ¥ä¸é€‚åº”æ€§**\n   - **éšå«æ–¹å‘**ï¼šè™½ç„¶æœªåœ¨â€œæœªæ¥å·¥ä½œâ€ä¸­æ˜ç¡®åˆ—å‡ºï¼Œä½†åŸºäºå…¶æ„ŸçŸ¥å±€é™æ€§ï¼Œè‡ªç„¶çš„å»¶ä¼¸æ˜¯**é›†æˆè§†è§‰æˆ–å…¶ä»–å¤–æ„ŸçŸ¥ä¼ æ„Ÿå™¨**ï¼Œä½¿ç­–ç•¥èƒ½å¤Ÿæ„ŸçŸ¥å’Œç†è§£éå¹³å¦åœ°å½¢ã€ç§»åŠ¨éšœç¢ç‰©å’ŒæœªçŸ¥ç‰©ä½“ï¼Œä»è€Œåœ¨æ›´å¹¿æ³›çš„ç¯å¢ƒä¸­å¯é è¿è¡Œã€‚\n\n### 5. **æ·±åŒ–ä»¿çœŸå»ºæ¨¡ä¸ç³»ç»Ÿè¾¨è¯†**\n   - **éšå«æ–¹å‘**ï¼šç»§ç»­æ¨è¿›æ•°æ®é©±åŠ¨çš„ä»¿çœŸå»ºæ¨¡æ–¹æ³•ï¼Œä»¥æ›´è‡ªåŠ¨åŒ–å’Œç²¾å‡†åœ°æ•è·æœºå™¨äººç¡¬ä»¶çš„å¤æ‚åŠ¨åŠ›å­¦ï¼Œè¿›ä¸€æ­¥ç¼©å°ä»¿çœŸåˆ°ç°å®çš„å·®è·ã€‚\n\n**æ€»ç»“**ï¼šZESTåœ¨é›¶æ ·æœ¬è¿åŠ¨æ¨¡ä»¿è¿ç§»æ–¹é¢å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†å…¶å½“å‰å½¢å¼åœ¨**æ³›åŒ–æ€§ã€ç¯å¢ƒæ„ŸçŸ¥çš„è‡ªä¸»æ€§ä»¥åŠæŠ€èƒ½ç®¡ç†çš„å¯æ‰©å±•æ€§**æ–¹é¢å­˜åœ¨å±€é™ã€‚æœªæ¥çš„å·¥ä½œå°†é‡ç‚¹å›´ç»•**å­¦ä¹ é€šç”¨è¡¨å¾ã€å¼€å‘é«˜çº§äººæœºäº¤äº’æ¥å£ã€ä»¥åŠç»“åˆç”Ÿæˆå¼è§„åˆ’**æ¥æ„å»ºä¸€ä¸ªæ›´å¼ºå¤§ã€æ›´æ™ºèƒ½çš„æœºå™¨äººè¿åŠ¨æ§åˆ¶ç³»ç»Ÿã€‚",
            "timestamp": "2026-02-03T16:08:33.839280",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T23:35:02Z",
    "created_at": "2026-02-03T15:53:20.602766",
    "updated_at": "2026-02-03T15:53:20.602773"
}