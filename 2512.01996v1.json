{
  "id": "2512.01996v1",
  "title": "Learning Sim-to-Real Humanoid Locomotion in 15 Minutes",
  "authors": [
    "Younggyo Seo",
    "Carmelo Sferrazza",
    "Juyue Chen",
    "Guanya Shi",
    "Rocky Duan",
    "Pieter Abbeel"
  ],
  "abstract": "Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive scale with thousands of parallel environments through carefully tuned design choices and minimalist reward functions. We demonstrate rapid end-to-end learning of humanoid locomotion controllers on Unitree G1 and Booster T1 robots under strong domain randomization, e.g., randomized dynamics, rough terrain, and push perturbations, as well as fast training of whole-body human-motion tracking policies. We provide videos and open-source implementation at: https://younggyo.me/fastsac-humanoid.",
  "url": "https://arxiv.org/abs/2512.01996v1",
  "html_url": "https://arxiv.org/html/2512.01996v1",
  "html_content": "Learning Sim-to-Real Humanoid Locomotion\nin 15 Minutes\nYounggyo Seo\n‚àó\nCarmelo Sferrazza\n‚àó\nJuyue Chen\nGuanya Shi\nRocky Duan\nPieter Abbeel\nAmazon FAR (Frontier AI & Robotics)\nAbstract\nMassively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes.\nHowever, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization.\nIn this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e.,\nFastSAC\nand\nFastTD3\n, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU.\nOur simple recipe stabilizes off-policy RL algorithms at massive scale with thousands of parallel environments through carefully tuned design choices and minimalist reward functions.\nWe demonstrate rapid end-to-end learning of humanoid locomotion controllers on Unitree G1 and Booster T1 robots under strong domain randomization, e.g., randomized dynamics, rough terrain, and push perturbations, as well as fast training of whole-body human-motion tracking policies.\nWe provide videos and open-source implementation at:\nhttps://younggyo.me/fastsac-humanoid\n.\nFigure 1:\nSummary of results.\nWe introduce a simple recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that learns robust humanoid locomotion policies in 15 minutes on a single RTX 4090 GPU, with strong domain randomization including randomized dynamics, rough terrain, and push perturbations.\nWe also show that our recipe based on off-policy RL algorithms is scalable and accelerates the training of whole-body tracking policies: trained with 4\n√ó\n\\times\nL40s GPUs and\n16384\n16384\nparallel environments, FastSAC and FastTD3 learn to complete the full sequence of dancing motion much faster than PPO under the same condition.\nFor sim-to-real deployment with Unitree G1 and Booster T1, we used the checkpoints saved at the points we marked as\n‚òÖ\n\\bigstar\n.\n1\nIntroduction\nIn recent years, reinforcement learning (RL) has undergone a dramatic shift driven by the emergence of massively parallel simulation frameworks\n(\nrudin2022learning\n;\nkaufmann2023champion\n)\n.\nBy scaling environment throughput to thousands of environments, these frameworks have reduced wall-clock training time from many hours to mere minutes for a wide range of benchmark tasks\n(\nmakoviychuk2021isaac\n;\nmittal2023orbit\n;\nzakka2025mujoco\n)\n.\nThis shift has had an outsized impact on robotics, where sim-to-real development is inherently iterative: a policy is trained in simulation, deployed on hardware, and reveals mismatches such as unmodeled dynamics or sensing inaccuracies\n(\nzhao2020sim\n)\n.\nThese discrepancies must then be corrected by improving the simulation environment, requiring the entire pipeline to be retrained\n(\nchebotar2019closing\n)\n.\nBecause these cycles repeat until the policy is reliable, fast simulation becomes essential for making such iteration feasible.\nDespite the speed offered by modern parallel simulators, these iterative cycles remain expensive in practice, especially for high-dimensional systems such as humanoids. Achieving robust transfer of policies to the real world typically requires expanding domain randomization\n(\nsadeghi2016cad2rl\n;\ntobin2017domain\n;\npeng2018sim\n)\n, randomizing terrain properties\n(\nrudin2022learning\n)\n, or shaping curricula that encourage low-effort, stable whole-body behavior.\nSuch components complicate exploration and reduce sample efficiency, pushing training for humanoid locomotion or tracking back into the multi-hour regime.\nThus, despite dramatic gains in raw throughput, achieving fast, reliable sim-to-real iteration for humanoid control remains a challenge.\nThis work introduces a simple and practical recipe that brings sim-to-real iteration time for humanoid robots back to the order of minutes.\nAt the core of this recipe are FastSAC and FastTD3\n(\nseo2025fasttd3\n)\n, efficient variants of popular off-policy RL algorithms, i.e., Soft Actor-Critic\n(\nhaarnoja2018learning\n)\nand TD3\n(\nfujimoto2018addressing\n)\n, which have been shown to learn humanoid control policies faster than on-policy RL algorithms such as PPO\n(\nschulman2017proximal\n)\n.\nWhile\nseo2025fasttd3\ndemonstrated the first sim-to-real deployment of FastTD3 policies to real humanoid hardware, the results were limited to relatively simple controllers for humanoids with only a subset of joints.\nIn this work, we show that with careful design choices and hyperparameters, FastSAC and FastTD3 can scale to full-body humanoid control, enabling rapid sim-to-real iterations for training locomotion policies with all joints or whole-body tracking policies that follow human motion.\nAnother important aspect of our fast sim-to-real recipe is its simplicity in reward design.\nBy adopting reward functions with essential terms, we can quickly sweep hyperparameters, isolate what matters for transfer, and avoid the brittle engineering often required in humanoid locomotion setups.\nWith our recipe, we train a full-fledged humanoid locomotion policy with randomized dynamics, rough terrain, push perturbations, and an automatic action-rate curriculum, all end-to-end in 15 minutes on a single RTX 4090 GPU.\nThe code for this recipe is available in the Holosoma repository\n(\nHolosoma\n)\nat\nhttps://github.com/amazon-far/holosoma\n.\n2\nRecipe\n2.1\nFastSAC and FastTD3: Off-Policy RL for Humanoid Control\nOur recipe is based on off-policy RL algorithms tuned for large-scale training with massively parallel simulation, i.e., FastTD3 and FastSAC\n(\nseo2025fasttd3\n)\n, instead of PPO\n(\nschulman2017proximal\n)\nthat has been a standard algorithm for sim-to-real RL due to the ease of scaling up with parallel simulation.\nThis is motivated by recent work that have demonstrated off-policy algorithms can also scale effectively and be faster than PPO in various benchmark tasks\n(\nli2023parallel\n;\nraffin2025isaacsim\n;\nseo2025fasttd3\n;\nshukla2025fastsac\n)\nby effectively re-using the data from simulation.\nNotably,\nseo2025fasttd3\nreport the first sim-to-real deployment of humanoid control policies trained with off-policy RL to real humanoid hardware.\nHowever, its results were limited to humanoid robots with a subset of joints.\nThis section describes how we train FastTD3 and FastSAC to achieve full-body humanoid control, enabling rapid sim-to-real iterations for training locomotion policies with all joints or whole-body tracking policies that follow human motion.\nIn particular, we improve the recipe of\nseo2025fasttd3\nin training FastSAC, which learns how to explore environments from data with its maximum entropy learning scheme instead of deterministic policies with fixed noise schedules (FastTD3), mitigating the exploration challenges of training with strong domain randomization.\nScaling up off-policy RL with massively parallel simulation\nSimilar to prior work\n(\nli2023parallel\n;\nshukla2025fastsac\n;\nraffin2025isaacsim\n;\nseo2025fasttd3\n)\n, we use massively parallel simulation for training FastSAC and FastTD3 agents.\nWe find that the effect of using more environments is particularly visible in challenging whole-body tracking tasks (see\nFigure\nÀú\n2f\n).\nWe also find that most of observations in\nseo2025fasttd3\nwith regard to scaling up off-policy RL also holds for full-body humanoid control as well.\nFor instance, we find that using large batch size up to 8K consistently improves performance.\nWe also find that taking more gradient steps per each simulation step usually ends up in a faster training, and slow simulation speed often becomes a bottleneck with more challenging setups such as training robots in non-flat terrains (see\nFigure\nÀú\n2b\n).\nThis makes off-policy RL, which can re-use data from previous interactions instead of discarding it, a more attractive choice for fast training.\n(a)\nClipped double Q-learning\n(b)\nNumber of update steps\n(c)\nNormalization\n(d)\nEffect of\nŒ≥\n\\gamma\n(e)\nEffect of\nŒ≥\n\\gamma\n(WBT)\n(f)\nNumber of environments (WBT)\nFigure 2:\nFastSAC: Analyses.\nWe investigate the effect of (a) Clipped double Q-learning, (b) number of update steps, (c) normalization techniques, and (d) discount factor\nŒ≥\n\\gamma\non a Unitree G1 locomotion task with rough terrain.\nWe further investigate the effect of (e) discount factor\nŒ≥\n\\gamma\nand (f) number of environments on a G1 whole-body tracking (WBT) task with a dancing motion.\nWe use a single RTX¬†4090¬†GPU for locomotion experiments (a-d) and 4\n√ó\n\\times\nL40s GPUs for whole-body tracking (e-f).\nJoint-limit-aware action bounds\nOne challenge in training off-policy RL algorithms such as SAC or TD3 is setting proper action bounds for its Tanh policy.\nFor instance,\nraffin2025isaacsim\nobserved that training often becomes unstable when trained in unbounded action space.\nTo address this challenge, we introduce a simple technique that sets the action bounds based on the robots‚Äô joint limits when using PD controllers. In particular, we calculate the difference between each joint‚Äôs limit and its default position, then use it as an action bound for each joint.\nWe find that this effectively reduces the need to tune action bounds for training FastSAC and FastTD3\n1\n1\n1\nInterestingly, after we have fully stabilized the training with all the components, we find that we can achieve stable training of FastSAC and FastTD3 agents with an unbounded action space. Nonetheless, we still keep joint-limit-aware action bounds as we expect this scheme to be helpful in training off-policy RL agents for other robots or tasks. We recommend training agents in an unbounded action space when encountering a failure case where the robot is not generating enough torque due to restrictive action bounds.\n.\nObservation and Layer normalization\nSimilar to\nseo2025fasttd3\n, we find that observation normalization is helpful for training.\nHowever, unlike\nseo2025fasttd3\n, we find that layer normalization\n(\nba2016layer\n)\nis helpful in stabilizing the performance in high-dimensional tasks (see\nFigure\nÀú\n2c\n).\nThis is aligned with prior observations that find layer normalization is helpful for training SAC\n(\nball2023efficient\n;\nnauman2024bigger\n)\nagents in challenging benchmark tasks.\nCritic learning hyperparameters\nWe find that using the average of Q-values improves FastSAC and FastTD3 performance over using Clipped double Q-learning (CDQ;\nfujimoto2018addressing\n) that uses the minimum (see\nFigure\nÀú\n2a\n).\nThis aligns with the observation of\nnauman2024bigger\nthat shows CDQ is harmful when used with layer normalization.\nWe find that low discount factor\nŒ≥\n=\n0.97\n\\gamma=0.97\nis helpful for simple velocity tracking tasks (see\nFigure\nÀú\n2d\n), while\nŒ≥\n=\n0.99\n\\gamma=0.99\nis helpful for challenging whole-body tracking tasks (see\nFigure\nÀú\n2e\n).\nFollowing prior work\n(\nli2023parallel\n;\nseo2025fasttd3\n)\n, we also use a distributional critic, i.e., C51\n(\nbellemare2017distributional\n)\n.\nWe find that distributional critic with quantile regression\n(\ndabney2018distributional\n)\nis too expensive in particular with large batch training.\nFastSAC: Exploration hyperparameters\nA widely-used implementation of SAC bounds the standard deviation\nœÉ\n\\sigma\nof the pre-tanh actions to be\ne\n2\ne^{2}\n(\nhuang2022cleanrl\n)\n.\nHowever, we find that, when combined with large initial value of the temperature\nŒ±\n\\alpha\n, this sometimes causes instability due to excessive exploration.\nWe instead set the maximum\nœÉ\n\\sigma\nto be\n1.0\n1.0\nand initialize\nŒ±\n\\alpha\nwith the low value of\n0.001\n0.001\n.\nWe also find that using auto-tuning for maximum entropy learning\n(\nhaarnoja2018soft2\n)\nconsistently outperforms using the fixed alpha values. For the target entropy, we find that using\n0.0\n0.0\n(for locomotion tasks) or\n‚àí\n|\nùíú\n|\n/\n2\n-|\\mathcal{A}|/2\n(for whole-body tracking tasks) works best in practice.\nFastTD3: Exploration hyperparameters\nFollowing prior work\n(\nli2023parallel\n;\nseo2025fasttd3\n)\n, we use mixed noise schedule that randomly samples Gaussian noise standard deviation from the range\n[\nœÉ\nmin\n,\nœÉ\nmax\n]\n[\\sigma_{\\texttt{min}},\\sigma_{\\texttt{max}}]\n.\nWe find that using low values, i.e.,\n(\nœÉ\nmin\n,\nœÉ\nmax\n)\n=\n(\n0.01\n,\n0.05\n)\n(\\sigma_{\\texttt{min}},\\sigma_{\\texttt{max}})=(0.01,0.05)\n, performs the best.\nFigure 3:\nLocomotion (velocity tracking) results.\nFastSAC and FastTD3 enable fast training of G1 and T1 humanoid locomotion policies with strong domain randomization such as rough terrain or\nPush-Strong\nthat applies push perturbations to humanoid robots every 1 to 3 seconds (max episode length is 20 seconds).\nFor non-\nPush-Strong\ntasks, we apply push perturbations every 5 to 10 seconds.\nWe use a single RTX 4090 GPU for all locomotion experiments.\nOptimization hyperparameters\nWe train FastSAC and FastTD3 using Adam optimizer\n(\nkingma2014adam\n)\nwith a learning rate of\n0.0003\n0.0003\n.\nWe find that weight decay\n0.1\n0.1\n, which\nseo2025fasttd3\nuses, is a too strong regularization for high-dimensional control tasks and thus we use weight decay of\n0.001\n0.001\n.\nSimilar to\nzhai2023sigmoid\nwhere using low\nŒ≤\n2\n\\beta_{2}\nfor Adam makes training stable with large batch sizes, we find that using\nŒ≤\n2\n=\n0.95\n\\beta_{2}=0.95\nslightly improves stability compared to using\nŒ≤\n2\n=\n0.99\n\\beta_{2}=0.99\n.\nRemark on additional techniques\nWe expect that recent advances in improving off-policy RL\n(\nd'oro2023sampleefficient\n;\nschwarzer2023bigger\n;\nnauman2024bigger\n;\nlee2024simba\n;\nsukhija2024maxinforl\n;\nlee2025hyperspherical\n;\nobando2025simplicial\n)\nwill be helpful for further improving the performance and stability of FastSAC and FastTD3.\nHowever, this work aims to keep the recipe as simple as possible and we expect the research community to advance the state-of-the-art based on our recipe.\n2.2\nSimple Reward Design\nReward design for humanoid locomotion and whole-body control has traditionally depended on heavy reward shaping, often 20+ terms\n(\nmittal2023orbit\n;\nHumanoidVerse\n)\n, e.g., tracking rewards for kinematic quantities, detailed posture regularizer, penalties on joint configurations, foot placement constraints, and shaping terms that strictly prescribe how the robot should move.\nThis complexity makes hyperparameter tuning difficult and often leads to brittle policy optimization.\nInspired by recent works that rely on much simpler reward functions\n(\nzakka2025mujoco\n;\nliao2025beyondmimic\n)\n, we show that robust and natural behaviors can emerge from substantially simpler objectives (less than 10 terms).\nSpecifically, we adopt a minimalist reward philosophy that only adds a reward term if necessarily needed, and aim to have a nearly identical set of rewards across algorithms and robots.\nOur goal is not to enforce a particular style, but to provide enough structure for robust locomotion and whole-body control while preserving behavioral richness.\nFewer reward terms also simplify hyperparameter tuning, enabling rapid sweeps crucial for sim-to-real iteration.\nLocomotion (velocity tracking)\nWe use a compact set of reward terms that cover only the essential components needed for stable humanoid gait transferrable from simulation to the real-world:\n‚Ä¢\nLinear and angular velocity tracking rewards to encourage the humanoid to follow commanded x-y speed and yaw rate. These are the main driver of emergent locomotion.\n‚Ä¢\nA simple foot-height tracking term\n(\nzakka2025mujoco\n;\nphase_rewards\n)\nto guide swing motion.\n‚Ä¢\nA default-pose penalty to avoid extreme joint configurations.\n‚Ä¢\nFeet penalties to encourage parallel relative orientation and prevent foot crossing.\n‚Ä¢\nA per-step alive reward that encourages remaining in valid, non-fallen states.\n‚Ä¢\nPenalties that keep the torso near a stable upright orientation.\n‚Ä¢\nA penalty on the action rate to smooth control outputs.\nWe terminate the episode on ground contact by the torso or other non-foot body parts.\nWe also use symmetry augmentation\n(\nmittal2024symmetry\n)\nto encourage symmetric walking pattern, which we also find to be helpful for faster convergence.\nAll penalties above are subject to a curriculum that ramp up their weights over the course of training as the episode length increases\n(\nHumanoidVerse\n)\n, considerably simplifying exploration.\nWe find that these terms are sufficient to produce robust locomotion across rough terrain, with randomized dynamics and external perturbations, without relying on extensive reward shaping or other carefully tuned heuristics, and are applicable across multiple robots (i.e., G1 and T1) and algorithms (i.e., FastSAC, FastTD3, and PPO).\nWhole-body tracking\nFor whole-body tracking, we follow the reward structure introduced in BeyondMimic\n(\nliao2025beyondmimic\n)\n, which already adheres to the same minimalist principles.\nThese rewards are built around tracking goals with lightweight regularization, together with DeepMimic-style termination conditions\n(\npeng2018deepmimic\n)\n.\nWe additionally find that introducing external disturbances in the form of velocity pushes further robustifies sim-to-real performance.\nFigure 4:\nImprovement from our FastSAC recipe.\nWhile a version of FastSAC was previously considered as a baseline to FastTD3\n(\nseo2025fasttd3\n)\nin the context of humanoid control, a straightforward implementation of FastSAC exhibited training instabilities.\nIn this work, we have stabilized and improved FastSAC with a carefully tuned set of hyperparameters and design choices.\n3\nExperiments\n3.1\nLocomotion (Velocity Tracking)\nSetup\nFor locomotion tasks, we train RL policies to maximize the sum of reward as we described in\nSection\nÀú\n2.2\n, i.e., by training the robots to achieve the target linear and angular velocities while minimizing several penalty terms.\nThroughout training, we randomly sample target velocity commands every 10 seconds.\nWhen sampling the target commands, we randomly set the target velocities to zero with 20% probability, so that the robot learns to stand instead of making it constantly walk on its position.\nUnless otherwise specified, we train all robots on a mix of flat and rough terrains, which stabilizes robot walking in sim-to-real deployment.\nWe apply various domain randomization techniques to further robustify sim-to-real deployment: push perturbations, action delay, PD-gain randomization, mass randomization, friction randomization, and center of mass randomization (only for G1).\nWe report linear velocity tracking reward in all learning curves.\nResults\nFigure\nÀú\n3\nshows that FastSAC and FastTD3 quickly train G1 and T1 humanoid robots to track velocity commands in 15 minutes, significantly outperforming PPO in terms of wall-time clock.\nWe emphasize this is achieved in the existence of strong domain randomization: our humanoids learn to stand and walk in rough terrain, with consistent push perturbations, action delay, center of mass randomization, etc.\nIn particular, we observe that FastSAC and FastTD3 enables fast training of locomotion policies with strong domain randomization such as\nPush-Strong\nthat applies push perturbations to robots every 1 to 3 seconds, while PPO struggles with such strong perturbations.\nWe also find that FastSAC slightly outperforms FastTD3 in several locomotion setups, which we hypothesize to be due to efficient exploration through its maximum entropy exploration scheme.\nFastSAC improvement over previous configuration\nFigure\nÀú\n4\nshows how our recipe for training FastSAC improves the performance over the previous version of FastSAC trained with configuration from\nseo2025fasttd3\n.\nSpecifically, we find that the use of layer normalization\n(\nseo2025fasttd3\n)\n, disabling CDQ\n(\nfujimoto2018addressing\n)\n, careful tuning of exploration and optimization hyperparameters is important for performance improvement (see\nSection\nÀú\n2\nfor details).\nFigure 5:\nWhole-body tracking results.\nWe show that FastSAC and FastTD3 are competitive or superior to PPO in whole-body motion tracking tasks.\nSee\nFigure\nÀú\n6\nfor the sim-to-real deployment of FastSAC policies to real hardware.\nWe use 4\n√ó\n\\times\nL40s GPUs for all whole-body tracking experiments.\nFigure 6:\nWhole-body tracking examples.\nWe demonstrate the sim-to-real deployment of whole-body tracking controllers for Unitree G1 trained with FastSAC (Top: Dance, Middle: Box Lifting, Bottom: Push).\nVideos are available at\nhttps://younggyo.me/fastsac-humanoid\n.\n3.2\nWhole-Body Tracking\nSetup\nFor whole-body tracking, we mostly follow the setup in BeyondMimic\n(\nliao2025beyondmimic\n)\n.\nWe train RL policies to maximize the sum of rewards as we described in\nSection\nÀú\n2.2\n.\nWe report the sum of tracking rewards in all learning curves.\nThroughout training, we randomly sample motion segments for each episode.\nUnlike BeyondMimic that minimizes the use of domain randomization for motion tracking, we find that using various domain randomization techniques stabilizes the behavior at deployment time.\nIn particular, we randomize friction, center of mass, joint position bias, body mass, PD gains, and also apply push perturbations.\nResults\nFigure\nÀú\n5\nshows that FastSAC and FastTD3 can also quickly train G1 humanoid robots to track human motion, being competitive or superior to PPO.\nWe find that FastSAC outperforms FastTD3 in the\nDance\ntask, which is a longer motion compared to the other tasks we considered.\nWe hypothesize better exploration through maximum entropy RL enables faster learning on more challenging tasks.\nFurther investigation into the performance difference between FastSAC and FastTD3 in more diverse types of tasks is an interesting future direction.\nSim-to-real deployment\nIn\nFigure\nÀú\n6\n, we further demonstrate the sim-to-real deployment of FastSAC whole-body tracking policies to real Unitree G1 humanoid hardware.\nWe find that FastSAC policies can complete several motions including the long motion like\nDance\nthat lasts more than 2 minutes.\nThese results show that FastSAC not only enables faster training in simulation but also actually learns deployable robust full-body humanoid control policies.\n4\nRelated Work\nReinforcement learning with massively parallel simulation\nMassively parallel simulation has significantly reduced the wall-clock time required to train RL policies.\nEarly work primarily relied on CPU-based parallelization by launching simulations across multiple processes\n(\nheess2017emergence\n;\nakkaya2019solving\n;\nstooke2018accelerated\n;\nespeholt2018impala\n;\nradosavovic2024learning\n)\n.\nWhile policy learning often leveraged multiple GPUs, overall throughput remains bottlenecked by the process-management overhead and inherently slow simulation speed.\nTo address these limitations, the idea of using GPU-based parallel environments have been proposed\n(\nliang2018gpu\n;\nmakoviychuk2021isaac\n;\nmittal2023orbit\n;\nGenesis\n;\nzakka2025mujoco\n)\n, scaling up environment throughput to thousands of environments.\nThis has been the key driver of recent successes in training controllers for diverse robots with impressive capabilities\n(\nrudin2022learning\n;\nagarwal2023legged\n;\ncheng2024extreme\n;\nsingh2024dextrah\n;\nzhuang2024humanoid\n;\nli2025reinforcement\n;\nhe2025hover\n;\nhe2025asap\n)\n.\nBuilding on this trend, our work focuses on accelerating sim-to-real iterations by combining massively parallel simulation with off-policy RL algorithms tuned for large-scale training regimes.\nAlgorithms for sim-to-real reinforcement learning\nProximal policy optimization (PPO;\nschulman2017proximal\n) has been the de-facto standard algorithm for sim-to-real RL, and is often the only supported algorithm in widely used learning frameworks\n(\nmakoviychuk2021isaac\n;\nmittal2023orbit\n;\nzakka2025mujoco\n;\nschwarke2025rsl\n)\n, largely due to the ease of scaling up on-policy RL with massively parallel environments.\nHowever, recent works have started to demonstrate that off-policy RL methods can also scale effectively in such large-scale training regimes\n(\nli2023parallel\n;\nraffin2025isaacsim\n;\nshukla2025fastsac\n;\nseo2025fasttd3\n)\n.\nNotably,\nseo2025fasttd3\nreport the first sim-to-real deployment of humanoid control policies trained with FastTD3, an efficient variant of TD3\n(\nfujimoto2018addressing\n)\noptimized for large-batch training with parallel simulation.\nHowever, their results were limited to humanoid controllers only with a subset of joints.\nIn this work, we further push this direction by developing a sim-to-real RL recipe based on FastSAC and FastTD3 that achieve full-body humanoid control that controls all joints for locomotion or follows human motion.\nWhile doing so, we have also stabilized and improved the performance of FastSAC, which has been shown to exhibit training instabilities for humanoid control in prior work\n(\nseo2025fasttd3\n)\n, with careful design choices.\nAlgorithm 1\nFastSAC: Pseudocode (distributional critic is omitted for simplicity)\n1:\nInitialize actor\nœÄ\nŒ∏\n\\pi_{\\theta}\n, two critics\nQ\nœï\n1\n,\nQ\nœï\n2\nQ_{\\phi_{1}},Q_{\\phi_{2}}\n, entropy temperature\nŒ±\n\\alpha\n, replay buffer\n‚Ñ¨\n\\mathcal{B}\n2:\nInitialize target critics\nQ\nœï\n1\ntarget\n,\nQ\nœï\n2\ntarget\nQ_{\\phi^{\\texttt{target}}_{1}},Q_{\\phi^{\\texttt{target}}_{2}}\nwith\nœï\n1\ntarget\n‚Üê\nœï\n1\n\\phi^{\\texttt{target}}_{1}\\leftarrow\\phi_{1}\nand\nœï\n2\ntarget\n‚Üê\nœï\n2\n\\phi^{\\texttt{target}}_{2}\\leftarrow\\phi_{2}\n3:\nfor\neach environment step\ndo\n4:\nSample\na\n‚àº\nœÄ\nŒ∏\n‚Äã\n(\no\n)\na\\sim\\pi_{\\theta}(o)\ngiven the current observation\no\no\n, and take action\na\na\n5:\nObserve next state\no\n‚Ä≤\no^{\\prime}\nand reward\nr\n‚Ä≤\nr^{\\prime}\n6:\nStore transition\nœÑ\n=\n(\no\n,\na\n,\no\n‚Ä≤\n,\nr\n‚Ä≤\n)\n\\tau=(o,a,o^{\\prime},r^{\\prime})\nin replay buffer\n‚Ñ¨\n‚Üê\n‚Ñ¨\n‚à™\n{\nœÑ\n}\n\\mathcal{B}\\leftarrow\\mathcal{B}\\cup\\{\\tau\\}\n7:\nfor\nj\n=\n1\nj=1\nto\nnum_updates\ndo\n8:\nSample mini-batch\nB\n=\n{\nœÑ\nk\n}\nk\n=\n1\n|\nB\n|\nB=\\{\\tau_{k}\\}_{k=1}^{|B|}\nfrom\n‚Ñ¨\n\\mathcal{B}\n9:\nCompute target Q-value via average:\n10:\ny\n=\nr\n‚Ä≤\n+\nŒ≥\n2\n‚Äã\n‚àë\ni\n=\n1\n2\n(\nQ\nœï\ni\ntarget\n‚Äã\n(\no\n‚Ä≤\n,\na\n~\n‚Ä≤\n)\n‚àí\nŒ±\n‚Äã\nlog\n‚Å°\nœÄ\nŒ∏\n‚Äã\n(\na\n~\n‚Ä≤\n|\no\n‚Ä≤\n)\n)\ny=r^{\\prime}+\\dfrac{\\gamma}{2}\\displaystyle\\sum_{i=1}^{2}\\left(Q_{\\phi^{\\texttt{target}}_{i}}(o^{\\prime},\\tilde{a}^{\\prime})-\\alpha\\log\\pi_{\\theta}(\\tilde{a}^{\\prime}|o^{\\prime})\\right)\nwith\na\n~\n‚Ä≤\n‚àº\nœÄ\nŒ∏\n(\n‚ãÖ\n|\no\n‚Ä≤\n)\n\\tilde{a}^{\\prime}\\sim\\pi_{\\theta}(\\cdot|o^{\\prime})\n11:\nUpdate critic:\n12:\nœï\ni\n‚Üê\nœï\ni\n‚àí\n‚àá\nœï\ni\n1\n|\nB\n|\n‚Äã\n‚àë\nœÑ\nk\n‚àà\nB\n(\nQ\nœï\ni\n‚Äã\n(\no\n,\na\n)\n‚àí\ny\n)\n2\n\\phi_{i}\\leftarrow\\phi_{i}-\\nabla_{\\phi_{i}}\\dfrac{1}{|B|}\\displaystyle\\sum_{\\tau_{k}\\in B}\\Big(Q_{\\phi_{i}}(o,a)-y\\Big)^{2}\nfor\ni\n‚àà\n{\n1\n,\n2\n}\ni\\in\\{1,2\\}\n13:\nUpdate actor with reparameterization trick:\n14:\nŒ∏\n‚Üê\nŒ∏\n+\n‚àá\nŒ∏\n1\n2\n‚Äã\n|\nB\n|\n‚Äã\n‚àë\nœÑ\nk\n‚àà\nB\n‚àë\ni\n=\n1\n2\n(\nQ\nœï\ni\n‚Äã\n(\no\n,\na\n~\n)\n‚àí\nŒ±\n‚Äã\nlog\n‚Å°\nœÄ\nŒ∏\n‚Äã\n(\na\n~\n|\no\n)\n)\n\\theta\\leftarrow\\theta+\\nabla_{\\theta}\\dfrac{1}{2|B|}\\displaystyle\\sum_{\\tau_{k}\\in B}\\sum_{i=1}^{2}\\Big(Q_{\\phi_{i}}(o,\\tilde{a})-\\alpha\\log\\pi_{\\theta}(\\tilde{a}|o)\\Big)\nwith\na\n~\n‚àº\nœÄ\nŒ∏\n(\n‚ãÖ\n|\no\n)\n\\tilde{a}\\sim\\pi_{\\theta}(\\cdot|o)\n15:\nUpdate entropy temperature:\n16:\nŒ±\n‚Üê\nŒ±\n‚àí\n‚àá\nŒ±\n1\n|\nB\n|\n‚Äã\n‚àë\nœÑ\nk\n‚àà\nB\n(\n‚Ñã\ntarget\n‚àí\n‚Ñã\n‚Äã\n(\no\n)\n)\n‚ãÖ\nŒ±\n\\alpha\\leftarrow\\alpha-\\nabla_{\\alpha}\\dfrac{1}{|B|}\\displaystyle\\sum_{\\tau_{k}\\in B}(\\mathcal{H}^{\\texttt{target}}-\\mathcal{H}(o))\\cdot\\alpha\n17:\nUpdate target critic\nœï\ni\ntarget\n‚Üê\nœÅ\n‚Äã\nœï\ni\ntarget\n+\n(\n1\n‚àí\nœÅ\n)\n‚Äã\nœï\ni\n\\phi^{\\texttt{target}}_{i}\\leftarrow\\rho\\phi^{\\texttt{target}}_{i}+(1-\\rho)\\phi_{i}\nfor\ni\n‚àà\n{\n1\n,\n2\n}\ni\\in\\{1,2\\}\n18:\nend\nfor\n19:\nend\nfor\n5\nConclusion\nBy combining FastSAC and FastTD3, scalable off-policy RL algorithms, with a streamlined training pipeline, our recipe closes the gap between the promise of high-throughput parallel simulation and the practical demands of sim-to-real humanoid learning.\nIn particular, we show that off-policy RL algorithms can be scaled effectively to reduce sim-to-real iteration time for learning whole-body humanoid controllers.\nIn this work, we have intentionally maintained a simple, minimalist design that other researchers can easily build upon.\nWe expect that incorporating recent advances in off-policy RL and humanoid learning into this recipe will push the state-of-the-art even further.\nTo support such progress, we provide an open-source implementation of our recipe\n(\nHolosoma\n)\n.\nWe hope this report serves as a blueprint for researchers aiming to rapidly iterate on humanoid policies.\nReferences",
  "preview_text": "Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive scale with thousands of parallel environments through carefully tuned design choices and minimalist reward functions. We demonstrate rapid end-to-end learning of humanoid locomotion controllers on Unitree G1 and Booster T1 robots under strong domain randomization, e.g., randomized dynamics, rough terrain, and push perturbations, as well as fast training of whole-body human-motion tracking policies. We provide videos and open-source implementation at: https://younggyo.me/fastsac-humanoid.\n\nLearning Sim-to-Real Humanoid Locomotion\nin 15 Minutes\nYounggyo Seo\n‚àó\nCarmelo Sferrazza\n‚àó\nJuyue Chen\nGuanya Shi\nRocky Duan\nPieter Abbeel\nAmazon FAR (Frontier AI & Robotics)\nAbstract\nMassively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes.\nHowever, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization.\nIn this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e.,\nFastSAC\nand\nFastTD3\n, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU.\nOur simple recipe stabilizes off-policy RL algorithms at massive scale with thousands of parallel environments through carefully tuned design choices and minimalist reward functions.\nWe demonstr",
  "is_relevant": null,
  "relevance_score": 0.0,
  "extracted_keywords": [],
  "one_line_summary": "",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "2025-12-01T18:55:17Z",
  "created_at": "2026-01-08T10:08:13.784708",
  "updated_at": "2026-01-08T10:08:13.784716"
}