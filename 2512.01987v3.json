{
    "id": "2512.01987v3",
    "title": "Forecasting in Offline Reinforcement Learning for Non-stationary Environments",
    "authors": [
        "Suzan Ece Ada",
        "Georg Martius",
        "Emre Ugur",
        "Erhan Oztop"
    ],
    "abstract": "ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºåœ¨æ— æ³•æ”¶é›†é¢å¤–äº¤äº’æ•°æ®æ—¶ï¼Œåˆ©ç”¨é¢„æ”¶é›†æ•°æ®é›†è®­ç»ƒç­–ç•¥æä¾›äº†ä¸€æ¡æœ‰å‰æ™¯çš„é€”å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰ç¦»çº¿RLæ–¹æ³•é€šå¸¸å‡è®¾ç¯å¢ƒå…·æœ‰å¹³ç¨³æ€§ï¼Œæˆ–ä»…åœ¨æµ‹è¯•æ—¶è€ƒè™‘åˆæˆæ‰°åŠ¨ï¼Œè¿™äº›å‡è®¾åœ¨ç°å®åœºæ™¯ä¸­å¾€å¾€ä¸æˆç«‹â€”â€”ç°å®åœºæ™¯å¸¸ä»¥çªå‘ã€æ—¶å˜çš„åç§»ä¸ºç‰¹å¾ã€‚æ­¤ç±»åç§»å¯èƒ½å¯¼è‡´éƒ¨åˆ†å¯è§‚æµ‹æ€§ï¼Œä½¿æ™ºèƒ½ä½“è¯¯åˆ¤çœŸå®çŠ¶æ€å¹¶é™ä½æ€§èƒ½ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†éå¹³ç¨³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„é¢„æµ‹æ¡†æ¶ï¼ˆFORLï¼‰ï¼Œè¯¥æ¡†æ¶èåˆäº†ï¼ˆiï¼‰åŸºäºæ¡ä»¶æ‰©æ•£çš„å€™é€‰çŠ¶æ€ç”Ÿæˆæ–¹æ³•ï¼ˆè®­ç»ƒæ—¶ä¸é¢„è®¾ä»»ä½•ç‰¹å®šçš„æœªæ¥éå¹³ç¨³æ¨¡å¼ï¼‰ï¼Œä»¥åŠï¼ˆiiï¼‰é›¶æ ·æœ¬æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ã€‚FORLé’ˆå¯¹æ˜“å—æ„å¤–ä¸”å¯èƒ½éé©¬å°”å¯å¤«åç§»å½±å“çš„ç¯å¢ƒï¼Œè¦æ±‚æ™ºèƒ½ä½“åœ¨æ¯ä¸ªå›åˆå¼€å§‹æ—¶å³å…·å¤‡é²æ£’æ€§èƒ½ã€‚é€šè¿‡åœ¨ç¦»çº¿RLåŸºå‡†æµ‹è¯•ä¸­å¼•å…¥çœŸå®ä¸–ç•Œæ—¶é—´åºåˆ—æ•°æ®ä»¥æ¨¡æ‹Ÿå®é™…éå¹³ç¨³æ€§ï¼Œå®è¯è¯„ä¼°è¡¨æ˜ï¼ŒFORLç›¸è¾ƒäºç°æœ‰ç«äº‰åŸºçº¿æ–¹æ³•èƒ½æŒç»­æå‡æ€§èƒ½ã€‚é€šè¿‡å°†é›¶æ ·æœ¬é¢„æµ‹ä¸æ™ºèƒ½ä½“ç»éªŒç›¸ç»“åˆï¼Œæˆ‘ä»¬è‡´åŠ›äºå¼¥åˆç¦»çº¿RLä¸ç°å®ä¸–ç•Œå¤æ‚éå¹³ç¨³ç¯å¢ƒä¹‹é—´çš„é¸¿æ²Ÿã€‚",
    "url": "https://arxiv.org/abs/2512.01987v3",
    "html_url": "https://arxiv.org/html/2512.01987v3",
    "html_content": "Forecasting in Offline Reinforcement Learning for Non-stationary Environments\nSuzan Ece Ada\n1,2\nGeorg Martius\n2\nEmre Ugur\n1\nErhan Oztop\n3,4\n1\nBogazici University, TÃ¼rkiye\n2\nUniversity of TÃ¼bingen, Germany\n3\nOzyegin University, TÃ¼rkiye\n4\nOsaka University, Japan\nece.ada@bogazici.edu.tr\nAbstract\nOffline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce\nF\norecasting in Non-stationary\nO\nffline\nRL\n(\nForl\n), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models.\nForl\ntargets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that\nForl\nconsistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agentâ€™s experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.\nFigure 1\n:\nSetting.\nThe agent does not know its location in the environment because its perception is offset every episode\nj\nj\nby an unknown offset\nb\nj\nb^{j}\n(only vertical offsets are illustrated).\nForl\nleverages historical offset data and offline RL data (from a stationary phase) to forecast and correct for new offsets at test time. Ground-truth offsets (\n,\n) are hidden throughout the evaluation episodes.\n1\nIntroduction\nOffline Reinforcement Learning (RL) leverages static datasets to avoid costly or risky online interactions\n[\n1\n,\n2\n]\n. Yet, agents trained on fully observable states often fail when deployed with noisy or corrupted observations. While robust offline RL methods address test-time perturbations, such as sensor noise or adversarial attacks\n[\n3\n]\n, a critical gap persists in addressing non-stationarity within the observation functionâ€”a challenge that fundamentally alters the agentâ€™s perception of the environment over time.\nPrior\nonline algorithms\nthat consider the scope of non-stationarity as the observation function focus on learning agent morphologies\n[\n4\n]\nand generalization in Block MDPs\n[\n5\n]\n. While this scope of non-stationarity holds significant potential for real-world applications\n[\n6\n]\n, it remains underexplored. We focus on the episodic evolution of the observation function at test-time in offline RL. In our setup, each dimension of an agentâ€™s state is influenced by an unknown, time-dependent constant additive offset that remains fixed within a single operational interval (an â€œepisodeâ€). This leads to a stream of evolving observation functions\n[\n7\n]\n, extending across multiple future episodes,\nwhere the offsets remain hidden throughout the prediction window\n. For instance, industrial robots might apply a daily calibration offset to each joint, while sensors can exhibit a deviation until the next scheduled recalibration. Similarly, in healthcare or finance, data may be partially aggregated or withheld to comply with regulations, effectively obscuring finer-grained variations and leaving a single offset as the dominant factor per episode. By only storing these representative offsets, we circumvent the challenges of continuous interaction buffers in bandwidth-constrained or privacy-sensitive environments. Because the offset can differ across state dimensions (e.g., different sensor or actuation channels), each state dimension can be affected by a different unknown bias that stays constant for that episode but evolves differently across episodes. Approaches that assume predefined perturbations can struggle with these abrupt, episodic shifts because such offsets violate the typical assumption of smoothly varying observation functions. Frequent retraining, hyperparameter optimization, or extensive online adaptation to new observation function evolution patterns are costly, risky (due to trial-and-error in safety-critical settings), and may be infeasible if these patterns no longer reflect assumptions made during training. By separating offset data (episodic calibration values) from the massive replay buffers, a zero-shot forecasting-based approach can anticipate each new offset from the beginning of the episode without requiring policy updates or making assumptions on task evolution at test time\n[\n8\n]\n. Modeling these multidimensional additive offsets as stable, per-episode constants presents a robust and efficient way to handle time-varying conditions in non-stationary environments where the evolution of tasks follows a non-Markovian, time-series pattern\n[\n9\n]\n, mitigating the risks of online exploration.\nFigure 2\n:\nOverview of the proposed\nForl\nframework at test-time. The observations are processed by both the trajectory buffer and the time-series\nforecasting foundation\nmodule\n[\n10\n]\n. Observation changes and actions sampled from the\npolicy\n(\nÎ”\nâ€‹\no\n,\na\n)\n(\\Delta o,a)\n, are stored in the trajectory buffer. The\ndiffusion model\ngenerates candidate states\n{\nğ’”\nt\n(\n0\n)\n}\nk\n\\{\\boldsymbol{s}_{t}^{(0)}\\}_{k}\nconditioned on\nğ‰\n(\nt\n,\nw\n)\n\\boldsymbol{\\tau}_{(t,w)}\n. The\ncandidate selection\nmodule then generates the estimated\ns\n~\nt\n\\tilde{s}_{t}\n.\nWe consider an offline RL setting during training where we have only access to a standard offline RL dataset collected from a stationary environment\n[\n3\n]\nwith fully observable states. Initial data may be collected under near-ideal conditions and then gradually affected by wear, tear, or other natural shiftsâ€”even as the underlying physical laws (dynamics) remain unchanged. At test time, however, we evaluate in a non-stationary environment where both the observation function and the observation space change due to time-dependent external factors. This setup can be interpreted as environments shifting along observation space dimensions while the initial state of the agent is sampled from a uniform distribution over the state space. A simplified version of this setup for an offset affecting only one dimension of the state is illustrated in Figure\n1\n. Here, the agent â€œknowsâ€ it is in a maze but does not know where it is in the maze. Furthermore,\nit will remain uncertain of its location across all episodes at test-time\n, as in every episode, a new offset leads to a systematic misalignment between perceived and actual positions. Importantly, these offsets may not conform to Gaussian or Markovian assumptions; instead, they may stem directly from complex, real-world time-series data\n[\n9\n]\nand remain constant throughout each episode. As a result, standard noise-driven or parametric state-estimation techniques, which typically rely on smoothly varying or randomly perturbed functions, cannot reliably identify these persistent, episode-wide offsets that are not available after the episode terminates. While zero-shot forecasting can adjust observation offsets, its performance depends on the forecasterâ€™s accuracy. Similarly, integrating zero-shot forecasting into a model-based offline RL approach\n[\n3\n]\ncan underperform when real-world offsets deviate from predefined assumptions about future observation functions. Our approach uses the insight that the belief of the true states can be refined from a sequence of actions and effects. For instance, in maze navigation, if an agent misjudges its location and hits a wall, analyzing its actions and delta observations leading to the collision can provide evidence for likely locations within the maze.\nWe propose the\nF\norecasting in Non-stationary\nO\nffline\nRL\n(\nForl\n) framework (Figure\n2\n) for test-time adaptation in non-stationary environments where the observation function is perturbed by an arbitrary time-series.\nOur framework has two main ingredients:\nforecasting offsets with a zero-shot time-series forecasting model\n[\n10\n]\nfrom past episode offsets (ground truth offsets after the episode terminates are not accessible at test-time) and\na within-episode update of the state estimation using a conditional diffusion model\n[\n11\n]\ntrained on offline stationary data.\nContributions.\nWe unify the strengths of probabilistic forecasting and decision-making under uncertainty to enable continuous adaptation when the environment diverges from predictions. Consequently, our framework:\n(1)\naccommodates future offsets\nwithout assuming specific non-stationarity patterns during training\n, eliminating the need for retraining and hyperparameter tuning when the agent encounters new, unseen non-stationary patterns at test time, and\n(2)\ntargets non-trivial non-stationarities at test time without requiring environment interaction or knowledge of POMDPs during training\n.\n(3)\nForl\nintroduces a novel, modular framework combining a conditional diffusion model (\nForl\n-\nDm\n) for multimodal belief generation with a lightweight Dimension-wise Closest Match (DCM) fusion strategy, validated by extensive experiments on no-access to past offsets, policy-agnostic plug-and-play, offset magnitude-scaling, and inter/intra-episode drifts.\n(4)\nWe propose a novel benchmark that integrates offsets from real-world time-series datasets with standard offline RL benchmarks and demonstrate that\nForl\nconsistently outperforms baseline methods.\nBackground: Diffusion Models\nDenoising diffusion models\n[\n12\n,\n13\n]\naim to model the data distribution with\np\nÎ¸\nâ€‹\n(\nğ’™\n0\n)\n:=\nâˆ«\np\nÎ¸\nâ€‹\n(\nğ’™\n0\n:\nN\n)\nâ€‹\nğ‘‘\nğ’™\n1\n:\nN\np_{\\theta}(\\boldsymbol{x}_{0}):=\\int p_{\\theta}(\\boldsymbol{x}_{0:N})\\,d\\boldsymbol{x}_{1:N}\nfrom samples\nx\n0\nx_{0}\nin the dataset. The joint distribution follows the Markov Chain\np\nÎ¸\nâ€‹\n(\nğ’™\n0\n:\nN\n)\n:=\nğ’©\nâ€‹\n(\nğ’™\nN\n;\nğŸ\n,\nğˆ\n)\nâ€‹\nâˆ\nn\n=\n1\nN\np\nÎ¸\nâ€‹\n(\nğ’™\nn\nâˆ’\n1\nâˆ£\nğ’™\nn\n)\np_{\\theta}\\left(\\boldsymbol{x}_{0:N}\\right):=\\mathcal{N}(\\boldsymbol{x}_{N};\\mathbf{0},\\mathbf{I})\\prod_{n=1}^{N}p_{\\theta}\\left(\\boldsymbol{x}_{n-1}\\mid\\boldsymbol{x}_{n}\\right)\\quad\nwhere\nğ’™\nn\n\\boldsymbol{x}_{n}\nis the noisy sample for diffusion timestep\nn\nn\nand\np\nÎ¸\nâ€‹\n(\nğ’™\nn\nâˆ’\n1\nâˆ£\nğ’™\nn\n)\n:=\nğ’©\nâ€‹\n(\nğ’™\nn\nâˆ’\n1\n;\nğ\nÎ¸\nâ€‹\n(\nğ’™\nn\n,\nn\n)\n,\nğšº\nÎ¸\nâ€‹\n(\nğ’™\nn\n,\nn\n)\n)\np_{\\theta}\\left(\\boldsymbol{x}_{n-1}\\mid\\boldsymbol{x}_{n}\\right):=\\mathcal{N}\\left(\\boldsymbol{x}_{n-1};\\boldsymbol{\\mu}_{\\theta}\\left(\\boldsymbol{x}_{n},n\\right),\\mathbf{\\Sigma}_{\\theta}\\left(\\boldsymbol{x}_{n},n\\right)\\right)\n. During training, we use the samples from the distribution\nq\nâ€‹\n(\nğ’™\nn\nâˆ£\nğ’™\n0\n)\n=\nğ’©\nâ€‹\n(\nğ’™\nn\n;\nÎ±\nÂ¯\nn\nâ€‹\nğ’™\n0\n,\n(\n1\nâˆ’\nÎ±\nÂ¯\nn\n)\nâ€‹\nğˆ\n)\nq\\left(\\boldsymbol{x}_{n}\\mid\\boldsymbol{x}_{0}\\right)=\\mathcal{N}\\left(\\boldsymbol{x}_{n};\\sqrt{\\bar{\\alpha}_{n}}\\boldsymbol{x}_{0},\\left(1-\\bar{\\alpha}_{n}\\right)\\mathbf{I}\\right)\nwhere\nÎ±\nÂ¯\nn\n=\nâˆ\ni\n=\n1\nn\nÎ±\ni\n\\bar{\\alpha}_{n}=\\prod_{i=1}^{n}\\alpha_{i}\n[\n11\n]\n. General information on diffusion models is given in\nSection\nËœ\nB.3\n.\n2\nMethod\nIn this section, we formulate our problem statement and describe our\nForl\ndiffusion model trained on the offline RL dataset to predict plausible states. Then, we introduce our online state estimation method,\nDimension-wise Closest Match that uses plausible states predicted by the multimodal\nForl\ndiffusion model (\nDm\n) and the states predicted from past episodes prior to evaluation by a probabilistic unimodal zero-shot time-series foundation model.\n2.1\nProblem Statement\nTraining (Offline Stationary MDP)\nWe begin with an episodic, stationary Markov Decision Process (MDP)\nâ„³\ntrain\n=\n(\nğ’®\n,\nğ’œ\n,\nğ’¯\n,\nâ„›\n,\nÏ\n0\n)\n\\mathcal{M}_{\\text{train}}=(\\mathcal{S},\\mathcal{A},\\mathcal{T},\\mathcal{R},\\rho_{0})\n, where the initial state distribution\nÏ\n0\n\\rho_{0}\nis a uniform distribution over the state space\nğ’®\n\\mathcal{S}\n. We only have access to an offline RL dataset\nğ’Ÿ\n=\n{\n(\nğ’”\nt\nk\n,\nğ’‚\nt\nk\n,\nğ’”\nt\n+\n1\nk\n,\nr\nt\nk\n)\n}\n\\mathcal{D}=\\{(\\boldsymbol{s}^{k}_{t},\\boldsymbol{a}^{k}_{t},\\boldsymbol{s}^{k}_{t+1},r^{k}_{t})\\}\nwith\nk\nk\ntransitions collected from this MDP. Crucially, our\nForl\ndiffusion model and a diffusion policy\n[\n14\n]\nare trained offline using this dataset, such as the standard D4RL benchmark\n[\n15\n]\n, without making any assumptions about how the environment might become nonâ€stationary at test time.\nTest Environment (Sequence of POMDPs)\nAt test time, the agent faces an infinite sequence of POMDPs\n{\nâ„³\n^\nj\n}\nj\n=\n1\nâˆ\n\\{\\hat{\\mathcal{M}}_{j}\\}_{j=1}^{\\infty}\n. Each POMDP is described by a 7-tuple\n[\n16\n]\nâ„³\n^\nj\n=\n(\nğ’®\n,\nğ’œ\n,\nğ’ª\nj\n,\nğ’¯\n,\nâ„›\n,\nÏ\n0\n,\nğ±\nj\n)\n\\hat{\\mathcal{M}}_{j}=\\bigl(\\mathcal{S},\\mathcal{A},\\mathcal{O}_{j},\\mathcal{T},\\mathcal{R},\\rho_{0},\\mathbf{x}_{j}\\bigr)\n, where the state space\nğ’®\n\\mathcal{S}\n, action space\nğ’œ\n\\mathcal{A}\n, transition function\nğ’¯\n\\mathcal{T}\n, and the reward function\nâ„›\n\\mathcal{R}\nremain identical to the training MDP.\nğ±\n\\mathbf{x}\nis the observation function, where we restrict ourselves to deterministic versions (\nğ±\n:\nğ’®\nâ†’\nğ’ª\n\\mathbf{x}:\\mathcal{S}\\rightarrow\\mathcal{O}\n)\n[\n17\n,\n6\n]\n.\nNon-stationary environments can be formulated in different ways.\nIn\nKhetarpal etÂ al. [\n6\n]\n, a\ngeneral non-stationary RL\nformulation is put forward,\nwhich allows each component of the underlying MDP or POMDP to evolve over time, i.e.\n(\nğ’®\nâ€‹\n(\nt\n)\n,\nğ’œ\nâ€‹\n(\nt\n)\n,\nğ’¯\nâ€‹\n(\nt\n)\n,\nâ„›\nâ€‹\n(\nt\n)\n,\nğ±\nâ€‹\n(\nt\n)\n,\nğ’ª\nâ€‹\n(\nt\n)\n)\n\\bigl(\\mathcal{S}(t),\\mathcal{A}(t),\\mathcal{T}(t),\\mathcal{R}(t),\\mathbf{x}(t),\\mathcal{O}(t)\\bigr)\n.\nA set\nÎº\n\\kappa\nspecifies which of these components vary, and a\ndriver\ndetermines how they evolve. In particular,\npassive\ndrivers of non-stationarity imply that exogenous factors alone govern the evolution of the environment, independent of the agentâ€™s actions.\nIn this work, we consider the scope of non-stationarity\n[\n6\n]\n(\nSection\nËœ\nB.2\n) to be the observation function and the observation space, i.e.\nÎº\n=\n{\nğ±\n,\nğ’ª\n}\n\\kappa=\\{\\mathbf{x},\\mathcal{O}\\}\n.\nWe consider the case where non-stationarity unfolds over episodes and where the observation function\nğ±\nj\n\\mathbf{x}_{j}\nis different in each episode\nj\nj\n.\nThe change in the observation function is assumed to have an additive structure and is independent of the agentâ€™s actions (passive nonâ€stationarity\n[\n6\n]\n).\nConcretely, the function\nğ±\nj\n\\mathbf{x}_{j}\noffsets states\ns\nt\ns_{t}\nby a fixed offset\nb\nj\nâˆˆ\nâ„\nn\nb^{j}\\in\\mathbb{R}^{n}\n:\nğ’ª\nj\n=\n{\ns\n+\nb\nj\n:\ns\nâˆˆ\nğ’®\n}\n,\nğ±\nj\nâ€‹\n(\ns\n)\n=\ns\n+\nb\nj\n.\n\\mathcal{O}_{j}\\;=\\;\\{\\,s+b^{j}:\\,s\\in\\mathcal{S}\\},\\quad\\mathbf{x}_{j}(s)\\;=\\;s+b^{j}.\nImportantly, the sequence\n(\nb\nj\n)\n(b^{j})\ncan evolve under arbitrary realâ€world timeâ€series data, and the agent\ndoes not have access to the ground-truth information throughout the evaluation\nâ€”similar to scenarios where observations are only available periodically and shifts occur between these intervals. Thus, the episodes have a temporal order, relating to Non-Stationary Decision Processes (NSDP), defining a sequence of POMDPs\n[\n7\n]\n(\nSection\nËœ\nB.2\n).\nPartial Observability and Historical Context\nSince\nb\nj\nb^{j}\nis\nnever directly observed\nfor\nP\nP\nepisodes into the future, each\nâ„³\n^\nj\n\\hat{\\mathcal{M}}_{j}\nis a POMDP. The agent receives only the offsetâ€shifted observations\n(\no\nt\n)\n(o_{t})\n, where\no\nt\n=\ns\nt\n+\nb\nj\no_{t}=s_{t}+b^{j}\nwithout any noise. Moreover, the agent may have access to a limited historical context of previous offsets\n(\nb\nj\nâˆ’\nC\n,\nâ€¦\n,\nb\nj\nâˆ’\n1\n)\n(b^{j-C},\\dots,b^{j-1})\nat discrete intervals\nP\nP\n, but\nno direct information\nabout future offsets\n(\nb\nj\n,\nb\nj\n+\n1\n,\nâ€¦\nâ€‹\nb\nj\n+\nP\nâˆ’\n1\n)\n(b^{j},b^{j+1},\\dots b^{j+P-1})\n.\nHence, the agent must forecast and/or adapt to unknown future offsets without prior nonâ€stationary training.\nPartial Identifiability\nDespite observing\no\nt\n=\ns\nt\n+\nb\nj\no_{t}=s_{t}+b^{j}\n, the agent cannot generally disentangle\ns\nt\ns_{t}\nfrom\nb\nj\nb^{j}\n.\nFor any single observation, there are infinite possible pairs of state and offset that yield\no\nt\n=\ns\nâ€²\n+\nb\nâ€²\no_{t}=s^{\\prime}+b^{\\prime}\n. Additionally, the initial state distribution\nÏ\n\\rho\nis uniform and does not provide information about\nb\nb\n.\nThus, we can only form a belief over\ns\nt\ns_{t}\nand refine that belief based on two sources of information:\na)\nthe sequence of actions and effects observed within an episode and\nb)\nthe sequence of past identified offsets.\nTo exploit source\na\n, we utilize a predictive model of commonly expected outcomes based on a diffusion model, which will be explained next. To make use of source\nb\n, we use a zero-shot forecasting model (see\nSection\nËœ\n2.3\nfor details). Afterwards, both pieces of information are fused (\nSection\nËœ\n2.4\n).\nFigure 3\n:\nCandidate states generated by\nForl\nDiffusion Model.\n2.2\nForl\nDiffusion Model\nIn our setting, we assume that the offsets added to the states are unobservable at test time, while the transition dynamics of the evaluation environment remain unchanged.\nTo eventually reduce the uncertainty about the underlying state,\nwe perform filtering or belief updates using the sequence of past interactions.\nTo understand why the history of interactions is indicative of a particular ground truth state, consider the following example in a maze environment illustrated in\nFig.\nËœ\n3\n. When the agent moves north for three steps and then bumps into a wall, the possible ground truth states can only be those three steps south of any wall. The agent cannot observe the hidden green trajectory of ground-truth states; it only has access to the sequence of observation changes\n(\nÎ”\nâ€‹\no\n)\n(\\Delta o)\nand action vectors, which narrows down the possible positions to four candidate regionsâ€”exactly those identified by our model.\nClearly, the distribution of possible states is highly multimodal,\nsuch that we propose using a diffusion model as a flexible predictive model of plausible states given the observed actions and outcomes.\nDiffusion models excel at capturing multimodal distributions\n[\n14\n]\n, making them well-suited for our task.\nWe train the diffusion model on offline data without offsets (\nb\nj\n=\n0\nb_{j}=0\n) which we detail below.\nTo distinguish between the trajectory timesteps in reinforcement learning (RL) and the timesteps in the diffusion process, we use the subscript\nt\nâˆˆ\n{\n0\n,\nâ€¦\n,\nT\n}\nt\\in\\{0,\\dots,T\\}\nto refer to RL timesteps and\nn\nâˆˆ\n{\n0\n,\nâ€¦\n,\nN\n}\nn\\in\\{0,\\dots,N\\}\nfor diffusion timesteps. We first begin by defining a subsequence of a trajectory\nğ‰\n(\nt\n,\nw\n)\n=\n[\n(\nÎ”\nâ€‹\no\nt\nâˆ’\nw\n+\n1\n,\na\nt\nâˆ’\nw\n)\n,\nâ‹¯\n,\n(\nÎ”\nâ€‹\no\nt\n,\na\nt\nâˆ’\n1\n)\n]\n.\n\\boldsymbol{\\tau}_{(t,w)}=\\left[\\left(\\Delta o_{t-w+1},a_{t-w}\\right),\\cdots,\\left(\\Delta o_{t},a_{t-1}\\right)\\right].\n(1)\nwhere delta observations\nÎ”\nâ€‹\no\nt\n=\no\nt\nâˆ’\no\nt\nâˆ’\n1\n=\ns\nt\nâˆ’\ns\nt\nâˆ’\n1\n\\Delta o_{t}=o_{t}-o_{t-1}=s_{t}-s_{t-1}\ndenote the state changes (effects),\nw\nw\nis the window size. Using a conditional diffusion model, we aim to model the distribution\np\nâ€‹\n(\nğ’”\nt\nâˆ£\nğ‰\n(\nt\n,\nw\n)\n)\np\\left(\\boldsymbol{s}_{t}\\mid\\boldsymbol{\\tau}_{(t,w)}\\right)\n. For that, we define the reverse process (denoising) as\np\nâ€‹\n(\ns\nt\n(\nN\n)\n)\nâ€‹\nâˆ\nn\n=\n1\nN\np\nÎ¸\nâ€‹\n(\nğ’”\nt\n(\nn\nâˆ’\n1\n)\nâˆ£\nğ’”\nt\n(\nn\n)\n,\nğ‰\n(\nt\n,\nw\n)\n)\n,\np\nâ€‹\n(\ns\nt\n(\nN\n)\n)\n=\nğ’©\nâ€‹\n(\nğ’”\nt\n(\nN\n)\n;\nğŸ\n,\nğˆ\n)\np\\big(s^{(N)}_{t}\\big)\\prod_{n=1}^{N}p_{\\theta}\\left(\\boldsymbol{s}_{t}^{(n-1)}\\mid\\boldsymbol{s}_{t}^{(n)},\\boldsymbol{\\tau}_{(t,w)}\\right),\\quad p\\big(s^{(N)}_{t}\\big)=\\mathcal{N}(\\boldsymbol{s}^{(N)}_{t};\\mathbf{0},\\mathbf{I})\n(2)\nand\np\nÎ¸\np_{\\theta}\nis modeled as the distribution\nğ’©\nâ€‹\n(\nğ’”\nt\n(\nn\nâˆ’\n1\n)\n;\nÎ¼\nÎ¸\nâ€‹\n(\nğ’”\nt\n(\nn\n)\n,\nğ‰\n(\nt\n,\nw\n)\n,\nn\n)\n,\nÎ£\nÎ¸\nâ€‹\n(\nğ’”\nt\n(\nn\n)\n,\nğ‰\n(\nt\n,\nw\n)\n,\nn\n)\n)\n\\mathcal{N}(\\boldsymbol{s}^{(n-1)}_{t};\\mu_{\\theta}(\\boldsymbol{s}^{(n)}_{t},\\boldsymbol{\\tau}_{(t,w)},n),\\Sigma_{\\theta}(\\boldsymbol{s}^{(n)}_{t},\\boldsymbol{\\tau}_{(t,w)},n))\nwith a learnable mean and variance.\nWe could directly supervise the training of\nÎ¼\nÎ¸\n\\mu_{\\theta}\nusing the forward (diffusion) process. Following\nSong etÂ al. [\n18\n], Ho etÂ al. [\n11\n]\n, we\ncompute a noisy sample\ns\nt\n(\nn\n)\ns_{t}^{(n)}\nbased on the true sample\ns\nt\n=\ns\nt\n(\n0\n)\ns_{t}=s_{t}^{(0)}\n:\ns\nt\n(\nn\n)\n=\nÎ±\nÂ¯\nâ€‹\n(\nn\n)\nâ€‹\ns\nt\n+\n1\nâˆ’\nÎ±\nÂ¯\nâ€‹\n(\nn\n)\nâ€‹\nÏµ\ns_{t}^{(n)}=\\sqrt{\\bar{\\alpha}(n)}s_{t}+\\sqrt{1-\\bar{\\alpha}(n)}\\boldsymbol{\\epsilon}\n(3)\nwhere\nÏµ\nâˆ¼\nğ’©\nâ€‹\n(\nğŸ\n,\nğ‘°\n)\n\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\boldsymbol{I})\nis the noise,\nÎ±\nÂ¯\nâ€‹\n(\nn\n)\n=\nâˆ\ni\n=\n1\nn\nÎ±\nâ€‹\n(\ni\n)\n\\bar{\\alpha}(n)=\\prod_{i=1}^{n}\\alpha(i)\nand the weighting factors\nÎ±\nâ€‹\n(\nn\n)\n=\ne\nâˆ’\n(\nÎ²\nmin\nâ€‹\n(\n1\nN\n)\n+\n(\nÎ²\nmax\nâˆ’\nÎ²\nmin\n)\nâ€‹\n2\nâ€‹\nn\nâˆ’\n1\n2\nâ€‹\nN\n2\n)\n\\alpha{(n)}=e^{-\\left(\\beta_{\\text{min}}\\left(\\frac{1}{N}\\right)+(\\beta_{\\text{max}}-\\beta_{\\text{min}})\\frac{2n-1}{2N^{2}}\\right)}\nwhere\nÎ²\nmax\n=\n10\n\\beta_{\\text{max}}=10\nand\nÎ²\nmin\n=\n0.1\n\\beta_{\\text{min}}=0.1\nare parameters introduced for empirical reasons\n[\n19\n]\n.\nWe can equally learn to predict the true samples by learning a noise model\n[\n20\n]\n. Hence, we train a noise model\nÏµ\nÎ¸\nâ€‹\n(\nğ’”\nt\n(\nn\n)\n,\nğ‰\n(\nt\n,\nw\n)\n,\nn\n)\n\\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{s}^{(n)}_{t},\\boldsymbol{\\tau}_{(t,w)},n)\nthat learns to predict the noise vector\nÏµ\n\\boldsymbol{\\epsilon}\n. By using the conditional version of the simplified surrogate objective from\n[\n11\n]\n, we minimize\nâ„’\np\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nn\n,\nğ‰\n,\nğ’”\nğ’•\n,\nÏµ\n[\nâ€–\nÏµ\nâˆ’\nÏµ\nÎ¸\nâ€‹\n(\nğ’”\n(\nn\n)\n,\nğ‰\n(\nt\n,\nw\n)\n,\nn\n)\nâ€–\n2\n]\n\\mathcal{L}_{p}(\\theta)=\\mathop{\\mathbb{E}}_{n,\\boldsymbol{\\tau},\\boldsymbol{s_{t}},\\boldsymbol{\\epsilon}}\\left[\\left\\|\\boldsymbol{\\epsilon}-\\boldsymbol{\\epsilon}_{\\theta}\\left(\\boldsymbol{s}^{(n)},\\boldsymbol{\\tau}_{(t,w)},n\\right)\\right\\|^{2}\\right]\n(4)\nwhere\nğ’”\nğ’•\n\\boldsymbol{s_{t}}\nis the state sampled from the dataset\nD\nD\nfor\nt\nâˆ¼\nU\nT\nâ€‹\n(\n{\nw\n,\nâ€¦\n,\nT\nâˆ’\n1\n}\n)\nt\\sim\\textit{U}_{T}(\\{w,\\dots,T-1\\})\n,\nğ’”\n(\nn\n)\n\\boldsymbol{s}^{(n)}\nis computed according to\nEq.\nËœ\n3\n,\nÏµ\n\\boldsymbol{\\epsilon}\nis the noise, and\nn\nâˆ¼\nU\nD\nâ€‹\n(\n{\n1\n,\nâ€¦\n,\nN\n}\n)\nn\\sim\\textit{U}_{D}(\\left\\{1,\\dots,N\\right\\})\nis the uniform distribution used for sampling the diffusion timestep.\nWe use the true data sample\nğ’”\nğ’•\n\\boldsymbol{s_{t}}\nfrom the offline RL dataset to obtain the noisy sample in\nEq.\nËœ\n3\n. Leveraging our modelâ€™s capacity to learn multimodal distributions, we generate a set of\nk\nk\nsamples\n{\nğ’”\nt\n(\n0\n)\n}\n\\{\\boldsymbol{s}_{t}^{(0)}\\}\nas our\npredicted state candidates\nin parallel from the reverse diffusion chain.\nWe use the noise prediction model\n[\n11\n]\nwith the reverse diffusion chain\nğ’”\nt\n(\nn\nâˆ’\n1\n)\nâˆ£\nğ’”\nt\n(\nn\n)\n\\boldsymbol{s}_{t}^{(n-1)}\\mid\\boldsymbol{s}_{t}^{(n)}\nformulated as\nğ’”\nt\n(\nn\n)\nÎ±\n(\nn\n)\nâˆ’\n1\nâˆ’\nÎ±\n(\nn\n)\nÎ±\n(\nn\n)\nâ€‹\n(\n1\nâˆ’\nÎ±\nÂ¯\n(\nn\n)\n)\nâ€‹\nÏµ\nÎ¸\nâ€‹\n(\nğ’”\nt\n(\nn\n)\n,\nğ‰\n(\nt\n,\nw\n)\n,\nn\n)\n+\n1\nâˆ’\nÎ±\n(\nn\n)\nâ€‹\nÏµ\n\\frac{\\boldsymbol{s}_{t}^{(n)}}{\\sqrt{\\alpha_{(n)}}}-\\frac{1-\\alpha_{(n)}}{\\sqrt{\\alpha_{(n)}(1-\\bar{\\alpha}_{(n)})}}\\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{s}_{t}^{(n)},\\boldsymbol{\\tau}_{(t,w)},n)+\\sqrt{1-\\alpha_{(n)}}\\boldsymbol{\\epsilon}\n(5)\nwhere\nÏµ\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nğˆ\n)\n\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(0,\\mathbf{I})\nfor\nn\n=\nN\n,\nâ€¦\n,\n1\nn=N,\\dots,1\n, and\nÏµ\n=\n0\n\\boldsymbol{\\epsilon}=0\nfor\nn\n=\n1\nn=1\n[\n11\n]\n. Below, we detail how the state candidates are used during an episode.\nAlgorithm 1\nCandidate Selection\n1:\nSample\n{\n{\nb\n^\n}\nl\n1\n,\nâ€¦\n,\n{\nb\n^\n}\nl\nP\n}\nâˆ¼\nZero-Shot FM\n\\{\\{\\hat{b}\\}^{1}_{l},\\dots,\\{\\hat{b}\\}^{P}_{l}\\}\\sim\\textit{Zero-Shot FM}\n2:\nfor\neach episode\np\n=\n1\n,\nâ‹¯\n,\nP\np=1,\\cdots,P\ndo\n3:\nt\n=\n0\nt=0\n4:\nReset environment\no\n0\nâˆ¼\nâ„°\no_{0}\\sim\\mathcal{E}\n5:\ns\n~\n\\tilde{s}\nâ†\n\\leftarrow\no\n0\nâˆ’\n{\nb\n^\n}\nl\np\nÂ¯\no_{0}-\\overline{\\{\\hat{b}\\}^{p}_{l}}\n6:\nInitialize\nğ‰\n(\nt\n,\nw\n)\n\\boldsymbol{\\tau}_{(t,w)}\n7:\nwhile\nnot\ndone\ndo\n8:\nSample\na\n(\n0\n)\nâˆ¼\nÏ€\nÏ•\nâ€‹\n(\na\n|\ns\n~\n)\na^{(0)}\\sim\\pi_{\\phi}(a|\\tilde{s})\n9:\nTake action\na\n(\n0\n)\na^{(0)}\nin\nâ„°\n\\mathcal{E}\n, observe\no\nt\n+\n1\no_{t+1}\n10:\n{\ns\n^\nt\n+\n1\nb\n}\nl\nâ†\n\\{\\hat{s}^{b}_{t+1}\\}_{l}\\leftarrow\no\nt\n+\n1\nâˆ’\n{\nb\n^\n}\nl\np\no_{t+1}-\\{\\hat{b}\\}^{p}_{l}\n11:\nğ‰\n(\nt\n+\n1\n,\nw\n)\n=\nPUSH\nâ¡\n(\nğ‰\n(\nt\n,\nw\n)\n,\n(\nÎ”\nâ€‹\no\nt\n+\n1\n,\na\n(\n0\n)\n)\n)\n\\boldsymbol{\\tau}_{(t+1,w)}=\\operatorname{PUSH}\\left(\\boldsymbol{\\tau}_{(t,w)},\\left(\\Delta o_{t+1},a^{(0)}\\right)\\right)\n12:\nğ‰\n(\nt\n+\n1\n,\nw\n)\n=\nPOP\nâ¡\n(\nğ‰\n(\nt\n+\n1\n,\nw\n)\n,\n(\nÎ”\nâ€‹\no\nt\nâˆ’\nw\n+\n1\n,\na\nt\nâˆ’\nw\n)\n)\n\\boldsymbol{\\tau}_{(t+1,w)}=\\operatorname{POP}\\left(\\boldsymbol{\\tau}_{(t+1,w)},\\left(\\Delta o_{t-w+1},a_{t-w}\\right)\\right)\n13:\nif\nt\n>\nw\nt>w\nthen\n14:\nSample\n{\nğ’”\nt\n+\n1\n(\n0\n)\n}\nk\n\\{\\boldsymbol{s}_{t+1}^{(0)}\\}_{k}\nfrom\nForl\nby Eq.\n5\n15:\ns\n~\n\\tilde{s}\nâ†\n\\leftarrow\nDCM\n(\n{\nğ’”\nt\n+\n1\n(\n0\n)\n}\nk\n,\n{\ns\n^\nt\n+\n1\nb\n}\nl\n\\{\\boldsymbol{s}_{t+1}^{(0)}\\}_{k},\\{\\hat{s}^{b}_{t+1}\\}_{l}\n)\n16:\nelse\n17:\ns\n~\n\\tilde{s}\nâ†\n\\leftarrow\no\nt\n+\n1\nâˆ’\n{\nb\n^\n}\nl\np\nÂ¯\no_{t+1}-\\overline{\\{\\hat{b}\\}^{p}_{l}}\n18:\nend if\n19:\nt\nâ†\nt\n+\n1\nt\\leftarrow t+1\n20:\nend while\n21:\nend for\n2.3\nForecasting using Zero-Shot Foundation Model\nBecause we assume that the offsets\nb\nj\nb^{j}\noriginate from a time series, we propose using a probabilistic zero-shot forecasting foundation model\n(Zero-Shot FM)\nsuch as Lag-Llama\n[\n10\n]\n, to forecast future offsets from past ones.\nWe assume that after\nP\nP\nepisodes, the true offsets are revealed, and we predict the offsets for the following\nP\nP\nepisodes. Using the probabilistic\nZero-Shot FM\nwe generate (\nb\n^\nl\nj\n,\nâ€¦\n,\nb\n^\nl\nj\n+\nP\nâˆ’\n1\n{\\hat{b}^{j}_{l},\\dots,\\hat{b}_{l}^{j+P-1}}\n), where (\nl\nl\n) denotes the number of samples generated for each episode (timestamp). Since Lag-Llama is a probabilistic model, it can generate multiple samples per timestamp, conditioned on\nC\nC\nnumber of past contexts (\nb\nj\nâˆ’\nC\n,\nâ€¦\n,\nb\nj\nâˆ’\n1\n{b^{j-C},\\dots,b^{j-1}}\n). In practice, we forecast every dimension of\nb\nb\nindependently since the\nZero-Shot FM\n(Lag-Llama\n[\n10\n]\n) is a univariate probabilistic model.\n2.4\nForl\nState Estimation\nThe next step in our method is to fuse the information from the forecaster and the diffusion model into a state estimate used for control at test time.\nAt the beginning of an episode, no information can be obtained from the diffusion model, so for the first\nw\nw\nsteps we only rely on the forecasterâ€™s mean prediction, i.e.\ns\n~\nt\n=\no\nt\nâˆ’\nb\n^\nj\nÂ¯\n{\\tilde{s}_{t}}=o_{t}-\\overline{\\hat{b}^{j}}\nwhere the mean is taken over the\nl\nl\nsamples.\nFigure 4\n:\nDistribution of samples produced by DCM (histograms for 10k samples for illustration).\nAs soon as\nw\nw\nsteps are taken, our\nForl\nState Estimation\nimproves on the inferred state as detailed below.\nFigure\n2\noffers an overview of the entire system and Algorithm\n1\nprovides a detailed pseudocode.\nTo recap, the diffusion model generates samples\n{\nğ’”\nt\n(\n0\n)\n}\nk\n\\{\\boldsymbol{s}_{t}^{(0)}\\}_{k}\nfrom the in-episode history\nÏ„\n\\tau\n,\nEq.\nËœ\n1\n. These samples represent a multimodal distribution of plausible state regions.\nThe\nZero-Shot FM\ngenerates\nl\nl\nsamples of offsets\n{\nb\n^\n}\nl\n\\{\\hat{b}\\}_{l}\nfrom which we compute forecasted states using\n{\nğ’”\n^\nt\n}\nl\n=\no\nt\nâˆ’\n{\nb\n^\n}\nl\n\\{\\hat{\\boldsymbol{s}}_{t}\\}_{l}=o_{t}-\\{\\hat{b}\\}_{l}\n.\nForl\n: Dimension-wise Closest Match (DCM)\nWe propose a lightweight approach to sample a good estimate based on the samples from the multimodal (\ndiffusion model\n{\nğ¬\nt\n(\n0\n)\n}\nk\n\\{\\boldsymbol{s}_{t}^{(0)}\\}_{k}\n) and unimodal (\nZero-Shot FM\n{\ns\n^\nt\nb\n}\nl\n\\{\\hat{s}^{b}_{t}\\}_{l}\n) distributions.\nLet\nğ’Ÿ\ndiffusion\n=\n{\nğ±\n1\n,\nâ€¦\n,\nğ±\nk\n}\n,\nğ’Ÿ\ntimeseries\n=\n{\nğ²\n1\n,\nâ€¦\n,\nğ²\nl\n}\n,\n\\mathcal{D}_{\\text{diffusion}}=\\{\\mathbf{x}_{1},\\dots,\\mathbf{x}_{k}\\},\\quad\\mathcal{D}_{\\text{timeseries}}=\\{\\mathbf{y}_{1},\\dots,\\mathbf{y}_{l}\\},\nwhere\nğ±\ni\n,\nğ²\nj\nâˆˆ\nâ„\nn\n\\mathbf{x}_{i},\\mathbf{y}_{j}\\in\\mathbb{R}^{n}\n. Then DCM constructs\nğ³\nâˆˆ\nâ„\nn\n\\mathbf{z}\\in\\mathbb{R}^{n}\nby\nz\nd\n=\ny\nj\nâˆ—\nâ€‹\n(\nd\n)\n,\nd\nwhere\nj\nâˆ—\nâ€‹\n(\nd\n)\n=\narg\nâ¡\nmin\nj\nâ¡\n(\nmin\ni\nâ¡\n|\nx\ni\n,\nd\nâˆ’\ny\nj\n,\nd\n|\n)\n,\nz_{d}\\;=\\;y_{j^{*}(d),d}\\quad\\text{where}\\quad j^{*}(d)\\;=\\;\\arg\\min_{j}\\Bigl(\\;\\min_{i}\\bigl|\\;x_{i,d}-y_{j,d}\\bigr|\\Bigr),\nwhere\nd\n=\n1\nâ€‹\nâ€¦\nâ€‹\nn\nd=1\\dots n\n.\nIn other words, for each dimension\nd\nd\n, we choose the sample from\nğ’Ÿ\ntimeseries\n\\mathcal{D}_{\\text{timeseries}}\nthat has the closest sample in\nğ’Ÿ\ndiffusion\n\\mathcal{D}_{\\text{diffusion}}\n. The process is straightforward yet effective, and under ideal sampling conditions for a toy dataset in\nFig.\nËœ\n4\n, DCM approximately samples from the product distribution. DCM uses a non-parametric search to find the forecast sample with the highest score, which corresponds to the minimum dimension-wise distance. DCMâ€™s prediction error is governed by the accuracy of the forecast samples in the unimodal\nğ’Ÿ\ntimeseries\n\\mathcal{D}_{\\text{timeseries}}\nthat achieves this best score. As we will demonstrate in the experiments, this approach empirically yields lower maximum errors and is more stable compared to other methods.\nForl\nAlgorithm\nAlgorithm\n1\nsummarizes the entire inference process at test time. We begin the episode by relying on the forecasted states\ns\n~\n0\n\\tilde{s}_{0}\n. As more transitions\n(\nÎ”\nâ€‹\no\nt\n,\na\nt\nâˆ’\n1\n)\n(\\Delta o_{t},a_{t-1})\nbecome available, the\nForl\ndiffusion model proposes candidate states\n{\nğ’”\nt\n(\n0\n)\n}\nk\n\\{\\boldsymbol{s}_{t}^{(0)}\\}_{k}\nthrough\nretrospection\nâ€”reasoning over the past in-episode experience to adapt state estimation on the fly when they begin to diverge from predictions. We then invoke DCM to blend the diffusion modelâ€™s candidates with the foundation modelâ€™s unimodal forecasts and obtain the final state estimate\ns\n~\nt\n\\tilde{s}_{t}\n. We use an off-the-shelf offline RL policy such as Diffusion-QL (\nDQL\n)\n[\n14\n]\nto select the agentâ€™s action\na\nt\na_{t}\n.\nFigure 5\n:\nZero-shot forecasting results of Lag-Llama\n[\n10\n]\nfor the first univariate series (plotted) from the\nreal-data-A,D\ndatasets; experiments use the\nfirst two series from each dataset\n.\nSummary\nBy combining a powerful\nzero-shot\nforecasting model with a\nconditional diffusion\nmechanism,\nForl\naddresses partial observability in continuous state and action space when ground-truth offsets are unavailable. This procedure is performed in the\nabsence of ground-truth offsets for past, current, and future episodes over the interval\nj\n:\nj\n+\nP\nj:j+P\nat test time\n. DCM provides a computationally inexpensive yet effective way of using the multimodal diffusion candidates and unimodal time-series forecasts. This robust adaptation approach yields a state estimate\ns\n~\nt\n\\tilde{s}_{t}\n, aligned with the agentâ€™s retrospective experience in the stationary offline RL dataset, incorporating a prospective external offset forecast.\nTable 1\n:\nNormalized scores (mean Â± std.) for\nForl\nframework and the baselines.\nBold are the best values, and those not significantly different (\np\n>\n0.05\np>0.05\n, Welchâ€™s t-test).\nmaze2d-medium\nDQL\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nDmbp\n+\nLag\nForl\n(ours)\nreal-data-A\n30.2\nÂ±\n\\pm\n6.5\n30.2\nÂ±\n\\pm\n8.6\n25.1\nÂ±\n\\pm\n9.8\n63.3\nÂ±\n\\pm\n6.7\nreal-data-B\n14.1\nÂ±\n\\pm\n12.1\n53.4\nÂ±\n\\pm\n14.6\n41.2\nÂ±\n\\pm\n21.1\n66.5\nÂ±\n\\pm\n18.2\nreal-data-C\n-2.3\nÂ±\n\\pm\n3.3\n56.7\nÂ±\n\\pm\n18.5\n56.9\nÂ±\n\\pm\n18.4\n86.3\nÂ±\n\\pm\n15.7\nreal-data-D\n4.7\nÂ±\n\\pm\n5.0\n36.9\nÂ±\n\\pm\n16.3\n38.5\nÂ±\n\\pm\n14.2\n103.4\nÂ±\n\\pm\n11.9\nreal-data-E\n3.5\nÂ±\n\\pm\n8.8\n8.7\nÂ±\n\\pm\n6.0\n11.4\nÂ±\n\\pm\n2.8\n51.2\nÂ±\n\\pm\n13.7\nAverage\n10.0\nÂ±\n\\pm\n37.2\nÂ±\n\\pm\n34.6\nÂ±\n\\pm\n74.1\nÂ±\n\\pm\nmaze2d-large\nreal-data-A\n16.2\nÂ±\n\\pm\n5.5\n2.4\nÂ±\n\\pm\n1.1\n4.2\nÂ±\n\\pm\n5.8\n42.9\nÂ±\n\\pm\n4.1\nreal-data-B\n-0.5\nÂ±\n\\pm\n2.9\n5.5\nÂ±\n\\pm\n9.0\n15.0\nÂ±\n\\pm\n14.6\n34.9\nÂ±\n\\pm\n9.2\nreal-data-C\n0.9\nÂ±\n\\pm\n1.7\n16.6\nÂ±\n\\pm\n7.5\n26.8\nÂ±\n\\pm\n8.4\n45.6\nÂ±\n\\pm\n4.1\nreal-data-D\n3.0\nÂ±\n\\pm\n6.6\n8.6\nÂ±\n\\pm\n3.2\n13.4\nÂ±\n\\pm\n4.1\n58.4\nÂ±\n\\pm\n6.5\nreal-data-E\n-2.1\nÂ±\n\\pm\n0.4\n2.6\nÂ±\n\\pm\n3.4\n0.9\nÂ±\n\\pm\n3.7\n12.0\nÂ±\n\\pm\n9.9\nAverage\n3.5\nÂ±\n\\pm\n7.1\nÂ±\n\\pm\n12.1\nÂ±\n\\pm\n38.8\nÂ±\n\\pm\nantmaze-umaze-diverse\nreal-data-A\n22.7\nÂ±\n\\pm\n3.0\n41.0\nÂ±\n\\pm\n5.2\n45.7\nÂ±\n\\pm\n4.8\n65.3\nÂ±\n\\pm\n8.7\nreal-data-B\n24.2\nÂ±\n\\pm\n3.5\n48.3\nÂ±\n\\pm\n7.0\n62.5\nÂ±\n\\pm\n13.2\n74.2\nÂ±\n\\pm\n10.8\nreal-data-C\n21.7\nÂ±\n\\pm\n3.5\n50.4\nÂ±\n\\pm\n8.3\n60.4\nÂ±\n\\pm\n3.9\n78.8\nÂ±\n\\pm\n8.5\nreal-data-D\n5.8\nÂ±\n\\pm\n2.3\n26.7\nÂ±\n\\pm\n6.3\n29.2\nÂ±\n\\pm\n5.9\n75.8\nÂ±\n\\pm\n8.0\nreal-data-E\n6.0\nÂ±\n\\pm\n6.8\n58.0\nÂ±\n\\pm\n16.6\n59.3\nÂ±\n\\pm\n7.6\n81.3\nÂ±\n\\pm\n6.9\nAverage\n16.1\nÂ±\n\\pm\n44.9\nÂ±\n\\pm\n51.4\nÂ±\n\\pm\n75.1\nÂ±\n\\pm\nantmaze-medium-diverse\nreal-data-A\n31.0\nÂ±\n\\pm\n6.5\n40.0\nÂ±\n\\pm\n5.7\n39.7\nÂ±\n\\pm\n4.0\n44.0\nÂ±\n\\pm\n7.9\nreal-data-B\n23.3\nÂ±\n\\pm\n4.8\n48.3\nÂ±\n\\pm\n4.8\n43.3\nÂ±\n\\pm\n16.0\n55.8\nÂ±\n\\pm\n7.0\nreal-data-C\n10.0\nÂ±\n\\pm\n2.3\n48.3\nÂ±\n\\pm\n3.4\n49.6\nÂ±\n\\pm\n3.7\n52.9\nÂ±\n\\pm\n9.5\nreal-data-D\n11.7\nÂ±\n\\pm\n5.4\n46.7\nÂ±\n\\pm\n7.5\n41.7\nÂ±\n\\pm\n6.6\n64.2\nÂ±\n\\pm\n8.6\nreal-data-E\n18.7\nÂ±\n\\pm\n4.5\n27.3\nÂ±\n\\pm\n8.6\n26.0\nÂ±\n\\pm\n5.5\n26.7\nÂ±\n\\pm\n4.7\nAverage\n18.9\nÂ±\n\\pm\n42.1\nÂ±\n\\pm\n40.1\nÂ±\n\\pm\n48.7\nÂ±\n\\pm\nantmaze-large-diverse\nreal-data-A\n11.0\nÂ±\n\\pm\n1.9\n11.3\nÂ±\n\\pm\n4.9\n9.0\nÂ±\n\\pm\n4.5\n34.3\nÂ±\n\\pm\n5.7\nreal-data-B\n5.8\nÂ±\n\\pm\n4.8\n9.2\nÂ±\n\\pm\n4.6\n8.3\nÂ±\n\\pm\n2.9\n46.7\nÂ±\n\\pm\n11.9\nreal-data-C\n5.4\nÂ±\n\\pm\n2.4\n22.1\nÂ±\n\\pm\n5.6\n17.9\nÂ±\n\\pm\n3.8\n33.8\nÂ±\n\\pm\n6.8\nreal-data-D\n2.5\nÂ±\n\\pm\n2.3\n14.2\nÂ±\n\\pm\n3.7\n14.2\nÂ±\n\\pm\n6.3\n46.7\nÂ±\n\\pm\n12.6\nreal-data-E\n5.3\nÂ±\n\\pm\n3.8\n3.3\nÂ±\n\\pm\n2.4\n3.3\nÂ±\n\\pm\n0.0\n11.3\nÂ±\n\\pm\n7.3\nAverage\n6.0\nÂ±\n\\pm\n12.0\nÂ±\n\\pm\n10.5\nÂ±\n\\pm\n34.6\nÂ±\n\\pm\nkitchen-complete\nreal-data-A\n16.6\nÂ±\n\\pm\n1.4\n7.2\nÂ±\n\\pm\n1.9\n8.7\nÂ±\n\\pm\n1.3\n12.0\nÂ±\n\\pm\n3.9\nreal-data-B\n12.9\nÂ±\n\\pm\n4.1\n32.7\nÂ±\n\\pm\n6.5\n20.0\nÂ±\n\\pm\n3.1\n33.1\nÂ±\n\\pm\n5.6\nreal-data-C\n13.4\nÂ±\n\\pm\n1.7\n23.9\nÂ±\n\\pm\n6.6\n20.5\nÂ±\n\\pm\n3.3\n23.9\nÂ±\n\\pm\n6.0\nreal-data-D\n7.5\nÂ±\n\\pm\n2.5\n24.0\nÂ±\n\\pm\n9.2\n28.1\nÂ±\n\\pm\n8.1\n27.1\nÂ±\n\\pm\n10.1\nreal-data-E\n18.5\nÂ±\n\\pm\n6.0\n2.8\nÂ±\n\\pm\n2.1\n6.2\nÂ±\n\\pm\n1.7\n10.3\nÂ±\n\\pm\n3.0\nAverage\n13.8\nÂ±\n\\pm\n18.1\nÂ±\n\\pm\n16.7\nÂ±\n\\pm\n21.3\nÂ±\n\\pm\ncube-single-play\nFQL\nFQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nForl\n-F (ours)\nreal-data-A\n0.0\nÂ±\n\\pm\n0.0\n0.0\nÂ±\n\\pm\n0.0\n23.7\nÂ±\n\\pm\n3.6\nreal-data-B\n0.0\nÂ±\n\\pm\n0.0\n15.0\nÂ±\n\\pm\n7.0\n60.0\nÂ±\n\\pm\n7.0\nreal-data-C\n0.4\nÂ±\n\\pm\n0.9\n10.0\nÂ±\n\\pm\n1.7\n42.1\nÂ±\n\\pm\n5.6\nreal-data-D\n0.0\nÂ±\n\\pm\n0.0\n0.8\nÂ±\n\\pm\n1.9\n70.0\nÂ±\n\\pm\n13.0\nreal-data-E\n0.0\nÂ±\n\\pm\n0.0\n0.0\nÂ±\n\\pm\n0.0\n32.7\nÂ±\n\\pm\n9.5\nAverage\n0.1\nÂ±\n\\pm\n5.2\nÂ±\n\\pm\n45.7\nÂ±\n\\pm\nantmaze-large-navigate\nreal-data-A\n22.7\nÂ±\n\\pm\n2.2\n1.3\nÂ±\n\\pm\n0.7\n24.3\nÂ±\n\\pm\n4.3\nreal-data-B\n21.7\nÂ±\n\\pm\n5.4\n29.2\nÂ±\n\\pm\n8.8\n40.0\nÂ±\n\\pm\n7.6\nreal-data-C\n5.0\nÂ±\n\\pm\n1.1\n34.6\nÂ±\n\\pm\n6.7\n55.8\nÂ±\n\\pm\n3.7\nreal-data-D\n0.8\nÂ±\n\\pm\n1.9\n37.5\nÂ±\n\\pm\n5.1\n75.8\nÂ±\n\\pm\n5.4\nreal-data-E\n10.0\nÂ±\n\\pm\n4.1\n3.3\nÂ±\n\\pm\n0.0\n15.3\nÂ±\n\\pm\n8.7\nAverage\n12.0\nÂ±\n\\pm\n21.2\nÂ±\n\\pm\n42.2\nÂ±\n\\pm\n3\nExperiments\nWe evaluate\nForl\nacross navigation and manipulation tasks in D4RL\n[\n15\n]\nand OGBench\n[\n21\n]\noffline RL environments, each augmented with five real-world non-stationarity domains sourced from\n[\n22\n]\n.\nFig.\nËœ\n5\npresents the ground truth, forecast mean, and standard deviation from Lag-Llama\n[\n10\n]\nfor the\nfirst series\nof\nreal-data-A\nand\nreal-data-D\n. Our experiments address the following questions:\n(1)\nDoes FORL maintain state-of-the-art performance when confronted with unseen non-stationary offsets?\n(2)\nHow can we use\nForl\nwhen we have no access to delayed past ground truth offsets?\n(3)\nHow does DCM compare to other fusion approaches?\n(4)\nCan\nForl\nhandle intra-episode non-stationarity?\n(5)\nHow gracefully does performance degrade as offset magnitude\nÎ±\n\\alpha\nis scaled from\n0\n(no offset\n)\nâ†’\n1\n(our evaluation setup\n)\n0\\text{ (no offset})\\rightarrow 1\\text{ (our evaluation setup})\n?\n(6)\nCan\nForl\nserve as a plug-and-play module for different offline RL algorithms without retraining?\nExtended results, forecasts for the remaining series, and implementation details are provided in the Appendix. Results average 5 seeds, unless noted.\nBaselines\nWe compare our approach with the following baselines:\nDQL\n[\n14\n]\n, Flow Q-learning (\nFQL\n)\n[\n23\n]\nare diffusion-based and flow-based offline RL policies, respectively, that do not incorporate forecast information.\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n,\nFQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nextend\nDQL\nand\nFQL\nby using the sample mean of the forecasted states\n{\ns\n^\nt\n}\nl\n\\{\\hat{s}_{t}\\}_{l}\nat each timestep (using the constant per-episode predicted\nb\nj\nb^{j}\n).\nDQL\n+\nLag\n-\ns\n~\n\\tilde{s}\nsimilarly extends\nDQL\nusing the median.\nDmbp\n+\nLag\nis a variant of the Diffusion Modelâ€“Based Predictor (\nDmbp\n)\n[\n3\n]\n(a robust offline RL algorithm designed to mitigate state-observation perturbations at test time, detailed in Appendix\nD\n) that integrates forecasted states from\nZero-Shot FM\n[\n10\n]\ninto its state prediction module. By using the model learned from the offline data,\nDmbp\n+\nLag\naims to refine the forecasted states to make robust state estimations. The underlying policies throughout our experiments are identical policy checkpoints for both our method and the baselines.\nFigure 6\n:\nVisualization of states, predicted states as the agent navigates the environment.\nIllustrative Example\nFigs.\nËœ\n6\nand\n16\nillustrate an agent navigating the\nmaze2d-large\nenvironment where the true position is labeled as â€œstateâ€. The agent receives an observation indicating where it\nbelieves\nit is located due to unknown time-dependent factors. The candidate states predicted by the\nForl\ndiffusion model are shown as circles. Importantly, the agentâ€™s\n(\nÎ”\nâ€‹\no\n,\na\n)\n(\\Delta o,a)\n-trajectory can reveal possible states for the agent.\nForl\nâ€™s diffusion model (\nDm\n) component predicts these candidate states by using observation changes\n(\nÎ”\nâ€‹\no\n)\n(\\Delta o)\nand corresponding actions\n(\na\n)\n(a)\n. The possible candidate regions where the agent can be are limited, and our model successfully captures these locations.\nForl\nâ€™s candidate selection module (DCM) uses the samples from the forecaster and the diffusion model to recover a close estimate for the state. In contrast, the baseline\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nrelies on the forecaster\n[\n10\n]\nfor state predictions, which are significantly farther from the actual state. Consistent with the results in\nFig.\nËœ\n7\n,\nForl\nreduces prediction errors at test-time, thereby improving performance.\nFigure 7\n:\nPrediction Error in recovering true agent state.\n3.1\nResults\nFORL outperforms both pure forecasting (\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n) and the two-stage strategy that first predicts offsets with a time-series model and then applies a noise-robust offline RL algorithm (\nDmbp\n+\nLag\n). Its advantage is consistent across previously unseen non-stationary perturbations from five domains, each introducing a distinct univariate series into a separate state dimension at test time.\nWe present the average normalized scores over the prediction length\nP\nP\nacross multiple episodes run in the D4RL\n[\n15\n]\nand OGBench\n[\n21\n]\nfor each time-series in Table\n1\n. We conduct pairwise Welchâ€™s t-tests across all settings.\nFigure\n7\nplots the\nâ„“\n2\n\\ell_{2}\nnorm between the ground-truth states\ns\nt\ns_{t}\nand those predicted by\nForl\nand the baselines in the\nantmaze\nand\nmaze2d\nenvironments. Consistent with the average scores,\nForl\nachieves the lowest prediction error on average.\nFigure 8\n:\nDM Ablations\n3.1.1\nNo Access to Past Offsets\nWe evaluate different variants of using\nDm\nand\nZero-Shot FM\nwhen we do not have any access to past offsets in\nFig.\nËœ\n8\n.\nForl\n-\nDm\n(\nDm\n):\nDiffusion Model utilizes the candidate states generated by the\nForl\nâ€™s diffusion model component (Section\n2.2\n), which can be a multimodal distribution (\nFig.\nËœ\n3\n). Compared to\nDm\n, the full\nForl\nframework yields a 97.8% relative performance improvement. Notably,\nDm\nperforms on par with our extended baselines that incorporate historical offsets and forecastingâ€”\nDmbp\n+\nLag\n,\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n, and\nDQL\n+\nLag\n-\ns\n~\n\\tilde{s}\n. Moreover, without access to historical offset information before evaluation,\nDm\nachieves a 151.4% improvement over\nDQL\n, demonstrating its efficacy as a standalone module trained solely on a standard, stationary offline RL dataset without offset labels.\nH-Lag\n:\nWe maintain a history of offsets generated by\nDm\nover the most recent\nC\nC\nepisodes (excluding the evaluation interval\nP\nP\n, since offsets are not revealed after episode termination at test-time). We then feed this history into the\nZero-Shot FM\nto generate offset samples for the next\nP\nP\nevaluation episodes. These samples are applied directly at test time.\nH-Lag\n+\ndcm\n:\nWe initially follow the same procedure in\nH-Lag\nto obtain predictions from\nZero-Shot FM\n. Then, we apply\nDCM\nto these predicted offsets and the candidate states generated by\nForl\nâ€™s diffusion model. We also compare against\nMed\n+\ndcm\nand\nMed\n+\nNoise\n, simpler median-based heuristics detailed in\nAppendix\nËœ\nG\n. Empirically,\nH-Lag\n+\ndcm\noutperforms\nH-Lag\n, demonstrating that DCM with\nForl\nâ€™s diffusion model can improve robustness. Overall, scores and prediction errors indicate that just using the samples from\nDm\nhas better scores on average, while\nH-Lag\n+\ndcm\nis more stable in\nFig.\nËœ\n15\n.\n3.1.2\nDimension-wise Closest Match (DCM) Ablations\nFigure 9\n:\nCandidate Selection\nWe compare\nForl\n(DCM) against four alternative fusion strategies.\nForl(KDE)\n: For each dimension, we fit a kernel density estimator (KDE) on\nğ’Ÿ\ndiffusion\n=\n{\nğ’”\nt\n(\n0\n)\n}\nk\n\\mathcal{D}_{\\text{diffusion}}=\\{\\boldsymbol{s}_{t}^{(0)}\\}_{k}\nand then we evaluate that probability density function for each point in\nğ’Ÿ\ntimeseries\n\\mathcal{D}_{\\text{timeseries}}\n. Then, we take the product of these densities in each dimension to obtain the weight for each sample\ns\n^\nt\nb\n\\hat{s}^{b}_{t}\n.\nWe obtain a single representative sample by taking the weighted average of samples in\nğ’Ÿ\ntimeseries\n\\mathcal{D}_{\\text{timeseries}}\n.\nTo ensure stability, when the sum of the weights is near zero, we use the mean of the\nğ’Ÿ\ntimeseries\n\\mathcal{D}_{\\text{timeseries}}\nas the states. We use Scottâ€™s rule\n[\n24\n]\nto compute the bandwidth.\nDM-FS\n-\ns\nÂ¯\n\\bar{s}\n,\nDM-FS\n-\ns\n~\n\\tilde{s}\nselect the closest prediction from DM to the mean and median of the\nZero-Shot FM\nâ€™s predictions, respectively.\nForl\n(\nmax\n) constructs a diagonal multivariate distribution from the dimension-wise mean and standard deviation of the forecasted states, then selects the sample predicted by our diffusion model with the highest likelihood under that distribution. Although all baselines fuse information using the same two sets generated by the diffusion model and\nZero-Shot FM\n, DCM has higher performance. In\nTable\nËœ\n8\nwe compute the maximum, minimum, and mean prediction error values over the test episodes used in\nFig.\nËœ\n6\n.\nForl\n(DCM) yields significantly stable prediction errors (\nMaximum Error\nâ†“\n\\texttt{Maximum Error}\\downarrow\n:\n2.40\n) for both maximum error and mean error compared to\nForl\n(\nmax\n) (\nMaximum Error\nâ†“\n\\texttt{Maximum Error}\\downarrow\n:\n9.33\n) demonstrating its robustness.\n3.1.3\nIntra-episode Non-stationarity\nFigure 10\n:\nIntra-Episode Performance\nOur framework can natively handle intra-episode offsets, where the offset changes every\nf\n=\n50\nf=50\ntimesteps. In this setting, the offsets become available after the episode terminates, but the agent is subject to a time-dependent unknown offset within the episode. Zero-shot forecasting foundation module can generate samples before the episode begins. Our diffusion model (\nForl\n-\nDm\n) itself does not rely on the forecasts of the foundation module and only tracks observation changes and actions which are invariant to the offsets. The DCM can adaptively fuse information from both models at each timestep without requiring any hyperparameters.\nTables\nËœ\n10\nand\n10\nshow the average scores for\nDQL\nvs.\nForl\n-\nDm\nand\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nvs.\nForl\n. Among the algorithms that do not use any past ground truth offsets\nDQL\nand\nForl\n-\nDm\n, only using the diffusion model of\nForl\nsignificantly increases performance. When we have access to past offsets,\nForl\nobtains a superior performance. This shows that our method covers both cases, when information is available and not available, even when offsets are not constant throughout the episode.\n3.1.4\nOffset-Scaling\nFigure 11\n:\nImpact of offset scaling\n(\nÎ±\n)\n(\\alpha)\non average normalized scores.\nWe scale the offsets with\nÎ±\n\\alpha\nacross all maze experiments. We conduct experiments in 5 environments (all antmaze and maze2d used in Table\n1\n) across 5 time-series dataset setups with\nÎ±\nâˆˆ\n\\alpha\\in\n{0, 0.25, 0.5, 0.75, 1.0}, where\nÎ±\n=\n0\n\\alpha=0\nis the standard offline RL environment used during training and\nÎ±\n=\n1.0\n\\alpha=1.0\nis our evaluation setup. The results show that\nForl\noutperforms the baselines, confirming its robustness. Even a small scaling of\n0.25\n0.25\nresults in a sudden drop in performance, whereas\nForl\nonly experiences a gradual decrease in Figure\n11\n. Detailed results for each environment and\nÎ±\n\\alpha\npairs are provided in Appendix Figure\n13\n.\nTable 2\n:\nNormalized scores (mean Â± std.) for FORL and baselines on\nmaze2d-large\n.\nBolds denote the best scores and those not significantly different (Welchâ€™s t-test, p > 0.05). Suffixes -T and -R denote the use of TD3+BC\n[\n2\n]\nand RORL\n[\n25\n]\n, respectively.\nTd\n3\nBc\nPolicy\nRorl\nPolicy\nmaze2d-large\nTd\n3\nBc\nTd\n3\nBc\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nDmbp\n+\nLag\n-T\nForl\n(ours)-T\nRorl\nRorl\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nDmbp\n+\nLag\n-R\nForl\n(ours)-R\nreal-data-A\n14.7\nÂ±\n\\pm\n5.7\n2.5\nÂ±\n\\pm\n2.7\n4.8\nÂ±\n\\pm\n3.9\n20.7\nÂ±\n\\pm\n3.5\n12.2\nÂ±\n\\pm\n2.3\n13.0\nÂ±\n\\pm\n2.0\n4.3\nÂ±\n\\pm\n5.0\n56.9\nÂ±\n\\pm\n3.0\nreal-data-B\n-0.9\nÂ±\n\\pm\n2.0\n4.6\nÂ±\n\\pm\n8.9\n11.7\nÂ±\n\\pm\n12.6\n56.8\nÂ±\n\\pm\n14.4\n1.2\nÂ±\n\\pm\n5.5\n13.1\nÂ±\n\\pm\n14.7\n28.5\nÂ±\n\\pm\n11.7\n98.5\nÂ±\n\\pm\n19.0\nreal-data-C\n0.8\nÂ±\n\\pm\n1.9\n21.6\nÂ±\n\\pm\n8.4\n29.5\nÂ±\n\\pm\n13.7\n56.9\nÂ±\n\\pm\n14.6\n3.1\nÂ±\n\\pm\n0.9\n60.6\nÂ±\n\\pm\n8.5\n39.4\nÂ±\n\\pm\n6.1\n139.0\nÂ±\n\\pm\n15.1\nreal-data-D\n2.5\nÂ±\n\\pm\n4.4\n14.9\nÂ±\n\\pm\n4.3\n14.4\nÂ±\n\\pm\n6.8\n29.5\nÂ±\n\\pm\n10.3\n-1.6\nÂ±\n\\pm\n0.7\n17.9\nÂ±\n\\pm\n6.8\n17.5\nÂ±\n\\pm\n6.0\n33.1\nÂ±\n\\pm\n2.3\nreal-data-E\n-2.3\nÂ±\n\\pm\n0.2\n1.0\nÂ±\n\\pm\n4.2\n2.0\nÂ±\n\\pm\n3.9\n8.0\nÂ±\n\\pm\n4.2\n-0.9\nÂ±\n\\pm\n2.0\n3.3\nÂ±\n\\pm\n4.4\n2.2\nÂ±\n\\pm\n4.5\n32.2\nÂ±\n\\pm\n15.3\nAverage\n3.0\nÂ±\n\\pm\n8.9\nÂ±\n\\pm\n12.5\nÂ±\n\\pm\n34.4\nÂ±\n\\pm\n2.8\nÂ±\n\\pm\n21.6\nÂ±\n\\pm\n18.4\nÂ±\n\\pm\n71.9\nÂ±\n\\pm\n3.1.5\nPolicy-Agnostic\nIn the\nmaze2d-large\nexperiments (in Table\n2\n,\nmaze2d-medium\nin Appendix\nTable\nËœ\n3\n), we use Robust Offline RL (\nRorl\n)\n[\n25\n]\n, and\nTd\n3\nBc\n[\n2\n]\noffline RL algorithms instead of\nDQL\n[\n14\n]\n, to analyze the effect of offline RL policy choice during evaluation.\nRorl\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nand\nTd\n3\nBc\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nextend\nRorl\nand\nTd\n3\nBc\nby using the sample mean of the forecasted states\n{\ns\n^\nt\n}\nl\n\\{\\hat{s}_{t}\\}_{l}\nat each timestep (using the constant per-episode predicted\nb\n^\nj\nÂ¯\n\\overline{\\hat{b}^{j}}\n). Results indicate that using a robust offline RL algorithm during training significantly increases performance (71.9) compared to\nDQL\n(38.8) and\nTd\n3\nBc\n(34.4) at test time when used with\nForl\n, no increase when used alone, and a marginal increase with Lag-Llama and\nDmbp\n+\nLag\n. We observe similar performance gains when applying\nForl\nto other policies (\nAppendices\nËœ\nE\nand\nE\n), including Implicit Q-Learning (\nIql\n)\n[\n26\n]\nand\nFQL\n[\n23\n]\n, as detailed in\nTable\nËœ\n4\nand\nTable\nËœ\n7\n.\n4\nRelated Work\nReinforcement Learning in Non-Stationary Environments\nExisting works in non-stationary reinforcement learning (RL) predominantly focus on adapting to changing transition dynamics and reward functions.\nAckermann etÂ al. [\n27\n]\npropose an offline RL framework that incorporates structured non-stationarity in reward and transition functions by learning hidden task representations and predicting them at test time. Although our work also investigates the intersection of non-stationary environments and offline RL, we assume stationarity during training. To learn adaptive policies\nonline\n, meta-learning algorithms have been proposed as a promising approach\n[\n28\n,\n29\n,\n30\n]\n.\nAl-Shedivat etÂ al. [\n29\n]\nexplores a competitive multi-agent environment where transition dynamics change. While these approaches provide valuable insights, they often require samples from the current environment and struggle in non-trivial non-stationarity, highlighting the need for more future-oriented methods\n[\n31\n,\n9\n]\n. Examples of such future-oriented approaches include Proactively Synchronizing Tempo (ProST)\n[\n9\n]\nand Prognosticator\n[\n31\n]\n, which address the evolution of transition and reward functions over time. ProST leverages a forecaster, namely, Auto-Regressive Integrated Moving Average (ARIMA), and a model predictor to optimize for future policies in environments to overcome the time-synchronization issue in time-elapsing MDPs. This approach aligns with our focus on time-varying environments and similarly utilizes real-world finance (e.g., stock price) time-series datasets to model non-stationarity. Both ProST and Prognosticator assume that states are fully observable during testing and that online interaction with the environment is possible during trainingâ€”conditions that are not always feasible in the real world. Instead, our approach assumes that states are not fully observable and that direct interaction with the environment during training is not feasible, necessitating that the policy be learned exclusively from a pre-collected dataset.\nRobust offline RL\nTesting-time robust offline RL methods\nDmbp\n[\n3\n]\n,\nRorl\n[\n25\n]\nexamine scenarios where a noise-free, stationary dataset is used for training, but corruption is introduced during testing. This is distinct from\n[\n3\n]\n, training-time robust offline RL\n[\n32\n,\n33\n]\n, which assumes a corrupted training dataset. Both\nRorl\n[\n25\n]\nand\nDmbp\n[\n3\n]\nassume access only to a clean, uncorrupted offline RL dataset, as\nForl\n, and they are evaluated in a perturbed environment. To the best of our knowledge,\nForl\nis the first work to extend this setting to a non-Markovian, time-evolving, non-stationary deployment environment. We focus on time-dependent exogenous factors from real-data that are aligned with the definition of a non-stationary environment\n[\n6\n]\n.\nDiffusion models in offline RL\nDiffusion models\n[\n12\n]\nhave seen widespread adoption in RL\n[\n34\n,\n35\n]\ndue to their remarkable expressiveness, particularly in representing multimodal distributions, scalability, and stable training properties. In the context of offline RL, diffusion models have been used for representing policies\n[\n14\n,\n36\n,\n37\n,\n38\n]\n, planners\n[\n39\n,\n40\n]\n, data synthesis\n[\n41\n,\n42\n]\n, and removing noise\n[\n3\n]\n. Notably, Diffusion Q-learning\n[\n14\n]\nleverages conditional diffusion model policies to learn from offline RL datasets, maintaining proximity to behavior policy while utilizing Q-value function guidance. In contrast, our method harnesses diffusion models to learn from a sequence of actions and effect tuples, leveraging the multimodal capabilities of diffusion models to identify diverse candidate locations of the hidden states.\n5\nConclusion\nWe introduce\nF\norecasting in Non-stationary\nO\nffline\nRL\n(\nForl\n), a novel framework designed to be robust to passive non-stationarities that arise at test time. This is crucial when an agent trained on an offline RL dataset is deployed in a non-stationary environment or when the environment begins to exhibit partial observability due to unknown, time-varying factors.\nForl\nleverages diffusion probabilistic models and zero-shot time series foundation models to correct unknown offsets in observations, thereby enhancing the adaptability of learned policies. Our empirical results across diverse time-series datasets, OGBench\n[\n21\n]\nand D4RL\n[\n15\n]\nbenchmarks, demonstrate that\nForl\nnot only bridges the gap between forecasting and non-stationary offline RL but also consistently outperforms the baselines. Our approach is currently limited by the assumption of additive perturbations. For future work, we plan to extend our work to more general observation transformations.\nAcknowledgments and Disclosure of Funding\nGeorg Martius is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 â€“ Project number 390727645. Co-funded by the European Union (ERC, REAL-RL, 101045454). Views and opinions expressed are, however, those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. This work was supported by the German Federal Ministry of Education and Research (BMBF): TÃ¼bingen AI Center, FKZ: 01IS18039A. This work was in part supported by the INVERSE project (101136067) funded by the European Union and JSPS KAKENHI Grant Numbers JP23K24926, JP25H01236. The numerical calculations reported in this paper were partially performed at TUBITAK ULAKBIM, High Performance and Grid Computing Center (TRUBA resources). The authors would like to thank RenÃ© Geist, TomÃ¡Å¡ DaniÅ¡, Ji Shi, and Leonard Franz for their valuable comments on the manuscript.\nReferences\nLevine etÂ al. [2020]\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu.\nOffline reinforcement learning: Tutorial, review, and perspectives on open problems.\nCoRR\n, abs/2005.01643, 2020.\nURL\nhttps://arxiv.org/abs/2005.01643\n.\nFujimoto and Gu [2021]\nScott Fujimoto and Shixiang Gu.\nA minimalist approach to offline reinforcement learning.\nIn A.Â Beygelzimer, Y.Â Dauphin, P.Â Liang, and J.Â Wortman Vaughan, editors,\nAdvances in Neural Information Processing Systems\n, 2021.\nZhihe and Xu [2024]\nYANG Zhihe and Yunjian Xu.\nDmbp: Diffusion model-based predictor for robust offline reinforcement learning against state observation perturbations.\nIn\nThe Twelfth International Conference on Learning Representations\n, 2024.\nTrabucco etÂ al. [2022]\nBrandon Trabucco, Mariano Phielipp, and Glen Berseth.\nAnymorph: Learning transferable polices by inferring agent morphology.\nIn\nInternational Conference on Machine Learning\n, pages 21677â€“21691. PMLR, 2022.\nZhang etÂ al. [2020]\nAmy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and Doina Precup.\nInvariant causal prediction for block mdps.\nIn\nInternational Conference on Machine Learning\n, pages 11214â€“11224. PMLR, 2020.\nKhetarpal etÂ al. [2022]\nKhimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup.\nTowards continual reinforcement learning: A review and perspectives.\nJournal of Artificial Intelligence Research\n, 75:1401â€“1476, 2022.\nChandak [2022]\nYash Chandak.\nReinforcement Learning for Non-stationary problems\n.\nPhD thesis, PhD thesis, University of Massachusetts Amherst, 2022.\nXie etÂ al. [2021]\nAnnie Xie, James Harrison, and Chelsea Finn.\nDeep reinforcement learning amidst continual structured non-stationarity.\nIn\nInternational Conference on Machine Learning\n, pages 11393â€“11403. PMLR, 2021.\nLee etÂ al. [2023]\nHyunin Lee, Yuhao Ding, Jongmin Lee, Ming Jin, Javad Lavaei, and Somayeh Sojoudi.\nTempo adaptation in non-stationary reinforcement learning.\nIn\nThirty-seventh Conference on Neural Information Processing Systems\n, 2023.\nRasul etÂ al. [2023]\nKashif Rasul, Arjun Ashok, AndrewÂ Robert Williams, Hena Ghonia, Rishika Bhagwatkar, Arian Khorasani, Mohammad JavadÂ Darvishi Bayazi, George Adamopoulos, Roland Riachi, Nadhir Hassen, Marin BiloÅ¡, Sahil Garg, Anderson Schneider, Nicolas Chapados, Alexandre Drouin, Valentina Zantedeschi, Yuriy Nevmyvaka, and Irina Rish.\nLag-llama: Towards foundation models for time series forecasting.\narXiv preprint arXiv:2310.08278\n, 2023.\nHo etÂ al. [2020]\nJonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising diffusion probabilistic models.\nIn H.Â Larochelle, M.Â Ranzato, R.Â Hadsell, M.F. Balcan, and H.Â Lin, editors,\nAdvances in Neural Information Processing Systems\n, volumeÂ 33, pages 6840â€“6851. Curran Associates, Inc., 2020.\nSohl-Dickstein etÂ al. [2015]\nJascha Sohl-Dickstein, EricÂ A. Weiss, Niru Maheswaranathan, and Surya Ganguli.\nDeep unsupervised learning using nonequilibrium thermodynamics.\nCoRR\n, abs/1503.03585, 2015.\nURL\nhttps://arxiv.org/abs/1503.03585\n.\nLuo [2022a]\nCalvin Luo.\nUnderstanding diffusion models: A unified perspective.\narXiv preprint arXiv:2208.11970\n, 2022a.\nURL\nhttps://arxiv.org/abs/2208.11970\n.\nWang etÂ al. [2023]\nZhendong Wang, JonathanÂ J Hunt, and Mingyuan Zhou.\nDiffusion policies as an expressive policy class for offline reinforcement learning.\nIn\nThe Eleventh International Conference on Learning Representations\n, 2023.\nFu etÂ al. [2020]\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.\nD4rl: Datasets for deep data-driven reinforcement learning.\narXiv preprint arXiv:2004.07219\n, 2020.\nURL\nhttps://arxiv.org/abs/2004.07219\n.\nKaelbling etÂ al. [1998]\nLeslieÂ Pack Kaelbling, MichaelÂ L Littman, and AnthonyÂ R Cassandra.\nPlanning and acting in partially observable stochastic domains.\nArtificial intelligence\n, 101(1-2):99â€“134, 1998.\nBonet [2012]\nBlai Bonet.\nDeterministic pomdps revisited.\narXiv preprint arXiv:1205.2659\n, 2012.\nURL\nhttps://arxiv.org/abs/1205.2659\n.\nSong etÂ al. [2020]\nYang Song, Jascha Sohl-Dickstein, DiederikÂ P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456\n, 2020.\nXiao etÂ al. [2021]\nZhisheng Xiao, Karsten Kreis, and Arash Vahdat.\nTackling the generative learning trilemma with denoising diffusion gans.\narXiv preprint arXiv:2112.07804\n, 2021.\nLuo [2022b]\nCalvin Luo.\nUnderstanding diffusion models: A unified perspective, 2022b.\nURL\nhttps://arxiv.org/abs/2208.11970\n.\nPark etÂ al. [2025a]\nSeohong Park, Kevin Frans, Benjamin Eysenbach, and Sergey Levine.\nOGBench: Benchmarking offline goal-conditioned RL.\nIn\nThe Thirteenth International Conference on Learning Representations\n, 2025a.\nURL\nhttps://openreview.net/forum?id=M992mjgKzI\n.\nAlexandrov etÂ al. [2020]\nAlexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, DanielleÂ C. Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, Lorenzo Stella, AliÂ Caner TÃ¼rkmen, and Yuyang Wang.\nGluonTS: Probabilistic and Neural Time Series Modeling in Python.\nJournal of Machine Learning Research\n, 21(116):1â€“6, 2020.\nPark etÂ al. [2025b]\nSeohong Park, Qiyang Li, and Sergey Levine.\nFlow q-learning.\nIn\nForty-second International Conference on Machine Learning\n, 2025b.\nURL\nhttps://openreview.net/forum?id=KVf2SFL1pi\n.\nScott [2015]\nDavidÂ W Scott.\nMultivariate density estimation: theory, practice, and visualization\n.\nJohn Wiley & Sons, 2015.\nYang etÂ al. [2022]\nRui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, and Lei Han.\nRorl: Robust offline reinforcement learning via conservative smoothing.\nAdvances in neural information processing systems\n, 35:23851â€“23866, 2022.\nKostrikov etÂ al. [2021]\nIlya Kostrikov, Ashvin Nair, and Sergey Levine.\nOffline reinforcement learning with implicit q-learning.\nCoRR\n, abs/2110.06169, 2021.\nURL\nhttps://arxiv.org/abs/2110.06169\n.\nAckermann etÂ al. [2024]\nJohannes Ackermann, Takayuki Osa, and Masashi Sugiyama.\nOffline reinforcement learning from datasets with structured non-stationarity.\nIn\nReinforcement Learning Conference\n, 2024.\nFinn etÂ al. [2019]\nChelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine.\nOnline meta-learning.\nIn\nInternational conference on machine learning\n, pages 1920â€“1930. PMLR, 2019.\nAl-Shedivat etÂ al. [2018]\nMaruan Al-Shedivat, Trapit Bansal, Yura Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel.\nContinuous adaptation via meta-learning in nonstationary and competitive environments.\nIn\nInternational Conference on Learning Representations\n, 2018.\nAda and Ugur [2024]\nSuzanÂ Ece Ada and Emre Ugur.\nUnsupervised meta-testing with conditional neural processes for hybrid meta-reinforcement learning.\nIEEE Robotics and Automation Letters\n, 9(10):8427â€“8434, 2024.\ndoi:\n10.1109/LRA.2024.3443496\n.\nChandak etÂ al. [2020]\nYash Chandak, Georgios Theocharous, Shiv Shankar, Martha White, Sridhar Mahadevan, and Philip Thomas.\nOptimizing for the future in non-stationary mdps.\nIn\nInternational Conference on Machine Learning\n, pages 1414â€“1425. PMLR, 2020.\nYe etÂ al. [2023]\nChenlu Ye, Rui Yang, Quanquan Gu, and Tong Zhang.\nCorruption-robust offline reinforcement learning with general function approximation.\nAdvances in Neural Information Processing Systems\n, 36:36208â€“36221, 2023.\nZhang etÂ al. [2022]\nXuezhou Zhang, Yiding Chen, Xiaojin Zhu, and Wen Sun.\nCorruption-robust offline reinforcement learning.\nIn\nInternational Conference on Artificial Intelligence and Statistics\n, pages 5757â€“5773. PMLR, 2022.\nZhu etÂ al. [2023]\nZhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang, Yong Yu, and Weinan Zhang.\nDiffusion models for reinforcement learning: A survey.\narXiv preprint arXiv:2311.01223\n, 2023.\nChen etÂ al. [2024]\nJiayu Chen, Bhargav Ganguly, Yang Xu, Yongsheng Mei, Tian Lan, and Vaneet Aggarwal.\nDeep generative models for offline policy learning: Tutorial, survey, and perspectives on future directions.\narXiv preprint arXiv:2402.13777\n, 2024.\nHe etÂ al. [2023a]\nLongxiang He, Linrui Zhang, Junbo Tan, and Xueqian Wang.\nDiffcps: Diffusion model based constrained policy search for offline reinforcement learning.\narXiv preprint arXiv:2310.05333\n, 2023a.\nHansen-Estruch etÂ al. [2023]\nPhilippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, JakubÂ Grudzien Kuba, and Sergey Levine.\nIdql: Implicit q-learning as an actor-critic method with diffusion policies.\narXiv preprint arXiv:2304.10573\n, 2023.\nURL\nhttps://arxiv.org/abs/2304.10573\n.\nAda etÂ al. [2024]\nSuzanÂ Ece Ada, Erhan Oztop, and Emre Ugur.\nDiffusion policies for out-of-distribution generalization in offline reinforcement learning.\nIEEE Robotics and Automation Letters\n, 9(4):3116â€“3123, 2024.\ndoi:\n10.1109/LRA.2024.3363530\n.\nJanner etÂ al. [2022]\nMichael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine.\nPlanning with diffusion for flexible behavior synthesis.\nIn\nInternational Conference on Machine Learning\n, 2022.\nColeman etÂ al. [2023]\nMatthew Coleman, Olga Russakovsky, Christine Allen-Blanchette, and YeÂ Zhu.\nDiscrete diffusion reward guidance methods for offline reinforcement learning.\nIn\nICML 2023 Workshop: Sampling and Optimization in Discrete Space\n, 2023.\nHe etÂ al. [2023b]\nHaoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, and Xuelong Li.\nDiffusion model is an effective planner and data synthesizer for multi-task reinforcement learning.\nIn A.Â Oh, T.Â Naumann, A.Â Globerson, K.Â Saenko, M.Â Hardt, and S.Â Levine, editors,\nAdvances in Neural Information Processing Systems\n, volumeÂ 36, pages 64896â€“64917. Curran Associates, Inc., 2023b.\nLiang etÂ al. [2023]\nZhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo.\nAdaptdiffuser: Diffusion models as adaptive self-evolving planners.\narXiv preprint arXiv:2302.01877\n, 2023.\nURL\nhttps://arxiv.org/abs/2302.01877\n.\nFujimoto etÂ al. [2019]\nScott Fujimoto, David Meger, and Doina Precup.\nOff-policy deep reinforcement learning without exploration.\nIn Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,\nProceedings of the 36th International Conference on Machine Learning\n, volumeÂ 97 of\nProceedings of Machine Learning Research\n, pages 2052â€“2062. PMLR, 09â€“15 Jun 2019.\nURL\nhttps://proceedings.mlr.press/v97/fujimoto19a.html\n.\nSiegel etÂ al. [2020]\nNoah Siegel, JostÂ Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller.\nKeep doing what worked: Behavior modelling priors for offline reinforcement learning.\nIn\nInternational Conference on Learning Representations\n, 2020.\nMa etÂ al. [2021]\nYechengÂ Jason Ma, Dinesh Jayaraman, and Osbert Bastani.\nConservative offline distributional reinforcement learning.\nIn A.Â Beygelzimer, Y.Â Dauphin, P.Â Liang, and J.Â Wortman Vaughan, editors,\nAdvances in Neural Information Processing Systems\n, 2021.\nURL\nhttps://openreview.net/forum?id=Z2vksUFuVst\n.\nJaques etÂ al. [2019]\nNatasha Jaques, Asma Ghandeharioun, JudyÂ Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard.\nWay off-policy batch deep reinforcement learning of implicit human preferences in dialog.\narXiv preprint arXiv:1907.00456\n, 2019.\nURL\nhttps://arxiv.org/abs/1907.00456\n.\nWu etÂ al. [2019]\nYifan Wu, George Tucker, and Ofir Nachum.\nBehavior regularized offline reinforcement learning.\narXiv preprint arXiv:1911.11361\n, 2019.\nURL\nhttps://arxiv.org/abs/1911.11361\n.\nPeng etÂ al. [2019]\nXueÂ Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.\nAdvantage-weighted regression: Simple and scalable off-policy reinforcement learning.\narXiv preprint arXiv:1910.00177\n, 2019.\nNair etÂ al. [2020]\nAshvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine.\nAwac: Accelerating online reinforcement learning with offline datasets.\narXiv preprint arXiv:2006.09359\n, 2020.\nNowozin etÂ al. [2016]\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka.\nf-gan: Training generative neural samplers using variational divergence minimization.\nAdvances in neural information processing systems\n, 29, 2016.\nKumar etÂ al. [2019]\nAviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.\nStabilizing off-policy q-learning via bootstrapping error reduction.\nAdvances in neural information processing systems\n, 32, 2019.\nYu etÂ al. [2021]\nTianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.\nCOMBO: conservative offline model-based policy optimization.\nCoRR\n, abs/2102.08363, 2021.\nURL\nhttps://arxiv.org/abs/2102.08363\n.\nPark etÂ al. [2024]\nSeohong Park, Kevin Frans, Sergey Levine, and Aviral Kumar.\nIs value learning really the main bottleneck in offline rl?\narXiv preprint arXiv:2406.09329\n, 2024.\nURL\nhttps://arxiv.org/abs/2406.09329\n.\nMazoure etÂ al. [2022]\nBogdan Mazoure, Ilya Kostrikov, Ofir Nachum, and JonathanÂ J Tompson.\nImproving zero-shot generalization in offline reinforcement learning using generalized similarity functions.\nIn S.Â Koyejo, S.Â Mohamed, A.Â Agarwal, D.Â Belgrave, K.Â Cho, and A.Â Oh, editors,\nAdvances in Neural Information Processing Systems\n, volumeÂ 35, pages 25088â€“25101. Curran Associates, Inc., 2022.\nPuterman [2014]\nMartinÂ L Puterman.\nMarkov decision processes: discrete stochastic dynamic programming\n.\nJohn Wiley & Sons, 2014.\nYang and Xu [2024]\nZhihe Yang and Yunjian Xu.\nDMBP: Diffusion model-based predictor for robust offline reinforcement learning against state observation perturbations (official implementation).\nhttps://github.com/zhyang2226/DMBP/tree/main\n, 2024.\nVanÂ Hasselt etÂ al. [2016]\nHado VanÂ Hasselt, Arthur Guez, and David Silver.\nDeep reinforcement learning with double q-learning.\nIn\nProceedings of the AAAI conference on artificial intelligence\n, volumeÂ 30, 2016.\nLillicrap etÂ al. [2016]\nTimothyÂ P. Lillicrap, JonathanÂ J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra.\nContinuous control with deep reinforcement learning.\nIn Yoshua Bengio and Yann LeCun, editors,\n4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings\n, 2016.\nFujimoto etÂ al. [2018]\nScott Fujimoto, Herke Hoof, and David Meger.\nAddressing function approximation error in actor-critic methods.\nIn\nInternational conference on machine learning\n, pages 1587â€“1596. PMLR, 2018.\nPeters and Schaal [2007]\nJan Peters and Stefan Schaal.\nReinforcement learning by reward-weighted regression for operational space control.\nIn\nProceedings of the 24th international conference on Machine learning\n, pages 745â€“750, 2007.\nWang etÂ al. [2018]\nQing Wang, Jiechao Xiong, Lei Han, peng sun, Han Liu, and Tong Zhang.\nExponentially weighted imitation learning for batched historical data.\nIn S.Â Bengio, H.Â Wallach, H.Â Larochelle, K.Â Grauman, N.Â Cesa-Bianchi, and R.Â Garnett, editors,\nAdvances in Neural Information Processing Systems\n, volumeÂ 31. Curran Associates, Inc., 2018.\nURL\nhttps://proceedings.neurips.cc/paper_files/paper/2018/file/4aec1b3435c52abbdf8334ea0e7141e0-Paper.pdf\n.\nTarasov etÂ al. [2022]\nDenis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov.\nCORL: Research-oriented deep offline reinforcement learning library.\nIn\n3rd Offline RL Workshop: Offline RL as a â€Launchpadâ€\n, 2022.\nURL\nhttps://openreview.net/forum?id=SyAS49bBcv\n.\nThomas [2021]\nGarrett Thomas.\nImplicit q-learning (iql) in pytorch.\nhttps://github.com/gwthomas/IQL-PyTorch\n, 2021.\nLiu etÂ al. [2022]\nXingchao Liu, Chengyue Gong, and Qiang Liu.\nFlow straight and fast: Learning to generate and transfer data with rectified flow.\narXiv preprint arXiv:2209.03003\n, 2022.\nLiu etÂ al. [2024]\nXingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and qiang liu.\nInstaflow: One step is enough for high-quality diffusion-based text-to-image generation.\nIn\nThe Twelfth International Conference on Learning Representations\n, 2024.\nURL\nhttps://openreview.net/forum?id=1k4yZbbDqX\n.\nFrans etÂ al. [2024]\nKevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel.\nOne step diffusion via shortcut models.\narXiv preprint arXiv:2410.12557\n, 2024.\nDing etÂ al. [2024]\nZihan Ding, Chi Jin, Difan Liu, Haitian Zheng, KrishnaÂ Kumar Singh, Qiang Zhang, Yan Kang, Zhe Lin, and Yuchen Liu.\nDollar: Few-step video generation via distillation and latent reward optimization.\narXiv preprint arXiv:2412.15689\n, 2024.\nLi etÂ al. [2024]\nJiachen Li, Weixi Feng, Wenhu Chen, and WilliamÂ Yang Wang.\nReward guided latent consistency distillation.\narXiv preprint arXiv:2403.11027\n, 2024.\nRonchetti and Huber [2009]\nElvezioÂ M Ronchetti and PeterÂ J Huber.\nRobust statistics\n.\nJohn Wiley & Sons Hoboken, NJ, USA, 2009.\nFischler and Bolles [1981]\nMartinÂ A. Fischler and RobertÂ C. Bolles.\nRandom sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography.\nCommun. ACM\n, 24(6):381â€“395, June 1981.\nISSN 0001-0782.\nWelford [1962]\nB.Â P. Welford.\nNote on a method for calculating corrected sums of squares and products.\nTechnometrics\n, 4:419â€“420, 1962.\nKnuth [1997]\nDonaldÂ Ervin Knuth.\nThe art of computer programming\n, volumeÂ 3.\nPearson Education, 1997.\nGodahewa etÂ al. [2021]\nRakshithaÂ Wathsadini Godahewa, Christoph Bergmeir, GeoffreyÂ I. Webb, Rob Hyndman, and Pablo Montero-Manso.\nMonash time series forecasting archive.\nIn\nThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)\n, 2021.\nDheeru and KarraÂ Taniskidou [2017]\nDua Dheeru and Efi KarraÂ Taniskidou.\nUci machine learning repository.\nhttp://archive.ics.uci.edu/ml\n, 2017.\nSalinas etÂ al. [2019]\nDavid Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, and Jan Gasthaus.\nHigh-dimensional multivariate forecasting with low-rank gaussian copula processes.\nAdvances in neural information processing systems\n, 32, 2019.\nLai etÂ al. [2018]\nGuokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu.\nModeling long-and short-term temporal patterns with deep neural networks.\nIn\nThe 41st international ACM SIGIR conference on research & development in information retrieval\n, pages 95â€“104, 2018.\nAlexandrov etÂ al. [2019]\nA.Â Alexandrov, K.Â Benidis, M.Â Bohlke-Schneider, V.Â Flunkert, J.Â Gasthaus, T.Â Januschowski, D.Â C. Maddix, S.Â Rangapuram, D.Â Salinas, J.Â Schulz, L.Â Stella, A.Â C. TÃ¼rkmen, and Y.Â Wang.\nGluonTS: Probabilistic Time Series Modeling in Python.\narXiv preprint arXiv:1906.05264\n, 2019.\nRonneberger etÂ al. [2015]\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-net: Convolutional networks for biomedical image segmentation.\nIn\nMedical image computing and computer-assisted interventionâ€“MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18\n, pages 234â€“241. Springer, 2015.\nWang [2023]\nZhendong Wang.\nDiffusion policies for offline rl â€” official pytorch implementation.\nhttps://github.com/Zhendong-Wang/Diffusion-Policies-for-Offline-RL\n, 2023.\nKingma and Ba [2015]\nDiederikÂ P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\nIn Yoshua Bengio and Yann LeCun, editors,\nICLR (Poster)\n, 2015.\nURL\nhttp://dblp.uni-trier.de/db/conf/iclr/iclr2015.html#KingmaB14\n.\nKostrikov [2021]\nIlya Kostrikov.\nOffline reinforcement learning with implicit q-learning (official implementation).\nhttps://github.com/ikostrikov/implicit_q_learning\n, 2021.\nPark [2025]\nSeohong Park.\nFql: Flow q-learning (official implementation).\nhttps://github.com/seohongpark/fql\n, 2025.\nFujimoto [2018]\nScott Fujimoto.\nA minimalist approach to offline reinforcement learning pytorch implementation.\nhttps://github.com/sfujim/TD3_BC\n, 2018.\nYang [2022]\nRui Yang.\nRorl: Robust offline reinforcement learning via conservative smoothing code repository.\nhttps://github.com/YangRui2015/RORL\n, 2022.\nTodorov etÂ al. [2012]\nEmanuel Todorov, Tom Erez, and Yuval Tassa.\nMujoco: A physics engine for model-based control.\nIn\nIntelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems\n, pages 5026â€“5033. IEEE, 2012.\nTrindade [2015]\nArtur Trindade.\nElectricityLoadDiagrams20112014.\nUCI Machine Learning Repository, 2015.\nDOI: https://doi.org/10.24432/C58C86.\nHunter [2007]\nJohnÂ D Hunter.\nMatplotlib: A 2d graphics environment.\nComputing in science & engineering\n, 9(03):90â€“95, 2007.\nForecasting in Offline Reinforcement Learning for Non-stationary Environments:\nSupplementary Material\nSuzan Ece Ada\n1,2\nGeorg Martius\n2\nEmre Ugur\n1\nErhan Oztop\n3,4\n1\nBogazici University, TÃ¼rkiye\n2\nUniversity of TÃ¼bingen, Germany\n3\nOzyegin University, TÃ¼rkiye\n4\nOsaka University, Japan\nece.ada@bogazici.edu.tr\nAppendix A\nRelated Work: Continued\nOffline Reinforcement Learning\nA high-level overview of existing work in offline reinforcement learning (RL) identifies three predominant strategies: policy constraint methods\n[\n43\n,\n44\n,\n14\n]\n, pessimistic value function methods which assign low values to OOD actions\n[\n45\n]\n, and model-based offline RL methods. Policy constraint methods actively avoid querying OOD actions during training by leveraging probabilistic metrics which can be explicit\n[\n46\n,\n47\n]\n, implicit\n[\n48\n,\n49\n]\nf\nf\n-divergence\n[\n50\n]\n, or integral probability metrics\n[\n51\n]\n. These metrics ensure that the learned policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nremains close to the behavior policy\nÏ€\nÎ²\n\\pi_{\\beta}\nthat generated the offline RL dataset\n[\n1\n]\n. Similarly, pessimistic value function approaches regularize the value function or the Q-function to avoid overestimations in OOD regions. Model-based offline RL methods\n[\n52\n]\n, on the other hand, focus on learning the environmentâ€™s dynamics, benefiting from the strengths of supervised learning approaches. However, in the same vein, these methods are susceptible to the distribution shift problem in Offline RL\n[\n1\n]\n. We detail the offline RL algorithms used in our experiments in\nAppendix\nËœ\nE\n.\nPark etÂ al. [\n53\n]\nemphasize the challenges of generalizing policies to test-time states which are not in the support of the offline RL dataset. While prior works have investigated the issue of generalization\n[\n38\n,\n54\n]\n, in testing-time robust RL methods\n[\n3\n]\nthis challenge is exacerbated through the introduction of noise into the states by an unknown adversary.\nAppendix B\nBackground\nB.1\nReinforcement Learning\nMarkov Decision Processes (MDPs) are often used to formalize Reinforcement Learning (RL). MDP is defined by the tuple\nâ„³\nâ‰\n(\nğ’®\n,\nğ’œ\n,\nğ’¯\n,\nâ„›\n,\nÏ\n0\n,\nÎ³\n)\n\\mathcal{M}\\doteq\\bigl(\\mathcal{S},\\mathcal{A},\\mathcal{T},\\mathcal{R},\\rho_{0},\\gamma\\bigr)\nwhere\nğ’®\n\\mathcal{S}\nis the state space,\nğ’œ\n\\mathcal{A}\nis the action space,\nğ’¯\n\\mathcal{T}\nis the transition function (which may be deterministic or stochastic),\nâ„›\n:\nğ’®\nÃ—\nğ’œ\nâ†’\nâ„\n\\mathcal{R}:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}\nis the reward function,\nÏ\n0\n\\rho_{0}\nis the initial state distribution and\nÎ³\nâˆˆ\n[\n0\n,\n1\n)\n\\gamma\\in[0,1)\nis the discount factor\n[\n55\n]\n. In online and off-policy RL algorithms an agent can interact with the environment using a parameterized policy\nÏ€\nÎ¸\nâ€‹\n(\nğ’‚\n|\nğ’”\n)\n\\pi_{\\theta}(\\boldsymbol{a}|\\boldsymbol{s})\n, to maximize the expected return\nğ”¼\nÏ€\nâ€‹\n[\nâˆ‘\nt\nÎ³\nt\nâ€‹\nr\nâ€‹\n(\nğ’”\nt\n,\nğ’‚\nt\n)\n]\n\\mathbb{E}_{\\pi}\\left[\\sum_{t}\\gamma^{t}r\\left(\\boldsymbol{s}_{t},\\boldsymbol{a}_{t}\\right)\\right]\n. In contrast, offline RL requires the agent to learn from a static dataset\nğ’Ÿ\n=\n{\n(\nğ’”\nk\n,\nğ’‚\nk\n,\nğ’”\nk\nâ€²\n,\nr\nk\n)\n}\nk\n=\n1\nN\n\\mathcal{D}=\\{(\\boldsymbol{s}_{k},\\boldsymbol{a}_{k},\\boldsymbol{s}^{\\prime}_{k},r_{k})\\}^{N}_{k=1}\ngenerated by a generally unknown behavior policy\nÏ€\nÎ²\nâ€‹\n(\nğ’‚\n|\nğ’”\n)\n\\pi_{\\beta}(\\boldsymbol{a}|\\boldsymbol{s})\n[\n1\n]\n.\nB.2\nNon-Stationary Environments\nWe next review tangential definitions and formalisms used for non-stationary environments.\nDefinition B.1\n(Partially Observable Markov Decision Processes\n[\n16\n,\n7\n,\n6\n]\n)\n.\nA Partially Observable Markov Decision Process (POMDP) is given by the tuple\nâ„³\n^\n=\n(\nğ’®\n,\nğ’œ\n,\nğ’ª\n,\nğ’¯\n,\nâ„›\n,\nğ±\n,\nÏ\n0\n,\nÎ³\n)\n,\n\\hat{\\mathcal{M}}\\,=\\,\\bigl(\\mathcal{S},\\mathcal{A},\\mathcal{O},\\mathcal{T},\\mathcal{R},\\mathbf{x},\\rho_{0},\\gamma\\bigr),\nwhere\nğ’ª\n\\mathcal{O}\nis the observation space and\nğ±\n\\mathbf{x}\nis the observation function. This function can be deterministic (\nğ±\n:\nğ’®\nâ†’\nğ’ª\n\\mathbf{x}:\\mathcal{S}\\rightarrow\\mathcal{O}\n)\n[\n17\n,\n6\n]\nor stochastic (\nğ±\nâ€‹\n(\no\nâˆ£\ns\n)\n\\mathbf{x}(o\\mid s)\n) for\no\nâˆˆ\nğ’ª\n,\ns\nâˆˆ\nğ’®\no\\in\\mathcal{O},s\\in\\mathcal{S}\n[\n7\n]\n.\nTo capture a broader range of non-stationary RL scenarios,\nKhetarpal etÂ al. [\n6\n]\npresent a\ngeneral non-stationary RL\nformulation, which allows each component of the underlying MDP or POMDP to evolve over time. Concretely, this time-varying tuple is denoted as\n(\nğ’®\nâ€‹\n(\nt\n)\n,\nğ’œ\nâ€‹\n(\nt\n)\n,\nğ’¯\nâ€‹\n(\nt\n)\n,\nâ„›\nâ€‹\n(\nt\n)\n,\nğ±\nâ€‹\n(\nt\n)\n,\nğ’ª\nâ€‹\n(\nt\n)\n)\n\\bigl(\\mathcal{S}(t),\\mathcal{A}(t),\\mathcal{T}(t),\\mathcal{R}(t),\\mathbf{x}(t),\\mathcal{O}(t)\\bigr)\nwhere each element is represented by a function\nÏ†\nâ€‹\n(\ni\n,\nt\n)\n\\varphi(i,t)\n, indicating its variation with time\nt\nt\nand input\ni\ni\n[\n6\n]\n.\nScope\n, denoted by a set\nÎº\n\\kappa\n, specifies which of these components vary.\nDefinition B.2\n(Scope of Non-Stationarity\n[\n6\n]\n)\n.\nGiven the general non-stationary RL framework, the\nscope of non-stationarity\nis the subset\nÎº\nâŠ†\n{\nğ’®\n,\nğ’œ\n,\nâ„›\n,\nğ’¯\n,\nğ±\n,\nğ’ª\n}\n,\n\\kappa\\,\\subseteq\\,\\{\\mathcal{S},\\mathcal{A},\\mathcal{R},\\mathcal{T},\\mathbf{x},\\mathcal{O}\\},\nindicating which components of the environment evolve over time.\nNotably,\nChandak [\n7\n]\ndefines Non-Stationary Decision Process (NSDP) as a sequence of POMDPs, with non-stationarity confined to the subset\nÎº\nâŠ†\n{\nâ„›\n,\nğ’¯\n,\nğ±\n}\n\\kappa\\subseteq\\{\\mathcal{R},\\mathcal{T},\\mathbf{x}\\}\n, where the initial state distribution\nÏ\n0\n,\nj\n\\rho_{0,j}\nvaries between POMDPs. Existing research in nonâ€‘stationary RL largely focuses on the episodic evolution of transition dynamics and reward functions\n[\n27\n]\n. In contrast, the evolution of observation functions remains underexplored, despite its potential for real-world applicability and thus demands further investigation\n[\n6\n]\n. Handling inaccurate state information is crucial in non-stationary RL since an agent may encounter non-stationarity due to its own imperfect perception of the state while the underlying physics of the environment remains unchanged\n[\n6\n]\n.\nB.3\nDiffusion Models\nDiffusion models\n[\n12\n]\naim to model the data distribution with\np\nÎ¸\nâ€‹\n(\nğ’™\n0\n)\n:=\nâˆ«\np\nÎ¸\nâ€‹\n(\nğ’™\n0\n:\nT\n)\nâ€‹\nğ‘‘\nğ’™\n1\n:\nT\np_{\\theta}(\\boldsymbol{x}_{0}):=\\int p_{\\theta}(\\boldsymbol{x}_{0:T})\\,d\\boldsymbol{x}_{1:T}\nfrom samples\nx\n0\nx_{0}\nin the dataset. The joint distribution\np\nÎ¸\nâ€‹\n(\nğ’™\n0\n:\nT\n)\np_{\\theta}(\\boldsymbol{x}_{0:T})\n, is modeled as a Markov chain, where\nğ’™\nğŸ\n,\nâ€¦\n,\nğ’™\nğ‘»\n\\boldsymbol{x_{1}},...,\\boldsymbol{x_{T}}\nare the latent variables with the same dimensionality as the data samples. The joint distribution is given by\np\nÎ¸\nâ€‹\n(\nğ’™\n0\n:\nT\n)\n:=\nğ’©\nâ€‹\n(\nğ’™\nT\n;\nğŸ\n,\nğˆ\n)\nâ€‹\nâˆ\nt\n=\n1\nT\np\nÎ¸\nâ€‹\n(\nğ’™\nt\nâˆ’\n1\nâˆ£\nğ’™\nt\n)\np_{\\theta}\\left(\\boldsymbol{x}_{0:T}\\right):=\\mathcal{N}(\\boldsymbol{x}_{T};\\mathbf{0},\\mathbf{I})\\prod_{t=1}^{T}p_{\\theta}\\left(\\boldsymbol{x}_{t-1}\\mid\\boldsymbol{x}_{t}\\right)\\quad\n(6)\nwhere\np\nÎ¸\nâ€‹\n(\nğ’™\nt\nâˆ’\n1\nâˆ£\nğ’™\nt\n)\n:=\nğ’©\nâ€‹\n(\nğ’™\nt\nâˆ’\n1\n;\nğ\nÎ¸\nâ€‹\n(\nğ’™\nt\n,\nt\n)\n,\nğšº\nÎ¸\nâ€‹\n(\nğ’™\nt\n,\nt\n)\n)\np_{\\theta}\\left(\\boldsymbol{x}_{t-1}\\mid\\boldsymbol{x}_{t}\\right):=\\mathcal{N}\\left(\\boldsymbol{x}_{t-1};\\boldsymbol{\\mu}_{\\theta}\\left(\\boldsymbol{x}_{t},t\\right),\\mathbf{\\Sigma}_{\\theta}\\left(\\boldsymbol{x}_{t},t\\right)\\right)\n. The forward process\nq\nâ€‹\n(\nğ’™\n1\n:\nT\nâˆ£\nğ’™\n0\n)\n:=\nâˆ\nt\n=\n1\nT\nq\nâ€‹\n(\nğ’™\nt\nâˆ£\nğ’™\nt\nâˆ’\n1\n)\nq(\\boldsymbol{x}_{1:T}\\mid\\boldsymbol{x}_{0}):=\\prod_{t=1}^{T}q(\\boldsymbol{x}_{t}\\mid\\boldsymbol{x}_{t-1})\ninvolves adding a small amount of Gaussian noise to the data sample at each diffusion timestep to obtain the latent variables following a variance schedule\n{\nÎ²\nt\n=\n1\nâˆ’\nÎ±\nt\nâˆˆ\n(\n0\n,\n1\n)\n}\nt\n=\n1\nT\n\\{\\beta_{t}=1-\\alpha_{t}\\in(0,1)\\}^{T}_{t=1}\n. Here, the encoder transitions are\nq\nâ€‹\n(\nğ’™\nt\nâˆ£\nğ’™\nt\nâˆ’\n1\n)\n:=\nğ’©\nâ€‹\n(\nğ’™\nt\n;\nÎ±\nt\nâ€‹\nğ’™\nt\nâˆ’\n1\n,\n(\n1\nâˆ’\nÎ±\nt\n)\nâ€‹\nğˆ\n)\nq(\\boldsymbol{x}_{t}\\mid\\boldsymbol{x}_{t-1}):=\\mathcal{N}\\left(\\boldsymbol{x}_{t};\\sqrt{\\alpha_{t}}\\boldsymbol{x}_{t-1},(1-\\alpha_{t})\\mathbf{I}\\right)\n[\n11\n]\n.\nAssuming we have access to the true data sample during training, using recursion and the reparameterization trick, we can obtain samples\nğ’™\nt\n\\boldsymbol{x}_{t}\nat any timestep t in closed form with\nq\nâ€‹\n(\nğ’™\nt\nâˆ£\nğ’™\n0\n)\n=\nğ’©\nâ€‹\n(\nğ’™\nt\n;\nÎ±\nÂ¯\nt\nâ€‹\nğ’™\n0\n,\n(\n1\nâˆ’\nÎ±\nÂ¯\nt\n)\nâ€‹\nğˆ\n)\nq\\left(\\boldsymbol{x}_{t}\\mid\\boldsymbol{x}_{0}\\right)=\\mathcal{N}\\left(\\boldsymbol{x}_{t};\\sqrt{\\bar{\\alpha}_{t}}\\boldsymbol{x}_{0},\\left(1-\\bar{\\alpha}_{t}\\right)\\mathbf{I}\\right)\nwhere\nÎ±\nÂ¯\nt\n=\nâˆ\ni\n=\n1\nt\nÎ±\ni\n\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\alpha_{i}\n[\n11\n]\n. During training, the evidence lower bound is oftentimes maximized by a simplified surrogate objective\n[\n11\n]\n. After learning the parameters of the reverse process, we can sample\nğ’™\nT\n\\boldsymbol{x}_{T}\nfrom\nğ’©\nâ€‹\n(\nğ’™\nT\n;\nğŸ\n,\nğˆ\n)\n\\mathcal{N}(\\boldsymbol{x}_{T};\\mathbf{0},\\mathbf{I})\nto start generating samples through an iterative denoising procedure.\nAppendix C\nDetails on\nForl\nC.1\nConditional Diffusion Model Details\nOur aim is to learn the reverse diffusion process, by modeling\np\nÎ¸\nâ€‹\n(\nğ’”\nt\n(\nn\nâˆ’\n1\n)\nâˆ£\nğ’”\nt\n(\nn\n)\n,\nğ‰\n(\nt\n,\nw\n)\n)\np_{\\theta}\\left(\\boldsymbol{s}_{t}^{(n-1)}\\mid\\boldsymbol{s}_{t}^{(n)},\\boldsymbol{\\tau}_{(t,w)}\\right)\nas a Gaussian distribution\nğ’©\nâ€‹\n(\nğ’”\nt\n(\nn\nâˆ’\n1\n)\n;\nÎ¼\nÎ¸\nâ€‹\n(\nğ’”\nt\n(\nn\n)\n,\nğ‰\n(\nt\n,\nw\n)\n,\nn\n)\n,\nÎ£\nÎ¸\nâ€‹\n(\nğ’”\nt\n(\nn\n)\n,\nğ‰\n(\nt\n,\nw\n)\n,\nn\n)\n)\n\\mathcal{N}(\\boldsymbol{s}^{(n-1)}_{t};\\mu_{\\theta}(\\boldsymbol{s}^{(n)}_{t},\\boldsymbol{\\tau}_{(t,w)},n),\\Sigma_{\\theta}(\\boldsymbol{s}^{(n)}_{t},\\boldsymbol{\\tau}_{(t,w)},n))\nwhere\nn\nn\nis the diffusion timestep and\nt\nt\nis the RL timestep. We approximate this mean\nÎ¼\nÎ¸\nâ€‹\n(\nğ’”\nt\n(\nn\n)\n,\nğ‰\n(\nt\n,\nw\n)\n,\nn\n)\n\\mu_{\\theta}(\\boldsymbol{s}^{(n)}_{t},\\boldsymbol{\\tau}_{(t,w)},n)\nusing a conditional noise model\nÏµ\nÎ¸\n\\boldsymbol{\\epsilon}_{\\theta}\nwith\nğ’”\nt\n(\nn\n)\nÎ±\n(\nn\n)\nâˆ’\n1\nâˆ’\nÎ±\n(\nn\n)\n1\nâˆ’\nÎ±\nÂ¯\n(\nn\n)\nâ€‹\nÎ±\n(\nn\n)\nâ€‹\nÏµ\nÎ¸\nâ€‹\n(\nğ’”\nt\n(\nn\n)\n,\nğ‰\n(\nt\n,\nw\n)\n,\nn\n)\n\\frac{\\boldsymbol{s}^{(n)}_{t}}{\\sqrt{\\alpha_{(n)}}}-\\frac{1-\\alpha_{(n)}}{\\sqrt{1-\\bar{\\alpha}_{(n)}}\\sqrt{\\alpha_{(n)}}}\\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{s}^{(n)}_{t},\\boldsymbol{\\tau}_{(t,w)},n)\nand fix the covariance\nÎ£\nÎ¸\nâ€‹\n(\nğ’”\nt\n(\nn\n)\n,\nğ‰\n(\nt\n,\nw\n)\n,\nn\n)\n\\Sigma_{\\theta}(\\boldsymbol{s}^{(n)}_{t},\\boldsymbol{\\tau}_{(t,w)},n)\nwith\n(\n1\nâˆ’\nÎ±\n(\nn\n)\n)\nâ€‹\nğˆ\n(1-\\alpha_{(n)})\\mathbf{I}\n[\n11\n]\n.\nSelecting a large number of diffusion timesteps can significantly increase the computational complexity of our algorithm. Hence, we use variance preserving stochastic differential equations (SDE)\n[\n18\n]\nfollowing the formulation in\n[\n19\n]\nÎ±\n(\nn\n)\n=\ne\nâˆ’\n(\nÎ²\nmin\nâ€‹\n(\n1\nN\n)\n+\n(\nÎ²\nmax\nâˆ’\nÎ²\nmin\n)\nâ€‹\n2\nâ€‹\nn\nâˆ’\n1\n2\nâ€‹\nN\n2\n)\n\\alpha_{(n)}=e^{-\\left(\\beta_{\\text{min}}\\left(\\frac{1}{N}\\right)+(\\beta_{\\text{max}}-\\beta_{\\text{min}})\\frac{2n-1}{2N^{2}}\\right)}\nwhere\nÎ²\nmax\n=\n10\n\\beta_{\\text{max}}=10\nand\nÎ²\nmin\n=\n0.1\n\\beta_{\\text{min}}=0.1\n.\nWe use the noise prediction model\n[\n11\n]\nwith reverse diffusion chain\nğ’”\nt\n(\nn\nâˆ’\n1\n)\nâˆ£\nğ’”\nt\n(\nn\n)\n=\nğ’”\nt\n(\nn\n)\nÎ±\n(\nn\n)\nâˆ’\n1\nâˆ’\nÎ±\n(\nn\n)\nÎ±\n(\nn\n)\nâ€‹\n(\n1\nâˆ’\nÎ±\nÂ¯\n(\nn\n)\n)\nâ€‹\nÏµ\nÎ¸\nâ€‹\n(\nğ’”\nt\n(\nn\n)\n,\nğ‰\n(\nt\n,\nw\n)\n,\nn\n)\n+\n1\nâˆ’\nÎ±\n(\nn\n)\nâ€‹\nÏµ\n\\boldsymbol{s}_{t}^{(n-1)}\\mid\\boldsymbol{s}_{t}^{(n)}=\\frac{\\boldsymbol{s}_{t}^{(n)}}{\\sqrt{\\alpha_{(n)}}}-\\frac{1-\\alpha_{(n)}}{\\sqrt{\\alpha_{(n)}(1-\\bar{\\alpha}_{(n)})}}\\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{s}_{t}^{(n)},\\boldsymbol{\\tau}_{(t,w)},n)+\\sqrt{1-\\alpha_{(n)}}\\boldsymbol{\\epsilon}\n(7)\nwhere\nÏµ\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nğˆ\n)\n\\quad\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(0,\\mathbf{I})\nfor\nn\n=\nN\n,\nâ€¦\n,\n1\nn=N,\\dots,1\n, and\nÏµ\n=\n0\n\\boldsymbol{\\epsilon}=0\nfor\nn\n=\n1\nn=1\n[\n11\n]\n.\nBy using the conditional version of the simplified surrogate objective from\n[\n11\n]\nwe minimize the\nForl\nmodel loss\nâ„’\np\nâ€‹\n(\nÎ¸\n)\n\\mathcal{L}_{p}(\\theta)\n, with\nEq.\nËœ\n4\nin the main paper.\nWe can train the noise prediction model by sampling\nğ’”\nt\n(\nn\n)\n\\boldsymbol{s}_{t}^{(n)}\nfor any diffusion timestep in the forward diffusion process, utilizing reparametrization and recursion\n[\n11\n]\n. We use the true data sample\nğ’”\nğ’•\n\\boldsymbol{s_{t}}\nfrom the offline RL dataset to obtain the noisy state\nğ’”\nt\n(\nn\n)\n=\nÎ±\nÂ¯\n(\nn\n)\nâ€‹\nğ’”\nğ’•\n+\n1\nâˆ’\nÎ±\nÂ¯\n(\nn\n)\nâ€‹\nÏµ\n\\boldsymbol{s}_{t}^{(n)}=\\sqrt{\\bar{\\alpha}_{(n)}}\\boldsymbol{s_{t}}+\\sqrt{1-\\bar{\\alpha}_{(n)}}\\boldsymbol{\\epsilon}\n. Leveraging our modelâ€™s capacity to learn multimodal distributions, we generate a set of\nk\nk\nsamples (predicted state candidates)\n{\nğ’”\nt\n(\n0\n)\n}\n\\{\\boldsymbol{s}_{t}^{(0)}\\}\nin parallel from the reverse diffusion chain using\nEq.\nËœ\n5\n.\nAlgorithm 2\nTraining\nRequire\n: Offline RL dataset\nğ’Ÿ\n\\mathcal{D}\nInitialize\n:\nÏµ\nÎ¸\n\\boldsymbol{\\epsilon}_{\\theta}\n1:\nfor\neach iteration\ndo\n2:\n{\n(\nğ‰\n(\nt\n,\nw\n)\n,\nğ’”\nt\n)\n}\nâˆ¼\nğ’Ÿ\n\\{\\boldsymbol{(\\tau}_{(t,w)},\\boldsymbol{s}_{t})\\}\\sim\\mathcal{D}\n3:\nn\nâˆ¼\nğ’°\nâ€‹\n(\n{\n1\n,\n2\n,\nâ€¦\n,\nN\n}\n)\nn\\sim\\mathcal{U}(\\{1,2,\\ldots,N\\})\n4:\nÏµ\nâˆ¼\nğ’©\nâ€‹\n(\nğŸ\n,\nğ‘°\n)\n\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\boldsymbol{I})\n5:\nTake gradient descent step on\nâˆ‡\nÎ¸\nâ€–\nÏµ\nâˆ’\nÏµ\nÎ¸\nâ€‹\n(\nÎ±\nÂ¯\n(\nn\n)\nâ€‹\nğ’”\nğ’•\n+\n1\nâˆ’\nÎ±\nÂ¯\n(\nn\n)\nâ€‹\nÏµ\n,\nğ‰\n(\nt\n,\nw\n)\n,\nn\n)\nâ€–\n2\n\\nabla_{\\theta}\\left\\|\\boldsymbol{\\epsilon}-\\boldsymbol{\\epsilon}_{\\theta}\\left(\\sqrt{\\bar{\\alpha}_{(n)}}\\boldsymbol{s_{t}}+\\sqrt{1-\\bar{\\alpha}_{(n)}}\\boldsymbol{\\epsilon},\\boldsymbol{\\tau}_{(t,w)},n\\right)\\right\\|^{2}\n6:\nend for\n7:\nreturn\nÏµ\nÎ¸\n\\epsilon_{\\theta}\nFigure 12\n:\nAs the agent navigates in the\nmaze2d-large\nenvironment\n[\n15\n]\n, illustrations of states; predicted states from\nForl\n(ours);\nForl\ndiffusion model predictions\n{\nğ’”\nt\n(\n0\n)\n}\nk\n\\{\\boldsymbol{s}_{t}^{(0)}\\}_{k}\n; observations; and predicted states from\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nare shown. Environment timesteps progress from left to right.\nAppendix D\nBaselines\nIn this section we explain the baselines used in Table\n1\nand Figure\n14\n. Diffusion Q-Learning (\nDQL\n)\n[\n14\n]\nserves as our base policy across all baselines and for our method in D4RL experiments\n[\n15\n]\n. Details on\nDQL\nare provided in Section\nE\nD.1\nDQL\nExtensions with\nZero-Shot FM\nFor our baselines, we use a diffusion-based offline RL policy,\nDQL\n[\n14\n]\n, with three variations:\nâ€¢\nDQL\n:\nDQL\npolicy directly generates actions conditioned on the observations. We include it to demonstrate how large episodic offsets degrade policy performance, and its consistently poor performance underscores the difficulty introduced by our offset settings.\nâ€¢\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n:\nWe use\nZero-Shot FM\n(Lag-Llama\n[\n10\n]\n) to forecast episodic offsets over a horizon of\nP\nP\nepisodes using pre-deployment offset history, and then subtract the mean predicted offset corresponding to that episode from the observations. Thus,\nDQL\ngenerates actions conditioned on the sample mean of the forecasted states\n{\ns\n^\nt\nb\n}\nl\n\\{\\hat{s}^{b}_{t}\\}_{l}\n. Although\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\noutperforms\nDQL\n, a straightforward application of forecasting and decision-making reveals performance degradation when offsets lack adaptive correction.\nâ€¢\nDQL\n+\nLag\n-\ns\n~\n\\tilde{s}\n:\nWe use the sample median instead of the sample mean in\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n, to mitigate outlier effects. As shown in Figure\n14\n, median-based bias removal yields no significant improvement over the mean-based approach.\nD.2\nDmbp\n+\nLag\nWe extend the Diffusion Modelâ€“Based Predictor (\nDmbp\n)\n[\n3\n]\n, a model-based, robust offline RL method trained on unperturbed D4RL benchmarks, to correct test-time state observation perturbations.\nDmbp\nhas proven robust under Gaussian noise (varying\nÏƒ\n\\sigma\n), uniform noise, and adversarial perturbationsâ€“including maximum actionâ€difference (MAD) and minimum Qâ€value (Minâ€Q) attacks. Similar to our framework, perturbed states are passed to\nDmbp\n, which removes the perturbations; the generated state is then fed to the policy. For a fair comparison, we adopt the policies also used in their work for our experiments,\nRorl\n[\n25\n]\nand\nTd\n3\nBc\n[\n2\n]\nalongside\nDQL\n.\nDmbp\n+\nLag\nextends\nDmbp\nwith the forecasting module in our experiments. At each timestep, we remove offset biases using the sample mean of forecasted states\n{\ns\n^\nt\nb\n}\nl\n\\{\\hat{s}^{b}_{t}\\}_{l}\n(as in\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n) before\nDmbp\ncorrection.\nDmbp\n+\nLag\nthus sequentially applies forecastâ€based offset compensation, followed by model based perturbation removal, and then queries the policy.\nDQL\n[\n14\n]\nremains the shared policy. A naive integration of\nDmbp\nrelies on initial groundâ€truth states and underperforms.\nFor the evaluation of\nDmbp\nwe use the open source code and the suggested hyperparameters available in\n[\n56\n]\n. To improve the performance of\nDmbp\n, we conducted evaluations at test-time and reported the best-performing results for a range of different diffusion timesteps.\nDmbp\nrequires the diffusion timesteps to be manually defined based on different noise scales and types. Hence, we use the diffusion timesteps across a range of noise scales\n{\n0.15\n,\n0.25\n,\n0.5\n}\n\\{0.15,0.25,0.5\\}\nfor maze and\n{\n0.05\n,\n0.1\n,\n0.15\n,\n0.25\n}\n\\{0.05,0.1,0.15,0.25\\}\nfor kitchen environments and identify that the best performance requires different noise scales across environment datasets. In particular, we report the best performing noise scale of\n0.15\n0.15\nfor the\nmaze2d-medium\nenvironment,\n0.25\n0.25\nfor the\nmaze2d-large\nenvironment,\n0.5\n0.5\nfor the\nantmaze-umaze-diverse\nenvironment,\n0.15\n0.15\nfor the\nantmaze-medium-diverse\nenvironment,\n0.25\n0.25\nfor the\nantmaze-large-diverse\nenvironment,\n0.05\n0.05\nfor\nkitchen-complete\n.\nAppendix E\nOffline Reinforcement Algorithms\nDQL\n[\n14\n]\nDQL\n[\n14\n]\nis an offline RL algorithm that uses policy regularization via a conditional diffusion model\n[\n11\n]\n.\nWang etÂ al. [\n14\n]\nshows that Gaussian policies lack the expressiveness needed to capture the possibly multimodal and skewed behavior policy in offline datasets, which, in turn, limits performance. To remedy this,\nDQL\nuses a conditional diffusion model for the behaviorâ€‘cloning term, shown as the first part of Equation\n8\n, based on a stateâ€‘conditioned version of the simplified Denoising Diffusion Probabilistic Models (DDPM) objective\n[\n11\n]\n. To steer action generation toward highâ€reward regions, the policy improvement loss also includes Qâ€‘value guidance, shown as the second term below\n[\n14\n]\nğ”¼\nn\nâˆ¼\nğ’°\n,\n(\ns\n,\na\n)\nâˆ¼\nğ’Ÿ\n,\nÏµ\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nI\n)\nâ€‹\n[\nâ€–\nÏµ\nâˆ’\nÏµ\nÏ•\nâ€‹\n(\nÎ±\nÂ¯\n(\nn\n)\nâ€‹\na\n+\n1\nâˆ’\nÎ±\nÂ¯\n(\nn\n)\nâ€‹\nÏµ\n,\ns\n,\nn\n)\nâ€–\n2\n]\nâˆ’\nÎ±\nâ€‹\nğ”¼\ns\nâˆ¼\nğ’Ÿ\n,\na\n0\nâˆ¼\nÏ€\nÏ•\nâ€‹\n[\nQ\nÏˆ\nâ€‹\n(\ns\n,\na\n0\n)\n]\n.\n\\mathbb{E}_{\\begin{subarray}{c}n\\sim\\mathcal{U},\\,(s,a)\\sim\\mathcal{D},\\,\\\\\n\\epsilon\\sim\\mathcal{N}(0,I)\\end{subarray}}\\Bigl[\\bigl\\|\\epsilon-\\epsilon_{\\phi}\\!\\bigl(\\sqrt{\\bar{\\alpha}_{(n)}}\\,a+\\sqrt{1-\\bar{\\alpha}_{(n)}}\\,\\epsilon,\\;s,\\;n\\bigr)\\bigr\\|^{2}\\Bigr]\\;-\\;\\alpha\\,\\mathbb{E}_{s\\sim\\mathcal{D},\\,a^{0}\\sim\\pi_{\\phi}}\\bigl[Q_{\\psi}(s,a^{0})\\bigr].\n(8)\nHere, a reverse diffusion process conditioned on state\ns\ns\n, denoted by\nÏ€\nÏ•\nâ€‹\n(\nğ’‚\nâˆ£\nğ’”\n)\n\\pi_{\\phi}\\left(\\boldsymbol{a}\\mid\\boldsymbol{s}\\right)\n, represents the policy. The Qâ€‘networks are trained using the double Qâ€‘learning trick\n[\n57\n]\nand Bellman operator minimization\n[\n43\n,\n58\n]\n, with\n[\n14\n]\nğ”¼\n(\nğ’”\nt\n,\nğ’‚\nt\n,\nğ’”\nt\n+\n1\n)\nâˆ¼\nğ’Ÿ\n,\nğ’‚\nt\n+\n1\n0\nâˆ¼\nÏ€\nÏ•\nâ€²\nâ€‹\n[\nâ€–\n(\nr\nâ€‹\n(\nğ’”\n,\nğ’‚\n)\n+\nÎ³\nâ€‹\nmin\ni\n=\n1\n,\n2\nâ¡\nQ\nÏˆ\ni\nâ€²\nâ€‹\n(\nğ’”\nt\n+\n1\n,\nğ’‚\nt\n+\n1\n0\n)\n)\nâˆ’\nQ\nÏˆ\ni\nâ€‹\n(\nğ’”\nt\n,\nğ’‚\nt\n)\nâ€–\n2\n]\n\\mathbb{E}_{(\\boldsymbol{s}_{t},\\boldsymbol{a}_{t},\\boldsymbol{s}_{t+1})\\sim\\mathcal{D},\\boldsymbol{a}^{0}_{t+1}\\sim\\pi_{\\phi^{\\prime}}}\\left[\\left\\|\\left(r(\\boldsymbol{s},\\boldsymbol{a})+\\gamma\\min_{i=1,2}Q_{\\psi_{i}^{\\prime}}(\\boldsymbol{s}_{t+1},\\boldsymbol{a}^{0}_{t+1})\\right)-Q_{\\psi_{i}}(\\boldsymbol{s}_{t},\\boldsymbol{a}_{t})\\right\\|^{2}\\right]\n(9)\nwhere\na\nt\n+\n1\n0\na^{0}_{t+1}\nis sampled from the diffusion policy conditioned on\ns\nt\n+\n1\ns_{t+1}\n, and\nQ\nÏˆ\ni\nQ_{\\psi_{i}}\n,\nQ\nÏˆ\ni\nâ€²\nQ_{\\psi^{\\prime}_{i}}\n,\nÏ•\nâ€²\n\\phi^{\\prime}\ndenote the critic and targetâ€critic networks, target policy network, respectively.\nTd\n3\nBc\n[\n2\n]\nTD3BC\n[\n2\n]\nextends the Twin Delayed Deep Deterministic policy gradient algorithm (TD3)\n[\n59\n]\nto the offline RL setup.\nTd\n3\nBc\nincorporates a behavior cloning regularization term, normalizes state features within the offline RL dataset, and scales the Q-function using a hyperparameter with an added normalization term. TD3BC is a straightforward yet effective method that is also computationally efficient. The results in\nTable\nËœ\n2\nand\nTable\nËœ\n3\nshow that although extending\nTd\n3\nBc\nwith the forecasting module (\nTd\n3\nBc\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n) and a combination of the forecasting module and a diffusion model-based predictor (\nDmbp\n+\nLag\n-T) improve performance,\nForl\n-T achieves better performance across a diverse range of non-stationarities.\nRorl\n[\n25\n]\nRORL\n[\n25\n]\naddresses adversarial perturbations of the observation function by learning a conservative policy that aims to be robust to outâ€‘ofâ€‘distribution (OOD) state and action pairs. To achieve this, it introduces a conservative smoothing mechanism that balances mitigating abrupt changes in the value function for proximate states and avoiding value overestimation in risky regions that are absent from the dataset. Concretely, RORL regularizes both the policy and the value function, leveraging bootstrapping Q-functions and conservative smoothing of the perturbed states. This formulation yields robust training under adversarial perturbations in the observation function while preserving strong performance even in unperturbed environments. However, although using RORL as our base policy improved performance in\nmaze2d\nenvironments,\nForl\nsignificantly outperforms both its naive usage, the extension with our forecasting module\nRorl\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n, and\nDmbp\n+\nLag\n-R in\nTable\nËœ\n2\nand\nTable\nËœ\n3\n. These results demonstrate that policies designed to be robust to sensor noise or adversarial attacks fail to cope with evolving observation functions that introduce nonâ€‘stationarity into the environment.\nIql\n[\n26\n]\nImplicit Q-Learning\n[\n26\n]\nfirst learns a value function by expectile loss and a Q-function by Mean Squared Error (MSE) Loss without using the policy and instead using actions from the dataset. In doing so, they avoid approximating the values of unseen actions. Then, it learns the policy using advantage weighted regression\n[\n60\n,\n61\n,\n48\n,\n49\n]\nusing the learned Q-function and value function. We use the open-source implementation of\nIql\nfrom\n[\n62\n]\nwhich references the source\n[\n63\n]\n.\nResults in\nTable\nËœ\n4\nshow\nForl\n-I outperforms the baselines\nIql\n,\nIql\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n, and\nDmbp\n+\nLag\n-I across all environments.\nFQL\n[\n23\n]\nFlow Q-learning (\nFQL\n)\n[\n23\n]\nis a recent offline RL policy that shows strong performance on the OGBench\n[\n21\n]\n. We use\nFQL\nfor the offline RL environments in OGBench and adopt the hyperparameters from the open-source implementation\n1\n1\n1\nhttps://github.com/seohongpark/fql/\n. Similar to\nDQL\n, which uses diffusion models,\nFQL\ncan learn an expressive policy.\nFQL\ntrains two policies: (i) a flow policy trained with flow matching on the offline RL dataset for behavior cloning conditioned on the state, and (ii) a one-step policy trained with a distillation loss using the flow policy\n[\n64\n,\n65\n,\n66\n,\n67\n,\n68\n]\nand a critic loss. This approach avoids expensive backpropagation through time; thus, it is fast during inference and training. Results in\nTables\nËœ\n1\nand\n7\nshow\nForl\n(also referred to as\nForl\n(DCM)) outperforms the baselines when all baselines use\nFQL\npolicy.\nTable 3\n:\nNormalized scores (mean Â± std.) for FORL and baselines with\nTd\n3\nBc\nand\nRorl\non\nmaze2d-medium\n.\nAlgorithms are grouped by their underlying policiesâ€”TD3 with Behavior Cloning (TD3+BC)\n[\n2\n]\nand Robust Offline Reinforcement Learning (RORL)\n[\n25\n]\nto highlight that performance variations stem from the algorithms themselves rather than the policies employed. Suffixes -T and -R denote the use of TD3+BC and RORL policies, respectively.\nTd\n3\nBc\nPolicy\nRorl\nPolicy\nmaze2d-medium\nTd\n3\nBc\nTd\n3\nBc\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nDmbp\n+\nLag\n-T\nForl\n(ours)-T\nRorl\nRorl\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nDmbp\n+\nLag\n-R\nForl\n(ours)-R\nreal-data-A\n37.4\nÂ±\n\\pm\n9.5\n16.2\nÂ±\n\\pm\n5.1\n16.2\nÂ±\n\\pm\n7.3\n22.1\nÂ±\n\\pm\n6.6\n80.7\nÂ±\n\\pm\n14.2\n57.9\nÂ±\n\\pm\n8.6\n47.8\nÂ±\n\\pm\n10.2\n52.7\nÂ±\n\\pm\n5.0\nreal-data-B\n3.6\nÂ±\n\\pm\n2.3\n6.1\nÂ±\n\\pm\n4.4\n14.4\nÂ±\n\\pm\n8.1\n28.6\nÂ±\n\\pm\n19.0\n37.8\nÂ±\n\\pm\n7.5\n85.9\nÂ±\n\\pm\n19.8\n91.0\nÂ±\n\\pm\n21.6\n109.6\nÂ±\n\\pm\n19.5\nreal-data-C\n-2.3\nÂ±\n\\pm\n1.3\n30.0\nÂ±\n\\pm\n10.0\n19.3\nÂ±\n\\pm\n6.2\n24.5\nÂ±\n\\pm\n10.2\n33.7\nÂ±\n\\pm\n4.6\n89.2\nÂ±\n\\pm\n15.8\n93.4\nÂ±\n\\pm\n16.3\n125.4\nÂ±\n\\pm\n14.5\nreal-data-D\n6.3\nÂ±\n\\pm\n3.4\n15.5\nÂ±\n\\pm\n3.8\n12.2\nÂ±\n\\pm\n2.6\n38.7\nÂ±\n\\pm\n13.4\n37.0\nÂ±\n\\pm\n17.0\n71.2\nÂ±\n\\pm\n27.2\n77.7\nÂ±\n\\pm\n26.2\n136.1\nÂ±\n\\pm\n11.9\nreal-data-E\n-3.7\nÂ±\n\\pm\n1.5\n9.7\nÂ±\n\\pm\n5.2\n11.5\nÂ±\n\\pm\n6.6\n15.1\nÂ±\n\\pm\n8.8\n60.9\nÂ±\n\\pm\n13.5\n10.0\nÂ±\n\\pm\n7.3\n14.2\nÂ±\n\\pm\n8.6\n61.2\nÂ±\n\\pm\n14.6\nAverage\n8.3\nÂ±\n\\pm\n15.5\nÂ±\n\\pm\n14.7\nÂ±\n\\pm\n25.8\nÂ±\n\\pm\n50.0\nÂ±\n\\pm\n62.8\nÂ±\n\\pm\n64.8\nÂ±\n\\pm\n97.0\nÂ±\n\\pm\nTable 4\n:\nNormalized scores (mean Â± std.) for FORL and baselines with\nIql\n.\nSuffix -I denote the use of\nIql\nalgorithm\n[\n26\n]\n.\nmaze2d-medium\nIql\nIql\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nDmbp\n+\nLag\n-I\nForl\n(ours)-I\nreal-data-A\n39.5\nÂ±\n\\pm\n7.8\n16.0\nÂ±\n\\pm\n4.9\n12.2\nÂ±\n\\pm\n7.7\n19.5\nÂ±\n\\pm\n2.9\nreal-data-B\n3.7\nÂ±\n\\pm\n7.3\n13.1\nÂ±\n\\pm\n8.7\n14.0\nÂ±\n\\pm\n9.5\n32.3\nÂ±\n\\pm\n13.1\nreal-data-C\n-1.1\nÂ±\n\\pm\n2.5\n33.1\nÂ±\n\\pm\n11.9\n28.0\nÂ±\n\\pm\n10.9\n31.8\nÂ±\n\\pm\n9.9\nreal-data-D\n10.1\nÂ±\n\\pm\n2.6\n12.4\nÂ±\n\\pm\n5.6\n12.7\nÂ±\n\\pm\n9.0\n40.0\nÂ±\n\\pm\n10.1\nreal-data-E\n-4.5\nÂ±\n\\pm\n0.2\n12.4\nÂ±\n\\pm\n5.2\n9.9\nÂ±\n\\pm\n2.9\n24.0\nÂ±\n\\pm\n8.4\nAverage\n9.6\nÂ±\n\\pm\n17.4\nÂ±\n\\pm\n15.4\nÂ±\n\\pm\n29.5\nÂ±\n\\pm\nmaze2d-large\nreal-data-A\n16.2\nÂ±\n\\pm\n6.2\n12.5\nÂ±\n\\pm\n5.5\n6.0\nÂ±\n\\pm\n4.7\n24.3\nÂ±\n\\pm\n9.4\nreal-data-B\n-0.6\nÂ±\n\\pm\n2.6\n3.3\nÂ±\n\\pm\n7.7\n13.5\nÂ±\n\\pm\n10.5\n42.8\nÂ±\n\\pm\n9.8\nreal-data-C\n0.1\nÂ±\n\\pm\n1.5\n27.8\nÂ±\n\\pm\n13.1\n27.9\nÂ±\n\\pm\n5.5\n46.7\nÂ±\n\\pm\n9.2\nreal-data-D\n1.2\nÂ±\n\\pm\n3.7\n11.2\nÂ±\n\\pm\n7.4\n8.0\nÂ±\n\\pm\n7.5\n23.9\nÂ±\n\\pm\n8.4\nreal-data-E\n-2.3\nÂ±\n\\pm\n0.2\n-1.5\nÂ±\n\\pm\n1.1\n1.5\nÂ±\n\\pm\n4.1\n7.9\nÂ±\n\\pm\n7.2\nAverage\n2.9\nÂ±\n\\pm\n10.7\nÂ±\n\\pm\n11.4\nÂ±\n\\pm\n29.1\nÂ±\n\\pm\nantmaze-umaze-diverse\nreal-data-A\n23.0\nÂ±\n\\pm\n1.4\n43.7\nÂ±\n\\pm\n6.1\n44.0\nÂ±\n\\pm\n9.6\n50.3\nÂ±\n\\pm\n11.0\nreal-data-B\n21.7\nÂ±\n\\pm\n5.4\n55.0\nÂ±\n\\pm\n6.2\n61.7\nÂ±\n\\pm\n9.5\n70.8\nÂ±\n\\pm\n13.2\nreal-data-C\n20.0\nÂ±\n\\pm\n3.2\n45.4\nÂ±\n\\pm\n4.0\n58.3\nÂ±\n\\pm\n5.1\n73.8\nÂ±\n\\pm\n5.4\nreal-data-D\n6.7\nÂ±\n\\pm\n5.6\n26.7\nÂ±\n\\pm\n8.6\n29.2\nÂ±\n\\pm\n6.6\n77.5\nÂ±\n\\pm\n4.8\nreal-data-E\n8.0\nÂ±\n\\pm\n8.7\n60.0\nÂ±\n\\pm\n11.8\n56.0\nÂ±\n\\pm\n9.8\n68.0\nÂ±\n\\pm\n13.0\nAverage\n15.9\nÂ±\n\\pm\n46.1\nÂ±\n\\pm\n49.8\nÂ±\n\\pm\n68.1\nÂ±\n\\pm\nantmaze-medium-diverse\nreal-data-A\n18.3\nÂ±\n\\pm\n5.5\n21.0\nÂ±\n\\pm\n5.8\n24.7\nÂ±\n\\pm\n4.0\n17.7\nÂ±\n\\pm\n3.5\nreal-data-B\n7.5\nÂ±\n\\pm\n3.5\n7.5\nÂ±\n\\pm\n5.4\n8.3\nÂ±\n\\pm\n7.8\n11.7\nÂ±\n\\pm\n7.5\nreal-data-C\n2.5\nÂ±\n\\pm\n3.7\n18.8\nÂ±\n\\pm\n6.1\n15.8\nÂ±\n\\pm\n4.8\n16.2\nÂ±\n\\pm\n7.4\nreal-data-D\n8.3\nÂ±\n\\pm\n4.2\n12.5\nÂ±\n\\pm\n5.9\n12.5\nÂ±\n\\pm\n7.8\n16.7\nÂ±\n\\pm\n2.9\nreal-data-E\n3.3\nÂ±\n\\pm\n4.1\n22.7\nÂ±\n\\pm\n8.0\n22.7\nÂ±\n\\pm\n12.3\n22.7\nÂ±\n\\pm\n6.4\nAverage\n8.0\nÂ±\n\\pm\n16.5\nÂ±\n\\pm\n16.8\nÂ±\n\\pm\n17.0\nÂ±\n\\pm\nAppendix F\nScaling Offsets\nFigure 13\n:\nAverage normalized scores of\nForl\n(ours) and baselines across offset scaling factors\nÎ±\nâˆˆ\n{\n0\n,\n0.25\n,\n0.5\n,\n0.75\n,\n1\n}\n\\alpha\\in\\{0,0.25,0.5,0.75,1\\}\nin the\nmaze2d-medium\n,\nmaze2d-large\n,\nantmaze-umaze-diverse\n,\nantmaze-medium-diverse\n,\nantmaze-large-diverse\nenvironments.\nScaling factor\nÎ±\n=\n0\n\\alpha=0\ncorresponds to a stationary D4RL\n[\n15\n]\ntest environment;\nÎ±\n=\n1\n\\alpha=1\nmatches the original experimental configuration in Fig.\n14\n. Results are averaged over 5 non-stationarities (\nreal-data-A\n,\nreal-data-B\n,\nreal-data-C\n,\nreal-data-D\n,\nreal-data-E\n) and 5 random seeds.\nWe conduct an analysis to quantify\nForl\nâ€™s sensitivity to different levels of non-stationarity. We scale the offset magnitude from 0 (standard D4RL offline RL environment\n[\n15\n]\nused in training) to 1 (our experiments) using scaling factors\n{\n0\n,\n0.25\n,\n0.5\n,\n0.75\n,\n1.0\n}\n\\{0,0.25,0.5,0.75,1.0\\}\n, and report performance over five random seeds across five environments in\nmaze2d\nand\nantmaze\nin Figure\n13\n. This analysis delivers two key insights: first,\nForl\nmatches baseline performance on the stationary offline RL dataset on which it was trained; second, it maintains superior results throughout the full range of non-stationarity magnitudes. Crucially,\nForl\nâ€™s performance degrades gracefully as the magnitude of offsets increases, whereas\nDQL\n(without forecasting) suffers steep drops. Furthermore, both\nDmbp\n+\nLag\nand\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\ndecline in a similar manner.\nDmbp\n+\nLag\ndegrades slightly more gracefully than\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nin\nmaze2d-large\nand\nantmaze-umaze-diverse\n.\nAppendix G\nWhat if we do not have access to past offsets?\nFigure 14\n:\nComparison of average normalized scores and prediction errors across all 25 experiments in D4RL\n[\n15\n]\nbetween\nForl\n(ours)\nand baseline methods, each evaluated over\n5 random seeds\n. LAG denotes the integration of a zero-shot time-series foundation model\n[\n10\n]\n. While\nForl\nand\nmax\nalso utilize this model, the subscripts are omitted for brevity.\nFigure 15\n:\nPerformance comparison without access to past offsets.\nAverage normalized scores and prediction errors for\nForl\n(ours)\nversus baselines, aggregated over 25 experiments (5 random seeds each) in D4RL\n[\n15\n]\n. LAG denotes the integration of a zero-shot time-series foundation model\n[\n10\n]\n. However, in this setting\nZero-Shot FM\nuses the samples from\nDm\ninstead of past offsets.\nTo analyze the challenging identifiability issue arising from (i) nonâ€‘smoothly varying offsets and (ii) the unobservability of ground truth offsets throughout the evaluation interval, we implement a set of methods for the setting where we never have delayed access to past ground truth offsets.\nFig.\nËœ\n14\nshows the average normalized scores and prediction accuracies over 25 environmentâ€“non-stationarity pairs across five random seeds in navigation control tasks with continuous state and action spaces. Overall, these methods underperform compared to\nForl\n. Among the cases with no access to past offsets (see\nFig.\nËœ\n15\n), the best-performing methods are our proposed candidate state generation module (\nForl\n-\nDm\n) and\nH-Lag\n+\ndcm\n, a version of\nForl\nthat utilizes\nZero-Shot FM\nin addition to\nDm\nwhich we detailed in\nSection\nËœ\n3.1.1\n.\nForl\n-\nDm\n(\nDm\n)\nForl\n-\nDm\n, which we also refer to as\nDm\nfor brevity, directly uses the state predicted by the diffusion model component. Given the multimodal nature of these candidate states, this selection does not fully leverage our framework. Yet,\nDm\noutperforms the baselines when no past offsets are used. Additional comparisons in\nTable\nËœ\n6\nwith other standard statistical methods like\nDm-Mad\n,\nDm-Ransac\n,\nDm-Running\n-\nÎ¼\n\\mu\n,\nDm-Running\n-\nÎ¼\n\\mu\n-p\nindicate that only using the sample predicted by the diffusion model yields higher scores on average.\nMed\n+\ndcm\nWe compute the median of the predicted offsets from the previous episode, beginning with the first episode predicted by\nForl\nâ€™s diffusion model. We then fit a Gaussian distribution centered at this median and sample\nl\nl\noffsets from it, matching the sample count of\nZero-Shot FM\n. Next, we apply\nDCM\nto these samples with the candidates generated by the diffusion model.\nFigure 16\n:\nIllustrations of states, observations, diffusion model predictions\n{\ns\nt\n(\n0\n)\n}\nk\n\\{\\boldsymbol{s}_{t}^{(0)}\\}_{k}\n, and predicted states from\nForl\n(DCM),\nForl\n-\nDm\n,\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n,\nDQL\n+\nLag\n-\ns\n~\n\\tilde{s}\n,\nForl\n(\nmax\n),\nDmbp\n+\nLag\nare shown.\nThese visualizations are from the same setting presented in\nFig.\nËœ\n12\n. Maximum, minimum, and mean prediction errors across episodes for this task are provided in\nTable\nËœ\n8\n.\nMed\n+\nNoise\nWe begin by computing the median offset produced by the diffusion model during the first evaluation episode similar to\nMed\n+\ndcm\n. Thereafter, we treat these offsets as evolving according to a random walk, where each offset is predicted as the previous value with white noise increments.\nDm-Mad\nDm-Mad\nfollows a state estimation based on robust statistics\n[\n69\n]\n.\nDm-Mad\ncomputes the coordinate-wise median of the differences between the observation and each denoiser prediction and discards any sample whose absolute deviation from this median exceeds\nÏµ\n\\epsilon\nx Median Absolute Deviation (MAD). Then, it takes the median of the remaining inliers to obtain a robust offset estimate and subtracts that offset from the observation to obtain the state\ns\nt\n~\n\\tilde{s_{t}}\n.\nDm-Ransac\nDm-Ransac\nis a RANdom SAmple Consensus (RANSAC)\n[\n70\n]\nbased state estimation using the samples from our\nDm\n.\nDm-Ransac\ncalculates the offsets using the states generated by our\nDm\nand the observation, setting an adaptive per-dimension threshold as\nÏµ\n\\epsilon\nx MAD. Then, it repeatedly samples a random offset candidate and chooses the candidate whose inlier set (differences within that threshold) is the largest. Then, it takes the average of those inliers to estimate the offset.\nDm-Running\n-\nÎ¼\n\\mu\nDm-Running\n-\nÎ¼\n\\mu\ncomputes a numerically stable, global running mean\n[\n71\n,\n72\n]\nof offsets from\nDm\naggregated across all timesteps and episodes.\nDm-Running\n-\nÎ¼\n\\mu\n-p\nDm-Running\n-\nÎ¼\n\\mu\n-p\ncomputes an online average of offsets\n[\n71\n,\n72\n]\nfrom\nDm\n(Running-\nÎ¼\n\\mu\n) per episode\np\np\n. Unlike\nDm-Running\n-\nÎ¼\n\\mu\n, in\nDm-Running\n-\nÎ¼\n\\mu\n-p\nthe statistics are reset to zero at the beginning of each episode.\nTable 5\n:\nNormalized scores (mean Â± std.) for no-access to past offsets setting.\nThis table shows the performance comparison of other heuristics variants using\nForl\n-\nDm\nor leveraging a\nZero-Shot FM\nin combination with\nForl\n-\nDm\nwhen we do not have access to past offsets.\nmaze2d-medium\nDQL\nMed\n+\nNoise\nMed\n+\ndcm\nH-Lag\nH-Lag\n+\ndcm\nForl\n-\nDm\nreal-data-A\n30.2\nÂ±\n\\pm\n6.5\n27.4\nÂ±\n\\pm\n14.5\n27.4\nÂ±\n\\pm\n12.2\n29.7\nÂ±\n\\pm\n11.3\n48.8\nÂ±\n\\pm\n10.0\n55.2\nÂ±\n\\pm\n10.6\nreal-data-B\n14.1\nÂ±\n\\pm\n12.1\n23.9\nÂ±\n\\pm\n14.3\n33.1\nÂ±\n\\pm\n19.5\n4.5\nÂ±\n\\pm\n12.3\n50.3\nÂ±\n\\pm\n12.9\n56.8\nÂ±\n\\pm\n24.7\nreal-data-C\n-2.3\nÂ±\n\\pm\n3.3\n17.6\nÂ±\n\\pm\n8.1\n-2.6\nÂ±\n\\pm\n2.5\n26.5\nÂ±\n\\pm\n5.4\n30.3\nÂ±\n\\pm\n5.6\n52.8\nÂ±\n\\pm\n10.3\nreal-data-D\n4.7\nÂ±\n\\pm\n5.0\n18.9\nÂ±\n\\pm\n9.1\n64.1\nÂ±\n\\pm\n12.7\n18.2\nÂ±\n\\pm\n13.7\n1.8\nÂ±\n\\pm\n3.8\n60.1\nÂ±\n\\pm\n20.2\nreal-data-E\n3.5\nÂ±\n\\pm\n8.8\n21.8\nÂ±\n\\pm\n15.6\n-2.1\nÂ±\n\\pm\n1.6\n56.7\nÂ±\n\\pm\n11.6\n45.7\nÂ±\n\\pm\n12.4\n60.5\nÂ±\n\\pm\n18.2\nAverage\n10.0\nÂ±\n\\pm\n21.9\nÂ±\n\\pm\n24.0\nÂ±\n\\pm\n27.1\nÂ±\n\\pm\n35.4\nÂ±\n\\pm\n57.1\nÂ±\n\\pm\nmaze2d-large\nreal-data-A\n16.2\nÂ±\n\\pm\n5.5\n6.4\nÂ±\n\\pm\n2.9\n-1.8\nÂ±\n\\pm\n0.5\n7.1\nÂ±\n\\pm\n3.9\n7.3\nÂ±\n\\pm\n2.0\n11.1\nÂ±\n\\pm\n4.3\nreal-data-B\n-0.5\nÂ±\n\\pm\n2.9\n-0.1\nÂ±\n\\pm\n1.7\n-1.4\nÂ±\n\\pm\n1.7\n-1.4\nÂ±\n\\pm\n2.3\n0.9\nÂ±\n\\pm\n4.5\n13.4\nÂ±\n\\pm\n10.3\nreal-data-C\n0.9\nÂ±\n\\pm\n1.7\n3.2\nÂ±\n\\pm\n4.7\n-2.0\nÂ±\n\\pm\n0.6\n3.4\nÂ±\n\\pm\n2.0\n0.5\nÂ±\n\\pm\n1.7\n9.4\nÂ±\n\\pm\n2.5\nreal-data-D\n3.0\nÂ±\n\\pm\n6.6\n3.7\nÂ±\n\\pm\n6.8\n47.6\nÂ±\n\\pm\n16.4\n1.0\nÂ±\n\\pm\n4.5\n2.9\nÂ±\n\\pm\n4.0\n7.4\nÂ±\n\\pm\n7.2\nreal-data-E\n-2.1\nÂ±\n\\pm\n0.4\n3.8\nÂ±\n\\pm\n4.7\n-0.5\nÂ±\n\\pm\n3.7\n1.8\nÂ±\n\\pm\n2.7\n7.2\nÂ±\n\\pm\n2.3\n7.7\nÂ±\n\\pm\n7.6\nAverage\n3.5\nÂ±\n\\pm\n3.4\nÂ±\n\\pm\n8.4\nÂ±\n\\pm\n2.4\nÂ±\n\\pm\n3.8\nÂ±\n\\pm\n9.8\nÂ±\n\\pm\nantmaze-umaze-diverse\nreal-data-A\n22.7\nÂ±\n\\pm\n3.0\n8.3\nÂ±\n\\pm\n2.4\n8.3\nÂ±\n\\pm\n4.7\n32.3\nÂ±\n\\pm\n6.3\n62.7\nÂ±\n\\pm\n11.2\n48.7\nÂ±\n\\pm\n9.1\nreal-data-B\n24.2\nÂ±\n\\pm\n3.5\n9.2\nÂ±\n\\pm\n5.4\n25.0\nÂ±\n\\pm\n13.2\n41.7\nÂ±\n\\pm\n5.1\n33.3\nÂ±\n\\pm\n2.9\n56.7\nÂ±\n\\pm\n8.1\nreal-data-C\n21.7\nÂ±\n\\pm\n3.5\n10.8\nÂ±\n\\pm\n7.7\n75.8\nÂ±\n\\pm\n6.4\n36.7\nÂ±\n\\pm\n3.2\n59.2\nÂ±\n\\pm\n11.5\n55.0\nÂ±\n\\pm\n8.7\nreal-data-D\n5.8\nÂ±\n\\pm\n2.3\n17.5\nÂ±\n\\pm\n11.6\n6.7\nÂ±\n\\pm\n6.3\n10.8\nÂ±\n\\pm\n2.3\n42.5\nÂ±\n\\pm\n7.5\n54.2\nÂ±\n\\pm\n7.2\nreal-data-E\n6.0\nÂ±\n\\pm\n6.8\n50.0\nÂ±\n\\pm\n13.5\n2.0\nÂ±\n\\pm\n3.0\n14.0\nÂ±\n\\pm\n8.3\n42.7\nÂ±\n\\pm\n11.9\n53.3\nÂ±\n\\pm\n7.8\nAverage\n16.1\nÂ±\n\\pm\n19.2\nÂ±\n\\pm\n23.6\nÂ±\n\\pm\n27.1\nÂ±\n\\pm\n48.1\nÂ±\n\\pm\n53.6\nÂ±\n\\pm\nantmaze-medium-diverse\nreal-data-A\n31.0\nÂ±\n\\pm\n6.5\n34.3\nÂ±\n\\pm\n10.4\n55.0\nÂ±\n\\pm\n6.8\n15.0\nÂ±\n\\pm\n4.9\n17.3\nÂ±\n\\pm\n6.3\n4.3\nÂ±\n\\pm\n0.9\nreal-data-B\n23.3\nÂ±\n\\pm\n4.8\n15.8\nÂ±\n\\pm\n9.9\n42.5\nÂ±\n\\pm\n4.6\n33.3\nÂ±\n\\pm\n4.2\n28.3\nÂ±\n\\pm\n3.5\n6.7\nÂ±\n\\pm\n4.8\nreal-data-C\n10.0\nÂ±\n\\pm\n2.3\n23.3\nÂ±\n\\pm\n5.4\n10.0\nÂ±\n\\pm\n4.3\n31.2\nÂ±\n\\pm\n4.9\n40.4\nÂ±\n\\pm\n6.7\n4.6\nÂ±\n\\pm\n1.7\nreal-data-D\n11.7\nÂ±\n\\pm\n5.4\n26.7\nÂ±\n\\pm\n6.3\n33.3\nÂ±\n\\pm\n15.9\n9.2\nÂ±\n\\pm\n3.5\n36.7\nÂ±\n\\pm\n6.8\n3.3\nÂ±\n\\pm\n3.5\nreal-data-E\n18.7\nÂ±\n\\pm\n4.5\n26.7\nÂ±\n\\pm\n18.1\n0.0\nÂ±\n\\pm\n0.0\n14.7\nÂ±\n\\pm\n5.6\n46.0\nÂ±\n\\pm\n16.2\n6.0\nÂ±\n\\pm\n4.3\nAverage\n18.9\nÂ±\n\\pm\n25.4\nÂ±\n\\pm\n28.2\nÂ±\n\\pm\n20.7\nÂ±\n\\pm\n33.7\nÂ±\n\\pm\n5.0\nÂ±\n\\pm\nantmaze-large-diverse\nreal-data-A\n11.0\nÂ±\n\\pm\n1.9\n5.0\nÂ±\n\\pm\n3.9\n1.3\nÂ±\n\\pm\n1.4\n9.0\nÂ±\n\\pm\n1.9\n11.3\nÂ±\n\\pm\n4.1\n11.0\nÂ±\n\\pm\n3.0\nreal-data-B\n5.8\nÂ±\n\\pm\n4.8\n7.5\nÂ±\n\\pm\n3.5\n5.0\nÂ±\n\\pm\n5.4\n9.2\nÂ±\n\\pm\n1.9\n7.5\nÂ±\n\\pm\n1.9\n11.7\nÂ±\n\\pm\n4.6\nreal-data-C\n5.4\nÂ±\n\\pm\n2.4\n5.4\nÂ±\n\\pm\n2.4\n1.7\nÂ±\n\\pm\n0.9\n10.8\nÂ±\n\\pm\n2.7\n12.9\nÂ±\n\\pm\n5.8\n12.1\nÂ±\n\\pm\n2.7\nreal-data-D\n2.5\nÂ±\n\\pm\n2.3\n15.8\nÂ±\n\\pm\n6.2\n11.7\nÂ±\n\\pm\n4.6\n8.3\nÂ±\n\\pm\n6.6\n14.2\nÂ±\n\\pm\n6.3\n15.0\nÂ±\n\\pm\n9.6\nreal-data-E\n5.3\nÂ±\n\\pm\n3.8\n5.3\nÂ±\n\\pm\n3.0\n1.3\nÂ±\n\\pm\n1.8\n8.7\nÂ±\n\\pm\n3.0\n6.0\nÂ±\n\\pm\n2.8\n8.7\nÂ±\n\\pm\n5.1\nAverage\n6.0\nÂ±\n\\pm\n7.8\nÂ±\n\\pm\n4.2\nÂ±\n\\pm\n9.2\nÂ±\n\\pm\n10.4\nÂ±\n\\pm\n11.7\nÂ±\n\\pm\nTable 6\n:\nAdditional results for no-access to past offsets setting.\nWe present alternative ways of using\nForl\nâ€™s diffusion model (\nDm\n) component when we do not have access to past offsets.\nmaze2d-medium\nDm-Mad\nDm-Ransac\nDm-Running\n-\nÎ¼\n\\mu\nDm-Running\n-\nÎ¼\n\\mu\n-p\nForl\n-\nDm\nreal-data-A\n44.3\nÂ±\n\\pm\n10.6\n46.8\nÂ±\n\\pm\n12.4\n37.8\nÂ±\n\\pm\n13.6\n37.4\nÂ±\n\\pm\n8.7\n55.2\nÂ±\n\\pm\n10.6\nreal-data-B\n46.8\nÂ±\n\\pm\n19.0\n44.6\nÂ±\n\\pm\n17.1\n42.4\nÂ±\n\\pm\n24.5\n51.6\nÂ±\n\\pm\n11.7\n56.8\nÂ±\n\\pm\n24.7\nreal-data-C\n46.4\nÂ±\n\\pm\n10.9\n44.5\nÂ±\n\\pm\n10.1\n41.8\nÂ±\n\\pm\n16.0\n61.7\nÂ±\n\\pm\n14.1\n52.8\nÂ±\n\\pm\n10.3\nreal-data-D\n47.7\nÂ±\n\\pm\n22.9\n49.9\nÂ±\n\\pm\n23.2\n44.4\nÂ±\n\\pm\n24.1\n52.5\nÂ±\n\\pm\n13.3\n60.1\nÂ±\n\\pm\n20.2\nreal-data-E\n42.9\nÂ±\n\\pm\n10.6\n52.4\nÂ±\n\\pm\n15.7\n48.3\nÂ±\n\\pm\n18.8\n24.2\nÂ±\n\\pm\n12.2\n60.5\nÂ±\n\\pm\n18.2\nAverage\n45.6\nÂ±\n\\pm\n47.6\nÂ±\n\\pm\n42.9\nÂ±\n\\pm\n45.5\nÂ±\n\\pm\n57.1\nÂ±\n\\pm\nTable 7\n:\nNormalized scores (mean Â± std.) for\nForl\nframework and the baselines.\ncube-single-play\nFQL\nForl\n-\nDm\n-F\nFQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nForl\n(\nmax\n)-F\nForl(KDE)\n-F\nForl\n-F (ours)\nreal-data-A\n0.0\nÂ±\n\\pm\n0.0\n3.0\nÂ±\n\\pm\n1.4\n0.0\nÂ±\n\\pm\n0.0\n0.0\nÂ±\n\\pm\n0.0\n0.0\nÂ±\n\\pm\n0.0\n23.7\nÂ±\n\\pm\n3.6\nreal-data-B\n0.0\nÂ±\n\\pm\n0.0\n6.7\nÂ±\n\\pm\n5.6\n15.0\nÂ±\n\\pm\n7.0\n43.3\nÂ±\n\\pm\n4.8\n2.5\nÂ±\n\\pm\n2.3\n60.0\nÂ±\n\\pm\n7.0\nreal-data-C\n0.4\nÂ±\n\\pm\n0.9\n4.6\nÂ±\n\\pm\n1.7\n10.0\nÂ±\n\\pm\n1.7\n39.6\nÂ±\n\\pm\n4.4\n38.3\nÂ±\n\\pm\n3.8\n42.1\nÂ±\n\\pm\n5.6\nreal-data-D\n0.0\nÂ±\n\\pm\n0.0\n2.5\nÂ±\n\\pm\n2.3\n0.8\nÂ±\n\\pm\n1.9\n7.5\nÂ±\n\\pm\n1.9\n0.0\nÂ±\n\\pm\n0.0\n70.0\nÂ±\n\\pm\n13.0\nreal-data-E\n0.0\nÂ±\n\\pm\n0.0\n8.0\nÂ±\n\\pm\n3.0\n0.0\nÂ±\n\\pm\n0.0\n21.3\nÂ±\n\\pm\n8.0\n0.0\nÂ±\n\\pm\n0.0\n32.7\nÂ±\n\\pm\n9.5\nAverage\n0.1\nÂ±\n\\pm\n5.0\nÂ±\n\\pm\n5.2\nÂ±\n\\pm\n22.3\nÂ±\n\\pm\n8.2\nÂ±\n\\pm\n45.7\nÂ±\n\\pm\nAppendix H\nCandidate Selection\nThe results in\nTable\nËœ\n9\n,\nTable\nËœ\n8\n, and\nTable\nËœ\n7\nshow that\nForl\n(DCM) more consistently performs better than other methods. Although the KDE-based\nForl(KDE)\nmethod performs well in\nantmaze-medium-diverse\n, it significantly underperforms in\ncube-single-play\n(\nTable\nËœ\n7\n). Moreover, it requires bandwidth selection and a fallback mechanism to handle numerical instability, which highlights the practical advantage of DCM.\nForl\n(\nmax\n), also referred to as\nmax\nfor brevity, can fail when the forecast mean of\nD\nt\nâ€‹\ni\nâ€‹\nm\nâ€‹\ne\nâ€‹\ns\nâ€‹\ne\nâ€‹\nr\nâ€‹\ni\nâ€‹\ne\nâ€‹\ns\nD_{timeseries}\nis biased, misleading it to select a candidate from a geometrically distant mode that appears more likely under an inaccurate forecast. In contrast, DCM succeeds because its state estimation is not dependent on the forecastâ€™s mean, but on a non-parametric search for the forecast sample with the highest score (minimum dimension-wise distance). Hence, DCMâ€™s prediction error is governed by the accuracy of the forecast sample in\nD\nt\nâ€‹\ni\nâ€‹\nm\nâ€‹\ne\nâ€‹\ns\nâ€‹\ne\nâ€‹\nr\nâ€‹\ni\nâ€‹\ne\nâ€‹\ns\nD_{timeseries}\nwith the best score. Empirically, this yields lower maximum and mean errors compared to\nmax\n. The timeseries forecaster can generate a large set of samples that can be systematically biased, which is why we observe that\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nand\nDQL\n+\nLag\n-\ns\n~\n\\tilde{s}\nalso have high maximum prediction errors in\nTable\nËœ\n8\n. While for this specific setting in\nFig.\nËœ\n16\n,\nForl\n(\nmax\n) and\nForl\n-\nDm\nare close to\nForl\n(DCM), although worse, we observe that across the test episodes,\nForl\n(DCM) outperforms\nForl\n(\nmax\n) and\nForl\n-\nDm\nin terms of maximum and mean error, demonstrating stability.\nTable 8\n:\nComparison of algorithm performance on error metrics.\nAlgorithm\nMinimum Error\nâ†“\n\\downarrow\nMaximum Error\nâ†“\n\\downarrow\nMean Error\nâ†“\n\\downarrow\nForl\n-DCM\n0.02\n2.40\n0.87\nÂ±\n\\pm\n0.60\nForl\n-\nmax\n0.01\n9.33\n2.05\nÂ±\n\\pm\n1.74\nDQL\n2.26\n11.30\n5.51\nÂ±\n\\pm\n2.13\nForl\n-\nDm\n(no past offsets)\n0.01\n9.94\n3.68\nÂ±\n\\pm\n2.25\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n0.04\n6.28\n1.76\nÂ±\n\\pm\n1.13\nDQL\n+\nLag\n-\ns\n~\n\\tilde{s}\n0.05\n6.56\n1.87\nÂ±\n\\pm\n1.19\nDmbp\n+\nLag\n0.03\n6.28\n1.69\nÂ±\n\\pm\n1.11\nTable 9\n:\nNormalized scores (mean Â± std.) for DCM candidate selection and the baselines.\nBold are the best values, and those not significantly different (\np\n>\n0.05\np>0.05\n, Welchâ€™s t-test).\nmaze2d-medium\nDM-FS\n-\ns\nÂ¯\n\\bar{s}\nDM-FS\n-\ns\n~\n\\tilde{s}\nForl\n(\nmax\n)\nForl(KDE)\nForl(DCM)\nreal-data-A\n40.6\nÂ±\n\\pm\n9.6\n73.7\nÂ±\n\\pm\n8.4\n41.2\nÂ±\n\\pm\n8.2\n49.7\nÂ±\n\\pm\n5.8\n63.3\nÂ±\n\\pm\n6.7\nreal-data-B\n59.8\nÂ±\n\\pm\n16.9\n59.3\nÂ±\n\\pm\n20.0\n58.9\nÂ±\n\\pm\n14.1\n96.2\nÂ±\n\\pm\n14.0\n66.5\nÂ±\n\\pm\n18.2\nreal-data-C\n64.9\nÂ±\n\\pm\n17.0\n67.8\nÂ±\n\\pm\n17.7\n66.1\nÂ±\n\\pm\n16.4\n94.9\nÂ±\n\\pm\n13.8\n86.3\nÂ±\n\\pm\n15.7\nreal-data-D\n45.1\nÂ±\n\\pm\n21.6\n45.0\nÂ±\n\\pm\n19.5\n44.4\nÂ±\n\\pm\n21.6\n86.9\nÂ±\n\\pm\n14.5\n103.4\nÂ±\n\\pm\n11.9\nreal-data-E\n12.5\nÂ±\n\\pm\n6.3\n20.8\nÂ±\n\\pm\n7.2\n11.8\nÂ±\n\\pm\n5.5\n49.3\nÂ±\n\\pm\n16.5\n51.2\nÂ±\n\\pm\n13.7\nAverage\n44.6\nÂ±\n\\pm\n53.3\nÂ±\n\\pm\n44.5\nÂ±\n\\pm\n75.4\nÂ±\n\\pm\n74.1\nÂ±\n\\pm\nmaze2d-large\nreal-data-A\n11.9\nÂ±\n\\pm\n5.5\n20.8\nÂ±\n\\pm\n6.2\n11.1\nÂ±\n\\pm\n2.3\n6.6\nÂ±\n\\pm\n2.2\n42.9\nÂ±\n\\pm\n4.1\nreal-data-B\n27.9\nÂ±\n\\pm\n14.7\n25.1\nÂ±\n\\pm\n11.7\n28.3\nÂ±\n\\pm\n7.1\n20.9\nÂ±\n\\pm\n7.2\n34.9\nÂ±\n\\pm\n9.2\nreal-data-C\n34.6\nÂ±\n\\pm\n6.8\n32.4\nÂ±\n\\pm\n5.8\n34.6\nÂ±\n\\pm\n13.6\n36.0\nÂ±\n\\pm\n7.3\n45.6\nÂ±\n\\pm\n4.1\nreal-data-D\n16.4\nÂ±\n\\pm\n12.0\n15.5\nÂ±\n\\pm\n9.0\n18.4\nÂ±\n\\pm\n9.9\n11.8\nÂ±\n\\pm\n3.2\n58.4\nÂ±\n\\pm\n6.5\nreal-data-E\n8.4\nÂ±\n\\pm\n5.0\n7.9\nÂ±\n\\pm\n3.9\n9.2\nÂ±\n\\pm\n6.1\n5.7\nÂ±\n\\pm\n5.1\n12.0\nÂ±\n\\pm\n9.9\nAverage\n19.8\nÂ±\n\\pm\n20.3\nÂ±\n\\pm\n20.3\nÂ±\n\\pm\n16.2\nÂ±\n\\pm\n38.8\nÂ±\n\\pm\nantmaze-umaze-diverse\nreal-data-A\n56.0\nÂ±\n\\pm\n13.3\n58.7\nÂ±\n\\pm\n7.4\n59.7\nÂ±\n\\pm\n9.7\n80.3\nÂ±\n\\pm\n4.1\n65.3\nÂ±\n\\pm\n8.7\nreal-data-B\n69.2\nÂ±\n\\pm\n11.3\n76.7\nÂ±\n\\pm\n11.3\n65.0\nÂ±\n\\pm\n10.5\n82.5\nÂ±\n\\pm\n8.0\n74.2\nÂ±\n\\pm\n10.8\nreal-data-C\n72.1\nÂ±\n\\pm\n5.4\n75.8\nÂ±\n\\pm\n4.8\n76.2\nÂ±\n\\pm\n5.4\n67.9\nÂ±\n\\pm\n7.9\n78.8\nÂ±\n\\pm\n8.5\nreal-data-D\n65.0\nÂ±\n\\pm\n11.3\n61.7\nÂ±\n\\pm\n15.1\n63.3\nÂ±\n\\pm\n6.8\n85.0\nÂ±\n\\pm\n4.8\n75.8\nÂ±\n\\pm\n8.0\nreal-data-E\n76.7\nÂ±\n\\pm\n16.2\n74.7\nÂ±\n\\pm\n18.3\n72.0\nÂ±\n\\pm\n15.0\n75.3\nÂ±\n\\pm\n8.0\n81.3\nÂ±\n\\pm\n6.9\nAverage\n67.8\nÂ±\n\\pm\n69.5\nÂ±\n\\pm\n67.2\nÂ±\n\\pm\n78.2\nÂ±\n\\pm\n75.1\nÂ±\n\\pm\nantmaze-medium-diverse\nreal-data-A\n39.0\nÂ±\n\\pm\n9.5\n27.3\nÂ±\n\\pm\n7.5\n36.0\nÂ±\n\\pm\n4.8\n62.7\nÂ±\n\\pm\n6.3\n44.0\nÂ±\n\\pm\n7.9\nreal-data-B\n34.2\nÂ±\n\\pm\n9.0\n35.8\nÂ±\n\\pm\n8.1\n36.7\nÂ±\n\\pm\n7.5\n59.2\nÂ±\n\\pm\n8.0\n55.8\nÂ±\n\\pm\n7.0\nreal-data-C\n36.2\nÂ±\n\\pm\n2.8\n34.6\nÂ±\n\\pm\n2.4\n37.1\nÂ±\n\\pm\n3.7\n49.2\nÂ±\n\\pm\n10.1\n52.9\nÂ±\n\\pm\n9.5\nreal-data-D\n25.0\nÂ±\n\\pm\n6.6\n17.5\nÂ±\n\\pm\n3.5\n37.5\nÂ±\n\\pm\n6.6\n77.5\nÂ±\n\\pm\n8.6\n64.2\nÂ±\n\\pm\n8.6\nreal-data-E\n28.7\nÂ±\n\\pm\n3.0\n25.3\nÂ±\n\\pm\n5.1\n26.7\nÂ±\n\\pm\n7.1\n36.7\nÂ±\n\\pm\n5.8\n26.7\nÂ±\n\\pm\n4.7\nAverage\n32.6\nÂ±\n\\pm\n28.1\nÂ±\n\\pm\n34.8\nÂ±\n\\pm\n57.1\nÂ±\n\\pm\n48.7\nÂ±\n\\pm\nantmaze-large-diverse\nreal-data-A\n25.0\nÂ±\n\\pm\n7.9\n21.0\nÂ±\n\\pm\n3.5\n21.7\nÂ±\n\\pm\n7.9\n21.7\nÂ±\n\\pm\n8.3\n34.3\nÂ±\n\\pm\n5.7\nreal-data-B\n25.0\nÂ±\n\\pm\n5.1\n30.0\nÂ±\n\\pm\n7.5\n20.0\nÂ±\n\\pm\n6.8\n47.5\nÂ±\n\\pm\n15.2\n46.7\nÂ±\n\\pm\n11.9\nreal-data-C\n25.8\nÂ±\n\\pm\n4.6\n21.7\nÂ±\n\\pm\n2.4\n23.8\nÂ±\n\\pm\n6.4\n40.0\nÂ±\n\\pm\n7.6\n33.8\nÂ±\n\\pm\n6.8\nreal-data-D\n14.2\nÂ±\n\\pm\n7.0\n15.8\nÂ±\n\\pm\n5.4\n21.7\nÂ±\n\\pm\n7.5\n35.0\nÂ±\n\\pm\n14.3\n46.7\nÂ±\n\\pm\n12.6\nreal-data-E\n21.3\nÂ±\n\\pm\n8.4\n22.0\nÂ±\n\\pm\n7.7\n20.7\nÂ±\n\\pm\n4.9\n8.0\nÂ±\n\\pm\n3.0\n11.3\nÂ±\n\\pm\n7.3\nAverage\n22.3\nÂ±\n\\pm\n22.1\nÂ±\n\\pm\n21.6\nÂ±\n\\pm\n30.4\nÂ±\n\\pm\n34.6\nÂ±\n\\pm\nTable 10\n:\nIntra-episode non-stationarity results with\nf\n=\n50\nf=50\n.\nWe compare methods with access to past offsets (\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nvs.\nForl\n) and without (\nDQL\nvs.\nForl\n-\nDm\n).\nmaze2d-medium\nDQL\nForl\n-\nDm\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nForl\nreal-data-A\n31.6\nÂ±\n\\pm\n1.6\n55.7\nÂ±\n\\pm\n8.7\n29.7\nÂ±\n\\pm\n9.2\n17.8\nÂ±\n\\pm\n4.4\nreal-data-B\n-4.7\nÂ±\n\\pm\n0.2\n42.2\nÂ±\n\\pm\n9.3\n63.0\nÂ±\n\\pm\n9.2\n100.1\nÂ±\n\\pm\n8.5\nreal-data-C\n-4.7\nÂ±\n\\pm\n0.2\n39.2\nÂ±\n\\pm\n7.2\n88.4\nÂ±\n\\pm\n9.4\n81.0\nÂ±\n\\pm\n8.3\nreal-data-D\n25.9\nÂ±\n\\pm\n5.3\n37.0\nÂ±\n\\pm\n8.4\n43.1\nÂ±\n\\pm\n8.1\n101.0\nÂ±\n\\pm\n7.1\nreal-data-E\n-4.7\nÂ±\n\\pm\n0.2\n57.0\nÂ±\n\\pm\n10.2\n5.8\nÂ±\n\\pm\n2.2\n66.7\nÂ±\n\\pm\n8.3\nAverage\n8.7\nÂ±\n\\pm\n46.2\nÂ±\n\\pm\n46.0\nÂ±\n\\pm\n73.3\nÂ±\n\\pm\nantmaze-umaze-diverse\nreal-data-A\n51.0\nÂ±\n\\pm\n7.4\n47.2\nÂ±\n\\pm\n10.6\n70.6\nÂ±\n\\pm\n14.5\n78.2\nÂ±\n\\pm\n8.0\nreal-data-B\n63.8\nÂ±\n\\pm\n10.2\n51.2\nÂ±\n\\pm\n10.3\n26.0\nÂ±\n\\pm\n16.8\n61.4\nÂ±\n\\pm\n9.7\nreal-data-C\n61.0\nÂ±\n\\pm\n9.3\n53.0\nÂ±\n\\pm\n6.6\n91.2\nÂ±\n\\pm\n2.5\n89.0\nÂ±\n\\pm\n5.1\nreal-data-D\n17.6\nÂ±\n\\pm\n5.9\n53.8\nÂ±\n\\pm\n5.7\n78.0\nÂ±\n\\pm\n4.4\n90.0\nÂ±\n\\pm\n2.9\nreal-data-E\n0.2\nÂ±\n\\pm\n0.4\n51.0\nÂ±\n\\pm\n11.6\n80.6\nÂ±\n\\pm\n9.7\n85.6\nÂ±\n\\pm\n5.5\nAverage\n38.7\nÂ±\n\\pm\n51.2\nÂ±\n\\pm\n69.3\nÂ±\n\\pm\n80.8\nÂ±\n\\pm\nantmaze-medium-diverse\nreal-data-A\n42.8\nÂ±\n\\pm\n6.9\n7.2\nÂ±\n\\pm\n1.8\n50.4\nÂ±\n\\pm\n2.9\n54.8\nÂ±\n\\pm\n5.2\nreal-data-B\n5.0\nÂ±\n\\pm\n3.1\n37.4\nÂ±\n\\pm\n3.8\n41.2\nÂ±\n\\pm\n4.8\n47.2\nÂ±\n\\pm\n4.6\nreal-data-C\n12.0\nÂ±\n\\pm\n5.1\n26.8\nÂ±\n\\pm\n2.4\n71.2\nÂ±\n\\pm\n2.9\n61.4\nÂ±\n\\pm\n4.5\nreal-data-D\n24.0\nÂ±\n\\pm\n4.7\n30.4\nÂ±\n\\pm\n2.9\n61.6\nÂ±\n\\pm\n5.0\n70.2\nÂ±\n\\pm\n6.6\nreal-data-E\n2.2\nÂ±\n\\pm\n2.3\n21.0\nÂ±\n\\pm\n3.3\n33.0\nÂ±\n\\pm\n5.0\n37.0\nÂ±\n\\pm\n3.4\nAverage\n17.2\nÂ±\n\\pm\n24.6\nÂ±\n\\pm\n51.5\nÂ±\n\\pm\n54.1\nÂ±\n\\pm\nantmaze-large-diverse\nreal-data-A\n13.0\nÂ±\n\\pm\n5.5\n5.8\nÂ±\n\\pm\n2.6\n16.0\nÂ±\n\\pm\n3.8\n27.4\nÂ±\n\\pm\n3.6\nreal-data-B\n1.2\nÂ±\n\\pm\n0.8\n11.4\nÂ±\n\\pm\n4.9\n19.4\nÂ±\n\\pm\n4.6\n50.2\nÂ±\n\\pm\n9.7\nreal-data-C\n4.8\nÂ±\n\\pm\n2.9\n7.2\nÂ±\n\\pm\n0.8\n31.8\nÂ±\n\\pm\n4.4\n48.6\nÂ±\n\\pm\n4.3\nreal-data-D\n4.6\nÂ±\n\\pm\n1.3\n11.0\nÂ±\n\\pm\n2.5\n25.2\nÂ±\n\\pm\n2.9\n48.2\nÂ±\n\\pm\n6.2\nreal-data-E\n2.4\nÂ±\n\\pm\n1.5\n13.6\nÂ±\n\\pm\n4.8\n12.8\nÂ±\n\\pm\n3.4\n13.4\nÂ±\n\\pm\n2.2\nAverage\n5.2\nÂ±\n\\pm\n9.8\nÂ±\n\\pm\n21.0\nÂ±\n\\pm\n37.6\nÂ±\n\\pm\nH.1\nSensitivity to Diffusion-generated Sample Size\nmax\nuses candidate states predicted by the diffusion model in the\nForl\nframework and Lag-Llama but uses maximum likelihood instead of DCM. Given the multimodal nature of the candidate state distributions, we conduct a sensitivity analysis on the number of denoiser samples, a shared hyperparameter for both\nmax\nand\nForl\n. We report results averaged over 5 random seeds across 25 tasks (\nantmaze\nand\nmaze2d\nwith\nreal-data-A,B,C,D,E\n). The results in\nFig.\nËœ\n17\nindicate that our diffusion modelâ€™s performance remains consistent across varying sample sizes, demonstrating robustness to the number of candidate states generated. Notably, the\nDmbp\nalgorithm uses 50 denoiser samples.\nFigure 17\n:\nAverage normalized scores over 25 experiments with diffusion-generated sample sizes of 32 and 50, each conducted with 5 random seeds.\nThe results indicate that the diffusion modelâ€™s performance remains consistent across varying sample sizes, demonstrating robustness to the number of candidate states generated.\nAppendix I\nZero-shot Foundation Model and Time-Series Datasets\nWe extract the first two univariate series from five timeâ€series datasets:\nreal-data-A\n(\naustralian-electricity-demand\n)\n[\n73\n]\n,\nreal-data-B\n(\nelectricity\n)\n[\n74\n]\n,\nreal-data-C\n(\nelectricity-hourly\n)\n[\n73\n,\n74\n]\n,\nreal-data-D\n(\nelectricity-nips\n)\n[\n74\n,\n75\n]\nand\nreal-data-E\n(\nexchange-rate\n2\n2\n2\nhttps://github.com/laiguokun/multivariate-time-series-data\n)\n[\n76\n]\n,\n[\n73\n]\nall accessed via GluonTS\n[\n22\n,\n77\n]\n. Figure\n5\npresents the ground truth, forecast mean, and standard deviation from Lag-Llama\n[\n10\n]\nfor the first series of\nreal-data-A\nand\nreal-data-D\n; forecasts for the remaining series and domains are provided in Figure\n18\n.\nWe aim to capture a broad spectrum of scenarios for a comprehensive evaluation. For instance, the\nelectricity-hourly\ndataset consists of hourly electricity usage data from various consumers, while the\naustralian-electricity-demand\ndataset has 30-minute interval records of electricity demand across different Australian states. The\nexchange-rate\ndataset, on the other hand, includes daily exchange rates of multiple currencies, including those of Australia, the United Kingdom, Canada, Switzerland, China, Japan, New Zealand, and Singapore.\nTo effectively represent diverse offset patterns in multiple directions, we apply feature scaling to the time-series data using a normalization\nx\nc\nâ€²\n=\nx\nâˆ’\nx\nÂ¯\nmax\nâ€‹\n(\nx\n)\nâˆ’\nmin\nâ€‹\n(\nx\n)\nx_{c}^{\\prime}=\\frac{x-\\bar{x}}{\\text{max}(x)-\\text{min}(x)}\nwhere sample mean\nx\nÂ¯\n\\bar{x}\n,\nmin\nâ€‹\n(\nx\n)\n\\text{min}(x)\nand\nmax\nâ€‹\n(\nx\n)\n\\text{max}(x)\nare computed from the available data up to the context length. Furthermore, we scale these values using the minimum and maximum state values observed in the offline RL dataset\n[\n15\n]\nfor navigation and minimum and maximum state values of the initial state distribution at test time for manipulation\n[\n15\n]\n, ensuring that the experiments cover diverse observation spaces that can accurately represent a wide range of scenarios. We group our results in terms of time-series datasets in\nTable\nËœ\n13\n.\nFigure 18\n:\nZero-shot forecasting results with Lag-Llama\n[\n10\n]\nfor first 2 time-series in univariate time-series datasets:\nreal-data-A\n(Australian-electricity-demand),\nreal-data-E\n(Exchange Rate),\nreal-data-B\n(Electricity),\nreal-data-C\n(Electricity Hourly),\nreal-data-D\n(Electricity Nips).\nI.1\nSensitivity of\nForl\nto Forecasting Errors\nTo analyze the sensitivity of\nForl\nto forecasting errors, we compare its performance against\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n, which only uses the forecasterâ€™s predictions.\nTable\nËœ\n11\npresents the average prediction error (\nâ†“\n\\downarrow\n) across all datasets in\nantmaze\nand\nmaze2d\nenvironments with 5 seeds.\nTable 11\n:\nSensitivity analysis of\nForl\nto forecasting errors.\nWe compare the average prediction error (\nâ†“\n\\downarrow\n) of our method against the\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nbaseline, which uses only the forecasterâ€™s predictions. The analysis is presented across five time-series datasets. Error reduction percentages are calculated from full-precision values before rounding.\nDataset\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n(\nâ†“\n\\downarrow\n)\nForl\n(\nâ†“\n\\downarrow\n)\nError Reduction\n(\nâ†‘\n\\uparrow\n)\nreal-data-A\n4.56\n3.32\n27.0%\nreal-data-B\n3.66\n2.29\n37.4%\nreal-data-C\n3.0\n2.69\n10.2%\nreal-data-D\n4.29\n1.87\n56.5%\nreal-data-E\n5.45\n5.21\n4.3%\nIn all datasets,\nForl\noutperforms the\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nbaseline.\nForl\nachieves its greatest impact on moderately challenging forecasts (a 56.5% error reduction on\nreal-data-D\n). Its behavior at the extremes further demonstrates its robustness:\nâ€¢\nForl\nstill refines the best forecast by 10.2% (\nreal-data-C\n)\nâ€¢\nForl\nimproves the worst forecast by 4.3% (\nreal-data-E\n).\nI.2\nState Prediction Accuracy\nTo evaluate the stateâ€prediction accuracy of our\nForl\nframework, we compare it against\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n. For each method, we report the mean\nâ„“\n2\n\\ell_{2}\nerror between the true state\ns\nt\ns_{t}\nand the predicted state\ns\n~\nt\n\\tilde{s}_{t}\nobtained during evaluation for a diffusion-generated sample size of 32.\nIn the resulting average prediction error table in\nTable\nËœ\n12\n:\nâ€¢\nEach\nrow\ncorresponds to the state estimation algorithm used at test time to generate states\ns\n~\nt\n\\tilde{s}_{t}\n, which are then provided to the policy to select actions.\nâ€¢\nEach\ncolumn\ncorresponds to a method whose state estimates are evaluated on that same rollout.\nThe entry at row\ni\ni\n, column\nj\nj\nis the mean\nâ„“\n2\n\\ell_{2}\nerror when method\nm\ni\nm_{i}\nis used in the environment, but predictions are produced by method\nm\nj\nm_{j}\n. When\ni\n=\nj\ni=j\n, this entry measures the selfâ€prediction error of each method; when\ni\nâ‰ \nj\ni\\neq j\n, it measures the error under an alternate method.\nAcross all method pairs,\nForl\nachieves lower mean\nâ„“\n2\n\\ell_{2}\nerrors, even in offâ€diagonal evaluations, demonstrating its superior stateâ€prediction performance compared to\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n. These findings are consistent with the normalized environment scores in\nTable\nËœ\n1\n.\nTable 12\n:\nComparison of prediction errors (\nâ†“\n\\downarrow\n).\nWe present state prediction accuracy for the proposed\nForl\nframework with the baselines across 5 random seeds.\nmaze2d-medium\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nForl\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n1.68\nÂ±\n0.46\n1.68\\pm 0.46\n1.35\nÂ±\n0.48\n\\bf 1.35\\pm 0.48\nForl\n1.39\nÂ±\n0.49\n1.39\\pm 0.49\n1.25\nÂ±\n0.43\n\\bf 1.25\\pm 0.43\nmaze2d-large\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nForl\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n2.37\nÂ±\n0.5\n2.37\\pm 0.5\n1.63\nÂ±\n0.57\n\\bf 1.63\\pm 0.57\nForl\n1.91\nÂ±\n0.44\n1.91\\pm 0.44\n1.39\nÂ±\n0.62\n\\bf 1.39\\pm 0.62\nantmaze-large-diverse\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nForl\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n8.07\nÂ±\n1.83\n8.07\\pm 1.83\n5.33\nÂ±\n2.17\n\\bf 5.33\\pm 2.17\nForl\n7.16\nÂ±\n1.71\n7.16\\pm 1.71\n5.89\nÂ±\n2.96\n\\bf 5.89\\pm 2.96\nantmaze-medium-diverse\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nForl\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n5.33\nÂ±\n1.15\n5.33\\pm 1.15\n4.75\nÂ±\n1.98\n\\bf 4.75\\pm 1.98\nForl\n4.94\nÂ±\n1.17\n4.94\\pm 1.17\n4.74\nÂ±\n1.63\n\\bf 4.74\\pm 1.63\nantmaze-umaze-diverse\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nForl\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\n3.51\nÂ±\n0.53\n3.51\\pm 0.53\n2.04\nÂ±\n0.47\n\\bf 2.04\\pm 0.47\nForl\n3.27\nÂ±\n0.61\n3.27\\pm 0.61\n2.31\nÂ±\n0.51\n\\bf 2.31\\pm 0.51\nTable 13\n:\nNormalized scores (mean Â± std.) for\nForl\nframework and the baselines grouped by time-series.\nBold are the best values, and those not significantly different (\np\n>\n0.05\np>0.05\n, Welchâ€™s t-test).\nreal-data-A\nDQL\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nDmbp\n+\nLag\nForl\n(ours)\nmaze2d-medium\n30.2\nÂ±\n\\pm\n6.5\n30.2\nÂ±\n\\pm\n8.6\n25.1\nÂ±\n\\pm\n9.8\n63.3\nÂ±\n\\pm\n6.7\nmaze2d-large\n16.2\nÂ±\n\\pm\n5.5\n2.4\nÂ±\n\\pm\n1.1\n4.2\nÂ±\n\\pm\n5.8\n42.9\nÂ±\n\\pm\n4.1\nantmaze-umaze-diverse\n22.7\nÂ±\n\\pm\n3.0\n41.0\nÂ±\n\\pm\n5.2\n45.7\nÂ±\n\\pm\n4.8\n65.3\nÂ±\n\\pm\n8.7\nantmaze-medium-diverse\n31.0\nÂ±\n\\pm\n6.5\n40.0\nÂ±\n\\pm\n5.7\n39.7\nÂ±\n\\pm\n4.0\n44.0\nÂ±\n\\pm\n7.9\nantmaze-large-diverse\n11.0\nÂ±\n\\pm\n1.9\n11.3\nÂ±\n\\pm\n4.9\n9.0\nÂ±\n\\pm\n4.5\n34.3\nÂ±\n\\pm\n5.7\nkitchen-complete\n16.6\nÂ±\n\\pm\n1.4\n7.2\nÂ±\n\\pm\n1.9\n8.7\nÂ±\n\\pm\n1.3\n12.0\nÂ±\n\\pm\n3.9\nreal-data-B\nmaze2d-medium\n14.1\nÂ±\n\\pm\n12.1\n53.4\nÂ±\n\\pm\n14.6\n41.2\nÂ±\n\\pm\n21.1\n66.5\nÂ±\n\\pm\n18.2\nmaze2d-large\n-0.5\nÂ±\n\\pm\n2.9\n5.5\nÂ±\n\\pm\n9.0\n15.0\nÂ±\n\\pm\n14.6\n34.9\nÂ±\n\\pm\n9.2\nantmaze-umaze-diverse\n24.2\nÂ±\n\\pm\n3.5\n48.3\nÂ±\n\\pm\n7.0\n62.5\nÂ±\n\\pm\n13.2\n74.2\nÂ±\n\\pm\n10.8\nantmaze-medium-diverse\n23.3\nÂ±\n\\pm\n4.8\n48.3\nÂ±\n\\pm\n4.8\n43.3\nÂ±\n\\pm\n16.0\n55.8\nÂ±\n\\pm\n7.0\nantmaze-large-diverse\n5.8\nÂ±\n\\pm\n4.8\n9.2\nÂ±\n\\pm\n4.6\n8.3\nÂ±\n\\pm\n2.9\n46.7\nÂ±\n\\pm\n11.9\nkitchen-complete\n12.9\nÂ±\n\\pm\n4.1\n32.7\nÂ±\n\\pm\n6.5\n20.0\nÂ±\n\\pm\n3.1\n33.1\nÂ±\n\\pm\n5.6\nreal-data-C\nmaze2d-medium\n-2.3\nÂ±\n\\pm\n3.3\n56.7\nÂ±\n\\pm\n18.5\n56.9\nÂ±\n\\pm\n18.4\n86.3\nÂ±\n\\pm\n15.7\nmaze2d-large\n0.9\nÂ±\n\\pm\n1.7\n16.6\nÂ±\n\\pm\n7.5\n26.8\nÂ±\n\\pm\n8.4\n45.6\nÂ±\n\\pm\n4.1\nantmaze-umaze-diverse\n21.7\nÂ±\n\\pm\n3.5\n50.4\nÂ±\n\\pm\n8.3\n60.4\nÂ±\n\\pm\n3.9\n78.8\nÂ±\n\\pm\n8.5\nantmaze-medium-diverse\n10.0\nÂ±\n\\pm\n2.3\n48.3\nÂ±\n\\pm\n3.4\n49.6\nÂ±\n\\pm\n3.7\n52.9\nÂ±\n\\pm\n9.5\nantmaze-large-diverse\n5.4\nÂ±\n\\pm\n2.4\n22.1\nÂ±\n\\pm\n5.6\n17.9\nÂ±\n\\pm\n3.8\n33.8\nÂ±\n\\pm\n6.8\nkitchen-complete\n13.4\nÂ±\n\\pm\n1.7\n23.9\nÂ±\n\\pm\n6.6\n20.5\nÂ±\n\\pm\n3.3\n23.9\nÂ±\n\\pm\n6.0\nreal-data-D\nmaze2d-medium\n4.7\nÂ±\n\\pm\n5.0\n36.9\nÂ±\n\\pm\n16.3\n38.5\nÂ±\n\\pm\n14.2\n103.4\nÂ±\n\\pm\n11.9\nmaze2d-large\n3.0\nÂ±\n\\pm\n6.6\n8.6\nÂ±\n\\pm\n3.2\n13.4\nÂ±\n\\pm\n4.1\n58.4\nÂ±\n\\pm\n6.5\nantmaze-umaze-diverse\n5.8\nÂ±\n\\pm\n2.3\n26.7\nÂ±\n\\pm\n6.3\n29.2\nÂ±\n\\pm\n5.9\n75.8\nÂ±\n\\pm\n8.0\nantmaze-medium-diverse\n11.7\nÂ±\n\\pm\n5.4\n46.7\nÂ±\n\\pm\n7.5\n41.7\nÂ±\n\\pm\n6.6\n64.2\nÂ±\n\\pm\n8.6\nantmaze-large-diverse\n2.5\nÂ±\n\\pm\n2.3\n14.2\nÂ±\n\\pm\n3.7\n14.2\nÂ±\n\\pm\n6.3\n46.7\nÂ±\n\\pm\n12.6\nkitchen-complete\n7.5\nÂ±\n\\pm\n2.5\n24.0\nÂ±\n\\pm\n9.2\n28.1\nÂ±\n\\pm\n8.1\n27.1\nÂ±\n\\pm\n10.1\nreal-data-E\nmaze2d-medium\n3.5\nÂ±\n\\pm\n8.8\n8.7\nÂ±\n\\pm\n6.0\n11.4\nÂ±\n\\pm\n2.8\n51.2\nÂ±\n\\pm\n13.7\nmaze2d-large\n-2.1\nÂ±\n\\pm\n0.4\n2.6\nÂ±\n\\pm\n3.4\n0.9\nÂ±\n\\pm\n3.7\n12.0\nÂ±\n\\pm\n9.9\nantmaze-umaze-diverse\n6.0\nÂ±\n\\pm\n6.8\n58.0\nÂ±\n\\pm\n16.6\n59.3\nÂ±\n\\pm\n7.6\n81.3\nÂ±\n\\pm\n6.9\nantmaze-medium-diverse\n18.7\nÂ±\n\\pm\n4.5\n27.3\nÂ±\n\\pm\n8.6\n26.0\nÂ±\n\\pm\n5.5\n26.7\nÂ±\n\\pm\n4.7\nantmaze-large-diverse\n5.3\nÂ±\n\\pm\n3.8\n3.3\nÂ±\n\\pm\n2.4\n3.3\nÂ±\n\\pm\n0.0\n11.3\nÂ±\n\\pm\n7.3\nkitchen-complete\n18.5\nÂ±\n\\pm\n6.0\n2.8\nÂ±\n\\pm\n2.1\n6.2\nÂ±\n\\pm\n1.7\n10.3\nÂ±\n\\pm\n3.0\nAppendix J\nPreliminary Results for Affine Transformation with Uniform Scaling and Bias\nWe use the fourth series in each time-series domain to perform isotropic scaling for the dimensions affected by non-stationarity using a scaling factor of\nÎ²\n=\n0.5\n\\beta=0.5\n, with bias coming from the first two series, respectively. We apply feature scaling to time-series data with\nx\nc\nâ€²\n=\n1\nâˆ’\nÎ²\n+\nÎ²\nâ‹…\nexp\nâ¡\n(\nx\nâˆ’\nx\nÂ¯\n2\nâ‹…\n(\nmax\nâ€‹\n(\nx\n)\nâˆ’\nmin\nâ€‹\n(\nx\n)\n)\n)\nx_{c}^{\\prime}=1-\\beta+\\beta\\cdot\\exp\\left(\\frac{x-\\bar{x}}{2\\cdot(\\text{max}(x)-\\text{min}(x))}\\right)\n. The offset scaling for the bias uses\nÎ±\n=\n1\n\\alpha=1\n, which is the standard value in our experiments. As in the other ablations with scaling offsets, we use the\nDQL\npolicy.\nTable\nËœ\n14\nshows that\nForl\noutperforms the baselines under this transformation. A large-scale analysis of more general transformations is left for future work.\nTable 14\n:\nPerformance under affine observation shifts.\nNormalized scores in\nmaze2d-large\nwith time-varying uniform scaling and bias.\nmaze2d-large\nDQL\nDQL\n+\nLag\n-\ns\nÂ¯\n\\bar{s}\nForl\n(ours)\nreal-data-A\n5.9\n6.1\n39.7\nreal-data-B\n2.4\n1.6\n13.8\nreal-data-C\n2.2\n22.2\n32.9\nreal-data-D\n0.7\n10.7\n56.1\nreal-data-E\n-2.0\n-2.3\n27.5\nAverage\n1.8\n7.7\n34.0\nAppendix K\nOffline Reinforcement Learning Environments\nK.1\nD4RL\nWe use the standard D4RL\n[\n15\n]\noffline RL environments\n[\n15\n]\nwith no modifications during training, namely\nantmaze-medium-diverse\n,\nmaze2d-medium\n,\nantmaze-large-diverse\n,\nmaze2d-large\n, and\nantmaze-umaze-diverse\n, where initial states are randomized both in the evaluation environment and in the offline dataset.\nFig.\nËœ\n19\nillustrates the environments used from the D4RL benchmark,\nkitchen-complete\n,\nantmaze-large-diverse\n,\nantmaze-medium-diverse\n,\nantmaze-umaze-diverse\n. The\nmaze2d-large\nand\nmaze2d-medium\nenvironments share the same maze configurations as\nantmaze-large-diverse\nand\nantmaze-medium-diverse\n, respectively.\nFor manipulation tasks, we train on the standard\nkitchen-complete\nenvironment. We sample the base joint angles from\nU\nâ€‹\n(\n[\nâˆ’\n1.5\n,\n0.17\n]\n)\nU([-1.5,0.17])\nand the shoulderâ€joint angles from\nU\nâ€‹\n(\n[\nâˆ’\n1.78\n,\nâˆ’\n1.16\n]\n)\nU([-1.78,-1.16])\nwhich are set based on the minimum and maximum state space dimension intervals in the offline RL dataset to evaluate partial identifiability at test-time. The offsets affect the state dimensions associated with the base and shoulder joint angles.\nFigure 19\n:\nantmaze-large\n,\nantmaze-medium\n,\nantmaze-umaze\n(-v1) and\nkitchen-complete\nenvironments in D4RL benchmark\n[\n15\n]\n.\nK.2\nOGBench\nOGBench benchmark\n[\n21\n]\ncontains both standard and goal-conditioned offline reinforcement learning tasks. To induce non-stationarity at test time, we follow the procedure from our D4RL experiments and use time-series data from GluonTS\n[\n77\n,\n22\n]\n. For all tasks in\nFig.\nËœ\n21\n, we use the default\nsingletask-v0\nvariant. We report results using the\nFQL\nalgorithm\n[\n23\n]\nwith its officially recommended hyperparameters. For the\nantmaze-large-navigate\nenvironment, we use the first two time series from the GluonTS\nreal-data-A,B,C,D,E\ndatasets and apply an offset scaling factor of\nÎ±\n=\n0.5\n\\alpha=0.5\n. For\ncube-single-play\n, we apply offsets to the first 17 observation dimensions, which include all joint positions, joint velocities, and end effector variables (position and yaw), using the first 17 time series from each of the GluonTS\nreal-data-A,B,C,D,E\ndatasets. Because the\nreal-data-A\ndataset only has five time series, we cycle through them repeatedly until all 17 dimensions are covered. Across all\ncube-single-play\nexperiments, we use an offset scaling factor of\nÎ±\n=\n0.25\n\\alpha=0.25\n.\nFigure 20\n:\nAverage normalized scores of\nForl\n(ours) and baselines for OGBench\nFigure 21\n:\ncube-single-play\nand\nantmaze-large-navigate\nenvironments in OGBench benchmark\n[\n21\n]\n.\nAppendix L\nImplementation Details\nDuring training, we use the original offline RL dataset\nwithout offsets\n. At test-time, the offsets affect the first two state dimensions, where each offset sequence is drawn from the first two univariate time-series from diverse datasets. The agentâ€™s policy receives only the offsetâ€corrupted observations, with no direct access to the true underlying states throughout\nP\nP\nepisodes. The time-series forecasting model, given the past\nC\nC\nground-truth offsets\n(\nb\nj\nâˆ’\nC\n,\nâ‹¯\n,\nb\nj\nâˆ’\n1\n)\n(b^{j-C},\\cdots,b^{j-1})\n, predicts the future offsets\n(\nb\nj\n,\nâ‹¯\n,\nb\nj\n+\nP\n)\n(b^{j},\\cdots,b^{j+P})\nduring testing.\nForl\nleverages these predictions and in-episode experience to dynamically adapt to unknown external perturbations. All experiments use 5 random seeds, except for (i) the preliminary affine-transformation results (\nAppendix\nËœ\nJ\n) and (ii) the focused error analysis across all evaluation episodes for the task shown in\nFig.\nËœ\n16\nwith results reported in\nTable\nËœ\n8\n.\nWe select the hyperparameters in Table\nLABEL:tab:combined_hyperparameters\nbased on the validation loss of the\nForl\ndiffusion model in D4RL and OGbench standard offline RL datasets. The validation loss is computed using the\nDm\nloss function in Eq.\n4\n. Hyperparameter optimization was conducted using a grid search, with the following ranges for\nmaze2d\nand\nantmaze\n: diffusion timesteps\nN\n=\n{\n10\n,\n20\n}\nN=\\{10,20\\}\n, number of hidden layers following the Temporal Unet model\n#\nâ€‹\nl\nâ€‹\na\nâ€‹\ny\nâ€‹\ne\nâ€‹\nr\nâ€‹\ns\n=\n{\n1\n,\n2\n,\n3\n}\n\\#layers=\\{1,2,3\\}\n, window size\nw\n=\n{\n128\n,\n256\n}\nw=\\{128,256\\}\n, and learning rate\nl\nâ€‹\nr\n=\n{\n0.0004\n,\n0.0006\n,\n0.0009\n}\nlr=\\{0.0004,0.0006,0.0009\\}\n. For the\nkitchen-complete\nand\ncube-single-play\nembedding dimension\n=\n{\n64\n,\n128\n}\n\\textit{embedding dimension}=\\{64,128\\}\n, learning rates\nl\nâ€‹\nr\n=\n{\n0.0004\n,\n0.0009\n}\nlr=\\{0.0004,0.0009\\}\nand\nw\n=\n{\n32\n,\n64\n}\nw=\\{32,64\\}\nare used for the grid search.\nThe architecture of our\nForl\nModel is a noise prediction conditional TemporalUnet diffusion model\n[\n78\n,\n39\n,\n3\n]\n. Different from the architecture used in\n[\n3\n]\n, we concatenate each element in\nğ‰\n(\nt\n,\nw\n)\n\\boldsymbol{\\tau}_{(t,w)}\nwith\nğ’”\nt\n(\nn\n)\n\\boldsymbol{s}_{t}^{(n)}\nand feed it to our model without additional encoders, using the diffusion timestep embedding in the Residual Temporal Blocks. For the TemporalUnet architecture, we concatenate the Unet model output with the time-embedding before feeding it to fully connected layers, particularly in the antmaze environments due to the large input size. The set of hyperparameters is provided in Table\nLABEL:tab:combined_hyperparameters\n. Although the\nForl\nconditional diffusion model is specifically utilized for time-dependent offsets in the first two dimensions of the state vector, it is trained for general-purpose state prediction, enabling it to predict all dimensions of the state in\nmaze2d\n,\nantmaze\n, and OGBench environments. This approach is taken because we do not assume prior knowledge of the evaluation environment.\nThe method for setting seeds involves a function that initializes the seed across all relevant libraries (PyTorch, CUDA, NumPy, Gym Environment, and Pythonâ€™s random module) to ensure the replicability of results. We use the open source implementation of\nDmbp\n3\n3\n3\nhttps://github.com/zhyang2226/DMBP/tree/main\nwith the suggested hyperparameters\n[\n56\n]\n, and the pretrained Lag-Llama\n4\n4\n4\nhttps://github.com/time-series-foundation-models/Lag-Llama\nmodel\n[\n10\n]\n.\nAppendix M\nExperiments compute resources\nExperiments were primarily conducted on an HPC cluster with NVIDIA A100 GPUs (40GB HBM2, PCIe 4.0/NVLink interconnect) and AMD EPYC 7302 CPUs (32 cores, 1TB RAM, 3TB local SSD), as well as on a workstation with an NVIDIA GeForce RTX 4090 (24GB GDDR6X), 128GB RAM, and a 2TB PCIe 4.0 NVMe SSD. A small portion of the experiments also ran on a cluster equipped with 4x NVIDIA V100 GPUs (16GB NVLink), 2x Intel Xeon Gold 6248R CPUs, and 384GB RAM. The total compute for published results is approximately 7,300 GPU-hours; additional failed and preliminary runs total approximately 1,500 GPU-hours.\nTable 15\n:\nHyperparameters for\nDQL\n[\n14\n,\n79\n,\n56\n]\nacross\nkitchen-complete\n,\nmaze2d\n, and\nantmaze\nenvironments.\nHyperparameters\nMaximum Timesteps\n1â€‰000â€‰000\nÎ³\n\\gamma\n0.99\nÏ„\n\\tau\n0.005\nLearning rate decay\ntrue\nT\n10\nÎ²\n\\beta\nSchedule\nvp\nLearning rate\n3\nÃ—\n10\nâˆ’\n4\n3\\times 10^{-4}\nÎ±\n\\alpha\n0.2\nBatch Size\n256\nHidden Size\n256\nReward tune\nno\nNormalize\nfalse\nOptimizer\nAdam\n[\n80\n]\nkitchen-complete\nmaze2d\nantmaze\ngn\n10.0\numazeâ€diverse: 3.0\nmediumâ€diverse: 1.0\nlargeâ€diverse: 7.0\numazeâ€diverse: 3.0\nmediumâ€diverse: 1.0\nlargeâ€diverse: 7.0\nÎ·\n\\eta\n0.005\numazeâ€diverse: 2.0\nmediumâ€diverse: 3.0\nlargeâ€diverse: 3.5\numazeâ€diverse: 2.0\nmediumâ€diverse: 3.0\nlargeâ€diverse: 3.5\nMaxQ Backup\nfalse\ntrue\ntrue\nTable 16\n:\nHyperparameters for Implicit Q-Learning (\nIql\n)\n[\n26\n,\n81\n,\n63\n,\n62\n]\nacross\nmaze2d\n, and\nantmaze\nenvironments.\nHyperparameters\nValue\nBatch Size\n256\nDiscount (\nÎ³\n\\gamma\n)\n0.99\nTarget Network Update (\nÏ„\n\\tau\n)\n0.005\nmaze2d\nÎ²\n=\n3.0\n\\beta=3.0\nÏ„\nIQL\n=\n0.7\n\\tau_{\\text{IQL}}=0.7\nNormalize Rewards = false\nantmaze\nÎ²\n=\n10.0\n\\beta=10.0\nÏ„\nIQL\n=\n0.9\n\\tau_{\\text{IQL}}=0.9\nNormalize Rewards = true\nTable 17\n:\nHyperparameters for Flow Q-Learning (\nFQL\n)\n[\n23\n,\n82\n]\nfor\ncube-single-play\nand\nantmaze-large-navigate\n.\nHyperparameters\nValue\nBatch Size\n256\nLearning Rate\n0.0003\n0.0003\nDiscount factor (\nÎ³\n\\gamma\n)\n0.99\nTarget network smoothing coefficient (\nÏ„\n\\tau\n)\n0.005\nBC Coefficient (\nÎ±\n\\alpha\n)\n10.0\nFlow Steps\n10\nActor Hidden Dimensions\n(512, 512, 512, 512)\nValue Hidden Dimensions\n(512, 512, 512, 512)\nantmaze-large-navigate\nBC Coefficient (\nÎ±\n\\alpha\n) = 10.0\ncube-single-play\nBC Coefficient (\nÎ±\n\\alpha\n) = 300.0\nTable 18\n:\nHyperparameters for\nTd\n3\nBc\n[\n59\n,\n83\n,\n56\n]\nacross\nkitchen-complete\n,\nmaze2d\n, and\nantmaze\nenvironments.\nHyperparameters\nValue\nMaximum Timesteps\n1â€‰000â€‰000\nExploration noise\n0.1\nBatch Size\n256\nDiscount factor\n0.99\nÏ„\n\\tau\n0.005\nPolicy Noise\n0.2\nPolicy Noise Clipping\n0.5\nPolicy update frequency\n2\nÎ±\n\\alpha\n2.5\nNormalize\ntrue\nOptimizer\nAdam\n[\n80\n]\nTable 19\n:\nHyperparameters for\nRorl\n[\n25\n,\n84\n,\n56\n]\nin\nmaze2d\nenvironments.\nHyperparameters\nÎ³\n\\gamma\n0.99\nsoft\nÏ„\n\\textbf{soft}_{\\tau}\n0.005\nQ Learning Rate\n3\nÃ—\n10\nâˆ’\n4\n3\\times 10^{-4}\nPolicy Learning Rate\n3\nÃ—\n10\nâˆ’\n4\n3\\times 10^{-4}\nÎ±\n\\alpha\n1.0\nAuto-tune entropy\ntrue\nMaxQ Backup\nfalse\nDeterministic Backup\nfalse\nÎ·\n\\eta\nâˆ’\n1\n-1\nBatch Size\n256\nHidden Size\n256\nTarget Update Interval\n1\nÏ„\n\\tau\n0.2\nNormalize\nfalse\nn sample\n20\nÎ²\nQ\n\\beta_{Q}\n0.0001\nÎ²\nP\n\\beta_{P}\n1.0\nÏµ\no\nâ€‹\no\nâ€‹\nd\n\\boldsymbol{\\epsilon}_{ood}\n0.01\nMaximum Timesteps\n3â€‰000â€‰000\nOptimizer\nAdam\n[\n80\n]\nmaze2d\nÎ²\nO\nâ€‹\nO\nâ€‹\nD\n\\beta_{OOD}\n0.5\nÏµ\nQ\n\\boldsymbol{\\epsilon}_{Q}\n0.01\nÏµ\nP\n\\boldsymbol{\\epsilon}_{P}\n0.03\nÎ»\nm\nâ€‹\na\nâ€‹\nx\n\\lambda_{max}\n1.0\nÎ»\nm\nâ€‹\ni\nâ€‹\nn\n\\lambda_{min}\n0.5\nÎ»\nd\nâ€‹\ne\nâ€‹\nc\nâ€‹\na\nâ€‹\ny\n\\lambda_{decay}\n10\nâˆ’\n6\n10^{-6}\nTable 20\n:\nHyperparameters for\nForl\nacross\nkitchen-complete\n,\nmaze2d\n,\nantmaze\n,\nantmaze-large-navigate\n,\ncube-single-play\nenvironments.\nHyperparameters\nBatch Size\n128\nHidden Size\n128\n# denoiser samples\n50\nOptimizer\nAdam\n[\n80\n]\nMaximum Timesteps\n300â€‰000\nkitchen-complete\nantmaze\nmaze2d\nantmaze-large-navigate\ncube-single-play\nEmbedding Dimension\n128\n64\n64\n64\n128\nw\nw\n32\n256\n128\n256\n64\nLearning rate\n4\nÃ—\n10\nâˆ’\n4\n4\\times 10^{-4}\n4\nÃ—\n10\nâˆ’\n4\n4\\times 10^{-4}\n9\nÃ—\n10\nâˆ’\n4\n9\\times 10^{-4}\n4\nÃ—\n10\nâˆ’\n4\n4\\times 10^{-4}\n9\nÃ—\n10\nâˆ’\n4\n9\\times 10^{-4}\nObservation Scale\n1\n1\n100\n1\n1\nTime Concatenation\ntrue\ntrue\nfalse\ntrue\ntrue\n# middle hidden layers\n1\n1\nlarge: 1\nmedium: 3\n1\n1\nN\n10\n10\nlarge: 10\nmedium: 20\n20\n20\nAppendix N\nLicenses for Existing Assets and Libraries\nN.1\nExisting Assets\nâ€¢\nThe\nD4RL\n[\n15\n]\n, including the FrankaÂ Kitchen\ntasks, are distributed under the Creative Commons\nAttributionÂ 4.0 (data) and ApacheÂ 2.0 (code) licenses as in\nhttps://github.com/Farama-Foundation/D4RL\n.\nâ€¢\nMuJoCo\n[\n85\n]\nis released under the ApacheÂ 2.0\nlicense as indicated in\nhttps://github.com/google-deepmind/mujoco/blob/main/LICENSE\n.\nâ€¢\nGymnasium\n(formerly OpenAIÂ Gym) is distributed under the MIT\nlicense as indicated in\nhttps://github.com/Farama-Foundation/Gymnasium/blob/main/LICENSE\n.\nâ€¢\nThe\nreal-data-B\ndataset (UCI â€œElectricity Load Diagrams\n2011-2014â€)\n[\n86\n]\nis distributed under the Creative Commons Attribution 4.0 International\nlicense as indicated in\nhttps://archive.ics.uci.edu/ml/datasets/electricityloaddiagrams20112014\n.\nâ€¢\nThe\nreal-data-D\nand\nreal-data-C\nvariants are derived from the same UCI\ndata and inherit the CC-BY-4.0 license.\nâ€¢\nThe\nreal-data-A\ndataset\n5\n5\n5\nHalf-hourly\ndemand for five Australian states\n[\n73\n]\nis distributed under the\nCreative Commons Attribution 4.0 International license as indicated in\nhttps://doi.org/10.5281/zenodo.4659727\n.\nâ€¢\nThe\nreal-data-E\ndataset introduced by\nLai etÂ al. [\n76\n]\nwith publicly available\nfinancial data; no explicit license is provided in the original\nrepository (\nhttps://github.com/laiguokun/multivariate-time-series-data\n), and it is used in\n[\n73\n]\n, which distributes its datasets under the Creative Commons Attribution 4.0 International license.\nN.2\nLibraries\nThe libraries used in our experiments are:\n1.\ndiffuser\nuses the MIT License.\n6\n6\n6\nhttps://github.com/jannerm/diffuser/blob/master/LICENSE\n2.\neinops\nuses the MIT License.\n7\n7\n7\nhttps://github.com/arogozhnikov/einops/blob/main/LICENSE\n3.\nimageio\nuses the BSD 2-Clause License.\n8\n8\n8\nhttps://github.com/imageio/imageio/blob/master/LICENSE\n4.\nloguru\nuses the MIT License.\n9\n9\n9\nhttps://github.com/Delgan/loguru/blob/master/LICENSE\n5.\nmatplotlib\n[\n87\n]\nuses a PSF-based license.\n10\n10\n10\nhttps://github.com/matplotlib/matplotlib/blob/master/LICENSE/LICENSE\n6.\nmujoco_py\nuses the MIT License.\n11\n11\n11\nhttps://github.com/openai/mujoco-py/blob/master/LICENSE\n7.\nnumpy\nuses the BSD 3-Clause License.\n12\n12\n12\nhttps://numpy.org/doc/stable/license.html\n8.\npandas\nuses the BSD 3-Clause License.\n13\n13\n13\nhttps://github.com/pandas-dev/pandas/blob/main/LICENSE\n9.\nscikit-video\nuses the BSD 3-Clause License.\n14\n14\n14\nhttps://github.com/scikit-video/scikit-video/blob/master/LICENSE.txt\n10.\ntorch\n(PyTorch)) is distributed under a permissive, BSD-style license that includes an express patent grant.\n15\n15\n15\nhttps://github.com/pytorch/pytorch/blob/main/LICENSE\n11.\ntqdm\nis licensed under MIT and MPL-2.0.\n16\n16\n16\nhttps://github.com/tqdm/tqdm/blob/master/LICENCE\n12.\nogbench\nuses the MIT License.\n17\n17\n17\nhttps://github.com/seohongpark/ogbench/blob/master/LICENSE",
    "preview_text": "Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.\n\nForecasting in Offline Reinforcement Learning for Non-stationary Environments\nSuzan Ece Ada\n1,2\nGeorg Martius\n2\nEmre Ugur\n1\nErhan Oztop\n3,4\n1\nBogazici University, TÃ¼rkiye\n2\nUniversity of TÃ¼bingen, Germany\n3\nOzyegin University, TÃ¼rkiye\n4\nOsaka University, Japan\nece.ada@bogazici.edu.tr\nAbstract\nOffline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios char",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "conditional diffusion-based candidate state generation",
        "zero-shot time-series foundation models",
        "offline reinforcement learning",
        "non-stationary environments"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºFORLæ¡†æ¶ï¼Œç»“åˆæ¡ä»¶æ‰©æ•£æ¨¡å‹å’Œé›¶æ ·æœ¬æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼Œä»¥å¤„ç†ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„éå¹³ç¨³ç¯å¢ƒé—®é¢˜ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T18:45:05Z",
    "created_at": "2026-01-10T10:50:19.441911",
    "updated_at": "2026-01-10T10:50:19.441920",
    "flag": true
}