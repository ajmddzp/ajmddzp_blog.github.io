{
    "id": "2601.11460v1",
    "title": "Learning Semantic-Geometric Task Graph-Representations from Human Demonstrations",
    "authors": [
        "Franziska Herbert",
        "Vignesh Prasad",
        "Han Liu",
        "Dorothea Koert",
        "Georgia Chalvatzaki"
    ],
    "abstract": "ä»äººç±»æ¼”ç¤ºä¸­å­¦ä¹ ç»“æ„åŒ–ä»»åŠ¡è¡¨ç¤ºå¯¹äºç†è§£é•¿æ—¶ç¨‹æ“ä½œè¡Œä¸ºè‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨åŒæ‰‹æ“ä½œåœºæ™¯ä¸­â€”â€”è¿™ç±»åœºæ™¯ä¸­çš„åŠ¨ä½œé¡ºåºã€ç‰©ä½“å‚ä¸åº¦å’Œäº¤äº’å‡ ä½•å…³ç³»å¯èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå¦‚ä½•ä»¥æ”¯æŒä»»åŠ¡è¿›ç¨‹æ¨ç†çš„å½¢å¼ï¼Œè”åˆæ•æ‰ä»»åŠ¡çš„ç¦»æ•£è¯­ä¹‰ç»“æ„ä¸ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„å‡ ä½•å…³ç³»çš„æ—¶é—´æ¼”åŒ–ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§è¯­ä¹‰-å‡ ä½•ä»»åŠ¡å›¾è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½å¤Ÿä»äººç±»æ¼”ç¤ºä¸­ç¼–ç ç‰©ä½“èº«ä»½ã€ç‰©ä½“é—´å…³ç³»åŠå…¶éšæ—¶é—´æ¼”åŒ–çš„å‡ ä½•ç‰¹å¾ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç»“åˆæ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œç¼–ç å™¨ä¸åŸºäºTransformerè§£ç å™¨çš„å­¦ä¹ æ¡†æ¶ï¼Œå°†åœºæ™¯è¡¨ç¤ºå­¦ä¹ ä¸åŸºäºåŠ¨ä½œæ¡ä»¶çš„ä»»åŠ¡è¿›ç¨‹æ¨ç†è§£è€¦ã€‚ç¼–ç å™¨ä»…ä½¿ç”¨æ—¶åºåœºæ™¯å›¾æ¥å­¦ä¹ ç»“æ„åŒ–è¡¨ç¤ºï¼Œè€Œè§£ç å™¨åˆ™æ ¹æ®åŠ¨ä½œä¸Šä¸‹æ–‡é¢„æµ‹æœªæ¥åŠ¨ä½œåºåˆ—ã€ç›¸å…³ç‰©ä½“åŠé•¿æ—¶ç¨‹å†…çš„ç‰©ä½“è¿åŠ¨è½¨è¿¹ã€‚é€šè¿‡å¯¹äººç±»æ¼”ç¤ºæ•°æ®é›†çš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°è¯­ä¹‰-å‡ ä½•ä»»åŠ¡å›¾è¡¨ç¤ºç‰¹åˆ«é€‚ç”¨äºåŠ¨ä½œå’Œç‰©ä½“é«˜åº¦å¯å˜çš„ä»»åŠ¡åœºæ™¯ï¼Œè€Œä¼ ç»Ÿçš„åºåˆ—æ¨¡å‹éš¾ä»¥æœ‰æ•ˆæ•æ‰æ­¤ç±»ä»»åŠ¡çš„è¿›ç¨‹ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬éªŒè¯äº†ä»»åŠ¡å›¾è¡¨ç¤ºå¯è¿ç§»è‡³å®ä½“åŒæ‰‹æœºå™¨äººç³»ç»Ÿï¼Œå¹¶ç”¨äºåœ¨çº¿åŠ¨ä½œé€‰æ‹©ï¼Œè¿™å‡¸æ˜¾äº†å…¶ä½œä¸ºå¯å¤ç”¨ä»»åŠ¡æŠ½è±¡è¡¨ç¤ºåœ¨æ“ä½œç³»ç»Ÿä¸‹æ¸¸å†³ç­–ä¸­çš„æ½œåŠ›ã€‚",
    "url": "https://arxiv.org/abs/2601.11460v1",
    "html_url": "https://arxiv.org/html/2601.11460v1",
    "html_content": "Learning Semantic-Geometric Task Graph-Representations from Human Demonstrations\nFranziska Herbert\n1,2\n, Vignesh Prasad\n1,2,3\n, Han Liu\n1,2\n, Dorothea Koert\n4,5\nand Georgia Chalvatzaki\n1,2,3\n1\nInteractive Robot Perception & Learning (PEARL) Lab, Computer Science Dept., TU Darmstadt, Germany.\n2\nHessian.AI, Darmstadt, Germany.\n3\nRobotics Institute Germany (RIG).\n4\nInteractive AI Algorithms & Cognitive Models for Human-AI Interaction (IKIDA), Computer Science Dept., TU Darmstadt, Germany.\n5\nCenter for Cognitive Science, TU Darmstadt, Germany. Contact:\nfranziska.herbert@tu-darmstadt.de\nAbstract\nLearning structured task representations from human demonstrations is essential for understanding long-horizon manipulation behaviors, particularly in bimanual settings where action ordering, object involvement, and interaction geometry can vary significantly. A key challenge lies in jointly capturing the discrete semantic structure of tasks and the temporal evolution of object-centric geometric relations in a form that supports reasoning over task progression. In this work, we introduce a semanticâ€“geometric task graph-representation that encodes object identities, inter-object relations, and their temporal geometric evolution from human demonstrations. Building on this formulation, we propose a learning framework that combines a Message Passing Neural Network (MPNN) encoder with a Transformer-based decoder, decoupling scene representation learning from action-conditioned reasoning about task progression. The encoder operates solely on temporal scene graphs to learn structured representations, while the decoder conditions on action-context to predict future action sequences, associated objects, and object motions over extended time horizons. Through extensive evaluation on human demonstration datasets, we show that semanticâ€“geometric task graph-representations are particularly beneficial for tasks with high action and object variability, where simpler sequence-based models struggle to capture task progression. Finally, we demonstrate that task graph representations can be transferred to a physical bimanual robot and used for online action selection, highlighting their potential as reusable task abstractions for downstream decision-making in manipulation systems.\nI\nIntroduction\nLearning successful robotic task executions from human demonstrations requires understanding four key elements:\nwhat\nactions to perform,\nhow\nto execute them,\nwhich\nobjects to manipulate, and\nin what sequence\n. Particularly in bimanual settings where action order and object interactions can vary, learning structured task representations from human demonstrations is crucial for understanding long-horizon manipulation behaviors.\nScene Graphs\n[\n12\n,\n1\n,\n6\n,\n11\n,\n21\n]\nprovide a structured representation for capturing semantic relationships and geometric information.\nThis representation is well-suited for manipulation tasks, where understanding task progression requires reasoning both about discrete action sequences and continuous geometric evolution of the scene.\nBy modeling tasks as temporal scene graphs, we can explicitly capture object interactions and action dependencies, information that is only implicitly represented in raw human demonstration videos.\nGraph Neural Networks\n[\n8\n,\n23\n,\n3\n]\nare naturally suited to learn from structured representations, enabling message passing to aggregate information about object interactions and relations across the graph.\nThis makes them a natural choice for modeling manipulation tasks, where understanding task progression requires reasoning about interactions between objects, hands, and their evolving spatial relationships.\nHowever, existing scene-graph-based approaches to task understanding\n[\n13\n,\n17\n,\n5\n]\ntypically emphasize either semantic structure (e.g., actionâ€“object relations) or geometric evolution (e.g., motion trajectories), but rarely integrate both in a unified representation. As a result, these methods struggle to capture task progression in tasks that exhibit variability in action ordering, object usage, or interaction geometry.\nWe hypothesize that jointly modeling\nsemantic relations\nand\ngeometric evolution\nwithin a single task graph-representation leads to richer object-centric task abstractions. Such representations should enable predicting how a task evolves given the current state and interaction history, and support improved generalization to unseen demonstrationsâ€”particularly for tasks characterized by high action and object variation.\nTo this end, we introduce a semanticâ€“geometric task graph-representation and a GNN-based encoder that jointly models object identities, inter-object relations, and their temporal geometric evolution, enabling the learning of temporal scene representations that capture object-centric relational dynamics over extended horizons.\nUnlike typical action recognition methods that predict a single label per frame\n[\n5\n,\n25\n,\n4\n]\n, our objective is to learn task-level representations that jointly capture semantic action structure and geometric scene evolution; we explicitly capture action-conditioned relational dynamics over time, enabling reasoning about task progression for extended temporal horizons. To this end, we predict future action sequences together with their associated objects and individual object motions. This joint learning encourages the emergence of more general task embeddings and enables forecasting the future evolution of the sceneâ€”an ability that is critical for downstream decision-making and planning for manipulation.\nOur key contributions are:\n(1)\na semanticâ€“geometric task graph-representation that jointly encodes object identities, inter-object relations, and their temporal geometric evolution to represent long-horizon manipulation tasks from human demonstrations;\n(2)\na graph-encoder-transformer-decoder architecture that learns task-level graph representations, capturing action-conditioned relational dynamics under variable action and object orderings;\n(3)\na joint learning framework that couples future action, object, and motion prediction to shape task-level representations capable of forecasting task progression over extended horizons; and\n(4)\na demonstration of transferability, showing that task graph-representations learned from human demonstrations can be reused on a physical bimanual robot for online action selection.\nFigure 1\n:\nGraph model architecture: the graph encoder transforms features into embeddings, MPNN learns graph embeddings, and prediction heads forecast actions, objects, and motions.\nII\nRelated Work\nLearning manipulation task progression from human demonstrations requires representations that capture both semantic task structure and geometric scene evolution.\nPrior work on graph-based action and motion prediction has primarily focused on whole-body skeleton-based representations\n[\n24\n,\n27\n,\n4\n]\n, some of which also make use of spatial-temporal GNNs\n[\n15\n,\n29\n,\n19\n]\n. While such representations have shown promising results for human activity analysis, tabletop manipulation, especially in robotic scenarios, semantic relationships between humans and manipulated objects, as well as task-dependent object-to-object interactions, are more relevant than skeletal joint configurations. In contrast, object-centric representations using scene graphs\n[\n12\n,\n1\n,\n6\n,\n11\n,\n21\n]\nprovide a natural framework for capturing these relationships in manipulation contexts.\nSuch scene graph representations find applications in Human-Object Interaction research\n[\n16\n,\n26\n,\n22\n]\n, however, they mainly focus on frame-level action recognition and segmentation in human videos and do not consider the future progression of a task.\nAlong similar lines, Dreher et al.\n[\n5\n]\nproposed using a scene graph with semantic spatial relations of salient objects and the human in table-top manipulation scenarios to perform GNN-based action prediction. However, like previous works, they only look at instantaneous frame-level predictions. For learning long-horizon plans, it is essential to understand how an action affects the scene, what trajectory to take, and what objects are salient.\nTo this end, Razali et al.\n[\n17\n]\nexplore a geometric scene graph representation for learning whole-body motion prediction from human demonstrations. They learn fine-grained motion progressions of the scene using the geometric relations between different objects in an action-conditioned manner. While they incorporate geometric reasoning, they just predict the evolution of the scene and the fine-grained motion in a zero-shot manner without adapting to the observed scene.\nLagamtzis et al.\n[\n14\n]\ntake a step further and explore simultaneous graph-based action recognition, future action prediction, and motion forecasting from human demonstrations. Their approach incorporates geometric information (object positions) to learn node embeddings for downstream predictions. However, their method neglects semantic edge features and global task context, focusing on single-step unimanual predictions at a fixed horizon. This limits capturing rich semantic relationships and reason about bimanual task progression over extended temporal spans.\nIn contrast, we introduce a semantic-geometric task graph-representation that addresses these limitations by jointly modeling three key aspects within a unified framework: (1) object identities and inter-object semantic relations (via edge features), (2) temporal geometric evolution (via node motion histories), and (3) global task context. Our method learns task-level representations that explicitly capture action-conditioned relational dynamics over extended horizons.\nBy jointly predicting future action sequences together with their associated objects and individual object motions, our approach enables forecasting full task progressions, bridging the gap between discrete symbolic task structure (action and object sequence) and continuous geometric scene evolution.\nIII\nFoundations\nA graph is represented as\nG\n=\n(\nV\n,\nE\n,\nu\n)\nG=(V,E,u)\n, where\nV\nV\nis a set of nodes,\nE\nE\nis a set of edges, and\nu\nu\nis a global graph feature.\nA\nspatial-temporal graph\nis a graph that changes over time\nt\nt\n.\nA spatial-temporal graph over\nH\nH\ndistinct time frames can be defined as\nG\n(\nt\n)\n=\n(\nV\n,\nE\n,\nu\n)\nG^{(t)}=(V,E,u)\nwith the feature matrices\nğ‘¿\nV\nâˆˆ\nâ„\nN\nÃ—\nH\nÃ—\nd\nV\n\\bm{X}^{V}\\in\\mathbb{R}^{N\\times H\\times d_{V}}\n,\nğ‘¿\nE\nâˆˆ\nâ„\nM\nÃ—\nH\nÃ—\nd\nE\n\\bm{X}^{E}\\in\\mathbb{R}^{M\\times H\\times d_{E}}\nand\nu\nâˆˆ\nâ„\nH\nÃ—\nd\nU\nu\\in\\mathbb{R}^{H\\times d_{U}}\n, with\nN\n,\nM\nN,M\nbeing the number of nodes and edges respectively.\nIn Graph Neural Networks (GNNs)\n[\n9\n,\n10\n,\n8\n,\n23\n]\n, node embeddings are iteratively updated by aggregating information from their neighbors. This is typically done using a function that computes a new node embedding based on the nodeâ€™s current state and its neighborsâ€™ states, allowing the model to learn local and global patterns in the graph.\nMessage Passing Neural Networks (MPNNs)\n[\n7\n]\ngeneralize GNNs by providing a flexible framework for learning node, edge and global embeddings through an iterative process of information exchange.\nGraph Convolutional Networks and Graph Attention Networks are different flavors of the more general MPNNs, that mostly differ in their aggregation function\n[\n3\n]\n.\nMPNNs can be formulated to learn edge embeddings\nf\nv\nâ€‹\nw\nf_{vw}\nand global graph embeddings\ng\nu\ng_{u}\n, in addition to the common node embeddings\nh\nv\nh_{v}\n[\n2\n]\n. This allows the model to explicitly capture relationships between nodes and their edges, as well as global attributes affecting the entire graph.\nThrough this updated message passing formulation, the different embeddings are all learned jointly.\nIV\nLearning Semantic-Geometric Task Graph-Representations\nWe propose an approach for learning semantic-geometric task graph-representations from bimanual human demonstrations.\nEach task is modeled as a sequence of actions, performed in order, where actions span over multiple time steps. The objective is to predict future task progression by modeling past object movements and semantic relationships in a graphical structure.\nWe develop a novel method using MPNNs for learning the action-conditioned relational dynamics to forecast actions and motions over long horizons.\nFigure\nLABEL:img:pipeline\ngives an overview of the pipeline.\nIV-A\nSemantic-Geometric Graph Structure\nLet\nğ’Ÿ\nraw\n\\mathcal{D}_{\\text{raw}}\nbe a dataset of manipulation demonstrations captured via RGB-D video.\nWe slice each video into temporal slices with\nH\nH\nhistorical frames and\nP\nP\nfuture frames.\nFor each frame\nt\nt\n, we construct a fully-connected, bi-directional spatial-temporal graph that encodes the scene state at frame\nt\nt\nalong with its history.\nAdditionally, we annotate each frame\nt\nt\nwith the current bimanual actions\na\nt\n=\n(\na\nt\nR\n,\na\nt\nL\n)\na_{t}=(a^{R}_{t},a^{L}_{t})\nand their corresponding objects\no\nt\n=\n(\no\nt\nR\n,\no\nt\nL\n)\no_{t}=(o^{R}_{t},o^{L}_{t})\n.\nIV-A\n1\nNode Features\nThe nodes represent objects in the scene and the userâ€™s hands.\nGoing forward,\nobjects\nalways include the hands.\nUnlike related work\n[\n5\n,\n14\n]\nwhich create separate nodes per time step in the graphâ€™s history, we use a single node per object with temporal features to reduce computational complexity and memory overhead.\nThe node feature matrix\nğ‘¿\nV\n\\bm{X}^{V}\nconsists of the concatenation of the nodeâ€™s one-hot encoded object ID\nc\nn\nc_{n}\nas well as its 3D coordinates\nğ’™\nn\n\\bm{x}_{n}\nfrom\nH\nH\npast frames, sampled at a rate\nS\nS\n(i.e. every\nS\nS\n-th frame).\nParameters\nH\nH\nand\nS\nS\ncontrol the length and temporal granularity of the history.\nThe complete node feature matrix\nğ‘¿\nt\nâˆ’\nH\n+\n1\n:\nt\nV\nâˆˆ\nâ„\nN\nÃ—\nH\nÃ—\nd\nV\n\\bm{X}^{V}_{t-H+1:t}\\in\\mathbb{R}^{N\\times H\\times d_{V}}\nis constructed as\nğ‘¿\nV\nâ€‹\n[\nn\n,\nt\n]\n=\n[\nc\nn\n;\nğ’™\nn\n,\nt\n]\nâˆ€\nn\nâˆˆ\nN\n,\nâˆ€\nt\nâˆˆ\nhistory\n\\bm{X}^{V}[n,t]=[c_{n};\\bm{x}_{n,t}]\\quad\\forall n\\in N,\\forall t\\in\\text{history}\n, with\nd\nV\nd_{V}\nas the sum of possible object classes and coordinate dimensionality.\nIV-A\n2\nEdge Features\nEdges capture semantic relationships between nodes, including static spatial relations (e.g., one object is right of the other) and dynamic movements (e.g., one object approaches another object)\n[\n30\n]\n. Multiple spatial relations can hold true for a pair of objects. The edge feature matrix\nğ‘¿\nt\nâˆ’\nH\n+\n1\n:\nt\nE\nâˆˆ\nâ„\nM\nÃ—\nH\nÃ—\nd\nE\n\\bm{X}^{E}_{t-H+1:t}\\in\\mathbb{R}^{M\\times H\\times d_{E}}\nis defined over the last\nH\nH\ntime steps sampled at a rate\nS\nS\n.\nWe can therefore use a multi-hot encoding to represent the relations between two nodes, where\nd\nE\nd_{E}\nis the number of possible relations.\nIV-A\n3\nGlobal Features\nWe use a global variable\nu\nu\nto encode information relevant to the entire graph and not specific to a certain node or edge.\nIn our case, this global variable consists of an one-hot encoded task ID of the current task. In contrast to node and edge features, the global features\nu\nâˆˆ\nâ„\nd\nU\nu\\in\\mathbb{R}^{d_{U}}\nare not defined over the history of the graph but just the current time step, with\nd\nU\nd_{U}\nbeing the number of possible tasks.\nIV-B\nGraph Neural Network Architecture\nOur goal is to predict future actions, action-objects, and object coordinates over the prediction horizon\nP\nP\n. Our architecture follows an encoder-decoder structure: the graph is first encoded through an MPNN that learns graph embeddings by propagating information across the graph, and these embeddings are then passed through multiple prediction heads to forecast the future progression of the task.\nThe overall architecture is depicted in Figure\n1\n.\nIV-B\n1\nMPNN Encoder\nNode, edge, and global features are encoded separately via linear transformations into a shared hidden dimension\nd\nMP\nd_{\\text{MP}}\n.\nGlobal embeddings are tiled to share the same temporal dimension\nH\nH\nas node and edge embeddings.\nTo consider the temporal aspect of the task, all embeddings are encoded with Rotary Position Embeddings (RoPE)\n[\n18\n]\nover their temporal dimension.\nThe frame id\nt\nt\ncorresponding to each entry in the embedding is used to choose the rotation parameters for that embedding.\nSubsequently, the MPNN refines node, edge, and global embeddings iteratively over\nK\nK\niterations.\nIn each iteration, first the edge embeddings are updated, followed by the node embeddings and the global embeddings.\nThe edge embeddings are updated using linear transformations with learnable weights\nğ‘¾\nk\nE\n\\bm{W}^{E}_{k}\nand non-linearities\nÎ±\n\\alpha\n:\nf\nv\nâ€‹\nw\n(\nk\n+\n1\n)\n\\displaystyle f_{vw}^{(k+1)}\n+\n=\nÎ±\n(\nğ‘¾\n1\n,\nk\nE\n[\nÎ±\n(\nğ‘¾\n2\n,\nk\nE\nf\nv\nâ€‹\nw\n(\nk\n)\n)\n;\n\\displaystyle\\mathrel{+}=\\alpha\\Big(\\bm{W}^{E}_{1,k}\\big[\\alpha\\big(\\bm{W}^{E}_{2,k}f_{vw}^{(k)}\\big);\nÎ±\n(\nğ‘¾\n3\n,\nk\nE\n[\nh\nv\n(\nk\n)\n;\nh\nw\n(\nk\n)\n]\n)\n;\ng\nu\n(\nk\n)\n]\n)\n.\n\\displaystyle\\quad\\quad\\quad\\alpha\\big(\\bm{W}^{E}_{3,k}[h_{v}^{(k)};h_{w}^{(k)}]\\big);g_{u}^{(k)}\\big]\\Big).\nCompared to\n[\n5\n]\n, we use individual weights\nğ‘¾\nk\nE\n\\bm{W}^{E}_{k}\nper message passing iteration\nk\nk\n, allowing to learn different features per iteration, and add residual connections to refine features across iterations.\nNode and global updates follow similarly.\nWe alternate message passing with temporal self-attention over the temporal dimension of the node and edge features.\nThis helps the model learn dependencies across time steps.\nIV-B\n2\nPrediction Decoder Models\nThe decoder predicts three components: future actions, associated objects, and object motions. Since high-level actions and low-level motions operate at different time scales (actions span multiple steps while motions change frame-by-frame), we design a multi-stage decoder to handle these different frequencies.\nIn the first stage, we predict the immediate next action-object pair\na\nt\n+\n1\n,\no\nt\n+\n1\na_{t+1},o_{t+1}\nusing an MLP with current action-object pair\na\nt\n,\no\nt\na_{t},o_{t}\nand the temporally averaged global graph embeddings\ng\nÂ¯\nu\nK\n\\bar{g}_{u}^{K}\nas inputs.\nPredicting the next action-object pairs is important to understand the immediate task progression.\nIn the second stage, we predict future semantic action-object pairs\na\nF\n,\no\nF\na_{F},o_{F}\n, i.e. the next high-level action that will execute after the current action terminates, using another MLP with predicted pair\na\nt\n+\n1\n,\no\nt\n+\n1\na_{t+1},o_{t+1}\nand temporally averaged global embeddings\ng\nÂ¯\nu\nK\n\\bar{g}_{u}^{K}\nas input.\nThis decouples high-level action forecasting from the current actionâ€™s duration, allowing the model to reason about task sequencing independently.\nBy separating scene representation learning in the encoder from action-conditioned reasoning in the decoder, the model avoids entangling action labels with perceptual representations while enabling explicit reasoning over task progression.\nThe overall goal of our model is to predict the actions, objects, and motions for each time step in the prediction horizon\nP\nP\n. To do so, we construct action-object queries that capture both action-object context and temporal information. Task progression depends on the motion history (encoded in graph embeddings) and on the semantic high-level action-object history. We track the\nn\npast\nn_{\\text{past}}\nmost recent semantic action-object pairs per hand, since action switches occur at different times for left and right hands.\nFor notational brevity, we denote action-object pairs as\na\ni\n=\n(\na\ni\n,\no\ni\n)\na_{i}=(a_{i},o_{i})\n.\nFor each hand\ni\nâˆˆ\n{\nR\n,\nL\n}\ni\\in\\{R,L\\}\n, we build a query\na\ni\n,\nQ\n=\n[\na\ni\n,\nHIST\nâ€‹\n|\na\ni\n,\nt\n+\n1\n|\nâ€‹\na\ni\n,\nF\n]\na_{i,Q}=[a_{i,\\text{HIST}}|a_{i,t+1}|a_{i,F}]\n, concatenating the past pairs\na\ni\n,\nHIST\na_{i,\\text{HIST}}\n, predicted next pairs\na\ni\n,\nt\n+\n1\na_{i,t+1}\n, and predicted future semantic pairs\na\ni\n,\nF\na_{i,F}\n.\nWe linearly transform the queries into embedding space and encode temporal structure using RoPE embeddings\n[\n18\n]\nbased on action start frames.\nAction queries from both hands are concatenated and transformed into embedding space for the final query vector\nQ\nA\nQ^{A}\n, which is used in two separate Transformer decoders\n[\n20\n]\n.\nThe action-object decoder attends to the (non-averaged) global embeddings\ng\nu\nK\ng_{u}^{K}\nto predict action and object sequences, while the motion decoder attends to node embeddings\nh\nv\nK\nh_{v}^{K}\nto predict object motion sequences.\nBoth outputs are projected to their respective output spaces via linear layers.\nIV-C\nTraining Long-Horizon Task Predictions\nOur model is trained on joint action and motion prediction.\nWe make use of a joint loss function that integrates multiple objectives, addressing both classification and regression tasks. Specifically, we train the action and object classifiers using weighted cross-entropy (CE) loss, which compensates for imbalances in the dataset by assigning higher weights to less frequent actions and objects. The motion prediction task is treated as a regression problem, using mean squared error (MSE) between predicted and ground-truth coordinates.\nThe total loss function is a weighted sum of these losses\nâ„’\nTotal\n=\nâˆ‘\na\nâ„’\na\nCE\n+\nâˆ‘\no\nâ„’\no\nCE\n+\nÎ²\nMSE\nâ€‹\nâ„’\nx\nt\n+\n1\n:\nt\n+\nP\nMSE\n,\n\\displaystyle\\mathcal{L}^{\\text{Total}}=\\sum_{a}\\mathcal{L}^{\\text{CE}}_{a}+\\sum_{o}\\mathcal{L}^{\\text{CE}}_{o}+\\beta_{\\text{MSE}}\\,\\mathcal{L}^{\\text{MSE}}_{x_{t+1:t+P}},\nwhere action losses include\n{\na\nt\n+\n1\n,\na\nF\n,\na\nt\n+\n1\n:\nt\n+\nP\n}\n\\{a_{t+1},a_{F},a_{t+1:t+P}\\}\n, object losses include\n{\no\nt\n+\n1\n,\no\nF\n,\no\nt\n+\n1\n:\nt\n+\nP\n}\n\\{o_{t+1},o_{F},o_{t+1:t+P}\\}\nand\nÎ²\nMSE\n\\beta_{\\text{MSE}}\nbalances regression and classification losses. We use the AdamW optimizer for efficient convergence.\nTo enable long-horizon task execution beyond the fixed prediction horizon\nP\nP\nand to determine which action to execute at each step, we employ\naction chunking\n[\n28\n]\n.\nFor each time step\nt\nt\n, we predict actions, objects, and motions over the next\nP\nP\nframes. This produces overlapping predictions that are reconciled using temporal ensembles with exponential decay weighting, where higher weights are assigned to older predictions.\nFinal predictions are computed as weighted averages of all overlapping predictions.\nFollowing this training regime, our model integrates object relationships and spatial-temporal reasoning to predict future task progression, including both short-term and long-term action and motion predictions.\nLeveraging action chunking and temporal ensembles ensures smooth, continuous predictions over extended horizons.\nIn the following section, we demonstrate the effectiveness of this method through extensive experiments on human manipulation datasets, highlighting its generalization across diverse tasks and demonstrators.\nFigure 2\n:\nPredictions on a\ncooking\ntask of the KIT Bimacs Dataset\nusing the observations of just 100 timesteps (denoted by the vertical dashed line).\nThe first three plots show the 3D motions for the different objects in the scene. The last plot shows the action and action-object predictions for both hands.\nV\nExperiments and Results\nIn this section, we evaluate whether semanticâ€“geometric task graph-representations learned from human demonstrations capture long-horizon task structure and generalize across tasks, subjects, and embodiments. Particularly, our experiments are designed to assess (i) when structured relational inductive biases are beneficial, and (ii) whether the learned representations can be transferred to a physical bimanual system. We evaluate the models on the KIT Bimanual Actions Dataset (Bimacs)\n[\n5\n]\n, containing RGB-D recordings of bimanual demonstrations for five cooking and four workshop tasks by six subjects with ten takes each.\nThe dataset provides ground-truth hand actions, 3D object bounding boxes, and semantic objectâ€“object relations.\nWe additionally annotated actionâ€“object labels, removed object detections in the background, applied small noise smoothing to the trajectories, standardized demonstration trajectories across the training set, and augmented the data through mirroring and temporal resampling.\nTo adapt the learned models to our laboratory setting, we collected an additional dataset of four cooking-related tabletop tasks: the\ncooking\nand\nwiping\ntasks from KIT(Bimacs), plus two new tasksâ€”inserting and removing objects from a bowl. Each task was performed by four subjects for ten takes, referred to as Ours(Bimacs).\nFor transferring the models to our bimanual robot, we recorded ten demonstrations using predefined primitive sequences, referred to as Ours(Robot).\nV-A\nBaseline Architectures\nTo show the benefits of our GNN-based encoder, we compare the performance of our encoder with four different encoder architectures. The following baselines are used to probe the role of different inductive biases in learning task representations.\nEach model uses a different encoder structure, but uses our decoder to ensure comparability between the models for our specific prediction objectives.\nDreher\n[\n5\n]\nAn iterative MPNN architecture that encodes the scene structure using only object identifiers as node features and semantic relations as edge features, with separate nodes representing different time instances. Unlike our proposed model, this baseline completely ignores geometric information. The GNN weights remain constant across all 10 iterations of the message passing layer.\nLagamtzis\n[\n14\n]\nA deep relational graph convolutional network (RGCN) that updates only node embeddings, neglecting semantic edge and global features. Node features comprise object identifiers and geometric information, with separate nodes representing each time step. We employ 20 RGCN blocks instead of the 36 used in the original paper for comparability of parameters.\nTransformer\nA classical transformer encoder\n[\n20\n]\nthat operates on the sequentialized representation of our node features, consisting of object ids and geometric information. This model does not utilize the graph structure of the input, has no access to the semantic edge features and treats the input purely as a sequence. The model uses 8 encoder layers, with 16 attention heads each.\nDecoder-Only\nThis baseline omits the encoder entirely. Node features are projected into the embedding space and fed directly into a Transformer decoder. Therefore, the model cannot exploit the underlying graph structure or exchange information across nodes. This architecture is included to quantify the encoderâ€™s contribution and highlight the importance of incorporating message passing or structured context.\nMPNN(Ours)\nThe GNN-based encoder architecture described in Section\nIV-B\n. We use\nK\n=\n3\nK=3\nmessage passing iterations, 2 attention heads in the temporal layer, and Leaky-ReLU as activation function.\nAn overview over all models is shown in Table\nI\n. The models of Dreher\n[\n5\n]\nand Lagamtzis\n[\n14\n]\nserve as ablation baselines for our GNN-based encoder architecture, isolating the contributions of combining semantic edge features with geometric node features. In contrast, the Transformer and Decoder-Only models are included to assess the benefits gained from explicitly leveraging the scene graph structure.\nThe hyperparameters of all encoder variants were selected such that their total parameter counts fall within the same range as our proposed encoder, ensuring a fair comparison across model architectures.\nFollowing extensive hyperparameter search, all models use\nH\n=\n10\nH=10\n,\nS\n=\n10\nS=10\n,\nP\n=\n10\nP=10\n,\nd\nM\nâ€‹\nP\n=\n64\nd_{MP}=64\n, 2-layer MLPs with Leaky-ReLU for action/object classifiers, 2-layer transformer decoders with 4 attention heads,\nn\npast\n=\n20\nn_{\\text{past}}=20\n, batch size 128, and\nÎ²\nMSE\n=\n1000\n\\beta_{\\text{MSE}}=1000\n.\nTABLE I\n:\nOverview over the different baseline models in this work.\nDreher\nLagamtzis\nTrans-\nDecoder-\nMPNN\n[\n5\n]\n[\n14\n]\nformer\nOnly\n(Ours)\nGraph-based\nâœ“\nâœ“\nâœ“\nGeometric info\nâœ“\nâœ“\nâœ“\nâœ“\nSemantic info\nâœ“\nâœ“\nEncoder-based\nâœ“\nâœ“\nâœ“\nâœ“\nFigure 3\n:\nAccuracies for the different action and object predictions of the different models on the various datasets (higher values are better).\nV-B\nPrediction Results on Human Demonstrations\nWe begin by examining results from training models on human demonstrations. All models are trained separately on each task from the two human datasets and in multi-task settings. Figure\n2\nvisualizes our MPNN modelâ€™s performance on an exemplary\ncooking\ntask demonstration. The task involves one hand approaching, lifting, and stirring with a whisk before placing it down, while the other hand holds the bowl, pours from a bottle, and holds the bowl again. The plot shows motion predictions for each object, plus action and object predictions for both hands. Motion predictions are accurate and match ground truth trajectories. Action predictions contain all relevant actions and align with object predictions, however, certain reaching actions (approach and retreat) are predicted for very short time spans that do not reflect true execution time.\nTowards the end, the right hand predictions flicker between place and stir.\nFigure\n3\nshows results for action and object prediction, and Figure\n4\nshows results for motion predictions across all tasks from KIT(Bimacs) and Ours(Bimacs) and a mixed version of both datasets. Evaluation uses leave-one-subject-out cross-validation over human subjects, averaging over 4 seeds per setting. Model performance is compared using classification accuracy for action and object predictions, and RMSE for motion prediction.\nDuring training, we observed that MPNN models converge faster or at a similar rate to the other models. We evaluate all models at that same epoch to provide comparable training data across models.\nKIT(Bimacs)\nWe begin by analyzing single-task results on KIT(Bimacs). For the auxiliary semantic next and future action predictions, MPNN and the Transformer achieve the highest accuracies, indicating their ability for accurate task progression prediction. The Decoder-only model achieves similar accuracies on simple tasks like\ncooking\n, but underperforms on complex tasks with higher object variation like\nwiping\nor action variation like\ncereals\n, revealing limitations of predicting task progression solely through memorizing past action/object sequences. Dreher\n[\n5\n]\nand Lagamtzis\n[\n14\n]\nperform poorly on semantic actions, demonstrating the need of combining geometric and semantic scene information.\nFor action prediction over horizon\nP\nP\n, model differences are smaller compared to semantic action predictions, though MPNN consistently benefits from modeling semanticâ€“geometric relations, particularly in tasks with high action or object variability. This objective requires predicting the next\nP\nP\nactions independently of individual action durations; models achieve high accuracies by simply repeating the current action without considering past semantic-geometric information or learning future task progression. Conversely, Dreher\n[\n5\n]\nperforms better on action prediction over the horizon than Lagamtzis\n[\n14\n]\n, while for the semantic action predictions it is the other way around. Dreher\n[\n5\n]\neven outperforms our MPNN on the horizon predictions of sawing, but falls back for the semantic predictions.\nFigure 4\n:\nResults of the RMSE for motion prediction of different models on the various datasets (lower is better).\nAccurate motion forecasting requires integrating past object interactions and relative movements, which are naturally captured by message passing over semanticâ€“geometric task graphs. Object predictions largely follow action predictions, but accuracy differences between models are smaller for simple tasks since multiple subsequent actions often involve the same objects, reducing complexity. For the\nwiping\ntask with more object variety, differences become apparent: our MPNN clearly outperforms all models including the Transformer. Overall object prediction accuracies are lower compared to other tasks, as sub-task order may vary across demonstrations, eliminating unique solutions for object predictions.\nAcross all tasks, our MPNN achieves the best motion prediction results, demonstrating its ability to utilize past scene information. Notably, Dreher\n[\n5\n]\nunderperforms here due to lacking geometric information.\nOurs(Bimacs)\nResults for\ncooking\nand\nwiping\non Ours(Bimacs) match those on KIT(Bimacs). The additional tasks (\ninsert\nand\ntake out\n) were chosen to include high object variation, further demonstrating the benefits of our MPNN model on tasks requiring past object motion reasoning. For these tasks, the Decoder-only model underperforms. Our MPNN model achieves the highest object prediction accuracy, followed by the Transformer. For object prediction over horizon\nP\nP\n, Dreher\n[\n5\n]\nranks second, confirming our KIT(Bimacs) findings that GNN encoders without geometric information can predict immediate task progression, but fall back for the semantic predictions.\nMulti-task Models\nWe also trained multi-task models on all cooking, all workshop, and all tasks of KIT(Bimacs) and Ours(Bimacs).\nThe results match single-task observations and\ndemonstrate that our MPNN encoder learns common concepts and can generalize between different tasks.\nNotably, Dreher\n[\n5\n]\nperforms worse on semantic action and object predictions for multi-task versus single-task models.\nMixed(Bimacs)\nThe KIT(Bimacs) and Ours(Bimacs) datasets contain demonstrations of two common tasks:\ncooking\nand\nwiping\n. Our additional demonstrations recorded in our own setup bridge the gap between human and robotic setups. To investigate whether our model can learn task progression independently of the underlying environment, we mix the two datasets and train all models on Mixed(Bimacs). Results show that our MPNN modelâ€™s action prediction accuracies do not dramatically decrease on the mixed dataset compared to individual datasets, and even improve for most objectives. The other models also improve when mixing datasets, likely due to the additional training data.\nV-C\nTransfer of Task Representations to Bimanual Robot\nTABLE II\n:\nCross-Validation Results of training the MPNN (Ours) Model on robot demonstrations, evaluating mixed models trained on human demonstrations on the robot dataset, and finetuning those models on robot data.\nMethod\nAccuracy\n(\nâ†‘\n)\n(\\uparrow)\nRMSE [m]\na\nt\n+\n1\na_{t+1}\na\nF\na_{F}\na\nt\n+\n1\n:\nt\n+\nP\na_{t+1:t+P}\no\nt\n+\n1\no_{t+1}\no\nF\no_{F}\no\nt\n+\n1\n:\nt\n+\nP\no_{t+1:t+P}\n(\nâ†“\n)\n(\\downarrow)\nRobot\n0.7460\nÂ±\n\\pm\n0.0501\n0.6594\nÂ±\n\\pm\n0.0362\n0.6443\nÂ±\n\\pm\n0.0453\n0.8452\nÂ±\n\\pm\n0.0317\n0.8538\nÂ±\n\\pm\n0.0087\n0.7840\nÂ±\n\\pm\n0.0148\n0.0547\nÂ±\n\\pm\n0.0013\nNot Finetuned\n0.9279\nÂ±\n\\pm\n0.0235\n0.9253\nÂ±\n\\pm\n0.0075\n0.7661\nÂ±\n\\pm\n0.0108\n0.8044\nÂ±\n\\pm\n0.0106\n0.9185\nÂ±\n\\pm\n0.0057\n0.7950\nÂ±\n\\pm\n0.0062\n0.0304\nÂ±\n\\pm\n0.0002\nFinetuned\n0.9454\nÂ±\n\\pm\n0.0249\n0.9635\nÂ±\n\\pm\n0.0165\n0.8815\nÂ±\n\\pm\n0.0166\n0.8878\nÂ±\n\\pm\n0.0107\n0.9459\nÂ±\n\\pm\n0.0146\n0.8664\nÂ±\n\\pm\n0.0059\n0.0212\nÂ±\n\\pm\n0.0009\nWe evaluate whether task graph representations learned from human demonstrations can be transferred to a physical bimanual robot and queried for online action selection under real-world constraints. The robot task is intentionally simple, serving as a feasibility study for transfer.\nIn this experiment, we implemented the primitive actions of the\ncooking\ntask.\nWe compare two settings: direct evaluation of our models trained on the mixed human dataset, and finetuning these on robot demonstrations (Table\nII\n).\nWe compare both against models trained solely on the robot data.\nDuring finetuning, we freeze the encoder and update only decoder parameters.\nModels trained exclusively on limited robot data perform poorly, while those trained on human demonstrations achieve higher accuracies and lower RMSE, indicating effective transfer.\nBest performance comes after finetuning, attributed to human-robot motion differences and embodiment mismatch that is not addressed in our work.\nConsequently, real-robot experiments use the finetuned mixed model.\nFor real-robot evaluation, the model is used only for online action selection.\nPredicted action-object pairs trigger predefined primitives only when no action is currently executing; predictions are dropped otherwise.\nActions that cannot be safely executed in the current state are filtered out by a precondition checker.\nLeft- and right-hand actions are queried independently.\nWe ran 10 trials with varied object placements.\nTask success requires completing the full bimanual action sequence without primitive failures.\nUsing the learned task representations on the robot results in a 90% success rate.\nThe one failure stems from a single missed left-hand action in the predicted sequence.\nWe also measure the action accuracies of the predicted sequences compared to the human ground truth, resulting in 99% for the left and 100% for the right hand.\nThe precondition checker rarely intervenes, blocking actions in only four trials where the model skips fast actions like\nplace\nor confuses objects between hands.\nThis results in an intervention rate of 13.51% for the left and 9.92% for the right hand.\nV-D\nDiscussion\nResults on human demonstrations indicate that the proposed semanticâ€“geometric task graph-representations support more accurate modeling of long-horizon task progression, particularly for motion forecasting and object-centric reasoning. The MPNN encoder consistently benefits from jointly encoding semantic relations and geometric evolution, highlighting the importance of integrating both information modalities into the graph. In contrast, encoders with weaker relational inductive biases struggle to capture task progression in demonstrations with high action and object variability, where sequence-level reasoning alone is insufficient.\nThese observations suggest that task variabilityâ€”both in terms of action ordering and object involvementâ€”is a key factor in determining when structured relational representations are advantageous. When tasks exhibit limited variation, simpler architectures can suffice; however, as the complexity of the interaction increases, explicitly modeling semanticâ€“geometric relations becomes increasingly important for learning transferable task abstractions.\nLearned task representations successfully transfer to a physical bimanual robot, but require finetuning on robot demonstrations for successful action predictions.\nThe robot experiment is intentionally restricted to a simple task and serves as a feasibility study rather than a discriminative evaluation of execution performance. While this setup validates that task representations learned from human demonstrations can be reused under real-world constraints, it does not require complex task reasoning or motion adaptation.\nExtending the robotic task complexity to include higher action and object variation will require richer demonstration data and tighter integration between task-level reasoning and low-level control.\nIn such settings, predefined robot primitives will likely become limiting. While in this work the model is used primarily for action selection, the predicted object motions offer a natural pathway toward integrating task graph representations with model predictive control or trajectory optimization.\nThis can enable more adaptive robotic planning for bimanual tasks, combining high-level task progression with direct low-level control.\nVI\nConclusion and Outlook\nIn this paper, we showed that our GNN-based encoder is able to learn semantic-geometric task-level representations that capture long-horizon action and motion sequences from human demonstrations.\nWhen compared against other state-of-the-art GNN-based approaches\n[\n5\n,\n13\n]\n, Transformers, and Decoder-only models,\nour results indicate the benefit of the proposed MPNN architecture for\npredicting task progression of bimanual manipulation sequences, especially for tasks with high action and object variation.\nWe further demonstrated that the learned task representations can be successfully transferred to a real bimanual robot to drive action selection under real-world constraints.\nOur approachâ€™s limitations include difficulty generalizing to robotic tasks without finetuning and the need for more training data variety to address human-robot movement mismatches.\nThe different temporal frequencies of high-level actions and low-level object motions easily cause memorization and unwanted conditioning on high-level inputs, requiring further research to better handle action switches.\nCurrently, we use our model only for action selection in real robotic tasks. Future work will explore using motion predictions for MPC or trajectory optimization.\nWe aim to enforce task understanding in multi-task settings and add modalities such as operator gaze and narrations to the graph.\nAcknowledgement\nThis research is funded by the EU Horizon Europe Projects\nâ€œMANiBOTâ€\n(101120823),\nâ€œARISEâ€\n(101135959), the German Research Foundation (DFG) Emmy Noether Programme (CH 2676/1-1), the European Research Council (ERC) project\nâ€œSIRENâ€\n(101163933), the German Federal Ministry of Research, Technology and Space (BMFTR) Project\nâ€œRIGâ€\n(16ME1001), and the German Federal Ministry of Research, Technology and Space (BMFTR) Project\nâ€œIKIDAâ€\n(01IS20045).\nThe authors gratefully acknowledge the computing time provided to\nthem on the high-performance computer Lichtenberg II at TU Darmstadt,\nfunded by the German Federal Ministry of Research, Technology and Space (BMFTR)\nand the State of Hesse.\nReferences\n[1]\nI. Armeni\net al.\n(2019)\n3d scene graph: a structure for unified semantics, 3d space, and camera\n.\nIn\nIEEE/CVF international conference on computer vision\n,\nCited by:\nÂ§I\n,\nÂ§II\n.\n[2]\nP. W. Battaglia\net al.\n(2018)\nRelational inductive biases, deep learning, and graph networks\n.\npreprint arXiv:1806.01261\n.\nCited by:\nÂ§III\n.\n[3]\nM. M. Bronstein\net al.\n(2021)\nGeometric deep learning: grids, groups, graphs, geodesics, and gauges\n.\npreprint arXiv:2104.13478\n.\nCited by:\nÂ§I\n,\nÂ§III\n.\n[4]\nJ. Butepage\net al.\n(2017)\nDeep representation learning for human motion prediction and classification\n.\nIn\nIEEE conference on computer vision and pattern recognition\n,\nCited by:\nÂ§I\n,\nÂ§II\n.\n[5]\nC. R. Dreher\net al.\n(2019)\nLearning object-action relations from bimanual human demonstration using graph networks\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§II\n,\nÂ§\nIV-A\n1\n,\nÂ§\nIV-B\n1\n,\nÂ§\nV-A\n,\nÂ§\nV-A\n,\nÂ§\nV-B\n,\nÂ§\nV-B\n,\nÂ§\nV-B\n,\nÂ§\nV-B\n,\nÂ§\nV-B\n,\nTABLE I\n,\nÂ§V\n,\nÂ§VI\n.\n[6]\nP. Gay\net al.\n(2018)\nVisual graphs from motion (vgfm): scene understanding with object geometry reasoning\n.\nIn\nAsian Conference on Computer Vision\n,\nCited by:\nÂ§I\n,\nÂ§II\n.\n[7]\nJ. Gilmer\net al.\n(2017)\nNeural message passing for quantum chemistry\n.\nIn\nInternational conference on machine learning\n,\nCited by:\nÂ§III\n.\n[8]\nD. Grinberg\n(2023)\nAn introduction to graph theory\n.\npreprint arXiv:2308.04512\n.\nCited by:\nÂ§I\n,\nÂ§III\n.\n[9]\nJ. L. Gross\net al.\n(2013)\nHandbook of graph theory\n.\nCRC press\n.\nCited by:\nÂ§III\n.\n[10]\nD. Joyner\net al.\n(2013)\nAlgorithmic graph theory and sage\n.\nCited by:\nÂ§III\n.\n[11]\nU. Kim\net al.\n(2019)\n3-d scene graph: a sparse and semantic representation of physical environments for intelligent agents\n.\nIEEE transactions on cybernetics\n.\nCited by:\nÂ§I\n,\nÂ§II\n.\n[12]\nR. Krishna\net al.\n(2017)\nVisual genome: connecting language and vision using crowdsourced dense image annotations\n.\nInternational journal of computer vision\n.\nCited by:\nÂ§I\n,\nÂ§II\n.\n[13]\nD. Lagamtzis\net al.\n(2023)\nExploiting spatio-temporal human-object relations using graph neural networks for human action recognition and 3d motion forecasting\n.\nIn\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nCited by:\nÂ§I\n,\nÂ§VI\n.\n[14]\nD. Lagamtzis\net al.\n(2023)\nGraph neural networks for joint action recognition, prediction and motion forecasting for industrial human-robot collaboration\n.\nIn\n56th International Symposium on Robotics\n,\nCited by:\nÂ§II\n,\nÂ§\nIV-A\n1\n,\nÂ§\nV-A\n,\nÂ§\nV-A\n,\nÂ§\nV-B\n,\nÂ§\nV-B\n,\nTABLE I\n.\n[15]\nM. Li\net al.\n(2021)\nSymbiotic graph neural networks for 3d skeleton-based human action recognition and motion prediction\n.\nIEEE transactions on pattern analysis and machine intelligence\n.\nCited by:\nÂ§II\n.\n[16]\nR. Morais\net al.\n(2021)\nLearning asynchronous and sparse human-object interaction in videos\n.\nIn\nIEEE/CVF conference on computer vision and pattern recognition\n,\nCited by:\nÂ§II\n.\n[17]\nH. Razali\net al.\n(2023)\nAction-conditioned generation of bimanual object manipulation sequences\n.\nIn\nAAAI conference on artificial intelligence\n,\nCited by:\nÂ§I\n,\nÂ§II\n.\n[18]\nJ. Su\net al.\n(2024)\nRoformer: enhanced transformer with rotary position embedding\n.\nNeurocomputing\n.\nCited by:\nÂ§\nIV-B\n1\n,\nÂ§\nIV-B\n2\n.\n[19]\nJ. Tao\net al.\n(2021)\nScene-perception graph convolutional networks for human action prediction\n.\nIn\nInternational Joint Conference on Neural Networks\n,\nCited by:\nÂ§II\n.\n[20]\nA. Vaswani\net al.\n(2017)\nAttention is all you need\n.\nAdvances in neural information processing systems\n.\nCited by:\nÂ§\nIV-B\n2\n,\nÂ§\nV-A\n.\n[21]\nJ. Wald\net al.\n(2020)\nLearning 3d semantic scene graphs from 3d indoor reconstructions\n.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\nCited by:\nÂ§I\n,\nÂ§II\n.\n[22]\nJ. Wu\net al.\n(2025)\nHierGAT: hierarchical spatial-temporal network with graph and transformer for video hoi detection\n.\nMultimedia Systems\n.\nCited by:\nÂ§II\n.\n[23]\nZ. Wu\net al.\n(2020)\nA comprehensive survey on graph neural networks\n.\nIEEE transactions on neural networks and learning systems\n.\nCited by:\nÂ§I\n,\nÂ§III\n.\n[24]\nB. Xiaohan Nie, C. Xiong, and S. Zhu\n(2015)\nJoint action recognition and pose estimation from video\n.\nIn\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\nÂ§II\n.\n[25]\nS. Yan\net al.\n(2018)\nSpatial temporal graph convolutional networks for skeleton-based action recognition\n.\nIn\nAAAI conference on artificial intelligence\n,\nCited by:\nÂ§I\n.\n[26]\nZ. Zeng\net al.\n(2023)\nCognition guided human-object relationship detection\n.\nIEEE Transactions on Image Processing\n.\nCited by:\nÂ§II\n.\n[27]\nD. Zhang\net al.\n(2021)\nNon-local graph convolutional network for joint activity recognition and motion prediction\n.\nIn\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nCited by:\nÂ§II\n.\n[28]\nT. Z. Zhao\net al.\n(2023)\nLearning fine-grained bimanual manipulation with low-cost hardware\n.\nRobotics: Science and Systems\n.\nCited by:\nÂ§\nIV-C\n.\n[29]\nY. Zheng\net al.\n(2024)\nSpatio-temporal fusion for human action recognition via joint trajectory graph\n.\nIn\nAAAI conference on artificial intelligence\n,\nCited by:\nÂ§II\n.\n[30]\nF. Ziaeetabar\net al.\n(2018)\nRecognition and prediction of manipulation actions using enriched semantic event chains\n.\nRobotics and Autonomous Systems\n.\nCited by:\nÂ§\nIV-A\n2\n.",
    "preview_text": "Learning structured task representations from human demonstrations is essential for understanding long-horizon manipulation behaviors, particularly in bimanual settings where action ordering, object involvement, and interaction geometry can vary significantly. A key challenge lies in jointly capturing the discrete semantic structure of tasks and the temporal evolution of object-centric geometric relations in a form that supports reasoning over task progression. In this work, we introduce a semantic-geometric task graph-representation that encodes object identities, inter-object relations, and their temporal geometric evolution from human demonstrations. Building on this formulation, we propose a learning framework that combines a Message Passing Neural Network (MPNN) encoder with a Transformer-based decoder, decoupling scene representation learning from action-conditioned reasoning about task progression. The encoder operates solely on temporal scene graphs to learn structured representations, while the decoder conditions on action-context to predict future action sequences, associated objects, and object motions over extended time horizons. Through extensive evaluation on human demonstration datasets, we show that semantic-geometric task graph-representations are particularly beneficial for tasks with high action and object variability, where simpler sequence-based models struggle to capture task progression. Finally, we demonstrate that task graph representations can be transferred to a physical bimanual robot and used for online action selection, highlighting their potential as reusable task abstractions for downstream decision-making in manipulation systems.\n\nLearning Semantic-Geometric Task Graph-Representations from Human Demonstrations\nFranziska Herbert\n1,2\n, Vignesh Prasad\n1,2,3\n, Han Liu\n1,2\n, Dorothea Koert\n4,5\nand Georgia Chalvatzaki\n1,2,3\n1\nInteractive Robot Perception & Learning (PEARL) Lab, Computer Science Dept., TU Darmstadt, Germany.\n2\nHessian.AI, D",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "human demonstrations",
        "task graph-representations",
        "manipulation",
        "bimanual settings",
        "Message Passing Neural Network",
        "Transformer",
        "scene graphs",
        "action selection"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ä»äººç±»æ¼”ç¤ºä¸­å­¦ä¹ è¯­ä¹‰-å‡ ä½•ä»»åŠ¡å›¾è¡¨ç¤ºçš„æ–¹æ³•ï¼Œç”¨äºåŒæ‰‹æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä½†æœªæ¶‰åŠå¼ºåŒ–å­¦ä¹ ã€VLAã€æ‰©æ•£æ¨¡å‹ã€Flow Matchingã€è¿åŠ¨æ§åˆ¶ã€VLMæˆ–å…¨èº«æ§åˆ¶ç­‰å…³é”®è¯ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-16T17:35:00Z",
    "created_at": "2026-01-20T17:50:01.930461",
    "updated_at": "2026-01-20T17:50:01.930469",
    "recommend": 0
}