{
    "id": "2601.14871v1",
    "title": "On-the-fly hand-eye calibration for the da Vinci surgical robot",
    "authors": [
        "Zejian Cui",
        "Ferdinando Rodriguez y Baena"
    ],
    "abstract": "åœ¨æœºå™¨äººè¾…åŠ©å¾®åˆ›æ‰‹æœ¯ï¼ˆRMISï¼‰ä¸­ï¼Œç²¾ç¡®çš„å·¥å…·å®šä½å¯¹äºç¡®ä¿æ‚£è€…å®‰å…¨å’Œæ‰‹æœ¯ä»»åŠ¡çš„æˆåŠŸæ‰§è¡Œè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¯¹äºå¦‚è¾¾èŠ¬å¥‡æœºå™¨äººè¿™ç±»çº¿ç¼†é©±åŠ¨æœºå™¨äººè€Œè¨€ï¼Œç”±äºç¼–ç å™¨è¯»æ•°è¯¯å·®ä¼šå¯¼è‡´ä½å§¿ä¼°è®¡åå·®ï¼Œå®ç°ç²¾å‡†å®šä½ä»å…·æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åœ¨çº¿è®¡ç®—æ‰‹çœ¼å˜æ¢çŸ©é˜µçš„æ ‡å®šæ¡†æ¶ï¼Œä»¥ç”Ÿæˆç²¾ç¡®çš„å·¥å…·å®šä½ç»“æœã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªç›¸äº’å…³è”çš„ç®—æ³•æ¨¡å—ï¼šç‰¹å¾å…³è”æ¨¡å—ä¸æ‰‹çœ¼æ ‡å®šæ¨¡å—ã€‚å‰è€…æ— éœ€é¢„è®­ç»ƒå³å¯ä¸ºå•ç›®å›¾åƒä¸­æ£€æµ‹åˆ°çš„å…³é”®ç‚¹æä¾›é²æ£’å¯¹åº”å…³ç³»ï¼Œåè€…é€šè¿‡é‡‡ç”¨å¤šç§æ»¤æ³¢æ–¹æ³•é€‚é…ä¸åŒæ‰‹æœ¯åœºæ™¯ã€‚ä¸ºéªŒè¯æ¡†æ¶æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨å…¬å¼€è§†é¢‘æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ï¼Œè¿™äº›æ•°æ®é›†æ¶µç›–ä½“å¤–ä¸ç¦»ä½“åœºæ™¯ä¸­å¤šç§æ‰‹æœ¯å™¨æ¢°æ‰§è¡Œä»»åŠ¡çš„æƒ…å†µï¼ŒåŒ…å«ä¸åŒå…‰ç…§æ¡ä»¶åŠå…³é”®ç‚¹æµ‹é‡ç²¾åº¦ç­‰çº§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è¯¥æ ‡å®šæ¡†æ¶åå·¥å…·å®šä½è¯¯å·®æ˜¾è‘—é™ä½ï¼Œå…¶ç²¾åº¦ä¸ç°æœ‰å…ˆè¿›æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶å…·æœ‰æ›´ä¼˜çš„æ—¶é—´æ•ˆç‡ã€‚",
    "url": "https://arxiv.org/abs/2601.14871v1",
    "html_url": "https://arxiv.org/html/2601.14871v1",
    "html_content": "On-the-fly hand-eye calibration for the da Vinci surgical robot\nZejian Cui\n1\nand Ferdinando Rodriguez y Baena\n1\n*This work is supported by funding from the Department of Mechanical Engineering, Imperial College London\n1\nBoth authors are with the Mechatronics in Medicine Laboratory, the Hamlyn Centre for Robotics Surgery, Department of Mechanical Engineering, Imperial College London, Exhibition Road, London, SW7 2AZ, UK\n{zejian.cui19, f.rodriguez}@imperial.ac.uk\nAbstract\nIn Robot-Assisted Minimally Invasive Surgery (RMIS), accurate tool localization is crucial to ensure patient safety and successful task execution. However, this remains challenging for cable-driven robots, such as the da Vinci robot, because erroneous encoder readings lead to pose estimation errors. In this study, we propose a calibration framework to produce accurate tool localization results through computing the hand-eye transformation matrix on-the-fly. The framework consists of two interrelated algorithms: the feature association block and the hand-eye calibration block, which provide robust correspondences for key points detected on monocular images without pre-training, and offer the versatility to accommodate various surgical scenarios by adopting an array of filter approaches, respectively. To validate its efficacy, we test the framework extensively on publicly available video datasets that feature multiple surgical instruments conducting tasks in both\nin vitro\nand\nex vivo\nscenarios, under varying illumination conditions and with different levels of key point measurement accuracy. The results show a significant reduction in tool localization errors under the proposed calibration framework, with accuracies comparable to other state-of-the-art methods while being more time-efficient.\nI\nINTRODUCTION\nHand-eye calibration is a well-established research topic in the field of robotics: it aims to find the homogeneous transformation matrix between the camera and robot base frame. When these two frames are closely aligned, the robot can accurately command its end-effector to a given pose indicated in the camera frame. Specifically, for medical robots, it is a common practice to incorporate a vision system, whether monocular or stereoscopic, allowing surgeons to react promptly during surgery according to real-time camera feedback. For a tele-operated surgical platform, such as the da Vinci robot (Intuitive Surgical Inc, Sunnyvale, California), surgeons operate on the surgeon-side console, observing patient-side instruments that mimic their hand movements, from the view of a stereo endoscopic camera. This calibration is usually conducted using the built-in set-up joints controllers (SUJs), both pre-operatively and intra-operatively, to ensure instrument movements are visually aligned with surgeonsâ€™ hand movements, contributing towards an intuitive and ergonomic operation. However, it was reported\n[\n7\n]\nthat, the calibration result provided by SUJs may introduce errors that impair instrumentâ€™s tip positioning accuracy. Although humans can automatically compensate for these errors during tele-operation, in cases where surgical subtasks automation, or human-robot interactions, are required, it is imperative to resolve these errors so that the instrument can reach the target position.\nIn addition to initial hand-eye calibration errors, tip positioning accuracy further suffers from inaccurate joint angle readings from the encoder, induced by the cable-driven mechanism\n[\n6\n]\n, which is intrinsic to the da Vinci Classic, and da Vinci Si systems. These encoder reading errors are time-varying, because of the wear and tear in cables and increasing non-linearities in pulley-cable friction. Richter\net al.\n[\n29\n]\nproposed a unified approach of lumping encoder reading errors into a complementary homogeneous transformation matrix, such that accurate tip positioning can be achieved through consistent estimations of this matrix. Alternatively, Hwang\net al.\n[\n15\n]\nturned to direct calibrations on encoder readings using a pre-trained neural network model. Their training-based approach relies on collecting ground truth joint angles using a bespoke marker, which can be time-consuming\nper se\n. Additionally, a model trained on one instrument may not be applicable to another instrument, even of the same type, due to variations in transmission kinematics degeneracy sustained by each instrument. Considering the large variety of surgical instruments and their disposable nature, in this study, we posit that on-the-fly hand-eye calibration approach offers better generalization for improving instrument positioning accuracy.\nI-A\nContributions\nIn this work, we aim to provide a generalizable on-the-fly hand-eye calibration framework for a wide range of surgical platforms that is also practicable for adoption in a real clinical monocular setup. To this end, we have made the following contributions\n1)\nA training-free key point association algorithm that leverages analytical Jacobian matrices and is easily generalizable for different surgical instruments for which CAD models are available.\n2)\nA visibility check algorithm that accelerates the data association process by promptly removing invisible key points from the candidate list.\n3)\nA pool of hand-eye calibration methods that adopt filter-based approaches to allow for different noise distributions that arise in various surgical scenarios.\nIt should be noted that, to make the framework more generalizable, key point detection methods are outside the scope of this study. It is assumed that either traditional or learning-based methods can be adopted. Although they may result in different levels of measurement accuracy, this discrepancy is taken into account during data analysis. It is reported that the most recent da Vinci V robot is already equipped with digital encoders, which have greatly improved positioning accuracy. Nevertheless, addressing this problem remains important for researchers who use the first-generation da Vinci research kit (dVRK)\n[\n16\n]\nas a testing platform. Even in the future, when researchers migrate to the dVRK Si, similar positioning inaccuracies are likely to persist. Additionally, recent years have seen an increasing number of similar cable-driven surgical robots enter the market, and hence the development of a generalizable methodology holds greater commercial potential.\nI-B\nRelated works\nI-B\n1\nClassic approach\nThe classic way to address the hand-eye calibration problem is to adopt an\nAX=XB\n[\n32\n]\n, or\nAX=YB\n[\n41\n]\nformulation, where\nX\n,\nY\nare the unknown matrices, and\nA\n,\nB\nare measured by an external sensing device. The overarching goal is to estimate\nX\n, the hand-eye transformation. Different mathematical representations can be leveraged to simplify the original equation by reducing the number of unknown parameters. These representations include dual-quaternions\n[\n8\n]\n, Lie groups\n[\n25\n]\n, screw motions\n[\n39\n]\n, Kronecker operator\n[\n31\n]\n, among others, which implicitly determine whether the rotation and translation components are estimated sequentially or simultaneously.\nThese hand-eye calibration solvers can be further divided based on whether an analytical or numerical approach is adopted. Zhang\net al.\n[\n38\n]\nproposed a computationally efficient approach that iteratively estimates the rotation and translation components, represented by dual-quaternions, until a convergence criterion is met. Pachtrachai\net al.\n[\n27\n]\nalso proposed an algorithm that alternately estimates rotation and translation using twist motions to achieve higher accuracy. Most methods in this category, however, require an external calibration object to provide\nA\nand\nB\nmeasurements. These objects, whether directly attached to the robot\n[\n26\n]\n, or wrapped around the tool shaft\n[\n5\n]\n, can be difficult to be adopted in a real clinical setup. To overcome this limitation, recent research has focused on marker-free hand-eye calibration approaches\n[\n28\n,\n40\n]\n, where the instrument and the robot themselves serve as calibration objects, and\nA\nand\nB\nare calculated through tool localization and pose estimation. Consequently, the accuracy of these methods may degrade significantly when the pose estimation accuracy is poor.\nI-B\n2\nEstimator-based approach\nSince encoder readings and forward kinematics already exist for most surgical robots, research has also turned to direct calibration methods via estimators that fuse vision and kinematics data. The advantage of such a hybrid approach is that it provides more consistent pose estimation results, especially in the presence of tool articulations and occlusions, as stated in\n[\n1\n]\n. Most estimator-based methods differ in the selection of statistical estimators and visual features incorporated in the estimation framework. Ye\net al.\n[\n37\n]\nfirst proposed an Extended Kalman Filter (EKF) framework where key point features obtained via template matching were leveraged to calibrate\nX\n. Similarly, Moccia\net al.\n[\n24\n]\ndeveloped an EKF framework that utilized the 3D instrument tip position reconstructed from stereo image triangulation. Lu\net al.\n[\n22\n]\nthen proposed a Particle Filter (PF) framework that incorporated key points detected via a pre-trained deep neural network.\nAlthough key points are arguably the most accessible features on the image plane, most data-driven key points-based methods require labor-intensive human labeling. Richter\net al.\n[\n29\n]\nfurther incorporated tool edge features into the PF framework, and Liang\net al.\n[\n20\n]\ndemonstrated the importance of edge features. However, obtaining instrument edge features is challenging in real clinical scenarios. First, the endoscopic camera most often focuses closely on the instrument tip part. Second, the most common way to extract edge features is via obtaining instrument masks followed by selecting the longest line using the Canny edge detector\n[\n4\n]\n, which is both time-consuming and noise-sensitive. Furthermore, some segmentation algorithms, such as the Segment Anything Model\n[\n17\n]\n, which was adopted in\n[\n19\n]\n, falls short of accuracy for surgical applications\n[\n13\n]\n.\nAdditionally, instrument silhouettes can also serve as features to assist pose estimation. Hao\net al.\n[\n12\n]\nproposed a PF framework where virtually rendered instrument silhouettes were compared with tool segmentation results to select particles assigned with accurate parameter estimations. They reported that expensive tool-rendering process limited the framework to 0.3 Hz, making it impractical for real-time applications without relying on external acceleration resources.\nI-B\n3\nOthers\nMore data-driven approaches have also emerged in recent years, in which a pre-trained model is used to produce end-to-end estimates of\nX\nand the joint angles. Liang\net al.\n[\n19\n]\nproposed a differentiable rendering-based method that iteratively calculates the optimal candidate by aligning rendered instrument silhouettes with reference masks, given a reliable initialization. Yang\net al.\n[\n35\n]\ndeveloped a render-and-compare pose estimation framework that incorporates Gaussian Splatting to cater for a monocular setup. Fan\net al.\n[\n9\n]\nproposed a Reinforcement Learning-based method where an agent is trained to align the pixel projections of key points on a virtual instrument with the corresponding observations in the image plane. Although promising results were achieved, due to the specific shape of the instrument, approaches based on silhouette rendering often encounter inaccuracies under poor initializations or when trapped in local minima.\nFigure 1\n:\nFramework Overview\nII\nMethods\nFigure 2\n:\nIllustration of key points on the instrument.\nFigure 3\n:\nIllustration of key point projection onto the image plane. The top and bottom images represent situations in which the top and right sides of the instrument are directly facing the camera, respectively.\nI\n1\n,\nI\n1\nâ€²\n,\nI\nc\nâ€‹\n1\n\\texttt{{I}}_{1},\\texttt{{I}}^{\\prime}_{1},\\texttt{{I}}_{c1}\nrepresent the edges and central line of the â€œroll-segmentâ€ cylinder projection, and\nI\n2\n,\nI\n2\nâ€²\n,\nI\nc\nâ€‹\n2\n\\texttt{{I}}_{2},\\texttt{{I}}^{\\prime}_{2},\\texttt{{I}}_{c2}\nrepresent those that of the â€œpitch-segmentâ€ cylinder projection.\nFigure 4\n:\nKey points association illustration. Red points represent key point projections via the hand-eye transformation matrix, and yellow points represent features detected on the image plane including outliers. Arrows represent established associations.\nII-A\nFramework overview\nThe overall framework is illustrated in Fig.\n1\n. Before operation, key points are defined on the instrument CAD model, as illustrated in Fig.\n2\nand Fig.\n3\n. During operation, key points in monocular images are detected and processed to be paired with their labels through the visibility check and JCBB algorithm blocks (Subsections.\nII-C\n,\nII-D\n, and\nII-E\n). Key points association is illustrated in Fig.\n4\n. These paired correspondences are subsequently sent to the state estimation block (Subsection.\nII-F\n) to obtain the calibrated hand-eye transformation matrix.\nII-B\nPreliminaries\nII-B\n1\nCoordinates transformation\nAltogether, there exist 6 parameters that consist of an additional homogeneous\n4\nÃ—\n4\n4\\times 4\ntransformation matrix\nT\nr\nr\nâ€²\n\\text{{T}}_{r}^{r^{\\prime}}\n.\nx\nr\nr\nâ€²\n=\n[\n(\nğš¯\nr\nr\nâ€²\n)\nT\n,\n(\nt\nr\nr\nâ€²\n)\nT\n]\nT\n\\texttt{{x}}_{r}^{r^{\\prime}}=[(\\mathbf{\\Theta}_{r}^{r^{\\prime}})^{\\textmd{T}},{(\\texttt{{t}}_{r}^{r^{\\prime}})}^{\\textmd{T}}]^{\\textmd{T}}\n, stores these unknown parameters.\nğš¯\nr\nr\nâ€²\n=\n[\nÎ±\n,\nÎ²\n,\nÎ³\n]\nT\n\\mathbf{\\Theta}_{r}^{r^{\\prime}}=[\\alpha,\\beta,\\gamma]^{\\textmd{T}}\n, represents\nZ-Y-X\nEuler angles, and\nt\nr\nr\nâ€²\n=\n[\nx\nr\nr\nâ€²\n,\ny\nr\nr\nâ€²\n,\nz\nr\nr\nâ€²\n]\nT\n\\texttt{{t}}^{r^{\\prime}}_{r}=[x^{r^{\\prime}}_{r},y^{r^{\\prime}}_{r},z^{r^{\\prime}}_{r}]^{\\textmd{T}}\n, represents translational components.\nT\nc\nr\nâ€²\n\\text{{T}}_{c}^{r^{\\prime}}\ncan be mathematically expressed using\nx\nr\nr\nâ€²\n\\texttt{{x}}_{r}^{r^{\\prime}}\n, in eq.\n1\n.\nT\nr\nr\nâ€²\n=\n[\nR\nr\nr\nâ€²\nâ€‹\n(\nÎ±\n,\nÎ²\n,\nÎ³\n)\nt\nr\nr\nâ€²\n0\n1\nÃ—\n3\n1\n]\n\\text{{T}}_{r}^{r^{\\prime}}=\\begin{bmatrix}\\text{{R}}^{r^{\\prime}}_{r}(\\alpha,\\beta,\\gamma)&\\texttt{{t}}^{r^{\\prime}}_{r}\\\\\n\\texttt{{0}}_{1\\times 3}&1\\end{bmatrix}\n(1)\nSpecifically,\nR\nr\nr\nâ€²\nâ€‹\n(\nÎ±\n,\nÎ²\n,\nÎ³\n)\n\\text{{R}}_{r}^{r^{\\prime}}(\\alpha,\\beta,\\gamma)\ncan be expanded into three sequential rotations around their individual axes, in eq.\n2\n, where\nc\nand\ns\nrepresent\ncos\nand\nsin\noperators, respectively.\nR\nr\nr\nâ€²\nâ€‹\n(\nÎ±\n,\nÎ²\n,\nÎ³\n)\n=\nR\nz\nâ€‹\n(\nÎ±\n)\nâ€‹\nR\ny\nâ€‹\n(\nÎ²\n)\nâ€‹\nR\nx\nâ€‹\n(\nÎ³\n)\n=\n[\nc\nâ€‹\nÎ±\nâ€‹\nc\nâ€‹\nÎ²\nc\nâ€‹\nÎ±\nâ€‹\ns\nâ€‹\nÎ²\nâ€‹\ns\nâ€‹\nÎ³\nâˆ’\ns\nâ€‹\nÎ±\nâ€‹\nc\nâ€‹\nÎ³\nc\nâ€‹\nÎ±\nâ€‹\ns\nâ€‹\nÎ²\nâ€‹\nc\nâ€‹\nÎ³\n+\ns\nâ€‹\nÎ±\nâ€‹\ns\nâ€‹\nÎ³\ns\nâ€‹\nÎ±\nâ€‹\nc\nâ€‹\nÎ²\ns\nâ€‹\nÎ±\nâ€‹\ns\nâ€‹\nÎ²\nâ€‹\ns\nâ€‹\nÎ³\n+\nc\nâ€‹\nÎ±\nâ€‹\nc\nâ€‹\nÎ³\ns\nâ€‹\nÎ±\nâ€‹\ns\nâ€‹\nÎ²\nâ€‹\nc\nâ€‹\nÎ³\nâˆ’\nc\nâ€‹\nÎ±\nâ€‹\ns\nâ€‹\nÎ³\nâˆ’\ns\nâ€‹\nÎ²\nc\nâ€‹\nÎ²\nâ€‹\ns\nâ€‹\nÎ³\nc\nâ€‹\nÎ²\nâ€‹\nc\nâ€‹\nÎ³\n]\n\\begin{split}&\\text{{R}}^{r^{\\prime}}_{r}(\\alpha,\\beta,\\gamma)=\\text{{R}}_{z}(\\alpha)\\text{{R}}_{y}(\\beta)\\text{{R}}_{x}(\\gamma)=\\\\\n&\\begin{bmatrix}\\textmd{c}\\alpha\\textmd{c}\\beta&\\textmd{c}\\alpha\\textmd{s}\\beta\\textmd{s}\\gamma-\\textmd{s}\\alpha\\textmd{c}\\gamma&\\textmd{c}\\alpha\\textmd{s}\\beta\\textmd{c}\\gamma+\\textmd{s}\\alpha\\textmd{s}\\gamma\\\\\n\\textmd{s}\\alpha\\textmd{c}\\beta&\\textmd{s}\\alpha\\textmd{s}\\beta\\textmd{s}\\gamma+\\textmd{c}\\alpha\\textmd{c}\\gamma&\\textmd{s}\\alpha\\textmd{s}\\beta\\textmd{c}\\gamma-\\textmd{c}\\alpha\\textmd{s}\\gamma\\\\\n-\\textmd{s}\\beta&\\textmd{c}\\beta\\textmd{s}\\gamma&\\textmd{c}\\beta\\textmd{c}\\gamma\\end{bmatrix}\\end{split}\n(2)\nThe initial hand-eye calibration matrix is represented as\nT\nr\nâ€²\nc\n\\text{{T}}_{r^{\\prime}}^{c}\n, in eq.\n3\n.\nT\nr\nâ€²\nc\n=\n[\nR\nr\nâ€²\nc\nt\nr\nâ€²\nc\n0\n1\nÃ—\n3\n1\n]\n\\text{{T}}_{r^{\\prime}}^{c}=\\begin{bmatrix}\\text{{R}}^{c}_{r^{\\prime}}&\\texttt{{t}}^{c}_{r^{\\prime}}\\\\\n\\texttt{{0}}_{1\\times 3}&1\\end{bmatrix}\n(3)\np\nis a key point located relative to the\nj\nk\nt\nâ€‹\nh\nj_{k}^{th}\nPSM joint, with its local position\nP\nj\nk\n=\n[\nx\nj\nk\n,\ny\nj\nk\n,\nz\nj\nk\n,\n1\n]\nT\n\\texttt{{P}}^{j_{k}}=[x^{j_{k}},y^{j_{k}},z^{j_{k}},1]^{\\textmd{T}}\n.\np\nc\n=\n[\n(\nt\nc\n)\nT\n,\n1\n]\nT\n=\n[\nx\nc\n,\ny\nc\n,\nz\nc\n,\n1\n]\nT\n\\texttt{{p}}^{c}=[(\\texttt{{t}}^{c})^{\\textmd{T}},1]^{\\textmd{T}}=[x^{c},y^{c},z^{c},1]^{\\textmd{T}}\n, and\np\nr\n=\n[\n(\nt\nr\n)\nT\n,\n1\n]\nT\n=\n[\nx\nr\n,\ny\nr\n,\nz\nr\n,\n1\n]\nT\n\\texttt{{p}}^{r}=[(\\texttt{{t}}^{r})^{\\textmd{T}},1]^{\\textmd{T}}=[x^{r},y^{r},z^{r},1]^{\\textmd{T}}\n, represent its homogeneous 3D positions in camera frame and PSM base frame, respectively. Since transformation\nT\nj\nk\nr\n\\text{{T}}_{j_{k}}^{r}\ncan be directly calculated from PSM encoder readings and Denavit-Hartenberg(DH) parameters, we have eq.\n4\n.\nP\nc\n=\nT\nr\nâ€²\nc\nâ€‹\nT\nr\nr\nâ€²\nâ€‹\nT\nj\nk\nr\nâ€‹\nP\nj\nk\n=\nT\nr\nâ€²\nc\nâ€‹\nT\nr\nr\nâ€²\nâ€‹\nP\nr\n=\n[\nR\nr\nâ€²\nc\nt\nr\nâ€²\nc\n0\n1\nÃ—\n3\n1\n]\nâ€‹\n[\nR\nr\nr\nâ€²\nâ€‹\n(\nÎ±\n,\nÎ²\n,\nÎ³\n)\nt\nr\nr\nâ€²\n0\n1\nÃ—\n3\n1\n]\nâ€‹\nP\nr\n=\n[\nR\nr\nâ€²\nc\nâ€‹\nR\nr\nr\nâ€²\nâ€‹\n(\nÎ±\n,\nÎ²\n,\nÎ³\n)\nâ€‹\nt\nr\n+\nR\nr\nâ€²\nc\nâ€‹\nt\nr\nr\nâ€²\n+\nt\nr\nâ€²\nc\n1\n]\n\\begin{split}\\texttt{{P}}^{c}&=\\text{{T}}^{c}_{r^{\\prime}}\\text{{T}}^{r^{\\prime}}_{r}\\text{{T}}^{r}_{j_{k}}\\texttt{{P}}^{j_{k}}=\\text{{T}}^{c}_{r^{\\prime}}\\text{{T}}^{r^{\\prime}}_{r}\\texttt{{P}}^{r}\\\\\n&=\\begin{bmatrix}\\text{{R}}^{c}_{r^{\\prime}}&\\texttt{{t}}^{c}_{r^{\\prime}}\\\\\n\\texttt{{0}}_{1\\times 3}&1\\end{bmatrix}\\begin{bmatrix}\\text{{R}}^{r^{\\prime}}_{r}(\\alpha,\\beta,\\gamma)&\\texttt{{t}}^{r^{\\prime}}_{r}\\\\\n\\texttt{{0}}_{1\\times 3}&1\\\\\n\\end{bmatrix}\\texttt{{P}}^{r}\\\\\n&=\\begin{bmatrix}\\text{{R}}^{c}_{r^{\\prime}}\\text{{R}}^{r^{\\prime}}_{r}(\\alpha,\\beta,\\gamma)\\texttt{{t}}^{r}+\\text{{R}}^{c}_{r^{\\prime}}\\texttt{{t}}^{r^{\\prime}}_{r}+\\texttt{{t}}^{c}_{r^{\\prime}}\\\\\n1\\end{bmatrix}\\end{split}\n(4)\nIn reality, it is the pixel projection of\np\nc\n\\texttt{{p}}^{c}\non the image plane that serves as observation, represented as\ny\n=\n[\nu\n,\nv\n]\nT\n\\texttt{{y}}=[u,v]^{\\textmd{T}}\n. When no camera distortions are present, we have eq.\n5\n.\n[\nu\nv\n1\n]\n=\n[\nf\nx\n0\nc\nx\n0\nf\ny\nc\ny\n0\n0\n1\n]\nâ€‹\n[\nx\nc\nz\nc\ny\nc\nz\nc\n1\n]\n\\begin{bmatrix}u\\\\\nv\\\\\n1\\end{bmatrix}=\\begin{bmatrix}f_{x}&0&c_{x}\\\\\n0&f_{y}&c_{y}\\\\\n0&0&1\\end{bmatrix}\\begin{bmatrix}\\frac{x^{c}}{z^{c}}\\\\\n\\frac{y^{c}}{z^{c}}\\\\\n1\\end{bmatrix}\n(5)\nII-B\n2\nJacobian calculations\nHaving established the relationship between visual measurements and unknown parameter inputs, here we derive essential Jacobian matrices that are required by subsequent procedures, from eq.\n4\nand\n5\n.\nWe first derive the Jacobian matrix\nH\nc\n\\text{{H}}^{c}\n, which operates between\np\nc\n\\texttt{{p}}^{c}\nand\nx\nr\nr\nâ€²\n\\texttt{{x}}^{r^{\\prime}}_{r}\n.\nH\nc\n\\text{{H}}^{c}\nconsists of partial differentiations over both rotational component\nğš¯\nr\nr\nâ€²\n\\mathbf{\\Theta}^{r^{\\prime}}_{r}\nand translational component\nt\nr\nr\nâ€²\n\\texttt{{t}}^{r^{\\prime}}_{r}\n, summarized in\neq.\n6\n. Since the relationship between\np\nc\n\\texttt{{p}}^{c}\nand\nt\nr\nr\nâ€²\n\\texttt{{t}}^{r^{\\prime}}_{r}\nis linear, we have eq.\n7\n.\nH\nc\n=\n[\nH\nğš¯\nc\nH\nt\nc\n]\n3\nÃ—\n6\n,\nH\nğš¯\ni\nâ€‹\nj\nc\n=\nâˆ‚\nP\ni\nc\nâˆ‚\nğš¯\nj\n,\nH\nt\ni\nâ€‹\nj\nc\n=\nâˆ‚\nP\ni\nc\nâˆ‚\nt\nj\n\\text{{H}}^{c}=\\begin{bmatrix}\\text{{H}}^{c}_{\\mathbf{\\Theta}}&\\text{{H}}^{c}_{\\texttt{{t}}}\\end{bmatrix}_{3\\times 6},\\text{{H}}^{c}_{{\\mathbf{\\Theta}}_{ij}}=\\frac{\\partial{\\texttt{{P}}^{c}_{i}}}{\\partial{\\mathbf{\\Theta}_{j}}},\\text{{H}}^{c}_{{\\texttt{{t}}}_{ij}}=\\frac{\\partial{\\texttt{{P}}^{c}_{i}}}{\\partial{\\texttt{{t}}_{j}}}\n(6)\nH\nt\nc\n=\nR\nr\nâ€²\nc\n\\text{{H}}^{c}_{\\text{{t}}}=\\text{{R}}^{c}_{r^{\\prime}}\n(7)\nSubsequently, we derive each column of\nH\nğš¯\nc\n\\text{{H}}^{c}_{\\mathbf{{\\Theta}}}\nbased on linear algebra through eq.\n8\nto\n11\n.\nR\nt\nâ€‹\nm\nâ€‹\np\nÎ±\n=\nR\nr\nâ€²\nc\nb\nt\nâ€‹\nm\nâ€‹\np\nÎ±\n=\nR\ny\nâ€‹\n(\nÎ²\n)\nâ€‹\nR\nx\nâ€‹\n(\nÎ³\n)\nâ€‹\nt\nr\n=\n[\nb\nx\n,\nb\ny\n,\nb\nz\n]\nT\nc\nt\nâ€‹\nm\nâ€‹\np\nÎ±\n=\n[\nb\nx\nsin\n(\nÎ±\n)\nâˆ’\nb\ny\ncos\n(\nÎ±\n)\n,\nb\nx\ncos\n(\nÎ±\n)\nâˆ’\nb\ny\nsin\n(\nÎ±\n)\n,\n0\n]\nT\n\\begin{split}\\text{{R}}_{tmp}^{\\alpha}&=\\text{{R}}^{c}_{r^{\\prime}}\\\\\n\\texttt{{b}}_{tmp}^{\\alpha}&=\\text{{R}}_{y}(\\beta)\\text{{R}}_{x}(\\gamma)\\texttt{{t}}_{r}=[b_{x},b_{y},b_{z}]^{\\textmd{T}}\\\\\n\\texttt{{c}}_{tmp}^{\\alpha}=[b_{x}\\textmd{sin}(\\alpha)-b_{y}&\\textmd{cos}(\\alpha),b_{x}\\textmd{cos}(\\alpha)-b_{y}\\textmd{sin}(\\alpha),0]^{\\textmd{T}}\\\\\n\\end{split}\n(8)\nR\nt\nâ€‹\nm\nâ€‹\np\nÎ²\n=\nR\nr\nâ€²\nc\nâ€‹\nR\nz\nâ€‹\n(\nÎ±\n)\nb\nt\nâ€‹\nm\nâ€‹\np\nÎ²\n=\nR\nx\nâ€‹\n(\nÎ³\n)\nâ€‹\nt\nr\n=\n[\nb\nx\n,\nb\ny\n,\nb\nz\n]\nT\nc\nt\nâ€‹\nm\nâ€‹\np\nÎ²\n=\n[\nâˆ’\nb\nx\nsin\n(\nÎ²\n)\n+\nb\nz\ncos\n(\nÎ²\n)\n,\n0\n,\nâˆ’\nb\nx\ncos\n(\nÎ²\n)\nâˆ’\nb\nz\nsin\n(\nÎ²\n)\n]\nT\n\\begin{split}\\text{{R}}_{tmp}^{\\beta}&=\\text{{R}}^{c}_{r^{\\prime}}\\text{{R}}_{z}(\\alpha)\\\\\n\\texttt{{b}}_{tmp}^{\\beta}&=\\text{{R}}_{x}(\\gamma)\\texttt{{t}}_{r}=[b_{x},b_{y},b_{z}]^{\\textmd{T}}\\\\\n\\texttt{{c}}_{tmp}^{\\beta}=[-b_{x}\\textmd{sin}(\\beta)+b_{z}&\\textmd{cos}(\\beta),0,-b_{x}\\textmd{cos}(\\beta)-b_{z}\\textmd{sin}(\\beta)]^{\\textmd{T}}\\\\\n\\end{split}\n(9)\nR\nt\nâ€‹\nm\nâ€‹\np\nÎ³\n=\nR\nr\nâ€²\nc\nâ€‹\nR\nz\nâ€‹\n(\nÎ±\n)\nâ€‹\nR\ny\nâ€‹\n(\nÎ²\n)\nb\nt\nâ€‹\nm\nâ€‹\np\nÎ³\n=\nt\nr\n=\n[\nb\nx\n,\nb\ny\n,\nb\nz\n]\nT\nc\nt\nâ€‹\nm\nâ€‹\np\nÎ³\n=\n[\n0\n,\nâˆ’\nb\ny\nsin\n(\nÎ³\n)\nâˆ’\nb\nz\ncos\n(\nÎ³\n)\n,\nb\ny\ncos\n(\nÎ³\n)\nâˆ’\nb\nz\nsin\n(\nÎ³\n)\n]\nT\n\\begin{split}\\text{{R}}_{tmp}^{\\gamma}&=\\text{{R}}^{c}_{r^{\\prime}}\\text{{R}}_{z}(\\alpha)\\text{{R}}_{y}(\\beta)\\\\\n\\texttt{{b}}_{tmp}^{\\gamma}&=\\texttt{{t}}_{r}=[b_{x},b_{y},b_{z}]^{\\textmd{T}}\\\\\n\\texttt{{c}}_{tmp}^{\\gamma}=[0,-b_{y}\\textmd{sin}(\\gamma)-&b_{z}\\textmd{cos}(\\gamma),b_{y}\\textmd{cos}(\\gamma)-b_{z}\\textmd{sin}(\\gamma)]^{\\textmd{T}}\\\\\n\\end{split}\n(10)\nH\nğš¯\nc\n=\n[\nR\nt\nâ€‹\nm\nâ€‹\np\nÎ±\nâ€‹\nc\nt\nâ€‹\nm\nâ€‹\np\nÎ±\n,\nR\nt\nâ€‹\nm\nâ€‹\np\nÎ²\nâ€‹\nc\nt\nâ€‹\nm\nâ€‹\np\nÎ²\n,\nR\nt\nâ€‹\nm\nâ€‹\np\nÎ³\nâ€‹\nc\nt\nâ€‹\nm\nâ€‹\np\nÎ³\n]\n\\text{{H}}^{c}_{\\mathbf{\\Theta}}=\\begin{bmatrix}\\text{{R}}_{tmp}^{\\alpha}\\texttt{{c}}_{tmp}^{\\alpha},&\\text{{R}}_{tmp}^{\\beta}\\texttt{{c}}_{tmp}^{\\beta},&\\text{{R}}_{tmp}^{\\gamma}\\texttt{{c}}_{tmp}^{\\gamma}\\end{bmatrix}\n(11)\nThen we derive the the intermediary Jacobian matrix\nH\nc\no\nâ€‹\nb\nâ€‹\ns\n\\text{{H}}^{obs}_{c}\n, which operates between observation\ny\nand\np\nc\n\\texttt{{p}}^{c}\n. Taking derivatives from eq.\n5\n, we have eq.\n12\n.\nH\nc\no\nâ€‹\nb\nâ€‹\ns\n=\n[\nf\nx\nz\nc\n0\nâˆ’\nf\nx\nâ€‹\nx\nc\nz\nc\nâ‹…\nz\nc\n0\ny\nc\nz\nc\nâˆ’\nf\ny\nâ€‹\ny\nc\nz\nc\nâ‹…\nz\nc\n]\n\\text{{H}}^{obs}_{c}=\\begin{bmatrix}\\frac{f_{x}}{z^{c}}&0&-f_{x}\\frac{x^{c}}{{z^{c}}\\cdot{z^{c}}}\\\\\n0&\\frac{y^{c}}{z^{c}}&-f_{y}\\frac{y^{c}}{{z^{c}}\\cdot{z^{c}}}\\end{bmatrix}\n(12)\nFinally, we have the overall Jacobian matrix\nH\n2\nÃ—\n6\n\\text{{H}}_{2\\times 6}\n, which operates between pixel observation and\nx\nr\nr\nâ€²\n\\texttt{{x}}^{r^{\\prime}}_{r}\n, in eq.\n13\n.\nH\n=\nH\nc\no\nâ€‹\nb\nâ€‹\ns\nâ€‹\nH\nc\n\\text{{H}}=\\text{{H}}^{obs}_{c}\\text{{H}}^{c}\n(13)\nTo summarize, for each key point\np\n, as long as we have the current parameter estimation\nx\nr\nr\nâ€²\n\\texttt{{x}}_{r}^{r^{\\prime}}\n, its pixel observation\ny\n, and its 3D position in PSM base frame\np\nr\n\\texttt{{p}}^{r}\n, we can calculate its Jacobian matrix\nH\nâ€‹\n(\nx\nr\nr\nâ€²\n,\ny\n)\n\\text{{H}}(\\texttt{{x}}_{r}^{r^{\\prime}},\\texttt{{y}})\n, which is essential to the following key points association procedure. To simplify annotations, we use\nx\nfor\nx\nr\nr\nâ€²\n\\texttt{{x}}_{r}^{r^{\\prime}}\nfor the rest of the paper.\nII-C\nJCBB data association\nII-C\n1\nIndividual Compatibility test\nUnder the Markov assumption\n[\n14\n]\n, unknown time-variant parameter\nx\nt\n\\texttt{{x}}^{t}\nevolves according to eq.(\n14\n), where\np\np\nrepresents state forward function,\nu\nt\n\\texttt{{u}}^{t}\nrepresents current input, and\ne\nt\n\\texttt{{e}}^{t}\nrepresents Gaussian input noises. In eq.(\n15\n),\ny\nt\n\\texttt{{y}}^{t}\nrepresents current measurement,\ng\ng\nrepresents mapping function, and\nv\nt\n\\texttt{{v}}^{t}\nrepresents Gaussian measurement noises.\nx\n^\nt\n=\np\nâ€‹\n(\nx\n^\nt\nâˆ’\n1\n,\nu\nt\n)\n+\ne\nt\n,\n\\displaystyle\\hat{\\texttt{{x}}}^{t}=p(\\hat{\\texttt{{x}}}^{t-1},\\texttt{{u}}^{t})+\\texttt{{e}}^{t},\nwhere\ne\nt\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nğšº\nğ\n)\n\\displaystyle\\text{where }\\texttt{{e}}^{t}\\sim\\mathcal{N}(\\texttt{{0}},\\mathbf{\\Sigma_{e}})\n(14)\ny\n^\nt\n=\ng\nâ€‹\n(\nx\n^\nt\n)\n+\nv\nt\n,\n\\displaystyle\\hat{\\texttt{{y}}}^{t}=g(\\hat{\\texttt{{x}}}^{t})+\\texttt{{v}}^{t},\nwhere\nv\nt\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nğšº\nğ¯\n)\n\\displaystyle\\text{where }\\texttt{{v}}^{t}\\sim\\mathcal{N}(\\texttt{{0}},\\mathbf{\\Sigma_{v}})\n(15)\nAt time\nt\nt\n, there are\nm\nm\nkey point observations in the measurement list\nğ’«\no\nâ€‹\nb\nâ€‹\ns\nt\n=\n{\n[\nu\n1\nt\n,\nv\n1\nt\n]\nT\nâ€‹\nâ€¦\nâ€‹\n[\nu\nm\nt\n,\nv\nm\nt\n]\nT\n}\n{\\mathcal{P}^{t}_{obs}}=\\{[u_{1}^{t},v_{1}^{t}]^{\\textmd{T}}...[u_{m}^{t},v_{m}^{t}]^{\\textmd{T}}\\}\n, and\nn\nn\npredicted key point features in the prediction list\nğ’«\np\nâ€‹\nr\nâ€‹\ne\nt\n=\n{\n[\nu\n^\n1\nt\n,\nv\n^\n1\nt\n]\nT\nâ€‹\nâ€¦\nâ€‹\n[\nu\n^\nn\nt\n,\nv\n^\nn\nt\n]\nT\n}\n{\\mathcal{P}^{t}_{pre}}=\\{[\\hat{u}_{1}^{t},\\hat{v}^{t}_{1}]^{\\textmd{T}}...[\\hat{u}^{t}_{n},\\hat{v}^{t}_{n}]^{\\textmd{T}}\\}\n. For simplicity, superscript\nt\nt\nis omitted in the rest of this subsection.\nâ„‹\ni\nâ€‹\nj\n\\mathcal{H}_{ij}\nrepresents the hypothesis that the\ni\nth\ni^{\\text{th}}\nobservation,\nğ’«\no\nâ€‹\nb\nâ€‹\ns\n,\ni\n\\mathcal{P}_{obs,i}\n, is associated with the\nj\nth\nj^{\\text{th}}\nkey point prediction,\nğ’«\np\nâ€‹\nr\nâ€‹\ne\n,\nj\n{\\mathcal{P}_{pre,j}}\n.\nâ„‹\ni\nâ€‹\n0\n\\mathcal{H}_{i0}\nindicates that no key point prediction matches, implying that the\ni\nt\nâ€‹\nh\ni^{th}\nobservation is an outlier and that\nâ„‹\ni\nâ€‹\n0\n\\mathcal{H}_{i0}\nis a trivial hypothesis. We now evaluate the likelihood of\nâ„‹\ni\nâ€‹\nj\n\\mathcal{H}_{ij}\nvia individual compatibility test. The innovation for the presumed association is defined as\nh\ni\nâ€‹\nj\n=\nğ’«\no\nâ€‹\nb\nâ€‹\ns\n,\ni\nâˆ’\nğ’«\np\nâ€‹\nr\nâ€‹\ne\n,\nj\n\\texttt{{h}}_{ij}=\\mathcal{P}_{obs,i}-\\mathcal{P}_{pre,j}\n. We define\nx\n^\n\\hat{\\texttt{{x}}}\nas the current state estimation, and\nx\nas the ground truth of\nx\n^\n\\hat{\\texttt{{x}}}\n. Similarly, we define\ny\n^\n\\hat{\\texttt{{y}}}\nas current observation, conditioned on hypothesis\nâ„‹\ni\nâ€‹\nj\n\\mathcal{H}_{ij}\n, and\ny\nas the ground truth.\nWe then define an implicit measurement function\nf\nâ€‹\n(\nx\n,\ny\n)\n=\ny\nâˆ’\ng\nâ€‹\n(\nx\n)\n=\n0\nf(\\texttt{{x}},\\texttt{{y}})=\\texttt{{y}}-g(\\texttt{{x}})=\\texttt{{0}}\n, for hypothesis\nâ„‹\ni\nâ€‹\nj\n\\mathcal{H}_{ij}\n. After applying the first-order Taylor expansion and conducting linearization at\n(\nx\n^\n,\ny\n^\n)\n(\\hat{\\texttt{{x}}},\\hat{\\texttt{{y}}})\n, we have eq.(\n16\n).\nf\nâ€‹\n(\nx\n,\ny\n)\nâ‰ˆ\ny\n^\nâˆ’\ng\nâ€‹\n(\nx\n^\n)\n+\nâˆ‚\nf\nâ€‹\n(\nx\n,\ny\n)\nâˆ‚\nx\nâ€‹\n(\nx\nâˆ’\nx\n^\n)\n+\nâˆ‚\nf\nâ€‹\n(\nx\n,\ny\n)\nâˆ‚\ny\nâ€‹\n(\ny\nâˆ’\ny\n^\n)\n=\nh\ni\nâ€‹\nj\n+\nH\ni\nâ€‹\nj\nâ€‹\n(\nx\n^\n)\nâ€‹\n(\nx\nâˆ’\nx\n^\n)\n+\n(\ny\nâˆ’\ny\n^\n)\n=\n0\n\\begin{split}f(\\texttt{{x}},\\texttt{{y}})&\\approx\\hat{\\texttt{{y}}}-g(\\hat{\\texttt{{x}}})+\\frac{\\partial{f(\\texttt{{x}},\\texttt{{y}})}}{\\partial{\\texttt{{x}}}}(\\texttt{{x}}-\\hat{\\texttt{{x}}})+\\frac{\\partial{f(\\texttt{{x}},\\texttt{{y}})}}{\\partial{\\texttt{{y}}}}(\\texttt{{y}}-\\hat{\\texttt{{y}}})\\\\\n&=\\texttt{{h}}_{ij}+\\text{{H}}_{ij}(\\hat{\\texttt{{x}}})(\\texttt{{x}}-\\hat{\\texttt{{x}}})+(\\texttt{{y}}-\\hat{\\texttt{{y}}})=\\texttt{{0}}\\end{split}\n(16)\nH\ni\nâ€‹\nj\nâ€‹\n(\nx\n^\n)\n\\text{{H}}_{ij}(\\hat{\\texttt{{x}}})\nrepresents the Jacobian matrix that corresponds to hypothesis\nâ„‹\ni\nâ€‹\nj\n\\mathcal{H}_{ij}\n, evaluated at\nx\n^\n\\hat{\\texttt{{x}}}\n. According to the previous subsection, the value of Jacobian matrix\nH\ni\nâ€‹\nj\nâ€‹\n(\nx\n^\n)\n\\text{{H}}_{ij}(\\hat{\\texttt{{x}}})\nalso depends on the 3D position of the\ni\nt\nâ€‹\nh\ni^{th}\nlabeled key point in PSM frame as well as pixel values of the\nj\nt\nâ€‹\nh\nj^{th}\nmeasurement, hence is directly affected by the verity of\nâ„‹\ni\nâ€‹\nj\n\\mathcal{H}_{ij}\n. We use\nC\ni\nâ€‹\nj\n\\text{{C}}_{ij}\nto represent the covariance matrix for innovation\nh\ni\nâ€‹\nj\n\\texttt{{h}}_{ij}\nin eq.(\n17\n).\nC\ni\nâ€‹\nj\n=\nH\ni\nâ€‹\nj\nâ€‹\n(\nx\n^\n)\nâ€‹\nğšº\nğ\nâ€‹\nH\ni\nâ€‹\nj\nâ€‹\n(\nx\n^\n)\nT\n+\nğšº\nğ¯\n\\text{{C}}_{ij}=\\text{{H}}_{ij}(\\hat{\\texttt{{x}}})\\mathbf{\\Sigma_{e}}\\text{{H}}_{ij}(\\hat{\\texttt{{x}}})^{\\textmd{T}}+\\mathbf{\\Sigma_{v}}\n(17)\nD\ni\nâ€‹\nj\n2\n=\nh\ni\nâ€‹\nj\nT\nâ€‹\nC\ni\nâ€‹\nj\nâˆ’\n1\nâ€‹\nh\ni\nâ€‹\nj\n<\nÏ‡\nd\n,\nÎ±\n2\nD_{ij}^{2}=\\texttt{{h}}_{ij}^{\\textmd{T}}\\text{{C}}_{ij}^{-1}\\texttt{{h}}_{ij}<\\chi^{2}_{d,\\alpha}\n(18)\nTo evaluate the credibility of\nâ„‹\ni\nâ€‹\nj\n\\mathcal{H}_{ij}\n, we introduce squared Mahalanobis distance as eq.(\n18\n), where\nd\n=\ndim\nâ€‹\n(\ny\n)\nd=\\text{dim}(\\texttt{{y}})\nand\nÎ±\n\\alpha\nis threshold confidence level. If the calculated Mahalanobis distance falls within the confidence interval of a chi-square distribution, the presumed hypothesis is deemed credible.\nThe advantage of adopting Mahalanobis distance as evaluation criterion over simple Euclidean distance is that they provide a better alignment description in statistical space\n[\n23\n]\n. This advantage becomes more evident when it comes to multiple pairings.\nII-C\n2\nJoint Compatibility test\nThe previous subsection focuses on evaluating individual compatibility for a single association hypothesis. However, in reality, there usually exists multiple unlabeled observations with outliers. Moreover, being individually compatible does not necessarily lead to a globally compatible outcome. To overcome these limitations, in this subsection, we expand the previous individual compatibility evaluation criterion to cases where multiple associations are present. The algorithm for conducting joint compatibility test is presented as Algorithm.\n1\n.\nWe assume that there exists a hypothesis list\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\n, which contains\nk\nk\nnon-trivial association hypotheses. These non-trivial hypotheses are denoted as\n{\nâ„‹\ni\n1\nâ€‹\nj\n1\nâ€‹\nâ€¦\nâ€‹\nâ„‹\ni\nk\nâ€‹\nj\nk\n}\n\\{\\mathcal{H}_{i_{1}j_{1}}...\\mathcal{H}_{i_{k}j_{k}}\\}\n. For each non-trivial hypothesis, we first calculate its corresponding innovation and Jacobian matrix and then stack them vertically. Their corresponding\nğšº\nğ\n\\mathbf{\\Sigma_{e}}\nand\nğšº\nğ¯\n\\mathbf{\\Sigma_{v}}\nmatrices are stacked diagonally. Then the covariance matrix\nC\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\text{{C}}_{\\mathcal{\\tilde{H}}_{set}}\nfor all innovations is calculated as per eq.(\n19\n).\nC\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n=\n[\nH\ni\n1\nâ€‹\nj\n1\nâ‹®\nH\ni\nk\nâ€‹\nj\nk\n]\n[\nğšº\nğğŸ\nâ‹±\nğšº\nğğ¤\n]\nâ€‹\n[\nH\ni\n1\nâ€‹\nj\n1\nâ‹®\nH\ni\nk\nâ€‹\nj\nk\n]\nT\n+\n[\nğšº\nğ¯ğŸ\nâ‹±\nğšº\nğ¯ğ¤\n]\n\\begin{split}\\text{{C}}_{\\mathcal{\\tilde{H}}_{set}}=\\begin{bmatrix}\\text{{H}}_{i_{1}j_{1}}\\\\\n\\vdots\\\\\n\\text{{H}}_{i_{k}j_{k}}\\end{bmatrix}&\\begin{bmatrix}\\mathbf{\\Sigma_{e1}}&&\\\\\n&\\ddots&\\\\\n&&\\mathbf{\\Sigma_{ek}}\\\\\n\\end{bmatrix}\\begin{bmatrix}\\text{{H}}_{i_{1}j_{1}}\\\\\n\\vdots\\\\\n\\text{{H}}_{i_{k}j_{k}}\\end{bmatrix}^{\\textmd{T}}\\\\\n+&\\begin{bmatrix}\\mathbf{\\Sigma_{v1}}&&\\\\\n&\\ddots&\\\\\n&&\\mathbf{\\Sigma_{vk}}\\\\\n\\end{bmatrix}\\end{split}\n(19)\nThe squared Mahalanobis distance\nD\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n2\nD^{2}_{\\mathcal{\\tilde{H}}_{set}}\nfor\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\nis expressed in eq.(\n20\n).\nD\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n2\n=\n[\nh\ni\n1\nâ€‹\nj\n1\nT\nâ‹¯\nh\ni\nk\nâ€‹\nj\nk\nT\n]\n1\nÃ—\n2\nâ€‹\nk\nâ€‹\nC\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\nâˆ’\n1\nâ€‹\n[\nh\ni\n1\nâ€‹\nj\n1\nâ‹®\nh\ni\nk\nâ€‹\nj\nk\n]\n2\nâ€‹\nk\nÃ—\n1\nD_{\\mathcal{\\tilde{H}}_{set}}^{2}=\\begin{bmatrix}\\texttt{{h}}_{i_{1}j_{1}}^{\\textmd{T}}&\\cdots&\\texttt{{h}}_{i_{k}j_{k}}^{\\textmd{T}}\\\\\n\\end{bmatrix}_{1\\times 2k}\\text{{C}}_{\\mathcal{\\tilde{H}}_{set}}^{-1}\\begin{bmatrix}\\texttt{{h}}_{i_{1}j_{1}}\\\\\n\\vdots\\\\\n\\texttt{{h}}_{i_{k}j_{k}}\\end{bmatrix}_{2k\\times 1}\n(20)\nSimilar to eq.(\n18\n), if\nD\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n2\n<\nÏ‡\nd\n,\nÎ±\n2\nD^{2}_{\\mathcal{\\tilde{H}}_{set}}<\\chi^{2}_{d,\\alpha}\n, where\nd\n=\n2\nâ€‹\nk\nd=2k\n,\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\nis deemed jointly compatible.\nDistance\nl\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\nl_{\\mathcal{\\tilde{H}}_{set}}\nis calculated as per eq.(\n21\n), representing the negative logarithm of matching likelihood\n[\n2\n]\n, which was shown as the optimal statistic to be optimized compared with the squared Mahalanobis distance. A smaller\nl\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\nl_{\\mathcal{\\tilde{H}}_{set}}\nindicates a more compact joint association.\nl\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n=\n2\nâ€‹\nk\nâ€‹\nlog\nâ¡\n(\n2\nâ€‹\nÏ€\n)\n+\nD\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n2\n+\nlog\nâ¡\n(\ndet\n(\nC\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n)\n)\nl_{\\mathcal{\\tilde{H}}_{set}}=2k\\log(2\\pi)+D^{2}_{\\mathcal{\\tilde{H}}_{set}}+\\log(\\det(\\text{{C}}_{\\mathcal{\\tilde{H}}_{set}}))\n(21)\nII-D\nJoint Compatibility Branch and Bound (JCBB)\nIn the previous subsection, it is assumed that hypothesis list\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\nis readily available. However, in reality, to obtain a reliable hypothesis list is challenging. In this subsection, we present an algorithm for obtaining the optimal\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\nthrough recursively establishing new hypotheses and self-updating, named Joint Compatibility Branch and Bound (JCBB).\nFor all observations in\nğ’«\no\nâ€‹\nb\nâ€‹\ns\n\\mathcal{P}_{obs}\n, we initially establish\nm\nÃ—\nn\nm\\times n\nindividual association hypotheses\n{\nâ„‹\ni\nâ€‹\nj\n}\n\\{\\mathcal{H}_{ij}\\}\n, with\ni\nâˆˆ\n[\n1\nâ€‹\nâ€¦\nâ€‹\nm\n]\ni\\in[1...m]\nand\nj\nâˆˆ\n[\n1\nâ€‹\nâ€¦\nâ€‹\nn\n]\nj\\in[1...n]\n. After conducting individual compatibility check, as per eq.(\n18\n), spurious hypotheses are pruned and outlier observations are determined. We then create a candidate hypothesis list\nâ„’\n=\n{\n{\nâ„‹\n1\nâ€‹\nj\n1\nâ€‹\nâ€¦\nâ€‹\nâ„‹\n1\nâ€‹\nj\nk\n1\n}\nâ€‹\nâ€¦\nâ€‹\n{\nâ„‹\nm\nâ€‹\nj\n1\nâ€‹\nâ€¦\nâ€‹\nâ„‹\nm\nâ€‹\nj\nk\nm\n}\n}\n\\mathcal{L}=\\{\\{\\mathcal{H}_{1j_{1}}...\\mathcal{H}_{1j_{k_{1}}}\\}...\\{\\mathcal{H}_{mj_{1}}...\\mathcal{H}_{mj_{k_{m}}}\\}\\}\n.\nâ„’\ni\n\\mathcal{L}_{i}\n, the\ni\nt\nâ€‹\nh\ni^{th}\nelement of\nâ„’\n\\mathcal{L}\n, contains all individually compatible hypotheses for\nğ’«\no\nâ€‹\nb\nâ€‹\ns\n,\ni\n\\mathcal{P}_{obs,i}\n.\nThe JCBB algorithm reads input as an empty list of candidate hypotheses,\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\n, and returns the final list of associations,\nâ„‹\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{H}_{set}\n, which possesses the highest number of non-trivial associations,\nn\np\nâ€‹\na\nâ€‹\ni\nâ€‹\nr\nn_{pair}\n, and the smallest negative logarithm of matching likelihood\nl\nâ„‹\ns\nâ€‹\ne\nâ€‹\nt\nl_{\\mathcal{H}_{set}}\n. The process alternates between adding new hypothesis from each element of\nâ„’\n\\mathcal{L}\nto\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\n, and conducting prompt joint compatibility check, as per Algorithm.(\n1\n), until the size of\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\nreaches\nm\nm\n. Algorithm\n2\nsummarizes how\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\naccumulates and self-updates in a recursive manner. To facilitate algorithm implementation, before adding new hypothesis to\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\n, the currently achievable maximum number of non-trivial associations,\nk\nm\nâ€‹\na\nâ€‹\nx\n+\nk\np\nk_{max}+k_{p}\n, is calculated, because a single key point prediction cannot be associated with multiple observations. Branches that do not satisfy the requirement stated in line\n22\nand\n29\nare promptly pruned. After appending a new non-trivial hypothesis,\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\ncontinues accumulating, as in case 2. On the contrary, in case 1 and 3,\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\nappends a trivial association for different causes. Case 1 allows for situations where no individually compatible hypotheses are currently available, whereas case 3 envisions that the currently matched key point prediction may be more appropriately associated with another observation, leading to a higher joint compatibility score.\nAlgorithm 1\nJoint Compatibility Evaluation\n1:\nğ’«\np\nâ€‹\nr\nâ€‹\ne\n,\nğ’«\no\nâ€‹\nb\nâ€‹\ns\n\\mathcal{P}_{pre},\\mathcal{P}_{obs}\n,\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\n2:\nl\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\nl_{\\mathcal{\\tilde{H}}_{set}}\n,\ni\nâ€‹\ns\nâ€‹\nt\nâ€‹\nr\nâ€‹\nu\nâ€‹\ne\nistrue\n3:\nInitialisation:\n4:\nğšº\nğ\n,\nğšğ¥ğ¥\n\\mathbf{\\Sigma_{e,all}}\n,\nğšº\nğ¯\n,\nğšğ¥ğ¥\n\\mathbf{\\Sigma_{v,all}}\n,\nH\na\nâ€‹\nl\nâ€‹\nl\n\\text{{H}}_{all}\n,\nh\na\nâ€‹\nl\nâ€‹\nl\n\\texttt{{h}}_{all}\nâŠ³\n\\triangleright\nEmpty for initialisation\n5:\nprocedure\nJC\n(\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\n)\n6:\nm\nâ†\nl\nâ€‹\ne\nâ€‹\nn\nâ€‹\n(\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n)\nm\\leftarrow len(\\mathcal{\\tilde{H}}_{set})\n7:\nif\nm\n=\n0\nm=0\nthen\nâŠ³\n\\triangleright\nNo observation\n8:\nreturn\nl\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n=\ninf\n,\ni\nâ€‹\ns\nâ€‹\nt\nâ€‹\nr\nâ€‹\nu\nâ€‹\ne\n=\nFalse\nl_{\\mathcal{\\tilde{H}}_{set}}=\\textbf{inf},istrue=\\textbf{False}\n9:\nelse\n10:\nfor\nall\nâ„‹\ni\nâ€‹\nj\nk\n\\mathcal{H}_{ij_{k}}\nin\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\ndo\n11:\nif\nj\nk\n=\n0\nj_{k}=0\nthen\nâŠ³\n\\triangleright\nNo matching\n12:\ncontinue\n13:\nelse\n14:\nh\ni\nâ€‹\nj\nk\n,\nH\ni\nâ€‹\nj\nk\nâ†\n\\texttt{{h}}_{ij_{k}},\\text{{H}}_{ij_{k}}\\leftarrow\neq.(\n6\n) to (\n13\n)\n15:\nğšº\nğ\n,\nğšğ¥ğ¥\n.\na\nâ€‹\nd\nâ€‹\nd\nâ€‹\n(\nğšº\nğ\n,\nğ¢\n)\n,\nğšº\nğ¯\n,\nğšğ¥ğ¥\n.\na\nâ€‹\nd\nâ€‹\nd\nâ€‹\n(\nğšº\nğ¯\n,\nğ¢\n)\n\\mathbf{\\Sigma_{e,all}}.add(\\mathbf{\\Sigma_{e,i}}),\\mathbf{\\Sigma_{v,all}}.add(\\mathbf{\\Sigma_{v,i}})\n16:\nH\na\nâ€‹\nl\nâ€‹\nl\n.\na\nâ€‹\nd\nâ€‹\nd\nâ€‹\n(\nH\ni\nâ€‹\nj\nk\n)\n,\nh\na\nâ€‹\nl\nâ€‹\nl\n.\na\nâ€‹\nd\nâ€‹\nd\nâ€‹\n(\nh\ni\nâ€‹\nj\nk\n)\n\\text{{H}}_{all}.add(\\text{{H}}_{ij_{k}}),\\texttt{{h}}_{all}.add(\\texttt{{h}}_{ij_{k}})\n17:\nend\nif\n18:\nend\nfor\n19:\nC\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n,\nD\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n2\n,\nl\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\nâ†\n\\text{{C}}_{\\mathcal{\\tilde{H}}_{set}},D^{2}_{\\mathcal{\\tilde{H}}_{set}},l_{\\mathcal{\\tilde{H}}_{set}}\\leftarrow\neq.(\n19\n),\n20\n,\n21\n20:\nreturn\nl\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n,\ni\nâ€‹\ns\nâ€‹\nt\nâ€‹\nr\nâ€‹\nu\nâ€‹\ne\n=\nD\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n2\n<\nÏ‡\nd\n,\nÎ±\n2\nl_{\\mathcal{\\tilde{H}}_{set}},istrue=D^{2}_{\\mathcal{\\tilde{H}}_{set}}<\\chi^{2}_{d,\\alpha}\n21:\nend\nif\n22:\nend\nprocedure\nAlgorithm 2\nJoint Compatibility Branch and Bound\n1:\nğ’«\np\nâ€‹\nr\nâ€‹\ne\n,\nğ’«\no\nâ€‹\nb\nâ€‹\ns\n\\mathcal{P}_{pre},\\mathcal{P}_{obs}\n,\nâ„’\n\\mathcal{L}\n2:\nâ„‹\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{H}_{set}\n3:\nInitialisation:\n4:\nâ„‹\ns\nâ€‹\ne\nâ€‹\nt\n=\n{\n}\n\\mathcal{H}_{set}=\\{\\}\n,\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n=\n{\n}\n\\mathcal{\\tilde{H}}_{set}=\\{\\}\n,\nn\np\nâ€‹\na\nâ€‹\ni\nâ€‹\nr\n=\n0\nn_{pair}=0\n,\nl\n=\n+\nâˆ\nl=+\\infty\n,\nn\n=\nl\nâ€‹\ne\nâ€‹\nn\nâ€‹\n(\nğ’«\np\nâ€‹\nr\nâ€‹\ne\n)\nn=len(\\mathcal{P}_{pre})\n,\nm\n=\nl\nâ€‹\ne\nâ€‹\nn\nâ€‹\n(\nğ’«\no\nâ€‹\nb\nâ€‹\ns\n)\nm=len(\\mathcal{P}_{obs})\n5:\nprocedure\nJCBB\n(\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\n)\nâŠ³\n\\triangleright\nStarting point\n6:\nk\ns\nâ†\nl\nâ€‹\ne\nâ€‹\nn\nâ€‹\n(\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n)\nk_{s}\\leftarrow len(\\mathcal{\\tilde{H}}_{set})\nâŠ³\n\\triangleright\nCount existing pairs\n7:\nk\np\nâ†\nP\nâ€‹\na\nâ€‹\ni\nâ€‹\nr\nâ€‹\ns\nâ€‹\n(\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n)\nk_{p}\\leftarrow Pairs(\\mathcal{\\tilde{H}}_{set})\nâŠ³\n\\triangleright\nCount non-trivial pairs\n8:\nif\nl\nâ€‹\ne\nâ€‹\nn\nâ€‹\n(\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n)\n=\nm\nlen(\\mathcal{\\tilde{H}}_{set})=m\nthen\n9:\nl\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n,\ni\nâ€‹\ns\nâ€‹\nt\nâ€‹\nr\nâ€‹\nu\nâ€‹\ne\nâ†\nJC\nâ€‹\n(\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n,\nğ’«\np\nâ€‹\nr\nâ€‹\ne\n,\nğ’«\no\nâ€‹\nb\nâ€‹\ns\n)\nl_{\\mathcal{\\tilde{H}}_{set}},istrue\\leftarrow\\textbf{JC}(\\mathcal{\\tilde{H}}_{set},\\mathcal{P}_{pre},\\mathcal{P}_{obs})\nâŠ³\n\\triangleright\nAlg.\n1\n10:\nif\n(\nn\np\nâ€‹\na\nâ€‹\ni\nâ€‹\nr\n=\n0\nn_{pair}=0\nor\nk\np\n>\nn\np\nâ€‹\na\nâ€‹\ni\nâ€‹\nr\nk_{p}>n_{pair}\n)\nand\ni\nâ€‹\ns\nâ€‹\nt\nâ€‹\nr\nâ€‹\nu\nâ€‹\ne\nistrue\nthen\n11:\nâ„‹\ns\nâ€‹\ne\nâ€‹\nt\nâ†\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n,\nn\np\nâ€‹\na\nâ€‹\ni\nâ€‹\nr\nâ†\nk\np\n,\nl\nâ†\nl\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{H}_{set}\\leftarrow\\mathcal{\\tilde{H}}_{set},n_{pair}\\leftarrow k_{p},l\\leftarrow l_{\\mathcal{\\tilde{H}}_{set}}\n12:\nreturn\nâŠ³\n\\triangleright\nPrioritise larger\nn\np\nâ€‹\na\nâ€‹\ni\nâ€‹\nr\nn_{pair}\n13:\nelse\nif\n14:\nk\np\n=\nn\np\nâ€‹\na\nâ€‹\ni\nâ€‹\nr\nk_{p}=n_{pair}\nand\nl\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n<\nl\nl_{\\mathcal{\\tilde{H}}_{set}}<l\nand\ni\nâ€‹\ns\nâ€‹\nt\nâ€‹\nr\nâ€‹\nu\nâ€‹\ne\nistrue\nthen\n15:\nâ„‹\ns\nâ€‹\ne\nâ€‹\nt\nâ†\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{H}_{set}\\leftarrow\\mathcal{\\tilde{H}}_{set}\n,\nl\nâ†\nl\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\nl\\leftarrow l_{\\mathcal{\\tilde{H}}_{set}}\n16:\nreturn\nâŠ³\n\\triangleright\nPrioritise smaller\nl\nl\n17:\nelse return\n18:\nend\nif\n19:\nelse\n20:\nif\nl\nâ€‹\ne\nâ€‹\nn\nâ€‹\n(\nâ„’\nk\ns\n+\n1\n)\n=\n0\nlen(\\mathcal{L}_{k_{s}+1})=0\nthen\nâŠ³\n\\triangleright\nNo compatible pairs\n21:\nk\nm\nâ€‹\na\nâ€‹\nx\nâ†\nM\nâ€‹\na\nâ€‹\nx\nâ€‹\nR\nâ€‹\ne\nâ€‹\nm\nâ€‹\nP\nâ€‹\na\nâ€‹\ni\nâ€‹\nr\nâ€‹\ns\nâ€‹\n(\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n,\nâ„’\n)\nk_{max}\\leftarrow MaxRemPairs(\\mathcal{\\tilde{H}}_{set},\\mathcal{L})\n22:\nif\nk\nm\nâ€‹\na\nâ€‹\nx\n+\nk\np\nâ‰¥\nn\np\nâ€‹\na\nâ€‹\ni\nâ€‹\nr\nk_{max}+k_{p}\\geq n_{pair}\nthen\n23:\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\nâ†\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n.\na\nâ€‹\nd\nâ€‹\nd\nâ€‹\n{\nN\nâ€‹\no\nâ€‹\nn\nâ€‹\ne\n}\n\\mathcal{\\tilde{H}}_{set}\\leftarrow\\mathcal{\\tilde{H}}_{set}.add\\{None\\}\n24:\nJCBB\n(\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{\\tilde{H}}_{set}\n)\nâŠ³\n\\triangleright\nCase 1: to line\n5\n25:\nend\nif\n26:\nend\nif\n27:\nfor\nâ„‹\nk\ns\n+\n1\nâ€‹\nj\nk\nâ€‹\nin\nâ€‹\nâ„’\nk\ns\n+\n1\n\\mathcal{H}_{k_{s}+1j_{k}}\\textbf{ in }\\mathcal{L}_{k_{s}+1}\ndo\n28:\nk\nm\nâ€‹\na\nâ€‹\nx\nâ†\nM\nâ€‹\na\nâ€‹\nx\nâ€‹\nR\nâ€‹\ne\nâ€‹\nm\nâ€‹\nP\nâ€‹\na\nâ€‹\ni\nâ€‹\nr\nâ€‹\ns\nâ€‹\n(\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n,\nâ„’\n,\nj\nk\n)\nk_{max}\\leftarrow MaxRemPairs(\\mathcal{\\tilde{H}}_{set},\\mathcal{L},j_{k})\n29:\nif\nk\nm\nâ€‹\na\nâ€‹\nx\n+\nk\np\nâ‰¥\nn\np\nâ€‹\na\nâ€‹\ni\nâ€‹\nr\nk_{max}+k_{p}\\geq n_{pair}\nthen\n30:\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\nâ†\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n.\na\nâ€‹\nd\nâ€‹\nd\nâ€‹\n{\nâ„‹\nk\ns\n+\n1\nâ€‹\nj\nk\n}\n\\mathcal{\\tilde{H}}_{set}\\leftarrow\\mathcal{\\tilde{H}}_{set}.add\\{\\mathcal{H}_{k_{s}+1j_{k}}\\}\n31:\nJCBB\nâ€‹\n(\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n)\n\\textbf{JCBB}(\\mathcal{\\tilde{H}}_{set})\nâŠ³\n\\triangleright\nCase 2: to line\n5\n32:\nend\nif\n33:\nend\nfor\n34:\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\nâ†\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n.\na\nâ€‹\nd\nâ€‹\nd\nâ€‹\n{\nN\nâ€‹\no\nâ€‹\nn\nâ€‹\ne\n}\n\\mathcal{\\tilde{H}}_{set}\\leftarrow\\mathcal{\\tilde{H}}_{set}.add\\{None\\}\n35:\nJCBB\n(\nâ„‹\n~\ns\nâ€‹\ne\nâ€‹\nt\n)\n(\\mathcal{\\tilde{H}}_{set})\nâŠ³\n\\triangleright\nCase 3: to line\n5\n36:\nend\nif\n37:\nend\nprocedure\n38:\nreturn\nâ„‹\ns\nâ€‹\ne\nâ€‹\nt\n\\mathcal{H}_{set}\nâŠ³\n\\triangleright\nFinal list of associations\nII-E\nKey points visibility test\nThe computational cost of the JCBB algorithm implementation depends on the size of\nğ’«\np\nâ€‹\nr\nâ€‹\ne\n\\mathcal{P}_{pre}\nand\nğ’«\no\nâ€‹\nb\nâ€‹\ns\n\\mathcal{P}_{obs}\n. Recent advancements in computer vision algorithms have improved the quality of\nğ’«\no\nâ€‹\nb\nâ€‹\ns\n\\mathcal{P}_{obs}\n, by providing more accurate measurements under various challenging surgical scenarios, via both marker-based and marker-free approaches. The development of feature detection algorithms is beyond the scope of this study. This subsection introduces an additional visibility-check block for facilitating the key point association process by reducing the size of\nğ’«\np\nâ€‹\nr\nâ€‹\ne\n\\mathcal{P}_{pre}\n, based on the observation that key points located on four different sides of an instrument cannot be all visible simultaneously.\nAs illustrated in Fig.\n2\n, twelve key points are labeled on the wrist section of a cylindrical instrument and can be classified by their position on the instrument, namely, front, back, left, and right. Intuitively, when key points located on the front (or left) side are directly exposed to the camera view, those on the back (or right) side are not visible, except in rare cases where the instrument rotates to a specific pose such that key points on two opposite sides align with two edge boundaries, making three sides visible simultaneously. In most cases, however, at most two sides are visible in the camera view.\nThe first step is to determine the number of visible sides in the current camera view based on a central-edge distance ratio check. We use\nI\n,\nI\nâ€²\n\\texttt{{I}},\\texttt{{I}}^{\\prime}\n, and\nI\nc\n\\texttt{{I}}_{c}\nto represent the upper edge, lower edge, and center line of the cylindrical instrument on an image plane, respectively. The calculation of\nI\nand\nI\nâ€²\n\\texttt{{I}}^{\\prime}\nwere presented in\n[\n21\n]\n, and\nI\nc\n\\texttt{{I}}_{c}\nis the line that connects the projection of certain instrument joints on the image plane.\nğ’ª\nr\nâ€‹\no\nâ€‹\nl\nâ€‹\nl\n=\n{\nğ¨\nğ«ğŸ\n,\nğ¨\nğ«ğ›\n,\nğ¨\nğ«ğ¥\n,\nğ¨\nğ«ğ«\n}\n\\mathcal{O}_{roll}=\\{\\mathbf{o_{rf}},\\mathbf{o_{rb},\\mathbf{o_{rl}},\\mathbf{o_{rr}}}\\}\nrepresents the pixel projection of all key points from the â€œrollâ€ family. We also have\nğ’ª\np\nâ€‹\ni\nâ€‹\nt\nâ€‹\nc\nâ€‹\nh\n=\n{\nğ¨\nğ©ğŸ\n,\nğ¨\nğ©ğ›\n,\nğ¨\nğ©ğ¥\n,\nğ¨\nğ©ğ«\n}\n\\mathcal{O}_{pitch}=\\{\\mathbf{o_{pf}},\\mathbf{o_{pb},\\mathbf{o_{pl}},\\mathbf{o_{pr}}}\\}\n,\nğ’ª\ne\nâ€‹\nn\nâ€‹\nd\n=\n{\nğ¨\nğğŸ\n,\nğ¨\nğğ›\n}\n\\mathcal{O}_{end}=\\{\\mathbf{o_{ef}},\\mathbf{o_{eb}}\\}\nand\nğ’ª\ng\nâ€‹\nr\nâ€‹\ni\nâ€‹\np\n=\n{\nğ¨\nğ ğ¥\n,\nğ¨\nğ ğ«\n}\n\\mathcal{O}_{grip}=\\{\\mathbf{o_{gl}},\\mathbf{o_{gr}}\\}\n, representing â€œpitchâ€, â€œend-effectorâ€ and â€œgripperâ€ families, respectively.\nFor example,\nd\nr\nâ€‹\nf\nc\nd^{c}_{rf}\nrepresents the center distance between\nğ¨\nğ«ğŸ\n\\mathbf{o_{rf}}\nand\nI\nc\n1\n\\texttt{{I}}_{c_{1}}\n, and\nd\no\nr\nâ€‹\nf\ne\nâ€‹\nd\nâ€‹\ng\nâ€‹\ne\n=\nm\nâ€‹\ni\nâ€‹\nn\nâ€‹\nâŸ¨\nd\no\nr\nâ€‹\nf\nL\n1\n,\nd\no\nr\nâ€‹\nf\nL\n2\nâŸ©\nd_{o_{rf}}^{edge}=min\\langle d_{o_{rf}}^{\\texttt{{L}}_{1}},d_{o_{rf}}^{\\texttt{{L}}_{2}}\\rangle\n, represents the closest edge distance between\nğ¨\nğ«ğŸ\n\\mathbf{o_{rf}}\nand two edge boundaries. Similar notations also apply to the other key points. The center-edge ratio\nÎ·\nf\nâ€‹\nb\n\\eta_{fb}\n, for front-back sides, and\nÎ·\nl\nâ€‹\nr\n\\eta_{lr}\n, left-right sides, are defined in eq.(\n22\n) and (\n23\n), respectively.\nÎ³\n\\gamma\nis a predefined ratio threshold. If either\nÎ·\nf\nâ€‹\nb\nâ‰¥\nÎ³\n\\eta_{fb}\\geq\\gamma\nor\nÎ·\nl\nâ€‹\nr\nâ‰¥\nÎ³\n\\eta_{lr}\\geq\\gamma\n, it indicates that one side is predominantly visible; otherwise two sides are visible.\nÎ·\nf\nâ€‹\nb\n=\nd\nr\nâ€‹\nf\nc\n+\nd\nr\nâ€‹\nb\nc\nd\nr\nâ€‹\nf\ne\nâ€‹\nd\nâ€‹\ng\nâ€‹\ne\n+\nd\nr\nâ€‹\nb\ne\nâ€‹\nd\nâ€‹\ng\nâ€‹\ne\nâ‰¥\nÎ³\n\\eta_{fb}=\\frac{d_{{rf}}^{c}+d_{{rb}}^{c}}{d_{{rf}}^{edge}+d_{{rb}}^{edge}}\\geq\\gamma\n(22)\nÎ·\nl\nâ€‹\nr\n=\nd\nr\nâ€‹\nl\nc\n+\nd\nr\nâ€‹\nr\nc\nd\nr\nâ€‹\nl\ne\nâ€‹\nd\nâ€‹\ng\nâ€‹\ne\n+\nd\nr\nâ€‹\nr\ne\nâ€‹\nd\nâ€‹\ng\nâ€‹\ne\nâ‰¥\nÎ³\n\\eta_{lr}=\\frac{d_{{rl}}^{c}+d_{{rr}}^{c}}{d_{{rl}}^{edge}+d_{{rr}}^{edge}}\\geq\\gamma\n(23)\nFigure 5\n:\nVisibility check examples. After conducting visibility check, the number of candidate key points is reduced.\nFigure 6\n:\nVisibility check decision tree\nThe second step is to determine which sides are visible by comparing the distribution of key points in relation their corresponding central line and edge boundaries. For key points in\nğ’ª\nr\nâ€‹\no\nâ€‹\nl\nâ€‹\nl\n\\mathcal{O}_{roll}\n, the central line is\nI\nc\n1\n\\texttt{{I}}_{c_{1}}\n, and the edge boundaries are\nI\n1\n\\texttt{{I}}_{1}\nand\nI\n1\nâ€²\n\\texttt{{I}}^{\\prime}_{1}\n; for key points in\nğ’ª\np\nâ€‹\ni\nâ€‹\nt\nâ€‹\nc\nâ€‹\nh\n\\mathcal{O}_{pitch}\nand\nğ’ª\ne\nâ€‹\nn\nâ€‹\nd\n\\mathcal{O}_{end}\n, the central line is\nI\nc\n2\n\\texttt{{I}}_{c_{2}}\n, and the edge boundaries are\nI\n2\n\\texttt{{I}}_{2}\nand\nI\n2\nâ€²\n\\texttt{{I}}^{\\prime}_{2}\n, as illustrated in Fig.\n3\n. For example, we denote\nm\nf\nu\nâ€‹\np\nm_{f}^{up}\nas the number of front-side key points in\n{\nğ’ª\nr\nâ€‹\no\nâ€‹\nl\nâ€‹\nl\n,\nğ’ª\np\nâ€‹\ni\nâ€‹\nt\nâ€‹\nc\nâ€‹\nh\n}\n\\{\\mathcal{O}_{roll},\\mathcal{O}_{pitch}\\}\n, that are either positioned between\nI\nc\n1\n\\texttt{{I}}_{c_{1}}\nand\nI\n1\n\\texttt{{I}}_{1}\n, or between\nI\nc\n2\n\\texttt{{I}}_{c_{2}}\nand\nI\n2\n\\texttt{{I}}_{2}\n. Similar notations also apply to the other key points. If it is determined that\nÎ·\nf\nâ€‹\nb\nâ‰¥\nÎ³\n\\eta_{fb}\\geq\\gamma\nfrom step 1, we then compare\nm\nf\nu\nâ€‹\np\nm_{f}^{up}\nwith\nm\nb\nu\nâ€‹\np\nm_{b}^{up}\nto determine whether it is the right or left side that is dominantly visible. In cases where two sides are visible, comparisons between\nm\nf\nu\nâ€‹\np\nm_{f}^{up}\nand\nm\nb\nu\nâ€‹\np\nm_{b}^{up}\n, as well as\nm\nl\nu\nâ€‹\np\nm_{l}^{up}\nand\nm\nr\nu\nâ€‹\np\nm_{r}^{up}\n, are required. An illustration of pruning invisible key points is shown in Fig\n5\n, and the decision tree for conducting visibility check is shown in Fig.\n6\n. Eventually, key points on invisible sides are removed from\nğ’«\np\nâ€‹\nr\nâ€‹\ne\n\\mathcal{P}_{pre}\n.\nII-F\nState estimation with known correspondences\nHaving established known correspondences, in this subsection, three statistical estimator-based approaches are presented and compared, for estimating the state vector in different realistic scenarios.\nII-F\n1\nExtended Kalman Filter\nAfter data associations, we have established\nm\nk\nm_{k}\nnon-trivial association hypotheses,\nâ„‹\nf\nâ€‹\ni\nâ€‹\nn\nâ€‹\na\nâ€‹\nl\n=\n{\nâ„‹\nm\n1\nâ€‹\nj\nm\nâ€‹\n1\nâ€‹\nâ€¦\nâ€‹\nâ„‹\nm\nk\nâ€‹\nj\nm\nk\n}\n\\mathcal{H}_{final}=\\{\\mathcal{H}_{m_{1}j_{m1}}...\\mathcal{H}_{m_{k}j_{m_{k}}}\\}\n. If\nm\nk\n=\n0\nm_{k}=0\n,\nâ„‹\nf\nâ€‹\ni\nâ€‹\nn\nâ€‹\na\nâ€‹\nl\n\\mathcal{H}_{final}\nis empty. The implementation of Extended Kalman Filter (EKF) for state estimation is presented in Algorithm.\n3\n. The state transition model is considered linear, and linearization of the measurement model has already been introduced, as per eq.(\n6\n) to (\n13\n). EKF assumes that only Gaussian noises exist during state transition and measurements collection and that\nğšº\nğ\n\\mathbf{\\Sigma_{e}}\nand\nğšº\nğ¯\n\\mathbf{\\Sigma_{v}}\nremain static throughout the process. In\n[\n29\n]\n, it was proved that errors in robot joint positions can be lumped into modifications to the current state vector\nx\nt\n\\texttt{{x}}^{t}\n, which is continuously estimated, and hence it can be justified that the state transition model is not affected by noisy inputs from the encoder. Eventually, EKF returns the posterior state estimation\nx\n^\nt\n\\hat{\\texttt{{x}}}^{t}\n, state covariance matrix\nğšº\n^\nğ±\nt\n\\mathbf{\\hat{\\Sigma}_{x}}^{t}\n, together with innovation list\nğ’«\ni\nâ€‹\nn\nâ€‹\nn\n\\mathcal{P}_{inn}\n.\nAlgorithm 3\nExtended Kalman Filter\n1:\nx\n^\nt\nâˆ’\n1\n\\hat{\\texttt{{x}}}^{t-1}\n,\nğšº\n^\nğ±\nt\nâˆ’\n1\n\\mathbf{\\hat{\\Sigma}_{x}}^{t-1}\n,\nâ„‹\nf\nâ€‹\ni\nâ€‹\nn\nâ€‹\na\nâ€‹\nl\n\\mathcal{H}_{final}\n2:\nx\n^\nt\n\\hat{\\texttt{{x}}}^{t}\n,\nğšº\n^\nğ±\nt\n\\mathbf{\\hat{\\Sigma}_{x}}^{t}\n3:\nprocedure\nEKF\n(\nx\n^\nt\nâˆ’\n1\n\\hat{\\texttt{{x}}}^{t-1}\n,\nğšº\n^\nğ±\nt\nâˆ’\n1\n\\mathbf{\\hat{\\Sigma}_{x}}^{t-1}\n,\nâ„‹\nf\nâ€‹\ni\nâ€‹\nn\nâ€‹\na\nâ€‹\nl\n\\mathcal{H}_{final}\n)\n4:\nx\nÂ¯\nt\n=\nx\n^\nt\nâˆ’\n1\n\\bar{\\texttt{{x}}}^{t}=\\hat{\\texttt{{x}}}^{t-1}\nâŠ³\n\\triangleright\nA priori estimation of\nx\n5:\nğšº\nÂ¯\nğ±\nt\n=\nğšº\n^\nğ±\nt\nâˆ’\n1\n+\nğšº\nğ\n\\mathbf{\\bar{\\Sigma}_{x}}^{t}=\\mathbf{\\hat{\\Sigma}_{x}}^{t-1}+\\mathbf{\\Sigma_{e}}\nâŠ³\n\\triangleright\nA priori estimation of\nğšº\nğ±\n\\mathbf{\\Sigma_{x}}\n6:\nğ’«\ni\nâ€‹\nn\nâ€‹\nn\n=\n{\n}\n\\mathcal{P}_{inn}=\\{\\}\nâŠ³\n\\triangleright\nInnovation list\n7:\nfor\nâ„‹\nm\ni\nâ€‹\nj\nm\ni\n\\mathcal{H}_{m_{i}j_{m_{i}}}\nin\nâ„‹\nf\nâ€‹\ni\nâ€‹\nn\nâ€‹\na\nâ€‹\nl\n\\mathcal{H}_{final}\ndo\n8:\ninn\n=\n[\nu\nj\nm\ni\n,\nv\nj\nm\ni\n]\nT\nâˆ’\n[\nu\n^\nm\ni\n,\nv\n^\nm\ni\n]\nT\n\\texttt{{inn}}=[u_{j_{m_{i}}},v_{j_{m_{i}}}]^{\\textmd{T}}-[\\hat{u}_{m_{i}},\\hat{v}_{m_{i}}]^{\\textmd{T}}\nâŠ³\n\\triangleright\nInnovation\n9:\nğ’«\ni\nâ€‹\nn\nâ€‹\nn\n.\na\nâ€‹\nd\nâ€‹\nd\nâ€‹\n{\ninn\n}\n\\mathcal{P}_{inn}.add\\{\\texttt{{inn}}\\}\n10:\nC\nm\ni\nâ€‹\nj\nm\ni\n=\nH\nm\ni\nâ€‹\nj\nm\ni\nâ€‹\nğšº\nÂ¯\nğ±\nt\nâ€‹\nH\nm\ni\nâ€‹\nj\nm\ni\nT\n+\nğšº\nğ¯\n\\text{{C}}_{m_{i}j_{m_{i}}}=\\text{{H}}_{m_{i}j_{m_{i}}}\\mathbf{\\bar{\\Sigma}_{x}}^{t}\\text{{H}}_{m_{i}j_{m_{i}}}^{\\textmd{T}}+\\mathbf{\\Sigma_{v}}\nâŠ³\n\\triangleright\neq.(\n17\n)\n11:\nK\nm\ni\nâ€‹\nj\nm\ni\n=\nğšº\nÂ¯\nğ±\nt\nâ€‹\nH\nm\ni\nâ€‹\nj\nm\ni\nT\nâ€‹\nC\nm\ni\nâ€‹\nj\nm\ni\nâˆ’\n1\n\\text{{K}}_{m_{i}j_{m_{i}}}=\\mathbf{\\bar{\\Sigma}_{x}}^{t}\\text{{H}}_{m_{i}j_{m_{i}}}^{\\textmd{T}}\\text{{C}}_{m_{i}j_{m_{i}}}^{-1}\n12:\nx\nÂ¯\nt\n=\nx\nÂ¯\nt\n+\nK\nm\ni\nâ€‹\nj\nm\ni\nâ€‹\ninn\n\\bar{\\texttt{{x}}}^{t}=\\bar{\\texttt{{x}}}^{t}+\\text{{K}}_{m_{i}j_{m_{i}}}\\texttt{{inn}}\n13:\nğšº\nÂ¯\nğ±\nt\n=\n(\nI\nâˆ’\nK\nm\ni\nâ€‹\nj\nm\ni\nâ€‹\nH\nm\ni\nâ€‹\nj\nm\ni\n)\nâ€‹\nğšº\nÂ¯\nğ±\nt\n\\mathbf{\\bar{\\Sigma}_{x}}^{t}=(\\text{{I}}-\\text{{K}}_{m_{i}j_{m_{i}}}\\text{{H}}_{m_{i}j_{m_{i}}})\\mathbf{\\bar{\\Sigma}_{x}}^{t}\n14:\nend\nfor\n15:\nreturn\nx\n^\nt\n=\nx\nÂ¯\nt\n,\nğšº\n^\nğ±\nt\n=\nğšº\nÂ¯\nğ±\nt\n,\nğ’«\ni\nâ€‹\nn\nâ€‹\nn\n\\hat{\\texttt{{x}}}^{t}=\\bar{\\texttt{{x}}}^{t},\\mathbf{\\hat{\\Sigma}_{x}}^{t}=\\mathbf{\\bar{\\Sigma}_{x}}^{t},\\mathcal{P}_{inn}\n16:\nend\nprocedure\nII-F\n2\nAdaptive Extended Kalman Filter\nThe implementation of Adaptive Extended Kalman Filter (AEKF) builds on EKF, as detailed in Algorithm.\n3\n. Compared with EKF, AEKF further relaxes the assumption that only Gaussian noises are present, by adaptively updating\nğšº\n^\nğ\nt\n\\mathbf{\\hat{\\Sigma}_{e}}^{t}\nand\nğšº\n^\nğ¯\nt\n\\mathbf{\\hat{\\Sigma}_{v}}^{t}\nbased on innovations and residuals, respectively. It allows for situations where the camera is suddenly repositioned, or observations contain varying levels of noises.\nAlgorithm 4\nAdaptive Extended Kalman Filter\n1:\nâ„‹\nf\nâ€‹\ni\nâ€‹\nn\nâ€‹\na\nâ€‹\nl\n\\mathcal{H}_{final}\n,\nx\n^\nt\nâˆ’\n1\n\\hat{\\texttt{{x}}}^{t-1}\n,\nğšº\n^\nğ±\nt\nâˆ’\n1\n\\mathbf{\\hat{\\Sigma}_{x}}^{t-1}\n,\nğšº\n^\nğ\nt\nâˆ’\n1\n\\mathbf{\\hat{\\Sigma}_{e}}^{t-1}\n,\nğšº\n^\nğ¯\nt\nâˆ’\n1\n\\mathbf{\\hat{\\Sigma}_{v}}^{t-1}\n2:\nx\n^\nt\n\\hat{\\texttt{{x}}}^{t}\n,\nğšº\n^\nğ±\nt\n\\mathbf{\\hat{\\Sigma}_{x}}^{t}\n,\nğšº\n^\nğ\nt\n\\mathbf{\\hat{\\Sigma}_{e}}^{t}\n,\nğšº\n^\nğ¯\nt\n\\mathbf{\\hat{\\Sigma}_{v}}^{t}\n3:\nprocedure\nAEKF\n(\nâ„‹\nf\nâ€‹\ni\nâ€‹\nn\nâ€‹\na\nâ€‹\nl\n\\mathcal{H}_{final}\n)\n4:\nx\n^\nt\n\\hat{\\texttt{{x}}}^{t}\n,\nğšº\n^\nğ±\nt\n\\mathbf{\\hat{\\Sigma}_{x}}^{t}\n,\nğ’«\ni\nâ€‹\nn\nâ€‹\nn\n\\mathcal{P}_{inn}\n=\nEKF\n(\nx\n^\nt\nâˆ’\n1\n\\hat{\\texttt{{x}}}^{t-1}\n,\nğšº\n^\nğ±\nt\nâˆ’\n1\n\\mathbf{\\hat{\\Sigma}_{x}}^{t-1}\n,\nâ„‹\nf\nâ€‹\ni\nâ€‹\nn\nâ€‹\na\nâ€‹\nl\n\\mathcal{H}_{final}\n)\nâŠ³\n\\triangleright\nAlg.\n3\n5:\nm\nk\n=\nl\nâ€‹\ne\nâ€‹\nn\nâ€‹\n(\nâ„‹\nf\nâ€‹\ni\nâ€‹\nn\nâ€‹\na\nâ€‹\nl\n)\nm_{k}=len(\\mathcal{H}_{final})\n6:\nğšº\n^\nğ\nt\n=\nÎ±\nf\nâ€‹\nğšº\n^\nğ\nt\nâˆ’\n1\n\\mathbf{\\hat{\\Sigma}_{e}}^{t}=\\alpha_{f}\\mathbf{\\hat{\\Sigma}_{e}}^{t-1}\n,\nğšº\n^\nğ¯\nt\n=\nÎ±\nf\nâ€‹\nğšº\n^\nğ¯\nt\nâˆ’\n1\n\\mathbf{\\hat{\\Sigma}_{v}}^{t}=\\alpha_{f}\\mathbf{\\hat{\\Sigma}_{v}}^{t-1}\n7:\nfor\nâ„‹\nm\ni\nâ€‹\nj\nm\ni\n\\mathcal{H}_{m_{i}j_{m_{i}}}\nin\nâ„‹\nf\nâ€‹\ni\nâ€‹\nn\nâ€‹\na\nâ€‹\nl\n\\mathcal{H}_{final}\ndo\n8:\nH\nm\ni\nâ€‹\nj\nm\ni\nâ†\nJacobian matrix evaluated at\nâ€‹\nx\n^\nt\n\\text{{H}}_{m_{i}j_{m_{i}}}\\leftarrow\\text{Jacobian matrix evaluated at }\\hat{\\texttt{{x}}}^{t}\n9:\ninn\n=\nthe\nm\ni\nt\nâ€‹\nh\nelement of\nâ€‹\nğ’«\ni\nâ€‹\nn\nâ€‹\nn\n\\texttt{{inn}}=\\text{the $m_{i}^{th}$ element of }\\mathcal{P}_{inn}\nâŠ³\n\\triangleright\nInnovation\n10:\nres\n=\n[\nu\nj\nm\ni\n,\nv\nj\nm\ni\n]\nT\nâˆ’\ng\nâ€‹\n(\nx\n^\nt\n)\n\\texttt{{res}}=[u_{j_{m_{i}}},v_{j_{m_{i}}}]^{\\textmd{T}}-g(\\hat{\\texttt{{x}}}^{t})\nâŠ³\n\\triangleright\nResidual\n11:\nğšº\n^\nğ¯\nt\n=\nğšº\n^\nğ¯\nt\n\\mathbf{\\hat{\\Sigma}_{v}}^{t}=\\mathbf{\\hat{\\Sigma}_{v}}^{t}\n+\n12:\n1\nâˆ’\nÎ±\nf\nm\nk\nâ€‹\n(\nres\nâ‹…\nres\nT\n+\nH\nm\ni\nâ€‹\nj\nm\ni\nâ€‹\nğšº\n^\nğ±\nt\nâˆ’\n1\nâ€‹\nH\nm\ni\nâ€‹\nj\nm\ni\nT\n)\n\\frac{1-\\alpha_{f}}{m_{k}}(\\texttt{{res}}\\cdot\\texttt{{res}}^{\\textmd{T}}+\\text{{H}}_{m_{i}j_{m_{i}}}\\mathbf{\\hat{\\Sigma}_{x}}^{t-1}\\text{{H}}_{m_{i}j_{m_{i}}}^{\\textmd{T}})\n13:\nC\nm\ni\nâ€‹\nj\nm\ni\n=\nH\nm\ni\nâ€‹\nj\nm\ni\nâ€‹\nğšº\n^\nğ±\nt\nâˆ’\n1\nâ€‹\nH\nm\ni\nâ€‹\nj\nm\ni\nT\n+\nğšº\n^\nğ¯\nt\nâˆ’\n1\n\\text{{C}}_{m_{i}j_{m_{i}}}=\\text{{H}}_{m_{i}j_{m_{i}}}\\mathbf{\\hat{\\Sigma}_{x}}^{t-1}\\text{{H}}_{m_{i}j_{m_{i}}}^{\\textmd{T}}+\\mathbf{\\hat{\\Sigma}_{v}}^{t-1}\n14:\nK\nm\ni\nâ€‹\nj\nm\ni\n=\nğšº\n^\nğ±\nt\nâˆ’\n1\nâ€‹\nH\nm\ni\nâ€‹\nj\nm\ni\nT\nâ€‹\nC\nm\ni\nâ€‹\nj\nm\ni\nâˆ’\n1\n\\text{{K}}_{m_{i}j_{m_{i}}}=\\mathbf{\\hat{\\Sigma}_{x}}^{t-1}\\text{{H}}_{m_{i}j_{m_{i}}}^{\\textmd{T}}\\text{{C}}_{m_{i}j_{m_{i}}}^{-1}\n15:\nğšº\n^\nğ\nt\n=\nğšº\n^\nğ\nt\n+\n1\nâˆ’\nÎ±\nf\nm\nk\nâ€‹\nK\nm\ni\nâ€‹\nj\nm\ni\nâ‹…\ninn\nâ‹…\ninn\nT\nâ‹…\nK\nm\ni\nâ€‹\nj\nm\ni\nT\n\\mathbf{\\hat{\\Sigma}_{e}}^{t}=\\mathbf{\\hat{\\Sigma}_{e}}^{t}+\\frac{1-\\alpha_{f}}{m_{k}}\\text{{K}}_{m_{i}j_{m_{i}}}\\cdot\\texttt{{inn}}\\cdot\\texttt{{inn}}^{\\textmd{T}}\\cdot\\text{{K}}_{m_{i}j_{m_{i}}}^{\\textmd{T}}\n16:\nend\nfor\n17:\nreturn\nx\n^\nt\n,\nğšº\n^\nğ±\nt\n,\nğšº\n^\nğ\nt\n,\nğšº\n^\nğ¯\nt\n\\hat{\\texttt{{x}}}^{t},\\mathbf{\\hat{\\Sigma}_{x}}^{t},\\mathbf{\\hat{\\Sigma}_{e}}^{t},\\mathbf{\\hat{\\Sigma}_{v}}^{t}\n18:\nend\nprocedure\nII-F\n3\nParticle Filter\nThe implementation of Particle Filter (PF) for state estimation is presented in Algorithm.\n5\n. Compared with AEKF, PF further reduces the procedure of measurement model linearization and usually demonstrates stronger resilience in the face of non-Gaussian noises. This is achieved through consistent particle weight updates and particle resampling. However, with an increase in the number of particles selected, the computational cost rises sharply\n[\n11\n]\n. Moreover, it sometimes incurs particle degeneration problem\n[\n33\n]\n, where only a handful of particles are assigned with infinitely high weights, representing the actual state, with the rest particles assigned with near-zero weights. In this case, PF fails to provide an accurate state estimation.\nAlgorithm 5\nParticle Filter\n1:\nx\n^\nt\nâˆ’\n1\n\\hat{\\texttt{{x}}}^{t-1}\n,\nğšº\n^\nğ±\nt\nâˆ’\n1\n\\mathbf{\\hat{\\Sigma}_{x}}^{t-1}\n,\nâ„‹\nf\nâ€‹\ni\nâ€‹\nn\nâ€‹\na\nâ€‹\nl\n\\mathcal{H}_{final}\n,\nN\np\nN_{p}\n,\nN\ne\nâ€‹\nf\nâ€‹\nf\nN_{eff}\n,\nğ’«\no\nâ€‹\nb\nâ€‹\ns\n\\mathcal{P}_{obs}\n2:\nx\n^\nt\n\\hat{\\texttt{{x}}}^{t}\n,\nğšº\n^\nğ±\nt\n\\mathbf{\\hat{\\Sigma}_{x}}^{t}\n3:\nprocedure\nPF\n(\nx\n^\nt\nâˆ’\n1\n)\n\\hat{\\texttt{{x}}}^{t-1})\n,\nğšº\n^\nğ±\nt\nâˆ’\n1\n\\mathbf{\\hat{\\Sigma}_{x}}^{t-1}\n,\nâ„‹\nf\nâ€‹\ni\nâ€‹\nn\nâ€‹\na\nâ€‹\nl\n\\mathcal{H}_{final}\n)\n4:\nfor\nall particles\ndo\n5:\nx\n^\np\nâ€‹\nt\nt\n\\hat{\\texttt{{x}}}^{t}_{pt}\n=\nG\nâ€‹\na\nâ€‹\nu\nâ€‹\ns\nâ€‹\ns\nâ€‹\ni\nâ€‹\na\nâ€‹\nn\nâ€‹\n(\nx\n^\nt\nâˆ’\n1\n,\nğšº\n^\nğ±\nt\nâˆ’\n1\n)\nGaussian(\\hat{\\texttt{{x}}}^{t-1},\\mathbf{\\hat{\\Sigma}_{x}}^{t-1})\n6:\nâ€–\ninn\np\nâ€‹\nt\nâ€–\n2\nâ†\n||\\texttt{{inn}}_{pt}||_{2}\\leftarrow\nupdate from\nx\n^\np\nâ€‹\nt\nt\n\\hat{\\texttt{{x}}}^{t}_{pt}\n7:\nw\np\nâ€‹\nt\n=\n1\nâ€–\ninn\np\nâ€‹\nt\nâ€–\n2\nw_{pt}=\\frac{1}{||\\texttt{{inn}}_{pt}||_{2}}\nâŠ³\n\\triangleright\nweight update\n8:\nend\nfor\n9:\nparticle weight list\n[\nw\np\nâ€‹\nt\n]\n[w_{pt}]\nnormalisation\n10:\nx\n^\nt\nâ†\n\\hat{\\texttt{{x}}}^{t}\\leftarrow\nâˆ‘\nx\n^\np\nâ€‹\nt\nt\nâ‹…\nw\np\nâ€‹\nt\n\\sum\\hat{\\texttt{{x}}}^{t}_{pt}\\cdot w_{pt}\n, across all\nN\np\nN_{p}\nparticles\n11:\nstratified resampling\nif\n1\nâˆ‘\nw\np\nâ€‹\nt\n2\n<\nN\ne\nâ€‹\nf\nâ€‹\nf\n\\frac{1}{\\sum w^{2}_{pt}}<N_{eff}\n12:\nreturn\nx\n^\nt\n\\hat{\\texttt{{x}}}^{t}\n,\nğšº\n^\nğ±\nt\n=\nğšº\n^\nğ±\nt\nâˆ’\n1\n\\mathbf{\\hat{\\Sigma}_{x}}^{t}=\\mathbf{\\hat{\\Sigma}_{x}}^{t-1}\n13:\nend\nprocedure\nIII\nExperiments and results\nIII-A\nDataset and hardware setup\nThe proposed algorithm framework was evaluated on different publicly available datasets, including multiple video recordings from both the SuPer dataset\n1\n1\n1\nhttps://sites.google.com/ucsd.edu/super-framework/home\nand the SurgPose dataset\n[\n34\n]\n. These datasets contain demonstrations of various da Vinci instruments performing either\nin-vitro\ntasks on tissue phantoms or\nex-vivo\ntasks on animal organs, with raw kinematics data recorded directly from the encoders. All codes for algorithm implementation and evaluation were written in Python 3.10 and run on HP Z2 Tower G9 workstation without GPU acceleration.\nIII-B\nEvaluation of JCBB data association\nVideo 32 from the SurgPose dataset was first used to validate the effectiveness of the JCBB feature association and visibility algorithm blocks. The video contains a total of 1,001 frames, in which two Long Needle Drivers (LND), attached to PSM1 and PSM3 respectively, operate on chicken gizzard. Illustrations of calibration performance are shown in Fig.\n7\n. In all video recordings within the SurgPose dataset, key points are marked on instruments using ultraviolet reactive paint and are labeled from 1 to 14, where labels 1-7 correspond to the PSM1 instrument, and labels 8-14 correspond to the PSM3 instrument. However, this labeling convention does not distinguish the side of the instrument to which each key point belongs. For example, key point 1 may refer to either â€œrbâ€ or â€œrfâ€ under the convention defined in this paper. Additionally, in some frames, key points 6 and 7 correspond to â€œpl/prâ€ and â€œrl/rrâ€, respectively, while in other frames these correspondences are reversed. Therefore, prior to validation, the key points were manually relabeled to match the convention adopted in this paper. Video 32 was selected because it features minimal instrument rotation, with one dominant instrument visible to the camera for each tool, which simplifies the relabeling process. The initial hand-eye calibration matrix was calculated via the RANSAC PnP algorithm\n[\n10\n]\n, using manually labeled 2D-3D correspondences from the first one hundred frames.\nKey points feature association results using the JCBB algorithm with different estimators are presented in Fig.\n8\n. The results demonstrate the high key points association accuracy achieved by the JCBB algorithm accompanied by visibility check. As shown in Fig.\n8\n, all labeled key points belonging to the PSM1 and PSM3 instruments appeared in all 1,001 frames. During data analysis, three scenarios were considered: (i) correct match, where the observed key point was matched to the correct label, (ii) mismatch, where the observed key point was matched to an incorrect label, and (iii) non-match, where the observed key point was omitted from the data association, such case not shown in the bars. It is observed that, with the inclusion of the visibility check block, the accuracy of feature association increased for all estimator approaches. This is primarily because after the visibility check, at least half of the candidate key points were removed, which effectively reduced the number of outliers in the candidate group. It is also noted that, compared to the EKF and PF approaches, the AEKF approach produced the largest number of non-match cases. This can be explained by the nature of the AEKF, which particularly adjusts the state and measurement covariances on-the-fly. These varying covariance matrices can affect the JCBB feature association results by influencing the computation of both the individual and joint Mahalanobis distances between candidate key point pairs. Consequently, key points associations established under static covariances may no longer satisfy the required thresholds once the variances are changed, leading to an increased number of non-match cases. However, the rise of non-matches does not necessarily compromise the overall calibration accuracy, as shown in Fig.\n9\n; on the contrary, it can significantly accelerate the entire algorithm pipeline. Fig.\n10\nshows that, with the visibility check, the overall algorithm implementation time was significantly reduced for all estimators. For the EKF and AEKF approaches, the filter implementation time was negligible compared to the feature association time. In contrast, for the PF approach, the filter implementation time was substantial and was determined by the number of selected particles.\nFigure 7\n:\nIllustrations of calibration performance. Overlays of key points, tool edges, and the tool skeleton are used to illustrate calibration performance. They show that, even in the presence of large initial calibration errors, reflected by disorientated and out-of-view tool overlays, the JCBB algorithm is still capable of establishing feature associations, resulting in effective calibration. It is noticed that, despite using different calibration approaches, the skeleton of the left instrument after calibration in video 5 does not align with its visual observation, indicating that the estimated RCM position is incorrect.\nFigure 8\n:\nSurgPose 32, feature association analysis. The three images in the top row and the three images in the bottom row show the key points association results of three estimators for PSM1 and PSM3, respectively.\nFigure 9\n:\nSurgPose video 32, 3D errors comparison. The two images in the top row show 3D errors for associated key points with the visibility check, while the two images in the bottom row show 3D errors without VS. It is observed that, without the visibility check, an increased number of incorrect key points associations were found, resulting in larger 3D errors.\nFigure 10\n:\nSurgPose video 32, time comparison. Compared to the EKF and AEKF approaches, the PF approach required more time for filter implementation. With the visibility check, the overall implementation time for all filter approaches was significantly reduced.\nIII-C\nEvaluation of 3D tracking performance\nBased on the key points association results, pixel discrepancies are fed into estimators, which in turn produce estimates of the calibrated hand-eye transformation matrix. Subsequently, the 3D positions of these key points can be estimated through forward kinematics and the calibrated hand-eye transformation matrix. The SurgPose dataset provides pixel coordinates of labeled key points on stereo images, as well as intrinsic camera parameters. Therefore, the ground-truth 3D position of all labeled key points can be obtained via triangulation implemented in OpenCV\n[\n3\n]\n. The reconstructed 3D positions of associated key points using different estimators are compared and analyzed. The performances of EKF, AEKF, and PF are also compared with that of the RANSAC PnP algorithm, which incrementally takes input as all 2D-3D key points correspondences from the past history. Video 5 was chosen for evaluation, featuring two LNDs in operation. To examine the impact of initial calibration errors, a set of initializations with varying levels of accuracy were prepared. To achieve this, the number of frames used for the initial calibration was set to 10, 50, 100 and 200, respectively. The results for key points 3D position reconstruction are presented in Fig.\n11\n. It shows that, with an increase of the initial frames, the 3D reconstruction errors for all key points on the PSM1 instrument decreased for all estimators, despite still being less accurate than the PnP method. This implies that these estimators are sensitive to initial calibration errors. The results also suggest that, for the AEKF approach, key point mismatches occurred when the number of initial frames was not sufficient, as reflected by the two additional key point categories compared to the other approaches. In contrast, for the PSM3 instrument, the 3D reconstruction accuracy did not improve significantly with more calibration frames, implying that the initial calibration result derived from the first 10 frames was sufficiently accurate. It is also noteworthy that the reconstruction accuracy of the estimator approaches exceeded that of the PnP approach, suggesting the advantage of estimator-based approaches when initial calibration errors are small. It is suspected that, the initial calibration accuracy for the PSM3 instrument was higher than that for the PSM1 instrument, despite using the same number of frames, because more visible key points were located on different sides of the PSM3 instrument, which made the initial calibration result more robust across the trial.\nFigure 11\n:\nSurgPose video 5, Initial calibration frame analysis. Mean errors are labeled on the images. ANOVA tests were conducted on the key point errors produced by different approaches, and all p-values were all smaller than\n10\nâˆ’\n5\n10^{-5}\n.\nIII-D\nEvaluation of robustness against disturbances\nTo evaluate the performance of different estimator-based approaches under sudden disturbances to the hand-eye transformation matrix, as occurs when the camera is abruptly repositioned, random disturbances of varying magnitudes were added to the hand-eye calibration matrix every 25 frames. Disturbances are categorized as â€œlowâ€, â€œmediumâ€, and â€œhighâ€. The â€œlowâ€ level applies random rotation noises of\nÂ±\n1\n\\pm 1\ndegrees to all three rotation components, and random translation noises of\nÂ±\n1\n\\pm 1\ncm to all three translation components. Similarly, the â€œmediumâ€ level applies random rotation noises of\nÂ±\n3\n\\pm 3\ndegrees and translation noises of\nÂ±\n3\n\\pm 3\ncm, and the â€œhighâ€ level applies random rotation noises of\nÂ±\n5\n\\pm 5\ndegrees and random translation noises of\nÂ±\n5\n\\pm 5\ncm. Video 6 from the SurgPose dataset was selected, featuring two LNDs operating on a pork liver, and the number of initial calibration frames was set to 100. Fig.\n12\nshows the 2D pixel association errors when no disturbances were added. Fig.\n13\nshows the performance of different estimators under random high-level disturbances. It shows that, compared to when no disturbances were added, larger 3D errors were found for all three estimator approaches, whereas the PnP method produced more consistent results. This difference can be attributed to the fact that estimators require sequential frames to gradually reduce calibration errors, whereas the PnP method relies only on the current frame and historical correspondences. Similar results were obtained for cases when â€œlowâ€ and â€œmediumâ€ levels of disturbances were added.\nFigure 12\n:\nSurgPose video 6, 2D pixel errors. The 2D key point pixel association errors for the PSM1 and PSM3 instruments, with different initial calibration frames, are presented. It is noticed that there is a significant decrease in PSM3 pixel errors for AEKF approach as the number of initial calibration frames rises to 200.\nFigure 13\n:\nSurgPose video 6, disturbance analysis, high level. The 3D reconstruction errors for all key points on the PSM1 and PSM3 instruments, with and without applied disturbances, are presented. Mean errors are labeled in the images. The two images in the bottom row show the trends of the six variables that constitute the on-the-fly calibrated hand-eye transformation matrix for the PSM1 and PSM3 instruments, respectively. Spikes appear every 25 frames, matching the frequency at which high-level disturbances were introduced.\nIII-E\nEvaluation of robustness against measurement errors\nPerformances of different estimators were also assessed on the SuPer dataset. Unlike the SurgPose dataset, where ground truth pixel coordinates of key points are directly provided, in the SuPer dataset, key points on the instrument are labeled using blue markers, and their pixel coordinates are extracted via HSV color masking, which introduces more measurement errors. As there lacks ground truth results for the 3D key points positions, the performance of three estimators are compared with that of the PnP method. Comparison of the reconstructed hand-eye calibration matrix on different video datasets are shown in Table.\nI\n.\nTABLE I\n:\nComparison of calibrated\nT\nc\nâ€‹\nr\n\\text{{T}}_{cr}\nresults on different video datasets.\nT\nc\nâ€‹\nr\n\\text{{T}}_{cr}\nvalues obtained from estimator-based approaches are compared with those produced by the PnP-RANSAC method.\nDataset\n|\nÎ”\nâ€‹\nt\n|\n|\\Delta\\texttt{{t}}|\n(mm)\nEKF\n|\nÎ”\nâ€‹\nt\n|\n|\\Delta\\texttt{{t}}|\n(mm)\nAEKF\n|\nÎ”\nâ€‹\nt\n|\n|\\Delta\\texttt{{t}}|\n(mm)\nPF\n|\nÎ”\nâ€‹\nr\n|\n|\\Delta\\texttt{{r}}|\n(rads)\nEKF\n|\nÎ”\nâ€‹\nr\n|\n|\\Delta\\texttt{{r}}|\n(rads)\nAEKF\n|\nÎ”\nâ€‹\nr\n|\n|\\Delta\\texttt{{r}}|\n(rads)\nPF\nSuPer Dataset Grasp 1\n2.74\nÂ±\n1.13\n2.74\\pm 1.13\n2.65\nÂ±\n1.30\n2.65\\pm 1.30\n14.71\nÂ±\n4.14\n14.71\\pm 4.14\n0.09\nÂ±\n0.03\n0.09\\pm 0.03\n0.14\nÂ±\n0.04\n0.14\\pm 0.04\n0.18\nÂ±\n0.02\n0.18\\pm 0.02\nSuPer Dataset Grasp 2\n4.51\nÂ±\n4.12\n4.51\\pm 4.12\n8.52\nÂ±\n2.31\n8.52\\pm 2.31\n16.55\nÂ±\n2.46\n16.55\\pm 2.46\n0.10\nÂ±\n0.04\n0.10\\pm 0.04\n0.14\nÂ±\n0.02\n0.14\\pm 0.02\n0.18\nÂ±\n0.02\n0.18\\pm 0.02\nSuPer Dataset Grasp 3\n11.34\nÂ±\n3.63\n11.34\\pm 3.63\n5.55\nÂ±\n1.85\n5.55\\pm 1.85\n16.24\nÂ±\n3.80\n16.24\\pm 3.80\n0.14\nÂ±\n0.03\n0.14\\pm 0.03\n0.21\nÂ±\n0.02\n0.21\\pm 0.02\n0.19\nÂ±\n0.03\n0.19\\pm 0.03\nSuPer Dataset Grasp 4\n6.67\nÂ±\n6.04\n6.67\\pm 6.04\n7.06\nÂ±\n5.73\n7.06\\pm 5.73\n21.43\nÂ±\n6.63\n21.43\\pm 6.63\n0.23\nÂ±\n0.32\n0.23\\pm 0.32\n0.21\nÂ±\n0.31\n0.21\\pm 0.31\n0.27\nÂ±\n0.31\n0.27\\pm 0.31\nSuPer Dataset Grasp 5\n5.91\nÂ±\n9.12\n5.91\\pm 9.12\n7.63\nÂ±\n9.06\n7.63\\pm 9.06\n20.04\nÂ±\n8.93\n20.04\\pm 8.93\n0.23\nÂ±\n0.28\n0.23\\pm 0.28\n0.21\nÂ±\n0.28\n0.21\\pm 0.28\n0.27\nÂ±\n0.28\n0.27\\pm 0.28\nSurg 5, PSM1, init 200\n13.90\nÂ±\n14.40\n13.90\\pm 14.40\n14.90\nÂ±\n14.02\n14.90\\pm 14.02\n14.19\nÂ±\n14.52\n14.19\\pm 14.52\n0.14\nÂ±\n0.13\n0.14\\pm 0.13\n0.20\nÂ±\n0.13\n0.20\\pm 0.13\n0.23\nÂ±\n0.13\n0.23\\pm 0.13\nSurg 5, PSM3, init 200\n2.98\nÂ±\n2.47\n2.98\\pm 2.47\n3.04\nÂ±\n2.97\n3.04\\pm 2.97\n6.82\nÂ±\n3.38\n6.82\\pm 3.38\n0.02\nÂ±\n0.02\n0.02\\pm 0.02\n0.02\nÂ±\n0.02\n0.02\\pm 0.02\n0.02\nÂ±\n0.02\n0.02\\pm 0.02\nSurg 6, PSM1, init 100\n3.12\nÂ±\n5.28\n3.12\\pm 5.28\n4.11\nÂ±\n4.69\n4.11\\pm 4.69\n6.15\nÂ±\n5.17\n6.15\\pm 5.17\n0.03\nÂ±\n0.04\n0.03\\pm 0.04\n0.04\nÂ±\n0.04\n0.04\\pm 0.04\n0.07\nÂ±\n0.04\n0.07\\pm 0.04\nSurg 6, PSM3, init 100\n2.84\nÂ±\n3.37\n2.84\\pm 3.37\n8.19\nÂ±\n4.00\n8.19\\pm 4.00\n16.22\nÂ±\n6.86\n16.22\\pm 6.86\n0.03\nÂ±\n0.03\n0.03\\pm 0.03\n0.07\nÂ±\n0.03\n0.07\\pm 0.03\n0.09\nÂ±\n0.03\n0.09\\pm 0.03\nIV\nDiscussions\nThrough extensive experiments on different video datasets, the effectiveness of the JCBB key point association algorithm, together with the visibility block, were validated. No matter whether it is estimator-based calibration approach or direct PnP method that is adopted, reliable key point association is crucial towards obtaining accurate on-the-fly hand-eye calibration results, and thus 3D tool tracking/pose reconstruction. The incorporation of the visibility check block not only reduces the number of outliers in the candidate list, thereby increasing feature association accuracy, but also accelerates the entire pipeline implementation, achieving a rate of over 100 Hz for the AEKF approach, which is faster than both the kinematics data streaming speed and the camera refresh rate, thus satisfying the requirement for interactive deployment. The practicability of key point association-based calibration approaches is also supported by the versatility of key point detection methods, encompassing both training based marker-free approaches and marker-based approaches. For example, by applying infrared fluorescent dye onto instrument key points, their pixel coordinates can be directly detected in the Firefly mode using the dVRK Xi\n[\n30\n]\n, without the requirement for pre-training. Additionally, this procedure is generalizable to different surgical instruments on different surgical platforms as long as their CAD models are available.\nThe performance of different key point association-based approaches was evaluated under challenges including initial hand-eye calibration errors, random intra-operative disturbances, and key point measurement errors. The results show that estimator-based approaches are more susceptible to the accuracy of initial calibration. Among the EKF, AEKF, and PF approaches, both the AEKF and PF approaches were designed to accommodate non-Gaussian noises, and their performance in 3D position reconstruction was shown to be better than that of the EKF. However, the computational time for the PF approach was significantly higher. Table.\nI\ncompares the estimated\nT\nc\nâ€‹\nr\n\\text{{T}}_{cr}\nbetween estimator-based approaches and the PnP-RANSAC method and shows significant discrepancies in the translational component. It can be explained from the observation that the PnP method tends to generate less consistent calibration results, as reflected by rapid changes in tool overlay, as shown in the videos included in the supporting material. Since all estimator-based approaches rely on an initial calibration result, it is suggested here that the PnP method is more suitable for conducting initial hand-eye calibration. The PF approach is suitable for extremely noisy environments where sufficient features can be observed, while the AEKF approach is more widely applicable to situations in which computational power is limited.\nCompared with other estimator-based approaches that use key point associations, Ye\net al.\n[\n37\n]\nreported an average pose reconstruction error of\n2.83\nÂ±\n2.19\n2.83\\pm 2.19\n(mm) in translation, and\n0.13\nÂ±\n0.10\n0.13\\pm 0.10\n(rads) in rotation, by using their EKF framework. Although no ground truth values for the instrument pose were provided in the original datasets, a common practice to find the ground truth pose is to run the PnP algorithm after manually labeling 2D key points on the image. Therefore, Table.\nI\nprovides a rough metric for comparison. The large discrepancy in\nÎ”\nâ€‹\nt\n\\Delta\\texttt{{t}}\nwas caused by the fact that the estimators solely focused on reducing pixel association errors but failed to account for the RCM constraint. This scenario is reflected by a mirrored tool skeleton overlay on the image, as shown in videos included in the supporting material. On the other hand, when the accuracy of 3D depth estimation takes priority, estimators that rely on key points can outperform those that rely on differential rendering and silhouette matching\n[\n12\n,\n19\n,\n36\n]\nin accuracy and speed.\nCompared with other recent pose estimation methods that do not rely on estimators or the PnP method, Fan\net al.\n[\n9\n]\nreported an average error of\n2.81\n2.81\nmm for 3D instrument joint position estimation on their semi-synthetic dataset, by using a reinforcement-learning based approach. This level of accuracy could also be achieved using estimator-based approaches on certain datasets, as shown in Fig.\n11\n. Although the JCBB feature association and visibility check algorithms were originally proposed for estimators-based approaches, they can also be valuable for other tool pose estimation algorithms, because they remove ambiguity in key points detected on the two symmetrical sides of the instrument, which is important when large articulations occur. Additionally, they can also be helpful in establishing inter-frame key point correspondences in an efficient way, which is an essential component in\n[\n35\n]\n.\nSince there also exist studies that address the pose tracking problem via direct calibration of joint angles\n[\n15\n,\n18\n]\n, it would be useful to conduct comparisons in the joint space. As shown in\n[\n15\n]\n, given the end-effector pose and the 3D position of joint 4 in the camera frame, joint angles 1-6 can be computed analytically. However, due to a lack of ground-truth instrument poses in the datasets, joint-space comparisons are difficult to evaluate in this study. To enable more extensive evaluations, it is hoped that future datasets will provide ground-truth results for both the instrument pose and the robot joint angles.\nV\nCONCLUSIONS\nIn this work, we proposed a generalizable on-the-fly hand-eye calibration framework with the aim of improving pose estimation/tool localization accuracy for the da Vinci surgical instruments, leveraging both raw kinematics data and key point features from monocular images. The framework consists of a JCBB feature association algorithm block and a calibration algorithm block. Without relying on any pre-training, the JCBB algorithm proved capable of establishing associations between 2D key points and their corresponding 3D positions with high accuracy, even in the presence of outliers. The performance of the JCBB algorithm was shown to be further enhanced with the fusion of the visibility check block, which effectively removed invisible key points from the candidate list. Given the established 2D-3D correspondences, several options were made available for the calibration algorithm block, including the EKF, AEKF, PF, and the PnP method. We provided a pool of calibration algorithms out of the concern that a single approach may fail to accommodate the different noise distributions that arise in various different surgical scenarios, such as large initial errors, sudden disturbances, and large measurement errors.\nThrough extensive experiments on different publicly available video datasets, the effectiveness of the proposed framework was validated and compared with other approaches. One issue identified was that the accuracy of the translational component of the estimated hand-eye calibration matrix remained inconsistent. This was suspected to be caused by the fact that estimator-based approaches prioritized reducing 2D association errors without accounting for physical constraints. Therefore, in future work, we intend to incorporate the RCM constraint into the framework. Additionally, we intend to test the proposed framework in more clinically realistic scenarios and expand the analysis to the joint space. Although the entire framework was programmed in Python for fast prototyping, to allow for better time performance, it will be rewritten in C++ and released as open source.\nVI\nAPPENDIX\nVI-A\nParameter selection\nParameters used in the experiments are listed in Table.\nII\nand\nIII\n.\nTABLE II\n:\nParameter values, JCBB algorithm block\nParameter\nExplanation\nValue\nÎ±\n\\alpha\nconfidence level for\nÏ‡\n\\chi\ntest\n0.975\nÎ³\n\\gamma\nratio test threshold\n100.00\nğšº\nğ\n\\mathbf{\\Sigma_{e}}\nstate covariance\ndiag{5,5,5,0.25,\n0.25,0.25}*1e-2\nğšº\nğ¯\n\\mathbf{\\Sigma_{v}}\nmeasurement covariance\ndiag{50,50}\nTABLE III\n:\nParameter values, estimators\nParameter\nExplanation\nValue\nÎ±\nf\n\\alpha_{f}\nAEKF forget factor\n0.6\nN\np\nN_{p}\nparticle number\n1000\nN\ne\nâ€‹\nf\nâ€‹\nf\nN_{eff}\neffective particle number\n100\nğšº\nğ\n\\mathbf{\\Sigma_{e}}\nstate covariance\ndiag{5,5,5,0.25,\n0.25,0.25}*1e-6\nğšº\nğ¯\n\\mathbf{\\Sigma_{v}}\nmeasurement covariance\ndiag{25,25}\nReferences\n[1]\nM. Allan, S. Ourselin, D. J. Hawkes, J. D. Kelly, and D. Stoyanov\n(2018)\n3-d pose estimation of articulated instruments in robotic minimally invasive surgery\n.\nIEEE transactions on medical imaging\n37\n(\n5\n),\npp.Â 1204â€“1213\n.\nCited by:\nÂ§\nI-B\n2\n.\n[2]\nJ. Blanco, J. Gonzalez-JimÃ©nez, and J. Fernandez-Madrigal\n(2012)\nAn alternative to the mahalanobis distance for determining optimal correspondences in data association\n.\nIEEE Transactions on Robotics\n28\n(\n4\n),\npp.Â 980â€“986\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-C\n2\n.\n[3]\nG. Bradski\n(2000)\nThe OpenCV Library\n.\nDr. Dobbâ€™s Journal of Software Tools\n.\nCited by:\nÂ§\nIII-C\n.\n[4]\nJ. Canny\n(1986)\nA computational approach to edge detection\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\nPAMI-8\n(\n6\n),\npp.Â 679â€“698\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nI-B\n2\n.\n[5]\nJ. Cartucho, C. Wang, B. Huang, D. S. Elson, A. Darzi, and S. Giannarou\n(2022)\nAn enhanced marker pattern that achieves improved accuracy in surgical tool tracking\n.\nComputer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization\n10\n(\n4\n),\npp.Â 400â€“408\n.\nCited by:\nÂ§\nI-B\n1\n.\n[6]\nZ. Cui, J. Cartucho, S. Giannarou, and F. Rodriguez y Baena\n(2023)\nCaveats on the first-generation da vinci research kit: latent technical constraints and essential calibrations\n.\nIEEE Robotics & Automation Magazine\n(\n),\npp.Â 2â€“17\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[7]\nC. Dâ€™Ettorre, A. Stilli, G. Dwyer, M. Tran, and D. Stoyanov\n(2021)\nAutonomous pick-and-place using the dvrk\n.\nInternational Journal of Computer Assisted Radiology and Surgery\n16\n(\n7\n),\npp.Â 1141â€“1149\n.\nCited by:\nÂ§I\n.\n[8]\nK. Daniilidis\n(1999)\nHand-eye calibration using dual quaternions\n.\nThe International Journal of Robotics Research\n18\n(\n3\n),\npp.Â 286â€“298\n.\nCited by:\nÂ§\nI-B\n1\n.\n[9]\nK. Fan, Z. Chen, Q. Liu, G. Ferrigno, and E. De Momi\n(2024)\nA reinforcement learning approach for real-time articulated surgical instrument 3d pose reconstruction\n.\nIEEE Transactions on Medical Robotics and Bionics\n.\nCited by:\nÂ§\nI-B\n3\n,\nÂ§IV\n.\n[10]\nM. A. Fischler and R. C. Bolles\n(1981)\nRandom sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography\n.\nCommunications of the ACM\n24\n(\n6\n),\npp.Â 381â€“395\n.\nCited by:\nÂ§\nIII-B\n.\n[11]\nD. Fox\n(2001)\nKLD-sampling: adaptive particle filters\n.\nAdvances in neural information processing systems\n14\n.\nCited by:\nÂ§\nII-F\n3\n.\n[12]\nR. Hao, O. Ã–zgÃ¼ner, and M. C. Ã‡avuÅŸoÄŸlu\n(2018)\nVision-based surgical tool pose estimation for the da vinciÂ® robotic surgical system\n.\nIn\n2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nVol.\n,\npp.Â 1298â€“1305\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nI-B\n2\n,\nÂ§IV\n.\n[13]\nS. He, R. Bao, J. Li, P. E. Grant, and Y. Ou\n(2023)\nAccuracy of segment-anything model (sam) in medical image segmentation tasks\n.\nCoRR\n.\nCited by:\nÂ§\nI-B\n2\n.\n[14]\nR. A. Howard\n(2012)\nDynamic probabilistic systems, volume i: markov models\n.\nVol.\n1\n,\nCourier Corporation\n.\nCited by:\nÂ§\nII-C\n1\n.\n[15]\nM. Hwang, B. Thananjeyan, S. Paradis, D. Seita, J. Ichnowski, D. Fer, T. Low, and K. Goldberg\n(2020)\nEfficiently calibrating cable-driven surgical robots with rgbd fiducial sensing and recurrent neural networks\n.\nIEEE Robotics and Automation Letters\n5\n(\n4\n),\npp.Â 5937â€“5944\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n,\nÂ§IV\n.\n[16]\nP. Kazanzides, Z. Chen, A. Deguet, G. S. Fischer, R. H. Taylor, and S. P. DiMaio\n(2014-06-01)\nAn open-source research kit for the da vinci surgical system\n.\nIn\nIEEE Intl. Conf. on Robotics and Auto. (ICRA)\n,\nHong Kong, China\n,\npp.Â 6434â€“6439\n.\nCited by:\nÂ§\nI-A\n.\n[17]\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W. Lo,\net al.\n(2023)\nSegment anything\n.\narXiv preprint arXiv:2304.02643\n.\nCited by:\nÂ§\nI-B\n2\n.\n[18]\nB. Li, H. Lin, F. Zhong, and Y. Liu\n(2024)\nReal-time geometric joint uncertainty tracking for surgical automation on the dvrk system\n.\nIn\n2024 IEEE International Conference on Robotics and Biomimetics (ROBIO)\n,\npp.Â 2144â€“2148\n.\nCited by:\nÂ§IV\n.\n[19]\nZ. Liang, Z. Chiu, F. Richter, and M. C. Yip\n(2025)\nDifferentiable rendering-based pose estimation for surgical robotic instruments\n.\narXiv preprint arXiv:2503.05953\n.\nCited by:\nÂ§\nI-B\n2\n,\nÂ§\nI-B\n3\n,\nÂ§IV\n.\n[20]\nZ. Liang, K. Miyata, X. Liang, F. Richter, and M. C. Yip\n(2025)\nEfficient surgical robotic instrument pose reconstruction in real world conditions using unified feature detection\n.\narXiv preprint arXiv:2510.03532\n.\nCited by:\nÂ§\nI-B\n2\n.\n[21]\nB. Lu, B. Li, Q. Dou, and Y. Liu\n(2022)\nA unified monocular camera-based and pattern-free hand-to-eye calibration algorithm for surgical robots with rcm constraints\n.\nIEEE/ASME Transactions on Mechatronics\n27\n(\n6\n),\npp.Â 5124â€“5135\n.\nCited by:\nÂ§\nII-E\n.\n[22]\nJ. Lu, F. Richter, and M. C. Yip\n(2022)\nPose estimation for robot manipulators via keypoint optimization and sim-to-real transfer\n.\nIEEE Robotics and Automation Letters\n7\n(\n2\n),\npp.Â 4622â€“4629\n.\nCited by:\nÂ§\nI-B\n2\n.\n[23]\nG. J. McLachlan\n(1999)\nMahalanobis distance\n.\nResonance\n4\n(\n6\n),\npp.Â 20â€“26\n.\nCited by:\nÂ§\nII-C\n1\n.\n[24]\nR. Moccia, C. Iacono, B. Siciliano, and F. Ficuciello\n(2020)\nVision-based dynamic virtual fixtures for tools collision avoidance in robotic surgery\n.\nIEEE Robotics and Automation Letters\n5\n(\n2\n),\npp.Â 1650â€“1655\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nI-B\n2\n.\n[25]\nK. Okamura and F. C. Park\n(1996)\nKinematic calibration using the product of exponentials formula\n.\nRobotica\n14\n(\n4\n),\npp.Â 415â€“421\n.\nCited by:\nÂ§\nI-B\n1\n.\n[26]\nO. Ã–zgÃ¼ner, T. Shkurti, S. Huang, R. Hao, R. C. Jackson, W. S. Newman, and M. C. Ã‡avuÅŸoÄŸlu\n(2020)\nCamera-robot calibration for the da vinci robotic surgery system\n.\nIEEE Transactions on Automation Science and Engineering\n17\n(\n4\n),\npp.Â 2154â€“2161\n.\nCited by:\nÂ§\nI-B\n1\n.\n[27]\nK. Pachtrachai, F. Vasconcelos, F. Chadebecq, M. Allan, S. Hailes, V. Pawar, and D. Stoyanov\n(2018)\nAdjoint transformation algorithm for handâ€“eye calibration with applications in robotic assisted surgery\n.\nAnnals of biomedical engineering\n46\n(\n10\n),\npp.Â 1606â€“1620\n.\nCited by:\nÂ§\nI-B\n1\n.\n[28]\nK. Pachtrachai, F. Vasconcelos, P. Edwards, and D. Stoyanov\n(2021)\nLearning to calibrate-estimating the hand-eye transformation without calibration objects\n.\nIEEE Robotics and Automation Letters\n6\n(\n4\n),\npp.Â 7309â€“7316\n.\nCited by:\nÂ§\nI-B\n1\n.\n[29]\nF. Richter, J. Lu, R. K. Orosco, and M. C. Yip\n(2022)\nRobotic tool tracking under partially visible kinematic chain: a unified approach\n.\nIEEE Transactions on Robotics\n38\n(\n3\n),\npp.Â 1653â€“1670\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nI-B\n2\n,\nÂ§I\n,\nÂ§\nII-F\n1\n.\n[30]\nA. Schmidt, O. Mohareri, S. P. DiMaio, and S. E. Salcudean\n(2024)\nSurgical tattoos in infrared: a dataset for quantifying tissue tracking and mapping\n.\nIEEE Transactions on Medical Imaging\n43\n(\n7\n),\npp.Â 2634â€“2645\n.\nExternal Links:\nDocument\nCited by:\nÂ§IV\n.\n[31]\nM. Shah\n(2013)\nSolving the robot-world/hand-eye calibration problem using the kronecker product\n.\nJournal of Mechanisms and Robotics\n5\n(\n3\n),\npp.Â 031007\n.\nCited by:\nÂ§\nI-B\n1\n.\n[32]\nY.C. Shiu and S. Ahmad\n(1989)\nCalibration of wrist-mounted robotic sensors by solving homogeneous transform equations of the form ax=xb\n.\nIEEE Transactions on Robotics and Automation\n5\n(\n1\n),\npp.Â 16â€“29\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nI-B\n1\n.\n[33]\nS. Thrun\n(2002)\nProbabilistic robotics\n.\nCommunications of the ACM\n45\n(\n3\n),\npp.Â 52â€“57\n.\nCited by:\nÂ§\nII-F\n3\n.\n[34]\nZ. Wu, A. Schmidt, R. Moore, H. Zhou, A. Banks, P. Kazanzides, and S. E. Salcudean\n(2025)\nSurgpose: a dataset for articulated robotic surgical tool pose estimation and tracking\n.\narXiv preprint arXiv:2502.11534\n.\nCited by:\nÂ§\nIII-A\n.\n[35]\nS. Yang, Z. Wu, M. Hong, Q. Li, D. Shen, S. E. Salcudean, and Y. Jin\n(2025)\nInstrument-splatting: controllable photorealistic reconstruction of surgical instruments using gaussian splatting\n.\nIn\nInternational Conference on Medical Image Computing and Computer-Assisted Intervention\n,\npp.Â 301â€“311\n.\nCited by:\nÂ§\nI-B\n3\n,\nÂ§IV\n.\n[36]\nS. Yang and Z. Chua\n(2025)\nReal-time capable learning-based visual tool pose correction via differentiable simulation\n.\narXiv preprint arXiv:2505.08875\n.\nCited by:\nÂ§IV\n.\n[37]\nM. Ye, L. Zhang, S. Giannarou, and G. Yang\n(2016)\nReal-time 3d tracking of articulated tools for robotic surgery\n.\nIn\nMedical Image Computing and Computer-Assisted Interventionâ€“MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part I 19\n,\npp.Â 386â€“394\n.\nCited by:\nÂ§\nI-B\n2\n,\nÂ§IV\n.\n[38]\nZ. Zhang, L. Zhang, and G. Yang\n(2017)\nA computationally efficient method for handâ€“eye calibration\n.\nInternational journal of computer assisted radiology and surgery\n12\n(\n10\n),\npp.Â 1775â€“1787\n.\nCited by:\nÂ§\nI-B\n1\n.\n[39]\nZ. Zhao and Y. Liu\n(2009)\nA handâ€“eye calibration algorithm based on screw motions\n.\nRobotica\n27\n(\n2\n),\npp.Â 217â€“223\n.\nCited by:\nÂ§\nI-B\n1\n.\n[40]\nM. Zhou, M. Hamad, J. Weiss, A. Eslami, K. Huang, M. Maier, C. P. Lohmann, N. Navab, A. Knoll, and M. A. Nasseri\n(2018)\nTowards robotic eye surgery: marker-free, online hand-eye calibration using optical coherence tomography images\n.\nIEEE Robotics and Automation Letters\n3\n(\n4\n),\npp.Â 3944â€“3951\n.\nCited by:\nÂ§\nI-B\n1\n.\n[41]\nH. Zhuang, Z. S. Roth, and R. Sudhakar\n(1994)\nSimultaneous robot/world and tool/flange calibration by solving homogeneous transformation equations of the form ax= yb\n.\nIEEE Transactions on Robotics and Automation\n10\n(\n4\n),\npp.Â 549â€“554\n.\nCited by:\nÂ§\nI-B\n1\n.",
    "preview_text": "In Robot-Assisted Minimally Invasive Surgery (RMIS), accurate tool localization is crucial to ensure patient safety and successful task execution. However, this remains challenging for cable-driven robots, such as the da Vinci robot, because erroneous encoder readings lead to pose estimation errors. In this study, we propose a calibration framework to produce accurate tool localization results through computing the hand-eye transformation matrix on-the-fly. The framework consists of two interrelated algorithms: the feature association block and the hand-eye calibration block, which provide robust correspondences for key points detected on monocular images without pre-training, and offer the versatility to accommodate various surgical scenarios by adopting an array of filter approaches, respectively. To validate its efficacy, we test the framework extensively on publicly available video datasets that feature multiple surgical instruments conducting tasks in both in vitro and ex vivo scenarios, under varying illumination conditions and with different levels of key point measurement accuracy. The results show a significant reduction in tool localization errors under the proposed calibration framework, with accuracies comparable to other state-of-the-art methods while being more time-efficient.\n\nOn-the-fly hand-eye calibration for the da Vinci surgical robot\nZejian Cui\n1\nand Ferdinando Rodriguez y Baena\n1\n*This work is supported by funding from the Department of Mechanical Engineering, Imperial College London\n1\nBoth authors are with the Mechatronics in Medicine Laboratory, the Hamlyn Centre for Robotics Surgery, Department of Mechanical Engineering, Imperial College London, Exhibition Road, London, SW7 2AZ, UK\n{zejian.cui19, f.rodriguez}@imperial.ac.uk\nAbstract\nIn Robot-Assisted Minimally Invasive Surgery (RMIS), accurate tool localization is crucial to ensure patient safety and successful task execution. However, this remains challenging for cable-driven robots, such a",
    "is_relevant": false,
    "relevance_score": 0.0,
    "extracted_keywords": [
        "hand-eye calibration",
        "surgical robot",
        "tool localization",
        "da Vinci robot",
        "Robot-Assisted Minimally Invasive Surgery"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºè¾¾èŠ¬å¥‡æ‰‹æœ¯æœºå™¨äººçš„å®æ—¶æ‰‹çœ¼æ ¡å‡†æ¡†æ¶ï¼Œä»¥æé«˜å·¥å…·å®šä½ç²¾åº¦ï¼Œä½†ä¸å¼ºåŒ–å­¦ä¹ ã€VLAã€æ‰©æ•£æ¨¡å‹ç­‰å…³é”®è¯æ— å…³ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T10:58:28Z",
    "created_at": "2026-01-27T15:53:19.964144",
    "updated_at": "2026-01-27T15:53:19.964151",
    "recommend": 0
}