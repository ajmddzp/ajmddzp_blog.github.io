{
  "id": "2512.02013v1",
  "title": "ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation",
  "authors": [
    "Chenyang Gu",
    "Jiaming Liu",
    "Hao Chen",
    "Runzhong Huang",
    "Qingpo Wuwu",
    "Zhuoyang Liu",
    "Xiaoqi Li",
    "Ying Li",
    "Renrui Zhang",
    "Peng Jia",
    "Pheng-Ann Heng",
    "Shanghang Zhang"
  ],
  "abstract": "Vision-Language-Action (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation. However, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation. Therefore, we aim to endow a VLA model with the capability to infer the \"how\" process from the \"what\" outcomes, transforming goal states into executable procedures. In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution. Unlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training. ManualVLA demonstrates strong real-world performance, achieving an average success rate 32% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.",
  "url": "https://arxiv.org/abs/2512.02013v1",
  "html_url": "https://arxiv.org/html/2512.02013v1",
  "html_content": "ManualVLA: A Unified VLA Model for Chain-of-Thought\nManual Generation and Robotic Manipulation\nChenyang Gu\n1\n,\nJiaming Liu\n‚àó‚Ä†\n1\n,\nHao Chen\n‚àó‚Ä†\n2\n,\nRunzhong Huang\n‚àó\n1\n,\nQingpo Wuwu\n1\n,\nZhuoyang Liu\n1\n,\nXiaoqi Li\n1\n,\nYing Li\n1\n,\nRenrui Zhang\n2\n,\nPeng Jia\n3\n,\nPheng-Ann Heng\n2\n,\nShanghang Zhang\n‚úâ\n1\n1\nState Key Laboratory of Multimedia Information Processing,\nSchool of Computer Science,\nPeking University\n2\nThe Chinese University of Hong Kong\n3\nSimplexity Robotics\nProject web page:\nhttps://sites.google.com/view/maunalvla\nEqual contribution\n‚Ä†\nProject leader\n‚úâ\nCorresponding author.\nAbstract\nVision‚ÄìLanguage‚ÄìAction (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation.\nHowever, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation.\nTherefore, we aim to endow a VLA model with the capability to infer the ‚Äúhow‚Äù process from the ‚Äúwhat‚Äù outcomes, transforming goal states into executable procedures.\nIn this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution.\nUnlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation.\nTo alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training.\nManualVLA demonstrates strong real-world performance, achieving an average success rate 32% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.\n1\nIntroduction\nFigure 1\n:\nOverview.\n(a)\nLong-horizon tasks with predefined goal states, such as LEGO assembly or object rearrangement, pose a significant challenge for intelligent robots, as they require not only imagining procedural manuals but also executing precise manipulations based on them.\n(b)\nWe address such tasks by introducing ManualVLA, a unified VLA model built upon a MoT architecture, which enables coherent collaboration between multimodal manual and action generation via a designed Manual Chain-of-Thought.\nRecently, building on internet-scale pretrained vision-language models\n[\nalayrac2022flamingo\n,\nkaramcheti2024prismatic\n]\n, vision-language-action (VLA) models have emerged\n[\nrt22023arxiv\n,\nkim2024openvla\n]\nand are trained on robot demonstrations to predict control actions.\nThese models exhibit impressive capabilities in robotic scene understanding\n[\nyang2025instructvla\n,\nlin2025onetwovla\n,\nliu2024robomamba\n]\nwhile demonstrating strong generalization and manipulation performance in out-of-lab scenarios\n[\nintelligence2025pi05visionlanguageactionmodelopenworld\n,\nliu2025hybridvla\n,\nbjorck2025gr00t\n]\n.\nSuch advances have significantly accelerated progress toward developing generalist robotic agents.\nHowever, when confronted with long-horizon tasks that require precisely defined goal states (Figure\n1\n(a)\n), such as LEGO assembly or object rearrangement, VLA models remain highly challenged.\nThe difficulty arises from two aspects: (1) the VLA model must execute precise manipulation strictly aligned with the predefined final scene or object configuration, and (2) it must integrate long-horizon planning with fine-grained control, while maintaining generalization to diverse real-world environments.\nIn contrast, for humans, upon accumulating sufficient manipulation experience, they can perform long-horizon, goal-specific tasks independently, without relying on further demonstrations.\nFor example, humans can assemble LEGO structures by inferring the target configuration, or reorganize household objects according to a predefined spatial arrangement.\nThis ability stems from humans‚Äô intuitive inference of intermediate cues, reasoning over spatial and causal relations, and decomposition of tasks into coherent, subgoal-directed manipulation steps.\nRecently, some hierarchical methods have attempted to emulate this ability by relying on detailed manuals\n[\nlong2025checkmanual\n]\nor human demonstration videos\n[\npapagiannis2025r+\n,\njain2024vid2robot\n,\nbahl2022human\n,\nwang2024dexcap\n]\n. However, such approaches are often limited in generalization to unseen final goal states and increase the dependence on human involvement.\nThis naturally leads to a question:\n‚ÄúIs it possible to endow a VLA model with the capability to infer the procedural ‚Äúhow‚Äù from the desired ‚Äúwhat,‚Äù thereby transforming a predefined final goal into a sequence of coherent and precise execution steps?‚Äù\nTo this end, we propose\nManualVLA\n, a unified VLA model based on a Mixture-of-Transformers (MoT)\n[\nliang2024mixture\n]\narchitecture, capable of generating multimodal manuals and actions directly from a final goal state.\nSince only the final state is provided and intermediate steps are unknown, previous VLA models that map sensory inputs directly to actions struggle with such long-horizon tasks. In contrast, as shown in Figure\n1\n(b)\n, ManualVLA selectively activates planning and action experts within a unified framework for subgoal manual and action generation.\nSpecifically, ManualVLA is equipped with a planning expert to generate intermediate manuals that integrate images, position prompts, and textual instructions.\nThese manuals are then used in our proposed\nManual Chain-of-Thought (ManualCoT)\nreasoning strategy, which guides the action expert by treating each subgoal step as an explicit condition for precise execution.\nFurthermore, a cross-task shared attention mechanism between the two experts enables long-context interactions between manual-generation features and action generation, providing implicit guidance for coherent manipulation.\nTo train ManualVLA\n, we first leverage the generative capability of VLM model (e.g., Janus-Pro\n[\nwu2024janus\n]\n) and fine-tune it on self-collected and simulation-synthesized manual data to acquire the planning expert capability.\nDue to the substantial uncertainty in the final goal states, a large amount of data is required to train the model with sufficient world knowledge for effective task planning. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting\n[\n2023_8_08-3dgs_for_real_time_radiance_field_rendering\n]\n, which automatically generates manual data for planning expert training.\nMeanwhile, we pretrain the action expert on large-scale open-source robotic datasets comprising over 400K trajectories\n[\nopen_x_embodiment_rt_x_2023\n,\nkhazatsky2024droid\n]\n. Benefiting from the rich manual conditions incorporated during action generation, ManualVLA requires only around 100 trajectories to achieve generalizable manipulation while finetuning on downstream tasks.\nExperimental results demonstrate that, when confronted with long-horizon and complex LEGO assembly and object rearrangement tasks,\nManualVLA not only generates accurate manuals but also achieves an average manipulation success rate of 32% higher than existing hierarchical SOTA baseline.\nNote that ManualVLA also achieves state-of-the-art (SOTA) performance on other general manipulation tasks.\nIn summary, our contributions are as follows:\n‚Ä¢\nWe address long-horizon tasks with precisely defined goal states by introducing ManualVLA, a unified VLA model built upon a MoT architecture that supports coherent multimodal manual generation and action execution.\n‚Ä¢\nWe design a Manual Chain-of-Thought (ManualCoT) reasoning process that translates generated manuals into precise actions, where each manual step provides explicit control conditions and its latent representation offers implicit guidance for manipulation.\n‚Ä¢\nEquipped with the proposed training strategy, ManualVLA demonstrates strong real-world performance, achieving superior manipulation accuracy and generalization compared with previous SOTA baselines on downstream tasks.\n2\nRelated Work\nVision-language models (VLMs)\n[\nalayrac2022flamingo\n,\nradford2021learning\n,\nkaramcheti2024prismatic\n]\nhave achieved strong multimodal reasoning by learning from internet-scale image-text data. Building on this progress, VLA models\n[\nkim2024openvla\n,\nrt22023arxiv\n]\nhave emerged as a promising approach for robot learning, enabling end-to-end mapping from multimodal observations to control signals.\nSubsequent works have further advanced VLA models by incorporating richer sensory understanding\n[\nqu2025spatialvla\n,\nli2025pointvlainjecting3dworld\n,\nliu2025mla\n,\nli20253ds\n]\n, exploring more robust action generation\n[\nblack2024pi_0\n,\nintelligence2025pi05visionlanguageactionmodelopenworld\n,\nwen2024diffusion\n,\nli2024cogact\n,\nliu2025hybridvla\n]\n, and developing optimized inference strategies\n[\npertsch2025fast\n,\nwen2025tinyvla\n,\nliu2024robomamba\n]\nas well as dual-system paradigms\n[\nzhang2024hirt\n,\nfigure2024helix\n,\nbjorck2025gr00t\n,\nchen2025fast\n]\n.\nNotably, recent approaches like MoTVLA\n[\nhuang2025motvlavisionlanguageactionmodelunified\n]\nand F1-VLA\n[\nlv2025f1\n]\nhave introduced MoT architectures into general robotic manipulation.\nHowever, when faced with long-horizon tasks that require precise goal specifications, these models continue to struggle.\nTo reconcile this issue, several works\n[\nhu2024video\n,\nzhou2024robodreamer\n,\nye2024latent\n,\nliang2024dreamitate\n]\nhave incorporated visual world modeling into VLA architectures to enable reasoning about the future images.\nThey typically decouple long-horizon progress into intermediate steps either by generating explicit pixel-level subgoals\n[\ngoogle2024pivot\n,\nwu2023-GR1\n]\nor by formulating compressed token representations\n[\nzhang2025dreamvla\n,\nbu2025agibot\n,\ngao2025adaworld\n,\nli2025unified\n]\n.\nNevertheless, these models struggle to capture the relationship between subgoals and fine-grained control. In contrast, we introduce a comprehensive Chain-of-Thought (CoT) reasoning process that combines both explicit and implicit cues to transform the generated manuals into precise actions.\nFinal goal conditioned manipulation\n, where robots must reach specified target states from given initial configurations, such as LEGO assembly or object rearrangement, represents a challenge in embodied AI.\nA prominent research direction uses human hand videos\n[\nbahl2022human\n,\nshaw2023videodex\n,\njain2024vid2robot\n]\nto present the desired intermediate procedure.\nMethods such as Vid2Robot\n[\njain2024vid2robot\n]\nand DexCap\n[\nwang2024dexcap\n]\nextract manipulation trajectories from egocentric videos, transferring human dexterity to robotic control.\nMeanwhile, several works\n[\nwang2022translating\n,\npun2025generating\n,\nzhang2025manual\n]\nexploit operation manuals or goal-state descriptions as guidance.\nCheckManual\n[\nlong2025checkmanual\n]\nconditions robot policies on predefined instruction manuals, while\n[\nwu2022targf\n,\nzeng2024lvdiffusor\n]\nutilize the final target scene configuration to inform execution goal.\nHowever, providing hand videos, human-crafted manuals, or additional reasoning models introduces extra human effort and computational cost, limiting the practicality of these approaches.\nIn contrast, we make the first attempt to address long-horizon, goal-conditioned manipulation through a unified VLA model that enables coherent collaboration between multimodal manual generation and action execution.\n3\nManualVLA\nIn this section, we first introduce the fundamentals of Vision-Language-Action (VLA) models in Section\n3.1\n. Then, Section\n3.2\npresents the architectural details of ManualVLA, followed by Section\n3.3\n, which elaborates on its working principles. Sections\n3.4\nand\n3.5\ndescribe the training strategies of ManualVLA and the digital-twin toolkit, respectively.\n3.1\nPreliminary\nVLA models integrate visual, linguistic, and proprioceptive inputs to generate robot control signals, exhibiting strong generalization in diverse manipulation tasks\n[\nkim2024openvla\n,\nblack2024pi_0\n]\n.\nDespite their impressive abilities, existing VLA models often struggle with long-horizon tasks with defined goal states, reflecting their limited world knowledge for planning the intermediate progress required to achieve such goals.\nTo address this limitation, we aim to endow the VLA model with a human-like capability to infer the procedural ‚Äúhow‚Äù from the desired ‚Äúwhat‚Äù. Therefore, we propose ManualVLA\nœÄ\nŒ∏\n\\pi_{\\theta}\n, a unified VLA model equipped with world knowledge that first reasons about multimodal manuals describing the task procedure, and subsequently generates the corresponding actions for execution.\nAs shown in Figure.\n2\n, given the language instruction\nl\nl\ntogether with\nthe images of the current state\n‚Ñê\nt\ncurrent\n\\mathcal{I}_{t}^{\\text{current}}\nand the final goal state\n‚Ñê\ngoal\n\\mathcal{I}^{\\text{goal}}\n,\nManualVLA first generates a manual that consists of the textual description of the target objects\nl\n^\nt\n\\hat{l}_{t}\n,\ntheir target 2D coordinates\np\nt\np_{t}\n, and the corresponding subgoal image\n‚Ñê\nt\nsubgoal\n\\mathcal{I}_{t}^{\\text{subgoal}}\n:\nœÄ\nŒ∏\n‚Äã\n(\n‚Ñê\nt\nsubgoal\n,\np\nt\n,\nl\n^\nt\n‚à£\n‚Ñê\ngoal\n,\n‚Ñê\nt\ncurrent\n,\nl\n)\n.\n\\pi_{\\theta}(\\mathcal{I}_{t}^{\\text{subgoal}},p_{t},\\hat{l}_{t}\\mid\\mathcal{I}^{\\text{goal}},\\mathcal{I}_{t}^{\\text{current}},l).\n(1)\nBased on this manual, we construct a prompted image\n‚Ñê\nt\nprompt\n\\mathcal{I}_{t}^{\\text{prompt}}\nby overlaying the target object‚Äôs final position as a mask on the current scene image\n‚Ñê\nt\ncurrent\n\\mathcal{I}_{t}^{\\text{current}}\n.\nFinally, the model takes the robot state\ns\nt\ns_{t}\nand the prompted image\n‚Ñê\nt\nprompt\n\\mathcal{I}_{t}^{\\text{prompt}}\n, together with\n‚Ñ±\nt\nsubgoal\n\\mathcal{F}_{t}^{\\text{subgoal}}\n,\n‚Ñ±\nt\np\n\\mathcal{F}^{p}_{t}\n, and\n‚Ñ±\nt\nl\n^\n\\mathcal{F}^{\\hat{l}}_{t}\n(the key and value features\nstored during the generation of\nl\n^\nt\n\\hat{l}_{t}\n,\np\nt\np_{t}\n, and\n‚Ñê\nt\nsubgoal\n\\mathcal{I}_{t}^{\\text{subgoal}}\n), as conditional inputs for modeling the action chunk\na\nt\n:\nt\n+\nh\na_{t:t+h}\n, enabling explicit subgoal-guided action generation.\nœÄ\nŒ∏\n‚Äã\n(\na\nt\n:\nt\n+\nh\n‚à£\ns\nt\n,\n‚Ñê\nt\nprompt\n,\n‚Ñ±\nt\nsubgoal\n,\n‚Ñ±\nt\np\n,\n‚Ñ±\nt\nl\n^\n)\n.\n\\pi_{\\theta}(a_{t:t+h}\\mid s_{t},\\mathcal{I}_{t}^{\\text{prompt}},\\mathcal{F}_{t}^{\\text{subgoal}},\\mathcal{F}^{p}_{t},\\mathcal{F}^{\\hat{l}}_{t}).\n(2)\nFigure 2\n:\nFramework of ManualVLA.\n(a)\nTo accomplish long-horizon tasks with defined goal states, we propose ManualVLA, a unified VLA model built upon a MoT architecture. The framework consists of two experts: a planning expert responsible for generating multimodal manuals, and an action expert responsible for predicting precise actions.\nThe planning expert processes human instructions, the current image, and the final goal image to generate intermediate manuals that combine next-step image, positions, and sub-task instructions. We introduce an explicit CoT reasoning process, where each positional indicator serves as a visual prompt embedded into the observation of the action expert.\n(b)\nAlong with the cross-task shared attention mechanism and the designed attention mask, the generated manual tokens are also used as conditioning signals for action generation, enabling an implicit CoT reasoning process that effectively guides the action expert.\n(c)\nManualVLA adopts a three-stage training strategy that aligns the planning and action experts for effective collaboration.\n3.2\nModel Architecture\nManualVLA adopts Janus-Pro\n[\nchen2025janus\n]\nas its foundation model due to its strong capability in general multimodal understanding and generation.\nUnlike atomic or short-horizon manipulation tasks, our goal is to perform long-horizon, goal-specific tasks, where the model must not only generate detailed manuals but also predict precise actions and enable effective interaction between the two processes.\nTo achieve this, we extend the basic VLM into a Mixture-of-Transformers (MoT) architecture, forming a unified VLA model that integrates collaborative planning and action experts.\nWe next detail the key components of our ManualVLA.\nVision Tokenizer and Encoder.\nAs shown in Figure\n2\n, given the distinct characteristics of discrete and continuous image-injection paradigms, as well as the differing demands of manual and action generation, ManualVLA employs two separate visual modules: a VQ-based vision tokenizer for manual generation and a continuous vision encoder for action generation.\nFor the vision tokenizer, ManualVLA adopts an encoder-quantizer-decoder architecture following VQGAN\n[\nesser2021taming\n]\n. The encoder and decoder are convolutional networks with a downsampling factor of 16, and the quantizer maintains a codebook\nùêô\n‚àà\n‚Ñù\n16\n,\n384\n√ó\n8\n\\mathbf{Z}\\in\\mathbb{R}^{16{,}384\\times 8}\n.\nFor the vision encoder, ManualVLA uses SigLIP-Large\n[\nzhai2023sigmoid\n]\nwith an input resolution of 384 to extract high-dimensional semantic features from input images.\nMixture-of-Transformers LLM.\nThe base language model in ManualVLA is DeepSeek-LLM 1.5B\n[\nbi2024deepseek\n]\n.\nTo integrate the distinct capabilities of manual generation and action generation, we construct a MoT architecture atop this LLM.\nThe proposed MoT extends the standard Transformer by introducing task-specific parameter sets for all non-embedding components, including feed-forward networks (FFN), attention projections, and layer normalizations, yielding the planning and action experts illustrated in Figure\n2\n(a)\n.\nWhen faced with complex and long-horizon tasks, this design enables the VLA model to efficiently handle heterogeneous tasks while preserving its ability to learn cross-task dependencies within a unified framework.\nTo formally describe this mechanism, we take a single MoT layer as an example.\nLet\nx\n=\n(\nx\n1\n,\n‚Ä¶\n,\nx\nn\n)\nx=(x_{1},\\dots,x_{n})\ndenote the input token sequence, where each token\nx\ni\nx_{i}\nis assigned to exactly one task category\nt\ni\n‚àà\nùíØ\n=\n{\nmanual\n,\naction\n}\nt_{i}\\in\\mathcal{T}=\\{\\text{manual},\\text{action}\\}\n.\nFor each task, we define task-dependent operator bundles\nŒò\nt\n=\n{\nŒ∏\nattn\nt\n,\nŒ∏\nffn\nt\n}\n\\Theta^{t}=\\{\\theta_{\\text{attn}}^{t},\\theta_{\\text{ffn}}^{t}\\}\nand corresponding mappings\nŒ¶\nattn\nt\n,\nŒ¶\nffn\nt\n:\n‚Ñù\nd\n‚Üí\n‚Ñù\nd\n\\Phi_{\\text{attn}}^{t},\\Phi_{\\text{ffn}}^{t}:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}\n,\nwhich are applied token-wise depending on the associated task.\nThen, a MoT layer acting on a mixed-task sequence can be compactly expressed as:\nMoT\nŒò\n‚Äã\n(\nx\n)\n=\nx\n+\nùí©\nffn\nt\n‚Äã\n(\n‚ãÖ\n)\n‚Äã\n(\nŒ¶\nffn\nt\n‚Äã\n(\n‚ãÖ\n)\n‚Äã\n(\nx\n+\nùí©\nattn\nt\n‚Äã\n(\n‚ãÖ\n)\n‚Äã\n(\nŒ¶\nattn\n‚Äã\n(\nx\n)\n)\n)\n)\n,\n\\mathrm{MoT}_{\\Theta}(x)\\;=\\;x\\;+\\;\\mathcal{N}_{\\text{ffn}}^{t(\\cdot)}\\!\\Big(\\Phi_{\\text{ffn}}^{t(\\cdot)}\\big(x\\;+\\;\\mathcal{N}_{\\text{attn}}^{t(\\cdot)}\\big(\\Phi_{\\text{attn}}(x)\\big)\\big)\\Big),\n(3)\nwhere the notation\nt\n‚Äã\n(\n‚ãÖ\n)\nt(\\cdot)\nindicates that each token at position\ni\ni\nuses its corresponding task parameters and\nùí©\nattn\nt\n\\mathcal{N}_{\\text{attn}}^{t}\n,\nùí©\nffn\nt\n\\mathcal{N}_{\\text{ffn}}^{t}\ndenote task-specific layer-normalization operators.\nFinally, to define the global attention operator\n[\ndeng2025emerging\n]\n, let\nX\n‚àà\n‚Ñù\nn\n√ó\nd\nX\\in\\mathbb{R}^{n\\times d}\ndenote the matrix form of the input sequence\nx\n=\n(\nx\n1\n,\n‚Ä¶\n,\nx\nn\n)\nx=(x_{1},\\dots,x_{n})\n, where each row corresponds to a token embedding. The operator is then given by\nQ\n\\displaystyle Q\n=\nX\n‚Äã\nW\nQ\nt\n‚Äã\n(\n‚ãÖ\n)\n,\nK\n=\nX\n‚Äã\nW\nK\nt\n‚Äã\n(\n‚ãÖ\n)\n,\nV\n=\nX\n‚Äã\nW\nV\nt\n‚Äã\n(\n‚ãÖ\n)\n,\n\\displaystyle=XW_{Q}^{t(\\cdot)},\\quad K=XW_{K}^{t(\\cdot)},\\quad V=XW_{V}^{t(\\cdot)},\n(4)\nA\n\\displaystyle A\n=\nsoftmax\n‚Äã\n(\nQ\n‚Äã\nK\n‚ä§\nd\nk\n)\n,\nŒ¶\nattn\n‚Äã\n(\nx\n)\n=\n(\nA\n‚Äã\nV\n)\n‚Äã\nW\nO\nt\n‚Äã\n(\n‚ãÖ\n)\n,\n\\displaystyle=\\mathrm{softmax}\\!\\Big(\\frac{QK^{\\top}}{\\sqrt{d_{k}}}\\Big),\\quad\\Phi_{\\text{attn}}(x)=(AV)\\,W_{O}^{t(\\cdot)},\nwhere the projections\nW\nQ\nt\n‚Äã\n(\n‚ãÖ\n)\n,\nW\nK\nt\n‚Äã\n(\n‚ãÖ\n)\n,\nW\nV\nt\n‚Äã\n(\n‚ãÖ\n)\n,\nW\nO\nt\n‚Äã\n(\n‚ãÖ\n)\nW_{Q}^{t(\\cdot)},W_{K}^{t(\\cdot)},W_{V}^{t(\\cdot)},W_{O}^{t(\\cdot)}\nare selected per token according to its task\nt\ni\nt_{i}\n,\nwhile the attention weight matrix\nA\nA\nis computed globally across all tokens.\nThis formulation enables ManualVLA to adaptively allocate computation according to the distinct characteristics of manual and action generation, while maintaining a unified architecture for the collaborative execution of final goal conditioned manipulation tasks.\nAction and Robot State Components.\nManualVLA employs a diffusion-based approach for action modeling, therefore, a noise encoder is introduced to inject the noised actions into the action expert, and a noise decoder predicts the noise from the latent representations. Both modules are implemented as two-layer MLPs. In addition, the robot state is incorporated into the action expert through another two-layer MLP (state encoder), enabling the model to condition action generation on the current proprioceptive state.\n3.3\nManual and Action Generation via CoT\nGiven the final goal state of a task, ManualVLA generates subgoal manuals at key steps.\nTo better produce corresponding action sequences based on the current observation and the generated manuals, we introduce a Manual Chain-of-Thought (ManualCoT) reasoning process including both explicit and implicit CoT.\nSpecifically, we introduce the details of manual generation in Section\n3.3.1\n, followed by the method for generating executable actions through ManualCoT in Sections\n3.3.2\n. Finally, in Sections\n3.3.3\n, we describe how both processes are unified within a single token sequence for the end-to-end training.\n3.3.1\nSubgoal Manual Generation\nTo accomplish long-horizon manipulation tasks with predefined goal states, we design a multimodal manual that consists of (1) textual descriptions for subtask reasoning, (2) next-step images providing semantically rich conditioning, and (3) low-level position prompts for precise manipulation guidance.\nAs shown in Figure\n2\n(a)\n, upon receiving the language instruction along with the current and final state images, ManualVLA first generates a textual component that describes object attributes and actions.\nFor target positions, we represent them using the pixel-level\n(\nU\n,\nV\n)\n(U,V)\ncoordinates of each object‚Äôs centroid.\nFinally, subgoal image generation helps the model better model the physical world dynamics.\nWe assume that accomplishing a long-horizon task does not require extreme dense temporal subgoals.\nInstead, providing guidance only at key frames where the task state changes is sufficient, such as when placing a pair of bricks onto the board.\nManualVLA generates a new manual only after the completion of the previous subgoal, ensuring efficient planning without redundant intermediate guidance. To achieve this, we first generate the text description in the manual.\nIf the generated description of the manipulated objects differs from the previous planning output, ManualVLA proceeds to produce an entirely new manual. For example, the text output changes from ‚Äúyellow blocks‚Äù to ‚Äúpurple blocks‚Äù. Otherwise, it reuses the previously generated manual for subsequent action generation.\n3.3.2\nManual-Conditioned Action Generation\nBased on the generated subgoal manual, ManualVLA executes actions in a closed-loop manner, progressively generating the action sequence until the subgoal is achieved.\nAs shown in Figure\n2\n(a)\n, leveraging the effectiveness of visual prompts for manipulation, we use the predicted\n(\nU\n,\nV\n)\n(U,V)\ncoordinates to overlay a mask on the current image, highlighting the affordance region that serves as input to the action expert.\nThis construction of a prompt image to guide the action learning is defined as\nexplicit CoT reasoning\n.\nMeanwhile, we introduce an implicit CoT reasoning process within the shared attention module.\nAs shown in Figure\n2\n(b)\n, in the latent space, the subgoal manual serves as a conditioning signal for action modeling through our constructed cross-task attention mask.\nThis conditioning information first informs the model about ‚Äúwhat‚Äù object to manipulate, then specifies ‚Äúwhere‚Äù the object should be placed, and finally provides the anticipated visual outcome after the manipulation. This CoT reasoning process performed in the latent space is referred to as\nimplicit CoT reasoning\n.\nBy incorporating both explicit and implicit CoT reasoning processes, ManualVLA significantly improves its success rate in long-horizon tasks, as demonstrated in Section\n4.3\n.\n3.3.3\nToken Sequence Design\nAfter introducing the processes of manual and action generation, this section describes how ManualVLA jointly learns both tasks within a unified token sequence, while employing planning and action experts to specialize in different tasks. As shown in Figure\n2\n, the language instruction, along with the current and goal scene images, is first inserted into the token sequence as the condition for subgoal manual generation. The subsequent tokens represent the generated manual, including object descriptions, target coordinates, and subgoal image, all of which are processed by the planning expert. Following this, the sequence incorporates the prompt image used in explicit CoT reasoning, as well as the robot state and noised action embeddings, which are handled by the action expert. As shown in Figure\n2\n(b)\n, a cross-task shared attention mechanism is designed to allow the action expert to attend to the subgoal manual representations while masking out earlier inputs, thereby enabling effective information exchange between the two experts and fostering coherent reasoning across planning and action generation.\n3.4\nTraining Strategy\nAs shown in Figure\n2\n(c)\n, we train ManualVLA in three stages.\nBefore training, we initialize ManualVLA with the pretrained parameters of Janus-Pro\n[\nchen2025janus\n]\n, and duplicate its LLM to separately initialize the planning and action experts.\nStage 1: Action expert pretraining.\nDuring this pretraining stage, we curated an assembly dataset by carefully filtering large-scale cross-embodiment datasets\n[\nopen_x_embodiment_rt_x_2023\n,\nkhazatsky2024droid\n,\nwu2024robomind\n]\nto update all parameters of the action expert.\nAs detailed in the Appendix\nA\n, the resulting dataset comprises over 400K trajectory samples.\nManualVLA was trained on this dataset for five epochs, where the only conditioning inputs are the language instruction, a current scene image, and robot state.\nFollowing diffusion policy\n[\nchi2023diffusion\n]\n, the training objective is the mean squared error (MSE) between the predicted noises\nœµ\n^\ni\n\\hat{\\epsilon}^{i}\nat the\ni\ni\n-th denoising steps and the ground-truth noises\nœµ\n\\epsilon\n, defined as:\n‚Ñí\naction\n=\nùîº\nœµ\n‚àº\nùí©\n‚Äã\n(\n0\n,\n1\n)\n,\ni\n‚Äã\n‚Äñ\nœµ\n^\ni\n‚àí\nœµ\n‚Äñ\n2\n2\n.\n\\mathcal{L}_{\\text{action}}=\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,1),i}\\left\\|\\hat{\\epsilon}^{i}-\\epsilon\\right\\|_{2}^{2}.\nStage 2: Manual expert pretraining.\nAt this stage, we train only the manual expert using data synthesized with our digital-twin toolkit, resulting in a dataset of over 10K frames for each task.\nFollowing Janus-Pro\n[\nchen2025janus\n]\n, the model is supervised with a cross-entropy loss\n‚Ñí\nmanual\n\\mathcal{L}_{\\text{manual}}\napplied to the subgoal manual, which includes the object description, target position, and subgoal image tokens.\nStage 3: Joint manual-action fine-tuning.\nBenefiting from the stable generation capabilities acquired during pretraining, we collect 100 demonstrations for each downstream task using master-puppet teleoperation. Objects are placed at diverse locations on the table to ensure sufficient variation. Each demonstration includes both action execution data and automatically extracted manual data.\nAll components of ManualVLA are then jointly trained using the token sequences defined in Section\n3.3.1\n.\nThe final objective is defined as:\n‚Ñí\nfinal\n=\n‚Ñí\nmanual\n+\n‚Ñí\naction\n.\n\\mathcal{L}_{\\text{final}}=\\mathcal{L}_{\\text{manual}}+\\mathcal{L}_{\\text{action}}.\nFigure 3\n:\nDigital-twin example.\n(a)\nWe reconstruct 3D Gaussian Splatting representations, which are then decomposed into the LEGO board and individual bricks.\n(b)\nWe iteratively place the bricks on the board or objects on the box.\n3.5\nReal-to-Render Data Generation\nTo construct downstream task data, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting\n[\n2023_8_08-3dgs_for_real_time_radiance_field_rendering\n]\nto automatically generate training data with intermediate target states. For example, in Figure\n3\n(a)\n, we first reconstruct 3D assets of the LEGO board and individual bricks through multi-view capture, and then align them to a unified Cartesian coordinate system for consistent spatial referencing. Subsequently, we follow an iterative placement procedure as shown in Figure\n3\n(b)\n, where given an initial state and a set of available bricks, we sequentially place each brick by randomly sampling a valid position on the board. At each intermediate state, we render the current configuration from a front-view camera perspective. This process produces photorealistic images for each assembly or rearrangement step, along with position and textual information.\nMore details are provided in Appendix\nA\n.\n4\nExperiments\nWe organize our experiments as follows. Section\n4.1\ndefines the tasks and introduces the baseline methods. Section\n4.2\ncompares ManualVLA with these baselines on manual generation and manipulation performance. In Section\n4.3\n, we assess the effectiveness of our MoT architecture and CoT reasoning mechanism. Finally, Section\n4.4\nevaluates the generalization of ManualVLA to unseen object shapes, backgrounds, and lighting conditions. All the above experiments are performed on a dual-arm Franka robotic platform. Additionally, we validate the advantages of our method on general manipulation tasks on RLBench\n[\njames2020rlbench\n]\nBenchmark in Section\n4.5\n.\n4.1\nExperimental Setup\n4.1.1\nTask Definition\nWe design three long-horizon tasks with defined goal states, challenging the model‚Äôs procedural reasoning and manipulation capabilities.\n(1) 2D LEGO Assembly:\nThe task begins with several LEGO bricks of different colors placed on a planar board.\nGiven the final 2D assembled structure as the goal, the model must infer a sequence of intermediate manipulation actions and execute them through coordinated bimanual control.\n(2) 3D LEGO Assembly:\nThe task extends the 2D LEGO Assembly task to a more challenging 3D setting, where the final configuration transitions from a planar layout to a 3D structure. This upgraded task imposes greater demands on the model‚Äôs spatial reasoning abilities.\n(3) Object Rearrangement:\nThe task begins with several objects of diverse shapes, sizes, and semantics scattered around a box.\nGiven a goal state in which all objects are placed at their designated positions inside the box, the model must progressively generate manipulation actions, alternating control of the left and right arms to prevent collisions.\n4.1.2\nBaselines\nWe compare ManualVLA against three categories of strong VLA baselines that represent the state of the art in robotic manipulation.\n(1) First category:\nœÄ\n0\n\\pi_{0}\n[\nblack2024pi_0\n]\n,\nœÄ\n0.5\n\\pi_{0.5}\n[\nintelligence2025pi05visionlanguageactionmodelopenworld\n]\n, and FAST\n[\npertsch2025fast\n]\n, which adopt robust action-generation paradigms.\nWe load the official pretrained weights provided by each method and strictly follow their fine-tuning protocols, except that we additionally embed the final goal image into the model as the target condition.\n(2) Second category:\nCoT-VLA\n[\nzhao2025cot\n]\n, which not only incorporates the final goal image as an additional condition, but also predicts key subgoal future images.\nFor fair comparison, we align the supervision of subgoal image generation with that used in ManualVLA.\n(3) Third category:\nWe introduce a hierarchical baseline that combines a VLM\n[\nchen2025janus\n]\nwith\nœÄ\n0.5\n\\pi_{0.5}\n.\nThe VLM is trained to generate visual and language prompts similar to our method, while\nœÄ\n0.5\n\\pi_{0.5}\nis trained to interpret these prompts and generate actions.\nThis baseline can be regarded as a hierarchical variant of our manual-generation approach.\n4.2\nMain Results\n4.2.1\nManual Generation\nTable 1\n:\nQuantitative results of ManualVLA in generating subgoal images and (\nU\n,\nV\nU,V\n) coordinates across the three downstream tasks.\nTask\nSubgoal Image\n(U,V)\nPSNR\n‚Üë\n\\uparrow\nFID\n‚Üì\n\\downarrow\nMAE\n‚Üì\n\\downarrow\n2D LEGO Assembly\n29.01\n36.39\n3.23\n3D LEGO Assembly\n28.68\n34.63\n3.58\nObject Rearrangement\n28.11\n24.46\n6.21\nTable 2\n:\nComparison of ManualVLA and baselines.\nWe report the manipulation success rate (S.R.) for the complete long-horizon tasks using 20 unseen test goal states, and additionally report the success rate of key intermediate steps.\nMethod\n2D LEGO Assembly\n3D LEGO Assembly\nObject Rearrangement\n2 bricks\n‚Üí\n\\rightarrow\n2 bricks\n‚Üí\n\\rightarrow\n2 bricks\n‚Üí\n\\rightarrow\nS.R.\n2 bricks\n‚Üí\n\\rightarrow\n2 bricks\n‚Üí\n\\rightarrow\n2 bricks\n‚Üí\n\\rightarrow\nS.R.\n2 objects\n‚Üí\n\\rightarrow\n2 objects\n‚Üí\n\\rightarrow\n2 objects\n‚Üí\n\\rightarrow\nS.R.\nœÄ\n0\n\\pi_{0}\n[\nblack2024pi_0\n]\n0.25\n0.20\n0.15\n0.15\n0.25\n0.15\n0.10\n0.10\n0.35\n0.20\n0.10\n0.10\nœÄ\n0.5\n\\pi_{0.5}\n[\nintelligence2025pi05visionlanguageactionmodelopenworld\n]\n0.30\n0.25\n0.25\n0.20\n0.30\n0.20\n0.15\n0.15\n0.45\n0.25\n0.15\n0.15\nFAST\n[\npertsch2025fast\n]\n0.20\n0.15\n0.10\n0.10\n0.15\n0.15\n0.05\n0.05\n0.20\n0.15\n0.05\n0.05\nCoT-VLA\n[\nzhao2025cot\n]\n0.40\n0.35\n0.30\n0.30\n0.35\n0.30\n0.25\n0.25\n0.60\n0.40\n0.30\n0.30\nVLM +\nœÄ\n0.5\n\\pi_{0.5}\n0.75\n0.70\n0.65\n0.60\n0.65\n0.45\n0.35\n0.35\n0.90\n0.55\n0.65\n0.50\nManualVLA\n0.95\n0.90\n0.85\n0.85\n0.90\n0.75\n0.65\n0.65\n0.90\n0.70\n0.80\n0.65\nWe first evaluate the capability of the planning expert in ManualVLA to generate high-fidelity manuals on 300 unseen test samples. As shown in Table\n1\n, our model produces satisfactory intermediate images across all three tasks, achieving high PSNR scores, indicating strong structural and pixel-level consistency with the ground truth. Furthermore, the low FID scores, particularly in the Object Rearrangement task, demonstrate that the generated image distribution closely matches that of real images, confirming their realism and fidelity. The remarkably low MAE scores further highlight ManualVLA‚Äôs precision in predicting the position of target objects. For language descriptions, we evaluate accuracy by checking the predicted object nouns, all of which are correctly generated on unseen test samples.\nFigure 4\n:\nFor each task, we visualize three components: (1) manual ground truth (GT), (2) manual predictions (Pred.) generated by ManualVLA, and (3) the final goal image.\nFigure 5\n:\nVisualization of real-world experiments on Franka Research 3 dual-arm robots, executed from left to right.\nFigure\n4\npresents qualitative results of ManualVLA on 2D and 3D LEGO assembly as well as object rearrangement tasks. The model accurately predicts intermediate steps that align with the ground truth, effectively capturing both spatial arrangements and object identities. In LEGO assembly, ManualVLA sequentially reconstructs the correct brick placements and colors, demonstrating precise step-wise reasoning.\nFor object rearrangement, it gradually progresses toward the final goal configuration and accurately generates the spatial relationships between objects.\nOverall, these results highlight ManualVLA‚Äôs strong intermediate reasoning capabilities in long-horizon tasks, establishing a reliable foundation for the action expert to generate accurate actions.\n4.2.2\nAction Generation\nAcross all three real-world long-horizon tasks, ManualVLA achieves the highest success rates, markedly outperforming all baselines. As shown in Table\n2\n, we report both step-wise subgoal accuracy and end-to-end task success. Compared with the strongest hierarchical baseline, ManualVLA improves the final task completion rate by 15%-30%. While baseline models often succeed in the early stages of a long-horizon pipeline, they typically fail to sustain this performance through the whole sequence. In contrast, ManualVLA mitigates this degradation by decomposing complex tasks into structured subgoal manuals and grounding them into precise actions through a combination of explicit and implicit reasoning, enabling consistent performance throughout the entire task.\nNote that, as reflected in the manual generation results, the generated manuals may contain minor inaccuracies. Nevertheless, with the ManualCoT strategy and the capacity of our MoT architecture, ManualVLA remains robust and can still produce reliable actions even under moderate manual errors.\nIn Appendix\nB\n, we also validate the advantages of our method on general manipulation tasks.\nThe qualitative rollouts in Figure\n5\nfurther corroborate these results. ManualVLA generates structured and interpretable intermediate states that reliably guide the dual-arm system through precise grasping and relocation motions. In both LEGO assembly tasks, the robot maintains accurate brick alignment across all stages, while in the object-rearrangement task, it robustly manipulates objects with varying shapes, textures, and occlusions. More visualizations, failure case analyses, and execution videos are provided in Appendix\nC\n, Appendix\nD\n, and the supplementary material.\nFigure 6\n:\nAblation study.\nWe investigate the impact of (a) the information contained in the generated manuals, (b) explicit and implicit CoT reasoning, (c) the MoT architecture design, and (d) the action generation paradigm on long-horizon manipulation success rates.\n4.3\nAblation Study\nWe conduct detailed ablation studies on the 2D LEGO Assembly task, reporting the long-horizon task success rate.\n(a) What information should a generated manual contain?\nWe explore three variants: generating only the target positions (\nU\n,\nV\nU,V\n), generating both target positions and subgoal images, and generating the full set of multimodal manual.\nAs shown in Figure\n6\n(a), increasing the amount of multimodal information in the manuals leads to improved manipulation performance.\nNote that in this experiment, we consistently use explicit CoT visual-prompt images as inputs to the action expert.\nThe results demonstrate that high-level textual descriptions for subtask understanding, next-step images for semantically reasoning, and position prompts for accurate localization all serve as critical implicit conditions that enable precise manipulation.\n(b) Importance of Explicit CoT and Implicit CoT.\nWe examine two variants of our ManualCoT reasoning:\n(1) No Explicit CoT, where the action expert receives only the latent features from the planning expert together with the current image; and\n(2) No Implicit CoT, where the action expert receives only the visual-prompt image.\nAs shown in Figure\n6\n(b), both variants lead to a noticeable performance degradation compared to ManualVLA, demonstrating that explicit and implicit CoT reasoning are jointly indispensable for solving long-horizon, goal-defined manipulation tasks.\n(c) The MOT Architecture and Action Generation Design.\nIn Figure\n6\n(c), we compare our MoT architecture with a standard Mixture-of-Experts (MoE) architecture (duplicate only FFNs in LLM)\n[\ndeng2025emerging\n]\n.\nThe results show that using an MoE strategy fails to produce high-quality manuals and actions simultaneously, both of which are crucial for long-horizon tasks.\nMeanwhile, we find that for precise manipulation tasks, diffusion-based action generation yields superior performance.\nMore ablations on how manual quality influences manipulation performance are provided in Appendix\nB\n.\n4.4\nGeneralization Analysis\nFirst, in Section\n4.2\n, all final goal states used for evaluation differ from those in the teleoperated manipulation training set. Therefore, our main results simultaneously validate the generalization capability of ManualVLA with respect to both final goal states and object positions.\nIn addition, we evaluate the robustness of ManualVLA under variations in background, object shape, and lighting. As shown in Table\n3\n, these unseen perturbations are introduced in the 2D LEGO Assembly task and differ from all configurations seen during training. ManualVLA exhibits only a modest performance drop, which can be attributed to the rich guidance provided by our proposed manual generation expert and the CoT reasoning strategy during action prediction.\nMoreover, our proposed digital-twin toolkit provides large-scale manual generation data, allowing the model to produce accurate manuals even in unseen scenarios.\nTable 3\n:\nGeneralization.\nWe report the mean success rate and performance degradation ratio for each task over 20 rollouts under variations in background, object shape, and lighting.\n2D LEGO\nOrigin\nBackground\nShape\nLighting\nVLM +\nœÄ\n0.5\n\\pi_{0.5}\n0.60\n0.45(-25%)\n0.35(-46%)\n0.50(-17%)\nManualVLA\n0.85\n0.65(-23%)\n0.60(-29%)\n0.70(-17%)\n4.5\nSimulation Experiment\nWe evaluate ManualVLA on the RLBench\n[\njames2020rlbench\n]\nbenchmark, comparing it against state-of-the-art (SOTA) VLA baselines. The results demonstrate ManualVLA‚Äôs robust capabilities in predicting future images and generating precise actions.\n4.5.1\nSimulation benchmark.\nTable 4\n:\nComparison of ManualVLA and baselines on RLBench.\nWe train all methods in the multi-task setting\n[\nshridhar2022peract\n]\nand report the success rates (S.R.) and variances (Var.).\nModels\nClose\nClose\nToilet\nSweep\nClose\nPhone\nUmbrella\nFrame\nWine at\nWater\nMean\nbox\nlaptop lid\nseat down\nto dustpan\nfridge\non base\nout\noff hanger\nrack\nplants\nS.R. & Var\nFAST\n[\npertsch2025fast\n]\n0.85\n0.70\n0.90\n0.45\n0.60\n0.15\n0.15\n0.25\n0.45\n0.15\n0.47\n¬±\n0.03\n\\pm 0.03\nœÄ\n0\n\\pi_{0}\n[\nblack2024pi_0\n]\n0.85\n0.95\n0.90\n0.85\n0.80\n0.25\n0.20\n0.65\n0.65\n0.25\n0.63\n¬±\n0.01\n\\pm 0.01\nœÄ\n0.5\n\\pi_{0.5}\n[\nintelligence2025pi05visionlanguageactionmodelopenworld\n]\n0.90\n0.80\n0.90\n0.50\n0.60\n0.15\n0.25\n0.35\n0.75\n0.35\n0.56\n¬±\n0.03\n\\pm 0.03\nCoT-VLA\n[\nzhao2025cot\n]\n0.90\n0.85\n0.90\n0.60\n0.70\n0.20\n0.30\n0.55\n0.55\n0.30\n0.59\n¬±\n0.03\n\\pm 0.03\nManualVLA¬†(ours)\n0.90\n0.90\n0.90\n1.00\n0.85\n0.30\n0.50\n0.70\n0.65\n0.35\n0.70\n¬±\n0.02\n\\pm 0.02\nTo assess the fundamental manipulation capabilities of our method across common manipulation tasks, we conduct experiments on 10 tasks in the RLBench\n[\njames2020rlbench\n]\nbenchmark based on the CoppeliaSim simulator. The task suite includes\nClose box\n,\nClose Laptop\n,\nToilet seat down\n,\nSweep to dustpan\n,\nClose fridge\n,\nPhone on base\n,\nTake umbrella out\n,\nTake frame off hanger\n,\nPlace wine at rack\n, and\nWater plants\n. All tasks are executed on a Franka Panda robot equipped with a front-view RGB camera to get the visual input. We collect the data by following pre-defined waypoints and utilizing the Open Motion Planning Library\n[\nsucan2012open\n]\n. Building upon the frame-sampling technique employed in previous studies\n[\nshridhar2022peract\n,\ngoyal2023rvt\n,\njia2025lift3d\n]\n, we construct a training dataset where each task contains 100 trajectories.\nTo generate ground-truth\n(\nU\n,\nV\n)\n(U,V)\nlabels, we follow the key-frame extraction procedure from prior work. Specifically, we extract the end-effector poses of the key frames and then use the camera parameters to project their world-coordinate positions into\n(\nU\n,\nV\n)\n(U,V)\ncoordinates.\n4.5.2\nTraining and evaluation details.\nWe compare ManualVLA against four state-of-the-art (SOTA) VLA models, including FAST\n[\npertsch2025fast\n]\n,\nœÄ\n0\n\\pi_{0}\n[\nblack2024pi_0\n]\n,\nœÄ\n0.5\n\\pi_{0.5}\n[\nintelligence2025pi05visionlanguageactionmodelopenworld\n]\n, and CoT-VLA\n[\nzhao2025cot\n]\n. While the former three adopt robust action-generation paradigms, CoT-VLA conditions on the final goal image and additionally predicts future subgoal images. Specifically, FAST\n[\npertsch2025fast\n]\nutilizes autoregressive action outputs,\nœÄ\n0\n\\pi_{0}\n[\nblack2024pi_0\n]\nand\nœÄ\n0.5\n\\pi_{0.5}\n[\nintelligence2025pi05visionlanguageactionmodelopenworld\n]\nemploy flow matching, and CoT-VLA\n[\nzhao2025cot\n]\ncombines autoregressive image generation with diffusion-based action prediction. For all baselines, we initialize with the official pretrained parameters and strictly adhere to their original fine-tuning configurations. To ensure a fair comparison, we align the subgoal supervision in CoT-VLA to match the formulation used in ManualVLA.\nFor ManualVLA‚Äôs input, the single-view RGB image is resized to\n384\n√ó\n384\n384\\times 384\n, with text instructions derived directly from the simulation environment and the robot state is aligned with the predicted actions.\nManualVLA model is trained for 500 epochs using the AdamW optimizer\n[\nloshchilov2017decoupled\n]\nand CosineAnnealingLR\n[\nloshchilov2017sgdrstochasticgradientdescent\n]\non 8 NVIDIA H20 GPUs, with mixed-precision training employed.\nFollowing\n[\nli2024cogact\n,\ngoyal2023rvt\n]\n, we evaluate all methods using 20 rollouts from the latest epoch checkpoint, repeating the evaluation three times for each task and reporting the average success rate along with the variance.\n4.5.3\nQuantitative results.\nAs presented in Table\n4\n, ManualVLA achieves an average success rate of 70% across 10 diverse tasks, surpassing the previous SOTA methods\nœÄ\n0\n\\pi_{0}\n[\nblack2023zero\n]\nand CoT-VLA\n[\nzhao2025cot\n]\nby margins of 7% and 11%, respectively.\nSpecifically, ManualVLA attains superior performance on 8 out of 10 tasks, highlighting the advantage of ManualCoT strategy in guiding precise action generation. By generating sub-goal images and constructing visual prompt images, ManualVLA effectively leverages the fine-grained affordance guidance provided by explicit CoT reasoning. Furthermore, the MoT architecture, equipped with a shared attention module, enables robust task understanding and action generation conditioned on the sub-goal manual within the latent space. Through the integration of both explicit and implicit CoT reasoning, ManualVLA demonstrates substantial improvements in tasks requiring precise actions, such as\nsweep to dustpan\nand\ntake out umbrella\n, compared to\nœÄ\n0\n\\pi_{0}\nand\nœÄ\n0.5\n\\pi_{0.5}\n.\n5\nConclusion\nIn this work, we address the challenge of enabling robots to autonomously perform long-horizon tasks with defined goal states, such as LEGO assembly and object rearrangement. To this end, we introduce ManualVLA, a unified VLA model built on a Mixture-of-Transformers architecture that couples multimodal manual generation with action execution. Central to the model is a Manual Chain-of-Thought (ManualCoT) process, which converts subgoal manuals into precise actions by using them as explicit control conditions and implicit manipulation cues. To support scalable training, we further develop a 3D Gaussian Splatting-based digital-twin pipeline that automatically produces large amounts of manual data.\nExperimental results on long-horizon tasks show that ManualVLA achieves a 32% higher average success rate than existing VLA methods.\nAppendix A\nAdditional Dataset Details\nFigure 7\n:\nReal-World Assets and Experimental Settings.\nWe provide visualizations of the assets used and the hardware settings for Dual-Arm Franka Platform tasks.\nThis section provides further details on our pretraining and real-world datasets used in the multi-stage training process. Furthermore, we elaborated in detail on the digital-twin data generation pipeline.\nA.1\nPretraining Data Curation\nAs outlined in Stage 1 of our training strategy, the action expert is pretrained on a large-scale, cross-embodiment assembly dataset. To build this dataset, we carefully aggregate and curate demonstrations from public robotic manipulation datasets, including Open X-Embodiment\n[\nopen_x_embodiment_rt_x_2023\n]\n, Droid\n[\nkhazatsky2024droid\n]\n, and Robomind\n[\nwu2024robomind\n]\n.\nThese datasets contain millions of trajectories from a diverse set of robotic platforms, sensor configurations, and task settings. However, only a subset of these demonstrations is directly relevant to the domain of robotic assembly and rearrangement. Therefore, a careful multi-stage filtering and unification process is required before pretraining. We first apply a task-level filtering pipeline designed to extract trajectories involving low-level manipulation primitives that commonly appear in assembly and rearrangement settings, such as precise object-manipulation, and general pick‚Äìplace.\nAfter filtering and integration, the assembly pretraining dataset contains more than 400,000 high-quality trajectory samples, each representing a structured demonstration of object manipulation. Although this is a fraction of the raw data available in the source corpora, the curated subset is specifically optimized for assembly-centric skills while remaining a robust foundation for the action expert to learn a wide range of manipulation primitives before fine-tuning on downstream tasks.\nTable 5\n:\nThe dataset name and sampling weight used in our mixed large-scale pretraining dataset.\nTraining Dataset Mixture\nFractal\n[\nbrohan2022rt\n]\n6.8%\nKuka\n[\nkalashnikov2018qt\n]\n10.5%\nBridge\n[\nebert2021bridge\n,\nwalke2023bridgedata\n]\n4.9%\nTaco Play\n[\nrosetebeas2022latent\n,\nmees2023grounding\n]\n2.5%\nJaco Play\n[\ndass2023jacoplay\n]\n0.4%\nBerkeley Cable Routing\n[\nluo2023multistage\n]\n0.2%\nRoboturk\n[\nDBLP:journals/corr/abs-1811-02790\n]\n2.0%\nViola\n[\nzhu2023viola\n]\n0.8%\nBerkeley Autolab UR5\n[\nBerkeleyUR5Website\n]\n1.0%\nToto\n[\nzhou2023train\n]\n1.7%\nLanguage Table\n[\nlynch2023interactive\n]\n3.7%\nStanford Hydra Dataset\n[\nbelkhale2023hydra\n]\n3.8%\nAustin Buds Dataset\n[\nzhu2022bottom\n]\n1.8%\nNYU Franka Play Dataset\n[\ncui2022play\n]\n0.7%\nFurniture Bench Dataset\n[\nheo2023furniturebench\n]\n2.1%\nUCSD Kitchen Dataset\n[\nucsd_kitchens\n]\n<\n<\n0.1%\nAustin Sailor Dataset\n[\nnasiriany2022sailor\n]\n1.9%\nAustin Sirius Dataset\n[\nliu2022robot\n]\n1.5%\nDLR EDAN Shared Control\n[\nquere_shared_2020\n]\n<\n<\n0.1%\nIAMLab CMU Pickup Insert\n[\nsaxena2023multiresolution\n]\n0.7%\nUTAustin Mutex\n[\nshah2023mutex\n]\n1.9%\nBerkeley Fanuc Manipulation\n[\nfanuc_manipulation2023\n]\n0.6%\nCMU Stretch\n[\nmendonca2023structured\n]\n0.1%\nBC-Z\n[\njang2022bc\n]\n6.3%\nFMB Dataset\n[\nluo2024fmb\n]\n6.0%\nDobbE\n[\nshafiullah2023dobbe\n]\n1.2%\nDROID\n[\nkhazatsky2024droid\n]\n14.2%\nStanford Kuka Dataset\n[\nlee2019makingsensevisiontouch\n]\n0.3%\nStanford Robocook Dataset\n[\nshi2023robocooklonghorizonelastoplasticobject\n]\n0.2%\nColumbia Cairlab Pusht Real\n[\nchi2023diffusion\n]\n<\n<\n0.1%\nUCSD Pick and Place\n0.8%\nManiskill\n[\ngu2023maniskill2unifiedbenchmarkgeneralizable\n]\n7.5%\nBerkeley RPT\n[\nradosavovic2023real\n]\n<\n<\n0.1%\nQUT Dexterous Manipulation\n[\nceola2023lhmanip\n]\n<\n<\n0.1%\nRoboSet\n[\nkumar2023robohive\n]\n5.2%\nBridgeData V2\n[\nwalke2023bridgedata\n]\n9.3%\nRoboMind\n[\nwu2024robomind\n]\n1.2%\nA.2\nReal-World Data Collection\nFor real-world experiments, we evaluate three downstream tasks (2D LEGO Assembly, 3D LEGO Assembly, Object Rearrangement) on the dual-arm Franka platform. Below, we detail the hardware configurations, task settings, and data protocols.\nHardware Configurations.\nWe equip the dual-arm Franka experimental environment with two Franka Research 3 arms each with a 3D-printed UMI gripper, the configurations of which is summarized in Table\n6\n. As shown in Figure\n7\n, we utilize an Intel RealSense 455 camera to capture a static third-person view at the speed of 30Hz.\nTable 6\n:\nThe hardware setups of the Franka Research 3, including joint position limits and velocity limit.\nJoint Number\nPosition Limits\nVelocity Limits\nJ1\n‚àí\n166\n‚Äã\n¬∞\n‚àº\n+\n166\n‚Äã\n¬∞\n-166\\degree\\sim+166\\degree\n150\n‚Äã\n¬∞\n/\ns\n150\\degree/s\nJ2\n‚àí\n105\n‚Äã\n¬∞\n‚àº\n+\n105\n‚Äã\n¬∞\n-105\\degree\\sim+105\\degree\n150\n‚Äã\n¬∞\n/\ns\n150\\degree/s\nJ3\n‚àí\n166\n‚Äã\n¬∞\n‚àº\n+\n166\n‚Äã\n¬∞\n-166\\degree\\sim+166\\degree\n150\n‚Äã\n¬∞\n/\ns\n150\\degree/s\nJ4\n‚àí\n176\n‚Äã\n¬∞\n‚àº\n‚àí\n7\n‚Äã\n¬∞\n-176\\degree\\sim-7\\degree\n150\n‚Äã\n¬∞\n/\ns\n150\\degree/s\nJ5\n‚àí\n165\n‚Äã\n¬∞\n‚àº\n+\n165\n‚Äã\n¬∞\n-165\\degree\\sim+165\\degree\n301\n‚Äã\n¬∞\n/\ns\n301\\degree/s\nJ6\n+\n25\n‚Äã\n¬∞\n‚àº\n+\n265\n‚Äã\n¬∞\n+25\\degree\\sim+265\\degree\n301\n‚Äã\n¬∞\n/\ns\n301\\degree/s\nJ7\n‚àí\n175\n‚Äã\n¬∞\n‚àº\n+\n175\n‚Äã\n¬∞\n-175\\degree\\sim+175\\degree\n301\n‚Äã\n¬∞\n/\ns\n301\\degree/s\nTask Settings.\nWe provide a detailed explanation of the assembly and rearrangement tasks and their success conditions.\n2D Assembly:\nThe final state of the task is that LEGO blocks of different colors are randomly inserted into the same layer position on the planar board, without any stacking of blocks. Given the final 2D assembled structure as the goal, the robot arms need to pick up the LEGO blocks from both sides of the board in turn and insert them into the correct corresponding position in the center of the board. We only consider it a success if the current Lego blocks on the board match the position of the final structure at each key intermediate step. For a complete evaluation, only if all key intermediate steps are successful will it be counted as successful.\n3D Assembly:\nBased on the 2D Assembly Task, the 3D Assembly Task allows for more complex placement situations such as stacking between LEGO blocks. For evaluation, we do not require the placement order of LEGO blocks, but we still require that the LEGO blocks placed at each key intermediate step conform to the placement position of the corresponding color LEGO block in the final state, and the final 3D LEGO shape needs to match the given shape.\nObject Rearrangement:\nUnlike Assembly tasks, we use bowls, bananas, beverage cans and other common objects in life. The goal of the task is to pack the objects on the table into the box in turn and conform to the given placement pattern. In the rearrangement task, the order of placement is very important since there should be no situation where the objects in the bowl are placed before the bowl during execution. We follow the same evaluation settings as Assembly tasks at key intermediate steps and final states.\nData Protocols.\nFor each task, we collect 100 demonstrations using 3DConnexion Spacemouse to teleoperate each Franka arm with target positions randomized on the table and box to promote data diversity. Language instructions are manually created and diversified via augmentation. Each trajectory is recorded at a frequency of 15hz, and each step contains a third-angle image shaped 640x480, the dual-arm end effector poses and the grippers discrete opening and closing. For each trajectory, we automatically filter key intermediate steps by grippers status to obtain the subgoal image, and use pixel matching to obtain the low-level\n(\nU\n,\nV\n)\n(U,V)\ncoordinates corresponding to each image.\nA.3\nDigital-Twin Data Generation\nTo train the planning expert (Stage 2) without the prohibitive cost of large-scale human annotation, we developed a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting (3DGS). The pipeline consists of two main steps:\nFigure 8\n:\nIterative manual generation examples for LEGO Assembly.\nEach row shows a sequence where two bricks are progressively stacked per iteration.\nScenes are rendered at each step using our digital-twin toolkit.\nFigure 9\n:\nIterative manual generation examples for Object Rearrangement.\nEach row shows a sequential process where objects are placed into the box one at a time.\nScenes are rendered at each step using our digital-twin toolkit.\n1.\nAsset Reconstruction:\nWe first reconstruct high-fidelity 3D assets of all relevant objects, including the LEGO board, individual bricks of various colors, and the objects used in the rearrangement task. This is achieved by capturing multi-view images of each object and using them to train a 3DGS\n[\n2023_8_08-3dgs_for_real_time_radiance_field_rendering\n]\nmodel. The resulting representations are then decomposed and aligned to a unified Cartesian coordinate system for consistent spatial referencing.\n2.\nIterative Scene Generation:\nGiven an initial state and a set of available objects, the toolkit iteratively and automatically generates intermediate task states. For LEGO assembly, it sequentially places each brick by randomly sampling a valid position on the board. At each intermediate step, we render a photorealistic image of the current scene from a fixed front-view camera perspective. This process provides the necessary data for training the planning expert: the rendered image serves as the subgoal image, the brick‚Äôs board position provides the (\nU, V\n) coordinates, and a corresponding textual description (e.g., ‚ÄùYellow blocks assemble‚Äù) is generated via templates.\nThis automated pipeline enabled us to generate a dataset of over 10,000 frames for each task, providing the rich data needed to effectively pretrain the planning expert. More visualizations of the generated sim-to-real data are shown in Figure\n8\nand Figure\n9\n.\nAppendix B\nAdditional Experimental Details\nIn this section, we report the additional ablation studies on the impact of manual generation quality and token sequence arrangement on model‚Äôs action accuracy.\nFigure 10\n:\nIterative manual generation examples for 2D LEGO Assembly.\nPred refers to the predictions generated by our model, while GT denotes the ground truth in the test set.\nFigure 11\n:\nIterative manual generation examples for 3D LEGO Assembly.\nPred refers to the predictions generated by our model, while GT denotes the ground truth in the test set.\nFigure 12\n:\nIterative manual generation examples for objects rearrangement.\nPred refers to the predictions generated by our model, while GT denotes the ground truth in the test set.\nB.1\nAdditional Ablation Studies\nThis section will present additional ablation studies to further validate our design choices.\nTable 7\n:\nComparison of manual generation quality impact on action generation.\nTraining Frames\nPSNR\n‚Üë\n\\uparrow\n2D LEGO Assembly\n2 bricks\n‚Üí\n\\rightarrow\n2 bricks\n‚Üí\n\\rightarrow\n2 bricks\n‚Üí\n\\rightarrow\nS.R.\n0.5K\n25.71\n0.35\n0.25\n0.20\n0.20\n1K\n26.61\n0.45\n0.35\n0.30\n0.25\n3K\n27.16\n0.65\n0.65\n0.60\n0.60\n6K\n28.29\n0.85\n0.80\n0.80\n0.80\n10K\n29.01\n0.95\n0.90\n0.85\n0.85\nB.1.1\nImpact of Manual Generation Quality on Action\nTo evaluate the robustness of the action expert under varying manual-generation quality, we compare five versions of our planning expert trained on datasets of 0.5K, 1K, 3K, 6K and 10K frames, respectively, for the 2D LEGO Assembly task. All these training frames are generated using our high-fidelity digital-twin toolkit. These planning experts produce manuals with differing levels of fidelity, quantified by the PSNR of the generated subgoal images. We condition ManualVLA on these manuals and measure the resulting task success rate over 20 rollouts, as reported in Table\n7\n. The results indicate that low-quality subgoal images lead to substantial error accumulation during action generation, significantly degrading overall performance. Once the training set reaches 1,000 samples, yielding manuals with PSNR above 27, the action expert exhibits stable and reliable behavior. This trend highlights ManualVLA‚Äôs robustness: when the planning expert is sufficiently trained, its explicit and implicit chain-of-thought reasoning reliably supports consistent action generation.\nB.1.2\nImpact of Token Sequence Arrangement\nTable 8\n:\nComparison of different token sequence order impact on action generation.\nHere,\nT\nT\nand\nP\nP\ndenote the textual description and coordinate pairs\n(\nU\n,\nV\n)\n(U,V)\nin the manual, while\nI\nsubgoal\nI_{\\text{subgoal}}\nand\nI\nprompt\nI_{\\text{prompt}}\nrefer to the subgoal image and the visual prompt image.\nToken Sequence\n2D LEGO Assembly\n2 bricks\n‚Üí\n\\rightarrow\n2 bricks\n‚Üí\n\\rightarrow\n2 bricks\n‚Üí\n\\rightarrow\nS.R.\nP\nP\n‚Üí\n\\rightarrow\nI\ns\n‚Äã\nu\n‚Äã\nb\n‚Äã\ng\n‚Äã\no\n‚Äã\na\n‚Äã\nl\nI_{subgoal}\n‚Üí\n\\rightarrow\nT\nT\n‚Üí\n\\rightarrow\nI\np\n‚Äã\nr\n‚Äã\no\n‚Äã\nm\n‚Äã\np\n‚Äã\nt\nI_{prompt}\n0.90\n0.85\n0.80\n0.80\nI\ns\n‚Äã\nu\n‚Äã\nb\n‚Äã\ng\n‚Äã\no\n‚Äã\na\n‚Äã\nl\nI_{subgoal}\n‚Üí\n\\rightarrow\nP\nP\n‚Üí\n\\rightarrow\nT\nT\n‚Üí\n\\rightarrow\nI\np\n‚Äã\nr\n‚Äã\no\n‚Äã\nm\n‚Äã\np\n‚Äã\nt\nI_{prompt}\n0.75\n0.75\n0.70\n0.70\nT\nT\n‚Üí\n\\rightarrow\nI\ns\n‚Äã\nu\n‚Äã\nb\n‚Äã\ng\n‚Äã\no\n‚Äã\na\n‚Äã\nl\nI_{subgoal}\n‚Üí\n\\rightarrow\nP\nP\n‚Üí\n\\rightarrow\nI\np\n‚Äã\nr\n‚Äã\no\n‚Äã\nm\n‚Äã\np\n‚Äã\nt\nI_{prompt}\n0.85\n0.80\n0.80\n0.80\nT\nT\n‚Üí\n\\rightarrow\nP\nP\n‚Üí\n\\rightarrow\nI\ns\n‚Äã\nu\n‚Äã\nb\n‚Äã\ng\n‚Äã\no\n‚Äã\na\n‚Äã\nl\nI_{subgoal}\n‚Üí\n\\rightarrow\nI\np\n‚Äã\nr\n‚Äã\no\n‚Äã\nm\n‚Äã\np\n‚Äã\nt\nI_{prompt}\n0.95\n0.90\n0.85\n0.85\nTo further analyze how the ordering of multimodal tokens in the generated manual influences action generation, we evaluate four different sequence arrangements, as shown in Table\n8\n. The modalities contained in the manual serve complementary purposes: the text instruction provides high-level semantic goals, the coordinate tokens\n(\nU\n,\nV\n)\n(U,V)\nspecify the future spatial locations of the objects to be manipulated, and the generated manual image offers step-wise visual cues synthesized by the planning expert. Finally, the visual prompt image is conditioned on the generated coordinates\n(\nU\n,\nV\n)\n(U,V)\n, and therefore we place it at the end of the token sequence. This ordering naturally forms a pipeline in which the model first performs implicit CoT reasoning, followed by explicit CoT reasoning, before producing the final action sequence.\nBecause the effectiveness of the action expert depends on how well it integrates semantic and visual information, the ordering of these tokens can substantially affect downstream policy performance. Our study on different generation orders of the three types of manual information reveals that the sequence of generating text first, then coordinates, and finally subgoal images yields the best task success rate. Meanwhile, the other sequence configurations introduce only minor performance degradation, demonstrating both the robustness of ManualVLA and the critical role of the combined implicit‚Äìexplicit CoT reasoning process in enabling strong action-generation performance.\nFigure 13\n:\nReal-World Task Execution Progress Visualization.\nWe provide visualizations of three real world tasks including assembly and rearrangement evaluated on dual-arm Franka robot platform.\nFigure 14\n:\nFailure cases in our two tasks: LEGO assembly and objects rearrangement.\nThe top two rows illustrates two LEGO failure cases and the bottom two rows shows a failure case of objects rearrangement task.\nAppendix C\nAdditional Qualitative Results\nThis section provides more qualitative results for manual generation and real-world robot rollouts.\nC.1\nManual Generation Visualization\nFigure\n12\n, Figure\n12\n, and Figure\n12\nprovide additional visualizations of the manuals generated by our planning expert across all three downstream tasks. These examples showcase the model‚Äôs ability to generate structured and interpretable intermediate states that accurately guide the subsequent action generation. For the LEGO assembly tasks, ManualVLA sequentially reconstructs the correct brick placements and colors, demonstrating precise step-wise reasoning. Similarly, for object rearrangement, it progressively generates subgoals that accurately capture the spatial relationships between objects, moving step-by-step toward the final goal configuration. Overall, these results highlight ManualVLA‚Äôs strong intermediate reasoning capabilities, establishing a reliable foundation for the action expert to generate accurate actions.\nC.2\nReal-World Rollout Visualization\nThe qualitative rollouts in Figure\n13\nfurther corroborate our quantitative findings, illustrating keyframes of the dual-arm real-world execution processes. The visualizations demonstrate that ManualVLA can follow the internally generated manuals to reliably guide the action expert in producing precise grasping, insertion, and placement motions.\nIn both the 2D and 3D LEGO assembly tasks, compared with the final goal image, the robot consistently maintains accurate brick placement throughout all stages. For the object-rearrangement task, also compared with the final goal image, it stably manipulates objects with varying shapes and occlusions.\nThese results collectively validate ManualVLA‚Äôs strong action generation capabilities, demonstrating its potential as a robust policy for real-world, long-horizon robotic manipulation.\nAppendix D\nFailure Case Analysis\nAlthough ManualVLA demonstrates strong overall performance, it is not without limitations. Through our experiments, we identified two primary failure modes, as illustrated in Figure\n14\n:\n1.\nOccasional Erroneous LEGO Placement.\nWhile ManualVLA is generally successful in accomplishing the LEGO assembly task, it can still produce incorrect placements. As shown in Cases 1 and 2 of Figure\n14\n, the system places the yellow bricks in incorrect positions due to model errors. Notably, however, the system is often able to recover from such mistakes and correctly place subsequent bricks.\n2.\nPlacement Errors Under Large Rotation Angles.\nIn the Objects Rearrangement task, certain scenarios require the robot arm to perform large rotations to achieve the correct placement orientation. ManualVLA may fail in these situations, as illustrated by Case 3 in Figure\n14\n, where the robot arm fails to place the spirit can into the box. We hypothesize that these failures stem from the limited number of such extreme rotation cases in the training data.",
  "preview_text": "Vision-Language-Action (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation. However, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation. Therefore, we aim to endow a VLA model with the capability to infer the \"how\" process from the \"what\" outcomes, transforming goal states into executable procedures. In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution. Unlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training. ManualVLA demonstrates strong real-world performance, achieving an average success rate 32% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.\n\nManualVLA: A Unified VLA Model for Chain-of-Thought\nManual Generation and Robotic Manipulation\nChenyang Gu\n1\n,\nJiaming Liu\n‚àó‚Ä†\n1\n,\nHao Chen\n‚àó‚Ä†\n2\n,\nRunzhong Huang\n‚àó\n1\n,\nQingpo Wuwu\n1\n,\nZhuoyang Liu\n1\n,\nXiaoqi Li\n1\n,\nYing Li\n1\n,\nRenrui Zhang\n2\n,\nPeng Jia\n3\n,\nPheng-Ann Heng\n2\n,\nShanghang Zhang\n‚úâ\n1\n1\nState Key Laboratory of Multimedia Information P",
  "is_relevant": null,
  "relevance_score": 0.0,
  "extracted_keywords": [],
  "one_line_summary": "",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "2025-12-01T18:59:50Z",
  "created_at": "2026-01-08T10:08:14.571379",
  "updated_at": "2026-01-08T10:08:14.571388"
}