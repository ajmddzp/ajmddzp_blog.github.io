{
    "id": "2601.15419v1",
    "title": "Learning a Unified Latent Space for Cross-Embodiment Robot Control",
    "authors": [
        "Yashuai Yan",
        "Dongheui Lee"
    ],
    "abstract": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„è·¨å½¢æ€äººå½¢æœºå™¨äººæ§åˆ¶æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºå…±äº«çš„æ½œåœ¨è¡¨å¾ï¼Œç»Ÿä¸€äººç±»ä¸å¤šæ ·åŒ–äººå½¢å¹³å°ï¼ˆåŒ…æ‹¬å•è‡‚ã€åŒè‡‚åŠè…¿å¼äººå½¢æœºå™¨äººï¼‰çš„è¿åŠ¨æ¨¡å¼ã€‚è¯¥æ–¹æ³•åˆ†ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œï¼šé¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ„å»ºè§£è€¦çš„æ½œåœ¨ç©ºé—´ï¼Œä»¥æ•æ‰ä¸åŒèº«ä½“éƒ¨ä½çš„å±€éƒ¨è¿åŠ¨æ¨¡å¼ï¼Œä»è€Œå®ç°å¯¹å½¢æ€å„å¼‚æœºå™¨äººçš„ç²¾å‡†çµæ´»è¿åŠ¨é‡å®šå‘ã€‚ä¸ºå¢å¼ºä¸åŒå½¢æ€é—´çš„å¯¹é½æ•ˆæœï¼Œæˆ‘ä»¬é’ˆå¯¹å…³é”®éƒ¨ä½ï¼ˆå¦‚æ‰‹è‡‚ï¼‰è®¾è®¡äº†ç»“åˆå…³èŠ‚æ—‹è½¬ä¸æœ«ç«¯æ‰§è¡Œå™¨å®šä½çš„å®šåˆ¶åŒ–ç›¸ä¼¼åº¦åº¦é‡ã€‚éšåï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨äººç±»æ•°æ®åœ¨è¯¥æ½œåœ¨ç©ºé—´ä¸­ç›´æ¥è®­ç»ƒç›®æ ‡æ¡ä»¶æ§åˆ¶ç­–ç•¥ã€‚é€šè¿‡æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼Œç­–ç•¥å­¦ä¹ åœ¨ç›®æ ‡æ–¹å‘å¼•å¯¼ä¸‹é¢„æµ‹æ½œåœ¨ç©ºé—´ä½ç§»ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®­ç»ƒå®Œæˆçš„ç­–ç•¥æ— éœ€è°ƒæ•´å³å¯ç›´æ¥éƒ¨ç½²äºå¤šç§æœºå™¨äººå¹³å°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ”¯æŒé€šè¿‡ä»…å­¦ä¹ è½»é‡çº§ã€æœºå™¨äººç‰¹å®šçš„åµŒå…¥å±‚ï¼Œé«˜æ•ˆåœ°å°†æ–°æœºå™¨äººçº³å…¥æ½œåœ¨ç©ºé—´ä½“ç³»ï¼Œä¸”å·²å­¦ä¹ çš„æ½œåœ¨ç­–ç•¥å¯ç›´æ¥è¿ç§»è‡³æ–°æœºå™¨äººã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨å¤šæ ·åŒ–äººå½¢å¹³å°ä¸Šå®ç°é²æ£’ã€å¯æ‰©å±•ä¸”ä¸å…·ä½“å½¢æ€æ— å…³çš„æœºå™¨äººæ§åˆ¶ã€‚",
    "url": "https://arxiv.org/abs/2601.15419v1",
    "html_url": "https://arxiv.org/html/2601.15419v1",
    "html_content": "\\useunder\n\\ul\nLearning a Unified Latent Space for Cross-Embodiment Robot Control\nYashuai Yan\n1\n, Dongheui Lee\n1,2\n1\nYashuai Yan and Dongheui Lee are with Autonomous Systems Lab, Technische UniversitÃ¤t Wien (TU Wien), Vienna, Austria (e-mail:\n{yashuai.yan, dongheui.lee}@tuwien.ac.at\n).\n2\nDongheui Lee is also with the Institute of Robotics and Mechatronics (DLR), German Aerospace Center, Wessling, Germany.\nAbstract\nWe present a scalable framework for cross-embodiment humanoid robot control by learning a shared latent representation that unifies motion across humans and diverse humanoid platforms, including single-arm, dual-arm, and legged humanoid robots. Our method proceeds in two stages: first, we construct a decoupled latent space that captures localized motion patterns across different body parts using contrastive learning, enabling accurate and flexible motion retargeting even across robots with diverse morphologies. To enhance alignment between embodiments, we introduce tailored similarity metrics that combine joint rotation and end-effector positioning for critical segments, such as arms. Then, we train a goal-conditioned control policy directly within this latent space using only human data. Leveraging a conditional variational autoencoder, our policy learns to predict latent space displacements guided by intended goal directions. We show that the trained policy can be directly deployed on multiple robots without any adaptation. Furthermore, our method supports the efficient addition of new robots to the latent space by learning only a lightweight, robot-specific embedding layer. The learned latent policies can also be directly applied to the new robots. Experimental results demonstrate that our approach enables robust, scalable, and embodiment-agnostic robot control across a wide range of humanoid platforms.\nI\nIntroduction\nControlling a diverse range of humanoid platforms through a unified interface remains a long-standing ambition in robotics. Achieving this goal, however, is highly challenging due to the vast variability in robot morphology, degrees of freedom, and kinematic constraints. A key requirement for scalable generalization is to develop control policies that can seamlessly transfer across embodiments without retraining or platform-specific finetuning. In this work, we address the problem of cross-embodiment humanoid robot control by learning a unified shared latent space. Control policies trained in the latent space can be deployed on any robot that is encoded in this space.\nRecent advancements in robot learning, particularly in cross-domain imitation learning\n[\n10\n]\n, have enabled more flexible and adaptable robot control across different embodiments. A common approach to bridging the embodiment gap is to learn task-relevant, domain-invariant representations\n[\n18\n,\n6\n,\n19\n]\n. For example, to enable robots to learn manipulation skills from human demonstrations, prior work\n[\n14\n,\n12\n,\n16\n]\naligns cross-domain skill representations between human videos and robot demonstrations. However, these methods often require a substantial amount of paired demonstration data from both domains, limiting scalability and hindering transferability to new, unseen platformsâ€”an increasingly important consideration given the rapid diversification of robotic morphologies.\nBeyond task-relevant feature extraction, another promising strategy for bridging the embodiment gap is to construct a shared latent space between embodiments, where semantically similar actions are mapped to proximate points regardless of the originating domain. Early efforts in this direction\n[\n4\n,\n9\n,\n1\n]\nfocused on learning shared representations for translating motions from humans to robots or animated characters, but relied heavily on manually collected paired datasets. To overcome this data bottleneck, Choi et al.\n[\n2\n]\nproposed a self-supervised paired data generation technique, automating the process of building correspondences between domains. More recently, Yan et al.\n[\n17\n]\nintroduced a contrastive learning approach to discover a shared latent space, enhancing both the expressiveness and smoothness of motion retargeting between humans and robots. However, the prior work addressed the retargeting from humans to a specific robot embodiment. Our primary goal is to learn a unified latent space across diverse embodiments, which later allows unified policies to control multiple robots.\nIn this work, we tackle the problem of cross-embodiment robot control, where the objective is to develop a single control framework capable of operating across diverse robotic embodiments. Specifically, we aim to achieve two key capabilities: (1) learning a shared latent representation that captures motion semantics across diverse embodiments, and (2) training a goal-conditioned control policy within this shared space that can be directly deployed to all robots that are encoded in the shared space.\nTo this end, we propose a two-stage approach. In the first stage, we learn a unified latent space that aligns motions from varied embodiments into a common representation. Building on insights from contrastive learning and motion retargeting, we introduce a decoupled latent structure that models different body parts independently (arms, legs, trunk). This modular design allows for finer-grained alignment across embodiments with asymmetric or partial limb structures and enables the use of customized similarity metrics, such as combining joint rotation and end-effector positioning for arm movements. Our method not only unifies diverse embodiments into a shared latent space\nbut also improves the cross-embodiment motion retargeting performance.\nIn the second stage, we leverage the learned latent space to train a goal-conditioned motion generation policy based purely on human motions. Using a conditional variational autoencoder (c-VAE) framework, we model motion generation as predicting latent displacements guided by intention features derived from end-effector velocities toward user-specified goals. By operating directly in the latent space, the policy achieves embodiment-agnostic control, allowing motions trained on human demonstrations to be seamlessly executed across multiple robot platforms. Importantly, the modularity of our system enables efficient scaling to new robots: integrating a new embodiment requires only learning a lightweight, robot-specific embedding layer, without re-training the entire model.\nThe proposed approach has the following contributions.\nâ€¢\na decoupled latent space architecture that enables fine-grained motion alignment across a wide range of robot morphologies.\nâ€¢\na modular contrastive learning framework with tailored similarity metrics, capable of jointly learning motion representations from both human and multiple robot embodiments.\nâ€¢\na goal-conditioned control policy that operates directly within the shared latent space, allowing accurate multi-robot control without any additional fine-tuning.\nâ€¢\na scalable framework by adding new robots through lightweight embedding training, enabling efficient deployment in multi-robot systems\nII\nMethodology\nIn this section, we address the cross-embodiment robot control as a two-stage problem. Our method first learns a unified latent space over different robots. Then, we formulate multi-robot control as controlling the shared latent space. An overview of the two stages is illustrated in Fig.\n1\nand Fig.\n2\n, respectively.\nFigure 1:\nLearning a Unified Latent Representation.\nOur architecture learns a shared latent space that unifies motion representations across diverse embodiments, including humans and various robots. To accurately model local motion patterns, we decouple the latent space into five subspaces corresponding to distinct body segments: left arm (LA), right arm (RA), trunk (TK), left leg (LL), and right leg (RL). The model comprises a human encoder (\nE\nh\nE_{h}\n), a cross-embodiment encoder (\nE\nX\nE_{X}\n), and a cross-embodiment decoder (\nD\nX\nD_{X}\n). To accommodate differences in pose dimensionality across robot platforms, each robot is assigned a learnable, robot-specific embedding layer\nE\nr\nE_{r}\n, which projects the raw pose representation into a shared cross-embodiment feature space. Conversely,\nD\nr\nD_{r}\nis the inverse mapping back to the original pose space.\nII-A\nProblem Formulation\nTo enable control across a diverse set of robotic embodiments within a multi-robot system, our approach comprises two main components: (1) learning a shared representation across all robots, and (2) training a control policy within this shared space.\nWe tackle the challenge of learning a shared latent space by enabling robots to retarget motions from other embodiments within this latent representation. To formalize the cross-embodiment motion retargeting problem, we represent a pose of embodiment\nA\nA\nas\nğ±\nA\nâˆˆ\nâ„\nJ\nA\nÃ—\nn\nA\n\\mathbf{x}_{A}\\in\\mathbb{R}^{J_{A}\\times n_{A}}\n, where\nA\nA\ndenotes a specific embodiment (e.g., a human or a robot),\nJ\nA\nJ_{A}\nis the number of joints in embodiment\nA\nA\n, and\nn\nA\nn_{A}\nis the dimensionality of its joint representation. In our setup, human joints are represented using quaternions (\nn\n=\n4\nn=4\n) while robot joints use scalar joint angles (\nn\n=\n1\nn=1\n).\nA motion for Embodiment\nA\nA\nis denoted as\nğ—\nA\n1\n:\nT\n=\n[\nğ±\nA\n1\n,\nğ±\nA\n2\n,\nâ‹¯\n,\nğ±\nA\nT\n]\n\\mathbf{X}_{A}^{1:T}=[\\mathbf{x}_{A}^{1},\\mathbf{x}_{A}^{2},\\cdots,\\mathbf{x}_{A}^{T}]\n. The goal of cross-embodiment motion retargeting is to learn a mapping function\nf\n:\nğ—\nA\n1\n:\nT\nâ†’\nğ˜\nB\n1\n:\nT\nf:\\mathbf{X}_{A}^{1:T}\\rightarrow\\mathbf{Y}_{B}^{1:T}\nthat transfers a motion sequence from embodiment\nA\nA\nto a corresponding motion in embodiment\nB\nB\n, such that the retargeted motion\nğ˜\nB\n1\n:\nT\n\\mathbf{Y}_{B}^{1:T}\npreserves the style and intent of the original motion\nğ—\nA\n1\n:\nT\n\\mathbf{X}_{A}^{1:T}\n.\nWe build on ImitationNet\n[\n17\n]\n, leveraging its unsupervised, contrastive learning-based approach to learn a shared latent space. Unlike the original formulation, limited to human-robot transfer, our method generalizes this latent space to accommodate arbitrary embodiments, facilitating broader cross-platform motion retargeting.\nII-B\nSegment-aware Latent Space and Similarity Metric\nThe prior approaches\n[\n1\n,\n17\n]\nemploy a single latent space to represent whole-body motions. However, this design can lead to ambiguous mappings between embodiments with differing morphologies. For example, the ATLAS robot possesses articulated arms, legs, and a trunk, while the H1 robot exhibits limited trunk articulation, and the TIAGO robot is restricted to arm movements only.\nTo address this limitation, we propose decoupling the latent space into distinct subspaces for different body segments. Specifically, we segment the body into five components: left arm (LA), right arm (RA), trunk (TK), left leg (LL), and right leg (RL). Each segment is modeled with its own dedicated latent space, allowing us to better capture localized motion features and accommodate embodiments with partial or asymmetric limb structures.\nThe decomposition of latent space also enables the use of customized similarity metrics for contrastive learning within each subspace. For example, end-effector position is critical in robot applications such as manipulation. In contrast, mimicking limb rotations is important for the trunk and legs to preserve visual fidelity during motion retargeting. Therefore, we design a hybrid similarity metric for the arm segments that jointly accounts for limb rotations and end-effector positions, while applying a rotation-based similarity metric to the other body segments.\nWe define the rotation-based similarity metric as follows:\nD\nR\n(\nğ±\nA\n,\nğ±\nB\n)\n=\nâˆ‘\nj\n(\n1\nâˆ’\n<\nq\nA\nj\n,\nq\nB\nj\n>\n2\n)\n,\nD_{R}(\\mathbf{x}_{A},\\mathbf{x}_{B})=\\sum_{j}(1-<q_{A}^{j},q_{B}^{j}>^{2}),\n(1)\nwhere\nğ±\nA\n\\mathbf{x}_{A}\nand\nğ±\nB\n\\mathbf{x}_{B}\nare poses from embodiment\nA\nA\nand\nB\nB\n, respectively. The index\nj\nj\ndenotes the number of joints within the body part being comparedâ€”for instance,\nj\n=\n2\nj=2\nfor a typical arm (lower and upper joints). The term\nq\nA\nj\nq_{A}^{j}\nrepresents the quaternion of the\nj\nj\n-th joint in embodiment\nA\nA\n, and\n<\nâ‹…\n,\nâ‹…\n>\n<\\cdot,\\cdot>\ndenotes the dot product between two quaternions.\nTo ensure consistency across embodiments with differing arm lengths and coordinate frames, we first express each EE position relative to the shoulder frame, and then normalize it by the corresponding arm length. The EE-based similarity is computed as:\nD\ne\nâ€‹\ne\nâ€‹\n(\nğ±\nA\n,\nğ±\nB\n)\n=\nâ€–\np\nA\ne\nâ€‹\ne\nâˆ’\np\nB\ne\nâ€‹\ne\nâ€–\n2\n,\nD_{ee}(\\mathbf{x}_{A},\\mathbf{x}_{B})=||p_{A}^{ee}-p_{B}^{ee}||_{2},\n(2)\nwhere\np\nA\ne\nâ€‹\ne\np_{A}^{ee}\nand\np\nB\ne\nâ€‹\ne\np_{B}^{ee}\ndenote the normalized positions of embodiments\nA\nA\nand\nB\nB\n, respectively.\nFor the arm latent subspaces (LA and RA), we compute the similarity as a weighted sum of rotation and EE-based distances:\nS\nk\nâ€‹\n(\nğ±\nA\n,\nğ±\nB\n)\n=\nD\nR\nâ€‹\n(\nğ±\nA\n,\nğ±\nB\n)\n+\nÏ‰\nâ€‹\nD\ne\nâ€‹\ne\nâ€‹\n(\nğ±\nA\n,\nğ±\nB\n)\n\\displaystyle S_{k}(\\mathbf{x}_{A},\\mathbf{x}_{B})=D_{R}(\\mathbf{x}_{A},\\mathbf{x}_{B})+\\omega D_{ee}(\\mathbf{x}_{A},\\mathbf{x}_{B})\n(3)\nfor\nk\nâˆˆ\n{\nL\nâ€‹\nA\n,\nR\nâ€‹\nA\n}\n,\n\\displaystyle\\text{for}\\quad k\\in\\{LA,RA\\},\nwhere\nÏ‰\n\\omega\nis a tunable weight balancing the importance of orientation versus end-effector positioning.\nFor the remaining body parts (TK, LL, RL), we rely solely on the rotation-based metric:\nS\nk\nâ€‹\n(\nğ±\nA\n,\nğ±\nB\n)\n=\nD\nR\nâ€‹\n(\nğ±\nA\n,\nğ±\nB\n)\n\\displaystyle S_{k}(\\mathbf{x}_{A},\\mathbf{x}_{B})=D_{R}(\\mathbf{x}_{A},\\mathbf{x}_{B})\n(4)\nfor\nk\nâˆˆ\n{\nT\nâ€‹\nK\n,\nL\nâ€‹\nL\n,\nR\nâ€‹\nL\n}\n.\n\\displaystyle\\text{for}\\quad k\\in\\{TK,LL,RL\\}.\nThis modular similarity framework allows us to more accurately align motions between embodiments with different capabilities, ultimately improving the quality of cross-embodiment motion retargeting.\nII-C\nLearning a Unified Latent Space\nTo enable motion retargeting across a diverse set of embodiments, we train a unified latent representation shared among humans and multiple robot types. This shared latent space is optimized using contrastive learning applied independently to each decoupled body-part subspace, as described in Section\nII-B\n. The key idea is to embed semantically similar motionsâ€”regardless of embodimentâ€”closely together in their respective latent subspaces.\nFigure\n1\npresents an overview of the model architecture used to learn this unified latent space. The system comprises a human encoder\nE\nh\nE_{h}\n, a cross-embodiment encoder\nE\nX\nE_{X}\n, a cross-embodiment decoder\nD\nX\nD_{X}\n, and robot-specific embedding layers (\nE\nr\n,\nD\nr\nE_{r},D_{r}\n). Since robots may have differing numbers of joints, their raw pose vectors\nğ±\nâˆˆ\nâ„\nJ\nÃ—\nn\n\\mathbf{x}\\in\\mathbb{R}^{J\\times n}\nvary in dimensionality. To handle this variation, each robot pose is first passed through a learnable robot-specific embedding layer, which transforms it into a fixed-size high-dimensional vector. This transformation ensures consistent input dimensionality to the shared encoder.\nAfter embedding, robot poses are processed by the shared encoder, which outputs five latent vectors, corresponding to five latent decoupled subspaces (LA, RA, TK, LL, RL). Similarly, the human encoder network\nE\nh\nE_{h}\nmaps the human poses into five latent vectors, each aligned with one of the body-part-specific subspaces.\nD\nX\nD_{X}\nthen reconstructs the latent vectors back into a shared embedding space, which is finally projected back to robot-specific joint angles by\nD\nr\nD_{r}\n.\nDuring training, each batch of data contains pose samples from both human and robot domains. For each subspace, we apply contrastive learning using randomly sampled triplets composed of poses from any embodiment. A triplet consists of an anchor sample, a positive sample that is semantically similar, and a negative sample that is less similar.\nLet (\nz\ni\no\n,\nz\nj\n+\nz_{i}^{o},z_{j}^{+}\n,\nz\nk\nâˆ’\nz_{k}^{-}\n) represent a triplet in a given subspace, where\nz\ni\no\nz_{i}^{o}\nis the anchor,\nz\nj\n+\nz_{j}^{+}\nis the positive sample, and\nz\nk\nz_{k}\nis the negative sample. We train the latent space using the Triplet Loss\n[\n8\n]\n, defined as:\nâ„’\ncontrastive\n=\nâˆ‘\nğ’®\nâˆ‘\n(\nz\ni\no\n,\nz\nj\n+\n,\nz\nk\nâˆ’\n)\nâˆˆ\nğ’®\nmax\nâ¡\n(\nâ€–\nz\ni\no\nâˆ’\nz\nj\n+\nâ€–\n2\nâˆ’\nâ€–\nz\ni\no\nâˆ’\nz\nk\nâˆ’\nâ€–\n+\nÎ±\n,\n0\n)\n,\n\\mathcal{L}_{\\text{contrastive}}=\\sum_{\\mathcal{S}}\\sum_{(z_{i}^{o},z_{j}^{+},z_{k}^{-})\\in\\mathcal{S}}\\\\\n\\max(||z_{i}^{o}-z_{j}^{+}||_{2}-||z_{i}^{o}-z_{k}^{-}||+\\alpha,0),\n(5)\nwhere\nğ’®\n\\mathcal{S}\ndenotes each subspace, and\nÎ±\n\\alpha\nis a margin hyperparameter that enforces a minimum separation between the positive and negative samples. Intuitively, the loss encourages the positive sample to lie closer to the anchor than the negative sample by at least\nÎ±\n\\alpha\n. In our experiments, we set\nÎ±\n=\n0.05\n\\alpha=0.05\n.\nTo train the decoder\nD\nX\nD_{X}\nto reconstruct robot motions from the latent space accurately, we apply a reconstruction loss\nâ„’\nrec\n\\mathcal{L}_{\\text{rec}}\n. This loss ensures that the decoded output closely matches the original input robot pose. Given an input robot pose\nğ±\nA\n\\mathbf{x}_{A}\nfrom embodiment\nA\nA\n, and the reconstructed pose\nğ±\n^\nA\n\\mathbf{\\hat{x}}_{A}\n, the reconstruction loss is defined as:\nâ„’\nrec\n=\nâ€–\nğ±\nA\nâˆ’\nğ±\n^\nA\nâ€–\n2\n.\n\\mathcal{L}_{\\text{rec}}=||\\mathbf{x}_{A}-\\mathbf{\\hat{x}}_{A}||_{2}.\n(6)\nHere,\nA\nA\ndenotes any robotic embodiment. The reconstruction loss serves to regularize the latent-to-pose mapping, ensuring that the decoder maintains fidelity to the input motion during encoding and decoding.\nHowever, for human motion data,\nğ±\nH\n\\mathbf{x}_{H}\n, direct reconstruction is not possible due to the lack of paired human-to-robot data. Instead, we employ a latent consistency loss\nâ„’\nltc\n\\mathcal{L}_{\\text{ltc}}\n, inspired by the circular loss introduced in ImitationNet\n[\n17\n]\n. This loss encourages the decoder to generate robot motions that remain faithful to the original human motion when viewed in the latent space.\nSpecifically,\nâ„’\nltc\n\\mathcal{L}_{\\text{ltc}}\nmeasures the discrepancy between the original human latent representation\nE\nh\nâ€‹\n(\nğ±\nH\n)\nE_{h}(\\mathbf{x}_{H})\nand the latent representation obtained by re-encoding the decoded robot motion:\nE\nX\nâ€‹\n(\nD\nX\nâ€‹\n(\nE\nh\nâ€‹\n(\nğ±\nH\n)\n)\n)\nE_{X}(D_{X}(E_{h}(\\mathbf{x}_{H})))\n. The loss is formally defined as:\nâ„’\nltc\n=\nâ€–\nE\nh\nâ€‹\n(\nğ±\nH\n)\nâˆ’\nE\nX\nâ€‹\n(\nD\nX\nâ€‹\n(\nE\nh\nâ€‹\n(\nğ±\nH\n)\n)\n)\nâ€–\n2\n\\mathcal{L}_{\\text{ltc}}=||E_{h}(\\mathbf{x}_{H})-E_{X}(D_{X}(E_{h}(\\mathbf{x}_{H})))||_{2}\n(7)\nThis latent consistency objective ensures that motions generated by decoding human embeddings can be re-encoded to yield representations close to the original human latent space. It effectively aligns the human and robot domains, thereby improving the quality of cross-embodiment motion retargeting.\nFinally, to enhance the temporal consistency of retargeted motions, we introduce a temporal loss\nâ„’\ntemporal\n\\mathcal{L}_{\\text{temporal}}\n, which focuses on aligning the end-effector velocities between human and robot motions. Specifically, we consider two consecutive human poses,\nğ±\nH\nt\n\\mathbf{x}_{H}^{t}\nand\nğ±\nH\nt\n+\n1\n\\mathbf{x}_{H}^{t+1}\n, and their corresponding retargeted robot poses,\nğ±\nA\nt\n\\mathbf{x}_{A}^{t}\nand\nğ±\nA\nt\n+\n1\n\\mathbf{x}_{A}^{t+1}\n, where\nA\nA\ndenotes a target robot embodiment.\nFrom these consecutive frames, we compute the human hand velocity\nv\nH\nhand\nv_{H}^{\\text{hand}}\nand robot EE velocity\nv\nA\nee\nv_{A}^{\\text{ee}}\n. The temporal loss is then defined as the L2 distance between these velocity vectors:\nâ„’\ntemporal\n=\nâ€–\nv\nH\nhand\nâˆ’\nv\nA\nee\nâ€–\n2\n\\mathcal{L}_{\\text{temporal}}=||v_{H}^{\\text{hand}}-v_{A}^{\\text{ee}}||_{2}\n(8)\nTo train the model end-to-end, we combine all previously defined objectives into a single weighted loss function:\nâ„’\ntotal\n=\nÎ»\nc\nâ€‹\nâ„’\ncontrastive\n+\nÎ»\nrec\nâ€‹\nâ„’\nrec\n+\nÎ»\nl\nâ€‹\nt\nâ€‹\nc\nâ€‹\nâ„’\nltc\n+\nÎ»\nt\nâ€‹\ne\nâ€‹\nm\nâ€‹\np\nâ€‹\nâ„’\ntemporal\n\\mathcal{L}_{\\text{total}}=\\lambda_{c}\\mathcal{L}_{\\text{contrastive}}+\\lambda_{\\text{rec}}\\mathcal{L}_{\\text{rec}}+\\lambda_{ltc}\\mathcal{L}_{\\text{ltc}}+\\lambda_{temp}\\mathcal{L}_{\\text{temporal}}\n(9)\nwhere we empirically set\nÎ»\nc\n=\n10\n,\nÎ»\nrec\n=\n5\n,\nÎ»\nl\nâ€‹\nt\nâ€‹\nc\n=\n1\n,\nÎ»\nt\nâ€‹\ne\nâ€‹\nm\nâ€‹\np\n=\n0.1\n\\lambda_{c}=10,\\lambda_{\\text{rec}}=5,\\lambda_{ltc}=1,\\lambda_{temp}=0.1\n.\nBy minimizing\nâ„’\ntotal\n\\mathcal{L}_{\\text{total}}\n, the model learns a unified latent space in which motions can be accurately and smoothly retargeted across a wide range of embodiments. This shared space enables us to train control policies within the latent domain and seamlessly deploy them across multiple robots, regardless of their structural differences.\nII-D\nAdding New Robots\nOur architecture is designed to facilitate efficient scalability by maintaining a shared latent space and shared network components across all embodiments. While the core encoders and decoderâ€”\nE\nh\n,\nE\nX\nE_{h},E_{X}\n, and\nD\nX\nD_{X}\nâ€”are jointly trained across multiple robots and humans, each robot retains its own learnable embedding layer to account for embodiment-specific differences.\nThis modular design enables seamless integration of new robots into the system. Specifically, once the shared components of the model are pre-trained, we can freeze the networks\nE\nh\n,\nE\nX\nE_{h},E_{X}\n, and\nD\nX\nD_{X}\n, and train only the embedding layers (\nE\nr\n,\nD\nr\nE_{r},D_{r}\n) for the new robot. This approach allows the new robot to align with the existing latent space without requiring retraining of the entire model, reducing computational cost and training time.\nAs a result, our framework supports scalable deployment to new robotic platforms while preserving the generalization and robustness of the unified control policy learned in the latent space.\nFigure 2:\nGoal-conditioned Latent-Space Robot Control.\nOur c-VAE framework learns to model goal-directed motion dynamics in our shared latent space purely from human demonstrations. During training, the model conditions on the current latent pose\nz\nt\nz_{t}\nand the average EE velocity toward a sampled goal\nv\nÂ¯\ne\nâ€‹\ne\n\\overline{v}_{ee}\nto predict the latent displacement\nd\nt\nd_{t}\n. At inference time,\nv\nÂ¯\ne\nâ€‹\ne\n\\overline{v}_{ee}\nis derived from the current robot pose and the user-specified goal position and time horizon. The decoder then generates the latent displacement\nd\n^\nt\n\\hat{d}_{t}\nusing\nz\nt\nz_{t}\n,\nv\nÂ¯\ne\nâ€‹\ne\n\\overline{v}_{ee}\n, and sampled latent noise. This autoregressive process iteratively updates the latent state, enabling smooth and goal-directed motion generation across robot embodiments.\nII-E\nLatent-Space Robot Control\nWith our cross-embodiment retargeting framework in Fig.\n1\n, human and robot motions are projected into a unified latent space, where semantically similar movements across embodiments are encoded close together. This shared representation enables a high-level abstraction of robot control: rather than specifying joint commands, robot behavior can be modulated by traversing this latent space.\nTo exploit this capability, we propose a goal-conditioned motion generation policy that operates directly in the latent space and is trained exclusively on human data. Crucially, this policy can be deployed to multiple robot platforms without any further fine-tuning. We focus on a practical and widely applicable control objective in roboticsâ€”generating arm movements that guide the robotâ€™s EE to reach a desired goal position.\nWe model this task using the c-VAE architecture, where the motion generation is guided by control signals. During training, we utilize human hand positions to extract the goal signal. Due to the modular structure of our decoupled latent space (see Sec.\nII-B\n), the policy targets only the relevant subspace (e.g., LA or RA), allowing efficient and focused learning for arm control.\nTraining proceeds as follows: from a human motion sequence\nğ—\nH\n1\n:\nT\n\\mathbf{X}_{H}^{1:T}\n, we sample three frames:\nğ±\nH\nt\n,\nğ±\nH\nt\n+\n1\n\\mathbf{x}_{H}^{t},\\mathbf{x}_{H}^{t+1}\nand a future goal frame\nğ±\nH\nt\nG\n\\mathbf{x}_{H}^{t_{G}}\n. Hand positions\np\nhand\nt\np_{\\text{hand}}^{t}\nand\np\nhand\nt\nG\np_{\\text{hand}}^{t_{G}}\nare extracted from these frames, and the average velocity vector toward the goal is computed as\nv\nÂ¯\nee\n=\np\nhand\nt\nG\nâˆ’\np\nhand\nt\nt\nG\nâˆ’\nt\n.\n\\overline{v}_{\\text{ee}}=\\frac{p_{\\text{hand}}^{t_{G}}-p_{\\text{hand}}^{t}}{t_{G}-t}.\nv\nÂ¯\nee\n\\overline{v}_{\\text{ee}}\nserves as an intention signal, providing directional guidance for generating future motion. Concurrently, the human encoder\nE\nh\nE_{h}\nencodes the poses\nğ±\nH\nt\n\\mathbf{x}_{H}^{t}\nand\nğ±\nH\nt\n+\n1\n\\mathbf{x}_{H}^{t+1}\ninto latent representation\nz\nt\nz_{t}\nand\nz\nt\n+\n1\nz_{t+1}\n, respectively.\nRather than reconstructing absolute poses, our model learns to predict the latent displacement vector\nd\nt\n=\nz\nt\n+\n1\nâˆ’\nz\nt\nd_{t}=z_{t+1}-z_{t}\n, which captures motion dynamics between consecutive frames. Predicting these deltas also benefits from the inherent inductive biases of sequential datasets, which contribute to more sample-efficient learning and improved generalizability, as noted in prior work\n[\n5\n,\n13\n,\n15\n]\n. The design of architecture (in Fig.\n2\n), conditioning on\nz\nt\nz_{t}\nand\nv\nÂ¯\ne\nâ€‹\ne\n\\overline{v}_{ee}\n, ensures that motion generation is informed both by the current pose and by the goal-directed intention.\nThe overall training objective for the c-VAE consists of a reconstruction loss and a KL divergence regularization:\nâ„’\ncvae\n=\nâ„’\nreconstruction\n+\nÎ»\nKL\nâ€‹\nâ„’\nKL\n.\n\\mathcal{L}_{\\text{cvae}}=\\mathcal{L}_{\\text{reconstruction}}+\\lambda_{\\text{KL}}\\mathcal{L}_{\\text{KL}}.\n(10)\nThe reconstruction loss,\nâ„’\nreconstruction\n=\nâ€–\nd\nt\nâˆ’\nd\n^\nt\nâ€–\n2\n2\n\\mathcal{L}_{\\text{reconstruction}}=||d_{t}-\\hat{d}_{t}||_{2}^{2}\n, encourages the faithful reproducing of human motions.\nThe KL divergence term,\nâ„’\nKL\n=\nD\nKL\n(\nğ’©\n(\n0\n,\nI\n)\n|\n|\nğ’©\n(\nÎ¼\n,\nÏƒ\n)\n)\n\\mathcal{L}_{\\text{KL}}=D_{\\text{KL}}(\\mathcal{N}(0,I)||\\mathcal{N}(\\mu,\\sigma))\n, aligns the encoderâ€™s output distribution with a standard Gaussian prior.\nÎ¼\n\\mu\nand\nÏƒ\n\\sigma\nare the predicted mean and variance of the latent distribution. We set\nÎ»\nK\nâ€‹\nL\n=\n10\nâˆ’\n4\n\\lambda_{KL}=10^{-4}\nto ensure that the regularization term supports, but does not dominate the reconstruction objective.\nAt inference time, we deploy the learned policy to generate robot motions toward a user-specified goal. The process begins with the user providing a target EE position\np\ne\nâ€‹\ne\nT\np_{ee}^{T}\nand a time horizon\nT\nT\n. Given the current EE position\np\ne\nâ€‹\ne\nt\np_{ee}^{t}\nat time step\nt\nt\n, we compute the goal-directed velocity vector as\nv\nÂ¯\ne\nâ€‹\ne\n=\np\ne\nâ€‹\ne\nT\nâˆ’\np\ne\nâ€‹\ne\nt\nT\nâˆ’\nt\n\\overline{v}_{ee}=\\frac{p_{ee}^{T}-p_{ee}^{t}}{T-t}\n.\nSimultaneously, the current robot pose is encoded into the latent space as\nz\nt\nz_{t}\n. The decoder, conditioned on\nz\nt\nz_{t}\nand the intention vector\nv\nÂ¯\ne\nâ€‹\ne\n\\overline{v}_{ee}\n, along with a sampled latent noise, predicts the latent displacement\nd\n^\nt\n\\hat{d}_{t}\nthat represents the transition to the next state. The next latent representation is then computed as\nz\nt\n+\n1\n=\nz\nt\n+\nd\n^\nt\nz_{t+1}=z_{t}+\\hat{d}_{t}\n.\nImportantly, at each step, the intention vector\nv\nÂ¯\ne\nâ€‹\ne\n\\overline{v}_{ee}\nis dynamically updated based on the predicted EE position and the original goal, enabling the policy to adapt to real-time changes and maintain goal-oriented motion. This process is executed in an autoregressive fashion, iteratively generating a sequence of latent states that are subsequently decoded into robot joint configurations.\nIII\nExperiments\nThis section is organized as follows: We begin by outlining the implementation details to support reproducibility. Next, we introduce the baseline methods. Finally, we present a comprehensive evaluation of our approach on multiple robots with different morphologies.\nIII-A\nTechnical Implementation\nIII-A\n1\nLearning a Unified Latent Space\nThe two encoders (\nE\nh\nE_{h}\nand\nE\nX\nE_{X}\n) and the decoder (\nD\nX\nD_{X}\n) in our architecture are implemented using multilayer perceptrons (MLPs). Each MLP consists of 8 fully connected layers, with 256 neurons per layer. We use Exponential Linear Units (ELU)\n[\n3\n]\nas the activation function for intermediate layers and apply a Tanh activation at the output layer to constrain the output range. Each latent subspace is 16-dimensional, with values bounded between -1 and 1.\nTo handle embodiment-specific differences in pose dimensionality, each robot is equipped with a learnable embedding layer\nE\nr\nE_{r}\nthat projects its joint space into a 1024-dimensional feature space. An inverse embedding layer\nD\nr\nD_{r}\nis used to map this shared representation back into the robot-specific joint space, enabling the reconstruction of the original pose.\nIII-A\n2\nLatent-Space Robot Control\nSimilarly, the encoder and decoder networks in the c-VAE are also implemented as MLPs, each comprising 8 linear layers. Every layer is followed by an ELU activation, except for the output layer, which uses no activation function to allow unbounded outputs. The encoder outputs a 32-dimensional latent distribution, representing the mean and variance parameters of a Gaussian distribution from which latent variables are sampled during training.\nIII-A\n3\nTraining Details\nWe developed and trained our method using the PyTorch framework. For optimization, we use Adam\n[\n11\n]\nwith a constant learning rate of\n10\nâˆ’\n3\n10^{-3}\n. Our models are trained with a batch size of\n10\n5\n10^{5}\non an NVIDIA A4000 GPU.\nIII-B\nDatasets\nWe use the HumanML3D dataset\n[\n7\n]\n, which contains 29,224 diverse human motion sequences comprising over 4 million human poses. Notably, our approach does not require any robot data collection. Instead, we sample robot joint configurations uniformly at random from their respective joint spaces during training and compute the corresponding robot poses using forward kinematics (FK). For FK computation, we employ PyTorch-Kinematics\n[\n20\n]\n, which efficiently parallelizes FK calculations on the GPU using only robot URDFs. This design enables the entire training pipelineâ€”from data sampling to neural network updatesâ€”to be executed entirely on GPUs, thereby improving training efficiency. At each training step, over\n10\n5\n10^{5}\nnew robot poses are sampled and immediately discarded after updating the networks, ensuring diverse and unbiased exposure without requiring dataset storage. Over the course of training, this results in billions of robot poses per embodiment. This large-scale synthetic sampling enables our model to explore the robotâ€™s kinematic space comprehensively and supports the formation of a smooth and expressive latent space.\nIII-C\nBaselines\nTo assess the performance of our framework, we evaluate it across three key settings: human-to-robot motion retargeting, cross-embodiment generalization, and robot end-effector control. Each baseline is selected to highlight specific aspects of our methodâ€™s capabilityâ€”representational efficiency, embodiment scalability, and control precision.\nIII-C\n1\nHuman-to-Robot Motion Retargeting\nWe first compare our method against ImitationNet\n[\n17\n]\n, a deep-learning-based approach originally designed to retarget motions from human demonstrations to a dual-arm TIAGo++ robot. Unlike our unified latent space that supports multiple embodiments, ImitationNet learns a shared space from humans for one single robot. For a fair evaluation, we train individual ImitationNet models for each target robot, such as human-to-JVRC and human-to-H1. This comparison highlights the scalability and generalization of our shared latent representation, which enables joint learning across all robots in a single model.\nIII-C\n2\nCross-Embodiment Motion Retargeting\nTo evaluate the effect of our decoupled latent space, we compare it against a monolithic, whole-body latent representation trained under an otherwise identical setup. We hypothesize that separating the latent space into semantically aligned subspaces (e.g., arms, legs, trunk) provides better modularity and embodiment-\ninvarianceâ€”critical when mapping motions between diverse\nmorphologies. By toggling between the decoupled and coupled\nvariants of our framework, we isolate and quantify the impact\nof this architectural choice.\nIII-D\nQuantitative and Qualitative Evaluation\nTo evaluate our models on diverse and unseen motions, we split the HumanML3D dataset into\n80\n%\n80\\%\nof training data and\n20\n%\n20\\%\nof testing data.\nIII-D\n1\nMetrics\nTo quantitatively evaluate motion retargeting quality and goal-reaching performance, we define the following metrics:\nâ€¢\nRotation Similarity (RS)\nmeasures the rotational consistency of body limb orientations across different embodiments. Specifically, it computes the average angular distance between corresponding joints as defined in Eq.\n1\n.\nâ€¢\nNormalized Distance Similarity (NDS)\nevaluates the spatial alignment of end-effector (or hand) positions after normalization. To account for scale differences across embodiments, all positions are transformed into the shoulder-local coordinate frame and normalized by arm length, as detailed in Sec.\nII-B\n. The formal definition is given in Eq.\n2\n.\nâ€¢\nNormalized Velocity Similarity (NVS)\nassesses the similarity of end-effector velocities between retargeted and reference motions. This metric is crucial for validating the consistency of latent dynamics across embodiments, as it reflects the correspondence between motion intent and execution. NVS is computed as in Eq.\n8\n.\nâ€¢\nDistance to Goal (DTG)\nmeasures the absolute position distance of the end-effector in goal-conditioned tasks. It is defined as the Euclidean distance between the predicted end-effector position at the final frame and the user-specified goal location.\nIII-D\n2\nMotion Retargeting\nRotation Similarity (in degree)\nNormalized Distance Similarity\nNormalized Velocity Similarity\nsource\ntarget\nImitationNet\nours\n(coupled)\nours\n(decoupled)\nImitationNet\nours\n(coupled)\nours\n(decoupled)\nImitationNet\nours\n(coupled)\nours\n(decoupled)\nHuman\nTIAGo++\n0.7183\n4.2622\n\\ul\n3.8293\n0.1325\n\\ul\n0.0492\n0.0401\n0.3762\n\\ul\n0.1252\n0.1071\nH1\n0.6483\n2.1268\n\\ul\n1.0947\n0.1081\n\\ul\n0.0353\n0.0263\n0.2881\n\\ul\n0.1116\n0.0962\nNAO\n0.6685\n4.2371\n\\ul\n2.7097\n0.1596\n\\ul\n0.0635\n0.0566\n0.3682\n\\ul\n0.1604\n0.1448\nJVRC\n0.5792\n1.9812\n\\ul\n1.4631\n0.1006\n\\ul\n0.0289\n0.0288\n0.2383\n\\ul\n0.1001\n0.0862\nJVRC\nH1\n-\n0.6962\n0.4699\n-\n0.0232\n0.0206\n-\n0.0822\n0.0869\nH1\nNAO\n-\n2.7630\n1.8093\n-\n0.0502\n0.0511\n-\n0.1316\n0.1300\nTIAGo++\nJVRC\n-\n3.3615\n2.2428\n-\n0.0402\n0.0360\n-\n0.1058\n0.1040\nTABLE I:\nMotion Retargeting.\nComparison of different imitation models. Notice that ImitationNet trains an individual model for each human-to-robot mapping, while our method trains a unified model across all embodiments. We also conduct ablation study on the latent space decomposition for cross-embodiment retargeting.\nWe evaluate our method and the baselines on the testing dataset using the defined metrics. Our focus is on assessing cross-embodiment motion retargeting performance across four distinct robot platforms: TIAGo++, H1, NAO, and JVRC. These robots span a range of morphologies, from mobile-base bimanual arms (TIAGo++) to fully legged humanoids (JVRC), introducing varying kinematic constraints and control challenges. Detailed specifications of each robot are provided in Appendix\nA\n.\nFor the baseline ImitationNet\n[\n17\n]\n, a separate model is trained for each robot, while our method trains a single unified model across human and all robot embodiments. Table\nI\npresents a quantitative comparison of our approach against the baselines. Our method significantly outperforms existing techniques in terms of end-effector position and velocity accuracy, both of which are critical for enabling precise goal-conditioned control policies in the learned latent space.\nHowever, we observe a decline in rotational accuracy when using a shared latent space across all embodiments. This degradation arises due to intrinsic differences in robot kinematics: for instance, TIAGO is restricted to arm motions, whereas H1 and NAO possess legged locomotion but have limited trunk mobility compared to humans or JVRC. To mitigate this, we adopt a decoupled latent space design, which not only recovers rotational performance comparable to ImitationNet but also yields further improvements in EE trajectory fidelity.\nFurthermore, the use of a unified model across diverse embodiments enables our method to support motion transfer among different robots via the shared latent space. This capability demonstrates the potential of our approach to scale to multi-robot systems, where a single policy can be used to control multiple robots. We explore this application further in the next section.\nIII-D\n3\nAdding New Robots\nsource\ntarget\nRS (degree)\nNDS\nNVS\nHuman\nG1\n1.2124\n0.0396\n0.1284\nATLAS\n1.4008\n0.0336\n0.1272\nKinova\n3.4249\n0.0491\n0.0764\nATLAS\nJVRC\n0.9862\n0.0329\n0.1218\nJVRC\nG1\n1.1174\n0.0398\n0.1245\nTIAGO\nATLAS\n1.1471\n0.0455\n0.1288\nKinova\nTIAGo++\n1.5011\n0.0372\n0.0902\nTABLE II:\nAdding New Robots.\nWe assess the generalizability and scalability of our latent space by introducing three new robotsâ€”G1, ATLAS, and Kinova. For new robots, we freeze most networks (\nE\nh\n,\nE\nX\nE_{h},E_{X}\n, and\nD\nX\nD_{X}\n) and only train the robot-specific embedding layers (\nE\nr\nE_{r}\nand\nD\nr\nD_{r}\n).\nTo assess the scalability of our method in incorporating new robots, we demonstrate that minimal additional effort is requiredâ€”specifically, only a single robot-specific embedding layer needs to be learned, while the rest of the network remains fixed.\nOur model is first trained end-to-end on the four primary robots introduced earlier. Once training is complete, we freeze the core networks (\nE\nh\nE_{h}\n,\nE\nX\nE_{X}\n, and\nD\nX\nD_{X}\n) and train only the robot-specific embedding layers (\nE\nr\nE_{r}\nand\nD\nr\nD_{r}\n) for the newly added robotsâ€”ATLAS, G1, and Kinova Gen3. This approach dramatically reduces the training time, requiring only about 15 minutes, compared to several hours for full retraining from scratch. We evaluate these new robots on both human-to-robot and robot-to-robot motion retargeting tasks, with results presented in Table\nII\n. Across all evaluation metrics, the performance of the new robots matches that of the original robots trained end-to-end. These results highlight the generalizability of the learned latent space across different embodiments, making our method highly scalable and adaptable to new platforms with minimal retraining.\nIII-D\n4\nVisual Resemblance\nFigure 3:\nComparison of visual resemblance.\nWe retarget various dynamic human motions onto different robots (H1, JVRC, and TIAGo, from left to right), and compare different retargeting models. The result shows that both ImitationNet and our method with a decoupled latent space obtain high-quality visual resemblance. Our method trains a single model on all robots, while ImitationNet overfits each robot to a separate model.\nFigure 4:\nCross-embodiment Motion Retargeting.\nWe translate motions between any embodiments, and the results showcase the capability of our method, capturing the motion semantics across diverse embodiments.\nA primary goal of motion retargeting is to transfer motions across different embodiments while maintaining their visual fidelity. In Fig.\n3\n, we present a qualitative comparison between our method and the baseline using dynamic, visually expressive motions. The top row depicts the original human motions to be retargeted. ImitationNet delivers high visual similarity by training a dedicated model for each robot. In contrast, our method using a coupled shared latent space struggles to accurately capture leg motions across embodiments. However, when the latent space is decoupled by body segments, our approach matches the visual quality of ImitationNet while retaining the benefit of a shared, generalizable latent space across all robots.\nFigure\n4\npresents additional results demonstrating our methodâ€™s ability to retarget motions between different robots. The results show that, despite variations in embodiment, the shared latent representation effectively preserves the visual resemblance of the original motion. Notably, our method achieves high-quality motion transfer across robots with significantly different kinematic structuresâ€”for example, from the legged H1 robot to the mobile-based TIAGO, and from TIAGO back to the legged ATLAS robot.\nIII-D\n5\nLatent-Space Robot Control\nDistance to Goal (in cm)\nTIAGO\nH1\nNAO\nJVRC\nG1\nATLAS\nc-VAE\n1.14\n0.44\n0.13\n0.45\n0.42\n0.56\nTABLE III:\nWe evaluate the latent-space policy on multiple robots and showcase the performance of goal reaching for each robot.\nFigure 5:\nLatent-Space Multi-Robot Control.\nA single policy controls the latent space to enable each robot to reach arbitrary goal positions (blue) from various starting poses (green). Intermediate waypoints along the generated trajectories are shown in purple.\nA key objective of our shared latent space is to enable the learning of control policies that generalize across different robot embodiments. Following the method described in Section\nII-E\n, we train a latent-space control policy conditioned on the robotâ€™s end-effector goal position. We evaluate this policy using two criteria: (i) the distance to goal (DTG) metric, which measures the accuracy of reaching the specified EE goal positions, and (ii) computational efficiency. Owing to the decoupled structure of our latent representation, each body segment can be independently controlled. In this study, we focus on learning a policy for controlling the right arm.\nTo assess the performance of the learned policy, we conduct 1,000 experiments per robot, each with a randomly sampled initial pose and goal EE position. The results, summarized in Table\nIII\n, show that our latent policy enables control of the robotic EE across diverse platforms and achieves sub-centimeter accuracy. Additionally, our latent-space robot control framework is also computationally efficient, supporting control frequencies of approximately 100 Hz.\nFigure\n5\nvisualizes representative trajectories generated by our latent-space control policy across different robots. Each robot starts from a random initial pose, with EE positions indicated by green spheres. The randomly sampled goal positions are shown in blue, and intermediate waypointsâ€”shown in purpleâ€”highlight the smoothness of the generated motion trajectories, further demonstrating the effectiveness of our learned controller.\nIII-D\n6\nMotion Editing within Latent Space\nFigure 6:\nLatent-Space Editing.\nOur latent space enables motion editing in the latent space, such as composing new motions by combining partial body movements from different sources.\nFigure 7:\nMotion Editing.\nOur method can synthesize new motions for any target embodiment from different source embodiments by composing their shared latent space.\nThanks to the decoupled design of our latent space, our method enables a range of versatile applications. One example is independent control of individual body segments, as demonstrated in Figure\n5\n. Additionally, the learned latent space supports motion editing capabilities, such as composing new motions by combining partial movements from different sources. Figure\n6\nillustrates this functionality: in the shown example, we synthesize a new ATLAS motion by stitching together the arm motion from TIAGO with the trunk and leg motion from ATLASâ€”seamlessly blended within the latent space. Figure\n7\nshowcases additional examples of motion synthesis, where target-domain motions are generated by combining source-domain motions from different embodiments. The high-quality visual consistency of the composed motions highlights the effectiveness of our cross-domain motion retargeting and the strong decoupling capabilities of the learned shared latent space.\nIII-D\n7\nAblation Study\nHuman-to-TIAGo++\nHuman-to-JVRC\nw\n,\nÎ»\nc\nw,\\lambda_{c}\nRS\nNDS\nNVS\nRS\nNDS\nNVS\n1.0\n,\n5.0\n1.0,5.0\n4.2224\n0.0487\n0.1077\n1.6395\n0.0367\n0.0919\n1.0\n,\nğŸğŸ\n\\mathbf{1.0},\\mathbf{10}\n3.8293\n0.0401\n0.1071\n1.4631\n0.0288\n0.0862\n1.0\n,\n15\n1.0,15\n3.6749\n0.0482\n0.1111\n1.3970\n0.0323\n0.0942\n0.5\n,\n10\n0.5,10\n3.0738\n0.0460\n0.1020\n1.1512\n0.0349\n0.0880\n1.5\n,\n10\n1.5,10\n3.2215\n0.0495\n0.1100\n1.1814\n0.0306\n0.1010\nTABLE IV:\nAblation Study on Hyperparameter\n.\nw\nw\nand\nÎ»\nc\n\\lambda_{c}\nare essential parameters for the similarity metric in Eq.\n3\nand objective function in Eq.\n9\n. Bold indicates the selected parameters.\nTo analyze the effect of hyperparameters on the training process, we conduct an ablation study on two key parameters,\nw\nw\nand\nÎ»\nc\n\\lambda_{c}\n, defined in Eq.\n3\nand Eq.\n9\n, respectively. The parameter\nw\nw\ncontrols the balance between limb rotations and hand positions in the similarity metric, while\nÎ»\nc\n\\lambda_{c}\ndetermines the weight of the contrastive loss in the overall objective. Table\nIV\nreports the motion retargeting performance on the TIAGo and JVRC robots under different parameter settings. Our selected configuration,\nw\n=\n1.0\nw=1.0\nand\nÎ»\nc\n=\n10\n\\lambda_{c}=10\n, achieves a strong trade-off between visual fidelity and end-effector controllability.\nFigure 8:\nTeleoperation from an RGB camera.\nThe dual-arm TIAGo-SEA is teleoperated in real time to perform object pick-and-place (top) and bimanual object transfer from the right hand to the left hand (bottom).\nFigure 9:\nRobot Control via Latent Space.\nBy operating in the latent space, the Kinova robot performs tasks such as reaching target positions and executing pick-and-place.\nFigure 10:\nRobot-to-Robot Retargeting.\nThe TIAGo-SEAâ€™s right arm follows the motion of the Kinova robot, after applying a transformation to better align the workspaces of the two robots.\nIII-D\n8\nExperiments on Hardware\nWe test our method on two different robot platforms: the TIAGo-SEA and Kinova Gen3 robot. To fully evaluate the capabilities of our method, we conduct different experiments: human-to-robot and robot-to-robot motion retargeting, and robot control through our latent space.\nFigure\n8\nshows real-time robot teleoperation using only an RGB camera with our method. The dual-armed TIAGo-SEA robot is teleoperated to perform object pick-and-place by throwing items into a trash bin. We also demonstrate bimanual object transfer, highlighting the utility of dual-arm teleoperation enabled by our approach.\nIn Figure\n10\n, the motion of the Kinova arm is transferred to the TIAGo-SEAâ€™s right arm. Since the Kinova is mounted in a tabletop setting, we transform its base link to correspond to the right shoulder, thereby aligning the workspaces of the two robots.\nFurthermore, Figure\n9\ndemonstrates our methodâ€™s ability to control robots directly in the shared latent space. The Kinova robot is controlled to pick up objects and drop them into a glass. Successful execution of these tasks requires precise end-effector control, which highlights the effectiveness of our method.\nIII-E\nDiscussion and Future Work\nIn this work, we proposed an unsupervised learning approach to construct a cross-embodiment latent space in which semantically similar motions are closely aligned, regardless of the source embodiment. Leveraging this shared representation, we introduced a c-VAE framework to learn goal-conditioned latent control policies from only human data, and the policies can be directly deployed across diverse robotic platforms.\nCurrently, our motion retargeting is trained on the HumanML3D dataset, which contains human motions in the SMPL model. Since the SMPL model does not capture hand movements, it considers the hands as part of the forearms. As a result, due to the missing hand motion in the SMPL, hand motion retargeting is not handled in this paper, limiting applications that require fine-grained teleoperation. In future work, we plan to address this limitation by using existing (or collecting) hand datasets to incorporate human hand imitation, enhancing the robotâ€™s latent control policy to support a wide range of manipulation tasks.\nFigure 11:\nMapping among different embodiments.\nFigure 12:\nVisualization of Latent Space.\nAfter dimension reduction with PCA, we visualize the projected latent space trajectories of the retargeted walking motion from multiple robots.\nIV\nConclusions\nCross-embodiment control is a critical capability for enabling scalable and flexible multi-robot systems. In this work, we introduced a two-stage framework to address this challenge: (1) learning a shared latent space that encodes semantically similar motions across diverse embodiments, and (2) training control policies within this space that can be deployed on a wide range of robots without further adaptation.\nIn the first stage, we leverage contrastive learning to build a generalizable and scalable latent representation across multiple robot and human embodiments. We validate this latent space through extensive quantitative and qualitative experiments on both human-to-robot and robot-to-robot motion retargeting. When adding new robots to our shared latent space, only a lightweight robot-specific embedding layer needs to be trained, while the rest of the model remains frozen. Despite this minimal adaptation, newly added robots achieve performance comparable to those trained end-to-end, demonstrating the generalizability of the learned latent space. As a result, trained control policies can be directly deployed on new robots, significantly enhancing the efficiency of multi-robot system management.\nIn the second stage, we introduce a goal-conditioned control method in the latent space using a c-VAE architecture. This policy is trained exclusively on human motions and can be directly deployed across multiple robots. Our experiments show that the learned policy achieves accurate end-effector control, achieving sub-centimeter errors across different platforms.\nOverall, our method demonstrates a promising path toward unified, scalable, and adaptable control across diverse robot embodiments.\nAppendix A\nRobots with different embodiments\nTABLE V:\nHuman body parts that robots can mimic\nRobot\nArms\nTrunk\nLegs\nArm-Ratio\nATLAS\nâœ“\nâœ“\nâœ“\n1.00\nH1\nâœ“\nâœ—\nâœ“\n0.90\nG1\nâœ“\nâœ—\nâœ“\n1.43\nJVRC\nâœ“\nâœ“\nâœ“\n0.78\nNAO\nâœ“\nâœ—\nâœ“\n0.53\nTIAGo++\nâœ“\nâœ—\nâœ—\n1.16\nKinova\nâœ“\nâœ—\nâœ—\n1.59\nWe evaluate our method on six robots that span a wide range of morphologies, from mobile-based bimanual platforms to fully bipedal humanoids. Figure\n11\nvisualizes the kinematic mappings between different embodiments, while Table\nV\nsummarizes the capabilities of each robot. For example, the ATLAS robot has fully actuated arms, legs, and a trunk, whereas the TIAGo robot is limited to arm movements only. The final column in the table reports the arm ratioâ€”defined as the length ratio between the forearm and upper armâ€”for each robot. These ratios vary significantly, ranging from 0.53 to 1.43, reflecting the morphological diversity of the platforms and highlighting the inherent challenges of developing a unified multi-robot control framework.\nAppendix B\nLatent Space Visualization\nTo better understand the structure of the shared latent space constructed across multiple embodiments, we visualize the projected latent trajectories using principal component analysis (PCA), as shown in Fig.\n12\n. An example walking motion (top) is first retargeted to several robots, such as the JVRC. The resulting robot motions are then encoded into the shared latent space, and their projected trajectories are plotted in Fig.\n12\n(bottom). The visualization demonstrates the alignment and consistency of the motion representations across different embodiments within the shared latent space.\nReferences\n[1]\nS. Choi, M. Pan, and J. Kim\n(2020-07)\nNonparametric motion retargeting for humanoid robots on shared latent space\n.\npp.\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[2]\nS. Choi, M. J. Song, H. Ahn, and J. Kim\nSelf-supervised motion retargeting with safety guarantee\n.\nIn\n2021 IEEE International Conference on Robotics and Automation (ICRA)\n,\nCited by:\nÂ§I\n.\n[3]\nD. Clevert, T. Unterthiner, and S. Hochreiter\n(2015-11)\nFast and accurate deep network learning by exponential linear units (elus)\n.\nUnder Review of ICLR2016 (1997)\n,\npp.\n.\nCited by:\nÂ§\nIII-A\n1\n.\n[4]\nB. Delhaisse, D. Esteban, L. Rozo, and D. Caldwell\nTransfer learning of shared latent spaces between robots with similar kinematic structure\n.\nIn\n2017 International Joint Conference on Neural Networks (IJCNN)\n,\nCited by:\nÂ§I\n.\n[5]\nM. Diomataris, N. Athanasiou, O. Taheri, X. Wang, O. Hilliges, and M. J. Black\n(2024)\nWANDR: intention-guided human motion generation\n.\nIn\nProceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\nÂ§\nII-E\n.\n[6]\nT. Franzmeyer, P. Torr, and J. F. Henriques\n(2022)\nLearn what matters: cross-domain imitation learning with task-relevant embeddings\n.\nIn\nAdvances in Neural Information Processing Systems\n,\nCited by:\nÂ§I\n.\n[7]\nC. Guo, S. Zou, X. Zuo, S. Wang, W. Ji, X. Li, and L. Cheng\n(2022-06)\nGenerating diverse and natural 3d human motions from text\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 5152â€“5161\n.\nCited by:\nÂ§\nIII-B\n.\n[8]\nE. Hoffer and N. Ailon\n(2014)\nDeep metric learning using triplet network\n.\nIn\nInternational Workshop on Similarity-Based Pattern Recognition\n,\nCited by:\nÂ§\nII-C\n.\n[9]\nH. Jang, B. Kwon, M. Yu, S. U. Kim, and J. Kim\n(2018)\nA variational u-net for motion retargeting\n.\nIn\nSIGGRAPH Asia 2018 Posters\n,\nCited by:\nÂ§I\n.\n[10]\nK. Kim, Y. Gu, J. Song, S. Zhao, and S. Ermon\n(2020)\nDomain adaptive imitation learning\n.\nIn\nProceedings of the 37th International Conference on Machine Learning\n,\nCited by:\nÂ§I\n.\n[11]\nD. P. Kingma and J. Ba\n(2015)\nAdam: A method for stochastic optimization\n.\nIn\n3rd International Conference on Learning Representations, ICLR 2015\n,\nY. Bengio and Y. LeCun (Eds.)\n,\nCited by:\nÂ§\nIII-A\n3\n.\n[12]\nZ. Lin, Y. Chen, and Z. Liu\nHierarchical human-to-robot imitation learning for long-horizon tasks via cross-domain skill alignment\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\nCited by:\nÂ§I\n.\n[13]\nH. Y. Ling, F. Zinno, G. Cheng, and M. van de Panne\n(2020)\nCharacter controllers using motion vaes\n.\nACM Trans. Graph.\n39\n(\n4\n).\nCited by:\nÂ§\nII-E\n.\n[14]\nK. Pertsch, R. Desai, V. Kumar, F. Meier, J. J. Lim, D. Batra, and A. Rai\n(2022)\nCross-domain transfer via semantic skill imitation\n.\n6th Conference on Robot Learning\n.\nCited by:\nÂ§I\n.\n[15]\nD. Rempe, T. Birdal, A. Hertzmann, J. Yang, S. Sridhar, and L. J. Guibas\n(2021)\nHuMoR: 3d human motion model for robust pose estimation\n.\nIn\nInternational Conference on Computer Vision (ICCV)\n,\nCited by:\nÂ§\nII-E\n.\n[16]\nM. Xu, Z. Xu, C. Chi, M. Veloso, and S. Song\n(2023)\nXSkill: cross embodiment skill discovery\n.\nIn\nProceedings of The 7th Conference on Robot Learning\n,\nProceedings of Machine Learning Research\n.\nCited by:\nÂ§I\n.\n[17]\nY. Yan, E. Valls Mascaro, and D. Lee\nImitationNet: unsupervised human-to-robot motion retargetting via shared latent space\n.\nIn\n2023 IEEE-RAS 22st International Conference on Humanoid Robots (Humanoids)\n,\nCited by:\nÂ§I\n,\nÂ§\nII-A\n,\nÂ§\nII-B\n,\nÂ§\nII-C\n,\nÂ§\nIII-C\n1\n,\nÂ§\nIII-D\n2\n.\n[18]\nZ. Yin, L. Sun, H. Ma, M. Tomizuka, and W. Li\nCross domain robot imitation with invariant representation\n.\nIn\n2022 International Conference on Robotics and Automation (ICRA)\n,\nCited by:\nÂ§I\n.\n[19]\nK. Zakka, A. Zeng, P. Florence, J. Tompson, J. Bohg, and D. Dwibedi\n(2022)\nXIRL: cross-embodiment inverse reinforcement learning\n.\nIn\nProceedings of the 5th Conference on Robot Learning\n,\nCited by:\nÂ§I\n.\n[20]\nPyTorch Kinematics\nExternal Links:\nDocument\nCited by:\nÂ§\nIII-B\n.",
    "preview_text": "We present a scalable framework for cross-embodiment humanoid robot control by learning a shared latent representation that unifies motion across humans and diverse humanoid platforms, including single-arm, dual-arm, and legged humanoid robots. Our method proceeds in two stages: first, we construct a decoupled latent space that captures localized motion patterns across different body parts using contrastive learning, enabling accurate and flexible motion retargeting even across robots with diverse morphologies. To enhance alignment between embodiments, we introduce tailored similarity metrics that combine joint rotation and end-effector positioning for critical segments, such as arms. Then, we train a goal-conditioned control policy directly within this latent space using only human data. Leveraging a conditional variational autoencoder, our policy learns to predict latent space displacements guided by intended goal directions. We show that the trained policy can be directly deployed on multiple robots without any adaptation. Furthermore, our method supports the efficient addition of new robots to the latent space by learning only a lightweight, robot-specific embedding layer. The learned latent policies can also be directly applied to the new robots. Experimental results demonstrate that our approach enables robust, scalable, and embodiment-agnostic robot control across a wide range of humanoid platforms.\n\n\\useunder\n\\ul\nLearning a Unified Latent Space for Cross-Embodiment Robot Control\nYashuai Yan\n1\n, Dongheui Lee\n1,2\n1\nYashuai Yan and Dongheui Lee are with Autonomous Systems Lab, Technische UniversitÃ¤t Wien (TU Wien), Vienna, Austria (e-mail:\n{yashuai.yan, dongheui.lee}@tuwien.ac.at\n).\n2\nDongheui Lee is also with the Institute of Robotics and Mechatronics (DLR), German Aerospace Center, Wessling, Germany.\nAbstract\nWe present a scalable framework for cross-embodiment humanoid robot control by learning a shared latent representation that unifies motion across humans",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "VLA",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å…±äº«æ½œåœ¨ç©ºé—´å®ç°è·¨å…·èº«äººå½¢æœºå™¨äººæ§åˆ¶çš„å¯æ‰©å±•æ¡†æ¶ï¼Œæ¶‰åŠå¼ºåŒ–å­¦ä¹ ã€å…¨èº«æ§åˆ¶å’Œè¿åŠ¨å­¦ä¹ ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šã€Šå­¦ä¹ ç”¨äºè·¨å…·èº«æœºå™¨äººæ§åˆ¶çš„ç»Ÿä¸€æ½œåœ¨ç©ºé—´ã€‹\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\næ§åˆ¶å½¢æ€å„å¼‚çš„äººå½¢æœºå™¨äººå¹³å°æ˜¯æœºå™¨äººå­¦çš„ä¸€é¡¹é•¿æœŸæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦ä¸ºæ¯ä¸ªæœºå™¨äººæ”¶é›†å¤§é‡é…å¯¹æ•°æ®æˆ–é‡æ–°è®­ç»ƒç­–ç•¥ï¼Œéš¾ä»¥é€‚åº”æœºå™¨äººå½¢æ€çš„å¿«é€Ÿå¤šæ ·åŒ–ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§èƒ½å¤Ÿ**è·¨å…·èº«ï¼ˆCross-Embodimentï¼‰** æ³›åŒ–çš„ç»Ÿä¸€æ§åˆ¶æ¡†æ¶ï¼Œä½¿å•ä¸€ç­–ç•¥èƒ½ç›´æ¥éƒ¨ç½²äºå¤šç§æœºå™¨äººï¼Œå¹¶é«˜æ•ˆæ‰©å±•åˆ°æ–°å¹³å°ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡ºä¸€ä¸ª**ä¸¤é˜¶æ®µ**çš„æ¡†æ¶ï¼š\n- **ç¬¬ä¸€é˜¶æ®µï¼šå­¦ä¹ ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´**ã€‚é€šè¿‡**å¯¹æ¯”å­¦ä¹ **ï¼Œæ„å»ºä¸€ä¸ªå…±äº«çš„æ½œåœ¨è¡¨ç¤ºï¼Œå°†äººç±»å’Œå¤šç§æœºå™¨äººï¼ˆå¦‚å•è‡‚ã€åŒè‡‚ã€è…¿å¼æœºå™¨äººï¼‰çš„è¿åŠ¨å¯¹é½ã€‚å…³é”®åˆ›æ–°åœ¨äº**è§£è€¦çš„æ½œåœ¨ç©ºé—´æ¶æ„**ï¼Œå°†èº«ä½“åˆ†ä¸ºå·¦/å³è‡‚ã€èº¯å¹²ã€å·¦/å³è…¿äº”ä¸ªç‹¬ç«‹å­ç©ºé—´ï¼Œå¹¶é’ˆå¯¹ä¸åŒéƒ¨ä½ï¼ˆå¦‚æ‰‹è‡‚ï¼‰è®¾è®¡**æ··åˆç›¸ä¼¼æ€§åº¦é‡**ï¼ˆç»“åˆå…³èŠ‚æ—‹è½¬å’Œæœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ï¼‰ï¼Œä»è€Œç²¾ç»†åœ°å¯¹é½ä¸åŒå½¢æ€çš„è¿åŠ¨ã€‚\n- **ç¬¬äºŒé˜¶æ®µï¼šåœ¨æ½œåœ¨ç©ºé—´ä¸­è®­ç»ƒæ§åˆ¶ç­–ç•¥**ã€‚ä½¿ç”¨**æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆc-VAEï¼‰**ï¼Œä»…åˆ©ç”¨äººç±»è¿åŠ¨æ•°æ®ï¼Œè®­ç»ƒä¸€ä¸ªä»¥ç›®æ ‡æœ«ç«¯ä½ç½®ä¸ºæ¡ä»¶çš„è¿åŠ¨ç”Ÿæˆç­–ç•¥ã€‚è¯¥ç­–ç•¥åœ¨æ½œåœ¨ç©ºé—´ä¸­é¢„æµ‹ä½ç§»ï¼Œå®ç°äº†**å…·èº«æ— å…³çš„æ§åˆ¶**ï¼Œå¯ç›´æ¥éƒ¨ç½²åˆ°ä»»ä½•å·²ç¼–ç çš„æœºå™¨äººä¸Šã€‚æ­¤å¤–ï¼Œæ¡†æ¶æ”¯æŒé€šè¿‡ä»…è®­ç»ƒè½»é‡çº§çš„**æœºå™¨äººç‰¹å®šåµŒå…¥å±‚**æ¥é«˜æ•ˆæ·»åŠ æ–°æœºå™¨äººã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\n- **è¿åŠ¨é‡å®šå‘**ï¼šåœ¨TIAGo++ã€H1ã€NAOã€JVRCç­‰å¤šç§æœºå™¨äººä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ«ç«¯æ‰§è¡Œå™¨è½¨è¿¹å’Œé€Ÿåº¦ä¿çœŸåº¦ä¸Šä¼˜äºåŸºçº¿ï¼ˆå¦‚ImitationNetï¼‰ï¼ŒåŒæ—¶é€šè¿‡è§£è€¦è®¾è®¡ä¿æŒäº†é«˜æ—‹è½¬ç›¸ä¼¼æ€§ã€‚\n- **ç­–ç•¥æ³›åŒ–**ï¼šåœ¨æ½œåœ¨ç©ºé—´ä¸­è®­ç»ƒçš„å•è‡‚æ§åˆ¶ç­–ç•¥ï¼Œæ— éœ€å¾®è°ƒå³å¯åœ¨å¤šä¸ªæœºå™¨äººä¸Šå®ç°**äºšå˜ç±³çº§**çš„ç›®æ ‡åˆ°è¾¾ç²¾åº¦ã€‚\n- **å¯æ‰©å±•æ€§**ï¼šä¸ºATLASã€G1ç­‰æ–°æœºå™¨äººä»…è®­ç»ƒåµŒå…¥å±‚ï¼ˆçº¦15åˆ†é’Ÿï¼‰ï¼Œå…¶é‡å®šå‘æ€§èƒ½ä¸ç«¯åˆ°ç«¯è®­ç»ƒçš„æœºå™¨äººç›¸å½“ï¼Œè¯æ˜äº†æ¡†æ¶çš„é«˜æ•ˆå¯æ‰©å±•æ€§ã€‚\n- **ç¡¬ä»¶éªŒè¯**ï¼šåœ¨TIAGo-SEAå’ŒKinova Gen3æœºå™¨äººä¸ŠæˆåŠŸæ¼”ç¤ºäº†å®æ—¶é¥æ“ä½œã€ç‰©ä½“æŠ“å–æ”¾ç½®ç­‰ä»»åŠ¡ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\næœ¬ç ”ç©¶ä¸º**å¯æ‰©å±•çš„å¤šæœºå™¨äººç³»ç»Ÿ**æä¾›äº†ä¸€ç§é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚å…¶æ ¸å¿ƒä»·å€¼åœ¨äºï¼š\n- **æŠ€æœ¯çªç ´**ï¼šé€šè¿‡è§£è€¦çš„æ½œåœ¨ç©ºé—´å’Œå®šåˆ¶åŒ–ç›¸ä¼¼æ€§åº¦é‡ï¼Œé¦–æ¬¡å®ç°äº†åœ¨å•ä¸€æ¨¡å‹ä¸­ç»Ÿä¸€å¤šç§å¼‚æ„æœºå™¨äººå½¢æ€çš„è¿åŠ¨è¡¨ç¤ºä¸æ§åˆ¶ã€‚\n- **å®ç”¨ä»·å€¼**ï¼šç­–ç•¥çš„â€œä¸€æ¬¡è®­ç»ƒï¼Œå¤šå¤„éƒ¨ç½²â€ç‰¹æ€§åŠè½»é‡çº§æ‰©å±•èƒ½åŠ›ï¼Œæ˜¾è‘—é™ä½äº†ä¸ºä¸åŒæœºå™¨äººå¹³å°å¼€å‘å’Œæ§åˆ¶ç­–ç•¥çš„æˆæœ¬ä¸å¤æ‚åº¦ã€‚\n- **åº”ç”¨å‰æ™¯**ï¼šä¸ºæœºå™¨äººé¥æ“ä½œã€æŠ€èƒ½è¿ç§»å’Œå¤šæœºå™¨äººååŒä»»åŠ¡å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œæ¨åŠ¨äº†å‘æ›´çµæ´»ã€é€šç”¨çš„æœºå™¨äººæ§åˆ¶ç³»ç»Ÿçš„æ¼”è¿›ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**è·¨å…·èº«æœºå™¨äººæ§åˆ¶**çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼šå¦‚ä½•ä¸ºå½¢æ€ã€è‡ªç”±åº¦ã€è¿åŠ¨å­¦çº¦æŸå·®å¼‚å·¨å¤§çš„ä¸åŒäººå½¢æœºå™¨äººå¹³å°ï¼ˆå¦‚å•è‡‚ã€åŒè‡‚ã€åŒè¶³æœºå™¨äººï¼‰ï¼Œå»ºç«‹ä¸€ä¸ªç»Ÿä¸€çš„æ§åˆ¶æ¡†æ¶ï¼Œä½¿å¾—ä¸€ä¸ªç­–ç•¥èƒ½å¤Ÿä¸ç»ä¿®æ”¹ç›´æ¥éƒ¨ç½²åˆ°æ‰€æœ‰æœºå™¨äººä¸Šã€‚\n\n### **æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ª**ä¸¤é˜¶æ®µã€åŸºäºè§£è€¦å…±äº«æ½œç©ºé—´çš„æ¡†æ¶**ï¼Œå…¶åˆ›æ–°æ€§ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹å››ä¸ªæ–¹é¢ï¼š\n\n1.  **è§£è€¦çš„ã€æ¨¡å—åŒ–çš„æ½œç©ºé—´æ¶æ„**\n    - **åˆ›æ–°**ï¼š å°†æ•´ä½“æ½œç©ºé—´æŒ‰èº«ä½“éƒ¨ä½ï¼ˆå·¦è‡‚ã€å³è‡‚ã€èº¯å¹²ã€å·¦è…¿ã€å³è…¿ï¼‰è§£è€¦ä¸ºäº”ä¸ªç‹¬ç«‹çš„å­ç©ºé—´ã€‚\n    - **ä»·å€¼**ï¼š è§£å†³äº†ä¸åŒæœºå™¨äººå½¢æ€ï¼ˆå¦‚æœ‰çš„æœ‰è…¿æ— èº¯å¹²ï¼Œæœ‰çš„åªæœ‰æ‰‹è‡‚ï¼‰å¯¼è‡´çš„æ˜ å°„æ¨¡ç³Šé—®é¢˜ã€‚è¿™ç§æ¨¡å—åŒ–è®¾è®¡å…è®¸å¯¹å±€éƒ¨è¿åŠ¨æ¨¡å¼è¿›è¡Œç»†ç²’åº¦å¯¹é½ï¼Œå¹¶èƒ½çµæ´»å¤„ç†éå¯¹ç§°æˆ–éƒ¨åˆ†è‚¢ä½“ç»“æ„çš„æœºå™¨äººã€‚\n\n2.  **å®šåˆ¶åŒ–çš„å¯¹æ¯”å­¦ä¹ ç›¸ä¼¼æ€§åº¦é‡**\n    - **åˆ›æ–°**ï¼š é’ˆå¯¹ä¸åŒèº«ä½“éƒ¨ä½è®¾è®¡ä¸åŒçš„ç›¸ä¼¼æ€§è®¡ç®—æ–¹å¼ã€‚å¯¹äºæ‰‹è‡‚ï¼Œç»“åˆäº†**å…³èŠ‚æ—‹è½¬ç›¸ä¼¼æ€§**å’Œ**æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ç›¸ä¼¼æ€§**ï¼ˆå…¬å¼3ï¼‰ï¼›å¯¹äºèº¯å¹²å’Œè…¿éƒ¨ï¼Œåˆ™ä»…ä½¿ç”¨å…³èŠ‚æ—‹è½¬ç›¸ä¼¼æ€§ï¼ˆå…¬å¼4ï¼‰ã€‚\n    - **ä»·å€¼**ï¼š è¿™ç§æ··åˆåº¦é‡æ›´ç¬¦åˆæœºå™¨äººä»»åŠ¡éœ€æ±‚ï¼ˆå¦‚æ“ä½œä»»åŠ¡é‡è§†æœ«ç«¯ç²¾åº¦ï¼Œè…¿éƒ¨è¿åŠ¨é‡è§†å§¿æ€æ¨¡ä»¿ï¼‰ï¼Œæ˜¾è‘—æå‡äº†è·¨å…·èº«è¿åŠ¨é‡å®šå‘çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚\n\n3.  **åœ¨å…±äº«æ½œç©ºé—´ä¸­è®­ç»ƒç›®æ ‡æ¡ä»¶æ§åˆ¶ç­–ç•¥**\n    - **åˆ›æ–°**ï¼š ä½¿ç”¨**æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨**ï¼Œ**ä»…åˆ©ç”¨äººç±»è¿åŠ¨æ•°æ®**ï¼Œåœ¨å…±äº«æ½œç©ºé—´ä¸­è®­ç»ƒä¸€ä¸ªé¢„æµ‹æ½œçŠ¶æ€ä½ç§»çš„ç›®æ ‡å¯¼å‘ç­–ç•¥ã€‚\n    - **ä»·å€¼**ï¼š ç­–ç•¥åœ¨æŠ½è±¡çš„ã€ä¸å…·èº«æ— å…³çš„æ½œç©ºé—´ä¸­è¿è¡Œï¼Œå› æ­¤è®­ç»ƒå®Œæˆåå¯ç›´æ¥éƒ¨ç½²åˆ°ä»»ä½•å·²ç¼–ç åˆ°è¯¥ç©ºé—´ä¸­çš„æœºå™¨äººä¸Šï¼Œæ— éœ€é’ˆå¯¹ä»»ä½•æœºå™¨äººè¿›è¡Œå¾®è°ƒï¼Œå®ç°äº†çœŸæ­£çš„â€œä¸€æ¬¡è®­ç»ƒï¼Œå¤šå¤„éƒ¨ç½²â€ã€‚\n\n4.  **é«˜æ•ˆã€å¯æ‰©å±•çš„æ–°æœºå™¨äººé›†æˆæ–¹æ³•**\n    - **åˆ›æ–°**ï¼š ç³»ç»Ÿæ ¸å¿ƒç»„ä»¶ï¼ˆç¼–ç å™¨ã€è§£ç å™¨ï¼‰åœ¨å¤šä¸ªæœºå™¨äººä¸Šé¢„è®­ç»ƒåå›ºå®šã€‚æ·»åŠ æ–°æœºå™¨äººæ—¶ï¼Œ**ä»…éœ€è®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„ã€æœºå™¨äººç‰¹å®šçš„åµŒå…¥å±‚**ï¼Œå°†æ–°æœºå™¨äººçš„å§¿æ€æ˜ å°„åˆ°å…±äº«ç‰¹å¾ç©ºé—´ã€‚\n    - **ä»·å€¼**ï¼š æå¤§é™ä½äº†æ‰©å±•åˆ°æ–°æœºå™¨äººå¹³å°çš„æˆæœ¬å’Œè€—æ—¶ï¼ˆè®ºæ–‡ä¸­ä»…éœ€çº¦15åˆ†é’Ÿï¼‰ï¼Œä½¿æ¡†æ¶å…·å¤‡é«˜åº¦çš„å¯æ‰©å±•æ€§ï¼Œé€‚åº”ä¸æ–­å¢é•¿çš„æœºå™¨äººå½¢æ€ã€‚\n\n### **è§£å†³æ–¹æ¡ˆï¼ˆæ–¹æ³•è®ºï¼‰**\nè®ºæ–‡é€šè¿‡ä¸€ä¸ªæ¸…æ™°çš„ä¸¤é˜¶æ®µæµç¨‹è§£å†³ä¸Šè¿°é—®é¢˜ï¼š\n\n**ç¬¬ä¸€é˜¶æ®µï¼šå­¦ä¹ ç»Ÿä¸€çš„æ½œç©ºé—´**\n- **è¾“å…¥**ï¼š äººç±»è¿åŠ¨æ•°æ® + é€šè¿‡æ­£å‘åŠ¨åŠ›å­¦ä»æœºå™¨äººURDFéšæœºé‡‡æ ·ç”Ÿæˆçš„å¤§é‡æœºå™¨äººå§¿æ€ï¼ˆæ— éœ€çœŸå®æœºå™¨äººæ¼”ç¤ºæ•°æ®ï¼‰ã€‚\n- **è¿‡ç¨‹**ï¼š\n    1.  é€šè¿‡æœºå™¨äººç‰¹å®šåµŒå…¥å±‚å°†ä¸åŒç»´åº¦çš„åŸå§‹å§¿æ€æ˜ å°„åˆ°ç»Ÿä¸€ç»´åº¦ã€‚\n    2.  ä½¿ç”¨**å¯¹æ¯”å­¦ä¹ **ï¼ŒåŸºäºå®šåˆ¶çš„ç›¸ä¼¼æ€§åº¦é‡ï¼Œè®­ç»ƒç¼–ç å™¨ç½‘ç»œï¼Œä½¿å¾—è¯­ä¹‰ç›¸ä¼¼çš„è¿åŠ¨ï¼ˆæ— è®ºæ¥è‡ªäººè¿˜æ˜¯å“ªç§æœºå™¨äººï¼‰åœ¨å¯¹åº”çš„è§£è€¦æ½œå­ç©ºé—´ä¸­ä½ç½®æ¥è¿‘ã€‚\n    3.  é€šè¿‡ç»„åˆ**å¯¹æ¯”æŸå¤±ã€é‡å»ºæŸå¤±ã€æ½œç©ºé—´ä¸€è‡´æ€§æŸå¤±ã€æ—¶åºæŸå¤±**æ¥ä¼˜åŒ–æ•´ä¸ªç¼–ç -è§£ç æ¶æ„ã€‚\n- **è¾“å‡º**ï¼š ä¸€ä¸ªèƒ½å¤Ÿå¯¹é½äººç±»ä¸å¤šç§æœºå™¨äººè¿åŠ¨çš„ã€è§£è€¦çš„å…±äº«æ½œç©ºé—´ã€‚\n\n**ç¬¬äºŒé˜¶æ®µï¼šåœ¨æ½œç©ºé—´ä¸­è®­ç»ƒæ§åˆ¶ç­–ç•¥**\n- **è¾“å…¥**ï¼š **ä»…äººç±»è¿åŠ¨æ•°æ®**ã€‚ä»ä¸­æå–å½“å‰å§¿æ€ã€ç›®æ ‡å§¿æ€ï¼Œå¹¶è®¡ç®—æŒ‡å‘ç›®æ ‡çš„å¹³å‡æœ«ç«¯é€Ÿåº¦ä½œä¸ºâ€œæ„å›¾â€ä¿¡å·ã€‚\n- **è¿‡ç¨‹**ï¼š\n    1.  ä½¿ç”¨c-VAEæ¡†æ¶ï¼Œä»¥å½“å‰æ½œçŠ¶æ€ `z_t` å’Œæ„å›¾ä¿¡å· `v_ee` ä¸ºæ¡ä»¶ï¼Œé¢„æµ‹ä¸‹ä¸€æ—¶åˆ»çš„æ½œçŠ¶æ€ä½ç§» `d_t`ã€‚\n    2.  è®­ç»ƒç›®æ ‡æ˜¯å‡†ç¡®é‡å»ºäººç±»è¿åŠ¨ä¸­çš„æ½œä½ç§»ã€‚\n- **éƒ¨ç½²**ï¼š\n    1.  ç»™å®šæœºå™¨äººçš„å½“å‰å§¿æ€å’Œç›®æ ‡æœ«ç«¯ä½ç½®ï¼Œè®¡ç®—æ„å›¾ä¿¡å·ã€‚\n    2.  å°†å½“å‰æœºå™¨äººå§¿æ€ç¼–ç åˆ°æ½œç©ºé—´ã€‚\n    3.  c-VAEè§£ç å™¨æ ¹æ®å½“å‰æ½œçŠ¶æ€å’Œæ„å›¾ä¿¡å·ï¼Œé¢„æµ‹æ½œä½ç§»ï¼Œä»è€Œæ›´æ–°æ½œçŠ¶æ€ã€‚\n    4.  å°†æ–°çš„æ½œçŠ¶æ€è§£ç å›æœºå™¨äººå…³èŠ‚ç©ºé—´ï¼Œç”Ÿæˆæ§åˆ¶å‘½ä»¤ã€‚\n    5.  æ­¤è¿‡ç¨‹ä»¥è‡ªå›å½’æ–¹å¼è¿è¡Œï¼Œç”Ÿæˆå¹³æ»‘ã€ç›®æ ‡å¯¼å‘çš„è¿åŠ¨è½¨è¿¹ã€‚\n\n### **å®é™…ä»·å€¼ä¸æ„ä¹‰**\n- **è§„æ¨¡åŒ–æœºå™¨äººæ§åˆ¶**ï¼š ä¸ºç®¡ç†å½¢æ€å„å¼‚çš„æœºå™¨äººèˆ°é˜Ÿæä¾›äº†ç»Ÿä¸€ã€é«˜æ•ˆçš„æ§åˆ¶æ¥å£ï¼Œé™ä½äº†éƒ¨ç½²å’Œç»´æŠ¤æˆæœ¬ã€‚\n- **æ•°æ®æ•ˆç‡ä¸å¯è¿ç§»æ€§**ï¼š ç­–ç•¥è®­ç»ƒä»…éœ€äººç±»æ•°æ®ï¼Œé¿å…äº†ä¸ºæ¯ä¸ªæœºå™¨äººæ”¶é›†å¤§é‡æ¼”ç¤ºæ•°æ®çš„éš¾é¢˜ï¼›ç­–ç•¥å…·å¤‡å¼€ç®±å³ç”¨çš„è·¨å¹³å°è¿ç§»èƒ½åŠ›ã€‚\n- **çµæ´»æ€§ä¸æ–°åº”ç”¨**ï¼š è§£è€¦çš„æ½œç©ºé—´æ”¯æŒ**è¿åŠ¨ç¼–è¾‘**ï¼ˆå¦‚ç»„åˆä¸åŒæœºå™¨äººçš„èº«ä½“éƒ¨ä½è¿åŠ¨ï¼‰å’Œ**æ¨¡å—åŒ–æ§åˆ¶**ï¼ˆç‹¬ç«‹æ§åˆ¶æ‰‹è‡‚æˆ–è…¿éƒ¨ï¼‰ï¼Œæ‹“å±•äº†æœºå™¨äººè¿åŠ¨ç”Ÿæˆçš„å¯èƒ½æ€§ã€‚\n- **ç¡¬ä»¶éªŒè¯**ï¼š è®ºæ–‡åœ¨TIAGoå’ŒKinovaç­‰çœŸå®æœºå™¨äººä¸ŠæˆåŠŸæ¼”ç¤ºäº†å®æ—¶é¥æ“ä½œã€ç›®æ ‡åˆ°è¾¾å’ŒæŠ“å–æ”¾ç½®ä»»åŠ¡ï¼Œè¯æ˜äº†æ–¹æ³•çš„å®ç”¨æ€§å’Œç²¾åº¦ã€‚\n\n**æ€»ç»“**ï¼š è¯¥è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®æ˜¯æå‡ºäº†ä¸€ä¸ª**å¯æ‰©å±•ã€ä¸å…·èº«æ— å…³çš„æœºå™¨äººå­¦ä¹ ä¸æ§åˆ¶èŒƒå¼**ã€‚å®ƒé€šè¿‡æ„å»ºä¸€ä¸ªç»“æ„åŒ–çš„å…±äº«è¯­ä¹‰ç©ºé—´ï¼Œå°†å¤æ‚çš„è·¨å¹³å°æ§åˆ¶é—®é¢˜è½¬åŒ–ä¸ºåœ¨è¯¥ç©ºé—´ä¸­çš„è½¨è¿¹è§„åˆ’é—®é¢˜ï¼Œä¸ºå®ç°é€šç”¨æœºå™¨äººæ§åˆ¶è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
            "timestamp": "2026-01-27T16:01:49.985592",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡å‘å±•è„‰ç»œæ¢³ç†\n\næœ¬æ–‡ã€ŠLearning a Unified Latent Space for Cross-Embodiment Robot Controlã€‹æ˜¯ä½œè€…å›¢é˜Ÿåœ¨è·¨å…·èº«æœºå™¨äººæ§åˆ¶ä¸è¿åŠ¨é‡å®šå‘æ–¹å‘ä¸Šç³»åˆ—å·¥ä½œçš„æœ€æ–°è¿›å±•ã€‚å…¶æ ¸å¿ƒå‘å±•è„‰ç»œå¯ä»¥æ¢³ç†ä¸ºä»¥ä¸‹å‡ ä¸ªå…³é”®é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½é’ˆå¯¹ç‰¹å®šç“¶é¢ˆè¿›è¡Œäº†æ”¹è¿›ï¼Œå¹¶é€æ¸åˆ†åŒ–å‡ºä¸åŒçš„ç ”ç©¶æ–¹å‘ã€‚\n\n### å‘å±•è„‰ç»œä¸æ ¸å¿ƒæ”¹è¿›\n\n#### ç¬¬ä¸€é˜¶æ®µï¼šä»ç‰¹å®šé…å¯¹æ•°æ®åˆ°æ— ç›‘ç£å­¦ä¹ ï¼ˆè§£å†³æ•°æ®ç“¶é¢ˆï¼‰\n- **æ—©æœŸå·¥ä½œï¼ˆå¦‚ [4, 9, 1]ï¼‰**ï¼šä¸“æ³¨äºä»äººç±»åˆ°**å•ä¸€ç‰¹å®šæœºå™¨äºº**æˆ–åŠ¨ç”»è§’è‰²çš„è¿åŠ¨é‡å®šå‘ã€‚è¿™äº›æ–¹æ³•ä¸¥é‡ä¾èµ–**æ‰‹åŠ¨æ”¶é›†çš„é…å¯¹æ•°æ®é›†**ï¼ˆå³åŒä¸€åŠ¨ä½œçš„äººç±»æ¼”ç¤ºå’Œå¯¹åº”çš„æœºå™¨äººå…³èŠ‚è½¨è¿¹ï¼‰ã€‚è¿™æå¤§åœ°é™åˆ¶äº†æ–¹æ³•çš„å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§ã€‚\n- **å…³é”®æ”¹è¿›**ï¼šåç»­å·¥ä½œï¼ˆå¦‚Choiç­‰äºº[2]ï¼‰å¼•å…¥äº†**è‡ªç›‘ç£çš„é…å¯¹æ•°æ®ç”ŸæˆæŠ€æœ¯**ï¼Œè‡ªåŠ¨åŒ–äº†æ„å»ºè·¨åŸŸå¯¹åº”å…³ç³»çš„è¿‡ç¨‹ï¼Œå‡å°‘äº†å¯¹æ˜‚è´µé…å¯¹æ•°æ®çš„ä¾èµ–ã€‚è¿™æ ‡å¿—ç€ä»â€œæ•°æ®é©±åŠ¨â€å‘â€œå­¦ä¹ é©±åŠ¨â€çš„è½¬å˜ã€‚\n\n#### ç¬¬äºŒé˜¶æ®µï¼šä»å•ä¸€åŸŸåˆ°å…±äº«æ½œåœ¨ç©ºé—´ï¼ˆè§£å†³è¡¨ç¤ºå¯¹é½é—®é¢˜ï¼‰\n- **å‰ä½œ ImitationNet [17]ï¼ˆæœ¬æ–‡ç›´æ¥åŸºç¡€ï¼‰**ï¼šæœ¬æ–‡ä½œè€…å›¢é˜Ÿä¹‹å‰çš„å·¥ä½œã€‚å®ƒé‡‡ç”¨**å¯¹æ¯”å­¦ä¹ **æ–¹æ³•ï¼Œä¸º**äººç±»å’Œä¸€ä¸ªç‰¹å®šçš„æœºå™¨äºº**ï¼ˆå¦‚TIAGo++ï¼‰å­¦ä¹ ä¸€ä¸ª**å…±äº«çš„æ½œåœ¨ç©ºé—´**ã€‚å…¶è´¡çŒ®åœ¨äºå®ç°äº†é«˜è´¨é‡ã€å¹³æ»‘çš„**æ— ç›‘ç£**äººæœºè¿åŠ¨é‡å®šå‘ã€‚\n- **å…³é”®æ”¹è¿›ä¸æœ¬æ–‡çš„å…³è”**ï¼š\n    - **å±€é™æ€§**ï¼šImitationNet å­¦ä¹ çš„å…±äº«ç©ºé—´æ˜¯**â€œä¸€å¯¹ä¸€â€**çš„ï¼ˆäººç±»â†”ç‰¹å®šæœºå™¨äººï¼‰ã€‚è¦æ§åˆ¶Nä¸ªä¸åŒçš„æœºå™¨äººï¼Œå°±éœ€è¦è®­ç»ƒNä¸ªç‹¬ç«‹çš„æ¨¡å‹ï¼Œä¸å…·å¤‡å¯æ‰©å±•æ€§ã€‚\n    - **æœ¬æ–‡çš„é£è·ƒ**ï¼šæœ¬æ–‡å°†å…±äº«æ½œåœ¨ç©ºé—´çš„æ¦‚å¿µä»â€œä¸€å¯¹ä¸€â€**æ‰©å±•ä¸ºâ€œä¸€å¯¹å¤šâ€ä¹ƒè‡³â€œå¤šå¯¹å¤šâ€**ã€‚ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªèƒ½**ç»Ÿä¸€å®¹çº³å¤šç§æœºå™¨äººå…·èº«å½¢æ€**çš„å•ä¸€æ½œåœ¨ç©ºé—´ï¼Œä»è€Œå®ç°çœŸæ­£çš„è·¨å…·èº«æ§åˆ¶ã€‚\n\n#### ç¬¬ä¸‰é˜¶æ®µï¼šä»æ•´ä½“æ½œåœ¨ç©ºé—´åˆ°è§£è€¦æ½œåœ¨ç©ºé—´ï¼ˆè§£å†³å½¢æ€å¤šæ ·æ€§é—®é¢˜ï¼‰\n- **å‰æœŸå…±äº«ç©ºé—´æ–¹æ³•ï¼ˆåŒ…æ‹¬ImitationNetï¼‰çš„å±€é™**ï¼šå®ƒä»¬ä½¿ç”¨**å•ä¸€çš„ã€æ•´ä½“çš„æ½œåœ¨å‘é‡**æ¥è¡¨ç¤ºå…¨èº«è¿åŠ¨ã€‚å½“æºåŸŸå’Œç›®æ ‡åŸŸçš„å½¢æ€ç»“æ„å·®å¼‚å·¨å¤§æ—¶ï¼ˆä¾‹å¦‚ï¼Œæœ‰è…¿çš„æœºå™¨äºº vs. åªæœ‰æ‰‹è‡‚çš„æœºå™¨äººï¼‰ï¼Œè¿™ç§æ•´ä½“è¡¨ç¤ºä¼šå¯¼è‡´æ¨¡ç³Šç”šè‡³é”™è¯¯çš„æ˜ å°„ã€‚\n- **æœ¬æ–‡çš„æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°**ï¼š\n    - **è§£è€¦çš„æ½œåœ¨ç©ºé—´æ¶æ„**ï¼šå°†æ½œåœ¨ç©ºé—´æŒ‰èº«ä½“éƒ¨ä½ï¼ˆå·¦è‡‚ã€å³è‡‚ã€èº¯å¹²ã€å·¦è…¿ã€å³è…¿ï¼‰**è§£è€¦ä¸ºäº”ä¸ªç‹¬ç«‹çš„å­ç©ºé—´**ã€‚æ¯ä¸ªå­ç©ºé—´ä¸“æ³¨äºå­¦ä¹ å±€éƒ¨è¿åŠ¨æ¨¡å¼ã€‚\n    - **å®šåˆ¶åŒ–çš„ç›¸ä¼¼æ€§åº¦é‡**ï¼šé’ˆå¯¹ä¸åŒèº«ä½“éƒ¨ä½è®¾è®¡ä¸åŒçš„å¯¹æ¯”å­¦ä¹ ç›¸ä¼¼åº¦å‡½æ•°ã€‚ä¾‹å¦‚ï¼Œå¯¹äºæ‰‹è‡‚ï¼Œç»“åˆäº†**å…³èŠ‚æ—‹è½¬ç›¸ä¼¼åº¦**å’Œ**æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ç›¸ä¼¼åº¦**ï¼›å¯¹äºè…¿å’Œèº¯å¹²ï¼Œåˆ™ä¸»è¦ä¾èµ–æ—‹è½¬ç›¸ä¼¼åº¦ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½æ›´ç²¾ç»†åœ°å¤„ç†ä¸åŒæœºå™¨äººä¹‹é—´è‚¢ä½“é•¿åº¦ã€è‡ªç”±åº¦æ•°é‡çš„å·®å¼‚ã€‚\n\n#### ç¬¬å››é˜¶æ®µï¼šä»è¿åŠ¨é‡å®šå‘åˆ°æ½œåœ¨ç©ºé—´æ§åˆ¶ï¼ˆè§£å†³æ§åˆ¶æ³›åŒ–é—®é¢˜ï¼‰\n- **å‰æœŸå·¥ä½œé‡å¿ƒ**ï¼šä¸»è¦é›†ä¸­åœ¨**è¿åŠ¨é‡å®šå‘**æœ¬èº«ï¼Œå³å¦‚ä½•å°†ä¸€ç§å½¢æ€çš„è¿åŠ¨è½¬æ¢ä¸ºå¦ä¸€ç§å½¢æ€çš„è¿åŠ¨ã€‚æ§åˆ¶ç­–ç•¥é€šå¸¸éœ€è¦åœ¨ç›®æ ‡æœºå™¨äººä¸Šå•ç‹¬è®­ç»ƒæˆ–è°ƒæ•´ã€‚\n- **æœ¬æ–‡çš„ç³»ç»Ÿæ€§è´¡çŒ®**ï¼š\n    - **ä¸¤é˜¶æ®µæ¡†æ¶**ï¼šæ˜ç¡®å°†â€œå­¦ä¹ å…±äº«ç©ºé—´â€ï¼ˆé˜¶æ®µä¸€ï¼‰å’Œâ€œåœ¨å…±äº«ç©ºé—´å†…å­¦ä¹ æ§åˆ¶ç­–ç•¥â€ï¼ˆé˜¶æ®µäºŒï¼‰è§£è€¦ã€‚\n    - **å…·èº«æ— å…³çš„æ§åˆ¶ç­–ç•¥**ï¼šåœ¨é˜¶æ®µäºŒï¼Œ**ä»…ä½¿ç”¨äººç±»è¿åŠ¨æ•°æ®**ï¼Œåœ¨å·²å­¦å¥½çš„å…±äº«æ½œåœ¨ç©ºé—´ä¸­è®­ç»ƒä¸€ä¸ª**ç›®æ ‡æ¡ä»¶åŒ–çš„æ§åˆ¶ç­–ç•¥**ï¼ˆé‡‡ç”¨c-VAEï¼‰ã€‚ç”±äºç­–ç•¥åœ¨â€œæŠ½è±¡â€çš„æ½œåœ¨ç©ºé—´ä¸­æ“ä½œï¼Œä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œå³å¯**ç›´æ¥éƒ¨ç½²åˆ°ä»»ä½•å·²ç¼–ç è¿›è¯¥ç©ºé—´çš„æœºå™¨äººä¸Šï¼Œæ— éœ€ä»»ä½•å¾®è°ƒ**ã€‚è¿™å®ç°äº†â€œä¸€æ¬¡è®­ç»ƒï¼Œå¤šå¤„éƒ¨ç½²â€ã€‚\n\n### ç€é‡å‘å±•çš„å‡ ä¸ªä¸åŒæ–¹å‘\n\nåŸºäºä¸Šè¿°è„‰ç»œï¼Œè¯¥é¢†åŸŸçš„ç ”ç©¶é€æ¸åˆ†åŒ–ä¸ºä»¥ä¸‹å‡ ä¸ªé‡ç‚¹æ–¹å‘ï¼š\n\n1.  **è¡¨ç¤ºå­¦ä¹ ä¸å¯¹é½æ–¹å‘**ï¼š\n    - **ç›®æ ‡**ï¼šå¦‚ä½•æ›´å¥½åœ°å­¦ä¹ è·¨åŸŸï¼ˆäºº-æœºã€æœº-æœºï¼‰çš„ã€ä¸ä»»åŠ¡ç›¸å…³çš„ã€åŸŸä¸å˜çš„è¡¨ç¤ºã€‚\n    - **æ¼”è¿›**ï¼šä»æ‰‹å·¥ç‰¹å¾ â†’ æ·±åº¦å­¦ä¹ ç‰¹å¾ â†’ ä½¿ç”¨å¯¹æ¯”å­¦ä¹ /å¯¹æŠ—å­¦ä¹ ç­‰æŠ€æœ¯çš„**å…±äº«æ½œåœ¨ç©ºé—´**ã€‚æœ¬æ–‡é€šè¿‡**è§£è€¦è®¾è®¡å’Œå®šåˆ¶åŒ–åº¦é‡**ï¼Œå°†æ­¤æ–¹å‘æ¨å‘äº†æ›´ç²¾ç»†ã€æ›´èƒ½å¤„ç†å½¢æ€å¼‚æ„çš„æ–°é«˜åº¦ã€‚\n\n2.  **æ•°æ®æ•ˆç‡ä¸å¯æ‰©å±•æ€§æ–¹å‘**ï¼š\n    - **ç›®æ ‡**ï¼šå‡å°‘å¯¹é…å¯¹æ•°æ®çš„ä¾èµ–ï¼Œå¹¶å¿«é€Ÿé€‚åº”æ–°æœºå™¨äººã€‚\n    - **æ¼”è¿›**ï¼šä»éœ€è¦å¤§é‡é…å¯¹æ•°æ® â†’ è‡ªç›‘ç£ç”Ÿæˆé…å¯¹æ•°æ® â†’ **å®Œå…¨æ— ç›‘ç£å­¦ä¹ **ï¼ˆå¦‚ImitationNetï¼‰ã€‚æœ¬æ–‡è¿›ä¸€æ­¥æå‡ºäº†**æ¨¡å—åŒ–å¯æ‰©å±•æ¡†æ¶**ï¼šå¢åŠ æ–°æœºå™¨äººæ—¶ï¼Œåªéœ€è®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„**æœºå™¨äººç‰¹å®šåµŒå…¥å±‚**ï¼Œè€Œå…±äº«ç¼–ç å™¨/è§£ç å™¨å’Œæ§åˆ¶ç­–ç•¥å…¨éƒ¨å†»ç»“ã€‚è¿™æå¤§åœ°æå‡äº†ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œéƒ¨ç½²æ•ˆç‡ã€‚\n\n3.  **æ§åˆ¶ç­–ç•¥æ³›åŒ–æ–¹å‘**ï¼š\n    - **ç›®æ ‡**ï¼šä½¿åœ¨å•ä¸€æ•°æ®æºï¼ˆå¦‚äººç±»ï¼‰ä¸Šè®­ç»ƒçš„ç­–ç•¥èƒ½ç›´æ¥æ§åˆ¶å¤šç§æœºå™¨äººã€‚\n    - **æ¼”è¿›**ï¼šä»ä¸ºæ¯ä¸ªæœºå™¨äººè®­ç»ƒç‹¬ç«‹ç­–ç•¥ â†’ å­¦ä¹ åŸŸè‡ªé€‚åº”ç­–ç•¥ â†’ **åœ¨æŠ½è±¡è¡¨ç¤ºç©ºé—´ï¼ˆæ½œåœ¨ç©ºé—´ï¼‰ä¸­å­¦ä¹ ç­–ç•¥**ã€‚æœ¬æ–‡æ˜ç¡®æå‡ºäº†â€œ**åœ¨å…±äº«æ½œåœ¨ç©ºé—´å†…å­¦ä¹ æ§åˆ¶**â€çš„èŒƒå¼ï¼Œå°†æ§åˆ¶é—®é¢˜ä»å…·ä½“çš„å…³èŠ‚ç©ºé—´æå‡åˆ°è¯­ä¹‰è¿åŠ¨ç©ºé—´ï¼Œæ˜¯å®ç°å¼ºæ³›åŒ–çš„å…³é”®ã€‚\n\n4.  **ç³»ç»Ÿåº”ç”¨ä¸å¤šåŠŸèƒ½æ€§æ–¹å‘**ï¼š\n    - **ç›®æ ‡**ï¼šè¶…è¶Šå•ä¸€çš„é‡å®šå‘ä»»åŠ¡ï¼Œå¼€å‘ç»Ÿä¸€æ¡†æ¶æ”¯æŒå¤šç§åº”ç”¨ã€‚\n    - **æœ¬æ–‡çš„å±•ç¤º**ï¼šåŸºäºè§£è€¦çš„æ½œåœ¨ç©ºé—´ï¼Œæœ¬æ–‡ä¸ä»…å®ç°äº†è¿åŠ¨é‡å®šå‘å’Œè·¨å…·èº«æ§åˆ¶ï¼Œè¿˜æ¼”ç¤ºäº†**æ½œåœ¨ç©ºé—´è¿åŠ¨ç¼–è¾‘**ï¼ˆå¦‚ç»„åˆä¸åŒæœºå™¨äººçš„èº«ä½“éƒ¨ä½è¿åŠ¨ï¼‰ã€**å®æ—¶é¥æ“ä½œ**ã€ä»¥åŠ**ç²¾ç¡®çš„æœ«ç«¯æ‰§è¡Œå™¨ç›®æ ‡åˆ°è¾¾æ§åˆ¶**ã€‚è¿™è¡¨æ˜è¯¥æ¡†æ¶æ­£æœç€æˆä¸ºä¸€ä¸ª**å¤šåŠŸèƒ½çš„è·¨å…·èº«æœºå™¨äººç¼–ç¨‹ä¸æ§åˆ¶å¹³å°**å‘å±•ã€‚\n\n### æ€»ç»“\næœ¬æ–‡çš„å‘å±•è„‰ç»œæ¸…æ™°åœ°å±•ç¤ºäº†ä¸€æ¡ä»**è§£å†³ç‰¹å®šã€æ•°æ®å¯†é›†å‹é—®é¢˜**ï¼Œåˆ°æ„å»º**é€šç”¨ã€å¯æ‰©å±•ã€æ•°æ®é«˜æ•ˆæ¡†æ¶**çš„æŠ€æœ¯è·¯å¾„ã€‚æ¯ä¸€æ­¥æ”¹è¿›éƒ½ç›´æŒ‡å‰åºå·¥ä½œçš„æ ¸å¿ƒç—›ç‚¹ï¼šæ•°æ®ç“¶é¢ˆã€è¡¨ç¤ºä¸å¯¹é½ã€æ³›åŒ–èƒ½åŠ›å·®ã€æ‰©å±•æˆæœ¬é«˜ã€‚é€šè¿‡å¼•å…¥**è§£è€¦çš„å…±äº«æ½œåœ¨ç©ºé—´**å’Œ**æ½œåœ¨ç©ºé—´å†…æ§åˆ¶**è¿™ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼Œæœ¬æ–‡æˆåŠŸåœ°å°†ç ”ç©¶æ–¹å‘ä»â€œå¦‚ä½•æ›´å¥½åœ°æ¨¡ä»¿â€æ¨è¿›åˆ°äº†â€œå¦‚ä½•ç”¨ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹æ§åˆ¶å’Œåè°ƒå¤šç§æœºå™¨äººâ€ï¼Œä¸º scalable multi-robot systems å¥ å®šäº†é‡è¦çš„ç†è®ºåŸºç¡€ã€‚",
            "timestamp": "2026-01-27T16:02:42.421375",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œä½œè€…ï¼ˆYashuai Yanï¼‰æ˜ç¡®å¼•ç”¨çš„å‰ä½œä¸»è¦æ˜¯ **ImitationNet**ã€‚è¿™ç¯‡å‰ä½œæ˜¯ä½œè€…å›¢é˜Ÿåœ¨è·¨å…·èº«ï¼ˆCross-Embodimentï¼‰è¿åŠ¨é‡å®šå‘é¢†åŸŸçš„åŸºç¡€å·¥ä½œã€‚æœ¬è®ºæ–‡ï¼ˆ*Learning a Unified Latent Space for Cross-Embodiment Robot Control*ï¼‰æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šçš„é‡å¤§æ‰©å±•å’Œæ”¹è¿›ã€‚\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†åˆ†æäº†è¿™ç¯‡å‰ä½œï¼Œå¹¶é‡ç‚¹å¯¹æ¯”äº†æœ¬è®ºæ–‡ä¸å…¶çš„åŒºåˆ«å’Œä¸»è¦æ”¹è¿›ï¼š\n\n| ç‰¹æ€§ç»´åº¦ | å‰ä½œï¼šImitationNet (Yan et al., Humanoids 2023) | æœ¬è®ºæ–‡ï¼šLearning a Unified Latent Space... | ä¸»è¦åŒºåˆ«ä¸æ”¹è¿› |\n| :--- | :--- | :--- | :--- |\n| **æ ¸å¿ƒé—®é¢˜** | **å•å¯¹å•è¿åŠ¨é‡å®šå‘**ï¼šä¸“æ³¨äºä»**äººç±»**åˆ°**å•ä¸ªç‰¹å®šæœºå™¨äºº**ï¼ˆå¦‚TIAGo++ï¼‰çš„è¿åŠ¨è¿ç§»ã€‚ | **å¤šå¯¹å¤šç»Ÿä¸€æ§åˆ¶**ï¼šæ—¨åœ¨ä¸º**å¤šç§**äººå½¢æœºå™¨äººï¼ˆåŠäººç±»ï¼‰æ„å»ºä¸€ä¸ª**ç»Ÿä¸€çš„å…±äº«æ½œåœ¨ç©ºé—´**ï¼Œå¹¶åœ¨æ­¤ç©ºé—´å†…è®­ç»ƒå¯è·¨å¹³å°éƒ¨ç½²çš„æ§åˆ¶ç­–ç•¥ã€‚ | **é—®é¢˜èŒƒå›´æ‰©å¤§**ï¼šä»è§£å†³â€œäººç±»->1ä¸ªæœºå™¨äººâ€çš„æ˜ å°„é—®é¢˜ï¼Œå‡çº§ä¸ºè§£å†³â€œNä¸ªå…·èº«ï¼ˆäººç±»+å¤šæœºå™¨äººï¼‰<->ç»Ÿä¸€è¡¨ç¤º<->Mä¸ªä»»åŠ¡â€çš„é€šç”¨æ§åˆ¶é—®é¢˜ã€‚ |\n| **æ½œåœ¨ç©ºé—´è®¾è®¡** | **å•ä¸€ã€æ•´ä½“çš„æ½œåœ¨ç©ºé—´**ï¼šå°†æ•´ä¸ªäººä½“è¿åŠ¨ç¼–ç åˆ°ä¸€ä¸ªæœªè§£è€¦çš„æ½œåœ¨å‘é‡ä¸­ã€‚ | **è§£è€¦çš„ã€åˆ†æ®µçš„æ½œåœ¨ç©ºé—´**ï¼šå°†æ½œåœ¨ç©ºé—´æŒ‰èº«ä½“éƒ¨ä½ï¼ˆå·¦è‡‚LAã€å³è‡‚RAã€èº¯å¹²TKã€å·¦è…¿LLã€å³è…¿RLï¼‰è§£è€¦ä¸ºäº”ä¸ªç‹¬ç«‹çš„å­ç©ºé—´ã€‚ | **æ¶æ„åˆ›æ–°**ï¼š<br>1. **è§£è€¦è®¾è®¡**ï¼šå…è®¸å¯¹ä¸åŒèº«ä½“éƒ¨ä½è¿›è¡Œç»†ç²’åº¦å¯¹é½ï¼Œèƒ½æ›´å¥½åœ°å¤„ç†å½¢æ€ä¸å¯¹ç§°ï¼ˆå¦‚åªæœ‰æ‰‹è‡‚çš„æœºå™¨äººï¼‰æˆ–éƒ¨åˆ†ç»“æ„ç¼ºå¤±çš„æœºå™¨äººã€‚<br>2. **æ¨¡å—åŒ–**ï¼šæ”¯æŒå¯¹èº«ä½“éƒ¨ä½è¿›è¡Œç‹¬ç«‹æ§åˆ¶ä¸ç¼–è¾‘ã€‚ |\n| **ç›¸ä¼¼æ€§åº¦é‡** | ä¸»è¦åŸºäº**å…³èŠ‚æ—‹è½¬**çš„ç›¸ä¼¼æ€§ï¼ˆå¦‚å››å…ƒæ•°ç‚¹ç§¯ï¼‰ã€‚ | **å®šåˆ¶åŒ–çš„æ··åˆç›¸ä¼¼æ€§åº¦é‡**ï¼š<br>- **å¯¹äºæ‰‹è‡‚ï¼ˆLA/RAï¼‰**ï¼šç»“åˆ**å…³èŠ‚æ—‹è½¬ç›¸ä¼¼æ€§**å’Œ**æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ç›¸ä¼¼æ€§**ï¼ˆ`S_k = D_R + Ï‰ * D_ee`ï¼‰ã€‚<br>- **å¯¹äºèº¯å¹²å’Œè…¿**ï¼šä»…ä½¿ç”¨å…³èŠ‚æ—‹è½¬ç›¸ä¼¼æ€§ã€‚ | **åº¦é‡æ”¹è¿›**ï¼šå¼•å…¥äº†å¯¹æœºå™¨äººä»»åŠ¡ï¼ˆå¦‚æ“ä½œï¼‰è‡³å…³é‡è¦çš„**æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®å¯¹é½**ã€‚è¿™ä½¿å¾—å­¦ä¹ åˆ°çš„è¡¨ç¤ºä¸ä»…è§†è§‰ç›¸ä¼¼ï¼Œè€Œä¸”**åŠŸèƒ½å‡†ç¡®**ï¼Œä¸ºåç»­çš„ç²¾ç¡®ç›®æ ‡æ§åˆ¶ç­–ç•¥å¥ å®šäº†åŸºç¡€ã€‚ |\n| **è®­ç»ƒæ•°æ®ä¸ç›®æ ‡** | ç›®æ ‡ï¼šå­¦ä¹ ä¸€ä¸ª**å…±äº«çš„æ½œåœ¨ç©ºé—´**ï¼Œå®ç°ä»äººç±»åˆ°**ä¸€ä¸ªç›®æ ‡æœºå™¨äºº**çš„é«˜è´¨é‡è¿åŠ¨é‡å®šå‘ã€‚éœ€è¦è¯¥æœºå™¨äººçš„è¿åŠ¨æ•°æ®ï¼ˆé€šè¿‡éšæœºé‡‡æ ·+æ­£å‘è¿åŠ¨å­¦ç”Ÿæˆï¼‰ã€‚ | **ä¸¤é˜¶æ®µæ¡†æ¶**ï¼š<br>1. **é˜¶æ®µä¸€**ï¼šå­¦ä¹ ä¸€ä¸ª**ç»Ÿä¸€**çš„æ½œåœ¨ç©ºé—´ï¼Œå¯¹é½**äººç±»å’Œå¤šä¸ªæœºå™¨äºº**çš„è¿åŠ¨ã€‚ä½¿ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œæ•°æ®åŒ…æ‹¬äººç±»åŠ¨ä½œå’Œæ‰€æœ‰æœºå™¨äººçš„éšæœºé‡‡æ ·å§¿æ€ã€‚<br>2. **é˜¶æ®µäºŒ**ï¼šåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­ï¼Œ**ä»…ä½¿ç”¨äººç±»æ•°æ®**è®­ç»ƒä¸€ä¸ª**ç›®æ ‡æ¡ä»¶æ§åˆ¶ç­–ç•¥**ï¼ˆc-VAEï¼‰ã€‚ | **èŒƒå¼è½¬å˜**ï¼š<br>1. **å¤šå…·èº«ç»Ÿä¸€**ï¼šæ¨¡å‹ä¸€æ¬¡æ€§å­¦ä¹ å®¹çº³å¤šä¸ªæœºå™¨äººï¼Œè€Œéä¸ºæ¯ä¸ªæœºå™¨äººè®­ç»ƒç‹¬ç«‹æ¨¡å‹ã€‚<br>2. **ç­–ç•¥ä¸é‡å®šå‘åˆ†ç¦»**ï¼šå°†â€œå­¦ä¹ é€šç”¨è¡¨ç¤ºâ€å’Œâ€œå­¦ä¹ æ§åˆ¶ç­–ç•¥â€è§£è€¦ã€‚ç­–ç•¥åœ¨æŠ½è±¡ä¸”ä¸å…·èº«æ— å…³çš„æ½œåœ¨ç©ºé—´ä¸­å­¦ä¹ ï¼Œå®ç°äº†**çœŸæ­£çš„é›¶æ ·æœ¬è·¨å¹³å°éƒ¨ç½²**ã€‚ |\n| **å¯æ‰©å±•æ€§** | æ·»åŠ æ–°æœºå™¨äººéœ€è¦**é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹**ï¼Œä¸ºæ–°çš„â€œäººç±»-æœºå™¨äººâ€å¯¹å­¦ä¹ ä¸€ä¸ªæ–°çš„æ½œåœ¨ç©ºé—´ã€‚ | **é«˜æ•ˆæ‰©å±•**ï¼šæ ¸å¿ƒç½‘ç»œï¼ˆ`E_h, E_X, D_X`ï¼‰åœ¨åˆå§‹å¤šæœºå™¨äººè®­ç»ƒåå†»ç»“ã€‚æ·»åŠ æ–°æœºå™¨äººåªéœ€è®­ç»ƒä¸€ä¸ª**è½»é‡çº§çš„ã€æœºå™¨äººç‰¹å®šçš„åµŒå…¥å±‚**ï¼ˆ`E_r, D_r`ï¼‰ï¼Œè€—æ—¶çŸ­ï¼ˆè®ºæ–‡ä¸­çº¦15åˆ†é’Ÿï¼‰ã€‚ | ** scalability å¤§å¹…æå‡**ï¼šå®ç°äº†â€œä¸€æ¬¡è®­ç»ƒï¼Œçµæ´»æ‰©å±•â€çš„æ¡†æ¶ï¼Œä½¿ç®¡ç†å¤šæœºå™¨äººç³»ç»Ÿå˜å¾—é«˜æ•ˆï¼Œç¬¦åˆæœºå™¨äººå½¢æ€å¿«é€Ÿå¤šæ ·åŒ–çš„è¶‹åŠ¿ã€‚ |\n| **è¾“å‡ºä¸åº”ç”¨** | è¾“å‡ºï¼šé‡å®šå‘åçš„æœºå™¨äººå…³èŠ‚è½¨è¿¹ã€‚ä¸»è¦ç”¨äº**è¿åŠ¨æ¨¡ä»¿/é‡å®šå‘**ã€‚ | è¾“å‡ºï¼š1) è·¨å…·èº«è¿åŠ¨é‡å®šå‘ï¼›2) **å¯ç›´æ¥éƒ¨ç½²çš„ã€ç›®æ ‡é©±åŠ¨çš„æ§åˆ¶ç­–ç•¥**ï¼›3) æ”¯æŒæ½œåœ¨ç©ºé—´å†…çš„è¿åŠ¨ç¼–è¾‘ä¸åˆæˆã€‚ | **åŠŸèƒ½å¢å¼º**ï¼šä»å•ä¸€çš„â€œé‡å®šå‘å·¥å…·â€å‡çº§ä¸ºä¸€ä¸ª**å®Œæ•´çš„æ§åˆ¶æ¡†æ¶**ã€‚æ ¸å¿ƒè´¡çŒ®åœ¨äºè¯æ˜äº†åœ¨ç»Ÿä¸€æ½œåœ¨ç©ºé—´ä¸­å­¦ä¹ çš„ç­–ç•¥å¯ä»¥**é›¶æ ·æœ¬**æ§åˆ¶å¤šä¸ªå½¢æ€å„å¼‚çš„æœºå™¨äººï¼Œå¹¶è¾¾åˆ°äºšå˜ç±³çº§çš„æ§åˆ¶ç²¾åº¦ã€‚ |\n| **å®éªŒéªŒè¯** | éªŒè¯åœºæ™¯ï¼šäººç±»åˆ°å•ä¸ªæœºå™¨äººï¼ˆTIAGo++ï¼‰çš„è¿åŠ¨é‡å®šå‘è´¨é‡ï¼ˆè§†è§‰ç›¸ä¼¼æ€§ã€å…³èŠ‚è¯¯å·®ç­‰ï¼‰ã€‚ | éªŒè¯åœºæ™¯å…¨é¢ï¼š<br>1. **å¤šæœºå™¨äººé‡å®šå‘**ï¼ˆäººç±»->å¤šæœºå™¨äººï¼Œæœºå™¨äºº->æœºå™¨äººï¼‰ã€‚<br>2. **æ–°æœºå™¨äººå¿«é€Ÿé›†æˆ**ã€‚<br>3. **è·¨å¹³å°æ§åˆ¶ç­–ç•¥**ï¼ˆåœ¨6ä¸ªä¸åŒæœºå™¨äººä¸Šæµ‹è¯•ç›®æ ‡åˆ°è¾¾ä»»åŠ¡ï¼‰ã€‚<br>4. **æ½œåœ¨ç©ºé—´ç¼–è¾‘**ã€‚<br>5. **å®ç‰©æœºå™¨äººå®éªŒ**ï¼ˆé¥æ“ä½œã€ç›´æ¥æ§åˆ¶ï¼‰ã€‚ | **è¯„ä¼°ç»´åº¦æå¤§ä¸°å¯Œ**ï¼šä¸ä»…è¯„ä¼°äº†é‡å®šå‘è´¨é‡ï¼ˆä¸”è¯æ˜è§£è€¦æ½œåœ¨ç©ºé—´åœ¨æ—‹è½¬ç²¾åº¦ä¸Šè¿½å¹³äº†ä¸ºå•ä¸ªæœºå™¨äººè®­ç»ƒçš„ImitationNetï¼‰ï¼Œæ›´é‡è¦çš„æ˜¯**ç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†å…¶ä½œä¸ºé€šç”¨æ§åˆ¶æ¡†æ¶çš„å¯è¡Œæ€§ã€å‡†ç¡®æ€§å’Œæ•ˆç‡**ã€‚ |\n\n### æ€»ç»“ï¼šæ ¸å¿ƒæ”¹è¿›ä¸åˆ›æ–°\n\næœ¬è®ºæ–‡ç›¸å¯¹äºå‰ä½œ **ImitationNet** çš„é£è·ƒä¸»è¦ä½“ç°åœ¨ï¼š\n\n1.  **ä»â€œæ˜ å°„â€åˆ°â€œç©ºé—´â€**ï¼šå°†æ ¸å¿ƒä»å­¦ä¹ ä¸€å¯¹ä¸€çš„**æ˜ å°„å‡½æ•°**ï¼Œè½¬å˜ä¸ºæ„å»ºä¸€ä¸ªå®¹çº³å¤šä¸»ä½“çš„**ç»Ÿä¸€è¯­ä¹‰ç©ºé—´**ã€‚è¿™æ˜¯ä¸€ä¸ªæ ¹æœ¬æ€§çš„èŒƒå¼æå‡ã€‚\n2.  **ä»â€œæ¨¡ä»¿â€åˆ°â€œæ§åˆ¶â€**ï¼šæœ€ç»ˆç›®çš„ä¸å†æ˜¯ç”Ÿæˆæ¨¡ä»¿åŠ¨ä½œï¼Œè€Œæ˜¯å­¦ä¹ ä¸€ä¸ªå¯åœ¨è¯¥ç©ºé—´ä¸­é©±åŠ¨çŠ¶æ€å˜åŒ–çš„ã€**ä¸å…·èº«æ— å…³çš„æ§åˆ¶ç­–ç•¥**ã€‚è¿™ä½¿å¾—åˆ©ç”¨ä¸°å¯Œçš„äººç±»æ•°æ®ä¸ºæœºå™¨äººè®­ç»ƒé€šç”¨ç­–ç•¥æˆä¸ºå¯èƒ½ã€‚\n3.  **ç³»ç»Ÿå¯æ‰©å±•æ€§è®¾è®¡**ï¼šé€šè¿‡**è§£è€¦æ½œåœ¨ç©ºé—´**å’Œ**è½»é‡çº§åµŒå…¥å±‚**çš„è®¾è®¡ï¼Œå®ç°äº†æ¨¡å‹çš„é«˜æ•ˆæ‰©å±•ï¼Œè§£å†³äº†è·¨å…·èº«æ§åˆ¶å®é™…éƒ¨ç½²ä¸­çš„å…³é”®ç“¶é¢ˆã€‚\n4.  **é¢å‘ä»»åŠ¡çš„è¡¨ç¤ºå­¦ä¹ **ï¼šé€šè¿‡å¼•å…¥**æ··åˆç›¸ä¼¼æ€§åº¦é‡**ï¼Œä½¿å­¦ä¹ åˆ°çš„æ½œåœ¨ç©ºé—´ä¸ä»…ä¿è¯è¿åŠ¨é£æ ¼çš„ç›¸ä¼¼æ€§ï¼Œæ›´ä¿è¯äº†å¯¹äºæœºå™¨äººä»»åŠ¡ï¼ˆå¦‚æœ«ç«¯æ‰§è¡Œå™¨å®šä½ï¼‰è‡³å…³é‡è¦çš„å‡ ä½•ä¸åŠŸèƒ½ä¸€è‡´æ€§ã€‚\n\nå› æ­¤ï¼Œæœ¬è®ºæ–‡æ˜¯ä½œè€…åœ¨å‰ä½œè§£å†³â€œ**å¦‚ä½•è®©ä¸€ä¸ªæœºå™¨äººåƒäººä¸€æ ·åŠ¨**â€çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥è§£å†³äº†â€œ**å¦‚ä½•ç”¨ä¸€ä¸ªç­–ç•¥è®©æ‰€æœ‰æœºå™¨äººéƒ½èƒ½å®Œæˆäººçš„ä»»åŠ¡**â€è¿™ä¸€æ›´å…·æŒ‘æˆ˜æ€§å’Œå®ç”¨ä»·å€¼çš„é—®é¢˜ã€‚",
            "timestamp": "2026-01-27T16:03:37.842081",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†å¯¹è¿™ç¯‡é¢˜ä¸ºã€ŠLearning a Unified Latent Space for Cross-Embodiment Robot Controlã€‹çš„è®ºæ–‡è¿›è¡Œæ·±å…¥åˆ†æï¼Œå¹¶è¯¦ç»†é˜è¿°å…¶æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ä¸ä»·å€¼åˆ†æ\n\næœ¬æ–‡çš„æ ¸å¿ƒç›®æ ‡æ˜¯è§£å†³**è·¨å…·èº«æœºå™¨äººæ§åˆ¶**çš„éš¾é¢˜ï¼Œå³å¼€å‘ä¸€ä¸ªç»Ÿä¸€çš„æ§åˆ¶æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥åº”ç”¨äºå½¢æ€å„å¼‚ï¼ˆå¦‚å•è‡‚ã€åŒè‡‚ã€è…¿å¼äººå½¢ï¼‰çš„æœºå™¨äººå¹³å°ï¼Œè€Œæ— éœ€ä¸ºæ¯ä¸ªæœºå™¨äººé‡æ–°è®­ç»ƒç­–ç•¥ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº**ä¸¤é˜¶æ®µæ¡†æ¶**ï¼š1ï¼‰å­¦ä¹ ä¸€ä¸ªè§£è€¦çš„ã€ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´æ¥å¯¹é½ä¸åŒâ€œèº«ä½“â€ï¼ˆäººç±»ä¸å„ç§æœºå™¨äººï¼‰çš„è¿åŠ¨è¯­ä¹‰ï¼›2ï¼‰åœ¨è¯¥æ½œåœ¨ç©ºé—´ä¸­ç›´æ¥è®­ç»ƒç›®æ ‡é©±åŠ¨çš„æ§åˆ¶ç­–ç•¥ï¼Œå¹¶å®ç°é›¶æ ·æœ¬è¿ç§»ã€‚\n\n**å®é™…ä»·å€¼**åœ¨äºæå¤§åœ°æå‡äº†æœºå™¨äººç³»ç»Ÿçš„**å¯æ‰©å±•æ€§**å’Œ**éƒ¨ç½²æ•ˆç‡**ã€‚æ–°æœºå™¨äººåªéœ€è®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„åµŒå…¥å±‚å³å¯èå…¥ç³»ç»Ÿï¼Œå¹¶ç«‹å³å¤ç”¨æ‰€æœ‰å·²è®­ç»ƒå¥½çš„é«˜çº§æ§åˆ¶ç­–ç•¥ï¼Œè¿™å¯¹äºç®¡ç†å¼‚æ„æœºå™¨äººé›†ç¾¤ã€å¿«é€Ÿé›†æˆæ–°æœºå™¨äººå¹³å°å…·æœ‰é‡å¤§æ„ä¹‰ã€‚\n\n---\n\n## å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è¿°\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†è¯´æ˜äº†è®ºæ–‡æå‡ºçš„å„é¡¹å…³é”®æŠ€æœ¯æ–¹æ³•ã€æ•°å­¦åŸç†åŠå…·ä½“å‚æ•°ã€‚\n\n| æ–¹æ³•æ¨¡å— | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°ä¸è¯´æ˜ |\n| :--- | :--- | :--- |\n| **1. è§£è€¦çš„æ½œåœ¨ç©ºé—´æ¶æ„** | **åŸç†**ï¼šä¸ºè§£å†³ä¸åŒæœºå™¨äººå½¢æ€ï¼ˆå¦‚æœ‰çš„æœ‰è…¿æ— èº¯å¹²ï¼Œæœ‰çš„åªæœ‰æ‰‹è‡‚ï¼‰å¯¼è‡´çš„è¿åŠ¨æ˜ å°„æ¨¡ç³Šé—®é¢˜ï¼Œä½œè€…æ²¡æœ‰ä½¿ç”¨å•ä¸€çš„æ½œåœ¨ç©ºé—´è¡¨ç¤ºå…¨èº«è¿åŠ¨ï¼Œè€Œæ˜¯å°†èº«ä½“è§£è€¦ä¸ºäº”ä¸ªç‹¬ç«‹éƒ¨åˆ†ï¼š**å·¦è‡‚(LA)ã€å³è‡‚(RA)ã€èº¯å¹²(TK)ã€å·¦è…¿(LL)ã€å³è…¿(RL)**ã€‚æ¯ä¸ªéƒ¨åˆ†æ‹¥æœ‰è‡ªå·±ç‹¬ç«‹çš„æ½œåœ¨å­ç©ºé—´ã€‚è¿™æ ·ï¼Œå¯¹äºç¼ºå°‘æŸäº›èº«ä½“éƒ¨ä½çš„æœºå™¨äººï¼ˆå¦‚TIAGOåªæœ‰æ‰‹è‡‚ï¼‰ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†æ—¶å°±åªæ¿€æ´»ç›¸å…³çš„å­ç©ºé—´ï¼ˆLA, RAï¼‰ï¼Œé¿å…äº†ç¼ºå¤±éƒ¨åˆ†å¯¹è¡¨ç¤ºçš„å¹²æ‰°ã€‚ | - **æ½œåœ¨å­ç©ºé—´æ•°é‡**ï¼š5ä¸ª (LA, RA, TK, LL, RL) <br> - **æ¯ä¸ªå­ç©ºé—´ç»´åº¦**ï¼š16ç»´ <br> - **è¾“å‡ºèŒƒå›´**ï¼šé€šè¿‡Tanhæ¿€æ´»å‡½æ•°çº¦æŸåœ¨[-1, 1]åŒºé—´å†… |\n| **2. å®šåˆ¶åŒ–çš„å¯¹æ¯”å­¦ä¹ ç›¸ä¼¼åº¦åº¦é‡** | **åŸç†**ï¼šåœ¨å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œéœ€è¦åº¦é‡ä¸åŒâ€œèº«ä½“â€å§¿æ€ä¹‹é—´çš„ç›¸ä¼¼æ€§ä»¥æ‹‰è¿‘è¯­ä¹‰ç›¸ä¼¼çš„æ ·æœ¬ã€‚ä½œè€…ä¸ºä¸åŒèº«ä½“éƒ¨ä½è®¾è®¡äº†å®šåˆ¶åŒ–çš„ç›¸ä¼¼åº¦å‡½æ•° `S_k`ï¼š<br> 1. **åŸºäºæ—‹è½¬çš„åº¦é‡ (ç”¨äºTK, LL, RL)**ï¼šè®¡ç®—å¯¹åº”å…³èŠ‚æ—‹è½¬ï¼ˆå››å…ƒæ•°ï¼‰çš„å·®å¼‚ã€‚å…¬å¼ä¸ºï¼š<br> `D_R(ğ±_A, ğ±_B) = Î£_j (1 - <q_A^j, q_B^j>^2)` <br> å…¶ä¸­ `<Â·,Â·>` æ˜¯å››å…ƒæ•°ç‚¹ç§¯ï¼Œå€¼è¶Šæ¥è¿‘1è¡¨ç¤ºæ—‹è½¬è¶Šç›¸ä¼¼ã€‚<br> 2. **æ··åˆåº¦é‡ (ç”¨äºLA, RA)**ï¼šç»“åˆæ—‹è½¬å’Œæœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ä¿¡æ¯ï¼Œè¿™å¯¹æœºæ¢°è‡‚æ“ä½œè‡³å…³é‡è¦ã€‚å…¬å¼ä¸ºï¼š<br> `S_k(ğ±_A, ğ±_B) = D_R(ğ±_A, ğ±_B) + Ï‰ * D_ee(ğ±_A, ğ±_B)` <br> å…¶ä¸­ `D_ee(ğ±_A, ğ±_B) = â€–p_A^ee - p_B^eeâ€–_2`ã€‚æœ«ç«¯ä½ç½® `p^ee` ä¼šè½¬æ¢åˆ°è‚©éƒ¨åæ ‡ç³»å¹¶**ç”¨è‡‚é•¿å½’ä¸€åŒ–**ï¼Œä»¥æ¶ˆé™¤å°ºå¯¸å’Œåæ ‡ç³»å·®å¼‚ã€‚ | - **æƒé‡ Ï‰**ï¼šç”¨äºå¹³è¡¡æ—‹è½¬ä¸æœ«ç«¯ä½ç½®çš„é‡è¦æ€§ã€‚è®ºæ–‡é€šè¿‡æ¶ˆèå®éªŒé€‰å®š `Ï‰ = 1.0`ã€‚<br> - **å½’ä¸€åŒ–**ï¼šå…³é”®æ­¥éª¤ï¼Œç¡®ä¿äº†ä¸åŒå°ºå¯¸æœºå™¨äººçš„æœ«ç«¯ä½ç½®åœ¨åº¦é‡ä¸Šå…·æœ‰å¯æ¯”æ€§ã€‚ |\n| **3. ç»Ÿä¸€æ½œåœ¨ç©ºé—´è®­ç»ƒçš„æ€»æŸå¤±å‡½æ•°** | **åŸç†**ï¼šé€šè¿‡ç»„åˆå¤šä¸ªæŸå¤±é¡¹ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è®­ç»ƒç¼–ç å™¨-è§£ç å™¨ç½‘ç»œï¼ŒåŒæ—¶ä¼˜åŒ–æ½œåœ¨ç©ºé—´çš„å¯¹é½æ€§ã€é‡å»ºç²¾åº¦å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚<br> **æ€»æŸå¤±å‡½æ•°**ï¼š<br> `â„’_total = Î»_c * â„’_contrastive + Î»_rec * â„’_rec + Î»_ltc * â„’_ltc + Î»_temp * â„’_temporal` <br> - **â„’_contrastive (å¯¹æ¯”æŸå¤±)**ï¼šä½¿ç”¨ä¸‰å…ƒç»„æŸå¤± (Triplet Loss)ã€‚å¯¹äºæ¯ä¸ªå­ç©ºé—´ï¼Œæ‹‰è¿‘é”šç‚¹æ ·æœ¬ `z_i^o` ä¸æ­£æ ·æœ¬ `z_j^+` çš„è·ç¦»ï¼Œæ¨è¿œå…¶ä¸è´Ÿæ ·æœ¬ `z_k^-` çš„è·ç¦»ï¼Œé—´éš”ä¸º `Î±`ï¼š<br> `max(â€–z_i^o - z_j^+â€–_2 - â€–z_i^o - z_k^-â€–_2 + Î±, 0)` <br> - **â„’_rec (é‡å»ºæŸå¤±)**ï¼šç¡®ä¿æœºå™¨äººå§¿æ€ç¼–ç åå†è§£ç èƒ½å‡†ç¡®è¿˜åŸã€‚`â„’_rec = â€–ğ±_A - ğ±Ì‚_Aâ€–_2` <br> - **â„’_ltc (æ½œåœ¨ä¸€è‡´æ€§æŸå¤±)**ï¼šé’ˆå¯¹æ— é…å¯¹æ•°æ®çš„äººç±»å§¿æ€ï¼Œç¡®ä¿äººç±»å§¿æ€ç¼–ç ã€è§£ç ä¸ºæœºå™¨äººå§¿æ€ã€å†ç¼–ç åï¼Œå…¶æ½œåœ¨è¡¨ç¤ºä¸æœ€åˆçš„äººç±»æ½œåœ¨è¡¨ç¤ºä¸€è‡´ã€‚`â„’_ltc = â€–E_h(ğ±_H) - E_X(D_X(E_h(ğ±_H)))â€–_2` <br> - **â„’_temporal (æ—¶é—´æŸå¤±)**ï¼šå¯¹é½äººç±»æ‰‹éƒ¨é€Ÿåº¦ä¸æœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨é€Ÿåº¦ï¼Œä¿è¯è¿åŠ¨åŠ¨æ€çš„ä¸€è‡´æ€§ã€‚`â„’_temporal = â€–v_H^hand - v_A^eeâ€–_2` | - **æŸå¤±æƒé‡**ï¼š`Î»_c = 10, Î»_rec = 5, Î»_ltc = 1, Î»_temp = 0.1` <br> - **ä¸‰å…ƒç»„æŸå¤±é—´éš” Î±**ï¼š`Î± = 0.05` <br> - **å…³é”®åˆ›æ–°**ï¼š`â„’_ltc` è§£å†³äº†**æ— éœ€é…å¯¹æ•°æ®**å³å¯è¿›è¡Œäººâ†’æœºå™¨äººè¿åŠ¨é‡å®šå‘çš„æ ¸å¿ƒéš¾é¢˜ã€‚ |\n| **4. é«˜æ•ˆé›†æˆæ–°æœºå™¨äºº** | **åŸç†**ï¼šåˆ©ç”¨æ¨¡å—åŒ–è®¾è®¡å®ç°å¯æ‰©å±•æ€§ã€‚ç³»ç»Ÿæ ¸å¿ƒç»„ä»¶â€”â€”äººç±»ç¼–ç å™¨ `E_h`ã€è·¨å…·èº«ç¼–ç å™¨ `E_X`ã€è·¨å…·èº«è§£ç å™¨ `D_X`â€”â€”åœ¨å¤šä¸ªæœºå™¨äººä¸Š**è”åˆè®­ç»ƒåå›ºå®š**ã€‚æ¯ä¸ªæœºå™¨äººç‹¬æœ‰çš„ã€ç”¨äºå°†å…¶å…³èŠ‚ç©ºé—´ä¸å…±äº«ç‰¹å¾ç©ºé—´äº’ç›¸æ˜ å°„çš„è½»é‡çº§åµŒå…¥å±‚ `E_r` å’Œ `D_r` åˆ™æ˜¯å¯å­¦ä¹ çš„ã€‚**æ·»åŠ æ–°æœºå™¨äººæ—¶ï¼Œåªéœ€å†»ç»“æ ¸å¿ƒç½‘ç»œï¼Œå•ç‹¬è®­ç»ƒè¯¥æœºå™¨äººçš„ `E_r` å’Œ `D_r` å³å¯**ï¼Œä½¿å…¶å§¿æ€èƒ½å¤Ÿä¸å·²æœ‰çš„ç»Ÿä¸€æ½œåœ¨ç©ºé—´å¯¹é½ã€‚ | - **åµŒå…¥å±‚ç»´åº¦**ï¼šå°†æœºå™¨äººåŸå§‹å§¿æ€æ˜ å°„åˆ°1024ç»´çš„å…±äº«ç‰¹å¾ç©ºé—´ã€‚<br> - **è®­ç»ƒæˆæœ¬**ï¼šä»…éœ€çº¦15åˆ†é’Ÿï¼ˆå¯¹æ¯”ä»å¤´è®­ç»ƒçš„æ•°å°æ—¶ï¼‰ï¼Œå®ç°äº†é«˜æ•ˆçš„æ‰©å±•ã€‚ |\n| **5. æ½œåœ¨ç©ºé—´ä¸­çš„ç›®æ ‡æ¡ä»¶æ§åˆ¶ç­–ç•¥ (c-VAE)** | **åŸç†**ï¼šåœ¨è®­ç»ƒå¥½çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œä½¿ç”¨æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨å­¦ä¹ ä¸€ä¸ªä»…åŸºäº**äººç±»æ•°æ®**çš„è¿åŠ¨ç”Ÿæˆç­–ç•¥ã€‚ç­–ç•¥å­¦ä¹ é¢„æµ‹æ½œåœ¨ç©ºé—´çš„ä½ç§» `d_t = z_{t+1} - z_t`ï¼Œæ¡ä»¶æ˜¯åŸºäºå½“å‰æ½œåœ¨çŠ¶æ€ `z_t` å’ŒæŒ‡å‘ç›®æ ‡çš„æ„å›¾ä¿¡å· `vÌ„_ee`ã€‚<br> **è®­ç»ƒ**ï¼šä»äººç±»è¿åŠ¨åºåˆ—ä¸­é‡‡æ ·å½“å‰å¸§ `x_H^t`ã€ä¸‹ä¸€å¸§ `x_H^{t+1}` å’Œæœªæ¥ç›®æ ‡å¸§ `x_H^{t_G}`ã€‚è®¡ç®—å¹³å‡ç›®æ ‡é€Ÿåº¦ä½œä¸ºæ¡ä»¶ï¼š`vÌ„_ee = (p_hand^{t_G} - p_hand^{t}) / (t_G - t)`ã€‚c-VAEç¼–ç å™¨ä»¥ `z_t` å’Œ `vÌ„_ee` ä¸ºè¾“å…¥ï¼Œè¾“å‡ºæ½œåœ¨åˆ†å¸ƒå‚æ•°(Î¼, Ïƒ)ï¼Œé‡‡æ ·å¾—åˆ°å™ªå£°å˜é‡ï¼Œè§£ç å™¨æ®æ­¤é¢„æµ‹ä½ç§» `dÌ‚_t`ã€‚<br> **æ¨ç†**ï¼šç”¨æˆ·ç»™å®šæœºå™¨äººæœ«ç«¯ç›®æ ‡ä½ç½® `p_ee^T` å’Œæ—¶é—´èŒƒå›´ `T`ï¼Œè®¡ç®—å½“å‰ç›®æ ‡é€Ÿåº¦ `vÌ„_ee = (p_ee^T - p_ee^t) / (T - t)`ã€‚ç»“åˆå½“å‰æœºå™¨äººå§¿æ€ç¼–ç  `z_t`ï¼Œè¾“å…¥è§£ç å™¨é¢„æµ‹ `dÌ‚_t`ï¼Œä»¥è‡ªå›å½’æ–¹å¼æ›´æ–°æ½œåœ¨çŠ¶æ€ï¼š`z_{t+1} = z_t + dÌ‚_t`ï¼Œæœ€ç»ˆè§£ç ä¸ºæœºå™¨äººå…³èŠ‚æŒ‡ä»¤ã€‚ | - **c-VAEæ½œåœ¨ç»´åº¦**ï¼š32ç»´ï¼ˆè¡¨ç¤ºé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®ï¼‰<br> - **KLæ•£åº¦æƒé‡**ï¼š`Î»_KL = 10^{-4}`ï¼ˆè¾ƒå°çš„æƒé‡ä¼˜å…ˆä¿è¯é‡å»ºç²¾åº¦ï¼‰<br> - **æ§åˆ¶é¢‘ç‡**ï¼šçº¦100 Hzï¼Œæ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚<br> - **å…³é”®ä¼˜åŠ¿**ï¼šç­–ç•¥åœ¨**äººç±»æ½œåœ¨ç©ºé—´**ä¸­è®­ç»ƒï¼Œä½†å¯ç›´æ¥éƒ¨ç½²åˆ°**ä»»ä½•**å·²å¯¹é½çš„æœºå™¨äººä¸Šï¼Œå®ç°**é›¶æ ·æœ¬è·¨å…·èº«æ§åˆ¶**ã€‚ |\n| **6. æ— éœ€æœºå™¨äººæ•°æ®çš„æ•°æ®ç”Ÿæˆç­–ç•¥** | **åŸç†**ï¼šæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹**ä¸éœ€è¦ä»»ä½•çœŸå®çš„æœºå™¨äººè¿åŠ¨æ•°æ®**ã€‚å¯¹äºæœºå™¨äººæ•°æ®ï¼Œä½œè€…åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­ï¼Œä»å„æœºå™¨äººçš„å…³èŠ‚ç©ºé—´**å‡åŒ€éšæœºé‡‡æ ·**å…³èŠ‚è§’ï¼Œç„¶ååˆ©ç”¨æœºå™¨äººçš„URDFæ¨¡å‹å’Œæ­£å‘è¿åŠ¨å­¦åº“ï¼ˆPyTorch-Kinematicsï¼‰åœ¨GPUä¸Šå®æ—¶è®¡ç®—å¯¹åº”çš„æœ«ç«¯å§¿æ€ã€‚è¿™äº›é‡‡æ ·å§¿æ€åœ¨ç”¨äºè®¡ç®—æŸå¤±åå³è¢«ä¸¢å¼ƒã€‚",
            "timestamp": "2026-01-27T16:04:49.576560",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä½¿ç”¨äº†å››ç±»æ ¸å¿ƒæŒ‡æ ‡æ¥å®šé‡è¯„ä¼°å…¶æ–¹æ³•çš„æ€§èƒ½ï¼Œä¸»è¦é›†ä¸­åœ¨**è¿åŠ¨é‡å®šå‘è´¨é‡**å’Œ**ç›®æ ‡å¯¼å‘æ§åˆ¶ç²¾åº¦**ä¸¤ä¸ªæ–¹é¢ã€‚\n\n| æŒ‡æ ‡åç§° | ç¼©å†™ | è®¡ç®—å…¬å¼/å‚è€ƒ | è¯„ä¼°ç›®çš„ | ç»†èŠ‚è¯´æ˜ |\n| :--- | :--- | :--- | :--- | :--- |\n| **æ—‹è½¬ç›¸ä¼¼åº¦** | RS | å…¬å¼(1): `D_R(x_A, x_B) = Î£_j (1 - <q_A^j, q_B^j>^2)` | è¯„ä¼°ä¸åŒå…·èº«æ™ºèƒ½ä½“ä¹‹é—´è‚¢ä½“**æ—‹è½¬å§¿æ€**çš„ä¸€è‡´æ€§ã€‚ | - è®¡ç®—å¯¹åº”å…³èŠ‚æ—‹è½¬çš„å››å…ƒæ•°ç‚¹ç§¯ã€‚<br>- å€¼è¶Šå°ï¼Œè¡¨ç¤ºæ—‹è½¬å¯¹é½è¶Šå¥½ï¼ˆå•ä½ï¼šåº¦ï¼‰ã€‚<br>- ç”¨äºè¯„ä¼°èº¯å¹²ã€è…¿éƒ¨ç­‰å¯¹è§†è§‰ä¿çœŸåº¦è¦æ±‚é«˜çš„éƒ¨ä½ã€‚ |\n| **å½’ä¸€åŒ–è·ç¦»ç›¸ä¼¼åº¦** | NDS | å…¬å¼(2): `D_ee(x_A, x_B) = \\| p_A^ee - p_B^ee \\|_2` | è¯„ä¼°**æœ«ç«¯æ‰§è¡Œå™¨ï¼ˆEEï¼‰ä½ç½®**çš„ç©ºé—´å¯¹é½ç²¾åº¦ã€‚ | - å°†EEä½ç½®è½¬æ¢åˆ°è‚©éƒ¨åæ ‡ç³»å¹¶**æŒ‰è‡‚é•¿å½’ä¸€åŒ–**ï¼Œä»¥æ¶ˆé™¤ä¸åŒæœºå™¨äººå°ºå¯¸å’Œåæ ‡ç³»çš„å½±å“ã€‚<br>- å€¼è¶Šå°ï¼Œè¡¨ç¤ºEEè½¨è¿¹åŒ¹é…è¶Šç²¾ç¡®ã€‚<br>- å¯¹æœºæ¢°è‡‚æ“æ§ç­‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚ |\n| **å½’ä¸€åŒ–é€Ÿåº¦ç›¸ä¼¼åº¦** | NVS | å…¬å¼(8): `â„’_temporal = \\| v_H^hand - v_A^ee \\|_2` | è¯„ä¼°é‡å®šå‘è¿åŠ¨ä¸å‚è€ƒè¿åŠ¨ä¹‹é—´**æœ«ç«¯æ‰§è¡Œå™¨é€Ÿåº¦**çš„ä¸€è‡´æ€§ã€‚ | - è¡¡é‡è¿åŠ¨æ„å›¾ä¸æ‰§è¡Œåœ¨**åŠ¨æ€ç‰¹æ€§**ä¸Šçš„å¯¹åº”å…³ç³»ã€‚<br>- å€¼è¶Šå°ï¼Œè¡¨ç¤ºé€Ÿåº¦è½®å»“åŒ¹é…è¶Šå¥½ï¼Œè¿åŠ¨æ›´å¹³æ»‘è‡ªç„¶ã€‚<br>- éªŒè¯äº†æ½œåœ¨ç©ºé—´åŠ¨åŠ›å­¦çš„ä¸€è‡´æ€§ã€‚ |\n| **ç›®æ ‡è·ç¦»** | DTG | `\\| p_ee^final - p_goal \\|_2` | è¯„ä¼°**ç›®æ ‡æ¡ä»¶æ§åˆ¶ç­–ç•¥**çš„ç²¾åº¦ï¼Œå³æœ«ç«¯æ‰§è¡Œå™¨åˆ°è¾¾æŒ‡å®šç›®æ ‡ä½ç½®çš„èƒ½åŠ›ã€‚ | - è®¡ç®—æœ€ç»ˆEEä½ç½®ä¸ç”¨æˆ·æŒ‡å®šç›®æ ‡ä½ç½®ä¹‹é—´çš„æ¬§æ°è·ç¦»ã€‚<br>- å€¼è¶Šå°ï¼Œè¡¨ç¤ºæ§åˆ¶ç­–ç•¥çš„**ç›®æ ‡è¾¾æˆç²¾åº¦è¶Šé«˜**ï¼ˆå•ä½ï¼šå˜ç±³ï¼‰ã€‚<br>- ç›´æ¥ä½“ç°å®ç”¨ä»·å€¼ã€‚ |\n\n**æŠ€æœ¯åˆ›æ–°ç‚¹**ï¼šNDSå’ŒNVSé€šè¿‡**å½’ä¸€åŒ–å¤„ç†**ï¼ˆåæ ‡ç³»è½¬æ¢å’Œè‡‚é•¿ç¼©æ”¾ï¼‰å®ç°äº†è·¨ä¸åŒå°ºå¯¸ã€ä¸åŒæ„å‹æœºå™¨äººçš„å…¬å¹³æ¯”è¾ƒï¼Œè¿™æ˜¯å¤„ç†å½¢æ€å¤šæ ·æ€§é—®é¢˜çš„å…³é”®è®¾è®¡ã€‚\n\n### äºŒã€ æ•°æ®é›†\n\nè®ºæ–‡ä¸»è¦ä½¿ç”¨ä¸€ä¸ª**äººç±»è¿åŠ¨æ•°æ®é›†**ï¼Œå¹¶åˆ›æ–°æ€§åœ°é‡‡ç”¨**åˆæˆæ–¹æ³•ç”Ÿæˆæœºå™¨äººæ•°æ®**ï¼Œé¿å…äº†ç¹ççš„çœŸå®æœºå™¨äººæ•°æ®é‡‡é›†ã€‚\n\n| æ•°æ®é›†/æ•°æ®æº | ç±»å‹ | å†…å®¹ä¸è§„æ¨¡ | ä½¿ç”¨æ–¹å¼ä¸ç»†èŠ‚ |\n| :--- | :--- | :--- | :--- |\n| **HumanML3D** | äººç±»è¿åŠ¨æ•°æ®ï¼ˆä¸»è¦æ•°æ®æºï¼‰ | - åŒ…å« **29,224** ä¸ªå¤šæ ·åŒ–çš„äººç±»è¿åŠ¨åºåˆ—ã€‚<br>- æ€»è®¡è¶…è¿‡ **400ä¸‡** ä¸ªäººä½“å§¿åŠ¿ã€‚ | 1. **è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†**ï¼š80% ç”¨äºè®­ç»ƒï¼Œ20% ç”¨äºæµ‹è¯•ã€‚<br>2. **æ•°æ®æ ¼å¼**ï¼šä½¿ç”¨SMPLäººä½“æ¨¡å‹ï¼Œå…³èŠ‚ç”¨å››å…ƒæ•°è¡¨ç¤º (`n=4`)ã€‚<br>3. **å±€é™æ€§**ï¼šSMPLæ¨¡å‹æœªåŒ…å«ç²¾ç»†çš„æ‰‹éƒ¨è¿åŠ¨ï¼Œæ‰‹è¢«è§†ä¸ºå‰è‡‚çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤è®ºæ–‡æ–¹æ³•ç›®å‰ä¸æ”¯æŒéœ€è¦ç²¾ç»†æ‰‹éƒ¨æ“ä½œçš„æ¨¡ä»¿ä»»åŠ¡ã€‚ |\n| **åˆæˆæœºå™¨äººæ•°æ®** | æœºå™¨äººå§¿æ€æ•°æ®ï¼ˆç¨‹åºç”Ÿæˆï¼‰ | - è®­ç»ƒä¸­ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŠ¨æ€ç”Ÿæˆ **10^5** ä¸ªæ–°çš„æœºå™¨äººå§¿æ€ã€‚<br>- æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ç´¯è®¡é‡‡æ ·**æ•°åäº¿**ä¸ªå§¿æ€ã€‚ | 1. **ç”Ÿæˆæ–¹æ³•**ï¼šä»å„æœºå™¨äººçš„å…³èŠ‚ç©ºé—´**å‡åŒ€éšæœºé‡‡æ ·**å…³èŠ‚è§’åº¦ã€‚<br>2. **å§¿æ€è®¡ç®—**ï¼šä½¿ç”¨ **PyTorch-Kinematics** åº“ï¼Œæ ¹æ®æœºå™¨äººURDFæ–‡ä»¶è¿›è¡Œ**å‰å‘è¿åŠ¨å­¦ï¼ˆFKï¼‰** è®¡ç®—ï¼Œå¾—åˆ°æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ç­‰ã€‚<br>3. **å…³é”®ä¼˜åŠ¿**ï¼š<br>   - **æ— éœ€çœŸå®æ•°æ®æ”¶é›†**ï¼šä»…éœ€æœºå™¨äººæ¨¡å‹ï¼ˆURDFï¼‰ã€‚<br>   - **å…¨é¢æ¢ç´¢**ï¼šéšæœºé‡‡æ ·èƒ½å¹¿æ³›è¦†ç›–æœºå™¨äººçš„æ•´ä¸ªè¿åŠ¨å­¦ç©ºé—´ã€‚<br>   - **é«˜æ•ˆè®­ç»ƒ**ï¼šæ•´ä¸ªæµæ°´çº¿ï¼ˆé‡‡æ ·ã€FKã€ç½‘ç»œæ›´æ–°ï¼‰å®Œå…¨åœ¨GPUä¸Šè¿è¡Œï¼Œæ•°æ®å³ç”¨å³å¼ƒï¼Œæ— éœ€å­˜å‚¨ï¼Œæå¤§æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚ |\n\n**å®é™…ä»·å€¼ä¸åˆ›æ–°**ï¼š**å®Œå…¨ä¾èµ–åˆæˆæœºå™¨äººæ•°æ®è¿›è¡Œè®­ç»ƒ**æ˜¯æœ¬æ–‡çš„ä¸€å¤§äº®ç‚¹ã€‚å®ƒæå¤§åœ°é™ä½äº†å°†æ–¹æ³•åº”ç”¨äºæ–°æœºå™¨äººçš„é—¨æ§›å’Œæˆæœ¬ï¼Œåªéœ€æä¾›æ–°æœºå™¨äººçš„URDFæ¨¡å‹å³å¯ï¼Œè¿™ä¸ºå®ç°**é«˜åº¦å¯æ‰©å±•çš„è·¨å¹³å°æ§åˆ¶**å¥ å®šäº†åšå®åŸºç¡€ã€‚",
            "timestamp": "2026-01-27T16:05:27.509760",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ä¸¤ä¸ªä¸»è¦æ•°æ®æºä¸Šè¿›è¡Œäº†å®éªŒï¼š\n\n1.  **äººç±»è¿åŠ¨æ•°æ®**ï¼š\n    - **æ•°æ®é›†**ï¼š**HumanML3D** æ•°æ®é›†ã€‚\n    - **å†…å®¹**ï¼šåŒ…å« 29,224 ä¸ªå¤šæ ·åŒ–çš„äººç±»è¿åŠ¨åºåˆ—ï¼Œæ€»è®¡è¶…è¿‡ 400 ä¸‡ä¸ªå§¿æ€ã€‚\n    - **ç”¨é€”**ï¼šä½œä¸º**ä¸»è¦çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æº**ï¼Œç”¨äºå­¦ä¹ ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´å’Œè®­ç»ƒç›®æ ‡æ¡ä»¶æ§åˆ¶ç­–ç•¥ã€‚æŒ‰ 80%/20% çš„æ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚\n\n2.  **æœºå™¨äººå§¿æ€æ•°æ®**ï¼š\n    - **ç”Ÿæˆæ–¹å¼**ï¼š**æ— éœ€æ”¶é›†çœŸå®çš„æœºå™¨äººæ¼”ç¤ºæ•°æ®**ã€‚\n    - **æ–¹æ³•**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»å„æœºå™¨äººçš„å…³èŠ‚ç©ºé—´**å‡åŒ€éšæœºé‡‡æ ·**å…³èŠ‚é…ç½®ï¼Œå¹¶ä½¿ç”¨**å‰å‘è¿åŠ¨å­¦**ï¼ˆé€šè¿‡ PyTorch-Kinematics åº“ï¼‰å®æ—¶è®¡ç®—å¯¹åº”çš„æœºå™¨äººå§¿æ€ã€‚\n    - **ä¼˜åŠ¿**ï¼šå®ç°äº†å®Œå…¨åœ¨ GPU ä¸Šè¿è¡Œçš„è®­ç»ƒæµç¨‹ï¼Œé¿å…äº†å­˜å‚¨å¤§è§„æ¨¡æœºå™¨äººæ•°æ®é›†çš„éœ€è¦ï¼Œå¹¶é€šè¿‡æ•°åäº¿æ¬¡çš„é‡‡æ ·å®ç°äº†å¯¹æœºå™¨äººè¿åŠ¨å­¦ç©ºé—´çš„å…¨é¢æ¢ç´¢ã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡å®šä¹‰äº†å››ä¸ªæ ¸å¿ƒé‡åŒ–æŒ‡æ ‡æ¥è¯„ä¼°è¿åŠ¨é‡å®šå‘å’Œç›®æ ‡æ§åˆ¶çš„è´¨é‡ï¼š\n\n| æŒ‡æ ‡ | å…¨ç§° | è®¡ç®—æ–¹å¼ | è¯„ä¼°ç›®çš„ |\n| :--- | :--- | :--- | :--- |\n| **RS** | æ—‹è½¬ç›¸ä¼¼åº¦ | åŸºäºå…¬å¼(1)ï¼Œè®¡ç®—å¯¹åº”å…³èŠ‚ä¹‹é—´çš„å¹³å‡è§’åº¦è·ç¦»ï¼ˆå•ä½ï¼šåº¦ï¼‰ã€‚ | è¯„ä¼°ä¸åŒå…·èº«ä¹‹é—´è‚¢ä½“**æ—‹è½¬å§¿æ€**çš„ä¸€è‡´æ€§ã€‚å€¼è¶Šä½è¶Šå¥½ã€‚ |\n| **NDS** | å½’ä¸€åŒ–è·ç¦»ç›¸ä¼¼åº¦ | åŸºäºå…¬å¼(2)ï¼Œè®¡ç®—å½’ä¸€åŒ–åçš„æœ«ç«¯æ‰§è¡Œå™¨ï¼ˆæˆ–äººæ‰‹ï¼‰ä½ç½®ä¹‹é—´çš„æ¬§æ°è·ç¦»ã€‚ | è¯„ä¼°**æœ«ç«¯æ‰§è¡Œå™¨ç©ºé—´ä½ç½®**çš„å¯¹é½ç²¾åº¦ã€‚å€¼è¶Šä½è¶Šå¥½ã€‚ |\n| **NVS** | å½’ä¸€åŒ–é€Ÿåº¦ç›¸ä¼¼åº¦ | åŸºäºå…¬å¼(8)ï¼Œè®¡ç®—æœ«ç«¯æ‰§è¡Œå™¨é€Ÿåº¦å‘é‡ä¹‹é—´çš„ L2 è·ç¦»ã€‚ | è¯„ä¼°**è¿åŠ¨åŠ¨æ€**ï¼ˆæ„å›¾ä¸æ‰§è¡Œï¼‰åœ¨è·¨å…·èº«æ—¶çš„ä¸€è‡´æ€§ã€‚å€¼è¶Šä½è¶Šå¥½ã€‚ |\n| **DTG** | ç›®æ ‡è·ç¦» | æœ€ç»ˆå¸§çš„é¢„æµ‹æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ä¸ç”¨æˆ·æŒ‡å®šç›®æ ‡ä½ç½®ä¹‹é—´çš„æ¬§æ°è·ç¦»ï¼ˆå•ä½ï¼šå˜ç±³ï¼‰ã€‚ | è¯„ä¼°**ç›®æ ‡æ¡ä»¶æ§åˆ¶ç­–ç•¥**çš„**åˆ°è¾¾ç²¾åº¦**ã€‚å€¼è¶Šä½è¶Šå¥½ã€‚ |\n\n### ä¸‰ã€ æ€§èƒ½æå‡ä¸å…³é”®ç»“æœ\nè®ºæ–‡é€šè¿‡å¤šä¸ªå®éªŒéªŒè¯äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸»è¦æ€§èƒ½æå‡ä½“ç°åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š\n\n1.  **è¿åŠ¨é‡å®šå‘æ€§èƒ½ï¼ˆå¯¹æ¯”åŸºçº¿ ImitationNetï¼‰**ï¼š\n    - **å¯¹æ¯”è®¾ç½®**ï¼šImitationNet ä¸ºæ¯ä¸ªâ€œäººâ†’ç‰¹å®šæœºå™¨äººâ€é…å¯¹å•ç‹¬è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œè€Œæœ¬æ–‡æ–¹æ³•ä½¿ç”¨**å•ä¸ªç»Ÿä¸€æ¨¡å‹**å¤„ç†æ‰€æœ‰å…·èº«ã€‚\n    - **æ€§èƒ½æå‡**ï¼š\n        - åœ¨ **NDS**ï¼ˆæœ«ç«¯ä½ç½®ç²¾åº¦ï¼‰å’Œ **NVS**ï¼ˆé€Ÿåº¦ä¸€è‡´æ€§ï¼‰æŒ‡æ ‡ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•ï¼ˆå°¤å…¶æ˜¯è§£è€¦æ½œåœ¨ç©ºé—´ç‰ˆæœ¬ï¼‰**æ˜¾è‘—ä¼˜äº** ImitationNetï¼ˆè§è¡¨ Iï¼‰ã€‚\n        - ä¾‹å¦‚ï¼Œåœ¨â€œäººâ†’TIAGo++â€ä»»åŠ¡ä¸­ï¼Œæœ¬æ–‡è§£è€¦æ¨¡å‹çš„ NDS ä¸º **0.0401**ï¼Œè¿œä¼˜äº ImitationNet çš„ **0.1325**ã€‚\n    - **é‡è¦å‘ç°**ï¼šè™½ç„¶ç»Ÿä¸€æ½œåœ¨ç©ºé—´æœ€åˆå¯¼è‡´ **RS**ï¼ˆæ—‹è½¬ç²¾åº¦ï¼‰ä¸‹é™ï¼Œä½†é€šè¿‡å¼•å…¥**è§£è€¦çš„æ½œåœ¨ç©ºé—´**ï¼ˆä¸ºä¸åŒèº«ä½“éƒ¨ä½è®¾è®¡ç‹¬ç«‹å­ç©ºé—´ï¼‰ï¼Œæ—‹è½¬ç²¾åº¦æ¢å¤åˆ°äº†ä¸ ImitationNet ç›¸å½“ç”šè‡³æ›´å¥½çš„æ°´å¹³ï¼ŒåŒæ—¶ä¿æŒäº†æœ«ç«¯ç²¾åº¦ä¼˜åŠ¿ã€‚\n\n2.  **è·¨å…·èº«æ³›åŒ–ä¸å¯æ‰©å±•æ€§**ï¼š\n    - **æ ¸å¿ƒèƒ½åŠ›**ï¼šæœ¬æ–‡æ–¹æ³•æ”¯æŒ**ä»»æ„ä¸¤ä¸ªå…·èº«ä¹‹é—´**çš„è¿åŠ¨é‡å®šå‘ï¼ˆå¦‚æœºå™¨äººåˆ°æœºå™¨äººï¼‰ï¼Œè€Œ ImitationNet ä»…æ”¯æŒäººåˆ°ç‰¹å®šæœºå™¨äººã€‚\n    - **æ·»åŠ æ–°æœºå™¨äºº**ï¼šåœ¨æ ¸å¿ƒç½‘ç»œï¼ˆ`E_h`, `E_X`, `D_X`ï¼‰é¢„è®­ç»ƒå¥½åï¼Œæ·»åŠ æ–°æœºå™¨äººï¼ˆå¦‚ ATLAS, G1, Kinovaï¼‰ä»…éœ€è®­ç»ƒè½»é‡çº§çš„**æœºå™¨äººç‰¹å®šåµŒå…¥å±‚**ï¼ˆ`E_r`, `D_r`ï¼‰ã€‚\n    - **æ€§èƒ½è¡¨ç°**ï¼šæ–°æ·»åŠ çš„æœºå™¨äººåœ¨å„é¡¹æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†ä¸åŸå§‹æœºå™¨äººç«¯åˆ°ç«¯è®­ç»ƒ**ç›¸å½“çš„æ€§èƒ½**ï¼ˆè§è¡¨ IIï¼‰ï¼Œè®­ç»ƒæ—¶é—´ä»…éœ€çº¦15åˆ†é’Ÿï¼Œè¯æ˜äº†æ–¹æ³•çš„**é«˜æ•ˆå¯æ‰©å±•æ€§**ã€‚\n\n3.  **æ½œåœ¨ç©ºé—´æ§åˆ¶ç­–ç•¥æ€§èƒ½**ï¼š\n    - **ç­–ç•¥è®­ç»ƒ**ï¼šä»…ä½¿ç”¨äººç±»æ•°æ®è®­ç»ƒä¸€ä¸ªç›®æ ‡æ¡ä»¶ c-VAE ç­–ç•¥ã€‚\n    - **é›¶æ ·æœ¬éƒ¨ç½²**ï¼šè¯¥ç­–ç•¥å¯ç›´æ¥éƒ¨ç½²åˆ°**å¤šä¸ªä¸åŒçš„æœºå™¨äºº**ä¸Šï¼Œ**æ— éœ€ä»»ä½•å¾®è°ƒ**ã€‚\n    - **æ§åˆ¶ç²¾åº¦**ï¼šåœ¨å…­ç§æœºå™¨äººä¸Šè¿›è¡Œçš„åˆ°è¾¾ä»»åŠ¡å®éªŒä¸­ï¼Œç­–ç•¥å®ç°äº†**äºšå˜ç±³çº§**çš„ç²¾åº¦ï¼ˆDTG ä»‹äº 0.13 cm åˆ° 1.14 cm ä¹‹é—´ï¼Œè§è¡¨ IIIï¼‰ã€‚\n    - **æ§åˆ¶é¢‘ç‡**ï¼šæ¡†æ¶è®¡ç®—é«˜æ•ˆï¼Œæ”¯æŒçº¦ **100 Hz** çš„æ§åˆ¶é¢‘ç‡ã€‚\n\n4.  **æ¶ˆèå®éªŒéªŒè¯**ï¼š\n    - **å…³é”®è®¾è®¡éªŒè¯**ï¼šå¯¹æ¯”â€œè€¦åˆâ€ä¸â€œè§£è€¦â€æ½œåœ¨ç©ºé—´è®¾è®¡ï¼Œè¯æ˜è§£è€¦è®¾è®¡åœ¨ä¿æŒé«˜æœ«ç«¯ç²¾åº¦çš„åŒæ—¶ï¼Œ**æ˜¾è‘—æå‡äº†æ—‹è½¬ç²¾åº¦**ï¼ˆRSï¼‰ï¼Œæ˜¯å–å¾—æœ€ä½³ç»¼åˆæ€§èƒ½çš„å…³é”®ã€‚\n    - **è¶…å‚æ•°åˆ†æ**ï¼šé€šè¿‡è°ƒæ•´ç›¸ä¼¼åº¦åº¦é‡ä¸­çš„æƒé‡ `w` å’ŒæŸå¤±å‡½æ•°ä¸­çš„å¯¹æ¯”æŸå¤±æƒé‡ `Î»_c`ï¼Œç¡®å®šäº†èƒ½å¹³è¡¡è§†è§‰ä¿çœŸåº¦å’Œæœ«ç«¯å¯æ§æ€§çš„æœ€ä¼˜å‚æ•°ï¼ˆ`w=1.0`, `Î»_c=10`ï¼‰ã€‚\n\n### æ€»ç»“\næœ¬æ–‡æ–¹æ³•é€šè¿‡åˆ›æ–°çš„**è§£è€¦ç»Ÿä¸€æ½œåœ¨ç©ºé—´**å’Œ**ä¸¤é˜¶æ®µå­¦ä¹ æ¡†æ¶**ï¼Œåœ¨å¤šä¸ªæœºå™¨äººå¹³å°ä¸Šå®ç°äº†ï¼š\n- **æ›´ç²¾å‡†**çš„è·¨å…·èº«è¿åŠ¨é‡å®šå‘ï¼ˆå°¤å…¶åœ¨æœ«ç«¯è½¨è¿¹å’Œé€Ÿåº¦ä¸Šï¼‰ã€‚\n- **å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›**ï¼Œå•ä¸€ç­–ç•¥å¯æ§åˆ¶å¤šç§æœºå™¨äººã€‚\n- **é«˜æ•ˆçš„å¯æ‰©å±•æ€§**ï¼Œæ·»åŠ æ–°æœºå™¨äººæˆæœ¬æä½ã€‚\n- **é«˜ç²¾åº¦ã€å®æ—¶çš„æ½œåœ¨ç©ºé—´æœºå™¨äººæ§åˆ¶**ã€‚\n\nè¿™äº›æ€§èƒ½æå‡ä¸ºå®ç°**è§„æ¨¡åŒ–ã€ç»Ÿä¸€çš„å¤šæœºå™¨äººæ§åˆ¶ç³»ç»Ÿ**æä¾›äº†åšå®çš„æŠ€æœ¯åŸºç¡€ã€‚",
            "timestamp": "2026-01-27T16:06:13.639268",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„æ·±å…¥é˜…è¯»ï¼Œä»¥ä¸‹æ˜¯è¯¥æ–¹æ³•ç›®å‰å­˜åœ¨çš„ä¸»è¦å±€é™æ€§ï¼š\n\n- **ç¼ºä¹ç²¾ç»†æ‰‹éƒ¨åŠ¨ä½œçš„æ¨¡ä»¿èƒ½åŠ›**\n    - è®ºæ–‡ä½¿ç”¨çš„è®­ç»ƒæ•°æ®é›†ï¼ˆHumanML3Dï¼ŒåŸºäºSMPLäººä½“æ¨¡å‹ï¼‰**ä¸åŒ…å«ç‹¬ç«‹çš„æ‰‹éƒ¨è¿åŠ¨æ•°æ®**ã€‚åœ¨SMPLæ¨¡å‹ä¸­ï¼Œæ‰‹éƒ¨è¢«è§†ä¸ºå‰è‡‚çš„ä¸€éƒ¨åˆ†ã€‚\n    - è¿™å¯¼è‡´æ–¹æ³•**æ— æ³•å¤„ç†éœ€è¦ç²¾ç»†æ‰‹æŒ‡æ“ä½œçš„ä»»åŠ¡**ï¼Œä¾‹å¦‚æŠ“æ¡ã€æ“ä½œå°ç‰©ä½“ç­‰ï¼Œé™åˆ¶äº†å…¶åœ¨éœ€è¦é«˜ç²¾åº¦é¥æ“ä½œæˆ–çµå·§æ“æ§åœºæ™¯ä¸­çš„åº”ç”¨ã€‚\n\n- **è¿åŠ¨é‡å®šå‘çš„æ—‹è½¬ç²¾åº¦å­˜åœ¨æƒè¡¡**\n    - å¦‚è¡¨Iæ‰€ç¤ºï¼Œä¸ä¸ºæ¯ä¸ªæœºå™¨äººå•ç‹¬è®­ç»ƒæ¨¡å‹çš„åŸºçº¿æ–¹æ³•ï¼ˆImitationNetï¼‰ç›¸æ¯”ï¼Œæœ¬æ–‡çš„ç»Ÿä¸€æ½œåœ¨ç©ºé—´æ–¹æ³•åœ¨**æ—‹è½¬ç›¸ä¼¼æ€§ï¼ˆRSï¼‰æŒ‡æ ‡ä¸Šæœ‰æ‰€ä¸‹é™**ã€‚\n    - è¿™ç§ä¸‹é™æºäºä¸åŒæœºå™¨äººå½¢æ€ï¼ˆå¦‚æœ‰çš„æœ‰èº¯å¹²ï¼Œæœ‰çš„æ²¡æœ‰ï¼‰ä¹‹é—´çš„å›ºæœ‰å·®å¼‚ã€‚å°½ç®¡é€šè¿‡è§£è€¦æ½œåœ¨ç©ºé—´çš„è®¾è®¡ç¼“è§£äº†æ­¤é—®é¢˜ï¼Œä½†åœ¨ç»Ÿä¸€è¡¨ç¤ºä¸ç‰¹å®šå½¢æ€çš„æœ€ä¼˜ç²¾åº¦ä¹‹é—´ä»å­˜åœ¨å†…åœ¨å¼ åŠ›ã€‚\n\n- **ä¾èµ–åˆæˆæ•°æ®ä¸ç†æƒ³åŒ–å‡è®¾**\n    - æ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æœºå™¨äººæ•°æ®å®Œå…¨é€šè¿‡**ä»å…³èŠ‚ç©ºé—´å‡åŒ€é‡‡æ ·å¹¶åˆ©ç”¨å‰å‘è¿åŠ¨å­¦ï¼ˆFKï¼‰è®¡ç®—ä½å§¿**æ¥ç”Ÿæˆï¼Œå¹¶æœªä½¿ç”¨çœŸå®çš„æœºå™¨äººè¿åŠ¨æ•°æ®ã€‚\n    - è¿™ç§æ–¹æ³•å‡è®¾æœºå™¨äººçš„å·¥ä½œç©ºé—´å¯ä»¥è¢«å……åˆ†ã€å‡åŒ€åœ°æ¢ç´¢ï¼Œä¸”**æœªè€ƒè™‘å®é™…æœºå™¨äººçš„åŠ¨åŠ›å­¦çº¦æŸã€æ‰§è¡Œå™¨æé™ã€è‡ªç¢°æ’ä»¥åŠä¸ç¯å¢ƒäº¤äº’çš„ç‰©ç†ç‰¹æ€§**ã€‚åœ¨çœŸå®ç¡¬ä»¶ä¸Šéƒ¨ç½²æ—¶ï¼Œå¯èƒ½éœ€è¦è¿›è¡Œé¢å¤–çš„å®‰å…¨æ£€æŸ¥å’Œè½¨è¿¹ä¼˜åŒ–ã€‚\n\n- **æ§åˆ¶ç­–ç•¥çš„å•ä¸€æ€§ä¸ä»»åŠ¡å±€é™**\n    - ç›®å‰å±•ç¤ºçš„æ½œåœ¨ç©ºé—´æ§åˆ¶ç­–ç•¥**ä¸»è¦é’ˆå¯¹å•è‡‚æœ«ç«¯æ‰§è¡Œå™¨ï¼ˆEEï¼‰çš„ä½ç½®åˆ°è¾¾ä»»åŠ¡**ï¼ˆGoal-conditioned reachingï¼‰ã€‚\n    - å¯¹äºæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚**åŒæ‰‹åè°ƒã€å…¨èº«å¹³è¡¡æ§åˆ¶ã€åŠ¨æ€è¿åŠ¨æˆ–ä¸å¤æ‚ç¯å¢ƒçš„äº¤äº’**ï¼Œè®ºæ–‡å°šæœªéªŒè¯å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç­–ç•¥æ˜¯åœ¨æ½œåœ¨ç©ºé—´ä¸­é€šè¿‡é¢„æµ‹ä½ç§»çš„è‡ªå›å½’æ–¹å¼ç”Ÿæˆï¼Œå…¶é•¿æœŸç”Ÿæˆçš„ç¨³å®šæ€§å’Œå¤æ‚æ€§ä»»åŠ¡çš„æ€§èƒ½æœ‰å¾…è¿›ä¸€æ­¥æ¢ç©¶ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡åœ¨è®¨è®ºéƒ¨åˆ†ï¼ˆIII-Eï¼‰åŠåˆ†æåŸºç¡€ä¸Šï¼Œå¯å»¶ä¼¸å‡ºä»¥ä¸‹æœªæ¥å·¥ä½œæ–¹å‘ï¼š\n\n- **é›†æˆæ‰‹éƒ¨åŠ¨ä½œä¸ç²¾ç»†æ“ä½œ**\n    - **æ”¶é›†æˆ–åˆ©ç”¨ç°æœ‰çš„æ‰‹éƒ¨è¿åŠ¨æ•°æ®é›†**ï¼Œæ‰©å±•äººä½“è¡¨ç¤ºä»¥åŒ…å«ç‹¬ç«‹çš„æ‰‹éƒ¨å…³èŠ‚ã€‚\n    - åœ¨è§£è€¦æ½œåœ¨ç©ºé—´ä¸­**ä¸ºæ‰‹éƒ¨æ–°å¢ä¸“é—¨çš„å­ç©ºé—´**ï¼Œå¹¶è®¾è®¡ç›¸åº”çš„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆå¦‚æŒ‡å°–ä½ç½®ã€ç‰©ä½“ç›¸å¯¹ä½å§¿ï¼‰ã€‚\n    - è¿™å°†ä½¿å­¦ä¹ åˆ°çš„æ§åˆ¶ç­–ç•¥èƒ½å¤Ÿæ”¯æŒ**æŠ“å–ã€å·¥å…·ä½¿ç”¨ç­‰ç²¾ç»†æ“ä½œä»»åŠ¡**ï¼Œå¤§å¹…æå‡æ–¹æ³•çš„å®ç”¨æ€§ã€‚\n\n- **èåˆç‰©ç†çº¦æŸä¸å®‰å…¨ä¿è¯**\n    - åœ¨æ½œåœ¨ç©ºé—´è§£ç ä¸ºæœºå™¨äººå…³èŠ‚è½¨è¿¹åï¼Œæˆ–ç›´æ¥åœ¨æ½œåœ¨ç©ºé—´è®­ç»ƒä¸­ï¼Œ**å¼•å…¥æœºå™¨äººåŠ¨åŠ›å­¦ã€æ‰­çŸ©é™åˆ¶ã€è‡ªç¢°æ’é¿å…ç­‰çº¦æŸ**ã€‚\n    - å¯ä»¥æ¢ç´¢**å®‰å…¨å±‚ï¼ˆSafety Layerï¼‰æˆ–çº¦æŸä¼˜åŒ–**ï¼Œç¡®ä¿ç”Ÿæˆçš„åŠ¨ä½œç”¨äºçœŸå®æœºå™¨äººæ—¶æ˜¯å¯è¡Œä¸”å®‰å…¨çš„ã€‚è¿™èƒ½å‡å°‘ä»¿çœŸåˆ°å®ç‰©çš„å·®è·ï¼ˆSim2Real Gapï¼‰ã€‚\n\n- **æ‰©å±•ä»»åŠ¡èŒƒå›´ä¸å¤æ‚æŠ€èƒ½å­¦ä¹ **\n    - å°†ç›®æ ‡æ¡ä»¶æ§åˆ¶ç­–ç•¥**ä»ç®€å•çš„ç‚¹å¯¹ç‚¹åˆ°è¾¾ï¼Œæ‰©å±•åˆ°æ›´å¤æ‚çš„æŠ€èƒ½**ï¼Œå¦‚ç‰©ä½“æ“çºµåºåˆ—ã€é¿éšœè¿åŠ¨ã€å…¨èº«åè°ƒä»»åŠ¡ï¼ˆå¦‚æ¬è¿ï¼‰ã€‚\n    - ç ”ç©¶å¦‚ä½•å°†**é«˜å±‚ä»»åŠ¡è§„åˆ’ä¸æ½œåœ¨ç©ºé—´ä½å±‚æ§åˆ¶ç›¸ç»“åˆ**ï¼Œå®ç°å¤šæ­¥éª¤ä»»åŠ¡çš„æ‰§è¡Œã€‚\n\n- **æ¢ç´¢æ›´é«˜æ•ˆçš„æ•°æ®åˆ©ç”¨ä¸åœ¨çº¿é€‚åº”**\n    - ç ”ç©¶**å°‘é‡çœŸå®æœºå™¨äººç¤ºèŒƒæ•°æ®**å¦‚ä½•ç”¨äºå¯¹æ½œåœ¨ç©ºé—´æˆ–æœºå™¨äººç‰¹å®šåµŒå…¥å±‚è¿›è¡Œå¾®è°ƒï¼Œä»¥æ›´å¥½åœ°é€‚åº”ç‰¹å®šç¡¬ä»¶æˆ–ç¯å¢ƒã€‚\n    - æ¢ç´¢**åœ¨çº¿å­¦ä¹ æˆ–è‡ªé€‚åº”æœºåˆ¶**ï¼Œä½¿ç³»ç»Ÿèƒ½åœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­æ ¹æ®ä¼ æ„Ÿå™¨åé¦ˆï¼ˆå¦‚åŠ›è§‰ã€è§¦è§‰ï¼‰è°ƒæ•´æ½œåœ¨ç©ºé—´ä¸­çš„è½¨è¿¹ã€‚\n\n- **è·¨å½¢æ€æ³›åŒ–çš„è¿›ä¸€æ­¥ç†è®ºåˆ†æ**\n    - å¯¹**è§£è€¦æ½œåœ¨ç©ºé—´ä¸ºä½•èƒ½æ›´å¥½åœ°å®ç°è·¨å½¢æ€å¯¹é½**è¿›è¡Œæ›´æ·±å…¥çš„ç†è®ºåˆ†ææˆ–å¯è§£é‡Šæ€§ç ”ç©¶ã€‚\n    - æ¢ç´¢å¦‚ä½•**è‡ªåŠ¨ç¡®å®šæœ€ä¼˜çš„èº«ä½“éƒ¨ä½åˆ’åˆ†æ–¹å¼**ï¼Œæˆ–å¦‚ä½•å¤„ç†å½¢æ€å·®å¼‚æå¤§çš„æœºå™¨äººï¼ˆå¦‚éäººå½¢æœºå™¨äººï¼‰ä¹‹é—´çš„æ˜ å°„é—®é¢˜ã€‚\n\n**æ€»ç»“**ï¼šè¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå…·æœ‰é«˜åº¦åˆ›æ–°æ€§å’Œå®ç”¨ä»·å€¼çš„è·¨å½¢æ€æ§åˆ¶æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒä¼˜åŠ¿åœ¨äº**å¯æ‰©å±•æ€§å’Œç»Ÿä¸€æ€§**ã€‚å½“å‰çš„ä¸»è¦å±€é™åœ¨äº**æ•°æ®è¡¨ç¤ºçš„ç²’åº¦ï¼ˆæ— æ‰‹éƒ¨ï¼‰å’Œå¯¹ç†æƒ³åŒ–ä»¿çœŸæ•°æ®çš„ä¾èµ–**ã€‚æœªæ¥çš„å·¥ä½œè‡ªç„¶å›´ç»•**å¢å¼ºç²¾ç»†æ“ä½œèƒ½åŠ›ã€æ³¨å…¥ç‰©ç†ç°å®æ€§ã€æ‰©å±•ä»»åŠ¡å¤æ‚æ€§**è¿™ä¸‰ä¸ªæ ¸å¿ƒæ–¹å‘å±•å¼€ï¼Œä»¥æ¨åŠ¨è¯¥æ–¹æ³•ä»å®éªŒå®¤æ¼”ç¤ºèµ°å‘å¤æ‚çš„å®é™…æœºå™¨äººåº”ç”¨ã€‚",
            "timestamp": "2026-01-27T16:06:52.445832",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T19:31:17Z",
    "created_at": "2026-01-27T15:53:25.748544",
    "updated_at": "2026-01-27T15:53:25.748552",
    "recommend": 0
}