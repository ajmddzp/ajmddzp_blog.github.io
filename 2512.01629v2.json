{
    "id": "2512.01629v2",
    "title": "SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge",
    "authors": [
        "Yumeng He",
        "Ying Jiang",
        "Jiayin Lu",
        "Yin Yang",
        "Chenfanfu Jiang"
    ],
    "abstract": "å…³èŠ‚åŒ–3Dç‰©ä½“å¯¹äºå…·èº«äººå·¥æ™ºèƒ½ã€æœºå™¨äººæŠ€æœ¯å’Œäº¤äº’å¼åœºæ™¯ç†è§£è‡³å…³é‡è¦ï¼Œä½†åˆ›å»ºå¯ç”¨äºä»¿çœŸçš„èµ„äº§ä»ç„¶åŠ³åŠ¨å¯†é›†ï¼Œä¸”éœ€è¦ä¸“å®¶å¯¹éƒ¨ä»¶å±‚çº§å’Œè¿åŠ¨ç»“æ„è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬æå‡ºäº†SPARKæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»å•å¼ RGBå›¾åƒé‡å»ºç‰©ç†ä¸€è‡´ã€å…·æœ‰è¿åŠ¨å­¦éƒ¨ä»¶å±‚çº§çš„å…³èŠ‚åŒ–ç‰©ä½“ã€‚ç»™å®šè¾“å…¥å›¾åƒï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æå–ç²—ç•¥çš„URDFå‚æ•°å¹¶ç”Ÿæˆéƒ¨ä»¶çº§å‚è€ƒå›¾åƒã€‚éšåï¼Œæˆ‘ä»¬å°†éƒ¨ä»¶å›¾åƒå¼•å¯¼ä¸æ¨æ–­å‡ºçš„ç»“æ„å›¾æ•´åˆåˆ°ç”Ÿæˆå¼æ‰©æ•£å˜æ¢å™¨ä¸­ï¼Œä»¥åˆæˆå…³èŠ‚åŒ–ç‰©ä½“çš„ä¸€è‡´éƒ¨ä»¶åŠå®Œæ•´å½¢çŠ¶ã€‚ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–URDFå‚æ•°ï¼Œæˆ‘ä»¬ç»“åˆå¯å¾®åˆ†å‰å‘è¿åŠ¨å­¦ä¸å¯å¾®åˆ†æ¸²æŸ“æŠ€æœ¯ï¼Œåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å¼€åˆçŠ¶æ€ç›‘ç£ä¸‹ä¼˜åŒ–å…³èŠ‚ç±»å‹ã€è½´çº¿å’ŒåŸç‚¹ä½ç½®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSPARKèƒ½å¤Ÿè·¨å¤šç§ç±»åˆ«ç”Ÿæˆé«˜è´¨é‡ã€å¯ç›´æ¥ç”¨äºä»¿çœŸçš„å…³èŠ‚åŒ–èµ„äº§ï¼Œæ”¯æŒæœºå™¨äººæ“ä½œä¸äº¤äº’å»ºæ¨¡ç­‰ä¸‹æ¸¸åº”ç”¨ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://heyumeng.com/SPARK/index.htmlã€‚",
    "url": "https://arxiv.org/abs/2512.01629v2",
    "html_url": "https://arxiv.org/html/2512.01629v2",
    "html_content": "SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge\nYumeng He\n1,2âˆ—\nYing Jiang\n1âˆ—\nJiayin Lu\n1âˆ—\nYin Yang\n3\nChenfanfu Jiang\n1\nAbstract\nArticulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further refine the URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering to optimize joint types, axes, and origins under VLM-generated open-state supervision. Extensive experiments show that SPARK produces high-quality, simulation-ready articulated assets across diverse categories, enabling downstream applications such as robotic manipulation and interaction modeling. Project page:\nhttps://heyumeng.com/SPARK/index.html\n.\nFigure 1\n:\nSPARK\nis a novel framework that integrates VLM-guided part-level and global image guidance with diffusion transformers to produce high-quality articulated object reconstructions.\nâ€ \nâ€ \n* equal contribution.\n1\nUCLA,\n2\nUSC,\n3\nUniversity of Utah.\nheyumeng@usc.edu\n,\njiayin_lu,yingjiang,cffjiang@ucla.edu\n,\nyin.yang@utah.edu\n1\nIntroduction\nArticulated objects are ubiquitous in everyday environments, and scalable creation of high-fidelity, interactable 3D assets is becoming increasingly important for embodied AI, robotics, and scene reconstruction. However, building articulated 3D models remains labor-intensive, often requiring manual labeling and expert modeling of part objects. With the advance of powerful generative models\n[\nliu2023zero\n,\nlin2023magic3d\n]\n, it is now possible to synthesize high-quality 3D assets directly from images or text prompts. However, the generated shapes are typically fused, making them difficult to reuse for downstream manipulation, animation, and simulation. Recent progress in part-level generation\n[\nlin2025partcrafter\n,\nchen2025autopartgen\n,\ntang2024partpacker\n]\nenables the synthesis of semantically meaningful 3D parts, supporting fine-grained editing and control. Yet, despite strong geometric quality, these models often lack kinematic consistency because their part segmentation is driven purely by appearance, ignoring the underlying motion structure.\nRecently, articulated object generation methods\n[\nliu2025artgs\n,\nchen2025freeart3d\n,\nqiu2025articulate\n,\nle2024articulate\n,\ngao2025partrm\n,\ngao2025meshart\n,\nsu2025artformercontrollablegenerationdiverse\n]\nhave sought to synthesize kinematic part-level objects together with their kinematic parameters, including joint and link properties. Many approaches\n[\nliu2025artgs\n,\nchen2025freeart3d\n,\nqiu2025articulate\n,\nle2024articulate\n,\ngao2025partrm\n]\nfirst generate or retrieve a 3D mesh and then segment it into movable parts under motion or multi-state image guidance, but these methods depend on template meshes or require multiple images for reliable segmentation. Other works\n[\ngao2025meshart\n,\nguo2025kinematic\n,\nsu2025artformercontrollablegenerationdiverse\n]\ndirectly generate articulated objects using kinematic graphs as structural priors, while they require explicit kinematic parameters as input. For full URDF synthesis, both optimization-based\n[\nlu2025dreamartgeneratinginteractablearticulated\n,\nliu2025artgs\n,\ngao2025partrm\n]\nand feed-forward\n[\nli2025urdf\n,\nle2024articulate\n,\nchen2024urdformer\n,\nwu2025dipodualstateimagescontrolled\n]\nmethods have been proposed to estimate articulation parameters from multi-state images, videos, meshes, or reconstructed point clouds. To minimize user input, our goal is to reconstruct both kinematic part-level articulated objects and their corresponding URDF parameters from a single image.\nTo address these challenges, we present SPARK, a framework that reconstructs simulation-ready articulated objects at the kinematic part level and estimates complete URDF parameters from a single RGB image. Given an input image, we first leverage visionâ€“language models (VLMs) to extract coarse URDF parameters for joints and links. We then generate part-level images and construct a structure graph that encodes parentâ€“child link relationships. Conditioning on both the structure graph and the synthesized part images, SPARK produces consistent part-level geometry and assembles them into a coherent articulated mesh through multi-level hierarchical attention. Finally, to further refine URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering with a feature-injection strategy, optimizing joint attributes under the supervision of VLM-generated open-state images. To summarize, our contributions are:\nâ€¢\nWe propose a novel framework that integrates a generative diffusion transformer with VLM priors to synthesize high-quality kinematic part-level articulated objects and accurately estimate the corresponding URDF parameters.\nâ€¢\nWe introduce part-image guidance and multi-level attention to enable consistent multi-part synthesis, along with a joint optimization component that refines kinematic parameters under VLM-guided supervision.\nâ€¢\nExtensive experiments demonstrate that SPARK produces accurate and simulation-consistent articulated assets across diverse object categories.\n2\nRelated Work\n2.1\n3D Object Generation\n3D object generation aims to synthesize 3D shapes from texts, images, and point clouds, producing various representations such as triangle meshes\n[\nxiang2024structured\n,\nhunyuan3d2025hunyuan3d\n,\nchen2025ultra3defficienthighfidelity3d\n,\nwu2024unique3d\n,\nli2025triposg\n,\nzhao2024di-pcg\n]\n, analytic primitives\n[\nye2025primitiveanything\n,\nma2024parameterizestructuredifferentiabletemplate\n]\n, point clouds\n[\nromanelis2024efficientscalablepointcloud\n,\nlan2024ga\n]\n, signed distance fields (SDFs)\n[\nantic2025sdfit\n]\n, neural radiance fields (NeRFs)\n[\nErkoc_2023_ICCV\n,\nwang2023prolificdreamer\n,\n10422989\n]\n, and 3D Gaussian splats (3DGS)\n[\nRen_2024_CVPR\n,\ntang2023dreamgaussian\n,\nroessle2024l3dg\n,\nchen2024textto3dusinggaussiansplatting\n,\nyi2023gaussiandreamer\n]\n. Among these, mesh generation is especially appealing for its high fidelity and compatibility with simulation and graphics pipelines\n[\nli2025triposg\n,\nxiang2024structured\n,\nzhao2025hunyuan3d\n]\n. To this end, TripoSG\n[\nli2025triposg\n]\nand TRELLIS\n[\nxiang2024structured\n]\nemploy flow-based models to generate high-fidelity 3D shapes, while Hunyuan3D\n[\nhunyuan3d2025hunyuan3d\n]\nadopts a Diffusion Transformer (DiT) that first generates geometry and then recovers textures. However, the generated shapes are typically fused, requiring additional 3D segmentation. SAMPart3D\n[\nyang2024sampart3d\n]\nand PartDistill\n[\numam2023partdistill\n]\nadopt 2D-to-3D distillation strategies to segment 3D shapes from 2D priors, though the results are often incomplete and geometrically inconsistent in occluded regions. To further obtain complete 3D parts, HoloPart\n[\nyang2025holopart\n]\nemploys a dual-attention diffusion model to complete partial segments, while PartField\n[\npartfield2025\n]\nleverages labeled 3D supervision to learn part features extending into the object interior. Instead of generating a complete shape followed by part extraction, recent approaches directly synthesize 3D parts from image latent representations\n[\ntang2024partpacker\n,\nchen2025autopartgen\n,\nlin2025partcrafter\n,\nyang2025omnipart\n,\nliu2025partcomposer\n]\n. PartComposer\n[\nliu2025partcomposer\n]\nand OmniPart\n[\nyang2025omnipart\n]\nuse spatial bounding boxes and 2D segmented part images, respectively, to guide part-level object generation with latent flow models and diffusion models. To further enhance structural guidance, AutoPartGen\n[\nchen2025autopartgen\n]\nexploits an autoregressive latent flow transformer to generate parts sequentially. In contrast, PartCrafter\n[\nlin2025partcrafter\n]\nand DualPacker\n[\ntang2024partpacker\n]\ngenerate all parts simultaneously using a DiT with part-wise latents or a rectified-flow model with dual latent volumes. However, the resulting parts often lack kinematic awareness, leading to over- or under-segmentation. Our goal is to generate articulated objects at the kinematic-part level from a single image.\n2.2\nArticulated Object Shape Reconstruction\nArticulated object shape reconstruction focuses on recovering kinematic part-level geometry for each movable component. Prior works typically first obtain a complete 3D representation either by generating a whole 3D model\n[\nlu2025dreamartgeneratinginteractablearticulated\n,\nchen2025freeart3d\n,\ngao2025partrm\n,\nliu2025artgs\n,\nvora2025articulate\n]\nor by retrieving an existing 3D mesh\n[\nqiu2025articulate\n]\nand then extract part-level articulated structures. Among these methods, Articulate AnyMesh\n[\nqiu2025articulate\n]\nand DreamArt\n[\nlu2025dreamartgeneratinginteractablearticulated\n]\nexplicitly segment the 3D representation using segmentation models\n[\nyang2025holopart\n,\nkirillov2023segment\n]\n, making their results sensitive to segmentation quality. While FreeArt3D\n[\nchen2025freeart3d\n]\n, ArtGS\n[\nliu2025artgs\n]\n, PartRM\n[\ngao2025partrm\n]\n, and ATOP\n[\nvora2025articulate\n]\nperform implicit part decomposition guided by SDS-based optimization, articulable embeddings, multi-state Gaussian fields, and motion embeddings, respectively. However, these methods may produce incomplete parts in occluded regions when decomposing whole 3D shapes.\nOn the other hand, Kinematic Kitbashing\n[\nguo2025kinematic\n]\nretrieves articulated parts and optimizes their assembly according to structure graphs. In contrast, some methods generate part-level articulated objects and complete meshes simultaneously from structural parameters\n[\ngao2025meshart\n,\nsu2025artformercontrollablegenerationdiverse\n,\nliu2024cage\n]\nor from multi-view images\n[\nliu2023paris\n,\nshen2025gaussianart\n]\n, using feed-forward generative networks or differentiable optimization frameworks. In contrast, our method focuses on simultaneously generating complete kinematic part-level articulated objects and the composed mesh from an image by leveraging VLM priors within an end-to-end framework.\n2.3\nArticulation Parameter Estimation\nArticulation parameter estimation aims to recover an articulated objectâ€™s kinematic graph, including both link and joint information\n[\nfeatherstone2008rigid\n]\n, representable in the Unified Robot Description Format (URDF)\n[\nquigley2015programming\n]\n, Simulation Description Format (SDF)\n[\nsdformat\n]\n, etc., for downstream simulation and manipulation\n[\nTodorov2012MuJoCoAP\n,\nXiang_2020_SAPIEN\n,\ncoleman2014reducing\n]\n.\nTo estimate articulation parameters, recent methods\n[\nli2025urdf\n,\nmandi2024real2codereconstructarticulatedobjects\n,\nchen2024urdformer\n,\nwu2025dipodualstateimagescontrolled\n]\ntrain end-to-end models on URDF-annotated data to directly predict URDF parameters from multi-state visual observations.\nSpecifically, Articulate-Anything\n[\nle2024articulate\n]\nand Real2Code\n[\nmandi2024real2codereconstructarticulatedobjects\n]\nleverage multimodal large language models (MLLMs) or large language models (LLMs) to infer URDF graphs or parameters from point clouds, images, or oriented bounding boxes, while URDFormer\n[\nchen2024urdformer\n]\nand DIPO\n[\nwu2025dipodualstateimagescontrolled\n]\npredict URDF structures from images using Transformer-based architectures.\nIn contrast, optimization-based methods such as DreamArt\n[\nlu2025dreamartgeneratinginteractablearticulated\n]\nand ArticulateGS\n[\nguo2025articulatedgs\n]\nexplicitly optimize URDF joint parameters under multi-state video supervision, whereas Part\n2\nGS\n[\nyu2025part2gspartawaremodelingarticulated\n]\n, GAMMA\n[\nyu2024gammageneralizablearticulationmodeling\n]\n, and ArtGS\n[\nliu2025artgs\n]\noptimize motion fields from openâ€“close image pairs, from which the corresponding URDF parameters are subsequently recovered. Unlike methods that optimize from scratch, Articulate-Anything\n[\nle2024articulate\n]\nand DreamArt\n[\nlu2025dreamartgeneratinginteractablearticulated\n]\nfirst generate URDF-like code and then refine it using VLM-based semantic feedback and generative video supervision. Inspired by this strategy, we use a VLM-generated URDF template and refine joint parameters via differentiable forward kinematics and rendering, guided by synthesized openâ€“close image pairs.\n3\nMethod\nFigure 2\n:\nPipeline Overview.\nWe use a VLM to generate per-part reference images, predicted open-state images, and URDF templates with preliminary joint and link estimations. A Diffusion Transformer (DiT) equipped with local, global, and hierarchical attention mechanisms simultaneously synthesizes part-level and complete articulated meshes from a single image with VLM priors. We further employ a generative texture model to generate realistic textures and refine the URDF parameters using differentiable forward kinematics and differentiable rendering under the guidance of the predicted open-state images.\nGiven an input image\nI\n0\nI_{0}\n, our goal is to generate an articulated object represented by a sequence of part-level meshes\n{\nğŒ\nk\n}\nk\n=\n1\nK\n\\{\\mathbf{M}_{k}\\}_{k=1}^{K}\nthat together form a composite whole mesh\nğŒ\n\\mathbf{M}\n, and to estimate its hierarchical URDF parameters\nğ®\n=\n{\nğ®\nâ„“\n,\nğ®\nj\n}\n\\mathbf{u}=\\{\\mathbf{u}_{\\ell},\\mathbf{u}_{j}\\}\n.\nHere,\nğ®\nâ„“\n\\mathbf{u}_{\\ell}\nrepresents the link nodes composed of the part meshes, while\nğ®\nj\n\\mathbf{u}_{j}\nencodes the kinematic information, including the joint type\nğ®\nj\ntype\n{\\mathbf{u}_{j}^{\\text{type}}}\n, joint axis\nğ®\nj\naxis\n{\\mathbf{u}_{j}^{\\text{axis}}}\n, joint origin\nğ®\nj\norigin\n{\\mathbf{u}_{j}^{\\text{origin}}}\n, and joint limits\nğ®\nj\nlimit\n{\\mathbf{u}_{j}^{\\text{limit}}}\n.\nTo achieve this, we introduce\nSPARK\n, a novel framework that integrates DiT with VLM priors and a differentiable optimization module.\nStarting from the input image\nI\n0\nI_{0}\n, the VLM-guided parsing stage first produces coarse URDF parameters\nğ®\n\\mathbf{u}\n, a set of per-part reference images\n{\nğ«\nk\n}\nk\n=\n1\nK\n\\{\\mathbf{r}_{k}\\}_{k=1}^{K}\n, and a predicted open-state image\nI\nopen\nI_{\\text{open}}\n.\nThese part images, together with the global input image and the inferred semantic link hierarchy, are integrated with a DiT to predict the 3D part-level meshes\n{\nğŒ\nk\n}\nk\n=\n1\nK\n\\{\\mathbf{M}_{k}\\}_{k=1}^{K}\nand the complete articulated object\nğŒ\n\\mathbf{M}\n, followed by texture generation. Based on the generated articulated objects, we further refine the joint parameters\nğ®\nâ€‹\nj\n\\mathbf{u}j\nusing differentiable forward kinematics combined with differentiable rendering and a feature-injection strategy, supervised by the predicted open-state image\nI\nâ€‹\nopen\nI{\\text{open}}\n.\n3.1\nVLM-guided Structural Reasoning\nIn this section, we describe how to extract part images\n{\nğ«\nk\n}\nk\n=\n1\nK\n\\{\\mathbf{r}_{k}\\}_{k=1}^{K}\n, and coarse URDF parameters\nğ®\n\\mathbf{u}\n, including link information\nğ®\nâ„“\n\\mathbf{u}_{\\ell}\nand joint attributes\nğ®\nj\ntype\n{\\mathbf{u}_{j}^{\\text{type}}}\n,\nğ®\nj\naxis\n{\\mathbf{u}_{j}^{\\text{axis}}}\n,\nğ®\nj\norigin\n{\\mathbf{u}_{j}^{\\text{origin}}}\n, and\nğ®\nj\nlimit\n{\\mathbf{u}_{j}^{\\text{limit}}}\n, from a single input image\nI\n0\nI_{0}\nthrough a VLM-guided parsing process.\nVLM-guided URDF Parameter Generation.\nThe VLM first infers the part hierarchy, including the number and connectivity of links and joints, contributing to a URDF template consistent with the predicted structure. The resulting template instantiates the standard URDF schema by declaring links\nğ®\nâ„“\n\\mathbf{u}_{\\ell}\n, parentâ€“child relations, and joint specifications\nğ®\nj\n=\n{\nğ®\nj\ntype\n,\nğ®\nj\naxis\n,\nğ®\nj\norigin\n,\nğ®\nj\nlimit\n}\n\\mathbf{u}_{j}=\\{{\\mathbf{u}_{j}^{\\text{type}}},{\\mathbf{u}_{j}^{\\text{axis}}},{\\mathbf{u}_{j}^{\\text{origin}}},{\\mathbf{u}_{j}^{\\text{limit}}}\\}\n. For link nodes\nğ®\nâ„“\n\\mathbf{u}_{\\ell}\n, the VLM assigns semantic identifiers or names, which are later used to associate each link with its corresponding part mesh. For joint parameters\nğ®\nj\n\\mathbf{u}_{j}\n, we categorize the attributes into discrete and continuous types. The discrete attributes are selected by the VLM from predefined dictionaries to ensure semantic and directional consistency. For instance,\nğ®\nj\ntype\nâˆˆ\n{\nfixed\n,\nrevolute\n,\nprismatic\n}\n{\\mathbf{u}_{j}^{\\text{type}}}\\in\\{\\textit{fixed},\\textit{revolute},\\textit{prismatic}\\}\ndefines the joint category, whereas\nğ®\nj\naxis\n{\\mathbf{u}_{j}^{\\text{axis}}}\nis selected from a canonical set of unit directions: front\n(\n0\n,\n0\n,\n1\n)\n(0,0,1)\n, back\n(\n0\n,\n0\n,\nâˆ’\n1\n)\n(0,0,-1)\n, up\n(\n0\n,\n1\n,\n0\n)\n(0,1,0)\n, down\n(\n0\n,\nâˆ’\n1\n,\n0\n)\n(0,-1,0)\n, right\n(\n1\n,\n0\n,\n0\n)\n(1,0,0)\n, and left\n(\nâˆ’\n1\n,\n0\n,\n0\n)\n(-1,0,0)\n. For prismatic joints, the mapping directly corresponds to these translational directions. For instance, if a drawer is expected to be pulled forward, we assign\n(\n0\n,\n0\n,\n1\n)\n(0,0,1)\n. For revolute joints, the mapping is determined by first identifying the rotation axis (among\nx\nx\n,\ny\ny\n, or\nz\nz\n) and then determining its sign. We follow the standard convention in which clockwise rotation is negative and counterclockwise rotation is positive. For example, if a door opens toward the left, it rotates around the\ny\ny\n-axis in a clockwise direction, which we define as\n(\n0\n,\nâˆ’\n1\n,\n0\n)\n(0,-1,0)\n. Another discrete attribute is the joint motion limit\nğ®\nj\nlimit\n{\\mathbf{u}_{j}^{\\text{limit}}}\n, which consists of two values:\nlower\nand\nupper\n. The VLM estimates these limits, where the lower bound is always set to 0. For the upper bound, we use predefined values that differ between large-range motions (e.g., doors or drawers) and small-range motions (e.g., buttons). The continuous attributes, including the joint origin\nğ®\nj\norigin\n{\\mathbf{u}_{j}^{\\text{origin}}}\nand joint motion limits\nğ®\nj\nlimit\n{\\mathbf{u}_{j}^{\\text{limit}}}\n, are coarsely estimated by the VLM as initial positions and thresholds.\nPart Image and Structural Guidance.\nAfter obtaining the coarse URDF parameters, we extract the part count\nK\nK\nand the semantic category of each link (e.g., drawer, door, frame). Based on these semantic labels, we generate a set of per-part reference images\n{\nğ«\nk\n}\nk\n=\n1\nK\n\\{\\mathbf{r}_{k}\\}_{k=1}^{K}\n, where each\nğ«\nk\n\\mathbf{r}_{k}\ncorresponds to a single semantic part. We then construct a structural graph according to the parentâ€“child relationships defined in the coarse URDF parameters. The generated part images and structural graph jointly serve as guidance for part-level generation in Sec.\n3.2\n.\n3.2\nPart-Articulated Object Generation\nGiven perâ€“part reference images\nâ„›\n=\n{\nğ«\nk\n}\nk\n=\n1\nK\n\\mathcal{R}=\\{\\mathbf{r}_{k}\\}_{k=1}^{K}\nand a single global image\nI\n0\nI_{0}\n, this stage reconstructs a set of\nK\nK\ngeometry latents, one per articulated part, that decode into a part-decomposable, simulation-ready 3D asset. We adopt a Diffusion-Transformer (DiT), itself inspired by TripoSG, and condition denoising on both local (per-part) and global (whole-object) visual evidence.\nMulti-Level Attention Mechanisms.\nWe begin by extracting visual guidance for each part. A shared DINOv2 encoder\n[\noquab2023dinov2\n]\nmaps every part image\nğ«\nk\n\\mathbf{r}_{k}\nand the replicated global image\nI\n0\nI_{0}\ninto token sequences, producing local embeddings\nğ„\nk\nloc\nâˆˆ\nâ„\nL\nÃ—\nd\n\\mathbf{E}^{\\mathrm{loc}}_{k}\\!\\in\\!\\mathbb{R}^{L\\times d}\nand a global embedding\nğ„\nglob\nâˆˆ\nâ„\nL\nÃ—\nd\n\\mathbf{E}^{\\mathrm{glob}}\\!\\in\\!\\mathbb{R}^{L\\times d}\n. These embeddings provide part-specific and object-level conditioning for the generative process. To integrate these image features into 3D reconstruction, the DiT maintains its own learnable latent tokens that represent the geometry to be generated. Let an object contain\nK\nK\nparts, each represented by\nN\nN\nlatent tokens. We stack them as\nZ\n=\n[\nZ\n1\n;\nâ€¦\n;\nZ\nK\n]\nâˆˆ\nâ„\nN\nâ€‹\nK\nÃ—\nC\nZ=[Z_{1};\\dots;Z_{K}]\\in\\mathbb{R}^{NK\\times C}\n, where\nZ\ni\nâˆˆ\nâ„\nN\nÃ—\nC\nZ_{i}\\in\\mathbb{R}^{N\\times C}\n.\nDuring denoising, the model fuses the image embeddings with the latent geometry tokens through alternating\nlocal\nand\nglobal\ncross-attention blocks. Local blocks query the corresponding part embedding\nğ„\nk\nloc\n\\mathbf{E}^{\\mathrm{loc}}_{k}\n, ensuring each part attends only to its own visual reference, while global blocks query\nğ„\nglob\n\\mathbf{E}^{\\mathrm{glob}}\nto inject whole-object context shared across parts. Within these blocks, latent-to-latent attention is computed using the local map\nğ€\ni\nlocal\n=\nsoftmax\nâ€‹\n(\nZ\ni\nâ€‹\nZ\ni\nâŠ¤\n/\nC\n)\nâˆˆ\nâ„\nN\nÃ—\nN\n\\mathbf{A}^{\\mathrm{local}}_{i}=\\mathrm{softmax}\\!\\big(Z_{i}Z_{i}^{\\top}/\\sqrt{C}\\big)\\in\\mathbb{R}^{N\\times N}\nand the global map\nğ€\nglobal\n=\nsoftmax\nâ€‹\n(\nZ\nâ€‹\nZ\nâŠ¤\n/\nC\n)\nâˆˆ\nâ„\nN\nâ€‹\nK\nÃ—\nN\nâ€‹\nK\n\\mathbf{A}^{\\mathrm{global}}=\\mathrm{softmax}\\!\\big(ZZ^{\\top}/\\sqrt{C}\\big)\\in\\mathbb{R}^{NK\\times NK}\n, where\nğ€\n\\mathbf{A}\ndenotes the attention weights that propagate information within each part and across the full object. To incorporate structural guidance from the VLM priors, we introduce a hierarchical attention mechanism that exchanges information between each parentâ€“child pair. A parent index map\nÏ€\n:\n{\n1\n,\nâ€¦\n,\nK\n}\nâ†’\n{\nâˆ’\n1\n,\n1\n,\nâ€¦\n,\nK\n}\n\\pi:\\{1,\\dots,K\\}\\!\\to\\!\\{-1,1,\\dots,K\\}\ndefines the link hierarchy;\nÏ€\nâ€‹\n(\nk\n)\n=\nâˆ’\n1\n\\pi(k)=-1\nfor root. Let\nğ’«\nâ€‹\n(\nu\n)\n\\mathcal{P}(u)\nand\nğ’\nâ€‹\n(\nu\n)\n\\mathcal{C}(u)\ndenote the token sets of the parent and child of token\nu\nu\n, respectively. Child tokens attend only to parent tokens via\nA\nu\nâ€‹\nv\nc\nâ†’\np\n=\nexp\nâ¡\n(\nZ\nu\nâ€‹\nZ\nv\nâŠ¤\n/\nC\n)\nâ€‹\n1\nâ€‹\n[\nv\nâˆˆ\nğ’«\nâ€‹\n(\nu\n)\n]\nâˆ‘\nv\nâ€²\nexp\nâ¡\n(\nZ\nu\nâ€‹\nZ\nv\nâ€²\nâŠ¤\n/\nC\n)\nâ€‹\n1\nâ€‹\n[\nv\nâ€²\nâˆˆ\nğ’«\nâ€‹\n(\nu\n)\n]\n.\nA^{c\\rightarrow p}_{uv}=\\frac{\\exp\\!\\big(Z_{u}Z_{v}^{\\top}/\\sqrt{C}\\big)\\,\\mathbf{1}[v\\in\\mathcal{P}(u)]}{\\sum_{v^{\\prime}}\\exp\\!\\big(Z_{u}Z_{v^{\\prime}}^{\\top}/\\sqrt{C}\\big)\\,\\mathbf{1}[v^{\\prime}\\in\\mathcal{P}(u)]}.\nThe updated latent tokens are then computed as\nZ\nâ€²\n=\nZ\n+\nA\nc\nâ†’\np\nâ€‹\nZ\nZ^{\\prime}=Z+A^{c\\rightarrow p}Z\n. Using the updated features\nZ\nâ€²\nZ^{\\prime}\n, parent tokens query their children via\nA\nu\nâ€‹\nv\np\nâ†’\nc\n=\nexp\nâ¡\n(\nZ\nu\nâ€‹\n(\nZ\nv\n)\nâŠ¤\n/\nC\n)\nâ€‹\n1\nâ€‹\n[\nv\nâˆˆ\nğ’\nâ€‹\n(\nu\n)\n]\nâˆ‘\nv\nâ€²\nexp\nâ¡\n(\nZ\nu\nâ€‹\n(\nZ\nv\nâ€²\n)\nâŠ¤\n/\nC\n)\nâ€‹\n1\nâ€‹\n[\nv\nâ€²\nâˆˆ\nğ’\nâ€‹\n(\nu\n)\n]\n.\nA^{p\\rightarrow c}_{uv}=\\frac{\\exp\\!\\big({Z}_{u}({Z}_{v})^{\\top}/\\sqrt{C}\\big)\\,\\mathbf{1}[v\\in\\mathcal{C}(u)]}{\\sum_{v^{\\prime}}\\exp\\!\\big({Z}_{u}({Z}_{v^{\\prime}})^{\\top}/\\sqrt{C}\\big)\\,\\mathbf{1}[v^{\\prime}\\in\\mathcal{C}(u)]}.\nWe then obtain the final representation as\nZ\nâ€²â€²\n=\nZ\nâ€²\n+\nA\np\nâ†’\nc\nâ€‹\nZ\nZ^{\\prime\\prime}=Z^{\\prime}+A^{p\\rightarrow c}Z\n.\nThis bidirectional scheme enables the transformer to incorporate local detail, global context, and structural (parentâ€“child) guidance.\nPosition Embedding.\nTo bind latent sequences to semantic parts and keep the imageâ€“part correspondence stable under shuffling, we adopt a dual scheme: (i) a learnable\npart embedding\nthat encodes each partâ€™s\nrelative\nindex within its object (0â€¦\nK\nâˆ’\n1\nK\\!-\\!1\n), and (ii) a learnable\nabsolute position embedding\nthat encodes the canonical part identity (e.g.,\nlink_0\n,\nlink_1\n, â€¦). At run time, the data loader supplies absolute indices as\nattention_kwargs[\"part_positions\"]\n; the transformer then\nadds\nboth embeddings to the hidden states before attention. This ensures that\nlink_0\nalways receives the same absolute code even if parts are shuffled for augmentation, preserving output order and enabling part-specific features.\nTraining Objective and Loss Design.\nWe train SPARK using Rectified Flow matching\n[\nliu2022rectified\n]\n. For an articulated object with\nK\nK\nparts, the VAE encoder maps each ground-truth part mesh to a latent\nz\nk\n,\n0\nâˆˆ\nâ„\nD\nz_{k,0}\\in\\mathbb{R}^{D}\n, while independent base latents\nz\nk\n,\n1\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nI\n)\nz_{k,1}\\sim\\mathcal{N}(0,I)\nare sampled. A shared timestep\nt\nâˆˆ\n(\n0\n,\n1\n)\nt\\in(0,1)\nis drawn for the whole object, and the rectified interpolation is defined as\nx\nk\nâ€‹\n(\nt\n)\n=\n(\n1\nâˆ’\nt\n)\nâ€‹\nz\nk\n,\n0\n+\nt\nâ€‹\nz\nk\n,\n1\nx_{k}(t)=(1-t)z_{k,0}+tz_{k,1}\n, forming the stacked latent\nX\nt\n=\n(\n1\nâˆ’\nt\n)\nâ€‹\nZ\n0\n+\nt\nâ€‹\nZ\n1\nX_{t}=(1-t)Z_{0}+tZ_{1}\n. The DiT is conditioned on the global image embedding\nc\nglobal\nc^{\\mathrm{global}}\n, part image embeddings\n{\nc\nk\npart\n}\n\\{c^{\\mathrm{part}}_{k}\\}\n, and absolute indices\n{\np\nk\n}\n\\{p_{k}\\}\n, ensuring stable inputâ€“output alignment. The target velocity field is time-invariant,\nU\nâ‹†\n=\nZ\n0\nâˆ’\nZ\n1\nU^{\\star}=Z_{0}-Z_{1}\n, and the network predicts\nV\nÎ¸\nâ€‹\n(\nX\nt\n,\nC\n,\nt\n)\nV_{\\theta}(X_{t},C,t)\nacross all parts. With per-part weights\nÎ±\nk\n\\alpha_{k}\n, timestep density\nÏ\nâ€‹\n(\nt\n)\n\\rho(t)\n, and reweighting\nw\nâ€‹\n(\nt\n)\nw(t)\n, the Rectified Flow loss becomes\nâ„’\nRF\n=\nğ”¼\nâ€‹\n[\nw\nâ€‹\n(\nt\n)\nâ€‹\nâˆ‘\nk\n=\n1\nK\nÎ±\nk\nâ€‹\nâ€–\nv\nÎ¸\nâ€‹\n(\nx\nk\nâ€‹\n(\nt\n)\n,\nC\n,\nt\n)\nâˆ’\nu\nk\nâ‹†\nâ€–\n2\n2\n]\n=\nğ”¼\nâ€‹\n[\nw\nâ€‹\n(\nt\n)\nâ€‹\nâ€–\nV\nÎ¸\nâ€‹\n(\nX\nt\n,\nC\n,\nt\n)\nâˆ’\nU\nâ‹†\nâ€–\nF\n2\n]\n\\mathcal{L}_{\\mathrm{RF}}=\\mathbb{E}[\\,w(t)\\sum_{k=1}^{K}\\alpha_{k}\\|v_{\\theta}(x_{k}(t),C,t)-u_{k}^{\\star}\\|_{2}^{2}\\,]=\\mathbb{E}[\\,w(t)\\|V_{\\theta}(X_{t},C,t)-U^{\\star}\\|_{F}^{2}\\,]\n.\nFinally, absolute\npart_positions\n[\n0\n,\nâ€¦\n,\nK\nâˆ’\n1\n]\n[0,\\dots,K{-}1]\npreserve semantic ordering under augmentation, and the VAE decoder maps the optimized latents to per-part meshes that are assembled into the articulated object.\nTexture Generation.\nAfter obtaining the articulated mesh, we apply textures to each kinematic part using Meshy\n[\nmeshy-ai\n]\n, guided by the corresponding per-part reference images. Since the generated part-level meshes may vary in scale or position, we further employ the Iterative Closest Point (ICP) algorithm\n[\nrusinkiewicz2001efficient\n]\nto align the textured meshes consistently with the input image.\n3.3\nJoint Optimization\nGenerating URDF parameters from images or 3D shapes can be achieved with end-to-end models, but such data-driven approaches often struggle to accurately estimate joint parameters due to the lack of kinematic guidance. To address this limitation, we apply a feature-injection strategy in the VLM prediction module to refine discrete joint parameters, such as the joint axis\nğ®\nj\naxis\n\\mathbf{u}_{j}^{\\text{axis}}\nand joint type\nğ®\nj\ntype\n\\mathbf{u}_{j}^{\\text{type}}\n. More specifically, we use the coarse URDF parameters together with the input image as conditions for the VLM to re-predict the\nğ®\nj\naxis\n\\mathbf{u}_{j}^{\\text{axis}}\nand\nğ®\nj\ntype\n\\mathbf{u}_{j}^{\\text{type}}\n.\nFor continuous parameter optimization, we incorporate differentiable forward kinematics and differentiable rendering to recover accurate continuous joint parameters, including the joint origin\nğ®\nj\norigin\n\\mathbf{u}_{j}^{\\text{origin}}\nand the joint angle\nÎ”\nâ€‹\nÎ¸\nâˆˆ\nâ„\n\\Delta\\theta\\in\\mathbb{R}\n, guided by the generated open-state reference image\nI\nopen\nI_{\\text{open}}\n. Let\nğƒ\n=\n(\nÎ”\nâ€‹\nğ­\n,\nÎ”\nâ€‹\nÎ¸\n)\n\\boldsymbol{\\xi}=(\\Delta\\mathbf{t},\\,\\Delta\\theta)\ndenote the learnable joint parameters, where\nÎ”\nâ€‹\nğ­\nâˆˆ\nâ„\n3\n\\Delta\\mathbf{t}\\!\\in\\!\\mathbb{R}^{3}\nrepresents the joint origin in the parent frame and\nÎ”\nâ€‹\nÎ¸\nâˆˆ\nâ„\n\\Delta\\theta\\!\\in\\!\\mathbb{R}\nis the rotation angle about the predefined axis. These parameters define a rigid-body rotation in\nSO\nâ€‹\n(\n3\n)\n\\mathrm{SO}(3)\nthat determines the local motion of the child link. Starting from the closed-state articulated object\nM\n0\nM^{0}\n, the differentiable forward kinematics function\nG\nâ€‹\n(\nâ‹…\n)\nG(\\cdot)\nis applied to obtain the transformed object\nM\nt\n=\nG\nâ€‹\n(\nM\n0\n,\nÎ”\nâ€‹\nğ­\n,\nÎ”\nâ€‹\nÎ¸\n)\nM^{\\mathrm{t}}=G(M^{0},\\,\\Delta\\mathbf{t},\\,\\Delta\\theta)\n.\nGiven a fixed camera\nğ’\n\\mathcal{C}\n, we employ a differentiable renderer\nâ„›\n\\mathcal{R}\nto generate a soft silhouette\nI\nsil\n=\nâ„›\nâ€‹\n(\nğ’\n,\nM\nt\n)\nI_{\\text{sil}}=\\mathcal{R}(\\mathcal{C},\\,M^{\\mathrm{t}})\nfrom the transformed object\nM\nt\nM^{\\mathrm{t}}\n. To supervise the optimization, our objective is to:\nmin\nğƒ\nâ„’\ntotal\n=\nâ„’\npixel\nâ€‹\n(\nI\nsil\n,\nI\nopen\n)\n+\nâ„’\nreg\nâ€‹\n(\nğƒ\n)\n.\n\\min_{\\boldsymbol{\\xi}}\\ \\ \\mathcal{L}_{\\mathrm{total}}=\\mathcal{L}_{\\mathrm{pixel}}\\!\\left(I_{\\text{sil}},\\,I_{\\text{open}}\\right)+\\mathcal{L}_{\\mathrm{reg}}(\\boldsymbol{\\xi}).\n(1)\nHere,\nâ„’\npixel\n\\mathcal{L}_{\\mathrm{pixel}}\ndenotes a pixel-level silhouette loss that measures the discrepancy between the rendered silhouette\nI\nsil\nI_{\\text{sil}}\nand the open-state reference image\nI\nopen\nI_{\\text{open}}\n, promoting accurate region and boundary alignment. Specifically,\nâ„’\npixel\n\\mathcal{L}_{\\mathrm{pixel}}\ncombines a region loss,\nâ„’\nregion\n=\n1\nâˆ’\n2\nâ€‹\nâŸ¨\nI\nsil\n,\nI\nopen\nâŸ©\nâ€–\nI\nsil\nâ€–\n+\nâ€–\nI\nopen\nâ€–\n\\mathcal{L}_{\\mathrm{region}}=1-\\frac{2\\langle I_{\\text{sil}},I_{\\text{open}}\\rangle}{\\|I_{\\text{sil}}\\|+\\|I_{\\text{open}}\\|}\n, which emphasizes region overlap and contour consistency, and a gradient-based edge loss,\nâ„’\nedge\n=\nâ€–\n|\nâˆ‡\nI\nsil\n|\nâˆ’\n|\nâˆ‡\nI\nopen\n|\nâ€–\n\\mathcal{L}_{\\mathrm{edge}}=\\|\\,|\\nabla I_{\\text{sil}}|-|\\nabla I_{\\text{open}}|\\,\\|\n, which preserves edge sharpness and boundary similarity. Moreover, we explore the regularization term\nâ„’\nreg\n\\mathcal{L}_{\\mathrm{reg}}\nthat constrains the joint translation\nÎ”\nâ€‹\nğ­\n\\Delta\\mathbf{t}\nand rotation\nÎ”\nâ€‹\nÎ¸\n\\Delta\\theta\nto remain close to the initial position, preventing instability and unrealistic motion:\nâ„’\nreg\n=\nÎ»\nt\nâ€‹\nâ€–\nÎ”\nâ€‹\nğ­\nâ€–\n2\n2\n+\nÎ»\nÎ¸\nâ€‹\nâ€–\nÎ”\nâ€‹\nÎ¸\nâ€–\n2\n2\n.\n\\mathcal{L}_{\\mathrm{reg}}=\\lambda_{t}\\,\\|\\Delta\\mathbf{t}\\|_{2}^{2}+\\lambda_{\\theta}\\,\\|\\Delta\\theta\\|_{2}^{2}.\n(2)\nHere,\nÎ»\nt\n\\lambda_{t}\nand\nÎ»\nÎ¸\n\\lambda_{\\theta}\nare weighting coefficients that balance the regularization strength between translational and rotational offsets, ensuring numerical stability during optimization.\n3.4\nData Curation and Augmentation\nFigure 3\n:\nQualitative Comparison on Shape Reconstruction.\nWe compare our results with OmniPart\n[\nyang2025omnipart\n]\n, PartCrafter\n[\nlin2025partcrafter\n]\n, and URDFormer\n[\nchen2024urdformer\n]\n. Our method fulfills accurate, high-fidelity articulated object shape reconstruction.\nFigure 4\n:\nQualitative Comparison on URDF Estimation.\nWe compare our results with Articulate-Anything\n[\nle2024articulate\n]\n, Articulate-AnyMesh\n[\nqiu2025articulate\n]\n.\nThe closed-state results are reconstructed or retrieved meshes, while the open-state configurations are obtained through kinematic transformations using the estimated URDF parameters. Our method achieves more accurate and physically consistent URDF estimation, leading to realistic articulation behavior.\nSPARK is trained on the open-source PartNet-Mobility dataset\n[\nXiang_2020_SAPIEN\n]\n, which contains\n2\n,\n347\n2,347\narticulated objects across\n46\n46\ncategories in a consistent format. This dataset provides paired meshes and rendered images, but only in a single canonical articulation state. In contrast, general 3D generation datasets such as Objaverse\n[\nobjaverse\n]\ninclude objects captured in diverse articulation statesâ€”for example, drawers that are partially opened or laptops at various folding angles. In addition, we observe that some assets are over-segmented, where a single kinematic link is divided into multiple mesh pieces. To improve data quality and articulation diversity, we perform targeted data curation and augmentation. Specifically, we merge over-segmented meshes according to the URDF link associations provided in the annotations to obtain a single consolidated mesh per link. Moreover, to increase diversity, we augment articulation states by sampling joint configurations across feasible motion ranges, generating multiple canonical poses per object and capturing realistic articulation variations (e.g., partially opened drawers or unfolded laptops).\n4\nExperiments\nImplementation Details.\nWe implement the differentiable forward kinematics module in PyTorch\n[\npaszke2019pytorch\n]\nand use PyTorch3D\n[\nravi2020accelerating\n]\nfor differentiable rendering. For VLM-guided structural reasoning, we employ GPT-4o to extract part labels and joint metadata from the input image, and use Gemini 2.5 Flash Image (Nano Banana) to synthesize both the part images and the open-state image. Our training data are built upon the PartNet-Mobility dataset\n[\nXiang_2020_SAPIEN\n]\n, augmented with three articulation states: open, closed, and half-open. To ensure high-quality samples, we repair non-watertight meshes, which would otherwise introduce surface artifacts, by voxelizing each mesh and re-extracting its surface at a resolution of 200. We train our models on four NVIDIA H100 GPUs with a batch size of 48 and a learning rate of\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\nfor 1,000 epochs, requiring around 60 hours.\nBaseline.\nWe divide our evaluation into articulated object shape reconstruction and URDF estimation. For shape reconstruction, we compare against state-of-the-art part-aware generation methods, including PartCrafter\n[\nlin2025partcrafter\n]\nand OmniPart\n[\nyang2025omnipart\n]\n, which generate 3D objects with functionally components, as well as URDFormer\n[\nchen2024urdformer\n]\n, which assembles articulated objects using structural guidance. For URDF estimation, we benchmark against Articulate Anything\n[\nle2024articulate\n]\nand Articulate Anymesh\n[\nqiu2025articulate\n]\n, aiming to recover URDF from a single image and a 3D mesh, respectively.\nTable 1\n:\nQuantitative Shape Reconstruction Comparison.\nWe report F-score to measure reconstruction accuracy, Chamfer Distance (CD) for geometric fidelity.\nMethods\nCD\nâ†“\n\\downarrow\nF-Score@0.1\nâ†‘\n\\uparrow\nF-Score@0.5\nâ†‘\n\\uparrow\nPartCrafter\n[\nlin2025partcrafter\n]\n\\cellcolor\nPalette40.4342\n\\cellcolor\nPalette40.3600\n\\cellcolor\nPalette40.8840\nOmniPart\n[\nyang2025omnipart\n]\n0.4971\n0.1928\n0.8469\nURDFormer\n[\nchen2024urdformer\n]\n1.0556\n0.0438\n0.1762\nOurs\n\\cellcolor\nPalette50.3959\n\\cellcolor\nPalette50.4214\n\\cellcolor\nPalette50.8934\n4.1\nQuantitative Evaluation\nTable 2\n:\nQuantitative URDF Parameter Estimation Comparison.\nWe evaluate articulated object URDF parameter estimation using AxisErr, PivotErr, and TypeErr, which measure joint axis deviation, joint pivot offset, and joint type misclassification.\nMethods\nAxisErr\nâ†“\n\\downarrow\nPivotErr\nâ†“\n\\downarrow\nTypeErr\nâ†“\n\\downarrow\nArticulate-Anything\n[\nle2024articulate\n]\n\\cellcolor\nPalette40.5491\n\\cellcolor\nPalette40.3529\n\\cellcolor\nPalette40.2500\nArticulate Anymesh\n[\nqiu2025articulate\n]\n1.1834\n0.9162\n0.7000\nOurs\n\\cellcolor\nPalette50.1577\n\\cellcolor\nPalette50.1653\n\\cellcolor\nPalette50.0500\nTo evaluate reconstruction quality, we use a test set of 50 images covering 25 articulated object categories from GAPartNet\n[\ngeng2023gapartnet\n]\n. For each method, we reconstruct meshes from the input images and compare them with the ground-truth geometry. Following prior work\n[\nlin2025partcrafter\n,\nyang2025omnipart\n]\n, we adopt two standard metrics: Chamfer Distance for geometric fidelity and F-Score for point-level alignment. We report F-Score@0.1 (strict matching) and F-Score@0.5 (coarse matching). For URDF parameter estimation, we compare against Articulate-Anything\n[\nle2024articulate\n]\nand Articulate AnyMesh\n[\nqiu2025articulate\n]\n, which predict joint axes, pivots, and type from images and meshes, respectively. Following prior work in articulated object understanding\n[\nchen2025freeart3d\n]\n, we quantify accuracy using AxisErr (angular deviation), PivotErr (positional error), and TypeErr (type classification).\nArticulated Object Shape Reconstruction.\nAs shown in Tab.\n1\n, our method outperforms all baselines across both metrics. Although PartCrafter and OmniPart achieve comparable performance at the loose threshold (F-Score@0.5), our approach significantly surpasses them at the strict threshold (F-Score@0.1), demonstrating superior fine-grained geometric recovery. URDFormer underperforms in both F-Score and Chamfer Distance because its bounding-boxâ€“driven template retrieval strategy can misidentify part types, leading to suboptimal geometric alignment and reduced reconstruction accuracy. In contrast, our method leverages part-level image and structure guidance from VLM priors, enabling accurate geometry generation with consistent topology and improved visual quality.\nURDF Parameter Estimation Comparison.\nAs shown in Tab.\n2\n, our method achieves substantial improvements in URDF parameter estimation across all metrics. The performance gap is particularly pronounced for continuous parameters, where our optimization component enables much more accurate recovery of joint axes and joint origins compared with Articulate-Anything and Articulate AnyMesh. Articulate-Anything relies on vision-language program synthesis without any geometry-based refinement, causing its predicted joint axes and pivots to deviate significantly from the true kinematic configuration. Articulate AnyMesh, in turn, depends heavily on noisy connecting-area geometry and heuristic hinge-point selection, which makes its joint axis and origin estimates sensitive to segmentation errors and mesh artifacts, as indicated in Fig.\n4\n.\n4.2\nQualitative Evaluation\nFig.\n3\npresents qualitative comparisons with baseline methods on the same test set described in Section\n4.1\n. PartCrafter often produces floating or disconnected components and may miss key geometric elements, such as table drawers or cabinet top surfaces. URDFormer, which relies on template retrieval, can generate mismatched part types, for instance, producing small cabinet-like ledges in reconstructed tables. OmniPart first segments the image and then reconstructs each part from its bounding box, making it highly sensitive to segmentation noise and box misalignment; as a result, it often yields distorted shapes in occluded or unseen regions, such as cabinet interiors or doors.\nTable 3\n:\nMesh Reconstruction Ablation.\nAblation on mesh reconstruction quality.\nVariants\nCD\nâ†“\n\\downarrow\nF-Score@0.1\nâ†‘\n\\uparrow\nF-Score@0.5\nâ†‘\n\\uparrow\nw/o Part Guidance\n0.4284\n0.3755\n0.8725\nw/o Data Aug.\n0.4200\n0.3675\n0.8883\nOurs\n\\cellcolor\nPalette50.3959\n\\cellcolor\nPalette50.4214\n\\cellcolor\nPalette50.8934\nTable 4\n:\nURDF Parameter Estimation Ablation.\nWe evaluate our joint optimization using the same metrics to assess URDF parameter estimation accuracy.\nVariants\nAxisErr\nâ†“\n\\downarrow\nPivotErr\nâ†“\n\\downarrow\nTypeErr\nâ†“\n\\downarrow\nw/o Joint Optimization\n0.3148\n0.2388\n0.2000\nOurs\n\\cellcolor\nPalette50.1577\n\\cellcolor\nPalette50.1653\n\\cellcolor\nPalette50.0500\nFor URDF parameter estimation, retrieval-based methods such as Articulate-Anything and Articulate AnyMesh start by retrieving a closed-state object with a similar overall shape. While this can produce roughly plausible geometry, it frequently retrieves incorrect part types, such as mismatched drawers in Fig.\n4\n. These methods also apply predicted URDF parameters directly to segmented parts, making them vulnerable to segmentation errors that lead to incorrect kinematic motions. For example, in the fridge case, Articulate-Anything misidentifies the side surface as the door, causing inconsistent opening motions, while Articulate AnyMesh may confuse the outer drawer surface with the internal drawer. In contrast, our optimization component yields substantially more accurate URDF parameters and coherent, physically plausible articulated motions. Additional in-the-wild results are provided in Fig.\n5\n.\nFigure 5\n:\nIn-the-wild image results.\nAdditional examples of shape reconstruction and open-state prediction on in-the-wild images.\n4.3\nAblation Study\nWe conduct an ablation study on the key components of our framework: part guidance, data augmentation, and joint optimization, to evaluate their contributions to mesh reconstruction and URDF parameter estimation. VLM-driven part guidance provides strong visual and structural priors. As shown in Fig.\n6\n, removing part guidance leads to missing doors in the cabinet. Without data augmentation, the model overfits to limited states and may confuse visually similar parts (e.g., misidentifying two cabinet doors). Incorporating both VLM-derived guidance and data augmentation yields cleaner part boundaries, more consistent decomposition, and higher-quality reconstructions. As reported in Table\n3\n, these components together achieve the best geometric accuracy. Joint optimization further refines URDF parameters, improving joint-type prediction, joint-axis orientation, and joint pivot estimation (Table\n4\n). Removing optimization leads to misaligned or drifting moving parts, such as shifted pivot points (Fig.\n6\n).\nFigure 6\n:\nAblation.\nWe conduct an ablation study on data augmentation, part guidance, and joint optimization.\n4.4\nApplications\nOur method generates high-quality articulated objects with accurate geometry and URDF parameters, enabling robot learning of tasks such as door opening and drawer pulling. We demonstrate this through a robot drawer-opening task on the generated drawer (Fig.\n7\n).\nFigure 7\n:\nRobot Learning.\nWe use the synthesized drawer to train a robot in Isaac Sim\n[\nmakoviychuk2021isaac\n]\nto open a drawer.\n5\nConclusion\nWe presented\nSPARK\n, a novel framework for reconstructing simulation-ready articulated objects from a single image, guided by VLM-derived part images and structural cues. By combining visionâ€“language priors, a diffusion transformer with hierarchical attention, and a differentiable joint optimization module, SPARK recovers high-quality part-level geometry and accurate articulation parameters. In future work, we plan to extend SPARK to more complex kinematic structures beyond simple revolute and prismatic joints, including multi-DOF joints, compound mechanisms, and closed-chain linkages commonly found in real-world appliances and tools.\nAppendix\nAppendix A\nNetwork Architecture Details\nIn this section, we provide additional details of the network backbone used by SPARK. Our model follows a part-aware Diffusion Transformer (DiT) design with dual-image conditioning and hierarchical attention over kinematic links, built on top of a Variational Autoencoder (VAE).\nA.1\nLatent Representation and Image Encoders\nGiven an articulated object with\nK\nK\nsemantic parts, we represent each part\nk\nk\nby a point-based surface encoding and a reference image:\nâ€¢\nPart surfaces\n. Each ground-truth part mesh is sampled into\nP\nP\noriented surface points, yielding\nğ’\nk\nâˆˆ\nâ„\nP\nÃ—\n6\n\\mathbf{S}_{k}\\in\\mathbb{R}^{P\\times 6}\n(3D position + normal). A shared VAE encoder maps\nğ’\nk\n\\mathbf{S}_{k}\nto a latent sequence\nğ³\nk\n,\n0\nâˆˆ\nâ„\nT\nÃ—\nD\nlat\n,\n\\mathbf{z}_{k,0}\\in\\mathbb{R}^{T\\times D_{\\text{lat}}},\nwhere\nT\nT\nis the number of latent tokens per part and\nD\nlat\nD_{\\text{lat}}\nis the latent dimension.\nâ€¢\nPart and global images\n. For conditioning, we use a per-part reference image\nr\nk\nr_{k}\nand a single global input image\nI\n0\nI_{0}\n. The global image is replicated\nK\nK\ntimes so each part sees the same holistic context.\nBoth\nr\nk\nr_{k}\nand\nI\n0\nI_{0}\nare encoded by a shared DINOv2 image encoder into token sequences\nE\nk\nloc\nâˆˆ\nâ„\nL\nÃ—\nd\nimg\n,\nE\nglob\nâˆˆ\nâ„\nL\nÃ—\nd\nimg\n,\nE^{\\text{loc}}_{k}\\in\\mathbb{R}^{L\\times d_{\\text{img}}},\\qquad E^{\\text{glob}}\\in\\mathbb{R}^{L\\times d_{\\text{img}}},\nwhere\nL\nL\nis the number of visual tokens and\nd\nimg\nd_{\\text{img}}\nis the image feature dimension. The global embedding\nE\nglob\nE^{\\text{glob}}\nis broadcast across parts, while\nE\nk\nloc\nE^{\\text{loc}}_{k}\ncarries part-specific appearance cues.\nWe stack the\nK\nK\nper-part latent sequences along the batch axis to obtain\nğ™\n0\n=\n[\nğ³\n1\n,\n0\nâ‹®\nğ³\nK\n,\n0\n]\nâˆˆ\nâ„\n(\nK\nâ€‹\nT\n)\nÃ—\nD\nlat\n,\n\\mathbf{Z}_{0}=\\begin{bmatrix}\\mathbf{z}_{1,0}\\\\\n\\vdots\\\\\n\\mathbf{z}_{K,0}\\end{bmatrix}\\in\\mathbb{R}^{(KT)\\times D_{\\text{lat}}},\nwhich is the clean latent representation used in the diffusion process.\nA.2\nDiffusion Transformer Backbone\nThe generative backbone is a DiT with\nL\nDiT\nL_{\\text{DiT}}\ntransformer blocks and hidden dimension\nD\nD\n(we use\nD\nâ‰«\nD\nlat\nD\\gg D_{\\text{lat}}\n). Before entering the transformer, we project the VAE latents and inject a timestep token:\nğ‡\n\\displaystyle\\mathbf{H}\n=\nProj\nâ€‹\n(\nğ™\nt\n)\nâˆˆ\nâ„\n(\nK\nâ€‹\nT\n)\nÃ—\nD\n,\n\\displaystyle=\\mathrm{Proj}(\\mathbf{Z}_{t})\\in\\mathbb{R}^{(KT)\\times D},\n(3)\nğ¡\ntime\n\\displaystyle\\mathbf{h}_{\\text{time}}\n=\nÏ•\ntime\nâ€‹\n(\nt\n)\nâˆˆ\nâ„\n1\nÃ—\nD\n,\n\\displaystyle=\\phi_{\\text{time}}(t)\\in\\mathbb{R}^{1\\times D},\n(4)\nğ‡\n~\n\\displaystyle\\tilde{\\mathbf{H}}\n=\n[\nğ¡\ntime\n;\nğ‡\n]\nâˆˆ\nâ„\n(\nK\nâ€‹\nT\n+\n1\n)\nÃ—\nD\n,\n\\displaystyle=\\big[\\,\\mathbf{h}_{\\text{time}};\\mathbf{H}\\,\\big]\\in\\mathbb{R}^{(KT+1)\\times D},\n(5)\nwhere\nğ™\nt\n\\mathbf{Z}_{t}\nis the noisy latent at time\nt\nt\n(defined in Sec.\nA.5\n),\nÏ•\ntime\n\\phi_{\\text{time}}\nis a sinusoidal+MLP timestep embedding, and\n[\nâ‹…\n;\nâ‹…\n]\n[\\,\\cdot;\\cdot\\,]\ndenotes concatenation along the token dimension.\nPart and position embeddings.\nTo make the model part-aware and robust to part shuffling, we add two learnable embeddings per part:\nâ€¢\nA\nrelative part embedding\ne\nrel\nâ€‹\n(\nk\n)\ne^{\\text{rel}}(k)\nthat encodes the index of part\nk\nk\nwithin its object,\nk\nâˆˆ\n{\n0\n,\nâ€¦\n,\nK\nâˆ’\n1\n}\nk\\in\\{0,\\dots,K{-}1\\}\n.\nâ€¢\nAn\nabsolute position embedding\ne\nabs\nâ€‹\n(\np\nk\n)\ne^{\\text{abs}}(p_{k})\nthat encodes the canonical identity of the corresponding link (e.g.,\nlink_0\n,\nlink_1\n, â€¦).\nDuring data loading, we maintain an array of absolute indices\np\nk\np_{k}\nthat is preserved under shuffling, so each semantic link always receives a consistent absolute embedding even when the parts are randomly permuted for augmentation. For each token that belongs to part\nk\nk\n, we add both embeddings to the hidden state:\nğ‡\n~\nâ†\nğ‡\n~\n+\nBroadcast\nâ€‹\n(\ne\nrel\nâ€‹\n(\nk\n)\n)\n+\nBroadcast\nâ€‹\n(\ne\nabs\nâ€‹\n(\np\nk\n)\n)\n,\n\\tilde{\\mathbf{H}}\\leftarrow\\tilde{\\mathbf{H}}+\\mathrm{Broadcast}\\big(e^{\\text{rel}}(k)\\big)+\\mathrm{Broadcast}\\big(e^{\\text{abs}}(p_{k})\\big),\n(6)\nwhere\nBroadcast\nâ€‹\n(\nâ‹…\n)\n\\mathrm{Broadcast}(\\cdot)\nexpands a per-part vector to all tokens of that part.\nA.3\nMulti-Level Attention and Image Conditioning\nWe interleave three types of attention to combine part-level geometry, local appearance, and global context:\nSelf-attention.\nEach transformer block first applies self-attention with rotary positional encoding over the latent tokens:\nğ€\nself\n=\nsoftmax\nâ€‹\n(\nğğŠ\nâŠ¤\n/\nD\n)\n,\nğ‡\nâ€²\n=\nğ‡\n+\nğ€\nself\nâ€‹\nğ•\n,\n\\mathbf{A}^{\\text{self}}=\\mathrm{softmax}\\big(\\mathbf{Q}\\mathbf{K}^{\\top}/\\sqrt{D}\\big),\\qquad\\mathbf{H}^{\\prime}=\\mathbf{H}+\\mathbf{A}^{\\text{self}}\\mathbf{V},\n(7)\nwhere\nğ\n,\nğŠ\n,\nğ•\n\\mathbf{Q},\\mathbf{K},\\mathbf{V}\nare the usual query, key, and value projections of\nğ‡\n~\n\\tilde{\\mathbf{H}}\n.\nGlobal cross-attention.\nIn a subset of layers indexed by\nâ„’\nglob\n\\mathcal{L}_{\\text{glob}}\n, we use cross-attention on the global image embedding\nE\nglob\nE^{\\text{glob}}\nto inject holistic object context shared across all parts:\nğ€\nglob\n=\nsoftmax\nâ€‹\n(\nğğŠ\nglob\nâŠ¤\n/\nD\n)\n,\nğ‡\nâ€²â€²\n=\nğ‡\nâ€²\n+\nğ€\nglob\nâ€‹\nğ•\nglob\n,\n\\mathbf{A}^{\\text{glob}}=\\mathrm{softmax}\\big(\\mathbf{Q}\\mathbf{K}^{\\text{glob}\\,\\top}/\\sqrt{D}\\big),\\qquad\\mathbf{H}^{\\prime\\prime}=\\mathbf{H}^{\\prime}+\\mathbf{A}^{\\text{glob}}\\mathbf{V}^{\\text{glob}},\n(8)\nwhere\nğŠ\nglob\n,\nğ•\nglob\n\\mathbf{K}^{\\text{glob}},\\mathbf{V}^{\\text{glob}}\nare projections of\nE\nglob\nE^{\\text{glob}}\n.\nLocal cross-attention.\nIn the remaining layers, indexed by\nâ„’\nloc\n\\mathcal{L}_{\\text{loc}}\n, we instead use cross-attention to the part-specific embeddings\nE\nk\nloc\nE^{\\text{loc}}_{k}\n. Tokens belonging to part\nk\nk\nonly attend to the visual features of that part:\nğ€\nk\nloc\n=\nsoftmax\nâ€‹\n(\nğ\nk\nâ€‹\nğŠ\nk\nloc\nâŠ¤\n/\nD\n)\n,\nğ‡\nk\nâ€²â€²\n=\nğ‡\nk\nâ€²\n+\nğ€\nk\nloc\nâ€‹\nğ•\nk\nloc\n.\n\\mathbf{A}^{\\text{loc}}_{k}=\\mathrm{softmax}\\big(\\mathbf{Q}_{k}\\mathbf{K}^{\\text{loc}\\,\\top}_{k}/\\sqrt{D}\\big),\\qquad\\mathbf{H}^{\\prime\\prime}_{k}=\\mathbf{H}^{\\prime}_{k}+\\mathbf{A}^{\\text{loc}}_{k}\\mathbf{V}^{\\text{loc}}_{k}.\n(9)\nAlternating global and local layers encourages the model to respect both global shape consistency and part-level details driven by the per-part images.\nA.4\nHierarchy Attention Implementation\nTo explicitly encode the predicted kinematic tree, we adopt a hierarchy attention module operating on parentâ€“child link pairs. Let\nÏ€\n:\n{\n1\n,\nâ€¦\n,\nK\n}\nâ†’\n{\nâˆ’\n1\n,\n1\n,\nâ€¦\n,\nK\n}\n\\pi:\\{1,\\dots,K\\}\\to\\{-1,1,\\dots,K\\}\ndenote the parent index map over parts, where\nÏ€\nâ€‹\n(\nk\n)\n=\nâˆ’\n1\n\\pi(k)=-1\nfor the root. We share this map across all tokens of a part.\nWe first define the sets of parent and child tokens for a given token\nu\nu\n:\nP\nâ€‹\n(\nu\n)\n=\n{\nv\nâˆ£\ntoken\nv\nbelongs to the parent part of\nu\n}\n,\nP(u)=\\{v\\mid\\text{token $v$ belongs to the parent part of $u$}\\},\nC\nâ€‹\n(\nu\n)\n=\n{\nv\nâˆ£\ntoken\nv\nbelongs to a child part of\nu\n}\n.\nC(u)=\\{v\\mid\\text{token $v$ belongs to a child part of $u$}\\}.\nChild-to-parent attention.\nGiven latent tokens\nğ™\n\\mathbf{Z}\n, we compute a child-to-parent attention matrix\nA\nu\nâ€‹\nv\nc\nâ†’\np\n=\nexp\nâ¡\n(\nğ³\nu\nâ€‹\nğ³\nv\nâŠ¤\n/\nC\n)\nâ€‹\n1\nâ€‹\n[\nv\nâˆˆ\nP\nâ€‹\n(\nu\n)\n]\nâˆ‘\nv\nâ€²\nexp\nâ¡\n(\nğ³\nu\nâ€‹\nğ³\nv\nâ€²\nâŠ¤\n/\nC\n)\nâ€‹\n1\nâ€‹\n[\nv\nâ€²\nâˆˆ\nP\nâ€‹\n(\nu\n)\n]\n,\nA^{c\\rightarrow p}_{uv}=\\frac{\\exp\\big(\\mathbf{z}_{u}\\mathbf{z}_{v}^{\\top}/\\sqrt{C}\\big)\\,\\mathbf{1}[v\\in P(u)]}{\\sum\\limits_{v^{\\prime}}\\exp\\big(\\mathbf{z}_{u}\\mathbf{z}_{v^{\\prime}}^{\\top}/\\sqrt{C}\\big)\\,\\mathbf{1}[v^{\\prime}\\in P(u)]},\n(10)\nand update the latent tokens via\nğ™\nâ€²\n=\nğ™\n+\nA\nc\nâ†’\np\nâ€‹\nğ™\n.\n\\mathbf{Z}^{\\prime}=\\mathbf{Z}+A^{c\\rightarrow p}\\mathbf{Z}.\n(11)\nThis step lets each child part aggregate structural context from its parent.\nParent-to-child attention.\nWe then allow parents to read back from their children using\nA\nu\nâ€‹\nv\np\nâ†’\nc\n=\nexp\nâ¡\n(\nğ³\nu\nâ€‹\nğ³\nv\nâŠ¤\n/\nC\n)\nâ€‹\n1\nâ€‹\n[\nv\nâˆˆ\nC\nâ€‹\n(\nu\n)\n]\nâˆ‘\nv\nâ€²\nexp\nâ¡\n(\nğ³\nu\nâ€‹\nğ³\nv\nâ€²\nâŠ¤\n/\nC\n)\nâ€‹\n1\nâ€‹\n[\nv\nâ€²\nâˆˆ\nC\nâ€‹\n(\nu\n)\n]\n,\nA^{p\\rightarrow c}_{uv}=\\frac{\\exp\\big(\\mathbf{z}_{u}\\mathbf{z}_{v}^{\\top}/\\sqrt{C}\\big)\\,\\mathbf{1}[v\\in C(u)]}{\\sum\\limits_{v^{\\prime}}\\exp\\big(\\mathbf{z}_{u}\\mathbf{z}_{v^{\\prime}}^{\\top}/\\sqrt{C}\\big)\\,\\mathbf{1}[v^{\\prime}\\in C(u)]},\n(12)\nand obtain the final hierarchy-aware representation\nğ™\nâ€²â€²\n=\nğ™\nâ€²\n+\nA\np\nâ†’\nc\nâ€‹\nğ™\nâ€²\n.\n\\mathbf{Z}^{\\prime\\prime}=\\mathbf{Z}^{\\prime}+A^{p\\rightarrow c}\\mathbf{Z}^{\\prime}.\n(13)\nIn practice, we implement this module as a separate transformer block that takes the current hidden states and the batched parent indices as input. To avoid cross-sample contamination, we offset parent indices per object and explicitly validate that no part attends to parents outside its own object. For efficiency, we compute aggregated parent/child features using scatter-add operations rather than explicit Python loops.\nA.5\nRectified Flow Training Objective\nWe train the DiT backbone using Rectified Flow matching. For each object, we draw an independent base latent per part,\nğ³\nk\n,\n1\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nğˆ\n)\n,\n\\mathbf{z}_{k,1}\\sim\\mathcal{N}(0,\\mathbf{I}),\nand define the clean and base stacks\nğ™\n0\n=\n[\nğ³\n1\n,\n0\nâ‹®\nğ³\nK\n,\n0\n]\n,\nğ™\n1\n=\n[\nğ³\n1\n,\n1\nâ‹®\nğ³\nK\n,\n1\n]\n.\n\\mathbf{Z}_{0}=\\begin{bmatrix}\\mathbf{z}_{1,0}\\\\\n\\vdots\\\\\n\\mathbf{z}_{K,0}\\end{bmatrix},\\qquad\\mathbf{Z}_{1}=\\begin{bmatrix}\\mathbf{z}_{1,1}\\\\\n\\vdots\\\\\n\\mathbf{z}_{K,1}\\end{bmatrix}.\nA shared timestep\nt\nâˆˆ\n(\n0\n,\n1\n)\nt\\in(0,1)\nis sampled per object from a non-uniform logit-normal density\nÏ\nâ€‹\n(\nt\n)\n\\rho(t)\nthat emphasizes informative ranges. The interpolated latent is\nğ—\nt\n=\n(\n1\nâˆ’\nt\n)\nâ€‹\nğ™\n0\n+\nt\nâ€‹\nğ™\n1\n,\n\\mathbf{X}_{t}=(1-t)\\mathbf{Z}_{0}+t\\mathbf{Z}_{1},\n(14)\nand the target velocity field is time-invariant,\nğ”\nâ‹†\n=\nğ™\n0\nâˆ’\nğ™\n1\n.\n\\mathbf{U}^{\\star}=\\mathbf{Z}_{0}-\\mathbf{Z}_{1}.\n(15)\nLet\nC\nC\ncollect all conditioning signals: the global image embedding\nc\nglobal\nc_{\\text{global}}\n, the per-part image embeddings\n{\nc\nk\npart\n}\n\\{c^{\\text{part}}_{k}\\}\n, and the absolute indices\n{\np\nk\n}\n\\{p_{k}\\}\n. The DiT predicts a velocity field\nV\nÎ¸\nâ€‹\n(\nğ—\nt\n,\nC\n,\nt\n)\nV_{\\theta}(\\mathbf{X}_{t},C,t)\nover all tokens. With per-part weights\nÎ±\nk\n\\alpha_{k}\n, timestep density\nÏ\nâ€‹\n(\nt\n)\n\\rho(t)\n, and a scalar reweighting function\nw\nâ€‹\n(\nt\n)\nw(t)\n, we optimize\nâ„’\nRF\n\\displaystyle\\mathcal{L}_{\\text{RF}}\n=\nğ”¼\nt\n,\nğ™\n0\n,\nğ™\n1\n,\nC\nâ€‹\n[\nw\nâ€‹\n(\nt\n)\nâ€‹\nâˆ‘\nk\n=\n1\nK\nÎ±\nk\nâ€‹\nâ€–\nv\nÎ¸\nâ€‹\n(\nğ±\nk\nâ€‹\n(\nt\n)\n,\nC\n,\nt\n)\nâˆ’\nğ®\nk\nâ‹†\nâ€–\n2\n2\n]\n\\displaystyle=\\mathbb{E}_{t,\\,\\mathbf{Z}_{0},\\,\\mathbf{Z}_{1},\\,C}\\Big[w(t)\\sum_{k=1}^{K}\\alpha_{k}\\big\\|v_{\\theta}(\\mathbf{x}_{k}(t),C,t)-\\mathbf{u}_{k}^{\\star}\\big\\|_{2}^{2}\\Big]\n(16)\n=\nğ”¼\nâ€‹\n[\nw\nâ€‹\n(\nt\n)\nâ€‹\nâ€–\nV\nÎ¸\nâ€‹\n(\nğ—\nt\n,\nC\n,\nt\n)\nâˆ’\nğ”\nâ‹†\nâ€–\nF\n2\n]\n.\n\\displaystyle=\\mathbb{E}\\Big[w(t)\\big\\|V_{\\theta}(\\mathbf{X}_{t},C,t)-\\mathbf{U}^{\\star}\\big\\|_{F}^{2}\\Big].\nHere, we use a reverse-velocity parameterization consistent with our sampler, and share\nt\nt\nacross all parts of the same object to keep the noise level aligned within an articulated asset.\nA.6\nClassifier-Free Guidance\nWe adopt classifier-free guidance at the object level. During training, with probability\np\ncfg\np_{\\text{cfg}}\nwe drop all image conditions for an entire object and replace both\nE\nk\nloc\nE^{\\text{loc}}_{k}\nand\nE\nglob\nE^{\\text{glob}}\nby learned â€œnullâ€ embeddings, yielding an unconditional branch. The DiT is thus trained on a mixture of conditional and unconditional samples.\nAt inference time, we evaluate the network twice for each diffusion step: once with all conditions dropped (\nV\nÎ¸\nuncond\nV_{\\theta}^{\\text{uncond}}\n) and once with full conditioning (\nV\nÎ¸\ncond\nV_{\\theta}^{\\text{cond}}\n). The guided prediction is\nV\nÎ¸\nguid\n=\nV\nÎ¸\nuncond\n+\ns\ncfg\nâ€‹\n(\nV\nÎ¸\ncond\nâˆ’\nV\nÎ¸\nuncond\n)\n,\nV_{\\theta}^{\\text{guid}}=V_{\\theta}^{\\text{uncond}}+s_{\\text{cfg}}\\big(V_{\\theta}^{\\text{cond}}-V_{\\theta}^{\\text{uncond}}\\big),\n(17)\nwhere\ns\ncfg\ns_{\\text{cfg}}\nis the guidance scale. This formulation lets us trade off fidelity to the input image against sample diversity while preserving multi-part consistency.\nAppendix B\nKinematic-Part Mesh Merging\nWe unify the raw PartNet-Mobility meshes into a single canonical GLB file per object, where each URDF link corresponds to exactly one mesh. In the original dataset, a single kinematic link may reference multiple OBJ files (e.g., different materials or subcomponents). For our purposes, a\nkinematic part\nis defined at the link level, so each link must appear as one rigid mesh that moves as a unit under the URDF articulation. We therefore merge all OBJs associated with the same link into a single geometry and export them.\nB.1\nURDF-Driven Link Grouping\nFor each object, we start from the original\nmobility.urdf\n. We parse all\n<link>\nelements and collect their associated mesh filenames from the\n<visual>\nblocks:\nâ€¢\nWe treat every non-base link (i.e., links whose name is not\nbase\n) as a candidate kinematic part.\nâ€¢\nFor each such link, we traverse all\n<visual>\nelements and extract the\nfilename\nattribute of the nested\n<mesh>\ntag.\nâ€¢\nThe original paths typically resemble\ntextured_objs/original-50.obj\n; we reduce them to basenames (e.g.,\noriginal-50.obj\n) and assume the corresponding geometry resides in the\ntextured_objs/\nfolder.\nThis yields a mapping\nğ’¢\n:\nlink name\nâ†¦\n{\nOBJ filenames\n}\n,\n\\mathcal{G}:\\text{link name}\\;\\mapsto\\;\\{\\text{OBJ filenames}\\},\nwhich defines how raw meshes should be grouped into kinematic parts. If a link has no valid mesh entries in the URDF, it is skipped.\nB.2\nPer-Link Geometry Cleaning and Merging\nWe load and merge meshes on a per-link basis using\ntrimesh\n. For each link\nâ„“\n\\ell\nwith mesh file list\nğ’¢\nâ€‹\n(\nâ„“\n)\n\\mathcal{G}(\\ell)\n:\n1.\nWe attempt to load each OBJ file from\ntextured_objs/\n. Files that cannot be loaded (missing or malformed) are skipped with a warning.\n2.\nEach successful load is converted to a pure-geometry\nTrimesh\n:\nâ€¢\nIf the loader returns a single\nTrimesh\n, we create a new mesh with the same vertices and faces (with\nprocess=False\nto avoid automatic repairs) and discard all materials.\nâ€¢\nIf the loader returns a\nScene\n, we iterate over its geometries and extract each\nTrimesh\nin the same way.\nâ€¢\nIn both cases, we assign a uniform gray face color\n[128, 128, 128, 255]\nto decouple our geometry pipeline from the original textures. Later, we re-render per-part images using our own lighting and camera setup (Sec.\nD\n), so we do not rely on baked-in materials.\n3.\nWe collect all such geometry-only meshes for link\nâ„“\n\\ell\ninto a list. If the list is empty, the link is effectively dropped.\n4.\nIf there is exactly one mesh, we keep it as-is. If there are multiple, we merge them by explicit vertex-face concatenation:\nV\nall\n\\displaystyle V_{\\text{all}}\n=\n[\nV\n1\nâ‹®\nV\nm\n]\n,\n\\displaystyle=\\begin{bmatrix}V_{1}\\\\\n\\vdots\\\\\nV_{m}\\end{bmatrix},\n(18)\nF\nall\n\\displaystyle F_{\\text{all}}\n=\n[\nF\n1\nF\n2\n+\n|\nV\n1\n|\nâ‹®\nF\nm\n+\nâˆ‘\ni\n=\n1\nm\nâˆ’\n1\n|\nV\ni\n|\n]\n,\n\\displaystyle=\\begin{bmatrix}F_{1}\\\\\nF_{2}+|V_{1}|\\\\\n\\vdots\\\\\nF_{m}+\\sum_{i=1}^{m-1}|V_{i}|\\end{bmatrix},\n(19)\nwhere\nV\ni\nV_{i}\nand\nF\ni\nF_{i}\nare the vertices and faces of the\ni\ni\n-th sub-mesh and\n|\nâ‹…\n|\n|\\cdot|\ndenotes the number of vertices. We then create a single\nTrimesh\nfrom\n(\nV\nall\n,\nF\nall\n)\n(V_{\\text{all}},F_{\\text{all}})\nwith a uniform gray color.\nThis procedure yields one rigid mesh per URDF link, with all subcomponents fused into the same local frame. We intentionally do not perform heavy processing at this stage (e.g., no automatic repairs or decimation) to preserve the original geometry as much as possible; watertight voxelization and cleanup are deferred to the next stage (Sec.\nC\n).\nAppendix C\nWatertight Part Preprocessing\nOur network operates on per-part surface samples extracted from mesh geometry (Sec.\nA\n). In practice, raw CAD / reconstruction data often contain small gaps, self-intersections, or open boundaries, which lead to unstable surface sampling and inconsistent volumes. Before training, we therefore convert every part mesh into a watertight surface via voxelization and marching cubes, combined with thin-part guards and aggressive post-cleaning. Unless otherwise stated, all experiments use a target voxel resolution of\nR\n=\n200\nR{=}200\nalong the largest object axis.\nC.1\nPer-part Voxelization and Pitch Selection\nGiven a per-part mesh with axis-aligned bounding box extents\nğ\n=\n(\ne\nx\n,\ne\ny\n,\ne\nz\n)\nâˆˆ\nâ„\n>\n0\n3\n,\n\\mathbf{e}=(e_{x},e_{y},e_{z})\\in\\mathbb{R}^{3}_{>0},\nwe first choose a voxel pitch\nÎ”\n\\Delta\nthat balances three goals: (1) roughly\nR\n=\n200\nR{=}200\nvoxels along the largest axis, (2) at least\nN\nmin\nN_{\\min}\nvoxels across the thinnest axis to avoid losing doors and sheet-like structures, and (3) a soft memory budget for the voxel grid.\nConcretely, we define two candidate pitches\nÎ”\nres\n=\nmax\nâ¡\n(\nğ\n)\nR\n,\nÎ”\nthin\n=\nmin\nâ¡\n(\nğ\n)\nN\nmin\n,\n\\Delta_{\\text{res}}=\\frac{\\max(\\mathbf{e})}{R},\\qquad\\Delta_{\\text{thin}}=\\frac{\\min(\\mathbf{e})}{N_{\\min}},\n(20)\nwhere\nN\nmin\nN_{\\min}\nis a small integer (we use\nN\nmin\n=\n3\nN_{\\min}{=}3\nin all experiments). The first term enforces the user-specified resolution along the largest axis; the second guarantees a minimum number of cells across the thinnest axis. We adopt the more conservative (higher-resolution) pitch\nÎ”\n=\nmin\nâ¡\n(\nÎ”\nres\n,\nÎ”\nthin\n)\n.\n\\Delta=\\min(\\Delta_{\\text{res}},\\,\\Delta_{\\text{thin}}).\n(21)\nGiven\nÎ”\n\\Delta\n, the approximate grid shape is\nğ§\n=\n(\nn\nx\n,\nn\ny\n,\nn\nz\n)\n=\n(\nâŒˆ\ne\nx\nÎ”\nâŒ‰\n,\nâŒˆ\ne\ny\nÎ”\nâŒ‰\n,\nâŒˆ\ne\nz\nÎ”\nâŒ‰\n)\n,\n\\mathbf{n}=(n_{x},n_{y},n_{z})=\\left(\\left\\lceil\\frac{e_{x}}{\\Delta}\\right\\rceil,\\left\\lceil\\frac{e_{y}}{\\Delta}\\right\\rceil,\\left\\lceil\\frac{e_{z}}{\\Delta}\\right\\rceil\\right),\n(22)\nand we estimate memory usage as\nM\nvox\nâ‰ˆ\nn\nx\nâ€‹\nn\ny\nâ€‹\nn\nz\nâ€‹\nbytes\n.\nM_{\\text{vox}}\\approx n_{x}n_{y}n_{z}\\;\\text{bytes}.\n(23)\nIf\nM\nvox\nM_{\\text{vox}}\nexceeds a rough cap (400â€‰MB in our implementation), we relax the pitch by a global scale factor so that\nM\nvox\nM_{\\text{vox}}\nfits into the budget. This procedure yields a per-part voxel grid that is fine enough for thin structures but remains tractable even for large assets.\nC.2\nClosed Occupancy and Marching Cubes\nWith the chosen pitch\nÎ”\n\\Delta\n, we voxelize each part mesh into an occupancy grid by calling the\nvoxelized\ninterface from\ntrimesh\n. To enforce watertightness, we explicitly convert the occupancy grid into a solid by filling interior and small holes:\n1.\nVoxelize the original mesh at pitch\nÎ”\n\\Delta\n.\n2.\nApply a fill operation to propagate occupancy to the interior, producing a fully closed solid voxel grid.\nWe then extract an isosurface using marching cubes on this filled grid. Because the occupancy is explicitly closed before marching cubes, the resulting surface is a watertight shell up to discretization artifacts.\nC.3\nAxis-wise Rescaling Back to Original Extents\nThe marching-cubes shell lives in the coordinate frame of the voxel grid, and its bounding box extents can deviate slightly from the original mesh due to discretization. To avoid systematic shrinkage of thin parts, we apply an anisotropic rescaling that exactly matches the original axis-aligned extents.\nLet\nğ\norig\n\\mathbf{e}^{\\text{orig}}\ndenote the original mesh extents and\nğ\nshell\n\\mathbf{e}^{\\text{shell}}\nthe extents of the marching-cubes shell. We compute a per-axis scale\nğ¬\n\\displaystyle\\mathbf{s}\n=\n(\ns\nx\n,\ns\ny\n,\ns\nz\n)\n,\n\\displaystyle=(s_{x},s_{y},s_{z}),\n(24)\ns\ni\n\\displaystyle s_{i}\n=\ne\ni\norig\nmax\nâ¡\n(\ne\ni\nshell\n,\nÎµ\n)\n,\ni\nâˆˆ\n{\nx\n,\ny\n,\nz\n}\n.\n\\displaystyle=\\frac{e^{\\text{orig}}_{i}}{\\max\\!\\big(e^{\\text{shell}}_{i},\\varepsilon\\big)},\\quad i\\in\\{x,y,z\\}.\nwith a small\nÎµ\n\\varepsilon\nto avoid division by zero. We then:\n1.\nTranslate the shell so that its bounding-box center is at the origin.\n2.\nApply the diagonal scale matrix\ndiag\nâ€‹\n(\nğ¬\n)\n\\mathrm{diag}(\\mathbf{s})\n.\n3.\nTranslate back so that the shell is centered at the original mesh center.\nThis axis-wise rescale ensures that each reconstructed part exactly matches the original size along all three axes, preserving joint clearances and articulated contact patterns.\nC.4\nRobust Mesh Cleanup and Early Exit for Watertight Parts\nAfter rescaling, we run a dedicated cleanup routine to remove numerical artifacts introduced by voxelization:\nâ€¢\nVertex welding.\nWe merge vertices within a small tolerance (we use a weld radius of\n10\nâˆ’\n6\n10^{-6}\nin world units) to eliminate near-degenerate triangles.\nâ€¢\nDegenerate and duplicate faces.\nWe repeatedly remove duplicate and zero-area faces, then drop unreferenced vertices and fix normals.\nâ€¢\nLargest connected component.\nTo discard floating fragments, we split the mesh into connected components and retain only the largest one (preferring components with more faces and, when available, larger volume).\nâ€¢\nOptional decimation.\nFor extremely dense outputs, we optionally apply quadratic decimation toward a target face count. For the experiments reported in this paper, we disable decimation (target faces set to zero) to avoid erasing small but semantically important details.\nFor parts that are already watertight and reasonably clean in the raw data, we take a conservative path: we skip voxelization and only apply the lightweight cleanup routine above. This preserves the original high-frequency geometry while still enforcing a consistent, watertight representation for problematic parts.\nAppendix D\nArticulation-Aware Data Augmentation\nAfter constructing watertight per-part meshes and canonical surface samples (Sec.\nC\n), we augment the dataset by rendering each articulated object at additional joint configurations while keeping the underlying geometry and part decomposition fixed. Concretely, for every\nmobility.urdf\nwith at least one revolute joint, we synthesize two extra variants:\nâ€¢\na\nmax\npose, where all revolute joints are placed at their upper joint limits; and\nâ€¢\na\nmid\npose, where all revolute joints are placed at half of their maximum angle.\nEach variant reuses the same watertight mesh and part-level surface samples but provides a new RGB rendering at a different articulation state. This encourages the model to become robust to joint motion while still reconstructing a consistent canonical geometry for each object.\nD.1\nURDF-Based Multi-Joint Forward Kinematics\nWe rely on the original PartNet-Mobility URDF to recover kinematic structure. For each object we parse:\nâ€¢\nall links\nâ„’\n\\mathcal{L}\n, each with one or more\n<visual>\nblocks that specify a mesh filename, an origin translation\nğ­\nvis\nâˆˆ\nâ„\n3\n\\mathbf{t}^{\\text{vis}}\\in\\mathbb{R}^{3}\n, and an RPY rotation\nğ«\nvis\nâˆˆ\nâ„\n3\n\\mathbf{r}^{\\text{vis}}\\in\\mathbb{R}^{3}\n;\nâ€¢\nall joints\nğ’¥\n\\mathcal{J}\n, each connecting a parent link\np\np\nand child link\nc\nc\nwith a joint origin\n(\nğ­\njoint\n,\nğ«\njoint\n)\n(\\mathbf{t}^{\\text{joint}},\\mathbf{r}^{\\text{joint}})\n, an axis\nğš\nâˆˆ\nâ„\n3\n\\mathbf{a}\\in\\mathbb{R}^{3}\n, and a type (we only use\nrevolute\njoints for augmentation).\nFor every link\nâ„“\nâˆˆ\nâ„’\n\\ell\\in\\mathcal{L}\n, we choose a representative visual transform\nT\nâ„“\nvis\nâˆˆ\nSE\nâ€‹\n(\n3\n)\nT^{\\text{vis}}_{\\ell}\\in\\mathrm{SE}(3)\nby preferring GLB-based visuals when present and falling back to the first mesh visual otherwise. This defines the transform from the link frame to a canonical visual frame for that link.\nWe then perform visual-space forward kinematics over the kinematic tree. Root links are detected as links that never appear as a joint child. For each root link\nr\nr\n, we initialize its visual transform in world coordinates as\nT\nvis\nworld\nâ€‹\n(\nr\n)\n=\nT\nr\nvis\n,\nT\nlink\nworld\nâ€‹\n(\nr\n)\n=\nI\n4\n,\nT^{\\text{world}}_{\\text{vis}}(r)=T^{\\text{vis}}_{r},\\qquad T^{\\text{world}}_{\\text{link}}(r)=I_{4},\nso that its visual frame coincides with its world frame by construction.\nFor a joint\nj\nâˆˆ\nğ’¥\nj\\in\\mathcal{J}\nwith parent link\np\np\nand child link\nc\nc\n, we form:\nT\nj\njoint\n\\displaystyle T^{\\text{joint}}_{j}\n=\nSE3\nâ€‹\n(\nR\nâ€‹\n(\nğ«\njoint\n)\n,\nğ­\njoint\n)\n,\n\\displaystyle=\\mathrm{SE3}\\big(R(\\mathbf{r}^{\\text{joint}}),\\,\\mathbf{t}^{\\text{joint}}\\big),\n(25)\nT\nj\nrot\nâ€‹\n(\nÎ¸\nj\n)\n\\displaystyle T^{\\text{rot}}_{j}(\\theta_{j})\n=\nSE3\nâ€‹\n(\nR\naxis\nâ€‹\n(\nğš\n,\nÎ¸\nj\n)\n,\n0\n)\n,\n\\displaystyle=\\mathrm{SE3}\\big(R_{\\text{axis}}(\\mathbf{a},\\theta_{j}),\\,\\mathbf{0}\\big),\n(26)\nwhere\nR\nâ€‹\n(\nâ‹…\n)\nR(\\cdot)\nconverts RPY to a rotation matrix and\nR\naxis\nâ€‹\n(\nğš\n,\nÎ¸\nj\n)\nR_{\\text{axis}}(\\mathbf{a},\\theta_{j})\nis the Rodrigues rotation about axis\nğš\n\\mathbf{a}\nwith angle\nÎ¸\nj\n\\theta_{j}\n.\nWe propagate transforms along the kinematic tree in topological order using:\nT\nlink\nworld\nâ€‹\n(\nc\n)\n\\displaystyle T^{\\text{world}}_{\\text{link}}(c)\n=\nT\nvis\nworld\nâ€‹\n(\np\n)\nâ€‹\nT\nj\njoint\nâ€‹\nT\nj\nrot\nâ€‹\n(\nÎ¸\nj\n)\n,\n\\displaystyle=T^{\\text{world}}_{\\text{vis}}(p)\\;T^{\\text{joint}}_{j}\\;T^{\\text{rot}}_{j}(\\theta_{j}),\n(27)\nT\nvis\nworld\nâ€‹\n(\nc\n)\n\\displaystyle T^{\\text{world}}_{\\text{vis}}(c)\n=\nT\nlink\nworld\nâ€‹\n(\nc\n)\nâ€‹\nT\nc\nvis\n.\n\\displaystyle=T^{\\text{world}}_{\\text{link}}(c)\\;T^{\\text{vis}}_{c}.\n(28)\nThis visual-space formulation matches the single-joint rendering script used during initial preprocessing and extends it to arbitrary joint depth without changing the relative placement of visual frames.\nTo align the multi-joint scene with the canonical coordinate system used in the main dataset, we pick a reference link\nr\nâ‹†\nr^{\\star}\nusing a simple heuristic (prefer a fixed child of the base link; otherwise use the parent of the first revolute joint). Let\nT\nvis\nworld\nâ€‹\n(\nr\nâ‹†\n)\nT^{\\text{world}}_{\\text{vis}}(r^{\\star})\ndenote the computed visual transform and\nT\nr\nâ‹†\nvis\nT^{\\text{vis}}_{r^{\\star}}\nbe its canonical visual transform. We apply a global similarity transform\nS\n=\nT\nr\nâ‹†\nvis\nâ€‹\n(\nT\nvis\nworld\nâ€‹\n(\nr\nâ‹†\n)\n)\nâˆ’\n1\n,\nS=T^{\\text{vis}}_{r^{\\star}}\\big(T^{\\text{world}}_{\\text{vis}}(r^{\\star})\\big)^{-1},\nand left-multiply all link and visual transforms by\nS\nS\n. This guarantees that the reference linkâ€™s visual frame exactly matches the canonical preprocessing pipeline, while preserving all relative joint poses.\nFinally, we instantiate a\ntrimesh.Scene\nby loading every mesh referenced in the link visuals and applying the corresponding world transform\nT\nworld\n=\nT\nlink\nworld\nâ€‹\n(\nâ„“\n)\nâ€‹\nT\nâ„“\n,\ncomponent\nvis\nT^{\\text{world}}=T^{\\text{world}}_{\\text{link}}(\\ell)\\;T^{\\text{vis}}_{\\ell,\\text{component}}\nto each visual component. The result is a normalized, articulated scene in the same global coordinate system as the watertight mesh used for sampling.\nD.2\nSampling Joint Angles: Reference, Mid, and Max Poses\nFor each revolute joint\nj\nâˆˆ\nğ’¥\nj\\in\\mathcal{J}\n, we read its upper limit\nÎ¸\nj\nmax\n\\theta^{\\max}_{j}\nfrom the URDF\n<limit>\ntag when available, defaulting to\nÎ¸\nj\nmax\n=\nÏ€\n\\theta^{\\max}_{j}=\\pi\n(180\nâˆ˜\n) if no limit is specified. We then define:\nÎ¸\nj\nref\n=\n0\n,\nÎ¸\nj\nmid\n=\n1\n2\nâ€‹\nÎ¸\nj\nmax\n,\nÎ¸\nj\nmax\n=\nÎ¸\nj\nmax\n.\n\\theta^{\\text{ref}}_{j}=0,\\qquad\\theta^{\\text{mid}}_{j}=\\frac{1}{2}\\,\\theta^{\\max}_{j},\\qquad\\theta^{\\text{max}}_{j}=\\theta^{\\max}_{j}.\nThe original preprocessed dataset already contains the reference pose with all joints set to\nÎ¸\nj\nref\n\\theta^{\\text{ref}}_{j}\n. Our augmentation script constructs two additional pose families:\nMax pose (\n_max\n).\nAll revolute joints are set to\nÎ¸\nj\nmax\n\\theta^{\\text{max}}_{j}\n, simultaneously driving each joint to its mechanically allowed extreme. This exposes the model to highly opened drawers, doors, and other articulated components.\nMid pose (\n_mid\n).\nAll revolute joints are set to\nÎ¸\nj\nmid\n\\theta^{\\text{mid}}_{j}\n, producing a configuration between the closed and fully open states. This captures typical everyday articulations without extreme self-occlusion.\nObjects without any revolute joints are left unchanged and contribute only their reference pose.\nD.3\nConsistent Normalization and Rendering Settings\nTo ensure consistent scale and camera framing across all poses, we normalize each object once using the reference pose. Given the reference scene\nğ’®\nref\n\\mathcal{S}_{\\text{ref}}\n(all joints at\nÎ¸\nj\nref\n\\theta^{\\text{ref}}_{j}\n), we compute its axis-aligned bounding box centroid\nğœ\n\\mathbf{c}\nand extents\nğ\n\\mathbf{e}\n. We then define:\nğ­\nnorm\n\\displaystyle\\mathbf{t}_{\\text{norm}}\n=\nâˆ’\nğœ\n,\n\\displaystyle=-\\mathbf{c},\n(29)\ns\nnorm\n\\displaystyle s_{\\text{norm}}\n=\n2\nmax\nâ¡\n(\ne\nx\n,\ne\ny\n,\ne\nz\n)\n.\n\\displaystyle=\\frac{2}{\\max(e_{x},e_{y},e_{z})}.\n(30)\nWe apply this normalization to all variants (reference, mid, max), so every object is centered at the origin and fits inside a unit cube regardless of articulation. Using a fixed normalization per object avoids small pose-induced scale changes and keeps camera parameters strictly comparable across augmentations.\nFor rendering, we reuse the same camera and lighting configuration as the main preprocessing pipeline:\nâ€¢\ncamera placed on a sphere of fixed radius (4 units) around the origin;\nâ€¢\nfield of view of 40\nâˆ˜\n;\nâ€¢\nhigh-resolution images at\n2048\nÃ—\n2048\n2048\\times 2048\npixels;\nâ€¢\nan environment light setup with multiple evenly spaced directional lights (36 directions) and fixed intensity.\nThe augmentation script calls a shared\nrender_single_view\nroutine with these settings, so augmented images are visually indistinguishable from the original dataset except for joint angles.",
    "preview_text": "Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further refine the URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering to optimize joint types, axes, and origins under VLM-generated open-state supervision. Extensive experiments show that SPARK produces high-quality, simulation-ready articulated assets across diverse categories, enabling downstream applications such as robotic manipulation and interaction modeling. Project page: https://heyumeng.com/SPARK/index.html.\n\nSPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge\nYumeng He\n1,2âˆ—\nYing Jiang\n1âˆ—\nJiayin Lu\n1âˆ—\nYin Yang\n3\nChenfanfu Jiang\n1\nAbstract\nArticulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further",
    "is_relevant": null,
    "relevance_score": 0.0,
    "extracted_keywords": [],
    "one_line_summary": "",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T12:51:56Z",
    "created_at": "2026-01-09T17:22:20.634591",
    "updated_at": "2026-01-09T17:22:20.634603",
    "flag": true
}