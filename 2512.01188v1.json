{
  "id": "2512.01188v1",
  "title": "Real-World Reinforcement Learning of Active Perception Behaviors",
  "authors": [
    "Edward S. Hu",
    "Jie Wang",
    "Xingfang Yuan",
    "Fiona Luo",
    "Muyao Li",
    "Gaspard Lambrechts",
    "Oleh Rybkin",
    "Dinesh Jayaraman"
  ],
  "abstract": "A robot's instantaneous sensory observations do not always reveal task-relevant state information. Under such partial observability, optimal behavior typically involves explicitly acting to gain the missing information. Today's standard robot learning techniques struggle to produce such active perception behaviors. We propose a simple real-world robot learning recipe to efficiently train active perception policies. Our approach, asymmetric advantage weighted regression (AAWR), exploits access to \"privileged\" extra sensors at training time. The privileged sensors enable training high-quality privileged value functions that aid in estimating the advantage of the target policy. Bootstrapping from a small number of potentially suboptimal demonstrations and an easy-to-obtain coarse policy initialization, AAWR quickly acquires active perception behaviors and boosts task performance. In evaluations on 8 manipulation tasks on 3 robots spanning varying degrees of partial observability, AAWR synthesizes reliable active perception behaviors that outperform all prior approaches. When initialized with a \"generalist\" robot policy that struggles with active perception tasks, AAWR efficiently generates information-gathering behaviors that allow it to operate under severe partial observability for manipulation tasks. Website: https://penn-pal-lab.github.io/aawr/",
  "url": "https://arxiv.org/abs/2512.01188v1",
  "html_url": "https://arxiv.org/html/2512.01188v1",
  "html_content": "Real-World Reinforcement Learning of\nActive Perception Behaviors\nEdward S.¬†Hu\n‚àó,1\nJie Wang\n‚àó,1\nXingfang Yuan\n‚àó,1\nFiona Luo\n1\nMuyao Li\n1\nGaspard Lambrechts\n2\nOleh Rybkin\n3\nDinesh Jayaraman\n1\n‚àó\nEqual contribution\n1\nUniversity of Pennsylvania\n2\nUniversity of Li√®ge\n3\nUC Berkeley\nAbstract\nA robot‚Äôs instantaneous sensory observations do not always reveal task-relevant state information. Under such partial observability, optimal behavior typically involves explicitly acting to gain the missing information.\nToday‚Äôs standard robot learning techniques struggle to produce such active perception behaviors.\nWe propose a simple real-world robot learning recipe to efficiently train active perception policies. Our approach, asymmetric advantage weighted regression (AAWR), exploits access to ‚Äúprivileged‚Äù extra sensors at training time. The privileged sensors enable training high-quality privileged value functions that aid in estimating the advantage of the target policy. Bootstrapping from a small number of potentially suboptimal demonstrations and an easy-to-obtain coarse policy initialization, AAWR quickly acquires active perception behaviors and boosts task performance. In evaluations on 8 manipulation tasks on 3 robots spanning varying degrees of partial observability, AAWR synthesizes reliable active perception behaviors that outperform all prior approaches. When initialized with a ‚Äúgeneralist‚Äù robot policy that struggles with active perception tasks, AAWR efficiently generates information-gathering behaviors that allow it to operate under severe partial observability for manipulation tasks. Website:\nhttps://penn-pal-lab.github.io/aawr/\n1\nIntroduction\nAny organism needs to extract information from the world via its sensory apparatus to make decisions, solve tasks, and survive. One strategy is to have high bandwidth and sophisticated sensors like the human eye, to sense as much information as possible and embrace the ‚Äúblooming, buzzing confusion of the senses‚Äù\n[\njames1890principles\n]\nthis entails, subject to natural limits, such as a local field of view. Another strategy is to use the ability to move around in the world to gather new information and overcome our sensory limitations - we scan our eyes across a crowded party to find a friend, and polish our glasses to get a clearer view.\nSuch information gathering behaviors are called active\n[\nbajcsy1988active\n]\nor interactive\n[\nbohg2017interactive\n]\nperception based on whether they only move a sensor around the world or if they also alter the world. In the following, we will use ‚Äúactive perception‚Äù as shorthand to refer to all such behaviors, except when the distinction is particularly pertinent.\nIn this work, we are interested in learning active perception behaviors to compensate for the limitations of various sensory setups in robots, ranging from entirely blind robots operating purely from proprioception, to robots operating with sophisticated multi-camera setups.\nIt has been hard to learn useful active perception behaviors for robotics, and not for lack of trying\n[\nbajcsy1988active\n,\nbohg2017interactive\n,\ncheng18reinforcement\n,\nyang2023seq2seq\n,\nchuang2024activevisionneedexploring\n,\nkrainin2011autonomous\n,\nrivlin2000control\n,\nliu2020target\n,\nburusa2024attention\n,\ndavison2003real\n,\nwu2015active\n]\n.\nOf the techniques commonly in vogue for robotics, imitation learning is ill-suited because acquiring optimal active perception demonstrations can be cumbersome and unnatural (e.g. forcing a teleoperator to look through wrist cameras). In theory, RL should be able to learn active perception behaviors from interaction, but in practice it is too sample-inefficient even in fully observed settings, leave alone the partially observed settings where active perception is relevant. Moreover, sim-to-real transfer is hard for such tasks because it is tied closely to sensory capabilities, and many sensory observations of interest, including depth, LIDAR, RGB cameras, audio, touch, contact, force sensors, etc., are all hard to simulate well enough to reliably learn transferable behaviors. This difficulty is also reflected in the inability of today‚Äôs state-of-the-art generalist policies trained on massive amounts of robot tele-operation data to perform even simple search tasks, as we will show in experiments.\nWe demonstrate that bootstrapping from suboptimal demonstrations and incorporating rewards in an offline-to-online RL algorithm is a viable approach for synthesizing active perception behaviors. Critically, the RL algorithm must be designed appropriately, and we theoretically derive that asymmetric access to extra sensors for critics and value functions is important to correctly estimate supervision signals for the policy in partially observed settings. Specifically, we take a weighted behavior cloning approach, by extending advantage weighted regression (AWR)\n[\nneumann2008fitted\n,\npeng2019advantage\n,\nnair2020awac\n,\nwang2020critic\n]\nto incorporate privileged, training-time only observations.\nWe call this approach\nAsymmetric AWR\n(AAWR). As an example, in one experiment, we give critics privileged access to object detectors to train open-loop policies that only receive proprioception and initial object positions. In another experiment, we give critics privileged access to privileged segmentation masks to train search policies in visually cluttered scenes.\nOur key contributions are as follows:\n(1)\nWe efficiently train real-world active perception policies by introducing AAWR, which uses privileged value functions to better supervise the policy.\n(2)\nWe provide theoretical justification for using privileged advantage estimates for AWR in POMDPs, by showing that maximizing the expected policy improvement in a POMDP results in the AAWR objective.\n(3)\nWe demonstrate that AAWR effectively learns a variety of active and interactive perception behaviors in 8 different settings - over varying types of partial observability, multiple types of simulated and real robots, and varied tasks.\nFigure 1:\nLeft: A robot with a single wrist image struggles to find objects in a heavily occluded scene. Right: An active perception policy searches through potential hiding spots to find the target object.\n2\nRelated Work\nActive perception policies are frequently trained to optimize information-theoretic objectives such as uncertainty reduction and next best viewpoint selection\n[\njayaraman2017learning\n,\ngrimes2023learning\n,\njiang2023fisherrf\n,\nhe2023active\n,\ntao2024view\n]\n.\nSuch approaches are used for task-agnostic applications like object tracking\n[\nrivlin2000control\n,\nbrowatzki2012active\n]\n, scene reconstruction\n[\ndavison2002simultaneous\n,\ndavison2003real\n,\ndong2019multi\n]\n, pose estimation\n[\nwu2015active\n]\n, or free-space navigation\n[\nhe2023active\n]\n. However, such approaches are not applicable to our setting in a variety of ways. First, many assume the ability to freely query views of a scene, without regard to task constraints\n[\njayaraman2017learning\n,\njiang2023fisherrf\n]\n. In our manipulation settings with clutter, there are many informative viewpoints that are difficult to reach due to physical constraints. Next, such information-theoretic metrics are not task-relevant - to locate a toy, a human may naturally look in drawers, shelves, cabinets, or other storage areas. But these information-theoretic metrics do not incorporate such task information, and may find the unseen back of a shelf just as interesting. In short, for training active perception policies for robots, we desire a more task-centric active perception approach that considers the constraints of the task and does perception-improving behavior to maximize the task success rate, instead of a task-agnostic metric.\nImitation and reinforcement learning are natural ways to synthesize task-relevant active perception policies, by using demonstrations and reward functions. Some prior works\n[\nyang2023seq2seq\n,\nchuang2024activevisionneedexploring\n,\nxiong2025via\n]\nuse imitation learning to train active perception policies on real robots, but performance is bounded by the demonstrator. However, acquiring optimal active perception demonstrations can be quite cumbersome (e.g. forcing human tele-operators to look through wrist cameras). On the other hand, RL approaches do not impose this burden, but real-world RL approaches for active perception\n[\nnovkovic2020object\n,\npitcher2024reinforcement\n]\nrequire heavy instrumentation (e.g. for constructing task-specific volumetric maps), limiting generality. Without such assumptions,\nRL methods are often limited to simulation due to sample inefficiencies\n[\nzaky2020active\n,\ngrimes2023learning\n,\nshang2023active\n]\n. Sim2real transfer, however, is not easily applicable to active perception tasks. This is because active perception tasks are closely related to sensory capabilities, and accurately simulating sensors (e.g. RGB, depth, touch, etc.) is difficult. Relative to prior work, we propose an RL method that efficiently learns active perception behavior on real robots while requiring minimal instrumentation (e.g. uncalibrated RGB cameras) at inference time.\nTo do this, we operate in an asymmetric training setting, exploiting privileged information\n[\nvapnik2009new\n]\nduring training time to improve policy training\n[\nlambrechts2023informed\n,\nhu2024privileged\n]\n. Privileged information approaches have been widely successful in solving partially observed tasks\n[\nli2020suphx\n,\nvasco2024super\n]\nand have been deployed on real world robots with sim2real transfer\n[\npinto2017asymmetric\n,\nlee2020learning\n,\nkumar2021rma\n,\nsalter2021attention\n]\n. As mentioned above however, it is difficult to perform sim-to-real transfer for active perception problems.\nFurther, asymmetric RL approaches for sim2real\n[\nkumar2021rma\n,\nlee2020learning\n]\nare designed to exploit billions of privileged simulator state transitions\n[\nchen2023sequential\n]\n, infeasible for privileged training in the real world, where\nwe only have small amounts of potentially noisy privileged observations. We develop a new ‚Äúasymmetric advantage weighted regression‚Äù RL algorithm that is more capable of learning efficiently in the real world, exploiting privileged additional sensors.\n3\nAsymmetric Reinforcement Learning in Active Perception POMDPs\nConsider a robot tasked with finding a toy in a cluttered room using just its wrist camera, as seen in\nFigure\nÀú\n1\n. The toy‚Äôs location is hidden to the robot, and it must scan the scene with its wrist camera in an efficient search path to find the toy quickly. These types of tasks where the robot has limited sensing but the reward and dynamics is dependent on some hidden environment state (e.g. toy location), are naturally modelled by partially observed Markov decision processes (POMDPs)\n[\nkaelbling1998planning\n]\n.\nA POMDP is represented by the tuple\n(\nùíÆ\n,\nùíú\n,\nùí™\n,\nT\n,\nR\n,\nE\n,\nP\n,\nŒ≥\n)\n(\\mathcal{S},\\mathcal{A},\\mathcal{O},T,R,E,P,\\gamma)\nwhere\nùíÆ\n\\mathcal{S}\nis the state space,\nùíú\n\\mathcal{A}\nthe action space, and\nùí™\n\\mathcal{O}\nthe observation space.\nThe dynamics are described by the transition density\nT\n‚Äã\n(\ns\nt\n+\n1\n‚à£\ns\nt\n,\na\nt\n)\nT(s_{t+1}\\mid s_{t},a_{t})\n, the reward density\nR\n‚Äã\n(\nr\nt\n‚à£\ns\nt\n,\na\nt\n)\nR(r_{t}\\mid s_{t},a_{t})\n, the observation density\nE\n‚Äã\n(\no\nt\n‚à£\ns\nt\n)\nE(o_{t}\\mid s_{t})\n, and the initial state density\nP\n‚Äã\n(\ns\n0\n)\nP(s_{0})\n. For the search task of\nFigure\nÀú\n1\n, the state would include the robot position and toy location, while the observation would be the wrist camera view.\nThe goal of policy synthesis in a POMDP is to find an optimal policy\nœÄ\n‚àó\n\\pi^{*}\nthat maximizes the expected return\nJ\n‚Äã\n(\nœÄ\n)\n=\nùîº\nœÄ\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\n‚Äã\nr\nt\n]\nJ(\\pi)=\\mathop{\\mathbb{E}}^{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}]\n, where the discount factor\nŒ≥\n\\gamma\nweights the importance of future rewards.\nIn a POMDP, such a\nœÄ\n‚àó\n\\pi^{*}\ngenerally requires access to the complete history\nh\nt\n=\n(\no\n0\n,\na\n0\n,\n‚Ä¶\n,\no\nt\n)\n‚àà\n‚Ñã\nh_{t}=(o_{0},a_{0},\\dots,o_{t})\\in\\mathcal{H}\nof past observations and actions.\nThis contrasts with an MDP, a special case of POMDP with\ns\nt\n=\no\nt\ns_{t}=o_{t}\n, in which the optimal policy depends only on the current state.\nBack to POMDPs, it is usually impractical to learn a policy conditioned on the full history, since its input space would grow exponentially with time.\nInstead, it is common to consider an ‚Äúagent state‚Äù\nf\n:\n‚Ñã\n‚Üí\nùíµ\nf\\colon\\mathcal{H}\\rightarrow\\mathcal{Z}\nthat is recurrent in the sense that\nz\nt\n=\nf\n‚Äã\n(\nh\nt\n)\n=\nu\n‚Äã\n(\nf\n‚Äã\n(\nh\nt\n‚àí\n1\n)\n,\na\nt\n‚àí\n1\n,\no\nt\n)\nz_{t}=f(h_{t})=u(f(h_{t-1}),a_{t-1},o_{t})\n, such as a sliding window. Then, the policy\nœÄ\n‚àà\nŒ†\n=\nùíµ\n‚Üí\nŒî\n‚Äã\n(\nùíú\n)\n\\pi\\in\\Pi=\\mathcal{Z}\\rightarrow\\Delta(\\mathcal{A})\nmust map from the agent state.\nInterestingly, when using an agent state and such policies, the POMDP can be transformed into an equivalent MDP whose state\n(\ns\nt\n,\nz\nt\n)\n(s_{t},z_{t})\nincludes both the environment state and the agent state, with policies\nœÄ\n‚àà\nŒ†\n\\pi\\in\\Pi\nconditioned on the latter state only\n[\njiang2019value\n,\ndong2022simple\n,\nlu2023reinforcement\n,\nsinha2024agent\n]\n.\n3.1\nBackground: Advantage Weighted Regression (AWR) for Markov Decision Processes\nAdvantage weighted regression (AWR)\n[\nneumann2008fitted\n,\npeng2019advantage\n]\nis a policy iteration algorithm for fully observed MDPs whose policy update objective is written as a behavior cloning loss, weighted by the estimated advantage of the transition. AWR is presented as a versatile algorithm that is able to leverage offline / off-policy data as well as on-policy data.\nMore formally, at each iteration, AWR seeks to find a policy\nœÄ\n:\nùíÆ\n‚Üí\nŒî\n‚Äã\n(\nùíú\n)\n\\pi\\colon\\mathcal{S}\\rightarrow\\Delta(\\mathcal{A})\nthat maximizes the expected surrogate improvement,\nŒ∑\n^\n‚Äã\n(\nœÄ\n)\n=\nùîº\ns\n‚àº\nd\nŒº\n‚Äã\n(\ns\n)\nùîº\na\n‚àº\nœÄ\n‚Äã\n(\na\n‚à£\ns\n)\nA\nŒº\n‚Äã\n(\ns\n,\na\n)\n‚âà\nJ\n‚Äã\n(\nœÄ\n)\n‚àí\nJ\n‚Äã\n(\nŒº\n)\n\\hat{\\eta}(\\pi)=\\mathop{\\mathbb{E}}_{s\\sim d_{\\mu}(s)}\\mathop{\\mathbb{E}}_{a\\sim\\pi(a\\mid s)}A^{\\mu}(s,a)\\approx J(\\pi)-J(\\mu)\nwith respect to a behavior policy\nŒº\n\\mu\n, under KL constraint\nùîº\ns\n‚àº\nd\nŒº\n‚Äã\n(\ns\n)\n[\nKL\n(\nœÄ\n(\n‚ãÖ\n‚à£\ns\n)\n‚à•\nŒº\n(\n‚ãÖ\n‚à£\ns\n)\n]\n‚â§\nŒµ\n\\mathbb{E}_{s\\sim d_{\\mu}(s)}\\left[\\text{KL}(\\pi(\\cdot\\mid s)\\parallel\\mu(\\cdot\\mid s)\\right]\\leq\\varepsilon\n.\nThe behavior policy\nŒº\n\\mu\ntypically corresponds to the mixture of all past policy iterates that generated the dataset of online interactions\nùíü\non\n\\mathcal{D}_{\\text{on}}\n.\nWhen relaxing the KL constraint with multiplier\nŒ≤\n>\n0\n\\beta>0\n, optimizing this soft-constrained objective is equivalent to maximizing the final AWR objective:\n‚Ñí\nAWR\n‚Äã\n(\nœÄ\n)\n=\nùîº\ns\n‚àº\nd\nŒº\n‚Äã\n(\ns\n)\nùîº\na\n‚àº\nŒº\n‚Äã\n(\na\n‚à£\ns\n)\n[\nexp\n‚Å°\n(\nA\nŒº\n‚Äã\n(\ns\n,\na\n)\n/\nŒ≤\n)\n‚Äã\nlog\n‚Å°\nœÄ\n‚Äã\n(\na\n‚à£\ns\n)\n]\n.\n\\mathcal{L}_{\\text{AWR}}(\\pi)=\\mathop{\\mathbb{E}}_{s\\sim d_{\\mu}(s)}\\mathop{\\mathbb{E}}_{a\\sim\\mu(a\\mid s)}\\bigg[\\exp\\big(A^{\\mu}(s,a)/\\beta\\big)\\log\\pi(a\\mid s)\\bigg].\n(1)\nThe original AWR algorithm\n[\npeng2019advantage\n]\nused either a return-based estimate or a\nTD\n‚Äã\n(\nŒª\n)\n\\text{TD}(\\lambda)\nestimate of the advantage, by learning a value function with Monte Carlo estimation.\nFollowup works\n[\nnair2020awac\n,\nwang2020critic\n]\nused a critic-based estimate of the advantage by learning a Q-function with TD learning, which improves sample efficiency by better leveraging off-policy samples. We build on these latter works. For more detailed background on AWR and related approaches, see\nAppendix\nÀú\nB\n.\n3.2\nThe Need for Asymmetric Training in POMDPs\nWe now derive the AWR objective for POMDPs, showing why it requires asymmetry during training.\nWe also show that the unprivileged value functions associated with a naive application of symmetric AWR cannot be learned by TD learning.\nAsymmetric and Symmetric AWR for POMDPs.\nWe aim to train a policy\nœÄ\n:\nùíµ\n‚Üí\nŒî\n‚Äã\n(\nùíú\n)\n\\pi\\colon\\mathcal{Z}\\rightarrow\\Delta(\\mathcal{A})\n, conditioned on the agent state\nz\nt\nz_{t}\n(equivalent to history\nh\nt\n=\n(\no\n1\n‚Äã\n‚Ä¶\n‚Äã\no\nt\n)\nh_{t}=(o_{1}\\ldots o_{t})\n) to maximize the return in POMDPs with an AWR-like objective. We consider the asymmetric learning paradigm in which the environment state\ns\ns\navailable during training (offline or online) but not during policy deployment. We introduce the asymmetric AWR (AAWR) objective:\n‚Ñí\nAAWR\n‚Äã\n(\nœÄ\n)\n=\nùîº\n(\ns\n,\nz\n)\n‚àº\nd\nŒº\n‚Äã\n(\ns\n,\nz\n)\nùîº\na\n‚àº\nŒº\n‚Äã\n(\na\n‚à£\nz\n)\n[\nexp\n‚Å°\n(\nA\nŒº\n‚Äã\n(\ns\n,\nz\n,\na\n)\n/\nŒ≤\n)\n‚Äã\nlog\n‚Å°\nœÄ\n‚Äã\n(\na\n‚à£\nz\n)\n]\n\\mathcal{L}_{\\text{AAWR}}(\\pi)=\\mathop{\\mathbb{E}}_{(s,z)\\sim d_{\\mu}(s,z)}\\mathop{\\mathbb{E}}_{a\\sim\\mu(a\\mid z)}\\bigg[\\exp\\big(A^{\\mu}({\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}s},z,a)/\\beta\\big)\\log\\pi(a\\mid z)\\bigg]\n(2)\nwhere\nA\nŒº\n‚Äã\n(\ns\n,\nz\n,\na\n)\n=\nQ\nŒº\n‚Äã\n(\ns\n,\nz\n,\na\n)\n‚àí\nV\nŒº\n‚Äã\n(\ns\n,\nz\n)\nA^{\\mu}({\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}s},z,a)=Q^{\\mu}({\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}s},z,a)-V^{\\mu}({\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}s},z)\nis the privileged advantage function, with\nQ\nŒº\n‚Äã\n(\ns\n,\nz\n,\na\n)\nQ^{\\mu}({\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}s},z,a)\nand\nV\nŒº\n‚Äã\n(\ns\n,\nz\n)\nV^{\\mu}({\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}s},z)\nthe privileged critic and value functions, formally defined in\nAppendix¬†C\n. See\nFigure\nÀú\n2\nfor a visual overview of the loss.\nFigure 2:\nTop row: The policy receives the partial observation. Bottom row: Privileged observations or state, available only during training, are given to the critic networks to estimate the advantage. The advantage estimates are used as weights in the loss, providing privileged supervision to the policy.\nIf the environment state\ns\ns\nis unavailable during training, one natural strategy is to solely use agent state\nz\nz\nto estimate the advantage. We call this unprivileged variant the symmetric AWR (SAWR) objective, which is just\nEquation\nÀú\n2\nwith environment state\ns\ns\nremoved from all terms.\nWhy is\n‚Ñí\nAAWR\n\\mathcal{L}_{\\text{AAWR}}\nthe right objective to implement AWR for POMDPs? To show this, we start by observing that the original AWR objective was derived as constrained policy improvement in an MDP setting.\nTo apply AWR to the POMDP setting with a policy\nœÄ\n‚àà\nŒ†\n=\nùíµ\n‚Üí\nŒî\n‚Äã\n(\nùíú\n)\n\\pi\\in\\Pi=\\mathcal{Z}\\rightarrow\\Delta(\\mathcal{A})\n, we can consider the equivalent MDP with state\n(\ns\n,\nz\n)\n(s,z)\n, as discussed in\nSection\nÀú\n3\n.\nWe then closely follow the original derivation in this MDP by additionally constraining the policy to be in\nŒ†\n=\nùíµ\n‚Üí\nŒî\n‚Äã\n(\nùíú\n)\n\\Pi=\\mathcal{Z}\\rightarrow\\Delta(\\mathcal{A})\n.\nTheorem 1\n(Asymmetric Advantage Weighted Regression).\nFor any POMDP and agent state\nf\n:\n‚Ñã\n‚Üí\nùíµ\nf\\colon\\mathcal{H}\\rightarrow\\mathcal{Z}\n, the Lagrangian relaxation with Lagrangian multiplier\nŒ≤\n>\n0\n\\beta>0\nof the following constrained optimization problem,\nmax\nœÄ\n‚àà\nŒ†\n\\displaystyle\\max_{\\pi\\in\\Pi}\nùîº\n(\ns\n,\nz\n)\n‚àº\nd\nŒº\n‚Äã\n(\ns\n,\nz\n)\nùîº\na\n‚àº\nœÄ\n‚Äã\n(\na\n‚à£\nz\n)\n[\nA\nŒº\n‚Äã\n(\ns\n,\nz\n,\na\n)\n]\n\\displaystyle\\mathop{\\mathbb{E}}_{(s,z)\\sim d_{\\mu}(s,z)}\\mathop{\\mathbb{E}}_{a\\sim\\pi(a\\mid z)}\\left[A^{\\mu}(s,z,a)\\right]\n(3)\ns.t.\nùîº\n(\ns\n,\nz\n)\n‚àº\nd\nŒº\n‚Äã\n(\ns\n,\nz\n)\n[\nKL\n(\nœÄ\n(\n‚ãÖ\n‚à£\nz\n)\n‚à•\nŒº\n(\n‚ãÖ\n‚à£\nz\n)\n]\n‚â§\nŒµ\n\\displaystyle\\mathop{\\mathbb{E}}_{(s,z)\\sim d_{\\mu}(s,z)}[\\text{KL}(\\pi(\\cdot\\mid z)\\parallel\\mu(\\cdot\\mid z)]\\leq\\varepsilon\n(4)\nis equivalent to the following optimization problem:\nmax\nœÄ\n‚àà\nŒ†\n‚Å°\n‚Ñí\nAAWR\n‚Äã\n(\nœÄ\n)\n\\max_{\\pi\\in\\Pi}\\mathcal{L}_{\\text{AAWR}}(\\pi)\n.\nThe proof is given in\nAppendix¬†C\nand concludes the validity of the AAWR objective.\nIn addition, we also show in\nAppendix\nÀú\nD\nthat optimizing the SAWR objective does not recover the correct solution, because\nan advantage estimator depending on agent state\nz\nz\nonly is insufficient for estimating the advantage of the equivalent MDP, whose state is\n(\ns\n,\nz\n)\n(s,z)\n.\nIn the example in\nFigure\nÀú\n1\n, it is clear that a privileged advantage estimator with access to toy locations will better estimate success.\nImplementation Details.\nTo instantiate asymmetric advantage weighted regression, we train\nV\nŒ∏\nŒº\nV^{\\mu}_{\\theta}\nand critic\nQ\nœï\nŒº\nQ^{\\mu}_{\\phi}\nnetworks to compute the advantage, mirroring extensions\n[\nwang2020critic\n,\nnair2020awac\n]\nof AWR that train critics to better leverage off-policy data instead of relying on MC returns.\nTo train the networks, we choose IQL\n[\nkostrikov2022offline\n]\n, a well known Q-learning algorithm known for its effectiveness in offline RL, offline-to-online RL finetuning\n[\npark2024ogbench\n]\nand real robot RL\n[\nfeng2023finetuning\n]\ntasks. The networks are trained using IQL‚Äôs expectile regression objective, see\nAppendix\nÀú\nA\nfor details.\nIn our POMDPs, in the symmetric setting, the unprivileged advantage estimator would be\nA\n^\nQ\n‚Äã\nV\nŒº\n‚Äã\n(\nz\nt\n,\na\nt\n)\n=\nQ\nœï\nŒº\n‚Äã\n(\nz\nt\n,\na\nt\n)\n‚àí\nV\nŒ∏\nŒº\n‚Äã\n(\nz\nt\n)\n\\hat{A}_{QV}^{\\mu}(z_{t},a_{t})=Q^{\\mu}_{\\phi}(z_{t},a_{t})-V^{\\mu}_{\\theta}(z_{t})\n.\nIn the asymmetric setting, the privileged advantage estimator would instead be\nA\n^\nQ\n‚Äã\nV\nŒº\n‚Äã\n(\ns\nt\n,\nz\nt\n,\na\nt\n)\n=\nQ\nœï\nŒº\n‚Äã\n(\ns\nt\n,\nz\nt\n,\na\nt\n)\n‚àí\nV\nŒ∏\nŒº\n‚Äã\n(\ns\nt\n,\nz\nt\n)\n\\hat{A}_{QV}^{\\mu}(s_{t},z_{t},a_{t})=Q^{\\mu}_{\\phi}(s_{t},z_{t},a_{t})-V^{\\mu}_{\\theta}(s_{t},z_{t})\n.\nIn\nAppendix\nÀú\nE\n, we show that the privileged value functions are the fixed point of the Bellman equations described by IQL‚Äôs objective. In contrast, we show that the unprivileged value functions are not the fixed point of their corresponding Bellman equations, which further motivates the use of AAWR instead of SAWR.\nWe consider an asymmetric learning setting in which the state\ns\nt\ns_{t}\nor privileged observations\no\nt\np\no_{t}^{p}\nfrom additional sensors are available during offline / online training time, but not during policy deployment. The privileged critics take in either observation and state\n(\no\nt\n,\ns\nt\n)\n(o_{t},s_{t})\n, or the augmented observation (\no\nt\n+\n=\n(\no\nt\n,\no\nt\np\n)\no^{+}_{t}=(o_{t},o_{t}^{p})\n) while the policy only receives\no\nt\no_{t}\n.\nAlgorithm 1\nAAWR Offline-to-Online Training\n1:\nprivileged\no\n+\no^{+}\n, partial\no\no\n,\nœÄ\n(\n‚ãÖ\n‚à£\no\n)\n\\pi(\\cdot\\mid o)\n,\nQ\n,\nV\nQ,V\n2:\nfor\ni\n=\n1\ni=1\nto\nN\noff\nN_{\\text{off}}\ndo\n3:\nUpdate\nQ\n,\nV\nQ,V\nusing\nùíü\noff\n\\mathcal{D}_{\\text{off}}\nand IQL loss\n4:\nUpdate\nœÄ\n\\pi\nusing\nùíü\noff\n\\mathcal{D}_{\\text{off}}\nand Eq.\n2\n5:\nfor\ni\n=\n1\ni=1\nto\nN\non\nN_{\\text{on}}\ndo\n6:\nCollect\n{\n(\no\nt\n,\no\nt\n+\n,\na\nt\n,\nr\nt\n,\no\nt\n+\n1\n,\no\nt\n+\n1\n+\n)\n}\nt\n=\n1\nT\n\\{(o_{t},o^{+}_{t},a_{t},r_{t},o_{t+1},o^{+}_{t+1})\\}^{T}_{t=1}\nwith\nœÄ\n\\pi\n7:\nùíü\non\n‚Üê\nùíü\non\n‚à™\n{\n(\no\nt\n,\no\nt\n+\n,\na\nt\n,\nr\nt\n,\no\nt\n+\n1\n,\no\nt\n+\n1\n+\n)\n}\nt\n=\n1\nT\n\\mathcal{D}_{\\text{on}}\\leftarrow\\mathcal{D}_{\\text{on}}\\cup\\{(o_{t},o^{+}_{t},a_{t},r_{t},o_{t+1},o^{+}_{t+1})\\}^{T}_{t=1}\n8:\nUpdate\nQ\n,\nV\nQ,V\nusing\nùíü\non\n,\nùíü\noff\n\\mathcal{D}_{\\text{on}},\\mathcal{D}_{\\text{off}}\nand IQL loss\n9:\nUpdate\nœÄ\n\\pi\nusing\nùíü\non\n,\nùíü\noff\n\\mathcal{D}_{\\text{on}},\\mathcal{D}_{\\text{off}}\nand Eq.\n2\nAlgorithm 2\nDeployment\n1:\npartial\no\no\n,\nœÄ\n\\pi\n2:\nfor\nt\n=\n1\nt=1\nto\nT\nT\ndo\n3:\no\n‚Üê\no\\leftarrow\nenv.step(\nœÄ\n(\n‚ãÖ\n‚à£\no\n)\n\\pi(\\cdot\\mid o)\n)\nFigure 3:\nLeft: The policy is trained using privileged sensors on offline / online data. Right: After training, privileged sensors are no longer available and only the policy is deployed.\n‚ñ∂\n\\blacktriangleright\nPrivileged Training\nWe follow the offline-to-online RL paradigm\n[\nnair2020awac\n,\nlee2022offline\n,\nkostrikov2022offline\n,\nfeng2023finetuning\n,\nnakamoto2023cal\n,\nyu2023actor\n]\nwhere the policy and value functions are first pre-trained on offline data using offline RL, and then are further fine-tuned with online interaction in the environment.\nFollowing lines 1-3 of\nAlgorithm\nÀú\n1\n: Given the offline data\nùíü\noff\n\\mathcal{D}_{\\text{off}}\n, we update\nQ\n,\nV\nQ,V\nusing the IQL objective and\nœÄ\n\\pi\nwith the\nEquation\nÀú\n2\nfor\nN\noff\nN_{\\text{off}}\ngradient steps. Next, in lines 4-8 of\nAlgorithm\nÀú\n1\n: We execute the policy in the environment and store online transitions into buffer\nùíü\non\n\\mathcal{D}_{\\text{on}}\n. We sample an equal number of transitions from both buffers to form a batch, following best practice from prior work\n[\nball2023efficient\n,\nfeng2023finetuning\n]\n. Using the batch, we update\nQ\n,\nV\nQ,V\nusing the IQL objective and\nœÄ\n\\pi\nwith the\nEquation\nÀú\n2\n.\n‚ñ∂\n\\blacktriangleright\nUnprivileged Deployment\n: During deployment, only partial observations\no\no\nremain available, which the policy uses to output actions, as seen in\nAlgorithm\nÀú\n2\n.\n4\nExperiments\nFigure 4:\nWe setup 8 different environments with diverse sensor setups and tasks to evaluate active perception behavior. Bottom row, we label the hiding spots for target objects.\nIn our experiments, we evaluate AAWR‚Äôs ability to learn active and interactive perception behaviors in a variety of tasks. We aim to answer the following questions. Does AAWR learn active perception behaviors more efficiently than other approaches? Does AAWR work in both offline-to-online and purely offline training settings?\nDoes active perception improve the ability of modern generalist policies to solve partially observed tasks?\n4.1\nTask setups\nWe evaluate AAWR in 8 different tasks, spanning both simulated and real world setups, see\nFigure\nÀú\n4\nfor images of all tasks. The tasks are grouped into simulated active perception tasks:\nCamouflage Pick, Fully Obs. Pick, Active Perception Koch\n, and real active/interactive perception tasks:\nBlind Pick, Bookshelf-P, Bookshelf-D, Shelf-Cabinet, Complex\n. In\nTable\nÀú\n1\n, we detail task properties like sensor setups, rewards, demonstrations, training budget, etc.\nIn the\nœÄ\n0\n\\pi_{0}\nhandoff tasks (\nBlind Pick, Bookshelf-P, Bookshelf-D, Shelf-Cabinet, Complex\n), we use three metrics to evaluate the search behavior. First,\nSearch\nis a score of the policy behavior with the following rubric: 1) the target appears in wrist camera frame, 2) the policy approaches the target, and 3) the policy remains fixated on the target after 5 timesteps. Next,\nCompletion\nis the rate at which\nœÄ\n0\n\\pi_{0}\nsuccessfully grabs the object after switching from the active perception policy. Finally,\nSteps\nis the amount of steps the policy takes to complete the task.\nSee\nAppendices\nÀú\nF\n,\nG\nand\nH\nfor comprehensive descriptions of the tasks.\nTable 1:\nThe 8 tasks vary in embodiment, nature of partial observability, privileged sensors, reward, demo quantity and quality, and training budget.\nTask\nPlatform\nTarget Obs.\nPrivileged Obs.\nReward\nDemos\nOffline Steps\nOnline Steps\nDescription\nCamouflage Pick\nSim. Koch\nSide Cam\nTrue Obj. Pos\nSparse\n100 suboptimal\n20K\n80K\nPick up barely\nvisible object\nFully Obs. Pick\nSim. xArm\nSide Cam\nTrue Obj. Pos\nSparse\n100 suboptimal\n20K\n20K\nPick up fully\nvisible object\nAP Koch\nSim. Koch\nWrist Cam\nTrue Obj. Pos\nSparse\n100 suboptimal\n100K\n900K\nLocate then pick\nup object\nBlind Pick\nReal Koch\nJoints, Init Obj. Pos\nObj. Pos Estimate\nDense\n100 suboptimal\n20K\n1.2K\nPick object from\nproprioception\nBookshelf-P\nReal Franka\nWrist Cam, Joints\nBbox, Mask\nDense\n‚àº\n\\sim\n150 suboptimal\n100K\n0\nLook for object &\nswitch to\nœÄ\n0\n\\pi_{0}\nBookshelf-D\nReal Franka\nWrist Cam, Joints\nBbox, Mask\nDense\n‚àº\n\\sim\n100 suboptimal\n100K\n0\nLook for object &\nswitch to\nœÄ\n0\n\\pi_{0}\nShelf-Cabinet\nReal Franka\nWrist Cam, Joints\nBbox, Mask\nDense\n‚àº\n\\sim\n30 suboptimal\n100K\n0\nLook for object &\nswitch to\nœÄ\n0\n\\pi_{0}\nComplex\nReal Franka\nWrist Cam, Joints\nBbox, Mask\nDense\n‚àº\n\\sim\n50 expert\n100K\n0\nLook for object &\nswitch to\nœÄ\n0\n\\pi_{0}\n4.2\nBaselines\nWe compare against symmetric advantage weighted regression (\nAWR\n) without privileged information. Its implementation is identical to that of AAWR, except for the inputs of the critic and value networks.\nNext, we compare against standard behavior cloning (\nBC\n), which performs imitation learning on the successful trajectories in the dataset.\n4.3\nResults\nFigure 5:\nEvaluation curves for the simulated experiments, over 10 seeds per method. The shaded regions indicate the offline pretraining phase. AAWR outperforms baselines in all simulated tasks.\nSimulated Active Perception tasks.\nFig.\n5\nshows these results, and videos are in Supp and website.\nFirst, we compare against AWR and BC on two simulated active perception tasks with varied degrees of observability, Camouflage Pick and Fully Obs. Pick. In both tasks, AAWR outperforms its non-privileged counterpart AWR, and BC in Camouflage Pick and Fully Obs. Pick, by approximately 2x and 3x respectively. While the gains from using privileged observations are in line with expectations for Camouflage Pick, where inferring the tiny marble from RGB is difficult, it is interesting to see gains even in a fully observable task, where the object position is always clearly inferable from vision. We hypothesize this is because the non-privileged critic needs to\nlearn\nto extract the object position from pixels, whereas the privileged critic does not.\nOne natural question is: how does AAWR compare against other popular approaches that leverage privileged information? On the simulated\nActive Perception Koch\ntask, we compare AAWR against\nDistillation\n[\nchen2023sequential\n]\n, which first trains a privileged expert policy and then distills it into a partially observed policy. We also compare against a variational information bottleneck approach (\nVIB\n)\n[\nhsu2022visionbased\n]\n, which gives the policy regularized access to privileged information.\nAs seen in\nFigure\nÀú\n5\n, only AAWR achieves 100% on the task by learning to scan the workspace, while other privileged baselines stagnate. Distillation gets high initial success but plateaus at 80%, learning a local maxima strategy of approaching the center of the workspace. This fails when the object is in the workspace corners, out of camera view. This behavior arises due to the privileged expert, unaware of the camera‚Äôs limited field of view, that goes straight for the object. AAWR is a policy iteration algorithm that works with both offline and online data, which allows it to discover the active perception behavior through online exploration, despite starting with suboptimal demos. VIB collapses during evaluation because privileged information is not available, even though it was trained to minimally use privileged information. See\nAppendix\nÀú\nH\nfor details and the website for videos.\nTable 2:\nKoch Interactive Perception.\nMethod\nGrasp‚Äâ%\nPick‚Äâ%\nBC\n47\n41\nOff. AWR\n65\n62\nOn. AWR\n71\n55\nOff. AAWR (ours)\n88\n71\nOn. AAWR (ours)\n94\n89\nReal Interactive Perception task.\nHere, we continue comparing to AWR and BC, this time including purely offline variants of AAWR and AWR, on the Blind Pick task in the real world. As seen in\nTable\nÀú\n2\n, both offline and online variants of AAWR outperform its unprivileged counterparts, and BC.\nAmong the offline methods, BC performs worst, exhibiting jerky and inaccurate movements. Both offline AWR and offline AAWR demonstrate better approaching and picking behavior, but offline AWR missed grasps and released the block frequently.\nOffline AAWR demonstrates more suboptimal behavior, such as releasing the candy after grasping. We observed that after online finetuning, the suboptimal behavior of offline AAWR is reduced, and online AAWR demonstrates the most consistent and robust open-loop picking behavior. Online AAWR reliably places its gripper over the object for grasping. In cases when the object slips from its grasp, the policy attempts to regrasp at the original location. See videos on the website.\nHandholding Foundation VLA Policies for Real Active Perception tasks.\nTable 3:\nIn the active perception handoff tasks, AAWR consistently outperforms baselines in terms of search behavior, completion, and speed. Bold = best, underline = second best.\nMethod\nBookshelf-P\nBookshelf-D\nShelf-Cabinet\nComplex\nSearch\n‚Üë\n\\uparrow\nCompletion\n‚Üë\n\\uparrow\nSteps\n‚Üì\n\\downarrow\nSearch\n‚Üë\n\\uparrow\nCompletion\n‚Üë\n\\uparrow\nSteps\n‚Üì\n\\downarrow\nSearch\n‚Üë\n\\uparrow\nCompletion\n‚Üë\n\\uparrow\nSteps\n‚Üì\n\\downarrow\nSearch\n‚Üë\n\\uparrow\nCompletion\n‚Üë\n\\uparrow\nSteps\n‚Üì\n\\downarrow\nAAWR\n92.4\n44.4\n36.6\n81.3\n44.4\n26.9\n78.2\n40.0\n46.3\n73.2\n50.0\n43.0\nAWR\n79.6\n0.0\n34.0\n62.6\n16.7\n30.2\n52.3\n10.0\n38.0\n33.2\n40.0\n67.0\nBC\n29.9\n20.0\n84.0\n47.7\n16.7\n22.5\n28.1\n15.0\n125.0\n31.5\n15.0\n77.0\nœÄ\n0\n\\pi_{0}\n11.0\n16.7\n263.3\n66.7\n33.3\n229.7\n10.0\n10.0\n280.0\n29.6\n20.0\n252.5\nExhaustive\n64.2\n44.0\n105.4\n96.0\n22.2\n106.7\n52.8\n45.0\n183.0\n78.2\n30.0\n297.0\nVLM+\nœÄ\n0\n\\pi_{0}\n31.4\n27.8\n322.3\n33.2\n16.7\n281.8\n28.2\n15.0\n382.0\n14.8\n10.0\n374.7\nWe find that\nœÄ\n0\n\\pi_{0}\n, a generalist foundation policy for manipulation tasks, is not good at searching tasks such as finding target objects in a cluttered scene from just a wrist camera (see in\nFigure\nÀú\n4\n).\nœÄ\n0\n\\pi_{0}\nis fundamentally limited by not having memory, which hampers its ability to handle POMDPs.\nWe now evaluate our approach‚Äôs ability to generate helper policies that condition on history (see\nAppendix\nÀú\nF\n) to handhold\nœÄ\n0\n\\pi_{0}\npolicies up to a configuration from which they could reasonably succeed. To achieve this, we propose a switching framework for the policy where an active perception policy is first run, and an object detector periodically checks the wrist image for the target object. Once the object is detected across two consecutive intervals, the robot switches to\nœÄ\n0\n\\pi_{0}\nto grasp the object.\nWe set up four realistic active perception tasks where the robot must search through a cluttered scene to find a target object and grasp it up (see\nFigure\nÀú\n4\n).\nIn the\nBookshelf-P\nand\nBookshelf-D\ntasks, we place a target object (either a toy pineapple or duck) on one of the three shelves, requiring the robot to scan the shelves both vertically and horizontally and stop at good viewpoint.\nIn the\nShelf-Cabinet\ntask, we add an additional cabinet with drawers and hiding spots near its top and inside cabinet door, increasing the complexity of the search.\nFinally, in\nComplex\ntask, we add an additional shelf on the floor, whose shelves are completely occluded from all side camera viewpoints.\nWe compare against several baselines.\nExhaustive\nis an engineered controller that methodically goes through all hiding spots in the scene, but is slow due to its heuristic pathfinding. Next, we again compare to unprivileged AWR and BC as helper policies. We compare against\nœÄ\n0\n\\pi_{0}\nitself, as well as a\nVLM+\nœÄ\n0\n\\pi_{0}\nvariant that queries the Gemini-2.5 VLM\n[\nteam2023gemini\n]\nto generate language commands that are executed by\nœÄ\n0\n\\pi_{0}\n, similar to the Hierarchical VLM-VLA baseline proposed in\n[\nshi2025hirobot\n]\n. As mentioned earlier in\nSection\nÀú\n4.1\n, we use the\nSearch\nmetric to judge the searching behavior,\nCompletion\nmetric to measure how useful is the handoff for\nœÄ\n0\n\\pi_{0}\n, and\nSteps\nfor speed.\nSee\nAppendix\nÀú\nF\nfor more details.\nFigure 6:\nExample rollouts in the cabinet shelf task. AAWR explores the top left of the cabinet and locates the target object (red arrow) while AWR only finds a small glimpse before drifting away.\nFor the\nBookshelf-P, Bookshelf-D\ntasks, we collected 250 demonstrations split among four demonstrators with varying qualities, resulting in a suboptimal dataset of about 50%\nœÄ\n0\n\\pi_{0}\nsuccess rate. For\nShelf-Cabinet, Complex\n, we collected smaller but higher quality datasets of 35 trajectories with 74% success rate and 50 trajectories with 94% success rate respectively. We perform offline training of AAWR, AWR, and BC on the same demonstration dataset, and for the same amount of gradient steps, before evaluating in the real world.\nAs seen in\nTable\nÀú\n4\n, AAWR consistently outperforms baselines in all metrics, learning sensible active perception behavior to aid a generalist policy. AAWR always outperforms non privileged AWR and BC, thus validating the usefulness of privileged information and the use of offline RL over supervised learning.\nWe find that\nœÄ\n0\n\\pi_{0}\nand VLM+\nœÄ\n0\n\\pi_{0}\nboth search poorly - they tend to take inefficient movements and fail to track the object. Exhaustive has decent search and completion rates, but takes much longer than AAWR. In fact, when normalized for time taken, AAWR scores 2-8 times higher in Search and Completion metrics than Exhaustive. See\nTable\nÀú\n5\nfor the time-normalized relative metrics.\nIn the\nBookshelf\ntasks, AAWR first learns to zoom out of the scene to see multiple shelves, then scans from bottom to up, and then approaches the target object once located. The AWR and BC baselines follow a relatively fixed search path that approaches the shelf, but the policies failed to efficiently scan the shelves. Even if the target object appears in the frame, the policies do not fixate on the object, reducing their search score and\nœÄ\n0\n\\pi_{0}\nsuccess rate.\nIn the\nShelf-Cabinet\ntask, AAWR searches through the right bookshelf, before moving to the left cabinet. Both AWR and BC do not thoroughly search the scene, preventing them from finding objects placed in the left cabinet‚Äôs drawer. In\nComplex\n, AAWR searches the bottom shelf, the right shelf, and then the left cabinet (see\nFigure\nÀú\n1\n). See\nFigure\nÀú\n6\nand the website for examples.\n5\nConclusion and Limitations\nWe aim to train active perception policies in the real world to allow robots to overcome their sensory limitations, a useful problem for which current approaches have shown limited success. We propose asymmetric advantage weighted regression (AAWR), a simple weighted behavior cloning technique that leverages privileged observations during training to efficiently train active perception policies. We provide a theoretical justification for AAWR, by deriving the validity of the AAWR objective for POMDP as opposed to its symmetric counterpart. Then, we show that AAWR successfully learns active and interactive perception behaviors in 8 different simulated and real world tasks.\nDespite our promising results, there is much room for improvement.\nInstead of switching between policies to execute active perception behaviors, AAWR could be used to directly fine-tune generalist foundation policies. Other forms of privileged information could be considered, such as foundation model outputs. Instead of using prespecified privileged information, useful features from the additional information could also be explicitly selected through representation learning. Because this work overcomes limitations of existing methods for active perceptions tasks, it would be worth exploring the scalability of AAWR to tasks with longer horizons where information-gathering challenges compound.\nFinally, AAWR could be applied to many other partially observed robotic (or other) tasks.\nAcknowledgments\nThis work was supported by the DARPA TIAMAT HR0011249042, NSF CAREER 2239301, ONR N00014-22-1-2677, and NSF SLES 2331783 grants. It used computing resources from the National Artificial Intelligence Research Resource Pilot (NAIRR 240077).\nGaspard Lambrechts is a postdoctoral researcher of the\nFund for Scientific Research\n(FNRS) from the\nWallonia-Brussels Federation\nin Belgium. The authors would like to thank the GRASP lab, anonymous NeurIPS reviewers, Antonio Loquercio, Kostas Daniilidis, Lei Zhang, Tianyou Wang, Siyu Li and Jiahui Lei for constructive feedback and support. The authors thank James Springer, Will Liang, Johnny Wang, Sam Wang and Tau Robotics for robot support.\nAppendix A\nAAWR Implementation Details\nInstead of the standard policy evaluation of the AWR algorithm\n[\npeng2019advantage\n,\nnair2020awac\n,\nwang2020critic\n]\n, we use implicit Q-learning (IQL)\n[\nkostrikov2022offline\n]\nand its optimistic policy evaluation, for its effectiveness in offline, and offline-to-online\n[\npark2024ogbench\n]\nand real robots\n[\nfeng2023finetuning\n]\ntasks.\nThe IQL algorithm learns both a value function\nV\nŒ∏\nŒº\nV^{\\mu}_{\\theta}\nand a critic\nQ\nœï\nŒº\nQ^{\\mu}_{\\phi}\n.\nExtending IQL to our POMDPs, in the symmetric setting, the unprivileged advantage estimator would be\nA\n^\nQ\n‚Äã\nV\nŒº\n‚Äã\n(\nz\nt\n,\na\nt\n)\n=\nQ\nœï\nŒº\n‚Äã\n(\nz\nt\n,\na\nt\n)\n‚àí\nV\nŒ∏\nŒº\n‚Äã\n(\nz\nt\n)\n\\hat{A}_{QV}^{\\mu}(z_{t},a_{t})=Q^{\\mu}_{\\phi}(z_{t},a_{t})-V^{\\mu}_{\\theta}(z_{t})\n.\nIn the asymmetric setting, the privileged advantage estimator would instead be\nA\n^\nQ\n‚Äã\nV\nŒº\n‚Äã\n(\ns\nt\n,\nz\nt\n,\na\nt\n)\n=\nQ\nœï\nŒº\n‚Äã\n(\ns\nt\n,\nz\nt\n,\na\nt\n)\n‚àí\nV\nŒ∏\nŒº\n‚Äã\n(\ns\nt\n,\nz\nt\n)\n\\hat{A}_{QV}^{\\mu}(s_{t},z_{t},a_{t})=Q^{\\mu}_{\\phi}(s_{t},z_{t},a_{t})-V^{\\mu}_{\\theta}(s_{t},z_{t})\n.\nThe privileged Q-function is trained using the 1-step TD error.\n‚Ñí\nQ\n‚Äã\n(\nœï\n)\n=\nùîº\n(\ns\nt\n,\nz\nt\n,\nr\nt\n,\ns\nt\n+\n1\n,\nz\nt\n+\n1\n)\n‚àº\nùíü\n[\n(\nr\nt\n+\nŒ≥\n‚Äã\nV\nŒ∏\nŒº\n‚Äã\n(\ns\nt\n+\n1\n,\nz\nt\n+\n1\n)\n‚àí\nQ\nœï\nŒº\n‚Äã\n(\ns\nt\n,\nz\nt\n,\na\nt\n)\n)\n2\n]\n\\mathcal{L}_{Q}(\\phi)=\\mathop{\\mathbb{E}}_{(s_{t},z_{t},r_{t},s_{t+1},z_{t+1})\\sim\\mathcal{D}}\\bigg[(r_{t}+\\gamma V^{\\mu}_{\\theta}(s_{t+1},z_{t+1})-Q^{\\mu}_{\\phi}(s_{t},z_{t},a_{t}))^{2}\\bigg]\n(5)\nThe privileged value function is trained to conservatively approximate the maximization\nmax\na\n‚Å°\nQ\nœï\nŒº\n‚Äã\n(\ns\n,\nz\n,\na\n)\n\\max_{a}Q^{\\mu}_{\\phi}(s,z,a)\nusing an asymmetric\nL\n2\nL_{2}\nloss (expectile regression):\n‚Ñí\nV\n‚Äã\n(\nŒ∏\n)\n=\nùîº\n(\ns\nt\n,\nz\nt\n,\na\nt\n)\n‚àº\nùíü\n[\n|\nœÑ\n‚àí\nùüô\n{\nQ\nœï\nŒº\n‚Äã\n(\ns\nt\n,\nz\nt\n,\na\nt\n)\n‚àí\nV\nŒ∏\nŒº\n‚Äã\n(\ns\nt\n,\nz\nt\n)\n<\n0\n}\n|\n‚Äã\n(\nQ\nœï\nŒº\n‚Äã\n(\ns\nt\n,\nz\nt\n,\na\nt\n)\n‚àí\nV\nŒ∏\nŒº\n‚Äã\n(\ns\nt\n,\nz\nt\n)\n)\n2\n]\n\\mathcal{L}_{V}(\\theta)=\\mathop{\\mathbb{E}}_{(s_{t},z_{t},a_{t})\\sim\\mathcal{D}}\\bigg[|\\tau-\\mathbbm{1}_{\\{Q^{\\mu}_{\\phi}(s_{t},z_{t},a_{t})-V^{\\mu}_{\\theta}(s_{t},z_{t})<0\\}}|(Q^{\\mu}_{\\phi}(s_{t},z_{t},a_{t})-V^{\\mu}_{\\theta}(s_{t},z_{t}))^{2}\\bigg]\n(6)\nwhere\nœÑ\n‚àà\n(\n0\n,\n1\n)\n\\tau\\in(0,1)\nis the expectile.\nAs\nœÑ\n‚Üí\n0\n\\tau\\rightarrow 0\n, the loss increasingly penalizes overestimates of\nV\nV\n.\nThe unprivileged value functions are trained analogously.\nAlgorithm 3\nAAWR Offline-to-Online\n1:\npolicy\nœÄ\n\\pi\n, critics\nQ\n,\nV\nQ,V\n, buffers\nùíü\noff\n,\nùíü\non\n\\mathcal{D}_{\\text{off}},\\mathcal{D}_{\\text{on}}\n2:\nfor\ni\n=\n1\ni=1\nto\nN\noff\nN_{\\text{off}}\ndo\n3:\nUpdate\nQ\n,\nV\nQ,V\nusing\nùíü\noff\n\\mathcal{D}_{\\text{off}}\nand Eq.\n5\nand Eq.\n6\n4:\nUpdate\nœÄ\n\\pi\nusing\nùíü\noff\n\\mathcal{D}_{\\text{off}}\nand Eq.\n2\n5:\nfor\ni\n=\n1\ni=1\nto\nN\non\nN_{\\text{on}}\ndo\n6:\nCollect\n{\n(\no\nt\n,\no\nt\n+\n,\na\nt\n,\nr\nt\n,\no\nt\n+\n1\n,\no\nt\n+\n1\n+\n)\n}\nt\n=\n1\nT\n\\{(o_{t},o^{+}_{t},a_{t},r_{t},o_{t+1},o^{+}_{t+1})\\}^{T}_{t=1}\nwith\nœÄ\n\\pi\n7:\nùíü\non\n‚Üê\nùíü\non\n‚à™\n{\n(\no\nt\n,\no\nt\n+\n,\na\nt\n,\nr\nt\n,\no\nt\n+\n1\n,\no\nt\n+\n1\n+\n)\n}\nt\n=\n1\nT\n\\mathcal{D}_{\\text{on}}\\leftarrow\\mathcal{D}_{\\text{on}}\\cup\\{(o_{t},o^{+}_{t},a_{t},r_{t},o_{t+1},o^{+}_{t+1})\\}^{T}_{t=1}\n8:\nUpdate\nQ\n,\nV\nQ,V\nusing\nùíü\non\n,\nùíü\noff\n\\mathcal{D}_{\\text{on}},\\mathcal{D}_{\\text{off}}\nand Eq.\n5\n, Eq.\n6\n9:\nUpdate\nœÄ\n\\pi\nusing\nùíü\non\n,\nùíü\noff\n\\mathcal{D}_{\\text{on}},\\mathcal{D}_{\\text{off}}\nand Eq.\n2\nNote that when\nœÑ\n=\n0.5\n\\tau=0.5\n, it corresponds to the standard\n1\n1\n-step TD update.\nIn\nAppendix\nÀú\nE\n, we show that the privileged value functions are the fixed point of the Bellman equations described by Eq.\n5\nand Eq.\n6\n. In contrast, we show that the unprivileged value functions are not the fixed point of their corresponding Bellman equations, which further motivates the use of AAWR instead of SAWR.\nWe train the models in an offline-to-online manner. Following lines 1-3 of\nAlgorithm\nÀú\n3\n: during the offline stage,\nQ\n,\nV\n,\nœÄ\nQ,V,\\pi\nare updated\nùíü\noff\n\\mathcal{D}_{\\text{off}}\nfor\nN\noff\nN_{\\text{off}}\ngradient steps on the offline dataset. After offline training, on-policy trajectories are collected by executing the target policy in the environment. These trajectories are added to the online buffer\nùíü\non\n\\mathcal{D}_{\\text{on}}\n. Following\n[\nfeng2023finetuning\n]\n, we use symmetric sampling where 50% of samples are from\nùíü\noff\n\\mathcal{D}_{\\text{off}}\nand the other 50% are from\nùíü\non\n\\mathcal{D}_{\\text{on}}\n.\nAppendix B\nAdvantage Weighted Regression Estimators\nThe original AWR algorithm\n[\npeng2019advantage\n]\nused a return-based estimate of the advantage\nA\n^\nV\nŒº\n‚Äã\n(\ns\nt\n,\na\nt\n)\n=\n‚àë\nk\n=\n0\n‚àû\nŒ≥\nk\n‚Äã\nr\nt\n+\nk\n‚àí\nV\n^\nŒº\n‚Äã\n(\ns\nt\n)\n\\hat{A}_{V}^{\\mu}(s_{t},a_{t})=\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k}-\\hat{V}^{\\mu}(s_{t})\nwhere\nV\n^\nŒº\n‚Äã\n(\ns\nt\n)\n\\hat{V}^{\\mu}(s_{t})\nis an approximation of the value function, learned by minimization of,\n‚Ñí\nMC\n‚Äã\n(\nV\n^\n)\n=\nùîº\ns\nt\n‚àº\nd\nŒº\n‚Äã\n(\ns\nt\n)\n[\n(\n‚àë\nk\n=\n0\n‚àû\nŒ≥\nk\n‚Äã\nr\nt\n+\nk\n‚àí\nV\n^\n‚Äã\n(\ns\nt\n)\n)\n2\n]\n.\n\\mathcal{L}_{\\text{MC}}(\\hat{V})=\\mathop{\\mathbb{E}}_{s_{t}\\sim d_{\\mu}(s_{t})}\\bigg[\\big(\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k}-\\hat{V}(s_{t})\\big)^{2}\\bigg].\n(7)\nFuture works have instead used a critic-based estimate of the advantage\nA\n^\nQ\nŒº\n‚Äã\n(\ns\nt\n,\na\nt\n)\n=\nQ\nœï\nŒº\n‚Äã\n(\ns\nt\n,\na\nt\n)\n‚àí\nùîº\na\n‚àº\nœÄ\n‚Äã\n(\na\n‚à£\ns\nt\n)\nQ\nœï\nŒº\n‚Äã\n(\ns\nt\n,\na\n)\n\\hat{A}_{Q}^{\\mu}(s_{t},a_{t})=Q^{\\mu}_{\\phi}(s_{t},a_{t})-\\mathop{\\mathbb{E}}_{a\\sim\\pi(a\\mid s_{t})}Q^{\\mu}_{\\phi}(s_{t},a)\n[\nnair2020awac\n,\nwang2020critic\n]\nwhere\nQ\nœï\nŒº\n‚Äã\n(\ns\nt\n,\na\nt\n)\nQ^{\\mu}_{\\phi}(s_{t},a_{t})\nis an approximation of the critic function, learned by minimization of,\n‚Ñí\nTD\n‚Äã\n(\nQ\n^\n)\n=\nùîº\ns\nt\n‚àº\nd\nŒº\n‚Äã\n(\ns\nt\n)\nùîº\na\nt\n‚àº\nœÄ\n‚Äã\n(\na\nt\n‚à£\ns\nt\n)\n[\n(\nr\nt\n+\nŒ≥\n‚Äã\nQ\n^\n‚Ä≤\n‚Äã\n(\ns\nt\n+\n1\n,\na\nt\n+\n1\n)\n‚àí\nQ\n^\n‚Äã\n(\ns\nt\n,\na\nt\n)\n)\n2\n]\n.\n\\mathcal{L}_{\\text{TD}}(\\hat{Q})=\\mathop{\\mathbb{E}}_{s_{t}\\sim d_{\\mu}(s_{t})}\\mathop{\\mathbb{E}}_{a_{t}\\sim\\pi(a_{t}\\mid s_{t})}\\bigg[\\big(r_{t}+\\gamma\\hat{Q}^{\\prime}(s_{t+1},a_{t+1})-\\hat{Q}(s_{t},a_{t})\\big)^{2}\\bigg].\n(8)\nThe typical learning procedure of AWR is summarized in\nAlgorithm\nÀú\n4\n.\nAlgorithm 4\nAdvantage Weighted Regression\n1:\npolicy\nœÄ\n\\pi\n, critic\nV\nV\n, buffer\nùíü\non\n\\mathcal{D}_{\\text{on}}\n2:\nfor\ni\n=\n1\ni=1\nto\nN\non\nN_{\\text{on}}\ndo\n3:\nCollect\n{\n(\ns\nt\n,\na\nt\n,\nr\nt\n,\ns\nt\n+\n1\n)\n}\nt\n=\n1\nT\n\\{(s_{t},a_{t},r_{t},s_{t+1})\\}^{T}_{t=1}\nwith\nœÄ\n\\pi\n4:\nùíü\non\n‚Üê\nùíü\non\n‚à™\n{\n(\ns\nt\n,\na\nt\n,\nr\nt\n,\ns\nt\n+\n1\n)\n}\nt\n=\n1\nT\n\\mathcal{D}_{\\text{on}}\\leftarrow\\mathcal{D}_{\\text{on}}\\cup\\{(s_{t},a_{t},r_{t},s_{t+1})\\}^{T}_{t=1}\n5:\nUpdate\nV\nV\nusing\nùíü\non\n\\mathcal{D}_{\\text{on}}\nand minimizing Eq.\n7\n6:\nUpdate\nœÄ\n\\pi\nusing\nùíü\non\n\\mathcal{D}_{\\text{on}}\nand maximizing Eq.\n1\nAppendix C\nAsymmetric Advantage Weighted Regression Derivation\nIn this section, we derive the AWR objective for POMDPs, which results in the AAWR objective. Since we consider a POMDP and an agent state\nf\n:\n‚Ñã\n‚Üí\nùíµ\nf\\colon\\mathcal{H}\\rightarrow\\mathcal{Z}\n, we can consider the equivalent environment-agent state MDP\n[\ndong2022simple\n,\nlu2023reinforcement\n,\nsinha2024agent\n]\n, whose state is\n(\ns\n,\nz\n)\n(s,z)\n, and restrain the class of fully observable policies\nŒ†\n+\n=\nùíÆ\n√ó\nùíµ\n‚Üí\nŒî\n‚Äã\n(\nùíú\n)\n\\Pi^{+}=\\mathcal{S}\\times\\mathcal{Z}\\rightarrow\\Delta(\\mathcal{A})\nfor this MDP to the agent-state policies,\nŒ†\n‚àí\n=\n{\nœÄ\n+\n‚àà\nŒ†\n+\n‚à£\n‚àÉ\nœÄ\n‚àà\nŒ†\n:\nœÄ\n+\n‚Äã\n(\na\n‚à£\ns\n,\nz\n)\n=\nœÄ\n‚Äã\n(\na\n‚à£\nz\n)\n,\n‚àÄ\ns\n‚àà\nùíÆ\n,\n‚àÄ\nz\n‚àà\nùíµ\n,\n‚àÄ\na\n‚àà\nùíú\n}\n.\n\\displaystyle\\Pi^{-}=\\left\\{\\pi^{+}\\in\\Pi^{+}\\mid\\exists\\pi\\in\\Pi:\\pi^{+}(a\\mid s,z)=\\pi(a\\mid z),\\forall s\\in\\mathcal{S},\\;\\forall z\\in\\mathcal{Z},\\;\\forall a\\in\\mathcal{A}\\right\\}\\!.\n(9)\nSince\nŒ†\n‚àí\n‚äÜ\nŒ†\n+\n\\Pi^{-}\\subseteq\\Pi^{+}\n, we have\nœÄ\n‚àí\n‚àà\nŒ†\n+\n\\pi^{-}\\in\\Pi^{+}\nand we can derive the AWR objective using this restricted set of policies following similar steps as\npeng2019advantage\n. In the following, we use\nœÄ\n‚àà\nŒ†\n\\pi\\in\\Pi\nto denote the partially observable policy\nœÄ\n‚Äã\n(\na\n‚à£\nz\n)\n\\pi(a\\mid z)\ncorresponding to\nœÄ\n‚àí\n‚àà\nŒ†\n‚àí\n\\pi^{-}\\in\\Pi^{-}\nwith\nœÄ\n‚àí\n‚Äã\n(\na\n‚à£\ns\n,\nz\n)\n=\nœÄ\n‚Äã\n(\na\n‚à£\nz\n)\n\\pi^{-}(a\\mid s,z)=\\pi(a\\mid z)\n.\nBefore deriving the AWR objective for the equivalent environment-agent state MDP, let us define the (normalized) discounted visitation measure of a policy\nœÄ\n‚àà\nŒ†\n\\pi\\in\\Pi\nin this MDP as:\nd\nœÄ\n‚Äã\n(\ns\n,\nz\n)\n=\n(\n1\n‚àí\nŒ≥\n)\n‚Äã\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\n‚Äã\np\n‚Äã\n(\ns\nt\n=\ns\n,\nz\nt\n=\nz\n)\n.\nd_{\\pi}(s,z)=(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}p(s_{t}=s,z_{t}=z).\n(10)\nIn the following, we denote the current policy\nœÄ\nk\n‚àí\n\\pi^{-}_{k}\nwith\nŒº\n‚àí\n\\mu^{-}\n, and its corresponding partially observable policy with\nŒº\n\\mu\nSince we work in the environment-agent state MDP, we consider the usual value functions definitions, where the state is\n(\ns\n,\nz\n)\n(s,z)\n,\nQ\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n\\displaystyle Q^{\\mu^{-}}(s,z,a)\n=\nùîº\nŒº\n‚àí\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\nr\nt\n|\ns\n0\n=\ns\n,\nz\n0\n=\nz\n,\na\n0\n=\na\n]\n\\displaystyle={\\mathbb{E}}^{\\mu^{-}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\middle|s_{0}=s,z_{0}=z,a_{0}=a\\right]\n(11)\nV\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n)\n\\displaystyle V^{\\mu^{-}}(s,z)\n=\nùîº\nŒº\n‚àí\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\nr\nt\n|\ns\n0\n=\ns\n,\nz\n0\n=\nz\n]\n.\n\\displaystyle={\\mathbb{E}}^{\\mu^{-}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\middle|s_{0}=s,z_{0}=z\\right]\\!.\n(12)\nNow, let us derive an objective to improve the policy. We want to maximize the policy improvement,\nŒ∑\n‚Äã\n(\nœÄ\n‚àí\n)\n=\nJ\n‚Äã\n(\nœÄ\n‚àí\n)\n‚àí\nJ\n‚Äã\n(\nŒº\n‚àí\n)\n.\n\\displaystyle\\eta(\\pi^{-})=J(\\pi^{-})-J(\\mu^{-}).\n(13)\nThe policy improvement is related to the advantage function of\nŒº\n‚àí\n\\mu^{-}\n[\nkakade2002approximately\n]\n,\nŒ∑\n‚Äã\n(\nœÄ\n‚àí\n)\n\\displaystyle\\eta(\\pi^{-})\n=\nùîº\nœÄ\n‚àí\n‚Äã\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\n‚Äã\nr\nt\n]\n‚àí\nùîº\nŒº\n‚àí\n‚Äã\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\n‚Äã\nr\nt\n]\n\\displaystyle=\\mathbb{E}^{\\pi^{-}}\\!\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\right]-\\mathbb{E}^{\\mu^{-}}\\!\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\right]\n(14)\n=\nùîº\nœÄ\n‚àí\n‚Äã\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\n‚Äã\nr\nt\n]\n‚àí\nùîº\n[\nV\nŒº\n‚àí\n‚Äã\n(\ns\n0\n,\nz\n0\n)\n]\n\\displaystyle=\\mathbb{E}^{\\pi^{-}}\\!\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\right]-\\mathop{\\mathbb{E}}\\left[V^{\\mu^{-}}(s_{0},z_{0})\\right]\n(15)\n=\nùîº\nœÄ\n‚àí\n‚Äã\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\n‚Äã\nr\nt\n‚àí\nV\nŒº\n‚àí\n‚Äã\n(\ns\n0\n,\nz\n0\n)\n]\n\\displaystyle=\\mathbb{E}^{\\pi^{-}}\\!\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}-V^{\\mu^{-}}(s_{0},z_{0})\\right]\n(16)\n=\nùîº\nœÄ\n‚àí\n‚Äã\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\n‚Äã\n(\nr\nt\n+\nŒ≥\n‚Äã\nV\nŒº\n‚àí\n‚Äã\n(\ns\nt\n+\n1\n,\nz\nt\n+\n1\n)\n‚àí\nV\nŒº\n‚àí\n‚Äã\n(\ns\nt\n,\nz\nt\n)\n)\n+\nV\nŒº\n‚àí\n‚Äã\n(\ns\n0\n,\nz\n0\n)\n‚àí\nV\nŒº\n‚àí\n‚Äã\n(\ns\n0\n,\nz\n0\n)\n]\n\\displaystyle=\\mathbb{E}^{\\pi^{-}}\\!\\Biggl[\\sum_{t=0}^{\\infty}\\gamma^{t}(r_{t}+\\gamma V^{\\mu^{-}}(s_{t+1},z_{t+1})-V^{\\mu^{-}}(s_{t},z_{t}))+V^{\\mu^{-}}(s_{0},z_{0})-V^{\\mu^{-}}(s_{0},z_{0})\\Biggr]\n(17)\n=\nùîº\nœÄ\n‚àí\n‚Äã\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\n‚Äã\n(\nr\nt\n+\nŒ≥\n‚Äã\nV\nŒº\n‚àí\n‚Äã\n(\ns\nt\n+\n1\n,\nz\nt\n+\n1\n)\n‚àí\nV\nŒº\n‚àí\n‚Äã\n(\ns\nt\n,\nz\nt\n)\n)\n]\n\\displaystyle=\\mathbb{E}^{\\pi^{-}}\\!\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}(r_{t}+\\gamma V^{\\mu^{-}}(s_{t+1},z_{t+1})-V^{\\mu^{-}}(s_{t},z_{t}))\\right]\n(18)\n=\nùîº\nœÄ\n‚àí\n‚Äã\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\n‚Äã\nA\nŒº\n‚àí\n‚Äã\n(\ns\nt\n,\nz\nt\n,\na\nt\n)\n]\n\\displaystyle=\\mathbb{E}^{\\pi^{-}}\\!\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}A^{\\mu^{-}}(s_{t},z_{t},a_{t})\\right]\n(19)\n=\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\n‚Äã\nùîº\nœÄ\n‚àí\n‚Äã\n[\nA\nŒº\n‚àí\n‚Äã\n(\ns\nt\n,\nz\nt\n,\na\nt\n)\n]\n\\displaystyle=\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{E}^{\\pi^{-}}\\!\\left[A^{\\mu^{-}}(s_{t},z_{t},a_{t})\\right]\n(20)\n=\n(\n1\n‚àí\nŒ≥\n)\n‚Äã\nùîº\n(\ns\n,\nz\n)\n‚àº\nd\nœÄ\n‚Äã\n(\ns\n,\nz\n)\nùîº\na\n‚àº\nœÄ\n‚Äã\n(\na\n‚à£\nz\n)\n[\nA\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n]\n\\displaystyle=(1-\\gamma)\\mathop{\\mathbb{E}}_{(s,z)\\sim d_{\\pi}(s,z)}\\mathop{\\mathbb{E}}_{a\\sim\\pi(a\\mid z)}\\!\\left[A^{\\mu^{-}}(s,z,a)\\right]\n(21)\nwhere\nA\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n=\nQ\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n‚àí\nV\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n)\nA^{\\mu^{-}}(s,z,a)=Q^{\\mu^{-}}(s,z,a)-V^{\\mu^{-}}(s,z)\n.\nThe objective\nŒ∑\n‚Äã\n(\nœÄ\n‚àí\n)\n\\eta(\\pi^{-})\nmay be inefficient to optimize, due to the dependence of the expectation on\nœÄ\n‚àí\n\\pi^{-}\nthrough\nd\nœÄ\nd_{\\pi}\n.\nInstead, we thus choose to optimize an off-policy surrogate objective where the samples are generated from policy\nŒº\n\\mu\n,\nŒ∑\n^\n‚Äã\n(\nœÄ\n‚àí\n)\n\\displaystyle\\hat{\\eta}(\\pi^{-})\n=\n(\n1\n‚àí\nŒ≥\n)\n‚Äã\nùîº\n(\ns\n,\nz\n)\n‚àº\nd\nŒº\n‚Äã\n(\ns\n,\nz\n)\nùîº\na\n‚àº\nœÄ\n‚Äã\n(\na\n‚à£\nz\n)\n[\nA\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n]\n\\displaystyle=(1-\\gamma)\\mathop{\\mathbb{E}}_{(s,z)\\sim d_{\\mu}(s,z)}\\mathop{\\mathbb{E}}_{a\\sim\\pi(a\\mid z)}\\!\\left[A^{\\mu^{-}}(s,z,a)\\right]\n(22)\nIn practice, we thus seek to approximately solve the following constrained optimization problem at each iteration,\nœÄ\nk\n+\n1\n‚àí\n‚àà\narg\n‚Äã\nmax\nœÄ\n‚àí\n‚àà\nŒ†\n‚àí\n\\displaystyle\\pi_{k+1}^{-}\\in\\operatorname*{arg\\,max}_{\\pi^{-}\\in\\Pi^{-}}\nùîº\n(\ns\n,\nz\n)\n‚àº\nd\nŒº\n‚Äã\n(\ns\n,\nz\n)\nùîº\na\n‚àº\nœÄ\n‚Äã\n(\na\n‚à£\nz\n)\n[\nA\nŒº\n‚àí\n(\ns\n,\nz\n,\na\n)\n)\n]\n\\displaystyle\\;\\mathop{\\mathbb{E}}_{(s,z)\\sim d_{\\mu}(s,z)}\\mathop{\\mathbb{E}}_{a\\sim\\pi(a\\mid z)}\\!\\left[A^{\\mu^{-}}(s,z,a))\\right]\n(23)\ns.t.\nKL\n(\nœÄ\n‚àí\n(\n‚ãÖ\n‚à£\ns\n,\nz\n)\n‚à•\nŒº\n‚àí\n(\n‚ãÖ\n‚à£\ns\n,\nz\n)\n)\n‚â§\nœµ\n\\displaystyle\\;\\text{KL}(\\pi^{-}(\\cdot\\mid s,z)\\parallel\\mu^{-}(\\cdot\\mid s,z))\\leq\\epsilon\n(24)\nNow that we have identified the desired constrained optimization problem, let us prove\nTheorem¬†1\n, which states that its relaxation corresponds to the AAWR objective.\nSee\n1\nProof.\nStarting from Eq.\n23\n, we note that two additional constraints are hidden in\nœÄ\n‚àí\n‚àà\nŒ†\n‚àí\n\\pi^{-}\\in\\Pi^{-}\n.\nThe first one is\n‚à´\na\n‚àà\nùíú\nœÄ\n‚Äã\n(\na\n‚à£\ns\n,\nz\n)\n=\n1\n,\n‚àÄ\ns\n‚àà\nùíÆ\n,\n‚àÄ\nz\n‚àà\nùíµ\n\\int_{a\\in\\mathcal{A}}\\pi(a\\mid s,z)=1,\\;\\forall s\\in\\mathcal{S},\\;\\forall z\\in\\mathcal{Z}\n.\nThe second one is\nœÄ\n‚àí\n‚àà\nŒ†\n‚àí\n‚äÜ\nŒ†\n+\n\\pi^{-}\\in\\Pi^{-}\\subseteq\\Pi^{+}\n, or equivalently,\nœÄ\n‚àí\n‚Äã\n(\na\n‚à£\ns\n1\n,\nz\n)\n=\nœÄ\n‚àí\n‚Äã\n(\na\n‚à£\ns\n2\n,\nz\n)\n,\n‚àÄ\ns\n1\n,\ns\n2\n‚àà\nùíÆ\n,\n‚àÄ\nz\n‚àà\nùíµ\n,\n‚àÄ\na\n‚àà\nùíú\n\\pi^{-}(a\\mid s_{1},z)=\\pi^{-}(a\\mid s_{2},z),\\;\\forall s_{1},s_{2}\\in\\mathcal{S},\\;\\forall z\\in\\mathcal{Z},\\;\\forall a\\in\\mathcal{A}\n.\nFollowing similar steps as\npeng2019advantage\n, by relaxing the KL constraint in a Lagrangian multiplier with multiplier\nŒ≤\n\\beta\n,\nœÄ\n‚àó\n‚Äã\n(\na\n‚à£\ns\n,\nz\n)\n\\displaystyle\\pi^{*}(a\\mid s,z)\n‚àù\nŒº\n‚àí\n‚Äã\n(\na\n‚à£\ns\n,\nz\n)\n‚Äã\nexp\n‚Å°\n(\n1\nŒ≤\n‚Äã\nA\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n)\n\\displaystyle\\propto\\mu^{-}(a\\mid s,z)\\exp\\left(\\frac{1}{\\beta}A^{\\mu^{-}}(s,z,a)\\right)\n(25)\n‚àù\nŒº\n‚Äã\n(\na\n‚à£\nz\n)\n‚Äã\nexp\n‚Å°\n(\n1\nŒ≤\n‚Äã\nA\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n)\n\\displaystyle\\propto\\mu(a\\mid z)\\exp\\left(\\frac{1}{\\beta}A^{\\mu^{-}}(s,z,a)\\right)\n(26)\nWe now substitute back the two additional constraints as additional constraints, so that we project the solution on the manifold of policies\nŒ†\n‚àí\n\\Pi^{-}\n.\nBy minimizing the KL divergence\nùîº\n(\ns\n,\nz\n)\n‚àº\nd\nŒº\n‚Äã\n(\ns\n,\nz\n)\n[\nKL\n(\nœÄ\n‚àó\n(\n‚ãÖ\n‚à£\ns\n,\nz\n)\n‚à•\nœÄ\n(\n‚ãÖ\n‚à£\nz\n)\n]\n\\mathop{\\mathbb{E}}_{(s,z)\\sim d_{\\mu}(s,z)}\\left[\\text{KL}(\\pi^{*}(\\cdot\\mid s,z)\\parallel\\pi(\\cdot\\mid z)\\right]\nto that target, we obtain\nœÄ\nk\n+\n1\n\\displaystyle\\pi_{k+1}\n=\narg\n‚Äã\nmax\nœÄ\n‚àà\nŒ†\n‚Äã\nùîº\n(\ns\n,\nz\n)\n‚àº\nd\nŒº\n‚Äã\n(\ns\n,\nz\n)\nùîº\na\n‚àº\nŒº\n‚Äã\n(\na\n‚à£\nz\n)\n[\nlog\n‚Å°\nœÄ\n‚Äã\n(\na\n‚à£\nz\n)\n‚Äã\nexp\n‚Å°\n(\n1\nŒ≤\n‚Äã\nA\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n)\n]\n\\displaystyle=\\operatorname*{arg\\,max}_{\\pi\\in\\Pi}\\mathop{\\mathbb{E}}_{(s,z)\\sim d_{\\mu}(s,z)}\\mathop{\\mathbb{E}}_{a\\sim\\mu(a\\mid z)}\\left[\\log\\pi(a\\mid z)\\exp\\left(\\frac{1}{\\beta}A^{\\mu^{-}}(s,z,a)\\right)\\right]\n(27)\nThis concludes the proof, for\nŒº\n=\nœÄ\nk\n\\mu=\\pi_{k}\n.\n‚àé\nWith the additional constraint that we use a parametrized policy\nœÄ\nŒ∏\n‚àà\nŒ†\nŒò\n\\pi_{\\theta}\\in\\Pi_{\\Theta}\n, we obtain\nŒ∏\nk\n+\n1\n\\displaystyle\\theta_{k+1}\n=\narg\n‚Äã\nmax\nŒ∏\n‚àà\nŒò\n‚Äã\nùîº\n(\ns\n,\nz\n)\n‚àº\nd\nŒº\n‚Äã\n(\ns\n,\nz\n)\nùîº\na\n‚àº\nŒº\n‚Äã\n(\na\n‚à£\nz\n)\n[\nlog\n‚Å°\nœÄ\nŒ∏\n‚Äã\n(\na\n‚à£\nz\n)\n‚Äã\nexp\n‚Å°\n(\n1\nŒ≤\n‚Äã\nA\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n)\n]\n\\displaystyle=\\operatorname*{arg\\,max}_{\\theta\\in\\Theta}\\mathop{\\mathbb{E}}_{(s,z)\\sim d_{\\mu}(s,z)}\\mathop{\\mathbb{E}}_{a\\sim\\mu(a\\mid z)}\\left[\\log\\pi_{\\theta}(a\\mid z)\\exp\\left(\\frac{1}{\\beta}A^{\\mu^{-}}(s,z,a)\\right)\\right]\n(28)\nEq.\n27\ncorresponds to the AAWR objective, and Eq.\n28\nis the AAWR objective in the context of parametrized function approximators.\nAppendix D\nProblem with Symmetric Advantage Weighted Regression\nWhile the asymmetric value functions followed the classical definitions in the environment-agent state MDP, the symmetric value functions are not standard because\nz\nz\nis not a Markovian state.\nWe select the following definition:\nA\nŒº\n‚Äã\n(\nz\n,\na\n)\n=\nQ\nŒº\n‚Äã\n(\nz\n,\na\n)\n‚àí\nV\nŒº\n‚Äã\n(\nz\n)\nA^{\\mu}(z,a)=Q^{\\mu}(z,a)-V^{\\mu}(z)\nwith,\nQ\nŒº\n‚àí\n‚Äã\n(\nz\n,\na\n)\n\\displaystyle Q^{\\mu^{-}}(z,a)\n=\nùîº\ns\n‚àº\nd\nŒº\n‚àí\n‚Äã\n(\ns\n‚à£\nz\n)\n[\nùîº\nŒº\n‚àí\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\nr\nt\n|\ns\n0\n,\n=\ns\n,\nz\n0\n=\nz\n,\na\n0\n=\na\n]\n]\n\\displaystyle=\\mathop{\\mathbb{E}}_{s\\sim d_{\\mu^{-}}(s\\mid z)}\\left[{\\mathbb{E}}^{\\mu^{-}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\middle|s_{0},=s,z_{0}=z,a_{0}=a\\right]\\right]\n(29)\nV\nŒº\n‚àí\n‚Äã\n(\nz\n)\n\\displaystyle V^{\\mu^{-}}(z)\n=\nùîº\ns\n‚àº\nd\nŒº\n‚àí\n‚Äã\n(\ns\n‚à£\nz\n)\n[\nùîº\nŒº\n‚àí\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\nr\nt\n|\ns\n0\n=\ns\n,\nz\n0\n=\nz\n]\n]\n\\displaystyle=\\mathop{\\mathbb{E}}_{s\\sim d_{\\mu^{-}}(s\\mid z)}\\left[{\\mathbb{E}}^{\\mu^{-}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\middle|s_{0}=s,z_{0}=z\\right]\\right]\n(30)\nBy definition, this choice provides unbiased symmetric value functions under the distribution\nd\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n)\nd_{\\mu^{-}}(s,z)\ninduced by the current policy\nŒº\n‚àí\n\\mu^{-}\n.\nQ\nŒº\n‚àí\n‚Äã\n(\nz\n,\na\n)\n\\displaystyle Q^{\\mu^{-}}(z,a)\n=\nùîº\ns\n‚àº\nd\nŒº\n‚àí\n‚Äã\n(\ns\n‚à£\nz\n)\n[\nQ\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n]\n\\displaystyle=\\mathop{\\mathbb{E}}_{s\\sim d_{\\mu^{-}}(s\\mid z)}\\left[Q^{\\mu^{-}}(s,z,a)\\right]\n(31)\nV\nŒº\n‚àí\n‚Äã\n(\nz\n)\n\\displaystyle V^{\\mu^{-}}(z)\n=\nùîº\ns\n‚àº\nd\nŒº\n‚àí\n‚Äã\n(\ns\n‚à£\nz\n)\n[\nV\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n)\n]\n\\displaystyle=\\mathop{\\mathbb{E}}_{s\\sim d_{\\mu^{-}}(s\\mid z)}\\left[V^{\\mu^{-}}(s,z)\\right]\n(32)\nLet us now prove\nTheorem\nÀú\n2\n, which proves that the symmetric AWR (SAWR) objective is different from the asymmetric AWR (AAWR) objective.\nTheorem 2\n(Symmetric Advantage Weighted Regression).\nIn general, for a POMDP and an agent state\nf\n:\n‚Ñã\n‚Üí\nùíµ\nf\\colon\\mathcal{H}\\rightarrow\\mathcal{Z}\n, we have\narg\n‚Äã\nmax\nœÄ\n‚àà\nŒ†\n‚Å°\n‚Ñí\nSAWR\n‚â†\narg\n‚Äã\nmax\nœÄ\n‚àà\nŒ†\n‚Å°\n‚Ñí\nAAWR\n\\operatorname*{arg\\,max}_{\\pi\\in\\Pi}\\mathcal{L}_{\\text{SAWR}}\\neq\\operatorname*{arg\\,max}_{\\pi\\in\\Pi}\\mathcal{L}_{\\text{AAWR}}\n.\nProof.\nCombining Eq.\n29\nand Eq.\n30\n, we have,\nA\nŒº\n‚àí\n‚Äã\n(\nz\n,\na\n)\n\\displaystyle A^{\\mu^{-}}(z,a)\n=\nùîº\ns\n‚àº\nd\nŒº\n‚àí\n‚Äã\n(\ns\n‚à£\nz\n)\n[\nA\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n]\n\\displaystyle=\\mathop{\\mathbb{E}}_{s\\sim d_{\\mu^{-}}(s\\mid z)}\\left[A^{\\mu^{-}}(s,z,a)\\right]\n(33)\nNow, it is straightforward to see that the SAWR objective,\n‚Ñí\nSAWR\n‚Äã\n(\nœÄ\n)\n\\displaystyle\\mathcal{L}_{\\text{SAWR}}(\\pi)\n=\nùîº\n(\ns\n,\nz\n)\n‚àº\nd\nŒº\n‚Äã\n(\ns\n,\nz\n)\nùîº\na\n‚àº\nŒº\n‚Äã\n(\na\n‚à£\nz\n)\n[\nexp\n‚Å°\n(\nA\nŒº\n‚àí\n‚Äã\n(\nz\n,\na\n)\n/\nŒ≤\n)\n‚Äã\nlog\n‚Å°\nœÄ\n‚Äã\n(\na\n‚à£\nz\n)\n]\n\\displaystyle=\\mathop{\\mathbb{E}}_{(s,z)\\sim d_{\\mu}(s,z)}\\mathop{\\mathbb{E}}_{a\\sim\\mu(a\\mid z)}\\bigg[\\exp\\big(A^{\\mu^{-}}(z,a)/\\beta\\big)\\log\\pi(a\\mid z)\\bigg]\n(34)\n=\nùîº\n(\ns\n,\nz\n)\n‚àº\nd\nŒº\n‚Äã\n(\ns\n,\nz\n)\nùîº\na\n‚àº\nŒº\n‚Äã\n(\na\n‚à£\nz\n)\n[\nexp\n‚Å°\n(\nùîº\ns\n‚àº\nd\nŒº\n‚àí\n‚Äã\n(\ns\n‚à£\nz\n)\n[\nA\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n]\n/\nŒ≤\n)\n‚Äã\nlog\n‚Å°\nœÄ\n‚Äã\n(\na\n‚à£\nz\n)\n]\n\\displaystyle=\\mathop{\\mathbb{E}}_{(s,z)\\sim d_{\\mu}(s,z)}\\mathop{\\mathbb{E}}_{a\\sim\\mu(a\\mid z)}\\bigg[\\exp\\bigg(\\mathop{\\mathbb{E}}_{s\\sim d_{\\mu^{-}}(s\\mid z)}\\left[A^{\\mu^{-}}(s,z,a)\\right]/\\beta\\bigg)\\log\\pi(a\\mid z)\\bigg]\n(35)\ndoes not correspond to the AAWR objective,\n‚Ñí\nAAWR\n‚Äã\n(\nœÄ\n)\n=\nùîº\n(\ns\n,\nz\n)\n‚àº\nd\nŒº\n‚Äã\n(\ns\n,\nz\n)\nùîº\na\n‚àº\nŒº\n‚Äã\n(\na\n‚à£\nz\n)\n[\nexp\n‚Å°\n(\nA\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n)\n‚Äã\nlog\n‚Å°\nœÄ\n‚Äã\n(\na\n‚à£\nz\n)\n]\n.\n\\mathcal{L}_{\\text{AAWR}}(\\pi)=\\mathop{\\mathbb{E}}_{(s,z)\\sim d_{\\mu}(s,z)}\\mathop{\\mathbb{E}}_{a\\sim\\mu(a\\mid z)}\\bigg[\\exp\\big(A^{\\mu^{-}}(s,z,a)\\big)\\log\\pi(a\\mid z)\\bigg].\n(36)\nIndeed, we can apply Jensen‚Äôs strict inequality over the the strictly convex exponential function, under the assumption that the distribution\nd\nŒº\n‚àí\n‚Äã\n(\ns\n‚à£\nz\n)\nd_{\\mu^{-}}(s\\mid z)\nis nondegenerate.\n‚àé\nAppendix E\nProblem with Symmetric Temporal Difference Learning\nLet us consider the asymmetric Bellman equations for the environment-agent state MDP,\nQ\n~\nŒº\n‚àí\n(\ns\n,\nz\n,\na\n)\n=\nùîº\nŒº\n‚àí\n[\nr\nt\n+\nŒ≥\nQ\n~\nŒº\n‚àí\n(\ns\nt\n+\n1\n,\nz\nt\n+\n1\n,\na\nt\n+\n1\n)\n|\ns\nt\n=\ns\n,\nz\nt\n=\nz\n,\na\nt\n=\na\n]\n.\n\\displaystyle\\tilde{Q}^{\\mu^{-}}(s,z,a)={\\mathbb{E}}^{\\mu^{-}}\\left[r_{t}+\\gamma\\tilde{Q}^{\\mu^{-}}(s_{t+1},z_{t+1},a_{t+1})\\middle|s_{t}=s,z_{t}=z,a_{t}=a\\right].\n(37)\nSince the underlying Bellman operator is\nŒ≥\n\\gamma\n-contractive, these equations have a unique solution\nQ\n~\nŒº\n‚àí\n\\widetilde{Q}^{\\mu^{-}}\n.\nBecause the environment and agent states form a Markovian variable\n(\ns\n,\nz\n)\n(s,z)\n, by definition of\nQ\nŒº\n‚àí\nQ^{\\mu^{-}}\n, we have\nQ\n~\nŒº\n‚àí\n=\nQ\nŒº\n‚àí\n\\tilde{Q}^{\\mu^{-}}=Q^{\\mu^{-}}\n.\nAs a consequence, we also have\nV\n~\nŒº\n‚àí\n=\nV\nŒº\n‚àí\n\\tilde{V}^{\\mu^{-}}=V^{\\mu^{-}}\nwhere\nV\n~\nŒº\n‚àí\n\\tilde{V}^{\\mu^{-}}\nis the unique fixed point of its analogous Bellman operator.\nLet us now consider the symmetric Bellman equations for the environment-agent state MDP,\nQ\n~\nŒº\n‚àí\n(\nz\n,\na\n)\n=\nùîº\ns\n‚Ä≤\n‚àº\nd\nŒº\n‚àí\n‚Äã\n(\ns\n‚Ä≤\n‚à£\nz\n)\n[\nùîº\nŒº\n‚àí\n[\nr\nt\n+\nŒ≥\nQ\n~\nŒº\n‚àí\n(\nz\nt\n+\n1\n,\na\nt\n+\n1\n)\n|\ns\nt\n=\ns\n‚Ä≤\n,\nz\nt\n=\nz\n,\na\nt\n=\na\n]\n]\n.\n\\displaystyle\\tilde{Q}^{\\mu^{-}}(z,a)=\\mathop{\\mathbb{E}}_{s^{\\prime}\\sim d_{\\mu^{-}}(s^{\\prime}\\mid z)}\\left[{\\mathbb{E}}^{\\mu^{-}}\\left[r_{t}+\\gamma\\tilde{Q}^{\\mu^{-}}(z_{t+1},a_{t+1})\\middle|s_{t}=s^{\\prime},z_{t}=z,a_{t}=a\\right]\\right].\n(38)\nIt is interesting to note that, by bootstrapping with\nQ\n~\nŒº\n‚àí\n\\tilde{Q}^{\\mu^{-}}\n, this Q-function considers the distribution of the state\n(\ns\nt\n+\n1\n,\nz\nt\n+\n1\n‚à£\ns\nt\n,\nz\nt\n,\na\nt\n)\n(s_{t+1},z_{t+1}\\mid s_{t},z_{t},a_{t})\nfrom the second timestep to be\np\n‚Äã\n(\nz\nt\n+\n1\n‚à£\ns\nt\n,\nz\nt\n,\na\nt\n)\n‚Äã\nd\nŒº\n‚àí\n‚Äã\n(\ns\nt\n+\n1\n|\nz\nt\n+\n1\n)\np(z_{t+1}\\mid s_{t},z_{t},a_{t})d_{\\mu^{-}}(s_{t+1}|z_{t+1})\ninstead of the true distribution\np\n‚Äã\n(\ns\nt\n+\n1\n,\nz\nt\n+\n1\n‚à£\ns\nt\n,\nz\nt\n,\na\nt\n)\np(s_{t+1},z_{t+1}\\mid s_{t},z_{t},a_{t})\n. As a result, by telescoping, we obtain,\nQ\n~\nŒº\n‚àí\n(\nz\n,\na\n)\n=\nùîº\ns\n‚àº\nd\nŒº\n‚àí\n‚Äã\n(\ns\n‚à£\nz\n)\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\nùîº\nŒº\n‚àí\n[\nùîº\ns\nt\n‚Ä≤\n‚àº\nd\nŒº\n‚àí\n‚Äã\n(\ns\nt\n‚Ä≤\n‚à£\nz\nt\n)\nr\n(\ns\nt\n‚Ä≤\n,\nz\nt\n,\na\nt\n)\n|\ns\n0\n=\ns\n,\nz\n0\n=\nz\n,\na\n0\n=\na\n]\n]\n\\displaystyle\\tilde{Q}^{\\mu^{-}}(z,a)=\\mathop{\\mathbb{E}}_{s\\sim d_{\\mu^{-}}(s\\mid z)}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{E}^{\\mu^{-}}\\left[\\mathop{\\mathbb{E}}_{s_{t}^{\\prime}\\sim d_{\\mu^{-}}(s_{t}^{\\prime}\\mid z_{t})}r(s_{t}^{\\prime},z_{t},a_{t})\\middle|s_{0}=s,z_{0}=z,a_{0}=a\\right]\\right]\n(39)\nwhere\nr\n‚Äã\n(\ns\n,\nz\n,\na\n)\n=\nùîº\n‚Äã\n[\nr\nt\n‚à£\ns\nt\n=\ns\n,\nz\nt\n=\nz\n,\na\nt\n=\na\n]\nr(s,z,a)=\\mathbb{E}[r_{t}\\mid s_{t}=s,z_{t}=z,a_{t}=a]\n. It contrasts with the true Q-function, which writes,\nQ\nŒº\n‚àí\n‚Äã\n(\ns\n,\nz\n,\na\n)\n\\displaystyle Q^{\\mu^{-}}(s,z,a)\n=\nùîº\nŒº\n‚àí\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\nr\nt\n|\ns\n0\n=\ns\n,\nz\n0\n=\nz\n,\na\n0\n=\na\n]\n\\displaystyle={\\mathbb{E}}^{\\mu^{-}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\middle|s_{0}=s,z_{0}=z,a_{0}=a\\right]\n(40)\n=\nùîº\ns\n‚àº\nd\nŒº\n‚àí\n‚Äã\n(\ns\n‚à£\nz\n)\n[\n‚àë\nt\n=\n0\n‚àû\nŒ≥\nt\nùîº\nŒº\n‚àí\n[\nr\n(\ns\nt\n,\nz\nt\n,\na\nt\n)\n|\ns\n0\n=\ns\n,\nz\n0\n=\nz\n,\na\n0\n=\na\n]\n]\n\\displaystyle=\\mathop{\\mathbb{E}}_{s\\sim d_{\\mu^{-}}(s\\mid z)}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{E}^{\\mu^{-}}\\left[r(s_{t},z_{t},a_{t})\\middle|s_{0}=s,z_{0}=z,a_{0}=a\\right]\\right]\n(41)\nIt can be concluded that the unprivileged fixed point\nQ\n~\nŒº\n‚àí\n\\tilde{Q}^{\\mu^{-}}\nand the unprivileged Q-function\nQ\nŒº\n‚àí\n{Q}^{\\mu^{-}}\ncan be different, as soon as the distribution\np\n‚Äã\n(\ns\nt\n,\nz\nt\n,\na\nt\n‚à£\ns\n0\n,\nz\n0\n,\na\n0\n)\np(s_{t},z_{t},a_{t}\\mid s_{0},z_{0},a_{0})\nis different from\np\n‚Äã\n(\nz\nt\n,\na\nt\n‚à£\ns\n0\n,\nz\n0\n,\na\n0\n)\n‚Äã\nd\nŒº\n‚àí\n‚Äã\n(\ns\nt\n‚à£\nz\nt\n)\np(z_{t},a_{t}\\mid s_{0},z_{0},a_{0})d^{\\mu^{-}}(s_{t}\\mid z_{t})\nat any timestep\nt\nt\n.\nAppendix F\nActive Perception Experimental Details\nF.1\nTask Definition\nIn these tasks, the robot must find objects placed out of view in cluttered environments. Similar to how a human may try to find ingredients in a messy kitchen, the robot must move its camera around occluders, zoom into hard to see spots (i.e. behind drawers), and zoom out to increase its overall view of the scene.\nFigure 7:\nFranka Robot Search Diagram\n: A visual overview of the Franka Active Perception experiments. Top half: Offline trajectories are collected through teleoperation collecting privileged masks and unprivileged wrist image and history features. The policy is trained using AAWR. Bottom half: During evaluation, only the unprivileged search policy is run to perform active perception to find the object, before switching to the generalist policy. The policy‚Äôs search behavior is graded with the rubric.\nIn this experiment, we train a ‚Äúhelper‚Äù active perception policy that searches the scene for the target object, and once located, hands off control to a generalist VLA policy to pick up the object. This addresses a weakness of such generalist policies - that they are typically trained in fully observed situations, and do not generalize to partial observations.\nWe set up four tasks, ordered in increasing complexity, where the robot must either find a toy\npineapple\nor\nduck\n. Objects and visual distractors are randomly placed at the beginning of each episode, the bookshelf and cabinet are placed in same position.\n‚Ä¢\nBookshelf-P\nand\nBookshelf-D\n: The robot must find either the pineapple or duck in a three-tier vertical bookshelf with multiple visual distractors (Fig.\nSection\nÀú\nF.3\n).\n‚Ä¢\nShelf-Cabinet\n: We make the scene more difficult by adding a cabinet on the left side. The cabinet creates additional hiding spots on its top drawer and shelf over the drawer.\n‚Ä¢\nComplex\n: In addition to the bookshelf and cabinet, we add a horizontal bookshelf. There are several completely occluded spots, such as the bottom cabinet drawer and objects placed in the horizontal bookshelf.\nFigure 8:\nThe Search % metric gives points for spotting, approaching, and fixating on the object.\nF.2\nMetrics\nSearch‚Äâ%\n: a 3-point rubric for grading search behavior, see\nFigure\nÀú\n8\nfor an example.\n1.\nPolicy spots the target object anywhere in the image\n[33%]\n.\n2.\nPolicy moves until target object falls into the target region of viewpoint\n[66%]\n.\n3.\nPolicy fixates on the target object inside the target region\n[100%]\n.\nCompletion\n: the grasping success rate of\nœÄ\n0\n\\pi_{0}\n(100 timestep limit), after switching from the active perception policy.\nSteps\n: mean number of steps for the policy to complete the first two stages of the rubric (finding and approaching). Episodes that fail to reach the first two stages count as a timeout (\nT\nm\n‚Äã\na\n‚Äã\nx\n=\n300\nT_{max}=300\nsteps).\nIn\nTable\nÀú\n5\n, we also compute normalized metrics to take into account time efficiency, by dividing Search and Completion by the Steps. We then compare all methods relative to the Exhaustive baseline, by dividing the time-normalized Search and Completion metrics by the Exhaustive baselines‚Äô time-normalized Search and Completion.\nF.3\nHardware and Scene Setup\nWe used the DROID robot setup\n[\nkhazatsky2024droid\n]\n, which consists of a 7 DoF Franka Emika Panda Robot Arm, a Robotiq 2F-85 parallel-jaw gripper, a wrist-mounted ZED Mini RGB-D camera and two side-mounted ZED 2 stereo cameras. The DROID set-up enables the usage of the generalist VLA policy\nœÄ\n0\n\\pi_{0}\n[\nintelligence2025pi\n]\n, specifically the FAST-DROID checkpoint.\n\\captionof\nfigureRobot hardware configuration: Franka Panda arm with wrist & side view camera.\n\\captionof\nfigureThe Complex task has heavy occlusion in the left cabinet, and bookshelf on the floor.\nF.4\nPolicy Design\n1.\nObservation Space\n:\n‚Ä¢\nPartial observation\n: wrist RGB 84\n√ó\n\\times\n84 image, end effector position of last 6 timesteps, occupancy grid feature.\n‚Ä¢\nPrivileged observation\n: Segmentation mask and bounding box of the target object.\n2.\nAction Space\n: Cartesian and angular velocities of end effector frame,\na\nt\n=\n[\nv\nx\n,\nv\ny\n,\nv\nz\n,\nœâ\nroll\n,\nœâ\npitch\n,\nœâ\nyaw\n]\na_{t}=[v_{x},v_{y},v_{z},\\omega_{\\mathrm{roll}},\\omega_{\\mathrm{pitch}},\\omega_{\\mathrm{yaw}}]\n. We use action chunks of length 5.\nThe search policy and privileged critic networks are constructed using an encoder / head architecture. We first detail the input processing steps below, as they are shared for AAWR, AWR and BC network:\nThe wrist image is first fed into a frozen DINO-V2\n[\noquab2023dinov2\n]\nencoder (ViT-S14) , and the resulting DINO-V2 features are reduced into a\n256\n√ó\n16\n256\\times 16\ndimensional latent using PCA. Next, the occupancy grid feature is constructed by projecting the historical camera rays (inferred through the gripper position) into the XZ dimension of the robot frame, resulting in a 2D occupancy grid where elements are 1 if the camera ray has passed through it in the episode.\nThe search policy takes in the wrist image features, a history of the last 6 gripper positions, and the occupancy grid feature. The occupancy grid is first processed through a convolutional encoder, and then is concatenated alongside the gripper position history and wrist image features. This latent is then fed into a small MLP to generate the action prediction.\nThe privileged critic networks also use the same inputs as the search policy, and take in an additional privileged target object segmentation mask. The mask is processed using a small convolutional encoder, and is concatenated with the other inputs.\nTo handoff from the search policy to\nœÄ\n0\n\\pi_{0}\n, we implement the following logic. Every 5 timesteps, we query an object detector to see if a target object is detected. If the target object detected in the previous query from 5 timesteps ago and the current query, then we handoff to\nœÄ\n0\n\\pi_{0}\n. This consecutive mechanism was implemented to prevent premature switching to\nœÄ\n0\n\\pi_{0}\n, since the object detector is not perfect and sometimes gives false positives. We find that the consecutive criteria rules out false positives and switches correctly when the target object is within view.\nF.5\nReward Design\nThe reward function incentivizes the robot to locate, approach, and fixate on the target object. Viewpoints that score highly under this reward function feature the object prominently in the top-center region of the wrist image. We chose to incentivize putting the object in this target region because the grippers occupy the bottom half of the wrist image, and find that this particular viewpoint optimizes for grasping success of\nœÄ\n0\n\\pi_{0}\n.\nTo define the reward, we first need privileged information about the object location and size in the wrist view image. To obtain object detection and segmentation of the target object, we used the DINO-X\n[\nren2024dinoxunifiedvisionmodel\n]\nAPI and the GroundedSAM\n[\nren2024grounded\n]\nModel for Open-World Object Detection and segmentation. We use a color segmentation mask to break the tie when the object detector detects multiple potential target objects.\nDuring the training phase, we infer the wrist camera images with DINO-X, obtain the bounding box and mask of the target object, and label the reward for offline RL training. See\nFigure\nÀú\n9\nfor examples of the object detection and reward pipeline.\nThe reward function consists of three terms:\n(i)\nDistance reward\nr\ndist\n=\n1\n‚àí\ntanh\n‚Å°\n(\n10\n‚ãÖ\nD\n‚Äã\n(\nc\n,\nc\n‚àó\n)\n1000\n)\n,\nD\n‚Äã\n(\nc\n,\nc\n‚àó\n)\n‚àà\n[\n0\n,\n1000\n]\n‚Äã\npx\n,\nr_{\\text{dist}}=1-\\tanh\\!\\bigl(10\\,\\cdot\\tfrac{D(c,c^{*})}{1000}\\bigr),\\qquad D(c,c^{*})\\in[0,1000]\\;\\text{px},\nwhere\nc\n,\nc\n‚àó\nc,c^{*}\nare the centroids of the bounding box, and target region, and\nD\nD\nis the L1 distance. This term smoothly saturates to 1 as the bounding box center approaches the image center.\n(ii)\nMask‚Äìarea reward\nr\narea\n=\nclip\n‚Å°\n(\nmask_area\n,\n1000\n,\n50000\n)\n50000\n,\nr_{\\text{area}}=\\frac{\\operatorname{clip}\\bigl(\\text{mask\\_area},1000,50000\\bigr)}{50000},\nwhich is proportional to the mask area subject to lower and upper limits, encouraging the robot to find viewpoints where the object is prominently in view.\n(iii)\nMask‚Äìoverlap reward\nr\noverlap\n=\nùüô\n‚Äã\n[\nmask_overlap\n>\n0.10\n]\n,\nr_{\\text{overlap}}=\\mathbbm{1}\\!\\bigl[\\textit{mask\\_overlap}>0.10\\bigr],\ngives a sparse binary bonus once the object mask intersects with the target region, defined as\nIoU\n>\n10\n%\n\\text{IoU}>10\\%\nwith a\n128\n√ó\n128\n128\\times 128\nregion centered at\nc\n‚àó\nc^{*}\n.\nThe per‚Äëstep reward is then composed as\nR\nt\n=\n0.5\n‚Äã\nr\ndist\n+\n0.3\n‚Äã\nr\narea\n+\n10\n‚Äã\nr\noverlap\n,\n\\boxed{R_{t}=0.5\\,r_{\\text{dist}}+0.3\\,r_{\\text{area}}+10\\,r_{\\text{overlap}}},\nFigure 9:\nAn example reward trajectory in\nComplex\nscene. Notice the trajectory is given a high peak reward when the detected pineapple overlap on top of the square area.\nDemonstrations\nWe collect demonstrations using the 5‚ÄêDoF 3Dconnexion SpaceMouse. During teleoperation, we label trajectories as successful if\nœÄ\n0\n\\pi_{0}\nsuccessfully grabs the target object after switching from the teleoperator. The demonstrations are collected with four different teleoperators, with success rates for the demonstrations roughly between 50-70%. We initially collect up to 250 demonstrations per task, but then we curate the dataset, dropping out trajectories with mislabeled object detections, noisy/faulty sensor readings, etc. After filtering, we end up with 152 demonstrations for Bookshelf-P, 109 for Bookshelf-D, 35 for Shelf-Cabinet, and 195 for Complex.\nF.6\nBaselines\nPlease watch the videos on our website (\nRW-RL Project Page\n) to better compare the differences among baselines.\n1.\nAWR\n: Advantage‚ÄêWeighted Regression, no access to privileged observations.\n2.\nBC\n: Filtered Behaviour Cloning, trained on successful trajectories only.\n3.\nExhaustive\n: Hard‚Äëcoded baseline that goes over every possible hiding location in a fixed order. This \"brute-force\" method gets high Search% score but takes much longer to search search.\n4.\nVLM+\nœÄ\n0\n\\pi_{0}\n: This baseline decomposes the task using a VLM for high level task planning and the\nœÄ\n0\n\\pi_{0}\nVLA for low level movement as proposed in HiRobot\n[\nshi2025hirobot\n]\n. This approach commonly used in works that solve long-horizon tasks with only foundation models. In practice, we query the Gemini-2.5-Flash\n[\nteam2023gemini\n]\nmodel with a task prompt template, which includes a series of searching-related instructions that the low-level VLA can follow. Then, we ask Gemini to choose among them. The prompt template is attached below.\n‚¨á\nYou\nare\nan\nexpert\nrobot\noperator\nusing\nthe\npi0\npolicy\n,\na\ngeneral\n-\npurpose\nrobot\nfoundation\nmodel\n.\nIt\nreceives\na\nnatural\nlanguage\ncommand\nand\nexecutes\non\nthe\nFranka\nPanda\nrobot\narm\n.\nYour\njob\nis\nto\nprovide\nnatural\n-\nlanguage\ninstructions\nto\nhelp\na\nsingle\n-\narmed\nrobot\nwith\na\nparallel\n-\njaw\ngripper\ncomplete\na\ntabletop\nmanipulation\ntask\n.\n‚Äî\nThe\noverall\ntask\nis\n:\nfind\na\n<|>\nTARGET_OBJECT_NAME\n<|>\nin\nthe\nscene\n.\nThe\nrobot\nhas\na\nwrist\n-\nmounted\ncamera\nand\ncan\nperform\nshort\nsequences\nof\nactions\n,\nsuch\nas\nopening\ndrawers\n,\nscanning\ncompartments\n,\nand\nmoving\nits\ncamera\nviewpoint\n.\nGiven\nthe\nfollowing\nconstraints\n:\n-\nThe\n<|>\nTARGET_OBJECT_NAME\n<|>\nmight\nbe\n**\npartially\nor\nfully\noccluded\n**.\n-\nIt\ncould\nbe\nlocated\n**\ninside\ndrawers\n**,\n**\nbehind\nobjects\n**,\nor\n**\non\nshelves\n**.\n‚Äî\nYour\njob\nis\nto\nbreak\nthis\ntask\ndown\ninto\nsmaller\ninstructions\nthat\nrobot\ncan\ncomplete\nEvery\nfew\nseconds\nwe\nwill\nask\nyou\nto\nprovide\na\nnatural\nlanguage\ninstruction\nfor\nthe\nrobot\n.\nWe\nwill\nprovide\ntwo\nimages\n:\n(1)\nan\nexternal\nview\nof\nthe\nrobot\nand\n(2)\na\nview\nfrom\na\ncamera\nmounted\non\nthe\nrobot\n‚Äô\ns\nwrist\n.\nYour\ninstruction\nshould\nrefer\nto\nrelevant\nobjects\nthat\nyou\nsee\nin\nthe\nimages\n,\nand\nshould\nhelp\nthe\nrobot\nmake\nprogress\ntowards\ncompleting\nthe\noverall\ntask\n(<|>\nOVERALL_TASK\n<|>).\nTo\nhelp\nyou\n,\nwe\n‚Äô\nve\nprepared\na\nlist\nof\ninstructions\nfor\nyou\nto\nchoose\nfrom\n:\n‚Äî\nlook\naround\nfor\nthe\n<|>\nTARGET_OBJECT_NAME\n<|>\nopen\nthe\ntop\ndrawer\nopen\nthe\nbottom\ndrawer\nlook\ninside\nthe\ntop\ndrawer\nlook\ninside\nthe\nbottom\ndrawer\nlook\nbehind\nthe\ntoys\nlook\nbehind\nthe\nblue\nblock\nlook\nbehind\nthe\ngreen\nblock\nlook\non\nthe\ntop\nshelf\nlook\non\nthe\nbottom\nshelf\nmove\nthe\nred\nblock\nto\nthe\nside\nmove\nthe\nblue\nblock\nto\nthe\nside\nmove\nthe\ngreen\nblock\nto\nthe\nside\nmove\ngriper\nto\nthe\nright\nmove\ngriper\nto\nthe\nleft\nmove\ngriper\nto\nthe\ncenter\nmove\ngriper\nto\nthe\nfront\nmove\ngriper\nto\nthe\nback\nmove\ngriper\nto\nthe\ntop\nmove\ngriper\nto\nthe\nbottom\nfind\nthe\n<|>\nTARGET_OBJECT_NAME\n<|>\nand\npick\nit\nup\npick\nup\nthe\n<|>\nTARGET_OBJECT_NAME\n<|>\n‚Äî\nHere\nis\nthe\nexternal\nview\n:\n<|>\nCURRENT_IMAGE\n<|>\nHere\nis\nthe\nwrist\nview\n:\n<|>\nCURRENT_WRIST_IMAGE\n<|>\nPlease\nprovide\nan\ninstruction\nfor\nthe\nrobot\nto\nfollow\n.\nWrite\nthe\ninstruction\nin\nall\nlower\ncase\nwith\nno\npunctuation\n.\nJust\nprovide\nthe\ninstruction\n;\ndo\nnot\nprovide\nadditional\nexplanation\n.\nListing¬†1:\nPrompt Template for VLM+\nœÄ\n0\n\\pi_{0}\nbaseline\nFigure 10:\nFailure analysis of AAWR, AWR, and BC policies in all 4 tasks. For each policy, we show the number of times each policy completes the first, second and third stage of the Search % rubric. AAWR completes all three stages the most, while AWR and BC fail to consistently approach and fixate on the target object.\nF.7\nResults\nTable 4:\nMetrics for active perception handoff tasks, AAWR consistently outperforms baselines. Bold = best, underline = second best.\nMethod\nBookshelf-P\nBookshelf-D\nShelf-Cabinet\nComplex\nSearch‚Äâ%\n‚Üë\n\\uparrow\nœÄ\n0\n\\pi_{0}\n%\n‚Üë\n\\uparrow\nSteps\n‚Üì\n\\downarrow\nSearch‚Äâ%\n‚Üë\n\\uparrow\nœÄ\n0\n\\pi_{0}\n%\n‚Üë\n\\uparrow\nSteps\n‚Üì\n\\downarrow\nSearch‚Äâ%\n‚Üë\n\\uparrow\nœÄ\n0\n\\pi_{0}\n%\n‚Üë\n\\uparrow\nSteps\n‚Üì\n\\downarrow\nSearch‚Äâ%\n‚Üë\n\\uparrow\nœÄ\n0\n\\pi_{0}\n%\n‚Üë\n\\uparrow\nSteps\n‚Üì\n\\downarrow\nAAWR\n92.4\n¬±\n\\pm\n5.0\n44.4\n¬±\n\\pm\n16.6\n36.6\n¬±\n\\pm\n4.7\n81.3\n¬±\n\\pm\n6.2\n44.4\n¬±\n\\pm\n11.7\n26.9\n¬±\n\\pm\n2.0\n78.2\n¬±\n\\pm\n7.0\n40.0\n¬±\n\\pm\n11.0\n46.3\n¬±\n\\pm\n4.5\n54.8\n¬±\n\\pm\n8.5\n20.0\n¬±\n\\pm\n8.9\n121.0\n¬±\n\\pm\n30.1\nAWR\n79.6\n¬±\n\\pm\n5.6\n0.0\n¬±\n\\pm\n0.0\n34.0\n¬±\n\\pm\n2.7\n62.6\n¬±\n\\pm\n6.5\n16.7\n¬±\n\\pm\n8.8\n30.2\n¬±\n\\pm\n10.1\n52.3\n¬±\n\\pm\n6.1\n10.0\n¬±\n\\pm\n6.7\n38.0\n¬±\n\\pm\n13.9\n13.2\n¬±\n\\pm\n5.0\n10.0\n¬±\n\\pm\n6.7\n217.0\n¬±\n\\pm\n29.3\nBC\n29.9\n¬±\n\\pm\n13.5\n20.0\n¬±\n\\pm\n12.6\n84.0\n¬±\n\\pm\n9.2\n47.7\n¬±\n\\pm\n4.0\n16.7\n¬±\n\\pm\n8.8\n22.5\n¬±\n\\pm\n2.1\n28.1\n¬±\n\\pm\n5.5\n15.0\n¬±\n\\pm\n8.0\n125.0\n¬±\n\\pm\n29.6\n46.4\n¬±\n\\pm\n8.5\n10.0\n¬±\n\\pm\n6.7\n138.0\n¬±\n\\pm\n30.4\nœÄ\n0\n\\pi_{0}\n11.0\n¬±\n\\pm\n11.0\n16.7\n¬±\n\\pm\n15.2\n263.3\n¬±\n\\pm\n36.7\n66.7\n¬±\n\\pm\n21.1\n33.3\n¬±\n\\pm\n19.2\n229.7\n¬±\n\\pm\n44.8\n10.0\n¬±\n\\pm\n10.0\n10.0\n¬±\n\\pm\n9.5\n280\n¬±\n\\pm\n20.0\n29.6\n¬±\n\\pm\n15.3\n20.0\n¬±\n\\pm\n12.6\n252.5\n¬±\n\\pm\n31.7\nExhaustive\n64.2\n¬±\n\\pm\n1.8\n44.0\n¬±\n\\pm\n11.7\n105.4\n¬±\n\\pm\n9.0\n96.0\n¬±\n\\pm\n2.7\n22.2\n¬±\n\\pm\n9.8\n106.7\n¬±\n\\pm\n8.6\n52.8\n¬±\n\\pm\n5.0\n45.0\n¬±\n\\pm\n11.1\n183.0\n¬±\n\\pm\n15.3\n78.2\n¬±\n\\pm\n7.8\n30.0\n¬±\n\\pm\n10.2\n297.0\n¬±\n\\pm\n30.8\nVLM+\nœÄ\n0\n\\pi_{0}\n31.4\n¬±\n\\pm\n10.2\n27.8\n¬±\n\\pm\n10.6\n322.3\n¬±\n\\pm\n31.9\n33.2\n¬±\n\\pm\n17.1\n16.7\n¬±\n\\pm\n16.7\n281.8\n¬±\n\\pm\n18.1\n28.2\n¬±\n\\pm\n7.3\n15.0\n¬±\n\\pm\n8.0\n382\n¬±\n\\pm\n12.6\n14.8\n¬±\n\\pm\n10.2\n10.0\n¬±\n\\pm\n9.5\n374.7\n¬±\n\\pm\n25.3\nTable 5:\nActive perception handoff tasks: Metrics normalized by time, and then compared to the exhaustive search. AAWR consistently is\n‚àº\n2\n‚àí\n8\n√ó\n{\\sim}2{-}8\\times\nbetter than exhaustive search in such metrics, while other baselines have mixed results.\nMethod\nBookshelf-P\nBookshelf-D\nShelf-Cabinet\nComplex\nSearch\nCompletion\nSearch\nCompletion\nSearch\nCompletion\nSearch\nCompletion\nAAWR\n4.14\n2.91\n3.36\n7.93\n5.86\n3.51\n1.72\n1.64\nAWR\n3.84\n0.00\n2.30\n2.65\n4.77\n1.07\n0.23\n0.46\nBC\n0.58\n0.57\n2.36\n3.56\n0.78\n0.49\n1.28\n0.72\nœÄ\n0\n\\pi_{0}\n0.07\n0.15\n0.32\n0.70\n0.12\n0.15\n0.45\n0.78\nVLM+\nœÄ\n0\n\\pi_{0}\n0.16\n0.21\n0.13\n0.28\n0.26\n0.16\n0.15\n0.26\nExhaustive\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nAs seen in\nTable\nÀú\n4\n, AAWR consistently outperforms baselines in all metrics, learning sensible active perception behavior to aid a generalist policy. We report the mean and standard error over 18 trials for all metrics. AAWR always outperforms non privileged AWR and BC, thus validating the usefulness of privileged information and the use of offline RL over supervised learning.\nThe Exhaustive baseline often has high search progress and\nœÄ\n0\n\\pi_{0}\n% success rate, but is much slower than AAWR, showing that AAWR learns to search efficiently. We find that\nœÄ\n0\n\\pi_{0}\nand VLM+\nœÄ\n0\n\\pi_{0}\nboth search poorly - they tend to take inefficient movements and fail to track the object. In\nFigure\nÀú\n10\n, we break down the failures of the active perception policies in each task by recording the number of stages completed in the 3-point rubric. Across all tasks, AAWR completes the task (all three stages) the most. AWR and BC often spot the object, but do not consistently approach and fixate on the object.\nIn the\nBookshelf\ntasks, AAWR first learns to zoom out of the scene to see multiple shelves, then scans from bottom to up, and then approaches the target object once located. The AWR and BC baselines follow a relatively fixed search path that approaches the shelf, but the policies failed to efficiently scan the shelves. Even if they luckily glimpse the target object, they do not fixate on the object, reducing their search score and\nœÄ\n0\n\\pi_{0}\nsuccess rate. See\nFigure\nÀú\n6\nfor a visualization.\nIn the\nShelf-Cabinet\ntask, AAWR searches through the right bookshelf, before moving to the left cabinet. Both AWR and BC do not thoroughly search the scene, preventing them from finding objects placed in the left cabinet‚Äôs drawer.\nIn\nComplex\n, AAWR searches the bottom shelf, the right shelf, and then the left cabinet (see\nFigure\nÀú\n1\n). See the\nwebsite\nfor comprehensive success and failure recordings of the policies over all tasks.\nF.8\nDataset ablation on the Complex task.\nWe ablate the demonstration quality by collecting two demonstration datasets. First, a suboptimal dataset of 195 demonstrations is collected by four different teleoperators. The dataset is quite diverse, suboptimal (success rate of 60%), and potentially hard to learn behaviors from. We found that all policies trained on the initial dataset struggled to reproduce certain behaviors, in particular moving to the left side of the scene, despite the existence of such trajectories in the dataset.\nNext, we collected a small 50 trajectory dataset from only one expert teleoperator, with a high success rate of 94%. As seen in\nTable\nÀú\n6\n, all approaches improve using the smaller, more optimal optimal dataset. AAWR still outperforms baselines, as it consistently approaches and fixates on the objects, maximizing the success rate of\nœÄ\n0\n\\pi_{0}\nafter handoff. In contrast, AWR and BC do not approach and fixate target object as well as AAWR, often switching to\nœÄ\n0\n\\pi_{0}\nwhen the target object is barely in view or in an odd location with respect to the gripper.\nTable 6:\nWe ablate the demonstration dataset of the Complex task, and find that all approaches benefit from a smaller but more optimal demonstration source. AAWR still outperforms all baselines.\nMethod\nComplex (Suboptimal Demos)\nComplex (Expert Demos)\nSearch‚Äâ%\n(‚Üë)\nœÄ\n0\n\\pi_{0}\n%\n(‚Üë)\nSteps\n(‚Üì)\nSearch‚Äâ%\n(‚Üë)\nœÄ\n0\n\\pi_{0}\n%\n(‚Üë)\nSteps\n(‚Üì)\nAAWR\n54.8\n20.0\n121.0\n73.2\n50.0\n43\nAWR\n13.2\n10.0\n217.0\n33.2\n40.0\n67\nBC\n46.4\n10.0\n138.0\n31.5\n15.0\n77\nœÄ\n0\n\\pi_{0}\n29.6\n20.0\n252.5\n29.6\n20.0\n252.5\nExhaustive\n78.2\n30.0\n297.0\n78.2\n30.0\n297.0\nVLM+\nœÄ\n0\n\\pi_{0}\n14.8\n10.0\n374.7\n14.8\n10.0\n374.7\nF.9\nReward Quality Analysis\nWe further analyze how reward signal quality and annotation noise affect search behaviors. In experiments, all episode begins without the target object in view, and the robot must actively search until it detects the object with visible and confident masks. Ideally, the object is in a clear view see\nFigure\nÀú\n9\n, where we give a high terminal reward. Consequently, early reward spikes typically indicate mislabeled frames.\nFor each dataset, we label the mislabeled episode using a heuristic. An episode is flagged as mislabeled if its reward exceeds a fixed threshold of\n5.0\n5.0\nwithin the first 30% timesteps. We counted the mislabeling rate of success trials, failure trials and total trials of each search datasets below:\nTable 7:\nPer-task policy performance and reward noise.\nTask\nSearch‚Äâ%\n(‚Üë)\nCompletion‚Äâ%\n(‚Üë)\nMislabel‚Äâ% (Succ./Fail/Total)\nObservation\nBookshelf-P\n92.4\n¬±\n5.0\n92.4\\pm 5.0\n44.4\n¬±\n16.6\n44.4\\pm 16.6\n4.6 / 0 / 2.7\nClean reward\nBookshelf-D\n81.3\n¬±\n6.2\n81.3\\pm 6.2\n44.4\n¬±\n11.7\n44.4\\pm 11.7\n75 / 56.8 / 68.8\nNoisy reward\nShelf-Cabinet\n78.2\n¬±\n7.0\n78.2\\pm 7.0\n40.0\n¬±\n11.0\n40.0\\pm 11.0\n17.1 / 30.8 / 23.8\nHarder scene with longer horizon\nComplex: Suboptimal\n54.8\n¬±\n8.5\n54.8\\pm 8.5\n20.0\n¬±\n8.9\n20.0\\pm 8.9\n13.4 / 11.8 / 12.8\nDiverse dirty data.\nComplex: Expert\n73.2\n¬±\n12.0\n73.2\\pm 12.0\n50.0\n¬±\n15.8\n50.0\\pm 15.8\n6.4 / 0 / 6.0\nExpert clean data.\nAs the table shows, higher quality data (e.g. clean, sparse reward with less detection error) correspond to better search and completion rates. Despite noisy rewards in Bookshelf-D, AAWR is still able to learn good search behavior.\nAppendix G\nBlind Pick Details\nTask Definition\nIn this real world experiment, the Koch robot must pick up a small rectangular candy (1 cm √ó 1 cm √ó 2 cm).\nIt operates blindly since it is given only the initial object position and its own joint positions during the episode. Interactive perception is required to solve the task, since the robot must sense when the object is gripped using its proprioception and then proceed to lift it up.\n‚Ä¢\nObservation\n: Initial object position, and current robot joint positions.\n‚Äì\nPartial observation\n: joint positions and initial Cartesian position of the target.\n‚Äì\nPrivileged observation\n: real time Cartesian position of the target at each timestep.\n‚Ä¢\nAction Space\n: Cartesian position commands relative to robot base and gripper joint control.\n\\captionof\nfigureHardware configuration: Koch robot with RGB-D camera in the back.\n\\captionof\nfigureKoch robot picking up the target object.\nHardware and Scene Setup\nWe utilized a Kochv1.1 robot\n[\nmoss2025kochv11\n]\n, an open-source, low-cost, 5 DoF robotic arm. The robot arm is operated via a Cartesian position controller respect to the robot base frame. The forward and inverse kinematic computations is computed in a MuJoCo simulation model synchronized with real robot‚Äôs joint positions in real-time.\nTo get the privileged cartesian position of the object, we set up a RealSense D455i RGBD camera pointed towards the robot workspace. We calibrate the D455i using an ArUco marker, and then use color segmentation to filter the point cloud to estimate the 3d position of the object on the table. The target object is randomly placed within a 10cm square region in front of the robot at the start of each trial.\nData Collection\nData for the Koch robot experiment was collected by executing approximately 100 demonstration episodes, in total containing around 3000 transitions. The demonstrations were gathered from a noisy hand-coded script, resulting in a success rate of approximately 20%.\nReward Design\n1.\nDistance penalty\nThe distance penalty is the reward term where we introduce privileged information: the real-time position of the target object. Th reward term is computed by:\nr\nt\n=\n‚àí\n‚Äñ\nx\nt\n‚àí\nx\n‚àó\n‚Äñ\n,\nr_{t}=-\\left\\lVert x_{t}-x^{*}\\right\\rVert,\nwhere\nx\nt\nx_{t}\nis the current Euclidean position of the target object computed via color segmentation and the depth camera,\nx\nx\nis the current Euclidean position of the robot‚Äôs end effector.\n2.\nGrasp reward\nUsing robot proprioception, we can determine if the gripper has a firm grasp on an object. More specifically, if the gripper receives a closing command, but the actuator cannot rotate the gripper to the commanded position, a firm grasp is detected by proprioception. Therefore, we have:\nr\ng\n‚Äã\nr\n‚Äã\na\n‚Äã\ns\n‚Äã\np\n=\nk\ng\n‚Äã\nr\n‚Äã\na\n‚Äã\ns\n‚Äã\np\n‚Äã\nùüô\n{\ngrasped\nt\n=\nTrue\n}\nr_{grasp}=k_{grasp}\\mathbbm{1}_{\\{\\text{grasped}_{t}=\\text{True}\\}}\n3.\nSuccess reward\nA larger reward when the robot fully accomplishes the task: picking up the target object and lifts it 7 cm above the robot‚Äôs base. In particular, the reward is given by:\nr\ns\n‚Äã\nu\n‚Äã\nc\n‚Äã\nc\n‚Äã\ne\n‚Äã\ns\n‚Äã\ns\n=\nk\ng\n‚Äã\nr\n‚Äã\na\n‚Äã\ns\n‚Äã\np\n‚Äã\nùüô\n{\nz\ne\n‚Äã\ne\n‚àí\nz\nb\n‚Äã\na\n‚Äã\ns\n‚Äã\ne\n>\n0.07\n‚àß\ngrasped\nt\n=\nT\n‚Äã\nr\n‚Äã\nu\n‚Äã\ne\n}\n,\nr_{success}=k_{grasp}\\mathbbm{1}_{\\{z_{ee}-z_{base}>0.07\\land\\text{grasped}_{t}=True\\}},\nwhere\nz\ne\n‚Äã\ne\nz_{ee}\nis the z-axis position of the end effector,\nz\nb\n‚Äã\na\n‚Äã\ns\n‚Äã\ne\nz_{base}\nis the z-axis position of the robot base.\nBaselines\n1.\nBC\n: Behavior Cloning using offline successful demonstrations only.\n2.\nAWR\n: Advantage Weighted Regression trained both offline and online.\n3.\nAAWR\n: Asymmetric Advantage Weighted Regression leveraging privileged information during offline and online training phases.\nMetrics\nPerformance is evaluated across 40 trials per method with the following metrics:\n1.\nGrasp %\n: Percentage of trials in which the robot successfully grasped the object.\n2.\nPick %\n: Percentage of trials where the robot successfully grasped and lifted the object.\nOnline Training and Evaluation\nAll methods underwent an initial 20,000-step offline pretraining phase followed by online fine-tuning using 1,200 transitions (¬†40-50 episodes), each lasting approximately 20 minutes.\nDuring evaluation, for each trial, a policy has 30 timesteps to accomplish the task. Evaluation results confirmed AAWR‚Äôs superior performance in grasping and picking success rates and demonstrated notably effective retrying behaviors following failed initial attempts.\nAppendix H\nSimulated Experimental Details\nH.1\nCamouflage Pick\nThe Camouflage Pick experiment requires a simulated Koch robot to pick up a tiny, hard-to-see marble initialized in a 10\n√ó\n\\times\n10 cm region (see\nFigure\nÀú\n11\n).\n‚Ä¢\nObservation Space\n:\n‚Äì\nPartial observation\n: 3rd person 84\n√ó\n\\times\n84 image of the robot and marble.\n‚Äì\nPrivileged observation\n: robot and marble positions using simulator state\n‚Ä¢\nAction Space\n: Cartesian velocity of end effector frame and gripper position\n‚Ä¢\nReward Function\n: Sparse reward that gives\n+\n1\n+1\nif the marble is in gripper and altitude is over 7cm.\n‚Ä¢\nDemonstrations\n: We collect 100 demos using a hand-coded script. The script is not perfect and gets around 30% success rate.\n‚Ä¢\nOffline / Online budget: 20K offline, 80K online\n(a)\nAAWR\n(b)\nAWR\n(c)\nBC\nFigure 11:\nCamouflage Pick (third-person camera only).\n(a)\nAAWR\n: succeeds with search then grasp.\n(b)\nAWR\n: fails due to mis-grasps.\n(c)\nBC\n: fails due to overfit on grasping without search\nWe compare against symmetric AWR, the non-privileged version of AAWR, and BC. We train BC for 20,000 offline steps, periodically checkpointing and evaluating it, and report the highest performing checkpoint. We pretrain AWR and AAWR for 20,000 offline steps. Then, we do online finetuning for 80,000 environment steps. While training, we periodically evaluate the policies by recording their average success over 100 trials. The success metric is the sparse reward function.\nWe train all models with a batch size of 256, learning rate of 0.0001, and the Adam optimizer. For online finetuning following\n[\nfeng2023finetuning\n]\n, we use an update-to-date ratio of 1 , performing gradient updates after every episode. For AWR and AAWR, we use an advantage temperature of 10.\nWe instantiate separate networks for the for the policy and value/critic networks. We use the same encoder / head recipe for all models, following\n[\nfeng2023finetuning\n]\n. We use a CNN to process the RGB image into a 50-dimensional latent, and a MLP to process the privileged information into a 50-dimensional latent. The latents are then fed into a MLP to get the output.\nAs seen in\nFigure\nÀú\n5\n, AAWR outperforms baselines. AWR and BC frequently completely miss the marble, while AAWR displays more accurate picking behavior. Even after picking, the small marble frequently slips out of the grasp, making the success rate for all of the policies rather low. See the website for videos.\nH.2\nFully Observed Pick\nThe Fully Observed Pick experiment requires a simulated xArm6 robot to pick up a block. To make the scene as fully observable as possible, we make the xArm6 robot invisible except for its grippers, making occlusion of the object by the robot impossible (see\nFigure\nÀú\n12\n). The object is randomly initialized in a\n25\n√ó\n25\n25\\times 25\ncm region in front of the arm.\n‚Ä¢\nObservation Space\n:\n‚Äì\nPartial observation\n: 3rd person 84\n√ó\n\\times\n84 image of the robot and block.\n‚Äì\nPrivileged observation\n: robot and object positions using simulator state\n‚Ä¢\nAction Space\n: Cartesian velocity of end effector frame and gripper position\n‚Ä¢\nReward Function\n: Sparse reward that gives\n+\n1\n+1\nif the block is in gripper and altitude is over 10cm.\n‚Ä¢\nDemonstrations\n: 100 demos using a hand-coded script. The script is not perfect and gets around 30% success rate.\n‚Ä¢\nOffline / Online budget: 20K offline, 20K online\nWe use the same baselines, hyperparameters, network, and evaluation configuration as the Camouflage Pick experiment. The only change is the offline / online budget - 20K steps offline, and 20K steps online.\n(a)\n(b)\n(c)\n(d)\nFigure 12:\nVisually ambiguous pick (fully observed). (a‚Äìb)\nAAWR\n: grasp centers then lifts (success).\n(c‚Äìd)\nAWR\n: hovers off-center, closes mid-air (fail).\nResults are in\nFigure\nÀú\n5\n. AAWR effectively solves the task with near perfect success rate, learning to accurately localize and grasp the object. AWR shows two failure modes. First, it struggles with positioning the gripper over the block. It often places the gripper in front of the block, which looks like a reasonable grasp from the camera angle, but is in reality quite off from the block (see\nFigure\nÀú\n12\n). Next when it is able to grasp the block, it does not lift up the block. BC displays similar failure modes. See the website for videos of the policies.\nH.3\nActive Perception Koch\nIn this task, we equip the simulated Koch robot with a wrist camera with a small field of view, and task it to pick up a cube randomly initialized in a 10\n√ó\n\\times\n20 cm region in front of it. (see\nFigure\nÀú\n13\n).\nAs the wrist camera has a narrow field of view and becomes self-occluded during grasp closure, the robot must first scan to rediscover the cube before executing the pick.\n‚Ä¢\nObservation Space\n:\n‚Äì\nPartial observation\n: Frame-stacked (past 3) grayscale wrist camera images of size\n84\n√ó\n84\n84\\times 84\n.\n‚Äì\nPrivileged observation\n: object positions using simulator state\n‚Ä¢\nAction Space\n: Cartesian velocity of end effector frame and gripper position\n‚Ä¢\nReward Function\n: Sparse reward that gives\n+\n1\n+1\nif the object is in gripper and altitude is over 7cm.\n‚Ä¢\nDemonstrations\n: We collect 100 demos using a hand-coded script. The scripted behavior uses state information to command the robot to go over the block and pick it up. The script is not perfect and gets around 30% success rate.\n‚Ä¢\nOffline / Online budget: 100K offline, 900K online\nFigure 13:\nWrist camera with limited field of view.\nWhen the gripper approaches and closes on the cube, the wrist camera becomes self-occluded,\nThe side view is shown for visualization/evaluation only and is not provided to the policy.\nWe compare against other approaches that use privileged information. The first is\nDistillation\n[\nczarnecki2019distilling\n,\nchen2023sequential\n]\n, which features a two-stage training process. In the first stage, teacher acquisition, a privileged teacher policy is trained on the collected successful demonstrations. In the second phase, distillation, the teacher is distilled into a student policy. Following\n[\nczarnecki2019distilling\n]\n, the distillation phase is performed over online rollouts from the student policy. In our setup, after the first stage, the teacher policy is able to get near perfect success rate on the task, although note that it is using privileged information to do so.\nThe second baseline is a variational information bottleneck approach\nVIB\n[\nhsu2022visionbased\n]\nthat trades off the RL return with a KL penalty for accessing privileged information. Concretely, this penalty is implemented by defining a privileged latent that comes from the posterior\nz\n‚àº\np\n(\n‚ãÖ\n‚à£\no\n+\n)\nz\\sim p(\\cdot\\mid o^{+})\n, and constraining the posterior to the unprivileged gaussian prior\nùí©\n‚Äã\n(\n0\n,\nI\n)\n\\mathcal{N}(0,I)\nvia KL divergence. During training, the policy\nœÄ\n‚Äã\n(\na\n‚à£\no\n,\nz\n)\n\\pi(a\\mid o,z)\nuses the latent from the privileged posterior, and during evaluation uses a latent sampled from the unprivileged prior.\nThe RL agent should learn to minimally use privileged information, since usage will negatively impact its overall return. We conduct sweeps over different weights of the KL term\nŒ≤\n=\n0.01\n,\n0.1\n,\n0.5\n,\n1\n,\n10\n\\beta=0.01,0.1,0.5,1,10\n, report the performance of the best performing weight (\nŒ≤\n=\n0.5\n\\beta=0.5\n).\nAll baselines are implemented in the same codebase, using the same encoder / head architecture configuration as AAWR. We conduct sanity checks to make sure the baselines work, such as making sure the privileged teacher and VIB policy with the privileged latent get high success rates.\n(a) AAWR\n(b) Distillation\n(c) VIB\nFigure 14:\nResulting behaviors on the Koch task.\n(a)\nAAWR\nactively scans the workspace, recenters the cube in view, grasps, and lifts with near 100% success at evaluation.\n(b)\nDistillation\nlearns a suboptimal ‚Äúgo-to-center‚Äù strategy and often closes off-target due to the absence of scanning in the teacher.\n(c)\nVIB\ndegrades at evaluation without privileged information; using only a prior latent leads to drift and low success.\nAs seen in\nFigure\nÀú\n14\n, only AAWR learns to do active perception by scanning the workspace, getting near 100% success rate during evaluation time. The distilled student learns a suboptimal behavior of just approaching the center of the workspace, because its privileged teacher never displays the scanning behavior. VIB does poorly during evaluation with no access to privileged information, even though it was trained to minimally use privileged information.\nAdditional video examples are available on our project page at\nhttps://penn-pal-lab.github.io/aawr/\n.\nNeurIPS Paper Checklist\n1.\nClaims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper‚Äôs contributions and scope?\nAnswer:\n[Yes]\nJustification: We provide experimental evidence and theoretical analysis for our claims.\nGuidelines:\n‚Ä¢\nThe answer NA means that the abstract and introduction do not include the claims made in the paper.\n‚Ä¢\nThe abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\n‚Ä¢\nThe claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\n‚Ä¢\nIt is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\n2.\nLimitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer:\n[Yes]\nJustification: We outline limitations in the conclusions section.\nGuidelines:\n‚Ä¢\nThe answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\n‚Ä¢\nThe authors are encouraged to create a separate \"Limitations\" section in their paper.\n‚Ä¢\nThe paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\n‚Ä¢\nThe authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\n‚Ä¢\nThe authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\n‚Ä¢\nThe authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\n‚Ä¢\nIf applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\n‚Ä¢\nWhile the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren‚Äôt acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\n3.\nTheory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\nAnswer:\n[Yes]\nJustification: We provide proofs and assumptions for all theoretical claims in the appendix.\nGuidelines:\n‚Ä¢\nThe answer NA means that the paper does not include theoretical results.\n‚Ä¢\nAll the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.\n‚Ä¢\nAll assumptions should be clearly stated or referenced in the statement of any theorems.\n‚Ä¢\nThe proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.\n‚Ä¢\nInversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.\n‚Ä¢\nTheorems and Lemmas that the proof relies upon should be properly referenced.\n4.\nExperimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\nAnswer:\n[Yes]\nJustification: We provide experimental details in the main text and appendix.\nGuidelines:\n‚Ä¢\nThe answer NA means that the paper does not include experiments.\n‚Ä¢\nIf the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\n‚Ä¢\nIf the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\n‚Ä¢\nDepending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\n‚Ä¢\nWhile NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\n(a)\nIf the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\n(b)\nIf the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\n(c)\nIf the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\n(d)\nWe recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.\n5.\nOpen access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\nAnswer:\n[Yes]\nJustification: We will release code for the algorithm and environments.\nGuidelines:\n‚Ä¢\nThe answer NA means that paper does not include experiments requiring code.\n‚Ä¢\nPlease see the NeurIPS code and data submission guidelines (\nhttps://nips.cc/public/guides/CodeSubmissionPolicy\n) for more details.\n‚Ä¢\nWhile we encourage the release of code and data, we understand that this might not be possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\n‚Ä¢\nThe instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\nhttps://nips.cc/public/guides/CodeSubmissionPolicy\n) for more details.\n‚Ä¢\nThe authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\n‚Ä¢\nThe authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\n‚Ä¢\nAt submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\n‚Ä¢\nProviding as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.\n6.\nExperimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\nAnswer:\n[Yes]\nJustification: See appendix for all details.\nGuidelines:\n‚Ä¢\nThe answer NA means that the paper does not include experiments.\n‚Ä¢\nThe experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\n‚Ä¢\nThe full details can be provided either with the code, in appendix, or as supplemental material.\n7.\nExperiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\nAnswer:\n[Yes]\nJustification: See appendix for all details.\nGuidelines:\n‚Ä¢\nThe answer NA means that the paper does not include experiments.\n‚Ä¢\nThe authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.\n‚Ä¢\nThe factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).\n‚Ä¢\nThe method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)\n‚Ä¢\nThe assumptions made should be given (e.g., Normally distributed errors).\n‚Ä¢\nIt should be clear whether the error bar is the standard deviation or the standard error of the mean.\n‚Ä¢\nIt is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.\n‚Ä¢\nFor asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).\n‚Ä¢\nIf error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.\n8.\nExperiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\nAnswer:\n[Yes]\nJustification: We provide details in the appendix.\nGuidelines:\n‚Ä¢\nThe answer NA means that the paper does not include experiments.\n‚Ä¢\nThe paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\n‚Ä¢\nThe paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.\n‚Ä¢\nThe paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn‚Äôt make it into the paper).\n9.\nCode of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics\nhttps://neurips.cc/public/EthicsGuidelines\n?\nAnswer:\n[Yes]\nJustification: We have reviewed it.\nGuidelines:\n‚Ä¢\nThe answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n‚Ä¢\nIf the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\n‚Ä¢\nThe authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\n10.\nBroader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\nAnswer:\n[Yes]\nJustification: We discuss such things in a section.\nGuidelines:\n‚Ä¢\nThe answer NA means that there is no societal impact of the work performed.\n‚Ä¢\nIf the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.\n‚Ä¢\nExamples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.\n‚Ä¢\nThe conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.\n‚Ä¢\nThe authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.\n‚Ä¢\nIf there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).\n11.\nSafeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\nAnswer:\n[N/A]\nJustification: We don‚Äôt foresee a high risk for misuse .\nGuidelines:\n‚Ä¢\nThe answer NA means that the paper poses no such risks.\n‚Ä¢\nReleased models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.\n‚Ä¢\nDatasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.\n‚Ä¢\nWe recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\n12.\nLicenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\nAnswer:\n[Yes]\nJustification: We properly credit the assets we used.\nGuidelines:\n‚Ä¢\nThe answer NA means that the paper does not use existing assets.\n‚Ä¢\nThe authors should cite the original paper that produced the code package or dataset.\n‚Ä¢\nThe authors should state which version of the asset is used and, if possible, include a URL.\n‚Ä¢\nThe name of the license (e.g., CC-BY 4.0) should be included for each asset.\n‚Ä¢\nFor scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.\n‚Ä¢\nIf assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets,\npaperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.\n‚Ä¢\nFor existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.\n‚Ä¢\nIf this information is not available online, the authors are encouraged to reach out to the asset‚Äôs creators.\n13.\nNew assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\nAnswer:\n[Yes]\nJustification: We will release the trained weights of our model.\nGuidelines:\n‚Ä¢\nThe answer NA means that the paper does not release new assets.\n‚Ä¢\nResearchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.\n‚Ä¢\nThe paper should discuss whether and how consent was obtained from people whose asset is used.\n‚Ä¢\nAt submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.\n14.\nCrowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\nAnswer:\n[N/A]\nJustification: No human experiments were conducted.\nGuidelines:\n‚Ä¢\nThe answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\n‚Ä¢\nIncluding this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.\n‚Ä¢\nAccording to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.\n15.\nInstitutional review board (IRB) approvals or equivalent for research with human subjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\nAnswer:\n[N/A]\nJustification: No humans experiments were conducted.\nGuidelines:\n‚Ä¢\nThe answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\n‚Ä¢\nDepending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.\n‚Ä¢\nWe recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.\n‚Ä¢\nFor initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.\n16.\nDeclaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.\nAnswer:\n[N/A]\nJustification: We do not use LLMs as a core part of the research.\nGuidelines:\n‚Ä¢\nThe answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.\n‚Ä¢\nPlease refer to our LLM policy (\nhttps://neurips.cc/Conferences/2025/LLM\n) for what should or should not be described.",
  "preview_text": "A robot's instantaneous sensory observations do not always reveal task-relevant state information. Under such partial observability, optimal behavior typically involves explicitly acting to gain the missing information. Today's standard robot learning techniques struggle to produce such active perception behaviors. We propose a simple real-world robot learning recipe to efficiently train active perception policies. Our approach, asymmetric advantage weighted regression (AAWR), exploits access to \"privileged\" extra sensors at training time. The privileged sensors enable training high-quality privileged value functions that aid in estimating the advantage of the target policy. Bootstrapping from a small number of potentially suboptimal demonstrations and an easy-to-obtain coarse policy initialization, AAWR quickly acquires active perception behaviors and boosts task performance. In evaluations on 8 manipulation tasks on 3 robots spanning varying degrees of partial observability, AAWR synthesizes reliable active perception behaviors that outperform all prior approaches. When initialized with a \"generalist\" robot policy that struggles with active perception tasks, AAWR efficiently generates information-gathering behaviors that allow it to operate under severe partial observability for manipulation tasks. Website: https://penn-pal-lab.github.io/aawr/\n\nReal-World Reinforcement Learning of\nActive Perception Behaviors\nEdward S.¬†Hu\n‚àó,1\nJie Wang\n‚àó,1\nXingfang Yuan\n‚àó,1\nFiona Luo\n1\nMuyao Li\n1\nGaspard Lambrechts\n2\nOleh Rybkin\n3\nDinesh Jayaraman\n1\n‚àó\nEqual contribution\n1\nUniversity of Pennsylvania\n2\nUniversity of Li√®ge\n3\nUC Berkeley\nAbstract\nA robot‚Äôs instantaneous sensory observations do not always reveal task-relevant state information. Under such partial observability, optimal behavior typically involves explicitly acting to gain the missing information.\nToday‚Äôs standard robot learning techniques struggle to produce such active perception behaviors.\nWe propose a simple real-worl",
  "is_relevant": null,
  "relevance_score": 0.0,
  "extracted_keywords": [],
  "one_line_summary": "",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "2025-12-01T02:05:20Z",
  "created_at": "2026-01-05T20:53:13.242045",
  "updated_at": "2026-01-05T20:53:13.242053"
}