{
    "id": "2601.13639v1",
    "title": "A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint",
    "authors": [
        "Deyun Qin",
        "Zezhi Liu",
        "Hanqian Luo",
        "Xiao Liang",
        "Yongchun Fang"
    ],
    "abstract": "åŸºäºè§†è§‰çš„æœºå™¨äººæ“ä½œä¸­çš„ä¸»åŠ¨æ„ŸçŸ¥æ—¨åœ¨å°†ç›¸æœºç§»åŠ¨åˆ°æ›´å…·ä¿¡æ¯é‡çš„è§‚æµ‹è§†è§’ï¼Œä»è€Œä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›é«˜è´¨é‡çš„æ„ŸçŸ¥è¾“å…¥ã€‚ç°æœ‰ä¸»åŠ¨æ„ŸçŸ¥æ–¹æ³•å¤§å¤šä¾èµ–è¿­ä»£ä¼˜åŒ–ï¼Œå¯¼è‡´æ—¶é—´å’Œè¿åŠ¨æˆæœ¬è¾ƒé«˜ï¼Œä¸”ä¸ä»»åŠ¡ç‰¹å®šç›®æ ‡ç´§å¯†è€¦åˆï¼Œé™åˆ¶äº†å…¶å¯è¿ç§»æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§é€šç”¨çš„å•æ¬¡å¤šæ¨¡æ€ä¸»åŠ¨æ„ŸçŸ¥æ¡†æ¶ç”¨äºæœºå™¨äººæ“ä½œã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿç›´æ¥æ¨æ–­æœ€ä¼˜è§†è§’ï¼ŒåŒ…å«æ•°æ®æ”¶é›†æµç¨‹å’Œæœ€ä¼˜è§†è§’é¢„æµ‹ç½‘ç»œã€‚å…·ä½“è€Œè¨€ï¼Œæ¡†æ¶å°†è§†è§’è´¨é‡è¯„ä¼°ä»æ•´ä½“æ¶æ„ä¸­è§£è€¦ï¼Œæ”¯æŒå¼‚æ„ä»»åŠ¡éœ€æ±‚ã€‚é€šè¿‡ç³»ç»Ÿé‡‡æ ·å’Œè¯„ä¼°å€™é€‰è§†è§’å®šä¹‰æœ€ä¼˜è§†è§’ï¼Œéšåé€šè¿‡é¢†åŸŸéšæœºåŒ–æ„å»ºå¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†å¤šæ¨¡æ€æœ€ä¼˜è§†è§’é¢„æµ‹ç½‘ç»œï¼Œåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å¯¹é½èåˆå¤šæ¨¡æ€ç‰¹å¾ï¼Œç›´æ¥é¢„æµ‹ç›¸æœºä½å§¿è°ƒæ•´ã€‚æ‰€ææ¡†æ¶åœ¨è§†è§’å—é™ç¯å¢ƒä¸‹çš„æœºå™¨äººæŠ“å–ä»»åŠ¡ä¸­å®ä¾‹åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å¼•å¯¼çš„ä¸»åŠ¨æ„ŸçŸ¥æ˜¾è‘—æå‡äº†æŠ“å–æˆåŠŸç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒçœŸå®ä¸–ç•Œè¯„ä¼°å®ç°äº†æ¥è¿‘ä¸¤å€çš„æŠ“å–æˆåŠŸç‡ï¼Œä¸”æ— éœ€é¢å¤–å¾®è°ƒå³å¯å®ç°æ— ç¼çš„ä»¿çœŸåˆ°ç°å®è¿ç§»ï¼ŒéªŒè¯äº†æ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚",
    "url": "https://arxiv.org/abs/2601.13639v1",
    "html_url": "https://arxiv.org/html/2601.13639v1",
    "html_content": "A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint\nDeyunÂ Qin,\nZezhiÂ Liu,\nHanqianÂ Luo,\nXiaoÂ Liang,\nand YongchunÂ Fang,\nSeniorÂ Member,Â IEEE\nThis work is supported in part by the National Natural Science Foundation of China under Grant U25A20473 and in part by the National Natural Science Foundation of China under Grant 62233011. (Corresponding author: Yongchun Fang)\nDeyun Qin, Zezhi Liu, Xiao Liang, and Yongchun Fang are with the Institute of Robotics and Automatic Information Systems, College of Artificial Intelligence, Nankai University, Tianjin 300350, China, and also with the Tianjin Key Laboratory of Intelligent Robotics, Nankai University, Tianjin 300350, China (e-mail: qindy@mail.nankai.edu.cn, zezhi.liu@mail.nankai.edu.cn, liangx@nankai.edu.cn, fangyc@nankai.edu.cn).\nHanqian Luo is with the College of Artificial Intelligence, Nankai University, Tianjin 300350, China, and also with the Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China (e-mail: hanqian.luo@connect.polyu.hk).\nAbstract\nActive perception in vision-based robotic manipulation aims to move the camera toward more informative observation viewpoints, thereby providing high-quality perceptual inputs for downstream tasks.\nMost existing active perception methods rely on iterative optimization, leading to high time and motion costs, and are tightly coupled with task-specific objectives, which limits their transferability.\nIn this paper, we propose a general one-shot multimodal active perception framework for robotic manipulation. The framework enables direct inference of optimal viewpoints and comprises a data collection pipeline and an optimal viewpoint prediction network.\nSpecifically, the framework decouples viewpoint quality evaluation from the overall architecture, supporting heterogeneous task requirements. Optimal viewpoints are defined through systematic sampling and evaluation of candidate viewpoints, after which large-scale training datasets are constructed via domain randomization.\nMoreover, a multimodal optimal viewpoint prediction network is developed, leveraging cross-attention to align and fuse multimodal features and directly predict camera pose adjustments.\nThe proposed framework is instantiated in robotic grasping under viewpoint-constrained environments.\nExperimental results demonstrate that active perception guided by the framework significantly improves grasp success rates.\nNotably, real-world evaluations achieve nearly double the grasp success rate and enable seamless sim-to-real transfer without additional fine-tuning, demonstrating the effectiveness of the proposed framework.\nThe project website can be found at:\nhttps://nkrobotlab.github.io/MVPNet/\nI\nIntroduction\nPerception serves as the initial and critical stage of the robotic manipulation pipeline\n[\n27\n]\n. The completeness and discriminability of observations directly impact the stability of downstream processes, including decision-making, planning, and execution, thereby substantially determining the overall task success rate\n[\n38\n,\n14\n,\n41\n]\n. Consequently, for vision-based robotic manipulation tasks, strategically selecting viewpoints to acquire informative perceptual inputs actively is crucial.\nFigure 1\n:\nConceptual illustration of the proposed â€œFocus-then-Executeâ€ active perception paradigm.\nHow to actively adjust the camera pose from a random initial viewpoint, based on the current observation and task requirements, to obtain a more informative observation viewpoint constitutes a core problem in active perception research\n[\n3\n]\n. In recent years, this problem has attracted extensive attention in various robotic vision tasks, such as robotic grasping\n[\n17\n,\n6\n,\n45\n,\n24\n]\n, object pose estimation\n[\n16\n]\n, and human pose estimation\n[\n19\n,\n8\n]\n. However, most existing methods formulate viewpoint selection as a sequential optimization process, which typically requires multiple perception-decision-action loops to gradually approach an ideal viewpoint. In real robotic systems, such multi-step viewpoint adjustment not only significantly increases time consumption and motion cost, but also imposes higher requirements on environmental reachability and execution stability. Furthermore, existing active perception frameworks are often tightly coupled with specific task objectives and evaluation functions, making them difficult to transfer and reuse across different tasks, thereby limiting their generality.\nAchieving effective active perception via a single viewpoint adjustment remains a challenging problem. To address this, we draw inspiration from how humans perform daily tasks. When a task intention or instruction arises, humans typically first reposition and orient themselves toward the target of the task, focusing attention on relevant regions\n[\n2\n]\n. This behavior enables them to acquire more comprehensive and discriminative information, which facilitates subsequent actions\n[\n38\n,\n18\n]\n, as illustrated in Fig.\n1\n. Notably, this adjustment process is typically completed in a single step. This efficiency is attributed to humansâ€™ long-term accumulated experience\n[\n34\n,\n37\n]\n, which enables them to directly determine which viewpoints are conducive to perceiving the target object. In contrast, existing robotic systems generally lack this one-shot active perception capability, and a unified framework that can unify the modeling of this capability across diverse task scenarios has yet to be established.\nIn this paper, we propose a data-driven multimodal active perception framework that directly predicts the optimal observation viewpoint, enabling improved perception with only a single relook, and generalizes across different tasks. Specifically, we first construct a pipeline for optimal viewpoint definition and data collection. The optimal observation viewpoint is determined by sampling candidate viewpoints and evaluating them via a task-dependent quality function, which can be tailored to accommodate diverse task requirements. Subsequently, large-scale training datasets are constructed via domain randomization. Notably, this pipeline eliminates the need for manual annotation, thereby substantially reducing the cost of dataset creation and facilitating rapid adaptation to new tasks. Second, we propose a multimodal optimal observation viewpoint prediction network, referred to as MVPNet. Leveraging the cross-attention mechanism, the proposed network adaptively aligns and fuses multimodal features while highlighting perceptually critical regions, enabling efficient active perception. Finally, we instantiate our framework in robotic grasping under viewpoint-constrained environments, as this task is highly sensitive to observation viewpoints. The main contributions of this work are summarized as follows.\n1.\nA general one-shot multimodal active perception framework is proposed, comprising a data collection pipeline and an optimal viewpoint prediction network. This framework enables the unified modeling of diverse task requirements, thereby extending its applicability to a broader range of task scenarios.\n2.\nAn optimal observation viewpoint data collection pipeline is established, in which optimal viewpoints are defined through task-specific viewpoint quality evaluation functions, and large-scale datasets are constructed via domain randomization.\n3.\nAn optimal observation viewpoint prediction network is developed. Utilizing the cross-attention mechanism, this network aligns and fuses multimodal features to predict the required camera pose adjustment.\n4.\nThe proposed framework is instantiated in robotic grasping under viewpoint-constrained environments, where data collection and network training are conducted, and its effectiveness and robustness are validated through extensive simulation and real-world experiments.\nII\nRelated Work\nII-A\nViewpoint Quality Evaluation\nCurrently, there is no unified definition of an â€œoptimal viewpoint.â€ Existing viewpoint theories evaluate viewpoints according to various criteria, including visible area, silhouette, depth, surface curvature, semantics, and aesthetics. Wu\net al.\n[\n38\n]\ndesign three task-agnostic viewpoint quality metrics to provide a comprehensive and uniform evaluation of viewpoints.\nIn contrast, in most mainstream robotic active perception tasks, the notion of an optimal viewpoint is generally regarded as task-dependent. For instance, Ma\net al.\n[\n24\n]\nutilize the distribution of grasp poses to guide camera movements. In contrast, Chen\net al.\n[\n8\n]\nassess the current human pose to identify the most promising next viewing angles for a drone. Although such task-specific active perception frameworks improve performance on the target task, their viewpoint evaluation functions are tightly coupled with the overall framework, which limits generalization across different task domains.\nThe framework proposed in this paper decouples the viewpoint quality evaluation function from the overall architecture, allowing task requirements to be accommodated by simply redefining the evaluation function. This design significantly improves the generality and extensibility of the framework.\nII-B\nDeep Learning-Based Grasp Pose Estimation\nCompared with traditional analytic methods based on geometric reasoning\n[\n32\n,\n4\n,\n33\n]\n, learning-based approaches for grasp pose estimation have achieved substantial gains in success rate, robustness, and generalization owing to advances in deep learning\n[\n26\n,\n20\n,\n21\n,\n9\n,\n41\n,\n43\n]\n.\nDepending on the 3D representation used as the network input, existing methods can be broadly categorized into Truncated Signed Distance Field (TSDF)-based methods and point cloud-based methods.\nBreyer\net al.\n[\n5\n]\npropose the Volumetric Grasping Network (VGN), which takes TSDF as input and directly predicts grasp quality, orientation, and width within the voxel space. Furthermore, Yu\net al.\n[\n42\n]\n, using VGN as the baseline, introduce the trustworthy robotic grasping problem and propose a credibility alignment framework to improve the consistency between predicted grasp probabilities and actual grasp success outcomes. Alternatively, Fang\net al.\n[\n12\n]\nrelease the large-scale point cloud grasping dataset GraspNet-1Billion and develop the GraspNet grasp pose estimation framework, providing a unified benchmark for general object grasping research. Building on this benchmark, Fang\net al.\n[\n11\n]\nfurther propose AnyGrasp, which leverages cross-view geometric consistency and object centroid modeling to improve grasping stability. Wu\net al.\n[\n39\n]\npresent Economic Grasp, implementing more efficient training through an economical supervision strategy that selects critical training samples.\nFigure 2\n:\nOverall framework of the proposed method, illustrated with robotic grasping in viewpoint-constrained environments:\n(a) sampling and evaluating candidate viewpoints to obtain the optimal viewpoint for each object, followed by dataset construction via domain randomization;\n(b) training the MVPNet based on the constructed dataset; and\n(c) deploying the trained network and conducting comparative evaluations.\nHowever, most existing models\n[\n12\n,\n36\n,\n11\n,\n39\n]\nare designed for tabletop grasping under top-down views. When directly applied to semi-enclosed environments, such as refrigerators or cabinets, where top-down viewpoints are unavailable, and the camera is initialized at a random pose, these models may fail to produce reliable grasp poses due to the significant domain gap between the training data. Such failure cases are illustrated in the bottom-left of Fig.\n2\n. Therefore, in viewpoint-constrained environments, selecting an optimal observation viewpoint to provide more informative inputs for grasp pose estimation models is crucial. Accordingly, this task is chosen to instantiate and validate the proposed framework.\nII-C\nActive Perception for Robotic Grasping\nFor robotic grasping in viewpoint-constrained environments, active perception enables the acquisition of more informative object observations, thereby better supporting grasp pose estimation. Breyer\net al.\n[\n6\n]\npropose a closed-loop Next-Best-View planning strategy driven by occlusion cues, which incrementally updates scene reconstruction and decides online whether to continue observation or execute grasping. Zhang\net al.\n[\n45\n]\npresent an affordance-driven Next-Best-View method (ACE-NBV) that, under a view-consistency assumption, uses novel-view synthesis to predict grasp affordance distributions for unobserved viewpoints and selects the next view by maximizing the virtual grasp quality. Ma\net al.\n[\n24\n]\nintroduce the Neural Graspness Field, which constructs an online neural field representation of grasp distributions during camera motion and plans next views by targeting regions of high uncertainty.\nThese methods typically require multiple optimization steps to reach a desired observation viewpoint, which increases both time and motion costs. Moreover, such methods are tightly coupled with robotic grasping, making them difficult to generalize to other tasks. In contrast, this paper proposes a data-driven active perception framework that can reach an optimal viewpoint in a single adjustment step and can be readily extended to other tasks.\nIII\nMethodology\nThe overall active perception framework is designed to learn a policy\nÏ€\n:\n(\nğ’ª\n,\nâ„’\n)\nâ†¦\nğ’¯\n\\pi:(\\mathcal{O},\\mathcal{L})\\mapsto\\mathcal{T}\n, which takes the current observation\nğ’ª\n\\mathcal{O}\nand the natural language instruction\nâ„’\n\\mathcal{L}\nspecifying the target object as inputs, and outputs the required camera pose adjustment\nğ’¯\n\\mathcal{T}\n. The overall implementation consists of three stages:\n(a)\nSynthetic Dataset Construction\n. Developing a large-scale photorealistic dataset for network training and evaluation;\n(b)\nPerception and Preprocessing\n. Acquiring environmental observations and performing preprocessing with integrated semantic information;\n(c)\nNetwork Architecture Construction\n. Building a transformer-based network that maps the preprocessed multimodal visual information to the required camera pose adjustment.\nHere, the framework is instantiated using robotic grasping in viewpoint-constrained environments to validate the overall pipeline.\nIII-A\nSynthetic Dataset Construction\nIsaac Sim\n[\n25\n]\nis leveraged to accelerate dataset construction, harnessing its photorealistic rendering and robust domain randomization for high-fidelity and large-scale synthetic data collection. Moreover, the process does not require manual annotation, thereby reducing the cost of dataset construction.\n(a)\n(b)\n(c)\nFigure 3\n:\n(a) Simulation setup; (b) Similar objects; (c) Novel objects. The simulation environment based on Isaac Sim serves for synthetic dataset construction and simulation-based experimental testing.\nIII-A\n1\nDefine Optimal Observation Viewpoint\nTo construct a dataset for network training, we first define the optimal viewpoint for each object as the dataset label by sampling candidate viewpoints around the object and evaluating their quality. Specifically, we construct a simulated scene in Isaac Sim, as shown in\nFig.\n3(a)\n, where initial camera viewpoints are randomly sampled around the object, all oriented toward the object center, while RGB-D images and camera poses are recorded. These images, together with the corresponding natural language descriptions of the target objects, are sequentially processed by Grounding DINO\n[\n22\n]\nand SAM2\n[\n30\n]\nfor object detection and segmentation. Meanwhile, Economic Grasp\n[\n39\n]\nis employed as the grasping model to compute grasp pose candidates, whose aggregated candidate scores are used to evaluate the quality of each viewpoint. For different downstream tasks, the viewpoint quality evaluation function can be customized accordingly. To ensure the reliability of the process, we randomize 1,500 viewpoints (i.e., camera positions) per object and conduct five repeated grasp pose detections for each point cloud, averaging the top 10 scores in each trial to improve the stability of the evaluation. The process involves 65 object categories, as shown in\nFig.\n3(b)\n. In total, it yields 97.5k images, 487.5k grasp detections, and 4.8 million grasp poses.\nAs an example, the observation viewpoint score distribution of the object is shown in\nFig.\n4\n. Since the scores exhibit continuous distributions in both the 3D space and their 2D projections, this problem can be effectively modeled and optimized in continuous space using neural networks.\nThe top 800 highest-scoring viewpoints undergo DBSCAN\n[\n10\n]\nclustering, with the centroid of the largest cluster selected as the objectâ€™s optimal observation viewpoint\nğ­\nb\nâ€‹\ne\nâ€‹\ns\nâ€‹\nt\n\\mathbf{t}_{best}\nfor the current state.\nğ­\nbest\n=\nğ­\nobj\ni\n+\nğ‘\nâ€‹\n(\nğª\nobj\ni\n)\nâ‹…\nğ©\ncam\nobj\ni\n.\n\\displaystyle\\mathbf{t}_{\\text{best}}=\\mathbf{t}_{\\text{obj}_{i}}+\\mathbf{R}(\\mathbf{q}_{\\text{obj}_{i}})\\cdot\\mathbf{p}_{\\text{cam}}^{\\text{obj}_{i}}.\n(1)\nDuring viewpoint sampling, the object translation\nğ­\nobj\ni\n\\mathbf{t}_{\\text{obj}_{i}}\nand rotation\nğ‘\nâ€‹\n(\nğª\nobj\ni\n)\n\\mathbf{R}(\\mathbf{q}_{\\text{obj}_{i}})\nremain constant. We define\nğ©\ncam\nobj\ni\n\\mathbf{p}_{\\text{cam}}^{\\text{obj}_{i}}\nto transform the optimal viewpoint\nğ­\nbest\n\\mathbf{t}_{\\text{best}}\nfrom the world coordinate frame into the\nobj\ni\n\\text{obj}_{i}\ncoordinate frame, as shown in\nEq.Â (\n1\n). With this formulation,\nğ­\nbest\n\\mathbf{t}_{\\text{best}}\ncan be computed for varying object positions\nand orientations.\nIII-A\n2\nData Collection\nOnce the optimal observation viewpoint is obtained for each object, we perform data collection using domain randomization, where initial camera viewpoints, object categories, poses, scales, and environmental lighting are randomized. The network inputs include mask images and point clouds, which are\nobtained by preprocessing the current-state observations. The camera pose adjustment\nÎ”\nâ€‹\nğ“\n=\n(\nÎ”\nâ€‹\nğ­\n,\nÎ”\nâ€‹\nğª\n)\n\\Delta\\mathbf{T}=(\\Delta\\mathbf{t},\\Delta\\mathbf{q})\nis used as\nthe learning target of the network and is computed as:\nÎ”\nâ€‹\nğ­\n=\nğ‘\nâ€‹\n(\nğª\ncam\nâˆ’\n1\n)\nâ‹…\n(\nğ­\nbest\nâˆ’\nğ­\ncam\n)\n,\nÎ”\nâ€‹\nğª\n=\nğª\ncam\nâˆ’\n1\nâ‹…\nğª\ntarget\n,\n\\displaystyle\\Delta\\mathbf{t}=\\mathbf{R}(\\mathbf{q}_{\\text{cam}}^{-1})\\cdot(\\mathbf{t}_{\\text{best}}-\\mathbf{t}_{\\text{cam}}),\\;\\Delta\\mathbf{q}=\\mathbf{q}_{\\text{cam}}^{-1}\\cdot\\mathbf{q}_{\\text{target}},\n(2)\nwhere\nÎ”\nâ€‹\nğ­\nâˆˆ\nâ„\n3\n\\Delta\\mathbf{t}\\in\\mathbb{R}^{3}\ndenotes the translation vector,\nand\nÎ”\nâ€‹\nğª\nâˆˆ\nâ„\n4\n\\Delta\\mathbf{q}\\in\\mathbb{R}^{4}\ndenotes a unit quaternion\nsatisfying\nâ€–\nÎ”\nâ€‹\nğª\nâ€–\n=\n1\n\\|\\Delta\\mathbf{q}\\|=1\n.\nThe optimal observation viewpoint\nğ­\nbest\n\\mathbf{t}_{\\text{best}}\nis obtained from the current pose of the object\nobj\ni\n\\text{obj}_{i}\nand its optimal viewpoint position\nğ©\ncam\nobj\ni\n\\mathbf{p}_{\\text{cam}}^{\\text{obj}_{i}}\nusing\n(\n1\n)\n. The randomly initialized camera translation\nğ­\ncam\n\\mathbf{t}_{\\text{cam}}\nand rotation\nğª\ncam\n\\mathbf{q}_{\\text{cam}}\ncan be obtained directly from the simulator. The quaternion\nğª\ntarget\n\\mathbf{q}_{\\text{target}}\nrepresents an orientation that faces the direction of the vector\nğ¯\n=\nğ­\nbest\nâˆ’\nğ­\nobj\n\\mathbf{v}=\\mathbf{t}_{\\text{best}}-\\mathbf{t}_{\\text{obj}}\n.\n(a)\n(b)\n(c)\n(d)\nFigure 4\n:\nAn example of the viewpoint score distribution of the object: (a) 3D distribution; (b) X-Y plane projection; (c) X-Z plane projection; (d) Y-Z plane projection. Each point represents a observation viewpoint, with color indicating its score:\nred\n(highest),\ngreen\n(medium), and\nblue\n(lowest).\nFigure 5\n:\nOverall framework of the perception and preprocessing modules together with MVPNet.\nThe dataset consists of three components: mask images, point clouds, and camera pose adjustments.\nThe mask images and point clouds are obtained by preprocessing RGB-D observations in conjunction with the corresponding natural language instructions, and they serve as the inputs to the network, while the camera pose adjustments provide the learning targets.\nUsing the domain randomization capabilities in Isaac Sim, we collect 17k data samples. Subsequently, the dataset is split into training and testing sets following a 9:1 ratio.\nIII-B\nPerception and Preprocessing\nA single camera is utilized for image acquisition to facilitate policy learning, which enhances the applicability of our approach in real-world deployments\n[\n44\n]\n. For both simulation and real-world settings, color and depth images are acquired at a resolution of\n1280\nÃ—\n720\n1280\\times 720\nusing a wrist-mounted camera, consistent with the dimensions used in the dataset. The depth image is then converted into a point cloud using the camera extrinsics and intrinsics.\nDue to background interference, directly feeding raw camera-captured images into the network may be suboptimal. On the one hand, cluttered scenes and irrelevant objects make it difficult for the network to focus on the target object, thereby impeding network convergence. On the other hand, background geometry unrelated to the target object may introduce irrelevant features; for example, the model may inadvertently rely on the geometry of the tabletop when predicting camera pose adjustments.\nFurthermore, in multi-object scenes, we aim to enable natural language-guided active perception for specific target objects. Therefore, the observations are preprocessed by incorporating target object language descriptions using Grounded SAMÂ 2\n[\n31\n]\n, which combines Grounding DINO\n[\n22\n]\nfor object detection and SAMÂ 2\n[\n30\n]\nfor segmentation.\nAs shown in Fig.\n5\n, its workflow involves: 1) Grounding DINO detects the target object based on the semantic information of the object to be grasped and the camera-captured RGB image, and then passes its bounding box to SAM 2 for segmentation to generate a mask; 2) The mask serves two purposes: it is used directly as an image input to MVPNet and is also applied for point cloud masking; 3) The processed point cloud undergoes data augmentation (e.g., random point dropout) and farthest point sampling before being fed into MVPNet. Therefore, the inputs fed into the network are preprocessed and augmented, which not only facilitates the learning of salient features but also improves generalization to novel objects, unseen scenes, and even real-world environments.\nRemark 1\nPrior studies have shown that when images containing color, texture, and shape information are jointly provided as network input, convolutional neural networks tend to rely more on color and texture information than on geometric shape for prediction\n[\n13\n]\n. In this work, it is evident that the model should primarily focus on the boundary geometry of the target object to infer the required camera pose adjustment, instead of relying on color or texture information. Accordingly, the image input to the network is represented as a binary mask of the target object, rather than a color mask containing appearance information. This design not only helps the network learn key features but also further improves its generalization capability across variations in object color, texture, and environmental conditions (e.g., lighting).\nIII-C\nNetwork Architecture Construction\nWe propose a multimodal optimal observation viewpoint prediction network, termed MVPNet, which comprises three primary components: feature extraction, feature alignment and fusion, and pose output.\nIII-C\n1\nFeature Extraction\nWe adopt ResNet\n[\n15\n]\nto encode the mask image and use two point cloud encoders to extract point cloud features, as the point cloud contains richer three-dimensional spatial relationships compared to the mask image. This spatial data is more directly interpretable than image pixel values and resides in the same metric space as the output transformation\nÎ”\nâ€‹\nğ“\n\\Delta\\mathbf{T}\n.\nSpecifically, a combined point cloud encoder architecture integrating both PointNeXt\n[\n29\n]\nand PointNet++\n[\n28\n]\nis employed for collaborative feature extraction. Although PointNeXt is an improved version of PointNet++, each exhibits distinct characteristics in feature extraction: PointNet++ employs hierarchical downsampling with a focus on traditional geometric modeling, making it more sensitive to fine-grained structural details; PointNeXt incorporates enhanced residual connections optimized for modern deep feature extraction, providing superior awareness of global context and high-level semantics. By combining both architectures, our model effectively captures local geometric details while maintaining robust global representations, thereby enhancing adaptability across diverse input conditions.\nIII-C\n2\nFeature Alignment and Fusion\nThe feature extractors output modality-specific feature vectors, which are treated as tokens. These point cloud and mask image tokens are concatenated into a unified sequence and passed to a Transformer\n[\n35\n]\nencoder, whose cross-attention mechanism implicitly establishes 2D-3D correspondences and fuses their representations.\nMoreover, a trainable classification ([CLS]) token is integrated within the Transformer encoder, similar to its use in BERT, where it serves as a summary representation for sequence-level prediction\n[\n46\n]\n. Here, this token acts as a global feature vector that aggregates information from both point cloud and mask image tokens via the cross-attention mechanism, and its corresponding output is then employed to predict the required camera pose transformation. It ensures prediction robustness by integrating comprehensive geometric features from the point cloud with complementary visual silhouette information from the mask image, maintaining reliable performance even with incomplete observations or noisy inputs. The 2D sinusoidal position embedding is added to preserve spatial information\n[\n7\n]\n.\nIII-C\n3\nPose Output\nWe apply MLP to map the Transformer outputs to the task space, which consists of translation\nÎ”\nâ€‹\nğ­\n\\Delta\\mathbf{t}\nand rotation\nÎ”\nâ€‹\nğª\n\\Delta\\mathbf{q}\n. During training, these outputs are compared with ground-truth labels, and the network parameters are optimized through the loss function defined in\nSection\nIII-D\n. During inference, the predicted outputs are used to adjust the camera pose toward the optimal observation viewpoint.\nIII-D\nLoss Function\nThe loss function comprises two components:\nâ„’\nt\nâ€‹\nr\nâ€‹\na\nâ€‹\nn\nâ€‹\ns\n\\mathcal{L}_{trans}\nfor translation and\nâ„’\nr\nâ€‹\no\nâ€‹\nt\n\\mathcal{L}_{rot}\nfor rotation. Specifically,\nâ„’\nt\nâ€‹\nr\nâ€‹\na\nâ€‹\nn\nâ€‹\ns\n\\mathcal{L}_{trans}\ncomputes MSE between predicted translation\nğ­\n^\n\\hat{\\mathbf{t}}\nand ground-truth translation\nğ­\n\\mathbf{t}\n, while\nâ„’\nr\nâ€‹\no\nâ€‹\nt\n\\mathcal{L}_{rot}\ncomputes the geodesic distance between predicted rotation\nğª\n^\n\\hat{\\mathbf{q}}\nand ground-truth rotation\nğª\n\\mathbf{q}\n. The total loss is defined as\nâ„’\nt\nâ€‹\no\nâ€‹\nt\nâ€‹\na\nâ€‹\nl\n=\nâ„’\nt\nâ€‹\nr\nâ€‹\na\nâ€‹\nn\nâ€‹\ns\n+\nÎ»\nâ€‹\nâ„’\nr\nâ€‹\no\nâ€‹\nt\n,\n\\displaystyle\\mathcal{L}_{total}=\\mathcal{L}_{trans}+\\lambda\\mathcal{L}_{rot},\n(3)\nwhere\nâ„’\nt\nâ€‹\nr\nâ€‹\na\nâ€‹\nn\nâ€‹\ns\n=\nâ€–\nğ­\n^\nâˆ’\nğ­\nâ€–\n2\n2\n,\nâ„’\nr\nâ€‹\no\nâ€‹\nt\n=\n2\nâ€‹\ncos\nâˆ’\n1\nâ¡\n(\n|\nâŸ¨\nğª\n^\n,\nğª\nâŸ©\n|\n)\n,\n\\displaystyle\\mathcal{L}_{trans}=\\left\\|\\hat{\\mathbf{t}}-\\mathbf{t}\\right\\|_{2}^{2},\\;\\mathcal{L}_{rot}=2\\cos^{-1}\\left(\\left|\\left\\langle\\hat{\\mathbf{q}},\\mathbf{q}\\right\\rangle\\right|\\right),\n(4)\nand\nÎ»\n\\lambda\nis a weighting parameter.\nIV\nSimulation and Evaluation\nEffectiveness and ablation experiments are first conducted in simulation on robotic grasping tasks under viewpoint-constrained environments to evaluate the proposed framework.\nIV-A\nSimulation Setup\nAs shown in Fig.\n3\n3(a)\n, we construct a virtual environment based on the Isaac Sim 4.0.0\n[\n25\n]\nsimulation platform, which includes a Franka Emika robotic arm equipped with its default parallel gripper. An Intel RealSense D435i camera mounted on the robotic arm wrist captures RGB-D images with a resolution of 1280\nÃ—\n\\times\n720. The cameraâ€™s observation viewpoints are constrained such that top-down observations of the object are not available, and only side-view observations are allowed. The robotic arm serves dual purposes: executing grasping actions and acting as a carrier for the camera, enabling active perception through camera pose adjustment. The test involves 65 similar and 18 novel objects for grasping evaluation, as shown in Fig.\n3(b)\nand Fig.\n3(c)\n. Object categories, positions, scales, and orientations are randomly selected. The camera is placed at a random side position facing the object center, with random minor offsets in orientation to simulate realistic viewing conditions. It is worth noting that some objects within the similar objects category may be inherently more difficult to grasp than those in the novel objects category. As a result, the grasp success rate on similar objects can be lower than that on novel objects.\nProcedure 1\nSimulation-Based Evaluation Pipeline\n0:\nTotal evaluation trials\nn\nn\n0:\nSR-1 and SR-5 for initial (\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\ninit\n) and optimized (\no\nâ€‹\np\nâ€‹\nt\nopt\n) viewpoints\n1:\nInitialize Accumulators:\n2:\nA\nâ€‹\nc\nâ€‹\nc\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\n(\n1\n)\nâ†\n0\n,\nA\nâ€‹\nc\nâ€‹\nc\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\n(\n5\n)\nâ†\n0\nAcc_{init}^{(1)}\\leftarrow 0,\\;Acc_{init}^{(5)}\\leftarrow 0\n;\nA\nâ€‹\nc\nâ€‹\nc\no\nâ€‹\np\nâ€‹\nt\n(\n1\n)\nâ†\n0\n,\nA\nâ€‹\nc\nâ€‹\nc\no\nâ€‹\np\nâ€‹\nt\n(\n5\n)\nâ†\n0\nAcc_{opt}^{(1)}\\leftarrow 0,\\;Acc_{opt}^{(5)}\\leftarrow 0\n3:\nfor\nt\n=\n1\nt=1\nto\nn\nn\ndo\n4:\nSetup:\nRandomly initialize\nobject\n,\ncamera\n,\nenv\n; record state\nS\n0\nS_{0}\n5:\n// â€” Phase 1: Evaluation at Initial Viewpoint â€”\n6:\nAcquire observation\nğ’ª\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\n\\mathcal{O}_{init}\nand language\nâ„’\n\\mathcal{L}\n7:\nSegment target object:\nğ’ª\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\nâ€²\n=\nM\ns\nâ€‹\n(\nğ’ª\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\n,\nâ„’\n)\n\\mathcal{O}^{\\prime}_{init}=M_{s}(\\mathcal{O}_{init},\\mathcal{L})\n8:\n(\nc\n1\n,\nc\n5\n)\nâ†\nGraspExecution\nâ€‹\n(\nğ’ª\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\nâ€²\n,\nS\n0\n)\n(c_{1},c_{5})\\leftarrow\\textsc{GraspExecution}(\\mathcal{O}^{\\prime}_{init},S_{0})\n9:\nA\nâ€‹\nc\nâ€‹\nc\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\n(\n1\n)\nâ†\nA\nâ€‹\nc\nâ€‹\nc\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\n(\n1\n)\n+\nc\n1\nAcc_{init}^{(1)}\\leftarrow Acc_{init}^{(1)}+c_{1}\n,\nA\nâ€‹\nc\nâ€‹\nc\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\n(\n5\n)\nâ†\nA\nâ€‹\nc\nâ€‹\nc\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\n(\n5\n)\n+\nc\n5\nAcc_{init}^{(5)}\\leftarrow Acc_{init}^{(5)}+c_{5}\n10:\n// â€” Phase 2: Evaluation at Optimized Viewpoint â€”\n11:\nPredict camera pose adjustment:\nÎ”\nâ€‹\nğ“\n=\nM\nv\nâ€‹\n(\nğ’ª\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\nâ€²\n)\n\\Delta\\mathbf{T}=M_{v}(\\mathcal{O}^{\\prime}_{init})\n12:\nMove camera according to\nÎ”\nâ€‹\nğ“\n\\Delta\\mathbf{T}\n13:\nAcquire optimized observation\nğ’ª\no\nâ€‹\np\nâ€‹\nt\n\\mathcal{O}_{opt}\n14:\nSegment target object:\nğ’ª\no\nâ€‹\np\nâ€‹\nt\nâ€²\n=\nM\ns\nâ€‹\n(\nğ’ª\no\nâ€‹\np\nâ€‹\nt\n,\nâ„’\n)\n\\mathcal{O}^{\\prime}_{opt}=M_{s}(\\mathcal{O}_{opt},\\mathcal{L})\n15:\n(\nc\n1\n,\nc\n5\n)\nâ†\nGraspExecution\nâ€‹\n(\nğ’ª\no\nâ€‹\np\nâ€‹\nt\nâ€²\n,\nS\n0\n)\n(c_{1},c_{5})\\leftarrow\\textsc{GraspExecution}(\\mathcal{O}^{\\prime}_{opt},S_{0})\n16:\nA\nâ€‹\nc\nâ€‹\nc\no\nâ€‹\np\nâ€‹\nt\n(\n1\n)\nâ†\nA\nâ€‹\nc\nâ€‹\nc\no\nâ€‹\np\nâ€‹\nt\n(\n1\n)\n+\nc\n1\nAcc_{opt}^{(1)}\\leftarrow Acc_{opt}^{(1)}+c_{1}\n,\nA\nâ€‹\nc\nâ€‹\nc\no\nâ€‹\np\nâ€‹\nt\n(\n5\n)\nâ†\nA\nâ€‹\nc\nâ€‹\nc\no\nâ€‹\np\nâ€‹\nt\n(\n5\n)\n+\nc\n5\nAcc_{opt}^{(5)}\\leftarrow Acc_{opt}^{(5)}+c_{5}\n17:\nend\nfor\n18:\nCompute Final Metrics:\n19:\nSR-1\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\n=\nA\nâ€‹\nc\nâ€‹\nc\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\n(\n1\n)\n/\nn\n,\nSR-5\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\n=\nA\nâ€‹\nc\nâ€‹\nc\ni\nâ€‹\nn\nâ€‹\ni\nâ€‹\nt\n(\n5\n)\n/\n(\n5\nÃ—\nn\n)\n\\text{SR-1}_{init}=Acc_{init}^{(1)}/n,\\;\\text{SR-5}_{init}=Acc_{init}^{(5)}/(5\\times n)\n20:\nSR-1\no\nâ€‹\np\nâ€‹\nt\n=\nA\nâ€‹\nc\nâ€‹\nc\no\nâ€‹\np\nâ€‹\nt\n(\n1\n)\n/\nn\n,\nSR-5\no\nâ€‹\np\nâ€‹\nt\n=\nA\nâ€‹\nc\nâ€‹\nc\no\nâ€‹\np\nâ€‹\nt\n(\n5\n)\n/\n(\n5\nÃ—\nn\n)\n\\text{SR-1}_{opt}=Acc_{opt}^{(1)}/n,\\;\\text{SR-5}_{opt}=Acc_{opt}^{(5)}/(5\\times n)\n21:\n22:\nFunction\nGraspExecution\n(\nğ’ª\nâ€²\n,\nS\nreset\n\\mathcal{O}^{\\prime},S_{\\text{reset}}\n)\n23:\nEstimate grasp candidates:\nğ’«\n=\nM\ng\nâ€‹\n(\nğ’ª\nâ€²\n)\n\\mathcal{P}=M_{g}(\\mathcal{O}^{\\prime})\n24:\nSelect top-5 candidates:\nğ’\n=\nTop-5\nâ€‹\n(\nğ’«\n)\n\\mathcal{C}=\\text{Top-5}(\\mathcal{P})\n25:\nc\nâ€‹\no\nâ€‹\nu\nâ€‹\nn\nâ€‹\nt\n1\nâ†\n0\n,\nc\nâ€‹\no\nâ€‹\nu\nâ€‹\nn\nâ€‹\nt\n5\nâ†\n0\ncount_{1}\\leftarrow 0,\\quad count_{5}\\leftarrow 0\n26:\nfor\nk\n=\n1\nk=1\nto\n5\n5\ndo\n27:\nExecute candidate\nc\nk\nâˆˆ\nğ’\nc_{k}\\in\\mathcal{C}\nin simulation\n28:\nif\nlift is successful\nthen\n29:\nc\nâ€‹\no\nâ€‹\nu\nâ€‹\nn\nâ€‹\nt\n5\nâ†\nc\nâ€‹\no\nâ€‹\nu\nâ€‹\nn\nâ€‹\nt\n5\n+\n1\ncount_{5}\\leftarrow count_{5}+1\n30:\nif\nk\n=\n1\nk=1\nthen\n31:\nc\nâ€‹\no\nâ€‹\nu\nâ€‹\nn\nâ€‹\nt\n1\nâ†\n1\ncount_{1}\\leftarrow 1\n32:\nend\nif\n33:\nend\nif\n34:\nReset environment to state\nS\nreset\nS_{\\text{reset}}\n35:\nend\nfor\n36:\nreturn\n(\nc\nâ€‹\no\nâ€‹\nu\nâ€‹\nn\nâ€‹\nt\n1\n,\nc\nâ€‹\no\nâ€‹\nu\nâ€‹\nn\nâ€‹\nt\n5\n)\n(count_{1},count_{5})\nIV-B\nImplementation Details\nEach experimental group undergoes 250 trials with the following evaluation protocol: First, we randomly initialize the camera and object poses, and then collect RGB-D observations from the current viewpoint. The RGB image, together with the semantic information of the target object, is provided to Grounded-SAM2\n[\n31\n]\nto obtain an instance mask, which is then applied to the point cloud to isolate the object of interest. The masked observations are subsequently fed into the grasping model to estimate grasp poses. We then select the top-5 highest-scoring grasp candidates and execute them sequentially in Isaac Sim to evaluate the grasping performance at the initial viewpoint. Subsequently, MVPNet predicts the optimal observation viewpoint, after which the camera is moved to that viewpoint to acquire new observations. The segmentation model and the grasping model are applied again for grasp pose estimation, followed by the same candidate selection and execution process. The overall procedure is summarized in Procedure\n1\n. We record both the first-attempt success rate (SR-1) and the average success rate over five attempts (SR-5) for comprehensive analysis. All success rates (SR-1 and SR-5) are reported in percentage (%). Grasping success is measured by lifting the object 10 cm without dropping it.\nWe train the model using the AdamW\n[\n23\n]\noptimizer with a weight decay of\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\n. The initial learning rate for the main network is set to\n5\nÃ—\n10\nâˆ’\n5\n5\\times 10^{-5}\n, while the backbone, a pre-trained ResNet-18, uses a lower rate of\n5\nÃ—\n10\nâˆ’\n6\n5\\times 10^{-6}\n. A StepLR scheduler is adopted, in which the learning rate decays by a factor of 0.7 every 20 epochs. The model is trained for 100 epochs with a batch size of 16. The weighting parameter\nÎ»\n\\lambda\nin Eq.\n(\n4\n)\nis set to 1. All training and inference are conducted on a workstation equipped with an Intel Xeon Silver 4416+ CPU and a single NVIDIA RTX 4090 GPU.\nIV-C\nEffectiveness\nFour representative grasp pose estimation models are used in the experiments: Economic Grasp\n[\n39\n]\n, GraspNet\n[\n12\n]\n, TRG\n[\n42\n]\n, and GIGA\n[\n17\n]\n. Economic Grasp and GraspNet take point clouds as input, whereas TRG and GIGA use TSDF as input.\nThis setup enables us to directly evaluate the performance improvement our active perception brings to various types of grasp estimation pipelines.\nThe results in Tables\nI\nshow that, after optimizing the initial observation viewpoints using MVPNet, the grasp success rates of all baselines are significantly improved for both similar objects and novel objects. This improvement is observed even though the observation viewpoint quality evaluation function is based on the Economic Grasp metric, indicating that the proposed framework is practical across different grasp pose estimation models. Specifically, for point cloud-based grasp models such as Economic Grasp and GraspNet, our approach results in clear gains in both SR-5 and SR-1, indicating that a more informative view helps these models generate more reliable grasp proposals. Compared with Economic Grasp, the improvement on GraspNet is more pronounced, as GraspNet is more sensitive to missing or incomplete geometric observations, and thus benefits more from improved viewpoints. TSDF-based models such as TRG and GIGA also benefit from active perception.\nIt should be noted that TRG and GIGA generate relatively few grasp poses; therefore, only SR-1 is reported. Furthermore, since they are primarily designed for grasp pose estimation from a fixed view, their grasp success rates under dynamic views are comparatively low.\nOverall, these results demonstrate that selecting more informative viewpoints guided by MVPNet provides grasp models with richer and more discriminative observations, enabling more accurate grasp pose estimations, and further validate the effectiveness of the proposed framework.\nTABLE I\n:\nSimulation Results of Different Grasp Pose Estimation Models Using Initial and Optimized Views.\nModel\nViewpoint\nSimilar Objects\nNovel Objects\nSR-5\nSR-1\nSR-5\nSR-1\nEconomic\nGrasp\nOptimized (Ours)\n64.8\n64.0\n63.2\n64.7\nInitial\n54.4\n53.4\n49.2\n48.0\nGraspNet\nOptimized (Ours)\n49.6\n54.0\n51.3\n55.3\nInitial\n32.0\n32.7\n32.3\n50.7\nTRG\nOptimized (Ours)\nâ€“\n24.0\nâ€“\n35.6\nInitial\nâ€“\n22.0\nâ€“\n32.0\nGIGA\nOptimized (Ours)\nâ€“\n26.0\nâ€“\n28.0\nInitial\nâ€“\n19.3\nâ€“\n23.3\nAverage Improvement\n+14.0\n+10.2\n+16.5\n+7.4\nIV-D\nAblations\nIn this subsection, detailed ablation studies are conducted to validate the effectiveness of the MVPNet design, covering input modalities, point cloud encoders, and network architecture. The ablation studies employ the same experimental setup and evaluation metrics as above, using Economic Grasp and GraspNet as the grasping models.\nIV-D\n1\nChoice of Input Modalities\nPoint cloud and mask image provide complementary 3D and 2D representations of the object, respectively. Leveraging both modalities enables the network to capture object geometry and silhouettes more comprehensively, leading to more reliable prediction. As shown in\nTable\nII\nand\nIII\n, the multimodal configuration that fuses point cloud and mask image achieves the best performance across most metrics, with especially strong gains on the novel objects SR-5 metric, which reflects comprehensive generalization capability. Consequently, removing either modality consistently degrades performance, highlighting the importance of multimodal input for robust optimal viewpoint prediction.\nTABLE II\n:\nAblation on Input Modalities (Economic Grasp).\nSimilar Objects\nNovel Objects\nInput Modalities\nSR-5\nSR-1\nSR-5\nSR-1\nPoint Cloud + Mask (Ours)\n66.8\n64.0\n62.8\n60.0\nw/o Point Cloud\n59.2\n62.0\n58.4\n58.0\nw/o Mask\n64.8\n64.0\n49.6\n48.0\nTABLE III\n:\nAblation on Input Modalities (GraspNet).\nSimilar Objects\nNovel Objects\nInput Modalities\nSR-5\nSR-1\nSR-5\nSR-1\nPoint Cloud + Mask (Ours)\n53.2\n54.0\n42.8\n46.0\nw/o Point Cloud\n49.6\n54.0\n41.2\n48.0\nw/o Mask\n58.0\n60.0\n40.4\n52.0\nIV-D\n2\nChoice of Point Cloud Encoders\nWe compare the point cloud encoders in MVPNet, which integrates PointNeXt\n[\n29\n]\nand PointNet++\n[\n28\n]\n, with several widely adopted alternatives, including Point Transformer\n[\n40\n]\nand LayerNorm\n[\n1\n]\n(demonstrated to be effective in DP3\n[\n44\n]\n).\nThe results in\nTable\nIV\nand\nTable\nV\ndemonstrate that the combination of PointNeXt and PointNet++ consistently achieves the best performance across all metrics, confirming the effectiveness of jointly leveraging both encoders.\nAs mentioned in Section\nIII-C\n1\n, PointNeXt captures global geometric structures, while PointNet++ focuses on local feature extraction, making their fusion highly complementary.\nThe combination improves feature extraction robustness across different object categories, initial viewpoints, and sensing conditions, leading to more stable viewpoint prediction performance.\nTABLE IV\n:\nAblation on Point Cloud Encoders (Economic Grasp).\nSimilar Objects\nNovel Objects\nPoint Cloud Encoders\nSR-5\nSR-1\nSR-5\nSR-1\nPointNeXt + PointNet++ (Ours)\n66.8\n64.0\n62.4\n68.0\nPointNet++ + LayerNorm\n62.4\n62.0\n51.6\n52.0\nPointNeXt + LayerNorm\n60.4\n64.0\n51.6\n46.0\nPointNeXt + Point Transformer\n65.2\n64.0\n59.6\n54.0\nPoint Transformer + LayerNorm\n54.0\n52.0\n58.4\n60.0\nPoint Transformer + PointNet++\n59.6\n64.0\n43.2\n46.0\nTABLE V\n:\nAblation on Point Cloud Encoders (GraspNet).\nSimilar Objects\nNovel Objects\nPoint Cloud Encoders\nSR-5\nSR-1\nSR-5\nSR-1\nPointNeXt + PointNet++ (Ours)\n46.8\n50.0\n60.0\n68.0\nPointNet++ + LayerNorm\n40.0\n40.0\n55.6\n64.0\nPointNeXt + LayerNorm\n38.4\n44.0\n54.8\n62.0\nPointNeXt + Point Transformer\n42.0\n46.0\n52.0\n60.0\nPoint Transformer + LayerNorm\n41.2\n48.0\n50.4\n62.0\nPoint Transformer + PointNet++\n34.8\n42.0\n50.4\n58.0\nIV-D\n3\nChoice of Network Architecture\nWe conduct network architectural ablation studies by individually removing PointNet++ and PointNeXt and replacing the Transformer with an MLP. As shown in\nTable\nVI\nand\nTable\nVII\n, and as mentioned in Section\nIII-C\n1\nand\nIV-D\n2\n, PointNet++ and PointNeXt extract point cloud features at different scales, and this multi-scale representation enhances overall network performance. Simultaneously, the Transformer offers superior capability for multimodal alignment and information fusion between point cloud and mask image features.\nTABLE VI\n:\nAblation on Network Architecture (Economic Grasp).\nSimilar Objects\nNovel Objects\nDesign Choices\nSR-5\nSR-1\nSR-5\nSR-1\nMVPNet (Ours)\n60.8\n64.0\n64.4\n66.0\nw/o PointNet++\n62.4\n62.0\n60.0\n60.0\nw/o PointNeXt\n51.6\n52.0\n54.4\n60.0\nw/o Transformer\n50.4\n52.0\n57.6\n62.0\nTABLE VII\n:\nAblation on Network Architecture (GraspNet).\nSimilar Objects\nNovel Objects\nDesign Choices\nSR-5\nSR-1\nSR-5\nSR-1\nMVPNet (Ours)\n48.8\n58.0\n51.2\n52.0\nw/o PointNet++\n46.0\n52.0\n48.8\n52.0\nw/o PointNeXt\n46.4\n50.0\n48.4\n56.0\nw/o Transformer\n48.4\n56.0\n46.0\n56.0\nOverall, although certain variants achieve slightly higher scores on individual metrics, MVPNet delivers the most consistent and robust performance across the majority of settings, particularly in SR-5 on Novel Objects. This demonstrates the necessity and complementarity of all architectural components and design choices.\nV\nReal World Experiments\nThe effectiveness of the proposed framework is further validated through real-world experiments, with the experimental setup and tested objects shown in Fig.\n6(a)\nand Fig.\n6(b)\n, respectively. An Intel RealSense D435 camera is used to capture visual data, and a Franka Research 3 robotic arm is employed both to execute object grasping and to reposition the camera for active perception. We conduct 20 evaluation rounds, in which three objects are randomly selected and placed for each round. In every round, we first execute grasping without optimal viewpoint prediction, where grasp poses are predicted directly from the initial camera viewpoint. We then apply active perception independently to each of the three objects, move the camera to the corresponding optimized viewpoints, and repeat grasp pose prediction and execution. An object is considered successfully grasped if it is picked up and placed into the bin, whereas two consecutive failed grasp attempts are counted as an object grasp failure. We report two evaluation metrics: (1) Grasp Success Rate (GSR), defined as the ratio of successful grasp executions; and (2) Declutter Rate (DR), defined as the average fraction of objects removed. Their formal definitions are given as\nGSR\n=\nn\nN\n,\nDR\n=\nn\nc\nN\nobj\n,\n\\displaystyle\\mathrm{GSR}=\\frac{n}{N},\\;\\mathrm{DR}=\\frac{n_{c}}{N_{\\mathrm{obj}}},\n(5)\nwhere\nn\nn\ndenotes the number of successful grasp attempts out of\nN\nN\ntotal attempts, and\nn\nc\nn_{c}\ndenotes the number of removed objects out of\nN\nobj\nN_{\\mathrm{obj}}\ntotal objects.\n(a)\n(b)\nFigure 6\n:\n(a) Real-world experimental scene setup; (b) Experimental objects.\nTable\nVIII\nsummarizes the real-world performance. After applying active perception using MVPNet, both the grasp success rate and declutter rate exhibit substantial improvements, with the grasp success rate nearly doubling. Fig.\n7\npresents several representative examples, showing that the generated grasp poses become more accurate and reasonable after active perception. It is noteworthy that our model is trained entirely on synthetic data, without any real-world fine-tuning, demonstrating seamless sim-to-real transfer. The real-world experimental results further demonstrate the effectiveness of the proposed framework.\nTABLE VIII\n:\nReal-World Results of Economic Grasp Using Initial and Optimized View.\nGrasp Viewpoint\nGSR (%)\nDR (%)\nOptimized (Ours)\n47.6 (40/84)\n66.7\nInitial\n25.5 (26/102)\n43.3\nFigure 7\n:\nActive perception in real-world scenarios. The second and fourth columns present the grasp poses generated by Economic Grasp under the initial and optimized viewpoints, respectively.\nThe red gripper\nindicates the highest-scoring grasp pose, while\nthe blue gripper\ndenotes the second-best candidate. The target object to be grasped is indicated by the bounding box and the masked region.\nVI\nConclusion\nThis paper aims to present a general active perception framework capable of optimal viewpoint prediction in a one-shot manner, comprising a data collection pipeline and an optimal viewpoint prediction network. The effectiveness of the proposed framework is validated through robotic grasping tasks in viewpoint-constrained environments, evaluated in both simulation and real-world settings. Moreover, the proposed framework enables seamless sim-to-real transfer, facilitating large-scale data collection in simulation and direct deployment in real-world systems without additional fine-tuning.\nFuture work will focus on extending the proposed framework to a broader range of robotic tasks, as well as exploring richer viewpoint representations to support more complex and dynamic manipulation scenarios.\nReferences\n[1]\nJ. L. Ba, J. R. Kiros, and G. E. Hinton\n(2016)\nLayer normalization\n.\narXiv preprint arXiv:1607.06450\n.\nCited by:\nÂ§\nIV-D\n2\n.\n[2]\nR. Bajcsy\n(1985)\nActive perception vs. passive perception\n.\nIn\nProceedings 3rd Workshop on Computer Vision: Representation and Control\n,\npp.Â 55â€“59\n.\nCited by:\nÂ§I\n.\n[3]\nR. Bajcsy, Y. Aloimonos, and J. K. Tsotsos\n(2018)\nRevisiting active perception\n.\nAutonomous Robots\n42\n(\n2\n),\npp.Â 177â€“196\n.\nCited by:\nÂ§I\n.\n[4]\nA. Bicchi and V. Kumar\n(2000)\nRobotic grasping and contact: a review\n.\nIn\nIEEE International Conference on Robotics and Automation.\n,\nVol.\n1\n,\npp.Â 348â€“353\n.\nCited by:\nÂ§\nII-B\n.\n[5]\nM. Breyer, J. J. Chung, L. Ott, R. Siegwart, and J. Nieto\n(2021)\nVolumetric grasping network: real-time 6 dof grasp detection in clutter\n.\nIn\nConference on Robot Learning\n,\npp.Â 1602â€“1611\n.\nCited by:\nÂ§\nII-B\n.\n[6]\nM. Breyer, L. Ott, R. Siegwart, and J. J. Chung\n(2022)\nClosed-loop next-best-view planning for target-driven grasping\n.\nIn\n2022 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp.Â 1411â€“1416\n.\nCited by:\nÂ§I\n,\nÂ§\nII-C\n.\n[7]\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko\n(2020)\nEnd-to-end object detection with transformers\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 213â€“229\n.\nCited by:\nÂ§\nIII-C\n2\n.\n[8]\nJ. Chen, B. He, C. D. Singh, C. FermÃƒÂ¼ller, and Y. Aloimonos\n(2024)\nActive human pose estimation via an autonomous uav agent\n.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\nVol.\n,\npp.Â 7801â€“7808\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[9]\nH. Duan, P. Wang, Y. Huang, G. Xu, W. Wei, and X. Shen\n(2021)\nRobotics dexterous grasping: the methods based on point cloud and deep learning\n.\nFrontiers in Neurorobotics\n15\n,\npp.Â 658280\n.\nCited by:\nÂ§\nII-B\n.\n[10]\nM. Ester, H. Kriegel, J. Sander, and X. Xu\n(1996)\nA density-based algorithm for discovering clusters in large spatial databases with noise\n.\nIn\nProceedings of the Second International Conference on Knowledge Discovery and Data Mining\n,\nVol.\n96\n,\npp.Â 226â€“231\n.\nCited by:\nÂ§\nIII-A\n1\n.\n[11]\nH. Fang, C. Wang, H. Fang, M. Gou, J. Liu, H. Yan, W. Liu, Y. Xie, and C. Lu\n(2023)\nAnygrasp: robust and efficient grasp perception in spatial and temporal domains\n.\nIEEE Transactions on Robotics\n39\n(\n5\n),\npp.Â 3929â€“3945\n.\nCited by:\nÂ§\nII-B\n,\nÂ§\nII-B\n.\n[12]\nH. Fang, C. Wang, M. Gou, and C. Lu\n(2020)\nGraspnet-1billion: a large-scale benchmark for general object grasping\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 11444â€“11453\n.\nCited by:\nÂ§\nII-B\n,\nÂ§\nII-B\n,\nÂ§\nIV-C\n.\n[13]\nR. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and W. Brendel\n(2018)\nImageNet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness\n.\nIn\nInternational Conference on Learning Representations\n,\nCited by:\nRemark 1\n.\n[14]\nM. Gualtieri and R. Platt\n(2017)\nViewpoint selection for grasp detection\n.\nIn\n2017 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\nVol.\n,\npp.Â 258â€“264\n.\nCited by:\nÂ§I\n.\n[15]\nK. He, X. Zhang, S. Ren, and J. Sun\n(2016)\nDeep residual learning for image recognition\n.\nIn\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n,\nCited by:\nÂ§\nIII-C\n1\n.\n[16]\nJ. Hu and P. R. Pagilla\n(2022)\nView planning for object pose estimation using point clouds: an active robot perception approach\n.\nIEEE Robotics and Automation Letters\n7\n(\n4\n),\npp.Â 9248â€“9255\n.\nCited by:\nÂ§I\n.\n[17]\nZ. Jiang, Y. Zhu, M. Svetlik, K. Fang, and Y. Zhu\n(2021)\nSynergies between affordance and geometry: 6-dof grasp detection via implicit representations\n.\narXiv preprint arXiv:2104.01542\n.\nCited by:\nÂ§I\n,\nÂ§\nIV-C\n.\n[18]\nR. S. Johansson, G. Westling, A. BÃ¤ckstrÃ¶m, and J. R. Flanagan\n(2001)\nEyeâ€“hand coordination in object manipulation\n.\nJournal of Neuroscience\n21\n(\n17\n),\npp.Â 6917â€“6932\n.\nCited by:\nÂ§I\n.\n[19]\nS. Kiciroglu, H. Rhodin, S. N. Sinha, M. Salzmann, and P. Fua\n(2020)\nActiveMoCap: optimized viewpoint selection for active human motion capture\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\nCited by:\nÂ§I\n.\n[20]\nK. Kleeberger, R. Bormann, W. Kraus, and M. F. Huber\n(2020)\nA survey on learning-based robotic grasping\n.\nCurrent Robotics Reports\n1\n(\n4\n),\npp.Â 239â€“249\n.\nCited by:\nÂ§\nII-B\n.\n[21]\nO. Kroemer, S. Niekum, and G. Konidaris\n(2021)\nA review of robot learning for manipulation: challenges, representations, and algorithms\n.\nJournal of Machine Learning Research\n22\n(\n30\n),\npp.Â 1â€“82\n.\nCited by:\nÂ§\nII-B\n.\n[22]\nS. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su,\net al.\n(2024)\nGrounding dino: marrying dino with grounded pre-training for open-set object detection\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 38â€“55\n.\nCited by:\nÂ§\nIII-A\n1\n,\nÂ§\nIII-B\n.\n[23]\nI. Loshchilov and F. Hutter\n(2017)\nDecoupled weight decay regularization\n.\narXiv preprint arXiv:1711.05101\n.\nCited by:\nÂ§\nIV-B\n.\n[24]\nH. Ma, M. Shi, B. Gao, and D. Huang\n(2024)\nActive perception for grasp detection via neural graspness field\n.\nAdvances in Neural Information Processing Systems\n37\n,\npp.Â 38122â€“38141\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n,\nÂ§\nII-C\n.\n[25]\nV. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin, A. Allshire, A. Handa,\net al.\n(2021)\nIsaac gym: high performance gpu-based physics simulation for robot learning\n.\narXiv preprint arXiv:2108.10470\n.\nCited by:\nÂ§\nIII-A\n,\nÂ§\nIV-A\n.\n[26]\nR. Newbury, M. Gu, L. Chumbley, A. Mousavian, C. Eppner, J. Leitner, J. Bohg, A. Morales, T. Asfour, D. Kragic,\net al.\n(2023)\nDeep learning approaches to grasp synthesis: a review\n.\nIEEE Transactions on Robotics\n39\n(\n5\n),\npp.Â 3994â€“4015\n.\nCited by:\nÂ§\nII-B\n.\n[27]\nT. Nguyen, S. Yuan, T. H. Nguyen, P. Yin, H. Cao, L. Xie, M. Wozniak, P. Jensfelt, M. Thiel, J. Ziegenbein, and N. Blunder\n(2024)\nMCD: diverse large-scale multi-campus dataset for robot perception\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 22304â€“22313\n.\nCited by:\nÂ§I\n.\n[28]\nC. R. Qi, L. Yi, H. Su, and L. J. Guibas\n(2017)\nPointnet++: deep hierarchical feature learning on point sets in a metric space\n.\nAdvances in Neural Information Processing Systems\n30\n.\nCited by:\nÂ§\nIII-C\n1\n,\nÂ§\nIV-D\n2\n.\n[29]\nG. Qian, Y. Li, H. Peng, J. Mai, H. Hammoud, M. Elhoseiny, and B. Ghanem\n(2022)\nPointnext: revisiting pointnet++ with improved training and scaling strategies\n.\nAdvances in Neural Information Processing Systems\n35\n,\npp.Â 23192â€“23204\n.\nCited by:\nÂ§\nIII-C\n1\n,\nÂ§\nIV-D\n2\n.\n[30]\nN. Ravi, V. Gabeur, Y. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. RÃ¤dle, C. Rolland, L. Gustafson, E. Mintun, J. Pan,\net al.\n(2025)\nSAM 2: segment anything in images and videos\n.\nIn\nThe Thirteenth International Conference on Learning Representations\n,\nCited by:\nÂ§\nIII-A\n1\n,\nÂ§\nIII-B\n.\n[31]\nT. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y. Chen, F. Yan, Z. Zeng, H. Zhang, F. Li, J. Yang, H. Li, Q. Jiang, and L. Zhang\n(2024)\nGrounded sam: assembling open-world models for diverse visual tasks\n.\narXiv preprint arXiv:2401.14159\n.\nCited by:\nÂ§\nIII-B\n,\nÂ§\nIV-B\n.\n[32]\nA. Sahbani, S. El-Khoury, and P. Bidaud\n(2012)\nAn overview of 3d object grasp synthesis algorithms\n.\nRobotics and Autonomous Systems\n60\n(\n3\n),\npp.Â 326â€“336\n.\nCited by:\nÂ§\nII-B\n.\n[33]\nK. B. Shimoga\n(1996)\nRobot grasp synthesis algorithms: a survey\n.\nThe International Journal of Robotics Research\n15\n(\n3\n),\npp.Â 230â€“266\n.\nCited by:\nÂ§\nII-B\n.\n[34]\nM. J. Tarr and S. Pinker\n(1989)\nMental rotation and orientation-dependence in shape recognition\n.\nCognitive psychology\n21\n(\n2\n),\npp.Â 233â€“282\n.\nCited by:\nÂ§I\n.\n[35]\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, and I. Polosukhin\n(2017)\nAttention is all you need\n.\nAdvances in Neural Information Processing Systems\n30\n.\nCited by:\nÂ§\nIII-C\n2\n.\n[36]\nC. Wang, H. Fang, M. Gou, H. Fang, J. Gao, and C. Lu\n(2021)\nGraspness discovery in clutters for fast and accurate grasp detection\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.Â 15964â€“15973\n.\nCited by:\nÂ§\nII-B\n.\n[37]\nM. Wexler, S. M. Kosslyn, and A. Berthoz\n(1998)\nMotor processes in mental rotation\n.\nCognition\n68\n(\n1\n),\npp.Â 77â€“94\n.\nCited by:\nÂ§I\n.\n[38]\nJ. Wu, X. Lin, B. He, C. FermÃƒÂ¼ller, and Y. Aloimonos\n(2025)\nViewActive: active viewpoint optimization from a single image\n.\nIn\n2025 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\nVol.\n,\npp.Â 11812â€“11818\n.\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-A\n.\n[39]\nX. Wu, J. Cai, J. Jiang, D. Zheng, Y. Wei, and W. Zheng\n(2024)\nAn economic framework for 6-dof grasp detection\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 357â€“375\n.\nCited by:\nÂ§\nII-B\n,\nÂ§\nII-B\n,\nÂ§\nIII-A\n1\n,\nÂ§\nIV-C\n.\n[40]\nX. Wu, L. Jiang, P. Wang, Z. Liu, X. Liu, Y. Qiao, W. Ouyang, T. He, and H. Zhao\n(2024)\nPoint transformer v3: simpler, faster, stronger\n.\nIn\n2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\nVol.\n,\npp.Â 4840â€“4851\n.\nCited by:\nÂ§\nIV-D\n2\n.\n[41]\nY. Wu, Y. Fu, and S. Wang\n(2023)\nInformation-theoretic exploration for adaptive robotic grasping in clutter based on real-time pixel-level grasp detection\n.\nIEEE Transactions on Industrial Electronics\n71\n(\n3\n),\npp.Â 2683â€“2693\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[42]\nH. Yu, X. Zhang, Z. Zhao, and C. He\n(2025)\nTrustworthy robotic grasping: a credibility alignment framework via self-regulation encoding\n.\nIEEE/ASME Transactions on Mechatronics\n(\n).\nExternal Links:\nDocument\nCited by:\nÂ§\nII-B\n,\nÂ§\nIV-C\n.\n[43]\nS. Yu, D. Zhai, and Y. Xia\n(2022)\nEGNet: efficient robotic grasp detection network\n.\nIEEE Transactions on Industrial Electronics\n70\n(\n4\n),\npp.Â 4058â€“4067\n.\nCited by:\nÂ§\nII-B\n.\n[44]\nY. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, and H. Xu\n(2024)\n3D diffusion policy: generalizable visuomotor policy learning via simple 3d representations\n.\nIn\nProceedings of Robotics: Science and Systems\n,\nCited by:\nÂ§\nIII-B\n,\nÂ§\nIV-D\n2\n.\n[45]\nX. Zhang, D. Wang, S. Han, W. Li, B. Zhao, Z. Wang, X. Duan, C. Fang, X. Li, and J. He\n(2023)\nAffordance-driven next-best-view planning for robotic grasping\n.\nIn\n7th Annual Conference on Robot Learning\n,\nCited by:\nÂ§I\n,\nÂ§\nII-C\n.\n[46]\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn\n(2023)\nLearning Fine-Grained Bimanual Manipulation with Low-Cost Hardware\n.\nIn\nProceedings of Robotics: Science and Systems\n,\nCited by:\nÂ§\nIII-C\n2\n.",
    "preview_text": "Active perception in vision-based robotic manipulation aims to move the camera toward more informative observation viewpoints, thereby providing high-quality perceptual inputs for downstream tasks. Most existing active perception methods rely on iterative optimization, leading to high time and motion costs, and are tightly coupled with task-specific objectives, which limits their transferability. In this paper, we propose a general one-shot multimodal active perception framework for robotic manipulation. The framework enables direct inference of optimal viewpoints and comprises a data collection pipeline and an optimal viewpoint prediction network. Specifically, the framework decouples viewpoint quality evaluation from the overall architecture, supporting heterogeneous task requirements. Optimal viewpoints are defined through systematic sampling and evaluation of candidate viewpoints, after which large-scale training datasets are constructed via domain randomization. Moreover, a multimodal optimal viewpoint prediction network is developed, leveraging cross-attention to align and fuse multimodal features and directly predict camera pose adjustments. The proposed framework is instantiated in robotic grasping under viewpoint-constrained environments. Experimental results demonstrate that active perception guided by the framework significantly improves grasp success rates. Notably, real-world evaluations achieve nearly double the grasp success rate and enable seamless sim-to-real transfer without additional fine-tuning, demonstrating the effectiveness of the proposed framework.\n\nA General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint\nDeyunÂ Qin,\nZezhiÂ Liu,\nHanqianÂ Luo,\nXiaoÂ Liang,\nand YongchunÂ Fang,\nSeniorÂ Member,Â IEEE\nThis work is supported in part by the National Natural Science Foundation of China under Grant U25A20473 and in part by the National Natural Science Foundation of China under Grant 62233011.",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "VLA",
        "multimodal",
        "active perception",
        "robotic manipulation",
        "viewpoint prediction",
        "sim-to-real transfer"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºæœºå™¨äººæ“ä½œçš„å•æ¬¡å¤šæ¨¡æ€ä¸»åŠ¨æ„ŸçŸ¥æ¡†æ¶ï¼Œé€šè¿‡é¢„æµ‹æœ€ä¼˜è§†ç‚¹æ¥æå‡æŠ“å–æˆåŠŸç‡ï¼Œå¹¶å®ç°æ— ç¼çš„ä»¿çœŸåˆ°ç°å®è¿ç§»ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T06:12:53Z",
    "created_at": "2026-01-27T15:53:08.482134",
    "updated_at": "2026-01-27T15:53:08.482142"
}