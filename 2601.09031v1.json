{
    "id": "2601.09031v1",
    "title": "Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation",
    "authors": [
        "Xuetao Li",
        "Wenke Huang",
        "Mang Ye",
        "Jifeng Xuan",
        "Bo Du",
        "Sheng Liu",
        "Miao Li"
    ],
    "abstract": "äººå½¢æœºå™¨äººæ“ä½œæ˜¯æ‰§è¡Œå¤šæ ·åŒ–äººç±»æ°´å¹³ä»»åŠ¡çš„å…³é”®ç ”ç©¶é¢†åŸŸï¼Œæ¶‰åŠé«˜å±‚è¯­ä¹‰æ¨ç†ä¸ä½å±‚åŠ¨ä½œç”Ÿæˆã€‚ç„¶è€Œï¼Œç²¾ç¡®çš„åœºæ™¯ç†è§£ä¸åŸºäºäººç±»æ¼”ç¤ºçš„æ ·æœ¬é«˜æ•ˆå­¦ä¹ ä»æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸¥é‡åˆ¶çº¦äº†ç°æœ‰æ¡†æ¶çš„é€‚ç”¨æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„RGMP-Sæ–¹æ³•â€”â€”åŸºäºè„‰å†²ç‰¹å¾çš„å¾ªç¯å‡ ä½•å…ˆéªŒå¤šæ¨¡æ€ç­–ç•¥ï¼Œå®ç°äº†é«˜å±‚æŠ€èƒ½æ¨ç†ä¸æ•°æ®é«˜æ•ˆè¿åŠ¨åˆæˆçš„ååŒä¼˜åŒ–ã€‚ä¸ºå°†é«˜å±‚æ¨ç†é”šå®šäºç‰©ç†ç°å®ï¼Œæˆ‘ä»¬åˆ©ç”¨è½»é‡çº§äºŒç»´å‡ ä½•å½’çº³åç½®ï¼Œåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å®ç°ç²¾ç¡®çš„ä¸‰ç»´åœºæ™¯ç†è§£ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æ„å»ºäº†é•¿æ—¶ç¨‹å‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨ï¼Œæœ‰æ•ˆå¯¹é½è¯­ä¹‰æŒ‡ä»¤ä¸ç©ºé—´çº¦æŸï¼Œæœ€ç»ˆåœ¨æœªè§ç¯å¢ƒä¸­å®ç°é²æ£’æ³›åŒ–ã€‚é’ˆå¯¹æœºå™¨äººåŠ¨ä½œç”Ÿæˆä¸­çš„æ•°æ®æ•ˆç‡é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€’å½’è‡ªé€‚åº”è„‰å†²ç½‘ç»œã€‚é€šè¿‡é€’å½’è„‰å†²å‚æ•°åŒ–æœºå™¨äºº-ç‰©ä½“äº¤äº’ä»¥ä¿æŒæ—¶ç©ºä¸€è‡´æ€§ï¼Œå……åˆ†æå–é•¿æ—¶ç¨‹åŠ¨æ€ç‰¹å¾ï¼ŒåŒæ—¶ç¼“è§£ç¨€ç–æ¼”ç¤ºåœºæ™¯ä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚æˆ‘ä»¬åœ¨Maniskillä»¿çœŸåŸºå‡†ä¸ä¸‰å¥—å¼‚æ„ç°å®æœºå™¨äººç³»ç»Ÿï¼ˆåŒ…æ‹¬è‡ªä¸»ç ”å‘çš„äººå½¢æœºå™¨äººã€æ¡Œé¢æœºæ¢°è‡‚åŠå•†ç”¨æœºå™¨äººå¹³å°ï¼‰ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚å®è¯ç»“æœéªŒè¯äº†æœ¬æ–¹æ³•ç›¸è¾ƒäºå‰æ²¿åŸºçº¿çš„ä¼˜è¶Šæ€§ï¼Œå¹¶è¯æ˜äº†æ‰€ææ¨¡å—åœ¨å¤šæ ·åŒ–æ³›åŒ–åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºä¿ƒè¿›å¯å¤ç°æ€§ï¼Œæºä»£ç ä¸æ¼”ç¤ºè§†é¢‘å·²å…¬å¼€äºhttps://github.com/xtli12/RGMP-S.gitã€‚",
    "url": "https://arxiv.org/abs/2601.09031v1",
    "html_url": "https://arxiv.org/html/2601.09031v1",
    "html_content": "Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation\nXuetao Li\nÄ°D\n, Wenke Huang\nÄ°D\n, Mang Ye\nÄ°D\n,Â , Jifeng Xuan\nÄ°D\n,\nBo Du\nÄ°D\n,Â , Sheng Liu\nÄ°D\n,Â , and Miao Li\nÄ°D\nThe authors are with the School of Computer Science, School of Robotics, Institute of Technological Sciences, Wuhan University, Wuhan, China 430072. E-mail:{xtli312, wenkehuang, yemang, jxuan, dubo, shengliu, miao.li}@whu.edu.cn, (Xuetao Li and Wenke Huang contributed equally, Jifeng Xuan and Miao Li are corresponding authors).\nA preliminary version of this work has appeared in Proceedings of the AAAI Conference on Artificial Intelligence (AAAI 2026)\n[\n1\n]\n.\nAbstract\nHumanoid robot manipulation is a crucial research area for executing diverse human-level tasks, involving high-level semantic reasoning and low-level action generation. However, precise scene understanding and sample-efficient learning from human demonstrations remain critical challenges, severely hindering the applicability and generalizability of existing frameworks.\nThis paper presents a novel RGMP-S, Recurrent Geometric-prior Multimodal Policy with Spiking features, facilitating both high-level skill reasoning and data-efficient motion synthesis.\nTo ground high-level reasoning in physical reality, we leverage lightweight 2D geometric inductive biases to enable precise 3D scene understanding within the vision-language model. Specifically, we construct a Long-horizon Geometric Prior Skill Selector that effectively aligns the semantic instructions with spatial constraints, ultimately achieving robust generalization in unseen environments.\nFor the data efficiency issue in robotic action generation, we introduce a Recursive Adaptive Spiking Network. We parameterize robot-object interactions via recursive spiking for spatiotemporal consistency, fully distilling long-horizon dynamic features while mitigating the overfitting issue in sparse demonstration scenarios.\nExtensive experiments are conducted across the Maniskill simulation benchmark and three heterogeneous real-world robotic systems, encompassing a custom-developed humanoid, a desktop manipulator, and a commercial robotic platform. Empirical results substantiate the superiority of our method over state-of-the-art baselines and validate the efficacy of the proposed modules in diverse generalization scenarios. To facilitate reproducibility, the source code and video demonstrations are publicly available at\nhttps://github.com/xtli12/RGMP-S.git\n.\nIndex Terms:\nHumanoid Robot Manipulation, Geometric Prior, Recurrent\nSpiking Feature Learning.\n1\nIntroduction\nHumanoid robots represent a pivotal frontier in embodied intelligence, capable of merging high-level cognition with dexterous physical interaction\n[\n53\n,\n29\n]\n, largely propelled by the emergence of foundation models trained on massive-scale datasets\n[\n63\n]\n. However, in real-world deployment, robotic agents face inherently unstructured environments governed by complex physical laws, which differ significantly from the static internet data distributions used in pre-training\n[\n25\n,\n23\n]\n. Due to the infinite variability of 3D scenes and the exorbitant cost of collecting high-fidelity interaction data, the prevailing â€brute-forceâ€ scaling paradigm, which relies on exhaustive statistical memorization, becomes prohibitive and sample-inefficient for mastering dynamic manipulation tasks\n[\n2\n]\n. Driven by such real-world challenges, augmenting architectures with explicit geometric reasoning and spatial awareness has been increasingly explored, as it equips agents with structural inductive biases to decipher physical constraints effectively\n[\n48\n]\n.\nFigure 1:\nOverview of our framework.\nBy integrating geometric commonsense with spatiotemporal features, our RGMP-S encodes the robot-target spatial relationships for manipulation tasks. RGMP-S achieves a 19% performance improvement and exhibits\n5\nÃ—\n5\\times\ngreater data efficiency compared to the Diffusion Policy (DP) baseline.\nVision-Language Models (VLMs), exemplified by PaLM-E and InstructBLIP, excel at parsing spatial semantic information from multimodal inputs to generate logical robotic task plans\n[\n13\n,\n12\n,\n31\n]\n. However, a critical gap remains in the execution phase: current architectures struggle to translate abstract instructions into contextually appropriate robotic skills\n[\n52\n]\n. The fundamental bottleneck lies in the 3D geometric grounding process, where VLMs, despite their proficiency in object identification, often fail to infer the physical interaction affordances dictated by specific geometries\n[\n3\n]\n. Specifically, regarding skill selection in open-world environments, this disconnect leads to a breakdown in differentiating manipulation primitives. Standard VLMs often confuse geometrically sensitive actions, such as grasping versus pinching, when encountering diverse object shapes\n[\n45\n,\n51\n]\n. For instance, a model trained to grasp standard Coke cans typically targets the cylindrical sidewall. When presented with a crushed Coke can, the VLM retains semantic recognition but fails to perceive the structural deformation. Consequently, the robot attempts to execute the original side-grasping primitive, which is no longer physically feasible for the flattened geometry, resulting in task failure, as illustrated in\nFig.\n1\n. In fact, the flattened profile necessitates a pinching motion rather than the conventional side grasp. Such failures stem from a lack of explicit\ngeometric reasoning\n. Existing approaches usually combine spatial indicators such as bounding boxes or contact points with high-level task descriptions\n[\n36\n,\n11\n]\n. However, these methods predominantly rely on coarse semantic embeddings. In dynamic settings, such dependence results in plans that maintain semantic validity yet remain physically inexecutable\n[\n43\n]\n. This deficiency emphasizes the critical necessity for systems capable of verifying skill feasibility through rigorous geometric analysis. This leads to a core research question:\nI)\nHow can robots effectively exploit geometric reasoning to reliably select feasible manipulation skills?\nDespite the remarkable progress in robotic manipulation driven by generative models, deploying these policies in dynamic environments remains a formidable challenge. A predominant paradigm, Diffusion Policies, has demonstrated exceptional capability in modeling action space by leveraging Gaussian noise to represent robotic behaviors\n[\n10\n]\n. By learning the gradient of the data distribution, these methods can generate high-fidelity trajectories that capture human-like dexterity\n[\n38\n]\n. However, this expressiveness comes at a steep computational cost. The inherent mechanism of diffusion models relies on an iterative reverse denoising process. As highlighted in recent analyses\n[\n63\n,\n49\n]\n, this sequential computation introduces prohibitive inference latency, rendering standard diffusion approaches ill-suited for high-frequency, responsive manipulation tasks where real-time feedback loops are critical. To circumvent the latency bottleneck of iterative solvers, recent advances in Vision-Language-Action (VLA) models and Transformer-based architectures advocate for direct, end-to-end action generation\n[\n7\n,\n63\n,\n52\n]\n. These methods typically tokenize visual observations and directly predict actions in a feed-forward manner, ensuring rapid inference. However, achieving robust generalization with these architectures has historically demanded a brute-force strategy: scaling model parameters to billions and training on massive, diverse datasets. While effective at scale, this approach is data-inefficient and computationally heavy. Crucially, these methods often treat visual inputs as flattened sequences of semantic tokens, ignoring the intrinsic spatiotemporal mechanisms of manipulation. They lack explicit inductive biases to capture the fine-grained\nspatial relationships\nbetween the robotic end-effector and the target object, and often fail to distinguish task-relevant features from the dense, noisy visual context\n[\n46\n,\n30\n]\n. Consequently, without massive data covering every variation, these black-box mappings are prone to latching onto spurious background correlations, leading to poor performance under domain shifts or in cluttered scenes\n[\n24\n]\n. Thus, a critical research gap exists between the need for expressive, real-time control and the requirement for data-efficient generalization. The challenge lies in moving beyond simple parameter scaling or inefficient iterative denoising. Instead, we must explore how to construct an architecture that naturally embeds the spatiotemporal structure of manipulation tasks. This leads to the other core research question:\nII)\nHow can robots effectively learn robust data-efficient policies from limited demonstrations?\nTo surmount the deficiency in spatial-geometric reasoning outlined in\nI)\n, inspired by the efficiency of human cognitive processes in synthesizing vision and logic, a preliminary version of this work published in AAAI 2026\n[\n1\n]\nexplores geometric priors via a lightweight commonsense tuning strategy. In particular, we aim to bridge the gap between 3D geometric perception and semantic task planning through implicit geometric information decoding from 2D inputs. Traditional explicit 3D reconstruction provides detailed spatial information, but it exhibits high computational latency and sensitivity to noise\n[\n47\n]\n, and lacks of adaptability for real-time robotic interaction. This motivates us to bypass costly reconstruction in favor of capturing implicit geometric structures directly from RGB inputs. We propose the\nLong-horizon Geometric-prior Skill Selector\n(LGSS) to encode spatial affordances into the latent space of the policy, effectively aligning 3D spatial indicators with robotic action spaces and commonsense reasoning. Specifically, we integrate geometric inductive biases into a VLM with minimal fine-tuning, requiring only a concise set of rule-based constraints. This approach offers a practical solution to realize robust decision-making in unseen environments. Furthermore, we believe that LGSS is essential because it introduces a decision-making process analogous to human cognition\n[\n5\n]\n, which mirrors the human ability to synthesize an action plan for the long-horizon manipulation task.\nTo this end, we develop the LGSS framework to enable the manipulation of diverse objects via rigorous geometric consistency checks, proving its effectiveness and robustness in real-world humanoid deployment.\n8\n14\n12\nFigure 2:\nPipeline of RGMP-S\n. Upon receiving a speech command, the robot utilizes LGSS (see\nÂ§\n3.1\nfor details) to identify and localize the target object. By integrating object coordinates, shape cues (extracted via Yolov8n-seg\n[\n58\n]\nmodel\nÏ•\n\\phi\n), and geometric-prior knowledge, the robot selects an appropriate skill from the library, where each primitive is associated with a pretrained RASNet model (see\nÂ§\n3.2\nfor details). The selected RASNet model subsequently executes the task precisely through adaptive recursive feature extraction and GMM-based refinement.\nTo surmount the overfitting risks inherent to limited data (\nII\n), we deviate from data-hungry scaling and instead embed explicit spatiotemporal inductive biases via recursive spiking dynamics, enabling the efficient distillation of robust skills from sparse demonstrations. In our conference version\n[\n1\n]\n, we employed recurrent computation to construct spatial relationships between the robotic end-effector and the target object, offering a foundational policy for action generation. However, standard recursive models are prone to vanishing gradients during global modeling\n[\n37\n]\nand often struggle to distinguish task-salient features from complex backgrounds\n[\n6\n]\n. We posit that effective spatial memory requires dynamic regulation to prevent the loss of key historical cues while suppressing noise. Purely spatial processing lacks the temporal reasoning necessary to filter redundant information from dense visual features\n[\n44\n]\n. This motivates us to introduce an adaptive mechanism that integrates spiking information to refine feature retention. In this journal version, we propose the\nRecursive Adaptive Spiking Network\n(RASNet), which incorporates Spiking Dense Feature Extraction (SDFE) to embed temporal reasoning into spatial feature understanding. We utilize Rotary Position Embeddings (RoPE) and recursive computation to form a global spatial memory, while simultaneously deploying an Adaptive Decay Mechanism and Adaptive Spike Neurons (ASN) to dynamically control memory retention and amplify task-critical features. Finally, we apply Gaussian Mixture Models (GMM) to approximate the joint motions. Compared with our conference version, SDFE not only effectively suppresses redundancy within dense information but also significantly enhances the generalization ability in unseen scenarios featuring complex backgrounds.\nOur primary contribution is to present RGMP-S, a unified framework that bridges the semantic-geometric gap to achieve robust, data-efficient, and generalizable robotic manipulation. To successfully facilitate high-level skill reasoning and low-level motion synthesis, we present the following technical contributions:\nâ€¢\nLong-horizon Geometric-prior Skill Selector (LGSS).\nWe introduce a lightweight geometric tuning strategy that fuses vision-language models with low-rank geometric adapters. By injecting shape-level commonsense biases, LGSS aligns semantic instructions with latent geometric constraints, enabling the precise selection of parameterized skills from a pretrained library without requiring task-specific fine-tuning.\nâ€¢\nRecursive Adaptive Spiking Network (RASNet).\nWe formulate a novel policy architecture to investigate intrinsic spatiotemporal mechanisms in robotic manipulation. By modulating latent representations through Adaptive Spike Neurons (ASN) and recursive computation, this network captures directional spatial dependencies within a spatiotemporally consistent latent space. This design effectively distills task-critical features from dense visual observations while mitigating optimization conflicts in sparse data regimes.\nâ€¢\nData-Efficient Generalization.\nWe conduct a rigorous validation of the RGMP-S framework across the ManiSkill benchmark and three diverse physical robotic platforms. Extensive real-world experiments demonstrate that our framework ensures robustness against dynamic disturbances, significantly outperforming the Diffusion Policy baseline with an 89% generalization success rate and 5\nÃ—\n\\times\nsuperior data efficiency.\nThis manuscript represents a substantial extension of our preliminary version published in AAAI 2026\n[\n1\n]\n. We augment the original work in the following aspects:\nâ€¢\nAlgorithmic Advancement in Reasoning.\nWe advance the preliminary skill selector to the LGSS via a hierarchical Chain-of-Thought (CoT) mechanism. This enhancement enables the structured decomposition of intricate goals into multi-stage sequences, significantly boosting performance on long-horizon manipulation tasks (e.g., towel folding and water pouring).\nâ€¢\nArchitectural Evolution for Robustness.\nWe evolve the previous visuomotor module with Spiking Dense Feature Extraction (SDFE) and Adaptive Spike Neurons (ASN) to dynamically filter redundant visual noise in cluttered scenes (as shown in\nFig.\n2\n). This addresses the limitations of the original model regarding feature redundancy and lack of temporal logic, ensuring robust action synthesis in unstructured environments.\nâ€¢\nExpanded Real-World Evaluation.\nWe broaden the experimental scope by incorporating a commercial Aloha robot\n[\n15\n]\nand introducing three additional challenging long-horizon manipulation tasks: towel folding, water pouring, and bin picking, as detailed in\nÂ§\n4.1\n.\n2\nRelated Work\n2.1\nVision-Language Models for Robotic Manipulation\nSemantic Planning and Grounding\n. A pioneering line of work leverages the semantic reasoning capabilities of Large Language Models (LLMs) and Vision-Language Models (VLMs) to decompose high-level instructions into a sequence of robotic primitives\n[\n3\n,\n13\n,\n12\n]\n. While these foundation models excel at high-level task planning, they often treat the physical world as a semantic abstraction. Consequently, they suffer from a\nsemantic-geometric gap\n, where the generated plans are logically sound but physically inexecutable due to a lack of low-level spatial awareness. Subsequently, a large body of methods\n[\n60\n,\n50\n,\n32\n,\n33\n]\nbegan to incorporate explicit 3D representations, such as voxels or neural radiance fields, to ground semantic queries into physical space. For instance, recent research has explored establishing dense correspondence between visual observations and linguistic concepts to enhance manipulation precision\n[\n40\n]\n. These methods mainly focus on explicit 3D reconstruction or external trajectory solvers to bridge the gap. However, when robots operate in unstructured open-world environments, explicit reconstruction exhibits high computational latency and sensitivity to perception noise\n[\n47\n]\n. Meanwhile, relying on external motion planners separates semantic reasoning from control, which is a suboptimal setting for dynamic interaction. In this paper, we focus on learning implicit geometric priors directly from 2D RGB inputs. By fusing geometric adapters with VLMs, our LGSS avoids costly reconstruction while maintaining robust spatial reasoning.\nGeometric Reasoning in Manipulation\n.\nWith the demand for precise object interaction, geometric-aware manipulation has been an active research field. One direction is introducing object-centric representations\n[\n59\n,\n35\n]\nthat predict keypoints or 6-DoF poses. However, these techniques struggle with category-agnostic objects or deformable materials where defining canonical poses is difficult. Recently, a specific line of research\n[\n51\n,\n45\n,\n14\n]\nleverages affordance maps or contact primitives to guide grasping. Therefore, these approaches heavily rely on the quality of pre-defined primitives and often fail to distinguish fine-grained manipulation modes (e.g., grasping versus pinching) under structural deformation. Recent works\n[\n26\n,\n61\n,\n36\n]\nhave proven the feasibility of combining bounding boxes with language. However, these methods reach only coarse spatial alignment, which is not suitable for learning a generalizable policy for diverse geometries and thus lead to task failure in unseen scenarios. In this paper, based on the Geometric Prior Skill Selector, we inject shape-level commonsense biases to enforce rigorous geometric consistency checks, thereby effectively mitigating uncertainties and handling dynamic constraints to ensure robust reasoning in tasks.\n2.2\nGenerative Robotic Policy Learning\nImitation learning\n[\n39\n,\n42\n]\nhas been an essential paradigm in robotics, with the goal of acquiring robust policies from expert demonstrations\n[\n41\n]\n. Existing works can be broadly divided into two branches: Transformer-based methods\n[\n62\n,\n7\n,\n9\n,\n22\n]\nand Diffusion-based methods\n[\n10\n,\n27\n]\n. Transformer architectures, such as the Decision Transformer\n[\n8\n]\n, treat control as a sequence modeling problem. Specifically, self-attention mechanisms in Transformers have been shown to effectively capture global dependencies\n[\n19\n]\n. However, achieving generalization with Transformers has historically demanded a brute-force scaling strategy involving billions of parameters and massive datasets\n[\n63\n]\n. In contrast, Diffusion Policies, they model the action distribution using iterative denoising processes. Despite their expressiveness in capturing multimodal distributions, the sequential denoising computation introduces prohibitive inference latency. Unlike standard diffusion approaches, we focus on a feed-forward recursive architecture that ensures high-frequency real-time responsiveness without the heavy computational strain typically associated with iterative solvers.\n2.3\nSpatiotemporal Representation\nLearning effective spatiotemporal representations is critical for mastering dynamic tasks. Traditional approaches utilize Recurrent Neural Networks (RNNs) or LSTMs\n[\n21\n]\nto model temporal dependencies. However, standard recursive models are prone to vanishing gradients and often struggle to filter redundant information from dense visual streams. Recently, Spiking Neural Networks (SNNs) have emerged as a powerful method for processing temporal information with high energy efficiency and sparse computation\n[\n18\n,\n56\n,\n57\n]\n. Related methods have explored SNNs for event-based vision and motion estimation\n[\n56\n,\n16\n]\n. The key difference between our RASNet and the aforementioned efforts is that ours is designed for dense visual feature extraction rather than purely neuromorphic processing. Inspired by the efficiency of spiking neurons, we construct the Adaptive Spike Neuron (ASN) to modulate feature retention. Furthermore, we investigate how to better maintain global spatial dependencies and develop the Recursive Adaptive Spiking Network, which captures intrinsic spatiotemporal mechanisms and ensures data efficiency under limited demonstrations.\n3\nMethodology\nTABLE I:\nNotations\ntable.\nDescription\nDescription\nğ’«\n\\mathcal{P}\nGenerated action plan\nâ„\n\\mathcal{I}\nCurrent user instruction\nğ’ª\n\\mathcal{O}\nCurrent visual observation\nğ’\n\\mathcal{C}\nPredefined contextual set\nğ’«\ni\n\\mathcal{P}_{i}\nAction sequence (\nEq.\n1\n)\nğ’²\n\\mathcal{W}\nAdaptive decay factors (\nEq.\n3\n)\nF\n0\nF_{0}\nInitial feature map (\nEq.\n2\n)\nX\nk\nX_{k}\nSpatial attention mask (\nEq.\n9\n)\nB\nk\nB_{k}\nSpatial attention feature (\nEq.\n9\n)\nD\nk\nD_{k}\nFeature for attention processing\nQ\nA\nQ_{A}\nSpiking dense feature (\nEq.\n11\n)\nF\nk\nF_{k}\nExtracted dense feature map\nğ’œ\n\\mathcal{A}\nAdaptive spike neuron function\nÎ¼\nk\n\\mu_{k}\nMean of\nk\nk\n-th GMM component\nF\nf\nF_{f}\nFused feature map (\nEq.\n14\n)\na\ni\nâ€‹\nn\na_{in}\nInitial action prediction (\nEq.\n16\n)\nÎ£\nk\n\\Sigma_{k}\nCovariance of\nk\nk\n-th GMM component\nl\nk\nl_{k}\nMahalanobis distance (\nEq.\n20\n)\nK\nK\nNumber of GMM components\nÎ¼\nk\nÏ‰\n\\mu_{k}^{\\omega}\nMean of\nk\nk\n-th GMM component\nOverview of RGMP-S.\nTo achieve generalizable robotic manipulation, we introduce\nR\necurrent\nG\neometric-prior\nM\nultimodal\nP\nolicy with\nS\npiking Features (RGMP-S). This framework generates task plans and 6-DoF actions for humanoid robots using egocentric RGB inputs. RGMP-S comprises two key components: the\nL\nong-horizon\nG\neometric-prior\nS\nkill\nS\nelector (LGSS) and the\nR\necursive\nA\ndaptive\nS\npiking\nNet\nwork (RASNet). The LGSS translates natural language commands and visual observations into executable skill sequences by incorporating geometric commonsense priors. Complementarily, the RASNet converts visual inputs into robotic manipulation actions through spatiotemporal memory modeling.\nInstead of employing computationally expensive explicit 3D reconstruction, the policy learns to extract implicit geometric information from RGB inputs by correlating spatial cues with action labels and commonsense reasoning. Additionally, we incorporate spiking neural networks to process dense features that capture fine-grained spatial perception. This approach enables the model to focus on task-relevant visual regions, emphasizing important details while suppressing redundant information, thus enhancing performance. This method is inspired by the natural and intuitive process of human object manipulation. In human grasping, the eyes first detect the target object, and the brain rapidly processes this sensory input to generate an appropriate motor command for the hand, allowing the arm to execute the grasping action and complete the task successfully. Similarly, our RGMP-S mimics this process by seamlessly integrating perception with action: the RGB image is analyzed to identify the object and its relevant geometric context, while the RASNet is employed to predict the joint angles required for the robotic arm to grasp the object. A summary of notations is provided in\nTab.\nI\n.\n3.1\nLong-horizon Geometric-prior Skill Selector\nMotivation of Geometric Priors.\nA fundamental challenge in robotics is the precise selection of skills (e.g., grasping versus pinching) for objects with diverse geometries or in previously unseen scenarios. While VLMs support object recognition and localization, they often struggle to translate semantic information into accurate actions, primarily due to their lack of explicit integration of\ngeometric priors\nin the vision-action mapping process. This limitation motivates the development of our novel LGSS framework, which seamlessly combines geometric reasoning with semantic task planning through geometric commonsense alignment.\nThe LGSS operates through two sequential phases to bridge semantic instruction and physical execution. Initially, a VLM\n[\n4\n]\nis employed to interpret human instructions, allowing the robot to detect and locate the target object within the provided image. This visual grounding step generates bounding boxes that anchor subsequent reasoning processes. Building on this visual localization, the system then analyzes spatial relationships and morphological characteristics of the identified objects. This geometric analysis incorporates commonsense knowledge about object properties and their functional implications for manipulation. The resulting object-centric representation, combining semantic and geometric features, guides the selection of appropriate manipulation strategies from a predefined skill library. This structured approach enables the robot to translate abstract commands into context-aware action sequences through a novel geometric-object decomposition mechanism that coordinates perception with action selection:\nğ’«\n=\np\nâ€‹\nl\nâ€‹\na\nâ€‹\nn\nâ€‹\n(\nâ„\n,\nğ’ª\n|\nğ’\n)\n,\n\\small\\mathcal{P}=plan(\\mathcal{I},{\\kern 1.0pt}{\\kern 1.0pt}\\mathcal{O}{\\kern 2.0pt}|{\\kern 2.0pt}\\mathcal{C}\\;),\n(1)\nwhere\nğ’«\n\\mathcal{P}\nrepresents the generated action plan,\nâ„\n\\mathcal{I}\nis the current natural language user instruction,\nğ’ª\n\\mathcal{O}\ndenotes the current visual observation, and\nğ’\n\\mathcal{C}\nconstitutes a predefined contextual set containing\nn\nn\nexamples\n{\n(\nâ„\ni\n,\nğ’ª\ni\n,\nğ’«\ni\n)\n}\ni\n=\n1\nn\n\\{({\\mathcal{I}_{i}},{\\mathcal{O}_{i}},{\\mathcal{P}_{i}})\\}_{i=1}^{n}\n, which encapsulates instructions, prompts, and commonsense reasoning to facilitate in-context learning.\nThe system processes an RGB image\nğ’ª\n\\mathcal{O}\nwhere a Visual-Language Model first identifies the target object with a bounding box. This localization provides the foundation for generating executable manipulation skills. The model combines predefined context\nğ’\n\\mathcal{C}\nwith geometric properties like spatial relationships and object shape to determine these skills. For example, when receiving the instruction\nâ€œI want Fantaâ€\n, the LGSS follows the contextual prompt\nâ€œPlease box the target object in the instructionâ€\n. It locates the specific beverage can among other items, then uses YOLOv8n-seg to determine its shape. Finally, the visual-language model integrates contextual information\nğ’\n\\mathcal{C}\nwith geometric reasoning to produce operational commands.\nThe LGSS adopts a modular design with minimal computational overhead. Its architecture integrates three key components: a visual-language interpretation module for command processing, a semantic segmentation module for object analysis, and a prompt-based skill selector for action determination. These elements operate synergistically to transform user instructions and visual inputs into appropriate skill choices. The complete inference process requires 105 milliseconds when executed on an NVIDIA 4090 GPU.\nVisual-Language Interpretation Module:\nThis component employs the Qwen-vl API for visual-language understanding, utilizing structured prompt engineering to incorporate prior knowledge. The preparation involves providing the API with carefully selected training instances, each containing: (1) an RGB image with an object bounding box, (2) corresponding semantic segmentation results, (3) manually annotated shape categories (such as â€œcylindricalâ€ or â€œcrushedâ€), and (4) the corresponding manipulation skill. For example, training data includes a Fanta can image with a bounding box and segmentation mask, labeled as â€œcylindrical shapeâ€ and paired with the â€œside graspâ€; similarly, an encircled Fanta can is associated with the â€œlift upâ€. The model is primed with 20 representative examples spanning common object categories to establish a robust mapping between visual features, geometric attributes, and manipulation strategies. During operation, the API processes input prompts formatted as:\nâ€œInstruction: Identify target object in image and output bounding box [x1, y1, x2, y2]â€\n. This configuration achieves 93.1% localization accuracy on our dataset with a mean latency of 45 ms per query.\nSemantic Segmentation Module:\nThis component processes the bounding box coordinates provided by the Qwen-vl API by extracting the corresponding image region and passing it to a YOLOv8-seg model. The model, which has been pre-trained on large-scale datasets and subsequently fine-tuned on our proprietary data, generates precise semantic segmentation masks to capture the objectâ€™s geometric structure. On our evaluation dataset, the module achieves a mean Intersection over Union (mIoU) of 97.6%, demonstrating the reliability of its shape extraction capabilities for subsequent geometric reasoning.\nFigure 3:\nStructure of Spatial Mixing Block and Channel Mixing Block.\nThe Spatial Mixing Block uses ADM to generate Dynamic Decay\nğ’²\n\\mathcal{W}\nrecursively, and employs RoPE to introduce directional awareness relative to spatial positions. These mechanisms collectively enhance the ability of the block to integrate spatial information effectively. The Channel Mixing Block reallocates channel-wise feature responses by integrating correlations between channels. See\nÂ§\n3.2\nfor details.\nGeometry-Grounded CoT Skill Reasoner:\nThis module orchestrates the translation of perceptual inputs into executable actions by synthesizing bounding box coordinates with morphological features via the Qwen-vl reasoning engine. Instead of simple instruction following, we deploy a structured Chain-of-Thought (CoT) protocol to enforce physical grounding. The system is instantiated with a constraint-aware prompt:\nâ€œ\nYou are a robot with five skills: Side Grasp, Lift Up, Top Pinch, Pour Water and Delivery. The image you are observing has a resolution of 640x480. Based on the observation, the coordinates of the bounding box and the shape information of the object, choose the skill without collision and generate an action plan based on these skills.\nâ€\nBy embedding task-specific primitives (e.g., Side Grasp, Pour Water) within this reasoning framework, the model explicitly maps latent geometric affordances to appropriate manipulation strategies. Crucially, this mechanism resolves operational ambiguities by integrating safety-critical constraints, prioritizing kinematically feasible and collision-free interactions based on environmental context. The final decision is derived from a confidence-weighted assessment of candidate skills, ensuring robust planning under uncertainty.\nSkill Library\nThe skill library encompasses a comprehensive set of predefined grasping configurations parameterized by finger joint angles, supporting three core manipulation primitives. Side grasping is employed when objects can be accessed laterally without obstruction. Lifting is utilized for vertical retrieval, often in contexts where obstacles limit horizontal approach. Pinching is reserved for handling thin or delicate items such as paper sheets and cables. These skills define hand posture without imposing object-specific constraints. The LGSS selects the appropriate skill by combining geometric priors with real-time RGB observations, determining feasible hand configurations for the current scenario. Each skill corresponds to distinct model parameters, and the RGMP-S subsequently generates corresponding action trajectories dynamically during real-time inference.\n3.2\nRecursive Adaptive Spiking Network\nMotivation of RASNet.\nEffective robotic operation necessitates a precise understanding of spatial relationships within visual scenes, particularly the correspondence between image regions and the robotic end-effector position. Existing methods often face challenges in inferring such relational structures in novel environments, primarily due to limitations in modeling spatial relationships and dense representation learning that hinder generalization. To address this issue, we introduce the RASNet framework. This framework adaptively learns spatial dependencies between the robot and target objects in previously unseen settings through recursive spiking feature processing, while enhancing robustness and mitigating overfitting by employing Gaussian action space modeling when training data is scarce.\nOur RASNet advances action generation through three interconnected technical innovations. An Adaptive Decay Mechanism (ADM) dynamically regulates the retention of historical context, enabling the model to capture long-range dependencies while maintaining spatial accuracy. Adaptive spiking neurons selectively amplify task-relevant visual features, such as object contours, and suppress non-essential background information within dense feature maps. These elements are unified by a hierarchical fusion framework, which seamlessly integrates multi-scale representations to preserve both high-level semantics and detailed spatial structure through learned attention weighting.\nSpecifically, the RASNet comprises three principal components: (1) a Stem layer for preliminary feature extraction, (2) three hierarchical processing stages, each containing two RASNet blocks, and (3) a multi-scale feature fusion module integrated with a GMM, as illustrated in Fig.\n2\n. Given an input RGB image\nğ’ª\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n3\n\\mathcal{O}\\in\\mathbb{R}^{H\\times W\\times 3}\n, the Stem layer performs initial spatial downsampling and feature embedding, reducing the resolution to\nH\n4\nÃ—\nW\n4\n\\frac{H}{4}\\times\\frac{W}{4}\nwhile preserving critical low-frequency information. This design significantly mitigates computational overhead for subsequent operations. The transformation can be formally expressed as:\nF\n0\n=\nğ’«\nâ€‹\n(\nS\nâ€‹\nR\nâ€‹\ne\nâ€‹\nL\nâ€‹\nU\nâ€‹\n(\nB\nâ€‹\nN\nâ€‹\n(\nğ’\n3\nÃ—\n3\nâ€‹\n(\nğ’ª\n)\n)\n)\n)\n,\n\\small F_{0}=\\mathcal{P}(SReLU(BN(\\mathcal{C}_{3\\times 3}(\\mathcal{O})))),\n(2)\nwhere\nğ’\n3\nÃ—\n3\nâ€‹\n(\nâ‹…\n)\n\\mathcal{C}_{3\\times 3}(\\cdot)\ndenotes a\n3\nÃ—\n3\n3\\times 3\nconvolutional layer,\nB\nâ€‹\nN\nâ€‹\n(\nâ‹…\n)\nBN(\\cdot)\nrefers to batch normalization, and\nS\nâ€‹\nR\nâ€‹\ne\nâ€‹\nL\nâ€‹\nU\nâ€‹\n(\nâ‹…\n)\nSReLU(\\cdot)\nrepresents the Squared ReLU activation function. This activation improves gradient flow and increases the networkâ€™s nonlinear representational capacity. The operator\nğ’«\nâ€‹\n(\nâ‹…\n)\n\\mathcal{P}(\\cdot)\ncorresponds to a max pooling layer. The resulting feature map\nF\n0\nF_{0}\nis then split into two parallel processing pipelines within the RASNet block at Stage 1: one for dynamic recursive computation and another for spiking-based dense feature extraction.\nDynamic Recursive Computation.\nIn this branch,\nF\n0\nF_{0}\nis first processed by the ADM to generate content-adaptive decay factors that modulate historical memory retention. This process extracts local spatial information through convolutional layers, applies channel-wise recalibration, and constrains the decay factor to the range\n[\n0\n,\n1\n]\n[0,1]\nvia a sigmoid activation function, formulated as:\nğ’²\n=\nÏƒ\nâ€‹\n(\nğ’\n1\nÃ—\n1\nâ€‹\n(\nS\nâ€‹\nR\nâ€‹\ne\nâ€‹\nL\nâ€‹\nU\nâ€‹\n(\nğ’\n3\nÃ—\n3\nâ€‹\n(\nF\n0\n)\n)\n)\n)\n,\n\\small\\mathcal{W}=\\sigma(\\mathcal{C}_{1\\times 1}(SReLU(\\mathcal{C}_{3\\times 3}(F_{0})))),\n(3)\nwhere\nğ’\n1\nÃ—\n1\nâ€‹\n(\nâ‹…\n)\n\\mathcal{C}_{1\\times 1}(\\cdot)\ndenotes a\n1\nÃ—\n1\n1\\times 1\nconvolutional layer that performs channel-wise recalibration, and\nÏƒ\nâ€‹\n(\nâ‹…\n)\n\\sigma(\\cdot)\nrepresents the sigmoid activation function. We then apply Rotary Position Embeddings (RoPE) to encode positional information via geometric rotations in feature space. This method enhances the capacity of model to capture position-sensitive dependencies. Unlike standard additive or learnable position embeddings, RoPE incorporates relative position awareness by applying rotational transformations to queries and keys during attention computation. This design offers two main advantages: it removes learnable position parameters, thereby reducing overfitting risk while preserving efficiency; and it encodes positional offsets as angular shifts, ensuring attention scores inherently reflect relative token distances. Formally, we first generate the keys\nK\ns\nK_{s}\nand values\nV\ns\nV_{s}\nfrom\nF\n0\nF_{0}\nvia a\n1\nÃ—\n1\n1\\times 1\nconvolution. To incorporate positional awareness, we apply the rotation matrix\nğ‘\nÎ˜\n\\mathbf{R}_{\\Theta}\nto pairs of channels in\nK\ns\nK_{s}\n. The rotation matrix is defined as:\nğ‘\nÎ˜\nâ€‹\n(\nx\n)\n=\n[\ncos\nâ¡\n(\nÎ¸\nh\n,\nw\n,\nj\n)\nâˆ’\nsin\nâ¡\n(\nÎ¸\nh\n,\nw\n,\nj\n)\nsin\nâ¡\n(\nÎ¸\nh\n,\nw\n,\nj\n)\ncos\nâ¡\n(\nÎ¸\nh\n,\nw\n,\nj\n)\n]\n,\n\\small\\mathbf{R}_{\\Theta}(x)=\\begin{bmatrix}\\cos(\\theta_{h,w,j})&-\\sin(\\theta_{h,w,j})\\\\\n\\sin(\\theta_{h,w,j})&\\cos(\\theta_{h,w,j})\\end{bmatrix},\n(4)\nwhere\nÎ¸\nh\n,\nw\n,\nj\n\\theta_{h,w,j}\nis the two-dimensional position encoding:\nÎ¸\nh\n,\nw\n,\nj\n=\nÎ¸\nj\nâ‹…\nx\nh\n+\nÎ¸\nj\nâ‹…\nx\nw\n,\nÎ¸\nj\n=\n1\n10000\n2\nâ€‹\nj\nC\n,\n\\small\\theta_{h,w,j}=\\theta_{j}\\cdot x_{h}+\\theta_{j}\\cdot x_{w},\\quad\\theta_{j}=\\frac{1}{10000^{\\frac{2j}{C}}},\n(5)\nwhere\nx\nh\nx_{h}\nand\nx\nw\nx_{w}\nrepresent the discrete horizontal and vertical coordinates of a pixel, and the frequency parameter\nÎ¸\n\\theta\ncontrols the rotation speed across channel dimensions (indexed by\nj\nâˆˆ\n[\n0\n,\nC\n/\n2\n)\nj\\in[0,C/2)\n). Using a base constant of 10000, low-frequency components emphasize global structure, while high-frequency components capture local positional details. By applying\nğ‘\nÎ˜\nâ€‹\n(\nx\n)\n\\mathbf{R}_{\\Theta}(x)\nto the query and key representations, we ensure that their inner product encodes relative positional information. After positional encoding,\nK\ns\nK_{s}\nand\nV\ns\nV_{s}\nare partitioned into\n16\nÃ—\n16\n16\\times 16\nimage patches. Sequential dependencies among these patches are modeled using a recurrent\nW\nâ€‹\nK\nâ€‹\nV\nWKV\nmechanism:\nW\nâ€‹\nK\nâ€‹\nV\ni\n=\nn\ni\n+\nexp\nâ¡\n(\nu\n)\nâŠ™\nk\ni\nâŠ™\nv\ni\nd\ni\n+\nexp\nâ¡\n(\nu\n)\nâŠ™\nk\ni\n,\nn\ni\n=\nn\ni\nâˆ’\n1\nâŠ™\nexp\nâ¡\n(\nâˆ’\nğ’²\n)\n+\nk\ni\nâŠ™\nv\ni\nâŸ\nCumulative memory of\nâ€‹\nk\ni\nâŠ™\nv\ni\n,\nd\ni\n=\nd\ni\nâˆ’\n1\nâŠ™\nexp\nâ¡\n(\nâˆ’\nğ’²\n)\n+\nk\ni\nâŸ\nCumulative memory of\nâ€‹\nk\ni\n,\n\\small\\begin{gathered}WKV_{i}=\\frac{n_{i}+\\exp(u)\\odot k_{i}\\odot v_{i}}{d_{i}+\\exp(u)\\odot k_{i}},\\\\\n\\underbrace{\\definecolor{tcbcolframe}{rgb}{1,0,0}\\definecolor{tcbcolback}{rgb}{1,1,0.75}\\definecolor{tcbcolframe}{rgb}{1,1,1}\\definecolor{tcbcolframe}{rgb}{1,1,1}\\hbox to119.09pt{\\vbox to17.85pt{\\pgfpicture\\makeatletter\\hbox{\\thinspace\\lower 0.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{{}{}{}{}\\pgfsys@beginscope\\pgfsys@invoke{ }{}{}{}{}{}{}{}{}\\definecolor[named]{pgffillcolor}{rgb}{1,1,1}\\pgfsys@color@gray@fill{1}\\pgfsys@invoke{ }\\pgfsys@fill@opacity{1.0}\\pgfsys@invoke{ }{{}{}{{}}}{{}{}{{}}}{}{}{{}{}{{}}}{{}{}{{}}}{}{}{{}{}{{}}}{{}{}{{}}}{}{}{{}{}{{}}}{{}{}{{}}}{}{}\\pgfsys@moveto{0.0pt}{4.26788pt}\\pgfsys@lineto{0.0pt}{13.57736pt}\\pgfsys@curveto{0.0pt}{15.93448pt}{1.91077pt}{17.84525pt}{4.26788pt}{17.84525pt}\\pgfsys@lineto{114.82451pt}{17.84525pt}\\pgfsys@curveto{117.18163pt}{17.84525pt}{119.09239pt}{15.93448pt}{119.09239pt}{13.57736pt}\\pgfsys@lineto{119.09239pt}{4.26788pt}\\pgfsys@curveto{119.09239pt}{1.91077pt}{117.18163pt}{0.0pt}{114.82451pt}{0.0pt}\\pgfsys@lineto{4.26788pt}{0.0pt}\\pgfsys@curveto{1.91077pt}{0.0pt}{0.0pt}{1.91077pt}{0.0pt}{4.26788pt}\\pgfsys@closepath\\pgfsys@fill\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\pgfsys@beginscope\\pgfsys@invoke{ }{}{}{}{}{}{}{}{}\\definecolor[named]{pgffillcolor}{rgb}{1,1,0.75}\\pgfsys@color@cmyk@fill{0}{0}{0.25}{0}\\pgfsys@invoke{ }\\pgfsys@fill@opacity{1.0}\\pgfsys@invoke{ }{{}{}{{}}}{{}{}{{}}}{}{}{{}{}{{}}}{{}{}{{}}}{}{}{{}{}{{}}}{{}{}{{}}}{}{}{{}{}{{}}}{{}{}{{}}}{}{}\\pgfsys@moveto{1.42262pt}{4.26788pt}\\pgfsys@lineto{1.42262pt}{13.57736pt}\\pgfsys@curveto{1.42262pt}{15.14877pt}{2.69647pt}{16.42262pt}{4.26788pt}{16.42262pt}\\pgfsys@lineto{114.82451pt}{16.42262pt}\\pgfsys@curveto{116.39592pt}{16.42262pt}{117.66977pt}{15.14877pt}{117.66977pt}{13.57736pt}\\pgfsys@lineto{117.66977pt}{4.26788pt}\\pgfsys@curveto{117.66977pt}{2.69647pt}{116.39592pt}{1.42262pt}{114.82451pt}{1.42262pt}\\pgfsys@lineto{4.26788pt}{1.42262pt}\\pgfsys@curveto{2.69647pt}{1.42262pt}{1.42262pt}{2.69647pt}{1.42262pt}{4.26788pt}\\pgfsys@closepath\\pgfsys@fill\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@fill@opacity{1.0}\\pgfsys@invoke{ }{{{}}{{}}{{}}{{}}{{}}{{}}{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{2.42262pt}{6.67262pt}\\pgfsys@invoke{ }\\hbox{{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\hbox{\\set@color{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}$\\displaystyle n_{i}=n_{i-1}\\odot\\exp(-\\mathcal{W})+k_{i}\\odot v_{i}$}}}}\\pgfsys@invoke{ }\\pgfsys@endscope}\\pgfsys@invoke{ }\\pgfsys@endscope{}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}_{\\textbf{Cumulative memory of }k_{i}\\odot v_{i}},\\underbrace{\\definecolor{tcbcolframe}{rgb}{1,0,0}\\definecolor{tcbcolback}{rgb}{1,1,0.75}\\definecolor{tcbcolframe}{rgb}{1,1,1}\\definecolor{tcbcolback}{rgb}{0.92,1,0.92}\\definecolor{tcbcolframe}{rgb}{1,1,1}\\hbox to100.14pt{\\vbox to17.85pt{\\pgfpicture\\makeatletter\\hbox{\\thinspace\\lower 0.0pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{\\the\\pgflinewidth}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{{}{}{}{}\\pgfsys@beginscope\\pgfsys@invoke{ }{}{}{}{}{}{}{}{}\\definecolor[named]{pgffillcolor}{rgb}{1,1,1}\\pgfsys@color@gray@fill{1}\\pgfsys@invoke{ }\\pgfsys@fill@opacity{1.0}\\pgfsys@invoke{ }{{}{}{{}}}{{}{}{{}}}{}{}{{}{}{{}}}{{}{}{{}}}{}{}{{}{}{{}}}{{}{}{{}}}{}{}{{}{}{{}}}{{}{}{{}}}{}{}\\pgfsys@moveto{0.0pt}{4.26788pt}\\pgfsys@lineto{0.0pt}{13.57736pt}\\pgfsys@curveto{0.0pt}{15.93448pt}{1.91077pt}{17.84525pt}{4.26788pt}{17.84525pt}\\pgfsys@lineto{95.87704pt}{17.84525pt}\\pgfsys@curveto{98.23416pt}{17.84525pt}{100.14493pt}{15.93448pt}{100.14493pt}{13.57736pt}\\pgfsys@lineto{100.14493pt}{4.26788pt}\\pgfsys@curveto{100.14493pt}{1.91077pt}{98.23416pt}{0.0pt}{95.87704pt}{0.0pt}\\pgfsys@lineto{4.26788pt}{0.0pt}\\pgfsys@curveto{1.91077pt}{0.0pt}{0.0pt}{1.91077pt}{0.0pt}{4.26788pt}\\pgfsys@closepath\\pgfsys@fill\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\pgfsys@beginscope\\pgfsys@invoke{ }{}{}{}{}{}{}{}{}\\definecolor[named]{pgffillcolor}{rgb}{0.92,1,0.92}\\pgfsys@color@rgb@fill{0.92}{1}{0.92}\\pgfsys@invoke{ }\\pgfsys@fill@opacity{1.0}\\pgfsys@invoke{ }{{}{}{{}}}{{}{}{{}}}{}{}{{}{}{{}}}{{}{}{{}}}{}{}{{}{}{{}}}{{}{}{{}}}{}{}{{}{}{{}}}{{}{}{{}}}{}{}\\pgfsys@moveto{1.42262pt}{4.26788pt}\\pgfsys@lineto{1.42262pt}{13.57736pt}\\pgfsys@curveto{1.42262pt}{15.14877pt}{2.69647pt}{16.42262pt}{4.26788pt}{16.42262pt}\\pgfsys@lineto{95.87704pt}{16.42262pt}\\pgfsys@curveto{97.44846pt}{16.42262pt}{98.7223pt}{15.14877pt}{98.7223pt}{13.57736pt}\\pgfsys@lineto{98.7223pt}{4.26788pt}\\pgfsys@curveto{98.7223pt}{2.69647pt}{97.44846pt}{1.42262pt}{95.87704pt}{1.42262pt}\\pgfsys@lineto{4.26788pt}{1.42262pt}\\pgfsys@curveto{2.69647pt}{1.42262pt}{1.42262pt}{2.69647pt}{1.42262pt}{4.26788pt}\\pgfsys@closepath\\pgfsys@fill\\pgfsys@invoke{ }\\pgfsys@invoke{ }\\pgfsys@endscope\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@fill@opacity{1.0}\\pgfsys@invoke{ }{{{}}{{}}{{}}{{}}{{}}{{}}{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{2.42262pt}{6.67262pt}\\pgfsys@invoke{ }\\hbox{{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\hbox{\\set@color{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}$\\displaystyle d_{i}=d_{i-1}\\odot\\exp(-\\mathcal{W})+k_{i}$}}}}\\pgfsys@invoke{ }\\pgfsys@endscope}\\pgfsys@invoke{ }\\pgfsys@endscope{}{}{}\\hss}\\pgfsys@discardpath\\pgfsys@invoke{ }\\pgfsys@endscope\\hss}}\\endpgfpicture}}}_{\\textbf{Cumulative memory of }k_{i}},\\end{gathered}\n(6)\nwhere\ni\nâˆˆ\n[\n0\n,\n(\nH\nÃ—\nW\n)\n/\n(\n16\nÃ—\n16\n)\n)\ni\\in[0,(H\\times W)/(16\\times 16))\nindexes the sequence of image patches, and\nexp\nâ¡\n(\nâ‹…\n)\n\\exp(\\cdot)\ndenotes the element-wise exponential function.\nk\ni\nk_{i}\nand\nv\ni\nv_{i}\ndenote the corresponding patch representations extracted from\nK\ns\nK_{s}\nand\nV\ns\nV_{s}\n. To ensure consistent accumulation, the initial states (at step\ni\n=\n0\ni=0\n) are set as\nn\n0\n=\nk\n0\nâŠ™\nv\n0\nn_{0}=k_{0}\\odot v_{0}\nand\nd\n0\n=\nk\n0\nd_{0}=k_{0}\n. The term\nu\nâˆˆ\n(\n0\n,\n1\n)\nu\\in(0,1)\nrepresents a learnable position bias, and\nğ’²\nâˆˆ\nâ„\nC\n\\mathcal{W}\\in\\mathbb{R}^{C}\nis the channel-wise time-decay vector.\nMotivation of Spatial Memory.\nThe recurrent connections in our framework build a spatial memory of observed scenes for the robot. This memory supports locating end-effector positions that are most relevant to the current task. However, recurrent computation is prone to vanishing gradients, which complicates training and often requires large amounts of data to alleviate this issue. To overcome this issue, we introduce an Adaptive Decay Mechanism that dynamically adjusts the decay rate of historical memory, preserving essential spatial information while adaptively emphasizing task-critical image patches. Here\nğ’²\n\\mathcal{W}\ndenotes the content-adaptive decay factors that govern memory retention (\nEq.\n3\n). Finally, a gating factor\nR\ns\nR_{s}\nproduces a dynamic weight to modulate how the output of the Spatial Mixing Block influences the current state:\nF\nS\nâ€‹\n0\n=\nÏƒ\nâ€‹\n(\nğ’\n1\nÃ—\n1\nâ€‹\n(\nF\n0\n)\n)\nâŠ™\nW\nâ€‹\nK\nâ€‹\nV\n\\small F_{S0}=\\sigma(\\mathcal{C}_{1\\times 1}(F_{0}))\\odot WKV\n(7)\nThen we apply the Channel Mixing Block to\nF\nS\nâ€‹\n0\nF_{S0}\nto reallocate channel-wise feature weights for feature extraction:\nF\nC\nâ€‹\n0\n=\nÏƒ\nâ€‹\n(\nğ’\n1\nÃ—\n1\nâ€‹\n(\nF\nS\nâ€‹\n0\n)\n)\nâŠ™\n(\nS\nâ€‹\nR\nâ€‹\ne\nâ€‹\nL\nâ€‹\nU\nâ€‹\n(\nğ’\n3\nÃ—\n3\nâ€‹\n(\nF\nS\nâ€‹\n0\n)\n)\n)\n\\small F_{C0}=\\sigma(\\mathcal{C}_{1\\times 1}(F_{S0}))\\odot(SReLU(\\mathcal{C}_{3\\times 3}(F_{S0})))\n(8)\nSpiking Dense Feature Extraction.\nIn the parallel branch, dense visual features are extracted using a Dense Block. These features capture contextual information and textural details of target objects. The input\nF\n0\nâˆˆ\nâ„\nH\n4\nÃ—\nW\n4\nÃ—\nC\nF_{0}\\in\\mathbb{R}^{\\frac{H}{4}\\times\\frac{W}{4}\\times C}\nis processed by the Dense Block, yielding an enriched representation\nF\nk\nâˆˆ\nâ„\nH\n4\nÃ—\nW\n4\nÃ—\n4\nâ€‹\nC\nF_{k}\\in\\mathbb{R}^{\\frac{H}{4}\\times\\frac{W}{4}\\times 4C}\n. This output then passes through a Linear layer with\nC\n/\nr\nC/r\noutput channels, producing two new feature maps\nB\nk\nB_{k}\nand\nD\nk\nD_{k}\n. The map\nB\nk\nB_{k}\nis subsequently used to generate a spatial attention mask\nX\nk\nX_{k}\n:\nX\nk\n=\nB\nk\nâ€‹\nI\nÂ¯\nk\nâ€‹\nB\nk\nT\n,\nX_{k}=B_{k}\\bar{I}_{k}B_{k}^{T},\n(9)\nwhere\nI\nÂ¯\n=\n1\nn\nâ€‹\n(\nI\nâˆ’\n1\nn\nâ€‹\nğŸ\n)\n\\bar{I}=\\frac{1}{n}\\left(I-\\frac{1}{n}\\mathbf{1}\\right)\nand\nn\n=\nh\nÃ—\nw\nn=h\\times w\n.\nI\nI\nand\nğŸ\n\\mathbf{1}\nare the\nn\nÃ—\nn\nn\\times n\nidentity matrix and matrix of all ones, respectively, and\nX\nk\nâˆˆ\nâ„\nn\nÃ—\nn\nX_{k}\\in\\mathbb{R}^{n\\times n}\n. We define\nx\n(\ni\n,\nj\n)\nâˆˆ\nX\nk\nx_{(i,j)}\\in X_{k}\n(where\n0\nâ‰¤\ni\n<\nn\n0\\leq i<n\n;\n0\nâ‰¤\nj\n<\nn\n0\\leq j<n\n), which encodes the global dependencies between the\ni\ni\n-th pixel and the\nj\nj\n-th pixel from\nB\nk\nB_{k}\n.\nX\nk\nX_{k}\nis fed into a softmax function and then multiplied by\nD\nk\nD_{k}\n:\nU\nk\n=\nsoftmax\nâ¡\n(\nX\nk\n)\nâ€‹\nD\nk\n,\nU_{k}=\\operatorname{softmax}\\left(X_{k}\\right)D_{k},\n(10)\nThen,\nU\nk\nU_{k}\nis reshaped and transposed to\nâ„\nn\nÃ—\nC\n/\nr\n\\mathbb{R}^{n\\times C/r}\n. Subsequently, it is fed to a\n1\nÃ—\n1\n1\\times 1\nconvolutional layer and added to the spiking feature\nğ’œ\nâ€‹\n(\nF\nk\n)\n\\mathcal{A}(F_{k})\nby our adaptive spike neuron:\nQ\nA\n=\nğ’\n1\nÃ—\n1\nâ€‹\n(\nU\nk\n)\n+\nğ’œ\nâ€‹\n(\nF\nk\n)\n.\nQ_{A}=\\mathcal{C}_{1\\times 1}\\left(U_{k}\\right)+\\mathcal{A}\\left(F_{k}\\right).\n(11)\nMotivation of Adaptive Spike Neuron.\nCurrent robotic manipulation models typically generate actions based solely on spatial perception, overlooking temporal information. To incorporate temporal reasoning into spatial feature understanding, we employ a spiking neuron mechanism. Conventional spiking neurons combine spatial information from previous timesteps with a decay factor to modulate the current signal, enabling robots to integrate observations from past frames when processing current visual input. However, traditional spiking neurons rely on a fixed decay factor and a binary signaling mechanism, which introduce training difficulties in spiking neural networks due to non-differentiable operations during gradient backpropagation. To address these issues, we introduce an adaptive spiking neuron. Our design replaces the static decay factor with a dynamically adjustable one at each timestep. Training focuses on learning this adaptive decay factor rather than directly optimizing the spiking neural network parameters. This approach mitigates the refractory period characteristic of spiking neurons and resolves gradient propagation challenges posed by non-differentiable components, resulting in a more stable and efficient learning process. The governing equations of our adaptive spike neuron are detailed as:\nV\nt\n=\nH\nt\nâˆ’\n1\n+\nX\nt\n,\nS\nt\n=\nÎ˜\nâ€‹\n(\nV\nt\nâˆ’\nu\nt\nâ€‹\nh\n)\n,\nH\nt\n=\nV\nt\nâ€‹\ne\nâˆ’\nÎ”\nâ€‹\nt\n/\nÏ„\nâ€‹\n(\n1\nâˆ’\nS\nt\n)\n+\nV\nr\nâ€‹\ne\nâ€‹\ns\nâ€‹\ne\nâ€‹\nt\nâ€‹\nS\nt\n,\n\\begin{gathered}V_{t}=H_{t-1}+X_{t},\\quad S_{t}=\\Theta(V_{t}-u_{th}),\\\\\nH_{t}=V_{t}e^{-\\Delta t/\\tau}(1-S_{t})+V_{reset}S_{t},\\end{gathered}\n(12)\nwhere\nÎ”\nâ€‹\nt\n\\Delta t\ndenotes the time interval (typically unit time), and\nV\nt\nV_{t}\nrepresents the membrane potential obtained through integration of spatial input\nX\nt\nX_{t}\nand temporal input\nH\nt\nâˆ’\n1\nH_{t-1}\n. Here\nX\nt\nX_{t}\nis extracted from dense visual features, and\nH\nt\nâˆ’\n1\nH_{t-1}\ncorresponds to the membrane potential from the previous timestep after undergoing reset or spike emission. If the membrane potential exceeds the threshold\nu\nt\nâ€‹\nh\nu_{th}\n, the neuron fires a spike; otherwise, it remains silent. The function\nÎ˜\nâ€‹\n(\nâ‹…\n)\n\\Theta(\\cdot)\ndenotes the Heaviside step function, defined as\nÎ˜\nâ€‹\n(\nx\n)\n=\n1\n\\Theta(x)=1\nfor\nx\nâ‰¥\n0\nx\\geq 0\nand\nÎ˜\nâ€‹\n(\nx\n)\n=\n0\n\\Theta(x)=0\notherwise, yielding a binary output\nS\nt\nâˆˆ\n{\n0\n,\n1\n}\nS_{t}\\in\\{0,1\\}\n. The term\nH\nt\nH_{t}\nindicates the temporal output, where\nÏ„\n\\tau\nrepresents the adaptive decay factor produced by our channel-wise attention mechanism. The reset potential\nV\nreset\nV_{\\text{reset}}\nis engaged after a spike is fired. In the absence of firing,\nV\nt\nV_{t}\ndecays exponentially according to the adaptive factor\nÏ„\n\\tau\n. To create an adaptive decay factor\nÏ„\n\\tau\nsuited for particular tasks, the computation of\nÏ„\n\\tau\nis seamlessly incorporated into the gradient backpropagation process of the policy:\nÏ„\n=\nğ”¼\nâ€‹\n[\nğ’\n1\nÃ—\n1\nâ€‹\n(\nR\nâ€‹\n(\nğ’\n1\nÃ—\n1\nâ€‹\n(\nF\nk\n)\n)\n)\n]\n,\n\\tau=\\mathbb{E}[\\mathcal{C}_{1\\times 1}\\left(R(\\mathcal{C}_{1\\times 1}(F_{k}))\\right)],\n(13)\nwhere\nğ”¼\nâ€‹\n[\nâ‹…\n]\n\\mathbb{E}[\\cdot]\ncomputes the average of mapped values across channels, and\nR\nâ€‹\n(\nâ‹…\n)\nR(\\cdot)\ndenotes a feature reshaping operation. This integration enables the decay factor to adjust dynamically based on the complexity of the current motion sequence. Finally, we apply a\n1\nÃ—\n1\n1\\times 1\nconvolutional layer to\nQ\nA\nQ_{A}\nto obtain query vectors\nQ\nQ\nwith matching channel dimensions to the key and value vectors\nK\nK\nand\nV\nV\n. A guided self-attention mechanism then merges the dynamically recurrent features and the spiking-based dense features as follows:\nG\n=\nsoftmax\nâ€‹\n(\nQ\nâ€‹\nK\nâŠ¤\nd\nk\n+\nB\n)\nâ€‹\nV\n,\n\\small G=\\mathrm{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_{k}}}+B\\right)V,\n(14)\nwhere\nd\nk\nd_{k}\ndenotes the dimensionality of the queries and keys, and\nB\nB\nrepresents a relative position bias. The feature map\nF\n1\nF_{1}\nis produced by down-sampling the output from two successive RASNet blocks using a\n3\nÃ—\n3\n3\\times 3\nconvolution. This procedure is repeated in Stage 2 and Stage 3. The resulting multi-scale features\nF\n1\nF_{1}\n,\nF\n2\nF_{2}\n, and\nF\n3\nF_{3}\nare subsequently fused through a set of learnable weights:\nF\nf\n=\nÎ±\n1\nâ€‹\n(\nğ’\n1\nÃ—\n1\nâ€‹\n(\nF\n1\n)\n)\n+\nÎ±\n2\nâ€‹\n(\nğ’\n1\nÃ—\n1\nâ€‹\n(\nU\nâ€‹\np\nâ€‹\n(\nF\n2\n)\n)\n)\n+\nÎ±\n3\nâ€‹\n(\nU\nâ€‹\np\nâ€‹\n(\nF\n3\n)\n)\n,\n\\small F_{f}=\\alpha_{1}(\\mathcal{C}_{1\\times 1}(F_{1}))+\\alpha_{2}(\\mathcal{C}_{1\\times 1}(Up(F_{2})))+\\alpha_{3}(Up(F_{3})),\n(15)\nwhere\nF\ni\nF_{i}\nrepresents the feature map produced at Stage\ni\ni\n(with\ni\n=\n1\n,\n2\n,\n3\ni=1,2,3\n), and\nÎ±\ni\n\\alpha_{i}\ndenotes learnable parameters that weight each level during feature fusion. The fused feature map\nF\nf\nF_{f}\nis then used to generate an initial action prediction:\na\ni\nâ€‹\nn\n=\nLinear\nâ€‹\n(\nğ’\n3\nÃ—\n3\nâ€‹\n(\nF\nf\n)\n)\n,\n\\small a_{in}=\\mathrm{Linear}\\bigl(\\mathcal{C}_{3\\times 3}(F_{f})\\bigr),\n(16)\nTo train the model, we minimize the mean-squared error between the predicted action and the ground-truth action from human demonstrations using the following loss:\nâ„’\n=\nMSE\nâ€‹\n(\na\ni\nâ€‹\nn\n,\na\nground\n)\n,\n\\mathcal{L}=\\mathrm{MSE}\\bigl(a_{in},\\;a_{\\mathrm{ground}}\\bigr),\n(17)\nwhere\nâ„’\n\\mathcal{L}\nis the loss and\na\nground\na_{\\mathrm{ground}}\nis the ground-truth action.\nInput:\nTraining epochs\nE\nE\n, conversation round\nT\nT\n, human instructions\nâ„\n\\mathcal{I}\n, human demonstrations\nğ’Ÿ\n\\mathcal{D}\nwith capacity\nM\nM\n, current observation\nğ’ª\n\\mathcal{O}\n, VLM model\nğ’²\n\\mathcal{W}\n, Stage of RGMP-S\nğ’®\n\\mathcal{S}\n, RGMP-S\nğ’¢\nm\n\\mathcal{G}_{m}\nOutput:\nRobotic actions\na\nâˆ—\na^{*}\n/*\nHuman Demonstration Collecting: */\nfor\ni\n=\n1\n,\n2\n,\nâ€¦\n,\nM\ni=1,2,...,M\ndo\nd\ni\nâ†\n(\nğ’ª\ni\n,\nğ’¥\ni\n)\nd_{i}\\leftarrow(\\mathcal{O}_{i},\\mathcal{J}_{i})\nthrough\nEq.\n21\n;\n//\nÂ§\n4.1\nğ’Ÿ\nâ†\nğ’Ÿ\nâˆª\n{\nd\ni\n}\n\\mathcal{D}\\leftarrow\\mathcal{D}\\cup\\{d_{i}\\}\nÎ˜\nâ†\nEM\nâ€‹\n(\nğ’Ÿ\n)\n\\Theta\\leftarrow\\mathrm{EM}(\\mathcal{D})\nvia\nEq.\n18\nend for\nreturn\nğ’Ÿ\n\\mathcal{D}\n,\nÎ˜\n\\Theta\n/*\nRGMP-S Training pipeline: */\nfor\ne\n=\n1\n,\n2\n,\nâ€¦\n,\nE\ne=1,2,...,E\ndo\n1ex\nF\n0\nâ†\nS\nâ€‹\nt\nâ€‹\ne\nâ€‹\nm\nâ€‹\n(\nğ’ª\ni\n)\nF_{0}\\leftarrow Stem(\\mathcal{O}_{i})\nby\nEq.\n2\n;\n//\nÂ§\n3.2\nğ’²\n,\nK\ns\n,\nV\ns\nâ†\nğ’œ\nğ’¹\nâ€‹\n(\nF\n0\n)\n{\\mathcal{W}},{K_{s}},{V_{s}}\\leftarrow\\mathcal{A_{d}}(F_{0})\n,\nâ„›\nâ€‹\n(\nF\n0\n)\n\\mathcal{R}(F_{0})\nby\nEq.\n3\n,\nEq.\n4\nF\n1\n,\nF\n2\n,\nF\n3\nâ†\nğ’®\nâ€‹\n(\nK\ns\n,\nV\ns\n,\nğ’²\n)\nÃ—\n3\n{F_{1},F_{2},F_{3}}\\leftarrow\\mathcal{S}({K_{s}},{V_{s}},{\\mathcal{W}})_{\\times 3}\na\ni\nâ€‹\nn\nâ†\nâ„³\nâ€‹\n(\nF\n1\n,\nF\n2\n,\nF\n3\n)\n{a_{in}}\\leftarrow\\mathcal{M}(F_{1},F_{2},F_{3})\nby\nEq.\n16\nâ„’\nâ†\n(\na\ni\nâ€‹\nn\n,\na\ng\nâ€‹\nr\nâ€‹\no\nâ€‹\nu\nâ€‹\nn\nâ€‹\nd\n)\n\\mathcal{L}\\leftarrow({a_{in}},{a_{ground}})\nthrough\nEq.\n17\nğ’¢\nm\nâ†\nğ’¢\nm\nâˆ’\nÎ·\nâ€‹\nâˆ‡\nâ„’\n\\mathcal{G}_{m}\\leftarrow\\mathcal{G}_{m}-\\eta\\nabla\\mathcal{L}\nend for\nreturn\nğ’¢\nm\n\\mathcal{G}_{m}\n1ex\n/*\nInference pipeline: */\nfor\nt\n=\n1\n,\n2\n,\nâ€¦\n,\nT\nt=1,2,...,T\ndo\nB\nâ€‹\no\nâ€‹\nx\nâ€‹\n(\nx\n1\n,\ny\n1\n,\nx\n2\n,\ny\n2\n)\nâ†\nğ’¬\nâ€‹\n(\nâ„\n,\nğ’ª\n|\nğ’\n)\nBox(x_{1},y_{1},x_{2},y_{2})\\leftarrow\\mathcal{Q}(\\mathcal{I},\\mathcal{O}|\\mathcal{C})\nby Eq.Â (\n1\n)\nğ’ª\ns\nâ†\n(\nx\n1\n,\ny\n1\n,\nx\n2\n,\ny\n2\n)\n,\nÏ•\na\nâ€‹\n(\nğ’ª\n,\nB\nâ€‹\no\nâ€‹\nx\n)\n\\mathcal{O}_{s}\\leftarrow(x_{1},y_{1},x_{2},y_{2}),\\kern 2.0pt\\phi_{a}(\\mathcal{O},Box)\nğ’®\ns\nâ€‹\ne\nâ€‹\nq\n,\nğ’«\nâ†\nğ’¬\nC\nâ€‹\no\nâ€‹\nT\nâ€‹\n(\nâ„\n,\nğ’ª\ns\n|\nğ’\n)\n\\mathcal{S}_{seq},\\mathcal{P}\\leftarrow\\mathcal{Q}_{CoT}(\\mathcal{I},\\mathcal{O}_{s}|\\mathcal{C})\nVoice\nâ†\n\\leftarrow\nresponse in\nğ’«\n\\mathcal{P}\nif\nğ’®\ns\nâ€‹\ne\nâ€‹\nq\nâ‰ \nâˆ…\n\\mathcal{S}_{seq}\\neq\\emptyset\nthen\nfor\ns\nk\ns_{k}\nin\nğ’®\ns\nâ€‹\ne\nâ€‹\nq\n\\mathcal{S}_{seq}\ndo\na\nâˆ—\nâ†\nğ’¢\nm\nâ€‹\n(\nğ’ª\n,\ns\nk\n,\nÎ˜\n)\na^{*}\\leftarrow\\mathcal{G}_{m}(\\mathcal{O},s_{k},\\Theta)\nthrough\nEq.\n20\nend for\nExecute\na\nâˆ—\na^{*}\nend if\nend for\nAlgorithmÂ 1\nThe RGMP-S Framework\nFigure 4:\nHardware configuration of our humanoid robot.\nThe platform integrates several core subsystems: an NVIDIA Jetson AGX Orin module for onboard computation; a head-mounted RGB camera for egocentric vision; an omnidirectional microphone array for audio input; a 6-degree-of-freedom robotic arm paired with a dexterous hand for manipulation; and a multi-axis joint module for torso mobility. These components collectively enable environmental perception, physical interaction, and autonomous movement. See\nÂ§\n4.1\nfor details.\nMotivation of Gaussian Mixture Model.\nA single Gaussian distribution\n[\n10\n]\noften regresses to the mean, which can suppress distinct action modes and compromise control precision. To better represent the multi-modal nature of robotic actions, we adopt a Gaussian Mixture Model (GMM). Let\nğ±\nâˆˆ\nâ„\nn\n\\mathbf{x}\\in\\mathbb{R}^{n}\ndenote the ground-truth joint configuration. The GMM with\nK\n=\n6\nK=6\ncomponents is defined as:\nP\nâ€‹\n(\nğ±\nâˆ£\nÎ˜\n)\n=\nâˆ‘\nk\n=\n1\nK\nÎ±\nk\nâ€‹\nğ’©\nâ€‹\n(\nğ±\nâˆ£\nÎ¼\nk\n,\nÎ£\nk\n)\n,\n\\small P(\\mathbf{x}\\mid\\Theta)=\\sum_{k=1}^{K}\\alpha_{k}\\,\\mathcal{N}\\left(\\mathbf{x}\\mid\\mu_{k},\\Sigma_{k}\\right),\n(18)\nwhere\nÎ±\nk\n\\alpha_{k}\n,\nÎ¼\nk\n\\mu_{k}\n, and\nÎ£\nk\n\\Sigma_{k}\ndenote the prior weight, mean, and covariance of the\nk\nk\n-th component. These parameters are estimated via the expectation-maximization algorithm to maximize the data likelihood. To refine the initial prediction\na\nin\na_{\\text{in}}\n, we compute the Mahalanobis distance to each GMM component. As the intrinsic metric for Gaussian distributions, it utilizes covariance to normalize deviations, ensuring statistical consistency by penalizing errors in low-variance dimensions:\nl\nk\n=\n(\na\nin\nâˆ’\nÎ¼\nk\nÏ‰\n)\nâŠ¤\nâ€‹\n(\nÎ£\nk\nÏ‰\n,\nÏ‰\n)\nâˆ’\n1\nâ€‹\n(\na\nin\nâˆ’\nÎ¼\nk\nÏ‰\n)\n.\n\\small l_{k}=\\sqrt{\\left(a_{\\text{in}}-\\mu_{k}^{\\omega}\\right)^{\\top}\\left(\\Sigma_{k}^{\\omega,\\omega}\\right)^{-1}\\left(a_{\\text{in}}-\\mu_{k}^{\\omega}\\right)}.\n(19)\nThe final action\na\nâˆ—\na^{*}\ncorresponds to the mean vector of the component with the smallest distance:\na\nâˆ—\n=\narg\nâ¡\nmin\nÎ¼\nk\nÏ‰\nâ¡\nl\nk\n.\n\\small a^{*}=\\arg\\min_{\\mu_{k}^{\\omega}}\\,l_{k}.\n(20)\nThe pseudocode of RGMP-S is presented in Algorithm\n1\n.\nFigure 5:\nExperimental validation within the interactive bar service scenario.\nEvaluation of the RGMP-S framework regarding the delivery of beverages and tissues. The training phase involves 40 expert trajectories per object category. Quantitative results demonstrate that RGMP-S attains superior success rates and robustness relative to the Diffusion Policy (DP) baseline. See\nÂ§\n4.2\nfor details.\n4\nExperiments\nThis section presents a systematic experimental evaluation of the proposed RGMP-S framework. Our experiments aim to validate the effectiveness and generalization capacity of the overall architecture and its two core modules: the LGSS and the RASNet. We analyze the distinct contribution of each module and compare the full framework against several state-of-the-art baselines on a range of manipulation tasks. The following sections detail the experimental setup, implementation specifics, evaluation metrics, and present a comprehensive discussion of the results.\n4.1\nExperimental Setup\nHardware Setup\n.\n\nOur experiments are conducted on three distinct robotic platforms. The primary platform is an upper-body humanoid robot with four integrated subsystems: vision, audio, decision-making, and actuation. A head-mounted egocentric RGB camera provides primary visual input. An iFLYTEK S0Y22F omnidirectional microphone and speaker array, mounted on the torso, handles voice interaction. An onboard NVIDIA Orin compute module performs high-level behavioral planning. The actuation system consists of two 6-degree-of-freedom (6-DOF) robotic arms, each equipped with a 6-DOF dexterous hand. The entire system (as shown in\nFig.\n4\n) is self-powered and communicates via a local wireless network, enabling fully untethered operation. To assess cross-embodiment generalization, we also employ a desktop dual-arm manipulation robot. This secondary platform is equipped with an overhead RGB camera and two 6-DOF robotic arms dedicated to executing precise physical manipulation tasks. The third platform comprises the Aloha robot, which incorporates a centrally positioned RGB camera and dual robotic arms equipped with wrist-mounted RGB sensors.\nDataset and Evaluation Criteria\n.\n\nTo verify the efficacy of the proposed RGMP-S framework, we curated a comprehensive dataset comprising 500 expert trajectories within the skill library. Each trajectory constitutes an operational path paired with a visual observation captured immediately before the initiation of a robotic task. In particular, the visual input facilitates the mapping to a specific action, while the trajectory delineates the transition of the manipulator from the starting configuration to the target spatial coordinates and end-effector pose. This data collection process is formulated in the following manner.\nd\ni\n=\n(\nğ’¥\n,\nğ’ª\n)\n,\nd_{i}=(\\mathcal{J,O}),\n(21)\nwhere\nğ’¥\n\\mathcal{J}\nsignifies the joint space of the robotic system and\nğ’ª\n\\mathcal{O}\nrepresents the corresponding perceptual observation. During physical experiments, the system performance is quantified via two complementary metrics. The skill selection accuracy\nA\nâ€‹\nc\nâ€‹\nc\ns\nAcc_{s}\nrepresents the capability of the model to identify the appropriate primitive for a given instruction. Furthermore, the execution success rate\nA\nâ€‹\nc\nâ€‹\nc\nt\nAcc_{t}\nevaluates the precision with which the manipulator interacts with the target object. Consequently, the comprehensive success rate\nA\nâ€‹\nc\nâ€‹\nc\nAcc\nis defined as the product of these two components.\nA\nâ€‹\nc\nâ€‹\nc\n=\nA\nâ€‹\nc\nâ€‹\nc\ns\nÃ—\nA\nâ€‹\nc\nâ€‹\nc\nt\nAcc=Acc_{s}\\times Acc_{t}\n(22)\nReal-world Task Configurations.\nTo verify the performance of the RGMP-S, we conduct a comprehensive comparative evaluation against several state-of-the-art baselines across 10 manipulation tasks. This evaluation suite comprises five simulation tasks from the ManiSkill2 benchmark and five physical experiments. The specific configurations of the real-world tasks are outlined in the following descriptions.\nInteractive Bar Service\n.\nIn this scenario, the humanoid robot engages in verbal dialogue with a human and provides assistance based on vocal instructions. These tasks include the delivery of beverages or tissues (as shown in\nFig.\n5\n). During evaluation, the skill selection accuracy\nA\nâ€‹\nc\nâ€‹\nc\ns\nAcc_{s}\nis recorded as 0 if the robot selects an inappropriate skill. Typical failures primarily involve unstable side grasps on deformed objects and collisions with peripheral obstacles during cluttered manipulation. Conversely, successful skill identification is denoted as\nA\nâ€‹\nc\nâ€‹\nc\ns\n=\n1\nAcc_{s}=1\n. Furthermore, the execution success rate\nA\nâ€‹\nc\nâ€‹\nc\nt\nAcc_{t}\nis recorded as 1 only upon the successful grasping of the target object. For each behavioral primitive, the model is trained using 40 expert trajectories and subsequently validated across 20 randomly positioned objects, yielding the mean success rate as the final accuracy.\nFigure 6:\nGeneralized manipulation capabilities of the RGMP-S.\nWe evaluate the RGMP-S regarding its ability to grasp various novel objects at randomized positions. The training phase relies on a dataset containing only 40 expert demonstrations focused on grasping a Fanta can. Experimental results indicate that RGMP-S achieves proficient success rates when grasping the Fanta can across diverse spatial configurations. Furthermore, the system demonstrates substantial zero-shot transfer to previously unencountered entities such as a Coke can, a spray bottle, and a human hand. This outcome highlights the inherent versatility and adaptability of the architecture across diverse manipulation scenarios. See\nÂ§\n4.2\n.\nZero-shot Grasping\n.\nTo assess the generalization capability of the RGMP-S framework within previously unseen environments (as shown in\nFig.\n6\n), we evaluate the performance of the action generation ability. The training phase utilizes a constrained dataset comprising 40 expert trajectories solely for grasping a Fanta can. During the inference stage, the zero-shot transfer capability is validated through grasping trials performed at randomized positions on novel objects with distinct geometries including a Coke can, a spray bottle, and a human hand. In addition to testing unseen categories, we examine spatial generalization by deploying the originally seen Fanta can in various randomized positions across the workspace. Each evaluation trial is quantified via a binary success metric, where a stable retrieval of the target is recorded as 1 and a failure as 0. For each object category, we conduct 100 independent trials at randomized locations to determine the final success rate.\nTowel Folding\n. The towel folding task involves the manipulation of deformable objects through a sequence of maneuvers intended to achieve a predefined configuration. This environment involves high degrees of freedom and stochastic physical properties including randomized initial poses and various fabric textures. Success is defined as aligning the fabric edges within a specified spatial tolerance (\nÂ±\n\\pm\n2 cm) and maintaining a stable folded geometry. The training corpus for this task comprises 100 expert trajectories that encompass diverse towel dimensions and colors. For evaluation, we employ a validation scheme involving randomized placements and novel towel instances to assess the generalization performance of the proposed framework.\nPouring Water\n. The primary objective of this task is the coordination of a source container to facilitate the successful transfer of liquid into a target receptacle. The environment involves complex fluid dynamics and stochastic physical properties including randomized initial positions and various container geometries. Success is defined as the spill-free transfer of water into the target receptacle. The training corpus for this task comprises 100 expert trajectories that encompass different initial fill levels and container types. For evaluation, we employ a validation scheme involving novel receptacles and randomized spatial configurations to assess the robust generalization capability.\nBin Picking\n. Within the bin picking scenario, the robot is tasked with retrieving objects from a storage bin and relocating them to a designated target zone under cluttered conditions. The environment involves complex occlusions and stochastic physical arrangements including randomized orientations and various object geometries within a confined space. Success is defined as the accurate retrieval of the target object and its stable placement within the goal area without unintended collisions. The training corpus for this task comprises 100 expert trajectories per object category to capture diverse grasping configurations. For evaluation, we employ a validation scheme involving novel object instances and randomized clutter patterns to assess the generalization robustness of the proposed framework (as shown in\nFig.\n7\n).\nFigure 7:\nValidation on long-horizon tasks.\nEvaluation of the RGMP-S framework regarding towel folding, water pouring, and bin picking. See\nÂ§\n4.2\n.\nFigure 8:\nPerformance within the ManiSkill2 benchmark.\nQuantitative evaluations on 5 representative manipulation tasks to compare the proposed framework with state-of-the-art baselines. Refer to\nÂ§\n4.2\n.\nSimulation Benchmark Specifications.\nTo evaluate the proposed framework, we select five representative tasks from the publicly available and standardized ManiSkill2 simulation benchmark (as shown in\nFig.\n8\n). These tasks demand high-precision interaction and robust spatial reasoning. The detailed specifications for each task are described as follows.\nOpenCabinetDrawer / CabinetDoor\n: These articulated object manipulation tasks require a single-arm mobile manipulator to actuate target components on a cabinet. These environments involve stochastic physical properties, including randomized friction and damping parameters for the joints. Success is defined as opening the target component to at least 90% of the maximum range and maintaining a static state. The training corpus for these tasks comprises 300 expert trajectories per target, spanning 25 and 42 unique cabinets for drawers and doors, respectively. For evaluation, we employ a validation scheme involving ten unseen cabinets to assess the generalization capability of the model.\nPushChair\n&\nMoveBucket\n: These tasks evaluate the capacity for precise bimanual coordination. In\nPushChair\n, a dual-arm robot must translate a swivel chair to a target location without causing the object to tip over. Success requires the chair to remain static within 15 cm of the destination. The\nMoveBucket\ntask involves a complex manipulation sequence where the robot must lift a bucket containing a ball onto a raised platform without losing the internal object. Both tasks leverage a dataset of 300 trajectories per object, featuring 26 chairs and 29 buckets for training.\nPlugCharger\n: This insertion task necessitates high-precision control of the end-effector. The manipulator must insert a charger into a wall receptacle. This task provides 1,000 successful trajectories for training. The validation scheme consists of two stages of 100 episodes, each featuring randomized initial joint configurations of the robot and varied initial poses of the charger relative to the wall.\nCounterparts.\nWe compare the proposed RGMP-S framework against several representative baselines to evaluate performance across diverse manipulation tasks.\nâ€¢\nResNet\n[CVPRâ€™16]\n[\n20\n]\n: Robust visual feature extraction via deep residual learning for manipulation tasks.\nâ€¢\nTransformer\n[NeurIPSâ€™17]\n[\n54\n]\n: Global dependency modeling in trajectory sequences through self-attention.\nâ€¢\nDiffusion Policy\n[RSSâ€™23]\n[\n10\n]\n: Robotic action distribution representation via an iterative denoising process.\nâ€¢\nManiSkill2 1st\n[RSSâ€™23]\n[\n17\n]\n: A finetuning mechanism that achieved 1st place in the ManiSkill2 challenge.\nâ€¢\nOcto\n[RSSâ€™24]\n[\n52\n]\n: An open-source generalist policy trained on massive datasets for robotic control.\nâ€¢\nOpenVLA\n[CoRLâ€™24]\n[\n28\n]\n: Integration of VLMs with action tokens for open-world manipulation.\nâ€¢\nRDT-1B\n[arXivâ€™24]\n[\n34\n]\n: Performance scaling in multiple robotic tasks via a diffusion transformer architecture.\nâ€¢\nDex-VLA\n[CoRLâ€™25]\n[\n55\n]\n: Alignment of vision and language representations for dexterous manipulation.\n4.2\nPerformance Comparison with SOTA Methods\nComprehensive evaluations are conducted against state of the art baselines across ten manipulation tasks, encompassing five physical scenarios and five simulation benchmarks.\nHuman-robot Interaction Analysis\n.\nTable\nII\ndetails the quantitative outcomes of the interactive bar service task. The tabulated data reveals that the absence of geometric guidance compromises the skill selection accuracy\nA\nâ€‹\nc\nâ€‹\nc\ns\nAcc_{s}\nof the standard Qwen-vl baseline, particularly when paired with the ResNet50 backbone. This limitation highlights the necessity of geometric inductive biases for precise decision-making. In contrast, the integration of our LGSS framework consistently outperforms the Qwen-vl counterpart across all diverse backbones. For example, within the Diffusion Policy configuration, LGSS outperforms the baseline by 15% on the squashed Coke task, which is calculated through the enhancement of overall accuracy from 0.40 to 0.55. This improvement primarily stems from the accurate identification of object orientations, which effectively mitigates unstable side grasps on deformed objects.\nFigure 9:\nQuantitative analysis of long-horizon tasks.\nThe proposed framework is compared against state of the art methods for towel folding, water pouring, and bin picking. See\nÂ§\n4.2\nfor details.\nGeneralized Performance Analysis\n.\nWe conduct extensive comparative experiments in Table\nIII\nwith representative state-of-the-art models on the zero-shot grasping scenario, and our RGMP-S achieves consistently superior performance. As depicted in the comparison, standard data-driven methods such as Octo and OpenVLA struggle to effectively generalize to novel categories like spray bottles and human hands due to the lack of explicit geometric reasoning. RGMP-S consistently outperforms all other counterparts on different targets. Specifically, RGMP-S outperforms the best baseline Dex-VLA by 12% in terms of average success rate. Notably, in the challenging human hand handover task, our method attains a success rate of 0.93 compared to 0.84 for the second-best method, demonstrating the remarkably robust zero-shot transfer capability of the framework even when trained on a highly constrained dataset. Moreover, regarding computational efficiency, RGMP-S achieves a high inference frequency of 75.2 Hz, significantly surpassing the computationally intensive models (e.g., 3.6 Hz for OpenVLA and 1.01 Hz for Diffusion Policy), thereby ensuring the low latency required for real-time robot control.\nTABLE II:\nAblation study regarding the LGSS framework and Qwen-vl.\nExperimental validations are conducted within environments featuring Fanta cans, Sprite cans, tissue paper, squashed Coke cans, and human hands with randomized object configurations for each trial. The training phase for each skill category involves 40 expert trajectories, while quantitative results are derived from 20 randomized validation trials. Comparative experiments are performed against representative baselines, including ResNet\n[\n20\n]\n, Transformer\n[\n54\n]\n, ManiSkill2-1st\n[\n17\n]\n, and Diffusion Policy\n[\n10\n]\n. The superior performance within each category is emphasized in\nbold\nand the second-best result is denoted with an\nunderline\n. See\nÂ§\n4.3\n.\nFanta\nSprite\nTissue\nSquashed Coke\nHuman Hand\nMethods\nA\nâ€‹\nc\nâ€‹\nc\ns\nAcc_{s}\nA\nâ€‹\nc\nâ€‹\nc\nt\nAcc_{t}\nA\nâ€‹\nc\nâ€‹\nc\nâ†‘\nAcc\\uparrow\nA\nâ€‹\nc\nâ€‹\nc\ns\nAcc_{s}\nA\nâ€‹\nc\nâ€‹\nc\nt\nAcc_{t}\nA\nâ€‹\nc\nâ€‹\nc\nâ†‘\nAcc\\uparrow\nA\nâ€‹\nc\nâ€‹\nc\ns\nAcc_{s}\nA\nâ€‹\nc\nâ€‹\nc\nt\nAcc_{t}\nA\nâ€‹\nc\nâ€‹\nc\nâ†‘\nAcc\\uparrow\nA\nâ€‹\nc\nâ€‹\nc\ns\nAcc_{s}\nA\nâ€‹\nc\nâ€‹\nc\nt\nAcc_{t}\nA\nâ€‹\nc\nâ€‹\nc\nâ†‘\nAcc\\uparrow\nA\nâ€‹\nc\nâ€‹\nc\ns\nAcc_{s}\nA\nâ€‹\nc\nâ€‹\nc\nt\nAcc_{t}\nA\nâ€‹\nc\nâ€‹\nc\nâ†‘\nAcc\\uparrow\nwith\nResNet50\nQwen-vl\n[\n4\n]\n0.65\n0.54\n0.35\n0.60\n0.42\n0.25\n0.65\n0.46\n0.30\n0.65\n0.46\n0.30\n0.70\n0.57\n0.40\nOur LGSS\n0.85\n0.53\n0.45\n0.75\n0.46\n0.35\n0.85\n0.47\n0.40\n0.85\n0.47\n0.40\n0.85\n0.56\n0.48\nwith\nTransformer\nQwen-vl\n[\n4\n]\n0.60\n0.58\n0.35\n0.65\n0.54\n0.30\n0.70\n0.50\n0.35\n0.60\n0.58\n0.35\n0.65\n0.62\n0.40\nOur LGSS\n0.80\n0.56\n0.45\n0.75\n0.53\n0.40\n0.85\n0.53\n0.45\n0.85\n0.53\n0.45\n0.85\n0.64\n0.54\nwith\nManiSkill2-1st\nQwen-vl\n[\n4\n]\n0.70\n0.57\n0.40\n0.65\n0.69\n0.45\n0.65\n0.53\n0.34\n0.65\n0.54\n0.35\n0.65\n0.62\n0.40\nOur LGSS\n0.85\n0.53\n0.45\n0.80\n0.68\n0.54\n0.85\n0.53\n0.45\n0.80\n0.56\n0.45\n0.85\n0.70\n0.60\nwith\nDiffusion Policy\nQwen-vl\n[\n4\n]\n0.65\n0.76\n0.49\n0.65\n0.75\n0.50\n0.65\n0.68\n0.44\n0.65\n0.62\n0.40\n0.70\n0.71\n0.50\nOur LGSS\n0.85\n0.76\n0.65\n0.80\n0.77\n0.62\n0.85\n0.69\n0.59\n0.85\n0.65\n0.55\n0.90\n0.83\n0.74\nTABLE III:\nGeneralized performance and efficiency comparison.\nModels are trained on 40 Fanta can trajectories. Evaluation across Fanta, Coke, spray, and hand targets demonstrates the robust zero-shot capability and real-time inference speed (\nf\nf\n) of RGMP-S. See details in\nÂ§\n4.2\n.\nMethods\nFanta\nâ†‘\n\\uparrow\nCoke\nâ†‘\n\\uparrow\nSpray\nâ†‘\n\\uparrow\nHand\nâ†‘\n\\uparrow\nAvg\nâ†‘\n\\uparrow\nf\nf\n(Hz)\nâ†‘\n\\uparrow\nManiSkill2-1st\n[\n17\n]\n0.70\n0.60\n0.63\n0.62\n0.64\n15.2\nOcto\n[\n52\n]\n0.65\n0.55\n0.58\n0.62\n0.60\n13.4\nOpenVLA\n[\n28\n]\n0.68\n0.58\n0.61\n0.60\n0.62\n3.6\nRDT-1b\n[\n34\n]\n0.70\n0.61\n0.60\n0.62\n0.64\n6.7\nDP\n[\n10\n]\n0.75\n0.65\n0.68\n0.72\n0.70\n1.01\nDex-VLA\n[\n55\n]\n0.87\n0.66\n0.71\n0.84\n0.77\n55.3\nOur RGMP\n[\n1\n]\n0.98\n0.78\n0.81\n0.90\n0.87\n78.6\nOur RGMP-S\n0.96\n0.83\n0.85\n0.93\n0.89\n75.2\nTABLE IV:\nAblation study of RASNet and GMM.\nQuantitative evaluations on tissue passing and squashed Coke manipulation tasks validate the effectiveness of the proposed components. See details in\nÂ§\n4.3\n.\nTissue\nSquashed Coke\nMethods\nGMM\nA\nâ€‹\nc\nâ€‹\nc\ns\nâ†‘\nAcc_{s}\\uparrow\nA\nâ€‹\nc\nâ€‹\nc\nt\nâ†‘\nAcc_{t}\\uparrow\nA\nâ€‹\nc\nâ€‹\nc\nâ†‘\nAcc\\uparrow\nA\nâ€‹\nc\nâ€‹\nc\ns\nâ†‘\nAcc_{s}\\uparrow\nA\nâ€‹\nc\nâ€‹\nc\nt\nâ†‘\nAcc_{t}\\uparrow\nA\nâ€‹\nc\nâ€‹\nc\nâ†‘\nAcc\\uparrow\nâ€“\n0.85\n0.58\n0.50\n0.80\n0.61\n0.49\nDP\n[\n10\n]\nâœ“\n\\checkmark\n0.80\n0.68\n0.56\n0.85\n0.65\n0.55\nâ€“\n0.80\n0.69\n0.55\n0.85\n0.71\n0.60\nOur RASNet\nâœ“\n\\checkmark\n0.85\n0.71\n0.60\n0.90\n0.77\n0.69\nLong-horizon Manipulation Analysis\n.\nWe summarize the comparative results of long-horizon manipulation tasks in Figure\n9\n. The figure clearly depicts that under complex physical constraints, standard vision-language models like Octo and OpenVLA present significantly suboptimal performance in these three tasks. This demonstrates the difficulty of handling complex stochastic physical properties without explicit geometric guidance. Our RGMP-S framework consistently outperforms all other counterparts on different scenarios. For example, in the Pouring Water scenario, RGMP-S outperforms the best baseline Diffusion Policy by approximately 16%. This metric is calculated through the successful transfer of liquid into target receptacles with randomized spatial configurations and various container geometries. We further conduct experiments within the towel folding scenario involving highly deformable objects, and our method achieves a superior success rate of 0.86 compared to 0.68 for Dex-VLA. This validates the robust generalization capability of the proposed framework in aligning fabric edges and maintaining stable folded geometry within high-degree-of-freedom environments. Similarly, in the Bin Picking task characterized by complex occlusions, RGMP-S attains the highest success rate of 0.83 and effectively avoids unintended collisions. Relative to the preliminary RGMP conference version\n[\n1\n]\n, RGMP-S incorporates dense spiking features to enrich spatiotemporal representations. This mechanism significantly enhances the robustness of action execution across the temporal dimension for long-horizon tasks and consequently yields consistently superior performance over the previous version RGMP.\nTABLE V:\nComponent contribution analysis.\nQuantitative evaluations across grasping generalization tasks validate the efficacy of RoPE, Spatial Mixing Block (SMB), Channel Mixing Block (CMB), and Guided Self-attention Module (GSM). Please see details in\nÂ§\n4.3\n.\nRoPE\nSMB\nCMB\nGSM\nFanta\nâ†‘\n\\uparrow\nCoke\nâ†‘\n\\uparrow\nSpray\nâ†‘\n\\uparrow\nHand\nâ†‘\n\\uparrow\nâ€“\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n0.88\n0.67\n0.73\n0.79\nâœ“\n\\checkmark\nâ€“\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n0.85\n0.76\n0.76\n0.84\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâ€“\nâœ“\n\\checkmark\n0.92\n0.63\n0.66\n0.75\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâ€“\n0.86\n0.61\n0.62\n0.73\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n0.96\n0.83\n0.85\n0.93\nFigure 10:\nAblation study and data efficiency analysis.\nThe left panel validates the Spiking Dense Feature Extraction (SDFE) module across long-horizon tasks. The right panel illustrates that RGMP-S achieves comparable performance to the Diffusion Policy baseline with\n5\nÃ—\n5\\times\nfewer training trajectories on grasping Fanta task. See details in\nÂ§\n4.3\n.\nTABLE VI:\nQuantitative comparison on the ManiSkill2 benchmark.\nComparison of average success rates (in %, mean\nÂ±\n\\pm\nstandard deviation) of our RGMP-S against state-of-the-art baselines across five diverse manipulation tasks. All results are averaged over multiple random seeds. See\nÂ§\n4.2\n.\nMethods\nPush Chair\nâ†‘\n\\uparrow\nMove\nBucket\nâ†‘\n\\uparrow\nPlug\nCharger\nâ†‘\n\\uparrow\nCabinet Door\nâ†‘\n\\uparrow\nCabinet Drawer\nâ†‘\n\\uparrow\nAverage Score\nâ†‘\n\\uparrow\nRDT-1b\n[\n34\n]\n3.97\nÂ±\n\\pm\n1.35\n1.46\nÂ±\n\\pm\n0.34\n1.75\nÂ±\n\\pm\n1.21\n13.44\nÂ±\n\\pm\n3.86\n11.62\nÂ±\n\\pm\n3.12\n6.44\nÂ±\n\\pm\n1.98\nDex-VLA\n[\n55\n]\n2.36\nÂ±\n\\pm\n1.33\n3.47\nÂ±\n\\pm\n1.36\n1.62\nÂ±\n\\pm\n1.35\n18.36\nÂ±\n\\pm\n2.93\n8.49\nÂ±\n\\pm\n2.48\n6.86\nÂ±\n\\pm\n1.89\nManiSkill2-1st\n[\n17\n]\n8.63\nÂ±\n\\pm\n2.36\n4.23\nÂ±\n\\pm\n1.39\n3.41\nÂ±\n\\pm\n1.42\n24.68\nÂ±\n\\pm\n3.22\n12.48\nÂ±\n\\pm\n3.28\n10.69\nÂ±\n\\pm\n2.33\nOcto\n[\n52\n]\n5.35\nÂ±\n\\pm\n1.24\n2.46\nÂ±\n\\pm\n1.26\n3.29\nÂ±\n\\pm\n1.48\n20.37\nÂ±\n\\pm\n3.98\n10.35\nÂ±\n\\pm\n1.48\n8.36\nÂ±\n\\pm\n1.89\nOpenVLA\n[\n28\n]\n6.33\nÂ±\n\\pm\n1.48\n4.49\nÂ±\n\\pm\n1.63\n4.28\nÂ±\n\\pm\n3.21\n24.29\nÂ±\n\\pm\n4.25\n15.47\nÂ±\n\\pm\n2.33\n10.97\nÂ±\n\\pm\n2.58\nDiffusion Policy\n[\n10\n]\n4.39\nÂ±\n\\pm\n1.22\n6.48\nÂ±\n\\pm\n2.39\n6.44\nÂ±\n\\pm\n2.36\n15.32\nÂ±\n\\pm\n1.48\n17.38\nÂ±\n\\pm\n2.68\n10.01\nÂ±\n\\pm\n2.03\nOur RGMP\n[\n1\n]\n14.32\nÂ±\n\\pm\n2.32\n8.46\nÂ±\n\\pm\n1.43\n7.44\nÂ±\n\\pm\n1.28\n26.48\nÂ±\n\\pm\n4.35\n20.46\nÂ±\n\\pm\n3.45\n15.43\nÂ±\n\\pm\n2.57\nOur RGMP-S\n16.47\nÂ±\n\\pm\n2.34\n9.64\nÂ±\n\\pm\n1.46\n8.45\nÂ±\n\\pm\n2.23\n26.48\nÂ±\n\\pm\n4.26\n20.16\nÂ±\n\\pm\n4.18\n16.24\nÂ±\n\\pm\n2.89\nFigure 11:\nLearning curves on ManiSkill2 benchmark.\nThe X-axis represents training steps, and the Y-axis denotes the success rate. Shaded areas correspond to the standard deviation. Our RGMP-S variants achieve stable convergence and superior performance across all tasks. Refer to\nÂ§\n4.2\n.\nFigure 12:\nFailure cases.\nOur method and current state-of-the-art models struggle in scenarios requiring fine-grained manipulation. Refer to\nÂ§\n4.3\n.\nSimulation Benchmark Analysis\n. We present a comprehensive evaluation on the ManiSkill2 benchmark, summarized in\nTab.\nVI\nand visualized via the radar chart in\nFig.\n8\nand learning curves in\nFig.\n11\n. As observed, Transformer-based and Diffusion Policy methods exhibit suboptimal performance when subjected to stochastic physical constraints. This degradation highlights the inherent challenge of adapting to randomized friction parameters without explicit geometric guidance. In contrast, our RGMP-S demonstrates superior robustness, consistently outperforming state-of-the-art baselines across all evaluated scenarios. Specifically, in the\nOpen Cabinet Door\ntask involving articulated objects, RGMP-S achieves a success rate of 26.48%, surpassing the ManiSkill2-1st baseline (24.68%) and Diffusion Policy (15.32%). This performance advantage is attributed to our methodâ€™s ability to handle stochastic dynamics while actuating target components to at least 90% of their maximum range. Furthermore, in the bimanual\nPushChair\ntask, RGMP-S achieves a score of 16.47%, nearly doubling the performance of the strongest external baseline (ManiSkill2-1st at 8.63%). This significant margin validates the efficacy of our approach in preventing object tipping and ensuring precise static positioning within 15 cm of the target destination.\n4.3\nDiagnostic Experiments\nAblation study\n. To rigorously evaluate the individual contributions of the proposed components within our RGMP-S framework, we conducted a series of ablation studies across diverse manipulation tasks. First, we investigated the efficacy of the Spiking Dense Feature Extraction (SDFE) module in long-horizon tasks. As illustrated in the left panel of Fig.\n10\n, removing the SDFE module leads to a significant performance degradation across all tested tasks (Fold, Pour, Bin-pick). For instance, in the â€Pourâ€ task, the success rate drops from 0.92 to approximately 0.65 without SDFE. This confirms that the SDFE module is crucial for capturing temporal dependencies and fine-grained spatial features necessary for complex, multi-stage manipulation. Second, we analyzed the impact of the RASNet module and the GMM component. Table\nIV\npresents quantitative results on tissue passing and squashed Coke manipulation tasks. The baseline Diffusion Policy achieves a combined accuracy (\nA\nâ€‹\nc\nâ€‹\nc\nAcc\n) of 0.50 and 0.49 on the Tissue and Squashed Coke tasks, respectively. Incorporating GMM into the baseline improves these scores to 0.56 and 0.55. However, our proposed RASNet method, even without GMM, outperforms the baseline, achieving accuracies of 0.55 and 0.60. The combination of RASNet and GMM yields the highest performance, reaching an accuracy of 0.60 on Tissue and 0.69 on Squashed Coke. This demonstrates that RASNet effectively enhances feature representation, and its synergy with GMM further enhances the robustness of the policy against object deformability and state uncertainty. Third, we performed a component contribution analysis to validate the efficacy of Rotary Positional Embeddings (RoPE), the Spatial Mixing Block (SMB), the Channel Mixing Block (CMB), and the Guided Self-attention Module (GSM) in grasping generalization tasks. As detailed in Table\nV\n, the full model achieves the highest success rates across all objects (Fanta: 0.96, Coke: 0.83, Spray: 0.85, Hand: 0.93). Removing any single component results in a noticeable performance drop. For example, omitting the GSM reduces the success rate on the â€Handâ€ object from 0.93 to 0.73, highlighting its importance in guiding attention to relevant geometric features. Similarly, the absence of SMB leads to a significant decrease in performance on the â€Fantaâ€ object (0.96 to 0.85), underscoring the necessity of spatial mixing for robust grasping.\nData Efficiency Analysis.\nWe further evaluated the data efficiency of RGMP-S compared to the Diffusion Policy on the Fanta grasping task. The right panel of Fig.\n10\nillustrates the dependency of the success rate on the quantity of training trajectories. Our method demonstrates superior data efficiency, achieving a high success rate with significantly fewer demonstrations. Specifically, RGMP-S reaches a performance level comparable to the converged Diffusion Policy (trained on\nâˆ¼\n\\sim\n200 trajectories) using only\nâˆ¼\n\\sim\n40 trajectories. This 5\nÃ—\n\\times\nimprovement in data efficiency is attributed to the inductive biases introduced by our RASNet, which allow for effective learning from sparse data.\nFailure Case Analysis and Limitations.\nTo ensure a comprehensive and rigorous evaluation, we investigate the boundary conditions where the proposed RGMP-S and other state-of-the-art baselines encounter difficulties. As visualized in\nFig.\n12\n, performance degradation is observed across all evaluated methods in scenarios demanding extreme perceptual precision. Specifically, in the\nFold Towel\ntask, when the deformable fabric lies completely flush against the table surface, the available vertical clearance for gripper insertion diminishes to the millimeter level. This insufficient manipulation margin allows for negligible error tolerance, frequently causing unintended collisions with the tabletop or failed grasping attempts due to precise z-axis control limits. Similarly, in the\nPlug Charger\ntask, the success rate drops significantly when the insertion alignment tolerance is stricter than 5 mm. Under such high-precision constraints, self-occlusion of the robotic gripper hinders the fine-grained visual feedback required for exact mating. It is worth noting that these failure modes reflect the intrinsic complexity of the physical interaction rather than specific algorithmic flaws. Empirical observations indicate that these scenarios are non-trivial even for human operators, where the one-shot success rate via teleoperation is also limited. These findings highlight the open challenges in handling high-fidelity contact dynamics and suggest future directions for integrating tactile feedback or active vision.\n5\nConclusion\nIn this paper, we present RGMP-S, a novel and robust framework for generalizable robotic manipulation. RGMP-S exploits both geometric-prior knowledge and spatiotemporal feature learning by constructing the Long-horizon Geometric-prior Skill Selector (LGSS) and the Recursive Adaptive Spiking Network (RASNet), effectively overcoming the semantic-geometric gap and acquiring generalizable dexterity in dynamic environments. Furthermore, for alleviating the data scarcity problem, we formulate an Adaptive Spike Neuron to modulate feature retention and integrate a Gaussian Mixture Model to refine action generation, enabling the policy to learn from limited demonstrations while preserving precision. Moreover, we maintain the supervision of geometric consistency to provide strong spatial constraints, boosting both long-horizon planning and fine-grained interaction performance. We experimentally demonstrate that RGMP-S performs favorably against many existing state-of-the-art methods across ten diverse manipulation tasks, including physical interactive scenarios and standard simulation benchmarks. We hope this work paves for future research on efficient and generalizable robotic learning as it is fully reproducible and includes:\nâ€¢\nRGMP-S, a strong baseline that outperforms existing approaches while maintaining\nhigh data efficiency\nand\nrobust generalization\nacross unseen scenarios.\nâ€¢\nA comprehensive benchmark comparison of the state-of-the-art on\nmultiple real-world\nand simulation tasks.\nReferences\n[1]\nX. ,Li, W. ,Huang, P. Nengyuan, Z. Kaiyan, Y. Songhua, W. Yiming, L. Mengde, Y. Mang, X. Jifeng, and M. Li\n(2026)\nRGMP: recurrent geometric-prior multimodal policy for generalizable humanoid robot manipulation\n.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n,\nCited by:\nGeneralizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation\n,\nÂ§1\n,\nÂ§1\n,\nÂ§1\n,\nÂ§4.2\n,\nTABLE III\n,\nTABLE VI\n.\n[2]\nA. T. Abu-Jassar, H. Attar, A. Amer, V. Lyashenko, V. Yevsieiev, and A. Solyman\n(2025)\nDevelopment and investigation of vision system for a small-sized mobile humanoid robot in a smart environment\n.\nInternational Journal of Crowd Science\n9\n(\n1\n),\npp.Â 29â€“43\n.\nCited by:\nÂ§1\n.\n[3]\nM. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman,\net al.\n(2022)\nDo as i can, not as i say: grounding language in robotic affordances\n.\narXiv preprint arXiv:2204.01691\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[4]\nJ. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou\n(2023)\nQwen-vl: a frontier large vision-language model with versatile abilities\n.\narXiv preprint arXiv:2308.12966\n.\nCited by:\nÂ§3.1\n,\nTABLE II\n,\nTABLE II\n,\nTABLE II\n,\nTABLE II\n.\n[5]\nP. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner,\net al.\n(2018)\nRelational inductive biases, deep learning, and graph networks\n.\narXiv preprint arXiv:1806.01261\n.\nCited by:\nÂ§1\n.\n[6]\nY. Bengio, P. Simard, and P. Frasconi\n(1994)\nLearning long-term dependencies with gradient descent is difficult\n.\nIEEE transactions on neural networks\n5\n(\n2\n),\npp.Â 157â€“166\n.\nCited by:\nÂ§1\n.\n[7]\nA. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu,\net al.\n(2022)\nRt-1: robotics transformer for real-world control at scale\n.\narXiv preprint arXiv:2212.06817\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n.\n[8]\nL. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch\n(2021)\nDecision transformer: reinforcement learning via sequence modeling\n.\nAdvances in neural information processing systems\n34\n,\npp.Â 15084â€“15097\n.\nCited by:\nÂ§2.2\n.\n[9]\nL. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch\n(2021)\nDecision transformer: reinforcement learning via sequence modeling\n.\nAdvances in neural information processing systems\n34\n,\npp.Â 15084â€“15097\n.\nCited by:\nÂ§2.2\n.\n[10]\nC. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song\n(2023)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nIn\nProceedings of Robotics: Science and Systems\n,\nCited by:\nÂ§1\n,\nÂ§2.2\n,\nÂ§3.2\n,\n3rd item\n,\nTABLE II\n,\nTABLE III\n,\nTABLE IV\n,\nTABLE VI\n.\n[11]\nX. Chu, J. Deng, G. You, W. Liu, X. Li, J. Ji, and Y. Zhang\n(2025)\nGraspCoT: integrating physical property reasoning for 6-dof grasping under flexible language instructions\n.\narXiv preprint arXiv:2503.16013\n.\nCited by:\nÂ§1\n.\n[12]\nW. Dai, J. Li, D. Li, A. Tiong, J. Zhao, W. Wang, B. Li, P. N. Fung, and S. Hoi\n(2023)\nInstructblip: towards general-purpose vision-language models with instruction tuning\n.\nAdvances in neural information processing systems\n36\n,\npp.Â 49250â€“49267\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[13]\nD. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. H. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. R. Florence\n(2023)\nPaLM-e: an embodied multimodal language model\n.\nIn\nInternational Conference on Machine Learning\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[14]\nH. Fang, C. Wang, M. Gou, and C. Lu\n(2023)\nGraspNet-1billion: a large-scale benchmark for general object grasping\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n45\n(\n9\n),\npp.Â 11427â€“11443\n.\nCited by:\nÂ§2.1\n.\n[15]\nZ. Fu, T. Z. Zhao, and C. Finn\n(2024)\nMobile aloha: learning bimanual mobile manipulation with low-cost whole-body teleoperation\n.\narXiv preprint arXiv:2401.02117\n.\nCited by:\n3rd item\n.\n[16]\nG. Gallego, T. Delbruck, G. Orchard, C. Bartolozzi, B. Taba, A. Censi, S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis, and D. Scaramuzza\n(2022)\nEvent-based vision: a survey\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n44\n(\n1\n),\npp.Â 154â€“180\n.\nCited by:\nÂ§2.3\n.\n[17]\nF. Gao, X. Li, J. Yu, and F. Shaung\n(2023)\nA two-stage fine-tuning strategy for generalizable manipulation skill of embodied ai\n.\nIn\nProceedings of Robotics: Science and Systems\n,\nCited by:\n4th item\n,\nTABLE II\n,\nTABLE III\n,\nTABLE VI\n.\n[18]\nS. Ghosh and G. Gallego\n(2025)\nEvent-based stereo depth estimation: a survey\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n47\n(\n10\n),\npp.Â 9130â€“9149\n.\nCited by:\nÂ§2.3\n.\n[19]\nK. Han, Y. Wang, H. Chen, X. Chen, J. Guo,\net al.\n(2023)\nA survey on vision transformer\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n45\n(\n1\n),\npp.Â 87â€“110\n.\nCited by:\nÂ§2.2\n.\n[20]\nK. He, X. Zhang, S. Ren, and J. Sun\n(2016)\nDeep residual learning for image recognition\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 770â€“778\n.\nCited by:\n1st item\n,\nTABLE II\n.\n[21]\nS. Hochreiter and J. Schmidhuber\n(1997)\nLong short-term memory\n.\nNeural Computation\n9\n(\n8\n),\npp.Â 1735â€“1780\n.\nCited by:\nÂ§2.3\n.\n[22]\nS. Hu, L. Shen, Y. Zhang, Y. Chen, and D. Tao\n(2024)\nOn transforming reinforcement learning with transformers: the development trajectory\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n46\n(\n12\n),\npp.Â 8580â€“8599\n.\nCited by:\nÂ§2.2\n.\n[23]\nW. Huang, J. Liang, Z. Shi, D. Zhu, G. Wan, H. Li, B. Du, D. Tao, and M. Ye\n(2025)\nLearn from downstream and be yourself in multimodal large language model fine-tuning\n.\nIn\nInternational Conference on Machine Learning\n,\nCited by:\nÂ§1\n.\n[24]\nW. Huang, J. Liang, G. Wan, D. Zhu, H. Li, J. Shao, M. Ye, B. Du, and D. Tao\n(2025)\nBe confident: uncovering overfitting in mllm multi-task tuning\n.\nIn\nInternational Conference on Machine Learning\n,\nCited by:\nÂ§1\n.\n[25]\nP. Intelligence, K. Black, N. Brown, J. Darpinian, K. Dhabalia, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai,\net al.\n(2025)\nÏ€\n0.5\n\\pi_{0.5}\n: A vision-language-action model with open-world generalization\n.\narXiv preprint arXiv:2504.16054\n.\nCited by:\nÂ§1\n.\n[26]\nS. Jin, J. Xu, Y. Lei, and L. Zhang\n(2024)\nReasoning grasping via multimodal large language model\n.\narXiv preprint arXiv:2402.06798\n.\nCited by:\nÂ§2.1\n.\n[27]\nF. Keller, N. M. Shafiullah,\net al.\n(2024)\nConsistency policy: accelerated visuomotor policies via consistency distillation\n.\narXiv preprint arXiv:2405.07503\n.\nCited by:\nÂ§2.2\n.\n[28]\nM. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi,\net al.\n(2024)\nOpenvla: an open-source vision-language-action model\n.\nIn\nConference on Robot Learning\n,\npp.Â 2679â€“2713\n.\nCited by:\n6th item\n,\nTABLE III\n,\nTABLE VI\n.\n[29]\nJ. Li, Y. Zhu, Y. Xie, Z. Jiang, M. Seo, G. Pavlakos, and Y. Zhu\n(2024)\nOkami: teaching humanoid robots manipulation skills through single video imitation\n.\narXiv preprint arXiv:2410.11792\n.\nCited by:\nÂ§1\n.\n[30]\nJ. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng\n(2023)\nCode as policies: language model programs for embodied control\n.\nIn\n2023 IEEE International Conference on Robotics and Automation\n,\npp.Â 9493â€“9500\n.\nCited by:\nÂ§1\n.\n[31]\nJ. Liang, W. Huang, G. Wan, Q. Yang, and M. Ye\n(2025)\nLorasculpt: sculpting lora for harmonizing general and specialized knowledge in multimodal large language models\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\nCited by:\nÂ§1\n.\n[32]\nJ. Liu, W. Sun, H. Yang, P. Deng, C. Liu, N. Sebe, H. Rahmani, and A. Mian\n(2025)\nDiff9d: diffusion-based domain-generalized category-level 9-dof object pose estimation\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n47\n(\n7\n),\npp.Â 5520â€“5537\n.\nCited by:\nÂ§2.1\n.\n[33]\nK. Liu, Y. Liu, and B. Chen\n(2025)\nGeneral 3d vision-language model with fast rendering and pre-training vision-language alignment\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n47\n(\n9\n),\npp.Â 7352â€“7368\n.\nCited by:\nÂ§2.1\n.\n[34]\nS. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su, and J. Zhu\n(2024)\nRdt-1b: a diffusion foundation model for bimanual manipulation\n.\narXiv preprint arXiv:2410.07864\n.\nCited by:\n7th item\n,\nTABLE III\n,\nTABLE VI\n.\n[35]\nL. Manuelli, W. Gao, P. Florence, and R. Tedrake\n(2019)\nKpam: keypoint affordances for category-level robotic manipulation\n.\nIn\nThe International Symposium of Robotics Research\n,\npp.Â 132â€“157\n.\nCited by:\nÂ§2.1\n.\n[36]\nA. Mousavian, C. Eppner, and D. Fox\n(2019)\n6-dof graspnet: variational grasp generation for object manipulation\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp.Â 2901â€“2910\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[37]\nR. Pascanu, T. Mikolov, and Y. Bengio\n(2013)\nOn the difficulty of training recurrent neural networks\n.\nIn\nInternational conference on machine learning\n,\npp.Â 1310â€“1318\n.\nCited by:\nÂ§1\n.\n[38]\nT. Pearce, T. Rashid, A. Kanervisto, D. Bignell, M. Sun, R. Georgescu, S. V. Macua, S. Z. Tan, I. Momennejad, K. Hofmann,\net al.\n(2023)\nImitating human behaviour with diffusion models\n.\narXiv preprint arXiv:2301.10677\n.\nCited by:\nÂ§1\n.\n[39]\nD. A. Pomerleau\n(1988)\nAlvinn: an autonomous land vehicle in a neural network\n.\nAdvances in neural information processing systems\n1\n.\nCited by:\nÂ§2.2\n.\n[40]\nS. Qiu, S. Anwar, and N. Barnes\n(2021)\nDense-resolution network for point cloud classification and segmentation\n.\nIn\nProceedings of the IEEE/CVF winter conference on applications of computer vision\n,\npp.Â 3813â€“3822\n.\nCited by:\nÂ§2.1\n.\n[41]\nH. Razali and Y. Demiris\n(2025)\nKeystate-driven long-term generation of bimanual object manipulation sequences\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n47\n(\n9\n),\npp.Â 7784â€“7795\n.\nCited by:\nÂ§2.2\n.\n[42]\nS. Ross, G. Gordon, and D. Bagnell\n(2011)\nA reduction of imitation learning and structured prediction to no-regret online learning\n.\nIn\nProceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics\n,\npp.Â 627â€“635\n.\nCited by:\nÂ§2.2\n.\n[43]\nJ. J. Rothert, S. Lang, M. Seidel, and M. Hanses\n(2024)\nSim-to-real transfer for a robotics task: challenges and lessons learned\n.\nIn\n2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation\n,\npp.Â 1â€“8\n.\nCited by:\nÂ§1\n.\n[44]\nK. Roy, A. Jaiswal, and P. Panda\n(2019)\nTowards spike-based machine intelligence with neuromorphic computing\n.\nNature\n575\n(\n7784\n),\npp.Â 607â€“617\n.\nCited by:\nÂ§1\n.\n[45]\nH. Shi, H. Xu, S. Clarke, Y. Li, and J. Wu\n(2023)\nRobocook: long-horizon elasto-plastic object manipulation with diverse tools\n.\narXiv preprint arXiv:2306.14447\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[46]\nM. Shridhar, L. Manuelli, and D. Fox\n(2022)\nCliport: what and where pathways for robotic manipulation\n.\nIn\nConference on robot learning\n,\npp.Â 894â€“906\n.\nCited by:\nÂ§1\n.\n[47]\nV. Sitzmann, J. Martel, A. Bergman, D. Lindell, and G. Wetzstein\n(2020)\nImplicit neural representations with periodic activation functions\n.\nAdvances in neural information processing systems\n33\n,\npp.Â 7462â€“7473\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[48]\nI. Skubis and K. Wodarski\n(2023)\nHUMANOID robots in managerial positions-decision-making process and human oversight.\n.\nScientific Papers of Silesian University of Technology. Organization & Management/Zeszyty Naukowe Politechniki Slaskiej. Seria Organizacji i Zarzadzanie\n(\n189\n).\nCited by:\nÂ§1\n.\n[49]\nY. Song, P. Dhariwal, M. Chen, and I. Sutskever\n(2023)\nConsistency models\n.\nIn\nInternational Conference on Machine Learning\n,\nCited by:\nÂ§1\n.\n[50]\nJ. Sun, Y. Wang, and D. Wang\n(2025)\nTowards real-world aerial vision guidance with categorical 6d pose tracker\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n47\n(\n7\n),\npp.Â 6055â€“6072\n.\nCited by:\nÂ§2.1\n.\n[51]\nC. Tang, D. Huang, W. Ge, W. Liu, and H. Zhang\n(2023)\nGraspgpt: leveraging semantic knowledge from a large language model for task-oriented grasping\n.\nIEEE Robotics and Automation Letters\n8\n(\n11\n),\npp.Â 7551â€“7558\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[52]\nO. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu,\net al.\n(2024)\nOcto: an open-source generalist robot policy\n.\nIn\nProceedings of Robotics: Science and Systems\n,\nCited by:\nÂ§1\n,\nÂ§1\n,\n5th item\n,\nTABLE III\n,\nTABLE VI\n.\n[53]\nY. Tong, H. Liu, and Z. Zhang\n(2024)\nAdvancements in humanoid robots: a comprehensive review and future prospects\n.\nIEEE/CAA Journal of Automatica Sinica\n11\n(\n2\n),\npp.Â 301â€“328\n.\nCited by:\nÂ§1\n.\n[54]\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, and I. Polosukhin\n(2017)\nAttention is all you need\n.\nAdvances in neural information processing systems\n30\n.\nCited by:\n2nd item\n,\nTABLE II\n.\n[55]\nJ. Wen, Y. Zhu, J. Li, Z. Tang, C. Shen, and F. Feng\n(2025)\nDexvla: vision-language model with plug-in diffusion expert for general robot control\n.\nIn\nConference on Robot Learning\n,\nCited by:\n8th item\n,\nTABLE III\n,\nTABLE VI\n.\n[56]\nY. Wu, R. Zhao, J. Zhu, F. Chen, M. Xu, G. Li, S. Song, L. Deng, G. Wang, H. Zheng,\net al.\n(2022)\nBrain-inspired global-local learning incorporated with neuromorphic computing\n.\nNature Communications\n13\n(\n1\n),\npp.Â 65\n.\nCited by:\nÂ§2.3\n.\n[57]\nM. Yao, X. Qiu, T. Hu, J. Hu, Y. Chou, K. Tian, J. Liao, L. Leng, B. Xu, and G. Li\n(2025)\nScaling spike-driven transformer with efficient spike firing approximation training\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n.\nCited by:\nÂ§2.3\n.\n[58]\nM. Yaseen\n(2024)\nWhat is yolov9: an in-depth exploration of the internal features of the next-generation object detector\n.\narXiv preprint arXiv:2409.07813\n.\nCited by:\nFigure 2\n.\n[59]\nA. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin, D. Duong, V. Sindhwani,\net al.\n(2021)\nTransporter networks: rearranging the visual world for robotic manipulation\n.\nIn\nConference on Robot Learning\n,\npp.Â 726â€“747\n.\nCited by:\nÂ§2.1\n.\n[60]\nJ. Zhang, J. Huang, S. Jin, and S. Lu\n(2024)\nVision-language models for vision tasks: a survey\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n46\n(\n8\n),\npp.Â 5625â€“5644\n.\nCited by:\nÂ§2.1\n.\n[61]\nT. Zhao, P. Liu, and K. Lee\n(2024)\nOmDet: large-scale vision-language multi-dataset pre-training with multimodal detection network\n.\nIET Computer Vision\n18\n(\n5\n),\npp.Â 626â€“639\n.\nCited by:\nÂ§2.1\n.\n[62]\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn\n(2023)\nLearning fine-grained bimanual manipulation with low-cost arms\n.\nIn\nProceedings of Robotics: Science and Systems\n,\nCited by:\nÂ§2.2\n.\n[63]\nB. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid,\net al.\n(2023)\nRt-2: vision-language-action models transfer web knowledge to robotic control\n.\nIn\nConference on Robot Learning\n,\npp.Â 2165â€“2183\n.\nCited by:\nÂ§1\n,\nÂ§1\n,\nÂ§2.2\n.",
    "preview_text": "Humanoid robot manipulation is a crucial research area for executing diverse human-level tasks, involving high-level semantic reasoning and low-level action generation. However, precise scene understanding and sample-efficient learning from human demonstrations remain critical challenges, severely hindering the applicability and generalizability of existing frameworks. This paper presents a novel RGMP-S, Recurrent Geometric-prior Multimodal Policy with Spiking features, facilitating both high-level skill reasoning and data-efficient motion synthesis. To ground high-level reasoning in physical reality, we leverage lightweight 2D geometric inductive biases to enable precise 3D scene understanding within the vision-language model. Specifically, we construct a Long-horizon Geometric Prior Skill Selector that effectively aligns the semantic instructions with spatial constraints, ultimately achieving robust generalization in unseen environments. For the data efficiency issue in robotic action generation, we introduce a Recursive Adaptive Spiking Network. We parameterize robot-object interactions via recursive spiking for spatiotemporal consistency, fully distilling long-horizon dynamic features while mitigating the overfitting issue in sparse demonstration scenarios. Extensive experiments are conducted across the Maniskill simulation benchmark and three heterogeneous real-world robotic systems, encompassing a custom-developed humanoid, a desktop manipulator, and a commercial robotic platform. Empirical results substantiate the superiority of our method over state-of-the-art baselines and validate the efficacy of the proposed modules in diverse generalization scenarios. To facilitate reproducibility, the source code and video demonstrations are publicly available at https://github.com/xtli12/RGMP-S.git.\n\nGeneralizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation\nXuetao Li\nÄ°D\n, Wenke Huang\nÄ°D\n, Mang Ye\nÄ°D\n,Â , Jifeng Xuan\nÄ°D\n,\nBo Du\n",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "VLA",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå‡ ä½•å…ˆéªŒå’Œé€’å½’å°–å³°ç‰¹å¾å­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºæå‡äººå½¢æœºå™¨äººæ“ä½œçš„é«˜å±‚è¯­ä¹‰æ¨ç†å’Œæ•°æ®æ•ˆç‡è¿åŠ¨åˆæˆï¼Œåœ¨æœªè§ç¯å¢ƒä¸­å®ç°é²æ£’æ³›åŒ–ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦\n\n**è®ºæ–‡æ ‡é¢˜ï¼š** ç”¨äºäººå½¢æœºå™¨äººæ“ä½œçš„å¯æ³›åŒ–å‡ ä½•å…ˆéªŒä¸é€’å½’è„‰å†²ç‰¹å¾å­¦ä¹ \n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\näººå½¢æœºå™¨äººæ“ä½œæ˜¯æ‰§è¡Œå¤šæ ·åŒ–äººç±»çº§åˆ«ä»»åŠ¡çš„å…³é”®é¢†åŸŸï¼Œæ¶‰åŠé«˜çº§è¯­ä¹‰æ¨ç†å’Œä½çº§åŠ¨ä½œç”Ÿæˆã€‚ç„¶è€Œï¼Œç²¾ç¡®çš„åœºæ™¯ç†è§£å’Œä»äººç±»æ¼”ç¤ºä¸­è¿›è¡Œæ ·æœ¬é«˜æ•ˆå­¦ä¹ ä»æ˜¯å…³é”®æŒ‘æˆ˜ï¼Œä¸¥é‡é˜»ç¢äº†ç°æœ‰æ¡†æ¶çš„é€‚ç”¨æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é¢ä¸´ä¸¤å¤§ç“¶é¢ˆï¼š**Iï¼‰è¯­ä¹‰-å‡ ä½•é¸¿æ²Ÿ**ï¼šè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ“…é•¿è¯­ä¹‰ç†è§£ï¼Œä½†ç¼ºä¹æ˜¾å¼çš„å‡ ä½•æ¨ç†èƒ½åŠ›ï¼Œå¯¼è‡´ç”Ÿæˆçš„æŠ€èƒ½è®¡åˆ’åœ¨ç‰©ç†ä¸Šä¸å¯è¡Œï¼ˆä¾‹å¦‚ï¼Œæ— æ³•åŒºåˆ†æŠ“æ¡ä¸æå–å˜å½¢ç‰©ä½“ï¼‰ã€‚**IIï¼‰æ•°æ®æ•ˆç‡ä½ä¸‹**ï¼šæ‰©æ•£ç­–ç•¥ç­‰ç”Ÿæˆæ¨¡å‹è¡¨è¾¾èƒ½åŠ›è™½å¼ºï¼Œä½†æ¨ç†å»¶è¿Ÿé«˜ï¼›è€Œå‰é¦ˆå¼æ¨¡å‹ï¼ˆå¦‚Transformerï¼‰å®ç°å®æ—¶æ§åˆ¶åˆéœ€è¦æµ·é‡æ•°æ®ï¼Œåœ¨ç¨€ç–æ¼”ç¤ºåœºæ™¯ä¸‹å®¹æ˜“è¿‡æ‹Ÿåˆã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸º **RGMP-S** çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—è§£å†³ä¸Šè¿°é—®é¢˜ï¼š\n- **é•¿æ—¶åŸŸå‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨ï¼ˆLGSSï¼‰**ï¼šæå‡ºä¸€ç§è½»é‡çº§çš„å‡ ä½•è°ƒä¼˜ç­–ç•¥ï¼Œå°†å‡ ä½•å¸¸è¯†ï¼ˆå¦‚ç‰©ä½“å½¢çŠ¶ï¼‰æ³¨å…¥VLMã€‚å®ƒé€šè¿‡**é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æœºåˆ¶**ï¼Œç»“åˆç‰©ä½“è¾¹ç•Œæ¡†å’Œåˆ†å‰²ä¿¡æ¯ï¼Œå¯¹è¯­ä¹‰æŒ‡ä»¤è¿›è¡Œå‡ ä½•ä¸€è‡´æ€§æ£€æŸ¥ï¼Œä»è€Œä»é¢„å®šä¹‰æŠ€èƒ½åº“ä¸­å¯é åœ°é€‰æ‹©ç‰©ç†ä¸Šå¯è¡Œçš„æ“ä½œæŠ€èƒ½ï¼ˆå¦‚ä¾§æŠ“ã€ä¸Šæã€æå–ï¼‰ã€‚\n- **é€’å½’è‡ªé€‚åº”è„‰å†²ç½‘ç»œï¼ˆRASNetï¼‰**ï¼šè®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç­–ç•¥æ¶æ„ï¼Œç”¨äºä»ç¨€ç–æ¼”ç¤ºä¸­é«˜æ•ˆå­¦ä¹ ç¨³å¥çš„åŠ¨ä½œç­–ç•¥ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š**1ï¼‰è‡ªé€‚åº”è¡°å‡æœºåˆ¶ï¼ˆADMï¼‰** å’Œ**æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰**ï¼Œä»¥é€’å½’æ–¹å¼æ„å»ºå…¨å±€ç©ºé—´è®°å¿†ï¼›**2ï¼‰è„‰å†²å¯†é›†ç‰¹å¾æå–ï¼ˆSDFEï¼‰** å’Œ**è‡ªé€‚åº”è„‰å†²ç¥ç»å…ƒï¼ˆASNï¼‰**ï¼ŒåŠ¨æ€è¿‡æ»¤å†—ä½™è§†è§‰å™ªå£°å¹¶å¢å¼ºä»»åŠ¡å…³é”®ç‰¹å¾ï¼›**3ï¼‰é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰** ç”¨äºç²¾ç»†åŒ–å¤šæ¨¡æ€åŠ¨ä½œç”Ÿæˆï¼Œæå‡æ§åˆ¶ç²¾åº¦ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nç ”ç©¶åœ¨**ManiSkill2ä»¿çœŸåŸºå‡†**å’Œ**ä¸‰ä¸ªå¼‚æ„çœŸå®æœºå™¨äººå¹³å°**ï¼ˆå®šåˆ¶äººå½¢æœºå™¨äººã€æ¡Œé¢æœºæ¢°è‡‚ã€å•†ç”¨Alohaæœºå™¨äººï¼‰ä¸Šè¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼Œæ¶µç›–äº¤äº’å¼æœåŠ¡ã€é›¶æ ·æœ¬æŠ“å–ã€æ¯›å·¾æŠ˜å ã€å€’æ°´ã€åˆ†æ‹£ç­‰10é¡¹ä»»åŠ¡ã€‚å…³é”®ç»“æœåŒ…æ‹¬ï¼š\n- **æ€§èƒ½ä¼˜è¶Š**ï¼šåœ¨çœŸå®ä¸–ç•Œäº¤äº’ä»»åŠ¡ä¸­ï¼ŒRGMP-Sæ¯”æ‰©æ•£ç­–ç•¥åŸºçº¿æ€§èƒ½æå‡**19%**ï¼Œæ•´ä½“æ³›åŒ–æˆåŠŸç‡é«˜è¾¾**89%**ã€‚\n- **æ•°æ®é«˜æ•ˆ**ï¼šåœ¨æŠ“å–ä»»åŠ¡ä¸­ï¼Œä»…éœ€**40æ¡**ä¸“å®¶è½¨è¿¹å³å¯è¾¾åˆ°æ‰©æ•£ç­–ç•¥ä½¿ç”¨**200æ¡**è½¨è¿¹çš„æ€§èƒ½ï¼Œæ•°æ®æ•ˆç‡æå‡**5å€**ã€‚\n- **å®æ—¶æ€§å¼º**ï¼šæ¨ç†é¢‘ç‡è¾¾åˆ°**75.2 Hz**ï¼Œè¿œè¶…æ‰©æ•£ç­–ç•¥ï¼ˆ1.01 Hzï¼‰å’ŒOpenVLAï¼ˆ3.6 Hzï¼‰ï¼Œæ»¡è¶³å®æ—¶æ§åˆ¶éœ€æ±‚ã€‚\n- **é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å¼º**ï¼šä»…åœ¨â€œèŠ¬è¾¾ç½â€æ•°æ®ä¸Šè®­ç»ƒï¼Œå³å¯æˆåŠŸæŠ“å–æœªè§è¿‡çš„â€œå¯ä¹ç½â€ã€â€œå–·é›¾ç“¶â€ç”šè‡³â€œäººæ‰‹â€ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\næœ¬å·¥ä½œä¸º**å®ç°é«˜æ•ˆã€ç¨³å¥ä¸”å¯æ³›åŒ–çš„æœºå™¨äººæ“ä½œ**æä¾›äº†é‡è¦æ€è·¯å’Œå®è·µæ–¹æ¡ˆã€‚å…¶ä»·å€¼ä½“ç°åœ¨ï¼š\n- **æ–¹æ³•è®ºåˆ›æ–°**ï¼šåˆ›é€ æ€§åœ°å°†**å‡ ä½•å…ˆéªŒæ¨ç†**ä¸**è„‰å†²æ—¶ç©ºç‰¹å¾å­¦ä¹ **ç›¸ç»“åˆï¼Œä¸ºå¼¥è¡¥è¯­ä¹‰è§„åˆ’ä¸ç‰©ç†æ‰§è¡Œä¹‹é—´çš„é¸¿æ²Ÿæä¾›äº†æ–°èŒƒå¼ã€‚\n- **å®ç”¨æ€§å¼º**ï¼šæ¡†æ¶å…¼å…·**é«˜æ•°æ®æ•ˆç‡**å’Œ**ä½æ¨ç†å»¶è¿Ÿ**ï¼Œé™ä½äº†å¯¹å¤§è§„æ¨¡æ¼”ç¤ºæ•°æ®å’Œæ˜‚è´µç®—åŠ›çš„ä¾èµ–ï¼Œæ›´è´´è¿‘å®é™…æœºå™¨äººéƒ¨ç½²éœ€æ±‚ã€‚\n- **å¼€æºä¸å¯å¤ç°**ï¼šå…¬å¼€äº†æºä»£ç å’Œè§†é¢‘æ¼”ç¤ºï¼Œä¸ºç¤¾åŒºæä¾›äº†å¼ºå¤§çš„åŸºå‡†ï¼Œæ¨åŠ¨äº†é€šç”¨æœºå™¨äººå­¦ä¹ çš„ç ”ç©¶ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **è®ºæ–‡æ ‡é¢˜**\nã€ŠGeneralizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulationã€‹\n\n### **æ ¸å¿ƒç ”ç©¶é—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³äººå½¢æœºå™¨äººçµå·§æ“ä½œä¸­çš„ä¸¤å¤§å…³é”®æŒ‘æˆ˜ï¼š\n1.  **è¯­ä¹‰-å‡ ä½•é¸¿æ²Ÿ**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰èƒ½ç†è§£é«˜çº§è¯­ä¹‰æŒ‡ä»¤ï¼Œä½†ç¼ºä¹å¯¹ç‰©ä½“**ä¸‰ç»´å‡ ä½•ç»“æ„**çš„ç²¾ç¡®æ„ŸçŸ¥ï¼Œå¯¼è‡´ç”Ÿæˆçš„æŠ€èƒ½è®¡åˆ’ï¼ˆå¦‚æŠ“å– vs. æå–ï¼‰åœ¨ç‰©ç†ä¸Šä¸å¯è¡Œã€‚\n2.  **æ•°æ®æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨åŠ¨æ€ã€éç»“æ„åŒ–ç¯å¢ƒä¸­ï¼Œä»**ç¨€ç–çš„äººç±»æ¼”ç¤º**ä¸­é«˜æ•ˆå­¦ä¹ é²æ£’ã€å¯æ³›åŒ–çš„åŠ¨ä½œç­–ç•¥éå¸¸å›°éš¾ã€‚ä¸»æµæ–¹æ³•ï¼ˆå¦‚æ‰©æ•£ç­–ç•¥ï¼‰å­˜åœ¨æ¨ç†å»¶è¿Ÿé«˜ã€æ•°æ®éœ€æ±‚å¤§ã€æ˜“è¿‡æ‹Ÿåˆç­‰é—®é¢˜ã€‚\n\n### **æ ¸å¿ƒè§£å†³æ–¹æ¡ˆï¼šRGMP-Sæ¡†æ¶**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸º **RGMP-S** çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ååŒå·¥ä½œæ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼š\n\n#### **1. æŠ€æœ¯åˆ›æ–°ä¸€ï¼šé•¿æ—¶åŸŸå‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨**\n*   **æ˜¯ä»€ä¹ˆ**ï¼šä¸€ä¸ªè½»é‡çº§çš„**å‡ ä½•æ¨ç†æ¨¡å—**ï¼Œç”¨äºæ¡¥æ¥è¯­ä¹‰æŒ‡ä»¤ä¸ç‰©ç†æ‰§è¡Œã€‚\n*   **å¦‚ä½•è§£å†³â€œè¯­ä¹‰-å‡ ä½•é¸¿æ²Ÿâ€**ï¼š\n    *   **åŸç†**ï¼šä¸ä¾èµ–è®¡ç®—æ˜‚è´µçš„æ˜¾å¼3Dé‡å»ºï¼Œè€Œæ˜¯ä»2D RGBå›¾åƒä¸­æå–**éšå¼å‡ ä½•å…ˆéªŒ**ï¼ˆå¦‚ç‰©ä½“å½¢çŠ¶ã€ç©ºé—´å…³ç³»ï¼‰ã€‚\n    *   **æ–¹æ³•**ï¼š\n        *   **è§†è§‰-è¯­è¨€è§£é‡Š**ï¼šä½¿ç”¨VLMï¼ˆQwen-VLï¼‰è§£ææŒ‡ä»¤å¹¶å®šä½ç›®æ ‡ç‰©ä½“ï¼ˆè¾“å‡ºè¾¹ç•Œæ¡†ï¼‰ã€‚\n        *   **è¯­ä¹‰åˆ†å‰²**ï¼šä½¿ç”¨YOLOv8-segä»è¾¹ç•Œæ¡†ä¸­æå–ç‰©ä½“çš„ç²¾ç¡®å½¢çŠ¶ä¿¡æ¯ã€‚\n        *   **å‡ ä½•æ¥åœ°çš„æ€ç»´é“¾æ¨ç†**ï¼šé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯ï¼Œå°†è¾¹ç•Œæ¡†åæ ‡ã€å½¢çŠ¶ç±»åˆ«ä¸**å¸¸è¯†å‡ ä½•çŸ¥è¯†**ç»“åˆï¼Œè¿›è¡Œå¤šæ­¥æ¨ç†ï¼Œé€‰æ‹©ç‰©ç†ä¸Šå¯è¡Œä¸”æ— ç¢°æ’çš„æ“ä½œæŠ€èƒ½ï¼ˆå¦‚ä¾§æŠ“ã€ä¸Šæã€æå–ï¼‰ã€‚\n    *   **ä»·å€¼**ï¼šä½¿æœºå™¨äººèƒ½åƒäººç±»ä¸€æ ·ï¼Œæ ¹æ®ç‰©ä½“çš„**å‡ ä½•å½¢å˜**ï¼ˆå¦‚å‹æ‰çš„æ˜“æ‹‰ç½ï¼‰è‡ªé€‚åº”é€‰æ‹©æ­£ç¡®æŠ€èƒ½ï¼Œå®ç°äº†**å¯¹æœªè§åœºæ™¯çš„é²æ£’æ³›åŒ–**ã€‚\n\n#### **2. æŠ€æœ¯åˆ›æ–°äºŒï¼šé€’å½’è‡ªé€‚åº”è„‰å†²ç½‘ç»œ**\n*   **æ˜¯ä»€ä¹ˆ**ï¼šä¸€ä¸ªæ–°å‹çš„**ç­–ç•¥ç½‘ç»œæ¶æ„**ï¼Œç”¨äºä»ç¨€ç–æ¼”ç¤ºä¸­é«˜æ•ˆå­¦ä¹ æ—¶ç©ºä¸€è‡´çš„åŠ¨ä½œç­–ç•¥ã€‚\n*   **å¦‚ä½•è§£å†³â€œæ•°æ®æ•ˆç‡ä¸æ³›åŒ–â€**ï¼š\n    *   **åŸç†**ï¼šå°†**è„‰å†²ç¥ç»ç½‘ç»œ**çš„åŠ¨æ€ç‰¹æ€§ä¸é€’å½’è®¡ç®—ç»“åˆï¼Œæ˜¾å¼åœ°å»ºæ¨¡æœºå™¨äºº-ç‰©ä½“äº¤äº’çš„æ—¶ç©ºæœºåˆ¶ã€‚\n    *   **æ ¸å¿ƒç»„ä»¶**ï¼š\n        *   **è‡ªé€‚åº”è¡°å‡æœºåˆ¶**ï¼šåŠ¨æ€è°ƒèŠ‚å†å²è®°å¿†çš„ä¿ç•™ï¼Œç¼“è§£é€’å½’ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚\n        *   **è‡ªé€‚åº”è„‰å†²ç¥ç»å…ƒ**ï¼šæ›¿ä»£ä¼ ç»ŸSNNä¸­å›ºå®šçš„è¡°å‡å› å­å’Œä¸å¯å¾®çš„è„‰å†²å‡½æ•°ï¼Œå®ç°å¯å¾®åˆ†è®­ç»ƒï¼Œèƒ½åŠ¨æ€æ”¾å¤§ä»»åŠ¡å…³é”®ç‰¹å¾ã€æŠ‘åˆ¶è§†è§‰å™ªå£°ã€‚\n        *   **è„‰å†²å¯†é›†ç‰¹å¾æå–**ï¼šåœ¨ç©ºé—´ç‰¹å¾ä¸­åµŒå…¥**æ—¶é—´æ¨ç†**èƒ½åŠ›ï¼Œå¸®åŠ©æ¨¡å‹ä»å¯†é›†çš„è§†è§‰æµä¸­è¿‡æ»¤å†—ä½™ä¿¡æ¯ã€‚\n        *   **é«˜æ–¯æ··åˆæ¨¡å‹ç²¾ç‚¼**ï¼šç”¨GMMå¯¹åŠ¨ä½œç©ºé—´è¿›è¡Œå¤šæ¨¡æ€å»ºæ¨¡ï¼Œé¿å…å•é«˜æ–¯å›å½’åˆ°å‡å€¼ï¼Œæå‡æ§åˆ¶ç²¾åº¦ã€‚\n    *   **ä»·å€¼**ï¼šèµ‹äºˆäº†ç­–ç•¥å¼ºå¤§çš„**æ—¶ç©ºç‰¹å¾æå–å’Œè®°å¿†èƒ½åŠ›**ï¼Œä½¿å…¶èƒ½ä»æå°‘çš„æ¼”ç¤ºï¼ˆå¦‚æ¯ä¸ªæŠ€èƒ½ä»…40æ¡è½¨è¿¹ï¼‰ä¸­å­¦ä¹ ï¼Œå¹¶æ³›åŒ–åˆ°æ–°ç‰©ä½“ã€æ–°èƒŒæ™¯ä¸­ï¼ŒåŒæ—¶ä¿æŒ**é«˜æ¨ç†é¢‘ç‡**ã€‚\n\n### **å®é™…ä»·å€¼ä¸å®éªŒéªŒè¯**\n*   **æ€§èƒ½æå‡**ï¼šåœ¨ManiSkill2ä»¿çœŸåŸºå‡†å’Œä¸‰ä¸ªå¼‚æ„çœŸå®æœºå™¨äººå¹³å°ï¼ˆè‡ªå®šä¹‰äººå½¢æœºå™¨äººã€æ¡Œé¢æœºæ¢°è‡‚ã€å•†ç”¨Alohaæœºå™¨äººï¼‰ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚\n*   **å…³é”®ç»“æœ**ï¼š\n    *   **æ³›åŒ–èƒ½åŠ›å¼º**ï¼šåœ¨é›¶æ ·æœ¬æŠ“å–ä»»åŠ¡ä¸­ï¼Œå¯¹æœªè§è¿‡ç‰©ä½“ï¼ˆå–·é›¾ç“¶ã€äººæ‰‹ï¼‰çš„æˆåŠŸç‡æ˜¾è‘—é«˜äºåŸºçº¿æ¨¡å‹ã€‚\n    *   **æ•°æ®æ•ˆç‡é«˜**ï¼šä»…éœ€**åŸºçº¿æ–¹æ³•1/5çš„æ¼”ç¤ºæ•°æ®**å³å¯è¾¾åˆ°ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚\n    *   **å®æ—¶æ€§é«˜**ï¼šæ¨ç†é¢‘ç‡è¾¾**75.2 Hz**ï¼Œè¿œè¶…æ‰©æ•£ç­–ç•¥ï¼Œæ»¡è¶³å®æ—¶æ§åˆ¶éœ€æ±‚ã€‚\n    *   **å¤„ç†å¤æ‚ä»»åŠ¡**ï¼šåœ¨æŠ˜å æ¯›å·¾ã€å€’æ°´ã€ç®±å†…å–ç‰©ç­‰é•¿æ—¶åŸŸã€å¤šæ­¥éª¤ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚\n\n### **æ€»ç»“**\nè¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°åœ¨äº**åˆ›é€ æ€§åœ°å°†å‡ ä½•å…ˆéªŒæ¨ç†ä¸è„‰å†²æ—¶ç©ºç‰¹å¾å­¦ä¹ ç›¸ç»“åˆ**ï¼Œæå‡ºäº†ä¸€ä¸ªæ—¢â€œèªæ˜â€ï¼ˆèƒ½è¿›è¡Œå‡ ä½•æ¨ç†ï¼‰åˆâ€œé«˜æ•ˆâ€ï¼ˆæ•°æ®éœ€æ±‚å°‘ã€æ¨ç†å¿«ï¼‰çš„æœºå™¨äººæ“ä½œæ¡†æ¶RGMP-Sã€‚å®ƒæœ‰æ•ˆåœ°è§£å†³äº†å½“å‰æœºå™¨äººå­¦ä¹ èŒƒå¼ä¸­**è¯­ä¹‰ä¸ç‰©ç†è„±èŠ‚**ä»¥åŠ**ä¸¥é‡ä¾èµ–å¤§æ•°æ®**çš„ç—›ç‚¹ï¼Œä¸ºå®ç°åœ¨å¼€æ”¾ä¸–ç•Œä¸­é€šç”¨ã€çµå·§çš„æœºå™¨äººæ“ä½œæä¾›äº†æœ‰å‰æ™¯çš„è·¯å¾„ã€‚",
            "timestamp": "2026-01-20T17:51:14.669071",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡å‘å±•è„‰ç»œæ¢³ç†ï¼šä»RGMPåˆ°RGMP-S\n\næœ¬æ–‡æå‡ºçš„ **RGMP-S** æ˜¯å…¶å‰ä½œ **RGMP** çš„æ‰©å±•ä¸æ·±åŒ–ã€‚é€šè¿‡æ¢³ç†ä»RGMPåˆ°RGMP-Sçš„æ¼”è¿›ï¼Œå¯ä»¥æ¸…æ™°åœ°çœ‹åˆ°è¯¥ç ”ç©¶æ–¹å‘åœ¨è§£å†³**äººå½¢æœºå™¨äººçµå·§æ“ä½œ**æ ¸å¿ƒæŒ‘æˆ˜ä¸Šçš„å‘å±•è„‰ç»œï¼Œä¸»è¦æ²¿ç€**é«˜å±‚è¯­ä¹‰-å‡ ä½•æ¨ç†**å’Œ**åº•å±‚æ•°æ®é«˜æ•ˆç­–ç•¥å­¦ä¹ **ä¸¤ä¸ªå…³é”®æ–¹å‘è¿›è¡Œè¿­ä»£ä¸èåˆã€‚\n\n### å‘å±•è„‰ç»œæ€»è§ˆ\n\n| å·¥ä½œé˜¶æ®µ | æ ¸å¿ƒæ¡†æ¶ | å…³é”®æ”¹è¿›æ–¹å‘ | è§£å†³çš„é—®é¢˜ | ä¸å‰ä½œç›¸æ¯”çš„è¿›æ­¥ |\n| :--- | :--- | :--- | :--- | :--- |\n| **å‰ä½œ (AAAI 2026)** | RGMP | 1. **å‡ ä½•å…ˆéªŒå¼•å…¥** <br> 2. **é€’å½’è®¡ç®—æ¶æ„** | è¯­ä¹‰-å‡ ä½•é¸¿æ²Ÿï¼›ç¨€ç–æ¼”ç¤ºä¸‹çš„ç­–ç•¥å­¦ä¹  | é¦–æ¬¡å°†å‡ ä½•å¸¸è¯†ä¸é€’å½’æ¨¡å‹ç»“åˆï¼Œå®ç°åˆæ­¥çš„æ³›åŒ–æ“ä½œã€‚ |\n| **æœ¬å·¥ä½œ (æ‰©å±•ç‰ˆ)** | **RGMP-S** | 1. **å‡ ä½•æ¨ç†å¢å¼º (LGSS)** <br> 2. **æ—¶ç©ºç‰¹å¾å­¦ä¹ æ·±åŒ– (RASNet)** <br> 3. **è¯„ä¼°èŒƒå›´æ‰©å±•** | é•¿è§†é‡ä»»åŠ¡è§„åˆ’ï¼›å¤æ‚èƒŒæ™¯ä¸‹çš„é²æ£’æ€§ï¼›æ›´å¹¿æ³›çš„æ³›åŒ–éªŒè¯ | åœ¨æ¨ç†æ¶æ„ã€ç‰¹å¾æå–æœºåˆ¶å’Œå®éªŒéªŒè¯ä¸Šè¿›è¡Œäº†ç³»ç»Ÿæ€§å¢å¼ºï¼Œæ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›æ˜¾è‘—æå‡ã€‚ |\n\n---\n\n### å‘å±•æ–¹å‘ä¸€ï¼šé«˜å±‚è¯­ä¹‰-å‡ ä½•æ¨ç†çš„æ¼”è¿›\n\nè¿™ä¸ªæ–¹å‘è‡´åŠ›äºè§£å†³ **â€œè¯­ä¹‰è®¡åˆ’åœ¨ç‰©ç†ä¸Šä¸å¯æ‰§è¡Œâ€** çš„æ ¹æœ¬é—®é¢˜ï¼Œå³è®©æœºå™¨äººä¸ä»…èƒ½å¬æ‡‚æŒ‡ä»¤ï¼Œè¿˜èƒ½ç†è§£ç‰©ä½“çš„å‡ ä½•ç»“æ„å¹¶é€‰æ‹©å¯è¡Œçš„æ“ä½œæŠ€èƒ½ã€‚\n\n- **RGMP çš„å¥ åŸºå·¥ä½œï¼š**\n    - **åšäº†ä»€ä¹ˆï¼š** æå‡ºäº†åˆæ­¥çš„**å‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨**ã€‚é€šè¿‡å°†è½»é‡åŒ–çš„2Då‡ ä½•å½’çº³åç½®ï¼ˆå¦‚ç‰©ä½“å½¢çŠ¶ï¼‰æ³¨å…¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç‰©ä½“å‡ ä½•å±æ€§ï¼ˆå¦‚â€œåœ†æŸ±å½¢â€ vs. â€œå‹æ‰çš„â€ï¼‰é€‰æ‹©ä¸åŒçš„æŠ“å–æŠ€èƒ½ï¼ˆå¦‚â€œä¾§æ¡â€ vs. â€œæå–â€ï¼‰ã€‚\n    - **æ”¹è¿›ç‚¹ï¼š** ç›¸è¾ƒäºä¼ ç»Ÿä»…ä¾èµ–è¯­ä¹‰åµŒå…¥æˆ–ç²—ç³™ç©ºé—´æç¤ºï¼ˆå¦‚è¾¹ç•Œæ¡†ï¼‰çš„VLMæ–¹æ³•ï¼ŒRGMPé¦–æ¬¡æ˜ç¡®åœ°å°†**å‡ ä½•å¸¸è¯†**ä½œä¸ºè¿æ¥è¯­ä¹‰ä¸åŠ¨ä½œçš„æ¡¥æ¢ï¼Œç¼“è§£äº†åœ¨ç‰©ä½“å½¢å˜ç­‰æœªè§åœºæ™¯ä¸‹çš„æŠ€èƒ½é€‰æ‹©é”™è¯¯ã€‚\n\n- **RGMP-S çš„å¢å¼ºä¸æ·±åŒ–ï¼š**\n    - **æ ¸å¿ƒæ”¹è¿›ï¼š** å°†æŠ€èƒ½é€‰æ‹©å™¨å‡çº§ä¸º **é•¿è§†é‡å‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨**ã€‚\n    - **æŠ€æœ¯å¢å¼ºï¼š**\n        1.  **ç»“æ„åŒ–æ¨ç†é“¾ï¼š** å¼•å…¥äº† **Chain-of-Thought** æœºåˆ¶ï¼Œèƒ½å¤Ÿå°†å¤æ‚çš„é•¿è§†é‡ä»»åŠ¡ï¼ˆå¦‚â€œå æ¯›å·¾â€ã€â€œå€’æ°´â€ï¼‰åˆ†è§£ä¸ºç»“æ„åŒ–çš„å¤šé˜¶æ®µæŠ€èƒ½åºåˆ—ã€‚\n        2.  **æ›´å¼ºçš„ç‰©ç† groundingï¼š** CoTæç¤ºä¸­æ˜ç¡®åŠ å…¥äº†**æ— ç¢°æ’**ã€**è¿åŠ¨å­¦å¯è¡Œ**ç­‰ç‰©ç†çº¦æŸï¼Œä½¿ç”Ÿæˆçš„è®¡åˆ’ä¸ä»…åœ¨è¯­ä¹‰ä¸Šåˆç†ï¼Œåœ¨ç‰©ç†ä¸Šä¹Ÿæ›´å¯é ã€‚\n    - **å¸¦æ¥çš„ä»·å€¼ï¼š** æ˜¾è‘—æå‡äº†åœ¨**é•¿è§†é‡ã€å¤šæ­¥éª¤**æ“ä½œä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ã€‚æ¨ç†è¿‡ç¨‹æ›´åŠ é€æ˜å’Œç¬¦åˆäººç±»è®¤çŸ¥ï¼Œå¢å¼ºäº†ç³»ç»Ÿåœ¨å¤æ‚ã€åŠ¨æ€ç¯å¢ƒä¸­çš„å†³ç­–é²æ£’æ€§ã€‚\n\n### å‘å±•æ–¹å‘äºŒï¼šåº•å±‚æ•°æ®é«˜æ•ˆç­–ç•¥å­¦ä¹ çš„æ¼”è¿›\n\nè¿™ä¸ªæ–¹å‘è‡´åŠ›äºè§£å†³ **â€œå¦‚ä½•ä»æœ‰é™çš„æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ é²æ£’ã€å®æ—¶çš„æ§åˆ¶ç­–ç•¥â€** çš„é—®é¢˜ï¼Œé¿å…å¯¹å¤§è§„æ¨¡æ•°æ®å’Œè¿­ä»£æ‰©æ•£æ¨¡å‹çš„ä¾èµ–ã€‚\n\n- **RGMP çš„å¥ åŸºå·¥ä½œï¼š**\n    - **åšäº†ä»€ä¹ˆï¼š** é‡‡ç”¨äº†**é€’å½’è®¡ç®—**æ¶æ„æ¥å»ºæ¨¡æœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨ä¸ç›®æ ‡ç‰©ä½“ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œä½œä¸ºåŠ¨ä½œç”Ÿæˆçš„åŸºç¡€ç­–ç•¥ã€‚\n    - **æ”¹è¿›ç‚¹ï¼š** ç›¸è¾ƒäºè®¡ç®—å»¶è¿Ÿé«˜çš„æ‰©æ•£ç­–ç•¥æˆ–éœ€è¦æµ·é‡æ•°æ®çš„Transformerç­–ç•¥ï¼Œé€’å½’æ¨¡å‹æä¾›äº†**å‰å‘ã€é«˜æ•ˆ**çš„æ¨ç†æ–¹å¼ã€‚ä½†ä½œè€…æŒ‡å‡ºå…¶å­˜åœ¨**æ¢¯åº¦æ¶ˆå¤±**å’Œéš¾ä»¥ä»å¤æ‚è§†è§‰èƒŒæ™¯ä¸­ç­›é€‰ä»»åŠ¡ç›¸å…³ç‰¹å¾çš„å±€é™ã€‚\n\n- **RGMP-S çš„å¢å¼ºä¸æ·±åŒ–ï¼š**\n    - **æ ¸å¿ƒæ”¹è¿›ï¼š** æå‡ºäº†å…¨æ–°çš„ **é€’å½’è‡ªé€‚åº”è„‰å†²ç½‘ç»œ**ã€‚\n    - **æŠ€æœ¯å¢å¼ºï¼š**\n        1.  **æ—¶ç©ºç‰¹å¾èåˆï¼š** å¼•å…¥ **è„‰å†²å¯†é›†ç‰¹å¾æå–** æ¨¡å—ã€‚SDFEåˆ©ç”¨**è‡ªé€‚åº”è„‰å†²ç¥ç»å…ƒ**æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»å…ƒçš„åŠ¨æ€ç‰¹æ€§ï¼Œèƒ½æœ‰æ•ˆæŠ‘åˆ¶å¯†é›†è§†è§‰ç‰¹å¾ä¸­çš„å†—ä½™ä¿¡æ¯ï¼ˆèƒŒæ™¯å™ªå£°ï¼‰ï¼ŒåŒæ—¶å¢å¼ºå¯¹ä»»åŠ¡å…³é”®ç‰¹å¾ï¼ˆç‰©ä½“è½®å»“ï¼‰çš„æ„ŸçŸ¥ã€‚\n        2.  **åŠ¨æ€è®°å¿†æœºåˆ¶ï¼š** é€šè¿‡**è‡ªé€‚åº”è¡°å‡æœºåˆ¶**åŠ¨æ€è°ƒèŠ‚å†å²ä¿¡æ¯çš„ä¿ç•™ç‡ï¼Œè§£å†³äº†æ ‡å‡†é€’å½’æ¨¡å‹çš„æ¢¯åº¦é—®é¢˜ï¼Œå¹¶æ„å»ºäº†æ›´æœ‰æ•ˆçš„**ç©ºé—´è®°å¿†**ã€‚\n        3.  **å¤šæ¨¡æ€åŠ¨ä½œè¡¨å¾ï¼š** æ²¿ç”¨å¹¶å¼ºè°ƒäº†**é«˜æ–¯æ··åˆæ¨¡å‹**æ¥æ‹ŸåˆåŠ¨ä½œåˆ†å¸ƒï¼Œé¿å…äº†å•é«˜æ–¯åˆ†å¸ƒå›å½’åˆ°å¹³å‡å€¼çš„å¼Šç«¯ï¼Œèƒ½æ›´å¥½åœ°æ•æ‰æ¼”ç¤ºæ•°æ®ä¸­çš„å¤šæ¨¡æ€ç‰¹æ€§ï¼ˆå¦‚ä¸åŒçš„æŠ“å–è§’åº¦ï¼‰ã€‚\n    - **å¸¦æ¥çš„ä»·å€¼ï¼š**\n        - **æ•°æ®æ•ˆç‡æå¤§æå‡ï¼š** å®éªŒè¡¨æ˜ï¼ŒRGMP-Sä»…éœ€ **1/5** çš„æ¼”ç¤ºæ•°æ®å³å¯è¾¾åˆ°ä¸æ‰©æ•£ç­–ç•¥åŸºçº¿ç›¸å½“çš„æ€§èƒ½ã€‚\n        - **æ¨ç†é€Ÿåº¦ä¸é²æ£’æ€§å…¼å¾—ï¼š** ä¿æŒé«˜æ¨ç†é¢‘ç‡ï¼ˆ75.2 Hzï¼‰çš„åŒæ—¶ï¼Œåœ¨æ‚ä¹±èƒŒæ™¯å’Œæœªè§ç‰©ä½“ä¸Šçš„æ³›åŒ–æˆåŠŸç‡æ˜¾è‘—æé«˜ã€‚\n        - **èµ‹äºˆç­–ç•¥â€œæ—¶ç©ºæ„ŸçŸ¥â€ï¼š** ASNçš„å¼•å…¥ä½¿ç­–ç•¥ä¸ä»…è€ƒè™‘å½“å‰ç©ºé—´ç‰¹å¾ï¼Œè¿˜èƒ½éšå¼åœ°åˆ©ç”¨æ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿™å¯¹äºå¤„ç†åŠ¨æ€å’Œé•¿è§†é‡ä»»åŠ¡è‡³å…³é‡è¦ã€‚\n\n### å‘å±•æ–¹å‘ä¸‰ï¼šç³»ç»Ÿæ•´åˆä¸è¯„ä¼°èŒƒå¼çš„æ‰©å±•\n\n- **RGMP çš„å¥ åŸºå·¥ä½œï¼š** æå‡ºäº†RGMPç»Ÿä¸€æ¡†æ¶çš„æ¦‚å¿µï¼Œå¹¶åœ¨ä»¿çœŸå’Œéƒ¨åˆ†çœŸå®æœºå™¨äººå¹³å°ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚\n- **RGMP-S çš„å¢å¼ºä¸æ·±åŒ–ï¼š**\n    - **ç³»ç»Ÿæ•´åˆæ›´æˆç†Ÿï¼š** LGSSå’ŒRASNetçš„å‡çº§ä½¿å¾—ä¸¤ä¸ªæ¨¡å—çš„åä½œæ›´åŠ ç´§å¯†å’Œé«˜æ•ˆï¼Œå½¢æˆäº†ä»â€œå¬æ‡‚æŒ‡ä»¤ -> å‡ ä½•åˆ†æ -> æŠ€èƒ½é€‰æ‹© -> æ—¶ç©ºç‰¹å¾æå– -> ç²¾ç»†åŠ¨ä½œç”Ÿæˆâ€çš„å®Œæ•´ã€é²æ£’æµæ°´çº¿ã€‚\n    - **è¯„ä¼°èŒƒå›´å¤§å¹…æ‰©å±•ï¼š**\n        1.  **ä»»åŠ¡å¤æ‚åº¦å¢åŠ ï¼š** æ–°å¢äº†â€œå æ¯›å·¾â€ã€â€œå€’æ°´â€ã€â€œç®±å†…å–ç‰©â€ç­‰æå…·æŒ‘æˆ˜æ€§çš„**é•¿è§†é‡ã€éåˆšæ€§ç‰©ä½“ã€åŠ¨æ€åœºæ™¯**ä»»åŠ¡ã€‚\n        2.  **æœºå™¨äººå¹³å°å¤šæ ·åŒ–ï¼š** åœ¨è‡ªå®šä¹‰äººå½¢æœºå™¨äººåŸºç¡€ä¸Šï¼Œå¢åŠ äº†æ¡Œé¢æœºæ¢°è‡‚å’Œå•†ç”¨Alohaæœºå™¨äººå¹³å°ï¼ŒéªŒè¯äº†æ¡†æ¶çš„**è·¨æœ¬ä½“æ³›åŒ–èƒ½åŠ›**ã€‚\n        3.  **åŸºå‡†æµ‹è¯•æ›´å…¨é¢ï¼š** åœ¨ManiSkill2ä»¿çœŸåŸºå‡†ä¸Šè¿›è¡Œäº†æ›´å¹¿æ³›çš„å¯¹æ¯”ï¼Œè¯æ˜äº†å…¶ç›¸å¯¹äºSOTAæ–¹æ³•çš„æŒç»­é¢†å…ˆä¼˜åŠ¿ã€‚\n\n### æ€»ç»“\n\nä»RGMPåˆ°RGMP-Sçš„å‘å±•è„‰ç»œï¼Œæ¸…æ™°åœ°å±•ç¤ºäº†ä¸€æ¡é€šå‘**é€šç”¨åŒ–ã€æ•°æ®é«˜æ•ˆã€é²æ£’**çš„äººå½¢æœºå™¨äººæ“ä½œæŠ€æœ¯çš„è·¯å¾„ï¼š\n\n1.  **ä»â€œæœ‰æ— â€åˆ°â€œä¼˜åŠ£â€ï¼š** RGMP**è§£å†³äº†**å‡ ä½•å…ˆéªŒå’Œé€’å½’æ¨¡å‹ç”¨äºæ“ä½œç­–ç•¥çš„â€œæœ‰æ— â€é—®é¢˜ï¼›RGMP-Såˆ™åœ¨æ­¤åŸºç¡€ä¸Š**ä¼˜åŒ–äº†**æ¨ç†çš„æ·±åº¦ï¼ˆCoTï¼‰å’Œç‰¹å¾å­¦ä¹ çš„è´¨é‡ï¼ˆè„‰å†²ç½‘ç»œï¼‰ï¼Œå®ç°äº†ä»â€œå¯ç”¨â€åˆ°â€œå¥½ç”¨ã€å¼ºå¤§â€çš„è·¨è¶Šã€‚\n2.  **åŒè½®é©±åŠ¨ï¼Œæ·±åº¦èåˆï¼š** ä¸¤ä¸ªæ ¸å¿ƒå‘å±•æ–¹å‘å¹¶è¡Œæ¨è¿›ä¸”ç›¸äº’ä¿ƒè¿›ã€‚æ›´å¥½çš„å‡ ä½•æ¨ç†ï¼ˆLGSSï¼‰ä¸ºåº•å±‚ç­–ç•¥æä¾›äº†æ›´å‡†ç¡®çš„æŠ€èƒ½å’Œçº¦æŸç›®æ ‡ï¼›æ›´å¼ºå¤§çš„æ—¶ç©ºç‰¹å¾å­¦ä¹ ï¼ˆRASNetï¼‰åˆ™èƒ½æ›´ç²¾å‡†åœ°æ‰§è¡Œè¿™äº›æŠ€èƒ½ï¼Œå°¤å…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹ã€‚ä¸¤è€…çš„æ·±åº¦èåˆæ˜¯RGMP-Sé«˜æ€§èƒ½çš„å…³é”®ã€‚\n3.  **é¢å‘çœŸå®ä¸–ç•Œçš„é—­ç¯ï¼š** ç ”ç©¶å§‹ç»ˆä»¥è§£å†³çœŸå®ä¸–ç•ŒæŒ‘æˆ˜ä¸ºå¯¼å‘ï¼ˆæ•°æ®ç¨€ç¼ºã€ç¯å¢ƒéç»“æ„åŒ–ã€ç‰©ä½“å¤šæ ·æ€§ï¼‰ï¼Œå¹¶é€šè¿‡ä¸æ–­æ‰©å±•å’Œä¸¥è‹›çš„å®éªŒè¯„ä¼°æ¥éªŒè¯å…¶å®é™…ä»·å€¼ã€‚RGMP-Såœ¨å¤šé¡¹çœŸå®é•¿è§†é‡ä»»åŠ¡ä¸Šçš„æˆåŠŸï¼Œæ ‡å¿—ç€è¯¥æ¡†æ¶å‘å®é™…åº”ç”¨è¿ˆå‡ºäº†åšå®çš„ä¸€æ­¥ã€‚\n\n**æœªæ¥æ–¹å‘** å·²åœ¨å¤±è´¥æ¡ˆä¾‹åˆ†æä¸­æŒ‡æ˜ï¼šå¦‚ä½•æ•´åˆè§¦è§‰åé¦ˆæˆ–ä¸»åŠ¨è§†è§‰ä»¥è§£å†³**æ¯«ç±³çº§ç²¾æ“ä½œ**å’Œ**ä¸¥é‡è‡ªé®æŒ¡**ç­‰æç«¯æƒ…å†µï¼Œå°†æ˜¯ä¸‹ä¸€ä¸ªéœ€è¦æ”»å…‹çš„å ¡å’ã€‚",
            "timestamp": "2026-01-20T17:52:11.945725",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œä½œè€…å›¢é˜Ÿåœ¨æœ¬æ–‡ï¼ˆRGMP-Sï¼‰ä¹‹å‰ï¼Œæœ‰ä¸€ç¯‡å‘è¡¨äºAAAI 2026çš„ä¼šè®®è®ºæ–‡ä½œä¸ºå…¶åˆæ­¥ç‰ˆæœ¬ã€‚ä»¥ä¸‹æ˜¯å…³äºè¿™ç¯‡å‰ä½œçš„è¯¦ç»†åˆ†æã€‚\n\n### å‰ä½œä¿¡æ¯æ€»è§ˆè¡¨\n\n| é¡¹ç›® | å‰ä½œ (AAAI 2026) | å½“å‰å·¥ä½œ (æœ¬æœŸåˆŠè®ºæ–‡ RGMP-S) | ä¸»è¦åŒºåˆ«ä¸æ”¹è¿› |\n| :--- | :--- | :--- | :--- |\n| **è®ºæ–‡æ ‡é¢˜** | **RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation** | **Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation** | æ ‡é¢˜æ›´ç²¾ç¡®åœ°åæ˜ äº†**æ–°å¢çš„è„‰å†²ç‰¹å¾å­¦ä¹ **æ ¸å¿ƒè´¡çŒ®ã€‚ |\n| **æ ¸å¿ƒæ¡†æ¶** | **RGMP** (Recurrent Geometric-prior Multimodal Policy) | **RGMP-S** (RGMP with **Spiking** features) | **â€œ-Sâ€åç¼€**æ˜ç¡®æ ‡è¯†äº†åœ¨åŸæœ‰æ¡†æ¶åŸºç¡€ä¸Šé›†æˆäº†**è„‰å†²ç¥ç»ç½‘ç»œ**æ¨¡å—ã€‚ |\n| **ä¸¤å¤§æ ¸å¿ƒæ¨¡å—** | 1. **å‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨** (åˆæ­¥ç‰ˆæœ¬) <br> 2. **é€’å½’è®¡ç®—æ¨¡å—** (ç”¨äºåŠ¨ä½œç”Ÿæˆ) | 1. **é•¿è§†è·å‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨ (LGSS)** <br> 2. **é€’å½’è‡ªé€‚åº”è„‰å†²ç½‘ç»œ (RASNet)** | 1. **LGSS**ï¼šå¢å¼ºäº†**å±‚æ¬¡åŒ–æ€ç»´é“¾**æœºåˆ¶ï¼Œæ”¯æŒæ›´å¤æ‚çš„**é•¿è§†è·ä»»åŠ¡åˆ†è§£**ã€‚<br> 2. **RASNet**ï¼šå®Œå…¨**é‡æ„äº†åŠ¨ä½œç”Ÿæˆç½‘ç»œ**ï¼Œå¼•å…¥äº†**è„‰å†²å¯†é›†ç‰¹å¾æå–**å’Œ**è‡ªé€‚åº”è„‰å†²ç¥ç»å…ƒ**ã€‚ |\n| **æŠ€æœ¯åˆ›æ–°ç‚¹** | 1. å°†è½»é‡çº§å‡ ä½•å…ˆéªŒä¸VLMç»“åˆã€‚<br>2. ä½¿ç”¨é€’å½’è®¡ç®—æ„å»ºæœºå™¨äºº-ç›®æ ‡ç©ºé—´å…³ç³»ã€‚ | 1. **LGSS**ï¼šé€šè¿‡**CoTæœºåˆ¶**å®ç°ç»“æ„åŒ–ä»»åŠ¡åˆ†è§£ã€‚<br>2. **RASNet**ï¼šåŒ…å«**SDFE** (è„‰å†²å¯†é›†ç‰¹å¾æå–)ã€**ASN** (è‡ªé€‚åº”è„‰å†²ç¥ç»å…ƒ)ã€**ADM** (è‡ªé€‚åº”è¡°å‡æœºåˆ¶)ã€‚<br>3. ä½¿ç”¨**é«˜æ–¯æ··åˆæ¨¡å‹**è¿›è¡ŒåŠ¨ä½œç²¾ç‚¼ã€‚ | **ä»â€œå‡ ä½•+é€’å½’â€å‡çº§ä¸ºâ€œå‡ ä½•+è„‰å†²é€’å½’â€**ã€‚é‡ç‚¹è§£å†³äº†å‰ä½œä¸­**ç‰¹å¾å†—ä½™**å’Œ**ç¼ºä¹æ—¶åºé€»è¾‘**çš„é—®é¢˜ï¼Œå¢å¼ºäº†åœ¨å¤æ‚èƒŒæ™¯å’Œé•¿åºåˆ—ä»»åŠ¡ä¸­çš„**é²æ£’æ€§**ä¸**æ³›åŒ–èƒ½åŠ›**ã€‚ |\n| **å®éªŒè¯„ä¼°** | åœ¨ManiSkillä»¿çœŸåŸºå‡†å’Œ**è‡ªç ”äººå½¢æœºå™¨äºº**ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚ | **å¤§å¹…æ‰©å±•äº†å®éªŒèŒƒå›´**ï¼š<br>1. å¢åŠ äº†**å•†ç”¨Alohaæœºå™¨äºº**å¹³å°ã€‚<br>2. å¼•å…¥äº†ä¸‰ä¸ªæ–°çš„**é•¿è§†è·æŒ‘æˆ˜ä»»åŠ¡**ï¼šæ¯›å·¾æŠ˜å ã€å€’æ°´ã€ç®±å†…å–ç‰©ã€‚<br>3. è¿›è¡Œäº†æ›´å…¨é¢çš„**æ¶ˆèå®éªŒ**å’Œ**æ•°æ®æ•ˆç‡åˆ†æ**ã€‚ | è¯„ä¼°æ›´**ç³»ç»Ÿã€ä¸¥æ ¼ã€å¤šæ ·åŒ–**ï¼Œæ—¨åœ¨éªŒè¯æ¡†æ¶åœ¨**æ›´å¤æ‚ä»»åŠ¡**å’Œ**ä¸åŒæœºå™¨äººæœ¬ä½“**ä¸Šçš„**é€šç”¨æ€§**ã€‚è¯æ˜äº†æ–°æ¨¡å—ï¼ˆå°¤å…¶æ˜¯SDFEï¼‰åœ¨é•¿è§†è·ä»»åŠ¡ä¸­çš„å…³é”®ä½œç”¨ã€‚ |\n| **ä¸»è¦æ”¹å–„ç›®æ ‡** | è§£å†³**è¯­ä¹‰-å‡ ä½•é¸¿æ²Ÿ**ï¼Œå®ç°ä»æ¼”ç¤ºä¸­**æ•°æ®é«˜æ•ˆ**çš„æŠ€èƒ½å­¦ä¹ ã€‚ | åœ¨ç»§æ‰¿å‰ä½œç›®æ ‡åŸºç¡€ä¸Šï¼Œ**é‡ç‚¹è§£å†³**ï¼š<br>1. **é•¿è§†è·æ¨ç†èƒ½åŠ›**ä¸è¶³ã€‚<br>2. åœ¨æ‚ä¹±åœºæ™¯ä¸­**ç‰¹å¾å†—ä½™**å’Œ**å™ªå£°æ•æ„Ÿ**é—®é¢˜ã€‚<br>3. **æ—¶åºåŠ¨æ€ç‰¹æ€§**å»ºæ¨¡èƒ½åŠ›å¼±ã€‚ | æ”¹å–„æ˜¯**æ ¹æœ¬æ€§çš„æ¶æ„æ¼”è¿›**ï¼Œè€Œéå¾®è°ƒã€‚**RASNetæ›¿ä»£äº†åŸæœ‰çš„é€’å½’æ¨¡å—**ï¼Œé€šè¿‡è„‰å†²åŠ¨åŠ›å­¦æ˜¾å¼åœ°å»ºæ¨¡**æ—¶ç©ºä¸€è‡´æ€§**ï¼Œä½¿ç­–ç•¥èƒ½æ›´å¥½åœ°**è’¸é¦é•¿è§†è·åŠ¨æ€ç‰¹å¾**å¹¶**æŠ‘åˆ¶è¿‡æ‹Ÿåˆ**ã€‚ |\n| **æ€§èƒ½è¡¨ç°** | å·²å±•ç°å‡ºä¼˜äºæ‰©æ•£ç­–ç•¥ç­‰åŸºçº¿æ–¹æ³•çš„æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚ | **æ€§èƒ½å…¨é¢æå‡**ï¼š<br>â€¢ åœ¨é›¶æ ·æœ¬æŠ“å–ä»»åŠ¡ä¸Šï¼Œå¹³å‡æˆåŠŸç‡ä»RGMPçš„87%æå‡è‡³**89%**ã€‚<br>â€¢ åœ¨é•¿è§†è·ä»»åŠ¡ï¼ˆå¦‚å€’æ°´ï¼‰ä¸Šï¼Œç›¸æ¯”æ‰©æ•£ç­–ç•¥åŸºçº¿æœ‰çº¦**16%** çš„æ€§èƒ½æå‡ã€‚<br>â€¢ ä¿æŒäº†**75.2 Hz**çš„é«˜å®æ—¶æ¨ç†é¢‘ç‡å’Œ**5å€**äºæ‰©æ•£ç­–ç•¥çš„æ•°æ®æ•ˆç‡ã€‚ | é€šè¿‡å¼•å…¥è„‰å†²ç‰¹å¾å­¦ä¹ å’Œå¢å¼ºçš„å‡ ä½•æ¨ç†ï¼Œåœ¨**ä¿æŒé«˜å®æ—¶æ€§**çš„åŒæ—¶ï¼Œè¿›ä¸€æ­¥æå‡äº†åœ¨**æœªè§è¿‡çš„å¤æ‚åœºæ™¯**å’Œ**é•¿åºåˆ—ä»»åŠ¡**ä¸­çš„**æˆåŠŸç‡å’Œé²æ£’æ€§**ã€‚ |\n\n### æ€»ç»“ï¼šä¸å‰ä½œçš„æ ¸å¿ƒåŒºåˆ«\n\næœ¬æœŸåˆŠè®ºæ–‡ **RGMP-S** ç›¸å¯¹äºå‰ä½œ **RGMP** çš„æ”¹è¿›ä¸»è¦ä½“ç°åœ¨ **æ·±åº¦ã€å¹¿åº¦å’Œæœºåˆ¶** ä¸‰ä¸ªå±‚é¢ï¼š\n\n1.  **ç®—æ³•æ·±åº¦å¢å¼º**ï¼šå°†åˆæ­¥çš„å‡ ä½•æŠ€èƒ½é€‰æ‹©å™¨å‡çº§ä¸ºå…·æœ‰**é“¾å¼æ€ç»´ï¼ˆCoTï¼‰** çš„ **LGSS**ï¼Œä½¿å…¶èƒ½è¿›è¡Œ**å¤šé˜¶æ®µã€ç»“æ„åŒ–çš„ä»»åŠ¡åˆ†è§£**ï¼Œæ˜¾è‘—æå‡äº†**é•¿è§†è·è§„åˆ’èƒ½åŠ›**ï¼ˆå¦‚æ¯›å·¾æŠ˜å ï¼‰ã€‚\n2.  **æ¶æ„æœºåˆ¶é©æ–°**ï¼šè¿™æ˜¯æœ€æ ¸å¿ƒçš„æ”¹è¿›ã€‚ç”¨å…¨æ–°çš„ **RASNet** å–ä»£äº†åŸæœ‰çš„é€’å½’åŠ¨ä½œç”Ÿæˆå™¨ã€‚\n    *   **åŠ¨æœº**ï¼šè§£å†³å‰ä½œæ¨¡å‹åœ¨**å¤æ‚èƒŒæ™¯**ä¸‹ç‰¹å¾å†—ä½™ã€ç¼ºä¹**æ—¶åºè¿‡æ»¤**èƒ½åŠ›çš„é—®é¢˜ã€‚\n    *   **æ ¸å¿ƒåˆ›æ–°**ï¼š\n        *   **Spiking Dense Feature Extraction (SDFE)**ï¼šå¼•å…¥è„‰å†²ç¥ç»å…ƒå¤„ç†å¯†é›†è§†è§‰ç‰¹å¾ï¼ŒåŠ¨æ€è¿‡æ»¤å†—ä½™ä¿¡æ¯ï¼Œå¢å¼ºå¯¹ä»»åŠ¡å…³é”®ç‰¹å¾ï¼ˆå¦‚ç‰©ä½“è½®å»“ï¼‰çš„èšç„¦ã€‚\n        *   **Adaptive Spike Neurons (ASN)**ï¼šç”¨**è‡ªé€‚åº”è¡°å‡å› å­**æ›¿ä»£ä¼ ç»Ÿè„‰å†²ç¥ç»å…ƒçš„å›ºå®šå‚æ•°ï¼Œè§£å†³äº†è®­ç»ƒä¸­çš„æ¢¯åº¦éš¾é¢˜ï¼Œå¹¶å®ç°äº†å¯¹æ—¶åºä¿¡æ¯çš„åŠ¨æ€è°ƒæ§ã€‚\n        *   **Adaptive Decay Mechanism (ADM)**ï¼šåœ¨é€’å½’è®¡ç®—ä¸­åŠ¨æ€è°ƒèŠ‚å†å²è®°å¿†ç•™å­˜ï¼Œæ›´å¥½åœ°æ•è·é•¿ç¨‹ç©ºé—´ä¾èµ–ã€‚\n    *   **ç»“æœ**ï¼šä½¿ç­–ç•¥å…·å¤‡äº†å†…åœ¨çš„**æ—¶ç©ºä¸€è‡´æ€§å»ºæ¨¡èƒ½åŠ›**ï¼Œä»ç¨€ç–æ¼”ç¤ºä¸­å­¦ä¹ åŠ¨æ€ç‰¹å¾æ›´é«˜æ•ˆï¼Œæ³›åŒ–èƒ½åŠ›æ›´å¼ºã€‚\n3.  **è¯„ä¼°å¹¿åº¦æ‹“å±•**ï¼šé€šè¿‡å¢åŠ **å•†ä¸šæœºå™¨äººå¹³å°**å’Œ**æ›´å…·æŒ‘æˆ˜æ€§çš„é•¿è§†è·ä»»åŠ¡**ï¼Œå…¨é¢éªŒè¯äº†æ¡†æ¶çš„**é€šç”¨æ€§**å’Œ**å®ç”¨æ€§**ï¼Œè¯æ˜äº†å…¶æ”¹è¿›ä¸ä»…åœ¨æŒ‡æ ‡ä¸Šï¼Œæ›´ä½“ç°åœ¨å®é™…åº”ç”¨çš„å¹¿åº¦ä¸Šã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼Œå‰ä½œRGMPæå‡ºäº†ä¸€ä¸ªèåˆå‡ ä½•å…ˆéªŒå’Œé€’å½’è®¡ç®—çš„åˆ›æ–°æ¡†æ¶ï¼Œè€Œæœ¬å·¥ä½œRGMP-Såˆ™é€šè¿‡å¼•å…¥è„‰å†²ç¥ç»ç½‘ç»œå’Œå¢å¼ºçš„æ¨ç†æœºåˆ¶ï¼Œå¯¹è¯¥æ¡†æ¶è¿›è¡Œäº†æ·±åˆ»çš„â€œæ™ºèƒ½åŒ–â€å‡çº§ï¼Œä½¿å…¶æ—¶ç©ºæ„ŸçŸ¥ã€æŠ—å¹²æ‰°å’Œå¤æ‚ä»»åŠ¡æ¨ç†èƒ½åŠ›å¾—åˆ°äº†è´¨çš„é£è·ƒã€‚**",
            "timestamp": "2026-01-20T17:52:58.970739",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## è®ºæ–‡å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\næœ¬æ–‡æå‡ºçš„RGMP-Sæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯æ¨¡å—ï¼š**é•¿æ—¶åŸŸå‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨**å’Œ**é€’å½’è‡ªé€‚åº”è„‰å†²ç½‘ç»œ**ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†çš„æŠ€æœ¯ç»†èŠ‚è¡¨æ ¼ï¼š\n\n| æ¨¡å—åç§° | æ ¸å¿ƒç»„ä»¶ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°ä¸å®ç° |\n|---------|---------|-------------------|--------------|\n| **é•¿æ—¶åŸŸå‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨** | è§†è§‰-è¯­è¨€è§£é‡Šæ¨¡å— | ä½¿ç”¨Qwen-VL APIè¿›è¡Œè§†è§‰-è¯­è¨€ç†è§£ï¼Œé€šè¿‡ç»“æ„åŒ–æç¤ºå·¥ç¨‹æ•´åˆå…ˆéªŒçŸ¥è¯†ã€‚è¾“å…¥æç¤ºæ ¼å¼ï¼š`\"Instruction: Identify target object in image and output bounding box [x1, y1, x2, y2]\"`ã€‚æ¨¡å‹ä½¿ç”¨20ä¸ªä»£è¡¨æ€§ç¤ºä¾‹å»ºç«‹è§†è§‰ç‰¹å¾ã€å‡ ä½•å±æ€§ä¸æ“ä½œç­–ç•¥çš„æ˜ å°„ã€‚ | - å®šä½å‡†ç¡®ç‡ï¼š93.1%<br>- æŸ¥è¯¢å»¶è¿Ÿï¼š45 ms<br>- è®­ç»ƒç¤ºä¾‹ï¼š20ä¸ªè·¨ç±»åˆ«æ ·æœ¬<br>- è¾“å…¥å›¾åƒåˆ†è¾¨ç‡ï¼š640Ã—480 |\n| | è¯­ä¹‰åˆ†å‰²æ¨¡å— | ä½¿ç”¨YOLOv8n-segæ¨¡å‹å¤„ç†è¾¹ç•Œæ¡†åŒºåŸŸï¼Œç”Ÿæˆç²¾ç¡®çš„è¯­ä¹‰åˆ†å‰²æ©ç ä»¥æ•è·ç‰©ä½“å‡ ä½•ç»“æ„ã€‚æ¨¡å‹åœ¨å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒï¼Œå¹¶åœ¨ä¸“æœ‰æ•°æ®ä¸Šå¾®è°ƒã€‚ | - å¹³å‡äº¤å¹¶æ¯”ï¼š97.6%<br>- æ¨¡å‹ï¼šYOLOv8n-seg<br>- è¾“å…¥ï¼šè¾¹ç•Œæ¡†åæ ‡æå–çš„å›¾åƒåŒºåŸŸ |\n| | å‡ ä½•åŸºç¡€é“¾å¼æ€ç»´æŠ€èƒ½æ¨ç†å™¨ | é‡‡ç”¨é“¾å¼æ€ç»´åè®®å°†æ„ŸçŸ¥è¾“å…¥è½¬æ¢ä¸ºå¯æ‰§è¡ŒåŠ¨ä½œã€‚æç¤ºæ¨¡æ¿åµŒå…¥ä»»åŠ¡ç‰¹å®šåŸè¯­ï¼ˆå¦‚ä¾§æŠ“ã€å€’æ°´ï¼‰ï¼Œé€šè¿‡ç½®ä¿¡åº¦åŠ æƒè¯„ä¼°å€™é€‰æŠ€èƒ½ã€‚å†³ç­–å‡½æ•°ï¼š<br>`ğ’« = plan(â„, ğ’ª | ğ’)`<br>å…¶ä¸­ğ’ä¸ºé¢„å®šä¹‰ä¸Šä¸‹æ–‡é›†`{(â„_i, ğ’ª_i, ğ’«_i)}`ï¼ŒåŒ…å«æŒ‡ä»¤ã€æç¤ºå’Œå¸¸è¯†æ¨ç†ã€‚ | - æŠ€èƒ½åº“ï¼š5ç§æŠ€èƒ½ï¼ˆä¾§æŠ“ã€æŠ¬èµ·ã€é¡¶éƒ¨æå–ã€å€’æ°´ã€é€’é€ï¼‰<br>- æ¨ç†æœºåˆ¶ï¼šç»“æ„åŒ–CoTåè®®<br>- è¾“å‡ºï¼šæŠ€èƒ½åºåˆ—ğ’« |\n| **é€’å½’è‡ªé€‚åº”è„‰å†²ç½‘ç»œ** | è‡ªé€‚åº”è¡°å‡æœºåˆ¶ | åŠ¨æ€è°ƒèŠ‚å†å²è®°å¿†ä¿ç•™ï¼Œç”Ÿæˆå†…å®¹è‡ªé€‚åº”è¡°å‡å› å­ï¼š<br>`ğ’² = Ïƒ(ğ’_{1Ã—1}(SReLU(ğ’_{3Ã—3}(F_0))))`<br>å…¶ä¸­Ïƒä¸ºsigmoidå‡½æ•°ï¼Œğ’ä¸ºå·ç§¯å±‚ï¼ŒSReLUä¸ºå¹³æ–¹ReLUæ¿€æ´»ã€‚ | - è¡°å‡å› å­èŒƒå›´ï¼š[0, 1]<br>- å·ç§¯æ ¸ï¼š3Ã—3å’Œ1Ã—1<br>- æ¿€æ´»å‡½æ•°ï¼šSReLU |\n| | æ—‹è½¬ä½ç½®ç¼–ç  | é€šè¿‡å‡ ä½•æ—‹è½¬åœ¨ç‰¹å¾ç©ºé—´ä¸­ç¼–ç ä½ç½®ä¿¡æ¯ï¼Œå¢å¼ºä½ç½®æ•æ„Ÿä¾èµ–ã€‚æ—‹è½¬çŸ©é˜µå®šä¹‰ä¸ºï¼š<br>`ğ‘_Î˜(x) = [[cos(Î¸), -sin(Î¸)], [sin(Î¸), cos(Î¸)]]`<br>å…¶ä¸­`Î¸_{h,w,j} = Î¸_jÂ·x_h + Î¸_jÂ·x_w`, `Î¸_j = 1/(10000^{2j/C})`ã€‚ | - åŸºç¡€å¸¸æ•°ï¼š10000<br>- é€šé“ç»´åº¦ï¼šC<br>- ä½ç½®ç¼–ç ï¼šäºŒç»´ç¦»æ•£åæ ‡(x_h, x_w) |\n| | é€’å½’è®¡ç®—æœºåˆ¶ | ä½¿ç”¨WKVæœºåˆ¶å»ºæ¨¡å›¾åƒå—é—´çš„åºåˆ—ä¾èµ–ï¼š<br>`WKV_i = (n_i + exp(u)âŠ™k_iâŠ™v_i) / (d_i + exp(u)âŠ™k_i)`<br>`n_i = n_{i-1}âŠ™exp(-ğ’²) + k_iâŠ™v_i`<br>`d_i = d_{i-1}âŠ™exp(-ğ’²) + k_i`<br>å…¶ä¸­k_iã€v_iä¸ºå›¾åƒå—è¡¨ç¤ºï¼Œuä¸ºå¯å­¦ä¹ ä½ç½®åç½®ã€‚ | - å›¾åƒå—å¤§å°ï¼š16Ã—16<br>- åˆå§‹çŠ¶æ€ï¼šn_0=k_0âŠ™v_0, d_0=k_0<br>- ä½ç½®åç½®ï¼šuâˆˆ(0,1) |\n| | è„‰å†²å¯†é›†ç‰¹å¾æå– | é€šè¿‡å¯†é›†å—æå–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”Ÿæˆç©ºé—´æ³¨æ„åŠ›æ©ç ï¼š<br>`X_k = B_k IÌ„_k B_k^T`<br>å…¶ä¸­`IÌ„ = (1/n)(I - (1/n)ğŸ)`ï¼Œn=hÃ—wã€‚éšåé€šè¿‡softmaxå’Œè‡ªé€‚åº”è„‰å†²ç¥ç»å…ƒèåˆç‰¹å¾ï¼š<br>`Q_A = ğ’_{1Ã—1}(U_k) + ğ’œ(F_k)`ã€‚ | - æ³¨æ„åŠ›çŸ©é˜µç»´åº¦ï¼šnÃ—n<br>- è¾“å‡ºé€šé“ï¼šC/r<br>- èåˆæ–¹å¼ï¼šé€å…ƒç´ ç›¸åŠ  |\n| | è‡ªé€‚åº”è„‰å†²ç¥ç»å…ƒ | å¼•å…¥åŠ¨æ€è¡°å‡å› å­æ›¿ä»£å›ºå®šè¡°å‡ï¼Œè§£å†³è„‰å†²ç¥ç»å…ƒä¸å¯å¾®é—®é¢˜ã€‚è†œç”µä½æ›´æ–°å…¬å¼ï¼š<br>`V_t = H_{t-1} + X_t`<br>`S_t = Î˜(V_t - u_th)`<br>`H_t = V_t e^{-Î”t/Ï„}(1 - S_t) + V_reset S_t`<br>å…¶ä¸­Ï„ä¸ºè‡ªé€‚åº”è¡°å‡å› å­ï¼Œé€šè¿‡é€šé“æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—ï¼š<br>`Ï„ = ğ”¼[ğ’_{1Ã—1}(R(ğ’_{1Ã—1}(F_k)))]`ã€‚ | - é˜ˆå€¼ï¼šu_th<br>- é‡ç½®ç”µä½ï¼šV_reset<br>- è¡°å‡å› å­Ï„ï¼šåŠ¨æ€è®¡ç®—<br>- æ—¶é—´é—´éš”ï¼šÎ”tï¼ˆå•ä½æ—¶é—´ï¼‰ |\n| | é«˜æ–¯æ··åˆæ¨¡å‹ç²¾ç‚¼ | ä½¿ç”¨GMMå»ºæ¨¡åŠ¨ä½œå¤šæ¨¡æ€åˆ†å¸ƒï¼Œé€šè¿‡é©¬æ°è·ç¦»é€‰æ‹©æœ€ä¼˜ç»„ä»¶ï¼š<br>`P(ğ±âˆ£Î˜) = Î£_{k=1}^K Î±_k ğ’©(ğ±âˆ£Î¼_k, Î£_k)`<br>`l_k = âˆš[(a_in - Î¼_k^Ï‰)^âŠ¤ (Î£_k^{Ï‰,Ï‰})^{-1} (a_in - Î¼_k^Ï‰)]`<br>æœ€ç»ˆåŠ¨ä½œé€‰æ‹©ï¼š`a^* = arg min_{Î¼_k^Ï‰} l_k`ã€‚ | - ç»„ä»¶æ•°ï¼šK=6<br>- å‚æ•°ä¼°è®¡ï¼šæœŸæœ›æœ€å¤§åŒ–ç®—æ³•<br>- è·ç¦»åº¦é‡ï¼šé©¬æ°è·ç¦» |\n\n### å…³é”®æŠ€æœ¯åŸç†æ€»ç»“\n\n1. **å‡ ä½•å…ˆéªŒæ³¨å…¥**ï¼šé€šè¿‡è½»é‡çº§å‡ ä½•é€‚é…å™¨å°†å½¢çŠ¶çº§å¸¸è¯†åç½®æ³¨å…¥VLMï¼Œå®ç°è¯­ä¹‰æŒ‡ä»¤ä¸æ½œåœ¨å‡ ä½•çº¦æŸçš„å¯¹é½ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šå¾®è°ƒã€‚\n\n2. **æ—¶ç©ºä¸€è‡´æ€§å»ºæ¨¡**ï¼š\n   - **é€’å½’è®¡ç®—**ï¼šé€šè¿‡WKVæœºåˆ¶å»ºç«‹ç©ºé—´è®°å¿†ï¼Œæ•è·æœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨ä¸ç›®æ ‡ç‰©ä½“çš„æ–¹å‘æ€§ç©ºé—´ä¾èµ–ã€‚\n   - **è„‰å†²æœºåˆ¶**ï¼šè‡ªé€‚åº”è„‰å†²ç¥ç»å…ƒåŠ¨æ€è¿‡æ»¤å†—ä½™è§†è§‰å™ªå£°ï¼Œå¢å¼ºæ—¶é—´ç»´åº¦ä¸Šçš„é²æ£’æ€§ã€‚\n\n3. **å¤šæ¨¡æ€åŠ¨ä½œç”Ÿæˆ**ï¼šGMMæœ‰æ•ˆè¡¨ç¤ºåŠ¨ä½œåˆ†å¸ƒçš„å¤šå³°ç‰¹æ€§ï¼Œé¿å…å•é«˜æ–¯åˆ†å¸ƒå›å½’åˆ°å‡å€¼å¯¼è‡´çš„æ§åˆ¶ç²¾åº¦æŸå¤±ã€‚\n\n4. **é«˜æ•ˆæ¨ç†è®¾è®¡**ï¼šæ•´ä¸ªæ¡†æ¶é‡‡ç”¨å‰é¦ˆæ¶æ„ï¼Œé¿å…æ‰©æ•£ç­–ç•¥çš„è¿­ä»£å»å™ªè¿‡ç¨‹ï¼Œå®ç°75.2 Hzçš„é«˜æ¨ç†é¢‘ç‡ï¼Œæ»¡è¶³å®æ—¶æ§åˆ¶éœ€æ±‚ã€‚\n\n### å®é™…ä»·å€¼ä½“ç°\n- **æ•°æ®æ•ˆç‡**ï¼šä»…éœ€40æ¡ä¸“å®¶è½¨è¿¹å³å¯è¾¾åˆ°ä¸æ‰©æ•£ç­–ç•¥ï¼ˆéœ€200æ¡è½¨è¿¹ï¼‰ç›¸å½“çš„æ€§èƒ½ï¼Œæ•°æ®æ•ˆç‡æå‡5å€ã€‚\n- **æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨é›¶æ ·æœ¬æŠ“å–ä»»åŠ¡ä¸­ï¼Œå¯¹æœªè§ç‰©ä½“ï¼ˆå¦‚å–·é›¾ç“¶ã€äººæ‰‹ï¼‰è¾¾åˆ°85%-93%çš„æˆåŠŸç‡ã€‚\n- **å®æ—¶æ€§**ï¼šæ¨ç†å»¶è¿Ÿä»…13.3msï¼Œé€‚åˆé«˜é¢‘ç‡å“åº”æ“ä½œä»»åŠ¡ã€‚\n\nè¿™äº›æŠ€æœ¯åˆ›æ–°å…±åŒè§£å†³äº†æœºå™¨äººæ“ä½œä¸­çš„è¯­ä¹‰-å‡ ä½•é¸¿æ²Ÿé—®é¢˜ï¼Œå¹¶åœ¨æœ‰é™æ¼”ç¤ºæ•°æ®ä¸‹å®ç°äº†é²æ£’ã€é€šç”¨çš„æ“ä½œèƒ½åŠ›ã€‚",
            "timestamp": "2026-01-20T17:53:52.482044",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä½¿ç”¨äº†**ä¸¤ç±»æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡**ï¼Œåˆ†åˆ«ç”¨äºè¯„ä¼°é«˜å±‚æŠ€èƒ½é€‰æ‹©å’Œåº•å±‚åŠ¨ä½œæ‰§è¡Œï¼Œå¹¶æœ€ç»ˆåˆæˆä¸€ä¸ªç»¼åˆæˆåŠŸç‡ã€‚\n\n| æŒ‡æ ‡åç§° | ç¬¦å·è¡¨ç¤º | å®šä¹‰ä¸è®¡ç®—æ–¹å¼ | è¯„ä¼°ç›®æ ‡ |\n| :--- | :--- | :--- | :--- |\n| **æŠ€èƒ½é€‰æ‹©å‡†ç¡®ç‡** | `Acc_s` | æ¨¡å‹ä¸ºç»™å®šæŒ‡ä»¤é€‰æ‹©æ­£ç¡®æ“ä½œæŠ€èƒ½ï¼ˆå¦‚ä¾§æŠ“ã€æå–ã€æèµ·ç­‰ï¼‰çš„æ¦‚ç‡ã€‚è¿™æ˜¯ä¸€ä¸ª**åˆ†ç±»å‡†ç¡®ç‡**ï¼ŒæˆåŠŸè®°ä¸º1ï¼Œå¤±è´¥è®°ä¸º0ã€‚ | è¯„ä¼°**é«˜å±‚è¯­ä¹‰-å‡ ä½•æ¨ç†**æ¨¡å—ï¼ˆLGSSï¼‰çš„æ€§èƒ½ï¼Œå³èƒ½å¦æ ¹æ®ç‰©ä½“å‡ ä½•å½¢çŠ¶å’Œåœºæ™¯çº¦æŸé€‰æ‹©ç‰©ç†ä¸Šå¯è¡Œçš„æŠ€èƒ½ã€‚ |\n| **ä»»åŠ¡æ‰§è¡ŒæˆåŠŸç‡** | `Acc_t` | æœºå™¨äººæˆåŠŸå®Œæˆä¸ç›®æ ‡ç‰©ä½“çš„ç‰©ç†äº¤äº’ï¼ˆå¦‚ç¨³å®šæŠ“å–ã€å€’æ°´ã€æŠ˜å ç­‰ï¼‰çš„æ¦‚ç‡ã€‚è¿™æ˜¯ä¸€ä¸ª**äºŒå…ƒæˆåŠŸæŒ‡æ ‡**ï¼Œä»…å½“ä»»åŠ¡è¢«å®Œå…¨ã€ç¨³å®šåœ°å®Œæˆæ—¶æ‰è®°ä¸º1ã€‚ | è¯„ä¼°**åº•å±‚åŠ¨ä½œç”Ÿæˆ**ç­–ç•¥ï¼ˆRASNetï¼‰çš„æ€§èƒ½ï¼Œå³ç”Ÿæˆçš„å…³èŠ‚åŠ¨ä½œèƒ½å¦åœ¨ç‰©ç†ä¸–ç•Œä¸­ç²¾ç¡®æ‰§è¡Œã€‚ |\n| **ç»¼åˆæˆåŠŸç‡** | `Acc` | **`Acc = Acc_s Ã— Acc_t`**ã€‚è¯¥æŒ‡æ ‡åæ˜ äº†ä»æŒ‡ä»¤ç†è§£åˆ°ä»»åŠ¡å®Œæˆçš„ç«¯åˆ°ç«¯ç³»ç»Ÿæ€§èƒ½ã€‚ | è¯„ä¼°**æ•´ä¸ªRGMP-Sæ¡†æ¶**çš„æ€»ä½“æ•ˆèƒ½ï¼Œå¼ºè°ƒäº†é«˜å±‚å†³ç­–å’Œåº•å±‚æ§åˆ¶ç¼ºä¸€ä¸å¯ã€‚ |\n\n**æŒ‡æ ‡ç»†èŠ‚ä¸ç‰¹ç‚¹ï¼š**\n- **ä¸¥æ ¼æ€§**ï¼š`Acc_t` çš„å®šä¹‰éå¸¸ä¸¥æ ¼ï¼Œè¦æ±‚ä»»åŠ¡â€œæˆåŠŸå®Œæˆâ€ï¼Œä¾‹å¦‚åœ¨æŠ“å–ä»»åŠ¡ä¸­å¿…é¡»ç¨³å®šæ‹¿èµ·ç‰©ä½“ï¼Œåœ¨å€’æ°´ä»»åŠ¡ä¸­å¿…é¡»æ— æ´’æ¼ã€‚\n- **æ¨¡å—åŒ–è¯„ä¼°**ï¼šé€šè¿‡åˆ†ç¦» `Acc_s` å’Œ `Acc_t`ï¼Œå¯ä»¥ç²¾å‡†å®šä½ç³»ç»Ÿç“¶é¢ˆï¼ˆæ˜¯é€‰é”™äº†æŠ€èƒ½ï¼Œè¿˜æ˜¯æŠ€èƒ½æ‰§è¡Œä¸åˆ°ä½ï¼‰ã€‚\n- **ä»»åŠ¡ç‰¹å¼‚æ€§**ï¼šåœ¨ä¸åŒä»»åŠ¡ä¸­ï¼Œ`Acc_t` çš„æˆåŠŸåˆ¤å®šæœ‰å…·ä½“æ ‡å‡†ï¼ˆè§ä¸‹æ–‡â€œä»»åŠ¡é…ç½®â€éƒ¨åˆ†ï¼‰ã€‚\n\n### äºŒã€ æ•°æ®é›†ä¸ä»»åŠ¡é…ç½®\n\nè®ºæ–‡çš„éªŒè¯ä½“ç³»éå¸¸å…¨é¢ï¼Œæ¶µç›–äº†**æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•**å’Œ**å¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œä»»åŠ¡**ã€‚\n\n#### 1. æ¨¡æ‹ŸåŸºå‡†æ•°æ®é›†ï¼šManiSkill2\n- **æ¥æº**ï¼šå…¬å¼€çš„æ ‡å‡†æœºå™¨äººæ“ä½œæ¨¡æ‹ŸåŸºå‡†ã€‚\n- **ç”¨é€”**ï¼šåœ¨å—æ§ç¯å¢ƒä¸­è¿›è¡Œå¤§è§„æ¨¡ã€å¯é‡å¤çš„ç®—æ³•æ¯”è¾ƒã€‚\n- **é€‰å–ä»»åŠ¡**ï¼šå…±5ä¸ªï¼Œæ¶µç›–ä¸åŒæ“ä½œç±»å‹ï¼š\n    - **OpenCabinetDrawer / OpenCabinetDoor**ï¼šæ“ä½œé“°æ¥ç‰©ä½“ï¼ˆæŠ½å±‰/é—¨ï¼‰ã€‚\n    - **PushChair**ï¼šåŒè‡‚åè°ƒæ¨åŠ¨æ¤…å­ã€‚\n    - **MoveBucket**ï¼šåŒè‡‚åè°ƒç§»åŠ¨å¸¦çƒçš„æ¡¶ã€‚\n    - **PlugCharger**ï¼šé«˜ç²¾åº¦çš„æ’å……ç”µå™¨ä»»åŠ¡ã€‚\n- **æ•°æ®è§„æ¨¡**ï¼šæ¯ä¸ªä»»åŠ¡ä½¿ç”¨**300-1000æ¡**ä¸“å®¶æ¼”ç¤ºè½¨è¿¹è¿›è¡Œè®­ç»ƒã€‚\n- **è¯„ä¼°æ–¹å¼**ï¼šåœ¨**æœªè§è¿‡çš„**ç‰©ä½“å®ä¾‹ï¼ˆå¦‚æ–°æŸœå­ã€æ–°æ¤…å­ï¼‰å’ŒéšæœºåŒ–çš„ç‰©ç†å‚æ•°ï¼ˆæ‘©æ“¦ã€é˜»å°¼ï¼‰ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œè¯„ä¼°æ³›åŒ–èƒ½åŠ›ã€‚\n\n#### 2. çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸ä»»åŠ¡\nä½œè€…æ„å»ºäº†åŒ…å«**500æ¡ä¸“å®¶è½¨è¿¹**çš„æŠ€èƒ½åº“ï¼Œç”¨äºè®­ç»ƒä¸åŒçš„æ“ä½œæŠ€èƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè®¾è®¡äº†5ç±»ç‰©ç†å®éªŒä»»åŠ¡è¿›è¡ŒéªŒè¯ï¼š\n\n| ä»»åŠ¡åç§° | æ ¸å¿ƒæŒ‘æˆ˜ | è®­ç»ƒæ•°æ®é‡ï¼ˆæ¯æŠ€èƒ½ï¼‰ | æˆåŠŸåˆ¤å®šæ ‡å‡† (`Acc_t`=1çš„æ¡ä»¶) | è¯„ä¼°é‡ç‚¹ |\n| :--- | :--- | :--- | :--- | :--- |\n| **äº¤äº’å¼é…’å§æœåŠ¡** | äººæœºå¯¹è¯ã€æ ¹æ®æŒ‡ä»¤é€‰æ‹©å¹¶æ‰§è¡ŒæŠ€èƒ½ï¼ˆé€’é¥®æ–™ã€çº¸å·¾ï¼‰ã€‚ | 40æ¡ä¸“å®¶è½¨è¿¹ | ç¨³å®šæŠ“å–ç›®æ ‡ç‰©ä½“å¹¶æˆåŠŸäº¤ä»˜ã€‚ | **æŠ€èƒ½é€‰æ‹©å‡†ç¡®æ€§** (`Acc_s`)ã€ç³»ç»Ÿæ•´ä½“äº¤äº’æˆåŠŸç‡ã€‚ |\n| **é›¶æ ·æœ¬æŠ“å–** | åœ¨**ä»…ç”¨â€œèŠ¬è¾¾ç½â€æ•°æ®è®­ç»ƒ**åï¼ŒæŠ“å–æœªè§è¿‡çš„ç‰©ä½“ï¼ˆå¯ä¹ç½ã€å–·é›¾ç“¶ã€äººæ‰‹ï¼‰ã€‚ | 40æ¡ï¼ˆä»…èŠ¬è¾¾ç½ï¼‰ | ä»éšæœºä½ç½®ç¨³å®šæŠ“å–å¹¶æ‹¿èµ·ç›®æ ‡ç‰©ä½“ã€‚ | **å‡ ä½•ä¸åŠ¨ä½œè¡¨ç¤ºçš„å¼ºæ³›åŒ–èƒ½åŠ›**ã€é›¶æ ·æœ¬è¿ç§»æ€§èƒ½ã€‚ |\n| **æ¯›å·¾æŠ˜å ** | æ“ä½œé«˜è‡ªç”±åº¦ã€çŠ¶æ€éšæœºçš„å¯å˜å½¢ç‰©ä½“ã€‚ | 100æ¡ä¸“å®¶è½¨è¿¹ | å°†æ¯›å·¾æŠ˜å å¹¶ä¿æŒç¨³å®šï¼Œè¾¹ç¼˜å¯¹é½å®¹å·®åœ¨ **Â±2 cm** å†…ã€‚ | **é•¿æ—¶ç¨‹ä»»åŠ¡è§„åˆ’**ã€å¯¹å¯å˜å½¢ç‰©ä½“çš„ç²¾ç»†æ“ä½œèƒ½åŠ›ã€‚ |\n| **å€’æ°´** | å¤„ç†å¤æ‚æµä½“åŠ¨åŠ›å­¦å’Œéšæœºåˆå§‹çŠ¶æ€ã€‚ | 100æ¡ä¸“å®¶è½¨è¿¹ | å°†æ°´ä»æºå®¹å™¨æ— æ´’æ¼åœ°å€’å…¥ç›®æ ‡å®¹å™¨ã€‚ | **åŠ¨æ€ç‰©è´¨æ“ä½œ**ã€æ‰‹çœ¼åè°ƒä¸è½¨è¿¹ç²¾åº¦ã€‚ |\n| **ç®±å†…å–ç‰©** | åœ¨æ‚ä¹±ã€é®æŒ¡çš„ç¯å¢ƒä¸­è¯†åˆ«å¹¶æŠ“å–ç‰¹å®šç‰©ä½“ã€‚ | 100æ¡ä¸“å®¶è½¨è¿¹ï¼ˆæ¯ç±»ç‰©ä½“ï¼‰ | ä»æ‚ä¹±ç®±å­ä¸­å‡†ç¡®å–å‡ºç›®æ ‡ç‰©ä½“ï¼Œå¹¶ç¨³å®šæ”¾ç½®åˆ°ç›®æ ‡åŒºåŸŸï¼Œæ— æ„å¤–ç¢°æ’ã€‚ | **å¤æ‚åœºæ™¯æ„ŸçŸ¥**ã€æŠ—é®æŒ¡èƒ½åŠ›ã€åœ¨çº¦æŸç©ºé—´ä¸­çš„è¿åŠ¨è§„åˆ’ã€‚ |\n\n**æ•°æ®é›†ä¸å®éªŒè®¾è®¡çš„ç‰¹ç‚¹ï¼š**\n1.  **æ•°æ®é«˜æ•ˆæ€§**ï¼šå¼ºè°ƒä½¿ç”¨**å°‘é‡æ¼”ç¤º**ï¼ˆä½è‡³40æ¡ï¼‰è¿›è¡Œå­¦ä¹ ï¼Œä¸éœ€è¦æµ·é‡æ•°æ®çš„â€œæš´åŠ›ç¼©æ”¾â€æ–¹æ³•å½¢æˆå¯¹æ¯”ã€‚\n2.  **æ³›åŒ–æ€§éªŒè¯**ï¼šé€šè¿‡**é›¶æ ·æœ¬æŠ“å–**å’Œ**æ¨¡æ‹ŸåŸºå‡†ä¸­çš„æœªè§å®ä¾‹**ï¼Œä¸¥æ ¼æµ‹è¯•æ¨¡å‹å¯¹**æ–°ç‰©ä½“ã€æ–°å½¢çŠ¶ã€æ–°åœºæ™¯**çš„é€‚åº”èƒ½åŠ›ã€‚\n3.  **ä»»åŠ¡å¤šæ ·æ€§**ï¼šæ¶µç›–äº†ä»ç®€å•æŠ“å–åˆ°å¤æ‚é•¿æ—¶ç¨‹æ“ä½œï¼ˆæŠ˜å ã€å€’æ°´ï¼‰ã€ä»åˆšæ€§ç‰©ä½“åˆ°å¯å˜å½¢ç‰©ä½“ã€ä»æ¸…æ´ç¯å¢ƒåˆ°æ‚ä¹±åœºæ™¯çš„å…¨æ–¹ä½æµ‹è¯•ã€‚\n4.  **å¹³å°å¤šæ ·æ€§**ï¼šåœ¨**è‡ªå®šä¹‰äººå½¢æœºå™¨äºº**ã€**æ¡Œé¢æœºæ¢°è‡‚**å’Œ**å•†ç”¨Alohaæœºå™¨äºº**ä¸‰ç§å¼‚æ„å¹³å°ä¸ŠéªŒè¯ï¼Œè¯æ˜äº†æ¡†æ¶çš„è·¨å¹³å°é€šç”¨æ€§ã€‚\n\n### æ€»ç»“\nè¯¥è®ºæ–‡æ„å»ºäº†ä¸€å¥—**å±‚æ¬¡æ¸…æ™°ã€å®šä¹‰ä¸¥æ ¼ã€è¦†ç›–å…¨é¢**çš„è¯„ä»·ä½“ç³»ã€‚**ç»¼åˆæˆåŠŸç‡ `Acc`** æ˜¯å…¶æ ¸å¿ƒçš„ç«¯åˆ°ç«¯æ€§èƒ½æŒ‡æ ‡ï¼Œè€Œ**æŠ€èƒ½é€‰æ‹©å‡†ç¡®ç‡ `Acc_s`** å’Œ**ä»»åŠ¡æ‰§è¡ŒæˆåŠŸç‡ `Acc_t`** åˆ™ç”¨äºè¯Šæ–­æ€§åˆ†æã€‚é€šè¿‡ç»“åˆ**å¤§è§„æ¨¡æ¨¡æ‹ŸåŸºå‡†ï¼ˆManiSkill2ï¼‰** å’Œ**ç²¾å¿ƒè®¾è®¡çš„å¤šæ ·åŒ–çœŸå®ä¸–ç•Œä»»åŠ¡**ï¼Œè®ºæ–‡ä»**æ³›åŒ–æ€§ã€æ•°æ®æ•ˆç‡ã€é²æ£’æ€§å’Œä»»åŠ¡å¤æ‚æ€§**ç­‰å¤šä¸ªç»´åº¦ï¼Œæœ‰åŠ›åœ°éªŒè¯äº†RGMP-Sæ¡†æ¶çš„ä¼˜è¶Šæ€§ã€‚",
            "timestamp": "2026-01-20T17:54:38.858866",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ä»¥ä¸‹ä¸¤ç±»æ•°æ®é›†/å¹³å°ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒéªŒè¯ï¼š\n\n#### 1. **ä»¿çœŸåŸºå‡†æµ‹è¯•**\n- **ManiSkill2 ä»¿çœŸåŸºå‡†**ï¼šé€‰å–äº†5ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„é«˜ç²¾åº¦äº¤äº’ä¸ç©ºé—´æ¨ç†ä»»åŠ¡è¿›è¡Œæµ‹è¯•ï¼š\n  - `OpenCabinetDrawer`ï¼šæ‰“å¼€æŠ½å±‰ï¼ˆ25ä¸ªè®­ç»ƒæŸœï¼Œ10ä¸ªæœªè§æµ‹è¯•æŸœï¼‰\n  - `OpenCabinetDoor`ï¼šæ‰“å¼€æŸœé—¨ï¼ˆ42ä¸ªè®­ç»ƒæŸœï¼Œ10ä¸ªæœªè§æµ‹è¯•æŸœï¼‰\n  - `PushChair`ï¼šåŒæ‰‹æ¨åŠ¨è½¬æ¤…ï¼ˆ26æŠŠè®­ç»ƒæ¤…ï¼‰\n  - `MoveBucket`ï¼šåŒæ‰‹æ¬è¿å¸¦çƒçš„æ¡¶ï¼ˆ29ä¸ªè®­ç»ƒæ¡¶ï¼‰\n  - `PlugCharger`ï¼šé«˜ç²¾åº¦æ’å…¥å……ç”µå™¨ï¼ˆ1000æ¡è®­ç»ƒè½¨è¿¹ï¼‰\n\n#### 2. **çœŸå®ä¸–ç•Œæœºå™¨äººå¹³å°ä¸ä»»åŠ¡**\nè®ºæ–‡åœ¨**ä¸‰ä¸ªå¼‚æ„çš„çœŸå®æœºå™¨äººç³»ç»Ÿ**ä¸Šè¿›è¡Œäº†éªŒè¯ï¼š\n- **è‡ªç ”äººå½¢æœºå™¨äºº**ï¼šé…å¤‡å¤´éƒ¨RGBç›¸æœºã€6è‡ªç”±åº¦æœºæ¢°è‡‚ä¸çµå·§æ‰‹ã€‚\n- **æ¡Œé¢åŒè‡‚æ“ä½œæœºå™¨äºº**ï¼šé…å¤‡é¡¶ç½®RGBç›¸æœºã€‚\n- **å•†ç”¨Alohaæœºå™¨äºº**ï¼šé…å¤‡ä¸­å¤®RGBç›¸æœºä¸è…•éƒ¨RGBä¼ æ„Ÿå™¨ã€‚\n\n**çœŸå®ä¸–ç•Œä»»åŠ¡é…ç½®**ï¼ˆå…±5ä¸ªåœºæ™¯ï¼‰ï¼š\n- **äº¤äº’å¼é…’å§æœåŠ¡**ï¼šæ ¹æ®è¯­éŸ³æŒ‡ä»¤é€’é€é¥®æ–™ï¼ˆå¦‚Fantaã€Spriteï¼‰æˆ–çº¸å·¾ã€‚\n- **é›¶æ ·æœ¬æŠ“å–**ï¼šè®­ç»ƒä»…ä½¿ç”¨40æ¡æŠ“å–**Fantaç½**çš„ä¸“å®¶è½¨è¿¹ï¼Œæµ‹è¯•æ—¶æ³›åŒ–åˆ°æœªè§è¿‡çš„ç‰©ä½“ï¼ˆCokeç½ã€å–·é›¾ç“¶ã€äººæ‰‹ï¼‰ã€‚\n- **é•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡**ï¼ˆæ–°å¢äºæœŸåˆŠç‰ˆæœ¬ï¼‰ï¼š\n  - `Towel Folding`ï¼šæŠ˜å æ¯›å·¾ï¼ˆ100æ¡ä¸“å®¶è½¨è¿¹ï¼Œæ¶‰åŠä¸åŒå°ºå¯¸/é¢œè‰²ï¼‰ã€‚\n  - `Water Pouring`ï¼šå€’æ°´ï¼ˆ100æ¡ä¸“å®¶è½¨è¿¹ï¼Œæ¶‰åŠä¸åŒåˆå§‹æ°´ä½/å®¹å™¨ï¼‰ã€‚\n  - `Bin Picking`ï¼šä»æ‚ä¹±ç®±å­ä¸­æ‹¾å–ç‰©ä½“ï¼ˆæ¯ç±»ç‰©ä½“100æ¡ä¸“å®¶è½¨è¿¹ï¼‰ã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨äº†ä¸€å¥—åˆ†å±‚çš„è¯„ä¼°æŒ‡æ ‡æ¥å…¨é¢è¡¡é‡ç³»ç»Ÿæ€§èƒ½ï¼š\n\n1. **ç»¼åˆæˆåŠŸç‡ (`Acc`)**ï¼š\n   ```math\n   Acc = Acc_s Ã— Acc_t\n   ```\n   - **æŠ€èƒ½é€‰æ‹©å‡†ç¡®ç‡ (`Acc_s`)**ï¼šé«˜å±‚æŠ€èƒ½é€‰æ‹©æ¨¡å—ï¼ˆLGSSï¼‰æ˜¯å¦æ­£ç¡®é€‰æ‹©äº†æ“ä½œæŠ€èƒ½ï¼ˆå¦‚ä¾§æ¡ã€æå–ã€æŠ¬èµ·ï¼‰ã€‚\n   - **ä»»åŠ¡æ‰§è¡ŒæˆåŠŸç‡ (`Acc_t`)**ï¼šåº•å±‚åŠ¨ä½œç”Ÿæˆæ¨¡å—ï¼ˆRASNetï¼‰æ˜¯å¦æˆåŠŸå®Œæˆäº†ç‰©ç†äº¤äº’ï¼ˆå¦‚ç¨³å®šæŠ“å–ã€å€’æ°´æˆåŠŸï¼‰ã€‚\n\n2. **æ•°æ®æ•ˆç‡**ï¼šè¡¡é‡æ¨¡å‹åœ¨**æœ‰é™æ¼”ç¤ºæ•°æ®**ä¸‹çš„å­¦ä¹ èƒ½åŠ›ã€‚è®ºæ–‡å¼ºè°ƒå…¶æ–¹æ³•åœ¨**ä»…éœ€å°‘é‡ä¸“å®¶è½¨è¿¹**ï¼ˆå¦‚40æ¡ï¼‰æ—¶å³å¯è¾¾åˆ°é«˜æ€§èƒ½ã€‚\n\n3. **æ¨ç†é€Ÿåº¦ (`f`ï¼Œå•ä½ï¼šHz)**ï¼šè¡¡é‡ç­–ç•¥çš„å®æ—¶æ€§ï¼Œå¯¹äºé«˜é¢‘å“åº”æ“ä½œè‡³å…³é‡è¦ã€‚\n\n4. **é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨**æœªè§è¿‡çš„ç‰©ä½“ç±»åˆ«**å’Œ**ç©ºé—´é…ç½®**ä¸Šçš„æˆåŠŸç‡ã€‚\n\n### ä¸‰ã€ä¸»è¦æ€§èƒ½æå‡ä¸å…³é”®ç»“æœ\nä¸ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒRGMP-Sæ¡†æ¶åœ¨å¤šä¸ªç»´åº¦ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼š\n\n#### 1. **æ•´ä½“æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›**\n- **åœ¨çœŸå®ä¸–ç•Œäº¤äº’ä»»åŠ¡ä¸­**ï¼ŒRGMP-Sç›¸æ¯”**Diffusion Policy (DP) åŸºçº¿**å®ç°äº† **19% çš„ç»¼åˆæ€§èƒ½æå‡**ï¼ˆ`Acc`ï¼‰ã€‚\n- **åœ¨é›¶æ ·æœ¬æŠ“å–ä»»åŠ¡ä¸­**ï¼ŒRGMP-Såœ¨**å¹³å‡æˆåŠŸç‡**ä¸Šæ¯”æœ€ä½³åŸºçº¿ **Dex-VLA é«˜å‡º 12%**ï¼Œåœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„â€œäººæ‰‹äº¤æ¥â€ä»»åŠ¡ä¸Šè¾¾åˆ° **93%** çš„æˆåŠŸç‡ï¼ˆDex-VLAä¸º84%ï¼‰ã€‚\n- **åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­**ï¼ˆå¦‚å€’æ°´ï¼‰ï¼ŒRGMP-Sæ¯”DPåŸºçº¿æå‡çº¦ **16%**ï¼›åœ¨æ¯›å·¾æŠ˜å ä»»åŠ¡ä¸Šè¾¾åˆ° **86%** æˆåŠŸç‡ï¼ˆDex-VLAä¸º68%ï¼‰ã€‚\n\n#### 2. **æ•°æ®æ•ˆç‡**\n- RGMP-Så±•ç°å‡º **5å€** äºDiffusion Policyçš„æ•°æ®æ•ˆç‡ã€‚**ä»…éœ€çº¦40æ¡ä¸“å®¶è½¨è¿¹**å³å¯è¾¾åˆ°DPä½¿ç”¨çº¦200æ¡è½¨è¿¹æ‰èƒ½è¾¾åˆ°çš„ç›¸è¿‘æ€§èƒ½æ°´å¹³ï¼ˆåœ¨FantaæŠ“å–ä»»åŠ¡ä¸Šï¼‰ã€‚\n\n#### 3. **æ¨ç†é€Ÿåº¦**\n- RGMP-Så®ç°äº† **75.2 Hz** çš„é«˜é¢‘å®æ—¶æ¨ç†ï¼Œè¿œè¶…è®¡ç®—å¯†é›†å‹æ¨¡å‹ï¼š\n  - **Diffusion Policyï¼š1.01 Hz**\n  - **OpenVLAï¼š3.6 Hz**\n  è¿™ç¡®ä¿äº†å…¶é€‚ç”¨äºéœ€è¦ä½å»¶è¿Ÿå®æ—¶æ§åˆ¶çš„æœºå™¨äººä»»åŠ¡ã€‚\n\n#### 4. **ä»¿çœŸåŸºå‡†ç»“æœ**\n- åœ¨ManiSkill2åŸºå‡†æµ‹è¯•çš„5ä¸ªä»»åŠ¡ä¸Šï¼ŒRGMP-Sçš„**å¹³å‡å¾—åˆ†**æ˜¾è‘—é«˜äºæ‰€æœ‰å¯¹æ¯”åŸºçº¿ã€‚\n- ä¾‹å¦‚ï¼Œåœ¨`PushChair`ä»»åŠ¡ä¸­ï¼ŒRGMP-Så¾—åˆ† **16.47%**ï¼Œå‡ ä¹æ˜¯å½“æ—¶æœ€ä½³å¤–éƒ¨åŸºçº¿ **ManiSkill2-1st (8.63%)** çš„ä¸¤å€ã€‚\n\n#### 5. **æ¨¡å—æœ‰æ•ˆæ€§éªŒè¯**\n- **LGSSæ¨¡å—**ï¼šåœ¨äº¤äº’é…’å§ä»»åŠ¡ä¸­ï¼Œä¸çº¯Qwen-VLåŸºçº¿ç›¸æ¯”ï¼Œç»“åˆDPåç«¯æ—¶ï¼ŒLGSSå°†å‹æ‰å¯ä¹ç½ä»»åŠ¡çš„ç»¼åˆå‡†ç¡®ç‡ä» **0.40 æå‡è‡³ 0.55**ï¼ˆç›¸å¯¹æå‡15%ï¼‰ã€‚\n- **RASNet + GMMæ¨¡å—**ï¼šåœ¨ç»„ç»‡ä¼ é€’ä»»åŠ¡ä¸­ï¼Œå°†ç»¼åˆå‡†ç¡®ç‡ä»åŸºçº¿çš„0.50æå‡è‡³ **0.60**ã€‚\n\n### æ€»ç»“\nè¯¥è®ºæ–‡é€šè¿‡**ä»¿çœŸä¸å¤šçœŸå®å¹³å°**çš„å¹¿æ³›å®éªŒï¼Œç³»ç»ŸéªŒè¯äº†RGMP-Sæ¡†æ¶çš„**ä¼˜è¶Šæ€§**ã€‚å…¶æ ¸å¿ƒä»·å€¼ä½“ç°åœ¨ï¼š\n- **æŠ€æœ¯åˆ›æ–°**ï¼šé€šè¿‡å‡ ä½•å…ˆéªŒï¼ˆLGSSï¼‰å’Œè„‰å†²æ—¶ç©ºç‰¹å¾å­¦ä¹ ï¼ˆRASNetï¼‰ï¼Œæœ‰æ•ˆå¼¥åˆäº†è¯­ä¹‰-å‡ ä½•é¸¿æ²Ÿã€‚\n- **å®é™…ä»·å€¼**ï¼šåœ¨ä¿æŒ**é«˜å®æ—¶æ€§ï¼ˆ75.2 Hzï¼‰** çš„åŒæ—¶ï¼Œå®ç°äº†**å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›**å’Œ**æé«˜çš„æ•°æ®æ•ˆç‡ï¼ˆ5å€äºåŸºçº¿ï¼‰**ï¼Œä¸ºåœ¨æ•°æ®ç¨€ç¼ºã€ç¯å¢ƒå¤šå˜çš„çœŸå®åœºæ™¯ä¸­éƒ¨ç½²é€šç”¨åŒ–æœºå™¨äººæ“ä½œæä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚",
            "timestamp": "2026-01-20T17:55:21.130437",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„æ·±å…¥åˆ†æï¼Œæœ¬æ–‡æå‡ºçš„RGMP-Sæ¡†æ¶è™½ç„¶åœ¨å¤šä¸ªæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n- **å¯¹æç«¯ç²¾ç»†æ“ä½œåœºæ™¯çš„é€‚åº”æ€§ä¸è¶³**ï¼šå¦‚å›¾12æ‰€ç¤ºï¼Œåœ¨éœ€è¦æ¯«ç±³çº§ç²¾åº¦ï¼ˆå¦‚æ¯›å·¾å®Œå…¨å¹³é“ºæ—¶çš„æŠ“å–ã€å……ç”µå™¨æ’æ‹”å…¬å·®<5mmï¼‰æˆ–å­˜åœ¨ä¸¥é‡è‡ªé®æŒ¡çš„åœºæ™¯ä¸­ï¼Œæ¨¡å‹æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚è¿™æºäºå½“å‰æ¡†æ¶ä¸»è¦ä¾èµ–RGBè§†è§‰è¾“å…¥ï¼Œç¼ºä¹**é«˜åˆ†è¾¨ç‡çš„è§¦è§‰åé¦ˆæˆ–ä¸»åŠ¨è§†è§‰æœºåˆ¶**æ¥è¡¥å¿è§†è§‰ä¿¡æ¯çš„ä¸è¶³ã€‚\n\n- **å¯¹é«˜åº¦åŠ¨æ€å’Œéåˆšæ€§ç‰©ä½“çš„æ³›åŒ–èƒ½åŠ›æœ‰é™**ï¼šè®ºæ–‡çš„å®éªŒä¸»è¦é’ˆå¯¹å½¢çŠ¶ç›¸å¯¹å›ºå®šçš„ç‰©ä½“ï¼ˆå¦‚æ˜“æ‹‰ç½ã€ç“¶å­ï¼‰æˆ–å…·æœ‰å›ºå®šæ“ä½œæ¨¡å¼çš„å˜å½¢ç‰©ä½“ï¼ˆå¦‚æŠ˜å æ¯›å·¾ï¼‰ã€‚å¯¹äº**å½¢çŠ¶å®æ—¶å‰§çƒˆå˜åŒ–ã€æµä½“æˆ–é¢—ç²’ç‰©**çš„æ“çºµï¼Œæ–‡ä¸­æœªè¿›è¡Œå……åˆ†éªŒè¯ã€‚LGSSæ¨¡å—çš„å‡ ä½•å…ˆéªŒå¯èƒ½éš¾ä»¥é€‚åº”æ­¤ç±»æ— é™å¯å˜å½¢çš„å‡ ä½•ç»“æ„ã€‚\n\n- **æŠ€èƒ½åº“çš„é™æ€æ€§ä¸ç»„åˆæ³›åŒ–ç“¶é¢ˆ**ï¼šLGSSä»ä¸€ä¸ª**é¢„å®šä¹‰çš„ã€å‚æ•°åŒ–çš„æŠ€èƒ½åº“**ä¸­é€‰æ‹©æŠ€èƒ½ã€‚è™½ç„¶è¿™æé«˜äº†å†³ç­–æ•ˆç‡ï¼Œä½†ä¹Ÿé™åˆ¶äº†ç³»ç»Ÿå¤„ç†**éœ€è¦æ–°é¢–æŠ€èƒ½ç»„åˆæˆ–æœªè§è¿‡çš„åŸå­åŠ¨ä½œ**çš„é•¿æ—¶ç¨‹ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç³»ç»Ÿçš„æ³›åŒ–åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºæŠ€èƒ½åº“çš„å®Œå¤‡æ€§ã€‚\n\n- **è®¡ç®—æ•ˆç‡ä¸æ¨¡å—åŒ–è®¾è®¡çš„æƒè¡¡**ï¼šRGMP-Sé‡‡ç”¨äº†æ¨¡å—åŒ–è®¾è®¡ï¼ˆLGSS + RASNetï¼‰ï¼Œè™½ç„¶æ•´ä½“æ¨ç†é¢‘ç‡ï¼ˆ75.2 Hzï¼‰å¾ˆé«˜ï¼Œä¼˜äºæ‰©æ•£ç­–ç•¥ç­‰åŸºçº¿ï¼Œä½†**å¤šæ¨¡å—ä¸²è”**ï¼ˆVLM APIè°ƒç”¨ã€åˆ†å‰²ã€æ¨ç†ï¼‰å¯èƒ½å¼•å…¥ç´¯ç§¯å»¶è¿Ÿï¼Œåœ¨éœ€è¦æä½å»¶è¿Ÿï¼ˆå¦‚é«˜é€ŸåŠ¨æ€æŠ“å–ï¼‰çš„åœºæ™¯ä¸­å¯èƒ½æˆä¸ºç“¶é¢ˆã€‚\n\n- **å¯¹ä»¿çœŸä¸çœŸå®ä¸–ç•Œå·®å¼‚çš„é²æ£’æ€§ä¾èµ–ç‰¹å®šè®­ç»ƒ**ï¼šå°½ç®¡åœ¨ManiSkill2åŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªçœŸå®å¹³å°ä¸ŠéªŒè¯äº†æœ‰æ•ˆæ€§ï¼Œä½†æˆåŠŸä¾èµ–äºåœ¨**ç‰¹å®šä»»åŠ¡ä¸Šæ”¶é›†çš„ä¸“å®¶è½¨è¿¹**è¿›è¡Œè®­ç»ƒã€‚å¯¹äºå…¨æ–°çš„ç‰©ä½“æˆ–ä»»åŠ¡ï¼Œä»ç„¶éœ€è¦ä¸€å®šé‡çš„ç¤ºèŒƒæ•°æ®æ¥è®­ç»ƒRASNetç­–ç•¥æˆ–æ›´æ–°æŠ€èƒ½åº“ï¼Œå¹¶éå®Œå…¨çš„â€œé›¶æ ·æœ¬â€é€‚åº”ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œæœªæ¥çš„ç ”ç©¶å·¥ä½œå¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹å‘å±•å¼€ï¼š\n\n- **èåˆå¤šæ¨¡æ€æ„ŸçŸ¥**ï¼šé›†æˆ**é«˜åˆ†è¾¨ç‡è§¦è§‰ä¼ æ„Ÿå™¨**ï¼ˆå¦‚å…‰å­¦è§¦è§‰ä¼ æ„Ÿå™¨ï¼‰å’Œ**æ·±åº¦ç›¸æœº**çš„æ•°æ®ã€‚è§¦è§‰åé¦ˆå¯ä»¥ç›´æ¥æä¾›æ¥è§¦åŠ›ã€æ»‘åŠ¨ä¿¡æ¯ï¼Œç”¨äºä¿®æ­£æŠ“å–ä½å§¿å’ŒåŠ›åº¦ï¼›æ·±åº¦ä¿¡æ¯å¯ä»¥è¡¥å……RGBåœ¨å‡ ä½•ç†è§£ä¸Šçš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨é®æŒ¡å’Œç²¾ç»†å¯¹é½ä»»åŠ¡ä¸­ã€‚å¯ä»¥æ¢ç´¢å¦‚ä½•å°†è§¦è§‰å’Œæ·±åº¦ä¿¡æ¯é«˜æ•ˆåœ°èå…¥LGSSçš„å‡ ä½•æ¨ç†å’ŒRASNetçš„ç‰¹å¾æå–ä¸­ã€‚\n\n- **å‘å±•åŠ¨æ€å‡ ä½•ä¸ç‰©ç†æ¨ç†èƒ½åŠ›**ï¼šå¢å¼ºLGSSæ¨¡å—ï¼Œä½¿å…¶ä¸ä»…èƒ½å¤„ç†é™æ€å½¢çŠ¶å…ˆéªŒï¼Œè¿˜èƒ½è¿›è¡Œ**ç®€å•çš„ç‰©ç†åŠ¨åŠ›å­¦é¢„æµ‹**ï¼ˆå¦‚æ¶²ä½“å€¾å€’è½¨è¿¹ã€å¯å˜å½¢ç‰©ä½“çš„åŠ›å­¦ç‰¹æ€§ï¼‰ã€‚è¿™å¯ä»¥é€šè¿‡å°†ç‰©ç†ä»¿çœŸå¼•æ“ä½œä¸ºæ¨ç†å·¥å…·ï¼Œæˆ–è®­ç»ƒè½»é‡çº§çš„ç‰©ç†ä¸–ç•Œæ¨¡å‹æ¥å®ç°ï¼Œä½¿æœºå™¨äººèƒ½é¢„æµ‹åŠ¨ä½œåæœå¹¶æå‰è§„åˆ’ã€‚\n\n- **å®ç°æŠ€èƒ½çš„è‡ªè¿›åŒ–ä¸ç»„åˆå­¦ä¹ **ï¼šå°†é™æ€æŠ€èƒ½åº“æ‰©å±•ä¸º**å¯åŠ¨æ€æ‰©å±•çš„æŠ€èƒ½å›¾æˆ–æŠ€èƒ½è¯­æ³•**ã€‚ç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§„åˆ’èƒ½åŠ›å’Œå¼ºåŒ–å­¦ä¹ ï¼Œè®©æœºå™¨äººèƒ½å¤Ÿ**è‡ªä¸»åˆæˆæ–°çš„æŠ€èƒ½åŸè¯­**ï¼Œæˆ–å°†å·²æœ‰æŠ€èƒ½ä»¥æ–°çš„åºåˆ—å’Œå‚æ•°ç»„åˆèµ·æ¥ï¼Œè§£å†³æ›´å¤æ‚çš„ä»»åŠ¡ã€‚è¿™æœ‰åŠ©äºçªç ´é¢„å®šä¹‰æŠ€èƒ½åº“çš„è¾¹ç•Œã€‚\n\n- **ä¼˜åŒ–æ¶æ„ä»¥å®ç°ç«¯åˆ°ç«¯é«˜æ•ˆæ¨ç†**ï¼šç ”ç©¶å¦‚ä½•å°†LGSSçš„å‡ ä½•æ¨ç†èƒ½åŠ›æ›´ç´§å¯†åœ°é›†æˆåˆ°ç­–ç•¥ç½‘ç»œä¸­ï¼Œå‡å°‘æ¨¡å—é—´çš„é€šä¿¡å¼€é”€ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æ¢ç´¢**è½»é‡çº§ã€å¯å¾®çš„å‡ ä½•æ¨ç†æ¨¡å—**ï¼Œä½¿å…¶èƒ½ä¸RASNetä¸€èµ·è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œåœ¨ä¿æŒå‡ ä½•æ„ŸçŸ¥ä¼˜åŠ¿çš„åŒæ—¶ï¼Œè¿›ä¸€æ­¥é™ä½å»¶è¿Ÿå¹¶æå‡æ•´ä½“ä¼˜åŒ–æ•ˆç‡ã€‚\n\n- **æ¨è¿›æ— ç›‘ç£ä¸è‡ªç›‘ç£æŠ€èƒ½è·å–**ï¼šå‡å°‘å¯¹å¤§é‡ä»»åŠ¡ç‰¹å®šä¸“å®¶ç¤ºèŒƒçš„ä¾èµ–ã€‚æœªæ¥å¯ä»¥æ¢ç´¢**ä»ç½‘ç»œè§†é¢‘ã€æœºå™¨äººè‡ªä¸»äº¤äº’æ¢ç´¢**ç­‰éç»“æ„åŒ–æ•°æ®ä¸­ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ æå–é€šç”¨çš„æ“çºµæ¦‚å¿µå’ŒæŠ€èƒ½ã€‚ç»“åˆä¸–ç•Œæ¨¡å‹å’Œå› æœå‘ç°æ–¹æ³•ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿä¸»åŠ¨æ¢ç´¢ç¯å¢ƒå¹¶å‘ç°æœ‰æ•ˆçš„äº¤äº’ç­–ç•¥ï¼Œå®ç°æ›´é«˜å±‚æ¬¡çš„æ•°æ®æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›ã€‚\n\n**æ€»ç»“**ï¼šRGMP-Sæ¡†æ¶åœ¨åˆ©ç”¨å‡ ä½•å…ˆéªŒå’Œè„‰å†²æ—¶ç©ºç‰¹å¾å­¦ä¹ æ–¹é¢åšå‡ºäº†é‡è¦åˆ›æ–°ï¼Œä¸ºæ ·æœ¬é«˜æ•ˆã€å¯æ³›åŒ–çš„äººå½¢æœºå™¨äººæ“çºµæä¾›äº†å¼ºæœ‰åŠ›çš„åŸºçº¿ã€‚æœªæ¥çš„å·¥ä½œåº”è‡´åŠ›äºçªç ´å…¶åœ¨**æ„ŸçŸ¥æ¨¡æ€ã€åŠ¨æ€ç‰©ç†ç†è§£ã€æŠ€èƒ½ç»„åˆåˆ›é€ æ€§ä»¥åŠå­¦ä¹ èŒƒå¼**æ–¹é¢çš„é™åˆ¶ï¼Œå‘æ›´é€šç”¨ã€æ›´è‡ªä¸»çš„ embodied AI è¿ˆè¿›ã€‚",
            "timestamp": "2026-01-20T17:55:55.305376",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-13T23:36:30Z",
    "created_at": "2026-01-20T17:49:47.356417",
    "updated_at": "2026-01-20T17:49:47.356424",
    "recommend": 0
}