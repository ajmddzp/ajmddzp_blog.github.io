{
    "id": "2601.07558v1",
    "title": "FlyCo: Foundation Model-Empowered Drones for Autonomous 3D Structure Scanning in Open-World Environments",
    "authors": [
        "Chen Feng",
        "Guiyong Zheng",
        "Tengkai Zhuang",
        "Yongqian Wu",
        "Fangzhan He",
        "Haojia Li",
        "Juepeng Zheng",
        "Shaojie Shen",
        "Boyu Zhou"
    ],
    "abstract": "å°½ç®¡åº”ç”¨å¹¿æ³›ï¼Œæ— äººæœºå¯¹å¼€æ”¾ä¸–ç•Œç›®æ ‡ç»“æ„è¿›è¡Œè‡ªä¸»ä¸‰ç»´æ‰«æä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰èŒƒå¼ä¾èµ–é™åˆ¶æ€§å‡è®¾æˆ–è´¹æ—¶çš„äººå·¥å…ˆéªŒçŸ¥è¯†ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€æ•ˆç‡å’Œé€‚åº”æ€§ã€‚è¿‘æœŸæ¶Œç°çš„åŸºç¡€æ¨¡å‹ä¸ºå¼¥åˆè¿™ä¸€å·®è·æä¾›äº†å·¨å¤§æ½œåŠ›ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šä½•ç§ç³»ç»Ÿæ¶æ„èƒ½æœ‰æ•ˆæ•´åˆåŸºç¡€æ¨¡å‹çŸ¥è¯†ä»¥å®Œæˆæ­¤ä»»åŠ¡ï¼Ÿæˆ‘ä»¬æå‡ºFlyCoä½œä¸ºè§£ç­”â€”â€”è¿™æ˜¯ä¸€ä¸ªåŸºäºåŸºç¡€æ¨¡å‹æ„å»ºçš„æ„ŸçŸ¥-é¢„æµ‹-è§„åˆ’å¾ªç¯æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šæ ·æœªçŸ¥å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å®ç°å®Œå…¨è‡ªä¸»ã€åŸºäºæç¤ºé©±åŠ¨çš„ä¸‰ç»´ç›®æ ‡æ‰«æã€‚FlyCoé€šè¿‡ä¸‰ä¸ªååŒé˜¶æ®µï¼Œå°†ä½äººå·¥æˆæœ¬çš„æç¤ºï¼ˆæ–‡æœ¬ã€è§†è§‰æ ‡æ³¨ï¼‰ç›´æ¥è½¬åŒ–ä¸ºç²¾ç¡®çš„è‡ªé€‚åº”æ‰«æèˆªçº¿ï¼šï¼ˆ1ï¼‰æ„ŸçŸ¥é˜¶æ®µèåˆå®æ—¶ä¼ æ„Ÿå™¨æ•°æ®ä¸è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œå®ç°é²æ£’çš„ç›®æ ‡å®šä½ä¸è·Ÿè¸ªï¼›ï¼ˆ2ï¼‰é¢„æµ‹é˜¶æ®µæç‚¼åŸºç¡€æ¨¡å‹çŸ¥è¯†å¹¶ç»“åˆå¤šæ¨¡æ€çº¿ç´¢ï¼Œæ¨æ–­éƒ¨åˆ†è§‚æµ‹ç›®æ ‡çš„å®Œæ•´å‡ ä½•ç»“æ„ï¼›ï¼ˆ3ï¼‰è§„åˆ’é˜¶æ®µåˆ©ç”¨é¢„æµ‹å‰ç»æ€§ç”Ÿæˆé«˜æ•ˆå®‰å…¨çš„é£è¡Œè·¯å¾„ï¼Œç¡®ä¿å¯¹ç›®æ ‡çš„å…¨é¢è¦†ç›–ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡å…³é”®ç»„ä»¶ä»¥æå‡å¼€æ”¾ä¸–ç•Œç›®æ ‡å®šä½çš„æ•ˆç‡å’Œé²æ£’æ€§ï¼Œåœ¨å½¢çŠ¶ç²¾åº¦ã€é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å’Œæ—¶åºç¨³å®šæ€§æ–¹é¢å¢å¼ºé¢„æµ‹è´¨é‡ï¼Œå¹¶å¹³è¡¡é•¿æ—¶ç¨‹é£è¡Œæ•ˆç‡ä¸å®æ—¶è®¡ç®—èƒ½åŠ›åŠåœ¨çº¿é¿éšœéœ€æ±‚ã€‚å¤§é‡å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œä¸ä»¿çœŸå®éªŒè¡¨æ˜ï¼ŒFlyCoèƒ½å®ç°ç²¾ç¡®åœºæ™¯ç†è§£ã€é«˜æ•ˆè¿è¡Œå’Œå®æ—¶å®‰å…¨ä¿éšœï¼Œåœ¨é™ä½äººå·¥æˆæœ¬çš„åŒæ—¶è¶…è¶Šç°æœ‰èŒƒå¼ï¼ŒéªŒè¯äº†æ‰€ææ¶æ„çš„å®ç”¨æ€§ã€‚ç³»ç»Ÿæ€§æ¶ˆèå®éªŒè¯å®äº†å„ç»„ä»¶çš„è´¡çŒ®ã€‚FlyCoè¿˜å¯ä½œä¸ºçµæ´»å¯æ‰©å±•çš„è“å›¾ï¼Œè½»æ¾èåˆæœªæ¥åŸºç¡€æ¨¡å‹ä¸æœºå™¨äººæŠ€æœ¯è¿›å±•ã€‚ä»£ç å³å°†å¼€æºã€‚",
    "url": "https://arxiv.org/abs/2601.07558v1",
    "html_url": "https://arxiv.org/html/2601.07558v1",
    "html_content": "\\corrauth\nBoyu Zhou, Southern University of Science and Technology,\nShenzhen, China.\nFlyCo: Foundation Model-Empowered Drones for Autonomous 3D Structure Scanning in Open-World Environments\nChen Feng\n1\n1\naffiliationmark:\nGuiyong Zheng\n2\n2\naffiliationmark:\nTengkai Zhuang\n3\n3\naffiliationmark:\nYongqian Wu\n3\n3\naffiliationmark:\nFangzhan He\n3\n3\naffiliationmark:\nHaojia Li\n1\n1\naffiliationmark:\nJuepeng Zheng\n2\n2\naffiliationmark:\nShaojie Shen\n1\n1\naffiliationmark:\nand Boyu Zhou\n3,4\n3,4\naffiliationmark:\n1\n1\naffiliationmark:\nDepartment of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China\n2\n2\naffiliationmark:\nSchool of Artificial Intelligence, Sun Yat-sen University, Zhuhai, China\n3\n3\naffiliationmark:\nDepartment of Mechanical and Energy Engineering, Southern University of Science and Technology, Shenzhen, China\n4\n4\naffiliationmark:\nDifferential Robotics, Hangzhou, China\nChen Feng and Guiyong Zheng contributed equally to this work.\nzhouby@sustech.edu.cn\nAbstract\nAutonomous 3D scanning of open-world target structures via drones remains challenging despite broad applications.\nExisting paradigms rely on restrictive assumptions or high-effort human priors, limiting practicality, efficiency, and adaptability.\nRecent foundation models (FMs) offer great potential to bridge this gap.\nThus, this paper investigates a critical research problem:\nWhat system architecture can effectively integrate FM knowledge for open-world target structure scanning?\nWe answer it with\nF\nly\nC\no, a principled FM-empowered perception-prediction-planning loop enabling fully autonomous, prompt-driven 3D target scanning in diverse unknown open-world environments.\nF\nly\nC\no directly translates low-effort human prompts (text, visual annotations) into precise adaptive scanning flights via three coordinated stages: (1) perception fuses streaming sensor data with vision-language FMs for robust target structure grounding and tracking; (2) prediction distills FM knowledge and combines multi-modal cues to infer the partially observed targetâ€™s complete geometry; (3) planning leverages predictive foresight to generate efficient and safe paths with comprehensive target coverage.\nBuilding on this, we further design key components to boost open-world target grounding efficiency and robustness, enhance prediction quality in terms of shape accuracy, zero-shot generalization, and temporal stability, and balance long-horizon flight efficiency with real-time computability and online collision avoidance.\nExtensive challenging real-world and simulation experiments show\nF\nly\nC\no delivers precise scene understanding, high efficiency, and real-time safety, outperforming existing paradigms with lower human effort and verifying the proposed architectureâ€™s practicality.\nComprehensive ablations validate each componentâ€™s contribution.\nF\nly\nC\no also serves as a flexible, extensible blueprint, readily leveraging future FMs and robotics advances.\nCode and development environments will be released.\nkeywords:\nAerial Systems: Perception and Autonomy, Aerial Systems: Applications, Integrated Planning and Learning, Autonomous Agents\n1\nIntroduction\nAutonomous, accurate 3D scanning of human-intended open-world structures with aerial robots is a key enabler for infrastructure inspection (\nBeyondVision\n2023\n), disaster response (\nEsri\n2023\n), and heritage documentation (\nMapalytics\n2024\n).\nHowever, achieving efficient scanning in diverse, unknown open-world environments remains challenging: the drone must precisely localize the user-specified structure amid clutter and execute compact, collision-free coverage without wasting time on irrelevant regions.\nFigure 1:\nTeaser.\n(A) Overview of the proposed FM-empowered aerial system to autonomously scan user-specified structures.\nIt seamlessly integrates physical world understanding and knowledge built into FMs with advanced autonomous flight capabilities, translating low-effort human prompts (text descriptions, visual annotations) into precise drone actions for efficient target scanning in unknown open-world environments.\nThis is achieved via a coordinated perception-prediction-planning loop, which operates entirely without human intervention or prior environmental knowledge.\n(B-C) Existing paradigms typically rely on effortful human priors and involvement, and tend to trigger re-flights for incomplete scans, both of which limit practicality, efficiency, and adaptability in open-world scenarios.\nHuman pilots handle this reliably.\nGiven an instruction such as â€œ\nscan the castle in the valley\n,â€ they can quickly identify the intended structure, infer its full layout from partial views for guiding motion, and coordinate sensing and motion to efficiently acquire dense coverage while avoiding obstacles in real-time.\nThis stems from rich world knowledge about 3D spaces and the seamless integration with flight skills.\nIn contrast, existing drones lack such generalizable capabilities and still rely heavily on expert operation or high-effort human priors to balance efficiency, information completeness, and safety across new scanning tasks.\nEarly solutions (\nDJI\n2024\n; Skydio\n2025\n) rely on pre-defined flight patterns (\ne.g.\n, lawnmower), performing limited adaptability to complex geometries and requiring user-configured parameters.\nSubsequent works (\nRoberts\net al.\n2017\n; Hepp\net al.\n2018\n; Smith\net al.\n2018\n) improve coverage quality via two-stage pipelines, combining coarse scans with offline fine-grained planning over human-processed geometry (Fig.\n1\nB).\nOther approaches (\nJing\net al.\n2016\n; Zhou\net al.\n2020\n) replace coarse scans with external priors like satellite imagery or pre-existing 3D models.\nWhile better handling complex structures, they still depend on human setup or prior data and fail to handle unexpected obstacles.\nExploration-based methods (\nSong and Jo\n2017\n; Bircher\net al.\n2018\n; Luo\net al.\n2024\n) further reduce human involvement by treating target scanning as online exploration (Fig.\n1\nC), but typically operate within user-specified 3D bounding boxes, making performance highly sensitive to box accuracy (overly large boxes waste effort on irrelevant regions, whereas tight boxes may miss target parts).\nMoreover, without global scene context, these systems react locally to current observations, generating inefficient paths.\nIn practice, both paradigms frequently necessitate post-flight human evaluations and repeated re-flights, undermining operational efficiency and flexibility.\nRecently, several works (\nFeng\net al.\n2023\n; GuÃ©don\net al.\n2023\n; Chen\net al.\n2024\n) incorporate reasoning about unobserved target geometry to provide global context.\nDespite their promise, these methods are evaluated primarily in small-scale simulations or simplified environments with no target-irrelevant obstacles and restricted structure diversity, leaving their zero-shot generalization to real-world scenes and practical applicability unclear.\nCollectively, existing paradigms fall short of the foundational understanding and world knowledge of human pilots, relying on manual setup, strong human priors, or idealized assumptions that hinder low-effort, efficient operations that are robust and support strong generalization in the open world.\nThe core bottleneck of existing aerial systemsâ€”\nlack of scene understanding and world knowledge\nâ€”is precisely where recent foundation models (FMs) offer the potential to reduce reliance on high-effort human priors.\nTheir success in vision-language alignment and visual generation suggests a compelling avenue towards human-like spatial understanding and imagination for drones.\nAlthough FMs have already advanced ground-robot navigation and manipulation, naively mirroring them fails to address the unique challenges of open-world 3D aerial scanning.\nFirst\n, most prior works integrate FMs into robotic systems via end-to-end (e2e) learning, requiring massive expert demonstrations.\nFor open-world aerial scanning, where the drone must simultaneously balance coverage, efficiency, and safety across highly diverse scenarios, collecting massive expert flight data is impractical.\nAdditionally, FMâ€™s high inference latency conflicts with the real-time response needed for smooth, efficient flight.\nSecond\n, FMs are primarily designed for language and 2D vision.\nDistilling their knowledge into 3D spatial domains to enable drones to recognize complex targets amid clutter and predict their unseen parts remains underexplored.\nConcretely, target recognition must handle streaming sensor data and stay robust to viewpoint shifts in long-sequence flights, while off-the-shelf FMs are memory-intensive and lack stability.\nFor reliable planning guidance, current prediction methods neither generalize well to novel structures in physical-scale nor infer plausible shapes from partial observations while strictly aligning with them; outputs unfaithful to observed parts risk geometric hallucinations, mismatches, and reduced scan efficiency (\nYan\net al.\n2025\n; Sec.\nPartial-surface regularization.\n).\nThese gaps go far beyond trivially fine-tuning FMs using 3D data and call for careful designs.\nThird\n, FM-driven robot motion strategies typically rely on short-horizon reasoning, which misaligns with 3D scanningâ€™s demand for efficient global coverage planning.\nThey also neglect temporal consistency with historical paths, risking trajectory inconsistencies and reduced efficiency.\nMoreover, global planning incurs heavy computational overhead, which leads to delayed planning updates that direct drones to target outdated surfaces while impairing real-time obstacle response.\nTogether, these gaps raise an unanswered system-level question:\nWhat architecture can effectively integrate foundation-model knowledge with flight skills to truly achieve performant, practical aerial target structure scanning across diverse open-world environments?\nIn this work, we address this question with\nF\nly\nC\no (Fig.\n1\nA and Extension 1), a holistic FM-powered aerial system enabling precise, efficient, and safe scanning of diverse complex user-specified 3D structures in unknown open-world environments.\nDriven solely by abstract human prompts (\ne.g.\n, textual descriptions, visual annotations), it eliminates the need for effortful priors or interventions (\ne.g.\n, 3D bounding boxes, human involvement) commonly required in existing paradigms.\nTo tackle the first challenge, we adopt a modular architecture inspired by expert pilotsâ€™ cognitive processes for scanning tasks, comprising prompt-grounded scene understanding, spatial structural prediction, and efficient, safe flight planning.\nOur design leverages FMsâ€™ strengths in perception and prediction while assigning long-horizon planning and real-time response, areas where FMs are deficient, to optimization-based methods.\nThis avoids the expert-data hunger of the e2e fashion while retaining the scene knowledge of FMs.\nAsynchronous operation of the three modules eliminates mutual blocking in e2e frameworks.\nNotably, the planning module harvests rich information from low-frequency FM inference outputs to conduct continual planning, reconciling the latency mismatch between slow FM reasoning and real-time flight needs.\nThe predictive capability enables global planning, overcoming the local myopia of existing methods.\nF\nly\nC\noâ€™s flexible perception-prediction-planning loop and modular interfaces further enable future performance upgrades via direct module replacement.\nFor the second challenge, we carefully design two modules: prompt-grounded scene perception and multi-modal surface prediction.\nLeveraging the generalizable understanding of FMs, perception aligns user prompts with streaming sensor data to segment target structure in 3D clutter, while our FM-tailored streaming-efficient adaptation and cross-modal refinement boost efficiency and ensure robustness across viewpoints and time.\nFor prediction, we introduce a multi-modal network that completes the targetâ€™s 3D surface from partial views.\nThis design fuses complementary advantages from multiple sources, where partial geometric cues anchor outputs to physical measurements, while visual and textual cues that integrate distilled FM knowledge ensure generalization, enabling reliable inference of unseen parts.\nArchitecturally, we adhere to a minimal-inductive-bias principle with a post-fusion scheme, allowing the model to learn rich structural priors from data while effectively absorbing FM knowledge.\nBeyond the network, we adopt complementary designs to address key challenges.\nSpecifically, a FM-based automatic data generation tool synthesizes aligned multi-modal data for large-scale annotation-free learning, boosting generalization across diverse structures and scales.\nAt inference, an adaptive densification strategy mitigates the inflexibility of fixed network outputs for structures with different scales, ensuring appropriate granularity for varying-sized targets without added model complexity.\nAdditionally, a lightweight partial-surface regularization during training explicitly encourages consistency with measured geometry, avoiding discrepancies between predicted and observed surfaces and enhancing temporal stability.\nTogether, these designs yield globally plausible and locally faithful predicted structures, converting uncertain unobserved regions into a clear global structural context to enable proactive goal-directed planning.\nFigure 2:\nIn-the-wild demonstrations across four scenarios.\nThe user-specified targets include: (A) an arch bridge linking buildings, (B) a large concert hall, (C) a castle gate, and (D) a red-brick building.\nBuilding on the prediction capability, we propose a prediction-aware hierarchical planner, reducing redundant flight caused by short-horizon myopia.\nTo ensure smooth path adjustments across historical and current paths, we present the injection of consistency awareness into global planning, improving long-term planning coherence and efficiency.\nMoreover, aiming at the high computational overhead of NP-hard coverage planning in physical-scale scenarios, we adopt two strategies: we employ inlier-driven intersection parity checking and gravitational-like viewpoint pruning to efficiently generate high-quality coverage viewpoints that keep pace with the latest prediction; we also adopt a two-level decomposition strategy to split the original problem into smaller solvable sub-problems, achieving the real-time solution of high-quality coverage paths.\nFinally, at the local level, guided by the global coverage path, we implement viewpoint-constrained trajectory optimization for high-frequency local planning, adapting to real-time safety requirements in the presence of emerging obstacles while preserving structural information completeness.\nOverall,\nF\nly\nC\no leverages FMs to understand complex structures from simple text/visual prompts and seamlessly integrate scene knowledge with flight skills, enabling low-effort, efficient, reliable 3D scanning.\nWe evaluate the full\nF\nly\nC\no system on a customized aerial platform via diverse challenging field experiments at unmapped wild sites, covering key challenges: precise scene understanding, robust prediction in large-scale scenes, and efficient safe scanning in cluttered environments (Fig.\n2\n).\nField trials confirm\nF\nly\nC\no reliably scans user-specified structures from simple prompts with efficient, robust performance.\nBenchmarked against state-of-the-art (SOTA) methods from existing paradigms in complex 3D simulated scenarios,\nF\nly\nC\no consistently outperforms competitors in terms of lighter human input,\n1.25\n1.25\n-\n3\nÃ—\n3\\times\nreduced flight time,\n4.3\n4.3\n-\n56.2\n%\n56.2\\%\nhigher target coverage, and\n11.6\n11.6\n-\n50.1\n%\n50.1\\%\nhigher success rate.\nWe complement these results with an in-depth analysis of\nF\nly\nC\noâ€™s advantages and practical viability relative to existing approaches.\nAblation studies further validate the importance of each module, confirming our design choices.\nIn summary, our main contributions are as follows:\n1)\nA principled and flexible FM-empowered aerial system architecture for open-world 3D targeted structure scanning.\nF\nly\nC\no seamlessly integrates generalizable FMs with advanced flight skills via a closed-loop perception-prediction-planning framework, enabling prompt-driven efficient scanning of complex structures in diverse, unknown open-world environments.\n2)\nA prompt-grounded perception module enabling practical FM-based 3D scene understanding for aerial scanning, in which FM-tailored streaming-efficient adaptation and cross-modal refinement boost efficiency and robustness across viewpoints and time.\n3)\nA novel multi-modal surface prediction pipeline harnessing FM world knowledge, fusing partial geometric and visual/textual cues for metrically grounded shape completion.\nIt features four key designs: a minimal-bias, post-fusion network powered by FMs, annotation-free data synthesis for large-scale learning, adaptive densification, and partial-surface regularization, achieving globally plausible, locally faithful results with strong zero-shot generalization and temporal stability.\n4)\nA prediction-aware hierarchical planner for efficient, safe long-horizon planning.\nIt incorporates consistency awareness injection, efficient viewpoint generation and two-level decomposition for real-time and consistent coverage planning; the local layer adopts viewpoint-constrained optimization to ensure safety amid unseen obstacles while preserving information completeness.\n5)\nExtensive validation of\nF\nly\nC\no via real-world in-the-wild flights and large-scale simulation benchmarks against SOTA methods, complemented by analyses and ablations, collectively demonstrating the systemâ€™s superior performance and architectural validity.\nSource code and development environments will be publicly available.\n2\nRelated Work\n2.1\nClassic 3D Aerial Scanning\nEarly work on aerial 3D scanning in practice and in the research literature largely followed pre-defined flight patterns, typically implemented as commercial tools for mapping and inspection (\nDJI\n2024\n,\n2025b\n; Litchi\n2025\n; PIX4D\n2024\n; Skydio\n2025\n).\nThese systems generate simple, parameterized trajectories such as lawnmower, orbit, and grid patterns once the user has drawn a polygon or box around the area to be scanned and chosen parameters like altitude, overlap, and speed.\nBecause the paths are planned without explicit consideration of the underlying 3D geometry or real-time perception feedback, they cannot adapt to cluttered surroundings and often require manual adjustment by a skilled operator.\nIn addition, the high-altitude, geometry-agnostic nature of these trajectories typically limits the level of structural detail and leads to incomplete coverage, especially for near-ground or recessed features that fall outside the effective viewing geometry.\nTo better account for structural complexity, subsequent methods further began to integrate target geometry into the planning process via two-stage pipelines (\nRoberts\net al.\n2017\n; Hepp\net al.\n2018\n; Smith\net al.\n2018\n; Peng and Isler\n2019\n; Feng\net al.\n2024\n).\nRepresentative two-stage strategies first execute a coarse data-collection flight, then manually reconstruct the 3D model of the target from the captured data, and perform offline optimization to generate a dense inspection path, for example via submodular viewpoint selection for close-range coverage (\nRoberts\net al.\n2017\n).\nBuilding on this idea,\nHepp\net al.\n2018\nand\nSmith\net al.\n2018\ndecoupled viewpoint generation from trajectory optimization and explicitly optimized sensor orientations to increase visibility, while\nPeng and Isler\n2019\nintroduced adaptive plane-wise grid sampling to reduce the viewpoint search from 3D to 2D.\nMethods like (\nFeng\net al.\n2024\n) further improved trajectory efficiency through topological guidance, while others (\nJing\net al.\n2016\n; Zhou\net al.\n2020\n; Zhang\net al.\n2021\n) leveraged prior information such as satellite imagery, CAD/BIM models, or pre-existing reconstructions to replace the initial data-collection stage for reducing flight time.\nAlthough these techniques improve scanning efficiency and achieve high-quality coverage, they remain tightly coupled to heavy human effort and scene-specific prior knowledge, and are typically executed in an offline fashion.\nAs a result, they offer limited real-time adaptability to unexpected obstacles and do not satisfy the level of autonomy required for open-world 3D target structure scanning.\n2.2\nExploration-based 3D Aerial Scanning\nTo reduce manual setup and improve adaptability in unknown environments, a class of online autonomous exploration strategies (\nSong\net al.\n2021\n; Bircher\net al.\n2018\n; Song\net al.\n2021\n; Luo\net al.\n2024\n) emerged.\nThey typically required users to specify a task range delineated by a 3D bounding box, under the premise that this volume fully encloses the target structure.\nSubsequently, drones then survey all unknown space within this volume using frontier-based (\nYamauchi\n1997\n) or next-best-view (NBV) (\nConnolly\n1985\n) algorithms.\nSong and Jo\n2017\nalternated between NBV selection and local scanning path planning, incrementally refining 3D coverage within the given bounded workspace.\nBircher\net al.\n2018\nused a receding-horizon NBV strategy that chooses viewpoints in a Rapidly-exploring Random Tree (RRT) over free space and selects paths maximizing surface information gain for 3D exploration and inspection.\nOn the other hand,\nSong\net al.\n2021\nextracted frontiers between current known and unknown space, then sampled viewpoints to observe these frontiers, and planned minimum-distance paths that visit the viewpoints sequentially.\nMore recently, Star-Searcher (\nLuo\net al.\n2024\n) further improved efficiency by introducing a LiDAR-camera cooperation workflow.\nIt adopted a frontier-based strategy to autonomously survey all occupied space within the given box using LiDAR and simultaneously generated the scanning trajectories for the camera in real-time to completely and detailly inspect all explored surfaces.\nBy replacing time-intensive coarse-to-fine workflows with single-pass missions, these methods reduce human effort on providing priors and enable real-time obstacle avoidance.\nHowever, this paradigm suffered from fundamental flaws.\nUser-specified bounding boxes are often imprecise, leading to excessive scanning of non-target areas or missing parts of the intended structure.\nConsequently, performance becomes highly sensitive to the quality of human inputs, which undermines practicality in open-world scenarios.\nBesides, frontier and NBV strategies typically make locally optimal decisions with respect to current volumetric or surface information gain, rather than global coverage of a specific structure.\nSuch local decision-making yields globally inefficient and temporally inconsistent trajectories, increasing overall mission time or causing revisits to already well-observed regions.\n2.3\nPrediction Involvement in Aerial Scanning\nBeyond pure exploration, a line of work (\nFeng\net al.\n2023\n; GuÃ©don\net al.\n2023\n; Chen\net al.\n2024\n; Liu\net al.\n2024\n) has recently started to bring predictive capabilities into aerial 3D scanning to provide global context guidance for pursuing higher efficiency, either by explicitly inferring unseen geometry or by learning to anticipate informative views.\nOn the geometric side, several methods (\ne.g.\n,\nFeng\net al.\n2023\n) trained specialized neural networks bridging mapping and planning.\nThey completed the target structure point cloud from partial volumetric maps and then ran coverage planning on the predicted model rather than on the raw, partially observed information.\nThis allows the planner to reason about occluded or yet-unseen parts to synthesize more structured paths, compared to purely local frontier or NBV strategies.\nIn parallel, the 3D vision community has made rapid progress on point cloud completion and generative 3D modeling, which similarly infer reasonable full shapes from partial observations (\nYuan\net al.\n2018\n; Aiello\net al.\n2022\n; Rong\net al.\n2024\n; Tochilkin\net al.\n2024\n; Li\net al.\n2025\n).\nWhile these models have not yet been deployed on real robotic platforms, they suggest a promising source for future aerial scanning systems.\nOn the view-selection side, learning-based approaches (\nGuÃ©don\net al.\n2023\n; Chen\net al.\n2024\n; Liu\net al.\n2024\n) replace hand-crafted information gain heuristics with policies or scoring networks trained to rank candidate viewpoints, for example, by reinforcement learning or self-supervision.\nThese policies implicitly encode priors about what shapes and configurations are likely and which viewpoints tend to maximize structural coverage in the next step.\nNevertheless, these methods are studied under highly simplified conditions, where the scene typically contains only the target structure, with no target-irrelevant obstacles or cluttered surroundings.\nEvaluation is conducted on limited synthetic benchmark datasets rather than through sustained deployment on real aerial platforms.\nAs a result, their practical viability in open-world settings where the target is embedded among many other objects and sensing conditions vary significantly remains unclear.\nIn particular, despite encouraging results in controlled environments, there is still no reliable, experimentally validated scheme for endowing drones with robust, generalizable predictive capabilities for 3D structure understanding in the wild.\n2.4\nFoundation Models in Robotics\nRecent years have seen a rapid adoption of foundation models (FMs) in robotics, primarily using large language models (LLMs), vision-language models (VLMs) for ground robots and manipulators.\nLLM-based systems (\nAhn\net al.\n2022\n; Driess\net al.\n2023\n) took natural-language prompts to specify high-level tasks and rely on the LLM to perform task decomposition, sub-goal generation, or chain-of-thought reasoning for mobile manipulators.\nIn these frameworks, the language model is grounded through affordance critics or embodied encoders, enabling robots to execute multi-step manipulation and navigation tasks directly from free-form textual instructions.\nVLM-based approaches (\nZitkovich\net al.\n2023\n; Zhang\net al.\n2024a\n; Kim\net al.\n2024\n; Black\net al.\n2024\n; Zhou\net al.\n2025\n) extended this idea by jointly leveraging vision and language.\nUni-NaVid (\nZhang\net al.\n2024a\n) employed pretrained VLMs to interpret language instructions and video observations for a range of navigation tasks (\ne.g.\n, instruction-following, object-goal navigation) within a unified model.\nVision-language-action models (\nZitkovich\net al.\n2023\n; Kim\net al.\n2024\n; Black\net al.\n2024\n; Zhou\net al.\n2025\n) further coupled large-scale vision-language pretraining with robot datasets, treating actions as tokens and learning policies that map images and language inputs directly to low-level control.\nThese systems have demonstrated broad generalization across many table-top manipulation and indoor navigation tasks and highlight the value of FM backbones as rich semantic priors for robot decision making.\nAt the same time, FM-in-robotics research follows a data-hungry, end-to-end learning paradigm and is developed for relatively structured, short- to medium-horizon tasks under generous compute budgets.\nTheir interfaces to FMs are typically formulated in terms of symbolic task steps, sub-goals, or pixel-space waypoints, rather than explicit 3D spatial representations.\nConsequently, while existing FM-based robotic schemes highlight the potential of large pretrained models to endow robots with scene understanding and world knowledge, they stop short of providing a practical recipe for open-world aerial 3D target structure scanning.\n3\nProblem Statement\nThe problem considered in this work is to scan all surfaces of a user-intended 3D structure\nS\nS\n(\ne.g.\n, a castle) in an unknown open-world environment\nW\n=\n{\nS\n,\nO\n1\n,\nâ€¦\n,\nO\nN\n}\nW=\\{S,O_{1},\\dots,O_{N}\\}\nusing an autonomous aerial robot, where\nO\ni\nO_{i}\ndenotes surrounding non-target obstacles (\ne.g.\n, trees, buildings).\nThe robot is equipped with a gimbal-mounted RGB camera and a 3D LiDAR sensor, which capture visual and geometric information, respectively.\nIt executes a five-degree-of-freedom (DoF) trajectory\nğ±\nâ€‹\n(\nt\n)\n\\mathbf{x}(t)\nover\nt\nâˆˆ\n[\n0\n,\nT\n]\nt\\in[0,T]\n, specifying the 3D position of the drone and the orientation of the camera gimbal (\ni.e.\n, pitch and yaw angle).\nThe camera is modeled by a viewing frustum with a limited field of view (FoV) and maximum sensing range; at each time\nt\nt\n, only those surface points of\nS\nS\nthat lie within this frustum and maintain direct line-of-sight visibility are considered visually observed.\nSince the LiDARâ€™s effective range typically exceeds that of the camera, geometry sensing is rarely the limiting factor, and in practice the bottleneck for usable scan quality lies in the camera.\nAccordingly, in our formulation, overall scanning completeness is effectively governed by the camera coverage.\nThe motion must satisfy hard safety and feasibility constraints: the drone should not collide with any element of\nW\nW\nat any time\nt\nt\nand must respect its dynamic limits.\nGiven a feasible trajectory\nğ±\nâ€‹\n(\nt\n)\n\\mathbf{x}(t)\n, we measure efficiency by the total flight time\nT\nâ€‹\n(\nğ±\n)\nT(\\mathbf{x})\n, and define an information completeness index\nC\nâ€‹\n(\nğ±\n)\nâˆˆ\n[\n0\n,\n1\n]\nC(\\mathbf{x})\\in[0,1]\nbased on the fraction of the surface of\nS\nS\nthat is ever seen by the camera along\nğ±\nâ€‹\n(\nt\n)\n\\mathbf{x}(t)\n.\nMoreover, human involvement is represented through a prior input\nh\nh\nwith an associated qualitative effort measure\nE\nâ€‹\n(\nh\n)\nE(h)\n.\nWe do not assume a precise numerical model of\nE\nâ€‹\n(\nh\n)\nE(h)\n.\nInstead, it reflects the relative level of human effort required by different forms of prior input.\nIntuitively, abstract, semantic, or easily specified inputs (\ne.g.\n, text descriptions or sparse visual annotations) that are far from low-level control signals correspond to lower\nE\nâ€‹\n(\nh\n)\nE(h)\n, whereas more concrete specifications or priors that require external data collection (such as 3D bounding box, pre-defined trajectory, satellite imagery, or 3D models) entail higher\nE\nâ€‹\n(\nh\n)\nE(h)\n.\nTo summarize these considerations, we characterize the task as a multi-objective problem over\nğ‰\nâ€‹\n(\nğ±\n,\nh\n)\n=\n(\nT\nâ€‹\n(\nğ±\n)\n,\n1\nâˆ’\nC\nâ€‹\n(\nğ±\n)\n,\nE\nâ€‹\n(\nh\n)\n)\n,\n\\mathbf{J}(\\mathbf{x},h)=\\big(T(\\mathbf{x}),\\,1-C(\\mathbf{x}),\\,E(h)\\big),\n(1)\nwhich captures the competing objectives of flight efficiency, coverage completeness, and human effort.\nThis formulation is purely illustrative and is not intended to be explicitly optimized.\nConceptually, it serves to formalize the design principles that motivate our system architecture, which seeks to achieve shorter flight time and more comprehensive visual coverage while relying solely on low-effort prior inputs from humans.\n4\nSystem Overview\nFigure 3:\nSystem architecture of\nF\nly\nC\no.\nGiven a human prompt, the system runs a closed-loop pipeline: leveraging FMs, perception grounds the target in 2D/3D from streaming RGB-LiDAR, and multi-modal prediction completes the target surface from partial observations; the planner asynchronously performs consistency-aware global coverage planning over the predicted geometry with real-time local trajectory replanning.\nAll modules are continuously updated until the mission terminates.\nAs illustrated in Fig.\n3\n,\nF\nly\nC\no operates as a closed-loop system that starts from abstract human prompts and ends with a complete scan of the user-intended structure.\nAt the beginning of a mission, the user provides a high-level prompt\nh\nh\n(\ni.e.\n, a textual description and sparse visual annotations on the onboard camera view) to specify the target structure\nS\nS\n.\nAs the drone flies,\nF\nly\nC\no processes real-time streaming RGB images and LiDAR data with cross-modal refinement to continuously and precisely segment the target in cluttered 3D environments, thereby grounding the user prompt in the physical target structure (Sec.\nPrompt-grounded Scene Perception\n).\nThe resulting segmented visual and geometric observations of\nS\nS\nare then fused with the original textual prompt to infer a complete 3D surface representation of the target structure, including parts that have not yet been observed while remaining faithful to current measurements (Sec.\nMulti-modal Surface Prediction\n).\nThe predicted geometry serves as an intermediate bridge that distills the unknown and complicated open-world scanning task into a structured coverage planning problem with a clear global context.\nBased on the latest prediction,\nF\nly\nC\no correspondingly plans a target-centric and efficient coverage path that balances short travel distances and flight consistency, surveying all unobserved structure areas without redundant scans.\nIn parallel, using the online map and following global paths, it dynamically generates the executable trajectory for the drone in real-time to ensure safety and information completeness. (Sec.\nPrediction-aware Hierarchical Planning\n)\nNew sensor measurements together with the latest derived odometry and mapping results incrementally update perception and prediction, which in turn refine the planning objectives over time.\nThis loop repeats until the system autonomously detects that the target surface is sufficiently covered.\nF\nly\nC\no then terminates the mission and delivers all scanned data to the user.\nThroughout the above computation, all components run asynchronously in separate threads, reconciling slow FM-driven scene understanding with fast flight skills to execute the mission efficiently, accurately, and safely with low human effort.\n5\nMethodology\n5.1\nPrompt-grounded Scene Perception\nThe primary role of the perception module is to interpret abstract user prompts and determine â€œ\nwhat to scan\nâ€ at each moment.\nIt translates textual descriptions and visual annotations into precise pixel-level and point-level target segmentation in complex 3D environments from streaming RGB and LiDAR data.\nRecent advances in visual segmentation foundation models, such as SAM2 (\nRavi\net al.\n2024\n), provide strong generic priors for object delineation and temporally consistent mask propagation in videos, making them a natural backbone for prompt-target grounding.\nHowever, SAM2 natively supports only interactive visual inputs (points, boxes, and masks) and therefore cannot directly handle free-form textual prompts.\nTo enable text-conditioned segmentation, we integrate knowledge from VLMs such as BEiT3 (\nWang\net al.\n2023\n) into SAM2 via an early-fusion scheme (\nZhang\net al.\n2024b\n), which alleviates segmentation ambiguities that arise when using text or visual prompts alone.\nNevertheless, naively applying such off-the-shelf models is still insufficient for our task, due to two key challenges: (1) adapting them to real-time streaming data, and (2) transferring their segmentation capability to 3D LiDAR data while ensuring robust target grounding across viewpoints and over long-horizon flights.\n5.1.1\nStreaming-efficient Adaptation for SAM2\nThe first practical challenge is that SAM2 is designed for offline video segmentation: it maintains a growing memory bank over all video frames to propagate masks, causing GPU memory usage and runtime to increase with sequence length.\nIn our long-horizon aerial scanning setting, where the drone may fly for thousands of frames, this behavior is impractical for onboard deployment.\nMoreover, the original SAM2 model is computationally heavy for edge-computing aerial platforms, making real-time processing of streaming image data infeasible.\nWe address these issues with two key designs.\nInstead of storing all frames, we maintain a sliding-window memory bank that retains only the most recent\nW\nm\nW_{m}\nsegmentation results, discarding older entries and thereby keeping the memory footprint bounded over long flights.\nIn parallel, we optimize the SAM2 backbone for onboard inference via lightweight quantization and graph-level compilation, which reduce per-frame latency without retraining.\n5.1.2\nCross-modal Refinement for Robust Target Grounding\nAs for the second challenge, a straightforward idea is to project the segmented 2D masks from the camera images onto the 3D LiDAR points using known extrinsics and intrinsics to identify the target structure in 3D space.\nHowever, this naive projection entirely relies on the accuracy and robustness of 2D segmentation, which may fluctuate or fail under strong viewpoint changes and occlusions during long-horizon flights (Sec.\nAblations on Perception\n).\nTo overcome this limitation, we propose a cross-modal refinement mechanism that jointly leverages visual and geometric cues to stabilize target grounding, as summarized in Algo.\n1\n.\nIn parallel to 2D segmentation, an independent branch processes the raw LiDAR point cloud.\nAt the beginning of a mission, the initial LiDAR data is partitioned into multiple groups using a density-based clustering algorithm (\nEster\net al.\n1996\n) that accounts for both point density and spatial proximity.\nEach cluster is then projected onto the image plane, and its overlap with the 2D mask in the first frame is evaluated.\nClusters whose projected points lie entirely within the mask are labeled as belonging to the target structure.\nDuring subsequent frames, geometric clustering is updated incrementally for computational efficiency: newly acquired LiDAR points are assigned to the nearest existing cluster if within a distance threshold, while points sufficiently far from all clusters are used to spawn new ones.\nThis yields a continuously updated geometric estimate of the target in 3D space.\nAlgorithm 1\nCross-modal Refinement Mechanism\n1:\nCurrent point cloud\nğ’«\nt\n\\mathcal{P}_{t}\n, current image\nâ„\nt\n\\mathcal{I}_{t}\n, point clusters\nğ’\nt\n\\mathcal{C}_{t}\n, memory bank\nâ„¬\nt\n\\mathcal{B}_{t}\n, textual prompt\nh\ntext\nh_{\\text{text}}\n, reinitialization threshold\nÎº\nreinit\n\\kappa_{\\text{reinit}}\n2:\nNotation:\nCurrent target 3D cluster\nğ’«\nt\ns\nâ€‹\ne\nâ€‹\ng\n\\mathcal{P}^{seg}_{t}\n, memory keyframes\nğ’¦\nt\n\\mathcal{K}_{t}\n, current 2D mask\nM\nt\nM_{t}\n, projected mask\nM\nt\nproj\nM^{\\text{proj}}_{t}\n, virtual prompt points\np\nv\np_{\\text{v}}\n3:\nğ’«\nt\ns\nâ€‹\ne\nâ€‹\ng\n,\nğ’\nt\n+\n1\nâ†\nGeometricClustering\nâ€‹\n(\nğ’«\nt\n,\nğ’\nt\n)\n\\mathcal{P}^{seg}_{t},\\mathcal{C}_{t+1}\\leftarrow\\text{GeometricClustering}(\\mathcal{P}_{t},\\mathcal{C}_{t})\n4:\nğ’¦\nt\nâ†\nKeyframeSelection\nâ€‹\n(\nâ„¬\nt\n,\nğ’«\nt\ns\nâ€‹\ne\nâ€‹\ng\n)\n\\mathcal{K}_{t}\\leftarrow\\text{KeyframeSelection}(\\mathcal{B}_{t},\\mathcal{P}^{seg}_{t})\n5:\nM\nt\n,\nâ„¬\nt\n+\n1\nâ†\nSAM2\nâ€‹\n(\nâ„\nt\n,\nğ’¦\nt\n,\nh\ntext\n)\nM_{t},\\mathcal{B}_{t+1}\\leftarrow\\text{SAM2}(\\mathcal{I}_{t},\\mathcal{K}_{t},h_{\\text{text}})\n6:\nM\nt\nproj\nâ†\nBackProjection\nâ€‹\n(\nğ’«\nt\ns\nâ€‹\ne\nâ€‹\ng\n)\nM^{\\text{proj}}_{t}\\leftarrow\\text{BackProjection}(\\mathcal{P}^{seg}_{t})\n7:\nif\nBoxIoU\n(\nM\nt\n,\nM\nt\nproj\nM_{t},M^{\\text{proj}}_{t}\n)\n<\nÎº\nreinit\n<\\kappa_{\\text{reinit}}\nthen\n8:\np\nv\nâ†\nFarthestPointSampling\nâ€‹\n(\nğ’«\nt\ns\nâ€‹\ne\nâ€‹\ng\n)\np_{\\text{v}}\\leftarrow\\text{FarthestPointSampling}(\\mathcal{P}^{seg}_{t})\n9:\nM\nt\n,\nâ„¬\nt\n+\n1\nâ†\nSAM2\nâ€‹\n(\nâ„\nt\n,\np\nv\n,\nh\ntext\n)\nM_{t},\\mathcal{B}_{t+1}\\leftarrow\\text{SAM2}(\\mathcal{I}_{t},p_{\\text{v}},h_{\\text{text}})\nâŠ³\n\\triangleright\nre-query with virtual prompts\n10:\nğ’\nt\n+\n1\nâ†\nUpdateGeometricClustering\nâ€‹\n(\nğ’«\nt\n,\nM\nt\n)\n\\mathcal{C}_{t+1}\\leftarrow\\text{UpdateGeometricClustering}(\\mathcal{P}_{t},M_{t})\n11:\nend\nif\n12:\nM\nt\nM_{t}\n,\nğ’«\nt\ns\nâ€‹\ne\nâ€‹\ng\n\\mathcal{P}^{seg}_{t}\nGiven the current geometric clustering, we first use it to choose informative and temporally stable keyframes from the memory bank.\nVanilla SAM2 simply selects the most recent\nW\ns\nW_{s}\nframes (\nW\ns\n<\nW\nm\nW_{s}<W_{m}\n) as memory keyframes for next-frame tracking, which ignores temporal consistency and geometric support, and can lead to drift.\nTo encode the temporal coherence of 2D masks, we maintain a linear Kalman filter over their motion in image space. The state vector is defined as\nğ›\n=\n[\nb\nx\n,\nb\ny\n,\nb\nw\n,\nb\nh\n,\nb\nË™\nx\n,\nb\nË™\ny\n,\nb\nË™\nw\n,\nb\nË™\nh\n]\nâŠ¤\n,\n\\mathbf{b}=[b_{x},b_{y},b_{w},b_{h},\\dot{b}_{x},\\dot{b}_{y},\\dot{b}_{w},\\dot{b}_{h}]^{\\top},\n(2)\nwhere\n(\nb\nx\n,\nb\ny\n)\n(b_{x},b_{y})\ndenote the center of the minimum bounding box of the segmentation,\nb\nw\n,\nb\nh\nb_{w},b_{h}\nits width and height, and the remaining entries their velocities.\nUsing a constant-velocity motion model\nğ…\n\\mathbf{F}\n, the state prediction at time\nt\nt\nis\nğ›\n^\nt\n|\nt\nâˆ’\n1\n=\nğ…\nâ€‹\nğ›\n^\nt\nâˆ’\n1\n|\nt\nâˆ’\n1\n.\n\\hat{\\mathbf{b}}_{t|t-1}=\\mathbf{F}\\hat{\\mathbf{b}}_{t-1|t-1}.\n(3)\nWe then quantify the temporal consistency of the current 2D mask\nM\nt\nM_{t}\nby the box Intersection-over-Union (IoU) between\nğ›\n^\nt\n|\nt\nâˆ’\n1\n\\hat{\\mathbf{b}}_{t|t-1}\nand the bounding box of\nM\nt\nM_{t}\n,\ns\nkf\n=\nBoxIoU\nâ€‹\n(\nğ›\n^\nt\n|\nt\nâˆ’\n1\n,\nM\nt\n)\n.\ns_{\\text{kf}}=\\texttt{BoxIoU}(\\hat{\\mathbf{b}}_{t|t-1},M_{t}).\n(4)\nThe Kalman update step,\nğ›\n^\nt\n|\nt\n=\nğ›\n^\nt\n|\nt\nâˆ’\n1\n+\nğ†\nt\nâ€‹\n(\nM\nt\nâˆ’\nğ‡\nâ€‹\nğ›\n^\nt\n|\nt\nâˆ’\n1\n)\n,\n\\hat{\\mathbf{b}}_{t|t}=\\hat{\\mathbf{b}}_{t|t-1}+\\mathbf{G}_{t}\\big(M_{t}-\\mathbf{H}\\hat{\\mathbf{b}}_{t|t-1}\\big),\n(5)\nwith gain\nğ†\nt\n\\mathbf{G}_{t}\nand observation model\nğ‡\n\\mathbf{H}\n, keeps the tracker locked to the current segmentation.\nConcurrently, we measure cross-modal geometric consistency by projecting the current 3D target cluster onto the image and computing the pixel-level IoU between the projected mask\nM\nt\nproj\nM^{\\text{proj}}_{t}\nand the 2D mask\nM\nt\nM_{t}\n,\ns\ngeo\n=\nPixelIoU\nâ€‹\n(\nM\nt\nproj\n,\nM\nt\n)\n.\ns_{\\text{geo}}=\\texttt{PixelIoU}(M^{\\text{proj}}_{t},M_{t}).\n(6)\nFor each candidate frame in the memory bank, we combine its temporal consistency and geometric alignment into a single keyframe score\ns\ni\n=\ns\nkf\n,\ni\n+\ns\ngeo\n,\ni\n,\ns_{i}=s_{\\text{kf},i}+s_{\\text{geo},i},\n(7)\nwhere\ns\nkf\n,\ni\ns_{\\text{kf},i}\nand\ns\ngeo\n,\ni\ns_{\\text{geo},i}\nare computed as above for frame\ni\ni\n.\nInstead of naively keeping the latest\nW\ns\nW_{s}\nframes, we select the top-\nW\ns\nW_{s}\nframes with the highest\ns\ni\ns_{i}\nas keyframes for SAM2â€™s memory attention, favoring frames that are both temporally stable and geometrically representative for achieving robust 2D mask propagation.\nFinally, the 3D cluster also serves as an anchor to detect and correct misidentification or drift in 2D segmentation over time.\nAt each frame, we compare the current 2D mask\nM\nt\nM_{t}\nwith the projection\nM\nt\nproj\nM^{\\text{proj}}_{t}\nof the geometric target cluster via box IoU.\nIf this score falls below a threshold\nÎº\nreinit\n\\kappa_{\\text{reinit}}\n, we regard the two modalities as inconsistent, indicating either segmentation drift or corrupted clustering.\nIn this case, we perform a cross-modal re-initialization: a subset of target points is selected from current target 3D cluster by farthest-point sampling (FPS) and projected to the image as virtual prompt points; these prompts are fed back to SAM2 with the original textual prompt to obtain a corrected mask, while the Kalman filter is re-initialized and geometric clustering is updated on the new result.\nConversely, when the IoU stays above\nÎº\nreinit\n\\kappa_{\\text{reinit}}\n, both visual and geometric estimates are considered reliable, and the geometric information is used solely for keyframe selection as described above.\nThis bidirectional interactionâ€”using 3D geometry to both repair and curate 2D memoryâ€”substantially improves the robustness of target grounding across viewpoints and over long-horizon flights.\nThe cross-refined 2D and 3D target regions with raw data are populated into a shared buffer, serving as the input for the prediction and planning stages.\n5.1.3\nSimultaneous Localization and Mapping (SLAM)\nOur aerial system achieves robust localization by fusing raw LiDAR point clouds with IMU data via a LiDAR-inertial odometry algorithm (\nXu\net al.\n2022\n).\nTo efficiently manage 3D information for facilitating downstream processes, we maintain two distinct representations of the environment: an Occupancy Grid Map (OGM) and a Euclidean Signed Distance Field (ESDF).\nThe OGM is updated in real-time by integrating newly localized LiDAR scans.\nBased on preceding segmentation results, each occupied voxel is classified as either â€œtargetâ€ or â€œnon-targetâ€.\nConcurrently, the ESDF is incrementally computed from the OGM to support efficient collision checking during trajectory optimization.\n5.2\nMulti-modal Surface Prediction\nFigure 4:\nOverview of our multi-modal surface predictor.\nIt first independently encodes each modality with foundation models, fuses heterogeneous features via alternating attention, and decodes the completed shape supervised by joint partial and completion Chamfer losses.\nThis module answers â€œ\nwhat the full structure looks like\nâ€ by predicting a complete 3D surface of the target from partial observations, empowering planning with comprehensive structural knowledge of the target.\nConcretely, it consumes three observation sources: (1) segmented 3D points from the perception module, (2) the corresponding segmented RGB images, and (3) the original textual prompt describing the targetâ€™s global semantics.\nRelying on a single modality is fundamentally insufficient in our setting.\nPurely geometric completion from LiDAR is severely constrained: as unordered 3D points without texture or rich semantic cues, 3D coordinates provide weak evidence about object parts, especially across large unobserved regions.\nAs a result, they tend to lean heavily on learned shape priors and often fail on out-of-distribution structures.\nConversely, vision and language FMs offer strong zero-shot generalization in isolation, but predictions obtained directly from a single visual or textual modality are geometrically ill-posed: without accurate metric scale or camera poses, they struggle to produce surfaces that match physical measurements.\nMulti-modal fusion that exploits the complementarity among these signals is therefore essential to realize the â€œ\nbridging\nâ€ role of prediction in our system (Sec.\nMulti-modal conditioning.\n).\nTo this end, we design a multi-modal surface prediction network together with an efficient training-inference pipeline that jointly encodes and fuses geometric, visual, and textual features, effectively incorporating foundation-model knowledge to infer a complete 3D surface of the target with correct metric scale, fine-grained granularity, globally plausible shape, and local fidelity to the inputs, as depicted in Fig.\n4\n.\n5.2.1\nMulti-modal Inputs and Preprocessing\nThe geometric input\nğ’«\ntar\n\\mathcal{P}_{\\text{tar}}\nis obtained by collecting all voxels labeled as â€œtargetâ€ in the latest OGM, converting them into 3D points, and downsampling to a fixed set of\nN\ni\nN_{i}\npoints using FPS.\nFor the visual input\nâ„\ntar\n\\mathcal{I}_{\\text{tar}}\n, we project\nğ’«\ntar\n\\mathcal{P}_{\\text{tar}}\nonto the image plane of each segmented RGB frame from the perception module and compute the fraction of projected points that fall inside the corresponding 2D mask.\nThe frame with the highest fraction is selected as\nâ„\ntar\n\\mathcal{I}_{\\text{tar}}\n, ensuring that the most informative view of the target is used.\nWe also record the associated camera parameters\nğ \ntar\n=\nğŠ\nâ€‹\n[\nğ‘\ntar\nâˆ£\nğ­\ntar\n]\n\\mathbf{g}_{\\text{tar}}=\\mathbf{K}\\big[\\mathbf{R}_{\\text{tar}}\\mid\\mathbf{t}_{\\text{tar}}\\big]\nfor later use, where\nğŠ\n\\mathbf{K}\n,\nğ‘\ntar\n\\mathbf{R}_{\\text{tar}}\n, and\nğ­\ntar\n\\mathbf{t}_{\\text{tar}}\ndenote the intrinsic matrix, rotation matrix, and translation vector, respectively.\nThe textual input\nh\ntext\nh_{\\text{text}}\nis directly taken from the userâ€™s original prompt.\nTo stabilize training and handle targets of varying physical scales, we apply a local normalization to both\nğ’«\ntar\n\\mathcal{P}_{\\text{tar}}\nand\nğ \ntar\n\\mathbf{g}_{\\text{tar}}\nusing the centroid\nğœ\ntar\n\\mathbf{c}_{\\text{tar}}\nand scale\ns\ntar\ns_{\\text{tar}}\nof\nğ’«\ntar\n\\mathcal{P}_{\\text{tar}}\n:\nğ’«\ntar\n=\nğ’«\ntar\nâˆ’\nğœ\ntar\ns\ntar\n,\nğ­\ntar\n=\nğ­\ntar\nâˆ’\nğœ\ntar\ns\ntar\n.\n{\\mathcal{P}}_{\\text{tar}}=\\frac{\\mathcal{P}_{\\text{tar}}-\\mathbf{c}_{\\text{tar}}}{s_{\\text{tar}}},\\quad{\\mathbf{t}}_{\\text{tar}}=\\frac{\\mathbf{t}_{\\text{tar}}-\\mathbf{c}_{\\text{tar}}}{s_{\\text{tar}}}.\n(8)\nFor notational simplicity, we reuse\nğ’«\ntar\n\\mathcal{P}_{\\text{tar}}\nand\nğ \ntar\n\\mathbf{g}_{\\text{tar}}\nto denote the normalized point cloud and camera pose in the following.\nBased on these inputs, our multi-modal predictor\nf\nÎ¸\nf_{\\theta}\nlearns a mapping from\n{\nğ’«\ntar\n,\nâ„\ntar\n,\nğ \ntar\n,\nh\ntext\n}\n\\{\\mathcal{P}_{\\text{tar}},\\mathcal{I}_{\\text{tar}},\\mathbf{g}_{\\text{tar}},h_{\\text{text}}\\}\nto the complete surface\nğ’«\ncomp\n\\mathcal{P}_{\\text{comp}}\nof the target structure with\nN\no\nN_{o}\npoints,\nf\nÎ¸\n:\n(\nğ’«\ntar\n,\nâ„\ntar\n,\nğ \ntar\n,\nh\ntext\n)\nâ†¦\nğ’«\ncomp\nâˆˆ\nâ„\nN\no\nÃ—\n3\n.\nf_{\\theta}:(\\mathcal{P}_{\\text{tar}},\\mathcal{I}_{\\text{tar}},\\mathbf{g}_{\\text{tar}},h_{\\text{text}})\\mapsto\\mathcal{P}_{\\text{comp}}\\in\\mathbb{R}^{N_{o}\\times 3}.\n(9)\n5.2.2\nNetwork Architecture\nAt a high level, our predictor\nf\nÎ¸\nf_{\\theta}\nadopts a feed-forward, post-fusion architecture with three stages: independent encoding of each modality, geometry-semantics fusion, and geometric decoding.\nThe design is guided by two principles: (1) retain only minimal 3D inductive bias in the network, so that most structural priors come from ample 3D-annotated data and FMs rather than hand-crafted features; and (2) maximally reuse rich world knowledge from existing vision-language FMs, while explicitly steering this knowledge into the geometric stream that ultimately defines the 3D surface.\nSpecifically, we let language and vision FMs first produce rich, task-agnostic embeddings, and defer multi-modal interaction to a late fusion stage where the geometric branch queries these embeddings through alternating self-cross attention layers.\nIn this way, the fusion layer primarily aligns and retrieves semantic context for geometry, keeping the overall architecture simple while allowing the 3D predictions to benefit directly from the expressive latent spaces of FMs and enabling inference compatible with closed-loop flight.\nModality-specific encoders with FM reuse.\nInstead of training heavy encoders from scratch, we reuse pre-trained FMs to obtain compact latent representations for language and imagery.\nIn particular, we use the text encoder in SigLIP (\nZhai\net al.\n2023\n) to derive a textual tokens\nâ„±\ntext\nâˆˆ\nâ„\n1\nÃ—\nD\nt\n\\mathcal{F}_{\\text{text}}\\in\\mathbb{R}^{1\\times D_{\\text{t}}}\nfrom the prompt\nh\ntext\nh_{\\text{text}}\n, and apply DINO (\nOquab\net al.\n2023\n) to the input image\nâ„\ntar\n\\mathcal{I}_{\\text{tar}}\n, yielding a set of\nK\nK\npatchified tokens\nâ„±\nâ„\nâˆˆ\nâ„\nK\nÃ—\nD\nâ„\n\\mathcal{F}_{\\mathcal{I}}\\in\\mathbb{R}^{K\\times D_{\\mathcal{I}}}\n.\nThis strategy avoids costly encoder training, accelerates convergence, and injects broad semantics about spatial structures into our predictor.\nGiven the current lack of powerful 3D FMs comparable to vision-language models, we deliberately adopt a lightweight PointNet++ backbone (\nQi\net al.\n2017\n) to encode the partial point cloud\nğ’«\ntar\n\\mathcal{P}_{\\text{tar}}\ninto geometric features\nâ„±\nğ’«\nâˆˆ\nâ„\nN\ni\nÃ—\nD\nğ’«\n\\mathcal{F}_{\\mathcal{P}}\\in\\mathbb{R}^{N_{i}\\times D_{\\mathcal{P}}}\n.\nThis design allows the geometric branch focus on capturing local shape patterns while high-level semantics and world priors are injected later through the fusion stage.\nAll three feature sets are then projected to a common feature dimension\nD\nf\nD_{\\text{f}}\nusing individual linear layers.\nWhile vision FMs provide strong generalizable knowledge, their representations are largely spatially agnostic with respect to the metric 3D frame: the same image feature may correspond to very different physical scales or viewpoints.\nTo mitigate this issue, we incorporate camera parameters\nğ \ntar\n\\mathbf{g}_{\\text{tar}}\ninto each layer of DINO via parameter-efficient feature modulation.\nA small MLP predicts per-channel scale and shift terms\n[\nğœ¸\n,\nğœ·\n]\n=\nMLP\nâ€‹\n(\nğ \ntar\n)\n,\n[\\boldsymbol{\\gamma},\\boldsymbol{\\beta}]=\\mathrm{MLP}(\\mathbf{g}_{\\text{tar}}),\n(10)\nwhich modulate the visual features as\nâ„±\nâ„\nâ†\n(\nğŸ\n+\nğœ¸\n)\nâŠ™\nâ„±\nâ„\n+\nğœ·\n,\n\\mathcal{F}_{\\mathcal{I}}\\leftarrow(\\mathbf{1}+\\boldsymbol{\\gamma})\\odot\\mathcal{F}_{\\mathcal{I}}+\\boldsymbol{\\beta},\n(11)\nwhere\nâŠ™\n\\odot\ndenotes element-wise multiplication.\nThis conditioning makes\nâ„±\nâ„\n\\mathcal{F}_{\\mathcal{I}}\naware of 3D spaces with negligible additional parameters, easing subsequent alignment with geometric features.\nGeometry-semantics fusion via alternating attention.\nTo simplify multi-modal fusion among these heterogeneous features, we group them into two types: a geometric one\nâ„±\nğ’«\n\\mathcal{F}_{\\mathcal{P}}\nand a semantic one obtained by concatenating visual and textual features,\nâ„±\nS\n=\nâ„±\nâ„\nâŠ•\nâ„±\ntext\n,\n\\mathcal{F}_{\\text{S}}=\\mathcal{F}_{\\mathcal{I}}\\oplus\\mathcal{F}_{\\text{text}},\n(12)\nwhere\nâŠ•\n\\oplus\ndenotes concatenation along the feature dimension.\nThe core of our fusion stage is the proposed alternating-attention layers, aligning two information sources into a shared latent space effectively.\nEach layer consists of geometry-wise and semantic-wise self-attention followed by global cross-attention:\nâ„±\nğ’«\n^\n\\displaystyle\\widehat{\\mathcal{F}_{\\mathcal{P}}}\n=\nSelfAttention\nâ€‹\n(\nâ„±\nğ’«\n)\n,\n\\displaystyle=\\mathrm{SelfAttention}(\\mathcal{F}_{\\mathcal{P}}),\n(13)\nâ„±\nS\n^\n\\displaystyle\\widehat{\\mathcal{F}_{\\text{S}}}\n=\nSelfAttention\nâ€‹\n(\nâ„±\nS\n)\n,\n\\displaystyle=\\mathrm{SelfAttention}(\\mathcal{F}_{\\text{S}}),\nâ„±\nğ’«\n~\n\\displaystyle\\widetilde{\\mathcal{F}_{\\mathcal{P}}}\n=\nCrossAttention\nâ€‹\n(\nâ„±\nğ’«\n^\n,\nâ„±\nS\n^\n)\n.\n\\displaystyle=\\mathrm{CrossAttention}(\\widehat{\\mathcal{F}_{\\mathcal{P}}},\\widehat{\\mathcal{F}_{\\text{S}}}).\nGeometry-wise self-attention captures intra-structural relations and details within the partial 3D shape, while semantic-wise self-attention aligns and consolidates prior knowledge coming from separate vision and language FMs.\nCrucially, in global cross-attention, we treat the geometric features as query and the fused semantic features as key and value.\nThis directional design reflects our modeling goal: the geometric features actively inherit world knowledge embedded in these FMs and draw upon the rich contextual information from semantic modalities.\nThis fashion allows the network to effectively â€œgrowâ€ a complete 3D shape from partially observed parts by absorbing complementary guidance from the image and text.\nBy stacking\nL\nL\nsuch layers, we obtain a deeply fused representation\nâ„±\nğ’«\n~\n\\widetilde{\\mathcal{F}_{\\mathcal{P}}}\nthat encodes both the local measured geometry and global semantic priors in a unified latent space.\nNotably, Sec.\nAlternating attention for knowledge fusion.\nconfirms that this fusion scheme effectively harvests FM knowledge into geometric predictions.\nGeometric decoding.\nThe final decoding stage maps the fused geometric feature back into 3D space.\nA MLP first predicts a set of candidate 3D points from\nâ„±\nğ’«\n~\n\\widetilde{\\mathcal{F}_{\\mathcal{P}}}\n.\nTo preserve the characteristics of the input surface while extrapolating into unobserved regions, we concatenate these predicted points with the original partial input\nğ’«\ntar\n\\mathcal{P}_{\\text{tar}}\nand apply farthest-point sampling to obtain a fixed-size output point cloud,\nğ’«\ncomp\n=\nFPS\nâ€‹\n(\nMLP\nâ€‹\n(\nâ„±\nğ’«\n~\n)\nâŠ•\nğ’«\ntar\n)\nâˆˆ\nâ„\nN\no\nÃ—\n3\n,\n\\mathcal{P}_{\\text{comp}}=\\mathrm{FPS}\\big(\\mathrm{MLP}(\\widetilde{\\mathcal{F}_{\\mathcal{P}}})\\oplus\\mathcal{P}_{\\text{tar}}\\big)\\in\\mathbb{R}^{N_{o}\\times 3},\n(14)\nIn this way, the resulting\nğ’«\ncomp\n\\mathcal{P}_{\\text{comp}}\naugments the physically measured surface with semantically guided completions and remains directly usable as a metric 3D representation.\n5.2.3\nTraining Objectives\nAs a source of foresight for planning, the predictor is required to satisfy two complementary objectives.\nFirst, it must recover the complete surface of the target structure so that the planner can reason about occluded and yet-unseen regions and optimize global coverage.\nSecond, it must remain strictly faithful to the partial observations provided by the LiDAR, preserving fine details on already observed surfaces and preventing the network from â€œ\ncorrecting\nâ€ valid measurements to avoid guiding redundant scans.\nIn practice, many completion methods that optimize only for global completeness (for example, by minimizing a symmetric distance to the full ground truth (\nYuan\net al.\n2018\n; Yu\net al.\n2021\n; Rong\net al.\n2024\n)) tend to hallucinate shapes that deviate from the input, a pathology that becomes particularly severe on complex or out-of-distribution structures.\nPrior works, such as SDS-Complete (\nKasten\net al.\n2023\n), enforce consistency with the input by maintaining an auxiliary implicit representation (\ne.g.\n, a neural signed distance field) and constraining the partial observations to lie on its zero-level set.\nWhile effective, such schemes introduce expensive additional memory and computation overhead due to the extra representation that must be stored and updated during training.\nInstead, we adopt a simple yet effective dual-objective supervision strategy that encodes both completeness and input consistency directly at the point-set level, without any auxiliary fields (Sec.\nPartial-surface regularization.\n).\nOur formulation is built on the permutation-invariant Chamfer distance (CD), used in both undirected and directed forms (denoted by\nCD\nu\n\\mathrm{CD}_{\\mathrm{u}}\nand\nCD\nd\n\\mathrm{CD}_{\\mathrm{d}}\n), defined for point sets\nğ’³\n,\nğ’´\nâŠ‚\nâ„\n3\n\\mathcal{X},\\mathcal{Y}\\subset\\mathbb{R}^{3}\nas\nCD\nu\nâ€‹\n(\nğ’³\n,\nğ’´\n)\n\\displaystyle\\mathrm{CD}_{\\mathrm{u}}(\\mathcal{X},\\mathcal{Y})\n=\n1\n|\nğ’³\n|\nâ€‹\nâˆ‘\nx\nâˆˆ\nğ’³\nmin\ny\nâˆˆ\nğ’´\nâ¡\nâ€–\nx\nâˆ’\ny\nâ€–\n2\n2\n+\n\\displaystyle=\\frac{1}{|\\mathcal{X}|}\\sum_{x\\in\\mathcal{X}}\\min_{y\\in\\mathcal{Y}}\\|x-y\\|_{2}^{2}+\n(15)\n1\n|\nğ’´\n|\nâ€‹\nâˆ‘\ny\nâˆˆ\nğ’´\nmin\nx\nâˆˆ\nğ’³\nâ¡\nâ€–\nx\nâˆ’\ny\nâ€–\n2\n2\n,\n\\displaystyle\\frac{1}{|\\mathcal{Y}|}\\sum_{y\\in\\mathcal{Y}}\\min_{x\\in\\mathcal{X}}\\|x-y\\|_{2}^{2},\nCD\nd\nâ€‹\n(\nğ’³\n,\nğ’´\n)\n=\n1\n|\nğ’´\n|\nâ€‹\nâˆ‘\ny\nâˆˆ\nğ’´\nmin\nx\nâˆˆ\nğ’³\nâ¡\nâ€–\nx\nâˆ’\ny\nâ€–\n2\n2\n.\n\\mathrm{CD}_{\\mathrm{d}}(\\mathcal{X},\\mathcal{Y})=\\frac{1}{|\\mathcal{Y}|}\\sum_{y\\in\\mathcal{Y}}\\min_{x\\in\\mathcal{X}}\\|x-y\\|_{2}^{2}.\n(16)\nTo promote global completeness, we minimize the undirected CD between the prediction\nğ’«\ncomp\n\\mathcal{P}_{\\text{comp}}\nand the ground-truth surface\nğ’«\ngt\n\\mathcal{P}_{\\text{gt}}\n, termed as\nâ„’\ncomp\n=\nCD\nu\nâ€‹\n(\nğ’«\ncomp\n,\nğ’«\ngt\n)\n\\mathcal{L}_{\\text{comp}}=\\mathrm{CD}_{\\mathrm{u}}(\\mathcal{P}_{\\text{comp}},\\mathcal{P}_{\\text{gt}})\n.\nTo enforce faithfulness to measurements with negligible extra training burden, we add a partial-surface regularization term given by the directed CD from the prediction\nğ’«\ncomp\n\\mathcal{P}_{\\text{comp}}\nto the input partial point cloud\nğ’«\ntar\n\\mathcal{P}_{\\text{tar}}\n, termed as\nâ„’\npar\n=\nCD\nd\nâ€‹\n(\nğ’«\ncomp\n,\nğ’«\ntar\n)\n\\mathcal{L}_{\\text{par}}=\\mathrm{CD}_{\\mathrm{d}}(\\mathcal{P}_{\\text{comp}},\\mathcal{P}_{\\text{tar}})\n.\nOur overall training loss function thus reads\nâ„’\n=\nâ„’\ncomp\n+\nâ„’\npar\n.\n\\mathcal{L}=\\mathcal{L}_{\\text{comp}}+\\mathcal{L}_{\\text{par}}.\n(17)\nFigure 5:\nAutomatic multi-modal data generation pipeline, which synthesizes partial point clouds, RGB images, and textual descriptions from the original 3D models.\n5.2.4\nAutomatic Multi-modal Data Generation\nTraining the proposed multi-modal predictor requires large quantities of aligned data across 3D geometry, images, camera poses, and text.\nHowever, existing datasets rarely provide all these modalities in a consistent form.\nWe therefore build an automatic data generation pipeline supported by FMs (Fig.\n5\n) that converts raw 3D assets into synchronized point clouds, RGB views, camera parameters, and textual descriptions at scale.\nRendering and point cloud synthesis.\nGiven a 3D model, we use Blender\n1\n1\n1\nhttps://www.blender.org/\nto render\n32\n32\nRGB-depth pairs from cameras placed uniformly on a viewing sphere around it, with distance equal to twice the radius of the modelâ€™s bounding sphere and fixed intrinsics.\nEach depth map is back-projected into 3D to obtain a per-view point cloud.\nThe complete ground-truth cloud\nğ’«\ngt\n\\mathcal{P}_{\\text{gt}}\nis constructed by merging the points from all views.\nTo generate diverse partial observations, we repeatedly sample a random subset of\n1\n1\n-\n8\n8\nviewpoints and fuse their points into a single input cloud\nğ’«\ntar\n\\mathcal{P}_{\\text{tar}}\nin\n32\n32\ntimes.\nThis stochastic strategy yields a wide distribution of inputs, from very sparse to densely populated partial views, which is crucial for the modelâ€™s robustness.\nText generation for global semantics.\nTo attach a global textual prompt to each 3D asset, we build an automatic captioning tool on top of the rendered multi-view images.\nFor each rendered RGB view, a pre-trained image captioning model (\nLi\net al.\n2023\n) produces five candidate sentences, and CLIP (\nRadford\net al.\n2021\n) scores them against the corresponding image; the top-scoring one is kept as the view-level caption.\nAll view-level captions for a given asset are then passed to GPT-4 (\nAchiam\net al.\n2023\n), which fuses them into a single, globally consistent description by removing view-dependent details, merging redundancies, and normalizing style.\nThe resulting captions are short, single-sentence, model-centric descriptions that emphasize overall shape, appearance, and salient geometric structure, such as â€œ\na stone Gothic church featuring a slender spire.\nâ€\nThese consolidated sentences serve as the textual input\nh\ntext\nh_{\\text{text}}\nused during training, and are designed to match the abstraction level of human prompts at test time.\nData augmentation.\nEach training sample is formed as an aligned tuple\n(\nğ’«\ntar\n,\nâ„\ntar\n,\nğ \ntar\n,\nh\ntext\n,\nğ’«\ngt\n)\n(\\mathcal{P}_{\\text{tar}},\\mathcal{I}_{\\text{tar}},\\mathbf{g}_{\\text{tar}},h_{\\text{text}},\\mathcal{P}_{\\text{gt}})\n.\nHere,\nğ’«\ntar\n\\mathcal{P}_{\\text{tar}}\nand\nâ„\ntar\n\\mathcal{I}_{\\text{tar}}\nwith its camera parameters\nğ \ntar\n\\mathbf{g}_{\\text{tar}}\nare randomly selected.\nTo enhance generalization, we apply several augmentations during training.\nVariations in camera intrinsics are simulated by cropping the rendered RGB imagesâ€”an efficient method that enhances robustness to different sensors without computationally expensive re-rendering.\nRobustness to camera pose noise is improved by applying random\nSE\nâ€‹\n(\n3\n)\n\\text{SE}(3)\nperturbations to the camera extrinsics,\ni.e.\n, injecting small translations and rotations into the camera pose.\nWe further reduce sensitivity to the absolute world frame by randomly shifting the coordinate origin via a global translation of the input coordinates.\nThe diversity of the input is further enriched by applying a random masking operation to the partial point clouds, which varies their point density and spatial distribution.\nOverall, this fully automatic pipeline generates large-scale, consistently aligned multi-modal data without manual labeling and can be readily extended to more 3D assets, forming a practical basis for future scaling of the predictor.\nFigure 6:\nInference-time Geometry Densification Scheme.\nStarting from sparse completion, it applies cascade upsampling adaptive to physical granularity, then extracts a watertight mesh.\n5.2.5\nInference-time Geometry Densification\nWhile the predictor already outputs a complete surface in the form of a fixed-size point set\nğ’«\ncomp\n\\mathcal{P}_{\\text{comp}}\n, a single resolution cannot accommodate the wide range of target scales encountered in practice.\nFor instance, a point budget that is sufficient to describe a small gate becomes too sparse for a large building facade, causing loss of fine details and, thereby hindering the provision of accurate guidance for planning.\nBlindly increasing the network output size would inflate training and inference cost, and is unnecessary for smaller targets.\nInstead, we decouple prediction quality from network output resolution by introducing a lightweight, inference-time densification scheme, as shown in Fig.\n6\n.\nConcretely, we train a small family of independent upsampling networks that map\nğ’«\ncomp\n\\mathcal{P}_{\\text{comp}}\nto progressively denser point sets with\n2\nâ€‹\nN\no\n,\n3\nâ€‹\nN\no\n,\nâ€¦\n,\nk\nâ€‹\nN\no\n2N_{o},3N_{o},\\dots,kN_{o}\npoints, using the same dual-objective supervision as the main predictor.\nAt inference time, given the normalized prediction\nğ’«\ncomp\n\\mathcal{P}_{\\text{comp}}\n, we apply these upsamplers in ascending order of density.\nFor each upsampled result\nğ’«\ndense\n(\nr\n)\n\\mathcal{P}^{(r)}_{\\text{dense}}\n, we rescale it back to the original metric frame by inversely applying the normalization in Eq. (\n8\n), and compute the minimum pairwise distance between points.\nOnce this spacing falls below a prescribed resolution threshold\nÏ\nmin\n\\rho_{\\text{min}}\n, we regard the current density as sufficient for the target and stop the cascade, taking the corresponding point cloud as the final densified prediction\nğ’«\ndense\nâˆ—\n\\mathcal{P}_{\\text{dense}}^{*}\n.\nThis procedure provides scale-adaptive geometric detail while adding only modest computation and no extra supervision to the core predictor.\nFigure 7:\nOverview of our prediction-aware hierarchical planner.\nA consistency-aware global thread builds viewpoints over the predicted mesh and solves ATSP in parallel to populate a global coverage path buffer, meanwhile a real-time local thread concurrently generates minimum-time, feasible, and safe trajectories with complete target coverage.\nWe then refine\nğ’«\ndense\nâˆ—\n\\mathcal{P}_{\\text{dense}}^{*}\ninto a watertight mesh\nğ’®\nmesh\n=\n(\nV\nmesh\n,\nE\nmesh\n)\n\\mathcal{S}_{\\text{mesh}}=(V_{\\text{mesh}},E_{\\text{mesh}})\nusing NKSR (\nHuang\net al.\n2023\n).\nBefore meshing, we perform a standard hidden-point-removal operation (GHPR) (\nKatz and Tal\n2015\n) to suppress outliers and small floaters induced by noise, which improves the geometric reliability of the final surface.\nThe resulting\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\nis directly consumed by the planner, giving it a scale-appropriate, metrically consistent geometric representation without enlarging the main prediction network.\n5.3\nPrediction-aware Hierarchical Planning\nAs the decision-making component of our system, the planner addresses â€œ\nhow to scan\nâ€ under predictive foresight, both efficiently and safely.\nAlthough the perception-prediction stack transforms the original open-world problem into a clear 3D coverage task over a predicted target structure, three key difficulties prevent the direct application of off-the-shelf coverage planners (\nCao\net al.\n2020\n; Wu\net al.\n2024\n; Du\net al.\n2025\n).\nFirst, the predicted geometry is inherently time-evolving: as new observations arrive, the completed surface is progressively refined.\nExisting planners are typically designed as offline, one-shot solvers on static models and treat each updated prediction as an independent coverage problem, disregarding past plans.\nSuch snapshot-wise planning often yields paths that are locally optimal but temporally inconsistent, leading to detours or back-and-forth motions that inflate total flight time.\nThis calls for temporally consistent coverage planning, where updates to the predicted geometry induce smooth adjustments of the planned path rather than drastic reversals of flight intent.\nSecond, these methods frequently struggle to produce high-quality paths promptly due to their reliance on combinatorial optimization formulations.\nIt makes computation increasingly expensive for large targets, hindering timely adaptation to evolving predictions and potentially wasting flight time on regions derived from outdated geometry.\nTherefore, rapid computation is essential for improving mission efficiency.\nThird, even with prediction filling in unseen parts of the target, the surrounding environment remains previously unseen obstacles that may emerge at any time, requiring the planner to react with low latency to maintain safety.\nYet existing solutions tightly couple coverage path generation and trajectory optimization, causing the update rate scale with the size of the target and undermining safe, agile operation in large, cluttered environments.\nThese challenges motivate our prediction-aware hierarchical planning framework, which decouples long-horizon objectivesâ€”information completeness and mission efficiencyâ€”from the immediate requirement of real-time safety while staying consistent and computationally efficient under the evolving predicted geometry.\nThe following subsections detail this framework and its two asynchronously executed planning layers.\n5.3.1\nPlanning Framework Overview\nAs illustrated in Fig.\n7\n, our planner is organized as a two-layer hierarchy that runs asynchronously at different rates.\nA global layer computes coverage paths over the predicted target geometry at a deliberative rate\nr\nG\nr_{\\text{G}}\n, and a high-rate local layer (\nr\nL\nâ‰«\nr\nG\nr_{\\text{L}}\\gg r_{\\text{G}}\n) continuously adjusts the executable trajectory using the online map while following the intent of the global path.\nAt the higher level, the global planner targets long-horizon efficiency and structural coverage.\nAt each planning cycle, it ingests the latest predicted target mesh\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\nand rapidly constructs a globally efficient, target-centric coverage path\nğ\nG\n\\mathbf{P}_{\\text{G}}\n.\nThis path is designed to (1) cover all yet-unobserved regions of the target with short travel distance, and (2) remain consistent with the historical flight to avoid unnecessary revisits and detours.\nSpecifically, the planner first generates a minimal set of safe viewpoints\nğ’±\nG\n\\mathcal{V}_{\\text{G}}\nthat jointly observe all uncovered surface patches, and then efficiently determines a traversal order that approximately minimizes path length, inserting safe intermediate waypoints as needed to form a continuous\nğ\nG\n\\mathbf{P}_{\\text{G}}\n.\nCrucially, temporal-consistency awareness is incorporated throughout both stages, ensuring that successive refinements of\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\ninduce smooth evolutions of the coverage path.\nFurthermore, we introduce two acceleration techniques to enhance computational tractability: fast safety checking for candidate viewpoints and a skeleton-based decomposition that enables parallelized path optimization.\nAt the lower level, the local planner handles real-time safety and dynamic trajectory adaptation.\nAt each update, it queries the most recent\nğ\nG\n\\mathbf{P}_{\\text{G}}\nand extracts the portion lying within a finite planning horizon\nH\nH\nahead of the current drone position, denoted\nğ\nL\n\\mathbf{P}_{\\text{L}}\n.\nUsing the current ESDF and OGM,\nğ\nL\n\\mathbf{P}_{\\text{L}}\nis first refined by the A* algorithm (\nHart\net al.\n1968\n) to ensure it lies entirely within the free space.\nAfterwards, the local planner aims to convert\nğ\nL\n\\mathbf{P}_{\\text{L}}\ninto a minimum-time, dynamically feasible, collision-free, and smooth trajectory in real-time.\nMeanwhile, this trajectory must also preserve the intended target coverage along\nğ\nL\n\\mathbf{P}_{\\text{L}}\nto reduce detours induced by backtracking motions (Sec.\nViewpoint-constrained trajectory optimization.\n).\nTo this end, a tailored viewpoint-constrained trajectory optimization is formulated, rigorously guaranteeing the above requirements while safeguarding real-time computability.\nFinally, the resulting trajectory is transformed into low-level commands to a PID controller augmented with an online Extended State Observer (\nHan\n2009\n) for robust execution.\nThis purpose-built planning framework directly addresses the challenges outlined above: it plans a temporally consistent global coverage path timely as the predicted structure is refined, and sustains low-latency responses to newly revealed obstacles regardless of target scale (Sec.\nAsynchronous computation framework.\n).\nFigure 8:\nIllustration and comparison of safe-configuration checks in a 2D example: (A) SDF-based evaluation, (B) Ray casting, and (C) the proposed inlier-based method. The red star denotes the viewpoint candidate to be checked.\n5.3.2\nConsistency-aware Global Coverage Planning on Predicted Geometry\nFast generation of safe configuration space.\nViewpoint generation first requires a safe configuration space to exclude viewpoints inside\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\n(essential to avoid collision risks from surface penetration).\nConventional methods face a trade-off: SDFs are reliable but computationally costly for large structures, while faster ray casting algorithm suffers errors in complex non-convex geometries.\nTo reconcile safety and efficiency, our solution takes both strengths in two steps:\n(1) identifying inliers within\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\nand (2) verifying viewpoint safety via ray intersection tests (Fig.\n8\n).\nWe use a cutting plane method to compute inliers of\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\n.\nFor efficiency, we downsample\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\nvertices to a manageable number (\ne.g.\n, 100) using FPS as representatives, then determine inliers for each.\nFor each representative, we first find an optimal cutting plane (its orientation\nğ§\nopt\n\\mathbf{n}_{\\text{opt}}\n) minimizing angular variance relative to normals (\nğ’©\nv\n\\mathcal{N}_{\\text{v}}\n) of neighboring vertices.\nThis is solved iteratively via quadratic programming:\nğ§\nopt\ni\n+\n1\n=\narg\nâ¡\nmin\nğ§\nopt\nâˆˆ\nâ„\n3\n,\nâ€–\nğ§\nopt\nâ€–\n2\n=\n1\nğ§\nopt\ni\nğ–³\nâ€‹\nCov\ni\nâ€‹\n(\nğ’©\nv\n)\nâ€‹\nğ§\nopt\ni\n,\n\\mathbf{n}_{\\text{opt}}^{i+1}=\\mathop{\\arg\\min}_{\\mathbf{n}_{\\text{opt}}\\in\\mathbb{R}^{3},||\\mathbf{n}_{\\text{opt}}||_{2}=1}{\\mathbf{n}_{\\text{opt}}^{i}}^{\\mathsf{T}}\\text{Cov}^{i}(\\mathcal{N}_{\\text{v}})\\mathbf{n}_{\\text{opt}}^{i},\n(18)\nwhere\ni\ni\nis the iteration index, and\nCov\ni\nâ€‹\n(\nğ’©\nv\n)\n\\text{Cov}^{i}(\\mathcal{N}_{\\text{v}})\nis the covariance matrix of neighboring vertex normals for the current plane.\nThis quadratic problem is solved analytically via singular value decomposition, starting from a random\nğ§\nopt\n0\n\\mathbf{n}_{\\text{opt}}^{0}\n.\nUpon\nğ§\nopt\n\\mathbf{n}_{\\text{opt}}\nconvergence, the corresponding inlier\nğ©\ninlier\n\\mathbf{p}_{\\text{inlier}}\nis calculated as:\narg\nâ¡\nmin\nğ©\ninlier\nâˆˆ\nâ„\n3\nâˆ‘\nğ§\nv\nâˆˆ\nğ’©\nv\nâ€–\n(\nğ©\ninlier\nâˆ’\nğ©\nv\n)\nÃ—\nğ§\nv\nâ€–\n2\n2\n,\n\\mathop{\\arg\\min}_{\\mathbf{p}_{\\text{inlier}}\\in\\mathbb{R}^{3}}\\sum\\limits_{\\mathbf{n}_{\\text{v}}\\in\\mathcal{N}_{\\text{v}}}||(\\mathbf{p}_{\\text{inlier}}-\\mathbf{p}_{\\text{v}})\\times\\mathbf{n}_{\\text{v}}||_{2}^{2},\n(19)\nwhere\nğ©\nv\n\\mathbf{p}_{\\text{v}}\nis the position of neighboring vertices on the optimal plane.\nIntuitively,\nğ©\ninlier\n\\mathbf{p}_{\\text{inlier}}\nrepresents the closest confluence of lines through each normal in\nğ’©\nv\n\\mathcal{N}_{\\text{v}}\n; this point is robustly positioned within\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\ndue to the meshâ€™s closed-surface property.\nInliers are organized into a KD-tree for efficient querying.\nFor each viewpoint candidate, we retrieve its nearest inlier, cast a finite ray from itself to this inlier, and test intersections with\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\n.\nSafety is determined by intersection parity: an odd count indicates the viewpoint is outside the mesh, while an even count indicates it is inside.\nSec.\nFast generation of safe configuration space.\nshows this method achieves reliable safety checks with high efficiency.\nViewpoint generation.\nWe next generate a minimal viewpoint set\nğ’±\nG\n\\mathcal{V}_{\\text{G}}\nthat ensures full coverage of all currently unobserved regions on\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\n.\nEach viewpoint is parameterized by the same five DoF as in Sec.\nProblem Statement\n,\nğ¯ğ©\n=\n{\np\nx\n,\np\ny\n,\np\nz\n,\nÎ¸\n,\nÏ•\n}\n\\mathbf{vp}=\\{p_{x},p_{y},p_{z},\\theta,\\phi\\}\n.\nWe first construct a dense pool of viewpoint candidates.\nFor each unobserved vertex on\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\n, we compute its normal and sample two candidates by translating the vertex along the normal in both directions by a user-defined distance.\nUsing the fast safety evaluation described above, we discard any candidate that lies inside\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\nor violates safety, and retain the remaining ones as feasible viewpoints.\nThis dense set is then pruned using a gravitation-like model that merges viewpoints that cover fewer regions into those covering more.\nWe sort all candidates by their visible vertex counts\nv\nâ€‹\nc\ni\nvc_{i}\nin descending order.\nProcessing them in this order, for each viewpoint\ni\ni\nwe collect its neighbors\nğ’±\nN\n\\mathcal{V}_{\\text{N}}\nwithin a given radius, and then update its position according to a coverage-weighted â€œgravitationalâ€ pull:\nğ©\ni\nâ†\nğ©\ni\n+\nâˆ‘\nğ©\nn\nâˆˆ\nğ’±\nN\nv\nâ€‹\nc\nn\nv\nâ€‹\nc\ni\nâ€‹\n(\nğ©\nn\nâˆ’\nğ©\ni\n)\n,\ns.t.\nâ€‹\nv\nâ€‹\nc\nn\n<\nv\nâ€‹\nc\ni\n.\n\\mathbf{p}_{i}\\leftarrow\\mathbf{p}_{i}+\\sum\\limits_{\\mathbf{p}_{n}\\in\\mathcal{V}_{\\text{N}}}\\frac{vc_{n}}{vc_{i}}(\\mathbf{p}_{n}-\\mathbf{p}_{i}),\\,\\text{s.t.}\\,vc_{n}<vc_{i}.\n(20)\nPitch and yaw are updated in the same way.\nAll neighbors in\nğ’±\nN\n\\mathcal{V}_{\\text{N}}\nare then removed from the candidate set.\nWe repeat the above procedure until all unobserved surface regions on\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\nare covered, and the remaining viewpoints constitute the minimal set\nğ’±\nG\n\\mathcal{V}_{\\text{G}}\n.\nFigure 9:\nIllustration of skeleton-based space decomposition in castle gate. (A) Input mesh and extracted skeleton, (B) Space decomposition using the skeleton.\nRoute determination in parallel.\nAs discussed above, directly determining the optimal visiting order of\nğ’±\nG\n\\mathcal{V}_{\\text{G}}\nis computationally expensive due to its NP-hard nature, especially for large-scale target structures.\nTo make the problem tractable, we adopt a two-level decomposition strategy that splits the original problem into smaller, efficiently solvable sub-problems.\nFirst, we extract a 3D skeleton from the previously computed inliers to capture the topology of\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\n(Fig.\n9\nA).\nBased on this skeleton, the target mesh is decomposed into multiple independent subspaces characterized by simple geometry (Fig.\n9\nB), following our prior work (\nFeng\net al.\n2024\n).\nEach viewpoint in\nğ’±\nG\n\\mathcal{V}_{\\text{G}}\nis then assigned to its nearest subspace according to Euclidean distance, yielding a set of disjoint viewpoint groups.\nUnlike our previous approach, which plans directly within each subspace, we further refine this grouping to balance computational load.\nSpecifically, for any subspace whose viewpoint group spans a radius larger than a predefined threshold\nR\ng\nR_{\\text{g}}\n, we construct a visibility graph where edges connect pairs of viewpoints with unobstructed line-of-sight (\ni.e.\n, no occlusion by\nğ’®\nmesh\n\\mathcal{S}_{\\text{mesh}}\n).\nThe viewpoints are then partitioned into approximately uniform, inter-visible convex groups, each with radius below\nR\ng\nR_{\\text{g}}\n.\nThis decomposition reduces the original route planning problem into two Asymmetric Traveling Salesperson Problems (ATSPs): (1) optimizing the group sequence; (2) finding the optimal traversal order of viewpoints within each group.\nBoth problems are solved using a mature ATSP solver (\nHelsgaun\n2000\n) and can be processed in parallel across groups.\nThis two-level strategy offers two key advantages: (1) it significantly reduces problem dimensionality and enables inherent parallelization (\ne.g.\n, decomposing a single\n100\nÃ—\n100\n100\\times 100\nproblem to four parallel\n25\nÃ—\n25\n25\\times 25\nproblems). (2) Group convexity obviates the need for collision checks among intra-group viewpoints.\nWhile not guaranteeing strict global optimality, it empirically achieves a favorable balance between computational efficiency and path quality.\nFigure 10:\nIllustration of consistency-promoting designs in global planning: (A) Reuse prior viewpoints for stable coverage across cycles; (B) Match prior groups to yield a temporally coherent global sequence.\nShown in 2D for clarity.\nInjecting consistency awareness.\nAs previously emphasized, the global path\nğ\nG\n\\mathbf{P}_{\\text{G}}\nshould also align with historical flight.\nThus, we integrate simple yet effective consistency-promoting designs into each step of the global planning process, as shown in Fig.\n10\n.\nFirst, for unobserved regions whose geometry remains unchanged since the last planning cycle, we preferentially reuse viewpoints from the previous set to minimize temporal variations in the viewpoint distribution (Fig.\n10\nA).\nBefore current viewpoint generation, we evaluate viewpoint reuse by comparing past and current observations:\nReuseVP\nâ€‹\n(\nğ¯ğ©\n)\n=\n{\nTrue\n,\nCD\nu\nâ€‹\n(\nğ’ª\nğ¯ğ©\nlast\n,\nğ’ª\nğ¯ğ©\ncur\n)\n<\nÏƒ\nFalse\n,\nelse\n\\text{ReuseVP}(\\mathbf{vp})=\\left\\{\\begin{array}[]{rcl}\\text{True},&&{\\text{CD}_{\\text{u}}(\\mathcal{O}^{\\text{last}}_{\\mathbf{vp}},\\mathcal{O}^{\\text{cur}}_{\\mathbf{vp}})<\\sigma}\\\\\n\\text{False},&&{\\text{else}}\\end{array}\\right.\n(21)\nwhere\nÏƒ\n\\sigma\nis the maximum of the average edge lengths of the previous and current meshes;\nğ’ª\nğ¯ğ©\nlast\n\\mathcal{O}^{\\text{last}}_{\\mathbf{vp}}\nand\nğ’ª\nğ¯ğ©\ncur\n\\mathcal{O}^{\\text{cur}}_{\\mathbf{vp}}\nare the last and current observations for viewpoint\nğ¯ğ©\n\\mathbf{vp}\n.\nAlgorithm 2\nConsistency-aware Global Coverage Planning\n1:\nCurrent predicted mesh\nğ’®\nmesh\ncur\n\\mathcal{S}_{\\text{mesh}}^{\\text{cur}}\n2:\nNotation:\nInliers\nğˆ\n\\mathbf{I}\n, skeleton\nğ’¬\n\\mathcal{Q}\n, viewpoint set\nğ’±\nG\n\\mathcal{V}_{\\text{G}}\n, viewpoint groups\nğ’¢\nğ’±\n\\mathcal{G}_{\\mathcal{V}}\n, group centroids\nâ„°\n\\mathcal{E}\n, group sequence\nS\nâ€‹\ne\nâ€‹\nq\nG\nSeq_{\\text{G}}\n, global path\nğ\nG\n\\mathbf{P}_{\\text{G}}\n, reused viewpoints\nğ’±\nR\n\\mathcal{V}_{\\text{R}}\n, matched sequence\nS\nâ€‹\ne\nâ€‹\nq\nM\nSeq_{\\text{M}}\n3:\nğˆ\nâ†\nConfigurationSpaceGeneration\nâ€‹\n(\nğ’®\nmesh\ncur\n)\n\\mathbf{I}\\leftarrow\\text{ConfigurationSpaceGeneration}(\\mathcal{S}_{\\text{mesh}}^{\\text{cur}})\n4:\nif\nğ’±\nG\nlast\nâ‰ \nâˆ…\n\\mathcal{V}_{\\text{G}}^{\\text{last}}\\neq\\emptyset\nthen\n5:\nğ’±\nR\nâ†\nFindReusedViewpoints\nâ€‹\n(\nğ’±\nG\nlast\n,\nğ’®\nmesh\ncur\n,\nğ’®\nmesh\nlast\n,\nğˆ\n)\n\\mathcal{V}_{\\text{R}}\\leftarrow\\text{FindReusedViewpoints}(\\mathcal{V}_{\\text{G}}^{\\text{last}},\\mathcal{S}_{\\text{mesh}}^{\\text{cur}},\\mathcal{S}_{\\text{mesh}}^{\\text{last}},\\mathbf{I})\n6:\nelse\nğ’±\nR\n=\nâˆ…\n\\mathcal{V}_{\\text{R}}=\\emptyset\n7:\nend\nif\n8:\nğ’±\nG\ncur\nâ†\nViewpointGeneration\nâ€‹\n(\nğ’±\nR\n,\nğ’®\nmesh\ncur\n,\nğˆ\n)\n\\mathcal{V}_{\\text{G}}^{\\text{cur}}\\leftarrow\\text{ViewpointGeneration}(\\mathcal{V}_{\\text{R}},\\mathcal{S}_{\\text{mesh}}^{\\text{cur}},\\mathbf{I})\n9:\nğ’¬\nâ†\nSkeletonExtraction\nâ€‹\n(\nğˆ\n)\n\\mathcal{Q}\\leftarrow\\text{SkeletonExtraction}(\\mathbf{I})\n10:\nğ’¢\nğ’±\ncur\n,\nâ„°\ncur\nâ†\nViewpointGrouping\nâ€‹\n(\nğ’±\nG\ncur\n,\nâ„°\nlast\n,\nğ’¬\n)\n\\mathcal{G}_{\\mathcal{V}}^{\\text{cur}},\\mathcal{E}^{\\text{cur}}\\leftarrow\\text{ViewpointGrouping}(\\mathcal{V}_{\\text{G}}^{\\text{cur}},\\mathcal{E}^{\\text{last}},\\mathcal{Q})\n11:\nif\nS\nâ€‹\ne\nâ€‹\nq\nG\nlast\nâ‰ \nâˆ…\nSeq_{\\text{G}}^{\\text{last}}\\neq\\emptyset\nthen\n12:\nS\nâ€‹\ne\nâ€‹\nq\nM\n,\nğ’¢\nğ’±\ncur\nâ†\nSequenceMatching\nâ€‹\n(\nS\nâ€‹\ne\nâ€‹\nq\nG\nlast\n,\nğ’¢\nğ’±\ncur\n)\nSeq_{\\text{M}},\\mathcal{G}_{\\mathcal{V}}^{\\text{cur}}\\leftarrow\\text{SequenceMatching}(Seq_{\\text{G}}^{\\text{last}},\\mathcal{G}_{\\mathcal{V}}^{\\text{cur}})\n13:\nelse\nS\nâ€‹\ne\nâ€‹\nq\nM\n=\nâˆ…\nSeq_{\\text{M}}=\\emptyset\n14:\nend\nif\n15:\nS\nâ€‹\ne\nâ€‹\nq\nG\ncur\nâ†\nFindGroupSequence\nâ€‹\n(\nS\nâ€‹\ne\nâ€‹\nq\nM\n,\nğ’¢\nğ’±\ncur\n)\nSeq_{\\text{G}}^{\\text{cur}}\\leftarrow\\text{FindGroupSequence}(Seq_{\\text{M}},\\mathcal{G}_{\\mathcal{V}}^{\\text{cur}})\n16:\nğ\nG\nâ†\nParallelATSPSolving\nâ€‹\n(\nS\nâ€‹\ne\nâ€‹\nq\nG\ncur\n,\nğ’¢\nğ’±\ncur\n)\n\\mathbf{P}_{\\text{G}}\\leftarrow\\text{ParallelATSPSolving}(Seq_{\\text{G}}^{\\text{cur}},\\mathcal{G}_{\\mathcal{V}}^{\\text{cur}})\n17:\nğ\nG\n\\mathbf{P}_{\\text{G}}\nSecond, to bolster temporal consistency in viewpoint clustering, group centroids from the preceding iteration serve as initial seeds for the current clustering.\nThird, to avoid erratic flight from unstable group visiting sequences, a matching step (Fig.\n10\nB) is added before invoking the ATSP solver: we align the first\nm\nm\ngroups from the historical sequence with current clustering results, via:\narg\nâ¡\nmin\ng\ncur\nâˆˆ\nğ’¢\nğ’±\ncur\nCD\nd\nâ€‹\n(\ng\nlast\n,\ng\ncur\n)\n,\n\\mathop{\\arg\\min}_{g^{\\text{cur}}\\in\\mathcal{G}_{\\mathcal{V}}^{\\text{cur}}}\\text{CD}_{\\text{d}}(g^{\\text{last}},g^{\\text{cur}}),\n(22)\nwhere\nğ’¢\nğ’±\ncur\n\\mathcal{G}_{\\mathcal{V}}^{\\text{cur}}\nis the current cycleâ€™s groups, and\ng\nlast\ng^{\\text{last}}\nis a group from the previous sequence.\nThis matching is done sequentially for each of the first\nm\nm\nhistorical groups; once matched, the current group is removed from\nğ’¢\nğ’±\ncur\n\\mathcal{G}_{\\mathcal{V}}^{\\text{cur}}\nto prevent duplicate assignments.\nCollectively, as summarized in Algo.\n2\n, the global planner integrates these careful designs to yield a temporally consistent global path\nğ\nG\n\\mathbf{P}_{\\text{G}}\nwith complete coverage, ready for the local planner to access.\nSec.\nConsistency-aware global planning.\ndemonstrates the efficiency improvement contributed by consistency-aware designs.\n5.3.3\nViewpoint-constrained Trajectory Optimization\nFor simultaneous optimization of spatial configuration and temporal allocation, the local trajectory\nğ±\nL\nâ€‹\n(\nt\n)\n\\mathbf{x}_{\\text{L}}(t)\nis parameterized via a differentiable piecewise polynomial class, MINCO (\nWang\net al.\n2022\n):\nğ±\nL\n(\nt\n)\n=\n{\nz\ni\n(\nt\n)\n=\nğœ\ni\nğ–³\nÏˆ\n(\nt\n)\nâˆˆ\nâ„\n5\n,\nt\nâˆˆ\n[\n0\n,\nT\ni\n]\n|\ni\n=\n1\n,\nâ€¦\n,\nK\n}\n,\n\\mathbf{x}_{\\text{L}}(t)=\\left\\{z_{i}(t)=\\mathbf{c}_{i}^{\\mathsf{T}}\\psi(t)\\in\\mathbb{R}^{5},t\\in[0,T_{i}]|i=1,\\dots,K\\right\\},\n(23)\nğœ\ni\nâˆˆ\nâ„\n(\n2\nâ€‹\ns\nâˆ’\n1\n)\nÃ—\n5\n,\nÏˆ\nâ€‹\n(\nt\n)\n=\n[\n1\n,\nt\n,\nt\n2\n,\nâ€¦\n,\nt\n2\nâ€‹\ns\nâˆ’\n1\n]\nğ–³\n,\n\\mathbf{c}_{i}\\in\\mathbb{R}^{(2s-1)\\times 5},\\psi(t)=\\left[1,t,t^{2},\\dots,t^{2s-1}\\right]^{\\mathsf{T}},\n(24)\nwhere the trajectory comprises\nK\nK\npolynomial pieces.\nThe coefficients matrix\nğœ\ni\n\\mathbf{c}_{i}\nand temporal basis function\nÏˆ\nâ€‹\n(\nt\n)\n\\psi(t)\ndepend on the polynomial degree\nd\nd\n.\nMINCO reformulates\nğœ\ni\n\\mathbf{c}_{i}\nvia a differentiable mapping\nâ„³\nâ€‹\n(\nâ‹…\n)\n\\mathcal{M}(\\cdot)\nusing intermediate points\nğ\n=\n{\nğª\ni\nâˆˆ\nâ„\n5\n|\ni\n=\n0\n,\n1\n,\nâ€¦\n,\nK\n}\n\\mathbf{Q}=\\left\\{\\mathbf{q}_{i}\\in\\mathbb{R}^{5}|i=0,1,\\dots,K\\right\\}\n(on the refined\nğ\nL\n\\mathbf{P}_{\\text{L}}\n) and time profiles\nğ“\n=\n{\nT\ni\n|\ni\n=\n1\n,\nâ€¦\n,\nK\n}\n\\mathbf{T}=\\left\\{T_{i}|i=1,\\dots,K\\right\\}\n:\nğœ\ni\n=\nâ„³\nâ€‹\n(\nğª\ni\n,\nT\ni\n)\n.\n\\mathbf{c}_{i}=\\mathcal{M}(\\mathbf{q}_{i},T_{i}).\n(25)\nThis enables efficient spatio-temporal trajectory deformation by manipulating\nğ\n\\mathbf{Q}\nand\nğ“\n\\mathbf{T}\n, highly advantageous for gradient-based optimization.\nFor brevity, we split\nâ„³\nâ€‹\n(\nâ‹…\n)\n\\mathcal{M}(\\cdot)\ninto positional (\nâ„³\npos\nâˆˆ\nâ„\n(\n2\nâ€‹\ns\nâˆ’\n1\n)\nÃ—\n3\n\\mathcal{M}^{\\text{pos}}\\in\\mathbb{R}^{(2s-1)\\times 3}\n) and orientational (\nâ„³\nori\nâˆˆ\nâ„\n(\n2\nâ€‹\ns\nâˆ’\n1\n)\nÃ—\n2\n\\mathcal{M}^{\\text{ori}}\\in\\mathbb{R}^{(2s-1)\\times 2}\n) components:\nğœ\ni\n=\nâ„³\npos\nâ€‹\n(\nğª\ni\n,\nT\ni\n)\nâŠ•\nâ„³\nori\nâ€‹\n(\nğª\ni\n,\nT\ni\n)\n,\n\\mathbf{c}_{i}=\\mathcal{M}^{\\text{pos}}(\\mathbf{q}_{i},T_{i})\\oplus\\mathcal{M}^{\\text{ori}}(\\mathbf{q}_{i},T_{i}),\n(26)\nz\ni\npos\nâ€‹\n(\nt\n)\n=\nâ„³\npos\nâ€‹\n(\nğª\ni\n,\nT\ni\n)\nâ€‹\nÏˆ\nâ€‹\n(\nt\n)\n,\nz\ni\nori\nâ€‹\n(\nt\n)\n=\nâ„³\nori\nâ€‹\n(\nğª\ni\n,\nT\ni\n)\nâ€‹\nÏˆ\nâ€‹\n(\nt\n)\n,\nz_{i}^{\\text{pos}}(t)=\\mathcal{M}^{\\text{pos}}(\\mathbf{q}_{i},T_{i})\\psi(t),z_{i}^{\\text{ori}}(t)=\\mathcal{M}^{\\text{ori}}(\\mathbf{q}_{i},T_{i})\\psi(t),\n(27)\ngenerating coefficients for 3D position and orientation (\ni.e.\n, pitch and yaw angles), respectively.\nWe then formulate the trajectory optimization problem as:\nmin\nğ\n,\nğ“\n\\displaystyle\\mathop{\\min}_{\\mathbf{Q},\\mathbf{T}}\\;\\;\nâˆ‘\ni\n=\n1\nK\nâˆ«\n0\nT\ni\nâ€–\n(\nz\ni\npos\nâ€‹\n(\nt\n)\n)\n(\nd\n)\nâ€–\n2\n2\nâ€‹\nğ‘‘\nt\n+\nw\nt\nâ€‹\nT\ni\n,\n\\displaystyle\\sum_{i=1}^{K}\\int_{0}^{T_{i}}\\|(z_{i}^{\\text{pos}}(t))^{(d)}\\|_{2}^{2}\\,dt+w_{\\text{t}}T_{i},\n(28a)\ns.t.\nâˆ’\nT\ni\n<\n0\n,\nâˆ’\nÎ¦\nâ€‹\n(\nz\ni\npos\nâ€‹\n(\nt\n)\n)\n<\n0\n,\n\\displaystyle-T_{i}<0,\\;-\\Phi(z_{i}^{\\text{pos}}(t))<0,\n(28b)\ng\ndyn\nâ€‹\n(\nz\ni\npos\nâ€‹\n(\nt\n)\n,\nz\ni\nori\nâ€‹\n(\nt\n)\n)\n<\n0\n,\n\\displaystyle g_{\\text{dyn}}(z_{i}^{\\text{pos}}(t),z_{i}^{\\text{ori}}(t))<0,\n(28c)\nC\nâ€‹\n(\nğ±\nL\nâ€‹\n(\nt\n)\n)\nâˆ’\nC\nâ€‹\n(\nğ\nL\n)\n=\n0\n.\n\\displaystyle C(\\mathbf{x}_{\\text{L}}(t))-C(\\mathbf{P}_{\\text{L}})=0.\n(28d)\nThe integral term promotes smoothness via the squared\nd\nd\n-th derivative of position, and\nw\nt\nâ€‹\nT\ni\nw_{\\text{t}}T_{i}\npenalizes total duration with weight\nw\nt\nw_{\\text{t}}\n.\nThe inequality constraints ensure positive time allocation, collision avoidance (positive ESDF values\nÎ¦\nâ€‹\n(\nâ‹…\n)\n\\Phi(\\cdot)\n), and dynamic feasibility (bounded velocity, acceleration, jerk, and angular velocity).\nEq. (\n28d\n) encodes coverage: ideally, the executed trajectory\nğ±\nL\nâ€‹\n(\nt\n)\n\\mathbf{x}_{\\text{L}}(t)\nshould achieve the same target coverage as the input path\nğ\nL\n\\mathbf{P}_{\\text{L}}\n.\nHowever, the coverage functional\nC\nâ€‹\n(\nâ‹…\n)\nC(\\cdot)\nis a global, non-differentiable visibility measure, and enforcing (\n28d\n) directly would lead to a highly non-convex mixed-integer problem, unsuitable for real-time optimization.\nFortunately, by construction of the global coverage path, full coverage over the current horizon is achieved precisely by aggregating observations from all viewpoints embedded in\nğ\nL\n\\mathbf{P}_{\\text{L}}\n.\nThis allows us to replace the abstract coverage constraint (\n28d\n) with a concrete geometric requirement: the optimized trajectory must traverse every viewpoint in\nğ\nL\n\\mathbf{P}_{\\text{L}}\n.\nSince\nğ\nL\n\\mathbf{P}_{\\text{L}}\nincludes both viewpoints and waypoints, we decompose\nğ\n\\mathbf{Q}\ninto\nW\nW\noptimizable waypoints (\nğ\nw\nâˆˆ\nâ„\nW\nÃ—\n5\n\\mathbf{Q}_{\\text{{w}}}\\in\\mathbb{R}^{W\\times 5}\n) and\nG\nG\nfixed viewpoints (\nğ\nv\nâˆˆ\nâ„\nG\nÃ—\n5\n\\mathbf{Q}_{\\text{{v}}}\\in\\mathbb{R}^{G\\times 5}\n).\nğ\n\\mathbf{Q}\nis constructed using binary selection matrices\nğ’\nw\n\\mathbf{S}_{\\text{{w}}}\nand\nğ’\nv\n\\mathbf{S}_{\\text{{v}}}\n:\nğ\n=\nğ’\nw\nâ€‹\nğ\nw\n+\nğ’\nv\nâ€‹\nğ\nv\nâˆˆ\nâ„\n(\nW\n+\nG\n)\nÃ—\n5\n,\n\\mathbf{Q}=\\mathbf{S}_{\\text{{w}}}\\mathbf{Q}_{\\text{{w}}}+\\mathbf{S}_{\\text{{v}}}\\mathbf{Q}_{\\text{{v}}}\\in\\mathbb{R}^{(W+G)\\times 5},\n(29)\nwhere\nğ’\nw\n\\mathbf{S}_{\\text{{w}}}\nand\nğ’\nv\n\\mathbf{S}_{\\text{{v}}}\nsatisfy Kronecker Delta Function\nÎ´\ni\n,\nidx\nâ€‹\n(\ni\n)\n\\delta_{i,\\text{idx}(i)}\n(\nidx\nâ€‹\n(\nâ‹…\n)\n\\text{idx}(\\cdot)\ndenotes the index of a point (waypoint or viewpoint) on the local path).\nThis ensures\nğ±\nL\nâ€‹\n(\nt\n)\n\\mathbf{x}_{\\text{L}}(t)\npasses through fixed viewpoints while optimizing via waypoints.\nTo ensure strictly positive\nT\ni\nT_{i}\nwithout explicit inequality constraints, we introduce unconstrained virtual variables\nğ’¯\n=\n{\nÏ„\ni\n}\n\\mathcal{T}=\\{\\tau_{i}\\}\nand map them to\nT\ni\nT_{i}\nthrough a smooth diffeomorphism\nÎ˜\nâ€‹\n(\nâ‹…\n)\n\\Theta(\\cdot)\n:\nT\ni\n=\nÎ˜\nâ€‹\n(\nÏ„\ni\n)\n=\n{\n1\n+\nÏ„\ni\n+\nÏ„\ni\n2\n2\n,\nÏ„\ni\nâ‰¥\n0\n,\n2\n2\nâˆ’\n2\nâ€‹\nÏ„\ni\n+\nÏ„\ni\n2\n,\nÏ„\ni\n<\n0\n,\nT_{i}=\\Theta(\\tau_{i})=\\begin{cases}1+\\tau_{i}+\\dfrac{\\tau_{i}^{2}}{2},&\\tau_{i}\\geq 0,\\\\[4.0pt]\n\\dfrac{2}{2-2\\tau_{i}+\\tau_{i}^{2}},&\\tau_{i}<0,\\end{cases}\n(30)\nwhich yields\nT\ni\n>\n0\nT_{i}>0\nfor all\nÏ„\ni\nâˆˆ\nâ„\n\\tau_{i}\\in\\mathbb{R}\n.\nFinally, we convert the inequality constraints to soft penalties by Smooth L1 loss terms and obtain an unconstrained objective:\nmin\nğ\nw\n,\nğ’¯\nğ’¥\ns\n+\nw\nt\nâ€‹\nğ’¥\nt\n+\nw\nd\nâ€‹\nğ’¥\nd\n+\nw\nc\nâ€‹\nğ’¥\nc\n+\nw\nsc\nâ€‹\nCov\nâ€‹\n(\nğ\nw\n)\n+\nw\ntc\nâ€‹\nCov\nâ€‹\n(\nğ’¯\n)\n.\n\\mathop{\\min}_{\\mathbf{Q}_{\\text{w}},\\mathcal{T}}\\mathcal{J}_{\\text{s}}+w_{\\text{t}}\\mathcal{J}_{\\text{t}}+w_{\\text{d}}\\mathcal{J}_{\\text{d}}+w_{\\text{c}}\\mathcal{J}_{\\text{c}}+w_{\\text{sc}}\\mathrm{Cov}(\\mathbf{Q}_{\\text{w}})+w_{\\text{tc}}\\mathrm{Cov}(\\mathcal{T}).\n(31)\nHere,\nğ’¥\ns\n\\mathcal{J}_{\\text{s}}\ncorresponds to the smoothness objective and\nğ’¥\nt\n\\mathcal{J}_{\\text{t}}\npenalizes total flight time, while\nğ’¥\nd\n\\mathcal{J}_{\\text{d}}\nand\nğ’¥\nc\n\\mathcal{J}_{\\text{c}}\nare Smooth L1 penalties that softly enforce dynamic feasibility and collision avoidance.\nThe covariance regularizers\nCov\nâ€‹\n(\nğ\nw\n)\n\\mathrm{Cov}(\\mathbf{Q}_{\\text{w}})\nand\nCov\nâ€‹\n(\nğ’¯\n)\n\\mathrm{Cov}(\\mathcal{T})\ndiscourage spatially oscillatory control points and highly uneven time allocation across segments, improving numerical stability.\nThe positive scalars\nw\nw\nare weighting coefficients that balance these terms.\nWe subsequently solve this unconstrained problem efficiently to produce the desired\nğ±\nL\nâ€‹\n(\nt\n)\n\\mathbf{x}_{\\text{L}}(t)\nby employing a gradient-based approach, notably the L-BFGS algorithm (\nLiu and Nocedal\n1989\n).\n6\nImplementation Details\n6.0.1\nPerception\nWe use the official small version of SAM2 as the segmentation FM to meet real-time requirements (\nâˆ¼\n2\n\\sim 2\nHz), and set the sliding-window memory bank capacity to\nW\nm\n=\n30\nW_{m}=30\n.\nIn cross-modal refinement,\nW\ns\n=\n7\nW_{s}=7\nkeyframes are kept for memory attention and the reinitialization threshold is set to\nÎº\nreinit\n=\n0.7\n\\kappa_{\\text{reinit}}=0.7\n.\nFor 3D mapping, we refer to an efficient volumetric method (\nHan\net al.\n2019\n) with a voxel resolution of\n0.1\n0.1\nm and\n10\n10\n-Hz update.\nFigure 11:\nA representative sample of the diverse, high-fidelity 3D assets used for multi-modal surface predictor training.\n6.0.2\nPrediction\nFor the main predictor, the geometric input and output are fixed to\nN\ni\n=\n2048\nN_{i}=2048\nand\nN\no\n=\n2048\nN_{o}=2048\npoints, respectively.\nThe DINO image encoder produces\nK\n=\n324\nK=324\npatch tokens per view, and\nfeature dimensions are set to\nD\nt\n=\n768\nD_{\\text{t}}=768\n,\nD\nâ„\n=\n768\nD_{\\mathcal{I}}=768\n, and\nD\nğ’«\n=\n512\nD_{\\mathcal{P}}=512\n.\nAll modalities are projected to a shared fusion space of\ndimension\nD\nf\n=\n512\nD_{\\text{f}}=512\n.\nIn the fusion stage, we stack\nL\n=\n10\nL=10\nalternating-attention layers as described.\nThe training dataset is constructed from a diverse collection of approximately\n70\n,\n000\n70,000\nhigh-fidelity distinct 3D models, including a curated subset of Objaverse (\nDeitke\net al.\n2023\n) and a small number of additional real-world scans from the Internet.\nFig.\n11\nshowcases a representative sample of these models, illustrating the datasetâ€™s broad coverage of object categories and scales.\nEach asset is processed by our automatic multi-modal pipeline to produce aligned tuples as training samples.\nWe implement the predictor in PyTorch and train on a server with eight NVIDIA RTX4090 GPUs, dual Intel Xeon CPUs, and\n512\n512\nGB RAM.\nWe use Distributed Data Parallel with a per-GPU batch size of\n42\n42\n(effective batch size\n336\n336\n), automatic mixed precision, and activation checkpointing to improve throughput and reduce memory usage.\nThe model is trained for\n50\n50\nepochs (about\n72\n72\nhours) using AdamW with an initial learning rate of\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\nand weight decay of\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\n.\nA cosine-annealing-with-warm-restarts scheduler restarts every epoch and decays the learning rate to a minimum of\n1\nÃ—\n10\nâˆ’\n5\n1\\times 10^{-5}\n.\nIn inference-time geometry densification, we use Snowflake Point Deconvolution in SnowflakeNet (\nXiang\net al.\n2021\n) as the base upsampling architecture, with upsampling factor\nk\n=\n10\nk=10\nand resolution threshold\nÏ\nmin\n=\n0.5\n\\rho_{\\text{min}}=0.5\nm in all experiments.\nThese upsamplers share the same training settings as the predictor but are trained for fewer epochs.\n6.0.3\nPlanning\nFor the hierarchical planner, the global and local layers run in separate threads with\nr\nG\n=\n1\nr_{\\text{G}}=1\nHz and\nr\nL\n=\n20\nr_{\\text{L}}=20\nHz, respectively.\nIn global planning, the viewpoint grouping radius is\nR\ng\n=\n8\nR_{\\text{g}}=8\nm and we match the first\nm\n=\n2\nm=2\ngroups in the historical sequence when performing sequence alignment.\nThe local planning horizon is set to\nH\n=\n10\nH=10\nm along the current global path and the MINCO parameterization uses\nd\n=\n4\nd=4\n(\ni.e.\n, snap) with one polynomial segment per edge of the A*-refined\nğ\nL\n\\mathbf{P}_{\\text{L}}\n.\nThe penalty weights in Eq. (\n31\n) are chosen as\nw\nt\n=\n60.0\nw_{\\text{t}}=60.0\n,\nw\nd\n=\n500.0\nw_{\\text{d}}=500.0\n,\nw\nc\n=\n500.0\nw_{\\text{c}}=500.0\n,\nw\nsc\n=\n60.0\nw_{\\text{sc}}=60.0\n, and\nw\ntc\n=\n80.0\nw_{\\text{tc}}=80.0\n, which we empirically found to give a good trade-off among smoothness, flight time, dynamic feasibility, and safety.\nOptimization is solved with a maximum of\n100\n100\niterations and a gradient tolerance of\n10\nâˆ’\n5\n10^{-5}\n.\n7\nExperiments\nIn this section, we present a comprehensive evaluation of\nF\nly\nC\no in both real-world and simulated settings.\nOur experiments are organized to answer three complementary questions:\n1)\nPracticality:\nCan the system reliably and precisely execute fully autonomous, prompt-grounded target structure scanning missions in diverse in-the-wild environments under real sensing, computation, and constraints?\n2)\nPerformance:\nUnder fair conditions, how does\nF\nly\nC\no compare with representative SOTA paradigms in terms of efficiency, data integrity, safety, run-to-run stability, and required human priors?\n3)\nAttribution:\nWhich components and design choices in perception, prediction, and planning account for improvements in overall system performance?\nWe address the first question through extensive field deployments in Sec.\nReal-world Experiments\n.\nAfterwards, Sec.\nComparisons with State-of-the-Art Methods\ndemonstrates that\nF\nly\nC\no represents a more advanced system architecture, achieving a superior efficiency-completeness trade-off with high reliability while requiring substantially lower-effort human priors.\nLastly, thorough ablation experiments in Sec.\nAblation Studies\nconfirm the individual contributions of each key module and design in our system.\nFigure 12:\nHardware platform and communication setup for real-world flights.\n7.1\nReal-world Experiments\nHere we aim to substantiate that\nF\nly\nC\no effectively leverages FMs to enable drones autonomously and accurately scan human-specified 3D structures in diverse open-world environments, relying only on intuitive, low-effort human prompts.\nWe present three proof-of-concept yet challenging experimentsâ€”conducted across four diverse in-the-wild sitesâ€”that demonstrate precise scene understanding, high efficiency, real-time safety, and reliable adaptability.\nIn all trials, our system accomplished the missions in a single attempt without re-flights or additional operator intervention, despite receiving only high-level human inputs, underscoring its practical viability and real-world applicability.\nThe rest of this part first describes the hardware platform and experimental setup, then details the three representative scanning experiments, and finally profiles the real-time performance and computational load of each module.\nFigure 13:\nPrecise scene understanding for adaptive scanning.\n(A) The target structure of human interest: an arch bridge spanning between two large buildings.\n(B) Human-inputted visual and textual prompts for target specification.\n(C) Left\nâ†’\n\\rightarrow\nright: precise visual/geometric target segmentation and reasonable surface prediction at three timestamps, even under the challenges of similar texture and spatial connection.\n(D) Demonstration of adaptively completing the scanning mission solely around the target and three corresponding real-world snapshots of this scanning flight shown in D(1-3).\n(E) Targeted 3D reconstruction with fine-grained details focusing only on the human-specified arch bridge.\n7.1.1\nHardware Platform and Experimental Setup\nAs the drone shown in Fig.\n12\n, the aerial platform of\nF\nly\nC\no is modified from a DJI M30 quadcopter (\nDJI\n2025a\n).\nFollowing our customization, it possesses a takeoff weight of\n4.5\n4.5\nkg, a\n668\n668\nmm wheelbase, and a\n0.55\n0.55\nm radius.\nFor sensing, we equip this drone with a Livox Mid360 3D LiDAR (\nLivox\n2025\n) and a DJI gimbal-mounted camera.\nThe former provides a maximum sensing range of\n70\n70\nm and a point acquisition rate of\n200\n,\n000\n200,000\nHz for dense geometric scanning.\nIts integrated inertial measurement unit (IMU) provides\n50\n50\nHz updates, which are crucial for accurate localization and mapping.\nConcurrently, the latter captures\n1920\nÃ—\n1080\n1920\\times 1080\nresolution RGB images at 30 frames per second and supports a pitch rotation range from\n+\n45\nâˆ˜\n+45\\,^{\\circ}\nto\nâˆ’\n100\nâˆ˜\n-100\\,^{\\circ}\n, crucial for flexible textural details capture.\nOnboard computation is handled by an NVIDIA Jetson Orin NX (\nNVIDIA\n2025\n) that includes an Ampere GPU and an Arm Cortex-A78AE v8.2 64-bit CPU.\nRemaining structural and electronic components adhere to a lightweight and compact design principle, collectively contributing to a flight endurance of approximately\n24\n24\nminutes on a single charge.\nIn addition, a personal laptop, featuring an Intel 24-core 5.4-GHz i9-13900HX CPU and an NVIDIA RTX4060 GPU, serves as the ground station, offering an intuitive interface for specifying user prompts and visualizing the flight in real-time.\nSince the prediction module is computationally intensive for the edge device but not strictly latency-critical (empirically below\n1\n1\ns is acceptable), it is executed on this laptop, while all other modules run on the onboard computer.\nCommunication between onboard and ground devices is maintained through a 4G cellular network, augmented by data compression techniques to ensure real-time and stable data transmission.\nThe maximum speed and gimbal angular speed of the drone are limited to\n1.0\n1.0\nm/s and 20\nâˆ˜\n/s, respectively, to balance perception quality and flight efficiency.\nNo prior site information or pre-built environmental maps about the experimental scenarios are provided to the drone.\nThe only human input is a one-sentence textual description together with sparse visual annotations at the start of each mission, where no manual intervention is allowed during flight.\n7.1.2\nPrecise Scene Understanding for Adaptive Scanning\nWe first validate our systemâ€™s ability to reliably understand the physical world, a capability enabled by foundation models and fundamental to translating abstract user prompts into accurate drone scanning actions.\nA key aspect of this understanding mirrors human perception: humans not only identify discrete structures but effortlessly recognize key parts (\ne.g.\na houseâ€™s front door or a teacupâ€™s handle) (\nHoffman and Richards\n1984\n; Biederman\n1987\n), focusing on these meaningful components.\nThis â€œpart attentionâ€ demands more nuanced scene understandingâ€”including fine-grained part-whole relationshipsâ€”than recognizing standalone structures.\nConsequently, adapting to scan specific parts of a larger structure is far more challenging than scanning entire ones, particularly when the target is visually interconnected with its surroundings.\nThis raises a critical question: Can drones develop precise scene understanding to adaptively scan user-specified targets, even when they are integral parts of larger, interconnected entities?\nTo answer this, we performed a real-world experiment in which the system was instructed to scan a distinctive arch bridge connecting large buildings on both sides (target highlighted in yellow, Fig.\n13\nA).\nWith our system, the user was only required to pick the target arch bridge by two simple steps: first, drawing a rough bounding box around the desired region in the camera feed; second, providing a simple text description that explicitly mentions the arch bridge (Fig.\n13\nB).\nEven though the inputs were not very precise, and the bridge looked visually similar to the nearby walls, our system still successfully segmented the target structure in the 2D image (Fig.\n13\nC, first two columns).\nThis achievement derives from the fusion of visual and textual understanding of the target from FMs: these modalities together established a reliable semantic reference, effectively clearing up ambiguities caused by imprecise user inputs and visual similarity.\nNext, this detailed 2D segmentation helped accurately split the 3D point cloud using projective geometry.\nThis step cleanly and stably separated the bridge from the connected buildings it attached to, despite their physical adjacency (Fig.\n13\nC, middle two columns).\nCrucially, boosted by 2D-3D cross-modal refinement, our perception module effectively transfers the world understanding of FMs into 3D space, enabling the drone to continuously track the target bridge throughout the flight, even as the droneâ€™s viewpoint shifted dynamically (Fig.\n13\nC).\nThis process of linking the userâ€™s request to the real-world target stayed stable and consistent throughout the entire mission.\nDrawing on these accurate perception results, our predictor inferred the complete surface of the arch bridge, even when some of its pillars were occluded from view by others (Fig.\n13\nC, right column).\nAn initial deviation in the height estimation of the upper walkway was progressively reduced as the drone collected more observations during flight.\nBuilding on this precise scene understanding and predictive foresight, the planner then generated adaptive, target-centric scanning paths that compactly encircled the bridge, precluding time waste on non-target regions (Fig.\n13\nD).\nDuring flight, it adapted trajectories in real-time to safely avoid obstaclesâ€”for instance, navigating between bridge pillars, bypassing nearby walls and buildingsâ€”while simultaneously ensuring efficient, comprehensive coverage of the specified target structure.\nOnce completing its missions, the drone transmitted all visual and geometric data it had scanned from the arch bridge to the user.\nThis raw data was subsequently processed with RealityScan (\nEpic\n2025\n) to reconstruct a high-fidelity, metrically accurate 3D model, even for such a complex structure (Fig.\n13\nE).\nThe final deliverables to the user included both the raw data and the reconstructed model.\nA full recording of this experiment is provided in Extension 2.\n7.1.3\nRobust Scanning for Large-scale Structures\nFigure 14:\nRobust scanning for large-scale structures.\n(A) Birdâ€™s-eye view of the experiment siteâ€”large-scale concert hall (100 m\nÃ—\n\\times\n60 m\nÃ—\n\\times\n35 m) and the textual description entered by the user.\n(B) Information completeness curve over time. The plot compares practical observations (green) against model predictions (red). The prediction modelâ€™s positive information gain exhibits its effectiveness in providing foresight guidance for planning.\n(C) The predicted geometry tightly aligns with partial observations and progressively converges towards the actual shape.\n(D) The velocity profile of this large-scale mission.\n(E) The scanning flight trajectory (red) around the concert hall. The rainbow-colored surface represents the geometric segmentation results. (E(1-2)) Two close-up views from the trajectory at key locations.\n(F) Distribution of trajectory generation latencies during the mission.\n(G) A resulting high-quality, photorealistic 3D model reconstructed after this autonomous scanning flight.\nIn our system, knowledge embedded in FMs is leveraged for the generalizable prediction of the target surface, forming a pivotal bridge between perception and planning.\nThis prediction, built on scene understanding, infers the targetâ€™s complete 3D geometry to inform efficient, safe scanning paths.\nHowever, operating in large-scale environments markedly challenges this module: due to limited sensor range, only a small portion of the target is visible at the beginning of the mission, and even later observations remain sparse relative to the overall structure.\nThis impairs accurate, consistent surface prediction over time.\nWhen prediction becomes unreliable, the foresight guidance for aerial robots is compromised, leading to back-and-forth movements that waste more time or even dangerous flight paths.\nBy distilling world knowledge from FMs,\nF\nly\nC\no strengthens spatial reasoning across diverse scenarios, thereby enabling generalizable 3D structure predictions (detailed quantitative evaluations are presented in Sec.\nZero-shot surface prediction comparisons.\n).\nThis capability allows the drone to maintain robust predictions, overcoming the challenges posed by limited observations in large-scale scenes.\nTo demonstrate this robustness in practice, we deployed our system at a\n100\n100\nm\nÃ—\n\\times\n60\n60\nm\nÃ—\n\\times\n35\n35\nm concert hall surrounded by hills and lakes (see Fig.\n14\nand Extension 3).\nThe mission covered\n1028.92\n1028.92\nm in\n1042.4\n1042.4\ns (Fig.\n14\nD), which, to the best of our knowledge, is the first demonstration of a continuous autonomous flight exceeding\n1000\n1000\nm for 3D target structure scanning in the wild.\nâ€˜Information completenessâ€™ in Fig.\n14\nB measures the proximity to the complete surface of the concert hall over time.\nThe two curves show that throughout the flight, the prediction module consistently provided more useful information compared to the direct observations from the drone.\nNotably, early on (\nt\n1\nt_{1}\n), even though only about\n35\n%\n35\\%\nof the target was initially observed, our predictor correctly recovered the concert hallâ€™s overall shape and key structural features.\nIt maintained a decent shape completeness of around\n63\n%\n63\\%\nand significantly increased the available information by about\n28\n%\n28\\%\n.\nSuch robustness stems from the modelâ€™s ability to leverage global contextual information embedded in the userâ€™s text description (â€œ\na circular structure with a domed roof\nâ€), which compensates for the scarcity of initial visual and geometric inputs.\nFundamentally, this reflects that the predictor inherits generalizable knowledge from FMs, enabling it to generate reasonable structural predictions even with limited other modalities data.\nAs the drone collected more observations, the model kept steadily improving its inference, becoming less uncertain over time and gradually converging on the correct shape (Fig.\n14\nB, C).\nGuided by these robust predictions, the scanning trajectory was notable for being smooth and coherent, with no frequent direction changes, unnecessary detours, or repeated visits to areas that had already been scanned (Fig.\n14\nE).\nThis well-organized path led to strong flight efficiency.\nThe system also performed well in terms of computational efficiency in trajectory generation.\nOver\n138\n138\nplanning cycles, the average time to generate each trajectory was only\n32\n32\nms, with a standard deviation of\n6.3\n6.3\nms (Fig.\n14\nF).\nThis computational performance allowed the system could respond quickly even in large-scale scenarios.\nAs a result, despite mild prediction inaccuracies early in the mission (such as at\nt\n1\nt_{1}\nand\nt\n2\nt_{2}\n), our planner maintained high adaptability to address discrepancies between predictions and reality in a timely manner.\n7.1.4\nEfficient and Safety-assured Scanning within Clutters\nFigure 15:\nEfficient and Safety-assured Scanning within Clutters.\n(A,D) Low-effort user prompts (text + sparse clicks) specifying the target\ncastle gate\nand\nred-brick building\n.\n(B) Castle-gate mission: (left\nâ†’\n\\rightarrow\nright) visual target segmentation, predicted complete surface, representative static obstacles and dynamic elements (\ne.g.\n, thin wires, pedestrians, adjacent structures, woods), and the corresponding 3D target segmentation (rainbow-colored) with executed trajectory.\n(C,K) High-fidelity reconstructions from the scanned data for the two missions.\n(E) Red-brick building site overview.\n(F) Specification of the drone, detailing a radius of 0.55 m.\n(G) Red-brick building: prompt-grounded segmentation and online surface predictions consistent with partial observations.\n(H) Minimum obstacle distance over time; the collision threshold equals the drone radius (red dashed line), remaining satisfied throughout the flight.\n(I(1-3)) Snapshots of key flight moments, demonstrating safe navigation through cluttered spaces (\ne.g.\n, a 2.5 m gap near walls and vegetation).\n(J) Complete flight result surrounding the red-brick building.\nGenerating targeted drone actions to scan human-intended structuresâ€”grounded in precise scene understandingâ€”is the final yet critical step in our system.\nIn unknown open-world environments, drones must not only efficiently conduct comprehensive scanning of user-specified structures but also handle unexpected dense obstacles and dynamic elements (\ne.g.\n, pedestrians).\nTherefore, how to simultaneously achieve both objectives emerges as a key challenge, which we addressed by the seamless integration of scene knowledge and flight skillsâ€”coordinating FMs with a hierarchical planner.\nIn this experiment, we tested our system in complex environments to validate the effectiveness of this approach (Fig.\n15\n).\nThe drone was tasked with scanning two targets in detail according to textual and visual prompts from the user (Fig.\n15\nA, D): a castle gate surrounded by park trees and a red-brick building in a dense forest.\nTable 1\n:\nAverage per-cycle computation time across four in-the-wild missions. Target extents indicate the length, width, and height of the minimal oriented bounding box.\nReal-world Scenarios\nTarget Extents (m)\nPerception (ms)\nPrediction (ms)\nâ€ \nPlanning (ms)\nâ€¡\nArch bridge (large building)\n8\nÃ—\n\\times\n36\nÃ—\n\\times\n12\n402.6\n149.3â€‰+â€‰432.5\n266.3\n|\n|\n||\n28.3\nConcert hall (campus)\n100\nÃ—\n\\times\n60\nÃ—\n\\times\n35\n397.7\n151.4â€‰+â€‰709.8\n457.7\n|\n|\n||\n32.0\nCastle gate (driving school)\n9\nÃ—\n\\times\n35\nÃ—\n\\times\n20\n394.3\n150.9â€‰+â€‰411.2\n234.6\n|\n|\n||\n34.5\nRed-brick building (forest)\n13\nÃ—\n\\times\n45\nÃ—\n\\times\n16\n398.2\n148.1â€‰+â€‰540.6\n309.2\n|\n|\n||\n35.7\nâ€ \nDecomposed as\nt\npred\n+\nt\ndense\nt_{\\text{pred}}+t_{\\text{dense}}\n, where\nt\npred\nt_{\\text{pred}}\nis the latency of the multi-modal surface prediction network and\nt\ndense\nt_{\\text{dense}}\nis the latency of inference-time geometry densification.\nâ€¡\nDecomposed as\nt\nglobal\n|\n|\nt\nlocal\nt_{\\text{global}}\\,||\\,t_{\\text{local}}\n, where\nt\nglobal\nt_{\\text{global}}\nis the latency of global coverage planning and\nt\nlocal\nt_{\\text{local}}\nis the latency of the local trajectory planning.\nAfter interpreting the prompts, the perception module accurately segmented the user-intended castle gate and red-brick building within cluttered surroundings (Fig.\n15\nB and G).\nThe red-brick building was challenging due to a nearby similar-looking structure (Fig.\n15\nE), but the system resolved this ambiguity using the userâ€™s visual clicks and rectification from 3D modality, which clarified linguistic uncertainties.\nBeyond this, intricate features like undulating roofs, irregular spires, and non-convex features hindered precise surface prediction.\nYet our multi-modal model inferred a qualitatively reasonable 3D target geometry consistent with inputs (Fig.\n15\nB and G).\nIn terms of scanning flight performance (Fig.\n15\nB, J), the drone operated efficiently with a compact trajectory around the targets, avoiding time wasted on irrelevant areas, local detours, or hasty directional shifts with unclear intent.\nMeanwhile, it dynamically adjusted flight trajectories and sensor viewpoints to cover all critical regions of the targets, eliminating redundant scans.\nThis efficiency arises from our decoupled system architecture: (1) it concretizes abstract scene knowledge (empowered by FMs) via the form of predicted surfaces; (2) it enables independent planning that fully leverages this guidance to generate global coverage paths with long-horizon temporal consistency.\nEnsuring real-time safety in these cluttered environments is equally non-trivial.\nThe castle gate, as the entrance to a driving school, had frequent pedestrian and vehicle traffic; surrounding woodlands, small structures, and thin wires further constrained flight (Fig.\n15\nB).\nWithin the forest, navigable space was limited to a narrow\n2.5\n2.5\n-m passageâ€”barely enough for the drone with a\n1.1\n1.1\n-m diameter (Fig.\n15\nF and I).\nDespite these high-risk conditions, our planner maintained consistent safe clearance from all obstacles to avoid collisions.\nFor the red-brick building mission specifically, Fig.\n15\nH tracks the droneâ€™s minimum distance to obstacles over\n500\n500\nseconds: this distance remained above the\n0.55\n0.55\n-m collision threshold (red dotted line, based on the AVâ€™s radius).\nThis safety assurance is attributed to the asynchronous hierarchical design of our planner, which allows the local planner can react swiftly to environmental changes while adhering to the efficient, target-centric global paths.\nTaken together, in our system, FMs transform complex, unknown open-world scenes into deterministic structured knowledge, empowering the planner to easily generate precise 3D scanning trajectories of the user-intended structure, with both high efficiency and real-time safety.\nComplete video demonstrations of both missions can be found in Extension 4 and 5.\n7.1.5\nReal-time Performance and Computational Profiling\nTab.\n1\nsummarizes the average per-update runtime of each module across the four real-world missions, measured over the full flight duration and expressed as wall-clock time per cycle for perception, prediction, and planning.\nOn the onboard computer, the perception stack (SAM2-based segmentation with cross-modal refinement) runs at about\n400\n400\nms per update in all scenarios, corresponding to an effective rate of roughly\n2\n2\n-\n2.5\n2.5\nHz.\nThis shows that prompt-grounded 2D-3D target segmentation can be sustained at a stable real-time frequency even in large, cluttered outdoor environments.\nThe prediction pipeline, executed on the ground station, has two stages: the multi-modal surface predictor and the inference-time densification module.\nThe forward pass of the predictor is nearly constant at\nâˆ¼\n150\n\\sim 150\nms across all targets, as it depends only on the fixed input-output point-set sizes.\nIn contrast, the densification cost naturally scales with physical target size, ranging from about\n0.4\n0.4\ns for the arch bridge and castle gate to about\n0.7\n0.7\ns for the concert hall.\nThis is expected, since the cascade of upsampling networks stops adaptively once the desired spatial resolution is reached.\nEven in the largest scene, however, the overall prediction latency stays below\n1\n1\ns and runs in its own low-rate thread, providing timely foresight without becoming a bottleneck for closed-loop flight.\nWith the maximum speed capped at\n1\n1\nm/s, this latency means that at most newly observed surface from\n1\n1\n-m movement is missing from each prediction update, so the predicted geometry remains closely aligned with the latest observations rather than drifting far from reality and misguiding the planner.\nFor planning, the advantages of the asynchronous hierarchical framework and computation-efficient designs are clearly reflected in the timing profile.\nGlobal coverage planning takes roughly\n250\n250\n-\n310\n310\nms per cycle, with runtime increasing with target scale as it reasons over the full predicted mesh and the associated viewpoint set; nevertheless, this cost is only\nâˆ¼\n40\n%\n\\sim 40\\%\nof the prediction latency, allowing to comfortably keep pace with prediction updates.\nConversely, the local planner maintains an almost constant runtime of about\n28\n28\n-\n36\n36\nms per call across all missions, since its complexity depends on the fixed-horizon path segment rather than the overall scene scale.\nThis separation enables high-rate trajectory replanning and rapid reactions to newly revealed obstacles, regardless of how large or complex the target structure is.\nTaken together, these results confirm that all components of\nF\nly\nC\no meet their intended real-time budgets: perception operates at a few hertz on the edge device, prediction delivers sub-second structural foresight on the ground station, and the global-local planner jointly preserves long-horizon efficiency and high-frequency safety.\nThis demonstrates that our FM-empowered scanning system is not only accurate and robust, but also computationally practical for real-world deployment.\nTable 2\n:\nHuman prior inputs required by different paradigms.\nE\nâ€‹\n(\nh\n)\nE(h)\nis the human effort measure defined in Sec.\nProblem Statement\n.\nMethods\nText\nSparse 2D Anno.\n3D BBox\nManual 3D Segmentation\nPre-defined 3D Flight Pattern\nE\nâ€‹\n(\nh\n)\nE(h)\nPlan3D\nâ€“\nâ€“\nâœ“\nâœ“\nâœ“\nHigh\nStar-Searcher\nâ€“\nâ€“\nâœ“\nâ€“\nâ€“\nMedium\nOurs\nâœ“\nâœ“\nâ€“\nâ€“\nâ€“\nLow\nFrom left to right, the prior inputs require increasing human effort (low\nâ†’\n\\rightarrow\nhigh).\n7.2\nComparisons with State-of-the-Art Methods\nTo complement the field demonstrations, we conduct systematic benchmark experiments in simulation to compare\nF\nly\nC\no against representative SOTA approaches from existing practical paradigms: (\ntwo-stage\nand\nexploration-based\n).\nFrom these experiments, our system achieves clearly superior performance in mission efficiency, information completeness, flight safety, and stability across runs.\nWe first introduce the simulated environments and experimental protocol, followed by the baselines and quantitative metrics used for evaluation.\nWe then report and analyze benchmark results, illustrating why\nF\nly\nC\no outperforms these alternative paradigms.\nFigure 16:\nOverview of the simulated environments used in our experiments: (A)\nWindmill\n, (B)\nChurch\n, (C)\nPagoda\n, and (D)\nSchloss\n.\nThe target structures are outlined in yellow.\n7.2.1\nSimulated Scenarios and Experimental Setup\nTo enable rigorous, quantitative benchmark comparisons, we perform these tests in the physically realistic AirSim simulator (\nMicrosoftResearch\n2025\n) across four complex, previously unseen scenarios that mirror typical real-world inspection targets.\nSpecifically, we consider: (1) a\nWindmill\nlocated in a rural, hilly landscape; (2) a\nChurch\nsituated in a cluttered European-style town; (3) a\nPagoda\nstanding at the center of a dense temple courtyard; and (4) a\nSchloss\nsurrounded by varied terrain and vegetation (see Fig.\n16\n).\nIn all scenarios, a virtual drone is equipped with a\n360\nâˆ˜\n360\\,^{\\circ}\nLiDAR and a gimbal-mounted RGB camera, and is tasked with collecting both geometric and visual data of the designated target.\nThe maximum translational speed is limited to\n1\n1\nm/s and the gimbalâ€™s maximum angular speed to\n20\nâˆ˜\n20\\,^{\\circ}\n/s, which is identical for all benchmarked approaches.\nFor each method, we run\n20\n20\nrepeatable trials per scenario on the same desktop computer (Intel Core i7-13700KF CPU,\n32\n32\nGB RAM, NVIDIA RTX4060 GPU) to ensure a fair computational comparison.\nTable 3\n:\nBenchmark results in simulation. For each scenario, we report mean (Avg), standard deviation (Std), maximum (Max), and minimum (Min) over 20 trials. Extent is the targetâ€™s 3D size (\nL\nx\nÃ—\nL\ny\nÃ—\nL\nz\nL_{x}\\times L_{y}\\times L_{z}\n) in meters.\nScenarios\nExtents (m)\nMethods\nFlight Time (s)\nâ†“\n\\,\\downarrow\nInfo. Completeness (%)\nâ†‘\n\\,\\uparrow\nSuccess Rate\nâ†‘\n\\,\\uparrow\nAvg\nStd\nMax\nMin\nAvg\nStd\nMax\nMin\nWindmill\n16\nÃ—\n\\times\n30\nÃ—\n\\times\n32\nPlan3D (\nZ\n1\nZ_{1}\n)\n676.6\n18.77\n722.4\n659.6\n71.67\n1.62\n75.54\n69.84\n15/20\nPlan3D (\nZ\n2\nZ_{2}\n)\n798.3\n27.17\n853.9\n759.3\n89.76\n2.59\n89.76\n82.11\n13/20\nStar-Searcher (\nZ\n1\nZ_{1}\n)\n787.4\n59.56\n936.7\n676.6\n58.83\n3.38\n64.47\n52.78\n20/20\nStar-Searcher (\nZ\n2\nZ_{2}\n)\n981.9\n77.50\n1167.0\n823.5\n84.30\n3.60\n89.49\n73.67\n20/20\nOurs\n329.4\n8.93\n357.9\n321.4\n96.04\n0.58\n97.11\n94.88\n20/20\nChurch\n35\nÃ—\n\\times\n17\nÃ—\n\\times\n18\nPlan3D (\nZ\n1\nZ_{1}\n)\n728.8\n27.99\n779.3\n676.6\n72.12\n1.71\n74.84\n69.58\n9/20\nPlan3D (\nZ\n2\nZ_{2}\n)\n830.4\n30.35\n865.3\n789.3\n90.21\n1.89\n92.33\n86.53\n7/20\nStar-Searcher (\nZ\n1\nZ_{1}\n)\n543.1\n45.20\n611.2\n463.7\n56.92\n2.89\n63.34\n52.83\n19/20\nStar-Searcher (\nZ\n2\nZ_{2}\n)\n787.3\n54.27\n893.4\n706.9\n78.40\n3.77\n83.38\n68.39\n19/20\nOurs\n336.6\n9.43\n364.8\n323.6\n94.53\n0.71\n95.54\n93.23\n20/20\nPagoda\n15\nÃ—\n\\times\n15\nÃ—\n\\times\n19\nPlan3D (\nZ\n1\nZ_{1}\n)\n432.5\n23.12\n478.8\n382.6\n51.22\n2.06\n54.70\n48.39\n10/20\nPlan3D (\nZ\n2\nZ_{2}\n)\n571.1\n24.14\n611.9\n541.9\n73.42\n0.84\n74.88\n72.48\n9/20\nStar-Searcher (\nZ\n1\nZ_{1}\n)\n282.1\n34.42\n367.0\n231.9\n37.55\n2.71\n40.98\n30.51\n18/20\nStar-Searcher (\nZ\n2\nZ_{2}\n)\n397.9\n45.98\n504.7\n318.8\n66.79\n3.61\n73.29\n59.62\n17/20\nOurs\n225.1\n6.22\n240.3\n216.2\n93.83\n0.63\n94.62\n92.33\n19/20\nSchloss\n22\nÃ—\n\\times\n33\nÃ—\n\\times\n20\nPlan3D (\nZ\n1\nZ_{1}\n)\n756.7\n32.29\n804.3\n701.6\n73.13\n1.97\n77.58\n70.13\n18/20\nPlan3D (\nZ\n2\nZ_{2}\n)\n981.4\n21.36\n1012.22\n961.9\n91.08\n0.69\n92.36\n90.08\n12/20\nStar-Searcher (\nZ\n1\nZ_{1}\n)\n501.9\n36.04\n593.3\n446.2\n58.55\n2.91\n63.28\n53.50\n17/20\nStar-Searcher (\nZ\n2\nZ_{2}\n)\n716.0\n55.05\n827.8\n611.9\n79.48\n3.40\n84.27\n70.43\n18/20\nOurs\n363.9\n7.52\n380.8\n349.5\n96.50\n0.66\n97.58\n95.45\n20/20\nZ\n1\nZ_{1}\nand\nZ\n2\nZ_{2}\n, respectively, represent the operation zones of different sizes specified by the user in the form of 3D bounding boxes.\n7.2.2\nBaselines and Evaluation Metrics\nWe benchmark\nF\nly\nC\no against two representative SOTA works, Plan3D (\nHepp\net al.\n2018\n) and Star-Searcher (\nLuo\net al.\n2024\n), that instantiate the dominant paradigms for 3D target structure scanning: a\ntwo-stage\npipeline and an\nexploration-based\nsystem.\nPlan3D follows a two-stage workflow: the drone first performs a coarse scan within a user-specified 3D bounding box; the user then manually segments the target, after which an offline coverage planner computes a refined close-range trajectory.\nSince no official implementation is available, we realize this paradigm by combining a Boustrophedon coverage planner (\nChoset\n2000\n) for the coarse path and FC-Planner (\nFeng\net al.\n2024\n) for fine 3D coverage in the second stage.\nStar-Searcher represents exploration-based scanning methods for drones equipped with LiDAR and a camera.\nGiven a user-defined 3D bounding box, it incrementally explores all occupied space in this volume using a frontier strategy, while simultaneously planning camera motions so that all discovered surfaces are visually inspected.\nWe use the official implementation and align its sensor suite and motion limits with those of our system for a fair comparison.\nNotably, both Plan3D and Star-Searcher depend on user-specified 3D bounding boxes to define the operational zone, and Plan3D additionally needs manual target segmentation between its coarse and fine stages.\nTo approximate a favourable yet realistic deployment for the baselines, we evaluate them under two bounding-box settings: a tighter box\nZ\n1\nZ_{1}\nand a larger box\nZ\n2\nZ_{2}\n.\nEven in the larger setting\nZ\n2\nZ_{2}\n, we supply a near-ideal box that tightly encloses the target and take-off point while minimizing irrelevant volume, which substantially biases the comparison in favour of the baselines and thus yields conservative estimates of\nF\nly\nC\noâ€™s advantage.\nPerformance is evaluated along four complementary dimensionsâ€”efficiency, scanned data integrity, safety, and stabilityâ€”using the following metrics.\nFlight time.\nMission efficiency is measured by the total airborne time (in seconds) from takeoff to landing.\nWe deliberately exclude any human setup time (such as bounding-box specification or manual segmentation), which effectively favors the baselines by ignoring their additional interaction overhead.\nLower values indicate higher efficiency.\nInformation completeness.\nData integrity is quantified as the percentage of the ground-truth target surface that is successfully reconstructed.\nBoth the reconstruction and ground-truth meshes are voxelized on a\n0.05\n0.05\nm\nÃ—\n\\times\n0.05\n0.05\nm grid, and completeness is computed as the ratio of recalled occupied grid cells to the total number of occupied cells in the ground truth.\nHigher percentages indicate more complete coverage.\nSuccess rate.\nSafety and reliability are captured by the success rate, defined as the fraction of runs that finish without collision and within a pre-defined time budget\nT\nmax\n=\n1500\nT_{\\max}=1500\ns.\nFor all methods, each trial terminates either when the algorithm declares completion (coverage saturated under its own stopping rule) or when the elapsed flight time exceeds\nT\nmax\nT_{\\max}\n; both collisions and timeouts are counted as failures.\nPerformance consistency.\nTo assess stability across runs, we report the variability of flight time and information completeness over\n20\n20\ntrials per scenario, using standard deviation and the dispersion of their empirical distributions (visualized via violin plots in Fig.\n17\n).\nTighter distributions (smaller spread) correspond to more stable behavior across different runs.\nFigure 17:\nViolin plots of flight time (A) and information completeness (B) across four scenarios, comparing our system to SOTA baselines. Shapes show the density; embedded box plots mark the median, interquartile range, and whiskers at\nÂ±\n\\pm\n1.5\nÃ—\n\\times\nIQR.\nFigure 18:\nBenchmark visualizations across the four scenarios. Rows show flight trajectories and reconstructions from the scanned data for our method versus baselines, highlighting more complete coverage and improved efficiency achieved by\nF\nly\nC\no.\n7.2.3\nBenchmark Results and Analysis\nWe now present and analyze the benchmark results to quantify both the magnitude and the origin of\nF\nly\nC\noâ€™s performance gains.\nOur discussion is organized around the four evaluation metricsâ€”efficiency, data integrity, safety, and stabilityâ€”and explicitly relates the observed outcomes to the objective formulated in Sec.\nProblem Statement\n.\nThe required human prior inputs of each paradigm are summarized in Tab.\n2\n.\nQuantitative results are reported in Tab.\n3\n, qualitative scanning trajectory and reconstruction visualizations are shown in Fig.\n18\n, and run-to-run variability is illustrated in Fig.\n17\n.\nComplete execution traces for all benchmark comparisons are provided in Extension 6.\nTab.\n3\nand Fig.\n17\njointly reveal that\nF\nly\nC\no achieves a consistent, scenario-wide advantage over both dominant paradigms, while requiring substantially lower-effort priors (Tab.\n2\n).\nAcross all four environments,\nF\nly\nC\no completes missions in\n225\n225\n-\n364\n364\ns on average, whereas the baselines mostly fall in the\n400\n400\n-\n1000\n1000\ns range, depending on the paradigm and the bounding-box setting.\nMeanwhile,\nF\nly\nC\no maintains high information completeness of\n93.8\n%\n93.8\\%\n-\n96.5\n%\n96.5\\%\n, while Plan3D and Star-Searcher often exhibit markedly lower completeness.\nImportantly, these gains are accompanied by strong reliability:\nF\nly\nC\no succeeds in\n79\n/\n80\n79/80\ntrials overall (with\n20\n/\n20\n20/20\nsuccess in three scenarios), whereas both baselines incur non-negligible failure due to collisions or timeouts, particularly in larger or more cluttered environments.\nThese trends map directly to the objective in Sec.\nProblem Statement\n, which seeks to (1) minimize mission time, (2) maximize target information completeness, and (3) minimize human effort, subject to safety and feasibility constraints.\nOn the efficiency-integrity frontier,\nF\nly\nC\no strictly dominates the baselines: it simultaneously reduces flight time by roughly\n1.25\nÃ—\n1.25\\times\nto\n3.0\nÃ—\n3.0\\times\n(depending on scenario) while often exceeding baselines by\n4.3\n4.3\n-\n56.2\n56.2\npercentage points in information completeness.\nNotably, this advantage is achieved even though we exclude all human interaction time from the flight-time metric, which biases the comparison in favor of the baselines.\nBeyond average performance,\nF\nly\nC\no also exhibits significantly greater stability.\nAs visualized in Fig.\n17\n, the distributions of both flight time and information completeness are notably tighter for\nF\nly\nC\no, with minimal long tails and fewer outliers across all scenarios.\nQuantitatively,\nF\nly\nC\no achieves very small run-to-run variability in completeness (Std\nâ‰¤\n0.71\n\\leq 0.71\nin all scenarios; Tab.\n3\n) and similarly low variability in flight time, indicating stable closed-loop behavior rather than occasional highly inefficient runs.\nRegarding human effort,\nF\nly\nC\no reduces operator burden by relying only on a text prompt and sparse 2D annotations, rather than effortful low-level priors.\nTaken together, the benchmark results substantiate that\nF\nly\nC\no empirically achieves a strictly better efficiency-completeness trade-off while simultaneously reducing human effort and maintaining high reliability, rather than improving one objective at the expense of another.\nWe next analyze the sources of\nF\nly\nC\noâ€™s gains from a system-design perspective.\nRather than attributing the improvements to individual algorithmic choices, we focus on how the FM-empowered perception-prediction-planning loop fundamentally reshapes the scanning process and resolves limitations inherent to existing paradigms.\nPerception: precise semantic grounding.\nThe first source of improvement lies in perception.\nWith FM-based grounding,\nF\nly\nC\no eliminates the need for explicit, low-level geometric intervention during operation.\nIn two-stage systems, manual 3D segmentation is used to prevent spending flight time on irrelevant regions, but it externalizes part of the perception problem to humans, whose effort is non-negligible in real deployments.\nExploration-based methods replace this step with a user-specified 3D bounding box; however, for non-convex and irregular targets, any box inevitably over-approximates the operational volume and introduces wasted traversal, most visibly in\nWindmill\n(Tab.\n3\n).\nCrucially, even with near-ideal\nZ\n2\nZ_{2}\nboxes that tightly enclose the target, both baselines still lag behind\nF\nly\nC\no in efficiency.\nMoreover, both baselines exhibit pronounced sensitivity to box specification: performance shifts substantially between\nZ\n1\nZ_{1}\nand\nZ\n2\nZ_{2}\n(Tab.\n3\n), casting doubt on their practical viability.\nIn contrast, FM-enabled perception anchors the scanning process directly to the semantic target throughout the mission.\nThis tight coupling avoids irrelevant exploration by construction, leading to immediate gains in flight efficiency without relying on either manual intervention or geometric over-approximation.\nPrediction: continuous global structural foresight.\nThe second and arguably most decisive advantage arises from prediction.\nExisting paradigms lack global context of the targetâ€™s complete structure during execution, whereas our shape inference exactly supplies this foresight.\nTwo-stage pipelines attempt to recover global information via a pre-defined coarse scan, making the downstream close-range stage highly sensitive to what was observed in that first pass (\ne.g.\n, omissions under\nZ\n1\nZ_{1}\n).\nExploration-based methods face a more fundamental limitation: without any global structural knowledge, decisions are made purely based on local frontier information.\nThis tends to produce discontinuous coverage with insufficient overlap, and ultimately poor reconstruction completeness, as reflected by the lower information completeness in Tab.\n3\n.\nBy contrast,\nF\nly\nC\no maintains a continuously updated global prediction of the target structure, inferred from partial observations and refined online.\nThis predictive capability provides foresight throughout the mission, allowing the system to proactively steer the drone towards yet-unobserved but structurally relevant regions.\nThus, coverage is not only more complete but also more purposeful, explaining the consistently high information completeness achieved across all scenarios.\nFigure 19:\nPerception ablation on real-world flight data.\n(A) Vanilla SAM2: 2D target masks degrade and eventually collapse as viewpoint shifts.\n(B) Ours: 2D/3D grounding stays temporally stable throughout the flight.\n(C-D) 3D segmentation comparison: Vanilla SAM2 leaks irrelevant regions into the target structure.\n(E) GPU memory usage over time.\nPlanning: efficient and safe scanning operation.\nFinally, the planning translates results of perception and prediction into concrete flight behavior.\nPrediction-guided global planning enables\nF\nly\nC\no to reason over the entire target structure while explicitly enforcing temporal consistency.\nThis suppresses unnecessary revisits and redundant scans that are common in exploration-based strategies driven by purely local decisions, thereby directly improving efficiency and coherence (Fig.\n18\n).\nEqually important is the asynchronous hierarchical design.\nUnlike serial two-stage pipelines, where planning and execution are tightly coupled and slow updates can delay reaction to newly observed obstacles,\nF\nly\nC\no decouples deliberative global planning from high-rate local replanning.\nThis design preserves rapid responsiveness to environmental changes, which is critical in cluttered or large-scale environments.\nThe higher success rates observed in complex scenarios such as\nChurch\nand\nPagoda\n(Tab.\n3\n) can be directly attributed to this ability to reconcile long-horizon planning with real-time safety constraints.\nOverall, these results confirm that\nF\nly\nC\noâ€™s advantages do not stem from isolated components, but from a coherent closed-loop system architecture empowered by FMs.\nThis integration enables the system to reduce flight time, increase information completeness, and maintain efficient, safe operation without relying on fragile low-level priors or manual intervention.\nAs such, the FM-empowered perception-prediction-planning loop represents a more appropriate and practical paradigm for open-world 3D target structure scanning than existing two-stage or exploration-based approaches.\n7.3\nAblation Studies\nComprehensive ablation studies are carried out to disentangle the contributions of individual modules in\nF\nly\nC\no and to validate the design choices of the proposed perception-prediction-planning loop for effectively turning FM-guidance into reliable 3D target structure scanning behavior in the open world.\nWe systematically remove or replace critical components to examine how each affects grounding robustness, structural foresight, flight efficiency and safety.\nThese experiments clearly reveal how FMs features and advanced flight skills affect the system performance.\nDetailed results and discussion are presented below, with additional complete recordings and cases in Extension 6.\n7.3.1\nAblations on Perception\nThis ablation study investigates whether off-the-shelf segmentation FMs can directly satisfy the perception requirements of open-world target structure scanning, and why a tailored perception design is necessary for reliable system-level operation.\nUnlike static image benchmarks, our task demands precise and temporally stable grounding of a user-specified target structure in both 2D and 3D throughout long-horizon flight under viewpoint changes, partial occlusions, and cluttered environments.\nWe conduct a qualitative comparison using a real-world flight bag from a castle gate scanning mission, which contains raw sensor streams recorded during autonomous flight.\nWe compare two approaches:\n(1)\nVanilla SAM2\n, which applies the original SAM2 for 2D segmentation and projects the masks onto the point cloud for 3D target extraction;\nand (2)\nF\nly\nC\noâ€™s perception module, which incorporates cross-modal refinement for temporally consistent target grounding.\nBoth methods receive the same one-time text prompt and sparse visual annotations at takeoff and run on the same onboard computer (NVIDIA Orin NX), with no further human input.\nWe evaluate the two methods along four dimensions that are critical for system integration:\n(1) 2D segmentation temporal stability;\n(2) geometric accuracy of the derived 3D target structure;\n(3) perception latency; and\n(4) GPU memory consumption over time.\nRepresentative results are shown in Fig.\n19\n.\nVanilla SAM2\ninitially produces plausible 2D segmentations, but its output becomes unstable as the viewpoint evolves (Fig.\n19\nA).\nAt\nt\n4\nt_{4}\n, it already misses large portions of the target, and from\nt\n5\nt_{5}\nonwards, the model persistently outputs empty masks, causing a complete perception failure.\nThis behavior highlights a fundamental limitation of directly applying frame-based segmentation models to long-horizon, streaming perception: without explicit temporal grounding and cross-modal correction, segmentation errors accumulate and eventually lead to failures.\nIn contrast, our method maintains stable and accurate target grounding throughout the entire flight (Fig.\n19\nB).\nBy continuously refining segmentation using geometric cues, the perception module dynamically corrects 2D errors and preserves temporal coherence.\nEven before outright failure,\nVanilla SAM2\nexhibits severe inconsistencies when lifted from 2D to 3D.\nAs illustrated in Fig.\n19\nC-D, projecting 2D masks onto the point cloud often introduces surrounding trees, walls, or other obstacles into the target structure.\nThis occurs because 2D segmentation alone cannot reason about spatial relationships or depth ordering, resulting in erroneous inclusion of geometrically unrelated regions.\nOur perception module explicitly enforces cross-modal consistency between image observations and partial geometry.\nAs a result, the recognized 3D target remains clean and well-aligned with the true physical structure, avoiding the accumulation of incorrect segmentations.\nBeyond accuracy, perception must also be computationally sustainable for long-duration missions.\nVanilla SAM2\nexhibits steadily increasing GPU memory usage over time, which arises from memory mechanisms not designed for continuous streaming inputs (Fig.\n19\nE).\nThis behavior makes it unsuitable for extended flights on resource-constrained onboard platforms.\nConversely, our perception incorporates a memory bank optimized for streaming sensor processing, allowing stable and bounded GPU memory consumption throughout the mission.\nMoreover, the average perception latency is reduced by more than half, from approximately\n800\n800\nms with\nVanilla SAM2\nto around\n400\n400\nms with our method.\nThis reduction enables real-time onboard operation without blocking the closed-loop flight.\nThese results demonstrate that perception for open-world target structure scanning cannot be treated as a plug-and-play application of existing segmentation FMs.\nAccurate and reliable system behavior requires perception modules that explicitly account for temporal continuity, 2D-3D consistency, and computational efficiency.\nThe proposed perception design fulfills these requirements, providing stable, accurate, and efficient target grounding that is essential for the downstream prediction and planning stages.\n7.3.2\nAblations on Prediction\nThis part investigates the role and effectiveness of the proposed multi-modal surface predictor.\nWe proceed from system-level impact to predictor-level design validation.\nFirst, we conduct an upper-bound analysis study, quantifying the contribution of predictive foresight to flight efficiency and coverage.\nWe then assess the intrinsic quality of our predictor under zero-shot surface prediction benchmarks against SOTA models.\nFinally, we ablate key design choices (multi-modal conditioning, the alternating-attention fusion strategy, and partial-surface regularization) to understand their effects on prediction performance.\nPredictor-level evaluations are performed on the zero-shot Objaverse (\nDeitke\net al.\n2023\n) and OmniObject3D (\nWu\net al.\n2023\n) datasets, comprising\n10\n,\n000\n10,000\nobjects and scenes entirely unseen during training.\nFor a fair comparison, all methods are trained using the same protocol as our predictor with\n2\n,\n048\n2,048\noutput points, and both predicted and ground-truth shapes are normalized to the unit sphere during inference.\nWe report L1 Chamfer Distance (L1-CD) for geometric fidelity and F-Score at a threshold of\n0.001\n0.001\nfor correspondence, and include inference latency where relevant.\nAll experiments use the same computational platform as Sec.\nComparisons with State-of-the-Art Methods\nto ensure fair and consistent comparisons across variants.\nTable 4\n:\nAblation experiments for upper-bound analysis. For each scenario, we report average statistics over 20 trials.\nVariants\nAvg Flight Time (s)\nâ†“\n\\,\\downarrow\nAvg Info. Comp. (%)\nâ†‘\n\\,\\uparrow\nW\n1\nC\n2\nP\n3\nS\n4\nW\n1\nC\n2\nP\n3\nS\n4\nGT\n310.1\n301.9\n203.2\n338.8\n98.42\n97.45\n97.86\n98.83\nOurs\n329.4\n336.6\n225.1\n363.9\n96.04\n94.53\n93.83\n96.50\n1\nWindmill\n,\n2\nChurch\n,\n3\nPagoda\n,\n4\nSchloss\n.\nFigure 20:\nUpper-bound analysis with privileged geometry. (A) GT-guided flight trajectory and resulting 3D reconstruction. (B) Our trajectory and reconstruction from the same mission.\nFigure 21:\nQualitative comparison of surface prediction on four representative zero-shot test cases. For each row, the columns show (left\nâ†’\n\\rightarrow\nright): (1) inputs, (2) predicted outputs from XMFNet, CRA-PCN, TripoSR, and our predictor, and (3) ground truth.\nUpper-bound analysis with privileged geometry.\nWe first probe a system-level question:\nIs our FM-empowered predictor informative enough to drive efficient and high-quality scanning behavior?\nTo answer this, we replace the predicted surface used by the planner with the ground-truth target geometry, yielding a privileged-geometry variant (termed as\nGT\n) that approximates the practical performance ceiling where complete structure is available throughout the mission.\nFollowing the experimental setup in Sec.\nComparisons with State-of-the-Art Methods\n, results in Tab.\n4\nshow that our prediction helps close most of the gap to this ceiling.\nCompared with\nGT\n, our method proves remarkably competitive: only\n20\n20\n-\n35\n35\ns slower in all four sites, corresponding to a modest\n6.2\n%\n6.2\\%\n-\n11.5\n%\n11.5\\%\nincrease in flight time.\nFig.\n20\nfurther visualizes the scanning trajectory comparison in\nChurch\nscenario.\nMeanwhile, the data integrity drop is small:\nF\nly\nC\no maintains\n93.83\n%\n93.83\\%\n-\n96.50\n%\n96.50\\%\ninformation completeness, staying within\n2.4\n2.4\n-\n4.0\n4.0\npercentage points of\nGT\nâ€™s near-perfect coverage.\nAlthough\nF\nly\nC\no never observes the full structure and receives no prior map, its multi-modal predictor provides sufficiently reliable global structural foresight to guide planning almost as if the full structure were available.\nThe remaining gap to\nGT\nis expected, as it reflects the intrinsic advantage of having privileged geometry at all times, while the FM-empowered predictor already matches the majority of\nGT\nâ€™s system-level benefit.\nIn other words, this upper-bound analysis experiment directly validates the powerful effectiveness of predictive capabilities in promoting open-world aerial scanning performance.\nTable 5\n:\nZero-shot surface prediction comparisons. We report averages on the zero-shot test set.\nMetrics\nXMFNet\nCRA-PCN\nTripoSR\nOurs\nInf. Lat. (ms)\nâ†“\nâ€ \n\\,\\downarrow^{\\dagger}\n156.9\n158.2\n645.8\n150.3\nL1-CD\nâ†“\n\\,\\downarrow\n37.393\n31.053\n44.831\n28.037\nF-Score\nâ†‘\n\\,\\uparrow\n0.601\n0.639\n0.569\n0.705\nâ€ \nInference latency measured with a batch size of 1.\nZero-shot surface prediction comparisons.\nThe generalization of the predictive module is critical for bridging perception and planning in open-world applications.\nTo evaluate our predictorâ€™s zero-shot performance, we benchmark it against three representative SOTA models from 3D vision fields: CRA-PCN (\nRong\net al.\n2024\n) for point cloud completion, XMFNet (\nAiello\net al.\n2022\n) for cross-modal prediction, and TripoSR (\nTochilkin\net al.\n2024\n) for 3D generation.\nSince TripoSR cannot provide the metric scale, we align its output to the ground truth using ICP (rigid transform + scale) before evaluation.\nThe quantitative results in Tab.\n5\nconfirm that our predictor performs the best performance on both geometric fidelity (L1-CD) and correspondence (F-Score), while also being the fastest at inference.\nCompared to TripoSR, we achieve a remarkable\n16.794\n16.794\n-point improvement in L1-CD and a\n23.90\n%\n23.90\\%\nincrease in F-score.\nThis significant margin highlights our methodâ€™s ability to address the limitations of off-the-shelf generative models when applied to physical-scale tasks.\nAgainst CRA-PCN, a point-cloud-only method, our predictor also surpasses the L1-CD by\n3.018\n3.018\nand the F-Score by\n10.32\n%\n10.32\\%\n.\nThis advantage stems from our effective utilization of multi-modal visual and textual information beyond geometry-only inference, which helps mitigate ill-posedness in geometry-unobserved or weakly constrained regions where outliers or noisy predictions are prone to emerge (Fig.\n21\n).\nEven when compared to XMFNet, which also fuses cross-modal data, our approach maintains a clear lead, we outperform it by\n9.356\n9.356\nin L1-CD and\n17.31\n%\n17.31\\%\nin F-score.\nThe visualizations further reveal XMFNetâ€™s unsatisfactory generalization on out-of-distribution cases, where it fails to infer complete geometries (Fig.\n21\n).\nMoreover, our model also maintains the lowest single-sample latency (\n150.3\n150.3\nms), supporting practical deployment in the closed-loop system.\nIn summary, these findings demonstrate our predictorâ€™s superior zero-shot generalization, which originates from its integration of world knowledge distilled from language and vision FMs.\nThis ability empowers the proposed system to robustly handle diverse and novel open-world scenarios with high adaptability and low cost.\nTable 6\n:\nAblation experiments on key design choices of the predictor, reporting averages on the zero-shot test set.\nVariants\nInput Modalities\nFusion Strategy\nSupervision\nL1-CD\nâ†“\n\\,\\downarrow\nF-Score\nâ†‘\n\\,\\uparrow\nImage\nCam. Param.\nText\nPoint Cloud\nâ„’\ncomp\n\\mathcal{L}_{\\text{comp}}\nâ„’\npar\n\\mathcal{L}_{\\text{par}}\nw/o Image\nâœ—\nâœ“\nâœ“\nâœ“\nAlternating Att.\n1\nâœ“\nâœ“\n31.565\n0.620\nw/o Cam. Param.\nâœ“\nâœ—\nâœ“\nâœ“\nAlternating Att.\n1\nâœ“\nâœ“\n30.747\n0.636\nw/o Text\nâœ“\nâœ“\nâœ—\nâœ“\nAlternating Att.\n1\nâœ“\nâœ“\n29.553\n0.673\nFusion-CA\nâœ“\nâœ“\nâœ“\nâœ“\nCross-att. Only\n2\nâœ“\nâœ“\n33.479\n0.614\nFusion-GSA\nâœ“\nâœ“\nâœ“\nâœ“\nGloal Self-att.\n3\nâœ“\nâœ“\n32.628\n0.621\nw/o Partial Loss\nâœ“\nâœ“\nâœ“\nâœ“\nAlternating Att.\n1\nâœ“\nâœ—\n30.578\n0.637\nOurs (Full Model)\nâœ“\nâœ“\nâœ“\nâœ“\nAlternating Att.\n1\nâœ“\nâœ“\n28.037\n0.705\n1\nAlternating attention,\n2\nCross-attention only,\n3\nGlobal self-attention.\nFigure 22:\nQualitative prediction ablations. (A) Removing text inputs prevents leveraging global semantics. (B) Disabling partial-surface loss harms geometry consistency.\nMulti-modal conditioning.\nIn this experiment, we study whether prediction accuracy benefits from jointly conditioning on multi-modal inputs, beyond the basic partial geometric observations alone.\nTo this end, we train three ablated alternatives, each omitting one key input: (1) image, (2) camera parameters, or (3) text.\nOur findings substantiate that the network effectively leverages each additional modality for a non-redundant positive gain (Tab.\n6\n).\nRemoving the image input leads to the largest drop, with the L1-CD increasing to\n31.565\n31.565\nand the F-Score decreasing to\n0.620\n0.620\n.\nIn the absence of visual context, the model cannot refer to FM-derived visual priors and thus struggles to infer the correct surface geometry.\nOmitting camera parameters also hurts performance (L1-CD rising to\n30.747\n30.747\nand the F-Score falling to\n0.636\n0.636\n), as it prevents the model from reliably aligning image evidence with 3D space, causing noisy or malformed predictions.\nThe removal of text input degrades accuracy as well, with the L1-CD increasing to\n29.553\n29.553\nand the F-Score decreasing to\n0.673\n0.673\n.\nThe qualitative results highlight the importance of this semantic guidance for high-level geometric control (Fig.\n22\nA).\nThe textual prompt contains the crucial global structural descriptor â€œ\nL-shaped\nâ€.\nWithout this text, the model defaults to reasoning a simple linear geometry from the ambiguous partial point cloud.\nIn contrast, thanks to the language FMsâ€™ precise understanding of the text input, our predictor correctly infers the overall layout, matching the ground truth.\nCollectively, these ablations reveal that each modality provides unique and complementary information, critical for enabling generalizable surface predictions.\nAlternating attention for knowledge fusion.\nWe next examine how the fusion design affects the predictorâ€™s ability to incorporate foundation-model knowledge.\nTo isolate the contribution of our alternating-attention fusion, we construct two variants with identical trainable parameters: (1)\nFusion-CA\n, which removes all self-attention and performs fusion purely via cross-attention, and (2)\nFusion-GSA\n, which directly concatenates the encoded modality features and applies only global self-attention for mixing.\nTab.\n6\nexhibits that both alternatives markedly underperform our full model, demonstrating that alternating attention is more advantageous.\nConcretely,\nFusion-CA\nyields a large degradation (L1-CD\n33.479\n33.479\n, F-Score\n0.614\n0.614\n), as removing self-attention limits cross-modal exchange to a single query-key-value interaction and weakens semantic alignment between vision and language priors before they are injected into the geometric stream.\nThus, the cross-attention stage receives less mutually consistent FM priors, and the predicted surface becomes less accurate.\nFusion-GSA\nperforms even worse (L1-CD\n32.628\n32.628\n, F-Score\n0.621\n0.621\n): while global self-attention permits unrestricted cross-modal mixing, it also blends modality information indiscriminately, injecting redundant or noisy context that is not necessarily relevant to completing the observed partial geometry.\nWithout an explicit step that allows the geometric stream selectively attend to the most useful FM cues, the fused representation becomes less task-aligned for surface prediction.\nIn contrast, alternating attention preserves rich cross-modal interaction while keeping fusion geometry-driven and selective, leading to more effective use of FM knowledge for precise surface completion.\nTable 7\n:\nEffect of partial-surface regularization on flight efficiency. Results are averaged over 20 trials per scenario.\nVariants\nAvg Flight Time (s)\nâ†“\n\\,\\downarrow\nWindmill\nChurch\nPagoda\nSchloss\nw/o Partial Loss\n418.8\n395.2\n261.7\n440.5\nOurs\n329.4\n336.6\n225.1\n363.9\nFigure 23:\nQualitative impact of partial-surface regularization. (A) flight trajectory and resulting 3D reconstruction of\nw/o Partial Loss\nvariant. (B) Our trajectory and reconstruction from the same mission.\nPartial-surface regularization.\nLastly, we examine the role of the partial-surface regularization that encourages consistency between the predicted surface and the input partial point cloud.\nWe train a variant by removing the partial-surface term from Eq. (\n17\n).\nAs reported in Tab.\n7\n, this ablation noticeably degrades flight efficiency (\nâˆ¼\n16\n\\sim 16\n-\n27\n%\n27\\%\n), because temporally unstable and geometry-inconsistent predictions (in scale or shape) trigger redundant revisits and unnecessary detours during scanning; Fig.\n23\nvisualizes a representative flight in the worst-affected scenario (\nWindmill\n).\nTo understand the source of this behavior, we further evaluate prediction accuracy.\nRemoving the partial loss increases L1-CD by\n2.541\n2.541\nand reduces the F-Score from\n0.705\n0.705\nto\n0.637\n0.637\n(Tab.\n6\n).\nThe quantitative drop suggests that, without an explicit constraint from the observed geometry, the network is more prone to hallucinate local structures or ignore fine details present in the input.\nFor instance, in Fig.\n22\nB, although the partial point cloud already reveals the two supporting pillars and the hollow region under the arch, the ablated model fails to preserve these concavities and instead predicts a spurious surface.\nSuch geometric artifacts directly mislead the planner to allocate viewpoints to non-existent regions, wasting flight time without improving coverage.\nThis experiment highlights that the partial loss regularization is essential, as it anchors the prediction to the latest measurements and promotes the prediction accuracy, thereby preventing prediction-induced inefficiency in downstream scanning.\n7.3.3\nAblations on Planning\nHere we take a closer look at how each component of our prediction-aware hierarchical planner effectively integrates flight skills into our system, yielding superior performance in both efficiency and safety.\nWe first isolate the configuration-space construction step and validate its utility across the four simulated scenarios.\nAfterwards, we conduct ablations on the remaining planner design choice, including (1) the temporal consistency awareness in the global planner, (2) the asynchronous computation framework, and (3) the viewpoint-constrained trajectory optimization.\nTheir experimental protocol and metrics are consistent with those in Sec.\nComparisons with State-of-the-Art Methods\n, with\n20\n20\ntrials executed for each variant in every simulated scenario.\nTable 8\n:\nEffect of the proposed configuration-space generation. Values denote per-scenario averages across 20 trials.\nMetrics\nScenarios\nSDF\nRay Casting\nOurs\nRuntime (ms)\nâ†“\n\\,\\downarrow\nWindmill\n1214.8\n28.5\n11.3\nChurch\n1514.4\n36.6\n15.2\nPagoda\n923.2\n17.9\n14.7\nSchloss\n1390.0\n44.1\n14.5\nViewpoint\nFeasibility (%)\nâ†‘\n\\,\\uparrow\nWindmill\n100.0\n95.6\n100.0\nChurch\n100.0\n93.7\n100.0\nPagoda\n100.0\n98.1\n100.0\nSchloss\n100.0\n94.2\n100.0\nFast generation of safe configuration space.\nWe evaluate the proposed fast configuration-space determination (Sec.\nFast generation of safe configuration space.\n) by replacing it with two common alternatives: SDF-based queries and ray casting against the predicted mesh.\nAll three variants use identical viewpoint sampling settings.\nWe measure (1) computational cost, reported as wall-clock runtime per update including both configuration-space construction and viewpoint checking, and (2) viewpoint feasibility, defined as the fraction of generated viewpoints that lie in free space outside the target geometry (\ni.e.\n, no viewpoints are allowed inside the target volume).\nTab.\n8\nsummarizes the results.\nOur solution achieves the lowest runtime in all four scenarios, taking only\n11.3\n11.3\n-\n15.2\n15.2\nms per update, compared with\n17.9\n17.9\n-\n44.1\n44.1\nms for ray casting and\n923.2\n923.2\n-\n1514.4\n1514.4\nms for SDF queries.\nThis translates to a\n1.2\n1.2\n-\n3.0\nÃ—\n3.0\\times\nspeedup over ray casting and a two-order-of-magnitude improvement over SDF-based fashion, while under the same structural complexity.\nMeanwhile, our method maintains\n100\n%\n100\\%\nviewpoint feasibility across all cases.\nIn contrast, ray casting occasionally produces infeasible viewpoints (feasibility\n93.7\n%\n93.7\\%\n-\n98.1\n%\n98.1\\%\n), which can lead to physically unreachable viewpoint requests and compromise downstream safety guarantees.\nOverall, this study confirms that the proposed method is both computationally lightweight and strictly feasible, making it well-suited for viewpoint generation in complex 3D structures.\nTable 9\n:\nAblation experiments on key design choices of the planner. For each scenario, we report mean (Avg), standard deviation (Std), maximum (Max), and minimum (Min) over 20 trials.\nScenarios\nVariants\nâ€ \nFlight Time (s)\nâ†“\n\\,\\downarrow\nInfo. Completeness (%)\nâ†‘\n\\,\\uparrow\nAvg Response\nLatency (ms)\nâ†“\n\\,\\downarrow\nSuccess Rate\nâ†‘\n\\,\\uparrow\nAvg\nStd\nMax\nMin\nAvg\nStd\nMax\nMin\nWindmill\nw/o CAGP\n543.4\n79.95\n713.9\n437.8\n93.57\n2.69\n96.82\n88.16\n26.54\n20/20\nw/o Async.\n363.8\n12.68\n388.1\n348.5\n95.50\n0.45\n96.43\n94.57\n270.91\n15/20\nw/o VCTO\n432.7\n23.64\n504.5\n399.6\n95.18\n1.38\n97.39\n92.02\n23.12\n20/20\nOurs\n329.4\n8.93\n357.9\n321.4\n96.04\n0.58\n97.11\n94.88\n25.04\n20/20\nChurch\nw/o CAGP\n598.3\n114.38\n846.7\n477.8\n91.48\n1.87\n95.74\n89.05\n27.93\n19/20\nw/o Async.\n350.8\n11.71\n372.1\n330.7\n94.79\n0.74\n95.78\n92.81\n290.37\n9/20\nw/o VCTO\n421.6\n13.06\n441.6\n401.1\n92.92\n2.01\n95.23\n90.05\n24.07\n19/20\nOurs\n336.6\n9.43\n364.8\n323.6\n94.53\n0.71\n95.54\n93.23\n25.18\n20/20\nPagoda\nw/o CAGP\n372.5\n51.57\n436.1\n295.4\n91.84\n1.52\n94.56\n88.47\n27.85\n19/20\nw/o Async.\n243.4\n15.43\n269.2\n218.6\n93.65\n0.97\n95.13\n92.60\n202.65\n10/20\nw/o VCTO\n240.0\n10.87\n266.7\n231.8\n93.08\n0.69\n94.86\n92.15\n25.52\n20/20\nOurs\n225.1\n6.22\n240.3\n216.2\n93.83\n0.63\n94.62\n92.33\n24.39\n19/20\nSchloss\nw/o CAGP\n671.9\n117.33\n941.2\n510.3\n95.48\n2.27\n98.21\n93.76\n29.26\n20/20\nw/o Async.\n377.1\n9.47\n391.6\n357.2\n96.87\n0.84\n97.79\n95.44\n266.39\n14/20\nw/o VCTO\n415.3\n16.42\n441.5\n374.2\n95.27\n1.09\n97.16\n93.40\n23.01\n20/20\nOurs\n363.9\n7.52\n380.8\n349.5\n96.50\n0.66\n97.58\n95.45\n24.27\n20/20\nâ€ \nCAGP\n: consistency awareness in global planning;\nAsync.\n: asynchronous computation framework;\nVCTO\n: viewpoint-constrained traj. opt.\nFigure 24:\nQualitative effects of planner ablations. (A) A tangled, time-consuming trajectory resulting from the removal of temporal consistency awareness. (B) Synchronous execution inflates response latency and causes a collision. (C) Deactivating viewpoint constraints introduces backtracking detours to recover missed coverage.\nConsistency-aware global planning.\nIn this study, we disable all consistency-promoting designs in the global planner, forcing it to re-generate the viewpoint set and re-compute the visiting sequence from scratch at every planning cycle.\nIn other words, this setting reverts our global planning to off-the-shelf memoryless 3D coverage planners that optimize each cycle in isolation, without accounting for the flight history.\nThe results show that removing these operations severely degrades mission efficiency, leading to less predictable and significantly longer flight times.\nAs quantified in Tab.\n9\n, the average flight time increases substantially across all scenarios: by\n214.0\n214.0\nseconds in\nWindmill\n(\n+\n64.9\n%\n+64.9\\%\n),\n261.7\n261.7\nseconds in\nChurch\n(\n+\n77.7\n%\n+77.7\\%\n),\n147.4\n147.4\nseconds in\nPagoda\n(\n+\n65.5\n%\n+65.5\\%\n), and\n308.0\n308.0\nseconds in the\nSchloss\n(\n+\n84.6\n%\n+84.6\\%\n).\nFurthermore, the flight-time standard deviations reveal a drastic increase in the variance of flight times, indicating that the resulting trajectories are not only longer on average but also highly erratic and unpredictable.\nThis inefficiency is caused by frequent and unnecessary reshuffling of viewpoint distributions and their visiting order, which overlooks global temporal consistency with historical paths, resulting in excessive back-and-forth maneuvers, as qualitatively illustrated by the tangled trajectory in Fig.\n24\nA.\nAsynchronous computation framework.\nHere, we assess the benefit of our decoupled, asynchronous architecture by replacing it with a conventional synchronous, serial execution scheme.\nUnder this configuration, the high-frequency local planner must wait for the computationally intensive global planner to finish each cycle before producing a new trajectory.\nThis enforced blocking severely compromises system responsiveness and proves critically detrimental to flight safety.\nAs a result, the mission success rate drops sharply from\n98.8\n%\n98.8\\%\nto\n60.0\n%\n60.0\\%\n(Tab.\n9\n).\nThe surge in failures directly stems from the systemâ€™s reduced ability to respond to unexpected obstacles.\nSpecifically, as reported in Tab.\n9\n, the average response latency increases by nearly an order of magnitudeâ€”from approximately\n30\n30\nms to\n200\n200\n-\n300\n300\nms.\nSuch excessive latency prevents the drone from reacting in time to obstacles that emerge between planning cycles, leading to frequent collisions, as qualitatively illustrated in Fig.\n24\nB.\nThese results confirm that asynchronous design is essential for maintaining real-time responsiveness and ensuring flight safety.\nViewpoint-constrained trajectory optimization.\nThis analysis validates our local trajectory generation strategy, which enforces that the trajectory preserves the structural coverage prescribed by the global path.\nTo this end, we evaluate a variant where this hard constraint is removed, allowing the optimizer to freely adjust intermediate points as movable waypoints rather than requiring the trajectory to pass through all designated viewpoints.\nAs listed in Tab.\n9\n, this relaxed alternative eventually reaches comparable information completeness, but at a clear cost in mission efficiency.\nWithout the constraint, the average flight time expands across all scenarios without this constraint, respectively by\n31.4\n%\n31.4\\%\n,\n25.3\n%\n25.3\\%\n,\n6.6\n%\n6.6\\%\n, and\n14.1\n%\n14.1\\%\n(Tab.\n9\n).\nThis inefficiency arises from suboptimal behaviors like backtracking; the planner might initially generate a shorter, smoother trajectory by skipping some crucial viewpoints, only to discover later that a target surface region was missed to scan.\nThis forces the planner to produce a long, inefficient detour to return to the vicinity of the skipped viewpoints to complete coverage (see Fig.\n24\nC for an example).\nThis evaluation demonstrates that our viewpoint-constrained optimization is a more efficient strategy, balancing the requirements of data integrity and minimizing flight time.\n8\nConclusion\nIn this work, we study the aerial 3D target structure scanning problem in open-world environments and answer a central system-level question:\nWhat architecture is currently well-suited for this problem?\nWe find that dominant paradigms remain constrained by either effortful human priors or restrictive assumptions, lack scene understanding that is necessary for high efficiency, and afford limited safety assurance in cluttered settings.\nThese limitations hinder practical viability across diverse scanning missions.\nTo this end, we propose\nF\nly\nC\no, a tailored perception-prediction-planning architecture that efficiently leverages foundation models to enable fully autonomous, prompt-driven target scanning in various, unknown open-world scenes, requiring only low-effort texts and visual annotations from human inputs.\nComprehensive evaluations, encompassing both in-the-wild demonstrations and quantitative simulations, substantiate that the proposed system demonstrates significant advantages over existing paradigms and represents a more suitable recipe for this open-world aerial scanning task.\nThis superior performance is rooted in the synergistic and efficient integration of three FM-empowered components that jointly close the loop from prompt to scanning flight.\nFirst, perception utilizes the world understanding of FMs to robustly ground abstract prompts onto physical targets without requiring manually defined operational zones, overcoming a key usability barrier of existing works when deployed in the open world.\nSecond, by distilling knowledge in FMs, the partial-to-complete predictor analyzes multi-modal cues and provides global foresight guidance to suppress redundant scanning, while its strong zero-shot generalization ensures system reliability in novel environments.\nThird, the prediction-aware hierarchical planner converts this foresight into efficient and safe trajectories, achieving\n1.25\n1.25\n-\n3.0\nÃ—\n3.0\\times\nefficiency gains with higher information completeness over competing baselines, while maintaining real-time responsiveness for safety-critical obstacle avoidance.\nIn summary,\nF\nly\nC\no offers a practical and extensible system blueprint: it seamlessly incorporates the powerful understanding and inherent knowledge of FMs with advanced flight skills into aerial robots without expensive data requirements while keeping online components within real-time budgets, and it supports flexible modular replacement to naturally accommodate future progress in FMs and robotics planning.\nThe full system implementation and development configurations will be released to enable reproducibility and future research.\nBeyond this,\nF\nly\nC\no enables a general capability for prompt-grounded, targeted 3D data acquisition in complex real environments, where manual specification of operational volumes or exhaustive exploration is undesirable.\nThis can support a range of field workflows that require repeatable, structure-centric capture,\ne.g.\n, infrastructure inspection, cultural-heritage documentation, and ecological habitat monitoring.\nBy turning lightweight human intent into safe and efficient scanning trajectories, our system may reduce operator burden and improve the scalability of collecting task-relevant 3D observations.\nAt the same time, realizing this potential broadly calls for further advances on several fronts, which we discuss next as limitations and future directions.\n8.1\nLimitations and Future Work\n8.1.1\nLimited utilization of textual information.\nAlthough text prompts play a crucial role in grounding user intent, their expressive power is not yet fully exploited in the current system.\nFirst, we intentionally restrict the textual input to short, structured phrases describing salient target attributes.\nThis design choice improves robustness and reduces ambiguity in open-world deployments, but it also limits the expressiveness of free-form natural language and constrains how flexibly users can specify complex intent.\nSecond, the quantitative gains brought by textual conditioning in surface prediction remain marginal compared with geometric and visual cues.\nA key reason lies in data imbalance during training: a single 3D model can generate many partial point clouds and image observations, all paired with the same textual description.\nThis many-to-one mapping biases learning towards geometry and vision while weakening the relative influence of language.\nSimilar dilemma has been observed in recent vision-language-action models (VLA), where language often serves as a weak signal rather than a dominant driver (\nZitkovich\net al.\n2023\n).\nFuture work will support richer language forms, explore improved data balancing strategies, and tighter cross-modal alignment objectives to better elevate the impact of text.\n8.1.2\nDependence on offboard computation for prediction.\nAt present, the surface prediction module runs on a ground device rather than fully onboard the aerial platform.\nThis choice is motivated by two considerations: prediction is not strictly time-critical for closed-loop flight, and incorporating FMs incurs substantial computational overhead that makes sub-second onboard inference impractical on current edge hardware.\nWhile reliable communication is available in most outdoor scanning scenarios, this dependency may become a limiting factor in communication-constrained environments such as underground mines, enclosed industrial facilities, or electromagnetically disturbed sites.\nTherefore, fully onboard deployment is a necessary step towards broader applicability.\nAn important future direction is to compress and distill foundation-model knowledge into lighter-weight predictors, enabling accurate yet efficient inference on edge devices and eliminating reliance on external communication.\n8.1.3\nScalability beyond single-drone, single-target missions.\nThe current system is constrained by the endurance of a single aerial robot and is thus designed to scan one target structure per mission.\nScaling this capability to city-scale or large natural environments calls for coordinated multi-drone operation.\nThis introduces new challenges, including dynamic high-level task allocation, inter-robot coordination, and real-time reassignment of scanning objectives based on partial observations.\nFuture work will investigate multi-agent extensions of the proposed architecture, where multiple drones collaboratively divide target structures, adaptively replan assignments, and jointly minimize overall mission time while maintaining safety and coverage guarantees.\nSuch extensions would upgrade the system from single-structure autonomy to large-scale, cooperative open-world scanning.\n{acks}\nWe thank Fei Gao for valuable suggestions to the manuscript.\nWe also thank Weiye Zhang and Mingjie Zhang for their early investigation on hardware platform selection.\nReferences\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat,\net al.\n(2023)\nGpt-4 technical report\n.\narXiv preprint arXiv:2303.08774\n.\nCited by:\nÂ§5.2.4\n.\nM. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman,\net al.\n(2022)\nDo as i can, not as i say: grounding language in robotic affordances\n.\narXiv preprint arXiv:2204.01691\n.\nCited by:\nÂ§2.4\n.\nE. Aiello, D. Valsesia, and E. Magli (2022)\nCross-modal learning for image-guided point cloud shape completion\n.\nAdvances in Neural Information Processing Systems\n35\n,\npp.Â 37349â€“37362\n.\nCited by:\nÂ§2.3\n,\nÂ§7.3.2\n.\nBeyondVision (2023)\nTransforming construction and infrastructure: discover amazing construction site drones surveillance\n.\nNote:\nhttps://www.youtube.com/watch?v=Np1CJ1uWEGU\nCited by:\nÂ§1\n.\nI. Biederman (1987)\nRecognition-by-components: a theory of human image understanding.\n.\nPsychological review\n94\n(\n2\n),\npp.Â 115\n.\nCited by:\nÂ§7.1.2\n.\nA. Bircher, M. Kamel, K. Alexis, H. Oleynikova, and R. Siegwart (2018)\nReceding horizon path planning for 3d exploration and surface inspection\n.\nAutonomous Robots\n42\n,\npp.Â 291â€“306\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nK. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter,\net al.\n(2024)\nÏ€\n\\pi\n0: A vision-language-action flow model for general robot control. corr, abs/2410.24164, 2024. doi: 10.48550\n.\narXiv preprint ARXIV.2410.24164\n.\nCited by:\nÂ§2.4\n.\nC. Cao, J. Zhang, M. Travers, and H. Choset (2020)\nHierarchical coverage path planning in complex 3d environments\n.\nIn\n2020 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 3206â€“3212\n.\nCited by:\nÂ§5.3\n.\nX. Chen, Q. Li, T. Wang, T. Xue, and J. Pang (2024)\nGennbv: generalizable next-best-view policy for active 3d reconstruction\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 16436â€“16445\n.\nCited by:\nÂ§1\n,\nÂ§2.3\n.\nH. Choset (2000)\nCoverage of known spaces: the boustrophedon cellular decomposition\n.\nAutonomous Robots\n9\n(\n3\n),\npp.Â 247â€“253\n.\nCited by:\nÂ§7.2.2\n.\nC. Connolly (1985)\nThe determination of next best views\n.\nIn\nProceedings. 1985 IEEE international conference on robotics and automation\n,\nVol.\n2\n,\npp.Â 432â€“435\n.\nCited by:\nÂ§2.2\n.\nM. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi (2023)\nObjaverse: a universe of annotated 3d objects\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 13142â€“13153\n.\nCited by:\nÂ§6.0.2\n,\nÂ§7.3.2\n.\nDJI (2024)\nDJIFlightPlanner\n.\nNote:\nhttps://www.djiflightplanner.com/\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nDJI (2025a)\nDJI matrice 30\n.\nExternal Links:\nLink\nCited by:\nÂ§7.1.1\n.\nDJI (2025b)\nDJI terra\n.\nNote:\nhttps://enterprise.dji.com/dji-terra\nCited by:\nÂ§2.1\n.\nD. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang,\net al.\n(2023)\nPalm-e: an embodied multimodal language model\n.\nCited by:\nÂ§2.4\n.\nJ. Du, B. Huang, and B. Jia (2025)\nAn efficient uav coverage path planning method for 3-d structures\n.\nIEEE Internet of Things Journal\n.\nCited by:\nÂ§5.3\n.\nEpic (2025)\nRealityScan\n.\nExternal Links:\nLink\nCited by:\nÂ§7.1.2\n.\nEsri (2023)\nMapping disaster response\n.\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nM. Ester, H. Kriegel, J. Sander, X. Xu,\net al.\n(1996)\nA density-based algorithm for discovering clusters in large spatial databases with noise\n.\nIn\nkdd\n,\nVol.\n96\n,\npp.Â 226â€“231\n.\nCited by:\nÂ§5.1.2\n.\nC. Feng, H. Li, F. Gao, B. Zhou, and S. Shen (2023)\nPredrecon: a prediction-boosted planning framework for fast and high-quality autonomous aerial reconstruction\n.\nIn\n2023 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 1207â€“1213\n.\nCited by:\nÂ§1\n,\nÂ§2.3\n.\nC. Feng, H. Li, M. Zhang, X. Chen, B. Zhou, and S. Shen (2024)\nFc-planner: a skeleton-guided planning framework for fast aerial coverage of complex 3d scenes\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 8686â€“8692\n.\nCited by:\nÂ§2.1\n,\nÂ§5.3.2\n,\nÂ§7.2.2\n.\nA. GuÃ©don, T. Monnier, P. Monasse, and V. Lepetit (2023)\nMACARONS: mapping and coverage anticipation with rgb online self-supervision\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 940â€“951\n.\nCited by:\nÂ§1\n,\nÂ§2.3\n.\nJ. Han (2009)\nFrom pid to active disturbance rejection control\n.\nIEEE transactions on Industrial Electronics\n56\n(\n3\n),\npp.Â 900â€“906\n.\nCited by:\nÂ§5.3.1\n.\nL. Han, F. Gao, B. Zhou, and S. Shen (2019)\nFiesta: fast incremental euclidean distance fields for online motion planning of aerial robots\n.\nIn\n2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 4423â€“4430\n.\nCited by:\nÂ§6.0.1\n.\nP. E. Hart, N. J. Nilsson, and B. Raphael (1968)\nA formal basis for the heuristic determination of minimum cost paths\n.\nIEEE transactions on Systems Science and Cybernetics\n4\n(\n2\n),\npp.Â 100â€“107\n.\nCited by:\nÂ§5.3.1\n.\nK. Helsgaun (2000)\nAn effective implementation of the linâ€“kernighan traveling salesman heuristic\n.\nEuropean journal of operational research\n126\n(\n1\n),\npp.Â 106â€“130\n.\nCited by:\nÂ§5.3.2\n.\nB. Hepp, M. NieÃŸner, and O. Hilliges (2018)\nPlan3d: viewpoint and trajectory optimization for aerial multi-view stereo reconstruction\n.\nACM Transactions on Graphics (TOG)\n38\n(\n1\n),\npp.Â 1â€“17\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nÂ§7.2.2\n.\nD. D. Hoffman and W. A. Richards (1984)\nParts of recognition\n.\nCognition\n18\n(\n1-3\n),\npp.Â 65â€“96\n.\nCited by:\nÂ§7.1.2\n.\nJ. Huang, Z. Gojcic, M. Atzmon, O. Litany, S. Fidler, and F. Williams (2023)\nNeural kernel surface reconstruction\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 4369â€“4379\n.\nCited by:\nÂ§5.2.5\n.\nW. Jing, J. Polden, P. Y. Tao, W. Lin, and K. Shimada (2016)\nView planning for 3d shape reconstruction of buildings with unmanned aerial vehicles\n.\nIn\n2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV)\n,\npp.Â 1â€“6\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nY. Kasten, O. Rahamim, and G. Chechik (2023)\nPoint cloud completion with pretrained text-to-image diffusion models\n.\nAdvances in Neural Information Processing Systems\n36\n,\npp.Â 12171â€“12191\n.\nCited by:\nÂ§5.2.3\n.\nS. Katz and A. Tal (2015)\nOn the visibility of point clouds\n.\nIn\nProceedings of the IEEE international conference on computer vision\n,\npp.Â 1350â€“1358\n.\nCited by:\nÂ§5.2.5\n.\nM. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi,\net al.\n(2024)\nOpenvla: an open-source vision-language-action model\n.\narXiv preprint arXiv:2406.09246\n.\nCited by:\nÂ§2.4\n.\nJ. Li, D. Li, S. Savarese, and S. Hoi (2023)\nBlip-2: bootstrapping language-image pre-training with frozen image encoders and large language models\n.\nIn\nInternational conference on machine learning\n,\npp.Â 19730â€“19742\n.\nCited by:\nÂ§5.2.4\n.\nW. Li, X. Zhang, Z. Sun, D. Qi, H. Li, W. Cheng, W. Cai, S. Wu, J. Liu, Z. Wang,\net al.\n(2025)\nStep1x-3d: towards high-fidelity and controllable generation of textured 3d assets\n.\narXiv preprint arXiv:2505.07747\n.\nCited by:\nÂ§2.3\n.\nLitchi (2025)\nLitchi\n.\nNote:\nhttps://flylitchi.com/\nCited by:\nÂ§2.1\n.\nD. C. Liu and J. Nocedal (1989)\nOn the limited memory bfgs method for large scale optimization\n.\nMathematical programming\n45\n(\n1\n),\npp.Â 503â€“528\n.\nCited by:\nÂ§5.3.3\n.\nZ. Liu, Z. Cao, J. Li, P. Guan, and J. Yu (2024)\nDG-nbv: a cognitive framework for direct generation of next best view in continuous view space\n.\nIEEE Transactions on Cognitive and Developmental Systems\n.\nCited by:\nÂ§2.3\n.\nLivox (2025)\nLivox mid-360 user manual\n.\nExternal Links:\nLink\nCited by:\nÂ§7.1.1\n.\nY. Luo, Z. Zhuang, N. Pan, C. Feng, S. Shen, F. Gao, H. Cheng, and B. Zhou (2024)\nStar-searcher: a complete and efficient aerial system for autonomous target search in complex unknown environments\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n,\nÂ§7.2.2\n.\nMapalytics (2024)\nHeritage preservation - digital double - unesco - mapalytics - 3d mapping\n.\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nMicrosoftResearch (2025)\nAirSim\n.\nExternal Links:\nLink\nCited by:\nÂ§7.2.1\n.\nNVIDIA (2025)\nNVIDIA jetson orin nx\n.\nExternal Links:\nLink\nCited by:\nÂ§7.1.1\n.\nM. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby,\net al.\n(2023)\nDinov2: learning robust visual features without supervision\n.\narXiv preprint arXiv:2304.07193\n.\nCited by:\nÂ§5.2.2\n.\nC. Peng and V. Isler (2019)\nAdaptive view planning for aerial 3d reconstruction\n.\nIn\n2019 International Conference on Robotics and Automation (ICRA)\n,\npp.Â 2981â€“2987\n.\nCited by:\nÂ§2.1\n.\nPIX4D (2024)\nPIX4Dcapture pro\n.\nNote:\nhttps://www.pix4d.com/product/pix4dcapture/\nCited by:\nÂ§2.1\n.\nC. R. Qi, L. Yi, H. Su, and L. J. Guibas (2017)\nPointnet++: deep hierarchical feature learning on point sets in a metric space\n.\nAdvances in neural information processing systems\n30\n.\nCited by:\nÂ§5.2.2\n.\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,\net al.\n(2021)\nLearning transferable visual models from natural language supervision\n.\nIn\nInternational conference on machine learning\n,\npp.Â 8748â€“8763\n.\nCited by:\nÂ§5.2.4\n.\nN. Ravi, V. Gabeur, Y. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. RÃ¤dle, C. Rolland, L. Gustafson,\net al.\n(2024)\nSam 2: segment anything in images and videos\n.\narXiv preprint arXiv:2408.00714\n.\nCited by:\nÂ§5.1\n.\nM. Roberts, D. Dey, A. Truong, S. Sinha, S. Shah, A. Kapoor, P. Hanrahan, and N. Joshi (2017)\nSubmodular trajectory optimization for aerial 3d scanning\n.\nIn\nProceedings of the IEEE International Conference on Computer Vision\n,\npp.Â 5324â€“5333\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nY. Rong, H. Zhou, L. Yuan, C. Mei, J. Wang, and T. Lu (2024)\nCra-pcn: point cloud completion with intra-and inter-level cross-resolution transformers\n.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n,\nVol.\n38\n,\npp.Â 4676â€“4685\n.\nCited by:\nÂ§2.3\n,\nÂ§5.2.3\n,\nÂ§7.3.2\n.\nSkydio (2025)\nSkydio software\n.\nNote:\nhttps://www.skydio.com/software\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nN. Smith, N. Moehrle, M. Goesele, and W. Heidrich (2018)\nAerial path planning for urban scene reconstruction: a continuous optimization method and benchmark\n.\nACM Transactions on Graphics (TOG)\n37\n(\n6\n),\npp.Â 1â€“15\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nS. Song and S. Jo (2017)\nOnline inspection path planning for autonomous 3d modeling using a micro-aerial vehicle\n.\nIn\n2017 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 6217â€“6224\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nS. Song, D. Kim, and S. Choi (2021)\nView path planning via online multiview stereo for 3-d modeling of large-scale structures\n.\nIEEE Transactions on Robotics\n38\n(\n1\n),\npp.Â 372â€“390\n.\nCited by:\nÂ§2.2\n.\nD. Tochilkin, D. Pankratz, Z. Liu, Z. Huang, A. Letts, Y. Li, D. Liang, C. Laforte, V. Jampani, and Y. Cao (2024)\nTriposr: fast 3d object reconstruction from a single image\n.\narXiv preprint arXiv:2403.02151\n.\nCited by:\nÂ§2.3\n,\nÂ§7.3.2\n.\nW. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som,\net al.\n(2023)\nImage as a foreign language: beit pretraining for vision and vision-language tasks\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 19175â€“19186\n.\nCited by:\nÂ§5.1\n.\nZ. Wang, X. Zhou, C. Xu, and F. Gao (2022)\nGeometrically constrained trajectory optimization for multicopters\n.\nIEEE Transactions on Robotics\n38\n(\n5\n),\npp.Â 3259â€“3278\n.\nCited by:\nÂ§5.3.3\n.\nT. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang, J. Wang, C. Qian,\net al.\n(2023)\nOmniobject3d: large-vocabulary 3d object dataset for realistic perception, reconstruction and generation\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 803â€“814\n.\nCited by:\nÂ§7.3.2\n.\nZ. Wu, P. Marais, and H. RÃ¼ther (2024)\nA uav-based sparse viewpoint planning framework for detailed 3d modelling of cultural heritage monuments\n.\nISPRS Journal of Photogrammetry and Remote Sensing\n218\n,\npp.Â 555â€“571\n.\nCited by:\nÂ§5.3\n.\nP. Xiang, X. Wen, Y. Liu, Y. Cao, P. Wan, W. Zheng, and Z. Han (2021)\nSnowflakenet: point cloud completion by snowflake point deconvolution with skip-transformer\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp.Â 5499â€“5509\n.\nCited by:\nÂ§6.0.2\n.\nW. Xu, Y. Cai, D. He, J. Lin, and F. Zhang (2022)\nFast-lio2: fast direct lidar-inertial odometry\n.\nIEEE Transactions on Robotics\n38\n(\n4\n),\npp.Â 2053â€“2073\n.\nCited by:\nÂ§5.1.3\n.\nB. Yamauchi (1997)\nA frontier-based approach for autonomous exploration\n.\nIn\nProceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRAâ€™97.â€™Towards New Computational Principles for Robotics and Automationâ€™\n,\npp.Â 146â€“151\n.\nCited by:\nÂ§2.2\n.\nH. Yan, Z. Li, K. Luo, L. Lu, and P. Tan (2025)\nSymmCompletion: high-fidelity and high-consistency point cloud completion with symmetry guidance\n.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n,\nVol.\n39\n,\npp.Â 9094â€“9102\n.\nCited by:\nÂ§1\n.\nX. Yu, Y. Rao, Z. Wang, Z. Liu, J. Lu, and J. Zhou (2021)\nPointr: diverse point cloud completion with geometry-aware transformers\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp.Â 12498â€“12507\n.\nCited by:\nÂ§5.2.3\n.\nW. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert (2018)\nPcn: point completion network\n.\nIn\n2018 international conference on 3D vision (3DV)\n,\npp.Â 728â€“737\n.\nCited by:\nÂ§2.3\n,\nÂ§5.2.3\n.\nX. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer (2023)\nSigmoid loss for language image pre-training\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp.Â 11975â€“11986\n.\nCited by:\nÂ§5.2.2\n.\nH. Zhang, Y. Yao, K. Xie, C. Fu, H. Zhang, and H. Huang (2021)\nContinuous aerial path planning for 3d urban scene reconstruction.\n.\nACM Trans. Graph.\n40\n(\n6\n),\npp.Â 225â€“1\n.\nCited by:\nÂ§2.1\n.\nJ. Zhang, K. Wang, S. Wang, M. Li, H. Liu, S. Wei, Z. Wang, Z. Zhang, and H. Wang (2024a)\nUni-navid: a video-based vision-language-action model for unifying embodied navigation tasks\n.\narXiv preprint arXiv:2412.06224\n.\nCited by:\nÂ§2.4\n.\nY. Zhang, T. Cheng, L. Zhu, R. Hu, L. Liu, H. Liu, L. Ran, X. Chen, W. Liu, and X. Wang (2024b)\nEvf-sam: early vision-language fusion for text-prompted segment anything model\n.\narXiv preprint arXiv:2406.20076\n.\nCited by:\nÂ§5.1\n.\nX. Zhou, K. Xie, K. Huang, Y. Liu, Y. Zhou, M. Gong, and H. Huang (2020)\nOffsite aerial path planning for efficient urban scene reconstruction\n.\nACM Transactions on Graphics (TOG)\n39\n(\n6\n),\npp.Â 1â€“16\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nZ. Zhou, Y. Zhu, J. Wen, C. Shen, and Y. Xu (2025)\nVision-language-action model with open-world embodied reasoning from pretrained knowledge\n.\narXiv preprint arXiv:2505.21906\n.\nCited by:\nÂ§2.4\n.\nB. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid,\net al.\n(2023)\nRt-2: vision-language-action models transfer web knowledge to robotic control\n.\nIn\nConference on Robot Learning\n,\npp.Â 2165â€“2183\n.\nCited by:\nÂ§2.4\n,\nÂ§8.1.1\n.\nAppendix A\nAppendix\nThis appendix provides supplementary video materials that qualitatively illustrate and further validate the proposed\nF\nly\nC\no system.\nExtension 1: Overall System Overview and Motivation.\nThis video provides an overview of the\nF\nly\nC\no system and its research context.\nIt first introduces the motivation behind autonomous 3D target structure scanning in open-world environments and highlights the limitations of existing paradigms.\nThe video then introduces the motivations behind our system design and summarizes the advancement against existing paradigms, highlighting the novelty of integrating foundation models to empower drones.\nFinally, representative results are presented, including in-the-wild flight demonstrations and simulation visualizations, to illustrate the systemâ€™s effectiveness and its ability to address the identified research challenges.\nExtension 2: Precise Scene Understanding for Adaptive Scanningâ€”Arch Bridge Linking Large Buildings.\nThis video corresponds to Sec.\nPrecise Scene Understanding for Adaptive Scanning\n.\nIt demonstrates\nF\nly\nC\noâ€™s ability to accurately ground user-specified targets in cluttered real-world scenes and to adaptively scan a distinctive arch bridge linking two large buildings, based solely on low-effort human prompts.\nExtension 3: Robust Scanning for Large-scale Structuresâ€”Concert Hall within Campus.\nThis video corresponds to Sec.\nRobust Scanning for Large-scale Structures\n.\nIt showcases\nF\nly\nC\noâ€™s performance in a large-scale scenario: scanning a 100 m\nÃ—\n\\times\n60 m\nÃ—\n\\times\n35 m concert hall nestled among hills and lakes.\nThis experiment specifically validates the robustness and stability during large-scale missionsâ€”addressing performance degradation caused by sparse initial observations, all enabled by the systemâ€™s generalizable surface prediction capability.\nExtension 4: Efficient and Safety-assured Scanning within Cluttersâ€”Castle Gate amidst Park Trees.\nThis video corresponds to Sec.\nEfficient and Safety-assured Scanning within Clutters\n.\nIt demonstrates the full deployment of the\nF\nly\nC\no system for scanning a castle gate.\nThis test showcases the systemâ€™s ability to seamlessly integrate flight skills with guidance from foundation models, achieving efficient and safe scanning flights while ensuring complete target coverage.\nExtension 5: Efficient and Safety-assured Scanning within Cluttersâ€”Red-brick Building within a Dense Forest.\nThis video corresponds to Sec.\nEfficient and Safety-assured Scanning within Clutters\n.\nIt records the complete procedure of\nF\nly\nC\no autonomously scanning a red-brick building surrounded by trees.\nThe results demonstrate\nF\nly\nC\noâ€™s ability to simultaneously maintain scanning efficiency, comprehensive coverage, and real-time obstacle avoidance in highly cluttered environments.\nExtension 6: Benchmark Comparisons and Ablation Studies.\nThis video corresponds to Sec.\nComparisons with State-of-the-Art Methods\nand Sec.\nAblation Studies\n.\nIt presents qualitative and quantitative comparisons between\nF\nly\nC\no and representative state-of-the-art approaches, highlighting advancements in mission efficiency, coverage completeness, and flight safety.\nThe video further includes visualizations of ablation studies, validating the contribution of individual system components and supporting the design choices of the proposed system architecture.\nAccess to High-resolution Videos.\nHigh-resolution versions of all supplementary videos are publicly available via the YouTube playlist:\nhttps://www.youtube.com/playlist?list=PLqjZjnqsCyl40rw3y15Yzc7Mdo-z1y2j8\n.",
    "preview_text": "Autonomous 3D scanning of open-world target structures via drones remains challenging despite broad applications. Existing paradigms rely on restrictive assumptions or effortful human priors, limiting practicality, efficiency, and adaptability. Recent foundation models (FMs) offer great potential to bridge this gap. This paper investigates a critical research problem: What system architecture can effectively integrate FM knowledge for this task? We answer it with FlyCo, a principled FM-empowered perception-prediction-planning loop enabling fully autonomous, prompt-driven 3D target scanning in diverse unknown open-world environments. FlyCo directly translates low-effort human prompts (text, visual annotations) into precise adaptive scanning flights via three coordinated stages: (1) perception fuses streaming sensor data with vision-language FMs for robust target grounding and tracking; (2) prediction distills FM knowledge and combines multi-modal cues to infer the partially observed target's complete geometry; (3) planning leverages predictive foresight to generate efficient and safe paths with comprehensive target coverage. Building on this, we further design key components to boost open-world target grounding efficiency and robustness, enhance prediction quality in terms of shape accuracy, zero-shot generalization, and temporal stability, and balance long-horizon flight efficiency with real-time computability and online collision avoidance. Extensive challenging real-world and simulation experiments show FlyCo delivers precise scene understanding, high efficiency, and real-time safety, outperforming existing paradigms with lower human effort and verifying the proposed architecture's practicality. Comprehensive ablations validate each component's contribution. FlyCo also serves as a flexible, extensible blueprint, readily leveraging future FM and robotics advances. Code will be released.\n\n\\corrauth\nBoyu Zhou, Southern University of Science and Technology,\nShenzhen, ",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "VLA",
        "VLM",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "FlyCoæ˜¯ä¸€ä¸ªåŸºäºåŸºç¡€æ¨¡å‹çš„æ— äººæœºç³»ç»Ÿï¼Œé€šè¿‡æ„ŸçŸ¥-é¢„æµ‹-è§„åˆ’å¾ªç¯å®ç°å¼€æ”¾ä¸–ç•Œä¸­è‡ªä¸»3Dç»“æ„æ‰«æï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œç›®æ ‡å®šä½å’Œè·Ÿè¸ªï¼Œå¹¶æ¶‰åŠè¿åŠ¨æ§åˆ¶å’Œæ•´ä½“ç³»ç»Ÿåè°ƒã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šFlyCoâ€”â€”åŸºäºåŸºç¡€æ¨¡å‹çš„æ— äººæœºå¼€æ”¾ä¸–ç•Œè‡ªä¸»ä¸‰ç»´ç»“æ„æ‰«æ\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nåœ¨åŸºç¡€è®¾æ–½æ£€æµ‹ã€ç¾å®³å“åº”ç­‰åº”ç”¨ä¸­ï¼Œæ— äººæœºå¯¹å¼€æ”¾ä¸–ç•Œä¸­ç‰¹å®šä¸‰ç»´ç»“æ„è¿›è¡Œè‡ªä¸»ã€ç²¾ç¡®æ‰«æè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äººå·¥é¢„è®¾ï¼ˆå¦‚ä¸‰ç»´è¾¹ç•Œæ¡†ï¼‰æˆ–é«˜æˆæœ¬çš„å…ˆéªŒçŸ¥è¯†ï¼Œç¼ºä¹å¯¹å¤æ‚åœºæ™¯çš„ç†è§£èƒ½åŠ›ï¼Œå¯¼è‡´å®ç”¨æ€§ã€æ•ˆç‡å’Œé€‚åº”æ€§å—é™ã€‚è¿‘æœŸï¼Œ**åŸºç¡€æ¨¡å‹**åœ¨è§†è§‰-è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä¸ºè§£å†³è¿™ä¸€ç“¶é¢ˆæä¾›äº†å¯èƒ½ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š**å¦‚ä½•è®¾è®¡ä¸€ç§ç³»ç»Ÿæ¶æ„ï¼Œèƒ½æœ‰æ•ˆèåˆåŸºç¡€æ¨¡å‹çš„çŸ¥è¯†ï¼Œå®ç°å¼€æ”¾ä¸–ç•Œä¸­çš„ç›®æ ‡ç»“æ„æ‰«æï¼Ÿ**\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡ºäº† **FlyCo**ï¼Œä¸€ä¸ªåŸºäºåŸºç¡€æ¨¡å‹çš„æ„ŸçŸ¥-é¢„æµ‹-è§„åˆ’é—­ç¯ç³»ç»Ÿï¼Œèƒ½å¤Ÿä»…å‡­ç®€å•çš„æ–‡æœ¬æˆ–è§†è§‰æç¤ºï¼Œåœ¨æœªçŸ¥å¼€æ”¾ç¯å¢ƒä¸­å®ç°å…¨è‡ªä¸»ã€é«˜æ•ˆã€å®‰å…¨çš„ä¸‰ç»´ç›®æ ‡æ‰«æã€‚å…¶æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åŒ…æ‹¬ï¼š\n- **æç¤ºé©±åŠ¨çš„åœºæ™¯æ„ŸçŸ¥æ¨¡å—**ï¼šèåˆæµå¼ä¼ æ„Ÿå™¨æ•°æ®ä¸è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡**è·¨æ¨¡æ€ç²¾ç‚¼**å’Œ**æµå¼é«˜æ•ˆé€‚é…**ï¼Œåœ¨å¤æ‚ä¸‰ç»´åœºæ™¯ä¸­é²æ£’åœ°å®šä½å’Œè·Ÿè¸ªç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡ç»“æ„ã€‚\n- **å¤šæ¨¡æ€è¡¨é¢é¢„æµ‹æ¨¡å—**ï¼šè®¾è®¡äº†ä¸€ä¸ªæ–°é¢–çš„**å¤šæ¨¡æ€ç½‘ç»œ**ï¼Œèåˆéƒ¨åˆ†å‡ ä½•è§‚æµ‹ã€è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œå¹¶åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„çŸ¥è¯†ï¼Œä»å±€éƒ¨è§‚æµ‹ä¸­æ¨æ–­ç›®æ ‡çš„å®Œæ•´ä¸‰ç»´å‡ ä½•å½¢çŠ¶ã€‚è¯¥æ¨¡å—é‡‡ç”¨äº†**æœ€å°å½’çº³åå·®æ¶æ„**ã€**è‡ªåŠ¨å¤šæ¨¡æ€æ•°æ®ç”Ÿæˆ**ã€**è‡ªé€‚åº”ç‚¹äº‘åŠ å¯†**å’Œ**å±€éƒ¨è¡¨é¢æ­£åˆ™åŒ–**ç­‰å…³é”®è®¾è®¡ï¼Œç¡®ä¿äº†é¢„æµ‹çš„å…¨å±€åˆç†æ€§ä¸å±€éƒ¨ä¿çœŸåº¦ã€‚\n- **é¢„æµ‹æ„ŸçŸ¥çš„åˆ†å±‚è§„åˆ’å™¨**ï¼šæ„å»ºäº†ä¸€ä¸ª**ä¸€è‡´æ€§æ„ŸçŸ¥çš„å…¨å±€è¦†ç›–è§„åˆ’å™¨**ï¼Œåˆ©ç”¨é¢„æµ‹çš„å®Œæ•´å‡ ä½•ä¿¡æ¯ç”Ÿæˆé«˜æ•ˆã€æ—¶åºä¸€è‡´çš„æ‰«æè·¯å¾„ï¼›åŒæ—¶ï¼Œé‡‡ç”¨**å¼‚æ­¥è®¡ç®—æ¡†æ¶**å’Œ**è§†ç‚¹çº¦æŸçš„è½¨è¿¹ä¼˜åŒ–**ï¼Œå®ç°äº†é«˜é¢‘å±€éƒ¨é‡è§„åˆ’ï¼Œåœ¨ä¿è¯è¦†ç›–å®Œæ•´æ€§çš„åŒæ—¶æ»¡è¶³å®æ—¶é¿éšœéœ€æ±‚ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nç ”ç©¶é€šè¿‡**çœŸå®ä¸–ç•Œé‡å¤–é£è¡Œå®éªŒ**å’Œ**å¤§è§„æ¨¡ä»¿çœŸåŸºå‡†æµ‹è¯•**å¯¹FlyCoè¿›è¡Œäº†å…¨é¢éªŒè¯ï¼š\n- **çœŸå®å®éªŒ**ï¼šåœ¨æ‹±æ¡¥ã€éŸ³ä¹å…ã€åŸå ¡å¤§é—¨ç­‰å¤šä¸ªå¤æ‚ã€æœªæµ‹ç»˜åœºæ™¯ä¸­ï¼ŒFlyCoä»…å‡­ç®€å•æç¤ºå³å¯æˆåŠŸå®Œæˆè‡ªä¸»æ‰«æï¼Œå±•ç¤ºäº†ç²¾ç¡®çš„åœºæ™¯ç†è§£ã€é«˜æ•ˆç‡å’Œå®æ—¶å®‰å…¨æ€§ã€‚\n- **ä»¿çœŸå¯¹æ¯”**ï¼šä¸ç°æœ‰ä¸»æµæ–¹æ³•ï¼ˆå¦‚ä¸¤é˜¶æ®µæ–¹æ³•Plan3Då’Œæ¢ç´¢å¼æ–¹æ³•Star-Searcherï¼‰ç›¸æ¯”ï¼ŒFlyCoåœ¨**é™ä½äººå·¥è¾“å…¥æˆæœ¬**çš„åŒæ—¶ï¼Œå®ç°äº†**1.25-3å€çš„é£è¡Œæ—¶é—´å‡å°‘**ã€**4.3%-56.2%çš„ç›®æ ‡è¦†ç›–ç‡æå‡**ä»¥åŠ**æ›´é«˜çš„ä»»åŠ¡æˆåŠŸç‡**ã€‚\n- **æ¶ˆèå®éªŒ**ï¼šç³»ç»Ÿæ€§åœ°éªŒè¯äº†æ„ŸçŸ¥ã€é¢„æµ‹ã€è§„åˆ’å„æ¨¡å—ä¸­å…³é”®è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œè¯å®äº†å…¶å¯¹äºæå‡ç³»ç»Ÿæ•´ä½“æ€§èƒ½çš„è´¡çŒ®ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\nFlyCoæå‡ºå¹¶éªŒè¯äº†ä¸€ç§**åŸåˆ™æ€§ä¸”çµæ´»çš„ç³»ç»Ÿæ¶æ„**ï¼Œé¦–æ¬¡æˆåŠŸåœ°å°†åŸºç¡€æ¨¡å‹çš„é€šç”¨åœºæ™¯çŸ¥è¯†ä¸å…ˆè¿›çš„æ— äººæœºé£è¡ŒæŠ€èƒ½æ— ç¼é›†æˆã€‚å®ƒ**æ˜¾è‘—é™ä½äº†ä¸‰ç»´æ‰«æä»»åŠ¡å¯¹äººå·¥å…ˆéªŒçš„ä¾èµ–**ï¼Œæ¨åŠ¨äº†æ— äººæœºåœ¨å¼€æ”¾ä¸–ç•Œä¸­æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„è‡ªä¸»åŒ–æ°´å¹³ã€‚è¯¥ç³»ç»Ÿä½œä¸ºä¸€ä¸ªå¯æ‰©å±•çš„è“å›¾ï¼Œèƒ½å¤Ÿæ–¹ä¾¿åœ°é›†æˆæœªæ¥æ›´å¼ºå¤§çš„åŸºç¡€æ¨¡å‹å’Œæœºå™¨äººæŠ€æœ¯è¿›å±•ï¼Œåœ¨åŸºç¡€è®¾æ–½å·¡æ£€ã€æ–‡åŒ–é—äº§æ•°å­—åŒ–ã€ç¾å®³ç°åœºè¯„ä¼°ç­‰é¢†åŸŸå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šFlyCo\n\n### **ä¸€ã€ ç ”ç©¶é—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³ä¸€ä¸ªå…³é”®çš„**ç³»ç»Ÿçº§é—®é¢˜**ï¼šåœ¨å¼€æ”¾ä¸–ç•Œçš„æœªçŸ¥ç¯å¢ƒä¸­ï¼Œå¦‚ä½•è®¾è®¡ä¸€ç§ç³»ç»Ÿæ¶æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨**åŸºç¡€æ¨¡å‹**çš„çŸ¥è¯†ï¼Œå®ç°æ— äººæœºå¯¹ç”¨æˆ·æŒ‡å®šçš„3Dç»“æ„è¿›è¡Œ**å®Œå…¨è‡ªä¸»ã€é«˜æ•ˆä¸”å®‰å…¨**çš„æ‰«æï¼Ÿ\n\n**ç°æœ‰æ–¹æ³•çš„ç“¶é¢ˆ**ï¼š\n1.  **ä¾èµ–å¼ºäººå·¥å…ˆéªŒ**ï¼šéœ€è¦ç”¨æˆ·æä¾›ç²¾ç¡®çš„3Dè¾¹ç•Œæ¡†ã€æ‰‹åŠ¨åˆ†å‰²æ¨¡å‹æˆ–é¢„å®šä¹‰é£è¡Œè·¯å¾„ï¼Œè´¹æ—¶è´¹åŠ›ä¸”ä¸çµæ´»ã€‚\n2.  **ç¼ºä¹åœºæ™¯ç†è§£**ï¼šç°æœ‰æ— äººæœºç³»ç»Ÿç¼ºä¹äººç±»é£è¡Œå‘˜æ‰€å…·æœ‰çš„é€šç”¨ä¸–ç•ŒçŸ¥è¯†ï¼ˆå¦‚è¯†åˆ«ç›®æ ‡ã€æ¨æ–­å®Œæ•´å‡ ä½•å½¢çŠ¶ï¼‰ï¼Œå¯¼è‡´æ‰«ææ•ˆç‡ä½ä¸‹ã€‚\n3.  **è§„åˆ’çŸ­è§†ä¸å®‰å…¨æ€§å†²çª**ï¼šåŸºäºæ¢ç´¢çš„æ–¹æ³•ï¼ˆå¦‚å‰æ²¿æ¢æµ‹ï¼‰åªåšå±€éƒ¨æœ€ä¼˜å†³ç­–ï¼Œå¯¼è‡´é£è¡Œè·¯å¾„ä½æ•ˆï¼›è€Œå…¨å±€è¦†ç›–è§„åˆ’è®¡ç®—é‡å¤§ï¼Œéš¾ä»¥åŒæ—¶ä¿è¯å®æ—¶é¿éšœã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼ˆè§£å†³æ–¹æ¡ˆï¼‰**\nFlyCo æå‡ºäº†ä¸€ç§**åŸåˆ™æ€§ã€æ¨¡å—åŒ–çš„ç³»ç»Ÿæ¶æ„**ï¼Œå°†åŸºç¡€æ¨¡å‹çš„â€œåœºæ™¯ç†è§£ä¸çŸ¥è¯†â€ä¸å…ˆè¿›çš„â€œé£è¡ŒæŠ€èƒ½â€æ— ç¼é›†æˆã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ª**ç”±åŸºç¡€æ¨¡å‹èµ‹èƒ½çš„æ„ŸçŸ¥-é¢„æµ‹-è§„åˆ’é—­ç¯ç³»ç»Ÿ**ã€‚\n\n**åˆ›æ–°æ¶æ„åˆ†è§£**ï¼š\n\n| æ¨¡å— | æ ¸å¿ƒåŠŸèƒ½ | å…³é”®æŠ€æœ¯åˆ›æ–° | è§£å†³çš„æ ¸å¿ƒæŒ‘æˆ˜ |\n| :--- | :--- | :--- | :--- |\n| **1. æç¤ºæ¥åœ°çš„åœºæ™¯æ„ŸçŸ¥** | å°†ç”¨æˆ·æ–‡æœ¬/è§†è§‰æç¤ºå®æ—¶è½¬åŒ–ä¸º3Dç›®æ ‡åˆ†å‰² | - **æµå¼é«˜æ•ˆé€‚é…**ï¼šå¯¹SAM2è¿›è¡Œæ»‘åŠ¨çª—å£å†…å­˜ç®¡ç†å’Œæ¨¡å‹è½»é‡åŒ–ï¼Œå®ç°é•¿æ—¶ç¨‹å®æ—¶å¤„ç†ã€‚<br>- **è·¨æ¨¡æ€ç²¾ç‚¼**ï¼šèåˆ2Dè§†è§‰åˆ†å‰²ä¸3Då‡ ä½•èšç±»ï¼ˆä½¿ç”¨å¡å°”æ›¼æ»¤æ³¢å’ŒIoUæ ¡éªŒï¼‰ï¼Œé€šè¿‡åŒå‘äº¤äº’ç¡®ä¿ç›®æ ‡è·Ÿè¸ªåœ¨è§†è§’å˜åŒ–ä¸‹çš„**é²æ£’æ€§**ã€‚ | åœ¨æœªçŸ¥æ‚ä¹±ç¯å¢ƒä¸­ï¼Œ**ç²¾ç¡®ã€ç¨³å®šåœ°è¯†åˆ«å’Œè·Ÿè¸ªç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡ç»“æ„**ï¼Œæ— éœ€äººå·¥æ¡†å®šèŒƒå›´ã€‚ |\n| **2. å¤šæ¨¡æ€è¡¨é¢é¢„æµ‹** | æ ¹æ®éƒ¨åˆ†è§‚æµ‹ï¼Œé¢„æµ‹ç›®æ ‡çš„å®Œæ•´3Då‡ ä½•è¡¨é¢ | - **å¤šæ¨¡æ€åèåˆç½‘ç»œ**ï¼šä»¥å‡ ä½•ç‚¹äº‘ä¸ºä¸»å¹²ï¼Œé€šè¿‡**äº¤æ›¿æ³¨æ„åŠ›æœºåˆ¶**èåˆå›¾åƒï¼ˆDINOï¼‰å’Œæ–‡æœ¬ï¼ˆSigLIPï¼‰çš„åŸºç¡€æ¨¡å‹ç‰¹å¾ï¼Œå®ç°çŸ¥è¯†è’¸é¦ã€‚<br>- **è‡ªåŠ¨å¤šæ¨¡æ€æ•°æ®ç”Ÿæˆ**ï¼šåˆ©ç”¨åŸºç¡€æ¨¡å‹ï¼ˆå›¾åƒæè¿°+GPT-4ï¼‰ä»3Dèµ„äº§è‡ªåŠ¨ç”Ÿæˆå¯¹é½çš„ï¼ˆç‚¹äº‘ã€å›¾åƒã€æ–‡æœ¬ï¼‰è®­ç»ƒæ•°æ®ï¼Œå®ç°**å¤§è§„æ¨¡æ— æ ‡æ³¨å­¦ä¹ **ã€‚<br>- **éƒ¨åˆ†è¡¨é¢æ­£åˆ™åŒ–**ï¼šåœ¨è®­ç»ƒæŸå¤±ä¸­åŠ å…¥é¢„æµ‹è¡¨é¢å¯¹è¾“å…¥éƒ¨åˆ†ç‚¹äº‘çš„çº¦æŸï¼Œç¡®ä¿é¢„æµ‹**å±€éƒ¨å¿ å®**äºè§‚æµ‹ï¼Œé¿å…å‡ ä½•å¹»è§‰ã€‚<br>- **æ¨ç†æ—¶å‡ ä½•è‡´å¯†åŒ–**ï¼šçº§è”ä¸Šé‡‡æ ·ç½‘ç»œï¼Œæ ¹æ®ç›®æ ‡ç‰©ç†å°ºåº¦è‡ªé€‚åº”è°ƒæ•´è¾“å‡ºç‚¹äº‘å¯†åº¦ï¼Œå…¼é¡¾ç»†èŠ‚ä¸æ•ˆç‡ã€‚ | ä¸ºè§„åˆ’æä¾›**å…¨å±€ç»“æ„é¢„è§**ï¼Œå…‹æœå±€éƒ¨è§‚æµ‹çš„å±€é™æ€§ï¼Œå®ç°é«˜æ•ˆã€ç›®æ ‡æ˜ç¡®çš„è¦†ç›–è§„åˆ’ã€‚ |\n| **3. é¢„æµ‹æ„ŸçŸ¥çš„åˆ†å±‚è§„åˆ’å™¨** | åŸºäºé¢„æµ‹çš„å‡ ä½•ï¼Œç”Ÿæˆé«˜æ•ˆã€å®‰å…¨ã€ä¸€è‡´çš„è¦†ç›–è·¯å¾„ | - **ä¸€è‡´æ€§æ„ŸçŸ¥çš„å…¨å±€è§„åˆ’**ï¼šåœ¨ç”Ÿæˆè§†ç‚¹å’Œè·¯å¾„æ—¶ï¼Œ**é‡ç”¨å†å²è§†ç‚¹ã€åŒ¹é…å†å²åºåˆ—ç»„**ï¼Œç¡®ä¿è§„åˆ’ç»“æœéšæ—¶é—´å¹³æ»‘æ¼”åŒ–ï¼Œé¿å…è·¯å¾„æŠ–åŠ¨å’Œé‡å¤æ‰«æã€‚<br>- **é«˜æ•ˆè®¡ç®—ç­–ç•¥**ï¼šé‡‡ç”¨**åŸºäºå†…ç‚¹çš„å¿«é€Ÿå®‰å…¨ç©ºé—´ç”Ÿæˆ**å’Œ**åŸºäºéª¨æ¶çš„ä¸¤çº§åˆ†è§£**ï¼Œå°†NP-hardçš„è¦†ç›–è·¯å¾„é—®é¢˜å¹¶è¡ŒåŒ–ï¼Œå®ç°**å®æ—¶æ±‚è§£**ã€‚<br>- **å¼‚æ­¥åˆ†å±‚æ¡†æ¶**ï¼š**å…¨å±€è§„åˆ’**ï¼ˆ~1Hzï¼‰è´Ÿè´£é•¿æ—¶ç¨‹è¦†ç›–ï¼›**å±€éƒ¨è§„åˆ’**ï¼ˆ~20Hzï¼‰è´Ÿè´£å®æ—¶é¿éšœå’Œè½¨è¿¹ä¼˜åŒ–ã€‚äºŒè€…å¼‚æ­¥è¿è¡Œï¼Œè§£è€¦äº†æ•ˆç‡ä¸å®‰å…¨å¯¹è®¡ç®—é€Ÿåº¦çš„çŸ›ç›¾ã€‚<br>- **è§†ç‚¹çº¦æŸè½¨è¿¹ä¼˜åŒ–**ï¼šåœ¨å±€éƒ¨è½¨è¿¹ä¼˜åŒ–ä¸­ç¡¬æ€§çº¦æŸæ— äººæœºå¿…é¡»ç»è¿‡å…³é”®è¦†ç›–è§†ç‚¹ï¼Œä»è€Œåœ¨é¿éšœæ—¶**ä¸ä¸¢å¤±ç›®æ ‡è¦†ç›–å®Œæ•´æ€§**ã€‚ | å°†é¢„æµ‹çš„â€œçŸ¥è¯†â€è½¬åŒ–ä¸º**å¯æ‰§è¡Œçš„åŠ¨ä½œ**ï¼Œåœ¨ä¿è¯**å®Œå…¨è¦†ç›–**å’Œ**é£è¡Œæ•ˆç‡**çš„åŒæ—¶ï¼Œæ»¡è¶³**å®æ—¶å®‰å…¨é¿éšœ**çš„åˆšæ€§éœ€æ±‚ã€‚ |\n\n### **ä¸‰ã€ æŠ€æœ¯åˆ›æ–°çš„å®é™…ä»·å€¼**\n1.  **æå¤§é™ä½äººå·¥æˆæœ¬**ï¼šç”¨æˆ·åªéœ€æä¾›**è‡ªç„¶è¯­è¨€æè¿°**ï¼ˆå¦‚â€œæ‰«æå±±è°·ä¸­çš„åŸå ¡â€ï¼‰å’Œ**ç¨€ç–çš„2Dæ ‡æ³¨**ï¼Œç³»ç»Ÿå³å¯è‡ªä¸»å®Œæˆåç»­æ‰€æœ‰å·¥ä½œï¼Œæ‘†è„±äº†å¯¹ä¸“å®¶æ“ä½œå’Œç²¾ç»†å…ˆéªŒçš„ä¾èµ–ã€‚\n2.  **æ˜¾è‘—æå‡æ‰«ææ•ˆç‡ä¸è´¨é‡**ï¼š\n    - **å®éªŒè¯æ˜**ï¼šåœ¨ä»¿çœŸä¸­ï¼Œç›¸æ¯”å‰æ²¿æ–¹æ³•ï¼ˆPlan3D, Star-Searcherï¼‰ï¼ŒFlyCo å°†é£è¡Œæ—¶é—´å‡å°‘ **1.25 - 3å€**ï¼Œç›®æ ‡è¦†ç›–ç‡æå‡ **4.3% - 56.2%**ï¼ŒæˆåŠŸç‡æå‡ **11.6% - 50.1%**ã€‚\n    - **å…³é”®åŸå› **ï¼šé¢„æµ‹æ¨¡å—æä¾›çš„å…¨å±€é¢„è§æ€§ï¼Œä½¿æ— äººæœºèƒ½ä¸»åŠ¨è§„åˆ’å»æœªè§‚æµ‹åŒºåŸŸï¼Œé¿å…äº†åŸºäºæ¢ç´¢æ–¹æ³•çš„â€œç›²ç›®æ€§â€å’Œé‡å¤æ‰«æã€‚\n3.  **å®ç°å¼€æ”¾ä¸–ç•Œçš„é²æ£’æ€§ä¸å®‰å…¨æ€§**ï¼š\n    - **çœŸå®ä¸–ç•ŒéªŒè¯**ï¼šåœ¨åŒ…å«æ¡¥æ¢ã€éŸ³ä¹å…ã€åŸå ¡ç­‰å¤šç§å¤æ‚ã€æ‚ä¹±åœºæ™¯ä¸­æˆåŠŸå®Œæˆå…¨è‡ªä¸»æ‰«æä»»åŠ¡ã€‚\n    - **å®‰å…¨ä¿è¯**ï¼šå¼‚æ­¥åˆ†å±‚è§„åˆ’å™¨ç¡®ä¿äº†å³ä½¿åœ¨å¯†é›†éšœç¢ç‰©ï¼ˆå¦‚æ ‘æ—ã€è¡Œäººï¼‰ä¸­ï¼Œä¹Ÿèƒ½ä»¥é«˜é¢‘ï¼ˆ20Hzï¼‰é‡æ–°è§„åˆ’è½¨è¿¹ï¼Œå§‹ç»ˆä¿æŒå®‰å…¨è·ç¦»ã€‚\n4.  **æä¾›å¯æ‰©å±•çš„ç³»ç»Ÿè“å›¾**ï¼š\n    - **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ„ŸçŸ¥ã€é¢„æµ‹ã€è§„åˆ’ä¸‰ä¸ªæ¨¡å—æ¥å£æ¸…æ™°ï¼Œå¯ä»¥**ç‹¬ç«‹å‡çº§**ï¼ˆä¾‹å¦‚ï¼Œæœªæ¥æœ‰æ›´å¼ºçš„3DåŸºç¡€æ¨¡å‹å¯ç›´æ¥æ›¿æ¢é¢„æµ‹æ¨¡å—ï¼‰ã€‚\n    - **å¹³è¡¡äº†FMèƒ½åŠ›ä¸æœºå™¨äººéœ€æ±‚**ï¼šæ²¡æœ‰é‡‡ç”¨æ•°æ®é¥¥æ¸´çš„ç«¯åˆ°ç«¯å­¦ä¹ ï¼Œè€Œæ˜¯é€šè¿‡æ¨¡å—åŒ–æ¶æ„åˆ©ç”¨FMåœ¨æ„ŸçŸ¥å’Œé¢„æµ‹ä¸Šçš„ä¼˜åŠ¿ï¼ŒåŒæ—¶ç”¨ä¼˜åŒ–æ–¹æ³•è§£å†³å…¶ä¸æ“…é•¿çš„å®æ—¶è§„åˆ’å’Œå®‰å…¨æ€§é—®é¢˜ã€‚\n\n### **æ€»ç»“**\nFlyCo çš„æ ¸å¿ƒåˆ›æ–°åœ¨äº**ç³»ç»Ÿæ€§åœ°å›ç­”äº†å¦‚ä½•å°†åŸºç¡€æ¨¡å‹æœ‰æ•ˆé›†æˆåˆ°æ— äººæœºæ‰«æç³»ç»Ÿä¸­**ã€‚å®ƒä¸æ˜¯ä¸€ä¸ªå•ä¸€çš„ç®—æ³•æ”¹è¿›ï¼Œè€Œæ˜¯ä¸€ä¸ª**å®Œæ•´çš„æ¶æ„è®¾è®¡**ï¼Œé€šè¿‡**æ„ŸçŸ¥-é¢„æµ‹-è§„åˆ’**çš„é—­ç¯ï¼Œå°†åŸºç¡€æ¨¡å‹çš„â€œè®¤çŸ¥æ™ºèƒ½â€è½¬åŒ–ä¸ºæ— äººæœºåœ¨ç‰©ç†ä¸–ç•Œä¸­â€œè¡ŒåŠ¨æ™ºèƒ½â€ï¼Œæœ€ç»ˆå®ç°äº†**ä»…éœ€ä½åŠªåŠ›äººç±»æç¤ºã€å³å¯åœ¨æœªçŸ¥å¼€æ”¾ä¸–ç•Œä¸­å¯¹ç‰¹å®šç»“æ„è¿›è¡Œé«˜æ•ˆã€å®‰å…¨ã€å®Œæ•´3Dæ‰«æ**çš„å®ç”¨åŒ–ç³»ç»Ÿã€‚è®ºæ–‡é€šè¿‡å¤§é‡çš„çœŸå®ä¸–ç•Œå®éªŒå’Œä»¿çœŸåŸºå‡†æµ‹è¯•ï¼Œå……åˆ†éªŒè¯äº†è¯¥æ¶æ„çš„ä¼˜è¶Šæ€§å’Œå¯è¡Œæ€§ã€‚",
            "timestamp": "2026-01-21T12:10:37.545746",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡ã€ŠFlyCo: Foundation Model-Empowered Drones for Autonomous 3D Structure Scanning in Open-World Environmentsã€‹æŠ€æœ¯è„‰ç»œä¸å‘å±•æ–¹å‘åˆ†æ\n\næœ¬æ–‡æå‡ºçš„ **FlyCo** ç³»ç»Ÿä»£è¡¨äº†æ— äººæœºè‡ªä¸»3Dç»“æ„æ‰«æé¢†åŸŸçš„ä¸€ä¸ªé‡è¦èŒƒå¼è½¬å˜ã€‚å®ƒå¹¶éå­¤ç«‹å­˜åœ¨ï¼Œè€Œæ˜¯å»ºç«‹åœ¨å¯¹è¯¥é¢†åŸŸ**ä¸‰ä¸ªä¸»è¦æŠ€æœ¯èŒƒå¼**ï¼ˆç»å…¸ä¸¤é˜¶æ®µã€æ¢ç´¢å¼ã€é¢„æµ‹å¼ï¼‰çš„æ·±åˆ»ç†è§£å’Œç³»ç»Ÿæ€§æ”¹è¿›ä¹‹ä¸Šã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº**é¦–æ¬¡ç³»ç»Ÿæ€§åœ°å°†åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„çŸ¥è¯†ä¸æœºå™¨äººé£è¡ŒæŠ€èƒ½æ·±åº¦èåˆ**ï¼Œå½¢æˆäº†ä¸€ä¸ªé—­ç¯çš„æ„ŸçŸ¥-é¢„æµ‹-è§„åˆ’æ¶æ„ï¼Œä»è€Œè§£å†³äº†å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­ä¾èµ–å¼ºäººå·¥å…ˆéªŒã€æ•ˆç‡ä½ä¸‹å’Œé€‚åº”æ€§å·®çš„æ ¸å¿ƒç“¶é¢ˆã€‚\n\nä»¥ä¸‹å°†æ¢³ç†è¯¥æ–¹å‘çš„å‘å±•è„‰ç»œï¼Œåˆ†ææ¯ä¸€æ­¥çš„æ”¹è¿›ï¼Œå¹¶ç€é‡é˜è¿°å‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚\n\n### ä¸€ã€æŠ€æœ¯å‘å±•è„‰ç»œä¸æ ¸å¿ƒæ”¹è¿›\n\n#### 1. **ç»å…¸ä¸¤é˜¶æ®µèŒƒå¼ (Classic Two-stage Paradigm)**\n*   **ä»£è¡¨å·¥ä½œ**: Plan3D (Hepp et al., 2018), Roberts et al. (2017), ä»¥åŠå•†ä¸šè½¯ä»¶ï¼ˆDJI Terra, PIX4Dcaptureï¼‰ã€‚\n*   **æ ¸å¿ƒæµç¨‹**: 1) **ç²—æ‰«æé˜¶æ®µ**: æ— äººæœºæ‰§è¡Œé¢„å®šä¹‰ï¼ˆå¦‚â€œå‰²è‰æœºâ€å¼ï¼‰æˆ–ç”¨æˆ·åˆ’å®šåŒºåŸŸçš„é£è¡Œï¼Œé‡‡é›†åˆå§‹æ•°æ®ã€‚2) **äººå·¥å¤„ç†ä¸ç¦»çº¿è§„åˆ’**: æ“ä½œå‘˜æ‰‹åŠ¨ä»ç²—æ•°æ®ä¸­åˆ†å‰²å‡ºç›®æ ‡ç»“æ„ï¼Œå¹¶åŸºäºæ­¤ç”Ÿæˆç²¾ç»†çš„3Dæ¨¡å‹ï¼Œæœ€åè¿›è¡Œç¦»çº¿çš„è¦†ç›–è·¯å¾„è§„åˆ’ã€‚3) **ç²¾æ‰«æé˜¶æ®µ**: æ‰§è¡Œç¦»çº¿ç”Ÿæˆçš„ç²¾ç»†è·¯å¾„ã€‚\n*   **ç›¸æ¯”äºçº¯é¢„å®šä¹‰æ¨¡å¼çš„æ”¹è¿›**:\n    *   **æ”¹è¿›ç‚¹**: å¼•å…¥äº†**ç›®æ ‡å‡ ä½•ä¿¡æ¯**åˆ°è§„åˆ’ä¸­ï¼Œé€šè¿‡ç¦»çº¿ä¼˜åŒ–æå‡äº†æ‰«æçš„è¦†ç›–è´¨é‡å’Œç»†èŠ‚ã€‚\n    *   **å±€é™æ€§**:\n        1.  **é«˜äººå·¥æˆæœ¬**: ä¸¥é‡ä¾èµ–æ“ä½œå‘˜è¿›è¡Œç›®æ ‡åˆ†å‰²å’Œå‚æ•°è°ƒæ•´ã€‚\n        2.  **ç¼ºä¹å®æ—¶æ€§**: ç¦»çº¿è§„åˆ’æ— æ³•åº”å¯¹é£è¡Œä¸­å‡ºç°çš„æœªçŸ¥éšœç¢ç‰©ã€‚\n        3.  **åƒµåŒ–ä¸çµæ´»**: å¯¹åˆå§‹ç²—æ‰«æçš„è´¨é‡æ•æ„Ÿï¼Œä¸”éš¾ä»¥é€‚åº”å¤æ‚ã€éå‡¸çš„å‡ ä½•å½¢çŠ¶ã€‚\n\n#### 2. **æ¢ç´¢å¼èŒƒå¼ (Exploration-based Paradigm)**\n*   **ä»£è¡¨å·¥ä½œ**: Star-Searcher (Luo et al., 2024), Bircher et al. (2018), Song and Jo (2017)ã€‚\n*   **æ ¸å¿ƒæµç¨‹**: ç”¨æˆ·æä¾›ä¸€ä¸ª**3DåŒ…å›´ç›’**ï¼Œå‡å®šç›®æ ‡å®Œå…¨ä½äºå…¶ä¸­ã€‚æ— äººæœºåœ¨çº¿åœ°æ¢ç´¢è¯¥ä½“ç§¯å†…çš„æ‰€æœ‰æœªçŸ¥ç©ºé—´ï¼ˆåŸºäºå‰æ²¿æˆ–ä¸‹ä¸€æœ€ä½³è§†è§’ç­–ç•¥ï¼‰ï¼ŒåŒæ—¶è§„åˆ’ç›¸æœºè¿åŠ¨ä»¥è¦†ç›–æ‰€æœ‰å·²å‘ç°çš„è¡¨é¢ã€‚\n*   **ç›¸æ¯”äºä¸¤é˜¶æ®µèŒƒå¼çš„æ”¹è¿›**:\n    *   **æ”¹è¿›ç‚¹**:\n        1.  **é™ä½äººå·¥ä»‹å…¥**: æ— éœ€æ‰‹åŠ¨åˆ†å‰²ï¼Œå®ç°äº†å•æ¬¡ä»»åŠ¡å®Œæˆã€‚\n        2.  **å®æ—¶é€‚åº”æ€§**: èƒ½å¤Ÿåœ¨çº¿é¿éšœï¼Œé€‚åº”æœªçŸ¥ç¯å¢ƒã€‚\n    *   **å±€é™æ€§**:\n        1.  **å¯¹å…ˆéªŒæ•æ„Ÿ**: æ€§èƒ½é«˜åº¦ä¾èµ–äºç”¨æˆ·æä¾›çš„åŒ…å›´ç›’ç²¾åº¦ã€‚è¿‡å¤§çš„ç›’å­å¯¼è‡´å¤§é‡éç›®æ ‡åŒºåŸŸæ‰«æï¼Œè¿‡å°çš„ç›’å­åˆ™å¯èƒ½é—æ¼ç›®æ ‡éƒ¨åˆ†ã€‚\n        2.  **å±€éƒ¨è¿‘è§†ä¸ä½æ•ˆ**: åŸºäºå±€éƒ¨ä¿¡æ¯å¢ç›Šï¼ˆå¦‚å‰æ²¿ï¼‰çš„å†³ç­–ç¼ºä¹å…¨å±€è§†é‡ï¼Œå¯¼è‡´è·¯å¾„å†—ä½™ã€é‡å¤è®¿é—®ï¼Œæ•´ä½“ä»»åŠ¡æ—¶é—´å¢é•¿ã€‚\n        3.  **ç›®æ ‡æ— å…³æ‰«æ**: æœ¬è´¨ä¸Šæ˜¯â€œä½“ç§¯æ¢ç´¢â€è€Œéâ€œç›®æ ‡æ‰«æâ€ï¼Œä¼šæµªè´¹èµ„æºåœ¨éç›®æ ‡ç‰©ä½“ä¸Šã€‚\n\n#### 3. **é¢„æµ‹å¼èŒƒå¼ (Prediction-involved Paradigm)**\n*   **ä»£è¡¨å·¥ä½œ**: PredRecon (Feng et al., 2023), Guedon et al. (2023), Chen et al. (2024)ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**: å¼•å…¥**é¢„æµ‹èƒ½åŠ›**ï¼Œä¸ºè§„åˆ’æä¾›**å…¨å±€ä¸Šä¸‹æ–‡**ã€‚é€šè¿‡ç¥ç»ç½‘ç»œä»éƒ¨åˆ†è§‚æµ‹ä¸­æ¨æ–­ç›®æ ‡çš„å®Œæ•´å‡ ä½•å½¢çŠ¶ï¼Œæˆ–å­¦ä¹ é¢„æµ‹ä¿¡æ¯é‡æœ€å¤§çš„ä¸‹ä¸€ä¸ªè§†è§’ã€‚\n*   **ç›¸æ¯”äºæ¢ç´¢å¼èŒƒå¼çš„æ”¹è¿›**:\n    *   **æ”¹è¿›ç‚¹**:\n        1.  **æä¾›å…¨å±€å¼•å¯¼**: é€šè¿‡é¢„æµ‹æœªè§‚å¯Ÿåˆ°çš„éƒ¨åˆ†ï¼Œè§„åˆ’å™¨å¯ä»¥è¿›è¡Œæ›´ç»“æ„åŒ–ã€å‰ç»æ€§çš„è·¯å¾„è§„åˆ’ï¼Œå…‹æœå±€éƒ¨è¿‘è§†é—®é¢˜ã€‚\n        2.  **æ½œåŠ›æå‡æ•ˆç‡**: ç†è®ºä¸Šèƒ½å‡å°‘å†—ä½™æ¢ç´¢ï¼Œæ›´ç›´æ¥åœ°è¦†ç›–ç›®æ ‡ã€‚\n    *   **å±€é™æ€§**:\n        1.  **æ³›åŒ–èƒ½åŠ›ä¸çœŸå®æ€§å­˜ç–‘**: ç°æœ‰æ–¹æ³•å¤šåœ¨ç®€åŒ–ä»¿çœŸç¯å¢ƒï¼ˆæ— æ‚ä¹±èƒŒæ™¯ã€ç›®æ ‡å•ä¸€ï¼‰ä¸­éªŒè¯ï¼Œå…¶**é›¶æ ·æœ¬æ³›åŒ–**åˆ°çœŸå®å¤æ‚åœºæ™¯çš„èƒ½åŠ›æœªç»è¯å®ã€‚\n        2.  **é¢„æµ‹ä¸è§‚æµ‹ä¸ä¸€è‡´**: æ—©æœŸé¢„æµ‹æ–¹æ³•å¯èƒ½äº§ç”Ÿä¸å·²è§‚æµ‹éƒ¨åˆ†å‡ ä½•ä¸ç¬¦çš„â€œå¹»è§‰â€ï¼Œè¯¯å¯¼è§„åˆ’ã€‚\n        3.  **ç³»ç»Ÿæ•´åˆä¸è¶³**: å¤šä¸ºç®—æ³•ç ”ç©¶ï¼Œç¼ºä¹åœ¨çœŸå®æ— äººæœºå¹³å°ä¸Šç«¯åˆ°ç«¯çš„ç³»ç»Ÿé›†æˆä¸éªŒè¯ã€‚\n\n#### 4. **åŸºç¡€æ¨¡å‹èµ‹èƒ½èŒƒå¼ (Foundation Model-Empowered Paradigm) - FlyCo**\n*   **ä»£è¡¨å·¥ä½œ**: **æœ¬æ–‡çš„FlyCoç³»ç»Ÿ**ã€‚\n*   **æ ¸å¿ƒåˆ›æ–°**: æ„å»ºä¸€ä¸ª**åŸåˆ™æ€§çš„ã€æ¨¡å—åŒ–çš„ç³»ç»Ÿæ¶æ„**ï¼Œå°†åŸºç¡€æ¨¡å‹çš„**ä¸–ç•ŒçŸ¥è¯†**ä¸å…ˆè¿›çš„**é£è¡ŒæŠ€èƒ½**æ— ç¼é›†æˆï¼Œå½¢æˆä¸€ä¸ªé—­ç¯çš„æ„ŸçŸ¥-é¢„æµ‹-è§„åˆ’å›è·¯ã€‚\n*   **ç›¸æ¯”äºä¹‹å‰æ‰€æœ‰èŒƒå¼çš„é©å‘½æ€§æ”¹è¿›**:\n    1.  **æ„ŸçŸ¥ï¼šä»å‡ ä½•å…ˆéªŒåˆ°è¯­ä¹‰ç†è§£**\n        *   **ä¹‹å‰**: ä¾èµ–3DåŒ…å›´ç›’ï¼ˆæ¢ç´¢å¼ï¼‰æˆ–äººå·¥åˆ†å‰²ï¼ˆä¸¤é˜¶æ®µå¼ï¼‰â€”â€”**éœ€è¦ä½å±‚æ¬¡ã€é«˜åŠªåŠ›çš„å‡ ä½•å…ˆéªŒ**ã€‚\n        *   **FlyCo**: åˆ©ç”¨è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLMs, SAM2ï¼‰ï¼Œä»…é€šè¿‡**æ–‡æœ¬æè¿°å’Œç¨€ç–2Dæ ‡æ³¨**è¿™ç§**ä½åŠªåŠ›ã€é«˜å±‚æ¬¡çš„è¯­ä¹‰æç¤º**ï¼Œå°±èƒ½åœ¨å¤æ‚3Dåœºæ™¯ä¸­æŒç»­ã€é²æ£’åœ°åˆ†å‰²å’Œè·Ÿè¸ªç›®æ ‡ã€‚å¼•å…¥äº†è·¨æ¨¡æ€ç²¾ç‚¼æœºåˆ¶ï¼Œç¡®ä¿äº†é•¿æ—¶é—´é£è¡Œä¸­çš„è§†è§’ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚\n    2.  **é¢„æµ‹ï¼šä»ç‰¹å®šè®­ç»ƒåˆ°é›¶æ ·æœ¬æ³›åŒ–**\n        *   **ä¹‹å‰**: ä½¿ç”¨åœ¨æœ‰é™åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„ä¸“ç”¨ç½‘ç»œï¼Œæ³›åŒ–èƒ½åŠ›å¼±ï¼Œéš¾ä»¥å¤„ç†çœŸå®ä¸–ç•Œçš„å¤šæ ·æ€§å’Œå°ºåº¦å˜åŒ–ã€‚\n        *   **FlyCo**: è®¾è®¡äº†ä¸€ä¸ª**å¤šæ¨¡æ€è¡¨é¢é¢„æµ‹ç½‘ç»œ**ï¼Œèåˆå‡ ä½•ã€è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ã€‚å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š\n            *   **åˆ©ç”¨FMå…ˆéªŒ**: é‡ç”¨é¢„è®­ç»ƒçš„è§†è§‰å’Œè¯­è¨€ç¼–ç å™¨ï¼Œæ³¨å…¥ä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†ã€‚\n            *   **è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆ**: åˆ©ç”¨FMï¼ˆGPT-4, å›¾åƒæè¿°æ¨¡å‹ï¼‰ä»3Dèµ„äº§è‡ªåŠ¨ç”Ÿæˆå¯¹é½çš„å¤šæ¨¡æ€è®­ç»ƒæ•°æ®ï¼Œå®ç°äº†å¤§è§„æ¨¡ã€æ— æ ‡æ³¨å­¦ä¹ ã€‚\n            *   **å±€éƒ¨ä¿çœŸæ­£åˆ™åŒ–**: é€šè¿‡Chamferè·ç¦»çº¦æŸï¼Œç¡®ä¿é¢„æµ‹å½¢çŠ¶ä¸¥æ ¼å¿ å®äºå·²è§‚æµ‹éƒ¨åˆ†ï¼Œé¿å…å‡ ä½•å¹»è§‰ã€‚\n            *   **è‡ªé€‚åº”ç¨ å¯†åŒ–**: åœ¨æ¨ç†æ—¶æ ¹æ®ç›®æ ‡ç‰©ç†å°ºåº¦è‡ªé€‚åº”ä¸Šé‡‡æ ·ï¼Œå¹³è¡¡ç»†èŠ‚ä¸è®¡ç®—æˆæœ¬ã€‚\n        *   **ç»“æœ**: å®ç°äº†**å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›**å’Œ**æ—¶é—´ç¨³å®šæ€§**ï¼Œä¸ºè§„åˆ’æä¾›äº†å¯é ã€å…¨å±€çš„ç»“æ„å‰ç»ã€‚\n    3.  **è§„åˆ’ï¼šä»å±€éƒ¨/ç¦»çº¿åˆ°å…¨å±€ä¸€è‡´ä¸”å®æ—¶å®‰å…¨**\n        *   **ä¹‹å‰**: ä¸¤é˜¶æ®µæ³•ç¦»çº¿è§„åˆ’ä¸å®æ—¶ï¼›æ¢ç´¢å¼æ³•å±€éƒ¨å†³ç­–ä½æ•ˆï¼›é¢„æµ‹å¼æ³•ç¼ºä¹ä¸å†å²è·¯å¾„çš„ä¸€è‡´æ€§è€ƒè™‘ã€‚\n        *   **FlyCo**: æå‡ºäº†**é¢„æµ‹æ„ŸçŸ¥çš„åˆ†å±‚è§„åˆ’å™¨**ã€‚\n            *   **å…¨å±€å±‚**: åœ¨é¢„æµ‹çš„å®Œæ•´å‡ ä½•ä¸Šè¿›è¡Œè¦†ç›–è§„åˆ’ï¼Œå¹¶æ³¨å…¥**ä¸€è‡´æ€§æ„ŸçŸ¥**ï¼ˆé‡ç”¨è§†ç‚¹ã€åŒ¹é…å†å²åºåˆ—ï¼‰ï¼Œç¡®ä¿è·¯å¾„å¹³æ»‘æ¼”è¿›ï¼Œé¿å…æŠ–åŠ¨å’Œå›æº¯ã€‚\n            *   **é«˜æ•ˆè®¡ç®—**: é‡‡ç”¨åŸºäºå†…ç‚¹çš„å¿«é€Ÿå®‰å…¨ç©ºé—´ç”Ÿæˆã€ç±»é‡åŠ›è§†ç‚¹å‰ªæå’ŒåŸºäºéª¨æ¶çš„ä¸¤çº§åˆ†è§£å¹¶è¡Œæ±‚è§£ï¼Œå®ç°äº†**å¤§è§„æ¨¡åœºæ™¯ä¸‹çš„å®æ—¶å…¨å±€è§„åˆ’**ã€‚\n            *   **å±€éƒ¨å±‚**: é«˜é¢‘è¿è¡Œçš„è½¨è¿¹ä¼˜åŒ–å™¨ï¼Œåœ¨éµå¾ªå…¨å±€è·¯å¾„æ„å›¾çš„åŒæ—¶ï¼Œ**å®æ—¶é¿éšœ**ã€‚åˆ›æ–°æ€§åœ°å¼•å…¥äº†**è§†ç‚¹çº¦æŸ**ï¼Œç¡®ä¿å±€éƒ¨é¿éšœä¸ä¼šç‰ºç‰²ç›®æ ‡è¦†ç›–å®Œæ•´æ€§ã€‚\n    4.  **ç³»ç»Ÿæ¶æ„ï¼šä»ç«¯åˆ°ç«¯å­¦ä¹ æˆ–æ¾æ•£æ¨¡å—åˆ°è§£è€¦å¼‚æ­¥é—­ç¯**\n        *   **ä¹‹å‰**: ç«¯åˆ°ç«¯å­¦ä¹ éœ€è¦å¤§é‡ä¸“å®¶æ•°æ®ä¸ç°å®ï¼›æ¾æ•£æ¨¡å—ç»„åˆéš¾ä»¥åè°ƒå®æ—¶æ€§ä¸è®¡ç®—å»¶è¿Ÿã€‚\n        *   **FlyCo**: é‡‡ç”¨**æ¨¡å—åŒ–ã€å¼‚æ­¥æ‰§è¡Œçš„é—­ç¯æ¶æ„**ã€‚æ„ŸçŸ¥ã€é¢„æµ‹ã€è§„åˆ’æ¨¡å—åœ¨ä¸åŒçº¿ç¨‹å¼‚æ­¥è¿è¡Œï¼Œé€šè¿‡å…±äº«ç¼“å†²åŒºé€šä¿¡ã€‚è¿™è§£å†³äº†FMæ¨ç†å»¶è¿Ÿé«˜ä¸é£è¡Œéœ€è¦å®æ—¶å“åº”ä¹‹é—´çš„çŸ›ç›¾ï¼Œä½¿ç³»ç»Ÿå…¼å…·**æ·±æ€ç†Ÿè™‘çš„å…¨å±€è§„åˆ’**å’Œ**æ¯«ç§’çº§çš„åº”æ€¥ååº”**èƒ½åŠ›ã€‚\n\n### äºŒã€ç€é‡å‘å±•çš„å‡ ä¸ªä¸åŒæ–¹å‘\n\nä»FlyCoçš„å·¥ä½œå¯ä»¥çœ‹å‡ºï¼Œè¯¥é¢†åŸŸæ­£æœç€ä»¥ä¸‹å‡ ä¸ªæ·±åº¦èåˆçš„æ–¹å‘æ¼”è¿›ï¼š\n\n1.  **ä»â€œå‡ ä½•é©±åŠ¨â€åˆ°â€œè¯­ä¹‰ä¸çŸ¥è¯†é©±åŠ¨â€**\n    *   æ—©æœŸå·¥ä½œå›´ç»•å‡ ä½•è¦†ç›–æœ€ä¼˜å±•å¼€ã€‚FlyCoæ ‡å¿—ç€æ–¹å‘è½¬å‘åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹æ‰€ç¼–ç çš„**é€šç”¨ä¸–ç•ŒçŸ¥è¯†å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›**ã€‚è¿™ä½¿å¾—ç³»ç»Ÿèƒ½ç†è§£â€œåŸå ¡â€ã€â€œæ‹±æ¡¥â€ç­‰æŠ½è±¡æ¦‚å¿µï¼Œå¹¶æ®æ­¤åšå‡ºæ™ºèƒ½å†³ç­–ï¼Œå¤§å¹…é™ä½äº†äººç±»äº¤äº’çš„é—¨æ§›å’Œä¸“ä¸šåŒ–è¦æ±‚ã€‚\n\n2.  **ä»â€œå­¤ç«‹ç®—æ³•â€åˆ°â€œç³»ç»Ÿæœºå™¨äººå­¦â€**\n    *   ç ”ç©¶é‡ç‚¹ä»æå‡ºå•ä¸ªæ›´å¥½çš„é¢„æµ‹æˆ–è§„åˆ’ç®—æ³•ï¼Œè½¬å‘æ„å»º**å®Œæ•´ã€é²æ£’ã€å¯å®é™…éƒ¨ç½²çš„æœºå™¨äººç³»ç»Ÿ**ã€‚FlyCoæ¶µç›–äº†ä»ç¡¬ä»¶å¹³å°ã€ä¼ æ„Ÿå™¨å¤„ç†ã€çŠ¶æ€ä¼°è®¡ã€åœ°å›¾æ„å»ºã€åˆ°é«˜å±‚ä»»åŠ¡ç†è§£çš„å®Œæ•´é“¾æ¡ï¼Œå¹¶æ·±å…¥è€ƒè™‘äº†è®¡ç®—è´Ÿè½½åˆ†é…ã€æ¨¡å—é—´å¼‚æ­¥é€šä¿¡ã€å®æ—¶æ€§èƒ½ç­‰å·¥ç¨‹ç°å®é—®é¢˜ã€‚\n\n3.  **ä»â€œå·²çŸ¥ç»“æ„åŒ–ç¯å¢ƒâ€åˆ°â€œå¼€æ”¾ä¸–ç•Œâ€**\n    *   åŸºå‡†æµ‹è¯•ç¯å¢ƒä»å¹²å‡€çš„ä»¿çœŸæˆ–å®éªŒå®¤åœºæ™¯ï¼Œè½¬å‘**å¤šæ ·ã€æ‚ä¹±ã€éç»“æ„åŒ–çš„çœŸå®é‡å¤–ç¯å¢ƒ**",
            "timestamp": "2026-01-21T12:11:45.570229",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## FlyCo ä¸å‰ä½œçš„å¯¹æ¯”åˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼ŒFlyCo æ˜¯ä½œè€…å›¢é˜Ÿåœ¨â€œæ— äººæœºè‡ªä¸»3Dç»“æ„æ‰«æâ€é¢†åŸŸçš„æœ€æ–°å·¥ä½œã€‚å…¶æ ¸å¿ƒå‰ä½œæ˜¯ **PredRecon** å’Œ **FC-Planner**ã€‚æ­¤å¤–ï¼Œè®ºæ–‡åœ¨â€œæ¢ç´¢å¼æ‰«æâ€æ–¹å‘ä¸Šå¼•ç”¨äº† **Star-Searcher**ï¼Œä½†è¯¥å·¥ä½œå¹¶éåŒä¸€å›¢é˜Ÿçš„æ ¸å¿ƒå‰ä½œï¼Œè€Œæ˜¯ä½œä¸ºå¯¹æ¯”çš„åŸºçº¿æ–¹æ³•ã€‚\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†åˆ†æäº† FlyCo ä¸ä¸¤ç¯‡æ ¸å¿ƒå‰ä½œï¼ˆPredReconï¼Œ FC-Plannerï¼‰çš„åŒºåˆ«ä¸ä¸»è¦æ”¹è¿›ï¼š\n\n| ç‰¹æ€§ç»´åº¦ | **å‰ä½œ1: PredRecon (ICRA 2023)** | **å‰ä½œ2: FC-Planner (ICRA 2024)** | **æœ¬æ–‡å·¥ä½œ: FlyCo** | **ä¸»è¦æ”¹è¿›ä¸åŒºåˆ«** |\n| :--- | :--- | :--- | :--- | :--- |\n| **æ ¸å¿ƒé—®é¢˜** | æå‡è‡ªä¸»ç©ºä¸­é‡å»ºçš„**é€Ÿåº¦ä¸è´¨é‡**ã€‚ | ä¸ºå¤æ‚3Dåœºæ™¯æä¾›**å¿«é€Ÿã€å®Œæ•´çš„è¦†ç›–è·¯å¾„è§„åˆ’**ã€‚ | **å¼€æ”¾ä¸–ç•Œ**ä¸­ï¼ŒåŸºäº**ç®€å•äººç±»æç¤º**ï¼Œå¯¹**æŒ‡å®šç›®æ ‡ç»“æ„**è¿›è¡Œå…¨è‡ªä¸»3Dæ‰«æã€‚ | **é—®é¢˜å®šä¹‰æ ¹æœ¬æ€§æ‰©å±•**ï¼šä»â€œé‡å»ºæ‰€æœ‰å¯è§åœºæ™¯â€æˆ–â€œè§„åˆ’å·²çŸ¥æ¨¡å‹çš„è·¯å¾„â€ï¼Œè½¬å˜ä¸ºåœ¨æœªçŸ¥ã€æ‚ä¹±ç¯å¢ƒä¸­ï¼Œ**ç†è§£å¹¶æ‰«æç”¨æˆ·è¯­ä¹‰æŒ‡å®šçš„å•ä¸ªç›®æ ‡**ã€‚ |\n| **ç³»ç»Ÿè¾“å…¥** | éœ€è¦ç”¨æˆ·é¢„å…ˆæŒ‡å®šä¸€ä¸ª**3Dè¾¹ç•Œæ¡†**ä½œä¸ºæ‰«æåŒºåŸŸã€‚ | éœ€è¦**å®Œæ•´çš„ã€é¢„å…ˆé‡å»ºå¥½çš„ç›®æ ‡3Dæ¨¡å‹**ä½œä¸ºè¾“å…¥ã€‚ | ä»…éœ€**ä½æˆæœ¬çš„æŠ½è±¡æç¤º**ï¼šæ–‡æœ¬æè¿°ï¼ˆå¦‚â€œæ‰«æå±±è°·ä¸­çš„åŸå ¡â€ï¼‰å’Œ/æˆ–ç¨€ç–çš„2Dè§†è§‰æ ‡æ³¨ï¼ˆå¦‚ç‚¹å‡»ï¼‰ã€‚ | **æå¤§é™ä½äº†äººç±»å…ˆéªŒéœ€æ±‚**ï¼šæ‘†è„±äº†å¯¹ç²¾ç¡®å‡ ä½•å…ˆéªŒï¼ˆ3Dæ¡†ã€å®Œæ•´æ¨¡å‹ï¼‰çš„ä¾èµ–ï¼Œä½¿ç³»ç»Ÿæ›´å®ç”¨ã€æ›´æ˜“ç”¨ã€‚ |\n| **æ ¸å¿ƒæŠ€æœ¯æ¨¡å—** | 1. **é¢„æµ‹æ¨¡å—**ï¼šç”¨ç¥ç»ç½‘ç»œä»éƒ¨åˆ†ä½“ç´ åœ°å›¾è¡¥å…¨ç›®æ ‡å‡ ä½•ã€‚<br>2. **è§„åˆ’æ¨¡å—**ï¼šåœ¨é¢„æµ‹çš„å‡ ä½•ä¸Šè¿›è¡Œè¦†ç›–è§„åˆ’ã€‚ | **è§„åˆ’æ¨¡å—**ï¼šä¸€ç§åŸºäºéª¨æ¶å¼•å¯¼çš„å±‚æ¬¡åŒ–è§„åˆ’æ¡†æ¶ï¼Œç”¨äºå¿«é€Ÿç”Ÿæˆå¤æ‚3Dæ¨¡å‹çš„è¦†ç›–è·¯å¾„ã€‚ | **ä¸‰ä½ä¸€ä½“çš„æ„ŸçŸ¥-é¢„æµ‹-è§„åˆ’é—­ç¯**ï¼š<br>1. **æ„ŸçŸ¥**ï¼šåŸºäºåŸºç¡€æ¨¡å‹ï¼Œå°†æç¤ºä¸æµå¼ä¼ æ„Ÿå™¨æ•°æ®å¯¹é½ï¼Œå®ç°é²æ£’çš„3Dç›®æ ‡å®šä½ä¸è·Ÿè¸ªã€‚<br>2. **é¢„æµ‹**ï¼šæ–°é¢–çš„å¤šæ¨¡æ€è¡¨é¢é¢„æµ‹å™¨ï¼Œèåˆå‡ ä½•ã€è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œå®Œæˆç›®æ ‡å½¢çŠ¶ã€‚<br>3. **è§„åˆ’**ï¼šé¢„æµ‹æ„ŸçŸ¥çš„å±‚æ¬¡åŒ–è§„åˆ’å™¨ï¼Œå¹³è¡¡é•¿æ—¶ç¨‹æ•ˆç‡ä¸å®æ—¶å®‰å…¨æ€§ã€‚ | **ç³»ç»Ÿæ€§æ¶æ„åˆ›æ–°**ï¼š<br>1. **å¼•å…¥åŸºç¡€æ¨¡å‹**ï¼šé¦–æ¬¡å°†VLMsã€åˆ†å‰²æ¨¡å‹ç­‰æ·±åº¦é›†æˆåˆ°æ— äººæœºæ‰«æç³»ç»Ÿä¸­ï¼Œè§£å†³äº†â€œåœºæ™¯ç†è§£â€å’Œâ€œä¸–ç•ŒçŸ¥è¯†â€çš„æ ¸å¿ƒç“¶é¢ˆã€‚<br>2. **å½¢æˆå®Œæ•´é—­ç¯**ï¼šä¸‰ä¸ªæ¨¡å—å¼‚æ­¥ååŒï¼Œå°†æŠ½è±¡æç¤ºæŒç»­è½¬åŒ–ä¸ºå…·ä½“è¡ŒåŠ¨ï¼Œå®ç°äº†ä»â€œæŒ‡å®šâ€åˆ°â€œå®Œæˆâ€çš„å…¨æµç¨‹è‡ªä¸»ã€‚ |\n| **æ„ŸçŸ¥èƒ½åŠ›** | ä¾èµ–ä¼ ç»Ÿçš„å‡ ä½•åˆ†å‰²ï¼ˆå¦‚åŸºäºè·ç¦»çš„èšç±»ï¼‰ï¼Œåœ¨å¤æ‚ã€ç›¸ä¼¼çº¹ç†åœºæ™¯ä¸­å®¹æ˜“å¤±è´¥ã€‚ | æ— æ„ŸçŸ¥æ¨¡å—ï¼Œå‡è®¾ç›®æ ‡æ¨¡å‹å·²å®Œç¾ç»™å®šã€‚ | **æç¤ºæ¥åœ°çš„åœºæ™¯æ„ŸçŸ¥**ï¼š<br>- åˆ©ç”¨SAM2è¿›è¡Œ2Dåˆ†å‰²ï¼Œå¹¶é›†æˆBEiT-3ç­‰VLMå¤„ç†æ–‡æœ¬æç¤ºã€‚<br>- æå‡º**è·¨æ¨¡æ€ç²¾ç‚¼æœºåˆ¶**ï¼ˆæ»‘åŠ¨çª—å£è®°å¿†ã€æ—¶åºæ»¤æ³¢å™¨ã€å‡ ä½•ä¸€è‡´æ€§æ£€æŸ¥ï¼‰ï¼Œç¡®ä¿åœ¨é•¿æ—¶ç¨‹ã€å¤šè§†è§’é£è¡Œä¸­ç›®æ ‡çš„2D/3Dåˆ†å‰²**ç¨³å®šã€é²æ£’**ã€‚ | **ä»å‡ ä½•æ„ŸçŸ¥åˆ°è¯­ä¹‰æ„ŸçŸ¥çš„é£è·ƒ**ï¼šç³»ç»Ÿèƒ½ç†è§£â€œåŸå ¡â€ã€â€œæ‹±æ¡¥â€ç­‰è¯­ä¹‰æ¦‚å¿µï¼Œå¹¶åœ¨3Dç‚¹äº‘ä¸­ç²¾ç¡®åˆ†ç¦»ç›®æ ‡ä¸èƒŒæ™¯æ‚ç‰©ï¼Œè§£å†³äº†å‰ä½œåœ¨å¤æ‚å¼€æ”¾ä¸–ç•Œä¸­ç›®æ ‡ grounding ä¸å¯é çš„é—®é¢˜ã€‚ |\n| **é¢„æµ‹èƒ½åŠ›** | é¢„æµ‹æ¨¡å—åŸºäº**çº¯å‡ ä½•ï¼ˆä½“ç´ ï¼‰è¾“å…¥**ï¼Œåœ¨è®­ç»ƒæ•°æ®åˆ†å¸ƒå¤–çš„ç»“æ„ä¸Šæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ | æ— é¢„æµ‹æ¨¡å—ã€‚ | **å¤šæ¨¡æ€è¡¨é¢é¢„æµ‹**ï¼š<br>- **è¾“å…¥**ï¼šéƒ¨åˆ†ç‚¹äº‘ + å…³é”®è§†è§’RGBå›¾åƒ + ç›¸æœºå‚æ•° + æ–‡æœ¬æè¿°ã€‚<br>- **ç½‘ç»œ**ï¼šé‡‡ç”¨â€œåèåˆâ€æ¶æ„å’Œ**äº¤æ›¿æ³¨æ„åŠ›æœºåˆ¶**ï¼Œæœ‰æ•ˆè’¸é¦åŸºç¡€æ¨¡å‹ä¸­çš„è§†è§‰/è¯­è¨€çŸ¥è¯†åˆ°å‡ ä½•è¡¥å…¨ä¸­ã€‚<br>- **è®­ç»ƒ**ï¼šåˆ©ç”¨åŸºç¡€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆå¤§è§„æ¨¡å¤šæ¨¡æ€å¯¹é½æ•°æ®ã€‚<br>- **æ­£åˆ™åŒ–**ï¼šéƒ¨åˆ†è¡¨é¢ä¸€è‡´æ€§æŸå¤±ï¼Œä¿è¯é¢„æµ‹ä¸å·²è§‚æµ‹éƒ¨åˆ†ä¸¥æ ¼å¯¹é½ã€‚ | **é¢„æµ‹è´¨é‡ä¸æ³›åŒ–æ€§å¤§å¹…æå‡**ï¼š<br>1. **å¤šæ¨¡æ€èåˆ**ï¼šåˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬æä¾›çš„ä¸°å¯Œå…ˆéªŒï¼Œè¡¥å…¨ç»“æœæ›´**å…¨å±€åˆç†**ï¼ˆç¬¦åˆè¯­ä¹‰ï¼‰ï¼Œä¸”**å±€éƒ¨å¿ å®**ï¼ˆè´´åˆè§‚æµ‹ï¼‰ã€‚<br>2. **é›¶æ ·æœ¬æ³›åŒ–**ï¼šåœ¨æœªè§è¿‡çš„å¤æ‚ç»“æ„ä¸Šè¡¨ç°æ˜¾è‘—ä¼˜äºçº¯å‡ ä½•è¡¥å…¨æ–¹æ³•ï¼ˆå¦‚CRA-PCNï¼‰å’Œç”Ÿæˆå¼æ¨¡å‹ï¼ˆå¦‚TripoSRï¼‰ã€‚ |\n| **è§„åˆ’èƒ½åŠ›** | åœ¨é¢„æµ‹çš„å‡ ä½•ä¸Šè¿›è¡Œè¦†ç›–è§„åˆ’ï¼Œä½†**æœªè€ƒè™‘æ—¶åºä¸€è‡´æ€§**ï¼Œä¸”è§„åˆ’ä¸æ‰§è¡Œè€¦åˆè¾ƒç´§ã€‚ | ä¸“æ³¨äºä¸º**é™æ€ã€å·²çŸ¥**çš„å¤æ‚æ¨¡å‹ç”Ÿæˆé«˜æ•ˆè¦†ç›–è·¯å¾„ï¼Œé‡‡ç”¨äº†éª¨æ¶åˆ†è§£ç­‰åŠ é€Ÿç­–ç•¥ã€‚ | **é¢„æµ‹æ„ŸçŸ¥çš„å±‚æ¬¡åŒ–è§„åˆ’**ï¼š<br>- **å…¨å±€å±‚**ï¼šåœ¨**åŠ¨æ€æ›´æ–°**çš„é¢„æµ‹ç½‘æ ¼ä¸Šè¿›è¡Œè¦†ç›–è§„åˆ’ï¼Œå¹¶æ³¨å…¥**ä¸€è‡´æ€§æ„ŸçŸ¥**ï¼ˆé‡ç”¨è§†ç‚¹ã€åŒ¹é…å†å²åºåˆ—ï¼‰ï¼Œé¿å…è·¯å¾„æŠ–åŠ¨ã€‚<br>- **å±€éƒ¨å±‚**ï¼š**é«˜é¢‘ç‡**æ‰§è¡Œ**è§†ç‚¹çº¦æŸçš„è½¨è¿¹ä¼˜åŒ–**ï¼Œåœ¨è·Ÿè¸ªå…¨å±€è·¯å¾„çš„åŒæ—¶ï¼Œå®æ—¶é¿éšœå¹¶ä¿è¯è¦†ç›–å®Œæ•´æ€§ã€‚<br>- **å¼‚æ­¥æ¡†æ¶**ï¼šè§£è€¦æ…¢é€Ÿçš„å…¨å±€è§„åˆ’å’Œå¿«é€Ÿçš„å±€éƒ¨é‡è§„åˆ’ã€‚ | **è§„åˆ’ä¸åŠ¨æ€åœºæ™¯ã€é¢„æµ‹æ·±åº¦é›†æˆ**ï¼š<br>1. **åˆ©ç”¨é¢„æµ‹**ï¼šå°†å¼€æ”¾ä¸–ç•Œé—®é¢˜è½¬åŒ–ä¸ºå¯¹â€œé¢„æµ‹æ¨¡å‹â€çš„è¦†ç›–é—®é¢˜ï¼Œè·å¾—å…¨å±€è§†é‡ï¼Œå…‹æœäº†æ¢ç´¢å¼æ–¹æ³•çš„çŸ­è§†ç¼ºé™·ã€‚<br>2. **ä¿è¯ä¸€è‡´æ€§**ï¼šè§„åˆ’è·¯å¾„éšé¢„æµ‹å¹³æ»‘æ¼”åŒ–ï¼Œé¿å…äº†å› é¢„æµ‹æ›´æ–°å¯¼è‡´çš„è·¯å¾„å‰§çƒˆå˜åŒ–å’Œæ•ˆç‡æŸå¤±ã€‚<br>3. **å…¼é¡¾å®‰å…¨ä¸æ•ˆç‡**ï¼šå¼‚æ­¥æ¶æ„ç¡®ä¿äº†åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸­ä»èƒ½ä¿æŒå®æ—¶é¿éšœèƒ½åŠ›ã€‚ |\n| **å®éªŒéªŒè¯** | ä¸»è¦åœ¨**ä»¿çœŸçš„ç®€åŒ–ç¯å¢ƒ**ï¼ˆæ— ç›®æ ‡æ— å…³éšœç¢ç‰©ï¼‰ä¸­è¿›è¡Œè¯„ä¼°ã€‚ | åœ¨ä»¿çœŸå’Œå°‘é‡ç®€å•ç°å®åœºæ™¯ä¸­ï¼Œé’ˆå¯¹**ç»™å®šæ¨¡å‹**è¿›è¡Œè·¯å¾„è§„åˆ’æµ‹è¯•ã€‚ | **å¤§è§„æ¨¡ç»¼åˆéªŒè¯**ï¼š<br>1. **çœŸå®ä¸–ç•Œ**ï¼šå››ä¸ªæå…·æŒ‘æˆ˜æ€§çš„é‡å¤–åœºæ™¯ï¼ˆæ‹±æ¡¥ã€éŸ³ä¹å…ã€åŸå ¡é—¨æ¥¼ã€çº¢ç –å»ºç­‘ï¼‰å…¨è‡ªä¸»é£è¡Œã€‚<br>2. **ä»¿çœŸåŸºå‡†**ï¼šåœ¨å››ä¸ªå¤æ‚ä»¿çœŸåœºæ™¯ä¸­ï¼Œä¸SOTAæ–¹æ³•ï¼ˆPlan3D, Star-Searcherï¼‰è¿›è¡Œå®šé‡å¯¹æ¯”ã€‚<br>3. **å…¨é¢æ¶ˆè**ï¼šå¯¹æ¯ä¸ªæ ¸å¿ƒæ¨¡å—å’Œè®¾è®¡é€‰æ‹©è¿›è¡Œæ¶ˆèå®éªŒã€‚ | **éªŒè¯çš„å¹¿åº¦ä¸æ·±åº¦æ˜¾è‘—åŠ å¼º**ï¼šè¯æ˜äº†ç³»ç»Ÿåœ¨**çœŸå®ã€å¼€æ”¾ã€æ‚ä¹±**ç¯å¢ƒä¸­çš„**å®ç”¨æ€§ã€é«˜æ•ˆæ€§ã€å®‰å…¨æ€§å’Œæ³›åŒ–æ€§**ã€‚è¿™æ˜¯å‰ä½œæœªèƒ½è¾¾åˆ°çš„é‡Œç¨‹ç¢‘ã€‚ |\n| **å®é™…ä»·å€¼ä¸åˆ›æ–°** | æå‡ºäº†â€œé¢„æµ‹è¾…åŠ©è§„åˆ’â€çš„æ¦‚å¿µï¼Œåœ¨å—æ§ç¯å¢ƒä¸‹æå‡äº†é‡å»ºæ•ˆç‡ã€‚ | æä¾›äº†é’ˆå¯¹å¤æ‚å·²çŸ¥ç»“æ„çš„é«˜æ•ˆè§„åˆ’å·¥å…·ã€‚ | **å®šä¹‰äº†ä¸€ä¸ªæ–°çš„å®ç”¨ç³»ç»ŸèŒƒå¼**ï¼šä¸€ä¸ª**å³æ’å³ç”¨**çš„è“å›¾ï¼Œèƒ½å¤Ÿå°†åŸºç¡€æ¨¡å‹çš„é€šç”¨ä¸–ç•ŒçŸ¥è¯†ä¸æœºå™¨äººçš„é«˜çº§é£è¡ŒæŠ€èƒ½æ— ç¼ç»“åˆï¼Œä»…é€šè¿‡è‡ªç„¶äº¤äº’å³å¯å®Œæˆä¸“ä¸šçš„3Dæ‰«æä»»åŠ¡ã€‚ä¸ºåŸºç¡€è®¾æ–½æ£€æµ‹ã€æ–‡åŒ–é—äº§æ•°å­—åŒ–ç­‰åº”ç”¨æä¾›äº†ä½é—¨æ§›ã€é«˜æ•ˆç‡çš„è§£å†³æ–¹æ¡ˆã€‚ | **ä»â€œä¸“ç”¨å·¥å…·â€åˆ°â€œé€šç”¨æ™ºèƒ½ä½“â€çš„æ¼”è¿›**ï¼šFlyCo ä¸å†æ˜¯ä¸€ä¸ªé’ˆå¯¹ç‰¹å®šå­é—®é¢˜ï¼ˆå¦‚è§„åˆ’ã€è¡¥å…¨ï¼‰çš„ç®—æ³•ï¼Œè€Œæ˜¯ä¸€ä¸ªå…·å¤‡**åœºæ™¯ç†è§£ã€ç©ºé—´æƒ³è±¡å’Œè‡ªä¸»å†³ç­–èƒ½åŠ›**çš„å®Œæ•´ç©ºä¸­æ™ºèƒ½ä½“ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº**ç³»ç»Ÿæ¶æ„**çš„æˆåŠŸè®¾è®¡ã€‚ |\n\n### æ€»ç»“ï¼šæ ¸å¿ƒåŒºåˆ«ä¸ä¸»è¦æ”¹å–„\n\n1.  **é—®é¢˜å±‚é¢çš„æ ¹æœ¬æ€§çªç ´**ï¼šå‰ä½œï¼ˆPredReconï¼Œ FC-Plannerï¼‰è§£å†³çš„æ˜¯â€œå¦‚ä½•æ‰«å¾—æ›´å¿«/æ›´å¥½â€çš„é—®é¢˜ï¼Œä½†å‰ææ˜¯â€œæ‰«å“ªé‡Œâ€å·²ç»é€šè¿‡3Dæ¡†æˆ–å®Œæ•´æ¨¡å‹äººä¸ºç²¾ç¡®å®šä¹‰äº†ã€‚**FlyCo è§£å†³äº†â€œæ‰«ä»€ä¹ˆâ€å’Œâ€œæ€ä¹ˆæ‰«â€çš„ç»Ÿä¸€é—®é¢˜**ï¼Œå°†äººç±»æŒ‡ä»¤ä»ä½çº§çš„å‡ ä½•å‚æ•°æå‡ä¸ºé«˜çº§çš„è¯­ä¹‰æç¤ºã€‚\n\n2.  **æŠ€æœ¯èŒƒå¼çš„èåˆä¸å‡çº§**ï¼š\n    *   **ä»ç®—æ³•åˆ°æ™ºèƒ½**ï¼šå°†å‰æ²¿çš„**åŸºç¡€æ¨¡å‹**ä½œä¸ºç³»ç»Ÿçš„â€œå¤§è„‘â€ï¼Œèµ‹äºˆäº†æ— äººæœºç±»ä¼¼äººç±»é£è¡Œå‘˜çš„åœºæ™¯ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚\n    *   **ä»å¼€ç¯åˆ°é—­ç¯**ï¼šæ„å»ºäº†**æ„ŸçŸ¥-é¢„æµ‹-è§„åˆ’**çš„ç´§å¯†é—­ç¯ï¼Œä½¿å¾—ç³»ç»Ÿèƒ½åœ¨çº¿é€‚åº”ç¯å¢ƒå˜åŒ–ã€çº æ­£é¢„æµ‹è¯¯å·®ã€ä¼˜åŒ–é£è¡Œè·¯å¾„ã€‚\n    *   **ä»å•æ¨¡æ€åˆ°å¤šæ¨¡æ€**ï¼šé¢„æµ‹å™¨åˆ›æ–°æ€§åœ°èåˆäº†**å‡ ä½•ã€è§†è§‰ã€è¯­è¨€**ä¸‰ç§æ¨¡æ€ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹çš„çŸ¥è¯†å¼¥è¡¥äº†çº¯å‡ ä½•æ–¹æ³•çš„æ³›åŒ–ä¸è¶³ã€‚\n\n3.  **å®ç”¨æ€§çš„å·¨å¤§æå‡**ï¼š\n    *   **äººåŠ›æˆæœ¬æä½**ï¼šç”¨æˆ·äº¤äº’å‹å¥½ï¼Œæ— éœ€ä¸“ä¸šçŸ¥è¯†å’Œç¹çå‡†å¤‡ã€‚\n    *   **ç¯å¢ƒé€‚åº”æ€§å¼º**ï¼šç›´æ¥åœ¨æœªçŸ¥ã€æ‚ä¹±çš„çœŸå®å¼€æ”¾ä¸–ç•Œä¸­éªŒè¯æˆåŠŸã€‚\n    *   **ç³»ç»Ÿé²æ£’æ€§é«˜**ï¼šé€šè¿‡è·¨æ¨¡æ€ç²¾ç‚¼ã€ä¸€è‡´æ€§è§„åˆ’ç­‰è®¾è®¡ï¼Œä¿è¯äº†é•¿æ—¶ç¨‹ä»»åŠ¡çš„ç¨³å®šæ‰§è¡Œã€‚\n\n**ç®€è€Œè¨€ä¹‹ï¼ŒFlyCo ç›¸å¯¹äºå‰ä½œï¼Œå®ç°äº†ä¸€æ¬¡ä»â€œä¸“æ³¨äºæ‰«æè¿‡ç¨‹çš„å±€éƒ¨ä¼˜åŒ–â€åˆ°â€œ",
            "timestamp": "2026-01-21T12:12:55.393108",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## FlyCoå…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\nFlyCoæ˜¯ä¸€ä¸ªç”±åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰èµ‹èƒ½çš„æ— äººæœºç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å¯¹ç”¨æˆ·æŒ‡å®š3Dç»“æ„è¿›è¡Œè‡ªä¸»æ‰«æçš„éš¾é¢˜ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºè®¾è®¡äº†ä¸€ä¸ª**æ„ŸçŸ¥-é¢„æµ‹-è§„åˆ’**çš„é—­ç¯æ¶æ„ï¼Œå°†åŸºç¡€æ¨¡å‹çš„é€šç”¨çŸ¥è¯†ä¸å…ˆè¿›çš„é£è¡ŒæŠ€èƒ½æ— ç¼é›†æˆã€‚ä»¥ä¸‹æ˜¯å…¶å…³é”®æŠ€æœ¯æ–¹æ³•çš„è¯¦ç»†è¯´æ˜ã€‚\n\n### å…³é”®æŠ€æœ¯æ–¹æ³•æ€»è§ˆè¡¨\n\n| æ¨¡å— | å…³é”®æŠ€æœ¯ | æ ¸å¿ƒç›®æ ‡ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…³é”®å‚æ•°/é…ç½® |\n| :--- | :--- | :--- | :--- | :--- |\n| **1. æ„ŸçŸ¥** | **åŸºäºæç¤ºçš„è·¨æ¨¡æ€åœºæ™¯æ„ŸçŸ¥** | å°†æŠ½è±¡çš„ç”¨æˆ·æç¤ºï¼ˆæ–‡æœ¬ã€è§†è§‰æ ‡æ³¨ï¼‰åœ¨æµå¼ä¼ æ„Ÿå™¨æ•°æ®ä¸­æŒç»­ã€é²æ£’åœ°å®šä½å¹¶åˆ†å‰²å‡ºç›®æ ‡ç»“æ„ã€‚ | **1.1 æµå¼é«˜æ•ˆSAM2é€‚é…:** <br> - **é—®é¢˜:** åŸå§‹SAM2ä¸ºç¦»çº¿è§†é¢‘è®¾è®¡ï¼Œå†…å­˜å’Œè®¡ç®—éšåºåˆ—å¢é•¿ã€‚ <br> - **æ–¹æ¡ˆ:** é‡‡ç”¨**æ»‘åŠ¨çª—å£è®°å¿†åº“**ï¼Œä»…ä¿ç•™æœ€è¿‘ `W_m` å¸§çš„åˆ†å‰²ç»“æœï¼Œé™åˆ¶å†…å­˜å ç”¨ã€‚å¯¹SAM2ä¸»å¹²è¿›è¡Œè½»é‡çº§é‡åŒ–å’Œå›¾çº§ç¼–è¯‘ä¼˜åŒ–ã€‚ <br><br> **1.2 è·¨æ¨¡æ€ç²¾ç‚¼æœºåˆ¶:** <br> - **å‡ ä½•èšç±»:** å¯¹åˆå§‹LiDARç‚¹äº‘ä½¿ç”¨åŸºäºå¯†åº¦çš„èšç±»ç®—æ³•ï¼ˆå¦‚DBSCANï¼‰ï¼Œå°†æŠ•å½±åˆ°é¦–å¸§2Dæ©ç å†…çš„ç°‡æ ‡è®°ä¸ºç›®æ ‡ã€‚åç»­å¸§ä¸­ï¼Œæ–°ç‚¹æ ¹æ®è·ç¦»é˜ˆå€¼åˆ†é…ç»™ç°æœ‰ç°‡æˆ–ç”Ÿæˆæ–°ç°‡ã€‚ <br> - **å…³é”®å¸§é€‰æ‹©:** ç»´æŠ¤2Dæ©ç è¾¹ç•Œæ¡†çš„**å¡å°”æ›¼æ»¤æ³¢å™¨**è¿›è¡Œè¿åŠ¨é¢„æµ‹ã€‚å…³é”®å¸§å¾—åˆ† `s_i = s_kf,i + s_geo,i`ï¼Œå…¶ä¸­ `s_kf,i` ä¸ºé¢„æµ‹æ¡†ä¸å½“å‰æ©ç çš„BoxIoUï¼Œ`s_geo,i` ä¸º3Dç›®æ ‡ç°‡æŠ•å½±æ©ç ä¸2Dæ©ç çš„PixelIoUã€‚é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ `W_s` å¸§ä½œä¸ºSAM2çš„è®°å¿†æ³¨æ„åŠ›å…³é”®å¸§ã€‚ <br> - **é‡åˆå§‹åŒ–:** è‹¥å½“å‰2Dæ©ç  `M_t` ä¸3DæŠ•å½±æ©ç  `M_t^proj` çš„BoxIoUä½äºé˜ˆå€¼ `Îº_reinit`ï¼Œåˆ™ä»å½“å‰3Dç›®æ ‡ç°‡ä¸­é‡‡æ ·ç‚¹ä½œä¸ºè™šæ‹Ÿæç¤ºç‚¹ï¼Œé‡æ–°æŸ¥è¯¢SAM2ä»¥çº æ­£æ¼‚ç§»ã€‚ | - æ»‘åŠ¨çª—å£å¤§å°: `W_m = 30` <br> - å…³é”®å¸§æ•°é‡: `W_s = 7` <br> - é‡åˆå§‹åŒ–é˜ˆå€¼: `Îº_reinit = 0.7` <br> - ä½“ç´ åœ°å›¾åˆ†è¾¨ç‡: `0.1 m` |\n| **2. é¢„æµ‹** | **å¤šæ¨¡æ€è¡¨é¢é¢„æµ‹ç½‘ç»œ** | èåˆéƒ¨åˆ†å‡ ä½•ã€è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œæ¨æ–­ç›®æ ‡ç»“æ„çš„å®Œæ•´3Dè¡¨é¢ï¼Œä¸ºè§„åˆ’æä¾›å…¨å±€ç»“æ„å…ˆéªŒã€‚ | **2.1 ç½‘ç»œæ¶æ„ (åèåˆ):** <br> - **ç¼–ç :** <br>   - **æ–‡æœ¬:** ä½¿ç”¨SigLIPæ–‡æœ¬ç¼–ç å™¨ï¼Œå¾—åˆ°ç‰¹å¾ `F_text âˆˆ R^(1Ã—D_t)`ã€‚ <br>   - **å›¾åƒ:** ä½¿ç”¨DINOv2æå–å›¾åƒå—ç‰¹å¾ `F_I âˆˆ R^(KÃ—D_I)`ã€‚é€šè¿‡å°MLPæ ¹æ®ç›¸æœºå‚æ•° `g_tar` é¢„æµ‹æ¯é€šé“çš„ç¼©æ”¾ `Î³` å’Œå¹³ç§» `Î²`ï¼Œå¯¹è§†è§‰ç‰¹å¾è¿›è¡Œè°ƒåˆ¶ï¼š`F_I â† (1+Î³) âŠ™ F_I + Î²`ã€‚ <br>   - **ç‚¹äº‘:** ä½¿ç”¨è½»é‡çº§PointNet++ç¼–ç éƒ¨åˆ†ç‚¹äº‘ `P_tar`ï¼Œå¾—åˆ°ç‰¹å¾ `F_P âˆˆ R^(N_iÃ—D_P)`ã€‚ <br> - **èåˆ (äº¤æ›¿æ³¨æ„åŠ›):** å°†è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾æ‹¼æ¥ä¸ºè¯­ä¹‰ç‰¹å¾ `F_S`ã€‚å †å  `L` å±‚äº¤æ›¿æ³¨æ„åŠ›æ¨¡å—ï¼Œæ¯å±‚åŒ…æ‹¬ï¼š <br>   1. **å‡ ä½•è‡ªæ³¨æ„åŠ›:** `F_P^ = SelfAttention(F_P)` <br>   2. **è¯­ä¹‰è‡ªæ³¨æ„åŠ›:** `F_S^ = SelfAttention(F_S)` <br>   3. **å…¨å±€äº¤å‰æ³¨æ„åŠ›:** `F_P~ = CrossAttention(F_P^, F_S^)` (ä»¥å‡ ä½•ç‰¹å¾ä¸ºQueryï¼Œè¯­ä¹‰ç‰¹å¾ä¸ºKey/Value)ã€‚ <br> - **è§£ç :** MLPä»èåˆç‰¹å¾ `F_P~` é¢„æµ‹å€™é€‰ç‚¹ï¼Œä¸è¾“å…¥ç‚¹ `P_tar` æ‹¼æ¥åé€šè¿‡æœ€è¿œç‚¹é‡‡æ ·(FPS)å¾—åˆ°å›ºå®šå¤§å°çš„å®Œæ•´ç‚¹äº‘ `P_comp`ã€‚ <br><br> **2.2 è®­ç»ƒç›®æ ‡ (åŒç›®æ ‡ç›‘ç£):** <br> - **å®Œæ•´æ€§æŸå¤±:** `L_comp = CD_u(P_comp, P_gt)`ï¼Œé¼“åŠ±é¢„æµ‹æ¥è¿‘çœŸå®å®Œæ•´è¡¨é¢ã€‚ <br> - **éƒ¨åˆ†è¡¨é¢æ­£åˆ™åŒ–æŸå¤±:** `L_par = CD_d(P_comp, P_tar)`ï¼Œå¼ºåˆ¶é¢„æµ‹ä¸è¾“å…¥çš„éƒ¨åˆ†è§‚æµ‹ä¿æŒä¸€è‡´ã€‚ <br> - **æ€»æŸå¤±:** `L = L_comp + L_par`ã€‚ <br><br> **2.3 è‡ªåŠ¨å¤šæ¨¡æ€æ•°æ®ç”Ÿæˆ:** <br> - ä½¿ç”¨Blenderä»3Dèµ„äº§æ¸²æŸ“å¤šè§†è§’RGB-æ·±åº¦å›¾ï¼Œåå‘æŠ•å½±ç”Ÿæˆç‚¹äº‘ã€‚éšæœºç»„åˆ1-8ä¸ªè§†è§’çš„ç‚¹äº‘ä½œä¸ºéƒ¨åˆ†è¾“å…¥ `P_tar`ã€‚ <br> - ä½¿ç”¨å›¾åƒæè¿°æ¨¡å‹ä¸ºæ¯ä¸ªè§†å›¾ç”Ÿæˆæè¿°ï¼Œå†ç”¨GPT-4èåˆæˆå…¨å±€æ–‡æœ¬æè¿° `h_text`ã€‚ <br><br> **2.4 æ¨ç†æ—¶å‡ ä½•è‡´å¯†åŒ–:** <br> - è®­ç»ƒä¸€ç³»åˆ—ç‹¬ç«‹çš„ä¸Šé‡‡æ ·ç½‘ç»œï¼Œå°† `P_comp` ä¸Šé‡‡æ ·è‡³ `2N_o, 3N_o, ..., kN_o` ä¸ªç‚¹ã€‚ <br> - æ¨ç†æ—¶ï¼ŒæŒ‰å¯†åº¦å‡åºåº”ç”¨ä¸Šé‡‡æ ·å™¨ï¼Œç›´åˆ°ç‚¹é—´æœ€å°è·ç¦»ä½äºåˆ†è¾¨ç‡é˜ˆå€¼ `Ï_min`ï¼Œå¾—åˆ°æœ€ç»ˆè‡´å¯†ç‚¹äº‘ `P_dense*`ã€‚éšåä½¿ç”¨NKSRç®—æ³•ç”Ÿæˆæ°´å¯†ç½‘æ ¼ `S_mesh`ã€‚ | - è¾“å…¥/è¾“å‡ºç‚¹æ•°: `N_i = N_o = 2048` <br> - ç‰¹å¾ç»´åº¦: `D_t=768, D_I=768, D_P=512, D_f=512` <br> - æ³¨æ„åŠ›å±‚æ•°: `L = 10` <br> - è®­ç»ƒæ•°æ®: ~70,000ä¸ª3Dæ¨¡å‹ <br> - æ‰¹æ¬¡å¤§å°: 336 (åˆ†å¸ƒå¼) <br> - å­¦ä¹ ç‡: 1e-4 (ä½™å¼¦é€€ç«) <br> - è‡´å¯†åŒ–é˜ˆå€¼: `Ï_min = 0.5 m` |\n| **3. è§„åˆ’** | **é¢„æµ‹æ„ŸçŸ¥çš„åˆ†å±‚è§„åˆ’å™¨** | åŸºäºé¢„æµ‹çš„å‡ ä½•ï¼Œç”Ÿæˆé«˜æ•ˆã€å®‰å…¨ã€æ—¶é—´ä¸€è‡´çš„è¦†ç›–è·¯å¾„ï¼Œå¹¶å®æ—¶é¿éšœã€‚ | **3.1 ä¸€è‡´æ€§æ„ŸçŸ¥çš„å…¨å±€è¦†ç›–è§„åˆ’:** <br> - **å¿«é€Ÿå®‰å…¨æ„å‹ç©ºé—´ç”Ÿæˆ:** <br>   1. **å†…ç‚¹è®¡ç®—:** å¯¹é¢„æµ‹ç½‘æ ¼ `S_mesh` ä¸‹é‡‡æ ·é¡¶ç‚¹ï¼Œä¸ºæ¯ä¸ªä»£è¡¨ç‚¹é€šè¿‡è¿­ä»£ä¼˜åŒ–æ‰¾åˆ°æœ€ä¼˜åˆ‡å‰²å¹³é¢æ³•å‘ `n_opt`ï¼Œæœ€å°åŒ–ä¸é‚»åŸŸé¡¶ç‚¹æ³•å‘çš„æ–¹å·®ï¼š`n_opt^(i+1) = argmin n_opt^T Cov^i(N_v) n_opt`ã€‚è®¡ç®—å†…ç‚¹ `p_inlier` ä½œä¸ºé‚»åŸŸæ³•å‘å°„çº¿æœ€è¿‘äº¤æ±‡ç‚¹ã€‚ <br>   2. **å®‰å…¨æ€§æ£€æŸ¥:** å¯¹è§†ç‚¹å€™é€‰ï¼Œå‘å…¶æœ€è¿‘å†…ç‚¹å‘å°„å°„çº¿ï¼Œä¸ç½‘æ ¼æ±‚äº¤ã€‚**äº¤ç‚¹æ•°å¥‡å¶æ€§**åˆ¤å®šå†…å¤–ï¼ˆå¥‡å¤–å¶å†…ï¼‰ã€‚ <br> - **è§†ç‚¹ç”Ÿæˆä¸å‰ªæ:** æ²¿æœªè§‚å¯Ÿé¡¶ç‚¹çš„æ³•å‘é‡‡æ ·å€™é€‰è§†ç‚¹ï¼Œä½¿ç”¨é‡åŠ›æ¨¡å‹åˆå¹¶è¦†ç›–èŒƒå›´å°çš„è§†ç‚¹åˆ°è¦†ç›–èŒƒå›´å¤§çš„è§†ç‚¹ï¼Œä½ç½®æ›´æ–°ï¼š`p_i â† p_i + Î£ (vc_n / vc_i) (p_n - p_i)`ã€‚ <br> - **å¹¶è¡Œè·¯å¾„ç¡®å®š (ä¸¤çº§åˆ†è§£):** <br>   1. ä»å†…ç‚¹æå–3Déª¨æ¶ï¼Œå°†ç½‘æ ¼åˆ†è§£ä¸ºå­ç©ºé—´ã€‚ <br>   2. å°†è§†ç‚¹æŒ‰æ¬§æ°è·ç¦»åˆ†é…ç»™å­ç©ºé—´ï¼Œè‹¥å­ç©ºé—´åŠå¾„è¶… `R_g`ï¼Œåˆ™åŸºäºå¯è§æ€§å›¾è¿›ä¸€æ­¥åˆ†å‰²ä¸ºå‡¸ç»„ã€‚ <br>   3. å¹¶è¡Œæ±‚è§£ä¸¤ä¸ªéå¯¹ç§°æ—…è¡Œå•†é—®é¢˜(ATSP)ï¼šç»„é—´é¡ºåºå’Œç»„å†…è§†ç‚¹é¡ºåºã€‚ <br> - **æ³¨å…¥ä¸€è‡´æ€§æ„ŸçŸ¥:** <br>   1. **è§†ç‚¹å¤ç”¨:** è‹¥åŒºåŸŸå‡ ä½•æœªå˜ï¼Œåˆ™å¤ç”¨ä¸Šä¸€å‘¨æœŸçš„è§†ç‚¹ã€‚ <br>   2. **åºåˆ—åŒ¹é…:** å°†å½“å‰åˆ†ç»„ `g_cur` ä¸ä¸Šä¸€å‘¨æœŸåºåˆ—çš„å‰ `m` ç»„è¿›è¡ŒåŒ¹é…ï¼š`argmin CD_d(g_last, g_cur)`ã€‚ <br><br> **3.2 è§†ç‚¹çº¦æŸçš„è½¨è¿¹ä¼˜åŒ– (å±€éƒ¨è§„åˆ’):** <br> - **å‚æ•°åŒ–:** ä½¿ç”¨MINCO (å¤šé¡¹å¼) å‚æ•°åŒ–è½¨è¿¹ `x_L(t)`ï¼Œé€šè¿‡ä¸­é—´ç‚¹ `Q` å’Œæ—¶é—´åˆ†é… `T` æ˜ å°„ç³»æ•°ï¼š`c_i = M(q_i, T_i)`ã€‚ <br> - **ä¼˜åŒ–é—®é¢˜:** <br>   `min_{Q_w, T} J_s + w_t J_t + w_d J_d + w_c J_c + w_sc Cov(Q_w) + w_tc Cov(T)` <br>   å…¶ä¸­ `J_s` ä¸ºå¹³æ»‘æ€§(é«˜é˜¶å¯¼æ•°å¹³æ–¹ç§¯åˆ†",
            "timestamp": "2026-01-21T12:13:59.790426",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡ã€ŠFlyCoã€‹çš„è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä»**ç³»ç»Ÿæ•´ä½“æ€§èƒ½**å’Œ**æ¨¡å—çº§æ€§èƒ½**ä¸¤ä¸ªå±‚é¢è®¾è®¡äº†å…¨é¢çš„è¯„ä»·æŒ‡æ ‡ï¼Œä»¥é‡åŒ–è¯„ä¼°FlyCoç³»ç»Ÿåœ¨**æ•ˆç‡ã€å®Œæ•´æ€§ã€å®‰å…¨æ€§ã€ç¨³å®šæ€§**å’Œ**é¢„æµ‹å‡†ç¡®æ€§**ç­‰æ–¹é¢çš„è¡¨ç°ã€‚\n\n#### 1. ç³»ç»Ÿæ•´ä½“æ€§èƒ½æŒ‡æ ‡ï¼ˆç”¨äºçœŸå®ä¸–ç•Œå®éªŒä¸ä»¿çœŸåŸºå‡†æµ‹è¯•ï¼‰\n\nè¿™äº›æŒ‡æ ‡ç”¨äºè¡¡é‡æ— äººæœºå®Œæˆæ•´ä¸ª3Dæ‰«æä»»åŠ¡çš„è¡¨ç°ã€‚\n\n| æŒ‡æ ‡ç±»åˆ« | å…·ä½“æŒ‡æ ‡ | è®¡ç®—å…¬å¼/å®šä¹‰ | ç‰©ç†æ„ä¹‰ä¸ç»†èŠ‚ |\n| :--- | :--- | :--- | :--- |\n| **æ•ˆç‡** | **é£è¡Œæ—¶é—´** | ä»èµ·é£åˆ°ç€é™†çš„æ€»ä»»åŠ¡æ—¶é—´ï¼ˆç§’ï¼‰ã€‚ | è¡¡é‡ä»»åŠ¡æ‰§è¡Œæ•ˆç‡ã€‚**è¶Šä½è¶Šå¥½**ã€‚è®ºæ–‡åœ¨åŸºå‡†æµ‹è¯•ä¸­**æ’é™¤äº†äººå·¥è®¾ç½®æ—¶é—´**ï¼ˆå¦‚ç»˜åˆ¶3Dè¾¹ç•Œæ¡†ï¼‰ï¼Œä»¥å…¬å¹³å¯¹æ¯”ï¼Œè¿™å®é™…ä¸Šå¯¹éœ€è¦å¤§é‡äººå·¥å…ˆéªŒçš„åŸºçº¿æ–¹æ³•æœ‰åˆ©ã€‚ |\n| **æ•°æ®å®Œæ•´æ€§** | **ä¿¡æ¯å®Œæ•´åº¦** | `C(ğ±) = (è¢«ç›¸æœºè§‚æµ‹åˆ°çš„ç›®æ ‡è¡¨é¢ç½‘æ ¼ä½“ç´ æ•°) / (ç›®æ ‡åœ°é¢çœŸå®è¡¨é¢æ€»ç½‘æ ¼ä½“ç´ æ•°)` | è¡¡é‡æ‰«æåé‡å»ºæ¨¡å‹çš„å®Œæ•´ç¨‹åº¦ã€‚ä½“ç´ åŒ–åˆ†è¾¨ç‡ä¸º **0.05m Ã— 0.05m**ã€‚**è¶Šé«˜è¶Šå¥½**ï¼Œ100%è¡¨ç¤ºå®Œå…¨è¦†ç›–ã€‚ |\n| **å®‰å…¨æ€§ä¸å¯é æ€§** | **æˆåŠŸç‡** | `(æˆåŠŸå®Œæˆä»»åŠ¡çš„è¯•éªŒæ¬¡æ•°) / (æ€»è¯•éªŒæ¬¡æ•°)` | è¡¡é‡ç³»ç»Ÿçš„é²æ£’æ€§ã€‚ä¸€æ¬¡è¯•éªŒè¢«è§†ä¸ºâ€œå¤±è´¥â€å¦‚æœï¼š1) å‘ç”Ÿç¢°æ’ï¼›2) é£è¡Œæ—¶é—´è¶…è¿‡é¢„è®¾çš„æœ€å¤§é¢„ç®— **`T_max = 1500ç§’`**ã€‚ |\n| **ç¨³å®šæ€§** | **æ€§èƒ½ä¸€è‡´æ€§** | é£è¡Œæ—¶é—´å’Œä¿¡æ¯å®Œæ•´åº¦åœ¨å¤šæ¬¡è¿è¡Œï¼ˆå¦‚20æ¬¡ï¼‰ä¸­çš„**æ ‡å‡†å·®**å’Œ**åˆ†å¸ƒ**ï¼ˆé€šè¿‡å°æç´å›¾å±•ç¤ºï¼‰ã€‚ | è¡¡é‡ç³»ç»Ÿåœ¨ä¸åŒè¿è¡Œä¸­çš„å¯é‡å¤æ€§å’Œé²æ£’æ€§ã€‚**æ ‡å‡†å·®è¶Šå°ã€åˆ†å¸ƒè¶Šé›†ä¸­**ï¼Œè¡¨æ˜ç³»ç»Ÿè¶Šç¨³å®šã€‚ |\n\n#### 2. æ¨¡å—çº§æ€§èƒ½æŒ‡æ ‡ï¼ˆç”¨äºæ¶ˆèå®éªŒï¼‰\n\nè¿™äº›æŒ‡æ ‡ç”¨äºæ·±å…¥è¯„ä¼°æ„ŸçŸ¥ã€é¢„æµ‹ã€è§„åˆ’ç­‰æ ¸å¿ƒå­æ¨¡å—çš„æ€§èƒ½ã€‚\n\n| æ¨¡å— | å…·ä½“æŒ‡æ ‡ | è®¡ç®—å…¬å¼/å®šä¹‰ | ç‰©ç†æ„ä¹‰ä¸ç»†èŠ‚ |\n| :--- | :--- | :--- | :--- |\n| **æ„ŸçŸ¥æ¨¡å—** | **2Dåˆ†å‰²ç¨³å®šæ€§** | å®šæ€§è¯„ä¼°é•¿æ—¶é£è¡Œä¸­2Dæ©ç æ˜¯å¦å‘ç”Ÿæ¼‚ç§»æˆ–å´©æºƒã€‚ | é€šè¿‡å¯¹æ¯”åŸå§‹SAM2ä¸FlyCoæ”¹è¿›ç‰ˆåœ¨çœŸå®é£è¡Œæ•°æ®ä¸Šçš„è¡¨ç°æ¥éªŒè¯ã€‚ |\n| | **3Dåˆ†å‰²å‡†ç¡®æ€§** | å®šæ€§è¯„ä¼°æŠ•å½±åˆ°ç‚¹äº‘åçš„3Dç›®æ ‡ç»“æ„æ˜¯å¦å¹²å‡€ï¼Œæ˜¯å¦æ··å…¥æ— å…³éšœç¢ç‰©ã€‚ | åŒä¸Šï¼Œé€šè¿‡å¯è§†åŒ–å¯¹æ¯”ã€‚ |\n| | **GPUå†…å­˜å ç”¨** | è®°å½•é•¿æ—¶é—´è¿è¡Œä¸­GPUå†…å­˜çš„ä½¿ç”¨é‡å˜åŒ–ã€‚ | è¯„ä¼°ç®—æ³•åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å¯æŒç»­æ€§ã€‚åŸå§‹SAM2å†…å­˜ä¼šæŒç»­å¢é•¿ï¼Œè€ŒFlyCoé€šè¿‡æ»‘åŠ¨çª—å£ä¿æŒç¨³å®šã€‚ |\n| | **æ„ŸçŸ¥å»¶è¿Ÿ** | å¤„ç†å•å¸§æ•°æ®æ‰€éœ€çš„å¹³å‡æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ã€‚ | è¡¡é‡å®æ—¶æ€§ã€‚FlyCoé€šè¿‡ä¼˜åŒ–å°†å»¶è¿Ÿä»~800msé™ä½è‡³~400msã€‚ |\n| **é¢„æµ‹æ¨¡å—** | **L1å€’è§’è·ç¦»** | `L1-CD = (1/|X|) Î£_{xâˆˆX} min_{yâˆˆY} ||x-y|| + (1/|Y|) Î£_{yâˆˆY} min_{xâˆˆX} ||x-y||` | è¡¡é‡é¢„æµ‹ç‚¹äº‘ `X` ä¸åœ°é¢çœŸå€¼ç‚¹äº‘ `Y` ä¹‹é—´çš„å‡ ä½•è¯¯å·®ã€‚**è¶Šä½è¶Šå¥½**ã€‚åœ¨**å•ä½çƒå½’ä¸€åŒ–**åçš„æ•°æ®ä¸Šè®¡ç®—ã€‚ |\n| | **F-Score** | `F-Score = (2 * Precision * Recall) / (Precision + Recall)`ï¼Œ å…¶ä¸­Precisionå’ŒRecallåœ¨é˜ˆå€¼ `0.001` ä¸‹è®¡ç®—ã€‚ | è¡¡é‡é¢„æµ‹ç‚¹äº‘ä¸åœ°é¢çœŸå€¼ç‚¹äº‘ä¹‹é—´çš„å¯¹åº”å…³ç³»è´¨é‡ï¼Œç»¼åˆäº†å‡†ç¡®ç‡å’Œå¬å›ç‡ã€‚**è¶Šé«˜è¶Šå¥½**ã€‚ |\n| | **æ¨ç†å»¶è¿Ÿ** | é¢„æµ‹ç½‘ç»œå¤„ç†å•ä¸ªæ ·æœ¬æ‰€éœ€çš„æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ã€‚ | è¯„ä¼°é¢„æµ‹æ¨¡å—çš„å®æ—¶æ€§æ½œåŠ›ã€‚FlyCoçš„é¢„æµ‹å™¨å»¶è¿Ÿçº¦ä¸º **150.3ms**ã€‚ |\n| **è§„åˆ’æ¨¡å—** | **é…ç½®ç©ºé—´ç”Ÿæˆæ•ˆç‡** | ç”Ÿæˆå®‰å…¨é…ç½®ç©ºé—´å¹¶æ£€æŸ¥è§†ç‚¹å¯è¡Œæ€§çš„å•æ¬¡æ›´æ–°è¿è¡Œæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ã€‚ | å¯¹æ¯”SDFæŸ¥è¯¢ã€å…‰çº¿æŠ•å°„å’ŒFlyCoæå‡ºçš„å¿«é€Ÿæ–¹æ³•ã€‚FlyCoæ–¹æ³•æœ€å¿«ï¼ˆ~15msï¼‰ã€‚ |\n| | **è§†ç‚¹å¯è¡Œæ€§** | `(ç”Ÿæˆçš„ä½äºç›®æ ‡å¤–éƒ¨è‡ªç”±ç©ºé—´çš„è§†ç‚¹æ•°é‡) / (ç”Ÿæˆçš„æ€»è§†ç‚¹æ•°é‡)` | è¡¡é‡ç”Ÿæˆçš„è§†ç‚¹æ˜¯å¦å®‰å…¨ï¼ˆä¸åµŒå…¥ç›®æ ‡å†…éƒ¨ï¼‰ã€‚**100%ä¸ºç†æƒ³å€¼**ã€‚ |\n| | **å¹³å‡å“åº”å»¶è¿Ÿ** | æœ¬åœ°è½¨è¿¹è§„åˆ’å™¨å•æ¬¡ç”Ÿæˆè½¨è¿¹çš„å¹³å‡æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ã€‚ | è¡¡é‡ç³»ç»Ÿå¯¹åŠ¨æ€éšœç¢ç‰©çš„å®æ—¶ååº”èƒ½åŠ›ã€‚FlyCoçš„å¼‚æ­¥æ¶æ„ä½¿å…¶ä¿æŒåœ¨~25msã€‚ |\n\n### äºŒã€ æ•°æ®é›†\n\nè®ºæ–‡ä½¿ç”¨äº†**åˆæˆæ•°æ®é›†**è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œ**ä»¿çœŸåŸºå‡†æµ‹è¯•**ï¼Œå¹¶è¿›è¡Œäº†**çœŸå®ä¸–ç•Œæ•°æ®é›†**é‡‡é›†ä»¥éªŒè¯ç³»ç»Ÿã€‚\n\n#### 1. è®­ç»ƒæ•°æ®é›†ï¼ˆç”¨äºå¤šæ¨¡æ€è¡¨é¢é¢„æµ‹å™¨ï¼‰\n\n| æ•°æ®é›†å±æ€§ | ç»†èŠ‚ |\n| :--- | :--- |\n| **æ•°æ®æ¥æº** | 1. **Objaverse** æ•°æ®é›†çš„ç²¾é€‰å­é›†ã€‚ <br> 2. ä»äº’è”ç½‘æ”¶é›†çš„å°‘é‡é¢å¤–çœŸå®ä¸–ç•Œæ‰«ææ¨¡å‹ã€‚ |\n| **è§„æ¨¡** | æ€»è®¡çº¦ **70,000ä¸ª**  distinct é«˜ä¿çœŸ3Dæ¨¡å‹ã€‚ |\n| **æ•°æ®ç”Ÿæˆæµç¨‹** | **å…¨è‡ªåŠ¨å¤šæ¨¡æ€æ•°æ®ç”Ÿæˆç®¡é“**ï¼ˆå›¾5ï¼‰ï¼š <br> 1. **æ¸²æŸ“ä¸ç‚¹äº‘åˆæˆ**ï¼šä½¿ç”¨Blenderä»æ¯ä¸ª3Dæ¨¡å‹åœ¨è§‚å¯Ÿçƒä¸Šå‡åŒ€é‡‡æ ·32ä¸ªè§†ç‚¹ï¼Œæ¸²æŸ“RGB-æ·±åº¦å›¾ï¼Œå¹¶åæŠ•å½±æˆç‚¹äº‘ã€‚é€šè¿‡éšæœºèåˆ1-8ä¸ªè§†ç‚¹çš„ç‚¹äº‘æ¥ç”Ÿæˆå¤šæ ·åŒ–çš„éƒ¨åˆ†è§‚æµ‹ç‚¹äº‘ `ğ’«_tar`ã€‚ <br> 2. **æ–‡æœ¬ç”Ÿæˆ**ï¼šä½¿ç”¨å›¾åƒæè¿°æ¨¡å‹ä¸ºæ¯ä¸ªæ¸²æŸ“è§†å›¾ç”Ÿæˆå€™é€‰æè¿°ï¼Œç”¨CLIPè¯„åˆ†ç­›é€‰æœ€ä½³æè¿°ï¼Œæœ€åç”¨GPT-4èåˆæ‰€æœ‰è§†å›¾æè¿°ï¼Œç”Ÿæˆä¸€ä¸ªå…¨å±€çš„ã€æ¨¡å‹ä¸­å¿ƒçš„æ–‡æœ¬æè¿° `h_text`ï¼ˆå¦‚â€œa stone Gothic church featuring a slender spireâ€ï¼‰ã€‚ <br> 3. **æ•°æ®å¢å¼º**ï¼šåº”ç”¨éšæœºè£å‰ªï¼ˆæ¨¡æ‹Ÿä¸åŒç›¸æœºå†…å‚ï¼‰ã€éšæœºSE(3)æ‰°åŠ¨ï¼ˆç›¸æœºå¤–å‚å™ªå£°ï¼‰ã€å…¨å±€åæ ‡å¹³ç§»ã€ç‚¹äº‘éšæœºæ©ç ç­‰ï¼Œä»¥æå‡æ³›åŒ–æ€§ã€‚ |\n| **æœ€ç»ˆæ•°æ®å½¢å¼** | æ¯ä¸ªè®­ç»ƒæ ·æœ¬æ˜¯ä¸€ä¸ªå¯¹é½çš„å…ƒç»„ï¼š`(ğ’«_tar, â„_tar, ğ _tar, h_text, ğ’«_gt)`ã€‚ |\n\n#### 2. ä»¿çœŸæµ‹è¯•æ•°æ®é›†ï¼ˆç”¨äºåŸºå‡†æ¯”è¾ƒä¸æ¶ˆèå®éªŒï¼‰\n\n| åœºæ™¯åç§° | ç¯å¢ƒæè¿° | ç›®æ ‡ç»“æ„å°ºå¯¸ï¼ˆé•¿Ã—å®½Ã—é«˜ï¼‰ | æŒ‘æˆ˜ |\n| :--- | :--- | :--- | :--- |\n| **Windmill** | ä¹¡æ‘ä¸˜é™µæ™¯è§‚ä¸­çš„é£è½¦ã€‚ | 16m Ã— 30m Ã— 32m | å¤æ‚å‡ ä½•å½¢çŠ¶ï¼Œå‘¨å›´æœ‰åœ°å½¢ã€‚ |\n| **Church** | æ‚ä¹±æ¬§å¼å°é•‡ä¸­çš„æ•™å ‚ã€‚ | 35m Ã— 17m Ã— 18m | å¯†é›†ç¯å¢ƒï¼Œç›®æ ‡å‘¨å›´æœ‰å¤§é‡éç›®æ ‡éšœç¢ç‰©ã€‚ |\n| **Pagoda** | å¯†é›†å¯ºåº™åº­é™¢ä¸­å¿ƒçš„å®å¡”ã€‚ | 15m Ã— 15m Ã— 19m | ç‹­çª„ç©ºé—´ï¼Œç²¾ç»†ç»“æ„ã€‚ |\n| **Schloss** | å¤šå˜åœ°å½¢å’Œæ¤è¢«ç¯ç»•çš„åŸå ¡ã€‚ | 22m Ã— 33m Ã— 20m | å¤§è§„æ¨¡ï¼Œéå‡¸ç»“æ„ï¼Œæ‚ä¹±ç¯å¢ƒã€‚ |\n| **ä»¿çœŸå™¨** | **AirSim**ï¼Œä¸€ä¸ªç‰©ç†é€¼çœŸçš„ä»¿çœŸå™¨ã€‚ | | |\n| **æµ‹è¯•åè®®** | æ¯ä¸ªåœºæ™¯ï¼Œæ¯ä¸ªæ–¹æ³•è¿è¡Œ **20æ¬¡** å¯é‡å¤è¯•éªŒã€‚ | | |\n\n#### 3. é›¶æ ·æœ¬é¢„æµ‹æµ‹è¯•æ•°æ®é›†ï¼ˆç”¨äºè¯„ä¼°é¢„æµ‹å™¨æ³›åŒ–èƒ½åŠ›ï¼‰\n\n| æ•°æ®é›†åç§° | ç»†èŠ‚ | ç”¨é€” |\n| :--- | :--- | :--- |\n| **Objaverse** | åŒ…å«çº¦10,000ä¸ª**æœªåœ¨è®­ç»ƒä¸­è§è¿‡**çš„ç‰©ä½“å’Œåœºæ™¯ã€‚ | ç”¨äºè¯„ä¼°é¢„æµ‹æ¨¡å‹åœ¨**é›¶æ ·æœ¬**è®¾ç½®ä¸‹çš„æ³›åŒ–æ€§èƒ½ï¼ˆè¡¨5ï¼Œå›¾21ï¼‰ã€‚ |\n| **OmniObject3D** | å¤§è§„æ¨¡çœŸå®æ‰«æ3Dç‰©ä½“æ•°æ®é›†ï¼ŒåŒæ ·åœ¨è®­ç»ƒä¸­æœªä½¿ç”¨ã€‚ | åŒä¸Šï¼Œå¢åŠ æµ‹è¯•çš„å¤šæ ·æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚ |\n\n#### 4. çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼ˆç”¨äºéªŒè¯ç³»ç»Ÿå¯è¡Œæ€§ï¼‰\n\nè®ºæ–‡æœªä½¿ç”¨é¢„å½•åˆ¶çš„æ ‡å‡†æ•°æ®é›†ï¼Œè€Œæ˜¯åœ¨**å››ä¸ªä¸åŒçš„é‡å¤–åœ°ç‚¹**è¿›è¡Œäº†å®é™…çš„è‡ªä¸»é£è¡Œå®éªŒï¼Œé‡‡é›†äº†ç¬¬ä¸€æ‰‹çš„ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆRGBå›¾åƒã€LiDARç‚¹äº‘ï¼‰ï¼Œä»¥éªŒè¯ç³»ç»Ÿåœ¨çœŸå®ã€æœªçŸ¥ã€å¼€æ”¾ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚\n- **åœºæ™¯**ï¼šè¿æ¥å»ºç­‘çš„æ‹±æ¡¥ã€å¤§å‹éŸ³ä¹å…ã€åŸå ¡å¤§é—¨ã€æ£®æ—ä¸­çš„çº¢ç –å»ºç­‘ã€‚\n- **æ•°æ®**ï¼šé£è¡Œè¿‡ç¨‹ä¸­å®æ—¶äº§ç”Ÿçš„æ„ŸçŸ¥ç»“æœï¼ˆåˆ†å‰²ç‚¹äº‘ï¼‰ã€é¢„æµ‹ç»“æœï¼ˆå®Œæ•´è¡¨é¢ç½‘æ ¼ï¼‰ã€è§„åˆ’è½¨è¿¹ã€ä»¥åŠæœ€ç»ˆé‡å»ºçš„3Dæ¨¡å‹ã€‚è¿™äº›æ•°æ®ç”¨äºç”Ÿæˆæ–‡ä¸­çš„å›¾è¡¨å’Œå®šæ€§ç»“æœï¼ˆå›¾13-15ï¼Œè¡¨1ï¼‰ã€‚",
            "timestamp": "2026-01-21T12:15:04.313371",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨**çœŸå®ä¸–ç•Œ**å’Œ**ä»¿çœŸç¯å¢ƒ**ä¸­è¿›è¡Œäº†å…¨é¢çš„å®éªŒéªŒè¯ã€‚\n\n#### 1. çœŸå®ä¸–ç•Œå®éªŒï¼ˆIn-the-wild Field Experimentsï¼‰\n- **åœºæ™¯**ï¼šå››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é‡å¤–åœºæ™¯ï¼Œç›®æ ‡ç»“æ„å¤šæ ·ä¸”ç¯å¢ƒå¤æ‚ã€‚\n    - **æ‹±æ¡¥**ï¼ˆArch Bridgeï¼‰ï¼šè¿æ¥ä¸¤æ ‹å¤§å‹å»ºç­‘çš„æ‹±å½¢ç»“æ„ã€‚\n    - **éŸ³ä¹å…**ï¼ˆConcert Hallï¼‰ï¼šå¤§å‹åœ†å½¢ç©¹é¡¶å»ºç­‘ï¼ˆ100m Ã— 60m Ã— 35mï¼‰ã€‚\n    - **åŸå ¡å¤§é—¨**ï¼ˆCastle Gateï¼‰ï¼šå…¬å›­æ ‘æœ¨ç¯ç»•çš„åŸå ¡å…¥å£ã€‚\n    - **çº¢ç –å»ºç­‘**ï¼ˆRed-brick Buildingï¼‰ï¼šèŒ‚å¯†æ£®æ—ä¸­çš„ç‹¬ç«‹å»ºç­‘ã€‚\n- **æ•°æ®**ï¼šä½¿ç”¨å®šåˆ¶æ— äººæœºå¹³å°ï¼ˆDJI M30æ”¹è£…ï¼‰é‡‡é›†**å®æ—¶RGBå›¾åƒ**å’Œ**LiDARç‚¹äº‘**æµæ•°æ®ã€‚æ— ä»»ä½•å…ˆéªŒåœ°å›¾æˆ–åœºæ™¯ä¿¡æ¯ã€‚\n\n#### 2. ä»¿çœŸå®éªŒï¼ˆSimulation Benchmarksï¼‰\n- **ä»¿çœŸå™¨**ï¼šä½¿ç”¨**AirSim**ï¼ˆé«˜ç‰©ç†çœŸå®æ€§ä»¿çœŸå™¨ï¼‰ã€‚\n- **åœºæ™¯**ï¼šå››ä¸ªå¤æ‚çš„ã€æœªè§è¿‡çš„3Dåœºæ™¯ï¼Œæ¨¡æ‹Ÿå…¸å‹çœŸå®ä¸–ç•Œæ£€æµ‹ç›®æ ‡ï¼š\n    - **é£è½¦**ï¼ˆWindmillï¼‰ï¼šä¹¡æ‘ä¸˜é™µæ™¯è§‚ã€‚\n    - **æ•™å ‚**ï¼ˆChurchï¼‰ï¼šæ‚ä¹±æ¬§å¼å°é•‡ä¸­çš„æ•™å ‚ã€‚\n    - **å®å¡”**ï¼ˆPagodaï¼‰ï¼šå¯†é›†å¯ºåº™åº­é™¢ä¸­å¿ƒã€‚\n    - **åŸå ¡**ï¼ˆSchlossï¼‰ï¼šå¤šå˜åœ°å½¢å’Œæ¤è¢«ç¯ç»•ã€‚\n- **æ•°æ®**ï¼šè™šæ‹Ÿæ— äººæœºé…å¤‡360Â° LiDARå’Œäº‘å°RGBç›¸æœºï¼Œé‡‡é›†å‡ ä½•ä¸è§†è§‰æ•°æ®ã€‚\n\n#### 3. é¢„æµ‹å™¨è®­ç»ƒä¸é›¶æ ·æœ¬æµ‹è¯•æ•°æ®é›†\n- **è®­ç»ƒæ•°æ®**ï¼šä½¿ç”¨çº¦ **70,000ä¸ª** é«˜ä¿çœŸ3Dæ¨¡å‹ï¼ˆåŒ…æ‹¬Objaverseå­é›†å’Œäº’è”ç½‘çœŸå®æ‰«æï¼‰ï¼Œé€šè¿‡**è‡ªåŠ¨å¤šæ¨¡æ€æ•°æ®ç”Ÿæˆæµæ°´çº¿**åˆæˆå¯¹é½çš„ï¼ˆç‚¹äº‘ã€RGBå›¾åƒã€ç›¸æœºå‚æ•°ã€æ–‡æœ¬æè¿°ï¼‰è®­ç»ƒæ ·æœ¬ã€‚\n- **é›¶æ ·æœ¬æµ‹è¯•æ•°æ®**ï¼šä½¿ç”¨ **Objaverse** å’Œ **OmniObject3D** æ•°æ®é›†ä¸­**å®Œå…¨æœªè§è¿‡çš„10,000ä¸ªç‰©ä½“å’Œåœºæ™¯**ï¼Œè¯„ä¼°é¢„æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡ä»**å®ç”¨æ€§**ã€**æ€§èƒ½**å’Œ**å½’å› **ä¸‰ä¸ªç»´åº¦è®¾è®¡è¯„ä¼°æŒ‡æ ‡ã€‚\n\n#### 1. ç³»ç»Ÿçº§æ€§èƒ½æŒ‡æ ‡ï¼ˆä»¿çœŸä¸çœŸå®å®éªŒï¼‰\n- **é£è¡Œæ—¶é—´**ï¼šä»èµ·é£åˆ°ç€é™†çš„æ€»ä»»åŠ¡æ—¶é—´ï¼ˆç§’ï¼‰ã€‚**è¶Šä½è¶Šå¥½**ï¼Œè¡¡é‡ä»»åŠ¡æ•ˆç‡ã€‚\n- **ä¿¡æ¯å®Œæ•´æ€§**ï¼šæˆåŠŸé‡å»ºçš„ç›®æ ‡è¡¨é¢å åœ°é¢çœŸå®è¡¨é¢çš„ç™¾åˆ†æ¯”ï¼ˆ%ï¼‰ã€‚**è¶Šé«˜è¶Šå¥½**ï¼Œè¡¡é‡æ•°æ®è¦†ç›–å®Œæ•´æ€§ã€‚\n    - è®¡ç®—æ–¹æ³•ï¼šå°†é‡å»ºç½‘æ ¼å’ŒçœŸå®ç½‘æ ¼åœ¨0.05mÃ—0.05mç½‘æ ¼ä¸Šä½“ç´ åŒ–ï¼Œè®¡ç®—å¬å›ç‡ã€‚\n- **æˆåŠŸç‡**ï¼šåœ¨é¢„å®šæ—¶é—´é¢„ç®—ï¼ˆ`T_max = 1500ç§’`ï¼‰å†…æ— ç¢°æ’å®Œæˆä»»åŠ¡çš„è¯•éªŒæ¯”ä¾‹ã€‚è¡¡é‡å®‰å…¨æ€§ä¸å¯é æ€§ã€‚\n- **æ€§èƒ½ä¸€è‡´æ€§**ï¼šé€šè¿‡**20æ¬¡é‡å¤è¯•éªŒ**çš„é£è¡Œæ—¶é—´å’Œä¿¡æ¯å®Œæ•´æ€§çš„**æ ‡å‡†å·®**åŠåˆ†å¸ƒï¼ˆå°æç´å›¾ï¼‰è¡¡é‡ç³»ç»Ÿç¨³å®šæ€§ã€‚\n\n#### 2. æ¨¡å—çº§è¯„ä¼°æŒ‡æ ‡ï¼ˆæ¶ˆèå®éªŒï¼‰\n- **æ„ŸçŸ¥æ¨¡å—**ï¼š\n    - **2Dåˆ†å‰²æ—¶åºç¨³å®šæ€§**ï¼šå®šæ€§è§‚å¯Ÿåˆ†å‰²æ©ç åœ¨é•¿æ—¶é—´é£è¡Œä¸­çš„æ¼‚ç§»æƒ…å†µã€‚\n    - **3Dåˆ†å‰²å‡ ä½•ç²¾åº¦**ï¼šå®šæ€§æ¯”è¾ƒæŠ•å½±åˆ°ç‚¹äº‘ä¸Šçš„ç›®æ ‡ç»“æ„çº¯å‡€åº¦ã€‚\n    - **æ„ŸçŸ¥å»¶è¿Ÿä¸GPUå†…å­˜æ¶ˆè€—**ï¼šå®æ—¶æ€§èƒ½ï¼ˆæ¯«ç§’/å¸§ï¼‰å’Œå†…å­˜å ç”¨çš„æ—¶åºæ›²çº¿ã€‚\n- **é¢„æµ‹æ¨¡å—**ï¼š\n    - **L1å€’è§’è·ç¦»**ï¼šé¢„æµ‹ç‚¹äº‘ä¸çœŸå®å®Œæ•´è¡¨é¢ä¹‹é—´çš„å‡ ä½•è¯¯å·®ï¼ˆL1 Chamfer Distanceï¼‰ã€‚**è¶Šä½è¶Šå¥½**ã€‚\n    - **F-Score**ï¼ˆé˜ˆå€¼0.001ï¼‰ï¼šè¡¡é‡é¢„æµ‹ç‚¹äº‘ä¸çœŸå®è¡¨é¢ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚**è¶Šé«˜è¶Šå¥½**ã€‚\n    - **æ¨ç†å»¶è¿Ÿ**ï¼šå•æ ·æœ¬é¢„æµ‹è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰ã€‚\n- **è§„åˆ’æ¨¡å—**ï¼š\n    - **é…ç½®ç©ºé—´ç”Ÿæˆæ•ˆç‡**ï¼šè¿è¡Œæ—¶ï¼ˆæ¯«ç§’/æ¬¡ï¼‰å’Œè§†ç‚¹å¯è¡Œæ€§ï¼ˆ%ï¼‰ã€‚\n    - **å…¨å±€è§„åˆ’ä¸€è‡´æ€§å½±å“**ï¼šé£è¡Œæ—¶é—´å¢é•¿ç™¾åˆ†æ¯”å’Œæ ‡å‡†å·®å˜åŒ–ã€‚\n    - **å±€éƒ¨è§„åˆ’å“åº”å»¶è¿Ÿ**ï¼šå¹³å‡è½¨è¿¹ç”Ÿæˆå»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰ã€‚\n\n### ä¸‰ã€ä¸»è¦æ€§èƒ½æå‡\nFlyCoç³»ç»Ÿåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚\n\n#### 1. ä¸SOTAæ–¹æ³•çš„ä»¿çœŸå¯¹æ¯”ç»“æœ\nå¯¹æ¯”æ–¹æ³•ï¼š**Plan3D**ï¼ˆä¸¤é˜¶æ®µèŒƒå¼ï¼‰å’Œ **Star-Searcher**ï¼ˆæ¢ç´¢å¼èŒƒå¼ï¼‰ã€‚\n- **é£è¡Œæ•ˆç‡**ï¼šFlyCoå¹³å‡é£è¡Œæ—¶é—´ **å‡å°‘1.25å€è‡³3.0å€**ã€‚\n- **è¦†ç›–å®Œæ•´æ€§**ï¼šFlyCoä¿¡æ¯å®Œæ•´æ€§ **é«˜å‡º4.3% è‡³ 56.2%**ã€‚\n- **æˆåŠŸç‡**ï¼šFlyCoæˆåŠŸç‡ **é«˜å‡º11.6% è‡³ 50.1%**ã€‚\n- **äººç±»å…ˆéªŒéœ€æ±‚**ï¼šFlyCoä»…éœ€**æ–‡æœ¬+ç¨€ç–2Dæ ‡æ³¨**ï¼ˆä½åŠªåŠ›ï¼‰ï¼Œè€ŒåŸºçº¿éœ€è¦**3Dè¾¹ç•Œæ¡†**ï¼ˆä¸­åŠªåŠ›ï¼‰æˆ–**æ‰‹åŠ¨3Dåˆ†å‰²+é¢„å®šä¹‰é£è¡Œæ¨¡å¼**ï¼ˆé«˜åŠªåŠ›ï¼‰ã€‚\n\n**ä»£è¡¨æ€§æ•°æ®ï¼ˆå››ä¸ªåœºæ™¯å¹³å‡ï¼‰**ï¼š\n| åœºæ™¯ | FlyCoé£è¡Œæ—¶é—´ (s) | vs. æœ€ä½³åŸºçº¿æå‡ | FlyCoä¿¡æ¯å®Œæ•´æ€§ (%) | vs. æœ€ä½³åŸºçº¿æå‡ |\n| :--- | :--- | :--- | :--- | :--- |\n| é£è½¦ | 329.4 | **å‡å°‘çº¦2.4å€** | 96.04 | **é«˜å‡ºçº¦6.3%** |\n| æ•™å ‚ | 336.6 | **å‡å°‘çº¦1.6å€** | 94.53 | **é«˜å‡ºçº¦4.3%** |\n| å®å¡” | 225.1 | **ä¸æœ€ä½³åŸºçº¿ç›¸å½“** | 93.83 | **é«˜å‡ºçº¦20.4%** |\n| åŸå ¡ | 363.9 | **å‡å°‘çº¦1.4å€** | 96.50 | **é«˜å‡ºçº¦5.4%** |\n\n#### 2. é¢„æµ‹å™¨é›¶æ ·æœ¬æ€§èƒ½\nåœ¨Objaverse/OmniObject3Dæµ‹è¯•é›†ä¸Šï¼š\n- **L1-CD**ï¼š**28.037**ï¼Œä¼˜äºCRA-PCN (31.053)ã€XMFNet (37.393) å’Œ TripoSR (44.831)ã€‚\n- **F-Score**ï¼š**0.705**ï¼Œä¼˜äºCRA-PCN (0.639)ã€XMFNet (0.601) å’Œ TripoSR (0.569)ã€‚\n- **æ¨ç†å»¶è¿Ÿ**ï¼š**150.3 ms**ï¼Œä¸ºå¯¹æ¯”æ–¹æ³•ä¸­æœ€å¿«ã€‚\n\n#### 3. çœŸå®ä¸–ç•Œæ¼”ç¤ºäº®ç‚¹\n- **å¤§è§„æ¨¡ä»»åŠ¡**ï¼šæˆåŠŸå®Œæˆè¶…**1000ç±³**è¿ç»­è‡ªä¸»é£è¡Œï¼ˆéŸ³ä¹å…åœºæ™¯ï¼‰ï¼Œé¦–æ¬¡æ¼”ç¤ºã€‚\n- **å®‰å…¨ä¿è¯**ï¼šåœ¨å¯†é›†éšœç¢ç‰©ï¼ˆå¦‚2.5ç±³çª„é€šé“ï¼‰ä¸­é£è¡Œï¼Œæœ€å°éšœç¢ç‰©è·ç¦»å§‹ç»ˆé«˜äºæ— äººæœºç¢°æ’é˜ˆå€¼ï¼ˆ0.55ç±³ï¼‰ã€‚\n- **å®æ—¶æ€§èƒ½**ï¼šç³»ç»Ÿå„æ¨¡å—æ»¡è¶³å®æ—¶æ€§è¦æ±‚ï¼ˆæ„ŸçŸ¥~2.5 Hzï¼Œå±€éƒ¨è§„åˆ’~20 Hzï¼‰ã€‚\n\n#### 4. æ¶ˆèå®éªŒéªŒè¯çš„å…³é”®æå‡æ¥æº\n- **æ„ŸçŸ¥**ï¼šè·¨æ¨¡æ€ç²¾ç‚¼æœºåˆ¶ä½¿2D/3Dç›®æ ‡è·Ÿè¸ªåœ¨é•¿æ—¶åºé£è¡Œä¸­ä¿æŒç¨³å®šï¼Œé¿å…äº†åŸå§‹SAM2çš„æ¼‚ç§»å’Œå´©æºƒã€‚\n- **é¢„æµ‹**ï¼š\n    - **å¤šæ¨¡æ€èåˆ**ï¼šè”åˆä½¿ç”¨å‡ ä½•ã€è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œæ¯”å•ä¸€æ¨¡æ€é¢„æµ‹ç²¾åº¦æ˜¾è‘—æå‡ã€‚\n    - **éƒ¨åˆ†è¡¨é¢æ­£åˆ™åŒ–**ï¼šè¯¥æŸå¤±é¡¹ä½¿é£è¡Œæ—¶é—´å‡å°‘çº¦16-27%ï¼Œé¿å…äº†å› é¢„æµ‹ä¸è§‚æµ‹ä¸ä¸€è‡´å¯¼è‡´çš„ä½æ•ˆé‡è®¿ã€‚\n- **è§„åˆ’**ï¼š\n    - **ä¸€è‡´æ€§æ„ŸçŸ¥å…¨å±€è§„åˆ’**ï¼šé¿å…è§†ç‚¹åºåˆ—çš„å‰§çƒˆé‡ç»„ï¼Œå°†é£è¡Œæ—¶é—´é¢å¤–å‡å°‘çº¦65-85%ã€‚\n    - **å¼‚æ­¥åˆ†å±‚æ¡†æ¶**ï¼šå°†å±€éƒ¨è§„åˆ’å“åº”å»¶è¿Ÿä¿æŒåœ¨~25msï¼Œè€ŒåŒæ­¥æ–¹æ¡ˆå¯¼è‡´å»¶è¿Ÿå¢è‡³200-300mså¹¶å¼•å‘ç¢°æ’ã€‚\n    - **è§†ç‚¹çº¦æŸè½¨è¿¹ä¼˜åŒ–**ï¼šé¿å…äº†å› è¦†ç›–ç¼ºå¤±å¯¼è‡´çš„å›æº¯é£è¡Œï¼Œæå‡äº†æ•ˆç‡ã€‚\n\n### æ€»ç»“\nFlyCoé€šè¿‡å…¶**åŸºç¡€æ¨¡å‹èµ‹èƒ½çš„æ„ŸçŸ¥-é¢„æµ‹-è§„åˆ’é—­ç¯æ¶æ„**ï¼Œåœ¨**å¤šæ ·åŒ–çš„çœŸå®å’Œä»¿çœŸæ•°æ®é›†**ä¸ŠéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚æ ¸å¿ƒæå‡ä½“ç°åœ¨ï¼š**å¤§å¹…é™ä½äººç±»æ“ä½œè´Ÿæ‹…**çš„åŒæ—¶ï¼Œå®ç°äº†**æ›´çŸ­çš„é£è¡Œæ—¶é—´**ã€**æ›´å®Œæ•´çš„ç›®æ ‡è¦†ç›–**å’Œ**æ›´é«˜çš„ä»»åŠ¡æˆåŠŸç‡**ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œ3Dç›®æ ‡æ‰«ææä¾›äº†ä¸€ä¸ªå®ç”¨ã€é«˜æ•ˆä¸”å®‰å…¨çš„ç³»ç»Ÿçº§è§£å†³æ–¹æ¡ˆã€‚",
            "timestamp": "2026-01-21T12:16:02.435081",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡ç¬¬8.1èŠ‚â€œLimitations and Future Workâ€ï¼Œ**FlyCoç³»ç»Ÿç›®å‰å­˜åœ¨ä¸‰ä¸ªä¸»è¦å±€é™æ€§**ï¼š\n\n### 1. æ–‡æœ¬ä¿¡æ¯åˆ©ç”¨æœ‰é™\n- **è¡¨è¾¾å½¢å¼å—é™**ï¼šå½“å‰ç³»ç»Ÿä»…æ”¯æŒç®€çŸ­ã€ç»“æ„åŒ–çš„æ–‡æœ¬çŸ­è¯­æ¥æè¿°ç›®æ ‡çš„å…³é”®å±æ€§ï¼ˆå¦‚â€œå¸¦åœ†é¡¶çš„åœ†å½¢ç»“æ„â€ï¼‰ã€‚è¿™è™½ç„¶æé«˜äº†åœ¨å¼€æ”¾ä¸–ç•Œéƒ¨ç½²ä¸­çš„é²æ£’æ€§å¹¶å‡å°‘äº†æ­§ä¹‰ï¼Œä½†**é™åˆ¶äº†è‡ªç”±å½¢å¼è‡ªç„¶è¯­è¨€çš„è¡¨è¾¾èƒ½åŠ›**ï¼Œç”¨æˆ·æ— æ³•ç”¨æ›´å¤æ‚ã€çµæ´»çš„è¯­è¨€æŒ‡å®šæ„å›¾ã€‚\n- **è®­ç»ƒæ•°æ®ä¸å¹³è¡¡**ï¼šåœ¨è®­ç»ƒå¤šæ¨¡æ€è¡¨é¢é¢„æµ‹å™¨æ—¶ï¼Œä¸€ä¸ª3Dæ¨¡å‹å¯ä»¥ç”Ÿæˆè®¸å¤šéƒ¨åˆ†ç‚¹äº‘å’Œå›¾åƒè§‚æµ‹ï¼Œä½†å®ƒä»¬éƒ½å¯¹åº”åŒä¸€ä¸ªæ–‡æœ¬æè¿°ã€‚è¿™ç§â€œå¤šå¯¹ä¸€â€çš„æ˜ å°„å¯¼è‡´å­¦ä¹ è¿‡ç¨‹**åå‘äºå‡ ä½•å’Œè§†è§‰æ¨¡æ€**ï¼Œå‰Šå¼±äº†è¯­è¨€ä¿¡æ¯çš„å½±å“åŠ›ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œè¿™ä¸è¿‘æœŸè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰ä¸­è¯­è¨€å¸¸ä½œä¸ºå¼±ä¿¡å·è€Œéä¸»å¯¼é©±åŠ¨å› ç´ çš„è§‚å¯Ÿä¸€è‡´ã€‚\n\n### 2. é¢„æµ‹æ¨¡å—ä¾èµ–ç¦»çº¿è®¡ç®—\n- **éå®Œå…¨æœºè½½è¿è¡Œ**ï¼šè¡¨é¢é¢„æµ‹æ¨¡å—ç›®å‰è¿è¡Œåœ¨åœ°é¢ç«™è®¾å¤‡ä¸Šï¼Œè€Œéå®Œå…¨é›†æˆåœ¨æ— äººæœºæœºè½½è®¡ç®—æœºä¸Šã€‚\n- **åŸå› **ï¼š\n    1. **éä¸¥æ ¼å®æ—¶æ€§**ï¼šé¢„æµ‹å¯¹äºé—­ç¯é£è¡Œå¹¶éä¸¥æ ¼æ—¶é—´å…³é”®ã€‚\n    2. **è®¡ç®—å¼€é”€å¤§**ï¼šé›†æˆåŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰ä¼šå¸¦æ¥å·¨å¤§çš„è®¡ç®—å¼€é”€ï¼Œåœ¨å½“å‰è¾¹ç¼˜ç¡¬ä»¶ä¸Šå®ç°äºšç§’çº§æœºè½½æ¨ç†ä¸åˆ‡å®é™…ã€‚\n- **å½±å“**ï¼šåœ¨é€šä¿¡å—é™çš„ç¯å¢ƒï¼ˆå¦‚åœ°ä¸‹çŸ¿äº•ã€å°é—­å·¥ä¸šè®¾æ–½æˆ–ç”µç£å¹²æ‰°åŒºåŸŸï¼‰ä¸­ï¼Œè¿™ç§å¯¹å¤–éƒ¨é€šä¿¡çš„ä¾èµ–å¯èƒ½æˆä¸ºé™åˆ¶ç³»ç»Ÿå¹¿æ³›åº”ç”¨çš„å…³é”®ç“¶é¢ˆã€‚\n\n### 3. å¯æ‰©å±•æ€§é™äºå•æœºå•ç›®æ ‡ä»»åŠ¡\n- **ä»»åŠ¡è§„æ¨¡å—é™**ï¼šå½“å‰ç³»ç»Ÿå—é™äºå•æ¶æ— äººæœºçš„ç»­èˆªèƒ½åŠ›ï¼Œè®¾è®¡ä¸ºæ¯æ¬¡ä»»åŠ¡æ‰«æä¸€ä¸ªç›®æ ‡ç»“æ„ã€‚\n- **æŒ‘æˆ˜**ï¼šå°†èƒ½åŠ›æ‰©å±•åˆ°åŸå¸‚å°ºåº¦æˆ–å¤§å‹è‡ªç„¶åœºæ™¯éœ€è¦**å¤šæ— äººæœºååŒæ“ä½œ**ï¼Œè¿™å¼•å…¥äº†æ–°çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŠ¨æ€é«˜å±‚ä»»åŠ¡åˆ†é…ã€æœºå™¨äººé—´åè°ƒä»¥åŠåŸºäºéƒ¨åˆ†è§‚æµ‹çš„å®æ—¶æ‰«æç›®æ ‡é‡åˆ†é…ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡æå‡ºäº†ç›¸åº”çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼š\n\n### 1. æå‡æ–‡æœ¬ä¿¡æ¯åˆ©ç”¨\n- **æ”¯æŒæ›´ä¸°å¯Œçš„è¯­è¨€å½¢å¼**ï¼šæœªæ¥å·¥ä½œå°†æ”¯æŒæ›´è‡ªç”±ã€ä¸°å¯Œçš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œä»¥æå‡ç”¨æˆ·æ„å›¾è¡¨è¾¾çš„çµæ´»æ€§ã€‚\n- **æ”¹è¿›æ•°æ®å¹³è¡¡ä¸è·¨æ¨¡æ€å¯¹é½**ï¼šæ¢ç´¢æ”¹è¿›çš„æ•°æ®å¹³è¡¡ç­–ç•¥å’Œæ›´ç´§å¯†çš„è·¨æ¨¡æ€å¯¹é½ç›®æ ‡ï¼Œä»¥å¢å¼ºæ–‡æœ¬æ¨¡æ€åœ¨é¢„æµ‹ä¸­çš„å½±å“åŠ›ï¼Œä½¿å…¶ä»â€œå¼±ä¿¡å·â€è½¬å˜ä¸ºæ›´ä¸»å¯¼çš„é©±åŠ¨å› ç´ ã€‚\n\n### 2. å®ç°å®Œå…¨æœºè½½éƒ¨ç½²\n- **æ¨¡å‹å‹ç¼©ä¸çŸ¥è¯†è’¸é¦**ï¼šä¸€ä¸ªé‡è¦çš„æ–¹å‘æ˜¯**å‹ç¼©å’Œè’¸é¦åŸºç¡€æ¨¡å‹çš„çŸ¥è¯†åˆ°æ›´è½»é‡çº§çš„é¢„æµ‹å™¨ä¸­**ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°å‡†ç¡®ä¸”é«˜æ•ˆçš„æ¨ç†ã€‚\n- **ç›®æ ‡**ï¼šæ¶ˆé™¤å¯¹å¤–éƒ¨é€šä¿¡çš„ä¾èµ–ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿåº”ç”¨äºé€šä¿¡å—é™çš„ç¯å¢ƒï¼Œæ‹“å®½å…¶é€‚ç”¨åœºæ™¯ã€‚\n\n### 3. æ‰©å±•è‡³å¤šæœºååŒä¸å¤§è§„æ¨¡ä»»åŠ¡\n- **ç ”ç©¶å¤šæ™ºèƒ½ä½“æ‰©å±•æ¶æ„**ï¼šæœªæ¥å·¥ä½œå°†ç ”ç©¶æ‰€æå‡ºæ¶æ„çš„**å¤šæ™ºèƒ½ä½“æ‰©å±•ç‰ˆæœ¬**ã€‚\n- **æ ¸å¿ƒæŒ‘æˆ˜**ï¼šå¤šä¸ªæ— äººæœºéœ€è¦ååŒåˆ’åˆ†ç›®æ ‡ç»“æ„ã€è‡ªé€‚åº”åœ°é‡æ–°è§„åˆ’ä»»åŠ¡åˆ†é…ï¼Œå¹¶åœ¨ä¿è¯å®‰å…¨å’Œè¦†ç›–åº¦çš„å‰æä¸‹å…±åŒæœ€å°åŒ–æ€»ä»»åŠ¡æ—¶é—´ã€‚\n- **æ„¿æ™¯**ï¼šå°†ç³»ç»Ÿèƒ½åŠ›ä»**å•ç»“æ„è‡ªä¸»æ‰«æå‡çº§ä¸ºå¤§è§„æ¨¡ã€åä½œçš„å¼€æ”¾ä¸–ç•Œæ‰«æ**ã€‚\n\n## æ€»ç»“\n**FlyCo**ä½œä¸ºä¸€ä¸ªåˆ›æ–°çš„ç³»ç»Ÿè“å›¾ï¼Œé€šè¿‡åŸºç¡€æ¨¡å‹èµ‹èƒ½ï¼Œåœ¨å¼€æ”¾ä¸–ç•Œ3Dç›®æ ‡ç»“æ„æ‰«æä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å…¶å½“å‰å±€é™æ€§ä¸»è¦é›†ä¸­åœ¨**è¯­è¨€æ¥å£çš„ä¸°å¯Œæ€§ã€è®¡ç®—çš„å®Œå…¨è‡ªä¸»æ€§ä»¥åŠä»»åŠ¡è§„æ¨¡çš„å¯æ‰©å±•æ€§**ä¸Šã€‚æœªæ¥çš„æ”¹è¿›å°†å›´ç»•**æ·±åŒ–å¤šæ¨¡æ€èåˆã€æ¨åŠ¨è¾¹ç¼˜æ™ºèƒ½è®¡ç®—å’Œå‘å±•ååŒè‡ªä¸»ç³»ç»Ÿ**å±•å¼€ï¼Œæ—¨åœ¨ä½¿è¯¥ç³»ç»Ÿæ›´å…·è¡¨è¾¾åŠ›ã€æ›´ç‹¬ç«‹ã€æ›´å¼ºå¤§ï¼Œæœ€ç»ˆå®ç°å¤§è§„æ¨¡ã€é«˜æ•ˆç‡çš„è‡ªä¸»3Dæ•°æ®é‡‡é›†ã€‚",
            "timestamp": "2026-01-21T12:16:29.370275",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T14:14:39Z",
    "created_at": "2026-01-21T12:09:10.659546",
    "updated_at": "2026-01-21T12:09:10.659555",
    "recommend": 0
}