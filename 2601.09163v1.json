{
    "id": "2601.09163v1",
    "title": "CEI: A Unified Interface for Cross-Embodiment Visuomotor Policy Learning in 3D Space",
    "authors": [
        "Tong Wu",
        "Shoujie Li",
        "Junhao Gong",
        "Changqing Guo",
        "Xingting Li",
        "Shilong Mu",
        "Wenbo Ding"
    ],
    "abstract": "åŸºäºå¤§è§„æ¨¡æ“ä½œæ•°æ®é›†è®­ç»ƒçš„æœºå™¨äººåŸºç¡€æ¨¡å‹åœ¨ä¹ å¾—é€šç”¨ç­–ç•¥æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç”±äºæ•°æ®é›†åå·®ï¼Œè¿™äº›æ¨¡å‹å¸¸ä¼šè¿‡æ‹Ÿåˆäºç‰¹å®šè§†è§’ã€æœºæ¢°è‡‚å½¢æ€ï¼Œå°¤å…¶æ˜¯å¹³è¡Œå¤¹çˆªã€‚ä¸ºçªç ´è¿™ä¸€å±€é™ï¼Œæˆ‘ä»¬æå‡ºè·¨å…·èº«äº¤äº’æ¡†æ¶ï¼ˆCross-Embodiment Interface, \\CEIï¼‰ï¼Œè¯¥æ¡†æ¶é€šè¿‡è·¨å…·èº«å­¦ä¹ å®ç°ä¸åŒæœºæ¢°è‡‚ä¸æœ«ç«¯æ‰§è¡Œå™¨å½¢æ€é—´çš„æ¼”ç¤ºè¿ç§»ã€‚\\CEIåˆ›æ–°æ€§åœ°æå‡ºâ€œåŠŸèƒ½ç›¸ä¼¼æ€§â€æ¦‚å¿µï¼Œå¹¶é‡‡ç”¨å®šå‘å€’è§’è·ç¦»è¿›è¡Œé‡åŒ–åº¦é‡ã€‚é€šè¿‡åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•å®ç°æœºå™¨äººè½¨è¿¹å¯¹é½ï¼Œè¿›è€Œä¸ºæœªè§è¿‡çš„æœºæ¢°è‡‚ä¸æœ«ç«¯æ‰§è¡Œå™¨åˆæˆè§‚æµ‹æ•°æ®ä¸åŠ¨ä½œæŒ‡ä»¤ã€‚å®éªŒè¡¨æ˜ï¼Œ\\CEIåœ¨ä»¿çœŸç¯å¢ƒä¸­æˆåŠŸå°†å¼—å…°å¡ç†ŠçŒ«æœºå™¨äººçš„æ•°æ®ä¸ç­–ç•¥è¿ç§»è‡³3ä¸ªä»»åŠ¡åœºæ™¯ä¸­çš„16ç§ä¸åŒå…·èº«å½¢æ€ï¼›åœ¨çœŸå®ä¸–ç•Œ6é¡¹ä»»åŠ¡ä¸­ï¼Œå®ç°äº†UR5+AG95å¤¹çˆªæœºå™¨äººä¸UR5+Xhandæœºå™¨äººé—´çš„åŒå‘è¿ç§»ï¼Œå¹³å‡è¿ç§»ç‡è¾¾åˆ°82.4%ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡æ‰€æå‡ºçš„æŠ€æœ¯æ–¹æ¡ˆè¯æ˜ï¼Œ\\CEIå¯è¿›ä¸€æ­¥æ‰©å±•è‡³ç©ºé—´æ³›åŒ–ä¸å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆé¢†åŸŸã€‚é¡¹ç›®ç½‘ç«™ï¼šhttps://cross-embodiment-interface.github.io/",
    "url": "https://arxiv.org/abs/2601.09163v1",
    "html_url": "https://arxiv.org/html/2601.09163v1",
    "html_content": "CEI\n: A Unified Interface for Cross-Embodiment Visuomotor Policy Learning in 3D Space\nTong Wu\n1\n, Shoujie Li\n12\n, Junhao Gong\n1\n, Changqing Guo\n1\n, Xingting Li\n1\n,\nShilong Mu\n3\n, and Wenbo Ding\n1\nManuscript received: September 26, 2025; Revised: December 6, 2025; Accepted: January 11, 2026.This paper was recommended for publication by Editor Aleksandra Faust upon evaluation of the Associate Editor and Reviewersâ€™ comments.\nThis work was supported by National Key R&D Program of China grant (2024YFB3816000), Guangdong Innovative and Entrepreneurial Research Team Program (2021ZT09L197), Shenzhen Science and Technology Program (JCYJ20220530143013030), Tsinghua Shenzhen International Graduate School-Shenzhen Pengrui Young Faculty Program of Shenzhen Pengrui Foundation (No. SZPR2023005) and Meituan.\nTong Wu and Shoujie Li contributed equally to this\nwork.\n(Corresponding author: Wenbo Ding, ding.wenbo@sz.tsinghua.edu.cn)\n1\nTong Wu, Shoujie Li, Junhao Gong, Changqing Guo, Xingting Li, Wenbo Ding are with Shenzhen International Graduate School, Tsinghua University, Shenzhen 518055, China. (email: {wu-t23, lsj20, gongjh24, gcq24, lixt25}@mails.tsinghua.edu.cn, ding.wenbo@sz.tsinghua.edu.cn))\n2\nShoujie Li is also with the School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore 639956, Singapore. (email: shoujie.li@ntu.edu.sg)\n3\nShilong Mu is with Xspark Ai, Shenzhen 518052, China. (email: mu.shilong@xspark-ai.com)Digital Object Identifier (DOI): see top of this page.\nAbstract\nRobotic foundation models trained on large-scale manipulation datasets have shown promise in learning generalist policies, but they often overfit to specific viewpoints, robot arms, and especially parallel-jaw grippers due to dataset biases. To address this limitation, we propose Cross-Embodiment Interface (\nCEI\n), a framework for cross-embodiment learning that enables the transfer of demonstrations across different robot arm and end-effector morphologies.\nCEI\nintroduces the concept of\nfunctional similarity\n, which is quantified using Directional Chamfer Distance. Then it aligns robot trajectories through gradient-based optimization, followed by synthesizing observations and actions for unseen robot arms and end-effectors. In experiments,\nCEI\ntransfers data and policies from a Franka Panda robot to\n16\ndifferent embodiments across\n3\ntasks in simulation, and supports bidirectional transfer between a UR5+AG95 gripper robot and a UR5+Xhand robot across\n6\nreal-world tasks, achieving an average transfer ratio of 82.4%. Finally, we demonstrate that\nCEI\ncan also be extended with spatial generalization and multimodal motion generation capabilities using our proposed techniques. Project website:\nhttps://cross-embodiment-interface.github.io/\n.\nI\nIntroduction\nEmerging robotic foundation models are built upon scaling laws\n[\n11\n]\nand fueled by the growing availability of large-scale real-world manipulation datasets\n[\n7\n,\n26\n]\n. However, these datasets often suffer from significant distributional imbalances, leading models to overfit to specific camera viewpoints and robot embodiments\n[\n4\n]\n. For instance, OXE\n[\n13\n]\naggregates data from 60 datasets spanning multiple robotic platforms, yet remains heavily skewed toward Franka and xArm robots, with nearly all end-effectors limited to parallel grippers. Such biases limit the modelsâ€™ ability to generalize\n[\n9\n]\n, especially when faced with embodiment variations.\nTo mitigate the embodiment biases, approaches such as Mirage\n[\n3\n]\nand RoVi-Aug\n[\n4\n]\nemploy techniques like cross-painting and generative models to synthesize visual observations, creating the illusion that the source robot is performing the task under test-time conditions. While these methods enable zero-shot deployment by bridging domain and embodiment gaps, their applicability is largely restricted to scenarios involving parallel-jaw grippers and Operational Space Control (OSC). These preconditions present significant challenges for transfer to more complex embodiments, such as multi-fingered dexterous hands. In fact, the limitation is further compounded by the scarcity of dexterous hand data\n[\n18\n]\n, which continues to hinder the development of generalizable policies across diverse robot embodiments.\nFigure 1:\nCross-embodiment interface.\nCEI\nenables cross-embodiment transfer between different robots by synthesizing demonstrations from a source embodiment to a target embodiment. We transfer data and policies from a Franka Panda robot to 16 target embodiments across 3 tasks in simulation, and demonstrate bidirectional transfer between a UR5+AG95 gripper and a UR5+Xhand setup across 6 real-world tasks. We also showcase\nCEI\nâ€™s compatibility with spatial generalization and multimodal motion generation.\nIntriguingly, the underlying similarity in manipulation strategies between parallel-jaw grippers and dexterous hands suggests the feasibility of such cross-embodiment learning. For instance, when humans grasp a bottle, they often adopt a gripper-like pose that naturally promotes force closure. This motivates the question of whether such shared manipulation affordances can be systematically exploited to enable effective transfer across distinct end-effector morphologies.\nTo enable policy learning across heterogeneous robot embodiments, we propose Cross-Embodiment Interface (\nCEI\n), a unified framework for cross-embodiment data synthesis.\nCEI\nleverages a novel notion of\nfunctional similarity\n, which captures shared interaction behaviors across different end-effectors, to align robot motions from a source embodiment to a target embodiment. This is accomplished by quantifying this similarity using the Directional Chamfer Distance\n[\n16\n]\nbetween embodimentsâ€™ functional representations, aligning trajectories via gradient-based optimization, and synthesizing corresponding observations and actions for the target robot. Through extensive experiments in both simulation and real world, we show that\nCEI\neffectively transfers demonstrations and policies from a source robot equipped with a parallel-jaw gripper to a target robot with a five-fingered dexterous hand and vice versa, with an overall transfer ratio of 82.4%. Furthermore, we demonstrate that\nCEI\nis compatible with spatial generalization and support multimodal data generation using our extended techniques.\nOur contributions are summarized as follows:\nâ€¢\nWe propose a novel concept of\nfunctional similarity\nbased on Directional Chamfer Distance, coupled with gradient-based trajectory alignment, to transfer task-relevant manipulation behaviors across embodiments.\nâ€¢\nWe propose a general pipeline that leverages embodiment information to augment both observations and actions, enabling the synthesis of point cloudâ€“based demonstrations across heterogeneous robots.\nâ€¢\nExperiments on 16 embodiments over 3 simulation tasks and bidirectional transfers over 6 real-world tasks demonstrate\nCEI\nâ€™s effectiveness on cross-embodiment learning. Furthermore, we showcase that\nCEI\ncan be extended with spatial generalization and multimodal motion generation for diverse data synthesis.\nII\nRelated Work\nII-A\nData Generation for Robotic Manipulation\nWhile recent imitation learning methods have exhibited impressive performance\n[\n6\n,\n24\n]\n, the high cost of collecting expert demonstrations poses significant challenges to scalability and real-world deployment. To mitigate this, online data generation approaches adapt existing demonstrations to novel object configurations and produce plausible interaction trajectories via rollouts in high-fidelity physics simulators\n[\n12\n]\n. Although these methods preserve physical realism, they tend to be computationally intensive, and challenging to apply directly in real-world settings.\nIn contrast, offline data generation synthesizes new demonstrations from existing datasets through trajectory transformations\n[\n20\n]\nor generative model-based visual augmentations\n[\n5\n]\n.\nCEI\nfocuses on cross-embodiment transfer through offline data generation, but addresses the challenge of extreme differences between end-effectors, enabling the learning of dexterous hand policies from parallel gripper data.\nII-B\nCross-embodiment Learning\nPrior work has explored bridging the embodiment gap through various strategies, including space alignment\n[\n25\n,\n22\n,\n2\n]\n, cross-painting\n[\n3\n,\n4\n]\n, dynamics modeling\n[\n10\n]\n, and reward model learning\n[\n23\n]\n. Other approaches incorporate embodiment information directly into the policy and train across a set of embodiments with varying kinematics and dynamics, demonstrating generalization to unseen morphologies within the training distribution\n[\n8\n,\n15\n,\n19\n]\n.\nRecently, many efforts have focused on collecting cross-embodiment datasets\n[\n7\n,\n13\n,\n17\n]\n, which have been shown to generalize effectively across embodiments\n[\n21\n,\n14\n]\n. In contrast to these efforts, our work addresses extreme cross-embodiment data generation, specifically demonstration transfer between a parallel gripper and a dexterous hand, without requiring embodiment-specific training data.\nIII\nProblem Formulation\nA visuomotor policy\nÏ€\n:\nğ’ª\nâ†’\nğ’œ\n\\pi:\\mathcal{O}\\rightarrow\\mathcal{A}\nmaps visual observations\no\nâˆˆ\nğ’ª\no\\in\\mathcal{O}\nto actions\na\nâˆˆ\nğ’œ\na\\in\\mathcal{A}\n. In imitation learning, such policies are typically trained from a dataset of demonstrations\nğ’Ÿ\n\\mathcal{D}\ncollected on a particular embodiment\nE\nE\n.\nWe represent each demonstration\nD\nE\n,\ns\n0\nâŠ†\nğ’Ÿ\nD_{E,s_{0}}\\subseteq\\mathcal{D}\nas a trajectory of observation-action pairs, conditioned on an initial task state\ns\n0\ns_{0}\nand robot embodiment\nE\nE\n:\nD\nE\n,\ns\n0\n=\n(\n(\no\n0\n,\na\n0\n)\n,\n(\no\n1\n,\na\n1\n)\n,\nâ€¦\n,\n(\no\nL\nâˆ’\n1\n,\na\nL\nâˆ’\n1\n)\nâˆ£\ns\n0\n,\nE\n)\n.\nD_{E,s_{0}}=\\left((o_{0},a_{0}),(o_{1},a_{1}),\\dots,(o_{L-1},a_{L-1})\\mid s_{0},E\\right).\nEach observation\no\nt\n=\n(\no\nt\npcd\n,\no\nt\narm\n,\no\nt\nee\n)\no_{t}=(o_{t}^{\\text{pcd}},o_{t}^{\\text{arm}},o_{t}^{\\text{ee}})\nconsists of a point cloud and the proprioceptive states of the robot arm and end-effector, while each action\na\nt\n=\n(\na\nt\narm\n,\na\nt\nee\n)\na_{t}=(a_{t}^{\\text{arm}},a_{t}^{\\text{ee}})\nspecifies joint position targets for robot arm and end-effector.\nOur objective is to generate a corresponding demonstration\nD\n^\nE\nâ€²\n,\ns\n0\n\\hat{D}_{E^{\\prime},s_{0}}\nfor a new target embodiment\nE\nâ€²\nE^{\\prime}\npotentially differing in morphology and kinematics, starting from the same initial task state\ns\n0\ns_{0}\n:\nD\n^\nE\nâ€²\n,\ns\n0\n=\n(\n(\no\n^\n0\n,\na\n^\n0\n)\n,\n(\no\n^\n1\n,\na\n^\n1\n)\n,\nâ€¦\n,\n(\no\n^\nL\nâˆ’\n1\n,\na\n^\nL\nâˆ’\n1\n)\nâˆ£\ns\n0\n,\nE\nâ€²\n)\n.\n\\hat{D}_{E^{\\prime},s_{0}}=\\left((\\hat{o}_{0},\\hat{a}_{0}),(\\hat{o}_{1},\\hat{a}_{1}),\\dots,(\\hat{o}_{L-1},\\hat{a}_{L-1})\\mid s_{0},E^{\\prime}\\right).\nUnlike prior work that focuses solely on the initial state or short trajectory segments, we aim to produce full demonstration trajectories that are executable on the target embodiment.\nEach embodiment\nE\nE\nincludes structural specifications such as joint limits, robot meshes and kinematic chains. We assume access to standardized robot description files (e.g., URDF and XML), which provide this information and allow embodiment-specific adaptation of actions and observations.\nIV\nMethod\nTo generate demonstrations for a new embodiment,\nCEI\nfirst defines the functional representations of both the source and target embodiments and employs the Directional Chamfer Distance to quantify the\nfunctional similarity\nbetween the two embodiments (Section\nIV-A\n). Leveraging the source demonstrations, functional representations and the metric,\nCEI\naligns the robot trajectories from the source embodiment to the target embodiment (Section\nIV-B\n). Finally,\nCEI\nsynthesizes corresponding observations and actions based on the aligned trajectories, obtaining the demonstrations for the target embodiment (Section\nIV-C\n). An overview of the full pipeline is illustrated in Fig.\n2\n.\nFigure 2:\nOverview of the pipeline. Given a source dataset, a source embodiment, and a target embodiment, we first define functional representations as sets of points with associated directions on both embodiments. We then compute functional similarity using the negative Directional Chamfer Distance between these representations. Trajectory alignment is performed by sequentially optimizing the functional similarity for each trajectory slice. Finally, we synthesize target actions with next joint positions and generate target observations by augmenting source point clouds with points sampled from the target embodiment. The viridis colormap is used to illustrate the temporal progression of the trajectory of functional representations.\nIV-A\nFunctional Similarity between Embodiments\nWhile two embodiments may differ significantly in morphology and kinematics, they can nonetheless exhibit similar object interaction behaviors. For example, in a pick-up task, a dexterous hand may grasp an object from opposing sides using the thumb and other four fingersâ€”a strategy that is functionally analogous to the symmetric grasp of a parallel gripper. In this section, we propose a method to formally quantify this\nfunctional similarity\nbetween embodiments.\nIV-A\n1\nFunctional Representation\nWe represent an embodimentâ€™s interaction feature through a set of point-direction pairs\nX\n=\n{\n(\np\ni\n,\nn\ni\n)\n}\ni\n=\n0\n,\n1\n,\nâ€¦\n,\nN\nX=\\{(p_{i},n_{i})\\}_{i=0,1,...,N}\n, referred to as the functional representation. As shown in Fig.\n2\n, the points\n{\np\ni\n}\n\\{p_{i}\\}\nlie on the embodimentâ€™s surface and reflect potential contact areas with objects. The associated directions\n{\nn\ni\n}\n\\{n_{i}\\}\nare automatically computed as the surface normal in the vicinity of\n{\np\ni\n}\n\\{p_{i}\\}\n. All point-direction pairs are transformed to the world frame via forward kinematics, ensuring alignment happens in a common frame across embodiments. While this concept is inspired by\naffordance\n[\n1\n]\nwhich captures potential interaction sites on the object, we instead focus on the embodiment, emphasizing how the end-effector interacts with the environment.\nIV-A\n2\nDirectional Chamfer Distance\nTo quantify the similarity between two functional representations, we adopt the\nDirectional Chamfer Distance\n(DCD), which jointly considers spatial proximity and directional alignment between point sets. Specifically, given functional representations\nX\n=\n{\n(\np\ni\n,\nn\ni\n)\n}\ni\n=\n1\nN\nX=\\{(p_{i},n_{i})\\}_{i=1}^{N}\nand\nX\nâ€²\n=\n{\n(\np\nj\nâ€²\n,\nn\nj\nâ€²\n)\n}\nj\n=\n1\nN\nâ€²\nX^{\\prime}=\\{(p^{\\prime}_{j},n^{\\prime}_{j})\\}_{j=1}^{N^{\\prime}}\nfor the source and target embodiments respectively, DCD is defined as:\nDCD\nâ€‹\n(\nX\n,\nX\nâ€²\n)\n\\displaystyle\\mathrm{DCD}(X,X^{\\prime})\n=\n1\nN\nâ€‹\nâˆ‘\ni\n=\n1\nN\nmin\nj\nâ¡\n(\nâ€–\np\ni\nâˆ’\np\nj\nâ€²\nâ€–\n2\nâˆ’\nÎ»\nâ‹…\nâŸ¨\nn\ni\n,\nn\nj\nâ€²\nâŸ©\n)\n\\displaystyle=\\frac{1}{N}\\sum_{i=1}^{N}\\min_{j}\\left(\\|p_{i}\\!-\\!p^{\\prime}_{j}\\|_{2}\\!-\\!\\lambda\\!\\cdot\\!\\langle n_{i},n^{\\prime}_{j}\\rangle\\right)\n(1)\n+\n1\nN\nâ€²\nâ€‹\nâˆ‘\nj\n=\n1\nN\nâ€²\nmin\ni\nâ¡\n(\nâ€–\np\nj\nâ€²\nâˆ’\np\ni\nâ€–\n2\nâˆ’\nÎ»\nâ‹…\nâŸ¨\nn\nj\nâ€²\n,\nn\ni\nâŸ©\n)\n,\n\\displaystyle+\\frac{1}{N^{\\prime}}\\sum_{j=1}^{N^{\\prime}}\\min_{i}\\left(\\|p^{\\prime}_{j}\\!-\\!p_{i}\\|_{2}\\!-\\!\\lambda\\!\\cdot\\!\\langle n^{\\prime}_{j},n_{i}\\rangle\\right),\nwhere\nÎ»\n\\lambda\nis a weighting factor balancing spatial and directional terms. This formulation ensures that each point-direction pair in one set is matched to its most similar counterpart in the other and calculates the distance of two sets. We define the negative DCD as the measure of\nfunctional similarity\n, which increases as the functional representations of two embodiments become more consistent.\nIV-B\nTrajectory Alignment\nTo bridge the embodiment gap, we leverage the defined\nfunctional similarity\nto align the joint (proprioceptive) trajectories between two robots. Given a source embodiment and an associated demonstration, we first compute the trajectory of its functional representation\n{\nX\nt\n}\nt\n=\n0\nL\nâˆ’\n1\n\\{X_{t}\\}_{t=0}^{L-1}\nusing a differentiable forward kinematics module\nFK\n, where\nX\nt\n=\nFK\nE\nâ€‹\n(\no\nt\narm\n,\no\nt\nee\n)\nX_{t}=\\text{FK}_{E}(o_{t}^{\\text{arm}},o_{t}^{\\text{ee}})\n.\nFor each frame\nt\nt\n, we initialize the target embodimentâ€™s joint configuration\nq\nt\nâ€²\nq^{\\prime}_{t}\nas a set of learnable variables and calculate the corresponding functional representation\nX\nt\nâ€²\n=\nFK\nE\nâ€²\nâ€‹\n(\nq\nt\nâ€²\n)\nX^{\\prime}_{t}=\\text{FK}_{E^{\\prime}}(q^{\\prime}_{t})\n.\nWe then optimize functional similarity between\nX\nt\nX_{t}\nand\nX\nt\nâ€²\nX^{\\prime}_{t}\nusing gradient descent:\nmin\nq\nt\nâ€²\nâ¡\nâ„’\nalign\nâ€‹\n(\nq\nt\nâ€²\n)\n=\nw\n1\nâ€‹\nDCD\nâ€‹\n(\nX\nt\n,\nX\nt\nâ€²\n)\n+\nw\n2\nâ€‹\nâ„’\nâ€‹\n(\nq\nt\nâ€²\n)\n,\n\\min_{q^{\\prime}_{t}}\\mathcal{L}_{\\text{align}}(q^{\\prime}_{t})=w_{1}\\mathrm{DCD}(X_{t},X^{\\prime}_{t})+w_{2}\\mathcal{L}(q^{\\prime}_{t}),\n(2)\nwhere\nâ„’\nâ€‹\n(\nq\nt\nâ€²\n)\n\\mathcal{L}(q^{\\prime}_{t})\npenalizes the out-of-range joint configuration, and\nw\n1\nw_{1}\n,\nw\n2\nw_{2}\nare weighting factors.\nRather than optimizing each frame independently, we treat the process as a sequential optimization problem. Specifically, the joint configuration at frame\nt\n+\n1\nt+1\nis initialized using the optimized result from frame\nt\nt\n:\nq\nt\n+\n1\nâ€²\nâ£\n(\n0\n)\nâ†\nq\n^\nt\n,\nq^{\\prime(0)}_{t+1}\\leftarrow\\hat{q}_{t},\nwhere\nq\nt\n+\n1\nâ€²\nâ£\n(\n0\n)\nq^{\\prime(0)}_{t+1}\nrepresents the initial joint configuration at\nt\n+\n1\nt+1\nand\nq\n^\nt\n\\hat{q}_{t}\ndenotes the optimized joint configuration at\nt\nt\n. This strategy not only accelerates convergence due to the small variation between adjacent frames, but also ensures temporal consistency in the generated trajectory.\nIV-C\nObservation and Action Generation\nIV-C\n1\nAction generation\nWe define the action at each timestep as the target embodimentâ€™s joint configuration at the next frame:\na\nt\n=\nq\n^\nt\n+\n1\n.\na_{t}=\\hat{q}_{t+1}.\nSince the embodiment may not achieve the target position in time, we adopt a closed-loop control strategy during execution, where the embodiment continuously applies\na\nt\na_{t}\nuntil its current joint configuration reaches the target position.\nIV-C\n2\nObservation generation\nThe proprioceptive observation of the target embodiment is directly derived from the aligned trajectory:\n(\no\n^\nt\narm\n,\no\n^\nt\nee\n)\n=\nq\n^\nt\n.\n(\\hat{o}_{t}^{\\text{arm}},\\hat{o}_{t}^{\\text{ee}})=\\hat{q}_{t}.\nTo synthesize the point cloud, we first remove points that lie outside the defined workspace, and mask any point that falls within a distance threshold\nÏ„\n\\tau\n(e.g., 5 mm) of the source robot. This proximity is calculated using the minimum Euclidean distance to a uniformly sampled point cloud derived from the robotâ€™s mesh. Next, we synthesize the point cloud for the target embodiment by first accessing target robot description file and then sampling points across the robotâ€™s mesh. Both the cropping and augmentation steps are conditioned on the current proprioceptive state\nq\n^\nt\n\\hat{q}_{t}\n. Finally we apply Farthest Point Sampling (FPS) to downsample the point cloud to a fixed number (1024).\nDuring inference, the same process is applied to ensure consistency between the synthesized observations and those encountered at test time.\nV\nExperimental Setup\nFigure 3:\nTasks and embodiments for simulation evaluation. We investigate 3 tasks and 16 embodiments, which are combinations of 4 robot arms (UR5e, IIWA, Kinova3 and Franka Panda) and 4 end-effectors (FourierRighthand, InspireRightHand, FourierLefthand and RobotiqThreeFinger gripper).\nV-A\nImplementation Details\nTo construct the robot description, we utilize the\nXML\nfiles of the robot arms and end-effectors. The points of functional representation are sampled from the finger pad meshes, and their associated directional vectors are generated using\nTorchSDF\n. For efficient trajectory alignment, we employ\npytorch_kinematics\nas a batch forward kinematics engine, enabling highly parallelized computations.\nOptimization is performed with a maximum of 300 steps at each timestep. Early stopping is triggered if the alignment loss\nâ„’\nalign\n\\mathcal{L}_{\\text{align}}\ndoes not improve for 10 consecutive steps. We empirically determine that setting the weighting factor\nÎ»\n=\n0.5\n\\lambda=0.5\n, and weights\nw\n1\n=\n1\nw_{1}=1\n,\nw\n2\n=\n1\nw_{2}=1\nyields robust performance across all tasks.\nV-B\nPolicy Training\nWe utilize 3D Diffusion Policy\n[\n24\n]\nto evaluate the synthesized data. The input consists of the current joint positions of both the robot arm and end-effector, along with a preprocessed point cloud of size [1024, 3]. The policy outputs target joint positions. We set the observation horizon to\nT\no\n=\n2\nT_{o}=2\n, action prediction horizon to\nT\np\n=\n16\nT_{p}=16\n, and action execution horizon to\nT\na\n=\n8\nT_{a}=8\n, following the settings in\n[\n24\n]\n. The model is trained for 3000 epochs using the AdamW optimizer with a learning rate of\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\nand a 500-step warmup for training stabilization. All experiments are conducted on a single RTX 4090 GPU.\nV-C\nEvaluation Setup\nV-C\n1\nSimulation\nWe evaluate cross-embodiment transfer and visuomotor policy learning in simulation using\n3\nmanipulation tasks and\n16\nrobot embodiments (combinations of\n4\narms and\n4\nend-effectors) from robosuite\n[\n27\n]\n(as shown in Fig.\n3\n). Demonstrations collected on a Franka Panda via teleoperation are transferred to target embodiments using\nCEI\n. We evaluate transferred trajectories via simulation replay and train DP3 policies on the synthesized data, measuring performance over 20 trials with three seeds.\nFigure 4:\nLeft: Setup and associated objects in real-world experiments. Right: Real-world tasks. We evaluate transfer from the AG95 gripper to the Xhand on PushCube, OpenDrawer, and PlaceBird, and from the Xhand to the AG95 gripper on PickCup, PackageBread, and InsertFlower.\nTABLE I:\nFull Evaluation across Tasks and Embodiment Combinations in Simulation.\nTasks\nOpenDrawer\nPickCube\nStackCube\nGrippers / Robot Arms\nUR5e\nIIWA\nKinova3\nPanda\nUR5e\nIIWA\nKinova3\nPanda\nUR5e\nIIWA\nKinova3\nPanda\nFourierRightHand\n100\n100\n100\n100\n100\n100\n100\n100\n81\n81\n72\n72\n69\n69\n79\n79\n53\n53\n38\n38\n37\n37\n34\n34\nInspireRightHand\n91\n91\n87\n87\n87\n87\n84\n84\n50\n50\n45\n45\n33\n33\n43\n43\n24\n24\n16\n16\n12\n12\n15\n15\nFourierLeftHand\n100\n100\n96\n96\n98\n98\n100\n100\n88\n88\n83\n83\n81\n81\n79\n79\n57\n57\n47\n47\n50\n50\n47\n47\nRobotiqThreeFinger\n78\n78\n91\n91\n93\n93\n93\n93\n19\n19\n31\n31\n41\n41\n52\n52\n0\n0\n5\n5\n9\n9\nV-C\n2\nReal world\nIn real world deployment, demonstrations are collected via keyboard teleoperation on a UR5 arm, with RGB-D data captured by a RealSense L515 to generate scene point clouds, as shown in Fig.\n4\n(left). DP3 policies are trained on\nCEI\n-generated data and evaluated over 10 trials per task. We evaluate transfer between the AG95 gripper and Xhand on\n6\nreal-world tasks (Fig.\n4\nright):\nâ€¢\nPushCube.\nA cube is placed on the table. The robot must approach and align its end-effector to securely enclose the cube, then push it 20 cm to the right.\nâ€¢\nOpenDrawer.\nA drawer is positioned on the left side of the table. The robot moves toward the handle, inserts its fingertip, and pulls it to the right by about 10 cm.\nâ€¢\nPlaceBird.\nA toy bird is positioned on the left side of the table, while a box is fixed on the right. The robot approaches, grasps the bird stably, moves it over the box, lowers it, and places the bird inside.\nâ€¢\nPickCup.\nA cup is placed near the center of the table. The robot approaches, grasps the cup, and lifts it about 5 cm from the surface.\nâ€¢\nPackageBread.\nA toaster with baked bread is placed on the right side of the table, with a bag fixed on the left. The robot approaches the toaster, picks up the bread, moves it above the bag, and deposits the bread inside.\nâ€¢\nInsertFlower.\nA bouquet of flowers is placed on the right side of the table, and a vase is fixed on the left. The robot approaches, grasps the bouquet, rotates and moves it above the vase, then inserts and releases it.\nFor the first three tasks, we collect 25 AG95 demonstrations and transfer to Xhand; for the latter three, we collect data with Xhand and transfer to AG95.\nVI\nResults\nVI-A\nSimulation Results\nVI-A\n1\nGripper-to-hands transfer\nTo evaluate whether CEI effectively bridges the extreme embodiment gap from parallel-jaw grippers to dexterous multi-fingered hands, we validate the synthesized data by replaying the robot trajectory online, initialized with the same state when collecting the data.\nTable\nI\npresents the comprehensive evaluation of 16 different embodiments. The results indicate that despite variations in kinematics and morphology,\nCEI\nis capable of bridging the cross-embodiment gap by leveraging\nfunctional similarity\n.\nWe further observe that transfer difficulty correlates with contact richness. While OpenDrawer involves simple interactions, PickCube requires stable grasp acquisition, and StackCube entails sequential contact maintenance. These contact-rich scenarios amplify the susceptibility of our geometry-based synthesis to physical disturbances (e.g., slippage), explaining the observed performance degradation.\nAnalyzing performance across end-effectors, we find that the FourierRightHand and FourierLeftHand consistently achieve the highest success rates across all three tasks. In contrast, the InspireRightHand experiences an average performance drop of approximately 40%.\nAlthough the RobotiqThreeFinger exhibits minimal loss in OpenDrawer, it struggles with StackCube as kinematic constraints force a transition from stable â€˜finger-padâ€™ grasping to a low-contact â€˜fingertipâ€™ strategy, thereby reducing manipulation reliability.\nVI-A\n2\nAblation study on cross-embodiment techniques\nTable\nII\npresents an ablation study to investigate the key components that enable\nCEI\nto achieve cross-embodiment transfer. We compare\nCEI\nagainst two baselines: (1) Binary Manual Specification (BMS) generates end-effector motions by linearly interpolating between manually specified open and close poses, while constraining the end-effector to match the gripper aperture of the source embodiment, and (2)\nCEI\nwithout Direction removes directional information and uses only positional features for functional representation. We use a subset of embodiments: Emb. 1 = UR5e + FourierRightHand, Emb. 2 = IIWA + InspireRightHand, Emb. 3 = Kinova3 + FourierLeftHand, and Emb. 4 = Panda + RobotiqThreeFinger.\nThe results show that\nCEI\nwithout Direction achieves an average success rate of only 32%, only half of\nCEI\n. Although it is still capable of completing the OpenDrawer task, it struggles with object grasping tasks, highlighting the critical role of directional information for grasp motions.\nWe also observed that BMS failed in PickCube and StackCube because linear interpolation between manually defined poses often yields failed grasps. Additionally, kinematic discrepancies (e.g., end-effector frame offsets) caused frequent failures in OpenDrawer, rendering BMS inferior to\nCEI\neven when the degree of opening were explicitly constrained to be the same.\nTABLE II:\nAblation Study on Cross-embodiment Techniques.\nTask\nMethod\nEmb. 1\nEmb. 2\nEmb. 3\nEmb. 4\nAvg.\nOpenDrawer\nBMS\n80\n80\n92\n92\n87\n87\n64\n64\n81\n81\nCEI\nw/o Dir.\n84\n84\nğŸğŸğŸ\n\\boldsymbol{100}\n91\n91\nğŸğŸğŸ\n\\boldsymbol{100}\n94\n94\nCEI\n(Ours)\nğŸğŸğŸ\n\\boldsymbol{100}\n87\n87\nğŸ—ğŸ–\n\\boldsymbol{98}\n93\n93\nğŸ—ğŸ“\n\\boldsymbol{95}\nPickCube\nBMS\n0\n0\n0\n0\n0\nCEI\nw/o Dir.\n7\n7\n2\n2\n0\n0\n2\n2\nCEI\n(Ours)\nğŸ–ğŸ\n\\boldsymbol{81}\nğŸ’ğŸ“\n\\boldsymbol{45}\nğŸ–ğŸ\n\\boldsymbol{81}\nğŸ“ğŸ\n\\boldsymbol{52}\nğŸ”ğŸ“\n\\boldsymbol{65}\nStackCube\nBMS\n0\n0\n0\n0\n0\nCEI\nw/o Dir.\n0\n0\n0\n0\n0\nCEI\n(Ours)\nğŸ“ğŸ‘\n\\boldsymbol{53}\nğŸğŸ”\n\\boldsymbol{16}\nğŸ“ğŸ\n\\boldsymbol{50}\nğŸ—\n\\boldsymbol{9}\nğŸ‘ğŸ\n\\boldsymbol{32}\nVI-A\n3\nSensitivity of functional representation\nTo evaluate the impact of different functional representation selections on transfer performance, we compare three variants: (1) Standard, which spans the entire finger pad to provide full coverage and is used across main experiments; (2) Reduced, which is limited to a subset of points near the center of the finger pad; and (3) Randomly Dropped (Rand. Drop.), where a subset of points is randomly removed from the full representation. We transfer 20 demonstrations with above three selections in the PickCube task using FourierRightHand and evaluate the success rate. As shown in Table\nIII\n, although the three selections differ, their success rates remain comparable, suggesting that\nCEI\nis robust to such variations and exhibits low sensitivity to the choice of functional representation.\nTABLE III:\nSensitivity Analysis of Functional Representations.\nFunctional\nRepresentation\nStandard\nReduced\nRand. Drop.\nSuccess Rate\n17\n17\n/\n20\n20\n18\n18\n/\n20\n20\n16\n16\n/\n20\n20\nFigure 5:\nQualitative evaluation. Left: Transfer from AG95 to Xhand in PushCube, OpenDrawer, and PlaceBird. Right: Transfer from Xhand to AG95 in PickCup, PackageBread, and InsertFlower. Manipulations of source policies are shown on the top rows, transferred ones on the bottom rows.\nVI-A\n4\nPolicy evaluation\nTable\nIV\npresents the policy evaluation results using synthesized cross-embodiment data. We observe that\nCEI\nachieves an average success rate of 62%, suggesting that the synthesized data effectively captures critical behaviors such as reaching the target position, grasping, and releasing, even without additional refinement or selection. We further compare\nCEI\nagainst two baselines: (1) No Augmentation, where the policy is trained solely on observations from source demonstrations, and (2)\nCEI\nwithout Inference Augmentation, where the policy uses raw observations during inference without additional augmentation. Policies trained without any augmentation fail to complete the tasks, demonstrating the necessity of targeted data augmentation for cross-embodiment generalization. Additionally, removing Inference Augmentation results in a 22% drop in success rate.\nThis performance degradation arises due to the partial and noisy observations during inference.\nTABLE IV:\nPolicy Evaluation on Synthesized Data Generated by\nCEI\n.\nTask\nMethod\nEmb. 1\nEmb. 2\nEmb. 3\nEmb. 4\nAvg.\nOpenDrawer\nNo Aug.\n0\n0\n0\n0\n0\nCEI\nw/o Inf. Aug.\nğŸğŸğŸ\n\\boldsymbol{100}\n79\n79\n32\n32\n64\n64\n69\n69\nCEI\n(Ours)\nğŸğŸğŸ\n\\boldsymbol{100}\nğŸ—ğŸ\n\\boldsymbol{90}\nğŸğŸğŸ\n\\boldsymbol{100}\nğŸ—ğŸ\n\\boldsymbol{90}\nğŸ—ğŸ“\n\\boldsymbol{95}\nPickCube\nNo Aug.\n0\n0\n0\n0\n0\nCEI\nw/o Inf. Aug.\n42\n42\n26\n26\n38\n38\n10\n10\n29\n29\nCEI\n(Ours)\nğŸ•ğŸ—\n\\boldsymbol{79}\nğŸ”ğŸ–\n\\boldsymbol{68}\nğŸ”ğŸ–\n\\boldsymbol{68}\nğŸğŸ‘\n\\boldsymbol{23}\nğŸ”ğŸ\n\\boldsymbol{60}\nStackCube\nNo Aug.\n0\n0\n0\n0\n0\nCEI\nw/o Inf. Aug.\n36\n36\n10\n10\n44\n44\n0\n23\n23\nCEI\n(Ours)\nğŸ“ğŸ“\n\\boldsymbol{55}\nğŸğŸ”\n\\boldsymbol{16}\nğŸ“ğŸ\n\\boldsymbol{52}\n0\nğŸ‘ğŸ\n\\boldsymbol{31}\nSource (ref.)\n100 / 100 / 97\n99\nVI-B\nReal-world Results\nVI-B\n1\nBidirectional transfer\nTable\nV\ndemonstrates the bidirectional transfer capabilities of\nCEI\non real-world tasks. We compare policies trained on synthesized data and deployed on the target embodiment against those trained on source data and deployed on the source embodiment. Results show that for simple tasks such as PushCube,\nCEI\nenables transfer from a parallel-jaw gripper to a dexterous hand without performance loss. Failures in tasks like OpenDrawer and PlaceBird are primarily due to challenges in dexterous contacts, such as fingers slipping off drawer handles. In transfers from a dexterous hand to a parallel gripper,\nCEI\nachieves similar performance.\nHowever, InsertFlower remains exceptionally challenging since the thin geometry of the flower stem makes it prone to slippage, leading to low success rates for both source and target embodiments.\nOverall,\nCEI\nachieves an average success rate of 70% across six tasks, with a transfer ratio (success rate of\nCEI\ndivided by that of the source embodiment) of 82.4%. Qualitative evaluation across 6 tasks is shown in Fig.\n5\n.\nTABLE V:\nReal-world Evaluation.\nAG95\nâ†’\n\\rightarrow\nXhand\nMethod\nPushCube\nOpenDrawer\nPlaceBird\nAverage\nCEI\n(Ours)\n10\n/\n10\n10/10\n8\n/\n10\n8/10\n7\n/\n10\n7/10\n8.3\n/\n10\n8.3/10\nSource\n10\n/\n10\n10/10\n10\n/\n10\n10/10\n10\n/\n10\n10/10\n10\n/\n10\n10/10\nXhand\nâ†’\n\\rightarrow\nAG95\nMethod\nPickCup\nPackageBread\nInsertFlower\nAverage\nCEI\n(Ours)\n6\n/\n10\n6/10\n9\n/\n10\n9/10\n2\n/\n10\n2/10\n5.7\n/\n10\n5.7/10\nSource\n8\n/\n10\n8/10\n9\n/\n10\n9/10\n4\n/\n10\n4/10\n7\n/\n10\n7/10\nFigure 6:\nSpatial generalization. Left: Experimental setup and task configuration, where each anchor is spaced 20â€‰cm apart. Around each anchor, we sample a\n10\nÃ—\n10\n10\\times 10\ngrid within the range\n[\nâˆ’\n8\nâ€‹\ncm\n,\n8\nâ€‹\ncm\n]\n[-8\\,\\text{cm},8\\,\\text{cm}]\nfor data generation. Middle: Our approach enables the Xhand to press the button across most of the table surface. Right: Evaluation results on the 10 anchors. Each anchor is evaluated over 5 trials, with the results distinguished by different colors.\nVI-B\n2\nTime cost of transfer\nTo study how efficient the data generation process is, we evaluate the efficiency of\nCEI\nâ€™s data generation by measuring the time required to synthesize 100, 200, 300 and 400 demonstrations. Given the substantial cost of collecting large-scale real-world demonstrations, we construct larger datasets by replicating a set of 25 collected trajectories through duplication at ratios of 4\nÃ—\n\\times\n, 8\nÃ—\n\\times\n, 12\nÃ—\n\\times\n, and 16\nÃ—\n\\times\n, respectively. We compare\nCEI\nwith MimicGen\n[\n12\n]\nand DemoGen\n[\n20\n]\non datasets with an average episode length of 105 steps. For MimicGen, we estimate time by multiplying the replay duration of each source trajectory by the number of generated demonstrations, and adding 20 seconds per trajectory for manual object resets, following\n[\n20\n]\n. Table\nVI\npresents the time required to generate those numbers of demonstrations. Results show that\nCEI\nrequires significantly less time than MimicGen, which highly depends on online execution. DemoGen generates hundreds of demonstrations in one second, while\nCEI\nrequires several minutes since it utilizes gradient-based optimization.\nTABLE VI:\nTime Cost for Generating Real-world Demonstrations.\n100 Demos\n200 Demos\n300 Demos\n400 Demos\nMimicGen\n6.4\n6.4\nh\n12.8\n12.8\nh\n19.2\n19.2\nh\n25.6\n25.6\nh\nDemoGen\n0.29\n0.29\ns\n0.54\n0.54\ns\n0.84\n0.84\ns\n1.14\n1.14\ns\nCEI\n(Ours)\n2.5\n2.5\nmin\n2.9\n2.9\nmin\n3.3\n3.3\nmin\n3.6\n3.6\nmin\nVII\nBroader Applications\nVII-A\nSpatial Generalization\nVII-A\n1\nSpatial augmentation\nCEI\ninherently facilitates spatial generalization, enabling the generation of diverse demonstrations across the entire workspace from only a single collected trajectory. Given a spatial transform\nğ’¯\ni\n\\mathcal{T}_{i}\nwhich encapsulates the possible translational or rotational offset of the objects, we first apply it to the functional representation trajectory:\nX\n~\nt\n=\nX\nt\n+\nâ„’\nâ€‹\n(\nt\n)\nâ€‹\n(\nğ’¯\ni\nâ€‹\n(\nX\nt\n)\nâˆ’\nX\nt\n)\n,\nt\n=\n0\n,\n1\n,\nâ€¦\n,\nL\nâˆ’\n1\n,\n\\tilde{X}_{t}=X_{t}+\\mathcal{L}(t)(\\mathcal{T}_{i}(X_{t})-X_{t}),\\quad t=0,1,\\ldots,L-1,\n(3)\nwhere\nâ„’\nâ€‹\n(\nt\n)\n=\nmin\nâ¡\n(\nt\n0.8\nâ€‹\nL\n,\n1\n)\n\\mathcal{L}(t)=\\min(\\frac{t}{0.8L},1)\nis the clipped linear growth, ensuring that the generated demonstrations share an initial state while diverging to different terminal states by interpolation. The target embodiment is subsequently aligned to the augmented trajectory\n{\nX\n~\nt\n}\nt\n=\n0\nL\nâˆ’\n1\n\\{\\tilde{X}_{t}\\}_{t=0}^{L-1}\nthrough the standard\nCEI\noptimization procedure.\nThe augmented point cloud is then obtained by applying\nğ’¯\ni\n\\mathcal{T}_{i}\nto the object point cloud and synthesizing the robot point cloud according to the augmented trajectory.\nVII-A\n2\nPress the button anywhere on the table\nWe assess the spatial generalization of\nCEI\nin the PressButton task. Starting from a single demonstration collected with AG95, we generate 1,000 demonstrations across 10 anchors, where each anchor samples a\n10\nÃ—\n10\n10\\times 10\ngrid within the range\n[\nâˆ’\n8\nâ€‹\ncm\n,\n8\nâ€‹\ncm\n]\n[-8\\,\\text{cm},8\\,\\text{cm}]\n(Fig.\n6\n, left). We then train DP3 and evaluate it on each anchor position for 5 trials. As illustrated in Fig.\n6\n(middle and right), our approach extends the policy to press the button over a wide area of the table, rather than being limited to the original position. Moreover, we observe that performance on the left side is better than on the right, likely because the camera is positioned to the right of center and oriented toward the left.\nVII-B\nMultimodal Motion Generation\nFigure 7:\nMultimodality of\nCEI\n.\nCEI\ngenerates two different manipulation motions in OpenDrawer task from the same demonstration.\nIn previous experiments, we observed that varying the initial joint configuration produces different aligned trajectories. For example, when transferring a grasp motion from a gripper to a dexterous hand, the resulting pose of the dexterous hand remains valid even if rotated by\n180\nâˆ˜\n180^{\\circ}\naround the heading direction. To exploit such multimodality, we introduce an initialization strategy designed for enhancing data diversity.\nVII-B\n1\nElite-based initialization strategy (EIS)\nInstead of relying on random joint configurations, we uniformly sample candidate configurations from the joint space and rank them according to their\nfunctional similarity\nwith the functional representation of the source embodimentâ€™s initial configuration. The top 10% of these candidates are then selected, and their mean configuration is used as the initialization. Subsequently, we proceed with the original process of\nCEI\nto obtain the aligned trajectory and synthesize corresponding demonstrations.\nVII-B\n2\nBimodal motion in OpenDrawer\nWe evaluate the extension in OpenDrawer task. As shown in Fig.\n7\n,\nCEI\ngenerates 2 different patterns that successfully open the drawer. Since we only manipulate the initialization and\nCEI\nproceeds the trajectory sequentially, the aligned trajectories remain temporally consistent while allowing for diverse motion patterns. We further train DP3 on 25 demonstrations with either an arbitrary motion pattern or a 1:1 mixture of patterns. As reported in Table\nVII\n, the multimodality has no adverse impact on task success.\nTABLE VII:\nSuccess Rates of Different Training Recipes.\nPattern 1\nPattern 2\nMixture of Pattern 1&2\nSuccess Rate\n8\n/\n10\n8/10\n7\n/\n10\n7/10\n7\n/\n10\n7/10\nVIII\nConclusion and Discussion\nIn this letter, we introduce\nCEI\n, a cross-embodiment framework that leverages\nfunctional similarity\nand automated data synthesis to transfer policies across diverse robots in both simulation and the real world. Beyond standard manipulation,\nCEI\nsupports spatial generalization and multimodal motion generation, providing a versatile foundation for scalable robot learning. While our current scope is point cloud-based cross-embodiment learning, preliminary results suggest our work also holds potential for RGB-based observations, as shown on our project website\nhttps://cross-embodiment-interface.github.io/\n. Although\nCEI\ncan synthesize hundreds of demonstrations in parallel within minutes, applying it to large-scale datasets remains an open direction that could further advance generalist policy learning. In addition, the current reliance on visual-kinematic inputs limits the detection of unstable contacts, such as the slippage seen in the InsertFlower task. Integrating tactile sensing would allow the policy to adapt to these physical disturbances in real-time, representing a key area for future improvement.\nReferences\n[1]\nS. Bahl, R. Mendonca, L. Chen, U. Jain, and D. Pathak\n(2023)\nAffordances from human videos as a versatile representation for robotics\n.\nIn\nProc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\nÂ§\nIV-A\n1\n.\n[2]\nE. Bauer, E. Nava, and R. K. Katzschmann\n(2025)\nLatent action diffusion for cross-embodiment manipulation\n.\nArXiv\n.\nCited by:\nÂ§\nII-B\n.\n[3]\nL. Y. Chen, K. Dharmarajan, K. Hari, C. Xu, Q. Vuong, and K. Goldberg\n(2024)\nMIRAGE: cross-embodiment zero-shot policy transfer with cross-painting\n.\nIn\nProc. Robotics: Science and Systems (RSS)\n,\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[4]\nL. Y. Chen, C. Xu, K. Dharmarajan, R. Cheng, K. Keutzer, M. Tomizuka, Q. Vuong, and K. Goldberg\n(2025)\nRoVi-Aug: robot and viewpoint augmentation for cross-embodiment robot learning\n.\nIn\nProc. Conference on Robot Learning (CoRL)\n,\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-B\n.\n[5]\nQ. Chen, S. C. Kiami, A. Gupta, and V. Kumar\n(2023)\nGenAug: retargeting behaviors to unseen situations via generative augmentation\n.\nIn\nProc. Robotics: Science and Systems (RSS)\n,\nCited by:\nÂ§\nII-A\n.\n[6]\nC. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. C. Burchfiel, and S. Song\n(2023)\nDiffusion Policy: visuomotor policy learning via action diffusion\n.\nIn\nProc. Robotics: Science and Systems (RSS)\n,\nCited by:\nÂ§\nII-A\n.\n[7]\nF. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, K. Daniilidis, C. Finn, and S. Levine\n(2022)\nBridge Data: boosting generalization of robotic skills with cross-domain datasets\n.\nIn\nProc. Robotics: Science and Systems (RSS)\n,\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[8]\nH. Furuta, Y. Iwasawa, Y. Matsuo, and S. S. Gu\n(2023)\nA system for morphology-task generalization via unified representation and behavior distillation\n.\nIn\nProc. International Conference on Learning Representations (ICLR)\n,\nCited by:\nÂ§\nII-B\n.\n[9]\nJ. Gao, A. Xie, T. Xiao, C. Finn, and D. Sadigh\n(2024)\nEfficient data collection for robotic manipulation via compositional generalization\n.\nIn\nProc. Robotics: Science and Systems (RSS)\n,\nCited by:\nÂ§I\n.\n[10]\nE. S. Hu, K. Huang, O. Rybkin, and D. Jayaraman\n(2022)\nKnow Thyself: transferable visual control policies through robot-awareness\n.\nIn\nProc. International Conference on Learning Representations (ICLR)\n,\nCited by:\nÂ§\nII-B\n.\n[11]\nF. Lin, Y. Hu, P. Sheng, C. Wen, J. You, and Y. Gao\n(2024)\nData scaling laws in imitation learning for robotic manipulation\n.\nIn\nProc. International Conference on Learning Representations (ICLR)\n,\nCited by:\nÂ§I\n.\n[12]\nA. Mandlekar, S. Nasiriany, B. Wen, I. Akinola, Y. Narang, L. Fan, Y. Zhu, and D. Fox\n(2023)\nMimicGen: a data generation system for scalable robot learning using human demonstrations\n.\nIn\nProc. Conference on Robot Learning (CoRL)\n,\nCited by:\nÂ§\nII-A\n,\nÂ§\nVI-B\n2\n.\n[13]\nA. Oâ€™Neill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain,\net al.\n(2024)\nOpen X-embodiment: robotic learning datasets and RT-X models\n.\nIn\nProc. the IEEE International Conference on Robotics and Automation (ICRA)\n,\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[14]\nOcto Model Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, C. Xu, J. Luo, T. Kreiman, Y. L. Tan, L. Y. Chen, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh, C. Finn, and S. Levine\n(2024)\nOcto: an open-source generalist robot policy\n.\nIn\nProc. Robotics: Science and Systems (RSS)\n,\nCited by:\nÂ§\nII-B\n.\n[15]\nL. Shao, F. Ferreira, M. Jorda, V. Nambiar, J. Luo, E. Solowjow, J. A. Ojea, O. Khatib, and J. Bohg\n(2020)\nUniGrasp: learning a unified model to grasp with multifingered robotic hands\n.\nIEEE Robotics and Automation Letters\n5\n(\n2\n),\npp.Â 2286â€“2293\n.\nCited by:\nÂ§\nII-B\n.\n[16]\nH. Shi, H. Xu, Z. Huang, Y. Li, and J. Wu\n(2024)\nRoboCraft: learning to see, simulate, and shape elasto-plastic objects in 3D with graph networks\n.\nThe International Journal of Robotics Research\n43\n(\n4\n),\npp.Â 533â€“549\n.\nCited by:\nÂ§I\n.\n[17]\nH. R. Walke, K. Black, T. Z. Zhao, Q. Vuong, C. Zheng, P. Hansen-Estruch, A. W. He, V. Myers, M. J. Kim, M. Du, A. Lee, K. Fang, C. Finn, and S. Levine\n(2023)\nBridgeData V2: a dataset for robot learning at scale\n.\nIn\nProc. Conference on Robot Learning (CoRL)\n,\nCited by:\nÂ§\nII-B\n.\n[18]\nC. Wang, H. Shi, W. Wang, R. Zhang, F. Li, and K. Liu\n(2024)\nDexCap: scalable and portable mocap data collection system for dexterous manipulation\n.\nIn\nProc. Robotics: Science and Systems (RSS)\n,\nCited by:\nÂ§I\n.\n[19]\nZ. Xu, B. Qi, S. Agrawal, and S. Song\n(2021)\nAdaGrasp: learning an adaptive gripper-aware grasping policy\n.\nIn\nProc. IEEE International Conference on Robotics and Automation (ICRA)\n,\nCited by:\nÂ§\nII-B\n.\n[20]\nZ. Xue, S. Deng, Z. Chen, Y. Wang, Z. Yuan, and H. Xu\n(2025)\nDemoGen: synthetic demonstration generation for data-efficient visuomotor policy learning\n.\nIn\nProc. Robotics: Science and Systems (RSS)\n,\nCited by:\nÂ§\nII-A\n,\nÂ§\nVI-B\n2\n.\n[21]\nJ. H. Yang, C. Glossop, A. Bhorkar, D. Shah, Q. Vuong, C. Finn, D. Sadigh, and S. Levine\n(2024)\nPushing the limits of cross-embodiment learning for manipulation and navigation\n.\nIn\nProc. Robotics: Science and Systems (RSS)\n,\nCited by:\nÂ§\nII-B\n.\n[22]\nH. Yuan, B. Zhou, Y. Fu, and Z. Lu\n(2025)\nCross-embodiment dexterous grasping with reinforcement learning\n.\nIn\nProc. International Conference on Learning Representations (ICLR)\n,\nCited by:\nÂ§\nII-B\n.\n[23]\nK. Zakka, A. Zeng, P. Florence, J. Tompson, J. Bohg, and D. Dwibedi\n(2022)\nXIRL: cross-embodiment inverse reinforcement learning\n.\nIn\nProc. Conference on Robot Learning (CoRL)\n,\nCited by:\nÂ§\nII-B\n.\n[24]\nY. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, and H. Xu\n(2024)\n3D Diffusion Policy: generalizable visuomotor policy learning via simple 3D representations\n.\nIn\nProc. Robotics: Science and Systems (RSS)\n,\nCited by:\nÂ§\nII-A\n,\nÂ§\nV-B\n.\n[25]\nQ. Zhang, T. Xiao, A. A. Efros, L. Pinto, and X. Wang\n(2021)\nLearning cross-domain correspondence for control with dynamics cycle-consistency\n.\nIn\nProc. International Conference on Learning Representations (ICLR)\n,\nCited by:\nÂ§\nII-B\n.\n[26]\nT. Z. Zhao, J. Tompson, D. Driess, P. Florence, S. K. S. Ghasemipour, C. Finn, and A. Wahid\n(2024)\nALOHA Unleashed: a simple recipe for robot dexterity\n.\nIn\nProc. Conference on Robot Learning (CoRL)\n,\nCited by:\nÂ§I\n.\n[27]\nY. Zhu, J. Wong, A. Mandlekar, R. MartÃ­n-MartÃ­n, A. Joshi, S. Nasiriany, and Y. Zhu\n(2020)\nRobosuite: a modular simulation framework and benchmark for robot learning\n.\narXiv preprint arXiv:2009.12293\n.\nCited by:\nÂ§\nV-C\n1\n.",
    "preview_text": "Robotic foundation models trained on large-scale manipulation datasets have shown promise in learning generalist policies, but they often overfit to specific viewpoints, robot arms, and especially parallel-jaw grippers due to dataset biases. To address this limitation, we propose Cross-Embodiment Interface (\\CEI), a framework for cross-embodiment learning that enables the transfer of demonstrations across different robot arm and end-effector morphologies. \\CEI introduces the concept of \\textit{functional similarity}, which is quantified using Directional Chamfer Distance. Then it aligns robot trajectories through gradient-based optimization, followed by synthesizing observations and actions for unseen robot arms and end-effectors. In experiments, \\CEI transfers data and policies from a Franka Panda robot to \\textbf{16} different embodiments across \\textbf{3} tasks in simulation, and supports bidirectional transfer between a UR5+AG95 gripper robot and a UR5+Xhand robot across \\textbf{6} real-world tasks, achieving an average transfer ratio of 82.4\\%. Finally, we demonstrate that \\CEI can also be extended with spatial generalization and multimodal motion generation capabilities using our proposed techniques. Project website: https://cross-embodiment-interface.github.io/\n\nCEI\n: A Unified Interface for Cross-Embodiment Visuomotor Policy Learning in 3D Space\nTong Wu\n1\n, Shoujie Li\n12\n, Junhao Gong\n1\n, Changqing Guo\n1\n, Xingting Li\n1\n,\nShilong Mu\n3\n, and Wenbo Ding\n1\nManuscript received: September 26, 2025; Revised: December 6, 2025; Accepted: January 11, 2026.This paper was recommended for publication by Editor Aleksandra Faust upon evaluation of the Associate Editor and Reviewersâ€™ comments.\nThis work was supported by National Key R&D Program of China grant (2024YFB3816000), Guangdong Innovative and Entrepreneurial Research Team Program (2021ZT09L197), Shenzhen Science and Technology Program (JCYJ20220530143013030), Tsinghua Shenzhen International Graduate School-Shenzhe",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "CEIæ˜¯ä¸€ä¸ªè·¨å…·èº«å­¦ä¹ çš„ç»Ÿä¸€æ¥å£æ¡†æ¶ï¼Œé€šè¿‡åŠŸèƒ½ç›¸ä¼¼æ€§é‡åŒ–å’Œè½¨è¿¹å¯¹é½ï¼Œå®ç°ä¸åŒæœºå™¨äººå½¢æ€é—´çš„ç­–ç•¥è¿ç§»ï¼Œå¹¶æ”¯æŒç©ºé—´æ³›åŒ–å’Œå¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šCEI: ç”¨äºä¸‰ç»´ç©ºé—´è·¨å…·èº«è§†è§‰è¿åŠ¨ç­–ç•¥å­¦ä¹ çš„ç»Ÿä¸€æ¥å£\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nå½“å‰åŸºäºå¤§è§„æ¨¡æ“ä½œæ•°æ®é›†è®­ç»ƒçš„æœºå™¨äººåŸºç¡€æ¨¡å‹ï¼Œè™½å±•ç°å‡ºå­¦ä¹ é€šç”¨ç­–ç•¥çš„æ½œåŠ›ï¼Œä½†å¸¸å› æ•°æ®åˆ†å¸ƒåå·®è€Œè¿‡æ‹Ÿåˆäºç‰¹å®šè§†è§’ã€æœºæ¢°è‡‚åŠï¼ˆå°¤å…¶æ˜¯ï¼‰å¹³è¡Œå¤¹çˆªã€‚è¿™ä¸¥é‡é™åˆ¶äº†æ¨¡å‹åœ¨é¢å¯¹ä¸åŒå½¢æ€æœºå™¨äººï¼ˆå¦‚ä»å¤¹çˆªåˆ°çµå·§æ‰‹ï¼‰æ—¶çš„æ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ï¼ˆå¦‚è§†è§‰åˆæˆï¼‰å¤šå±€é™äºå¹³è¡Œå¤¹çˆªï¼Œéš¾ä»¥åº”å¯¹æç«¯å½¢æ€å·®å¼‚ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§èƒ½å¤Ÿè·¨è¶Šä¸åŒæœºå™¨äººå½¢æ€è¿›è¡Œæ¼”ç¤ºæ•°æ®ä¸ç­–ç•¥è¿ç§»çš„ç»Ÿä¸€æ¡†æ¶ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡º**è·¨å…·èº«æ¥å£ï¼ˆCEIï¼‰**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š\n- **åŠŸèƒ½ç›¸ä¼¼æ€§**ï¼šæå‡ºæ–°æ¦‚å¿µï¼Œé€šè¿‡**å®šå‘å€’è§’è·ç¦»ï¼ˆDCDï¼‰**é‡åŒ–ä¸åŒæœ«ç«¯æ‰§è¡Œå™¨åœ¨ç‰©ä½“äº¤äº’è¡Œä¸ºä¸Šçš„ç›¸ä¼¼æ€§ï¼ŒåŒæ—¶è€ƒè™‘ç©ºé—´é‚»è¿‘ä¸æ–¹å‘å¯¹é½ã€‚\n- **æ¢¯åº¦è½¨è¿¹å¯¹é½**ï¼šåŸºäºåŠŸèƒ½ç›¸ä¼¼æ€§ï¼Œé€šè¿‡å¯å¾®å‰å‘è¿åŠ¨å­¦å’Œæ¢¯åº¦ä¼˜åŒ–ï¼Œå°†æºæœºå™¨äººçš„å…³èŠ‚è½¨è¿¹åºåˆ—å¼åœ°å¯¹é½åˆ°ç›®æ ‡æœºå™¨äººï¼Œç¡®ä¿æ—¶åºä¸€è‡´æ€§ã€‚\n- **ç»Ÿä¸€æ•°æ®åˆæˆ**ï¼šåˆ©ç”¨å¯¹é½åçš„è½¨è¿¹ï¼ŒåŒæ­¥åˆæˆç›®æ ‡æœºå™¨äººçš„ç‚¹äº‘è§‚æµ‹ï¼ˆé€šè¿‡æ©è”½ä¸å¢å¼ºï¼‰å’Œå…³èŠ‚åŠ¨ä½œï¼Œç”Ÿæˆå®Œæ•´çš„è·¨å…·èº«æ¼”ç¤ºæ•°æ®ã€‚\n- **æ‰©å±•åº”ç”¨**ï¼šæ¡†æ¶å¯è¿›ä¸€æ­¥æ”¯æŒ**ç©ºé—´æ³›åŒ–**ï¼ˆä»å•æ¡è½¨è¿¹ç”Ÿæˆæ•´ä¸ªå·¥ä½œç©ºé—´çš„æ¼”ç¤ºï¼‰ä¸**å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆ**ï¼ˆé€šè¿‡ç²¾è‹±åˆå§‹åŒ–ç­–ç•¥äº§ç”Ÿå¤šæ ·åŒ–åŠ¨ä½œæ¨¡å¼ï¼‰ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\n- **ä»¿çœŸå®éªŒ**ï¼šå°†Franka Pandaçš„æ¼”ç¤ºè¿ç§»è‡³**16ç§**ä¸åŒæœºæ¢°è‡‚ä¸æœ«ç«¯æ‰§è¡Œå™¨ç»„åˆï¼ˆåŒ…æ‹¬çµå·§æ‰‹ï¼‰ï¼Œåœ¨å¼€æŠ½å±‰ã€æŠ“ç«‹æ–¹ä½“ã€å †å ç«‹æ–¹ä½“ä¸‰ä¸ªä»»åŠ¡ä¸ŠéªŒè¯äº†æœ‰æ•ˆæ€§ã€‚ä»»åŠ¡æˆåŠŸç‡éšæ¥è§¦å¤æ‚åº¦å¢åŠ è€Œä¸‹é™ï¼Œä½†å…³é”®ä»»åŠ¡ï¼ˆå¦‚æŠ“å–ï¼‰å¹³å‡æˆåŠŸç‡ä»è¾¾**65%**ã€‚æ¶ˆèå®éªŒè¯å®äº†æ–¹å‘ä¿¡æ¯ä¸åºåˆ—ä¼˜åŒ–çš„é‡è¦æ€§ã€‚\n- **çœŸå®ä¸–ç•Œå®éªŒ**ï¼šåœ¨UR5æœºæ¢°è‡‚ä¸Šï¼Œå®ç°äº†AG95å¹³è¡Œå¤¹çˆªä¸Xhandäº”æŒ‡çµå·§æ‰‹ä¹‹é—´çš„**åŒå‘ç­–ç•¥è¿ç§»**ï¼Œæ¶µç›–æ¨ç«‹æ–¹ä½“ã€å¼€æŠ½å±‰ã€æ”¾ç½®é¸Ÿç©å…·ç­‰6ä¸ªä»»åŠ¡ï¼Œå¹³å‡æˆåŠŸç‡è¾¾**70%**ï¼Œè¿ç§»æ¯”ï¼ˆç›®æ ‡æˆåŠŸç‡/æºæˆåŠŸç‡ï¼‰ä¸º**82.4%**ã€‚\n- **æ•ˆç‡**ï¼šç”Ÿæˆæ•°ç™¾æ¡æ¼”ç¤ºä»…éœ€æ•°åˆ†é’Ÿï¼Œè¿œå¿«äºä¾èµ–åœ¨çº¿ç‰©ç†ä»¿çœŸçš„æ–¹æ³•ã€‚\n- **ç­–ç•¥å­¦ä¹ **ï¼šä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒçš„3D Diffusion Policyåœ¨ä»¿çœŸä»»åŠ¡ä¸­å¹³å‡æˆåŠŸç‡è¾¾**62%**ï¼Œæ˜¾è‘—ä¼˜äºæ— æ•°æ®å¢å¼ºçš„åŸºçº¿ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\nCEIä¸ºè§£å†³æœºå™¨äººå­¦ä¹ ä¸­çš„**å…·èº«åå·®**é—®é¢˜æä¾›äº†æ–°é¢–ä¸”æœ‰æ•ˆçš„æ–¹æ¡ˆã€‚å…¶ä»·å€¼åœ¨äºï¼š\n- **æ¨åŠ¨é€šç”¨æœºå™¨äººå­¦ä¹ **ï¼šä½¿å¾—åˆ©ç”¨ä¸°å¯Œçš„å¤¹çˆªæ•°æ®æ¥è®­ç»ƒçµå·§æ‰‹ç­–ç•¥æˆä¸ºå¯èƒ½ï¼Œç¼“è§£äº†çµå·§æ‰‹æ•°æ®ç¨€ç¼ºçš„ç“¶é¢ˆï¼Œæœ‰åŠ©äºæ„å»ºæ›´å…·æ³›åŒ–èƒ½åŠ›çš„æœºå™¨äººåŸºç¡€æ¨¡å‹ã€‚\n- **æå‡æ•°æ®åˆ©ç”¨æ•ˆç‡**ï¼šæä¾›äº†ä¸€ç§é«˜æ•ˆçš„ç¦»çº¿æ•°æ®ç”Ÿæˆä¸å¢å¼ºç®¡é“ï¼Œèƒ½æ˜¾è‘—é™ä½é’ˆå¯¹æ–°æœºå™¨äººå¹³å°æ”¶é›†æ¼”ç¤ºæ•°æ®çš„æˆæœ¬ã€‚\n- **æ–¹æ³•é€šç”¨æ€§**ï¼šæ¡†æ¶ä¸ä¾èµ–äºç‰¹å®šä»»åŠ¡æˆ–æ§åˆ¶å™¨ï¼ŒåŸºäºç‚¹äº‘å’Œè¿åŠ¨å­¦ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰è§†è§‰è¿åŠ¨ç­–ç•¥å­¦ä¹ æµç¨‹ä¸­ï¼Œå¹¶ä¸ºç©ºé—´æ³›åŒ–ã€å¤šæ¨¡æ€ç”Ÿæˆç­‰æ›´å¹¿æ³›çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šCEI\n\n### **ä¸€ã€ ç ”ç©¶èƒŒæ™¯ä¸æ ¸å¿ƒé—®é¢˜**\nå½“å‰åŸºäºå¤§è§„æ¨¡æ•°æ®è®­ç»ƒçš„æœºå™¨äººåŸºç¡€æ¨¡å‹å­˜åœ¨ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼š**æ•°æ®åå·®å¯¼è‡´çš„â€œå…·èº«è¿‡æ‹Ÿåˆâ€**ã€‚å…·ä½“è¡¨ç°ä¸ºï¼š\n- **æ•°æ®é›†ä¸å¹³è¡¡**ï¼šç°æœ‰å¤§è§„æ¨¡æ“ä½œæ•°æ®é›†ï¼ˆå¦‚OXEï¼‰ä¸¥é‡åå‘äºç‰¹å®šæœºå™¨äººï¼ˆå¦‚Frankaã€xArmï¼‰å’Œç‰¹å®šæœ«ç«¯æ‰§è¡Œå™¨ï¼ˆ**å‡ ä¹å…¨æ˜¯å¹³è¡Œå¤¹çˆª**ï¼‰ã€‚\n- **æ³›åŒ–èƒ½åŠ›å—é™**ï¼šæ¨¡å‹éš¾ä»¥ç›´æ¥è¿ç§»åˆ°å½¢æ€å’Œè¿åŠ¨å­¦å·®å¼‚å·¨å¤§çš„æ–°æœºå™¨äººä¸Šï¼Œå°¤å…¶æ˜¯**ä»å¹³è¡Œå¤¹çˆªè¿ç§»åˆ°å¤šæŒ‡çµå·§æ‰‹**ï¼Œå› ä¸ºçµå·§æ‰‹æ•°æ®æåº¦ç¨€ç¼ºã€‚\n\n**æ ¸å¿ƒé—®é¢˜**ï¼šå¦‚ä½•åˆ©ç”¨ä¸°å¯Œçš„å¹³è¡Œå¤¹çˆªæ¼”ç¤ºæ•°æ®ï¼Œä¸ºå½¢æ€è¿¥å¼‚çš„æœºå™¨äººï¼ˆå¦‚çµå·§æ‰‹ï¼‰**è‡ªåŠ¨åˆæˆå¯æ‰§è¡Œçš„è§†è§‰è¿åŠ¨ç­–ç•¥æ•°æ®**ï¼Œä»è€Œå®ç°è·¨å…·èº«çš„ç­–ç•¥å­¦ä¹ ï¼Ÿ\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nCEI æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è·¨å…·èº«æ•°æ®åˆæˆæ¡†æ¶ï¼Œå…¶åˆ›æ–°æ€§ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹ä¸‰ä¸ªå±‚é¢ï¼š\n\n1.  **æ¦‚å¿µåˆ›æ–°ï¼šåŠŸèƒ½ç›¸ä¼¼æ€§**\n    - æå‡ºäº† **â€œåŠŸèƒ½ç›¸ä¼¼æ€§â€** è¿™ä¸€æ ¸å¿ƒæ¦‚å¿µã€‚å®ƒè¶…è¶Šäº†ç®€å•çš„å‡ ä½•å½¢æ€åŒ¹é…ï¼Œå…³æ³¨ä¸åŒæœ«ç«¯æ‰§è¡Œå™¨åœ¨**æ‰§è¡Œä»»åŠ¡æ—¶çš„äº¤äº’è¡Œä¸ºæ˜¯å¦ç›¸ä¼¼**ï¼ˆä¾‹å¦‚ï¼Œå¹³è¡Œå¤¹çˆªçš„å¯¹ç§°æŠ“æ¡ä¸çµå·§æ‰‹æ‹‡æŒ‡å¯¹å››æŒ‡çš„æŠ“æ¡åœ¨åŠŸèƒ½ä¸Šæ˜¯ç›¸ä¼¼çš„ï¼‰ã€‚\n    - ä½¿ç”¨ **æ–¹å‘æ€§å€’è§’è·ç¦»** æ¥é‡åŒ–è¿™ç§ç›¸ä¼¼æ€§ï¼ŒåŒæ—¶è€ƒè™‘ç©ºé—´ä½ç½®å’Œè¡¨é¢æ³•å‘æ–¹å‘çš„å¯¹é½ã€‚\n\n2.  **æ–¹æ³•åˆ›æ–°ï¼šåŸºäºæ¢¯åº¦çš„è½¨è¿¹å¯¹é½ä¸æ•°æ®åˆæˆ**\n    - **æ¢¯åº¦ä¼˜åŒ–å¯¹é½**ï¼šå°†è½¨è¿¹å¯¹é½å»ºæ¨¡ä¸ºä¸€ä¸ª**åºåˆ—ä¼˜åŒ–é—®é¢˜**ã€‚å¯¹äºæºæœºå™¨äººçš„æ¯ä¸€å¸§ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç›®æ ‡æœºå™¨äººçš„å…³èŠ‚é…ç½®ï¼Œä»¥æœ€å¤§åŒ–ä¸æºæœºå™¨äººå½“å‰åŠŸèƒ½è¡¨ç¤ºçš„åŠŸèƒ½ç›¸ä¼¼æ€§ï¼Œå¹¶åˆ©ç”¨å‰ä¸€å¸§ç»“æœåˆå§‹åŒ–å½“å‰å¸§ï¼Œä¿è¯æ—¶åºä¸€è‡´æ€§ã€‚\n    - **ç«¯åˆ°ç«¯æ•°æ®ç”Ÿæˆ**ï¼šä¸ä»…åˆæˆåŠ¨ä½œï¼ˆç›®æ ‡å…³èŠ‚ä½ç½®ï¼‰ï¼Œè¿˜**åŒæ­¥åˆæˆå¯¹åº”çš„è§†è§‰è§‚æµ‹**ï¼ˆç‚¹äº‘ï¼‰ã€‚é€šè¿‡æ©è”½æºæœºå™¨äººç‚¹äº‘å¹¶é‡‡æ ·æ·»åŠ ç›®æ ‡æœºå™¨äººç‚¹äº‘ï¼Œç”Ÿæˆç¬¦åˆç›®æ ‡æœºå™¨äººè§†è§’çš„è§‚æµ‹ã€‚\n\n3.  **æ¡†æ¶é€šç”¨æ€§æ‰©å±•**\n    - å±•ç¤ºäº†CEIæ¡†æ¶å¯è½»æ¾æ‰©å±•è‡³**ç©ºé—´æ³›åŒ–**ï¼ˆä»å•æ¡è½¨è¿¹ç”Ÿæˆè¦†ç›–æ•´ä¸ªå·¥ä½œç©ºé—´çš„æ•°æ®ï¼‰å’Œ**å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆ**ï¼ˆé€šè¿‡ç²¾è‹±åˆå§‹åŒ–ç­–ç•¥ï¼Œä»åŒä¸€æºæ¼”ç¤ºç”Ÿæˆå¤šç§æœ‰æ•ˆçš„ç›®æ ‡è¿åŠ¨æ¨¡å¼ï¼‰ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆï¼šCEI æŠ€æœ¯æµç¨‹**\nCEI çš„è§£å†³æ–¹æ¡ˆæ˜¯ä¸€ä¸ªæ¸…æ™°çš„ä¸‰æ­¥æµæ°´çº¿ï¼š\n\n1.  **åŠŸèƒ½è¡¨ç¤ºä¸ç›¸ä¼¼æ€§è®¡ç®—**\n    - ä¸ºæ¯ä¸ªæœºå™¨äººå®šä¹‰**åŠŸèƒ½è¡¨ç¤º**ï¼šä¸€ç»„ä½äºæœ«ç«¯æ‰§è¡Œå™¨æ½œåœ¨æ¥è§¦åŒºåŸŸï¼ˆå¦‚æ‰‹æŒ‡å«ï¼‰çš„`(ç‚¹ï¼Œæ³•å‘)`å¯¹ã€‚\n    - é€šè¿‡**æ–¹å‘æ€§å€’è§’è·ç¦»**è®¡ç®—æºæœºå™¨äººä¸ç›®æ ‡æœºå™¨äººåŠŸèƒ½è¡¨ç¤ºä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œä½œä¸ºå¯¹é½çš„ä¼˜åŒ–ç›®æ ‡ã€‚\n\n2.  **åºåˆ—è½¨è¿¹å¯¹é½**\n    - è¾“å…¥æºæœºå™¨äººæ¼”ç¤ºè½¨è¿¹ã€‚\n    - å¯¹äºæ¯ä¸€æ—¶é—´æ­¥ `t`ï¼Œå°†ç›®æ ‡æœºå™¨äººçš„å…³èŠ‚é…ç½® `q_tâ€˜` è®¾ä¸ºå¯å­¦ä¹ å˜é‡ã€‚\n    - é€šè¿‡æœ€å°åŒ–æŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼š`â„’_align = w1 * DCD(åŠŸèƒ½è¡¨ç¤º_Xt, åŠŸèƒ½è¡¨ç¤º_Xtâ€™) + w2 * å…³èŠ‚é™ä½æƒ©ç½š`ã€‚\n    - é‡‡ç”¨**åºåˆ—åˆå§‹åŒ–**ï¼š`q_{t+1}â€˜ çš„åˆå§‹å€¼ = ä¼˜åŒ–åçš„ q_tâ€˜`ï¼Œç¡®ä¿è½¨è¿¹å¹³æ»‘ã€‚\n\n3.  **è§‚æµ‹ä¸åŠ¨ä½œåˆæˆ**\n    - **åŠ¨ä½œåˆæˆ**ï¼šç›®æ ‡æœºå™¨äººåœ¨æ—¶åˆ» `t` çš„åŠ¨ä½œå³å¯¹é½åçš„ä¸‹ä¸€å¸§å…³èŠ‚ä½ç½® `a_t = q_{t+1}â€˜`ã€‚\n    - **è§‚æµ‹åˆæˆ**ï¼š\n        - **æœ¬ä½“æ„ŸçŸ¥**ï¼šç›´æ¥ä½¿ç”¨å¯¹é½åçš„å…³èŠ‚ä½ç½® `q_tâ€˜`ã€‚\n        - **ç‚¹äº‘**ï¼šä»æºåœºæ™¯ç‚¹äº‘ä¸­**ç§»é™¤**é è¿‘æºæœºå™¨äººçš„ç‚¹ï¼Œç„¶å**æ·»åŠ **æ ¹æ®ç›®æ ‡æœºå™¨äººå½“å‰ä½å§¿ä»å…¶ç½‘æ ¼æ¨¡å‹é‡‡æ ·çš„ç‚¹ï¼Œæœ€åè¿›è¡Œä¸‹é‡‡æ ·ã€‚\n\n### **å››ã€ å®é™…ä»·å€¼ä¸æ„ä¹‰**\n- **è§£å†³æ•°æ®ç¨€ç¼ºéš¾é¢˜**ï¼šä¸ºæ•°æ®ç¨€å°‘çš„æœºå™¨äººå½¢æ€ï¼ˆå¦‚çµå·§æ‰‹ï¼‰æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œ**åˆ©ç”¨ä¸°å¯Œæ˜“å¾—çš„å¹³è¡Œå¤¹çˆªæ•°æ®å³å¯è®­ç»ƒçµå·§æ‰‹ç­–ç•¥**ã€‚\n- **æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›**ï¼šæ˜¯æ„å»ºçœŸæ­£â€œé€šç”¨â€æœºå™¨äººåŸºç¡€æ¨¡å‹çš„å…³é”®ä¸€æ­¥ï¼Œä½¿æ¨¡å‹èƒ½é€‚åº”å¤šæ ·åŒ–çš„ç‰©ç†å®ä½“ã€‚\n- **é«˜æ•ˆå®ç”¨**ï¼šå±äºç¦»çº¿æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œåˆæˆæ•°ç™¾æ¡æ¼”ç¤ºä»…éœ€æ•°åˆ†é’Ÿï¼Œè¿œå¿«äºåŸºäºç‰©ç†ä»¿çœŸçš„åœ¨çº¿ç”Ÿæˆæ–¹æ³•ï¼Œå…·å¤‡å®é™…éƒ¨ç½²çš„å¯è¡Œæ€§ã€‚\n- **åŒå‘è¿ç§»**ï¼šå®éªŒéªŒè¯äº†åœ¨å¹³è¡Œå¤¹çˆªï¼ˆUR5+AG95ï¼‰ä¸çµå·§æ‰‹ï¼ˆUR5+Xhandï¼‰ä¹‹é—´çš„**åŒå‘ç­–ç•¥è¿ç§»**ï¼Œåœ¨6ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ä¸Šå¹³å‡è¿ç§»æˆåŠŸç‡é«˜è¾¾**82.4%**ã€‚\n\n**æ€»ç»“**ï¼šCEI çš„æ ¸å¿ƒåœ¨äºé€šè¿‡å½¢å¼åŒ–å®šä¹‰å¹¶ä¼˜åŒ–â€œåŠŸèƒ½ç›¸ä¼¼æ€§â€ï¼Œåœ¨ç®—æ³•å±‚é¢æ„å»ºäº†ä¸€ä¸ªè¿æ¥ä¸åŒæœºå™¨äººå½¢æ€çš„â€œæ¥å£â€ï¼Œå®ç°äº†è·¨å…·èº«è¡Œä¸ºä¸æ•°æ®çš„ä¼ é€’ï¼Œä¸ºè§„æ¨¡åŒ–ã€é€šç”¨åŒ–çš„æœºå™¨äººå­¦ä¹ æä¾›äº†æ–°çš„æ•°æ®å¼•æ“ã€‚",
            "timestamp": "2026-01-20T17:57:46.948034",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡ã€ŠCEI: A Unified Interface for Cross-Embodiment Visuomotor Policy Learning in 3D Spaceã€‹å‘å±•è„‰ç»œåˆ†æ\n\nåŸºäºè®ºæ–‡å†…å®¹åŠå…¶å¼•ç”¨çš„ç›¸å…³æ–‡çŒ®ï¼Œå¯ä»¥æ¢³ç†å‡º**è·¨å…·èº«ï¼ˆCross-Embodimentï¼‰è§†è§‰è¿åŠ¨ç­–ç•¥å­¦ä¹ **æ–¹å‘çš„å‘å±•è„‰ç»œã€‚è¯¥é¢†åŸŸä¸»è¦å›´ç»•**å¦‚ä½•å…‹æœæœºå™¨äººå½¢æ€ï¼ˆè‡‚ã€æœ«ç«¯æ‰§è¡Œå™¨ï¼‰å·®å¼‚å¸¦æ¥çš„æ•°æ®åå·®å’Œæ³›åŒ–éš¾é¢˜**å±•å¼€ï¼Œæ¼”è¿›è·¯å¾„æ¸…æ™°ï¼Œä¸»è¦æ²¿ç€ä»¥ä¸‹å‡ ä¸ªå…³é”®æ–¹å‘å‘å±•ï¼š\n\n### å‘å±•è„‰ç»œæ€»è§ˆ\nè¯¥é¢†åŸŸä»æ—©æœŸå…³æ³¨**é¢†åŸŸè‡ªé€‚åº”ï¼ˆDomain Adaptationï¼‰**å’Œ**ç‰¹å®šå½¢æ€ç­–ç•¥å­¦ä¹ **ï¼Œé€æ­¥æ¼”è¿›åˆ°**åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®æ„å»ºåŸºç¡€æ¨¡å‹**ï¼Œå¹¶æœ€ç»ˆèšç„¦äºè§£å†³**æç«¯å½¢æ€å·®å¼‚ï¼ˆå¦‚å¹³è¡Œå¤¹çˆªä¸çµå·§æ‰‹ï¼‰ä¸‹çš„æ•°æ®ç”Ÿæˆä¸ç­–ç•¥è¿ç§»**è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ã€‚CEI æ˜¯è¿™ä¸€è„‰ç»œä¸­é’ˆå¯¹â€œæç«¯è·¨å…·èº«æ•°æ®åˆæˆâ€é—®é¢˜çš„æœ€æ–°è§£å†³æ–¹æ¡ˆã€‚\n\nä»¥ä¸‹æ˜¯æŒ‰ä¸»è¦å‘å±•æ–¹å‘æ¢³ç†çš„æ¼”è¿›æ­¥éª¤ï¼š\n\n---\n\n### å‘å±•æ–¹å‘ä¸€ï¼šä»é¢†åŸŸè‡ªé€‚åº”åˆ°å…·èº«é—´éš™å¼¥åˆï¼ˆBridging the Embodiment Gapï¼‰\n\næ—©æœŸå·¥ä½œä¸»è¦è§£å†³è§†è§‰è§‚å¯Ÿå±‚é¢çš„é¢†åŸŸå·®å¼‚ï¼ˆå¦‚è§†è§’ã€å…‰ç…§ï¼‰ï¼Œåæ¥æ‰æ„è¯†åˆ°æœºå™¨äººæœ¬ä½“å½¢æ€å·®å¼‚æ˜¯æ›´æ ¹æœ¬çš„éšœç¢ã€‚\n\n1.  **æ—©æœŸé¢†åŸŸè‡ªé€‚åº”æ–¹æ³•**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šåŸºäºå¾ªç¯ä¸€è‡´æ€§ï¼ˆDynamics Cycle-Consistencyï¼‰ç­‰æ–¹æ³•ï¼Œå­¦ä¹ ä¸åŒé¢†åŸŸï¼ˆå¦‚ä»¿çœŸä¸çœŸå®ï¼‰ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€25ã€‘ã€‚\n    *   **æ”¹è¿›ä¸å±€é™**ï¼šè¿™äº›æ–¹æ³•ä¸»è¦å…³æ³¨**è§†è§‰è§‚å¯Ÿç©ºé—´**æˆ–**åŠ¨æ€æ¨¡å‹**çš„æ˜ å°„ã€10, 25ã€‘ï¼Œå¹¶æœªæ˜¾å¼å¤„ç†æœºå™¨äºº**å½¢æ€å’Œè¿åŠ¨å­¦**çš„æ ¹æœ¬å·®å¼‚ï¼Œå› æ­¤éš¾ä»¥ç›´æ¥åº”ç”¨äºä»å¤¹çˆªåˆ°çµå·§æ‰‹ç­‰å½¢æ€è¿¥å¼‚çš„è¿ç§»ã€‚\n\n2.  **æ˜¾å¼å¼¥åˆå…·èº«é—´éš™çš„åˆæ­¥å°è¯•**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼š**MIRAGE**ã€3ã€‘å’Œ **RoVi-Aug**ã€4ã€‘ã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼šè¿™äº›æ–¹æ³•å¼•å…¥äº†â€œè·¨ç»˜åˆ¶ï¼ˆcross-paintingï¼‰â€å’Œç”Ÿæˆæ¨¡å‹æŠ€æœ¯ï¼Œ**åœ¨åƒç´ å±‚é¢åˆæˆç›®æ ‡æœºå™¨äººçš„è§†è§‰è§‚å¯Ÿ**ï¼Œåˆ¶é€ å‡ºæºæœºå™¨äººæ­£åœ¨ç›®æ ‡å½¢æ€ä¸‹æ‰§è¡Œä»»åŠ¡çš„â€œå¹»è±¡â€ã€‚è¿™å®ç°äº†ä»**è§‚å¯Ÿå±‚é¢**çš„é›¶æ ·æœ¬éƒ¨ç½²ã€‚\n    *   **å…³é”®å±€é™ï¼ˆCEI çš„å‡ºå‘ç‚¹ï¼‰**ï¼š\n        *   **å½¢æ€å‡è®¾å¼º**ï¼šä¸¥é‡ä¾èµ–**å¹³è¡Œå¤¹çˆª**å’Œ**æ“ä½œç©ºé—´æ§åˆ¶ï¼ˆOSCï¼‰**ï¼Œéš¾ä»¥è¿ç§»åˆ°å¤šæŒ‡çµå·§æ‰‹ç­‰å¤æ‚å½¢æ€ã€4ã€‘ã€‚\n        *   **ä»…å¤„ç†è§‚å¯Ÿ**ï¼šæœªåŒæ­¥ç”Ÿæˆä¸åˆæˆè§‚å¯Ÿç›¸åŒ¹é…çš„ã€å¯ä¾›ç›®æ ‡æœºå™¨äººæ‰§è¡Œçš„**å…³èŠ‚åŠ¨ä½œ**ã€‚ç­–ç•¥åœ¨â€œçœ‹åˆ°â€ç›®æ ‡æœºå™¨äººåï¼Œä»å¯èƒ½è¾“å‡ºä¸é€‚åˆå…¶è¿åŠ¨å­¦çš„åŠ¨ä½œã€‚\n\n### å‘å±•æ–¹å‘äºŒï¼šåŸºäºå¤§è§„æ¨¡è·¨å…·èº«æ•°æ®é›†çš„é€šç”¨ç­–ç•¥å­¦ä¹ \n\nå¦ä¸€æ¡è·¯å¾„æ—¨åœ¨é€šè¿‡æ”¶é›†æµ·é‡ã€å¤šæ ·åŒ–çš„æœºå™¨äººæ•°æ®ï¼Œç›´æ¥è®­ç»ƒå‡ºå¯¹å½¢æ€å˜åŒ–å…·æœ‰é²æ£’æ€§çš„â€œé€šç”¨â€ç­–ç•¥ã€‚\n\n1.  **å¤§è§„æ¨¡æ•°æ®é›†çš„æ„å»º**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼š**Bridge Data**ã€7ã€‘ã€**BridgeData V2**ã€17ã€‘ã€**Open X-embodiment (OXE)**ã€13ã€‘ã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼šæ±‡é›†æ¥è‡ªæ•°åä¸ªä¸åŒæœºå™¨äººå¹³å°çš„çœŸå®ä¸–ç•Œæ¼”ç¤ºæ•°æ®ï¼Œè¯æ˜äº†**æ•°æ®è§„æ¨¡ä¸å¤šæ ·æ€§**å¯¹ç­–ç•¥æ³›åŒ–èƒ½åŠ›çš„é‡è¦æ€§ã€‚OXE ç­‰æ•°æ®é›†ä¸ºè®­ç»ƒ **RT-X** ç­‰å¤§å‹æœºå™¨äººåŸºç¡€æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€13, 14ã€‘ã€‚\n    *   **å…³é”®å±€é™ï¼ˆCEI çš„åŠ¨æœºï¼‰**ï¼š\n        *   **æ•°æ®åˆ†å¸ƒåå·®**ï¼šå³ä½¿æ˜¯å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä¹Ÿä¸¥é‡åå‘äº Frankaã€xArm ç­‰æµè¡Œæœºæ¢°è‡‚å’Œ**å¹³è¡Œå¤¹çˆª**ã€13ã€‘ã€‚çµå·§æ‰‹æ•°æ®æå…¶ç¨€ç¼ºã€18ã€‘ï¼Œå¯¼è‡´æ¨¡å‹ä»ä¼šè¿‡æ‹Ÿåˆåˆ°ä¸»æµå½¢æ€ã€‚\n        *   **æ— æ³•å¤„ç†æç«¯æœªè§å½¢æ€**ï¼šæ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™äºè®­ç»ƒæ•°æ®ä¸­å½¢æ€çš„åˆ†å¸ƒã€‚å¯¹äºè®­ç»ƒæ•°æ®ä¸­å®Œå…¨æœªå‡ºç°è¿‡çš„ã€å½¢æ€å·®å¼‚æå¤§çš„æœºå™¨äººï¼ˆå¦‚ä»ä»…æœ‰å¤¹çˆªçš„æ•°æ®é›†æ³›åŒ–åˆ°çµå·§æ‰‹ï¼‰ï¼Œæ€§èƒ½ä¼šæ€¥å‰§ä¸‹é™ã€‚\n\n### å‘å±•æ–¹å‘ä¸‰ï¼šè·¨å…·èº«ç­–ç•¥ä¸è¡¨ç¤ºçš„ç»Ÿä¸€å­¦ä¹ \n\nè¿™éƒ¨åˆ†å·¥ä½œå°è¯•åœ¨ç­–ç•¥ç½‘ç»œæ¶æ„æˆ–å­¦ä¹ è¿‡ç¨‹ä¸­åµŒå…¥å½¢æ€ä¿¡æ¯ï¼Œä»¥æ”¯æŒå¯¹ä¸€ç»„å·²çŸ¥å½¢æ€çš„æ³›åŒ–ã€‚\n\n1.  **ç»Ÿä¸€ç­–ç•¥æ¶æ„**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼š**UniGrasp**ã€15ã€‘ã€**AdaGrasp**ã€19ã€‘ã€ä»¥åŠä¸€äº›å°†å½¢æ€ç¼–ç ä¸ç­–ç•¥å…±åŒè®­ç»ƒçš„æ–¹æ³•ã€8ã€‘ã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼šè®¾è®¡èƒ½å¤Ÿæ¥æ”¶**å½¢æ€æè¿°ï¼ˆå¦‚URDFï¼‰** ä½œä¸ºè¾“å…¥çš„ç­–ç•¥ç½‘ç»œï¼Œåœ¨**ä¸€ç»„ç»™å®šçš„ã€å¤šæ ·çš„å½¢æ€ä¸Š**è¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨æ³›åŒ–åˆ°è¯¥é›†åˆå†…çš„æœªè§å½¢æ€ã€‚\n    *   **å…³é”®å±€é™**ï¼š\n        *   **éœ€è¦ç›®æ ‡å½¢æ€æ•°æ®**ï¼šé€šå¸¸éœ€è¦åœ¨åŒ…å«ç›®æ ‡å½¢æ€ï¼ˆæˆ–ç›¸ä¼¼å½¢æ€ï¼‰çš„**æ··åˆæ•°æ®é›†**ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ— æ³•å®ç°ä»å•ä¸€æºå½¢æ€åˆ°å®Œå…¨æœªçŸ¥ç›®æ ‡å½¢æ€çš„é›¶æ ·æœ¬è¿ç§»ã€‚\n        *   **æ³›åŒ–èŒƒå›´æœ‰é™**ï¼šæ€§èƒ½é«˜åº¦ä¾èµ–äºè®­ç»ƒæ‰€è§å½¢æ€çš„è¦†ç›–èŒƒå›´ï¼Œå¯¹äºåƒä»å¤¹çˆªåˆ°çµå·§æ‰‹è¿™æ ·çš„â€œåˆ†å¸ƒå¤–â€æ³›åŒ–æŒ‘æˆ˜å·¨å¤§ã€‚\n\n### å‘å±•æ–¹å‘å››ï¼šç¦»çº¿çš„è·¨å…·èº«æ•°æ®ç”Ÿæˆä¸åˆæˆï¼ˆCEI æ‰€å±æ–¹å‘ï¼‰\n\nè¿™æ˜¯ CEI ç›´æ¥æ¨è¿›çš„æ–¹å‘ï¼Œæ—¨åœ¨ä¸ä¾èµ–ç›®æ ‡å½¢æ€ä»»ä½•çœŸå®æ•°æ®çš„æƒ…å†µä¸‹ï¼Œ**ç®—æ³•æ€§åœ°åˆæˆ**å¯ç”¨äºè®­ç»ƒç›®æ ‡ç­–ç•¥çš„æ¼”ç¤ºæ•°æ®ã€‚\n\n1.  **é€šç”¨æ•°æ®ç”ŸæˆæŠ€æœ¯**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼š**MimicGen**ã€12ã€‘ï¼ˆåœ¨çº¿ï¼‰å’Œ **DemoGen**ã€20ã€‘ï¼ˆç¦»çº¿ï¼‰ã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼š\n        *   **MimicGen**ï¼šåœ¨é«˜ä¿çœŸç‰©ç†æ¨¡æ‹Ÿå™¨ä¸­é‡æ”¾æºè½¨è¿¹ï¼Œé€šè¿‡ç‰©ç†ä»¿çœŸç”Ÿæˆæ–°é…ç½®ä¸‹çš„æ•°æ®ï¼Œä¿è¯ç‰©ç†çœŸå®æ€§ï¼Œä½†**è®¡ç®—æˆæœ¬é«˜**ä¸”éš¾ä»¥ç”¨äºçœŸå®ä¸–ç•Œã€‚\n        *   **DemoGen**ï¼šé€šè¿‡è½¨è¿¹å˜æ¢ç¦»çº¿ç”Ÿæˆæ•°æ®ï¼Œ**é€Ÿåº¦å¿«**ï¼Œä½†ä¸»è¦å¤„ç†**ç‰©ä½“é…ç½®å˜åŒ–**ï¼Œæœªè§£å†³**æœºå™¨äººå½¢æ€å˜åŒ–**çš„æ ¹æœ¬é—®é¢˜ã€‚\n    *   **å…³é”®å±€é™**ï¼šå®ƒä»¬éƒ½**æœªä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†æœ«ç«¯æ‰§è¡Œå™¨å½¢æ€çš„æç«¯å·®å¼‚**ã€‚\n\n2.  **CEI çš„é©å‘½æ€§æ”¹è¿›**\n    *   **æ ¸å¿ƒåˆ›æ–°**ï¼šæå‡ºäº†é¦–ä¸ªä¸“é—¨è§£å†³**ä»å¹³è¡Œå¤¹çˆªåˆ°å¤šæŒ‡çµå·§æ‰‹ï¼ˆåŠåå‘ï¼‰** è¿™ç±»æç«¯è·¨å…·èº«æ•°æ®ç”Ÿæˆé—®é¢˜çš„æ¡†æ¶ã€‚\n    *   **å…·ä½“æ”¹è¿›ç‚¹**ï¼š\n        *   **æå‡ºâ€œåŠŸèƒ½ç›¸ä¼¼æ€§â€æ¦‚å¿µ**ï¼šè¶…è¶Šå‡ ä½•å¤–å½¢ï¼Œä»**ç‰©ä½“äº¤äº’è¡Œä¸º**çš„å±‚é¢å®šä¹‰å½¢æ€é—´çš„ç›¸ä¼¼æ€§ã€‚è¿™æ˜¯å®ç°æç«¯è¿ç§»çš„ç†è®ºåŸºç¡€ã€‚\n        *   **å¼•å…¥æ–¹å‘æ€§å€’è§’è·ç¦»**ï¼šä½¿ç”¨ **DCD**ã€16ã€‘é‡åŒ–åŠŸèƒ½ç›¸ä¼¼æ€§ï¼ŒåŒæ—¶è€ƒè™‘**ç©ºé—´è·ç¦»å’Œè¡¨é¢æ³•å‘å¯¹é½**ï¼Œè¿™å¯¹æŠ“å–ç­‰æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡è‡³å…³é‡è¦ï¼ˆ ablation å®éªŒè¯æ˜æ–¹å‘ä¿¡æ¯æ˜¯å…³é”®ï¼‰ã€‚\n        *   **ç«¯åˆ°ç«¯çš„è½¨è¿¹ã€è§‚å¯Ÿã€åŠ¨ä½œåˆæˆ**ï¼š\n            *   **åŸºäºæ¢¯åº¦çš„è½¨è¿¹å¯¹é½**ï¼šé€šè¿‡ä¼˜åŒ– DCDï¼Œå°†æºè½¨è¿¹åœ¨**åŠŸèƒ½å±‚é¢**å¯¹é½åˆ°ç›®æ ‡å½¢æ€ï¼Œç”Ÿæˆç›®æ ‡å…³èŠ‚è½¨è¿¹ã€‚\n            *   **åŒæ­¥åˆæˆç‚¹äº‘è§‚å¯Ÿå’Œå…³èŠ‚åŠ¨ä½œ**ï¼šä¸ä»…ç”Ÿæˆçœ‹èµ·æ¥åƒç›®æ ‡æœºå™¨äººçš„è§‚å¯Ÿï¼Œè¿˜ç”Ÿæˆä¸ä¹‹ä¸¥æ ¼åŒ¹é…ã€å¯æ‰§è¡Œçš„åŠ¨ä½œåºåˆ—ï¼Œå½¢æˆå®Œæ•´çš„æ¼”ç¤ºæ•°æ®å¯¹ `(o, a)`ã€‚\n        *   **å®ç°çœŸæ­£çš„é›¶æ ·æœ¬åŒå‘è¿ç§»**ï¼šä»…éœ€æºæœºå™¨äººæ•°æ®å’Œç›®æ ‡æœºå™¨äººçš„æè¿°æ–‡ä»¶ï¼ˆURDFï¼‰ï¼Œå³å¯ç”Ÿæˆç›®æ ‡æ•°æ®ï¼Œå¹¶åœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œä¸­éªŒè¯äº†**å¤¹çˆªâ†”çµå·§æ‰‹**çš„åŒå‘ç­–ç•¥è¿ç§»ï¼Œå¹³å‡è¿ç§»ç‡è¾¾ **82.4%**ã€‚\n        *   **æ‹“å±•æ€§**ï¼šæ¡†æ¶å¤©ç„¶æ”¯æŒ**ç©ºé—´æ³›åŒ–**ï¼ˆä»å•æ¡è½¨è¿¹ç”Ÿæˆè¦†ç›–å·¥ä½œç©ºé—´çš„å¤šæ ·æ•°æ®ï¼‰å’Œ**å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆ**ï¼ˆé€šè¿‡åˆå§‹åŒ–ç­–ç•¥äº§ç”Ÿä¸åŒçš„æœ‰æ•ˆåŠ¨ä½œæ¨¡å¼ï¼‰ã€‚\n\n### æ€»ç»“ï¼šå‘å±•è„‰ç»œä¸ CEI çš„å®šä½\n\n| å‘å±•é˜¶æ®µ | æ ¸å¿ƒæ€æƒ³ | ä»£è¡¨æ€§å·¥ä½œ/æ–¹æ³• | ç›¸å¯¹äºå‰ä½œçš„æ”¹è¿› | é—ç•™é—®é¢˜/å±€é™ |\n| :--- | :--- | :--- | :--- | :--- |\n| **1. é¢†åŸŸè‡ªé€‚åº”** | å¯¹é½ä¸åŒé¢†åŸŸçš„è§†è§‰/åŠ¨æ€ç‰¹å¾ã€‚ | å¾ªç¯ä¸€è‡´æ€§ã€25ã€‘ã€Robot-Awarenessã€10ã€‘ | å¼€å§‹å¤„ç†ä»¿çœŸåˆ°çœŸå®çš„å·®å¼‚ã€‚ | æœªå¤„ç†æœºå™¨äººæœ¬ä½“å½¢æ€å·®å¼‚ã€‚ |\n| **2. è§†è§‰å±‚é¢å¼¥åˆ** | åœ¨å›¾åƒå±‚é¢â€œç»˜åˆ¶â€ç›®æ ‡æœºå™¨äººã€‚ | MIRAGEã€3ã€‘, RoVi-Augã€4ã€‘ | å®ç°äº†è§‚å¯Ÿå±‚é¢çš„é›¶æ ·æœ¬è·¨å…·èº«ã€‚ | å‡è®¾å¹³è¡Œå¤¹çˆªï¼›æœªç”ŸæˆåŒ¹é…çš„åŠ¨ä½œï¼›éš¾ä»¥å¤„ç†çµå·§æ‰‹ã€‚ |\n| **3. å¤§æ•°æ®é©±åŠ¨** | æ”¶é›†å¤šæ ·æ•°æ®è®­ç»ƒé€šç”¨ç­–ç•¥ã€‚ | OXEã€13ã€‘, RT-X/Octoã€14ã€‘ | è¯æ˜äº†å¤§è§„æ¨¡æ•°æ®çš„å¨åŠ›ï¼Œæ„å»ºäº†åŸºç¡€æ¨¡å‹ã€‚ | æ•°æ®å­˜åœ¨å½¢æ€åå·®ï¼›å¯¹æç«¯æœªè§å½¢æ€æ³›åŒ–å·®ã€‚ |\n| **4. ç»Ÿä¸€ç­–ç•¥å­¦ä¹ ** | å°†å½¢æ€ç¼–ç ä½œä¸ºç­–ç•¥è¾“å…¥ã€‚ | UniGraspã€15ã€‘, AdaGraspã€19ã€‘ | å¯æ³›åŒ–åˆ°è®­ç»ƒåˆ†å¸ƒå†…çš„æœªè§å½¢æ€ã€‚ | éœ€è¦ç›®æ ‡å½¢æ€ç›¸å…³æ•°æ®ï¼›æ— æ³•å¤„ç†åˆ†å¸ƒå¤–æç«¯å½¢æ€ã€‚ |\n| **5. ç¦»çº¿æ•°æ®ç”Ÿæˆ** | ç®—æ³•æ€§åˆæˆæ–°æ•°æ®ã€‚ | MimicGenã€12ã€‘, DemoGenã€20ã€‘ | é«˜æ•ˆæ‰©å……æ•°æ®ï¼Œè§£å†³ç‰©ä½“é…ç½®æ³›åŒ–ã€‚ | æœªè§£å†³æœºå™¨äººå½¢æ€å˜åŒ–çš„æ ¹æœ¬é—®é¢˜ã€‚ |\n| **6. æç«¯è·¨å…·èº«åˆæˆ (CEI)** | **åŸºäºåŠŸèƒ½ç›¸ä¼¼æ€§çš„è½¨è¿¹ä¸æ•°æ®åˆæˆ**ã€‚ | **CEI**",
            "timestamp": "2026-01-20T17:58:54.007760",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## CEIè®ºæ–‡çš„å‰ä½œåˆ†æä¸å¯¹æ¯”\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼ŒCEIçš„ç ”ç©¶å»ºç«‹åœ¨å¤šä¸ªç›¸å…³å·¥ä½œçš„åŸºç¡€ä¸Šï¼Œä½†å…¶æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºè§£å†³**æç«¯å½¢æ€å·®å¼‚ï¼ˆå¦‚å¹³è¡Œå¤¹çˆªä¸çµå·§æ‰‹ï¼‰ä¹‹é—´çš„è·¨å…·èº«æ•°æ®ç”Ÿæˆ**ã€‚è®ºæ–‡ä¸­æ˜ç¡®æåŠå¹¶å¯¹æ¯”äº†æ•°ç¯‡å…³é”®çš„å‰ä½œã€‚ä»¥ä¸‹æ˜¯è¿™äº›å‰ä½œçš„è¯¦ç»†åˆ†æï¼Œå¹¶ä»¥è¡¨æ ¼å½¢å¼æ€»ç»“CEIä¸å®ƒä»¬çš„æ ¸å¿ƒåŒºåˆ«å’Œæ”¹è¿›ã€‚\n\n### å…³é”®å‰ä½œåˆ†æ\n\n| å‰ä½œåç§° (å¼•ç”¨ç¼–å·) | æ ¸å¿ƒæ–¹æ³• / å…³æ³¨ç‚¹ | ä¸CEIçš„ä¸»è¦åŒºåˆ« | CEIçš„ä¸»è¦æ”¹è¿› / åˆ›æ–° |\n| :--- | :--- | :--- | :--- |\n| **Mirage [3]** | **è·¨ç»˜åˆ¶**ï¼šé€šè¿‡å›¾åƒç¼–è¾‘æŠ€æœ¯ï¼ˆå¦‚å°†æºæœºå™¨äººçš„å›¾åƒâ€œç»˜åˆ¶â€æˆç›®æ ‡æœºå™¨äººçš„å¤–è§‚ï¼‰æ¥åˆæˆè§†è§‰è§‚æµ‹ï¼Œä»¥å®ç°é›¶æ ·æœ¬ç­–ç•¥è¿ç§»ã€‚ | 1. **å…³æ³¨å±‚é¢**ï¼šä¸»è¦è§£å†³**è§†è§‰å¤–è§‚**çš„å·®å¼‚ï¼Œå±äºå›¾åƒåŸŸè½¬æ¢ã€‚<br>2. **å½¢æ€é™åˆ¶**ï¼šæ–¹æ³•**ä¸¥é‡ä¾èµ–å¹³è¡Œå¤¹çˆª**å’Œæ“ä½œç©ºé—´æ§åˆ¶ï¼ˆOSCï¼‰ï¼Œéš¾ä»¥è¿ç§»åˆ°çµå·§æ‰‹ç­‰å¤æ‚å½¢æ€ã€‚<br>3. **æ•°æ®ç”Ÿæˆ**ï¼šä¾§é‡äºè§‚æµ‹ï¼ˆå›¾åƒï¼‰çš„åˆæˆï¼Œå¯¹åŠ¨ä½œçš„é€‚é…èƒ½åŠ›æœ‰é™ã€‚ | 1. **å…³æ³¨åŠŸèƒ½è€Œéå¤–è§‚**ï¼šæå‡º**åŠŸèƒ½ç›¸ä¼¼æ€§**æ¦‚å¿µï¼Œä»æœºå™¨äºº**ä¸ç‰©ä½“äº¤äº’çš„è¡Œä¸ºæœ¬è´¨**å‡ºå‘è¿›è¡Œå¯¹é½ï¼Œä¸ä¾èµ–è§†è§‰å¤–è§‚åŒ¹é…ã€‚<br>2. **è§£å†³æç«¯å½¢æ€å·®å¼‚**ï¼šä¸“é—¨è®¾è®¡ç”¨äºåœ¨**å¹³è¡Œå¤¹çˆªä¸å¤šæŒ‡çµå·§æ‰‹**ä¹‹é—´è¿›è¡ŒåŒå‘æ•°æ®ä¸ç­–ç•¥è¿ç§»ã€‚<br>3. **å…¨è½¨è¿¹ç”Ÿæˆ**ï¼šåŒæ­¥åˆæˆ**è§‚æµ‹ï¼ˆç‚¹äº‘ï¼‰å’ŒåŠ¨ä½œï¼ˆå…³èŠ‚è½¨è¿¹ï¼‰**ï¼Œç”Ÿæˆå®Œæ•´ã€å¯æ‰§è¡Œçš„ç›®æ ‡æœºå™¨äººæ¼”ç¤ºè½¨è¿¹ã€‚ |\n| **RoVi-Aug [4]** | **æœºå™¨äººä¸è§†ç‚¹å¢å¼º**ï¼šç»“åˆç”Ÿæˆæ¨¡å‹å’Œè·¨ç»˜åˆ¶æŠ€æœ¯ï¼ŒåŒæ—¶å¢å¼ºæœºå™¨äººå¤–è§‚å’Œç›¸æœºè§†ç‚¹ï¼Œä»¥ç¼©å°é¢†åŸŸå’Œå…·èº«å·®è·ã€‚ | 1. **é—®é¢˜èŒƒå›´**ï¼šåŒæ—¶å¤„ç†**å…·èº«å·®å¼‚**å’Œ**è§†ç‚¹å·®å¼‚**ã€‚<br>2. **æ–¹æ³•åŸºç¡€**ï¼šåŒæ ·åŸºäºç”Ÿæˆå¼è§†è§‰å¢å¼ºï¼Œå…¶**æœ‰æ•ˆæ€§å‰ææ˜¯æºä¸ç›®æ ‡å½¢æ€åœ¨è§†è§‰å’Œè¿åŠ¨å­¦ä¸Šä¸èƒ½ç›¸å·®å¤ªå¤§**ï¼ˆæ–‡ä¸­æŒ‡å‡ºå…¶é€‚ç”¨æ€§ä»å±€é™äºå¹³è¡Œå¤¹çˆªå’ŒOSCï¼‰ã€‚<br>3. **æ•°æ®åŸºç¡€**ï¼šéœ€è¦ä¸€å®šé‡çš„è·¨å…·èº«æ•°æ®æ¥è®­ç»ƒç”Ÿæˆæ¨¡å‹ã€‚ | 1. **ä¸“æ”»å½¢æ€é¸¿æ²Ÿ**ï¼šCEI**ä¸“æ³¨äºè§£å†³ç”±å·¨å¤§å½¢æ€å·®å¼‚ï¼ˆç‰¹åˆ«æ˜¯æœ«ç«¯æ‰§è¡Œå™¨ï¼‰å¸¦æ¥çš„æ ¹æœ¬æ€§æŒ‘æˆ˜**ï¼Œè€Œä¸ä¸»è¦å¤„ç†è§†ç‚¹å˜åŒ–ã€‚<br>2. **åŸºäºå‡ ä½•ä¸ä¼˜åŒ–çš„æ— æ•°æ®ç”Ÿæˆ**ï¼š**æ— éœ€ä¸ºç›®æ ‡å½¢æ€æ”¶é›†ä»»ä½•æ¼”ç¤ºæ•°æ®**ã€‚ä»…åˆ©ç”¨æœºå™¨äººçš„URDF/XMLæè¿°æ–‡ä»¶ï¼Œé€šè¿‡åŸºäºæ¢¯åº¦çš„è½¨è¿¹ä¼˜åŒ–å®ç°è¿ç§»ï¼Œè§£å†³äº†çµå·§æ‰‹æ•°æ®ç¨€ç¼ºçš„ç“¶é¢ˆã€‚<br>3. **å¼•å…¥é‡åŒ–æŒ‡æ ‡**ï¼šåˆ›æ–°æ€§åœ°ä½¿ç”¨**å®šå‘å€’è§’è·ç¦»**æ¥å½¢å¼åŒ–å’Œé‡åŒ–â€œåŠŸèƒ½ç›¸ä¼¼æ€§â€ï¼Œä¸ºè·¨å…·èº«å¯¹é½æä¾›äº†å¯ä¼˜åŒ–çš„æ˜ç¡®ç›®æ ‡ã€‚ |\n| **MimicGen [12]** | **åœ¨çº¿æ•°æ®ç”Ÿæˆ**ï¼šåœ¨é«˜ä¿çœŸç‰©ç†æ¨¡æ‹Ÿå™¨ä¸­ï¼Œé€šè¿‡è°ƒæ•´ç‰©ä½“é…ç½®å¹¶æ‰§è¡Œç­–ç•¥ rollout æ¥ç”Ÿæˆæ–°çš„ã€ç‰©ç†ä¸ŠçœŸå®çš„æ¼”ç¤ºã€‚ | 1. **ç”ŸæˆèŒƒå¼**ï¼š**åœ¨çº¿ç”Ÿæˆ**ï¼Œä¾èµ–æ¨¡æ‹Ÿå™¨ä¸­çš„ç‰©ç†æ‰§è¡Œã€‚<br>2. **è®¡ç®—æˆæœ¬**ï¼šæ¯æ¬¡ç”Ÿæˆéƒ½éœ€è¦è¿›è¡Œç‰©ç†ä»¿çœŸï¼Œ**è®¡ç®—å¯†é›†ã€è€—æ—¶**ã€‚<br>3. **é€‚ç”¨æ€§**ï¼šä¸»è¦é€‚ç”¨äºä»¿çœŸç¯å¢ƒï¼Œç›´æ¥åº”ç”¨äºçœŸå®ä¸–ç•Œæˆæœ¬é«˜ï¼ˆéœ€æ‰‹åŠ¨é‡ç½®ç‰©ä½“ç­‰ï¼‰ã€‚ | 1. **ç¦»çº¿ç”ŸæˆèŒƒå¼**ï¼šCEIæ˜¯**å®Œå…¨ç¦»çº¿çš„æ•°æ®åˆæˆæ–¹æ³•**ã€‚ç»™å®šæºæ¼”ç¤ºï¼Œç›´æ¥é€šè¿‡è®¡ç®—å’Œä¼˜åŒ–ç”Ÿæˆç›®æ ‡æ¼”ç¤ºï¼Œ**ä¸ä¾èµ–ç‰©ç†ä»¿çœŸ**ã€‚<br>2. **æ•ˆç‡å¤§å¹…æå‡**ï¼šå¦‚è¡¨VIæ‰€ç¤ºï¼Œç”Ÿæˆæ•°ç™¾æ¡æ¼”ç¤ºä»…éœ€**æ•°åˆ†é’Ÿ**ï¼Œè€ŒMimicGenéœ€è¦**æ•°å°æ—¶**ï¼Œæ•ˆç‡é«˜å‡ºæ•°ä¸ªæ•°é‡çº§ã€‚<br>3. **æ‘†è„±ç‰©ç†çº¦æŸ**ï¼šä¸ä¾èµ–äºæ¨¡æ‹Ÿå™¨æ˜¯å¦æ”¯æŒç›®æ ‡æœºå™¨äººæˆ–ç²¾ç¡®çš„ç‰©ç†å‚æ•°ï¼Œåªè¦æœ‰æœºå™¨çš„è¿åŠ¨å­¦æè¿°å³å¯å·¥ä½œï¼Œ**æ›´é€šç”¨ã€æ›´è½»é‡**ã€‚ |\n| **DemoGen [20]** | **ç¦»çº¿è½¨è¿¹å˜æ¢**ï¼šé€šè¿‡å¯¹ç°æœ‰æ¼”ç¤ºè½¨è¿¹è¿›è¡Œå‡ ä½•å˜æ¢ï¼ˆå¦‚å¹³ç§»ã€æ—‹è½¬ï¼‰æ¥ç”Ÿæˆæ–°æ•°æ®ï¼Œä¸»è¦ç”¨äº**ç©ºé—´æ³›åŒ–**ã€‚ | 1. **æ ¸å¿ƒèƒ½åŠ›**ï¼šä¸“æ³¨äº**åŒä¸€æœºå™¨äºº**åœ¨**ä¸åŒç©ºé—´ä½ç½®**æ‰§è¡Œç›¸åŒä»»åŠ¡çš„æ•°æ®ç”Ÿæˆã€‚<br>2. **å½¢æ€å¤„ç†**ï¼š**ä¸å…·å¤‡è·¨ä¸åŒæœºå™¨äººå½¢æ€è¿›è¡Œæ•°æ®è¿ç§»çš„èƒ½åŠ›**ã€‚ | 1. **æ‰©å±•ç”Ÿæˆç»´åº¦**ï¼šCEIçš„æ ¸å¿ƒæ˜¯**è·¨å½¢æ€ç”Ÿæˆ**ï¼Œè€ŒDemoGenæ˜¯**ç©ºé—´ç”Ÿæˆ**ã€‚CEI**å°†ç©ºé—´æ³›åŒ–ä½œä¸ºå…¶ä¸€ä¸ªæ‰©å±•åº”ç”¨**ï¼ˆè§VII-AèŠ‚ï¼‰ï¼Œå±•ç¤ºäº†å…¶æ¡†æ¶çš„çµæ´»æ€§ã€‚<br>2. **æ–¹æ³•æœ¬è´¨ä¸åŒ**ï¼šDemoGenæ˜¯ç®€å•çš„å‡ ä½•å˜æ¢ï¼›CEIæ¶‰åŠå¤æ‚çš„åŸºäºåŠŸèƒ½ç›¸ä¼¼æ€§çš„è½¨è¿¹ä¼˜åŒ–å’Œè§‚æµ‹åˆæˆã€‚CEIåœ¨éœ€è¦æ—¶ä¹Ÿå¯ä»¥é›†æˆç±»ä¼¼DemoGençš„ç©ºé—´å˜æ¢æ¥å¢å¼ºæ•°æ®å¤šæ ·æ€§ã€‚ |\n\n### æ€»ç»“ï¼šCEIçš„æ ¸å¿ƒçªç ´\n\nç»¼åˆæ¥çœ‹ï¼ŒCEIç›¸å¯¹äºå…¶å‰ä½œçš„æ ¸å¿ƒåŒºåˆ«å’Œæ”¹å–„é›†ä¸­äºä»¥ä¸‹å‡ ç‚¹ï¼š\n\n1.  **é—®é¢˜å®šä¹‰ä¸Šçš„æ·±åŒ–**ï¼šä»å‰ä½œä¸»è¦è§£å†³**è§†è§‰åŸŸé€‚åº”**æˆ–**æœ‰é™å½¢æ€å˜åŒ–**ï¼Œè½¬å‘æ”»å…‹**å¹³è¡Œå¤¹çˆªä¸çµå·§æ‰‹ä¹‹é—´æç«¯å½¢æ€å·®å¼‚**è¿™ä¸€æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚\n2.  **æ–¹æ³•è®ºä¸Šçš„åˆ›æ–°**ï¼š\n    *   **æå‡ºâ€œåŠŸèƒ½ç›¸ä¼¼æ€§â€**ï¼šè¶…è¶Šäº†è¡¨é¢å¯¹é½ï¼ˆå¤–è§‚ã€å…³èŠ‚ç©ºé—´ï¼‰ï¼ŒæŠ“ä½äº†â€œäº¤äº’è¡Œä¸ºâ€è¿™ä¸€æœ¬è´¨ï¼Œä¸ºè·¨å…·èº«å­¦ä¹ æä¾›äº†æ–°çš„ç†è®ºè§†è§’ã€‚\n    *   **æ„å»ºåŸºäºä¼˜åŒ–çš„ç¦»çº¿ç”Ÿæˆç®¡é“**ï¼šç»“åˆDCDåº¦é‡ã€å¯å¾®åˆ†è¿åŠ¨å­¦å’Œåºåˆ—ä¼˜åŒ–ï¼Œå®ç°äº†**æ— éœ€ç›®æ ‡æ•°æ®ã€æ— éœ€ä»¿çœŸ**çš„é«˜æ•ˆã€é«˜è´¨é‡è½¨è¿¹ä¸è§‚æµ‹åˆæˆã€‚\n3.  **èƒ½åŠ›ä¸Šçš„æ‹“å±•**ï¼š\n    *   **åŒå‘è¿ç§»**ï¼šå®ç°äº†å¤¹çˆªâ†’çµå·§æ‰‹å’Œçµå·§æ‰‹â†’å¤¹çˆªçš„åŒå‘æœ‰æ•ˆè¿ç§»ã€‚\n    *   **åŸºç¡€æ¡†æ¶çš„å…¼å®¹æ€§**ï¼šå±•ç¤ºäº†å…¶æ ¸å¿ƒæ–¹æ³•å¯è½»æ¾ä¸**ç©ºé—´æ³›åŒ–**å’Œ**å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆ**ç­‰æŠ€æœ¯ç»“åˆï¼Œå½¢æˆä¸€ä¸ªæ›´å¼ºå¤§çš„æ•°æ®åˆæˆå·¥å…·ç®±ã€‚\n\n**ç®€è¨€ä¹‹ï¼ŒCEIçš„æ ¸å¿ƒæ”¹è¿›åœ¨äºï¼šå®ƒä¸å†è¯•å›¾è®©æœºå™¨äººâ€œçœ‹èµ·æ¥â€ä¸€æ ·ï¼ˆå¦‚Mirageï¼‰ï¼Œæˆ–åªåœ¨ç›¸ä¼¼å½¢æ€é—´è¿ç§»ï¼ˆå¦‚RoVi-Augï¼‰ï¼Œè€Œæ˜¯é€šè¿‡è®©æœºå™¨äººåœ¨ä»»åŠ¡ä¸­â€œè¡Œä¸ºåŠŸèƒ½â€ç­‰æ•ˆï¼Œä»è€Œåœ¨ç®—æ³•å±‚é¢æ‰“é€šäº†æˆªç„¶ä¸åŒçš„æœºå™¨äººå½¢æ€ä¹‹é—´çš„æ•°æ®ä¸ç­–ç•¥è¿ç§»é€šé“ã€‚**",
            "timestamp": "2026-01-20T17:59:40.849081",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†å¯¹è¿™ç¯‡é¢˜ä¸ºã€ŠCEI: A Unified Interface for Cross-Embodiment Visuomotor Policy Learning in 3D Spaceã€‹çš„è®ºæ–‡è¿›è¡Œæ·±å…¥åˆ†æï¼Œå¹¶è¯¦ç»†é˜è¿°å…¶æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ä¸ä»·å€¼åˆ†æ\n\næœ¬æ–‡çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ä¸ªåä¸º **CEI** çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººåŸºç¡€æ¨¡å‹å› æ•°æ®åè§è€Œéš¾ä»¥åœ¨ä¸åŒå½¢æ€çš„æœºå™¨äººï¼ˆç‰¹åˆ«æ˜¯ä»å¹³è¡Œå¤¹çˆªåˆ°çµå·§æ‰‹ï¼‰ä¹‹é—´è¿›è¡Œç­–ç•¥è¿ç§»çš„éš¾é¢˜ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨**åŠŸèƒ½ç›¸ä¼¼æ€§**æ¥å¯¹é½ä¸åŒæœºå™¨äººçš„è¿åŠ¨è½¨è¿¹ï¼Œå¹¶åˆæˆç›®æ ‡æœºå™¨äººçš„è§‚æµ‹å’ŒåŠ¨ä½œæ•°æ®ã€‚\n\n### å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è¿°\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†è¯´æ˜äº†CEIæ¡†æ¶ä¸­çš„å…³é”®æŠ€æœ¯ç»„ä»¶ã€å…¶æ•°å­¦åŸç†ã€å®ç°ç»†èŠ‚åŠå‚æ•°ã€‚\n\n| æŠ€æœ¯æ¨¡å— | æ ¸å¿ƒæ¦‚å¿µä¸ç›®æ ‡ | æ•°å­¦åŸç†ä¸æ¨å¯¼ç»†èŠ‚ | å…·ä½“å‚æ•°ä¸å®ç°ç»†èŠ‚ |\n| :--- | :--- | :--- | :--- |\n| **1. åŠŸèƒ½è¡¨ç¤º** | å°†æœºå™¨äººçš„äº¤äº’èƒ½åŠ›æŠ½è±¡ä¸ºä¸€ç»„**ç‚¹-æ–¹å‘å¯¹** `X = {(p_i, n_i)}`ï¼Œç”¨äºé‡åŒ–ä¸åŒå½¢æ€æœºå™¨äººä¹‹é—´çš„**åŠŸèƒ½ç›¸ä¼¼æ€§**ã€‚ | - **ç‚¹ `p_i`**ï¼šé‡‡æ ·è‡ªæœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨ï¼ˆå¦‚æ‰‹æŒ‡æŒ‡è…¹ï¼‰è¡¨é¢çš„ç‚¹ï¼Œä»£è¡¨æ½œåœ¨çš„ç‰©ä½“æ¥è§¦åŒºåŸŸã€‚<br>- **æ–¹å‘ `n_i`**ï¼šç‚¹ `p_i` å¤„çš„è¡¨é¢æ³•å‘é‡ï¼Œä»£è¡¨æ¥è§¦æ—¶çš„å¯èƒ½æ–½åŠ›æ–¹å‘ã€‚<br>- é€šè¿‡æ­£å‘è¿åŠ¨å­¦å°†æ‰€æœ‰ç‚¹-æ–¹å‘å¯¹è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ç³»ï¼Œç¡®ä¿åœ¨ä¸åŒæœºå™¨äººä¹‹é—´è¿›è¡Œå¯¹é½æ—¶æœ‰ä¸€ä¸ªå…±åŒçš„å‚è€ƒç³»ã€‚ | - **é‡‡æ ·æ¥æº**ï¼šä»æ‰‹æŒ‡æŒ‡å«ç½‘æ ¼è¡¨é¢é‡‡æ ·ã€‚<br>- **æ³•å‘é‡è®¡ç®—**ï¼šä½¿ç”¨ `TorchSDF` åº“è‡ªåŠ¨è®¡ç®—é‡‡æ ·ç‚¹é™„è¿‘çš„è¡¨é¢æ³•çº¿ã€‚<br>- **ç›®çš„**ï¼šå°†å¤æ‚çš„å‡ ä½•å½¢æ€è½¬åŒ–ä¸ºå¯ç”¨äºé‡åŒ–æ¯”è¾ƒçš„é›†åˆã€‚ |\n| **2. åŠŸèƒ½ç›¸ä¼¼æ€§åº¦é‡ï¼ˆæ–¹å‘å€’è§’è·ç¦»ï¼‰** | é‡åŒ–ä¸¤ä¸ªåŠŸèƒ½è¡¨ç¤ºé›†åˆ `X` å’Œ `X'` ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ŒåŒæ—¶è€ƒè™‘**ç©ºé—´è·ç¦»**å’Œ**æ–¹å‘å¯¹é½**ã€‚ | é‡‡ç”¨**æ–¹å‘å€’è§’è·ç¦»**ï¼š<br>```math<br>DCD(X, X') = (1/N) Î£_{i=1}^{N} min_j (â€–p_i - p'_jâ€–â‚‚ - Î»Â·âŸ¨n_i, n'_jâŸ©) <br>                + (1/N') Î£_{j=1}^{N'} min_i (â€–p'_j - p_iâ€–â‚‚ - Î»Â·âŸ¨n'_j, n_iâŸ©)<br>```<br>**æ¨å¯¼ä¸è§£é‡Š**ï¼š<br>1. **ç¬¬ä¸€é¡¹**ï¼šå¯¹äºæºé›†åˆ `X` ä¸­çš„æ¯ä¸ªç‚¹ `(p_i, n_i)`ï¼Œåœ¨ç›®æ ‡é›†åˆ `X'` ä¸­å¯»æ‰¾æœ€åŒ¹é…çš„ç‚¹ `(p'_j, n'_j)`ã€‚åŒ¹é…ä»£ä»·ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š<br>   - `â€–p_i - p'_jâ€–â‚‚`ï¼šç©ºé—´æ¬§æ°è·ç¦»ï¼Œè¶Šå°è¶Šå¥½ã€‚<br>   - `-Î»Â·âŸ¨n_i, n'_jâŸ©`ï¼šæ³•å‘é‡ç‚¹ç§¯çš„è´Ÿå€¼ã€‚ç”±äºç‚¹ç§¯ `âŸ¨Â·,Â·âŸ©` è¶Šå¤§è¡¨ç¤ºæ–¹å‘è¶Šä¸€è‡´ï¼ŒåŠ ä¸Šè´Ÿå·åï¼Œæ•´ä½“ä»£ä»·è¶Šå°è¡¨ç¤ºæ–¹å‘è¶ŠåŒ¹é…ã€‚<br>2. **ç¬¬äºŒé¡¹**ï¼šå¯¹ç§°åœ°ï¼Œä»ç›®æ ‡é›†åˆå‘æºé›†åˆè¿›è¡ŒåŒ¹é…ï¼Œç¡®ä¿åº¦é‡çš„å¯¹ç§°æ€§ã€‚<br>3. **æœ€ç»ˆç›¸ä¼¼åº¦**ï¼šè®ºæ–‡å°† **`-DCD(X, X')`** å®šä¹‰ä¸ºåŠŸèƒ½ç›¸ä¼¼åº¦ï¼Œå³ `DCD` å€¼è¶Šå°ï¼ˆè´Ÿå€¼è¶Šå¤§ï¼‰ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ã€‚ | - **å¹³è¡¡å‚æ•° `Î»`**ï¼šç”¨äºæƒè¡¡ç©ºé—´è·ç¦»å’Œæ–¹å‘ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚è®ºæ–‡é€šè¿‡å®éªŒç¡®å®š `Î» = 0.5`ã€‚<br>- **è·ç¦»åº¦é‡**ï¼šä½¿ç”¨ L2 èŒƒæ•°è®¡ç®—ç©ºé—´è·ç¦»ã€‚<br>- **æ–¹å‘åº¦é‡**ï¼šä½¿ç”¨å‘é‡ç‚¹ç§¯è®¡ç®—æ–¹å‘ä¸€è‡´æ€§ã€‚ |\n| **3. åŸºäºæ¢¯åº¦çš„è½¨è¿¹å¯¹é½** | å°†æºæœºå™¨äººçš„æ¼”ç¤ºè½¨è¿¹ï¼ˆåŠŸèƒ½è¡¨ç¤ºåºåˆ— `{X_t}`ï¼‰é€šè¿‡ä¼˜åŒ–ï¼Œå¯¹é½åˆ°ç›®æ ‡æœºå™¨äººçš„å…³èŠ‚ç©ºé—´ï¼Œç”Ÿæˆç›®æ ‡è½¨è¿¹ `{q'_t}`ã€‚ | **ä¼˜åŒ–é—®é¢˜å®šä¹‰**ï¼š<br>å¯¹äºæ¯ä¸€æ—¶é—´æ­¥ `t`ï¼Œåˆå§‹åŒ–ç›®æ ‡æœºå™¨äººå…³èŠ‚é…ç½® `q'_t` ä¸ºå¯å­¦ä¹ å˜é‡ï¼Œé€šè¿‡æ­£å‘è¿åŠ¨å­¦ `FK_{E'}(q'_t)` å¾—åˆ°å…¶åŠŸèƒ½è¡¨ç¤º `X'_t`ã€‚æœ€å°åŒ–å¯¹é½æŸå¤±ï¼š<br>```math<br>min_{q'_t} â„’_align(q'_t) = wâ‚Â·DCD(X_t, X'_t) + wâ‚‚Â·â„’_reg(q'_t)<br>```<br>**æŸå¤±é¡¹åˆ†è§£**ï¼š<br>- **`DCD(X_t, X'_t)`**ï¼šé©±åŠ¨ç›®æ ‡æœºå™¨äººçš„åŠŸèƒ½è¡¨ç¤ºä¸æºæœºå™¨äººåœ¨å½“å‰å¸§çš„åŠŸèƒ½è¡¨ç¤ºç›¸ä¼¼ã€‚<br>- **`â„’_reg(q'_t)`**ï¼šæ­£åˆ™åŒ–é¡¹ï¼Œæƒ©ç½šè¶…å‡ºå…³èŠ‚é™ä½çš„é…ç½®ï¼Œç¡®ä¿ç”Ÿæˆçš„è½¨è¿¹åœ¨ç‰©ç†ä¸Šå¯è¡Œã€‚<br><br>**åºåˆ—ä¼˜åŒ–ç­–ç•¥**ï¼š<br>ä¸ºé¿å…é€å¸§ç‹¬ç«‹ä¼˜åŒ–å¯¼è‡´è½¨è¿¹ä¸è¿ç»­ï¼Œé‡‡ç”¨åºåˆ—åˆå§‹åŒ–ï¼š<br>`q'_{t+1}^{(0)} â† qÌ‚_t`<br>å³ç¬¬ `t+1` å¸§çš„ä¼˜åŒ–åˆå§‹å€¼ï¼Œä½¿ç”¨ç¬¬ `t` å¸§ä¼˜åŒ–åçš„ç»“æœ `qÌ‚_t`ã€‚è¿™ä¿è¯äº†è½¨è¿¹çš„æ—¶ç©ºå¹³æ»‘æ€§ï¼Œå¹¶åŠ é€Ÿæ”¶æ•›ã€‚ | - **ä¼˜åŒ–å™¨**ï¼šä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ã€‚<br>- **æŸå¤±æƒé‡**ï¼š`wâ‚ = 1`, `wâ‚‚ = 1`ã€‚<br>- **æ­£åˆ™é¡¹**ï¼š`â„’_reg` å…·ä½“å½¢å¼æœªåœ¨æ­£æ–‡è¯¦è¿°ï¼Œé€šå¸¸ä¸ºå¯¹è¶…å‡ºé™ä½çš„å…³èŠ‚æ–½åŠ çš„äºŒæ¬¡æƒ©ç½šã€‚<br>- **ä¼˜åŒ–æ­¥æ•°**ï¼šæ¯å¸§æœ€å¤§ä¼˜åŒ–300æ­¥ï¼Œè‹¥æŸå¤±è¿ç»­10æ­¥æ— æ”¹å–„åˆ™æå‰åœæ­¢ã€‚<br>- **æ­£å‘è¿åŠ¨å­¦å¼•æ“**ï¼šä½¿ç”¨ `pytorch_kinematics` å®ç°æ‰¹é‡å¯å¾®è®¡ç®—ã€‚ |\n| **4. è§‚æµ‹ä¸åŠ¨ä½œåˆæˆ** | æ ¹æ®å¯¹é½åçš„ç›®æ ‡æœºå™¨äººå…³èŠ‚è½¨è¿¹ `{qÌ‚_t}`ï¼Œç”Ÿæˆå¯ç”¨äºè®­ç»ƒç›®æ ‡æœºå™¨äººç­–ç•¥çš„å®Œæ•´æ¼”ç¤ºæ•°æ® `DÌ‚_{E', sâ‚€}`ã€‚ | **åŠ¨ä½œç”Ÿæˆ**ï¼š<br>`a_t = qÌ‚_{t+1}`<br>å°†åŠ¨ä½œå®šä¹‰ä¸ºä¸‹ä¸€å¸§çš„ç›®æ ‡å…³èŠ‚ä½ç½®ã€‚æ‰§è¡Œæ—¶é‡‡ç”¨é—­ç¯æ§åˆ¶ï¼ŒæŒç»­æ–½åŠ è¯¥åŠ¨ä½œç›´è‡³æœºå™¨äººåˆ°è¾¾ç›®æ ‡ä½ç½®ã€‚<br><br>**è§‚æµ‹ç”Ÿæˆ**ï¼š<br>1. **æœ¬ä½“æ„ŸçŸ¥è§‚æµ‹**ï¼šç›´æ¥å–è‡ªå¯¹é½åçš„è½¨è¿¹ï¼Œ`(oÌ‚_t^{arm}, oÌ‚_t^{ee}) = qÌ‚_t`ã€‚<br>2. **ç‚¹äº‘è§‚æµ‹åˆæˆ**ï¼š<br>   - **è£å‰ª**ï¼šç§»é™¤å·¥ä½œç©ºé—´å¤–çš„ç‚¹ï¼Œå¹¶**æ©ç **æ‰è·ç¦»æºæœºå™¨äººç½‘æ ¼è¡¨é¢å°äºé˜ˆå€¼ `Ï„` çš„ç‚¹ï¼ˆé˜²æ­¢æºæœºå™¨äººé®æŒ¡ç›®æ ‡è§†è§’ï¼‰ã€‚<br>   - **å¢å¼º**ï¼šæ ¹æ®å½“å‰å…³èŠ‚çŠ¶æ€ `qÌ‚_t`ï¼Œä»ç›®æ ‡æœºå™¨äººç½‘æ ¼è¡¨é¢é‡‡æ ·ç‚¹ï¼Œæ·»åŠ åˆ°ç‚¹äº‘ä¸­ã€‚<br>   - **é™é‡‡æ ·**ï¼šä½¿ç”¨æœ€è¿œç‚¹é‡‡æ ·å°†ç‚¹äº‘å›ºå®šåˆ°1024ä¸ªç‚¹ã€‚ | - **è·ç¦»é˜ˆå€¼ `Ï„`**ï¼šä¾‹å¦‚ 5 mmï¼Œç”¨äºç¡®å®šéœ€è¦æ©ç çš„æºæœºå™¨äººé™„è¿‘ç‚¹äº‘åŒºåŸŸã€‚<br>- **ç‚¹äº‘å¤§å°**ï¼šå›ºå®šä¸º `[1024, 3]`ã€‚<br>- **æ¨ç†æ—¶ä¸€è‡´æ€§**ï¼šæµ‹è¯•æ—¶å¯¹çœŸå®è§‚æµ‹åº”ç”¨ç›¸åŒçš„æ©ç å’Œå¢å¼ºæµç¨‹ï¼Œç¡®ä¿è®­ç»ƒä¸æµ‹è¯•çš„è§‚æµ‹åˆ†å¸ƒä¸€è‡´ã€‚ |\n| **5. æ‰©å±•åº”ç”¨ï¼šç©ºé—´æ³›åŒ–** | ä»å•æ¡æ¼”ç¤ºè½¨è¿¹ï¼Œé€šè¿‡ç©ºé—´å˜æ¢ç”Ÿæˆè¦†ç›–æ•´ä¸ªå·¥ä½œç©ºé—´çš„å¤šæ ·åŒ–æ¼”ç¤ºæ•°æ®ã€‚ | ç»™å®šä¸€ä¸ªç©ºé—´å˜æ¢ `ğ’¯_i`ï¼ˆå¦‚å¹³ç§»ï¼‰ï¼Œé¦–å…ˆå¯¹æºåŠŸèƒ½è¡¨ç¤ºè½¨è¿¹è¿›è¡Œ**æ¸è¿›å¼æ’å€¼**ï¼š<br>```math<br>XÌƒ_t = X_t + â„’(t)Â·(ğ’¯_i(X_t) - X_t), t=0,1,...,L-1<br>```<br>å…¶ä¸­ `â„’(t) = min(t/(0.8L), 1)` æ˜¯ä¸€ä¸ªè£å‰ªçº¿æ€§å¢é•¿å‡½æ•°ã€‚<br>**åŸç†**ï¼š<br>- åœ¨è½¨è¿¹åˆæœŸ (`t` è¾ƒå°)ï¼Œ`â„’(t)â‰ˆ0`ï¼Œ`XÌƒ_t â‰ˆ X_t`ï¼Œä¿è¯æ‰€æœ‰ç”Ÿæˆè½¨è¿¹çš„**åˆå§‹çŠ¶æ€ç›¸åŒ**ã€‚<br>- åœ¨è½¨è¿¹åæœŸ (`t` æ¥è¿‘ `L`)ï¼Œ`â„’(t)=1`ï¼Œ`XÌƒ_t = ğ’¯_i(X_t)`ï¼Œä½¿å¾—è½¨è¿¹**æœ€ç»ˆçŠ¶æ€**æ ¹æ®å˜æ¢ `ğ’¯_i` å‘ç”Ÿå˜åŒ–ã€‚<br>- éšåï¼Œä½¿ç”¨æ ‡å‡†çš„CEIè½¨è¿¹å¯¹é½æµç¨‹ï¼Œå°†ç›®æ ‡æœºå™¨äººä¸è¿™ä¸ªâ€œç©ºé—´å¢å¼ºåâ€çš„è½¨è¿¹ `{XÌƒ_t}` å¯¹é½ï¼Œå¹¶åˆæˆç›¸åº”çš„ç‚¹äº‘ï¼ˆå¯¹ç‰©ä½“ç‚¹äº‘åº”ç”¨ `ğ’¯_i`ï¼‰ã€‚ | - **æ’å€¼å‡½æ•°**ï¼šä½¿ç”¨åœ¨80%è½¨è¿¹é•¿åº¦å¤„é¥±å’Œçš„çº¿æ€§å‡½æ•°ï¼Œç¡®ä¿ä»»åŠ¡ååŠæ®µå®Œå…¨æ‰§è¡Œå˜æ¢åçš„åŠ¨ä½œã€‚ |\n| **6. æ‰©å±•åº”ç”¨ï¼šå¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆ** | åˆ©ç”¨åŠŸèƒ½ç›¸ä¼¼æ€§åº¦é‡çš„å¤šè§£ç‰¹æ€§ï¼Œä»å•æ¡æºæ¼”ç¤ºç”Ÿæˆå¤šç§å¯è¡Œçš„ç›®æ ‡æœºå™¨äººè¿åŠ¨æ¨¡å¼ã€‚ | **ç²¾è‹±åˆå§‹åŒ–ç­–ç•¥**ï¼š<br>1. ä»ç›®æ ‡æœºå™¨äººçš„å…³èŠ‚ç©ºé—´ä¸­å‡åŒ€é‡‡æ ·å¤§é‡åˆå§‹é…ç½®å€™é€‰ã€‚<br>2. è®¡ç®—æ¯ä¸ªå€™é€‰é…ç½®çš„åŠŸèƒ½è¡¨ç¤ºä¸æºæœºå™¨äººåˆå§‹åŠŸèƒ½è¡¨ç¤ºçš„ `-",
            "timestamp": "2026-01-20T18:00:45.959228",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡é‡‡ç”¨çš„æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡æ˜¯**ä»»åŠ¡æˆåŠŸç‡**ï¼Œå¹¶åœ¨ä¸åŒå®éªŒè®¾ç½®ä¸‹è¿›è¡Œäº†ç»†åŒ–å’Œå¯¹æ¯”ã€‚å…·ä½“æŒ‡æ ‡ç»†èŠ‚å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| è¯„ä»·æŒ‡æ ‡ | è®¡ç®—æ–¹å¼ä¸ç»†èŠ‚ | å®éªŒåœºæ™¯ | ä½œç”¨ä¸è¯´æ˜ |\n| :--- | :--- | :--- | :--- |\n| **ä»»åŠ¡æˆåŠŸç‡** | æˆåŠŸå®Œæˆä»»åŠ¡çš„è¯•éªŒæ¬¡æ•° / æ€»è¯•éªŒæ¬¡æ•°ã€‚ | ä»¿çœŸä¸çœŸå®ä¸–ç•Œå®éªŒçš„æ ¸å¿ƒæŒ‡æ ‡ã€‚ | ç›´æ¥è¡¡é‡**è·¨å…·èº«æ•°æ®åˆæˆ**å’Œ**ç­–ç•¥å­¦ä¹ **çš„æœ‰æ•ˆæ€§ã€‚åœ¨ä»¿çœŸä¸­é€šè¿‡æ¨¡æ‹Ÿå™¨å›æ”¾éªŒè¯è½¨è¿¹å¯è¡Œæ€§ï¼Œåœ¨çœŸå®ä¸–ç•Œä¸­é€šè¿‡ç­–ç•¥éƒ¨ç½²è¯„ä¼°æœ€ç»ˆæ€§èƒ½ã€‚ |\n| **è·¨å…·èº«è½¬ç§»ç‡** | (ç›®æ ‡å…·èº«ç­–ç•¥æˆåŠŸç‡) / (æºå…·èº«ç­–ç•¥æˆåŠŸç‡) Ã— 100%ã€‚ | çœŸå®ä¸–ç•ŒåŒå‘è½¬ç§»å®éªŒï¼ˆè¡¨ Vï¼‰ã€‚ | é‡åŒ–**CEIæ¡†æ¶çš„è½¬ç§»æ•ˆç‡**ï¼Œè¡¡é‡åœ¨å…‹æœå…·èº«å·®å¼‚åï¼Œç›®æ ‡æœºå™¨äººèƒ½ä¿ç•™æºæœºå™¨äººæ€§èƒ½çš„ç™¾åˆ†æ¯”ã€‚è®ºæ–‡ä¸­å¹³å‡è¾¾åˆ° **82.4%**ã€‚ |\n| **æ—¶é—´æˆæœ¬** | ç”ŸæˆæŒ‡å®šæ•°é‡æ¼”ç¤ºæ‰€éœ€çš„æ€»æ—¶é—´ï¼ˆç§’/åˆ†é’Ÿ/å°æ—¶ï¼‰ã€‚ | çœŸå®ä¸–ç•Œæ•°æ®ç”Ÿæˆæ•ˆç‡å¯¹æ¯”ï¼ˆè¡¨ VIï¼‰ã€‚ | è¯„ä¼°**CEIçš„ç¦»çº¿æ•°æ®ç”Ÿæˆæ•ˆç‡**ï¼Œä¸åœ¨çº¿æ–¹æ³•ï¼ˆMimicGenï¼‰å’Œå¿«é€Ÿè½¨è¿¹å˜æ¢æ–¹æ³•ï¼ˆDemoGenï¼‰è¿›è¡Œå¯¹æ¯”ã€‚ |\n| **å¯¹é½æŸå¤±** | å…¬å¼(2): `â„’_align = w1 * DCD(X_t, X'_t) + w2 * â„’(q'_t)`ã€‚ | è½¨è¿¹å¯¹é½ä¼˜åŒ–è¿‡ç¨‹ã€‚ | **å†…éƒ¨ä¼˜åŒ–æŒ‡æ ‡**ï¼Œç”¨äºæ¢¯åº¦ä¸‹é™ï¼Œç¡®ä¿ç›®æ ‡æœºå™¨äººçš„åŠŸèƒ½è¡¨ç¤ºä¸æºæœºå™¨äººå¯¹é½ï¼ŒåŒæ—¶æ»¡è¶³å…³èŠ‚é™åˆ¶ã€‚DCDï¼ˆå®šå‘å€’è§’è·ç¦»ï¼‰æ˜¯æ ¸å¿ƒç›¸ä¼¼åº¦åº¦é‡ã€‚ |\n\n**å…³é”®æŒ‡æ ‡æ·±åº¦è§£æï¼š**\n- **å®šå‘å€’è§’è·ç¦»**ï¼š è¿™æ˜¯**CEI**çš„æŠ€æœ¯æ ¸å¿ƒï¼Œç”¨äºé‡åŒ–â€œåŠŸèƒ½ç›¸ä¼¼æ€§â€ã€‚å®ƒä¸ä»…è®¡ç®—ç‚¹äº‘çš„ç©ºé—´è·ç¦»ï¼ˆ`||p_i - p'_j||_2`ï¼‰ï¼Œè¿˜åŠ å…¥äº†æ–¹å‘å¯¹é½é¡¹ï¼ˆ`-Î»Â·<n_i, n'_j>`ï¼‰ã€‚è¿™ä½¿å¾—åŒ¹é…è¿‡ç¨‹æ›´å…³æ³¨**æ¥è§¦ç‚¹çš„æ³•çº¿æ–¹å‘**ï¼Œå¯¹äºæŠ“å–ç­‰éœ€è¦ç‰¹å®šæ¥è§¦åŠ›å‘çš„ä»»åŠ¡è‡³å…³é‡è¦ï¼ˆè§æ¶ˆèå®éªŒè¡¨IIï¼‰ã€‚\n- **æˆåŠŸç‡çš„åˆ†å±‚è¯„ä¼°**ï¼š å®éªŒè®¾è®¡å·§å¦™åœ°åŒºåˆ†äº† **â€œè½¨è¿¹å¯è¡Œæ€§â€**ï¼ˆä»¿çœŸä¸­ç›´æ¥å›æ”¾åˆæˆè½¨è¿¹ï¼‰å’Œ **â€œç­–ç•¥æ€§èƒ½â€**ï¼ˆç”¨åˆæˆæ•°æ®è®­ç»ƒç­–ç•¥å¹¶è¯„ä¼°ï¼‰ã€‚è¿™åˆ†åˆ«éªŒè¯äº†**æ•°æ®åˆæˆè´¨é‡**å’Œ**ç­–ç•¥å­¦ä¹ æ•ˆæœ**ã€‚\n\n### äºŒã€ ä½¿ç”¨çš„æ•°æ®é›†\n\nè®ºæ–‡å·¥ä½œä¸»è¦åŸºäº**è‡ªè¡Œæ”¶é›†çš„æ¼”ç¤ºæ•°æ®**ï¼Œå¹¶åˆ©ç”¨**CEI**å¯¹å…¶è¿›è¡Œè·¨å…·èº«åˆæˆï¼Œè€Œéç›´æ¥ä½¿ç”¨ç°æœ‰å¤§å‹æ•°æ®é›†ã€‚æ•°æ®ç»†èŠ‚å¦‚ä¸‹ï¼š\n\n| æ•°æ®é›†ç±»å‹ | æºæœºå™¨äºº | ä»»åŠ¡ä¸æ•°é‡ | æ•°æ®å†…å®¹ä¸æ ¼å¼ | ç”¨é€” |\n| :--- | :--- | :--- | :--- | :--- |\n| **ä»¿çœŸæºæ•°æ®** | Franka Panda (å¸¦å¹³è¡Œå¤¹çˆª) | 3ä¸ªä»»åŠ¡ï¼šOpenDrawer, PickCube, StackCubeã€‚é€šè¿‡é¥æ“ä½œæ”¶é›†æ¼”ç¤ºã€‚ | - **è§‚æµ‹**ï¼š ç‚¹äº‘ (`o_t^pcd`)ã€æœºæ¢°è‡‚å…³èŠ‚çŠ¶æ€ (`o_t^arm`)ã€æœ«ç«¯æ‰§è¡Œå™¨çŠ¶æ€ (`o_t^ee`)ã€‚<br>- **åŠ¨ä½œ**ï¼š æœºæ¢°è‡‚å’Œæœ«ç«¯æ‰§è¡Œå™¨çš„ç›®æ ‡å…³èŠ‚ä½ç½® (`a_t^arm`, `a_t^ee`)ã€‚ | ä½œä¸º**æºæ¼”ç¤º**ï¼Œç”¨äºå‘**16ç§**ç›®æ ‡å…·èº«ï¼ˆ4ç§æœºæ¢°è‡‚ Ã— 4ç§æœ«ç«¯æ‰§è¡Œå™¨ï¼‰è¿›è¡Œè½¬ç§»å’Œç­–ç•¥è®­ç»ƒã€‚ |\n| **çœŸå®ä¸–ç•Œæºæ•°æ®** | 1. UR5 + AG95 å¹³è¡Œå¤¹çˆª<br>2. UR5 + Xhand äº”æŒ‡çµå·§æ‰‹ | 6ä¸ªä»»åŠ¡ï¼ŒåŒå‘è½¬ç§»ï¼š<br>- AG95â†’Xhand: PushCube, OpenDrawer, PlaceBird (å„25æ¡æ¼”ç¤º)<br>- Xhandâ†’AG95: PickCup, PackageBread, InsertFlower (å„25æ¡æ¼”ç¤º) | - **è§‚æµ‹**ï¼š ç”±RealSense L515 RGB-Dç›¸æœºç”Ÿæˆçš„åœºæ™¯ç‚¹äº‘ï¼Œä»¥åŠæœºå™¨äººæœ¬ä½“æ„ŸçŠ¶æ€ã€‚<br>- **åŠ¨ä½œ**ï¼š å…³èŠ‚ä½ç½®ç›®æ ‡ã€‚é€šè¿‡é”®ç›˜é¥æ“ä½œæ”¶é›†ã€‚ | ç”¨äºéªŒè¯**CEIåœ¨çœŸå®å¤æ‚åœºæ™¯ä¸‹çš„åŒå‘è·¨å…·èº«è½¬ç§»èƒ½åŠ›**ï¼Œä»ç®€å•å¤¹çˆªåˆ°çµå·§æ‰‹ï¼Œåä¹‹äº¦ç„¶ã€‚ |\n| **åˆæˆæ•°æ®ï¼ˆç”±CEIç”Ÿæˆï¼‰** | å¤šç§ç›®æ ‡æœºå™¨äººï¼ˆä»¿çœŸ16ç§ï¼ŒçœŸå®ä¸–ç•Œ2ç§ï¼‰ | ä¸æºä»»åŠ¡å¯¹åº”ï¼Œé€šè¿‡**CEI**æ¡†æ¶è‡ªåŠ¨åˆæˆã€‚ | æ ¼å¼ä¸æºæ•°æ®ä¸€è‡´ï¼ŒåŒ…å«ä¸ºç›®æ ‡æœºå™¨äººåˆæˆçš„ç‚¹äº‘è§‚æµ‹ã€å…³èŠ‚çŠ¶æ€è§‚æµ‹å’Œå…³èŠ‚åŠ¨ä½œã€‚ | 1. **ç›´æ¥è¯„ä¼°**ï¼šåœ¨ä»¿çœŸä¸­å›æ”¾ï¼ŒéªŒè¯åˆæˆè½¨è¿¹çš„å¯è¡Œæ€§ï¼ˆè¡¨Iï¼‰ã€‚<br>2. **ç­–ç•¥è®­ç»ƒ**ï¼šä½œä¸ºè®­ç»ƒé›†ï¼Œç”¨äºè®­ç»ƒ3D Diffusion Policyï¼Œè¯„ä¼°æœ€ç»ˆç­–ç•¥æ€§èƒ½ï¼ˆè¡¨IV, Vï¼‰ã€‚ |\n| **ç©ºé—´æ³›åŒ–åˆæˆæ•°æ®** | AG95 â†’ Xhand | PressButtonä»»åŠ¡ã€‚ä»**1æ¡**æºæ¼”ç¤ºï¼Œé€šè¿‡ç©ºé—´å˜æ¢ç”Ÿæˆ**1000æ¡**æ¼”ç¤ºï¼Œåˆ†å¸ƒåœ¨10ä¸ªé”šç‚¹å‘¨å›´çš„10x10ç½‘æ ¼ä¸Šã€‚ | åº”ç”¨ç©ºé—´å˜æ¢ `ğ’¯_i` å¯¹åŠŸèƒ½è¡¨ç¤ºè½¨è¿¹å’Œç‰©ä½“ç‚¹äº‘è¿›è¡Œæ’å€¼ï¼Œå†åˆ©ç”¨CEIç”Ÿæˆå¯¹åº”ç›®æ ‡æœºå™¨äººçš„å…¨å¥—æ•°æ®ã€‚ | ç”¨äºéªŒè¯**CEIæ”¯æŒä»å•æ¡æ¼”ç¤ºè¿›è¡Œå¤§è§„æ¨¡ç©ºé—´æ•°æ®å¢å¼º**çš„èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡ç­–ç•¥çš„å·¥ä½œç©ºé—´æ³›åŒ–æ€§èƒ½ï¼ˆå›¾6ï¼‰ã€‚ |\n\n**æ•°æ®é›†å…³é”®ç‚¹åˆ†æï¼š**\n- **è§£å†³æ•°æ®åå·®é—®é¢˜**ï¼š å½“å‰å¤§å‹æœºå™¨äººæ•°æ®é›†ï¼ˆå¦‚OXEï¼‰å­˜åœ¨ä¸¥é‡åå·®ï¼Œè¿‡åº¦é›†ä¸­äºFranka/xArmå’Œå¹³è¡Œå¤¹çˆªã€‚**CEI**çš„åˆ›æ–°åœ¨äºï¼Œ**æ— éœ€ä¸ºç›®æ ‡å…·èº«ï¼ˆå°¤å…¶æ˜¯ç¨€ç¼ºçš„çµå·§æ‰‹ï¼‰æ”¶é›†ä»»ä½•çœŸå®æ•°æ®**ï¼Œå³å¯ä»ä¸°å¯Œçš„å¤¹çˆªæ•°æ®ä¸­åˆæˆå‡ºå¯ç”¨æ¼”ç¤ºã€‚\n- **ä»¿çœŸåˆ°çœŸå®çš„ä¸€è‡´æ€§**ï¼š åœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­ï¼Œæ•°æ®æ¨¡æ€ä¿æŒç»Ÿä¸€ï¼ˆä»¥**ç‚¹äº‘**ä¸ºæ ¸å¿ƒè§†è§‰è§‚æµ‹ï¼‰ï¼Œç¡®ä¿äº†æ–¹æ³•éªŒè¯çš„è¿è´¯æ€§å’Œç»“è®ºçš„å¯é æ€§ã€‚\n- **ä»»åŠ¡è®¾è®¡å…·æœ‰æ¢¯åº¦éš¾åº¦**ï¼š ä»ç®€å•çš„æ¨åŠ¨ï¼ˆPushCubeï¼‰åˆ°éœ€è¦ç¨³å®šæŠ“å–å’Œæ”¾ç½®çš„å¤æ‚ä»»åŠ¡ï¼ˆPlaceBird, InsertFlowerï¼‰ï¼Œä»»åŠ¡éš¾åº¦é€’å¢ï¼Œå…¨é¢æ£€éªŒäº†æ–¹æ³•åœ¨**ä¸åŒæ¥è§¦ä¸°å¯Œåº¦**åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼ˆå¦‚è¡¨Iç»“æœåˆ†ææ‰€ç¤ºï¼‰ã€‚",
            "timestamp": "2026-01-20T18:01:31.016634",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ä¸è¯„ä¼°æŒ‡æ ‡åˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨**ä»¿çœŸ**å’Œ**çœŸå®ä¸–ç•Œ**ä¸¤ç§ç¯å¢ƒä¸‹è¿›è¡Œäº†å®éªŒï¼Œå…·ä½“æ•°æ®é›†æ„å»ºå¦‚ä¸‹ï¼š\n\n#### 1. **ä»¿çœŸå®éªŒæ•°æ®é›†**\n- **ä»»åŠ¡**ï¼š3ä¸ªåŸºç¡€æ“ä½œä»»åŠ¡  \n  - `OpenDrawer`ï¼ˆæ‰“å¼€æŠ½å±‰ï¼‰  \n  - `PickCube`ï¼ˆæŠ“å–ç«‹æ–¹ä½“ï¼‰  \n  - `StackCube`ï¼ˆå †å ç«‹æ–¹ä½“ï¼‰  \n- **æœºå™¨äººå½¢æ€**ï¼š16ç§ä¸åŒçš„æœºå™¨äººç»„åˆ  \n  - **æœºæ¢°è‡‚**ï¼šUR5eã€IIWAã€Kinova3ã€Franka Pandaï¼ˆå…±4ç§ï¼‰  \n  - **æœ«ç«¯æ‰§è¡Œå™¨**ï¼šFourierRightHandã€InspireRightHandã€FourierLeftHandã€RobotiqThreeFingerï¼ˆå…±4ç§ï¼‰  \n- **æ•°æ®æ¥æº**ï¼šä½¿ç”¨Franka Pandaæœºå™¨äººé€šè¿‡é¥æ“ä½œæ”¶é›†æ¼”ç¤ºæ•°æ®ï¼Œå¹¶é€šè¿‡CEIæ¡†æ¶è¿ç§»åˆ°å…¶ä»–15ç§æœºå™¨äººå½¢æ€ã€‚\n\n#### 2. **çœŸå®ä¸–ç•Œå®éªŒæ•°æ®é›†**\n- **ä»»åŠ¡**ï¼š6ä¸ªæ“ä½œä»»åŠ¡ï¼ˆåˆ†ä¸ºä¸¤ç»„ï¼‰  \n  - **AG95å¹³è¡Œå¤¹çˆª â†’ Xhandçµå·§æ‰‹**ï¼š  \n    - `PushCube`ï¼ˆæ¨åŠ¨ç«‹æ–¹ä½“ï¼‰  \n    - `OpenDrawer`ï¼ˆæ‰“å¼€æŠ½å±‰ï¼‰  \n    - `PlaceBird`ï¼ˆæ”¾ç½®ç©å…·é¸Ÿï¼‰  \n  - **Xhandçµå·§æ‰‹ â†’ AG95å¹³è¡Œå¤¹çˆª**ï¼š  \n    - `PickCup`ï¼ˆæŠ“å–æ¯å­ï¼‰  \n    - `PackageBread`ï¼ˆåŒ…è£…é¢åŒ…ï¼‰  \n    - `InsertFlower`ï¼ˆæ’å…¥èŠ±æŸï¼‰  \n- **æœºå™¨äººå¹³å°**ï¼šUR5æœºæ¢°è‡‚ + AG95å¤¹çˆª / Xhandçµå·§æ‰‹  \n- **æ•°æ®æ”¶é›†**ï¼šé€šè¿‡é”®ç›˜é¥æ“ä½œæ”¶é›†25æ¡æ¼”ç¤ºè½¨è¿¹ï¼Œä½¿ç”¨RGB-Dç›¸æœºï¼ˆRealSense L515ï¼‰ç”Ÿæˆåœºæ™¯ç‚¹äº‘ã€‚\n\n---\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\n1. **æˆåŠŸç‡ï¼ˆSuccess Rateï¼‰**  \n   - åœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œä¸­ï¼Œé€šè¿‡**åœ¨çº¿å›æ”¾åˆæˆè½¨è¿¹**æˆ–**éƒ¨ç½²è®­ç»ƒå¥½çš„ç­–ç•¥**æ¥è¯„ä¼°ä»»åŠ¡å®Œæˆæƒ…å†µã€‚\n   - ä»¿çœŸï¼šæ¯ä¸ªä»»åŠ¡-æœºå™¨äººç»„åˆè¿›è¡Œ20æ¬¡è¯•éªŒï¼Œè®¡ç®—å¹³å‡æˆåŠŸç‡ã€‚\n   - çœŸå®ä¸–ç•Œï¼šæ¯ä¸ªä»»åŠ¡è¿›è¡Œ10æ¬¡è¯•éªŒï¼Œè®¡ç®—æˆåŠŸç‡ã€‚\n\n2. **è¿ç§»æ¯”ç‡ï¼ˆTransfer Ratioï¼‰**  \n   - å®šä¹‰ï¼š`è¿ç§»åæˆåŠŸç‡ / æºæœºå™¨äººæˆåŠŸç‡`ã€‚\n   - ç”¨äºé‡åŒ–è·¨å½¢æ€è¿ç§»çš„æ€§èƒ½ä¿æŒç¨‹åº¦ã€‚\n\n3. **æ—¶é—´æˆæœ¬ï¼ˆTime Costï¼‰**  \n   - æµ‹é‡ç”Ÿæˆ100â€“400æ¡æ¼”ç¤ºæ‰€éœ€çš„æ—¶é—´ï¼Œä¸åŸºçº¿æ–¹æ³•ï¼ˆMimicGenã€DemoGenï¼‰å¯¹æ¯”ã€‚\n\n---\n\n### ä¸‰ã€æ€§èƒ½æå‡ä¸å…³é”®ç»“æœ\n\n#### 1. **ä»¿çœŸå®éªŒç»“æœ**\n- **æ•´ä½“æˆåŠŸç‡**ï¼šCEIåœ¨16ç§æœºå™¨äººå½¢æ€ä¸Šå¹³å‡æˆåŠŸç‡è¾¾åˆ°**62%**ï¼ˆç­–ç•¥è¯„ä¼°ç»“æœï¼‰ã€‚\n- **è·¨å½¢æ€è¿ç§»æœ‰æ•ˆæ€§**ï¼š  \n  - ä»å¹³è¡Œå¤¹çˆªåˆ°çµå·§æ‰‹çš„è¿ç§»ä¸­ï¼Œ`OpenDrawer`ä»»åŠ¡æˆåŠŸç‡æœ€é«˜ï¼ˆå¹³å‡95%ï¼‰ï¼Œ`StackCube`è¾ƒä½ï¼ˆå¹³å‡32%ï¼‰ï¼Œè¯´æ˜ä»»åŠ¡æ¥è§¦å¤æ‚åº¦å½±å“è¿ç§»éš¾åº¦ã€‚\n  - **æ¶ˆèå®éªŒ**ï¼šCEIå®Œæ•´æ–¹æ³•ç›¸æ¯”**æ— æ–¹å‘ä¿¡æ¯**çš„ç‰ˆæœ¬ï¼ŒæˆåŠŸç‡æå‡çº¦ä¸€å€ï¼ˆ65% vs. 32%ï¼‰ï¼›ç›¸æ¯”**æ‰‹åŠ¨çº¿æ€§æ’å€¼ï¼ˆBMSï¼‰**ï¼Œåœ¨æŠ“å–ä»»åŠ¡ä¸Šä»0%æå‡è‡³65%ã€‚\n\n#### 2. **çœŸå®ä¸–ç•Œå®éªŒç»“æœ**\n- **åŒå‘è¿ç§»æˆåŠŸç‡**ï¼š  \n  - AG95 â†’ Xhandï¼šå¹³å‡æˆåŠŸç‡ **83%**ï¼ˆ25/30ï¼‰  \n  - Xhand â†’ AG95ï¼šå¹³å‡æˆåŠŸç‡ **57%**ï¼ˆ17/30ï¼‰  \n  - **æ•´ä½“å¹³å‡æˆåŠŸç‡**ï¼š**70%**ï¼ˆ42/60ï¼‰\n- **è¿ç§»æ¯”ç‡**ï¼š**82.4%**ï¼ˆå³è¿ç§»åæ€§èƒ½è¾¾åˆ°æºæœºå™¨äººæ€§èƒ½çš„82.4%ï¼‰ã€‚\n- **æŒ‘æˆ˜ä»»åŠ¡**ï¼š`InsertFlower`å› ç‰©ä½“ç»†å°æ˜“æ»‘è½ï¼ŒæˆåŠŸç‡æœ€ä½ï¼ˆAG95: 40%, Xhand: 20%ï¼‰ã€‚\n\n#### 3. **æ•ˆç‡å¯¹æ¯”**\n- **æ•°æ®ç”Ÿæˆæ—¶é—´**ï¼š  \n  - CEIç”Ÿæˆ100æ¡æ¼”ç¤ºä»…éœ€ **2.5åˆ†é’Ÿ**ï¼Œè¿œå¿«äºåœ¨çº¿æ–¹æ³•MimicGenï¼ˆ6.4å°æ—¶ï¼‰ï¼Œä½†æ…¢äºçº¯è½¨è¿¹å˜æ¢æ–¹æ³•DemoGenï¼ˆ0.29ç§’ï¼‰ã€‚\n\n#### 4. **æ‰©å±•åŠŸèƒ½éªŒè¯**\n- **ç©ºé—´æ³›åŒ–**ï¼šåœ¨`PressButton`ä»»åŠ¡ä¸­ï¼Œé€šè¿‡å•æ¡æ¼”ç¤ºç”Ÿæˆ1000æ¡ç©ºé—´åˆ†å¸ƒæ•°æ®ï¼Œç­–ç•¥å¯è¦†ç›–æ¡Œé¢å¤§éƒ¨åˆ†åŒºåŸŸï¼ˆ80cmÃ—80cmï¼‰ã€‚\n- **å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆ**ï¼šåœ¨`OpenDrawer`ä»»åŠ¡ä¸­ç”Ÿæˆä¸¤ç§ä¸åŒçš„å¼€æŠ½å±‰åŠ¨ä½œæ¨¡å¼ï¼ŒæˆåŠŸç‡å‡ä¿æŒåœ¨70%ä»¥ä¸Šã€‚\n\n---\n\n### å››ã€æ ¸å¿ƒç»“è®º\n- **æŠ€æœ¯åˆ›æ–°ä»·å€¼**ï¼šCEIé€šè¿‡**åŠŸèƒ½ç›¸ä¼¼æ€§åº¦é‡**ä¸**æ¢¯åº¦ä¼˜åŒ–å¯¹é½**ï¼Œé¦–æ¬¡å®ç°äº†ä»å¹³è¡Œå¤¹çˆªåˆ°å¤šæŒ‡çµå·§æ‰‹çš„**æç«¯è·¨å½¢æ€æ¼”ç¤ºè¿ç§»**ï¼Œåœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œä¸­å¹³å‡è¿ç§»æ¯”ç‡è¶…è¿‡82%ã€‚\n- **å®é™…åº”ç”¨æ„ä¹‰**ï¼šå¤§å¹…é™ä½äº†ä¸ºæ–°å‹æœºå™¨äººæ”¶é›†æ¼”ç¤ºæ•°æ®çš„æˆæœ¬ï¼Œå¹¶ä¸º**æœºå™¨äººåŸºç¡€æ¨¡å‹**æä¾›äº†è·¨å½¢æ€æ³›åŒ–çš„æ•°æ®åˆæˆå·¥å…·ã€‚",
            "timestamp": "2026-01-20T18:02:06.382242",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„åˆ†æï¼Œ**CEI** æ¡†æ¶è™½ç„¶å–å¾—äº†æ˜¾è‘—çš„è·¨å…·èº«å­¦ä¹ æ•ˆæœï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹å‡ ä¸ªå…³é”®å±€é™æ€§ï¼š\n\n### 1. **å¯¹ç‰©ç†æ¥è§¦ä¸åŠ¨æ€äº¤äº’çš„å»ºæ¨¡ä¸è¶³**\n- **é—®é¢˜**ï¼šæ–¹æ³•åŸºäº**å‡ ä½•åŠŸèƒ½ç›¸ä¼¼æ€§**è¿›è¡Œè½¨è¿¹å¯¹é½ï¼Œä½†æœªæ˜¾å¼å»ºæ¨¡**æ¥è§¦åŠ›å­¦**ï¼ˆå¦‚æ‘©æ“¦åŠ›ã€æ»‘åŠ¨ã€åŠ›é—­åˆï¼‰ã€‚è¿™åœ¨æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡ï¼ˆå¦‚ `StackCube`ã€`InsertFlower`ï¼‰ä¸­è¡¨ç°æ˜æ˜¾ï¼ŒæˆåŠŸç‡ä¸‹é™ã€‚\n- **è¡¨ç°**ï¼šåœ¨éœ€è¦ç¨³å®šæŠ“æ¡æˆ–ç²¾ç»†æ“ä½œçš„ä»»åŠ¡ä¸­ï¼Œåˆæˆçš„è½¨è¿¹å¯èƒ½åœ¨ç‰©ç†ä»¿çœŸæˆ–ç°å®ä¸­å› æ»‘åŠ¨ã€ç¢°æ’è€Œå¤±è´¥ã€‚\n- **ä¾‹è¯**ï¼š`InsertFlower` ä»»åŠ¡ä¸­ï¼ŒèŠ±èŒç»†æ»‘ï¼Œä»…é å‡ ä½•å¯¹é½æ— æ³•ä¿è¯ç¨³å®šæŠ“æ¡ï¼Œå¯¼è‡´æˆåŠŸç‡ä½ã€‚\n\n### 2. **ä¾èµ–è§†è§‰-è¿åŠ¨å­¦è¾“å…¥ï¼Œç¼ºä¹å¤šæ¨¡æ€æ„ŸçŸ¥**\n- **é—®é¢˜**ï¼šå½“å‰æ¡†æ¶ä»…ä½¿ç”¨**ç‚¹äº‘**å’Œ**å…³èŠ‚çŠ¶æ€**ä½œä¸ºè¾“å…¥ï¼Œç¼ºä¹**è§¦è§‰ä¼ æ„Ÿ**æˆ–**åŠ›åé¦ˆ**ä¿¡æ¯ã€‚\n- **å½±å“**ï¼šæ— æ³•å®æ—¶æ£€æµ‹å’Œé€‚åº”ä¸ç¨³å®šçš„æ¥è§¦çŠ¶æ€ï¼ˆå¦‚æ»‘åŠ¨ï¼‰ï¼Œé™åˆ¶äº†åœ¨åŠ¨æ€ã€éç»“æ„åŒ–ç¯å¢ƒä¸­çš„é²æ£’æ€§ã€‚\n\n### 3. **è®¡ç®—æ•ˆç‡ä¸å¯æ‰©å±•æ€§æŒ‘æˆ˜**\n- **é—®é¢˜**ï¼šè™½ç„¶æ¯”åœ¨çº¿ç”Ÿæˆæ–¹æ³•ï¼ˆå¦‚ `MimicGen`ï¼‰å¿«ï¼Œä½†**åŸºäºæ¢¯åº¦çš„è½¨è¿¹å¯¹é½**ä»éœ€æ•°åˆ†é’Ÿç”Ÿæˆæ•°ç™¾æ¡ç¤ºæ•™ï¼Œéš¾ä»¥ç›´æ¥åº”ç”¨äº**è¶…å¤§è§„æ¨¡æ•°æ®é›†**ï¼ˆå¦‚ç™¾ä¸‡çº§ï¼‰ã€‚\n- **ç»†èŠ‚**ï¼šæ¯å¸§éœ€ä¼˜åŒ–æœ€å¤š300æ­¥ï¼Œå°½ç®¡å·²æ‰¹é‡å¹¶è¡Œï¼Œä½†è®¡ç®—å¼€é”€ä»å¯èƒ½æˆä¸ºç“¶é¢ˆã€‚\n\n### 4. **ä»»åŠ¡ä¸å½¢æ€æ³›åŒ–çš„è¾¹ç•Œ**\n- **é—®é¢˜**ï¼šå®éªŒéªŒè¯é›†ä¸­äº**æŠ“å–ç±»ä»»åŠ¡**å’Œ**ç‰¹å®šå½¢æ€è½¬æ¢**ï¼ˆå¹³è¡Œå¤¹çˆª â†” çµå·§æ‰‹ï¼‰ã€‚å¯¹äºæ›´æç«¯çš„å½¢æ€å·®å¼‚ï¼ˆå¦‚å¤šè‡‚æœºå™¨äººã€éä»¿äººå½¢æ€ï¼‰æˆ–éæŠ“å–ä»»åŠ¡ï¼ˆå¦‚æ¨ã€æ‹§ï¼‰çš„æ³›åŒ–èƒ½åŠ›æœªå……åˆ†éªŒè¯ã€‚\n- **æ½œåœ¨é£é™©**ï¼šåŠŸèƒ½ç›¸ä¼¼æ€§å‡è®¾å¯èƒ½åœ¨æŸäº›ä»»åŠ¡-å½¢æ€ç»„åˆä¸­å¤±æ•ˆã€‚\n\n### 5. **åˆå§‹çŠ¶æ€ä¾èµ–ä¸è½¨è¿¹å¤šæ ·æ€§é™åˆ¶**\n- **é—®é¢˜**ï¼šè½¨è¿¹å¯¹é½ä¸¥é‡ä¾èµ–**æºç¤ºæ•™çš„åˆå§‹çŠ¶æ€**ã€‚è™½ç„¶è®ºæ–‡æå‡ºäº†**ç²¾è‹±åˆå§‹åŒ–ç­–ç•¥**æ¥å¢åŠ å¤šæ ·æ€§ï¼Œä½†ç”Ÿæˆçš„è¿åŠ¨æ¨¡å¼ä»å—é™äºæºè½¨è¿¹çš„å®è§‚ç»“æ„ã€‚\n- **è¡¨ç°**ï¼šå¤šæ¨¡æ€ç”Ÿæˆèƒ½åŠ›æœ‰é™ï¼Œå¯èƒ½æ— æ³•è¦†ç›–æ‰€æœ‰å¯è¡Œçš„ç›®æ ‡æœºå™¨äººè¿åŠ¨ç­–ç•¥ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\n### 1. **é›†æˆç‰©ç†æ„ŸçŸ¥ä¸è§¦è§‰åé¦ˆ**\n- **æ–¹å‘**ï¼šåœ¨åŠŸèƒ½ç›¸ä¼¼æ€§ä¼˜åŒ–ä¸­å¼•å…¥**æ¥è§¦åŠ›æ¨¡å‹**æˆ–**è§¦è§‰ä¿¡å·é¢„æµ‹**ã€‚\n- **æ–¹æ³•**ï¼š\n  - ä½¿ç”¨**ç‰©ç†ä»¿çœŸå™¨**æä¾›æ¥è§¦åŠ›ç›‘ç£ï¼Œæˆ–åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥åŠ›é—­åˆçº¦æŸã€‚\n  - èåˆ**è§¦è§‰ä¼ æ„Ÿå™¨æ•°æ®**ï¼ˆå¦‚ GelSightã€Digitï¼‰ï¼Œå®ç°â€œè§†è§‰-è§¦è§‰â€è·¨æ¨¡æ€å¯¹é½ã€‚\n- **é¢„æœŸä»·å€¼**ï¼šæå‡åœ¨æ»‘åŠ¨ã€æŸ”æ€§ç‰©ä½“æ“ä½œç­‰ä»»åŠ¡ä¸­çš„æˆåŠŸç‡å’Œé²æ£’æ€§ã€‚\n\n### 2. **å¼€å‘æ›´é«˜æ•ˆçš„å¯æ‰©å±•ç®—æ³•**\n- **æ–¹å‘**ï¼šåŠ é€Ÿè½¨è¿¹å¯¹é½è¿‡ç¨‹ï¼Œä½¿å…¶é€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®åˆæˆã€‚\n- **æ–¹æ³•**ï¼š\n  - é‡‡ç”¨**ç¥ç»ç½‘ç»œè¿‘ä¼¼**å¯¹é½è¿‡ç¨‹ï¼Œè®­ç»ƒä¸€ä¸ªâ€œå¯¹é½ç½‘ç»œâ€ç›´æ¥é¢„æµ‹ç›®æ ‡å…³èŠ‚è½¨è¿¹ã€‚\n  - åˆ©ç”¨**å…ƒå­¦ä¹ **æˆ–**å¿«é€Ÿè‡ªé€‚åº”**æŠ€æœ¯ï¼Œå‡å°‘å¯¹æ–°å½¢æ€çš„ä¼˜åŒ–æ­¥æ•°ã€‚\n- **é¢„æœŸä»·å€¼**ï¼šå®ç°åˆ†é’Ÿçº§ç”Ÿæˆæ•°åƒæ¡ç¤ºæ•™ï¼ŒåŠ©åŠ›åŸºç¡€æ¨¡å‹è®­ç»ƒã€‚\n\n### 3. **æ‰©å±•ä»»åŠ¡ä¸å½¢æ€æ³›åŒ–èŒƒå›´**\n- **æ–¹å‘**ï¼šéªŒè¯å¹¶æå‡æ¡†æ¶åœ¨æ›´å¹¿æ³›ä»»åŠ¡å’Œæœºå™¨äººå½¢æ€ä¸Šçš„æœ‰æ•ˆæ€§ã€‚\n- **æ–¹æ³•**ï¼š\n  - åœ¨æ›´å¤šæ ·åŒ–çš„**ä»»åŠ¡é›†**ï¼ˆå¦‚è£…é…ã€å·¥å…·ä½¿ç”¨ï¼‰å’Œ**æœºå™¨äººå½¢æ€é›†**ï¼ˆå¦‚ç§»åŠ¨æœºæ¢°è‡‚ã€å››è¶³æœºå™¨äººï¼‰ä¸Šè¿›è¡Œæµ‹è¯•ã€‚\n  - æ¢ç´¢**åˆ†å±‚åŠŸèƒ½è¡¨ç¤º**ï¼Œä»¥æ•æ‰ä¸åŒç²’åº¦ï¼ˆå¦‚æ•´ä½“å§¿æ€ã€æŒ‡å°–ç»†èŠ‚ï¼‰çš„ç›¸ä¼¼æ€§ã€‚\n- **é¢„æœŸä»·å€¼**ï¼šçœŸæ­£å®ç°â€œä»»ä½•ä»»åŠ¡ï¼Œä»»ä½•æœºå™¨äººâ€çš„é€šç”¨ç­–ç•¥è¿ç§»ã€‚\n\n### 4. **æ¢ç´¢RGBè§‚æµ‹çš„å…¼å®¹æ€§**\n- **æ–¹å‘**ï¼šå°†å½“å‰ç‚¹äº‘æ¡†æ¶æ‰©å±•è‡³ä¸»æµçš„**RGBè§‚æµ‹**ã€‚\n- **æ–¹æ³•**ï¼š\n  - åˆ©ç”¨**ç¥ç»æ¸²æŸ“**æŠ€æœ¯ï¼ˆå¦‚ NeRFã€Diffusionï¼‰åˆæˆç›®æ ‡æœºå™¨äººçš„RGBå›¾åƒã€‚\n  - ç»“åˆ**è·¨ç»˜ç”»**æŠ€æœ¯ï¼Œåœ¨åƒç´ çº§å¯¹é½ä¸åŒå½¢æ€çš„å¤–è§‚ã€‚\n- **é¢„æœŸä»·å€¼**ï¼šä¸ç°æœ‰åŸºäºRGBçš„å¤§è§„æ¨¡æ•°æ®é›†å’Œæ¨¡å‹ï¼ˆå¦‚ RT-Xï¼‰æ›´å¥½å…¼å®¹ã€‚\n\n### 5. **ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸ç¦»çº¿æ•°æ®ç”Ÿæˆ**\n- **æ–¹å‘**ï¼šå°† `CEI` åˆæˆçš„æ•°æ®ä½œä¸º**é«˜è´¨é‡åˆå§‹æ•°æ®é›†**ï¼Œç»“åˆ**ç¦»çº¿å¼ºåŒ–å­¦ä¹ **æˆ–**ç¤ºèŒƒå¢å¼ºRL**è¿›è¡Œç­–ç•¥å¾®è°ƒã€‚\n- **æ–¹æ³•**ï¼š\n  - ä½¿ç”¨ `CEI` æ•°æ®é¢„è®­ç»ƒç­–ç•¥ï¼Œå†åœ¨ç›®æ ‡ç¯å¢ƒä¸­è¿›è¡Œå°‘é‡**åœ¨çº¿äº¤äº’å¾®è°ƒ**ã€‚\n  - æ„å»º**åŠ¨åŠ›å­¦æ¨¡å‹**ï¼Œå¯¹åˆæˆè½¨è¿¹è¿›è¡Œç‰©ç†åˆç†æ€§ä¿®æ­£ã€‚\n- **é¢„æœŸä»·å€¼**ï¼šå¼¥è¡¥çº¯å‡ ä½•åˆæˆçš„ä¸è¶³ï¼Œæå‡æœ€ç»ˆç­–ç•¥åœ¨çœŸå®ç‰©ç†ä¸–ç•Œä¸­çš„æ€§èƒ½ã€‚\n\n### 6. **æ„å»ºå¼€æºåŸºå‡†ä¸å·¥å…·é“¾**\n- **æ–¹å‘**ï¼šå°† `CEI` æ¡†æ¶æ¨¡å—åŒ–ã€å¼€æºåŒ–ï¼Œå¹¶å»ºç«‹æ ‡å‡†è¯„ä¼°åŸºå‡†ã€‚\n- **æ–¹æ³•**ï¼š\n  - å‘å¸ƒ**è·¨å…·èº«æ•°æ®åˆæˆç®¡é“**çš„ä»£ç åº“ã€‚\n  - å®šä¹‰ä¸€å¥—æ ‡å‡†çš„**è·¨å…·èº«è½¬ç§»ä»»åŠ¡é›†ã€å½¢æ€é›†å’Œè¯„ä¼°åè®®**ã€‚\n- **é¢„æœŸä»·å€¼**ï¼šé™ä½ç¤¾åŒºç ”ç©¶é—¨æ§›ï¼ŒåŠ é€Ÿè·¨å…·èº«å­¦ä¹ é¢†åŸŸå‘å±•ã€‚\n\n---\n\n## æ€»ç»“\n`CEI` çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºäº†**åŠŸèƒ½ç›¸ä¼¼æ€§**è¿™ä¸€æ¦‚å¿µï¼Œå¹¶é€šè¿‡**åŸºäºæ¢¯åº¦çš„è½¨è¿¹å¯¹é½**å®ç°äº†ä»å¹³è¡Œå¤¹çˆªåˆ°çµå·§æ‰‹ç­‰æç«¯å½¢æ€é—´çš„ç­–ç•¥è¿ç§»ã€‚å…¶å½“å‰å±€é™æ€§ä¸»è¦æºäº**çº¯å‡ ä½•è§†è§’**å’Œ**é™æ€ä¼˜åŒ–èŒƒå¼**ã€‚æœªæ¥çš„æ”¹è¿›å°†å›´ç»•**å¼•å…¥ç‰©ç†æ„ŸçŸ¥**ã€**æå‡è®¡ç®—æ•ˆç‡**ã€**æ‰©å±•æ³›åŒ–è¾¹ç•Œ**ä»¥åŠ**èåˆå¤šæ¨¡æ€å­¦ä¹ **å±•å¼€ï¼Œæœ€ç»ˆç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªé«˜æ•ˆã€é²æ£’ã€é€šç”¨çš„æœºå™¨äººç­–ç•¥è¿ç§»ä¸æ•°æ®åˆæˆå¹³å°ã€‚",
            "timestamp": "2026-01-20T18:02:49.826315",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-14T05:07:11Z",
    "created_at": "2026-01-20T17:49:48.108427",
    "updated_at": "2026-01-20T17:49:48.108434"
}