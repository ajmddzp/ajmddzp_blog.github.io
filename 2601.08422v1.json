{
    "id": "2601.08422v1",
    "title": "Teaching Robots Like Dogs: Learning Agile Navigation from Luring, Gesture, and Speech",
    "authors": [
        "Taerim Yoon",
        "Dongho Kang",
        "Jin Cheng",
        "Fatemeh Zargarbashi",
        "Yijiang Huang",
        "Minsung Ahn",
        "Stelian Coros",
        "Sungjoon Choi"
    ],
    "abstract": "åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®©è…¿å¼æœºå™¨äººå­¦ä¼šè§£è¯»äººç±»ç¤¾äº¤ä¿¡å·ï¼Œå¹¶é€šè¿‡ç‰©ç†æ€§äººå·¥å¼•å¯¼äº§ç”Ÿæ°å½“è¡Œä¸ºã€‚ç„¶è€Œï¼Œå½“å­¦ä¹ è¿‡ç¨‹éœ€è¦å¤§é‡äººå·¥æä¾›çš„æ•°æ®æ—¶ï¼Œé€šè¿‡ç‰©ç†äº’åŠ¨è¿›è¡Œå­¦ä¹ ä¼šç»™ç”¨æˆ·å¸¦æ¥æ²‰é‡è´Ÿæ‹…ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§äººåœ¨å›è·¯æ¡†æ¶ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿä»¥æ•°æ®é«˜æ•ˆçš„æ–¹å¼è·å–å¯¼èˆªè¡Œä¸ºï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€è‡ªç„¶äººç±»è¾“å…¥ï¼ˆç‰¹åˆ«æ˜¯æ‰‹åŠ¿å’Œè¯­éŸ³æŒ‡ä»¤ï¼‰è¿›è¡Œæ§åˆ¶ã€‚æˆ‘ä»¬åŸºäºç‰©ç†ä»¿çœŸé‡å»ºäº¤äº’åœºæ™¯ï¼Œå¹¶é€šè¿‡æ•°æ®èšåˆæ¥ç¼“è§£æœ‰é™ç¤ºèŒƒæ•°æ®å¸¦æ¥çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºçš„æ¸è¿›å¼ç›®æ ‡æç¤ºç­–ç•¥èƒ½åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°æä¾›æ°å½“æŒ‡ä»¤å’Œå¯¼èˆªç›®æ ‡ï¼Œä»è€Œå®ç°æ›´ç²¾å‡†çš„å¯¼èˆªæ•ˆæœï¼Œå¹¶å¢å¼ºäººç±»è¾“å…¥ä¸æœºå™¨äººè¡Œä¸ºä¹‹é—´çš„åŒ¹é…åº¦ã€‚æˆ‘ä»¬åœ¨å…­ç§ç°å®ä¸–ç•Œçš„æ•æ·å¯¼èˆªåœºæ™¯ä¸­å¯¹æ¡†æ¶è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬è·¨è¶Šéšœç¢ç‰©æˆ–è§„é¿éšœç¢ç­‰åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å‡ ä¹æ‰€æœ‰æµ‹è¯•åœºæ™¯ä¸­å‡å–å¾—æˆåŠŸï¼Œåœ¨æ€»è®¡ä¸è¶³1å°æ—¶çš„ç¤ºèŒƒæ•°æ®æ¡ä»¶ä¸‹å®ç°äº†97.15%çš„ä»»åŠ¡æˆåŠŸç‡ã€‚",
    "url": "https://arxiv.org/abs/2601.08422v1",
    "html_url": "https://arxiv.org/html/2601.08422v1",
    "html_content": "Teaching Robots Like Dogs: Learning Agile Navigation from Luring, Gesture, and Speech\nTaerimÂ Yoon\n1\n,\nDonghoÂ Kang\n2\n,\nJinÂ Cheng\n2\n,\nFatemehÂ Zargarbashi\n2\n,\nYijiangÂ Huang\n2\n,\nMinsungÂ Ahn\n3\n,\nStelianÂ Coros\n2\n, and\nSungjoonÂ Choi\n1\nCorresponding authors: Sungjoon Choi.\n1\nTaerimÂ Yoon, and SungjoonÂ Choi are with the Department of Artificial Intelligence, Korea University, 145 Anam-ro, Seongbuk-gu, Seoul, Korea (email: taerimyoon@korea.ac.kr; sungjoon-choi@korea.ac.kr).\n2\nDonghoÂ Kang, JinÂ Cheng, FatemehÂ Zargarbashi, YijiangÂ Huang, and Stelian Coros are with the Department of Computer Science, ETH Zurich, Wasserwerkstrasse 12, 8092 Zurich, Switzerland (email: kangd@ethz.ch, jicheng@ethz.ch, fatemeh.zargarbashi@ethz.ch, huang@hey.com, scoros@ethz.ch).\n3\nMinsungÂ Ahn is with the Department of Mechanical and Aerospace Engineering, UCLA, 420 Westwood Plaza, Los Angeles, 90095, CA, USA (email: aminsung@ucla.edu)\nAbstract\nIn this work, we aim to enable legged robots to learn how to interpret human social cues and produce appropriate behaviors through physical human guidance.\nHowever, learning through physical engagement can place a heavy burden on users when the process requires large amounts of human-provided data.\nTo address this, we propose a human-in-the-loop framework that enables robots to acquire navigational behaviors in a data-efficient manner and to be controlled via multimodal natural human inputs, specifically gestural and verbal commands.\nWe reconstruct interaction scenes using a physics-based simulation and aggregate data to mitigate distributional shifts arising from limited demonstration data.\nOur\nprogressive goal cueing\nstrategy adaptively feeds appropriate commands and navigation goals during training, leading to more accurate navigation and stronger alignment between human input and robot behavior.\nWe evaluate our framework across six real-world agile navigation scenarios, including jumping over or avoiding obstacles.\nOur experimental results show that our proposed method succeeds in almost all trials across these scenarios, achieving a 97.15% task success rate with less than 1 hour of demonstration data in total.\nI\nIntroduction\nDogs have remarkable abilities to interpret human behavior through social cues and seamlessly perform given tasks.\nFor instance, they can follow a pointing gesture, track a handlerâ€™s gaze, or respond to brief verbal commands with precise actions.\nIn contrast, current robot operation still primarily relies on manual control interfaces such as joysticks or keyboards.\nThese methods require continuous attention and fine-grained control, limiting their practicality for everyday use by non-experts.\nMotivated by this, we aim to develop a system that maps natural human cues to corresponding robot behaviors.\nBuilding such a system involves two core challenges.\nFirst, single-modal commands, such as verbal or gestural instructions alone, introduce ambiguity.\nFor instance, verbal commands such as â€œgo thereâ€ are inherently ambiguous unless the system correctly interprets the userâ€™s pointing gesture that specifies the target location.\nSecond, collecting large-scale real-world humanâ€“robot interaction data to learn such multimodal associations is costly.\nTherefore, it is crucial that the system can learn new interaction patterns from only a few interactions.\nPrior work on natural robot interfaces has explored gesture-based control or language-conditioned navigation, but often relies on a single modality and substantial data collection.\nIn this paper, we introduce a human-in-the-loop framework for teaching and controlling quadruped robots through multimodal interactions, termed\nLURE\n(\nL\nuring-based\nU\nserâ€“\nR\nobot\nE\nngagement), which enables data-efficient learning from few demonstrations.\nInspired by the\nluring\ntechnique used by professional dog trainers, users first physically guide the robot with a teaching rod while issuing gestures and verbal commands.\nWe then reconstruct the interaction scenes in a physics-based simulator, where the robot learns navigation skills that overcome distributional shifts arising from limited data, i.e., mismatches between the training and deployment distributions\n[\n14\n]\n.\nDuring training, we adaptively determine intermediate commands and navigation goals to improve context-aligned navigation, a process we refer to as\nprogressive goal cueing\n.\nAt run time, the learned policy enables intuitive control via human gestures and verbal commands, allowing the robot to perform agile movements in cluttered environments.\nFigure 1\n:\nThe proposed framework enables agile robot behaviors through verbal and gesture-based commands.\nWe demonstrate that the proposed framework enables users to control quadruped robots to overcome obstacles using verbal and gesture commands.\nFor instance, in the\nJump Over\nscenario shown in\nFigure\n1\n, the robot correctly interprets the userâ€™s intention to jump over the box and executes the desired action.\nExtensive experiments show that our system enables navigation through cluttered environments in a data-efficient manner with less than 1 hour of data across six scenarios.\nTo summarize the key contributions of this work:\n1.\nWe present a system that enables users to teach and control quadruped robots in cluttered environments through real-time gestural and verbal commands.\n2.\nWe propose a data-aggregation technique that enables robots to learn new interaction patterns using only a few demonstrations.\n3.\nWe experimentally show that verbal and nonverbal cues complement each other in interpreting user intent.\nII\nRelated Work\nOur goal is to enable users to teach and control a legged robot through natural interactions.\nThis objective aligns with the human-in-the-loop control paradigm, as the robot is operated through human feedback.\nOur approach also relates to learning from demonstrations, as we leverage interactive human demonstrations to acquire navigation skills.\nIn the following sections, we discuss related work on both aspects.\nII-A\nHuman-in-the-Loop Robot Interfaces\nRecent efforts in robot control have increasingly focused on designing more intuitive interfaces that move beyond manual devices such as joysticks or keypads.\nIn particular, controlling robots through human gestures and verbal commands has been a long-standing objective in human-robot interaction\n[\n8\n]\n.\nWhile numerous studies have investigated the utility of these modalities, they have primarily focused on using either gestures or verbal commands in isolation.\nA number of studies proposed a straightforward gesture-based human-robot interaction by manually mapping a predefined set of human gestures to a discrete number of robot actions.\nFor instance,\nShin\net al.\n[\n15\n]\nand\nZafar\net al.\n[\n24\n]\nmap current body keypoints directly to discretized robot actions.\nWhile these approaches are simple, discrete actions lack expressiveness, motivating continuous gesture-to-action interfaces.\nIn this direction,\nXie\net al.\n[\n22\n]\ntransferred hand motions at the velocity level for continuous robot control, while\nCuan\net al.\n[\n7\n]\nmapped user gestures to robot actions using imitation learning.\nHowever, directly mapping hand motions to low-level robot actions requires continuous, exhaustive user attention, similar to joystick-based teleoperation.\nVerbal command-based control interfaces have been explored, particularly in the context of language-conditioned navigation.\nCheng\net al.\n[\n5\n]\nproposed a vision-language-action model for legged navigation, enabling text-driven goal specification.\nMore recently,\nXue\net al.\n[\n23\n]\nembedded latent vision-language representations into a humanoid whole-body controller, enabling visual navigation from language instructions.\nDespite these advances, many language-based systems still suffer from spatial ambiguities.\nFor instance, interpreting a vague command like â€œgo thereâ€ can be challenging, as the destination is not explicitly specified in language but must be resolved through supplementary cues such as gaze or pointing.\nTo address these limitations, we complement the strength of verbal commands and gestures within a unified real-time interface.\nBy grounding language with accompanying user gestures, our system resolves spatial ambiguities and provides clearer intent signals for navigation.\nII-B\nLearning Navigation from Demonstrations\nLearning from Demonstration (LfD) enables users to teach behaviors through examples, providing an intuitive way to teach by direct demonstration rather than relying on handcrafted rules\n[\n18\n,\n2\n]\n.\nIn line with this, we adopt LfD as a natural framework for learning new behaviors from interactive and physically guided user demonstrations.\nEarly applications of LfD to navigation focused on single-goal imitation, with policies trained to follow expert-provided trajectories.\nA pivotal work by\nPomerleau [\n12\n]\ntrained neural networks to imitate human driving behavior for lane following, while more recent approaches\n[\n20\n]\nguide path imitation using environment feature maps.\nRecent work advances toward multi-skill navigation that interprets high-level commands and produces context-appropriate behaviors.\nFor example,\nWu\net al.\n[\n21\n]\nproposed a generative imitation learning framework for target-driven visual navigation in indoor scenes, enabling policies to generalize to novel goals based on demonstration data.\nSingh\net al.\n[\n17\n]\nproposed a goal-conditioned navigation framework that grounds implicit free-form language expressions into spatial targets, enabling mobile agents to interpret diverse instructions such as â€œgo to the bedâ€.\nDespite these advances, LfD is prone to distributional shift due to limited demonstration coverage, which can degrade performance or lead to task failure.\nAs in prior work, we study demonstration-based navigation, where robots navigate using verbal instructions and gestures.\nIn particular, we focus on mitigating distributional shifts under limited demonstrations via data augmentation.\nFigure 2\n:\nThe problem is formulated as an MDP with observations\n(\nğ±\n,\nğ†\n,\nğœ\n)\n(\\mathbf{x},\\bm{\\rho},\\mathbf{c})\nand actions representing the navigation goal\nğ \n\\mathbf{g}\n.\nThe transition dynamics include the pretrained locomotion controller\nÏ€\nu\n\\pi_{u}\n, the robot dynamics\nf\nf\n, and the human model\nh\nh\n, while the reward is defined as the alignment between the predicted and intended goals.\nIII\nProblem Formulation\nLet the robot state be\nğ±\n\\mathbf{x}\n, and the control input be\nğ®\n\\mathbf{u}\n, where\nğ±\n\\mathbf{x}\nconsists of the robotâ€™s base position and orientation, joint angles, and their corresponding velocities, and\nğ®\n\\mathbf{u}\ndenotes the motor torque commands.\nWe assume that the robot dynamics are defined by a function\nf\nf\n, such that\nğ±\nâ€²\n=\nf\nâ€‹\n(\nğ±\n,\nğ®\n)\n\\mathbf{x}^{\\prime}=f(\\mathbf{x},\\mathbf{u})\n, where\nğ±\nâ€²\n\\mathbf{x}^{\\prime}\ndenotes the state at the next time step.\nThe robot operates in a cluttered environment, where the surrounding environment is represented by a heightmap\nğ†\n\\bm{\\rho}\n.\nOur objective is to enable a human user to control the robot to perform a navigation task\nğ’¯\n\\mathcal{T}\nvia interaction commands\nğœ\n\\mathbf{c}\n, consisting of a gesture command\nğ¦\n\\mathbf{m}\nand a verbal command\nğ¯\n\\mathbf{v}\n.\nSpecifically, we model the human as a function\nh\nh\nthat produces an intended navigation goal\nğ \nâˆ—\n\\mathbf{g}^{*}\ntogether with interaction commands\nğœ\n\\mathbf{c}\n, expressed as\n(\nğœ\n,\nğ \nâˆ—\n)\n=\nh\nâ€‹\n(\nğ±\n,\nğ†\n,\nğ’¯\n)\n(\\mathbf{c},\\mathbf{g}^{*})=h(\\mathbf{x},\\bm{\\rho},\\mathcal{T})\n.\nGiven the interaction commands\nğœ\n\\mathbf{c}\n, the robot interprets the userâ€™s intent and moves toward the intended goal\nğ \nâˆ—\n\\mathbf{g}^{*}\n.\nFormally, we develop a policy\nÏ€\n\\pi\nthat generates control inputs\nğ®\n=\nÏ€\nâ€‹\n(\nğ±\n,\nğœ\n,\nğ†\n)\n\\mathbf{u}=\\pi(\\mathbf{x},\\mathbf{c},\\bm{\\rho})\n, enabling the robot to infer human intent and accomplish the navigation task\nğ’¯\n\\mathcal{T}\nby reaching the intended goal\nğ \nâˆ—\n\\mathbf{g}^{*}\n.\nAs shown in\nFigure\n2\n, we decompose the problem into two subsystems: a\nhigh-level navigation module\nand a\nlow-level locomotion controller\n.\nThe navigation module, denoted as\nÏ€\ng\n\\pi_{g}\n, serves as a high-level SystemÂ 2 planner that interprets human intent and predicts navigation goals\nğ \n\\mathbf{g}\n, represented as\nğ \n=\nÏ€\ng\nâ€‹\n(\nğœ\n,\nğ±\n,\nğ†\n)\n\\mathbf{g}=\\pi_{g}(\\mathbf{c},\\mathbf{x},\\bm{\\rho})\n.\nThe locomotion controller, represented as\nğ®\n=\nÏ€\nu\nâ€‹\n(\nğ \n,\nğ±\n,\nğ†\n)\n\\mathbf{u}=\\pi_{u}(\\mathbf{g},\\mathbf{x},\\bm{\\rho})\n, functions as a fast and reactive SystemÂ 1 module that executes agile behaviors to overcome obstacles.\nBy integrating these two modules, robot is controlled directly by high-level interaction commands.\nWe pretrain a locomotion controller\nÏ€\nu\n\\pi_{u}\ncapable of tracking arbitrary goals across diverse terrains and obstacles, and subsequently focus on the high-level navigation problem.\nWe formulate this problem as a Markov Decision Process (MDP) problem, where the observation is defined as\n(\nğ±\n,\nğ†\n,\nğœ\n)\n(\\mathbf{x},\\bm{\\rho},\\mathbf{c})\nand the action corresponds to the navigation goal\nğ \n\\mathbf{g}\nexpressed in the robot frame.\nIn more detail, the navigation policy\nÏ€\ng\n\\pi_{g}\noutputs navigation goal\nğ \n\\mathbf{g}\nbased on the interaction signal\nğœ\n\\mathbf{c}\n, where the reward function is defined by how closely\nğ \n\\mathbf{g}\nmatches the intended navigation goal\nğ \nâˆ—\n\\mathbf{g}^{*}\n.\nThe locomotion controller\nÏ€\nu\n\\pi_{u}\nthen takes the navigation goal\nğ \n\\mathbf{g}\nto produce a control signal\nğ®\n=\nÏ€\nu\nâ€‹\n(\nğ \n,\nğ±\n,\nğ†\n)\n\\mathbf{u}=\\pi_{u}(\\mathbf{g},\\mathbf{x},\\bm{\\rho})\n.\nSubsequently, the robot dynamics\nf\nf\nproduce the robot state\nğ±\nâ€²\n\\mathbf{x}^{\\prime}\nat the next time step, and the human model\nh\nh\ngenerates the corresponding gesture\nğ¦\nâ€²\n\\mathbf{m}^{\\prime}\nand language command\nğ¯\nâ€²\n\\mathbf{v}^{\\prime}\n, completing the state transition.\nWe address this MDP using data aggregation\n[\n14\n]\nto mitigate the distributional shift problem, which is augmenting expert actions based on the current policyâ€™s state distribution, denoted as\nğ’Ÿ\nÏ€\ng\n\\mathcal{D}^{\\pi_{g}}\n.\nFormally, the problem is formulated as\narg\nâ¡\nmin\nÏ€\ng\nâˆˆ\nÎ \nâ¡\nğ”¼\n(\nğœ\n,\nğ±\n,\nğ†\n,\nğ \nâˆ—\n)\nâˆ¼\nğ’Ÿ\nÏ€\ng\nâ€‹\n[\nâ€–\nğ \nâˆ—\nâˆ’\nÏ€\ng\nâ€‹\n(\nğœ\n,\nğ±\n,\nğ†\n)\nâ€–\n2\n]\n.\n\\arg\\min_{\\pi_{g}\\in\\Pi}\\,\\mathbb{E}_{(\\mathbf{c},\\mathbf{x},\\bm{\\rho},{\\mathbf{g}^{*}})\\sim\\mathcal{D}^{\\pi_{g}}}\\left[\\left\\|{{\\mathbf{g}^{*}}}-\\pi_{g}(\\mathbf{c},\\mathbf{x},\\bm{\\rho})\\right\\|^{2}\\right].\n(1)\nAddressing distributional shifts is particularly important, as the high cost of collecting interaction data leads to limited demonstrations, making the system especially prone to them.\nData aggregation requires two key components.\nFirst, it requires the transition dynamics, including the human model\nh\nh\n.\nSince manually encoding the intended navigation goal\nğ \n\\mathbf{g}\nand the corresponding interaction commands\nğœ\n\\mathbf{c}\nis impractical, we use demonstration data to approximate the human model as\nh\n~\n\\tilde{h}\n, as discussed in\nSection\nIV-D\n.\nSecond, data aggregation requires an expert policy to provide actions at previously unseen states.\nWe construct a locally approximated expert\nÏ€\n~\ng\n\\tilde{\\pi}_{g}\nthat produces actions for novel states and enables efficient data aggregation, as described in\nSection\nIV-C\n.\nFigure 3\n:\nOverview of the proposed framework. We first establish a locomotion controller that follows the navigation goal\nğ \n\\mathbf{g}\n. In StageÂ 1, we collect interaction data\nğ’Ÿ\n=\n{\nğ¯\n,\nğ¦\n,\nğ†\n,\nğ±\n,\nğ \nâˆ—\n}\n\\mathcal{D}=\\{\\mathbf{v},\\mathbf{m},\\bm{\\rho},\\mathbf{x},\\mathbf{g}^{*}\\}\nthrough natural interactions between two participants. In StageÂ 2, we reconstruct the interaction scene from\nğ’Ÿ\n\\mathcal{D}\nand train the navigation module via data aggregation. The framework progressively provides the interaction and navigation goals only after the robot reaches its current goal, ensuring alignment between the command and behavior. Finally, in StageÂ 3, the user can control the robot through interaction.\nIV\nMethod\nOur goal is to develop a policy that can interpret the verbal command\nğ¯\n\\mathbf{v}\nand gesture command\nğ¦\n\\mathbf{m}\nto reach the navigation goal\nğ \nâˆ—\n\\mathbf{g}^{*}\n.\nTo achieve this, our framework comprises several key components, as illustrated in\nFigure\n3\n.\nWe begin by introducing an agile locomotion controller that enables the robot to perform dynamic movements such as stepping over or around obstacles.\nNext, we collect interaction data using this locomotion controller.\nFinally, we reconstruct interaction scenes in which robots are trained via data augmentation, enabling control through natural gestural and verbal commands with limited demonstration data.\nIV-A\nLocomotion controller\nAs a preliminary stage, we develop a locomotion controller for low-level agility of the robot.\nAs shown in the StageÂ 0 from\nFigure\n3\n, the locomotion controller consists of a velocity planner and a velocity tracker.\nThe velocity planner converts the navigation goal\nğ \n\\mathbf{g}\ninto a velocity command guiding the robot toward its target.\nIn more detail, it outputs a heading velocity command\nv\ncom\n=\nK\nâ€‹\nâ€–\nğ \nx\nâ€‹\ny\nâˆ’\nğ©\nx\nâ€‹\ny\nâ€–\n2\nv_{\\text{com}}=K\\|\\mathbf{g}^{xy}-\\mathbf{p}^{xy}\\|_{2}\nand an rotation command\nÏ•\ncom\n=\nyaw\nâ€‹\n(\nR\nâŠ¤\nâ€‹\n(\nğ \nâˆ’\nğ©\n)\n)\n\\phi_{\\text{com}}=\\text{yaw}(R^{\\top}(\\mathbf{g}-\\mathbf{p}))\n, where\nğ©\nâˆˆ\nâ„\n3\n\\mathbf{p}\\in\\mathbb{R}^{3}\nis the robotâ€™s base position,\nR\nâˆˆ\nSO\nâ€‹\n(\n3\n)\nR\\in\\mathrm{SO}(3)\nis the robotâ€™s rotation matrix and\n(\nâ‹…\n)\nx\nâ€‹\ny\n(\\cdot)^{xy}\nindicates ground-projected coordinates.\nThe other component is the velocity tracker, which enables the robot to track the commanded velocity.\nAlthough our framework is compatible with any velocity tracker, we trained a specialized tracker designed for dynamic locomotion to support agile behaviors, such as obstacle jumping.\nMore details on our velocity tracking policy are discussed in\nSection\nIV-F\n.\nIV-B\nHuman-in-the-loop data collection through luring\nWe collect humanâ€“robot interaction data by luring, as illustrated in StageÂ 1 of\nFigure\n3\n.\nThe process requires two participants: one issues both verbal and non-verbal commands, while the other guides the robot using a teaching rod.\nThis setup allows us to collect paired interaction and navigation data, which we refer to as the interaction data\nğ’Ÿ\n\\mathcal{D}\n.\nDetails on the representation of verbal commands and gestural cues are provided in\nSection\nIV-F\n.\nAs shown in\nFigure\n3\n, the interaction data\nğ’Ÿ\n\\mathcal{D}\nconsists of the robot state\nğ±\n\\mathbf{x}\n, the target position\nğ \nâˆ—\n\\mathbf{g}^{*}\n, the gesture command\nğ¦\n\\mathbf{m}\n, the verbal command\nğ¯\n\\mathbf{v}\n, and the obstacle representation\nğ†\n\\bm{\\rho}\n.\nAll data is recorded in a global coordinate frame, providing sufficient information for full scene reconstruction.\nThis scene reconstruction is a crucial component of our framework, as it enables data aggregation that improves robustness to distributional shifts, as discussed in\nSection\nIV-C\n.\nIV-C\nData aggregation\nWe reconstruct the interaction scene in the simulation for data aggregation, as shown in StageÂ 2 of\nFigure\n3\n.\nSpecifically, we recreate the terrain using obstacle information\nÏ\n\\rho\nand replay the gesture\nğ¦\n\\mathbf{m}\nand verbal commands\nğ¯\n\\mathbf{v}\n.\nThe robot is trained to follow the intended goal location\nğ \nâˆ—\n\\mathbf{g}^{*}\n, defined by the teaching rod position in the interaction data.\nOverall, our training procedure resembles the data aggregation method introduced in DAgger\n[\n14\n]\n, where the agent explores using its current policy while the expertâ€™s actions are augmented to guide learning.\nIn more detail, we define the local expert policy\nÏ€\n~\ng\n\\tilde{\\pi}_{g}\nthat outputs the expert navigation goal\nğ \n~\n\\tilde{\\mathbf{g}}\nin the robot frame, directing the robot towards the global target position\nğ \nâˆ—\n\\mathbf{g}^{*}\n.\nBy aggregating these actions, the system can efficiently generate corrective actions in the local frame that steer the robot toward the global target\nğ \nâˆ—\n\\mathbf{g}^{*}\n, even when the current policy drifts away from the expert trajectory.\nAs a result, the robot learns to recover from deviations and reach the intended target, effectively mitigating the distributional shift.\nThis approach is justified by our data-collection setup, where the velocity planner drives the robot toward the goal in the robot frame, producing demonstrations consistent with the behavior encoded in the local expert policy\nÏ€\n~\ng\n\\tilde{\\pi}_{g}\n.\nWe apply domain randomization to deliberately force the agent to deviate from the expert trajectory, exposing it to a broader range of states and enabling it to learn recovery behaviors\n[\n10\n]\n.\nFor example, we vary the robotâ€™s mass and inertia, and apply external pushes to the base.\nTo further improve robustness, we introduce the following techniques, with detailed parameter settings provided in the appendix.\nâ€¢\nTerrain scaling\nWe divide the terrain into a uniform grid of\nS\nS\ntiles and scale each tile by a random factor to randomize the spacing between obstacles and goals.\nâ€¢\nBinary obstacle map\nWe threshold the height map at\nh\nthres\nh_{\\text{thres}}\nand feed the resulting binary obstacle map to the navigation module, thereby promoting consistent navigation across varying obstacle heights.\nâ€¢\nDistractor objects.\nWe introduce square and circular distractor objects to mitigate overfitting to specific obstacle arrangements.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 4\n:\nIllustration of data collection procedures for six interactive navigation scenarios.\n(a)\nGo there\n, (b)\nCome here\n, and (c)\nFollow me\nrepresent human-robot interactions in open space.\n(d)\nCome around\nand (e)\nJump over\nillustrate interactions involving a box obstacle.\n(f)\nZigzag\ndemonstrates interaction with multiple tire obstacles.\nIV-D\nProgressive goal cueing\nIt is crucial to provide interaction commands and navigation goals that align with the robotâ€™s current state during training, as misalignment can lead to inconsistent supervision.\nOne naive approach is to feed gesture and verbal commands to the policy according to their original timestamps.\nHowever, during early training, the policy often progresses more slowly through tasks, leading to new commands being issued before the robot has completed the preceding behavior.\nSince the interaction data were collected by issuing new commands in response to the robotâ€™s task progress, this navie replay strategy can introduce stateâ€“command misalignment.\nTo address this, we propose a\nprogressive goal cueing\nthat dynamically presents the interaction command based on the robotâ€™s state.\nAs illustrated in StageÂ 2 of\nFigure\n3\n, each interaction command is held constant while the robot moves toward the current goal and is updated once the target is reached.\nThis allows the human model\nh\nh\nto wait for the robot rather than rushing through the episode when the robot struggles to navigate correctly in the early stages.\nOverall, it results in more context-aware control and enables finer movements during execution, as demonstrated later in\nSection\nV-C\n.\nAt each timestep, we apply progressive goal cueing with a 50% probability; otherwise, it advances to the next interaction according to the simulation clock.\nThis design choice prevents excessive waiting and improves responsiveness during real-world deployment.\nAdditionally, the waiting behavior naturally fades out as the robot begins producing correct movements in the latter stage of training.\nIV-E\nInteraction Data Augmentation\nSince our model is trained from limited interaction data, preventing overfitting and promoting generalization across both gesture and verbal modalities are essential.\nTherefore, we apply modality-specific input augmentation for both gestures and verbal commands.\nFor gestures, we perturb the extracted keypoints with noise\nÏµ\nğ¦\n\\epsilon_{\\mathbf{m}}\nfor every simulation time step.\nThis models the natural variability in repeated human motions as well as noise introduced during pose estimation.\nFor verbal commands, we leverage a Large Language Model (LLM)\n[\n1\n]\nto generate semantically equivalent expressions.\nSpecifically, for each verbal command collected in\nSection\nV-A\n, we prompt the LLM to produce\nN\nv\nN_{v}\nalternative phrasings, from which one is randomly sampled during training.\nIV-F\nImplementation details\nThis section provides additional implementation details regarding the perception and command representations, as well as the velocity tracker policy for low-level locomotion control.\n(a)\n(b)\nFigure 5\n:\nAnalysis of collected data in terms of\n(a) total wall clock time and\n(b) number of episodes for each scenario\nIV-F\n1\nRepresentation\nWe describe how each input modality is represented and captured in real time.\nThe verbal command is segmented using Silero VAD\n[\n4\n]\n, transcribed using Whisper\n[\n13\n]\n, and encoded with a pretrained text encoder\n[\n19\n]\n.\nNext, the human gesture is captured using motion capture cameras (OptiTrack Prime X22). We consider six upper-body keypointsâ€”the shoulders, elbows, and wrists on both the left and right sidesâ€”and express their 3D positions in the robot frame as\nğ©\nsh.r\n,\nğ©\nel.r\n,\nğ©\nwr.r\n,\nğ©\nsh.l\n,\nğ©\nel.l\n,\nğ©\nwr.l\n\\mathbf{p}_{\\text{sh.r}},\\mathbf{p}_{\\text{el.r}},\\mathbf{p}_{\\text{wr.r}},\\mathbf{p}_{\\text{sh.l}},\\mathbf{p}_{\\text{el.l}},\\mathbf{p}_{\\text{wr.l}}\n. Using these keypoints, we define the gesture command as\nğ¦\n=\n[\nğ\nsh.r,el.r\n,\nğ\nel.r,wr.r\n,\nğ\nsh.l,el.l\n,\nğ\nel.l,wr.l\n,\nğ\nsh.l,sh.r\n,\n(\nğ©\nsh.l\n+\nğ©\nsh.r\n)\nx\nâ€‹\ny\n/\n2\n,\nÏ•\nh\n]\n\\mathbf{m}=\\big[\\mathbf{d}_{\\text{sh.r,el.r}},\\ \\mathbf{d}_{\\text{el.r,wr.r}},\\ \\mathbf{d}_{\\text{sh.l,el.l}},\\ \\mathbf{d}_{\\text{el.l,wr.l}},\\ \\mathbf{d}_{\\text{sh.l,sh.r}},\\ (\\mathbf{p}_{\\text{sh.l}}+\\mathbf{p}_{\\text{sh.r}})^{xy}/2,\\ \\phi_{h}\\big]\n,\nwhere\nğ\n\\mathbf{d}\ndenotes the unit vector between two points, and\nÏ•\nh\n\\phi_{h}\nrepresents the human yaw.\nRegarding the obstacles, we measure their dimensions in advance and estimate their poses using motion capture cameras.\nUsing this information, we obtain a heightmap in the robot frame for the locomotion controller\nÏ€\nu\n\\pi_{u}\nand a thresholded binary mask for the navigation module\nÏ€\ng\n\\pi_{g}\n.\nIV-F\n2\nVelocity tracker\nWe train a velocity tracker operating at 50Â Hz to overcome the obstacles.\nThe training scheme is similar to the work of\nCheng\net al.\n[\n6\n]\nand is trained in Isaac Gym\n[\n11\n]\nto track velocity commands\nv\ncom\nâˆˆ\n[\n0.0\n,\n1.0\n]\nâ€‹\nm\n/\ns\nv_{\\text{com}}\\in[0.0,1.0]~\\mathrm{m/s}\nand angular velocity commands\nw\ncom\nâˆˆ\n[\nâˆ’\nÏ€\n/\n3\n,\nÏ€\n/\n3\n]\nâ€‹\nrad\n/\ns\nw_{\\text{com}}\\in[-\\pi/3,\\pi/3]~\\mathrm{rad/s}\n.\nTo improve robustness, we introduce a stand boolean in the state representation by thresholding velocity commands\nv\ncom\nv_{\\text{com}}\n, which enhances walkâ€“stand transitions.\nFor real-world deployment, we use a Kalman filterâ€“based state estimator\n[\n3\n]\nthat fuses joint encoder, foot contact, and motion capture data for stable feedback.\nAlthough the robot can be trained to move faster, we limit its speed for safety.\nV\nResults\nWe conduct a series of experiments to assess the effectiveness and generality of our framework.\nWe begin by describing the interaction scenarios considered in this study and then compare our method with imitation learning baselines to evaluate our proposed data aggregation and progressive goal cueing strategy.\nWe then show that our system can quickly learn new interaction patterns from novel users with less than five minutes of interaction.\nNext, we conduct an ablation study to demonstrate how verbal and gestural cues complement each other.\nFinally, we demonstrate multi-obstacle navigation in a real-world environment, showing that our system can robustly execute complex task sequences using a single unified policy.\nV-A\nData Collection\nWe collect interaction data, denoted as\nğ’Ÿ\n\\mathcal{D}\n, comprising human interaction\nğœ\n\\mathbf{\\mathbf{c}}\n, robot states\nğ±\n\\mathbf{x}\n, obstacle representations\nğ†\n\\bm{\\rho}\n, and target goals\nğ \nâˆ—\n\\mathbf{g}^{*}\n, recorded at a frequency of 10Hz.\nWe define six interaction scenarios, as shown in\nFigure\n4\n, and a stop scenario.\nâ€¢\nStop:\nA person raises both arms and says â€œstop,â€ prompting the robot to halt at its current position.\nâ€¢\nGo there:\nA person points with one hand and says â€œgo there,â€ and the robot moves toward the indicated location.\nâ€¢\nCome here:\nA person calls the robot by saying â€œcome hereâ€ with the arms dropped, and the robot moves directly toward the person.\nâ€¢\nFollow me:\nA person raises either their left or right hand and says â€œfollow me,â€ and the robot follows on the corresponding side of the raised hand.\nâ€¢\nCome around:\nA person stands in front of a box and says â€œcome around,â€ prompting the robot to move toward the human while avoiding the obstacle. Both hands move towards the direction intended for coming around the obstacle (left or right).\nâ€¢\nJump over:\nA person waves one hand over their head while saying â€œjump over,â€ causing the robot to jump over the obstacle.\nâ€¢\nZigzag:\nA person gestures with one hand to indicate the movement direction while saying â€œzigzag,â€ and the robot navigates between the tires in a zigzag pattern when viewed from above.\nThe summary of total data collection time and the number of episodes per scenario is detailed in\nFigure\n5\n.\nWhile we introduce these scenarios as representative cases, the proposed framework can support a much broader range of interactions\nwithout relying on predefined cases\n.\nV-B\nBaselines\nWe compare our proposed method to three imitation learning algorithms.\nV-B\n1\nBC\n[\n12\n]\nAs a most basic baseline, we train the navigation policy using standard Behavior Cloning (BC).\nThe policy is learned purely from expert demonstrations.\nV-B\n2\nGAIL\n[\n9\n]\nWe use Generative Adversarial Imitation Learning (GAIL) as a baseline to compare against inverse reinforcement learning methods that are known for their generalization capabilities.\nThe policy is learned via adversarial training to match the expert behavior distribution.\nThis baseline allows us to compare our approach against an imitation framework that focuses on generalization.\nV-B\n3\nDAgger\n[\n14\n]\nWe train the policy using data aggregation with a local expert\nÏ€\n~\ng\n\\tilde{\\pi}_{g}\nconsistent with our proposed framework.\nHowever, we do not apply progressive goal cueing.\nInstead, interaction commands are issued according to the simulation time clock.\nWe reconstruct the scene and apply domain randomization to induce a distributional shift, from which the policy learns robust recovery, as described in\nIV-C\n.\nV-B\n4\nLURE (Ours)\nOur proposed method, LURE, utilizes data aggregation and domain randomization as in DAgger. In addition, we apply progressive goal cueing.\n(a)\n(b)\nFigure 6\n:\nComparison of baseline methods across six interactive navigation scenarios.\n(a) Navigation error (m) of BC, GAIL, DAgger, and our proposed method.\n(b) Success rate (%) for the same scenarios.\nTABLE I:\nSuccess Rate and Navigation Error per Scenario, Subject, and Adaptation\nScenario\nSubject #1\nSubject #2\nSubject #3\nSuccuess Rate\nâ†‘\n\\uparrow\nNavigation Error\nâ†“\n\\downarrow\nSuccuess Rate\nâ†‘\n\\uparrow\nNavigation Error\nâ†“\n\\downarrow\nSuccuess Rate\nâ†‘\n\\uparrow\nNavigation Error\nâ†“\n\\downarrow\nw/o Adpt.\nAdpt.\nw/o Adpt.\nAdpt.\nw/o Adpt.\nAdpt.\nw/o Adpt.\nAdpt.\nw/o Adpt.\nAdpt.\nw/o Adpt.\nAdpt.\nGo there\n0.66\n0.87\n0.71\n0.53\n0.86\n0.99\n0.57\n0.51\n0.68\n1.00\n0.72\n0.54\nCome here\n0.90\n1.00\n0.39\n0.28\n1.00\n1.00\n0.38\n0.18\n1.00\n1.00\n0.26\n0.22\nFollow me\n0.88\n1.00\n0.68\n0.36\n0.81\n0.90\n0.50\n0.30\n1.00\n1.00\n0.39\n0.23\nCome around\n0.37\n0.95\n1.13\n0.41\n0.95\n0.95\n0.65\n0.43\n0.10\n1.00\n2.20\n0.89\nJump over\n0.87\n0.95\n1.08\n0.53\n0.73\n0.95\n0.70\n0.55\n0.48\n0.92\n1.11\n0.45\nZigzag\n0.76\n0.95\n0.99\n0.36\n0.50\n0.70\n1.45\n0.73\n0.60\n1.00\n1.04\n0.36\nV-C\nEvaluating Success Rate and Navigation Error\nWe measure both the robotâ€™s success rate and navigation error, as shown in\nFigure\n6\n.\nWe first segment each interaction scene based on the issued verbal command â€œStop,â€ treating each segment as an independent episode.\nFor example, the period between saying â€œGo Thereâ€ and issuing â€œStopâ€ is regarded as one episode.\nAt the beginning of each episode, we reset the robotâ€™s position to the ground-truth position from data to ensure consistent evaluation.\nDuring evaluation, we disable progressive goal cueing to avoid underestimating the performance of other baselines.\nFor each interaction episode, we evaluate performance on five terrain instances with different scale factors and 20 robots per terrain, resulting in 100 evaluation runs.\nThe reported success rates and navigation errors are then averaged over each scenario.\nTask Success\nWe define task-specific success criteria as follows:\nâ€¢\nGo there, Come here, Follow me:\nThe final position of the robot must be within 1 meter of the ground-truth base position.\nâ€¢\nCome around:\nThe robot must pass through the correct lateral corridor beside the box, following the direction indicated by the user.\nâ€¢\nJump over:\nThe robot must pass through three reference points located along the forward direction of the box, centered at\nâˆ’\n0.5\n-0.5\nm,\n0\nm, and\n+\n0.5\n+0.5\nm relative to the box. A pass is counted as successful if the robot comes within 0.2 m of each point.\nâ€¢\nZigzag:\nThe robot must pass through all midpoints between the tires while avoiding stepping on any of them. A pass is counted as successful if the robot comes within 0.2 m of each point.\nNavigation Error\nWe compute the navigation error as the Mean Squared Error (MSE) between the robotâ€™s base position and a ground-truth trajectory, measured at each timestep during the interaction.\nThe ground-truth trajectory corresponds to the robot motion recorded in the interaction data under the same interaction commands.\nThe final error is obtained by averaging the per-frame MSE over the entire interaction duration.\nThis metric reflects how accurately the policy follows the intended trajectory specified by the interaction commands.\nThe results summarized in\nFigure\n6\nshow that LURE achieves the best performance, with a clear margin, attaining an average success rate of 97.15%.\nBy comparing DAgger with BC, we observe that incorporating data aggregation reduces navigation error by 24.6%.\nIntroducing progressive goal cueing, on top of this, yields an additional 15.2% reduction in navigation error over the DAgger baseline.\nIn terms of success rate, data aggregation improves performance by 18.6%, and progressive goal cueing provides a further 13.7% gain.\nNotably, GAIL outperforms BC on the most challenging Zigzag task, but its overall performance degrades when all skills are considered together.\nOn average, GAIL achieves a 29.32% lower success rate than BC, which may be explained by training instability and mode collapse often observed in adversarial imitation learning.\nTABLE II:\nSuccess Rate (%) and Navigation Error (m) per Scenario and Modality\nScenario\nSucc. Rate\nâ†‘\n\\uparrow\nPos. Error (m)\nâ†“\n\\downarrow\nBoth\nVerb.\nGest.\nBoth\nVerb.\nGest.\nGo there\n0.85\n0.35\n0.15\n0.52\n1.15\n1.79\nCome here\n0.91\n0.01\n0.15\n0.40\n1.74\n1.09\nFollow me\n0.98\n0.16\n0.28\n0.29\n1.98\n1.24\nCome around\n0.90\n0.02\n0.22\n0.54\n2.26\n1.85\nJump over\n0.83\n0.67\n0.23\n0.49\n1.03\n1.64\nZigzag\n0.96\n0.00\n0.16\n0.35\n1.93\n1.26\nV-D\nAdaptation to novel user\nWe evaluate whether our system can generalize to novel users through quick adaptation.\nThis is essential because different users often employ different interaction cues to command the robot for the same task.\nWe first train a model using the initial interaction dataset from a single user described in\nSection\nV-A\n, and then fine-tune it using an average of 4.5 minutes of data collected from each new subject.\nFor evaluation, measure the success rate and navigation error in the reconstructed simulation environment, following the same protocol described in\nSection\nV-C\n.\nAs shown in\nTable\nI\n, the success rates before adaptation are 74.00%, 80.83%, and 64.33% for three subjects.\nThis indicates that the model works to some degree even without personalization.\nAfter adaptation, the success rates increase to 95.33%, 91.50%, and 98.67%, resulting in an average improvement of 22.11%.\nThese results demonstrate that our framework can quickly adapt to new users with only a small amount of data.\nFigure 7\n:\nThe robot (1) zigzags through the tires, (2) briefly stops, (3) jumps over the box, and (4) returns to the starting point according to the real-time userâ€™s instruction.\nThe visualization overlays the robotâ€™s trajectory on a reconstructed 3D scene of the real environment.\nThe figure shows the robot completing these sequences five consecutive times.\nV-E\nAblation on Gesture and Verbal Modalities\nWe use both gesture and verbal modalities for interaction cues.\nTo evaluate their individual contributions, we train a model in which each modality is masked with a 10% probability during training.\nThis enables the model to function even when one modality is absent, enabling meaningful evaluation under partial inputs.\nAs shown in\nTable\nII\n, the evaluation is conducted across all six scenarios to measure how each modality contributes to task success and navigation error.\nUsing gesture only results in an average success rate of 19.83%, which is 70.67% lower than when both modalities are used.\nIn practice, the robot also tends to move only when a gesture is held for an extended period.\nThese indicate that gesture alone is often insufficient for effectively interpreting the userâ€™s intent.\nOn the other hand, using verbal commands alone results in a navigation error of 1.6817\nm\n\\mathrm{m}\n, whereas combining both modalities reduces the error by 1.25\nm\n\\mathrm{m}\nto 0.4317\nm\n\\mathrm{m}\n.\nThis is because verbal commands can lack spatial grounding.\nIn the\nGo there\nscenario, for instance, the robot does not know where the user is pointing and begins to move randomly.\nOne exception is the\nJump over\nscenario, where the robot can locate the box without a gesture, yielding a relatively high success rate of 67%. However, this is still lower than the multimodal setting, which achieves 83%.\nV-F\nMulti-Obstacle Navigation\nTo demonstrate that our framework can robustly handle diverse obstacles while seamlessly switching between multiple locomotion skills based on user intent, we conducted an obstacle course experiment, as illustrated in\nFigure\n7\n.\nThe course contains both boxes and tires: the robot first navigates through the tire corridor, avoiding each tire according to the userâ€™s interaction commands.\nAfter clearing the tires, the user issues a â€œstopâ€ command, upon which the robot halts and then performs the â€œJump overâ€ command to overcome the box.\nFinally, in response to the â€œGo thereâ€ command, the robot returns to the starting point.\nAs shown in\nFigure\n7\n, our framework completes this entire sequence five times in a row, demonstrating strong real-world robustness and smooth task transitions under a single unified policy.\nThe robotâ€™s trajectory is obtained using a motion capture system, while the obstacle geometries are reconstructed using a consumer-grade mobile 3D scanning software.\nVI\nLimitations\nWhile our framework enables effective interaction-based learning and control, it currently lacks zero-shot generalization across different users due to substantial inter-subject variability in gesture styles.\nUnlike verbal commands, which benefit from semantic augmentation via Large Language Models (LLMs), our gesture representation is augmented only through noise injection and therefore lacks semantic diversity.\nA promising direction is to develop gesture representations that are semantically structured and jointly grounded in the robot state and surrounding objects, building upon pretrained motion representations such as\nZhang\net al.\n[\n25\n]\n.\nMoreover, our progressive goal cueing provides a simple and efficient way to approximate human behavior in novel states.\nHowever, it only adjusts the timing of replaying human interaction data based on the robotâ€™s state.\nIdeally, human behavior should be generated more dynamically in response to novel states, rather than being constrained to temporally re-aligned demonstrations.\nFinally, our current vision system is restricted to controlled environments, as the objects are modeled as circular or square primitives with pre-measured dimensions, and the human wears a motion capture suit for pose estimation.\nCamera-based human pose estimation and scene reconstruction\n[\n16\n,\nmihajlovic2025volumetricsmpl\n]\ncould potentially remove these assumptions and enable operation in unstructured, real-world scenarios.\nVII\nConclusion and Future Work\nWe presented a framework that enables robots to learn and execute navigation skills in real time through natural human interaction.\nBy reconstructing interaction scenes and augmenting data within these environments, our approach enables policies to robustly recover from distributional shifts across diverse interaction conditions.\nExperimental results demonstrate that the proposed framework effectively integrates verbal and gestural cues, enabling robust control across a range of tasks and environments.\nMoreover, our findings highlight the complementary nature of speech and gesture, showing that their combination provides a powerful interface for teaching and controlling robots to execute context-appropriate behaviors, including agile maneuvers such as jumping over obstacles.\nOverall, this work advances an intuitive and practical system that directly maps high-level human intent to executable robot actions.\nIn future work, we will explore extending our framework beyond navigation to more general whole-body behaviors, such as locomotion manipulation, by grounding multimodal commands using a similar data augmentation strategy.\nReferences\n[1]\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat,\net al.\n(2023)\nGpt-4 technical report\n.\narXiv preprint arXiv:2303.08774\n.\nCited by:\nÂ§\nIV-E\n.\n[2]\nB. D. Argall, S. Chernova, M. Veloso, and B. Browning\n(2009)\nA survey of robot learning from demonstration\n.\nRobotics and Autonomous Systems\n57\n(\n5\n),\npp.Â 469â€“483\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-B\n.\n[3]\nM. Bloesch, M. Hutter, M. A. Hoepflinger, S. Leutenegger, C. Gehring, C. D. Remy, and R. Siegwart\n(2013)\nState estimation for legged robots-consistent fusion of leg kinematics and imu\n.\nRobotics\n17\n,\npp.Â 17â€“24\n.\nCited by:\nÂ§\nIV-F\n2\n.\n[4]\nH. Bredin and A. Laurent\n(2021)\nEnd-to-end speaker segmentation for overlap-aware resegmentation\n.\nIn\nInterspeech 2021\n,\npp.Â 3111â€“3115\n.\nExternal Links:\nDocument\n,\nISSN 2958-1796\nCited by:\nÂ§\nIV-F\n1\n.\n[5]\nA. Cheng, Y. Ji, Z. Yang, Z. Gongye, X. Zou, J. Kautz, E. Biyik, H. Yin, S. Liu, and X. Wang\n(2025-06)\nNaVILA: Legged Robot Vision-Language-Action Model for Navigation\n.\nIn\nProceedings of Robotics: Science and Systems\n,\nLosAngeles, CA, USA\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-A\n.\n[6]\nX. Cheng, K. Shi, A. Agarwal, and D. Pathak\n(2024)\nExtreme parkour with legged robots\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 11443â€“11450\n.\nCited by:\nÂ§\nIV-F\n2\n.\n[7]\nC. Cuan, T. E. Lee, E. Fisher, A. Francis, L. Takayama, T. Zhang, A. Toshev, and S. Pirk\n(2024)\nGesture2path: imitation learning for gesture-aware navigation\n.\nIn\nInternational Conference on Social Robotics\n,\npp.Â 264â€“279\n.\nCited by:\nÂ§\nII-A\n.\n[8]\nM. A. Goodrich and A. C. Schultz\n(2007)\nHumanâ€“robot interaction: a survey\n.\nFoundations and Trends in Humanâ€“Computer Interaction\n1\n(\n3\n),\npp.Â 203â€“275\n.\nCited by:\nÂ§\nII-A\n.\n[9]\nJ. Ho and S. Ermon\n(2016)\nGenerative adversarial imitation learning\n.\nAdvances in neural information processing systems\n29\n.\nCited by:\nÂ§\nV-B\n2\n.\n[10]\nD. Kang, J. Cheng, M. Zamora, F. Zargarbashi, and S. Coros\n(2023)\nRL + model-based control: using on-demand optimal control to learn versatile legged locomotion\n.\nIEEE Robotics and Automation Letters\n8\n(\n10\n),\npp.Â 6619â€“6626\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nIV-C\n.\n[11]\nV. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin, A. Allshire, A. Handa,\net al.\n(2021)\nIsaac gym: high performance gpu-based physics simulation for robot learning\n.\narXiv preprint arXiv:2108.10470\n.\nCited by:\nÂ§\nIV-F\n2\n.\n[12]\nD. A. Pomerleau\n(1989)\nAlvinn: an autonomous land vehicle in a neural network\n.\nIn\nAdvances in neural information processing systems\n,\npp.Â 305â€“313\n.\nCited by:\nÂ§\nII-B\n,\nÂ§\nV-B\n1\n.\n[13]\nA. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever\n(2023)\nRobust speech recognition via large-scale weak supervision\n.\nIn\nInternational conference on machine learning\n,\npp.Â 28492â€“28518\n.\nCited by:\nÂ§\nIV-F\n1\n.\n[14]\nS. Ross, G. Gordon, and D. Bagnell\n(2011)\nA reduction of imitation learning and structured prediction to no-regret online learning\n.\nIn\nProceedings of the fourteenth international conference on artificial intelligence and statistics\n,\npp.Â 627â€“635\n.\nCited by:\nÂ§I\n,\nÂ§III\n,\nÂ§\nIV-C\n,\nÂ§\nV-B\n3\n.\n[15]\nS. Shin, T. Evetts, H. Saylor, H. Kim, S. Woo, W. Rhee, and S. Kim\n(2024)\nNon-verbal interaction and interface with a quadruped robot using body and hand gestures: design and user experience evaluation\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 293â€“308\n.\nCited by:\nÂ§\nII-A\n.\n[16]\nS. Shin, J. Kim, E. Halilaj, and M. J. Black\n(2024)\nWham: reconstructing world-grounded humans with accurate 3d motion\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 2070â€“2080\n.\nCited by:\nÂ§VI\n.\n[17]\nR. Singh, S. Nair, A. Mandlekar, S. Savarese, Y. Zhu, and A. Garg\n(2022)\nIlliterate robots: differentiable goal selection with semantic reasoning\n.\nIn\nConference on Robot Learning\n,\npp.Â 1686â€“1696\n.\nCited by:\nÂ§\nII-B\n.\n[18]\nA. L. Thomaz and C. Breazeal\n(2006)\nReinforcement learning with human teachers: evidence of feedback and guidance with implications for learning performance\n.\nIn\nProceedings of the 21st National Conference on Artificial Intelligence\n,\npp.Â 1000â€“1005\n.\nCited by:\nÂ§\nII-B\n.\n[19]\nL. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei\n(2022)\nText embeddings by weakly-supervised contrastive pre-training\n.\narXiv preprint arXiv:2212.03533\n.\nCited by:\nÂ§\nIV-F\n1\n.\n[20]\nM. Wigness, J. Rogers, and R. Hansen\n(2022)\nRobot navigation from human demonstration: learning control behaviors with environment feature maps\n.\nIEEE Transactions on Intelligent Vehicles\n7\n(\n2\n),\npp.Â 290â€“300\n.\nCited by:\nÂ§\nII-B\n.\n[21]\nQ. Wu, X. Gong, K. Xu, D. Manocha, J. Dong, and J. Wang\n(2020)\nTowards target-driven visual navigation in indoor scenes via generative imitation learning\n.\nIEEE Robotics and Automation Letters\n6\n(\n1\n),\npp.Â 175â€“182\n.\nCited by:\nÂ§\nII-B\n.\n[22]\nJ. Xie, Z. Xu, J. Zeng, Y. Gao, and K. Hashimoto\n(2025)\nHumanâ€“robot interaction using dynamic hand gesture for teleoperation of quadruped robots with a robotic arm\n.\nElectronics\n14\n(\n5\n),\npp.Â 860\n.\nCited by:\nÂ§\nII-A\n.\n[23]\nH. Xue, X. Huang, D. Niu, Q. Liao, T. Kragerud, J. T. Gravdahl, X. B. Peng, G. Shi, T. Darrell, K. Sreenath,\net al.\n(2025)\nLeverb: humanoid whole-body control with latent vision-language instruction\n.\narXiv preprint arXiv:2506.13751\n.\nCited by:\nÂ§\nII-A\n.\n[24]\nM. H. Zafar, E. F. LangÃ¥s, and F. Sanfilippo\n(2024)\nReal-time gesture-based control of a quadruped robot using a stacked convolutional bi-long short-term memory (bi-lstm) neural network\n.\nIn\n2024 10th International Conference on Automation, Robotics and Applications (ICARA)\n,\npp.Â 81â€“86\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-A\n.\n[25]\nM. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, and Z. Liu\n(2024)\nMotiondiffuse: text-driven human motion generation with diffusion model\n.\nIEEE transactions on pattern analysis and machine intelligence\n46\n(\n6\n),\npp.Â 4115â€“4128\n.\nCited by:\nÂ§VI\n.\nTABLE III\n:\nHyperparameters and data augmentation details\nCategory\nSetting\nDomain\nRandomization\nExternal perturbation magnitude:\nf\nâˆˆ\n[\n0\n,\n0.5\n]\nâ€‹\nm\n/\ns\nf\\in[0,0.5]~\\mathrm{m/s}\n; applied at an average interval of 3 seconds.\nTerrain scaling factor:\ns\nâˆˆ\n[\n0.75\n,\n1.25\n]\ns\\in[0.75,1.25]\nover\n3\nÃ—\n3\n3\\times 3\ntiles.\nBinary height map threshold:\n0.05\nâ€‹\nm\n0.05~\\mathrm{m}\n.\nData\nAugmentation\nLLM-based verbal command augmentation:\nN\nv\n=\n20\nN_{v}=20\n.\nGesture keypoint noise:\nÏµ\nğ¦\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\n0.01\n)\n\\epsilon_{\\mathbf{m}}\\sim\\mathcal{N}(0,0.01)\n.",
    "preview_text": "In this work, we aim to enable legged robots to learn how to interpret human social cues and produce appropriate behaviors through physical human guidance. However, learning through physical engagement can place a heavy burden on users when the process requires large amounts of human-provided data. To address this, we propose a human-in-the-loop framework that enables robots to acquire navigational behaviors in a data-efficient manner and to be controlled via multimodal natural human inputs, specifically gestural and verbal commands. We reconstruct interaction scenes using a physics-based simulation and aggregate data to mitigate distributional shifts arising from limited demonstration data. Our progressive goal cueing strategy adaptively feeds appropriate commands and navigation goals during training, leading to more accurate navigation and stronger alignment between human input and robot behavior. We evaluate our framework across six real-world agile navigation scenarios, including jumping over or avoiding obstacles. Our experimental results show that our proposed method succeeds in almost all trials across these scenarios, achieving a 97.15% task success rate with less than 1 hour of demonstration data in total.\n\nTeaching Robots Like Dogs: Learning Agile Navigation from Luring, Gesture, and Speech\nTaerimÂ Yoon\n1\n,\nDonghoÂ Kang\n2\n,\nJinÂ Cheng\n2\n,\nFatemehÂ Zargarbashi\n2\n,\nYijiangÂ Huang\n2\n,\nMinsungÂ Ahn\n3\n,\nStelianÂ Coros\n2\n, and\nSungjoonÂ Choi\n1\nCorresponding authors: Sungjoon Choi.\n1\nTaerimÂ Yoon, and SungjoonÂ Choi are with the Department of Artificial Intelligence, Korea University, 145 Anam-ro, Seongbuk-gu, Seoul, Korea (email: taerimyoon@korea.ac.kr; sungjoon-choi@korea.ac.kr).\n2\nDonghoÂ Kang, JinÂ Cheng, FatemehÂ Zargarbashi, YijiangÂ Huang, and Stelian Coros are with the Department of Computer Science, ETH Zurich, Wasserwerkstrasse 12, 8092 Zurich, Switzerland (email: kangd@ethz.ch, jicheng@ethz.ch, fatemeh.zargarbashi@ethz.ch, huang@hey.com, scoros@ethz.ch).\n3\nMinsungÂ A",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§äººæœºäº¤äº’æ¡†æ¶ï¼Œä½¿è…¿å¼æœºå™¨äººèƒ½å¤Ÿé€šè¿‡æ‰‹åŠ¿å’Œè¯­éŸ³å‘½ä»¤é«˜æ•ˆå­¦ä¹ æ•æ·å¯¼èˆªè¡Œä¸ºï¼Œé‡ç‚¹å…³æ³¨ç‰©ç†å¼•å¯¼å’Œä»¿çœŸæ•°æ®èšåˆã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šã€Šåƒæ•™ç‹—ä¸€æ ·æ•™æœºå™¨äººï¼šä»å¼•è¯±ã€æ‰‹åŠ¿å’Œè¯­éŸ³ä¸­å­¦ä¹ æ•æ·å¯¼èˆªã€‹\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nå½“å‰æœºå™¨äººæ“ä½œä¸»è¦ä¾èµ–æ‘‡æ†ã€é”®ç›˜ç­‰æ‰‹åŠ¨æ§åˆ¶ç•Œé¢ï¼Œéœ€è¦æŒç»­å…³æ³¨å’Œç²¾ç»†æ§åˆ¶ï¼Œé™åˆ¶äº†éä¸“ä¸šç”¨æˆ·çš„æ—¥å¸¸ä½¿ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç‹—ç­‰åŠ¨ç‰©èƒ½é€šè¿‡è§£è¯»äººç±»çš„ç¤¾ä¼šæ€§çº¿ç´¢ï¼ˆå¦‚æ‰‹åŠ¿ã€è¯­éŸ³ï¼‰æ— ç¼æ‰§è¡Œä»»åŠ¡ã€‚å—æ­¤å¯å‘ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ä¸ªç³»ç»Ÿï¼Œèƒ½å¤Ÿå°†è‡ªç„¶çš„äººç±»çº¿ç´¢æ˜ å°„åˆ°ç›¸åº”çš„æœºå™¨äººè¡Œä¸ºï¼Œä½¿å››è¶³æœºå™¨äººèƒ½å¤Ÿé€šè¿‡ç›´è§‚çš„å¤šæ¨¡æ€äº¤äº’è¿›è¡Œå­¦ä¹ å’Œæ§åˆ¶ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸º **LURE** çš„äººæœºäº¤äº’æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š\n- **å¤šæ¨¡æ€äº¤äº’å­¦ä¹ **ï¼šç»“åˆ**æ‰‹åŠ¿**ï¼ˆé€šè¿‡åŠ¨ä½œæ•æ‰ç³»ç»Ÿè·å–ä¸ŠåŠèº«å…³é”®ç‚¹ï¼‰å’Œ**è¯­éŸ³**ï¼ˆé€šè¿‡è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬ç¼–ç ï¼‰ä½œä¸ºè¾“å…¥ï¼Œå…±åŒè§£æç”¨æˆ·æ„å›¾ï¼Œè§£å†³äº†å•ä¸€æ¨¡æ€ï¼ˆå¦‚æ¨¡ç³Šçš„â€œå»é‚£é‡Œâ€æŒ‡ä»¤ï¼‰çš„æ­§ä¹‰é—®é¢˜ã€‚\n- **æ•°æ®é«˜æ•ˆå­¦ä¹ **ï¼šå—è®­çŠ¬â€œå¼•è¯±â€æŠ€æœ¯å¯å‘ï¼Œé€šè¿‡ç‰©ç†å¼•å¯¼ï¼ˆä½¿ç”¨æ•™å­¦æ†ï¼‰ç»“åˆè¯­éŸ³æ‰‹åŠ¿æ”¶é›†å°‘é‡æ¼”ç¤ºæ•°æ®ï¼ˆæ€»è®¡<1å°æ—¶ï¼‰ã€‚åœ¨ç‰©ç†ä»¿çœŸä¸­é‡å»ºäº¤äº’åœºæ™¯ï¼Œå¹¶é‡‡ç”¨**æ•°æ®èšåˆ**æŠ€æœ¯ï¼ˆåŸºäºDAggerï¼‰æ¥ç¼“è§£å› æ•°æ®æœ‰é™å¯¼è‡´çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚\n- **æ¸è¿›å¼ç›®æ ‡æç¤º**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ ¹æ®æœºå™¨äººå½“å‰çŠ¶æ€åŠ¨æ€æä¾›äº¤äº’æŒ‡ä»¤å’Œå¯¼èˆªç›®æ ‡ï¼Œç¡®ä¿æŒ‡ä»¤ä¸è¡Œä¸ºå¯¹é½ï¼Œä»è€Œè·å¾—æ›´ç²¾ç¡®çš„å¯¼èˆªæ•ˆæœã€‚\n- **æ¨¡å—åŒ–æ¶æ„**ï¼šç³»ç»Ÿåˆ†ä¸ºé«˜å±‚å¯¼èˆªæ¨¡å—ï¼ˆè§£è¯»æ„å›¾ï¼Œé¢„æµ‹ç›®æ ‡ï¼‰å’Œé¢„è®­ç»ƒçš„ä½å±‚æ•æ·è¿åŠ¨æ§åˆ¶å™¨ï¼ˆæ‰§è¡Œè·³è·ƒã€é¿éšœç­‰åŠ¨ä½œï¼‰ï¼Œå®ç°äº†é«˜å±‚æ„å›¾åˆ°åº•å±‚åŠ¨ä½œçš„æ˜ å°„ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\nåœ¨å…­ç§çœŸå®ä¸–ç•Œæ•æ·å¯¼èˆªåœºæ™¯ï¼ˆå¦‚â€œå»é‚£é‡Œâ€ã€â€œè·³è¿‡å»â€ã€â€œç»•è¡Œâ€ã€â€œä¹‹å­—å½¢â€é€šè¿‡éšœç¢ç‰©ç­‰ï¼‰ä¸­è¿›è¡Œè¯„ä¼°ï¼š\n- **é«˜æˆåŠŸç‡**ï¼šæå‡ºçš„LUREæ–¹æ³•å¹³å‡ä»»åŠ¡æˆåŠŸç‡é«˜è¾¾**97.15%**ï¼Œæ˜¾è‘—ä¼˜äºè¡Œä¸ºå…‹éš†ã€ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ç­‰åŸºçº¿æ–¹æ³•ã€‚\n- **æ•°æ®é«˜æ•ˆ**ï¼šä»…ä½¿ç”¨æ€»è®¡ä¸åˆ°1å°æ—¶çš„æ¼”ç¤ºæ•°æ®ã€‚\n- **å¤šæ¨¡æ€äº’è¡¥æ€§**ï¼šæ¶ˆèå®éªŒè¡¨æ˜ï¼Œç»“åˆæ‰‹åŠ¿å’Œè¯­éŸ³æ¯”å•ç‹¬ä½¿ç”¨ä»»ä¸€æ¨¡æ€æˆåŠŸç‡å¤§å¹…æå‡ï¼ˆå¹³å‡æå‡çº¦70%ï¼‰ï¼Œå¯¼èˆªè¯¯å·®é™ä½çº¦1.25ç±³ã€‚\n- **å¿«é€Ÿç”¨æˆ·é€‚åº”**ï¼šå¯¹æ–°ç”¨æˆ·è¿›è¡Œå¹³å‡4.5åˆ†é’Ÿçš„å¾®è°ƒåï¼ŒæˆåŠŸç‡å¹³å‡æå‡22.11%ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„ä¸ªæ€§åŒ–é€‚åº”èƒ½åŠ›ã€‚\n- **å¤æ‚ä»»åŠ¡åºåˆ—**ï¼šæœºå™¨äººèƒ½å¤Ÿåœ¨ä¸€ä¸ªç»Ÿä¸€ç­–ç•¥ä¸‹ï¼Œè¿ç»­å®ŒæˆåŒ…å«å¤šç§éšœç¢å’ŒæŠ€èƒ½åˆ‡æ¢çš„å¤æ‚å¯¼èˆªè¯¾ç¨‹ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\nè¿™é¡¹å·¥ä½œåœ¨**æœºå™¨äººè‡ªç„¶äº¤äº’æ§åˆ¶**é¢†åŸŸå–å¾—äº†é‡è¦è¿›å±•ã€‚å…¶ä»·å€¼åœ¨äºï¼š\n- **å®ç”¨æ€§**ï¼šæä¾›äº†ä¸€ç§ç›´è§‚ã€æ•°æ®é«˜æ•ˆçš„æ•™å­¦ä¸æ§åˆ¶èŒƒå¼ï¼Œé™ä½äº†éä¸“å®¶ç”¨æˆ·çš„æ“ä½œé—¨æ§›ï¼Œæ¨åŠ¨äº†æœºå™¨äººå‘æ—¥å¸¸åº”ç”¨çš„è¿ˆè¿›ã€‚\n- **æŠ€æœ¯è´¡çŒ®**ï¼šæå‡ºçš„**å¤šæ¨¡æ€èåˆ**ã€**åŸºäºä»¿çœŸçš„æ•°æ®èšåˆ**å’Œ**æ¸è¿›å¼ç›®æ ‡æç¤º**ç­–ç•¥ï¼Œä¸ºè§£å†³æ¼”ç¤ºå­¦ä¹ ä¸­çš„æ•°æ®ç¨€ç¼ºå’Œåˆ†å¸ƒåç§»é—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚\n- **å¯å‘æ€§**ï¼šå±•ç¤ºäº†ç»“åˆç”Ÿç‰©å¯å‘ï¼ˆè®­çŠ¬ï¼‰ä¸å…ˆè¿›æœºå™¨å­¦ä¹ æŠ€æœ¯çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥å¼€å‘æ›´é€šç”¨ã€é²æ£’çš„å…¨èº«è¡Œä¸ºäº¤äº’ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚æœªæ¥çš„å·¥ä½œå¯æ‰©å±•è‡³æ›´å¤æ‚çš„æ“ä½œä»»åŠ¡å’Œæ›´éç»“æ„åŒ–çš„è§†è§‰ç¯å¢ƒã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **ä¸€ã€ ç ”ç©¶ç›®æ ‡ä¸æ‹Ÿè§£å†³çš„æ ¸å¿ƒé—®é¢˜**\n\nè¿™ç¯‡è®ºæ–‡æ—¨åœ¨è§£å†³**å¦‚ä½•è®©å››è¶³æœºå™¨äººåƒç‹—ä¸€æ ·ï¼Œé€šè¿‡è‡ªç„¶çš„äººç±»ç¤¾äº¤çº¿ç´¢ï¼ˆå¦‚æ‰‹åŠ¿å’Œè¯­éŸ³ï¼‰è¿›è¡Œé«˜æ•ˆå­¦ä¹ å’Œæ•æ·å¯¼èˆª**ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒé’ˆå¯¹ä»¥ä¸‹ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼š\n\n1.  **äº¤äº’çš„æ¨¡ç³Šæ€§**ï¼šå•ä¸€æ¨¡æ€æŒ‡ä»¤ï¼ˆå¦‚ä»…è¯´â€œå»é‚£é‡Œâ€æˆ–ä»…åšæ‰‹åŠ¿ï¼‰å­˜åœ¨æ­§ä¹‰ï¼Œéš¾ä»¥ç²¾ç¡®ä¼ è¾¾ç”¨æˆ·æ„å›¾ã€‚\n2.  **æ•°æ®æ”¶é›†çš„é«˜æˆæœ¬**ï¼šä¸ºå­¦ä¹ å¤æ‚çš„å¤šæ¨¡æ€äººæœºäº¤äº’ï¼Œæ”¶é›†å¤§è§„æ¨¡çœŸå®ä¸–ç•Œæ•°æ®æˆæœ¬é«˜æ˜‚ï¼Œç³»ç»Ÿå¿…é¡»èƒ½å¤Ÿä»**å°‘é‡æ¼”ç¤º**ä¸­é«˜æ•ˆå­¦ä¹ ã€‚\n\n### **äºŒã€ æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ç‚¹**\n\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸º **LURE** çš„æ•´ä½“æ¡†æ¶ï¼Œå…¶åˆ›æ–°ç‚¹ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹ä¸‰ä¸ªæ–¹é¢ï¼š\n\n#### **1. åŸºäºâ€œè¯±å¯¼â€çš„å¤šæ¨¡æ€äººæœºäº¤äº’æ•™å­¦ä¸æ§åˆ¶ç³»ç»Ÿ**\n- **çµæ„Ÿæ¥æº**ï¼šå€Ÿé‰´ä¸“ä¸šè®­çŠ¬å¸ˆä½¿ç”¨çš„â€œè¯±å¯¼â€æŠ€æœ¯ã€‚\n- **æ–¹æ³•**ï¼šç”¨æˆ·ä½¿ç”¨ä¸€æ ¹æ•™å­¦æ†**ç‰©ç†å¼•å¯¼**æœºå™¨äººï¼ŒåŒæ—¶é…åˆå‘å‡º**æ‰‹åŠ¿å’Œè¯­éŸ³å‘½ä»¤**ã€‚è¿™ç§æ–¹å¼èƒ½è‡ªç„¶åœ°äº§ç”Ÿå¤šæ¨¡æ€å¯¹é½çš„æ¼”ç¤ºæ•°æ®ï¼ˆ`{æ‰‹åŠ¿ï¼Œ è¯­éŸ³ï¼Œ ç›®æ ‡ä½ç½®}`ï¼‰ã€‚\n- **ä»·å€¼**ï¼šå°†é«˜å±‚çš„ç”¨æˆ·æ„å›¾ï¼ˆé€šè¿‡è‡ªç„¶äº¤äº’ä¼ è¾¾ï¼‰ç›´æ¥æ˜ å°„åˆ°ä½å±‚çš„æœºå™¨äººåŠ¨ä½œï¼Œå®ç°äº†æ¯”é¥æ†æˆ–å•ä¸€æ¨¡æ€æ›´ç›´è§‚ã€è´Ÿæ‹…æ›´è½»çš„æ§åˆ¶ã€‚\n\n#### **2. åŸºäºä»¿çœŸçš„æ•°æ®èšåˆä¸å¢å¼ºæŠ€æœ¯**\n- **æ ¸å¿ƒé—®é¢˜**ï¼šæœ‰é™çš„æ¼”ç¤ºæ•°æ®ä¼šå¯¼è‡´**åˆ†å¸ƒåç§»**ï¼Œå³è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸éƒ¨ç½²æ—¶é‡åˆ°çš„çŠ¶æ€åˆ†å¸ƒä¸åŒ¹é…ï¼Œå¯¼è‡´ç­–ç•¥å¤±è´¥ã€‚\n- **è§£å†³æ–¹æ¡ˆ**ï¼š\n    - **åœºæ™¯é‡å»º**ï¼šå°†çœŸå®ä¸–ç•Œæ”¶é›†çš„äº¤äº’æ•°æ®ï¼ˆåœ°å½¢ã€æœºå™¨äººçŠ¶æ€ã€äººç±»å‘½ä»¤ï¼‰åœ¨ç‰©ç†ä»¿çœŸå™¨ï¼ˆå¦‚Isaac Gymï¼‰ä¸­ç²¾ç¡®é‡å»ºã€‚\n    - **æ•°æ®èšåˆ**ï¼šé‡‡ç”¨ç±»ä¼¼ **DAgger** çš„ç®—æ³•ï¼Œè®©å½“å‰ç­–ç•¥åœ¨ä»¿çœŸä¸­æ¢ç´¢ï¼Œå¹¶ç”¨ä¸€ä¸ª**å±€éƒ¨ä¸“å®¶ç­–ç•¥**ï¼ˆåŸºäºæ¼”ç¤ºæ•°æ®æ„å»ºï¼‰æ¥æä¾›â€œçº æ­£åŠ¨ä½œâ€ï¼Œä»è€Œç”Ÿæˆè¦†ç›–æ›´å¹¿çŠ¶æ€åˆ†å¸ƒçš„æ–°æ•°æ®ã€‚\n    - **é¢†åŸŸéšæœºåŒ–**ï¼šåœ¨ä»¿çœŸä¸­éšæœºåŒ–æœºå™¨äººå‚æ•°ï¼ˆè´¨é‡ã€æƒ¯æ€§ï¼‰ã€æ–½åŠ å¤–éƒ¨æ‰°åŠ¨ã€éšæœºç¼©æ”¾åœ°å½¢ã€æ·»åŠ å¹²æ‰°ç‰©ä½“ç­‰ï¼Œå¼ºåˆ¶ç­–ç•¥å­¦ä¹ é²æ£’çš„æ¢å¤èƒ½åŠ›ã€‚\n- **ä»·å€¼**ï¼šç”¨**æ€»è®¡ä¸åˆ°1å°æ—¶**çš„æ¼”ç¤ºæ•°æ®ï¼ŒæˆåŠŸè®­ç»ƒå‡ºåœ¨6ç§å¤æ‚å¯¼èˆªåœºæ™¯ä¸­å¹³å‡æˆåŠŸç‡é«˜è¾¾ **97.15%** çš„ç­–ç•¥ï¼Œå®ç°äº†æé«˜çš„**æ•°æ®æ•ˆç‡**ã€‚\n\n#### **3. æ¸è¿›å¼ç›®æ ‡æç¤ºç­–ç•¥**\n- **é—®é¢˜**ï¼šåœ¨è®­ç»ƒåˆæœŸï¼Œç­–ç•¥æ‰§è¡Œæ…¢ï¼Œå¦‚æœæŒ‰åŸå§‹æ—¶é—´æˆ³å›æ”¾äººç±»å‘½ä»¤ï¼Œä¼šå¯¼è‡´å‘½ä»¤ä¸æœºå™¨äººå½“å‰çŠ¶æ€**é”™ä½**ï¼ˆä¾‹å¦‚ï¼Œæ–°å‘½ä»¤å‘å‡ºæ—¶ï¼Œæœºå™¨äººè¿˜æœªå®Œæˆä¸Šä¸€ä¸ªåŠ¨ä½œï¼‰ã€‚\n- **è§£å†³æ–¹æ¡ˆ**ï¼š**Progressive Goal Cueing**ã€‚è®­ç»ƒæ—¶ï¼Œç³»ç»Ÿä¼šç­‰å¾…æœºå™¨äººåˆ°è¾¾å½“å‰ç›®æ ‡åï¼Œæ‰æä¾›ä¸‹ä¸€ä¸ªäº¤äº’å‘½ä»¤å’Œå¯¼èˆªç›®æ ‡ã€‚\n- **ä»·å€¼**ï¼šç¡®ä¿äº†è®­ç»ƒè¿‡ç¨‹ä¸­å‘½ä»¤ä¸è¡Œä¸ºçš„å¯¹é½ï¼Œæä¾›äº†æ›´ä¸€è‡´çš„ç›‘ç£ä¿¡å·ï¼Œä»è€Œæå‡äº†å¯¼èˆªç²¾åº¦å’Œå‘½ä»¤-è¡Œä¸ºçš„ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç­–ç•¥åœ¨DAggeråŸºç¡€ä¸Šè¿›ä¸€æ­¥å°†å¯¼èˆªè¯¯å·®é™ä½äº†15.2%ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆæ¶æ„æ€»ç»“**\n\nè®ºæ–‡çš„è§£å†³æ–¹æ¡ˆæ˜¯ä¸€ä¸ªåˆ†å±‚ã€æ•°æ®é©±åŠ¨çš„ç®¡é“ï¼š\n\n```mermaid\ngraph TD\n    A[**é˜¶æ®µ0: é¢„å¤‡**] --> B[è®­ç»ƒä¸€ä¸ªé€šç”¨çš„ä½å±‚**æ•æ·è¿åŠ¨æ§åˆ¶å™¨**<br/>Ï€_u: è·Ÿè¸ªä»»æ„é€Ÿåº¦æŒ‡ä»¤];\n    B --> C[**é˜¶æ®µ1: æ•°æ®æ”¶é›†**<br/>ä¸¤äººåä½œ: ä¸€äººå‘ä»¤ï¼Œ ä¸€äººç”¨æ†â€œè¯±å¯¼â€];\n    C --> D[è·å¾—å¤šæ¨¡æ€äº¤äº’æ•°æ®é›† ğ’Ÿ];\n    D --> E[**é˜¶æ®µ2: ä»¿çœŸè®­ç»ƒ**];\n    E --> F[åœ¨ä»¿çœŸä¸­**é‡å»ºäº¤äº’åœºæ™¯**];\n    F --> G[åº”ç”¨**æ•°æ®èšåˆ** + **é¢†åŸŸéšæœºåŒ–**];\n    G --> H[åº”ç”¨**æ¸è¿›å¼ç›®æ ‡æç¤º**];\n    H --> I[è®­ç»ƒ**é«˜å±‚å¯¼èˆªç­–ç•¥** Ï€_g<br/>è¾“å…¥: æ‰‹åŠ¿+è¯­éŸ³+çŠ¶æ€+åœ°å›¾ï¼Œ è¾“å‡º: å¯¼èˆªç›®æ ‡];\n    I --> J[**é˜¶æ®µ3: å®æ—¶æ§åˆ¶**<br/>ç”¨æˆ·é€šè¿‡è‡ªç„¶æ‰‹åŠ¿å’Œè¯­éŸ³ç›´æ¥æ§åˆ¶æœºå™¨äºº];\n    J --> K[**ç»“æœ**: æ•æ·ã€é²æ£’ã€æ„å›¾å¯¹é½çš„å¯¼èˆª];\n```\n\n### **å››ã€ å®é™…ä»·å€¼ä¸æ„ä¹‰**\n\n- **å®ç”¨æ€§**ï¼šä¸ºéä¸“å®¶ç”¨æˆ·æä¾›äº†æå…¶ç›´è§‚çš„æœºå™¨äººæ•™å­¦ä¸æ§åˆ¶æ¥å£ï¼Œé™ä½äº†ä½¿ç”¨é—¨æ§›ã€‚\n- **é«˜æ•ˆæ€§**ï¼šé€šè¿‡ä»¿çœŸæ•°æ®èšåˆï¼Œæå¤§å‡å°‘äº†å¯¹æ˜‚è´µã€è€—æ—¶çš„çœŸå®ä¸–ç•Œæ•°æ®æ”¶é›†çš„ä¾èµ–ã€‚\n- **é²æ£’æ€§**ï¼šç­–ç•¥èƒ½å¤„ç†åŠ¨æ€ç¯å¢ƒã€ç”¨æˆ·å·®å¼‚ï¼ˆé€šè¿‡å°‘é‡æ•°æ®å¿«é€Ÿé€‚åº”æ–°ç”¨æˆ·ï¼‰ï¼Œå¹¶èƒ½æµç•…ç»„åˆå¤šç§æŠ€èƒ½ï¼ˆå¦‚ç»•è¡Œã€è·³è·ƒã€ä¹‹å­—å½¢è¡Œèµ°ï¼‰ã€‚\n- **å­¦æœ¯è´¡çŒ®**ï¼šä¸º**å¤šæ¨¡æ€äººæœºäº¤äº’**ã€**æ•°æ®é«˜æ•ˆçš„æ¨¡ä»¿å­¦ä¹ **ä»¥åŠ**ä»¿çœŸåˆ°çœŸå®çš„è¿ç§»**æä¾›äº†ç³»ç»Ÿæ€§çš„æ¡†æ¶å’Œå®è¯éªŒè¯ï¼Œå¼ºè°ƒäº†**æ‰‹åŠ¿ä¸è¯­éŸ³çš„äº’è¡¥æ€§**å¯¹äºæ¶ˆé™¤æ­§ä¹‰ã€ç²¾å‡†ä¼ è¾¾ç©ºé—´æ„å›¾çš„å…³é”®ä½œç”¨ã€‚",
            "timestamp": "2026-01-20T17:51:10.908343",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡ã€ŠTeaching Robots Like Dogs: Learning Agile Navigation from Luring, Gesture, and Speechã€‹æŠ€æœ¯è„‰ç»œä¸å‘å±•æ–¹å‘åˆ†æ\n\nåŸºäºè¯¥è®ºæ–‡åŠç›¸å…³æ–‡çŒ®ï¼Œè¯¥ç ”ç©¶æ–¹å‘ï¼ˆ**åŸºäºè‡ªç„¶äº¤äº’çš„æœºå™¨äººå¯¼èˆªä¸æ§åˆ¶**ï¼‰çš„å‘å±•è„‰ç»œå¯æ¢³ç†ä¸ºä»¥ä¸‹å‡ ä¸ªå…³é”®é˜¶æ®µå’Œåˆ†æ”¯æ–¹å‘ã€‚æ¯ä¸€æ­¥çš„æ”¹è¿›éƒ½æ—¨åœ¨è§£å†³å‰ä½œçš„å±€é™æ€§ï¼Œæ¨åŠ¨ç³»ç»Ÿæ›´**å®ç”¨ã€é«˜æ•ˆã€é²æ£’å’Œç›´è§‚**ã€‚\n\n### å‘å±•è„‰ç»œæ¢³ç†\n\n#### 1. **æ—©æœŸé˜¶æ®µï¼šåŸºäºé¢„å®šä¹‰æ˜ å°„çš„ç¦»æ•£æ§åˆ¶**\n*   **ä»£è¡¨æ€§å·¥ä½œ**ï¼šå¦‚Shin et al. (2024)ã€Zafar et al. (2024) ç­‰ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šå°†ä¸€ç»„**é¢„å®šä¹‰çš„äººç±»æ‰‹åŠ¿**ç›´æ¥æ˜ å°„åˆ°**ç¦»æ•£çš„æœºå™¨äººåŠ¨ä½œ**ï¼ˆå¦‚å‰è¿›ã€å·¦è½¬ã€åœæ­¢ï¼‰ã€‚\n*   **æ”¹è¿›ä¸å±€é™**ï¼š\n    *   **æ”¹è¿›**ï¼šç›¸æ¯”ä¼ ç»Ÿæ‘‡æ†ï¼Œæä¾›äº†æ›´ç›´è§‚çš„äº¤äº’æ–¹å¼ã€‚\n    *   **å±€é™**ï¼š\n        *   **è¡¨è¾¾èƒ½åŠ›æœ‰é™**ï¼šç¦»æ•£åŠ¨ä½œæ— æ³•å®ç°ç²¾ç»†ã€è¿ç»­çš„æ§åˆ¶ã€‚\n        *   **ç¼ºä¹æ³›åŒ–èƒ½åŠ›**ï¼šæ— æ³•å¤„ç†é¢„å®šä¹‰é›†åˆä¹‹å¤–çš„å‘½ä»¤æˆ–å¤æ‚ç¯å¢ƒã€‚\n        *   **æ¨¡æ€å•ä¸€**ï¼šé€šå¸¸åªä½¿ç”¨æ‰‹åŠ¿**æˆ–**è¯­éŸ³ï¼Œæœªèƒ½èåˆå¤šæ¨¡æ€ä¿¡æ¯ã€‚\n\n#### 2. **å‘å±•é˜¶æ®µï¼šè¿ç»­æ§åˆ¶ä¸æ¨¡ä»¿å­¦ä¹ **\n*   **ä»£è¡¨æ€§å·¥ä½œ**ï¼š\n    *   **è¿ç»­æ‰‹åŠ¿æ˜ å°„**ï¼šXie et al. (2025) å°†æ‰‹éƒ¨è¿åŠ¨é€Ÿåº¦çº§è”æ˜ å°„åˆ°æœºå™¨äººè¿ç»­æ§åˆ¶ã€‚\n    *   **æ¨¡ä»¿å­¦ä¹  (LfD) åº”ç”¨**ï¼šCuan et al. (2024) ä½¿ç”¨æ¨¡ä»¿å­¦ä¹ å°†ç”¨æˆ·æ‰‹åŠ¿æ˜ å°„ä¸ºæœºå™¨äººåŠ¨ä½œã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šè¿½æ±‚æ›´è‡ªç„¶ã€æ›´ç²¾ç»†çš„**è¿ç»­æ§åˆ¶**ï¼Œå¹¶åˆ©ç”¨**æ¼”ç¤ºæ•°æ®**å­¦ä¹ è¡Œä¸ºç­–ç•¥ï¼Œè€Œéç¡¬ç¼–ç ã€‚\n*   **æ”¹è¿›ä¸å±€é™**ï¼š\n    *   **æ”¹è¿›**ï¼š\n        *   å®ç°äº†æ›´å¹³æ»‘ã€æ›´æ‹Ÿäººçš„æœºå™¨äººè¿åŠ¨ã€‚\n        *   æ¨¡ä»¿å­¦ä¹ é™ä½äº†ä¸ºå¤æ‚ä»»åŠ¡æ‰‹å·¥è®¾è®¡ç­–ç•¥çš„éš¾åº¦ã€‚\n    *   **å±€é™**ï¼š\n        *   **ç”¨æˆ·è´Ÿæ‹…é‡**ï¼šè¿ç»­æ˜ å°„ä»éœ€ç”¨æˆ·é«˜åº¦ä¸“æ³¨ï¼Œè¿›è¡Œâ€œå¾®æ“â€ï¼Œç±»ä¼¼äºé«˜çº§æ‘‡æ†æ§åˆ¶ï¼Œå¹¶æœªå®Œå…¨è§£æ”¾ç”¨æˆ·ã€‚\n        *   **æ•°æ®æ•ˆç‡ä¸åˆ†å¸ƒåç§»**ï¼šçº¯æ¨¡ä»¿å­¦ä¹ ï¼ˆå¦‚BCï¼‰ä¸¥é‡ä¾èµ–å¤§é‡ã€é«˜è´¨é‡çš„æ¼”ç¤ºæ•°æ®ï¼Œä¸”å®¹æ˜“å› è®­ç»ƒä¸éƒ¨ç½²çŠ¶æ€åˆ†å¸ƒä¸åŒï¼ˆåˆ†å¸ƒåç§»ï¼‰è€Œæ€§èƒ½éª¤é™ã€‚\n        *   **è¯­ä¹‰æ¨¡ç³Šæ€§**ï¼šä¾‹å¦‚ï¼Œè¯­éŸ³å‘½ä»¤â€œå»é‚£é‡Œâ€ç¼ºä¹ç©ºé—´ groundingã€‚\n\n#### 3. **æ·±åŒ–é˜¶æ®µï¼šè¯­è¨€é©±åŠ¨å¯¼èˆªä¸ç›®æ ‡æ¡ä»¶ç­–ç•¥**\n*   **ä»£è¡¨æ€§å·¥ä½œ**ï¼š\n    *   **è¯­è¨€-è§†è§‰-åŠ¨ä½œæ¨¡å‹**ï¼šCheng et al. (2025) çš„NaVILAï¼Œé€šè¿‡æ–‡æœ¬æŒ‡å®šå¯¼èˆªç›®æ ‡ã€‚\n    *   **è¯­è¨€ grounding å¯¼èˆª**ï¼šSingh et al. (2022) å°†è‡ªç”±å½¢å¼è¯­è¨€æŒ‡ä»¤ï¼ˆå¦‚â€œå»åºŠè¾¹â€ï¼‰è½¬åŒ–ä¸ºç©ºé—´ç›®æ ‡ã€‚\n    *   **æ½œç©ºé—´è¯­è¨€æ§åˆ¶**ï¼šXue et al. (2025) å°†è§†è§‰-è¯­è¨€è¡¨å¾åµŒå…¥äººå½¢æœºå™¨äººå…¨èº«æ§åˆ¶å™¨ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šåˆ©ç”¨**é«˜çº§ã€æŠ½è±¡çš„è¯­è¨€æŒ‡ä»¤**æ¥é©±åŠ¨æœºå™¨äººï¼Œå®ç°**ç›®æ ‡å¯¼å‘**å’Œ**è¯­ä¹‰ç†è§£**çš„å¯¼èˆªã€‚\n*   **æ”¹è¿›ä¸å±€é™**ï¼š\n    *   **æ”¹è¿›**ï¼š\n        *   äº¤äº’æ›´ç¬¦åˆäººç±»ä¹ æƒ¯ï¼Œæ— éœ€å­¦ä¹ ç‰¹å®šæ§åˆ¶ä¿¡å·ã€‚\n        *   èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚ã€æ›´æŠ½è±¡çš„æŒ‡ä»¤ã€‚\n    *   **å±€é™**ï¼š\n        *   **ç©ºé—´æ¨¡ç³Šæ€§**ï¼šè¯­è¨€æœ¬èº«å¸¸ç¼ºä¹ç²¾ç¡®çš„ç©ºé—´ä¿¡æ¯ï¼ˆå¦‚â€œé‚£é‡Œâ€æŒ‡ä»£ä¸æ˜ï¼‰ã€‚\n        *   **å¤šæŠ€èƒ½ä¸æ•æ·æ€§**ï¼šæ—©æœŸå·¥ä½œå¤šå…³æ³¨äºå¹³é¢ç§»åŠ¨å¯¼èˆªï¼Œå¯¹å››è¶³æœºå™¨äººçš„**åŠ¨æ€ã€æ•æ·è¡Œä¸º**ï¼ˆå¦‚è·³è·ƒã€ç»•è¡Œï¼‰æ”¯æŒæœ‰é™ã€‚\n        *   **æ•°æ®ä¸æ³›åŒ–**ï¼šä»éœ€å¤§é‡äº¤äº’æ•°æ®ï¼Œä¸”å¯¹æœªè§è¿‡çš„æŒ‡ä»¤æˆ–ç”¨æˆ·æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚\n\n#### 4. **èåˆä¸é«˜æ•ˆå­¦ä¹ é˜¶æ®µï¼šå¤šæ¨¡æ€äº¤äº’ä¸æ•°æ®é«˜æ•ˆç®—æ³•ï¼ˆæœ¬è®ºæ–‡å·¥ä½œï¼‰**\n*   **ä»£è¡¨æ€§å·¥ä½œ**ï¼šæœ¬æ–‡æå‡ºçš„ **LURE** æ¡†æ¶ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼š**èåˆè¯­éŸ³ä¸æ‰‹åŠ¿**ä»¥äº’è¡¥ä¼˜åŠ¿ï¼Œå¹¶é‡‡ç”¨**æ•°æ®é«˜æ•ˆ**çš„å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨**å°‘é‡æ¼”ç¤º**å°±èƒ½å®ç°é²æ£’çš„æ•æ·å¯¼èˆªã€‚\n*   **ç›¸å¯¹äºå‰ä½œçš„ç³»ç»Ÿæ€§æ”¹è¿›**ï¼š\n\n| å¯¹æ¯”ç»´åº¦ | å‰ä½œå…¸å‹å±€é™ | æœ¬è®ºæ–‡ (LURE) çš„æ”¹è¿› |\n| :--- | :--- | :--- |\n| **äº¤äº’æ¨¡æ€** | **å•æ¨¡æ€ä¸ºä¸»**ï¼ˆä»…æ‰‹åŠ¿æˆ–ä»…è¯­è¨€ï¼‰ï¼Œå­˜åœ¨æ¨¡ç³Šæ€§æˆ–è¡¨è¾¾é™åˆ¶ã€‚ | **å¤šæ¨¡æ€èåˆ**ï¼šè¯­éŸ³æä¾›ä»»åŠ¡è¯­ä¹‰ï¼ˆâ€œåšä»€ä¹ˆâ€ï¼‰ï¼Œæ‰‹åŠ¿æä¾›ç©ºé—´ groundingï¼ˆâ€œåœ¨å“ªé‡Œâ€ï¼‰ï¼Œ**äº’è¡¥è§£å†³æ¨¡ç³Šæ€§**ï¼ˆè§è¡¨IIå®éªŒï¼‰ã€‚ |\n| **æ•°æ®æ•ˆç‡** | éœ€è¦**å¤§é‡**çœŸäººæ¼”ç¤ºæ•°æ®ï¼Œæ”¶é›†æˆæœ¬é«˜ã€‚ | **æ•°æ®é«˜æ•ˆ**ï¼šæå‡º **â€œå¼•è¯±-é‡å»º-èšåˆâ€** æµç¨‹ã€‚ç‰©ç†å¼•å¯¼ï¼ˆå¼•è¯±ï¼‰æ”¶é›†å°‘é‡æ•°æ® â†’ åœ¨ç‰©ç†ä»¿çœŸä¸­**é‡å»ºåœºæ™¯** â†’ åˆ©ç”¨ **DAggerå¼æ•°æ®èšåˆ** å’Œ**é¢†åŸŸéšæœºåŒ–** å¯¹æŠ—åˆ†å¸ƒåç§»ï¼Œç”¨ **<1å°æ—¶æ€»æ•°æ®** å­¦ä¹ 6ç§æŠ€èƒ½ã€‚ |\n| **å­¦ä¹ é²æ£’æ€§** | æ¨¡ä»¿å­¦ä¹ æ˜“å—**åˆ†å¸ƒåç§»**å½±å“ï¼Œåœ¨åç¦»æ¼”ç¤ºè½¨è¿¹æ—¶å¤±è´¥ã€‚ | **ä»¿çœŸå†…æ•°æ®èšåˆä¸å¢å¼º**ï¼šåœ¨é‡å»ºçš„ä»¿çœŸç¯å¢ƒä¸­ï¼Œè®©ç­–ç•¥åœ¨éšæœºæ‰°åŠ¨ä¸‹æ¢ç´¢ï¼Œå¹¶ç”¨**å±€éƒ¨ä¸“å®¶ç­–ç•¥**æä¾›çº æ­£åŠ¨ä½œï¼Œ**ä¸»åŠ¨å­¦ä¹ æ¢å¤èƒ½åŠ›**ï¼Œå¤§å¹…æå‡é²æ£’æ€§ï¼ˆå›¾6ï¼ŒDAgger vs BCï¼‰ã€‚ |\n| **æŒ‡ä»¤-è¡Œä¸ºå¯¹é½** | è®­ç»ƒæ—¶æŒ‡ä»¤æŒ‰å›ºå®šæ—¶é—´æˆ³æ’­æ”¾ï¼Œä¸å­¦ä¹ ä¸­çš„æœºå™¨äººçŠ¶æ€å¯èƒ½**é”™ä½**ï¼Œå¯¼è‡´ç›‘ç£ä¿¡å·æ··ä¹±ã€‚ | **æ¸è¿›å¼ç›®æ ‡æç¤º**ï¼šæ ¹æ®æœºå™¨äººå½“å‰çŠ¶æ€ï¼ˆæ˜¯å¦åˆ°è¾¾å½“å‰ç›®æ ‡ï¼‰åŠ¨æ€æ¨è¿›æŒ‡ä»¤ï¼Œç¡®ä¿è®­ç»ƒæ—¶**æŒ‡ä»¤ä¸æœºå™¨äººè¿›åº¦å¯¹é½**ï¼Œä»è€Œå­¦åˆ°æ›´ç²¾ç¡®çš„è¡Œä¸ºï¼ˆå›¾6ï¼ŒLURE vs DAggerï¼‰ã€‚ |\n| **æŠ€èƒ½èŒƒå›´** | å¤šé›†ä¸­äº**åŸºç¡€ç§»åŠ¨**æˆ–**å®¤å†…å¯¼èˆª**ã€‚ | **æ•æ·å¯¼èˆª**ï¼šä¸“æ³¨äºå››è¶³æœºå™¨äººåœ¨æ‚ä¹±ç¯å¢ƒä¸­çš„**åŠ¨æ€æŠ€èƒ½**ï¼Œå¦‚â€œè·³è¿‡å»â€ã€â€œç»•è¿‡æ¥â€ã€â€œä¹‹å­—å½¢â€é€šè¿‡ï¼ˆå›¾4ï¼‰ï¼Œå¹¶ä¸é¢„è®­ç»ƒçš„**ä½å±‚æ•æ·è¿åŠ¨æ§åˆ¶å™¨**è§£è€¦é›†æˆã€‚ |\n| **ç”¨æˆ·é€‚åº”æ€§** | é€šå¸¸å‡è®¾ç”¨æˆ·é£æ ¼ä¸€è‡´ï¼Œé›¶æ ·æœ¬æ³›åŒ–åˆ°æ–°ç”¨æˆ·éš¾ã€‚ | **å¿«é€Ÿç”¨æˆ·é€‚åº”**ï¼šåŸºç¡€æ¨¡å‹åœ¨æ–°ç”¨æˆ·ä¸Šå·²æœ‰ä¸€å®šè¡¨ç°ï¼Œä»…éœ€**å¹³å‡4.5åˆ†é’Ÿ**æ–°æ•°æ®å¾®è°ƒï¼ŒæˆåŠŸç‡å¹³å‡æå‡ **22.11%**ï¼ˆè¡¨Iï¼‰ï¼Œå®ç°äº†**å¿«é€Ÿä¸ªæ€§åŒ–**ã€‚ |\n\n### ç€é‡å‘å±•çš„å‡ ä¸ªä¸åŒæ–¹å‘\n\nä»è„‰ç»œçœ‹ï¼Œè¯¥é¢†åŸŸä¸»è¦æ²¿ä¸‰ä¸ªæ ¸å¿ƒæ–¹å‘æ·±åŒ–ï¼š\n\n1.  **äº¤äº’è‡ªç„¶æ€§ä¸å¤šæ¨¡æ€èåˆæ–¹å‘**\n    *   **ç›®æ ‡**ï¼šä½¿äººæœºäº¤äº’æ— é™æ¥è¿‘äººäººäº¤äº’ã€‚\n    *   **æ¼”è¿›**ï¼šç¦»æ•£å‘½ä»¤ â†’ è¿ç»­æ§åˆ¶ â†’ è‡ªç„¶è¯­è¨€ â†’ **å¤šæ¨¡æ€ï¼ˆè¯­éŸ³+æ‰‹åŠ¿+è§†çº¿ç­‰ï¼‰èåˆ**ã€‚æœ¬è®ºæ–‡æ˜¯æ­¤æ–¹å‘çš„å…¸å‹ä»£è¡¨ï¼Œé€šè¿‡å¤šæ¨¡æ€è§£å†³äº†å•ä¸€æ¨¡æ€çš„å›ºæœ‰ç¼ºé™·ã€‚\n\n2.  **å­¦ä¹ ç®—æ³•æ•°æ®æ•ˆç‡ä¸é²æ£’æ€§æ–¹å‘**\n    *   **ç›®æ ‡**ï¼šç”¨å°½å¯èƒ½å°‘ã€æˆæœ¬ä½çš„çœŸå®æ•°æ®ï¼Œå­¦åˆ°èƒ½åº”å¯¹ç°å®ä¸–ç•Œå˜åŒ–çš„é²æ£’ç­–ç•¥ã€‚\n    *   **æ¼”è¿›**ï¼šè¡Œä¸ºå…‹éš†ï¼ˆBCï¼Œæ•°æ®é¥¥æ¸´ï¼‰ â†’ é€†å¼ºåŒ–å­¦ä¹ /å¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ï¼ˆGAILï¼Œä¸ç¨³å®šï¼‰ â†’ **åœ¨çº¿æ•°æ®èšåˆï¼ˆDAggerï¼‰** â†’ **ä»¿çœŸé‡å»º + æ•°æ®èšåˆ + é¢†åŸŸéšæœºåŒ–**ï¼ˆæœ¬è®ºæ–‡ï¼‰ã€‚æœ¬è®ºæ–‡å°†DAggerä¸**ç‰©ç†ä»¿çœŸåœºæ™¯é‡å»º**ç»“åˆï¼Œåˆ›é€ äº†é«˜æ•ˆã€å®‰å…¨çš„â€œæ•°æ®åˆ¶é€ å‚â€ã€‚\n\n3.  **ä»»åŠ¡å¤æ‚æ€§ä¸ç³»ç»Ÿå±‚çº§åŒ–æ–¹å‘**\n    *   **ç›®æ ‡**ï¼šä»ç®€å•ä»»åŠ¡æ‰©å±•åˆ°å¤æ‚ã€åŠ¨æ€çš„å…¨èº«è¡Œä¸ºæ§åˆ¶ã€‚\n    *   **æ¼”è¿›**ï¼šå¹³é¢å¯¼èˆª â†’ é¿éšœ â†’ **åŠ¨æ€æ•æ·è¿åŠ¨ï¼ˆè·‘ã€è·³ï¼‰** â†’ **åˆ†å±‚æ¶æ„ï¼ˆé«˜å±‚å¯¼èˆªè§„åˆ’+ä½å±‚è¿åŠ¨æ‰§è¡Œï¼‰**ã€‚æœ¬è®ºæ–‡é‡‡ç”¨äº†æ˜ç¡®çš„åˆ†å±‚æ¶æ„ï¼ˆ`Ï€_g` å’Œ `Ï€_u`ï¼‰ï¼Œå°†**æ„å›¾ç†è§£**ä¸**æ•æ·æ‰§è¡Œ**åˆ†ç¦»ï¼Œä½¿å¾—é«˜å±‚äº¤äº’ç­–ç•¥å¯ä»¥ä¸“æ³¨äºç†è§£ï¼Œè€Œæ— éœ€å­¦ä¹ åº•å±‚çš„å¤æ‚åŠ¨åŠ›å­¦ã€‚\n\n**æ€»ç»“**ï¼šæœ¬è®ºæ–‡ï¼ˆLUREï¼‰ç«™åœ¨äº†è¯¥å‘å±•è„‰ç»œçš„å½“å‰å‰æ²¿ï¼Œå®ƒ**ç³»ç»Ÿæ€§åœ°æ•´åˆäº†å¤šæ¨¡æ€äº¤äº’ã€æ•°æ®é«˜æ•ˆå­¦ä¹ å’Œåˆ†å±‚æ•æ·æ§åˆ¶**è¿™ä¸‰ä¸ªå…³é”®æ–¹å‘ä¸Šçš„å…ˆè¿›æ€æƒ³ï¼Œå¹¶é€šè¿‡**æ¸è¿›å¼ç›®æ ‡æç¤º**ç­‰åˆ›æ–°ç»†èŠ‚è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæœ€ç»ˆå®ç°äº†ä¸€ä¸ª**ä»…éœ€å°‘é‡æ¼”ç¤ºã€èƒ½é€šè¿‡è‡ªç„¶æ–¹å¼æ§åˆ¶å››è¶³æœºå™¨äººå®ŒæˆåŠ¨æ€å¯¼èˆªä»»åŠ¡**çš„å®ç”¨ç³»ç»Ÿã€‚å…¶æœªæ¥æ–¹å‘ï¼ˆå¦‚åŸºäºå­¦ä¹ çš„åŠ¨æ€äººç±»æ¨¡å‹ã€æ›´é€šç”¨çš„è§†è§‰æ„ŸçŸ¥ã€è¯­ä¹‰åŒ–æ‰‹åŠ¿è¡¨å¾ï¼‰ä¹Ÿæ¸…æ™°åœ°æŒ‡å‡ºäº†è¯¥é¢†åŸŸä¸‹ä¸€æ­¥çš„çªç ´ç‚¹ã€‚",
            "timestamp": "2026-01-20T17:52:14.375221",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡ã€ŠTeaching Robots Like Dogs: Learning Agile Navigation from Luring, Gesture, and Speechã€‹çš„å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œæœ¬æ–‡çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ä¸ªåä¸º **LURE** çš„å¤šæ¨¡æ€äººæœºäº¤äº’æ¡†æ¶ï¼Œç”¨äºæ•°æ®é«˜æ•ˆåœ°æ•™å››è¶³æœºå™¨äººè¿›è¡Œæ•æ·å¯¼èˆªã€‚è®ºæ–‡åœ¨â€œç›¸å…³å·¥ä½œâ€éƒ¨åˆ†å¼•ç”¨äº†å¤šç¯‡å‰ä½œï¼Œè¿™äº›å·¥ä½œä¸ºæœ¬æ–‡çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚ä»¥ä¸‹å°†ä¸»è¦åŸºäºè®ºæ–‡å¼•ç”¨çš„**ç›´æ¥ç›¸å…³å‰ä½œ**ï¼Œä»¥è¡¨æ ¼å½¢å¼è¿›è¡Œè¯¦ç»†å¯¹æ¯”åˆ†æã€‚\n\néœ€è¦è¯´æ˜çš„æ˜¯ï¼Œè®ºæ–‡ä½œè€…å›¢é˜Ÿï¼ˆå¦‚Sungjoon Choi, Stelian Corosç­‰ï¼‰å¯èƒ½æœ‰å¤šç¯‡ç›¸å…³ç ”ç©¶ï¼Œä½†æœ¬æ–‡æ­£æ–‡ä¸­æ˜ç¡®å¼•ç”¨å¹¶ä½œä¸ºåŸºçº¿æˆ–å¯¹æ¯”çš„â€œå‰ä½œâ€ä¸»è¦æŒ‡**ç ”ç©¶æ–¹æ³•è®ºä¸Šçš„å…ˆé©±å·¥ä½œ**ï¼Œè€Œéç‰¹æŒ‡åŒä¸€ä½œè€…å›¢é˜Ÿçš„ç³»åˆ—è®ºæ–‡ã€‚å› æ­¤ï¼Œä¸‹è¡¨å°†èšç„¦äºè®ºæ–‡ä¸­æ˜ç¡®å¯¹æ¯”çš„**æ ¸å¿ƒåŸºçº¿æ–¹æ³•**åŠå…¶ç›¸å…³ä»£è¡¨æ€§å·¥ä½œã€‚\n\n### æ ¸å¿ƒå‰ä½œä¸æœ¬æ–‡å¯¹æ¯”åˆ†æè¡¨\n\n| å‰ä½œ/åŸºçº¿æ–¹æ³• | æ ¸å¿ƒæ€æƒ³ | ä¸æœ¬æ–‡ï¼ˆLUREï¼‰çš„ä¸»è¦åŒºåˆ« | æœ¬æ–‡çš„ä¸»è¦æ”¹å–„ä¸åˆ›æ–° |\n| :--- | :--- | :--- | :--- |\n| **1. è¡Œä¸ºå…‹éš† (BC)** <br> Pomerleau (1989) [12] | **çº¯æ¨¡ä»¿å­¦ä¹ **ï¼šç›´æ¥ä»ä¸“å®¶æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ ç­–ç•¥ï¼Œæœ€å°åŒ–åŠ¨ä½œé¢„æµ‹è¯¯å·®ã€‚ | - **æ•°æ®åˆ©ç”¨**ï¼šä»…ä½¿ç”¨é™æ€çš„æ¼”ç¤ºæ•°æ®é›†ã€‚<br>- **åˆ†å¸ƒåç§»**ï¼šæ— æ³•å¤„ç†è®­ç»ƒæ•°æ®åˆ†å¸ƒä¹‹å¤–çš„çŠ¶æ€ï¼Œå®¹æ˜“å› ç´¯ç§¯è¯¯å·®å¯¼è‡´å¤±è´¥ã€‚<br>- **äº¤äº’æ€§**ï¼šéäº¤äº’å¼è®­ç»ƒï¼Œæ— æ³•ä»é”™è¯¯ä¸­å­¦ä¹ ã€‚ | - **å¼•å…¥æ•°æ®èšåˆ (Data Aggregation)**ï¼šåœ¨ä»¿çœŸä¸­é‡å»ºäº¤äº’åœºæ™¯ï¼Œè®©æ™ºèƒ½ä½“åœ¨è‡ªèº«ç­–ç•¥äº§ç”Ÿçš„çŠ¶æ€åˆ†å¸ƒä¸‹æ¢ç´¢ï¼Œå¹¶èšåˆä¸“å®¶çº æ­£åŠ¨ä½œã€‚è¿™æ˜¾è‘—ç¼“è§£äº†åˆ†å¸ƒåç§»é—®é¢˜ã€‚<br>- **ç»“æœ**ï¼šç›¸æ¯”BCï¼Œå¯¼èˆªè¯¯å·®é™ä½24.6%ï¼ŒæˆåŠŸç‡æå‡18.6%ã€‚ |\n| **2. DAgger** <br> Ross et al. (2011) [14] | **æ•°æ®èšåˆç®—æ³•**ï¼šä¸€ç§æ¨¡ä»¿å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡è¿­ä»£åœ°è®©å½“å‰ç­–ç•¥ä¸ç¯å¢ƒäº¤äº’ï¼Œå¹¶æŸ¥è¯¢ä¸“å®¶æä¾›è¿™äº›æ–°çŠ¶æ€ä¸‹çš„åŠ¨ä½œï¼Œæ¥èšåˆæ•°æ®ï¼Œä»¥è§£å†³åˆ†å¸ƒåç§»ã€‚ | - **ç›®æ ‡æç¤ºæœºåˆ¶**ï¼šåœ¨æœ¬æ–‡çš„DAggeråŸºçº¿ä¸­ï¼Œäº¤äº’å‘½ä»¤ï¼ˆæ‰‹åŠ¿ã€è¯­éŸ³ï¼‰æŒ‰ç…§ä»¿çœŸæ—¶é’ŸåŸæ ·å›æ”¾ï¼Œ**æœªè€ƒè™‘æœºå™¨äººå½“å‰çŠ¶æ€**ã€‚<br>- **è®­ç»ƒå¯¹é½**ï¼šå½“ç­–ç•¥æ—©æœŸè®­ç»ƒç¼“æ…¢æ—¶ï¼Œæ–°çš„å‘½ä»¤å¯èƒ½åœ¨æœºå™¨äººæœªå®Œæˆä¸Šä¸€ä¸ªåŠ¨ä½œæ—¶å°±å‘å‡ºï¼Œå¯¼è‡´**çŠ¶æ€-å‘½ä»¤é”™ä½**ï¼Œäº§ç”Ÿä¸ä¸€è‡´çš„ç›‘ç£ä¿¡å·ã€‚ | - **æå‡ºæ¸è¿›å¼ç›®æ ‡æç¤º (Progressive Goal Cuing)**ï¼šåŠ¨æ€åœ°æ ¹æ®æœºå™¨äººçŠ¶æ€ï¼ˆæ˜¯å¦åˆ°è¾¾å½“å‰ç›®æ ‡ï¼‰æ¥æä¾›ä¸‹ä¸€ä¸ªäº¤äº’å‘½ä»¤å’Œå¯¼èˆªç›®æ ‡ã€‚è¿™ç¡®ä¿äº†å‘½ä»¤ä¸æœºå™¨äººè¡Œä¸ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¼ºå¯¹é½ã€‚<br>- **ç»“æœ**ï¼šåœ¨DAggeråŸºç¡€ä¸Šï¼Œå¯¼èˆªè¯¯å·®è¿›ä¸€æ­¥é™ä½15.2%ï¼ŒæˆåŠŸç‡æå‡13.7%ã€‚ |\n| **3. GAIL** <br> Ho & Ermon (2016) [9] | **ç”Ÿæˆå¯¹æŠ—å¼æ¨¡ä»¿å­¦ä¹ **ï¼šé€šè¿‡å¯¹æŠ—è®­ç»ƒï¼Œä½¿ç­–ç•¥ç”Ÿæˆçš„è¡Œä¸ºåˆ†å¸ƒä¸ä¸“å®¶æ¼”ç¤ºåˆ†å¸ƒåŒ¹é…ï¼Œæ—¨åœ¨è·å¾—æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ | - **è®­ç»ƒç¨³å®šæ€§**ï¼šGAILå› å…¶å¯¹æŠ—è®­ç»ƒæ€§è´¨ï¼Œå¸¸é¢ä¸´è®­ç»ƒä¸ç¨³å®šå’Œæ¨¡å¼åå¡Œçš„é—®é¢˜ã€‚<br>- **å¤šæŠ€èƒ½æ€§èƒ½**ï¼šåœ¨æœ¬æ–‡éœ€è¦åŒæ—¶å­¦ä¹ å¤šç§å¯¼èˆªæŠ€èƒ½çš„åœºæ™¯ä¸‹ï¼ŒGAILçš„æ•´ä½“è¡¨ç°å¯èƒ½ä¸‹é™ã€‚ | - **é‡‡ç”¨æ›´ç¨³å®šã€æ•°æ®é«˜æ•ˆçš„å­¦ä¹ æ¡†æ¶**ï¼šç»“åˆäº†DAggerçš„æ•°æ®èšåˆæ€æƒ³å’Œç²¾å¿ƒè®¾è®¡çš„æ¸è¿›å¼ç›®æ ‡æç¤ºï¼Œé¿å…äº†GAILçš„ä¸ç¨³å®šæ€§ã€‚<br>- **ç»“æœ**ï¼šGAILåœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„â€œä¹‹å­—å½¢â€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºBCï¼Œä½†åœ¨æ‰€æœ‰æŠ€èƒ½ä¸Šçš„å¹³å‡æˆåŠŸç‡æ¯”BCä½29.32%ï¼Œè€ŒLUREå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚ |\n| **4. å•æ¨¡æ€äº¤äº’ç ”ç©¶** <br> e.g., Xie et al. [22] (æ‰‹åŠ¿), <br> Cheng et al. [5] (è¯­è¨€) | **å­¤ç«‹ä½¿ç”¨æ‰‹åŠ¿æˆ–è¯­è¨€**ï¼šåˆ†åˆ«æ¢ç´¢åŸºäºæ‰‹åŠ¿çš„è¿ç»­æ§åˆ¶æˆ–åŸºäºè¯­è¨€çš„å¯¼èˆªï¼Œä½†æœªå°†ä¸¤è€…åœ¨**å®æ—¶**æ§åˆ¶ä¸­æ·±åº¦èåˆã€‚ | - **æ¨¡æ€å­¤ç«‹**ï¼šä»…ä½¿ç”¨å•ä¸€æ¨¡æ€ï¼Œå­˜åœ¨å›ºæœ‰ç¼ºé™·ã€‚<br>  - **ä»…æ‰‹åŠ¿**ï¼šéœ€è¦ç”¨æˆ·æŒç»­å…³æ³¨å’Œç²¾ç»†æ§åˆ¶ï¼Œç±»ä¼¼æ‘‡æ†ã€‚<br>  - **ä»…è¯­è¨€**ï¼šå­˜åœ¨ç©ºé—´æ¨¡ç³Šæ€§ï¼ˆå¦‚â€œå»é‚£é‡Œâ€éœ€è¦é¢å¤–çº¿ç´¢ï¼‰ã€‚<br>- **æ¥å£ç›´è§‚æ€§**ï¼šæœªèƒ½åƒäººç±»äº¤æµä¸€æ ·è‡ªç„¶ç»“åˆå¤šç§çº¿ç´¢ã€‚ | - **æå‡ºç»Ÿä¸€çš„å¤šæ¨¡æ€å®æ—¶äº¤äº’æ¥å£**ï¼š**åŒæ—¶**åˆ©ç”¨æ‰‹åŠ¿å’Œè¯­éŸ³å‘½ä»¤ï¼Œè®©å®ƒä»¬**äº’è¡¥**ã€‚<br>  - **æ‰‹åŠ¿**ï¼šæä¾›ç©ºé—´ groundingï¼ˆæŒ‡å‘å“ªé‡Œï¼‰ã€‚<br>  - **è¯­éŸ³**ï¼šæä¾›è¯­ä¹‰æ„å›¾ï¼ˆåšä»€ä¹ˆï¼Œå¦‚â€œè·³è¿‡å»â€ï¼‰ã€‚<br>- **ç»“æœ**ï¼šæ¶ˆèå®éªŒè¡¨æ˜ï¼ŒåŒæ¨¡æ€æ¯”ä»…ç”¨æ‰‹åŠ¿æˆåŠŸç‡å¹³å‡é«˜70.67%ï¼Œæ¯”ä»…ç”¨è¯­è¨€å¯¼èˆªè¯¯å·®ä½1.25ç±³ã€‚ |\n| **5. ä¼ ç»Ÿç¤ºæ•™å­¦ä¹  (LfD) for Navigation** <br> e.g., Wu et al. [21], Singh et al. [17] | **ä»æ¼”ç¤ºä¸­å­¦ä¹ å¯¼èˆª**ï¼šä»ä¸“å®¶è½¨è¿¹å­¦ä¹ å•ç›®æ ‡æˆ–å¤šæŠ€èƒ½å¯¼èˆªï¼Œå¸¸ä½¿ç”¨ç¯å¢ƒç‰¹å¾å›¾æˆ– grounding è¯­è¨€æŒ‡ä»¤åˆ°ç©ºé—´ç›®æ ‡ã€‚ | - **æ•°æ®æ•ˆç‡ä¸åˆ†å¸ƒåç§»**ï¼šè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ¼”ç¤ºæ•°æ®ï¼Œä¸”å¯¹ç”±æœ‰é™æ•°æ®å¼•èµ·çš„åˆ†å¸ƒåç§»é—®é¢˜å…³æ³¨ä¸è¶³æˆ–è§£å†³æ–¹æ³•ä¸åŒã€‚<br>- **ç‰©ç†äº¤äº’ä¸ä»¿çœŸåˆ©ç”¨**ï¼šè¾ƒå°‘æ¶‰åŠé€šè¿‡**ç‰©ç†å¼•å¯¼ï¼ˆå¼•è¯±ï¼‰**æ”¶é›†æ•°æ®ï¼Œå¹¶åœ¨**ç‰©ç†ä»¿çœŸä¸­é‡å»ºåœºæ™¯**è¿›è¡Œé«˜æ•ˆæ•°æ®å¢å¼ºå’Œè®­ç»ƒã€‚ | - **ç«¯åˆ°ç«¯çš„â€œå¼•è¯±-ä»¿çœŸé‡å»º-è®­ç»ƒâ€æµç¨‹**ï¼š<br>  1. **å¼•è¯±æ”¶é›†**ï¼šå—è®­çŠ¬æŠ€æœ¯å¯å‘ï¼Œç”¨æ•™å­¦æ†ç‰©ç†å¼•å¯¼æœºå™¨äººï¼ŒåŒæ—¶è®°å½•å¤šæ¨¡æ€å‘½ä»¤ï¼Œæ•°æ®æ•ˆç‡é«˜ï¼ˆæ€»æ•°æ®<1å°æ—¶ï¼‰ã€‚<br>  2. **ä»¿çœŸé‡å»ºä¸èšåˆ**ï¼šåœ¨ç‰©ç†ä»¿çœŸä¸­ç²¾ç¡®é‡å»ºäº¤äº’åœºæ™¯è¿›è¡Œè®­ç»ƒï¼Œç»“åˆ**é¢†åŸŸéšæœºåŒ–**ï¼ˆè´¨é‡ã€å¤–åŠ›ã€åœ°å½¢ç¼©æ”¾ã€å¹²æ‰°ç‰©ï¼‰å’Œ**æ•°æ®å¢å¼º**ï¼ˆæ‰‹åŠ¿å™ªå£°ã€LLMç”ŸæˆåŒä¹‰è¯­éŸ³ï¼‰ï¼Œæå¤§æå‡äº†ç­–ç•¥çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ |\n\n### æ€»ç»“ï¼šæœ¬æ–‡ç›¸å¯¹äºå‰ä½œçš„æ ¸å¿ƒæ”¹è¿›\n\n1.  **æ–¹æ³•è®ºåˆ›æ–°**ï¼šåˆ›é€ äº† **LURE** æ¡†æ¶ï¼Œå°† **â€œå¼•è¯±å¼â€ç‰©ç†äººæœºäº¤äº’æ•°æ®æ”¶é›†**ã€**ç‰©ç†ä»¿çœŸåœºæ™¯é‡å»º**ã€**é’ˆå¯¹å¤šæ¨¡æ€äº¤äº’çš„æ•°æ®èšåˆä¸å¢å¼º**ï¼Œä»¥åŠ**æ¸è¿›å¼ç›®æ ‡æç¤º**æœ‰æœºç»“åˆï¼Œå½¢æˆäº†ä¸€ä¸ª**æ•°æ®é«˜æ•ˆ**ï¼ˆ<1å°æ—¶æ•°æ®ï¼‰ä¸”**æ€§èƒ½å¼ºå¤§**ï¼ˆ97.15%æˆåŠŸç‡ï¼‰çš„é—­ç¯å­¦ä¹ ç³»ç»Ÿã€‚\n\n2.  **æŠ€æœ¯ç‚¹æ·±åŒ–**ï¼š\n    *   **ç›¸å¯¹äºBC/DAgger**ï¼šå¼•å…¥äº†**æ¸è¿›å¼ç›®æ ‡æç¤º**ï¼Œè§£å†³äº†äº¤äº’è®­ç»ƒä¸­çš„æ—¶åºå¯¹é½é—®é¢˜ï¼Œè¿™æ˜¯å¯¹ç»å…¸æ•°æ®èšåˆç®—æ³•åœ¨äº¤äº’å¼åœºæ™¯ä¸‹çš„é‡è¦æ”¹è¿›ã€‚\n    *   **ç›¸å¯¹äºå•æ¨¡æ€ç ”ç©¶**ï¼šå®è¯äº†**æ‰‹åŠ¿ä¸è¯­éŸ³åœ¨å¤šæ¨¡æ€æ¥å£ä¸­çš„äº’è¡¥æ€§**ï¼Œå¹¶æ„å»ºäº†ç»Ÿä¸€çš„å®æ—¶å¤„ç†ç®¡é“ã€‚\n    *   **ç›¸å¯¹äºä¼ ç»ŸLfD**ï¼šå¼ºè°ƒå¹¶ç³»ç»ŸåŒ–åœ°è§£å†³äº†**å°è§„æ¨¡æ¼”ç¤ºæ•°æ®ä¸‹çš„åˆ†å¸ƒåç§»é—®é¢˜**ï¼Œé€šè¿‡ä»¿çœŸé‡å»ºå’Œä¸€ç³»åˆ—å¢å¼ºæŠ€æœ¯å®ç°äº†å¼ºé²æ£’æ€§ã€‚\n\n3.  **å®é™…ä»·å€¼æå‡**ï¼šä½¿å¾—éä¸“ä¸šç”¨æˆ·èƒ½å¤Ÿé€šè¿‡æœ€è‡ªç„¶çš„â€œæ‰‹åŠ¿+è¯­éŸ³â€æ–¹å¼ï¼Œåœ¨**æ‚ä¹±ç¯å¢ƒ**ä¸­å®æ—¶æ•™å¯¼å’Œæ§åˆ¶å››è¶³æœºå™¨äººå®Œæˆ**æ•æ·å¯¼èˆª**ï¼ˆå¦‚ç»•è¡Œã€è·³è·ƒï¼‰ï¼Œå¤§å¤§é™ä½äº†æœºå™¨äººæ“ä½œçš„é—¨æ§›ï¼Œæé«˜äº†äººæœºäº¤äº’çš„ç›´è§‚æ€§å’Œå®ç”¨æ€§ã€‚",
            "timestamp": "2026-01-20T17:53:08.270263",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## è®ºæ–‡å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\næœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º **LURE** çš„äººæœºäº¤äº’æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°‘é‡æ¼”ç¤ºæ•°æ®ï¼Œè®©å››è¶³æœºå™¨äººå­¦ä¹ ç†è§£äººç±»çš„å¤šæ¨¡æ€ï¼ˆæ‰‹åŠ¿å’Œè¯­éŸ³ï¼‰ç¤¾äº¤çº¿ç´¢ï¼Œå¹¶æ‰§è¡Œæ•æ·å¯¼èˆªä»»åŠ¡ã€‚å…¶æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åœ¨äº**æ•°æ®é«˜æ•ˆå­¦ä¹ **å’Œ**å¤šæ¨¡æ€æ„å›¾å¯¹é½**ã€‚\n\nä»¥ä¸‹æ˜¯è®ºæ–‡æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•åŠå…¶è¯¦ç»†æŠ€æœ¯ç»†èŠ‚ï¼š\n\n| æ–¹æ³•åç§° | æ ¸å¿ƒç›®æ ‡ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°/å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. åŸºäºâ€œå¼•è¯±â€çš„æ•°æ®æ”¶é›†** | ä½æˆæœ¬è·å–å¤šæ¨¡æ€äº¤äº’æ•°æ®å¯¹ `(c, g*)`ï¼Œå…¶ä¸­ `c` ä¸ºäº¤äº’æŒ‡ä»¤ï¼ˆæ‰‹åŠ¿ `m` + è¯­éŸ³ `v`ï¼‰ï¼Œ`g*` ä¸ºæœŸæœ›å¯¼èˆªç›®æ ‡ã€‚ | **åŸç†**ï¼šæ¨¡ä»¿ä¸“ä¸šè®­çŠ¬å¸ˆçš„â€œå¼•è¯±â€æŠ€æœ¯ã€‚ç”±ä¸¤åå‚ä¸è€…åä½œï¼šä¸€äººå‘å‡ºè¯­éŸ³å’Œæ‰‹åŠ¿æŒ‡ä»¤ï¼Œå¦ä¸€äººä½¿ç”¨æ•™å­¦æ†ç‰©ç†å¼•å¯¼æœºå™¨äººåˆ°è¾¾ç›®æ ‡ä½ç½® `g*`ã€‚ç³»ç»ŸåŒæ­¥è®°å½•æ‰€æœ‰æ•°æ®ã€‚ <br> **æ•°å­¦è¡¨ç¤º**ï¼šæ•°æ®é›†åˆ `ğ’Ÿ = {v, m, Ï, x, g*}`ã€‚å…¶ä¸­ `Ï` ä¸ºç¯å¢ƒé«˜åº¦å›¾ï¼Œ`x` ä¸ºæœºå™¨äººçŠ¶æ€ï¼ˆä½ç½®ã€å§¿æ€ã€å…³èŠ‚è§’åŠé€Ÿåº¦ï¼‰ã€‚ | - æ•°æ®è®°å½•é¢‘ç‡ï¼š**10 Hz**ã€‚ <br> - æ€»æ¼”ç¤ºæ•°æ®é‡ï¼š**< 1å°æ—¶**ï¼ˆè¦†ç›–6ä¸ªåœºæ™¯ï¼‰ã€‚ <br> - æ‰‹åŠ¿æ•æ‰ï¼šä½¿ç”¨ **OptiTrack Prime X22** åŠ¨ä½œæ•æ‰ç³»ç»Ÿã€‚ |\n| **2. åŸºäºç‰©ç†ä»¿çœŸçš„æ•°æ®èšåˆ** | åœ¨ä»¿çœŸä¸­é‡å»ºäº¤äº’åœºæ™¯ï¼Œé€šè¿‡æ•°æ®èšåˆï¼ˆDAggerï¼‰ç¼“è§£å› æ¼”ç¤ºæ•°æ®æœ‰é™å¯¼è‡´çš„**åˆ†å¸ƒåç§»**é—®é¢˜ï¼Œä½¿ç­–ç•¥èƒ½å­¦ä¹ ä»é”™è¯¯ä¸­æ¢å¤ã€‚ | **åŸç†**ï¼šåœ¨ä»¿çœŸä¸­å›æ”¾è®°å½•çš„åœºæ™¯ï¼ˆåœ°å½¢ `Ï`ã€æŒ‡ä»¤ `c`ï¼‰ã€‚å®šä¹‰**å±€éƒ¨ä¸“å®¶ç­–ç•¥** `Ï€Ìƒ_g`ï¼Œå®ƒåœ¨æœºå™¨äººåæ ‡ç³»ä¸­ç”ŸæˆæŒ‡å‘å…¨å±€ç›®æ ‡ `g*` çš„å¯¼èˆªå­ç›®æ ‡ `gÌƒ`ã€‚å½“å½“å‰ç­–ç•¥ `Ï€_g` å¯¼è‡´çŠ¶æ€åç¦»æ¼”ç¤ºåˆ†å¸ƒæ—¶ï¼Œ`Ï€Ìƒ_g` æä¾›çº æ­£åŠ¨ä½œå¹¶åŠ å…¥èšåˆæ•°æ®é›†ã€‚ <br> **æ•°å­¦ä¼˜åŒ–ç›®æ ‡**ï¼ˆç®€åŒ–ï¼‰ï¼š <br> `arg min_Ï€_g ğ”¼_(c,x,Ï,g*)âˆ¼ğ’Ÿ^Ï€_g [ â€– g* - Ï€_g(c, x, Ï) â€–Â² ]` <br> å…¶ä¸­ `ğ’Ÿ^Ï€_g` æ˜¯å½“å‰ç­–ç•¥è®¿é—®çš„çŠ¶æ€åˆ†å¸ƒä¸ä¸“å®¶åŠ¨ä½œçš„èšåˆæ•°æ®é›†ã€‚ | **é¢†åŸŸéšæœºåŒ–å‚æ•°**ï¼š <br> - å¤–åŠ›æ‰°åŠ¨ï¼šå¤§å° `f âˆˆ [0, 0.5] m/s`ï¼Œå¹³å‡é—´éš” **3ç§’**ã€‚ <br> - åœ°å½¢ç¼©æ”¾ï¼šå°†åœ°å½¢åˆ†ä¸º `3Ã—3` ç½‘æ ¼ï¼Œæ¯ä¸ªç½‘æ ¼ç¼©æ”¾å› å­ `s âˆˆ [0.75, 1.25]`ã€‚ <br> - äºŒè¿›åˆ¶é«˜åº¦å›¾é˜ˆå€¼ï¼š`h_thres = 0.05 m`ã€‚ <br> - æ·»åŠ å¹²æ‰°ç‰©ï¼šéšæœºç”Ÿæˆæ–¹å½¢å’Œåœ†å½¢å¹²æ‰°ç‰©ä½“ã€‚ |\n| **3. æ¸è¿›å¼ç›®æ ‡æç¤º** | è§£å†³è®­ç»ƒæ—©æœŸç­–ç•¥æ‰§è¡Œæ…¢å¯¼è‡´çš„**çŠ¶æ€-æŒ‡ä»¤é”™ä½**é—®é¢˜ï¼Œç¡®ä¿åœ¨è®­ç»ƒä¸­æä¾›çš„æŒ‡ä»¤ä¸æœºå™¨äººå½“å‰çŠ¶æ€ä¸Šä¸‹æ–‡å¯¹é½ã€‚ | **åŸç†**ï¼šåœ¨ä»¿çœŸè®­ç»ƒä¸­ï¼Œä¸ä¸¥æ ¼æŒ‰åŸå§‹æ—¶é—´æˆ³å›æ”¾æŒ‡ä»¤ï¼Œè€Œæ˜¯æ ¹æ®æœºå™¨äººçŠ¶æ€åŠ¨æ€æ¨è¿›æŒ‡ä»¤åºåˆ—ã€‚å½“å‰å¯¼èˆªç›®æ ‡è¾¾æˆåï¼Œæ‰æä¾›ä¸‹ä¸€ç»„äº¤äº’æŒ‡ä»¤ `c` å’Œå¯¼èˆªç›®æ ‡ `g*`ã€‚è¿™æ¨¡æ‹Ÿäº†äººç±»ç­‰å¾…æœºå™¨äººå®ŒæˆåŠ¨ä½œåå†å‘å‡ºæ–°æŒ‡ä»¤çš„åˆç†äº¤äº’æ¨¡å¼ã€‚ <br> **ç®—æ³•é€»è¾‘**ï¼š <br> ``` if robot_reached_current_goal(): update_to_next_command_and_goal() else: hold_current_command() ``` | - è®­ç»ƒä¸­ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ä»¥ **50% æ¦‚ç‡** åº”ç”¨æ¸è¿›å¼æç¤ºï¼Œå¦åˆ™æŒ‰ä»¿çœŸæ—¶é’Ÿæ¨è¿›ã€‚è¿™å¹³è¡¡äº†å¯¹é½æ€§ä¸å“åº”é€Ÿåº¦ã€‚ <br> - éšç€ç­–ç•¥æ€§èƒ½æå‡ï¼Œç­‰å¾…è¡Œä¸ºè‡ªç„¶å‡å°‘ã€‚ |\n| **4. å¤šæ¨¡æ€è¾“å…¥å¢å¼º** | é˜²æ­¢æ¨¡å‹å¯¹æœ‰é™çš„æ¼”ç¤ºæ•°æ®è¿‡æ‹Ÿåˆï¼Œæå‡å¯¹**è¯­éŸ³æŒ‡ä»¤å¤šæ ·æ€§**å’Œ**æ‰‹åŠ¿å™ªå£°**çš„é²æ£’æ€§ã€‚ | **åŸç†**ï¼š <br> **1. æ‰‹åŠ¿å¢å¼º**ï¼šå¯¹æå–çš„äººä½“å…³é”®ç‚¹åæ ‡æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œæ¨¡æ‹Ÿå§¿æ€ä¼°è®¡è¯¯å·®å’Œäººä½“è¿åŠ¨è‡ªç„¶æ³¢åŠ¨ã€‚ <br> `m_augmented = m + Ïµ_m, where Ïµ_m âˆ¼ ğ’©(0, 0.01)` <br> **2. è¯­éŸ³å¢å¼º**ï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè¯­ä¹‰ç›¸åŒçš„å¤šæ ·åŒ–è¡¨è¾¾ï¼Œæ›¿ä»£åŸå§‹è¯­éŸ³æŒ‡ä»¤æ–‡æœ¬ã€‚ | - æ‰‹åŠ¿å™ªå£°æ–¹å·®ï¼š**0.01**ã€‚ <br> - æ¯æ¡è¯­éŸ³æŒ‡ä»¤ç”±LLMç”Ÿæˆ **N_v = 20** ç§æ›¿ä»£è¡¨è¾¾ï¼Œè®­ç»ƒæ—¶éšæœºé‡‡æ ·ã€‚ <br> - ä½¿ç”¨ **GPT-4** ä½œä¸ºLLMè¿›è¡Œæ–‡æœ¬æ”¹å†™ã€‚ |\n| **5. åˆ†å±‚æ§åˆ¶æ¶æ„** | è§£è€¦é«˜å±‚æ„å›¾ç†è§£ä¸åº•å±‚æ•æ·è¿åŠ¨æ‰§è¡Œï¼Œä½¿ç³»ç»Ÿæ¨¡å—åŒ–ä¸”æ˜“äºè®­ç»ƒã€‚ | **åŸç†**ï¼šå°†å®Œæ•´ç­–ç•¥ `Ï€` åˆ†è§£ä¸ºï¼š <br> **é«˜å±‚å¯¼èˆªæ¨¡å— `Ï€_g`**ï¼šä½œä¸ºâ€œç³»ç»Ÿ2â€è§„åˆ’å™¨ï¼Œæ ¹æ®å¤šæ¨¡æ€æŒ‡ä»¤ `c`ã€çŠ¶æ€ `x` å’Œç¯å¢ƒ `Ï`ï¼Œé¢„æµ‹æœºå™¨äººåæ ‡ç³»ä¸‹çš„å±€éƒ¨å¯¼èˆªç›®æ ‡ `g`ã€‚ <br> `g = Ï€_g(c, x, Ï)` <br> **åº•å±‚è¿åŠ¨æ§åˆ¶å™¨ `Ï€_u`**ï¼šä½œä¸ºâ€œç³»ç»Ÿ1â€ååº”å¼æ‰§è¡Œå™¨ï¼Œæ¥æ”¶ç›®æ ‡ `g`ï¼Œè¾“å‡ºç”µæœºæ‰­çŸ© `u`ï¼Œæ§åˆ¶æœºå™¨äººå®Œæˆè·¨è¶Šéšœç¢ç­‰æ•æ·åŠ¨ä½œã€‚ <br> `u = Ï€_u(g, x, Ï)` <br> `Ï€_u` æ˜¯é¢„å…ˆè®­ç»ƒå¥½çš„ï¼Œæœ¬æ–‡é‡ç‚¹è®­ç»ƒ `Ï€_g`ã€‚ | - `Ï€_u` æ§åˆ¶é¢‘ç‡ï¼š**50 Hz**ã€‚ <br> - `Ï€_u` é€Ÿåº¦å‘½ä»¤èŒƒå›´ï¼šçº¿é€Ÿåº¦ `v_com âˆˆ [0.0, 1.0] m/s`ï¼Œè§’é€Ÿåº¦ `w_com âˆˆ [-Ï€/3, Ï€/3] rad/s`ã€‚ <br> - çŠ¶æ€ä¼°è®¡ï¼šèåˆç¼–ç å™¨ã€è¶³ç«¯æ¥è§¦å’ŒåŠ¨æ•æ•°æ®çš„**å¡å°”æ›¼æ»¤æ³¢å™¨**ã€‚ |\n| **6. å¤šæ¨¡æ€è¡¨å¾ä¸æ„ŸçŸ¥** | å°†è¿ç»­çš„è¯­éŸ³å’Œæ‰‹åŠ¿ä¿¡å·è½¬åŒ–ä¸ºå¯ä¾›ç¥ç»ç½‘ç»œå¤„ç†çš„å›ºå®šç»´åº¦å‘é‡ã€‚ | **åŸç†**ï¼š <br> **è¯­éŸ³ `v`**ï¼šéŸ³é¢‘ç» **Silero VAD** åˆ†æ®µï¼Œ**Whisper** è½¬å½•ä¸ºæ–‡æœ¬ï¼Œå†ç”¨é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨ï¼ˆå¦‚ **Text Embeddings**ï¼‰è½¬æ¢ä¸ºå‘é‡ã€‚ <br> **æ‰‹åŠ¿ `m`**ï¼šåŸºäº6ä¸ªä¸Šèº«å…³é”®ç‚¹ï¼ˆåŒè‚©ã€åŒè‚˜ã€åŒè…•ï¼‰çš„3Dåæ ‡ï¼Œè®¡ç®—å‡ ä½•ç‰¹å¾ï¼š <br> `m = [d_sh.r,el.r, d_el.r,wr.r, d_sh.l,el.l, d_el.l,wr.l, d_sh.l,sh.r, (p_sh.l + p_sh.r)^xy / 2, Ï•_h]` <br> å…¶ä¸­ `d` æ˜¯å•ä½æ–¹å‘å‘é‡ï¼Œæœ€åä¸€é¡¹ `Ï•_h` æ˜¯äººä½“åèˆªè§’ã€‚ <br> **ç¯å¢ƒ `Ï`**ï¼šä¸ºå¯¼èˆªæ¨¡å— `Ï€_g` æä¾›äºŒå€¼åŒ–éšœç¢ç‰©æ©ç ï¼ˆé˜ˆå€¼åŒ–é«˜åº¦å›¾ï¼‰ã€‚ | - å…³é”®ç‚¹åæ ‡åœ¨**æœºå™¨äººåæ ‡ç³»**ä¸­è¡¨ç¤ºã€‚ <br> é«˜åº¦å›¾äºŒå€¼åŒ–é˜ˆå€¼ï¼š`0.05 m`ã€‚ |\n\n**æ€»ç»“**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæŠ€æœ¯è´¡çŒ®åœ¨äºæ„å»ºäº†ä¸€ä¸ª**æ•°æ®é«˜æ•ˆ**çš„é—­ç¯å­¦ä¹ æ¡†æ¶ã€‚å®ƒé€šè¿‡**åŸºäºç‰©ç†ä»¿çœŸçš„æ•°æ®èšåˆ**è§£å†³äº†å°æ ·æœ¬å­¦ä¹ çš„åˆ†å¸ƒåç§»éš¾é¢˜ï¼Œå¹¶é€šè¿‡**æ¸è¿›å¼ç›®æ ‡æç¤º**ç¡®ä¿äº†è®­ç»ƒä¿¡å·çš„ä¸€è‡´æ€§ã€‚**å¤šæ¨¡æ€å¢å¼º**æå‡äº†æ³›åŒ–èƒ½åŠ›ï¼Œè€Œ**åˆ†å±‚æ¶æ„**åˆ™åˆ†ç¦»äº†å¤æ‚çš„æ„å›¾ç†è§£ä¸è¿åŠ¨ç”Ÿæˆé—®é¢˜ã€‚è¿™äº›æ–¹æ³•å…±åŒä½¿å¾—æœºå™¨äººèƒ½å¤Ÿä»ä¸è¶³ä¸€å°æ—¶çš„çœŸäººæ¼”ç¤ºä¸­ï¼Œå­¦ä¹ ç†è§£å¹¶æ‰§è¡Œç»“åˆäº†æ‰‹åŠ¿å’Œè¯­éŸ³çš„å¤æ‚å¯¼èˆªä»»åŠ¡ï¼Œæœ€ç»ˆåœ¨çœŸå®ä¸–ç•Œæµ‹è¯•ä¸­è¾¾åˆ°äº† **97.15%** çš„æˆåŠŸç‡ã€‚",
            "timestamp": "2026-01-20T17:54:06.209450",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä½¿ç”¨äº†ä¸¤ä¸ªæ ¸å¿ƒå®šé‡æŒ‡æ ‡æ¥è¯„ä¼°å…¶æå‡ºçš„ **LURE** æ¡†æ¶çš„æ€§èƒ½ï¼Œå…·ä½“ç»†èŠ‚å¦‚ä¸‹ï¼š\n\n| æŒ‡æ ‡åç§° | è®¡ç®—æ–¹å¼ | è¯„ä»·ç›®çš„ | ä»»åŠ¡ç‰¹å¼‚æ€§æˆåŠŸæ ‡å‡† |\n| :--- | :--- | :--- | :--- |\n| **ä»»åŠ¡æˆåŠŸç‡** | æˆåŠŸå®Œæˆä»»åŠ¡çš„è¯•éªŒæ¬¡æ•° / æ€»è¯•éªŒæ¬¡æ•° | è¡¡é‡ç³»ç»Ÿåœ¨ç‰¹å®šäº¤äº’åœºæ™¯ä¸‹**æ•´ä½“ä»»åŠ¡å®Œæˆèƒ½åŠ›**çš„å®è§‚æŒ‡æ ‡ã€‚ | **Go there, Come here, Follow me:** æœºå™¨äººæœ€ç»ˆä½ç½®ä¸æ•°æ®é›†ä¸­è®°å½•çš„åŸºå‡†ä½ç½®è·ç¦» < 1 ç±³ã€‚<br>**Come around:** æœºå™¨äººå¿…é¡»ä»ç”¨æˆ·æŒ‡ç¤ºçš„æ­£ç¡®ä¾§ï¼ˆå·¦æˆ–å³ï¼‰ç»•è¿‡ç®±å­ã€‚<br>**Jump over:** æœºå™¨äººå¿…é¡»ä¾æ¬¡é€šè¿‡ç®±å­å‰æ–¹ä¸‰ä¸ªå‚è€ƒç‚¹ï¼ˆ-0.5m, 0m, +0.5mï¼‰ï¼Œæ¯ä¸ªç‚¹å®¹å·®ä¸º 0.2 ç±³ã€‚<br>**Zigzag:** æœºå™¨äººå¿…é¡»é€šè¿‡æ‰€æœ‰è½®èƒä¹‹é—´çš„ä¸­ç‚¹ï¼Œä¸”ä¸èƒ½è¸©åˆ°è½®èƒï¼Œæ¯ä¸ªä¸­ç‚¹å®¹å·®ä¸º 0.2 ç±³ã€‚ |\n| **å¯¼èˆªè¯¯å·®** | å‡æ–¹è¯¯å·®ï¼š`MSE(æœºå™¨äººå®é™…è½¨è¿¹, æ•°æ®é›†ä¸­è®°å½•çš„â€œçœŸå®â€è½¨è¿¹)` | è¡¡é‡ç­–ç•¥**éµå¾ªç”±äº¤äº’æŒ‡ä»¤æŒ‡å®šçš„é¢„æœŸè½¨è¿¹çš„ç²¾ç¡®åº¦**çš„å¾®è§‚æŒ‡æ ‡ã€‚åæ˜ äº†ç­–ç•¥å¯¹æŒ‡ä»¤çš„**ç†è§£ä¸æ‰§è¡Œçš„å¿ å®åº¦**ã€‚ | ä¸é€‚ç”¨ã€‚è¯¥æŒ‡æ ‡åœ¨æ•´ä¸ªäº¤äº’æŒç»­æ—¶é—´å†…ï¼Œé€æ—¶é—´æ­¥è®¡ç®—æœºå™¨äººåŸºåº§ä½ç½®ä¸æ•°æ®é›†ä¸­å¯¹åº”â€œçœŸå®â€è½¨è¿¹çš„å‡æ–¹è¯¯å·®ï¼Œç„¶åå–æ•´ä¸ªäº¤äº’æ—¶æ®µçš„å¹³å‡å€¼ã€‚ |\n\n**æŒ‡æ ‡ä½¿ç”¨é€»è¾‘**ï¼š\n- **æˆåŠŸç‡** æ˜¯æœ€ç»ˆéªŒæ”¶æ ‡å‡†ï¼Œå›ç­”â€œèƒ½å¦å®Œæˆä»»åŠ¡â€ã€‚\n- **å¯¼èˆªè¯¯å·®** æ˜¯è¿‡ç¨‹æ€§èƒ½æŒ‡æ ‡ï¼Œè§£é‡Šâ€œä»»åŠ¡å®Œæˆçš„è´¨é‡å¦‚ä½•â€ï¼Œå°¤å…¶åœ¨ä»»åŠ¡æˆåŠŸä½†è·¯å¾„ä¸ä¼˜æ—¶æä¾›æ›´ç»†è‡´çš„æ´å¯Ÿã€‚\n\n### äºŒã€ æ•°æ®é›†\n\nè®ºæ–‡æ„å»ºäº†ä¸€ä¸ªä¸“é—¨ç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„**å¤šæ¨¡æ€äººæœºäº¤äº’æ•°æ®é›†** `ğ’Ÿ`ã€‚\n\n| æ•°æ®é›†å±æ€§ | å…·ä½“ç»†èŠ‚ |\n| :--- | :--- |\n| **æ•°æ®å†…å®¹** | ä¸€ä¸ªå…­å…ƒç»„é›†åˆï¼š`ğ’Ÿ = {ğ¯, ğ¦, ğ†, ğ±, ğ *}`<br>- **ğ¯**: è¯­éŸ³æŒ‡ä»¤ï¼ˆå¦‚â€œgo thereâ€ï¼‰ã€‚<br>- **ğ¦**: æ‰‹åŠ¿æŒ‡ä»¤ï¼ˆåŸºäºäººä½“å…³é”®ç‚¹çš„å‘é‡è¡¨ç¤ºï¼‰ã€‚<br>- **ğ†**: ç¯å¢ƒé«˜åº¦å›¾/éšœç¢ç‰©è¡¨ç¤ºã€‚<br>- **ğ±**: æœºå™¨äººçŠ¶æ€ï¼ˆä½ç½®ã€å§¿æ€ã€å…³èŠ‚è§’ã€é€Ÿåº¦ç­‰ï¼‰ã€‚<br>- **ğ ***: ç›®æ ‡ä½ç½®ï¼ˆç”±æ•™å­¦æ†å°–ç«¯å®šä¹‰ï¼‰ã€‚ |\n| **é‡‡é›†æ–¹å¼** | **â€œå¼•è¯±â€æ³•**ï¼šä¸¤åå‚ä¸è€…åä½œã€‚<br>1. **æŒ‡ä»¤è€…**ï¼šå‘å‡ºè¯­éŸ³å’Œæ‰‹åŠ¿å‘½ä»¤ã€‚<br>2. **å¼•å¯¼è€…**ï¼šä½¿ç”¨æ•™å­¦æ†ç‰©ç†å¼•å¯¼æœºå™¨äººå®Œæˆç›®æ ‡åŠ¨ä½œã€‚<br>3. **åŒæ­¥è®°å½•**ï¼šæ‰€æœ‰æ¨¡æ€æ•°æ®åœ¨**å…¨å±€åæ ‡ç³»**ä¸‹ä»¥ **10Hz** é¢‘ç‡åŒæ­¥é‡‡é›†ã€‚ |\n| **åœºæ™¯æ„æˆ** | åŒ…å« **7** ä¸ªäº¤äº’åœºæ™¯ï¼ˆ6ä¸ªå¯¼èˆª + 1ä¸ªåœæ­¢ï¼‰ï¼š<br>1. **Stop**ï¼šåœæ­¢ã€‚<br>2. **Go there**ï¼šæŒ‡å‘æŸå¤„å¹¶å‰å¾€ã€‚<br>3. **Come here**ï¼šå¬å”¤æœºå™¨äººå‰æ¥ã€‚<br>4. **Follow me**ï¼šè®©æœºå™¨äººè·Ÿéšåœ¨æŒ‡å®šä¾§ã€‚<br>5. **Come around**ï¼šç»•å¼€éšœç¢ç‰©æ¥è¿‘äººã€‚<br>6. **Jump over**ï¼šè·³è¿‡ç®±å­ã€‚<br>7. **Zigzag**ï¼šåœ¨è½®èƒé—´ä¹‹å­—å½¢ç©¿è¡Œã€‚ |\n| **æ•°æ®è§„æ¨¡** | - **æ€»é‡‡é›†æ—¶é—´**ï¼š**å°‘äº1å°æ—¶**ï¼ˆæ‰€æœ‰åœºæ™¯ç´¯è®¡ï¼‰ã€‚<br>- **åœºæ™¯åˆ†å¸ƒä¸å‡**ï¼šä¾‹å¦‚ï¼Œâ€œJump overâ€åœºæ™¯æ•°æ®é‡æœ€å¤§ï¼ˆçº¦18åˆ†é’Ÿï¼Œ35ä¸ªç‰‡æ®µï¼‰ï¼Œâ€œCome aroundâ€æ•°æ®é‡æœ€å°ï¼ˆçº¦4åˆ†é’Ÿï¼Œ10ä¸ªç‰‡æ®µï¼‰ã€‚å…·ä½“åˆ†å¸ƒè§è®ºæ–‡å›¾5ã€‚ |\n| **å…³é”®ç‰¹å¾** | 1. **å¤šæ¨¡æ€å¯¹é½**ï¼šè¯­éŸ³ã€æ‰‹åŠ¿ã€æœºå™¨äººçŠ¶æ€ã€ç¯å¢ƒã€ç›®æ ‡åœ¨æ—¶é—´ä¸Šä¸¥æ ¼å¯¹é½ã€‚<br>2. **å…¨å±€ä¿¡æ¯**ï¼šæ‰€æœ‰æ•°æ®åœ¨å…¨å±€åæ ‡ç³»ä¸­è®°å½•ï¼Œæ”¯æŒå®Œæ•´çš„**åœºæ™¯é‡å»º**ï¼Œè¿™æ˜¯åç»­åœ¨ä»¿çœŸä¸­è¿›è¡Œæ•°æ®èšåˆï¼ˆData Aggregationï¼‰çš„åŸºç¡€ã€‚<br>3. **çœŸå®äº¤äº’**ï¼šæ•°æ®æ¥æºäºçœŸå®çš„äººæœºç‰©ç†äº¤äº’ï¼Œè€Œéè„šæœ¬ç”Ÿæˆï¼ŒåŒ…å«äº†äººç±»æŒ‡ä»¤çš„è‡ªç„¶æ—¶åºå’Œå˜åŒ–ã€‚ |\n\n### æ€»ç»“\nè¯¥è®ºæ–‡çš„è¯„ä»·ä½“ç³»**ç´§å¯†ç»“åˆå…¶ç ”ç©¶ç›®æ ‡**ï¼š\n- **æŒ‡æ ‡** åŒæ—¶å…³æ³¨ä»»åŠ¡å®Œæˆçš„**æœ‰æ•ˆæ€§**ï¼ˆæˆåŠŸç‡ï¼‰å’ŒæŒ‡ä»¤æ‰§è¡Œçš„**å‡†ç¡®æ€§**ï¼ˆå¯¼èˆªè¯¯å·®ï¼‰ã€‚\n- **æ•°æ®é›†** æ˜¯å…¶æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°ç‚¹ä¹‹ä¸€ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„â€œå¼•è¯±â€é‡‡é›†æµç¨‹ï¼Œè·å¾—äº†**å°è§„æ¨¡ä½†é«˜è´¨é‡ã€å¤šæ¨¡æ€å¯¹é½çš„çœŸå®äº¤äº’æ•°æ®**ï¼Œä¸ºåç»­åœ¨ä»¿çœŸä¸­é€šè¿‡æ•°æ®èšåˆå’Œæ¸è¿›ç›®æ ‡æç¤ºç­‰æ–¹æ³•å®ç°é«˜æ•ˆå­¦ä¹ å¥ å®šäº†åŸºç¡€ã€‚æ•°æ®é›†çš„**å…¨å±€å¯é‡å»ºæ€§**æ˜¯å…¶æŠ€æœ¯æ¡†æ¶å¾—ä»¥å®ç°çš„å…³é”®å‰æã€‚",
            "timestamp": "2026-01-20T17:54:42.852913",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†\nè®ºæ–‡**æœªä½¿ç”¨**å…¬å¼€çš„ã€é¢„å…ˆå­˜åœ¨çš„å¤§å‹æ•°æ®é›†ã€‚å…¶æ ¸å¿ƒæ•°æ®æ¥æºäº**è‡ªä¸»é‡‡é›†çš„çœŸå®ä¸–ç•Œäººæœºäº¤äº’æ¼”ç¤ºæ•°æ®**ã€‚å…·ä½“æ„æˆå¦‚ä¸‹ï¼š\n\n1.  **æ•°æ®æ”¶é›†æ–¹å¼**ï¼š\n    - é‡‡ç”¨ **â€œå¼•è¯±â€** æŠ€æœ¯ï¼Œç”±ä¸¤åå‚ä¸è€…åä½œå®Œæˆï¼šä¸€äººå‘å‡ºè¯­éŸ³å’Œæ‰‹åŠ¿æŒ‡ä»¤ï¼Œå¦ä¸€äººä½¿ç”¨æ•™å­¦æ†ç‰©ç†å¼•å¯¼æœºå™¨äººã€‚\n    - åœ¨çœŸå®ç‰©ç†ç¯å¢ƒä¸­ï¼Œé’ˆå¯¹**å…­ç§å…·ä½“çš„äº¤äº’å¯¼èˆªåœºæ™¯**å’Œä¸€ä¸ªåœæ­¢åœºæ™¯è¿›è¡Œæ•°æ®é‡‡é›†ã€‚\n\n2.  **å…­ç§äº¤äº’åœºæ™¯**ï¼ˆå¦‚å›¾4æ‰€ç¤ºï¼‰ï¼š\n    - **å¼€æ”¾ç©ºé—´äº¤äº’**ï¼š`Go there`ï¼ˆå»é‚£é‡Œï¼‰ã€`Come here`ï¼ˆè¿‡æ¥ï¼‰ã€`Follow me`ï¼ˆè·Ÿæˆ‘æ¥ï¼‰ã€‚\n    - **éšœç¢ç‰©äº¤äº’**ï¼š`Come around`ï¼ˆç»•è¿‡æ¥ï¼‰ã€`Jump over`ï¼ˆè·³è¿‡å»ï¼‰ã€‚\n    - **å¤šéšœç¢ç‰©äº¤äº’**ï¼š`Zigzag`ï¼ˆä¹‹å­—å½¢ç©¿è¿‡è½®èƒé˜µï¼‰ã€‚\n    - **åœæ­¢æŒ‡ä»¤**ï¼š`Stop`ã€‚\n\n3.  **æ•°æ®è§„æ¨¡**ï¼š\n    - **æ€»æ¼”ç¤ºæ•°æ®é‡å°äº1å°æ—¶**ï¼ˆå…·ä½“æ—¶é—´åˆ†å¸ƒè§å›¾5ï¼‰ï¼Œä½“ç°äº†å…¶**æ•°æ®é«˜æ•ˆæ€§**çš„æ ¸å¿ƒä¸»å¼ ã€‚\n    - æ”¶é›†çš„æ•°æ®åŒ…æ‹¬ï¼šæœºå™¨äººçŠ¶æ€ `x`ã€ç›®æ ‡ä½ç½® `g*`ã€æ‰‹åŠ¿å‘½ä»¤ `m`ã€è¯­éŸ³å‘½ä»¤ `v`ã€éšœç¢ç‰©é«˜åº¦å›¾ `Ï`ã€‚\n\n4.  **æ•°æ®ä½¿ç”¨ä¸å¢å¼º**ï¼š\n    - **æ ¸å¿ƒæ­¥éª¤**ï¼šå°†é‡‡é›†çš„çœŸå®äº¤äº’åœºæ™¯åœ¨ç‰©ç†ä»¿çœŸå™¨ä¸­è¿›è¡Œ**é‡å»º**ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œ**æ•°æ®èšåˆ**å’Œ**é¢†åŸŸéšæœºåŒ–**ï¼Œä»¥è§£å†³æœ‰é™æ¼”ç¤ºæ•°æ®å¸¦æ¥çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚\n    - **æ•°æ®å¢å¼º**ï¼š\n        - **è¯­éŸ³**ï¼šä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ä¸ºæ¯æ¡æŒ‡ä»¤ç”Ÿæˆå¤šç§è¯­ä¹‰ç›¸åŒçš„è¡¨è¾¾ï¼ˆ`N_v = 20`ï¼‰ã€‚\n        - **æ‰‹åŠ¿**ï¼šå¯¹å…³é”®ç‚¹æ·»åŠ é«˜æ–¯å™ªå£°ï¼ˆ`Ïµ_m ~ N(0, 0.01)`ï¼‰ã€‚\n        - **ç¯å¢ƒ**ï¼šè¿›è¡Œåœ°å½¢ç¼©æ”¾ã€æ·»åŠ å¹²æ‰°ç‰©ä½“ç­‰ã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\n\n1.  **ä»»åŠ¡æˆåŠŸç‡**ï¼š\n    - æ ¹æ®æ¯ä¸ªåœºæ™¯å®šä¹‰å…·ä½“çš„æˆåŠŸæ ‡å‡†ï¼ˆå¦‚ï¼šåˆ°è¾¾æŒ‡å®šèŒƒå›´å†…ã€é€šè¿‡ç‰¹å®šè·¯å¾„ç‚¹ã€é¿å¼€éšœç¢ç‰©ç­‰ï¼‰ã€‚\n    - **è®¡ç®—æ–¹å¼**ï¼šåœ¨ä»¿çœŸä¸­ï¼Œå¯¹æ¯ä¸ªäº¤äº’ç‰‡æ®µè¿›è¡Œå¤šæ¬¡éšæœºåŒ–æµ‹è¯•ï¼ˆ5ç§åœ°å½¢ Ã— 20ä¸ªæœºå™¨äºº = 100æ¬¡è¿è¡Œï¼‰ï¼Œè®¡ç®—æˆåŠŸå®Œæˆä»»åŠ¡çš„æ¯”ç‡ã€‚\n\n2.  **å¯¼èˆªè¯¯å·®**ï¼š\n    - å®šä¹‰ä¸ºæœºå™¨äººåŸºåº§ä½ç½®ä¸**çœŸå®æ¼”ç¤ºè½¨è¿¹**ä¹‹é—´çš„**å‡æ–¹è¯¯å·®**ã€‚\n    - **è®¡ç®—æ–¹å¼**ï¼šåœ¨ç›¸åŒçš„äº¤äº’æŒ‡ä»¤ä¸‹ï¼Œé€æ—¶é—´æ­¥æ¯”è¾ƒæœºå™¨äººå®é™…è½¨è¿¹ä¸è®°å½•çš„çœŸå®è½¨è¿¹ï¼Œå¹¶æ±‚å–æ•´ä¸ªäº¤äº’æ—¶æ®µå†…çš„å¹³å‡MSEã€‚\n    - è¯¥æŒ‡æ ‡è¡¡é‡ç­–ç•¥åœ¨ç»™å®šæŒ‡ä»¤ä¸‹**å¤ç°æ¼”ç¤ºè€…æ„å›¾è½¨è¿¹çš„ç²¾ç¡®åº¦**ã€‚\n\n### ä¸‰ã€ æ€§èƒ½æå‡ä¸ç»“æœåˆ†æ\n\n1.  **æ•´ä½“æ€§èƒ½**ï¼š\n    - æå‡ºçš„ **LURE** æ–¹æ³•åœ¨å…­ç§åœºæ™¯ä¸Šçš„**å¹³å‡ä»»åŠ¡æˆåŠŸç‡è¾¾åˆ° 97.15%**ï¼Œå¯¼èˆªè¯¯å·®æœ€ä½ã€‚\n    - è¿™è¯æ˜äº†è¯¥æ¡†æ¶èƒ½å¤Ÿä»å°‘é‡æ¼”ç¤ºä¸­é«˜æ•ˆå­¦ä¹ ï¼Œå¹¶å®ç°é²æ£’ã€æ•æ·çš„å¯¼èˆªã€‚\n\n2.  **ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”**ï¼ˆå¦‚å›¾6å’Œæ­£æ–‡æ‰€è¿°ï¼‰ï¼š\n    - **åŸºçº¿æ–¹æ³•**ï¼šè¡Œä¸ºå…‹éš†ã€ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ã€DAggerï¼ˆæ— æ¸è¿›ç›®æ ‡æç¤ºï¼‰ã€‚\n    - **æ€§èƒ½æå‡**ï¼š\n        - **æ•°æ®èšåˆçš„æœ‰æ•ˆæ€§**ï¼šä¸çº¯è¡Œä¸ºå…‹éš†ç›¸æ¯”ï¼Œå¼•å…¥æ•°æ®èšåˆçš„ **DAgger** å°†å¯¼èˆªè¯¯å·®é™ä½äº† **24.6%**ï¼ŒæˆåŠŸç‡æå‡äº† **18.6%**ã€‚\n        - **æ¸è¿›ç›®æ ‡æç¤ºçš„æœ‰æ•ˆæ€§**ï¼šåœ¨DAggeråŸºç¡€ä¸Šï¼Œå¢åŠ **æ¸è¿›ç›®æ ‡æç¤º**ç­–ç•¥çš„ **LURE** æ–¹æ³•ï¼Œè¿›ä¸€æ­¥å°†å¯¼èˆªè¯¯å·®é™ä½äº† **15.2%**ï¼ŒæˆåŠŸç‡æå‡äº† **13.7%**ã€‚\n    - **ç»“è®º**ï¼šæ•°æ®èšåˆè§£å†³äº†åˆ†å¸ƒåç§»ï¼Œè€Œæ¸è¿›ç›®æ ‡æç¤ºç¡®ä¿äº†è®­ç»ƒè¿‡ç¨‹ä¸­æŒ‡ä»¤ä¸æœºå™¨äººçŠ¶æ€çš„æ­£ç¡®å¯¹é½ï¼Œä¸¤è€…ç»“åˆå¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚\n\n3.  **å¤šæ¨¡æ€äº’è¡¥æ€§éªŒè¯**ï¼ˆå¦‚è¡¨IIæ‰€ç¤ºï¼‰ï¼š\n    - **ä»…ä½¿ç”¨æ‰‹åŠ¿**ï¼šå¹³å‡æˆåŠŸç‡ä»…ä¸º19.83%ï¼Œè¡¨æ˜æ‰‹åŠ¿æœ¬èº«å­˜åœ¨æ­§ä¹‰ï¼Œéš¾ä»¥ç‹¬ç«‹ä¼ è¾¾å®Œæ•´æ„å›¾ã€‚\n    - **ä»…ä½¿ç”¨è¯­éŸ³**ï¼šå¹³å‡å¯¼èˆªè¯¯å·®é«˜è¾¾1.68ç±³ï¼Œè¡¨æ˜å¦‚â€œå»é‚£é‡Œâ€ç­‰æŒ‡ä»¤ç¼ºä¹ç©ºé—´ groundingã€‚\n    - **ç»“åˆä¸¤è€…**ï¼šæˆåŠŸç‡è¾¾åˆ°81.83%ï¼Œè¯¯å·®é™è‡³0.43ç±³ï¼Œå……åˆ†è¯æ˜äº†**è¯­éŸ³ä¸æ‰‹åŠ¿åœ¨å¤šæ¨¡æ€äº¤äº’ä¸­çš„äº’è¡¥ä»·å€¼**ã€‚\n\n4.  **å¯¹æ–°ç”¨æˆ·çš„å¿«é€Ÿé€‚åº”**ï¼ˆå¦‚è¡¨Iæ‰€ç¤ºï¼‰ï¼š\n    - **æ— éœ€é€‚åº”**ï¼šæ¨¡å‹å¯¹ä¸‰ä¸ªæ–°ç”¨æˆ·çš„å¹³å‡åˆå§‹æˆåŠŸç‡çº¦ä¸º73%ã€‚\n    - **å¿«é€Ÿé€‚åº”å**ï¼šä»…ç”¨å¹³å‡**4.5åˆ†é’Ÿ**çš„æ–°ç”¨æˆ·æ•°æ®è¿›è¡Œå¾®è°ƒåï¼Œå¹³å‡æˆåŠŸç‡æå‡è‡³ **95%ä»¥ä¸Š**ï¼ˆå¹³å‡æå‡22.11%ï¼‰ã€‚\n    - è¿™ä½“ç°äº†æ¡†æ¶çš„**å®ç”¨æ€§å’Œä¸ªæ€§åŒ–æ½œåŠ›**ã€‚\n\n### æ€»ç»“\nè¯¥è®ºæ–‡é€šè¿‡åœ¨**è‡ªå»ºçš„ã€å°è§„æ¨¡çœŸå®äººæœºäº¤äº’æ•°æ®é›†**ä¸Šè¿›è¡Œå®éªŒï¼Œä¸»è¦ä½¿ç”¨**ä»»åŠ¡æˆåŠŸç‡**å’Œ**å¯¼èˆªè¯¯å·®**ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶æå‡ºçš„ **LURE æ¡†æ¶**ï¼ˆç»“åˆæ•°æ®èšåˆä¸æ¸è¿›ç›®æ ‡æç¤ºï¼‰èƒ½å¤Ÿä»¥**æ•°æ®é«˜æ•ˆ**çš„æ–¹å¼ï¼ˆ<1å°æ—¶æ€»æ•°æ®ï¼‰å­¦ä¹ å¤æ‚çš„å¤šæ¨¡æ€å¯¼èˆªæŠ€èƒ½ï¼Œåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–æ€§å’Œå¤šæ¨¡æ€äº’è¡¥æ€§ã€‚",
            "timestamp": "2026-01-20T17:55:21.220929",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡ç¬¬VIèŠ‚ï¼ˆLimitationsï¼‰åŠå…¨æ–‡å†…å®¹ï¼Œè¯¥ç ”ç©¶å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n- **æ‰‹åŠ¿æ³›åŒ–èƒ½åŠ›æœ‰é™**ï¼šç³»ç»Ÿç¼ºä¹**é›¶æ ·æœ¬è·¨ç”¨æˆ·æ³›åŒ–èƒ½åŠ›**ã€‚ä¸åŒç”¨æˆ·çš„æ‰‹åŠ¿é£æ ¼å·®å¼‚æ˜¾è‘—ï¼Œè€Œå½“å‰çš„æ‰‹åŠ¿è¡¨ç¤ºä»…é€šè¿‡å™ªå£°æ³¨å…¥è¿›è¡Œæ•°æ®å¢å¼ºï¼Œç¼ºä¹**è¯­ä¹‰å±‚é¢çš„å¤šæ ·æ€§**ã€‚è¿™é™åˆ¶äº†ç³»ç»Ÿå¯¹æ–°ç”¨æˆ·çš„ç›´æ¥é€‚åº”èƒ½åŠ›ã€‚\n\n- **äººç±»è¡Œä¸ºæ¨¡æ‹Ÿçš„é™æ€æ€§**ï¼š**æ¸è¿›å¼ç›®æ ‡æç¤º**ç­–ç•¥ä»…èƒ½æ ¹æ®æœºå™¨äººçŠ¶æ€**é‡æ”¾å¹¶è°ƒæ•´å·²æœ‰æ¼”ç¤ºæ•°æ®çš„æ—¶é—´æˆ³**ï¼Œè€Œæ— æ³•åœ¨é‡åˆ°å…¨æ–°çŠ¶æ€æ—¶**åŠ¨æ€ç”Ÿæˆ**ç¬¦åˆäººç±»ç›´è§‰çš„äº¤äº’æŒ‡ä»¤ã€‚è¿™é™åˆ¶äº†ç³»ç»Ÿåœ¨å¤æ‚ã€æœªè§è¿‡åœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚\n\n- **æ„ŸçŸ¥ç³»ç»Ÿä¾èµ–å—æ§ç¯å¢ƒ**ï¼š\n    - **ç¯å¢ƒæ„ŸçŸ¥**ï¼šéšœç¢ç‰©è¢«å»ºæ¨¡ä¸ºé¢„å…ˆæµ‹é‡å¥½å°ºå¯¸çš„åœ†å½¢æˆ–æ–¹å½¢å‡ ä½•åŸºå…ƒï¼Œæ— æ³•å¤„ç†ç°å®ä¸–ç•Œä¸­å¤æ‚ã€æœªçŸ¥çš„ç‰©ä½“å½¢çŠ¶ã€‚\n    - **äººä½“å§¿æ€ä¼°è®¡**ï¼šéœ€è¦ç”¨æˆ·ç©¿ç€**åŠ¨ä½œæ•æ‰æœ**ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­ä¸åˆ‡å®é™…ï¼Œé™åˆ¶äº†ç³»ç»Ÿçš„å¯éƒ¨ç½²æ€§ã€‚\n\n- **äº¤äº’æ¨¡å¼çš„å±€é™æ€§**ï¼šå½“å‰æ¡†æ¶ä¸“æ³¨äº**å¯¼èˆªä»»åŠ¡**ï¼Œå°šæœªæ‰©å±•åˆ°æ›´å¤æ‚çš„å…¨èº«è¡Œä¸ºï¼Œä¾‹å¦‚**ç§»åŠ¨æ“ä½œ**ï¼ˆlocomotion manipulationï¼‰ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nåŸºäºä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡åœ¨ç¬¬VIIèŠ‚ï¼ˆConclusion and Future Workï¼‰åŠå±€é™æ€§åˆ†æä¸­æå‡ºäº†ä»¥ä¸‹æœªæ¥å·¥ä½œæ–¹å‘ï¼š\n\n- **å¼€å‘è¯­ä¹‰åŒ–çš„æ‰‹åŠ¿è¡¨ç¤º**ï¼šå€Ÿé‰´é¢„è®­ç»ƒçš„è¿åŠ¨è¡¨ç¤ºï¼ˆå¦‚MotionDiffuseï¼‰ï¼Œç ”ç©¶èƒ½å¤Ÿ**ç»“åˆæœºå™¨äººçŠ¶æ€å’Œå‘¨å›´ç‰©ä½“è¿›è¡Œè”åˆè¯­ä¹‰ grounding çš„æ‰‹åŠ¿è¡¨ç¤º**ã€‚è¿™å°†æå‡æ‰‹åŠ¿æŒ‡ä»¤çš„è¯­ä¹‰ä¸°å¯Œæ€§å’Œè·¨ç”¨æˆ·æ³›åŒ–èƒ½åŠ›ã€‚\n\n- **å®ç°åŠ¨æ€çš„äººç±»è¡Œä¸ºç”Ÿæˆ**ï¼šå¼€å‘æ›´é«˜çº§çš„**äººç±»è¡Œä¸ºæ¨¡å‹**ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®æœºå™¨äººçš„å®æ—¶çŠ¶æ€å’Œä»»åŠ¡ä¸Šä¸‹æ–‡ï¼Œ**åŠ¨æ€ç”Ÿæˆ**åˆé€‚çš„äº¤äº’æŒ‡ä»¤ï¼ˆæ‰‹åŠ¿å’Œè¯­è¨€ï¼‰ï¼Œè€Œä¸ä»…ä»…æ˜¯é‡æ”¾æ¼”ç¤ºã€‚\n\n- **è½¬å‘åŸºäºè§†è§‰çš„æ„ŸçŸ¥ç³»ç»Ÿ**ï¼š\n    - **äººä½“å§¿æ€ä¼°è®¡**ï¼šé‡‡ç”¨**åŸºäºæ‘„åƒå¤´çš„äººä½“å§¿æ€ä¼°è®¡æŠ€æœ¯**ï¼Œæ‘†è„±å¯¹åŠ¨ä½œæ•æ‰æœçš„ä¾èµ–ã€‚\n    - **åœºæ™¯é‡å»º**ï¼šåˆ©ç”¨**è§†è§‰åœºæ™¯é‡å»ºæŠ€æœ¯**ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨éç»“æ„åŒ–ã€çœŸå®çš„æœªçŸ¥ç¯å¢ƒä¸­æ„ŸçŸ¥å’Œç†è§£éšœç¢ç‰©ã€‚\n\n- **æ‰©å±•è¡Œä¸ºèŒƒå›´**ï¼šå°†å½“å‰çš„å¤šæ¨¡æ€äº¤äº’ä¸æ•°æ®å¢å¼ºæ¡†æ¶**æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„å…¨èº«æ§åˆ¶ä»»åŠ¡**ï¼Œä¾‹å¦‚è®©æœºå™¨äººåœ¨å¯¼èˆªçš„åŒæ—¶æ‰§è¡Œç®€å•çš„æ“ä½œä»»åŠ¡ï¼ˆå¦‚æŠ“å–ã€æ¨åŠ¨ï¼‰ã€‚\n\n- **å¢å¼ºè¯­è¨€-æ‰‹åŠ¿çš„æ·±åº¦èåˆ**ï¼šè¿›ä¸€æ­¥æ¢ç´¢è¯­è¨€å’Œæ‰‹åŠ¿æ¨¡æ€åœ¨æ›´ç»†ç²’åº¦ä»»åŠ¡ä¸­çš„äº’è¡¥æœºåˆ¶ï¼Œå¯èƒ½æ¶‰åŠå¼€å‘æ›´å¼ºå¤§çš„å¤šæ¨¡æ€èåˆæ¶æ„ï¼Œä»¥å¤„ç†æ›´æ¨¡ç³Šæˆ–å¤æ‚çš„ç”¨æˆ·æŒ‡ä»¤ã€‚\n\n**æ€»ç»“**ï¼šè¯¥å·¥ä½œçš„æ ¸å¿ƒå±€é™æ€§åœ¨äºå…¶å¯¹å—æ§æ„ŸçŸ¥çš„ä¾èµ–ã€æ‰‹åŠ¿æ³›åŒ–èƒ½åŠ›çš„ä¸è¶³ä»¥åŠäººç±»è¡Œä¸ºæ¨¡æ‹Ÿçš„é™æ€æ€§ã€‚æœªæ¥çš„æ”¹è¿›å°†é›†ä¸­äº**åˆ©ç”¨å…ˆè¿›çš„è§†è§‰æ„ŸçŸ¥ã€è¯­ä¹‰åŒ–è¡¨ç¤ºå­¦ä¹ å’ŒåŠ¨æ€è¡Œä¸ºç”ŸæˆæŠ€æœ¯**ï¼Œä»¥æ„å»ºä¸€ä¸ªèƒ½åœ¨çœŸå®ä¸–ç•Œä¸­è¢«ä¸åŒç”¨æˆ·è‡ªç„¶ã€çµæ´»æ“æ§çš„æœºå™¨äººç³»ç»Ÿã€‚",
            "timestamp": "2026-01-20T17:55:43.098590",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-13T10:47:12Z",
    "created_at": "2026-01-20T17:49:42.747748",
    "updated_at": "2026-01-20T17:49:42.747756"
}