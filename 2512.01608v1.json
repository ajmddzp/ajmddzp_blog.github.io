{
    "id": "2512.01608v1",
    "title": "Integrated YOLOP Perception and Lyapunov-based Control for Autonomous Mobile Robot Navigation on Track",
    "authors": [
        "Mo Chen"
    ],
    "abstract": "æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§ç”¨äºéå®Œæ•´å·®é€Ÿé©±åŠ¨ç§»åŠ¨æœºå™¨äººçš„å®æ—¶è‡ªä¸»è½¨è¿¹å¯¼èˆªæ¡†æ¶ï¼Œé€šè¿‡å°†å¤šä»»åŠ¡è§†è§‰æ„ŸçŸ¥ä¸å¯è¯æ˜ç¨³å®šçš„è·Ÿè¸ªæ§åˆ¶å™¨ç›¸ç»“åˆå®ç°ã€‚æ„ŸçŸ¥æ¨¡å—åˆ©ç”¨2Dåˆ°3Dçš„ç›¸æœºæŠ•å½±ã€åŸºäºå¼§é•¿çš„å‡åŒ€ç‚¹äº‘é‡é‡‡æ ·ä»¥åŠé€šè¿‡é²æ£’QRæœ€å°äºŒä¹˜ä¼˜åŒ–æ±‚è§£çš„ä¸‰æ¬¡å¤šé¡¹å¼æ‹Ÿåˆï¼Œé‡å»ºè½¦é“ä¸­å¿ƒçº¿ã€‚æ§åˆ¶å™¨é‡‡ç”¨åŸºäºLyapunovç¨³å®šæ€§çš„è®¾è®¡æ–¹æ³•ï¼Œè°ƒèŠ‚æœºå™¨äººçš„çº¿é€Ÿåº¦å’Œè§’é€Ÿåº¦ï¼Œç¡®ä¿åœ¨åŠ¨æ€ä¸”éƒ¨åˆ†å¯è§‚æµ‹çš„è½¦é“ç¯å¢ƒä¸­ï¼Œä½ç½®ä¸èˆªå‘åå·®å…·æœ‰æœ‰ç•Œçš„è¯¯å·®åŠ¨æ€ç‰¹æ€§å¹¶æ¸è¿‘æ”¶æ•›ï¼Œä¸”æ— éœ€ä¾èµ–é«˜ç²¾åº¦å…ˆéªŒåœ°å›¾æˆ–å…¨çƒå«æ˜Ÿå®šä½ç³»ç»Ÿã€‚åœ¨åµŒå…¥å¼å¹³å°ä¸Šå¼€å±•çš„çœŸå®ä¸–ç•Œå®éªŒéªŒè¯äº†ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€å®æ—¶æ€§ã€è½¨è¿¹å¹³æ»‘æ€§ä»¥åŠé—­ç¯ç¨³å®šæ€§ï¼Œå®ç°äº†å¯é çš„è‡ªä¸»å¯¼èˆªã€‚",
    "url": "https://arxiv.org/abs/2512.01608v1",
    "html_url": "https://arxiv.org/html/2512.01608v1",
    "html_content": "2025 Year â€‰ Master Thesis\nIntegrated YOLOP Perception and Lyapunov-based Control for Autonomous Mobile Robot Navigation on Track\nGraduate School of Science and Technology\nMasterâ€™s Program in Science and Technology\nGreen Science and Engineering Division\nB2378474\nMO CHEN\nSupervisor â€ƒâ€†Wenjing Cao\n\\newdateformat\nmyformat\n\\THEYEAR\n-\n\\twodigit\n\\THEMONTH\n-\n\\twodigit\n\\THEDAY\n\\myformat\nDecember 1, 2025\nContents\n1\nIntroduction\n1\nResearch background\n2\nResearch object\n3\nResearch purpose\n4\nThesis Organization\n2\nBasic Theory and Methods\n1\nYOLOP\n2\nPolynomial curve fitting\n3\nLyapunov stability\n3\nModeling\n1\nRobot kinematic modeling\n2\nGeometric path kinematic modeling\n4\nRobot Autonomous Navigation Algorithms\n1\nYOLOP\n2\nPolynomial curve fitting\n3\nLyapunov-based control\n4\nConventional Lyapunov-based controllers used for comparison\n5\nImplementation process of autonomous navigation\n5\nExperiment Results and Discussion\n1\nExperimental scenario\n2\nPerception results\n3\nPreset path based tracking results\n4\nVision-based path tracking results\n6\nConclusion\nChapter 1\nIntroduction\n1\nResearch background\nIn the 1990s, the modern scientific and technological revolution marked by computer technology, microelectronics technology, information technology, network technology, etc., entered a rapid development stage, which became the intrinsic driving force to promote the development of robotics technology, and robotics technology has developed rapidly. Among them, autonomous mobile robots(AMRs) can rely on the sensors they carry to perceive and understand the external environment, make real-time decisions according to the needs of the task, carry out closed-loop control, and operate in an autonomous or semi-autonomous manner. It is a new type of robot with certain self-learning and adaptive ability in known or unknown environment. Navigation is an important problem that needs to be solved for AMRs to realize autonomous control, which refers to the process of mobile robot sensing the environment and its own state through sensors and learning, and realizing the process of pointing to the target autonomous movement in an obstructed environment. Since the first mobile robot, Shakey, was introduced in the 1960s, mobile robot navigation has been receiving a lot of attention due to its comprehensiveness and practicality\n[\nverginis2021adaptive\n]\n. As one of the core technologies of robots, navigation provides the basis for robots to accomplish various complex tasks, and makes robots widely used in transportation, cleaning, logistics and other scenarios, and active in production and life.\nThe framework of mobile robot navigation is a hierarchical structure, as demonstrated in Figure 1.1. This hierarchical framework typically encompasses four key modules: perception, decision, path planning, and robot control\n[\nteng2023motion\n]\n, and contains the main parts such as mapping\n[\ntaketomi2017visual\n]\nand localization\n[\nmalagon2015mobile\n]\n. The perception module serves as the front end of the mobile robot navigation, utilizing sensors to gather data about the surrounding environment. This data is utilized to perform essential tasks such as localization, obstacle detection, path prediction, object detection, and object tracking. This perception module integrates various data sources like high-definition (HD) maps, cameras, radars, lasers, the global positioning system (GPS), and the inertial measurement unit (IMU) to perform tasks like location pinpointing, object detection, and tracking. These tasks enable the system to identify and comprehend diverse elements and obstacles, including roads, lane lines, people, precise robot locations and predicted trajectories. The decision module receives information from the perception module, and then analyzes and reasons based on this information to make appropriate decisions. It considers factors such as rules of various specific scenarios and needs of service recipients and generates a behavioural strategy to respond to the current scenario. The planning module will devise a collision-free trajectory considering the kinematics and dynamics constraints of the robot. It takes the behavioural strategy provided by the decision module and combines it with priori data and real-time perception data to generate a planned path. This path guides the robotâ€™s driving direction, turns, and ensures the robot reaches its destination safely and efficiently. The control module is responsible for precisely and smoothly tracking the planned path through actual robot operations, including acceleration, steering, etc\n[\npendleton2017perception\n]\n.\nFigure 1:\nModules of the autonomous navigation system\nIn order to realize the navigation of AMRs, the perception module recognizes various environmental information through various sensor information: such as road boundaries, terrain features, obstacles, etc. In the navigation of unmanned vehicles, it is also necessary to recognize traffic signs, typical intersections and other information. The robot determines the reachable and unreachable areas in the direction of travel, determines the relative position in the environment, and predicts the movement of dynamic obstacles through environmental perception, thus providing a basis for local path planning. On the simultaneous localization and mapping (SLAM) side, some groups now embed ORB-SLAM3 directly in service-robot pipelines: for example, Xiao et al. enhanced the original front-end with adaptive thresholds and inertial cues to keep hospital logistics robots reliable under extreme illumination swings\n[\nxiao2025improving\n]\n. Complementing these single-SLAM efforts, the fusion trend remains strong: Yu et al. tightly coupled VINS-Fusion with vehicle-motion constraints to upgrade pose accuracy in autonomous cars by an order of magnitude\n[\nyu2022tightly\n]\n. Parallel progress in road-structure perception shows a similar pattern. Kunchala et al. employed LaneNet as the drivable-space extractor in a lightweight navigation module released just last week, demonstrating real-time performance on rural Indian highways\n[\nsiddaiyan2025enhancing\n]\n. Dong et al. blended SCNN layers into a hybrid spatialâ€“temporal encoder-decoder, showing that SCNN-based message passing lifts detection F1 by 4 % in adverse lighting\n[\ndong2023hybrid\n]\n. These advancements continue to push the limits of AMR perception, leading to more robust and reliable systems.\nThe path planning module occupies a central role within the hierarchical framework, acting as a crucial interface between the perception, decision-making, and control modules. Fundamentally, it addresses constrained optimization problems within a complex convex space. In addition, it plays a significant role in multi-agent clustering, obstacle avoidance, and target tracking control, representing a foundational and widely applicable challenge. Therefore, the path planning algorithm constitutes the core component of AMR navigation. Recent progress in local planning for robot navigation has been characterized by algorithm-specific innovations. For conventional planning and control algorithms, Melchiorre et al. introduced a novel APF method enhanced with local attractors to mitigate collision risks and avoid local minima, validated through real-world robotic experiments that demonstrated improved path predictability and obstacle avoidance efficiency\n[\nmelchiorre2023experiments\n]\n. Bui et al. developed an model predictive control (MPC) based local trajectory planner for UAVs, which processes local point cloud data to generate and optimize safe, smooth, and dynamically feasible paths. Simulation results indicated shorter, smoother trajectories and improved energy efficiency\n[\nbui2024model\n]\n. For reinforcement learning algorithms, to improve learning efficiency, stability, and obstacle avoidance in Gazebo simulations, Chen et al.\n[\nchen2024enhanced\n]\nproposed an enhanced deep Q-network (DQN) algorithm for indoor mobile robot local path planning, featuring a dueling DQN structure, prioritized experience replay, and maximum entropy integration. Recent advances in robot navigation have yielded increasingly robust and adaptive local planners through algorithm-specific enhancements in control theory and deep reinforcement learning.\n2\nResearch object\nThis study takes an AMR, AI Formula, provided by Honda as the research object. It is a sophisticated system designed with a focus on advanced hardware components to support autonomous driving\n[\npublished_papers/49092462\n]\n. It features a three-wheel configuration, with two differential drive wheels and a passive wheel as illustrated in Figure 1.2. This setup ensures stability, particularly during straight-line driving, by fixing the yaw axis and allowing controlled yaw angles during turns to prevent spinning. The vehicleâ€™s propulsion system includes in-wheel motors with differential drive and a braking system consisting of both disc brakes and regenerative braking. The passive wheel utilizes a spring and damper system for smooth operation, with caster angles and dampers to reduce vibration and enhance handling.\nFigure 2:\nAI Formula robot\nThe robot is equipped with an advanced sensor suite. The stereo camera acquires scene images through synchronized left and right perspectives and uses parallax to calculate depth information, providing the robot with high-precision three-dimensional perception, supporting tasks such as obstacle avoidance, positioning and grasping, and achieving safer, more intelligent environmental interaction and efficient operation capabilities. The inertial measurement unit (IMU) integrates accelerometers, gyroscopes and sometimes magnetometers, and can output 6 DOF motion posture and acceleration information in real time, providing a high-frequency, high-precision data foundation for robot posture solution, stable control and navigation. The wheel encoder detects the wheel rotation angular displacement or pulse number, and outputs the wheel speed and mileage in real time, providing key basic data for the robot chassis closed-loop speed control, odometer positioning and slip detection. It is small in size, fast in response, low in cost, easy to install and maintain, and has strong anti-interference ability. Global Navigation Satellite System (GNSS) is a global autonomous geographic positioning satellite system. It receives multi-constellation satellite signals in real time and outputs centimeter-level positioning, speed, and timing information, providing accurate and reliable basic reference for outdoor robots to build global coordinates, path planning and multi-sensor fusion positioning. Figure 1.3 shows the specific installation locations of these sensors.\nFigure 3:\nAI Formula robot constituent parts\n3\nResearch purpose\nThis study aims to leverage a robot developed by Honda to achieve autonomous navigation on a Formula One race track without relying on HD maps or GNSS data for positioning or path referencing. The top view of the Formula One track is shown in the figure. In this research, a stereo camera is employed to detect and extract lane line features from the environment in real time. These detected lane lines are subsequently used to infer the trackâ€™s centerline, which serves as the basis for generating a smooth and continuous navigation path. To ensure that the robot follows this planned path accurately and efficiently, a control algorithm is designed and implemented. The controller enables the robot to perform robust trajectory tracking, allowing it to navigate the track autonomously, smoothly, and at relatively high speeds. This approach emphasizes real-world applicability by minimizing reliance on external infrastructure and prior map data.\nFigure 4:\nAI Formula race course\n4\nThesis Organization\nChapter 1 introduces the background, object and purposes of the study. Chapter 2, Theoretical Background, serves as a basic introduction and introduces the basic theories and principles related to the study. Chapter 3, Problem Statement, elaborates on the kinematic modeling of the robot and geometric paths. Chapter 4, Control Algorithm discusses the development methods and strategies of the control method in detail. Chapter 5, Results and Discussion, presents the results of the field experiments and provides a comprehensive analysis of the results. Finally, Chapter 6, Conclusion, summarizes this study and proposes directions for future research.\nChapter 2\nBasic Theory and Methods\n1\nYOLOP\nThe YOLO (You Only Look Once) series represents a family of real-time object detection algorithms that have significantly advanced the field of computer vision since its inception by Redmon et al.\n[\nredmon2016you\n]\n. Unlike traditional two-stage detectors such as R-CNN, which separate the processes of region proposal and classification, YOLO treats object detection as a single regression problem, directly predicting bounding boxes and class probabilities from full images in one evaluation. This design dramatically improves inference speed while maintaining competitive accuracy, making YOLO particularly suitable for real-time applications. Over the years, the YOLO architecture has undergone several iterations, each improving upon its predecessor in terms of accuracy, speed, and efficiency. YOLOv3 introduced multi-scale predictions and residual connections, enhancing detection of small objects. YOLOv4 and YOLOv5 further improved performance through better data augmentation, backbone networks, and training strategies. More recent developments, such as YOLOv6 and YOLOv7, have focused on industrial deployment and lightweight design, offering flexible trade-offs between speed and accuracy for various use cases\n[\nbochkovskiy2020yolov4\n]\n.\nYOLOP (You Only Look Once for Panoptic Driving Perception) is a task-specific extension of the YOLO framework designed to address the complex perceptual needs of autonomous driving systems proposed by Wu et al. Unlike conventional approaches that employ separate models for each task, YOLOP achieves end-to-end learning by sharing a common backbone and utilizing task-specific heads, thereby reducing computational cost and improving inference efficiency. As shown in Fig 2.1, panoptic driving perception single-shot network, YOLOP), contains one shared encoder and three subsequent decoders to solve specific tasks. The model is built upon a lightweight variant of YOLOv5, incorporating attention modules and feature pyramid networks to facilitate effective multi-scale feature sharing across tasks. Experimental results on the BDD100K dataset demonstrate that YOLOP achieves real-time performance while maintaining high accuracy across all tasks, highlighting its practicality for real-world deployment in autonomous vehicles\n[\nwu2022yolop\n]\n. YOLOP represents a significant step toward holistic scene understanding by harmonizing object-level and pixel-level perception within a single framework.\nFigure 1:\nThe architecture of YOLOP\n[\nwu2022yolop\n]\nYOLOP integrates three key perception tasksâ€”object detection, drivable area segmentation, and lane line detectionâ€”into a unified multi-task network. Figure 3.1 shows examples of training results for three perception tasks of YOLOP. Among them, lane line detection is formulated as a binary semantic segmentation task, where each pixel in the input image is classified as either belonging to a lane line or the background. This is achieved through a dedicated segmentation head that shares a common encoder with the other tasks. The decoder structure for lane detection involves feeding the low-level feature map from the Feature Pyramid Network (FPN) into a lightweight upsampling path. This path performs three successive upsampling operations to restore the feature map to the original image resolution\n(\nW\n,\nH\n,\n2\n)\n(W,H,2)\n, producing a two-channel output that encodes the per-pixel probability distribution over the lane line and background classes. The loss function for lane detection, denoted as\nL\nll-seg\nL_{\\text{ll-seg}}\n, combines a standard cross-entropy loss\nL\nce\nL_{\\text{ce}}\nwith an Intersection-over-Union (IoU) loss\nL\nIoU\nL_{\\text{IoU}}\n, as shown in the following equation:\nL\nll-seg\n=\nL\nce\n+\nL\nIoU\n.\nL_{\\text{ll-seg}}=L_{\\text{ce}}+L_{\\text{IoU}}.\n(1)\nThe inclusion of IoU loss is particularly important for lane line detection, as lane markings are typically sparse and thin. IoU loss provides stronger supervision for such classes by explicitly penalizing mismatches between the predicted and ground truth segmentation regions. This dual-loss formulation improves the modelâ€™s ability to localize narrow structures like lane lines with higher accuracy.\nFigure 2:\nExamples of YOLOP\n[\nwu2022yolop\n]\nIn this study, YOLOP is chosen as the perception module due to its efficiency, real-time performance, and integrated multi-task capability. Unlike conventional object detection algorithms that focus solely on detecting discrete objects, YOLOP simultaneously performs object detection, drivable area segmentation, and lane line detection within a unified architecture. It not only provides good lane line detection results for the AMR navigation of this study, but also provides effective assistance for subsequent roadblock detection and detection of other robots on the track. This multi-task design significantly reduces computational overhead and system complexity, making it highly suitable for embedded or resource-constrained platforms. Compared to visual SLAM algorithms, which typically require substantial computational resources and are sensitive to environmental variability, YOLOP demonstrates robust performance in outdoor scenes while maintaining real-time inference speeds. Due to hardware limitations, online visual SLAM is not feasible for this application; hence, YOLOP provides a practical and lightweight alternative that ensures adequate environmental perception for autonomous navigation.\nFurthermore, although conventional image processing methods such as OpenCV-based lane detection may perform well on straight and clearly marked roads, their effectiveness significantly deteriorates in more complex scenarios. In particular, under conditions such as road curvature, dashed lane markings, and strong outdoor lighting variations, OpenCV methods are prone to instability and false detections. In contrast, YOLOPâ€™s deep learning-based approach exhibits greater robustness and generalization in these challenging environments, making it a more reliable solution for lane detection in real-world autonomous driving scenarios.\n2\nPolynomial curve fitting\nPolynomial interpolation is a fundamental method in path planning, widely used in robotics, autonomous vehicles, and computer-aided manufacturing. This approach involves constructing a polynomial function that passes through a given set of waypoints, ensuring smooth and continuous motion between them. Figure 2.3 illustrates the use of polynomial interpolation in curve fitting. This figure shows low-order polynomial interpolation. It can be seen from the figure that the fitting curve is smooth. Corresponding to autonomous navigation, the generated path points will be smooth. The primary advantage of polynomial interpolation lies in its ability to provide closed-form solutions with continuous derivatives, which are crucial for generating feasible and dynamically consistent paths. Depending on the application requirements, various orders of polynomials may be employedâ€”such as cubic or quinticâ€”to satisfy constraints on position, velocity, and acceleration at specific time instances.\nFigure 3:\nPolynomial interpolation in curve fitting\nIn practice, low-order polynomials like cubic polynomials are often used for point-to-point motion with velocity constraints, while higher-order polynomials, such as quintic or even septic, are preferred when smoothness up to acceleration or jerk is required. For example, quintic polynomial interpolation ensures continuous acceleration and is commonly used in manipulator path planning where minimizing abrupt dynamic changes is important\n[\nsciavicco2012modelling\n]\n. The coefficients of these polynomials are typically determined by solving a system of linear equations derived from the boundary conditions, including position, velocity, and sometimes acceleration at the start and end points.\nWhen performing polynomial interpolation on the path, the robotâ€™s posture, velocity, acceleration and other differential terms at the initial and final positions need to be given as constraints. If it is required to pass through two path points\nÎ¸\nâ€‹\n(\n0\n)\n\\theta(0)\nand\nÎ¸\nâ€‹\n(\nt\n0\n)\n\\theta(t_{0})\nwithin time\nt\n0\nt_{0}\n, and then consider the velocity constraints\nÎ¸\nË™\nâ€‹\n(\n0\n)\n\\dot{\\theta}(0)\nand\nÎ¸\nË™\nâ€‹\n(\nt\n0\n)\n\\dot{\\theta}(t_{0})\nof the initial and final points, a total of four degrees of freedom, the cubic polynomial can be completely determined\nÎ¸\nâ€‹\n(\nt\n)\n=\na\n0\n+\na\n1\nâ€‹\nt\n+\na\n2\nâ€‹\nt\n2\n+\na\n3\nâ€‹\nt\n3\n\\theta(t)=a_{0}+a_{1}t+a_{2}t^{2}+a_{3}t^{3}\n(2)\nPolynomial coefficients are determined by the position and velocity constraints.\n[\nÎ¸\nâ€‹\n(\n0\n)\nÎ¸\nâ€‹\n(\nt\n0\n)\nÎ¸\nË™\nâ€‹\n(\n0\n)\nÎ¸\nË™\nâ€‹\n(\nt\n0\n)\n]\n=\n[\n1\n0\n0\n0\n1\nt\n0\nt\n0\n2\nt\n0\n3\n0\n1\n0\n0\n0\n1\n2\nâ€‹\nt\n0\n3\nâ€‹\nt\n0\n2\n]\nâ€‹\n[\na\n0\na\n1\na\n2\na\n3\n]\n\\begin{bmatrix}\\theta(0)\\\\\n\\theta(t_{0})\\\\\n\\dot{\\theta}(0)\\\\\n\\dot{\\theta}(t_{0})\\end{bmatrix}=\\begin{bmatrix}1&0&0&0\\\\\n1&t_{0}&t_{0}^{2}&t_{0}^{3}\\\\\n0&1&0&0\\\\\n0&1&2t_{0}&3t_{0}^{2}\\end{bmatrix}\\begin{bmatrix}a_{0}\\\\\na_{1}\\\\\na_{2}\\\\\na_{3}\\end{bmatrix}\n(3)\nFinally, we can obtain the coefficients of the cubic polynomial as\n[\na\n0\na\n1\na\n2\na\n3\n]\n=\n[\nÎ¸\nâ€‹\n(\n0\n)\nÎ¸\nË™\nâ€‹\n(\n0\n)\n3\nt\n0\n2\nâ€‹\n[\nÎ¸\nâ€‹\n(\nt\n0\n)\nâˆ’\nÎ¸\nâ€‹\n(\n0\n)\n]\nâˆ’\n2\nt\n0\nâ€‹\nÎ¸\nË™\nâ€‹\n(\n0\n)\nâˆ’\n1\nt\n0\nâ€‹\nÎ¸\nË™\nâ€‹\n(\nt\n0\n)\nâˆ’\n2\nt\n0\n3\nâ€‹\n[\nÎ¸\nâ€‹\n(\nt\n0\n)\nâˆ’\nÎ¸\nâ€‹\n(\n0\n)\n]\n+\n1\nt\n0\n2\nâ€‹\n[\nÎ¸\nË™\nâ€‹\n(\n0\n)\nâˆ’\nÎ¸\nË™\nâ€‹\n(\nt\n0\n)\n]\n]\n\\begin{bmatrix}a_{0}\\\\\na_{1}\\\\\na_{2}\\\\\na_{3}\\end{bmatrix}=\\begin{bmatrix}\\theta(0)\\\\\n\\dot{\\theta}(0)\\\\\n\\frac{3}{t_{0}^{2}}\\left[\\theta(t_{0})-\\theta(0)\\right]-\\frac{2}{t_{0}}\\dot{\\theta}(0)-\\frac{1}{t_{0}}\\dot{\\theta}(t_{0})\\\\\n-\\frac{2}{t_{0}^{3}}\\left[\\theta(t_{0})-\\theta(0)\\right]+\\frac{1}{t_{0}^{2}}\\left[\\dot{\\theta}(0)-\\dot{\\theta}(t_{0})\\right]\\end{bmatrix}\n(4)\nIf more time differential constraints are considered, a polynomial curve of higher order is required, but the increase of the polynomial order will affect the calculation efficiency and thus reduce the real-time performance. Moreover, when using equidistant polynomial interpolation, as the order of the polynomial increases, the function analytical interval may be too small, resulting in the Runge phenomenon as shown in Figure 2.4: violent oscillations occur at both ends of the interpolation. High-order polynomial interpolation, as demonstrated by the Runge phenomenon, leads to oscillations at the endpoints of the generated path. This results in uneven path points and reduced effectiveness in autonomous navigation.\nFigure 4:\nRunge phenomenon\nIn this study, cubic polynomial interpolation is adopted for path planning due to its balance between computational efficiency and trajectory smoothness. Compared to linear interpolation, which only ensures position continuity and often leads to abrupt changes in velocity and acceleration, cubic interpolation guarantees continuity of both position and velocity, resulting in smoother and more dynamically feasible paths for robots. On the other hand, higher-order interpolation methods, while capable of satisfying more constraints, are more susceptible to numerical instability and Rungeâ€™s phenomenon especially when fitting across multiple waypoints. Therefore, cubic interpolation offers a practical compromise, providing sufficient smoothness for path tracking while avoiding the complexity and instability associated with high-order polynomials.\n3\nLyapunov stability\nLyapunov stability is a cornerstone concept in the analysis of dynamical systems, particularly in control theory and differential equations. Introduced by the Russian mathematician Aleksandr M. Lyapunov in the late 19th century, it provides a rigorous framework to assess whether the trajectories of a system remain bounded and close to an equilibrium point in response to small perturbations.\nFormally, an equilibrium point\nx\ne\nx_{e}\nof a dynamical system\nx\nË™\n=\nf\nâ€‹\n(\nx\n)\n\\dot{x}=f(x)\nis said to be Lyapunov stable if, for every\nÏµ\n>\n0\n\\epsilon>0\n, there exists a\nÎ´\n>\n0\n\\delta>0\nsuch that\nâ€–\nx\nâ€‹\n(\n0\n)\nâˆ’\nx\ne\nâ€–\n<\nÎ´\n\\|x(0)-x_{e}\\|<\\delta\nimplies\nâ€–\nx\nâ€‹\n(\nt\n)\nâˆ’\nx\ne\nâ€–\n<\nÏµ\n\\|x(t)-x_{e}\\|<\\epsilon\nfor all\nt\nâ‰¥\n0\nt\\geq 0\n. If, in addition,\nâ€–\nx\nâ€‹\n(\nt\n)\nâˆ’\nx\ne\nâ€–\nâ†’\n0\n\\|x(t)-x_{e}\\|\\to 0\nas\nt\nâ†’\nâˆ\nt\\to\\infty\n, the equilibrium is said to be\nasymptotically stable\n. These definitions do not require explicit solutions of the system, making Lyapunovâ€™s approach broadly applicable to nonlinear and time-invariant systems\n[\nkhalil2002nonlinear\n]\n.\nThe first method proposed by Lyapunov, often referred to as the indirect method, assesses the stability of a nonlinear system by examining its linear approximation near an equilibrium point. Given a nonlinear autonomous system\nx\nË™\n=\nf\nâ€‹\n(\nx\n)\n,\nf\nâ€‹\n(\n0\n)\n=\n0\n,\n\\dot{x}=f(x),\\quad f(0)=0,\n(5)\nthe Jacobian matrix\nA\n=\nâˆ‚\nf\nâˆ‚\nx\n|\nx\n=\n0\nA=\\left.\\frac{\\partial f}{\\partial x}\\right|_{x=0}\nis computed at the equilibrium. The local behavior of the system is then approximated by the linear system\nx\nË™\n=\nA\nâ€‹\nx\n.\n\\dot{x}=Ax.\n(6)\nLyapunovâ€™s first method states that if all eigenvalues of\nA\nA\nhave negative real parts, that is,\nRe\nâ€‹\n(\nÎ»\ni\n)\n<\n0\n\\text{Re}(\\lambda_{i})<0\nfor all\ni\ni\n, then the equilibrium at the origin is asymptotically stable. If any eigenvalue has a positive real part, the equilibrium is unstable. However, if any eigenvalue lies on the imaginary axis, the method is inconclusive. This method is particularly useful due to its simplicity and the availability of linear algebraic tools, although it only provides local stability information and is limited to systems with differentiable right-hand sides\n[\nslotine1991applied\n]\n.\nThe second, or direct method, is more powerful and broadly applicable, especially for nonlinear systems. It avoids linearization by constructing a scalar Lyapunov function\nV\nâ€‹\n(\nx\n)\n:\nâ„\nn\nâ†’\nâ„\nV(x):\\mathbb{R}^{n}\\to\\mathbb{R}\n, analogous to an energy function, satisfying the following properties:\nâ€¢\nV\nâ€‹\n(\n0\n)\n=\n0\nV(0)=0\n, and\nV\nâ€‹\n(\nx\n)\n>\n0\nV(x)>0\nfor all\nx\nâ‰ \n0\nx\\neq 0\n(positive definite);\nâ€¢\nThe time derivative\nV\nË™\nâ€‹\n(\nx\n)\n=\nâˆ‚\nV\nâˆ‚\nx\nâ€‹\nf\nâ€‹\n(\nx\n)\n\\dot{V}(x)=\\frac{\\partial V}{\\partial x}f(x)\nis non-positive, i.e.,\nV\nË™\nâ€‹\n(\nx\n)\nâ‰¤\n0\n\\dot{V}(x)\\leq 0\n(negative semi-definite).\nIf such a function exists, then the equilibrium at the origin is Lyapunov stable. If\nV\nË™\nâ€‹\n(\nx\n)\n<\n0\n\\dot{V}(x)<0\n(negative definite), the equilibrium is asymptotically stable. In practice, candidate Lyapunov functions are often constructed using quadratic forms, such as\nV\nâ€‹\n(\nx\n)\n=\nx\nâŠ¤\nâ€‹\nP\nâ€‹\nx\n,\nV(x)=x^{\\top}Px,\n(7)\nwhere\nP\n=\nP\nâŠ¤\n>\n0\nP=P^{\\top}>0\nis a positive definite matrix. The choice of\nP\nP\ncan be determined using Lyapunovâ€™s matrix equation for linear systems or through heuristic design in nonlinear contexts. The second method is particularly powerful for analyzing global stability and is extensively employed in modern nonlinear control system design, including adaptive and robust control frameworks\n[\nvidyasagar2002nonlinear\n]\n.\nLyapunov stability theory offers a fundamental framework for assessing the behavior of dynamical systems near equilibrium. While the first method provides local stability analysis through system linearization, it is limited in handling nonlinear dynamics. In contrast, Lyapunovâ€™s second method allows direct stability analysis without linearization and is better suited for nonlinear systems. Therefore, this study adopts the second method to design the path-tracking controller, as it enables the construction of control laws that ensure asymptotic stability and robustness against model uncertainties and external disturbances, which are critical for reliable autonomous navigation. Compared to traditional methods such as PID control or pure pursuit algorithms which often rely on heuristic tuning and may struggle with stability and performance under complex dynamic conditions. The Lyapunov-based approach offers a more systematic and theoretically grounded framework for ensuring stability and convergence, particularly in nonlinear and time-varying environments.\nChapter 3\nModeling\n1\nRobot kinematic modeling\nThe nomenclature of the main variables in this section is shown in Table 3.1.\nTable 1:\nDescription of variables and their units\nVariable\nDescription\nUnit\nx\nx\nRobot geometric center horizontal coordinate\nm\ny\ny\nRobot geometric center Longitudinal coordinate\nm\nÏ•\n\\phi\nRobot heading angle\nrad\nr\nr\nWheel radius\nm\nr\nm\nr_{m}\nRobot movement radius\nm\nd\nd\nWheelbase\nm\nv\nv\nRobot linear velocity\nm/s\nÏ‰\n\\omega\nRobotâ€™s angular velocity\nrad/s\nÏ‰\nL\n\\omega_{L}\nLeft wheel angular velocity\nrad/s\nÏ‰\nR\n\\omega_{R}\nRight wheel angular velocity\nrad/s\nv\nL\nv_{L}\nLeft wheel linear velocity\nm/s\nv\nR\nv_{R}\nRight wheel linear velocity\nm/s\nt\nt\nTime index\ns\nThis section introduces the kinematic model of the AI Formula. As a three-wheel differential model, it is important to note that the front wheels are responsible for both propulsion and steering, while the rear wheels remain passive. Therefore, the robot model is simplified to a two-wheel differential model to facilitate the subsequent controller design. The robot under consideration follows a structural configuration to a two-wheel differential drive, as illustrated in Figure 3.1. In this model, point\nO\nO\nrepresents the geometric center of the simplified model. The position of the geometric center is given by the coordinates\nO\nâ€‹\n(\nx\n,\ny\n)\nO(x,y)\n, where\nv\nv\nand\nÏ‰\n\\omega\nrepresent the robotâ€™s linear and angular velocities, respectively, and\nÏ•\n\\phi\ndenotes the heading angle.\nFigure 1:\nTwo-wheel differential model\nIn this model, the robotâ€™s geometric center and mass center are assumed to coincide, and the coincidence point is at the midpoint of the wheel axle. Let\np\n=\n[\nx\ny\nÏ•\n]\nT\np=\\begin{bmatrix}x&y&\\phi\\end{bmatrix}^{T}\nrepresent the robotâ€™s posture in the global coordinate system\nx\nâ€‹\nO\nâ€‹\ny\nxOy\n, and\nM\n=\n(\nx\n,\ny\n)\nM=(x,y)\nrepresents the coordinates of the robotâ€™s mass center in the global coordinate system. Let the robotâ€™s wheel diameter be\n2\nâ€‹\nr\n2r\n, and the distance between the two driving wheels be\n2\nâ€‹\nd\n2d\n.\nLet the local coordinates of the robot be\nx\nâ€²\nâ€‹\nO\nâ€‹\ny\nâ€²\nx^{\\prime}Oy^{\\prime}\n, then define the robotâ€™s linear velocity\nv\nv\nas its velocity along the\nx\nâ€²\nx^{\\prime}\n-axis, the heading angle\nÏ•\n\\phi\nas the angle between the\nx\nâ€²\nx^{\\prime}\n-axis and the\nx\nx\n-axis of the global coordinate system, the angular velocity\nÏ‰\n\\omega\nas the instantaneous angular rate of rotation of the\nx\nâ€²\nx^{\\prime}\n-axis around point\nO\nO\n, and the entire system uses\nu\n=\n[\nv\nÏ‰\n]\nT\nu=\\begin{bmatrix}v&\\omega\\end{bmatrix}^{T}\nas the input control variable. Let\nÏ‰\nL\n\\omega_{L}\n,\nÏ‰\nR\n\\omega_{R}\nand\nv\nL\nv_{L}\n,\nv\nR\nv_{R}\nbe the angular velocity, linear velocity of the left and right driving wheels of the robot, respectively, and also the drive variable.\nBelow, weâ€™ll analyze the relationship between the control variable and the drive variable. First, we have\nv\n=\nv\nL\n+\nv\nR\n2\nv=\\frac{v_{L}+v_{R}}{2}\n(1)\nAs shown in the Figure 3.2, within time\nÎ”\nâ€‹\nt\nâ†’\n0\n\\Delta t\\rightarrow 0\n, the robot moves from posture\np\n1\np_{1}\nto posture\np\n2\np_{2}\n, and the heading angle\nÏ•\n\\phi\nchanges by\nÎ”\nâ€‹\nÏ•\n\\Delta\\phi\n. The right wheel moves\nÎ”\nâ€‹\nl\n=\n(\nv\nR\nâˆ’\nv\nL\n)\nâ€‹\nÎ”\nâ€‹\nt\n\\Delta l=(v_{R}-v_{L})\\Delta t\nmore than the left wheel. Then\nÎ”\nâ€‹\nÏ•\nâ‰ˆ\nsin\nâ¡\nÎ”\nâ€‹\nÏ•\n=\nÎ”\nâ€‹\nl\n2\nâ€‹\nd\n\\Delta\\phi\\approx\\sin\\Delta\\phi=\\frac{\\Delta l}{2d}\n, thus obtaining\nÏ‰\n=\nÎ”\nâ€‹\nÏ•\nÎ”\nâ€‹\nt\n=\nv\nR\nâˆ’\nv\nL\n2\nâ€‹\nd\n\\omega=\\frac{\\Delta\\phi}{\\Delta t}=\\frac{v_{R}-v_{L}}{2d}\n(2)\nThis allows us to determine the robotâ€™s motion radius\nr\nm\n=\nv\nÏ‰\n=\nv\nR\n+\nv\nL\nv\nR\nâˆ’\nv\nL\nâ€‹\nd\nr_{m}=\\frac{v}{\\omega}=\\frac{v_{R}+v_{L}}{v_{R}-v_{L}}d\n(3)\nFigure 2:\nKinematic model of two-wheel differential robot\nIn the end, the relationship between the robotâ€™s control and drive variables can be represented by a matrix as\n[\nv\nÏ‰\n]\n=\n[\n1\n2\n1\n2\n1\n2\nâ€‹\nd\nâˆ’\n1\n2\nâ€‹\nd\n]\nâ€‹\n[\nv\nR\nv\nL\n]\n\\begin{bmatrix}v\\\\\n\\omega\\end{bmatrix}=\\begin{bmatrix}\\frac{1}{2}&\\frac{1}{2}\\\\\n\\frac{1}{2d}&-\\frac{1}{2d}\\end{bmatrix}\\begin{bmatrix}v_{R}\\\\\nv_{L}\\end{bmatrix}\n(4)\nThe kinematic model of the two-wheel differential drive relates the control inputs to the time derivatives of the vehicleâ€™s position and orientation. The center of rotation for the angular velocity is located at the geometric center. The following relationship is derived from the kinematic equations of the two-wheel differential model:\nğ©\nË™\n=\n[\nx\nË™\ny\nË™\nÏ•\nË™\n]\n=\n[\ncos\nâ¡\nÏ•\n0\nsin\nâ¡\nÏ•\n0\n0\n1\n]\nâ€‹\n[\nv\nÏ‰\n]\n\\dot{\\mathbf{p}}=\\begin{bmatrix}\\dot{x}\\\\\n\\dot{y}\\\\\n\\dot{\\phi}\\end{bmatrix}=\\begin{bmatrix}\\cos\\phi&0\\\\\n\\sin\\phi&0\\\\\n0&1\\end{bmatrix}\\begin{bmatrix}v\\\\\n\\omega\\end{bmatrix}\n(5)\nIt can also be further written as Eq. (3.6) based on the relationship between the control variable and the drive variable.\nğ©\nË™\n=\n[\nx\nË™\ny\nË™\nÎ¸\nË™\n]\n=\n[\ncos\nâ¡\nÏ•\n0\nsin\nâ¡\nÏ•\n0\n0\n1\n]\nâ€‹\n[\n1\n2\n1\n2\n1\n2\nâ€‹\nd\nâˆ’\n1\n2\nâ€‹\nd\n]\nâ€‹\n[\nv\nR\nv\nL\n]\n\\dot{\\mathbf{p}}=\\begin{bmatrix}\\dot{x}\\\\\n\\dot{y}\\\\\n\\dot{\\theta}\\end{bmatrix}=\\begin{bmatrix}\\cos\\phi&0\\\\\n\\sin\\phi&0\\\\\n0&1\\end{bmatrix}\\begin{bmatrix}\\frac{1}{2}&\\frac{1}{2}\\\\\n\\frac{1}{2d}&-\\frac{1}{2d}\\end{bmatrix}\\begin{bmatrix}v_{R}\\\\\nv_{L}\\end{bmatrix}\n(6)\nThe above equation shows that the autonomous mobile robot is a nonholonomic system with a 3-dimensional state vector and 2 control inputs. Therefore, the autonomous mobile robot has motion constraints that cannot move omnidirectionally, and path smoothness and continuity need to be coordinated in path planning. This is also the reason why we introduce the path tracking algorithm based on the Lyapunov controller.\n2\nGeometric path kinematic modeling\nThe nomenclature of the main variables in this section is shown in Table 3.2.\nTable 2:\nDescription of variables and their units\nVariable\nDescription\nUnit\nx\nt\nx_{t}\nTarget point horizontal coordinate\nm\ny\nt\ny_{t}\nTarget point Longitudinal coordinate\nm\nÏ•\nt\n\\phi_{t}\nTarget point heading angle\nrad\nv\nt\nv_{t}\nRobot target velocity\nm/s\nÎ±\n\\alpha\nDefined angular error\nrad\nÎ²\n\\beta\nDefined angular error\nrad\nThe geometric path to be studied is illustrated in the accompanying Figure 3.3. To facilitate the system description, the following notations are adopted. Global coordinate frame is denoted as\nx\nâ€‹\nO\nâ€‹\ny\nxOy\n, which defines the workspace. The robotâ€™s geometric center is represented by the point\nO\nâ€‹\n(\nx\n,\ny\n)\nO(x,y)\n, whose position evolves over time. The robot moves with a linear velocity\nv\nv\nand an angular velocity\nÏ‰\n\\omega\n, and its motion direction is given by the heading angle\nÏ•\n\\phi\n. The target point to be tracked is denoted as\nO\nt\nâ€‹\n(\nx\nt\n,\ny\nt\n)\nO_{t}(x_{t},y_{t})\n, with a corresponding target velocity\nv\nt\nv_{t}\nand target heading direction\nÏ•\nt\n\\phi_{t}\n. The scalar\nÏ\n\\rho\ndenotes the Euclidean distance between the robot and the target point, while\nÎ¸\n\\theta\nrepresents the angle formed by the vector from the robotâ€™s center\nO\nO\nto the target position\nO\nt\nO_{t}\nrelative to the global frame. These notations form the basis for describing the relative motion and formulating the control laws.\nFigure 3:\nMobile robot tracking a moving target point\nThe robotâ€™s kinematics is described by the following set of differential equations, which characterize the motion of a non-holonomic mobile robot based on its velocity inputs:\nx\nË™\n\\displaystyle\\dot{x}\n=\nv\nâ€‹\ncos\nâ¡\nÏ•\n\\displaystyle=v\\cos\\phi\n(7)\ny\nË™\n\\displaystyle\\dot{y}\n=\nv\nâ€‹\nsin\nâ¡\nÏ•\n\\displaystyle=v\\sin\\phi\n(8)\nÏ•\nË™\n\\displaystyle\\dot{\\phi}\n=\nÏ‰\n\\displaystyle=\\omega\n(9)\nThe relative positions and orientations of the robot and the moving target point can be expressed through the following set of geometric relationships:\nÏ\nâ€‹\ncos\nâ¡\nÎ¸\n\\displaystyle\\rho\\cos\\theta\n=\nx\nt\nâˆ’\nx\n\\displaystyle=x_{t}-x\n(10)\nÏ\nâ€‹\nsin\nâ¡\nÎ¸\n\\displaystyle\\rho\\sin\\theta\n=\ny\nt\nâˆ’\ny\n\\displaystyle=y_{t}-y\n(11)\nÎ±\n\\displaystyle\\alpha\n=\nÎ¸\nâˆ’\nÏ•\n\\displaystyle=\\theta-\\phi\n(12)\nÎ²\n\\displaystyle\\beta\n=\nÎ¸\nâˆ’\nÏ•\nt\n\\displaystyle=\\theta-\\phi_{t}\n(13)\nTheir derivatives with respect to time\nt\nt\nare\nÏ\nË™\n\\displaystyle\\dot{\\rho}\n=\nv\nt\nâ€‹\ncos\nâ¡\nÎ²\nâˆ’\nv\nâ€‹\ncos\nâ¡\nÎ±\n\\displaystyle=v_{t}\\cos\\beta-v\\cos\\alpha\n(14)\nÎ±\nË™\n\\displaystyle\\dot{\\alpha}\n=\nv\nâ€‹\nsin\nâ¡\nÎ±\nÏ\nâˆ’\nv\nt\nâ€‹\nsin\nâ¡\nÎ²\nÏ\nâˆ’\nÏ‰\n,\nÏ\nâ‰ \n0\n\\displaystyle=v\\frac{\\sin\\alpha}{\\rho}-v_{t}\\frac{\\sin\\beta}{\\rho}-\\omega,\\quad\\rho\\neq 0\n(15)\nÎ²\nË™\n\\displaystyle\\dot{\\beta}\n=\nv\nâ€‹\nsin\nâ¡\nÎ±\nÏ\nâˆ’\nv\nt\nâ€‹\nsin\nâ¡\nÎ²\nÏ\nâˆ’\nÏ•\nË™\nt\n,\nÏ\nâ‰ \n0\n\\displaystyle=v\\frac{\\sin\\alpha}{\\rho}-v_{t}\\frac{\\sin\\beta}{\\rho}-\\dot{\\phi}_{t},\\quad\\rho\\neq 0\n(16)\nÏ•\nË™\nt\n\\dot{\\phi}_{t}\nis calculated from the three look ahead points shown in the figure, and its essence is the target angular velocity\nÏ‰\nt\n\\omega_{t}\n. By introducing the target angular velocity, the robot can drive to the next target point more smoothly when it reaches the target point.\nÏ•\nË™\nt\n\\dot{\\phi}_{t}\nis computed based on three look-ahead points, as illustrated in the Figure. 3.4 . Essentially,\nÏ•\nË™\nt\n\\dot{\\phi}_{t}\nrepresents the target angular velocity, denoted as\nÏ‰\nt\n\\omega_{t}\n, which serves as a predictive control reference for the robotâ€™s rotational motion. By incorporating this target angular velocity into the control framework, the robot is able to adjust its heading more smoothly as it approaches each designated target point along the path. This predictive adjustment enhances the continuity and fluidity of motion, thereby improving trajectory tracking performance and overall navigation stability.\nÏ•\nA\nâ€‹\nB\n\\displaystyle\\phi_{AB}\n=\na\nâ€‹\nr\nâ€‹\nc\nâ€‹\nt\nâ€‹\na\nâ€‹\nn\nâ€‹\n(\ny\nB\nâˆ’\ny\nA\nx\nB\nâˆ’\nx\nA\n)\n\\displaystyle=arctan(\\frac{y_{B}-y_{A}}{x_{B}-x_{A}})\n(17)\nÏ•\nB\nâ€‹\nC\n\\displaystyle\\phi_{BC}\n=\na\nâ€‹\nr\nâ€‹\nc\nâ€‹\nt\nâ€‹\na\nâ€‹\nn\nâ€‹\n(\ny\nC\nâˆ’\ny\nB\nx\nC\nâˆ’\nx\nB\n)\n\\displaystyle=arctan(\\frac{y_{C}-y_{B}}{x_{C}-x_{B}})\n(18)\nÏ•\nË™\nt\n\\displaystyle\\dot{\\phi}_{t}\n=\nÏ•\nB\nâ€‹\nC\nâˆ’\nÏ•\nA\nâ€‹\nB\nÎ”\nâ€‹\nt\n\\displaystyle=\\frac{\\phi_{BC}-\\phi_{AB}}{\\Delta t}\n(19)\nFigure 4:\nThree look ahead points\nEq. (3.14-3.16) represent the kinematic model of the geometric path in terms of a new set of state variables expressed in polar coordinates:\nÏ\n\\rho\n,\nÎ±\n\\alpha\n, and\nÎ²\n\\beta\n. Here,\nÏ\n\\rho\ndenotes the linear tracking error, measuring the distance between the robot and the moving target. The variables\nÎ±\n\\alpha\nand\nÎ²\n\\beta\nrepresent angular tracking errors, with\nÎ±\n\\alpha\nindicating the misalignment between the robotâ€™s heading and the line-of-sight to the target, and\nÎ²\n\\beta\ncapturing the angular deviation between the targetâ€™s heading and the same reference direction. These polar-coordinate variables are essential for formulating error dynamics and designing effective tracking controllers.\nChapter 4\nRobot Autonomous Navigation Algorithms\n1\nYOLOP\nThe nomenclature of the main variables in this section is shown in Table 4.1.\nTable 1:\nDescription of variables\nVariable\nDescription\nx\n,\ny\nx,y\nOriginal pixel coordinates in the input image\nx\nâ€²\n,\ny\nâ€²\nx^{\\prime},y^{\\prime}\nRescaled coordinates after letterbox padding\nr\nr\nImage resizing ratio\np\nx\n,\np\ny\np_{x},p_{y}\nPadding values in horizontal and vertical directions\nx\nnorm\nx_{\\text{norm}}\nNormalized pixel value\nÎ¼\n\\mu\nMean for normalization (per channel)\nÏƒ\n\\sigma\nStandard deviation for normalization (per channel)\nI\nI\nInput image tensor\nF\nF\nShared feature map from the encoder\nO\ndet\nO_{\\text{det}}\nOutput of the object detection head\nO\nll\nO_{\\text{ll}}\nOutput of the lane line segmentation head\ns\ni\ns_{i}\nConfidence score for the\ni\ni\n-th bounding box\nb\ni\nb_{i}\ni\ni\n-th predicted bounding box\nÏ„\nconf\n\\tau_{\\text{conf}}\nConfidence threshold for filtering\nÏ„\nIoU\n\\tau_{\\text{IoU}}\nIoU threshold for Non-Maximum Suppression\nIoU\nâ€‹\n(\nb\ni\n,\nb\nj\n)\n\\text{IoU}(b_{i},b_{j})\nIntersection-over-Union between boxes\nb\ni\nb_{i}\nand\nb\nj\nb_{j}\ny\n^\nll\nâ€‹\n(\ni\n,\nj\n)\n\\hat{y}_{\\text{ll}}(i,j)\nFinal predicted class at pixel\n(\ni\n,\nj\n)\n(i,j)\nP\nc\nâ€‹\n(\ni\n,\nj\n)\nP_{c}(i,j)\nPredicted probability of class\nc\nc\nat pixel\n(\ni\n,\nj\n)\n(i,j)\nâ„’\nCE\n\\mathcal{L}_{\\text{CE}}\nCross-entropy loss\nâ„’\nIoU\n\\mathcal{L}_{\\text{IoU}}\nIoU-based segmentation loss\nT\nâ€‹\nP\n,\nF\nâ€‹\nP\n,\nF\nâ€‹\nN\nTP,FP,FN\nTrue positives, false positives, false negatives\nThis section outlines the implementation of a real-time multi-task perception system based on the YOLOP architecture. The system performs joint object detection and lane line segmentation on single-frame RGB inputs.\nInput images are first resized using the letterbox method, which preserves the original aspect ratio while fitting the networkâ€™s input resolution (typically 640\nÃ—\n\\times\n640). Letting\nr\nr\ndenote the resizing ratio and\n(\np\nx\n,\np\ny\n)\n(p_{x},p_{y})\nthe padding applied to the width and height respectively, the transformation of spatial coordinates is given by:\n(\nx\nâ€²\n,\ny\nâ€²\n)\n=\n(\nr\nâ€‹\nx\n+\np\nx\n,\nr\nâ€‹\ny\n+\np\ny\n)\n(x^{\\prime},y^{\\prime})=(rx+p_{x},\\;ry+p_{y})\n(1)\nThis operation ensures consistent spatial correspondence between input and output domains. After resizing, each pixel value is normalized per channel using dataset-specific mean\nÎ¼\n\\mu\nand standard deviation\nÏƒ\n\\sigma\n:\nx\nnorm\n=\nx\n/\n255\nâˆ’\nÎ¼\nÏƒ\nx_{\\text{norm}}=\\frac{x/255-\\mu}{\\sigma}\n(2)\nThe resulting tensor is converted to floating-point precision and passed to the model for inference.\nThe implementation employs the YOLOP architecture as proposed in the original work\n[\nwu2022yolop\n]\n, which integrates a shared backbone with three task-specific heads. Given an input image\nI\nI\n, the model produces a joint output:\nF\n=\nBackbone\nâ€‹\n(\nI\n)\n,\n(\nO\ndet\n,\nO\nda\n,\nO\nll\n)\n=\nHeads\nâ€‹\n(\nF\n)\nF=\\text{Backbone}(I),\\quad(O_{\\text{det}},O_{\\text{da}},O_{\\text{ll}})=\\text{Heads}(F)\n(3)\nwhere\nF\nF\ndenotes the shared feature representation extracted by the CSPDarknet backbone. The detection head\nO\ndet\nO_{\\text{det}}\npredicts bounding boxes and class probabilities, while\nO\nll\nO_{\\text{ll}}\nprovides a semantic segmentation map for lane markings. In the current implementation, the drivable area head\nO\nda\nO_{\\text{da}}\nis omitted.\nThe detection head outputs anchor-based predictions over multiple spatial scales. Postprocessing involves confidence filtering and Non-Maximum Suppression (NMS). Given a confidence threshold\nÏ„\nconf\n\\tau_{\\text{conf}}\n, candidate detections\nb\ni\nb_{i}\nwith score\ns\ni\ns_{i}\nare retained if:\ns\ni\nâ‰¥\nÏ„\nconf\ns_{i}\\geq\\tau_{\\text{conf}}\n(4)\nRedundant detections are suppressed using the standard IoU metric:\nIoU\nâ€‹\n(\nb\ni\n,\nb\nj\n)\n=\n|\nb\ni\nâˆ©\nb\nj\n|\n|\nb\ni\nâˆª\nb\nj\n|\n\\text{IoU}(b_{i},b_{j})=\\frac{|b_{i}\\cap b_{j}|}{|b_{i}\\cup b_{j}|}\n(5)\nBoxes with IoU exceeding a predefined threshold\nÏ„\nIoU\n\\tau_{\\text{IoU}}\nare eliminated in favor of the higher-scoring detection. The remaining bounding box coordinates are rescaled to match the original image resolution via the inverse of the letterbox transformation:\n(\nx\n,\ny\n)\n=\nx\nâ€²\nâˆ’\np\nx\nr\n(x,y)=\\frac{x^{\\prime}-p_{x}}{r}\n(6)\nThe lane line segmentation head produces a coarse pixel-wise prediction map of shape\n[\n1\n,\nC\n=\n2\n,\nH\n/\n8\n,\nW\n/\n8\n]\n[1,C=2,H/8,W/8]\n, where\nC\nC\ndenotes the number of semantic classes (lane vs. background). The predicted feature map is first cropped to remove padding, then upsampled to the original image resolution using bilinear interpolation. Final segmentation labels are obtained by selecting the most probable class per pixel:\ny\n^\nll\nâ€‹\n(\ni\n,\nj\n)\n=\narg\nâ¡\nmax\nc\nâˆˆ\n{\n0\n,\n1\n}\nâ¡\nP\nc\nâ€‹\n(\ni\n,\nj\n)\n\\hat{y}_{\\text{ll}}(i,j)=\\arg\\max_{c\\in\\{0,1\\}}P_{c}(i,j)\n(7)\nThis operation yields a binary segmentation mask indicating lane line presence. During training, this head is optimized using a composite loss combining cross-entropy and IoU-based terms:\nâ„’\nll-seg\n=\nâ„’\nCE\n+\nâ„’\nIoU\n,\nâ„’\nIoU\n=\n1\nâˆ’\nT\nâ€‹\nP\nT\nâ€‹\nP\n+\nF\nâ€‹\nP\n+\nF\nâ€‹\nN\n\\mathcal{L}_{\\text{ll-seg}}=\\mathcal{L}_{\\text{CE}}+\\mathcal{L}_{\\text{IoU}},\\quad\\mathcal{L}_{\\text{IoU}}=1-\\frac{TP}{TP+FP+FN}\n(8)\nAlthough the present implementation focuses on inference, the underlying architecture and behavior remain consistent with the supervised multi-task learning objective defined in the original model.\nThis implementation preserves the computational structure of the YOLOP model while enabling efficient real-time inference. Each processing stage is tightly coupled with the mathematical formulation presented in the original work, ensuring both architectural fidelity and practical performance.\n2\nPolynomial curve fitting\nThe nomenclature of the main variables in this section is shown in Table 4.2.\nTable 2:\nDescription of variables\nVariable\nDescription\n(\nu\n,\nv\n)\n(u,v)\nPixel coordinates in image frame.\nK\nK\nCamera intrinsic matrix.\np\ncam\np_{\\text{cam}}\nPoint in camera coordinate frame\np\nveh\np_{\\text{veh}}\nPoint in vehicle coordinate frame\nÎ»\n\\lambda\nDepth scale factor from projection\nP\ni\nP_{i}\nOriginal detected 3D point along lane polyline\nS\ni\nS_{i}\nCumulative arc length up to the\ni\ni\n-th point\nÎ”\nâ€‹\ns\n\\Delta s\nResampling interval in arc length\nL\nk\nL_{k}\nTarget arc length for the\nk\nk\n-th resampled point\nQ\nk\nQ_{k}\nInterpolated 3D point after resampling\n(\nx\ni\n,\ny\ni\n)\n(x_{i},y_{i})\n2D projection of resampled lane points for fitting\na\na\nCoefficient vector of the cubic polynomial\nV\nV\nVandermonde matrix constructed from\nx\ni\nx_{i}\ny\ny\nVector of corresponding\ny\ni\ny_{i}\nvalues\nQ\n,\nR\nQ,R\nOrthogonal and upper triangular matrices from QR decomposition\nThis section presents the mathematical foundation of the lane line reconstruction algorithm. The method consists of two core components: arc-length-based resampling of parameterized polylines and cubic polynomial curve fitting. These steps transform noisy and unevenly distributed raw lane detection points into a smooth and uniformly sampled trajectory suitable for downstream path planning and control.\nThe process begins by projecting 2D pixel coordinates from the image space into the 3D vehicle coordinate system. Given a pixel location\n(\nu\n,\nv\n)\n(u,v)\n, a camera intrinsic matrix\nK\nK\n, and an extrinsic transformation matrix\nT\nveh\nâ†\ncam\nT_{\\text{veh}\\leftarrow\\text{cam}}\n, the 3D point\np\nveh\nâˆˆ\nâ„\n3\np_{\\text{veh}}\\in\\mathbb{R}^{3}\nis obtained via:\nK\nâˆ’\n1\nâ€‹\n[\nu\nv\n1\n]\n=\nÎ»\nâ€‹\n[\nX\nc\nY\nc\nZ\nc\n]\nâ‡’\np\ncam\n=\n(\nX\nc\n,\nY\nc\n,\nZ\nc\n)\nâŠ¤\n,\nK^{-1}\\begin{bmatrix}u\\\\\nv\\\\\n1\\end{bmatrix}=\\lambda\\begin{bmatrix}X_{c}\\\\\nY_{c}\\\\\nZ_{c}\\end{bmatrix}\\Rightarrow p_{\\text{cam}}=(X_{c},Y_{c},Z_{c})^{\\top},\n(9)\nwhere\nÎ»\n\\lambda\nis a depth scaling factor determined via a flat ground assumption (e.g.,\nZ\nc\n=\n0\nZ_{c}=0\n) or by depth sensors.\np\nveh\n=\nT\nveh\nâ†\ncam\nâ‹…\n[\np\ncam\n1\n]\n.\np_{\\text{veh}}=T_{\\text{veh}\\leftarrow\\text{cam}}\\cdot\\begin{bmatrix}p_{\\text{cam}}\\\\\n1\\end{bmatrix}.\n(10)\nTo suppress outliers and improve fitting quality, points outside the region of interest (ROI)\n[\nx\nmin\n,\nx\nmax\n]\nÃ—\n[\ny\nmin\n,\ny\nmax\n]\n[x_{\\min},x_{\\max}]\\times[y_{\\min},y_{\\max}]\nare excluded.\nDetected lane points often appear as sparse or irregularly spaced polylines in the vehicle frame. To enhance stability, these polylines are resampled at constant arc-length intervals. Let the original 3D point sequence be\n{\nP\ni\n}\ni\n=\n0\nn\nâˆ’\n1\n\\{P_{i}\\}_{i=0}^{n-1}\n. The cumulative arc-length\nS\ni\nS_{i}\nis computed iteratively:\nS\n0\n=\n0\n,\nS\ni\n=\nS\ni\nâˆ’\n1\n+\nâ€–\nP\ni\nâˆ’\nP\ni\nâˆ’\n1\nâ€–\n2\n.\nS_{0}=0,\\quad S_{i}=S_{i-1}+\\|P_{i}-P_{i-1}\\|_{2}.\n(11)\nDefine the uniform resampling arc-length targets:\nL\nk\n=\nk\nâ‹…\nÎ”\nâ€‹\ns\n,\nk\n=\n0\n,\n1\n,\nâ€¦\n,\nâŒŠ\nS\nn\nâˆ’\n1\nÎ”\nâ€‹\ns\nâŒ‹\n.\nL_{k}=k\\cdot\\Delta s,\\quad k=0,1,\\ldots,\\left\\lfloor\\frac{S_{n-1}}{\\Delta s}\\right\\rfloor.\n(12)\nFor each\nL\nk\nL_{k}\n, locate segment\n[\nS\nj\n,\nS\nj\n+\n1\n]\n[S_{j},S_{j+1}]\nand compute interpolated point\nQ\nk\nQ_{k}\n:\nQ\nk\n=\n(\n1\nâˆ’\nt\n)\nâ€‹\nP\nj\n+\nt\nâ€‹\nP\nj\n+\n1\n,\nt\n=\nL\nk\nâˆ’\nS\nj\nS\nj\n+\n1\nâˆ’\nS\nj\n.\nQ_{k}=(1-t){P_{j}+tP}_{j+1},\\quad t=\\frac{L_{k}-S_{j}}{S_{j+1}-S_{j}}.\n(13)\nThis procedure produces arc-length-equidistant points, ensuring consistent geometric distribution for subsequent fitting.\nTo construct a continuous lane curve, we fit a third-order polynomial to the resampled set\n{\n(\nx\ni\n,\ny\ni\n)\n}\n\\{(x_{i},y_{i})\\}\n:\np\nâ€‹\n(\nx\n)\n=\na\n0\n+\na\n1\nâ€‹\nx\n+\na\n2\nâ€‹\nx\n2\n+\na\n3\nâ€‹\nx\n3\n,\np(x)=a_{0}+a_{1}x+a_{2}x^{2}+a_{3}x^{3},\n(14)\nwhere\nğš\n=\n[\na\n0\n,\na\n1\n,\na\n2\n,\na\n3\n]\nâŠ¤\n\\mathbf{a}=[a_{0},a_{1},a_{2},a_{3}]^{\\top}\nare the coefficients to be estimated.\np\nâ€‹\n(\nx\n)\np(x)\ndenote the cubic polynomial function obtained through curve fitting, where\nx\ni\nx_{i}\nand\ny\ni\ny_{i}\nrepresent the horizontal and vertical coordinates of the input sample points, respectively. The value\np\nâ€‹\n(\nx\ni\n)\np(x_{i})\ncorresponds to the predicted output of the fitting function at point\nx\ni\nx_{i}\n. The closer\np\nâ€‹\n(\nx\ni\n)\np(x_{i})\nis to\ny\ni\ny_{i}\n, the smaller the fitting error, indicating a more accurate and effective fitting performance.\nThe least-squares problem is formulated by constructing the Vandermonde matrix\nV\nâˆˆ\nâ„\nn\nÃ—\n4\nV\\in\\mathbb{R}^{n\\times 4}\nand observation vector\nğ²\n\\mathbf{y}\n:\nV\n=\n[\n1\nx\n1\nx\n1\n2\nx\n1\n3\n1\nx\n2\nx\n2\n2\nx\n2\n3\nâ‹®\nâ‹®\nâ‹®\nâ‹®\n1\nx\nn\nx\nn\n2\nx\nn\n3\n]\n,\nğ²\n=\n[\ny\n1\ny\n2\nâ‹®\ny\nn\n]\n.\nV=\\begin{bmatrix}1&x_{1}&x_{1}^{2}&x_{1}^{3}\\\\\n1&x_{2}&x_{2}^{2}&x_{2}^{3}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n1&x_{n}&x_{n}^{2}&x_{n}^{3}\\end{bmatrix},\\quad\\mathbf{y}=\\begin{bmatrix}y_{1}\\\\\ny_{2}\\\\\n\\vdots\\\\\ny_{n}\\end{bmatrix}.\n(15)\nThe fitting process involves minimizing the squared error between the predicted values\np\nâ€‹\n(\nx\ni\n)\np(x_{i})\nand the observed values\ny\ni\ny_{i}\nmin\nğš\nâ€‹\nâˆ‘\ni\n=\n1\nN\n(\np\nâ€‹\n(\nx\ni\n)\nâˆ’\ny\ni\n)\n2\n\\min_{\\mathbf{a}}\\sum_{i=1}^{N}\\left(p(x_{i})-y_{i}\\right)^{2}\n(16)\nTo solve this robustly, QR decomposition with column pivoting is applied:\nV\n=\nQ\nâ€‹\nR\nâ‡’\nğš\n=\nR\nâˆ’\n1\nâ€‹\nQ\nâŠ¤\nâ€‹\nğ²\n,\nV=QR\\Rightarrow\\mathbf{a}=R^{-1}Q^{\\top}\\mathbf{y},\n(17)\nthus avoiding explicit inversion of\nV\nâŠ¤\nâ€‹\nV\nV^{\\top}V\n.\nFinally, the polynomial is evaluated over a uniformly spaced set of\nx\nx\n-coordinates:\ny\ni\n=\na\n0\n+\na\n1\nâ€‹\nx\ni\n+\na\n2\nâ€‹\nx\ni\n2\n+\na\n3\nâ€‹\nx\ni\n3\n.\ny_{i}=a_{0}+a_{1}x_{i}+a_{2}x_{i}^{2}+a_{3}x_{i}^{3}.\n(18)\nThe result will be smooth and regularly spaced lane coordinates\n(\nx\ni\n,\ny\ni\n)\n(x_{i},y_{i})\n. suitable for integration into motion planning systems. Then, based on the generated left and right lane line path points, the sum of their horizontal coordinates is divided by 2 to obtain the center line of the lane, and finally we can obtain the required target point.\nDue to the many different situations in the actual course, the robot may fail to detect two lane lines in the image. Therefore, we supplement the algorithm so that the robot can navigate autonomously even when the lane lines are insufficient or missing. The handling logic is as follows:\n1.\nWhen both lane lines are detected, navigate according to the generated target point.\n2.\nWhen one lane line is missing, generate the lane line on the missing side based on the known lane width (\n3.5\nâ€‹\nm\n3.5m\n), then calculate the center line to obtain the target point.\n3.\nWhen no lane line is detected in the image, the robot drives forward at the preset minimum speed\nv\nmin\nv_{\\text{min}}\n(which will be introduced in Chapter 5).\nThis section has presented a mathematically grounded pipeline for lane reconstruction, involving coordinate transformation, arc-length resampling, and cubic polynomial fitting. The use of QR-based least-squares optimization enhances robustness to outliers and ill-conditioned data, while arc-length parametrization improves the geometric regularity of the input. Together, these methods enable the conversion of sparse detections into a high-quality, continuous lane model.\n3\nLyapunov-based control\nThe nomenclature of the main scalars and preset parameters in this section is shown in Table 4.3.\nTable 3:\nDescription of scalars and preset parameters\nVariable\nDescription\nV\nV\nThe total Lyapunov function\nV\n1\nV_{1}\nLyapunov function term related to linear tracking error\nV\n2\nV_{2}\nLyapunov function term related to angular tracking error\nk\n1\nk_{1}\nWeight on\nÎ±\n\\alpha\nin Lyapunov\nV\n2\nV_{2}\nk\n2\nk_{2}\nWeight on\nÎ²\n\\beta\nin Lyapunov\nV\n2\nV_{2}\nr\nÎ»\nv\n\\lambda_{v}\nGain that shrinks distance\nÏ\n\\rho\nÎ»\na\n\\lambda_{a}\nGain that drives heading angle error to 0\nThe primary objective of the controller is to enable the robot to accurately track a target point that changes its position dynamically over time. To achieve this goal, a controller grounded in Lyapunov stability theory is proposed. This controller is designed to compute both the linear and angular velocities required for the robot to adjust its motion in real-time. By continuously regulating the robotâ€™s position and orientation, the controller ensures that the robot follows the real-time changing path target points effectively and stably. The design and implementation of the controller are firmly based on the kinematic model of the robot, which provides a mathematical representation of its motion characteristics. Through the utilization of this model, the controller can precisely determine the necessary control inputs to guide the robot in a manner that guarantees convergence to the desired path.\nThe following candidate Lyapunov function is chosen for controller design to facilitate the stability analysis and guide the formulation of the control laws:\nV\n\\displaystyle V\n=\nV\n1\n+\nV\n2\n\\displaystyle=V_{1}+V_{2}\n(19)\nV\n1\n\\displaystyle V_{1}\n=\n1\n2\nâ€‹\nÏ\n2\n\\displaystyle=\\frac{1}{2}\\rho^{2}\n(20)\nV\n2\n\\displaystyle V_{2}\n=\n1\nâˆ’\ncos\nâ¡\nÎ±\nk\n1\n+\n1\nâˆ’\ncos\nâ¡\nÎ²\nk\n2\n\\displaystyle=\\frac{1-\\cos\\alpha}{k_{1}}+\\frac{1-\\cos\\beta}{k_{2}}\n(21)\nwhere\nV\n1\nV_{1}\nand\nV\n2\nV_{2}\nare scalar functions specifically designed to quantify the linear and angular tracking errors, respectively. The term\nV\n1\nV_{1}\nis associated with the translational deviation of the robot from the desired target position, while\nV\n2\nV_{2}\nreflects the orientation error between the robotâ€™s current heading and the desired direction. By appropriately constructing\nV\n1\nV_{1}\nand\nV\n2\nV_{2}\n, the Lyapunov function\nV\nV\nserves as a measure of the overall tracking performance. Ensuring that the time derivative of\nV\nV\nis negative definite guarantees the asymptotic convergence of both position and orientation errors, thereby validating the stability and effectiveness of the proposed controller.\nDifferentiating\nV\n1\nV_{1}\nwith respect to time\nt\nt\n, and taking into account the system dynamics as described in Eq. (3.14), we obtain the time derivative\nV\nË™\n1\n\\dot{V}_{1}\n,\nV\nË™\n1\n=\nÏ\nâ€‹\n(\nv\nt\nâ€‹\ncos\nâ¡\nÎ²\nâˆ’\nv\nâ€‹\ncos\nâ¡\nÎ±\n)\n\\dot{V}_{1}=\\rho(v_{t}\\cos\\beta-v\\cos\\alpha)\n(22)\nTo ensure the non-positivity of\nV\nË™\n1\n\\dot{V}_{1}\n, the linear velocity\nv\nv\nis designed in such a way that it counteracts the growth of the linear tracking error. The control law for\nv\nv\nis therefore derived as follows:\nv\n=\nv\nt\nâ€‹\ncos\nâ¡\nÎ²\ncos\nâ¡\nÎ±\n+\nÎ»\nv\nâ€‹\nÏ\nâ€‹\ncos\nâ¡\nÎ±\nv=\\frac{v_{t}\\cos\\beta}{\\cos\\alpha}+\\lambda_{v}\\rho\\cos\\alpha\n(23)\nwhere\nÎ»\nv\n>\n0\n\\lambda_{v}>0\nis a control parameter. Substituting\nv\nv\ninto Eq. (4.22)\nV\nË™\n1\n=\nâˆ’\nÎ»\nv\nâ€‹\nÏ\n2\nâ€‹\ncos\n2\nâ¡\nÎ±\nâ‰¤\n0\n\\dot{V}_{1}=-\\lambda_{v}\\rho^{2}\\cos^{2}\\alpha\\leq 0\n(24)\nThus, the designed linear velocity\nv\nv\nensures that\nV\nË™\n1\n\\dot{V}_{1}\nremains non-positive, which leads to the convergence of the position error\nÏ\n\\rho\n. However, it is important to note that\nv\nv\nis directly proportional to several variables, including the distance\nÏ\n\\rho\n, the target velocity\nv\nt\nv_{t}\n, the cosine of the heading error\ncos\nâ¡\nÎ²\n\\cos\\beta\n, and the inverse cosine of the orientation error\ncos\nâˆ’\n1\nâ¡\nÎ±\n\\cos^{-1}\\alpha\n. These variables tend to have large magnitudes during the initial phase of the systemâ€™s operation when the robot is far from the target or significantly misaligned. As a result, the computed value of\nv\nv\nmay become excessively large at the beginning, which could potentially lead to high control effort or actuator saturation. Therefore, additional considerations or modifications, such as gain tuning or saturation constraints, may be required to ensure the practical feasibility and smooth performance of the controller in real-world implementations.\nThis issue becomes even more pronounced when designing the angular velocity\nÏ‰\n\\omega\nto ensure that\nV\nË™\n2\n\\dot{V}_{2}\nis non-positive, as both control inputs jointly influence the overall system dynamics. In particular, the dependence of\nÏ‰\n\\omega\non variables that also affect\nv\nv\ncan exacerbate the problem of excessive control effort in the initial stages, potentially leading to undesirable transient behavior or instability. To mitigate this effect and to prevent the linear velocity from reaching impractically high values, the expression for\nv\nv\nis accordingly modified as follows:\nv\n=\n(\nv\nt\nâ€‹\ncos\nâ¡\nÎ²\n+\nÎ»\nv\nâ€‹\nÏ\n)\nâ€‹\ncos\nâ¡\nÎ±\nv=(v_{t}\\cos\\beta+\\lambda_{v}\\rho)\\cos\\alpha\n(25)\nSubstituting into Eq. (4.22) and Eq. (3.14)\nV\nË™\n1\n=\nâˆ’\nÎ»\nv\nâ€‹\nÏ\n2\nâ€‹\ncos\n2\nâ¡\nÎ±\n+\nv\nt\nâ€‹\nÏ\nâ€‹\nsin\n2\nâ¡\nÎ±\nâ€‹\ncos\nâ¡\nÎ²\n\\dot{V}_{1}=-\\lambda_{v}\\rho^{2}\\cos^{2}\\alpha+v_{t}\\rho\\sin^{2}\\alpha\\cos\\beta\n(26)\nÏ\nË™\n=\nâˆ’\nÎ»\nv\nâ€‹\nÏ\nâ€‹\ncos\n2\nâ¡\nÎ±\n+\nv\nt\nâ€‹\nsin\n2\nâ¡\nÎ±\nâ€‹\ncos\nâ¡\nÎ²\n\\dot{\\rho}=-\\lambda_{v}\\rho\\cos^{2}\\alpha+v_{t}\\sin^{2}\\alpha\\cos\\beta\n(27)\nFrom Eq. (4.26), it is clear that\nV\nË™\n1\n\\dot{V}_{1}\nwill not be non-positive until\nÎ±\nâ†’\n0\n\\alpha\\to 0\n, the new\nv\nv\nin Eq. (4.25) is designed to reduce the contribution of the target velocity\nv\nt\nv_{t}\n, by multiplying with\nc\nâ€‹\no\nâ€‹\ns\nâ€‹\nÎ±\ncos\\alpha\nand\nc\nâ€‹\no\nâ€‹\ns\nâ€‹\nÎ²\ncos\\beta\n, which are bounded by magnitude 1. The contribution of\nÏ\n\\rho\nis attenuated by a factor of\nc\nâ€‹\no\nâ€‹\ns\nâ€‹\nÎ±\ncos\\alpha\n.\nFor determining the controller input\nÏ‰\n\\omega\naccordingly, differentiating\nV\n2\nV_{2}\nwith respect to time\nt\nt\nand considering Eq. (3.15), Eq. (3.16) and Eq. (4.25)\nV\nË™\n2\n\\displaystyle\\dot{V}_{2}\n=\n(\nsin\nâ¡\nÎ±\nk\n1\nâ€‹\nÏ\n+\nsin\nâ¡\nÎ²\nk\n2\nâ€‹\nÏ\n)\n[\n(\nsin\nâ¡\n2\nâ€‹\nÎ±\n2\ncos\nÎ²\nâˆ’\nsin\nÎ²\n)\nv\nt\n\\displaystyle=\\left(\\frac{\\sin\\alpha}{k_{1}\\rho}+\\frac{\\sin\\beta}{k_{2}\\rho}\\right)\\bigg[\\left(\\frac{\\sin 2\\alpha}{2}\\cos\\beta-\\sin\\beta\\right)v_{t}\n(28)\nâˆ’\nÏ‰\nsin\nâ¡\nÎ±\nk\n1\nâˆ’\nÏ•\nË™\nt\nsin\nâ¡\nÎ²\nk\n2\n+\nsin\nâ¡\n2\nâ€‹\nÎ±\n2\nÎ»\nv\n(\nsin\nâ¡\nÎ±\nk\n1\n+\nsin\nâ¡\nÎ²\nk\n2\n)\n]\n\\displaystyle\\quad{}-\\omega\\frac{\\sin\\alpha}{k_{1}}-\\dot{\\phi}_{t}\\frac{\\sin\\beta}{k_{2}}+\\frac{\\sin 2\\alpha}{2}\\lambda_{v}\\left(\\frac{\\sin\\alpha}{k_{1}}+\\frac{\\sin\\beta}{k_{2}}\\right)\\bigg]\nLetting\nÏ‰\n\\displaystyle\\omega\n=\nÎ»\nÎ±\nsin\nÎ±\n+\n(\nsin\nâ¡\nÎ±\nk\n1\nâ€‹\nÏ\n+\nsin\nâ¡\nÎ²\nk\n2\nâ€‹\nÏ\n)\n[\n(\nk\n1\nâ€‹\nsin\nâ¡\n2\nâ€‹\nÎ±\n2\nâ€‹\nsin\nâ¡\nÎ±\ncos\nÎ²\nâˆ’\nk\n1\nâ€‹\nsin\nâ¡\nÎ²\nsin\nâ¡\nÎ±\n)\nv\nt\n\\displaystyle=\\lambda_{\\alpha}\\sin\\alpha+\\left(\\frac{\\sin\\alpha}{k_{1}\\rho}+\\frac{\\sin\\beta}{k_{2}\\rho}\\right)\\bigg[\\left(\\frac{k_{1}\\sin 2\\alpha}{2\\sin\\alpha}\\cos\\beta-\\frac{k_{1}\\sin\\beta}{\\sin\\alpha}\\right)v_{t}\n(29)\nâˆ’\nÏ•\nË™\nt\nk\n1\nâ€‹\nsin\nâ¡\nÎ²\nk\n2\nâ€‹\nsin\nâ¡\nÎ±\n+\nk\n1\nâ€‹\nsin\nâ¡\n2\nâ€‹\nÎ±\n2\nâ€‹\nsin\nâ¡\nÎ±\nÎ»\nv\n(\nsin\nâ¡\nÎ±\nk\n1\n+\nsin\nâ¡\nÎ²\nk\n2\n)\n]\n\\displaystyle\\quad{}-\\dot{\\phi}_{t}\\frac{k_{1}\\sin\\beta}{k_{2}\\sin\\alpha}+\\frac{k_{1}\\sin 2\\alpha}{2\\sin\\alpha}\\lambda_{v}\\left(\\frac{\\sin\\alpha}{k_{1}}+\\frac{\\sin\\beta}{k_{2}}\\right)\\bigg]\nSubstituting Eq. (4.29) into Eq. (4.28) yields,\nV\nË™\n2\n=\nâˆ’\nÎ»\nÎ±\nâ€‹\nsin\n2\nâ¡\nÎ±\nk\n1\n\\dot{V}_{2}=-\\frac{\\lambda_{\\alpha}\\sin^{2}\\alpha}{k_{1}}\n(30)\nThis shows that\nV\n2\nV_{2}\nis a non-increasing function of time, since its time derivative\nV\nË™\n2\n\\dot{V}_{2}\nis designed to be non-positive. Given\nV\n2\n>\n0\nV_{2}>0\nas defined,\nV\n2\nV_{2}\nconverges to a non-negative limit asymptotically. A similar conclusion holds for\nV\n1\nV_{1}\n, which was previously shown to be non-increasing under the designed control input\nv\nv\n. Consequently, both Lyapunov functions\nV\n1\nV_{1}\nand\nV\n2\nV_{2}\nremain bounded and approach steady-state values over time. Since\nV\n1\nV_{1}\nand\nV\n2\nV_{2}\nare constructed based on the tracking errors in the state variables\nÏ\n\\rho\n,\nÎ±\n\\alpha\n, and\nÎ²\n\\beta\n, it can be inferred that these variables are also bounded for all\nt\nâ‰¥\n0\nt\\geq 0\n. This boundedness ensures that the robotâ€™s position and orientation remain within a controlled and predictable range during operation, providing a necessary condition for the overall stability and safety of the tracking control system. Figure 4.1 illustrates the implementation logic for the Lyapunov-based controller.\nFigure 1:\nLyapunuov-based controller diagram\nThus, under the action of the designed controller, asymptotic convergence of the tracking errors can be achieved. Specifically, the position error\nÏ\n\\rho\ntends to zero, and the orientation errors\nÎ±\n\\alpha\nand\nÎ²\n\\beta\nalso converge to zero as time progresses, i.e.,\nÏ\nâ†’\n0\n\\rho\\to 0\n,\nÎ±\nâ†’\n0\n\\alpha\\to 0\n, and\nÎ²\nâ†’\n0\n\\beta\\to 0\nas\nt\nâ†’\nâˆ\nt\\to\\infty\n. This indicates that the robot is able to accurately track the moving target point in both position and orientation. The convergence is guaranteed even when the target point itself is in motion, traveling with a velocity\nv\nt\nv_{t}\nalong a time-varying direction\nÏ•\nt\n\\phi_{t}\n. The control strategy effectively compensates for the dynamic nature of the targetâ€™s trajectory, ensuring that the robot aligns its motion with the target in real time. As a result, the proposed Lyapunov-based controller not only stabilizes the tracking errors but also enables the robot to follow the moving target smoothly and reliably.\n4\nConventional Lyapunov-based controllers used for comparison\nThe performance of the designed controller is evaluated through a comparative analysis with conventional Lyapunov-based controllers, specifically the one proposed in\n[\nhuang2009control\n]\n. In that work, a Lyapunov-based control strategy was developed for an autonomous mobile robot (AMR) to track a moving target point. Similar to the present approach, both the linear and angular velocities, denoted as\nv\nv\nand\nÏ‰\n\\omega\n, are computed using the Lyapunov second method to ensure stability of the tracking process.\nThe comparative Lyapunov function of controller is\nV\n\\displaystyle V\n=\nV\n1\n+\nV\n2\n\\displaystyle=V_{1}+V_{2}\n(31)\nV\n1\n\\displaystyle V_{1}\n=\n1\n2\nâ€‹\nÏ\n2\n,\nV\n2\n=\n1\n2\nâ€‹\n(\nÎ±\n2\n+\nÎ²\n2\n)\n\\displaystyle=\\frac{1}{2}\\rho^{2},\\quad V_{2}=\\frac{1}{2}(\\alpha^{2}+\\beta^{2})\n(32)\nIn the comparative controller, the Lyapunov function is similarly decomposed into two components,\nV\n1\nV_{1}\nand\nV\n2\nV_{2}\n, which are individually designed to address the linear and angular tracking errors, respectively. Specifically,\nV\n1\nV_{1}\ncaptures the translational error between the robot and the moving target point, while\nV\n2\nV_{2}\nrepresents the deviation in orientation.\nBy choosing appropriate values for\nv\nv\nand\nÏ‰\n\\omega\n, the time derivatives\nV\nË™\n1\n\\dot{V}_{1}\nand\nV\nË™\n2\n\\dot{V}_{2}\ncan be made non-positive, ensuring stability.\nv\nv\nand\nÏ‰\n\\omega\nare found to be\nv\n=\n(\nv\nt\nâ€‹\ncos\nâ¡\nÎ²\n+\nÎ»\nv\nâ€‹\nÏ\n)\nâ€‹\ncos\nâ¡\nÎ±\nv=(v_{t}\\cos\\beta+\\lambda_{v}\\rho)\\cos\\alpha\n(33)\nÏ‰\n=\nÎ»\nÎ±\nâ€‹\nÎ±\n+\nÎ±\n+\nÎ²\nÏ\nâ€‹\n(\nsin\nâ¡\n2\nâ€‹\nÎ±\n2\nâ€‹\nÎ±\nâ€‹\ncos\nâ¡\nÎ²\nâˆ’\nsin\nâ¡\nÎ²\nÎ±\n)\nâ€‹\nv\nt\nâˆ’\nÎ²\nÎ±\nâ€‹\nÏ•\nË™\nt\n+\nsin\nâ¡\n2\nâ€‹\nÎ±\n2\nâ€‹\nÎ±\nâ€‹\nÎ»\nv\nâ€‹\n(\nÎ±\n+\nÎ²\n)\n\\omega=\\lambda_{\\alpha}\\alpha+\\frac{\\alpha+\\beta}{\\rho}\\left(\\frac{\\sin 2\\alpha}{2\\alpha}\\cos\\beta-\\frac{\\sin\\beta}{\\alpha}\\right)v_{t}-\\frac{\\beta}{\\alpha}\\dot{\\phi}_{t}+\\frac{\\sin 2\\alpha}{2\\alpha}\\lambda_{v}(\\alpha+\\beta)\n(34)\n5\nImplementation process of autonomous navigation\nThe following explains the overall process of robot autonomous navigation. The flow chart of autonomous navigation is shown in Figure 4.2. The following is a brief description of the steps shown in the flow chart.\n1.\nSubscribe to the image obtained by the camera.\n2.\nDetect the lane lines in the image through YOLOP.\n3.\nPerform polynomial interpolation on the obtained left and right lane lines to generate smooth path points with equal spacing.\n4.\nCalculate the center line based on the left and right lane lines to obtain the target point.\n5.\nBased on the target point, calculate the required linear velocity and angular velocity through the controller.\n6.\nSend the velocity command to the robot kinematic model to achieve the motor rotation.\nFigure 2:\nAutonomous navigation flow chart\nChapter 5\nExperiment Results and Discussion\n1\nExperimental scenario\nTo further validate the practical applicability of the proposed autonomous navigation strategy in a race track environment, real-world experiments were conducted. The evaluation was carried out using the AI Formula unmanned vehicle, as introduced in Chapter 1, which was controlled by the Lyapunov-based control system developed in this study. The main hardware and software configurations used in the experimental setup are detailed as Table. 5.1.\nTable 1:\nHardware and software configuration\nHardware and software\nModel/Specification\nPC\nJetson AGX Orin\nOS\nUbuntu 20.04\nROS version\nROS Foxy\nCamera\nZEDX\nGNSS+IMU\nVN200\nWheel encoder\nFBLG2360T\nThe experimental tests were conducted on the race track described in Chapter 1. A complete lap of autonomous navigation was performed at varying speeds, starting line, finishing line and different scenarios shown in Figures 5.2 to 5.6 at the locations indicated in Figure 5.1. The experimental data collected under these conditions were subsequently compared and analyzed. Based on prior experience with parameter tuning during preliminary experiments, the control gains were set to\nÎ»\nv\n=\n0.075\n\\lambda_{v}=0.075\nand\nÎ»\na\n=\n0.15\n\\lambda_{a}=0.15\n. For the proposed controller, the parameters were configured as\nk\n1\n=\n0.8\nk_{1}=0.8\nand\nk\n2\n=\n50\nk_{2}=50\n.\nFigure 1:\nTop view and starting line, finishing line and different scenarios of the race track\nIn addition, to ensure operational safety and maintain the reliability of visual perception during autonomous navigation, upper and lower bounds were imposed on both the linear and angular velocities of the robot. These constraints are particularly important to mitigate the risk of recognition failures caused by direct sunlight interfering with the stereo cameraâ€™s field of view during operation. By limiting the robotâ€™s motion within predefined velocity ranges, the system can avoid excessive speed that may lead to unstable behavior.\n0.6\nâ‰¤\nv\nâ‰¤\n1.75\n,\nwhen\nâ€‹\nv\nt\n=\n1.5\nâ€‹\nm\n/\ns\n\\displaystyle 0.6\\leq v\\leq 1.75,\\text{when}\\,v_{t}=1.5m/s\n(1)\n0.6\nâ‰¤\nv\nâ‰¤\n2.25\n,\nwhen\nâ€‹\nv\nt\n=\n2.0\nâ€‹\nm\n/\ns\n\\displaystyle 0.6\\leq v\\leq 2.25,\\text{when}\\,v_{t}=2.0m/s\n(2)\nâˆ’\n0.4\nâ‰¤\nÏ‰\nâ‰¤\n0.4\n\\displaystyle-0.4\\leq\\omega\\leq 0.4\n(3)\n2\nPerception results\nThis section primarily presents the recognition results obtained using the YOLOP model, along with the corresponding outcomes of the polynomial curve fitting process applied to the detected lane lines. The purpose of this analysis is to evaluate the accuracy and reliability of the visual perception module in extracting lane features and generating smooth, continuous representations of the driving path, which are essential for subsequent path planning and control. It should be noted that the centers in the following grid maps are the geometric centers of the robotâ€™s kinematic model, and the direction in which the robot moves is recorded as the x direction. Each subplot in Section 5.2, the green dotted line corresponds to the obtained lane line, while the blue dotted line represents the center line calculated by obtained lane lines.\nFigure 5.2 illustrates the scenario in which the lane line captured in the image are straight, solid lines. As shown, the recognition performance is highly satisfactory. The lane line detected by the YOLOP and the corresponding centerline generated through polynomial fitting closely align with expectations. The resulting fitted curves are smooth and accurate, thereby providing reliable and high-quality input for the controller in the autonomous navigation system.\nFigure 2:\nPerception and fitting results of straight and solid lines\nFigure 5.3 depicts the scenario in which the lane lines in the image are straight and consist of dotted and solid lines. It can be observed that the recognition results differ to some extent from the lane lines and centerline generated through polynomial fitting. This discrepancy is primarily attributed to the intermittent gaps in the dotted lines, which introduce deviations in the fitting results from the actual lane geometry. Nevertheless, in the regions where the white line segments are present, both the detection and the fitted curves are consistent with expectations, demonstrating reliable performance in those segments.\nFigure 3:\nPerception and fitting results of straight and dotted lines\nFigure 5.4 presents the scenario in which the lane lines in the image are both curve and solid. As illustrated, the recognition results are highly satisfactory. The detected lane line and the corresponding centerline obtained through polynomial fitting closely align with the expected geometry. The fitted curves are smooth and accurate, thereby offering high-quality input to the controller for effective path tracking.\nFigure 4:\nPerception and fitting results of curve and solid lines\nFigure 5.5 illustrates the scenario in which the lane lines in the image are curve and consist of dotted and straight line. It is evident that the recognition performance on the side with the dotted line, as well as the polynomial fitting results for the lane line and the resulting centerline, are suboptimal. This reduced accuracy is primarily due to the presence of intermittent gaps in the dotted line, which cause the fitting process to deviate from the true lane geometry. Consequently, both the recognition and fitting outcomes do not fully meet expectations in this case.\nFigure 5:\nPerception and fitting results of curve and dotted lines\nFigure 5.6 depicts the scenario in which the image contains a zebra crossing. As observed, the recognition performance in this case is significantly degraded. The presence of dense and irregular patterns interferes with lane line detection, resulting in a distorted polynomial fitting outcome. Consequently, a reliable and continuous centerline cannot be generated, leading to a failure in providing stable input to the controller at that moment.\nFigure 6:\nPerception and fitting results of zebra crossing\nAlthough the perception and fitting results in the five aforementioned images vary in quality, it is important to note that the controllerâ€™s input primarily relies on three look ahead points. From the visualizations, it can be observed that at a distance of approximately 2â€“3 meters from the vehicleâ€™s coordinate origin (with each grid unit in the figure corresponding to 0.5 Ã— 0.5 meters in the real world), the perception outcomesâ€”regardless of whether the lane is straight or curved, solid or dottedâ€”tend to remain stable and accurately reflect the expected centerline. Based on this observation, a point located 2 meters ahead of the vehicle center along the x-axis is selected as the primary target point. Additionally, two more points spaced at 0.5-meter intervals in the positive x-axis from the target point are chosen to form a set of three look ahead points. This configuration ensures that the controller receives consistent and reliable input during the robotâ€™s autonomous navigation process.\n3\nPreset path based tracking results\nThis section primarily presents the results of the robotâ€™s path tracking performance under the control of the Lyapunov-based controller based on preset path. The objective is to evaluate the effectiveness and stability of this control strategy in guiding the robot along a given path, independent of real-time visual perception. By analyzing tracking accuracy and motion behavior, the results aim to validate the controllerâ€™s capability to achieve smooth and reliable tracking effect under varying target speed conditions. In each subplot in Section 5.3, the blue curve corresponds to the trajectory-following control method, while the red curve represents the results obtained using the comparative method.\nFigure 5.7 illustrates the experimental comparison of path tracking performance between the two control methods at\nv\nt\n=\n1.5\nâ€‹\nm\n/\ns\nv_{t}=1.5m/s\n. It is evident that the trajectory of proposed Lyapunov-based controller, exhibits a smooth and stable path throughout the entire course. In contrast, the comparative method fails to effectively follow the preset path and, in some segments, deviates significantly from the reference path. The robot controlled by the proposed strategy maintains accurate tracking with producing a smoother driving trajectory.. This superior performance highlights the robustness and precision of the proposed method in executing reliable and continuous path tracking, thereby demonstrating its effectiveness in autonomous navigation tasks based on preset path.\nFigure 7:\nTrajectory results of the two controllers when the target speed\nv\nt\n=\n1.5\nâ€‹\nm\n/\ns\nv_{t}=1.5m/s\nFigure 5.10, from top to bottom, displays the time-series data of the robotâ€™s\nv\nv\n,\nÏ‰\n\\omega\n,\nx\nx\nalong the x-axis,\ny\ny\nalong the y-axis, and\nÏ•\n\\phi\nwhen\nv\nt\n=\n1.5\nâ€‹\nm\n/\ns\nv_{t}=1.5m/s\n. It is evident that the proposed method results in significantly smoother and more stable control performance compared to the comparative method. The linear speed remains close to the target with minimal fluctuations, whereas the comparative method exhibits multiple sharp drops and large deviations, even triggering velocity and acceleration limit constraints during execution. Similarly, the angular velocity under the proposed method maintains lower variance, indicating more consistent heading adjustments. In the positional plots along both the\nx\nx\n- and\ny\ny\n-axes, the proposed method follows a continuous and predictable trajectory, while the comparative method shows larger deviations, particularly around the middle and later segments of the trajectory. Notably, the orientation\nÏ•\n\\phi\nremains stable throughout the entire path with the proposed controller, whereas the comparative method introduces sudden jumps and discontinuities, indicating control instability. These results collectively validate the superior tracking accuracy, dynamic smoothness, and control robustness of the proposed Lyapunov-based method under moderate-speed navigation.\nFigure 8:\nResults with two controllers when the target speed\nv\nt\n=\n1.5\nâ€‹\nm\n/\ns\nv_{t}=1.5m/s\n, from top to bottom: linear speed, angular speed, x-axis position, y-axis position, and the orientation\nTable 5.2 provides a detailed quantitative comparison of the proposed method and the comparative method across several performance metrics under the condition of a target speed\nv\nt\n=\n1.5\nâ€‹\nm/s\nv_{t}=1.5\\,\\text{m/s}\n. The experimental results demonstrate the clear superiority of the proposed Lyapunov-based control strategy in terms of tracking accuracy, control stability, and overall efficiency. In terms of task completion, the proposed method completes the course in 183.60â€‰s, which is 8.1â€‰s faster than the comparative method, while simultaneously achieving a higher average linear speed. This indicates that the proposed controller is capable of maintaining higher velocity without compromising path stability. From a path tracking perspective, the proposed method yields a significantly lower mean absolute error(MAE) of lateral compared to the comparative method, as well as a substantial reduction in the mean MAE of orientation. These results confirm improved lateral precision and heading consistency during navigation. In terms of target speed, the proposed controller demonstrates a much lower root mean square error (RMSE) of linear speed compared to the comparative method. Furthermore, the linear speed deviation is reduced by more than two-thirds, reflecting more stable and accurate speed control. Finally, the accumulated orientation is considerably lower for the proposed method compared to the comparative method, indicating smoother directional changes and better dynamic stability.\nTable 2:\nPerformance metrics comparison between proposed and comparative methods when the target speed\nv\nt\n=\n1.5\nâ€‹\nm\n/\ns\nv_{t}=1.5m/s\nProposed method\nComparative method\nCompletion time (s)\n183.60\n191.70\nAverage Linear speed (m/s)\n1.45\n1.35\nAverage angular speed (rad/s)\n0.032\n0.035\nMAE of lateral (m)\n0.20\n1.13\nMAE of orientation (rad)\n0.069\n0.31\nRMSE of linear speed (m/s)\n0.014\n0.090\nLinear speed deviation (%)\n3.33\n10.00\nAccumulated orientation (rad)\n7.43\n19.71\nOverall, these results quantitatively validate the effectiveness and robustness of the proposed trajectory-following control approach. It not only enables more accurate and stable path tracking but also enhances motion efficiency and smoothness, outperforming the comparative method across all evaluated metrics.\nFigure 5.9 presents the experimental comparison of path tracking performance between the proposed Lyapunov-based controller and the comparative method at a target speed of\nv\nt\n=\n2.0\nâ€‹\nm/s\nv_{t}=2.0\\,\\text{m/s}\n. As shown in the figure, the trajectory generated by the proposed method closely follows the preset path throughout the entire course, maintaining high consistency and smoothness. In contrast, the comparative method exhibits a substantial deviation from the preset path in the latter segment of the trajectory, where it fails to complete the loop and diverges significantly from the intended route.\nFigure 9:\nTrajectory results of the two controllers when the target speed\nv\nt\n=\n2.0\nâ€‹\nm\n/\ns\nv_{t}=2.0m/s\nFigure 5.10, from top to bottom, displays the time-series data of the robotâ€™s\nv\nv\n,\nÏ‰\n\\omega\n,\nx\nx\nalong the x-axis,\ny\ny\nalong the y-axis, and\nÏ•\n\\phi\nwhen\nv\nt\n=\n2.0\nâ€‹\nm\n/\ns\nv_{t}=2.0m/s\n. The proposed method exhibits more stable and consistent control performance compared to the comparative method . The linear speed remains close to the target speed with minimal fluctuation throughout the trajectory, whereas the comparative method shows significant speed degradation in the later stage, indicating loss of velocity tracking. Similarly, the angular velocity under the proposed controller remains well-regulated, with reduced oscillation amplitude, while the comparative method exhibits pronounced instability, particularly in the final segment, where excessive fluctuations and sharp peaks occur. In terms of position tracking along both the\nx\nx\n- and\ny\ny\n-axes, the trajectories of the proposed method align closely with the reference path, while the comparative method demonstrates increasing deviation as time progresses. The orientation profile\nÏ•\n\\phi\nfurther confirms the superior performance of the proposed method. The heading angle changes smoothly without abrupt transitions, in contrast to the comparative method, which suffers from a sudden and unstable orientation reversal near the 70â€“80â€‰s mark. These results collectively highlight the robustness, accuracy, and dynamic stability of the proposed Lyapunov-based controller in maintaining reliable path tracking at higher speeds.\nFigure 10:\nResults with two controllers when the target speed\nv\nt\n=\n2.0\nâ€‹\nm\n/\ns\nv_{t}=2.0m/s\n, from top to bottom: linear speed, angular speed, x-axis position, y-axis position, and the orientation\nTable 5.3 presents a quantitative comparison of the proposed method and the comparative method under the target speed\nv\nt\n=\n2.0\nâ€‹\nm/s\nv_{t}=2.0\\,\\text{m/s}\n, across a range of performance evaluation metrics. The results demonstrate that the proposed Lyapunov-based control strategy maintains its superior performance, even under higher-speed conditions. In terms of efficiency, the proposed method achieves a shorter completion time, while also attaining a higher average linear speed. These improvements indicate that the proposed controller allows the robot to complete the task more quickly while preserving trajectory integrity. From the perspective of path tracking, the proposed method exhibits better performance, with a lower MAE of lateral and a reduced MAE of orientation. Although the tracking errors are slightly higher than those observed at the lower speed of\n1.5\nâ€‹\nm/s\n1.5\\,\\text{m/s}\n, the overall accuracy remains within acceptable bounds and superior to the baseline. Regarding speed control, the proposed method achieves a significantly lower RMSE of linear speed compared to the comparative method, and a notably smaller linear speed deviation , reflecting consistent and stable velocity regulation even at elevated speeds. Finally, the accumulated orientation of the proposed method is slightly lower than that of the comparative method, indicating smoother turning behavior and better directional stability. Notably, the accumulated orientation at\n2.0\nâ€‹\nm/s\n2.0\\,\\text{m/s}\nremains close to that observed at\n1.5\nâ€‹\nm/s\n1.5\\,\\text{m/s}\n, suggesting that the proposed controller successfully maintains its stability across different speed regimes.\nTable 3:\nPerformance metrics comparison between proposed and comparative methods when the target speed\nv\nt\n=\n2.0\nâ€‹\nm\n/\ns\nv_{t}=2.0m/s\nProposed method\nComparative method\nCompletion time (s)\n136.60\n148.90\nAverage linear speed (m/s)\n1.93\n1.80\nAverage angular speed (rad/s)\n0.042\n0.062\nMAE of lateral (m)\n0.26\n0.49\nMAE of orientation (rad)\n0.079\n0.11\nRMSE of linear speed (m/s)\n0.013\n0.15\nLinear speed deviation (%)\n3.50\n10.00\nAccumulated orientation (rad)\n7.83\n8.22\nOverall, these results affirm the robustness and adaptability of the proposed trajectory-following control framework. It continues to outperform the comparative method across all evaluated metrics, even under increased dynamic complexity associated with higher target speeds.\n4\nVision-based path tracking results\nThis section primarily presents the results of the robotâ€™s path tracking performance under the control of the Lyapunov-based controller. The objective is to evaluate the effectiveness and stability of the proposed control strategy in guiding the robot along the planned trajectory. By analyzing the tracking accuracy and overall motion behavior, the results serve to validate the controllerâ€™s ability to ensure smooth and reliable autonomous navigation under varying target speed conditions. Each subplot in Section 5.4, the blue curve corresponds to the proposed Lyapunov-based control method, while the red curve represents the results obtained using the comparative method.\nFigure 5.11 illustrates the experimental comparison of path tracking performance between the two control methods at\nv\nt\n=\n1.5\nâ€‹\nm\n/\ns\nv_{t}=1.5m/s\n. It is clearly observable that the trajectory of proposed Lyapunov-based controller, exhibits a smoother and more stable path compared to the comparative method. The robot following the proposed control strategy demonstrates reduced lateral oscillations and minimizes unnecessary rotational movements while approaching and tracking the target points. This improved behavior indicates enhanced control precision and better dynamic stability, thereby validating the effectiveness of the proposed method in achieving more efficient and reliable autonomous navigation.\nFigure 11:\nTrajectory results of the two controllers when the target speed\nv\nt\n=\n1.5\nâ€‹\nm\n/\ns\nv_{t}=1.5m/s\nFigure 5.12, from top to bottom, displays the time-series data of the robotâ€™s\nv\nv\n,\nÏ‰\n\\omega\n,\nx\nx\nalong the x-axis,\ny\ny\nalong the y-axis, and\nÏ•\n\\phi\nwhen\nv\nt\n=\n1.5\nâ€‹\nm\n/\ns\nv_{t}=1.5m/s\n. It can be observed that the proposed method achieves more stable control behavior, as reflected by smoother profiles in both linear and angular speeds. Furthermore, the frequency of triggering the lower bound of linear speed as well as the upper and lower bounds of acceleration is noticeably reduced compared to the comparative method. These limit-triggering events primarily occur when the robot passes through regions with frequent interruptions in visual cues, such as areas with concentrated dotted lines and zebra crossings in Figure 5.2, which lead to unstable target point inputs.\nFigure 12:\nResults with two controllers when the target speed\nv\nt\n=\n1.5\nâ€‹\nm\n/\ns\nv_{t}=1.5m/s\n, from top to bottom: linear speed, angular speed, x-axis position, y-axis position, and the orientation\nTable 5.4 presents a quantitative comparison between the proposed method and the comparative method across a range of evaluation metrics. The results clearly demonstrate that the proposed control strategy achieves superior performance in multiple aspects. Specifically, it achieves a slightly shorter completion time and a higher average linear speed , indicating improved efficiency. Moreover, the proposed method exhibits a significantly lower MAE in lateral positioning and orientation , reflecting enhanced tracking accuracy and directional stability. In terms of speed control, it also achieves lower RMSE of linear speed and reduced linear speed deviation with target speed, which indicates more stable velocity regulation. Finally, the accumulated orientation is lower, indicating that the proposed method enables smoother and more efficient path following. Collectively, these results confirm the effectiveness and robustness of the proposed control approach over the comparative method.\nTable 4:\nPerformance metrics comparison between proposed and comparative methods when the target speed\nv\nt\n=\n1.5\nâ€‹\nm\n/\ns\nv_{t}=1.5m/s\nProposed method\nComparative method\nCompletion time (s)\n206.54\n207.00\nAverage Linear speed (m/s)\n1.35\n1.31\nAverage angular speed (rad/s)\n0.037\n0.038\nMAE of lateral (m)\n0.51\n0.61\nMAE of orientation (rad)\n0.15\n0.23\nRMSE of linear speed (m/s)\n0.091\n0.13\nLinear speed deviation (%)\n10.00\n12.67\nAccumulated orientation (rad)\n20.36\n26.02\nFigure 5.13 presents an experimental comparison of the trajectory tracking performance of the two control strategies when\nv\nt\n=\n2.0\nâ€‹\nm\n/\ns\nv_{t}=2.0m/s\n. It is evident that the trajectory of the proposed Lyapunov-based controller continues to demonstrate a smoother and more stable path compared to the comparative method. The robot operating under the proposed control scheme exhibits reduced lateral oscillations and minimized unnecessary rotational motions while tracking the designated target points. Nevertheless, it is noted that the overall smoothness of the trajectory is slightly diminished relative to the results observed at lower speeds, reflecting the increased difficulty of maintaining optimal tracking performance as speed increases.\nFigure 13:\nTrajectory results of the two controllers when the target speed\nv\nt\n=\n2.0\nâ€‹\nm\n/\ns\nv_{t}=2.0m/s\nFigure 5.14, from top to bottom, displays the time-series data of the robotâ€™s\nv\nv\n,\nÏ‰\n\\omega\n,\nx\nx\nalong the x-axis,\ny\ny\nalong the y-axis, and\nÏ•\n\\phi\nwhen\nv\nt\n=\n2.0\nâ€‹\nm\n/\ns\nv_{t}=2.0m/s\n. It can be observed that the angular speed of the robot under the baseline control method exhibits a significantly wider range of variation, indicating less stable motion and poorer trajectory tracking performance. In contrast, the proposed Lyapunov-based controller demonstrates a more consistent and constrained angular velocity profile, reflecting its superior path tracking capability and enhanced stability during navigation. This reduction in angular spped fluctuations suggests that the proposed method enables smoother directional adjustments. However, it is also noted that the overall control performance is somewhat diminished compared to the results obtained at lower speeds. This performance degradation is likely attributable to the increased dynamic complexity and reduced response time associated with higher target velocities, which pose greater challenges for maintaining precise control.\nFigure 14:\nResults with two controllers when the target speed\nv\nt\n=\n2.0\nâ€‹\nm\n/\ns\nv_{t}=2.0m/s\n, from top to bottom: linear speed, angular speed, x-axis position, y-axis position, and the orientation\nTable 5.5 presents a comprehensive quantitative comparison between the proposed method and the comparative method across multiple evaluation metrics when\nv\nt\n=\n2.0\nâ€‹\nm\n/\ns\nv_{t}=2.0m/s\n. The proposed method demonstrates overall superior performance. It achieves a marginally shorter completion time and a higher average linear speed, indicating improved operational efficiency. Although the average angular speed values are similar, significant advantages are observed in tracking accuracy. Specifically, the proposed method yields a substantially MAE of lateral and MAE of orientation. Additionally, it exhibits a RMSE in linear speed and a smaller linear speed deviation, which reflect enhanced stability in velocity control. Furthermore, the accumulated orientation is also lower for the proposed method, suggesting smoother heading transitions throughout the trajectory. Collectively, these results underscore the effectiveness, accuracy, and robustness of the proposed Lyapunov-based control strategy under high-speed autonomous navigation conditions. However, it is important to note that, overall, all the aforementioned evaluation metrics exhibit a decline in performance when compared to those obtained under lower-speed conditions. As a result, while the proposed method still outperforms the comparative method at higher speeds, its absolute performance across metrics such as tracking accuracy, velocity stability, and orientation control is diminished relative to its performance in low-speed scenarios.\nTable 5:\nPerformance metrics comparison between proposed and comparative methods when the target speed\nv\nt\n=\n2.0\nâ€‹\nm\n/\ns\nv_{t}=2.0m/s\nProposed method\nComparative method\nCompletion time (s)\n165.25\n185.51\nAverage linear speed (m/s)\n1.70\n1.51\nAverage angular speed (rad/s)\n0.046\n0.037\nMAE of lateral (m)\n0.68\n0.99\nMAE of orientation (rad)\n0.20\n0.29\nRMSE of linear speed (m/s)\n0.51\n0.70\nLinear speed deviation (%)\n15.00\n23.50\nAccumulated orientation (rad)\n28.64\n31.61\nChapter 6\nConclusion\nThis study has integrate a vision-based autonomous navigation strategy for AMR operating in race tracks, without relying on HD maps or GNSS. The research was motivated by the limitations of conventional map-based navigation methods in dynamic, unstructured, or GPS-denied scenarios, aiming to develop a lightweight, robust, and real-time navigation framework suitable for practical deployment.\nThe proposed system is composed of three key modules: perception, path generation, and control. For perception, a deep learning-based model, YOLOP, was employed to extract lane line information from stereo camera inputs. To generate a navigable trajectory, polynomial fitting was applied to the detected lane lines to compute a smooth centerline in the robotâ€™s local coordinate system. A set of three look-ahead points was then selected from the fitted path to serve as references for the control module. The control strategy was based on Lyapunov stability theory, enabling robust and smooth trajectory tracking while accounting for system constraints on linear and angular velocities.\nExtensive real-world experiments were conducted using a Honda-provided AI Formula unmanned vehicle on a Formula One race track. The experiments included a variety of scenarios such as straight and curved paths, solid and dotted lane lines, and visually challenging environments like zebra crossings. The results demonstrated that the perception and fitting modules could generally provide accurate and stable inputs to the controller, especially within the critical 2â€“3 meter region ahead of the vehicle. The Lyapunov-based controller showed superior performance compared to a comparative method in terms of trajectory smoothness, error reduction, and velocity stability, particularly under low speed conditions.\nThis study evaluates the effectiveness of the proposed control strategy under a preset path tracking framework. In this setting, the robot was required to follow a preset path without relying on real-time perception input. Experimental results demonstrated that the proposed Lyapunov-based controller maintained reliable and smooth trajectory tracking throughout the entire course, even at higher target speeds. In contrast, the comparative method frequently deviated from the preset path, and in some cases, failed to complete the full trajectory due to control instability.\nIn addition to the preset path tracking experiments, this study also presents the effectiveness of the proposed control strategy under a vision-based tracking framework. Comparisons confirmed that the proposed method achieved lower MAE in lateral and orientation tracking, reduced velocity fluctuations, and more stable angular motion. The proposed method consistently outperformed the baseline in terms of tracking accuracy, velocity consistency, and heading stability, confirming its robustness and adaptability not only in preset path tracking but also in scenarios perception-driven navigation.However, it was also observed that system performance declined slightly at higher target speeds, due to increased sensitivity to visual disturbances and dynamic response limitations. For instance, under the vision-based navigation framework, when the target speed increased from\n1.5\nâ€‹\nm/s\n1.5\\,\\text{m/s}\nto\n2.0\nâ€‹\nm/s\n2.0\\,\\text{m/s}\n, the mean absolute lateral error increased from\n0.20\nâ€‹\nm\n0.20\\,\\text{m}\nto\n0.26\nâ€‹\nm\n0.26\\,\\text{m}\n, and the mean absolute orientation error rose from\n0.069\nâ€‹\nrad\n0.069\\,\\text{rad}\nto\n0.079\nâ€‹\nrad\n0.079\\,\\text{rad}\n. Similarly, in the preset path tracking experiments, the accumulated orientation increased from\n7.43\nâ€‹\nrad\n7.43\\,\\text{rad}\nto\n7.83\nâ€‹\nrad\n7.83\\,\\text{rad}\n, and the RMSE of linear speed slightly increased from\n0.014\nâ€‹\nm/s\n0.014\\,\\text{m/s}\nto\n0.013\nâ€‹\nm/s\n0.013\\,\\text{m/s}\n, although still remaining at a low level. Despite these challenges, the overall framework remains effective and adaptable, offering a promising alternative for autonomous navigation in real-world scenarios where reliance on external localization infrastructure is infeasible. This further underscores the controllerâ€™s versatility and its suitability for a wide range of autonomous navigation applications.\nIn future work, we plan to incorporate optimization algorithms to dynamically adjust the controllerâ€™s preset parameters according to different operating speeds. The goal is to enhance the adaptability of the control system, allowing the robot to maintain a stable and accurate tracking performance across a wider range of speed conditions. Moreover, to address more complex real-world scenarios anticipated in future competitionsâ€”such as navigating across zebra crossings and avoiding both static and dynamic obstaclesâ€”it will be necessary to introduce additional perception capabilities. In particular, advanced deep learning-based object detection and semantic segmentation methods will be explored to enhance the robotâ€™s understanding of its environment.\nAcknowledgments\nFirst of all, I would like to express my sincerest gratitude to my supervisor, Professor Cao Wenjing. Professor Cao has always led me forward on the academic path with her keen academic insight. Whenever I was confused or encountered a problem, her timely and profound guidance made me suddenly enlightened and solved the problem. Professor Caoâ€™s profound attainments in the field of control theory have benefited me a lot. She made precise and forward-looking suggestions for my research in group meetings and seminars, laying a solid foundation for the successful completion of this thesis. Moreover, thanks to Professor Takashi Suzuki and Professor Edyta Dzieminska for their diligent participation in my thesis review and defense, offering crucial insights that enhanced my work.\nSecondly, I would like to express my sincere gratitude to the students in the robotics group of our laboratory. We inspired each other in discussions, worked side by side in experiments, and reaped results in code modification and debugging. It is this unity and collaboration that has enabled the robot field project to proceed smoothly and made our academic exploration more meaningful.\nAt the same time, I would like to thank all the members of the Control Lab. At every critical point in the research, everyone generously shared their experiences, made valuable suggestions, and encouraged me when I faced difficulties, which kept me motivated to move forward.\nFinally, I would like to thank my parents. Your understanding and support are the most steadfast backing on my road to study, and are also the source of my constant pursuit of excellence and progress.\nThese experiences will become the most valuable assets in my academic journey. In the future, I will take this as an encouragement to continue exploring in the field I love and constantly surpass myself.",
    "preview_text": "This work presents a real-time autonomous track navigation framework for nonholonomic differential-drive mobile robots by jointly integrating multi-task visual perception and a provably stable tracking controller. The perception pipeline reconstructs lane centerlines using 2D-to-3D camera projection, arc-length based uniform point resampling, and cubic polynomial fitting solved via robust QR least-squares optimization. The controller regulates robot linear and angular velocities through a Lyapunov-stability grounded design, ensuring bounded error dynamics and asymptotic convergence of position and heading deviations even in dynamic and partially perceived lane scenarios, without relying on HD prior maps or global satellite localization. Real-world experiments on embedded platforms verify system fidelity, real-time execution, trajectory smoothness, and closed-loop stability for reliable autonomous navigation.\n\n2025 Year â€‰ Master Thesis\nIntegrated YOLOP Perception and Lyapunov-based Control for Autonomous Mobile Robot Navigation on Track\nGraduate School of Science and Technology\nMasterâ€™s Program in Science and Technology\nGreen Science and Engineering Division\nB2378474\nMO CHEN\nSupervisor â€ƒâ€†Wenjing Cao\n\\newdateformat\nmyformat\n\\THEYEAR\n-\n\\twodigit\n\\THEMONTH\n-\n\\twodigit\n\\THEDAY\n\\myformat\nDecember 1, 2025\nContents\n1\nIntroduction\n1\nResearch background\n2\nResearch object\n3\nResearch purpose\n4\nThesis Organization\n2\nBasic Theory and Methods\n1\nYOLOP\n2\nPolynomial curve fitting\n3\nLyapunov stability\n3\nModeling\n1\nRobot kinematic modeling\n2\nGeometric path kinematic modeling\n4\nRobot Autonomous Navigation Algorithms\n1\nYOLOP\n2\nPolynomial curve fitting\n3\nLyapunov-based control\n4\nConventional Lyapunov-based controllers used for comparison\n5\nImplementation process of autonomous navigation\n5\nExperiment Results and Discussion\n1\nExperimental scenario\n2\nPerception results\n3\nPreset path based tracking results\n4\nVision-based path tracking results\n6\nConclusion\nChapter 1\nIntroduction\n1\nResearch bac",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "visual perception",
        "robot navigation",
        "real-time system",
        "Lyapunov stability",
        "polynomial curve fitting",
        "YOLOP",
        "embedded implementation"
    ],
    "one_line_summary": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰æ„ŸçŸ¥ä¸æé›…æ™®è¯ºå¤«ç¨³å®šæ§åˆ¶çš„ç§»åŠ¨æœºå™¨äººèµ›é“è‡ªä¸»å¯¼èˆªæ¡†æ¶ï¼Œæœªæ¶‰åŠç”Ÿæˆæ¨¡å‹æˆ–å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T12:29:02Z",
    "created_at": "2026-01-09T11:27:08.825332",
    "updated_at": "2026-01-09T11:27:08.825341",
    "flag": true
}