{
  "id": "2601.08665v1",
  "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
  "authors": [
    "Shaoan Wang",
    "Yuanfei Luo",
    "Xingyu Chen",
    "Aocheng Luo",
    "Dongyue Li",
    "Chang Liu",
    "Sheng Chen",
    "Yangang Zhang",
    "Junzhi Yu"
  ],
  "abstract": "VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.",
  "url": "https://arxiv.org/abs/2601.08665v1",
  "html_url": "https://arxiv.org/html/2601.08665v1",
  "html_content": "1]ByteDance Seed\n2]Peking University\n3]Zhongguancun Academy\n\\contribution\n[*]Co-first authors\n\\contribution\n[â€ ]Corresponding authors\n\\contribution\n[â€¡]Project lead\nVLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory\nShaoan Wang\n1,2,âˆ—\nYuanfei Luo\n1,âˆ—\nXingyu Chen\n2,3,â€ \nAocheng Luo\n2\nDongyue Li\n2\nChang Liu\n2\nSheng Chen\n1,â€¡\nYangang Zhang\n1\nJunzhi Yu\n2,â€ \n[\n[\n[\nwangshaoan@stu.pku.edu.cn\nluoyuanfei@bytedance.com\n(\nJanuary 13, 2026\n)\nAbstract\nVision-Language-Action (VLA) models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large Vision-Language Models (VLMs). However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought (AdaCoT) mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module (VLingMem) that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments.\nFor training, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, successfully executing practical navigation tasks, including previously unseen and untrained tasks, and demonstrating strong cross-domain and cross-task generalization.\n\\correspondence\n,\n\\checkdata\n[Project Page]\nhttps://wsakobe.github.io/VLingNav-web/\nFigure 1\n:\nOverview of VLingNav.\nVLingNav is a VLA model enhanced with adaptive CoT reasoning and visual-assisted linguistic memory. This architecture allows the model to leverage historical visual and linguistic memory, achieving SOTA results on several embodied navigation benchmarks. Furthermore, VLingNav can be deployed zero-shot on real-world robots to perform diverse and complex navigation tasks.\n1\nIntroduction\nEmbodied navigation\n[\n57\n]\nis a fundamental capability for intelligent robots, enabling purposeful movement through previously unseen, structurally complex environments in response to human instructions. As robots are increasingly deployed in open-world settings from household service scenarios to industrial inspection, navigation systems must deliver accurate perception and decision-making while robustly generalizing to novel scenes and tasks.\nTraditional modular approaches\n[\n8\n,\n65\n,\n9\n]\ndecompose navigation into submodules (e.g., perception, mapping, planning) by leveraging mature techniques such as visual foundation models\n[\n44\n,\n32\n]\n, SLAM\n[\n38\n,\n58\n]\n, and path-planning algorithms\n[\n21\n]\n. However, these pipelines require manually defined module interfaces; over-reliance on hand-crafted rules compromises robustness and induces error accumulation, limiting adaptability in dynamically complex environments. Recent advances in large-scale visionâ€“languageâ€“action (VLA) models have made compelling progress toward this goal. By unifying multimodal scene understanding with language-conditioned action generation, VLA-based agents substantially improve the adaptability and expressiveness of embodied navigation systems.\nDespite this progress, current VLA models are reactive systems, often lacking the explicit reasoning mechanisms, memory structures, and interpretability that are important for reliable real-world deployment.\nMost existing models operate under a fixed inference budget, producing actions with a predetermined amount of computation, and therefore cannot increase deliberation when faced with ambiguity.\nIn addition, these models often lack persistent semantic memory, relying solely on limited context windows. Without a mechanism to retain historical context, agents struggle to track their progress over extended trajectories, resulting in redundant exploration, looping behaviors, and poor adaptation to dynamic changes in the environment.\nAddressing these limitations requires rethinking VLA architectures from a linguistic perspective, moving beyond passive perception-action mapping toward active reasoning, memory construction, and interpretable decision-making. Motivated by principles from cognitive science and human problem solving, we argue that effective embodied navigation demands two missing capabilities:\n1) adaptive reasoning, enabling the agent to adjust the granularity of its internal deliberation according to task complexity; and\n2) linguistically grounded long-term memory, providing stable cross-modal semantics that support consistent and context-aware navigation behavior.\nFurthermore, most current VLA training paradigms rely on supervised fine-tuning (SFT) via imitation learning. However, this approach often limits generalization, preventing models from performing beyond expert demonstrations. While post-training paradigms rooted in reinforcement learning (RL) have proven effective for enhancing LLMs and VLMs on complex tasks\n[\n16\n,\n47\n,\n13\n]\n, their application in embodied navigation remains preliminary\n[\n14\n,\n31\n,\n82\n]\n. Notably, existing efforts typically focus on autoregressive RL in discrete space, leaving the exploration of RL for refining continuous control policies an open area for further investigation.\nIn this work, we present VLingNav, a linguistic-driven VLA framework designed to endow embodied agents with cognitive abilities through two core components. First, inspired by the fast-and-slow thinking paradigm, we introduce an Adaptive Chain-of-Thought (AdaCoT) mechanism. AdaCoT dynamically triggers explicit reasoning only when necessary, allowing the agent to efficiently switch between fast reactive execution and deliberate planning depending on the situation. Second, to handle long-term spatial dependencies, we develop a Visual-assisted Linguistic Memory module (VLingMem). By constructing a persistent cross-modal memory, VLingMem enables the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic tasks, thereby ensuring coherent decision-making over extended interactions.\nTo support the training of such cognitively enriched VLA models, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, incorporating adaptive CoT annotations that teach the model when to think and what to think about. Beyond imitation learning, we further employ online expert-guided RL for post-training, enabling VLingNav to acquire self-improving navigation behaviors that surpass the limitations of supervised demonstrations.\nExtensive experiments across diverse embodied navigation benchmarks show that VLingNav achieves state-of-the-art performance, outperforming existing VLA-based agents in both success rate and efficiency metrics. Notably, VLingNav transfers to real-world robots in a zero-shot manner, successfully executing novel navigation tasks in the real world without any additional fine-tuning. These results highlight the strong generalization ability of linguistic-driven cognition and demonstrate the promise of integrating adaptive reasoning and persistent memory into VLA models for embodied navigation. The contributions are as follows:\nâ€¢\nWe propose VLingNav, a novel framework integrating Adaptive Chain-of-Thought (AdaCoT) and Visual-Assisted Linguistic Memory (VLingMem). AdaCoT enables the agent to dynamically switch between fast execution and slow deliberation based on task complexity, while VLingMem eliminates redundant exploration and infers movement trends through persistent cross-modal storage.\nâ€¢\nWe construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce flexible reasoning patterns. We further introduce an online expert-guided RL post-training stage, empowering the model to surpass the limitations of imitation learning and acquire more robust, self-optimized navigation behaviors.\nâ€¢\nWe conduct extensive experiments across standard embodied navigation benchmarks, demonstrating that VLingNav achieves state-of-the-art performance, with significant gains in long-horizon reasoning and success rate. Moreover, VLingNav exhibits remarkable zero-shot transfer to real-world robot platforms, successfully executing unseen tasks and illustrating strong cross-domain and cross-task generalization.\n2\nRelated Works\n2.1\nEmbodied Navigation Models\nAs a core task in robotics, navigation has long attracted significant attention from robotics researchers\n[\n11\n]\n. With the rise of embodied AI in recent years, robot navigation has gradually shifted from classical point-to-point navigation\n[\n22\n]\nto more intelligent embodied navigation. Embodied navigation includes subtasks such as vision-language navigation (VLN)\n[\n73\n,\n75\n,\n56\n,\n10\n,\n70\n]\n, object goal navigation (ObjectNav)\n[\n42\n,\n62\n,\n60\n]\n, image goal navigation (ImageNav)\n[\n25\n,\n59\n,\n63\n]\n, and embodied visual tracking (EVT)\n[\n52\n,\n84\n,\n30\n]\n, emphasizing that robots follow natural language instructions to perceive, reason, and plan in unseen environments.\nEmbodied navigation methods can be broadly categorized into modular and end-to-end approaches. The modular paradigm relies on well-established components such as off-the-shelf large models\n[\n85\n,\n86\n]\n, SLAM\n[\n38\n,\n5\n]\n, vision foundation models\n[\n71\n,\n39\n]\n, and planning algorithms\n[\n21\n]\n. It decomposes the navigation task into distinct modules (\ne\n.\ng\n., perception, localization, planning) and aligns them via manually defined interfaces. This design yields high interpretability and strong zero-shot transfer\n[\n63\n]\n. However, integrating multiple modules inevitably incurs information loss\n[\n34\n]\n; moreover, tight coupling across modules increases system fragility\n[\n34\n]\n.\nEnd-to-end approaches leverage data-driven learning to directly map sensor inputs to robot actions\n[\n59\n,\n66\n,\n69\n]\n. By removing manually designed interfaces and mitigating information loss, these methods have achieved notable progress\n[\n43\n,\n42\n]\n. However, they exhibit limited generalization and can produce abnormal actions under out-of-distribution conditions. With the rapid advancement of large models in recent years, an increasing number of studies have adopted pre-trained VLMs as the backbone to enhance generalization, environmental perception, and spatial understanding.\nNaVid\n[\n73\n]\nrepresents the first embodied navigation VLA model. It designs a video-based VLM and finetunes on VLN datasets, demonstrating robust generalization capabilities. However, its inference time increases significantly with longer video streams, making real-world deployment challenging.\nBuilding upon NaVid, Uni-NaVid\n[\n75\n]\nintroduces a video-stream compression mechanism to control the number of visual tokens. Moreover, Uni-NaVid extends the model to multiple categories of embodied navigation tasks, achieving state-of-the-art performance across diverse benchmarks. Similarly, NaVILA\n[\n10\n]\nand StreamVLN\n[\n56\n]\nadopt similar architectures; they further incorporate large-scale open-world navigation data and leverage KV cache to jointly improve both generalization and inference speed.\nJanusVLN\n[\n70\n]\nenhances 3D understanding by fusing spatial features produced by VGGT\n[\n51\n]\n, thereby exhibiting strong instruction-following performance. Notably, all the aforementioned works represent robot actions as discrete tokens. This simplification leads to inefficient action quality and weak adaptability in dynamic scenarios.\nTo address this limitation, TrackVLA\n[\n52\n]\ndesigns an anchor-based diffusion policy that directly outputs the robotâ€™s motion trajectory, substantially improving both action quality and efficiency. NavFoM\n[\n74\n]\nfurther extends the model by introducing TVI tokens, enabling inputs from cross-embodiment navigation data.\nNevertheless, existing navigation VLA models rely solely on action labels for finetuning and thus fail to exploit the inherent reasoning capabilities of VLMs\n[\n2\n,\n13\n]\n. In addition, they maintain history only through implicit visual features, without explicit memory, which ultimately prevents fully unlocking the potential of the VLM backbone.\n2.2\nEmbodied Chain-of-Thought\nWith the chain-of-thought significantly enhancing the performance of LLM and VLM on complex tasks\n[\n55\n,\n54\n,\n20\n]\n, several studies have attempted to extend this paradigm to embodied tasks. By explicitly outputting the reasoning process before a robot executes actions, the inherent reasoning capabilities of the VLM can be better leveraged. This approach aims to enhance the modelâ€™s competencies in task decomposition, environmental perception, and decision-making, ultimately improving the accuracy and quality of the actions generated by the model, as well as its generalization ability and performance in real-world scenarios. Embodied-CoT\n[\n68\n]\nfirst utilizes structured textual instructions enriched with spatial localization information. CoT-VLA\n[\n83\n]\nand VPP\n[\n19\n,\n72\n]\nintegrate reasoning via future image prediction.\nÏ€\n0.5\n\\pi_{0.5}\n[\n3\n]\nperforms task decomposition and reasoning through text.\nChatVLA-2\n[\n87\n]\nenhances the modelâ€™s performance in complex visual reasoning tasks by introducing additional open-world visual reasoning pre-training data. ThinkAct\n[\n20\n]\ndesigns a dual-system framework that bridges high-level reasoning with low-level action. However, the aforementioned methods are limited to tabletop manipulation tasks and have not been extended to navigation in open spaces. OctoNav\n[\n14\n]\nimproves the modelâ€™s performance in navigation tasks and enhances interpretability by executing CoT at fixed frequency. However, the requirement for manual configuration of the CoT frequency impedes the full exploitation of CoTâ€™s potential. Aux-Think\n[\n53\n]\nconstructs a VLN dataset with CoT labeling, and experiments show that using CoT as an auxiliary task during training enhances the modelâ€™s navigation performance, while excessive reasoning affects the modelâ€™s efficiency and performance.\nN\nâ€‹\na\nâ€‹\nv\nâ€‹\nA\n3\nNavA^{3}\n[\n76\n]\nadopts GPT-4o as the reasoning-VLM for task decomposition and 3D spatial localization, but it suffers from long reasoning latency, making it difficult to deploy on real robots.\nIn contrast to previous work, we propose an adaptive thinking strategy that balances reasoning efficiency with navigation capability.\n2.3\nMemory in VLA Models\nFor long-horizon embodied tasks, VLA models must possess robust memory capabilities. RoboFlamingo\n[\n27\n]\n, for instance, compresses visionâ€“language representations into latent tokens and propagates them through a Long Short-Term Memory (LSTM) network. However, the resulting latent representations are relatively coarse-grained, leading to a significant loss of fine-grained perceptual history.\nIn contrast, MemoryVLA\n[\n48\n]\nintegrates high-level cognitive semantics and fine-grained perceptual details within a unified memory framework, enabling effective temporal modeling for long-horizon manipulation tasks. However, it only employs a single implicit cognitive token to serve as the semantic memory, failing to fully leverage the reasoning capabilities of LLM.\nOn the navigation side, video-based VLA models\n[\n73\n,\n75\n,\n10\n,\n56\n,\n52\n,\n30\n]\ncommonly encode historical image observations as inputs to provide implicit visual memory. However, such implicit memory can hinder learning to focus on key regions, and semantic information is further degraded as visual features are repeatedly compressed.\nFinally, Mem2Ego\n[\n78\n]\nand MapNav\n[\n77\n]\nincorporate global map information into VLA models as memory components. Yet current VLM backbones lack native support for map-format inputs, and the representation design of maps for VLAs remains under-explored.\nCompared with latent-, vision-, or map-based memories, language memory is better aligned with the VLA framework, thanks to large-scale language pretraining. Therefore, we design the memory module from a linguistic perspective and use visual features as auxiliary signals.\n2.4\nPost-training for VLA Models\nReinforcement Learning enhances the exploration capability of large models, unlocks their reasoning potential, and shows promise for mitigating issues such as covariate shift and causal confusion induced by imitation learning. Notably, OctoNav\n[\n14\n]\n, VLN-R1\n[\n37\n]\n, and Nav-R1\n[\n31\n]\nhave convergently integrated GRPO\n[\n47\n]\ninto navigation VLA models, enabling the simultaneous optimization of CoT outputs and actions.\nFigure 2\n:\nThe overall framework of VLingNav.\nThe framework takes video streams and multimodal instruction as input to produce robot action for navigation with tailored linguistic designs. AdaCoT can adaptively generate linguistic thinking according to its observation, while VLingMem summarizes CoT cues with key visual features for globally informed decision-making.\nRecent advances in large reasoning models (\ne\n.\ng\n.,\nDeepSeek-R1\n[\n16\n]\n) show that RL can drive remarkable progress even when relying solely on outcome-based rewards. Several studies have also attempted to leverage outcome-based rewards for the RL post-training of VLA models. For instance, SimpleVLA-RL\n[\n26\n]\npioneers outcome-based rewards for the RL post-training of OpenVLA-OFT\n[\n23\n]\n, achieving a substantial improvement in success rate on the manipulation benchmarks. ActiveVLN\n[\n82\n]\n, caches all historical actions and states into model tokens and leverages GRPO to implement outcome-based RL through this mechanism.\nThe aforementioned work remains confined to autoregressive action outputs, failing to support more advanced continuous action prediction. Recently, ReinFlow\n[\n80\n]\naddresses this by formulating flow matching as an MDP, enabling RL training via PPO\n[\n45\n]\nor GRPO.\nExisting VLA-RL frameworks either adopt discrete autoregressive action with limited policy space or continual flow-based action with slow inference speed. We adopt an MLP-based continuous action to overcome the above drawbacks. In addition, we introduce prior expert knowledge into the RL framework to improve online learning efficiency and performance.\n3\nMethodology\n3.1\nNavigation Task Definitions\nEmbodied navigation tasks can be defined as follows: a mobile robot is provided with an instruction\nâ„\n\\mathcal{I}\nand a sequence of visual observations\nğ’ª\n1\n:\nt\nâˆˆ\nâ„\nW\nÃ—\nH\nÃ—\n3\n\\mathcal{O}_{1:t}\\in\\mathbb{R}^{W\\times H\\times 3}\n, captured by the egocentric camera mounted on the robot at each time step\n{\n1\n,\nâ€¦\n,\nt\n}\n\\{1,...,t\\}\n. Given these observations and the instruction, the policy model\nÏ€\n\\pi\nis required to output the next action\na\nt\nâˆˆ\nğ”¸\n=\n{\nv\n,\nÏ‰\n}\na_{t}\\in\\mathbb{A}=\\{v,\\omega\\}\nfor the robot. The robot accomplishes the navigation task by executing the action predicted by the model, which can be formulated as\na\nt\n=\nÏ€\nâ€‹\n(\nâ„\n,\nğ’ª\n1\n:\nt\n)\na_{t}=\\pi(\\mathcal{I},\\mathcal{O}_{1:t})\n. VLingNav is capable of performing multiple embodied navigation tasks, including ObjectNav, EVT, and ImageNav. ObjectNav requires the robot to explore unseen environments given a textual description of an object category and to locate an object that matches the specified goal. EVT focuses on identifying the correct target described by textual instructions in dynamic, crowded scenarios and maintaining continuous tracking of the moving target. ImageNav is analogous to ObjectNav, with the key difference that the goal is specified by an image rather than text. Similarly, the robot must explore unseen environments and find the location corresponding to the image goal.\n3.2\nVLingNav Overview\nVLingNav extends a video-based VLM, specifically LLaVA-Video-7B\n[\n81\n]\n, and integrates an action model to enable simultaneous text token generation and trajectory planning. For text token prediction, the model follows a conventional autoregressive paradigm. For trajectory planning, the action model conditions on the VLM backboneâ€™s outputs to predict a motion trajectory\nÏ„\n=\n{\na\n1\n,\na\n2\n,\nâ€¦\nâ€‹\na\nn\n}\n\\tau=\\{a_{1},a_{2},\\dots\\,a_{n}\\}\n, where\nn\nn\nis the trajectory horizon and each\na\nâˆˆ\nâ„\n3\n=\n(\nx\n,\ny\n,\nÎ¸\n)\na\\in\\mathbb{R}^{3}=(x,y,\\theta)\ndenotes a waypoint that encapsulates both position and orientation.\n3.3\nVLingNav Architecture\n3.3.1\nObservation Encoding\nFor the video-based VLA model, the number of image frames grows over time during online inference. This substantially increases computational burden, making it difficult to ensure inference efficiency when deploying on real robots. Moreover, for low-speed mobile robots, adjacent egocentric frames captured at high FPS contain substantial redundant visual information.\nPrior studies explore two main strategies to mitigate this issue. One merges visual tokens from historical frames to reduce redundancy among adjacent frames\n[\n75\n,\n4\n]\n; however, this operation often distorts original semantic features and introduces additional computation. The other uniformly samples the video stream to reduce frame count\n[\n10\n]\n, which inevitably causes delayed and inaccurate decisions due to insufficient short-term observations at low sampling rates.\nTo address the limitations of these two approaches, we propose a dynamic FPS sampling strategy. Inspired by the Ebbinghaus forgetting curve\n[\n12\n]\n, historical frames are sampled according to their time intervals relative to the current frame. Specifically, older historical frames, regarded as long-term memory, are sampled at a lower rate to simulate the forgetting process. In contrast, recent historical frames, considered as short-term memory, are sampled at a guaranteed higher rate. The relationship between the sampling rate and the time interval approximately satisfies the following:\nf\ns\nâ€‹\n(\ni\n)\n=\nf\ns\nm\nâ€‹\na\nâ€‹\nx\nâ€‹\ne\nâˆ’\nÎ”\nâ€‹\nT\ns\nf_{s}(i)=f_{s}^{max}e^{-\\frac{\\Delta T}{s}}\n(1)\nwhere\nf\ns\nf_{s}\ndenotes the sampling rate,\nf\ns\nm\nâ€‹\na\nâ€‹\nx\nf_{s}^{max}\nrepresents the maximum sampling rate,\nÎ”\nâ€‹\nT\n=\nt\nâˆ’\ni\n\\Delta T=t-i\nstands for the time interval from latest frame\nt\nt\nto frame\ni\ni\n, and\ns\ns\nsignifies the stability of memory. Through this approach, we can control the number of input image tokens while selectively preserving more important images.\nAfter sampling the input visual observations, we need to encode and map the visual observations into the latent space of the VLM backbone. Following LLaVA-Video, we employ a pre-trained vision encoder (SigLIP-400M\n[\n71\n]\n), to encode the input egocentric video stream\nğ’ª\n1\n:\nt\n=\n{\nğ¨\n1\n,\nâ‹¯\n,\nğ¨\nt\n}\n\\mathcal{O}_{1:t}=\\{\\mathbf{o}_{1},\\cdots,\\mathbf{o}_{t}\\}\nof the robot. This encoding process yields visual features\nğ•\n1\n:\nt\nâˆˆ\nâ„\nN\nÃ—\nC\n\\mathbf{V}_{1:t}\\in\\mathbb{R}^{N\\times C}\n, where\nN\nN\nstands for the number of image patches (\nN\n=\n729\nN=729\n) and\nC\nC\ndenotes the embedding dimension (\nC\n=\n1152\nC=1152\n). To efficiently summarize historical visual information, we process past observations using a grid pooling strategy. This approach downsamples the feature maps of historical observations, enabling the model to capture high-level semantic features while effectively controlling computational costs. Similar to dynamic FPS, we also determine the downsampling ratio for grid pooling based on time intervals. The specific operation is defined as follows:\ng\nâ€‹\n(\ni\n)\n\\displaystyle g(i)\n=\nâŒŠ\ne\nâˆ’\nÎ”\nâ€‹\nT\ng\nâŒ‹\n\\displaystyle=\\lfloor e^{-\\frac{\\Delta T}{g}}\\rfloor\n(2)\nğ•\nt\ni\nâ€²\n\\displaystyle\\mathbf{V}^{\\prime}_{t_{i}}\n=\nğ’¢\nâ€‹\n(\nğ•\nt\ni\n,\ng\nâ€‹\n(\ni\n)\n)\n\\displaystyle=\\mathcal{G}(\\mathbf{V}_{t_{i}},g(i))\n(3)\nwhere\nğ•\nt\ni\nâ€²\n\\mathbf{V}^{\\prime}_{t_{i}}\nis the\ni\ni\n-th visual feature after grid pooling operation\nğ’¢\nâ€‹\n(\nâ‹…\n)\n\\mathcal{G(\\cdot)}\nwith stride\ng\nâ€‹\n(\ni\n)\ng(i)\n.\nFurthermore, to eliminate the temporal inconsistency in the video stream caused by dynamic FPS sampling, we incorporate timestamp information for each frame within the visual observations. Specifically, a temporal-aware indicator token\nE\nT\nâ€‹\n(\nâ‹…\n)\nâˆˆ\nâ„\nC\nE^{T}(\\cdot)\\in\\mathbb{R}^{C}\nis introduced prior to each frame, which can reflect the time interval between a given historical visual observation and the current observation. By encoding timestamp information using Rotary Position Embedding (RoPE)\n[\n49\n]\n,\nE\nT\nE^{T}\nenables the model to perceive the absolute time interval between different historical frames and the current frame. It can be expressed as follows:\nE\nT\nâ€‹\n(\nÎ”\nâ€‹\nT\n)\n=\nE\nb\nâ€‹\na\nâ€‹\ns\nâ€‹\ne\nT\n+\nR\nâ€‹\no\nâ€‹\nP\nâ€‹\nE\nâ€‹\n(\nÎ”\nâ€‹\nT\n)\nE^{T}(\\Delta T)=E^{T}_{base}+RoPE(\\Delta T)\n(4)\nFor the projection of visual features, we follow the well-established framework of VLMs\n[\n29\n]\n. Specifically, a cross-modality projector based on a two-layer Multi-Layer Perceptron (MLP)\nğ’«\nâ€‹\n(\nâ‹…\n)\n\\mathcal{P}(\\cdot)\nis employed to map the visual features\nğ•\n\\mathbf{V}\ninto the latent space of the VLM, yielding the projection result as\nğ„\nt\nV\n=\nğ’«\nâ€‹\n(\nğ•\nt\nâ€²\n)\n\\mathbf{E}^{V}_{t}=\\mathcal{P}(\\mathbf{V}_{t}^{{}^{\\prime}})\n, where\nğ„\nt\nV\n\\mathbf{E}^{V}_{t}\nrepresents the projected visual token.\n3.3.2\nAdaptive CoT & Visual-Assisted Linguistic Memory\nAs illustrated in Fig.\n2\n, we concatenate the visual tokens\nğ„\nt\nV\n\\mathbf{E}_{t}^{V}\nwith the language tokens\nğ„\nI\n\\mathbf{E}^{I}\nand the temporal-aware indicator tokens\nğ„\nT\n\\mathbf{E}^{T}\nto form the input sequence of the VLM. To balance the modelâ€™s inference performance and efficiency, we train the model using a large scale high-quality adaptive CoT data (detailed in Sec.\n4\n) endowing it with the ability to autonomously decide whether to perform CoT reasoning for a given input. Specifically, for the current input, the VLM first predicts a CoT indicator token (\n<think_on>\nor\n<think_off>\n). Upon outputting\n<think_on>\n, the model generates the specific content of CoT in an autoregressive manner, which consists of two components:\nâ€¢\nThe reasoning content, enclosed within\n<think>\nand\n</think>\ntokens. This content includes perception of the visual observation, task decomposition and analysis, assessment of whether the current location has been visited, and determination of the next action.\nâ€¢\nThe environmental summary of the current observation, enclosed within\n<summary>\nand\n</summary>\ntokens. This summary is incorporated into subsequent inputs as linguistic memory.\n3.3.3\nAction Model\nTo transfer the reasoning and decision-making knowledge of the VLM backbone into the robot-specific action space, we integrate an MLP-based action model\nğ’œ\nÎ¸\nâ€‹\n(\nâ‹…\n)\n\\mathcal{A}_{\\theta}(\\cdot)\ninto VLingNav. Specifically, the hidden state vector\nh\nt\np\nâ€‹\nr\nâ€‹\ne\nâ€‹\nd\n\\textbf{h}_{t}^{pred}\ncorresponding to the final token predicted by the VLM backbone is used as the condition to guide the action model in converting this representation into robot motion trajectory\nÏ„\n\\tau\n, which can be formulated as:\nÏ„\n^\nt\n=\nğ’œ\nÎ¸\nâ€‹\n(\nh\nt\np\nâ€‹\nr\nâ€‹\ne\nâ€‹\nd\n)\n\\hat{\\mathbf{\\tau}}_{t}=\\mathcal{A}_{\\theta}\\left(\\textbf{h}_{t}^{pred}\\right)\n(5)\nwhere\nÏ„\n^\nt\n\\hat{\\mathbf{\\tau}}_{t}\nis the predicted motion trajectory in current timestamp\nt\nt\n. The pseudocode presented in Alg.\n1\nillustrates the complete online inference process of VLingNav in detail.\nAlgorithm 1\nVLingNav Online Inference\n1:\nInput:\nObservation video stream\nğ’ª\n=\n{\nğ¨\n1\n,\nğ¨\n2\n,\nâ€¦\n,\nğ¨\nt\n}\n\\mathcal{O}=\\{\\mathbf{o}_{1},\\mathbf{o}_{2},\\dots,\\mathbf{o}_{t}\\}\n, Instruction\nI\nI\n2:\nInitialize:\nMemory\nâ„³\nâ†\nâˆ…\n\\mathcal{M}\\leftarrow\\emptyset\n, Visual Cache\nğ’±\nâ†\nâˆ…\n\\mathcal{V}\\leftarrow\\emptyset\n3:\nprocedure\nOnlineInference\n(\nI\n,\nğ’ª\nI,\\mathcal{O}\n)\n4:\nwhile\ntrue\ndo\n5:\nğ„\nI\nâ†\nTokenizer\nâ€‹\n(\nI\n)\n\\mathbf{E}^{I}\\leftarrow\\text{Tokenizer}(I)\n6:\nğ¯\nt\nâ†\nVisionEncoder\nâ€‹\n(\nğ¨\nt\n)\n\\mathbf{v}_{t}\\leftarrow\\text{VisionEncoder}(\\mathbf{o}_{t})\nâŠ³\n\\triangleright\nEncode the current visual frame\n7:\nğ’±\nâ†\nCache\nâ€‹\n(\nğ’±\n,\nğ¯\nt\n)\n\\mathcal{V}\\leftarrow\\text{Cache}(\\mathcal{V},\\mathbf{v}_{t})\nâŠ³\n\\triangleright\nUpdate visual cache with the new feature\n8:\nğ„\nV\nâ†\nSampling&Pooling\nâ€‹\n(\nğ’±\n)\n\\mathbf{E}^{V}\\leftarrow\\text{Sampling\\&Pooling}(\\mathcal{V})\nâŠ³\n\\triangleright\nObtain visual tokens from cache\n9:\nğ„\nT\nâ†\nRoPE\nâ€‹\n(\nÎ”\nâ€‹\nt\n)\n\\mathbf{E}^{T}\\leftarrow\\text{RoPE}(\\Delta t)\nâŠ³\n\\triangleright\nCreate temporal-aware indicator token\n10:\nğ„\nM\nâ†\nTokenizer\nâ€‹\n(\nâ„³\n)\n\\mathbf{E}^{M}\\leftarrow\\text{Tokenizer}(\\mathcal{M})\n11:\nğ„\nCoT\nâ†\nLLM.forward\nâ€‹\n(\nğ„\nI\n,\nğ„\nT\n,\nğ„\nV\n,\nğ„\nM\n)\n\\mathbf{E}^{\\text{CoT}}\\leftarrow\\text{LLM.forward}(\\mathbf{E}^{I},\\mathbf{E}^{T},\\mathbf{E}^{V},\\mathbf{E}^{M})\n12:\nif\nğ„\nCoT\n=\n<think_on>\n\\mathbf{E}^{\\text{CoT}}=\\texttt{<think\\_on>}\nthen\n13:\nc\nt\nâ†\nLLM.generate\nâ€‹\n(\nğ„\nI\n,\nğ„\nT\n,\nğ„\nV\n,\nğ„\nM\n,\nğ„\nCoT\n)\nc_{t}\\leftarrow\\text{LLM.generate}(\\mathbf{E}^{I},\\mathbf{E}^{T},\\mathbf{E}^{V},\\mathbf{E}^{M},\\mathbf{E}^{\\text{CoT}})\n14:\nâ„³\nâ†\nUpdateMemory\nâ€‹\n(\nâ„³\n,\nc\nt\n)\n\\mathcal{M}\\leftarrow\\text{UpdateMemory}(\\mathcal{M},c_{t})\n15:\nend\nif\n16:\nh\nt\npred\nâ†\nğ„\nCoT\nâ€‹\n[\nâˆ’\n1\n]\nh_{t}^{\\text{pred}}\\leftarrow\\mathbf{E}^{\\text{CoT}}[-1]\nâŠ³\n\\triangleright\nUse the hidden state of the last token as input for the action model\n17:\nÏ„\n^\nt\nâ†\nA\nÎ¸\nâ€‹\n(\nh\nt\npred\n)\n\\hat{\\tau}_{t}\\leftarrow A_{\\theta}(h_{t}^{\\text{pred}})\nâŠ³\n\\triangleright\nGenerate the next trajectory\n18:\nif\nÏ„\n^\nt\n=\nstop\n\\hat{\\tau}_{t}=\\texttt{stop}\nthen\n19:\nbreak\n20:\nelse\n21:\nExecuteAction\nâ€‹\n(\nÏ„\n^\nt\n)\n\\text{ExecuteAction}(\\hat{\\tau}_{t})\n22:\nend\nif\n23:\nend\nwhile\n24:\nend\nprocedure\n4\nData Collection\nTable 1:\nComparison of existing navigation datasets. Nav-AdaCoT-2.9M is the first dataset to integrate three navigation tasks (ObjNav, Track, ImageNav) and provide adaptive chain-of-thought reasoning.\nDataset\nScenes\nInstruction Capability\nN\ns\nâ€‹\nt\nâ€‹\ne\nâ€‹\np\nN_{step}\nN\nc\nâ€‹\no\nâ€‹\nt\nN_{cot}\nAction\nHM3D\nMP3D\nN\ns\nâ€‹\nc\nâ€‹\ne\nâ€‹\nn\nâ€‹\ne\nN_{scene}\nObjNav\nTrack\nImageNav\nModality\nHM3D ObjNav\n[\n41\n]\nâœ“\n\\checkmark\nÃ—\n\\times\n80\nâœ“\n\\checkmark\nÃ—\n\\times\nÃ—\n\\times\nL\n-\n-\nDes.\nMP3D ObjNav\n[\n7\n]\nÃ—\n\\times\nâœ“\n\\checkmark\n56\nâœ“\n\\checkmark\nÃ—\n\\times\nÃ—\n\\times\nL\n-\n-\nDes.\nSOON\n[\n88\n]\nâœ“\n\\checkmark\nÃ—\n\\times\n90\nâœ“\n\\checkmark\nÃ—\n\\times\nÃ—\n\\times\nL\n30K\n-\nDes.\nHM3D OVON\n[\n66\n]\nâœ“\n\\checkmark\nÃ—\n\\times\n181\nâœ“\n\\checkmark\nÃ—\n\\times\nÃ—\n\\times\nL\n53K\n-\nDes.\nEVT-Bench\n[\n52\n]\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n703\nÃ—\n\\times\nâœ“\n\\checkmark\nÃ—\n\\times\nL\n855K\n-\nTraj.\nHM3D ImgNav\n[\n25\n]\nâœ“\n\\checkmark\nÃ—\n\\times\n145\nÃ—\n\\times\nÃ—\n\\times\nâœ“\n\\checkmark\nL\n-\n-\nDes.\nOctoNav-Bench\n[\n14\n]\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n438\nâœ“\n\\checkmark\nÃ—\n\\times\nâœ“\n\\checkmark\nV, L\n45K\n10K\nDes.\nNav-CoT-110K\n[\n31\n]\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n342\nâœ“\n\\checkmark\nÃ—\n\\times\nÃ—\n\\times\nV, L\n110K\n110K\nDes.\n\\rowcolor\ngray!20\nNav-AdaCoT-2.9M (Ours)\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n718\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nâœ“\n\\checkmark\nV, L\n2.9M\n472K\nTraj.\nFigure 3\n:\nData distribution and instruction word cloud for the VLingNav training dataset.\nOur framework is trained on Nav-AdaCoT-2.9M, a large-scale dataset we constructed, supplemented by public open-world video datasets. Tab.\n1\npresents a statistical comparison between Nav-AdaCoT-2.9M and existing public embodied navigation datasets, evaluating metrics such as scene number, task type, total steps, CoT annotation number, and action modalities.\nNotably, our dataset surpasses others in scene number, task variety, and input modality richness. It also features the largest count of CoT annotations to date. Furthermore, Nav-AdaCoT-2.9M employs trajectory-based annotation, which provides finer-grained supervision compared to discrete action-based datasets.\n4.1\nEmbodied Navigation Data\n4.1.1\nNavigation Data Generation\nTo ensure both diversity and comparability, we construct our training data from several widely used embodied navigation benchmarks.\nObject-Goal Navigation.\nWe use data from three benchmarks:\nâ€¢\nHM3D ObjNav\n[\n41\n]\n: For this category-level search task, we utilize a subset of the human demonstration data provided by Habitat-Web\n[\n42\n]\n.\nâ€¢\nMP3D ObjNav\n[\n7\n]\n: We collect shortest-path trajectories to serve as training data.\nâ€¢\nHM3D OVON\n[\n66\n]\n: For this zero-shot, open-vocabulary task, we also collect shortest-path trajectories.\nVisual Tracking.\nWe leverage EVT-Bench\n[\n52\n]\nto curate a multi-person indoor tracking dataset.\nImage-Goal Navigation.\nWe use the HM3D Instance ImageNav\n[\n24\n]\nbenchmark. For this, we also generate shortest-path trajectories and derive step-by-step action labels.\nLeveraging these existing resources, we propose Nav-AdaCoT-2.9M, a large-scale dataset encompassing 2.9M step-by-step adaptive Chain-of-Thought trajectories. Distinct from prior datasets that predominantly furnish only instructions and expert action labels, Nav-AdaCoT-2.9M explicitly integrates structured reasoning that is aligned with observations and instructions. This design effectively bridges the domains of perception, language, and action. As the cornerstone for the supervised fine-tuning phase of VLingNav, this dataset facilitates the acquisition of structured reasoning capabilities in VLingNav prior to the reinforcement learning-based post-training.\nFigure 4\n:\nThe autonomous adaptive CoT labeling pipeline of VLingNav.\n4.1.2\nAutonomous Adaptive CoT labeling pipeline\nWe propose an autonomous adaptive Chain-of-Thought data labeling pipeline, specifically designed to construct high-quality CoT labels for embodied navigation and reasoning tasks. This pipeline leverages the reasoning capabilities of Vision-Language Models to generate coherent, step-by-step CoT rationales that justify navigation decisions in complex environments. As illustrated in Fig.\n4\n, we applied adaptive CoT labeling to the entire embodied navigation dataset described in the previous section.\nTo generate high-quality Chain-of-Thought labels, we designed a composite prompt for Qwen2.5-VL-72B\n[\n2\n]\nthat incorporates five essential components: 1) navigation instructions, 2) egocentric visual stream input (the most recent 10 frames, included to reduce the computational load of the VLM), 3) prior memory content, 4) expert trajectories at each step, and 5) explicit formatting requirements. This prompt guides the VLM to reason about spatial relationships, environmental constraints, and the semantic meaning of instructions, thereby generating structured, step-by-step CoT sequences. The outputs follow a standardized format: reasoning processes are enclosed within\n<think>\nâ€‹\nâ‹¯\nâ€‹\n</think>\n\\texttt{<think>}\\cdots\\texttt{</think>}\ntags, while summaries are contained in\n<summary>\nâ€‹\nâ‹¯\nâ€‹\n</summary>\n\\texttt{<summary>}\\cdots\\texttt{</summary>}\ntags. This formatting ensures transparent alignment among observations, reasoning, and memory.\nWhen this pipeline was executed across our diverse set of environments, approximately 472K CoT responses were generated from 2.9M samples. Each response includes a detailed CoT analysis and decision-making process for the navigation scenario, as well as linguistic memory describing the current environmental context. These raw outputs were further refined through a two-stage filtering procedure:\n1) Rule-based checks: Responses that were incomplete or logically inconsistent were discarded.\n2) Quality verification: Decisions were cross-validated against expert navigation trajectories to ensure accuracy.\nFollowing refinement, we constructed the Nav-AdaCoT-2.9M dataset. Serving as the Supervised Fine-Tuning data for VLingNav, this dataset provides rich reasoning trajectories that tightly integrate perception, instruction following, and navigation decision-making.\n4.2\nOpen-World Video Data\nFurthermore, co-training with open-world video data has been shown in multiple studies\n[\n75\n,\n56\n,\n3\n]\nto enhance model generalization and reduce the sim-to-real transfer gap. Consistent with these findings, we incorporate a variety of publicly available open-world video datasets\n[\n1\n,\n81\n,\n13\n]\ninto our training data. Beyond prior efforts, our approach not only improves general visual understanding but also further strengthens adaptive reasoning through additional adaptive CoT annotations. Specifically, we utilize three datasets, LLaVA-Video-178K\n[\n81\n]\n, Video-R1\n[\n13\n]\n, and ScanQA\n[\n1\n]\n, comprising a total of 1.6M samples, and construct an adaptive CoT-based video dataset by categorizing samples according to difficulty. In particular, the Video-R1 dataset, which contains relatively challenging video QA pairs, is organized as a CoT-annotated subset, whereas the other two datasets are formatted as non-CoT subsets. This design enables the model to further develop the ability to autonomously decide whether reasoning is required for a given input.\n4.3\nDataset Statistics\nUltimately, the training dataset for VLingNav comprises the two aforementioned types of datasets, totaling 4.5M training samples. Specifically, it includes 2.9M samples of embodied navigation data and 1.6M samples of open-world video data, with the detailed data distribution illustrated in Fig.\n3\n.\n5\nTraining Recipe\nFigure 5\n:\nOnline post-training with a hybrid rollout procedure.\n5.1\nModel Pre-train\nThe VLM backbone used in VLingNav does not natively support adaptive reasoning. To address this, we first conduct a pre-training stage on our custom open-world adaptive CoT video dataset (detailed in Sec.\n4\n). Following standard VLM training paradigms, we fine-tune the model for a single epoch. This process equips the model with the foundational ability to perform adaptive visual reasoning. The training is supervised using a standard cross-entropy (CE) loss, applied at the token level.\n5.2\nSupervised Fine-Tuning\nFollowing the pre-training phase, we perform supervised fine-tuning (SFT) to establish robust navigation and video reasoning capabilities. Specifically, we train the model using standard imitation learning on a combined dataset that integrates our embodied navigation data with the open-world video data. This co-training strategy ensures the model retains general-purpose visual reasoning while acquiring task-specific navigation skills. The training objective can be formalized as:\nmin\nÎ¸\nâ¡\nâ„’\nSFT\nâ€‹\n(\nÎ¸\n)\n=\nÎ±\nâ€‹\nâ„’\nMSE\nâ€‹\n(\nÏ„\n^\nt\n,\nÏ„\nt\ng\nâ€‹\nt\n)\n+\n(\n1\nâˆ’\nÎ±\n)\nâ€‹\nâ„’\nCE\nâ€‹\n(\nE\nt\np\nâ€‹\nr\nâ€‹\ne\nâ€‹\nd\n,\nE\nt\ng\nâ€‹\nt\n)\n\\min_{\\theta}\\;\\;\\mathcal{L}_{\\text{SFT}}(\\theta)=\\alpha\\mathcal{L}_{\\text{MSE}}(\\hat{\\tau}_{t},\\tau_{t}^{gt})+(1-\\alpha)\\mathcal{L}_{\\text{CE}}(E_{t}^{pred},E_{t}^{gt})\n(6)\nwhere\nâ„’\nMSE\n\\mathcal{L}_{\\text{MSE}}\nis the Mean Squared Error loss that supervises the predicted action trajectory\nÏ„\n^\nt\n\\hat{\\tau}_{t}\nagainst the ground-truth trajectory\nÏ„\nt\ng\nâ€‹\nt\n\\tau^{gt}_{t}\n,\nâ„’\nCE\n\\mathcal{L}_{\\text{CE}}\nis the Cross-Entropy loss that supervises the generation of all textual outputs, including both the CoT reasoning and the VQA responses.\nÎ±\n\\alpha\nis a hyperparameter that balances the contribution of the two loss components.\n5.3\nOnline Expert-guided Post-training\nTo address the limitations of offline imitation learning, such as covariate shift, and to better align the VLMâ€™s high-level representations with the closed-loop robot continuous action, we introduce an online post-training stage.\nStarting from the SFT checkpoint, the agent actively interacts with the simulation environment to collect fresh, on-policy trajectories. The policy is then updated using a hybrid objective function. This objective combines outcome-driven optimization with expert-guided supervision. This dual approach allows the model to explore more effective strategies while preventing catastrophic forgetting of the expert policy.\n5.3.1\nProbabilistic Continuous Action Model\nExisting VLA architectures often employ discrete tokenization for actions, which sacrifices precision; others use generative models like diffusion or flow matching, which incur high computational costs due to iterative denoising. To address this trade-off between high-precision continuous control and efficient inference, we propose a lightweight probabilistic projection head.\nLet\nğ¡\nt\n\\mathbf{h}_{t}\ndenote the visual-linguistic features extracted from the VLM backbone at timestep\nt\nt\n. We parameterize the policy\nÏ€\nÎ¸\nâ€‹\n(\nğš\nt\n|\nğ¬\nt\n)\n\\pi_{\\theta}(\\mathbf{a}_{t}|\\mathbf{s}_{t})\nas a multivariate Gaussian distribution. Specifically, the action head projects\nğ¡\nt\n\\mathbf{h}_{t}\nto predict the mean\nğ\nÎ¸\nâ€‹\n(\nğ¡\nt\n)\n\\boldsymbol{\\mu}_{\\theta}(\\mathbf{h}_{t})\nand the logarithm of the standard deviation\nlog\nâ¡\nğˆ\nÎ¸\nâ€‹\n(\nğ¡\nt\n)\n\\log\\boldsymbol{\\sigma}_{\\theta}(\\mathbf{h}_{t})\n:\nÏ€\nÎ¸\nâ€‹\n(\nğš\nt\n|\nğ¬\nt\n)\n=\nğ’©\nâ€‹\n(\nğ\nÎ¸\nâ€‹\n(\nğ¡\nt\n)\n,\ndiag\nâ€‹\n(\nğˆ\nÎ¸\nâ€‹\n(\nğ¡\nt\n)\n2\n)\n)\n\\pi_{\\theta}(\\mathbf{a}_{t}|\\mathbf{s}_{t})=\\mathcal{N}\\left(\\boldsymbol{\\mu}_{\\theta}(\\mathbf{h}_{t}),\\text{diag}\\left(\\boldsymbol{\\sigma}_{\\theta}(\\mathbf{h}_{t})^{2}\\right)\\right)\n(7)\nDuring online post-training rollout, stochastic exploration is implemented by sampling actions\nğš\nt\n\\mathbf{a}_{t}\nfrom the policy distribution\nğš\nt\nâˆ¼\nÏ€\nÎ¸\n(\nâ‹…\nâˆ£\nğ¬\nt\n)\n\\mathbf{a}_{t}\\sim\\pi_{\\theta}(\\cdot\\mid\\mathbf{s}_{t})\n. In contrast, for the validation phase, deterministic execution is adopted, where actions\nğš\nt\n\\mathbf{a}_{t}\nare set to the mean value\nğš\nt\n=\nğ\nÎ¸\nâ€‹\n(\nğ¡\nt\n)\n\\mathbf{a}_{t}=\\boldsymbol{\\mu}_{\\theta}(\\mathbf{h}_{t})\nof the policyâ€™s action distribution conditioned on the hidden state\nğ¡\nt\n\\mathbf{h}_{t}\n.\n5.3.2\nHybrid Rollout\nTo balance exploration with successful task completion, we employ a hybrid data collection strategy. As illustrated in Fig.\n5\n, we alternate between two rollout modes:\nâ€¢\nNaive rollout: The current policy\nÏ€\nÎ¸\n\\pi_{\\theta}\ninteracts with the environment independently. We store the complete interaction trajectories\nÏ„\n=\n{\n(\nğ¬\nt\n,\nğš\nt\n,\nr\nt\n)\n}\n\\tau=\\{(\\mathbf{s}_{t},\\mathbf{a}_{t},r_{t})\\}\n, and only successful trajectories are filtered out and incorporated into the hybrid buffer. As on-policy data, this dataset accurately reflects the current policyâ€™s capabilities and provides high-quality positive examples for reinforcing successful action sequences.\nâ€¢\nExpert-guided rollout: To address inefficient exploration and mitigate erroneous behaviors, the system incorporates an expert policy\nÏ€\nâˆ—\n\\pi^{*}\n(implemented via a Shortest Path planner in simulator). When the agent triggers an irrational condition (\ne\n.\ng\n., oscillating or stuck for\nk\nk\nsteps, here\nk\n=\n15\nk=15\n) or eventually fails. By taking control and demonstrating a recovery path, the expert provides high-quality, corrective trajectories. These demonstrations are then added to the hybrid buffer, enriching it with valuable examples of how to escape difficult states and improving overall agent robustness.\n5.3.3\nOnline Fine-tuning with Augmented Loss\nPure reinforcement learning can be unstable and sample-inefficient under sparse rewards and long horizons, while pure imitation learning may overfit to the expert state distribution and suffer from covariate shift. We therefore adopt a demonstration-augmented online post-training scheme\n[\n40\n]\n, where interaction data provides an outcome-driven learning signal and expert-guided trajectories provide a stabilizing supervised signal. Specially, we optimize the following composite objective:\nmin\nÎ¸\nâ¡\nâ„’\npost\nâ€‹\n(\nÎ¸\n)\n\\displaystyle\\min_{\\theta}\\;\\;\\mathcal{L}_{\\text{post}}(\\theta)\n=\nÎ»\nâ€‹\nâ„’\nRL\nâ€‹\n(\nÎ¸\n)\n+\n(\n1\nâˆ’\nÎ»\n)\nâ€‹\nâ„’\nSFT\nâ€‹\n(\nÎ¸\n)\n\\displaystyle=\\lambda\\mathcal{L}_{\\text{RL}}(\\theta)+(1-\\lambda)\\mathcal{L}_{\\text{SFT}}(\\theta)\n(8)\nwhere\nâ€‹\nâ„’\nRL\nâ€‹\n(\nÎ¸\n)\n\\displaystyle\\text{where }\\;\\;\\mathcal{L}_{\\text{RL}}(\\theta)\n=\nâˆ’\nğ”¼\nt\nâ€‹\n[\nmin\nâ¡\n(\nr\nt\nâ€‹\n(\nÎ¸\n)\nâ€‹\nA\nt\n,\nclip\nâ€‹\n(\nr\nt\nâ€‹\n(\nÎ¸\n)\n,\n1\nâˆ’\nÏµ\n,\n1\n+\nÏµ\n)\nâ€‹\nA\nt\n)\n]\n,\n\\displaystyle=-\\mathbb{E}_{t}\\left[\\min\\Big(r_{t}(\\theta)A_{t},\\;\\mathrm{clip}(r_{t}(\\theta),1-\\epsilon,1+\\epsilon)\\,A_{t}\\Big)\\right],\nwhere\nâ„’\nRL\n\\mathcal{L}_{\\text{RL}}\nis a PPO-style policy-gradient objective,and we use REINFORCE++\n[\n18\n]\nto calculate the advantage\nA\nt\nA_{t}\n.\nâ„’\nSFT\n\\mathcal{L}_{\\text{SFT}}\nis the imitation loss defined in Eq.\n6\n.\n5.4\nImplementation Details\n5.4.1\nTraining Details\nVLingNav is trained on a cluster with 128 NVIDIA A100 GPUs using a three-stage training pipeline. In the first stage, we leverage open-world video data for pre-training to endow the model with adaptive general visual reasoning capabilities. Consistent with standard VLM practices\n[\n29\n]\n, this pre-training runs for a single epoch. In the second stage, all embodied navigation data and open-world video data are mixed and randomly shuffled for co-training 20K steps with a total batch size of 512. In the online post-training phase, the policy undergoes 10 rollout iterations of updates using the training datasets of HM3D OVON, HM3D Instance ImageNav and EVT-Bench DT benchmarks. For each iteration, we use the current policy to collect 128 episodes of on-policy data, which are then added to the hybrid buffer before the model is updated. For open-world video data, all videos are sampled at 1 FPS to reduce redundancy between consecutive frames. Throughout all stage training, only the visual encoderâ€™s parameters are frozen; all other components are updated. The hyperparameters are set to\nÎ±\n=\n0.5\n\\alpha=0.5\nand\nÎ»\n=\n0.01\n\\lambda=0.01\n, which is determined by the scale of different losses.\n5.4.2\nInference Details\nDuring inference, we maintain a compact and consistent model architecture by not using task-specific tokens for task partitioning. Instead, at each step, the model autoregressively predicts a CoT indicator token. Based on this indicator, it may then generate CoT content. Finally, the hidden state corresponding to the last generated token is fed into the action module, which predicts the robotâ€™s future motion trajectory.\n6\nExperiments\nTo comprehensively evaluate the performance of VLingNav, we conducted a series of extensive experiments in both simulation and the real world. We first quantitatively compare VLingNav against state-of-the-art methods on several standard embodied navigation benchmarks. We then conduct detailed ablation studies to validate the effectiveness of each of our proposed key components. Furthermore, we have validated that the proposed model framework and training recipe in VLingNav demonstrate emergent generalization capabilities across diverse domains and tasks. Finally, we demonstrate VLingNavâ€™s ability to transfer to a real-world robot and complete practical navigation tasks in a zero-shot manner, verifying its real-world generalization and utility.\n6.1\nExperiment Setups\n6.1.1\nBenchmarks.\nOur method is evaluated on multiple public benchmarks, including those for Object Goal Navigation (HM3Dv1 ObjNav, HM3Dv2 ObjNav, MP3D ObjNav, and HM3D OVON), Embodied Visual Tracking (EVT-Bench), and Image Goal Navigation (HM3D Instance ImageNav). Notably, a shared model checkpoint is used across all tasks, with no additional fine-tuning performed for any individual task.\n6.1.2\nBaselines.\nWe conduct a comprehensive comparison of VLingNav against current state-of-the-art models, categorized into three groups: (1) modular methods, often separate the model into perception, mapping and planning,\ni\n.\ne\n.\n[\n66\n,\n63\n,\n62\n,\n67\n,\n74\n,\n8\n,\n25\n,\n79\n]\n, (2) end-to-end small-scale models often leverage a pre-trained network for visual feature extraction, which are then integrated with a policy network to output robot actions,\ni\n.\ne\n.\n[\n42\n,\n43\n,\n66\n,\n64\n,\n84\n,\n69\n]\n, and (3) VLA models\n[\n75\n,\n52\n,\n74\n,\n30\n,\n31\n,\n89\n]\n.\n6.1.3\nMetrics.\nTo evaluate navigation performance, we use standard metrics from public benchmarks, including Success Rate (SR), Success-weighted Path Length (SPL), Tracking Rate (TR), and Collision Rate (CR).\n6.2\nSimulation Experiments\n6.2.1\nObject Goal Navigation\nTable 2:\nPerformance on object goal navigation.\nComparison on HM3D ObjNav\n[\n41\n]\nand MP3D ObjNav\n[\n7\n]\nbenchmarks. The\nbest\nand the\nsecond best\nresults are denoted by\nbold\nand\nunderline\n.\nMethod\nHM3Dv1\nHM3Dv2\nMP3D\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nVLFM\n[\n65\n]\n52.5\n30.4\n63.6\n32.5\n36.4\n17.5\nSG-Nav\n[\n62\n]\n54.0\n24.9\n49.6\n25.5\n40.2\n16.0\nL3MVN\n[\n67\n]\n54.2\n25.5\n36.6\n15.7\n-\n-\nUniGoal\n[\n63\n]\n54.5\n25.1\n-\n-\n41.0\n16.4\nHabitat-Web\n[\n66\n]\n57.6\n23.8\n31.6\n8.5\nInstructNav\n[\n33\n]\n-\n-\n58.0\n20.9\n-\n-\nApexNav\n[\n79\n]\n59.6\n33.0\n76.2\n38.0\n39.2\n17.8\nOVRL\n[\n60\n]\n62.0\n26.8\n-\n-\n28.6\n7.4\nOVRL-v2\n[\n59\n]\n62.8\n28.1\n-\n-\n-\n-\nLFG\n[\n46\n]\n68.9\n36.0\n-\n-\n-\n-\nPirlNav\n[\n43\n]\n70.4\n34.1\n-\n-\n-\n-\nFiLM-Nav\n[\n64\n]\n61.7\n37.3\n77.0\n41.3\n-\n-\nCogNav\n[\n6\n]\n72.5\n26.2\n-\n-\n46.6\n16.1\nUni-NaVid\n[\n75\n]\n73.7\n37.1\n-\n-\n-\n-\n\\rowcolor\ngray!20\nVLingNav (SFT)\n70.6\n38.2\n76.4\n32.6\n47.4\n25.8\n\\rowcolor\ngray!20\nVLingNav\n79.1\n42.9\n83.0\n40.5\n58.9\n26.5\nTable 3:\nPerformance on object goal navigation.\nComparison on HM3D-OVON\n[\n66\n]\nbenchmark. The\nbest\nand the\nsecond best\nresults are denoted by\nbold\nand\nunderline\n.\nMethod\nVal Seen\nVal Seen\nSynonyms\nVal Unseen\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nBC\n11.1\n4.5\n9.9\n3.8\n5.4\n1.9\nDAgger\n11.1\n4.5\n9.9\n3.8\n5.4\n1.9\nRL\n18.1\n9.4\n15.0\n7.4\n10.2\n4.7\nBCRL\n39.2\n18.7\n27.8\n11.7\n18.6\n7.5\nDAgRL\n41.3\n21.2\n29.4\n14.4\n18.3\n7.9\nVLFM\n[\n65\n]\n35.2\n18.6\n32.4\n17.3\n35.2\n19.6\nDAgRL\n+\n+\nOD\n[\n66\n]\n38.5\n21.1\n39.0\n21.4\n37.1\n19.8\nUni-NaVid\n[\n75\n]\n41.3\n21.1\n43.9\n21.8\n39.5\n19.8\nTANGO\n[\n90\n]\n-\n-\n-\n-\n35.5\n19.5\nFiLM-Nav\n[\n64\n]\n44.9\n24.5\n40.1\n23.1\n40.8\n24.4\nMTU3D\n[\n89\n]\n55.0\n23.6\n45.0\n14.7\n40.8\n12.1\nNavFoM\n[\n74\n]\n37.7\n25.5\n43.3\n29.9\n43.6\n31.3\nNav-R1\n[\n31\n]\n58.4\n26.3\n48.1\n23.1\n42.2\n20.1\n\\rowcolor\ngray!20\nVLingNav (SFT)\n45.9\n26.5\n44.8\n27.1\n41.5\n22.4\n\\rowcolor\ngray!20\nVLingNav\n59.3\n29.7\n56.8\n30.1\n50.1\n24.6\nFirst, we compared the performance metrics of VLingNav with those of state-of-the-art methods on the Object Goal Navigation task. Specifically, our evaluations were conducted across multiple publicly available benchmarks including HM3Dv1, HM3Dv2, MP3D and HM3D OVON.\nAs presented in Tab.\n2\n, VLingNav achieved SOTA performance on three closed-vocabulary benchmarks, significantly outperforming prior methods on both SR and SPL metrics. On HM3Dv1, VLingNav reaches 79.1 SR and 42.9 SPL, improving over previous SOTA video-based VLA model Uni-NaVid (73.7/37.1) by +5.4 SR (+7.3%) and +3.9 SPL (+15.6%). A comparable performance improvement is also observed in HM3Dv2, where VLingNav achieves 83.0 SR and 40.5 SPL, surpassing FiLM-Nav (77.0/41.3) by +6.0 SR (+7.8%). It is noted that the SPL result achieved by our method is slightly lower than that of FiLM-Nav. This discrepancy primarily arises because the FiLM-Nav model only selects the next frontier position and then relies on a shortest path planner to reach itâ€”an approach that confers greater advantages in the simulator compared to our method, which directly outputs trajectory-based actions. On the MP3D benchmarkâ€”where long-range exploration scenarios predominateâ€”VLingNav achieves an SR of 58.9 and an SPL of 26.5. These results significantly outperform those of the prior SOTA methods CogNav (46.6/16.1) and ApexNav (39.2/17.8). Specifically, our method yields an +26.4% improvement in SR and a substantial +32.8% enhancement in SPL. This impressive result demonstrates that VLingNav possesses robust exploration and memory capabilities, validating its effectiveness in complex long-range navigation tasks. Collectively, these results show that VLingNav not only exhibits enhanced object-exploration capabilities in diverse and challenging unseen environments, but also produces substantially shorter and more efficient trajectories across benchmark tests, highlighting the benefits of adaptive reasoning and long-horizon linguistic memory in the Object Goal Navigation task.\nTo further validate the generalization capability of VLingNav, we evaluate its performance on HM3D OVONâ€”a more challenging open-vocabulary object navigation benchmark. This benchmark comprises three distinct test splits: (1)\nval seen\n, which includes object categories present in the training set; (2)\nval seen synonym\n, which consists of goal categories synonymous with those encountered during training; and (3)\nval unseen\n, which contains object categories not present in the training dataset. As illustrated in Tab.\n3\n, VLingNav achieves the best performance across all three test splits, with SRs improved by 0.9 (+1.5%), 8.7 (+18.1%), and 6.6 (+15.1%) respectively compared to the previous SOTA methods. This result demonstrates the strong cross-domain generalization capability of VLingNav.\n6.2.2\nEmbodied Visual Tracking\nTable 4:\nPerformance on embodied visual tracking.\nComparison on EVT-Bench\n[\n52\n]\n.\nâ€ \n{\\dagger}\n: Use GroundingDINO\n[\n32\n]\nas the open-vocabulary detector.\nâ€¡\n{\\ddagger}\n: Use SoM\n[\n61\n]\nwith GPT-4o\n[\n36\n]\nas the visual foundation model.\nMethod\nSingle Target Tracking\nDistracted Tracking\nSR\nâ†‘\n\\uparrow\nTR\nâ†‘\n\\uparrow\nCR\nâ†“\n\\downarrow\nSR\nâ†‘\n\\uparrow\nTR\nâ†‘\n\\uparrow\nCR\nâ†“\n\\downarrow\nIBVS\nâ€ \n{\\dagger}\n[\n17\n]\n42.9\n56.2\n3.75\n10.6\n28.4\n6.14\nPoliFormer\nâ€ \n{\\dagger}\n[\n69\n]\n4.67\n15.5\n40.1\n2.62\n13.2\n44.5\nEVT\n[\n84\n]\n24.4\n39.1\n42.5\n3.23\n11.2\n47.9\nEVT\nâ€¡\n{\\ddagger}\n[\n84\n]\n32.5\n49.9\n40.5\n15.7\n35.7\n53.3\nUni-NaVid\n[\n75\n]\n53.3\n67.2\n12.6\n31.9\n50.1\n21.3\nTrackVLA\n[\n52\n]\n85.1\n78.6\n1.65\n57.6\n63.2\n5.80\nNavFoM\n[\n74\n]\n86.0\n80.5\n-\n61.4\n68.2\n-\nNavFoM*\n[\n74\n]\n88.4\n80.7\n-\n62.0\n67.9\n-\nTrackVLA++\n[\n30\n]\n86.0\n81.0\n2.10\n66.5\n68.8\n4.71\n\\rowcolor\ngray!20\nVLingNav (SFT)\n87.2\n78.9\n1.23\n66.1\n69.7\n4.78\n\\rowcolor\ngray!20\nVLingNav\n88.4\n81.2\n2.07\n67.6\n73.5\n5.51\nTo evaluate the efficacy of the proposed method for the Embodied Visual Tracking (EVT) task, we conduct a comprehensive comparative analysis on the EVT-Bench. Specifically, we evaluate two representative and challenging splits:\n(1)\nSingle-Target Tracking\n: The agent must continuously track a single designated target in complex unseen environments.\n(2)\nDistracted Tracking\n: A more complex scenario in which the agent must sustain stable tracking of the correct target under instructions while resisting interference from multiple distractors.\nAs illustrated in Tab.\n4\n, VLingNav demonstrates SOTA performance across both splits. In the\nSingle Target Tracking\ntask, VLingNav achieves an SR of 88.4 and a TR of 81.2, matching or slightly surpassing the previous best methods like NavFoM and TrackVLA++. In the more challenging\nDistracted Tracking\nscenario, VLingNav establishes a clear advantage, achieving an SR of 67.6 and a TR of 73.5. This represents a significant improvement of 1.1 (+1.7%) in SR and 4.7 (+6.8%) in TR compared to the previous SOTA method TrackVLA++. Notably, VLingNav outperforms NavFoM with multi-view setting while using only a monocular camera, demonstrating robust tracking and precise recognition. These results strongly validate the superior tracking capability and robustness of our approach, especially in complex environments with distractors.\nFigure 6\n:\nPerformance visualization of VLingNav across various navigation benchmarks.\n6.2.3\nImage Goal Navigation\nTable 5:\nPerformance on image goal navigation.\nComparison on HM3D Instance ImageNav benchmark\n[\n41\n]\n. The\nbest\nand the\nsecond best\nresults are denoted by\nbold\nand\nunderline\n.\nMethod\nHM3D Instance ImageNav\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nKrantz\net al.\n[\n24\n]\n8.3\n3.5\nOVRL-v2-IIN\n[\n59\n]\n24.8\n11.8\nPSL\n[\n50\n]\n23.0\n11.4\nGOAT\n[\n8\n]\n37.4\n16.1\nMod-IIN\n[\n25\n]\n56.1\n23.3\nUniGoal\n[\n63\n]\n60.2\n23.7\n\\rowcolor\ngray!20\nVLingNav (SFT)\n51.1\n32.6\n\\rowcolor\ngray!20\nVLingNav\n60.8\n37.4\nTo further evaluate the Image Goal Navigation capabilities of our model, we evaluate VLingNav on the HM3D Instance ImageNav benchmark. This task requires the agent to navigate to a specific object instance depicted in a goal image within complex and unseen environments. As presented in Table\n5\n, VLingNav achieves state-of-the-art results on this benchmark. It achieves an SR of 60.8, which is slightly higher than the previous SOTA method UniGoal (60.2/23.7). Note that, UniGoal leverages the LightGlue\n[\n28\n]\nkeypoint-matching algorithm as an additional criterion, whereas VLingNav relies solely on the modelâ€™s implicit reasoning. More impressively, VLingNav demonstrates a substantial improvement in navigation efficiency, achieving an SPL of 37.4. This represents a remarkable 13.7 (+57.8%) improvement over UniGoal. This significant gain in SPL underscores our modelâ€™s ability to not only successfully find the target instance but also to do so via much more direct and efficient paths, highlighting the advanced reasoning and planning abilities of VLingNav.\n6.2.4\nVisualization Results\nFig.\n6\nillustrates several visualizations of VLingNav on the simulation benchmarks, including the robotâ€™s egocentric visual observations, a top-down scene map, input instructions, and the outputs of adaptive CoT and the predicted trajectory. These examples show that our model efficiently accomplishes multiple embodied navigation tasks while adaptively generating CoT reasoning. This not only enhances interpretability but also improves the overall quality of the navigation process.\n6.3\nReal-World Experiments\n6.3.1\nRobot Platform and Deployments\nFigure 7\n:\nReal-world robot platform setup.\nFigure 8\n:\nReal-world experiment results.\nWe provide a visualization of our robot platform in Fig.\n8\n. The platform is based on the Unitree Go2 quadruped robot, equipped with an Intel RealSense D457 camera on its head. In our work, we only utilize the RGB frames with a resolution of 1280\nÃ—\n\\times\n800 from the camera, under a horizontal field of view (HFOV) of 90\nâˆ˜\n. Additionally, a portable Wi-Fi is mounted on the back of the robot to enable communication with the remote server through the Internet.\nVLingNav is deployed on a remote server equipped with an NVIDIA RTX 4090 GPU. During real-world deployment, the server receives the instructions and images captured by the camera via the Internet. To ensure efficient communication, the images are compressed before transmission. After receiving the incoming data, the model performs inference and predicts the future trajectory, which is then transmitted to the quadruped robot for execution. Given that real-world navigation is an online process, we cache visual tokens from historically observed images. As a result, at each step the model only encodes the latest frame, which significantly improves inference efficiency. Furthermore, by leveraging VLingNavâ€™s visual memory compression strategy, our model maintains an inference latency of under 300 ms across 500 video frames. Including communication overhead (approximately 100 ms), VLingNav achieves an effective inference speed of around 2.5 FPS during long-horizon, real-world robot experiments.\nUpon receiving the predicted trajectory, the robot employs a nonlinear model predictive control (NMPC) module for trajectory tracking\n[\n15\n]\n. Formulating the task as an optimization problem based on a kinematic unicycle model, the controller computes optimal linear and angular velocities over a receding horizon.\n6.3.2\nObject Goal Navigation\nWe evaluated the Object Goal Navigation performance of VLingNav against the SOTA method Uni-NaVid, across three representative scenarios: home, office, and outdoor environment.\nFor each scenario, we selected three distinct target objects: (i) the table, washing machine, and microwave for the home environment; (ii) the TV, elevator, and trashbin for the office environment; (iii) the bike, light pole, and tree for the outdoor environment.\nTo mitigate the effects of randomness, we conducted 10 repeated trials for each target object. As shown in Fig.\n8\n, VLingNav achieves a significantly higher success rate than Uni-NaVid across all tested scenarios. These results validate the robust object recognition, exploration, and cross-scenario generalization capabilities of our model.\n6.3.3\nEmbodied Visual Tracking\nWe evaluated the embodied visual tracking performance of our method against Uni-NaVid across three representative scenarios: (i) single-target tracking in open spaces, (ii) single-target tracking in cluttered indoor environments, and (iii) distracted tracking in crowded scenes with frequent occlusions and nearby distractors. To mitigate randomness, we conducted 10 repeated trials per scenario. As shown in Fig.\n8\n, our method consistently outperforms Uni-NaVid in tracking success rate, with the largest margins appearing in the distracted setting where transient occlusions and target switches are common. These results validate the effectiveness of our adaptive reasoning for re-identification after occlusion and the benefit of precise trajectory control, highlighting strong generalization to dynamic, cluttered environments.\n6.3.4\nImage Goal Navigation\nWe further evaluated Image Goal Navigation by comparing our method with UniGoal across three representative scene categoriesâ€”home, office, and outdoor environments. For each category, we selected two image-specified goals and conducted 10 repeated trials per goal. As shown in Fig.\n8\n, our approach achieves a substantially higher success rate than UniGoal in all categories. These results suggest that multi-task training induces robust cross-modal grounding from text to images, while the combination of Adaptive CoT and linguistic memory supports reliable localization and efficient long-horizon navigation to visually specified targets under variations in camera intrinsics, viewpoints, and lighting.\nFigure 9\n:\nQualitative performance of VLingNav in real-world deployments.\n6.3.5\nVisualization Results\nReal-world experimental results are shown in Fig.\n9\n, where we evaluate the navigation capabilities of VLingNav under challenging scenarios. Specifically, we test the model across three representative scenario categories: office environments, household settings, and outdoor scenes. Within each category, we assess three core capabilities: object goal navigation, embodied visual tracking, and image goal navigation.\nNotably, the model weights deployed on the real-world robot are the same as those used in the simulation experiments described in the previous section; no additional fine-tuning on real-world data is performed.\nThe results demonstrate that VLingNav exhibits strong sim-to-real transfer in both recognition and planning while sustaining high-frequency inference in real-world scenarios, enabling zero-shot deployment in complex environments.\n6.4\nEmergence of Cross-Task and Cross-Domain Capabilities\nJoint training on multi-task navigation datasets leads VLingNav to exhibit emergent behaviors that generalize beyond any single task, yielding both cross-task and cross-domain capabilities.\n6.4.1\nCross-task Performance\nWe observe clear cross-task transfer in real-world experiments (Fig.\n9\n). For instance, although the visual tracking task contains only language-format instructions, VLingNav can directly track targets specified by image goals in a zero-shot manner. Moreover, it composes behaviors across tasks: (1) Search for a language-described target, then switch to tracking that target. (2) Search for the target in the image goal and subsequently track it after locating it.\nThis compositionality arises from the VLA modelâ€™s shared, unified architecture and co-training on multi-task navigation datasets. Together, these factors enable the model to learn common navigation priors and transfer them successfully across diverse navigation tasks.\n6.4.2\nCross-domain Performance\nSecond, we observe robust cross-domain generalization. Although trained only to track humans, VLingNav reliably tracks dynamic non-human targets. Moreover, VLingNav successfully localizes and navigates to out-of-distribution objectives specified only by fine-grained textual instructions, including category-ambiguous objects disambiguated by color, spatial constraints, or detailed descriptions. These behaviors indicate that multi-task learning, when co-trained with general visual understanding data, can substantially enhance VLingNavâ€™s generalization across domains.\n6.5\nAblation Studies\nTo evaluate the contribution of each component and training strategy in VLingNav, we conducted comprehensive ablation studies. These studies were performed on the ObjectNav task using the HM3D OVON\nval unseen\nbenchmark, the EVT task using the EVT-Bench\nDistracted Tracking\nbenchmark, and the ImageNav task using the HM3D Instance ImageNav\nval\nbenchmark. For consistency, we adhered to the same training procedures and evaluation settings as those used for the full model. Below, we summarize the empirical results and analyze the key findings.\n6.5.1\nAdaptive CoT\nTable 6:\nAblation study on Chain-of-Thought strategies.\nr\nC\nâ€‹\no\nâ€‹\nT\nr_{CoT}\nindicates the average percentage of steps where CoT reasoning is activated.\nCoT Strategy\nObjNav\nTrack\nImageNav\nğ’“\nğ‘ª\nâ€‹\nğ’\nâ€‹\nğ‘»\n\\boldsymbol{r_{CoT}}\n(%)\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nTR\nâ†‘\n\\uparrow\nCR\nâ†“\n\\downarrow\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nw/o CoT\n36.2\n16.5\n62.7\n68.5\n6.28\n56.3\n27.3\n0.0\nDense CoT (Per-step)\n25.3\n13.0\n59.8\n70.1\n26.3\n19.6\n13.2\n100.0\nFixed Interval (\nk\n=\n5\nk=5\n)\n42.5\n23.5\n68.5\n74.2\n9.18\n48.2\n28.7\n20.0\nFixed Interval (\nk\n=\n20\nk=20\n)\n39.7\n19.4\n66.2\n70.8\n11.9\n51.3\n31.2\n5.0\n\\rowcolor\ngray!20\nAdaptive CoT (Ours)\n50.1\n24.6\n67.6\n73.5\n5.51\n60.8\n37.4\n2.1\nTo assess the impact of different reasoning strategies, we conducted an ablation study detailed in Tab.\n6\n. The results show that both a complete lack of reasoning (â€œw/o CoTâ€) and exhaustive reasoning at every step (â€œDense CoTâ€) lead to suboptimal performance. While fixed-interval reasoning provides a moderate improvement, it remains inflexible. Our proposed Adaptive CoT strategy demonstrates clear superiority. It achieves the highest performance across all benchmarks. Remarkably, it accomplishes this while maintaining an exceptionally low reasoning frequency (\nr\nC\nâ€‹\no\nâ€‹\nT\n=\n2.1\n%\nr_{CoT}=\\text{2.1}\\%\n), far more efficient than even the sparse fixed-interval method. This highlights that dynamically and intelligently activating reasoning only when needed is crucial for creating high-performing, efficient embodied agents.\n6.5.2\nVisual-assisted Linguistic Memory\nTable 7:\nAblation study on memory modalities.\nMemory Mode\nObjNav\nTrack\nImageNav\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nTR\nâ†‘\n\\uparrow\nCR\nâ†“\n\\downarrow\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nw/o Memory\n15.4\n3.5\n37.5\n59.1\n1.90\n21.0\n3.7\nVisual-only\n45.2\n20.3\n66.8\n70.6\n7.85\n57.9\n33.7\nLanguage-only\n18.8\n4.4\n40.2\n55.2\n3.25\n23.3\n7.5\n\\rowcolor\ngray!20\nVLingMem (Ours)\n50.1\n24.6\n67.6\n73.5\n5.51\n60.8\n37.4\nWe conducted an ablation study to evaluate the VLingMem module and assess how long-horizon context affects navigation performance. As summarized in Tab.\n7\n, removing the memory module entirely (â€œw/o Memoryâ€) leads to a substantial performance drop. This is particularly pronounced in large or multi-room layouts, where agents frequently get stuck in loops or revisit dead ends.\nUsing a naive replay buffer that stores only visual features (â€œVisual-onlyâ€) or only linguistic memory (â€œLinguistic-onlyâ€) partially recovers performance but remains inferior to our full approach.\nIn contrast, our proposed VLingMem achieves the best results with minimal latency overhead. Qualitatively, VLingMem enables the agent to remember the environment layout and avoid revisiting explored regions, yielding higher success rates with more efficient paths.\n6.5.3\nCo-train with Open-world Video Data\nTable 8:\nAblation study on open-world video co-training.\nTraining Data\nObjNav\nTrack\nImageNav\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nSR\nâ†‘\n\\uparrow\nTR\nâ†‘\n\\uparrow\nCR\nâ†“\n\\downarrow\nSR\nâ†‘\n\\uparrow\nSPL\nâ†‘\n\\uparrow\nw/o Co-training\n43.1\n20.6\n66.5\n70.2\n7.62\n50.2\n32.7\n\\rowcolor\ngray!20\nw/ Co-training\n50.1\n24.6\n67.6\n73.5\n5.51\n60.8\n37.4\nWe further evaluated the impact of co-training with open-world video data. As presented in Tab.\n8\n, the results demonstrate a significant performance improvement compared to the model trained solely on embodied navigation data. This co-training strategy effectively enriches the modelâ€™s semantic priors, thereby enhancing cross-modal grounding and generalization capabilities while notably reducing the sim-to-real gap.\n6.5.4\nSFT Training Steps\nWe investigated the relationship between model performance and the number of training steps. As shown in Fig.\n11\n, model performance scales positively with the number of training steps (where 1 epoch\nâ‰ˆ\n\\approx\n10K training steps). The success rate rises steadily as the model is exposed to more data.\nNotably, we found that excessive training leads to diminishing returns and eventual performance degradation, likely due to overfitting on the simulation data. This highlights the need for a balanced training strategy that maximizes performance without incurring unnecessary computational cost or risking overfitting.\nFigure 10\n:\nAblation study on training steps.\nFigure 11\n:\nAblation study on online post-training iteration steps.\nFigure 12\n:\nAblation study on multi-task learning. We present the multi-task synergy of VLingNav and illustrate the performance comparison between models trained on a single task and those trained on multiple tasks.\n6.5.5\nOnline Post-training\nWe evaluated the effect of our online post-training phase, which follows the SFT stage. Across all benchmarks (Tab.\n2\n,\n3\n,\n4\n,\n5\n), the post-trained VLingNav model significantly outperforms the SFT checkpoint. This phase is critical for teaching the agent to find shortcuts, recover from errors, and handle the distribution shift that occurs beyond the static demonstration data.\nAs shown in Fig.\n11\n, the ablation studies on the rollout strategy demonstrate that the proposed Hybrid Rollout exhibits the highest effectiveness, yielding the optimal performance. Here, we evaluate on the HM3D OVON\nval unseen\nsplit. While the Expert Rollout (DAgger-like) also delivers strong performance, it still exhibits a performance gap compared to the Hybrid Rollout. However, the Naive Rollout fails to improve performance, likely due to the sparse reward signals and the long-horizon nature of the task making value estimation too difficult. This confirms that our expert-guided framework successfully optimizes the policy. It uses expert data to correct faulty behaviors while simultaneously using on-policy data to explore and discover better strategies, thereby outperforming pure imitation learning and finding a more robust policy.\n6.5.6\nMulti-task Synergy\nFinally, we investigated how jointly training on ObjectNav, EVT, and ImageNav affects generalization. As shown in Fig.\n12\n, models trained on a single task consistently underperform the multi-task model, even on their respective specialized benchmarks.\nMore importantly, this multi-task training strategy fosters emergent cross-domain and cross-task capabilities, leading to a notable performance improvement on out-of-distribution tasks. These findings demonstrate that multi-task learning facilitates the transfer of skills across different domains. This synergy enhances the modelâ€™s ability to reason and plan across diverse modalities, tasks, and target categories.\n7\nDiscussion\nWe further discuss the core contributions of our approach, their broader implications, and the key insights revealed by our experimental results.\n1) Effectiveness of Adaptive Thinking:\nInspired by dual-process theory, our adaptive Chain-of-Thought (AdaCoT) mechanism autonomously allocates â€œcognitive resourcesâ€, balancing efficiency and deliberation. When faced with simple, unambiguous navigation scenarios, such as traversing a straight corridor, the model opts for â€œfast thinkingâ€ (\n<think_off>\n) and directly outputs actions, ensuring fluid and real-time navigation. Conversely, at critical decision positions, in complex environments, or when encountering ambiguityâ€”like choosing a direction at an intersection or searching for an occluded objectâ€”the model triggers â€œslow thinkingâ€ (\n<think_on>\n), generating a detailed reasoning output. This adaptability not only significantly enhances decision quality but also proves that deliberate thought at a small fraction of key steps (shown to be average 2.1% in our experiments) is sufficient to substantially boost overall task success. This finding is crucial for deploying efficient, intelligent navigation on resource-constrained robot platforms.\n2) Synergy of Visual-Assisted Linguistic Memory:\nNavigation is a long-horizon decision-making process by its nature. Our proposed Visual-Assisted Linguistic Memory (VLingMem) module effectively addresses the memory deficiencies in traditional VLA models. Unlike methods that rely solely on implicit visual features, VLingMem distills key visual observations into concise linguistic summaries (\n<summary>\nâ‹¯\n\\cdots\n</summary>\n) and integrates them into the modelâ€™s context. This design offers two primary advantages. First, linguistic memory is more robust against information decay than compressed visual features, enabling the model to clearly recall critical semantic information such as â€œI have already checked this roomâ€ or â€œthere is a locked door on the left,â€ thereby effectively preventing redundant exploration and inefficient paths. Second, this linguistic memory forms a powerful synergy with the AdaCoT mechanism. When the model chooses not to engage in detailed CoT, the persistent linguistic memory still provides the necessary historical context, ensuring a coherent decision-making process. This synergy serves as a pivotal factor in enhancing the robustness of VLingNav in long-horizon and complex environments, while significantly improving the efficiency and quality of VLingNavâ€™s exploration.\n3) Beyond Imitation Learning: The Value of Online Expert-guided RL:\nOur research confirms that VLA models trained exclusively via imitation learning (SFT) are constrained by both the quality and coverage of expert data. Such models are additionally prone to critical issues, including causal confusion and covariate shift. To address these limitations, we introduce a post-training phase using expert-guided reinforcement learning. VLingNav enables autonomous exploration and policy refinement through real-time online interaction with the environment, while directly deriving rewards from prior expert policy. Compared to rule-based RL, the introduction of expert knowledge allows the model to discover superior or more robust navigation strategies with higher efficiency.\nThe significant performance gains observed in our experiments underscore that RL post-training is a critical step to unlock the full potential of VLA models, transforming them from mere â€œimitatorsâ€ into genuine â€œproblem solvers.â€\n4) Generality and Real-world Generalization:\nA notable achievement of VLingNav is its generality. By training on the large-scale, multi-task Nav-AdaCoT-2.9M dataset, VLingNav achieves state-of-the-art or competitive performance across all these tasks using a single, unified set of model weights, obviating the need for task-specific fine-tuning. This demonstrates that our approach successfully captures the underlying, universal cognitive structures of embodied navigation. Even more encouraging is VLingNavâ€™s ability to transfer to real-world robot platforms in a zero-shot manner and complete practical navigation tasks. This indicates that, through high-quality simulation training and a powerful cognitive architecture, the model learns generalizable representations of space, language, and action, rather than just patterns specific to the simulator, successfully bridging the sim-to-real gap.\nIn summary, VLingNav, with its unique cognitive architecture, provides a powerful paradigm for developing more intelligent, efficient, and interpretable embodied agents. It demonstrates the immense potential of combining principles from human cognition, such as adaptive thinking and episodic memory, with advanced machine learning paradigms like VLAs and RL.\n8\nConclusion and Limitation\nIn this work, we introduce VLingNav, a Vision-Language-Action model grounded in linguistic-driven cognition to address critical challenges in embodied navigation. By synergistically integrating adaptive reasoning, multimodal memory, and online expert-guided RL post-training, VLingNav achieves state-of-the-art performance across a range of embodied navigation benchmarks and can directly transfer to real-world robot platforms in a zero-shot manner.\nWhile VLingNav has achieved progress in embodied navigation, it has several limitations that point to promising directions for future research. First, the current model primarily relies on monocular egocentric observations as input. Due to the limited field of view (FOV) inherent in monocular vision, such input constrains the modelâ€™s perceptual capabilities. Following recent work\n[\n74\n]\n, we will explore integrating multi-view observations to improve navigation efficiency. Second, the current model adopts a single-system architecture, which restricts its prediction frequency. This limitation impedes rapid decision-making and obstacle handling in highly dynamic environments. To address this, we plan to upgrade VLingNav to a dual-system structure that supports high-frequency action outputs, thereby enhancing fundamental navigation performance, such as obstacle avoidance. Finally, the current approach uses only an MPC-based waypoint controller and lacks a more flexible locomotion model\n[\n35\n]\n. Incorporating such a locomotion controller could increase movement speed and expand the robotâ€™s reachable areas. We therefore plan to integrate locomotion capabilities into VLingNav in future work.\n9\nAcknowledgements\nWe sincerely thank Yunke Cai, Haiquan Chen, Shuai Chu, Taifeng Gao, Bo Jiang, Yunfei Li, Yunfei Liu, Tao Wang, Xibin Wu, and Tingshuai Yan for their strong support and fruitful discussions.\nReferences\nAzuma et al. [2022]\nDaichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe.\nScanqa: 3d question answering for spatial scene understanding.\nIn\nproceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pages 19129â€“19139, 2022.\nBai et al. [2025]\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al.\nQwen2. 5-vl technical report.\narXiv preprint arXiv:2502.13923\n, 2025.\nBlack et al. [2025]\nKevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Robert Equi, Chelsea Finn, Niccolo Fusai, Manuel Y Galliker, et al.\nÏ€\n0.5\n\\pi_{0.5}\n: a vision-language-action model with open-world generalization.\nIn\n9th Annual Conference on Robot Learning\n, 2025.\nBolya et al. [2022]\nDaniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman.\nToken merging: Your vit but faster.\narXiv preprint arXiv:2210.09461\n, 2022.\nCampos et al. [2021]\nCarlos Campos, Richard Elvira, Juan J GÃ³mez RodrÃ­guez, JosÃ© MM Montiel, and Juan D TardÃ³s.\nOrb-slam3: An accurate open-source library for visual, visualâ€“inertial, and multimap slam.\nIEEE transactions on robotics\n, 37(6):1874â€“1890, 2021.\nCao et al. [2025]\nYihan Cao, Jiazhao Zhang, Zhinan Yu, Shuzhen Liu, Zheng Qin, Qin Zou, Bo Du, and Kai Xu.\nCognav: Cognitive process modeling for object goal navigation with llms.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, pages 9550â€“9560, 2025.\nChang et al. [2017]\nAngel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niebner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang.\nMatterport3d: Learning from rgb-d data in indoor environments.\nIn\n2017 International Conference on 3D Vision (3DV)\n, pages 667â€“676. IEEE, 2017.\nChang et al. [2023]\nMatthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min, Kavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra, et al.\nGoat: Go to any thing.\narXiv preprint arXiv:2311.06430\n, 2023.\nChen et al. [2025]\nSheng Chen, Peiyu He, Jiaxin Hu, Ziyang Liu, Yansheng Wang, Tao Xu, Chi Zhang, Chongchong Zhang, Chao An, Shiyu Cai, et al.\nAstra: Toward general-purpose mobile robots via hierarchical multimodal learning.\narXiv preprint arXiv:2506.06205\n, 2025.\nCheng et al. [2025]\nAn-Chieh Cheng, Yandong Ji, Zhaojing Yang, Xueyan Zou, Jan Kautz, Erdem Biyik, Hongxu Yin, Sifei Liu, and Xiaolong Wang.\nNavila: Legged robot vision-language-action model for navigation.\nIn\nRSS\n, 2025.\nDeSouza and Kak [2002]\nGuilherme N DeSouza and Avinash C Kak.\nVision for mobile robot navigation: A survey.\nIEEE transactions on pattern analysis and machine intelligence\n, 24(2):237â€“267, 2002.\nEbbinghaus [2013]\nHermann Ebbinghaus.\n[image] memory: A contribution to experimental psychology.\nAnnals of neurosciences\n, 20(4):155, 2013.\nFeng et al. [2025]\nKaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue.\nVideo-r1: Reinforcing video reasoning in mllms.\narXiv preprint arXiv:2503.21776\n, 2025.\nGao et al. [2025]\nChen Gao, Liankai Jin, Xingyu Peng, Jiazhao Zhang, Yue Deng, Annan Li, He Wang, and Si Liu.\nOctonav: Towards generalist embodied navigation.\narXiv preprint arXiv:2506.09839\n, 2025.\nGrandia et al. [2023]\nRuben Grandia, Fabian Jenelten, Shao Yang, Farbod Farshidian, and Marco Hutter.\nPerceptive locomotion through nonlinear model predictive control.\nIEEE Transactions on Robotics\n, 39(5):3402â€“3421, 2023.\nGuo et al. [2025]\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\narXiv preprint arXiv:2501.12948\n, 2025.\nGupta et al. [2016]\nMeenakshi Gupta, Swagat Kumar, Laxmidhar Behera, and Venkatesh K Subramanian.\nA novel vision-based tracking algorithm for a human-following mobile robot.\nIEEE Transactions on Systems, Man, and Cybernetics: Systems\n, 47(7):1415â€“1427, 2016.\nHu [2025]\nJian Hu.\nReinforce++: A simple and efficient approach for aligning large language models.\narXiv preprint arXiv:2501.03262\n, 2025.\n[19]\nYucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen.\nVideo prediction policy: A generalist robot policy with predictive visual representations.\nIn\nForty-second International Conference on Machine Learning\n.\nHuang et al. [2025]\nChi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang.\nThinkact: Vision-language-action reasoning via reinforced visual latent planning.\narXiv preprint arXiv:2507.16815\n, 2025.\nKaraman et al. [2011]\nSertac Karaman, Matthew R Walter, Alejandro Perez, Emilio Frazzoli, and Seth Teller.\nAnytime motion planning using the rrt.\nIn\n2011 IEEE International Conference on Robotics and Automation\n, pages 1478â€“1483. ieee, 2011.\nKavraki et al. [2002]\nLydia E Kavraki, Petr Svestka, J-C Latombe, and Mark H Overmars.\nProbabilistic roadmaps for path planning in high-dimensional configuration spaces.\nIEEE Transactions on Robotics and Automation\n, 12(4):566â€“580, 2002.\nKim et al. [2025]\nMoo Jin Kim, Chelsea Finn, and Percy Liang.\nFine-tuning vision-language-action models: Optimizing speed and success.\narXiv preprint arXiv:2502.19645\n, 2025.\nKrantz et al. [2022]\nJacob Krantz, Stefan Lee, Jitendra Malik, Dhruv Batra, and Devendra Singh Chaplot.\nInstance-specific image goal navigation: Training embodied agents to find object instances.\narXiv preprint arXiv:2211.15876\n, 2022.\nKrantz et al. [2023]\nJacob Krantz, Theophile Gervet, Karmesh Yadav, Austin Wang, Chris Paxton, Roozbeh Mottaghi, Dhruv Batra, Jitendra Malik, Stefan Lee, and Devendra Singh Chaplot.\nNavigating to objects specified by images.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, pages 10916â€“10925, 2023.\nLi et al. [2025]\nHaozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, et al.\nSimplevla-rl: Scaling vla training via reinforcement learning.\narXiv preprint arXiv:2509.09674\n, 2025.\nLi et al. [2023]\nXinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al.\nVision-language foundation models as effective robot imitators.\narXiv preprint arXiv:2311.01378\n, 2023.\nLindenberger et al. [2023]\nPhilipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys.\nLightglue: Local feature matching at light speed.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n, pages 17627â€“17638, 2023.\nLiu et al. [2023]\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning.\nIn\nNeurIPS\n, 2023.\nLiu et al. [2025a]\nJiahang Liu, Yunpeng Qi, Jiazhao Zhang, Minghan Li, Shaoan Wang, Kui Wu, Hanjing Ye, Hong Zhang, Zhibo Chen, Fangwei Zhong, et al.\nTrackvla++: Unleashing reasoning and memory capabilities in vla models for embodied visual tracking.\narXiv preprint arXiv:2510.07134\n, 2025a.\nLiu et al. [2025b]\nQingxiang Liu, Ting Huang, Zeyu Zhang, and Hao Tang.\nNav-r1: Reasoning and navigation in embodied scenes.\narXiv preprint arXiv:2509.10884\n, 2025b.\nLiu et al. [2024]\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al.\nGrounding dino: Marrying dino with grounded pre-training for open-set object detection.\nIn\nEuropean conference on computer vision\n, pages 38â€“55. Springer, 2024.\nLong et al. [2024]\nYuxing Long, Wenzhe Cai, Hongcheng Wang, Guanqi Zhan, and Hao Dong.\nInstructnav: Zero-shot system for generic instruction navigation in unexplored environment.\narXiv preprint arXiv:2406.04882\n, 2024.\nLuo et al. [2018]\nWenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, and Yizhou Wang.\nEnd-to-end active object tracking via reinforcement learning.\nIn\nInternational conference on machine learning\n, pages 3286â€“3295. PMLR, 2018.\nMiki et al. [2022]\nTakahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter.\nLearning robust perceptive locomotion for quadrupedal robots in the wild.\nScience robotics\n, 7(62):eabk2822, 2022.\nOpenAI [2024]\nOpenAI.\nIntroducing 4o image generation.\nhttps://openai.com/index/introducing-4o-image\n, 2024.\nAccessed: 2025-04-29.\nQi et al. [2025]\nZhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, and Hengshuang Zhao.\nVln-r1: Vision-language navigation via reinforcement fine-tuning.\narXiv preprint arXiv:2506.17221\n, 2025.\nQin et al. [2018]\nTong Qin, Peiliang Li, and Shaojie Shen.\nVins-mono: A robust and versatile monocular visual-inertial state estimator.\nIEEE Transactions on Robotics\n, 34(4):1004â€“1020, 2018.\nRadford et al. [2021]\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\nLearning transferable visual models from natural language supervision.\nIn\nInternational Conference on Machine Learning\n, pages 8748â€“8763. PMLR, 2021.\nRajeswaran et al. [2017]\nAravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine.\nLearning complex dexterous manipulation with deep reinforcement learning and demonstrations.\narXiv preprint arXiv:1709.10087\n, 2017.\nRamakrishnan et al. [2021]\nSanthosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, et al.\nHabitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai.\narXiv preprint arXiv:2109.08238\n, 2021.\nRamrakhya et al. [2022]\nRam Ramrakhya, Eric Undersander, Dhruv Batra, and Abhishek Das.\nHabitat-web: Learning embodied object-search strategies from human demonstrations at scale.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 5173â€“5183, 2022.\nRamrakhya et al. [2023]\nRam Ramrakhya, Dhruv Batra, Erik Wijmans, and Abhishek Das.\nPirlnav: Pretraining with imitation and rl finetuning for objectnav.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 17896â€“17906, 2023.\nRavi et al. [2024]\nNikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, et al.\nSam 2: Segment anything in images and videos.\narXiv preprint arXiv:2408.00714\n, 2024.\nSchulman et al. [2017]\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\nProximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347\n, 2017.\nShah et al. [2023]\nDhruv Shah, Michael Robert Equi, BÅ‚aÅ¼ej OsiÅ„ski, Fei Xia, Brian Ichter, and Sergey Levine.\nNavigation with large language models: Semantic guesswork as a heuristic for planning.\nIn\nConference on Robot Learning\n, pages 2683â€“2699. PMLR, 2023.\nShao et al. [2024]\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al.\nDeepseekmath: Pushing the limits of mathematical reasoning in open language models.\narXiv preprint arXiv:2402.03300\n, 2024.\nShi et al. [2025]\nHao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, and Gao Huang.\nMemoryvla: Perceptual-cognitive memory in vision-language-action models for robotic manipulation.\narXiv preprint arXiv:2508.19236\n, 2025.\nSu et al. [2024]\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.\nRoformer: Enhanced transformer with rotary position embedding.\nNeurocomputing\n, 568:127063, 2024.\nSun et al. [2024]\nXinyu Sun, Lizhao Liu, Hongyan Zhi, Ronghe Qiu, and Junwei Liang.\nPrioritized semantic learning for zero-shot instance navigation.\nIn\nEuropean Conference on Computer Vision\n, pages 161â€“178. Springer, 2024.\nWang et al. [2025a]\nJianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny.\nVggt: Visual geometry grounded transformer.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 5294â€“5306, 2025a.\nWang et al. [2025b]\nShaoan Wang, Jiazhao Zhang, Minghan Li, Jiahang Liu, Anqi Li, Kui Wu, Fangwei Zhong, Junzhi Yu, Zhizheng Zhang, and He Wang.\nTrackvla: Embodied visual tracking in the wild.\narXiv preprint arXiv:2505.23189\n, 2025b.\nWang et al. [2025c]\nShuo Wang, Yongcai Wang, Wanting Li, Xudong Cai, Yucheng Wang, Maiyue Chen, Kaihui Wang, Zhizhong Su, Deying Li, and Zhaoxin Fan.\nAux-think: Exploring reasoning strategies for data-efficient vision-language navigation.\narXiv preprint arXiv:2505.11886\n, 2025c.\nWang and Zhou [2024]\nXuezhi Wang and Denny Zhou.\nChain-of-thought reasoning without prompting.\nAdvances in Neural Information Processing Systems\n, 37:66383â€“66409, 2024.\nWei et al. [2022]\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems\n, 35:24824â€“24837, 2022.\nWei et al. [2025]\nMeng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, Wenzhe Cai, Hanqing Wang, Yilun Chen, et al.\nStreamvln: Streaming vision-and-language navigation via slowfast context modeling.\narXiv preprint arXiv:2507.05240\n, 2025.\nWu et al. [2024]\nYuchen Wu, Pengcheng Zhang, Meiying Gu, Jin Zheng, and Xiao Bai.\nEmbodied navigation with multi-modal information: A survey from tasks to methodology.\nInformation Fusion\n, 113:102532, 2024.\nXu et al. [2022]\nWei Xu, Yixi Cai, Dongjiao He, Jiarong Lin, and Fu Zhang.\nFast-lio2: Fast direct lidar-inertial odometry.\nIEEE Transactions on Robotics\n, 38(4):2053â€“2073, 2022.\nYadav et al. [2023a]\nKarmesh Yadav, Arjun Majumdar, Ram Ramrakhya, Naoki Yokoyama, Alexei Baevski, Zsolt Kira, Oleksandr Maksymets, and Dhruv Batra.\nOvrl-v2: A simple state-of-art baseline for imagenav and objectnav.\narXiv preprint arXiv:2303.07798\n, 2023a.\nYadav et al. [2023b]\nKarmesh Yadav, Ram Ramrakhya, Arjun Majumdar, Vincent-Pierre Berges, Sachit Kuhar, Dhruv Batra, Alexei Baevski, and Oleksandr Maksymets.\nOffline visual representation learning for embodied navigation.\nIn\nWorkshop on Reincarnating Reinforcement Learning at ICLR 2023\n, 2023b.\nYang et al. [2023]\nJianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao.\nSet-of-mark prompting unleashes extraordinary visual grounding in gpt-4v.\narXiv preprint arXiv:2310.11441\n, 2023.\nYin et al. [2024]\nHang Yin, Xiuwei Xu, Zhenyu Wu, Jie Zhou, and Jiwen Lu.\nSg-nav: Online 3d scene graph prompting for llm-based zero-shot object navigation.\nAdvances in neural information processing systems\n, 37:5285â€“5307, 2024.\nYin et al. [2025]\nHang Yin, Xiuwei Xu, Linqing Zhao, Ziwei Wang, Jie Zhou, and Jiwen Lu.\nUnigoal: Towards universal zero-shot goal-oriented navigation.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 19057â€“19066, 2025.\nYokoyama and Ha [2025]\nNaoki Yokoyama and Sehoon Ha.\nFilm-nav: Efficient and generalizable navigation via vlm fine-tuning.\narXiv preprint arXiv:2509.16445\n, 2025.\nYokoyama et al. [2024a]\nNaoki Yokoyama, Sehoon Ha, Dhruv Batra, Jiuguang Wang, and Bernadette Bucher.\nVlfm: Vision-language frontier maps for zero-shot semantic navigation.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 42â€“48. IEEE, 2024a.\nYokoyama et al. [2024b]\nNaoki Yokoyama, Ram Ramrakhya, Abhishek Das, Dhruv Batra, and Sehoon Ha.\nHm3d-ovon: A dataset and benchmark for open-vocabulary object goal navigation.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, pages 5543â€“5550. IEEE, 2024b.\nYu et al. [2023]\nBangguo Yu, Hamidreza Kasaei, and Ming Cao.\nL3mvn: Leveraging large language models for visual target navigation.\nIn\n2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, pages 3554â€“3560. IEEE, 2023.\nZawalski et al. [2024]\nMichaÅ‚ Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine.\nRobotic control via embodied chain-of-thought reasoning.\narXiv preprint arXiv:2407.08693\n, 2024.\n[69]\nKuo-Hao Zeng, Zichen Zhang, Kiana Ehsani, Rose Hendrix, Jordi Salvador, Alvaro Herrasti, Ross Girshick, Aniruddha Kembhavi, and Luca Weihs.\nPoliformer: Scaling on-policy rl with transformers results in masterful navigators.\nIn\n8th Annual Conference on Robot Learning\n.\nZeng et al. [2025]\nShuang Zeng, Dekang Qi, Xinyuan Chang, Feng Xiong, Shichao Xie, Xiaolong Wu, Shiyi Liang, Mu Xu, and Xing Wei.\nJanusvln: Decoupling semantics and spatiality with dual implicit memory for vision-language navigation.\narXiv preprint arXiv:2509.22548\n, 2025.\nZhai et al. [2023]\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.\nSigmoid loss for language image pre-training.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n, pages 11975â€“11986, 2023.\n[72]\nJianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, and Jianyu Chen.\nUp-vla: A unified understanding and prediction model for embodied agent.\nIn\nForty-second International Conference on Machine Learning\n.\nZhang et al. [2024a]\nJiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, and He Wang.\nNavid: Video-based vlm plans the next step for vision-and-language navigation.\nRobotics: Science and Systems\n, 2024a.\nZhang et al. [2025a]\nJiazhao Zhang, Anqi Li, Yunpeng Qi, Minghan Li, Jiahang Liu, Shaoan Wang, Haoran Liu, Gengze Zhou, Yuze Wu, Xingxing Li, et al.\nEmbodied navigation foundation model.\narXiv preprint arXiv:2509.12129\n, 2025a.\nZhang et al. [2025b]\nJiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang, and He Wang.\nUni-navid: A video-based vision-language-action model for unifying embodied navigation tasks.\nRobotics: Science and Systems\n, 2025b.\nZhang et al. [2025c]\nLingfeng Zhang, Xiaoshuai Hao, Yingbo Tang, Haoxiang Fu, Xinyu Zheng, Pengwei Wang, Zhongyuan Wang, Wenbo Ding, and Shanghang Zhang.\nNava\n3\n: Understanding any instruction, navigating anywhere, finding anything.\narXiv preprint arXiv:2508.04598\n, 2025c.\nZhang et al. [2025d]\nLingfeng Zhang, Xiaoshuai Hao, Qinwen Xu, Qiang Zhang, Xinyao Zhang, Pengwei Wang, Jing Zhang, Zhongyuan Wang, Shanghang Zhang, and Renjing Xu.\nMapNav: A novel memory representation via annotated semantic maps for VLM-based vision-and-language navigation.\nIn\nProceedings of the 63rd Annual Meeting of the Association for Computational Linguistics\n, pages 13032â€“13056, Vienna, Austria, July 2025d.\nZhang et al. [2025e]\nLingfeng Zhang, Yuecheng Liu, Zhanguang Zhang, Matin Aghaei, Yaochen Hu, Hongjian Gu, Mohammad Ali Alomrani, David Gamaliel Arcos Bravo, Raika Karimi, Atia Hamidizadeh, et al.\nMem2ego: Empowering vision-language models with global-to-ego memory for long-horizon embodied navigation.\narXiv preprint arXiv:2502.14254\n, 2025e.\nZhang et al. [2025f]\nMingjie Zhang, Yuheng Du, Chengkai Wu, Jinni Zhou, Zhenchao Qi, Jun Ma, and Boyu Zhou.\nApexnav: An adaptive exploration strategy for zero-shot object navigation with target-centric semantic fusion.\narXiv preprint arXiv:2504.14478\n, 2025f.\nZhang et al. [2025g]\nTonghe Zhang, Chao Yu, Sichang Su, and Yu Wang.\nReinflow: Fine-tuning flow matching policy with online reinforcement learning.\narXiv preprint arXiv:2505.22094\n, 2025g.\nZhang et al. [2024b]\nYuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li.\nVideo instruction tuning with synthetic data.\narXiv preprint arXiv:2410.02713\n, 2024b.\nZhang et al. [2025h]\nZekai Zhang, Weiye Zhu, Hewei Pan, Xiangchen Wang, Rongtao Xu, Xing Sun, and Feng Zheng.\nActivevln: Towards active exploration via multi-turn rl in vision-and-language navigation.\narXiv preprint arXiv:2509.12618\n, 2025h.\nZhao et al. [2025]\nQingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al.\nCot-vla: Visual chain-of-thought reasoning for vision-language-action models.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 1702â€“1713, 2025.\nZhong et al. [2024]\nFangwei Zhong, Kui Wu, Hai Ci, Churan Wang, and Hao Chen.\nEmpowering embodied visual tracking with visual foundation models and offline rl.\nIn\nEuropean Conference on Computer Vision\n, pages 139â€“155. Springer, 2024.\nZhou et al. [2024]\nGengze Zhou, Yicong Hong, and Qi Wu.\nNavgpt: Explicit reasoning in vision-and-language navigation with large language models.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n, volume 38, pages 7641â€“7649, 2024.\nZhou et al. [2025]\nGengze Zhou, Yicong Hong, Zun Wang, Xin Eric Wang, and Qi Wu.\nNavgpt-2: Unleashing navigational reasoning capability for large vision-language models.\nIn\nEuropean Conference on Computer Vision\n, pages 260â€“278, 2025.\n[87]\nZhongyi Zhou, Yichen Zhu, Xiaoyu Liu, Zhibin Tang, Junjie Wen, Yaxin Peng, Chaomin Shen, and Yi Xu.\nChatvla-2: Vision-language-action model with open-world reasoning.\nIn\nThe Thirty-ninth Annual Conference on Neural Information Processing Systems\n.\nZhu et al. [2021]\nFengda Zhu, Xiwen Liang, Yi Zhu, Qizhi Yu, Xiaojun Chang, and Xiaodan Liang.\nSoon: Scenario oriented object navigation with graph-based exploration.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 12689â€“12699, 2021.\nZhu et al. [2025]\nZiyu Zhu, Xilin Wang, Yixuan Li, Zhuofan Zhang, Xiaojian Ma, Yixin Chen, Baoxiong Jia, Wei Liang, Qian Yu, Zhidong Deng, et al.\nMove to understand a 3d scene: Bridging visual grounding and exploration for efficient and versatile embodied navigation.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, pages 8120â€“8132, 2025.\nZiliotto et al. [2025]\nFilippo Ziliotto, Tommaso Campari, Luciano Serafini, and Lamberto Ballan.\nTango: training-free embodied ai agents for open-world tasks.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 24603â€“24613, 2025.",
  "preview_text": "VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.\n\n1]ByteDance Seed\n2]Peking University\n3]Zhongguancun Academy\n\\contribution\n[*]Co-first authors\n\\contribution\n",
  "is_relevant": true,
  "relevance_score": 8.0,
  "extracted_keywords": [
    "Reinforcement Learning",
    "VLA",
    "locomotion",
    "VLM"
  ],
  "one_line_summary": "VLingNavæå‡ºä¸€ç§åŸºäºè‡ªé€‚åº”æ¨ç†å’Œè§†è§‰è¾…åŠ©è¯­è¨€è®°å¿†çš„VLAæ¨¡å‹ï¼Œç”¨äºå…·èº«å¯¼èˆªä»»åŠ¡ï¼Œé€šè¿‡åœ¨çº¿ä¸“å®¶å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æå‡æ€§èƒ½ã€‚",
  "detailed_summary": "## VLingNav è®ºæ–‡æ‘˜è¦\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\néšç€è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„å‘å±•ï¼Œå…·èº«å¯¼èˆªä»»åŠ¡å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰VLAæ¨¡å‹å¤§å¤šé‡‡ç”¨ä»è§‚å¯Ÿåˆ°åŠ¨ä½œçš„è¢«åŠ¨æ˜ å°„ï¼Œç¼ºä¹**æ˜¾å¼æ¨ç†èƒ½åŠ›**å’Œ**æŒä¹…è®°å¿†æœºåˆ¶**ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚ã€é•¿ç¨‹çš„å¯¼èˆªä»»åŠ¡ã€‚è¿™é™åˆ¶äº†å…¶åœ¨åŠ¨æ€å¼€æ”¾ä¸–ç•Œä¸­çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡º **VLingNav**ï¼Œä¸€ä¸ªåŸºäºè¯­è¨€é©±åŠ¨è®¤çŸ¥çš„VLAå¯¼èˆªæ¡†æ¶ï¼ŒåŒ…å«ä¸¤å¤§æ ¸å¿ƒåˆ›æ–°ï¼š\n- **è‡ªé€‚åº”æ€ç»´é“¾ï¼ˆAdaCoTï¼‰**ï¼šå—äººç±»è®¤çŸ¥åŒè¿‡ç¨‹ç†è®ºå¯å‘ï¼Œè¯¥æœºåˆ¶èƒ½åŠ¨æ€åˆ¤æ–­ä½•æ—¶éœ€è¦å¯åŠ¨æ˜¾å¼æ¨ç†ï¼Œä½¿æ™ºèƒ½ä½“åœ¨**å¿«é€Ÿç›´è§‰æ‰§è¡Œ**ä¸**æ…¢é€Ÿå®¡æ…è§„åˆ’**é—´çµæ´»åˆ‡æ¢ï¼Œå¹³è¡¡æ•ˆç‡ä¸æ€§èƒ½ã€‚\n- **è§†è§‰è¾…åŠ©è¯­è¨€è®°å¿†ï¼ˆVLingMemï¼‰**ï¼šæ„å»ºä¸€ä¸ªæŒä¹…çš„è·¨æ¨¡æ€è¯­ä¹‰è®°å¿†æ¨¡å—ï¼Œå°†å…³é”®è§†è§‰è§‚å¯Ÿæ€»ç»“ä¸ºè¯­è¨€æè¿°å¹¶å­˜å‚¨ï¼Œä½¿æ™ºèƒ½ä½“èƒ½**å›å¿†å†å²ä¿¡æ¯**ä»¥é¿å…é‡å¤æ¢ç´¢ï¼Œå¹¶**æ¨æ–­åŠ¨æ€è¶‹åŠ¿**ã€‚\næ­¤å¤–ï¼Œç ”ç©¶æ„å»ºäº†è¿„ä»Šæœ€å¤§çš„å…·èº«å¯¼èˆªæ¨ç†æ•°æ®é›† **Nav-AdaCoT-2.9M**ï¼Œå¹¶å¼•å…¥äº†**åœ¨çº¿ä¸“å®¶å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒ**é˜¶æ®µï¼Œä½¿æ¨¡å‹èƒ½è¶…è¶Šæ¨¡ä»¿å­¦ä¹ çš„å±€é™ï¼Œè·å¾—æ›´é²æ£’ã€è‡ªæ¢ç´¢çš„å¯¼èˆªè¡Œä¸ºã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\nVLingNav åœ¨å¤šä¸ªæ ‡å‡†å…·èº«å¯¼èˆªåŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬ç›®æ ‡å¯¼èˆªã€è§†è§‰è·Ÿè¸ªã€å›¾åƒç›®æ ‡å¯¼èˆªï¼‰ä¸Šå‡å–å¾—äº†**æœ€å…ˆè¿›çš„æ€§èƒ½**ã€‚ä¾‹å¦‚ï¼Œåœ¨HM3Dç›®æ ‡å¯¼èˆªä»»åŠ¡ä¸Šï¼ŒæˆåŠŸç‡ï¼ˆSRï¼‰è¾¾åˆ°79.1%ï¼Œè¾ƒä¹‹å‰æœ€ä½³æ¨¡å‹æå‡æ˜¾è‘—ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒVLingNav èƒ½å¤Ÿ**é›¶æ ·æœ¬è¿ç§»**åˆ°çœŸå®ä¸–ç•Œçš„æœºå™¨äººå¹³å°ï¼ŒæˆåŠŸæ‰§è¡Œæœªè§è¿‡çš„å¤æ‚å¯¼èˆªä»»åŠ¡ï¼Œå±•ç°äº†å¼ºå¤§çš„è·¨é¢†åŸŸã€è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\nVLingNav é€šè¿‡å°†è‡ªé€‚åº”æ¨ç†å’ŒæŒä¹…è®°å¿†æ·±åº¦é›†æˆåˆ°VLAæ¨¡å‹ä¸­ï¼Œä¸ºæ„å»ºæ›´æ™ºèƒ½ã€é«˜æ•ˆä¸”å¯è§£é‡Šçš„å…·èº«æ™ºèƒ½ä½“æä¾›äº†æ–°èŒƒå¼ã€‚å…¶æˆåŠŸéªŒè¯äº†**è¯­è¨€é©±åŠ¨è®¤çŸ¥**åœ¨å…·èº«å¯¼èˆªä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶ä¸ºè§£å†³é•¿ç¨‹è§„åˆ’ã€åŠ¨æ€ç¯å¢ƒé€‚åº”ç­‰å…³é”®æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œæ¨åŠ¨äº†å…·èº«AIå‘å®é™…åº”ç”¨åœºæ™¯çš„è½åœ°ã€‚",
  "qa_pairs": [
    {
      "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
      "answer": "## VLingNav è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **ä¸€ã€ è®ºæ–‡æ—¨åœ¨è§£å†³çš„æ ¸å¿ƒé—®é¢˜**\nå½“å‰åŸºäºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„å…·èº«å¯¼èˆªç³»ç»Ÿå­˜åœ¨ä¸‰ä¸ªä¸»è¦ç“¶é¢ˆï¼š\n1.  **ç¼ºä¹æ˜¾å¼æ¨ç†èƒ½åŠ›**ï¼šç°æœ‰æ¨¡å‹å¤šä¸ºâ€œååº”å¼â€ç³»ç»Ÿï¼Œç›´æ¥ä»è§‚æµ‹æ˜ å°„åˆ°åŠ¨ä½œï¼Œç¼ºä¹äººç±»å¼çš„å†…éƒ¨æ€è€ƒå’Œè§„åˆ’è¿‡ç¨‹ï¼Œéš¾ä»¥å¤„ç†å¤æ‚ã€é•¿è§†é‡çš„å¯¼èˆªä»»åŠ¡ã€‚\n2.  **ç¼ºä¹æŒä¹…è®°å¿†æœºåˆ¶**ï¼šæ¨¡å‹é€šå¸¸ä¾èµ–æœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œæ— æ³•æœ‰æ•ˆä¿ç•™å’Œåˆ©ç”¨å†å²è§‚æµ‹ä¿¡æ¯ï¼Œå¯¼è‡´é‡å¤æ¢ç´¢ã€è·¯å¾„å¾ªç¯ï¼Œä¸”éš¾ä»¥é€‚åº”åŠ¨æ€ç¯å¢ƒå˜åŒ–ã€‚\n3.  **è®­ç»ƒèŒƒå¼å±€é™**ï¼šä¸»è¦ä¾èµ–æ¨¡ä»¿å­¦ä¹ ï¼ˆSFTï¼‰ï¼Œå—é™äºä¸“å®¶æ¼”ç¤ºæ•°æ®çš„è´¨é‡å’Œè¦†ç›–èŒƒå›´ï¼Œéš¾ä»¥è¶…è¶Šæ¼”ç¤ºæ°´å¹³ï¼Œä¸”å­˜åœ¨åå˜é‡åç§»ç­‰é—®é¢˜ã€‚\n\n### **äºŒã€ æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº† **VLingNav** æ¡†æ¶ï¼Œé€šè¿‡ä¸¤å¤§æ ¸å¿ƒæ¨¡å—å’Œä¸€ä¸ªåˆ›æ–°çš„è®­ç»ƒèŒƒå¼æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚\n\n#### **1. è‡ªé€‚åº”æ€ç»´é“¾æœºåˆ¶**\n- **çµæ„Ÿæ¥æº**ï¼šå€Ÿé‰´äººç±»è®¤çŸ¥çš„â€œåŒè¿‡ç¨‹ç†è®ºâ€ï¼ˆå¿«æ€è€ƒä¸æ…¢æ€è€ƒï¼‰ã€‚\n- **æŠ€æœ¯å®ç°**ï¼š**AdaCoT** æœºåˆ¶ã€‚æ¨¡å‹åœ¨æ¯ä¸€æ­¥æ¨ç†å‰ï¼Œä¼šå…ˆé¢„æµ‹ä¸€ä¸ªæŒ‡ç¤ºç¬¦ï¼ˆ`<think_on>` æˆ– `<think_off>`ï¼‰ã€‚\n    - **`<think_off>`**ï¼šè§¦å‘â€œå¿«æ€è€ƒâ€ï¼Œæ¨¡å‹ç›´æ¥è¾“å‡ºåŠ¨ä½œï¼Œä¿è¯å®æ—¶æ€§å’Œæ•ˆç‡ã€‚\n    - **`<think_on>`**ï¼šè§¦å‘â€œæ…¢æ€è€ƒâ€ï¼Œæ¨¡å‹ä¼šç”Ÿæˆè¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ï¼ˆåŒ…å«åœ¨ `<think>` æ ‡ç­¾å†…ï¼‰ï¼Œå¯¹ç¯å¢ƒã€ä»»åŠ¡ã€å†å²è¿›è¡Œæ·±åº¦åˆ†æï¼Œç„¶åå†è¾“å‡ºåŠ¨ä½œã€‚\n- **æ ¸å¿ƒä»·å€¼**ï¼š**åŠ¨æ€æƒè¡¡æ•ˆç‡ä¸æ€§èƒ½**ã€‚ä»…åœ¨ä»»åŠ¡å¤æ‚ã€å­˜åœ¨æ­§ä¹‰æˆ–éœ€è¦å…³é”®å†³ç­–æ—¶è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œé¿å…äº†æ¯ä¸€æ­¥éƒ½æ¨ç†å¸¦æ¥çš„è®¡ç®—å¼€é”€ï¼Œå®ç°äº†æ™ºèƒ½åŒ–çš„â€œè®¡ç®—èµ„æºâ€åˆ†é…ã€‚\n\n#### **2. è§†è§‰è¾…åŠ©è¯­è¨€è®°å¿†æ¨¡å—**\n- **çµæ„Ÿæ¥æº**ï¼šè§£å†³é•¿è§†é‡ä»»åŠ¡ä¸­çš„ä¿¡æ¯é—å¿˜é—®é¢˜ã€‚\n- **æŠ€æœ¯å®ç°**ï¼š**VLingMem** æ¨¡å—ã€‚å½“æ¨¡å‹è¿›è¡Œæ…¢æ€è€ƒï¼ˆ`<think_on>`ï¼‰æ—¶ï¼Œé™¤äº†ç”Ÿæˆæ¨ç†é“¾ï¼Œè¿˜ä¼šç”Ÿæˆä¸€ä¸ªå¯¹å½“å‰è§‚æµ‹çš„**è¯­è¨€æ‘˜è¦**ï¼ˆåŒ…å«åœ¨ `<summary>` æ ‡ç­¾å†…ï¼‰ã€‚\n    - **è®°å¿†å½¢å¼**ï¼šæ‘˜è¦è¢«è½¬åŒ–ä¸ºè¯­è¨€ä»¤ç‰Œï¼Œä½œä¸º**æŒä¹…åŒ–çš„ã€è·¨æ¨¡æ€çš„è¯­ä¹‰è®°å¿†**ï¼Œè¾“å…¥åˆ°åç»­æ—¶é—´æ­¥çš„æ¨¡å‹ä¸­ã€‚\n    - **è§†è§‰è¾…åŠ©**ï¼šè®°å¿†çš„ç”Ÿæˆå’Œè°ƒç”¨è¿‡ç¨‹ä¼šç»“åˆå…³é”®è§†è§‰ç‰¹å¾ï¼Œå¢å¼ºå…¶å‡†ç¡®æ€§å’Œç©ºé—´æ„ŸçŸ¥ã€‚\n- **æ ¸å¿ƒä»·å€¼**ï¼šæ„å»ºäº†**å¯è§£é‡Šã€æŠ—è¡°å‡çš„é•¿æœŸè®°å¿†**ã€‚ä½¿æœºå™¨äººèƒ½è®°ä½â€œå·²æ¢ç´¢è¿‡åŒºåŸŸâ€ã€â€œç›®æ ‡å¤§è‡´æ–¹å‘â€ã€â€œç¯å¢ƒåŠ¨æ€å˜åŒ–â€ç­‰å…³é”®è¯­ä¹‰ä¿¡æ¯ï¼Œæœ‰æ•ˆé¿å…é‡å¤æ¢ç´¢ï¼Œæå‡é•¿ç¨‹å¯¼èˆªçš„è¿è´¯æ€§ã€‚\n\n#### **3. åˆ›æ–°çš„æ•°æ®ä¸è®­ç»ƒèŒƒå¼**\n- **å¤§è§„æ¨¡æ•°æ®é›†**ï¼šæ„å»ºäº† **Nav-AdaCoT-2.9M**ï¼Œè¿™æ˜¯ç›®å‰æœ€å¤§çš„å¸¦æœ‰**è‡ªé€‚åº”CoTæ ‡æ³¨**çš„å…·èº«å¯¼èˆªæ•°æ®é›†ã€‚å®ƒä¸ä»…æ•™æ¨¡å‹â€œæ€è€ƒä»€ä¹ˆâ€ï¼Œæ›´é‡è¦çš„æ˜¯æ•™æ¨¡å‹â€œä½•æ—¶éœ€è¦æ€è€ƒâ€ã€‚\n- **ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹**ï¼š\n    1.  **é¢„è®­ç»ƒ**ï¼šåœ¨å¼€æ”¾ä¸–ç•Œè§†é¢‘æ•°æ®ä¸Šè®­ç»ƒï¼Œèµ‹äºˆæ¨¡å‹åŸºç¡€çš„è§†è§‰æ¨ç†å’Œè‡ªé€‚åº”æ€è€ƒèƒ½åŠ›ã€‚\n    2.  **ç›‘ç£å¾®è°ƒ**ï¼šåœ¨å¯¼èˆªæ•°æ®å’Œå¼€æ”¾ä¸–ç•Œæ•°æ®ä¸Šè”åˆè®­ç»ƒï¼Œå­¦ä¹ å…·ä½“çš„å¯¼èˆªæŠ€èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚\n    3.  **åœ¨çº¿ä¸“å®¶å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒ**ï¼šè¿™æ˜¯çªç ´æ¨¡ä»¿å­¦ä¹ ç“¶é¢ˆçš„å…³é”®ã€‚\n        - **æ··åˆç­–ç•¥**ï¼šç»“åˆâ€œæœ´ç´ ç­–ç•¥ rolloutâ€ï¼ˆæ”¶é›†æˆåŠŸè½¨è¿¹ï¼‰å’Œâ€œä¸“å®¶å¼•å¯¼ rolloutâ€ï¼ˆåœ¨æ™ºèƒ½ä½“å¡ä½æˆ–å¤±è´¥æ—¶ï¼Œç”±æœ€çŸ­è·¯å¾„è§„åˆ’å™¨ä»‹å…¥æ¼”ç¤ºï¼‰ã€‚\n        - **æŸå¤±å‡½æ•°**ï¼šç»“åˆæ¨¡ä»¿å­¦ä¹ æŸå¤±ï¼ˆ`â„’_SFT`ï¼‰å’Œ PPO é£æ ¼çš„å¼ºåŒ–å­¦ä¹ æŸå¤±ï¼ˆ`â„’_RL`ï¼‰ï¼Œåœ¨é¼“åŠ±æ¢ç´¢çš„åŒæ—¶ç¨³å®šè®­ç»ƒã€‚\n        - **åŠ¨ä½œæ¨¡å‹**ï¼šé‡‡ç”¨è½»é‡åŒ–çš„**æ¦‚ç‡è¿ç»­åŠ¨ä½œæ¨¡å‹**ï¼ˆMLPé¢„æµ‹é«˜æ–¯åˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®ï¼‰ï¼Œå¹³è¡¡äº†è¿ç»­æ§åˆ¶çš„é«˜ç²¾åº¦ä¸æ¨ç†æ•ˆç‡ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆæ€»ç»“**\nVLingNav é€šè¿‡ **â€œè‡ªé€‚åº”æ¨ç†â€ + â€œæŒä¹…è¯­ä¹‰è®°å¿†â€ + â€œæ··åˆå¼ºåŒ–å­¦ä¹ â€** ä¸‰ä½ä¸€ä½“çš„æ–¹å¼ï¼Œç³»ç»Ÿæ€§åœ°è§£å†³äº†VLAå¯¼èˆªæ¨¡å‹åœ¨**è®¤çŸ¥æ·±åº¦ã€å†å²åˆ©ç”¨å’Œç­–ç•¥ä¼˜åŒ–**æ–¹é¢çš„ä¸è¶³ã€‚\n\n1.  **è§£å†³â€œæ€ä¹ˆæƒ³â€å’Œâ€œä½•æ—¶æƒ³â€**ï¼šé€šè¿‡ AdaCoTï¼Œè®©æ¨¡å‹åƒäººä¸€æ ·åœ¨ç®€å•åœºæ™¯å¿«é€Ÿååº”ï¼Œåœ¨å¤æ‚åœºæ™¯æ·±æ€ç†Ÿè™‘ã€‚\n2.  **è§£å†³â€œè®°ä¸ä½â€**ï¼šé€šè¿‡ VLingMemï¼Œå°†å…³é”®å†å²ä¿¡æ¯ä»¥è¯­è¨€å½¢å¼å›ºåŒ–ï¼Œå½¢æˆæœ‰æ•ˆçš„ä»»åŠ¡å·¥ä½œè®°å¿†ã€‚\n3.  **è§£å†³â€œå­¦ä¸å¥½â€**ï¼šé€šè¿‡æ„å»ºé«˜è´¨é‡è‡ªé€‚åº”CoTæ•°æ®é›†å’Œå¼•å…¥åœ¨çº¿RLåè®­ç»ƒï¼Œè®©æ¨¡å‹ä¸ä»…èƒ½æ¨¡ä»¿ä¸“å®¶ï¼Œè¿˜èƒ½é€šè¿‡ä¸ç¯å¢ƒäº¤äº’è‡ªæˆ‘ä¼˜åŒ–ï¼Œå‘ç°æ›´é²æ£’ã€æ›´é«˜æ•ˆçš„ç­–ç•¥ã€‚\n\n### **å››ã€ å®é™…ä»·å€¼ä¸éªŒè¯**\n- **æ€§èƒ½å“è¶Š**ï¼šåœ¨å¤šä¸ªæ ‡å‡†å…·èº«å¯¼èˆªåŸºå‡†æµ‹è¯•ï¼ˆObjectNav, ImageNav, Embodied Visual Trackingï¼‰ä¸Šè¾¾åˆ°**æœ€å…ˆè¿›æ°´å¹³**ï¼Œå°¤å…¶åœ¨é•¿è§†é‡å’Œå¼€æ”¾è¯æ±‡ä»»åŠ¡ä¸Šä¼˜åŠ¿æ˜æ˜¾ã€‚\n- **é›¶æ ·æœ¬ç°å®è¿ç§»**ï¼š**æœªç»ä»»ä½•ç°å®æ•°æ®å¾®è°ƒ**ï¼Œå³å¯ç›´æ¥éƒ¨ç½²åˆ°çœŸå®å››è¶³æœºå™¨äººä¸Šï¼ŒæˆåŠŸå®Œæˆå®¶åº­ã€åŠå…¬å®¤ã€æˆ·å¤–ç­‰å¤šç§åœºæ™¯çš„å¤æ‚å¯¼èˆªä»»åŠ¡ï¼Œè¯æ˜äº†å…¶å¼ºå¤§çš„**è·¨é¢†åŸŸã€è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›**ã€‚\n- **é«˜å®ç”¨æ€§**ï¼šæ¨¡å‹æ¨ç†å»¶è¿Ÿä½ï¼ˆ<300msï¼‰ï¼Œæ”¯æŒé•¿åºåˆ—è§†é¢‘è¾“å…¥ï¼Œå…·å¤‡å®é™…æœºå™¨äººéƒ¨ç½²çš„å¯è¡Œæ€§ã€‚\n\n**ç»“è®º**ï¼šVLingNav ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ€§èƒ½æ›´å¼ºçš„å¯¼èˆªæ¨¡å‹ï¼Œå®ƒæ›´æä¾›äº†ä¸€ç§å°†**äººç±»è®¤çŸ¥åŸç†**ï¼ˆåŒè¿‡ç¨‹ç†è®ºã€å·¥ä½œè®°å¿†ï¼‰ä¸**å…ˆè¿›æœºå™¨å­¦ä¹ èŒƒå¼**ï¼ˆVLAã€RLï¼‰æ·±åº¦èåˆçš„æ¡†æ¶ï¼Œä¸ºæ„å»ºæ›´æ™ºèƒ½ã€å¯è§£é‡Šã€èƒ½é€‚åº”å¼€æ”¾ä¸–ç•Œçš„å…·èº«æ™ºèƒ½ä½“æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "timestamp": "2026-01-20T17:51:09.664323",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
      "answer": "## VLingNav åŠå…¶å‰ä½œå‘å±•è„‰ç»œåˆ†æ\n\næœ¬æ–‡æå‡ºçš„ **VLingNav** æ˜¯è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å…·èº«å¯¼èˆªé¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚è¦ç†è§£å…¶åˆ›æ–°ä¸ä»·å€¼ï¼Œéœ€æ¢³ç†è¯¥æ–¹å‘çš„å‘å±•è„‰ç»œã€‚å…¶æ¼”è¿›ä¸»è¦å›´ç»• **ä¸‰ä¸ªæ ¸å¿ƒå‘å±•æ–¹å‘**ï¼š**æ¨¡å‹æ¶æ„çš„æ¼”è¿›**ã€**æ¨ç†èƒ½åŠ›çš„å¢å¼º**ã€**è®­ç»ƒèŒƒå¼çš„ä¼˜åŒ–**ã€‚æ¯ä¸€æ­¥æ”¹è¿›éƒ½æ—¨åœ¨è§£å†³å‰ä»£å·¥ä½œçš„å…³é”®ç“¶é¢ˆã€‚\n\n### å‘å±•è„‰ç»œæ¢³ç†\n\n#### æ–¹å‘ä¸€ï¼šæ¨¡å‹æ¶æ„çš„æ¼”è¿› (ä»æ¨¡å—åŒ–åˆ°ç«¯åˆ°ç«¯VLA)\n\nè¿™ä¸€æ–¹å‘å…³æ³¨å¦‚ä½•å°†æ„ŸçŸ¥ã€è§„åˆ’ã€æ§åˆ¶æ›´æœ‰æ•ˆåœ°æ•´åˆã€‚\n\n1.  **ä¼ ç»Ÿæ¨¡å—åŒ–æ–¹æ³• (å¦‚ [8, 65, 9])**\n    *   **ç‰¹ç‚¹**ï¼šå°†å¯¼èˆªåˆ†è§£ä¸ºæ„ŸçŸ¥ã€å»ºå›¾(SLAM)ã€è·¯å¾„è§„åˆ’ç­‰ç‹¬ç«‹æ¨¡å—ï¼Œä¾èµ–æ‰‹å·¥è®¾è®¡çš„æ¥å£ã€‚\n    *   **æ”¹è¿›å‰**ï¼šä¾èµ–å›ºå®šè§„åˆ™ï¼Œæ¨¡å—é—´è¯¯å·®ç´¯ç§¯ï¼Œåœ¨åŠ¨æ€å¤æ‚ç¯å¢ƒä¸­é€‚åº”æ€§å·®ã€‚\n    *   **ä»·å€¼**ï¼šé«˜å¯è§£é‡Šæ€§ï¼Œé›¶æ ·æœ¬è¿ç§»èƒ½åŠ›å¼ºã€‚\n\n2.  **æ—©æœŸç«¯åˆ°ç«¯å­¦ä¹ æ¨¡å‹ (å¦‚ [42, 43, 66])**\n    *   **ç‰¹ç‚¹**ï¼šç”¨æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œç›´æ¥ä»ä¼ æ„Ÿå™¨è¾“å…¥æ˜ å°„åˆ°åŠ¨ä½œã€‚\n    *   **æ”¹è¿›**ï¼šå‡å°‘äº†æ‰‹å·¥æ¥å£ï¼Œç¼“è§£äº†ä¿¡æ¯æŸå¤±ã€‚\n    *   **å±€é™**ï¼šæ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œåˆ†å¸ƒå¤–åœºæ™¯æ˜“äº§ç”Ÿå¼‚å¸¸åŠ¨ä½œã€‚\n\n3.  **ç¬¬ä¸€ä»£å¯¼èˆªVLAæ¨¡å‹ (å¦‚ NaVid [73])**\n    *   **ç‰¹ç‚¹**ï¼šé¦–æ¬¡å°†é¢„è®­ç»ƒçš„å¤§è§„æ¨¡è§†è§‰-è¯­è¨€æ¨¡å‹ä½œä¸ºä¸»å¹²ï¼Œç”¨äºå…·èº«å¯¼èˆªã€‚\n    *   **æ”¹è¿›**ï¼šç»§æ‰¿äº†VLMå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œåœºæ™¯ç†è§£èƒ½åŠ›ã€‚\n    *   **å±€é™**ï¼šæ¨ç†æ—¶å»¶éšè§†é¢‘æµå¢é•¿è€Œæ˜¾è‘—å¢åŠ ï¼Œéš¾ä»¥å®æ—¶éƒ¨ç½²ã€‚\n\n4.  **é«˜æ•ˆè§†é¢‘å¤„ç†VLAæ¨¡å‹ (å¦‚ Uni-NaVid [75], NaVILA [10])**\n    *   **ç‰¹ç‚¹**ï¼šå¼•å…¥è§†é¢‘æµå‹ç¼©ã€KVç¼“å­˜ç­‰æŠ€æœ¯ã€‚\n    *   **æ”¹è¿›**ï¼šåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œå¤§å¹…æå‡äº†æ¨ç†é€Ÿåº¦ï¼Œä½¿å®æ—¶éƒ¨ç½²æˆä¸ºå¯èƒ½ã€‚\n    *   **å±€é™**ï¼šåŠ¨ä½œè¡¨ç¤ºä¸ºç¦»æ•£ä»¤ç‰Œï¼ŒåŠ¨ä½œè´¨é‡å’Œå¹³æ»‘æ€§å—é™ã€‚\n\n5.  **è¿ç»­åŠ¨ä½œé¢„æµ‹VLAæ¨¡å‹ (å¦‚ TrackVLA [52])**\n    *   **ç‰¹ç‚¹**ï¼šé‡‡ç”¨åŸºäºé”šç‚¹çš„æ‰©æ•£ç­–ç•¥æˆ–ç±»ä¼¼æ–¹æ³•ï¼Œç›´æ¥è¾“å‡ºè¿ç»­çš„è¿åŠ¨è½¨è¿¹ã€‚\n    *   **æ”¹è¿›**ï¼šæ˜¾è‘—æå‡äº†åŠ¨ä½œè´¨é‡å’Œåœ¨åŠ¨æ€åœºæ™¯ä¸­çš„é€‚åº”æ€§ã€‚\n    *   **å±€é™**ï¼šæ‰©æ•£æ¨¡å‹è¿­ä»£å»å™ªå¯¼è‡´è®¡ç®—æˆæœ¬é«˜ï¼Œæ¨ç†é€Ÿåº¦æ…¢ã€‚\n\n6.  **VLingNav (æœ¬æ–‡)**\n    *   **ç‰¹ç‚¹**ï¼šé‡‡ç”¨**è½»é‡çº§MLPåŠ¨ä½œå¤´**ï¼Œè¾“å‡ºå‚æ•°åŒ–çš„é«˜æ–¯åˆ†å¸ƒæ¥é¢„æµ‹è¿ç»­è½¨è¿¹ã€‚\n    *   **æ”¹è¿›**ï¼šåœ¨**é«˜ç²¾åº¦è¿ç»­æ§åˆ¶**å’Œ**é«˜æ•ˆæ¨ç†**ä¹‹é—´å–å¾—äº†æ›´å¥½å¹³è¡¡ï¼Œé¿å…äº†æ‰©æ•£æ¨¡å‹çš„è¿­ä»£å¼€é”€ã€‚\n    *   **åˆ›æ–°æ¶æ„ç»„ä»¶**ï¼š\n        *   **åŠ¨æ€FPSé‡‡æ ·ä¸ç½‘æ ¼æ± åŒ–**ï¼šæ¨¡æ‹Ÿäººç±»è®°å¿†é—å¿˜æ›²çº¿ï¼Œæ™ºèƒ½ç®¡ç†å†å²è§†è§‰ä¿¡æ¯ï¼Œå¹³è¡¡è®¡ç®—è´Ÿæ‹…ä¸ä¿¡æ¯ä¿ç•™ã€‚\n        *   **æ—¶é—´æ„ŸçŸ¥æŒ‡ç¤ºä»¤ç‰Œ**ï¼šæ³¨å…¥æ—¶é—´é—´éš”ä¿¡æ¯ï¼Œè§£å†³éå‡åŒ€é‡‡æ ·å¸¦æ¥çš„æ—¶åºä¸ä¸€è‡´é—®é¢˜ã€‚\n\n#### æ–¹å‘äºŒï¼šæ¨ç†èƒ½åŠ›çš„å¢å¼º (ä»ååº”å¼æ˜ å°„åˆ°è‡ªé€‚åº”æ¨ç†)\n\nè¿™ä¸€æ–¹å‘å…³æ³¨å¦‚ä½•è®©æ¨¡å‹ä¸ä»…â€œçœ‹åˆ°å°±åŠ¨â€ï¼Œè¿˜èƒ½â€œæ€è€ƒåå†åŠ¨â€ã€‚\n\n1.  **ååº”å¼VLAæ¨¡å‹ (å¤šæ•°æ—©æœŸå·¥ä½œ)**\n    *   **ç‰¹ç‚¹**ï¼šç›´æ¥ä»è§‚æµ‹æ˜ å°„åˆ°åŠ¨ä½œï¼Œç¼ºä¹æ˜¾å¼æ¨ç†ã€‚\n    *   **å±€é™**ï¼šæ— æ³•å¤„ç†å¤æ‚ã€æ¨¡ç³Šæˆ–é•¿è§†é‡ä»»åŠ¡ï¼Œå†³ç­–è¿‡ç¨‹ä¸é€æ˜ã€‚\n\n2.  **å›ºå®šé¢‘ç‡çš„CoTæ¨¡å‹ (å¦‚ OctoNav [14])**\n    *   **ç‰¹ç‚¹**ï¼šåœ¨å¯¼èˆªä¸­å¼•å…¥æ€ç»´é“¾æ¨ç†ï¼Œä½†ä»¥å›ºå®šæ—¶é—´é—´éš”è§¦å‘ã€‚\n    *   **æ”¹è¿›**ï¼šæä¾›äº†å†³ç­–çš„å¯è§£é‡Šæ€§ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šæœ‰åŠ©äºå¤æ‚è§„åˆ’ã€‚\n    *   **å±€é™**ï¼š**æ— æ³•åŠ¨æ€è°ƒæ•´**ã€‚ç®€å•åœºæ™¯ä¸‹å†—ä½™æ€è€ƒé™ä½æ•ˆç‡ï¼›å¤æ‚åœºæ™¯å¯èƒ½æ€è€ƒä¸è¶³ã€‚\n\n3.  **VLingNav (æœ¬æ–‡)ï¼šè‡ªé€‚åº”æ€ç»´é“¾**\n    *   **ç‰¹ç‚¹**ï¼šæå‡º **AdaCoT** æœºåˆ¶ï¼Œå—äººç±»è®¤çŸ¥åŒè¿‡ç¨‹ç†è®ºå¯å‘ã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼š\n        *   **åŠ¨æ€è§¦å‘**ï¼šæ¨¡å‹è‡ªä¸»é¢„æµ‹ `<think_on>` æˆ– `<think_off>` ä»¤ç‰Œï¼Œä»…åœ¨å¿…è¦æ—¶ï¼ˆå¦‚é‡åˆ°æ­§ä¹‰ã€å…³é”®å†³ç­–ç‚¹ï¼‰è¿›è¡Œæ…¢æ€è€ƒï¼ˆè¯¦ç»†æ¨ç†ï¼‰ï¼Œå¦åˆ™è¿›è¡Œå¿«æ€è€ƒï¼ˆç›´æ¥è¾“å‡ºåŠ¨ä½œï¼‰ã€‚\n        *   **å†…å®¹ç»“æ„åŒ–**ï¼šCoTå†…å®¹åŒ…å«ç¯å¢ƒæ„ŸçŸ¥ã€ä»»åŠ¡åˆ†æã€å†å²è®¿é—®åˆ¤æ–­ç­‰ï¼Œå¹¶ç”Ÿæˆ**è¯­è¨€è®°å¿†æ‘˜è¦**ã€‚\n    *   **ä»·å€¼**ï¼šå®ç°äº†**æ•ˆç‡ä¸æ€§èƒ½çš„æœ€ä¼˜æƒè¡¡**ã€‚å®éªŒè¡¨æ˜ï¼Œä»…åœ¨å¤§çº¦ **2.1%** çš„æ­¥éª¤ä¸­è§¦å‘è¯¦ç»†æ¨ç†ï¼Œå´èƒ½å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œæ˜¯è¿ˆå‘â€œç±»äººâ€å¼¹æ€§è®¤çŸ¥çš„å…³é”®ä¸€æ­¥ã€‚\n\n#### æ–¹å‘ä¸‰ï¼šè®­ç»ƒèŒƒå¼çš„ä¼˜åŒ– (ä»çº¯æ¨¡ä»¿å­¦ä¹ åˆ°æ··åˆä¼˜åŒ–)\n\nè¿™ä¸€æ–¹å‘å…³æ³¨å¦‚ä½•åˆ©ç”¨æ›´ä¼˜çš„æ•°æ®å’Œè®­ç»ƒæ–¹æ³•ï¼Œè§£é”æ¨¡å‹æ½œåŠ›ã€‚\n\n1.  **çº¯æ¨¡ä»¿å­¦ä¹  (SFT)**\n    *   **ç‰¹ç‚¹**ï¼šåœ¨ä¸“å®¶æ¼”ç¤ºæ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒã€‚\n    *   **å±€é™**ï¼šå—é™äºæ¼”ç¤ºæ•°æ®è´¨é‡å’Œè¦†ç›–èŒƒå›´ï¼Œæ˜“å‡ºç°åå˜é‡åç§»ã€å› æœæ··æ·†ï¼Œæ— æ³•è¶…è¶Šä¸“å®¶ç­–ç•¥ã€‚\n\n2.  **å¼•å…¥æ€ç»´é“¾æ ‡æ³¨æ•°æ® (å¦‚ Aux-Think [53], Nav-CoT-110K [31])**\n    *   **ç‰¹ç‚¹**ï¼šæ„å»ºå¸¦æœ‰CoTæ ‡æ³¨çš„å¯¼èˆªæ•°æ®é›†ã€‚\n    *   **æ”¹è¿›**ï¼šæ¿€å‘äº†VLMå›ºæœ‰çš„æ¨ç†èƒ½åŠ›ï¼Œå°†æ„ŸçŸ¥ã€è¯­è¨€ã€åŠ¨ä½œæ›´ç´§å¯†åœ°å¯¹é½ã€‚\n    *   **å±€é™**ï¼šæ•°æ®è§„æ¨¡æœ‰é™ï¼Œä¸”CoTæ¨¡å¼å›ºå®šã€‚\n\n3.  **VLingNav (æœ¬æ–‡)ï¼šå¤§è§„æ¨¡è‡ªé€‚åº”CoTæ•°æ® + åœ¨çº¿ä¸“å®¶å¼•å¯¼RL**\n    *   **æ•°æ®å±‚é¢çš„æ”¹è¿›**ï¼š\n        *   **æ„å»º Nav-AdaCoT-2.9M**ï¼šè¿„ä»Šæœ€å¤§çš„å…·èº«å¯¼èˆªæ¨ç†æ ‡æ³¨æ•°æ®é›†ã€‚\n        *   **æ ‡æ³¨åˆ›æ–°**ï¼šä¸ä»…æä¾›â€œæƒ³ä»€ä¹ˆâ€ï¼ˆæ¨ç†å†…å®¹ï¼‰ï¼Œè¿˜é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹æ•™å¯¼æ¨¡å‹â€œä½•æ—¶æƒ³â€ï¼ˆè‡ªé€‚åº”è§¦å‘ï¼‰ã€‚\n        *   **å¤šä»»åŠ¡ä¸å¼€æ”¾ä¸–ç•Œæ•°æ®ååŒè®­ç»ƒ**ï¼šèåˆç‰©ä½“å¯¼èˆªã€è§†è§‰è·Ÿè¸ªã€å›¾åƒç›®æ ‡å¯¼èˆªä¸‰å¤§ä»»åŠ¡æ•°æ®ï¼Œå¹¶åŠ å…¥å¼€æ”¾ä¸–ç•Œè§†é¢‘æ•°æ®ï¼Œæå¤§å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè·¨æ¨¡æ€åŸºç¡€ã€‚\n    *   **è®­ç»ƒç­–ç•¥çš„æ”¹è¿›**ï¼š\n        *   **åœ¨çº¿ä¸“å®¶å¼•å¯¼å¼ºåŒ–å­¦ä¹ **ï¼šåœ¨SFTåï¼Œå¢åŠ åœ¨çº¿RLå¾®è°ƒé˜¶æ®µã€‚\n        *   **æ··åˆæ»šåŠ¨ç­–ç•¥**ï¼šç»“åˆ**æœ´ç´ æ»šåŠ¨**ï¼ˆæ”¶é›†æˆåŠŸè½¨è¿¹ï¼‰å’Œ**ä¸“å®¶å¼•å¯¼æ»šåŠ¨**ï¼ˆåœ¨æ™ºèƒ½ä½“å¡ä½æˆ–å¤±è´¥æ—¶ï¼Œç”±æœ€çŸ­è·¯å¾„è§„åˆ’å™¨ä»‹å…¥æ¼”ç¤ºï¼‰ï¼Œæ„å»ºæ··åˆç¼“å†²æ± ã€‚\n        *   **å¤åˆæŸå¤±å‡½æ•°**ï¼šä¼˜åŒ–ç»“åˆäº†PPOé£æ ¼ç­–ç•¥æ¢¯åº¦æŸå¤±å’Œæ¨¡ä»¿å­¦ä¹ æŸå¤±çš„å¤åˆç›®æ ‡ã€‚\n    *   **ä»·å€¼**ï¼šä½¿æ¨¡å‹ä»â€œæ¨¡ä»¿è€…â€è½¬å˜ä¸ºâ€œé—®é¢˜è§£å†³è€…â€ã€‚èƒ½å¤Ÿæ¢ç´¢æ¯”æ¼”ç¤ºæ›´ä¼˜æˆ–æ›´é²æ£’çš„ç­–ç•¥ï¼Œæœ‰æ•ˆå…‹æœäº†çº¯æ¨¡ä»¿å­¦ä¹ çš„å±€é™ï¼Œæ˜¯æ€§èƒ½è¾¾åˆ°SOTAçš„å…³é”®ã€‚\n\n### æ€»ç»“\n\nVLingNav çš„æˆåŠŸå¹¶éå•ä¸€æŠ€æœ¯çš„çªç ´ï¼Œè€Œæ˜¯æ²¿ç€ä¸Šè¿°ä¸‰ä¸ªå‘å±•æ–¹å‘ï¼Œå¯¹å‰äººå·¥ä½œè¿›è¡Œç³»ç»Ÿæ€§é›†æˆä¸åˆ›æ–°çš„ç»“æœï¼š\n1.  **åœ¨æ¶æ„ä¸Š**ï¼Œå®ƒé€‰æ‹©äº†æ›´é«˜æ•ˆçš„è¿ç»­åŠ¨ä½œé¢„æµ‹æ–¹æ¡ˆï¼Œå¹¶è®¾è®¡äº†æ™ºèƒ½çš„è®°å¿†ç®¡ç†æ¨¡å—ã€‚\n2.  **åœ¨æ¨ç†ä¸Š**ï¼Œå®ƒé¦–åˆ›äº†è‡ªé€‚åº”CoTï¼Œå®ç°äº†æŒ‰éœ€æ€è€ƒçš„ç±»äººè®¤çŸ¥å¼¹æ€§ã€‚\n3.  **åœ¨è®­ç»ƒä¸Š**ï¼Œå®ƒé€šè¿‡æ„å»ºè¶…å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„**è‡ªé€‚åº”CoTæ•°æ®é›†**ï¼Œå¹¶å¼•å…¥**åœ¨çº¿ä¸“å®¶å¼•å¯¼RL**ï¼Œå½»åº•é‡Šæ”¾äº†VLAæ¨¡å‹åœ¨å¯¼èˆªä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚\n\næœ€ç»ˆï¼ŒVLingNav å±•ç°å‡ºçš„**å¤šä»»åŠ¡ç»Ÿä¸€æ€§èƒ½**ã€**å“è¶Šçš„é›¶æ ·æœ¬çœŸå®ä¸–ç•Œè¿ç§»èƒ½åŠ›**ä»¥åŠ**æ–°å…´çš„è·¨ä»»åŠ¡ç»„åˆèƒ½åŠ›**ï¼Œæ ‡å¿—ç€å…·èº«å¯¼èˆªVLAæ¨¡å‹ä»â€œå¯ç”¨â€å‘â€œé€šç”¨ã€é«˜æ•ˆã€å¯é â€è¿ˆå‡ºäº†åšå®çš„ä¸€æ­¥ã€‚å…¶å·¥ä½œä¹Ÿä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ï¼šç»§ç»­æ·±åŒ–å¼¹æ€§è®¤çŸ¥æ¶æ„ã€æ¢ç´¢æ›´ä¼˜çš„å¤šæ¨¡æ€è®°å¿†ã€ä»¥åŠå¼€å‘æ›´é«˜æ•ˆçš„ä»¿çœŸåˆ°çœŸå®è¿ç§»æ–¹æ³•ã€‚",
      "timestamp": "2026-01-20T17:52:07.981775",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
      "answer": "## VLingNav çš„å‰ä½œåˆ†æ\n\nVLingNav çš„ç ”ç©¶å›¢é˜Ÿï¼ˆä¸»è¦æ¥è‡ªå­—èŠ‚è·³åŠ¨å’ŒåŒ—äº¬å¤§å­¦ï¼‰åœ¨è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹å’Œå…·èº«å¯¼èˆªé¢†åŸŸæœ‰ä¸€ç³»åˆ—ç›¸å…³çš„å‰æœŸå·¥ä½œã€‚è¿™äº›å·¥ä½œä¸º VLingNav çš„æå‡ºå¥ å®šäº†åŸºç¡€ã€‚ä»¥ä¸‹è¡¨æ ¼è¯¦ç»†æ¢³ç†äº†å…¶æ ¸å¿ƒå‰ä½œï¼Œå¹¶é‡ç‚¹å¯¹æ¯”äº† VLingNav ä¸è¿™äº›å‰ä½œåœ¨æŠ€æœ¯è·¯çº¿å’Œå…·ä½“æ”¹è¿›ä¸Šçš„åŒºåˆ«ã€‚\n\n| è®ºæ–‡åç§° | å‘è¡¨å¹´ä»½/çŠ¶æ€ | æ ¸å¿ƒè´¡çŒ®ä¸æŠ€æœ¯ç‰¹ç‚¹ | ä¸ VLingNav çš„ä¸»è¦åŒºåˆ«åŠ VLingNav çš„æ”¹è¿› |\n| :--- | :--- | :--- | :--- |\n| **NaVid** (Video-based VLM Plans the Next Step for Vision-and-Language Navigation) | 2024 (RSS) | **é¦–ä¸ªåŸºäºè§†é¢‘çš„VLAå¯¼èˆªæ¨¡å‹**ã€‚å°†å¯¼èˆªä»»åŠ¡æ„å»ºä¸ºè§†é¢‘é—®ç­”é—®é¢˜ï¼Œä½¿ç”¨è§†é¢‘VLMï¼ˆLLaVA-Videoï¼‰ä½œä¸ºä¸»å¹²ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å­¦ä¹ ä»è§†è§‰è§‚å¯Ÿå’Œè¯­è¨€æŒ‡ä»¤åˆ°ç¦»æ•£åŠ¨ä½œï¼ˆå¦‚â€œå‰è¿›â€ã€â€œå·¦è½¬â€ï¼‰çš„æ˜ å°„ã€‚å±•ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼š<br>1. **æ¨ç†æœºåˆ¶**ï¼šNaVid æ˜¯**ååº”å¼**æ¨¡å‹ï¼Œç¼ºä¹æ˜¾å¼çš„ã€å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ã€‚VLingNav å¼•å…¥äº†**è‡ªé€‚åº”æ€ç»´é“¾ï¼ˆAdaCoTï¼‰**ï¼Œä½¿æ¨¡å‹å…·å¤‡åŠ¨æ€è§¦å‘æ˜¾å¼æ¨ç†çš„èƒ½åŠ›ï¼Œæå‡äº†å†³ç­–çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚<br>2. **è®°å¿†æœºåˆ¶**ï¼šNaVid ä»…é€šè¿‡è¾“å…¥å†å²è§†é¢‘å¸§æä¾›**éšå¼è§†è§‰è®°å¿†**ï¼Œä¿¡æ¯æ˜“è¡°å‡ã€‚VLingNav è®¾è®¡äº†**è§†è§‰è¾…åŠ©è¯­è¨€è®°å¿†ï¼ˆVLingMemï¼‰**ï¼Œæ„å»ºäº†æŒä¹…ã€å¯å›é¡¾çš„è·¨æ¨¡æ€è¯­ä¹‰è®°å¿†ï¼Œæœ‰æ•ˆé˜²æ­¢é‡å¤æ¢ç´¢ã€‚<br>3. **åŠ¨ä½œè¾“å‡º**ï¼šNaVid è¾“å‡º**ç¦»æ•£åŠ¨ä½œä»¤ç‰Œ**ï¼Œæ§åˆ¶ç²¾åº¦å’Œæµç•…æ€§æœ‰é™ã€‚VLingNav é€šè¿‡MLPåŠ¨ä½œæ¨¡å‹ç›´æ¥è¾“å‡º**è¿ç»­è½¨è¿¹**ï¼Œæ§åˆ¶æ›´ç²¾ç»†ã€é€‚åº”åŠ¨æ€åœºæ™¯èƒ½åŠ›æ›´å¼ºã€‚<br>4. **è®­ç»ƒèŒƒå¼**ï¼šNaVid ä»…ä½¿ç”¨**æ¨¡ä»¿å­¦ä¹ ï¼ˆSFTï¼‰**ã€‚VLingNav å¢åŠ äº†**åœ¨çº¿ä¸“å®¶å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒ**é˜¶æ®µï¼Œä½¿ç­–ç•¥èƒ½è¶…è¶Šæ¼”ç¤ºæ•°æ®ï¼Œè‡ªæˆ‘ä¼˜åŒ–ï¼Œè·å¾—æ›´é²æ£’çš„è¡Œä¸ºã€‚ |\n| **Uni-NaVid** (A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks) | 2025 (RSS) | **å¤šä»»åŠ¡ç»Ÿä¸€çš„VLAå¯¼èˆªæ¨¡å‹**ã€‚åœ¨NaVidåŸºç¡€ä¸Šï¼Œå¼•å…¥äº†**è§†é¢‘æµå‹ç¼©æœºåˆ¶**ä»¥æ§åˆ¶è§†è§‰ä»¤ç‰Œæ•°é‡ï¼Œæå‡äº†æ¨ç†æ•ˆç‡ã€‚åŒæ—¶ï¼Œå°†æ¨¡å‹æ‰©å±•åˆ°ç‰©ä½“å¯¼èˆªï¼ˆObjNavï¼‰ã€å›¾åƒç›®æ ‡å¯¼èˆªï¼ˆImageNavï¼‰å’Œè§†è§‰è·Ÿè¸ªï¼ˆEVTï¼‰ç­‰å¤šä¸ªä»»åŠ¡ï¼Œä½¿ç”¨å•ä¸€æ¨¡å‹å®ç°SOTAæ€§èƒ½ã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼š<br>1. **è‡ªé€‚åº”æ¨ç†**ï¼šUni-NaVid è™½ç„¶æ”¯æŒå¤šä»»åŠ¡ï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹ä»æ˜¯å›ºå®šçš„æˆ–éœ€è¦æ‰‹åŠ¨é…ç½®é¢‘ç‡ã€‚VLingNav çš„ **AdaCoT æ˜¯è‡ªé€‚åº”çš„**ï¼Œæ¨¡å‹èƒ½è‡ªä¸»å†³å®šâ€œä½•æ—¶æ€è€ƒâ€ä»¥åŠâ€œæ€è€ƒä»€ä¹ˆâ€ï¼Œåœ¨æ•ˆç‡ä¸æ€§èƒ½é—´å–å¾—æ›´å¥½å¹³è¡¡ï¼ˆå¹³å‡ä»…2.1%çš„æ­¥éª¤è§¦å‘è¯¦ç»†æ¨ç†ï¼‰ã€‚<br>2. **ç»“æ„åŒ–è®°å¿†**ï¼šUni-NaVid çš„è®°å¿†ä»ä¾èµ–äºå‹ç¼©åçš„å†å²è§†è§‰ç‰¹å¾ï¼Œæ˜¯éšå¼çš„ã€‚VLingNav çš„ **VLingMem æ˜¯æ˜¾å¼ã€ç»“æ„åŒ–çš„è¯­è¨€è®°å¿†**ï¼Œèƒ½ä¸»åŠ¨æ€»ç»“ç¯å¢ƒå…³é”®ä¿¡æ¯ï¼ˆ`<summary>`ï¼‰ï¼Œå¹¶ä¸æ¨ç†è¿‡ç¨‹ååŒï¼Œé•¿æœŸè§„åˆ’èƒ½åŠ›æ›´å¼ºã€‚<br>3. **è®­ç»ƒæ•°æ®ä¸ç­–ç•¥**ï¼šUni-NaVid ä¸»è¦ä¾èµ–ç°æœ‰å¯¼èˆªæ•°æ®é›†è¿›è¡ŒSFTã€‚VLingNav æ„å»ºäº†**è¿„ä»Šæœ€å¤§çš„å¸¦æ¨ç†æ ‡æ³¨çš„æ•°æ®é›† Nav-AdaCoT-2.9M**ï¼Œå¹¶å¼•å…¥äº†**RLåè®­ç»ƒ**ï¼Œä½¿å…¶åœ¨å¤æ‚ã€é•¿è§†é‡ä»»åŠ¡ï¼ˆå¦‚MP3D ObjNavï¼‰ä¸Šè¡¨ç°æ˜¾è‘—æ›´ä¼˜ï¼ˆSRæå‡æ˜¾è‘—ï¼‰ã€‚ |\n| **TrackVLA** (Embodied Visual Tracking in the Wild) | 2025 (arXiv) | **ä¸“æ³¨äºå…·èº«è§†è§‰è·Ÿè¸ªï¼ˆEVTï¼‰çš„VLAæ¨¡å‹**ã€‚è®¾è®¡äº†**åŸºäºé”šç‚¹çš„æ‰©æ•£ç­–ç•¥**ï¼Œç›´æ¥è¾“å‡ºæœºå™¨äººçš„è¿åŠ¨è½¨è¿¹ï¼Œç›¸æ¯”ç¦»æ•£åŠ¨ä½œåœ¨è·Ÿè¸ªå¹³æ»‘æ€§å’Œç²¾åº¦ä¸Šæœ‰å¤§å¹…æå‡ã€‚åœ¨EVT-Benchä¸Šå–å¾—äº†å½“æ—¶SOTAçš„ç»“æœã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼š<br>1. **é€šç”¨æ€§ä¸æ¶æ„**ï¼šTrackVLA æ˜¯**ä¸“é—¨ä¸ºè·Ÿè¸ªä»»åŠ¡è®¾è®¡**çš„æ¨¡å‹ã€‚VLingNav æ˜¯ä¸€ä¸ª**é€šç”¨å¯¼èˆªæ¡†æ¶**ï¼Œé€šè¿‡å¤šä»»åŠ¡ååŒè®­ç»ƒï¼Œåœ¨ObjNavã€ImageNavå’ŒEVTä»»åŠ¡ä¸Šå‡è¾¾åˆ°SOTAï¼Œå®ç°äº†â€œä¸€ä¸ªæ¨¡å‹è§£å†³å¤šä¸ªé—®é¢˜â€ã€‚<br>2. **æ¨ç†ä¸è®°å¿†**ï¼šTrackVLA æœªå¼ºè°ƒæ˜¾å¼æ¨ç†å’Œé•¿æœŸè®°å¿†æ¨¡å—ã€‚VLingNav å¼•å…¥äº†**AdaCoTå’ŒVLingMemè¿™ä¸¤ä¸ªæ ¸å¿ƒè®¤çŸ¥æ¨¡å—**ï¼Œä½¿å…¶åœ¨å¤æ‚å¹²æ‰°åœºæ™¯ï¼ˆå¦‚Distracted Trackingï¼‰ä¸­è¡¨ç°æ›´é²æ£’ï¼Œèƒ½æ›´å¥½åœ°è¿›è¡Œé‡è¯†åˆ«å’Œè½¨è¿¹è§„åˆ’ã€‚<br>3. **åŠ¨ä½œæ¨¡å‹**ï¼šTrackVLA ä½¿ç”¨**æ‰©æ•£æ¨¡å‹**ç”Ÿæˆè½¨è¿¹ï¼Œæ¨ç†æ—¶éœ€è¦è¿­ä»£å»å™ªï¼Œè®¡ç®—å¼€é”€è¾ƒå¤§ã€‚VLingNav é‡‡ç”¨**è½»é‡åŒ–çš„MLPåŠ¨ä½œæ¨¡å‹**ï¼ˆè¾“å‡ºé«˜æ–¯åˆ†å¸ƒå‚æ•°ï¼‰ï¼Œåœ¨ä¿è¯è¿ç»­æ§åˆ¶ç²¾åº¦çš„åŒæ—¶ï¼Œå®ç°äº†**æ›´é«˜çš„æ¨ç†æ•ˆç‡**ï¼ˆ~300mså»¶è¿Ÿï¼‰ï¼Œæ›´åˆ©äºå®æ—¶æœºå™¨äººéƒ¨ç½²ã€‚ |\n| **NavFoM** (Embodied Navigation Foundation Model) | 2025 (arXiv) | **å¯¼èˆªåŸºç¡€æ¨¡å‹**ã€‚å¼•å…¥äº†**TVIï¼ˆTime, View, Instanceï¼‰ä»¤ç‰Œ**ï¼Œèƒ½å¤Ÿèåˆæ¥è‡ªä¸åŒæœºå™¨äººå½¢æ€ï¼ˆ embodiment ï¼‰çš„è·¨è§†è§’å¯¼èˆªæ•°æ®ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–æ€§å’Œç©ºé—´ç†è§£èƒ½åŠ›ã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼š<br>1. **è®¤çŸ¥æœºåˆ¶è®¾è®¡**ï¼šNavFoM ä¾§é‡äº**æ•°æ®èåˆä¸è¡¨å¾å­¦ä¹ **ã€‚VLingNav çš„æ ¸å¿ƒåˆ›æ–°åœ¨äº**æ¨¡ä»¿äººç±»è®¤çŸ¥è¿‡ç¨‹çš„æ¶æ„è®¾è®¡**ï¼ˆåŒè¿‡ç¨‹æ¨ç†+è¯­è¨€è®°å¿†ï¼‰ï¼Œæ—¨åœ¨æå‡æ¨¡å‹çš„å†…åœ¨å†³ç­–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚<br>2. **è®°å¿†å½¢å¼**ï¼šNavFoM å¯èƒ½åˆ©ç”¨å¤šè§†è§’ä¿¡æ¯æ„å»ºæ›´ä¸°å¯Œçš„åœºæ™¯è¡¨ç¤ºï¼Œä½†æœªæ˜ç¡®è®¾è®¡æŒä¹…è®°å¿†æ¨¡å—ã€‚VLingNav æ˜ç¡®æå‡ºäº†**VLingMem**ï¼Œä»¥è¯­è¨€ä¸ºä¸­å¿ƒæ„å»ºè®°å¿†ï¼Œä¸VLMä¸»å¹²å¤©ç„¶å¯¹é½ï¼Œæ›´ä¸“æ³¨äºåˆ©ç”¨å†å²ä¿¡æ¯æŒ‡å¯¼æœªæ¥å†³ç­–ã€‚<br>3. **è®­ç»ƒç­–ç•¥**ï¼šVLingNav å¼ºè°ƒäº†**RLåè®­ç»ƒ**çš„å…³é”®ä½œç”¨ï¼Œå®éªŒè¡¨æ˜å…¶èƒ½æ˜¾è‘—æå‡SFTåŸºçº¿çš„æ€§èƒ½ã€‚è€ŒNavFoM çš„å·¥ä½œé‡ç‚¹æ›´å¤šåœ¨é¢„è®­ç»ƒå’Œæ•°æ®å±‚é¢ã€‚ |\n\n### æ€»ç»“ï¼šVLingNav çš„æ ¸å¿ƒæ”¹è¿›\n\nVLingNav å¹¶éå¯¹å‰ä½œåŠŸèƒ½çš„ç®€å•å åŠ ï¼Œè€Œæ˜¯åœ¨**è®¤çŸ¥æ¶æ„**å’Œ**è®­ç»ƒèŒƒå¼**ä¸Šè¿›è¡Œäº†æ ¹æœ¬æ€§çš„é©æ–°ï¼š\n\n1.  **ä»ååº”å¼åˆ°è®¤çŸ¥å¼**ï¼šå‰ä½œï¼ˆNaVid, Uni-NaVidï¼‰æœ¬è´¨æ˜¯å¼ºå¤§çš„â€œæ„ŸçŸ¥-åŠ¨ä½œâ€æ˜ å°„å™¨ã€‚VLingNav é€šè¿‡ **AdaCoT** èµ‹äºˆäº†æ¨¡å‹â€œæ€è€ƒâ€çš„èƒ½åŠ›ï¼Œå®ç°äº†**åŠ¨æ€çš„ã€æŒ‰éœ€çš„æ˜¾å¼æ¨ç†**ï¼Œè¿™æ˜¯ä»â€œæ¡ä»¶åå°„â€åˆ°â€œæ·±æ€ç†Ÿè™‘â€çš„å…³é”®è·¨è¶Šã€‚\n\n2.  **ä»éšå¼è®°å¿†åˆ°æ˜¾å¼è®°å¿†**ï¼šå‰ä½œä¾èµ–å†å²è§†è§‰è¾“å…¥ä½œä¸ºéšå¼è®°å¿†ã€‚VLingNav çš„ **VLingMem** åˆ›å»ºäº†ä»¥è¯­è¨€ä¸ºè½½ä½“çš„ã€å¯å­˜å‚¨å’Œè°ƒç”¨çš„æ˜¾å¼è¯­ä¹‰è®°å¿†ï¼Œè§£å†³äº†é•¿è§†é‡ä»»åŠ¡ä¸­çš„ä¿¡æ¯é—å¿˜å’Œå¾ªç¯æ¢ç´¢é—®é¢˜ã€‚\n\n3.  **ä»çº¯æ¨¡ä»¿åˆ°äº¤äº’å­¦ä¹ **ï¼šå‰ä½œä¸»è¦ä¾èµ–ä¸“å®¶æ¼”ç¤ºï¼ˆSFTï¼‰ã€‚VLingNav å¼•å…¥äº†**åœ¨çº¿ä¸“å®¶å¼•å¯¼çš„RL**ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡ä¸ç¯å¢ƒçš„ä¸»åŠ¨äº¤äº’æ¥å‘ç°æ›´ä¼˜ç­–ç•¥ã€çº æ­£é”™è¯¯ï¼Œçªç ´äº†æ¨¡ä»¿å­¦ä¹ çš„æ•°æ®ç“¶é¢ˆå’Œåå˜é‡åç§»é—®é¢˜ã€‚\n\n4.  **ä»ä¸“ç”¨æ¨¡å‹åˆ°é€šç”¨ä¸”é«˜æ•ˆçš„æ™ºèƒ½ä½“**ï¼šVLingNav æ•´åˆäº†ä¸Šè¿°æ”¹è¿›ï¼Œåœ¨ä¸€ä¸ªæ¡†æ¶å†…ç»Ÿä¸€äº†å¤šä¸ªå¯¼èˆªä»»åŠ¡ï¼Œå¹¶å…¼é¡¾äº†é«˜æ€§èƒ½ï¼ˆå„é¡¹ä»»åŠ¡SOTAï¼‰ä¸é«˜æ•ˆç‡ï¼ˆä½æ¨ç†é¢‘ç‡ã€å¿«é€Ÿè¿ç»­åŠ¨ä½œé¢„æµ‹ï¼‰ï¼Œæœ€ç»ˆå®ç°äº†**é›¶æ ·æœ¬è¿ç§»åˆ°çœŸå®æœºå™¨äºº**ï¼Œå®Œæˆäº†ä»ä»¿çœŸç®—æ³•åˆ°å®ç”¨ç³»ç»Ÿçš„éªŒè¯ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼ŒVLingNav çš„ä¸»è¦æ”¹å–„åœ¨äºå°†è®¤çŸ¥ç§‘å­¦åŸç†ï¼ˆåŒè¿‡ç¨‹ç†è®ºã€è¯­è¨€è®°å¿†ï¼‰ç³»ç»Ÿæ€§åœ°èå…¥VLAæ¨¡å‹æ¶æ„ï¼Œå¹¶é€šè¿‡åˆ›æ–°çš„æ•°æ®é›†æ„å»ºå’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆSFT+RLï¼‰ï¼Œæ‰“é€ å‡ºä¸€ä¸ªå…·å¤‡è‡ªé€‚åº”æ¨ç†ã€æŒä¹…è®°å¿†å’Œå¼ºæ³›åŒ–èƒ½åŠ›çš„â€œè®¤çŸ¥å‹â€å…·èº«å¯¼èˆªæ™ºèƒ½ä½“ã€‚**",
      "timestamp": "2026-01-20T17:53:08.283160",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
      "answer": "## VLingNav å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è§£\n\nVLingNav çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶**è®¤çŸ¥å¯å‘çš„æ¶æ„è®¾è®¡**å’Œ**æ•°æ®ä¸è®­ç»ƒç­–ç•¥**ã€‚ä¸‹è¡¨è¯¦ç»†è¯´æ˜äº†å…¶å…³é”®æŠ€æœ¯æ–¹æ³•ã€æ•°å­¦åŸç†åŠå…·ä½“å‚æ•°ã€‚\n\n| æ¨¡å—/æ–¹æ³• | æ ¸å¿ƒæ€æƒ³ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°/å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. è‡ªé€‚åº”æ€ç»´é“¾ (AdaCoT)** | å—äººç±»åŒè¿‡ç¨‹ç†è®ºå¯å‘ï¼ŒåŠ¨æ€è§¦å‘æ˜¾å¼æ¨ç†ï¼Œå¹³è¡¡æ•ˆç‡ä¸æ·±æ€ã€‚ | **1.1 è§¦å‘æœºåˆ¶ï¼š** <br> - æ¨¡å‹åœ¨æ¯ä¸ªæ—¶é—´æ­¥é¦–å…ˆç”Ÿæˆä¸€ä¸ª CoT æŒ‡ç¤ºç¬¦ token (`<think_on>` æˆ– `<think_off>`)ã€‚<br> - è¯¥å†³ç­–é€šè¿‡ç›‘ç£å¾®è°ƒä»æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ ï¼Œè€Œéå›ºå®šè§„åˆ™ã€‚æ¨¡å‹å­¦ä¼šåœ¨æ„ŸçŸ¥åˆ°**ç¯å¢ƒå¤æ‚æ€§ã€å†³ç­–æ¨¡ç³Šæ€§æˆ–å…³é”®è·¯å£**æ—¶è§¦å‘ `think_on`ã€‚<br><br>**1.2 æ¨ç†å†…å®¹ç”Ÿæˆï¼š** <br> - è‹¥è§¦å‘ `think_on`ï¼Œæ¨¡å‹ä»¥è‡ªå›å½’æ–¹å¼ç”Ÿæˆç»“æ„åŒ–æ–‡æœ¬ï¼š<br> Â  - **æ¨ç†è¿‡ç¨‹**ï¼šå°è£…åœ¨ `<think> ... </think>` ä¸­ï¼ŒåŒ…å«ç¯å¢ƒæ„ŸçŸ¥ã€ä»»åŠ¡åˆ†è§£ã€ä½ç½®é‡è®¿åˆ¤æ–­ã€ä¸‹ä¸€æ­¥è¡ŒåŠ¨å†³ç­–ã€‚<br> Â  - **ç¯å¢ƒæ‘˜è¦**ï¼šå°è£…åœ¨ `<summary> ... </summary>` ä¸­ï¼Œä½œä¸ºè¯­è¨€è®°å¿†å­˜å…¥åç»­ä¸Šä¸‹æ–‡ã€‚<br><br>**1.3 æ•°å­¦åŸç†ï¼š** <br> æœ¬è´¨æ˜¯ä¸€ä¸ªæ¡ä»¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ã€‚ç»™å®šè§‚æµ‹åºåˆ— \\( \\mathbf{O}_{1:t} \\)ã€æŒ‡ä»¤ \\( \\mathcal{I} \\) å’Œå†å²è®°å¿† \\( \\mathcal{M} \\)ï¼Œæ¨¡å‹å‚æ•° \\( \\theta \\) ä¼˜åŒ–ç”Ÿæˆåºåˆ— \\( \\mathbf{Y}_t \\)ï¼ˆåŒ…å«æŒ‡ç¤ºç¬¦ã€CoTã€æ‘˜è¦ï¼‰çš„æ¦‚ç‡ï¼š<br> \\[ P_{\\theta}(\\mathbf{Y}_t | \\mathcal{I}, \\mathbf{O}_{1:t}, \\mathcal{M}) = \\prod_{i} P_{\\theta}(y_i | y_{<i}, \\mathcal{I}, \\mathbf{O}_{1:t}, \\mathcal{M}) \\]<br> è®­ç»ƒæ—¶ä½¿ç”¨äº¤å‰ç†µæŸå¤± \\( \\mathcal{L}_{CE} \\) è¿›è¡Œç›‘ç£ã€‚ | - **æ¨ç†é¢‘ç‡**ï¼šå®éªŒæµ‹å¾—å¹³å‡ä»… **2.1%** çš„æ—¶é—´æ­¥ä¼šè§¦å‘æ˜¾å¼æ¨ç†ã€‚<br>- **æ¨¡å‹åŸºç¡€**ï¼šåŸºäº **LLaVA-Video-7B** çš„ VLM ä¸»å¹²ã€‚<br>- **è®­ç»ƒæ•°æ®**ï¼šä½¿ç”¨ **Nav-AdaCoT-2.9M** æ•°æ®é›†ä¸­å¸¦è‡ªé€‚åº” CoT æ ‡æ³¨çš„æ ·æœ¬è¿›è¡Œç›‘ç£å¾®è°ƒã€‚ |\n| **2. è§†è§‰è¾…åŠ©è¯­è¨€è®°å¿† (VLingMem)** | æ„å»ºæŒä¹…ã€è·¨æ¨¡æ€çš„è¯­ä¹‰è®°å¿†ï¼Œä»¥è¯­è¨€æ‘˜è¦ä¸ºæ ¸å¿ƒï¼Œè§†è§‰ç‰¹å¾ä¸ºè¾…åŠ©ï¼Œè§£å†³é•¿æ—¶ç¨‹ç©ºé—´ä¾èµ–ã€‚ | **2.1 è®°å¿†æ„å»ºï¼š** <br> - **æ ¸å¿ƒ**ï¼šå°† AdaCoT ä¸­ç”Ÿæˆçš„ `<summary>` æ–‡æœ¬ä½œä¸º**è¯­è¨€è®°å¿†**ï¼Œé€šè¿‡ tokenizer è½¬æ¢ä¸º token åºåˆ— \\( \\mathbf{E}^M \\)ï¼Œæ‹¼æ¥åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„æ¨¡å‹è¾“å…¥ä¸­ã€‚<br> - **è¾…åŠ©**ï¼šå†å²è§†è§‰ç‰¹å¾ç»è¿‡åŠ¨æ€é‡‡æ ·å’Œæ± åŒ–åï¼Œä¹Ÿä½œä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†ï¼Œæä¾›ç»†ç²’åº¦è§†è§‰çº¿ç´¢ã€‚<br><br>**2.2 è§†è§‰å†å²å¤„ç†ï¼ˆåŠ¨æ€FPSä¸ç½‘æ ¼æ± åŒ–ï¼‰ï¼š** <br> ä¸ºè§£å†³è§†é¢‘åºåˆ—è¿‡é•¿é—®é¢˜ï¼Œæå‡ºåŸºäºâ€œè‰¾å®¾æµ©æ–¯é—å¿˜æ›²çº¿â€çš„å‹ç¼©ç­–ç•¥ã€‚<br> **a) åŠ¨æ€FPSé‡‡æ ·ï¼š** <br> é‡‡æ ·ç‡ \\( f_s(i) \\) éšå¸§é¾„è¡°å‡ï¼š<br> \\[ f_s(i) = f_s^{max} \\cdot e^{-\\frac{\\Delta T}{s}} \\]<br> å…¶ä¸­ \\( \\Delta T = t - i \\) æ˜¯å½“å‰å¸§ \\( t \\) ä¸å†å²å¸§ \\( i \\) çš„æ—¶é—´å·®ï¼Œ\\( s \\) æ˜¯è®°å¿†ç¨³å®šæ€§å‚æ•°ï¼Œ\\( f_s^{max} \\) æ˜¯æœ€å¤§é‡‡æ ·ç‡ã€‚<br><br> **b) ç½‘æ ¼æ± åŒ– (Grid Pooling)ï¼š** <br> å¯¹é‡‡æ ·åçš„è§†è§‰ç‰¹å¾ \\( \\mathbf{V}_{t_i} \\) è¿›è¡Œç©ºé—´ä¸‹é‡‡æ ·ï¼Œæ­¥é•¿ \\( g(i) \\) ä¹Ÿéšå¸§é¾„è¡°å‡ï¼š<br> \\[ g(i) = \\lfloor e^{-\\frac{\\Delta T}{g}} \\rfloor \\]<br> \\[ \\mathbf{V}'_{t_i} = \\mathcal{G}(\\mathbf{V}_{t_i}, g(i)) \\]<br> å…¶ä¸­ \\( \\mathcal{G}(\\cdot) \\) æ˜¯ç½‘æ ¼æ± åŒ–æ“ä½œã€‚<br><br>**2.3 æ—¶é—´æ„ŸçŸ¥ç¼–ç ï¼š** <br> ä¸ºæ¯ä¸ªå¸§å¼•å…¥æ—¶é—´æ„ŸçŸ¥æŒ‡ç¤ºç¬¦ token \\( E^T(\\Delta T) \\)ï¼Œä½¿ç”¨ RoPE ç¼–ç ç»å¯¹æ—¶é—´é—´éš”ï¼š<br> \\[ E^T(\\Delta T) = E^T_{base} + \\text{RoPE}(\\Delta T) \\] | - **è§†è§‰ç¼–ç å™¨**ï¼š**SigLIP-400M**ã€‚<br>- **è§†è§‰ token æ•°**ï¼šæ¯å¸§ \\( N = 729 \\) ä¸ª patchã€‚<br>- **åµŒå…¥ç»´åº¦**ï¼š\\( C = 1152 \\)ã€‚<br>- **è®°å¿†ç¨³å®šæ€§å‚æ•° (s, g)**ï¼šè®ºæ–‡æœªç»™å‡ºå…·ä½“å€¼ï¼Œéœ€é€šè¿‡å®éªŒè°ƒæ•´ã€‚<br>- **æœ€å¤§é‡‡æ ·ç‡ \\( f_s^{max} \\)**ï¼šæœªæ˜ç¡®ï¼Œæ§åˆ¶çŸ­æœŸè®°å¿†çš„å¸§ç‡ã€‚ |\n| **3. æ¦‚ç‡è¿ç»­åŠ¨ä½œæ¨¡å‹** | é‡‡ç”¨è½»é‡çº§ MLP æŠ•å½±å¤´ï¼Œå°† VLM çš„éšçŠ¶æ€æ˜ å°„ä¸ºè¿ç»­è½¨è¿¹ï¼Œå¹³è¡¡ç²¾åº¦ä¸æ¨ç†æ•ˆç‡ã€‚ | **3.1 ç­–ç•¥å‚æ•°åŒ–ï¼š** <br> åŠ¨ä½œæ¨¡å‹ \\( \\mathcal{A}_{\\theta} \\) ä»¥ VLM æœ€åä¸€ä¸ªé¢„æµ‹ token çš„éšçŠ¶æ€ \\( \\mathbf{h}_t^{pred} \\) ä¸ºæ¡ä»¶ï¼Œè¾“å‡ºä¸€ä¸ªå¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼š<br> \\[ \\pi_{\\theta}(\\mathbf{a}_t | \\mathbf{s}_t) = \\mathcal{N}\\left( \\boldsymbol{\\mu}_{\\theta}(\\mathbf{h}_t), \\text{diag}\\left( \\boldsymbol{\\sigma}_{\\theta}(\\mathbf{h}_t)^2 \\right) \\right) \\]<br> å…¶ä¸­å‡å€¼ \\( \\boldsymbol{\\mu}_{\\theta} \\) å’Œå¯¹æ•°æ ‡å‡†å·® \\( \\log \\boldsymbol{\\sigma}_{\\theta} \\) å‡ç”± MLP é¢„æµ‹ã€‚<br><br>**3.2 åŠ¨ä½œæ‰§è¡Œï¼š** <br> - **è®­ç»ƒ/æ¢ç´¢**ï¼šä»åˆ†å¸ƒä¸­é‡‡æ · \\( \\mathbf{a}_t \\sim \\pi_{\\theta}(\\cdot \\mid \\mathbf{s}_t) \\)ã€‚<br> - **éªŒè¯/éƒ¨ç½²**ï¼šä½¿ç”¨ç¡®å®šæ€§è¾“å‡º \\( \\mathbf{a}_t = \\boldsymbol{\\mu}_{\\theta}(\\mathbf{h}_t) \\)ã€‚<br><br>**3.3 ç›‘ç£ä¿¡å·ï¼š** <br> åœ¨ SFT é˜¶æ®µï¼Œä½¿ç”¨å‡æ–¹è¯¯å·® (MSE) æŸå¤±ç›‘ç£é¢„æµ‹è½¨è¿¹ \\( \\hat{\\tau}_t \\) ä¸ä¸“å®¶è½¨è¿¹ \\( \\tau_t^{gt} \\)ï¼š<br> \\[ \\mathcal{L}_{MSE} = \\| \\hat{\\tau}_t - \\tau_t^{gt} \\|^2 \\] | - **åŠ¨ä½œç©ºé—´**ï¼š\\( \\mathbf{a}_t \\in \\mathbb{R}^3 = (x, y, \\theta) \\)ï¼Œè¡¨ç¤ºäºŒç»´å¹³é¢ä¸Šçš„ä½ç½®å’Œæœå‘ã€‚<br>- **è½¨è¿¹é•¿åº¦ \\( n \\)**ï¼šè®ºæ–‡æœªæ˜ç¡®ï¼ŒæŒ‡ä¸€æ¬¡é¢„æµ‹çš„æœªæ¥è·¯å¾„ç‚¹æ•°é‡ã€‚<br>- **æŸå¤±æƒé‡ \\( \\alpha \\)**ï¼šåœ¨ SFT æ€»æŸå¤± \\( \\mathcal{L}_{SFT} \\) ä¸­ï¼Œå¹³è¡¡åŠ¨ä½œæŸå¤±å’Œè¯­è¨€æŸå¤±ï¼Œè®¾ä¸º **0.5**ã€‚ |\n| **4. åœ¨çº¿ä¸“å®¶å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒ** | è¶…è¶Šçº¯æ¨¡ä»¿å­¦ä¹ ï¼Œé€šè¿‡ç¯å¢ƒäº¤äº’å’Œä¸“å®¶å¼•å¯¼ï¼Œå­¦ä¹ æ›´é²æ£’ã€è‡ªæ¢ç´¢çš„å¯¼èˆªè¡Œä¸ºã€‚ | **4.1 æ··åˆæ•°æ®æ”¶é›† (Hybrid Rollout)ï¼š** <br> - **æœ´ç´ æ”¶é›†**ï¼šå½“å‰ç­–ç•¥ \\( \\pi_{\\theta} \\) ä¸ç¯å¢ƒäº¤äº’ï¼Œä»…å°†**æˆåŠŸ**çš„è½¨è¿¹å­˜å…¥ç¼“å†²æ± ã€‚<br> - **ä¸“å®¶å¼•å¯¼æ”¶é›†**ï¼šå½“ç­–ç•¥é™·å…¥å¾ªç¯æˆ–å¡ä½ï¼ˆè¿ç»­ \\( k \\) æ­¥æ— æ•ˆï¼‰æ—¶ï¼Œåˆ‡æ¢è‡³ä¸“å®¶ç­–ç•¥ \\( \\pi^* \\)ï¼ˆæ¨¡æ‹Ÿå™¨ä¸­æœ€çŸ­è·¯å¾„è§„åˆ’å™¨ï¼‰æ¥ç®¡ï¼Œç”Ÿæˆçº æ­£è½¨è¿¹å¹¶å­˜å…¥ç¼“å†²æ± ã€‚<br><br>**4.2 å¤åˆæŸå¤±å‡½æ•°ï¼š** <br> ä¼˜åŒ–ä»¥ä¸‹ç›®æ ‡ï¼Œç»“åˆ RL çš„æ¢ç´¢å’Œ SFT çš„ç¨³å®šæ€§ï¼š<br> \\[ \\min_{\\theta} \\mathcal{L}_{post}(\\theta) = \\lambda \\mathcal{L}_{RL}(\\theta) + (1-\\lambda) \\mathcal{L}_{SFT}(\\theta) \\]<br> **a) RL æŸå¤± \\( \\mathcal{L}_{RL} \\)**ï¼šé‡‡ç”¨ PPO é£æ ¼çš„ clipped surrogate ç›®æ ‡ï¼š<br> \\[ \\mathcal{L}_{RL}(\\theta) = -\\mathbb{E}_t \\left[ \\min\\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t \\right) \\right] \\]<br> å…¶ä¸­ \\( r_t(\\theta) = \\frac{\\pi_{\\theta}(\\mathbf{a}_t|\\mathbf{s}_t)}{\\pi_{\\theta_{old}}(\\mathbf{a}_t|\\mathbf{s}_t)} \\) æ˜¯æ¦‚ç‡æ¯”ï¼Œ\\( A_t \\) æ˜¯ä½¿ç”¨",
      "timestamp": "2026-01-20T17:54:12.835939",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
      "answer": "## VLingNav è®ºæ–‡åˆ†æï¼šè¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡ (Evaluation Metrics)\n\nè®ºæ–‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä½¿ç”¨äº†æ ‡å‡†åŒ–çš„è¯„ä»·æŒ‡æ ‡ï¼Œä»¥å…¨é¢è¯„ä¼°å¯¼èˆªæ€§èƒ½ã€‚ä¸»è¦æŒ‡æ ‡å¦‚ä¸‹ï¼š\n\n| æŒ‡æ ‡åç§° | ç¼©å†™ | å®šä¹‰ä¸ç»†èŠ‚ | é€‚ç”¨ä»»åŠ¡ |\n| :--- | :--- | :--- | :--- |\n| **æˆåŠŸç‡** | **SR (Success Rate)** | **æ ¸å¿ƒæŒ‡æ ‡**ã€‚ä»»åŠ¡æˆåŠŸå®Œæˆçš„è½¨è¿¹æ¯”ä¾‹ã€‚å¯¹äº ObjectNav/ImageNavï¼ŒæŒ‡åœ¨é™å®šæ­¥æ•°å†…åˆ°è¾¾ç›®æ ‡ä½ç½®ï¼›å¯¹äº EVTï¼ŒæŒ‡åœ¨è·Ÿè¸ªè¿‡ç¨‹ä¸­æŒç»­é”å®šæ­£ç¡®ç›®æ ‡ã€‚å€¼è¶Šé«˜è¶Šå¥½ã€‚ | ObjectNav, ImageNav, EVT |\n| **æˆåŠŸç‡åŠ æƒè·¯å¾„é•¿åº¦** | **SPL (Success weighted by Path Length)** | **è¡¡é‡æ•ˆç‡çš„å…³é”®æŒ‡æ ‡**ã€‚åœ¨æˆåŠŸç‡çš„åŸºç¡€ä¸Šï¼Œæƒ©ç½šæ›´é•¿çš„è·¯å¾„ã€‚è®¡ç®—å…¬å¼ä¸ºï¼š`SPL = (1/N) * Î£ (S_i * (L_i* / max(L_i, L_i*)))`ï¼Œå…¶ä¸­ `S_i` ä¸ºæˆåŠŸä¸å¦ï¼ˆ0/1ï¼‰ï¼Œ`L_i*` ä¸ºæœ€ä¼˜è·¯å¾„é•¿åº¦ï¼Œ`L_i` ä¸ºå®é™…è·¯å¾„é•¿åº¦ã€‚å€¼è¶Šé«˜ï¼Œè¡¨ç¤ºè·¯å¾„è¶ŠçŸ­ã€è¶Šé«˜æ•ˆã€‚ | ObjectNav, ImageNav |\n| **è·Ÿè¸ªç‡** | **TR (Tracking Rate)** | **EVTä»»åŠ¡ä¸“ç”¨**ã€‚è¡¡é‡åœ¨è·Ÿè¸ªè¿‡ç¨‹ä¸­ï¼Œæ™ºèƒ½ä½“è§†é‡å†…æ­£ç¡®ç›®æ ‡å¯è§çš„æ—¶é—´æ¯”ä¾‹ã€‚åæ˜ äº†è·Ÿè¸ªçš„ç¨³å®šæ€§å’ŒæŒç»­æ€§ã€‚å€¼è¶Šé«˜è¶Šå¥½ã€‚ | EVT |\n| **ç¢°æ’ç‡** | **CR (Collision Rate)** | **å®‰å…¨æ€§æŒ‡æ ‡**ã€‚æ™ºèƒ½ä½“åœ¨å¯¼èˆªè¿‡ç¨‹ä¸­å‘ç”Ÿç¢°æ’çš„æ­¥æ•°æ¯”ä¾‹ã€‚å€¼è¶Šä½ï¼Œè¡¨æ˜å¯¼èˆªè¶Šå®‰å…¨ã€è¶Šå¹³æ»‘ã€‚ | EVT |\n\n**æŒ‡æ ‡ç»†èŠ‚æ€»ç»“**ï¼š\n*   **SR å’Œ SPL** æ˜¯ embodied navigation æœ€æ ¸å¿ƒçš„æŒ‡æ ‡ï¼Œåˆ†åˆ«è¡¡é‡**ä»»åŠ¡å®Œæˆèƒ½åŠ›**å’Œ**å®Œæˆæ•ˆç‡**ã€‚\n*   **TR å’Œ CR** ä¸“é—¨é’ˆå¯¹åŠ¨æ€ã€å¤æ‚çš„**Embodied Visual Tracking (EVT)** ä»»åŠ¡ï¼Œåˆ†åˆ«è¯„ä¼°**è·Ÿè¸ªè´¨é‡**å’Œ**è¿åŠ¨å®‰å…¨æ€§**ã€‚\n*   æ‰€æœ‰å®éªŒå‡æŠ¥å‘Šäº†è¿™äº›æŒ‡æ ‡çš„å¯¹æ¯”ï¼Œè¯æ˜äº† VLingNav åœ¨**æˆåŠŸç‡ã€è·¯å¾„æ•ˆç‡å’Œä»»åŠ¡ç‰¹å®šæ€§èƒ½**ä¸Šçš„å…¨é¢ä¼˜åŠ¿ã€‚\n\n### äºŒã€ æ•°æ®é›† (Datasets)\n\nè®ºæ–‡ä½¿ç”¨äº†å¤§é‡å…¬å¼€å’Œè‡ªå»ºçš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œå…·ä½“å¦‚ä¸‹ï¼š\n\n#### 1. è®­ç»ƒæ•°æ®é›† (Training Datasets)\n\n| æ•°æ®é›†åç§° | è§„æ¨¡/ç»†èŠ‚ | ä»»åŠ¡ç±»å‹ | å…³é”®ç‰¹ç‚¹ä¸ç”¨é€” |\n| :--- | :--- | :--- | :--- |\n| **Nav-AdaCoT-2.9M (è‡ªå»º)** | **2.9M** æ­¥çº§è½¨è¿¹ï¼ŒåŒ…å« **472K** æ¡è‡ªé€‚åº” CoT æ ‡æ³¨ã€‚ | ObjectNav, EVT, ImageNav | **è®ºæ–‡æ ¸å¿ƒè´¡çŒ®ä¹‹ä¸€**ã€‚å½“å‰æœ€å¤§çš„ã€å¸¦æ¨ç†æ ‡æ³¨çš„å¯¼èˆªæ•°æ®é›†ã€‚ç‰¹ç‚¹ï¼š<br>1. **å¤šä»»åŠ¡èåˆ**ï¼šé¦–æ¬¡ç»Ÿä¸€äº†ä¸‰å¤§å¯¼èˆªä»»åŠ¡ã€‚<br>2. **è‡ªé€‚åº”CoTæ ‡æ³¨**ï¼šä½¿ç”¨è‡ªä¸»æ ‡æ³¨æµç¨‹ï¼ˆåŸºäº Qwen2.5-VL-72Bï¼‰ç”Ÿæˆâ€œä½•æ—¶æ€è€ƒâ€å’Œâ€œæ€è€ƒä»€ä¹ˆâ€çš„æ ‡æ³¨ã€‚<br>3. **è½¨è¿¹çº§åŠ¨ä½œ**ï¼šæä¾›è¿ç»­è½¨è¿¹ç›‘ç£ï¼Œæ¯”ç¦»æ•£åŠ¨ä½œæ›´ç²¾ç»†ã€‚<br>**ç”¨é€”**ï¼šä½œä¸º VLingNav ç›‘ç£å¾®è°ƒ (SFT) é˜¶æ®µçš„æ ¸å¿ƒæ•°æ®ã€‚ |\n| **å¼€æ”¾ä¸–ç•Œè§†é¢‘æ•°æ®** | **1.6M** æ ·æœ¬ï¼Œæ¥è‡ª LLaVA-Video-178K, Video-R1, ScanQAã€‚ | é€šç”¨è§†é¢‘é—®ç­” (VideoQA) | **ç”¨äºæå‡æ³›åŒ–èƒ½åŠ›**ã€‚ç‰¹ç‚¹ï¼š<br>1. **å¢å¼ºè§†è§‰ç†è§£**ï¼šæä¾›ä¸°å¯Œçš„å¼€æ”¾ä¸–ç•Œå…ˆéªŒçŸ¥è¯†ã€‚<br>2. **éƒ¨åˆ†å¸¦CoTæ ‡æ³¨**ï¼šVideo-R1 ç­‰å›°éš¾æ ·æœ¬è¢«ç»„ç»‡ä¸º CoT å­é›†ï¼Œè¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹çš„**è‡ªé€‚åº”æ¨ç†èƒ½åŠ›**ã€‚<br>**ç”¨é€”**ï¼šä¸å¯¼èˆªæ•°æ®**ååŒè®­ç»ƒ (Co-training)**ï¼Œå‡å°‘ä»¿çœŸåˆ°ç°å®çš„å·®è·ã€‚ |\n| **æ€»è®¡è®­ç»ƒæ•°æ®** | **4.5M** æ ·æœ¬ (2.9Må¯¼èˆª + 1.6Mè§†é¢‘)ã€‚ | - | æ”¯æ’‘äº† VLingNav çš„ä¸‰é˜¶æ®µè®­ç»ƒï¼ˆé¢„è®­ç»ƒã€SFTã€åœ¨çº¿RLï¼‰ã€‚ |\n\n#### 2. è¯„ä¼°åŸºå‡†æ•°æ®é›† (Evaluation Benchmarks)\n\nè®ºæ–‡åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†ä¸Šè¿›è¡Œäº†å…¨é¢æµ‹è¯•ï¼Œä»¥è¯æ˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚\n\n| åŸºå‡†åç§° | åœºæ™¯/è§„æ¨¡ | è¯„ä¼°ä»»åŠ¡ | å…³é”®ç‰¹ç‚¹ä¸æŒ‘æˆ˜ |\n| :--- | :--- | :--- | :--- |\n| **HM3D ObjNav (v1 & v2)** | HM3D åœºæ™¯ (1000ä¸ªå¤§è§„æ¨¡3Då®¤å†…ç¯å¢ƒ)ã€‚ | Object Goal Navigation | **å°é—­è¯æ±‡è¡¨**ç‰©ä½“ç›®æ ‡å¯¼èˆªã€‚è¦æ±‚æ™ºèƒ½ä½“åœ¨æœªçŸ¥å®¤å†…ç¯å¢ƒä¸­æ‰¾åˆ°æŒ‡å®šç±»åˆ«çš„ç‰©ä½“ï¼ˆå¦‚â€œæ²™å‘â€ã€â€œç”µè§†â€ï¼‰ã€‚ |\n| **MP3D ObjNav** | Matterport3D åœºæ™¯ (90ä¸ªçœŸå®æ‰«æå®¶åº­ç¯å¢ƒ)ã€‚ | Object Goal Navigation | ä¸ HM3D ç±»ä¼¼ï¼Œä½†åœºæ™¯æ¥è‡ªä¸åŒçš„æ•°æ®é›†ï¼Œç”¨äºæµ‹è¯•**è·¨åœºæ™¯æ³›åŒ–**èƒ½åŠ›ã€‚ |\n| **HM3D OVON** | HM3D åœºæ™¯ã€‚ | Open-Vocabulary Object Goal Navigation | **å¼€æ”¾è¯æ±‡è¡¨**ç‰©ä½“ç›®æ ‡å¯¼èˆªã€‚æ›´å…·æŒ‘æˆ˜æ€§ï¼Œæµ‹è¯•é›†åŒ…å«**è®­ç»ƒæœªè§ç±»åˆ« (unseen)** å’Œ**åŒä¹‰è¯ç±»åˆ« (synonym)**ï¼Œä¸“é—¨è¯„ä¼°æ¨¡å‹çš„**é›¶æ ·æœ¬æ³›åŒ–**å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚ |\n| **EVT-Bench** | HM3D & MP3D åœºæ™¯ï¼ŒåŒ…å«703ä¸ªåŠ¨æ€è¿½è¸ª episodeã€‚ | Embodied Visual Tracking | **åŠ¨æ€ç›®æ ‡è¿½è¸ª**ã€‚åŒ…å«ä¸¤ä¸ªå­ä»»åŠ¡ï¼š<br>1. **å•ç›®æ ‡è¿½è¸ª**ï¼šåœ¨å¤æ‚ç¯å¢ƒä¸­æŒç»­è·Ÿè¸ªä¸€ä¸ªæŒ‡å®šç›®æ ‡ã€‚<br>2. **åˆ†å¿ƒè¿½è¸ª**ï¼šåœ¨å­˜åœ¨å¤šä¸ªå¹²æ‰°ç›®æ ‡çš„æƒ…å†µä¸‹ï¼Œç¨³å®šè·Ÿè¸ªæ­£ç¡®ç›®æ ‡ã€‚æµ‹è¯•æ¨¡å‹çš„**åŠ¨æ€æ„ŸçŸ¥ã€é‡è¯†åˆ«å’ŒæŠ—å¹²æ‰°èƒ½åŠ›**ã€‚ |\n| **HM3D Instance ImageNav** | HM3D åœºæ™¯ã€‚ | Image Goal Navigation | **å›¾åƒç›®æ ‡å¯¼èˆª**ã€‚ç»™å®šä¸€å¼ ç›®æ ‡ç‰©ä½“çš„**å®ä¾‹å›¾åƒ**ï¼Œè¦æ±‚æ™ºèƒ½ä½“åœ¨æœªçŸ¥ç¯å¢ƒä¸­å¯¼èˆªåˆ°è¯¥**ç‰¹å®šç‰©ä½“å®ä¾‹**æ‰€åœ¨çš„ä½ç½®ã€‚æµ‹è¯•æ¨¡å‹çš„**ç»†ç²’åº¦è§†è§‰åŒ¹é…å’Œè·¨æ¨¡æ€ grounding èƒ½åŠ›**ã€‚ |\n\n**æ•°æ®é›†ä½¿ç”¨æ€»ç»“**ï¼š\n*   **è®­ç»ƒ**ï¼šé€šè¿‡æ„å»º **Nav-AdaCoT-2.9M** å¹¶èåˆ**å¼€æ”¾ä¸–ç•Œè§†é¢‘æ•°æ®**ï¼Œè§£å†³äº†é«˜è´¨é‡ã€å¤§è§„æ¨¡å¯¼èˆªæ¨ç†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œå¹¶æå‡äº†æ¨¡å‹çš„é€šç”¨è§†è§‰ç†è§£èƒ½åŠ›ã€‚\n*   **è¯„ä¼°**ï¼šåœ¨ **ObjectNav (å°é—­/å¼€æ”¾è¯æ±‡)ã€EVTã€ImageNav** è¿™ä¸‰å¤§ç±»ä¸»æµä»»åŠ¡ã€å…± **7ä¸ª** å…·æœ‰ä¸åŒæŒ‘æˆ˜æ€§çš„å…¬å¼€åŸºå‡†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œè¯æ˜äº† VLingNav å“è¶Šçš„**ç»¼åˆæ€§èƒ½**å’Œå¼ºå¤§çš„**è·¨ä»»åŠ¡ã€è·¨é¢†åŸŸã€é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›**ã€‚",
      "timestamp": "2026-01-20T17:55:01.580547",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
      "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨å¤šä¸ªæ ‡å‡†**ä»¿çœŸ**å’Œ**çœŸå®ä¸–ç•Œ**æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ¶µç›–ä¸‰å¤§ç±»å¯¼èˆªä»»åŠ¡ï¼š\n\n| ä»»åŠ¡ç±»åˆ« | å…·ä½“æ•°æ®é›† | è¯´æ˜ |\n| :--- | :--- | :--- |\n| **ç‰©ä½“ç›®æ ‡å¯¼èˆª (ObjectNav)** | 1. **HM3Dv1 ObjNav** <br> 2. **HM3Dv2 ObjNav** <br> 3. **MP3D ObjNav** <br> 4. **HM3D OVON** | å‰ä¸‰ä¸ªä¸ºå°é—­è¯æ±‡å¯¼èˆªï¼ŒHM3D OVON ä¸ºæ›´å…·æŒ‘æˆ˜æ€§çš„å¼€æ”¾è¯æ±‡å¯¼èˆªã€‚ |\n| **å…·èº«è§†è§‰è·Ÿè¸ª (EVT)** | **EVT-Bench** | åŒ…å«å•ç›®æ ‡è·Ÿè¸ªå’Œå¸¦å¹²æ‰°çš„è·Ÿè¸ªä¸¤ä¸ªå­ä»»åŠ¡ã€‚ |\n| **å›¾åƒç›®æ ‡å¯¼èˆª (ImageNav)** | **HM3D Instance ImageNav** | æ ¹æ®ç›®æ ‡å›¾åƒå¯¼èˆªåˆ°ç‰¹å®šç‰©ä½“å®ä¾‹ã€‚ |\n| **çœŸå®ä¸–ç•Œéƒ¨ç½²** | è‡ªå®šä¹‰åœºæ™¯ï¼ˆå®¶åº­ã€åŠå…¬å®¤ã€æˆ·å¤–ï¼‰ | åœ¨Unitree Go2å››è¶³æœºå™¨äººä¸Šè¿›è¡Œé›¶æ ·æœ¬è¿ç§»æµ‹è¯•ã€‚ |\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨å„åŸºå‡†æµ‹è¯•çš„æ ‡å‡†æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼š\n\n| æŒ‡æ ‡ | å…¨ç§° | å«ä¹‰ |\n| :--- | :--- | :--- |\n| **SR** | Success Rate | æˆåŠŸç‡ï¼Œä»»åŠ¡æˆåŠŸå®Œæˆçš„æ¯”ç‡ã€‚ |\n| **SPL** | Success-weighted Path Length | æˆåŠŸç‡åŠ æƒè·¯å¾„é•¿åº¦ï¼Œè¡¡é‡æˆåŠŸè·¯å¾„çš„æ•ˆç‡ã€‚ |\n| **TR** | Tracking Rate | è·Ÿè¸ªç‡ï¼Œåœ¨EVTä»»åŠ¡ä¸­æŒç»­è·Ÿè¸ªç›®æ ‡çš„æ¯”ç‡ã€‚ |\n| **CR** | Collision Rate | ç¢°æ’ç‡ï¼Œæœºå™¨äººå‘ç”Ÿç¢°æ’çš„æ¯”ç‡ï¼ˆè¶Šä½è¶Šå¥½ï¼‰ã€‚ |\n\n### ä¸‰ã€ä¸»è¦æ€§èƒ½æå‡ï¼ˆä¸å…ˆå‰SOTAæ–¹æ³•å¯¹æ¯”ï¼‰\nä»¥ä¸‹ä¸ºVLingNavåœ¨å…³é”®åŸºå‡†æµ‹è¯•ä¸Šçš„**æ˜¾è‘—æ€§èƒ½æå‡**ï¼š\n\n#### 1. ç‰©ä½“ç›®æ ‡å¯¼èˆª (ObjectNav)\n- **HM3Dv1**:\n    - **SR**: **79.1** vs. å…ˆå‰SOTA (Uni-NaVid: 73.7) â†’ **æå‡ +5.4 (+7.3%)**\n    - **SPL**: **42.9** vs. 37.1 â†’ **æå‡ +5.8 (+15.6%)**\n- **MP3D** (é•¿è·ç¦»æ¢ç´¢):\n    - **SR**: **58.9** vs. å…ˆå‰SOTA (CogNav: 46.6) â†’ **æå‡ +12.3 (+26.4%)**\n    - **SPL**: **26.5** vs. 16.1 â†’ **æå‡ +10.4 (+64.6%)**\n- **HM3D OVON** (å¼€æ”¾è¯æ±‡ï¼Œ`val unseen` åˆ†å‰²):\n    - **SR**: **50.1** vs. å…ˆå‰SOTA (Nav-R1: 42.2) â†’ **æå‡ +7.9 (+18.7%)**\n\n#### 2. å…·èº«è§†è§‰è·Ÿè¸ª (EVT)\n- **EVT-Bench (Distracted Tracking)**:\n    - **SR**: **67.6** vs. å…ˆå‰SOTA (TrackVLA++ï¼š 66.5) â†’ **æå‡ +1.1 (+1.7%)**\n    - **TR**: **73.5** vs. 68.8 â†’ **æå‡ +4.7 (+6.8%)**\n- **å…³é”®ä»·å€¼**: VLingNavä»…ä½¿ç”¨å•ç›®ç›¸æœºï¼Œæ€§èƒ½è¶…è¶Šäº†ä½¿ç”¨å¤šè§†è§’è®¾ç½®çš„NavFoMï¼Œå±•ç¤ºäº†å¼ºå¤§çš„è·Ÿè¸ªé²æ£’æ€§ã€‚\n\n#### 3. å›¾åƒç›®æ ‡å¯¼èˆª (ImageNav)\n- **HM3D Instance ImageNav**:\n    - **SR**: **60.8** vs. å…ˆå‰SOTA (UniGoal: 60.2) â†’ **æå‡ +0.6 (åŸºæœ¬æŒå¹³)**\n    - **SPL**: **37.4** vs. 23.7 â†’ **æ˜¾è‘—æå‡ +13.7 (+57.8%)**\n- **å…³é”®ä»·å€¼**: SPLçš„å¤§å¹…æå‡è¡¨æ˜VLingNavèƒ½è§„åˆ’å‡º**æ›´çŸ­ã€æ›´é«˜æ•ˆ**çš„è·¯å¾„ï¼Œä½“ç°äº†å…¶é«˜çº§æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚\n\n#### 4. çœŸå®ä¸–ç•Œé›¶æ ·æœ¬è¿ç§»\n- åœ¨å®¶åº­ã€åŠå…¬å®¤ã€æˆ·å¤–åœºæ™¯ä¸­ï¼ŒVLingNavåœ¨**ObjectNavã€EVTã€ImageNav**ä¸‰ä¸ªä»»åŠ¡ä¸Šçš„æˆåŠŸç‡å‡**æ˜¾è‘—é«˜äº**å¯¹æ¯”æ–¹æ³•Uni-NaVidå’ŒUniGoalã€‚\n- è¿™è¯æ˜äº†æ¨¡å‹å¼ºå¤§çš„**ä»¿çœŸåˆ°ç°å®è¿ç§»èƒ½åŠ›**å’Œ**è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›**ã€‚\n\n### å››ã€æ ¸å¿ƒç»“è®º\n- **å…¨é¢é¢†å…ˆ**: VLingNavåœ¨å¤šä¸ªä¸»æµå¯¼èˆªåŸºå‡†ä¸Šå–å¾—äº†**æœ€å…ˆè¿›ï¼ˆSOTAï¼‰æˆ–æå…·ç«äº‰åŠ›**çš„æ€§èƒ½ã€‚\n- **æ•ˆç‡ä¸è´¨é‡å¹¶é‡**: ä¸ä»…åœ¨æˆåŠŸç‡ï¼ˆSRï¼‰ä¸Šæœ‰æ‰€æå‡ï¼Œæ›´åœ¨è¡¡é‡è·¯å¾„æ•ˆç‡çš„SPLæŒ‡æ ‡ä¸Šè¡¨ç°å‡º**å·¨å¤§ä¼˜åŠ¿**ï¼Œè¿™ç›´æ¥å½’åŠŸäºå…¶**è‡ªé€‚åº”æ¨ç†ï¼ˆAdaCoTï¼‰** å’Œ**è§†è§‰è¾…åŠ©è¯­è¨€è®°å¿†ï¼ˆVLingMemï¼‰** æ¨¡å—ã€‚\n- **å¼ºæ³›åŒ–æ€§**: åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„å¼€æ”¾è¯æ±‡å¯¼èˆªï¼ˆOVONï¼‰å’Œå¸¦å¹²æ‰°çš„è·Ÿè¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶æˆåŠŸå®ç°**é›¶æ ·æœ¬çœŸå®ä¸–ç•Œéƒ¨ç½²**ï¼ŒéªŒè¯äº†å…¶æ¶æ„å’Œè®­ç»ƒèŒƒå¼çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚\n\n**æ€»ç»“**ï¼šVLingNavé€šè¿‡åˆ›æ–°çš„è®¤çŸ¥æ¶æ„ï¼Œåœ¨ä¿æŒé«˜æˆåŠŸç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†å¯¼èˆªæ•ˆç‡ï¼Œå¹¶åœ¨è·¨ä»»åŠ¡ã€è·¨é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ä¸Šè®¾ç«‹äº†æ–°çš„æ ‡æ†ã€‚",
      "timestamp": "2026-01-20T17:55:38.618094",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
      "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡ç¬¬8èŠ‚ï¼ˆConclusion and Limitationï¼‰çš„æ€»ç»“ï¼ŒVLingNav å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n- **æ„ŸçŸ¥è¾“å…¥å—é™**ï¼šæ¨¡å‹ä»…ä¾èµ–**å•ç›®ç¬¬ä¸€è§†è§’ï¼ˆegocentricï¼‰è§‚æµ‹**ï¼Œè§†é‡ï¼ˆFOVï¼‰æœ‰é™ï¼Œé™åˆ¶äº†ç¯å¢ƒæ„ŸçŸ¥çš„å…¨é¢æ€§å’Œæ•ˆç‡ã€‚\n- **å†³ç­–é¢‘ç‡ä¸è¶³**ï¼šé‡‡ç”¨**å•ä¸€ç³»ç»Ÿæ¶æ„**ï¼Œé¢„æµ‹é¢‘ç‡è¾ƒä½ï¼Œéš¾ä»¥åœ¨é«˜åº¦åŠ¨æ€ç¯å¢ƒä¸­å®ç°å¿«é€Ÿå†³ç­–å’Œé¿éšœã€‚\n- **è¿åŠ¨æ§åˆ¶æ¨¡å—ç®€å•**ï¼šä»…ä½¿ç”¨åŸºäº**éçº¿æ€§æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆNMPCï¼‰çš„è·¯å¾„ç‚¹è·Ÿè¸ªå™¨**ï¼Œç¼ºä¹æ›´çµæ´»çš„è¿åŠ¨æ¨¡å‹ï¼Œé™åˆ¶äº†æœºå™¨äººçš„ç§»åŠ¨é€Ÿåº¦å’Œå¯åˆ°è¾¾åŒºåŸŸã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡æå‡ºäº†æ˜ç¡®çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼š\n\n1. **å¤šè§†è§’æ„ŸçŸ¥èåˆ**\n   - å€Ÿé‰´è¿‘æœŸå·¥ä½œï¼ˆå¦‚ NavFoMï¼‰ï¼Œè®¡åˆ’é›†æˆ**å¤šè§†è§’è§‚æµ‹**ï¼Œä»¥æå‡ç¯å¢ƒæ„ŸçŸ¥çš„å®Œæ•´æ€§å’Œå¯¼èˆªæ•ˆç‡ã€‚\n\n2. **åŒç³»ç»Ÿæ¶æ„å‡çº§**\n   - è®¡åˆ’å°† VLingNav å‡çº§ä¸º**åŒç³»ç»Ÿç»“æ„**ï¼Œä»¥æ”¯æŒ**é«˜é¢‘åŠ¨ä½œè¾“å‡º**ï¼Œä»è€Œå¢å¼ºåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å®æ—¶é¿éšœå’Œå¿«é€Ÿå†³ç­–èƒ½åŠ›ã€‚\n\n3. **é›†æˆé«˜çº§è¿åŠ¨æ¨¡å‹**\n   - è®¡åˆ’å¼•å…¥**æ›´çµæ´»çš„è¿åŠ¨æ¨¡å‹ï¼ˆlocomotion modelï¼‰**ï¼Œä»¥æå‡æœºå™¨äººçš„ç§»åŠ¨é€Ÿåº¦ã€çµæ´»æ€§å’Œç¯å¢ƒé€šè¿‡èƒ½åŠ›ã€‚\n\n4. **å…¶ä»–æ½œåœ¨æ–¹å‘ï¼ˆéšå«äºå…¨æ–‡ï¼‰**\n   - **è®°å¿†ä¸æ¨ç†çš„è¿›ä¸€æ­¥ä¼˜åŒ–**ï¼šå½“å‰çš„è¯­è¨€è®°å¿†å’Œè‡ªé€‚åº”æ¨ç†æœºåˆ¶ä»æœ‰æå‡ç©ºé—´ï¼Œæœªæ¥å¯æ¢ç´¢æ›´é«˜æ•ˆçš„è®°å¿†å‹ç¼©ã€æ£€ç´¢å’Œæ¨ç†è§¦å‘æœºåˆ¶ã€‚\n   - **è®­ç»ƒæ•°æ®ä¸æ•ˆç‡**ï¼šè®ºæ–‡æåˆ°è¿‡åº¦è®­ç»ƒä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼Œæœªæ¥éœ€ç ”ç©¶æ›´ä¼˜çš„è®­ç»ƒç­–ç•¥ã€æ•°æ®å¢å¼ºæˆ–è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ã€‚\n   - **æ³›åŒ–ä¸é²æ£’æ€§**ï¼šè™½ç„¶å·²å±•ç¤ºè¾ƒå¼ºçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨æç«¯ç¯å¢ƒï¼ˆå¦‚æç«¯å…‰ç…§ã€å¯†é›†åŠ¨æ€éšœç¢ï¼‰ä¸‹çš„é²æ£’æ€§ä»éœ€è¿›ä¸€æ­¥éªŒè¯å’Œæå‡ã€‚\n\n## æ€»ç»“\nVLingNav åœ¨è‡ªé€‚åº”æ¨ç†ã€è¯­è¨€è®°å¿†å’Œé›¶æ ·æœ¬æ³›åŒ–æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æ ¸å¿ƒå±€é™æ€§é›†ä¸­åœ¨**æ„ŸçŸ¥å¹¿åº¦ã€å†³ç­–é€Ÿåº¦å’Œè¿åŠ¨çµæ´»æ€§**ä¸Šã€‚æœªæ¥çš„æ”¹è¿›å°†ä¸»è¦å›´ç»•**å¤šæ¨¡æ€æ„ŸçŸ¥ã€é«˜é¢‘å†³ç­–æ¶æ„å’Œé«˜çº§è¿åŠ¨æ§åˆ¶**å±•å¼€ï¼Œä»¥æ¨åŠ¨å…¶åœ¨æ›´å¤æ‚ã€åŠ¨æ€çš„å®æ™¯ç¯å¢ƒä¸­è½åœ°åº”ç”¨ã€‚",
      "timestamp": "2026-01-20T17:55:56.258123",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    }
  ],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-13T15:43:43Z",
  "created_at": "2026-01-20T17:49:45.503471",
  "updated_at": "2026-01-20T17:49:45.503481"
}