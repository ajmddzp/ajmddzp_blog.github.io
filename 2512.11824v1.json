{
    "id": "2512.11824v1",
    "title": "ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision",
    "authors": [
        "Rosh Ho",
        "Jian Zhang"
    ],
    "abstract": "本文提出ReGlove系统，该系统将低成本商用气动康复手套改造为视觉引导的辅助矫形器。慢性上肢功能障碍影响全球数百万人，而现有的辅助技术仍价格高昂或依赖不可靠的生物信号。本平台通过集成安装在手腕上的摄像头与边缘计算推理引擎（Raspberry Pi 5），实现了无需依赖稳定肌肉信号的情境感知抓取功能。通过采用实时YOLO计算机视觉模型，系统实现了96.73%的抓取动作分类准确率，端到端延迟低于40.00毫秒。基于标准基准的物理验证结果表明，系统在YCB物体操作任务中成功率达82.71%，并在27项日常生活活动（ADL）任务中表现出可靠的性能。ReGlove总成本低于250美元，且完全由商用组件构成，为传统肌电（EMG）控制设备无法覆盖的人群提供了一种可及性强、基于视觉的上肢辅助技术基础。",
    "url": "https://arxiv.org/abs/2512.11824v1",
    "html_url": "https://arxiv.org/html/2512.11824v1",
    "html_content": "ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision\nRosh Ho\n1\nand Jian Zhang\n1\n1\nRosh Ho and Jian Zhang are with Columbia University, New York, NY, USA. E-mail: {r.ho, jz3607}@columbia.edu\nAbstract\nThis paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves\n96.73\n%\n96.73\\text{\\,}\\mathrm{\\char 37\\relax}\ngrasp classification accuracy with sub-\n40.00\nms\n40.00\\text{\\,}\\mathrm{ms}\nend-to-end latency. Physical validation using standardized benchmarks shows\n82.71\n%\n82.71\\text{\\,}\\mathrm{\\char 37\\relax}\nsuccess on YCB object manipulation and reliable performance across\n27.00\n27.00\\text{\\,}\nActivities of Daily Living (ADL) tasks. With a total cost under $\n250.00\n250.00\\text{\\,}\nand exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.\nI\nINTRODUCTION\nUpper-limb impairment resulting from stroke, spinal cord injury, or neuromuscular disorders affects over\n5.00\nmillion\n5.00\\text{\\,}\\mathrm{m}\\mathrm{i}\\mathrm{l}\\mathrm{l}\\mathrm{i}\\mathrm{o}\\mathrm{n}\nAmericans, significantly impacting independence and quality of life. While sophisticated robotic orthoses exist commercially, their high cost (often exceeding $\n10 000.00\n10\\,000.00\\text{\\,}\n) and complexity limit widespread adoption, particularly for chronic conditions requiring long-term use.\nThis work explores an alternative paradigm: functionally enhancing mass-produced, low-cost pneumatic rehabilitation gloves with vision-based control to create accessible assistive devices. Commercial pneumatic gloves present an attractive starting point, costing under $\n50.00\n50.00\\text{\\,}\nwhile offering inherent compliance and safety through soft actuation. However, they typically operate through simple manual controls or require reliable surface electromyography (sEMG) signals—a significant limitation for patients with weak or noisy muscle activation due to neurological damage.\nRecent advances in computer vision for prosthetic control demonstrate that visual context can robustly inform grasp selection\n[\ndegol2016\n,\ntaverne2019\n]\n. However, these approaches have not been systematically applied to orthotic applications using commercial components. The ReGlove system bridges this gap by integrating established computer vision techniques with affordable, commercially available hardware.\nThis paper presents three key contributions: (1) An integrated hardware-software architecture that transforms commercial pneumatic gloves into vision-guided orthoses using readily available components; (2) A lightweight perception pipeline based on YOLO architectures that achieves real-time grasp classification on edge computing hardware; and (3) A comprehensive performance evaluation establishing baseline functionality across standardized benchmarks including YCB object manipulation and ADL tasks. Through this proof-of-concept, we demonstrate a viable pathway toward assistive devices that balance capability with accessibility.\nII\nRELATED WORK\nII-A\nActuation for Hand Assistance\nHand assistive devices primarily employ cable-driven or pneumatic actuation. Cable-driven systems\n[\nkim2025\n,\nrose2019\n]\ntransmit force from proximal motors through tendon-like mechanisms, offering precise control but suffering from mechanical complexity, cable management issues, and limited compliance.\nPneumatic actuators, used in commercial rehabilitation gloves, provide inherent compliance and safety through soft, inflatable chambers\n[\nlim2023\n]\n. Clinical evidence supports their efficacy in improving hand function, with randomized trials showing significant improvements in active range of motion and grip strength for chronic stroke patients\n[\nfardipour2022\n,\nko2023\n]\n. Their commercial availability and low cost (<$\n50.00\n50.00\\text{\\,}\n) make them a practical foundation for accessible assistive technology.\nAlternative approaches include shape-memory alloys\n[\nbutzer2021\n]\nand motorized exoskeletons, but these face challenges in reliability, weight, and cost that limit practical deployment.\nII-B\nControl Modalities\nTraditional control methods include manual triggers and surface electromyography (sEMG). Manual control requires use of the contralateral limb, making it impractical for independent use. sEMG-based control can enable more natural actuation but often fails for patients with weak or noisy signals due to neuromuscular degeneration\n[\nxu2025\n]\n.\nVision-based control, successfully demonstrated in prosthetic systems\n[\ndegol2016\n,\nzhang2025\n]\n, offers a promising alternative by relying on object context rather than biological signals. Prior work primarily used computationally intensive architectures like VGG-16, limiting real-time performance on low-power hardware. We adapt this approach using modern YOLO architectures optimized for edge deployment, making vision-based control practical for orthotic applications where EMG may be unreliable.\nIII\nSYSTEM DESIGN\nThe ReGlove system integrates a pneumatic glove with a vision-based control pipeline (Fig.\n2\n). A wrist-mounted camera captures the visual scene, a Raspberry Pi 5 runs the grasp classifier, and an ESP32 microcontroller operates the pneumatic components. A binary intent signal (tactile switch or sEMG) initiates the control loop.\nIII-A\nHardware Implementation\nFigure 1:\nComplete wiring schematic for the pneumatic control system, illustrating connections between the Raspberry Pi 5, ESP32 microcontroller, solenoid valves, air pumps, and power supply components. The diagram shows both digital control signals and pneumatic pathways.\nThe pneumatic subsystem uses a commercial rehabilitation glove with ethylene-vinyl acetate (EVA) bellows actuators, providing one degree of freedom per finger for bidirectional flexion and extension. We employ two HG095 mini air pumps (\n6.00\nl\nmin\n−\n1\n6.00\\text{\\,}\\mathrm{l}\\text{\\,}{\\mathrm{min}}^{-1}\nflow rate) for inflation and vacuum generation, and six ZHV-0519 three-way solenoid valves for individual finger control.\nSafety Considerations: The system incorporates multiple safety features including an exhaust solenoid that actively regulates pressure during flexion cycles, preventing over-pressurization and ensuring fail-safe operation. This design eliminates risk of actuator failure or user injury from excessive pressure buildup, maintaining compliance with soft robotic safety standards for human-worn devices.\nThumb Adaptation: The commercial glove’s single-DOF design limits thumb opposition. We address this with a custom 3D-printed thermoplastic polyurethane (TPU) brace that maintains partial abduction while allowing pneumatic flexion, preserving capability for most functional grasp types\n[\nzheng2011\n]\n.\nPneumatic Circuit: The system employs a semi-closed loop design with separate inflation and deflation subloops. During extension, the inflation pump activates while selected finger solenoids open; during flexion, the vacuum pump activates with reversed valve states. An exhaust solenoid regulates pressure between cycles.\nControl Inputs: While the system architecture supports multiple input modalities (sEMG, EEG, EOG), we use a simple tactile switch for benchtop validation to isolate vision system performance. This allows future drop-in replacement with sEMG once IRB approval is secured for clinical studies.\nThe total hardware cost is approximately $\n235.00\n235.00\\text{\\,}\n(Table\nI\n), with detailed specifications in supplementary materials.\nTABLE I:\nHardware Cost Breakdown (as of October 2025)\nComponent\nCost (USD)\nPneumatic glove with finger control\n$17.00\nZHV-0519 three-way solenoid valves (×6)\n$19.50\nVinyl tubing (4 ×\n5\nmm\n5\\text{\\,}\\mathrm{mm}\n)\n$7.50\nHG095 12 V DC,\n6\nL\nmin\n−\n1\n6\\text{\\,}\\mathrm{L}\\text{\\,}{\\mathrm{min}}^{-1}\nair pumps (×2)\n$3.46\nESP32-WROOM-32D Microcontroller\n$4.29\nRaspberry Pi 5 (8 GB)\n$81.19\nLogitech c270 (wrist-mounted camera)\n$24.00\nMyoWare sEMG sensors\n$39.90\nIRLZ44N MOSFET (×8)\n$8.96\n12 V rechargeable battery\n$28.99\nTotal\n$234.79\nNote: Costs are approximate and vary based on supplier.\nIII-B\nVision Pipeline & Model Development\nFigure 2:\nEnd-to-end system workflow.\nThe wrist-mounted camera captures the visual scene and streams RGB frames to the Raspberry Pi 5 for inference using the lightweight YOLO-based grasp classifier.\nThe predicted grasp type is forwarded to the ESP32 microcontroller, which manages valve-switching logic for the pneumatic circuit and actuates the glove accordingly.\nA binary intent signal (tactile switch or sEMG) initiates the control loop, while the pumps and solenoid manifold generate positive or negative pressure to drive finger extension or flexion.\nThis diagram summarizes the integration of sensing, inference, pneumatic routing, and actuation within the complete assistive architecture.\nWe used a grasp classification system using three publicly available datasets: DeepGrasping (\n885.00\n885.00\\text{\\,}\nimages)\n[\ndegol2016\n]\n, ImageNet subset (\n5180\n5180\\text{\\,}\nimages), and HandCam (\n250\n250\\text{\\,}\nimages)\n[\ntaverne2019\n]\n. To address class imbalance, we applied extensive data augmentation including geometric transformations, photometric adjustments, and occlusion modeling, yielding approximately\n2000\n2000\\text{\\,}\nimages per grasp type (pinch, power, three-jaw chuck, tool, key).\nWe evaluated multiple architectures under identical training conditions:\n•\nVGG-16 & VGG-16 + Depth: Baseline models replicating prior work\n[\ndegol2016\n]\n•\nYOLO v11 & v12: Modern lightweight object detectors optimized for edge deployment\nDepth augmentation using synthetic depth maps from DepthAnything\n[\nyang2024\n]\ndid not improve performance, likely due to inconsistency in synthetic depth quality. Both YOLO variants significantly outperformed VGG-based approaches (Table\nII\n), with YOLO v11 achieving\n96.67\n%\n96.67\\text{\\,}\\mathrm{\\char 37\\relax}\naccuracy versus\n82.59\n%\n82.59\\text{\\,}\\mathrm{\\char 37\\relax}\nfor VGG-16. YOLO’s superior performance stems from architectural features that preserve spatial structure (SPPF, FPN/PAN layers) and integrated augmentation mechanisms that improve robustness to lighting and background variation.\nGiven its optimal accuracy-latency tradeoff, we selected YOLO v11 for system integration, achieving\n0.90\nms\n0.90\\text{\\,}\\mathrm{ms}\ninference latency on Raspberry Pi 5—well below the\n10.00\nms\nto\n20.00\nms\n10.00\\text{\\,}\\mathrm{ms}20.00\\text{\\,}\\mathrm{ms}\nthreshold for human-perceptible feedback\n[\njerald2009relatingscene\n]\n.\nTABLE II:\nGrasp Classification Model Performance Comparison\nModel\nAccuracy (%)\nInference Time (ms)\nVGG-16\n82.59\n82.59\\text{\\,}\n7.24\n7.24\\text{\\,}\n±\n0.45\n0.45\\text{\\,}\nVGG-16 + Depth\n79.91\n79.91\\text{\\,}\n7.32\n7.32\\text{\\,}\n±\n0.52\n0.52\\text{\\,}\nYOLO v11\n96.67\n96.67\\text{\\,}\n0.90\n0.90\\text{\\,}\n±\n0.15\n0.15\\text{\\,}\nYOLO v12\n96.45\n96.45\\text{\\,}\n0.50\n0.50\\text{\\,}\n±\n0.08\n0.08\\text{\\,}\nIV\nEXPERIMENTAL RESULTS\nIV-A\nGrasp Classification Performance\nThe YOLO v11 model achieved a mean grasp classification accuracy of\n96.67\n%\n96.67\\text{\\,}\\mathrm{\\char 37\\relax}\n(\n95.00\n%\n95.00\\text{\\,}\\mathrm{\\char 37\\relax}\nCI:\n95.20\n%\nto\n97.80\n%\n95.20\\text{\\,}\\mathrm{\\char 37\\relax}97.80\\text{\\,}\\mathrm{\\char 37\\relax}\n) on the test set. Analysis of the confusion matrix (supplementary Fig. S1) revealed that most misclassifications occurred between geometrically similar pinch and three-jaw chuck grasps. Performance degradation was primarily observed for scale-ambiguous objects where visual cues alone were insufficient to infer absolute size.\nThe model’s inference latency of\n0.90\n±\n0.15\n$0.90\\text{\\,}$\\pm$0.15\\text{\\,}$\nms enables real-time operation, with total image preprocessing and classification completing in under\n2.00\nms\n2.00\\text{\\,}\\mathrm{m}\\mathrm{s}\n. This represents a\n8.00\n×\n8.00\\text{\\,}\\times\nspeedup compared to VGG-16 while maintaining superior accuracy.\nIV-B\nPhysical Grasping Performance\nWe evaluated physical grasping capability using standardized benchmarks to assess functional utility.\nIV-B1\nYCB Object Set\nUsing the YCB Gripper Assessment Protocol\n[\ncalli2015\n]\n, ReGlove achieved an overall success rate of\n82.71\n%\n82.71\\text{\\,}\\mathrm{\\char 37\\relax}\n(\n215.50\n215.50\\text{\\,}\n/\n260.50\n260.50\\text{\\,}\npoints). Performance was robust for objects with defined edges and surfaces (cups, blocks, utensils) but lower for small, smooth, or low-friction items (marbles, coins, washers). This performance gap primarily reflects mechanical limitations of the compliant EVA actuators rather than perception errors. Full results are available in supplementary materials (Table S-III).\nIV-B2\nActivities of Daily Living (ADL)\nOn a subset of\n27.00\n27.00\\text{\\,}\nADL tasks based on Matheus & Dollar\n[\nmatheus2010\n]\n, the system achieved a mean performance score of\n2.65\n±\n0.28\n$2.65\\text{\\,}$\\pm$0.28\\text{\\,}$\nout of\n3.00\n3.00\\text{\\,}\n(\n0.00\n0.00\\text{\\,}\n=failed,\n3.00\n3.00\\text{\\,}\n=excellent). The system excelled at tasks involving power or tripod grasps (pouring liquids, manipulating utensils) but struggled with fine manipulation requiring precise fingertip control (unwrapping tablets, rotating small bolts).\n0\n2\n2\n4\n4\n6\n6\n8\n8\n10\n10\n12\n12\n14\n14\n16\n16\n18\n18\n20\n20\n22\n22\n24\n24\n26\n26\n28\n28\n30\n30\n32\n32\n34\n34\nFill water\nFill in coffee\nPour coffee\nOpen jar\nPut skillet on stove\nServe food\nPour juice\nPour wine\nServe wine\nSweep the floor\nSweep dust\nFluff up a pillow\nGrasp garment\nIron\nFold shirt\nOpen pen\nWrite\nErase\nScrew a bolt\nPaint\nGlue\nUnpack tablets\nApply toothpaste\nComb hair\nDon glasses\nOpen handbag\nStrap shoes\n4.82\n3.40\n5.73\n9.77\n5.32\n12.99\n6.90\n7.37\n4.70\n9.62\n10.11\n4.36\n7.12\n17.78\n13.95\n2.28\n23.93\n14.31\n12.68\n9.80\n8.77\n11.12\n12.92\n5.82\n6.11\n2.49\n2.88\n9.72\n8.97\n10.46\n16.11\n9.48\n18.81\n12.51\n13.75\n10.25\n16.06\n15.95\n5.21\n9.23\n23.50\n19.20\n6.87\n28.68\n20.45\n31.40\n14.63\n14.66\n20.12\n15.70\n8.35\n6.11\n7.30\n7.82\nExecution Time (s)\nADL Tasks\nHuman Execution Time\nReGlove Execution Time\nFigure 3:\nComparative analysis of human versus ReGlove execution times across\n27.00\n27.00\\text{\\,}\nActivities of Daily Living (ADL) tasks. Blue bars represent average human performance, while red bars show ReGlove-assisted performance.\nMulti-phase operations revealed limitations in sequential grasp switching, highlighting the need for more sophisticated control hierarchies. Complete task-by-task results are provided in supplementary materials (Table S-IV, Figure S2).\nFigure 4:\nHand configuration comparisons: (a) bare hand, (b) hand with 3D printed thumb brace, (c) complete orthosis glove worn over thumb brace. The brace maintains functional thumb positioning while allowing pneumatic flexion.\nIV-C\nIntegrated System Performance\nThe complete assistive system achieved end-to-end latency of\n38.00\n±\n6.40\n$38.00\\text{\\,}$\\pm$6.40\\text{\\,}$\nms from image capture to glove actuation, confirming real-time responsiveness for interactive use. The system reliably executed all five grasp types under live inference conditions without performance degradation during extended operation.\nDuring\n90.00\n90.00\\text{\\,}\n-minute continuous testing sessions, the waist-mounted pneumatic unit maintained stable operation without overheating or pressure drift. Average power consumption was\n10.30\n±\n1.20\n$10.30\\text{\\,}$\\pm$1.20\\text{\\,}$\nW, compatible with commercially available\n12.00\nV\n12.00\\text{\\,}\\mathrm{V}\nportable battery packs for untethered operation.\nTABLE III:\nSummary of System Performance Evaluation\nMetric\nPerformance\nSoftware Performance\nGrasp Classification Accuracy\n96.67\n%\n96.67\\text{\\,}\\mathrm{\\char 37\\relax}\nInference Latency\n0.90\n±\n0.15\n$0.90\\text{\\,}$\\pm$0.15\\text{\\,}$\nms\nHardware Performance\nYCB Object Success Rate\n82.71\n%\n82.71\\text{\\,}\\mathrm{\\char 37\\relax}\nADL Task Score (\n0.00\nto\n3.00\n0.00\\text{\\,}3.00\\text{\\,}\n)\n2.65\n±\n0.28\n$2.65\\text{\\,}$\\pm$0.28\\text{\\,}$\nIntegrated System\nEnd-to-End Latency\n38.00\n±\n6.40\n$38.00\\text{\\,}$\\pm$6.40\\text{\\,}$\nms\nAverage Power Draw\n10.30\n±\n1.20\n$10.30\\text{\\,}$\\pm$1.20\\text{\\,}$\nW\nContinuous Operation Duration\n90.00\n90.00\\text{\\,}\nminutes\nV\nDISCUSSION\nThe ReGlove system demonstrates that commercial pneumatic rehabilitation gloves can be effectively converted into vision-guided assistive orthoses through integration with modern computer vision and low-cost computing hardware. This approach offers a affordable (under $\n250.00\n250.00\\text{\\,}\n), non-invasive pathway toward functional hand assistance that circumvents the limitations of EMG-based control.\nV-A\nTechnical Performance and Significance\nThe system’s\n96.67\n%\n96.67\\text{\\,}\\mathrm{\\char 37\\relax}\ngrasp classification accuracy and\n38.00\nms\n38.00\\text{\\,}\\mathrm{m}\\mathrm{s}\nend-to-end latency compare favorably with prior vision-based prosthetic systems requiring more complex hardware\n[\ndegol2016\n,\ntaverne2019\n]\n. More significantly, by relying exclusively on visual context rather than biological signals, the approach extends accessibility to patient populations with unreliable EMG due to neuromuscular degeneration\n[\nxu2025\n]\n.\nThe performance gap between software perception (\n96.67\n%\n96.67\\text{\\,}\\mathrm{\\char 37\\relax}\naccuracy) and physical execution (\n82.71\n%\n82.71\\text{\\,}\\mathrm{\\char 37\\relax}\nYCB success) highlights the mechanical limitations of commercial pneumatic gloves rather than perception shortcomings. This suggests that relatively simple hardware improvements—such as high-friction fingertip coatings or reinforced actuator segments—could significantly enhance functional performance without increasing system complexity or cost.\nV-B\nLimitations and Design Considerations\nSeveral important limitations warrant discussion. The current ”pause-and-select” control paradigm requires users to position their hand and trigger a single, static grasp. This does not support dynamic tasks requiring mid-manipulation grasp adjustments or provide mechanisms for user correction of mispredicted grasps.\nThe system’s performance with small, smooth objects remains limited by the compliant nature of pneumatic actuation. While this compliance enhances safety, it reduces precision for fine manipulation tasks. Future iterations could incorporate variable-stiffness mechanisms or hybrid actuation approaches to balance safety and dexterity.\nOur benchtop validation used a healthy operator, which allowed controlled testing of core functionality but leaves open questions about real-world performance with impaired users. The simplified binary intent detection (tactile switch) served as a reliable trigger for technical validation but may not reflect the control challenges faced by target users.\nV-C\nFuture Directions\nBuilding on this proof-of-concept, several research directions appear promising:\n•\nMulti-modal control integration: Subsequent iterations will incorporate surface electromyography (sEMG) as the primary intent detection modality, operating in concert with the existing vision-based grasp classification. This hybrid approach will enable more natural actuation paradigms while maintaining the robustness of visual context awareness. Additionally, implementation of closed-loop force control will enhance manipulation precision and user experience.\n•\nHardware refinement: Improved actuator geometry, high-friction surfaces, and variable-stiffness mechanisms to enhance grip stability and fine manipulation capability.\n•\nControl hierarchy expansion: Temporal grasp sequencing and gesture prediction to enable complex, multi-phase tasks like opening containers or using tools.\n•\nClinical translation: Formal studies with stroke and SCI patients to quantify ADL improvement, user acceptance, and long-term usability.\n•\nSystem integration: Miniaturization of pneumatic components and development of fully self-contained wearable form factors.\nThe modular architecture supports incremental improvement in each of these areas while maintaining the core benefits of affordability and accessibility.\nVI\nCONCLUSION\nThis work presents ReGlove, an end-to-end demonstration of vision-guided pneumatic hand assistance using exclusively commercial components and open-source software. The system achieves real-time dexterous grasping with\n96.67\n%\n96.67\\text{\\,}\\mathrm{\\char 37\\relax}\nclassification accuracy and\n82.71\n%\n82.71\\text{\\,}\\mathrm{\\char 37\\relax}\nphysical success on standardized benchmarks, while maintaining a total cost under $\n250.00\n250.00\\text{\\,}\n.\nBy bridging affordable rehabilitation hardware with modern computer vision, ReGlove offers a practical pathway toward restoring functional hand capability for individuals with chronic upper-limb impairment. The approach demonstrates that intelligent assistive technology need not be complex or expensive to be effective, providing a foundation for future development of accessible devices that can significantly impact quality of life for underserved populations.\nSUPPLEMENTARY MATERIALS\nAdditional materials are available as ancillary files with this arXiv submission, including:\n•\nConfusion matrix analysis (Fig. S1)\n•\nComplete YCB benchmark results (Table S-III)\n•\nDetailed ADL task performance (Table S-IV, Figure S2)\n•\nHardware specifications and wiring diagrams",
    "preview_text": "This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \\SI{96.73}{\\percent} grasp classification accuracy with sub-\\SI{40.00}{\\milli\\second} end-to-end latency. Physical validation using standardized benchmarks shows \\SI{82.71}{\\percent} success on YCB object manipulation and reliable performance across \\SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \\$\\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.\n\nReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision\nRosh Ho\n1\nand Jian Zhang\n1\n1\nRosh Ho and Jian Zhang are with Columbia University, New York, NY, USA. E-mail: {r.ho, jz3607}@columbia.edu\nAbstract\nThis paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves\n96.73\n%\n96.73\\text{\\,}\\mathrm{\\char 37\\relax}\ngrasp classification",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "computer vision",
        "edge computing",
        "assistive technology",
        "real-time inference",
        "YCB object manipulation",
        "Activities of Daily Living (ADL)"
    ],
    "one_line_summary": "ReGlove是一种基于腕部视觉引导的软体气动手套，用于日常生活活动辅助，不涉及生成模型或扩散模型技术。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T01:06:59Z",
    "created_at": "2026-01-09T11:23:18.746861",
    "updated_at": "2026-01-09T11:23:18.746871",
    "flag": true
}