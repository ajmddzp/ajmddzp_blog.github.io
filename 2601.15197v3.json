{
    "id": "2601.15197v3",
    "title": "LangForce: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "authors": [
        "Shijie Lian",
        "Bin Yu",
        "Xiaopeng Lin",
        "Laurence T. Yang",
        "Zhaolong Shen",
        "Changti Wu",
        "Yuzhuo Miao",
        "Cong Huang",
        "Kai Chen"
    ],
    "abstract": "è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å¸¸éš¾ä»¥æ³›åŒ–è‡³æ–°æŒ‡ä»¤æˆ–å¤æ‚å¤šä»»åŠ¡åœºæ™¯ã€‚æˆ‘ä»¬å‘ç°åœ¨å½“å‰è®­ç»ƒèŒƒå¼ä¸­å­˜åœ¨ä¸€ä¸ªå…³é”®ç¼ºé™·ï¼šç›®æ ‡é©±åŠ¨çš„æ•°æ®æ”¶é›†ä¼šå¯¼è‡´æ•°æ®é›†åå·®ã€‚åœ¨æ­¤ç±»æ•°æ®é›†ä¸­ï¼Œä»…å‡­è§†è§‰è§‚å¯Ÿå³å¯é«˜åº¦é¢„æµ‹è¯­è¨€æŒ‡ä»¤ï¼Œå¯¼è‡´æŒ‡ä»¤ä¸åŠ¨ä½œä¹‹é—´çš„æ¡ä»¶äº’ä¿¡æ¯è¶‹è¿‘äºé›¶ï¼Œæˆ‘ä»¬å°†æ­¤ç°è±¡ç§°ä¸ºä¿¡æ¯åç¼©ã€‚å…¶ç»“æœæ˜¯æ¨¡å‹é€€åŒ–ä¸ºä»…ä¾èµ–è§†è§‰çš„ç­–ç•¥ï¼Œå¿½ç•¥è¯­è¨€çº¦æŸå¹¶åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸­å¤±æ•ˆã€‚\n\nä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºLangForceæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡è´å¶æ–¯åˆ†è§£å¼ºåˆ¶å®ç°æŒ‡ä»¤è·Ÿéšã€‚é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒåˆ†æ”¯æ¶æ„ï¼Œåˆ†åˆ«ä¼°è®¡ä»…åŸºäºè§†è§‰çš„å…ˆéªŒåˆ†å¸ƒ$p(a \\mid v)$å’Œè¯­è¨€æ¡ä»¶çš„åéªŒåˆ†å¸ƒ$Ï€(a \\mid v, \\ell)$ã€‚éšåé€šè¿‡æœ€å¤§åŒ–åŠ¨ä½œä¸æŒ‡ä»¤é—´çš„æ¡ä»¶ç‚¹äº’ä¿¡æ¯ï¼ˆPMIï¼‰æ¥ä¼˜åŒ–ç­–ç•¥ã€‚è¯¥ç›®æ ‡å‡½æ•°èƒ½æœ‰æ•ˆæŠ‘åˆ¶è§†è§‰æ·å¾„ä¾èµ–ï¼Œå¹¶å¯¹æ˜¾å¼è§£é‡Šè¯­è¨€æŒ‡ä»¤çš„åŠ¨ä½œç»™äºˆå¥–åŠ±ã€‚\n\næ— éœ€é¢å¤–æ•°æ®ï¼ŒLangForceæ˜¾è‘—æå‡äº†æ³›åŒ–èƒ½åŠ›ã€‚åœ¨SimplerEnvå’ŒRoboCasaå¹³å°ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†å…¶æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…¶ä¸­åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„OOD SimplerEnvåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†11.3%çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†æœ¬æ–¹æ³•åœ¨åŠ¨ä½œä¸­é²æ£’åœ°é”šå®šè¯­è¨€çš„èƒ½åŠ›ã€‚",
    "url": "https://arxiv.org/abs/2601.15197v3",
    "html_url": "https://arxiv.org/html/2601.15197v3",
    "html_content": "BayesianVLA\n: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries\nShijie Lian\n1,2,\nBin Yu\n2,4,\n1\n1\nfootnotemark:\n1\nXiaopeng Lin\n2,5,\n1\n1\nfootnotemark:\n1\nLaurence T. Yang\n6,1,\nZhaolong Shen\n2,7\nChangti Wu\n2,8\nYuzhuo Miao\n1,2\nCong Huang\n2,3\nKai Chen\n2,3,9,\n2\n2\nfootnotemark:\n2\n1\nHUST\n2\nZGCA\n3\nZGCI\n4\nHIT\n5\nHKUST(GZ)\n6\nZZU\n7\nBUAA\n8\nECNU\n9\nDeepCybo\nEqual contributionCorresponding author\nAbstract\nVision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios.\nWe identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias.\nIn such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term\nInformation Collapse\n.\nConsequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings.\nTo address this, we propose\nBayesianVLA\n, a novel framework that enforces instruction following via Bayesian decomposition.\nBy introducing learnable\nLatent Action Queries\n, we construct a dual-branch architecture to estimate both a vision-only prior\np\nâ€‹\n(\na\nâˆ£\nv\n)\np(a\\mid v)\nand a language-conditioned posterior\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\n\\pi(a\\mid v,\\ell)\n.\nWe then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions.\nThis objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command.\nWithout requiring new data,\nBayesianVLA\nsignificantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an\n11.3%\nimprovement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.\nBayesianVLA\n: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries\nShijie Lian\n1,2,\nâ€ \nâ€ \nthanks:\nEqual contribution\nBin Yu\n2,4,\n1\n1\nfootnotemark:\n1\nXiaopeng Lin\n2,5,\n1\n1\nfootnotemark:\n1\nLaurence T. Yang\n6,1,\nâ€ \nâ€ \nthanks:\nCorresponding author\nZhaolong Shen\n2,7\nChangti Wu\n2,8\nYuzhuo Miao\n1,2\nCong Huang\n2,3\nKai Chen\n2,3,9,\n2\n2\nfootnotemark:\n2\n1\nHUST\n2\nZGCA\n3\nZGCI\n4\nHIT\n5\nHKUST(GZ)\n6\nZZU\n7\nBUAA\n8\nECNU\n9\nDeepCybo\nhttps://github.com/ZGC-EmbodyAI/BayesianVLA\n1\nIntroduction\nVision-Language-Action (VLA) models\n(Kim\net al.\n,\n2024\n; Liu\net al.\n,\n2025\n; Bjorck\net al.\n,\n2025b\n; Black\net al.\n,\n2025\n)\nhave emerged as a promising paradigm for general-purpose robot manipulation, leveraging the vast knowledge of pre-trained Vision-Language Models (VLMs) to ground natural language instructions into physical actions.\nBy training on large-scale datasets of human demonstrations, these models aim to learn a policy\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\n\\pi(a\\mid v,\\ell)\nthat can execute diverse tasks specified by language\nâ„“\n\\ell\ngiven visual observations\nv\nv\n.\nWhile demonstrating strong performance in in-distribution settings, current VLA models still face challenges in generalizing to novel instructions or complex multi-task scenarios, particularly in out-of-distribution (OOD) environments\n(Xu\net al.\n,\n2025\n)\n. This limitation is especially pronounced during post-training, where fine-tuning on narrow, task-specific datasets can lead to catastrophic forgetting of the VLMâ€™s general capabilities and impair its ability to generalize to new tasks.\nWe hypothesize that this fragility is exacerbated by a prevalent bias in current robotic datasets.\nMost robotic datasets are collected in a goal-driven manner, where a human operator performs a specific task repeatedly in a fixed scene. In such datasets, the mapping from visual scene\nv\nv\nto language instruction\nâ„“\n\\ell\nis nearly injective: seeing a cabinet in the scene almost invariably implies the task â€œopen the cabinet,â€ while seeing a bottle implies â€œpick up the bottle.â€ This deterministic coupling results in a sharp conditional distribution\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\np(\\ell\\mid v)\n.\nFrom a Bayesian perspective, the optimal policy can be decomposed as:\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\n=\np\nâ€‹\n(\nâ„“\nâˆ£\na\n,\nv\n)\nâ€‹\np\nâ€‹\n(\na\nâˆ£\nv\n)\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\n.\n\\pi(a\\mid v,\\ell)=\\frac{p(\\ell\\mid a,v)\\,p(a\\mid v)}{p(\\ell\\mid v)}.\n(1)\nHere,\np\nâ€‹\n(\na\nâˆ£\nv\n)\np(a\\mid v)\nrepresents a vision-only prior (i.e., what actions are likely in this scene?), and\np\nâ€‹\n(\nâ„“\nâˆ£\na\n,\nv\n)\np(\\ell\\mid a,v)\nis the likelihood (i.e., how well does action\na\na\nexplain instruction\nâ„“\n\\ell\n?).\nWhen\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\np(\\ell\\mid v)\nis sharp, the model can predict\nâ„“\n\\ell\nsolely from\nv\nv\nwithout attending to\na\na\n.\nConsequently, the likelihood term\np\nâ€‹\n(\nâ„“\nâˆ£\na\n,\nv\n)\np(\\ell\\mid a,v)\ncollapses to\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\np(\\ell\\mid v)\n, and the posterior policy degenerates to the prior:\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\nâ‰ˆ\np\nâ€‹\n(\na\nâˆ£\nv\n)\n.\n\\pi(a\\mid v,\\ell)\\approx p(a\\mid v).\n(2)\nIn other words, the model effectively ignores the language instruction, learning a â€œvision shortcutâ€ that fails whenever the task is ambiguous or the environment changes.\nTo address this, we propose\nBayesianVLA\n, a novel framework that explicitly enforces instruction following via Bayesian decomposition.\nOur key insight is to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions, which is equivalent to maximizing the log-likelihood ratio (LLR):\nlog\nâ¡\np\nâ€‹\n(\nâ„“\nâˆ£\na\n,\nv\n)\nâˆ’\nlog\nâ¡\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\n\\log p(\\ell\\mid a,v)-\\log p(\\ell\\mid v)\n.\nThis objective penalizes the vision shortcut by requiring the action\na\na\nto provide\nadditional\ninformation about\nâ„“\n\\ell\nthat cannot be inferred from\nv\nv\nalone.\nWe instantiate this framework by introducing\nLatent Action Queries\nâ€”a set of learnable tokens injected into the VLM.\nThese queries serve a dual purpose: they act as a bottleneck to extract action-relevant features for a downstream Diffusion Transformer (DiT) policy, and they enable a dual-branch training strategy.\nIn the\nPriori Branch\n, queries attend only to vision to learn\np\nâ€‹\n(\na\nâˆ£\nv\n)\np(a\\mid v)\n; in the\nPosteriori Branch\n, they attend to both vision and language to learn\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\n\\pi(a\\mid v,\\ell)\n. By optimizing the LLR between these branches,\nBayesianVLA\nlearns to ground language robustly without requiring new data.\nOur contributions are three-fold:\n1.\nWe identify and empirically validate the â€œvision shortcutâ€ pathology in current VLA training, showing that standard models often ignore language in favor of dataset-specific visual correlations.\n2.\nWe propose\nBayesianVLA\n, a method that leverages Latent Action Queries and a dual-branch Bayesian objective to recover the true language-conditioned policy from biased data.\n3.\nWe demonstrate that\nBayesianVLA\nachieves state-of-the-art performance on SimplerEnv and RoboCasa, with a remarkable\n11.3%\nimprovement in OOD generalization on SimplerEnv, proving its effectiveness in breaking the vision shortcut.\n2\nMotivation: The Illusion of Instruction Following\nBefore detailing our method, we present empirical evidence to substantiate our hypothesis: that standard VLA models trained on goal-driven datasets often learn a vision-only policy\np\nâ€‹\n(\na\nâˆ£\nv\n)\np(a\\mid v)\nrather than a true language-conditioned policy\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\n\\pi(a\\mid v,\\ell)\n.\nSpecifically, we employ the Qwen3VL-4B-GR00T model from starVLA\n(starVLA,\n2025\n)\nas our representative VLA architecture.\nWe conduct three pilot experiments to reveal this\nillusion of instruction following\n.\nIn all three experiments, we train the model by feeding\nonly\nthe visual observation\nv\nv\n(masking the language instruction\nâ„“\n\\ell\n), effectively testing the vision-only prior\np\nâ€‹\n(\na\nâˆ£\nv\n)\np(a\\mid v)\n.\n2.1\nExperiment 1: The Vision Shortcut in ID Testing\nWe first train a standard VLA model on a subset of the Humanoid robot tabletop manipulation data from PhysicalAI-Robotics-GR00T-X-Embodiment-Sim\n(Bjorck\net al.\n,\n2025b\n)\nand evaluate on 24 tasks from the RoboCasa benchmark\n(Nasiriany\net al.\n,\n2024\n)\n.\nAveraged across all 24 tasks, the vision-only model achieves a success rate of 44.6%, which is close to the language-conditioned baseline of 47.8%.\nThis small gap reveals that the model can succeed without relying on language instructions, as the training and evaluation scenes and tasks are highly similar, enabling the model to learn a near-deterministic mapping from vision to action. Figure\n1\nprovides a relevant example.\nFigure 1:\nExamples of the vision shortcut in RoboCasa\n(Nasiriany\net al.\n,\n2024\n)\n. Training data exhibits visual diversity but limited task diversity. As a result, the model learns to execute tasks directly based on specific visual cues rather than relying on language instructions.\n2.2\nExperiment 2: Failure in Ambiguous Scenarios\nTo further investigate this behavior, we train a standard VLA model on the LIBERO benchmark\n(Liu\net al.\n,\n2023\n)\n, which contains four subsets: Spatial, Object, Long, and Goal.\nWe train on all four training sets and evaluate on all four test sets.\nThe vision-only model achieves success rates comparable to the full VLA model on three subsets (Spatial: 90.2%, Object: 99.6%, Long: 86.0% in Vision-Only, Spatial: 98.8%, Object: 99.0%, Long: 94.4% in Baseline), where each visual scene corresponds to a single task.\nHowever, on the LIBERO Goal subset, the vision-only success rate plummets to 9.8% (98.0% in Baseline).\nThe key difference is that LIBERO Goal presents inherent ambiguity: multiple valid tasks are associated with the same object configuration during training.\nFor instance, a scene with multiple bowls, a stove, and a drawer could correspond to either â€œput bowl in drawerâ€ or â€œput bowl on stoveâ€.\nThis confirms that while the model can exploit vision-action correlations in unambiguous datasets, when multiple tasks share the same visual context, due to a lack of language to resolve ambiguity, the model is dominated by the prior\np\nâ€‹\n(\na\nâˆ£\nv\n)\np(a\\mid v)\nlearned from dataset statistics.\nFigure\n2\n(a) illustrates examples where the same visual scene in LIBERO Goal corresponds to multiple distinct tasks.\nFigure 2:\n(a) In LIBERO Goal\n(Liu\net al.\n,\n2023\n)\n, the same scene corresponds to multiple tasks, revealing the ambiguity that vision-only models fail to resolve. (b) Action loss curves on BridgeDataV2\n(Walke\net al.\n,\n2023\n)\nshow that the vision-only model achieves comparable training loss to the full vision-language model, indicating the presence of visual shortcuts even in diverse, in-the-wild datasets.\n2.3\nExperiment 3: Catastrophic Failure in OOD Generalization\nFinally, we test the generalization capability by training on the high-quality BridgeDataV2 dataset\n(Walke\net al.\n,\n2023\n)\n(diverse, in-the-wild scenes) and evaluating on SimplerEnv\n(Li\net al.\n,\n2024c\n)\n(simulation, OOD).\nDuring training on BridgeDataV2, the vision-only model achieves an action loss of 0.13, comparable to the full language-conditioned modelâ€™s loss of 0.08 (as shown in Figure\n2\n(b)).\nThis indicates that even in diverse, in-the-wild scenarios, the model can still identify visual shortcuts (e.g., specific lighting or background features mapping to specific actions) to minimize the training objective without truly grounding the language instructions.\nHowever, this reliance on visual shortcuts proves catastrophic for generalization.\nWhen evaluated on SimplerEnv, which presents visually distinct simulation environments, the vision-only baseline achieves near 0% success.\nThis confirms that the low training loss on Bridge was achieved by overfitting to domain-specific visual patterns rather than learning generalizable manipulation skills. Consequently, when these specific visual cues are absent in the OOD environment, the policy fails completely.\n2.4\nTheoretical Insight: Information Collapse\nWe formalize the â€œvision shortcutâ€ as a collapse of the conditional mutual information (CMI) between instructions and actions. Ideally, a robust VLA policy should maintain high\nI\nâ€‹\n(\nâ„“\n;\na\nâˆ£\nv\n)\nI(\\ell;a\\mid v)\n, meaning the action choice significantly reduces uncertainty about the instruction.\nHowever, the CMI is bounded by the conditional entropy of the language:\nI\nâ€‹\n(\nâ„“\n;\na\nâˆ£\nv\n)\n=\nH\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\nâˆ’\nH\nâ€‹\n(\nâ„“\nâˆ£\na\n,\nv\n)\nâ‰¤\nH\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\n.\nI(\\ell;a\\mid v)=H(\\ell\\mid v)-H(\\ell\\mid a,v)\\leq H(\\ell\\mid v).\n(3)\nIn goal-driven datasets, the deterministic mapping\nv\nâ†’\nâ„“\nv\\to\\ell\nimplies\nH\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\nâ‰ˆ\n0\nH(\\ell\\mid v)\\approx 0\n(Xu\net al.\n,\n2025\n)\n. Consequently,\nI\nâ€‹\n(\nâ„“\n;\na\nâˆ£\nv\n)\nI(\\ell;a\\mid v)\nis forced to vanish, theoretically preventing the model from learning any dependency between\na\na\nand\nâ„“\n\\ell\nbeyond what is already captured by\nv\nv\n.\nTo break this deadlock, we cannot rely on standard likelihood maximization. Instead, we must explicitly intervene to maximize the\ninformation gain\nprovided by the action. This motivates our use of the Log-Likelihood Ratio (LLR), which effectively estimates the Pointwise Mutual Information (PMI), rewarding the policy only when it captures the specific semantics of\nâ„“\n\\ell\nthat are\nnot\npredictable from\nv\nv\n.\n3\nMethod:\nBayesianVLA\nIn this section, we introduce\nBayesianVLA\n, a framework designed to mitigate the vision shortcut in VLA models. We present the overall framework of\nBayesianVLA\nin Figure\n3\n. We first formalize the problem through a Bayesian lens (Section\n3.1\n), deriving an objective that maximizes the mutual information between actions and instructions.\nWe then present our architecture, which uses\nLatent Action Queries\nto instantiate this decomposition (Section\n3.2\n), and detail our dual-branch training strategy (Section\n3.3\n).\nFigure 3:\nThe framework of\nBayesianVLA\n. The framework employs a dual-branch architecture with shared VLM weights. The Priori Branch (left) processes\n[\nv\n,\nğ’¬\n,\nâ„“\n]\n[v,\\mathcal{Q},\\ell]\nwith causal masking to learn the vision-only prior\np\nâ€‹\n(\na\nâˆ£\nv\n)\np(a\\mid v)\n. The Posteriori Branch (right) processes\n[\nv\n,\nâ„“\n,\nğ’¬\n]\n[v,\\ell,\\mathcal{Q}]\nto learn the full policy\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\n\\pi(a\\mid v,\\ell)\n. Latent Action Queries\nğ’¬\n\\mathcal{Q}\nserve as a bottleneck interface, and the LLR objective (in Eq.\n7\n) encourages the model to maximize the information between actions and instructions. At inference, only the Posteriori Branch is used, incurring no additional computational overhead.\n3.1\nObjective Formulation\nAs established in Section\n2.4\n, standard VLA training on goal-driven datasets leads to information collapse where\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\nâ†’\np\nâ€‹\n(\na\nâˆ£\nv\n)\n\\pi(a\\mid v,\\ell)\\to p(a\\mid v)\n. To counteract this, we propose to regularize the policy by maximizing the conditional Pointwise Mutual Information (PMI) between the action and the instruction.\nThis objective can be formulated as maximizing the Log-Likelihood Ratio (LLR) between the posterior policy and the vision-only prior:\nâ„’\nLLR\n=\nlog\nâ¡\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\np\nâ€‹\n(\na\nâˆ£\nv\n)\n=\nlog\nâ¡\np\nâ€‹\n(\nâ„“\nâˆ£\na\n,\nv\n)\nâˆ’\nlog\nâ¡\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\n.\n\\mathcal{L}_{\\text{LLR}}=\\log\\frac{\\pi(a\\mid v,\\ell)}{p(a\\mid v)}=\\log p(\\ell\\mid a,v)-\\log p(\\ell\\mid v).\n(4)\nThe detailed derivation is provided in Appendix\nA\n.\nThis formulation requires us to simultaneously model the posterior\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\n\\pi(a\\mid v,\\ell)\nand the prior\np\nâ€‹\n(\na\nâˆ£\nv\n)\np(a\\mid v)\n. In the following sections, we describe how\nBayesianVLA\nefficiently instantiates these two distributions using a shared architecture with Latent Action Queries.\n3.2\nLatent Action Queries\nTo efficiently instantiate the proposed Bayesian decomposition within a unified VLM architecture, we introduce\nLatent Action Queries\n.\nWe extend the VLM vocabulary with\nK\n=\n64\nK=64\nlearnable tokens, denoted as\nğ’¬\n=\n{\n<|action_0|>\n,\nâ€¦\n,\n<|action_K|>\n}\n\\mathcal{Q}=\\{\\texttt{<|action\\_0|>},\\dots,\\texttt{<|action\\_K|>}\\}\n.\nThese queries function as a dedicated bottleneck interface between the VLM (e.g., Qwen3-VL\n(Bai\net al.\n,\n2025\n)\n) and the continuous action head (a Diffusion Transformer\n(Peebles and Xie,\n2023\n)\n).\nUnlike recent VLA architectures such as\nÏ€\n0\n\\pi_{0}\n(Black\net al.\n,\n2024\n)\nand GR00T\n(Bjorck\net al.\n,\n2025b\n; GEAR-Team\net al.\n,\n2025\n)\n, which typically feed the hidden states of all input tokens to the action expert, we append\nğ’¬\n\\mathcal{Q}\nto the input sequence and exclusively use their corresponding hidden states\nğ‡\nğ’¬\nâˆˆ\nâ„\nK\nÃ—\nD\n\\mathbf{H}_{\\mathcal{Q}}\\in\\mathbb{R}^{K\\times D}\nto condition the action head.\nThis design choice is critical: by leveraging the causal masking inherent in decoder-only VLMs, we can precisely control the information encoded in\nğ‡\nğ’¬\n\\mathbf{H}_{\\mathcal{Q}}\nsimply by changing the position of\nğ’¬\n\\mathcal{Q}\nin the input sequence. This flexibility enables the strict separation of vision-only and vision-language contexts required for our dual-branch strategy.\n3.3\nDual-Branch Training Framework\nWe propose a training paradigm with two parallel branches sharing the same VLM weights but different input structures.\n1. Priori Branch (Vision-Only).\nTo estimate the prior\np\nâ€‹\n(\na\nâˆ£\nv\n)\np(a\\mid v)\n, we construct the input sequence as:\nInput\nprior\n=\n[\nv\n,\nğ’¬\n,\nâ„“\n]\n.\n\\text{Input}_{\\text{prior}}=[v,\\mathcal{Q},\\ell].\n(5)\nDue to the causal attention mask of the decoder-only VLM, the tokens in\nğ’¬\n\\mathcal{Q}\ncan attend to the visual observation\nv\nv\nbut\ncannot\nattend to the language instruction\nâ„“\n\\ell\n(which appears later). Thus, the hidden states\nğ‡\nğ’¬\nprior\n\\mathbf{H}_{\\mathcal{Q}}^{\\text{prior}}\nencode purely visual information.\nWe use these features to predict the action\na\na\n, optimizing a flow-matching loss\nâ„’\nprior\n\\mathcal{L}_{\\text{prior}}\nto learn the datasetâ€™s inherent action bias.\n2. Posteriori Branch (Vision + Language).\nTo estimate the true policy\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\n\\pi(a\\mid v,\\ell)\n, we arrange the input as:\nInput\npost\n=\n[\nv\n,\nâ„“\n,\nğ’¬\n]\n.\n\\text{Input}_{\\text{post}}=[v,\\ell,\\mathcal{Q}].\n(6)\nHere,\nğ’¬\n\\mathcal{Q}\nappears after\nâ„“\n\\ell\n, allowing it to attend to both vision and language. The resulting hidden states\nğ‡\nğ’¬\npost\n\\mathbf{H}_{\\mathcal{Q}}^{\\text{post}}\nencode the full context. We optimize a main flow-matching loss\nâ„’\nmain\n\\mathcal{L}_{\\text{main}}\nto learn the expert action.\n3. Maximizing the Likelihood Ratio.\nIn addition to action prediction, we explicitly optimize the LLR objective. We treat the VLMâ€™s language modeling loss as a proxy for\nlog\nâ¡\np\nâ€‹\n(\nâ„“\nâˆ£\nâ€¦\n)\n\\log p(\\ell\\mid\\dots)\n.\nSpecifically, in the Priori Branch, the language tokens\nâ„“\n\\ell\nattend to\n[\nv\n,\nğ’¬\n]\n[v,\\mathcal{Q}]\n. Since\nğ’¬\n\\mathcal{Q}\nencodes the action information\na\na\n(via the prior), the probability of generating\nâ„“\n\\ell\nin this branch approximates\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n,\na\nprior\n)\np(\\ell\\mid v,a_{\\text{prior}})\n.\nIn the Posteriori Branch, we can compute a baseline\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\np(\\ell\\mid v)\nby detaching gradients or using a separate pass.\nHowever, a more direct and numerically stable approach is to maximize the difference in log-probabilities of the language tokens between the two branches. We define the LLR loss as:\nâ„’\nLLR\n=\nlog\nâ¡\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n,\nğ‡\nğ’¬\nprior\n)\nâˆ’\nsg\nâ€‹\n(\nlog\nâ¡\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\n)\n,\n\\mathcal{L}_{\\text{LLR}}=\\log p(\\ell\\mid v,\\mathbf{H}_{\\mathcal{Q}}^{\\text{prior}})-\\text{sg}\\left(\\log p(\\ell\\mid v)\\right),\n(7)\nwhere\nsg\nâ€‹\n(\nâ‹…\n)\n\\text{sg}(\\cdot)\ndenotes the stop-gradient operator. We maximize this term (minimize\nâˆ’\nâ„’\nLLR\n-\\mathcal{L}_{\\text{LLR}}\n) to force the action representations\nğ‡\nğ’¬\n\\mathbf{H}_{\\mathcal{Q}}\nto carry information that explains\nâ„“\n\\ell\n.\nThe stop-gradient operation is employed to prevent the model from trivially maximizing the ratio by degrading the baseline\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\np(\\ell\\mid v)\n(i.e., damaging the VLMâ€™s general language capabilities) rather than improving the numerator.\n3.4\nTotal Training Objective\nWe train the action decoder using the Rectified Flow Matching objective\n(Liu\net al.\n,\n2022\n; Bjorck\net al.\n,\n2025b\n)\n. Specifically, we apply this objective to both the Priori Branch (conditioned on\nğ‡\nğ’¬\nprior\n\\mathbf{H}_{\\mathcal{Q}}^{\\text{prior}}\n) and the Posteriori Branch (conditioned on\nğ‡\nğ’¬\npost\n\\mathbf{H}_{\\mathcal{Q}}^{\\text{post}}\n).\nGiven a condition\nğ‚\nâˆˆ\n{\nğ‡\nğ’¬\npost\n,\nğ‡\nğ’¬\nprior\n}\n\\mathbf{C}\\in\\{\\mathbf{H}_{\\mathcal{Q}}^{\\text{post}},\\mathbf{H}_{\\mathcal{Q}}^{\\text{prior}}\\}\n, the flow-matching loss is defined as:\nâ„’\nFM\nâ€‹\n(\nÏˆ\n;\nğ‚\n)\n=\nğ”¼\nt\n,\nğš\n0\n,\nğš\n1\nâ€‹\n[\nâ€–\nv\nÏˆ\nâ€‹\n(\nğš\nt\n,\nt\n,\nğ‚\n)\nâˆ’\n(\nğš\n1\nâˆ’\nğš\n0\n)\nâ€–\n2\n]\n,\n\\mathcal{L}_{\\text{FM}}(\\psi;\\mathbf{C})=\\mathbb{E}_{t,\\mathbf{a}_{0},\\mathbf{a}_{1}}\\left[||v_{\\psi}(\\mathbf{a}_{t},t,\\mathbf{C})-(\\mathbf{a}_{1}-\\mathbf{a}_{0})||^{2}\\right],\n(8)\nwhere\nv\nÏˆ\nv_{\\psi}\nis the Diffusion Transformer (DiT) predicting the velocity field,\nğš\n1\n\\mathbf{a}_{1}\nis the ground truth action trajectory,\nğš\n0\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nI\n)\n\\mathbf{a}_{0}\\sim\\mathcal{N}(0,I)\nis sampled from a standard Gaussian, and\nğš\nt\n=\n(\n1\nâˆ’\nt\n)\nâ€‹\nğš\n0\n+\nt\nâ€‹\nğš\n1\n\\mathbf{a}_{t}=(1-t)\\mathbf{a}_{0}+t\\mathbf{a}_{1}\nrepresents the interpolated state at timestep\nt\nâˆˆ\n[\n0\n,\n1\n]\nt\\in[0,1]\n.\nThe final training loss combines the action prediction losses from both branches with the LLR maximization term:\nâ„’\ntotal\n=\n(\n1\nâˆ’\nÎ»\n)\nâ€‹\nâ„’\nFM\nâ€‹\n(\nÏˆ\n;\nğ‡\nğ’¬\npost\n)\n+\nÎ»\nâ€‹\nâ„’\nFM\nâ€‹\n(\nÏˆ\n;\nğ‡\nğ’¬\nprior\n)\nâˆ’\nÎ²\nâ€‹\nâ„’\nLLR\n,\n\\mathcal{L}_{\\text{total}}=(1-\\lambda)\\mathcal{L}_{\\text{FM}}(\\psi;\\mathbf{H}_{\\mathcal{Q}}^{\\text{post}})+\\lambda\\mathcal{L}_{\\text{FM}}(\\psi;\\mathbf{H}_{\\mathcal{Q}}^{\\text{prior}})-\\beta\\mathcal{L}_{\\text{LLR}},\n(9)\nwhere\nÎ»\n\\lambda\nbalances the contribution of the prior and posterior action losses, and\nÎ²\n\\beta\ncontrols the strength of the LLR regularization.\nDuring inference, we exclusively execute the Posteriori Branch to obtain\nğ‡\nğ’¬\npost\n\\mathbf{H}_{\\mathcal{Q}}^{\\text{post}}\nand generate actions via the DiT.\nThis ensures that our method incurs no additional computational overhead compared to standard VLA baselines at test time\n.\n4\nExperiment\nTo comprehensively evaluate the effectiveness of\nBayesianVLA\n, we conducted extensive experiments on two simulation benchmarks: SimplerEnv, and RoboCasa.\nOur training pipeline is built upon the StarVLA framework\n(starVLA,\n2025\n)\n, distributed across 16 NVIDIA H100 GPUs, and strictly follows its default training protocols to ensure fair comparison.\nIn our experiments,\nBayesianVLA\nis instantiated on the QwenGR00T architecture from StarVLA.\nBroader simulation benchmarks and real-world robot experiments are currently underway.\nWe employ the AdamW optimizer\n(Loshchilov and Hutter,\n2017\n)\ninitialized with a learning rate of 1e-5 and a cosine annealing schedule.\nSystem-level optimizations include DeepSpeed ZeRO-2\n(Rasley\net al.\n,\n2020\n)\n, gradient clipping at a norm of 1.0, and no gradient accumulation.\nAll baseline performance metrics are obtained from their original papers or other peer-reviewed publications.\nTable 1:\nResults of evaluating the VLA models with the WidowX robot in the SimplerEnv simulation environment\n. We highlight the best results in\nbold\nand the second-best results with\nunderline\n.\nMethod\nPut Spoon\non Towel\nPut Carrot\non Plate\nStack Green Block\non Yellow Block\nPut Eggplant\nin Yellow Basket\nAverage\nRT-1-X\n(Oâ€™Neill\net al.\n,\n2024\n)\n0.0\n4.2\n0.0\n0.0\n1.1\nOcto-Small\n(Team\net al.\n,\n2024\n)\n41.7\n8.2\n0.0\n56.7\n26.7\nOpenVLA\n(Kim\net al.\n,\n2024\n)\n4.2\n0.0\n0.0\n12.5\n4.2\nOpenVLA-OFT\n(Kim\net al.\n,\n2025\n)\n12.5\n4.2\n4.2\n72.5\n23.4\nRoboVLM\n(Li\net al.\n,\n2024b\n)\n50.0\n37.5\n0.0\n83.3\n42.7\nCogACT\n(Li\net al.\n,\n2024a\n)\n71.7\n50.8\n15.0\n67.5\n51.3\nSpatialVLA\n(Qu\net al.\n,\n2025\n)\n20.8\n20.8\n25.0\n70.8\n34.4\nTraceVLA\n(Zheng\net al.\n,\n2025b\n)\n12.5\n16.6\n16.6\n65.0\n27.7\nVideoVLA\n(Shen\net al.\n,\n2025\n)\n75.0\n20.8\n45.8\n70.8\n53.1\nÏ€\n0\n\\pi_{0}\n(Black\net al.\n,\n2024\n)\n29.2\n62.5\n29.2\n91.6\n53.1\nÏ€\n0.5\n\\pi_{0.5}\n(Black\net al.\n,\n2025\n)\n49.3\n64.7\n44.7\n69.7\n57.1\nIsaac-GR00T-N1.6-Bridge\n(GEAR-Team\net al.\n,\n2025\n)\n64.5\n65.5\n5.5\n93.0\n57.1\nQwenGR00T (Baseline) + Qwen3-VL-4B\n(starVLA,\n2025\n)\n87.5\n50.0\n29.2\n64.2\n55.2\nBayesianVLA\n+ Qwen3-VL-4B\n89.6\n63.8\n33.3\n79.2\n66.5\n4.1\nExperiments on SimplerEnv\nWe utilize two large-scale subsets from the Open X-Embodiment (OXE) dataset: BridgeDataV2\n(Walke\net al.\n,\n2023\n)\nand Fractal\n(Brohan\net al.\n,\n2022\n)\n.\nThe model is fine-tuned for 40k steps on a cluster of 16 GPUs (batch size 16 per device).\nThis benchmark includes four manipulation tasks: â€œPut spoon on towelâ€, â€œPut carrot on plateâ€, â€œStack green cube on yellow cubeâ€, and â€œPut eggplant in yellow basketâ€.\nFor each task, we evaluate the VLA policies using the official evaluation scripts provided by the SimplerEnv repository\n(Li\net al.\n,\n2024c\n)\n.\nTo mitigate the effects of randomness, we run 480 independent trials and report the average performance (Avg@480).\nTable 2:\nResults of evaluating the VLA models with the GR1 robot in the RoboCasa Tabletop simulation environment\n. The results for the first five baseline methods are sourced from the official starVLA experiments\n(starVLA,\n2025\n)\n. We highlight the best results in\nbold\nand the second-best results with\nunderline\n.\nTask\nIsaac-GR00T\nN1.6\nQwenGR00T\n+Qwen3VL\nQwenPI\n+Qwen3VL\nQwenOFT\n+Qwen3VL\nQwenFAST\n+Qwen3VL\nVisionOnly\nQwenGR00T\nBayesianVLA\n+Qwen3VL\nPnP Bottle To Cabinet Close\n51.5\n46.0\n26.0\n30.0\n38.0\n70.0\n60.0\nPnP Can To Drawer Close\n13.0\n80.0\n62.0\n76.0\n44.0\n78.0\n72.0\nPnP Cup To Drawer Close\n8.5\n54.0\n42.0\n44.0\n56.0\n42.0\n46.0\nPnP Milk To Microwave Close\n14.0\n48.0\n50.0\n44.0\n44.0\n50.0\n54.0\nPnP Potato To Microwave Close\n41.5\n28.0\n42.0\n32.0\n14.0\n44.0\n32.0\nPnP Wine To Cabinet Close\n16.5\n46.0\n32.0\n36.0\n14.0\n40.0\n56.0\nPnP Novel From Cuttingboard To Basket\n58.0\n48.0\n40.0\n50.0\n54.0\n58.0\n62.0\nPnP Novel From Cuttingboard To Cardboardbox\n46.5\n40.0\n46.0\n40.0\n42.0\n26.0\n44.0\nPnP Novel From Cuttingboard To Pan\n68.5\n68.0\n60.0\n70.0\n58.0\n72.0\n68.0\nPnP Novel From Cuttingboard To Pot\n65.0\n52.0\n40.0\n54.0\n58.0\n50.0\n40.0\nPnP Novel From Cuttingboard To Tieredbasket\n46.5\n56.0\n44.0\n38.0\n40.0\n20.0\n36.0\nPnP Novel From Placemat To Basket\n58.5\n42.0\n44.0\n32.0\n36.0\n48.0\n46.0\nPnP Novel From Placemat To Bowl\n57.5\n44.0\n52.0\n58.0\n38.0\n32.0\n42.0\nPnP Novel From Placemat To Plate\n63.0\n48.0\n50.0\n52.0\n42.0\n34.0\n70.0\nPnP Novel From Placemat To Tieredshelf\n28.5\n18.0\n28.0\n24.0\n18.0\n16.0\n24.0\nPnP Novel From Plate To Bowl\n57.0\n60.0\n52.0\n60.0\n52.0\n26.0\n34.0\nPnP Novel From Plate To Cardboardbox\n43.5\n50.0\n40.0\n50.0\n30.0\n38.0\n48.0\nPnP Novel From Plate To Pan\n51.0\n54.0\n36.0\n66.0\n48.0\n44.0\n52.0\nPnP Novel From Plate To Plate\n78.7\n70.0\n48.0\n68.0\n50.0\n60.0\n70.0\nPnP Novel From Tray To Cardboardbox\n51.5\n38.0\n34.0\n44.0\n28.0\n50.0\n60.0\nPnP Novel From Tray To Plate\n71.0\n56.0\n64.0\n56.0\n34.0\n64.0\n66.0\nPnP Novel From Tray To Pot\n64.5\n50.0\n44.0\n62.0\n46.0\n52.0\n62.0\nPnP Novel From Tray To Tieredbasket\n57.0\n36.0\n50.0\n54.0\n36.0\n42.0\n44.0\nPnP Novel From Tray To Tieredshelf\n31.5\n16.0\n28.0\n30.0\n16.0\n16.0\n22.0\nAverage\n47.6\n47.8\n43.9\n48.8\n39.0\n44.7\n50.4\nThe results are summarized in Table\n1\n.\nBayesianVLA\nconsistently outperforms comparison baselines, achieving a state-of-the-art average success rate of 66.5%.\nNotably, compared to the direct baseline QwenGR00T (55.2%) built on the same StarVLA framework, our method delivers an absolute improvement of 11.3%, validating that the performance gain stems from our proposed Bayesian decomposition rather than the base architecture.\nSignificant improvements are observed in tasks requiring precise object identification and manipulation, such as\nâ€œPut Carrot on Plateâ€\n(+13.6%) and\nâ€œPut Eggplant in Yellow Basketâ€\n(+15.0%).\nFurthermore,\nBayesianVLA\nsurpasses other recent strong competitors, including the flow-matching-based\nÏ€\n0.5\n\\pi_{0.5}\n(57.1%) and the dual-system Isaac-GR00T-N1.6 (57.1%).\nThese results confirm that by explicitly optimizing the mutual information between language and action,\nBayesianVLA\neffectively mitigates the vision shortcut.\nFundamentally, this validates that our approach prevents the policy from collapsing into a spurious vision-only prior\np\nâ€‹\n(\na\n|\nv\n)\np(a|v)\ncaused by dataset determinism, and instead compels the model to learn the true causal dependency of actions on language instructions.\n4.2\nExperiments on RoboCasa\nWe evaluate our method on the RoboCasa GR1 Tabletop Manipulation Benchmark\n(Nasiriany\net al.\n,\n2024\n)\n, which consists of 24 diverse manipulation tasks.\nThese tasks feature complex interactions with articulated objects and varied geometries, exemplified by specific tasks like â€œPnPBottleToCabinetCloseâ€ and â€œPnPCanToDrawerCloseâ€, as well as scenarios involving appliances like microwaves and toasters.\nFor training, we utilize the Humanoid Robot Tabletop Manipulation subset from the PhysicalAI-Robotics-GR00T-X-Embodiment-Sim\n(Bjorck\net al.\n,\n2025b\n)\ndataset.\nApart from the dataset, all experimental configurations are identical to those detailed in Section\n4.1\n.\nTo guarantee statistical significance, we evaluate each task using 50 independent trials and report the average success rate (Avg@50).\nThe quantitative results on RoboCasa are presented in Table\n2\n.\nConsistent with the empirical evidence presented in our motivation (Section\n2\n), the VisionOnly baseline achieves a surprisingly high success rate of 44.7%, lagging only slightly behind the standard QwenGR00T baseline (47.8%).\nThis observation reconfirms the prevalence of the vision shortcut in this benchmark, suggesting that a significant portion of tasks can be solved by relying solely on visual cues.\nHowever,\nBayesianVLA\nbreaks this performance ceiling, achieving a state-of-the-art average success rate of 50.4% and surpassing all competing baselines, including QwenOFT (48.8%) and Isaac-GR00T (47.6%).\nCrucially, our method demonstrates substantial gains in tasks where the vision-only policy falters.\nFor instance, in the task â€œPnP Novel From Placemat To Plateâ€,\nBayesianVLA\nachieves 70.0% success, significantly outperforming both the VisionOnly baseline (34.0%) and the standard QwenGR00T (48.0%).\nThese results indicate that maximizing the LLR objective successfully forces the policy to extract and utilize task-specifying information from language, rather than settling for local optima based on visual shortcuts.\n4.3\nPreservation of General Capabilities\nFigure 4:\nQualitative comparison of general multimodal reasoning.\nWe present a case where the model is asked to solve a mathematical problem. The standard VLA baseline (QwenGR00T) suffers from catastrophic forgetting; while the text before the comma implies â€œdifferentiating all terms togetherâ€, the subsequent output degenerates into repetitive and meaningless gibberish (bottom right). In contrast,\nBayesianVLA\n(top right) retains the VLMâ€™s original reasoning and language generation capabilities (left), successfully solving the problem.\nA prevalent view is that fine-tuning VLMs for robotic action generation (VLA training) diminishes the modelâ€™s foundational reasoning and multimodal understanding, potentially leading to a loss of general conversational abilities\n(Zhou\net al.\n,\n2025\n; Xu\net al.\n,\n2025\n; Hancock\net al.\n,\n2025\n; Yu\net al.\n,\n2026\n)\n.\nChatVLA\n(Zhou\net al.\n,\n2025\n)\nattributes this to two factors:\nspurious forgetting\n, where robot training overwrites crucial visual-text alignments, and\ntask interference\n, where competing control and understanding tasks degrade performance when trained jointly.\nWe observe similar degradation in our baseline: as shown in Figure\n4\n, the standard QwenGR00T model loses its ability to converse coherently even when prompted with pure text inputs.\nIn contrast,\nBayesianVLA\nremarkably preserves these capabilities when queried with language instructions.\nFigure 5:\nAdditional qualitative comparison.\nDemonstrating the preservation of general VLM capabilities on another example.\nIt is crucial to note a nuanced distinction: while\nBayesianVLA\npreserves normal text-only conversational ability, its general vision-language conversation (image + text inputs) can still degrade after VLA training.\nWe hypothesize this is mainly because, in our training setup, the vision tower (and the multimodal fusion/projection layers) must be adapted for control, which can shift visual representations away from the pre-trained visionâ€“language alignment manifold.\nSuch specialization is expected for an embodied agent optimized for action.\nImportantly,\nBayesianVLA\nhelps prevent the\ntext-only language behavior\nof the backbone from collapsing.\nIn the baseline, the vision shortcut can render instruction tokens effectively redundant for control, weakening the training signal that encourages meaningful language processing and increasing drift in shared parameters, which manifests as failures even on pure text queries (Fig.\n4\nand Fig.\n5\n).\nConversely, our methodâ€™s LLR objective enforces a strong dependency on language.\nThis acts as a regularizer that maintains the functional utility of instruction tokens, thereby preserving the backboneâ€™s text-only conversational ability even as the visual modality is specialized for control.\nThis preservation is of significant practical value: it ensures that the VLM backbone does not degenerate into a shallow feature mapper.\nBy retaining its linguistic core, the agent preserves the potential for higher-level reasoning and generalization to novel instructions, which are the primary motivations for employing Foundation Models in robotics.\n4.4\nAblation Studies\nWe conduct ablation studies on SimplerEnv to validate the contributions of individual components in\nBayesianVLA\n. All experiments utilize the Qwen3-VL-4B backbone, and results are presented in Table\n3\n.\nEffectiveness of Bayesian Decomposition.\nComparing the full\nBayesianVLA\n(63.5%) with the â€œ+ Action Queryâ€ ablation (57.5%), we observe a significant performance boost (+6.0%).\nThis indicates that while the architectural changes provide some benefit, the core improvement stems from our dual-branch Bayesian learning objective.\nBy explicitly modeling and maximizing the pointwise mutual information (PMI) between instructions and actions, the model effectively overcomes the vision shortcut, validating the central hypothesis of this work.\nPotential of Latent Action Queries.\nEven without the dual-branch definition, introducing Latent Action Queries (â€œQwenGR00T + Action Queryâ€) improves upon the QwenGR00T baseline (55.2%\nâ†’\n\\to\n57.5%).\nThis suggests that Latent Action Queries function as a promising architectural inductive bias.\nUnlike standard approaches that feed full sequences of vision and language token embeddings into the action decoder, our query-based mechanism forces the VLM to compress and summarize task-relevant information into a compact set of latent tokens.\nFrom a computational perspective, this design is highly efficient.\nIt decouples the complexity of the Diffusion Transformer (DiT) from the length of the VLM input context.\nSpecifically, the complexity of condition processing in the DiT is reduced from\nO\nâ€‹\n(\nN\n2\n)\nO(N^{2})\n(scaling with the massive number of vision-language tokens\nN\nN\n) to\nO\nâ€‹\n(\nK\n2\n)\nO(K^{2})\n(scaling with the small, constant number of query tokens\nK\nK\n), thereby streamlining the action generation process.\nTable 3:\nAblation study on SimplerEnv\n. All experiments are based on the Qwen3-VL-4B backbone. We compare the baseline QwenGR00T, the addition of Latent Action Queries, and the full\nBayesianVLA\nframework to validate the contributions of each component.\nMethod\nPut Spoon\non Towel\nPut Carrot\non Plate\nStack Green Block\non Yellow Block\nPut Eggplant\nin Yellow Basket\nAverage\nQwenGR00T\n(starVLA,\n2025\n)\n87.5\n50.0\n29.2\n54.2\n55.2\nQwenGR00T + Action Query\n74.6\n58.3\n29.2\n67.9\n57.5\nBayesianVLA\n89.6\n63.8\n33.3\n79.2\n66.5\n5\nDiscussion\nBased on our analysis of the vision shortcut and the Bayesian decomposition framework, we discuss several potential insights that may guide future research and community practices.\nRethinking Data Collection Strategies.\nOur experiments suggest that the deterministic mapping from visual scenes to language instructions (\nH\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\nâ‰ˆ\n0\nH(\\ell\\mid v)\\approx 0\n) in goal-driven datasets is a significant factor contributing to the vision shortcut. To mitigate this, we hypothesize that a shift in data collection strategies could be beneficial. Prioritizing data collection in ambiguous scenariosâ€”where the task cannot be inferred solely from the initial observationâ€”might naturally increase the conditional entropy of language. By enriching datasets with scenes that support multiple valid tasks, models may be forced to rely more heavily on instructions for disambiguation.\nLeveraging Human Data for Robustness.\nRecently, there has been growing interest in training robot models on large-scale human video data, such as HRDT\n(Bi\net al.\n,\n2025\n)\n, In-N-On\n(Cai\net al.\n,\n2025\n)\n, METIS\n(Fu\net al.\n,\n2025\n)\n, and PhysBrain\n(Lin\net al.\n,\n2025\n)\n. Unlike curated robot datasets, human activities are inherently multimodal and context-dependent; the same environment often hosts a wide variety of behaviors, potentially leading to a less sharp\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\np(\\ell\\mid v)\n. We conjecture that injecting action knowledge from such rich human distributions might help mitigate the information collapse observed in robot-only datasets.\nWorld Models as an Alternative Bayesian Formulation.\nBeyond the VLM framework focused on in this work, recent studies have also explored adapting World Models for VLA control, as seen in F1-VLA\n(Lv\net al.\n,\n2025\n)\n, Mantis\n(Yang\net al.\n,\n2025\n)\n, and InternVLA-A1\n(contributors,\n2026\n)\n.\nFrom a theoretical perspective, these approaches can be viewed as an alternative instantiation of the Bayesian rule, specifically performing inverse dynamics on imagined futures. If we consider\nv\nv\nas a sequence of past frames\nv\nâ‰¤\nt\nv_{\\leq t}\n, and treat the future state\nv\nt\n+\n1\nv_{t+1}\nas a latent variable generated by the model (conditioned on\nâ„“\n\\ell\n), the action inference can be expressed as:\np\nâ€‹\n(\na\nâˆ£\nv\nâ‰¤\nt\n,\nv\nt\n+\n1\n,\nâ„“\n)\n=\np\nâ€‹\n(\nv\nt\n+\n1\nâˆ£\nv\nâ‰¤\nt\n,\na\n,\nâ„“\n)\nâ€‹\np\nâ€‹\n(\na\nâˆ£\nv\nâ‰¤\nt\n,\nâ„“\n)\np\nâ€‹\n(\nv\nt\n+\n1\nâˆ£\nv\nâ‰¤\nt\n,\nâ„“\n)\n.\np(a\\mid v_{\\leq t},v_{t+1},\\ell)=\\frac{p(v_{t+1}\\mid v_{\\leq t},a,\\ell)\\,p(a\\mid v_{\\leq t},\\ell)}{p(v_{t+1}\\mid v_{\\leq t},\\ell)}.\n(10)\nHere, the numerator\np\nâ€‹\n(\nv\nt\n+\n1\nâˆ£\nv\nâ‰¤\nt\n,\na\n,\nâ„“\n)\np(v_{t+1}\\mid v_{\\leq t},a,\\ell)\nrepresents a\nworld model\n(forward dynamics) predicting the future state. The term\np\nâ€‹\n(\na\nâˆ£\nv\nâ‰¤\nt\n,\nâ„“\n)\np(a\\mid v_{\\leq t},\\ell)\nserves as an action prior, and the denominator\np\nâ€‹\n(\nv\nt\n+\n1\nâˆ£\nv\nâ‰¤\nt\n,\nâ„“\n)\np(v_{t+1}\\mid v_{\\leq t},\\ell)\nrepresents the future prediction marginalized over actions.\nIn this formulation, the policy execution involves first â€œimaginingâ€ a desired future\nv\nt\n+\n1\nv_{t+1}\nconsistent with\nâ„“\n\\ell\n, and then inferring the optimal action\na\na\nvia the equation above.\nSince world models are typically trained on vast amounts of video data, the predictive distribution (the numerator) is often rich and highly sensitive to the action\na\na\n. We hypothesize that this sensitivity prevents the collapse of the numerator to the denominator. This suggests that world model-based architectures could offer another robust technical path toward solving the vision shortcut, which we plan to explore in future work.\n6\nRelated Work\nWe build our work upon the following rigorous foundations:\nVision-Language-Action Dataset and Benchmark.\nThe advancement of generalist robot policies relies heavily on large-scale datasets and rigorous benchmarks.\nLIBERO\n(Liu\net al.\n,\n2023\n)\npioneered the systematic study of knowledge transfer in lifelong robot learning.\nTo scale up real-world data, BridgeData V2\n(Walke\net al.\n,\n2023\n)\nprovided diverse interaction trajectories on low-cost hardware.\nThis effort was expanded by Open X-Embodiment (OXE)\n(Oâ€™Neill\net al.\n,\n2024\n)\n, which aggregated data across 22 robot embodiments, and Droid\n(Khazatsky\net al.\n,\n2024\n)\n, which further increased diversity with distributed data collection.\nFor scalable evaluation, RoboCasa\n(Nasiriany\net al.\n,\n2024\n)\nintroduced a large-scale simulation framework with realistic kitchen environments, while SimplerEnv\n(Li\net al.\n,\n2024c\n)\nprovided a simulated evaluation proxy to correlate with real-world performance, addressing the reproducibility crisis in physical evaluation.\nMore recently, RoboTwin 2.0\n(Chen\net al.\n,\n2025\n)\noffered a unified benchmark for bimanual manipulation with automated data generation, and AgiBot-World\n(Bu\net al.\n,\n2025\n)\nscaled training data to over 1 million trajectories with human-in-the-loop verification.\nVision-Language-Action Models.\nTo bridge the gap between semantic understanding and physical control, Vision-Language-Action (VLA) models have emerged as a dominant paradigm.\nEarly works like Octo\n(Team\net al.\n,\n2024\n)\nestablished the transformer-based policy as a versatile initialization, utilizing diffusion heads to handle multimodal action distributions.\nBuilding on pre-trained VLMs, OpenVLA\n(Kim\net al.\n,\n2024\n)\nand its optimized variant OpenVLA-OFT\n(Kim\net al.\n,\n2025\n)\nfine-tune large language models for robotic control, demonstrating strong generalization.\nTo further enhance action modeling, specialized architectures have been proposed. CogACT\n(Li\net al.\n,\n2024a\n)\nand RDT-1B\n(Liu\net al.\n,\n2025\n)\nemploy dedicated diffusion transformers to capture complex action dynamics.\nRecent works explore advanced architectures and generation algorithms to better balance reasoning and control. The GR00T series (N1, N1.5, N1.6)\n(Bjorck\net al.\n,\n2025b\n,\na\n; GEAR-Team\net al.\n,\n2025\n)\nutilizes a dual-system design, architecturally coupling a VLM for perception with a diffusion head for action generation.\nIn parallel, the\nÏ€\n0\n\\pi_{0}\nseries (\nÏ€\n0\n\\pi_{0}\n,\nÏ€\n0.5\n\\pi_{0.5}\n, FAST)\n(Black\net al.\n,\n2024\n,\n2025\n; Pertsch\net al.\n,\n2025\n)\nleverages Flow Matching as a superior generation objective, adopting a two-stage training paradigm that pre-trains the VLM backbone before fine-tuning.\nSpecifically, FAST introduces discrete motion tokens to efficiently encode actions for the VLM, while\nÏ€\n0.5\n\\pi_{0.5}\nincorporates subtask decomposition to enhance long-horizon planning.\nOther approaches like X-VLA\n(Zheng\net al.\n,\n2025a\n)\nintroduce embodiment-specific soft prompts to facilitate cross-embodiment generalization. By learning separate sets of embeddings for each data source, X-VLA effectively leverages heterogeneous robot data with minimal additional parameters.\nSpatialVLA\n(Qu\net al.\n,\n2025\n)\nargues that spatial understanding is central to manipulation, introducing Ego3D Position Encoding and Adaptive Action Grids to inject 3D information and learn transferable spatial action knowledge.\nThen, VideoVLA\n(Shen\net al.\n,\n2025\n)\nexplores transforming video generation models into robot manipulators.\nBy jointly predicting action sequences and future visual outcomes, it leverages the â€œvisual imaginationâ€ of generative models to enhance generalization across novel tasks and objects.\nBayesVLA\n(Xu\net al.\n,\n2025\n)\nalso employs Bayesian decomposition to address language diversity limitations in VLA training. Using a CLIP-based encoder and diffusion transformer, it adopts a two-stage framework: the first stage trains a vision-conditioned prior on large-scale vision-action pairs to capture multimodal action distributions, while the second stage freezes the prior and injects language-aware tokens to refine policy conditioning on instructions.\nHowever, BayesVLA does not scale its training to large-scale VLMs, while other VLA approaches lack systematic solutions to the vision shortcut problem, where models ignore language instructions in goal-driven datasets.\n7\nConclusion\nIn this work, we identified a critical pathology in standard VLA training: the\nvision shortcut\n, where models learn to ignore language instructions in goal-driven datasets, leading to a collapse of the mutual information between instructions and actions.\nThis phenomenon results in catastrophic failure when the model faces ambiguous scenarios or out-of-distribution environments where visual shortcuts no longer hold.\nTo address this, we introduced\nBayesianVLA\n, a novel framework that formalizes VLA learning through a Bayesian lens.\nBy decomposing the policy into a vision-only prior and a language-conditioned posterior, we derived a training objective that explicitly maximizes the Log-Likelihood Ratio (LLR), effectively optimizing the Pointwise Mutual Information (PMI) between language and action.\nWe instantiated this decomposition using a unified architecture with Latent Action Queries and a dual-branch training strategy, which incurs no additional computational overhead during inference.\nOur extensive experiments on SimplerEnv and RoboCasa demonstrate that\nBayesianVLA\neffectively mitigates the vision shortcut problem.\nIt significantly outperforms baselines under various conditions; in out-of-distribution simulation environments, where standard methods fail completely,\nBayesianVLA\nachieves robust generalization.\nThese results highlight the importance of introducing structural inductive biases in learning robust, generalizable robot policies.\nWe hope our work inspires further research into the information-theoretic foundations of embodied AI, pushing towards robots that truly understand and reliably execute human intent.\nLimitation and Future Work\nWhile\nBayesianVLA\noffers significant improvements in robustness, the dual-branch architecture introduces a limitation regarding computational overhead during training.\nSince the model must compute both the Priori and Posteriori branches, the computational cost per iteration theoretically increases.\nHowever, we note that the visual input prefix is identical for both branches, and the number of visual tokens vastly outnumbers that of the language and latent action query tokens.\nBy employing a prefix prefill strategy to compute and reuse the visual representations (e.g., vision encoder outputs) for both branches, the actual increase in training time is marginal.\nThus, the additional computational overhead remains within a completely acceptable range.\nFor future work, we intend to provide a more comprehensive empirical evaluation in subsequent versions of this manuscript.\nWe plan to extend our experimental validation to include the RoboTwin and LIBERO benchmarks, alongside real-world robot experiments.\nWe also aim to scale our framework to larger foundation models, such as conducting experiments with\nBayesianVLA\non Qwen3VL-8B.\nAdditionally, we plan to include more extensive ablation studies to further dissect the contribution of each component and hyperparameter within our Bayesian decomposition framework, thereby rendering the analysis more complete.\nReferences\nS. Bai, Y. Cai, R. Chen, K. Chen, X. Chen, Z. Cheng, L. Deng, W. Ding, C. Gao, C. Ge, W. Ge, Z. Guo, Q. Huang, J. Huang, F. Huang, B. Hui, S. Jiang, Z. Li, M. Li, M. Li, K. Li, Z. Lin, J. Lin, X. Liu, J. Liu, C. Liu, Y. Liu, D. Liu, S. Liu, D. Lu, R. Luo, C. Lv, R. Men, L. Meng, X. Ren, X. Ren, S. Song, Y. Sun, J. Tang, J. Tu, J. Wan, P. Wang, P. Wang, Q. Wang, Y. Wang, T. Xie, Y. Xu, H. Xu, J. Xu, Z. Yang, M. Yang, J. Yang, A. Yang, B. Yu, F. Zhang, H. Zhang, X. Zhang, B. Zheng, H. Zhong, J. Zhou, F. Zhou, J. Zhou, Y. Zhu, and K. Zhu (2025)\nQwen3-VL technical report\n.\narXiv preprint arXiv:2511.21631\n.\nCited by:\nÂ§3.2\n.\nH. Bi, L. Wu, T. Lin, H. Tan, Z. Su, H. Su, and J. Zhu (2025)\nH-RDT: human manipulation enhanced bimanual robotic manipulation\n.\narXiv preprint arXiv:2507.23523\n.\nCited by:\nÂ§5\n.\nJ. Bjorck, V. Blukis, F. CastaÃ±eda, N. Cherniadev, X. Da, R. Ding, L. J. Fan, Y. Fang, D. Fox, F. Hu, S. Huang, J. Jang, X. Jiang, K. Kundalia, J. Kautz, Z. Li, K. Lin, Z. Lin, L. Magne, Y. Man, A. Mandlekar, A. Narayan, S. Nasiriany, S. Reed, Y. L. Tan, G. Wang, J. Wang, Q. Wang, S. Wang, J. Xiang, Y. Xie, Y. Xu, S. Ye, Z. Yu, Y. Zhao, Z. Zhang, R. Zheng, and Y. Zhu (2025a)\nGR00T N1.5: an improved open foundation model for generalist humanoid robots\n.\nNote:\nhttps://research.nvidia.com/labs/gear/gr00t-n1_5/\nAccessed: 2026-01-19\nCited by:\nÂ§6\n.\nJ. Bjorck, F. CastaÃ±eda, N. Cherniadev, X. Da, R. Ding, L. Fan, Y. Fang, D. Fox, F. Hu, S. Huang,\net al.\n(2025b)\nGR00T N1: an open foundation model for generalist humanoid robots\n.\narXiv preprint arXiv:2503.14734\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nÂ§3.2\n,\nÂ§3.4\n,\nÂ§4.2\n,\nÂ§6\n.\nK. Black, N. Brown, J. Darpinian, K. Dhabalia, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, M. Y. Galliker, D. Ghosh, L. Groom, K. Hausman, B. Ichter, S. Jakubczak, T. Jones, L. Ke, D. LeBlanc, S. Levine, A. Li-Bell, M. Mothukuri, S. Nair, K. Pertsch, A. Z. Ren, L. X. Shi, L. Smith, J. T. Springenberg, K. Stachowicz, J. Tanner, Q. Vuong, H. Walke, A. Walling, H. Wang, L. Yu, and U. Zhilinsky (2025)\nÏ€\n0.5\n\\pi_{0.5}\n: a vision-language-action model with open-world generalization\n.\narXiv preprint arXiv:2504.16054\n.\nCited by:\nÂ§1\n,\nTable 1\n,\nÂ§6\n.\nK. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter,\net al.\n(2024)\nÏ€\n0\n\\pi_{0}\n: a vision-language-action flow model for general robot control\n.\narXiv preprint arXiv:2410.24164\n.\nCited by:\nÂ§3.2\n,\nTable 1\n,\nÂ§6\n.\nA. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich (2022)\nRT-1: robotics transformer for real-world control at scale\n.\nIn\narXiv preprint arXiv:2212.06817\n,\nCited by:\nÂ§4.1\n.\nQ. Bu, J. Cai, L. Chen, X. Cui, Y. Ding, S. Feng, S. Gao, X. He, X. Huang, S. Jiang,\net al.\n(2025)\nAgibot world colosseo: a large-scale manipulation platform for scalable and intelligent embodied systems\n.\narXiv preprint arXiv:2503.06669\n.\nCited by:\nÂ§6\n.\nX. Cai, R. Qiu, G. Chen, L. Wei, I. Liu, T. Huang, X. Cheng, and X. Wang (2025)\nIn-N-On: scaling egocentric manipulation with in-the-wild and on-task data\n.\narXiv preprint arXiv:2511.15704\n.\nCited by:\nÂ§5\n.\nT. Chen, Z. Chen, B. Chen, Z. Cai, Y. Liu, Z. Li, Q. Liang, X. Lin, Y. Ge, Z. Gu,\net al.\n(2025)\nRobotwin 2.0: a scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation\n.\narXiv preprint arXiv:2506.18088\n.\nCited by:\nÂ§6\n.\nI. contributors (2026)\nInternVLA-A1: unifying understanding, generation and action for robotic manipulation\n.\narXiv preprint arXiv:2601.02456\n.\nCited by:\nÂ§5\n.\nY. Fu, N. Chen, J. Zhao, S. Shan, G. Yao, P. Wang, Z. Wang, and S. Zhang (2025)\nMETIS: multi-source egocentric training for integrated dexterous vision-language-action model\n.\narXiv preprint arXiv:2511.17366\n.\nCited by:\nÂ§5\n.\nGEAR-Team, A. Azzolini, J. Bjorck, V. Blukis, F. CastaÃ±eda, R. Chand,\net al.\n(2025)\nGR00T n1.6: an improved open foundation model for generalist humanoid robots\n.\nNote:\nhttps://research.nvidia.com/labs/gear/gr00t-n1_6/\nCited by:\nÂ§3.2\n,\nTable 1\n,\nÂ§6\n.\nA. J. Hancock, X. Wu, L. Zha, O. Russakovsky, and A. Majumdar (2025)\nActions as language: fine-tuning vlms into vlas without catastrophic forgetting\n.\narXiv preprint arXiv:2509.22195\n.\nCited by:\nÂ§4.3\n.\nA. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis,\net al.\n(2024)\nDroid: a large-scale in-the-wild robot manipulation dataset\n.\narXiv preprint arXiv:2403.12945\n.\nCited by:\nÂ§6\n.\nM. J. Kim, C. Finn, and P. Liang (2025)\nFine-tuning vision-language-action models: optimizing speed and success\n.\narXiv preprint arXiv:2502.19645\n.\nCited by:\nTable 1\n,\nÂ§6\n.\nM. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. P. Foster, P. R. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang, and C. Finn (2024)\nOpenVLA: an open-source vision-language-action model\n.\nIn\nAnnual Conference on Robot Learning (CoRL)\n,\nCited by:\nÂ§1\n,\nTable 1\n,\nÂ§6\n.\nQ. Li, Y. Liang, Z. Wang, L. Luo, X. Chen, M. Liao, F. Wei, Y. Deng, S. Xu, Y. Zhang,\net al.\n(2024a)\nCogACT: a foundational vision-language-action model for synergizing cognition and action in robotic manipulation\n.\narXiv preprint arXiv:2411.19650\n.\nCited by:\nTable 1\n,\nÂ§6\n.\nX. Li, P. Li, M. Liu, D. Wang, J. Liu, B. Kang, X. Ma, T. Kong, H. Zhang, and H. Liu (2024b)\nTowards generalist robot policies: what matters in building vision-language-action models\n.\narXiv preprint arXiv:2412.14058\n.\nCited by:\nTable 1\n.\nX. Li, K. Hsu, J. Gu, O. Mees, K. Pertsch, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani, S. Levine, J. Wu, C. Finn, H. Su, Q. Vuong, and T. Xiao (2024c)\nSimplerEnv: evaluating real-world robot manipulation policies in simulation\n.\nIn\nAnnual Conference on Robot Learning (CoRL)\n,\nCited by:\nÂ§2.3\n,\nÂ§4.1\n,\nÂ§6\n.\nX. Lin, S. Lian, B. Yu, R. Yang, C. Wu, Y. Miao, Y. Jin, Y. Shi, C. Huang, B. Cheng,\net al.\n(2025)\nPhysBrain: human egocentric data as a bridge from vision language models to physical intelligence\n.\narXiv preprint arXiv:2512.16793\n.\nCited by:\nÂ§5\n.\nB. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone (2023)\nLIBERO: benchmarking knowledge transfer for lifelong robot learning\n.\nAdvances in neural information processing systems (NeurIPS)\n36\n,\npp.Â 44776â€“44791\n.\nCited by:\nFigure 2\n,\nÂ§2.2\n,\nÂ§6\n.\nS. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su, and J. Zhu (2025)\nRDT-1B: a diffusion foundation model for bimanual manipulation\n.\nIn\nInternational Conference on Learning Representations (ICLR)\n,\nCited by:\nÂ§1\n,\nÂ§6\n.\nX. Liu, C. Gong,\net al.\n(2022)\nFlow straight and fast: learning to generate and transfer data with rectified flow\n.\nIn\nInternational Conference on Learning Representations (ICLR)\n,\nCited by:\nÂ§3.4\n.\nI. Loshchilov and F. Hutter (2017)\nDecoupled weight decay regularization\n.\nIn\nInternational Conference on Learning Representations (ICLR)\n,\nCited by:\nÂ§4\n.\nQ. Lv, W. Kong, H. Li, J. Zeng, Z. Qiu, D. Qu, H. Song, Q. Chen, X. Deng, and J. Pang (2025)\nF1: a vision-language-action model bridging understanding and generation to actions\n.\narXiv preprint arXiv:2509.06951\n.\nCited by:\nÂ§5\n.\nS. Nasiriany, A. Maddukuri, L. Zhang, A. Parikh, A. Lo, A. Joshi, A. Mandlekar, and Y. Zhu (2024)\nRoboCasa: large-scale simulation of everyday tasks for generalist robots\n.\nIn\nRobotics: Science and Systems\n,\nCited by:\nFigure 1\n,\nFigure 1\n,\nÂ§2.1\n,\nÂ§4.2\n,\nÂ§6\n.\nA. Oâ€™Neill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain,\net al.\n(2024)\nOpen x-embodiment: robotic learning datasets and rt-x models: open x-embodiment collaboration\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 6892â€“6903\n.\nCited by:\nTable 1\n,\nÂ§6\n.\nW. Peebles and S. Xie (2023)\nScalable diffusion models with transformers\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)\n,\npp.Â 4195â€“4205\n.\nCited by:\nÂ§3.2\n.\nK. Pertsch, K. Stachowicz, B. Ichter, D. Driess, S. Nair, Q. Vuong, O. Mees, C. Finn, and S. Levine (2025)\nFAST: efficient action tokenization for vision-language-action models\n.\narXiv preprint arXiv:2501.09747\n.\nCited by:\nÂ§6\n.\nD. Qu, H. Song, Q. Chen, Y. Yao, X. Ye, Y. Ding, Z. Wang, J. Gu, B. Zhao, D. Wang,\net al.\n(2025)\nSpatialvla: exploring spatial representations for visual-language-action model\n.\narXiv preprint arXiv:2501.15830\n.\nCited by:\nTable 1\n,\nÂ§6\n.\nJ. Rasley, S. Rajbhandari, O. Ruwase, and Y. He (2020)\nDeepspeed: system optimizations enable training deep learning models with over 100 billion parameters\n.\nIn\nProceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining\n,\npp.Â 3505â€“3506\n.\nCited by:\nÂ§4\n.\nY. Shen, F. Wei, Z. Du, Y. Liang, Y. Lu, J. Yang, N. Zheng, and B. Guo (2025)\nVideoVLA: video generators can be generalizable robot manipulators\n.\nIn\nAdvances in neural information processing systems (NeurIPS)\n,\nExternal Links:\nLink\nCited by:\nTable 1\n,\nÂ§6\n.\nstarVLA (2025)\nStarVLA: a lego-like codebase for vision-language-action model developing\n.\nGitHub\n.\nNote:\nGitHub repository\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§2\n,\nTable 1\n,\nTable 2\n,\nTable 3\n,\nÂ§4\n.\nO. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu,\net al.\n(2024)\nOcto: an open-source generalist robot policy\n.\narXiv preprint arXiv:2405.12213\n.\nCited by:\nTable 1\n,\nÂ§6\n.\nH. R. Walke, K. Black, T. Z. Zhao, Q. Vuong, C. Zheng, P. Hansen-Estruch, A. W. He, V. Myers, M. J. Kim, M. Du,\net al.\n(2023)\nBridgedata v2: a dataset for robot learning at scale\n.\nIn\nAnnual Conference on Robot Learning (CoRL)\n,\npp.Â 1723â€“1736\n.\nCited by:\nFigure 2\n,\nÂ§2.3\n,\nÂ§4.1\n,\nÂ§6\n.\nK. Xu, Z. Zhu, A. Chen, S. Zhao, Q. Huang, Y. Yang, H. Lu, R. Xiong, M. Tomizuka, and Y. Wang (2025)\nSeeing to act, prompting to specify: a bayesian factorization of vision language action policy\n.\narXiv preprint arXiv:2512.11218\n.\nCited by:\nÂ§1\n,\nÂ§2.4\n,\nÂ§4.3\n,\nÂ§6\n.\nY. Yang, X. Li, Y. Chen, J. Song, Y. Wang, Z. Xiao, J. Su, Y. Qiaoben, P. Liu, and Z. Deng (2025)\nMantis: a versatile vision-language-action model with disentangled visual foresight\n.\narXiv preprint arXiv:2511.16175\n.\nCited by:\nÂ§5\n.\nB. Yu, S. Lian, X. Lin, Y. Wei, Z. Shen, C. Wu, Y. Miao, X. Wang, B. Wang, C. Huang, and K. Chen (2026)\nTwinBrainVLA: unleashing the potential of generalist vlms for embodied tasks via asymmetric mixture-of-transformers\n.\narXiv preprint arXiv:2601.14133\n.\nCited by:\nÂ§4.3\n.\nJ. Zheng, J. Li, Z. Wang, D. Liu, X. Kang, Y. Feng, Y. Zheng, J. Zou, Y. Chen, J. Zeng,\net al.\n(2025a)\nX-VLA: soft-prompted transformer as scalable cross-embodiment vision-language-action model\n.\narXiv preprint arXiv:2510.10274\n.\nCited by:\nÂ§6\n.\nR. Zheng, Y. Liang, S. Huang, J. Gao, H. D. III, A. Kolobov, F. Huang, and J. Yang (2025b)\nTraceVLA: visual trace prompting enhances spatial-temporal awareness for generalist robotic policies\n.\narXiv preprint arXiv:2412.10345\n.\nCited by:\nTable 1\n.\nZ. Zhou, Y. Zhu, J. Wen, C. Shen, and Y. Xu (2025)\nVision-language-action model with open-world embodied reasoning from pretrained knowledge\n.\narXiv preprint arXiv:2505.21906\n.\nCited by:\nÂ§4.3\n.\nAppendix A\nDerivation of the LLR Objective\nIn this section, we provide the derivation for the Log-Likelihood Ratio (LLR) objective used in BayesVLA.\nOur goal is to maximize the conditional Pointwise Mutual Information (PMI) between the action\na\na\nand the language instruction\nâ„“\n\\ell\n, given the visual observation\nv\nv\n. The PMI is defined as:\nPMI\nâ€‹\n(\na\n,\nâ„“\nâˆ£\nv\n)\n=\nlog\nâ¡\nÏ€\nâ€‹\n(\na\n,\nâ„“\nâˆ£\nv\n)\np\nâ€‹\n(\na\nâˆ£\nv\n)\nâ€‹\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\n\\text{PMI}(a,\\ell\\mid v)=\\log\\frac{\\pi(a,\\ell\\mid v)}{p(a\\mid v)p(\\ell\\mid v)}\n(11)\nUsing the chain rule of probability\nÏ€\nâ€‹\n(\na\n,\nâ„“\nâˆ£\nv\n)\n=\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\nâ€‹\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\n\\pi(a,\\ell\\mid v)=\\pi(a\\mid v,\\ell)p(\\ell\\mid v)\n, we can rewrite the PMI as:\nPMI\nâ€‹\n(\na\n,\nâ„“\nâˆ£\nv\n)\n=\nlog\nâ¡\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\nâ€‹\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\np\nâ€‹\n(\na\nâˆ£\nv\n)\nâ€‹\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\n=\nlog\nâ¡\nÏ€\nâ€‹\n(\na\nâˆ£\nv\n,\nâ„“\n)\np\nâ€‹\n(\na\nâˆ£\nv\n)\n\\text{PMI}(a,\\ell\\mid v)=\\log\\frac{\\pi(a\\mid v,\\ell)p(\\ell\\mid v)}{p(a\\mid v)p(\\ell\\mid v)}=\\log\\frac{\\pi(a\\mid v,\\ell)}{p(a\\mid v)}\n(12)\nThis corresponds to the first form of our LLR objective: the log-ratio between the posterior policy and the vision-only prior.\nAlternatively, using the chain rule\nÏ€\nâ€‹\n(\na\n,\nâ„“\nâˆ£\nv\n)\n=\np\nâ€‹\n(\nâ„“\nâˆ£\na\n,\nv\n)\nâ€‹\np\nâ€‹\n(\na\nâˆ£\nv\n)\n\\pi(a,\\ell\\mid v)=p(\\ell\\mid a,v)p(a\\mid v)\n, we can rewrite the PMI as:\nPMI\nâ€‹\n(\na\n,\nâ„“\nâˆ£\nv\n)\n=\nlog\nâ¡\np\nâ€‹\n(\nâ„“\nâˆ£\na\n,\nv\n)\nâ€‹\np\nâ€‹\n(\na\nâˆ£\nv\n)\np\nâ€‹\n(\na\nâˆ£\nv\n)\nâ€‹\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\n=\nlog\nâ¡\np\nâ€‹\n(\nâ„“\nâˆ£\na\n,\nv\n)\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\n=\nlog\nâ¡\np\nâ€‹\n(\nâ„“\nâˆ£\na\n,\nv\n)\nâˆ’\nlog\nâ¡\np\nâ€‹\n(\nâ„“\nâˆ£\nv\n)\n\\text{PMI}(a,\\ell\\mid v)=\\log\\frac{p(\\ell\\mid a,v)p(a\\mid v)}{p(a\\mid v)p(\\ell\\mid v)}=\\log\\frac{p(\\ell\\mid a,v)}{p(\\ell\\mid v)}=\\log p(\\ell\\mid a,v)-\\log p(\\ell\\mid v)\n(13)\nThis corresponds to the second form of our LLR objective, which we optimize in practice. It represents the difference between the log-likelihood of the instruction given the action and vision, and the log-likelihood of the instruction given vision alone. Maximizing this quantity encourages the model to select actions\na\na\nthat make the instruction\nâ„“\n\\ell\nmore probable than it would be based on vision\nv\nv\nalone.",
    "preview_text": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose LangForce, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \\mid v)$ and a language-conditioned posterior $Ï€(a \\mid v, \\ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, LangForce significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.\n\nBayesianVLA\n: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries\nShijie Lian\n1,2,\nBin Yu\n2,4,\n1\n1\nfootnotemark:\n1\nXiaopeng Lin\n2,5,\n1\n1\nfootnotemark:\n1\nLaurence T. Yang\n6,1,\nZhaolong Shen\n2,7\nChangti Wu\n2,8\nYuzhuo Miao\n1,2\nCong Huang\n2,3\nKai Chen\n2,3,9,\n2\n2\nfootnotemark:\n2\n1\nHUST\n2\nZGCA\n3\nZGCI\n4\nHIT\n5\nHKUST(GZ)\n6\nZZU\n7\nBUAA\n8\nECNU\n9\nDeepCybo\nEqual contributionCorresponding author\nAbstract\nVision-Language-Action (VLA) models have sho",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "VLA",
        "Bayesian decomposition",
        "Latent Action Queries",
        "Information Collapse",
        "generalization"
    ],
    "one_line_summary": "LangForceé€šè¿‡è´å¶æ–¯åˆ†è§£å’Œæ½œåœ¨åŠ¨ä½œæŸ¥è¯¢è§£å†³VLAæ¨¡å‹ä¸­çš„ä¿¡æ¯å´©æºƒé—®é¢˜ï¼Œæå‡æŒ‡ä»¤è·Ÿéšå’Œæ³›åŒ–èƒ½åŠ›ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šBayesianVLA: é€šè¿‡æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢å¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹è¿›è¡Œè´å¶æ–¯åˆ†è§£\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨é¢å¯¹æ–°æŒ‡ä»¤æˆ–å¤æ‚å¤šä»»åŠ¡åœºæ™¯æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚æœ¬æ–‡æŒ‡å‡ºï¼Œå½“å‰åŸºäºç›®æ ‡é©±åŠ¨æ•°æ®é›†çš„è®­ç»ƒèŒƒå¼å­˜åœ¨ä¸€ä¸ªå…³é”®ç¼ºé™·ï¼š**ä¿¡æ¯åç¼©**ã€‚ç”±äºæ•°æ®é›†ä¸­è§†è§‰è§‚å¯Ÿä¸è¯­è¨€æŒ‡ä»¤é«˜åº¦å¯é¢„æµ‹ï¼Œå¯¼è‡´åŠ¨ä½œä¸æŒ‡ä»¤ä¹‹é—´çš„æ¡ä»¶äº’ä¿¡æ¯æ¶ˆå¤±ï¼Œæ¨¡å‹é€€åŒ–ä¸ºä»…ä¾èµ–è§†è§‰çš„â€œæ·å¾„ç­–ç•¥â€ï¼Œä»è€Œåœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æˆ–æ¨¡ç³Šåœºæ™¯ä¸­å¤±è´¥ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\nä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† **BayesianVLA** æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š\n- **è´å¶æ–¯åˆ†è§£ä¸ä¿¡æ¯æœ€å¤§åŒ–ç›®æ ‡**ï¼šå°†ç­–ç•¥åˆ†è§£ä¸ºä»…è§†è§‰å…ˆéªŒ \\(p(a|v)\\) å’Œè¯­è¨€æ¡ä»¶åéªŒ \\(\\pi(a|v,\\ell)\\)ï¼Œå¹¶ä¼˜åŒ–**æ¡ä»¶ç‚¹äº’ä¿¡æ¯ï¼ˆPMIï¼‰**ï¼Œå³æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶æ¯”ï¼ˆLLRï¼‰ã€‚è¯¥ç›®æ ‡æƒ©ç½šè§†è§‰æ·å¾„ï¼Œå¥–åŠ±èƒ½æ˜ç¡®è§£é‡Šè¯­è¨€æŒ‡ä»¤çš„åŠ¨ä½œã€‚\n- **æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢**ï¼šå¼•å…¥ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢ä»¤ç‰Œä½œä¸ºVLMä¸ä¸‹æ¸¸æ‰©æ•£Transformerç­–ç•¥ä¹‹é—´çš„ç“¶é¢ˆæ¥å£ã€‚\n- **åŒåˆ†æ”¯è®­ç»ƒæ¶æ„**ï¼šåˆ©ç”¨å› æœæ³¨æ„åŠ›æ©ç ï¼Œé€šè¿‡è°ƒæ•´æŸ¥è¯¢ä»¤ç‰Œåœ¨è¾“å…¥åºåˆ—ä¸­çš„ä½ç½®ï¼Œå®ç°**å…ˆéªŒåˆ†æ”¯**ï¼ˆä»…è§†è§‰ï¼‰å’Œ**åéªŒåˆ†æ”¯**ï¼ˆè§†è§‰+è¯­è¨€ï¼‰çš„å¹¶è¡Œè®­ç»ƒï¼Œä»¥åˆ†åˆ«å»ºæ¨¡å…ˆéªŒå’ŒåéªŒç­–ç•¥ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nåœ¨ SimplerEnv å’Œ RoboCasa åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼š\n- **æ˜¾è‘—æå‡OODæ³›åŒ–èƒ½åŠ›**ï¼šåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„OOD SimplerEnvåŸºå‡†ä¸Šï¼Œç›¸æ¯”åŸºçº¿QwenGR00Tå®ç°äº†**11.3%** çš„ç»å¯¹æ€§èƒ½æå‡ï¼Œå¹³å‡æˆåŠŸç‡è¾¾åˆ°äº†66.5%çš„SOTAæ°´å¹³ã€‚\n- **æœ‰æ•ˆç¼“è§£è§†è§‰æ·å¾„**ï¼šåœ¨RoboCasaåŸºå‡†ä¸Šï¼ŒBayesianVLAä»¥50.4%çš„å¹³å‡æˆåŠŸç‡è¶…è¶Šæ‰€æœ‰åŸºçº¿ï¼Œåœ¨è§†è§‰æ·å¾„å¤±æ•ˆçš„ä»»åŠ¡ä¸Šï¼ˆå¦‚â€œPnP Novel From Placemat To Plateâ€ï¼‰è¡¨ç°å°¤ä¸ºçªå‡ºã€‚\n- **ä¿ç•™åŸºç¡€æ¨¡å‹èƒ½åŠ›**ï¼šä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒBayesianVLAåœ¨VLAè®­ç»ƒåæ›´å¥½åœ°ä¿ç•™äº†éª¨å¹²VLMçš„çº¯æ–‡æœ¬å¯¹è¯å’Œæ¨ç†èƒ½åŠ›ï¼Œé¿å…äº†ç¾éš¾æ€§é—å¿˜ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\næœ¬ç ”ç©¶ä»ä¿¡æ¯è®ºè§’åº¦æ­ç¤ºäº†VLAè®­ç»ƒä¸­çš„æ ¹æœ¬æ€§ç—…ç†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„è´å¶æ–¯åˆ†è§£æ¡†æ¶æ¥å¼ºåˆ¶æ¨¡å‹éµå¾ªæŒ‡ä»¤ã€‚**BayesianVLA** åœ¨ä¸å¢åŠ æ¨ç†å¼€é”€çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†åœ¨å…·èº«AIä¸­å¼•å…¥ç»“æ„æ€§å½’çº³åç½®çš„é‡è¦æ€§ï¼Œä¸ºæ„å»ºçœŸæ­£ç†è§£å¹¶å¯é æ‰§è¡Œäººç±»æ„å›¾çš„é€šç”¨æœºå™¨äººç­–ç•¥æä¾›äº†æ–°çš„æ€è·¯å’ŒæŠ€æœ¯è·¯å¾„ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šBayesianVLA\n\n### **ä¸€ã€ æ ¸å¿ƒé—®é¢˜ï¼šè§†è§‰æ·å¾„ä¸ä¿¡æ¯åç¼©**\n\nå½“å‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­å­˜åœ¨ä¸€ä¸ªæ ¹æœ¬æ€§ç¼ºé™·ï¼š**æ¨¡å‹å€¾å‘äºå¿½ç•¥è¯­è¨€æŒ‡ä»¤ï¼Œä»…ä¾èµ–è§†è§‰è§‚å¯Ÿæ¥é¢„æµ‹åŠ¨ä½œ**ã€‚è®ºæ–‡å°†è¿™ä¸€ç°è±¡ç§°ä¸º **â€œè§†è§‰æ·å¾„â€** ï¼Œå…¶æ ¹æºåœ¨äºè®­ç»ƒæ•°æ®çš„ **â€œä¿¡æ¯åç¼©â€**ã€‚\n\n- **é—®é¢˜æ ¹æº**ï¼šä¸»æµæœºå™¨äººæ•°æ®é›†ï¼ˆå¦‚BridgeDataV2, RoboCasaï¼‰ä»¥ç›®æ ‡é©±åŠ¨æ–¹å¼æ”¶é›†ã€‚åœ¨è¿™ç§æ•°æ®é›†ä¸­ï¼Œ**è§†è§‰åœºæ™¯ `v` å’Œè¯­è¨€æŒ‡ä»¤ `â„“` ä¹‹é—´å­˜åœ¨è¿‘ä¹ç¡®å®šæ€§çš„æ˜ å°„**ï¼ˆä¾‹å¦‚ï¼Œçœ‹åˆ°æŸœå­å°±å¯¹åº”â€œæ‰“å¼€æŸœå­â€ï¼‰ã€‚è¿™å¯¼è‡´æ¡ä»¶åˆ†å¸ƒ `p(â„“|v)` éå¸¸å°–é”ï¼Œæ¡ä»¶ç†µ `H(â„“|v) â‰ˆ 0`ã€‚\n- **ä¸¥é‡åæœ**ï¼šæ ¹æ®è´å¶æ–¯å…¬å¼ï¼Œæœ€ä¼˜ç­–ç•¥ `Ï€(a|v,â„“) = [p(â„“|a,v) * p(a|v)] / p(â„“|v)`ã€‚å½“ `p(â„“|v)` å°–é”æ—¶ï¼Œä¼¼ç„¶é¡¹ `p(â„“|a,v)` ä¼šåç¼©åˆ° `p(â„“|v)`ï¼Œä½¿å¾—åéªŒç­–ç•¥é€€åŒ–ä¸ºè§†è§‰å…ˆéªŒï¼š`Ï€(a|v,â„“) â‰ˆ p(a|v)`ã€‚**æ¨¡å‹å®è´¨ä¸Šå˜æˆäº†ä¸€ä¸ªæ— è§†è¯­è¨€çš„ã€ä»…åŸºäºè§†è§‰çš„ç­–ç•¥**ã€‚\n- **å®é™…è¡¨ç°**ï¼šåœ¨åˆ†å¸ƒå†…æµ‹è¯•æ—¶æ€§èƒ½å°šå¯ï¼Œä½†åœ¨**æ¨¡ç³Šåœºæ™¯**ï¼ˆåŒä¸€è§†è§‰åœºæ™¯å¯¹åº”å¤šä¸ªä»»åŠ¡ï¼‰æˆ–**åˆ†å¸ƒå¤–**ç¯å¢ƒï¼ˆè§†è§‰çº¿ç´¢å˜åŒ–ï¼‰ä¸­ä¼šå®Œå…¨å¤±è´¥ã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ï¼šè´å¶æ–¯åˆ†è§£ä¸æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢**\n\nè®ºæ–‡æå‡ºäº† **BayesianVLA** æ¡†æ¶ï¼Œé€šè¿‡è´å¶æ–¯åˆ†è§£å¼ºåˆ¶æ¨¡å‹éµå¾ªæŒ‡ä»¤ã€‚å…¶åˆ›æ–°ç‚¹æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¶æ„ä¸è®­ç»ƒç›®æ ‡ã€‚\n\n#### **1. ç†è®ºåˆ›æ–°ï¼šæœ€å¤§åŒ–æ¡ä»¶ç‚¹äº’ä¿¡æ¯**\n- **æ ¸å¿ƒç›®æ ‡**ï¼šæœ€å¤§åŒ–åŠ¨ä½œ `a` å’ŒæŒ‡ä»¤ `â„“` ä¹‹é—´çš„**æ¡ä»¶ç‚¹äº’ä¿¡æ¯**ã€‚\n- **å®ç°æ–¹å¼**ï¼šä¼˜åŒ–**å¯¹æ•°ä¼¼ç„¶æ¯”**ï¼š\n  ```math\n  â„’_LLR = log [Ï€(a|v,â„“) / p(a|v)] = log p(â„“|a,v) - log p(â„“|v)\n  ```\n  è¯¥ç›®æ ‡æƒ©ç½šäº†â€œè§†è§‰æ·å¾„â€ï¼Œå¥–åŠ±é‚£äº›èƒ½ä¸ºæŒ‡ä»¤æä¾›**è¶…å‡ºè§†è§‰è§‚å¯Ÿä¹‹å¤–çš„é¢å¤–ä¿¡æ¯**çš„åŠ¨ä½œã€‚\n\n#### **2. æ¶æ„åˆ›æ–°ï¼šæ½œåœ¨åŠ¨ä½œæŸ¥è¯¢ä¸åŒåˆ†æ”¯è®¾è®¡**\n- **æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢**ï¼šåœ¨VLMè¯æ±‡è¡¨ä¸­å¼•å…¥ä¸€ç»„**å¯å­¦ä¹ çš„ä»¤ç‰Œ** `ğ’¬`ï¼ˆå¦‚64ä¸ªï¼‰ã€‚è¿™äº›æŸ¥è¯¢ä½œä¸ºVLMä¸ä¸‹æ¸¸æ‰©æ•£TransformeråŠ¨ä½œå¤´ä¹‹é—´çš„**ç“¶é¢ˆæ¥å£**ã€‚\n- **å…³é”®è®¾è®¡**ï¼šåˆ©ç”¨è§£ç å™¨VLMçš„**å› æœæ³¨æ„åŠ›æ©ç **ï¼Œé€šè¿‡ç®€å•åœ°æ”¹å˜ `ğ’¬` åœ¨è¾“å…¥åºåˆ—ä¸­çš„ä½ç½®ï¼Œå°±èƒ½ç²¾ç¡®æ§åˆ¶å…¶èƒ½è®¿é—®çš„ä¿¡æ¯ã€‚\n- **åŒåˆ†æ”¯è®­ç»ƒæ¡†æ¶**ï¼š\n  - **å…ˆéªŒåˆ†æ”¯**ï¼šè¾“å…¥ä¸º `[v, ğ’¬, â„“]`ã€‚ç”±äºå› æœæ©ç ï¼Œ`ğ’¬` åªèƒ½çœ‹åˆ°è§†è§‰ `v`ï¼Œä»è€Œå­¦ä¹ **è§†è§‰å…ˆéªŒç­–ç•¥ `p(a|v)`**ã€‚\n  - **åéªŒåˆ†æ”¯**ï¼šè¾“å…¥ä¸º `[v, â„“, ğ’¬]`ã€‚`ğ’¬` èƒ½çœ‹åˆ°è§†è§‰å’Œè¯­è¨€ï¼Œä»è€Œå­¦ä¹ **å®Œæ•´çš„æ¡ä»¶ç­–ç•¥ `Ï€(a|v,â„“)`**ã€‚\n- **è®­ç»ƒç›®æ ‡**ï¼šæ€»æŸå¤±ç»“åˆäº†åŒåˆ†æ”¯çš„åŠ¨ä½œé¢„æµ‹æŸå¤±å’Œå¯¹æ•°ä¼¼ç„¶æ¯”æœ€å¤§åŒ–é¡¹ï¼š\n  ```math\n  â„’_total = (1-Î»)â„’_FM(ğ‡_ğ’¬_post) + Î»â„’_FM(ğ‡_ğ’¬_prior) - Î²â„’_LLR\n  ```\n- **æ¨ç†æ•ˆç‡**ï¼š**ä»…ä½¿ç”¨åéªŒåˆ†æ”¯**ï¼Œå› æ­¤**åœ¨æµ‹è¯•æ—¶ä¸äº§ç”Ÿä»»ä½•é¢å¤–è®¡ç®—å¼€é”€**ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆçš„ä»·å€¼ä¸æ•ˆæœ**\n\n#### **1. å®é™…ä»·å€¼**\n- **æå‡æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„OODåŸºå‡†æµ‹è¯•ï¼ˆSimplerEnvï¼‰ä¸Šï¼Œç›¸æ¯”å¼ºåŸºçº¿ï¼ˆQwenGR00Tï¼‰å–å¾—äº† **11.3% çš„ç»å¯¹æå‡**ï¼Œåœ¨RoboCasaä¸Šä¹Ÿè¾¾åˆ°äº†SOTAæ°´å¹³ã€‚\n- **ä¿ç•™é€šç”¨èƒ½åŠ›**ï¼šä¸æ ‡å‡†VLAè®­ç»ƒå¯¼è‡´VLMæ–‡æœ¬å¯¹è¯èƒ½åŠ›å´©æºƒä¸åŒï¼ŒBayesianVLAçš„LLRç›®æ ‡ä½œä¸ºä¸€ç§æ­£åˆ™åŒ–ï¼Œ**ä¿ç•™äº†éª¨å¹²VLMçš„çº¯æ–‡æœ¬å¯¹è¯å’Œæ¨ç†èƒ½åŠ›**ï¼ˆè§å›¾4,5ï¼‰ï¼Œè¿™å¯¹äºä¾èµ–é«˜çº§è¯­ä¹‰ç†è§£çš„æœºå™¨äººè‡³å…³é‡è¦ã€‚\n- **æ— éœ€æ–°æ•°æ®**ï¼šè¯¥æ–¹æ³•ç›´æ¥ä»å­˜åœ¨åè§çš„ç°æœ‰æ•°æ®ä¸­æ¢å¤å‡ºçœŸæ­£çš„è¯­è¨€æ¡ä»¶ç­–ç•¥ï¼Œ**ä¸è¦æ±‚æ”¶é›†æ–°çš„ã€æ›´å¹³è¡¡çš„æ•°æ®é›†**ã€‚\n\n#### **2. æŠ€æœ¯è´¡çŒ®æ€»ç»“**\n- **è¯Šæ–­**ï¼šé¦–æ¬¡ä»ä¿¡æ¯è®ºè§’åº¦ï¼ˆæ¡ä»¶äº’ä¿¡æ¯åç¼©ï¼‰å½¢å¼åŒ–å¹¶å®è¯éªŒè¯äº†VLAè®­ç»ƒä¸­çš„â€œè§†è§‰æ·å¾„â€ç—…ç†ã€‚\n- **æ–¹æ³•**ï¼šæå‡ºäº†ä¸€ä¸ªåŸºäºè´å¶æ–¯åˆ†è§£å’Œæ½œåœ¨åŠ¨ä½œæŸ¥è¯¢çš„**åŸåˆ™æ€§æ¡†æ¶**ï¼Œé€šè¿‡æœ€å¤§åŒ–PMIæ¥å¼ºåˆ¶è¯­è¨€ groundingã€‚\n- **éªŒè¯**ï¼šé€šè¿‡å¤§é‡å®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ‰“ç ´è§†è§‰æ·å¾„ï¼Œåœ¨åˆ†å¸ƒå¤–å’Œæ¨¡ç³Šåœºæ™¯ä¸­å®ç°é²æ£’æ³›åŒ–ï¼ŒåŒæ—¶ä¿ç•™äº†æ¨¡å‹çš„æ ¸å¿ƒè¯­è¨€èƒ½åŠ›ã€‚\n\n**æ€»ä¹‹ï¼ŒBayesianVLA çš„æ ¸å¿ƒåœ¨äºé€šè¿‡ä¸€ä¸ªç²¾å·§çš„ã€å—è´å¶æ–¯ç†è®ºå¯å‘çš„åŒåˆ†æ”¯æ¶æ„å’Œè®­ç»ƒç›®æ ‡ï¼Œè¿«ä½¿VLAæ¨¡å‹å¿…é¡»åˆ©ç”¨è¯­è¨€æŒ‡ä»¤ä¸­çš„ä¿¡æ¯æ¥å†³ç­–åŠ¨ä½œï¼Œä»è€Œè§£å†³äº†å½“å‰è®­ç»ƒèŒƒå¼ä¸‹æ¨¡å‹â€œå‡è£…â€éµå¾ªæŒ‡ä»¤çš„æ ¹æœ¬é—®é¢˜ã€‚**",
            "timestamp": "2026-01-27T16:01:50.202636",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡ã€ŠLangForce: Bayesian Decomposition of Vision Language Action Models via Latent Action Queriesã€‹å‘å±•è„‰ç»œæ¢³ç†\n\næœ¬æ–‡æå‡ºçš„ **BayesianVLA** æ˜¯ VLA æ¨¡å‹å‘å±•ä¸­çš„ä¸€ä¸ªé‡è¦èŠ‚ç‚¹ï¼Œæ—¨åœ¨è§£å†³â€œè§†è§‰æ·å¾„â€è¿™ä¸€æ ¸å¿ƒç—…ç†ã€‚å…¶å‘å±•è„‰ç»œå¯ä»¥è¿½æº¯åˆ°å¤šä¸ªå¹¶è¡Œä¸”ç›¸äº’äº¤ç»‡çš„ç ”ç©¶æ–¹å‘ã€‚ä»¥ä¸‹æ¢³ç†äº†ä»æ—©æœŸæ•°æ®é›†æ„å»ºåˆ°æœ€æ–°æ¶æ„åˆ›æ–°çš„å…³é”®æ­¥éª¤ï¼Œå¹¶ç€é‡åˆ†æäº†å‡ ä¸ªä¸»è¦å‘å±•æ–¹å‘ã€‚\n\n### å‘å±•è„‰ç»œæ€»è§ˆ\n\nVLA æ¨¡å‹çš„å‘å±•éµå¾ªâ€œ**æ•°æ®è§„æ¨¡åŒ– â†’ æ¶æ„é€šç”¨åŒ– â†’ ä¸“ä¸šåŒ–ä¸é²æ£’æ€§æå‡**â€çš„è·¯å¾„ã€‚æ—©æœŸå·¥ä½œé›†ä¸­äºæ„å»ºå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æœºå™¨äººæ•°æ®é›†ï¼Œä¸ºè®­ç»ƒé€šç”¨ç­–ç•¥å¥ å®šåŸºç¡€ã€‚éšåï¼Œç ”ç©¶é‡ç‚¹è½¬å‘å¦‚ä½•å°†å¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹é€‚é…åˆ°æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œäº§ç”Ÿäº†å¤šç§æ¶æ„èŒƒå¼ã€‚è¿‘æœŸå·¥ä½œåˆ™å¼€å§‹æ·±å…¥è§£å†³é€šç”¨åŒ–è¿‡ç¨‹ä¸­çš„å…·ä½“ç“¶é¢ˆï¼Œå¦‚è·¨å…·èº«æ³›åŒ–ã€ç©ºé—´ç†è§£ã€é•¿è§†é‡è§„åˆ’ï¼Œä»¥åŠæœ¬æ–‡å…³æ³¨çš„**æŒ‡ä»¤è·Ÿéšé²æ£’æ€§**é—®é¢˜ã€‚\n\n### å…³é”®å‘å±•é˜¶æ®µä¸æ”¹è¿›\n\n#### æ–¹å‘ä¸€ï¼š**æ•°æ®é›†ä¸åŸºå‡†çš„è§„æ¨¡åŒ–ä¸å¤šæ ·åŒ–**\nè¿™æ˜¯VLAæ¨¡å‹å‘å±•çš„åŸºçŸ³ï¼Œæ¯ä¸€æ­¥æ”¹è¿›éƒ½åœ¨æ‰©å¤§æ•°æ®è§„æ¨¡ã€ä¸°å¯Œåœºæ™¯å’Œä»»åŠ¡å¤šæ ·æ€§ï¼Œä»¥æ”¯æŒæ›´é€šç”¨ç­–ç•¥çš„å­¦ä¹ ã€‚\n\n1.  **LIBERO (2023)**\n    *   **æ”¹è¿›**ï¼šé¦–æ¬¡ç³»ç»ŸåŒ–åœ°ç ”ç©¶æœºå™¨äººç»ˆèº«å­¦ä¹ ä¸­çš„çŸ¥è¯†è¿ç§»é—®é¢˜ï¼Œæä¾›äº†ç»“æ„åŒ–çš„åŸºå‡†æ¥è¯„ä¼°æ¨¡å‹åœ¨æ–°ä»»åŠ¡ã€æ–°å¯¹è±¡ã€æ–°ç›®æ ‡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚\n    *   **ä»·å€¼**ï¼šæ­ç¤ºäº†ä»»åŠ¡æ¨¡ç³Šæ€§ï¼ˆåŒä¸€åœºæ™¯å¯¹åº”å¤šä¸ªä»»åŠ¡ï¼‰å¯¹æ¨¡å‹æ³›åŒ–çš„æŒ‘æˆ˜ï¼Œä¸ºåç»­ç ”ç©¶ï¼ˆå¦‚æœ¬æ–‡ï¼‰æä¾›äº†å…³é”®çš„è¯Šæ–­å·¥å…·ã€‚\n\n2.  **BridgeData V2 (2023) & Droid (2024)**\n    *   **æ”¹è¿›**ï¼šä»å®éªŒå®¤å¯æ§ç¯å¢ƒæ‰©å±•åˆ°â€œé‡å¤–â€çœŸå®ä¸–ç•Œæ•°æ®ã€‚BridgeData V2 åœ¨ä½æˆæœ¬ç¡¬ä»¶ä¸Šæ”¶é›†äº†å¤šæ ·åŒ–çš„äº¤äº’è½¨è¿¹ï¼›Droid è¿›ä¸€æ­¥é€šè¿‡åˆ†å¸ƒå¼æ•°æ®æ”¶é›†æå¤§æå‡äº†æ•°æ®å¤šæ ·æ€§ã€‚\n    *   **ä»·å€¼**ï¼šæä¾›äº†å¤§è§„æ¨¡ã€å¼‚æ„çš„çœŸå®æœºå™¨äººæ•°æ®ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´å¹¿æ³›çš„æ“ä½œæŠ€èƒ½å’Œåº”å¯¹ç¯å¢ƒå˜åŒ–ã€‚\n\n3.  **Open X-Embodiment (OXE) (2024)**\n    *   **æ”¹è¿›**ï¼šå®ç°äº†**è·¨å…·èº«**æ•°æ®é›†çš„èšåˆï¼Œæ•´åˆäº†22ç§ä¸åŒæœºå™¨äººå¹³å°çš„æ•°æ®ã€‚\n    *   **ä»·å€¼**ï¼šé¦–æ¬¡è¯æ˜å¯ä»¥åœ¨ä¸åŒå½¢æ€çš„æœºå™¨äººæ•°æ®ä¸Šè”åˆè®­ç»ƒï¼Œå­¦ä¹ é€šç”¨çš„â€œæœºå™¨äººç­–ç•¥â€ï¼Œæ¨åŠ¨äº†é€šç”¨æœºå™¨äººåŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚\n\n4.  **RoboCasa (2024) & SimplerEnv (2024)**\n    *   **æ”¹è¿›**ï¼šæä¾›äº†å¯æ‰©å±•ã€å¯é‡å¤çš„**ä»¿çœŸè¯„ä¼°åŸºå‡†**ã€‚RoboCasa æä¾›å¤æ‚çš„å¨æˆ¿åœºæ™¯ï¼›SimplerEnv åˆ™æä¾›äº†ä¸€ä¸ªä¸çœŸå®ä¸–ç•Œæ€§èƒ½ç›¸å…³çš„è½»é‡çº§ä»¿çœŸä»£ç†ã€‚\n    *   **ä»·å€¼**ï¼šè§£å†³äº†æœºå™¨äººç‰©ç†è¯„ä¼°çš„â€œå¯é‡å¤æ€§å±æœºâ€ï¼Œä½¿ç®—æ³•å¯¹æ¯”æ›´åŠ å…¬å¹³ã€é«˜æ•ˆã€‚\n\n#### æ–¹å‘äºŒï¼š**VLA æ¨¡å‹æ¶æ„ä¸è®­ç»ƒèŒƒå¼çš„æ¼”è¿›**\næ­¤æ–¹å‘å…³æ³¨å¦‚ä½•å°†é¢„è®­ç»ƒVLMçš„çŸ¥è¯†æœ‰æ•ˆåœ°â€œçŒè¾“â€åˆ°æ§åˆ¶ç­–ç•¥ä¸­ï¼Œå¹¶æå‡åŠ¨ä½œå»ºæ¨¡èƒ½åŠ›ã€‚\n\n1.  **Octo (2024) & OpenVLA ç³»åˆ— (2024-2025)**\n    *   **æ”¹è¿›**ï¼šç¡®ç«‹äº†åŸºäºTransformerçš„é€šç”¨ç­–ç•¥æ¶æ„ã€‚OpenVLA ç›´æ¥å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå…¨å‚æ•°å¾®è°ƒï¼Œå°†å…¶ç”¨äºæœºå™¨äººæ§åˆ¶ã€‚\n    *   **ä»·å€¼**ï¼šéªŒè¯äº†ç›´æ¥åˆ©ç”¨å¼ºå¤§VLMè¿›è¡Œç«¯åˆ°ç«¯ç­–ç•¥å­¦ä¹ çš„å¯è¡Œæ€§ï¼Œä½†é¢ä¸´è®¡ç®—æˆæœ¬é«˜å’Œç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ã€‚OpenVLA-OFT é€šè¿‡ä¼˜åŒ–å™¨å¾®è°ƒç­‰æŠ€æœ¯æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚\n\n2.  **GR00T ç³»åˆ— (2025) & Ï€â‚€ ç³»åˆ— (2024-2025)**\n    *   **GR00Tçš„æ”¹è¿›**ï¼šé‡‡ç”¨**åŒç³»ç»Ÿè®¾è®¡**ï¼Œæ¶æ„ä¸Šè€¦åˆä¸€ä¸ªç”¨äºæ„ŸçŸ¥çš„VLMå’Œä¸€ä¸ªç”¨äºåŠ¨ä½œç”Ÿæˆçš„æ‰©æ•£å¤´ï¼Œå¹¶è¿›è¡ŒååŒè®­ç»ƒã€‚\n    *   **Ï€â‚€ç³»åˆ—çš„æ”¹è¿›**ï¼šå¼•å…¥**æµåŒ¹é…**ä½œä¸ºæ›´ä¼˜çš„ç”Ÿæˆç›®æ ‡ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆå…ˆé¢„è®­ç»ƒVLMä¸»å¹²ï¼Œå†å¾®è°ƒï¼‰ã€‚Ï€â‚€.â‚… åŠ å…¥äº†å­ä»»åŠ¡åˆ†è§£ä»¥å¢å¼ºé•¿è§†é‡è§„åˆ’èƒ½åŠ›ã€‚\n    *   **ä»·å€¼**ï¼šä»£è¡¨äº†å½“å‰é«˜æ€§èƒ½VLAçš„ä¸¤å¤§ä¸»æµæ¶æ„èŒƒå¼ï¼Œåˆ†åˆ«ä»ç³»ç»Ÿè®¾è®¡å’Œè®­ç»ƒç›®æ ‡ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ã€‚\n\n3.  **ä¸“ä¸šåŒ–æ¶æ„æ¢ç´¢ (2025)**\n    *   **X-VLA**ï¼šå¼•å…¥**å…·èº«ç‰¹å®šçš„è½¯æç¤º**ï¼Œä»¥æå°‘çš„å‚æ•°é‡å®ç°è·¨å…·èº«æ³›åŒ–ã€‚\n    *   **SpatialVLA**ï¼šå¼ºè°ƒ**ç©ºé—´ç†è§£**ï¼Œå¼•å…¥3Dä½ç½®ç¼–ç å’Œè‡ªé€‚åº”åŠ¨ä½œç½‘æ ¼æ¥æ³¨å…¥å¯è¿ç§»çš„ç©ºé—´çŸ¥è¯†ã€‚\n    *   **VideoVLA**ï¼šæ¢ç´¢å°†**è§†é¢‘ç”Ÿæˆæ¨¡å‹**è½¬åŒ–ä¸ºæœºå™¨äººæ“ä½œå™¨ï¼Œé€šè¿‡è”åˆé¢„æµ‹åŠ¨ä½œåºåˆ—å’Œæœªæ¥è§†è§‰ç»“æœï¼Œåˆ©ç”¨æ¨¡å‹çš„â€œè§†è§‰æƒ³è±¡åŠ›â€ã€‚\n    *   **ä»·å€¼**ï¼šé’ˆå¯¹VLAçš„ä¸åŒè–„å¼±ç¯èŠ‚ï¼ˆè·¨å¹³å°ã€ç©ºé—´æ¨ç†ã€é•¿è§†é‡æƒ³è±¡ï¼‰è¿›è¡Œä¸“é¡¹å¢å¼ºï¼Œä½“ç°äº†é¢†åŸŸå†…çš„ç²¾ç»†åŒ–æ¢ç´¢ã€‚\n\n#### æ–¹å‘ä¸‰ï¼š**è§£å†³æ ¸å¿ƒç—…ç†ï¼šæŒ‡ä»¤è·Ÿéšä¸æ³›åŒ–é²æ£’æ€§**\nè¿™æ˜¯æœ¬æ–‡æ‰€å¤„çš„æ ¸å¿ƒæ–¹å‘ï¼Œæ—¨åœ¨è§£å†³æ•°æ®åå·®å¯¼è‡´çš„æ¨¡å‹é€€åŒ–é—®é¢˜ã€‚\n\n1.  **é—®é¢˜å‘ç°ä¸å®è¯ (å‰æœŸå·¥ä½œï¼Œå¦‚æœ¬æ–‡ç¬¬2èŠ‚æ‰€è¿°)**\n    *   **æ”¹è¿›**ï¼šé€šè¿‡ç³»ç»Ÿçš„å¯¹ç…§å®éªŒï¼ˆå¦‚â€œä»…è§†è§‰â€è®­ç»ƒï¼‰ï¼Œå®è¯æ­ç¤ºäº†å½“å‰VLAæ¨¡å‹åœ¨ç›®æ ‡é©±åŠ¨æ•°æ®é›†ä¸Šå­˜åœ¨ä¸¥é‡çš„â€œ**è§†è§‰æ·å¾„**â€é—®é¢˜ã€‚æ¨¡å‹å®é™…ä¸Šå­¦ä¹ çš„æ˜¯ `p(a|v)` è€ŒéçœŸæ­£çš„ `Ï€(a|v, â„“)`ï¼Œå¯¼è‡´åœ¨æ¨¡ç³Šæˆ–OODåœºæ™¯ä¸‹å¤±è´¥ã€‚\n    *   **ä»·å€¼**ï¼šç²¾å‡†å®šä½äº†VLAæ³›åŒ–èƒ½åŠ›å·®çš„æ ¹æœ¬åŸå› ä¹‹ä¸€â€”â€”**æ¡ä»¶äº’ä¿¡æ¯åç¼©**ï¼Œä¸ºåç»­è§£å†³æ–¹æ¡ˆæä¾›äº†æ˜ç¡®é¶ç‚¹ã€‚\n\n2.  **BayesVLA (Xu et al., 2025) - åŒæœŸç›¸å…³å·¥ä½œ**\n    *   **æ”¹è¿›**ï¼šåŒæ ·é‡‡ç”¨**è´å¶æ–¯åˆ†è§£**æ€è·¯ï¼Œä½¿ç”¨CLIPç¼–ç å™¨å’Œæ‰©æ•£Transformerï¼Œé€šè¿‡ä¸¤é˜¶æ®µæ¡†æ¶ï¼ˆå…ˆè®­ç»ƒè§†è§‰æ¡ä»¶å…ˆéªŒï¼Œå†æ³¨å…¥è¯­è¨€æ„ŸçŸ¥tokenï¼‰æ¥åº”å¯¹è¯­è¨€å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚\n    *   **å±€é™**ï¼šæœªå°†è®­ç»ƒæ‰©å±•åˆ°å¤§è§„æ¨¡VLMï¼Œä¸”å…¶è§£å†³æ–¹æ¡ˆæœªç³»ç»Ÿæ€§é’ˆå¯¹â€œè§†è§‰æ·å¾„â€ç—…ç†ã€‚\n\n3.  **æœ¬æ–‡å·¥ä½œï¼šBayesianVLA**\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼š\n        *   **ç†è®ºæ¡†æ¶**ï¼šæ˜ç¡®æå‡ºé€šè¿‡æœ€å¤§åŒ–åŠ¨ä½œä¸æŒ‡ä»¤çš„**æ¡ä»¶ç‚¹äº’ä¿¡æ¯**æ¥å¯¹æŠ—ä¿¡æ¯åç¼©ï¼Œå¹¶æ¨å¯¼å‡ºå¯ä¼˜åŒ–çš„å¯¹æ•°ä¼¼ç„¶æ¯”ç›®æ ‡ã€‚\n        *   **æ¶æ„åˆ›æ–°**ï¼šå¼•å…¥**æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢**ä½œä¸ºVLMä¸åŠ¨ä½œå¤´ä¹‹é—´çš„ç“¶é¢ˆæ¥å£ï¼Œå¹¶åˆ©ç”¨å› æœæ©ç å·§å¦™åœ°å®ç°**åŒåˆ†æ”¯æ¶æ„**ï¼Œåˆ†åˆ«å»ºæ¨¡å…ˆéªŒ `p(a|v)` å’ŒåéªŒ `Ï€(a|v, â„“)`ã€‚\n        *   **è®­ç»ƒç­–ç•¥**ï¼šåœ¨å…±äº«çš„VLMæƒé‡ä¸Šï¼Œé€šè¿‡åŒåˆ†æ”¯å’ŒLLRæŸå¤±è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œå¼ºåˆ¶æ¨¡å‹å…³æ³¨è¯­è¨€æŒ‡ä»¤ä¸­æ— æ³•ä»è§†è§‰ä¸­æ¨æ–­çš„ä¿¡æ¯ã€‚\n        *   **å®é™…ä»·å€¼**ï¼š**æ— éœ€æ–°æ•°æ®**ï¼Œå³åœ¨OODåŸºå‡†ä¸Šå–å¾—æ˜¾è‘—æå‡ï¼ˆå¦‚SimplerEnvä¸Š+11.3%ï¼‰ï¼ŒåŒæ—¶è¾ƒå¥½åœ°ä¿ç•™äº†VLMä¸»å¹²çš„è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œæ¨ç†æ—¶æ— é¢å¤–è®¡ç®—å¼€é”€ã€‚\n\n### æ€»ç»“ï¼šå‡ ä¸ªä¸»è¦å‘å±•æ–¹å‘\n\n1.  **æ•°æ®ä¸è¯„ä¼°é©±åŠ¨æ–¹å‘**ï¼šæŒç»­è¿½æ±‚æ›´å¤§è§„æ¨¡ã€æ›´å¤šæ ·åŒ–ã€æ›´è·¨åŸŸçš„æ•°æ®é›†ï¼Œå¹¶å»ºç«‹ä¸¥è°¨çš„ä»¿çœŸä¸å®ä½“è¯„ä¼°åŸºå‡†ï¼Œä¸ºæ¨¡å‹å‘å±•æä¾›ç‡ƒæ–™å’Œæ ‡å°ºã€‚\n2.  **æ¶æ„ä¸ç®—æ³•é©±åŠ¨æ–¹å‘**ï¼šæ¢ç´¢æ›´é«˜æ•ˆçš„VLMé€‚é…æ–¹æ³•ï¼ˆå¾®è°ƒã€æç¤ºã€åŒç³»ç»Ÿï¼‰ã€æ›´ä¼˜çš„åŠ¨ä½œè¡¨ç¤ºä¸ç”Ÿæˆæ–¹å¼ï¼ˆæ‰©æ•£ã€æµåŒ¹é…ã€tokenåŒ–ï¼‰ï¼Œä»¥åŠé’ˆå¯¹ç‰¹å®šèƒ½åŠ›ï¼ˆç©ºé—´ã€è§„åˆ’ã€è·¨å¹³å°ï¼‰çš„ä¸“é¡¹å¢å¼ºã€‚\n3.  **é²æ£’æ€§ä¸æ³›åŒ–ç†è®ºæ–¹å‘**ï¼šæ·±å…¥åˆ†ææ¨¡å‹å¤±è´¥çš„æ ¹æœ¬åŸå› ï¼ˆå¦‚æœ¬æ–‡çš„â€œè§†è§‰æ·å¾„â€ã€ä¿¡æ¯åç¼©ï¼‰ï¼Œå¹¶ä»ä¿¡æ¯è®ºã€è´å¶æ–¯ç†è®ºç­‰è§’åº¦æå‡ºæ­£åˆ™åŒ–æˆ–ç»“æ„åŒ–è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨æå‡æ¨¡å‹å¯¹æŒ‡ä»¤çš„ç†è§£å¿ å®åº¦å’Œåœ¨åˆ†å¸ƒå¤–åœºæ™¯çš„ç¨³å®šæ€§ã€‚\n\n**BayesianVLA** å±äºç¬¬ä¸‰ä¸ªæ–¹å‘ä¸Šçš„ä»£è¡¨æ€§è¿›å±•ã€‚å®ƒæ²¡æœ‰è¿½æ±‚æ›´å¤§çš„æ¨¡å‹æˆ–æ›´å¤šçš„æ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ªç²¾å·§çš„ç†è®ºé©±åŠ¨æ¡†æ¶ï¼Œç›´å‡»å½“å‰VLAè®­ç»ƒèŒƒå¼ä¸­çš„å›ºæœ‰ç¼ºé™·ï¼Œä»è€Œåœ¨ç°æœ‰æ•°æ®ä¸Šâ€œæŒ¤â€å‡ºäº†æ›´é«˜çš„æ³›åŒ–æ€§èƒ½å’Œé²æ£’æ€§ã€‚è¿™æ ‡å¿—ç€VLAç ”ç©¶ä»â€œè§„æ¨¡æ‰©å¼ â€å‘â€œè´¨é‡æå‡â€å’Œâ€œåŸç†æ·±åŒ–â€çš„é‡è¦è½¬å˜ã€‚",
            "timestamp": "2026-01-27T16:02:56.615067",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡ã€ŠLangForce: Bayesian Decomposition of Vision Language Action Models via Latent Action Queriesã€‹çš„å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œæœ¬æ–‡çš„æ ¸å¿ƒå·¥ä½œæ˜¯æå‡º **BayesianVLA** æ¡†æ¶ã€‚å…¶æœ€ç›´æ¥ã€æœ€ç›¸å…³çš„å‰ä½œæ˜¯ **BayesVLA**ï¼ˆXu et al., 2025ï¼‰ã€‚æ­¤å¤–ï¼Œè®ºæ–‡åœ¨â€œç›¸å…³å·¥ä½œâ€éƒ¨åˆ†æ¢³ç†äº†VLAæ¨¡å‹çš„å‘å±•è„‰ç»œï¼Œå…¶ä¸­ä¸€äº›ä»£è¡¨æ€§å·¥ä½œå¯è§†ä¸ºå…¶æŠ€æœ¯èƒŒæ™¯å’Œçµæ„Ÿæ¥æºã€‚ä»¥ä¸‹è¡¨æ ¼è¯¦ç»†åˆ†æäº†è¿™äº›å‰ä½œï¼Œå¹¶é‡ç‚¹å¯¹æ¯”äº†æœ¬æ–‡ï¼ˆBayesianVLAï¼‰ä¸æœ€ç›´æ¥çš„å‰ä½œï¼ˆBayesVLAï¼‰çš„åŒºåˆ«å’Œæ”¹è¿›ã€‚\n\n### å‰ä½œè¯¦ç»†åˆ†æè¡¨\n\n| å‰ä½œåç§° (å¹´ä»½) | æ ¸å¿ƒè´¡çŒ® / æ–¹æ³• | ä¸æœ¬æ–‡ï¼ˆBayesianVLAï¼‰çš„å…³ç³» | æœ¬æ–‡ç›¸å¯¹äºè¯¥å‰ä½œçš„æ”¹è¿› / åŒºåˆ« |\n| :--- | :--- | :--- | :--- |\n| **BayesVLA** (Xu et al., 2025) | **æ ¸å¿ƒå‰ä½œ**ï¼šåŒæ ·é‡‡ç”¨è´å¶æ–¯åˆ†è§£æ€è·¯è§£å†³VLAæ¨¡å‹çš„è¯­è¨€ä¾èµ–é—®é¢˜ã€‚å…·ä½“æ–¹æ³•ï¼š<br>1. **ä¸¤é˜¶æ®µè®­ç»ƒ**ï¼šç¬¬ä¸€é˜¶æ®µåœ¨å¤§è§„æ¨¡è§†è§‰-åŠ¨ä½œå¯¹æ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ª**è§†è§‰æ¡ä»¶å…ˆéªŒ** `p(a|v)`ï¼›ç¬¬äºŒé˜¶æ®µå†»ç»“å…ˆéªŒï¼Œæ³¨å…¥**è¯­è¨€æ„ŸçŸ¥ä»¤ç‰Œ**æ¥å¾®è°ƒç­–ç•¥ï¼Œä½¿å…¶æ¡ä»¶äºæŒ‡ä»¤ `Ï€(a|v, l)`ã€‚<br>2. **æ¶æ„**ï¼šä½¿ç”¨CLIPç¼–ç å™¨å’Œæ‰©æ•£Transformerï¼ˆDiTï¼‰ã€‚<br>3. **ç›®æ ‡**ï¼šæ—¨åœ¨ç¼“è§£è®­ç»ƒæ•°æ®ä¸­è¯­è¨€å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚ | **ç›´æ¥ç»§æ‰¿ä¸å‘å±•çš„å…³ç³»**ã€‚æœ¬æ–‡è®¤åŒå…¶ç”¨è´å¶æ–¯æ¡†æ¶åˆ†è§£ç­–ç•¥çš„æ ¸å¿ƒç†å¿µï¼Œå¹¶å°†å…¶ä½œä¸ºä¸»è¦å¯¹æ¯”å’Œè¶…è¶Šçš„å¯¹è±¡ã€‚ | **æœ¬æ–‡è¿›è¡Œäº†å…¨é¢ã€æ ¹æœ¬æ€§çš„æ”¹è¿›**ï¼š<br>1. **æ¶æ„ä¸è®­ç»ƒä¸€ä½“åŒ–**ï¼šBayesVLAæ˜¯**ä¸¤é˜¶æ®µã€æ¨¡å—åŒ–**çš„ï¼ˆå…ˆè®­ç»ƒå…ˆéªŒï¼Œå†å¾®è°ƒåéªŒï¼‰ã€‚è€Œ**BayesianVLAæ˜¯å•é˜¶æ®µã€ç«¯åˆ°ç«¯**çš„ï¼Œé€šè¿‡**æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢**å’Œ**åŒåˆ†æ”¯æ¶æ„**åœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…åŒæ—¶å­¦ä¹ å…ˆéªŒå’ŒåéªŒã€‚<br>2. **å¯æ‰©å±•æ€§**ï¼šBayesVLAæœªå°†å…¶æ–¹æ³•æ‰©å±•åˆ°**å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹**ã€‚**BayesianVLAåˆ™ç›´æ¥æ„å»ºåœ¨Qwen3-VLç­‰å¤§å‹VLMä¹‹ä¸Š**ï¼Œè¯æ˜äº†æ–¹æ³•åœ¨å¤§æ¨¡å‹ä¸Šçš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚<br>3. **ç›®æ ‡å‡½æ•°**ï¼šBayesVLAæœªæ˜ç¡®é’ˆå¯¹â€œè§†è§‰æ·å¾„â€é—®é¢˜æå‡ºç³»ç»Ÿè§£å†³æ–¹æ¡ˆã€‚**BayesianVLAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæ˜ç¡®æå‡ºäº†æœ€å¤§åŒ–æ¡ä»¶ç‚¹äº’ä¿¡æ¯**çš„**å¯¹æ•°ä¼¼ç„¶æ¯”ç›®æ ‡**ï¼Œç›´æ¥æƒ©ç½šæ¨¡å‹å¿½ç•¥è¯­è¨€çš„è¡Œä¸ºã€‚<br>4. **è®¡ç®—ä¸è®¾è®¡**ï¼šBayesianVLAå¼•å…¥äº†**æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢**ä½œä¸ºVLMä¸åŠ¨ä½œè§£ç å™¨ä¹‹é—´çš„**ç“¶é¢ˆæ¥å£**ï¼Œä¸ä»…å®ç°äº†çµæ´»çš„åŒåˆ†æ”¯æ§åˆ¶ï¼Œè¿˜**é™ä½äº†DiTçš„æ¡ä»¶å¤„ç†å¤æ‚åº¦**ï¼ˆä»O(NÂ²)é™è‡³O(KÂ²)ï¼‰ã€‚æ¨ç†æ—¶æ— é¢å¤–å¼€é”€ã€‚ |\n| **GR00T ç³»åˆ—** (Bjorck et al., 2025b, a; GEAR-Team et al., 2025) | **èƒŒæ™¯/åŸºçº¿**ï¼šä¸€ç³»åˆ—å¼€æºé€šç”¨äººå½¢æœºå™¨äººåŸºç¡€æ¨¡å‹ã€‚é‡‡ç”¨**åŒç³»ç»Ÿè®¾è®¡**ï¼Œæ¶æ„ä¸Šå°†ç”¨äºæ„ŸçŸ¥çš„VLMä¸ç”¨äºåŠ¨ä½œç”Ÿæˆçš„æ‰©æ•£å¤´è€¦åˆã€‚æ˜¯å½“å‰é«˜æ€§èƒ½VLAçš„å…¸å‹ä»£è¡¨ï¼Œæœ¬æ–‡çš„åŸºçº¿æ¨¡å‹ï¼ˆQwenGR00Tï¼‰åŸºäºæ­¤æ¡†æ¶ã€‚ | **æŠ€æœ¯èƒŒæ™¯ä¸æ€§èƒ½åŸºå‡†**ã€‚æœ¬æ–‡çš„æ–¹æ³•æ˜¯åœ¨æ­¤ç±»å…ˆè¿›æ¶æ„ä¸Šè¿›è¡Œçš„**æ”¹è¿›æ’ä»¶**ï¼Œè€Œéæ›¿ä»£æ•´ä¸ªæ¶æ„ã€‚ | **æ ¸å¿ƒåŒºåˆ«åœ¨äºå­¦ä¹ èŒƒå¼**ï¼š<br>- GR00Tç­‰æ ‡å‡†VLAæ¨¡å‹ä½¿ç”¨æ ‡å‡†çš„ç›‘ç£å­¦ä¹ ï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå®¹æ˜“é™·å…¥â€œè§†è§‰æ·å¾„â€ã€‚<br>- **BayesianVLAåœ¨GR00Tæ¶æ„å†…éƒ¨å¼•å…¥äº†è´å¶æ–¯åˆ†è§£å’ŒLLRç›®æ ‡**ï¼Œ**æ”¹å˜äº†ä¼˜åŒ–ç›®æ ‡**ï¼Œè¿«ä½¿æ¨¡å‹å¿…é¡»åˆ©ç”¨è¯­è¨€ä¿¡æ¯ï¼Œä»è€Œè§£å†³äº†GR00Tç³»åˆ—æ¨¡å‹åœ¨OODå’Œæ¨¡ç³Šåœºæ™¯ä¸‹æš´éœ²å‡ºçš„æ³›åŒ–å¼±ç‚¹ã€‚ |\n| **Ï€â‚€ ç³»åˆ—** (Black et al., 2024, 2025; Pertsch et al., 2025) | **èƒŒæ™¯/åŸºçº¿**ï¼šå¦ä¸€ç³»åˆ—é«˜æ€§èƒ½VLAæ¨¡å‹ã€‚æ ¸å¿ƒç‰¹ç‚¹æ˜¯é‡‡ç”¨**æµåŒ¹é…**ä½œä¸ºç”Ÿæˆç›®æ ‡ï¼Œå¹¶é‡‡ç”¨**ä¸¤é˜¶æ®µè®­ç»ƒ**èŒƒå¼ï¼ˆå…ˆé¢„è®­ç»ƒVLMä¸»å¹²ï¼Œå†å¾®è°ƒï¼‰ã€‚ä¾‹å¦‚ï¼ŒÏ€â‚€.5å¼•å…¥äº†å­ä»»åŠ¡åˆ†è§£æ¥å¢å¼ºé•¿ç¨‹è§„åˆ’ã€‚ | **æŠ€æœ¯èƒŒæ™¯ä¸æ€§èƒ½åŸºå‡†**ã€‚åŒæ ·æ˜¯æœ¬æ–‡åœ¨å®éªŒä¸­å¯¹æ¯”å’Œè¶…è¶Šçš„å¼ºåŸºçº¿ã€‚ | **æ”¹å–„åœ¨äºé—®é¢˜æ ¹æºçš„æ²»ç†**ï¼š<br>- Ï€â‚€ç³»åˆ—é€šè¿‡æ”¹è¿›ç”Ÿæˆç›®æ ‡ï¼ˆæµåŒ¹é…ï¼‰å’Œè®­ç»ƒç­–ç•¥æ¥æå‡æ€§èƒ½ï¼Œä½†**æ²¡æœ‰ç³»ç»Ÿæ€§åœ°è§£å†³æ•°æ®é›†åå·®å¯¼è‡´çš„â€œä¿¡æ¯åç¼©â€é—®é¢˜**ã€‚<br>- **BayesianVLAç›´æŒ‡è¯¥ç—…ç†ï¼Œé€šè¿‡ä¿¡æ¯è®ºç›®æ ‡ï¼ˆæœ€å¤§åŒ–PMIï¼‰ä»æ ¹æœ¬ä¸Šå¼ºåˆ¶æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤è·Ÿéš**ï¼Œä»è€Œåœ¨OODæ³›åŒ–ä¸Šå–å¾—äº†æ›´æ˜¾è‘—çš„æå‡ï¼ˆå¦‚SimplerEnvä¸Š+11.3%ï¼‰ã€‚ |\n| **OpenVLA / OpenVLA-OFT** (Kim et al., 2024, 2025) | **èƒŒæ™¯/å…ˆé©±**ï¼šæ—©æœŸå°†å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒç”¨äºæœºå™¨äººæ§åˆ¶çš„VLAå·¥ä½œã€‚OpenVLA-OFTä¼˜åŒ–äº†å¾®è°ƒé€Ÿåº¦ã€‚ | **é¢†åŸŸå…ˆé©±**ã€‚æœ¬æ–‡çš„æ–¹æ³•å±äºè¯¥é¢†åŸŸæ›´å‰æ²¿çš„å‘å±•ï¼Œé’ˆå¯¹çš„æ˜¯è¿™äº›æ—©æœŸæ¨¡å‹åœ¨è§„æ¨¡åŒ–åº”ç”¨ä¸­æš´éœ²å‡ºçš„æ–°é—®é¢˜ã€‚ | **ä»æ€§èƒ½ä¼˜åŒ–åˆ°æ³›åŒ–åŸç†çš„æ·±åŒ–**ï¼š<br>- OpenVLAä¸»è¦å…³æ³¨å¦‚ä½•å°†VLMèƒ½åŠ›è¿ç§»åˆ°æ§åˆ¶ä»»åŠ¡ï¼Œå¹¶æå‡è®­ç»ƒæ•ˆç‡ã€‚<br>- **BayesianVLAå…³æ³¨çš„æ˜¯â€œè¿ç§»åâ€çš„é²æ£’æ€§é—®é¢˜**ï¼Œå³æ¨¡å‹åœ¨çœ‹ä¼¼å­¦ä¼šæŒ‡ä»¤è·Ÿéšåï¼Œå…¶ä¾èµ–æ˜¯å¦çœŸæ­£å»ºç«‹åœ¨è¯­è¨€ä¸Šã€‚æœ¬æ–‡æä¾›äº†è¯Šæ–­å·¥å…·ï¼ˆè§†è§‰æ·å¾„å®éªŒï¼‰å’ŒåŸç†æ€§è§£å†³æ–¹æ¡ˆï¼ˆè´å¶æ–¯åˆ†è§£ï¼‰ã€‚ |\n| **SpatialVLA, VideoVLA** (Qu et al., 2025; Shen et al., 2025) | **åŒæœŸ/ç›¸å…³æ–¹å‘**ï¼šæ¢ç´¢VLAä¸­ç‰¹å®šèƒ½åŠ›çš„å¢å¼ºã€‚SpatialVLAå¼ºè°ƒ**ç©ºé—´è¡¨ç¤º**ï¼›VideoVLAæ¢ç´¢åˆ©ç”¨**è§†é¢‘ç”Ÿæˆæ¨¡å‹**çš„â€œè§†è§‰æƒ³è±¡åŠ›â€æ¥å¢å¼ºæ³›åŒ–ã€‚ | **å¹³è¡Œç ”ç©¶æ–¹å‘**ã€‚è¿™äº›å·¥ä½œä»ä¸åŒè§’åº¦ï¼ˆç©ºé—´ã€è§†é¢‘é¢„æµ‹ï¼‰æå‡VLAèƒ½åŠ›ï¼Œè€Œæœ¬æ–‡ä»**ä¿¡æ¯è®ºå’Œè®­ç»ƒåŠ¨åŠ›å­¦**è§’åº¦åˆ‡å…¥ã€‚ | **åˆ‡å…¥è§’åº¦ä¸åŒ**ï¼š<br>- SpatialVLA/VideoVLAé€šè¿‡**æ³¨å…¥æ–°çš„è¡¨å¾æˆ–æ¨¡å—**æ¥å¢å¼ºæ¨¡å‹èƒ½åŠ›ã€‚<br>- **BayesianVLAåˆ™é€šè¿‡**æ”¹è¿›è®­ç»ƒç›®æ ‡å’Œæ¶æ„è®¾è®¡**ï¼Œæ¥**çº æ­£ç°æœ‰è®­ç»ƒæ•°æ®ä¸Šçš„å­¦ä¹ åå·®**ï¼Œé‡Šæ”¾æ¨¡å‹å·²æœ‰æ¶æ„çš„æ½œåŠ›ã€‚å®ƒæ˜¯ä¸€ç§â€œçŸ«æ­£â€è€Œéâ€œå¢å¼ºâ€çš„èŒƒå¼ã€‚ |\n\n### æ€»ç»“ï¼šBayesianVLAç›¸å¯¹äºå‰ä½œï¼ˆå°¤å…¶æ˜¯BayesVLAï¼‰çš„æ ¸å¿ƒæ”¹è¿›\n\n1.  **ä»â€œåˆ†è§£â€åˆ°â€œä¸€ä½“åŒ–åˆ†è§£ä¸ä¼˜åŒ–â€**ï¼š\n    - **å‰ä½œBayesVLA**ï¼šæå‡ºäº†è´å¶æ–¯åˆ†è§£çš„æ€æƒ³ï¼Œä½†å®ç°ä¸Šæ˜¯å‰²è£‚çš„**ä¸¤é˜¶æ®µæµç¨‹**ã€‚å…ˆéªŒæ¨¡å‹å’ŒåéªŒæ¨¡å‹çš„è®­ç»ƒæ˜¯åˆ†ç¦»çš„ã€‚\n    - **æœ¬æ–‡BayesianVLA**ï¼šé€šè¿‡**æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢å’ŒåŒåˆ†æ”¯æ¶æ„**ï¼Œåœ¨**å•é˜¶æ®µã€ç«¯åˆ°ç«¯**çš„è®­ç»ƒä¸­**åŒæ—¶ä¸”ååŒåœ°**å­¦ä¹ å…ˆéªŒ `p(a|v)` å’ŒåéªŒ `Ï€(a|v,l)`ï¼Œå¹¶é€šè¿‡LLRç›®æ ‡ç›´æ¥ä¼˜åŒ–äºŒè€…çš„å…³ç³»ã€‚\n\n2.  **ä»â€œç¼“è§£æ•°æ®å±€é™â€åˆ°â€œæ ¹æ²»è®­ç»ƒç—…ç†â€**ï¼š\n    - **å‰ä½œBayesVLA**ï¼šä¸»è¦ç›®æ ‡æ˜¯å…‹æœæ•°æ®ä¸­è¯­è¨€å¤šæ ·æ€§çš„ä¸è¶³ã€‚\n    - **æœ¬æ–‡BayesianVLA**ï¼šæ˜ç¡®è¯Šæ–­å‡ºâ€œä¿¡æ¯åç¼©â€å’Œâ€œè§†è§‰æ·å¾„â€è¿™ä¸€æ ¹æœ¬æ€§è®­ç»ƒç—…ç†ï¼Œå¹¶å°†LLRç›®æ ‡ï¼ˆæœ€å¤§åŒ–æ¡ä»¶PMIï¼‰ä½œä¸º**ç›´æ¥çš„æ²»ç–—æ–¹æ¡ˆ**ï¼Œä»ä¼˜åŒ–åŸç†ä¸Šå¼ºåˆ¶è¯­è¨€ä¸åŠ¨ä½œçš„å…³è”ã€‚\n\n3.  **æ¶æ„åˆ›æ–°ä¸å¯æ‰©å±•æ€§**ï¼š\n    - **å‰ä½œBayesVLA**ï¼šä½¿ç”¨ç›¸å¯¹è¾ƒå°çš„CLIP+DiTæ¶æ„ï¼ŒæœªéªŒè¯åœ¨å¤§è§„æ¨¡VLMä¸Šçš„æ•ˆæœã€‚\n    - **æœ¬æ–‡BayesianVLA**ï¼šåˆ›æ–°æ€§åœ°æå‡º**æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢**ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡ã€é«˜æ•ˆã€ä¸TransformeråŸç”Ÿå…¼å®¹çš„æ¥å£è®¾è®¡ã€‚å®ƒæˆåŠŸé›†æˆåˆ°Qwen3-VL-4Bç­‰å¤§æ¨¡å‹ä¸­ï¼Œå¹¶å®ç°äº†SOTAæ€§èƒ½ï¼Œè¯æ˜äº†å…¶**å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§**ã€‚\n\n4.  **ä¿ç•™é€šç”¨èƒ½åŠ›çš„é™„åŠ ä»·å€¼**ï¼š\n    - æœ¬æ–‡å‘ç°ï¼Œç”±äºLLRç›®æ ‡å¼ºåŒ–äº†æ¨¡å‹å¯¹è¯­è¨€æŒ‡ä»¤çš„å¤„ç†å’Œä¾èµ–ï¼Œæ„å¤–åœ°èµ·åˆ°äº†**æ­£åˆ™åŒ–ä½œç”¨**ï¼Œå¸®åŠ©ä¿ç•™äº†VLMä¸»å¹²åœ¨**çº¯æ–‡æœ¬å¯¹è¯**ä¸Šçš„é€šç”¨èƒ½åŠ›ï¼Œç¼“è§£äº†VLAå¾®è°ƒä¸­å¸¸è§çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚è¿™æ˜¯å‰ä½œæœªæåŠçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼ŒBayesianVLAçš„æ ¸å¿ƒè¿›æ­¥åœ¨äºï¼šå®ƒå°†ä¸€ä¸ªè‰¯å¥½çš„ç†è®ºæ€æƒ³ï¼ˆè´å¶æ–¯åˆ†è§£ï¼‰ï¼Œè½¬åŒ–ä¸ºä¸€ä¸ªç²¾å·§ã€é«˜æ•ˆã€å¯æ‰©å±•ä¸”ç›´æŒ‡é—®é¢˜æ ¸å¿ƒçš„å·¥ç¨‹æ¡†æ¶ï¼Œå¹¶é€šè¿‡ä¸¥è°¨çš„å®éªŒè¯æ˜äº†å…¶åœ¨æå‡VLAæ¨¡å‹é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„å“è¶Šæ•ˆæœã€‚**",
            "timestamp": "2026-01-27T16:04:08.795446",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨è¯¦ç»†è§£æã€ŠBayesianVLAã€‹è®ºæ–‡æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæ–¹æ³•è§£æ\n\nè®ºæ–‡ã€ŠBayesianVLAã€‹æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­å› æ•°æ®é›†åå·®å¯¼è‡´çš„â€œè§†è§‰æ·å¾„â€é—®é¢˜ã€‚å…¶æ ¸å¿ƒæ˜¯é€šè¿‡è´å¶æ–¯åˆ†è§£ï¼Œå¼ºåˆ¶æ¨¡å‹å­¦ä¹ çœŸæ­£ä¾èµ–äºè¯­è¨€æŒ‡ä»¤çš„ç­–ç•¥ã€‚å…³é”®æŠ€æœ¯æ–¹æ³•æ€»ç»“å¦‚ä¸‹ï¼š\n\n| æ–¹æ³•ç»„ä»¶ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°/å®ç° |\n| :--- | :--- | :--- |\n| **1. é—®é¢˜å½¢å¼åŒ–ä¸ç›®æ ‡å‡½æ•°** | **æ ¸å¿ƒæ´å¯Ÿ**ï¼šæ ‡å‡†è®­ç»ƒç›®æ ‡ï¼ˆæœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ $\\log \\pi(a \\mid v, \\ell)$ï¼‰åœ¨æ•°æ®é›†å­˜åœ¨å¼ºè§†è§‰-è¯­è¨€å…³è”ï¼ˆ$p(\\ell \\mid v)$ å°–é”ï¼‰æ—¶ï¼Œä¼šå¯¼è‡´æ¡ä»¶äº’ä¿¡æ¯ $I(\\ell; a \\mid v)$ åç¼©ï¼Œæ¨¡å‹é€€åŒ–ä¸ºè§†è§‰å…ˆéªŒç­–ç•¥ $p(a \\mid v)$ã€‚<br><br>**è´å¶æ–¯åˆ†è§£**ï¼šæ ¹æ®è´å¶æ–¯å®šç†ï¼Œæœ€ä¼˜ç­–ç•¥å¯åˆ†è§£ä¸ºï¼š<br>$\\pi(a \\mid v, \\ell) = \\frac{p(\\ell \\mid a, v) \\cdot p(a \\mid v)}{p(\\ell \\mid v)}$<br>å…¶ä¸­ $p(a \\mid v)$ æ˜¯è§†è§‰å…ˆéªŒï¼Œ$p(\\ell \\mid a, v)$ æ˜¯ä¼¼ç„¶ã€‚<br><br>**æ–°ç›®æ ‡å‡½æ•°**ï¼šä¸ºé¿å…åç¼©ï¼Œæ”¹ä¸ºæœ€å¤§åŒ–**æ¡ä»¶é€ç‚¹äº’ä¿¡æ¯**ï¼š<br>$\\text{PMI}(a, \\ell \\mid v) = \\log \\frac{\\pi(a \\mid v, \\ell)}{p(a \\mid v)} = \\log \\frac{p(\\ell \\mid a, v)}{p(\\ell \\mid v)}$<br>è¿™ç­‰ä»·äºæœ€å¤§åŒ–**å¯¹æ•°ä¼¼ç„¶æ¯”**ï¼š<br>$\\mathcal{L}_{\\text{LLR}} = \\log p(\\ell \\mid a, v) - \\log p(\\ell \\mid v)$<br>è¯¥ç›®æ ‡æƒ©ç½šä»…ä»è§†è§‰ $v$ å³å¯é¢„æµ‹æŒ‡ä»¤ $\\ell$ çš„æƒ…å†µï¼Œå¥–åŠ±é‚£äº›èƒ½ä¸º $\\ell$ æä¾›é¢å¤–ä¿¡æ¯çš„åŠ¨ä½œ $a$ã€‚ | **è®­ç»ƒç›®æ ‡**ï¼š$\\mathcal{L}_{\\text{total}} = (1-\\lambda)\\mathcal{L}_{\\text{FM}}(\\psi; \\mathbf{H}_{\\mathcal{Q}}^{\\text{post}}) + \\lambda\\mathcal{L}_{\\text{FM}}(\\psi; \\mathbf{H}_{\\mathcal{Q}}^{\\text{prior}}) - \\beta\\mathcal{L}_{\\text{LLR}}$<br>**è¶…å‚æ•°**ï¼š$\\lambda$ å¹³è¡¡å…ˆéªŒä¸åéªŒåŠ¨ä½œæŸå¤±ï¼ˆè®ºæ–‡æœªç»™å‡ºå…·ä½“å€¼ï¼Œéœ€è°ƒå‚ï¼‰ï¼Œ$\\beta$ æ§åˆ¶ LLR æ­£åˆ™åŒ–å¼ºåº¦ã€‚ |\n| **2. æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢** | **è®¾è®¡ç›®çš„**ï¼šåœ¨ç»Ÿä¸€çš„ VLM æ¶æ„ä¸­å®ä¾‹åŒ–è´å¶æ–¯åˆ†è§£ï¼Œä½œä¸º VLM ä¸ä¸‹æ¸¸è¿ç»­åŠ¨ä½œå¤´ï¼ˆDiTï¼‰ä¹‹é—´çš„**ç“¶é¢ˆæ¥å£**ã€‚å®ƒå…è®¸é€šè¿‡ç®€å•çš„åºåˆ—ä½ç½®è°ƒæ•´ï¼Œç²¾ç¡®æ§åˆ¶æŸ¥è¯¢æ‰€èƒ½è®¿é—®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä»…è§†è§‰æˆ–è§†è§‰+è¯­è¨€ï¼‰ã€‚<br><br>**å®ç°æ–¹å¼**ï¼šåœ¨ VLM çš„è¯è¡¨ä¸­æ·»åŠ  $K$ ä¸ªå¯å­¦ä¹ çš„ç‰¹æ®Šä»¤ç‰Œï¼ˆTokenï¼‰ï¼Œè®°ä¸º $\\mathcal{Q} = \\{\\texttt{<\\|action\\_0\\|>}, \\dots, \\texttt{<\\|action\\_K\\|>}\\}$ã€‚è¿™äº›ä»¤ç‰Œè¢«é™„åŠ åˆ°è¾“å…¥åºåˆ—ä¸­ï¼Œ**ä»…ä½¿ç”¨**å®ƒä»¬å¯¹åº”çš„éšè—çŠ¶æ€ $\\mathbf{H}_{\\mathcal{Q}} \\in \\mathbb{R}^{K \\times D}$ æ¥æ¡ä»¶åŒ–åŠ¨ä½œç”Ÿæˆå™¨ã€‚<br><br>**ä¼˜åŠ¿**ï¼š<br>1. **è®¡ç®—æ•ˆç‡**ï¼šå°† DiT çš„æ¡ä»¶å¤„ç†å¤æ‚åº¦ä» $O(N^2)$ï¼ˆ$N$ ä¸ºæ‰€æœ‰è§†è§‰è¯­è¨€ä»¤ç‰Œæ•°ï¼‰é™ä½åˆ° $O(K^2)$ï¼ˆ$K$ ä¸ºå°çš„å¸¸æ•°ï¼‰ã€‚<br>2. **ä¿¡æ¯æ§åˆ¶**ï¼šåˆ©ç”¨è§£ç å™¨ VLM çš„å› æœæ³¨æ„åŠ›æ©ç ï¼Œé€šè¿‡æ”¹å˜ $\\mathcal{Q}$ åœ¨è¾“å…¥åºåˆ—ä¸­çš„ä½ç½®ï¼Œå³å¯å®ç°ä¿¡æ¯æµçš„ä¸¥æ ¼åˆ†ç¦»ã€‚ | **å‚æ•°**ï¼š$K = 64$ ä¸ªæŸ¥è¯¢ä»¤ç‰Œã€‚<br>**ç»´åº¦**ï¼šéšè—çŠ¶æ€ç»´åº¦ $D$ å–å†³äºéª¨å¹² VLMï¼ˆå¦‚ Qwen3-VLï¼‰ã€‚<br>**åŠ¨ä½œå¤´**ï¼šä½¿ç”¨ Diffusion Transformer ä½œä¸ºè¿ç»­åŠ¨ä½œç”Ÿæˆå™¨ã€‚ |\n| **3. åŒåˆ†æ”¯è®­ç»ƒæ¡†æ¶** | **æ ¸å¿ƒæ€æƒ³**ï¼šä½¿ç”¨å…±äº«æƒé‡çš„å•ä¸€ VLMï¼Œé€šè¿‡æ„å»ºä¸¤ç§ä¸åŒçš„è¾“å…¥åºåˆ—ï¼ŒåŒæ—¶å­¦ä¹ è§†è§‰å…ˆéªŒ $p(a \\mid v)$ å’Œè¯­è¨€åéªŒç­–ç•¥ $\\pi(a \\mid v, \\ell)$ã€‚<br><br>**å…ˆéªŒåˆ†æ”¯**ï¼š<br>è¾“å…¥åºåˆ—ï¼š$\\text{Input}_{\\text{prior}} = [v, \\mathcal{Q}, \\ell]$<br>ç”±äºå› æœæ©ç ï¼Œ$\\mathcal{Q}$ åªèƒ½çœ‹åˆ°å‰é¢çš„è§†è§‰ $v$ï¼Œæ— æ³•çœ‹åˆ°åé¢çš„è¯­è¨€ $\\ell$ã€‚å› æ­¤å…¶éšè—çŠ¶æ€ $\\mathbf{H}_{\\mathcal{Q}}^{\\text{prior}}$ ç¼–ç çº¯è§†è§‰ä¿¡æ¯ï¼Œç”¨äºé¢„æµ‹åŠ¨ä½œï¼ŒæŸå¤±ä¸º $\\mathcal{L}_{\\text{prior}}$ï¼Œå­¦ä¹  $p(a \\mid v)$ã€‚<br><br>**åéªŒåˆ†æ”¯**ï¼š<br>è¾“å…¥åºåˆ—ï¼š$\\text{Input}_{\\text{post}} = [v, \\ell, \\mathcal{Q}]$<br>æ­¤æ—¶ $\\mathcal{Q}$ èƒ½çœ‹åˆ°å‰é¢çš„ $v$ å’Œ $\\ell$ï¼Œå…¶éšè—çŠ¶æ€ $\\mathbf{H}_{\\mathcal{Q}}^{\\text{post}}$ ç¼–ç å®Œæ•´ä¸Šä¸‹æ–‡ï¼ŒæŸå¤±ä¸º $\\mathcal{L}_{\\text{main}}$ï¼Œå­¦ä¹  $\\pi(a \\mid v, \\ell)$ã€‚<br><br>**LLR ç›®æ ‡çš„å®ç°**ï¼š<br>åˆ©ç”¨ VLM çš„è¯­è¨€å»ºæ¨¡æŸå¤±ä½œä¸º $\\log p(\\ell \\mid \\dots)$ çš„ä»£ç†ã€‚<br>åœ¨å…ˆéªŒåˆ†æ”¯ä¸­ï¼Œè¯­è¨€ $\\ell$ çš„ç”Ÿæˆæ¦‚ç‡åŸºäº $[v, \\mathcal{Q}]$ï¼Œç”±äº $\\mathcal{Q}$ ç¼–ç äº†å…ˆéªŒåŠ¨ä½œä¿¡æ¯ï¼Œè¿™è¿‘ä¼¼äº $p(\\ell \\mid v, a_{\\text{prior}})$ã€‚<br>LLR æŸå¤±è®¡ç®—ä¸ºï¼š<br>$\\mathcal{L}_{\\text{LLR}} = \\log p(\\ell \\mid v, \\mathbf{H}_{\\mathcal{Q}}^{\\text{prior}}) - \\text{sg}(\\log p(\\ell \\mid v))$<br>å…¶ä¸­ $\\text{sg}(\\cdot)$ æ˜¯**åœæ­¢æ¢¯åº¦**æ“ä½œï¼Œé˜²æ­¢æ¨¡å‹é€šè¿‡é™ä½åŸºçº¿ $p(\\ell \\mid v)$ï¼ˆæŸå®³ VLM çš„é€šç”¨èƒ½åŠ›ï¼‰æ¥ç®€å•åœ°æœ€å¤§åŒ–æ¯”å€¼ã€‚ | **è®­ç»ƒæµç¨‹**ï¼šæ¯ä¸ªè®­ç»ƒæ­¥åŒæ—¶è¿›è¡Œä¸¤ä¸ªåˆ†æ”¯çš„å‰å‘ä¼ æ’­ï¼Œå…±äº« VLM æƒé‡ã€‚<br>**åŠ¨ä½œæŸå¤±**ï¼šä½¿ç”¨**æ•´æµæµåŒ¹é…**æŸå¤±ï¼š<br>$\\mathcal{L}_{\\text{FM}}(\\psi; \\mathbf{C}) = \\mathbb{E}_{t,\\mathbf{a}_0,\\mathbf{a}_1} [\\| v_{\\psi}(\\mathbf{a}_t, t, \\mathbf{C}) - (\\mathbf{a}_1 - \\mathbf{a}_0) \\|^2]$<br>å…¶ä¸­ $\\mathbf{a}_t = (1-t)\\mathbf{a}_0 + t\\mathbf{a}_1$, $t \\in [0,1]$ã€‚<br>**æ¨ç†**ï¼šä»…ä½¿ç”¨åéªŒåˆ†æ”¯ï¼Œæ— é¢å¤–è®¡ç®—å¼€é”€ã€‚ |\n| **4. æ•´ä½“è®­ç»ƒä¸æ¨ç†** | **æ€»æŸå¤±**ï¼šå¦‚ä¸Šè¿°è¡¨æ ¼æ‰€ç¤ºï¼Œç»“åˆäº†åéªŒåŠ¨ä½œæŸå¤±ã€å…ˆéªŒåŠ¨ä½œæŸå¤±å’Œ LLR æ­£åˆ™åŒ–é¡¹ã€‚<br><br>**ä¼˜åŒ–ç»†èŠ‚**ï¼šä½¿ç”¨ AdamW ä¼˜åŒ–å™¨ï¼Œåˆå§‹å­¦ä¹ ç‡ 1e-5ï¼Œä½™å¼¦é€€ç«è°ƒåº¦ã€‚é‡‡ç”¨ DeepSpeed ZeRO-2ã€æ¢¯åº¦è£å‰ªï¼ˆèŒƒæ•° 1.0ï¼‰ç­‰ç³»ç»Ÿçº§ä¼˜åŒ–ã€‚<br><br>**å…³é”®ä¼˜åŠ¿**ï¼šè¯¥æ–¹æ³•**æ— éœ€æ”¶é›†æ–°æ•°æ®**ï¼Œä»…é€šè¿‡æ”¹å˜æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç›®æ ‡ï¼Œå³å¯åœ¨å­˜åœ¨åå·®çš„ç°æœ‰æ•°æ®é›†ä¸­æ¢å¤å‡ºçœŸæ­£çš„è¯­è¨€æ¡ä»¶ç­–ç•¥ã€‚ | **ç¡¬ä»¶**ï¼š16 å— NVIDIA H100 GPUã€‚<br>**éª¨å¹²ç½‘ç»œ**ï¼šåŸºäº StarVLA æ¡†æ¶ï¼Œå®ä¾‹åŒ–ä¸º QwenGR00T æ¶æ„ï¼ˆQwen3-VL-4Bï¼‰ã€‚<br>**è®­ç»ƒæ•°æ®**ï¼šBridgeDataV2, Fractal, PhysicalAI-Robotics-GR00T-X-Embodiment-Sim ç­‰ã€‚ |\n\n### **æŠ€æœ¯åˆ›æ–°ä¸å®é™…ä»·å€¼æ€»ç»“**\n\n1.  **ç†è®ºåˆ›æ–°**ï¼šé¦–æ¬¡ä»ä¿¡æ¯è®ºè§’åº¦å°† VLA æ¨¡å‹çš„å¤±è´¥æ¨¡å¼å½¢å¼åŒ–ä¸ºâ€œä¿¡æ¯åç¼©â€ï¼Œå¹¶æå‡ºäº†åŸºäºæ¡ä»¶ PMI æœ€å¤§åŒ–çš„è´å¶æ–¯åˆ†è§£è§£å†³æ–¹æ¡ˆï¼Œä¸ºç†è§£æ¨¡å‹é€€åŒ–æä¾›äº†æ¸…æ™°çš„ç†è®ºæ¡†æ¶ã€‚\n2.  **æ¶æ„åˆ›æ–°**ï¼šæå‡ºçš„â€œæ½œåœ¨åŠ¨ä½œæŸ¥è¯¢â€æ˜¯ä¸€ç§ç®€æ´è€Œé«˜æ•ˆçš„æ¶æ„è®¾è®¡ï¼Œå®ƒä½œä¸ºä¿¡æ¯ç“¶é¢ˆå’Œå¯æ§æ¥å£ï¼Œå·§å¦™åœ°åˆ©ç”¨ VLM çš„å› æœæ³¨æ„åŠ›æœºåˆ¶å®ç°åŒåˆ†æ”¯å­¦ä¹ ï¼Œè®¡ç®—å¼€é”€ä½ã€‚\n3.  **è®­ç»ƒç­–ç•¥åˆ›æ–°**ï¼šåŒåˆ†æ”¯è®­ç»ƒä¸ LLR ç›®æ ‡ç›¸ç»“åˆï¼Œæ˜¯ä¸€ç§æœ‰æ•ˆçš„**å†…éƒ¨å»å**æ–¹æ³•ã€‚å®ƒè¿«ä½¿æ¨¡å‹æŒ–æ˜åŠ¨ä½œä¸æŒ‡ä»¤ä¹‹é—´è¶…è¶Šè§†è§‰ç›¸å…³æ€§çš„å› æœè”ç³»ï¼Œä»è€Œæé«˜äº†å¯¹æ¨¡ç³Šåœºæ™¯å’Œåˆ†å¸ƒå¤–ç¯å¢ƒçš„æ³›åŒ–èƒ½åŠ›ã€‚\n4.  **å®é™…ä»·å€¼**ï¼š\n    *   **æå‡æ€§èƒ½**ï¼šåœ¨ SimplerEnv å’Œ RoboCasa åŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶ŠåŸºçº¿ï¼Œå°¤å…¶åœ¨ OOD åœºæ™¯ä¸‹æå‡è¾¾ 11.3%ã€‚\n    *   **ä¿æŒé€šç”¨èƒ½åŠ›**ï¼šLLR ç›®æ ‡ä¸­çš„åœæ­¢æ¢¯åº¦æ“ä½œæœ‰åŠ©äºä¿ç•™éª¨å¹² VLM çš„æ–‡æœ¬å¯¹è¯èƒ½åŠ›ï¼Œ",
            "timestamp": "2026-01-27T16:05:20.054252",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡åœ¨ä¸¤ä¸ªæ ¸å¿ƒä»¿çœŸåŸºå‡†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œ**æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡å‡ä¸ºä»»åŠ¡æˆåŠŸç‡**ã€‚å…·ä½“ç»†èŠ‚å¦‚ä¸‹ï¼š\n\n| åŸºå‡†åç§° | è¯„ä»·æŒ‡æ ‡åç§° | æŒ‡æ ‡å®šä¹‰ä¸è®¡ç®—ç»†èŠ‚ | ç»Ÿè®¡æ˜¾è‘—æ€§å¤„ç† |\n| :--- | :--- | :--- | :--- |\n| **SimplerEnv** | **å¹³å‡æˆåŠŸç‡ (Avg@480)** | å¯¹æ¯ä¸ªä»»åŠ¡ï¼Œä½¿ç”¨å®˜æ–¹è¯„ä¼°è„šæœ¬è¿›è¡Œ **480 æ¬¡ç‹¬ç«‹è¯•éªŒ**ï¼Œè®¡ç®—æˆåŠŸæ‰§è¡Œä»»åŠ¡çš„æ¬¡æ•°æ¯”ä¾‹ã€‚ | é€šè¿‡å¤§é‡ç‹¬ç«‹è¯•éªŒï¼ˆ480æ¬¡ï¼‰æ¥**å‡å°‘éšæœºæ€§å½±å“**ï¼Œç¡®ä¿ç»“æœç¨³å®šå¯é ã€‚ |\n| **RoboCasa** | **å¹³å‡æˆåŠŸç‡ (Avg@50)** | å¯¹æ¯ä¸ªä»»åŠ¡ï¼Œè¿›è¡Œ **50 æ¬¡ç‹¬ç«‹è¯•éªŒ**ï¼Œè®¡ç®—å¹³å‡æˆåŠŸç‡ã€‚ | ä½¿ç”¨50æ¬¡è¯•éªŒä¿è¯**ç»Ÿè®¡æ˜¾è‘—æ€§**ï¼Œä»¥éªŒè¯æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„é²æ£’æ€§ã€‚ |\n\n**æŒ‡æ ‡æ ¸å¿ƒè§£è¯»**ï¼š\n- **ä»»åŠ¡æˆåŠŸç‡** æ˜¯æœºå™¨äººæ“ä½œé¢†åŸŸæœ€ç›´æ¥ã€æœ€å…³é”®çš„æŒ‡æ ‡ï¼Œç›´æ¥åæ˜ äº†ç­–ç•¥å®ŒæˆæŒ‡å®šæŒ‡ä»¤çš„èƒ½åŠ›ã€‚\n- ä¸¤ä¸ªåŸºå‡†å‡é‡‡ç”¨**å¤šæ¬¡ç‹¬ç«‹è¯•éªŒå–å¹³å‡**çš„æ–¹å¼ï¼Œè¿™æ˜¯ä¸ºäº†æ¶ˆé™¤ä»¿çœŸç¯å¢ƒä¸­çš„éšæœºå™ªå£°ï¼ˆå¦‚ç‰©ç†å¼•æ“éšæœºåˆå§‹åŒ–ã€åŠ¨ä½œæ‰§è¡Œå™ªå£°ç­‰ï¼‰ï¼Œä½¿æ€§èƒ½å¯¹æ¯”æ›´å…¬å¹³ã€æ›´å…·è¯´æœåŠ›ã€‚\n- è®ºæ–‡é€šè¿‡å¯¹æ¯” **BayesianVLA** ä¸ä¼—å¤šåŸºçº¿æ¨¡å‹ï¼ˆå¦‚ `Ï€0.5`ã€`Isaac-GR00T`ã€`QwenGR00T`ç­‰ï¼‰åœ¨è¯¥æŒ‡æ ‡ä¸Šçš„è¡¨ç°ï¼Œæ¥è¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚\n\n### äºŒã€ ä½¿ç”¨çš„æ•°æ®é›†\n\nè®ºæ–‡åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¸­ä½¿ç”¨äº†å¤šä¸ªå¤§è§„æ¨¡æœºå™¨äººæ“ä½œæ•°æ®é›†ï¼Œå…·ä½“ç»†èŠ‚å¦‚ä¸‹è¡¨ï¼š\n\n| æ•°æ®é›†åç§° | ç”¨é€” | å…³é”®ç‰¹ç‚¹ä¸ç»†èŠ‚ | åœ¨æœ¬æ–‡ä¸­çš„ä½œç”¨ |\n| :--- | :--- | :--- | :--- |\n| **BridgeDataV2** (Walke et al., 2023) | **è®­ç»ƒ & åŠ¨æœºåˆ†æ** | - **ç‰¹ç‚¹**ï¼šå¤§è§„æ¨¡ã€â€œé‡å¤–â€é‡‡é›†çš„çœŸå®æœºå™¨äººæ•°æ®ï¼Œåœºæ™¯å’Œä»»åŠ¡**å¤šæ ·åŒ–**ã€‚<br>- **ç»†èŠ‚**ï¼šåŒ…å«å¤šç§æœºå™¨äººå¹³å°åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­çš„æ“ä½œè½¨è¿¹ã€‚ | 1. **è®­ç»ƒ**ï¼šç”¨äºåœ¨SimplerEnvåŸºå‡†ä¸Šå¾®è°ƒæ¨¡å‹ã€‚<br>2. **åŠ¨æœºå®éªŒ**ï¼šç”¨äºå±•ç¤ºå³ä½¿åœ¨å¤šæ ·æ•°æ®ä¸­ï¼Œæ ‡å‡†æ¨¡å‹ä¹Ÿä¼šè¿‡æ‹Ÿåˆåˆ°**è§†è§‰æ·å¾„**ï¼ˆå¦‚ç‰¹å®šèƒŒæ™¯ã€å…‰ç…§ï¼‰ï¼Œå¯¼è‡´OODæ³›åŒ–å¤±è´¥ã€‚ |\n| **Fractal** (Brohan et al., 2022) | **è®­ç»ƒ** | - **ç‰¹ç‚¹**ï¼šå¤§è§„æ¨¡ä»¿çœŸæ•°æ®ï¼Œæ˜¯RT-1æ•°æ®é›†çš„ä¸€éƒ¨åˆ†ï¼ŒåŒ…å«ä¸°å¯Œçš„æ¡Œé¢æ“ä½œä»»åŠ¡ã€‚ | ä¸BridgeDataV2**ç»„åˆä½¿ç”¨**ï¼Œåœ¨SimplerEnvåŸºå‡†ä¸Šè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæä¾›æ›´ä¸°å¯Œçš„è®­ç»ƒæ ·æœ¬ã€‚ |\n| **PhysicalAI-Robotics-GR00T-X-Embodiment-Sim** (Bjorck et al., 2025b) çš„ **Humanoid Robot Tabletop Manipulation** å­é›† | **è®­ç»ƒ** | - **ç‰¹ç‚¹**ï¼šé’ˆå¯¹ç±»äººæœºå™¨äººï¼ˆGR1ï¼‰çš„æ¡Œé¢æ“ä½œä»¿çœŸæ•°æ®ï¼Œæ˜¯GR00Té¡¹ç›®çš„ä¸€éƒ¨åˆ†ã€‚ | ä¸“é—¨ç”¨äºåœ¨ **RoboCasa** åŸºå‡†ä¸Šè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œç¡®ä¿ä»»åŠ¡å’Œæœºå™¨äººå½¢æ€åŒ¹é…ã€‚ |\n| **LIBERO** (Liu et al., 2023) | **åŠ¨æœºåˆ†æ** | - **ç‰¹ç‚¹**ï¼šç³»ç»Ÿæ€§ç ”ç©¶ç»ˆèº«å­¦ä¹ çš„åŸºå‡†ï¼ŒåŒ…å«å››ä¸ªå­é›†ï¼ˆSpatial, Object, Long, **Goal**ï¼‰ã€‚<br>- **å…³é”®ç»†èŠ‚**ï¼š**Goalå­é›†**å…·æœ‰**å†…åœ¨æ¨¡ç³Šæ€§**â€”â€”åŒä¸€è§†è§‰åœºæ™¯å¯¹åº”å¤šä¸ªå¯èƒ½ä»»åŠ¡ã€‚ | ç”¨äº**å…³é”®è¯Šæ–­å®éªŒ**ï¼šè¯æ˜åœ¨ä»»åŠ¡æ˜ç¡®çš„å­é›†ä¸Šï¼Œè§†è§‰ä»…ç”¨æ¨¡å‹è¡¨ç°æ¥è¿‘å®Œæ•´æ¨¡å‹ï¼›è€Œåœ¨**æ¨¡ç³Šçš„Goalå­é›†**ä¸Šï¼Œè§†è§‰ä»…ç”¨æ¨¡å‹æ€§èƒ½æš´è·Œï¼Œæ­ç¤ºäº†æ ‡å‡†æ¨¡å‹å¿½ç•¥è¯­è¨€æŒ‡ä»¤çš„æœ¬è´¨é—®é¢˜ã€‚ |\n\n### ä¸‰ã€ è¯„ä¼°åŸºå‡†ï¼ˆæµ‹è¯•é›†ï¼‰\n\næ¨¡å‹åœ¨ä»¥ä¸‹ä¸¤ä¸ªä»¿çœŸåŸºå‡†ä¸Šè¿›è¡Œæœ€ç»ˆæ€§èƒ½è¯„ä¼°ï¼š\n\n| åŸºå‡†åç§° | è¯„ä¼°ä»»åŠ¡ä¸¾ä¾‹ | ç¯å¢ƒä¸ä»»åŠ¡ç‰¹ç‚¹ | è¯„ä¼°ç›®çš„ |\n| :--- | :--- | :--- | :--- |\n| **SimplerEnv** (Li et al., 2024c) | 1. æŠŠå‹ºå­æ”¾åˆ°æ¯›å·¾ä¸Š<br>2. æŠŠèƒ¡èåœæ”¾åˆ°ç›˜å­ä¸Š<br>3. å°†ç»¿è‰²ç§¯æœ¨å †å åˆ°é»„è‰²ç§¯æœ¨ä¸Š<br>4. æŠŠèŒ„å­æ”¾è¿›é»„è‰²ç¯®å­é‡Œ | - **ç‰¹ç‚¹**ï¼š**è§†è§‰åˆ†å¸ƒå¤–ï¼ˆOODï¼‰** çš„ä»¿çœŸç¯å¢ƒã€‚è®­ç»ƒæ•°æ®ï¼ˆBridgeV2, Fractalï¼‰æ¥è‡ªçœŸå®æˆ–ä¸åŒä»¿çœŸç¯å¢ƒï¼Œè€Œæµ‹è¯•åœ¨æ­¤ç‰¹å®šä»¿çœŸä¸­è¿›è¡Œã€‚<br>- **æŒ‘æˆ˜**ï¼šæµ‹è¯•åœºæ™¯çš„**è§†è§‰å¤–è§‚ã€çº¹ç†ã€å…‰ç…§**ä¸è®­ç»ƒæ•°æ®æœ‰æ˜¾è‘—å·®å¼‚ã€‚ | **æ ¸å¿ƒè¯„ä¼°OODæ³›åŒ–èƒ½åŠ›**ã€‚æ—¨åœ¨æ£€éªŒæ¨¡å‹æ˜¯å¦çœŸæ­£ç†è§£äº†è¯­è¨€æŒ‡ä»¤ï¼Œè€Œéä»…ä»…è®°å¿†äº†è®­ç»ƒæ•°æ®ä¸­çš„è§†è§‰å…³è”ã€‚BayesianVLAåœ¨æ­¤è·å¾—**11.3%** çš„æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†å…¶æ‰“ç ´â€œè§†è§‰æ·å¾„â€çš„æœ‰æ•ˆæ€§ã€‚ |\n| **RoboCasa** (Nasiriany et al., 2024) GR1 Tabletop Benchmark | 1. PnP Bottle To Cabinet Close<br>2. PnP Can To Drawer Close<br>3. PnP Novel From Placemat To Plate<br>... (å…±24ä¸ªä»»åŠ¡) | - **ç‰¹ç‚¹**ï¼š**å¤æ‚ã€å¤šæ ·åŒ–**çš„æ¡Œé¢æ“ä½œä»»åŠ¡ï¼Œæ¶‰åŠå¸¦å…³èŠ‚çš„ç‰©ä½“ï¼ˆæ©±æŸœã€æŠ½å±‰ï¼‰ã€ç”µå™¨ï¼ˆå¾®æ³¢ç‚‰ï¼‰åŠå¤šç§å‡ ä½•å½¢çŠ¶çš„å®¹å™¨ã€‚<br>- **æŒ‘æˆ˜**ï¼šä»»åŠ¡å¤æ‚ï¼Œéœ€è¦ç²¾ç¡®çš„ç©ºé—´ç†è§£å’Œä¸ç‰©ä½“çš„äº¤äº’ã€‚ | **è¯„ä¼°åœ¨å¤æ‚ã€åŒåˆ†å¸ƒï¼ˆIDï¼‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½**ã€‚å°½ç®¡æ˜¯IDæµ‹è¯•ï¼Œä½†è®ºæ–‡å‘ç°è¯¥åŸºå‡†å­˜åœ¨ä¸¥é‡çš„â€œè§†è§‰æ·å¾„â€é—®é¢˜ï¼ˆVisionOnlyåŸºçº¿æˆåŠŸç‡é«˜è¾¾44.7%ï¼‰ã€‚BayesianVLAåœ¨æ­¤ä»å–å¾—SOTAï¼Œè¯æ˜å…¶èƒ½è¿«ä½¿æ¨¡å‹åˆ©ç”¨è¯­è¨€ä¿¡æ¯æ¥è§£å†³å¤æ‚ä»»åŠ¡ã€‚ |\n\n### æ€»ç»“\næœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªä¸¥è°¨çš„è¯„ä¼°ä½“ç³»ï¼š\n1.  **æŒ‡æ ‡**ï¼šé‡‡ç”¨**ä»»åŠ¡æˆåŠŸç‡**è¿™ä¸€æ ¸å¿ƒæŒ‡æ ‡ï¼Œå¹¶é€šè¿‡å¤§é‡é‡å¤å®éªŒç¡®ä¿ç»“æœå¯é æ€§ã€‚\n2.  **æ•°æ®**ï¼šä½¿ç”¨**å¤šæ ·åŒ–ã€å¤§è§„æ¨¡**çš„å…¬å¼€æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶åˆ©ç”¨**LIBERO**è¿›è¡Œæ·±å…¥çš„ç—…ç†å­¦åˆ†æã€‚\n3.  **åŸºå‡†**ï¼šåœ¨ **SimplerEnv (OOD)** å’Œ **RoboCasa (å¤æ‚ID)** ä¸¤ä¸ªäº’è¡¥çš„åŸºå‡†ä¸Šè¿›è¡Œå…¨é¢æµ‹è¯•ï¼Œåˆ†åˆ«éªŒè¯æ¨¡å‹çš„**æ³›åŒ–èƒ½åŠ›**å’Œ**å¤æ‚ä»»åŠ¡è§£å†³èƒ½åŠ›**ã€‚\n\nè¿™ç§è¯„ä¼°è®¾è®¡æœ‰åŠ›åœ°æ”¯æ’‘äº†è®ºæ–‡çš„æ ¸å¿ƒè®ºç‚¹ï¼š**BayesianVLA é€šè¿‡è´å¶æ–¯åˆ†è§£å’Œæœ€å¤§åŒ–äº’ä¿¡æ¯ï¼Œæœ‰æ•ˆç¼“è§£äº†æ•°æ®åå·®å¯¼è‡´çš„â€œè§†è§‰æ·å¾„â€é—®é¢˜ï¼Œä»è€Œåœ¨ä¿æŒè¯­è¨€ç†è§£èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†ç­–ç•¥çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚**",
            "timestamp": "2026-01-27T16:06:12.793803",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ä¸¤ä¸ªä¸»æµçš„æœºå™¨äººæ“ä½œä»¿çœŸåŸºå‡†ä¸Šè¿›è¡Œäº†ç³»ç»Ÿå®éªŒï¼š\n\n1.  **SimplerEnv** (Li et al., 2024c)\n    *   **è®­ç»ƒæ•°æ®**ï¼šä½¿ç”¨Open X-Embodiment (OXE)æ•°æ®é›†çš„ä¸¤ä¸ªå¤§è§„æ¨¡å­é›†ï¼š\n        *   **BridgeDataV2** (Walke et al., 2023)ï¼šåŒ…å«å¤šæ ·åŒ–ã€çœŸå®ä¸–ç•Œåœºæ™¯çš„æœºå™¨äººæ“ä½œè½¨è¿¹ã€‚\n        *   **Fractal** (Brohan et al., 2022)ï¼šå¤§è§„æ¨¡æœºå™¨äººæ¼”ç¤ºæ•°æ®é›†ã€‚\n    *   **è¯„ä¼°ä»»åŠ¡**ï¼šåŒ…å«4ä¸ªå…·ä½“çš„æ“ä½œä»»åŠ¡ï¼š\n        *   `Put spoon on towel`\n        *   `Put carrot on plate`\n        *   `Stack green block on yellow block`\n        *   `Put eggplant in yellow basket`\n\n2.  **RoboCasa** (Nasiriany et al., 2024)\n    *   **è®­ç»ƒæ•°æ®**ï¼šä½¿ç”¨ **PhysicalAI-Robotics-GR00T-X-Embodiment-Sim** æ•°æ®é›†ä¸­çš„ **Humanoid Robot Tabletop Manipulation** å­é›† (Bjorck et al., 2025b)ã€‚\n    *   **è¯„ä¼°ä»»åŠ¡**ï¼šGR1æ¡Œé¢æ“ä½œåŸºå‡†ï¼ŒåŒ…å« **24ä¸ª** å¤šæ ·åŒ–çš„æ“ä½œä»»åŠ¡ï¼Œæ¶‰åŠä¸é“°æ¥ç‰©ä½“ï¼ˆå¦‚æ©±æŸœã€æŠ½å±‰ï¼‰å’Œå¤æ‚å‡ ä½•ä½“çš„äº¤äº’ï¼ˆä¾‹å¦‚ `PnP Bottle To Cabinet Close`, `PnP Can To Drawer Close`ï¼‰ã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨çš„æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡æ˜¯ **ä»»åŠ¡æˆåŠŸç‡ (Success Rate)**ã€‚\n\n*   **SimplerEnv**ï¼šå¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œ **480æ¬¡** ç‹¬ç«‹è¯•éªŒ (`Avg@480`)ï¼ŒæŠ¥å‘Šå¹³å‡æˆåŠŸç‡ã€‚\n*   **RoboCasa**ï¼šå¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œ **50æ¬¡** ç‹¬ç«‹è¯•éªŒ (`Avg@50`)ï¼ŒæŠ¥å‘Šå¹³å‡æˆåŠŸç‡ã€‚\n*   **å¯¹æ¯”åŸºçº¿**ï¼šè®ºæ–‡ä¸å½“å‰æœ€å…ˆè¿›çš„VLAæ¨¡å‹è¿›è¡Œäº†å…¨é¢å¯¹æ¯”ï¼ŒåŒ…æ‹¬ `RT-1-X`, `Octo`, `OpenVLA`, `Ï€0`ç³»åˆ—, `GR00T`ç³»åˆ—, `QwenGR00T`ç­‰ã€‚\n\n### ä¸‰ã€ å…³é”®æ€§èƒ½æå‡\n**BayesianVLA** åœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ³›åŒ–æ–¹é¢ã€‚\n\n1.  **åœ¨ SimplerEnv ä¸Šçš„æ€§èƒ½** (è¡¨1)\n    *   **å¹³å‡æˆåŠŸç‡**ï¼š`BayesianVLA` è¾¾åˆ° **66.5%**ã€‚\n    *   **æ ¸å¿ƒå¯¹æ¯”**ï¼šç›¸æ¯”å…¶ç›´æ¥åŸºçº¿ **QwenGR00T (55.2%)**ï¼Œå–å¾—äº† **+11.3%** çš„ç»å¯¹æå‡ã€‚è¿™è¯æ˜äº†æ€§èƒ½å¢ç›Šæºäºè®ºæ–‡æå‡ºçš„è´å¶æ–¯åˆ†è§£æ–¹æ³•ï¼Œè€ŒéåŸºç¡€æ¶æ„ã€‚\n    *   **ä»»åŠ¡çº§æå‡**ï¼šåœ¨éœ€è¦ç²¾ç¡®ç‰©ä½“è¯†åˆ«çš„ä»»åŠ¡ä¸Šæå‡å°¤ä¸ºæ˜æ˜¾ï¼Œä¾‹å¦‚ï¼š\n        *   `Put Carrot on Plate`: **+13.8%** (ä»50.0%æå‡è‡³63.8%)\n        *   `Put Eggplant in Yellow Basket`: **+15.0%** (ä»64.2%æå‡è‡³79.2%)\n    *   **è¶…è¶Šå…¶ä»–SOTA**ï¼š`BayesianVLA` çš„è¡¨ç°è¶…è¿‡äº†å…¶ä»–å¼ºåŠ²ç«äº‰è€…ï¼Œå¦‚ `Ï€0.5` (57.1%) å’Œ `Isaac-GR00T-N1.6-Bridge` (57.1%)ã€‚\n\n2.  **åœ¨ RoboCasa ä¸Šçš„æ€§èƒ½** (è¡¨2)\n    *   **å¹³å‡æˆåŠŸç‡**ï¼š`BayesianVLA` è¾¾åˆ° **50.4%**ï¼Œåˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚\n    *   **æ ¸å¿ƒå‘ç°**ï¼šå®éªŒè¯å®äº†â€œè§†è§‰æ·å¾„â€é—®é¢˜çš„æ™®éæ€§ã€‚ä»…ä½¿ç”¨è§†è§‰çš„åŸºçº¿æ¨¡å‹(`VisionOnly`)æˆåŠŸç‡é«˜è¾¾ **44.7%**ï¼Œä¸æ ‡å‡† `QwenGR00T` åŸºçº¿ (47.8%) å·®è·å¾ˆå°ï¼Œè¯´æ˜è®¸å¤šä»»åŠ¡ä»…å‡­è§†è§‰çº¿ç´¢å³å¯è§£å†³ã€‚\n    *   **æ–¹æ³•æœ‰æ•ˆæ€§**ï¼š`BayesianVLA` æˆåŠŸæ‰“ç ´äº†è¿™ä¸€æ€§èƒ½å¤©èŠ±æ¿ï¼Œè¶…è¶Šäº†æ‰€æœ‰å¯¹æ¯”åŸºçº¿ï¼ŒåŒ…æ‹¬ `QwenOFT` (48.8%) å’Œ `Isaac-GR00T N1.6` (47.6%)ã€‚\n    *   **å…³é”®ä»»åŠ¡æå‡**ï¼šåœ¨è§†è§‰æ·å¾„å¤±æ•ˆçš„ä»»åŠ¡ä¸Šï¼Œ`BayesianVLA` ä¼˜åŠ¿æ˜æ˜¾ã€‚ä¾‹å¦‚ï¼Œåœ¨ `PnP Novel From Placemat To Plate` ä»»åŠ¡ä¸­ï¼ŒæˆåŠŸç‡é«˜è¾¾ **70.0%**ï¼Œæ˜¾è‘—ä¼˜äºè§†è§‰åŸºçº¿ (34.0%) å’Œæ ‡å‡†åŸºçº¿ (48.0%)ã€‚\n\n### å››ã€ æ€»ç»“\n**æŠ€æœ¯åˆ›æ–°ä»·å€¼**ï¼š`BayesianVLA` é€šè¿‡å¼•å…¥**æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢**å’Œ**åŒåˆ†æ”¯è´å¶æ–¯ç›®æ ‡**ï¼Œæœ‰æ•ˆè§£å†³äº†VLAæ¨¡å‹è®­ç»ƒä¸­çš„â€œè§†è§‰æ·å¾„â€å’Œâ€œä¿¡æ¯åç¼©â€é—®é¢˜ã€‚å…¶å®éªŒç»“æœåœ¨**ä¸ä¾èµ–æ–°æ•°æ®**çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹è¯­è¨€æŒ‡ä»¤çš„éµå¾ªèƒ½åŠ›å’Œ**åˆ†å¸ƒå¤–æ³›åŒ–æ€§èƒ½**ï¼ˆSimplerEnvä¸Š+11.3%ï¼‰ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä½¿æœºå™¨äººç­–ç•¥çœŸæ­£ç†è§£å’Œæ‰§è¡Œäººç±»æ„å›¾æ–¹é¢çš„å®é™…ä»·å€¼ã€‚",
            "timestamp": "2026-01-27T16:06:51.620814",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„æ·±å…¥é˜…è¯»ï¼Œä»¥ä¸‹æ˜¯**BayesianVLA**æ–¹æ³•çš„ä¸»è¦å±€é™æ€§ï¼š\n\n### 1. **è®­ç»ƒé˜¶æ®µçš„è®¡ç®—å¼€é”€å¢åŠ **\n- **é—®é¢˜æè¿°**ï¼š è®ºæ–‡æå‡ºçš„åŒåˆ†æ”¯æ¶æ„ï¼ˆå…ˆéªŒåˆ†æ”¯ä¸åéªŒåˆ†æ”¯ï¼‰åœ¨è®­ç»ƒæ—¶éœ€è¦åŒæ—¶è®¡ç®—ä¸¤ä¸ªåˆ†æ”¯ï¼Œè¿™**ç†è®ºä¸Šå¢åŠ äº†æ¯æ¬¡è¿­ä»£çš„è®¡ç®—æˆæœ¬**ã€‚\n- **å…·ä½“å½±å“**ï¼š è™½ç„¶è®ºæ–‡æŒ‡å‡ºé€šè¿‡**å‰ç¼€é¢„å¡«å……ç­–ç•¥**ï¼ˆPrefill Strategyï¼‰å¯ä»¥å¤ç”¨è§†è§‰ç¼–ç å™¨çš„è¾“å‡ºï¼Œä»è€Œå°†å®é™…å¢åŠ çš„æ—¶é—´æ§åˆ¶åœ¨å¯æ¥å—èŒƒå›´å†…ï¼Œä½†è¿™ä»ç„¶æ˜¯ä¸€ä¸ªé¢å¤–çš„å·¥ç¨‹ä¼˜åŒ–è´Ÿæ‹…ï¼Œå¹¶ä¸”å¯èƒ½å½±å“å¤§è§„æ¨¡è®­ç»ƒçš„æ•ˆç‡ã€‚\n\n### 2. **è§†è§‰-è¯­è¨€å¯¹è¯èƒ½åŠ›çš„æ½œåœ¨é€€åŒ–**\n- **é—®é¢˜æè¿°**ï¼š è®ºæ–‡æ‰¿è®¤ï¼Œå°½ç®¡**BayesianVLA**æˆåŠŸä¿ç•™äº†VLMéª¨å¹²çš„**çº¯æ–‡æœ¬å¯¹è¯èƒ½åŠ›**ï¼Œä½†å…¶**è§†è§‰-è¯­è¨€å¯¹è¯èƒ½åŠ›**ï¼ˆå¦‚å›¾åƒ+æ–‡æœ¬è¾“å…¥çš„å¤šæ¨¡æ€æ¨ç†ï¼‰åœ¨VLAè®­ç»ƒåä»å¯èƒ½é€€åŒ–ã€‚\n- **æ ¹æœ¬åŸå› **ï¼š è®ºæ–‡å‡è®¾è¿™æ˜¯å› ä¸º**è§†è§‰ç¼–ç å™¨**å’Œ**å¤šæ¨¡æ€èåˆ/æŠ•å½±å±‚**å¿…é¡»ä¸ºæ§åˆ¶ä»»åŠ¡è¿›è¡Œé€‚é…ï¼Œè¿™å¯èƒ½å¯¼è‡´è§†è§‰è¡¨å¾åç¦»é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€å¯¹é½æµå½¢ã€‚è¿™ç§â€œä¸“ä¸šåŒ–â€å¯¹äºä¸ºåŠ¨ä½œä¼˜åŒ–çš„å…·èº«æ™ºèƒ½ä½“æ¥è¯´æ˜¯é¢„æœŸçš„ï¼Œä½†ç‰ºç‰²äº†éƒ¨åˆ†é€šç”¨çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚\n\n### 3. **å®éªŒéªŒè¯çš„å¹¿åº¦æœ‰å¾…æ‰©å±•**\n- **é—®é¢˜æè¿°**ï¼š å½“å‰å·¥ä½œä¸»è¦åœ¨**SimplerEnv**å’Œ**RoboCasa**ä¸¤ä¸ªæ¨¡æ‹ŸåŸºå‡†ä¸Šè¿›è¡ŒéªŒè¯ã€‚è®ºæ–‡åœ¨â€œå±€é™æ€§ä¸æœªæ¥å·¥ä½œâ€éƒ¨åˆ†æ˜ç¡®æŒ‡å‡ºï¼Œè®¡åˆ’åœ¨åç»­ç‰ˆæœ¬ä¸­æä¾›æ›´å…¨é¢çš„è¯„ä¼°ã€‚\n- **å…·ä½“ç¼ºå¤±**ï¼š\n    - **æ›´å¤šåŸºå‡†**ï¼š å¦‚RoboTwinã€LIBEROç­‰ã€‚\n    - **çœŸå®ä¸–ç•Œæœºå™¨äººå®éªŒ**ï¼š ç›®å‰æ‰€æœ‰å®éªŒå‡åœ¨ä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œï¼Œç¼ºä¹ç‰©ç†ä¸–ç•Œçš„éªŒè¯ã€‚\n    - **æ›´å¤§è§„æ¨¡çš„åŸºç¡€æ¨¡å‹**ï¼š å®éªŒåŸºäºQwen3-VL-4Bï¼Œæœªæ‰©å±•åˆ°å¦‚8Bç­‰æ›´å¤§å‚æ•°é‡çš„æ¨¡å‹ã€‚\n    - **æ›´è¯¦å°½çš„æ¶ˆèç ”ç©¶**ï¼š å¯¹è´å¶æ–¯åˆ†è§£æ¡†æ¶ä¸­æ¯ä¸ªç»„ä»¶å’Œè¶…å‚æ•°çš„è´¡çŒ®åˆ†æå°šä¸å®Œæ•´ã€‚\n\n### 4. **æ–¹æ³•å¯¹æ•°æ®åå·®çš„ç¼“è§£ç¨‹åº¦å­˜åœ¨ä¸Šé™**\n- **é—®é¢˜æè¿°**ï¼š **BayesianVLA**çš„æ ¸å¿ƒæ˜¯é€šè¿‡ä¼˜åŒ–ç›®æ ‡ï¼ˆLLRï¼‰æ¥â€œæƒ©ç½šâ€è§†è§‰æ·å¾„ï¼Œè¿«ä½¿æ¨¡å‹å…³æ³¨è¯­è¨€æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•**æœ¬è´¨ä¸Šæ˜¯åœ¨æœ‰åçš„æ•°æ®é›†ä¸Šè¿›è¡Œæ­£åˆ™åŒ–**ã€‚\n- **æ½œåœ¨é£é™©**ï¼š å¦‚æœæ•°æ®é›†çš„åå·®æå…¶ä¸¥é‡ï¼ˆå³ `H(â„“âˆ£v) â‰ˆ 0`ï¼‰ï¼Œæ¨¡å‹å¯èƒ½ä»ç„¶éš¾ä»¥å­¦ä¹ åˆ°çœŸæ­£é²æ£’çš„è¯­è¨€-åŠ¨ä½œ groundingï¼Œå› ä¸ºä¼˜åŒ–ä¿¡å·æœ¬èº«å¯èƒ½å¾ˆå¼±ã€‚æ–¹æ³•çš„æ•ˆæœé«˜åº¦ä¾èµ–äºLLRç›®æ ‡èƒ½å¦æä¾›è¶³å¤Ÿå¼ºçš„æ¢¯åº¦ä¿¡å·æ¥å…‹æœæ•°æ®åå·®ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nåŸºäºè®ºæ–‡çš„â€œè®¨è®ºâ€å’Œâ€œå±€é™æ€§ä¸æœªæ¥å·¥ä½œâ€éƒ¨åˆ†ï¼Œæœªæ¥ç ”ç©¶å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹å‘å±•å¼€ï¼š\n\n### 1. **æ•°æ®ç­–ç•¥çš„æ ¹æœ¬æ€§å˜é©**\n- **æ”¶é›†â€œæ¨¡ç³Šæ€§â€æ•°æ®**ï¼š ä¸»åŠ¨åœ¨**æ— æ³•ä»…ä»åˆå§‹è§‚å¯Ÿæ¨æ–­ä»»åŠ¡**çš„åœºæ™¯ä¸­æ”¶é›†æ•°æ®ã€‚ä¾‹å¦‚ï¼ŒåŒä¸€è§†è§‰åœºæ™¯å¯¹åº”å¤šä¸ªåˆç†ä»»åŠ¡ï¼Œè¿«ä½¿æ¨¡å‹å¿…é¡»ä¾èµ–è¯­è¨€æŒ‡ä»¤è¿›è¡Œæ¶ˆæ­§ã€‚è¿™èƒ½è‡ªç„¶å¢åŠ è¯­è¨€çš„æ¡ä»¶ç†µ `H(â„“âˆ£v)`ï¼Œä»æ ¹æœ¬ä¸Šç¼“è§£ä¿¡æ¯åå¡Œã€‚\n- **åˆ©ç”¨äººç±»æ´»åŠ¨æ•°æ®**ï¼š èåˆå¤§è§„æ¨¡äººç±»è§†é¢‘æ•°æ®ï¼ˆå¦‚HRDT, METISï¼‰ã€‚äººç±»æ´»åŠ¨å…·æœ‰å†…åœ¨çš„å¤šæ¨¡æ€å’Œä¸Šä¸‹æ–‡ä¾èµ–æ€§ï¼ŒåŒä¸€ç¯å¢ƒä¸­çš„è¡Œä¸ºå¤šæ ·æ€§æ›´é«˜ï¼Œå¯èƒ½æä¾›æ›´ä¸°å¯Œã€æ›´å°‘ç¡®å®šæ€§çš„ `p(â„“âˆ£v)` åˆ†å¸ƒï¼Œæœ‰åŠ©äºç¼“è§£æœºå™¨äººä¸“ç”¨æ•°æ®ä¸­çš„ä¿¡æ¯åå¡Œã€‚\n\n### 2. **æ¶æ„ä¸ç®—æ³•æ‰©å±•**\n- **æ‰©å±•åˆ°ä¸–ç•Œæ¨¡å‹æ¡†æ¶**ï¼š è®ºæ–‡æŒ‡å‡ºï¼ŒåŸºäºä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelsï¼‰çš„VLAæ–¹æ³•ï¼ˆå¦‚F1-VLA, Mantisï¼‰æä¾›äº†å¦ä¸€ç§è´å¶æ–¯å…¬å¼çš„å®ä¾‹åŒ–ã€‚æœªæ¥å¯ä»¥å°†**BayesianVLA**çš„æ ¸å¿ƒæ€æƒ³ï¼ˆæœ€å¤§åŒ–åŠ¨ä½œä¸æŒ‡ä»¤çš„äº’ä¿¡æ¯ï¼‰ä¸**ä¸–ç•Œæ¨¡å‹çš„å‰å‘é¢„æµ‹èƒ½åŠ›**ç›¸ç»“åˆã€‚ä¸–ç•Œæ¨¡å‹å¯¹åŠ¨ä½œ `a` æ•æ„Ÿï¼Œå…¶é¢„æµ‹åˆ†å¸ƒä¸°å¯Œï¼Œå¯èƒ½èƒ½æ›´è‡ªç„¶åœ°é¿å…åˆ†å­ï¼ˆä¼¼ç„¶ï¼‰åç¼©åˆ°åˆ†æ¯ï¼ˆè¾¹ç¼˜ä¼¼ç„¶ï¼‰ã€‚\n- **æ¢ç´¢æ›´é«˜æ•ˆçš„è®­ç»ƒæœºåˆ¶**ï¼š è®¾è®¡æ›´è½»é‡åŒ–çš„åŒåˆ†æ”¯è®­ç»ƒç­–ç•¥ï¼Œæˆ–ç ”ç©¶å•åˆ†æ”¯ä½†èƒ½éšå¼å»ºæ¨¡å…ˆéªŒä¸åéªŒå·®å¼‚çš„æ–¹æ³•ï¼Œä»¥è¿›ä¸€æ­¥é™ä½è®­ç»ƒå¼€é”€ã€‚\n- **å‚æ•°è§„æ¨¡æ‰©å±•**ï¼š å°†æ¡†æ¶åº”ç”¨åˆ°å‚æ•°é‡æ›´å¤§çš„VLMéª¨å¹²ï¼ˆå¦‚Qwen3-VL-8Bï¼‰ä¸Šï¼ŒéªŒè¯å…¶å¯æ‰©å±•æ€§å’Œæ€§èƒ½æå‡çš„è§„å¾‹ã€‚\n\n### 3. **èƒ½åŠ›ä¿ç•™ä¸æƒè¡¡çš„ä¼˜åŒ–**\n- **è§£è€¦è§†è§‰è¡¨å¾å­¦ä¹ **ï¼š ç ”ç©¶å¦‚ä½•æ›´å¥½åœ°**è§£è€¦ç”¨äºæ§åˆ¶çš„è§†è§‰ç‰¹å¾**å’Œ**ç”¨äºå¤šæ¨¡æ€ç†è§£çš„è§†è§‰ç‰¹å¾**ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡å¼•å…¥é€‚é…å™¨ï¼ˆAdapterï¼‰ã€æç¤ºè¯ï¼ˆPromptï¼‰æˆ–ä¸“é—¨çš„æŠ•å½±ç½‘ç»œï¼Œåœ¨å¾®è°ƒæ—¶å°½å¯èƒ½å°‘åœ°ç ´åé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€å¯¹é½ï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™é€šç”¨çš„è§†è§‰-è¯­è¨€å¯¹è¯èƒ½åŠ›ã€‚\n- **å¤šä»»åŠ¡ååŒè®­ç»ƒ**ï¼š æ¢ç´¢åœ¨VLAè®­ç»ƒä¸­è”åˆä¼˜åŒ–æ§åˆ¶æŸå¤±å’Œè¯­è¨€å»ºæ¨¡/è§†è§‰é—®ç­”æŸå¤±ï¼Œä»¥æ˜ç¡®åœ°æ­£åˆ™åŒ–å’Œä¿ç•™éª¨å¹²æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚\n\n### 4. **æ›´å…¨é¢ä¸ä¸¥æ ¼çš„è¯„ä¼°**\n- **è·¨åŸºå‡†ä¸çœŸå®ä¸–ç•ŒéªŒè¯**ï¼š åœ¨æ›´å¤šæ ·åŒ–çš„ä»¿çœŸåŸºå‡†ï¼ˆå¦‚LIBERO-Goalçš„æ¨¡ç³Šåœºæ™¯ï¼‰å’ŒçœŸå®çš„ç‰©ç†æœºå™¨äººå¹³å°ä¸Šè¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œè¯æ˜æ–¹æ³•çš„æ³›åŒ–æ€§å’Œå®ç”¨æ€§ã€‚\n- **æ·±å…¥çš„è¯Šæ–­æ€§åˆ†æ**ï¼š\n    - **å®šé‡æµ‹é‡ä¿¡æ¯åå¡Œ**ï¼š åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›´æ¥ç›‘æµ‹æ¡ä»¶äº’ä¿¡æ¯ `I(â„“; aâˆ£v)` çš„å˜åŒ–ï¼Œæ›´ç›´è§‚åœ°å±•ç¤ºæ–¹æ³•å¦‚ä½•ç¼“è§£è¯¥é—®é¢˜ã€‚\n    - **æ¶ˆèç ”ç©¶**ï¼š ç³»ç»Ÿåˆ†ææŸå¤±å‡½æ•°ä¸­å¹³è¡¡å› å­ `Î»` å’Œ `Î²`ã€æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢çš„æ•°é‡ `K`ã€ä»¥åŠåŒåˆ†æ”¯çš„å…·ä½“å®ç°æ–¹å¼å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“ã€‚\n\n### 5. **ç†è®ºåŸºç¡€çš„æ·±åŒ–**\n- **ä¿¡æ¯è®ºè§†è§’çš„è¿›ä¸€æ­¥æ¢ç´¢**ï¼š æ›´æ·±å…¥åœ°åˆ†æåœ¨æ•°æ®åå·®å­˜åœ¨ä¸‹ï¼Œä¸åŒè®­ç»ƒç›®æ ‡ï¼ˆå¦‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡ vs. äº’ä¿¡æ¯æœ€å¤§åŒ–ï¼‰çš„æ”¶æ•›æ€§å’Œé²æ£’æ€§ï¼Œä¸ºæ–¹æ³•è®¾è®¡æä¾›æ›´åšå®çš„ç†è®ºæŒ‡å¯¼ã€‚\n\n**æ€»ç»“**ï¼š **BayesianVLA** çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºé€šè¿‡è´å¶æ–¯åˆ†è§£å’Œäº’ä¿¡æ¯æœ€å¤§åŒ–ï¼Œå·§å¦™åœ°**åœ¨ç°æœ‰æœ‰åæ•°æ®ä¸Š**æå‡äº†VLAæ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšå’ŒOODæ³›åŒ–èƒ½åŠ›ã€‚å…¶ä¸»è¦çš„å±€é™åœ¨äº**è®­ç»ƒå¼€é”€**ã€**å¤šæ¨¡æ€èƒ½åŠ›ä¿ç•™çš„æƒè¡¡**ä»¥åŠ**å®éªŒéªŒè¯çš„å¹¿åº¦**ã€‚æœªæ¥çš„æ”¹è¿›å°†å›´ç»•**æ•°æ®ã€æ¶æ„ã€è¯„ä¼°**ä¸‰ä¸ªå±‚é¢å±•å¼€ï¼Œæ—¨åœ¨æ„å»ºæ›´é«˜æ•ˆã€æ›´é€šç”¨ã€æ›´ç»å¾—èµ·å®è·µæ£€éªŒçš„å…·èº«æ™ºèƒ½æ¨¡å‹ã€‚",
            "timestamp": "2026-01-27T16:07:45.530256",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T17:15:22Z",
    "created_at": "2026-01-27T15:53:23.226934",
    "updated_at": "2026-01-27T15:53:23.226941"
}