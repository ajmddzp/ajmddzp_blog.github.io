{
    "id": "2512.02018v1",
    "title": "Data-Centric Visual Development for Self-Driving Labs",
    "authors": [
        "Anbang Liu",
        "Guanzhong Hu",
        "Jiayi Wang",
        "Ping Guo",
        "Han Liu"
    ],
    "abstract": "è‡ªåŠ¨é©¾é©¶å®éªŒå®¤ä¸ºå‡å°‘ç”Ÿç‰©ç§‘å­¦é¢†åŸŸä¸­åŠ³åŠ¨å¯†é›†ã€è€—æ—¶ä¸”å¸¸ä¸å¯é‡å¤çš„å®éªŒæµç¨‹æä¾›äº†æå…·å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå…¶ä¸¥æ ¼çš„ç²¾åº¦è¦æ±‚ä¾èµ–äºé«˜åº¦é²æ£’çš„æ¨¡å‹ï¼Œè€Œè¿™äº›æ¨¡å‹çš„è®­ç»ƒåˆéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ã€‚ä½†åœ¨å®é™…å¸¸è§„æ“ä½œä¸­ï¼Œæ­¤ç±»æ•°æ®å°¤å…¶æ˜¯è´Ÿæ ·æœ¬éš¾ä»¥è·å–ã€‚æœ¬æ–‡èšç„¦äºç§»æ¶²æ“ä½œâ€”â€”è‡ªåŠ¨é©¾é©¶å®éªŒå®¤ä¸­æœ€å…³é”®ä¸”å¯¹ç²¾åº¦è¦æ±‚æœ€é«˜çš„æ­¥éª¤ã€‚ä¸ºå…‹æœè®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ç§èåˆçœŸå®ä¸è™šæ‹Ÿæ•°æ®ç”Ÿæˆçš„æ··åˆå¼æµæ°´çº¿ã€‚çœŸå®æ•°æ®é‡‡é›†éƒ¨åˆ†é‡‡ç”¨â€œäººåœ¨å›è·¯â€ç­–ç•¥ï¼Œå°†è‡ªåŠ¨åŒ–é‡‡é›†ä¸é€‰æ‹©æ€§äººå·¥éªŒè¯ç›¸ç»“åˆï¼Œä»¥æœ€å°çš„äººåŠ›æŠ•å…¥å®ç°æœ€é«˜çš„å‡†ç¡®æ€§ã€‚è™šæ‹Ÿæ•°æ®éƒ¨åˆ†åˆ™é€šè¿‡å‚è€ƒæ¡ä»¶åŒ–ã€æç¤ºå¼•å¯¼çš„å›¾åƒç”ŸæˆæŠ€æœ¯å¯¹çœŸå®æ•°æ®è¿›è¡Œå¢å¼ºï¼Œå¹¶è¿›ä¸€æ­¥ç­›é€‰å’ŒéªŒè¯ä»¥ç¡®ä¿å¯é æ€§ã€‚ä¸¤æ¡è·¯å¾„ç›¸ç»“åˆï¼Œç”Ÿæˆäº†ä¸€ä¸ªç±»åˆ«å‡è¡¡çš„æ•°æ®é›†ï¼Œä»è€Œæ”¯æŒé²æ£’çš„æ°”æ³¡æ£€æµ‹æ¨¡å‹è®­ç»ƒã€‚åœ¨ä¿ç•™çš„çœŸå®æµ‹è¯•é›†ä¸Šï¼Œä»…ä½¿ç”¨å…¨è‡ªåŠ¨é‡‡é›†çš„çœŸå®å›¾åƒè®­ç»ƒçš„æ¨¡å‹è¾¾åˆ°äº†99.6%çš„å‡†ç¡®ç‡ï¼›è€Œåœ¨è®­ç»ƒä¸­æ··åˆä½¿ç”¨çœŸå®ä¸ç”Ÿæˆæ•°æ®æ—¶ï¼Œå‡†ç¡®ç‡ä»ä¿æŒåœ¨99.4%ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ•°æ®é‡‡é›†ä¸å®¡æ ¸çš„å·¥ä½œé‡ã€‚æœ¬æ–¹æ³•ä¸ºè‡ªåŠ¨é©¾é©¶å®éªŒå®¤å·¥ä½œæµæä¾›äº†å¯æ‰©å±•ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„è§†è§‰åé¦ˆæ•°æ®ä¾›ç»™ç­–ç•¥ï¼Œä¹Ÿä¸ºç½•è§äº‹ä»¶æ£€æµ‹åŠæ›´å¹¿æ³›çš„è§†è§‰ä»»åŠ¡ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚",
    "url": "https://arxiv.org/abs/2512.02018v1",
    "html_url": "https://arxiv.org/html/2512.02018v1",
    "html_content": "Data-Centric Visual Development for Self-Driving Labs\nAnbang Liu\nâ€ ,1\nGuanzhong Hu\nÂ§,2\nJiayi Wang\nâ€ ,3\nPing Guo\nÂ§,4\nHan Liu\nâ€ ,5\nâ€ \nDepartment of Computer Science, Northwestern University\nÂ§\nDepartment of Mechanical Engineering, Northwestern University\n1\nanbangliu2027@u.northwestern.edu\n2\nguanzhonghu2028@u.northwestern.edu\n3\njiayiwang2020@u.northwestern.edu\n4\nping.guo@northwestern.edu\n5\nhanliu@northwestern.edu\nAbstract\nSelf-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.\n1\nIntroduction\nFigure 1\n:\nReal track workflow.\nAfter each aspiration, the robot holds the pipette tip at a fixed inspection place and the camera takes a photo. A quick quality check removes bad frames (e.g., off-center, or missing the tip). The remaining frames are screened by a lightweight classifier: good photos are accepted automatically, borderline cases are sent to a brief human review, and only poor-quality frames are discarded. Both bubble and no-bubble images are kept, so the process yields a steady, labeled stream of high-quality real data with minimal supervision.\nModern AI is powered by three enginesâ€”models, algorithms, and dataâ€”but in many practical settings the limiting factor is no longer architectural capacity. It is the availability of the right data, especially the negative or failure cases that determine reliability\n[\n4\n,\n40\n,\n24\n,\n35\n]\n. Self-driving laboratories (SDLs) illustrate this gap vividly\n[\n32\n,\n15\n]\n. Todayâ€™s SDL pipelines often lack visual feedback, so there is no closed-loop perception to catch errors during routine operations\n[\n32\n,\n15\n]\n. Adding such visual feedback is not a trivial matter of training â€œone more classifierâ€. It requires a repeatable way to produce sufficient, task-relevant images drawn from the same physical workflow the feedback will monitor\n[\n39\n,\n9\n]\n. In pipettingâ€”the backbone operation for most wet-lab protocolsâ€”one important failure situation is air bubbles inside pipette tips. These events are rare in competent operation, and images of them are scarce\n[\n37\n,\n18\n,\n13\n,\n8\n]\n. As a result, performance in this setting is constrained by data rather than by the choice of model or algorithm\n[\n4\n,\n40\n]\n.\nWe cast the problem as visual development: building the data supply chain that makes visual feedback feasible in SDLs\n[\n24\n,\n35\n]\n. The question is not â€œwhich detector is best,â€ but â€œhow do we continuously produce the images a detector needs, with minimal human effort, and at a cost that scales?â€ We answer this by designing a\ndata-centric methodology\nthat turns pipetting itself into a steady source of training data for\nbinary classification of bubble presence\ninside pipette tips. The methodology couples\ntwo coordinated tracks (real and virtual)\nso that scarcity in routine operation does not translate into scarcity at training time. On the virtual side we leverage modern generative paradigms and domain randomization to increase the prevalence of informative failures\n[\n10\n,\n14\n,\n16\n,\n25\n,\n33\n]\n. The design goals are straightforward: integrate perception into the existing physical workflow without disrupting throughput, concentrate human effort only where uncertainty requires it, and use generation strategically to raise the prevalence of error data.\nThree obstacles motivate our design. First, rarity and imbalance: in well-run labs most aspirations are correct, so bubbles form a long tail that leads to inherently imbalanced datasets\n[\n4\n,\n40\n]\n. Second, subtlety and variability: bubbles occupy a small spatial extent, can be partly occluded by the meniscus, and present differently under changes in illumination, liquid color, tip geometry, and viewpoint\n[\n37\n,\n18\n,\n13\n]\n. Third, throughput: capturing and validating mistakes has historically required technician attention, which does not scale to the volumes that modern vision training regimes expect. Annotation hoursâ€”not GPU hoursâ€”set the ceiling\n[\n28\n,\n21\n]\n. Together, these factors create a data bottleneck.\nOur methodology addresses these obstacles with two complementary tracks, both focused on data acquisition and selection. In the real track, as shown in Fig.\n1\n, we insert perception into the pipetting loop with a fixed camera and a programmable routine. After each aspiration event, the system performs event-triggered capture, applies lightweight prescreening with a simple classifier, and then routes by confidence: high-confidence cases are accepted automatically, while borderline cases are sent to human review. This concentrates expert time on uncertain examples and lets the pipeline operate around the clock with minimal supervision. Importantly, the task is formulated at the image levelâ€”\nbubble presence\nâ€”so the pipeline does not rely on costly pixel-level annotation or mask drawing. Event-triggered capture and confidence-based routing convert sporadic snapshots from a liquid-handling robot into a continuous, quality-controlled stream of labeled real images.\nFigure 2\n:\nVirtual track workflow.\nStarting from a real reference tip image, we programmatically build prompts that fix viewpoint and background but vary lab factors (color, level, bubble count/size/distribution) and specify the intended class (bubble vs. no-bubble). We batch-generate variations, run the same quality gate as in the real track, enforce label consistency with the current classifier, and perform light human spot-checks. Both bubble and no-bubble images that pass are standardized to\n600\nÃ—\n1500\n600{\\times}1500\nand added to the synthetic set for mixed training.\nIn the virtual track, as shown in Fig.\n2\n, we decide what to synthesize using priors from the physical setup. Real tip photos act as references, and prompts are derived from factors that matter in the labâ€”\nliquid color, liquid level, bubble count, bubble size, bubble distribution, and lighting\n. A modern text-guided image generator (Gemini 2.5 Flash Image) synthesizes images of liquid-containing tips with and without bubbles. Candidates are prescreened by the same classifier, and those that pass are finalized by human verification\n[\n10\n,\n14\n,\n16\n,\n25\n]\n. The objective is not to perfectly control bubble morphologyâ€”which remains stochasticâ€”but to raise the prevalence of useful failure examples at very low marginal cost. Because prompts mirror physical variation and reference images anchor appearance, selected synthetic samples align with the downstream task and can be interleaved with real samples\n[\n33\n]\n.\nBoth tracks feed a unified, class-balanced dataset used for training and evaluation. We provide standardized splits and training scripts so that others can reproduce our protocol and extend it to new checkpoints in the lab. For a concrete instantiation, we use an EfficientNetV2-L backbone for\nbinary classification of bubble presence\n[\n31\n]\n. On a held-out real test set, a model trained only on automatically acquired real images attains 99.6% accuracy while reducing technician time relative to manual collection. When we mix real and generated images during training, accuracy remains close to the real-only baseline (99.4%) while further reducing real collection and manual effort\n[\n24\n,\n35\n,\n20\n]\n. The point is methodological: the feedback loop becomes viable not because the classifier is novel, but because the data engine supplies what the classifier needs, sustainably and at low cost.\nOur study contributes to a broader data-centric perspective in AI for science and engineering. Instead of treating rare mistakes as insurmountable scarcity, we convert them into abundant supervision by coupling automation on the real side, and with\nreference-conditioned, prompt-steered generation\non the virtual side\n[\n24\n,\n35\n,\n10\n,\n14\n]\n. The same recipeâ€”automate reality and use physically guided generation to oversample failuresâ€”extends beyond bubble detection. It applies to other SDL visual checkpoints where errors are rare yet consequential, such as droplet misplacement, tip clogging, or cross-contamination traces\n[\n32\n,\n15\n,\n39\n]\n. It also applies to industrial visual inspection and other scientific imaging pipelines where staging faults is slow, expensive, or disruptive\n[\n38\n,\n26\n]\n. In these settings, the central artifact is not another network architecture, but a repeatable data process that turns operational pain points into scalable training signals.\nOur contributions are fourfold:\n1.\nProblem framing:\nwe formulate visual development for SDLs and identify why lack of visual feedback is a data supply problem rather than an architectural one, with bubble-in-tip classification as a concrete high-impact use case.\n2.\nReal-world acquisition:\nwe design an\nevent-triggered, confidence-aware\ncollection loop that yields reliable real images with minimal manual intervention.\n3.\nPhysically guided generation:\nwe propose a\nreference-conditioned, prompt-steered\nsynthesis strategy that selects synthetic images aligned with lab conditions and raises the prevalence of informative failures at low cost.\n4.\nUnified dataset and evaluation:\nwe release standardized splits and training scripts and report results showing that real-only training achieves 99.6% accuracy while mixed training maintains 99.4% accuracy with reduced real collection and manual effort.\nTaken together, these elements offer a practical methodology for adding visual feedback to SDL workflows. By focusing on the\ndata engine\nâ€”how images are captured, selected, and combinedâ€”we enable scalable, low-cost data creation for rare-event visual quality control, moving practice toward the level of reliability that modern models promise but cannot reach without the right data\n[\n4\n,\n40\n]\n.\n2\nRelated Work\nSDLs and Laboratory Automation.\nSelf-driving laboratories (SDLs) aim to automate the scientific loop but most reports still emphasize planning, orchestration, and cloud execution rather than pervasive vision checkpoints inside unit operations\n[\n32\n,\n34\n,\n15\n]\n. Vision-enabled stations exist for liquid-level control in chemistry setups\n[\n39\n]\nand for task-specific automation platforms such as RoboCulture that integrate manipulation, sensing, and behavior trees for long-duration experiments\n[\n2\n]\n. In life-science automation, several works argue that liquid handlers lack integrated vision quality control, motivating computer-vision add-ons around accessible systems (e.g., OT-2)\n[\n17\n]\n. In robotics for aliquoting, YOLO-based perception has been used to guide manipulators\n[\n19\n]\n, and AI models have been explored for liquid-level monitoring in assembly contexts\n[\n29\n]\n. Compared to these lines, our focus is not a particular controller or station but the\ndata supply chain\nfor a visual checkpoint (bubble/no-bubble) that SDLs currently miss, with an explicit mechanism to continuously create and curate training data from the pipetting loop itself.\nVision for Transparent Containers, Liquids, and Bubbles.\nDatasets and methods for materials in transparent vessels (Vector-LabPics) demonstrate segmentation of vessels and phases but target general lab scenes rather than rare failure signatures inside pipette tips\n[\n9\n]\n. LCDTC shifts toward liquid content estimation in containers using detection baselines\n[\n36\n]\n. Bubble research in two-phase or boiling flows explores segmentation and tracking (Mask R-CNN, SORT; BubbleID)\n[\n8\n]\n, robust detection under occlusion and overlap\n[\n13\n]\n, and generalized bubble mask extraction with weighted losses\n[\n18\n]\n. In pipetting contexts, recent work detects liquid retention in tips and proposes architectural tweaks to YOLOv8 for complex backgrounds\n[\n37\n]\n. We differ by (1) targeting the\nrare error\nâ€œair bubble in tipâ€ as the supervision unit, (2) designing a bi-track engine to lift prevalence at data collecting time, and (3) releasing a balanced dataset where evaluation is on held-out\nreal\ntip images drawn from the same workflow.\nData-Centric Learning and Imbalance.\nLong-tailed and imbalanced recognition motivates reweighting with effective numbers\n[\n4\n]\n, taxonomies and empirical syntheses of deep long-tailed learning\n[\n40\n]\n, classic over/under-sampling (SMOTE-style)\n[\n3\n]\n, and adaptive synthetic sampling (ADASYN)\n[\n11\n]\n. Streaming and drifting settings call for standardized evaluation across imbalance regimes\n[\n1\n]\n. Broad surveys argue for data-centric pipelines and augmentation beyond architectural changes\n[\n24\n,\n35\n]\n. Our pipeline operationalizes these insights for SDL checkpoints: rather than solely reweighting a scarce minority, we\nmanufacture\nadditional, task-aligned data through event-triggered collection and reference-conditioned synthesis.\nSynthetic Data and Generative Models.\nGenerative modeling and representation learning (GANs, diffusion, visionâ€“language) provide powerful tools to increase diversity\n[\n10\n,\n14\n,\n16\n,\n25\n]\n. Domain randomization shows that broad appearance variation can close sim-to-real gaps in robotic perception\n[\n33\n]\n. For detection specifically, synthetic imagery can boost few-shot regimes, and CLIP can filter false positives from synthetic sets\n[\n20\n]\n. Our â€œvirtual trackâ€ aligns with these trends but is intentionally\nreference-conditioned and prompt-steered\n: real tip photos anchor appearance, prompts enumerate lab-relevant attributes (liquid color/level, bubble count/size/distribution, lighting), and a lightweight classifier plus human verification enforce task alignment before mixing with real data.\nBackbones and Detection Frameworks.\nModern detectors and backbones form the toolset rather than the novelty in our work: EfficientNet/EfficientNetV2 for accuracyâ€“efficiency scaling\n[\n30\n,\n31\n]\n, residual networks\n[\n12\n]\n, ViT and hierarchical Swin Transformers\n[\n7\n,\n23\n]\n, and one-/two-stage detectors from YOLO\n[\n27\n]\nto RetinaNet with focal loss for imbalance in dense detection\n[\n22\n]\n. Classical HOG and early deep features (DeCAF) contextualize the evolution of representations\n[\n5\n,\n6\n]\n. We fix a single off-the-shelf classifierâ€”EfficientNetV2-Lâ€”to isolate the data engineâ€™s effect, and the\nbi-track pipeline\nalone delivers strong real-set performance without bespoke architectures.\nPrior work establishes why SDLs require vision and shows how liquids and bubbles can be detected in broader laboratory contexts. Research on long-tailed recognition and class imbalance explains why rare failures throttle reliability, and generative modeling offers a practical way to expand training data. We unify these threads into a practical, closed-loop\ndata engine\nfor an SDL visual quality-control taskâ€”bubble-in-tipâ€”by coupling automated real capture with prompt-steered, selection-based synthesis and confidence-guided human review.\n3\nMethod\n3.1\nOverview\nWe propose a bi-track data engine for bubble-in-tip perception in self-driving laboratory (SDL) workflows. In the\nreal track\n, we integrate vision into pipetting by coordinating an ABLE Labs NOTABLE liquid-handling robot that performs pipetting, a fixed industrial camera that captures the tip immediately afterward, and a lightweight classifier that prescreens each image and routes ambiguous cases to human audit, enabling continuous 24/7 acquisition with minimal supervision. The\nvirtual track\ncomplements scarcity by reference-conditioned, prompt-steered synthesis (Gemini 2.5 Flash Image), followed by classifier-consistency filtering and sparse human spot-checks. In both tracks we explicitly\ntarget both\nbubble (\ny\n=\n1\ny{=}1\n) and no-bubble (\ny\n=\n0\ny{=}0\n) cases, filter\nunqualified\nimages (e.g., blur, framing, occlusion), not dispreferred classes. A standard EfficientNetV2-L classifier\n[\n31\n]\nis trained with class-balanced loss to mitigate long-tail effects\n[\n4\n]\n. This section formalizes three components: classifier and loss, real track, and virtual track.\n3.2\nClassifier and Loss (EfficientNetV2-L)\nLet\nx\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n3\nx\\!\\in\\!\\mathbb{R}^{H\\times W\\times 3}\nbe an image;\ny\nâˆˆ\n{\n0\n,\n1\n}\ny\\!\\in\\!\\{0,1\\}\nthe label (1=bubble, 0=no-bubble);\nh\nÎ¸\nâ€‹\n(\nâ‹…\n)\nh_{\\theta}(\\cdot)\nthe EfficientNetV2-L feature extractor\n[\n31\n]\n;\n(\nğ°\n,\nb\n)\n(\\mathbf{w},b)\nthe linear head;\nÏƒ\n\\sigma\nthe sigmoid;\nğ’Ÿ\n\\mathcal{D}\nthe training set;\nn\ny\nn_{y}\nthe sample count of class\ny\ny\n;\nÎ²\nâˆˆ\n[\n0\n,\n1\n)\n\\beta\\!\\in\\![0,1)\nthe class-balance hyperparameter.\nf\nÎ¸\nâ€‹\n(\nx\n)\n=\nÏƒ\nâ€‹\n(\nğ°\nâŠ¤\nâ€‹\nh\nÎ¸\nâ€‹\n(\nx\n)\n+\nb\n)\n.\nf_{\\theta}(x)\\;=\\;\\sigma\\!\\big(\\mathbf{w}^{\\top}h_{\\theta}(x)+b\\big).\n(1)\nÎ±\ny\n=\n1\nâˆ’\nÎ²\n1\nâˆ’\nÎ²\nn\ny\n.\n\\alpha_{y}\\;=\\;\\frac{1-\\beta}{1-\\beta^{\\,n_{y}}}\\quad.\n(2)\nâ„’\nCB\n=\n1\n|\nğ’Ÿ\n|\nâ€‹\nâˆ‘\n(\nx\n,\ny\n)\nâˆˆ\nğ’Ÿ\nÎ±\ny\n\\displaystyle\\mathcal{L}_{\\mathrm{CB}}\\;=\\;\\frac{1}{|\\mathcal{D}|}\\!\\!\\sum_{(x,y)\\in\\mathcal{D}}\\alpha_{y}\n[\nâˆ’\ny\nlog\nf\nÎ¸\n(\nx\n)\n\\displaystyle\\Big[-y\\log f_{\\theta}(x)\n(3)\nâˆ’\n(\n1\nâˆ’\ny\n)\nlog\n(\n1\nâˆ’\nf\nÎ¸\n(\nx\n)\n)\n]\n.\n\\displaystyle-(1-y)\\log\\!\\big(1-f_{\\theta}(x)\\big)\\Big].\nAfter producing the posterior\nf\nÎ¸\nâ€‹\n(\nx\n)\nf_{\\theta}(x)\nand computing the class weights\nÎ±\ny\n\\alpha_{y}\nvia the effective-number formula, we minimize the weighted binary cross-entropy\nâ„’\nCB\n\\mathcal{L}_{\\mathrm{CB}}\nover\nğ’Ÿ\n\\mathcal{D}\nto update\n(\nÎ¸\n,\nğ°\n,\nb\n)\n(\\theta,\\mathbf{w},b)\n.\n3.3\nReal Track: Triggered Capture, Prescreen, Human-in-the-loop\nLet\nc\nÎ¸\nâ€‹\n(\nx\n)\nc_{\\theta}(x)\nbe the model confidence;\nÏ„\nA\n\\tau_{A}\nthe auto-accept threshold;\nÏ„\nR\n\\tau_{R}\nthe review threshold\n(\n0.5\nâ‰¤\nÏ„\nR\n<\nÏ„\nA\nâ‰¤\n1\n)\n(0.5\\!\\leq\\!\\tau_{R}\\!<\\!\\tau_{A}\\!\\leq\\!1)\n;\nq\nâ€‹\n(\nx\n)\nâˆˆ\n[\n0\n,\n1\n]\nq(x)\\!\\in\\![0,1]\na scalar image-quality score (sharpness/framing heuristics) with threshold\nÏ„\nq\n\\tau_{q}\n;\nroute\nâ€‹\n(\nâ‹…\n)\n\\textsf{route}(\\cdot)\nthe router;\nh\nâ€‹\n(\nâ‹…\n)\nh(\\cdot)\na human audit;\ny\n^\nâ€‹\n(\nx\n)\n\\hat{y}(x)\nthe assigned label.\nc\nÎ¸\nâ€‹\n(\nx\n)\n=\nmax\nâ¡\n{\nf\nÎ¸\nâ€‹\n(\nx\n)\n,\n1\nâˆ’\nf\nÎ¸\nâ€‹\n(\nx\n)\n}\n.\nc_{\\theta}(x)\\;=\\;\\max\\!\\big\\{\\,f_{\\theta}(x),\\;1-f_{\\theta}(x)\\,\\big\\}.\n(4)\nroute\nâ€‹\n(\nx\n)\n=\n{\nA\nif\nâ€‹\nq\nâ€‹\n(\nx\n)\nâ‰¥\nÏ„\nq\nâˆ§\nc\nÎ¸\nâ€‹\n(\nx\n)\nâ‰¥\nÏ„\nA\n,\nR\nif\nâ€‹\nq\nâ€‹\n(\nx\n)\nâ‰¥\nÏ„\nq\nâˆ§\nÏ„\nR\nâ‰¤\nc\nÎ¸\nâ€‹\n(\nx\n)\n<\nÏ„\nA\n,\nD\notherwise\n(drop unqualified)\n.\n\\textsf{route}(x)\\;=\\;\\begin{cases}\\text{A}&\\text{if }q(x)\\!\\geq\\!\\tau_{q}\\;\\wedge\\;c_{\\theta}(x)\\!\\geq\\!\\tau_{A},\\\\\n\\text{R}&\\text{if }q(x)\\!\\geq\\!\\tau_{q}\\;\\wedge\\;\\tau_{R}\\!\\leq\\!c_{\\theta}(x)\\!<\\!\\tau_{A},\\\\\n\\text{D}&\\text{otherwise}\\quad\\text{(drop unqualified)}.\\end{cases}\n(5)\ny\n^\nâ€‹\n(\nx\n)\n=\n{\nğŸ™\nâ€‹\n[\nf\nÎ¸\nâ€‹\n(\nx\n)\nâ‰¥\n0.5\n]\n,\nif\nroute\nâ€‹\n(\nx\n)\n=\nA\n,\nh\nâ€‹\n(\nx\n)\n,\nif\nroute\nâ€‹\n(\nx\n)\n=\nR\n.\n\\hat{y}(x)\\;=\\;\\begin{cases}\\mathbb{1}\\!\\big[f_{\\theta}(x)\\!\\geq\\!0.5\\big],&\\text{if }\\textsf{route}(x)=\\text{A},\\\\\nh(x),&\\text{if }\\textsf{route}(x)=\\text{R}.\\end{cases}\n(6)\nAfter each capture, we first score image quality\nq\nâ€‹\n(\nx\n)\nq(x)\nand model confidence\nc\nÎ¸\nâ€‹\n(\nx\n)\nc_{\\theta}(x)\n. The router then applies the thresholds: frames that satisfy\nq\nâ€‹\n(\nx\n)\nâ‰¥\nÏ„\nq\nq(x)\\geq\\tau_{q}\nand\nc\nÎ¸\nâ€‹\n(\nx\n)\nâ‰¥\nÏ„\nA\nc_{\\theta}(x)\\geq\\tau_{A}\nare auto-accepted and labeled by\nğŸ™\nâ€‹\n[\nf\nÎ¸\nâ€‹\n(\nx\n)\nâ‰¥\n0.5\n]\n\\mathbb{1}[f_{\\theta}(x)\\geq 0.5]\n, frames with\nq\nâ€‹\n(\nx\n)\nâ‰¥\nÏ„\nq\nq(x)\\geq\\tau_{q}\nand\nÏ„\nR\nâ‰¤\nc\nÎ¸\nâ€‹\n(\nx\n)\n<\nÏ„\nA\n\\tau_{R}\\leq c_{\\theta}(x)<\\tau_{A}\nare sent to human audit\nh\nâ€‹\n(\nx\n)\nh(x)\n, and frames with\nq\nâ€‹\n(\nx\n)\n<\nÏ„\nq\nq(x)<\\tau_{q}\nare dropped. Accepted and audited samples form\nğ’Ÿ\nreal\n=\n{\n(\nx\n,\ny\n^\nâ€‹\n(\nx\n)\n)\n}\n\\mathcal{D}_{\\mathrm{real}}=\\{(x,\\hat{y}(x))\\}\n.\n(a)\nreal & no-bubble\n(b)\nreal & bubble\n(c)\nreal & bubble\n(d)\nvirtual & no-bubble\n(e)\nvirtual & bubble\n(f)\nvirtual & bubble\nFigure 3\n:\nDataset examples.\n3.4\nVirtual Track: Reference-Conditioned, Prompt-Steered Synthesis\nLet\nG\nÏˆ\nG_{\\psi}\nbe a text-guided image generator;\nz\nâˆ¼\np\nâ€‹\n(\nz\n)\nz\\!\\sim\\!p(z)\nnoise;\nr\nr\na real tip reference image;\nÏ•\n\\phi\na prompt encoding lab factors (liquid color/level, bubble count/size/distribution, lighting) and the\nintended class\n;\nx\n~\n\\tilde{x}\nthe synthesized image;\ny\n~\nâˆˆ\n{\n0\n,\n1\n}\n\\tilde{y}\\!\\in\\!\\{0,1\\}\nthe intended label from\nÏ•\n\\phi\n;\nÎº\nÎ¸\n\\kappa_{\\theta}\na classifier-consistency score;\nÏ„\nk\n\\tau_{k}\na keep threshold;\nğ’œ\nÏ\n\\mathcal{A}_{\\rho}\na\nÏ\n\\rho\n-fraction spot-check set.\nx\n~\n=\nG\nÏˆ\nâ€‹\n(\nz\n;\nr\n,\nÏ•\n)\n,\ny\n~\n=\nâ„“\nâ€‹\n(\nÏ•\n)\nâˆˆ\n{\n0\n,\n1\n}\n.\n\\tilde{x}\\;=\\;G_{\\psi}\\!\\big(z;\\,r,\\phi\\big),\\qquad\\tilde{y}\\;=\\;\\ell(\\phi)\\in\\{0,1\\}.\n(7)\nÎº\nÎ¸\nâ€‹\n(\nx\n~\n,\ny\n~\n)\n=\n{\nf\nÎ¸\nâ€‹\n(\nx\n~\n)\n,\ny\n~\n=\n1\n,\n1\nâˆ’\nf\nÎ¸\nâ€‹\n(\nx\n~\n)\n,\ny\n~\n=\n0\n.\n\\kappa_{\\theta}(\\tilde{x},\\tilde{y})\\;=\\;\\begin{cases}f_{\\theta}(\\tilde{x}),&\\tilde{y}=1,\\\\\n1-f_{\\theta}(\\tilde{x}),&\\tilde{y}=0.\\end{cases}\n(8)\nğ’Ÿ\nsyn\n=\n{\n(\nx\n~\n,\ny\n~\n)\n:\nÎº\nÎ¸\nâ‰¥\nÏ„\nk\nâˆ§\nq\nâ€‹\n(\nx\n~\n)\nâ‰¥\nÏ„\nq\n}\nâˆª\nğ’œ\nÏ\n.\n\\mathcal{D}_{\\mathrm{syn}}\\;=\\;\\{(\\tilde{x},\\tilde{y}):\\kappa_{\\theta}\\!\\geq\\!\\tau_{k}\\;\\wedge\\;q(\\tilde{x})\\!\\geq\\!\\tau_{q}\\}\\ \\cup\\ \\mathcal{A}_{\\rho}.\n(9)\nGiven a prompt\nÏ•\n\\phi\nthat specifies appearance factors and the\nintended\nclass\ny\n~\n=\nâ„“\nâ€‹\n(\nÏ•\n)\n\\tilde{y}=\\ell(\\phi)\n, we synthesize\nx\n~\n=\nG\nÏˆ\nâ€‹\n(\nz\n;\nr\n,\nÏ•\n)\n\\tilde{x}=G_{\\psi}(z;r,\\phi)\n, then compute the consistency score\nÎº\nÎ¸\nâ€‹\n(\nx\n~\n,\ny\n~\n)\n\\kappa_{\\theta}(\\tilde{x},\\tilde{y})\nand the quality score\nq\nâ€‹\n(\nx\n~\n)\nq(\\tilde{x})\n. We keep a sample only if it passes both semantic consistency (\nÎº\nÎ¸\nâ‰¥\nÏ„\nk\n\\kappa_{\\theta}\\geq\\tau_{k}\n) and basic quality (\nq\nâ‰¥\nÏ„\nq\nq\\geq\\tau_{q}\n), plus a small\nÏ\n\\rho\n-fraction spot-audit to control drift.\n3.5\nTraining on the Unified Set\nLet\nÎ´\n,\nÏµ\nâˆˆ\n(\n0\n,\n1\n]\n\\delta,\\epsilon\\!\\in\\!(0,1]\nbe sampling proportions for the real and synthetic pools, respectively. We form a proportioned union of (multi)sets as\nğ’Ÿ\n=\nÎ´\nâ€‹\nğ’Ÿ\nreal\nâˆª\nÏµ\nâ€‹\nğ’Ÿ\nsyn\n,\n\\mathcal{D}\\;=\\;\\delta\\,\\mathcal{D}_{\\mathrm{real}}\\;\\cup\\;\\epsilon\\,\\mathcal{D}_{\\mathrm{syn}},\n(10)\nwhere\nÎ´\nâ€‹\nğ’Ÿ\n\\delta\\,\\mathcal{D}\ndenotes a subsample (or reweighted multiset) drawn from\nğ’Ÿ\n\\mathcal{D}\nat proportion\nÎ´\n\\delta\n, targeting class balance across\ny\nâˆˆ\n{\n0\n,\n1\n}\ny\\in\\{0,1\\}\n. The training objective is\nmin\nÎ¸\n,\nğ°\n,\nb\nâ¡\nâ„’\nCB\nâ€‹\n(\nğ’Ÿ\n)\n.\n\\min_{\\theta,\\mathbf{w},b}\\ \\mathcal{L}_{\\mathrm{CB}}(\\mathcal{D}).\n(11)\nResults on held-out\nreal\ndata validate that a mainstream backbone with a bi-track data engine attains very strong accuracy without bespoke architectures.\n4\nExperiment\nSource\nClass\nTotal\n\\cellcolor\nBubbleCol\nBubble\n\\cellcolor\nNoBubbleCol\nNo-bubble\nReal\n\\cellcolor\nBubbleCol1701\n(53.1%)\n\\cellcolor\nNoBubbleCol1501\n(46.9%)\n3202\nVirtual\n\\cellcolor\nBubbleCol1523\n(50.4%)\n\\cellcolor\nNoBubbleCol1499\n(49.6%)\n3022\nOverall\n\\cellcolor\nBubbleCol\n3224\n(52.0%)\n\\cellcolor\nNoBubbleCol\n3000\n(48.0%)\n6224\nBubble (1)\nNo-bubble (0)\n% = within-source proportion\nTable 1\n:\nDataset composition.\nCounts per source and class; percentages are computed within each source (row).\n4.1\nDatasets\nScope and sources.\nAs shown in\nTableÂ 1\n, our dataset contains\n6,224\ntip images comprising\n3,202 real\ncaptures and\n3,022 virtual\nrenders. Real images are acquired by an ABLE Labs NOTABLE liquid-handling robot instrumented with a fixed FLIR camera. Each data is labeled for\nbubble presence\n(binary). Virtual images are produced by reference-conditioned, prompt-steered synthesis (Gemini 2.5 Flash Image), with prompts specifying intended class (bubble/no-bubble) and appearance factors (liquid color/level, bubble count/size/distribution, lighting). All images are center-aligned crops at\n600\nÃ—\n\\times\n1500\npx: when the source is taller than target we trim from the\ntop\n(preserving the meniscus region), and when wider we crop\nsymmetrically\nfrom left/right; if narrower, we letterbox pad to the target width. Dataset examples are shown in Fig.\n3\n.\nComposition.\nReal images cover two tip lengths (\nlong\n/\nshort\n) crossed with five colors (transparent, red, yellow, blue, green). Virtual images sample colors at random. Bubble count is uniformly specified from\n1â€“15\nand nominal bubble diameter is\nâˆ¼\n\\sim\n0.2â€“1.5â€‰mm\nat capture scale. We keep both\nbubble\nand\nno-bubble\nclasses in each track and filter out only\nunqualified\nframes (e.g., blur, misframing) via an image-quality score before labeling.\nSplits and leakage control.\nWe evaluate only on\nheld-out real\nimages to avoid domain confounds. We perform\nrandom, stratified\nsplits by class:\nReal\nâ†’\n\\rightarrow\ntrain/val/test\n=\n=\n2242/480/480\nimages with per-class counts (Train:\n1191/1051\n, Val:\n255/225\n, Test:\n255/225\nfor bubble/no-bubble), and\nVirtual\nâ†’\n\\rightarrow\ntrain\n=\n=\n3022\nimages (Train:\n1523/1499\n). The training\npool\nis the union of the real-train and synthetic-train pools, but the actual training set uses proportional subsampling from each source:\nğ’Ÿ\ntrain\n=\nS\nÎ±\nâ€‹\n(\nğ’Ÿ\nreal\ntrain\n)\nâˆª\nS\nÎ²\nâ€‹\n(\nğ’Ÿ\nsyn\ntrain\n)\n,\n\\mathcal{D}_{\\mathrm{train}}\\;=\\;S_{\\alpha}\\!\\big(\\mathcal{D}_{\\mathrm{real}}^{\\mathrm{train}}\\big)\\ \\cup\\ S_{\\beta}\\!\\big(\\mathcal{D}_{\\mathrm{syn}}^{\\mathrm{train}}\\big),\nwhere\nS\nÏ\nâ€‹\n(\nâ‹…\n)\nS_{\\rho}(\\cdot)\nselects a\nÏ\n\\rho\n-fraction without replacement (we report\nÎ±\n,\nÎ²\n\\alpha,\\beta\nwith results). Validation and test sets are\nreal-only\n(real-val and real-test, respectively). We fix a global random seed for reproducibility.\nLabels and quality control.\nGround truth for real images is binaryâ€”bubble present (1) or no-bubble (0). Frames that fail the quality gate (occluded tip, or off-center framing) are discarded. Ambiguous but otherwise qualified frames are routed to human audit before inclusion. Virtual images inherit an intended label from the prompt. Each candidate must pass a classifier-consistency check by the current model and the same quality gate, and we also perform sparse human spot-checks on a random subset. This policy ensures that the selection method removes only unqualified images rather than preferentially excluding either class.\n(a)\nLeft eye level\n(b)\nTop-down\n(c)\nRight eye level\nFigure 4\n:\nABLE Labs\nNotable\nliquid-handling robot and our fixed-camera setup from three viewpoints: (a) left eye level, (b) top-down, and (c) right eye level.\n4.2\nData Collection Setup\nHardware.\nAs shown in Fig.\n4\n, we instrument an ABLE Labs\nNotable\nliquid-handling robot with a fixed industrial cameraâ€“lens pair: a FLIR Blackfly S\nBFS-U3-50S4C-C\n(USB3, color) and an Edmund Optics C-mount lens (6â€‰mm, F/1.4, #67709). The camera is mounted on an external mount and rigidly aimed at the robotâ€™s predefined inspection place (the repeatable capturing place after pipetting), so that every capture is taken from a nearly identical viewpoint. This out-of-workspace mounting avoids any interference with the robotâ€™s motion. The chosen focal length and aperture provide a field of view that comfortably contains the full pipette tip and a small margin of surrounding background, while keeping the depth of field large enough that tips remain in focus despite minor height variations in the robot motion. We calibrate the mount orientation once and lock all adjustable joints, so that subsequent data collection sessions can be resumed without re-tuning the camera pose. In addition, we place a matte-black backdrop at the robotâ€™s predefined inspection place to suppress background clutter and reflections, improving signal-to-noise in the tip ROI and contributing to the near-100% accuracy model. This simple mechanical and optical setup favors robustness and reproducibility over complexity, and can be cheaply replicated in other laboratories without specialized vision hardware.\nIllumination.\nWe use standard overhead LED lighting (neutral white,\nâˆ¼\n4000\nâ€‹\nâ€“\nâ€‹\n5000\nâ€‹\nK\n\\sim\\!4000\\text{â€“}5000\\,\\mathrm{K}\n) with no strobes or auxiliary lights. This choice deliberately avoids carefully tuned studio lighting. Illumination settings, such as dimmer level or which ceiling fixtures are enabled, may be adjusted during a collection session to increase data diversity and expose the model to small changes in intensity and shadow patterns. We do not synchronize lighting with the robot or the camera, so the pipeline remains mechanically simple and does not depend on fragile trigger wiring or timing assumptions.\nTriggering & timing.\nAfter the robot completes aspiration and holds the tip at a predefined inspection place, a Python program triggers the camera to acquire an image within\n<\n700\nâ€‹\nms\n<\\!700\\,\\mathrm{ms}\nof motion stop to avoid motion blur. The trigger is issued by polling the robot state and firing only once the target location is satisfied, yielding a deterministic capture environment in the liquid-handling program. We use a fixed exposure time and gain configuration, so that the only major source of variability in the captured frames comes from the liquid interface and bubble patterns rather than from timing jitter. The end-to-end cycle time is dominated by liquid handling, and the vision stage adds\n<\n3\nâ€‹\ns\n<\\!3\\,\\mathrm{s}\nper aspiration, which is small compared to other steps and therefore does not materially slow down the overall protocol.\nQuality gate.\nFrames are rejected immediately if they fail basic acquisition criteria: at first, we segment each captured imageâ€™s tip region of interest (ROI) using Otsuâ€™s thresholding. If no valid ROI is detected, the frame is discarded. Concretely, we apply a global threshold on the grayscale image, identify connected components corresponding to the bright tip against the black background, and keep only components that satisfy reasonable geometric constraints (such as minimum area and aspect ratio). Frames in which the tip is partially outside the field of view, too blurred to pass segmentation, or occluded by unexpected objects are thus filtered out before entering the manual or automated labeling pipeline. This lightweight â€œquality gateâ€ can be run during collection and prevents corrupted observations from polluting the dataset or biasing downstream model evaluation.\n4.3\nProcesses\nIn the real track.\nWe first calibrate the imaging geometry by fixing the FLIR camera and lens on an external mount, aligning the optical axis with the tipâ€™s hold position, setting exposure and white balance. During collection, the ABLE Labs NOTABLE liquid handling robot executes aspiration and moves the pipette to a predefined inspection location, then a Python trigger acquires a frame within\n<\n700\n<\\!700\nms of motion stop. Each frame undergoes a quality gate that detects a valid tip ROI using OTSU-based segmentation, followed by geometric checks on ROI ratio, tip vertex angle, and bilateral side straightness via Hough lines. Frames that fail are dropped immediately. Qualified frames are then prescreened by a lightweight classifier and routed by confidence (auto-accept versus human review) to concentrate expert effort on borderline cases, yielding a reliable stream of both bubble and no-bubble exemplars without per-pixel annotation. This procedure embeds vision into wet-lab workflows and complements domain work on bubble and liquid perception in broader settings\n[\n9\n,\n18\n,\n13\n,\n8\n]\n.\nIn the virtual track.\nTo mitigate rarity, we synthesize reference-conditioned variations with GeminiÂ 2.5 Flash Image in Batch (FILE) mode: (1) upload each real reference to the Files API to obtain a\nfile_uri\n; (2) build a JSONL where each line specifies a\nGenerateContentRequest\nthat fixes viewpoint/background while programmatically randomizing liquid color, fill level, and either bubble count in\n[\n1\n,\n15\n]\n[1,15]\nor an explicitly bubble-free constraint; (3) submit a Batch job, poll to completion, then download and parse the results, saving returned images. Each candidate inherits an\nintended\nlabel from the prompt (bubble/no-bubble) and is screened by the current classifier for label consistency and by the same quality gate as real data, with sparse human spot-checks. This selection-based synthesis leverages advances in generative modeling\n[\n10\n,\n14\n,\n16\n,\n25\n]\nand follows the spirit of domain randomization for robust transfer\n[\n33\n]\n, while maintaining task alignment through reference conditioning. Related use of synthetic imagery for scarce-supervision detection is echoed in few-shot pipelines\n[\n20\n]\n.\nData processing.\nAll accepted images (real and synthetic) are standardized to\n600\nÃ—\n1500\n600{\\times}1500\n: if either side is smaller, we first upscale proportionally. If width exceeds 600â€‰px, we center-crop horizontally. If height exceeds 1500â€‰px, we top-crop to anchor the bottom of the tip, preserving the inspection region. We then apply light augmentations to improve robustness while respecting lab physics: small rotations (\nÂ±\n2\nâˆ˜\n\\pm 2^{\\circ}\n), translation/crop jitter (\nâ‰¤\n3\n%\n\\leq 3\\%\n), brightness/contrast and gamma jitter to emulate LED variations, mild Gaussian noise/blur to model sensor and slight motion, and\nno\nvertical flips (to avoid inverting gravity/meniscus). Class imbalance is handled at sampling and loss levels (e.g., effective-number weighting or minority oversampling) to keep bubble/no-bubble prevalence balanced per batch\n[\n4\n,\n3\n,\n11\n]\n, and we train a standard EfficientNetV2-L classifier\n[\n31\n]\non mixed mini-batches drawn from the curated real and synthetic pools according to the predefined mixing ratios.\n4.4\nResults\nMix (%)\nTrain (Syn:Real)\nAcc\nâ†‘\n\\uparrow\nPrec\nâ†‘\n\\uparrow\nRec\nâ†‘\n\\uparrow\nF1\nâ†‘\n\\uparrow\n0\n0:2240\n\\cellcolor\nFirst\n0.9958\n\\cellcolor\nFirst\n0.9961\n\\cellcolor\nFirst\n0.9961\n\\cellcolor\nFirst\n0.9961\n25\n560:1680\n\\cellcolor\nFirst\n0.9958\n\\cellcolor\nFirst\n0.9961\n\\cellcolor\nFirst\n0.9961\n\\cellcolor\nFirst\n0.9961\n50\n1120:1120\n\\cellcolor\nSecond0.9938\n\\cellcolor\nSecond0.9922\n\\cellcolor\nFirst\n0.9961\n\\cellcolor\nSecond0.9942\n75\n1680:560\n0.9917\n\\cellcolor\nSecond0.9922\n\\cellcolor\nSecond0.9922\n0.9922\n\\rowcolor\nblack!3\n100\n2240:0\n0.8503\n0.8333\n0.8984\n0.8647\nTable 2\n:\nPerformance on a held-out\nreal\ntest set under different synthetic:real training mixes (fixed budget 2,240). Higher is better (\nâ†‘\n\\uparrow\n). Top two per results are colored as\nfirst\nand\nsecond\n.\nMixed training on a real test set.\nWe ablate the proportion of synthetic images while keeping a fixed training budget of 2,240 images. Validation and test are\nreal-only\nand remain constant across runs. As shown in\nTableÂ 2\n, accuracy is essentially unchanged when up to 25% of the training set is synthetic. At 50â€“75% synthetic, the drop is small (still\nâ‰¥\n\\geq\n99.17% accuracy). Training on 100% synthetic images induces a marked domain gap. Precision dips slightly at 50% synthetic, whereas recall remains high. This pattern suggests that selection-based synthesis preserves discriminative cues for bubbles but cannot fully substitute for real images. In practice, mixes in the 25â€“75% range retain near-baseline accuracy while reducing the real collection load. This observation is consistent with prior findings that synthetic data works best as a complement rather than a replacement\n[\n20\n,\n33\n,\n24\n,\n35\n]\n.\nCost, throughput, and acceptance.\nOur selection-based synthesis generates 3,600 candidates in about 30 minutes at a marginal cost of $68. After classifier-consistency checks and the same quality gate used for real images, we retain 3,022 images (bubble and no-bubble combined), yielding an acceptance rate of\n83.9%\n. This corresponds to\nâ‰ˆ\n\\approx\n$0.0225\nper accepted image and\nâˆ¼\n\\sim\n101\naccepted images per minute. By contrast, the real track acquires roughly one frame every\nâˆ¼\n\\sim\n10â€‰s, and no-bubble captures succeed nearly 100% of the time, and deliberately producing bubble frames via careful pipetting succeeds only\nâˆ¼\n\\sim\n46%\n, which results in an effective\nâˆ¼\n\\sim\n21â€‰s per\naccepted\nbubble frame in addition to occasional human audits. The percentage of human audits is less than 10%. Over the campaign we attempted\nâˆ¼\n\\sim\n7,000 captures and retained 3,202 after quality gating and audit (1,701 bubble, 1,501 no-bubble), corresponding to an overall acceptance of\nâˆ¼\n\\sim\n46%. Synthesis therefore efficiently oversamples the rare failure class and stabilizes class balance at scale, consistent with reports that task-matched synthetic data, mixed judiciously, can deliver strong benefits with minimal downside\n[\n33\n,\n20\n,\n24\n,\n35\n]\n.\n5\nConclusion\nWe presented a data-centric methodology for visual development in self-driving labs that supplies the visual-feedback data lacking from pipetting workflows. The approach couples two coordinated tracks. The real track inserts perception into the physical loop with event-triggered capture, lightweight prescreening, confidence-based routing, and human review to yield reliable real images with minimal manual effort. The virtual track uses reference-conditioned, prompt-steered generation to synthesize liquid-filled tip images with and without bubbles, followed by prescreening and human verification, and both tracks feed a class-balanced dataset for training and evaluation. Experiments show that models trained on automatically acquired real data achieve 99.6% accuracy on held-out real images, and that mixing real with generated data maintains 99.4% accuracy while further reducing real collection and manual effort. Limitations include the stochastic control of synthetic appearance and evaluation on a single task. Future work will broaden visual quality-control tasks, improve controllability and calibration, and study domain shift. More broadly, the recipe offers a scalable, low-cost data supply chain for rare-event and general vision tasks.\n6\nCode and Dataset Availability\nCode and dataset are available at\nGitHub\n.\nReferences\nAguiar etÂ al. [2022]\nGabriel Aguiar, Bartosz Krawczyk, and Alberto Cano.\nA survey on learning from imbalanced data streams: taxonomy, challenges, empirical study, and reproducible experimental framework.\narXiv preprint arXiv:2204.03719\n, 2022.\nAngers etÂ al. [2025]\nKevin Angers, Kourosh Darvish, Naruki Yoshikawa, Sargol Okhovatian, Dawn Bannerman, Ilya Yakavets, Florian Shkurti, AlÃ¡n Aspuru-Guzik, and Milica Radisic.\nRoboculture: A robotics platform for automated biological experimentation.\narXiv preprint arXiv:2505.14941\n, 2025.\nChawla etÂ al. [2002]\nNiteshÂ V. Chawla, KevinÂ W. Bowyer, LawrenceÂ O. Hall, and W.Â Philip Kegelmeyer.\nSmote: Synthetic minority over-sampling technique.\nJournal of Artificial Intelligence Research\n, 16:321â€“357, 2002.\nCui etÂ al. [2019]\nYin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie.\nClass-balanced loss based on effective number of samples.\narXiv preprint arXiv:1901.05555\n, 2019.\nDalal and Triggs [2005]\nNavneet Dalal and Bill Triggs.\nHistograms of oriented gradients for human detection.\nIn\nProceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)\n, pages 886â€“893, 2005.\nDonahue etÂ al. [2013]\nJeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell.\nDecaf: A deep convolutional activation feature for generic visual recognition.\narXiv preprint arXiv:1310.1531\n, 2013.\nDosovitskiy etÂ al. [2020]\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.\nAn image is worth 16Ã—16 words: Transformers for image recognition at scale, 2020.\narXiv preprint arXiv:2010.11929.\nDunlap etÂ al. [2024]\nChristy Dunlap, Changgen Li, Hari Pandey, Ngan Le, and Han Hu.\nBubbleid: A deep learning framework for bubble interface dynamics analysis.\narXiv preprint arXiv:2405.07994\n, 2024.\nEppel etÂ al. [2020]\nSagi Eppel, Haoping Xu, Mor Bismuth, and AlÃ¡n Aspuru-Guzik.\nComputer vision for recognition of materials and vessels in chemistry lab settings and the vector-labpics data set.\nACS Central Science\n, 6(10):1743â€“1752, 2020.\nGoodfellow etÂ al. [2014]\nIanÂ J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.\nGenerative adversarial networks.\narXiv preprint arXiv:1406.2661\n, 2014.\nHe etÂ al. [2008]\nHaibo He, Yang Bai, EdwardoÂ A. Garcia, and Shutao Li.\nAdasyn: Adaptive synthetic sampling approach for imbalanced learning.\nIn\n2008 IEEE International Joint Conference on Neural Networks (IJCNN)\n, pages 1322â€“1328, 2008.\nHe etÂ al. [2016]\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn\nCVPR\n, pages 770â€“778, 2016.\nHessenkemper etÂ al. [2022]\nHendrik Hessenkemper, Sebastian Starke, Yazan Atassi, Thomas Ziegenhein, and Dirk Lucas.\nBubble identification from images with machine learning methods.\narXiv preprint arXiv:2202.03107\n, 2022.\nHo etÂ al. [2020]\nJonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising diffusion probabilistic models.\narXiv preprint arXiv:2006.11239\n, 2020.\nHysmith etÂ al. [2024]\nHolland Hysmith, Elham Foadian, ShaktiÂ P. Padhy, SergeiÂ V. Kalinin, RobÂ G. Moore, OlgaÂ S. Ovchinnikova, and Mahshid Ahmadi.\nThe future of self-driving laboratories: from human in the loop interactive ai to gamification.\nDigital Discovery\n, 3:621â€“636, 2024.\nKarras etÂ al. [2019]\nTero Karras, Samuli Laine, and Timo Aila.\nA style-based generator architecture for generative adversarial networks.\narXiv preprint arXiv:1812.04948\n, 2019.\nKhan etÂ al. [2025]\nSanaÂ Ullah Khan, VilhelmÂ Krarup MÃ¸ller, Rasmus JohnÂ Normand Frandsen, and Marjan Mansourvar.\nReal-time ai-driven quality control for laboratory automation: a novel computer vision solution for the opentrons ot-2 liquid handling robot.\nApplied Intelligence\n, 55(524), 2025.\nKim and Park [2021]\nYewon Kim and Hyungmin Park.\nDeep learning-based automated and universal bubble detection and mask extraction in complex two-phase flows.\nScientific Reports\n, 11(1):8940, 2021.\nL. etÂ al. [2023]\nA.Â Rybak L.â€‰ V.Â Cherkasov V.â€‰ I.Â Malyshev D.â€‰ and G. Carbone.\nBlood serum recognition method for robotic aliquoting using different versions of the yolo neural network.\nIn\nAdvances in Service and Industrial Robotics (RAAD 2023), Mechanisms and Machine Science, Vol. 135\n, pages 150â€“157, 2023.\nLin etÂ al. [2023]\nShaobo Lin, Kun Wang, Xingyu Zeng, and Rui Zhao.\nExplore the power of synthetic data on few-shot object detection.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\n, 2023.\nLin etÂ al. [2014]\nTsungâ€Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C.Â Lawrence Zitnick, and Piotr DollÃ¡r.\nMicrosoft coco: Common objects in context.\narXiv preprint arXiv:1405.0312\n, 2014.\nLin etÂ al. [2017]\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r.\nFocal loss for dense object detection.\narXiv preprint arXiv:1708.02002\n, 2017.\nLiu etÂ al. [2021]\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using shifted windows.\narXiv preprint arXiv:2103.14030\n, 2021.\nMumuni and Mumuni [2022]\nAlhassan Mumuni and Fuseini Mumuni.\nData augmentation: A comprehensive survey of modern approaches.\nArray\n, 16:100258, 2022.\nRadford etÂ al. [2021]\nAlec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision, 2021.\narXiv preprint arXiv:2103.00020.\nRamÃ­rez-Sanz etÂ al. [2023]\nJosÃ©Â Miguel RamÃ­rez-Sanz, Jose-Alberto Maestro-Prieto, Ãlvar Arnaiz-GonzÃ¡lez, and AndrÃ©s Bustillo.\nSemi-supervised learning for industrial fault detection and diagnosis: A review and outlook.\nISA Transactions\n, 143:255â€“270, 2023.\nRedmon etÂ al. [2016]\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.\nYou only look once: Unified, real-time object detection.\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n, pages 779â€“788, 2016.\nRussakovsky etÂ al. [2014]\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, AlexanderÂ C. Berg, and Li Fei-Fei.\nImagenet large scale visual recognition challenge.\narXiv preprint arXiv:1409.0575\n, 2014.\nSimeth etÂ al. [2021]\nAlexej Simeth, Jessica PlaÃŸmann, and Peter Plapper.\nDetection of fluid level in bores for batch size one assembly automation using convolutional neural network.\nIn\nAdvances in Production Management Systems. Artificial Intelligence for Sustainable and Resilient Production Systems (APMS 2021)\n, pages 86â€“93, 2021.\nTan and Le [2019]\nMingxing Tan and QuocÂ V. Le.\nEfficientnet: Rethinking model scaling for convolutional neural networks.\narXiv preprint arXiv:1905.11946\n, 2019.\nTan and Le [2021]\nMingxing Tan and QuocÂ V. Le.\nEfficientnetv2: Smaller models and faster training.\narXiv preprint arXiv:2104.00298\n, 2021.\nTobias and Wahab [2025]\nAlexanderÂ V. Tobias and Adam Wahab.\nAutonomous â€˜self-drivingâ€™ laboratories: a review of technology and policy implications.\nRoyal Society Open Science\n, 12(7):250646, 2025.\nTobin etÂ al. [2017]\nJosh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel.\nDomain randomization for transferring deep neural networks from simulation to the real world, 2017.\narXiv preprint arXiv:1703.06907.\nTom etÂ al. [2024]\nGary Tom, StefanÂ P. Schmid, SterlingÂ G. Baird, Yang Cao, Kourosh Darvish, Han Hao, Stanley Lo, Sergio Pablo-GarcÃ­a, EllaÂ M. Rajaonson, Marta Skreta, Naruki Yoshikawa, Samantha Corapi, GunÂ Deniz Akkoc, Felix Strieth-Kalthoff, Martin Seifrid, and AlÃ¡n Aspuru-Guzik.\nSelf-driving laboratories for chemistry and materials science.\nChemical Reviews\n, 124(16):9633â€“9732, 2024.\nWang etÂ al. [2024]\nZaitian Wang, Pengfei Wang, Kunpeng Liu, Pengyang Wang, Yanjie Fu, Chang-Tien Lu, CharuÂ C. Aggarwal, Jian Pei, and Yuanchun Zhou.\nA comprehensive survey on data augmentation.\narXiv preprint arXiv:2405.09591\n, 2024.\nWu etÂ al. [2023]\nYou Wu, Hengzhou Ye, Yaqing Yang, Zhaodong Wang, and Shuiwang Li.\nLiquid content detection in transparent containers: A benchmark.\nSensors\n, 23(15):6656, 2023.\nYin etÂ al. [2024]\nYanpu Yin, Jiahui Lei, and Wei Tao.\nDetection of liquid retention on pipette tips in high-throughput liquid handling workstations based on improved yolov8 algorithm with attention mechanism.\nElectronics\n, 13(14):2836, 2024.\nZajec etÂ al. [2024]\nPatrik Zajec, JoÅ¾eÂ M. RoÅ¾anec, Spyros Theodoropoulos, Mihail Fontul, Erik Koehorst, BlaÅ¾ Fortuna, and Dunja Mladenic.\nFew-shot learning for defect detection in manufacturing.\nInternational Journal of Production Research\n, 62(19):6979â€“6998, 2024.\nZepel etÂ al. [2020]\nTara Zepel, Veronica Lai, Lars P.Â E. Yunker, and JasonÂ E. Hein.\nAutomated liquid-level monitoring and control using computer vision, 2020.\nChemRxiv preprint 10.26434/chemrxiv.12798143.v1.\nZhang etÂ al. [2021]\nYifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and Jiashi Feng.\nDeep long-tailed learning: A survey.\narXiv preprint arXiv:2110.04596\n, 2021.\n\\thetitle\nSupplementary Material\nSupplementary: Prompt Templates for Virtual Data Generation\nWe generate images with GeminiÂ 2.5 Flash Image in Batch (FILE) mode.\nBelow are the exact\nuser\nprompts sent to the model. Placeholders are\n{liquid_color}\n,\n{level_pct}\n,\n{bubble_count}\n.\nBubble-Present Prompt (parameterized)\nUser prompt:\nUse the provided reference photo of a pipette tip with bubbles to create a photorealistic variation. Only edit the liquid inside the tip. Set the liquid level to\nâˆ¼\n\\sim\n{level_pct}% of the tip length. Set the liquid color to {liquid_color} with realistic translucency/absorption matching the scene illumination. Insert {bubble_count} small, realistic air bubbles inside the liquid column only; bubbles should be spherical to slightly oblate (\nâˆ¼\n\\sim\n0.2--1.5 mm) with correct refraction, soft internal caustics, and specular highlights consistent with scene lighting. Distribute some near the inner wall/meniscus and some in the central volume. Keep the tip geometry, markings, background, camera viewpoint, exposure, depth-of-field, and sensor noise unchanged. Keep the meniscus physically plausible for the chosen level and color. Do not add foam, droplets on the exterior, text, or artifacts. Preserve the original image resolution (e.g.,\n600\nÃ—\n1500\n600{\\times}1500\n) and cropping. Return only the final edited image; no text output.\nBubble-Free Prompt (parameterized)\nUser prompt:\nUse the provided reference photo of a pipette tip to create a photorealistic variation. Only edit the liquid inside the tip. Set the liquid level to\nâˆ¼\n\\sim\n{level_pct}% of the tip length. Set the liquid color to {liquid_color} with realistic translucency/absorption matching the scene illumination. Remove all air bubbles inside the liquid column; the liquid must be perfectly bubble-free. Keep the tip geometry, markings, background, camera viewpoint, exposure, depth-of-field, and sensor noise unchanged. Keep the meniscus physically plausible for the chosen level and color. Do not add foam, droplets on the exterior, text, or artifacts. Preserve the original image resolution (e.g.,\n600\nÃ—\n1500\n600{\\times}1500\n) and cropping. Return only the final edited image; no text output.\nIn both cases, we fix the camera viewpoint and background via the provided\nreference image, randomize\n{liquid_color}\nand\n{level_pct}\n, and\nfor the bubble-present setting additionally randomize\n{bubble_count}\n.",
    "preview_text": "Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.\n\nData-Centric Visual Development for Self-Driving Labs\nAnbang Liu\nâ€ ,1\nGuanzhong Hu\nÂ§,2\nJiayi Wang\nâ€ ,3\nPing Guo\nÂ§,4\nHan Liu\nâ€ ,5\nâ€ \nDepartment of Computer Science, Northwestern University\nÂ§\nDepartment of Mechanical Engineering, Northwestern University\n1\nanbangliu2027@u.northwestern.edu\n2\nguanzhonghu2028@u.northwestern.edu\n3\njiayiwang2020@u.northwestern.edu\n4\nping.guo@northwestern.edu\n5\nhanliu@northwestern.edu\nAbstract\nSelf-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and o",
    "is_relevant": null,
    "relevance_score": 0.0,
    "extracted_keywords": [],
    "one_line_summary": "",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T18:59:57Z",
    "created_at": "2026-01-08T10:08:14.723009",
    "updated_at": "2026-01-08T10:08:14.723019",
    "flag": true
}