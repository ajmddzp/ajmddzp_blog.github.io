{
    "id": "2601.08161v1",
    "title": "Robust Subpixel Localization of Diagonal Markers in Large-Scale Navigation via Multi-Layer Screening and Adaptive Matching",
    "authors": [
        "Jing Tao",
        "Banglei Guan",
        "Yang Shang",
        "Shunkun Liang",
        "Qifeng Yu"
    ],
    "abstract": "æœ¬æ–‡æå‡ºä¸€ç§é²æ£’ã€é«˜ç²¾åº¦çš„å®šä½æ–¹æ³•ï¼Œä»¥è§£å†³å¤§è§„æ¨¡é£è¡Œå¯¼èˆªä¸­å¤æ‚èƒŒæ™¯å¹²æ‰°å¯¼è‡´çš„å®šä½å¤±æ•ˆé—®é¢˜ï¼Œå¹¶å…‹æœä¼ ç»Ÿæ»‘åŠ¨çª—å£åŒ¹é…æŠ€æœ¯å›ºæœ‰çš„è®¡ç®—æ•ˆç‡ä¸è¶³ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŒ…å«å¤šå±‚è§’ç‚¹ç­›é€‰ä¸è‡ªé€‚åº”æ¨¡æ¿åŒ¹é…çš„ä¸‰å±‚æ¡†æ¶ã€‚é¦–å…ˆï¼Œé€šè¿‡å…‰ç…§å‡è¡¡åŒ–ä¸ç»“æ„ä¿¡æ¯æå–å®ç°é™ç»´å¤„ç†ã€‚é‡‡ç”¨ç”±ç²—åˆ°ç²¾çš„å€™é€‰ç‚¹é€‰å–ç­–ç•¥ï¼Œæ˜¾è‘—é™ä½æ»‘åŠ¨çª—å£è®¡ç®—æˆæœ¬ï¼Œå®ç°å¯¹æ ‡è®°ç‰©ä½ç½®çš„å¿«é€Ÿä¼°è®¡ã€‚æœ€åï¼Œä¸ºå€™é€‰ç‚¹ç”Ÿæˆè‡ªé€‚åº”æ¨¡æ¿ï¼Œé€šè¿‡æ”¹è¿›çš„æ¨¡æ¿åŒ¹é…ä¸ç›¸å…³ç³»æ•°æå€¼æ‹Ÿåˆå®ç°äºšåƒç´ çº§ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚å¤§è§„æ¨¡ç¯å¢ƒä¸­èƒ½æœ‰æ•ˆæå–å¹¶å®šä½å¯¹è§’æ ‡è®°ç‰©ï¼Œéå¸¸é€‚ç”¨äºå¯¼èˆªä»»åŠ¡ä¸­çš„è§†åœºæµ‹é‡ã€‚",
    "url": "https://arxiv.org/abs/2601.08161v1",
    "html_url": "https://arxiv.org/html/2601.08161v1",
    "html_content": "Robust Subpixel Localization of Diagonal Markers in Large-Scale Navigation via Multi-Layer Screening and Adaptive Matching\nJing Tao\nCollege of Aerospace Science and Engineering, National University of Defense Technology, Hunan 410073, China\nHunan Provincial Key Laboratory of Image Measurement and Vision Navigation, Hunan 410073, China\nBanglei Guan\nCollege of Aerospace Science and Engineering, National University of Defense Technology, Hunan 410073, China\nHunan Provincial Key Laboratory of Image Measurement and Vision Navigation, Hunan 410073, China\nYang Shang\nCollege of Aerospace Science and Engineering, National University of Defense Technology, Hunan 410073, China\nHunan Provincial Key Laboratory of Image Measurement and Vision Navigation, Hunan 410073, China\nShunkun Liang\nCollege of Aerospace Science and Engineering, National University of Defense Technology, Hunan 410073, China\nHunan Provincial Key Laboratory of Image Measurement and Vision Navigation, Hunan 410073, China\nQifeng Yu\nCollege of Aerospace Science and Engineering, National University of Defense Technology, Hunan 410073, China\nHunan Provincial Key Laboratory of Image Measurement and Vision Navigation, Hunan 410073, China\nAbstract\nThis paper proposes a robust, high-precision positioning methodology to address localization failures arising from complex background interference in large-scale flight navigation and the computational inefficiency inherent in conventional sliding window matching techniques. The proposed methodology employs a three-tiered framework incorporating multi-layer corner screening and adaptive template matching. Firstly, dimensionality is reduced through illumination equalization and structural information extraction. A coarse-to-fine candidate selection strategy minimizes sliding window computational costs, enabling rapid estimation of the markerâ€™s position. Finally, adaptive templates are generated for candidate points, achieving subpixel precision through improved template matching with correlation coefficient extremum fitting. Experimental results demonstrate the methodâ€™s effectiveness in extracting and localizing diagonal markers in complex, large-scale environments, making it ideal for field-of-view measurement in navigation tasks.\n1\nIntroduction\nIn the field of photogrammetry, cooperative markers have become essential for complex measurement tasks, serving as well-defined geometric benchmarks for manual layout\n[\n1\n,\n2\n,\n3\n,\n4\n]\n. Unlike naturally occurring features, these artificial markers effectively counteract challenges arising from environmental illumination fluctuations and background texture interference by virtue of their specially designed encoded patterns (\ne.g.\n, concentric circles\n[\n5\n]\n, checkerboards\n[\n6\n,\n7\n]\n, and recent diagonal coding schemes\n[\n8\n,\n9\n]\n) and precise geometric constraints. They deliver indispensable advantages in mission-critical applications including but not limited to: autonomous spacecraft rendezvous and docking\n[\n10\n,\n11\n]\n, industrial robotic systems requiring high-precision grasping\n[\n12\n,\n13\n]\n, and vision-based navigation for unmanned aerial vehicles\n[\n14\n,\n15\n]\n. Particularly under large field-of-view conditions involving multi-target measurements, the detection efficiency and subpixel localization accuracy of cooperative markers, especially for low-resolution targets\n[\n16\n]\n, directly determine the reliability of subsequent three-dimensional reconstruction and pose estimation processes, thereby constituting a pivotal success factor for photogrammetric mission execution.\nFigure 1:\nFlowchart of the automated diagonal marker extraction method in complex wide-area scenes.\nCurrent research in calibration target detection primarily focuses on circular markers\n[\n17\n,\n18\n,\n19\n]\nand checkerboard patterns\n[\n20\n,\n21\n,\n22\n]\n. Recent advancements have applied deep learning to optimize feature encoding\n[\n23\n]\nand enhance feature robustness\n[\n24\n]\n. Nevertheless, these methods often require extensive training data and exhibit limited interpretability in systems demanding high-precision analysis.\nIn contrast, automated extraction methods for diagonally arranged markers have received relatively little attention. Most existing studies concentrate on controlled indoor calibration environments, which may not be directly applicable to complex outdoor settings\n[\n12\n,\n25\n]\n. Although orthogonal intersecting diagonal markers offer structural simplicity and scalability, their practical application faces significant challenges: 1) In outdoor environments, dynamic illumination changes and background interference can degrade marker features. This necessitates advanced photometric adaptation mechanisms\n[\n26\n]\n; 2) For large field-of-view applications where marker regions are only 3â€“5 pixels wide, existing subpixel methods struggle with background noise suppression.\nTo enable the efficient and stable extraction of cooperative markers in wide-area vision, this study systematically examines the gradient distribution characteristics of diagonal markers. As shown in Fig. 1, we propose a novel cooperative localization algorithm that integrates a multi-layer corner screening mechanism with adaptive template matching. Experimental results show that the proposed method enhances detection robustness in complex scenarios while maintaining subpixel accuracy. It also provides a feasible technical solution for large-scale photogrammetry. The key contributions of this paper are threefold:\nâˆ™\n\\bullet\nA novel large-scale positioning framework that integrates multi-layer corner screening with adaptive template matching, overcoming traditional localization failures in complex, large-scale environments.\nâˆ™\n\\bullet\nAn optimized template matching framework that integrates coarse-to-fine hierarchical positioning with advanced matching techniques, overcoming the efficiency bottlenecks typically encountered in conventional two-stage sliding registration methods.\nâˆ™\n\\bullet\nA geometry-driven dynamic template generation algorithm that incorporates angular constraints and grayscale distribution modeling. This adaptive system achieves subpixel positioning accuracy under extreme changes in view angles and illumination conditions.\n2\nRelated Work\nSubpixel positioning of feature markers is vital in digital photogrammetry as it enables high-precision measurements. In 1973, Hueckel\net al.\nintroduced the first subpixel edge detection algorithm\n[\n27\n]\n, which detects edges at the subpixel level without increasing camera resolution, thereby reducing system costs. Over the years, more advanced subpixel positioning algorithms, such as gradient-based and least-squares fitting methods\n[\n28\n,\n29\n,\n30\n]\n, have been developed and widely applied in industrial inspection and remote sensing.\nSeveral methods have also been proposed for diagonal marker extraction. Shang\net al.\n[\n31\n]\nintroduced the first automatic recognition framework for circular diagonal markers, providing a new approach to high-precision positioning. Wang\net al.\n[\n32\n]\nproposed the Robust Subpixel Feature Extraction Method (RSFEM), combining template matching and gradient analysis to improve robustness and accuracy in complex environments. Cai\net al.\n[\n33\n]\ndeveloped a circular diagonal marker extraction method using the Hough circle detection algorithm for challenging environments. However, these methods still struggle with issues like significant background noise and varying lighting conditions, which compromise accuracy and efficiency. Therefore, developing more effective diagonal marker extraction algorithms remains a key focus.\nThis paper presents a high-precision visual localization framework that effectively tackles two primary challenges in large-scale navigation: background interference and computational efficiency in sliding window matching. Unlike Ref.\n32\n, our method abandons template matching in the coarse positioning phase, enabling the detection of more potential candidate points in interference scenarios and greatly improving operational efficiency. We also propose an adaptive template generation method for fine positioning, which enhances the performance of the algorithm. Overall, our framework shows more advantages in computational efficiency and positioning accuracy in large-field-of-view flight scenarios, making it more suitable for high-precision navigation systems on flight platforms.\n3\nMethod\nThe overall workflow of the algorithm, illustrated in Fig. 1, comprises three core modules: image preprocessing, multi-layer point screening, and subpixel location. The core concept involves first suppressing background interference via image preprocessing to facilitate subsequent extraction. This is followed by coarse positioning to obtain candidate point locations, and finally, fine positioning to achieve subpixel accuracy.\n3.1\nImage Preprocessing\nTo mitigate the challenges of non-uniform illumination and noise interference in diagonal marker extraction within complex operational environments, this study introduces a multi-stage collaborative preprocessing framework.\nIn the initial processing stage, a noise suppression model is constructed using the Gradient-Domain Weighted Guided Image Filter (GDWGIF)\n[\n34\n]\n. The associated energy function is formulated as follows:\nE\n=\nmin\nâ€‹\nâˆ‘\ni\nâˆˆ\nÎ©\nk\n(\n(\na\nk\nÃ—\nI\ni\n+\nb\nk\nâˆ’\nq\ni\n)\n2\n+\nÎ»\nT\n^\nI\nâ€‹\n(\nk\n)\nÃ—\n(\na\nk\nâˆ’\nÏˆ\nk\n)\n2\n)\n,\nE=\\min\\sum\\limits_{i\\in{\\Omega_{k}}}{({{({a_{k}}\\times{I_{i}}+{b_{k}}-{q_{i}})}^{2}}}+\\frac{\\lambda}{{{{\\hat{T}}_{I}}(k)}}\\times{({a_{k}}-{\\psi_{k}})^{2}}),\n(1)\nwhere\nq\nq\ndenotes the guidance image,\nI\nI\nrepresents the input image,\na\nk\n,\nb\nk\n{a_{k},b_{k}}\nare linear coefficients within local window\nÎ©\nk\n{\\Omega_{k}}\n,\nT\n^\nI\nâ€‹\n(\nk\n)\n\\hat{T}_{I}(k)\nconstitutes the gradient magnitude-based edge-aware weighting function,\nÏˆ\nk\n\\psi_{k}\nembodies the gradient domain constraint term, and\nÎ»\n\\lambda\nis the regularization parameter.\nDistinguished from conventional guided filtering approaches\n[\n35\n,\n36\n]\n, GDWGIF integrates gradient-domain constraints with adaptive weighting, thereby achieving superior noise attenuation while simultaneously preserving edge integrity and mitigating over-smoothing artifacts. The algorithm details can be found in the authorâ€™s previous work Ref.\n34\n.\nFigure 2:\nComparison of annular gradient scanning curves: (a) Grayscale curve of a pseudo-diagonal marker; (b) Grayscale curve of a diagonal marker.\nTo counteract luminance degradation induced by heterogeneous illumination in field measurement scenarios, a dynamic gamma correction model is implemented for intensity normalization. The model modulates local exposure through a nonlinear mapping between the local neighborhood luminance mean and the gamma parameter:\nL\n^\nk\nâ€²\n=\n[\nL\n^\nk\n]\nÎ³\n,\nÎ³\n=\nÎ±\nâ‹…\nÎ¼\nâ€‹\n(\nk\n)\n,\n{\\hat{L}^{\\prime}_{k}}={[{\\hat{L}_{k}}]^{\\gamma}},\\gamma=\\alpha\\cdot\\mu(k),\n(2)\nwhere\nL\nk\n{L_{k}}\nsignifies the initial illumination layer,\nÎ¼\nâ€‹\n(\nk\n)\n\\mu(k)\ncorresponds to the local neighborhood luminance mean, and\nÎ±\n\\alpha\nfunctions as a contrast-adaptive scaling factor.\nUnlike conventional global gamma correction, which uses a fixed gamma value and struggles with illumination heterogeneity, our approach offers significant advantages through local adaptability and dynamic parameterization: 1) Gamma values are dynamically adjusted for each region based on local luminance, enabling tailored correction for overexposed and underexposed areas; 2) The contrast-adaptive scaling factor optimizes local contrast during normalization, preventing global contrast imbalance; 3) The model suppresses grayscale saturation in overexposed regions and compression in underexposed regions, reducing luminance distortion while preserving continuity across regional boundaries. Consequently, the model demonstrates superior performance in intensity normalization accuracy and enhanced detail preservation capabilities under complex lighting conditions.\nTo achieve efficient wide-area feature localization, data dimensionality reduction is achieved through structural information exploitation. As established by Ref.\n37\n, structural components encapsulate the predominant information content of images. Based on this, we propose a structural component-based information condensation method that uses multi-scale neighborhood analysis:\ns\nk\n=\n[\n1\nâˆ’\nexp\n(\nâˆ’\n(\nÏƒ\nd\n,\nk\nÎ¾\n)\nt\n]\n,\n{s_{k}}=[1-\\exp(-{(\\frac{{{\\sigma_{d}}_{,k}}}{\\xi})^{t}}],\n(3)\nwhere\nd\nd\nindicates the neighborhood diameter across scales,\nÏƒ\nd\n,\nk\n\\sigma_{d,k}\ndenotes local standard deviation, and\nt\nt\nregulates structural sensitivity. Parameter configurations with\nt\n>\n1\nt>1\nenhance high-frequency edge features, whereas\n0\n<\nt\n<\n1\n0<t<1\nsuppresses noise interference. As demonstrated in Fig.\n1\n(b), this method adaptively enhances salient structural features through nonlinear contrast mapping, generating a high signal-to-noise ratio feature space conducive to precise diagonal marker localization.\nFigure 3:\nSelf-adaptive template generation diagram: (a) Determination of template parameters; (b) Template generation; (c) Correlation coefficient graph.\n3.2\nMulti-layer Point Screening\nGiven the geometric and imaging characteristics of diagonal markers, we propose a multi-layer point screening framework (see Fig.\n1\n(c)). This framework first utilizes the prominent corner responses in the central region of diagonal markers in gradient space. A conservative detection strategy based on curvature analysis ensures the completeness of candidate points. By establishing a vector space curvature model (Eq. (\n4\n)), we quantify the local curvature features of contour points using the dot product of neighborhood displacement vectors. Normalization ensures rotation-invariant detection. The mathematical expression is as follows:\nÎº\nâ€‹\n(\np\nk\n)\n=\n1\nâˆ’\nâŸ¨\nv\nâ†’\np\nâ€‹\nr\nâ€‹\ne\nâ€‹\nv\n,\nv\nâ†’\nn\nâ€‹\ne\nâ€‹\nx\nâ€‹\nt\nâŸ©\nâ€–\nv\nâ†’\np\nâ€‹\nr\nâ€‹\ne\nâ€‹\nv\nâ€–\nâ‹…\nâ€–\nv\nâ†’\nn\nâ€‹\ne\nâ€‹\nx\nâ€‹\nt\nâ€–\n+\nÎµ\n.\n\\kappa({p_{k}})=1-\\frac{{\\left\\langle{{{\\overrightarrow{v}}_{prev}},{{\\overrightarrow{v}}_{next}}}\\right\\rangle}}{{\\left\\|{{{\\overrightarrow{v}}_{prev}}}\\right\\|\\cdot\\left\\|{{{\\overrightarrow{v}}_{next}}}\\right\\|+\\varepsilon}}.\n(4)\nHere,\nv\nâ†’\np\nâ€‹\nr\nâ€‹\ne\nâ€‹\nv\n{\\overrightarrow{v}_{prev}}\nand\nv\nâ†’\nn\nâ€‹\ne\nâ€‹\nx\nâ€‹\nt\n{\\overrightarrow{v}_{next}}\nare the displacement vectors within radius\nr\nr\nof the current pointâ€™s front and rear windows, and\nÎµ\n\\varepsilon\nis a small constant for stability. Matrix operations optimize curvature calculations, enhancing computational efficiency by reducing complexity.\nIn the primary screening stage, we adopt a dual-threshold mechanism. The peak curvature must exceed three times the standard deviation of the mean, and adjacent peaks must be at least five pixels apart. Combining this with structural component data from preprocessing, we retain only candidate points in the highest 70% percentile of structural components. Additionally, we propose a noise-aware dynamic region verification criterion. By analyzing connected-domain area distribution and structural noise levels, we establish an adaptive threshold function to effectively eliminate anomalous interference regions.\nTraditional corner detection methods tend to produce excessive feature points in areas with complex textures. To address this issue, we introduce a maximum suppression algorithm based on curvature density analysis. This algorithm combines spatial distribution entropy optimization with adaptive density estimation for intelligent sparse processing of feature points.\nTo begin with, we construct a curvature-weighted spatial density field, which is mathematically defined as:\nÏ\nâ€‹\n(\np\ni\n)\n=\nâˆ‘\nj\nâˆˆ\nğ’©\nâ€‹\n(\ni\n)\nÎº\nâ€‹\n(\np\nj\n)\nâ‹…\nexp\nâ¡\n(\nâˆ’\nâ€–\np\ni\nâˆ’\np\nj\nâ€–\n2\n2\nâ€‹\nÏƒ\n2\n)\n,\n\\rho({p_{i}})=\\sum\\limits_{j\\in{\\cal N}(i)}\\kappa({p_{j}})\\cdot\\exp\\left({-\\frac{{{{\\left\\|{{p_{i}}-{p_{j}}}\\right\\|}^{2}}}}{{2{\\sigma^{2}}}}}\\right),\n(5)\nhere,\nj\nj\ndenotes the index of a neighborhood point around center\ni\ni\n, and\nÎº\nâ€‹\n(\np\nj\n)\n\\kappa({p_{j}})\nrepresents the normalized curvature value.\nOur approach innovates by integrating a curvature weight coefficient into the density estimation process, unlike the conventional mean shift algorithm\n[\n38\n]\n. This allows feature points with higher significance to dominate the density estimation.\nDuring the iterative filtering phase, we apply an improved mean shift strategy to update the positions of candidate points:\np\ni\nn\nâ€‹\ne\nâ€‹\nw\n=\nâˆ‘\nw\nj\nâ€‹\np\nj\nâˆ‘\nw\nj\n,\nw\nj\n=\nÏ\nâ€‹\n(\np\nj\n)\nâ‹…\ne\nâˆ’\nâ€–\np\nj\nâˆ’\np\ni\nâ€–\n/\nh\n.\np_{i}^{new}=\\frac{{\\sum{{w_{j}}{p_{j}}}}}{{\\sum{{w_{j}}}}},{w_{j}}=\\rho({p_{j}})\\cdot{e^{-\\left\\|{{p_{j}}-{p_{i}}}\\right\\|/h}}.\n(6)\nIn this formula, the curvature density weight\nw\nj\n{w_{j}}\ndirects feature points towards salient regions, supported by an exponential decay term. The parameter\nh\nh\n, representing the attenuation coefficient, controls the convergence rate. To enhance spatial distribution, we define spatial grid entropy as:\nH\n=\nâˆ’\nâˆ‘\nP\nâ€‹\n(\nn\n)\nâ€‹\nln\nâ¡\nP\nâ€‹\n(\nn\n)\n.\nH=-\\sum{P(n)\\ln P(n)}.\n(7)\nHere,\nP\nâ€‹\n(\nk\n)\n{P(k)}\nrepresents the distribution probability of points within the\nn\nn\n-th grid. By employing a greedy strategy, the algorithm iteratively removes redundant points that yield the maximum entropy increment\nÎ”\nâ€‹\nH\n\\Delta H\nuntil a predefined entropy threshold is attained. This process effectively reduces spatial overlap while preserving key features, thereby streamlining subsequent candidate point selection.\nDiagonal markers maintain two geometric invariants under interference: 1) Center symmetry: Adjacent quadrants exhibit complementary grayscale distributions; 2) Double line intersection: A stable gradient extremum forms along the diagonal direction. To verify these invariants, we design a geometric checker that combines grayscale analysis with LSD line detection\n[\n39\n]\n, implementing a two-stage screening process to ensure structural consistency.\n1) Center symmetry:\nThis paper introduces a symmetry verification method based on annular gradient scanning (see Fig.\n2\n) to address the grayscale distribution of candidate regions. A polar coordinate system is centered on the candidate points, with sampling performed at 10Â° intervals along an adaptive radius. The grayscale distribution is processed through first-order differentiation and Gaussian difference filtering, transforming extremum detection into zero-crossing identification. This enhances sensitivity to weak edges. The derivative curve of the true diagonal exhibits strict bimodal symmetry (Fig.\n2\n(b)), with a phase difference near 180Â° and opposite gradient polarity, indicating centrosymmetry. In contrast, pseudo-feature points exhibit multi-peak clutter or a dominant single-peak mode (Fig.\n2\n(a)). By applying phase difference and gradient polarity constraints, this method reduces reliance on absolute light intensity in traditional grayscale matching, ensuring high verification accuracy even in the presence of noise interference.\n2) Double line intersection:\nTo efficiently detect straight lines, this study employs the LSD method\n[\n39\n]\n. By verifying intersections of detected lines, the method refines candidate points and determines angle parameters, assisting in the generation of adaptive templates. Specifically, it extracts straight-line features from the local area around the candidate point to check for nearby intersections of two lines. When the detection area contains two straight lines, a line-extension strategy is applied to handle such cases effectively.\nl\ni\nâ€²\n=\nE\nâ€‹\nx\nâ€‹\nt\nâ€‹\ne\nâ€‹\nn\nâ€‹\nd\nâ€‹\n(\nl\ni\n,\nÎ”\nâ€‹\ns\n)\n,\nÎ”\nâ€‹\ns\n=\n5\n.\n{l_{i}}^{\\prime}=Extend({l_{i}},\\Delta s),\\Delta s{\\rm{=5}}.\n(8)\nHere,\nâ„“\n=\n{\nl\ni\n}\n\\ell=\\{{l_{i}}\\}\nrepresents the set of detected straight-line segments, and\nE\nâ€‹\nx\nâ€‹\nt\nâ€‹\ne\nâ€‹\nn\nâ€‹\nd\nâ€‹\n(\n)\nExtend()\nextends these segments by\nÎ”\nâ€‹\ns\n\\Delta s\npixels at both ends. This ensures that potential intersection points are detected, even if the lines are incomplete. To verify the intersection, we compute the Euclidean distance between the intersection of the extended lines and the candidate point. If the distance is below a threshold, the candidate point is retained; otherwise, it is discarded. This process ensures geometric consistency and provides necessary angle parameters for template generation. Experimental results show that this method significantly improves candidate point screening accuracy in complex scenes, offering robust geometric support for template matching.\n3.3\nSubpixel Location\n3.3.1\nTemplate Adaptive Generation\nTemplate matching generally involves feature matching in image space by utilizing a predefined target feature template. However, conventional methods often necessitate the generation of multiple sets of rotated and scaled templates to perform exhaustive matching, which not only leads to significant computational redundancy but also increases the likelihood of mismatches. To tackle these challenges, this study introduces an innovative adaptive template generation framework that is based on the coupling of geometric and photometric information. This approach effectively provides high-precision initial conditions for subpixel positioning, ensuring improved accuracy and efficiency in template matching processes.\nFigure 4:\nImaging diagram of the actual project.\nFigure 5:\nStability test. (a) Different scenarios; (b) Different perspectives; (c) Complex environment.\nThe core parameters for template generation are derived from geometric and photometric constraints. Using the two straight-line angle parameters,\nÏ•\n1\n{\\phi_{1}}\nand\nÏ•\n2\n{\\phi_{2}}\n, extracted in Section 3.2, we calculate the template angle as\nÎ¸\n=\nÏ•\n1\nâˆ’\nÏ•\n2\n\\theta={\\phi_{1}}-{\\phi_{2}}\n. To model the grayscale distribution, we create a template that simulates the actual diagonal mark. Specifically, within the local coordinate system, we design a\n3\nÃ—\n3\n3\\times 3\nsampling window, offset by 5 pixels along the direction angle\nÎ¸\n/\n2\n\\theta/2\n(illustrated as the blue cross in Fig.\n3\n(a)). To better align the template with real-world imaging conditions, we apply a defocus degradation model based on the optical system. Using the blur parameter\nÏƒ\nb\n{\\sigma_{b}}\n, estimated from the Point Spread Function (PSF), we perform Gaussian convolution on the template to simulate defocus blur. This frequency-domain convolution ensures that the templateâ€™s high-frequency features are spectrally consistent with the actual image.\nTo resolve the matching ambiguity caused by the rotational symmetry of the template (as shown in the comparison between template 1 and template 2 in Fig.\n3\n(b)), we introduce a criterion based on feature space projection:\nm\nâˆ—\n=\narg\nâ¡\nmin\nm\nâˆˆ\n{\n1\n,\n2\n}\nâ¡\nâ€–\nP\nâˆ’\nT\n(\nm\n)\nâ€–\n1\n,\n{m^{*}}=\\arg{\\min_{m\\in\\{1,2\\}}}{\\left\\|{{\\rm{P-}}{{\\rm{T}}^{(m)}}}\\right\\|_{1}},\n(9)\nwhere\nP\n{\\rm{P}}\nrepresents the local image block sampling vector, and\nT\n(\nm\n)\n{{\\rm{T}}^{(m)}}\ndenotes the sampling vector of the two possible templates. To assess the effectiveness of the generated templates, the correlation coefficient curves for the two templates are shown in Fig.\n3\n(c). The results indicate that the adaptive template exhibits a more ideal single-peak characteristic, improving the accuracy of subsequent subpixel positioning.\nThis adaptive template generation framework not only significantly reduces computational complexity but also enhances the accuracy and robustness of template matching, offering reliable support for subpixel positioning in complex scenes.\n3.3.2\nSubpixel Positioning Techniques\nSubpixel target localization represents a significant advancement in enhancing the precision of cooperative marker measurement systems, offering substantial theoretical and practical engineering value. After the initial coarse positioning of feature points, further refinement of their coordinates within the localized extraction region is essential to achieve subpixel accuracy. Traditional methods that employ Normalized Cross-Correlation (NCC) template matching are computationally constrained due to their inherent mechanisms. Specifically, the sliding-window approach necessitates pixel-wise traversal to generate correlation coefficient matrices, resulting in prohibitive computational complexity that renders real-time implementation impractical. Drawing on the findings of Ref.\n40\n, this study introduces an accelerated matching framework. By integrating feature recombination, matrix operations, and quadratic surface extremum fitting, this framework achieves efficient subpixel positioning.\nThe core acceleration strategy reconfigures the neighborhood mapping operation through tensor reshaping. Given a template window of size\n(\n2\nâ€‹\nr\n+\n1\n)\nÃ—\n(\n2\nâ€‹\nr\n+\n1\n)\n(2r+1)\\times(2r+1)\nand an input feature map\nF\nâˆˆ\nR\nH\nÃ—\nW\nÃ—\nd\nF\\in{{\\rm{R}}^{H\\times W\\times d}}\n, we construct the expanded feature tensor:\nF\n~\nâ€‹\n(\nx\n,\ny\n,\n(\nz\nâˆ’\n1\n)\nâ€‹\nr\n2\n+\n(\ni\nâˆ’\n1\n)\nâ€‹\nr\n+\nj\n)\n=\nF\nâ€‹\n(\nx\n+\ni\nâˆ’\nr\n,\ny\n+\nj\nâˆ’\nr\n,\nz\n)\n,\n\\tilde{F}(x,y,(z-1){r^{2}}+(i-1)r+j)=F(x+i-r,y+j-r,z),\n(10)\nwhere\nF\n~\nâˆˆ\nR\nH\nÃ—\nW\nÃ—\n(\nr\n2\nâ€‹\nd\n)\n\\tilde{F}\\in{{\\rm{R}}^{H\\times W\\times({r^{2}}d)}}\n,\n(\nH\n,\nW\n,\nd\n)\n(H,W,d)\nrepresent the spatial dimensions and feature dimension,\n(\ni\n,\nj\n)\n(i,j)\nare the template coordinates, and\nz\nz\nis the feature dimension index. The NCC calculation between the reference template\nF\n~\nr\nâ€‹\ne\nâ€‹\nf\n{\\tilde{F}_{ref}}\nand the target region\nF\n~\nt\nâ€‹\na\nâ€‹\nr\n{\\tilde{F}_{tar}}\nis then reformulated as matrix multiplication by reshaping the expanded tensor:\nR\n=\nF\n~\nr\nâ€‹\ne\nâ€‹\nf\nâ‹…\nF\n~\nt\nâ€‹\na\nâ€‹\nr\nâ€–\nF\n~\nr\nâ€‹\ne\nâ€‹\nf\nâ€–\n2\nâ€‹\nâ€–\nF\n~\nt\nâ€‹\na\nâ€‹\nr\nâ€–\n2\n.\nR=\\frac{{{{\\tilde{F}}_{ref}}\\cdot{{\\tilde{F}}_{tar}}}}{{{{\\left\\|{{{\\tilde{F}}_{ref}}}\\right\\|}_{2}}{{\\left\\|{{{\\tilde{F}}_{tar}}}\\right\\|}_{2}}}}.\n(11)\nThis transformation reduces the time complexity from\nO\nâ€‹\n(\nM\nâ€‹\nN\nâ€‹\nk\n2\n)\nO(MN{k^{2}})\nto\nO\nâ€‹\n(\nM\nâ€‹\nN\n+\nk\n2\n)\nO(MN+{k^{2}})\n, significantly improving computational efficiency.\nAfter obtaining the correlation coefficient matrix\nR\nR\n, a local neighborhood quadratic surface model is constructed:\nc\nâ€‹\n(\nx\n,\ny\n)\n=\na\nâ€‹\nx\n2\n+\nb\nâ€‹\ny\n2\n+\nc\nâ€‹\nx\nâ€‹\ny\n+\nd\nâ€‹\nx\n+\ne\nâ€‹\ny\n+\nf\n,\nc(x,y)=a{x^{2}}+b{y^{2}}+cxy+dx+ey+f,\n(12)\nwhere the weighted least squares method is applied to solve for the coefficients\n{\na\n,\nb\n,\nc\n,\nd\n,\ne\n,\nf\n}\n\\{a,b,c,d,e,f\\}\n, and the subpixel offset is determined as follows:\nÎ”\nâ€‹\nx\n=\n2\nâ€‹\nb\nâ€‹\nc\nâˆ’\nc\nâ€‹\ne\nc\n2\nâˆ’\n4\nâ€‹\na\nâ€‹\nb\n,\nÎ”\nâ€‹\ny\n=\n2\nâ€‹\na\nâ€‹\ne\nâˆ’\nc\nâ€‹\nd\nc\n2\nâˆ’\n4\nâ€‹\na\nâ€‹\nb\n.\n\\Delta x=\\frac{{2bc-ce}}{{{c^{2}}-4ab}},\\Delta y=\\frac{{2ae-cd}}{{{c^{2}}-4ab}}.\n(13)\nBy applying these offsets to the coarse positioning results, high-precision target position correction is achieved, completing the subpixel localization. The experimental results, shown in Fig.\n1\n(e), demonstrate that the red dot position is significantly more accurate than the blue dot position.\nTable 1:\nDeviations of checkerboard corner detection by two methods\nImage number\nMaximum deviations/pixel\nAverage deviations/pixel\nDeviations\nâ‰¤\n0.1\n\\leq 0.1\npixel/%\n1\n0.2512\n0.0181\n98.57\n2\n0.128\n0.0146\n99.47\n3\n0.2094\n0.0185\n98.04\n4\n0.3457\n0.0211\n97.93\n5\n0.1582\n0.0151\n98.74\n6\n0.0924\n0.01\n100\n4\nExperiment\nThis study evaluates the proposed diagonal marker extraction algorithm using both a laboratory-constructed multi-form diagonal sign dataset and real-world engineering measurement data. To ensure scientific rigor and comparability, we benchmark the performance of the proposed algorithm against Zhangâ€™s checkerboard point extraction algorithm\n[\n39\n]\n, which is widely recognized for its precision and broad applicability, particularly in camera calibration tasks. This algorithm serves as an ideal comparison for validating the accuracy of the proposed method.\nFigure 6:\nThe sample images obtained from the flight experiment and the results of marker extraction.\nIn addition to the laboratory dataset, we further assess the algorithmâ€™s accuracy and robustness using engineering measurement data obtained from a multi-aperture flight platform. Fig.\n4\nillustrates the engineering scene captured by the platform. The data acquisition process involves an Unmanned Air Vehicle (UAV) equipped with a multi-aperture imaging system, which captures aerial images of a ground-based cooperative target. The system features a single camera with a resolution of 4096 Ã— 3000 pixels, paired with a 150 mm fixed-focus lens. The algorithm is implemented in the Matlab R2021a development environment.\n4.1\nRobustness Verification\nTo validate the algorithmâ€™s robustness in complex scenarios, we conducted comprehensive experiments across three critical dimensions: adaptability to multi-form cooperative markers, stability under perspective variations, and performance in challenging field environments. Fig.\n5\nillustrates the feature extraction performance under various test conditions.\nFig.\n5\n(a) shows the detection results for multi-scenario cooperative markers, including standard checkerboard patterns and specially designed diagonal markers. The algorithm effectively handles diverse structural configurations by leveraging geometric constraints of diagonal features. Fig.\n5\n(b) demonstrates the algorithmâ€™s stable positioning accuracy under varying observation angles, despite target deformation and scale changes caused by perspective projection. Fig.\n5\n(c) highlights the systemâ€™s performance in outdoor environments, successfully addressing challenges such as wide-field imaging, illumination variations, background interference, and motion blur.\nThe experimental results show that the proposed algorithm achieves robust multi-scenario extraction by simply inputting the targetâ€™s initial physical dimensions, eliminating the need for manual parameter tuning. It shows superior adaptability to varying geometries, wide angles, and dynamic interference, proving its practical value in engineering. In contrast, mainstream methods primarily focus on checkerboard pattern detection, with limited capability for diagonal marker localization in complex environments. For instance, Zhangâ€™s checkerboard corner detection algorithm\n[\n39\n]\nand existing diagonal marker methods\n[\n32\n,\n33\n]\nfail to reliably position diagonal markers in the challenging scenarios shown in Fig.\n5\n(c).\nFigure 7:\nMeasurement error results of the two methods in flight experiments.\nFigure 8:\nComparison of the results of the two methods under flight experiments: (a) positioning errors; (b) algorithm processing time.\n4.2\nPrecision Verification\n4.2.1\nCheckerboard Positioning\nTo validate the positioning accuracy of the proposed algorithm, we conducted comparative experiments under controlled laboratory conditions using identical checkerboard patterns. As shown in Fig.\n5\n(b), both our method and Zhangâ€™s conventional algorithm\n[\n39\n]\nwere applied to extract corner coordinates from standardized calibration images. The quantitative evaluation was performed by analyzing the differences in detected corner positions between the two methods.\nAs detailed in Table\n1\n, the maximum deviations observed for both methods approach 0.3 pixels, which can be attributed to angular distortions of the checkerboard pattern, especially at the edge corner points. However, overall, the proposed method demonstrates an average deviation of less than 0.03 pixels, with over 97% of corner points deviating by less than 0.1 pixels. These results show that the proposed methodology provides positioning precision comparable to Zhangâ€™s established corner extraction approach\n[\n39\n]\nin controlled environments, meeting the stringent accuracy requirement (\nâ‰¤\n0.1\n\\leq 0.1\npixel) for practical engineering applications.\n4.2.2\nFlight Platforms Positioning\nAs depicted in fig.\n6\n, the multi-aperture aerial positioning system employs five synchronized long-focus optical sensors to capture images of ground-based cooperative targets, achieving spatial positioning for the flight platform through an absolute pose estimation algorithm. To evaluate the performance of the proposed adaptive positioning algorithm under dynamic flight conditions, a quantitative comparison of positioning errors was conducted among three methods: manual feature extraction\n[\n31\n]\n, the proposed automatic feature extraction method, and reference measurements from the Position and Orientation System (POS).\nFig.\n7\nshows the accuracy assessment of our feature extraction algorithm using real-flight imagery. Positional precision is evaluated by the Euclidean distance between the derived coordinates and ground-truth positions. For directional accuracy (specifically azimuth/orientation), the angular error is calculated as follows: First, the re-projection error (in pixels) is determined. This pixel error is then converted to an angular error (in radians) by multiplying it with the cameraâ€™s angular resolution, which is intrinsically linked to the calibrated focal length of the optics. Finally, the angular error is typically reported in arc-minutes for clarity. The detailed mathematical derivation and specific steps of this angular error calculation process can be found in Ref\n41\n. The error envelope and curve in Fig.\n7\ndemonstrate that our automatic feature extraction method matches the accuracy of manual extraction, exhibiting a slightly narrower fluctuation range which indicates improved robustness. These results confirm that the proposed positioning algorithm achieves centimeter-level spatial accuracy and arc-minute-level orientation accuracy, proving its effectiveness in dynamic aerial environments.\nTo comprehensively evaluate the robustness of the algorithm, this study compares the proposed method with a previously developed automatic corner extraction algorithm RSFEM\n[\n32\n]\n. The comparison reveals that the template matching method in the RSFEMâ€™s coarse positioning process performs poorly when the flight platformâ€™s imaging quality is subpar. To ensure a fair and feasible comparison, we incorporate local region of interest (ROI) segmentation and coarse position guidance into the process. As depicted in Fig.\n8\n(a), the error distribution profiles indicate that our method consistently achieves subpixel precision, even in challenging field conditions. Quantitative analysis reveals significant improvements: approximately a 29% reduction in mean coordinate deviation and a 15% decrease in dispersion metrics. These enhancements are attributed to our scene-adaptive template generation framework, which dynamically adjusts the matching mode based on environmental feedback. This innovative approach effectively reduces the sensitivity of traditional algorithms to background clutter, significantly enhancing the positioning systemâ€™s reliability in complex environments.\n4.3\nComputational Efficiency\nTo assess the real-time processing capabilities, we conducted comparative benchmarking of computational efficiency across different methods using standardized hardware configurations. As shown in Fig.\n8\n(b), our method demonstrates faster processing throughput than RSFEM\n[\n32\n]\nin large-scale scenarios (4096Ã—3000 resolution). Notably, while scene complexity significantly affects the efficiency of diagonal marker localization in traditional approaches, our method maintains less than 12% variance in efficiency across test sequences. These results underscore the superior computational stability of our method, with processing time deviations 38% lower than those of the comparison methods. Future work will focus on implementing CUDA-accelerated parallel computing architectures and CPU SIMD optimization techniques to further enhance real-time performance in task-critical applications.\n5\nConclusion\nThis study tackles two key challenges in wide-area aerial navigation: positioning failures caused by complex background interference and the inefficiency of traditional sliding-window matching in large-scale scenarios. We propose a diagonal marker positioning method that combines multi-layer corner screening and adaptive template matching in a three-tiered processing framework. First, data is simplified through illumination equalization and structural information extraction. Next, a coarse-to-fine candidate point screening strategy reduces computational overhead and efficiently estimates the rough marker position. Finally, adaptive template generation and an improved matching algorithm enable subpixel-level positioning using the fitting correlation coefficient extremum method. Experimental results demonstrate the stability of the proposed method in complex, wide-area scenes. Our algorithm outperforms existing methods in accuracy and robustness, effectively addressing positioning challenges in dynamic flight platforms under strong interference. This research not only advances feature extraction theory in complex scenes but also lays a foundation for UAV autonomous navigation and aerial surveying applications.\nIn future work, we aim to further enhance the applicability and real-time performance of the algorithm, with the goal of achieving versatile performance across multiple scenarios. Additionally, we plan to integrate deep learning algorithms to further improve the robustness of feature extraction.\nAcknowledgement\nThis work was supported by the Hunan Provincial Natural Science Foundation for Excellent Young Scholars (Grant No. 2023JJ20045), the Foundation of National Key Laboratory of Human Factors Engineering (Grant No. GJSD22006) , and the National Natural Science Foundation of China (Grant No. 12372189).\nDisclosures\nThe authors declare no conflicts of interest.\nData Availability\nData underlying the results presented in this paper are not publicly available at this time, but may be obtained from the authors upon reasonable request.\nReferences\nXie etÂ al. [2025]\nZ.Â Xie, Y.Â Liu, and D.Â Li.\nA Scalable Tracking System Using Sparse Dot-Based Markers.\nIn\nIEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)\n, pages 1516â€“1517, 2025.\ndoi:\n10.1109/VRW66409.2025.00404\n.\nLei etÂ al. [2025]\nT.Â Lei, B.Â Guan, M.Â Liang, X.Â Li, J.Â Liu, J.Â Tao, Y.Â Shang, and Q.Â Yu.\nEvent-based multi-view photogrammetry for high-dynamic, high-velocity target measurement.\narXiv e-prints\n, arXiv:2506.00578, 2025.\ndoi:\n10.48550/arXiv.2506.00578\n.\nKrogius etÂ al. [2019]\nM.Â Krogius, A.Â Haggenmiller, and E.Â Olson.\nFlexible Layouts for Fiducial Tags.\nIn\nProceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, 2019.\nWang etÂ al. [2024a]\nS.Â Wang, Q.Â Zhao, D.Â Li, Y.Â Hu, M.Â Zhu, F.Â Yuan, J.Â Shao, and J.Â Yu.\nSpatially Compact Visual Navigation System for Automated Suturing Robot Toward Oral and Maxillofacial Surgery.\nIEEE Transactions on Instrumentation and Measurement\n, 73:1â€“12, 2024.\ndoi:\n10.1109/TIM.2024.3427843\n.\nNegre etÂ al. [2008]\nA.Â Negre, C.Â Pradalier, and M.Â Dunbabin.\nRobust vision-based underwater homing using self-similar landmarks.\nJ. Field Robot.\n, 25(6â€“7):360â€“377, 2008.\nZhang etÂ al. [2011]\nZ.Â Zhang, Y.Â Matsushita, and Y.Â Ma.\nCamera calibration with lens distortion from low-rank textures.\nIn\nCVPR 2011\n, pages 2321â€“2328, 2011.\ndoi:\n10.1109/CVPR.2011.5995548\n.\nKang etÂ al. [2021]\nS.Â Kang, S.Â D.Â Kim, and M.Â Kim.\nStructural-Information-Based Robust Corner Point Extraction for Camera Calibration Under Lens Distortions and Compression Artifacts.\nIEEE Access\n, 9:151037â€“151048, 2021.\ndoi:\n10.1109/ACCESS.2021.3126570\n.\nLiu etÂ al. [2025a]\nZ.Â Liu, S.Â Liang, B.Â Guan, D.Â Tan, Y.Â Shang, and Q.Â Yu.\nCollimator-assisted high-precision calibration method for event cameras.\nOpt. Lett.\n, 50(13):4254â€“4257, 2025.\ndoi:\n10.1364/OL.564294\n.\nKlokmose etÂ al. [2014]\nC.Â N.Â Klokmose, J.Â B.Â Kristensen, R.Â Bagge, and K.Â Halskov.\nBullsEye: High-Precision Fiducial Tracking for Table-based Tangible Interaction.\nIn\nProceedings of the Ninth ACM International Conference on Interactive Tabletops and Surfaces\n, pages 269â€“278, 2014.\ndoi:\n10.1145/2669485.2669503\n.\nWang etÂ al. [2023]\nY.Â Wang, Z.Â Lai, Q.Â Zhang, Y.Â Qu, and S.Â Han.\nAccurate extrinsic calibration of solid-state light detection and ranging and camera system by coarse-to-fine grid-aligning.\nOptical Engineering\n, 62(7):074101, 2023.\ndoi:\n10.1117/1.OE.62.7.074101\n.\nJayatilleke and Zhang [2013]\nL.Â Jayatilleke and N.Â Zhang.\nLandmark-based localization for Unmanned Aerial Vehicles.\nIn\n2013 IEEE International Systems Conference (SysCon)\n, pages 448â€“451, 2013.\ndoi:\n10.1109/SysCon.2013.6549921\n.\nWang etÂ al. [2018]\nG.Â Wang, Z.Â Shi, Y.Â Shang, and Q.Â Yu.\nAutomatic Extraction of Diagonal Markers Based on Template Matching and Peaks of Gradient Histogram.\nActa Optica Sinica\n, (8), 2018.\nHe etÂ al. [2025a]\nB.Â He, Z.Â Zhao, W.Â Shao, C.Â Zhang, and T.Â Gao.\nIdentification and localization of circular coded target under non-uniform illumination.\nMeasurement\n, 247:116769, 2025.\ndoi:\n10.1016/j.measurement.2025.116769\n.\nZhang etÂ al. [2024a]\nY.Â Zhang, C.Â Lan, H.Â Zhang, G.Â Ma, and H.Â Li.\nMultimodal Remote Sensing Image Matching via Learning Features and Attention Mechanism.\nIEEE Transactions on Geoscience and Remote Sensing\n, 62:1â€“20, 2024.\ndoi:\n10.1109/TGRS.2023.3348980\n.\nWan etÂ al. [2018]\nG.Â Wan, X.Â Yang, R.Â Cai, H.Â Li, Y.Â Zhou, H.Â Wang, and S.Â Song.\nRobust and Precise Vehicle Localization Based on Multi-Sensor Fusion in Diverse City Scenes.\nIn\n2018 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 4670â€“4677, 2018.\ndoi:\n10.1109/ICRA.2018.8461224\n.\nJurado etÂ al. [2023]\nD.Â Jurado, R.Â MuÃ±oz-Salinas, S.Â Garrido-Jurado, and R.Â Medina-Carnicer.\nPlanar fiducial markers: a comparative study.\nVirtual Reality\n, 27:1â€“17, 2023.\ndoi:\n10.1007/s10055-023-00772-5\n.\nHuang etÂ al. [2021]\nK.Â Huang, Y.Â Wang, and L.Â Kneip.\nDynamic Event Camera Calibration.\nIn\n2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, pages 7021â€“7028, 2021.\ndoi:\n10.1109/IROS51168.2021.9636398\n.\nDong etÂ al. [2021]\nS.Â Dong, J.Â Ma, Z.Â Su, and C.Â Li.\nRobust circular marker localization under non-uniform illuminations based on homomorphic filtering.\nMeasurement\n, 170:108700, 2021.\ndoi:\n10.1016/j.measurement.2020.108700\n.\nLu etÂ al. [2024]\nZ.Â Lu, Y.Â Wu, S.Â Yang, K.Â Zhang, and Q.Â Quan.\nFast and Omnidirectional Relative Position Estimation With Circular Markers for UAV Swarm.\nIEEE Transactions on Instrumentation and Measurement\n, 73:1â€“11, 2024.\ndoi:\n10.1109/TIM.2024.3472903\n.\nQiao etÂ al. [2013]\nY.Â Qiao, Y.Â Tang, and J.Â Li.\nImproved Harris sub-pixel corner detection algorithm for chessboard image.\nIn\nProceedings of 2013 2nd International Conference on Measurement, Information and Control\n, volumeÂ 02, pages 1408â€“1411, 2013.\ndoi:\n10.1109/MIC.2013.6758222\n.\nBao etÂ al. [2025]\nQ.Â Bao, D.Â Ren, F.Â He, S.Â Piano, C.Â Zhang, and H.Â Zhao.\nEvent-based chessboard scanning profilometry.\nOptics and Lasers in Engineering\n, 186:108765, 2025.\ndoi:\n10.1016/j.optlaseng.2024.108765\n.\nZhang etÂ al. [2022]\nY.Â Zhang, T.Â Bu, J.Â Zhang, S.Â Tang, Z.Â Yu, J.Â K.Â Liu, and T.Â Huang.\nDecoding Pixel-Level Image Features From Two-Photon Calcium Signals of Macaque Visual Cortex.\nNeural computation\n, pages 1369â€“1397, 2022.\nHuang etÂ al. [2023]\nQ.Â Huang, J.Â DeGol, V.Â Fragoso, S.Â N.Â Sinha, and J.Â J.Â Leonard.\nOptimizing Fiducial Marker Placement for Improved Visual Localization.\nIEEE Robotics and Automation Letters\n, 8(5):2756â€“2763, 2023.\ndoi:\n10.1109/LRA.2023.3260700\n.\nLiu etÂ al. [2025b]\nZ.Â Liu, B.Â Guan, Y.Â Shang, Y.Â Bian, P.Â Sun, and Q.Â Yu.\nStereo Event-Based, 6-DOF Pose Tracking for Uncooperative Spacecraft.\nIEEE Transactions on Geoscience and Remote Sensing\n, 63:1â€“13, 2025.\ndoi:\n10.1109/TGRS.2025.3530915\n.\nLuo etÂ al. [2025]\nY.Â Luo, J.Â Zhang, and C.Â Li.\nCPIFuse: Toward realistic color and enhanced textures in color polarization image fusion.\nInformation Fusion\n, 120:103111, 2025.\ndoi:\n10.1016/j.inffus.2025.103111\n.\nFeng etÂ al. [2024]\nB.Â Feng, J.Â Xiao, J.Â Zhang, L.Â Li, Y.Â Wu, and Q.Â Ye.\nColor-polarization synergistic target detection method considering shadow interference.\nDefence Technology\n, 37:50â€“61, 2024.\ndoi:\n10.1016/j.dt.2024.01.007\n.\nHueckel [1973]\nM.Â H.Â Hueckel.\nA Local Visual Operator Which Recognizes Edges and Lines.\nJ. ACM\n, 20(4):634â€“647, 1973.\ndoi:\n10.1145/321784.321791\n.\nLiu etÂ al. [2022]\nW.Â Liu, X.Â Yang, H.Â Sun, X.Â Yang, X.Â Yu, and H.Â Gao.\nA Novel Subpixel Circle Detection Method Based on the Blurred Edge Model.\nIEEE Transactions on Instrumentation and Measurement\n, 71:1â€“11, 2022.\ndoi:\n10.1109/TIM.2021.3130924\n.\nXu etÂ al. [2021]\nC.Â Xu, X.Â Yang, Z.Â He, J.Â Qiu, and H.Â Gao.\nPrecise Positioning of Circular Mark Points and Transistor Components in Surface Mounting Technology Applications.\nIEEE Transactions on Industrial Informatics\n, 17(4):2534â€“2544, 2021.\ndoi:\n10.1109/TII.2020.2999023\n.\nZheng etÂ al. [2025]\nZ.Â Zheng, Z.Â Zhou, R.Â Chen, J.Â Liu, C.Â Liu, L.Â Zhang, L.Â Zhou, M.Â Xu, L.Â Wang, W.Â Wu, and J.Â Peng.\nPrecise subpixel luminance extraction method for De-Mura of AMOLED displays.\nDisplays\n, 86:102889, 2025.\ndoi:\n10.1016/j.displa.2024.102889\n.\nShang etÂ al. [2002]\nY.Â Shang, Q.Â Yu, and H.Â Lu.\nAutomatic recognition and precise positioning of circular diagonal signs.\nIn\nNational Conference on Optoelectronic Technology\n, 2002.\nWang etÂ al. [2019]\nG.Â Wang, Z.Â Shi, Y.Â Shang, X.Â Sun, W.Â Zhang, and Q.Â Yu.\nPrecise monocular vision-based pose measurement system for lunar surface sampling manipulator.\nScience China Technological Sciences\n, (10):12, 2019.\nCai etÂ al. [2025]\nY.Â Cai, X.Â Liu, Z.Â Li, B.Â Hu, W.Â Chen, Y.Â Yin, and Q.Â Yu.\nSub-pixel detection and localization of circular diagonal markers in complex field imaging environments.\nIn\nInternational Conference on Optical and Photonic Engineering (icOPEN 2024)\n, volume 13509, page 135090M. SPIE, 2025.\ndoi:\n10.1117/12.3057777\n.\nTao etÂ al. [2024]\nJ.Â Tao, Y.Â Li, B.Â Guan, Y.Â Shang, and Q.Â Yu.\nSimultaneous Enhancement and Noise Suppression Under Complex Illumination Conditions.\nIEEE Transactions on Instrumentation and Measurement\n, 73:1â€“11, 2024.\ndoi:\n10.1109/TIM.2024.3436108\n.\nHe etÂ al. [2013]\nK.Â He, J.Â Sun, and X.Â Tang.\nGuided Image Filtering.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n, 35(6):1397â€“1409, 2013.\ndoi:\n10.1109/TPAMI.2012.213\n.\nLi etÂ al. [2015]\nZ.Â Li, J.Â Zheng, Z.Â Zhu, W.Â Yao, and S.Â Wu.\nWeighted Guided Image Filtering.\nIEEE Transactions on Image Processing\n, 24(1):120â€“129, 2015.\ndoi:\n10.1109/TIP.2014.2371234\n.\nZhang etÂ al. [2023]\nJ.Â Zhang, Y.Â Luo, J.Â Huang, Y.Â Liu, and J.Â Ma.\nMulti-exposure image fusion via perception enhanced structural patch decomposition.\nInformation Fusion\n, 99:101895, 2023.\ndoi:\n10.1016/j.inffus.2023.101895\n.\nComaniciu and Meer [2002]\nD.Â Comaniciu and P.Â Meer.\nMean shift: a robust approach toward feature space analysis.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n, 24(5):603â€“619, 2002.\ndoi:\n10.1109/34.1000236\n.\nZhang [2000]\nZ.Â Zhang.\nA flexible new technique for camera calibration.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n, 22(11):1330â€“1334, 2000.\ndoi:\n10.1109/34.888718\n.\nCao etÂ al. [2024]\nF.Â Cao, T.Â Shi, K.Â Han, P.Â Wang, and W.Â An.\nLog-Gabor filter-based high-precision multi-modal remote sensing image matching.\nActa Geodaetica et Cartographica Sinica\n, 53(3):526â€“536, 2024.\nLiang etÂ al. [2025]\nS.Â Liang, B.Â Li, B.Â Guan, Y.Â Shang, X.Â Zhu, and Q.Â Yu.\nAccurate Pose Estimation for Flight Platforms based on Divergent Multi-Aperture Imaging System.\nArXiv\n, abs/2502.19708, 2025.",
    "preview_text": "This paper proposes a robust, high-precision positioning methodology to address localization failures arising from complex background interference in large-scale flight navigation and the computational inefficiency inherent in conventional sliding window matching techniques. The proposed methodology employs a three-tiered framework incorporating multi-layer corner screening and adaptive template matching. Firstly, dimensionality is reduced through illumination equalization and structural information extraction. A coarse-to-fine candidate selection strategy minimizes sliding window computational costs, enabling rapid estimation of the marker's position. Finally, adaptive templates are generated for candidate points, achieving subpixel precision through improved template matching with correlation coefficient extremum fitting. Experimental results demonstrate the method's effectiveness in extracting and localizing diagonal markers in complex, large-scale environments, making it ideal for field-of-view measurement in navigation tasks.\n\nRobust Subpixel Localization of Diagonal Markers in Large-Scale Navigation via Multi-Layer Screening and Adaptive Matching\nJing Tao\nCollege of Aerospace Science and Engineering, National University of Defense Technology, Hunan 410073, China\nHunan Provincial Key Laboratory of Image Measurement and Vision Navigation, Hunan 410073, China\nBanglei Guan\nCollege of Aerospace Science and Engineering, National University of Defense Technology, Hunan 410073, China\nHunan Provincial Key Laboratory of Image Measurement and Vision Navigation, Hunan 410073, China\nYang Shang\nCollege of Aerospace Science and Engineering, National University of Defense Technology, Hunan 410073, China\nHunan Provincial Key Laboratory of Image Measurement and Vision Navigation, Hunan 410073, China\nShunkun Liang\nCollege of Aerospace Science and Engineering, National University of Defense Technology, Hunan 410073, China\nHunan Provincial Key Laboratory of Image Measurement and V",
    "is_relevant": false,
    "relevance_score": 0.0,
    "extracted_keywords": [
        "localization",
        "navigation",
        "template matching",
        "subpixel",
        "corner screening"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¤§è§„æ¨¡å¯¼èˆªä¸­å¤æ‚èƒŒæ™¯ä¸‹å¯¹è§’æ ‡è®°çš„é²æ£’äºšåƒç´ å®šä½æ–¹æ³•ï¼ŒåŸºäºå¤šå±‚ç­›é€‰å’Œè‡ªé€‚åº”æ¨¡æ¿åŒ¹é…ï¼Œä»¥æé«˜å®šä½ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-13T02:51:31Z",
    "created_at": "2026-01-20T17:49:40.833764",
    "updated_at": "2026-01-20T17:49:40.833774"
}