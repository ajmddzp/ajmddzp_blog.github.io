{
    "id": "2601.14848v1",
    "title": "From Observation to Prediction: LSTM for Vehicle Lane Change Forecasting on Highway On/Off-Ramps",
    "authors": [
        "Mohamed Abouras",
        "Catherine M. Elias"
    ],
    "abstract": "匝道是研究相对不足的路段，尽管它们在高速公路交互中引入了更高程度的变化。预测车辆在这些区域的行为可以减少不确定性的影响，并提高道路安全性。本文研究了这一兴趣区域与直线高速公路路段之间的差异。利用多层LSTM架构，结合ExiD无人机数据集对兴趣区域模型进行训练。在此过程中，测试了不同的预测时间范围和不同模型的工作流程。结果显示，在长达4秒的时间范围内，预测结果表现出巨大潜力：最大时间范围内，兴趣区域的预测准确率约为76%，而一般高速公路场景的预测准确率可达94%。",
    "url": "https://arxiv.org/abs/2601.14848v1",
    "html_url": "https://arxiv.org/html/2601.14848v1",
    "html_content": "From Observation to Prediction: LSTM for Vehicle Lane Change Forecasting on Highway On/Off-Ramps\nMohamed Abouras\n1,2\nand Catherine M. Elias\n1\n,\n2\n​\n{}^{1,2\\href https://orcid.org/0000-0002-1444-9816\\,}\n*This work was not supported by any organization\n1\nC-DRiVeS Lab: Cognitive Driving Research in Vehicular Systems, Cairo, Egypt\ncdrives.researchlab@gmail.com\n2\nComputer Science and Engineering Department - Faculty of Media Engineering and Technology - German University in Cairo, Egypt\nmohamed.abouras@student.guc.edu.eg, catherine.elias@ieee.org\nAbstract\nOn and off-ramps are understudied road sections even though they introduce a higher level of variation in highway interactions. Predicting vehicles’ behavior in these areas can decrease the impact of uncertainty and increase road safety. In this paper, the difference between this Area of Interest (AoI) and a straight highway section is studied. Multi-layered LSTM architecture to train the AoI model with ExiD drone dataset is utilized. In the process, different prediction horizons and different models’ workflow are tested. The results show great promise on horizons up to 4 seconds with prediction accuracy starting from about 76% for the AoI and 94% for the general highway scenarios on the maximum horizon.\nI\nIntroduction and Related Work\nDriving is accompanied by uncertainty; a driver’s predictions and reactions are the deciding factors of whether that uncertainty results in an unfortunate outcome, over one million people die from road injuries every year\n[\n11\n,\n15\n]\n. Highways add speed to the list of challenges on the road which in turn increases the arbitrary factors present and the outcome lethality. A lot of the classical, physics-based approaches such as constant velocity and constant acceleration models and Kalman filter model which were used in relevant literature have tried to predict the volatile aspects surrounding this dilemma\n[\n1\n,\n10\n]\n. These approaches while fast, computationally, they cannot account for complex scene factors like interactions between vehicles\n[\n1\n,\n10\n]\n. Alternatively, some classical machine learning (ML) approaches such as support vector machine (SVM)\n[\n6\n]\nfailed to model the hidden state of the entire scene which constrained the capacity for generalization\n[\n10\n]\n. These models also struggled when the number of input features increased, which made the classification task significantly harder. In addition, manually extracting relevant features can be both challenging and time-consuming. Mistakes made during this early stage of feature annotation can carry through the entire pipeline and negatively affect the model’s overall performance\n[\n8\n]\n.\nDeep Learning (DL) has been a rising tool to overcome classical limitations and has been used to study different scenarios from perceiving the environment to predicting behavior and giving safe decisions. These DL models can construct complex representations of the scene, such as Bird’s Eye View (BEV) representations that stack historical perception outputs and HD map information. This capability shows high improvement in generalizing the highly dynamic and varied road environments. While a lot of AI-based research has delved into relevant scenarios, relatively few papers have explored prediction as reviewed by surveys\n[\n13\n,\n3\n,\n5\n,\n1\n]\n, which review different DL and ML approaches for vehicle trajectory and motion prediction. Other survey AI techniques for accident prediction and unsafe driving patterns\n[\n2\n]\n. Some studies, as reviewed by\n[\n1\n]\nand\n[\n3\n]\n, utilize INTERACTION\n[\n16\n]\ndataset which discuss merging lane situations\n[\n12\n]\n. Fewer tried to predict behavior on the highway exiting and merging scenarios. These scenarios are important leverage points on the road as they are nodes of great variation in both behavior and acceleration. Because of the importance of these scenarios, this paper aims to expand on these events in the general context that is the highway.\nFollowing the methodology of previous literature, similar LSTM networks are utilized to make predictions regarding lane changes at exits and entrances on the highway. This approach is well-supported as LSTMs are a special type of Recurrent Neural Networks (RNNs) that excel at modeling temporal dependencies in sequential data\n[\n14\n,\n5\n]\n. While traditional RNNs mostly struggle with long dependencies, LSTMs overcome this challenge through their selective memory gates.\nPrior studies have demonstrated the effectiveness of LSTM-based models for highway lane change and behavior prediction. For instance,\n[\n9\n]\nutilized multi-LSTM trained on HighD dataset and achieved up to 92.43% accuracy in Cooperative Adaptive Cruise Control (CACC) scenarios by incorporating surrounding vehicles. Others study aggressive lane changes on highways, achieving 98.5% prediction accuracy\n[\n14\n]\n.\nGetting accurate lane change predictions can lead to a safer and smoother highway experience. Additionally, the aim of this study is not only to increase the accuracy over the model in the existing literature but also to study key differences between the conditions on the normal highway scenarios and the variations on highway exit and entrance points. This paper makes the following contributions:\n1.\nIt extends highway behavior studies by specifically focusing on merging and diverging maneuvers at highway entry and exit ramps.\n2.\nIt compares various multi-layered LSTM architectures to evaluate the effectiveness of end-to-end versus modular learning approaches.\n3.\nIt presents a survey of safe lane change durations from existing literature to inform the selection of different prediction horizons.\n4.\nIt investigates feature bias across different maneuver classes to understand which inputs most influence model predictions.\nIn section\nII\n, this paper will begin by discussing the methodology of this study. Then, in section\nIII\n, the results of the experiments conducted will be reviewed. Section\nIV\nconcludes the study and proposes future recommendations.\nFigure 1:\nArea of Interest\nII\nMethodology\nThis section discusses the approach taken to predict lane change maneuvers using a stacked LSTM architecture. First, the ExiD and HighD datasets are prepared and processed. Then, the prediction range’s motivation as well as the model architecture and the training process are discussed.\nII-A\nData Processing and Preparation\nThe research environment is defined as structured, where vehicles’ speeds are over 100 km/h and traffic rules, such as using designated lanes, are applied. The Area of Interest (AoI) lies in highway sections that directly connect the highway to the off-ramps (exits) and on-ramps (entrances), as shown in Figure\n1\n. To study exits and entrances, ExiD drone dataset is used, which is a real and accurate trajectory dataset at highway entries and exits in Germany collected by LevelXData\n[\n7\n]\n. LevelXData also provide another drone trajectory dataset recorded on German highways\n[\n4\n]\n, which is used to compare the AoI with general highway scenarios.\nTABLE I:\nFeatures chosen for training from ExiD and HighD\nDataset Features\nEgo’s Position X(X), Y(Y)\nEgo’s Velocity X(VX), Y(VY)\nEgo’s Acceleration X(AX), Y(AY)\nRelative Surrounding Distances (dP)\nRelative Surrounding Velocities (dV)\nRelative Surrounding Accelerations (dA)\nAfter the research environment is defined, the data is prepared by recording an observation period of five frames, or one-fifth of a second, for each vehicle (ego) on the road section. This observation period was found to be optimal by Qassemabadi et al. in\n[\n9\n]\n.\nd\n​\np\n=\n(\nx\ns\n−\nx\ne\n)\n2\n+\n(\ny\ns\n−\ny\ne\n)\n2\ndp=\\sqrt{(x_{s}-x_{e})^{2}+(y_{s}-y_{e})^{2}}\n(1)\nwhere\nd\n​\np\ndp\nis the difference in position, with\ns\ns\ndenoting the surrounding vehicle and\ne\ne\ndenoting the ego vehicle.\nd\n​\nv\n=\n(\nv\ns\n−\nv\ne\n)\n2\n+\n(\nv\ns\n−\nv\ne\n)\n2\ndv=\\sqrt{(v_{s}-v_{e})^{2}+(v_{s}-v_{e})^{2}}\n(2)\nwhere\nd\n​\nv\ndv\nis the difference in velocity, with\ns\ns\ndenoting the surrounding vehicle and\ne\ne\ndenoting the ego vehicle.\nd\n​\na\n=\n(\na\ns\n−\na\ne\n)\n2\n+\n(\na\ns\n−\na\ne\n)\n2\nda=\\sqrt{(a_{s}-a_{e})^{2}+(a_{s}-a_{e})^{2}}\n(3)\nwhere\nd\n​\na\nda\nis the difference in acceleration, with\ns\ns\ndenoting the surrounding vehicle and\ne\ne\ndenoting the ego vehicle.\nThe observed features are ego’s position (x, y), velocity (x, y), and acceleration (x, y), found in Table\nI\n. The euclidean relative distance eq (\n1\n) , velocity eq (\n2\n), and acceleration eq (\n3\n) of the ego’s surrounding vehicles are calculated to link between the ego and its acting environment. There are three output classes: Lane Change Left (LCL), Lane Keep (LK), and Lane Change Right (LCR). To avoid class-bias, the original number of entries of each class, found in Table\nII\n, was balanced for training and testing. According to the observation period, the models predict the output class after a specified temporal horizon.\nTABLE II:\nDifferent classes distributions for each scenario after preparation and before balancing\nTime in Seconds\nLeft\nKeep\nRight\n1\n150645\n36080998\n178715\nHighD\n2\n271444\n33058674\n320126\n3\n677843\n11174034\n479914\n4\n436755\n27194418\n514418\n———–\n1\n316972\n13857584\n200593\nExiD\n2\n521974\n3537031\n3537031\n3\n793145\n575544\n575544\n4\n1282296\n16941285\n930741\nII-B\nPrediction Horizon\nTo predict any action, a reliable reference must be established based on the action’s behavior. Thus, to understand lane change maneuvers, an experiment was performed involving two vehicle types–a sedan and an SUV–each driven by experienced drivers on controlled, empty inter-city highways. The drivers exceeded 130 km/h speeds and covered both straight and curved sections, performing 20 lane change maneuvers each. A passenger used a stopwatch to time a full lane change (until the vehicle crosses the lane mark completely) and answered a short survey about the safety perception of the maneuver. Results revealed durations ranging from 2 to 6 seconds.\nII-C\nModel Architecture\nFigure 2:\nEnd-to-End Model\nFigure 3:\nMulti-L Models\nAs the number of learning features for the model is substantial, a stack of multiple LSTMs is used; additionally, as the dataset is relatively huge, overfitting must be avoided. Using multiple LSTMs allows the model to extract hierarchical features and increase long-term memory dependency. It also increases the model’s capacity to learn from the given data. Cross-Entropy is used as the loss function, as it directly addresses the classification task. Additionally, a RelU layer is introduced to the model to help prevent convergence issues and learn more complex features. To avoid over-fitting, a dropout rate of 0.2 between the two LSTM layers is used. The model’s parameters are set based on the literature review, especially\n[\n9\n]\non normal highway scenarios. The aforementioned parameters are as shown in Table\nIII\n.\nThere are two models: one model is End-to-End (E2E), shown in Figure\n2\n, where the output is directly defined as either LCL, LCR, or LK; and the other is Multi-Layered (Multi-L), where the first model determines the output as either Lane Change or LK, and if a lane change is detected, the direction is then defined as either Right or Left by the second model, as shown in Figure\n3\n. If the initial Multi-L model can deliver faster binary lane change predictions with less resources, the road user is expected to detect significant road changes in time, making a slower second-staged direction prediction acceptable.\nTABLE III:\nModel Parameters\nParameter\nDescription\nValue\n8 surrounding cars,\n5 frames,\n(dp, dv, da)\n120\nInput Dimension\n5 frames, ego position,\nego velocity,\nego acceleration\n30\nOutput Dimension\nLCL, LK, LCR for E2E /\n(LC, LK), (LCL, LCR)\nfor Multi-L\n3/1\nBatch Size\nNumber of training cases per iteration\n32\nHidden Layer Number\nNumber of LSTM layers\n2\nDropout Rate\nPercentage of dropout neurons\nper iteration\n0.2\nNumber of Epochs\nNumber of training passes\nthrough the dataset\n100\nLoss Function\nFunction to calculate loss\nCross Entropy\nActivation Function\nFunction to activate output neurons\nRelU\nOptimizer\nFunction to minimize prediction errors\nRMSProp\nII-D\nTraining Process\nThe data is split into a training-to-testing ratio of 80:20. While training, the features affecting class bias are recorded and observed. All models tested are compared for a comprehensive study.\nIII\nResults\nThis section explores the results of the experiments. Firstly, the results of the prediction horizon experiment are recorded. Then, the different stacked LSTM models are evaluated. Finally, the feature bias observation is reviewed.\nIII-A\nPrediction Horizon\nAs the experienced passengers comment on the maneuver, they consider non-empty environments as well in their answers, as per the given follow-up questions. The safe records primarily concentrate on four seconds, and mostly conclude any maneuver under four seconds as very dangerous. There are only three maneuvers from the survey that surpassed four seconds, and they are perceived, mostly, as of unsafe nature, check Figure\n4\n.\nFigure 4:\nSurvey Outcome\nIII-B\nModel Evaluation\nThe models are evaluated by recording the confusion matrix–for detailed breakdown of the predictions–along with the classification report, f1-score, accuracy, precision, recall, number of parameters, model size, inference time, and RAM and GPU allocation. The real prediction validity is evaluated by capturing the real data and comparing it to the prediction output. Predicting lane change direction yields lower prediction accuracy than simply predicting a lane change. Increasing the prediction horizon yields less accurate results in both models. Additionally, the AoI is observed to have added complexity to the models’ prediction accuracy. Nevertheless, the models, whether E2E or Multi-L, are flourishing and promising. Check tables\nIV\n,\nV\n,\nVI\n,\nVII\n,\nVIII\n, and\nIX\nfor detailed output.\nTABLE IV:\nTraining on HighD data only, E2E model\nEnd-to-End\nNormal Highway Section\nHorizon(seconds)\n1\n2\n3\n4\nAccuracy\n0.9778\n0.9844\n0.9742\n0.9459\nPrecision\n0.9737\n0.9824\n0.9749\n0.9485\nRecall\n0.9824\n0.9865\n0.9733\n0.9430\nF1-Score\n0.9779\n0.9844\n0.9741\n0.9457\nTABLE V:\nTraining on HighD data only, Model 1: Binary model for lane change.\nModel 1\nNormal Highway Section\nHorizon(seconds)\n1\n2\n3\n4\nAccuracy\n0.9858\n0.9884\n0.9763\n0.9436\nPrecision\n0.9774\n0.9831\n0.9755\n0.9558\nRecall\n0.9947\n0.9939\n0.9772\n0.9303\nF1-Score\n0.9860\n0.9885\n0.9764\n0.9429\nTABLE VI:\nTraining on HighD data only, Model 2: Binary model for lane change direction.\nModel 2\nNormal Highway Section\nHorizon(seconds)\n1\n2\n3\n4\nAccuracy\n1\n0.9999\n0.9996\n0.9951\nPrecision\n1\n0.9999\n0.9998\n0.995\nRecall\n1\n1\n0.9993\n0.9941\nF1-Score\n1\n0.9999\n0.9995\n0.9945\nTABLE VII:\nTraining on AoI, E2E model.\nEnd-to-End\nArea of Interest\nHorizon(seconds)\n1\n2\n3\n4\nAccuracy\n0.8279\n0.8229\n0.7809\n0.7688\nPrecision\n0.8204\n0.8174\n0.7666\n0.7534\nRecall\n0.8289\n0.8092\n0.7497\n0.7287\nF1-Score\n0.8244\n0.8128\n0.7569\n0.7376\nTABLE VIII:\nTraining on AoI, Model 1: Binary model for lane change.\nModel 1\nArea of Interest\nHorizon(seconds)\n1\n2\n3\n4\nAccuracy\n0.8777\n0.8749\n0.8547\n0.8331\nPrecision\n0.8387\n0.8768\n0.8595\n0.8390\nRecall\n0.9070\n0.8736\n0.8595\n0.8250\nF1-Score\n0.8715\n0.8752\n0.8542\n0.8319\nTABLE IX:\nTraining on AoI, Model 2: Binary model for lane change direction.\nModel 2\nArea of Interest\nHorizon(seconds)\n1\n2\n3\n4\nAccuracy\n0.9669\n0.9454\n0.934\n0.8216\nPrecision\n0.9665\n0.9352\n0.9199\n0.8299\nRecall\n0.9762\n0.9721\n0.9742\n0.8566\nF1-Score\n0.9713\n0.9533\n0.9462\n0.843\nThe following points can be concluded from the results in the tables:\n1.\nEven the lowest prediction accuracy for a 4-second horizon is quite promising.\n2.\nSince E2E and Multi-L models have very similar accuracies, the accuracy will not be the determinant factor for which model to adopt.\nAccording to the rest of the evaluations, found in Table\nX\nand Table\nXI\n, the following is concluded:\n1.\nAll three models (E2E, Multi-L model 1, and Multi-L model 2) have similar size, inference time, and number of parameters.\n2.\nRAM and GPU allocated for Multi-L model 1 is the greatest, even greater than the E2E model, making it the most consuming of system resources.\n3.\nUsing the Multi-L approach will roughly double all resources and time compared to using the E2E approach.\nTABLE X:\nData of number of parameters, size, RAM and GPU used, and inference time for the multi-layer models.\nMulti Layer\nModel 1\nModel 2\nNumber of Parameters\n275,585\n275,585\nSize\n∼\n\\sim\n1.05MB\n∼\n\\sim\n1.05MB\nRAM Used\n3414.17 MB\n2201.97 MB\nGPU Used\nAllocated: 495.44 MB\nReserved: 762.00 MB\nAllocated: 240.62 MB\nReserved: 552.00 MB\nInference Time\nt =\n∼\n\\sim\n1.5 seconds\nt =\n∼\n\\sim\n1.5 seconds\nTABLE XI:\nComparing Multi-Layer full pipeline to End-to-End model\nMulti-L Models\nE2E Model\nNumber of Parameters\n551,170\n275,843\nSize\nBoth models sizes\n∼\n\\sim\n1.05MB\nRAM Used\n5,616.14 MB\n3302.57 MB\nGPU Used\nAllocated: 736.06 MB\nAllocated: 285.57 MB\nReserved: 550.00 MB\nInference Time\nRoughly 2 × t, worst case\nt =\n∼\n\\sim\n1.05 seconds\nAccordingly, the best model approach in accuracy, resources, and inference is the E2E model. The model’s output was then studied on random cases, such as in Figure\n5\n, and it was found that even when the model fails with the normal evaluation, it correctly evaluates that the vehicle is moving noticeably in the direction of a lane change, even if a lane change doesn’t necessarily occur after the prediction horizon. By effectively recognizing subtle motion cues, the model shows strong practical utility and promise for reliable use in real-world scenarios. It is understood, however, that because of the 1-second inference delay, the model with 1-second prediction horizon becomes obsolete and that for other models, inference delay reduces the available reaction time for road users.\nFigure 5:\nVehicle path example vs Prediction over time.\nIII-C\nFeature Bias Observation\nTABLE XII:\nTop Feature Biases per Class: Left (1), Keep (0), Right (2)\n#\nLeft (Class 1)\nKeep (Class 0)\nRight (Class 2)\n1\nt3_ego_x\nt2_ego_x\nt0_ego_x\n2\nt0_ego_x\nt1_ego_x\nt1_ego_x\n3\nt0_ego_y\nt4_ego_x\nt2_ego_x\n4\nt1_ego_y\nt2_leftFollowing_dp\nt1_ego_y\n5\nt3_ego_y\nt4_rightPreceding_dp\nt4_ego_y\n6\nt2_ego_x\nt4_ego_y\nt2_ego_y\n7\nt2_ego_y\nt4_leftFollowing_dp\nt3_ego_x\n8\nt2_leftPreceding_dp\nt1_leftFollowing_dp\nt2_leftPreceding_dp\n9\nt3_leftFollowing_dp\nt1_ego_y\nt2_rightPreceding_dp\n10\nt1_ego_x\nt3_ego_y\nt3_rightPreceding_dp\n11\nt4_leftPreceding_dp\nt0_ego_x\nt3_ego_y\n12\nt3_leftPreceding_dp\nt0_ego_y\nt3_following_dp\n13\nt4_ego_y\nt3_rightPreceding_dp\nt4_leftFollowing_dp\n14\nt0_leftFollowing_dp\nt3_following_dp\nt3_leftPreceding_dp\n15\nt2_preceding_dp\nt0_following_dp\nt0_ego_y\nSince the dataset is originally unbalanced, it is integral to observe which features bias the E2E model to each output class. The report in Table\nXII\nshows the observed most influential 15 features, ordered from greatest to least influence, on the model according to each class. The conclusion of the report is the following:\n•\nThe features common in influencing the model are: the vehicle’s position and the Left Following vehicles’ dp.\n•\nThe features biasing the model towards Class Left are: Left Preceding and Preceding vehicles’ dp.\n•\nThe features biasing the model towards Class Keep are: Right Preceding and Following vehicles’ dp.\n•\nThe features biasing the model towards Class Right are: Left Preceding, Right Preceding, and Following vehicles’ dp.\nIV\nConclusion and Future Recommendations\nIn conclusion, a non-complex model such as two stacked LSTMs can predict with high accuracy whether a vehicle will keep its lane or change it and in which direction. It is noted that an inference time of one second decreases the decision period. The AoI is found to be a promising domain to expand upon as it poses higher uncertainty than normal highway scenarios. While the prediction horizon experiment found that 4 seconds is a reasonable time to complete a lane change, there were many cases of vehicles taking more than this period in the dataset. The models still hold promise for further study at longer temporal horizons despite their focus on a 1-second to 4-second prediction horizon. One E2E model is found to be better than a Multi-L model in speed and resource consumption. Exploring longer prediction horizons and different architectures is recommended. Additionally, a generative path model that confidently generates future scenarios is proposed for future work. Applying the proposed E2E model can leverage the confident generated paths to estimate potential future maneuvers over extended prediction horizons.\nReferences\n[1]\nV. Bharilya and N. Kumar\n(2024)\nMachine learning for autonomous vehicle’s trajectory prediction: a comprehensive survey, challenges, and future research directions\n.\nVehicular Communications\n46\n,\npp. 100733\n.\nCited by:\n§I\n,\n§I\n.\n[2]\nZ. Halim, R. Kalsoom, S. Bashir, and G. Abbas\n(2016-10)\nArtificial intelligence techniques for driving safety and vehicle crash prediction\n.\nArtificial Intelligence Review\n46\n,\npp.\n.\nExternal Links:\nDocument\nCited by:\n§I\n.\n[3]\nR. Huang, G. Zhuo, L. Xiong, S. Lu, and W. Tian\n(2023)\nA review of deep learning-based vehicle motion prediction for autonomous driving\n.\nSustainability\n15\n(\n20\n).\nExternal Links:\nLink\n,\nISSN 2071-1050\n,\nDocument\nCited by:\n§I\n.\n[4]\nR. Krajewski, J. Bock, L. Kloeker, and L. Eckstein\n(2018)\nThe highd dataset: a drone dataset of naturalistic vehicle trajectories on german highways for validation of highly automated driving systems\n.\nIn\n2018 21st International Conference on Intelligent Transportation Systems (ITSC)\n,\npp. 2118–2125\n.\nExternal Links:\nDocument\nCited by:\n§\nII-A\n.\n[5]\nJ. Liu, X. Mao, Y. Fang, D. Zhu, and M. Q.-H. Meng\n(2021)\nA survey on deep-learning approaches for vehicle trajectory prediction in autonomous driving\n.\nCoRR\nabs/2110.10436\n.\nExternal Links:\nLink\n,\n2110.10436\nCited by:\n§I\n,\n§I\n.\n[6]\nM. Manzour, A. Ballardini, R. Izquierdo, and M. Sotelo\n(2024)\nVehicle lane change prediction based on knowledge graph embeddings and bayesian inference\n.\nIn\n2024 IEEE Intelligent Vehicles Symposium (IV)\n,\npp. 1893–1900\n.\nCited by:\n§I\n.\n[7]\nT. Moers, L. Vater, R. Krajewski, J. Bock, A. Zlocki, and L. Eckstein\n(2022)\nThe exid dataset: a real-world trajectory dataset of highly interactive highway scenarios in germany\n.\nIn\n2022 IEEE Intelligent Vehicles Symposium (IV)\n,\npp. 958–964\n.\nExternal Links:\nDocument\nCited by:\n§\nII-A\n.\n[8]\nZ. S. Nejad, H. Heravi, A. R. Jounghani, A. Shahrezaie, and A. Ebrahimi\n(2021)\nVehicle trajectory prediction in top-view image sequences based on deep learning method\n.\narXiv preprint arXiv:2102.01749\n.\nCited by:\n§I\n.\n[9]\nA. N. Qasemabadi, S. Mozaffari, M. Rezaei, M. Ahmadi, and S. Alirezaee\n(2023)\nA novel model for driver lane change prediction in cooperative adaptive cruise control systems\n.\nIn\n2023 International Symposium on Signals, Circuits and Systems (ISSCS)\n,\nVol.\n,\npp. 1–4\n.\nExternal Links:\nDocument\nCited by:\n§I\n,\n§\nII-A\n,\n§\nII-C\n.\n[10]\nS. Qiao, F. Gao, J. Wu, and R. Zhao\n(2024)\nAn enhanced vehicle trajectory prediction model leveraging lstm and social-attention mechanisms\n.\nIEEE Access\n12\n(\n),\npp. 1718–1726\n.\nExternal Links:\nDocument\nCited by:\n§I\n.\n[11]\nH. Ritchie\n(2024)\nMore than a million people die from road injuries every year\n.\nNote:\nhttps://ourworldindata.org/data-insights/more-than-a-million-people-die-from-road-injuries-every-year\nAccessed: 2025-07\nCited by:\n§I\n.\n[12]\nO. Siebinga, A. Zgonnikov, and D. Abbink\n(2022)\nA human factors approach to validating driver models for interaction-aware automated vehicles\n.\nACM Transactions on Human-Robot Interaction (THRI)\n11\n(\n4\n),\npp. 1–21\n.\nCited by:\n§I\n.\n[13]\nA. Singh\n(2023)\nTrajectory-prediction with vision: a survey\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp. 3318–3323\n.\nCited by:\n§I\n.\n[14]\nR. Singh, S. Mozaffari, M. Rezaei, and S. Alirezaee\n(2023)\nLSTM-based preceding vehicle behaviour prediction during aggressive lane change for acc application\n.\nIn\n2023 International Symposium on Signals, Circuits and Systems (ISSCS)\n,\nVol.\n,\npp. 1–4\n.\nExternal Links:\nDocument\nCited by:\n§I\n,\n§I\n.\n[15]\nWorld Health Organization\n(2023-12-13)\nRoad traffic injuries\n.\nNote:\nhttps://www.who.int/news-room/fact-sheets/detail/road-traffic-injuries\nAccessed: 2025-07\nCited by:\n§I\n.\n[16]\nW. Zhan, L. Sun, D. Wang, H. Shi, A. Clausse, M. Naumann, J. Kümmerle, H. Königshof, C. Stiller, A. de La Fortelle, and M. Tomizuka\n(2019-09)\nINTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic Maps\n.\narXiv:1910.03088 [cs, eess]\n.\nCited by:\n§I\n.",
    "preview_text": "On and off-ramps are understudied road sections even though they introduce a higher level of variation in highway interactions. Predicting vehicles' behavior in these areas can decrease the impact of uncertainty and increase road safety. In this paper, the difference between this Area of Interest (AoI) and a straight highway section is studied. Multi-layered LSTM architecture to train the AoI model with ExiD drone dataset is utilized. In the process, different prediction horizons and different models' workflow are tested. The results show great promise on horizons up to 4 seconds with prediction accuracy starting from about 76% for the AoI and 94% for the general highway scenarios on the maximum horizon.\n\nFrom Observation to Prediction: LSTM for Vehicle Lane Change Forecasting on Highway On/Off-Ramps\nMohamed Abouras\n1,2\nand Catherine M. Elias\n1\n,\n2\n​\n{}^{1,2\\href https://orcid.org/0000-0002-1444-9816\\,}\n*This work was not supported by any organization\n1\nC-DRiVeS Lab: Cognitive Driving Research in Vehicular Systems, Cairo, Egypt\ncdrives.researchlab@gmail.com\n2\nComputer Science and Engineering Department - Faculty of Media Engineering and Technology - German University in Cairo, Egypt\nmohamed.abouras@student.guc.edu.eg, catherine.elias@ieee.org\nAbstract\nOn and off-ramps are understudied road sections even though they introduce a higher level of variation in highway interactions. Predicting vehicles’ behavior in these areas can decrease the impact of uncertainty and increase road safety. In this paper, the difference between this Area of Interest (AoI) and a straight highway section is studied. Multi-layered LSTM architecture to train the AoI model with ExiD drone dataset is utilized. In the process, different prediction horizons and different models’ workflow are tested. The results show great promise on horizons up to 4 seconds with prediction accuracy starting from about 76% for the AoI and 94% for the general highway scenarios on the maximum horizon.\nI\nIntroduction",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "LSTM",
        "vehicle lane change forecasting",
        "highway on/off-ramps",
        "prediction accuracy",
        "ExiD drone dataset"
    ],
    "one_line_summary": "这篇论文使用多层LSTM架构，基于ExiD无人机数据集，预测高速公路匝道区域的车辆变道行为，以提高道路安全性。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T10:31:03Z",
    "created_at": "2026-01-27T15:53:19.393392",
    "updated_at": "2026-01-27T15:53:19.393398",
    "recommend": 0
}