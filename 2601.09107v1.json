{
    "id": "2601.09107v1",
    "title": "Vision Foundation Models for Domain Generalisable Cross-View Localisation in Planetary Ground-Aerial Robotic Teams",
    "authors": [
        "Lachlan Holden",
        "Feras Dayoub",
        "Alberto Candela",
        "David Harvey",
        "Tat-Jun Chin"
    ],
    "abstract": "è¡Œæ˜Ÿæœºå™¨äººæŠ€æœ¯çš„ç²¾ç¡®å®šä½æ˜¯å®ç°æœªæ¥ä»»åŠ¡è§„æ¨¡ä¸èŒƒå›´æ‰©å±•æ‰€éœ€é«˜çº§è‡ªä¸»æ€§çš„å…³é”®ã€‚æœºæ™ºå·ç›´å‡æœºåŠå¤šé¢—è¡Œæ˜Ÿè½¨é“å™¨çš„æˆåŠŸä¸ºæœªæ¥åˆ©ç”¨åœ°é¢-ç©ºä¸­æœºå™¨äººå›¢é˜Ÿçš„ä»»åŠ¡å¥ å®šäº†åŸºç¡€ã€‚æœ¬æ–‡ç ”ç©¶å¦‚ä½•è®©æ¼«æ¸¸è½¦é€šè¿‡æœºå™¨å­¦ä¹ ï¼Œåœ¨æœ‰é™è§†åœºå•ç›®åœ°é¢è§†è§’RGBå›¾åƒè¾“å…¥æ¡ä»¶ä¸‹ï¼Œåœ¨å±€éƒ¨èˆªç©ºåœ°å›¾ä¸­å®ç°è‡ªä¸»å®šä½ã€‚æœºå™¨å­¦ä¹ æ–¹æ³•é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºï¼šå…·å¤‡çœŸå®ä½ç½®æ ‡æ³¨çš„å®é™…å¤ªç©ºæ•°æ®æä¸ºç¨€ç¼ºã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åˆ›æ–°æ–¹æ³•ï¼Œé€šè¿‡è·¨è§†è§’å®šä½åŒç¼–ç å™¨æ·±åº¦ç¥ç»ç½‘ç»œå®ç°æ¼«æ¸¸è½¦åœ¨èˆªç©ºåœ°å›¾ä¸­çš„å®šä½ã€‚æˆ‘ä»¬åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œå¹¶ç»“åˆæµ·é‡åˆæˆæ•°æ®æ¥å¼¥åˆä¸çœŸå®å›¾åƒé—´çš„é¢†åŸŸå·®å¼‚ã€‚åŒæ—¶è´¡çŒ®äº†åœ¨è¡Œæ˜Ÿæ¨¡æ‹Ÿè®¾æ–½ä¸­é‡‡é›†çš„çœŸå®ä¸–ç•Œæ¼«æ¸¸è½¦è½¨è¿¹è·¨è§†è§’æ•°æ®é›†ï¼ˆåŒ…å«å¯¹åº”çœŸå®å®šä½æ•°æ®ï¼‰ï¼Œä»¥åŠæµ·é‡åˆæˆå›¾åƒå¯¹æ•°æ®é›†ã€‚é€šè¿‡å°†ç²’å­æ»¤æ³¢å™¨ä¸è·¨è§†è§’ç½‘ç»œç»“åˆè¿›è¡ŒçŠ¶æ€ä¼°è®¡ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŸºäºåœ°é¢è§†è§’å›¾åƒåºåˆ—ï¼Œåœ¨ç®€å•ä¸å¤æ‚è½¨è¿¹ä¸Šå®ç°ç²¾ç¡®ä½ç½®ä¼°ç®—ã€‚",
    "url": "https://arxiv.org/abs/2601.09107v1",
    "html_url": "https://arxiv.org/html/2601.09107v1",
    "html_content": "Vision Foundation Models for Domain Generalisable\nCross-View Localisation in Planetary Groundâ€“Aerial Robotic Teams\nLachlan Holden\n1\n, Feras Dayoub\n1\n, Alberto Candela\n2\n, David Harvey\n3\n, and Tat-Jun Chin\n1\nAbstract\nAccurate localisation in planetary robotics enables the advanced autonomy required to support the increased scale and scope of future missions. The successes of the Ingenuity helicopter and multiple planetary orbiters lay the groundwork for future missions that use groundâ€“aerial robotic teams. In this paper, we consider rovers using machine learning to localise themselves in a local aerial map using limited field-of-view monocular ground-view RGB images as input. A key consideration for machine learning methods is that real space data with ground-truth position labels suitable for training is scarce. In this work, we propose a novel method of localising rovers in an aerial map using cross-view-localising dual-encoder deep neural networks. We leverage semantic segmentation with vision foundation models and high volume synthetic data to bridge the domain gap to real images. We also contribute a new cross-view dataset of real-world rover trajectories with corresponding ground-truth localisation data captured in a planetary analogue facility, plus a high volume dataset of analogous synthetic image pairs. Using particle filters for state estimation with the cross-view networks allows accurate position estimation over simple and complex trajectories based on sequences of ground-view images.\nFigure 1:\nA high-level overview outlining our method of estimating the state of a rover within an aerial image.\nâ€ \nâ€ \nfootnotetext:\n1\nAI for Space Group and\n3\nAndy Thomas Centre for Space Resources, The University of Adelaide.\n{lachlan.holden,feras.dayoub,david.harvey,tat-jun.chin}@adelaide.edu.au\nâ€ \nâ€ \nfootnotetext:\n2\nJet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109, USA.\nalberto.candela.garza@jpl.nasa.gov\nâ€ \nâ€ \nfootnotetext:\nPortions of this research were conducted by the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration (80NM0018D0004).\nI\nIntroduction\nAccurate localisation remains a critical challenge for autonomous planetary rovers, especially without GPS in space. With the success of helicopter-and-orbiter-based imaging, there is growing potential to localise rovers using ground-view images and pre-collected aerial maps. Accurate localisation is a key part of growing rover autonomy, which is required to support future planetary exploration missions. In this work, we consider absolute map-based cross-view localisation for rovers using a monocular RGB camera with a constrained\n90\nâ€‹\nÂ°\nhorizontal field of view.\nRGB cameras are near-ubiquitous on planetary robotics platforms, and so vision-based localisation is of particular interest. Visual odometry is a common approach, but this, like other forms of odometry used for localisation, is subject to drift. Map-based localisation can instead provide an absolute form of localisation based on landmarks or features, and so is not subject to the same drift over time.\nCross-view localisation is a problem that has gained attention in both terrestrial and planetary contexts. On Earth, deep learning methods have shown success in localising car-collected ground-view images within satellite imagery\n[\n25\n,\n19\n,\n27\n]\n. Similar methods have been applied to the space domain\n[\n3\n,\n26\n]\n, but due to the limited availability of ground-truth labelled space data suitable for training of machine learning methods, they rely primarily on synthetic images.\nA problem with relying on synthetic images for training is that they have have different appearance characteristics than real images. Machine learning models are typically sensitive to this and fail to generalise to real data if trained only on synthetic data. This problem is known as the â€œdomain gapâ€.\nOur contributions in this work, as outlined in\nFig.\n1\n, are as follows: we introduce a new cross-view dataset of real and synthetic images in a planetary-analogue setting; we leverage semantic segmentation of rocks using vision foundation models to bridge the synthetic-real domain gap; we adapt a dual-encoder cross-view localisation network and train it with synthetic data; and we demonstrate accurate localisation with these methods using particle filters and real-world rover data.\nFigure 2:\nFull rock segmentation pipeline using LLMDet and SAMÂ 2.\nII\nRelated work\nHere, we present an overview of current navigation and localisation techniques for planetary rovers, as well as works related to cross-view localisation and semantic segmentation.\nII-A\nPlanetary navigation and localisation\nPlanetary localisation has traditionally relied on visual odometry from stereo reconstruction and matching\n[\n12\n,\n1\n]\n, and feature-based map matching techniques\n[\n6\n,\n5\n]\nto estimate rover position. As stereo cameras and SLAM methods offer relative localisation, they are prone to drift over time\n[\n22\n]\n. Recent efforts have also explored matching rover imagery to aerial or satellite maps for absolute localisation\n[\n22\n,\n10\n,\n4\n]\n. These, however, often require rich geometry via stereo or depth, or handcrafted features, limiting their applicability in monocular setups.\nII-B\nDeep-learning-based cross-view localisation\nCross-view localisation using deep learning has gained traction in both terrestrial\n[\n25\n,\n19\n,\n27\n]\nand planetary\n[\n3\n,\n26\n,\n8\n]\ndomains. These works approach the problem via dual-encoder architectures, which encode ground and aerial images into a shared feature space, enabling localisation via feature similarity.\nMethods like TransGeo\n[\n27\n]\napply these approaches to Earth data, while LunarCV\n[\n3\n]\nand others\n[\n26\n,\n8\n]\napply this to simulated lunar data, with\n[\n3\n]\nrequiring the roverâ€™s bearing to be known. In\n[\n8\n]\n, integrating these models into sequential localisation frameworks with traditional odometry data using particle filters is explored. Training these models on planetary data, however, remains challenging due to the scarcity of real images with ground-truth localisation data.\nII-C\nSemantic segmentation and object detection\nSemantic segmentation is widely used in planetary robotics for terrain understanding, hazard detection, and localisation\n[\n20\n]\n. Rocks in particular serve as reliable visual landmarks on the Martian surface. Prior work has explored multi-class\n[\n24\n,\n2\n]\nand rock-specific\n[\n23\n,\n15\n]\nsegmentation using CNNs and transformers.\nTo bridge the domain gap between synthetic and real data, recent advances in vision foundation models such as LLMDet\n[\n9\n]\nfor open-vocabulary object detection and SAMÂ 2\n[\n18\n]\nfor segmentation offer robust, domain-invariant feature extraction. These models, pre-trained on broad, large-scale data, allow generalisation across domains without requiring task-specific training.\nIII\nMethod\nOur goal is to estimate a roverâ€™s pose in a local aerial map using ground-view,\n90\nâ€‹\nÂ°\nhorizontal field-of-view images. We decompose this into three parts, highlighted in\nFig.\n1\n: applying semantic segmentation to handle the domain gap (\nSec.\nIII-A\n), training a dual-encoder network to perform cross-view matching (\nSec.\nIII-B\n), and integrating this into a particle filter for state estimation over time (\nSec.\nIII-C\n).\nIII-A\nDomain-robust input via rock segmentation\nTo address the domain gap, we use foundation models LLMDet\n[\n9\n]\nfor object detection and SAMÂ 2\n[\n18\n]\nfor segmentation in our ground view images, as shown in\nFig.\n2\n.\nFor a ground-view image\nI\ng\nâ€‹\n[\n0\n,\n1\n]\nW\nâ€‹\nH\nâ€‹\nC\nI_{g}\\in[0,1]^{W\\times H\\times C}\nand full-stop-separated prompt string\nP\nP\n, the LLMDet network produces a set of\nK\nK\nbounding boxes\nB\nk\n=\n(\nx\n1\n,\ny\n1\n,\nx\n2\n,\ny\n2\n)\nk\nB_{k}=(x_{1},y_{1},x_{2},y_{2})_{k}\n:\nâ„¬\ng\n=\nLLMDet\nâ€‹\n(\nI\ng\n,\nP\n)\n=\n{\nB\nk\n}\nk\n=\n1\nK\nâ€‹\n.\n\\mathcal{B}_{g}=\\mathrm{LLMDet}(I_{g},P)=\\{B_{k}\\}_{k=1}^{K}\\text{.}\n(1)\nEach box\nB\nk\nB_{k}\nsurrounds one identified object that matches the prompt. We use the prompt string\nP\n=\nâ€œ\nrock.\nâ€\nP=\\text{``{rock.}''}\nto identify the rocks visible in our images.\nThese bounding boxes are then passed on to the SAMÂ 2 network, which attempts to produce a segmented boolean mask\nS\nk\nâ€‹\n{\n0\n,\n1\n}\nW\nâ€‹\nH\nS_{k}\\in\\{0,1\\}^{W\\times H}\nfor each box in\nâ„¬\ng\n\\mathcal{B}_{g}\n:\nğ’®\ng\n=\nSAM\nâ€‹\n(\nâ„¬\ng\n)\n=\n{\nS\nk\n}\nk\n=\n1\nK\nâ€‹\n.\n\\mathcal{S}_{g}=\\mathrm{SAM}(\\mathcal{B}_{g})=\\{S_{k}\\}_{k=1}^{K}\\text{.}\n(2)\nThe masks are filtered by size, and then the overall boolean rock mask\nR\ng\nâ€‹\n{\n0\n,\n1\n}\nW\nâ€‹\nH\nR_{g}\\in\\{0,1\\}^{W\\times H}\nis created from the pixel-wise\n(\nu\n,\nv\n)\n(u,v)\nlogical-OR combination across these masks\nR\ng\n,\n(\nu\n,\nv\n)\n=\n\\slimits@\nk\n=\n1\nK\nâ€‹\nS\nk\n,\n(\nu\n,\nv\n)\nâ€‹\n.\nR_{g,(u,v)}=\\textstyle\\bigveeop\\slimits@_{k=1}^{K}S_{k,(u,v)}\\text{.}\n(3)\nIII-B\nCross-view embedding via dual-encoder network\nWe use a network architecture based on stageÂ 1 of TransGeo\n[\n27\n]\nas our cross-view localising backbone, shown in\nFig.\n3\n. The network follows a dual-encoder structure, with separate learnable weights for the ground-view and aerial-view images. In their respective branches, the input images\nI\nâ€‹\n[\n0\n,\n1\n]\nW\nâ€‹\nH\nâ€‹\nC\nI\\in[0,1]^{W\\times H\\times C}\nfor\nH\n=\nW\n=\n256\nH=W=256\n,\nC\n=\n3\nC=3\nare divided into patches. These are passed alongside learnable positional encodings and a class token into a transformer encoder block. A fully-connected layer then produces the embeddings\ne\nâ€‹\nR\nD\ne\\in\\mathbb R^{D}\nof dimension\nD\n=\n32\nD=32\n, which are normalised so that\nâˆ¥\ne\nâˆ¥\n2\n=\n1\n\\lVert e\\rVert_{2}=1\n.\nFigure 3:\nDual-encoder cross-view localising network structure, adapted from\n[\n27\n]\n. We use\nL\n=\n12\nL=12\nencoder blocks.\nThe network is trained using a soft-margin triplet loss\n[\n11\n]\n. Our datasets consist of\nN\nN\nmatched image pairs\n{\n(\nI\ng\n,\nI\na\n)\nn\n}\nn\n=\n1\nN\n\\{(I_{g},I_{a})_{n}\\}_{n=1}^{N}\n. For a given query ground image\nI\ng\n,\ni\nI_{g,i}\n, its positive matching aerial image\nI\na\n+\n=\nI\na\n,\ni\nI_{a}^{+}=I_{a,i}\nand one negative non-matching aerial image\nI\na\nâˆ’\n=\nI\na\n,\nn\nâ€‹\ni\nI_{a}^{-}=I_{a,n\\ne i}\nare found. The triplet loss is then calculated as\nâ„’\ntriplet\n=\nlog\nâ¡\n(\n1\n+\ne\n(\nd\n+\nâˆ’\nd\nâˆ’\n)\n)\n\\mathcal{L}_{\\text{triplet}}=\\log\\left(1+e^{\\alpha(d^{+}-d^{-})}\\right)\n(4)\nfor\nd\n+\n=\nâˆ¥\ne\ng\n,\ne\na\n+\nâˆ¥\n2\n2\nâ€‹\n,\nâ€‹\nd\nâˆ’\n=\nâˆ¥\ne\ng\n,\ne\na\nâˆ’\nâˆ¥\n2\n2\nâ€‹\n.\nd^{+}=\\lVert e_{g},e_{a}^{+}\\rVert_{2}^{2}\\text{, }d^{-}=\\lVert e_{g},e_{a}^{-}\\rVert_{2}^{2}\\text{.}\n(5)\nThis loss is exhaustively applied to each of the\n2\nâ€‹\nB\nâ€‹\n(\nB\nâˆ’\n1\n)\n2B(B-1)\ntriplets â€“ including analogous triplets of the form\n(\nI\na\n,\nI\ng\n+\n,\nI\ng\nâˆ’\n)\n(I_{a},I_{g}^{+},I_{g}^{-})\nâ€“ in a training batch of\nB\nB\npairs.\nAs we are considering the circumstance where\nI\ng\nI_{g}\nhas a\n90\nâ€‹\nÂ°\nhorizontal field of view, we define\nI\na\nI_{a}\nand\nI\ng\nI_{g}\nso that the camera location of\nI\ng\nI_{g}\nis the lower-left corner of\nI\na\nI_{a}\n, and the forwards direction of\nI\ng\nI_{g}\nruns along the diagonal towards the centre of\nI\na\nI_{a}\n.\nThe same network structure and training process is used for localising using the segmented rock masks, with segmentation outputs\nR\ng\nR_{g}\nand\nR\na\nR_{a}\nused in place of\nI\ng\nI_{g}\nand\nI\na\nI_{a}\n.\nIII-C\nParticle filtering for state estimation\nThe rover is localised over time with a particle filter\n[\n21\n]\n, relative to an overall geo-referenced local aerial map image\nI\nA\nI_{A}\n(or corresponding rock mask\nR\nA\nR_{A}\n). The filter uses outputs from our networks as its measurement model.\nThe state space\nğ’™\nt\n=\n[\nx\n,\ny\n,\n]\n\\boldsymbol{x}_{t}=[x,y,\\beta]\nis used for the state of our rover at time\nt\nt\n, comprising the roverâ€™s lateral position\nx\nx\n,\ny\ny\nand bearing angle . Hence, particles\nğ’™\nt\n(\nm\n)\n\\boldsymbol{x}_{t}^{(m)}\nwith weights\nw\nt\n(\nm\n)\nw_{t}^{(m)}\nfor\nm\n=\n1\n,\nâ€¦\n,\nM\nm=1,\\ldots,M\nare tracked.\nPrediction uses an approximately constant velocity model, allowing for variations in bearing. At each prediction step, new bearings and distances\nN\nt\n(\nm\n)\n(\n,\n2\nt\nâˆ’\n1\n(\nm\n)\n)\nand\nd\nt\n(\nm\n)\nN\n(\n,\nd\n)\nd\n2\n{}_{t}^{(m)}\\sim\\mathrm{N}({}_{t-1}^{(m)},^{2})\\text{ and }d_{t}^{(m)}\\sim\\mathrm{N}({}_{d},{}_{d}^{2})\n(6)\nare sampled, and then the state is propagated as\nğ’™\nt\n(\nm\n)\n=\n[\nx\nt\nâˆ’\n1\n(\nm\n)\n+\nd\nt\n(\nm\n)\nsin\n,\nt\n(\nm\n)\ny\nt\nâˆ’\n1\n(\nm\n)\n+\nd\nt\n(\nm\n)\ncos\n,\nt\n(\nm\n)\n]\nt\n(\nm\n)\n.\n\\boldsymbol{x}_{t}^{(m)}=[x_{t-1}^{(m)}+d_{t}^{(m)}\\sin{}_{t}^{(m)},y_{t-1}^{(m)}+d_{t}^{(m)}\\cos{}_{t}^{(m)},{}_{t}^{(m)}]\\text{.}\n(7)\nThe observation at each time step is a new ground image\nI\ng\n,\nt\nI_{g,t}\n. To re-weight the particles, the localised aerial patch\nI\na\n,\nt\n(\nm\n)\nI_{a,t}^{(m)}\nis extracted from\nI\nA\nI_{A}\nfor each particle.\nI\na\n,\nt\n(\nm\n)\nI_{a,t}^{(m)}\nis aligned to the particle state\nğ’™\nt\n(\nm\n)\n\\boldsymbol{x}_{t}^{(m)}\nas with\nI\ng\nI_{g}\nin\nSec.\nIII-B\n. The embeddings\ne\ng\n,\nt\ne_{g,t}\nand\ne\na\n,\nt\n(\nm\n)\ne_{a,t}^{(m)}\nare calculated, and the cosine similarity\ns\nt\n(\nm\n)\ns_{t}^{(m)}\nbetween them is found. The weights are then calculated as\nw\nt\n(\nm\n)\n=\nb\ns\nt\n(\nm\n)\nw_{t}^{(m)}=b^{s_{t}^{(m)}}\n(8)\nfor base\nb\nb\nbefore re-normalisation. Resampling is employed to avoid sample impoverishment.\nIV\nDataset\nTo investigate this cross-view localisation method, we have created a cross-view dataset from images and motion data captured with a rover in the Extraterrestrial Environment Simulation (Exterres) laboratory at the University of Adelaide. This is complemented by a higher-volume synthetic dataset designed to appear similar to the laboratory environment, generated using PANGU\n[\n16\n]\n. The combined datasets are available at\nhttps://doi.org/10.5281/zenodo.17364038\n, and examples are presented in the supplementary video.\nIV-A\nLaboratory-collected data\nThe laboratory dataset was collected in a rectangular sandpit of approximately\n\\qtyproduct\n3 x 5. The pit has blackout curtains around all four sides and ceiling, and is lit by ambient lights in the top corners. A mix of real and foam prop rocks are distributed across the surface, to serve as the primary features for localisation.\nSix different rock configurations were set up, and then for each, an aerial still RGB image was collected. A Leo Rover\n[\n14\n]\nwith an attached camera was then driven remotely across the surface in multiple trajectories, and ground-view RGB video was captured. While the rover was driving, an OptiTrack infra-red motion capture system\n[\n17\n]\nwas capturing the six-degree-of-freedom position and orientation of a cluster of IR markers attached to the rover. The tracked locations of additional IR markers in the corners of the pit, together with checkerboard calibration of the aerial camera, enabled the aerial images to be rectified and geo-referenced to the rover motion capture data. A few sample images are shown in\nFig.\n4\n, and further statistics about the dataset are presented in\nTab.\nI\n.\nFigure 4:\nExamples of ground view (bottom) and rectified aerial view (top) images in our planetary analogue dataset.\nTABLE I:\nDataset statistics summary.\nDescription\nValue\nLaboratory dataset\nRock configurations\n6\nTraverses\nâ€”per configuration\n3 to 9\nâ€”total\n40\nTraverse duration\nâ€”average\n32\ns\n32\\text{\\,}\\mathrm{s}\nâ€”total\n20\nmin\n20\\text{\\,}\\mathrm{min}\n58\ns\n58\\text{\\,}\\mathrm{s}\nCamera hardware\nGoPro HERO10 Black\nAerial image resolution\n\\numproduct\n5568 x 4176\nGround video\nâ€”resolution\n\\numproduct\n3840 x 2160\nâ€”horizontal field of view\n*\n*\n*\nThe original video is\n120\nâ€‹\nÂ°\nhorizontal field of view, but we crop to the central\n90\nâ€‹\nÂ°\nfor our experiments.\n120\nâ€‹\nÂ°\nâ€”capture rate\n30\nHz\n30\\text{\\,}\\mathrm{Hz}\nMotion capture refresh rate\n120\nHz\n120\\text{\\,}\\mathrm{Hz}\nSynthetic dataset\nScenes\n10\nImage pairs per scene\n500\nAerial image resolution\n\\numproduct\n512 x 512\nGround image\nâ€”resolution\n\\numproduct\n512 x 512\nâ€”horizontal field of view\n90\nâ€‹\nÂ°\nIV-B\nSynthetic data\nThe synthetic dataset, created using the PANGU software\n[\n16\n]\n, comprises pairs of aerial and ground images of procedurally generated terrain and rocks, tuned to produce output that resembles the laboratory-collected data. The synthetic scenes, like the real-world sandpit, are limited to a rectangular area. Fractal algorithms built into PANGU generate a height map and a colour albedo map for the base terrain of each scene. Additional built-in algorithms generate and place rocks within the scenes, following prescribed size, colour, depth, and shape distributions.\nTen scenes are generated, each initialised with different random seeds. In each scene, a set of random rover poses are sampled across the terrain, and RGB images of a ground view and corresponding aerial view are captured. The lighting configuration is varied for each image pair, with sun azimuth and elevation angles sampled uniformly from\n[\n0\nâ€‹\nÂ°\n,\n360\nâ€‹\nÂ°\n)\n[$$,$$)\nand\n[\n40\nâ€‹\nÂ°\n,\n60\nâ€‹\nÂ°\n]\n[$$,$$]\nrespectively. Further details can again be found in\nTab.\nI\n.\nV\nExperiments\nIn this section, we outline the experiments and results for the different elements of our cross-view localisation method.\nV-A\nDomain-robust rock segmentation\nFirst, we demonstrate the ability of foundation models to segment rocks from our images.\nFig.\n5\nshows qualitative examples of LLMDet + SAMÂ 2 successfully addressing the domain gap by segmenting rocks across data domains.\nFigure 5:\nQualitative examples showing the success of the LLMDet + SAMÂ 2 segmentation on a real Mars image (top left), real planetary analogue image (top right), and synthetic image (bottom).\nA ground-truth set of segmented rock masks was constructed by manually guiding SAMÂ 2. Images from three of the ground-truth real data trajectories (\nN\n=\n96\nN=96\n) were segmented from manually selected points describing each visible rock.\nTo evaluate the combination of LLMDetâ€™s object detection with SAMÂ 2 against manual prompting, the full segmentation pipeline in\nFig.\n2\nwas run on the same images to create a prediction set of masks. LLMDet was configured with box and text thresholds\nt\nb\n=\nt\nt\n=\n0.2\nt_{b}=t_{t}=0.2\n. This pipeline is also compared against using SAMÂ 2 alone, where a uniform grid of points is used to guide the segmenter to attempt to find key objects in the images. SAMÂ 2 used a\n\\numproduct\n32 x 32 point grid, with IoU threshold 0.95, stability score threshold 0.97, box non-maximum suppression threshold 0.95, and zero crop layers.\nThe pixel-level prediction statistics are summarised in\nTab.\nII\n. These demonstrate that the use of LLMDet with SAMÂ 2 over SAMÂ 2 alone is highly advantageous, and that this model has high precision at the cost of weaker recall. This is useful, as preliminary experiments indicated that our neural network was much more sensitive to false positives than false negatives in real-world localisation.\nTABLE II:\nAutomatic rock mask prediction statistics, compared against ground truth manually guided SAMÂ 2 masks.\nPrecision\nRecall\nAutomatic method\nMean\nStd.\nMean\nStd.\nLLMDet + SAM 2\n0.934\n0.063\n0.671\n0.287\nSAM 2\n0.770\n0.189\n0.443\n0.257\nTABLE III:\nValidation results of the cross-view localising neural network backbone. Details top-\nk\nk\nmatching results, as well as the mean similarity separation\nd\n+\nâˆ’\nd\nâˆ’\nd^{+}-d^{-}\nas in\n5\n, where higher is better.\nValidation image-matching results\nTraining data\nValidation data\ntop-1\ntop-5\ntop-1%\ntop-20%\nMean similarity separation\nSynthetic RGB (\nN\n=\n4500\nN=4500\n)\nSynthetic RGB (\nN\n=\n1000\nN=1000\n)\n88.0%\n98.0%\n99.0%\n100.0%\n0.8698\nReal RGB (\nN\n=\n676\nN=676\n)\n1.2%\n4.9%\n6.2%\n46.9%\n0.1263\nReal RGB (\nN\n=\n682\nN=682\n)\nReal RGB (\nN\n=\n676\nN=676\n)\n5.2%\n19.8%\n22.2%\n90.2%\n0.6007\nSynthetic RGB + Real RGB\nReal RGB (\nN\n=\n676\nN=676\n)\n16.1%\n56.8%\n62.7%\n99.4%\n0.6578\nSynthetic masks (\nN\n=\n4500\nN=4500\n)\nSynthetic masks (\nN\n=\n1000\nN=1000\n)\n66.9%\n87.1%\n91.5%\n99.7%\n0.7938\nReal masks (\nN\n=\n96\nN=96\n)\n14.6%\n49.0%\nâ€“\n80.2%\n0.3430\nV-B\nLearning dual-encoder cross-view embeddings\nNext, we train and evaluate our network backbone on different combinations of training and validation data. Throughout this section, we consider a synthetic and a real dataset. The synthetic dataset comprises PANGU image pairs (\nN\ntrain\n=\n4000\nN_{\\text{train}}=4000\n,\nN\nval\n=\n1000\nN_{\\text{val}}=1000\n) at random poses. The real dataset comprises image pairs (\nN\ntrain\n=\n682\nN_{\\text{train}}=682\n,\nN\nval\n=\n676\nN_{\\text{val}}=676\n) extracted from real rover trajectories. The six real data rock configurations were divided into training and validation sets, and the corresponding image pairs from two runs in each configuration were extracted, sampled from the ground video at\n1\nHz\n1\\text{\\,}\\mathrm{Hz}\n. The validation results of matching aerial images to ground images are presented in table\nTab.\nIII\n.\nEach of these networks was trained to 500 epochs with a batch size of 32. We use the ASAM optimiser\n[\n13\n]\nwith a learning rate of\n1\n10\nâˆ’\n5\n1\\text{\\times}{10}^{-5}\nand cosine scheduling. The weight of the soft-margin triplet loss as in\n4\nis set to\n=\n20\n\\alpha=20\n. The model weights are initialised as in\n[\n27\n]\n.\nThe first network is trained on our synthetic RGB image pairs. The results indicate that, like previous works, the network is able to learn to localise in synthetic space-domain data (top-20% matching = 100%). They also show that the network trained on synthetic data does not successfully generalise to also localise in real data (top-20% = 46.9%), validating our concern with the domain gap problem.\nNext a network is trained on real data. Note that the image pairs used in the real data training set are sampled from trajectories and hence have a much smaller viewpoint difference between image pairs, so the overall matching scores are lower. We find that a network trained on real data only has reasonable matching performance (top-20% = 90.2%), but a second network trained on synthetic data and then fine-tuned for a further 500 epochs on real data has considerably better performance (top-20% = 99.4%).\nFinally, we consider a network trained only on masked synthetic images. To help this network be robust to noise, the training data for the synthetic masks was produced with SAMÂ 2â€™s grid initialisation. This network successfully matches images in the synthetic masked validation set (top-20% = 99.7%). Evaluating this network on the ground-truth masked images demonstrates its successful domain adaptation (top-20% = 80.2%).\nFigure 6:\nThe six trajectories, A to F, used in the validation set for evaluating our particle filter. The start point of each is marked with a circle.\nV-C\nParticle filtering for state estimation\nFinally, we evaluate our neural network backbone as the measurement model for state estimation in a particle filter.\nV-C\n1\nReal data inference\nTo evaluate the localisation performance, we consider the six trajectories (runs A to F) that formed the validation partition of the real dataset, plotted in\nFig.\n6\n. For each trajectory we run a particle filter with\nM\n=\n500\nM=500\nparticles. We sample images from the ground-view video at\n1\nHz\n1\\text{\\,}\\mathrm{Hz}\n, and discard images that differ from the previous by a distance of less than\n0.1\nm\n0.1\\text{\\,}\\mathrm{m}\n. We condition our process model with\n=\n15\nâ€‹\nÂ°\n=$$\n,\n=\nd\n0.25\nm\n{}_{d}=$0.25\\text{\\,}\\mathrm{m}$\n, and\n=\nd\n0.1\nm\n{}_{d}=$0.1\\text{\\,}\\mathrm{m}$\n. Our particles are similarly initialised in a cloud around the starting state with\n=\n15\nâ€‹\nÂ°\n=$$\nand\n=\nd\n0.1\nm\n{}_{d}=$0.1\\text{\\,}\\mathrm{m}$\n. Our measurement model uses base\nb\n=\n1\n10\n10\nb=$1\\text{\\times}{10}^{10}$\n.\nWe start by considering three experiments â€“ RGB data with the network trained only on synthetic data (â€œRGB-Syntheticâ€), RGB data with the network trained on synthetic and fine-tuned on real data (â€œRGB-Synthetic+Realâ€), and masked data with the network trained on synthetic data only and the real data semgented using LLMDet and SAMÂ 2 (â€œMask-LLMDet+SAMâ€). Note that the data used to fine-tune the second network does not contain any images from the rock configurations used for runs A to F. The distribution of range and bearing errors compared to the ground truth at each timestep in aggregate and by run is shown in\nFig.\n7\n.\nFigure 7:\nComparison of the distribution of particle filter range and bearing errors at each timestep in aggregate and separated by run.\nThese results indicate that the localising network performs best when it is trained on real-world representative data, bypassing the domain gap. That said, our method of using semantic segmentation to overcome the domain gap allows for successful localisation in instances where real-world data is not available. Our method has a considerably lower median and maximum distance error than the synthetic-only no-domain-adaptation method across all six runs, and a similar trend for bearing error across all but one (run E).\nV-C\n2\nAutomatic versus manual segmentation\nWe also compare the inference data from the LLMDet and SAMÂ 2 segmentation pipeline against our manual ground-truth segmentation. The mask-trained network was evaluated on both image sets for three runs, with results shown in\nFig.\n8\n. Clearly the manual segmentation consistently performs better, but the difference in median distance error is small.\nFigure 8:\nComparison of particle filter errors between automatic and manual semantic segmentation of ground-view images.\nTwo qualitative examples of the localisation runs across the different methods are shown in\nFig.\n9\n. These results particularly highlight the importance of domain gap adaptation, as the RGB-Synthetic network runs diverge completely. They also demonstrate that all three other approaches are able to track the roverâ€™s state successfully, but that slightly larger local errors occur at times with the mask networks compared to the fine-tuned RGB network.\nFigure 9:\nQualitative examples of particle filter runs A (left) and B (right). Black line and circles indicate ground truth location.\nFigure 10:\nDistance error over time for known-bearing, unknown-location particle initiation across runs A to C.\nV-C\n3\nLost-in-space localisation\nWe also investigate the case of lost-in-space particle initiation, where the roverâ€™s starting bearing is known, but its position is unknown. In these experiments, the particles are initialised in an evenly-spaced grid across the sandpit surface. These results are shown in\nFig.\n10\n. As can be seen, for all three evaluated runs, the real-image fine-tuned network is able to successfully localise the rover after some initialisation period. The LLMDet+SAM network shows reasonable performance for two out of three runs, but diverges for the third (run A).\nV-D\nCompute resources\nAll of these experiments were run using a single RTX 2080Â Ti GPU. This is more powerful than typical space-qualified compute hardware at present, but as the demand for autonomy and interest in machine learning continues to increase, more advanced compute modules are becoming space qualified\n[\n7\n]\n.\nOn this hardware, the segmentation of a ground-view image using LLMDet and SAMÂ 2 takes a mean of\n811\nms\n811\\text{\\,}\\mathrm{ms}\n, with a standard deviation of\n190\nms\n190\\text{\\,}\\mathrm{ms}\n. The inference with our TransGeo-based network backbone takes a mean of\n30\nms\n30\\text{\\,}\\mathrm{ms}\nper particle, with a standard deviation of\n2.8\nms\n2.8\\text{\\,}\\mathrm{ms}\n. This results in an approximate\n15\ns\n15\\text{\\,}\\mathrm{s}\nprocessing time for each ground-view image. This does exceed the\n1\nHz\n1\\text{\\,}\\mathrm{Hz}\nat which the ground-view images were sampled, but this is a small-scale experiment and real rovers typically drive much more slowly. The frequency of the ground image sampling could also be reduced, especially if this absolute localisation method were to be used as intermittent correction for traditional relative odometry methods.\nVI\nConclusions\nWe successfully demonstrate the applicability of a dual-encoder cross-view localisation network to a planetary rover localising itself in a local aerial image. Specifically, we have investigated the challenging setting of monocular RGB images with a limited\n90\nâ€‹\nÂ°\nhorizontal-field-of-view. We use synthetic data and semantic segmentation with vision foundation models to explicitly address the domain gap between synthetic and real data, and demonstrate a successful method of localisation on real data that does not require any real labelled data at train time. We also contribute a dataset of labelled synthetic image pairs and ground-truth-localised rover trajectories with corresponding georeferenced aerial images.\nValuable future work would include reducing network size and complexity for better inference times on space-grade hardware, and integrating this absolute localisation method with traditional odometry-based relative localisation.\n\\AtNextBibliography\nReferences\n[1]\nS. Andolfo, F. Petricca, and A. Genova\n(2023)\nPrecise pose estimation of the NASA Mars 2020 Perseverance rover through a stereo-vision-based approach\n.\n40\n(\n3\n),\npp.Â 684â€“700\n.\nExternal Links:\nISSN 1556-4967\n,\nDocument\n,\nLink\nCited by:\nÂ§\nII-A\n.\n[2]\nA. M. Barrett, M. R. Balme, M. Woods, S. Karachalios, D. Petrocelli, L. Joudrier, and E. Sefton-Nash\n(2022-01-01)\nNOAH-H, a deep-learning, terrain classification system for Mars: Results for the ExoMars Rover candidate landing sites\n.\n371\n,\npp.Â 114701\n.\nExternal Links:\nISSN 0019-1035\n,\nDocument\n,\nLink\nCited by:\nÂ§\nII-C\n.\n[3]\nZ. Chen, K. Li, H. Li, Z. Fu, H. Zhang, and Y. Guo\n(2024-04-11)\nMetric localization for lunar rovers via cross-view image matching\n.\n2\n(\n1\n),\npp.Â 12\n.\nExternal Links:\nISSN 2731-9008\n,\nDocument\n,\nLink\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nII-B\n.\n[4]\nM. Dinsdale, W. Hamilton, R. Marc, P. Weclewski, A. Dysli, C. Barclay, A. Daoud-Moraru, B. Brayzier, T. Cooper, M. Braun, D. R. Hamill, G. D. Roy, and V. Croes\n(2022)\nAbsolute Localisation by Map Matching for Sample Fetch Rover\n.\nCited by:\nÂ§\nII-A\n.\n[5]\nK. Ebadi and A. Agha-Mohammadi\n(2018)\nRover Localization in Mars Helicopter Aerial Maps: Experimental Results in a Mars-Analogue Environment\n.\nIn\nProceedings of the 2018 International Symposium on Experimental Robotics\n,\nJ. Xiao, T. KrÃ¶ger, and O. Khatib (Eds.)\n,\npp.Â 72â€“84\n.\nExternal Links:\nDocument\n,\nISBN 978-3-030-33950-0\nCited by:\nÂ§\nII-A\n.\n[6]\nK. Ebadi, K. Coble, D. Kogan, D. Atha, R. Schwartz, C. Padgett, and J. V. Hook\n(2022)\nToward Autonomous Localization of Planetary Robotic Explorers by Relying on Semantic Mapping\n.\nIn\nAERO\n,\npp.Â 1â€“10\n.\nExternal Links:\nISSN 1095-323X\n,\nDocument\n,\nLink\nCited by:\nÂ§\nII-A\n.\n[7]\nM. A. Felix, W. S. Slater, D. C. Landauer, R. E. Pinson, and B. B.W. Rutherford\n(2024)\nTotal Ionizing Dose Radiation Testing of NVIDIA Jetson Orin NX System on Module\n.\nIn\nIEEE Space Computing Conference\n,\nVol.\n,\npp.Â 116â€“121\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nV-D\n.\n[8]\nV. Franchi and E. Ntagiou\n(2022)\nPlanetary Rover Localisation via Surface and Orbital Image Matching\n.\nIn\nAERO\n,\npp.Â 1â€“14\n.\nExternal Links:\nISSN 1095-323X\n,\nDocument\n,\nLink\nCited by:\nÂ§\nII-B\n,\nÂ§\nII-B\n.\n[9]\nS. Fu, Q. Yang, Q. Mo, J. Yan, X. Wei, J. Meng, X. Xie, and W. Zheng\n(2025)\nLLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models\n.\nIn\nCVPR\n,\npp.Â 14987â€“14997\n.\nExternal Links:\nLink\nCited by:\nÂ§\nII-C\n,\nÂ§\nIII-A\n.\n[10]\nJ. V. Hook, R. Schwartz, K. Ebadi, K. Coble, and C. Padgett\n(2022)\nTopographical Landmarks for Ground-Level Terrain Relative Navigation on Mars\n.\nIn\nAERO\n,\npp.Â 1â€“6\n.\nExternal Links:\nISSN 1095-323X\n,\nDocument\n,\nLink\nCited by:\nÂ§\nII-A\n.\n[11]\nS. Hu, M. Feng, R. M. H. Nguyen, and G. H. Lee\n(2018-06)\nCVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-Localization\n.\nIn\nCVPR\n,\npp.Â 7258â€“7267\n.\nExternal Links:\nDocument\n,\nLink\n,\nISBN 978-1-5386-6420-9\nCited by:\nÂ§\nIII-B\n.\n[12]\nY. Kou, W. Wan, K. Di, Z. Liu, M. Peng, Y. Wang, B. Xie, B. Wang, and C. Zhao\n(2025)\nCross-Site Visual Localization of Zhurong Mars Rover Based on Self-Supervised Keypoint Extraction and Robust Matching\n.\n63\n,\npp.Â 1â€“20\n.\nExternal Links:\nISSN 1558-0644\n,\nDocument\n,\nLink\nCited by:\nÂ§\nII-A\n.\n[13]\nJ. Kwon, J. Kim, H. Park, and I. K. Choi\n(2021-07-01)\nASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks\n.\nIn\nICML\n,\npp.Â 5905â€“5914\n.\nExternal Links:\nISSN 2640-3498\n,\nLink\nCited by:\nÂ§\nV-B\n.\n[14]\nLeo Rover\n(2025)\nLeo Rover - Outdoor Robotics Kit for research\n(Website)\nExternal Links:\nLink\nCited by:\nÂ§\nIV-A\n.\n[15]\nH. Liu, M. Yao, X. Xiao, and Y. Xiong\n(2023)\nRockFormer: A U-Shaped Transformer Network for Martian Rock Segmentation\n.\n61\n,\npp.Â 1â€“16\n.\nExternal Links:\nISSN 1558-0644\n,\nDocument\n,\nLink\nCited by:\nÂ§\nII-C\n.\n[16]\nPANGU v6: Planet and Asteroid Natural Scene Generation Utility\nExternal Links:\nLink\nCited by:\nÂ§\nIV-B\n,\nÂ§IV\n.\n[17]\nOptiTrack\n(2025)\nMotion Capture Systems\n(Website)\nOptiTrack\n.\nExternal Links:\nLink\nCited by:\nÂ§\nIV-A\n.\n[18]\nN. Ravi, V. Gabeur, Y. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. RÃ¤dle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, K. V. Alwala, N. Carion, C. Wu, R. Girshick, P. Dollar, and C. Feichtenhofer\n(2024-10-04)\nSAM 2: Segment Anything in Images and Videos\n.\nIn\nThe Thirteenth International Conference on Learning Representations\n,\nExternal Links:\nLink\nCited by:\nÂ§\nII-C\n,\nÂ§\nIII-A\n.\n[19]\nM. Shugaev, I. Semenov, K. Ashley, M. Klaczynski, N. Cuntoor, M. W. Lee, and N. Jacobs\n(2024)\nArcGeo: Localizing Limited Field-of-View Images using Cross-view Matching\n.\nIn\nWACV\n,\npp.Â 208â€“217\n.\nExternal Links:\nISSN 2642-9381\n,\nDocument\n,\nLink\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[20]\nR. M. Swan, D. Atha, H. A. Leopold, M. Gildner, S. Oij, C. Chiu, and M. Ono\n(2021)\nAI4MARS: A Dataset for Terrain-Aware Autonomous Driving on Mars\n.\nIn\nCVPR Workshops\n,\npp.Â 1982â€“1991\n.\nCited by:\nÂ§\nII-C\n.\n[21]\nS. Thrun, W. Burgard, and D. Fox\n(2005)\nProbabilistic Robotics\n.\nIntelligent Robotics and Autonomous Agents\n,\nMIT Press\n.\nExternal Links:\nISBN 978-0-262-20162-9\nCited by:\nÂ§\nIII-C\n.\n[22]\nV. Verma, J. Nash, L. Saldyt, Q. Dwight, H. Wang, S. Myint, J. Biesiadecki, M. Maimone, A. Tumbar, A. Ansar, G. Kubiak, and R. Hogg\n(2024)\nEnabling Long & Precise Drives for The Perseverance Mars Rover via Onboard Global Localization\n.\nIn\nAERO\n,\npp.Â 1â€“18\n.\nExternal Links:\nDocument\n,\nLink\n,\nISBN 979-8-3503-0462-6\nCited by:\nÂ§\nII-A\n.\n[23]\nP. Wei, Z. Sun, and H. Tian\n(2025-01-10)\nRocknet: lightweight network for real-time segmentation of Martian rocks\n.\n22\n(\n1\n),\npp.Â 41\n.\nExternal Links:\nISSN 1861-8219\n,\nDocument\n,\nLink\nCited by:\nÂ§\nII-C\n.\n[24]\nJ. Zhang, L. Lin, Z. Fan, W. Wang, and J. Liu\n(2024)\nS5Mars: Semi-Supervised Learning for Mars Semantic Segmentation\n.\n62\n,\npp.Â 1â€“15\n.\nExternal Links:\nISSN 1558-0644\n,\nDocument\n,\nLink\nCited by:\nÂ§\nII-C\n.\n[25]\nW. Zhang, J. Li, H. Chen, and J. Wu\n(2024)\nMT-GEO: A Multi-Scale Feature Extraction Network for Cross-View Geo-Localization Between Street-View and Remote Sensing Imagery\n.\nIn\nIGARSS\n,\npp.Â 6964â€“6968\n.\nExternal Links:\nISSN 2153-7003\n,\nDocument\n,\nLink\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[26]\nX. Zhao, L. Cui, X. Wei, C. Liu, and J. Yin\n(2024)\nLunar Rover Cross-View Localization Through Integration of Rover and Orbital Images\n.\n62\n,\npp.Â 1â€“14\n.\nExternal Links:\nISSN 1558-0644\n,\nDocument\n,\nLink\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nII-B\n.\n[27]\nS. Zhu, M. Shah, and C. Chen\n(2022)\nTransGeo: Transformer Is All You Need for Cross-view Image Geo-localization\n.\nIn\nCVPR\n,\npp.Â 1152â€“1161\n.\nExternal Links:\nISSN 2575-7075\n,\nDocument\n,\nLink\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nII-B\n,\nFigure 3\n,\nÂ§\nIII-B\n,\nÂ§\nV-B\n.",
    "preview_text": "Accurate localisation in planetary robotics enables the advanced autonomy required to support the increased scale and scope of future missions. The successes of the Ingenuity helicopter and multiple planetary orbiters lay the groundwork for future missions that use ground-aerial robotic teams. In this paper, we consider rovers using machine learning to localise themselves in a local aerial map using limited field-of-view monocular ground-view RGB images as input. A key consideration for machine learning methods is that real space data with ground-truth position labels suitable for training is scarce. In this work, we propose a novel method of localising rovers in an aerial map using cross-view-localising dual-encoder deep neural networks. We leverage semantic segmentation with vision foundation models and high volume synthetic data to bridge the domain gap to real images. We also contribute a new cross-view dataset of real-world rover trajectories with corresponding ground-truth localisation data captured in a planetary analogue facility, plus a high volume dataset of analogous synthetic image pairs. Using particle filters for state estimation with the cross-view networks allows accurate position estimation over simple and complex trajectories based on sequences of ground-view images.\n\nVision Foundation Models for Domain Generalisable\nCross-View Localisation in Planetary Groundâ€“Aerial Robotic Teams\nLachlan Holden\n1\n, Feras Dayoub\n1\n, Alberto Candela\n2\n, David Harvey\n3\n, and Tat-Jun Chin\n1\nAbstract\nAccurate localisation in planetary robotics enables the advanced autonomy required to support the increased scale and scope of future missions. The successes of the Ingenuity helicopter and multiple planetary orbiters lay the groundwork for future missions that use groundâ€“aerial robotic teams. In this paper, we consider rovers using machine learning to localise themselves in a local aerial map using limited field-of-view monocular ground-view RGB images as input. A key con",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "VLA",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰åŸºç¡€æ¨¡å‹å’Œè·¨è§†å›¾å®šä½çš„æ–¹æ³•ï¼Œç”¨äºè¡Œæ˜Ÿåœ°é¢-ç©ºä¸­æœºå™¨äººå›¢é˜Ÿä¸­çš„å®šä½ï¼Œä¸VLAã€locomotionå’Œwhole body controlæœ‰é—´æ¥ç›¸å…³æ€§ï¼Œä½†ä¸Reinforcement Learningã€diffusionã€Flow Matchingå’ŒVLMæ— ç›´æ¥å…³è”ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-14T03:11:05Z",
    "created_at": "2026-01-20T17:49:47.803714",
    "updated_at": "2026-01-20T17:49:47.803724"
}