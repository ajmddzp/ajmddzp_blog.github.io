{
    "id": "2601.13565v1",
    "title": "Learning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation",
    "authors": [
        "Yu Qin",
        "Shimeng Fan",
        "Fan Yang",
        "Zixuan Xue",
        "Zijie Mai",
        "Wenrui Chen",
        "Kailun Yang",
        "Zhiyong Li"
    ],
    "abstract": "å¼€æ”¾è¯æ±‡6Dç‰©ä½“å§¿æ€ä¼°è®¡ä½¿æœºå™¨äººèƒ½å¤Ÿä»…å‡­è‡ªç„¶è¯­è¨€æŒ‡å¯¼æ¥æ“ä½œä»»æ„æœªè§è¿‡çš„ç‰©ä½“ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•çš„ä¸€ä¸ªå…³é”®å±€é™åœ¨äºå…¶ä¾èµ–æ— çº¦æŸçš„å…¨å±€åŒ¹é…ç­–ç•¥ã€‚åœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­ï¼Œå°è¯•å°†é”šç‚¹ç‰¹å¾ä¸æ•´ä¸ªæŸ¥è¯¢å›¾åƒç©ºé—´è¿›è¡ŒåŒ¹é…ä¼šå¼•å…¥è¿‡å¤šæ­§ä¹‰ï¼Œå› ä¸ºç›®æ ‡ç‰¹å¾ææ˜“ä¸èƒŒæ™¯å¹²æ‰°ç‰©æ··æ·†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºç»†ç²’åº¦å¯¹åº”å§¿æ€ä¼°è®¡æ¡†æ¶ï¼ˆFiCoPï¼‰ï¼Œè¯¥æ¡†æ¶å°†ä»æ˜“å—å™ªå£°å½±å“çš„å…¨å±€åŒ¹é…è½¬å‘ç©ºé—´å—é™çš„å—çº§å¯¹åº”ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨å—å¯¹å—å…³è”çŸ©é˜µä½œä¸ºç»“æ„å…ˆéªŒæ¥ç¼©å°åŒ¹é…èŒƒå›´ï¼Œæœ‰æ•ˆæ»¤é™¤æ— å…³æ‚æ³¢ï¼Œé˜²æ­¢å…¶é™ä½å§¿æ€ä¼°è®¡ç²¾åº¦ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„è§£è€¦é¢„å¤„ç†ï¼Œå°†è¯­ä¹‰ç›®æ ‡ä¸ç¯å¢ƒå™ªå£°åˆ†ç¦»ã€‚å…¶æ¬¡ï¼Œæå‡ºè·¨è§†è§’å…¨å±€æ„ŸçŸ¥æ¨¡å—ï¼Œé€šè¿‡æ˜¾å¼ä¸Šä¸‹æ–‡æ¨ç†èåˆåŒè§†è§’ç‰¹å¾å¹¶å»ºç«‹ç»“æ„å…±è¯†ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†å—å…³è”é¢„æµ‹å™¨ï¼Œç”Ÿæˆç²¾ç¡®çš„å—çº§å…³è”å›¾ä½œä¸ºç©ºé—´è¿‡æ»¤å™¨ï¼Œå®ç°æŠ—å™ªå£°çš„ç»†ç²’åº¦åŒ¹é…ã€‚åœ¨REAL275å’ŒToyota-Lightæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFiCoPç›¸æ¯”æœ€å…ˆè¿›æ–¹æ³•å°†å¹³å‡å¬å›ç‡åˆ†åˆ«æå‡8.0%å’Œ6.1%ï¼Œå‡¸æ˜¾äº†å…¶åœ¨å¤æ‚æ— çº¦æŸå¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ä¸ºæœºå™¨äººæä¾›é²æ£’æ³›åŒ–æ„ŸçŸ¥çš„èƒ½åŠ›ã€‚æºä»£ç å°†åœ¨https://github.com/zjjqinyu/FiCoPå…¬å¼€ã€‚",
    "url": "https://arxiv.org/abs/2601.13565v1",
    "html_url": "https://arxiv.org/html/2601.13565v1",
    "html_content": "Learning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation\nYu Qin\n1\n, Shimeng Fan\n2\n, Fan Yang\n1\n, Zixuan Xue\n1\n, Zijie Mai\n1\n, Wenrui Chen\n1,3\n,\nKailun Yang\n1,3,âˆ—\n, and Zhiyong Li\n1,3,âˆ—\nThis work was supported by the National Key R&D Program of China under Grant 2022YFB4701400/2022YFB4701404, the National Natural Science Foundation of China under Grant 62273137, 62473139, No. U21A20518, and No. U23A20341, the Hunan Provincial Research and Development Project under Grant 2025QK3019, the Hunan Science Fund for Distinguished Young Scholars under Grant 2024JJ2027, the Special Funds for Construction of Innovative Provinces in Hunan Province under Grant 2025QK1005, and the State Key Laboratory of Autonomous Intelligent Unmanned Systems (the opening project number ZZKF2025-2-10).\n1\nY. Qin, F. Yang, Z. Xue, Z. Mai, W. Chen, K. Yang, and Z. Li are with the School of Artificial Intelligence and Robotics, Hunan University, Changsha 410012, China. (E-mail: kailun.yang@hnu.edu.cn, zhiyong.li@hnu.edu.cn.)\n2\nS. Fan is with the School of Computer Science and Engineering, Hunan University of Science and Technology, Xiangtan 411201, China.\n3\nW. Chen, K. Yang, and Z. Li are also with the National Engineering Research Center of Robot Visual Perception and Control Technology, Hunan University, Changsha 410082, China.\nâˆ—\nCorresponding authors: Kailun Yang and Zhiyong Li.\nAbstract\nOpen-vocabulary 6D object pose estimation empowers robots to manipulate arbitrary unseen objects guided solely by natural language. However, a critical limitation of existing approaches is their reliance on unconstrained global matching strategies. In open-world scenarios, trying to match anchor features against the entire query image space introduces excessive ambiguity, as target features are easily confused with background distractors. To resolve this, we propose Fine-grained Correspondence Pose Estimation (FiCoP), a framework that transitions from noise-prone global matching to spatially-constrained patch-level correspondence. Our core innovation lies in leveraging a patch-to-patch correlation matrix as a structural prior to narrowing the matching scope, effectively filtering out irrelevant clutter to prevent it from degrading pose estimation. Firstly, we introduce an object-centric disentanglement preprocessing to isolate the semantic target from environmental noise. Secondly, a Cross-Perspective Global Perception (CPGP) module is proposed to fuse dual-view features, establishing structural consensus through explicit context reasoning. Finally, we design a Patch Correlation Predictor (PCP) that generates a precise block-wise association map, acting as a spatial filter to enforce fine-grained, noise-resilient matching. Experiments on the REAL275 and Toyota-Light datasets demonstrate that FiCoP improves Average Recall by 8.0% and 6.1%, respectively, compared to the state-of-the-art method, highlighting its capability to deliver robust and generalized perception for robotic agents operating in complex, unconstrained open-world environments. The source code will be made publicly available at\nhttps://github.com/zjjqinyu/FiCoP\n.\nI\nIntroduction\nIn the pursuit of general-purpose robotics, 6D object pose estimation is the prerequisite for enabling agents to interact with the physical world. While traditional methods rely on pre-scanned CAD models\n[\n6\n,\n16\n,\n22\n]\n, the field is shifting towards open-vocabulary paradigms enabled by Vision-Language Models (VLMs)\n[\n25\n,\n30\n]\n, allowing robots to perceive novel objects via textual descriptions.\nHowever, achieving robust pose estimation in unconstrained environments remains a formidable challenge, particularly when there are large viewpoint differences between the reference (anchor) and the current observation (query).\nFigure 1:\nComparison between our method and previous methods.\nPrevious methods\n[\n5\n,\n4\n]\nbased on global matching are prone to incorrect matching.\nOur method gradually refines the matching area through object-centric disentanglement and patch-to-patch correlation priors, aiming to promote more accurate matching.\nExisting open-vocabulary methods\n[\n5\n,\n4\n]\ntypically approach this problem through global semantic matching.\nThey extract features from the anchor and attempt to match them with the entire query image.\nFrom the perspective of robotic manipulation, such an unconstrained search is inherently challenging in cluttered scenes.\nWhen the viewpoint changes drastically, the objectâ€™s appearance deforms, and simple global similarity scores fail to distinguish the object parts from the changing appearance and similar-looking background noise, as shown in Fig.\n1\n(a).\nThis leads to a high error rate in pose prediction, as pixels in the anchor are often incorrectly matched to clutter in the query view.\nRegarding the inherent limitations of multimodal features, VLM representations suffer from a granularity mismatch: they are excellent at capturing high-level semantics but lack the spatial discrimination during global matching. Without a mechanism to constrain the search space, the distractors overwhelm the geometric information of the object.\nTo overcome these issues, we draw inspiration from the cognitive mechanisms of humans.\nWhen humans re-identify an object from a new angle, we do not scan every detail against the entire scene.\nInstead, we perform a hierarchical process\n[\n10\n]\n: we first locate the position of the object, then focus on relevant local regions that share structural similarity, and finally establish precise correspondences within those focused areas.\nThis coarse-to-fine attention mechanism naturally filters out irrelevant visual information.\nGuided by this insight, this letter proposes FiCoP for\nFine-Grained\nCorrespondence\nPose\nEstimation.\nUnlike previous methods\n[\n5\n,\n4\n]\nthat perform coarse-grained global matching, FiCoP introduces a fine-grained correspondence learning mechanism, as shown in Fig.\n1\n(b).\nThe core concept is to utilize a patch-to-patch correlation matrix as a strong spatial prior. Instead of allowing an anchor pixel to match with any query pixel, our model first locates the object and then predicts which local patches in the query are structurally correlated with the anchor. This narrows the matching scope significantly.\nIf an anchor patch is predicted to correspond only to the query patch, the subsequent pixel-level matching is confined within this region, automatically suppressing interference from the rest of the image.\nTo comprehensively evaluate the effectiveness of our approach, we have conducted extensive evaluations on the REAL275\n[\n27\n]\nand Toyota-Light\n[\n11\n]\nbenchmarks.\nThe results demonstrate that FiCoP establishes a new state-of-the-art in open-vocabulary pose estimation, outperforming existing methods\n[\n5\n,\n4\n,\n28\n,\n17\n,\n7\n,\n21\n,\n24\n]\nby a substantial margin.\nSpecifically, on the REAL275 and Toyota-Light datasets, our method achieves an average recall improvement of 8.0% and 6.1%, respectively, compared to the state-of-the-art Horyon method that relies on global matching.\nCrucially, qualitative analysis confirms that our fine-grained strategy successfully recovers accurate pose parameters even in scenarios with drastic viewpoint alterations where traditional global features typically collapse.\nThese findings support our hypothesis that imposing spatial constraints via patch-level priors can effectively mitigate background ambiguity and enhance robust perception in unconstrained environments.\nThe main contributions of this letter can be summarized as follows:\nâ€¢\nWe design a Patch Correlation Predictor (PCP) as the engine of our fine-grained strategy.\nIt explicitly computes the patch-wise similarity map to generate a spatially constrained search region, ensuring that final pixel correspondences are both semantically correct and geometrically precise.\nâ€¢\nWe propose a Cross-Perspective Global Perception (CPGP) module, a transformer-based module that fuses features from both views to reason about structural consistency, enabling the model to predict patch correlations even under large geometric deformations.\nâ€¢\nWe introduce an Object-Centric Disentanglement preprocessing pipeline that utilizes an open-vocabulary object detection model for preliminary object localization.\nBy replacing the unstable self-predicting masks employed in existing techniques with the robust zero-shot segmentation capabilities of the SAM model, we ensure that feature correlation is strictly confined to the object region, thereby unleashing the potential of open-vocabulary foundation models.\nII\nRelated Work\nII-A\n6D Pose Estimation in Classic Settings\nThe classic 6D pose estimation methods are generally categorized into instance-level, category-level, and unseen objects.\nInstance-level methods, such as DenseFusion\n[\n26\n]\n, RCVPose\n[\n29\n]\n, SCFlow\n[\n8\n]\n, and Uni6D\n[\n13\n]\n, achieve high precision by establishing geometric correspondences or direct regression but are limited to determining the pose of specific instances seen during training.\nTo generalize across instances, category-level methods\n[\n27\n,\n3\n,\n18\n]\nlearn shared canonical representations to estimate poses for objects within known categories. However, they rely heavily on category-specific shape priors.\nFor unseen objects, methods like Megapose\n[\n15\n]\nand Gen6D\n[\n20\n]\nemploy render-and-compare optimization or reference image matching. While generalizing better, they still necessitate CAD models or multi-view references, restricting their utility in unstructured environments.\nUnlike these traditional approaches that rely heavily on explicit CAD models or category-specific shape priors, our method adopts a model-free paradigm driven solely by textual descriptions, enabling true zero-shot generalization in the open world.\nFigure 2:\nThe framework of our proposed Fine-grained Correspondence Pose Estimation (FiCoP) model.\nIt consists of two stages: (a) a preprocessing pipeline that utilizes an open-vocabulary object detection model and a SAM model to generate cropped object images and masks; (b) a model forwarding process that takes the preprocessing results as input to generate high-resolution features for anchor and query, as well as patch correlation maps.\nFigure 3:\nStructure of the Cross-Perspective Global Perception (CPGP) module. This module facilitates information interaction between the anchor and query perspectives.\nII-B\nOpen-Vocabulary 6D Pose Estimation\nThe advent of large-scale Visual Language Models (VLMs) like CLIP\n[\n25\n]\nhas shifted computer vision from closed-set recognition to open-vocabulary learning.\nIn 6D pose estimation, category-level methods such as OV9D\n[\n2\n]\nand LightPose\n[\n12\n]\nutilize cross-modal knowledge to generalize across textual descriptions, yet they remain confined to known categories.\nThe pioneering open-vocabulary paradigm for unseen objects, introduced by Oryon\n[\n5\n]\n, overcomes this by specifying objects solely through a textual prompt, using a CLIP-based fusion module to integrate semantic cues with local geometry for cross-scene segmentation and matching without any prior object model.\nThe subsequent model Horyon\n[\n4\n]\nsignificantly advances this paradigm by addressing the critical limitations of low-resolution fusion features and background clutter.\nDue to the challenging nature of this paradigm, estimated pose accuracy is typically not very precise, especially when dealing with significant view differences between anchors and queries.\nAdditionally, relative pose estimation methods\n[\n28\n,\n17\n,\n24\n]\n, and point cloud registration methods\n[\n7\n,\n21\n,\n24\n]\nhave been adapted for open-vocabulary pose estimation by incorporating external open-vocabulary object detectors or masks.\nHowever, these methods typically rely on unconstrained global semantic matching, which often degenerates into ambiguity when facing substantial viewpoint discrepancies or background clutter.\nIn contrast, our work advances this paradigm by introducing a fine-grained patch-constrained mechanism and explicit cross-perspective interaction, effectively recovering the precise geometric structural consistency that is often lost in coarse global features.\nIII\nMethodology\nIII-A\nProblem Formulation\nWe describe the open-vocabulary 6D object pose estimation problem: given an image pair consisting of a reference anchor\nğˆ\nA\n\\mathbf{I}^{A}\nand a current observation query\nğˆ\nQ\n\\mathbf{I}^{Q}\n, along with a natural language description\nT\nT\nof the target object, our goal is to predict the rigid transformation\nğ“\nA\nâ†’\nQ\nâˆˆ\nS\nâ€‹\nE\nâ€‹\n(\n3\n)\n\\mathbf{T}_{A\\to Q}\\in SE(3)\n. This transformation aligns the objectâ€™s coordinate system from the anchor view to the query view.\nUnlike the traditional paradigm requiring CAD models\n[\n29\n,\n8\n,\n15\n,\n20\n]\n, we rely solely on\nT\nT\nto identify the target, necessitating a robust mechanism to filter environmental distractors and establish precise correspondences across large viewpoint changes.\nIII-B\nObject-Centric Disentanglement Preprocessing\nTo address the global matching ambiguity highlighted in the introduction, we first implement an Object-Centric Disentanglement Preprocessing. As shown in Fig.\n2\n(a), this stage first employs GroundingDINO\n[\n19\n]\nto localize the target object in both anchor and query images based on the provided text prompt\nT\nT\n. This module outputs bounding boxes that tightly enclose the object of interest. Subsequently, the Segment Anything Model (SAM)\n[\n14\n]\nis applied to generate preliminary segmentation masks for the localized objects.\nThis preprocessing step effectively isolates the object from background clutter while outputting high-quality object masks, which is critical for subsequent accurate pose estimation.\nIII-C\nFeature Extraction and Fusion\nAs illustrated in Fig.\n2\n(b), we employ DINOv2\n[\n23\n]\nas our visual backbone\nÏ•\nV\n\\phi_{V}\nto extract hierarchical features from both cropped images,\ni.e.\nğ„\nA\n=\nÏ•\nV\nâ€‹\n(\nğˆ\nA\n)\n\\mathbf{E}^{A}=\\phi_{V}(\\mathbf{I}^{A})\n,\nğ„\nQ\n=\nÏ•\nV\nâ€‹\n(\nğˆ\nQ\n)\n\\mathbf{E}^{Q}=\\phi_{V}(\\mathbf{I}^{Q})\n.\nThe self-supervised pretrained DINOv2 provides robust, generalizable representations that are particularly effective for unseen object categories. For textual understanding, we utilize the CLIP text encoder\nÏ•\nT\n\\phi_{T}\n[\n25\n]\nto convert the text prompt\nT\nT\ninto a dense embedding\nğ\nT\n\\mathbf{e}^{T}\n.\nThe extracted visual features\nğ„\nA\n\\mathbf{E}^{A}\nand\nğ„\nQ\n\\mathbf{E}^{Q}\nare fused with the text embedding\nğ\nT\n\\mathbf{e}^{T}\nthrough a feature fusion module\nÏ•\nT\nâ€‹\nV\n\\phi_{TV}\n.\nÏ•\nT\nâ€‹\nV\n\\phi_{TV}\nadopts the same design as the Oryon\n[\n5\n]\nmodel. It integrates visual and textual information through a multi-stage process designed to establish robust cross-modal correspondences. This fusion strategy effectively aligns visual features with textual semantics, creating a unified representation.\nFigure 4:\nStructure of the Patch Correlation Predictor (PCP).\nA patch correlation map is generated through a carefully designed process to establish fine-grained spatial correspondences between anchor and query.\nIII-D\nCross-Perspective Global Perception Module\nTo handle drastic viewpoint changes where simple similarity metrics fail, we introduce the Cross-Perspective Global Perception (CPGP) module\nÏ•\nP\n\\phi_{P}\n.\nÏ•\nP\n\\phi_{P}\nestablishes robust correspondences between anchor and query views through a multi-layer transformer architecture enhanced with textual guidance. As illustrated in Fig.\n3\n, the module receives two fused features\nğ„\nÂ¯\nA\n\\overline{\\mathbf{E}}^{A}\nand\nğ„\nÂ¯\nQ\n\\overline{\\mathbf{E}}^{Q}\nfrom the previous stage, which are first combined through a concatenation operation before passing through a down projection layer. The down-projection layer reduces feature channel dimensions while preserving key information and reducing computational complexity. The core of the module consists of\nL\n1\nL_{1}\nidentical transformer layers, each layer including a self-attention mechanism and a cross-attention mechanism. The self-attention is employed for intra-view reasoning to aggregate local geometric context, and the cross-attention mechanism is employed for inter-view communication, where the anchor features query the observation features. This allows the model to search for semantically analogous regions across the perspective gap, effectively modeling the geometric deformation caused by viewpoint shifts.\nThe multi-layer structure allows for progressive refinement of cross-view relationships, where each subsequent layer builds upon the previous one to establish increasingly precise correspondences. After processing through all\nL\n1\nL_{1}\nlayers, the enhanced features are projected back to the original number of channels through an up projection layer and separated to produce refined representations\nğ„\n~\nA\n\\tilde{\\mathbf{E}}^{A}\nand\nğ„\n~\nQ\n\\tilde{\\mathbf{E}}^{Q}\nfor both views. The output is a pair of structurally aligned feature maps that encode implicitly established global correspondences. This module effectively bridges the perspective gap by learning to align visual features across different perspectives while being guided by textual semantics.\nIII-E\nPatch Correlation Predictor\nThe Patch Correlation Predictor module\nÏ•\nC\n\\phi_{C}\nis the core innovation of FiCoP, implementing the coarse-to-fine cognitive mechanism. Instead of allowing unconstrained pixel-wise matching, the PCP generates a patch-to-patch correlation matrix as a spatial structural prior.\nÏ•\nC\n\\phi_{C}\nestablishes fine-grained spatial correspondences between feature maps from different perspectives through a structured patch-based correlation analysis. As illustrated in Fig.\n4\n, the module processes two token feature sequences:\nğ„\n~\nA\n\\widetilde{\\mathbf{E}}^{A}\nand\nğ„\n~\nQ\n\\widetilde{\\mathbf{E}}^{Q}\n. Firstly, the module reshapes both features into spatial feature maps, then computes their correlation through matrix multiplication to obtain a similarity map of dimensionality\nâ„\nH\n2\nâ€‹\nW\n2\nÃ—\nH\n1\nÃ—\nW\n1\n\\mathbb{R}^{H_{2}W_{2}\\times H_{1}\\times W_{1}}\n. This similarity map is subsequently partitioned into\nG\nâ€‹\n1\nÃ—\nG\nâ€‹\n2\nG1\\times G2\nnon-overlapping patches of size\nP\nÃ—\nP\nP\\times P\n, where each patch represents local correlation patterns between the two views. The patch-split operation transforms the representation into\nâ„\nN\np\nÃ—\nP\n2\nÃ—\nH\n2\nÃ—\nW\n2\n\\mathbb{R}^{N_{p}\\times P^{2}\\times H_{2}\\times W_{2}}\n, with\nN\np\n=\nH\n1\nâ€‹\nW\n1\nP\n2\nN_{p}=\\frac{H_{1}W_{1}}{P^{2}}\ndenoting the total number of patches.\nThe patch-level features then pass through\nL\n2\nL_{2}\nconvolutional blocks, each comprising a Conv2D layer, BatchNorm, and ReLU activation, which refine the patch representations while preserving spatial structure. A final convolutional layer with kernel size\nP\nP\nand stride\nP\nP\naggregates information within each patch, followed by a Softmax activation to produce normalized correlation scores. The output\nC\np\nC_{p}\nis reshaped to\nâ„\nN\np\nÃ—\nH\n2\nP\nÃ—\nW\n2\nP\n\\mathbb{R}^{N_{p}\\times\\frac{H_{2}}{P}\\times\\frac{W_{2}}{P}}\n, representing the probability of correspondence between each patch in the anchor view and query view. The resulting map\nC\np\nC_{p}\nexplicitly indicates which local patches in the query are structurally correlated with a specific patch in the anchor. This map acts as a spatial filter: it highlights relevant object parts while suppressing irrelevant regions. This step effectively narrows the search scope for the final matching phase, ensuring that feature interactions are spatially constrained to valid topological regions.\nFigure 5:\nTraining objectives and inference process. (a) The optimization objective comprises two components: a contrastive loss for feature matching and a classification loss for the patch correlation map. (b) During inference, high-similarity feature pairs are selected from fine-grained regions, and relative poses are computed using the Point DSC algorithm.\nIII-F\nDecoder\nTo recover pixel-perfect correspondences, the Decoder upsamples the coarse, spatially-constrained features back to the original resolution. The decoder of the FiCoP model shares the same architecture as that of the Oryon model. It consists of three upsampling layers, using\nM\nA\nM^{A}\nor\nM\nQ\nM^{Q}\nas guidance, to upsample the fully interactive multimodal features\nE\n~\nA\n\\widetilde{E}^{A}\nor\nE\n~\nQ\n\\widetilde{E}^{Q}\nto the original image resolution, obtaining\nE\n^\nA\n\\widehat{E}^{A}\nor\nE\n^\nQ\n\\widehat{E}^{Q}\nfor subsequent dense feature matching.\nIII-G\nOptimization and Inference Process\nAs shown in Fig.\n5\n(a), FiCoP incorporates two optimization objectives: feature matching and patch correlation map. For the feature matching loss function, our goal is to maximize similarity between features at matching positions while minimizing similarity at non-matching positions between anchors and queries. We employ the same contrastive loss as Oryon\n[\n5\n]\n:\nâ„’\nF\n=\nL\nP\n+\nL\nN\n\\mathcal{L}_{F}=L_{P}+L_{N}\n. For the patch correlation map, we treat it as a classification problem involving positive and negative samples. The loss is computed using a binary cross-entropy loss function:\nâ„’\nC\n=\nâˆ’\n1\nN\nâ€‹\nâˆ‘\nn\n=\n1\nN\n(\nw\np\nâ‹…\nC\ng\nâ€‹\nt\nâ€‹\n(\nn\n)\nâ‹…\nlog\nâ¡\n(\nC\np\nâ€‹\n(\nn\n)\n)\n+\n(\n1\nâˆ’\nC\ng\nâ€‹\nt\nâ€‹\n(\nn\n)\n)\nâ‹…\nlog\nâ¡\n(\n1\nâˆ’\nC\np\nâ€‹\n(\nn\n)\n)\n)\n\\mathcal{L}_{C}=-\\frac{1}{N}\\sum_{n=1}^{N}(w_{p}\\cdot C_{gt}(n)\\cdot\\log(C_{p}(n))+(1-C_{gt}(n))\\cdot\\log(1-C_{p}(n)))\n, where\nN\n=\nN\np\nâ‹…\nG\nâ€‹\n1\nâ‹…\nG\nâ€‹\n2\nN=N_{p}\\cdot G1\\cdot G2\n,\nw\np\n=\nN\nn\nâ€‹\ne\nâ€‹\ng\n/\nN\np\nâ€‹\no\nâ€‹\ns\nw_{p}=N_{neg}/{N_{pos}}\nis the positive sample weights. The final total loss function is defined as:\nâ„’\n=\nÎ»\n1\nâ€‹\nâ„’\nF\n+\nÎ»\n2\nâ€‹\nâ„’\nC\n\\mathcal{L}=\\lambda_{1}\\mathcal{L}_{F}+\\lambda_{2}\\mathcal{L}_{C}\n.\nThe inference process is shown in Fig.\n5\n(b), which uses index\nn\nn\nto traverse the number of channels\nN\nâ€‹\np\nNp\nin\nC\np\nC_{p}\n, generating the mask\nM\n^\nn\nA\n\\widehat{M}^{A}_{n}\nfor the\nn\nn\n-th patch of the anchor.\nFor the query, we binarize\nC\np\nâ€‹\n(\nn\n)\nC_{p}(n)\nto generate the query patch mask\nM\n^\nn\nQ\n\\widehat{M}^{Q}_{n}\n.\nThe binarization method sets patches with values greater than\nÏ„\n\\tau\nto 1 and the rest to 0. Apply\nM\nA\nM^{A}\nand\nM\n^\nn\nA\n\\widehat{M}^{A}_{n}\nto the feature\nE\n^\nA\n\\widehat{E}^{A}\nto filter out the valid anchor feature set\nF\nA\nF^{A}\n. Similarly, obtain the valid query feature set\nF\nQ\nF^{Q}\n. Calculate the cosine similarity between\nF\nA\nF^{A}\nand\nF\nQ\nF^{Q}\n, adding feature pairs with similarity greater than\nd\nt\nâ€‹\nh\nd_{th}\nto the matching set. Finally, compute the relative pose\nğ“\nA\nâ†’\nQ\n\\mathbf{T}_{A\\to Q}\nof the matching set using the PointDSC algorithm\n[\n1\n]\n.\nIV\nExperimental Results\nIV-A\nImplementation Details\nOur model was trained on an NVIDIA RTX A6000 GPU with the batch size set to\n32\n32\n, for a total of\n20\n20\nepochs.\nWe adopt the Adam optimizer with an initial learning rate of\n0.001\n0.001\nto train our model.\nA cosine annealing scheduler is employed to gradually decay the learning rate throughout training, which helps stabilize convergence and prevent premature overfitting.\nIV-B\nDatasets and Metrics\nIV-B\n1\nDatasets\nOur model is trained on the synthetic ShapeNet6D dataset\n[\n9\n]\n, where objects are paired with text descriptions to enable open-vocabulary learning. For evaluation, we strictly adhere to the zero-shot setting, ensuring that objects in the test sets are unseen during training.\nWe employ two real-world datasets to assess robustness: REAL275\n[\n27\n]\nand Toyota-Light\n[\n11\n]\n.\nREAL275 is a benchmark constructed from complex indoor environments. It challenges the model with significant background clutter, layout variations, and partial occlusions, serving as a rigorous test for cross-instance generalization. Toyota-Light is a dataset specifically designed to evaluate environmental consistency. It features objects captured under extreme lighting variations, testing the modelâ€™s capability to maintain performance across drastic illumination changes.\nTABLE I:\nThe comparison results of our method with other methods on the REAL275 and Toyota-Light datasets. The best results are in\nbold\n.\nMethod\nREAL275\nToyota-Light\nAR\nADD\nmIoU\nAR\nADD\nmIoU\nPoseDiffusion\n[\n28\n]\n9.5\n0.8\n-\n8.1\n1.6\n-\nRelPose++\n[\n17\n]\n23.1\n12.8\n-\n30.5\n11.6\n-\nObjectMatch\n[\n7\n]\n21.0\n11.0\n81.3\n8.2\n4.3\n82.1\nSIFT\n[\n21\n]\n33.5\n18.1\n81.3\n29.9\n14.6\n82.1\nLatentFusion\n[\n24\n]\n19.8\n8.2\n81.3\n26.0\n10.3\n82.1\nOryon\n[\n5\n]\n32.2\n24.3\n66.5\n30.3\n20.9\n68.1\nHoryon\n[\n4\n]\n57.9\n51.6\n81.3\n33.0\n25.1\n82.1\nOurs\n65.9\n55.2\n88.8\n39.1\n25.6\n89.5\nIV-B\n2\nEvaluation Metrics\nIn this study, two core evaluation metrics in the 6D pose estimation field, namely Average Recall (AR) and Average Point Distance (ADD), are mainly used to evaluate the accuracy of predicted poses.\nAR is a comprehensive evaluation system that integrates three complementary metrics: Visible Surface Discrepancy (VSD), Maximum Symmetry-aware Surface Distance (MSSD), and Maximum Symmetry-aware Projection Distance (MSPD), and calculates their average as the final metric.\nADD is a widely used intuitive geometric error metric that calculates the average distance of a three-dimensional model point set of an object under the transformation between the true pose and the estimated pose, and uses 10% of the object diameter as the threshold to determine the correctness of the estimation, thus quantitatively reflecting the absolute accuracy of pose estimation.\nIn addition, we report the mean Intersection over Union (mIoU) metric between the predicted masks and the ground truth masks.\nTABLE II:\nThe ablation experimental results of our method on the REAL275 and Toyota-Light datasets. The best results are in\nbold\n.\nModel variants\nREAL275\nToyota-Light\nAR\nVSD\nMSSD\nMSPD\nADD\nmIoU\nAR\nVSD\nMSSD\nMSPD\nADD\nmIoU\nBaseline (FiCoP)\n65.9\n48.2\n72.7\n77.8\n55.2\n88.8\n39.1\n13.3\n51.0\n53.0\n25.6\n89.5\nw/o PCP\n62.0\n43.2\n69.7\n73.2\n46.5\n88.8\n36.8\n12.0\n47.7\n50.8\n20.5\n89.5\nw/o CPGP\n60.9\n43.1\n67.4\n72.1\n48.7\n88.8\n37.1\n12.3\n48.4\n50.7\n20.5\n89.5\nw/o SAM\n63.6\n46.0\n70.3\n74.5\n52.4\n88.1\n38.0\n12.4\n49.6\n52.0\n20.8\n89.3\nw/o Preprocessing\n34.1\n27.2\n37.6\n37.5\n29.1\n68.4\n33.0\n12.3\n42.6\n44.2\n19.8\n80.4\nIV-C\nQuantitative Results\nWe select seven methods PoseDiffusion\n[\n28\n]\n, RelPose++\n[\n17\n]\n, ObjectMatch\n[\n7\n]\n, SIFT\n[\n21\n]\n, LatentFusion\n[\n24\n]\n, Oryon\n[\n5\n]\nand Horyon\n[\n4\n]\nas baseline comparisons. Oryon does not employ cropping preprocessing and utilizes its own predicted object masks. All other methods utilize the open-vocabulary object detection model GroundingDINO for cropping preprocessing. PoseDiffusion and RelPose++ are sparse-view methods that rely solely on RGB data. ObjectMatch, SIFT, and LatentFusion utilize object masks predicted by Horyon.\nThe data reveals that PoseDiffusion performs poorly on both datasets. ObjectMatch shows slightly better results on REAL275 but performs comparably on Toyota-Light. RelPose++, SIFT, and LatentFusion achieved comparable performance to Oryon on both datasets by employing cropping preprocessing to eliminate most background interference. The novel method Horyon significantly outperforms the aforementioned approaches, demonstrating exceptional competitiveness. Our method achieved the highest score, surpassing all selected baseline methods, thereby proving that our approach has attained state-of-the-art performance in the field of open-vocabulary 6D pose estimation.\nFigure 6:\nAblation study on the impact of patch granularity on the REAL275 and Toyota-Light datasets. The bar chart reports the Average Recall (AR) and Average Point Distance (ADD) metrics.\nIV-D\nAblation Studies\nIV-D\n1\nComponent Analysis\nWe verify the effectiveness of our proposed architecture by systematically removing key modules, as detailed in Table\nII\n. First, removing the Patch Correlation Predictor (PCP) leads to a significant performance decline. This confirms that without the patch-level spatial prior to narrow the matching scope, the model becomes susceptible to feature ambiguity, failing to distinguish target parts from similar local patterns. Second, ablating the Cross-Perspective Global Perception (CPGP) module results in comparable degradation, demonstrating that the explicit reasoning provided by this module is essential for establishing structural consensus under large viewpoint changes. Finally, the most drastic drop occurs when removing the Object-Centric Disentanglement Preprocessing. This validates our fundamental insight: topologically isolating the target from background clutter is a prerequisite. Without this noise-free input, the global matching space is overwhelmed by environmental distractors, rendering robust pose estimation impossible.\nIV-D\n2\nImpact of Patch Granularity\nTo investigate the influence of the spatial priorâ€™s density, we conduct ablation studies on the grid size of patches. In practice, we use square patches with grid size represented by\nG\nG\n(\ni.e.\n,\nG\n=\nG\nâ€‹\n1\n=\nG\nâ€‹\n2\nG=G1=G2\n). The grid size determines the granularity of the spatial constraints: a smaller\nG\nG\nimplies coarser regions with more context but looser constraints, while a larger\nG\nG\nenforces tighter spatial filtering but reduces the semantic context within each patch.\nAs shown in Fig.\n6\n, we evaluate the model performance with\nG\nâˆˆ\n{\n4\n,\n6\n,\n8\n,\n12\n}\nG\\in\\{4,6,8,12\\}\non both the REAL275 and Toyota-Light datasets. The model exhibits the lowest performance across all metrics when\nG\n=\n4\nG=4\n. This suggests that when the patch size is too large, the â€œspatial priorâ€ becomes too loose to effectively filter out background clutter. The coarse regions likely contain a mixture of object and background features, leading to ambiguity during the matching process.\nIncreasing the granularity yields significant performance gains.\nThis confirms our hypothesis that a finer-grained correlation matrix provides a more precise search zone, effectively suppressing background distractors and enforcing structural consistency. Interestingly, further increasing the granularity to\nG\n=\n12\nG=12\ndoes not guarantee continuous improvement. We attribute this to the fact that excessively small patches may lack sufficient local semantic context to be uniquely distinctive, potentially introducing noise into the correlation matrix.\nWhile Toyota-Light shows a slight continued increase at\nG\n=\n12\nG=12\n, considering the balance between performance stability across datasets and computational efficiency, we adopt\nG\n=\n8\nG=8\nas the optimal setting for our final model.\nTABLE III:\nOptimization results for the binary threshold\nÏ„\n\\tau\n. The best results are in\nbold\n.\nHyperparameter setting\nREAL275\nToyota-Light\nAR\nADD\nAR\nADD\nÏ„\n=\n0.01\n\\tau=0.01\n66.1\n55.1\n38.2\n22.1\nÏ„\n=\n0.02\n\\tau=0.02\n66.4\n55.6\n38.6\n23.8\nÏ„\n=\n0.03\n\\tau=0.03\n65.9\n55.2\n39.1\n25.1\nÏ„\n=\n0.04\n\\tau=0.04\n65.9\n55.2\n39.1\n25.6\nÏ„\n=\n0.05\n\\tau=0.05\n65.7\n54.9\n39.0\n24.8\nFigure 7:\nQualitative comparison of predicted poses on the REAL275\n[\n27\n]\nand Toyota-Light\n[\n11\n]\nbenchmarks. The figure shows the poses predicted by SIFT\n[\n21\n]\n, Oryon\n[\n5\n]\n, and our method. The 3D spatial coordinates of the objects are converted to RGB colors.\nFigure 8:\nVisualization of patch correlation maps. The first column shows a patch from the anchor. The second column displays the relevant patches predicted by the model in the query. The third column presents the ground truth relevant patches.\nFigure 9:\nVisualization of real-world open-vocabulary 6D pose estimation. Left: The experimental setup featuring a robotic arm equipped with an eye-in-hand RGBD camera in a cluttered tabletop environment. Right: Qualitative results on unseen objects specified by text prompts. The coordinate systems and yellow 3D bounding boxes represent the estimated poses.\nIV-D\n3\nSensitivity Analysis of Correlation Threshold\nThe binarization threshold\nÏ„\n\\tau\nacts as a spatial filter to determine valid patch correspondences. As shown in Table\nIII\n, varying\nÏ„\n\\tau\nreveals a trade-off between recall and noise suppression: a low threshold (\nÏ„\n=\n0.01\n\\tau=0.01\n) admits excessive background clutter, while an aggressive threshold (\nÏ„\n=\n0.05\n\\tau=0.05\n) risks discarding valid object features. Performance remains robust within the range of\n[\n0.02\n,\n0.04\n]\n[0.02,0.04]\n, peaking at\nÏ„\n=\n0.04\n\\tau=0.04\nfor Toyota-Light. This confirms that our module effectively learns distinct representations, allowing a simple threshold to reliably separate the object from distractors. We adopt\nÏ„\n=\n0.04\n\\tau=0.04\nas the default setting, as it strikes the optimal balance by effectively filtering background noise.\nIV-E\nQualitative Results\nIV-E\n1\nVisualization of Predicted Pose Results\nFor a fair comparison, two open-source methods SIFT\n[\n21\n]\nand Oryon\n[\n5\n]\nare evaluated.\nWe run them alongside our method on the same hardware platform to estimate object poses in open-world scenarios using text prompts.\nVisualizing the predicted pose results from all three methods yields the outcome shown in Fig.\n7\n.\nIt can be observed that the objects in the four scenes exhibit significant perspective differences between the anchor and query images, presenting considerable difficulty. Consequently, SIFT and Oryon struggle to predict accurate object poses in these scenarios.\nIn contrast, our method accurately predicts object poses even in these challenging scenes thanks to the carefully designed fine-grained correspondence strategy.\nIV-E\n2\nVisualization of Patch Correlation Maps\nTo explore the role played by patch correlation maps, we visualized the masks generated by their binarization in Fig.\n8\n, overlaid as heatmaps on the RGB image. It can be observed that in the four demonstrated scenarios, despite significant perspective differences between anchors and queries, the patch correlation maps accurately align key regions between anchors and queries, achieving fine-grained correspondence to eliminate interference.\nIV-E\n3\nVisualization in Real-World Scenarios\nTo further verify the generalization capability of FiCoP in physical environments, we have conducted real-world experiments using a robotic manipulator equipped with an eye-in-hand RGBD camera.\nAs shown in Fig.\n9\n, the setup involves a cluttered tabletop scene containing various everyday objects that were not seen during training.\nThe system is tasked with estimating the 6D pose of target objects described solely by text prompts.\nCrucially, the successful pose estimation of these diverse items verifies that FiCoP possesses genuine open-vocabulary capabilities, enabling it to perceive arbitrary real-world objects driven strictly by natural language without specific fine-tuning.\nThe experimental results highlight two key strengths of our method.\nFirst, it exhibits exceptional robustness to large perspective variations.\nAs observed in the â€œscrewdriverâ€ and â€œspray bottleâ€ cases, the anchor images are taken from a frontal or side view, while the query images captured by the robot are from a steep top-down angle.\nDespite this significant geometric deformation, FiCoP accurately aligns the 3D bounding boxes.\nSecond, the method effectively handles semantic ambiguity in cluttered scenes.\nEven with interference from attribute-similar (\ne.g.\n, color) objects, our fine-grained correspondence mechanism successfully isolates clutter interference, ensuring precise pose estimation for robotic interaction.\nThis robustness confirms that our method effectively supports open-vocabulary pose estimation and proves its suitability for practical deployment in complex, unconstrained real-world scenarios.\nV\nConclusion and Future Work\nThis work addresses the ambiguity inherent in unconstrained global matching for open-vocabulary 6D pose estimation.\nWe have proposed FiCoP, a framework that transitions from noise-prone global search to spatially-constrained fine-grained correspondence learning.\nBy integrating object-centric disentanglement and a novel patch correlation predictor (PCP) with a Cross-Perspective Perception (CPGP), our approach leverages a structural prior to filter environmental distractors, ensuring that feature matching is confined to topologically valid regions.\nThis coarse-to-fine mechanism effectively bridges the gap between VLM-based semantic understanding and precise geometric alignment.\nExperiments on the REAL275 and Toyota-Light datasets demonstrate that FiCoP significantly outperforms state-of-the-art baselines, offering superior robustness against large viewpoint variations and background clutter.\nThese findings confirm that explicit spatial constraints are essential for generalized perception, paving the way for reliable robotic manipulation in complex, open-world environments.\nIn the future, we aim to investigate 3D-aware vision-language foundation models to further bridge the semantic-geometric granularity gap intrinsic to 2D pre-training.\nAdditionally, we plan to extend FiCoP into an active perception framework, enabling robots to autonomously adjust viewpoints to minimize matching ambiguity in highly occluded and unconstrained environments.\nReferences\n[1]\nX. Bai\net al.\n(2021)\nPointDSC: Robust point cloud registration using deep spatial consistency\n.\nIn\nProc. CVPR\n,\npp.Â 15859â€“15869\n.\nCited by:\nÂ§\nIII-G\n.\n[2]\nJ. Cai\net al.\n(2024)\nOpen-vocabulary category-level object pose and size estimation\n.\nIEEE Robotics and Automation Letters\n9\n(\n9\n),\npp.Â 7661â€“7668\n.\nCited by:\nÂ§\nII-B\n.\n[3]\nK. Chen and Q. Dou\n(2021)\nSGPA: Structure-guided prior adaptation for category-level 6D object pose estimation\n.\nIn\nProc. ICCV\n,\npp.Â 2753â€“2762\n.\nCited by:\nÂ§\nII-A\n.\n[4]\nJ. Corsetti, D. Boscaini, F. Giuliari, C. Oh, A. Cavallaro, and F. Poiesi\n(2026)\nHigh-resolution open-vocabulary object 6D pose estimation\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n48\n(\n2\n),\npp.Â 2066â€“2077\n.\nCited by:\nFigure 1\n,\nÂ§I\n,\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nIV-C\n,\nTABLE I\n.\n[5]\nJ. Corsetti, D. Boscaini, C. Oh, A. Cavallaro, and F. Poiesi\n(2024)\nOpen-vocabulary object 6D pose estimation\n.\nIn\nProc. CVPR\n,\npp.Â 18071â€“18080\n.\nCited by:\nFigure 1\n,\nÂ§I\n,\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nIII-C\n,\nÂ§\nIII-G\n,\nFigure 7\n,\nÂ§\nIV-C\n,\nÂ§\nIV-E\n1\n,\nTABLE I\n.\n[6]\nW. Deng\net al.\n(2025)\nPos3R: 6D pose estimation for unseen objects made easy\n.\nIn\nProc. CVPR\n,\npp.Â 16818â€“16828\n.\nCited by:\nÂ§I\n.\n[7]\nC. GÃ¼meli, A. Dai, and M. NieÃŸner\n(2023)\nObjectMatch: Robust registration using canonical object correspondences\n.\nIn\nProc. CVPR\n,\npp.Â 13082â€“13091\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nIV-C\n,\nTABLE I\n.\n[8]\nY. Hai, R. Song, J. Li, and Y. Hu\n(2023)\nShape-constraint recurrent flow for 6D object pose estimation\n.\nIn\nProc. CVPR\n,\npp.Â 4831â€“4840\n.\nCited by:\nÂ§\nII-A\n,\nÂ§\nIII-A\n.\n[9]\nY. He, Y. Wang, H. Fan, J. Sun, and Q. Chen\n(2022)\nFS6D: Few-shot 6D pose estimation of novel objects\n.\nIn\nProc. CVPR\n,\npp.Â 6804â€“6814\n.\nCited by:\nÂ§\nIV-B\n1\n.\n[10]\nS. Hochstein and M. Ahissar\n(2002)\nView from the top: hierarchies and reverse hierarchies in the visual system\n.\nNeuron\n36\n(\n5\n),\npp.Â 791â€“804\n.\nCited by:\nÂ§I\n.\n[11]\nT. Hodan\net al.\n(2018)\nBOP: Benchmark for 6D object pose estimation\n.\nIn\nProc. ECCV\n,\nVol.\n11214\n,\npp.Â 19â€“35\n.\nCited by:\nÂ§I\n,\nFigure 7\n,\nÂ§\nIV-B\n1\n.\n[12]\nP. Hou, Y. Zhang, W. Zhou, B. Ye, and Y. Wu\n(2025)\nA lightweight network for category-level open-vocabulary object pose estimation with enhanced cross implicit space transformation\n.\nEngineering Applications of Artificial Intelligence\n155\n,\npp.Â 110890\n.\nExternal Links:\nISSN 0952â€“1976\nCited by:\nÂ§\nII-B\n.\n[13]\nX. Jiang, D. Li, H. Chen, Y. Zheng, R. Zhao, and L. Wu\n(2022)\nUni6D: A unified CNN framework without projection breakdown for 6D pose estimation\n.\nIn\nProc. CVPR\n,\npp.Â 11164â€“11174\n.\nCited by:\nÂ§\nII-A\n.\n[14]\nA. Kirillov\net al.\n(2023)\nSegment anything\n.\nIn\nProc. ICCV\n,\npp.Â 3992â€“4003\n.\nCited by:\nÂ§\nIII-B\n.\n[15]\nY. LabbÃ©\net al.\n(2022)\nMegaPose: 6D pose estimation of novel objects via render & compare\n.\nIn\nProc. CoRL\n,\nVol.\n205\n,\npp.Â 715â€“725\n.\nCited by:\nÂ§\nII-A\n,\nÂ§\nIII-A\n.\n[16]\nT. Lee\net al.\n(2023)\nTTA-COPE: Test-time adaptation for category-level object pose estimation\n.\nIn\nProc. CVPR\n,\npp.Â 21285â€“21295\n.\nCited by:\nÂ§I\n.\n[17]\nA. Lin, J. Y. Zhang, D. Ramanan, and S. Tulsiani\n(2024)\nRelPose++: Recovering 6D poses from sparse-view observations\n.\nIn\nProc. 3DV\n,\npp.Â 106â€“115\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nIV-C\n,\nTABLE I\n.\n[18]\nJ. Lin, Z. Wei, Z. Li, S. Xu, K. Jia, and Y. Li\n(2021)\nDualPoseNet: Category-level 6D object pose and size estimation using dual pose network with refined learning of pose consistency\n.\nIn\nProc. ICCV\n,\npp.Â 3540â€“3549\n.\nCited by:\nÂ§\nII-A\n.\n[19]\nS. Liu\net al.\n(2024)\nGrounding DINO: Marrying DINO with grounded pre-training for open-set object detection\n.\nIn\nProc. ECCV\n,\nVol.\n15105\n,\npp.Â 38â€“55\n.\nCited by:\nÂ§\nIII-B\n.\n[20]\nY. Liu\net al.\n(2022)\nGen6D: Generalizable model-free 6-DoF object pose estimation from RGB images\n.\nIn\nProc. ECCV\n,\nVol.\n13692\n,\npp.Â 298â€“315\n.\nCited by:\nÂ§\nII-A\n,\nÂ§\nIII-A\n.\n[21]\nD.G. Lowe\n(1999)\nObject recognition from local scale-invariant features\n.\nIn\nProc. ICCV\n,\npp.Â 1150â€“1157\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nFigure 7\n,\nÂ§\nIV-C\n,\nÂ§\nIV-E\n1\n,\nTABLE I\n.\n[22]\nS. Moon, H. Son, D. Hur, and S. Kim\n(2025)\nCo-op: Correspondence-based novel object pose estimation\n.\nIn\nProc. CVPR\n,\npp.Â 11622â€“11632\n.\nCited by:\nÂ§I\n.\n[23]\nM. Oquab\net al.\n(2024)\nDINOv2: Learning robust visual features without supervision\n.\nTransactions on Machine Learning Research Journal\n2024\n.\nCited by:\nÂ§\nIII-C\n.\n[24]\nK. Park, A. Mousavian, Y. Xiang, and D. Fox\n(2020)\nLatentFusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation\n.\nIn\nProc. CVPR\n,\npp.Â 10707â€“10716\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nIV-C\n,\nTABLE I\n.\n[25]\nA. Radford\net al.\n(2021)\nLearning transferable visual models from natural language supervision\n.\nIn\nProc. ICML\n,\nVol.\n139\n,\npp.Â 8748â€“8763\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nIII-C\n.\n[26]\nC. Wang\net al.\n(2019)\nDenseFusion: 6D object pose estimation by iterative dense fusion\n.\nIn\nProc. CVPR\n,\npp.Â 3343â€“3352\n.\nCited by:\nÂ§\nII-A\n.\n[27]\nH. Wang, S. Sridhar, J. Huang, J. Valentin, S. Song, and L. J. Guibas\n(2019)\nNormalized object coordinate space for category-level 6D object pose and size estimation\n.\nIn\nProc. CVPR\n,\npp.Â 2642â€“2651\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n,\nFigure 7\n,\nÂ§\nIV-B\n1\n.\n[28]\nJ. Wang, C. Rupprecht, and D. Novotny\n(2023)\nPoseDiffusion: Solving pose estimation via diffusion-aided bundle adjustment\n.\nIn\nProc. ICCV\n,\npp.Â 9739â€“9749\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nIV-C\n,\nTABLE I\n.\n[29]\nY. Wu, M. Zand, A. Etemad, and M. Greenspan\n(2022)\nVote from the center: 6 DoF pose estimation in RGB-D images by radial keypoint voting\n.\nIn\nProc. ECCV\n,\nVol.\n13670\n,\npp.Â 335â€“352\n.\nCited by:\nÂ§\nII-A\n,\nÂ§\nIII-A\n.\n[30]\nX. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer\n(2023)\nSigmoid loss for language image pre-training\n.\nIn\nProc. ICCV\n,\npp.Â 11941â€“11952\n.\nCited by:\nÂ§I\n.",
    "preview_text": "Open-vocabulary 6D object pose estimation empowers robots to manipulate arbitrary unseen objects guided solely by natural language. However, a critical limitation of existing approaches is their reliance on unconstrained global matching strategies. In open-world scenarios, trying to match anchor features against the entire query image space introduces excessive ambiguity, as target features are easily confused with background distractors. To resolve this, we propose Fine-grained Correspondence Pose Estimation (FiCoP), a framework that transitions from noise-prone global matching to spatially-constrained patch-level correspondence. Our core innovation lies in leveraging a patch-to-patch correlation matrix as a structural prior to narrowing the matching scope, effectively filtering out irrelevant clutter to prevent it from degrading pose estimation. Firstly, we introduce an object-centric disentanglement preprocessing to isolate the semantic target from environmental noise. Secondly, a Cross-Perspective Global Perception (CPGP) module is proposed to fuse dual-view features, establishing structural consensus through explicit context reasoning. Finally, we design a Patch Correlation Predictor (PCP) that generates a precise block-wise association map, acting as a spatial filter to enforce fine-grained, noise-resilient matching. Experiments on the REAL275 and Toyota-Light datasets demonstrate that FiCoP improves Average Recall by 8.0% and 6.1%, respectively, compared to the state-of-the-art method, highlighting its capability to deliver robust and generalized perception for robotic agents operating in complex, unconstrained open-world environments. The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP.\n\nLearning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation\nYu Qin\n1\n, Shimeng Fan\n2\n, Fan Yang\n1\n, Zixuan Xue\n1\n, Zijie Mai\n1\n, Wenrui Chen\n1,3\n,\nKailun Yang\n1,3,âˆ—\n, and Zhiyong Li\n1,3",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "6D object pose estimation",
        "open-vocabulary",
        "fine-grained correspondence",
        "cross-perspective perception",
        "patch-level matching",
        "robotic manipulation"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºFiCoPæ¡†æ¶ï¼Œé€šè¿‡ç»†ç²’åº¦è¡¥ä¸çº§å¯¹åº”å’Œè·¨è§†è§’æ„ŸçŸ¥ï¼Œæ”¹è¿›å¼€æ”¾è¯æ±‡6Dç‰©ä½“å§¿æ€ä¼°è®¡ï¼Œä»¥å¢å¼ºæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T03:48:54Z",
    "created_at": "2026-01-27T15:53:07.091844",
    "updated_at": "2026-01-27T15:53:07.091851"
}