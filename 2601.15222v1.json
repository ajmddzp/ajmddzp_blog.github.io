{
    "id": "2601.15222v1",
    "title": "MonoRace: Winning Champion-Level Drone Racing with Robust Monocular AI",
    "authors": [
        "Stavrow A. Bahnam",
        "Robin Ferede",
        "Till M. Blaha",
        "Anton E. Lang",
        "Erin Lucassen",
        "Quentin Missinne",
        "Aderik E. C. Verraest",
        "Christophe De Wagter",
        "Guido C. H. E. de Croon"
    ],
    "abstract": "è‡ªä¸»æ— äººæœºç«é€Ÿæ˜¯æœºå™¨äººç ”ç©¶é¢†åŸŸçš„é‡è¦å‰æ²¿ã€‚å®ƒè¦æ±‚äººå·¥æ™ºèƒ½èƒ½å¤Ÿåœ¨èµ„æºä¸æ—¶é—´ä¸¥æ ¼å—é™çš„æ¡ä»¶ä¸‹ï¼Œåœ¨è½»é‡çº§é£è¡Œæœºå™¨äººä¸Šå®æ—¶è¿è¡Œï¼ŒåŒæ—¶å°†ç‰©ç†ç³»ç»Ÿæ€§èƒ½æ¨å‘æé™ã€‚è¯¥é¢†åŸŸç°æœ‰æœ€å…ˆè¿›ç³»ç»Ÿé‡‡ç”¨ç«‹ä½“ç›¸æœºä¸æƒ¯æ€§æµ‹é‡å•å…ƒç»„åˆï¼Œæ›¾åœ¨å—æ§å®¤å†…ç¯å¢ƒä¸­å‡»è´¥äººç±»æ— äººæœºç«é€Ÿå† å†›ã€‚æœ¬æ–‡æå‡ºMonoRaceç³»ç»Ÿï¼šä¸€ç§ä»…ä½¿ç”¨å•ç›®å·å¸˜å¿«é—¨ç›¸æœºä¸æƒ¯æ€§æµ‹é‡å•å…ƒçš„æœºè½½ç«é€Ÿæ–¹æ¡ˆï¼Œå¯åœ¨æ— å¤–éƒ¨è¿åŠ¨è¿½è¸ªç³»ç»Ÿçš„ç«èµ›ç¯å¢ƒä¸­å®ç°æ³›åŒ–åº”ç”¨ã€‚è¯¥æ–¹æ¡ˆå…·å¤‡é²æ£’çš„çŠ¶æ€ä¼°è®¡èƒ½åŠ›ï¼Œå°†åŸºäºç¥ç»ç½‘ç»œçš„éšœç¢é—¨åˆ†å‰²æŠ€æœ¯ä¸æ— äººæœºåŠ¨åŠ›å­¦æ¨¡å‹ç›¸ç»“åˆï¼Œå¹¶åŒ…å«ç¦»çº¿ä¼˜åŒ–æµç¨‹â€”â€”åˆ©ç”¨å·²çŸ¥éšœç¢é—¨å‡ ä½•ç»“æ„ä¼˜åŒ–æ‰€æœ‰çŠ¶æ€ä¼°è®¡å‚æ•°ã€‚è¯¥ç¦»çº¿ä¼˜åŒ–å®Œå…¨åŸºäºæœºè½½é£è¡Œæ•°æ®ï¼Œå¯¹ç²¾ç»†è°ƒæ•´å…³é”®çš„å¤–éƒ¨ç›¸æœºæ ‡å®šå‚æ•°è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œå…¶å¯¼å¼•æ§åˆ¶ç³»ç»Ÿé‡‡ç”¨ç›´æ¥å‘é€ç”µæœºæŒ‡ä»¤çš„ç¥ç»ç½‘ç»œï¼Œæ‘’å¼ƒäº†ä¼ ç»Ÿå†…ç¯æ§åˆ¶å™¨ã€‚è¿™ä¸ªè½»é‡åŒ–ç½‘ç»œåœ¨é£è¡Œæ§åˆ¶å™¨ä¸Šä»¥500èµ«å…¹é¢‘ç‡è¿è¡Œã€‚æ‰€æå‡ºçš„æ–¹æ¡ˆåœ¨2025å¹´é˜¿å¸ƒæ‰æ¯”è‡ªä¸»æ— äººæœºç«é€Ÿé”¦æ ‡èµ›ä¸­å¤ºå† ï¼Œåœ¨ç›´æ¥æ·˜æ±°èµ›ä¸­æˆ˜èƒœæ‰€æœ‰å‚èµ›äººå·¥æ™ºèƒ½å›¢é˜ŸåŠä¸‰ä½äººç±»ä¸–ç•Œå† å†›é£è¡Œå‘˜ã€‚è¿™æ ‡å¿—ç€è‡ªä¸»æ— äººæœºç«é€Ÿç ”ç©¶çš„æ–°é‡Œç¨‹ç¢‘ï¼šç³»ç»Ÿåœ¨èµ›é“ä¸Šè¾¾åˆ°æ¯å°æ—¶100å…¬é‡Œçš„é€Ÿåº¦ï¼Œå¹¶æˆåŠŸåº”å¯¹ç›¸æœºå¹²æ‰°ä¸æƒ¯æ€§æµ‹é‡å•å…ƒé¥±å’Œç­‰é—®é¢˜ã€‚",
    "url": "https://arxiv.org/abs/2601.15222v1",
    "html_url": "https://arxiv.org/html/2601.15222v1",
    "html_content": "MonoRace: Winning Champion-Level Drone\nRacing with Robust Monocular AI\nStavrow A. Bahnam\nâ€ \n,\nRobin Ferede\nâˆ—,â€ \n,\nTill M. Blaha,\nAnton E. Lang\nErin Lucassen,\nQuentin Missinne,\nAderik E.C. Verraest\nChristophe De Wagter,\nGuido C.H.E. de Croon\nControl & Operation, Delft University of Technology, Delft & 2629 HS, The Netherlands.\nâˆ—\nCorresponding author. Email: R.Ferede@tudelft.nl\nâ€ \nThese authors contributed equally to this work.\nAbstract\nAutonomous drone racing represents a major frontier in robotics research. It requires an Artificial Intelligence (AI) that can run on board light-weight flying robots under tight resource and time constraints, while pushing the physical system to its limits. The state of the art in this area consists of a system with a stereo camera and an inertial measurement unit (IMU) that beat human drone racing champions in a controlled indoor environment. Here, we present MonoRace: an onboard drone racing approach that uses a monocular, rolling-shutter camera and IMU that generalizes to a competition environment without any external motion tracking system. The approach features robust state estimation that combines neural-network-based gate segmentation with a drone model. Moreover, it includes an offline optimization procedure that leverages the known geometry of gates to refine any state estimation parameter. This offline optimization is based purely on onboard flight data and is important for fine-tuning the vital external camera calibration parameters. Furthermore, the guidance and control are performed by a neural network that foregoes inner loop controllers by directly sending motor commands. This small network runs on the flight controller at 500 Hz. The proposed approach won the 2025 Abu Dhabi Autonomous Drone Racing Competition (A2RL), outperforming all competing AI teams and three human world champion pilots in a direct knockout tournament. It set a new milestone in autonomous drone racing research, reaching speeds up to 100 km/h on the competition track and successfully coping with problems such as camera interference and IMU saturation.\nINTRODUCTION\nAutonomous drone racing (ADR) is a proving ground for high-speed perception, state estimation, planning, and control  (\n?\n,\n?\n). The high speeds attained in drone racing represent a major challenge for the algorithms empowering autonomous flight. At such speeds, complex aerodynamic effects become increasingly difficult to model  (\n?\n), undermining both conventional control approaches and the simulation environments used for learning-based methods. In addition, computer vision in autonomous drone racing is considerably hampered by the large inter-frame displacements of imaged objects, resulting in high optical flow and motion blur. Furthermore, autonomous drone racing rewards pushing the physical drone system to its limits, requiring close-to-optimal control while being constrained by the requirement of fast, onboard processing.\nIn recent years, autonomous drone racing has seen rapid advances in control, perception, and state estimation. Early control approaches relied on differential-flatness-based trajectory tracking  (\n?\n,\n?\n,\n?\n,\n?\n), followed by nonlinear model predictive control (NMPC) for faster and more aggressive flight  (\n?\n,\n?\n,\n?\n,\n?\n,\n?\n,\n?\n,\n?\n,\n?\n). Today, neural network controllers trained with deep reinforcement learning represent the state of the art  (\n?\n,\n?\n,\n?\n,\n?\n,\n?\n), increasingly replacing modular trajectory-tracking pipelines. Recent work further replaces even low-level controllers with neural networks  (\n?\n,\n?\n,\n?\n,\n?\n). On the perception side, the best results to date are obtained by stereo Visual Inertial Odometry (VIO) with geometric gate detection, fused through EKF-based state estimation  (\n?,\n?\n). Although learned monocular VIO  (\n?\n,\n?\n,\n?\n), visual-model-predictive localization  (\n?\n,\n?\n) and end-to-end vision-based policies  (\n?\n,\n?\n,\n?\n,\n?\n) are emerging as promising alternatives, they have not yet matched this state-of-the-art performance.\nThese advancements in autonomous drone racing have been stimulated by international competitions. Initially, these were purely an academic affair and organized alongside the IEEE IROS conferences. The first series of ADR competitions ran alongside IROS from 2016 to 2019: top speeds grew from 0.6 m/s (Daejeon, 2016)  (\n?\n) to 0.7 m/s (Vancouver, 2017)  (\n?\n), 2.0 m/s (Madrid, 2018), and Â 2.5 m/s (Macau, 2019). In 2019, Lockheed Martin and the Drone Racing League raised the bar with the AlphaPilot/AIRR seriesâ€”a four-event competition with standardized hardware and a $1Â M prize. TU Delftâ€™s MAVLab won the championship with an average velocity of 6.8 m/s and a peak of 9.2 m/s  (\n?\n), an order-of-magnitude jump over the early IROS speeds, while UZHâ€™s runner-up reached up to 8 m/s on the same platform  (\n?\n). Even so, human FPV drone racing pilots remained faster on comparable tracks.\nIn 2022, the â€˜Swiftâ€™ approach  (\n?\n) enabled an autonomous drone for the first time to beat champion-level pilots in head-to-head races. Perception and state estimation of Swift combined the Intel RealSense stereo Visual-Inertial Odometry (VIO), deep-neural-network-based gate detection on a front-looking camera, and a learned residual observation model trained with motion-capture supervision. Control was handled by a deep-RL policy that outputs the same low-level commands as human pilots (collective thrust and body rates). On an indoor track with peak speeds around 22 m/s  (\n?\n), Swift won multiple heats and set the fastest recorded time, making it the first champion-level performance by an autonomous system.\nHowever, several challenges remain on the way to real-world application of this technology  (\n?,\n?\n). Importantly, the AI for autonomous drone racing should not rely on external infrastructure such as a motion tracking system. Achieving independence from external infrastructure will reduce economic costs and substantially broaden the types of environments in which it can be applied. Moreover, it is desirable to further reduce the number of sensors used by the ADR solution, preferably only using a single forward-looking camera and onboard inertial sensors.\nThe outstanding challenges in autonomous drone racing have led to the organization of a new international competition. The Abu Dhabi Autonomous Racing League (A2RL) and Drone Champions League (DCL) organized a new drone racing competition with a $1 M price pool in April 2025. The goal of the competition was to push the state of the art by enforcing fully onboard perception with a single rolling-shutter CMOS camera and prohibiting any external aids at any stage. To this end, the organizers created the drone hardware accordingly and determined both the race track and the competition hall, which did not feature a motion tracking system. Nor did they allow any modification or the precision measurement of the track.\nThe competition was held on April 11 and 12, 2025, in Abu Dhabi. It comprised multiple events in which the autonomous racing drones competed with each other, with as main event being the â€œGrand Challengeâ€ that focused on a single-drone time trial. At the same dates and in the same space, human FPV drone racing pilots competed in the DCL â€œFalcon cupâ€. The best-ranked human FPV pilots were subsequently pitted against the best-ranked autonomous drones in a direct knockout tournament, starting with quarter finals (so in total three rounds till victory).\nIn this article, we present an onboard drone racing approach that only uses a single, rolling-shutter camera and an Inertial Measurement Unit (IMU). The approach won the above-mentioned Grand Challenge. Moreover, it enabled an autonomous drone to win for the first time an independently organized human vs. AI drone racing competition, sequentially beating three human FPV world champions in a direct knockout tournament. The approach includes a particularly robust state estimation pipeline that combines neural-network-based gate segmentation with a drone model. The robustness is illustrated by the fact that it can handle\n50\n%\n50\\%\ncorrupted images. Moreover, the state estimation can cope with the saturation of accelerometers, which regularly occurred in the high-\ng\ng\nmaneuvers during the very fast flight through the indoor racing track. Furthermore, we present an offline optimization procedure that leverages the known shapes and sizes of gates to refine any state estimation parameter. This offline optimization is based purely on onboard flight data and enables self-supervised post-flight fine-tuning of any vital parameters, like the external camera calibration.\nThe guidance and control in our system are executed by a single neural network, a Guidance-and-Control Network (G&CNet), which maps the estimated state directly to control actions. Originally developed for optimal real-time spacecraft landing  (\n?\n), this concept has recently been adapted to high-speed quadrotor flight  (\n?\n). The G&CNet is trained in simulation using reinforcement learning and directly outputs motor commands, fully replacing traditional inner-loop controllers. Until now, however, such nets relied on state estimates from highly accurate external motion-capture systems. In contrast, our approach uses only onboard vision-based state estimation. The resulting small network (only 3Ã—64 neurons) runs on the 32-bit flight controller at 500 Hz and is able to successfully cross the reality gap. The proposed drone racing approach set a new milestone in autonomous drone racing research, beating all other AI systems and three human world-champion FPV racers, while reaching speeds up to 100 km/h on the competition track.\nA\nB\nC\nD\nFigure 1\n:\n(\nA\n) Close-up of the supplied externally made competition robot with its 5.1-inch propellers, its drone-race autopilot board with normal IMU, a single CMOS rolling-shutter camera connected to a GPU-equipped NVIDIA ORIN NX companion computer, and 8 bright orange LED strips to provide good visibility for the spectators. The robot is looking at an â€˜A2RL x DCLâ€™ competition gate. (\nB\n) A time-lapse of the robot performing the split-S maneuver. (\nC\n) Overview of the entire track. The races consisted of two laps through eleven gates. Four robots are shown at the four start positions during a multi-robot race. (\nD\n) Results of the AI against human races.\nRESULTS\nCompetition Format\nThe â€˜A2RL x DCLâ€™ race was held in April 2025 in a 100 Ã— 30 m indoor hall and featured a track with 11 square gates, each with a 1.5 m inner opening. A photograph of the track can be seen in Fig.\n1\n, included two double-gates and a split-S maneuver, where the drone passed through an upper gate before diving directly through a lower gate beneath it in the opposite direction. The objective was to complete two laps in minimal time. The organizers defined the completion time as the interval from the first passage of Gate 1 to the second passage of Gate 11. No external localization system was provided nor allowed, so all state estimation, control, and any fine-tuning had to rely solely on onboard sensing.\nThe event comprised the following four challenges:\n1.\nGrand Challenge â€” 15 minutes individual trial for AI teams (fastest time counts).\n2.\nAI vs Human â€” three knockout rounds in a head-to-head race (AI teams and FPV pilots).\n3.\nMulti-Drone Race â€” single race with four AI drones flying simultaneously.\n4.\nDrag Race â€” an 83â€‰m straight track with four gates, with a choice between two middle gates.\nOur system won the Grand Challenge, AI vs Human, and Drag Race, and placed third in the Multi-Drone Race due to the absence of collision-avoidance capabilities, while more than doubling the speed of the slowest drones. This paper focuses on the first two events. MonoRace is specifically designed for these scenarios, emphasizing high-speed, agile autonomous flight with onboard perception.\nPerformance\nOur controller achieved the fastest completion time of 16.56 s, outperforming three world champion-level FPV pilots.\nThe official organizer-recorded times\n1\n1\n1\nThe times in Fig.\n1\nare provided by the race organizers and are measured using a VTx-based RF timing systemÂ  (\n?\n). All other times reported in this paper use our onboard state-estimationâ€“based timing, which explains the small numerical differences.\nfor the knockout rounds are shown in Fig.\n1\n. This result demonstrates that G&CNets with low-level motor control can exceed human performance in high-speed drone racing. During the competition, we trained a variety of neural network controllers, summarized in Table\n1\n. Each network was named after its fastest real-world completion time, rounded down to the nearest second and prefixed with an â€Mâ€ (e.g., M16 for a policy completing the track in approximately 16 s). The comparison of lap times across different G&CNets in Fig.\n2\nillustrates a clear trade-off between speed and robustness: more aggressive controllers consistently produced faster laps, whereas more conservative controllers prioritized safety and reliability. Remarkably, our most reliable network M23, completed 43 flights with an 88.4% success rate, where most mishaps were development or hardware-related.\nModel\nM16\nM17\nM18\nM19\nM22\nM23\nRetrained from\nM17\nX\nX\nX\nM23\nX\nInitialization\nfrom ground\nuniform\nuniform\nuniform\nuniform\nuniform\nÎ¸\ncam\n\\theta_{\\text{cam}}\n(deg)\n50\n43\n43\n43\n43\n45\ng\nsize\ng_{\\mathrm{size}}\n(m)\n0.55\nâˆ—\n0.55^{*}\n0.45\n0.50\n0.45\n0.40\n0.40\ng\nthickness\ng_{\\mathrm{thickness}}\n(m)\n0.8\nâˆ—\n0.8^{*}\n1\n1\n1\n1\n1\nh\nground\nh_{\\text{ground}}\n(m)\n0.4\n0\n0\n0\n0\n0\nv\nground\nv_{\\text{ground}}\n(m/s)\n10\n2\n2\n2\n2\n2\nRewards\nÎ»\nprog\n\\lambda_{\\text{prog}}\n0\n1\n1\n1\n1\n1\nv\nm\nâ€‹\na\nâ€‹\nx\nv_{max}\n-\n99\n99\n99\n10\n10\nÎ»\ngate\n\\lambda_{\\text{gate}}\n30\n10\n1\n1\n0\n1.5\nÎ»\nÎ©\n\\lambda_{\\Omega}\n0\n0.001\n0.001\n0.001\n0.001\n0.001\nÎ»\noffset\n\\lambda_{\\text{offset}}\n2\n0\n1\n1\n0\n1.5\nÎ»\nperc\n\\lambda_{\\text{perc}}\n0.1\n0.05\n0.01\n0.01\n0.01\n0.01\nÎ»\nÎ”\nâ€‹\nğ’–\n\\lambda_{\\Delta\\bm{u}}\n0.100\n0.005\n0\n0\n0\n0\nÎ”\nâ€‹\nu\nthresh\n\\Delta u_{\\text{thresh}}\n0.5\n0.3\n0\n0\n0\n0\nÎ»\nğ’–\n\\lambda_{\\bm{u}}\n0\n0\n0\n0.01\n0\n0\nÎ»\ncrash\n\\lambda_{\\text{crash}}\n10\n10\n10\n10\n10\n10\nParameter\nNominal\nRandomization\nk\nÏ‰\nk_{\\omega}\n1.55\nÃ—\n10\nâˆ’\n6\n1.55\\times 10^{-6}\n15\n40\n40\n50\n50\n50\nk\nx\nk_{x}\n5.37\nÃ—\n10\nâˆ’\n5\n5.37\\times 10^{-5}\n20\n40\n40\n50\n50\n50\nk\ny\nk_{y}\n5.37\nÃ—\n10\nâˆ’\n5\n5.37\\times 10^{-5}\n20\n40\n40\n50\n50\n50\nk\nx\nâ€‹\n2\nk_{x2}\n4.10\nÃ—\n10\nâˆ’\n3\n4.10\\times 10^{-3}\n45\n40\n40\n50\n50\n50\nk\ny\nâ€‹\n2\nk_{y2}\n1.51\nÃ—\n10\nâˆ’\n2\n1.51\\times 10^{-2}\n45\n40\n40\n50\n50\n50\nk\nÎ±\nk_{\\alpha}\n3.145\n3.145\n45\n40\n40\n50\n50\n50\nk\nhor\nk_{\\text{hor}}\n7.245\n7.245\n45\n50\n40\n50\n50\n50\nk\np\nâ€‹\n1\nk_{p1}\n4.99\nÃ—\n10\nâˆ’\n5\n4.99\\times 10^{-5}\n45\n40\n40\n50\n50\n50\nk\np\nâ€‹\n2\nk_{p2}\n3.78\nÃ—\n10\nâˆ’\n5\n3.78\\times 10^{-5}\n45\n40\n40\n50\n50\n50\nk\np\nâ€‹\n3\nk_{p3}\n4.82\nÃ—\n10\nâˆ’\n5\n4.82\\times 10^{-5}\n45\n40\n40\n50\n50\n50\nk\np\nâ€‹\n4\nk_{p4}\n3.83\nÃ—\n10\nâˆ’\n5\n3.83\\times 10^{-5}\n45\n40\n40\n50\n50\n50\nJ\nx\nJ_{x}\nâˆ’\n0.89\n-0.89\n50\n40\n40\n50\n50\n50\nk\nq\nâ€‹\n1\nk_{q1}\n2.05\nÃ—\n10\nâˆ’\n5\n2.05\\times 10^{-5}\n45\n40\n40\n50\n50\n50\nk\nq\nâ€‹\n2\nk_{q2}\n2.46\nÃ—\n10\nâˆ’\n5\n2.46\\times 10^{-5}\n45\n40\n40\n50\n50\n50\nk\nq\nâ€‹\n3\nk_{q3}\n2.02\nÃ—\n10\nâˆ’\n5\n2.02\\times 10^{-5}\n45\n40\n40\n50\n50\n50\nk\nq\nâ€‹\n4\nk_{q4}\n2.57\nÃ—\n10\nâˆ’\n5\n2.57\\times 10^{-5}\n45\n40\n40\n50\n50\n50\nJ\ny\nJ_{y}\n0.96\n0.96\n50\n50\n40\n50\n50\n50\nk\nr\nâ€‹\n1\nk_{r1}\n3.38\nÃ—\n10\nâˆ’\n3\n3.38\\times 10^{-3}\n45\n40\n40\n50\n50\n50\nk\nr\nâ€‹\n2\nk_{r2}\n3.38\nÃ—\n10\nâˆ’\n3\n3.38\\times 10^{-3}\n-\n40\n40\n50\n50\n50\nk\nr\nâ€‹\n3\nk_{r3}\n3.38\nÃ—\n10\nâˆ’\n3\n3.38\\times 10^{-3}\n-\n40\n40\n50\n50\n50\nk\nr\nâ€‹\n4\nk_{r4}\n3.38\nÃ—\n10\nâˆ’\n3\n3.38\\times 10^{-3}\n-\n40\n40\n50\n50\n50\nk\nr\nâ€‹\n5\nk_{r5}\n3.24\nÃ—\n10\nâˆ’\n4\n3.24\\times 10^{-4}\n45\n40\n40\n50\n50\n50\nk\nr\nâ€‹\n6\nk_{r6}\n3.24\nÃ—\n10\nâˆ’\n4\n3.24\\times 10^{-4}\n-\n40\n40\n50\n50\n50\nk\nr\nâ€‹\n7\nk_{r7}\n3.24\nÃ—\n10\nâˆ’\n4\n3.24\\times 10^{-4}\n-\n40\n40\n50\n50\n50\nk\nr\nâ€‹\n8\nk_{r8}\n3.24\nÃ—\n10\nâˆ’\n4\n3.24\\times 10^{-4}\n-\n40\n40\n50\n50\n50\nJ\nz\nJ_{z}\nâˆ’\n0.34\n-0.34\n50\n50\n40\n50\n50\n50\nÏ‰\nmin\n\\omega_{\\min}\n341.75\n341.75\n20\n40\n40\n50\n50\n50\nÏ‰\nmax\n\\omega_{\\max}\n3100.00\n3100.00\n15\n33\n30\n30\n30\n40\nk\nk\n0.50\n0.50\n45\n50\n40\n50\n50\n50\nÏ„\n\\tau\n0.025\n0.025\n30\n50\n40\n50\n50\n55\nPPO config\nactivation\ntanh\ntanh\nrelu\ntanh\nrelu\nrelu\nlayers\nÏ€\n\\pi\n3\n3\n3\n3\n3\n3\nneurons\nÏ€\n\\pi\n64\n64\n64\n64\n64\n64\nlayers\nV\nV\n3\n3\n3\n3\n3\n3\nneurons\nV\nV\n256\n256\n64\n256\n64\n64\nent_coef\n0.003\n0.005\n0.003\n0.003\n0\n0\nTable 1\n:\nTraining parameters for all models used in the competition.\nFig.\n2\nalso presents the performance of 100 rollouts in our simulation environment for each network. Following the approach of  (\n?\n), we applied domain randomization to all model parameters; for this evaluation, we used 30% randomization for all parameters. All networks maintained an approximately 90% success rate despite substantial randomization. Robustness of the control for variations in drone properties is obtained by using even stronger randomization during training\n2\n2\n2\nLAP16 is an exception, as some parameters use less than 30% randomization. However, most parameters are randomized\nâ‰¥\n\\geq\n40%, and evaluation with 30% randomization still achieves a 94% success rate.\n(See parameters in Table\n1\n). The results show that real-world completion times closely match the average simulated times, highlighting randomized simulation as a powerful and reliable tool for predicting real-world performance. Simulated crash rates, in contrast, were less predictive, as the simulator does not accurately model vision-based state estimation errors that occasionally caused real-world crashes.\nA\nB\nFigure 2\n:\n(\nA\n) G&CNet completion times: real flights versus simulation. (\nB\n) Fastest lap trajectory top view (M16) and the corresponding measured forces, moments, and actuator outputs versus the nominal model.\nThe robustness of our approach becomes most evident when analyzing the trajectory of our fastest lap as seen in Fig.\n2\n. Measured specific forces, moments, and motor speeds deviated substantially from the nominal model but largely stayed within â€” and at times exceeded â€” the 30% randomization bounds. Even in instances where the measurements exceeded these limits, the controller maintained stable flight and completed the lap. This ability to operate reliably beyond the nominal design envelope highlights the power of excessive randomization in training, allowing the drone to handle unmodeled dynamics while still achieving human-beating performance.\nEvaluation and optimization with gate reprojections\nOne of the great challenges of the A2RL competition is the absence of any external motion tracking or positioning system. This prevents the use of any â€œground truthâ€ position, which is typically used for fitting the droneâ€™s parameters or simply debugging the state estimation pipeline.\nWe therefore combine the droneâ€™s onboard images with the known sizes and shapes of the racing gates to still allow for evaluation and optimization in the absence of any external ground truth system.\nOur proposed evaluation and optimization method is rooted in â€œprediction error minimizationâ€, a central concept in both state estimation filters  (\n?\n) and theories on the functioning of human brains  (\n?\n). Ignoring observation noise, a perfect state estimate will result in a perfect correspondence between the expected observation of the world with the actual observation of the world. This prediction error, which is termed â€œinnovationâ€ in the control systems literature, is what leads state corrections in Kalman filters. Whereas typically this innovation is only used for correcting the state, one can actually also use it to optimize any part of the involved models and parameters. Suppose that we have two sets of state estimation filter parameters,\nÎ˜\n1\n\\Theta_{1}\nand\nÎ˜\n2\n\\Theta_{2}\n, capturing aspects such as a droneâ€™s dynamics and its sensor properties (e.g., noise variance). If\nÎ˜\n1\n=\nÎ˜\nâˆ—\n\\Theta_{1}=\\Theta^{*}\n, i.e., it corresponds to the true dynamics and sensor properties, then we expect to have a smaller accumulated filtering innovation than when filtering the state with\nÎ˜\n2\nâ‰ \nÎ˜\nâˆ—\n\\Theta_{2}\\neq\\Theta^{*}\n. This enables, for instance, evolutionary algorithms to optimize parameters like the variances of the different sensor measurements  (\n?\n), by minimizing the innovation over a given set of flight data.\nAlthough we did investigate the use of the filterâ€™s accumulated absolute innovation, we converged on a related but more sensitive metric that better captures long-term prediction quality, namely the correspondence between the expected racing gate pixels and the actually observed racing gate pixels in the gate segmentation images. Specifically, we use the state estimate together with the flight plan (map of the drone racing gates) to re-project gate masks in the image, and compare them with the actual observed gate segmentation with the help of the Intersection over Union (IoU). We then take the average of the IoU over one or more flights. This method can be used to assess the impact of changing any parameter in the state estimation pipeline, ranging from various parameters used in the acceptance of corner measurements to drone dynamics parameters. A higher resulting IoU indicates an improvement in the resulting state estimation.\nBesides the evaluation of code changes, we also used the IoU in an offline optimization process. Although it can, in principle, optimize any parameter, we mainly used the method for the estimation of the extrinsic camera calibration parameters. Due to the flexible TPU camera mount, the angular offsets of the camera to the droneâ€™s body would regularly change, for instance, after any impact, modification or repair of the drone. The calibration setup present at the competition did not allow for a highly precise extrinsic calibration. However, over an entire flight, the onboard gate segmentation images provided enough evidence for the fine-tuning of these vital parameters. Hence, after a crash or repair, we flew the drone through the track with one of our safer networks, and subsequently optimized the camera extrinsic angles with the help of this IoU method. We used Bayesian optimization  (\n?\n), since this allowed good results with limited samples - obeying the strict timing of the drone racing competition.\nFig.\n3\nshows the functioning of the mask-based camera extrinsics optimization method, applied both in simulation (top) and on real flight data (bottom).\nIn the simulation experiment, we initialized the camera extrinsics with a 2-degree error relative to ground truth (for all angles). On the top left, the segmented gate (red) and the re-projected gate (white) do not fully overlap, yielding an average IoU of\n0.83\n0.83\n. After optimization with\n40\n40\niterations, the overlap improves significantly (average IoU\n0.91\n0.91\n), and the estimated extrinsics\n(\n1.9\n,\n54.6\n,\n2.3\n)\n(1.9,54.6,2.3)\nare within\n1\nâˆ˜\n1^{\\circ}\nof the ground truth\n(\n2\n,\n55\n,\n2\n)\n(2,55,2)\n.\nA\nB\nFigure 3\n:\nSelf-supervised refinement from onboard data only using mask-based extrinsic optimization. (\nA\n) Camera extrinsics optimization in the Software In the Loop (SIL) simulator. The estimated extrinsics align within\nâ‰ˆ\n0.5\nâˆ˜\n\\approx 0.5^{\\circ}\nof ground truth in only 40 steps optimizing a single log. (\nB\n) Result of the self-supervised refinement applied to camera extrinsics optimization on real flight data for only\n40\n40\nsteps.\nAt the bottom of the figure, we applied the same method to real race data. Here, we take the extrinsics estimated by Kalibr\n3\n3\n3\nhttps://github.com/ethz-asl/kalibr\n(\n?\n) as comparison. The initial estimate\n(\nâˆ’\n3.4\n,\n53.9\n,\nâˆ’\n4.5\n)\n(-3.4,53.9,-4.5)\ndeviates from the calibration\n(\nâˆ’\n1.4\n,\n51.9\n,\nâˆ’\n2.5\n)\n(-1.4,51.9,-2.5)\n(again 2 degrees offset per angle) and the overlap with the segmented gate is poor (average IoU\n0.64\n0.64\n). After\n40\n40\noptimizer calls, the estimated extrinsics\n(\nâˆ’\n0.8\n,\n51.9\n,\nâˆ’\n2.5\n)\n(-0.8,51.9,-2.5)\nalign much better with the Kalibr result, and the average IoU improves to\n0.78\n0.78\n. This demonstrates that the optimization script can recover accurate extrinsics both in simulation and with real flight images, within about\n1\nâˆ˜\n1^{\\circ}\nerror.\nTo test robustness, we ran this optimization for ten different single simulated flights with random\nğ’°\nâ€‹\n(\nâˆ’\n2\nâˆ˜\n,\n2\nâˆ˜\n)\n\\mathcal{U}(-2^{\\circ},2^{\\circ})\nperturbations in camera extrinsics.\nThe mean absolute errors of the optimized estimates were:\nPitch\nâ€‹\n0.14\nâˆ˜\nâ€‹\n(\nÏƒ\n=\n0.09\n)\n,\nRoll\nâ€‹\n0.49\nâˆ˜\nâ€‹\n(\nÏƒ\n=\n0.20\n)\n,\nYaw\nâ€‹\n0.71\nâˆ˜\nâ€‹\n(\nÏƒ\n=\n0.28\n)\n\\text{Pitch }0.14^{\\circ}\\ (\\sigma=0.09),\\quad\\text{Roll }0.49^{\\circ}\\ (\\sigma=0.20),\\quad\\text{Yaw }0.71^{\\circ}\\ (\\sigma=0.28)\nindicating sub-degree accuracy across runs.\nIMU saturation\nThe combination of an aggressive reinforcement learning (RL) controller capable of producing thrusts of up to\n7\nâ€‹\ng\n7g\n, structural vibrations, and noise of the accelerometer led to reaching the accelerometer limit of\n16\nâ€‹\ng\n16g\n. Upon saturation, the accelerometer provided measurements with sometimes extreme errors that quickly led to divergence in the IMU-based Kalman filter state, resulting in multiple crashes. This primarily occurred during the Split-S maneuver, but also after gate\n1\n1\nin the second lap, where we make a sharp turn with high velocity. To address this issue, we implemented a model-based acceleration prediction mechanism specifically activated during IMU saturation events. When the discrepancy between the measured accelerations and the droneâ€™s dynamic model-based estimates exceeds a predefined threshold, the Kalman filter incorporates these model-predicted accelerations instead of the saturated sensor readings. Additionally, we inflated the Kalman filterâ€™s uncertainty in both position and attitude states during such events to rely more on visual sensor measurements, thereby ensuring stable and accurate state estimation during aggressive flight maneuvers.\nFig.\n4\na illustrates a trajectory segment where the drone crashed into the bottom gate of a Split-S maneuver due to IMU saturation in the body-z axis. The red trajectory represents state predictions derived from a Kalman filter (KF) using IMU data exclusively for the prediction step. The green trajectory shows state predictions from a Kalman filter that uses model-based acceleration estimates during IMU saturation. The model-corrected Kalman filter accurately predicts the trajectory of the drone, clearly indicating the imminent crash into the gate.\nThe adjacent acceleration plot compares the modeled thrust (green) and the measured IMU acceleration in the z-direction (red). Between timestamps\nt\n1\nt_{1}\nand\nt\n2\nt_{2}\n, significant discrepancies are evident due to IMU saturation.\nThe bottom row shows gate reprojections: green indicates estimates from the model-corrected KF, and red indicates estimates from the IMU-predicted KF. Accurate state estimation is confirmed when the re-projected gates align closely with the actual gate positions in the images (purple). This visualization demonstrates that incorporating model-based predictions significantly improves the state estimate during aggressive maneuvers, such as the Split-S.\nThe controller that secured victory in the AI Grand Challenge, M17, achieved an overall success rate of\n5\n5\nout of\n7\n7\nflights. The two crashes were caused by IMU saturation. In fact, the IMU-only prediction resulted in\n2\n2\nsuccessful flights and\n2\n2\ncrashes, corresponding to a\n50\n%\n50\\%\nsuccess rate. In contrast, incorporating model-based correction during IMU saturation led to a success rate of\n100\n%\n100\\%\n, with all remaining 3 out of 3 flights successfully completed, on top of correctly measuring the crash in the previously failed logs.\nA\nB\nFigure 4\n:\nOnboard sensor data corruption. (\nA\n) Comparison of the accelerometer-based versus model-based predicted trajectories, thrust and gate reprojections during a split-S with accelerometer saturation. (\nB\n) Successful dual lap completion despite 50% image corruption (left) and single lap completion with 75% image corruption from camera interference-caused purple horizontal bars, outdated top pixels and vertically shifted parts of the image.\nCamera interference\nThe drone designed by the organizers originally featured long MIPI cables for the camera connection. These cables were prone to bending under high thrust loads and occasionally came into contact with other hardware components, like the high noise SSD, resulting in electromagnetic interference. As illustrated in Fig.\n4\nb, this led to substantial image corruption that primarily occurred during periods of high acceleration in the z-axis. Across\n10\n10\nrecorded flights, between\n8\n%\n8\\%\nand\n75\n%\n75\\%\nof captured images were corrupted. Two flights ended in crashes: one due to complete camera failure mid-flight, and another following a flight in which\n75\n%\n75\\%\nof the images were corrupted. In the latter case, the drone flew more than 25 m without valid images, successfully passing through one gate but crashing into a second gate, as shown in the right image of Fig.\n4\nb.\nAlthough the hardware problem was later solved,\nMonoRace can address this type of problem thanks to its incorporated fail-safe strategies. First, the gate segmentation network does not find gates in the corrupted parts of the frames (Fig.\n4\nb.3). Next, a RANSAC-based outlier rejection matches the 2D affine transformation between priors to corner candidates to reject erroneous corners (Fig.\n4\nb.2). Finally, our KF also filters out measurements that deviate too much from the predicted state, based on the uncertainty level of the Kalman Filter (Fig.\n4\nb.4). Combined, this allowed the drone to survive most of the interference, while still refining the position with any occasional good frame.\nDISCUSSION\nIn this work, we have presented MonoRace: a fully onboard, monocular, rolling-shutter perception-control pipeline for autonomous drone racing that won the April 2025 Abu Dhabi Autonomous Drone Racing Competition (A2RL), outperforming all competing AI teams and three world champion FPV pilots in direct knockout heats. On the competition track, the vehicle reached a maximum speed of 28.23 m/s, which, to our knowledge, is the fastest fully onboard, autonomous flight reported to date. For context,  (\n?\n) reports 22 m/s on a different\n30\nÃ—\n30\nÃ—\n8\n30\\times 30\\times 8\nm track  (\n?\n); our results were obtained on a\n76\nÃ—\n18\nÃ—\n5.4\n76\\times 18\\times 5.4\nm track, using a monocular CMOS camera, a saturating IMU and no external aids at any stage.\nA key element is the Guidance-and-Control Network (G&CNet) that directly outputs motor commands, paired with vision-based state estimation. Extensive domain randomization helped bridge the simulation-reality gap at high speeds where aerodynamic effects are less predictable  (\n?\n). By bypassing intermediate control loops, the G&CNet achieved a very low end-to-end latency; the controller operated at 500 Hz with full throttle authority, allowing the motor commands to change from 0-100\n%\n\\%\nin\n2\n2\nms.\nThe monocular visual state estimator proved robust to camera interference and IMU saturation. The drone successfully completed the full track with\n50\n%\n50\\%\nof the frames being corrupted. During periods of high\ng\ng\nloading and structural vibration that caused IMU saturation, we replaced measured accelerations with model predictions, which stabilized the estimator.\nAlthough the proposed drone racing approach set a new milestone, several improvements are still possible. First, the vision and control remain decoupled. On the one hand, this results in a larger necessity for reward shaping, e.g., including an explicit penalty for not looking at the gates. On the other hand, the control network expects perfect state estimation, while the estimated state is always uncertain to a given extent. A possible solution avenue is to perform end-to-end learning. Second, the vision pipeline heavily relies on the rectangular shape of the gates for both the corner detection and the localization relative to the gate. In contrast, humans can easily adapt to different gate shapes, with no training necessary. In order to deal with different gate shapes, one could train a neural network to directly map an image to relative gate poses. It would be straightforward to artificially generate a dataset for this end. However, refinement in the real world without any available ground-truth is less evident. The mask reprojection idea could perhaps be used to retrieve the ground truth relative positions needed for such refinement. Third, the proposed approach does not yet detect and avoid other drones on the track. This did not hamper its performance in the human vs. AI tournament, but it did result in a crash during the multi-AI event, in which four AI drones flew the track at the same time. In that event, our drone took off first, flying fast, but then had to double the other drones. Without drone detection and avoidance, it flew into the drone of another team, resulting in a 3\nrd\nplace in that event. The main challenge here will be to detect and avoid other drones, while not adding too much computational effort and complexity to the vision and control pipeline.\nAlthough the proposed approach focuses on agile flight for drone racing, it will have a much broader impact. The proposed guidance and control neural network is small enough to comfortably run on a microcontroller and still approximate time-optimal flight. The required computation time for this is a fraction of traditional pipelines that first plan trajectories and then attempt to execute them with the help of online guidance and control. The current study illustrates that G&CNets have the potential to unlock optimal control for even the smallest and cheapest robots.\nIn this article, the application is FPV drone racing, an e-sports in which drones may play a similar role in the future as chess programs play in the board game of chess today: to assist human players in training and unlocking novel moves and strategies that have not yet been thought of by human beings. Further generalizing the approach away from drone-race-specific elements, such as racing gates, will allow for many more real-world applications. Of course, the extremely fast flight speeds immediately suggest military applications, where fast flight has not only offensive but also defensive utility. However, we hope and expect the main application to lie in the societal domain; Currently, fully autonomous drones are typically still rather slow, which means that they fly at speeds that are sub-optimal for energy usage. Slight changes to the reward functions can enable smooth and swift flight, with the ability to adapt speed to the requirements of the environment and the mission. This will, in general, allow drones to perform longer-range and duration missions, while slowing down when safety requires it.\nMATERIALS AND METHODS\nOverview\nHere, we provide a brief overview of the proposed approach. A high-level summary of the method is illustrated in Fig.\n5\n. We introduce hardware, the state-estimation algorithm, and the reinforcement-learning-based control strategy. A more detailed explanation of each component is provided in the Perception and Control sections.\nThe drone uses a custom carbon fiber frame designed by the competition organizers and has a total mass of 966 g. Its sensor suite consists of a monocular rolling-shutter camera with a wide field of view (155Â° horizontal Ã— 115Â° vertical), and an IMU integrated into the flight controller. The IMU provides accelerometer measurements at 1000 Hz and gyroscope measurements at 2000 Hz. All computation is performed onboard, using an NVIDIA Jetson Orin NX as the main processing unit.\nOur state estimation approach begins by capturing images of\n820\nÃ—\n616\n820\\times 616\nat 90 Hz, which are adaptively cropped and resized to\n384\nÃ—\n384\n384\\times 384\nbased on the predicted gate locations. The resulting crops are processed by our gate-segmentation model, GateNet. From the resulting segmentation masks, our gate-specific corner detector, QuAdGate, extracts gate edges and computes their intersections to accurately localize gate corners. After corner detection, we apply homography-based outlier rejection between the detected corners and the expected corner projections derived from the prior state estimate. Finally, we estimate the droneâ€™s pose using a Perspective-n-Point (PnP) solution combined with attitude estimates from the Extended Kalman Filter (EKF). The pose estimates from visual processing are then fused with high-rate IMU data in an EKF to ensure robust and accurate state estimation even during aggressive flight maneuvers. Due to the high\ng\ng\n-loads and structural vibrations, the accelerometer saturates. To mitigate this, we employ a dynamic drone model to detect IMU saturation and correct the state predictions accordingly.\nFigure 5\n:\nOverview of the pipeline and physical location of various elements. An IMX219 camera image is grabbed and timestamped on the Orin. Adaptive state-based cropping selects the part of the wide-field-of-view image with the best statistics for detecting corners. Gate segmentation is performed on the GPU, and subsequently, precise corner detection is performed from edge identification. After outlier rejection, the relative pose is computed with respect to the assumed gate location. After sensor fusion with delay compensation, the state is sent to the flight controller, which runs a 500Hz local state estimation filter and the direct-to-motor neural G&CNet, which was trained in RL simulation with randomization and is being executed with zero-shot transfer.\nFinally, the estimated states are used as an input to a Guidance & Control Network (G&CNet), which operates at 500 Hz and directly outputs motor commands. This light neural network runs on the flight controller itself, which is equipped with an STM32H743 ARM processor (480 MHz). The network distinguishes itself through its real-time responsiveness and adaptability, enabling precise trajectory tracking, aggressive maneuvering and instant re-planning in complex, high-speed racing environments. It allows faster flight by removing the unpredictability of a traditional inner-loop controller near saturation  (\n?\n). The networks are trained with reinforcement learning in a simplified simulation of the race drone that captures the dominant actuator dynamics, aerodynamic forces, and moments. Using this simulation together with a reward function, the policy is optimized via PPO to complete the track in minimal time. To support robust sim-to-real transfer, broad domain randomization is applied to all model parameters, following the approach of  (\n?\n). During the competition, various combinations of randomization ranges, reward functions, and PPO hyperparameters were explored to achieve different speed/robustness trade-offs. The network with the fastest lap time was ultimately deployed against the human pilots, which it successfully outperformed.\nPerception\nAdaptive Cropping\nWe use a modified camera driver to capture images at 90 Hz with the full field of view of\n155\nâˆ˜\nÃ—\n115\nâˆ˜\n155^{\\circ}\\times 115^{\\circ}\nand a resolution of\n820\nÃ—\n616\n820\\times 616\npixels. As a first preprocessing step, the images are undistorted. To reduce computational load and enable real-time processing on the Jetson Orin NX at 90 Hz, we subsequently limit our vision pipeline to a\n384\nÃ—\n384\n384\\times 384\npixel region (\n29.2\n%\n29.2\\%\nof the original image).\nFor each incoming frame, we predict the expected pixel locations of all gate corners using the current state estimate. Based on visibility and distance, we select the image area with the two closest visible gates for further processing. Gates are excluded when viewed at an excessively oblique angle that would result in strong aspect-ratio distortions; in such situations, even small pixel-level errors can induce large errors in the estimated lateral position and heading estimation. We also discard gates when the projected center of the gate lies outside the image boundaries.\nIf the predicted gate corners are within a\n384\nÃ—\n384\n384\\times 384\nregion of the full-resolution image, we directly crop that region. Otherwise, we first resize the image and then crop the\n384\nÃ—\n384\n384\\times 384\nwindow around the predicted gate location. This adaptive cropping strategy improves computational efficiency while minimizing loss of pixel accuracy, ensuring that the perception module receives a consistent input resolution regardless of gate distance or viewing angle. It effectively analyzes distant gates with original resolution while downscaling large, close gates.\nIn Fig.\n6\nC,\nwe compare several strategies for feeding the original image to the\n384\nÃ—\n384\n384\\times 384\npixel network, analyzed on the three fastest successful flights in the competition. The first option is simply resizing the full image to\n384\nÃ—\n384\n384\\times 384\n(â€˜Resizedâ€™) without preservation of the horizontalâ€“vertical aspect ratio (AR). â€˜Resized fixed ARâ€™ refers to first scaling the image to\n511\nÃ—\n384\n511\\times 384\n, followed by a\n384\nÃ—\n384\n384\\times 384\ncrop. â€˜Center croppingâ€™ directly crops a\n384\nÃ—\n384\n384\\times 384\nregion from the center of the original image without resizing.\nâ€˜Adaptive croppingâ€™ achieves the best performance across all metrics: it obtains the highest IoU with the gate segmentation, detects\n36\n%\n36\\%\nmore corners than the simple resized approach, and yields\n25\n%\n25\\%\nmore frames with sufficient points for a valid PnP solution. The adaptive cropping is able to detect more corners, especially in gates that are far away, as these become too small to detect when resizing the image.\nIn contrast, center cropping performs substantially worse due to the reduction in the field of view combined with the very high pitch angles. In one of the three evaluated flights, this method even led to divergence of the Kalman filter, which in turn explains the markedly lower IoU observed for the center-cropping approach.\nGateNet\nNetwork\nFor gate segmentation, we adopt a U-Netâ€“style architecture  (\n?\n), inspired by prior work  (\n?\n), which we refer to as GateNet. This network consists of an encoderâ€“decoder with skip connections from the encoder to the corresponding decoder layer, following the standard U-Net design, where channel dimensions are scaled by a factor\nf\nf\n. Each block consists of double\n3\nÃ—\n3\n3\\times 3\nconvolutional layers with batch normalization and ReLU activation functions.\nThe network structure first consists of initial double convolutional blocks, denoted as\ninc\n.\ndownk\ndenotes a max-pooling layer followed by a double convolutional block with\nk\nk\noutput channels. Before applying\ndownk\n, the corresponding features are stored as skip connections for the decoder.\nupk\ndenotes a transposed convolution and batch normalization, followed by adding the skip connections from the corresponding encoder layer, and then a double convolutional block with\nk\nk\noutput channels. Finally,\noutc-1\ndenotes a\n1\nÃ—\n1\n1\\times 1\nconvolution mapping to a single output channel and a sigmoid activation function. The resulting network is defined as:\nEncoder\nDecoder\nOutputs\ninc-64/f\nâ†’\nup4-64/f\nâ†’\noutc4-1\nâ†“\nâ†‘\ndown1-128/f\nâ†’\nup3-64/f\nâ†’\noutc3-1\nâ†“\nâ†‘\ndown2-256/f\nâ†’\nup2-128/f\nâ†’\noutc2-1\nâ†“\nâ†‘\ndown3-512/f\nâ†’\nup1-256/f\nâ†’\noutc1-1\nâ†“\nâ†—\ndown4-512/f\nâ†’\noutc0-1\n\\displaystyle\\begin{array}[]{c c c}\\text{Encoder}&\\text{Decoder}&\\text{Outputs}\\\\[5.69054pt]\n\\texttt{inc-64/f}&\\rightarrow\\texttt{up4-64/f}&\\rightarrow\\texttt{outc4-1}\\\\[2.84526pt]\n\\downarrow&\\uparrow&\\\\[2.84526pt]\n\\texttt{down1-128/f}&\\rightarrow\\texttt{up3-64/f}&\\rightarrow\\texttt{outc3-1}\\\\[2.84526pt]\n\\downarrow&\\uparrow&\\\\[2.84526pt]\n\\texttt{down2-256/f}&\\rightarrow\\texttt{up2-128/f}&\\rightarrow\\texttt{outc2-1}\\\\[2.84526pt]\n\\downarrow&\\uparrow&\\\\[2.84526pt]\n\\texttt{down3-512/f}&\\rightarrow\\texttt{up1-256/f}&\\rightarrow\\texttt{outc1-1}\\\\[2.84526pt]\n\\downarrow&\\nearrow&\\\\[2.84526pt]\n\\texttt{down4-512/f}&&\\rightarrow\\texttt{outc0-1}\\\\\n\\end{array}\nGateNet produces five output maps\n{\ny\n0\n,\ny\n1\n,\ny\n2\n,\ny\n3\n,\ny\n4\n}\n\\{y_{0},y_{1},y_{2},y_{3},y_{4}\\}\nat progressively increasing resolutions. These multi-scale predictions are supervised using auxiliary losses between each output map and appropriately down-scaled ground-truth masks, improving gradient flow and overall segmentation accuracy. During deployment, only the highest-resolution output map is used.\nTraining setup\nFor both training and deployment, we use a resolution of\n384\nÃ—\n384\n384\\times 384\nand\nf\n=\n4\nf=4\n. Before training, Xavier uniform weight initialization  (\n?\n) is applied. GateNet is trained for\n100\n100\nepochs with a base learning rate of\n1\nâ‹…\n10\nâˆ’\n3\n1\\cdot 10^{-3}\n, reduced by a factor of\n0.1\n\\sqrt{0.1}\nat epochs\n10\n10\n,\n33\n33\n,\n66\n66\nand\n90\n90\n, resulting in a final learning rate of\n1\nâ‹…\n10\nâˆ’\n5\n1\\cdot 10^{-5}\n. Training is performed with the AdamW optimizer and a batch size of\n16\n16\n. Each output map is supervised using a combination of Dice loss and binary cross-entropy (BCE) loss:\nâ„’\ni\n=\nâ„’\nDice\nâ€‹\n(\ny\ni\n,\ny\n^\ni\n)\n+\n2\nâ‹…\nâ„’\nBCE\nâ€‹\n(\ny\ni\n,\ny\n^\ni\n)\n\\displaystyle\\mathcal{L}_{i}=\\mathcal{L}_{\\text{Dice}}(y_{i},\\hat{y}_{i})+2\\cdot\\mathcal{L}_{\\text{BCE}}(y_{i},\\hat{y}_{i})\nFinally, we apply output-specific scaling factors to emphasize higher-resolution predictions:\nâ„’\ntot\n=\n4\nâ‹…\nâ„’\n0\n+\n2\nâ‹…\nâ„’\n1\n+\nâˆ‘\ni\n=\n2\n4\nâ„’\ni\n.\n\\displaystyle\\mathcal{L}_{\\text{tot}}=4\\cdot\\mathcal{L}_{0}+2\\cdot\\mathcal{L}_{1}+\\sum_{i=2}^{4}\\mathcal{L}_{i}.\nSynthetic data\nTo reduce the need for manual labeling and improve generalization across different track layouts, we generate synthetic data in addition to manually labeled real-world images. In our synthetic data pipeline, each gate face is subjected to a series of randomized transformations, including scaling, rotation, and perspective warping to simulate varied viewpoints and in-flight camera distortions. Once transformed, the gate is composited onto a background image sampled from a curated set of representative environments (warehouses, parking lots, empty halls, etc.)\nTo increase photometric variability and simulate lighting changes, both gate and background images undergo HSV-based color augmentations independently. This includes random shifts in hue, saturation, and value to mimic different lighting conditions and material reflectances. Furthermore, we introduce image-level noise by applying Gaussian noise with randomly sampled variance across images. This noise emulates sensor imperfections like thermal noise  (\n?\n) together with environmental artifacts, and contributes to the robustness of the trained model. All augmentation strategies were applied in a randomized manner per sample, ensuring a high degree of variation across the synthetic dataset (See Fig.\n6\nB). The resulting images form a diverse and richly augmented set that helps bridge the domain gap between synthetic and real-world data, and operate the robot in new environments without any additional image labeling.\nData augmentations\nThe synthetic and real-world datasets are merged and fed through a unified dataloader (in a 3500:500 synthetic to real split) to ensure consistent preprocessing and augmentation across all samples. During training, each image undergoes randomized affine transformationsâ€”comprising rotation, translation, and scalingâ€”to enhance spatial variability and viewpoint diversity. Additional photometric augmentations are performed in the HSV color space to also vary labeled images, introducing random perturbations in hue, saturation, and value to emulate lighting variations and material appearance changes.\nTo further mimic the visual artifacts encountered during high-speed flight, artificial motion blur is applied by convolving each image with a square averaging kernel of random size (5â€“15 pixels) and random orientation. Complementary noise models, including thermal noise and kernel-based blurring, are also introduced to replicate sensor noise and lens-induced degradation. These augmentations, applied stochastically at load time, yield a more diverse and robust training distribution that facilitates improved generalization.\nQuAdGate\nTo obtain sub-pixel steady corners needed for precise PnP, despite sometimes rounded or incomplete gate segmentations, a solution is proposed that exploits the entire gate edge.\nThe algorithm, called QuAdGate, extracts inner and outer gate corners from the segmentation masks (See Fig.\n6\nD). It detects and matches gate corners by combining the segmentation output with predicted corner coordinates (priors) based on state estimation. QuAdGate first uses a line detector to extract line segments from the segmentation mask and computes their intersections to identify corner candidates. It then matches these candidates to the priors using handcrafted descriptors obtained from the segmentation map.\nA\nB\nC\nResized\nResized\nfixed AR\nCenter\ncropping\nAdaptive\ncropping\nIoU\n0.47\n0.48\n0.36\n0.50\nDetected corners\n19168\n19436\n15437\n26516\nFrames with PnP\n2950\n2913\n2426\n3647\nD\nFigure 6\n:\n(\nA\n) Vision Pipeline Elements. (\nB\n) Data augmentation from composited gate (a) and representative background. Individual augmentations include (b) HSV-based color augmentation, (c) directional brightness gradient, (d) Gaussian blur, (e) additive Gaussian noise, (f) lens distortion, and (g) rolling-shutter kernel blur. Combining these augmentations results in (h) fully augmented image.\n(\nC\n) Adaptive cropping allows light-weight processing of the most interesting part of the image, as illustrated by the best score in terms of IoU, number of accepted corners and â€˜frames with PnPâ€™ computed for the three fastest successful flights.\n(\nD\n) â€˜QuAdGateâ€™ precise corner fitting from possibly damaged segmentations based on edge detections. Lines intersections define precise corner candidates (blue). Around each candidate, local segmentation values are sampled (clockwise from the top-left) to form descriptors (cyan). These descriptors are then used to match candidates with the priors (green), yielding the final accepted corners (orange). Even with substantially misplaced priors (green), QuAdGate successfully matches them with the correct candidates (orange) while rejecting spurious corner candidates (blue).\nCorner detection\nThe segmentation mask is first derotated using the estimated attitude such that the vertical axis in the image matches the up-direction in the world frame. The corner detection begins by identifying lines using the OpenCV implementation of the Line Segment Detector (LSD)  (\n?\n), applied to the segmentation masks. In our implementation, the parameters are set to\nscale\n=\n0.8\n\\texttt{scale}=0.8\n,\nsigma_scale\n=\n0.8\n\\texttt{sigma\\_scale}=0.8\n,\nquant\n=\n25.0\n\\texttt{quant}=25.0\n, and\nang_th\n=\n30.0\n\\texttt{ang\\_th}=30.0\n.\nThese are chosen to balance detecting the majority of lines from the segmentation masks while ensuring that each side of the gate is represented by exactly one line to avoid multiple detections. Next, the detected lines are extended equally in both directions such that their total length increases by a factor of\n5\n/\n3\n5/3\n. The intersections between these extended lines are then computed to determine gate corner candidates. By relying on the intersections of the detected lines rather than the extremities of the segmentation mask itself, as in De Wagter et al.  (\n?\n), the accuracy of the corner coordinates is no longer affected by rounding or other minor imperfections in the mask.\nDescriptors\nTo associate corner candidates with prior gate corners projected from the EKF estimate, handcrafted descriptors are extracted from the thresholded segmentation mask. Each descriptor captures four pixel values in the local neighborhood of a candidate corner:\nd\nâ€‹\n(\np\n)\n=\n[\nv\nT\nâ€‹\nL\n,\nv\nT\nâ€‹\nR\n,\nv\nB\nâ€‹\nR\n,\nv\nB\nâ€‹\nL\n]\nT\n\\displaystyle d(p)=[v_{TL},v_{TR},v_{BR},v_{BL}]^{T}\nwhere\nd\nâ€‹\n(\np\n)\nd(p)\nis the descriptor of corner candidate\np\np\n, and\nv\nTL\n,\nv\nTR\n,\nv\nBR\n,\nv\nBL\nv_{\\text{TL}},v_{\\text{TR}},v_{\\text{BR}},v_{\\text{BL}}\nare the corresponding pixel values at the top-left, top-right, bottom-right, and bottom-left, respectively. These positions are determined by moving\n5\n5\npixels along the directions of the intersecting lines. For instance, the top-left value is obtained by moving 5 pixels to the left along the horizontal line and 5 pixels up along the vertical line. For a top-right outer corner, the resulting prior descriptor is\n[\n0\n,\n0\n,\n0\n,\n1\n]\nT\n[0,0,0,1]^{T}\n, with\n0\nrepresenting regions of empty space and\n1\n1\nmarking pixels belonging to the segmentation mask.\nCorner matching\nAn initial filtering step matches corner candidates to priors. Each candidate is first classified by corner type, allowing no discrepancy between the descriptor of a prior and that of a corner candidate. Candidates are further restricted to lie within\n100\n100\npixels of the corresponding prior. Next, an optimal 2D affine transformation from priors to candidates, constrained to four degrees of freedom (translation, rotation, and uniform scaling), is estimated using RANSAC  (\n?\n) to filter out incorrect matches and other outliers. We employ OpenCVâ€™s\nestimateAffinePartial2D\nwith\nRansacThreshold\n=\n5.0\n\\texttt{RansacThreshold}=5.0\n. Lastly, the solution is rejected if the translation in any direction exceeds\n150\n150\npixels, indicating poor convergence.\nPerspective n Points\nIn contrast to  (\n?\n), who solve a separate PnP problem for each gate, we combine the corners detected from multiple gates (two in our case) in a single PnP optimization when possible.\nThis approach offers two key advantages over treating each gate independently.\nFirst, when the\n2\n2\nDâ€“\n3\n3\nD correspondences come from gates located at different depths (non-coplanar), the variation in spatial configuration improves the ability of PnP to distinguish between translation and rotation. This is illustrated in Fig.\n7\na, where we show in green the reprojection of the first two gates using the estimated drone state based on the initial PnP with a different number of tracked corners. It can be seen that using even a single corner from a second gate improves this reprojection (which corresponds to a more accurate state) and results in a heading estimation difference of approximately\n2\nâˆ˜\n2^{\\circ}\n. This improvement is particularly important for long tracks, where initial heading errors strongly influence state estimation.\nA second important advantage of merging corners from multiple gates in a single PnP optimization is that it allows PnP to reach its required threshold of at least four points more often. For the fastest successful flight, we detected enough corners for PnP\n963\n963\nout of\n1620\n1620\nframes, of which\n263\n263\n(\n27\n%\n27\\%\n) were multi-gate PnP cases. Among these, only\n82\n82\ninstances had two gates with at least four detected corners each, while\n181\n181\nframes benefited from partial detections.\nSparse corner detections occur most frequently at distant gates, as the wide camera field of view and limited pixel resolution make segmentation and corner detection challenging.\nIn Fig.\n7\nb we show that PnP nevertheless benefits from the partial gate corner detections in our three fastest flights. It can be seen that including these additional corners from different gates reduces both position and attitude errors. The difference is most visible when the distance to the primary gate is large.\nA\nB\nC\nFigure 7\n:\nAnalysis of the PnP position and attitude accuracy in function of number of gates, gate distance and gate-derotation. (\nA\n) PnP heading initialization error depends on the number of gates. Even a single point on a second gate improves the reprojection. (\nB\n) PnP accuracy statistics for the three fastest successful. The accuracy becomes unacceptable for single gates beyond about 5m but remains good with derotated double gates. (\nC\n) Overview of the measurement candidates for the Kalman filter during the fastest successful flight, M16, against Killian Rousseau. When multiple gates are in view, the raw PnP becomes visibly better.\nThe OpenCV implementation returns the camera position in a world coordinate system by combining the relative pose to the gate with the gateâ€™s known world coordinates. The solution remains sensitive to geometry, especially at long viewing distances to gates as small rotational errors, for instance due to remaining delays combined with fast body rotations, can still result in important position errors.\nFurthermore, when the gate is very close, motion blur degrades the quality of gate segmentation, resulting in less reliable corner detections. Lastly, PnP is less reliable when using a small number of points. To address this, we only use the full PnP solution when the gate distance is within a range of 2\nm to 5 m and when a sufficient number of corners (at least six) are detected. If one of the two conditions is not met, we instead use the attitude from the Kalman filter state combined with the relative translation (w.r.t. the gate) estimate from PnP to compute the droneâ€™s world position. This fallback approach ensures a more accurate position estimation, especially for far gates, as shown in the left plot in Fig.\n7\nb, denoted as â€De-rotated (2 gates)â€. In Fig.\n7\nc, we show the position estimate from OpenCV (Raw PnP) for each frame with a successful PnP and the de-rotated EKF Attitude method for our fastest flight. In green, we highlight the estimates that were used in our Kalman Filter as measurement candidates. This fallback approach ensures a more stable and\nrobust position estimation. Especially far away from the next gate, the blue crosses much better represent the droneâ€™s position than the highly noisy orange crosses.\nSince our position estimate is strongly coupled with our (EKF) attitude estimation, several measures are needed to feed minimal errors in the state estimation. Therefore, we only update our attitude from PnP in three scenarios that provide sufficiently reliable attitude updates:\n1.\nWhen the distance to the gate is between 2 m and 5 m, and more than six corners are detected.\n2.\nWhen at least two gates are detected, which improves the ability to distinguish between translation and rotation, making the PnP attitude estimate reliable enough for attitude updates (see right plot in Fig.\n7\nb).\n3.\nWhen the drone is stationary (not in flight). In this case, we use the PnP attitude to correct for any potential misalignment in the initial pose estimate, particularly in the otherwise unobservable droneâ€™s heading.\nState Estimation\nIMU Saturation\nDuring our high-speed flights, aggressive maneuvers reaching up to\n7\nâ€‹\ng\n7g\n, combined with structural vibrations and variations in IMU quality across different drones, pushed the accelerometers beyond their\n16\nâ€‹\ng\n16g\nmeasurement range. When saturation occurs, the sensor output is not consistently limited to\nÂ±\n16\nâ€‹\ng\n\\pm 16g\n, but often is inconsistent, making such events more difficult to detect. This saturation or corruption of accelerometer data caused divergence in the Kalman filter, almost inevitably leading to crashes.\nTo overcome these sensor limitations and enable higher-speed flight, we implemented an onboard algorithm that continuously compares predicted accelerations from a dynamic drone model with measured accelerations during all high-speed competition flights. The predicted accelerations are derived from the quadcopter model used in our RL simulator, as defined in Equation\n6\n. When the difference between the predicted and measured accelerations exceeds a predefined threshold, the system switches to using the model-based acceleration predictions for state estimation in the Kalman filter.\nTo reduce high-frequency noise, the raw IMU accelerations are smoothed with a first-order low-pass (exponential moving-average) filter. The identical filter is applied to the model-predicted accelerations so that both signals experience the same time delay:\nğš\nfilt\nâ€‹\n[\nn\n]\n=\nÎ±\nâ€‹\nğš\nfilt\nâ€‹\n[\nn\nâˆ’\n1\n]\n+\n(\n1\nâˆ’\nÎ±\n)\nâ€‹\nğš\nâ€‹\n[\nn\n]\n\\mathbf{a}_{\\text{filt}}[n]=\\alpha\\,\\mathbf{a}_{\\text{filt}}[n\\!-\\!1]\\;+\\;(1-\\alpha)\\,\\mathbf{a}[n]\n(1)\nFor the IMU saturation detection, we compare the filtered sensor output with the filtered model estimate. If the Euclidean norm of their difference exceeds a heuristic threshold\nÏƒ\n=\n22\nâ€‹\nm/s\n2\n\\sigma=22\\ \\text{m/s}^{2}\n, we switch to the model-based predicted accelerations (in all 3 axes) for our state prediction in the KF:\nâ€–\nğš\nfilt\nmodel\nâˆ’\nğš\nfilt\nIMU\nâ€–\n2\n>\nÏƒ\n.\n\\bigl\\lVert\\mathbf{a}_{\\text{filt}}^{\\text{model}}-\\mathbf{a}_{\\text{filt}}^{\\text{IMU}}\\bigr\\rVert_{2}\\;>\\;\\sigma.\n(2)\nKalman Filter\nWe implement an Extended Kalman Filter (EKF) for fusing IMU measurements with position and attitude measurements obtained from the PnP solutions.\nWe define the state vector\nğ±\nâˆˆ\nâ„\n16\n\\mathbf{x}\\in\\mathbb{R}^{16}\nas:\nğ±\n=\n[\nx\ny\nz\nv\nx\nv\ny\nv\nz\nq\nw\nq\nx\nq\ny\nq\nz\nb\nx\nb\ny\nb\nz\nb\np\nb\nq\nb\nr\n]\nâŠ¤\n\\mathbf{x}=\\setcounter{MaxMatrixCols}{16}\\begin{bmatrix}x&y&z&v_{x}&v_{y}&v_{z}&q_{w}&q_{x}&q_{y}&q_{z}&b_{x}&b_{y}&b_{z}&b_{p}&b_{q}&b_{r}\\end{bmatrix}^{\\top}\nThe control input vector\nğ®\nâˆˆ\nâ„\n6\n\\mathbf{u}\\in\\mathbb{R}^{6}\nconsists of IMU measurements. As described before, we use the modeled acceleration instead of the acceleration measured from the accelerometer in cases where the accelerometer saturates.\nğ®\n=\n[\na\nx\na\ny\na\nz\np\nq\nr\n]\nâŠ¤\n\\mathbf{u}=\\begin{bmatrix}a_{x}&a_{y}&a_{z}&p&q&r\\end{bmatrix}^{\\top}\nThe process noise vector\nğ°\nâˆˆ\nâ„\n12\n\\mathbf{w}\\in\\mathbb{R}^{12}\ncaptures IMU noise and bias drift:\nğ°\n=\n[\nw\nx\nw\ny\nw\nz\nw\np\nw\nq\nw\nr\nw\nb\nâ€‹\nx\nw\nb\nâ€‹\ny\nw\nb\nâ€‹\nz\nw\nb\nâ€‹\np\nw\nb\nâ€‹\nq\nw\nb\nâ€‹\nr\n]\nâŠ¤\n\\mathbf{w}=\\setcounter{MaxMatrixCols}{12}\\begin{bmatrix}w_{x}&w_{y}&w_{z}&w_{p}&w_{q}&w_{r}&w_{bx}&w_{by}&w_{bz}&w_{bp}&w_{bq}&w_{br}\\end{bmatrix}^{\\top}\nThe continuous-time process model is given by:\nğ±\nË™\n=\nğŸ\nc\nâ€‹\n(\nğ±\n,\nğ®\n,\nğ°\n)\n=\n[\nğ©\nË™\nğ¯\nË™\nğª\nË™\nğ›\nË™\n]\n=\n[\nğ¯\nğ‘\nâ€‹\n(\nğª\n)\nâ€‹\n[\na\nx\nâˆ’\nb\nx\nâˆ’\nw\nx\na\ny\nâˆ’\nb\ny\nâˆ’\nw\ny\na\nz\nâˆ’\nb\nz\nâˆ’\nw\nz\n]\nâŠ¤\n+\nğ \n1\n2\nâ€‹\nğª\nâŠ—\n[\n0\np\nâˆ’\nb\np\nâˆ’\nw\np\nq\nâˆ’\nb\nq\nâˆ’\nw\nq\nr\nâˆ’\nb\nr\nâˆ’\nw\nr\n]\nâŠ¤\n[\nw\nb\nâ€‹\nx\nw\nb\nâ€‹\ny\nw\nb\nâ€‹\nz\nw\nb\nâ€‹\np\nw\nb\nâ€‹\nq\nw\nb\nâ€‹\nr\n]\nâŠ¤\n]\n\\dot{\\mathbf{x}}=\\mathbf{f}_{c}(\\mathbf{x},\\mathbf{u},\\mathbf{w})=\\begin{bmatrix}\\dot{\\mathbf{p}}\\\\\n\\dot{\\mathbf{v}}\\\\\n\\dot{\\mathbf{q}}\\\\\n\\dot{\\mathbf{b}}\\end{bmatrix}=\\begin{bmatrix}\\mathbf{v}\\\\\n\\mathbf{R}(\\mathbf{q})\\begin{bmatrix}a_{x}-b_{x}-w_{x}&&a_{y}-b_{y}-w_{y}&&a_{z}-b_{z}-w_{z}\\end{bmatrix}^{\\top}+\\mathbf{g}\\\\\n\\frac{1}{2}\\mathbf{q}\\otimes\\begin{bmatrix}0&&p-b_{p}-w_{p}&&q-b_{q}-w_{q}&&r-b_{r}-w_{r}\\end{bmatrix}^{\\top}\\\\\n\\begin{bmatrix}w_{bx}&w_{by}&w_{bz}&w_{bp}&w_{bq}&w_{br}\\end{bmatrix}^{\\top}\\end{bmatrix}\nwhere:\nâ€¢\nğ©\n=\n[\nx\n,\ny\n,\nz\n]\nâŠ¤\n\\mathbf{p}=[x,y,z]^{\\top}\nis the position in world coordinates,\nâ€¢\nğ¯\n=\n[\nv\nx\n,\nv\ny\n,\nv\nz\n]\nâŠ¤\n\\mathbf{v}=[v_{x},v_{y},v_{z}]^{\\top}\nis the velocity in world coordinates,\nâ€¢\nğª\n=\n[\nq\nw\n,\nq\nx\n,\nq\ny\n,\nq\nz\n]\nâŠ¤\n\\mathbf{q}=[q_{w},q_{x},q_{y},q_{z}]^{\\top}\nis the orientation quaternion,\nâ€¢\nğ›\n=\n[\nb\nx\n,\nb\ny\n,\nb\nz\n,\nb\np\n,\nb\nq\n,\nb\nr\n]\nâŠ¤\n\\mathbf{b}=[b_{x},b_{y},b_{z},b_{p},b_{q},b_{r}]^{\\top}\nare the accelerometer and gyro biases,\nâ€¢\nğ‘\nâ€‹\n(\nğª\n)\n\\mathbf{R}(\\mathbf{q})\nis the rotation matrix from body to world frame,\nâ€¢\nğ \n=\n[\n0\n,\n0\n,\ng\n]\nâŠ¤\n\\mathbf{g}=[0,0,g]^{\\top}\nis the gravity vector,\nâ€¢\nâŠ—\n\\otimes\ndenotes quaternion multiplication.\nWe discretize the process model with forward euler to obtain the discrete state transition model:\nğ’™\nk\n+\n1\n=\nğŸ\nâ€‹\n(\nğ’™\nk\n,\nğ’–\nk\n,\nğ’˜\nk\n)\n=\nğ’™\nk\n+\nğŸ\nc\nâ€‹\n(\nğ’™\nk\n,\nğ’–\nk\n,\nğ’˜\nk\n)\nâ‹…\nÎ”\nâ€‹\nt\nk\n\\bm{x}_{k+1}=\\mathbf{f}(\\bm{x}_{k},\\bm{u}_{k},\\bm{w}_{k})=\\bm{x}_{k}+\\mathbf{f}_{c}(\\bm{x}_{k},\\bm{u}_{k},\\bm{w}_{k})\\cdot\\Delta t_{k}\nThe measurement model corresponds to position and orientation estimates obtained from the PnP algorithm:\nğ¡\nâ€‹\n(\nğ±\n)\n=\n[\nx\ny\nz\nq\nw\nq\nx\nq\ny\nq\nz\n]\nâŠ¤\n\\mathbf{h}(\\mathbf{x})=\\begin{bmatrix}x&y&z&q_{w}&q_{x}&q_{y}&q_{z}\\end{bmatrix}^{\\top}\nThe EKF performs a prediction step for every incoming IMU measurement\nğ®\nk\n\\mathbf{u}_{k}\n:\nğ±\nk\n+\n1\n\\displaystyle\\mathbf{x}_{k+1}\n=\nğŸ\nâ€‹\n(\nğ±\nk\n,\nğ®\nk\n,\nğŸ\n)\n\\displaystyle=\\mathbf{f}(\\mathbf{x}_{k},\\mathbf{u}_{k},\\mathbf{0})\nğ\nk\n+\n1\n\\displaystyle\\mathbf{P}_{k+1}\n=\nğ…\nk\nâ€‹\nğ\nk\nâ€‹\nğ…\nk\nâŠ¤\n+\nğ‹\nk\nâ€‹\nğ\nk\nâ€‹\nğ‹\nk\nâŠ¤\n\\displaystyle=\\mathbf{F}_{k}\\mathbf{P}_{k}\\mathbf{F}_{k}^{\\top}+\\mathbf{L}_{k}\\mathbf{Q}_{k}\\mathbf{L}_{k}^{\\top}\nAn update step is performed whenever a new PnP measurement\nğ³\nk\n=\n[\nx\n,\ny\n,\nz\n,\nq\nw\n,\nq\nx\n,\nq\ny\n,\nq\nz\n]\nâŠ¤\n\\mathbf{z}_{k}=[x,y,z,q_{w},q_{x},q_{y},q_{z}]^{\\top}\nbecomes available:\nğ±\nk\n+\n1\n\\displaystyle\\mathbf{x}_{k+1}\n=\nğ±\nk\n+\nğŠ\nk\nâ€‹\n(\nğ³\nk\nâˆ’\nğ¡\nâ€‹\n(\nğ±\nk\n)\n)\n\\displaystyle=\\mathbf{x}_{k}+\\mathbf{K}_{k}\\left(\\mathbf{z}_{k}-\\mathbf{h}(\\mathbf{x}_{k})\\right)\nğ\nk\n+\n1\n\\displaystyle\\mathbf{P}_{k+1}\n=\n(\nğˆ\nâˆ’\nğŠ\nk\nâ€‹\nğ‡\nk\n)\nâ€‹\nğ\nk\n\\displaystyle=\\left(\\mathbf{I}-\\mathbf{K}_{k}\\mathbf{H}_{k}\\right)\\mathbf{P}_{k}\nHere:\nâ€¢\nğŠ\nk\n\\mathbf{K}_{k}\nis the Kalman gain calculated by\nğŠ\nk\n=\nğ\nk\nâ€‹\nğ‡\nk\nâŠ¤\nâ€‹\n(\nğ‡\nk\nâ€‹\nğ\nk\nâ€‹\nğ‡\nk\nâŠ¤\n+\nğ‘\nk\n)\nâˆ’\n1\n\\mathbf{K}_{k}=\\mathbf{P}_{k}\\mathbf{H}_{k}^{\\top}\\left(\\mathbf{H}_{k}\\mathbf{P}_{k}\\mathbf{H}_{k}^{\\top}+\\mathbf{R}_{k}\\right)^{-1}\nâ€¢\nğ…\nk\n=\nâˆ‚\nğŸ\nâˆ‚\nğ±\n|\nğ±\nk\n,\nğ°\nk\n=\n0\n\\mathbf{F}_{k}=\\left.\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{x}}\\right|_{\\mathbf{x}_{k},\\mathbf{w}_{k}=0}\nis the Jacobian of the process model w.r.t the state,\nâ€¢\nğ‹\nk\n=\nâˆ‚\nğŸ\nâˆ‚\nğ°\n|\nğ±\nk\n,\nğ°\nk\n=\n0\n\\mathbf{L}_{k}=\\left.\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{w}}\\right|_{\\mathbf{x}_{k},\\mathbf{w}_{k}=0}\nis the Jacobian of the process model w.r.t. process noise,\nâ€¢\nğ\nk\n\\mathbf{Q}_{k}\n,\nğ‘\nk\n\\mathbf{R}_{k}\nare the process and measurement noise covariances,\nâ€¢\nğ‡\nk\n=\nâˆ‚\nğ¡\nâˆ‚\nğ±\n\\mathbf{H}_{k}=\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{x}}\nis the measurement Jacobian,\nAccurate sensor fusion in our visual-inertial pipeline requires temporal synchronization between the camera and the IMU data. Our system establishes this by employing a timestamping mechanism and compensating for transport and processing latencies using pre-calibrated delays. Upon the arrival of each image, the correctly timestamped sensor measurements are fused to update a state estimate corresponding to the imageâ€™s time of capture. To provide the flight controller with low-latency pose estimates, this delayed state is propagated forward using all subsequent IMU measurements that have not yet been fused. In Software-In-The-Loop (SITL) simulations on the A2RL competition track, with simulated delays of 17ms for images and 0.5ms for IMU data, our method reduced the RMS trajectory error to 0.103m. This presents a significant improvement over the 0.289m error from a baseline approach that naively processes the image-derived pose updates with the most recent IMU measurements at their time of arrival.\nThe measurement noise of the PnP estimation is modeled as:\nÏƒ\npos\n2\n=\n0.02\nâ‹…\nd\ngate\n2\nN\nc\n2\nâ‹…\nN\ng\n\\sigma_{\\text{pos}}^{2}=\\frac{0.02\\cdot d_{\\text{gate}}^{2}}{N_{c}^{2}\\cdot N_{g}}\n(3)\nÏƒ\nquat\n2\n=\n0.01\nâ‹…\nd\ngate\n2\nN\nc\n2\nâ‹…\nN\ng\n\\sigma_{\\text{quat}}^{2}=\\frac{0.01\\cdot d_{\\text{gate}}^{2}}{N_{c}^{2}\\cdot N_{g}}\n(4)\nOutlier rejection is performed based on the Kalman filter position estimate, its covariance, and the number of detected corners. A PnP measurement is accepted if the following condition holds:\nâ€–\nğ±\npos\nâˆ’\nğ±\nPnP\nâ€–\n2\n<\n16\nâ‹…\nN\nc\n2\nâ‹…\ntrace\nâ€‹\n(\nğ\npos\n)\n\\left\\|\\mathbf{x}_{\\text{pos}}-\\mathbf{x}_{\\text{PnP}}\\right\\|^{2}<16\\cdot N_{c}^{2}\\cdot\\text{trace}(\\mathbf{P}_{\\text{pos}})\n(5)\nwhere\nğ±\npos\n\\mathbf{x}_{\\text{pos}}\nis the position estimate from the Kalman filter,\nğ±\nPnP\n\\mathbf{x}_{\\text{PnP}}\nis the position estimated from PnP, and\nğ\npos\n\\mathbf{P}_{\\text{pos}}\nis the position covariance matrix from the Kalman filter.\nControl\nWe use G&CNets  (\n?\n) to directly compute low-level motor commands from the predicted states provided by the vision pipeline. These networks are trained using Proximal Policy Optimization (PPO) in a simulation environment. The following sections detail our reinforcement learning (RL) pipeline, which includes a detailed model for the quadcopter (\nQuadcopter Model\n), an initialization strategy designed to cover a wide range of flight conditions (\nInitialization\n), parameter randomization to improve sim-to-real transfer (\nDomain Randomization\n), the design of our reward function (\nReward Function\n), and the policy architecture and training specifics (\nPolicy and Training\n).\nQuadcopter Model\nTo train the neural control policies, we use a simulation model of the quadcopter, similar to the one described in  (\n?\n). The state vector\nğ±\n\\mathbf{x}\nand control input vector\nğ®\n\\mathbf{u}\nof the quadcopter are defined as follows:\nğ±\n=\n[\nx\ny\nz\nv\nx\nv\ny\nv\nz\nq\nw\nq\nx\nq\ny\nq\nz\np\nq\nr\nÏ‰\n1\nÏ‰\n2\nÏ‰\n3\nÏ‰\n4\n]\nâŠ¤\n\\mathbf{x}=\\setcounter{MaxMatrixCols}{17}\\begin{bmatrix}x&y&z&v_{x}&v_{y}&v_{z}&q_{w}&q_{x}&q_{y}&q_{z}&p&q&r&\\omega_{1}&\\omega_{2}&\\omega_{3}&\\omega_{4}\\end{bmatrix}^{\\top}\nğ®\n=\n[\nu\n1\nu\n2\nu\n3\nu\n4\n]\nâŠ¤\n\\mathbf{u}=\\begin{bmatrix}u_{1}&u_{2}&u_{3}&u_{4}\\end{bmatrix}^{\\top}\nHere,\nğ’‘\n=\n(\nx\n,\ny\n,\nz\n)\n\\bm{p}=(x,y,z)\nand\nğ’—\n=\n(\nv\nx\n,\nv\ny\n,\nv\nz\n)\n\\bm{v}=(v_{x},v_{y},v_{z})\nrepresent the position and linear velocity of the quadcopter in the world frame (North-East-Down, NED, convention), respectively. The unit quaternion\nğ’’\n=\n(\nq\nw\n,\nq\nx\n,\nq\ny\n,\nq\nz\n)\n\\bm{q}=(q_{w},q_{x},q_{y},q_{z})\nrepresents the orientation, while\nğ›€\n=\n(\np\n,\nq\n,\nr\n)\n\\bm{\\Omega}=(p,q,r)\nare the body angular rates. The variables\nğ\n=\n(\nÏ‰\n1\n,\nÏ‰\n2\n,\nÏ‰\n3\n,\nÏ‰\n4\n)\n\\bm{\\omega}=(\\omega_{1},\\omega_{2},\\omega_{3},\\omega_{4})\ndenote the angular velocities of the individual motors. The control input\nğ®\n\\mathbf{u}\nconsists of the motor commands applied to each rotor.\nThe equations of motion of the quadcopter are defined as:\nğ’‘\nË™\n\\displaystyle\\dot{\\bm{p}}\n=\nğ’—\n\\displaystyle=\\bm{v}\nğ’—\nË™\n\\displaystyle\\dot{\\bm{v}}\n=\nR\nâ€‹\n(\nğ’’\n)\nâ€‹\nğ‘­\n+\nğ’ˆ\n\\displaystyle=R(\\bm{q})\\bm{F}+\\bm{g}\nğ’’\nË™\n\\displaystyle\\dot{\\bm{q}}\n=\n1\n2\nâ€‹\nğ’’\nâŠ—\n[\n0\np\nq\nr\n]\nâŠ¤\n\\displaystyle=\\frac{1}{2}\\bm{q}\\otimes\\begin{bmatrix}0&p&q&r\\end{bmatrix}^{\\top}\nÎ©\nË™\n\\displaystyle\\dot{\\Omega}\n=\nğ‘´\n\\displaystyle=\\bm{M}\nÏ‰\nË™\n\\displaystyle\\dot{\\omega}\n=\n(\nğ\nğ’„\nâˆ’\nğ\n)\n/\nÏ„\n\\displaystyle=(\\bm{\\omega_{c}}-\\bm{\\omega})/\\tau\nHere\nR\nâ€‹\n(\nğ’’\n)\nR(\\bm{q})\nis the rotation matrix from body to world frame and\nğ’ˆ\n=\n[\n0\n,\n0\n,\ng\n]\nâŠ¤\n\\bm{g}=[0,0,g]^{\\top}\nis the gravitational acceleration vector.\nğ‘­\n=\n[\nD\nx\n,\nD\ny\n,\nT\n]\nâŠ¤\n\\bm{F}=[D_{x},D_{y},T]^{\\top}\ncontains the (specific) thrust and drag forces in the body frame. The models for these forces follow previous work  (\n?,â€Š?\n), but include two extensions motivated by data collected at higher flight speeds: a quadratic drag term\nv\nâ€‹\n|\nv\n|\nv|v|\n, and additional dependencies involving the advance ratio\nÎ¼\n\\mu\nand the angle off attack\nÎ±\n\\alpha\n(\n?\n). These effects become increasingly relevant at the high speeds reached with more aggressive policies.\nThe resulting expressions for the specific forces are\nğ‘­\n=\n[\nâˆ’\nk\nx\nâ€‹\nv\nx\nB\nâ€‹\nâˆ‘\ni\n=\n1\n4\nÏ‰\ni\nâˆ’\nk\nx\nâ€‹\n2\nâ€‹\nv\nx\nB\nâ€‹\n|\nv\nx\nB\n|\nâˆ’\nk\ny\nâ€‹\nv\ny\nB\nâ€‹\nâˆ‘\ni\n=\n1\n4\nÏ‰\ni\nâˆ’\nk\ny\nâ€‹\n2\nâ€‹\nv\ny\nB\nâ€‹\n|\nv\ny\nB\n|\nâˆ’\nk\nÏ‰\nâ€‹\n(\n1\n+\nk\nÎ±\nâ€‹\nÎ±\n+\nk\nhor\nâ€‹\nÎ¼\n)\nâ€‹\nâˆ‘\ni\n=\n1\n4\nÏ‰\ni\n2\n]\n\\bm{F}=\\begin{bmatrix}-k_{x}v^{B}_{x}\\sum_{i=1}^{4}\\omega_{i}-k_{x2}v^{B}_{x}|v^{B}_{x}|\\\\\n-k_{y}v^{B}_{y}\\sum_{i=1}^{4}\\omega_{i}-k_{y2}v^{B}_{y}|v^{B}_{y}|\\\\\n-k_{\\omega}\\left(1+k_{\\alpha}\\alpha+k_{\\text{hor}}\\mu\\right)\\sum_{i=1}^{4}\\omega_{i}^{2}\\end{bmatrix}\n(6)\nHere,\nğ¯\nB\n\\mathbf{v}^{B}\ndenotes the velocity\nğ¯\n\\mathbf{v}\nexpressed in the body frame. The angle of attack of the propeller blade is given by\nÎ±\n\\alpha\n, and\nÎ¼\n\\mu\nrepresents the effective advance ratio of the blade. These quantities are defined as follows:\nÎ±\n=\ntan\nâˆ’\n1\nâ¡\n(\nv\nz\nB\nr\nâ€‹\nÏ‰\nÂ¯\n)\nÎ¼\n=\ntan\nâˆ’\n1\nâ¡\n(\nv\nx\nB\nâ€‹\n2\n+\nv\ny\nB\nâ€‹\n2\nr\nâ€‹\nÏ‰\nÂ¯\n)\nwith\nÏ‰\nÂ¯\n=\nâˆ‘\ni\n=\n1\n4\nÏ‰\ni\n\\displaystyle\\alpha=\\tan^{-1}\\Big(\\frac{v^{B}_{z}}{r\\bar{\\omega}}\\Big)\\quad\\mu=\\tan^{-1}\\Big(\\frac{v^{B2}_{x}+v^{B2}_{y}}{r\\bar{\\omega}}\\Big)\\quad\\text{with}\\quad\\bar{\\omega}=\\sum_{i=1}^{4}\\omega_{i}\nwhere\nr\nr\nis the propeller radius\n4\n4\n4\nr\nr\nwas estimated to be\n0.0485775\n0.0485775\nand is not randomized.\n,\nÏ‰\ni\n\\omega_{i}\nare the individual motor angular velocities, and\nÏ‰\nÂ¯\n\\bar{\\omega}\nis the total rotational speed across all four motors.\nThe angular acceleration\nğ‘´\n=\n(\nM\nx\n\\bm{M}=(M_{x}\n,\nM\ny\nM_{y}\n,\nM\nz\n)\nM_{z})\naround the body axes are modeled as\nğ‘´\n=\n[\nâˆ’\nk\np\nâ€‹\n1\nâ€‹\nÏ‰\n1\n2\nâˆ’\nk\np\nâ€‹\n2\nâ€‹\nÏ‰\n2\n2\n+\nk\np\nâ€‹\n3\nâ€‹\nÏ‰\n3\n2\n+\nk\np\nâ€‹\n4\nâ€‹\nÏ‰\n4\n2\n+\nJ\nx\nâ€‹\nq\nâ€‹\nr\nâˆ’\nk\nq\nâ€‹\n1\nâ€‹\nÏ‰\n1\n2\n+\nk\nq\nâ€‹\n2\nâ€‹\nÏ‰\n2\n2\nâˆ’\nk\nq\nâ€‹\n3\nâ€‹\nÏ‰\n3\n2\n+\nk\nq\nâ€‹\n4\nâ€‹\nÏ‰\n4\n2\n+\nJ\ny\nâ€‹\np\nâ€‹\nr\nâˆ’\nk\nr\nâ€‹\n1\nâ€‹\nÏ‰\n1\n+\nk\nr\nâ€‹\n2\nâ€‹\nÏ‰\n2\n+\nk\nr\nâ€‹\n3\nâ€‹\nÏ‰\n3\nâˆ’\nk\nr\nâ€‹\n4\nâ€‹\nÏ‰\n4\nâˆ’\nk\nr\nâ€‹\n5\nâ€‹\nÏ‰\nË™\n1\n+\nk\nr\nâ€‹\n6\nâ€‹\nÏ‰\nË™\n2\n+\nk\nr\nâ€‹\n7\nâ€‹\nÏ‰\nË™\n3\nâˆ’\nk\nr\nâ€‹\n8\nâ€‹\nÏ‰\nË™\n4\n+\nJ\nz\nâ€‹\np\nâ€‹\nq\n]\n\\bm{M}=\\begin{bmatrix}-k_{p1}\\omega_{1}^{2}-k_{p2}\\omega_{2}^{2}+k_{p3}\\omega_{3}^{2}+k_{p4}\\omega_{4}^{2}+J_{x}qr\\\\\n-k_{q1}\\omega_{1}^{2}+k_{q2}\\omega_{2}^{2}-k_{q3}\\omega_{3}^{2}+k_{q4}\\omega_{4}^{2}+J_{y}pr\\\\\n-k_{r1}\\omega_{1}+k_{r2}\\omega_{2}+k_{r3}\\omega_{3}-k_{r4}\\omega_{4}-k_{r5}\\dot{\\omega}_{1}+k_{r6}\\dot{\\omega}_{2}+k_{r7}\\dot{\\omega}_{3}-k_{r8}\\dot{\\omega}_{4}+J_{z}pq\\end{bmatrix}\n(7)\nEach moment component is expressed as a weighted combination of squared rotor speeds, capturing how differential thrust produces roll, pitch, and yaw moments. Gyroscopic coupling terms are also included. These terms become increasingly important at high rotational rates due to the droneâ€™s elongated mass distributionâ€”with the battery mounted at the front and the Jetson Orin at the rearâ€”which introduces inertia asymmetry and makes the coupling terms non-negligible. The values of\nJ\nx\nJ_{x}\n,\nJ\ny\nJ_{y}\n, and\nJ\nz\nJ_{z}\nwere identified experimentally through a free-fall spinning throw of the drone without propellers. In this experiment, the gyroscope directly measures the cross-coupling effects, enabling estimation of the inertia-difference terms via linear regression.\nSimilar to  (\n?\n) the steady-state motor response\nğ\nc\n\\bm{\\omega}_{c}\nis modeled as\nÏ‰\nc\n,\ni\n=\n(\nÏ‰\nmax\nâˆ’\nÏ‰\nmin\n)\nâ€‹\nk\nâ€‹\nu\ni\n2\n+\n(\n1\nâˆ’\nk\nl\n)\nâ€‹\nu\ni\n+\nÏ‰\nmin\n\\displaystyle\\omega_{c,i}=(\\omega_{\\text{max}}-\\omega_{\\text{min}})\\sqrt{ku_{i}^{2}+(1-k_{l})u_{i}}+\\omega_{\\text{min}}\nThe nominal parameter values used in the quadcopter model are listed in Table\n1\n.\nIn simulation, these equations of motion are discretized using forward Euler integration with a time step of\nÎ”\nâ€‹\nt\n=\n0.01\nâ€‹\ns\n\\Delta t=0.01\\,\\text{s}\n.\nInitialization\nTo ensure broad coverage of possible flight conditions, we use two initialization schemes: one that places the drone on the ground near the first gate, and one that samples uniformly from the full flight volume.\nIn the\nground initialization\n, the droneâ€™s position is sampled around a nominal starting point\n(\n8\n,\nâˆ’\n22\n,\n0\n)\n(8,-22,0)\nwith uniform noise in the horizontal directions:\nx\n0\n,\ny\n0\nâˆ¼\nğ’°\nâ€‹\n(\nâˆ’\n0.5\n,\n0.5\n)\n+\n(\n8\n,\nâˆ’\n22\n)\n,\nz\n0\n=\n0\n.\nx_{0},y_{0}\\sim\\mathcal{U}(-0.5,0.5)+(8,-22),\\quad z_{0}=0.\nThe drone starts with zero linear and angular velocity:\nğ’—\n0\n=\nğŸ\n,\nÏ•\n0\n=\nÎ¸\n0\n=\n0\n,\n\\bm{v}_{0}=\\bm{0},\\quad\\phi_{0}=\\theta_{0}=0,\nÏˆ\n0\nâˆ¼\nğ’°\nâ€‹\n(\nâˆ’\nÏ€\n4\n,\nÏ€\n4\n)\n+\nÏˆ\ngate\n,\nğ›€\n0\n=\nğŸ\n,\nğ’–\n0\n=\nğŸ\n\\psi_{0}\\sim\\mathcal{U}\\left(-\\frac{\\pi}{4},\\frac{\\pi}{4}\\right)+\\psi_{\\text{gate}},\\quad\\bm{\\Omega}_{0}=\\bm{0},\\quad\\bm{u}_{0}=\\bm{0}\nIn the\nuniform initialization\n, the drone is randomly placed anywhere in the flight arena:\nx\n0\nâˆ¼\nğ’°\nâ€‹\n(\n1\n,\n95\n)\n,\ny\n0\nâˆ¼\nğ’°\nâ€‹\n(\nâˆ’\n27\n,\n1\n)\n,\nz\n0\nâˆ¼\nğ’°\nâ€‹\n(\nâˆ’\n5\n,\n0\n)\n,\nx_{0}\\sim\\mathcal{U}(1,95),\\quad y_{0}\\sim\\mathcal{U}(-27,1),\\quad z_{0}\\sim\\mathcal{U}(-5,0),\nğ’—\n0\nâˆ¼\nğ’°\nâ€‹\n(\nâˆ’\n0.5\n,\n0.5\n)\n3\n,\nÏ•\n0\n,\nÎ¸\n0\nâˆ¼\nğ’°\nâ€‹\n(\nâˆ’\nÏ€\n9\n,\nÏ€\n9\n)\n,\nÏˆ\n0\nâˆ¼\nğ’°\nâ€‹\n(\nâˆ’\nÏ€\n,\nÏ€\n)\n,\n\\bm{v}_{0}\\sim\\mathcal{U}(-0.5,0.5)^{3},\\quad\\phi_{0},\\theta_{0}\\sim\\mathcal{U}\\left(-\\frac{\\pi}{9},\\frac{\\pi}{9}\\right),\\quad\\psi_{0}\\sim\\mathcal{U}(-\\pi,\\pi),\nğ›€\n0\nâˆ¼\nğ’°\nâ€‹\n(\nâˆ’\n0.1\n,\n0.1\n)\n3\n,\nğ’–\n0\nâˆ¼\nğ’°\nâ€‹\n(\nâˆ’\n1\n,\n1\n)\n4\n.\n\\bm{\\Omega}_{0}\\sim\\mathcal{U}(-0.1,0.1)^{3},\\quad\\bm{u}_{0}\\sim\\mathcal{U}(-1,1)^{4}.\nThe target gate is chosen as the nearest gate located ahead of the drone, i.e., with the drone positioned behind its plane.\nDomain Randomization\nTo improve sim-to-real transfer, we apply parameter randomization at the beginning of each episode. The nominal parameters of the A2RL quadcopter are identified using linear regression on high-speed flight data (see Table\n1\n). Similar to the approach inÂ  (\n?\n), we randomize each parameter by a specified percentage around its nominal value. The level of randomization is selected based on a trade-off between robustness and performance. Table\n1\nlists the ranges used for the competition policies. In all policies except M16, each parameter is sampled independently. For this exception, yaw moment model parameters are randomized jointly:\nk\nr\nâ€‹\n1\n=\nk\nr\nâ€‹\n2\n=\nk\nr\nâ€‹\n3\n=\nk\nr\nâ€‹\n4\nâˆ¼\nğ’°\nâ€‹\n(\nmin\n,\nmax\n)\n,\nk\nr\nâ€‹\n5\n=\nk\nr\nâ€‹\n6\n=\nk\nr\nâ€‹\n7\n=\nk\nr\nâ€‹\n8\nâˆ¼\nğ’°\nâ€‹\n(\nmin\n,\nmax\n)\n.\nk_{r1}=k_{r2}=k_{r3}=k_{r4}\\sim\\mathcal{U}(\\mathrm{min},\\mathrm{max}),\\quad k_{r5}=k_{r6}=k_{r7}=k_{r8}\\sim\\mathcal{U}(\\mathrm{min},\\mathrm{max}).\nThe corresponding table entries for\nk\nr\nâ€‹\n2\nk_{r2}\nâ€“\nk\nr\nâ€‹\n4\nk_{r4}\nand\nk\nr\nâ€‹\n6\nk_{r6}\nâ€“\nk\nr\nâ€‹\n8\nk_{r8}\nare marked with â€™â€“â€™ to indicate equality rather than independent sampling.\nReward Function\nThe reward function balances task completion, flight smoothness, and perceptual robustness. All components are defined at each timestep\nk\nk\n. A detailed overview of the parameter values used in the expressions of the reward function can be found in Table\n1\n.\nThe\nprogress reward\nencourages movement toward the target gate:\nr\nprog\n,\nk\n=\nÎ»\nprog\nâ€‹\nmin\nâ¡\n(\nâ€–\nğ©\nk\nâˆ’\n1\nâˆ’\nğ©\ng\nk\nâ€–\nâˆ’\nâ€–\nğ©\nk\nâˆ’\nğ©\ng\nk\nâ€–\n,\nv\nmax\nâ€‹\nÎ”\nâ€‹\nt\n)\nr_{\\text{prog},k}=\\lambda_{\\text{prog}}\\min\\left(\\left\\|\\mathbf{p}_{k-1}-\\mathbf{p}_{g_{k}}\\right\\|-\\left\\|\\mathbf{p}_{k}-\\mathbf{p}_{g_{k}}\\right\\|,v_{\\max}\\Delta t\\right)\nwhere\nğ©\nk\n\\mathbf{p}_{k}\nis the droneâ€™s position and\nğ©\ng\nk\n\\mathbf{p}_{g_{k}}\nthe center of the current gate. This type of progress-based reward is widely used in reinforcement-learning approaches to drone racing  (\n?,â€Š?,â€Š?,â€Š?,â€Š?\n). To encourage safer behavior for slower policies, we cap the maximum achievable progress at\nv\nmax\nâ€‹\nÎ”\nâ€‹\nt\nv_{\\max}\\Delta t\n. This constraint limits the effective maximum speed of the policy and discourages overly aggressive or unsafe trajectories.\nThe\ngate reward\nprovides a bonus upon successfully passing a gate:\nr\ngate\n,\nk\n=\nÎ»\ngate\nif gate is passed, otherwise\nâ€‹\n0\n.\nr_{\\text{gate},k}=\\lambda_{\\text{gate}}\\quad\\text{if gate is passed, otherwise }0.\nThe\nangular rate penalty\ndiscourages high angular velocities:\np\nrate\n,\nk\n=\nÎ»\nrate\nâ€‹\nâ€–\nğ›€\nk\nâ€–\n2\n.\np_{\\text{rate},k}=\\lambda_{\\text{rate}}\\left\\|\\bm{\\Omega}_{k}\\right\\|^{2}.\nThe\ngate offset penalty\ndiscourages off-center gate crossings:\np\noffset\n,\nk\n=\nÎ»\noffset\nâ€‹\nâ€–\nğ©\nk\nâˆ’\nğ©\ng\nk\nâ€–\nif gate is passed, otherwise\nâ€‹\n0\n.\np_{\\text{offset},k}=\\lambda_{\\text{offset}}\\left\\|\\mathbf{p}_{k}-\\mathbf{p}_{g_{k}}\\right\\|\\quad\\text{if gate is passed, otherwise }0.\nThe\nperception penalty\nencourages the drone to keep the next gate within the forward-facing cameraâ€™s view:\np\nperc\n,\nk\n=\nÎ»\nperc\nâ€‹\nÎ¸\ncam\nif\nâ€‹\nÎ¸\ncam\n>\nÏ€\n/\n3\n,\notherwise\nâ€‹\n0\n,\np_{\\text{perc},k}=\\lambda_{\\text{perc}}\\,\\theta_{\\text{cam}}\\quad\\text{if }\\theta_{\\text{cam}}>\\pi/3,\\text{ otherwise }0,\nwhere\nÏ†\n\\varphi\nis the angle between the optical axis and the center of the next gate.\nThe\nmotor penalty\ndiscourages large changes in motor commands:\np\nÎ”\nâ€‹\nğ’–\n,\nk\n=\nÎ»\nÎ”\nâ€‹\nğ’–\nâ€‹\nâˆ‘\ni\n=\n0\n4\nmax\nâ¡\n(\n|\nu\ni\nâ€‹\n[\nk\n]\nâˆ’\nu\ni\nâ€‹\n[\nk\nâˆ’\n1\n]\n|\nâˆ’\nÎ”\nâ€‹\nu\nthresh\n,\n0\n)\n.\np_{\\Delta\\bm{u},k}=\\lambda_{\\Delta\\bm{u}}\\sum_{i=0}^{4}\\max\\left(\\left|u_{i}[k]-u_{i}[k-1]\\right|-\\Delta u_{\\text{thresh}},0\\right).\nThis penalty was introduced after observing that an early policy exhibited highly aggressive bangâ€“bang control, switching almost exclusively between minimum and maximum motor commands, which led to motor overheating.\nThe\nlow action penalty\ndiscourages low actions:\np\nğ’–\n,\nk\n=\nÎ»\nğ’–\nâ€‹\nâˆ‘\ni\n=\n0\n4\nmax\nâ¡\n(\n0.5\nâˆ’\nu\ni\n,\n0\n)\n.\np_{\\bm{u},k}=\\lambda_{\\bm{u}}\\sum_{i=0}^{4}\\max(0.5-u_{i},0).\nThis penalty was initially added to further reduce bangâ€“bang behavior and was used for one of the networks used in competition. However, in practice, the\nmotor penalty\nproved more effective, and it became our primary method for regularizing the control inputs.\nThe\ncrash penalty\napplies a fixed penalty when a collision occurs:\np\ncrash\n,\nk\n=\nÎ»\ncrash\nif collision, otherwise\nâ€‹\n0\n.\np_{\\text{crash},k}=\\lambda_{\\text{crash}}\\quad\\text{if collision, otherwise }0.\nThe total reward at timestep\nk\nk\nis:\nr\nk\n=\nr\nprog\n,\nk\n+\nr\ngate\n,\nk\nâˆ’\np\nrate\n,\nk\nâˆ’\np\noffset\n,\nk\nâˆ’\np\nperc\n,\nk\nâˆ’\np\nÎ”\nâ€‹\nğ’–\n,\nk\nâˆ’\np\nğ’–\n,\nk\nâˆ’\np\ncrash\n,\nk\n.\nr_{k}=r_{\\text{prog},k}+r_{\\text{gate},k}-p_{\\text{rate},k}-p_{\\text{offset},k}-p_{\\text{perc},k}-p_{\\Delta\\bm{u},k}-p_{\\bm{u},k}-p_{\\text{crash},k}.\nCollision detection and gate passing\nIn simulation, the drone is modeled as a point mass. The gateâ€™s inner size varies during training and is denoted by\ng\nsize\ng_{\\mathrm{size}}\n, while the outer size is fixed at\n2.7\nâ€‹\nm\n2.7\\,\\mathrm{m}\nand the gate thickness at\ng\nthickness\ng_{\\mathrm{thickness}}\n. All gates share the same inner size, except during the training of policy M16, where gates 2 and 6 were reduced to\n0.45\nâ€‹\nm\n0.45\\,\\mathrm{m}\n. Additionally, the gate thickness was reduced to\n0.8\n,\nm\n0.8,\\mathrm{m}\nfor policy M16.\nA gate collision is registered when the drone intersects the gateâ€™s collision box or crosses the gate plane outside the inner square opening (i.e., more than\ng\nsize\n/\n2\ng_{\\mathrm{size}}/2\nfrom the center along either axis). A ground collision occurs when\nz\n<\nâˆ’\nh\nground\nz<-h_{\\mathrm{ground}}\nwhile the droneâ€™s speed exceeds\nv\nground\nv_{\\mathrm{ground}}\n. An out-of-bounds event occurs when\nx\nâˆ‰\n[\n1\n,\n95\n]\nx\\notin[1,95]\nor\ny\nâˆ‰\n[\nâˆ’\n27\n,\n1\n]\ny\\notin[-27,1]\n, or when the angular velocity exceeds\n1700\nâ€‹\ndeg\n/\ns\n1700\\,\\mathrm{deg/s}\n.\nIn the reinforcement learning setup, an episode is terminated whenever a gate collision or an out-of-bounds event occurs.\nPolicy and Training\nThe selected neural policy is a three-layer fully connected network with ReLU activation functions and 64 neurons per layer (See Fig\n5\n). The policy takes in 24 observations, including the quadcopterâ€™s state and information about current and future gates, similar to prior workÂ  (\n?\n):\nğ’™\nobs\n=\n[\nğ’‘\ng\ni\n,\nğ’—\ng\ni\n,\nğš½\ng\ni\n,\nğ›€\n,\nğ\n,\nğ’‘\ng\ni\n+\n1\ng\ni\n,\nÏˆ\ng\ni\n+\n1\ng\ni\n]\nT\n\\bm{x}_{\\text{obs}}=[\\bm{p}^{g_{i}},\\bm{v}^{g_{i}},\\bm{\\Phi}^{g_{i}},\\bm{\\Omega},\\bm{\\omega},\\bm{p}_{g_{i+1}}^{g_{i}},\\psi_{g_{i+1}}^{g_{i}}]^{T}\nHere,\ng\ni\ng_{i}\ndenotes the reference frame of the\ni\ni\n-th gate. The vector\nğ’‘\ng\ni\n\\bm{p}^{g_{i}}\nis the position in the current gate frame,\nğ’—\ng\ni\n\\bm{v}^{g_{i}}\nthe velocity,\nğš½\n=\n(\nÏ•\n,\nÎ¸\n,\nÏˆ\n)\n\\bm{\\Phi}=(\\phi,\\theta,\\psi)\nthe Euler angles,\nğ›€\n\\bm{\\Omega}\nthe angular velocity in the world frame, and\nğ\n\\bm{\\omega}\nthe angular velocity in the body frame. The components\nğ’‘\ng\ni\n+\n1\ng\ni\n\\bm{p}_{g_{i+1}}^{g_{i}}\nand\nÏˆ\ng\ni\n+\n1\ng\ni\n\\psi_{g_{i+1}}^{g_{i}}\nrepresent the relative position and yaw angle of the next gate, expressed in the current gate frame.\nAlthough the simulator uses quaternions internally, we represent orientation using Euler angles in the observation vector. The network outputs four motor commands\nğ’–\n\\bm{u}\n.\nThe quadcopter model, initialization scheme, domain randomization, and reward function described above are used to construct a custom Gym environment for training. We train the policy using the Proximal Policy Optimization (PPO) algorithmÂ  (\n?\n), implemented via the Stable-Baselines3 libraryÂ  (\n?\n). Table\n1\nprovides an overview of the reward function parameters, collision detection settings, PPO training parameters, and domain randomization used for the competition policies.\nReferences and Notes\nAcknowledgments\nWe gratefully acknowledge the many participants of previous races whose dedication and efforts laid the foundation for this work. Their contributions created the knowledge base and inspiration that made further progress possible. We also sincerely thank ADR and AIRR for their help in pushing this field of research. Finally, we would like to thank A2RL and DCL for this opportunity.\nFunding:\nParts of this work, in particular, the development of the RL-based control, were performed with the support of â€˜SPEARâ€™ project nr 101119774 under HORIZON-CL4-2022-DIGITAL-EMERGING-02-06.",
    "preview_text": "Autonomous drone racing represents a major frontier in robotics research. It requires an Artificial Intelligence (AI) that can run on board light-weight flying robots under tight resource and time constraints, while pushing the physical system to its limits. The state of the art in this area consists of a system with a stereo camera and an inertial measurement unit (IMU) that beat human drone racing champions in a controlled indoor environment. Here, we present MonoRace: an onboard drone racing approach that uses a monocular, rolling-shutter camera and IMU that generalizes to a competition environment without any external motion tracking system. The approach features robust state estimation that combines neural-network-based gate segmentation with a drone model. Moreover, it includes an offline optimization procedure that leverages the known geometry of gates to refine any state estimation parameter. This offline optimization is based purely on onboard flight data and is important for fine-tuning the vital external camera calibration parameters. Furthermore, the guidance and control are performed by a neural network that foregoes inner loop controllers by directly sending motor commands. This small network runs on the flight controller at 500Hz. The proposed approach won the 2025 Abu Dhabi Autonomous Drone Racing Competition (A2RL), outperforming all competing AI teams and three human world champion pilots in a direct knockout tournament. It set a new milestone in autonomous drone racing research, reaching speeds up to 100 km/h on the competition track and successfully coping with problems such as camera interference and IMU saturation.\n\nMonoRace: Winning Champion-Level Drone\nRacing with Robust Monocular AI\nStavrow A. Bahnam\nâ€ \n,\nRobin Ferede\nâˆ—,â€ \n,\nTill M. Blaha,\nAnton E. Lang\nErin Lucassen,\nQuentin Missinne,\nAderik E.C. Verraest\nChristophe De Wagter,\nGuido C.H.E. de Croon\nControl & Operation, Delft University of Technology, Delft & 2629 HS, The Netherlands.\nâˆ—\nCorres",
    "is_relevant": true,
    "relevance_score": 5.0,
    "extracted_keywords": [
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "MonoRace æ˜¯ä¸€ç§åŸºäºå•ç›®æ‘„åƒå¤´å’ŒIMUçš„è‡ªä¸»æ— äººæœºç«é€ŸAIç³»ç»Ÿï¼Œé€šè¿‡ç¥ç»ç½‘ç»œå®ç°çŠ¶æ€ä¼°è®¡å’Œç›´æ¥ç”µæœºæ§åˆ¶ï¼Œåœ¨æ¯”èµ›ä¸­å‡»è´¥äººç±»å† å†›ï¼Œå±•ç¤ºäº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„é«˜æ•ˆè¿åŠ¨æ§åˆ¶èƒ½åŠ›ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T17:53:29Z",
    "created_at": "2026-01-27T15:53:23.660700",
    "updated_at": "2026-01-27T15:53:23.660707",
    "recommend": 0
}