{
    "id": "2601.22545v1",
    "title": "Adapting Reinforcement Learning for Path Planning in Constrained Parking Scenarios",
    "authors": [
        "Feng Tao",
        "Luca Paparusso",
        "Chenyi Gu",
        "Robin Koehler",
        "Chenxu Wu",
        "Xinyu Huang",
        "Christian Juette",
        "David Paz",
        "Ren Liu"
    ],
    "abstract": "åœ¨å—é™ç¯å¢ƒä¸­è¿›è¡Œå®æ—¶è·¯å¾„è§„åˆ’ä»ç„¶æ˜¯è‡ªä¸»ç³»ç»Ÿé¢ä¸´çš„ä¸€é¡¹åŸºç¡€æ€§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ç»å…¸è§„åˆ’å™¨è™½ç„¶åœ¨ç†æƒ³æ„ŸçŸ¥å‡è®¾ä¸‹æœ‰æ•ˆï¼Œä½†å¯¹å®é™…æ„ŸçŸ¥çº¦æŸå¾€å¾€è¾ƒä¸ºæ•æ„Ÿï¼Œä¸”ä¾èµ–åœ¨çº¿æœç´¢è¿‡ç¨‹ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚åœ¨å¤æ‚ç¯å¢ƒä¸­ï¼Œè¿™ä½¿å…¶éš¾ä»¥å®ç°å®æ—¶éƒ¨ç½²ã€‚ä¸ºå…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºåœè½¦åœºæ™¯å®æ—¶è·¯å¾„è§„åˆ’çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨é‚£äº›éœ€è¦é¢‘ç¹å€’è½¦å’Œè°ƒæ•´çš„ç‹­çª„ç©ºé—´æŒ‘æˆ˜æ€§åœºæ™¯ã€‚ä¸ç»å…¸è§„åˆ’å™¨ä¸åŒï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆæ— éœ€ç†æƒ³åŒ–ã€ç»“æ„åŒ–çš„æ„ŸçŸ¥è¾“å…¥ï¼ŒåŸåˆ™ä¸Šå¯é¿å…å¯¹å®šä½ä¸è·Ÿè¸ªç­‰é™„åŠ æ¨¡å—çš„ä¾èµ–ï¼Œä»è€Œå®ç°æ›´ç®€æ´å®ç”¨çš„éƒ¨ç½²ã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼Œç­–ç•¥é€šè¿‡å•æ­¥å‰å‘ä¼ æ’­ç”ŸæˆåŠ¨ä½œï¼Œå…¶è½»é‡åŒ–ç‰¹æ€§è¶³ä»¥æ»¡è¶³å®æ—¶éƒ¨ç½²éœ€æ±‚ã€‚æˆ‘ä»¬å°†è¯¥ä»»åŠ¡å»ºæ¨¡ä¸ºåŸºäºè‡ªè¡Œè½¦æ¨¡å‹åŠ¨åŠ›å­¦çš„åºåˆ—å†³ç­–é—®é¢˜ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨é—­ç¯è®¾ç½®ä¸­ç›´æ¥å­¦ä¹ ç¬¦åˆè½¦è¾†è¿åŠ¨å­¦ä¸ç¯å¢ƒçº¦æŸçš„å¯¼èˆªç­–ç•¥ã€‚æˆ‘ä»¬å¼€å‘äº†æ¶µç›–å¤šæ ·åŒ–æŒ‘æˆ˜åœºæ™¯çš„æ–°åŸºå‡†æµ‹è¯•é›†ä»¥æ”¯æŒè®­ç»ƒä¸è¯„ä¼°ã€‚è¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æˆåŠŸç‡å’Œæ•ˆç‡ï¼Œè¾ƒç»å…¸è§„åˆ’å™¨åŸºçº¿æˆåŠŸç‡æå‡96%ï¼Œæ•ˆç‡æå‡52%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†åŸºå‡†æµ‹è¯•é›†ä½œä¸ºå¼€æºèµ„æºå‘å¸ƒï¼Œä»¥ä¿ƒè¿›è‡ªä¸»ç³»ç»Ÿé¢†åŸŸçš„åç»­ç ”ç©¶ã€‚ç›¸å…³èµ„æºåŠé…å¥—å·¥å…·å¯é€šè¿‡https://github.com/dqm5rtfg9b-collab/Constrained_Parking_Scenariosè·å–ã€‚",
    "url": "https://arxiv.org/abs/2601.22545v1",
    "html_url": "https://arxiv.org/html/2601.22545v1",
    "html_content": "Adapting Reinforcement Learning for Path Planning in Constrained Parking Scenarios\nFeng Tao, Luca Paparusso, Chenyi Gu, Robin Koehler, Chenxu Wu, Xinyu Huang, Christian Juette\nDavid Paz, Ren Liu\nBosch Research\nCorresponding author:\ndavid.pazruiz@us.bosch.com\nAbstract\nReal-time path planning in constrained environments remains a fundamental challenge for autonomous systems. Traditional classical planners, while effective under perfect perception assumptions, are often sensitive to real-world perception constraints and rely on online search procedures that incur high computational costs. In complex surroundings, this renders real-time deployment prohibitive.\nTo overcome these limitations, we introduce a Deep Reinforcement Learning (DRL) framework for real-time path planning in parking scenarios. In particular, we focus on challenging scenes with tight spaces that require a high number of reversal maneuvers and adjustments. Unlike classical planners, our solution does not require ideal and structured perception, and in principle, could avoid the need for additional modules such as localization and tracking, resulting in a simpler and more practical implementation. Also, at test time, the policy generates actions through a single forward pass at each step, which is lightweight enough for real-time deployment. The task is formulated as a sequential decision-making problem grounded in a bicycle model dynamics, enabling the agent to directly learn navigation policies that respect vehicle kinematics and environmental constraints in the closed-loop setting. A new benchmark is developed to support both training and evaluation, capturing diverse and challenging scenarios. Our approach achieves state-of-the-art success rates and efficiency, surpassing classical planner baselines by\n+96%\nin success rate and\n+52%\nin efficiency.\nFurthermore, we release our benchmark as an open-source resource for the community to foster future research in autonomous systems. The benchmark and accompanying tools are available at\nhttps://github.com/dqm5rtfg9b-collab/Constrained_Parking_Scenarios\n.\n1\nIntroduction\nClassical path planners, such as Hybrid A*\n(Dolgov\net al.\n,\n2008\n)\n, have long been widely used in autonomous systems to compute feasible trajectories. Given precise and complete perception observations, these methods can generate near-optimal\n1\n1\n1\nIn principle, Hybrid A* can recover the globally optimal path if allowed sufficient search time. However, in practice it is often combined with heuristic shortcuts such as Reeds-Shepp curves to accelerate search, which yields a solution but not truly the optimal one.\npaths for tasks such as autonomous parking. However, in real-world scenarios, perception is inherently uncertain and often occluded in tight spaces, leading to brittle plans. For instance, as shown in Figure\n1\n, paths computed under partial observability may result in unavoidable collisions. Moreover, classical planners do not retain prior knowledge beyond simple heuristics, causing them to repeatedly search for solutions online. This introduces significant risk of exceeding onboard computational limits, particularly in complex surroundings. Finally, the integration of classical planners into a full autonomy stack requires additional modulesâ€”such as localization and path trackingâ€”that themselves introduce uncertainty and compounding errors across the system. These limitations motivate us to explore alternative approaches for solving the path planning task in constrained environments.\n(a)\nDead-end parking\n(b)\nCorridor parking\nFigure 1:\nNon-optimal paths generated by the Hybrid A* planner in constrained parking environments. The start pose is denoted by magenta rounded rectangles, and the target pose by cyan rounded rectangles. Planned paths are shown in blue, with shaded gray regions indicating the space occupied by the vehicle along its intermediate poses. Due to partial observability, the current solutions for both cases are likely to result in collisions (obstacles are denoted by solid red lines).\nRecent advances in machine learning have inspired AI-based solutions for path planning\n(Jiang\net al.\n,\n2023\n; Lazzaroni\net al.\n,\n2023\n; Chi\net al.\n,\n2023\n; Yang\net al.\n,\n2024\n; Zheng\net al.\n,\n2025\n)\n. Broadly, these approaches can be categorized into open-loop and closed-loop training paradigms. Open-loop methods, such as supervised imitation learning\n(Ahn\net al.\n,\n2022\n)\n, are simple to implement but prone to distribution shift, limiting their generalizability to unseen scenarios. They also do not explicitly enforce that predicted paths are dynamically feasible and trackable, particularly for challenging parking maneuvers. Closed-loop training, by contrast, directly accounts for sequential decision-making and vehicle feedback, thereby improving robustness and generalization. Yet, closed-loop learning for constrained path planning remains underexplored,\nespecially in the autonomous driving field\n, largely due to (1) the absence of a standardized benchmark that reflects the tight spatial conditions encountered in practice, and (2) the challenge of designing RL pipelines, where the reward function and training strategy must be carefully tuned.\nIn this work, we propose a Reinforcement Learning-based planner to address the limitations of both classical and existing AI-driven approaches. In particular, we formulate path planning as a sequential decision-making problem under a bicycle model dynamics, enabling the planner to explicitly respect kinematic constraints. We develop our own RL training strategy with curriculum learning and balance effective exploration with precise vehicle control by adopting an action-chunking mechanism\n(Li\net al.\n,\n2025\n)\n. To support both training and evaluation, we construct a benchmark, named as\nParkBench\n, tailored to constrained scenarios and build a simulation environment that leverages this benchmark for closed-loop interactions. Our approach achieves the state-of-the-art performance on the proposed benchmark, significantly surpassing classical planner baselines in a large margin.\nOverall, our key contributions are summarized below:\nâ€¢\nWe formulate the path planning problem as a reinforcement learning task grounded in a bicycle model dynamics, and provide a detailed design methodology.\nâ€¢\nWe propose an action chunking wrapper as a mechanism to reconcile accurate movement control with effective RL exploration.\nâ€¢\nWe achieve state-of-the-art results on constrained path planning and release our benchmark,\nParkBench\n, as an open-source dataset to foster future research in this direction.\n2\nRelated Works\n2.1\nClassical Path Planners\nClassical planners form the foundation of autonomous navigation and parking systems. Among them, Hybrid A* is one of the most widely adopted algorithms, combining grid-based search with continuous state interpolation to ensure feasible trajectories under vehicle kinematics. In principle, Hybrid A* can recover the optimal path with sufficient search time, but in practice, it is accelerated through heuristic shortcuts such as Reeds-Shepp curves\n(Reeds and Shepp,\n1990\n)\n, yielding near-optimal solutions that are computationally tractable. In our work, we adopt a publicly available Hybrid A* implementation\n2\n2\n2\nhttps://github.com/AtsushiSakai/PythonRobotics/tree/master/PathPlanning/HybridAStar.\n(Sakai\net al.\n,\n2018\n)\nas a strong baseline, ensuring a fair comparison between reinforcement learning-based and classical planning approaches.\n2.2\nAI-based Planning Approaches\nRecent years have seen growing interest in leveraging learning-based methods for path planning. For instance, VAD\n(Jiang\net al.\n,\n2023\n)\npredicts a sequence of future waypoints conditioned on scene context, achieving good performance on the nuScenes benchmark\n(Caesar\net al.\n,\n2020\n)\n. However, its training follows the open-loop paradigm and thus suffers from covariate shift, where compounding errors lead to distribution drift at test time, and they cannot guarantee dynamically trackable paths in complex maneuvers such as parking. Another family of supervised approaches leverages diffusion models for trajectory generation. Examples such as Diffusion Policy\n(Chi\net al.\n,\n2023\n)\ndemonstrate strong generative capabilities, but generated paths must be explicitly constrained to ensure trackability, and they require large-scale expert demonstrations for training.\nIn contrast, closed-loop reinforcement learning (RL) approaches train agents through trial-and-error interactions in simulated environments, directly accounting for sequential decision-making. While promising, existing RL studies\n(Lazzaroni\net al.\n,\n2023\n; Al-Mousa\net al.\n,\n2025\n)\non parking remain limited by overly simplified environments and do not address the tight constrained spaces that characterize realistic parking scenarios. This gap highlights the need for more challenging benchmarks and robust learning methods that can generalize beyond toy settings.\nDeep reinforcement learning (DRL) has been widely explored in mobile robot navigation\n(Zhu and Zhang,\n2021\n)\n. However, these works are not directly applicable to the parking task we study in this work. Most navigation methods\n(PÃ©rez-Dâ€™Arpino\net al.\n,\n2021\n; Ruan\net al.\n,\n2019\n; Xu\net al.\n,\n2022\n; Akmandor\net al.\n,\n2022\n)\nassume differential-drive robots with highly flexible motion capabilities, whereas parking requires vehicle modeling governed by nonholonomic constraints such as the bicycle or the Ackermann-steering models. These kinematic models restrict maneuverability. A related study uses an RC-car platform and combines model-free and model-based RL for indoor navigation\n(Kahn\net al.\n,\n2018\n)\n. Despite these efforts, navigation goals are typically treated as waypoints without enforcing precise final orientation, while parking requires exact terminal conditions. To the best of our knowledge, few learning-based methods jointly consider these constraints, motivating the development of our RL-based parking planner.\n2.3\nCombining Classical and Learning-based Methods\nAnother active line of research integrates classical planners with machine learning techniques to combine the strengths of both paradigms. Recent works (e.g.,\n(Shan\net al.\n,\n2023\n; Jiang\net al.\n,\n2025\n)\n) use learned models to guide search, accelerate tree expansion, or provide better heuristics for classical planners. These hybrid approaches hold promise for balancing efficiency and generalizability. However, such methods remain outside the scope of this work, as our focus is on demonstrating the viability of a purely reinforcement learning-based planner in constrained path planning scenarios. We view the integration of RL with classical heuristics as a valuable direction for future research.\n2.4\nParking Evaluation Benchmark\nTo the best of the authorsâ€™ knowledge, there are few practical benchmarks available for evaluating path planners, particularly in constrained parking scenarios. Among the limited existing attempts, the E2E Parking benchmark\n(Yang\net al.\n,\n2024\n)\nleveraged CARLA\n(Dosovitskiy\net al.\n,\n2017\n)\nto create a parking simulation environment, but its task setting is restricted to rear-in perpendicular parking in wide open spaces. Another notable effort is the TPCAP benchmark\n(Li\net al.\n,\n2022\n)\n, designed for an autonomous parking competition and consisting of 20 parking challenge cases. However, TPCAP represents obstacles as solid shapes and focuses solely on planning, which makes its formulation incompatible with existing autonomous driving pipelines. Moreover, the scenarios in TPCAP are overly simplified and not representative of realistic real-world conditions.\n3\nMethodology\nIn this section, we present a reinforcement learning (RL) framework for path planning in constrained parking scenarios, designed as a drop-in replacement for the Hybrid A* module in the autonomous driving pipeline. This design choice ensures compatibility with existing autonomy stacks and enables a fair comparison against a strong classical baseline as well. Our methodology is organized into five components. We first formulate the parking problem as a sequential decision-making task under vehicle kinematics. Next, we describe the input representation, which mirrors Hybrid A* to maintain pipeline consistency, along with our strategy to address the resulting training challenges. We then introduce our benchmark and simulation environment, followed by detailed training objective and reward design. Finally, we introduce a plug-in action-chunking mechanism that balances exploration efficiency with maneuver precision.\n(a)\nOne example of the non-feasible initial pose (blocked by a wall).\n(b)\nInitial poses generated by our rollout method for scenario (a).\nFigure 2:\nChallenges in spawning ego initial poses in the sparse obstacle representation environment and our rollout solution. Arrows in (b) represent the heading directions, respectively.\n3.1\nProblem Formulation\nWe formulate the constrained path planning task as a sequential decision-making problem. The vehicle state is defined as\n(\nx\n,\ny\n,\nÎ¸\n,\nÎ´\n)\n(x,\\,y,\\,\\theta,\\,\\delta)\n, representing the 2D position of the rear-axle center, the heading angle, and the front wheel steering angle, respectively. The environment provides obstacle information in the form of contour pointsâ€™ 2D coordinates, such as in lidar scans, of the type\n(\nx\no\nâ€‹\nb\nâ€‹\ns\n,\n1\n,\ny\no\nâ€‹\nb\nâ€‹\ns\n,\n1\n,\nâ€¦\n,\nx\no\nâ€‹\nb\nâ€‹\ns\n,\nN\n,\ny\no\nâ€‹\nb\nâ€‹\ns\n,\nN\n)\n(x^{obs,1},\\,y^{obs,1},...,\\,x^{obs,N},\\,y^{obs,N})\n, where\nN\nN\nis the maximum number of obstacle points considered. A target parking pose\n(\nx\ng\nâ€‹\no\nâ€‹\na\nâ€‹\nl\n,\ny\ng\nâ€‹\no\nâ€‹\na\nâ€‹\nl\n,\nÎ¸\ng\nâ€‹\no\nâ€‹\na\nâ€‹\nl\n)\n(x^{goal},\\,y^{goal},\\,\\theta^{goal})\nis also provided. Here, all the coordinates are expressed in world frame.\nAt each iteration, the simulator updates the pose of the vehicle by executing the selected 1-step control action through a kinematic bicycle model\n(Rajamani,\n2006\n)\n, a common abstraction for autonomous driving applications. This ensures that the learned policy respects nonholonomic constraints. The control space is discrete, and consists of two components (\nÎ”\nâ€‹\ns\n\\Delta s\n,\nâ–³\nâ€‹\nÎ´\n\\triangle\\delta\n):\n1.\ndisplacement along longitudinal axis\nÎ”\nâ€‹\ns\nâˆˆ\n{\n+\nd\nâ€‹\ns\n,\nâˆ’\nd\nâ€‹\ns\n,\n0\n}\n\\Delta s\\in\\{+ds,\\,-ds,\\,0\\}\n, representing forward, backward, or no motion with distance\nd\nâ€‹\ns\n>\n0\nds>0\nin meters,\n2.\nfront wheel steering change\nâ–³\nâ€‹\nÎ´\nâˆˆ\n{\n+\nd\nâ€‹\nÎ´\n,\nâˆ’\nd\nâ€‹\nÎ´\n,\n0\n}\n\\triangle\\delta\\in\\{+d\\delta,\\,-d\\delta,\\,0\\}\n, representing left, right, or no change in radians. When the vehicle does not move, left and right steering changes are possible.\nThe ego state is therefore updated via the discrete-space bicycle model as:\nx\nk\n+\n1\n\\displaystyle x_{k+1}\n=\nx\nk\n+\nÎ”\nâ€‹\ns\nk\nâ‹…\ncos\nâ¡\n(\nÎ¸\nk\n)\n,\n\\displaystyle=x_{k}+\\Delta s_{k}\\cdot\\cos(\\theta_{k}),\n(1)\ny\nk\n+\n1\n\\displaystyle y_{k+1}\n=\ny\nk\n+\nÎ”\nâ€‹\ns\nk\nâ‹…\nsin\nâ¡\n(\nÎ¸\nk\n)\n,\n\\displaystyle=y_{k}+\\Delta s_{k}\\cdot\\sin(\\theta_{k}),\nÎ¸\nk\n+\n1\n\\displaystyle\\theta_{k+1}\n=\nÎ¸\nk\n+\nÎ”\nâ€‹\ns\n/\nW\nB\nâ‹…\ntan\nâ¡\n(\nÎ´\nk\n)\n,\n\\displaystyle=\\theta_{k}+\\Delta s/W_{B}\\cdot\\tan(\\delta_{k}),\nÎ´\nk\n\\displaystyle\\delta_{k}\n=\nÎ´\nk\nâˆ’\n1\n+\nÎ”\nâ€‹\nÎ´\nk\n,\n\\displaystyle=\\delta_{k-1}+\\Delta\\delta_{k},\nwhere\nW\nB\nW_{B}\ndenotes the vehicle wheelbase in meters and subscript\nk\nk\ndenotes the iteration. Iteratively applying these updates will produce the complete planned path (waypoint sequence).\nThe planning objective is to generate a feasible action sequence that drives the vehicle from its initial state to the target parking pose without collisions, while make sure the derived path is reasonable (we will quantify the quality of the path in subsection\n3.4\n).\n3.2\nInput Representation\nTo ensure fairness in comparison and maintain pipeline consistency, we design the input to our RL planner to match the information used by the Hybrid A* baseline. Specifically, the input includes the egoâ€™s current pose, the target pose, and obstacle contour features extracted from the environment. While this ensures comparability, it also introduces additional challenges for RL: unlike Hybrid A*, the RL training process must handle diverse ego initializations, and the sparse obstacle representation (given as obstacle contours) makes certain spawn positions particularly problematic. The key issue is that collision and feasibility checks cannot be reliably performed for the initial pose using only sparse contours, which can result in infeasible configurations such as the ego starting outside a wall or in positions with no valid path into the parking space (as shown in Figure\n2(a)\n).\nTo address this challenge, we employ a roll-out function that gradually drives the ego away from the target pose using the bicycle model, with perturbations added to the heading for diversity. This procedure guarantees that the sampled initial poses are feasible, effectively reducing variance in training and improving convergence. An illustration of the sampled initial poses is shown in Figure\n2(b)\n.\nFigure 3:\nOur feature extractor architecture for vectorized environment observations.\nThe sparse contour representation of obstacles originates from the design of the existing pipeline, where Hybrid A* performs collision checks during its search from a given feasible starting pose to the target parking spot. Since Hybrid A* only requires obstacle boundaries for this purpose, obstacles are encoded as contours rather than dense occupancy maps or volumetric representations. To process this observation space effectively for the RL planner, we propose a feature extractor (Figure\n3\n) that employs cross-attention to force the ego to attend to obstacle information. All coordinates are transformed into the ego-centric coordinate system at each time step (such that the ego is always located at the origin) and normalized before being passed to the feature extractor. We also impose a finite horizon range on the input to mimic the sensing limits of a perception module. This design choice makes the setting more realistic for deployment and could, in future work, help reduce reliance on additional downstream modules such as localization and path tracking, since the RL agent directly outputs control commands. It may also provide robustness to perception noise in the first frame and open the door to handling dynamic obstacles (e.g., moving vehicles or pedestrians). While these aspects are beyond the scope of this work, our input design highlights the potential of RL-based planners to integrate seamlessly into more complex real-world scenarios.\nIt is worth mentioning that our feature extractor is intentionally lightweight to ensure feasibility for real-time, on-device deployment, which is a key requirement of practical parking systems.\n3.3\nBenchmark and Simulation Environment\nThe missing of proper benchmarks for parking evaluation motivate the development of our benchmark,\nParkBench\n, which is specifically tailored to constrained parking scenarios. Each scenario specifies the egoâ€™s initial pose, the target pose, and the positions of obstacles (contours) that define tight maneuvering spaces. Our current\nParkBench\nincludes 51 set of scenario layouts (all extracted from real-world dataset) for rear-in parking tasks, ranging from narrow aisles to occluded corner spots, reflecting the challenges of real-world parking. Detailed layouts are provided in Appendix\nG\n.\nBased on this benchmark, we build a simulation environment that follows the Gym interface, ensuring compatibility with standard RL libraries. The environment is initialized by loading one of the benchmark scenarios, after which the RL agent can interact with it and evolve through sequential actions (see Figure\n4\nfor an overview of our closed-loop method). Our simulator design is similar in spirit to\n(Scheel\net al.\n,\n2022\n)\n, which was developed for closed-loop training in autonomous driving. Note here, the environment state updates are computed under the bicycle model with the simplifying assumption that no dynamic obstacles are present,\ni.e.\n, only static obstacles are considered.\nFigure 4:\nOverview of our closed-loop path generation method. The simulator is initialized with a realistic parking scenario, and the environment is iteratively updated based on the RL policy. This framework enables both training the policy and extracting planned paths during inference.\n3.4\nRL Training with Curriculum Learning\nOnce the simulator is available, we can train the RL policy by interacting with the environment in a closed-loop manner. We adopt Stable Baselines3 (SB3)\n(Raffin\net al.\n,\n2021\n)\nas the training framework and use Proximal Policy Optimization (PPO)\n(Schulman\net al.\n,\n2017\n)\nas the base algorithm.\nThe key to successful RL training lies in the design of the reward function. However, designing dense rewards for parking tasks is challenging, and ill-defined reward functions often lead to unintended behaviors. To address this, we adopt a sparse reward formulation instead. The reward function consists of the following components:\nâ€¢\nGoal achievement\n: a positive reward is given if the ego reaches the target pose within tolerance.\nâ€¢\nCollision penalty\n: a negative reward is applied if the ego collides with any obstacle contour.\nâ€¢\nOut-of-bounds penalty\n: a negative reward if it moves too far away from the valid maneuvering space.\nâ€¢\nIdle penalty\n: a small negative reward to discourage the agent from remaining idle.\nâ€¢\nDirection-change penalty\n: a small negative reward to penalize gear changes (switching between forward and backward) for smooth paths.\nâ€¢\nTime penalty\n: a small negative reward applied at each step to incentivize faster completion.\nOf course, sparse rewards will make it difficult for an RL agent to learn, especially in complicated tasks. To make training more effective, we integrate this sparse reward design with curriculum learning\n(Florensa\net al.\n,\n2017\n)\nby gradually increasing scenario difficulty: starting from initial poses close to the target, then progressively moving further away from the target plus heading angle perturbation (refer Figure\n2(b)\n). This progression helps the agent first acquire basic maneuvering skills before tackling the full complexity of constrained parking tasks. It also reduces unsafe or wasteful exploration: early stages restrict initial conditions to feasible neighborhoods (near the target with small heading perturbations), keeping rollouts within valid free space and mitigating collisions and feasibility violations. As the difficulty increases, the agent gradually expands its coverage while retaining a learned prior over valid configurations.\nWith this sparse reward design and curriculum learning strategy, policies can be trained end-to-end within our simulation environment, producing agents capable of executing collision-free parking maneuvers in tightly constrained spaces.\n3.5\nAction Chunking for Efficient Learning\nThe default setting for RL algorithms is to select and execute one primitive action at a time. However, this setting is not well suited to parking tasks in constrained spaces. Training RL agents in such environments requires balancing the trade-off between exploration efficiency and precise movement control. Fine-grained primitive actions (\ne.g.\n, small steering adjustments) enable accurate maneuvering but make exploration highly inefficient due to long horizons. Conversely, coarse actions improve exploration efficiency but reduce maneuver precision, often leading to collisions.\nTo address this challenge, we adopt an action chunking mechanism, inspired by a recent work on Q-chunking\n(Li\net al.\n,\n2025\n)\n. In our formulation, a chunk corresponds to a short sequence of low-level control commands executed as a single macro-action. This reduces the effective planning horizon while preserving sufficient control fidelity, enabling efficient exploration without sacrificing maneuver precision. Different from the Q-chunking work, which introduces a modified Q-value function\nQ\nâ€‹\n(\ns\nt\n,\na\nt\n:\nt\n+\nh\n)\nQ(s_{t},a_{t:t+h})\nwhere\nh\nh\ndenotes the chunk length, and is therefore restricted to Q-learningâ€“based methods, our formulation is more general. In particular, our action chunking mechanism is implemented as an environment wrapper, allowing it to be seamlessly applied to any RL algorithm without modifying the underlying training objective. The pseudocode for our training pipeline is show in Appendix\nF\n(Algorithm\n1\n).\n4\nExperiments\nIn this section, we will evaluate our training methodology on the\nParkBench\nbenchmark and compare it with both the classical and standard RL baselines.\nTable 1:\nComparison on ParkBench. Best results are marked in\nbold\n. â€œCLâ€ denotes whether curriculum learning is used, and â€œChunkingâ€ denotes whether action chunking is used. PPO with curriculum training but without action chunking exhibits a large number of pivot points due to oscillatory behavior, which motivated the introduction of action chunking.\nMethod\nCL\nChunking\nSucc. (%)\nâ†‘\n\\;\\uparrow\nTime (s)\nâ†“\n\\;\\downarrow\nDist. (m)\nâ†“\n\\;\\downarrow\nPivot Points\nâ†“\n\\;\\downarrow\nHybrid A*\nâœ—\nâœ—\n47.1\n0.42\n22.3\n3.2\nPPO (Ours)\nâœ“\nâœ—\n62.7\n0.72\n21.7\n53.4\nPPO (Ours)\nâœ“\nâœ“\n92.2\n0.20\n19.2\n4.3\n4.1\nEvaluation Setup\nWe first train our RL approach with action chunking (\nh\n=\n4\nh=4\n) as well as the standard RL baseline following the same training strategy described in\n3.4\n. The detailed reward values and curriculum learning stages are provided as follows:\nReward values:\nThe reward function is defined as:\nr\n=\nR\ng\nâ‹…\nğŸ™\ngoal\n+\nR\nc\nâ‹…\nğŸ™\ncollision\n+\nR\nout\nâ‹…\nğŸ™\nout_of_bounds\n+\nR\ngear\nâ‹…\nğŸ™\ndirection_change\n+\nR\nidle\nâ‹…\nğŸ™\nidle\n+\nR\ntime\n,\nr=R_{\\text{g}}\\cdot\\mathbbm{1}_{\\text{goal}}+R_{\\text{c}}\\cdot\\mathbbm{1}_{\\text{collision}}+R_{\\text{out}}\\cdot\\mathbbm{1}_{\\text{out\\_of\\_bounds}}+R_{\\text{gear}}\\cdot\\mathbbm{1}_{\\text{direction\\_change}}+R_{\\text{idle}}\\cdot\\mathbbm{1}_{\\text{idle}}+R_{\\text{time}},\n(2)\nwhere\nR\ng\n=\n3\n,\nR\nc\n=\nâˆ’\n3\n,\nR\nout\n=\nâˆ’\n3\n,\nR\ngear\n=\nâˆ’\n0.01\n,\nR\nidle\n=\nâˆ’\n0.2\n,\nR\ntime\n=\nâˆ’\n0.01\nR_{\\text{g}}=3,R_{\\text{c}}=-3,R_{\\text{out}}=-3,R_{\\text{gear}}=-0.01,R_{\\text{idle}}=-0.2,R_{\\text{time}}=-0.01\n, and we use\nğŸ™\nc\nâ€‹\no\nâ€‹\nn\nâ€‹\nd\nâ€‹\ni\nâ€‹\nt\nâ€‹\ni\nâ€‹\no\nâ€‹\nn\n\\mathbbm{1}_{condition}\nto denote the indicator of the condition is reached, which equals 1 when condition holds and 0 otherwise. The tolerance for reaching the target pose is set as 0.2 meter (with respect to the geometric center) and\nÂ±\n3\n\\pm 3\ndegrees in heading difference.\nCurriculum learning stages:\nIn this work, we define a multi-stage curriculum learning process. In particular, we set up 8 stages for the complete training iterations, the first 7 stages are illustrated in Figure\n5\nand the last stage uses the logged initial poses for the learning agent.\nFigure 5:\nFirst seven stages in the curriculum learning. Stage 1 and 2 directly inherit the heading angle from rollout results. For other stages, the ego heading is reset to a collision-free angle sampled from the stage-specific range. All seven stages, the lateral offset is taken from the corresponding rollout result.\nWe evaluate our method on the\nParkBench\nusing the original logged ego pose as the starting point for each scenario, ensuring consistency across different planners. To assess performance, we report four key metrics: (1)\nSuccess rate\n: the fraction of cases where the ego successfully reaches the target pose within a tolerance on position and orientation; (2)\nPlanning time\n: the average computation time required to generate a feasible trajectory; (3)\nTravel distance\n: the total path length of the executed trajectory, measuring efficiency; and (4)\nPivot points\n: the number of direction changes (forward\nâ†”\n\\leftrightarrow\nbackward) in the trajectory, reflecting maneuver smoothness. These metrics jointly capture robustness, efficiency, and practicality of the planner in constrained parking scenarios.\n(a)\nOne intermediate step in Figure\n7\n(a).\n(b)\nOne intermediate step in Figure\n7\n(d).\nFigure 6:\nExamples of attention maps for one single decision-making step, respectively, in ego frame. We highlight the top 20 attention weights over the obstacle points.\n4.2\nResults Compare\nWe compare our method against the Hybrid A* baseline on\nParkBench\n. In addition to our full model, we also report a variant that uses PPO with curriculum learning but\nwithout\naction chunking to isolate the effect of chunking. Both learning methods start from the same logged ego poses as Hybrid A*. Results are summarized in Table\n1\n. All evaluations are conducted on the same laptop with\nCPU\nIntel 12th Gen Core i5-1245U,\nPython\n3.9,\nPyTorch\n2.6.0,\nSB3\n2.2.1. No GPU usage.\nOur RL planner outperforms Hybrid A* across nearly all metrics, achieving higher success rates, substantially lower planning time, shorter paths, and comparable pivot counts, indicating smooth and more efficient maneuvers in constrained settings. For clarity, Table\n1\nreports the classical baseline, PPO+Curriculum, and our full model (+Action Chunking).\nWe omit plain PPO and PPO+Action-Chunking in Table\n1\nbecause, without curriculum learning, both variants fail to acquire the parking behavior and achieve nearly zero success\n.\nWe also implemented standard SAC, DQN, DDPG, and other popular off-the-shelf RL algorithms\n(Andrychowicz\net al.\n,\n2017\n)\n. Without the proposed action-chunking wrapper, these methods achieve near-zero success rates. Therefore, we excluded them from the table for clarity.\n4.3\nQualitative Results\nFigure\n7\nshows some representative success cases from the parking scenarios in\nParkBench\n. The examples demonstrate that our RL policy can generate human-like paths and is capable of conducting long-horizon planning. We also visualize the attention maps from the feature extractor module to verify that the model can correctly identifies the obstacles most critical for planning at a given time frame. The plots in Figure\n6\nshow that, at the current ego pose, the agent appropriately attends to the relevant obstacles.\n(a)\nSmooth path for a long-range scenario\n(b)\nSmooth path a short-range scenario\n(c)\nPath for a corridor parking scenario\n(d)\nPath for a tight space scenario\nFigure 7:\nRepresentative planned paths generated in different parking scenarios. Panels (a) and (b) show smooth paths from the ego vehicleâ€™s start pose (magenta) to the target pose (cyan), avoiding obstacles (red). Panels (c) and (d) illustrate highly constrained cases where the planner introduces multiple pivot points, resulting in non-optimal but collision-free paths.\n4.4\nReal-Vehicle Deployment\nSince our RL-based planner is designed with the goal of replacing the classical planner in the current pipeline, deployment on a real vehicle is straightforward. We first export the trained RL checkpoint to a compact C++ inference module (via ONNX) and integrate it into the onboard planning stack. At runtime, the RL planner receives the target pose from the user along with obstacle information from the perception system, and generates a collision-free reference path in the ego frame. This path is then tracked by a standard controller, which produces steering and velocity commands consistent with vehicle-dynamics and comfort constraints, enabling seamless execution of the learned policy on the real vehicle. The deployment was conducted on our in-house test platform using the trained policy. The demonstration videos currently remain internal due to organizational policy, but we will be able to share more details in a future release.\n4.5\nLimitations\nThrough extensive evaluation across diverse rear-in parking scenarios to assess the generalization capability of the learned policy, we identify two main limitations.\n(1) Degraded performance in open/empty spaces.\nWhile the planner performs well in tightly constrained environments (remains\n90\n%\n+\n90\\%+\n), its success rate drops in sparsely constrained scenes. We hypothesize the cause: empty-space scenarios were underrepresented during training. Future work include augmenting observations with free-space/clearance features and incorporating empty-space cases into the curriculum.\n(2) Manually specified curriculum.\nThe present eight-stage curriculum is hand-crafted for rear-in parking and does not transfer cleanly to other maneuvers (\ne.g.\n, parallel parking), limiting scalability and parallelization of training. Future work include exploring automatic curricula to broaden the task coverage.\n5\nConclusion\nIn this paper, we presented an RL framework for path planning in constrained parking spaces. We introduced\nParkBench\n, a benchmark tailored to diverse and realistic parking scenarios, and designed a training methodology that integrates a plug-in action chunking wrapper with curriculum learning. Our approach achieves state-of-the-art performance, outperforming a classical Hybrid A* baseline by a significant margin.\nWe open-sourced all layouts, vehicle parameters, and our RL training methodology to encourage broader community adoption and improvement. Our goal is to provide a standardized reference framework that others can build upon, refine, and potentially surpass.\nFor future work, we plan to expand\nParkBench\nwith additional scenarios to cover a broader range of parking maneuvers, including head-in and parallel parking. We also want to improve the scalability of our RL training methodology by developing an automatic curriculum learning scheme.\nReferences\nJ. Ahn, M. Kim, and J. Park (2022)\nAutonomous driving using imitation learning with look ahead point for semi structured environments\n.\nScientific Reports\n12\n(\n1\n),\npp.Â 21285\n.\nCited by:\nÂ§1\n.\nN. Ãœ. Akmandor, H. Li, G. Lvov, E. Dusel, and T. Padir (2022)\nDeep reinforcement learning based robot navigation in dynamic environments using occupancy values of motion primitives\n.\nIn\n2022 IEEE/RSJ international conference on intelligent robots and systems (IROS)\n,\npp.Â 11687â€“11694\n.\nCited by:\nÂ§2.2\n.\nA. Al-Mousa, A. Arrabi, and H. Daoud (2025)\nA reinforcement learning-based reverse-parking system for autonomous vehicles\n.\nIET Intelligent Transport Systems\n19\n(\n1\n),\npp.Â e12614\n.\nCited by:\nÂ§2.2\n.\nM. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba (2017)\nHindsight experience replay\n.\nAdvances in neural information processing systems\n30\n.\nCited by:\nÂ§4.2\n.\nH. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom (2020)\nNuscenes: a multimodal dataset for autonomous driving\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 11621â€“11631\n.\nCited by:\nÂ§2.2\n.\nC. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song (2023)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nThe International Journal of Robotics Research\n,\npp.Â 02783649241273668\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nD. Dolgov, S. Thrun, M. Montemerlo, and J. Diebel (2008)\nPractical search techniques in path planning for autonomous driving\n.\nann arbor\n1001\n(\n48105\n),\npp.Â 18â€“80\n.\nCited by:\nÂ§1\n.\nA. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun (2017)\nCARLA: an open urban driving simulator\n.\nIn\nConference on robot learning\n,\npp.Â 1â€“16\n.\nCited by:\nÂ§2.4\n.\nC. Florensa, D. Held, M. Wulfmeier, M. Zhang, and P. Abbeel (2017)\nReverse curriculum generation for reinforcement learning\n.\nIn\nConference on robot learning\n,\npp.Â 482â€“495\n.\nCited by:\nÂ§3.4\n.\nB. Jiang, S. Chen, Q. Xu, B. Liao, J. Chen, H. Zhou, Q. Zhang, W. Liu, C. Huang, and X. Wang (2023)\nVAD: vectorized scene representation for efficient autonomous driving\n.\nICCV\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nM. Jiang, Y. Li, S. Zhang, S. Chen, C. Wang, and M. Yang (2025)\nHope: a reinforcement learning-based hybrid policy path planner for diverse parking scenarios\n.\nIEEE Transactions on Intelligent Transportation Systems\n.\nCited by:\nÂ§2.3\n.\nG. Kahn, A. Villaflor, B. Ding, P. Abbeel, and S. Levine (2018)\nSelf-supervised deep reinforcement learning with generalized computation graphs for robot navigation\n.\nIn\n2018 IEEE International Conference on Robotics and Automation (ICRA)\n,\nVol.\n,\npp.Â 5129â€“5136\n.\nExternal Links:\nDocument\nCited by:\nÂ§2.2\n.\nL. Lazzaroni, A. Pighetti, F. Bellotti, A. Capello, M. Cossu, and R. Berta (2023)\nAutomated parking in carla: a deep reinforcement learning-based approach\n.\nIn\nInternational Conference on Applications in Electronics Pervading Industry, Environment and Society\n,\npp.Â 352â€“357\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nB. Li, L. Fan, Y. Ouyang, S. Tang, X. Wang, D. Cao, and F. Wang (2022)\nOnline competition of trajectory planning for automated parking: benchmarks, achievements, learned lessons, and future perspectives\n.\nIEEE Transactions on Intelligent Vehicles\n8\n(\n1\n),\npp.Â 16â€“21\n.\nCited by:\nÂ§2.4\n.\nQ. Li, Z. Zhou, and S. Levine (2025)\nReinforcement learning with action chunking\n.\nExternal Links:\nLink\nCited by:\nÂ§1\n,\nÂ§3.5\n.\nC. PÃ©rez-Dâ€™Arpino, C. Liu, P. Goebel, R. MartÃ­n-MartÃ­n, and S. Savarese (2021)\nRobot navigation in constrained pedestrian environments using reinforcement learning\n.\nIn\n2021 IEEE International Conference on Robotics and Automation (ICRA)\n,\nVol.\n,\npp.Â 1140â€“1146\n.\nExternal Links:\nDocument\nCited by:\nÂ§2.2\n.\nA. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann (2021)\nStable-baselines3: reliable reinforcement learning implementations\n.\nJournal of Machine Learning Research\n22\n(\n268\n),\npp.Â 1â€“8\n.\nExternal Links:\nLink\nCited by:\nÂ§3.4\n.\nR. Rajamani (2006)\nVehicle dynamics and control\n.\nSpringer\n.\nCited by:\nÂ§3.1\n.\nJ. Reeds and L. Shepp (1990)\nOptimal paths for a car that goes both forwards and backwards\n.\nPacific journal of mathematics\n145\n(\n2\n),\npp.Â 367â€“393\n.\nCited by:\nÂ§2.1\n.\nX. Ruan, D. Ren, X. Zhu, and J. Huang (2019)\nMobile robot navigation based on deep reinforcement learning\n.\nIn\n2019 Chinese Control And Decision Conference (CCDC)\n,\nVol.\n,\npp.Â 6174â€“6178\n.\nExternal Links:\nDocument\nCited by:\nÂ§2.2\n.\nA. Sakai, D. Ingram, J. Dinius, K. Chawla, A. Raffin, and A. Paques (2018)\nPythonrobotics: a python code collection of robotics algorithms\n.\narXiv preprint arXiv:1808.10703\n.\nCited by:\nÂ§2.1\n.\nO. Scheel, L. Bergamini, M. Wolczyk, B. OsiÅ„ski, and P. Ondruska (2022)\nUrban driver: learning to drive from real-world demonstrations using policy gradients\n.\nIn\nConference on Robot Learning\n,\npp.Â 718â€“728\n.\nCited by:\nÂ§3.3\n.\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017)\nProximal policy optimization algorithms\n.\narXiv preprint arXiv:1707.06347\n.\nCited by:\nÂ§3.4\n.\nY. Shan, Z. Zhong, L. Fan, and H. Kai (2023)\nDeepParking: deep learning-based planning method for autonomous parking\n.\nIn\nInternational Conference on Spatial Data and Intelligence\n,\npp.Â 21â€“33\n.\nCited by:\nÂ§2.3\n.\nZ. Xu, B. Liu, X. Xiao, A. Nair, and P. Stone (2022)\nBenchmarking reinforcement learning techniques for autonomous navigation\n.\narXiv preprint arXiv:2210.04839\n.\nCited by:\nÂ§2.2\n.\nY. Yang, D. Chen, T. Qin, X. Mu, C. Xu, and M. Yang (2024)\nE2e parking: autonomous parking by the end-to-end neural network on the carla simulator\n.\nIn\n2024 IEEE Intelligent Vehicles Symposium (IV)\n,\npp.Â 2375â€“2382\n.\nCited by:\nÂ§1\n,\nÂ§2.4\n.\nY. Zheng, R. Liang, K. ZHENG, J. Zheng, L. Mao, J. Li, W. Gu, R. Ai, S. E. Li, X. Zhan, and J. Liu (2025)\nDiffusion-based planning for autonomous driving with flexible guidance\n.\nIn\nThe Thirteenth International Conference on Learning Representations\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nK. Zhu and T. Zhang (2021)\nDeep reinforcement learning based mobile robot navigation: a review\n.\nTsinghua Science and Technology\n26\n(\n5\n),\npp.Â 674â€“691\n.\nCited by:\nÂ§2.2\n.\nAppendix\nAppendix A\nUsage of Large Language Models\nDuring the preparation of this work, the authors used\nChatGPT-5\n, a large language model, for grammar and language editing. However, all content was subsequently reviewed and revised by the authors for correctness, and the authors take full responsibility for the final manuscript.\nAppendix B\nVehicle and Bicycle-Model Parameters\nIn this work, we use the bicycle model to update the environment. The detailed parameters for the bicycle model are listed below (Table\n2\n):\nName\nSymbol\nValue\nUnit / Notes\nWheelbase\nW\nB\nW_{B}\n3.0\nm\nVehicle width\nW\nW\n2.0\nm\nVehicle length\nL\nL\n4.95\nm\nRear overhang (rear center\nâ†’\n\\to\nbumper)\nL\nB\nL_{B}\n1.025\nm\nFront overhang (rear center\nâ†’\n\\to\nfront bumper)\nL\nF\nL_{F}\n3.925\nm\nMax steering angle\nÎ´\nmax\n\\delta_{\\max}\n32\nâˆ˜\n32^{\\circ}\ndeg\nTable 2:\nPhysical and geometric parameters used by the kinematic bicycle model.\nThe vehicle reference frame is the\nrear-axle center\nat the origin,\n+\nx\n+x\nforward,\n+\ny\n+y\nto the left.\nDue to the tight space in the parking scenarios, we adopt a precise polygon footprint for collision check instead of a plain rectangle.\nThe polygon is constructed by cropping each corner of the rectangle by a longitudinal offset\nc\nâ€‹\nr\nâ€‹\no\nâ€‹\np\nl\ncrop_{l}\nand a lateral offset\nc\nâ€‹\nr\nâ€‹\no\nâ€‹\np\nw\ncrop_{w}\n.\nThe resulting eight-vertex polygon is defined in the vehicle frame (rear-axle center at the origin,\n+\nx\n+x\nforward,\n+\ny\n+y\nleft):\npolygon\n=\n[\nâˆ’\nL\nB\n+\nc\nâ€‹\nr\nâ€‹\no\nâ€‹\np\nl\nâˆ’\nW\n/\n2\nL\nF\nâˆ’\nc\nâ€‹\nr\nâ€‹\no\nâ€‹\np\nl\nâˆ’\nW\n/\n2\nL\nF\nâˆ’\nW\n/\n2\n+\nc\nâ€‹\nr\nâ€‹\no\nâ€‹\np\nw\nL\nF\nW\n/\n2\nâˆ’\nc\nâ€‹\nr\nâ€‹\no\nâ€‹\np\nw\nL\nF\nâˆ’\nc\nâ€‹\nr\nâ€‹\no\nâ€‹\np\nl\nW\n/\n2\nâˆ’\nL\nB\n+\nc\nâ€‹\nr\nâ€‹\no\nâ€‹\np\nl\nW\n/\n2\nâˆ’\nL\nB\nW\n/\n2\nâˆ’\nc\nâ€‹\nr\nâ€‹\no\nâ€‹\np\nw\nâˆ’\nL\nB\nâˆ’\nW\n/\n2\n+\nc\nâ€‹\nr\nâ€‹\no\nâ€‹\np\nw\n]\n,\n\\texttt{polygon}=\\begin{bmatrix}-L_{B}+crop_{l}&-W/2\\\\\n\\;\\;L_{F}-crop_{l}&-W/2\\\\\n\\;\\;L_{F}&-W/2+crop_{w}\\\\\n\\;\\;L_{F}&\\;\\;W/2-crop_{w}\\\\\n\\;\\;L_{F}-crop_{l}&\\;\\;W/2\\\\\n-L_{B}+crop_{l}&\\;\\;W/2\\\\\n-L_{B}&\\;\\;W/2-crop_{w}\\\\\n-L_{B}&-W/2+crop_{w}\\end{bmatrix},\nwhere\nc\nâ€‹\nr\nâ€‹\no\nâ€‹\np\nl\ncrop_{l}\n= 0.3\nm\nand\nc\nâ€‹\nr\nâ€‹\no\nâ€‹\np\nw\ncrop_{w}\n= 0.2\nm\n.\nAppendix C\nRL algorithm settings\nThe following table (Table\n3\n) summarizes the hyperparameter settings used in our RL training. The maximum episode length varies across curriculum stages. Specifically, we set it to\n[\n100\n,\n200\n,\n400\n,\n400\n,\n800\n,\n800\n,\n800\n,\n1000\n]\n[100,200,400,400,800,800,800,1000]\nfor the 8 stages, respectively. All other parameters not listed are kept at their default values.\nParameter\nValue\nTraining batch size\n256\nBatch size per GPU\n1024\nNum. PPO epochs\n10\nDiscount factor\nÎ³\n\\gamma\n1.0\nMax. episode length\ndepends on the training stage\nInitial LR\nÎ±\n(\n0\n)\n\\alpha^{(0)}\n3\nÃ—\n10\nâˆ’\n4\n3\\times 10^{-4}\nLR schedule\nConstant\nEntropy coefficient\n0.001\nGPU usage during inference\nNot used\nTable 3:\nRL algorithm settings and hyperparameters used during training.\nAppendix D\nEnvironment action space\nIn our environment design, we use a discrete action space (shown in Table\n4\n) parameterized by the steering increment\nÎ”\nâ€‹\nÎ´\n\\Delta\\delta\nand a signed speed\nv\nv\n. In our environment,\nÎ”\nâ€‹\nÎ´\nâˆˆ\n{\nâˆ’\n8\n,\n0\n,\n8\n}\n\\Delta\\delta\\in\\{-8,0,8\\}\nand\nv\nâˆˆ\n{\nâˆ’\n0.8\n,\n0\n,\n0.8\n}\nv\\in\\{-0.8,0,0.8\\}\n. The eight actions listed below are derived from statistics of motion primitives generated by our classical planner; this choice yields smooth curvature changes and reliable tracking under a pure-pursuit controller. We exclude the idle\n(\n0\n,\n0\n)\n(0,0)\naction to avoid no-operation steps.\nTable 4:\nDiscrete action space used in all RL experiments. Each action is a primitive\n(\nÎ”\nâ€‹\nÎ´\n,\nv\n)\n(\\Delta\\delta,v)\nwith steering increment\nÎ”\nâ€‹\nÎ´\n\\Delta\\delta\nin degrees and longitudinal speed\nv\nv\nin m/s. Time step is\n0.1\nâ€‹\ns\n0.1s\n.\nIndex\nÎ”\nâ€‹\nÎ´\n\\Delta\\delta\n[deg]\nv\nv\n[m/s]\nDescription\n0\nâˆ’\n8\n-8\n+\n0.8\n+0.8\nTurn right, forward\n1\n0\n+\n0.8\n+0.8\nStraight, forward\n2\n+\n8\n+8\n+\n0.8\n+0.8\nTurn left, forward\n3\nâˆ’\n8\n-8\nâˆ’\n0.8\n-0.8\nTurn right, reverse\n4\n0\nâˆ’\n0.8\n-0.8\nStraight, reverse\n5\n+\n8\n+8\nâˆ’\n0.8\n-0.8\nTurn left, reverse\n6\nâˆ’\n8\n-8\n0\nPre-steer right (no translation)\n7\n+\n8\n+8\n0\nPre-steer left (no translation)\nAppendix E\nHybrid A* hyperparameters\nWe adopt the public available path planning repository as mentioned in section\n2.1\n. To accelerate path searching, we exclude obstacle points located more than 25 meters from the egoâ€™s initial position. The bicycle model follows the same configuration as our simulation environment, with a wheelbase of 3.0 m, a width of 2.0 m, a length of 4.95 m, and a maximum steering angle of\n32\nâˆ˜\n32^{\\circ}\n. Table\n5\nshows the ablation study we conducted on the hyperparameters of Hybrid A*. We report the best-performing configuration in the paper.\nHeuristic\nÎ”\nâ€‹\nx\n,\ny\n\\Delta x,y\n(m)\nÎ”\nâ€‹\nÎ¸\n\\Delta\\theta\nMotion res. (m)\n#Steer\nSuccess (%)\nTime (s)\nDist. (m)\nPivots\nDefault values\nâ€ \n0.1\n8\nâˆ˜\n8^{\\circ}\n1.0\n9\n37.3\n3.64\n23.1\n3.6\n0.32\n8\nâˆ˜\n8^{\\circ}\n1.0\n9\n41.2\n0.41\n22.8\n4.1\n0.5\n8\nâˆ˜\n8^{\\circ}\n1.0\n9\n47.1\n0.74\n23.4\n3.7\n0.5\n8\nâˆ˜\n8^{\\circ}\n0.5\n9\n45.1\n0.64\n24.3\n3.7\n0.5\n8\nâˆ˜\n8^{\\circ}\n2.0\n9\n29.4\n0.70\n25.9\n1.7\n0.5\n5\nâˆ˜\n\\textbf{5}^{\\circ}\n1.0\n20\n47.1\n0.42\n22.3\n3.2\n1.0\n5\nâˆ˜\n5^{\\circ}\n1.0\n20\n41.2\n0.96\n23.8\n1.7\nNew values\nâ€¡\n0.1\n8\nâˆ˜\n8^{\\circ}\n1.0\n9\n35.3\n5.29\n20.0\n3.6\n0.32\n8\nâˆ˜\n8^{\\circ}\n1.0\n9\n41.2\n0.53\n19.8\n4.1\n0.5\n8\nâˆ˜\n8^{\\circ}\n1.0\n9\n43.1\n0.69\n19.3\n3.9\n0.5\n8\nâˆ˜\n8^{\\circ}\n2.0\n9\n29.4\n0.82\n24.8\n1.8\n0.5\n8\nâˆ˜\n8^{\\circ}\n0.5\n9\n43.1\n0.5\n19.9\n3.6\nTable 5:\nAblation study on Hybrid A* hyperparameters.\nNotes.\nâ€ \nThe default values correspond to the existing heuristics in the public repository.\nâ€¡\nThe new values correspond to our new experiments. Specifically, we set the switch-back penalty cost to 2.0, the backward penalty cost to 1.3, the steering angle penalty cost to 0.2, the steering change penalty cost to 0.1, and the heuristic cost to 1.0.\nAppendix F\nRL algorithm\nPseudocode: integrating curriculum learning and an action-chunking wrapper with the PPO algorithm. Shown in Algorithm\n1\n.\nAlgorithm 1\nTraining RL Planner with Action Chunking and Curriculum Learning\n1:\nBenchmark\nâ„¬\n\\mathcal{B}\n, simulator\nEnv\n, policy\nÏ€\nÎ¸\n\\pi_{\\theta}\n, chunk length\nh\nh\n, curriculum scheduler\nğ’\n\\mathcal{C}\n, PPO optimizer, total_steps\nN\nN\n2:\nInitialize rollout buffer\nğ’Ÿ\nâ†\nâˆ…\n\\mathcal{D}\\leftarrow\\emptyset\n, policy params\nÎ¸\n\\theta\n3:\nfor\niteration\n=\n1\n,\n2\n,\nâ€¦\n=1,2,\\dots\ndo\n4:\nSelect curriculum level\nc\nâ†\nğ’\nâ€‹\n(\niteration\n)\nc\\leftarrow\\mathcal{C}(\\text{iteration})\n5:\nstep\nâ†\n0\n\\leftarrow 0\n6:\nwhile\nstep\n<\nN\n<N\ndo\n7:\nSample scenario\n(\ng\n,\nO\n)\nâˆ¼\nâ„¬\n(g,O)\\sim\\mathcal{B}\nâŠ³\n\\triangleright\ng\ng\n: target pose,\nO\nO\n: obstacle contours\n8:\nRollout init\n:\np\n0\nâ†\nRolloutFromTarget\nâ€‹\n(\ng\n,\nO\n,\nc\n)\n\\;p_{0}\\leftarrow\\textsc{RolloutFromTarget}(g,O,c)\nâŠ³\n\\triangleright\np\n0\np_{0}\n: ego initial pose\n9:\nReset env\n:\nEnv\n.\nReset\nâ€‹\n(\ng\n,\nO\n,\np\n0\n)\n\\texttt{Env}.\\textsc{Reset}(g,O,p_{0})\n10:\nstate\n:\ns\n0\nâ†\nEgoCentric\nâ€‹\n(\np\n0\n,\ng\n0\n,\nO\n0\n)\ns_{0}\\leftarrow\\text{EgoCentric}(p_{0},g_{0},O_{0})\nâŠ³\n\\triangleright\ncoordinate transform, normalization, range clip\n11:\ndone\nâ†\n\\leftarrow\nFalse\n12:\nwhile\nnot done\ndo\n13:\nChunked action\n:\na\nt\n:\nt\n+\nh\nâˆ¼\nÏ€\nÎ¸\nâ€‹\n(\ns\nt\n)\na_{t:t+h}\\sim\\pi_{\\theta}(s_{t})\nâŠ³\n\\triangleright\na\nt\n:\nt\n+\nh\na_{t:t+h}\nencodes\nh\nh\nprimitive steps\n14:\nR\nâ†\n0\nR\\leftarrow 0\n15:\nfor\nk\n=\n0\nk=0\nto\nh\nâˆ’\n1\nh-1\ndo\nâŠ³\n\\triangleright\nAction chunk wrapper executes\nh\nh\nlow-level steps\n16:\n(\np\nt\n+\nk\n+\n1\n,\ng\nt\n+\nk\n+\n1\n,\nO\nt\n+\nk\n+\n1\n,\nr\n,\ndone\n)\nâ†\nEnv\n.\nStep\nâ€‹\n(\nprimitive\nâ€‹\n(\na\nt\n+\nk\n)\n)\n(p_{t+k+1},g_{t+k+1},O_{t+k+1},r,\\text{done})\\leftarrow\\texttt{Env}.\\textsc{Step}(\\text{primitive}(a_{t+k}))\n17:\nR\nâ†\nR\n+\nr\nR\\leftarrow R+r\n;\ns\nâ†\nEgoCentric\nâ€‹\n(\np\nt\n+\nk\n+\n1\n,\ng\nt\n+\nk\n+\n1\n,\nO\nt\n+\nk\n+\n1\n)\n\\;s\\leftarrow\\text{EgoCentric}(p_{t+k+1},g_{t+k+1},O_{t+k+1})\n18:\nif\ndone\nthen\nbreak\n19:\nend\nif\n20:\nend\nfor\n21:\nStore transition\n(\ns\nt\n,\na\nt\n,\nR\n,\ns\n,\ndone\n)\n\\big(s_{t},a_{t},R,s,\\text{done}\\big)\ninto\nğ’Ÿ\n\\mathcal{D}\n22:\ns\nt\n+\n1\nâ†\ns\ns_{t+1}\\leftarrow s\n23:\nif\nğ’Ÿ\n\\mathcal{D}\nis full\nthen\n24:\nUpdatePolicyPPO\n(\nÏ€\nÎ¸\n,\nğ’Ÿ\n)\n(\\pi_{\\theta},\\mathcal{D})\n;\n25:\nğ’Ÿ\nâ†\nâˆ…\n\\mathcal{D}\\leftarrow\\emptyset\n26:\nend\nif\n27:\nend\nwhile\n28:\nend\nwhile\n29:\nend\nfor\nAppendix G\nLayouts in ParkBench\nAll 51 ParkBench parking-scenario layouts (17Ã—3 grid):\nFigure 8:\nAll 51 parking cases in the\nParkBench\nbenchmark, arranged in 17\nÃ—\n\\times\n3 grids across three pages. The magenta rectangle denotes the initial ego pose, while the cyan rectangle indicates the target pose. Obstacles are shown in red. The planned paths generated by our AI planner are illustrated in blue. The planner succeeds in 47 out of 51 scenarios, failing in 4 cases.",
    "preview_text": "Real-time path planning in constrained environments remains a fundamental challenge for autonomous systems. Traditional classical planners, while effective under perfect perception assumptions, are often sensitive to real-world perception constraints and rely on online search procedures that incur high computational costs. In complex surroundings, this renders real-time deployment prohibitive. To overcome these limitations, we introduce a Deep Reinforcement Learning (DRL) framework for real-time path planning in parking scenarios. In particular, we focus on challenging scenes with tight spaces that require a high number of reversal maneuvers and adjustments. Unlike classical planners, our solution does not require ideal and structured perception, and in principle, could avoid the need for additional modules such as localization and tracking, resulting in a simpler and more practical implementation. Also, at test time, the policy generates actions through a single forward pass at each step, which is lightweight enough for real-time deployment. The task is formulated as a sequential decision-making problem grounded in a bicycle model dynamics, enabling the agent to directly learn navigation policies that respect vehicle kinematics and environmental constraints in the closed-loop setting. A new benchmark is developed to support both training and evaluation, capturing diverse and challenging scenarios. Our approach achieves state-of-the-art success rates and efficiency, surpassing classical planner baselines by +96% in success rate and +52% in efficiency. Furthermore, we release our benchmark as an open-source resource for the community to foster future research in autonomous systems. The benchmark and accompanying tools are available at https://github.com/dqm5rtfg9b-collab/Constrained_Parking_Scenarios.\n\nAdapting Reinforcement Learning for Path Planning in Constrained Parking Scenarios\nFeng Tao, Luca Paparusso, Chenyi Gu, Robin Koehler, Chenxu Wu, Xinyu Huang, Christia",
    "is_relevant": true,
    "relevance_score": 7.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "constrained reinforcement learning",
        "goal conditioned reinforcement learning",
        "world model"
    ],
    "one_line_summary": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å®æ—¶è·¯å¾„è§„åˆ’æ¡†æ¶ï¼Œä¸“æ³¨äºå—é™åœè½¦åœºæ™¯ï¼Œé€šè¿‡é—­ç¯è®¾ç½®å­¦ä¹ å°Šé‡è½¦è¾†åŠ¨åŠ›å­¦å’Œç¯å¢ƒçº¦æŸçš„å¯¼èˆªç­–ç•¥ï¼Œå¹¶å¼€å‘äº†æ–°çš„åŸºå‡†æµ‹è¯•ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦\n\n**è®ºæ–‡æ ‡é¢˜ï¼š** åœ¨å—é™åœè½¦åœºæ™¯ä¸­åº”ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè·¯å¾„è§„åˆ’\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nåœ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­ï¼Œå—é™ç¯å¢ƒä¸‹çš„å®æ—¶è·¯å¾„è§„åˆ’æ˜¯ä¸€é¡¹æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä»¥æ··åˆA*ä¸ºä»£è¡¨çš„ä¼ ç»Ÿè§„åˆ’å™¨åœ¨ç†æƒ³æ„ŸçŸ¥å‡è®¾ä¸‹æœ‰æ•ˆï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå®ƒä»¬å¯¹æ„ŸçŸ¥ä¸ç¡®å®šæ€§æ•æ„Ÿï¼Œä¸”ä¾èµ–åœ¨çº¿æœç´¢ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥æ»¡è¶³å¤æ‚åœºæ™¯ä¸‹çš„å®æ—¶æ€§è¦æ±‚ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦å®šä½ã€è·Ÿè¸ªç­‰é¢å¤–æ¨¡å—ï¼Œå¢åŠ äº†ç³»ç»Ÿå¤æ‚æ€§å’Œè¯¯å·®ç´¯ç§¯é£é™©ã€‚å› æ­¤ï¼Œç ”ç©¶ä¸€ç§æ›´é²æ£’ã€é«˜æ•ˆä¸”æ˜“äºé›†æˆçš„è§„åˆ’æ–¹æ³•è‡³å…³é‡è¦ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº**æ·±åº¦å¼ºåŒ–å­¦ä¹ **çš„å®æ—¶è·¯å¾„è§„åˆ’æ¡†æ¶ï¼Œä¸“é—¨é’ˆå¯¹éœ€è¦å¤šæ¬¡å€’è½¦å’Œè°ƒæ•´çš„**ç´§çº¦æŸåœè½¦åœºæ™¯**ã€‚ä¸»è¦æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬ï¼š\n- **é—®é¢˜å»ºæ¨¡**ï¼šå°†è·¯å¾„è§„åˆ’å»ºæ¨¡ä¸ºåŸºäº**è‡ªè¡Œè½¦æ¨¡å‹åŠ¨åŠ›å­¦**çš„åºåˆ—å†³ç­–é—®é¢˜ï¼Œç¡®ä¿è§„åˆ’è·¯å¾„æ»¡è¶³è½¦è¾†è¿åŠ¨å­¦çº¦æŸã€‚\n- **è®­ç»ƒç­–ç•¥**ï¼šé‡‡ç”¨**è¯¾ç¨‹å­¦ä¹ **å’Œ**ç¨€ç–å¥–åŠ±å‡½æ•°**ï¼Œå¼•å¯¼æ™ºèƒ½ä½“ä»æ˜“åˆ°éš¾å­¦ä¹ åœè½¦æŠ€èƒ½ï¼Œå¹¶å¼•å…¥**åŠ¨ä½œåˆ†å—**æœºåˆ¶ï¼Œä»¥å¹³è¡¡æ¢ç´¢æ•ˆç‡ä¸ç²¾ç»†æ§åˆ¶éœ€æ±‚ã€‚\n- **åŸºå‡†ä¸ä»¿çœŸ**ï¼šæ„å»ºäº†åä¸º **`ParkBench`** çš„ä¸“ç”¨åŸºå‡†æµ‹è¯•é›†ï¼ˆåŒ…å«51ä¸ªæºè‡ªçœŸå®æ•°æ®çš„åœºæ™¯ï¼‰å’Œé…å¥—çš„é—­ç¯ä»¿çœŸç¯å¢ƒï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸæ ‡å‡†åŒ–è¯„ä¼°å·¥å…·çš„ç©ºç™½ã€‚\n- **ç³»ç»Ÿè®¾è®¡**ï¼šè¾“å…¥è®¾è®¡ä¸ä¼ ç»Ÿè§„åˆ’å™¨ï¼ˆæ··åˆA*ï¼‰ä¿æŒä¸€è‡´ï¼ˆè½¦è¾†ä½å§¿ã€ç›®æ ‡ä½å§¿ã€éšœç¢ç‰©è½®å»“ï¼‰ï¼Œä¾¿äºç›´æ¥æ›¿æ¢ç°æœ‰è§„åˆ’æ¨¡å—ï¼Œå¹¶è®¾è®¡äº†è½»é‡çº§ç‰¹å¾æå–å™¨ä»¥å®ç°å®æ—¶æ¨ç†ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nåœ¨`ParkBench`åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„RLè§„åˆ’å™¨æ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ï¼š\n- **æˆåŠŸç‡**ï¼šè¾¾åˆ°**92.2%**ï¼Œæ¯”æ··åˆA*åŸºçº¿ï¼ˆ47.1%ï¼‰æå‡äº†**+96%**ã€‚\n- **è§„åˆ’æ•ˆç‡**ï¼šå¹³å‡è§„åˆ’æ—¶é—´ä»…**0.20ç§’**ï¼Œæ¯”åŸºçº¿ï¼ˆ0.42ç§’ï¼‰æå‡äº†**+52%**ã€‚\n- **è·¯å¾„è´¨é‡**ï¼šç”Ÿæˆçš„è·¯å¾„æ›´çŸ­ã€æ›´å¹³æ»‘ï¼ˆå¹³å‡è½¬å‘åˆ‡æ¢ç‚¹ä»…4.3ä¸ªï¼Œæ¥è¿‘åŸºçº¿æ°´å¹³ï¼‰ã€‚\n- **å®šæ€§åˆ†æ**ï¼šå¯è§†åŒ–æ˜¾ç¤ºï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿè¿›è¡Œé•¿æ—¶ç¨‹è§„åˆ’ï¼Œå…¶æ³¨æ„åŠ›æœºåˆ¶èƒ½æœ‰æ•ˆå…³æ³¨å…³é”®éšœç¢ç‰©ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\næœ¬ç ”ç©¶è¯æ˜äº†**çº¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•**åœ¨å¤æ‚ã€ç´§çº¦æŸåœè½¦ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚å…¶ä»·å€¼åœ¨äºï¼š\n- **æŠ€æœ¯è´¡çŒ®**ï¼šæä¾›äº†ä¸€å¥—å®Œæ•´ã€å¯å¤ç°çš„RLè§„åˆ’æ–¹æ³•è®ºï¼ŒåŒ…æ‹¬é—®é¢˜å»ºæ¨¡ã€è®­ç»ƒæŠ€å·§å’Œè¯„ä¼°åŸºå‡†ã€‚\n- **å®ç”¨ä»·å€¼**ï¼šè§„åˆ’å™¨è®¾è®¡ä¸ºç°æœ‰è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„â€œå³æ’å³ç”¨â€æ›¿ä»£æ¨¡å—ï¼Œè®¡ç®—è½»é‡ï¼Œ**å•æ­¥å‰å‘æ¨ç†**å³å¯ç”ŸæˆåŠ¨ä½œï¼Œå…·å¤‡å®é™…éƒ¨ç½²æ½œåŠ›ã€‚\n- **ç¤¾åŒºè´¡çŒ®**ï¼šå¼€æºäº†`ParkBench`åŸºå‡†ã€ä»£ç å’Œè®­ç»ƒæ–¹æ³•ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–å¹³å°ï¼Œæœ‰æœ›æ¨åŠ¨è‡ªåŠ¨é©¾é©¶è§„åˆ’é¢†åŸŸçš„å‘å±•ã€‚æœªæ¥å·¥ä½œå¯æ‰©å±•è‡³å¹³è¡Œæ³Šè½¦ç­‰æ›´å¤šåœºæ™¯ï¼Œå¹¶æ¢ç´¢è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ ä»¥æå‡æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **ä¸€ã€ è®ºæ–‡æ—¨åœ¨è§£å†³çš„æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**åœ¨å¤æ‚ã€ç‹­çª„çš„å—é™åœè½¦åœºæ™¯ä¸­ï¼Œä¼ ç»Ÿè·¯å¾„è§„åˆ’æ–¹æ³•ï¼ˆå¦‚Hybrid A*ï¼‰çš„å±€é™æ€§**ï¼Œå…·ä½“åŒ…æ‹¬ï¼š\n1.  **å¯¹æ„ŸçŸ¥ä¸ç¡®å®šæ€§æ•æ„Ÿ**ï¼šä¾èµ–ç²¾ç¡®ã€å®Œæ•´çš„éšœç¢ç‰©ä¿¡æ¯ï¼Œåœ¨ç°å®ä¸–ç•Œçš„éƒ¨åˆ†å¯è§‚æµ‹ï¼ˆå¦‚é®æŒ¡ï¼‰ç¯å¢ƒä¸‹å®¹æ˜“è§„åˆ’å‡ºå¯¼è‡´ç¢°æ’çš„è·¯å¾„ã€‚\n2.  **åœ¨çº¿è®¡ç®—æˆæœ¬é«˜**ï¼šæ¯æ¬¡è§„åˆ’éƒ½éœ€è¦é‡æ–°æœç´¢ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­éš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚\n3.  **ç³»ç»Ÿé›†æˆå¤æ‚**ï¼šéœ€è¦ä¾èµ–å®šä½ã€è·¯å¾„è·Ÿè¸ªç­‰é¢å¤–æ¨¡å—ï¼Œå¢åŠ äº†ç³»ç»Ÿå¤æ‚æ€§å’Œè¯¯å·®ç´¯ç§¯é£é™©ã€‚\n\n### **äºŒã€ è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ª**åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç«¯åˆ°ç«¯è·¯å¾„è§„åˆ’æ¡†æ¶**ï¼Œå…¶åˆ›æ–°ç‚¹ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹å››ä¸ªæ–¹é¢ï¼š\n\n1.  **é—®é¢˜å»ºæ¨¡ä¸é—­ç¯è®­ç»ƒèŒƒå¼**ï¼š\n    - **åˆ›æ–°**ï¼šå°†åœè½¦è·¯å¾„è§„åˆ’**å½¢å¼åŒ–ä¸ºä¸€ä¸ªåŸºäºè‡ªè¡Œè½¦æ¨¡å‹çš„åºåˆ—å†³ç­–é—®é¢˜**ã€‚æ™ºèƒ½ä½“åœ¨é—­ç¯ä»¿çœŸç¯å¢ƒä¸­ç›´æ¥å­¦ä¹ æ»¡è¶³è½¦è¾†è¿åŠ¨å­¦çº¦æŸçš„å¯¼èˆªç­–ç•¥ã€‚\n    - **ä»·å€¼**ï¼šé¿å…äº†å¼€ç¯æ¨¡ä»¿å­¦ä¹ ä¸­çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œç”Ÿæˆçš„è·¯å¾„å¤©ç„¶å…·æœ‰åŠ¨æ€å¯è¡Œæ€§å’Œå¯è·Ÿè¸ªæ€§ã€‚\n\n2.  **â€œåŠ¨ä½œåˆ†å—â€æœºåˆ¶**ï¼š\n    - **åˆ›æ–°**ï¼šå¼•å…¥ä¸€ä¸ª**é€šç”¨çš„ç¯å¢ƒåŒ…è£…å™¨**ï¼Œå°†å¤šä¸ªä½å±‚æ§åˆ¶åŠ¨ä½œç»„åˆæˆä¸€ä¸ªâ€œå®åŠ¨ä½œâ€æ‰§è¡Œã€‚\n    - **ä»·å€¼**ï¼šæœ‰æ•ˆå¹³è¡¡äº†**æ¢ç´¢æ•ˆç‡**ï¼ˆå‡å°‘å†³ç­–æ­¥æ•°ï¼Œç¼©çŸ­è§„åˆ’è§†é‡ï¼‰ä¸**æ“æ§ç²¾åº¦**ï¼ˆä¿ç•™ç»†ç²’åº¦æ§åˆ¶èƒ½åŠ›ï¼‰ï¼Œè§£å†³äº†åœ¨ç‹­çª„ç©ºé—´ä¸­RLè®­ç»ƒæ•ˆç‡ä½ä¸‹çš„å…³é”®éš¾é¢˜ã€‚\n\n3.  **ä¸“ç”¨åŸºå‡†æ•°æ®é›†ä¸ä»¿çœŸç¯å¢ƒ**ï¼š\n    - **åˆ›æ–°**ï¼šæ„å»ºå¹¶å¼€æºäº† **`ParkBench` åŸºå‡†**ï¼ŒåŒ…å«51ä¸ªæºè‡ªçœŸå®æ•°æ®ã€å…·æœ‰æŒ‘æˆ˜æ€§çš„å€’è½¦å…¥åº“åœºæ™¯ã€‚\n    - **ä»·å€¼**ï¼šå¡«è¡¥äº†è¯¥é¢†åŸŸç¼ºä¹æ ‡å‡†åŒ–ã€è´´è¿‘ç°å®çš„è¯„ä¼°åŸºå‡†çš„ç©ºç™½ï¼Œä¸ºç¤¾åŒºæä¾›äº†å¯é çš„è®­ç»ƒå’Œæµ‹è¯•å¹³å°ã€‚\n\n4.  **å®Œæ•´çš„è®­ç»ƒæ–¹æ³•å­¦**ï¼š\n    - **åˆ›æ–°**ï¼šç»“åˆäº†**è¯¾ç¨‹å­¦ä¹ **å’Œ**ç¨€ç–å¥–åŠ±è®¾è®¡**ã€‚é€šè¿‡ä»æ˜“åˆ°éš¾ï¼ˆåˆå§‹ä½ç½®ä»ç›®æ ‡ç‚¹é™„è¿‘é€æ¸å¤–ç§»ï¼‰çš„è¯¾ç¨‹ï¼Œå¼•å¯¼æ™ºèƒ½ä½“é«˜æ•ˆå­¦ä¹ ã€‚\n    - **ä»·å€¼**ï¼šå…‹æœäº†ç¨€ç–å¥–åŠ±ä¸‹RLéš¾ä»¥å­¦ä¹ çš„æŒ‘æˆ˜ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤ŸæŒæ¡ä»ç®€å•åˆ°å¤æ‚çš„å®Œæ•´åœè½¦æŠ€èƒ½ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆæ¦‚è¿°**\nè®ºæ–‡é€šè¿‡ä¸€ä¸ªé›†æˆçš„æŠ€æœ¯æ¡†æ¶æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼š\n\n1.  **ç³»ç»Ÿè®¾è®¡**ï¼š\n    - **è¾“å…¥**ï¼šä¸Hybrid A*å¯¹é½ï¼ŒåŒ…æ‹¬è½¦è¾†å½“å‰ä½å§¿ã€ç›®æ ‡ä½å§¿ä»¥åŠç¨€ç–çš„éšœç¢ç‰©è½®å»“ç‚¹äº‘ï¼ˆæ¨¡æ‹Ÿæ¿€å…‰é›·è¾¾æ•°æ®ï¼‰ï¼Œç¡®ä¿äº†ä¸ç°æœ‰è‡ªåŠ¨é©¾é©¶æ ˆçš„å…¼å®¹æ€§ã€‚\n    - **æ¨¡å‹**ï¼šä½¿ç”¨è½»é‡çº§çš„ç‰¹å¾æå–å™¨ï¼ˆå«äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼‰å¤„ç†å‘é‡åŒ–è¾“å…¥ï¼Œè¾“å‡ºç¦»æ•£æ§åˆ¶åŠ¨ä½œï¼ˆçºµå‘ä½ç§»å’Œæ–¹å‘ç›˜è½¬è§’å¢é‡ï¼‰ã€‚\n    - **åŠ¨åŠ›å­¦**ï¼šä¸¥æ ¼é‡‡ç”¨**è‡ªè¡Œè½¦æ¨¡å‹**æ›´æ–°è½¦è¾†çŠ¶æ€ï¼Œä¿è¯æ‰€æœ‰åŠ¨ä½œç¬¦åˆéå®Œæ•´çº¦æŸã€‚\n\n2.  **è®­ç»ƒæµç¨‹**ï¼š\n    ```\n    1. åˆå§‹åŒ–ï¼šåŠ è½½ParkBenchåœºæ™¯ï¼Œé€šè¿‡â€œæ»šå‡ºâ€å‡½æ•°ç”Ÿæˆå¯è¡Œçš„åˆå§‹ä½å§¿ã€‚\n    2. è¯¾ç¨‹å­¦ä¹ ï¼šæŒ‰8ä¸ªé˜¶æ®µé€æ­¥å¢åŠ åˆå§‹ä½å§¿ä¸ç›®æ ‡ç‚¹çš„è·ç¦»å’Œè§’åº¦åå·®ã€‚\n    3. äº¤äº’ä¸å­¦ä¹ ï¼šæ™ºèƒ½ä½“åœ¨ä»¿çœŸç¯å¢ƒä¸­ä½¿ç”¨PPOç®—æ³•è¿›è¡Œè®­ç»ƒã€‚\n        - æ¯æ­¥å†³ç­–é‡‡ç”¨â€œåŠ¨ä½œåˆ†å—â€æœºåˆ¶ã€‚\n        - å¥–åŠ±å‡½æ•°ä¸ºç¨€ç–å½¢å¼ï¼ˆæˆåŠŸ+3ï¼Œç¢°æ’-3ï¼Œè¶…æ—¶/é—²ç½®ç­‰æœ‰å°æƒ©ç½šï¼‰ã€‚\n    4. æ”¶æ•›ï¼šå¾—åˆ°å¯ç›´æ¥è¾“å‡ºæ§åˆ¶æŒ‡ä»¤çš„RLç­–ç•¥ã€‚\n    ```\n\n3.  **éƒ¨ç½²**ï¼šè®­ç»ƒå¥½çš„ç­–ç•¥å¯å¯¼å‡ºä¸ºONNXæ¨¡å‹ï¼Œé›†æˆåˆ°C++è§„åˆ’æ ˆä¸­ï¼Œæ›¿æ¢åŸæœ‰çš„Hybrid A*æ¨¡å—ï¼Œå®ç°ä»æ„ŸçŸ¥ä¿¡æ¯åˆ°å‚è€ƒè·¯å¾„çš„å®æ—¶ç”Ÿæˆã€‚\n\n### **å››ã€ å®é™…ä»·å€¼ä¸æ•ˆæœ**\n- **æ€§èƒ½æå‡**ï¼šåœ¨`ParkBench`ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸æ¯”Hybrid A*åŸºçº¿ï¼Œ**æˆåŠŸç‡æå‡96%ï¼ˆä»47.1%åˆ°92.2%ï¼‰ï¼Œè§„åˆ’æ•ˆç‡æå‡52%**ï¼Œä¸”è·¯å¾„æ›´å¹³æ»‘ã€‚\n- **å·¥ç¨‹å®ç”¨æ€§**ï¼š\n    - **å®æ—¶æ€§**ï¼šæ¨ç†æ—¶ä»…éœ€å•æ¬¡å‰å‘ä¼ æ’­ï¼Œè®¡ç®—è½»é‡ã€‚\n    - **ç®€åŒ–ç³»ç»Ÿ**ï¼šåŸåˆ™ä¸Šå¯å‡å°‘å¯¹ç‹¬ç«‹å®šä½å’Œè·Ÿè¸ªæ¨¡å—çš„ä¾èµ–ï¼Œç³»ç»Ÿæ›´é²æ£’ã€‚\n    - **å¼€æºè´¡çŒ®**ï¼šå‘å¸ƒäº†å®Œæ•´çš„åŸºå‡†ã€ä»£ç å’Œè®­ç»ƒæ–¹æ³•ï¼Œæ¨åŠ¨é¢†åŸŸç ”ç©¶ã€‚\n- **å±€é™æ€§**ï¼šåœ¨è¿‡äºç©ºæ—·çš„åœºæ™¯ä¸­æ€§èƒ½ä¸‹é™ï¼Œä¸”å½“å‰è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ä¸ºæ‰‹åŠ¨è®¾è®¡ï¼Œéš¾ä»¥æ‰©å±•åˆ°å…¶ä»–åœè½¦ç±»å‹ï¼ˆå¦‚å¹³è¡Œæ³Šè½¦ï¼‰ã€‚",
            "timestamp": "2026-02-03T15:54:28.699805",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è‡ªåŠ¨é©¾é©¶è·¯å¾„è§„åˆ’å‘å±•è„‰ç»œåˆ†æ\n\næœ¬æ–‡ã€ŠAdapting Reinforcement Learning for Path Planning in Constrained Parking Scenariosã€‹æ˜¯è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯**å—é™åœºæ™¯ï¼ˆå¦‚æ³Šè½¦ï¼‰è·¯å¾„è§„åˆ’**æ–¹å‘çš„æœ€æ–°è¿›å±•ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äº**é¦–æ¬¡ç³»ç»Ÿæ€§åœ°å°†æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰åº”ç”¨äºçœŸå®ã€é«˜çº¦æŸçš„æ³Šè½¦åœºæ™¯ï¼Œå¹¶æ„å»ºäº†ä¸“ç”¨åŸºå‡†ï¼ˆParkBenchï¼‰**ã€‚è¦ç†è§£å…¶ä»·å€¼ï¼Œéœ€æ¢³ç†è¯¥æ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œå…¶æ¼”è¿›ä¸»è¦æ²¿ç€**è§„åˆ’èŒƒå¼**å’Œ**æŠ€æœ¯èåˆ**ä¸¤ä¸ªç»´åº¦å±•å¼€ã€‚\n\n### å‘å±•è„‰ç»œä¸å…³é”®æ”¹è¿›\n\n#### ç¬¬ä¸€é˜¶æ®µï¼šç»å…¸è§„åˆ’å™¨ä¸»å¯¼æ—¶æœŸ\n*   **ä»£è¡¨å·¥ä½œ**ï¼šHybrid A* (Dolgov et al., 2008) åŠå…¶å˜ç§ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šåŸºäºæœç´¢çš„ç®—æ³•ï¼Œåœ¨çŠ¶æ€ç©ºé—´ï¼ˆä½ç½®ã€æœå‘ï¼‰ä¸­å¯»æ‰¾æ»¡è¶³è½¦è¾†è¿åŠ¨å­¦ï¼ˆå¦‚è‡ªè¡Œè½¦æ¨¡å‹ï¼‰çº¦æŸçš„å¯è¡Œè·¯å¾„ã€‚\n*   **ä¼˜ç‚¹**ï¼šåœ¨æ„ŸçŸ¥ä¿¡æ¯å®Œæ•´ã€ç²¾ç¡®æ—¶ï¼Œèƒ½ç”Ÿæˆè¿‘ä¹æœ€ä¼˜çš„è·¯å¾„ï¼Œç†è®ºå®Œå¤‡ã€‚\n*   **å±€é™æ€§ï¼ˆæœ¬æ–‡é’ˆå¯¹çš„ç—›ç‚¹ï¼‰**ï¼š\n    1.  **å¯¹æ„ŸçŸ¥æ•æ„Ÿ**ï¼šä¾èµ–å®Œç¾çš„ç¯å¢ƒè¡¨ç¤ºï¼ˆå¦‚éšœç¢ç‰©è½®å»“ï¼‰ï¼Œåœ¨ç°å®çš„éƒ¨åˆ†å¯è§‚æµ‹ã€é®æŒ¡åœºæ™¯ä¸‹è§„åˆ’è„†å¼±ï¼Œæ˜“å¯¼è‡´ç¢°æ’ï¼ˆå¦‚å›¾1æ‰€ç¤ºï¼‰ã€‚\n    2.  **è®¡ç®—æˆæœ¬é«˜**ï¼šä¾èµ–åœ¨çº¿æœç´¢ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸‹è®¡ç®—è€—æ—¶ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚\n    3.  **ç³»ç»Ÿé›†æˆå¤æ‚**ï¼šéœ€è¦ç‹¬ç«‹çš„å®šä½ã€è·¯å¾„è·Ÿè¸ªç­‰æ¨¡å—ï¼Œå¼•å…¥äº†é¢å¤–çš„è¯¯å·®ç´¯ç§¯å’Œç³»ç»Ÿå¤æ‚æ€§ã€‚\n\n#### ç¬¬äºŒé˜¶æ®µï¼šAIé©±åŠ¨è§„åˆ’æ–¹æ³•çš„å…´èµ·\næ­¤é˜¶æ®µåˆå¯åˆ†ä¸ºä¸¤ä¸ªä¸»è¦å‘å±•æ–¹å‘ï¼š\n\n**1. å¼€ç¯å­¦ä¹ èŒƒå¼**\n*   **ä»£è¡¨å·¥ä½œ**ï¼šç›‘ç£æ¨¡ä»¿å­¦ä¹  (Ahn et al., 2022)ã€VAD (Jiang et al., 2023)ã€æ‰©æ•£æ¨¡å‹ (Chi et al., 2023)ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šä»ä¸“å®¶æ•°æ®ï¼ˆå¦‚ç»å…¸è§„åˆ’å™¨ç”Ÿæˆçš„è½¨è¿¹ï¼‰ä¸­å­¦ä¹ ï¼Œç›´æ¥é¢„æµ‹è·¯å¾„æˆ–è½¨è¿¹ã€‚\n*   **æ”¹è¿›**ï¼š**å®ç°äº†ç«¯åˆ°ç«¯çš„é¢„æµ‹ï¼Œæ¨ç†é€Ÿåº¦å¿«ï¼ˆå•æ¬¡å‰å‘ä¼ æ’­ï¼‰**ï¼Œé¿å…äº†åœ¨çº¿æœç´¢ã€‚\n*   **å±€é™æ€§**ï¼š\n    *   **åˆ†å¸ƒåç§»**ï¼šè®­ç»ƒä¸æµ‹è¯•æ•°æ®åˆ†å¸ƒä¸ä¸€è‡´æ—¶ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚\n    *   **åŠ¨æ€å¯è¡Œæ€§æ— ä¿è¯**ï¼šé¢„æµ‹çš„è·¯å¾„å¯èƒ½ä¸æ»¡è¶³è½¦è¾†è¿åŠ¨å­¦çº¦æŸï¼Œå¯¼è‡´æ— æ³•è¢«åº•å±‚æ§åˆ¶å™¨è·Ÿè¸ªã€‚\n    *   **ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®**ã€‚\n\n**2. é—­ç¯å­¦ä¹ èŒƒå¼ï¼ˆæœ¬æ–‡æ‰€å±æ–¹å‘ï¼‰**\n*   **æ—©æœŸæ¢ç´¢**ï¼šä¸€äº›ç ”ç©¶å°†RLç”¨äºæœºå™¨äººå¯¼èˆª (Zhu and Zhang, 2021) æˆ–ç®€åŒ–æ³Šè½¦ä»»åŠ¡ (Lazzaroni et al., 2023)ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šå°†è§„åˆ’å»ºæ¨¡ä¸º**åºåˆ—å†³ç­–é—®é¢˜**ï¼Œæ™ºèƒ½ä½“åœ¨ä¸ä»¿çœŸç¯å¢ƒçš„äº¤äº’ä¸­é€šè¿‡è¯•é”™å­¦ä¹ ç­–ç•¥ã€‚\n*   **æ”¹è¿›**ï¼š\n    *   **æ˜¾å¼å¤„ç†åŠ¨æ€çº¦æŸ**ï¼šåœ¨ç¯å¢ƒä¸­é›†æˆè‡ªè¡Œè½¦æ¨¡å‹ï¼Œè¿«ä½¿ç­–ç•¥å­¦ä¹ ç¬¦åˆè½¦è¾†è¿åŠ¨å­¦çš„åŠ¨ä½œã€‚\n    *   **æ›´å¥½çš„æ³›åŒ–æ€§**ï¼šé€šè¿‡äº¤äº’å­¦ä¹ ï¼Œå¯¹æœªè§è¿‡çš„åˆå§‹çŠ¶æ€å’Œéšœç¢ç‰©å¸ƒå±€æœ‰æ›´å¼ºé²æ£’æ€§ã€‚\n*   **å‰æœŸå·¥ä½œçš„å±€é™æ€§**ï¼š\n    *   **åœºæ™¯è¿‡äºç®€åŒ–**ï¼šç¯å¢ƒä¸å¤ŸçœŸå®ï¼Œæœªå……åˆ†è€ƒè™‘ç°å®æ³Šè½¦ä¸­çš„**æåº¦å—é™ç©ºé—´**ã€‚\n    *   **ç¼ºä¹æ ‡å‡†åŸºå‡†**ï¼šæ²¡æœ‰å…¬è®¤çš„ã€åæ˜ çœŸå®æŒ‘æˆ˜çš„è¯„ä¼°æ•°æ®é›†ï¼Œé˜»ç¢äº†å…¬å¹³æ¯”è¾ƒå’Œè¿›å±•ã€‚\n    *   **è®­ç»ƒæŒ‘æˆ˜**ï¼šç¨€ç–å¥–åŠ±ã€é•¿è§†é‡ã€æ¢ç´¢æ•ˆç‡ä½ç­‰é—®é¢˜ä½¿å¾—åœ¨å¤æ‚åœºæ™¯ä¸­è®­ç»ƒæœ‰æ•ˆçš„RLç­–ç•¥éå¸¸å›°éš¾ã€‚\n\n#### ç¬¬ä¸‰é˜¶æ®µï¼šæœ¬æ–‡å·¥ä½œâ€”â€”é¢å‘çœŸå®çº¦æŸçš„é—­ç¯RLè§„åˆ’\næœ¬æ–‡åœ¨å‰è¿°é—­ç¯å­¦ä¹ èŒƒå¼çš„åŸºç¡€ä¸Šï¼Œåšå‡ºäº†å¤šé¡¹å…³é”®æ”¹è¿›ï¼Œæ¨åŠ¨äº†è¯¥æ–¹å‘çš„å®ç”¨åŒ–ï¼š\n\n1.  **é—®é¢˜å®šä¹‰ä¸å»ºæ¨¡çš„æ·±åŒ–**ï¼š\n    *   **æ”¹è¿›**ï¼šä¸ä»…å°†è§„åˆ’è§†ä¸ºé—­ç¯å†³ç­–é—®é¢˜ï¼Œæ›´**ä¸¥æ ¼åœ°é”šå®šåœ¨è‡ªè¡Œè½¦æ¨¡å‹åŠ¨åŠ›å­¦**ä¸Šï¼Œå¹¶è®¾è®¡äº†åŒ…å«ä½ç§»å’Œè½¬å‘è§’å˜åŒ–çš„**ç¦»æ•£åŠ¨ä½œç©ºé—´**ï¼Œç¡®ä¿äº†ç­–ç•¥è¾“å‡ºåœ¨ç‰©ç†ä¸Šæ˜¯å¯æ‰§è¡Œçš„ã€‚\n\n2.  **è®­ç»ƒæ–¹æ³•è®ºçš„ç³»ç»Ÿæ€§åˆ›æ–°**ï¼š\n    *   **æ”¹è¿›**ï¼šé’ˆå¯¹ç¨€ç–å¥–åŠ±å’Œæ¢ç´¢éš¾é¢˜ï¼Œæå‡ºäº†**ç»„åˆè§£å†³æ–¹æ¡ˆ**ï¼š\n        *   **è¯¾ç¨‹å­¦ä¹ **ï¼šä»æ˜“åˆ°éš¾ï¼ˆä»é è¿‘ç›®æ ‡ä½å§¿åˆ°è¿œç¦»ï¼‰é€æ­¥è®­ç»ƒï¼Œç¨³å®šå­¦ä¹ è¿‡ç¨‹ï¼Œå‡å°‘æ— æ•ˆæ¢ç´¢ã€‚\n        *   **åŠ¨ä½œåˆ†å—æœºåˆ¶**ï¼šå°†å¤šä¸ªä½çº§åŠ¨ä½œç»„åˆæˆä¸€ä¸ªâ€œå®åŠ¨ä½œâ€æ‰§è¡Œã€‚**è¿™å¹³è¡¡äº†æ¢ç´¢æ•ˆç‡ï¼ˆå‡å°‘å†³ç­–æ­¥æ•°ï¼‰ä¸æ§åˆ¶ç²¾åº¦ï¼ˆä¿ç•™ç»†ç²’åº¦åŠ¨ä½œï¼‰çš„çŸ›ç›¾**ï¼Œæ˜¯è§£å†³é•¿è§†é‡ã€ç²¾ç»†æ“æ§ä»»åŠ¡çš„å…³é”®ã€‚\n\n3.  **åŸºå‡†ä¸è¯„ä¼°çš„æ ‡å‡†åŒ–è´¡çŒ®**ï¼š\n    *   **æ”¹è¿›**ï¼š**åˆ›å»ºäº†ParkBenchåŸºå‡†**ï¼ŒåŒ…å«51ä¸ªä»çœŸå®æ•°æ®æå–çš„ã€é«˜åº¦å—é™çš„å€’è½¦å…¥åº“åœºæ™¯ã€‚è¿™è§£å†³äº†è¯¥é¢†åŸŸé•¿æœŸç¼ºä¹**å…·æœ‰æŒ‘æˆ˜æ€§ã€æ ‡å‡†åŒ–æµ‹è¯•åºŠ**çš„é—®é¢˜ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†å…¬å¹³æ¯”è¾ƒçš„åŸºç¡€ã€‚\n\n4.  **ç³»ç»Ÿé›†æˆä¸å®ç”¨åŒ–è®¾è®¡**ï¼š\n    *   **æ”¹è¿›**ï¼šè¾“å…¥è¡¨ç¤ºä¸ç»å…¸è§„åˆ’å™¨ï¼ˆHybrid A*ï¼‰ä¿æŒä¸€è‡´ï¼ˆä½å§¿+éšœç¢ç‰©è½®å»“ï¼‰ï¼Œä½¿å…¶èƒ½ä½œä¸ºâ€œå³æ’å³ç”¨â€æ¨¡å—æ›¿æ¢ç°æœ‰è‡ªåŠ¨é©¾é©¶æ ˆä¸­çš„è§„åˆ’æ¨¡å—ã€‚**è½»é‡åŒ–çš„ç‰¹å¾æå–å™¨è®¾è®¡**ï¼ˆä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ï¼‰å…¼é¡¾äº†æ€§èƒ½ä¸å®æ—¶æ€§éœ€æ±‚ã€‚\n\n#### ç¬¬å››é˜¶æ®µï¼ˆæœªæ¥æ–¹å‘ï¼‰ï¼šæ··åˆæ–¹æ³•ä¸æ‰©å±•\n*   **ä»£è¡¨æ–¹å‘**ï¼šç»“åˆç»å…¸æ–¹æ³•ä¸å­¦ä¹ æ–¹æ³•çš„ä¼˜åŠ¿ (Shan et al., 2023; Jiang et al., 2025)ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šç”¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚ç¥ç»ç½‘ç»œï¼‰ä¸ºç»å…¸è§„åˆ’å™¨æä¾›æ›´å¥½çš„å¯å‘å¼å‡½æ•°ã€å¼•å¯¼æœç´¢æˆ–åŠ é€Ÿå‰ªæã€‚\n*   **æ½œåœ¨æ”¹è¿›**ï¼šæœ‰æœ›åœ¨**ä¿è¯æœ€ä¼˜æ€§/å®Œå¤‡æ€§**ï¼ˆæ¥è‡ªç»å…¸æ–¹æ³•ï¼‰å’Œ**æå‡è®¡ç®—æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›**ï¼ˆæ¥è‡ªå­¦ä¹ æ–¹æ³•ï¼‰ä¹‹é—´å–å¾—æ›´å¥½å¹³è¡¡ã€‚æœ¬æ–‡ä½œè€…ä¹ŸæŒ‡å‡ºè¿™æ˜¯æœ‰ä»·å€¼çš„æœªæ¥æ–¹å‘ã€‚\n\n### ç€é‡å‘å±•çš„å‡ ä¸ªä¸åŒæ–¹å‘æ€»ç»“\n\n1.  **ä»å¼€ç¯åˆ°é—­ç¯çš„å­¦ä¹ èŒƒå¼**ï¼šä»å•çº¯æ¨¡ä»¿æ•°æ®ï¼Œè½¬å‘åœ¨ä¸ç¯å¢ƒåŠ¨åŠ›å­¦äº¤äº’ä¸­å­¦ä¹ ï¼Œä»¥**è§£å†³åˆ†å¸ƒåç§»å’ŒåŠ¨æ€å¯è¡Œæ€§**é—®é¢˜ã€‚\n2.  **ä»ç®€åŒ–ä»¿çœŸåˆ°çœŸå®çº¦æŸå»ºæ¨¡**ï¼šç ”ç©¶åœºæ™¯ä»å¼€æ”¾ã€ç®€å•ç¯å¢ƒï¼Œæ¼”è¿›åˆ°**é«˜åº¦å—é™ã€å¤æ‚çš„çœŸå®æ³Šè½¦åœºæ™¯**ï¼Œå¯¹ç®—æ³•çš„é²æ£’æ€§å’Œç²¾ç»†æ§åˆ¶èƒ½åŠ›æå‡ºæ›´é«˜è¦æ±‚ã€‚\n3.  **ä»å­¤ç«‹ç®—æ³•åˆ°ç³»ç»Ÿé›†æˆ**ï¼šç ”ç©¶ç„¦ç‚¹ä»è®¾è®¡å­¤ç«‹ç®—æ³•ï¼Œè½¬å‘è€ƒè™‘å¦‚ä½•**ä¸ç°æœ‰è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿé›†æˆ**ï¼ˆè¾“å…¥/è¾“å‡ºå…¼å®¹æ€§ã€å®æ—¶æ€§ï¼‰ï¼Œå¹¶è‡´åŠ›äº**å‡å°‘å¯¹é¢å¤–æ¨¡å—ï¼ˆå¦‚ç²¾å‡†å®šä½ï¼‰çš„ä¾èµ–**ã€‚\n4.  **ä»é€šç”¨å¯¼èˆªåˆ°ç‰¹å®šä»»åŠ¡**ï¼šæ—©æœŸRLå¯¼èˆªå¤šé’ˆå¯¹å·®åˆ†é©±åŠ¨æœºå™¨äººï¼Œæœ¬æ–‡ç­‰å·¥ä½œåˆ™**ä¸“é—¨é’ˆå¯¹å…·æœ‰éå®Œæ•´çº¦æŸï¼ˆé˜¿å…‹æ›¼è½¬å‘ï¼‰çš„è½¦è¾†ï¼Œå¹¶å¼ºè°ƒç²¾ç¡®çš„ç»ˆç«¯ä½å§¿ï¼ˆæ³Šè½¦è¦æ±‚ï¼‰**ï¼Œä»»åŠ¡ç‰¹å¼‚æ€§æ›´å¼ºã€‚\n5.  **ä»æ–¹æ³•åˆ›æ–°åˆ°åŸºå‡†å»ºè®¾**ï¼šéšç€é¢†åŸŸå‘å±•ï¼Œ**æ„å»ºé«˜è´¨é‡ã€æŒ‘æˆ˜æ€§çš„åŸºå‡†**ï¼ˆå¦‚æœ¬æ–‡çš„ParkBenchï¼‰æˆä¸ºæ¨åŠ¨é¢†åŸŸè¿›æ­¥çš„å…³é”®åŸºç¡€è®¾æ–½ï¼Œä½¿è¯„ä¼°æ›´å…¬å¹³ã€å¯é‡å¤ã€‚\n\n**ç»“è®º**ï¼šæœ¬æ–‡æ ‡å¿—ç€åŸºäºå¼ºåŒ–å­¦ä¹ çš„è·¯å¾„è§„åˆ’ç ”ç©¶è¿›å…¥äº†ä¸€ä¸ªæ–°é˜¶æ®µï¼Œå³**é¢å‘çœŸå®ä¸–ç•Œé«˜çº¦æŸåœºæ™¯ã€å…·å¤‡ç³»ç»Ÿå®ç”¨åŒ–æ½œåŠ›çš„æ·±åº¦é—­ç¯å­¦ä¹ **ã€‚å®ƒé€šè¿‡**åˆ›æ–°çš„è®­ç»ƒç­–ç•¥ï¼ˆè¯¾ç¨‹å­¦ä¹ +åŠ¨ä½œåˆ†å—ï¼‰** å’Œ**åŸºç¡€æ€§åŸºå‡†å»ºè®¾**ï¼Œæ˜¾è‘—æå‡äº†DRLåœ¨å¤æ‚æ³Šè½¦ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ˆæˆåŠŸç‡+96%ï¼‰å’Œæ•ˆç‡ï¼Œä¸ºå®Œå…¨æ•°æ®é©±åŠ¨çš„è‡ªåŠ¨é©¾é©¶è§„åˆ’å™¨æä¾›äº†å¼ºæœ‰åŠ›çš„å¯è¡Œæ€§è¯æ˜ã€‚æœªæ¥çš„å‘å±•å°†æ²¿ç€**æ‰©å±•åœºæ™¯ï¼ˆå¹³è¡Œæ³Šè½¦ç­‰ï¼‰ã€è‡ªåŠ¨åŒ–è¯¾ç¨‹å­¦ä¹ ã€ä»¥åŠä¸ä¼ ç»Ÿè§„åˆ’æ–¹æ³•çš„æ·±åº¦èåˆ**ç­‰æ–¹å‘ç»§ç»­æ¼”è¿›ã€‚",
            "timestamp": "2026-02-03T15:55:29.649004",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œä½œè€…å›¢é˜Ÿåœ¨**â€œçº¦æŸåœè½¦åœºæ™¯ä¸­çš„è·¯å¾„è§„åˆ’â€** è¿™ä¸€ç ”ç©¶æ–¹å‘ä¸Šçš„**ç›´æ¥å‰ä½œå¹¶æœªåœ¨æ–‡ä¸­æ˜ç¡®åˆ—å‡º**ã€‚è®ºæ–‡çš„â€œç›¸å…³å·¥ä½œâ€éƒ¨åˆ†ç»¼è¿°äº†è¯¥é¢†åŸŸçš„ç»å…¸æ–¹æ³•å’Œç°æœ‰AIæ–¹æ³•ï¼Œä½†å¹¶æœªæŒ‡æ˜å“ªäº›æ˜¯æœ¬æ–‡ä½œè€…å›¢é˜Ÿä¹‹å‰å‘è¡¨çš„å·¥ä½œã€‚\n\nç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥ä»è®ºæ–‡å¼•ç”¨çš„æ–‡çŒ®ä¸­ï¼Œæ¢³ç†å‡ºä¸æœ¬æ–‡æ–¹æ³•è®ºç´§å¯†ç›¸å…³çš„**å…³é”®å‰ä½œ**ï¼ˆå¹¶éä¸€å®šæ˜¯åŒä¸€ä½œè€…å›¢é˜Ÿï¼Œä½†ä¸ºæœ¬æ–‡æä¾›äº†æ ¸å¿ƒæ€è·¯ï¼‰ã€‚è¿™äº›å·¥ä½œä¸æœ¬æ–‡çš„åŒºåˆ«å’Œæ”¹è¿›æ˜¯æœ¬æ–‡åˆ›æ–°çš„é‡è¦åŸºç¡€ã€‚\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†åˆ†æäº†è¿™äº›å…³é”®å‰ä½œåŠå…¶ä¸æœ¬æ–‡çš„å¯¹æ¯”ï¼š\n\n| å‰ä½œåç§° / ç±»åˆ« | æ ¸å¿ƒæ–¹æ³• | ä¸æœ¬æ–‡çš„åŒºåˆ« | æœ¬æ–‡çš„ä¸»è¦æ”¹å–„ä¸å…·ä½“æ”¹åŠ¨ |\n| :--- | :--- | :--- | :--- |\n| **ç»å…¸è§„åˆ’å™¨ï¼šHybrid A*** <br> (Dolgov et al., 2008) | ç»“åˆç½‘æ ¼æœç´¢ä¸è¿ç»­çŠ¶æ€æ’å€¼çš„æ··åˆçŠ¶æ€A*ç®—æ³•ï¼Œä½¿ç”¨Reeds-Sheppæ›²çº¿ç­‰å¯å‘å¼æ–¹æ³•åŠ é€Ÿï¼Œç”Ÿæˆæ»¡è¶³è½¦è¾†è¿åŠ¨å­¦çš„è·¯å¾„ã€‚ | 1. **å¼€ç¯è§„åˆ’**ï¼šå‡è®¾ç¯å¢ƒä¿¡æ¯å®Œå…¨ã€ç²¾ç¡®å·²çŸ¥ï¼Œè§„åˆ’ä¸æ‰§è¡Œåˆ†ç¦»ã€‚<br>2. **åœ¨çº¿è®¡ç®—é‡å¤§**ï¼šæ¯æ¬¡è§„åˆ’éƒ½éœ€é‡æ–°æœç´¢ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸­å®æ—¶æ€§å·®ã€‚<br>3. **ä¾èµ–å®Œæ•´æ¨¡å—**ï¼šéœ€ä¸å®šä½ã€è·Ÿè¸ªç­‰æ¨¡å—é›†æˆï¼Œè¯¯å·®ä¼šç´¯ç§¯ã€‚<br>4. **å¯¹æ„ŸçŸ¥ä¸ç¡®å®šæ€§è„†å¼±**ï¼šå¦‚å›¾1æ‰€ç¤ºï¼Œåœ¨éƒ¨åˆ†å¯è§‚æµ‹çš„çº¦æŸåœºæ™¯ä¸­æ˜“è§„åˆ’å‡ºç¢°æ’è·¯å¾„ã€‚ | **æ ¹æœ¬æ€§èŒƒå¼è½¬å˜**ï¼š<br>1. **é—­ç¯å­¦ä¹ ä¸æ‰§è¡Œ**ï¼šå°†è§„åˆ’é—®é¢˜å»ºæ¨¡ä¸ºåŸºäºè‡ªè¡Œè½¦æ¨¡å‹çš„**åºåˆ—å†³ç­–é—®é¢˜**ï¼Œæ™ºèƒ½ä½“åœ¨ä»¿çœŸä¸­é€šè¿‡è¯•é”™å­¦ä¹ ï¼Œç›´æ¥è¾“å‡ºæ§åˆ¶åŠ¨ä½œï¼Œå®ç°äº†è§„åˆ’ä¸æ§åˆ¶çš„é—­ç¯ã€‚<br>2. **å®æ—¶æ€§å¼º**ï¼šè®­ç»ƒå¥½çš„ç­–ç•¥åœ¨æµ‹è¯•æ—¶ä»…éœ€å•æ¬¡å‰å‘ä¼ æ’­ï¼Œ**è®¡ç®—å¼€é”€æä½**ï¼ˆè®ºæ–‡æ˜¾ç¤ºè§„åˆ’æ—¶é—´ä»Hybrid A*çš„0.42ç§’é™è‡³0.20ç§’ï¼‰ã€‚<br>3. **å¯¹æ„ŸçŸ¥æ›´é²æ£’**ï¼šè¾“å…¥ä½¿ç”¨ç¨€ç–çš„éšœç¢ç‰©è½®å»“ç‚¹ï¼ˆæ¨¡æ‹Ÿæ¿€å…‰é›·è¾¾ï¼‰ï¼Œå¹¶è®¾è®¡äº†ç‰¹å¾æå–å™¨ï¼ˆå«æ³¨æ„åŠ›æœºåˆ¶ï¼‰æ¥å¤„ç†è¿™ç§è¡¨ç¤ºï¼Œ**é™ä½äº†å¯¹ç†æƒ³åŒ–ã€ç»“æ„åŒ–æ„ŸçŸ¥çš„ä¾èµ–**ã€‚<br>4. **æ€§èƒ½å¤§å¹…æå‡**ï¼šåœ¨è‡ªå»ºåŸºå‡†ä¸Šï¼ŒæˆåŠŸç‡æå‡**+96%**ï¼ˆä»47.1%åˆ°92.2%ï¼‰ï¼Œæ•ˆç‡ï¼ˆæ—…è¡Œè·ç¦»ï¼‰æå‡**+52%**ã€‚ |\n| **å¼€ç¯AIè§„åˆ’æ–¹æ³• (e.g., VAD, æ‰©æ•£æ¨¡å‹)** <br> (Jiang et al., 2023; Chi et al., 2023) | **ç›‘ç£å­¦ä¹ /æ¨¡ä»¿å­¦ä¹ **ï¼šä»ä¸“å®¶æ•°æ®ä¸­å­¦ä¹ ï¼Œç›´æ¥é¢„æµ‹ä¸€ç³»åˆ—è·¯å¾„ç‚¹ï¼ˆwaypointsï¼‰ã€‚å±äº**å¼€ç¯**é¢„æµ‹ï¼Œä¸ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’å¼åé¦ˆã€‚ | 1. **åˆ†å¸ƒåç§»é—®é¢˜**ï¼šè®­ç»ƒä¸æµ‹è¯•æ•°æ®åˆ†å¸ƒä¸åŒæ—¶ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚<br>2. **åŠ¨æ€å¯è¡Œæ€§æ— ä¿è¯**ï¼šé¢„æµ‹çš„è·¯å¾„ç‚¹åºåˆ—å¯èƒ½æ— æ³•è¢«è½¦è¾†åŠ¨åŠ›å­¦æ¨¡å‹å®é™…è·Ÿè¸ªï¼Œå°¤å…¶åœ¨éœ€è¦å¤šæ¬¡å€’è½¦çš„å¤æ‚æ³Šè½¦åœºæ™¯ä¸­ã€‚<br>3. **éœ€è¦å¤§é‡ä¸“å®¶æ•°æ®**ï¼ˆå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹ï¼‰ã€‚ | **ä»å¼€ç¯åˆ°é—­ç¯çš„æ”¹è¿›**ï¼š<br>1. **å¼•å…¥é—­ç¯å¼ºåŒ–å­¦ä¹ **ï¼šé€šè¿‡RLåœ¨ä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œ**åºåˆ—å†³ç­–è®­ç»ƒ**ï¼Œä½¿æ™ºèƒ½ä½“å­¦ä¼šè€ƒè™‘åŠ¨ä½œçš„é•¿æœŸåæœå’Œè½¦è¾†åŠ¨åŠ›å­¦çº¦æŸï¼Œä»æ ¹æœ¬ä¸Šæå‡äº†**æ³›åŒ–æ€§å’Œé²æ£’æ€§**ã€‚<br>2. **æ˜¾å¼å»ºæ¨¡åŠ¨åŠ›å­¦**ï¼šå°†è‡ªè¡Œè½¦æ¨¡å‹ä½œä¸ºç¯å¢ƒæ›´æ–°çš„åŸºç¡€ï¼Œç¡®ä¿å­¦åˆ°çš„ç­–ç•¥**å¤©ç”Ÿæ»¡è¶³è½¦è¾†è¿åŠ¨å­¦**ï¼Œç”Ÿæˆçš„è·¯å¾„å¿…ç„¶æ˜¯åŠ¨æ€å¯è¡Œçš„ã€‚<br>3. **æ— éœ€ä¸“å®¶æ¼”ç¤º**ï¼šé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°å’Œè¯¾ç¨‹å­¦ä¹ ï¼Œè®©æ™ºèƒ½ä½“ä»é›¶å¼€å§‹è‡ªä¸»å­¦ä¹ ç­–ç•¥ã€‚ |\n| **æ—©æœŸé—­ç¯RLæ³Šè½¦ç ”ç©¶** <br> (e.g., Lazzaroni et al., 2023) | åœ¨ç®€åŒ–ä»¿çœŸç¯å¢ƒï¼ˆå¦‚CARLAï¼‰ä¸­ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ³Šè½¦ã€‚ | 1. **åœºæ™¯è¿‡äºç®€åŒ–**ï¼šç¯å¢ƒä¸å¤Ÿå¤æ‚ï¼Œæœªèƒ½å……åˆ†ä½“ç°çœŸå®ä¸–ç•Œä¸­**ç©ºé—´æåº¦å—é™**çš„æ³Šè½¦æŒ‘æˆ˜ã€‚<br>2. **ç¼ºä¹æ ‡å‡†åŸºå‡†**ï¼šéš¾ä»¥è¿›è¡Œå…¬å¹³ã€ç³»ç»Ÿçš„è¯„ä¼°å’Œæ¯”è¾ƒã€‚ | **é’ˆå¯¹çœŸå®çº¦æŸåœºæ™¯çš„æ·±åº¦ä¼˜åŒ–**ï¼š<br>1. **æå‡ºä¸“ç”¨åŸºå‡† `ParkBench`**ï¼šåŒ…å«51ä¸ªä»çœŸå®æ•°æ®æå–çš„ã€å…·æœ‰æŒ‘æˆ˜æ€§çš„å€’è½¦å…¥åº“åœºæ™¯ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸ**æ ‡å‡†åŒ–è¯„ä¼°åŸºå‡†çš„ç©ºç™½**ã€‚<br>2. **ç³»ç»ŸåŒ–çš„è®­ç»ƒæ–¹æ³•è®º**ï¼š<br>   - **è¯¾ç¨‹å­¦ä¹ **ï¼šè®¾è®¡8é˜¶æ®µè¯¾ç¨‹ï¼Œä»æ¥è¿‘ç›®æ ‡ç‚¹çš„ç®€å•çŠ¶æ€å¼€å§‹ï¼Œé€æ­¥å¢åŠ åˆå§‹ä½å§¿çš„éš¾åº¦å’Œæ‰°åŠ¨ï¼Œ**ç¨³å®šè®­ç»ƒå¹¶æå‡æˆåŠŸç‡**ã€‚<br>   - **åŠ¨ä½œåˆ†å—æœºåˆ¶**ï¼šåˆ›æ–°æ€§åœ°å°†`åŠ¨ä½œåˆ†å—`ä½œä¸ºç¯å¢ƒåŒ…è£…å™¨ï¼Œè®©æ™ºèƒ½ä½“ä¸€æ¬¡è¾“å‡ºä¸€ä¸ªçŸ­åŠ¨ä½œåºåˆ—ï¼ˆå®åŠ¨ä½œï¼‰ã€‚è¿™**å¹³è¡¡äº†æ¢ç´¢æ•ˆç‡ï¼ˆç¼©çŸ­æœ‰æ•ˆè§„åˆ’è§†é‡ï¼‰ä¸æ§åˆ¶ç²¾åº¦**ï¼Œè§£å†³äº†ç»†ç²’åº¦åŠ¨ä½œæ¢ç´¢éš¾ã€ç²—ç²’åº¦åŠ¨ä½œæ˜“ç¢°æ’çš„çŸ›ç›¾ã€‚ |\n| **ç”¨äºå¯¼èˆªçš„é€šç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ** <br> (Zhu and Zhang, 2021 ç­‰ç»¼è¿°) | å°†DRLå¹¿æ³›åº”ç”¨äºç§»åŠ¨æœºå™¨äººå¯¼èˆªï¼Œä½†å¤§å¤šé’ˆå¯¹å·®é€Ÿé©±åŠ¨æ¨¡å‹ã€‚ | 1. **æœºå™¨äººæ¨¡å‹ä¸åŒ**ï¼šå¤šæ•°ç ”ç©¶é’ˆå¯¹çµæ´»æ€§é«˜çš„å·®é€Ÿé©±åŠ¨æœºå™¨äººï¼Œè€Œæ³Šè½¦éœ€è¦**éå®Œæ•´çº¦æŸ**çš„è‡ªè¡Œè½¦/é˜¿å…‹æ›¼è½¬å‘æ¨¡å‹ã€‚<br>2. **ä»»åŠ¡ç›®æ ‡ä¸åŒ**ï¼šé€šç”¨å¯¼èˆªå¸¸ä»¥åˆ°è¾¾æŸä¸ªä½ç½®åŒºåŸŸä¸ºç›®æ ‡ï¼Œè€Œæ³Šè½¦è¦æ±‚**ç²¾ç¡®çš„ç»ˆç«¯ä½å§¿**ï¼ˆä½ç½®å’Œæœå‘ï¼‰ã€‚ | **é’ˆå¯¹ç‰¹å®šä»»åŠ¡ä¸æ¨¡å‹çš„å®šåˆ¶åŒ–**ï¼š<br>1. **ä¸¥æ ¼çš„ä»»åŠ¡å®šä¹‰**ï¼šå°†çŠ¶æ€æ˜ç¡®ä¸º`(x, y, Î¸, Î´)`ï¼Œç›®æ ‡ä¸º`(x_goal, y_goal, Î¸_goal)`ï¼Œå¹¶åœ¨å¥–åŠ±å‡½æ•°ä¸­è®¾ç½®ä¸¥æ ¼çš„å®¹å·®ï¼Œ**å¼ºåˆ¶è¦æ±‚ç²¾ç¡®æ³Šå…¥**ã€‚<br>2. **åŸºäºè‡ªè¡Œè½¦æ¨¡å‹çš„ä»¿çœŸ**ï¼šæ•´ä¸ªè®­ç»ƒç¯å¢ƒåŸºäºç²¾ç¡®çš„è‡ªè¡Œè½¦æ¨¡å‹åŠ¨åŠ›å­¦æ„å»ºï¼Œç¡®ä¿å­¦ä¹ ä¸çœŸå®è½¦è¾†ç‰¹æ€§ä¸€è‡´ã€‚ |\n\n### æ€»ç»“ï¼šæœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ä¸æ”¹è¿›\n\næœ¬æ–‡å¹¶éå¯¹æŸç¯‡ç‰¹å®šâ€œå‰ä½œâ€çš„ç®€å•æ”¹è¿›ï¼Œè€Œæ˜¯**é’ˆå¯¹â€œçº¦æŸæ³Šè½¦è·¯å¾„è§„åˆ’â€è¿™ä¸€å…·ä½“é—®é¢˜ï¼Œè¿›è¡Œäº†ä¸€æ¬¡ç³»ç»Ÿæ€§çš„æ–¹æ³•é‡æ„ä¸å·¥ç¨‹å®ç°**ã€‚å…¶æ ¸å¿ƒæ”¹è¿›ä½“ç°åœ¨ä¸‰ä¸ªå±‚é¢ï¼š\n\n1.  **æ–¹æ³•è®ºå±‚é¢**ï¼š**åšå®šé‡‡ç”¨å¹¶ä¼˜åŒ–äº†é—­ç¯æ·±åº¦å¼ºåŒ–å­¦ä¹ èŒƒå¼**ï¼Œå…‹æœäº†ç»å…¸è§„åˆ’å™¨å®æ—¶æ€§å·®ã€ä¾èµ–å®Œç¾æ„ŸçŸ¥çš„é—®é¢˜ï¼Œä¹Ÿè§£å†³äº†å¼€ç¯AIæ–¹æ³•å­˜åœ¨åˆ†å¸ƒåç§»ã€æ— æ³•ä¿è¯åŠ¨æ€å¯è¡Œæ€§çš„ç¼ºé™·ã€‚\n2.  **æŠ€æœ¯å®ç°å±‚é¢**ï¼šæå‡ºäº†**åŠ¨ä½œåˆ†å—åŒ…è£…å™¨ + å¤šé˜¶æ®µè¯¾ç¨‹å­¦ä¹ **çš„ç»„åˆè®­ç»ƒç­–ç•¥ï¼Œè¿™æ˜¯ç¨³å®šã€é«˜æ•ˆè®­ç»ƒå‡ºé«˜æ€§èƒ½æ³Šè½¦ç­–ç•¥çš„å…³é”®æŠ€æœ¯åˆ›æ–°ã€‚\n3.  **ç”Ÿæ€è´¡çŒ®å±‚é¢**ï¼šåˆ›å»ºå¹¶å¼€æºäº†**`ParkBench`åŸºå‡†æµ‹è¯•é›†ä¸ä»¿çœŸç¯å¢ƒ**ï¼Œä¸ºé¢†åŸŸç ”ç©¶æä¾›äº†è‡³å…³é‡è¦çš„å…¬å…±å¹³å°å’Œè¯„ä»·æ ‡å‡†ï¼Œæ¨åŠ¨äº†ç ”ç©¶çš„å¯å¤ç°æ€§ä¸å¯æ¯”æ€§ã€‚\n\nå› æ­¤ï¼Œæœ¬æ–‡çš„â€œå‰ä½œâ€æ›´åº”è¢«è§†ä¸ºä¸€ä¸ª**ç ”ç©¶é¢†åŸŸç°çŠ¶çš„é›†åˆ**ã€‚æœ¬æ–‡çš„è´¡çŒ®åœ¨äºè¯†åˆ«äº†è¯¥é¢†åŸŸçš„æ ¸å¿ƒç—›ç‚¹ï¼ˆç¼ºä¹åŸºå‡†ã€é—­ç¯RLè®­ç»ƒéš¾ï¼‰ï¼Œå¹¶ç»™å‡ºäº†ä¸€å¥—å®Œæ•´ã€æœ‰æ•ˆä¸”æ€§èƒ½å“è¶Šçš„è§£å†³æ–¹æ¡ˆã€‚",
            "timestamp": "2026-02-03T15:56:26.524602",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## è®ºæ–‡å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\næœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„è·¯å¾„è§„åˆ’æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè§£å†³å—é™åœè½¦åœºæ™¯ä¸‹çš„å®æ—¶è·¯å¾„è§„åˆ’é—®é¢˜ã€‚å…¶æ ¸å¿ƒæ–¹æ³•æ—¨åœ¨å…‹æœä¼ ç»Ÿç»å…¸è§„åˆ’å™¨ï¼ˆå¦‚Hybrid A*ï¼‰åœ¨æ„ŸçŸ¥ä¸ç¡®å®šæ€§ã€è®¡ç®—æˆæœ¬å’Œç³»ç»Ÿé›†æˆå¤æ‚æ€§æ–¹é¢çš„å±€é™ã€‚\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†è¯´æ˜äº†è®ºæ–‡æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€å…¶æŠ€æœ¯ç»†èŠ‚ã€æ•°å­¦åŸç†åŠå…·ä½“å‚æ•°ï¼š\n\n| å…³é”®æŠ€æœ¯æ–¹æ³• | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç†æ¨å¯¼ | å…·ä½“å‚æ•°ä¸è¯´æ˜ |\n| :--- | :--- | :--- |\n| **1. åŸºäºè‡ªè¡Œè½¦æ¨¡å‹çš„åºè´¯å†³ç­–é—®é¢˜å»ºæ¨¡** | **é—®é¢˜å®šä¹‰**ï¼šå°†è·¯å¾„è§„åˆ’å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚<br>**çŠ¶æ€ç©ºé—´ (State)**ï¼šè½¦è¾†çŠ¶æ€å®šä¹‰ä¸º \\( s = (x, y, \\theta, \\delta) \\)ï¼Œå…¶ä¸­ \\((x, y)\\) æ˜¯åè½´ä¸­å¿ƒçš„2Dåæ ‡ï¼Œ\\(\\theta\\) æ˜¯èˆªå‘è§’ï¼Œ\\(\\delta\\) æ˜¯å‰è½®è½¬å‘è§’ã€‚<br>**åŠ¨ä½œç©ºé—´ (Action)**ï¼šé‡‡ç”¨ç¦»æ•£æ§åˆ¶ç©ºé—´ï¼ŒåŒ…å«ä¸¤ä¸ªåˆ†é‡ï¼š<br>   - çºµå‘ä½ç§» \\(\\Delta s \\in \\{+ds, -ds, 0\\}\\) (å‰è¿›/åé€€/é™æ­¢ï¼Œè·ç¦»ä¸º \\(ds\\) ç±³)<br>   - å‰è½®è½¬å‘è§’å˜åŒ– \\(\\Delta \\delta \\in \\{+d\\delta, -d\\delta, 0\\}\\) (å·¦è½¬/å³è½¬/ä¸å˜ï¼Œå¼§åº¦)<br>**çŠ¶æ€è½¬ç§»æ¨¡å‹**ï¼šä½¿ç”¨**è‡ªè¡Œè½¦è¿åŠ¨å­¦æ¨¡å‹**è¿›è¡Œæ›´æ–°ï¼š<br>\\[<br>\\begin{aligned}<br>x_{k+1} &= x_k + \\Delta s_k \\cdot \\cos(\\theta_k) \\\\<br>y_{k+1} &= y_k + \\Delta s_k \\cdot \\sin(\\theta_k) \\\\<br>\\theta_{k+1} &= \\theta_k + \\frac{\\Delta s_k}{W_B} \\cdot \\tan(\\delta_k) \\\\<br>\\delta_k &= \\delta_{k-1} + \\Delta \\delta_k<br>\\end{aligned}<br>\\]<br>å…¶ä¸­ \\(W_B\\) ä¸ºè½¦è¾†è½´è·ã€‚è¯¥æ¨¡å‹ç¡®ä¿äº†å­¦ä¹ åˆ°çš„ç­–ç•¥å°Šé‡è½¦è¾†çš„éå®Œæ•´çº¦æŸã€‚ | - **è½¦è¾†å‚æ•°** (é™„å½•B): <br>  - è½´è· \\(W_B = 3.0 \\, \\text{m}\\)<br>  - è½¦å®½ \\(W = 2.0 \\, \\text{m}\\)<br>  - è½¦é•¿ \\(L = 4.95 \\, \\text{m}\\)<br>  - æœ€å¤§è½¬å‘è§’ \\(\\delta_{\\max} = 32^\\circ\\)<br>- **åŠ¨ä½œå‚æ•°** (é™„å½•D): <br>  - ç¦»æ•£åŠ¨ä½œé›†å…±8ä¸ªï¼Œä¸åŒ…æ‹¬(0,0)ç©ºé—²åŠ¨ä½œã€‚<br>  - é€Ÿåº¦ \\(v \\in \\{-0.8, 0, 0.8\\} \\, \\text{m/s}\\) (å¯¹åº” \\(\\Delta s = v \\cdot \\Delta t\\), \\(\\Delta t = 0.1s\\))<br>  - è½¬å‘è§’å¢é‡ \\(\\Delta \\delta \\in \\{-8^\\circ, 0, +8^\\circ\\}\\) |\n| **2. é’ˆå¯¹ç¨€ç–éšœç¢ç‰©è¡¨ç¤ºçš„è¾“å…¥ç‰¹å¾æå–å™¨** | **è¾“å…¥è®¾è®¡**ï¼šä¸ºä¸Hybrid A*å…¬å¹³å¯¹æ¯”ï¼Œè¾“å…¥ä¿¡æ¯ä¸å…¶ä¸€è‡´ï¼ŒåŒ…æ‹¬ï¼šè‡ªè½¦å½“å‰ä½å§¿ã€ç›®æ ‡ä½å§¿ã€ç¯å¢ƒæä¾›çš„éšœç¢ç‰©è½®å»“ç‚¹é›† \\(O = \\{(x^{obs,i}, y^{obs,i})\\}_{i=1}^N\\)ã€‚<br>**é¢„å¤„ç†**ï¼šå°†æ‰€æœ‰åæ ‡è½¬æ¢åˆ°ä»¥è‡ªè½¦ä¸ºä¸­å¿ƒçš„åæ ‡ç³»ï¼ˆè‡ªè½¦ä½äºåŸç‚¹ï¼‰ï¼Œå¹¶è¿›è¡Œå½’ä¸€åŒ–å’Œè·ç¦»æˆªæ–­ï¼ˆæ¨¡æ‹Ÿæ„ŸçŸ¥èŒƒå›´é™åˆ¶ï¼‰ã€‚<br>**ç½‘ç»œæ¶æ„** (å›¾3)ï¼šè®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§ç‰¹å¾æå–å™¨ï¼Œæ ¸å¿ƒæ˜¯ä½¿ç”¨**äº¤å‰æ³¨æ„åŠ›æœºåˆ¶**ï¼Œè¿«ä½¿è‡ªè½¦çŠ¶æ€ï¼ˆæŸ¥è¯¢ï¼ŒQueryï¼‰ä¸éšœç¢ç‰©ç‚¹é›†ï¼ˆé”®å’Œå€¼ï¼ŒKey/Valueï¼‰è¿›è¡Œäº¤äº’ã€‚è¿™ä½¿æ™ºèƒ½ä½“èƒ½å¤ŸåŠ¨æ€å…³æ³¨ä¸å½“å‰å†³ç­–æœ€ç›¸å…³çš„éšœç¢ç‰©ã€‚ | - **éšœç¢ç‰©è¡¨ç¤º**ï¼šæœ€å¤§ç‚¹æ•° \\(N\\) æœªæ˜ç¡®ç»™å‡ºï¼Œä½†è¾“å…¥è¿›è¡Œäº†èŒƒå›´æˆªæ–­ã€‚<br>- **ç½‘ç»œç‰¹ç‚¹**ï¼šè½»é‡åŒ–è®¾è®¡ï¼Œç¡®ä¿å®æ—¶æ€§ã€‚æ³¨æ„åŠ›æƒé‡çš„å¯è§†åŒ–ï¼ˆå›¾6ï¼‰è¡¨æ˜æ¨¡å‹èƒ½æ­£ç¡®å…³æ³¨å…³é”®éšœç¢ç‰©ã€‚ |\n| **3. åŸºäºè¯¾ç¨‹å­¦ä¹ çš„ç¨€ç–å¥–åŠ±è®­ç»ƒç­–ç•¥** | **å¥–åŠ±å‡½æ•°è®¾è®¡**ï¼šé‡‡ç”¨ç¨€ç–å¥–åŠ±ï¼Œé¿å…ç¨ å¯†å¥–åŠ±è®¾è®¡å›°éš¾å¯¼è‡´çš„æ„å¤–è¡Œä¸ºã€‚å¥–åŠ±å‡½æ•°ä¸ºï¼š<br>\\[<br>r = R_g \\cdot \\mathbb{1}_{\\text{goal}} + R_c \\cdot \\mathbb{1}_{\\text{collision}} + R_{out} \\cdot \\mathbb{1}_{\\text{out\\_of\\_bounds}} + R_{gear} \\cdot \\mathbb{1}_{\\text{direction\\_change}} + R_{idle} \\cdot \\mathbb{1}_{\\text{idle}} + R_{time}<br>\\]<br>**è¯¾ç¨‹å­¦ä¹ **ï¼šé€šè¿‡**é€æ­¥å¢åŠ éš¾åº¦**æ¥å¼•å¯¼è®­ç»ƒã€‚å…±8ä¸ªé˜¶æ®µï¼š<br>  1. åˆå§‹ä½å§¿é è¿‘ç›®æ ‡ï¼Œèˆªå‘è§’ä¸ rollout ç»“æœä¸€è‡´ã€‚<br>  2-7. é€æ­¥å¢åŠ åˆå§‹ä½å§¿ä¸ç›®æ ‡çš„è·ç¦»ï¼Œå¹¶åœ¨å„é˜¶æ®µç‰¹å®šèŒƒå›´å†…é‡‡æ ·æ— ç¢°æ’çš„èˆªå‘è§’ã€‚<br>  8. ä½¿ç”¨åŸºå‡†æµ‹è¯•ä¸­è®°å½•çš„åˆå§‹ä½å§¿ã€‚ | - **å¥–åŠ±å€¼** (4.1èŠ‚): <br>  - \\(R_g = +3\\) (åˆ°è¾¾ç›®æ ‡)<br>  - \\(R_c = -3\\) (ç¢°æ’)<br>  - \\(R_{out} = -3\\) (å‡ºç•Œ)<br>  - \\(R_{gear} = -0.01\\) (æ¢å‘æƒ©ç½š)<br>  - \\(R_{idle} = -0.2\\) (ç©ºé—²æƒ©ç½š)<br>  - \\(R_{time} = -0.01\\) (æ—¶é—´æƒ©ç½š)<br>- **ç›®æ ‡å®¹å·®**ï¼šä½ç½®0.2ç±³ï¼Œèˆªå‘è§’Â±3åº¦ã€‚<br>- **æœ€å¤§å›åˆé•¿åº¦**ï¼šéšè¯¾ç¨‹é˜¶æ®µä»100æ­¥é€’å¢è‡³1000æ­¥ã€‚ |\n| **4. åŠ¨ä½œåˆ†å—æœºåˆ¶** | **é—®é¢˜**ï¼šåŸå§‹RLé€æ­¥æ‰§è¡ŒåŸå­åŠ¨ä½œï¼Œåœ¨å—é™ç©ºé—´ä¸­å­˜åœ¨**æ¢ç´¢æ•ˆç‡**ä¸**æ“æ§ç²¾åº¦**çš„çŸ›ç›¾ã€‚å°æ­¥é•¿å¯¼è‡´è§„åˆ’è§†é‡è¿‡é•¿ï¼Œæ¢ç´¢ä½æ•ˆï¼›å¤§æ­¥é•¿åˆ™ç²¾åº¦ä¸è¶³ï¼Œæ˜“ç¢°æ’ã€‚<br>**è§£å†³æ–¹æ¡ˆ**ï¼šå¼•å…¥**åŠ¨ä½œåˆ†å—**æœºåˆ¶ä½œä¸ºç¯å¢ƒåŒ…è£…å™¨ã€‚ä¸€ä¸ªâ€œå—â€æ˜¯ç”± \\(h\\) ä¸ªä½çº§æ§åˆ¶å‘½ä»¤ç»„æˆçš„çŸ­åºåˆ—ï¼Œåœ¨ç¯å¢ƒä¸­ä½œä¸ºä¸€ä¸ªå®åŠ¨ä½œæ‰§è¡Œã€‚<br>**æ•°å­¦è¡¨è¿°**ï¼šåœ¨æ—¶é—´æ­¥ \\(t\\)ï¼Œç­–ç•¥è¾“å‡ºä¸€ä¸ªå—åŠ¨ä½œ \\(a_{t:t+h}\\)ï¼Œå®ƒç¼–ç äº†æ¥ä¸‹æ¥ \\(h\\) ä¸ªåŸå§‹åŠ¨ä½œã€‚ç¯å¢ƒåŒ…è£…å™¨å°†å…¶å±•å¼€ï¼Œä¾æ¬¡æ‰§è¡Œ \\(h\\) ä¸ªåŸå§‹æ­¥ï¼Œå¹¶å°†è¿™ \\(h\\) æ­¥çš„ç´¯ç§¯å¥–åŠ±å’Œæœ€ç»ˆçŠ¶æ€è¿”å›ç»™RLç®—æ³•ã€‚è¿™æœ‰æ•ˆç¼©çŸ­äº†æœ‰æ•ˆè§„åˆ’è§†é‡ï¼ŒåŒæ—¶ä¿æŒäº†æ§åˆ¶ç²¾åº¦ã€‚ | - **åˆ†å—é•¿åº¦**ï¼š\\(h = 4\\) (å®éªŒä¸­ä½¿ç”¨)ã€‚<br>- **å®ç°**ï¼šä½œä¸ºé€šç”¨ç¯å¢ƒåŒ…è£…å™¨å®ç°ï¼Œä¸ä¿®æ”¹PPOç­‰ç®—æ³•çš„å†…éƒ¨ç›®æ ‡å‡½æ•°ï¼Œå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚ |\n| **5. ParkBench åŸºå‡†æµ‹è¯•ä¸ä»¿çœŸç¯å¢ƒ** | **åŸºå‡†æ„å»º**ï¼šåˆ›å»ºäº†åä¸º **ParkBench** çš„ä¸“ç”¨åŸºå‡†ï¼ŒåŒ…å«51ä¸ªä»çœŸå®æ•°æ®æå–çš„å€’è½¦å…¥åº“åœºæ™¯å¸ƒå±€ï¼Œæ¶µç›–ç‹­çª„é€šé“ã€è¢«é®æŒ¡è§’è½ç­‰æŒ‘æˆ˜ã€‚<br>**ä»¿çœŸç¯å¢ƒ**ï¼šåŸºäºGymæ¥å£æ„å»ºï¼Œä½¿ç”¨è‡ªè¡Œè½¦æ¨¡å‹è¿›è¡ŒçŠ¶æ€æ›´æ–°ï¼Œä»…è€ƒè™‘é™æ€éšœç¢ç‰©ã€‚ç¯å¢ƒåˆå§‹åŒ–æ—¶åŠ è½½ä¸€ä¸ªåœºæ™¯ï¼Œæ™ºèƒ½ä½“é€šè¿‡åºè´¯åŠ¨ä½œä¸ä¹‹äº¤äº’ã€‚ | - **åœºæ™¯æ•°é‡**ï¼š51ä¸ªã€‚<br>- **éšœç¢ç‰©è¡¨ç¤º**ï¼šçº¢è‰²è½®å»“çº¿ï¼ˆå¦‚å›¾1,7,8æ‰€ç¤ºï¼‰ã€‚<br>- **å¼€æº**ï¼šåŸºå‡†å¸ƒå±€ã€è½¦è¾†å‚æ•°å’Œè®­ç»ƒæ–¹æ³•å‡å·²å¼€æºã€‚ |\n\n**æ€»ç»“**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åœ¨äº**ç³»ç»Ÿæ€§åœ°å°†DRLåº”ç”¨äºä¸¥æ ¼å—è¿åŠ¨å­¦å’Œç¯å¢ƒçº¦æŸçš„åœè½¦è§„åˆ’é—®é¢˜**ã€‚é€šè¿‡**è‡ªè¡Œè½¦æ¨¡å‹å»ºæ¨¡**ç¡®ä¿åŠ¨æ€å¯è¡Œæ€§ï¼Œ**ç¨€ç–å¥–åŠ±+è¯¾ç¨‹å­¦ä¹ **è§£å†³å¤æ‚ä»»åŠ¡æ¢ç´¢éš¾é¢˜ï¼Œ**åŠ¨ä½œåˆ†å—**å¹³è¡¡æ•ˆç‡ä¸ç²¾åº¦ï¼Œå¹¶è¾…ä»¥**ä¸“ç”¨åŸºå‡†(ParkBench)** å’Œ**è½»é‡çº§ç‰¹å¾æå–ç½‘ç»œ**ã€‚è¿™äº›æ–¹æ³•å…±åŒä½œç”¨ï¼Œä½¿å¾—çº¯RLè§„åˆ’å™¨åœ¨æˆåŠŸç‡å’Œæ•ˆç‡ä¸Šå¤§å¹…è¶…è¶Šä¼ ç»ŸHybrid A*ï¼ˆæˆåŠŸç‡+96%ï¼Œæ•ˆç‡+52%ï¼‰ï¼Œå¹¶å±•ç°å‡ºå®æ—¶éƒ¨ç½²çš„æ½œåŠ›ã€‚",
            "timestamp": "2026-02-03T15:57:32.129091",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡ (Evaluation Metrics)\n\nè®ºæ–‡ä½¿ç”¨äº†å››ä¸ªæ ¸å¿ƒæŒ‡æ ‡æ¥å…¨é¢è¯„ä¼°è·¯å¾„è§„åˆ’å™¨çš„æ€§èƒ½ã€‚å…·ä½“ç»†èŠ‚å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æŒ‡æ ‡åç§° | ç¬¦å·/å•ä½ | è®¡ç®—æ–¹å¼ä¸ç»†èŠ‚ | è¯„ä¼°ç›®çš„ |\n| :--- | :--- | :--- | :--- |\n| **æˆåŠŸç‡** | Succ. (%) â†‘ | åœ¨å®¹å¿åº¦èŒƒå›´å†…æˆåŠŸåˆ°è¾¾ç›®æ ‡ä½å§¿çš„åœºæ™¯æ¯”ä¾‹ã€‚**å®¹å¿åº¦è®¾å®š**ï¼šä½ç½®è¯¯å·® â‰¤ 0.2ç±³ï¼ˆç›¸å¯¹äºå‡ ä½•ä¸­å¿ƒï¼‰ï¼Œèˆªå‘è§’è¯¯å·® â‰¤ Â±3åº¦ã€‚ | è¡¡é‡è§„åˆ’å™¨çš„**åŸºæœ¬åŠŸèƒ½æ€§å’Œé²æ£’æ€§**ã€‚åœ¨å¤æ‚çº¦æŸåœºæ™¯ä¸‹ï¼Œè¿™æ˜¯æœ€é‡è¦çš„æŒ‡æ ‡ã€‚ |\n| **è§„åˆ’æ—¶é—´** | Time (s) â†“ | ç”Ÿæˆä¸€æ¡å¯è¡Œè½¨è¿¹æ‰€éœ€çš„**å¹³å‡è®¡ç®—æ—¶é—´**ã€‚åœ¨æµ‹è¯•æ—¶ï¼ŒRLç­–ç•¥æ¯æ­¥ä»…éœ€ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œè®¡ç®—å¼€é”€æä½ã€‚ | è¯„ä¼°ç®—æ³•çš„**å®æ—¶æ€§**å’Œ**è®¡ç®—æ•ˆç‡**ï¼Œæ˜¯å®é™…éƒ¨ç½²çš„å…³é”®ã€‚ |\n| **è¡Œé©¶è·ç¦»** | Dist. (m) â†“ | æ‰§è¡Œè½¨è¿¹çš„**æ€»è·¯å¾„é•¿åº¦**ã€‚ç”±ç­–ç•¥äº§ç”Ÿçš„è¿ç»­åŠ¨ä½œåºåˆ—é€šè¿‡è‡ªè¡Œè½¦æ¨¡å‹ç§¯åˆ†å¾—åˆ°ã€‚ | è¡¡é‡è·¯å¾„çš„**ç©ºé—´æ•ˆç‡**ã€‚æ›´çŸ­çš„è·¯å¾„é€šå¸¸æ„å‘³ç€æ›´å°‘çš„èƒ½è€—å’Œæ—¶é—´ã€‚ |\n| **è½¬å‘ç‚¹æ•°é‡** | Pivot Points â†“ | è½¨è¿¹ä¸­**æ–¹å‘æ”¹å˜ï¼ˆå‰è¿› â†” åé€€ï¼‰çš„æ¬¡æ•°**ã€‚æ¯æ¬¡æ¢å‘è®¡ä¸ºä¸€ä¸ªè½¬å‘ç‚¹ã€‚ | åæ˜ è·¯å¾„çš„**å¹³æ»‘åº¦**å’Œ**æ“çºµèˆ’é€‚æ€§**ã€‚è¿‡å¤šçš„è½¬å‘ç‚¹è¡¨æ˜è·¯å¾„æŒ¯è¡ã€ä¸ä¼˜ï¼Œç±»ä¼¼äºäººç±»é©¾é©¶å‘˜éœ€è¦å¤šæ¬¡å‰åè°ƒæ•´ã€‚ |\n\n**æŒ‡æ ‡å…³è”æ€§**ï¼šè¿™å››ä¸ªæŒ‡æ ‡å…±åŒæ„æˆäº†ä¸€ä¸ªå¤šç»´åº¦çš„è¯„ä¼°ä½“ç³»ã€‚é«˜æˆåŠŸç‡æ˜¯å‰æï¼Œä½†è¿˜éœ€ç»“åˆçŸ­çš„è§„åˆ’æ—¶é—´ã€åˆç†çš„è¡Œé©¶è·ç¦»å’Œè¾ƒå°‘çš„è½¬å‘ç‚¹ï¼Œæ‰èƒ½è¯æ˜è§„åˆ’å™¨åœ¨**çº¦æŸåœºæ™¯**ä¸‹å…¼å…·**é«˜æ•ˆã€å¹³æ»‘å’Œå®ç”¨**ã€‚\n\n### äºŒã€ æ•°æ®é›†/åŸºå‡† (Dataset/Benchmark)\n\nè®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ä¹‹ä¸€æ˜¯åˆ›å»ºå¹¶å¼€æºäº†åä¸º **ParkBench** çš„ä¸“ç”¨åŸºå‡†ã€‚\n\n| å±æ€§ | ç»†èŠ‚æè¿° |\n| :--- | :--- |\n| **åç§°** | **ParkBench** (Constrained Parking Scenarios Benchmark) |\n| **æ ¸å¿ƒç›®çš„** | ä¸“é—¨ä¸º**çº¦æŸåœè½¦åœºæ™¯**çš„è·¯å¾„è§„åˆ’ç®—æ³•ï¼ˆç‰¹åˆ«æ˜¯åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼‰æä¾›è®­ç»ƒå’Œè¯„ä¼°æ ‡å‡†ã€‚å¼¥è¡¥äº†è¯¥é¢†åŸŸç¼ºä¹å…·æœ‰æŒ‘æˆ˜æ€§ã€æ ‡å‡†åŒ–åŸºå‡†çš„ç©ºç™½ã€‚ |\n| **åœºæ™¯æ¥æº** | æ‰€æœ‰51ä¸ªåœºæ™¯å¸ƒå±€å‡**æå–è‡ªçœŸå®ä¸–ç•Œæ•°æ®é›†**ï¼Œç¡®ä¿äº†åœºæ™¯çš„**çœŸå®æ€§å’Œä»£è¡¨æ€§**ã€‚ |\n| **åœºæ™¯ç±»å‹** | ä¸“æ³¨äº **â€œå€’è½¦å…¥åº“â€** ä»»åŠ¡ã€‚åŒ…å«ä»ç‹­çª„é€šé“åˆ°è¢«é®æŒ¡çš„è§’è½è½¦ä½ç­‰å¤šç§æŒ‘æˆ˜æ€§åœºæ™¯ï¼Œæ—¨åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„ç´§çº¦æŸç©ºé—´ã€‚ |\n| **åœºæ™¯æ•°é‡** | **51ä¸ª** ä¸åŒçš„åœè½¦åœºæ™¯å¸ƒå±€ï¼ˆè®ºæ–‡é™„å½•Gçš„å›¾8å±•ç¤ºäº†å…¨éƒ¨åœºæ™¯ï¼‰ã€‚ |\n| **æ•°æ®æ„æˆ** | æ¯ä¸ªåœºæ™¯æ˜ç¡®æŒ‡å®šï¼š<br>1. **è‡ªè½¦åˆå§‹ä½å§¿** `(x_init, y_init, Î¸_init)`<br>2. **ç›®æ ‡åœè½¦ä½å§¿** `(x_goal, y_goal, Î¸_goal)`<br>3. **éšœç¢ç‰©ä¿¡æ¯**ï¼šä»¥**è½®å»“ç‚¹**çš„2Dåæ ‡ `(x_obs, y_obs)` å½¢å¼ç»™å‡ºã€‚è¿™ç§ç¨€ç–è¡¨ç¤ºä¸ç»å…¸è§„åˆ’å™¨ï¼ˆå¦‚Hybrid A*ï¼‰ä½¿ç”¨çš„è¾“å…¥ä¸€è‡´ï¼Œç¡®ä¿äº†å¯¹æ¯”çš„å…¬å¹³æ€§ã€‚ |\n| **é…å¥—ç¯å¢ƒ** | åŸºäºParkBenchæ„å»ºäº†ä¸€ä¸ªéµå¾ªGymæ¥å£çš„**ä»¿çœŸç¯å¢ƒ**ï¼Œç”¨äºæ”¯æŒå¼ºåŒ–å­¦ä¹ çš„**é—­ç¯è®­ç»ƒ**å’Œè¯„ä¼°ã€‚ç¯å¢ƒä½¿ç”¨è‡ªè¡Œè½¦æ¨¡å‹æ›´æ–°è½¦è¾†çŠ¶æ€ï¼Œå¹¶å‡è®¾åªæœ‰é™æ€éšœç¢ç‰©ã€‚ |\n| **å¼€æºåœ°å€** | https://github.com/dqm5rtfg9b-collab/Constrained_Parking_Scenarios |\n| **ä¸ç°æœ‰åŸºå‡†å¯¹æ¯”** | **1. E2E Parking Benchmark**ï¼šä»…é™äºå®½æ•ç©ºé—´ä¸‹çš„å‚ç›´å€’è½¦å…¥åº“ï¼ŒæŒ‘æˆ˜æ€§ä¸è¶³ã€‚<br>**2. TPCAP Benchmark**ï¼šåœºæ™¯è¿‡äºç®€åŒ–ï¼Œéšœç¢ç‰©è¡¨ç¤ºä¸ºå®å¿ƒå½¢çŠ¶ï¼Œä¸ç°æœ‰è‡ªåŠ¨é©¾é©¶æµç¨‹ä¸å…¼å®¹ã€‚<br>**ParkBench** çš„ä¼˜åŠ¿åœ¨äºå…¶**ç´§çº¦æŸã€é«˜çœŸå®æ€§ã€ä»¥åŠä¸ç°æœ‰ç®¡é“å…¼å®¹**çš„ç¨€ç–éšœç¢ç‰©è¡¨ç¤ºã€‚ |\n\n**æ€»ç»“**ï¼šè®ºæ–‡é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ **ParkBench åŸºå‡†** å’Œä¸€å¥—å…¨é¢çš„**è¯„ä»·æŒ‡æ ‡**ï¼Œä¸ºçº¦æŸåœè½¦åœºæ™¯ä¸‹çš„è·¯å¾„è§„åˆ’ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªåšå®ã€å¯å¤ç°çš„è¯„ä¼°æ¡†æ¶ã€‚è¿™ä¸ä»…æ˜¯å…¶æ–¹æ³•å–å¾—SOTAæ€§èƒ½çš„éªŒè¯åŸºç¡€ï¼Œä¹Ÿæ˜¯å…¶å¯¹ç¤¾åŒºçš„é‡è¦è´¡çŒ®ï¼Œæ—¨åœ¨æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶å‘å±•ã€‚",
            "timestamp": "2026-02-03T15:58:07.323216",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†\nè®ºæ–‡**æœªä½¿ç”¨**å…¬å¼€çš„é€šç”¨è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ï¼ˆå¦‚nuScenesï¼‰ï¼Œè€Œæ˜¯**ä¸“é—¨æ„å»ºå¹¶å¼€æºäº†ä¸€ä¸ªåä¸º `ParkBench` çš„æ–°åŸºå‡†æ•°æ®é›†**ã€‚\n- **æ•°æ®æ¥æº**ï¼šæ‰€æœ‰åœºæ™¯å¸ƒå±€å‡**æå–è‡ªçœŸå®ä¸–ç•Œæ•°æ®é›†**ï¼Œç¡®ä¿äº†åœºæ™¯çš„çœŸå®æ€§ã€‚\n- **åœºæ™¯ç‰¹ç‚¹**ï¼šä¸“æ³¨äº**å—é™åœè½¦åœºæ™¯**ï¼ŒåŒ…æ‹¬ç‹­çª„é€šé“ã€è¢«é®æŒ¡çš„è§’è½è½¦ä½ç­‰ï¼Œæ—¨åœ¨åæ˜ ç°å®ä¸–ç•Œä¸­å…·æœ‰æŒ‘æˆ˜æ€§çš„åœè½¦æ¡ä»¶ã€‚\n- **åœºæ™¯æ•°é‡**ï¼šåŒ…å« **51ä¸ª** ä¸åŒçš„åå‘å…¥åº“åœè½¦åœºæ™¯å¸ƒå±€ã€‚\n- **å¼€æºåœ°å€**ï¼š`https://github.com/dqm5rtfg9b-collab/Constrained_Parking_Scenarios`\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡ä½¿ç”¨äº†å››ä¸ªæ ¸å¿ƒæŒ‡æ ‡æ¥å…¨é¢è¯„ä¼°è·¯å¾„è§„åˆ’å™¨çš„æ€§èƒ½ï¼š\n\n| æŒ‡æ ‡ | è¯´æ˜ | ä¼˜åŒ–æ–¹å‘ |\n| :--- | :--- | :--- |\n| **æˆåŠŸç‡** | åœ¨ä½ç½®å’Œæœå‘å®¹å·®èŒƒå›´å†…ï¼ŒæˆåŠŸæŠµè¾¾ç›®æ ‡ä½å§¿çš„åœºæ™¯æ¯”ä¾‹ã€‚ | **â†‘** è¶Šé«˜è¶Šå¥½ |\n| **è§„åˆ’æ—¶é—´** | ç”Ÿæˆå¯è¡Œè½¨è¿¹æ‰€éœ€çš„å¹³å‡è®¡ç®—æ—¶é—´ï¼ˆç§’ï¼‰ã€‚ | **â†“** è¶Šä½è¶Šå¥½ |\n| **è¡Œé©¶è·ç¦»** | æ‰§è¡Œè½¨è¿¹çš„æ€»è·¯å¾„é•¿åº¦ï¼ˆç±³ï¼‰ã€‚ | **â†“** è¶ŠçŸ­è¶Šé«˜æ•ˆ |\n| **è½¬å‘ç‚¹** | è½¨è¿¹ä¸­å‰è¿›/åé€€æ–¹å‘åˆ‡æ¢çš„æ¬¡æ•°ï¼Œåæ˜ æ“çºµå¹³æ»‘åº¦ã€‚ | **â†“** è¶Šå°‘è¶Šå¹³æ»‘ |\n\n### ä¸‰ã€ æ€§èƒ½æå‡ï¼ˆä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ï¼‰\nè®ºæ–‡å°†æå‡ºçš„å®Œæ•´RLè§„åˆ’å™¨ï¼ˆPPO + è¯¾ç¨‹å­¦ä¹  + åŠ¨ä½œåˆ†å—ï¼‰ä¸ç»å…¸çš„ **Hybrid A\\*** è§„åˆ’å™¨åœ¨ `ParkBench` ä¸Šè¿›è¡Œäº†å¯¹æ¯”ã€‚\n\n**ä¸»è¦æ€§èƒ½æå‡å¦‚ä¸‹ï¼š**\n\n| æ–¹æ³• | æˆåŠŸç‡ | è§„åˆ’æ—¶é—´ | è¡Œé©¶è·ç¦» | è½¬å‘ç‚¹ |\n| :--- | :--- | :--- | :--- | :--- |\n| **Hybrid A\\* (åŸºçº¿)** | 47.1% | 0.42 ç§’ | 22.3 ç±³ | 3.2 |\n| **PPO (Oursï¼Œå®Œæ•´æ–¹æ³•)** | **92.2%** | **0.20 ç§’** | **19.2 ç±³** | **4.3** |\n| **æ€§èƒ½æå‡** | **+95.8% (ç›¸å¯¹æå‡)** | **-52.4%** | **-13.9%** | ç•¥æœ‰å¢åŠ  |\n\n**æ ¸å¿ƒç»“è®º**ï¼š\n1.  **æˆåŠŸç‡å¤§å¹…é¢†å…ˆ**ï¼šå®Œæ•´RLæ–¹æ³•çš„æˆåŠŸç‡ï¼ˆ92.2%ï¼‰è¿œè¶…Hybrid A*ï¼ˆ47.1%ï¼‰ï¼Œ**ç›¸å¯¹æå‡é«˜è¾¾95.8%**ã€‚è¿™è¯æ˜äº†RLæ–¹æ³•åœ¨å¤æ‚å—é™åœºæ™¯ä¸­å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§å’Œå¯»è·¯èƒ½åŠ›ã€‚\n2.  **æ•ˆç‡æ˜¾è‘—æå‡**ï¼š\n    - **è§„åˆ’é€Ÿåº¦å¿«ä¸€å€ä»¥ä¸Š**ï¼ˆ0.20ç§’ vs 0.42ç§’ï¼‰ï¼Œ**æ•ˆç‡æå‡52%**ï¼Œæ»¡è¶³äº†å®æ—¶æ€§è¦æ±‚ã€‚\n    - ç”Ÿæˆçš„è·¯å¾„æ›´çŸ­ï¼ˆ19.2ç±³ vs 22.3ç±³ï¼‰ï¼Œè¡Œé©¶æ›´é«˜æ•ˆã€‚\n3.  **è·¯å¾„å¹³æ»‘åº¦**ï¼šè™½ç„¶è½¬å‘ç‚¹æ•°é‡ç•¥æœ‰å¢åŠ ï¼ˆ4.3 vs 3.2ï¼‰ï¼Œä½†è€ƒè™‘åˆ°æˆåŠŸç‡çš„å¤§å¹…æå‡å’Œåœºæ™¯çš„å¤æ‚æ€§ï¼Œè¿™ç§è½»å¾®çš„éæœ€ä¼˜æ€§ï¼ˆå¤šæ¬¡è°ƒæ•´ï¼‰æ˜¯å¯æ¥å—çš„ï¼Œä¸”è·¯å¾„æ•´ä½“ä»è¾ƒä¸ºåˆç†ã€‚\n\n**å®é™…ä»·å€¼ä½“ç°**ï¼š\n- **å®æ—¶æ€§**ï¼šå•æ­¥å‰å‘æ¨ç†ï¼Œè®¡ç®—è½»é‡ï¼Œé€‚åˆè½¦è½½éƒ¨ç½²ã€‚\n- **å¼ºæ³›åŒ–æ€§**ï¼šåœ¨çœŸå®æ•°æ®æ„å»ºçš„ã€å¤šæ ·åŒ–çš„å›°éš¾åœºæ™¯ä¸­å–å¾—é«˜æˆåŠŸç‡ï¼Œè¯æ˜äº†æ–¹æ³•çš„å®ç”¨æ½œåŠ›ã€‚\n- **ç®€åŒ–ç³»ç»Ÿ**ï¼šæ–¹æ³•è®¾è®¡ä¸Šå‡å°‘äº†å¯¹ç²¾ç¡®å®šä½ã€è·Ÿè¸ªç­‰æ¨¡å—çš„ä¾èµ–ï¼Œæœ‰æœ›é™ä½ç³»ç»Ÿå¤æ‚æ€§å’Œè¯¯å·®ç´¯ç§¯ã€‚",
            "timestamp": "2026-02-03T15:58:36.175212",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡ç¬¬4.5èŠ‚ï¼ˆLimitationsï¼‰åŠå…¨æ–‡å†…å®¹ï¼Œè¯¥ç ”ç©¶å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n### 1. **åœ¨å¼€æ”¾/ç©ºæ—·ç©ºé—´ä¸­çš„æ€§èƒ½ä¸‹é™**\n- **ç°è±¡**ï¼šåœ¨çº¦æŸç´§å¯†çš„ç¯å¢ƒä¸­ï¼Œè§„åˆ’å™¨æˆåŠŸç‡ä¿æŒåœ¨ **90%+**ï¼Œä½†åœ¨éšœç¢ç‰©ç¨€ç–çš„å¼€æ”¾åœºæ™¯ä¸­ï¼ŒæˆåŠŸç‡æ˜¾è‘—ä¸‹é™ã€‚\n- **åŸå› åˆ†æ**ï¼šè®­ç»ƒæ•°æ®ä¸­**ç©ºæ—·åœºæ™¯çš„ä»£è¡¨æ€§ä¸è¶³**ã€‚æ¨¡å‹ä¸»è¦é’ˆå¯¹â€œçº¦æŸåœè½¦åœºæ™¯â€è¿›è¡Œä¼˜åŒ–ï¼Œå¯¼è‡´å¯¹è‡ªç”±ç©ºé—´çš„ç‰¹å¾å­¦ä¹ ä¸å……åˆ†ã€‚\n- **å½±å“**ï¼šé™åˆ¶äº†è§„åˆ’å™¨åœ¨æ··åˆç¯å¢ƒï¼ˆå¦‚ä»å¼€æ”¾åŒºåŸŸé©¶å…¥ç‹­çª„è½¦ä½ï¼‰ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n### 2. **æ‰‹åŠ¨è®¾è®¡çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥**\n- **ç°è±¡**ï¼šè®ºæ–‡é‡‡ç”¨çš„**å…«é˜¶æ®µè¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰æ˜¯äººå·¥è®¾è®¡çš„**ï¼Œä¸“é—¨é’ˆå¯¹â€œå€’è½¦å…¥åº“ï¼ˆrear-in parkingï¼‰â€ä»»åŠ¡ã€‚\n- **åŸå› åˆ†æ**ï¼š\n    - è¯¥è¯¾ç¨‹ç­–ç•¥**æ— æ³•ç›´æ¥è¿ç§»åˆ°å…¶ä»–åœè½¦æ“ä½œ**ï¼ˆå¦‚å¹³è¡Œåœè½¦ã€æ–œå‘åœè½¦ï¼‰ã€‚\n    - æ‰‹åŠ¨è®¾è®¡é™åˆ¶äº†è®­ç»ƒè¿‡ç¨‹çš„**å¯æ‰©å±•æ€§å’Œå¹¶è¡ŒåŒ–**ï¼Œå¢åŠ äº†æ‰©å±•åˆ°æ–°åœºæ™¯çš„å·¥ç¨‹æˆæœ¬ã€‚\n- **å½±å“**ï¼šæ–¹æ³•éš¾ä»¥å¿«é€Ÿé€‚åº”å¤šæ ·åŒ–çš„åœè½¦ä»»åŠ¡ï¼Œé™åˆ¶äº†å…¶ä½œä¸ºé€šç”¨åœè½¦è§„åˆ’è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚\n\n### 3. **ç¯å¢ƒä¸ä»»åŠ¡çš„ç®€åŒ–å‡è®¾**\n- **ç°è±¡**ï¼šç ”ç©¶åŸºäº**é™æ€ç¯å¢ƒ**ï¼ˆæ— åŠ¨æ€éšœç¢ç‰©ï¼‰å’Œ**å®Œç¾æ„ŸçŸ¥å‡è®¾**ï¼ˆéšœç¢ç‰©è½®å»“å·²çŸ¥ï¼‰ã€‚\n- **åŸå› åˆ†æ**ï¼š\n    - ä»¿çœŸç¯å¢ƒæœªè€ƒè™‘ç§»åŠ¨è½¦è¾†ã€è¡Œäººç­‰åŠ¨æ€å…ƒç´ ã€‚\n    - è¾“å…¥å‡è®¾éšœç¢ç‰©è½®å»“ï¼ˆå¦‚æ¥è‡ªæ¿€å…‰é›·è¾¾ï¼‰æ˜¯å‡†ç¡®ä¸”å®Œæ•´çš„ï¼Œæœªç³»ç»Ÿç ”ç©¶**æ„ŸçŸ¥å™ªå£°ã€é®æŒ¡æˆ–ä¸ç¡®å®šæ€§**å¯¹è§„åˆ’çš„å½±å“ã€‚\n- **å½±å“**ï¼šåœ¨çœŸå®ä¸–ç•Œå¤æ‚ã€åŠ¨æ€çš„åœè½¦åœºæ™¯ä¸­ï¼Œè§„åˆ’å™¨çš„é²æ£’æ€§å°šæœªå¾—åˆ°éªŒè¯ã€‚\n\n### 4. **åŸºå‡†åœºæ™¯çš„è¦†ç›–èŒƒå›´æœ‰é™**\n- **ç°è±¡**ï¼š`ParkBench` åŸºå‡†ç›®å‰ä»…åŒ…å« **51ä¸ª** çœŸå®ä¸–ç•Œè¡ç”Ÿçš„â€œå€’è½¦å…¥åº“â€åœºæ™¯ã€‚\n- **åŸå› åˆ†æ**ï¼šè™½ç„¶åœºæ™¯å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½†**æœªæ¶µç›–æ‰€æœ‰å¸¸è§çš„åœè½¦ç±»å‹**ï¼ˆå¦‚ä¾§æ–¹åœè½¦ã€æ–œåˆ—å¼åœè½¦ã€å¤šæ®µå¼æ‰å¤´åœè½¦ï¼‰ã€‚\n- **å½±å“**ï¼šè¯„ä¼°çš„å…¨é¢æ€§æœ‰å¾…æå‡ï¼Œç»“è®ºå¯èƒ½æ— æ³•æ¨å¹¿åˆ°æ‰€æœ‰åœè½¦åœºæ™¯ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nåŸºäºä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡åœ¨â€œç»“è®ºä¸æœªæ¥å·¥ä½œâ€éƒ¨åˆ†åŠåˆ†æä¸­æå‡ºäº†ä»¥ä¸‹æ”¹è¿›æ–¹å‘ï¼š\n\n### 1. **å¢å¼ºåœ¨å¼€æ”¾ç©ºé—´çš„æ³›åŒ–èƒ½åŠ›**\n- **æ”¹è¿›è§‚æµ‹è¡¨ç¤º**ï¼šåœ¨æ™ºèƒ½ä½“çš„è§‚æµ‹è¾“å…¥ä¸­**å¢åŠ è‡ªç”±ç©ºé—´/å¯é€šè¡ŒåŒºåŸŸçš„ç‰¹å¾**ï¼ˆå¦‚æœ€è¿‘éšœç¢ç‰©è·ç¦»ã€å»“æ¸…ç©ºé—´åœ°å›¾ï¼‰ã€‚\n- **æ‰©å……è®­ç»ƒæ•°æ®**ï¼šå°†**ç©ºæ—·åœºæ™¯æ¡ˆä¾‹**çº³å…¥è¯¾ç¨‹å­¦ä¹ é˜¶æ®µï¼Œä½¿æ¨¡å‹èƒ½å‡è¡¡å­¦ä¹ åœ¨çº¦æŸç©ºé—´å’Œå¼€æ”¾ç©ºé—´ä¸­çš„è§„åˆ’ç­–ç•¥ã€‚\n\n### 2. **å¼€å‘è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ æœºåˆ¶**\n- **ç ”ç©¶æ–¹å‘**ï¼šæ¢ç´¢**è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ ï¼ˆAutomatic Curriculum Learningï¼‰** æ–¹æ³•ã€‚\n- **é¢„æœŸæ”¶ç›Š**ï¼š\n    - æ ¹æ®æ™ºèƒ½ä½“çš„å­¦ä¹ è¿›åº¦**è‡ªåŠ¨è°ƒæ•´ä»»åŠ¡éš¾åº¦**ã€‚\n    - ä½¿æ–¹æ³•èƒ½**æ— ç¼æ‰©å±•åˆ°æ–°çš„åœè½¦ä»»åŠ¡**ï¼ˆå¦‚å¹³è¡Œåœè½¦ï¼‰ï¼Œå‡å°‘äººå·¥å¹²é¢„ï¼Œæå‡è®­ç»ƒæ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚\n\n### 3. **æ‰©å±•åŸºå‡†ä¸ä»»åŠ¡èŒƒå›´**\n- **ä¸°å¯Œ `ParkBench` åŸºå‡†**ï¼šå¢åŠ **æ›´å¤šåœè½¦æ“ä½œç±»å‹**çš„åœºæ™¯ï¼Œå¦‚ï¼š\n    - å¹³è¡Œåœè½¦ï¼ˆParallel Parkingï¼‰\n    - æ–œå‘åœè½¦ï¼ˆAngled Parkingï¼‰\n    - æ­£å‘åœè½¦ï¼ˆHead-in Parkingï¼‰\n- **å¼€å‘æ›´å¤æ‚çš„åœºæ™¯**ï¼šå¼•å…¥**åŠ¨æ€éšœç¢ç‰©**ï¼ˆå¦‚ç§»åŠ¨çš„è½¦è¾†ã€è¡Œäººï¼‰ï¼Œä½¿åŸºå‡†æ›´è´´è¿‘çœŸå®ä¸–ç•Œã€‚\n\n### 4. **å‘æ›´çœŸå®çš„é—­ç¯ç³»ç»Ÿæ¼”è¿›**\n- **é›†æˆæ„ŸçŸ¥ä¸ç¡®å®šæ€§**ï¼šåœ¨è®­ç»ƒå’Œä»¿çœŸä¸­å¼•å…¥**æ„ŸçŸ¥å™ªå£°å’Œé®æŒ¡æ¨¡å‹**ï¼Œç ”ç©¶è§„åˆ’å™¨å¯¹ä¸å®Œç¾æ„ŸçŸ¥çš„é²æ£’æ€§ã€‚\n- **æ¢ç´¢ä¸ä¸‹æ¸¸æ¨¡å—çš„ç´§è€¦åˆ**ï¼šè¿›ä¸€æ­¥ç ”ç©¶RLè§„åˆ’å™¨**å‡å°‘ç”šè‡³æ›¿ä»£å®šä½ã€è·¯å¾„è·Ÿè¸ªæ¨¡å—**çš„æ½œåŠ›ï¼Œå®ç°æ›´ç®€æ´çš„è‡ªä¸»é©¾é©¶æ ˆã€‚\n- **ç»“åˆç»å…¸æ–¹æ³•ä¸å­¦ä¹ èŒƒå¼**ï¼šä½œä¸ºæœªæ¥æ–¹å‘ï¼Œæ¢ç´¢**æ··åˆæ–¹æ³•**ï¼Œä¾‹å¦‚ç”¨å­¦ä¹ æ¨¡å‹ä¸ºç»å…¸è§„åˆ’å™¨ï¼ˆå¦‚Hybrid A*ï¼‰æä¾›æ›´å¥½çš„å¯å‘å¼å‡½æ•°æˆ–å¼•å¯¼æœç´¢ï¼Œç»“åˆä¸¤è€…çš„ä¼˜åŠ¿ã€‚\n\n### 5. **ä¼˜åŒ–ç®—æ³•ä¸éƒ¨ç½²**\n- **ç ”ç©¶æ›´é«˜æ•ˆçš„RLè®­ç»ƒç­–ç•¥**ï¼šè¶…è¶ŠPPOå’ŒåŠ¨ä½œåˆ†å—ï¼Œæ¢ç´¢æ›´é€‚åˆé•¿æ—¶ç¨‹ã€ç¨€ç–å¥–åŠ±è§„åˆ’çš„å…ˆè¿›RLç®—æ³•ã€‚\n- **æå‡å®æ—¶æ€§ä¸è½»é‡åŒ–**ï¼šè¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥ç½‘ç»œå’Œç‰¹å¾æå–å™¨ï¼Œç¡®ä¿åœ¨**èµ„æºå—é™çš„è½¦è½½ç¡¬ä»¶**ä¸Šçš„é«˜æ•ˆéƒ¨ç½²ã€‚\n\n**æ€»ç»“**ï¼šè¯¥è®ºæ–‡æå‡ºçš„RLè§„åˆ’å™¨åœ¨**çº¦æŸåœè½¦åœºæ™¯**ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œä½†å…¶**æ³›åŒ–æ€§ã€å¯æ‰©å±•æ€§å’Œå¯¹çœŸå®ä¸–ç•Œå¤æ‚æ€§çš„é€‚åº”èƒ½åŠ›**æ˜¯å½“å‰çš„ä¸»è¦å±€é™ã€‚æœªæ¥çš„å·¥ä½œå°†å›´ç»•**è‡ªåŠ¨åŒ–è¯¾ç¨‹å­¦ä¹ ã€åŸºå‡†æ‰©å±•ã€åŠ¨æ€ç¯å¢ƒå»ºæ¨¡ä»¥åŠæ„ŸçŸ¥-è§„åˆ’ä¸€ä½“åŒ–**ç­‰æ–¹å‘å±•å¼€ï¼Œæ—¨åœ¨æ‰“é€ ä¸€ä¸ªæ›´å¼ºå¤§ã€æ›´é€šç”¨çš„è‡ªåŠ¨é©¾é©¶åœè½¦è§£å†³æ–¹æ¡ˆã€‚",
            "timestamp": "2026-02-03T15:59:15.305763",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T04:35:49Z",
    "created_at": "2026-02-03T15:53:05.228628",
    "updated_at": "2026-02-03T15:53:05.228634"
}