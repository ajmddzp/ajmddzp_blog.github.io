{
    "id": "2601.22823v1",
    "title": "Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment",
    "authors": [
        "Mathieu Petitbois",
        "RÃ©my Portelas",
        "Sylvain Lamprier"
    ],
    "abstract": "æˆ‘ä»¬ç ”ç©¶ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­é€šè¿‡å­è½¨è¿¹æ ‡æ³¨å‡½æ•°è¿›è¡Œæ˜¾å¼é£æ ¼ç›‘ç£çš„é£æ ¼æ¡ä»¶ç­–ç•¥ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œç”±äºåˆ†å¸ƒåç§»ä»¥åŠé£æ ¼ä¸å¥–åŠ±ä¹‹é—´çš„å›ºæœ‰å†²çªï¼Œå®ç°é£æ ¼ä¸é«˜ä»»åŠ¡æ€§èƒ½çš„ååŒå°¤ä¸ºå›°éš¾ã€‚ç°æœ‰æ–¹æ³•å°½ç®¡å¼•å…¥äº†å¤šç§é£æ ¼å®šä¹‰ï¼Œå´å¾€å¾€éš¾ä»¥æœ‰æ•ˆè°ƒå’Œè¿™ä¸¤ä¸ªç›®æ ‡ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†è¡Œä¸ºé£æ ¼çš„ç»Ÿä¸€å®šä¹‰ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå®ç”¨æ¡†æ¶ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†é£æ ¼æ¡ä»¶éšå¼Qå­¦ä¹ ï¼ˆSCIQLï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç¦»çº¿ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚åè§é‡æ ‡æ³¨ä¸ä»·å€¼å­¦ä¹ ï¼‰ï¼Œå¹¶ç»“åˆæ–°å‹é—¨æ§ä¼˜åŠ¿åŠ æƒå›å½’æœºåˆ¶ï¼Œåœ¨ä¿æŒé£æ ¼å¯¹é½çš„åŒæ—¶é«˜æ•ˆä¼˜åŒ–ä»»åŠ¡æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºç°æœ‰ç¦»çº¿æ–¹æ³•ï¼ŒSCIQLåœ¨ä¸¤é¡¹æŒ‡æ ‡ä¸Šå‡å–å¾—äº†æ›´ä¼˜è¡¨ç°ã€‚ä»£ç ã€æ•°æ®é›†åŠå¯è§†åŒ–å†…å®¹è¯¦è§ï¼šhttps://sciql-iclr-2026.github.io/ã€‚",
    "url": "https://arxiv.org/abs/2601.22823v1",
    "html_url": "https://arxiv.org/html/2601.22823v1",
    "html_content": "1\n1\nfootnotetext:\nCorrespondence to\nmathieu.petitbois@ubisoft.com\nOffline Reinforcement Learning of\nHigh-Quality Behaviors Under Robust Style Alignment\nMathieu Petitbois\n*,1,2\n,\nRÃ©my Portelas\n1\n,\nSylvain Lamprier\n2\n1\nUbisoft La Forge\n2\nUniversity of Angers\nAbstract\nWe study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in:\nhttps://sciql-iclr-2026.github.io/\n.\n1\nIntroduction\nA task can often be performed through diverse means and approaches. As such, while the majority of the sequential decision making literature has focused on learning agents that seek to optimize task performance, there has been a growing interest in the development of diverse agents that display a variety of behavioral styles, which can be crucial in human-robot interaction, autonomous driving and video games. While many previous works tackled diverse policy learning by relying on online interactions\n(Nilsson and Cully,\n2021\n; Wu\net al.\n,\n2023\n)\n, the widespread availability of pre-recorded diverse behavior data\n(Hofmann,\n2019\n; Mahmood\net al.\n,\n2019a\n; Zhang\net al.\n,\n2019\n; Fu\net al.\n,\n2021\n; Lee\net al.\n,\n2024a\n; Jia\net al.\n,\n2024\n; Park\net al.\n,\n2025\n)\ncatalyzed much progress in the learning of policies from such data without further environment interactions, allowing the training of high-performing agents in a more sample-efficient, less time-consuming and safer way\n(Levine\net al.\n,\n2020\n)\n. Such methods can be divided into two categories: Imitation Learning (IL) methods\n(Pomerleau,\n1991\n; Florence\net al.\n,\n2021b\n; Chi\net al.\n,\n2024b\n)\nmimic expert trajectories, while offline Reinforcement Learning (RL) methods\n(Kumar\net al.\n,\n2020\n; Kostrikov\net al.\n,\n2021\n; Fujimoto and Gu,\n2021\n; Chen\net al.\n,\n2021\n; Nair\net al.\n,\n2021\n; Garg\net al.\n,\n2023\n)\ntarget high-performing behaviors based on observed rewards. Although some recent work has focused on diverse policy learning in both offline IL\n(Zhan\net al.\n,\n2020\n; Yang\net al.\n,\n2024\n)\nand offline RL\n(Mao\net al.\n,\n2024\n)\n, several challenges remain.\nChallenge 1: Style definition.\nLiterature dealing with style alignment ranges from discrete trajectory labels\n(Zhan\net al.\n,\n2020\n; Yang\net al.\n,\n2024\n)\nto unsupervised clusters\n(Mao\net al.\n,\n2024\n)\nand continuous latent encodings\n(Petitbois\net al.\n,\n2025\n)\n, with distinct trade-offs: unsupervised definitions are often uncontrollable and hard to interpret, while supervised ones rely on manual labels and incur significant labeling costs. Additionally, since play styles span multiple timescales, attributing each local step to a style is non-trivial and can take part in\ncredit assignment problems. Furthermore, depending on the definition of style, assessing the alignment of an agentâ€™s behavior with respect to a target style may be difficult, which complicates alignment measurement and hinders policy controllability. As such, a key challenge is to derive a\ngeneral\ndefinition that addresses\ninterpretability\n,\nlabeling cost\n,\nalignment measurement\n, and\ncredit assignment\n.\nChallenge 2: Addressing distribution shift.\nWhile offline IL and offline RL are known to suffer from distribution shift due to environment stochasticity and compounding errors\n(Levine\net al.\n,\n2020\n)\n, the addition of style conditioning can exacerbate the issue by creating mismatches at inference time between visited states and target styles. For instance, a running policy may trip and fall into an out-of-distribution state-style configuration without the ability to recalibrate. While some previous work addressed this issue\n(Petitbois\net al.\n,\n2025\n)\n, most of them lack mechanisms to perform robust style alignment. Consequently, an open question is how to achieve\nrobust style alignment\nwithout relying on further environment interactions.\nChallenge 3: Solving task and style misalignment.\nStyle alignment and task performance are often incompatible. For instance, a crawling policy may not achieve the same speed as a running one. Optimizing conflicting objectives of style alignment and task performance has been explored in offline RL, either by directly seeking compromises between them\n(Lin\net al.\n,\n2024a\n;\nb\n; Yuan\net al.\n,\n2025\n)\n, or by shifting optimal policies from one objective to the other\n(Mao\net al.\n,\n2024\n)\n, but always at the cost of style alignment. Consequently, ensuring\nrobust style alignment while optimizing task performance\nremains an open problem.\nIn this paper, we address these challenges through the following contributions:\n(1)\nWe propose a novel\ngeneral\nview of the stylized policy learning problem as a generalization of the goal-conditioned RL (GCRL) problem\n(Park\net al.\n,\n2025\n)\n.\n(2)\nWe instantiate our definition within the supervised data-programming framework\n(Ratner\net al.\n,\n2017\n)\nby using labeling functions as in\nZhan\net al.\n(\n2020\n); Yang\net al.\n(\n2024\n)\nbut on trajectory windows rather than full trajectories, capturing the multi-timescale nature of styles and mitigating high\ncredit assignment\nchallenges by design. The use of labeling functions also allows users to\nquickly\nprogram various\nmeaningful\nstyle annotations for both training data and evaluation data, making the\nalignment measurement\neasier at inference.\n(3)\nWe introduce Style-Conditioned Implicit-Q-Learning (SCIQL), a style-conditioned offline RL algorithm inspired by IQL\n(Kostrikov\net al.\n,\n2021\n)\nwhich leverages advantage signals to guide the policy towards the activation of target styles, making efficient use of\nstyle-relabeling\n(Petitbois\net al.\n,\n2025\n)\nand trajectory stitching\n(Char\net al.\n,\n2022\n)\nto allow for\nrobust style alignment\n.\n(4)\nMaking use of the casting of stylized policy learning problem as a RL problem, we introduce the notion of\nGated Advantage Weighted Regression (GAWR)\nin the stylized policy learning context by using advantage functions as gates to allow\nstyle-conditioned task performance optimization\n.\n(5)\nWe demonstrate through a set of experiments on diverse stylized RL tasks that our method effectively outperforms previous work on both\nstyle alignment\nand\nstyle-conditioned task performance optimization\n. Code, datasets and visuals are available in:\nhttps://sciql-iclr-2026.github.io/\n.\n2\nRelated work\nIL and offline RL.\nImitation Learning seeks to learn policies by mimicking expert demonstrations, usually stored as trajectory datasets, and can be grouped into different categories, including Behavior Cloning, classical Inverse RL (IRL), and Apprenticeship / Adversarial IRL. Behavior Cloning (BC)\n(Pomerleau,\n1991\n)\nperforms supervised regression of actions given states but suffers from compounding errors and distribution shifts\n(Ross\net al.\n,\n2011\n)\n. Classical IRL\n(Ng and Russell,\n2000\n; Fu\net al.\n,\n2018\n; Arora and Doshi,\n2020\n)\ninfers a reward under which the demonstration policy is optimal to optimize it via online RL. It is robust to distribution shifts but requires environment interactions. Apprenticeship / Adversarial IRL (e.g., GAIL\n(Ho and Ermon,\n2016\n)\n) learns policies directly via implicit rewards, combining IRLâ€™s robustness with BCâ€™s direct learning, but typically requires online interactions. On the other hand, offline RL does not assume optimal demonstrations. It uses reward signals to train policies offline and tackles distribution shifts via sequence modeling\n(Chen\net al.\n,\n2021\n)\n, biased BC\n(Nair\net al.\n,\n2021\n; Fujimoto and Gu,\n2021\n)\n, policy conservativeness\n(Kumar\net al.\n,\n2020\n)\n, expectile regression\n(Kostrikov\net al.\n,\n2021\n)\n, or Q-value exponential weighting\n(Garg\net al.\n,\n2023\n)\n. In this work, we leverage offline RL techniques to jointly optimize behavior styles and task performance from reward signals, without assuming demonstration optimality.\nDiverse policy learning.\nCapturing diverse behavior from a pre-recorded dataset has been addressed in the literature under various scopes. Several methods aim to capture a demonstration datasetâ€™s multimodality at the action level through imitation learning techniques\n(Florence\net al.\n,\n2021a\n; Shafiullah\net al.\n,\n2022\n; Pearce\net al.\n,\n2023\n; Chi\net al.\n,\n2024a\n; Lee\net al.\n,\n2024b\n)\nwhile other methods aim to learn higher-timescale behavior diversity by learning to capture various behavior styles in both an unsupervised and supervised approach. In the IRL setting, InfoGAIL\n(Li\net al.\n,\n2017\n)\n, Intention-GAN\n(Hausman\net al.\n,\n2017\n)\nand DiverseGAIL\n(Wang\net al.\n,\n2017\n)\naim to identify various behavior styles from demonstration data and train policies to reconstruct them using IRL techniques.\nTirinzoni\net al.\n(\n2025\n)\naim to learn a forward-backward representation of a state successor measure\n(Dayan,\n1993\n; Touati and Ollivier,\n2021\n)\nto learn through IRL a policy optimizing a high variety of rewards with a bias towards a demonstration dataset. Also in the online RL setting,\nwen2025constrainedstylelearningimperfect\nmodel the stylized policy learning problem as a constrained MDP problem to imitate style under the constraint of near-optimal task performance while we tackle offline learning, prioritize style alignment over task performance and introduce relabeing strategies for robustness. In a BC setting, WZBC\n(Petitbois\net al.\n,\n2025\n)\nlearns a latent space of trajectories to employ trajectory-similarity-weighted-regression to improve robustness to compounding errors in trajectory reconstruction. Further, SORL\n(Mao\net al.\n,\n2024\n)\nlearns a set of diverse representative policies through the EM algorithm and enhances them to perform stylized offline RL. In the supervised setting, CTVAE\n(Zhan\net al.\n,\n2020\n)\naugments trajectory variational auto-encoders with trajectory style labels to perform imitation learning under style calibration, while BCPMI\n(Yang\net al.\n,\n2024\n)\nperforms a behavior cloning regression weighted by mutual information estimates between state-action pairs and style labels. Our method falls into the offline supervised learning category as in CTVAE and BCPMI as we employ supervised style labels to derive style reward signals for our policy to optimize. However, we consider styles defined on subtrajectories, unlike CTVAE, our method is model-free and unlike BCPMI, we use reinforcement learning signals to enhance the robustness of our method to distribution shift and allow for both task performance and style alignment optimization.\nGoal-Conditioned RL.\nGoal-Conditioned RL (GCRL)\n(Kaelbling,\n1993\n; Liu\net al.\n,\n2022\n; Park\net al.\n,\n2025\n)\nencompasses methods that learn policies to achieve diverse goals efficiently and reliably. As our style alignment objective consists in visiting state-action pairs of high-probability to contribute to a given style, it shares with GCRL the same challenges of sparse rewards, long-term decision making and trajectory stitching. To address these challenges,\nGhosh\net al.\n(\n2019\n); Yang\net al.\n(\n2022\n)\ncombine imitation learning with Hindsight Experience Replay (HER)\n(Andrychowicz\net al.\n,\n2017\n)\n, while\nChebotar\net al.\n(\n2021\n); Kostrikov\net al.\n(\n2021\n); Park\net al.\n(\n2024\n); Canesse\net al.\n(\n2024\n); Kobanda\net al.\n(\n2025\n)\nadditionally learn goal-conditioned value functions to extract policies using offline RL techniques. Unlike GCRL, which focuses on achieving specific goals, our framework addresses performing RL tasks under stylistic constraints. This can be viewed as a generalization from goal-reaching to executing diverse RL tasks while maintaining stylistic alignment.\n3\nPreliminaries\nMarkov decision process.\nWe consider a\nÎ³\n\\gamma\n-discounted Markov Decision Process (MDP)\nâ„³\n=\n(\nğ’®\n,\nğ’œ\n,\nÎ¼\n,\np\n,\nr\n,\nÎ³\n)\n\\mathcal{M}=(\\mathcal{S},\\mathcal{A},\\mu,p,r,\\gamma)\n,\nwhere\nğ’®\n\\mathcal{S}\nis the state space,\nğ’œ\n\\mathcal{A}\nthe action space,\nÎ¼\nâˆˆ\nÎ”\nâ€‹\n(\nğ’®\n)\n\\mu\\in\\Delta(\\mathcal{S})\nthe initial state distribution,\np\n:\nğ’®\nÃ—\nğ’œ\nâ†’\nÎ”\nâ€‹\n(\nğ’®\n)\np:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\Delta(\\mathcal{S})\nthe transition kernel,\nr\n:\nğ’®\nÃ—\nğ’œ\nâ†’\n[\nr\nmin\n,\nr\nmax\n]\nr:\\mathcal{S}\\times\\mathcal{A}\\rightarrow[r_{\\min},r_{\\max}]\na reward function,\nand\nÎ³\nâˆˆ\n[\n0\n,\n1\n)\n\\gamma\\in[0,1)\nthe discount factor.\nAn agent is modeled by a policy\nÏ€\n:\nğ’®\nâ†’\nÎ”\nâ€‹\n(\nğ’œ\n)\n\\pi:\\mathcal{S}\\rightarrow\\Delta(\\mathcal{A})\n.\nAt timestep\nt\nt\n, the agent observes\ns\nt\nâˆˆ\nğ’®\ns_{t}\\in\\mathcal{S}\n, selects\na\nt\nâˆˆ\nğ’œ\na_{t}\\in\\mathcal{A}\n, and the environment transitions to\ns\nt\n+\n1\nâˆ¼\np\n(\nâ‹…\nâˆ£\ns\nt\n,\na\nt\n)\ns_{t+1}\\sim p(\\cdot\\mid s_{t},a_{t})\n.\nThis interaction generates a trajectory\nÏ„\n=\n(\ns\n0\n,\na\n0\n,\nr\n0\n,\nâ€¦\n,\ns\nT\nâˆ’\n1\n,\na\nT\nâˆ’\n1\n,\nr\nT\nâˆ’\n1\n,\ns\nT\n)\n\\tau=(s_{0},a_{0},r_{0},\\ldots,s_{T-1},a_{T-1},r_{T-1},s_{T})\n,\nwhere\nr\nt\n=\nr\nâ€‹\n(\ns\nt\n,\na\nt\n)\nr_{t}=r(s_{t},a_{t})\nand\nT\nT\ndenotes a termination or truncation horizon.\nWe assume access to a finite dataset\nğ’Ÿ\n\\mathcal{D}\nof such trajectories collected by an unknown behavior policy\nÏ€\nğ’Ÿ\n\\pi_{\\mathcal{D}}\n.\nStyle and diversity in imitation learning.\nTo train a policy towards a target behavior, traditional IL methods leverage\nğ’Ÿ\n\\mathcal{D}\nby mimicking its behaviors under the assumption of the combined expertise and homogeneity of its trajectories. In contrast, we assume that\nğ’Ÿ\n\\mathcal{D}\nâ€™s behaviors can possibly display a high amount of heterogeneity. Previous literature\n(Zhan\net al.\n,\n2020\n; Mao\net al.\n,\n2024\n; Yang\net al.\n,\n2024\n)\ndescribes this heterogeneity through various definitions of behavior styles. Denoting\nğ’¯\n~\n\\tilde{\\mathcal{T}}\nas the set of (overlapping) subtrajectories, we can generalize those definitions by defining a style as the\nlabeling\nof a subtrajectory\nÏ„\nt\n:\nt\n+\nh\nâˆˆ\nğ’¯\n~\n\\tau_{t:t+h}\\in\\tilde{\\mathcal{T}}\ngiven a comparison\ncriterion\ntowards a\ntask\nto perform. Hence, a style translates into a specific way to carry out a given task given a criterion. A\ntask\nin the MDP framework is generally defined through its reward function\nr\n:\nğ’®\nÃ—\nğ’œ\nâ†’\n[\nr\nmin\n,\nr\nmax\n]\nr:\\mathcal{S}\\times\\mathcal{A}\\rightarrow[r_{\\mathrm{min}},r_{\\mathrm{max}}]\nto maximize along the trajectory. Given a task, an agent can display a range of behaviors that varies greatly. A\ncriterion\nÎ»\n:\nğ’¯\n~\nâ†’\nâ„’\nâ€‹\n(\nÎ»\n)\n\\lambda:\\tilde{\\mathcal{T}}\\rightarrow\\mathcal{L(\\lambda)}\nis a tool to describe such variations. It can range from\nâ€the vector of an unsupervised learned trajectory encoderâ€\nto\nâ€the speed class of my agentâ€\nand projects any subtrajectory into a\nlabel\nin\nâ„’\nâ€‹\n(\nÎ»\n)\n\\mathcal{L}(\\lambda)\n. For instance, we can have\nz\nâˆˆ\nâ„’\nâ€‹\n(\nÎ»\n)\n=\nâ„\nd\nz\\in\\mathcal{L}(\\lambda)=\\mathbb{R}^{d}\nor\nâ€fastâ€\nâˆˆ\nâ„’\nâ€‹\n(\nÎ»\n)\n=\n{\nâ€slowâ€\n,\nâ€fastâ€\n}\n\\textit{\"fast\"}\\in\\mathcal{L}(\\lambda)=\\{\\textit{\"slow\"},\\textit{\"fast\"}\\}\n. Consequently, in the more general sense, aligning oneâ€™s behavior to a given\nbehavior style\ncorresponds to generating subtrajectories which verify a certain label, given a criterion and a task.\nStyle labeling and data programming.\nThe various definitions of behavior styles in the literature can be divided into unsupervised settings\n(Li\net al.\n,\n2017\n; Hausman\net al.\n,\n2017\n; Wang\net al.\n,\n2017\n; Mao\net al.\n,\n2024\n; Petitbois\net al.\n,\n2025\n)\nand supervised settings\n(Zhan\net al.\n,\n2020\n; Yang\net al.\n,\n2024\n)\n. In particular, following\nZhan\net al.\n(\n2020\n); Yang\net al.\n(\n2024\n)\n, we focus on the data programming\n(Ratner\net al.\n,\n2017\n)\nparadigm, using labeling functions as the criterion. However, unlike\nZhan\net al.\n(\n2020\n); Yang\net al.\n(\n2024\n)\n, which define their labeling functions on full trajectories given any criterion\nÎ»\n\\lambda\n, we define ours as hard-coded functions on subtrajectories\nÎ»\n:\nğ’¯\n~\nâ†’\nâŸ¦\n0\n,\n|\nÎ»\n|\nâˆ’\n1\nâŸ§\n\\lambda:\\tilde{\\mathcal{T}}\\rightarrow\\llbracket 0,|\\lambda|-1\\rrbracket\n, with\n|\nÎ»\n|\n|\\lambda|\nthe number of categories of\nÎ»\n\\lambda\n. Using such labeling functions has several benefits. As noted in\nZhan\net al.\n(\n2020\n)\n, labeling functions are simple to specify yet highly flexible. They reduce\nlabeling cost\nby eliminating manual annotation, which is often time-consuming and expensive, and, crucially, they enhance interpretability, a key limitation of unsupervised approaches, thereby enabling clearer notions of\ninterpretability\nand more direct\nalignment measurement\n. While previous works as\nZhan\net al.\n(\n2020\n); Yang\net al.\n(\n2024\n)\nhave focused on trajectory-level labels\nÎ»\nâ€‹\n(\nÏ„\n)\n\\lambda(\\tau)\n, we argue that relying on per-timestep labeling functions, defined in our framework as labels of windows, is a more pragmatic choice. Indeed, as various styles can have various timescales, styles can in fact vary across a trajectory, which can lead to avoidable\ncredit assignment\nissues. As such, given a labeling function\nÎ»\n\\lambda\n, we annotate the dataset\nğ’Ÿ\n\\mathcal{D}\nby marking each state-action pair\n(\ns\nt\n,\na\nt\n)\n(s_{t},a_{t})\nof each of its trajectories\nÏ„\n\\tau\nas â€contributingâ€ to the style of its corresponding window of radius\nw\nâ€‹\n(\nÎ»\n)\nw(\\lambda)\n:\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n=\n{\n(\ns\nt\n,\na\nt\n,\ns\nt\n+\n1\n,\nz\nt\n)\n:\nt\n=\n0\n,\nâ€¦\n,\nT\nâˆ’\n1\n,\nÏ„\nâˆˆ\nğ’Ÿ\n}\n,\n\\lambda(\\mathcal{D})=\\{(s_{t},a_{t},s_{t+1},z_{t}):t=0,\\ldots,T-1,\\ \\tau\\in\\mathcal{D}\\},\nwith\nz\nt\n=\nÎ»\nâ€‹\n(\nÏ„\nt\nâˆ’\nw\nâ€‹\n(\nÎ»\n)\n+\n1\n:\nt\n+\nw\nâ€‹\n(\nÎ»\n)\n)\nz_{t}=\\lambda(\\tau_{t-w(\\lambda)+1:t+w(\\lambda)})\n. Boundary handling (e.g., truncated windows) is implementation-specific and omitted for clarity.\nThis per-timestep annotation allows styles to vary along trajectories and mitigates credit assignment issues arising for episode scale labeling as in\nYang\net al.\n(\n2024\n)\n.\nTask performance metric.\nTraditionally, the RL problem corresponds to the maximization of the\ntask performance metric\n, defined as the expected discounted cumulated sum of rewards, given any given reward function\nr\nr\n:\nJ\nâ€‹\n(\nÏ€\n)\n=\nğ”¼\nÏ€\nâ€‹\n[\nâˆ‘\nt\n=\n0\nâˆ\nÎ³\nt\nâ€‹\nr\nâ€‹\n(\ns\nt\n,\na\nt\n)\n]\n.\nJ(\\pi)=\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t},a_{t})\\right].\n(1)\nTrue style alignment objective.\nAdditionally, given a criterion\nÎ»\n\\lambda\nand a target style\nz\nâˆˆ\nâ„’\nâ€‹\n(\nÎ»\n)\nz\\in\\mathcal{L}(\\lambda)\n, a natural objective for style alignment would correspond to the generation of trajectories which maximally exhibit style\nz\nz\n:\nS\nğŸ™\nâ€‹\n(\nÏ€\n,\nÎ»\n,\nz\n)\n=\nğ”¼\nÏ€\nâ€‹\n[\nâˆ‘\nt\n=\n0\nâˆ\nÎ³\nt\nâ€‹\nğŸ™\nâ€‹\n{\nÎ»\nâ€‹\n(\nÏ„\nt\nâˆ’\nw\nâ€‹\n(\nÎ»\n)\n+\n1\n:\nt\n+\nw\nâ€‹\n(\nÎ»\n)\n)\n=\nz\n}\n]\n.\nS^{\\mathbbm{1}}(\\pi,\\lambda,z)=\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbbm{1}\\{\\lambda(\\tau_{t-w(\\lambda)+1:t+w(\\lambda)})=z\\}\\right].\n(2)\nThis objective is well-defined but generally\nnon-Markovian\nwith respect to the original state space, since the label depends on a trajectory window (past and future).\nOptimizingÂ equation\n2\ndirectly would require augmenting the state with sufficient trajectory context. Nevertheless, a simpler probabilistic relaxation can be obtained by conditioning on the current state-action pair:\np\nÏ€\nÎ»\nâ€‹\n(\nz\nâˆ£\ns\nt\n,\na\nt\n)\n=\nâ„™\nÏ€\nâ€‹\n(\nÎ»\nâ€‹\n(\nÏ„\nt\nâˆ’\nw\nâ€‹\n(\nÎ»\n)\n+\n1\n:\nt\n+\nw\nâ€‹\n(\nÎ»\n)\n)\n=\nz\nâˆ£\ns\nt\n,\na\nt\n)\n,\np_{\\pi}^{\\lambda}(z\\mid s_{t},a_{t})=\\mathbb{P}_{\\pi}\\!\\left(\\lambda(\\tau_{t-w(\\lambda)+1:t+w(\\lambda)})=z\\mid s_{t},a_{t}\\right),\n(3)\nyielding the equivalent objective\nS\n~\np\nâ€‹\n(\nÏ€\n,\nÎ»\n,\nz\n)\n=\nğ”¼\nÏ€\nâ€‹\n[\nâˆ‘\nt\n=\n0\nâˆ\nÎ³\nt\nâ€‹\np\nÏ€\nÎ»\nâ€‹\n(\nz\nâˆ£\ns\nt\n,\na\nt\n)\n]\n.\n\\tilde{S}^{p}(\\pi,\\lambda,z)=\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}p_{\\pi}^{\\lambda}(z\\mid s_{t},a_{t})\\right].\n(4)\nHowever, computing\np\nÏ€\nÎ»\nâ€‹\n(\nz\nâˆ£\ns\n,\na\n)\np_{\\pi}^{\\lambda}(z\\mid s,a)\nfor arbitrary policies\nÏ€\n\\pi\nrequires modeling long-horizon trajectory distributions and is generally intractable.\nSurrogate Markov style objective.\nInstead, in order to fit the MDP and RL framework, we construct a\nsurrogate Markovian style reward\nfrom the demonstration dataset.\nUsing the annotated dataset\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n\\lambda(\\mathcal{D})\n, we estimate a label predictor\np\nÏ€\nğ’Ÿ\nÎ»\nâ€‹\n(\nz\nâˆ£\ns\n,\na\n)\np_{\\pi_{\\mathcal{D}}}^{\\lambda}(z\\mid s,a)\n,\nrepresenting the probability that a state-action pair\n(\ns\n,\na\n)\n(s,a)\nlies in the center of a window labeled\nz\nz\nin the dataset\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n\\lambda(\\mathcal{D})\ngenerated by\nÏ€\nğ’Ÿ\n\\pi_{\\mathcal{D}}\n.\nWe then define the surrogate objective:\nS\np\nâ€‹\n(\nÏ€\n,\nÎ»\n,\nz\n)\n=\nğ”¼\nÏ€\nâ€‹\n[\nâˆ‘\nt\n=\n0\nâˆ\nÎ³\nt\nâ€‹\np\nÏ€\nğ’Ÿ\nÎ»\nâ€‹\n(\nz\nâˆ£\ns\nt\n,\na\nt\n)\n]\n,\nS^{p}(\\pi,\\lambda,z)=\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}p_{\\pi_{\\mathcal{D}}}^{\\lambda}(z\\mid s_{t},a_{t})\\right],\n(5)\nThis defines a standard Markov reward function\nr\nğ’Ÿ\nÎ»\nâ€‹\n(\ns\n,\na\n)\n=\np\nÏ€\nğ’Ÿ\nÎ»\nâ€‹\n(\nz\nâˆ£\ns\n,\na\n)\nr_{\\mathcal{D}}^{\\lambda}(s,a)=p_{\\pi_{\\mathcal{D}}}^{\\lambda}(z\\mid s,a)\n,\nand therefore a well-posed MDP optimization problem. In the following, we will refer to this surrogate objective when writing about style alignment optimization. In general, it provides a tractable approximation that encourages policies to visit state-action pairs judged as style-consistent by the dataset-derived label model. This casting of (\n4\n) to (\n5\n) enables the direct application of standard offline reinforcement learning methods on the demonstration dataset\nğ’Ÿ\n\\mathcal{D}\n. We show experimentally that this objective still permits high alignment when computing the empirical true style alignment metric in Equation\n16\n.\n4\nOptimizing Task Performance Under Style Alignment\nIn this work, we aim to learn\nstylized and high-performing policies\nfrom a fixed offline dataset\nğ’Ÿ\n\\mathcal{D}\n.\nGiven a style criterion\nÎ»\n\\lambda\nand a target style label\nz\nz\n, let\nÎ \nÎ»\nâ€‹\n(\nz\n)\n=\narg\nâ¡\nmax\nÏ€\nâ¡\nS\np\nâ€‹\n(\nÏ€\n,\nÎ»\n,\nz\n)\n\\Pi^{\\lambda}(z)=\\arg\\max_{\\pi}S^{p}(\\pi,\\lambda,z)\n(6)\ndenote the set of policies achieving maximal alignment with style\nz\nz\n.\nAs the behaviors in\nğ’Ÿ\n\\mathcal{D}\nmay vary in quality, our conceptual objective is to select, among these style-optimal policies, one that attains the highest task performance:\nâˆ€\nz\nâˆˆ\nâ„’\n(\nÎ»\n)\n,\nÏ€\nr\n|\nÎ»\n,\nâˆ—\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\nâˆˆ\narg\nmax\nÏ€\nâˆˆ\nÎ \nÎ»\nâ€‹\n(\nz\n)\nJ\n(\nÏ€\n)\n,\n\\forall z\\in\\mathcal{L}(\\lambda),\\pi^{r|\\lambda,*}(\\cdot|\\cdot,z)\\in\\arg\\max_{\\pi\\in\\Pi^{\\lambda}(z)}J(\\pi),\n(7)\nThis formulation expresses our two goals. First, we want to learn a style conditioned policy\nÏ€\nÎ»\n,\nâˆ—\n\\pi^{\\lambda,*}\nsuch that\nâˆ€\nz\nâˆˆ\nâ„’\n(\nÎ»\n)\n,\nÏ€\nÎ»\n,\nâˆ—\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\nâˆˆ\nÎ \nÎ»\n(\nz\n)\n\\forall z\\in\\mathcal{L}(\\lambda),\\pi^{\\lambda,*}(\\cdot|\\cdot,z)\\in\\Pi^{\\lambda}(z)\n. Secondly, we want to learn\nÏ€\nr\n|\nÎ»\n,\nâˆ—\n\\pi^{r|\\lambda,*}\nby increasing the task performance of\nÏ€\nÎ»\n,\nâˆ—\n\\pi^{\\lambda,*}\nwithout hurting its style alignment. Achieving those goals in an offline reinforcement learning setting raises challenges.\nChallenge 1: Offline policy improvement under distribution shift.\nRegardless of the specific reward function\nr\nr\n, and in particular for the surrogate style reward defined in EquationÂ equation\n5\n, offline RL algorithms must operate without further interaction with the environment. This makes it necessary to keep the learned policy close to the data-generating policy\nÏ€\nğ’Ÿ\n\\pi_{\\mathcal{D}}\nin order to avoid out-of-distribution actions whose value estimates may be severely overestimated. This constraint directly conflicts with the objective of improving upon the behavior present in\nğ’Ÿ\n\\mathcal{D}\n, creating a fundamental tension between policy improvement and distributional shift.\nChallenge 2: Sparse and missing style supervision.\nThe surrogate style reward in EquationÂ equation\n5\nis typically sparse: many styles are only exhibited in a small subset of states, and some stateâ€“style pairs\n(\ns\n,\nz\n)\n(s,z)\nmay be entirely absent from the labeled dataset\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n\\lambda(\\mathcal{D})\n. As a consequence, direct behavior cloning of style-conditioned policies is often impossible, since no demonstration exists for many relevant\n(\ns\n,\na\n,\nz\n)\n(s,a,z)\ntuples. FigureÂ 1 illustrates this issue: transitions\n(\ns\n,\na\n)\n(s,a)\nthat do not appear in trajectories labeled with style\nz\nz\nprovide no immediate supervision for aligning with\nz\nz\n, calling for planning capabilities beyond single-step imitation. Moreover, a target style\nz\nz\nmay never appear within a single complete trajectory in\nğ’Ÿ\n\\mathcal{D}\n, requiring the policy to\nstitch\ntogether partial behaviors from different trajectories. This further emphasizes the need for long-term decision-making and trajectory stitching in order to achieve consistent style execution.\nChallenge 3: Intractability of explicit projection onto the style-optimal set.\nThe objective in EquationÂ equation\n7\ninvolves projecting onto the set of style-optimal policies\nÎ \nÎ»\nâ€‹\n(\nz\n)\n\\Pi^{\\lambda}(z)\nbefore optimizing task performance. Explicitly computing this set is infeasible in practice: it would require solving a full offline RL problem to global optimality for the style objective, enumerating all optimal policies, and then performing a second constrained optimization step within this set. Furthermore, approximation errors in value estimation and limited dataset coverage make exact constraint enforcement brittle in the offline setting. These difficulties call for a practical policy improvement rule that approximates the projection behavior of EquationÂ equation\n7\nwhile remaining stable under offline training.\nFigure 1:\nLong term decision making and stitching challenges for style alignment optimization.\nAchieving movement styles such as high-speed running may require to standing and accelerating, which means navigating through different speed styles and demands\nlong-term decision making\n. Also, trajectories in\nğ’Ÿ\n\\mathcal{D}\nmay not cover all the speed styles, calling for\ntrajectory stitching\nsuch as (slow\nâ†’\n\\rightarrow\nmedium) and (medium\nâ†’\n\\rightarrow\nfast).\n4.1\nAddressing challenge 1: Offline policy learning under distribution shift.\nThe first cornerstone of our objective is to be able to extract from\nğ’Ÿ\n\\mathcal{D}\na policy\nÏ€\nr\n,\nâˆ—\n:\nğ’®\nâ†’\nÎ”\nâ€‹\n(\nğ’œ\n)\n\\pi^{r,*}:\\mathcal{S}\\to\\Delta(\\mathcal{A})\nthat maximizes any objective\nJ\nâ€‹\n(\nÏ€\n)\nJ(\\pi)\n. For this, we employ the well-known IQL algorithm\n(Kostrikov\net al.\n,\n2021\n)\n, which mitigates value overestimation by estimating the optimal value function through expectile regression:\nâ„’\nV\nr\nâ€‹\n(\nÏ•\nr\n)\n\\displaystyle\\mathcal{L}_{V^{r}}(\\phi^{r})\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\nD\nâ€‹\n[\nâ„“\n2\nÎº\nâ€‹\n(\nQ\nÎ¸\nÂ¯\nr\nr\nâ€‹\n(\ns\n,\na\n)\nâˆ’\nV\nÏ•\nr\nr\nâ€‹\n(\ns\n)\n)\n]\n\\displaystyle=\\mathbb{E}_{(s,a)\\sim D}\\left[\\ell_{2}^{\\kappa}\\big(Q^{r}_{\\bar{\\theta}^{r}}(s,a)-V^{r}_{\\phi^{r}}(s)\\big)\\right]\n(8)\nâ„’\nQ\nr\nâ€‹\n(\nÎ¸\nr\n)\n\\displaystyle\\mathcal{L}_{Q^{r}}(\\theta^{r})\n=\nğ”¼\n(\ns\n,\na\n,\ns\nâ€²\n)\nâˆ¼\nD\nâ€‹\n[\n(\nr\nâ€‹\n(\ns\n,\na\n)\n+\nÎ³\nâ€‹\nV\nÏ•\nr\nr\nâ€‹\n(\ns\nâ€²\n)\nâˆ’\nQ\nÎ¸\nr\nr\nâ€‹\n(\ns\n,\na\n)\n)\n2\n]\n\\displaystyle=\\mathbb{E}_{(s,a,s^{\\prime})\\sim D}\\Big[\\big(r(s,a)+\\gamma V^{r}_{\\phi^{r}}(s^{\\prime})-Q^{r}_{\\theta^{r}}(s,a)\\big)^{2}\\Big]\n(9)\nwhere\nâ„“\n2\nÎº\nâ€‹\n(\nu\n)\n=\n|\nÎº\nâˆ’\nğŸ™\nâ€‹\n{\nu\n<\n0\n}\n|\nâ€‹\nu\n2\n,\nÎº\nâˆˆ\n[\n0.5\n,\n1\n)\n\\ell_{2}^{\\kappa}(u)=|\\kappa-\\mathbbm{1}\\{u<0\\}|u^{2},\\kappa\\in[0.5,1)\nis the expectile loss, an asymmetric squared loss that biases\nV\nÏ•\nr\nr\nV^{r}_{\\phi^{r}}\ntoward the upper tail of the\nQ\nÎ¸\nr\nr\nQ^{r}_{\\theta^{r}}\ndistribution. The trained\nV\nÏ•\nr\nr\nV^{r}_{\\phi^{r}}\nand\nQ\nÎ¸\nr\nr\nQ^{r}_{\\theta^{r}}\nare then used to learn a policy network\nÏ€\nÏˆ\nr\nr\n\\pi^{r}_{\\psi^{r}}\nvia Advantage-Weighted Regression (AWR)\n(Peng\net al.\n,\n2019\n)\n:\nJ\nÏ€\nr\nâ€‹\n(\nÏˆ\nr\n)\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\nğ’Ÿ\nâ€‹\n[\ne\nÎ²\nr\nâ‹…\nA\nÎ¸\nÂ¯\nr\n,\nÏ•\nr\nr\nâ€‹\n(\ns\n,\na\n)\nâ€‹\nlog\nâ¡\nÏ€\nÏˆ\nr\nr\nâ€‹\n(\na\n|\ns\n)\n]\nJ_{\\pi^{r}}(\\psi^{r})=\\mathbb{E}_{(s,a)\\sim\\mathcal{D}}\\left[e^{{\\beta^{r}\\cdot A^{r}_{\\bar{\\theta}^{r},\\phi^{r}}(s,a)}}\\log\\pi^{r}_{\\psi^{r}}(a|s)\\right]\n(10)\nwith\nÎ²\nâˆˆ\n(\n0\n,\nâˆ\n]\n\\beta\\in(0,\\infty]\nan inverse temperature and advantage:\nA\nÎ¸\nÂ¯\nr\n,\nÏ•\nr\nr\nâ€‹\n(\ns\n,\na\n)\n=\nQ\nÎ¸\nÂ¯\nr\nr\nâ€‹\n(\ns\n,\na\n)\nâˆ’\nV\nÏ•\nr\nr\nâ€‹\n(\ns\n)\n,\nA^{r}_{\\bar{\\theta}^{r},\\phi^{r}}(s,a)=Q^{r}_{\\bar{\\theta}^{r}}(s,a)-V^{r}_{\\phi^{r}}(s),\nwhich measures how much better or worse action\na\na\nin state\ns\ns\nis compared to the baseline value. This procedure corresponds to cloning dataset state-action pairs with a bias toward actions with higher advantages.\n4.2\nAddressing challenge 2: Sparse and missing style supervision.\nTo optimize for style alignment, we introduce SCIQL, a simple adaptation of IQL which employs the same principles of relabeling as the GCRL literature\n(Park\net al.\n,\n2025\n)\nto optimize for any given criterion\nÎ»\n\\lambda\nthe style-conditioned alignment objective:\nÏ€\nÎ»\n,\nâˆ—\n:\nğ’®\nâ†’\nÎ”\n(\nğ’œ\n)\nâˆˆ\nÎ \nÎ»\n(\nz\n)\n,\nâˆ€\nz\nâˆˆ\nâ„’\n(\nÎ»\n)\n\\pi^{\\lambda,*}:\\mathcal{S}\\rightarrow\\Delta(\\mathcal{A})\\in\\Pi^{\\lambda}(z),\\forall z\\in\\mathcal{L}(\\lambda)\n. As in IQL, SCIQL first fits the optimal style-conditioned value functions through neural networks\nV\nÏ•\nÎ»\nÎ»\nV_{\\phi_{\\lambda}}^{\\lambda}\nand\nQ\nÎ¸\nÎ»\nÎ»\nQ_{\\theta_{\\lambda}}^{\\lambda}\nusing expectile regression:\nâ„’\nV\nÎ»\nâ€‹\n(\nÏ•\nÎ»\n)\n\\displaystyle\\mathcal{L}_{V^{\\lambda}}(\\phi^{\\lambda})\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\nğ’Ÿ\nz\nâˆ¼\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n[\nâ„“\nÎº\n2\nâ€‹\n(\nQ\nÎ¸\nÂ¯\nÎ»\nÎ»\nâ€‹\n(\ns\n,\na\n,\nz\n)\nâˆ’\nV\nÏ•\nÎ»\nÎ»\nâ€‹\n(\ns\n,\nz\n)\n)\n]\n,\n\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}(s,a)\\sim\\mathcal{D}\\\\\nz\\sim\\lambda(\\mathcal{D})\\end{subarray}}\\Big[\\ell_{\\kappa}^{2}\\!\\big(Q^{\\lambda}_{\\bar{\\theta}^{\\lambda}}(s,a,z)-V^{\\lambda}_{\\phi^{\\lambda}}(s,z)\\big)\\Big],\n(11)\nâ„’\nQ\nÎ»\nâ€‹\n(\nÎ¸\nÎ»\n)\n\\displaystyle\\mathcal{L}_{Q^{\\lambda}}(\\theta^{\\lambda})\n=\nğ”¼\n(\ns\n,\na\n,\ns\nâ€²\n)\nâˆ¼\nğ’Ÿ\nz\nâˆ¼\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n[\n(\nÏ‡\nÏ‰\nÎ»\nÎ»\nâ€‹\n(\ns\n,\na\n,\nz\n)\n+\nÎ³\nâ€‹\nV\nÏ•\nÎ»\nÎ»\nâ€‹\n(\ns\nâ€²\n,\nz\n)\nâˆ’\nQ\nÎ¸\nÎ»\nÎ»\nâ€‹\n(\ns\n,\na\n,\nz\n)\n)\n2\n]\n.\n\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}(s,a,s^{\\prime})\\sim\\mathcal{D}\\\\\nz\\sim\\lambda(\\mathcal{D})\\end{subarray}}\\Big[\\big(\\chi^{\\lambda}_{\\omega^{\\lambda}}(s,a,z)+\\gamma V^{\\lambda}_{\\phi^{\\lambda}}(s^{\\prime},z)-Q^{\\lambda}_{\\theta^{\\lambda}}(s,a,z)\\big)^{2}\\Big].\n(12)\nwith\nÏ‡\nÏ‰\nÏ‡\nÎ»\nâ€‹\n(\ns\n,\na\n,\nz\n)\n\\chi_{\\omega_{\\chi}}^{\\lambda}(s,a,z)\nan estimator of\np\nÏ€\nğ’Ÿ\nÎ»\nâ€‹\n(\nz\n|\ns\n,\na\n)\np_{\\pi_{\\mathcal{D}}}^{\\lambda}(z|s,a)\n. Comparing between several strategies, we empirically found (see Appendix\nE.1\n) that taking\nÏ‡\nÏ‰\nÎ»\nÎ»\nâ€‹\n(\ns\n,\na\n,\nz\n)\n=\nğŸ™\nâ€‹\n(\nz\n=\nz\nc\n)\n\\chi^{\\lambda}_{\\omega^{\\lambda}}(s,a,z)=\\mathbbm{1}(z=z_{\\mathrm{c}})\nwith\nz\nc\nz_{\\mathrm{c}}\nthe associated label of\n(\ns\n,\na\n,\ns\nâ€²\n)\n(s,a,s^{\\prime})\nwithin\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n\\lambda(\\mathcal{D})\nto be one of the best performing methods, which we kept for its simplicity. This sampling of style labels outside the joint distribution\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n,\na\n,\nz\n)\np^{\\mathcal{\\lambda(D)}}(s,a,z)\nenables to address\ndistribution-shift\n. After that, we extract a style-conditioned policy\nÏ€\nÏˆ\nÎ»\nÎ»\n\\pi^{\\lambda}_{\\psi^{\\lambda}}\nthrough AWR by optimizing:\nJ\nÏ€\nÎ»\nâ€‹\n(\nÏˆ\nÎ»\n)\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\nğ’Ÿ\nz\nâˆ¼\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n[\ne\nÎ²\nÎ»\nâ‹…\nA\nÎ¸\nÂ¯\nÎ»\n,\nÏ•\nÎ»\nÎ»\nâ€‹\n(\ns\n,\na\n,\nz\n)\nâ€‹\nlog\nâ¡\nÏ€\nÏˆ\nÎ»\nÎ»\nâ€‹\n(\na\n|\ns\n,\nz\n)\n]\nJ_{\\pi^{\\lambda}}(\\psi^{\\lambda})=\\mathbb{E}_{\\begin{subarray}{c}(s,a)\\sim\\mathcal{D}\\\\\nz\\sim\\lambda(\\mathcal{D})\\end{subarray}}\\Big[e^{\\beta^{\\lambda}\\cdot A^{\\lambda}_{\\bar{\\theta}^{\\lambda},\\phi^{\\lambda}}(s,a,z)}\\log\\pi^{\\lambda}_{\\psi^{\\lambda}}(a|s,z)\\Big]\n(13)\nThis objective drives\nÏ€\nÏˆ\nÎ»\nÎ»\n\\pi^{\\lambda}_{\\psi^{\\lambda}}\nto copy the datasetâ€™s actions with a bias toward actions likely to lead in the future to the visitation of state-actions pairs of high likelihood of contribution to the style in conditioning. This formulation effectively works with styles outside of the joint distribution and leads as we see in the experiment section\n2\nto a more\nrobust style alignment\n.\n4.3\nAddressing challenge 3: Approximating the projection onto the style-optimal set\nRecall our conceptual objective in EquationÂ equation\n7\n: for each target label\nz\nz\n, we seek to maximize task performance\nwithin\nthe set of policies that maximize style alignment.\nA natural way to formalize this objective is through multi-objective or constrained reinforcement learning, for instance by optimizing a weighted combination of task and style returns or by maximizing task performance subject to a constraint on style alignment.\nWhile these frameworks are well-suited when a precise trade-off or constraint threshold is available, in our setting such a threshold is not naturally defined: different styles may admit different achievable alignment scores, and selecting a universal constraint level introduces an additional hyperparameter that is difficult to tune from data alone. Moreover, in the offline setting, constrained optimization approaches typically introduce additional dual variables or constraint hyperparameters whose optimization can be sensitive to value approximation errors and limited dataset coverage, making stable training challenging in practice. Nevertheless, exploring principled constrained or multi-objective formulations for stylized offline reinforcement learning is an interesting direction for future work.\nInstead, we propose to adopt a more direct\npolicy improvement principle\nthat approximates the projection behavior of EquationÂ equation\n7\nwhile remaining simple and stable under offline training.\nLet\nÏ€\nÎ»\n\\pi^{\\lambda}\ndenote a style-aligned policy for label\nz\nz\n, with associated style advantage\nA\nÎ»\nâ€‹\n(\ns\n,\na\n,\nz\n)\nA^{\\lambda}(s,a,z)\n, and let\nA\nr\nâ€‹\n(\ns\n,\na\n)\nA^{r}(s,a)\ndenote the task advantage.\nPositive style advantage indicates transitions that improve alignment with the target style, while negative values correspond to detrimental transitions.\nOur objective is to incorporate task improvement signals primarily when they are unlikely to harm style alignment, thereby encouraging increases in task performance without sacrificing controllability.\nGated Advantage Weighted Regression (GAWR).\nWe implement this principle through a gated combination of style and task advantages.\nConcretely, we define the\ngated advantage\nÎ¾\nâ€‹\n(\nA\nÎ»\n,\nA\nr\n)\nâ€‹\n(\ns\n,\na\n,\nz\n)\n=\nA\nÎ»\nâ€‹\n(\ns\n,\na\n,\nz\n)\n+\nÏƒ\nâ€‹\n(\nA\nÎ»\nâ€‹\n(\ns\n,\na\n,\nz\n)\n)\nâ‹…\nA\nr\nâ€‹\n(\ns\n,\na\n)\n,\n\\xi(A^{\\lambda},A^{r})(s,a,z)=A^{\\lambda}(s,a,z)+\\sigma\\!\\big(A^{\\lambda}(s,a,z)\\big)\\cdot A^{r}(s,a),\n(14)\nwhere\nÏƒ\nâ€‹\n(\nâ‹…\n)\n\\sigma(\\cdot)\nis the sigmoid function. Advantages can be normalized with an exponential moving average in order to share a similar order of magnitude.\nWe then perform a weighted behavior cloning update using\nÎ¾\n\\xi\nas the preference score, yielding the style-conditioned task policy\nÏ€\nr\n|\nÎ»\n\\pi^{r|\\lambda}\n:\nJ\nÏ€\nr\n|\nÎ»\nâ€‹\n(\nÏˆ\nr\n|\nÎ»\n)\n\\displaystyle J_{\\pi^{r|\\lambda}}(\\psi^{r|\\lambda})\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\nğ’Ÿ\nz\nâˆ¼\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n[\ne\nÎ²\nr\n|\nÎ»\nâ€‹\nÎ¾\nâ€‹\n(\nA\nÎ¸\nÂ¯\nÎ»\n,\nÏ•\nÎ»\nÎ»\n,\nA\nÎ¸\nÂ¯\nr\n,\nÏ•\nr\nr\n)\nâ€‹\n(\ns\n,\na\n,\nz\n)\nâ€‹\nlog\nâ¡\nÏ€\nÏˆ\nr\n|\nÎ»\nr\n|\nÎ»\nâ€‹\n(\na\nâˆ£\ns\n,\nz\n)\n]\n.\n\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}(s,a)\\sim\\mathcal{D}\\\\\nz\\sim\\lambda(\\mathcal{D})\\end{subarray}}\\Big[e^{\\beta^{r|\\lambda}\\xi\\big(A^{\\lambda}_{\\bar{\\theta}^{\\lambda},\\phi^{\\lambda}},A^{r}_{\\bar{\\theta}^{r},\\phi^{r}}\\big)(s,a,z)}\\log\\pi^{r|\\lambda}_{\\psi^{r|\\lambda}}(a\\mid s,z)\\Big].\n(15)\nCompared to an unconstrained combination such as\nA\nÎ»\nâ€‹\n(\ns\n,\na\n,\nz\n)\n+\nA\nr\nâ€‹\n(\ns\n,\na\n)\nA^{\\lambda}(s,a,z)+A^{r}(s,a)\n, which may allow task optimization to dominate and reduce style alignment, GAWR explicitly modulates the task signal based on the style signal. This yields a practical approximation to the projection in EquationÂ equation\n7\nand empirically produces improvements in task performance while preserving high style alignment.\nWe provide the full training pipeline in Algorithm\n1\n. As in IQL and related methods, while written separately,\n(Kostrikov\net al.\n,\n2021\n; Park\net al.\n,\n2024\n)\n, value learning and policy extraction can be performed jointly in practice. This allows the whole training within a single loop, simplifying considerably the process.\nAlgorithm 1\nStyle-Conditioned Implicit Q-Learning with Gated Advantage Weighted Regression.\nInput:\noffline dataset\nğ’Ÿ\n\\mathcal{D}\n, labeling function\nÎ»\n\\lambda\nInitialize\nÏ•\nÎ»\n,\nÎ¸\nr\n,\nÎ¸\nÂ¯\nr\n,\nÎ¸\nÎ»\n,\nÎ¸\nÂ¯\nÎ»\n,\nÏˆ\nr\n|\nÎ»\n\\phi^{\\lambda},\\theta^{r},\\bar{\\theta}^{r},\\theta^{\\lambda},\\bar{\\theta}^{\\lambda},\\psi^{r|\\lambda}\nwhile\nnot converged\ndo\n# Train the task value functions\nÏ•\nr\nâ†\nÏ•\nr\nâˆ’\nÎ½\nV\nr\nâ€‹\nâˆ‡\nâ„’\nV\nr\nâ€‹\n(\nÏ•\nr\n)\n\\phi^{r}\\leftarrow\\phi^{r}-\\nu_{V^{r}}\\nabla\\mathcal{L}_{V^{r}}(\\phi^{r})\n(Equation\n8\n)\nÎ¸\nr\nâ†\nÎ¸\nr\nâˆ’\nÎ½\nQ\nr\nâ€‹\nâˆ‡\nâ„’\nQ\nr\nâ€‹\n(\nÎ¸\nr\n)\n\\theta^{r}\\leftarrow\\theta^{r}-\\nu_{Q^{r}}\\nabla\\mathcal{L}_{Q^{r}}(\\theta^{r})\n(Equation\n9\n)\nÎ¸\nÂ¯\nr\nâ†\n(\n1\nâˆ’\nÏ…\nPolyak\n)\nâ€‹\nÎ¸\nÂ¯\nr\n+\nÏ…\nPolyak\nâ€‹\nÎ¸\nr\n\\bar{\\theta}^{r}\\leftarrow(1-\\upsilon_{\\mathrm{Polyak}})\\bar{\\theta}^{r}+\\upsilon_{\\mathrm{Polyak}}\\theta^{r}\nend\nwhile\nwhile\nnot converged\ndo\n# Train the style value functions\nÏ•\nÎ»\nâ†\nÏ•\nÎ»\nâˆ’\nÎ½\nV\nÎ»\nâ€‹\nâˆ‡\nâ„’\nV\nÎ»\nâ€‹\n(\nÏ•\nÎ»\n)\n\\phi^{\\lambda}\\leftarrow\\phi^{\\lambda}-\\nu_{V^{\\lambda}}\\nabla\\mathcal{L}_{V^{\\lambda}}(\\phi^{\\lambda})\n(Equation\n11\n)\nÎ¸\nÎ»\nâ†\nÎ¸\nÎ»\nâˆ’\nÎ½\nQ\nÎ»\nâ€‹\nâˆ‡\nâ„’\nQ\nÎ»\nâ€‹\n(\nÎ¸\nÎ»\n)\n\\theta^{\\lambda}\\leftarrow\\theta^{\\lambda}-\\nu_{Q^{\\lambda}}\\nabla\\mathcal{L}_{Q^{\\lambda}}(\\theta^{\\lambda})\n(Equation\n12\n)\nÎ¸\nÂ¯\nÎ»\nâ†\n(\n1\nâˆ’\nÏ…\nPolyak\n)\nâ€‹\nÎ¸\nÂ¯\nÎ»\n+\nÏ…\nPolyak\nâ€‹\nÎ¸\nÎ»\n\\bar{\\theta}^{\\lambda}\\leftarrow(1-\\upsilon_{\\mathrm{Polyak}})\\bar{\\theta}^{\\lambda}+\\upsilon_{\\mathrm{Polyak}}\\theta^{\\lambda}\nend\nwhile\nwhile\nnot converged\ndo\n# Train the policy\nÏ€\nÏˆ\nÎ»\nÎ»\n\\pi^{\\lambda}_{\\psi^{\\lambda}}\nthrough GAWR\nÏˆ\nr\n|\nÎ»\nâ†\nÏˆ\nr\n|\nÎ»\n+\nÎ½\nÏ€\nr\n|\nÎ»\nâ€‹\nâˆ‡\nJ\nÏ€\nr\n|\nÎ»\nâ€‹\n(\nÏˆ\nr\n|\nÎ»\n)\n\\psi^{r|\\lambda}\\leftarrow\\psi^{r|\\lambda}+\\nu_{\\pi^{r|\\lambda}}\\nabla J_{\\pi^{r|\\lambda}}(\\psi^{r|\\lambda})\n(Equation\n15\n)\nend\nwhile\nreturn\nPolicy\nÏ€\nÏˆ\nr\n|\nÎ»\nr\n|\nÎ»\n\\pi^{r|\\lambda}_{\\psi^{r|\\lambda}}\n5\nExperiments\nTable 1:\nStyle alignment results\n. SCIQL achieves the best style alignment performance by a large margin compared to previous baselines for every dataset. The results are the averages over the criteria, labels and 5 seeds. A more detailed table can be found in Appendix\nD\n.\nDataset\nBC\nCBC\nBCPMI\nSORL (\nÎ²\n=\n0\n\\beta=0\n)\nSCBC\nSCIQL\ncircle2d-inplace-v0\n29.1 Â± 6.3\n58.6 Â± 2.3\n58.9 Â± 2.6\n58.9 Â± 2.7\n68.6 Â± 2.0\n74.6 Â± 9.3\ncircle2d-navigate-v0\n29.1 Â± 5.3\n58.9 Â± 2.7\n59.9 Â± 2.3\n60.0 Â± 3.3\n67.2 Â± 1.8\n75.5 Â± 4.7\nhalfcheetah-fixed-v0\n30.0 Â± 5.9\n51.2 Â± 9.0\n58.1 Â± 8.4\n53.1 Â± 10.6\n58.0 Â± 5.3\n78.0 Â± 1.8\nhalfcheetah-stitch-v0\n30.0 Â± 6.8\n52.1 Â± 7.6\n58.9 Â± 11.3\n48.4 Â± 12.5\n57.4 Â± 4.7\n78.0 Â± 1.1\nhalfcheetah-vary-v0\n30.0 Â± 4.5\n52.0 Â± 12.0\n52.6 Â± 17.2\n46.7 Â± 9.5\n31.7 Â± 4.2\n78.9 Â± 0.7\nhumenv-simple-v0\n50.0 Â± 44.4\n89.1 Â± 22.0\n79.2 Â± 26.7\n79.4 Â± 26.9\n99.6 Â± 0.0\n99.6 Â± 0.0\nhumenv-complex-v0\n33.3 Â± 4.0\n47.1 Â± 12.8\n44.6 Â± 18.4\n47.7 Â± 6.9\n33.2 Â± 3.5\n83.5 Â± 6.2\n5.1\nExperimental setup\nAfter introducing environments in section\n5.1.1\n, we tackle the following experimental questions:\n1.\nHow does SCIQL compare to previous work on style alignment?\n2.\nDoes GAWR help SCIQL perform style conditioned task performance optimization?\n3.\nHow does SCIQL compare to previous work on style conditioned task performance optimization?\n5.1.1\nEnvironments, tasks, labels and datasets\nCircle2d\nis a modified version of the environment from\nLi\net al.\n(\n2017\n)\nand consists of a 2D plane where an agent can roam within a confined square to draw a target circle. For this environment, we define the labels:\nposition\n,\nmovement_direction\n,\nturn_direction\n,\nradius\n,\nspeed\n, and\ncurvature_noise\n. We generate two datasets using a hard-coded agent that draws circles with various centers and radii, orientations (clockwise and counter-clockwise), speeds, and action noise levels. The first dataset,\ncircle2d-inplace-v0\n, is obtained by drawing the circle directly from the start position, while the\ncircle2d-navigate-v0\ndataset is obtained by navigating to a target position before drawing the circle.\nHalfCheetah\n(Todorov\net al.\n,\n2012\n)\nis a task where the objective is to control a planar 6-DoF robot to move as far as possible in the forward direction. For this environment, we define the labels:\nspeed\n,\nangle\n,\ntorso_height\n,\nbackfoot_height\n, and\nfrontfoot_height\n. We train a diverse set of HalfCheetah policies using SAC\n(Haarnoja\net al.\n,\n2018\n)\nto generate three datasets:\nhalfcheetah-fixed-v0\n, where the policy is fixed throughout the trajectory;\nhalfcheetah-stitch-v0\n, where trajectories are split into short segments; and\nhalfcheetah-vary-v0\n, where the policy changes during the trajectory.\nHumEnv\n(Tirinzoni\net al.\n,\n2025\n)\nis a higher dimensional task consisting in controlling a SMPL skeleton\n(Loper\net al.\n,\n2023\n)\nwith 358-dimensional observations through a 69-dimensional action space to move as fast as possible on a flat plane. In\nhumenv-simple-v0\n, the humanoid is initialized in a standing position. We generate a stylized dataset using the Metamotivo-M1 model provided in\nTirinzoni\net al.\n(\n2025\n)\n, leading to various ways of moving at different heights and speeds and focus on a\nhead_height\ncriterion of 2 labels,\nlow\nand\nhigh\n. In\nhumenv-complex-v0\n, the humanoid is initialized in a lying down position, and the dataset is generated as in\nhumenv-simple-v0\n, but with style variations within the trajectory. Also, in\nhumenv-complex-v0\n, we define a\nspeed\ncriterion of 3 labels:\nimmobile\n,\nslow\nand\nfast\n, and a finer\nhead_height\ncriterion of 3 labels:\nlow\n,\nmedium\nand\nhigh\n. Further details about each environment, task, labeling function and dataset are provided in Appendix\nA\n.\n5.1.2\nBaselines and model details\nWe compare SCIQL against external state-of-the-art algorithms and a hierarchy of ablations designed to isolate the contributions of SCIQLâ€™s components. For the ablations, we begin with standard\nBC\nPomerleau (\n1991\n)\nas a non-conditioned reference. We then introduce\nConditioned BC (CBC)\n, which incorporates style conditioning using the current trajectory style. Finally, to analyze the benefits of style relabeling, we introduce\nSCBC\n, an IL variant of\nSCIQL\nwhich performs hindsight style relabeling by sampling style labels from the future trajectory, but without value functions. For external comparisons, we evaluate against\nBCPMI\n(Yang\net al.\n,\n2024\n)\n, which extends CBC via mutual-information weighting, and an adapted version of\nSORL\n(Mao\net al.\n,\n2024\n)\n(see Appendix\nC\n), which serves as the primary benchmark for optimizing task performance under style constraints. Further details on architectures and hyperparameters are provided in Appendix\nC\nand Appendix\nB\n.\n5.2\nResults on style alignment\nFigure 2:\nPareto fronts and hypervolumes of SORL and SCIQL.\nWe compare\nSORL\n(in blue) and\nSCIQL\n(in red).\nThe shaded areas (\n,\n) represent the hypervolumes covered by the methods.\nMarkers indicate different trade-off configurations:\nSORL\nis evaluated at\nÎ²\n=\n0\n\\beta=0\n(\n),\nÎ²\n=\n1\n\\beta=1\n(\n), and\nÎ²\n=\n3\n\\beta=3\n(\n).\nSCIQL\nis evaluated with style only\nÎ»\n\\lambda\n(\n), style-prioritized\nÎ»\n>\nr\n\\lambda>r\n(\n), and task-prioritized\nr\n>\nÎ»\nr>\\lambda\n(\n).\nSCIQL consistently achieves a larger hypervolume and dominates the Pareto frontier.\nOur first set of experiments evaluates the capability of SCIQL to achieve style alignment compared to baselines. For each style label\nz\nâˆˆ\nâ„’\nâ€‹\n(\nÎ»\n)\nz\\in\\mathcal{L}(\\lambda)\nof each criterion\nÎ»\n\\lambda\n, we perform 10 rollouts across 5 seeds, conditioned on\nz\nz\n(except BC, which does not support label conditioning). Each generated trajectory\nÏ„\n=\n{\n(\ns\nt\n,\na\nt\n)\n,\nt\nâˆˆ\n{\n0\n,\nâ€¦\n,\n|\nÏ„\n|\nâˆ’\n1\n}\n}\n\\tau=\\{(s_{t},a_{t}),\\,t\\in\\{0,\\dots,|\\tau|-1\\}\\}\nis then annotated as\nÎ»\nâ€‹\n(\nÏ„\n)\n=\n{\n(\ns\nt\n,\na\nt\n,\nz\nt\n)\n,\nt\nâˆˆ\n{\n0\n,\nâ€¦\n,\n|\nÏ„\n|\nâˆ’\n1\n}\n}\n\\lambda(\\tau)=\\{(s_{t},a_{t},z_{t}),\\,t\\in\\{0,\\dots,|\\tau|-1\\}\\}\nwith\nz\nt\n=\nÎ»\nâ€‹\n(\nÏ„\nt\nâˆ’\nw\nâ€‹\n(\nÎ»\n)\n+\n1\n:\nt\n+\nw\nâ€‹\n(\nÎ»\n)\n)\n,\nâˆ€\nt\nâˆˆ\n{\n0\n,\nâ€¦\n,\n|\nÏ„\n|\nâˆ’\n1\n}\nz_{t}=\\lambda(\\tau_{t-w(\\lambda)+1:t+w(\\lambda)}),\\forall t\\in\\{0,\\dots,|\\tau|-1\\}\n. For each annotated trajectory, we compute its empirical normalized undiscounted style alignment:\nS\n^\nğŸ™\nâ€‹\n(\nÎ»\nâ€‹\n(\nÏ„\n)\n,\nz\n)\n=\n1\n|\nÏ„\n|\nâ€‹\nâˆ‘\nt\n=\n0\n|\nÏ„\n|\nâˆ’\n1\nğŸ™\nâ€‹\n{\nz\nt\n=\nz\n}\n,\n\\hat{S}^{\\mathbbm{1}}(\\lambda(\\tau),z)=\\frac{1}{|\\tau|}\\sum_{t=0}^{|\\tau|-1}\\mathbbm{1}\\{z_{t}=z\\},\n(16)\nwhere the normalization by the trajectory length\n|\nÏ„\n|\n|\\tau|\nensures that\nS\n^\nğŸ™\nâ€‹\n(\nÎ»\nâ€‹\n(\nÏ„\n)\n,\nz\n)\nâˆˆ\n[\n0\n,\n1\n]\n\\hat{S}^{\\mathbbm{1}}(\\lambda(\\tau),z)\\in[0,1]\n, which hence represents the fraction of timesteps labeled as contributing to the target label. We then average alignments over 10 episodes to compute the empirical normalized undiscounted style alignment of our policy,\nS\n^\nğŸ™\nâ€‹\n(\nÏ€\n,\nÎ»\n,\nz\n)\n\\hat{S}^{\\mathbbm{1}}(\\pi,\\lambda,z)\n, which can be seen as the analogue of a GCRL success rate in the SCRL context. Because of the multiplicity of criteria and labels (see Appendix\nD\n), we report average alignments across all criteria and labels in Table\n1\n, with full results provided in Appendix\nD\n. Standard deviations are computed as the average across 5 seeds for the different tested\n(\nÎ»\n,\nz\n)\n(\\lambda,z)\n. We observe that SCIQL achieves the best style alignment performance by a large margin compared to previous baselines for every dataset, highlighting its effectiveness in long-term decision making and stitching, unlike prior methods. In particular, the performance gap between BC and CBC underscores the necessity of style conditioning. Moreover, the similar performance of SORL in imitation mode (\nÎ²\n=\n0\n\\beta=0\n), BCPMI, and CBC can be explained by the similarity of their objectives (see Appendix\nC\n), all corresponding to a weighted CBC without style relabeling. The performance gap between SCBC and the previous baselines further highlights the importance of integrating trajectory stitching and style relabeling within stylized policies, while the dominance of SCIQL demonstrates the additional benefits of value learning, which augments relabeling by integrating randomly sampled styles during training and enables more effective policy extraction overall.\n5.3\nResults on style-conditioned task performance optimization\nTo evaluate the capability of SCIQL to perform style-conditioned task performance optimization, we plot in Figure\n2\nthe average style alignments and normalized returns of SCIQL without GAWR (\nÎ»\n\\lambda\n), with a style-based GAWR (\nÎ»\n>\nr\n\\lambda>r\n), and with a reward-based GAWR (\nr\n>\nÎ»\nr>\\lambda\n) to compare to a task performance prioritization setting. We compare against SORL with various temperatures\nÎ²\n\\beta\n, which control the importance of task performance in the SORL objective (see Appendix\nC\n).\nTask performance optimization under style alignment\nFirst, we observe that incorporating task reward signals with GAWR increases the score of SCIQL while preserving the style alignment in nearly all environments, showing a decrease only for the halfcheetah-stitch-v0. Additionally, SCIQL (\nr\n>\nÎ»\nr>\\lambda\n), which prioritizes task performance to style alignment, shows logically greater performance but at the cost of style alignment.\nThis shows that the GAWR effectively permits the increase of task performance of our policies while maintaining style alignment.\nAchieving overall better joint performance\nSecondly, we compute the Hypervolumes (HV) of SORL and SCIQL variants and observe that SCIQL achieves a substantial improvement of +41.2% to +163.9% (see Figure\n2\n) of over SORL across environments. This indicates that SCIQL achieves a better overall task-performance to style-alignment tradeoff than SORL. In particular,\nSCIQL\nâ€‹\n(\nÎ»\n>\nr\n)\n\\mathrm{SCIQL}(\\lambda>r)\nlies closer to the ideal point\n(\n100\n,\n100\n)\n(100,100)\n, corresponding to a reduction in Euclidean distance to the ideal point of 18-28%.\nThis shows that SCIQL reaches a stronger joint performance between objectives than SORL, effectively shifting the Pareto frontier closer to theoretical optimality\n. See Appendix\nD\nand Appendix\nE\nfor more details and ablations.\n6\nConclusion\nWe propose a novel general definition of behavior styles within the sequential decision making framework and instantiate it by the use of labeling functions to learn\ninterpretable\nstyles with a low\nlabeling cost\nand easy\nalignment measurement\nwhile effectively avoiding unnecessary\ncredit assignment\nissues by relying on subtrajectories labeling. We then present the SCIQL algorithm which leverages Gated AWR to solve long-term decision making and trajectory stitching challenges while providing superior performance in both style alignment and style-conditioned task performance compared to previous work. An interesting next step would be to find ways to scale it to a multiplicity of criteria. Finding mechanisms to enhance the representation span of labeling functions could also be interesting, as well as integrating zero-shot capabilities to generate on the fly style-conditioned reinforcement learning policies.\n7\nReproducibility Statement\nTo ensure the reproducibility of our work, we detail our environments, tasks labels and datasets in Appendix\nA\n, the choice of architecture and hyperparameter in Appendix\nB\nand the baselines we use in Appendix\nC\n. Moreover, we provide links to clean implementations of our algorithms in JAX\n(Bradbury\net al.\n,\n2018\n)\nalong with the datasets in the following project page:\nhttps://sciql-iclr-2026.github.io/\n.\n8\nAcknowledgments\nThis work was granted access to the HPC resources of IDRIS under the allocations AD011014679R1, AD011014679R2 and A0181016109 made by GENCI.\nReferences\nM. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba (2017)\nHindsight experience replay\n.\nAdvances in neural information processing systems\n.\nCited by:\nÂ§2\n.\nS. Arora and P. Doshi (2020)\nA survey of inverse reinforcement learning: challenges, methods and progress\n.\nExternal Links:\n1806.06877\n,\nLink\nCited by:\nÂ§2\n.\nJ. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang (2018)\nJAX: composable transformations of Python+NumPy programs\nExternal Links:\nLink\nCited by:\nAppendix B\n,\nÂ§7\n.\nA. Canesse, M. Petitbois, L. Denoyer, S. Lamprier, and R. Portelas (2024)\nNavigation with qphil: quantizing planner for hierarchical implicit q-learning\n.\nExternal Links:\n2411.07760\n,\nLink\nCited by:\nÂ§2\n.\nI. Char, V. Mehta, A. Villaflor, J. M. Dolan, and J. Schneider (2022)\nBATS: best action trajectory stitching\n.\nExternal Links:\n2204.12026\n,\nLink\nCited by:\nÂ§1\n.\nY. Chebotar, K. Hausman, Y. Lu, T. Xiao, D. Kalashnikov, J. Varley, A. Irpan, B. Eysenbach, R. Julian, C. Finn, and S. Levine (2021)\nActionable models: unsupervised offline reinforcement learning of robotic skills\n.\nExternal Links:\n2104.07749\n,\nLink\nCited by:\nÂ§2\n.\nL. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch (2021)\nDecision transformer: reinforcement learning via sequence modeling\n.\nExternal Links:\n2106.01345\n,\nLink\nCited by:\nÂ§1\n,\nÂ§2\n.\nC. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song (2024a)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nExternal Links:\n2303.04137\nCited by:\nÂ§2\n.\nC. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song (2024b)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nThe International Journal of Robotics Research\n.\nCited by:\nÂ§1\n.\nP. Dayan (1993)\nImproving generalization for temporal difference learning: the successor representation\n.\nNeural Computation\n5\n(\n4\n),\npp.Â 613â€“624\n.\nExternal Links:\nDocument\nCited by:\nÂ§2\n.\nM. D. Donsker and S. R. S. Varadhan (1975)\nAsymptotic evaluation of certain markov process expectations for large time, i\n.\nCommunications on Pure and Applied Mathematics\n28\n(\n1\n),\npp.Â 1â€“47\n.\nExternal Links:\nDocument\n,\nLink\n,\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.3160280102\nCited by:\nAppendix C\n.\nP. Florence, C. Lynch, A. Zeng, O. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch, and J. Tompson (2021a)\nImplicit behavioral cloning\n.\nExternal Links:\n2109.00137\nCited by:\nÂ§2\n.\nP. Florence, C. Lynch, A. Zeng, O. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch, and J. Tompson (2021b)\nImplicit behavioral cloning\n.\nExternal Links:\n2109.00137\n,\nLink\nCited by:\nÂ§1\n.\nJ. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine (2021)\nD4RL: datasets for deep data-driven reinforcement learning\n.\nExternal Links:\n2004.07219\n,\nLink\nCited by:\nÂ§1\n.\nJ. Fu, K. Luo, and S. Levine (2018)\nLearning robust rewards with adversarial inverse reinforcement learning\n.\nExternal Links:\n1710.11248\n,\nLink\nCited by:\nÂ§2\n.\nS. Fujimoto and S. S. Gu (2021)\nA minimalist approach to offline reinforcement learning\n.\nExternal Links:\n2106.06860\n,\nLink\nCited by:\nÂ§1\n,\nÂ§2\n.\nD. Garg, J. Hejna, M. Geist, and S. Ermon (2023)\nExtreme q-learning: maxent rl without entropy\n.\nExternal Links:\n2301.02328\n,\nLink\nCited by:\nÂ§1\n,\nÂ§2\n.\nD. Ghosh, A. Gupta, J. Fu, A. Reddy, C. Devin, B. Eysenbach, and S. Levine (2019)\nLearning to reach goals without reinforcement learning\n.\nArXiv\nabs/1912.06088\n.\nCited by:\nÂ§2\n.\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine (2018)\nSoft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor\n.\nExternal Links:\n1801.01290\n,\nLink\nCited by:\nÂ§A.2\n,\nÂ§5.1.1\n.\nK. Hausman, Y. Chebotar, S. Schaal, G. Sukhatme, and J. Lim (2017)\nMulti-modal imitation learning from unstructured demonstrations using generative adversarial nets\n.\nExternal Links:\n1705.10479\n,\nLink\nCited by:\nÂ§2\n,\nÂ§3\n.\nJ. Ho and S. Ermon (2016)\nGenerative adversarial imitation learning\n.\nExternal Links:\n1606.03476\n,\nLink\nCited by:\nÂ§2\n.\nK. Hofmann (2019)\nMinecraft as ai playground and laboratory\n.\nIn\nProceedings of the Annual Symposium on Computer-Human Interaction in Play\n,\nCHI PLAY â€™19\n,\nNew York, NY, USA\n,\npp.Â 1\n.\nExternal Links:\nISBN 9781450366885\n,\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nX. Jia, D. Blessing, X. Jiang, M. Reuss, A. Donat, R. Lioutikov, and G. Neumann (2024)\nTowards diverse behaviors: a benchmark for imitation learning with human demonstrations\n.\nExternal Links:\n2402.14606\n,\nLink\nCited by:\nÂ§1\n.\nL. P. Kaelbling (1993)\nLearning to achieve goals\n.\nIn\nProceedings of the 13th International Joint Conference on Artificial\nIntelligence. ChambÃ©ry, France, August 28 - September 3,\n1993\n,\nR. Bajcsy (Ed.)\n,\npp.Â 1094â€“1099\n.\nCited by:\nÂ§2\n.\nA. Kobanda, W. Radji, M. Petitbois, O. Maillard, and R. Portelas (2025)\nOffline goal-conditioned reinforcement learning with projective quasimetric planning\n.\nExternal Links:\n2506.18847\n,\nLink\nCited by:\nÂ§2\n.\nI. Kostrikov, A. Nair, and S. Levine (2021)\nOffline reinforcement learning with implicit q-learning\n.\nExternal Links:\n2110.06169\n,\nLink\nCited by:\nAppendix D\n,\nÂ§1\n,\nÂ§1\n,\nÂ§2\n,\nÂ§2\n,\nÂ§4.1\n,\nÂ§4.3\n.\nA. Kumar, A. Zhou, G. Tucker, and S. Levine (2020)\nConservative q-learning for offline reinforcement learning\n.\nExternal Links:\n2006.04779\n,\nLink\nCited by:\nÂ§1\n,\nÂ§2\n.\nD. Lee, C. Eom, and M. Kwon (2024a)\nAD4RL: autonomous driving benchmarks for offline reinforcement learning with value-based dataset\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 8239â€“8245\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nS. Lee, Y. Wang, H. Etukuru, H. J. Kim, N. M. M. Shafiullah, and L. Pinto (2024b)\nBehavior generation with latent actions\n.\nExternal Links:\n2403.03181\n,\nLink\nCited by:\nÂ§2\n.\nS. Levine, A. Kumar, G. Tucker, and J. Fu (2020)\nOffline reinforcement learning: tutorial, review, and perspectives on open problems\n.\nExternal Links:\n2005.01643\n,\nLink\nCited by:\nÂ§1\n,\nÂ§1\n.\nY. Li, J. Song, and S. Ermon (2017)\nInfoGAIL: interpretable imitation learning from visual demonstrations\n.\nExternal Links:\n1703.08840\n,\nLink\nCited by:\nÂ§2\n,\nÂ§3\n,\nÂ§5.1.1\n.\nQ. Lin, Z. Liu, D. Mo, and C. Yu (2024a)\nAn offline adaptation framework for constrained multi-objective reinforcement learning\n.\nExternal Links:\n2409.09958\n,\nLink\nCited by:\nÂ§1\n.\nQ. Lin, C. Yu, Z. Liu, and Z. Wu (2024b)\nPolicy-regularized offline multi-objective reinforcement learning\n.\nExternal Links:\n2401.02244\n,\nLink\nCited by:\nÂ§1\n.\nM. Liu, M. Zhu, and W. Zhang (2022)\nGoal-conditioned reinforcement learning : problems and solutions\n.\nIJCAI\n.\nCited by:\nÂ§2\n.\nM. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black (2023)\nSMPL: a skinned multi-person linear model\n.\nIn\nSeminal Graphics Papers: Pushing the Boundaries, Volume 2\n,\nExternal Links:\nISBN 9798400708978\n,\nLink\nCited by:\nÂ§A.3\n,\nÂ§5.1.1\n.\nN. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black (2019a)\nAMASS: archive of motion capture as surface shapes\n.\nIn\nInternational Conference on Computer Vision\n,\npp.Â 5442â€“5451\n.\nCited by:\nÂ§1\n.\nN. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black (2019b)\nAMASS: archive of motion capture as surface shapes\n.\nIn\nInternational Conference on Computer Vision\n,\npp.Â 5442â€“5451\n.\nCited by:\nÂ§A.3\n.\nY. Mao, C. Wu, X. Chen, H. Hu, J. Jiang, T. Zhou, T. Lv, C. Fan, Z. Hu, Y. Wu, Y. Hu, and C. Zhang (2024)\nStylized offline reinforcement learning: extracting diverse high-quality behaviors from heterogeneous datasets\n.\nIn\nThe Twelfth International Conference on Learning Representations\n,\nExternal Links:\nLink\nCited by:\nAppendix C\n,\nAppendix C\n,\nAppendix C\n,\nAppendix C\n,\nAppendix C\n,\nAppendix C\n,\nAppendix C\n,\nÂ§1\n,\nÂ§1\n,\nÂ§1\n,\nÂ§2\n,\nÂ§3\n,\nÂ§3\n,\nÂ§5.1.2\n.\nA. Nair, A. Gupta, M. Dalal, and S. Levine (2021)\nAWAC: accelerating online reinforcement learning with offline datasets\n.\nExternal Links:\n2006.09359\n,\nLink\nCited by:\nÂ§1\n,\nÂ§2\n.\nA. Y. Ng and S. J. Russell (2000)\nAlgorithms for inverse reinforcement learning\n.\nIn\nProceedings of the Seventeenth International Conference on Machine Learning\n,\nICML â€™00\n,\nSan Francisco, CA, USA\n,\npp.Â 663â€“670\n.\nExternal Links:\nISBN 1558607072\nCited by:\nÂ§2\n.\nO. Nilsson and A. Cully (2021)\nPolicy gradient assisted map-elites\n.\nIn\nProceedings of the Genetic and Evolutionary Computation Conference\n,\nGECCO â€™21\n,\nNew York, NY, USA\n,\npp.Â 866â€“875\n.\nExternal Links:\nISBN 9781450383509\n,\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nS. Nishimori (2024)\nJAX-corl: clean sigle-file implementations of offline rl algorithms in jax\n.\nExternal Links:\nLink\nCited by:\nAppendix B\n.\nS. Park, K. Frans, B. Eysenbach, and S. Levine (2025)\nOGBench: benchmarking offline goal-conditioned rl\n.\nIn\nInternational Conference on Learning Representations (ICLR)\n,\nCited by:\nAppendix D\n,\nÂ§1\n,\nÂ§1\n,\nÂ§2\n,\nÂ§4.2\n.\nS. Park, D. Ghosh, B. Eysenbach, and S. Levine (2024)\nHIQL: offline goal-conditioned rl with latent states as actions\n.\nExternal Links:\n2307.11949\n,\nLink\nCited by:\nAppendix D\n,\nÂ§2\n,\nÂ§4.3\n.\nT. Pearce, T. Rashid, A. Kanervisto, D. Bignell, M. Sun, R. Georgescu, S. V. Macua, S. Z. Tan, I. Momennejad, K. Hofmann, and S. Devlin (2023)\nImitating human behaviour with diffusion models\n.\nExternal Links:\n2301.10677\nCited by:\nÂ§2\n.\nX. B. Peng, A. Kumar, G. Zhang, and S. Levine (2019)\nAdvantage-weighted regression: simple and scalable off-policy reinforcement learning\n.\nExternal Links:\n1910.00177\n,\nLink\nCited by:\nAppendix C\n,\nAppendix C\n,\nAppendix C\n,\nAppendix C\n,\nAppendix C\n,\nÂ§4.1\n.\nM. Petitbois, R. Portelas, S. Lamprier, and L. Denoyer (2025)\nOffline learning of controllable diverse behaviors\n.\nExternal Links:\n2504.18160\n,\nLink\nCited by:\nÂ§1\n,\nÂ§1\n,\nÂ§1\n,\nÂ§2\n,\nÂ§3\n.\nD. A. Pomerleau (1991)\nEfficient training of artificial neural networks for autonomous navigation\n.\nNeural Computation\n3\n(\n1\n),\npp.Â 88â€“97\n.\nExternal Links:\nDocument\nCited by:\nAppendix C\n,\nÂ§1\n,\nÂ§2\n,\nÂ§5.1.2\n.\nA. Ratner, C. D. Sa, S. Wu, D. Selsam, and C. RÃ© (2017)\nData programming: creating large training sets, quickly\n.\nExternal Links:\n1605.07723\n,\nLink\nCited by:\nÂ§1\n,\nÂ§3\n.\nS. Ross, G. J. Gordon, and J. A. Bagnell (2011)\nA reduction of imitation learning and structured prediction to no-regret online learning\n.\nExternal Links:\n1011.0686\n,\nLink\nCited by:\nÂ§2\n.\nJ. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel (2017)\nTrust region policy optimization\n.\nExternal Links:\n1502.05477\n,\nLink\nCited by:\nAppendix C\n,\nAppendix C\n.\nN. M. M. Shafiullah, Z. J. Cui, A. Altanzaya, and L. Pinto (2022)\nBehavior transformers: cloning\nk\nk\nmodes with one stone\n.\nExternal Links:\n2206.11251\nCited by:\nÂ§2\n.\nA. Tirinzoni, A. Touati, J. Farebrother, M. Guzek, A. Kanervisto, Y. Xu, A. Lazaric, and M. Pirotta (2025)\nZero-shot whole-body humanoid control via behavioral foundation models\n.\nExternal Links:\n2504.11054\n,\nLink\nCited by:\nÂ§A.3\n,\nÂ§A.3\n,\nÂ§A.3\n,\nÂ§2\n,\nÂ§5.1.1\n.\nE. Todorov, T. Erez, and Y. Tassa (2012)\nMuJoCo: a physics engine for model-based control\n.\nIn\n2012 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\nVol.\n,\npp.Â 5026â€“5033\n.\nExternal Links:\nDocument\nCited by:\nÂ§A.2\n,\nÂ§5.1.1\n.\nA. Touati and Y. Ollivier (2021)\nLearning one representation to optimize all rewards\n.\nExternal Links:\n2103.07945\n,\nLink\nCited by:\nÂ§2\n.\nM. Towers, A. Kwiatkowski, J. Terry, J. U. Balis, G. De Cola, T. Deleu, M. GoulÃ£o, A. Kallinteris, M. Krimmel, A. KG,\net al.\n(2024)\nGymnasium: a standard interface for reinforcement learning environments\n.\narXiv preprint arXiv:2407.17032\n.\nCited by:\nÂ§A.2\n,\nÂ§A.2\n.\nZ. Wang, J. Merel, S. Reed, G. Wayne, N. de Freitas, and N. Heess (2017)\nRobust imitation of diverse behaviors\n.\nExternal Links:\n1707.02747\n,\nLink\nCited by:\nÂ§2\n,\nÂ§3\n.\nS. Wu, J. Yao, H. Fu, Y. Tian, C. Qian, Y. Yang, Q. FU, and Y. Wei (2023)\nQuality-similar diversity via population based reinforcement learning\n.\nIn\nThe Eleventh International Conference on Learning Representations\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nH. Yang, J. Yao, W. Liu, Q. Wang, H. Qin, H. Kong, K. Tang, J. Xiong, C. Yu, K. Li, J. Xing, H. Chen, J. Zhuo, Q. Fu, Y. Wei, and H. Fu (2024)\nDiverse policies recovering via pointwise mutual information weighted imitation learning\n.\nExternal Links:\n2410.15910\n,\nLink\nCited by:\nAppendix C\n,\nÂ§1\n,\nÂ§1\n,\nÂ§1\n,\nÂ§2\n,\nÂ§3\n,\nÂ§3\n,\nÂ§3\n,\nÂ§5.1.2\n.\nR. Yang, Y. Lu, W. Li, H. Sun, M. Fang, Y. Du, X. Li, L. Han, and C. Zhang (2022)\nRethinking goal-conditioned supervised learning and its connection to offline rl\n.\nExternal Links:\n2202.04478\n,\nLink\nCited by:\nÂ§2\n.\nY. Yuan, Z. Zheng, Z. Dong, and J. Hao (2025)\nMODULI: unlocking preference generalization via diffusion models for offline multi-objective reinforcement learning\n.\nExternal Links:\n2408.15501\n,\nLink\nCited by:\nÂ§1\n.\nE. Zhan, A. Tseng, Y. Yue, A. Swaminathan, and M. Hausknecht (2020)\nLearning calibratable policies using programmatic style-consistency\n.\nExternal Links:\n1910.01179\n,\nLink\nCited by:\nÂ§1\n,\nÂ§1\n,\nÂ§1\n,\nÂ§2\n,\nÂ§3\n,\nÂ§3\n.\nR. Zhang, C. Walshe, Z. Liu, L. Guan, K. S. Muller, J. A. Whritner, L. Zhang, M. M. Hayhoe, and D. H. Ballard (2019)\nAtari-head: atari human eye-tracking and demonstration dataset\n.\nExternal Links:\n1903.06754\n,\nLink\nCited by:\nÂ§1\n.\nAppendix A\nEnvironments, tasks, labels and datasets\nIn this section, we detail our environments, tasks, labels and datasets.\nA.1\nCircle2d\n(a)\nEnvironment\n(b)\nTask\n(c)\nPosition labels\nFigure 3:\nCircle2d environment visualizations.\nEnvironment\nThe Circle2d environment consists of a 2D plane where an agent can roam around within a confined square. Its state space\nğ’®\n\\mathcal{S}\ncorresponds to the history of the previous 4\n(\nx\nagent\n,\ny\nagent\n,\nÎ¸\nagent\n)\nâˆˆ\n[\n[\nx\nm\nâ€‹\ni\nâ€‹\nn\n,\nx\nm\nâ€‹\na\nâ€‹\nx\n]\nÃ—\n[\ny\nm\nâ€‹\ni\nâ€‹\nn\n,\ny\nm\nâ€‹\na\nâ€‹\nx\n]\nÃ—\n[\nÎ¸\nm\nâ€‹\ni\nâ€‹\nn\n,\nÎ¸\nm\nâ€‹\na\nâ€‹\nx\n]\n]\n=\n[\nâˆ’\n50.0\n,\n50.0\n]\nÃ—\n[\nâˆ’\n50.0\n,\n50.0\n]\nÃ—\n[\nâˆ’\nÏ€\n,\nÏ€\n]\n(x_{\\mathrm{agent}},y_{\\mathrm{agent}},\\theta_{\\mathrm{agent}})\\in[[x_{min},x_{max}]\\times[y_{min},y_{max}]\\times[\\theta_{min},\\theta_{max}]]=[-50.0,50.0]\\times[-50.0,50.0]\\times[-\\pi,\\pi]\n, padded if needed by repeating the oldest triplet (namely for the beginning of the trajectory). Its action space\nğ’œ\n\\mathcal{A}\nis\n[\nâˆ’\n1\n,\n1\n]\n2\n[-1,1]^{2}\nwhere the first dimension maps onto an angular shift\nÎ”\nâ€‹\nÎ¸\nâˆˆ\n[\nÎ”\nâ€‹\nÎ¸\nm\nâ€‹\ni\nâ€‹\nn\n,\nÎ”\nâ€‹\nÎ¸\nm\nâ€‹\na\nâ€‹\nx\n]\n=\n[\nâˆ’\nÏ€\n,\nÏ€\n]\n\\Delta\\theta\\in[\\Delta\\theta_{min},\\Delta\\theta_{max}]=[-\\pi,\\pi]\nin radians and the second dimension maps onto a speed in\n[\nv\nm\nâ€‹\ni\nâ€‹\nn\n,\nv\nm\nâ€‹\na\nâ€‹\nx\n]\n=\n[\n0.5\n,\n3.0\n]\n[v_{min},v_{max}]=[0.5,3.0]\n. At first, the environment is initialized by sampling a random position from\n[\n[\n0.7\nâ‹…\nx\nm\nâ€‹\ni\nâ€‹\nn\n,\n0.7\nâ‹…\nx\nm\nâ€‹\na\nâ€‹\nx\n]\nÃ—\n[\n0.7\nâ‹…\ny\nm\nâ€‹\ni\nâ€‹\nn\n,\n0.7\nâ‹…\ny\nm\nâ€‹\na\nâ€‹\nx\n]\n]\n[[0.7\\cdot x_{min},0.7\\cdot x_{max}]\\times[0.7\\cdot y_{min},0.7\\cdot y_{max}]]\nand a random orientation from\n[\nâˆ’\nÏ€\n,\nÏ€\n]\n[-\\pi,\\pi]\n. At each timestep\nt\nt\n, given a state\ns\nt\ns_{t}\nand an action\na\nt\na_{t}\n, the agent rotates by the corresponding\nÎ”\nâ€‹\nÎ¸\nt\n\\Delta\\theta_{t}\nbefore moving by the displacement vector\nÎ”\nâ€‹\nv\nt\n\\Delta v_{t}\n. The episode is truncated after 1000 timesteps have been reached.\nWe display a minimal visual example of our environment in Figure\n3(a)\n.\nTask\nIn Circle2D, we define the task as drawing a target circle given its center\nx\nâ€‹\ny\ntarget\nxy_{\\mathrm{target}}\nand its radius\nr\nâ€‹\na\nâ€‹\nd\nâ€‹\ni\nâ€‹\nu\nâ€‹\ns\ntarget\nradius_{\\mathrm{target}}\nand encode it by a reward:\nr\nâ€‹\n(\ns\nt\n,\na\nt\n)\n=\nâˆ’\n|\nâ€–\nx\nâ€‹\ny\nagent\nâˆ’\nx\nâ€‹\ny\ntarget\nâ€–\n2\n2\nâˆ’\nr\nâ€‹\na\nâ€‹\nd\nâ€‹\ni\nâ€‹\nu\nâ€‹\ns\ntarget\n|\nr(s_{t},a_{t})=-|||xy_{\\mathrm{agent}}-xy_{\\mathrm{target}}||_{2}^{2}-radius_{\\mathrm{target}}|\n. In this work, we consider the same fixed circle target along experiments and we display its associated reward colormap in Figure\n3(b)\n.\nDatasets\nWe generate for this environment two datasets by using a hard-coded agent which draws circles of various centers and radii, with different orientations (clockwise and counter-clockwise) and different speed and noise levels on the actions. The first dataset\ncircle2d-inplace-v0\nis obtained by directly performing the circle at start position, while the\ncircle2d-navigate-v0\ndataset is obtained by moving around a target position before drawing the circle. We plot in Figure\n4\nthe datasetsâ€™ trajectories.\n(a)\n5%\n(b)\n20%\n(c)\n50%\n(d)\n100%\nFigure 4:\nCircle2d datasets trajectory visualizations at different percentages.\nThe top row corresponds to the\ncircle2d-inplace-v0\nwhile the bottom row corresponds to the\ncircle2d-navigate-v0\nCriteria and labels\nWe present below the various labeling functions we designed for Circle2d.\nâ€¢ position\n: The position labeling function\nÎ»\nposition\n\\lambda_{\\mathrm{position}}\npartitions the 2D plane into a fixed grid and assigns to each timestep the index of the cell containing the current position. Concretely, the\nx\nx\n-axis range\n[\nâˆ’\n30\n,\n30\n]\n[-30,30]\n(real units) is split uniformly into\n4\n4\nbins and the\ny\ny\n-axis is split at\n0\ninto\n2\n2\nbins, yielding\n4\nÃ—\n2\n=\n8\n4\\times 2=8\nareas. At timestep\nt\nt\n, with window size\nw\nw\n, we read every\n(\nx\nt\nâ€²\n,\ny\nt\nâ€²\n)\n(x_{t^{\\prime}},y_{t^{\\prime}})\nin the window\nÏ„\nt\nâˆ’\nw\n+\n1\n:\nt\n+\nw\n\\tau_{t-w+1:t+w}\nand set the label as the majority area. The label set is\nâ„’\nâ€‹\n(\nÎ»\n)\n=\nâŸ¦\n0\n,\n7\nâŸ§\n\\mathcal{L}(\\lambda)=\\llbracket 0,7\\rrbracket\n. In practice, we take\nw\n=\n1\nw=1\nto mitigate unnecessary credit assignment issues. We plot in Figure\n5\nthe corresponding visuals and histograms.\nâ€¢ movement direction\n: The movementâ€“direction labeling function\nÎ»\nmove\n\\lambda_{\\mathrm{move}}\ndiscretizes the instantaneous displacement direction. For each timestep\nt\nâ€²\nt^{\\prime}\n, we compute\nÎ”\nâ€‹\np\nt\nâ€²\n=\np\nt\nâ€²\n+\n1\nâˆ’\np\nt\nâ€²\n\\Delta p_{t^{\\prime}}=p_{t^{\\prime}+1}-p_{t^{\\prime}}\nand\nÎ¸\nt\nâ€²\n=\natan2\nâ€‹\n(\nÎ”\nâ€‹\ny\nt\nâ€²\n,\nÎ”\nâ€‹\nx\nt\nâ€²\n)\n\\theta_{t^{\\prime}}=\\mathrm{atan2}(\\Delta y_{t^{\\prime}},\\Delta x_{t^{\\prime}})\n, and uniformly quantize\n[\nâˆ’\nÏ€\n,\nÏ€\n)\n[-\\pi,\\pi)\ninto\nK\n=\n8\nK=8\nbins. With window size\nw\nw\n, the label at\nt\nt\nis the majority direction bin over\n{\nÎ¸\nt\nâ€²\n}\nt\nâ€²\nâˆˆ\nÏ„\nt\nâˆ’\nw\n+\n1\n:\nt\n+\nw\n\\{\\theta_{t^{\\prime}}\\}_{t^{\\prime}\\in\\tau_{t-w+1:t+w}}\n. If\nâ€–\nÎ”\nâ€‹\np\nt\nâ€²\nâ€–\n<\n0.1\n\\|\\Delta p_{t^{\\prime}}\\|<0.1\n(real units) for a frame, it contributes an undetermined class\nu\nu\n(non-promptable). Thus\nâ„’\nâ€‹\n(\nÎ»\n)\n=\nâŸ¦\n0\n,\n8\nâŸ§\n\\mathcal{L}(\\lambda)=\\llbracket 0,8\\rrbracket\n, with promptable bins\n0..7\n0..7\nand\n8\n=\nu\n8=u\n. In practice we use\nw\n=\n1\nw=1\nto mitigate unnecessary credit assignment issues. See Figure\n6\nfor visuals and histograms.\nâ€¢ turn direction\n: The turnâ€“direction labeling function\nÎ»\nturn\n\\lambda_{\\mathrm{turn}}\ninherently operates on a centered temporal window to estimate local angular velocity. Let\n(\nÎ¸\nt\n)\nt\n(\\theta_{t})_{t}\nbe the unwrapped heading; on an odd window\nW\nt\nW_{t}\n(default size\n11\n11\n), we form\nÎ”\nâ€‹\nÎ¸\nt\nâ€²\n=\nÎ¸\nt\nâ€²\n+\n1\nâˆ’\nÎ¸\nt\nâ€²\n\\Delta\\theta_{t^{\\prime}}=\\theta_{t^{\\prime}+1}-\\theta_{t^{\\prime}}\nand compute\nÏ‰\nÂ¯\nt\n=\n1\n|\nW\nt\n|\nâ€‹\nâˆ‘\nt\nâ€²\nâˆˆ\nW\nt\nÎ”\nâ€‹\nÎ¸\nt\nâ€²\n\\bar{\\omega}_{t}=\\frac{1}{|W_{t}|}\\sum_{t^{\\prime}\\in W_{t}}\\Delta\\theta_{t^{\\prime}}\n. If\n|\nÏ‰\nÂ¯\nt\n|\n<\n0.1\nâ€‹\nrad\n/\nstep\n|\\bar{\\omega}_{t}|<0.1~\\mathrm{rad/step}\nwe label â€œstraight,â€ else â€œleftâ€ if\nÏ‰\nÂ¯\nt\n>\n0\n\\bar{\\omega}_{t}>0\n(counter-clockwise) and â€œrightâ€ if\nÏ‰\nÂ¯\nt\n<\n0\n\\bar{\\omega}_{t}<0\n(clockwise). We set\nâ„’\nâ€‹\n(\nÎ»\n)\n=\n{\n0\n,\n1\n,\n2\n}\n\\mathcal{L}(\\lambda)=\\{0,1,2\\}\nwith\n0\n=\n0=\nright,\n1\n=\n1=\nleft,\n2\n=\n2=\nstraight (non-promptable). We plot in Figure\n7\nits visuals and histograms.\nâ€¢ radius category\n: The radius labeling function\nÎ»\nradius\n\\lambda_{\\mathrm{radius}}\nalso works directly on centered windows. First, on a short window\nW\nt\nstr\nW^{\\text{str}}_{t}\n(default size\n11\n11\n) we test straightness via the mean absolute heading increment; if it is below\n0.1\nâ€‹\nrad\n/\nstep\n0.1~\\mathrm{rad/step}\n, the label is â€œstraight.â€ Otherwise, on a larger window of positions\nW\nt\nrad\nW^{\\text{rad}}_{t}\n(default size\n51\n51\n) we fit a circle by least squares and take its radius\nr\nt\nr_{t}\n. We uniformly partition\n[\n2\n,\n11\n]\n[2,11]\n(real units) into\nK\n=\n3\nK=3\nbins and assign the corresponding bin; the straight case is encoded as bin\nK\nK\n. Thus\nâ„’\nâ€‹\n(\nÎ»\n)\n=\nâŸ¦\n0\n,\nK\nâŸ§\n\\mathcal{L}(\\lambda)=\\llbracket 0,K\\rrbracket\n, where\n0\n.\n.\nK\nâˆ’\n1\n0..K-1\ndenote increasing-radius curved motion and\nK\nK\ndenotes straight (non-promptable). See Figure\n8\n.\nâ€¢ speed category\n: The speed labeling function\nÎ»\nspeed\n\\lambda_{\\mathrm{speed}}\nbins the scalar speed. For each timestep\nt\nâ€²\nt^{\\prime}\nwe compute the speed\nv\nt\nâ€²\nv_{t^{\\prime}}\nand uniformly partition\n[\n0.5\n,\n3.0\n]\n[0.5,3.0]\n(real units) into\nK\n=\n3\nK=3\nbins. With window size\nw\nw\n, the label at\nt\nt\nis the majority speed bin over\n{\nv\nt\nâ€²\n}\nt\nâ€²\nâˆˆ\nÏ„\nt\nâˆ’\nw\n+\n1\n:\nt\n+\nw\n\\{v_{t^{\\prime}}\\}_{t^{\\prime}\\in\\tau_{t-w+1:t+w}}\n. Hence\nâ„’\nâ€‹\n(\nÎ»\n)\n=\nâŸ¦\n0\n,\nK\nâˆ’\n1\nâŸ§\n\\mathcal{L}(\\lambda)=\\llbracket 0,K-1\\rrbracket\n. In practice we take\nw\n=\n1\nw=1\nto mitigate unnecessary credit assignment issues. We plot in Figure\n9\nthe corresponding visuals and histograms.\nâ€¢ curvature noise\n: The curvature-noise labeling function\nÎ»\nnoise\n\\lambda_{\\mathrm{noise}}\ncomputes a variability statistic on a centered window. With unwrapped heading\n(\nÎ¸\nt\n)\nt\n(\\theta_{t})_{t}\n, we define\nÎ”\nâ€‹\nÎ¸\nt\nâ€²\n=\nÎ¸\nt\nâ€²\n+\n1\nâˆ’\nÎ¸\nt\nâ€²\n\\Delta\\theta_{t^{\\prime}}=\\theta_{t^{\\prime}+1}-\\theta_{t^{\\prime}}\nand\nÎ”\n2\nâ€‹\nÎ¸\nt\nâ€²\n=\nÎ”\nâ€‹\nÎ¸\nt\nâ€²\n+\n1\nâˆ’\nÎ”\nâ€‹\nÎ¸\nt\nâ€²\n\\Delta^{2}\\theta_{t^{\\prime}}=\\Delta\\theta_{t^{\\prime}+1}-\\Delta\\theta_{t^{\\prime}}\n. On an odd window\nW\nt\nW_{t}\n(default size\n51\n51\n), we take\nÏƒ\nt\n=\nstd\nâ€‹\n(\n{\nÎ”\n2\nâ€‹\nÎ¸\nt\nâ€²\n}\nt\nâ€²\nâˆˆ\nW\nt\n)\n\\sigma_{t}=\\mathrm{std}\\big(\\{\\Delta^{2}\\theta_{t^{\\prime}}\\}_{t^{\\prime}\\in W_{t}}\\big)\nand uniformly bin\nÏƒ\nt\n\\sigma_{t}\ninto\nK\n=\n3\nK=3\ncategories over\n[\n0.0\n,\n0.8\n]\n[0.0,0.8]\n. Hence\nâ„’\nâ€‹\n(\nÎ»\n)\n=\nâŸ¦\n0\n,\nK\nâˆ’\n1\nâŸ§\n\\mathcal{L}(\\lambda)=\\llbracket 0,K-1\\rrbracket\n. We plot in Figure\n10\nits visuals and histograms.\nNotes.\nFor all labels that use windows, the implementation ensures an odd, centered window around\nt\nt\n; where relevant, â€œstraightâ€/â€œundeterminedâ€ classes are excluded from promptable labels but kept in\nâ„’\nâ€‹\n(\nÎ»\n)\n\\mathcal{L}(\\lambda)\nfor completeness. Bin edges are uniform by default and configurable through the class constructors.\n(a)\ninplace - 5%\n(b)\ninplace - 100%\n(c)\nnavigate - 5%\n(d)\nnavigate - 100%\nFigure 5:\nCircle2d position label visualizations at different percentages.\n(a)\ninplace - 5%\n(b)\ninplace - 100%\n(c)\nnavigate - 5%\n(d)\nnavigate - 100%\nFigure 6:\nCircle2d movement direction label visualizations at different percentages.\n(a)\ninplace - 5%\n(b)\ninplace - 100%\n(c)\nnavigate - 5%\n(d)\nnavigate - 100%\nFigure 7:\nCircle2d turn direction label visualizations at different percentages.\n(a)\ninplace - 5%\n(b)\ninplace - 100%\n(c)\nnavigate - 5%\n(d)\nnavigate - 100%\nFigure 8:\nCircle2d radius label visualizations at different percentages.\n(a)\ninplace - 5%\n(b)\ninplace - 100%\n(c)\nnavigate - 5%\n(d)\nnavigate - 100%\nFigure 9:\nCircle2d speed label visualizations at different percentages.\n(a)\ninplace - 5%\n(b)\ninplace - 100%\n(c)\nnavigate - 5%\n(d)\nnavigate - 100%\nFigure 10:\nCircle2d curvature noise visualizations at different percentages.\nA.2\nHalfCheetah\nEnvironment\nHalfCheetah\n(Todorov\net al.\n,\n2012\n; Towers\net al.\n,\n2024\n)\nis an environment consisting of controlling a 6-DoF 2-dimensional robot composed of 9 body parts and 8 joints connecting them. The environment has a time limit of 1000 timesteps. Details about this environment can be read in\nTowers\net al.\n(\n2024\n)\n.\nTask\nAs implemented in\nTowers\net al.\n(\n2024\n)\n, at each timestep\nt\nt\n, the agent applies continuous control actions\nğš\nt\nâˆˆ\nâ„\nd\n\\mathbf{a}_{t}\\in\\mathbb{R}^{d}\nthat drive the joints of the cheetah.\nThe environment evaluates performance using a reward which\nencourages rapid forward progress while penalizing excessive control effort.\nFormally, the forward velocity of the torso is\nv\nt\n=\nx\nt\n+\n1\nâˆ’\nx\nt\nÎ”\nâ€‹\nt\n,\nv_{t}=\\frac{x_{t+1}-x_{t}}{\\Delta t},\nwhere\nx\nt\nx_{t}\nis the torso position along the horizontal axis and\nÎ”\nâ€‹\nt\n\\Delta t\nis\nthe simulator timestep. The reward combines a positive term proportional to\nforward velocity with a quadratic control penalty:\nr\nt\n=\nw\nf\nâ€‹\nv\nt\nâˆ’\nw\nc\nâ€‹\nâˆ‘\ni\n=\n1\nd\na\nt\n,\ni\n2\n,\nr_{t}\\;=\\;w_{f}\\,v_{t}\\;-\\;w_{c}\\sum_{i=1}^{d}a_{t,i}^{2},\nwhere\nw\nf\nw_{f}\nis the forward-reward weight and\nw\nc\nw_{c}\nis the control-cost weight.\nThus, the agent must learn to run efficiently: moving forward quickly while\nkeeping joint torques as small as possible.\nDatasets\nTo generate the datasets, we train a diverse set of HalfCheetah policies through\nSAC\n(Haarnoja\net al.\n,\n2018\n)\n. We construct several\narchetype\npolicies defined by\nGaussian-shaped reward functions that bias behavior toward specific styles.\nThe\nHeight\narchetype rewards the torso maintaining a target vertical\nposition\nz\ntorso\nz_{\\text{torso}}\nat specified values, thereby inducing qualitatively\ndistinct gaits:\ncrawling\n(\nz\nâ‰ˆ\n0.5\nz\\approx 0.5\nwith\nÏƒ\n=\n0.04\n\\sigma=0.04\n),\nnormal running\n(\nz\nâ‰ˆ\n0.6\nz\\approx 0.6\nwith\nÏƒ\n=\n0.04\n\\sigma=0.04\n), or\nupright running\n(\nz\nâ‰ˆ\n0.7\nz\\approx 0.7\nwith\nÏƒ\n=\n0.04\n\\sigma=0.04\n).\nThe\nSpeed\narchetype rewards locomotion close to a desired forward\nvelocity, producing policies that move at\nslow pace\n(\nv\nâ‰ˆ\n1.5\nv\\approx 1.5\n),\nmedium pace\n(\nv\nâ‰ˆ\n5.0\nv\\approx 5.0\n), or\nfast pace\n(\nv\nâ‰ˆ\n10.0\nv\\approx 10.0\n).\nFinally, the\nAngle\narchetype shapes behavior around the torso pitch\nangle, leading to policies that prefer\nupright\n(\nÎ¸\nâ‰ˆ\nâˆ’\n0.2\n\\theta\\approx-0.2\nwith\nÏƒ\n=\n0.05\n\\sigma=0.05\n),\nflat\n(\nÎ¸\nâ‰ˆ\n0.0\n\\theta\\approx 0.0\nwith\nÏƒ\n=\n0.05\n\\sigma=0.05\n), or\ncrouched\n(\nÎ¸\nâ‰ˆ\n0.2\n\\theta\\approx 0.2\nwith\nÏƒ\n=\n0.05\n\\sigma=0.05\n) postures while still\nadvancing forward. These archetypes yield a diverse collection of locomotion\nstyles that serve as structured variations of the base HalfCheetah task.\nThen, we generate three datasets:\nhalfcheetah-fixed-v0\n, where the archetype policy is fixed during the\ntrajectory;\nhalfcheetah-stitch-v0\n, where the trajectories are cut into\nshorter segments from the\nhalfcheetah-fixed-v0\ndataset; and\nhalfcheetah-vary-v0\n, where the policy archetype changes within the\nsame trajectory. Each dataset contains\n10\n6\n=\n1000\nâ€‹\n(\nepisodes\n)\nâˆ—\n1000\nâ€‹\n(\ntimesteps\n)\n10^{6}=1000(\\text{episodes})*1000(\\text{timesteps})\nsteps, with the stitch datasets containing more episodes as it cuts the fix dataset episodes.\nCriteria and labels\nWe present below the various labeling functions we designed for HalfCheetah.\nEach labeling function\nÎ»\n\\lambda\nmaps raw environment signals to a discrete label\nsequence, optionally smoothed by a majority vote over a window\nÏ„\nt\nâˆ’\nw\n+\n1\n:\nt\n+\nw\n\\tau_{t-w+1:t+w}\n.\nIn practice, we take\nw\n=\n1\nw=1\nto mitigate unnecessary credit assignment issues.\nâ€¢ speed\n:\nThe speed labeling function\nÎ»\nspeed\n\\lambda_{\\mathrm{speed}}\ndiscretizes the forward\nvelocity magnitude\n|\nv\nt\n|\n|v_{t}|\n. We define a range\n[\nv\nmin\n,\nv\nmax\n]\n=\n[\n0.1\n,\n10.0\n]\n[v_{\\min},v_{\\max}]=[0.1,10.0]\n(real units) and split it uniformly into\nK\n=\n3\nK=3\nbins, yielding the labels\nâ„’\nâ€‹\n(\nÎ»\nspeed\n)\n=\nâŸ¦\n0\n,\n2\nâŸ§\n\\mathcal{L}(\\lambda_{\\mathrm{speed}})=\\llbracket 0,2\\rrbracket\n.\nAt timestep\nt\nt\n, we assign the bin index corresponding to\n|\nv\nt\n|\n|v_{t}|\n, and take the\nmajority bin across the window. See Figure\n11\n.\nâ€¢ angle\n:\nThe angle labeling function\nÎ»\nangle\n\\lambda_{\\mathrm{angle}}\ndiscretizes the torso\npitch\nÎ¸\nt\n\\theta_{t}\n. We define\n[\nÎ¸\nmin\n,\nÎ¸\nmax\n]\n=\n[\nâˆ’\n0.3\n,\n0.3\n]\n[\\theta_{\\min},\\theta_{\\max}]=[-0.3,0.3]\n(radians)\nand split uniformly into\nK\n=\n3\nK=3\nbins, yielding the label set\nâ„’\nâ€‹\n(\nÎ»\nangle\n)\n=\nâŸ¦\n0\n,\n2\nâŸ§\n\\mathcal{L}(\\lambda_{\\mathrm{angle}})=\\llbracket 0,2\\rrbracket\n.\nAt timestep\nt\nt\n, we assign the bin index of\nÎ¸\nt\n\\theta_{t}\n, and take the majority\nlabel over the window. See Figure\n12\n.\nâ€¢ torso height\n:\nThe torsoâ€“height labeling function\nÎ»\ntorso\n\\lambda_{\\mathrm{torso}}\ndiscretizes the\nvertical torso position\nh\nt\nh_{t}\n. We define\n[\nh\nmin\n,\nh\nmax\n]\n=\n[\n0.4\n,\n0.8\n]\n[h_{\\min},h_{\\max}]=[0.4,0.8]\n(real units) and split into\nK\n=\n3\nK=3\nbins, giving\nâ„’\nâ€‹\n(\nÎ»\ntorso\n)\n=\nâŸ¦\n0\n,\n2\nâŸ§\n\\mathcal{L}(\\lambda_{\\mathrm{torso}})=\\llbracket 0,2\\rrbracket\n.\nLabels are assigned per timestep and smoothed by majority vote.\nSee Figure\n13\n.\nâ€¢ back-foot height\n:\nThe back-foot labeling function\nÎ»\nbf\n\\lambda_{\\mathrm{bf}}\ndiscretizes the vertical\nposition of the back foot\nh\nt\nbf\nh^{\\mathrm{bf}}_{t}\n. We define\n[\nh\nmin\n,\nh\nmax\n]\n=\n[\n0.0\n,\n0.3\n]\n[h_{\\min},h_{\\max}]=[0.0,0.3]\nand split into\nK\n=\n4\nK=4\nbins, giving\nâ„’\nâ€‹\n(\nÎ»\nbf\n)\n=\nâŸ¦\n0\n,\n3\nâŸ§\n\\mathcal{L}(\\lambda_{\\mathrm{bf}})=\\llbracket 0,3\\rrbracket\n.\nLabels are taken per timestep and majority-voted.\nSee Figure\n14\n.\nâ€¢ front-foot height\n:\nThe front-foot labeling function\nÎ»\nff\n\\lambda_{\\mathrm{ff}}\ndiscretizes the vertical\nposition of the front foot\nh\nt\nff\nh^{\\mathrm{ff}}_{t}\nin the same manner as the\nback-foot:\n[\n0.0\n,\n0.3\n]\n[0.0,0.3]\nsplit into\nK\n=\n4\nK=4\nbins, yielding\nâ„’\nâ€‹\n(\nÎ»\nff\n)\n=\nâŸ¦\n0\n,\n3\nâŸ§\n\\mathcal{L}(\\lambda_{\\mathrm{ff}})=\\llbracket 0,3\\rrbracket\n.\nSee Figure\n15\n.\n(a)\nhalfcheetah-fix-v0\n(b)\nhalfcheetah-stitch-v0\n(c)\nhalfcheetah-vary-v0\nFigure 11:\nHalfCheetah speed label histograms.\n(a)\nhalfcheetah-fix-v0\n(b)\nhalfcheetah-stitch-v0\n(c)\nhalfcheetah-vary-v0\nFigure 12:\nHalfCheetah angle label histograms.\n(a)\nhalfcheetah-fix-v0\n(b)\nhalfcheetah-stitch-v0\n(c)\nhalfcheetah-vary-v0\nFigure 13:\nHalfCheetah torso height label histograms.\n(a)\nhalfcheetah-fix-v0\n(b)\nhalfcheetah-stitch-v0\n(c)\nhalfcheetah-vary-v0\nFigure 14:\nHalfCheetah backfoot height label histograms.\n(a)\nhalfcheetah-fix-v0\n(b)\nhalfcheetah-stitch-v0\n(c)\nhalfcheetah-vary-v0\nFigure 15:\nHalfCheetah frontfoot height label histograms.\nA.3\nHumEnv\nEnvironment\nThe HumEnv environment\n(Tirinzoni\net al.\n,\n2025\n)\nis built on the SMPL skeleton\n(Loper\net al.\n,\n2023\n)\n, which consists of 24 rigid bodies, among which 23 are actuated. This SMPL skeleton is widely used in character animation and is well suited for expressing natural human-like stylized behaviors. HumEnvâ€™s observations consist of the concatenation of the body poses (70 D), body rotations (144 D) and angular velocities (144D) resulting in a 358-dimensional vector. It moves the body using a proportional derivative controller resulting in a 69-dimensional action space. Consequently, this task has a higher dimensionality of (358, 69) compared to HalfCheetahâ€™s (17, 6) dimensionality. We consider two types of HumEnv environments, HumEnv-Simple, which initializes the humanoid in a standing position, and HumEnv-Complex, which initializes the humanoid in a lying down position.\nTask\nAt each timestep\nt\nt\n, the agent applies continuous control actions\nğš\nt\nâˆˆ\nâ„\nd\n\\mathbf{a}_{t}\\in\\mathbb{R}^{d}\n. The environments evaluate performance using a\nreward that encourages high-speed movement in the horizontal plane, modulated\nby a control efficiency term. Formally, let\nğ¯\nt\n,\nx\nâ€‹\ny\n\\mathbf{v}_{t,xy}\ndenote the\nvelocity vector of the center of mass projected onto the horizontal plane\n(ignoring vertical movement). The reward is defined as the norm of this velocity,\nscaled by a multiplicative control factor:\nr\nt\n=\nÎ±\nâ€‹\n(\nğš\nt\n)\nâ‹…\nâ€–\nğ¯\nt\n,\nx\nâ€‹\ny\nâ€–\n2\n,\nr_{t}\\;=\\;\\alpha(\\mathbf{a}_{t})\\cdot\\|\\mathbf{v}_{t,xy}\\|_{2},\nwhere\nÎ±\nâ€‹\n(\nğš\nt\n)\nâˆˆ\n[\n0.8\n,\n1.0\n]\n\\alpha(\\mathbf{a}_{t})\\in[0.8,1.0]\nis a smoothness coefficient derived\nfrom a quadratic tolerance function on the control inputs\nğš\nt\n\\mathbf{a}_{t}\nprovided in\nTirinzoni\net al.\n(\n2025\n)\n.\nDatasets\nWe generated for each environment a stylized dataset using the Metamotivo-M1 model provided in\nTirinzoni\net al.\n(\n2025\n)\n, using various ways of moving at different heights and speeds. Since, the Metamotivo-M1 model was trained with a regularization towards the AMASS motion-capture dataset\n(Mahmood\net al.\n,\n2019b\n)\n, it provides more natural and human-like stylized behaviors.\nCriteria and labels\nWe present below the various labeling functions we designed for the HumEnv environments. Each labeling function\nÎ»\n\\lambda\nmaps raw environment signals to a discrete label sequence, optionally smoothed by a majority vote over a window\nÏ„\nt\nâˆ’\nw\n+\n1\n:\nt\n+\nw\n\\tau_{t-w+1:t+w}\n. In practice, we take\nw\n=\n1\nw=1\nto mitigate\nunnecessary credit assignment issues.\nâ€¢ simple - head height\n:\nFor HumEnv-Simple, we focused our study on a single\nhead_height\ncriterion of two labels, namely\nlow\nand\nhigh\n. The simple - head_height labeling function discretizes the vertical head position\nh\nt\nh_{t}\nusing a single threshold at\n1.2\n1.2\n. This results in\nK\n=\n2\nK=2\nbins (\nh\nt\n<\n1.2\nh_{t}<1.2\nand\nh\nt\nâ‰¥\n1.2\nh_{t}\\geq 1.2\n), yielding the label set\nâ„’\nâ€‹\n(\nÎ»\nsimple\nâ€‹\n_\nâ€‹\nhead\n)\n=\nâŸ¦\n0\n,\n1\nâŸ§\n\\mathcal{L}(\\lambda_{\\mathrm{simple\\_head}})=\\llbracket 0,1\\rrbracket\n.\nSee Figure\n16(a)\n.\nâ€¢ complex - speed\n:\nFor the HumEnv-Complex, we added a new\nspeed\ncriterion. The speed labeling function\nÎ»\nspeed\n\\lambda_{\\mathrm{speed}}\ndiscretizes the center-of-mass\nvelocity magnitude\n|\nv\nt\n|\n|v_{t}|\n. Based on the agentâ€™s movement capabilities, we define\nthree distinct regimes: immobile (\n|\nv\nt\n|\n<\n0.2\n|v_{t}|<0.2\n), slow (\n0.2\nâ‰¤\n|\nv\nt\n|\nâ‰¤\n3.0\n0.2\\leq|v_{t}|\\leq 3.0\n),\nand fast (\n|\nv\nt\n|\n>\n3.0\n|v_{t}|>3.0\n). This yields\nK\n=\n3\nK=3\nbins with labels\nâ„’\nâ€‹\n(\nÎ»\nspeed\n)\n=\nâŸ¦\n0\n,\n2\nâŸ§\n\\mathcal{L}(\\lambda_{\\mathrm{speed}})=\\llbracket 0,2\\rrbracket\n.\nSee Figure\n16(b)\n.\nâ€¢ complex - head height\n:\nFor the HumEnv-Complex, we also extended the\ncomplex - head_height\ncriterion by adding a new label for a total of 3 labels. The head-height labeling function\nÎ»\ncomplex\nâ€‹\n_\nâ€‹\nhead\n\\lambda_{\\mathrm{complex\\_head}}\ndiscretizes the vertical\nposition of the agentâ€™s head\nh\nt\nh_{t}\n. We define thresholds at\n0.4\n0.4\nand\n1.2\n1.2\nto capture different postures: lying down, crouching and standing.\nThe space is split into\nK\n=\n3\nK=3\nbins:\nh\nt\n<\n0.4\nh_{t}<0.4\n,\n0.4\nâ‰¤\nh\nt\nâ‰¤\n1.2\n0.4\\leq h_{t}\\leq 1.2\n, and\nh\nt\n>\n1.2\nh_{t}>1.2\n, yielding\nâ„’\nâ€‹\n(\nÎ»\ncomplex\nâ€‹\n_\nâ€‹\nhead\n)\n=\nâŸ¦\n0\n,\n2\nâŸ§\n\\mathcal{L}(\\lambda_{\\mathrm{complex\\_head}})=\\llbracket 0,2\\rrbracket\n.\nSee Figure\n16(c)\n.\n(a)\nsimple - head_height\n(b)\ncomplex - speed\n(c)\ncomplex - head_height\nFigure 16:\nHumEnv label histograms.\nAppendix B\nArchitectures and Hyperparameters\nOptimization:\nFor all baselines, when necessary, labels are encoded as latent variables of dimension\n16\n16\nvia an embedding matrix. We optimize all networks using the Adam optimizer with a learning rate of\n3\nâ‹…\n10\nâˆ’\n3\n3\\cdot 10^{-3}\n, employing cosine learning-rate decay for the policies, a batch size of 256, and\n10\n5\n10^{5}\ngradient steps for the\nÏ‡\n\\chi\nestimators and\n10\n6\n10^{6}\nfor the other networks. Value functions\nV\nV\nadditionally use layer normalization. Unless otherwise specified, we use the IQL hyperparameters\nÎ²\n=\n3\n\\beta=3\n,\nÎº\n=\n0.7\n\\kappa=0.7\n, and\nÎ³\n=\n0.99\n\\gamma=0.99\n, and perform Polyak averaging on the\nQ\nQ\n-networks with coefficient 0.005.\nArchitectures:\nFor Circle2d and HalfCheetah, the policies\nÏ€\n\\pi\n, value networks\nV\n,\nQ\nV,Q\n, and estimators\nÏ‡\n\\chi\nare MLPs with hidden size\n[\n256\n,\n256\n]\n[256,256]\nand ReLU activations. For HumEnv, the policies are MLPs with hidden size\n[\n1024\n,\n1024\n,\n1024\n]\n[1024,1024,1024]\nand ReLU activations.\nRelabeling:\nIn SCIQL, we use\np\nr\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\np_{\\mathrm{r}}^{\\lambda(\\mathcal{D})}\nas\np\nm\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\np_{\\mathrm{m}}^{\\lambda(\\mathcal{D})}\nfor all criteria of all environments.\nImplementations:\nOur implementations are written in JAX\n(Bradbury\net al.\n,\n2018\n)\n, and take inspiration from\nNishimori (\n2024\n)\n, allowing short training durations. In Circle2D and HalfCheetah, we get for BC (\nâ‰ˆ\n2\nâ€‹\nm\nâ€‹\ni\nâ€‹\nn\n\\approx 2\\mathrm{min}\n), CBC (\nâ‰ˆ\n3\nâ€‹\nm\nâ€‹\ni\nâ€‹\nn\n\\approx 3\\mathrm{min}\n), BCPMI (\nâ‰ˆ\n4\nâ€‹\nm\nâ€‹\ni\nâ€‹\nn\n\\approx 4\\mathrm{min}\n), SORL (\nâ‰ˆ\n15\nâ€‹\nm\nâ€‹\ni\nâ€‹\nn\n\\approx 15\\mathrm{min}\n), SCBC (\nâ‰ˆ\n3\nâ€‹\nm\nâ€‹\ni\nâ€‹\nn\n\\approx 3\\mathrm{min}\n) and SCIQL (\nâ‰ˆ\n35\nâ€‹\nm\nâ€‹\ni\nâ€‹\nn\n\\approx 3\\mathrm{5min}\n) on a NVIDIA V100 GPU for training runs. In HumEnv, we get for BC (\nâ‰ˆ\n5\nâ€‹\nm\nâ€‹\ni\nâ€‹\nn\n\\approx 5\\mathrm{min}\n), CBC (\nâ‰ˆ\n5\nâ€‹\nm\nâ€‹\ni\nâ€‹\nn\n\\approx 5\\mathrm{min}\n), BCPMI (\nâ‰ˆ\n6\nâ€‹\nm\nâ€‹\ni\nâ€‹\nn\n\\approx 6\\mathrm{min}\n), SORL (\nâ‰ˆ\n23\nâ€‹\nm\nâ€‹\ni\nâ€‹\nn\n\\approx 23\\mathrm{min}\n), SCBC (\nâ‰ˆ\n5\nâ€‹\nm\nâ€‹\ni\nâ€‹\nn\n\\approx 5\\mathrm{min}\n) and SCIQL (\nâ‰ˆ\n45\nâ€‹\nm\nâ€‹\ni\nâ€‹\nn\n\\approx 45\\mathrm{min}\n) on a NVIDIA A100 GPU for training runs. Our code and datasets can be found on our project website:\nhttps://sciql-iclr-2026.github.io/\n.\nAppendix C\nBaselines\nIn this subsection, we describe in more details our baselines. For the following, we define the dataset joint sampling distribution:\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\np^{\\mathcal{\\lambda(D)}}(z|s,a)\nand a mixture\np\nm\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\np^{\\mathcal{\\lambda(D)}}_{\\mathrm{m}}(z|s,a)\nof a set of sampling distributions:\np\nc\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\np_{\\mathrm{c}}^{\\mathcal{\\lambda(D)}}(z|s,a)\nwhich corresponds to the Dirac distribution of the style label associated to\n(\ns\n,\na\n)\n(s,a)\nwithin its trajectory in\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n\\lambda(\\mathcal{D})\n,\np\nf\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\np_{\\mathrm{f}}^{\\mathcal{\\lambda(D)}}(z|s,a)\nwhich corresponds to the uniform distribution on the styles associated to the future state-actions pairs within\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n\\lambda(\\mathcal{D})\nstarting from\n(\ns\n,\na\n)\n(s,a)\nand\np\nr\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n)\np_{\\mathrm{r}}^{\\mathcal{\\lambda(D)}}(z)\nwhich corresponds to the uniform distribution of the style labels over the entire dataset\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n\\lambda(\\mathcal{D})\n.\nBehavior Cloning (BC).\nBC\n(Pomerleau,\n1991\n)\nis the simplest of our baselines and learns by maximizing the likelihood of actions given states through supervised learning on\nğ’Ÿ\n\\mathcal{D}\n:\nJ\nBC\nâ€‹\n(\nÏ€\n)\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nğ’Ÿ\nâ€‹\n(\ns\n,\na\n)\nâ€‹\n[\nlog\nâ¡\nÏ€\nâ€‹\n(\na\n|\ns\n)\n]\n.\nJ_{\\mathrm{BC}}(\\pi)=\\mathbb{E}_{(s,a)\\sim p^{\\mathcal{D}}(s,a)}[\\log\\pi(a|s)].\n(17)\nWe use this baseline as a reference for style alignment performance without conditioning.\nConditioned Behavior Cloning (CBC).\nCBC is the simplest style-conditioned method of our baselines and consists in concatenating to BCâ€™s states their associated label within\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n\\lambda(\\mathcal{D})\n:\nJ\nCBC\nâ€‹\n(\nÏ€\n)\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nğ’Ÿ\nâ€‹\n(\ns\n,\na\n)\n,\nz\nâˆ¼\np\ncur\nğ’Ÿ\nâ€‹\n(\nz\n|\ns\n,\na\n)\nâ€‹\n[\nlog\nâ¡\nÏ€\nâ€‹\n(\na\n|\ns\n,\nz\n)\n]\nJ_{\\mathrm{CBC}}(\\pi)=\\mathbb{E}_{(s,a)\\sim p^{\\mathcal{D}}(s,a),z\\sim p_{\\mathrm{cur}}^{\\mathcal{D}}(z|s,a)}[\\log\\pi(a|s,z)]\n(18)\nThis baseline serves as a reference to test the various benefits of subsequent methods to better perform style alignment optimization.\nBehavior Cloning with Pointwise Mutual Information weighting (BCPMI).\nBCPMI\n(Yang\net al.\n,\n2024\n)\nseeks to address credit assignment issues between stateâ€“action pairs and style labels by relying on their mutual information estimates. For this, BCPMI uses Mutual Information Neural Estimation (MINE). In the information-theoretic setting, let\nS\nS\n,\nA\nA\n, and\nZ\nZ\nbe random variables corresponding to states, actions, and styles, respectively. The mutual information between stateâ€“action pairs\n(\nS\n,\nA\n)\n(S,A)\nand styles\nZ\nZ\ncan be written as the Kullbackâ€“Leibler (KL) divergence between the joint distribution\nP\nS\n,\nA\n,\nZ\nP_{S,A,Z}\nand the product of their marginals\nP\nS\n,\nA\nâŠ—\nP\nZ\nP_{S,A}\\otimes P_{Z}\n:\nI\nâ€‹\n(\nS\n,\nA\n;\nZ\n)\n=\nD\nK\nâ€‹\nL\nâ€‹\n(\nP\nS\n,\nA\n,\nZ\nâˆ¥\nP\nS\n,\nA\nâŠ—\nP\nZ\n)\n.\nI(S,A;Z)=D_{KL}(P_{S,A,Z}\\,\\|\\,P_{S,A}\\otimes P_{Z}).\n(19)\nAs directly estimating this mutual information is difficult, MINE relies on the Donskerâ€“Varadhan lower bound:\nI\nâ€‹\n(\nS\n,\nA\n;\nZ\n)\nâ‰¥\nsup\nT\nâˆˆ\nâ„±\nğ”¼\n(\ns\n,\na\n,\nz\n)\nâˆ¼\nP\nS\n,\nA\n,\nZ\nâ€‹\n[\nT\nâ€‹\n(\ns\n,\na\n,\nz\n)\n]\nâˆ’\nlog\nâ¡\n(\nğ”¼\n(\ns\n,\na\n,\nz\n)\nâˆ¼\nP\nS\n,\nA\nâŠ—\nP\nZ\nâ€‹\n[\ne\nT\nâ€‹\n(\ns\n,\na\n,\nz\n)\n]\n)\n,\nI(S,A;Z)\\geq\\sup_{T\\in\\mathcal{F}}\\mathbb{E}_{(s,a,z)\\sim P_{S,A,Z}}[T(s,a,z)]-\\log\\Big(\\mathbb{E}_{(s,a,z)\\sim P_{S,A}\\otimes P_{Z}}[e^{T(s,a,z)}]\\Big),\n(20)\nwhere\nâ„±\n\\mathcal{F}\ndenotes a class of functions\nT\n:\nğ’®\nÃ—\nğ’œ\nÃ—\nğ’µ\nâ†’\nâ„\nT:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{Z}\\rightarrow\\mathbb{R}\n. According to\nDonsker and Varadhan (\n1975\n)\n, optimizing this bound yields\nT\nâˆ—\nâ€‹\n(\ns\n,\na\n,\nz\n)\n=\nlog\nâ¡\np\nâ€‹\n(\ns\n,\na\n,\nz\n)\np\nâ€‹\n(\ns\n,\na\n)\nâ€‹\np\nâ€‹\n(\nz\n)\n=\nlog\nâ¡\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\np\nâ€‹\n(\nz\n)\n.\nT^{*}(s,a,z)=\\log\\frac{p(s,a,z)}{p(s,a)p(z)}=\\log\\frac{p(z|s,a)}{p(z)}.\n(21)\nBCPMI trains a neural network to approximate\nT\nâˆ—\nâ€‹\n(\ns\n,\na\n,\nz\n)\nT^{*}(s,a,z)\nand uses it to weight CBCâ€™s learning objective, increasing the impact of transitions with high style relevance while reducing that of less relevant ones:\nJ\nMINE\nâ€‹\n(\nT\n)\n\\displaystyle J_{\\mathrm{MINE}}(T)\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n,\na\n)\n,\nz\nâˆ¼\np\nc\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\nâ€‹\n[\nT\nâ€‹\n(\ns\n,\na\n,\nz\n)\n]\nâˆ’\nlog\nâ¡\n(\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nğ’Ÿ\nâ€‹\n(\ns\n,\na\n)\n,\nz\nâˆ¼\np\nr\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n)\nâ€‹\n[\ne\nT\nâ€‹\n(\ns\n,\na\n,\nz\n)\n]\n)\n,\n\\displaystyle=\\mathbb{E}_{(s,a)\\sim p^{\\lambda(\\mathcal{D})}(s,a),\\,z\\sim p_{\\mathrm{c}}^{\\lambda(\\mathcal{D})}(z|s,a)}[T(s,a,z)]-\\log\\Big(\\mathbb{E}_{(s,a)\\sim p^{\\mathcal{D}}(s,a),\\,z\\sim p_{\\mathrm{r}}^{\\lambda(\\mathcal{D})}(z)}[e^{T(s,a,z)}]\\Big),\n(22)\nJ\nBC\nâˆ’\nPMI\nâ€‹\n(\nÏ€\n)\n\\displaystyle J_{\\mathrm{BC-PMI}}(\\pi)\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n,\na\n)\n,\nz\nâˆ¼\np\nc\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\nâ€‹\n[\nexp\nâ¡\n(\nT\nâˆ—\nâ€‹\n(\ns\n,\na\n,\nz\n)\n)\nâ€‹\nlog\nâ¡\nÏ€\nâ€‹\n(\na\n|\ns\n,\nz\n)\n]\n.\n\\displaystyle=\\mathbb{E}_{(s,a)\\sim p^{\\lambda(\\mathcal{D})}(s,a),\\,z\\sim p_{\\mathrm{c}}^{\\lambda(\\mathcal{D})}(z|s,a)}[\\exp(T^{*}(s,a,z))\\log\\pi(a|s,z)].\n(23)\nThis baseline is notable as it constitutes a first step toward addressing the credit assignment challenges in style-conditioned policy learning. However, as it strictly focuses on imitation learning rather than task performance, it does not support style mixing and is therefore not designed to address distribution shifts at inference time, unlike our method.\nStylized Offline Reinforcement Learning (SORL):\nSORL\n(Mao\net al.\n,\n2024\n)\nis an important baseline to consider since it both addresses the optimization of policy diversity and task performance. Initially designed within a unsupervised learning setting, SORL is a two step algorithm which aims to learn a diverse set of high-performing policies from\nğ’Ÿ\n\\mathcal{D}\n. First, SORL uses the Expectation-Maximisation (EM) algorithm to first learn a finite set of diverse policies\n{\nÎ¼\n(\ni\n)\n}\n\\{\\mu^{(i)}\\}\nto capture the heterogeneity of\nğ’Ÿ\n\\mathcal{D}\n. The E step aims to fit an estimate\np\n^\nâ€‹\n(\nz\n=\ni\n|\nÏ„\n)\n\\hat{p}(z=i|\\tau)\nto the posteriors\np\nâ€‹\n(\nz\n=\ni\n|\nÏ„\n)\np(z=i|\\tau)\n, associating each trajectory to a given style among\nN\nN\nstyles. The M step aims to train the stylized policies\n{\nÎ¼\n(\ni\n)\n}\n\\{\\mu^{(i)}\\}\naccording to their associated style through\np\n^\nâ€‹\n(\nz\n=\ni\n|\nÏ„\n)\n\\hat{p}(z=i|\\tau)\n:\nE step:\nâ€‹\nâˆ€\ni\nâˆˆ\n{\n0\n,\nâ€¦\n,\nN\nâˆ’\n1\n}\n,\np\n^\nâ€‹\n(\nz\n=\ni\n|\nÏ„\n)\nâ‰ˆ\n1\nZ\nâ€‹\nâˆ‘\n(\ns\n,\na\n)\nâˆˆ\nÏ„\nÎ¼\n(\ni\n)\nâ€‹\n(\na\n|\ns\n)\n\\displaystyle\\text{\\lx@text@underline{E step:} }\\forall i\\in\\{0,...,N-1\\},\\hat{p}(z=i|\\tau)\\approx\\frac{1}{Z}\\sum_{(s,a)\\in\\tau}\\mu^{(i)}(a|s)\n(24)\nM step:\nâ€‹\nâˆ€\ni\nâˆˆ\n{\n0\n,\nâ€¦\n,\nN\nâˆ’\n1\n}\n,\nJ\nSORL - M step\nâ€‹\n(\nÎ¼\n(\ni\n)\n)\n=\n1\n|\nğ’Ÿ\n|\nâ€‹\nâˆ‘\nÏ„\nâˆˆ\nğ’Ÿ\nâˆ‘\ni\n=\n1\nm\np\n^\nâ€‹\n(\nz\n=\ni\n|\nÏ„\n)\nâ€‹\nâˆ‘\n(\ns\n,\na\n)\nâˆˆ\nÏ„\nlog\nâ¡\nÎ¼\n(\ni\n)\nâ€‹\n(\na\n|\ns\n)\n\\displaystyle\\text{\\lx@text@underline{M step:} }\\forall i\\in\\{0,...,N-1\\},J_{\\text{SORL - M step}}(\\mu^{(i)})=\\frac{1}{|\\mathcal{D}|}\\sum_{\\tau\\in\\mathcal{D}}\\sum_{i=1}^{m}\\hat{p}(z=i|\\tau)\\sum_{(s,a)\\in\\tau}\\log\\mu^{(i)}(a|s)\n(25)\nThen, to perform task performance optimization while preserving a certain amount of diversity, SORL proposes to train from\n{\nÎ¼\n(\ni\n)\n}\n\\{\\mu^{(i)}\\}\na set of policies\n{\nÏ€\n(\ni\n)\n}\n\\{\\pi^{(i)}\\}\nby solving the following constrained problem:\nâˆ€\ni\nâˆˆ\n{\n0\n,\nâ€¦\n,\nN\nâˆ’\n1\n}\n,\nÏ€\n(\ni\n)\n=\narg\nâ¡\nmax\nÏ€\n(\ni\n)\nâ¡\nJ\nâ€‹\n(\nÏ€\n(\ni\n)\n)\n\\displaystyle\\forall i\\in\\{0,...,N-1\\},\\quad\\pi^{(i)}=\\arg\\max_{\\pi^{(i)}}J(\\pi^{(i)})\n(26)\ns.t.\nğ”¼\ns\nâˆ¼\nÏ\nÎ¼\n(\ni\n)\nâ€‹\n(\ns\n)\nD\nK\nâ€‹\nL\n(\nÏ€\n(\ni\n)\n(\nâ‹…\n|\ns\n)\nâˆ¥\nÎ¼\n(\ni\n)\n(\nâ‹…\n|\ns\n)\n)\nâ‰¤\nÏµ\n,\nâˆ«\na\nÏ€\n(\ni\n)\n(\na\n|\ns\n)\nd\na\n=\n1\n,\nâˆ€\ns\n.\n\\displaystyle\\text{s.t.}\\quad\\mathbb{E}_{s\\sim\\rho_{\\mu^{(i)}}(s)}D_{KL}\\big(\\pi^{(i)}(\\cdot|s)\\,\\|\\,\\mu^{(i)}(\\cdot|s)\\big)\\leq\\epsilon,\\quad\\int_{a}\\pi^{(i)}(a|s)\\,da=1,\\;\\forall s.\n(27)\nBy using its associated Lagrangian optimization problem,\nMao\net al.\n(\n2024\n)\nshow that this problem can be cast into a Stylized Advantage Weighted Regression (SAWR) objective:\nâˆ€\ni\nâˆˆ\n{\n0\n,\nâ€¦\n,\nN\nâˆ’\n1\n}\n,\nJ\nSORL - SAWR\nâ€‹\n(\nÏ€\n(\ni\n)\n)\n=\nğ”¼\nÏ„\nâˆ¼\nğ’Ÿ\nâ€‹\np\n^\nâ€‹\n(\nz\n=\ni\n|\nÏ„\n)\nâ€‹\nâˆ‘\n(\ns\n,\na\n)\nâˆˆ\nÏ„\nlog\nâ¡\nÏ€\n(\ni\n)\nâ€‹\n(\na\n|\ns\n)\nâ€‹\nexp\nâ¡\n(\n1\nÎ±\nâ€‹\nA\nr\nâ€‹\n(\ns\n,\na\n)\n)\n.\n\\forall i\\in\\{0,...,N-1\\},J_{\\text{SORL - SAWR}}(\\pi^{(i)})=\\mathbb{E}_{\\tau\\sim\\mathcal{D}}\\hat{p}(z=i|\\tau)\\sum_{(s,a)\\in\\tau}\\log\\pi^{(i)}(a|s)\\exp\\left(\\frac{1}{\\alpha}A^{r}(s,a)\\right).\n(28)\nIn our supervised setting, the first step translates into the learning of a style conditioned policy\nÎ¼\nÎ»\n,\nâˆ—\n:\nğ’®\nâ†’\nÎ”\n(\nğ’œ\n)\nâˆˆ\nargmax\nÏ€\nS\n(\nÎ¼\n,\nz\n)\n,\nâˆ€\nz\nâˆˆ\nâ„’\n(\nÎ»\n)\n\\mu^{\\lambda,*}:\\mathcal{S}\\rightarrow\\Delta(\\mathcal{A})\\in\\mathrm{argmax}_{\\pi}\\;S(\\mu,z),\\forall z\\in\\mathcal{L}(\\lambda)\nby optimizing the style alignment objective while the second step translates into optimizing\nÎ¼\nÎ»\n,\nâˆ—\n\\mu^{\\lambda,*}\nâ€™s performance by learning under the solution\nÏ€\nr\n,\nÎ»\n,\nâˆ—\n\\pi^{r,\\lambda,*}\nof the following constrained problem:\nâˆ€\nz\nâˆˆ\nâ„’\n(\nÎ»\n)\n,\nÏ€\nr\n,\nÎ»\n,\nâˆ—\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n=\nargmax\nÏ€\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\nJ\n(\nÏ€\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n)\n\\displaystyle\\forall z\\in\\mathcal{L}(\\lambda),\\pi^{r,\\lambda,*}(\\cdot|\\cdot,z)=\\underset{\\pi(\\cdot|\\cdot,z)}{\\mathrm{argmax}}\\;J(\\pi(\\cdot|\\cdot,z))\n(29)\ns.t.\nğ”¼\ns\nâˆ¼\nÏ\nÎ¼\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\nâ€‹\n(\ns\n)\nD\nK\nâ€‹\nL\n(\nÏ€\n(\nâ‹…\n|\ns\n,\nz\n)\n|\n|\nÎ¼\n(\nâ‹…\n|\ns\n,\nz\n)\n)\nâ‰¤\nÎµ\n,\nâˆ«\na\nÏ€\n(\nâ‹…\n|\ns\n,\nz\n)\n=\n1\n,\nâˆ€\ns\n\\displaystyle\\text{s.t. }\\mathbb{E}_{s\\sim\\rho_{\\mu(\\cdot|\\cdot,z)}(s)}D_{KL}(\\pi(\\cdot|s,z)||\\mu(\\cdot|s,z))\\leq\\varepsilon,\\int_{a}\\pi(\\cdot|s,z)=1,\\forall s\n(30)\nLet\nz\nâˆˆ\nâ„’\nâ€‹\n(\nÎ»\n)\nz\\in\\mathcal{L}(\\lambda)\nbe a style label. Following a similar path as\nPeng\net al.\n(\n2019\n)\nand\nMao\net al.\n(\n2024\n)\n, we can state that maximizing\nJ\n(\nÏ€\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n)\nJ(\\pi(\\cdot|\\cdot,z))\nis similar to maximizing the expected improvement\nÎ·\n(\nÏ€\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n)\n=\nJ\n(\nÏ€\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n)\nâˆ’\nJ\n(\nÎ¼\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n)\n\\eta(\\pi(\\cdot|\\cdot,z))=J(\\pi(\\cdot|\\cdot,z))-J(\\mu(\\cdot|\\cdot,z))\n, which can be express as\nSchulman\net al.\n(\n2017\n)\nshow as:\nÎ·\n(\nÏ€\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n)\n=\nğ”¼\ns\nâˆ¼\nÏ\nÏ€\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\nâ€‹\n(\ns\n)\nğ”¼\na\nâˆ¼\nÏ€\n(\nâ‹…\n|\ns\n,\nz\n)\n[\nA\nÎ¼\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n(\ns\n,\na\n)\n]\n\\eta(\\pi(\\cdot|\\cdot,z))=\\mathbb{E}_{s\\sim\\rho_{\\pi(\\cdot|\\cdot,z)}(s)}\\mathbb{E}_{a\\sim\\pi(\\cdot|s,z)}[A^{\\mu(\\cdot|\\cdot,z)}(s,a)]\n(31)\nLike\nPeng\net al.\n(\n2019\n)\nshowed, we can substitute\nÏ\nÏ€\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n\\rho_{\\pi(\\cdot|\\cdot,z)}\nfor\nÏ\nÎ¼\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n\\rho_{\\mu(\\cdot|\\cdot,z)}\nto simplify this optimization problem as the resulting error has been shown to be bounded by\nD\nK\nâ€‹\nL\n(\nÏ€\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n|\n|\nÎ¼\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n)\nD_{KL}(\\pi(\\cdot|\\cdot,z)||\\mu(\\cdot|\\cdot,z))\nSchulman\net al.\n(\n2017\n)\n. Furthermore,\nPeng\net al.\n(\n2019\n)\nand\nMao\net al.\n(\n2024\n)\napproximate\nA\nÎ¼\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\nâ€‹\n(\ns\n,\na\n)\nA^{\\mu(\\cdot|\\cdot,z)}(s,a)\nby the advantage\nA\nÎ¼\nâ€‹\n(\ns\n,\na\n)\nA^{\\mu}(s,a)\nwhere\nÎ¼\n\\mu\nrepresents the policy distribution of the dataset. In our setting, we will use the advantage\nA\nr\nâ€‹\n(\ns\n,\na\n)\nA^{r}(s,a)\nestimated through IQL to be coherent with SCIQL. Consequently, SORLâ€™s stylized advantage weighted regression becomes in our context:\nÏ€\nr\n,\nÎ»\n,\nâˆ—\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n=\nargmax\nÏ€\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\nğ”¼\ns\nâˆ¼\nÏ\nÎ¼\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\nâ€‹\n(\ns\n)\nğ”¼\na\nâˆ¼\nÏ€\n(\nâ‹…\n|\ns\n,\nz\n)\n[\nA\nr\n(\ns\n,\na\n)\n]\n\\displaystyle\\pi^{r,\\lambda,*}(\\cdot|\\cdot,z)=\\underset{\\pi(\\cdot|\\cdot,z)}{\\mathrm{argmax}}\\;\\mathbb{E}_{s\\sim\\rho_{\\mu(\\cdot|\\cdot,z)}(s)}\\mathbb{E}_{a\\sim\\pi(\\cdot|s,z)}[A^{r}(s,a)]\n(32)\ns.t.\nğ”¼\ns\nâˆ¼\nÏ\nÎ¼\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\nâ€‹\n(\ns\n)\nD\nK\nâ€‹\nL\n(\nÏ€\n(\nâ‹…\n|\ns\n,\nz\n)\n|\n|\nÎ¼\n(\nâ‹…\n|\ns\n,\nz\n)\n)\nâ‰¤\nÎµ\n,\nâˆ«\na\nÏ€\n(\nâ‹…\n|\ns\n,\nz\n)\n=\n1\n,\nâˆ€\ns\n\\displaystyle\\text{s.t. }\\mathbb{E}_{s\\sim\\rho_{\\mu(\\cdot|\\cdot,z)}(s)}D_{KL}(\\pi(\\cdot|s,z)||\\mu(\\cdot|s,z))\\leq\\varepsilon,\\int_{a}\\pi(\\cdot|s,z)=1,\\forall s\n(33)\nAs\nPeng\net al.\n(\n2019\n)\nand\nMao\net al.\n(\n2024\n)\n, we compute the corresponding Lagrangian of this optimization problem:\nL\n(\nÏ€\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n,\nÎ±\nÎ¼\n,\nğœ¶\nÏ€\n)\n=\n\\displaystyle L(\\pi(\\cdot|\\cdot,z),\\alpha^{\\mu},\\bm{\\alpha}^{\\pi})=\nğ”¼\ns\nâˆ¼\nÏ\nÎ¼\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n[\nğ”¼\na\nâˆ¼\nÏ€\n(\nâ‹…\n|\ns\n,\nz\n)\nA\nr\n(\ns\n,\na\n)\n\\displaystyle\\mathbb{E}_{s\\sim\\rho_{\\mu(\\cdot|\\cdot,z)}}\\Big[\\mathbb{E}_{a\\sim\\pi(\\cdot|s,z)}A^{r}(s,a)\n(34)\n+\nÎ±\nÎ¼\n(\nÎµ\nâˆ’\nD\nK\nâ€‹\nL\n(\nÏ€\n(\nâ‹…\n|\ns\n,\nz\n)\nâˆ¥\nÎ¼\n(\nâ‹…\n|\ns\n,\nz\n)\n)\n)\n]\n\\displaystyle+\\alpha^{\\mu}\\big(\\varepsilon-D_{KL}(\\pi(\\cdot|s,z)\\,\\|\\,\\mu(\\cdot|s,z))\\big)\\Big]\n(35)\n+\nâˆ«\ns\nğœ¶\ns\nÏ€\nâ€‹\n(\n1\nâˆ’\nâˆ«\na\nÏ€\nâ€‹\n(\na\n|\ns\n,\nz\n)\nâ€‹\nğ‘‘\na\n)\nâ€‹\nğ‘‘\ns\n\\displaystyle+\\int_{s}\\bm{\\alpha}_{s}^{\\pi}\\Big(1-\\int_{a}\\pi(a|s,z)\\,da\\Big)ds\n(36)\n=\n\\displaystyle=\nâˆ«\ns\nÏ\nÎ¼\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n(\ns\n)\nd\ns\n[\nâˆ«\na\nÏ€\n(\na\n|\ns\n,\nz\n)\nd\na\nA\nr\n(\ns\n,\na\n)\n\\displaystyle\\int_{s}\\rho_{\\mu(\\cdot|\\cdot,z)}(s)ds\\Big[\\int_{a}\\pi(a|s,z)daA^{r}(s,a)\n(37)\n+\nÎ±\nÎ¼\n(\nÎµ\nâˆ’\nâˆ«\na\nÏ€\n(\na\n|\ns\n,\nz\n)\nlog\nÏ€\nâ€‹\n(\na\n|\ns\n,\nz\n)\nÎ¼\nâ€‹\n(\na\n|\ns\n,\nz\n)\nd\na\n]\n\\displaystyle+\\alpha^{\\mu}\\big(\\varepsilon-\\int_{a}\\pi(a|s,z)\\log\\frac{\\pi(a|s,z)}{\\mu(a|s,z)}da\\Big]\n(38)\n+\nâˆ«\ns\nğœ¶\ns\nÏ€\nâ€‹\n(\n1\nâˆ’\nâˆ«\na\nÏ€\nâ€‹\n(\na\n|\ns\n,\nz\n)\nâ€‹\nğ‘‘\na\n)\nâ€‹\nğ‘‘\ns\n\\displaystyle+\\int_{s}\\bm{\\alpha}_{s}^{\\pi}\\Big(1-\\int_{a}\\pi(a|s,z)\\,da\\Big)ds\n=\n\\displaystyle=\n(39)\nwith\nÎ±\nÎ¼\nâ‰¥\n0\n\\alpha^{\\mu}\\geq 0\nand\nğœ¶\nÏ€\n=\n{\nğœ¶\ns\nÏ€\nâˆˆ\nâ„\n,\ns\nâˆˆ\nğ’®\n}\n\\bm{\\alpha}^{\\pi}=\\{\\bm{\\alpha}_{s}^{\\pi}\\in\\mathbb{R},s\\in\\mathcal{S}\\}\nthe Lagrange multipliers. We differentiate\nL\n(\nÏ€\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n,\nÎ±\nÎ¼\n,\nğœ¶\nÏ€\n)\nL(\\pi(\\cdot|\\cdot,z),\\alpha^{\\mu},\\bm{\\alpha}^{\\pi})\nas:\nâˆ‚\nL\nâˆ‚\nÏ€\nâ€‹\n(\na\n|\ns\n,\nz\n)\n=\nÏ\nÎ¼\n(\nâ‹…\n|\ns\n,\nz\n)\nâ€‹\n(\ns\n)\nâ€‹\n[\nA\nr\nâ€‹\n(\ns\n,\na\n)\nâˆ’\nÎ±\nÎ¼\nâ€‹\nlog\nâ¡\nÏ€\nâ€‹\n(\na\n|\ns\n,\nz\n)\n+\nÎ±\nÎ¼\nâ€‹\nlog\nâ¡\nÎ¼\nâ€‹\n(\na\n|\ns\n,\nz\n)\nâˆ’\nÎ±\nÎ¼\n]\nâˆ’\nğœ¶\ns\nÏ€\n\\frac{\\partial L}{\\partial\\pi(a|s,z)}=\\rho_{\\mu(\\cdot|s,z)}(s)\\Big[A^{r}(s,a)-\\alpha^{\\mu}\\log\\pi(a|s,z)+\\alpha^{\\mu}\\log\\mu(a|s,z)-\\alpha^{\\mu}\\Big]-\\bm{\\alpha}_{s}^{\\pi}\n(40)\nSetting this derivative to zero yields the following closed-form solution:\nÏ€\nâˆ—\nâ€‹\n(\na\n|\ns\n,\nz\n)\n=\n1\nZ\nâ€‹\n(\ns\n,\nz\n)\nâ€‹\nÎ¼\nâ€‹\n(\na\n|\ns\n,\nz\n)\nâ€‹\nexp\nâ¡\n(\n1\nÎ±\nÎ¼\nâ€‹\nA\nr\nâ€‹\n(\ns\n,\na\n)\n)\n,\n\\pi^{*}(a|s,z)=\\frac{1}{Z(s,z)}\\,\\mu(a|s,z)\\,\\exp\\!\\Bigg(\\frac{1}{\\alpha^{\\mu}}A^{r}(s,a)\\Bigg),\n(41)\nwhere\nZ\nâ€‹\n(\ns\n,\nz\n)\nZ(s,z)\nis the normalization term defined as:\nZ\nâ€‹\n(\ns\n,\nz\n)\n=\nexp\nâ¡\n(\n1\nÏ\nÎ¼\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\nâ€‹\n(\ns\n)\nâ€‹\nğœ¶\ns\nÏ€\nÎ±\nÎ¼\n+\n1\n)\n.\nZ(s,z)=\\exp\\!\\Bigg(\\frac{1}{\\rho_{\\mu(\\cdot|\\cdot,z)}(s)}\\frac{\\bm{\\alpha}_{s}^{\\pi}}{\\alpha^{\\mu}}+1\\Bigg).\n(42)\nFinally, as\nPeng\net al.\n(\n2019\n)\nand\nMao\net al.\n(\n2024\n)\n, we estimate\nÏ€\nâˆ—\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n\\pi^{*}(\\cdot|\\cdot,z)\nwith a neural network policy\nÏ€\nÏˆ\n(\nâ‹…\n|\nâ‹…\n,\nz\n)\n\\pi_{\\psi}(\\cdot|\\cdot,z)\nby solving:\narg\nâ¡\nmin\nÏˆ\n\\displaystyle\\arg\\min_{\\psi}\\;\nğ”¼\ns\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n|\nz\n)\n[\nD\nK\nâ€‹\nL\n(\nÏ€\nâˆ—\n(\nâ‹…\n|\ns\n,\nz\n)\nâˆ¥\nÏ€\nÏˆ\n(\nâ‹…\n|\ns\n,\nz\n)\n)\n]\n\\displaystyle\\mathbb{E}_{s\\sim p^{\\lambda(\\mathcal{D})}(s|z)}\\Big[D_{KL}\\big(\\pi^{*}(\\cdot|s,z)\\,\\|\\,\\pi_{\\psi}(\\cdot|s,z)\\big)\\Big]\n(43)\n=\narg\nâ¡\nmin\nÏˆ\n\\displaystyle=\\;\\arg\\min_{\\psi}\\;\nğ”¼\ns\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n|\nz\n)\nâ€‹\n[\nâˆ«\na\n(\nÏ€\nâˆ—\nâ€‹\n(\na\n|\ns\n,\nz\n)\nâ€‹\nlog\nâ¡\nÏ€\nâˆ—\nâ€‹\n(\na\n|\ns\n,\nz\n)\nâˆ’\nÏ€\nâˆ—\nâ€‹\n(\na\n|\ns\n,\nz\n)\nâ€‹\nlog\nâ¡\nÏ€\nÏˆ\nâ€‹\n(\na\n|\ns\n,\nz\n)\n)\nâ€‹\nğ‘‘\na\n]\n\\displaystyle\\mathbb{E}_{s\\sim p^{\\lambda(\\mathcal{D})}(s|z)}\\Bigg[\\int_{a}\\Big(\\pi^{*}(a|s,z)\\log\\pi^{*}(a|s,z)-\\pi^{*}(a|s,z)\\log\\pi_{\\psi}(a|s,z)\\Big)da\\Bigg]\n(44)\n=\narg\nâ¡\nmin\nÏˆ\n\\displaystyle=\\;\\arg\\min_{\\psi}\\;\nâˆ’\nğ”¼\ns\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n|\nz\n)\nâ€‹\n[\nâˆ«\na\nÏ€\nâˆ—\nâ€‹\n(\na\n|\ns\n,\nz\n)\nâ€‹\nlog\nâ¡\nÏ€\nÏˆ\nâ€‹\n(\na\n|\ns\n,\nz\n)\nâ€‹\nğ‘‘\na\n]\n\\displaystyle-\\mathbb{E}_{s\\sim p^{\\lambda(\\mathcal{D})}(s|z)}\\Bigg[\\int_{a}\\pi^{*}(a|s,z)\\log\\pi_{\\psi}(a|s,z)\\,da\\Bigg]\n(45)\n=\narg\nâ¡\nmin\nÏˆ\n\\displaystyle=\\;\\arg\\min_{\\psi}\\;\nâˆ’\nğ”¼\ns\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n|\nz\n)\nâ€‹\n[\nâˆ«\na\n1\nZ\nâ€‹\n(\ns\n,\nz\n)\nâ€‹\nÎ¼\nâ€‹\n(\na\n|\ns\n,\nz\n)\nâ€‹\nexp\nâ¡\n(\n1\nÎ±\nÎ¼\nâ€‹\nA\nr\nâ€‹\n(\ns\n,\na\n)\n)\nâ€‹\nlog\nâ¡\nÏ€\nÏˆ\nâ€‹\n(\na\n|\ns\n,\nz\n)\nâ€‹\nğ‘‘\na\n]\n\\displaystyle-\\mathbb{E}_{s\\sim p^{\\lambda(\\mathcal{D})}(s|z)}\\Bigg[\\int_{a}\\frac{1}{Z(s,z)}\\mu(a|s,z)\\exp\\!\\left(\\tfrac{1}{\\alpha^{\\mu}}A^{r}(s,a)\\right)\\log\\pi_{\\psi}(a|s,z)\\,da\\Bigg]\n(46)\n=\narg\nâ¡\nmin\nÏˆ\n\\displaystyle=\\;\\arg\\min_{\\psi}\\;\nâˆ’\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n,\na\n|\nz\n)\nâ€‹\n[\n1\nZ\nâ€‹\n(\ns\n,\nz\n)\nâ€‹\nexp\nâ¡\n(\n1\nÎ±\nÎ¼\nâ€‹\nA\nr\nâ€‹\n(\ns\n,\na\n)\n)\nâ€‹\nlog\nâ¡\nÏ€\nÏˆ\nâ€‹\n(\na\n|\ns\n,\nz\n)\n]\n\\displaystyle-\\mathbb{E}_{(s,a)\\sim p^{\\lambda(\\mathcal{D})}(s,a|z)}\\,\\left[\\frac{1}{Z(s,z)}\\exp\\!\\left(\\tfrac{1}{\\alpha^{\\mu}}A^{r}(s,a)\\right)\\log\\pi_{\\psi}(a|s,z)\\;\\right]\n(47)\n=\narg\nâ¡\nmin\nÏˆ\n\\displaystyle=\\;\\arg\\min_{\\psi}\\;\nâˆ’\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n,\na\n)\nâ€‹\n[\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\nâ€‹\n1\nZ\nâ€‹\n(\ns\n,\nz\n)\nâ€‹\nexp\nâ¡\n(\n1\nÎ±\nÎ¼\nâ€‹\nA\nr\nâ€‹\n(\ns\n,\na\n)\n)\nâ€‹\nlog\nâ¡\nÏ€\nÏˆ\nâ€‹\n(\na\n|\ns\n,\nz\n)\n]\n\\displaystyle-\\mathbb{E}_{(s,a)\\sim p^{\\lambda(\\mathcal{D})}(s,a)}\\left[p(z|s,a)\\;\\frac{1}{Z(s,z)}\\exp\\!\\left(\\tfrac{1}{\\alpha^{\\mu}}A^{r}(s,a)\\right)\\log\\pi_{\\psi}(a|s,z)\\right]\n(48)\nBy neglecting the absorbing constant as\nPeng\net al.\n(\n2019\n); Mao\net al.\n(\n2024\n)\n, we can finally express the SORL objective in our supervised version:\narg\nâ¡\nmin\nÏˆ\nâˆ’\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n,\na\n)\nâ€‹\n[\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\nâ€‹\nexp\nâ¡\n(\n1\nÎ±\nÎ¼\nâ€‹\nA\nr\nâ€‹\n(\ns\n,\na\n)\n)\nâ€‹\nlog\nâ¡\nÏ€\nÏˆ\nâ€‹\n(\na\n|\ns\n,\nz\n)\n]\n\\arg\\min_{\\psi}\\;-\\mathbb{E}_{(s,a)\\sim p^{\\lambda(\\mathcal{D})}(s,a)}\\left[p(z|s,a)\\;\\exp\\!\\left(\\tfrac{1}{\\alpha^{\\mu}}A^{r}(s,a)\\right)\\log\\pi_{\\psi}(a|s,z)\\right]\n(49)\nAs we want to optimize this objective for all\nz\nâˆˆ\nâ„’\nâ€‹\n(\nÎ»\n)\nz\\in\\mathcal{L}(\\lambda)\n, we write below the general objective:\narg\nâ¡\nmin\nÏˆ\nâˆ’\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n,\na\n)\nâ€‹\n[\n1\n|\nÎ»\n|\nâ€‹\nâˆ‘\nz\n=\n0\n|\nÎ»\n|\nâˆ’\n1\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\nâ€‹\nexp\nâ¡\n(\n1\nÎ±\nÎ¼\nâ€‹\nA\nr\nâ€‹\n(\ns\n,\na\n)\n)\nâ€‹\nlog\nâ¡\nÏ€\nÏˆ\nâ€‹\n(\na\n|\ns\n,\nz\n)\n]\n\\arg\\min_{\\psi}\\;-\\mathbb{E}_{(s,a)\\sim p^{\\lambda(\\mathcal{D})}(s,a)}\\left[\\frac{1}{|\\lambda|}\\sum_{z=0}^{|\\lambda|-1}p(z|s,a)\\;\\exp\\!\\left(\\tfrac{1}{\\alpha^{\\mu}}A^{r}(s,a)\\right)\\log\\pi_{\\psi}(a|s,z)\\right]\n(50)\nAs in SCIQL, we can employ several strategies to estimate\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\np(z|s,a)\nthrough an estimator\nÏ‡\nâ€‹\n(\ns\n,\na\n,\nz\n)\n\\chi(s,a,z)\nwhich we all detail in Appendix\nE.1\n. Additionally, the advantage functions can be learned offline through IQL as in SCIQL. Hence, we can obtain our adapted SORL objectives by taking\nÎ²\n=\n1\n/\nÎ±\nÎ¼\n\\beta=1/\\alpha^{\\mu}\n:\nâ„’\nSORL\nâ€‹\n(\nV\nr\n)\n\\displaystyle\\mathcal{L}_{\\mathrm{SORL}}(V_{r})\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nğ’Ÿ\nâ€‹\n(\ns\n,\na\n)\nâ€‹\n[\nâ„“\nÎº\n2\nâ€‹\n(\nQ\nÂ¯\nr\nâ€‹\n(\ns\n,\na\n)\nâˆ’\nV\nr\nâ€‹\n(\ns\n)\n)\n]\n\\displaystyle=\\mathbb{E}_{(s,a)\\sim p^{\\mathcal{D}}(s,a)}[\\ell_{\\kappa}^{2}(\\bar{Q}_{r}(s,a)-V_{r}(s))]\n(51)\nâ„’\nSORL\nâ€‹\n(\nQ\nr\n)\n\\displaystyle\\mathcal{L}_{\\mathrm{SORL}}(Q_{r})\n=\nğ”¼\n(\ns\n,\na\n,\ns\nâ€²\n)\nâˆ¼\np\nğ’Ÿ\nâ€‹\n(\ns\n,\na\n,\ns\nâ€²\n)\n[\nr\n(\ns\n,\na\n)\n+\nÎ³\nV\nr\n(\ns\nâ€²\n)\nâˆ’\nQ\nr\n(\ns\n,\na\n)\n)\n2\n]\n\\displaystyle=\\mathbb{E}_{(s,a,s^{\\prime})\\sim p^{\\mathcal{D}}(s,a,s^{\\prime})}[r(s,a)+\\gamma V_{r}(s^{\\prime})-Q_{r}(s,a))^{2}]\n(52)\nJ\nSORL\nâ€‹\n(\nÏ€\n)\n\\displaystyle J_{\\mathrm{SORL}}(\\pi)\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nğ’Ÿ\nâ€‹\n(\ns\n,\na\n)\nâ€‹\n1\n|\nÎ»\n|\nâ€‹\nâˆ‘\nz\n=\n0\n|\nÎ»\n|\nâˆ’\n1\nÏ‡\nâ€‹\n(\ns\n,\na\n,\nz\n)\nâ€‹\ne\nÎ²\nâ€‹\nA\nr\nâ€‹\n(\ns\n,\na\n)\nâ€‹\nlog\nâ¡\nÏ€\nâ€‹\n(\na\n|\ns\n,\nz\n)\n\\displaystyle=\\mathbb{E}_{(s,a)\\sim p^{\\mathcal{D}}(s,a)}\\frac{1}{|\\lambda|}\\sum_{z=0}^{|\\lambda|-1}\\chi(s,a,z)e^{\\beta A^{r}(s,a)}\\log\\pi(a|s,z)\n(53)\nStyle-Conditioned Behavior Cloning (SCBC):\nSCBC corresponds to a simpler behavior cloning version of SCIQL whose objective can be written as:\nJ\nSCBC\nâ€‹\n(\nÏ€\n)\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nğ’Ÿ\nâ€‹\n(\ns\n,\na\n)\n,\nz\nâˆ¼\np\nf\nğ’Ÿ\nâ€‹\n(\nz\n|\ns\n,\na\n)\nâ€‹\n[\nlog\nâ¡\nÏ€\nâ€‹\n(\na\n|\ns\n,\nz\n)\n]\nJ_{\\mathrm{SCBC}}(\\pi)=\\mathbb{E}_{(s,a)\\sim p^{\\mathcal{D}}(s,a),z\\sim p^{\\mathcal{D}}_{\\mathrm{f}}(z|s,a)}[\\log\\pi(a|s,z)]\n(54)\nThis baseline is interesting as it shows both how style mixing with hindsight relabeling can be beneficial to style alignment while highlighting the impact of value learning when compared to SCIQL. For instance, value learning allows for relabeling outside of\np\nf\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\np_{\\mathrm{f}}^{\\lambda(\\mathcal{D})}\non top of optimizing the policy.\nAppendix D\nAdditional tables\nTable 2:\nExperiment complexity\nEnvironment\nCriterion\nğ’\nğ¥ğšğ›ğğ¥ğ¬\n\\bm{n_{\\mathrm{labels}}}\nğ’\nğğšğ­ğšğ¬ğğ­ğ¬\n\\bm{n_{\\mathrm{datasets}}}\nğ’\nğ¬ğğğğ¬\n\\bm{n_{\\mathrm{seeds}}}\nTotal trainings\nğ’\nğğ¯ğšğ¥\nâ€‹\n_\nâ€‹\nğğ©ğ¢ğ¬ğ¨ğğğ¬\n\\bm{n_{\\mathrm{eval\\_episodes}}}\nTotal evals episodes\ncircle2d\nposition\n8\n8\n2\n2\n5\n5\n80\n80\n10\n10\n800\n800\nmovement_direction\n8\n8\n2\n2\n5\n5\n80\n80\n10\n10\n800\n800\nturn_direction\n2\n2\n2\n2\n5\n5\n20\n20\n10\n10\n200\n200\nradius\n15\n15\n2\n2\n5\n5\n150\n150\n10\n10\n1500\n1500\nspeed\n15\n15\n2\n2\n5\n5\n150\n150\n10\n10\n1500\n1500\ncurvature_noise\n3\n3\n2\n2\n5\n5\n45\n45\n10\n10\n450\n450\nhalfcheetah\nspeed\n3\n3\n3\n3\n5\n5\n45\n45\n10\n10\n450\n450\nangle\n3\n3\n3\n3\n5\n5\n45\n45\n10\n10\n450\n450\ntorso_height\n3\n3\n3\n3\n5\n5\n45\n45\n10\n10\n450\n450\nbackfoot_height\n4\n4\n3\n3\n5\n5\n60\n60\n10\n10\n600\n600\nfrontfoot_height\n4\n4\n3\n3\n5\n5\n60\n60\n10\n10\n600\n600\nhumenv-simple\nhead_height\n2\n2\n1\n1\n5\n5\n10\n10\n10\n10\n100\n100\nhumenv-complex\nspeed\n3\n3\n1\n1\n5\n5\n15\n15\n10\n10\n150\n150\nhead_height\n3\n3\n1\n1\n5\n5\n15\n15\n10\n10\n150\n150\nall\n14 criteria\n76\n76\n_\n\\_\n_\n\\_\n820\n820\n_\n\\_\n8200\n8200\nIn this section, we display the full results for both style alignment and style-conditioned task performance optimization. These tables are computed for each environment and criterion\nÎ»\n\\lambda\nby averaging performance across 5 seeds and all labels in\nâ„’\nâ€‹\n(\nÎ»\n)\n\\mathcal{L}(\\lambda)\n. Table\n2\nreports the evaluation complexity statistics of our experiments, which, for each algorithm variant, requires 820 training runs and 8200 evaluation episodes. Normalized per seed, this corresponds to\n820\n/\n5\n=\n164\n820/5=164\nruns per algorithm, which justifies our use of averages in Table\n1\n, Table\n3\n, and Table\nLABEL:tab:Style-conditioned_task_performance_optimization_results_(full)\n. In the following, we write additional remarks about the full results tables.\nStyle alignment:\nIn Table\n3\n, SCIQL achieves better style alignment on most criteria, while being slightly lower on the\nturn_direction\n,\nradius\n, and\nspeed\ncriteria of Circle2d. This can be explained by the fact that these criteria do not require relabeling, and we show in Appendix\nE.2\nthat optimal performance can be recovered by changing the sampling distribution from\np\nr\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\np_{\\mathrm{r}}^{\\lambda(\\mathcal{D})}\nthat we globally use to\np\nc\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\np_{\\mathrm{c}}^{\\lambda(\\mathcal{D})}\nfor those particular criteria. Additionally, methods that do not perform style relabeling perform worse in inplace than in navigate for styles corresponding to specific subsets of the state space, such as position, highlighting the importance of style relabeling for alignment. For halfcheetah, SCIQL largely dominates all baselines demonstrating SCIQLâ€™s robustness to noisier trajectories. Namely, in the halfcheetah-vary-v0, SCIQL dominates even more the baselines. In particular, we recall from Appendix\nC\nthat SCBC sees an important decrease in its style alignment. This can be explained by the nature of the relabeling used in SCBC. For a given observed state-action pair in the dataset\n(\ns\n,\na\n)\n(s,a)\n, SCBC samples a future style\nz\nf\nz_{\\mathrm{f}}\nfrom the future of the trajectory and considers\n(\ns\n,\na\n,\nz\nf\n)\n(s,a,z_{\\mathrm{f}})\nas expert behavior. Indeed, for SCBC, every action is expert to reach the styles in the future of its trajectory. However, when style variations occur within the trajectory, for instance when alternating low and high speeds\n(\nz\nslow\n,\nâ€¦\n,\nz\nfast\n,\nâ€¦\n,\nz\nslow\n,\nâ€¦\n)\n(z_{\\mathrm{slow}},...,z_{\\mathrm{fast}},...,z_{\\mathrm{slow}},...)\nan action contributing to high speed\n(\ns\n,\na\n,\nz\nt\n)\n(s,a,z_{t})\nwith\nz\nf\n=\nz\nfast\nz_{\\mathrm{f}}=z_{\\mathrm{fast}}\ncould be relabeled as\n(\ns\n,\na\n,\nz\nslow\n)\n(s,a,z_{\\mathrm{slow}})\n, provoking the learning of an action for high speeds while being conditioned on\nz\nslow\nz_{\\mathrm{slow}}\n. SCIQL solves this problem by adding an advantage weighted regression mechanism to always strive to reach as fast as possible style alignment, consequently lowering the weights of wrong labels.\nTable 3:\nStyle alignment results (full).\nDataset\nBC\nCBC\nBCPMI\nSORL (\nÎ²\n=\n0\n\\beta=0\n)\nSCBC\nSCIQL\ncircle2d-inplace-v0 - position\n12.5\nÂ±\n6.9\n12.5\\pm 6.9\n15.0\nÂ±\n10.3\n15.0\\pm 10.3\n16.3\nÂ±\n13.5\n16.3\\pm 13.5\n14.9\nÂ±\n11.6\n14.9\\pm 11.6\n65.9\nÂ±\n11.5\n65.9\\pm 11.5\n98.0\nÂ±\n0.3\n98.0\\pm 0.3\ncircle2d-inplace-v0 - movement_direction\n12.5\nÂ±\n0.2\n12.5\\pm 0.2\n4.4\nÂ±\n1.6\n4.4\\pm 1.6\n4.1\nÂ±\n1.4\n4.1\\pm 1.4\n5.3\nÂ±\n4.2\n5.3\\pm 4.2\n12.5\nÂ±\n0.3\n12.5\\pm 0.3\n20.5\nÂ±\n4.4\n20.5\\pm 4.4\ncircle2d-inplace-v0 - turn_direction\n50.0\nÂ±\n25.1\n50.0\\pm 25.1\n100.0\nÂ±\n0.0\n100.0\\pm 0.0\n100.0\nÂ±\n0.1\n100.0\\pm 0.1\n100.0\nÂ±\n0.1\n100.0\\pm 0.1\n100.0\nÂ±\n0.0\n100.0\\pm 0.0\n82.6\nÂ±\n26.3\n82.6\\pm 26.3\ncircle2d-inplace-v0 - radius\n33.3\nÂ±\n1.2\n33.3\\pm 1.2\n99.1\nÂ±\n2.0\n99.1\\pm 2.0\n99.7\nÂ±\n0.6\n99.7\\pm 0.6\n99.8\nÂ±\n0.4\n99.8\\pm 0.4\n100.0\nÂ±\n0.0\n100.0\\pm 0.0\n96.1\nÂ±\n5.3\n96.1\\pm 5.3\ncircle2d-inplace-v0 - speed\n33.3\nÂ±\n4.2\n33.3\\pm 4.2\n99.9\nÂ±\n0.1\n99.9\\pm 0.1\n99.9\nÂ±\n0.0\n99.9\\pm 0.0\n99.9\nÂ±\n0.0\n99.9\\pm 0.0\n99.9\nÂ±\n0.0\n99.9\\pm 0.0\n91.6\nÂ±\n13.3\n91.6\\pm 13.3\ncircle2d-inplace-v0 - curvature_noise\n33.3\nÂ±\n0.0\n33.3\\pm 0.0\n33.3\nÂ±\n0.0\n33.3\\pm 0.0\n33.3\nÂ±\n0.1\n33.3\\pm 0.1\n33.3\nÂ±\n0.0\n33.3\\pm 0.0\n33.3\nÂ±\n0.0\n33.3\\pm 0.0\n59.1\nÂ±\n6.1\n59.1\\pm 6.1\ncircle2d-inplace-v0 - all\n29.1\nÂ±\n6.3\n29.1\\pm 6.3\n58.6\nÂ±\n2.3\n58.6\\pm 2.3\n58.9\nÂ±\n2.6\n58.9\\pm 2.6\n58.9\nÂ±\n2.7\n58.9\\pm 2.7\n68.6\nÂ±\n2.0\n68.6\\pm 2.0\n74.6\nÂ±\n9.3\n74.6\\pm 9.3\ncircle2d-navigate-v0 - position\n12.5\nÂ±\n7.4\n12.5\\pm 7.4\n16.7\nÂ±\n9.5\n16.7\\pm 9.5\n24.0\nÂ±\n11.8\n24.0\\pm 11.8\n22.3\nÂ±\n14.8\n22.3\\pm 14.8\n58.5\nÂ±\n9.5\n58.5\\pm 9.5\n98.4\nÂ±\n0.2\n98.4\\pm 0.2\ncircle2d-navigate-v0 - movement_direction\n12.5\nÂ±\n0.2\n12.5\\pm 0.2\n5.7\nÂ±\n4.9\n5.7\\pm 4.9\n3.2\nÂ±\n0.2\n3.2\\pm 0.2\n4.9\nÂ±\n3.7\n4.9\\pm 3.7\n12.5\nÂ±\n0.2\n12.5\\pm 0.2\n27.0\nÂ±\n5.7\n27.0\\pm 5.7\ncircle2d-navigate-v0 - turn_direction\n50.0\nÂ±\n13.4\n50.0\\pm 13.4\n100.0\nÂ±\n0.0\n100.0\\pm 0.0\n100.0\nÂ±\n0.0\n100.0\\pm 0.0\n100.0\nÂ±\n0.1\n100.0\\pm 0.1\n99.6\nÂ±\n0.1\n99.6\\pm 0.1\n96.0\nÂ±\n5.7\n96.0\\pm 5.7\ncircle2d-navigate-v0 - radius\n33.3\nÂ±\n10.6\n33.3\\pm 10.6\n98.1\nÂ±\n1.7\n98.1\\pm 1.7\n98.8\nÂ±\n1.4\n98.8\\pm 1.4\n99.7\nÂ±\n0.4\n99.7\\pm 0.4\n99.2\nÂ±\n0.9\n99.2\\pm 0.9\n95.8\nÂ±\n5.6\n95.8\\pm 5.6\ncircle2d-navigate-v0 - speed\n33.3\nÂ±\n0.0\n33.3\\pm 0.0\n99.9\nÂ±\n0.0\n99.9\\pm 0.0\n99.9\nÂ±\n0.0\n99.9\\pm 0.0\n99.6\nÂ±\n0.7\n99.6\\pm 0.7\n99.9\nÂ±\n0.0\n99.9\\pm 0.0\n96.0\nÂ±\n4.5\n96.0\\pm 4.5\ncircle2d-navigate-v0 - curvature_noise\n33.3\nÂ±\n0.0\n33.3\\pm 0.0\n33.3\nÂ±\n0.1\n33.3\\pm 0.1\n33.3\nÂ±\n0.3\n33.3\\pm 0.3\n33.3\nÂ±\n0.0\n33.3\\pm 0.0\n33.4\nÂ±\n0.1\n33.4\\pm 0.1\n40.0\nÂ±\n6.7\n40.0\\pm 6.7\ncircle2d-navigate-v0 - all\n29.1\nÂ±\n5.3\n29.1\\pm 5.3\n58.9\nÂ±\n2.7\n58.9\\pm 2.7\n59.9\nÂ±\n2.3\n59.9\\pm 2.3\n60.0\nÂ±\n3.3\n60.0\\pm 3.3\n67.2\nÂ±\n1.8\n67.2\\pm 1.8\n75.5\nÂ±\n4.7\n75.5\\pm 4.7\nhalfcheetah-fixed-v0 - speed\n33.3\nÂ±\n11.2\n33.3\\pm 11.2\n73.9\nÂ±\n11.8\n73.9\\pm 11.8\n77.6\nÂ±\n9.0\n77.6\\pm 9.0\n73.0\nÂ±\n20.3\n73.0\\pm 20.3\n95.9\nÂ±\n1.2\n95.9\\pm 1.2\n96.0\nÂ±\n1.6\n96.0\\pm 1.6\nhalfcheetah-fixed-v0 - angle\n33.3\nÂ±\n4.5\n33.3\\pm 4.5\n57.7\nÂ±\n15.5\n57.7\\pm 15.5\n68.0\nÂ±\n11.3\n68.0\\pm 11.3\n60.0\nÂ±\n15.5\n60.0\\pm 15.5\n55.2\nÂ±\n7.4\n55.2\\pm 7.4\n99.1\nÂ±\n1.1\n99.1\\pm 1.1\nhalfcheetah-fixed-v0 - torso_height\n33.3\nÂ±\n6.0\n33.3\\pm 6.0\n70.9\nÂ±\n11.1\n70.9\\pm 11.1\n82.2\nÂ±\n10.0\n82.2\\pm 10.0\n73.2\nÂ±\n8.9\n73.2\\pm 8.9\n79.3\nÂ±\n8.3\n79.3\\pm 8.3\n96.8\nÂ±\n3.5\n96.8\\pm 3.5\nhalfcheetah-fixed-v0 - backfoot_height\n25.0\nÂ±\n2.5\n25.0\\pm 2.5\n26.9\nÂ±\n2.6\n26.9\\pm 2.6\n29.6\nÂ±\n3.9\n29.6\\pm 3.9\n28.4\nÂ±\n2.8\n28.4\\pm 2.8\n32.4\nÂ±\n6.8\n32.4\\pm 6.8\n47.5\nÂ±\n2.0\n47.5\\pm 2.0\nhalfcheetah-fixed-v0 - frontfoot_height\n25.0\nÂ±\n5.5\n25.0\\pm 5.5\n26.5\nÂ±\n3.9\n26.5\\pm 3.9\n33.3\nÂ±\n7.8\n33.3\\pm 7.8\n30.7\nÂ±\n5.7\n30.7\\pm 5.7\n27.0\nÂ±\n3.0\n27.0\\pm 3.0\n50.5\nÂ±\n0.8\n50.5\\pm 0.8\nhalfcheetah-fixed-v0 - all\n30.0\nÂ±\n5.9\n30.0\\pm 5.9\n51.2\nÂ±\n9.0\n51.2\\pm 9.0\n58.1\nÂ±\n8.4\n58.1\\pm 8.4\n53.1\nÂ±\n10.6\n53.1\\pm 10.6\n58.0\nÂ±\n5.3\n58.0\\pm 5.3\n78.0\nÂ±\n1.8\n78.0\\pm 1.8\nhalfcheetah-stitch-v0 - speed\n33.3\nÂ±\n8.7\n33.3\\pm 8.7\n79.9\nÂ±\n8.0\n79.9\\pm 8.0\n70.1\nÂ±\n17.7\n70.1\\pm 17.7\n57.1\nÂ±\n23.2\n57.1\\pm 23.2\n92.0\nÂ±\n3.3\n92.0\\pm 3.3\n96.3\nÂ±\n0.5\n96.3\\pm 0.5\nhalfcheetah-stitch-v0 - angle\n33.3\nÂ±\n8.0\n33.3\\pm 8.0\n50.4\nÂ±\n14.2\n50.4\\pm 14.2\n72.1\nÂ±\n18.9\n72.1\\pm 18.9\n55.0\nÂ±\n20.4\n55.0\\pm 20.4\n60.8\nÂ±\n5.8\n60.8\\pm 5.8\n99.5\nÂ±\n0.2\n99.5\\pm 0.2\nhalfcheetah-stitch-v0 - torso_height\n33.3\nÂ±\n9.9\n33.3\\pm 9.9\n72.6\nÂ±\n7.2\n72.6\\pm 7.2\n87.1\nÂ±\n7.7\n87.1\\pm 7.7\n71.5\nÂ±\n10.7\n71.5\\pm 10.7\n80.1\nÂ±\n6.8\n80.1\\pm 6.8\n96.9\nÂ±\n1.4\n96.9\\pm 1.4\nhalfcheetah-stitch-v0 - backfoot_height\n25.0\nÂ±\n3.8\n25.0\\pm 3.8\n28.6\nÂ±\n2.7\n28.6\\pm 2.7\n30.0\nÂ±\n6.3\n30.0\\pm 6.3\n28.0\nÂ±\n3.4\n28.0\\pm 3.4\n27.3\nÂ±\n3.9\n27.3\\pm 3.9\n47.0\nÂ±\n2.4\n47.0\\pm 2.4\nhalfcheetah-stitch-v0 - frontfoot_height\n25.0\nÂ±\n3.6\n25.0\\pm 3.6\n29.1\nÂ±\n5.9\n29.1\\pm 5.9\n35.3\nÂ±\n6.0\n35.3\\pm 6.0\n30.2\nÂ±\n5.0\n30.2\\pm 5.0\n27.0\nÂ±\n3.5\n27.0\\pm 3.5\n50.3\nÂ±\n0.8\n50.3\\pm 0.8\nhalfcheetah-stitch-v0 - all\n30.0\nÂ±\n6.8\n30.0\\pm 6.8\n52.1\nÂ±\n7.6\n52.1\\pm 7.6\n58.9\nÂ±\n11.3\n58.9\\pm 11.3\n48.4\nÂ±\n12.5\n48.4\\pm 12.5\n57.4\nÂ±\n4.7\n57.4\\pm 4.7\n78.0\nÂ±\n1.1\n78.0\\pm 1.1\nhalfcheetah-vary-v0 - speed\n33.3\nÂ±\n6.9\n33.3\\pm 6.9\n63.3\nÂ±\n15.5\n63.3\\pm 15.5\n56.4\nÂ±\n23.2\n56.4\\pm 23.2\n54.3\nÂ±\n14.3\n54.3\\pm 14.3\n37.8\nÂ±\n5.8\n37.8\\pm 5.8\n96.7\nÂ±\n0.1\n96.7\\pm 0.1\nhalfcheetah-vary-v0 - angle\n33.3\nÂ±\n4.6\n33.3\\pm 4.6\n59.2\nÂ±\n24.2\n59.2\\pm 24.2\n46.4\nÂ±\n22.1\n46.4\\pm 22.1\n39.7\nÂ±\n10.8\n39.7\\pm 10.8\n34.8\nÂ±\n3.9\n34.8\\pm 3.9\n99.2\nÂ±\n0.6\n99.2\\pm 0.6\nhalfcheetah-vary-v0 - torso_height\n33.3\nÂ±\n7.6\n33.3\\pm 7.6\n79.3\nÂ±\n10.9\n79.3\\pm 10.9\n92.6\nÂ±\n7.5\n92.6\\pm 7.5\n77.0\nÂ±\n11.8\n77.0\\pm 11.8\n36.2\nÂ±\n6.1\n36.2\\pm 6.1\n98.8\nÂ±\n0.3\n98.8\\pm 0.3\nhalfcheetah-vary-v0 - backfoot_height\n25.0\nÂ±\n1.7\n25.0\\pm 1.7\n29.6\nÂ±\n4.5\n29.6\\pm 4.5\n32.9\nÂ±\n27.3\n32.9\\pm 27.3\n31.8\nÂ±\n5.3\n31.8\\pm 5.3\n25.1\nÂ±\n2.2\n25.1\\pm 2.2\n49.5\nÂ±\n1.4\n49.5\\pm 1.4\nhalfcheetah-vary-v0 - frontfoot_height\n25.0\nÂ±\n1.8\n25.0\\pm 1.8\n28.7\nÂ±\n5.1\n28.7\\pm 5.1\n34.9\nÂ±\n5.7\n34.9\\pm 5.7\n30.6\nÂ±\n5.3\n30.6\\pm 5.3\n24.8\nÂ±\n2.8\n24.8\\pm 2.8\n50.4\nÂ±\n1.0\n50.4\\pm 1.0\nhalfcheetah-vary-v0 - all\n30.0\nÂ±\n4.5\n30.0\\pm 4.5\n52.0\nÂ±\n12.0\n52.0\\pm 12.0\n52.6\nÂ±\n17.2\n52.6\\pm 17.2\n46.7\nÂ±\n9.5\n46.7\\pm 9.5\n31.7\nÂ±\n4.2\n31.7\\pm 4.2\n78.9\nÂ±\n0.7\n78.9\\pm 0.7\nhumenv-simple-v0 - head_height\n50.0\nÂ±\n44.4\n50.0\\pm 44.4\n89.1\nÂ±\n22.0\n89.1\\pm 22.0\n79.2\nÂ±\n26.7\n79.2\\pm 26.7\n79.4\nÂ±\n26.9\n79.4\\pm 26.9\n99.6\nÂ±\n0.0\n99.6\\pm 0.0\n99.6\nÂ±\n0.0\n99.6\\pm 0.0\nhumenv-simple-v0 - all\n50.0\nÂ±\n44.4\n50.0\\pm 44.4\n89.1\nÂ±\n22.0\n89.1\\pm 22.0\n79.2\nÂ±\n26.7\n79.2\\pm 26.7\n79.4\nÂ±\n26.9\n79.4\\pm 26.9\n99.6\nÂ±\n0.0\n99.6\\pm 0.0\n99.6\nÂ±\n0.0\n99.6\\pm 0.0\nhumenv-complex-v0 - speed\n33.3\nÂ±\n5.2\n33.3\\pm 5.2\n32.6\nÂ±\n7.1\n32.6\\pm 7.1\n32.1\nÂ±\n13.6\n32.1\\pm 13.6\n34.3\nÂ±\n4.7\n34.3\\pm 4.7\n34.1\nÂ±\n5.8\n34.1\\pm 5.8\n83.7\nÂ±\n5.9\n83.7\\pm 5.9\nhumenv-complex-v0 - head_height\n33.3\nÂ±\n2.7\n33.3\\pm 2.7\n61.6\nÂ±\n18.5\n61.6\\pm 18.5\n57.1\nÂ±\n23.3\n57.1\\pm 23.3\n61.1\nÂ±\n9.2\n61.1\\pm 9.2\n32.4\nÂ±\n1.3\n32.4\\pm 1.3\n83.3\nÂ±\n6.6\n83.3\\pm 6.6\nhumenv-complex-v0 - all\n33.3\nÂ±\n4.0\n33.3\\pm 4.0\n47.1\nÂ±\n12.8\n47.1\\pm 12.8\n44.6\nÂ±\n18.4\n44.6\\pm 18.4\n47.7\nÂ±\n6.9\n47.7\\pm 6.9\n33.2\nÂ±\n3.5\n33.2\\pm 3.5\n83.5\nÂ±\n6.2\n83.5\\pm 6.2\nStyle-conditioned task performance optimization results:\nWe see in Table\nLABEL:tab:Style-conditioned_task_performance_optimization_results_(full)\nthat choosing SORLâ€™s temperature\nÎ²\nSORL\n\\beta_{\\mathrm{SORL}}\nis challenging, as finding a good balance between style alignment and task performance is highly sensitive to its value. For instance, in halfcheetah-vary-v0 - speed, as in many other settings, increasing\nÎ²\nSORL\n\\beta_{\\mathrm{SORL}}\nfrom\n0\nto\n1\n1\nleads to an immediate drop in style alignment. In halfcheetah-vary-v0 - torso height, the decreases occur more gradually, with drops appearing both when moving from\nÎ²\nSORL\n=\n0\n\\beta_{\\mathrm{SORL}}=0\nto\nÎ²\nSORL\n=\n1\n\\beta_{\\mathrm{SORL}}=1\nand from\nÎ²\nSORL\n=\n1\n\\beta_{\\mathrm{SORL}}=1\nto\nÎ²\nSORL\n=\n3\n\\beta_{\\mathrm{SORL}}=3\n. In contrast, SCIQL shows no such degradation. These examples highlight that tuning SORLâ€™s temperature for style-conditioned task performance optimization can be troublesome, as it requires precise adjustment and the optimal value may vary across styles. SCIQLâ€™s temperature parameter\nÎ²\nSCIQL\n\\beta_{\\mathrm{SCIQL}}\ndiffers fundamentally: it does not encode the trade-off between style alignment and task performance. Instead, it is inherited directly from IQLâ€™s temperature parameter\nÎ²\nIQL\n\\beta_{\\mathrm{IQL}}\n, while the trade-off itself is handled by the Gated Advantage Weighted Regression. Experimentally, we find that setting\nÎ²\nSCIQL\n\\beta_{\\mathrm{SCIQL}}\nequal to the values of\nÎ²\nIQL\n\\beta_{\\mathrm{IQL}}\ncommonly used in the literature, typically chosen as\n1.0\n1.0\n,\n3.0\n3.0\n, and\n10.0\n10.0\n(Kostrikov\net al.\n,\n2021\n; Park\net al.\n,\n2024\n;\n2025\n)\n, is an effective heuristic. Hence, SCIQL maintains strong alignment by design while significantly improving task performance, without requiring precise fine-tuning.\nTable 4:\nStyle-conditioned task performance optimization results (full).\nDataset\nMetric\nSORL (\nÎ²\n=\n0\n\\beta=0\n)\nSORL (\nÎ²\n=\n1\n\\beta=1\n)\nSORL (\nÎ²\n=\n3\n\\beta=3\n)\nSCIQL (\nÎ»\n\\lambda\n)\nSCIQL (\nÎ»\n>\nr\n\\lambda>r\n)\nSCIQL (\nr\n>\nÎ»\nr>\\lambda\n)\ncircle2d-inplace-v0 - all\nStyle\n58.9\nÂ±\n2.7\n58.9\\pm 2.7\n54.5\nÂ±\n4.6\n54.5\\pm 4.6\n53.9\nÂ±\n4.2\n53.9\\pm 4.2\n74.6\nÂ±\n9.3\n74.6\\pm 9.3\n71.6\nÂ±\n4.8\n71.6\\pm 4.8\n47.9\nÂ±\n9.3\n47.9\\pm 9.3\ncircle2d-inplace-v0 - all\nTask\n16.6\nÂ±\n6.2\n16.6\\pm 6.2\n70.4\nÂ±\n3.8\n70.4\\pm 3.8\n73.6\nÂ±\n3.3\n73.6\\pm 3.3\n6.6\nÂ±\n2.8\n6.6\\pm 2.8\n68.6\nÂ±\n6.9\n68.6\\pm 6.9\n89.1\nÂ±\n3.3\n89.1\\pm 3.3\ncircle2d-inplace-v0 - position\nStyle\n14.9\nÂ±\n11.6\n14.9\\pm 11.6\n15.5\nÂ±\n5.5\n15.5\\pm 5.5\n12.1\nÂ±\n3.2\n12.1\\pm 3.2\n98.0\nÂ±\n0.3\n98.0\\pm 0.3\n96.1\nÂ±\n1.9\n96.1\\pm 1.9\n31.5\nÂ±\n6.8\n31.5\\pm 6.8\ncircle2d-inplace-v0 - position\nTask\n12.8\nÂ±\n7.4\n12.8\\pm 7.4\n79.2\nÂ±\n8.8\n79.2\\pm 8.8\n80.4\nÂ±\n7.7\n80.4\\pm 7.7\n2.6\nÂ±\n0.6\n2.6\\pm 0.6\n17.3\nÂ±\n4.1\n17.3\\pm 4.1\n69.3\nÂ±\n7.8\n69.3\\pm 7.8\ncircle2d-inplace-v0 - movement_direction\nStyle\n5.3\nÂ±\n4.2\n5.3\\pm 4.2\n5.5\nÂ±\n3.4\n5.5\\pm 3.4\n4.7\nÂ±\n1.7\n4.7\\pm 1.7\n20.5\nÂ±\n4.4\n20.5\\pm 4.4\n14.5\nÂ±\n2.3\n14.5\\pm 2.3\n12.5\nÂ±\n0.8\n12.5\\pm 0.8\ncircle2d-inplace-v0 - movement_direction\nTask\n0.5\nÂ±\n0.1\n0.5\\pm 0.1\n0.6\nÂ±\n0.1\n0.6\\pm 0.1\n0.6\nÂ±\n0.2\n0.6\\pm 0.2\n1.3\nÂ±\n0.2\n1.3\\pm 0.2\n80.8\nÂ±\n11.3\n80.8\\pm 11.3\n93.4\nÂ±\n3.3\n93.4\\pm 3.3\ncircle2d-inplace-v0 - turn_direction\nStyle\n100.0\nÂ±\n0.1\n100.0\\pm 0.1\n98.2\nÂ±\n1.3\n98.2\\pm 1.3\n97.9\nÂ±\n2.2\n97.9\\pm 2.2\n82.6\nÂ±\n26.3\n82.6\\pm 26.3\n85.5\nÂ±\n11.3\n85.5\\pm 11.3\n64.0\nÂ±\n16.9\n64.0\\pm 16.9\ncircle2d-inplace-v0 - turn_direction\nTask\n14.3\nÂ±\n3.2\n14.3\\pm 3.2\n88.4\nÂ±\n1.7\n88.4\\pm 1.7\n90.1\nÂ±\n3.1\n90.1\\pm 3.1\n6.9\nÂ±\n5.8\n6.9\\pm 5.8\n90.8\nÂ±\n3.7\n90.8\\pm 3.7\n95.0\nÂ±\n1.9\n95.0\\pm 1.9\ncircle2d-inplace-v0 - radius_category\nStyle\n99.8\nÂ±\n0.4\n99.8\\pm 0.4\n77.1\nÂ±\n12.2\n77.1\\pm 12.2\n72.6\nÂ±\n5.3\n72.6\\pm 5.3\n96.1\nÂ±\n5.3\n96.1\\pm 5.3\n99.9\nÂ±\n0.1\n99.9\\pm 0.1\n57.1\nÂ±\n16.3\n57.1\\pm 16.3\ncircle2d-inplace-v0 - radius_category\nTask\n28.3\nÂ±\n10.0\n28.3\\pm 10.0\n78.0\nÂ±\n4.6\n78.0\\pm 4.6\n87.4\nÂ±\n2.3\n87.4\\pm 2.3\n6.5\nÂ±\n3.2\n6.5\\pm 3.2\n53.9\nÂ±\n10.4\n53.9\\pm 10.4\n90.2\nÂ±\n2.2\n90.2\\pm 2.2\ncircle2d-inplace-v0 - speed_category\nStyle\n99.9\nÂ±\n0.0\n99.9\\pm 0.0\n97.4\nÂ±\n4.8\n97.4\\pm 4.8\n96.2\nÂ±\n5.0\n96.2\\pm 5.0\n91.6\nÂ±\n13.3\n91.6\\pm 13.3\n94.5\nÂ±\n7.6\n94.5\\pm 7.6\n88.4\nÂ±\n14.7\n88.4\\pm 14.7\ncircle2d-inplace-v0 - speed_category\nTask\n21.0\nÂ±\n8.2\n21.0\\pm 8.2\n86.3\nÂ±\n3.6\n86.3\\pm 3.6\n91.8\nÂ±\n2.4\n91.8\\pm 2.4\n19.5\nÂ±\n6.2\n19.5\\pm 6.2\n91.5\nÂ±\n2.1\n91.5\\pm 2.1\n93.2\nÂ±\n2.0\n93.2\\pm 2.0\ncircle2d-inplace-v0 - curvature_noise\nStyle\n33.3\nÂ±\n0.0\n33.3\\pm 0.0\n33.5\nÂ±\n0.3\n33.5\\pm 0.3\n39.8\nÂ±\n8.0\n39.8\\pm 8.0\n59.1\nÂ±\n6.1\n59.1\\pm 6.1\n38.9\nÂ±\n5.5\n38.9\\pm 5.5\n33.6\nÂ±\n0.3\n33.6\\pm 0.3\ncircle2d-inplace-v0 - curvature_noise\nTask\n22.8\nÂ±\n8.0\n22.8\\pm 8.0\n89.6\nÂ±\n4.2\n89.6\\pm 4.2\n91.3\nÂ±\n4.2\n91.3\\pm 4.2\n2.6\nÂ±\n0.8\n2.6\\pm 0.8\n77.5\nÂ±\n9.7\n77.5\\pm 9.7\n93.3\nÂ±\n2.4\n93.3\\pm 2.4\ncircle2d-navigate-v0 - all\nStyle\n60.0\nÂ±\n3.3\n60.0\\pm 3.3\n58.0\nÂ±\n5.2\n58.0\\pm 5.2\n57.6\nÂ±\n4.0\n57.6\\pm 4.0\n75.5\nÂ±\n4.7\n75.5\\pm 4.7\n76.5\nÂ±\n2.9\n76.5\\pm 2.9\n56.7\nÂ±\n6.1\n56.7\\pm 6.1\ncircle2d-navigate-v0 - all\nTask\n18.5\nÂ±\n7.3\n18.5\\pm 7.3\n69.7\nÂ±\n4.6\n69.7\\pm 4.6\n72.7\nÂ±\n3.9\n72.7\\pm 3.9\n7.9\nÂ±\n4.6\n7.9\\pm 4.6\n66.2\nÂ±\n6.5\n66.2\\pm 6.5\n87.7\nÂ±\n3.8\n87.7\\pm 3.8\ncircle2d-navigate-v0 - position\nStyle\n22.3\nÂ±\n14.8\n22.3\\pm 14.8\n15.7\nÂ±\n4.5\n15.7\\pm 4.5\n13.9\nÂ±\n3.1\n13.9\\pm 3.1\n98.4\nÂ±\n0.2\n98.4\\pm 0.2\n96.0\nÂ±\n2.2\n96.0\\pm 2.2\n35.9\nÂ±\n10.4\n35.9\\pm 10.4\ncircle2d-navigate-v0 - position\nTask\n19.8\nÂ±\n10.2\n19.8\\pm 10.2\n63.3\nÂ±\n13.8\n63.3\\pm 13.8\n69.4\nÂ±\n13.1\n69.4\\pm 13.1\n2.8\nÂ±\n0.6\n2.8\\pm 0.6\n20.1\nÂ±\n2.8\n20.1\\pm 2.8\n64.1\nÂ±\n9.3\n64.1\\pm 9.3\ncircle2d-navigate-v0 - movement_direction\nStyle\n4.9\nÂ±\n3.7\n4.9\\pm 3.7\n5.8\nÂ±\n5.4\n5.8\\pm 5.4\n5.6\nÂ±\n4.1\n5.6\\pm 4.1\n27.0\nÂ±\n5.7\n27.0\\pm 5.7\n18.4\nÂ±\n4.0\n18.4\\pm 4.0\n12.6\nÂ±\n0.8\n12.6\\pm 0.8\ncircle2d-navigate-v0 - movement_direction\nTask\n0.4\nÂ±\n0.0\n0.4\\pm 0.0\n0.7\nÂ±\n0.6\n0.7\\pm 0.6\n0.4\nÂ±\n0.1\n0.4\\pm 0.1\n1.1\nÂ±\n0.1\n1.1\\pm 0.1\n63.3\nÂ±\n13.4\n63.3\\pm 13.4\n94.5\nÂ±\n1.3\n94.5\\pm 1.3\ncircle2d-navigate-v0 - turn_direction\nStyle\n100.0\nÂ±\n0.1\n100.0\\pm 0.1\n99.6\nÂ±\n0.4\n99.6\\pm 0.4\n99.8\nÂ±\n0.1\n99.8\\pm 0.1\n96.0\nÂ±\n5.7\n96.0\\pm 5.7\n100.0\nÂ±\n0.0\n100.0\\pm 0.0\n81.9\nÂ±\n6.3\n81.9\\pm 6.3\ncircle2d-navigate-v0 - turn_direction\nTask\n18.4\nÂ±\n11.4\n18.4\\pm 11.4\n92.5\nÂ±\n3.2\n92.5\\pm 3.2\n93.4\nÂ±\n2.6\n93.4\\pm 2.6\n2.7\nÂ±\n1.3\n2.7\\pm 1.3\n94.4\nÂ±\n2.4\n94.4\\pm 2.4\n95.4\nÂ±\n1.4\n95.4\\pm 1.4\ncircle2d-navigate-v0 - radius_category\nStyle\n99.7\nÂ±\n0.4\n99.7\\pm 0.4\n91.2\nÂ±\n7.0\n91.2\\pm 7.0\n91.3\nÂ±\n11.5\n91.3\\pm 11.5\n95.8\nÂ±\n5.6\n95.8\\pm 5.6\n99.7\nÂ±\n0.1\n99.7\\pm 0.1\n77.1\nÂ±\n16.8\n77.1\\pm 16.8\ncircle2d-navigate-v0 - radius_category\nTask\n30.9\nÂ±\n9.4\n30.9\\pm 9.4\n83.0\nÂ±\n2.8\n83.0\\pm 2.8\n88.0\nÂ±\n1.8\n88.0\\pm 1.8\n16.3\nÂ±\n7.4\n16.3\\pm 7.4\n64.3\nÂ±\n8.4\n64.3\\pm 8.4\n87.1\nÂ±\n3.8\n87.1\\pm 3.8\ncircle2d-navigate-v0 - speed_category\nStyle\n99.6\nÂ±\n0.7\n99.6\\pm 0.7\n97.1\nÂ±\n6.3\n97.1\\pm 6.3\n99.6\nÂ±\n0.8\n99.6\\pm 0.8\n96.0\nÂ±\n4.5\n96.0\\pm 4.5\n99.2\nÂ±\n1.1\n99.2\\pm 1.1\n99.0\nÂ±\n1.8\n99.0\\pm 1.8\ncircle2d-navigate-v0 - speed_category\nTask\n21.6\nÂ±\n5.0\n21.6\\pm 5.0\n89.8\nÂ±\n3.6\n89.8\\pm 3.6\n90.6\nÂ±\n3.4\n90.6\\pm 3.4\n15.3\nÂ±\n8.7\n15.3\\pm 8.7\n92.7\nÂ±\n4.5\n92.7\\pm 4.5\n95.3\nÂ±\n2.2\n95.3\\pm 2.2\ncircle2d-navigate-v0 - curvature_noise\nStyle\n33.3\nÂ±\n0.0\n33.3\\pm 0.0\n38.9\nÂ±\n7.9\n38.9\\pm 7.9\n35.4\nÂ±\n4.6\n35.4\\pm 4.6\n40.0\nÂ±\n6.7\n40.0\\pm 6.7\n45.8\nÂ±\n9.8\n45.8\\pm 9.8\n33.6\nÂ±\n0.7\n33.6\\pm 0.7\ncircle2d-navigate-v0 - curvature_noise\nTask\n19.7\nÂ±\n7.7\n19.7\\pm 7.7\n88.8\nÂ±\n3.6\n88.8\\pm 3.6\n94.5\nÂ±\n2.1\n94.5\\pm 2.1\n9.0\nÂ±\n9.7\n9.0\\pm 9.7\n62.4\nÂ±\n7.5\n62.4\\pm 7.5\n89.9\nÂ±\n4.7\n89.9\\pm 4.7\nhalfcheetah-fix-v0 - all\nStyle\n53.1\nÂ±\n10.6\n53.1\\pm 10.6\n44.4\nÂ±\n6.1\n44.4\\pm 6.1\n41.3\nÂ±\n4.1\n41.3\\pm 4.1\n78.0\nÂ±\n1.8\n78.0\\pm 1.8\n78.1\nÂ±\n1.5\n78.1\\pm 1.5\n49.7\nÂ±\n5.4\n49.7\\pm 5.4\nhalfcheetah-fix-v0 - all\nTask\n32.1\nÂ±\n8.4\n32.1\\pm 8.4\n72.8\nÂ±\n5.6\n72.8\\pm 5.6\n80.6\nÂ±\n3.1\n80.6\\pm 3.1\n47.6\nÂ±\n2.3\n47.6\\pm 2.3\n56.5\nÂ±\n2.5\n56.5\\pm 2.5\n76.6\nÂ±\n5.5\n76.6\\pm 5.5\nhalfcheetah-fix-v0 - speed\nStyle\n73.0\nÂ±\n20.3\n73.0\\pm 20.3\n31.9\nÂ±\n9.4\n31.9\\pm 9.4\n34.6\nÂ±\n2.2\n34.6\\pm 2.2\n96.0\nÂ±\n1.6\n96.0\\pm 1.6\n95.6\nÂ±\n3.1\n95.6\\pm 3.1\n37.4\nÂ±\n6.5\n37.4\\pm 6.5\nhalfcheetah-fix-v0 - speed\nTask\n42.5\nÂ±\n13.2\n42.5\\pm 13.2\n72.5\nÂ±\n10.7\n72.5\\pm 10.7\n84.1\nÂ±\n2.4\n84.1\\pm 2.4\n48.1\nÂ±\n1.7\n48.1\\pm 1.7\n51.6\nÂ±\n1.9\n51.6\\pm 1.9\n87.5\nÂ±\n5.9\n87.5\\pm 5.9\nhalfcheetah-fix-v0 - angle\nStyle\n60.0\nÂ±\n15.5\n60.0\\pm 15.5\n41.4\nÂ±\n10.7\n41.4\\pm 10.7\n30.9\nÂ±\n2.7\n30.9\\pm 2.7\n99.1\nÂ±\n1.1\n99.1\\pm 1.1\n99.5\nÂ±\n0.1\n99.5\\pm 0.1\n69.9\nÂ±\n8.9\n69.9\\pm 8.9\nhalfcheetah-fix-v0 - angle\nTask\n26.2\nÂ±\n5.3\n26.2\\pm 5.3\n68.4\nÂ±\n9.9\n68.4\\pm 9.9\n83.2\nÂ±\n4.2\n83.2\\pm 4.2\n38.0\nÂ±\n2.0\n38.0\\pm 2.0\n48.9\nÂ±\n1.9\n48.9\\pm 1.9\n68.0\nÂ±\n6.3\n68.0\\pm 6.3\nhalfcheetah-fix-v0 - torso_height\nStyle\n73.2\nÂ±\n8.9\n73.2\\pm 8.9\n89.7\nÂ±\n4.7\n89.7\\pm 4.7\n84.0\nÂ±\n7.9\n84.0\\pm 7.9\n96.8\nÂ±\n3.5\n96.8\\pm 3.5\n98.0\nÂ±\n1.9\n98.0\\pm 1.9\n63.8\nÂ±\n5.1\n63.8\\pm 5.1\nhalfcheetah-fix-v0 - torso_height\nTask\n33.8\nÂ±\n8.9\n33.8\\pm 8.9\n73.1\nÂ±\n1.4\n73.1\\pm 1.4\n73.9\nÂ±\n1.7\n73.9\\pm 1.7\n50.3\nÂ±\n1.2\n50.3\\pm 1.2\n51.5\nÂ±\n1.0\n51.5\\pm 1.0\n68.8\nÂ±\n6.2\n68.8\\pm 6.2\nhalfcheetah-fix-v0 - backfoot_height\nStyle\n28.4\nÂ±\n2.8\n28.4\\pm 2.8\n34.7\nÂ±\n3.4\n34.7\\pm 3.4\n31.0\nÂ±\n4.6\n31.0\\pm 4.6\n47.5\nÂ±\n2.0\n47.5\\pm 2.0\n49.2\nÂ±\n1.2\n49.2\\pm 1.2\n37.6\nÂ±\n2.8\n37.6\\pm 2.8\nhalfcheetah-fix-v0 - backfoot_height\nTask\n34.7\nÂ±\n6.6\n34.7\\pm 6.6\n85.4\nÂ±\n1.5\n85.4\\pm 1.5\n86.4\nÂ±\n1.9\n86.4\\pm 1.9\n63.1\nÂ±\n5.0\n63.1\\pm 5.0\n76.2\nÂ±\n1.6\n76.2\\pm 1.6\n82.3\nÂ±\n4.4\n82.3\\pm 4.4\nhalfcheetah-fix-v0 - frontfoot_height\nStyle\n30.7\nÂ±\n5.7\n30.7\\pm 5.7\n24.1\nÂ±\n2.4\n24.1\\pm 2.4\n26.0\nÂ±\n3.0\n26.0\\pm 3.0\n50.5\nÂ±\n0.8\n50.5\\pm 0.8\n48.2\nÂ±\n1.2\n48.2\\pm 1.2\n39.9\nÂ±\n3.8\n39.9\\pm 3.8\nhalfcheetah-fix-v0 - frontfoot_height\nTask\n23.5\nÂ±\n7.9\n23.5\\pm 7.9\n64.4\nÂ±\n4.6\n64.4\\pm 4.6\n75.4\nÂ±\n5.3\n75.4\\pm 5.3\n38.3\nÂ±\n1.7\n38.3\\pm 1.7\n54.5\nÂ±\n5.9\n54.5\\pm 5.9\n76.3\nÂ±\n4.9\n76.3\\pm 4.9\nhalfcheetah-stitch-v0 - all\nStyle\n48.4\nÂ±\n12.5\n48.4\\pm 12.5\n41.1\nÂ±\n4.8\n41.1\\pm 4.8\n42.1\nÂ±\n4.9\n42.1\\pm 4.9\n78.0\nÂ±\n1.1\n78.0\\pm 1.1\n60.8\nÂ±\n6.0\n60.8\\pm 6.0\n33.8\nÂ±\n6.2\n33.8\\pm 6.2\nhalfcheetah-stitch-v0 - all\nTask\n31.9\nÂ±\n10.3\n31.9\\pm 10.3\n81.3\nÂ±\n3.1\n81.3\\pm 3.1\n78.3\nÂ±\n5.6\n78.3\\pm 5.6\n47.0\nÂ±\n2.3\n47.0\\pm 2.3\n70.0\nÂ±\n6.0\n70.0\\pm 6.0\n80.4\nÂ±\n9.0\n80.4\\pm 9.0\nhalfcheetah-stitch-v0 - speed\nStyle\n57.1\nÂ±\n23.2\n57.1\\pm 23.2\n34.0\nÂ±\n2.3\n34.0\\pm 2.3\n38.1\nÂ±\n4.7\n38.1\\pm 4.7\n96.3\nÂ±\n0.5\n96.3\\pm 0.5\n47.6\nÂ±\n11.2\n47.6\\pm 11.2\n32.6\nÂ±\n5.2\n32.6\\pm 5.2\nhalfcheetah-stitch-v0 - speed\nTask\n32.7\nÂ±\n14.3\n32.7\\pm 14.3\n83.3\nÂ±\n3.0\n83.3\\pm 3.0\n81.3\nÂ±\n5.0\n81.3\\pm 5.0\n47.2\nÂ±\n0.7\n47.2\\pm 0.7\n78.7\nÂ±\n8.5\n78.7\\pm 8.5\n84.0\nÂ±\n8.5\n84.0\\pm 8.5\nhalfcheetah-stitch-v0 - angle\nStyle\n55.0\nÂ±\n20.4\n55.0\\pm 20.4\n31.5\nÂ±\n3.3\n31.5\\pm 3.3\n34.7\nÂ±\n6.5\n34.7\\pm 6.5\n99.5\nÂ±\n0.2\n99.5\\pm 0.2\n92.5\nÂ±\n6.1\n92.5\\pm 6.1\n38.0\nÂ±\n6.0\n38.0\\pm 6.0\nhalfcheetah-stitch-v0 - angle\nTask\n25.5\nÂ±\n8.8\n25.5\\pm 8.8\n83.4\nÂ±\n4.2\n83.4\\pm 4.2\n79.7\nÂ±\n9.7\n79.7\\pm 9.7\n41.1\nÂ±\n4.2\n41.1\\pm 4.2\n54.8\nÂ±\n6.6\n54.8\\pm 6.6\n79.7\nÂ±\n7.1\n79.7\\pm 7.1\nhalfcheetah-stitch-v0 - torso_height\nStyle\n71.5\nÂ±\n10.7\n71.5\\pm 10.7\n83.0\nÂ±\n10.6\n83.0\\pm 10.6\n77.7\nÂ±\n5.9\n77.7\\pm 5.9\n96.9\nÂ±\n1.4\n96.9\\pm 1.4\n85.1\nÂ±\n7.4\n85.1\\pm 7.4\n44.5\nÂ±\n8.3\n44.5\\pm 8.3\nhalfcheetah-stitch-v0 - torso_height\nTask\n33.7\nÂ±\n10.9\n33.7\\pm 10.9\n74.1\nÂ±\n1.3\n74.1\\pm 1.3\n69.8\nÂ±\n4.1\n69.8\\pm 4.1\n48.3\nÂ±\n2.2\n48.3\\pm 2.2\n59.5\nÂ±\n5.5\n59.5\\pm 5.5\n82.1\nÂ±\n7.5\n82.1\\pm 7.5\nhalfcheetah-stitch-v0 - backfoot_height\nStyle\n28.0\nÂ±\n3.4\n28.0\\pm 3.4\n30.6\nÂ±\n5.0\n30.6\\pm 5.0\n32.0\nÂ±\n3.7\n32.0\\pm 3.7\n47.0\nÂ±\n2.4\n47.0\\pm 2.4\n39.1\nÂ±\n3.8\n39.1\\pm 3.8\n29.0\nÂ±\n6.3\n29.0\\pm 6.3\nhalfcheetah-stitch-v0 - backfoot_height\nTask\n41.2\nÂ±\n9.2\n41.2\\pm 9.2\n87.0\nÂ±\n1.8\n87.0\\pm 1.8\n84.6\nÂ±\n4.5\n84.6\\pm 4.5\n60.7\nÂ±\n3.7\n60.7\\pm 3.7\n80.8\nÂ±\n6.4\n80.8\\pm 6.4\n76.2\nÂ±\n9.8\n76.2\\pm 9.8\nhalfcheetah-stitch-v0 - frontfoot_height\nStyle\n30.2\nÂ±\n5.0\n30.2\\pm 5.0\n26.5\nÂ±\n2.9\n26.5\\pm 2.9\n28.0\nÂ±\n3.6\n28.0\\pm 3.6\n50.3\nÂ±\n0.8\n50.3\\pm 0.8\n39.5\nÂ±\n1.3\n39.5\\pm 1.3\n24.8\nÂ±\n5.0\n24.8\\pm 5.0\nhalfcheetah-stitch-v0 - frontfoot_height\nTask\n26.5\nÂ±\n8.3\n26.5\\pm 8.3\n78.5\nÂ±\n5.3\n78.5\\pm 5.3\n76.1\nÂ±\n4.9\n76.1\\pm 4.9\n37.8\nÂ±\n0.8\n37.8\\pm 0.8\n76.3\nÂ±\n3.2\n76.3\\pm 3.2\n79.8\nÂ±\n12.0\n79.8\\pm 12.0\nhalfcheetah-vary-v0 - all\nStyle\n46.7\nÂ±\n9.5\n46.7\\pm 9.5\n37.0\nÂ±\n3.0\n37.0\\pm 3.0\n31.1\nÂ±\n2.0\n31.1\\pm 2.0\n78.9\nÂ±\n0.7\n78.9\\pm 0.7\n77.8\nÂ±\n1.0\n77.8\\pm 1.0\n41.8\nÂ±\n5.0\n41.8\\pm 5.0\nhalfcheetah-vary-v0 - all\nTask\n35.9\nÂ±\n9.0\n35.9\\pm 9.0\n79.0\nÂ±\n3.2\n79.0\\pm 3.2\n82.6\nÂ±\n3.1\n82.6\\pm 3.1\n50.6\nÂ±\n1.3\n50.6\\pm 1.3\n58.0\nÂ±\n1.7\n58.0\\pm 1.7\n84.6\nÂ±\n3.2\n84.6\\pm 3.2\nhalfcheetah-vary-v0 - speed\nStyle\n54.3\nÂ±\n14.3\n54.3\\pm 14.3\n33.3\nÂ±\n0.3\n33.3\\pm 0.3\n33.4\nÂ±\n0.2\n33.4\\pm 0.2\n96.7\nÂ±\n0.1\n96.7\\pm 0.1\n96.9\nÂ±\n0.4\n96.9\\pm 0.4\n40.7\nÂ±\n6.1\n40.7\\pm 6.1\nhalfcheetah-vary-v0 - speed\nTask\n42.7\nÂ±\n9.3\n42.7\\pm 9.3\n88.2\nÂ±\n2.4\n88.2\\pm 2.4\n88.7\nÂ±\n2.2\n88.7\\pm 2.2\n48.1\nÂ±\n1.3\n48.1\\pm 1.3\n50.7\nÂ±\n0.9\n50.7\\pm 0.9\n84.1\nÂ±\n5.2\n84.1\\pm 5.2\nhalfcheetah-vary-v0 - angle\nStyle\n39.7\nÂ±\n10.8\n39.7\\pm 10.8\n32.9\nÂ±\n4.2\n32.9\\pm 4.2\n31.8\nÂ±\n2.0\n31.8\\pm 2.0\n99.2\nÂ±\n0.6\n99.2\\pm 0.6\n98.7\nÂ±\n1.8\n98.7\\pm 1.8\n44.3\nÂ±\n5.2\n44.3\\pm 5.2\nhalfcheetah-vary-v0 - angle\nTask\n19.0\nÂ±\n7.4\n19.0\\pm 7.4\n83.1\nÂ±\n3.6\n83.1\\pm 3.6\n84.7\nÂ±\n2.3\n84.7\\pm 2.3\n48.0\nÂ±\n2.1\n48.0\\pm 2.1\n55.3\nÂ±\n1.1\n55.3\\pm 1.1\n84.8\nÂ±\n3.0\n84.8\\pm 3.0\nhalfcheetah-vary-v0 - torso_height\nStyle\n77.0\nÂ±\n11.8\n77.0\\pm 11.8\n60.7\nÂ±\n4.1\n60.7\\pm 4.1\n36.9\nÂ±\n3.2\n36.9\\pm 3.2\n98.8\nÂ±\n0.3\n98.8\\pm 0.3\n98.8\nÂ±\n0.3\n98.8\\pm 0.3\n59.3\nÂ±\n7.1\n59.3\\pm 7.1\nhalfcheetah-vary-v0 - torso_height\nTask\n37.3\nÂ±\n11.7\n37.3\\pm 11.7\n68.2\nÂ±\n2.9\n68.2\\pm 2.9\n74.0\nÂ±\n3.0\n74.0\\pm 3.0\n50.5\nÂ±\n0.5\n50.5\\pm 0.5\n50.9\nÂ±\n1.3\n50.9\\pm 1.3\n87.2\nÂ±\n1.9\n87.2\\pm 1.9\nhalfcheetah-vary-v0 - backfoot_height\nStyle\n31.8\nÂ±\n5.3\n31.8\\pm 5.3\n32.8\nÂ±\n3.8\n32.8\\pm 3.8\n27.4\nÂ±\n3.5\n27.4\\pm 3.5\n49.5\nÂ±\n1.4\n49.5\\pm 1.4\n45.7\nÂ±\n1.2\n45.7\\pm 1.2\n28.2\nÂ±\n2.9\n28.2\\pm 2.9\nhalfcheetah-vary-v0 - backfoot_height\nTask\n48.1\nÂ±\n7.5\n48.1\\pm 7.5\n80.3\nÂ±\n2.9\n80.3\\pm 2.9\n82.6\nÂ±\n4.7\n82.6\\pm 4.7\n69.0\nÂ±\n1.7\n69.0\\pm 1.7\n75.0\nÂ±\n1.8\n75.0\\pm 1.8\n87.9\nÂ±\n1.6\n87.9\\pm 1.6\nhalfcheetah-vary-v0 - frontfoot_height\nStyle\n30.6\nÂ±\n5.3\n30.6\\pm 5.3\n25.4\nÂ±\n2.8\n25.4\\pm 2.8\n25.9\nÂ±\n1.3\n25.9\\pm 1.3\n50.4\nÂ±\n1.0\n50.4\\pm 1.0\n48.7\nÂ±\n1.2\n48.7\\pm 1.2\n36.5\nÂ±\n3.6\n36.5\\pm 3.6\nhalfcheetah-vary-v0 - frontfoot_height\nTask\n32.4\nÂ±\n8.9\n32.4\\pm 8.9\n75.4\nÂ±\n4.0\n75.4\\pm 4.0\n83.0\nÂ±\n3.1\n83.0\\pm 3.1\n37.5\nÂ±\n1.1\n37.5\\pm 1.1\n58.0\nÂ±\n3.2\n58.0\\pm 3.2\n79.0\nÂ±\n4.3\n79.0\\pm 4.3\nhumenv-simple-v0 - head_height\nStyle\n79.4\nÂ±\n26.9\n79.4\\pm 26.9\n99.1\nÂ±\n0.9\n99.1\\pm 0.9\n99.4\nÂ±\n0.4\n99.4\\pm 0.4\n99.6\nÂ±\n0.0\n99.6\\pm 0.0\n99.6\nÂ±\n0.1\n99.6\\pm 0.1\n99.5\nÂ±\n0.2\n99.5\\pm 0.2\nhumenv-simple-v0 - head_height\nTask\n14.6\nÂ±\n14.5\n14.6\\pm 14.5\n16.0\nÂ±\n7.5\n16.0\\pm 7.5\n20.0\nÂ±\n12.5\n20.0\\pm 12.5\n19.1\nÂ±\n7.1\n19.1\\pm 7.1\n31.7\nÂ±\n4.8\n31.7\\pm 4.8\n36.5\nÂ±\n0.4\n36.5\\pm 0.4\nhumenv-simple-v0 - all\nStyle\n79.4\nÂ±\n26.9\n79.4\\pm 26.9\n99.1\nÂ±\n0.9\n99.1\\pm 0.9\n99.4\nÂ±\n0.4\n99.4\\pm 0.4\n99.6\nÂ±\n0.0\n99.6\\pm 0.0\n99.6\nÂ±\n0.1\n99.6\\pm 0.1\n99.5\nÂ±\n0.2\n99.5\\pm 0.2\nhumenv-simple-v0 - all\nTask\n14.6\nÂ±\n14.5\n14.6\\pm 14.5\n16.0\nÂ±\n7.5\n16.0\\pm 7.5\n20.0\nÂ±\n12.5\n20.0\\pm 12.5\n19.1\nÂ±\n7.1\n19.1\\pm 7.1\n31.7\nÂ±\n4.8\n31.7\\pm 4.8\n36.5\nÂ±\n0.4\n36.5\\pm 0.4\nhumenv-complex-v0 - speed\nStyle\n34.3\nÂ±\n4.7\n34.3\\pm 4.7\n28.8\nÂ±\n8.4\n28.8\\pm 8.4\n22.6\nÂ±\n11.1\n22.6\\pm 11.1\n83.7\nÂ±\n5.9\n83.7\\pm 5.9\n91.6\nÂ±\n8.9\n91.6\\pm 8.9\n33.3\nÂ±\n3.7\n33.3\\pm 3.7\nhumenv-complex-v0 - speed\nTask\n5.7\nÂ±\n1.6\n5.7\\pm 1.6\n39.7\nÂ±\n5.2\n39.7\\pm 5.2\n33.6\nÂ±\n8.2\n33.6\\pm 8.2\n12.0\nÂ±\n1.6\n12.0\\pm 1.6\n16.2\nÂ±\n2.3\n16.2\\pm 2.3\n40.0\nÂ±\n2.6\n40.0\\pm 2.6\nhumenv-complex-v0 - head_height\nStyle\n61.1\nÂ±\n9.2\n61.1\\pm 9.2\n22.0\nÂ±\n13.6\n22.0\\pm 13.6\n24.3\nÂ±\n18.9\n24.3\\pm 18.9\n83.3\nÂ±\n6.6\n83.3\\pm 6.6\n90.1\nÂ±\n9.3\n90.1\\pm 9.3\n33.3\nÂ±\n4.9\n33.3\\pm 4.9\nhumenv-complex-v0 - head_height\nTask\n4.5\nÂ±\n3.8\n4.5\\pm 3.8\n19.6\nÂ±\n5.3\n19.6\\pm 5.3\n20.5\nÂ±\n9.3\n20.5\\pm 9.3\n10.0\nÂ±\n2.8\n10.0\\pm 2.8\n15.7\nÂ±\n2.6\n15.7\\pm 2.6\n41.9\nÂ±\n3.8\n41.9\\pm 3.8\nhumenv-complex-v0 - all\nStyle\n47.7\nÂ±\n6.9\n47.7\\pm 6.9\n25.4\nÂ±\n11.0\n25.4\\pm 11.0\n23.5\nÂ±\n15.0\n23.5\\pm 15.0\n83.5\nÂ±\n6.2\n83.5\\pm 6.2\n90.8\nÂ±\n9.1\n90.8\\pm 9.1\n33.3\nÂ±\n4.3\n33.3\\pm 4.3\nhumenv-complex-v0 - all\nTask\n5.1\nÂ±\n2.7\n5.1\\pm 2.7\n29.7\nÂ±\n5.2\n29.7\\pm 5.2\n27.1\nÂ±\n8.8\n27.1\\pm 8.8\n11.0\nÂ±\n2.2\n11.0\\pm 2.2\n15.9\nÂ±\n2.5\n15.9\\pm 2.5\n41.0\nÂ±\n3.2\n41.0\\pm 3.2\nAppendix E\nAblations\nFor the following, we define the dataset joint sampling distribution:\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\np^{\\mathcal{\\lambda(D)}}(z|s,a)\nand a mixture\np\nm\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\np^{\\mathcal{\\lambda(D)}}_{\\mathrm{m}}(z|s,a)\nof a set of sampling distributions:\np\nc\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\np_{\\mathrm{c}}^{\\mathcal{\\lambda(D)}}(z|s,a)\nwhich corresponds to the Dirac distribution of the style label associated to\n(\ns\n,\na\n)\n(s,a)\nwithin its trajectory in\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n\\lambda(\\mathcal{D})\n,\np\nf\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\np_{\\mathrm{f}}^{\\mathcal{\\lambda(D)}}(z|s,a)\nwhich corresponds to the uniform distribution on the styles associated to the future state-actions pairs within\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n\\lambda(\\mathcal{D})\nstarting from\n(\ns\n,\na\n)\n(s,a)\nand\np\nr\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n)\np_{\\mathrm{r}}^{\\mathcal{\\lambda(D)}}(z)\nwhich corresponds to the uniform distribution of the style labels over the entire dataset\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n\\lambda(\\mathcal{D})\n.\nE.1\nHow do we need to estimate\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\np(z|s,a)\n?\nFigure 17:\nStyle alignments histograms for different\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\np(z|s,a)\nestimation strategies.\nEstimating\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\np(z|s,a)\nrelates to estimating the correspondence between a state-action pair and a style which is a key component of our problem. We tested for this purpose four distinct strategies to form an estimator\nÏ‡\nâ€‹\n(\ns\n,\na\n,\nz\n)\n\\chi(s,a,z)\nof\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\np(z|s,a)\n. A first strategy noted\nind\nconsists in taking as the estimator the indicator of\n{\nz\n=\nz\nc\n}\n\\{z=z_{\\mathrm{c}}\\}\nwith\nz\nc\nz_{c}\nthe associated label of\n(\ns\n,\na\n)\n(s,a)\nwithin\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n\\lambda(\\mathcal{D})\n:\nâˆ€\n(\ns\n,\na\n,\nz\nc\n)\nâˆˆ\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n,\nÏ‡\nind\nâ€‹\n(\ns\n,\na\n,\nz\n)\n=\nÏ‡\nind\nâ€‹\n(\nz\nc\n,\nz\n)\n=\nğŸ™\nâ€‹\n(\nz\n=\nz\nc\n)\n\\forall(s,a,z_{c})\\in\\lambda(\\mathcal{D}),\\chi_{\\mathrm{ind}}(s,a,z)=\\chi_{\\mathrm{ind}}(z_{c},z)=\\mathbbm{1}(z=z_{\\mathrm{c}})\n(55)\nAs\nÎ»\n\\lambda\ncan attribute several labels to\n(\ns\n,\na\n)\n(s,a)\nwithin\nğ’Ÿ\n\\mathcal{D}\n, we can state that:\nâˆ€\n(\ns\n,\na\n)\nâˆˆ\nğ’Ÿ\n,\nğ”¼\nz\nc\nâˆ¼\np\nc\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\nâ€‹\n[\nÏ‡\nind\nâ€‹\n(\nz\nc\n,\nz\n)\n]\n\\displaystyle\\forall(s,a)\\in\\mathcal{D},\\mathbb{E}_{z_{\\mathrm{c}}\\sim p_{\\mathrm{c}}^{\\lambda(\\mathcal{D})}(z|s,a)}[\\chi_{\\mathrm{ind}}(z_{c},z)]\n=\nğ”¼\nz\nc\nâˆ¼\np\nc\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\nâ€‹\n[\nğŸ™\nâ€‹\n(\nz\n=\nz\nc\n)\n]\nâ‰ˆ\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\n\\displaystyle=\\mathbb{E}_{z_{\\mathrm{c}}\\sim p_{\\mathrm{c}}^{\\lambda(\\mathcal{D})}(z|s,a)}[\\mathbbm{1}(z=z_{\\mathrm{c}})]\\approx p(z|s,a)\n(56)\nas the expectation of an indicator variable is the probability of its associated event. Hence, using\nÏ‡\nind\n\\chi_{\\mathrm{ind}}\ncan be justified when relying on a sufficient number of samples during training.\nAnother approach noted\nMINE\nis to use the MINE estimator described in Appendix\nC\nto estimate:\nT\nâˆ—\nâ€‹\n(\ns\n,\na\n,\nz\n)\n=\nlog\nâ¡\np\nâ€‹\n(\ns\n,\na\n,\nz\n)\np\nâ€‹\n(\ns\n,\na\n)\nâ€‹\np\nâ€‹\n(\nz\n)\n=\nlog\nâ¡\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\np\nâ€‹\n(\nz\n)\nT^{*}(s,a,z)=\\log\\frac{p(s,a,z)}{p(s,a)p(z)}=\\log\\frac{p(z|s,a)}{p(z)}\n(57)\nby optimizing:\nJ\nMINE\nâ€‹\n(\nT\n)\n\\displaystyle J_{\\mathrm{MINE}}(T)\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n,\na\n)\n,\nz\nâˆ¼\np\nc\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\nâ€‹\n[\nT\nâ€‹\n(\ns\n,\na\n,\nz\n)\n]\nâˆ’\nlog\nâ¡\n(\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n,\na\n)\n,\nz\nâˆ¼\np\nr\nğ’Ÿ\nâ€‹\n(\nz\n)\nâ€‹\n[\ne\nT\nâ€‹\n(\ns\n,\na\n,\nz\n)\n]\n)\n\\displaystyle=\\mathbb{E}_{(s,a)\\sim p^{\\lambda(\\mathcal{D})}(s,a),z\\sim p_{\\mathrm{c}}^{\\lambda(\\mathcal{D})}(z|s,a)}[T(s,a,z)]-\\log\\left(\\mathbb{E}_{(s,a)\\sim p^{\\lambda(\\mathcal{D})}(s,a),z\\sim p_{\\mathrm{r}}^{\\mathcal{D}}(z)}\\left[e^{T(s,a,z)}\\right]\\right)\n(58)\nand taking:\nÏ‡\nMINE\nâ€‹\n(\ns\n,\na\n,\nz\n)\n\\displaystyle\\chi_{\\mathrm{MINE}}(s,a,z)\n=\np\nr\nğ’Ÿ\nâ€‹\n(\nz\n)\nâ€‹\ne\nT\nâ€‹\n(\ns\n,\na\n,\nz\n)\n\\displaystyle=p_{\\mathrm{r}}^{\\mathcal{D}}(z)e^{T(s,a,z)}\n(59)\nâ‰ˆ\np\nr\nğ’Ÿ\nâ€‹\n(\nz\n)\nâ€‹\ne\nlog\nâ¡\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\np\nâ€‹\n(\nz\n)\n\\displaystyle\\approx p_{\\mathrm{r}}^{\\mathcal{D}}(z)e^{\\log\\frac{p(z|s,a)}{p(z)}}\n(60)\nâ‰ˆ\np\nr\nğ’Ÿ\nâ€‹\n(\nz\n)\nâ€‹\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\np\nâ€‹\n(\nz\n)\n\\displaystyle\\approx p_{\\mathrm{r}}^{\\mathcal{D}}(z)\\frac{p(z|s,a)}{p(z)}\n(61)\nâ‰ˆ\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\n\\displaystyle\\approx p(z|s,a)\n(62)\nAlso, as we seek to approximate\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\nâˆˆ\n[\n0\n,\n1\n]\np(z|s,a)\\in[0,1]\nwith discrete labels, we propose to train directly a neural network\nÏ‡\nâ€‹\n(\ns\n,\na\n,\nz\n)\n\\chi(s,a,z)\nwithin the MINE objective, taking\np\nr\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n)\np_{\\mathrm{r}}^{\\lambda(\\mathcal{D})}(z)\nas an approximation of\np\nâ€‹\n(\nz\n)\np(z)\n:\nJ\nMINE\nâ€‹\n(\nÏ‡\n)\n=\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n,\na\n)\n,\nz\nâˆ¼\np\nc\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n|\ns\n,\na\n)\nâ€‹\n[\nlog\nâ¡\nÏ‡\nâ€‹\n(\ns\n,\na\n,\nz\n)\np\nr\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n)\n]\nâˆ’\nlog\nâ¡\n(\nğ”¼\n(\ns\n,\na\n)\nâˆ¼\np\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\ns\n,\na\n)\n,\nz\nâˆ¼\np\nr\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n)\nâ€‹\n[\ne\nlog\nâ¡\nÏ‡\nâ€‹\n(\ns\n,\na\n,\nz\n)\np\nr\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\nâ€‹\n(\nz\n)\n]\n)\nJ_{\\mathrm{MINE}}(\\chi)=\\mathbb{E}_{(s,a)\\sim p^{\\lambda(\\mathcal{D})}(s,a),z\\sim p_{\\mathrm{c}}^{\\lambda(\\mathcal{D})}(z|s,a)}[\\log\\frac{\\chi(s,a,z)}{p_{\\mathrm{r}}^{\\lambda(\\mathcal{D})}(z)}]-\\log\\left(\\mathbb{E}_{(s,a)\\sim p^{\\lambda(\\mathcal{D})}(s,a),z\\sim p_{\\mathrm{r}}^{\\lambda(\\mathcal{D})}(z)}\\left[e^{\\log\\frac{\\chi(s,a,z)}{p^{\\lambda(\\mathcal{D})}_{\\mathrm{r}}(z)}}\\right]\\right)\n(63)\nwith\nÏ‡\n\\chi\nâ€™s output activations taken as a sigmoid and a softmax to define the\nsigmoid\nand\nsoftmax\nstrategies respectively.\nWe evaluate the impact of each strategy on style alignment and report the results in Table\n5\nand Figure\n17\n. For SORL, both\nMINE\nand\nsoftmax\nachieve the best performance, while for SCIQL the best results are obtained with\nind\nand\nsoftmax\n. Accordingly, in our experiments we adopt\nsoftmax\nfor SORL and\nind\nfor SCIQL.\nTable 5:\nStyle alignments for different\np\nâ€‹\n(\nz\n|\ns\n,\na\n)\np(z|s,a)\nestimation strategies.\nDataset\nSORL\n(ind)\nSORL\n(MINE)\nSORL\n(sigmoid)\nSORL\n(softmax)\nSCIQL\n(ind)\nSCIQL\n(MINE)\nSCIQL\n(sigmoid)\nSCIQL\n(softmax)\nmujoco_halfcheetah-fix\n30.3\nÂ±\n3.4\n30.3\\pm 3.4\n52.6\nÂ±\n12.4\n52.6\\pm 12.4\n44.0\nÂ±\n11.7\n44.0\\pm 11.7\n53.1\nÂ±\n10.6\n53.1\\pm 10.6\n78.0\nÂ±\n1.8\n78.0\\pm 1.8\n67.4\nÂ±\n8.1\n67.4\\pm 8.1\n69.0\nÂ±\n7.1\n69.0\\pm 7.1\n77.9\nÂ±\n1.1\n77.9\\pm 1.1\nmujoco_halfcheetah-stitch\n30.0\nÂ±\n4.5\n30.0\\pm 4.5\n52.7\nÂ±\n10.8\n52.7\\pm 10.8\n43.0\nÂ±\n10.7\n43.0\\pm 10.7\n48.4\nÂ±\n12.5\n48.4\\pm 12.5\n78.0\nÂ±\n1.1\n78.0\\pm 1.1\n67.4\nÂ±\n8.0\n67.4\\pm 8.0\n69.5\nÂ±\n6.0\n69.5\\pm 6.0\n77.8\nÂ±\n1.5\n77.8\\pm 1.5\nmujoco_halfcheetah-vary\n29.7\nÂ±\n4.3\n29.7\\pm 4.3\n47.0\nÂ±\n10.1\n47.0\\pm 10.1\n42.7\nÂ±\n11.5\n42.7\\pm 11.5\n46.7\nÂ±\n9.5\n46.7\\pm 9.5\n78.9\nÂ±\n0.7\n78.9\\pm 0.7\n73.6\nÂ±\n5.7\n73.6\\pm 5.7\n67.1\nÂ±\n6.4\n67.1\\pm 6.4\n78.8\nÂ±\n0.9\n78.8\\pm 0.9\nrandom_circles-inplace-v0\n29.4\nÂ±\n3.5\n29.4\\pm 3.5\n59.1\nÂ±\n2.7\n59.1\\pm 2.7\n46.6\nÂ±\n11.9\n46.6\\pm 11.9\n58.9\nÂ±\n2.7\n58.9\\pm 2.7\n74.7\nÂ±\n9.3\n74.7\\pm 9.3\n74.3\nÂ±\n2.0\n74.3\\pm 2.0\n53.6\nÂ±\n19.8\n53.6\\pm 19.8\n73.7\nÂ±\n7.7\n73.7\\pm 7.7\nrandom_circles-navigate-v0\n29.1\nÂ±\n6.1\n29.1\\pm 6.1\n59.9\nÂ±\n3.2\n59.9\\pm 3.2\n46.9\nÂ±\n8.7\n46.9\\pm 8.7\n60.0\nÂ±\n3.3\n60.0\\pm 3.3\n75.5\nÂ±\n4.7\n75.5\\pm 4.7\n75.5\nÂ±\n4.6\n75.5\\pm 4.6\n62.1\nÂ±\n12.9\n62.1\\pm 12.9\n75.4\nÂ±\n4.3\n75.4\\pm 4.3\nall_datasets\n29.8\nÂ±\n4.0\n29.8\\pm 4.0\n53.8\nÂ±\n8.6\n53.8\\pm 8.6\n44.9\nÂ±\n11.3\n44.9\\pm 11.3\n53.2\nÂ±\n8.5\n53.2\\pm 8.5\n77.2\nÂ±\n3.2\n77.2\\pm 3.2\n70.9\nÂ±\n6.0\n70.9\\pm 6.0\n64.9\nÂ±\n10.1\n64.9\\pm 10.1\n76.9\nÂ±\n2.8\n76.9\\pm 2.8\nE.2\nWhat is the impact of the choice of\np\nm\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\np_{\\mathrm{m}}^{\\lambda(\\mathcal{D})}\n?\nTo address the lower performance of SCIQL on the\nturn_direction\n,\nradius\n, and\nspeed\ncriteria of Circle2d, we evaluated SCIQL by sampling styles from\np\nc\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\np_{\\mathrm{c}}^{\\lambda(\\mathcal{D})}\nrather than\np\nr\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\np_{\\mathrm{r}}^{\\lambda(\\mathcal{D})}\n. As shown in the histogram in Figure\n18\n, using\np\nc\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\np_{\\mathrm{c}}^{\\lambda(\\mathcal{D})}\nimproves style alignment to its maximum score, highlighting both SCIQLâ€™s flexibility in varying its style sampling distributions and the potential importance of this choice when optimizing style alignment.\nFigure 18:\nSCIQL performance under\np\nr\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\np_{\\mathrm{r}}^{\\lambda(\\mathcal{D})}\nvs\np\nc\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\np_{\\mathrm{c}}^{\\lambda(\\mathcal{D})}\n?\nE.3\nHow robust is SCIQL to imperfect style annotations?\nWhile relying on labeling functions allows for explainable and precise style annotations, style annotations could in practice be imperfect due to the noisiness of domain experts. For instance, alternative labeling approaches such as human generated labels or VLMs could provide noisy labels due to biases, stochasticity and unclear cuts between style transitions. All those imperfections can have a significant impact on style alignment. Hence, to measure the robustness of SCIQL in comparison to the baselines, we simulate labeling imperfections by modifying the labeling procedure such that for a given criterion\nÎ»\n\\lambda\n, each state-action-style triplet\n(\ns\nt\n,\na\nt\n,\nz\nt\n)\n(s_{t},a_{t},z_{t})\nof\nÎ»\nâ€‹\n(\nğ’Ÿ\n)\n\\lambda(\\mathcal{D})\nis polluted with a probability\nÎ¶\n\\zeta\nby changing its label\nz\nt\nz_{t}\nto another label\nz\n~\nt\n\\tilde{z}_{t}\nsampled uniformly among other available labels of\nâ„’\nâ€‹\n(\nÎ»\n)\n\\mathcal{L}(\\lambda)\n. We plot in Figure\n19\nthe evolution of the style alignment of the different baselines for the halfcheetah-fix-v0 - speed in Subfigure\n19(a)\n, halfcheetah-fix-v0 - angle tasks in Subfigure\n19(b)\nand the average of those evolutions as halfcheetah-fix-v0 - speed + angle in Subfigure\n19(c)\n.\nFirst, for noise levels going from\n0.0\n0.0\nto\n0.6\n0.6\n, we see that SCIQL maintains a very good style alignment. More precisely, SCIQL is on average (i.e. in speed_label + angle_label) better aligned with a noise level of\n0.6\n0.6\nthan all of the other baselines with no noise. The other baselines lose all their alignment even for small noise levels such as\n0.2\n0.2\n, obtaining style alignments equal to BCâ€™s, which means that the baselines consider any noisy label as uninformative noise and ignore them, losing all conditioning capabilities. This shows that\nSCIQL is significantly more robust to label noise than any test baseline\n, highlighting the benefits of integrating RL signals to style alignment training.\nSecond, above a certain noise threshold\nÎ¶\nÂ¯\n\\bar{\\zeta}\n, we see that SCIQLâ€™s alignment plummets towards\n0\n, which is in fact a good feature. A possible intuition is that this threshold corresponds to the noise level above which the true labeling of each state-action pair is no longer the majority in the noisy dataset. Beyond this threshold, for SCIQL,\nthe best outcome for alignment is to reach wrong labels\n. Indeed, for each state-action pair\n(\ns\n,\na\n)\n(s,a)\n, the probability of labeling to the right label\nz\nz\nis\np\nright\n=\n1\nâˆ’\nÎ¶\np_{\\mathrm{right}}=1-\\zeta\n, while the probability of choosing a wrong label is\np\nwrong\n=\nÎ¶\np_{\\mathrm{wrong}}=\\zeta\n. Since wrong labels are sampled uniformly, each individual wrong label\nz\n~\ni\nâˆˆ\nZ\nwrong\n=\nâ„’\nâ€‹\n(\nÎ»\n)\n\\\n{\nz\n}\n\\tilde{z}_{i}\\in Z_{\\mathrm{wrong}}=\\mathcal{L}(\\lambda)\\backslash\\{z\\}\nhas a probability\np\ni\n=\nÎ¶\n|\nÎ»\n|\nâˆ’\n1\np_{\\mathrm{i}}=\\frac{\\zeta}{|\\lambda|-1}\nto be selected,\n|\nÎ»\n|\n|\\lambda|\nbeing the total numbers of labels in\nâ„’\nâ€‹\n(\nÎ»\n)\n\\mathcal{L}(\\lambda)\n. Consequently, for the right label to maintain the majority position, the threshold needs to verify:\nâˆ€\nz\n~\ni\nâˆˆ\nZ\nwrong\n,\np\ntrue\n>\np\ni\nâ‡”\np\ntrue\n>\nmax\nz\n~\ni\nâˆˆ\nZ\nwrong\nâ€‹\np\ni\nâ‡”\n1\nâˆ’\nÎ¶\n>\nÎ¶\n|\nÎ»\n|\nâˆ’\n1\nâ‡”\n|\nÎ»\n|\nâˆ’\n1\n|\nÎ»\n|\n>\nÎ¶\n\\forall\\tilde{z}_{i}\\in Z_{\\mathrm{wrong}},p_{\\mathrm{true}}>p_{i}\\Leftrightarrow p_{\\mathrm{true}}>\\underset{\\tilde{z}_{i}\\in Z_{\\mathrm{wrong}}}{\\max}\\;p_{i}\\Leftrightarrow 1-\\zeta>\\frac{\\zeta}{|\\lambda|-1}\\Leftrightarrow\\frac{|\\lambda|-1}{|\\lambda|}>\\zeta\n(64)\nAlso, as described in Appendix\nA\n, both speed and angle criteria have the same number of\n|\nÎ»\n|\n=\n3\n|\\lambda|=3\nlabels each and as such, for both labels\nÎ¶\nÂ¯\n=\n|\nÎ»\n|\nâˆ’\n1\n|\nÎ»\n|\n=\n2\n3\n\\bar{\\zeta}=\\frac{|\\lambda|-1}{|\\lambda|}=\\frac{2}{3}\n, which\ncorresponds to the observed threshold and consequently supports our intuition\n.\n(a)\nspeed_label\n(b)\nangle_label\n(c)\nspeed_label + angle_label\nFigure 19:\nEvolution of style alignment under noisy labels.\nFor noise labels\nÎ¶\nâˆˆ\n{\n0.0\n,\n0.1\n,\nâ€¦\n,\n1.0\n}\n\\zeta\\in\\{0.0,0.1,...,1.0\\}\n, we compare the evolution of style alignment of\nBC\n(\n),\nCBC\n(\nâˆ™\n\\bullet\n),\nBC-PMI\n(\nâˆ™\n\\bullet\n),\nSCBC\n(\nâˆ™\n\\bullet\n),\nSORL\n(\nâˆ™\n\\bullet\n)\nand\nSCIQL\n(\nâˆ™\n\\bullet\n). We see that SCIQL maintains an overall better alignment before the noise threshold (vertical\n) where the true label is the majority, and then misaligns itself beyond the noise threshold, which corresponds to following intentionally the wrong styles accordingly to the noisy labeling.",
    "preview_text": "We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.\n\n1\n1\nfootnotetext:\nCorrespondence to\nmathieu.petitbois@ubisoft.com\nOffline Reinforcement Learning of\nHigh-Quality Behaviors Under Robust Style Alignment\nMathieu Petitbois\n*,1,2\n,\nRÃ©my Portelas\n1\n,\nSylvain Lamprier\n2\n1\nUbisoft La Forge\n2\nUniversity of Angers\nAbstract\nWe study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offli",
    "is_relevant": true,
    "relevance_score": 7.0,
    "extracted_keywords": [
        "Offline Reinforcement Learning",
        "goal conditioned reinforcement learning",
        "constrained reinforcement learning",
        "safe reinforcement learning"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•SCIQLï¼Œé€šè¿‡é£æ ¼æ¡ä»¶ç­–ç•¥å’Œé—¨æ§ä¼˜åŠ¿åŠ æƒå›å½’æœºåˆ¶ï¼Œåœ¨ç¦»çº¿æ•°æ®ä¸­å­¦ä¹ é«˜è´¨é‡è¡Œä¸ºå¹¶ä¿æŒé£æ ¼å¯¹é½ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šåŸºäºé²æ£’é£æ ¼å¯¹é½çš„é«˜è´¨é‡è¡Œä¸ºç¦»çº¿å¼ºåŒ–å­¦ä¹ \n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nåœ¨åºåˆ—å†³ç­–ä»»åŠ¡ä¸­ï¼Œæ™ºèƒ½ä½“ä¸ä»…éœ€è¦è¿½æ±‚é«˜æ€§èƒ½ï¼Œè¿˜éœ€å±•ç°å‡ºå¤šæ ·åŒ–çš„è¡Œä¸ºé£æ ¼ï¼Œè¿™åœ¨äººæœºäº¤äº’ã€è‡ªåŠ¨é©¾é©¶å’Œè§†é¢‘æ¸¸æˆç­‰é¢†åŸŸè‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–åœ¨çº¿äº¤äº’æˆ–ç¦»çº¿æ•°æ®é›†æ¥å­¦ä¹ é£æ ¼åŒ–ç­–ç•¥ï¼Œä½†é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼š**é£æ ¼å®šä¹‰ä¸ç»Ÿä¸€**ï¼ˆéš¾ä»¥è§£é‡Šã€æ ‡æ³¨æˆæœ¬é«˜ã€å¯¹é½åº¦é‡éš¾ï¼‰ã€**åˆ†å¸ƒåç§»åŠ å‰§**ï¼ˆé£æ ¼æ¡ä»¶åŒ–åŠ å‰§äº†ç¦»çº¿å­¦ä¹ ä¸­çš„çŠ¶æ€-é£æ ¼ä¸åŒ¹é…ï¼‰ä»¥åŠ**ä»»åŠ¡ä¸é£æ ¼ç›®æ ‡å†²çª**ï¼ˆä¼˜åŒ–ä»»åŠ¡æ€§èƒ½å¸¸ä»¥ç‰ºç‰²é£æ ¼å¯¹é½ä¸ºä»£ä»·ï¼‰ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œä»ç¦»çº¿æ•°æ®ä¸­å­¦ä¹ æ—¢èƒ½é«˜åº¦å¯¹é½æŒ‡å®šé£æ ¼ï¼Œåˆèƒ½ä¿æŒé«˜æ€§èƒ½çš„ç­–ç•¥ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡é¦–å…ˆæå‡ºäº†ä¸€ä¸ª**ç»Ÿä¸€çš„é£æ ¼å®šä¹‰æ¡†æ¶**ï¼Œå°†é£æ ¼è§†ä¸ºåŸºäºå­è½¨è¿¹æ ‡æ³¨å‡½æ•°çš„ç›‘ç£ä¿¡å·ï¼Œä»è€Œå…¼é¡¾äº†å¯è§£é‡Šæ€§ã€ä½æ ‡æ³¨æˆæœ¬å’Œæ˜“äºå¯¹é½è¯„ä¼°ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº† **Style-Conditioned Implicit Q-Learning (SCIQL)** ç®—æ³•ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š\n- **é£æ ¼æ¡ä»¶åŒ–ç¦»çº¿RL**ï¼šå€Ÿé‰´ç›®æ ‡æ¡ä»¶åŒ–RLçš„æ€æƒ³ï¼Œåˆ©ç”¨ hindsight é£æ ¼é‡æ ‡æ³¨å’Œè½¨è¿¹æ‹¼æ¥æŠ€æœ¯ï¼Œè§£å†³é£æ ¼ç›‘ç£ç¨€ç–å’Œé•¿æœŸå†³ç­–é—®é¢˜ï¼Œå®ç°é²æ£’é£æ ¼å¯¹é½ã€‚\n- **é—¨æ§ä¼˜åŠ¿åŠ æƒå›å½’ (GAWR)**ï¼šé€šè¿‡ä¸€ä¸ªä»¥é£æ ¼ä¼˜åŠ¿ä¸ºé—¨æ§çš„æœºåˆ¶ï¼Œå°†ä»»åŠ¡ä¼˜åŠ¿ä¿¡å·èåˆåˆ°ç­–ç•¥ä¼˜åŒ–ä¸­ã€‚è¿™ä½¿å¾—ç®—æ³•èƒ½åœ¨ä¼˜å…ˆä¿è¯é£æ ¼å¯¹é½çš„å‰æä¸‹ï¼Œæœ‰æ•ˆæå‡ä»»åŠ¡æ€§èƒ½ï¼Œè¿‘ä¼¼å®ç°äº†åœ¨é£æ ¼æœ€ä¼˜ç­–ç•¥é›†åˆä¸Šçš„æŠ•å½±ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\nåœ¨å¤šä¸ªç¯å¢ƒï¼ˆCircle2D, HalfCheetah, HumEnvï¼‰å’Œå¤šæ ·åŒ–é£æ ¼å‡†åˆ™ä¸Šçš„å®éªŒè¡¨æ˜ï¼š\n- **é£æ ¼å¯¹é½**ï¼šSCIQL åœ¨é£æ ¼å¯¹é½æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼ˆBCã€CBCã€BCPMIã€SORLç­‰ï¼‰ã€‚\n- **ä»»åŠ¡æ€§èƒ½ä¼˜åŒ–**ï¼šé€šè¿‡ GAWR æœºåˆ¶ï¼ŒSCIQL èƒ½åœ¨å‡ ä¹ä¸æŸå®³é£æ ¼å¯¹é½çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæå‡ç­–ç•¥çš„ä»»åŠ¡å›æŠ¥ï¼Œå…¶è·å¾—çš„å¸•ç´¯æ‰˜å‰æ²¿æ˜¾è‘—ä¼˜äºå¯¹æ¯”æ–¹æ³• SORLã€‚\n- **é²æ£’æ€§**ï¼šSCIQL å¯¹æ ‡æ³¨å™ªå£°å±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ï¼Œåœ¨è¾ƒé«˜å™ªå£°æ°´å¹³ä¸‹ä»èƒ½ä¿æŒè‰¯å¥½å¯¹é½ï¼Œè€Œå…¶ä»–æ–¹æ³•æ€§èƒ½ä¼šè¿…é€Ÿé€€åŒ–ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\næœ¬ç ”ç©¶ä¸ºç¦»çº¿é£æ ¼åŒ–ç­–ç•¥å­¦ä¹ æä¾›äº†ä¸€ä¸ªé€šç”¨ã€å¯è§£é‡Šä¸”å®ç”¨çš„æ¡†æ¶ã€‚æ‰€æå‡ºçš„ SCIQL ç®—æ³•é¦–æ¬¡åœ¨ç¦»çº¿è®¾å®šä¸‹ï¼Œæœ‰æ•ˆåè°ƒäº†é£æ ¼å¯¹é½ä¸ä»»åŠ¡æ€§èƒ½ä¼˜åŒ–è¿™ä¸¤ä¸ªå¸¸å†²çªçš„ç›®æ ‡ã€‚å…¶æŠ€æœ¯æ€è·¯ï¼ˆå¦‚å­è½¨è¿¹æ ‡æ³¨ã€é£æ ¼é‡æ ‡æ³¨ã€GAWRï¼‰ä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦å€Ÿé‰´ã€‚è¿™é¡¹å·¥ä½œæ¨åŠ¨äº†å¯æ§ã€å¤šæ ·ä¸”é«˜æ€§èƒ½æ™ºèƒ½ä½“çš„å‘å±•ï¼Œåœ¨éœ€è¦ä¸ªæ€§åŒ–æˆ–é£æ ¼åŒ–è¡Œä¸ºçš„å®é™…åº”ç”¨åœºæ™¯ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **ç ”ç©¶é—®é¢˜**\næœ¬æ–‡æ—¨åœ¨è§£å†³**ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOffline RLï¼‰ä¸­ï¼Œå¦‚ä½•ä»é¢„å…ˆè®°å½•çš„å¤šæ ·åŒ–è¡Œä¸ºæ•°æ®é›†ä¸­ï¼ŒåŒæ—¶å­¦ä¹ å‡ºæ—¢ç¬¦åˆç‰¹å®šè¡Œä¸ºé£æ ¼ï¼ˆStyle Alignmentï¼‰åˆå…·æœ‰é«˜ä»»åŠ¡æ€§èƒ½ï¼ˆTask Performanceï¼‰çš„ç­–ç•¥**ã€‚è¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œå› ä¸ºé£æ ¼å¯¹é½å’Œä»»åŠ¡æ€§èƒ½å¾€å¾€å­˜åœ¨å†…åœ¨å†²çªï¼ˆä¾‹å¦‚ï¼Œçˆ¬è¡Œé£æ ¼æ— æ³•è¾¾åˆ°å¥”è·‘é£æ ¼çš„é€Ÿåº¦ï¼‰ã€‚\n\n### **ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜**\n1.  **é£æ ¼å®šä¹‰ï¼ˆStyle Definitionï¼‰**ï¼šç°æœ‰æ–¹æ³•å¯¹â€œè¡Œä¸ºé£æ ¼â€çš„å®šä¹‰ä¸ç»Ÿä¸€ï¼Œä»ç¦»æ•£è½¨è¿¹æ ‡ç­¾ã€æ— ç›‘ç£èšç±»åˆ°è¿ç»­æ½œåœ¨ç¼–ç ï¼Œå„æœ‰ä¼˜åŠ£ã€‚ç¼ºä¹ä¸€ä¸ª**é€šç”¨ã€å¯è§£é‡Šã€æ˜“äºæ ‡æ³¨å’Œè¯„ä¼°**çš„é£æ ¼å®šä¹‰ã€‚\n2.  **åˆ†å¸ƒåç§»ï¼ˆDistribution Shiftï¼‰**ï¼šç¦»çº¿å­¦ä¹ å›ºæœ‰çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œåœ¨å¼•å…¥é£æ ¼æ¡ä»¶åå˜å¾—æ›´åŠ å¤æ‚ã€‚ç­–ç•¥åœ¨æ¨æ–­æ—¶å¯èƒ½é‡åˆ°æœªè§è¿‡çš„â€œçŠ¶æ€-é£æ ¼â€ç»„åˆï¼Œå¯¼è‡´é£æ ¼å¯¹é½å¤±æ•ˆã€‚\n3.  **ç›®æ ‡å†²çªï¼ˆTask and Style Misalignmentï¼‰**ï¼šç›´æ¥ä¼˜åŒ–é£æ ¼å¥–åŠ±å’Œä»»åŠ¡å¥–åŠ±çš„åŠ æƒå’Œï¼Œé€šå¸¸ä¼šå¯¼è‡´å…¶ä¸­ä¸€ä¸ªç›®æ ‡ï¼ˆå°¤å…¶æ˜¯é£æ ¼å¯¹é½ï¼‰çš„å¦¥åã€‚å¦‚ä½•åœ¨ä¼˜å…ˆä¿è¯é£æ ¼å¯¹é½çš„å‰æä¸‹æå‡ä»»åŠ¡æ€§èƒ½ï¼Œæ˜¯ä¸€ä¸ªå¼€æ”¾é—®é¢˜ã€‚\n\n### **æ ¸å¿ƒåˆ›æ–°ç‚¹ä¸è§£å†³æ–¹æ¡ˆ**\n\n#### **1. ç»Ÿä¸€çš„é£æ ¼å®šä¹‰æ¡†æ¶**\n- **åˆ›æ–°ç‚¹**ï¼šæå‡ºä¸€ä¸ª**é€šç”¨**çš„é£æ ¼å®šä¹‰ï¼Œå°†é£æ ¼åŒ–ç­–ç•¥å­¦ä¹ é—®é¢˜**æ³›åŒ–ä¸ºç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼ˆGCRLï¼‰é—®é¢˜**ã€‚\n- **è§£å†³æ–¹æ¡ˆ**ï¼š\n    - ä½¿ç”¨**æ•°æ®ç¼–ç¨‹ï¼ˆData Programmingï¼‰** èŒƒå¼ï¼Œé€šè¿‡**å­è½¨è¿¹æ ‡æ³¨å‡½æ•°ï¼ˆSubtrajectory Labeling Functionsï¼‰** æ¥æ˜¾å¼åœ°å®šä¹‰é£æ ¼ã€‚\n    - ä¸ä¹‹å‰å¯¹æ•´ä¸ªè½¨è¿¹æ ‡æ³¨ä¸åŒï¼Œæœ¬æ–‡å¯¹**è½¨è¿¹çª—å£ï¼ˆwindowï¼‰** è¿›è¡Œæ ‡æ³¨ã€‚è¿™æ›´ç¬¦åˆé£æ ¼çš„å¤šæ—¶é—´å°ºåº¦ç‰¹æ€§ï¼Œå¹¶**ä»è®¾è®¡ä¸Šç¼“è§£äº†ä¿¡ç”¨åˆ†é…ï¼ˆCredit Assignmentï¼‰é—®é¢˜**ã€‚\n    - æ ‡æ³¨å‡½æ•°ç”±ç”¨æˆ·å®šä¹‰ï¼Œ**æˆæœ¬ä½ã€å¯è§£é‡Šæ€§å¼º**ï¼Œå¹¶ä¸”ä¾¿äºåœ¨æ¨æ–­æ—¶**ç›´æ¥è¯„ä¼°å¯¹é½ç¨‹åº¦**ã€‚\n\n#### **2. æ–°é¢–çš„ç®—æ³•ï¼šSCIQL**\n**Style-Conditioned Implicit Q-Learning (SCIQL)** æ˜¯æœ¬æ–‡çš„æ ¸å¿ƒç®—æ³•åˆ›æ–°ï¼Œå®ƒå»ºç«‹åœ¨ç»å…¸çš„ç¦»çº¿RLç®—æ³•IQLä¹‹ä¸Šï¼Œå¹¶è¿›è¡Œäº†å…³é”®æ‰©å±•ä»¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚\n\n- **è§£å†³åˆ†å¸ƒåç§»ä¸ç¨€ç–ç›‘ç£**ï¼š\n    - **é£æ ¼é‡æ ‡æ³¨ï¼ˆStyle Relabelingï¼‰**ï¼šå—GCRLä¸­ hindsight æŠ€æœ¯çš„å¯å‘ï¼Œåœ¨è®­ç»ƒæ—¶ï¼Œä¸ä»…ä½¿ç”¨å½“å‰çŠ¶æ€-åŠ¨ä½œå¯¹çš„å®é™…é£æ ¼æ ‡ç­¾ï¼Œè¿˜ä¼š**éšæœºé‡‡æ ·æ•°æ®é›†ä¸­å…¶ä»–é£æ ¼æ ‡ç­¾**è¿›è¡Œæ¡ä»¶å­¦ä¹ ã€‚è¿™ä½¿ç­–ç•¥èƒ½å¤Ÿå­¦ä¹ å¦‚ä½•ä¸ºä»»ä½•ç›®æ ‡é£æ ¼ç”Ÿæˆè¡Œä¸ºï¼Œå¢å¼ºäº†é¢å¯¹åˆ†å¸ƒå¤–é£æ ¼æ—¶çš„**é²æ£’æ€§**ã€‚\n    - **è½¨è¿¹ç¼åˆï¼ˆTrajectory Stitchingï¼‰**ï¼šé€šè¿‡ä»·å€¼å‡½æ•°å­¦ä¹ ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿè§„åˆ’é•¿æœŸå†³ç­–ï¼Œå°†æ•°æ®é›†ä¸­ä¸åŒè½¨è¿¹çš„ç‰‡æ®µâ€œç¼åˆâ€èµ·æ¥ï¼Œä»¥å®ç°æ•°æ®é›†ä¸­å¯èƒ½ä¸å­˜åœ¨çš„å®Œæ•´é£æ ¼åºåˆ—ã€‚\n\n- **è§£å†³ç›®æ ‡å†²çªï¼šé—¨æ§ä¼˜åŠ¿åŠ æƒå›å½’ï¼ˆGAWRï¼‰**\n    - **æ ¸å¿ƒåˆ›æ–°æœºåˆ¶**ï¼šè¿™æ˜¯å®ç°â€œåœ¨ä¿æŒé£æ ¼å¯¹é½çš„å‰æä¸‹ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½â€çš„å…³é”®ã€‚\n    - **å·¥ä½œåŸç†**ï¼š\n        1.  åˆ†åˆ«å­¦ä¹ **é£æ ¼ä¼˜åŠ¿å‡½æ•°** `A_Î»`ï¼ˆè¡¡é‡åŠ¨ä½œå¯¹é£æ ¼å¯¹é½çš„è´¡çŒ®ï¼‰å’Œ**ä»»åŠ¡ä¼˜åŠ¿å‡½æ•°** `A_r`ï¼ˆè¡¡é‡åŠ¨ä½œå¯¹ä»»åŠ¡å›æŠ¥çš„è´¡çŒ®ï¼‰ã€‚\n        2.  å®šä¹‰**é—¨æ§ä¼˜åŠ¿**ï¼š`Î¾ = A_Î» + Ïƒ(A_Î») * A_r`ï¼Œå…¶ä¸­ `Ïƒ` æ˜¯Sigmoidå‡½æ•°ã€‚\n        3.  **é—¨æ§é€»è¾‘**ï¼š`Ïƒ(A_Î»)` ä½œä¸ºä¸€ä¸ªè½¯å¼€å…³ã€‚å½“ `A_Î»` å¾ˆå¤§ï¼ˆå½“å‰åŠ¨ä½œéå¸¸æœ‰åˆ©äºé£æ ¼å¯¹é½ï¼‰æ—¶ï¼Œå¼€å…³æ‰“å¼€ï¼Œä»»åŠ¡ä¼˜åŠ¿ `A_r` è¢«çº³å…¥æ›´æ–°ï¼›å½“ `A_Î»` å¾ˆå°æˆ–ä¸ºè´Ÿæ—¶ï¼Œå¼€å…³å…³é—­ï¼Œæ›´æ–°å‡ ä¹åªä¾èµ–äºé£æ ¼ä¼˜åŠ¿ã€‚è¿™ç¡®ä¿äº†**ä»»åŠ¡æ€§èƒ½çš„ä¼˜åŒ–ä¸ä¼šä»¥æŸå®³é£æ ¼å¯¹é½ä¸ºä»£ä»·**ã€‚\n        4.  ä½¿ç”¨è¿™ä¸ªé—¨æ§ä¼˜åŠ¿ `Î¾` è¿›è¡Œä¼˜åŠ¿åŠ æƒå›å½’ï¼ˆAWRï¼‰æ¥æå–æœ€ç»ˆç­–ç•¥ã€‚\n\n### **å®é™…ä»·å€¼ä¸æ€»ç»“**\n- **æŠ€æœ¯ä»·å€¼**ï¼šæä¾›äº†ä¸€ä¸ª**åŸåˆ™æ€§ä¸”å®ç”¨**çš„æ¡†æ¶ï¼Œå°†é£æ ¼åŒ–å­¦ä¹ ç»Ÿä¸€åˆ°GCRLèŒƒå¼ä¸‹ï¼Œå¹¶é€šè¿‡SCIQLç®—æ³•ï¼Œç‰¹åˆ«æ˜¯GAWRæœºåˆ¶ï¼Œé¦–æ¬¡åœ¨ç¦»çº¿è®¾ç½®ä¸­æœ‰æ•ˆåè°ƒäº†é£æ ¼å¯¹é½ä¸ä»»åŠ¡æ€§èƒ½è¿™ä¸¤ä¸ªç»å¸¸å†²çªçš„ç›®æ ‡ã€‚\n- **åº”ç”¨ä»·å€¼**ï¼šåœ¨éœ€è¦**å¤šæ ·åŒ–ã€å¯æ§ä¸”é«˜æ€§èƒ½**æ™ºèƒ½ä½“çš„åœºæ™¯ä¸­å…·æœ‰ç›´æ¥åº”ç”¨æ½œåŠ›ï¼Œä¾‹å¦‚ï¼š\n    - **è§†é¢‘æ¸¸æˆ**ï¼šç”Ÿæˆå…·æœ‰ä¸åŒæˆ˜æ–—é£æ ¼ã€ç§»åŠ¨æ–¹å¼çš„NPCã€‚\n    - **æœºå™¨äººäº¤äº’**ï¼šè®©æœºå™¨äººä»¥ç‰¹å®šé£æ ¼ï¼ˆå¦‚è°¨æ…ã€å¿«é€Ÿï¼‰æ‰§è¡Œä»»åŠ¡ï¼Œæå‡äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œå¯é¢„æµ‹æ€§ã€‚\n    - **è‡ªåŠ¨é©¾é©¶**ï¼šå­¦ä¹ ä¸åŒé©¾é©¶é£æ ¼ï¼ˆå¦‚ä¿å®ˆå‹ã€æ•ˆç‡å‹ï¼‰ä¸‹çš„å®‰å…¨ç­–ç•¥ã€‚\n\n**æ€»ç»“**ï¼šæœ¬æ–‡é€šè¿‡ä¸€ä¸ª**æ–°é¢–çš„ã€åŸºäºå­è½¨è¿¹æ ‡æ³¨çš„é£æ ¼å®šä¹‰**å’Œä¸€ä¸ª**åˆ›æ–°çš„ã€ç»“åˆäº†é‡æ ‡æ³¨ä¸é—¨æ§ä¼˜åŠ¿æœºåˆ¶çš„ç¦»çº¿RLç®—æ³•ï¼ˆSCIQLï¼‰**ï¼Œç³»ç»Ÿåœ°è§£å†³äº†é£æ ¼åŒ–ç¦»çº¿RLä¸­çš„å®šä¹‰ã€é²æ£’æ€§å’Œç›®æ ‡å†²çªä¸‰å¤§æŒ‘æˆ˜ï¼Œåœ¨å¤šä¸ªå®éªŒç¯å¢ƒä¸­å®ç°äº†**æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•çš„é£æ ¼å¯¹é½ä¸ä»»åŠ¡æ€§èƒ½è”åˆä¼˜åŒ–**ã€‚",
            "timestamp": "2026-02-03T15:54:39.435968",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„é£æ ¼å¯¹é½ä¸é«˜æ€§èƒ½è¡Œä¸ºå­¦ä¹ ï¼šå‘å±•è„‰ç»œæ¢³ç†\n\næœ¬æ–‡ã€ŠOffline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignmentã€‹æ˜¯ä½œè€…Mathieu PetitboisåŠå…¶å›¢é˜Ÿåœ¨â€œé£æ ¼åŒ–ç­–ç•¥å­¦ä¹ â€æ–¹å‘ä¸Šçš„æœ€æ–°å·¥ä½œã€‚è¦ç†è§£å…¶è´¡çŒ®ï¼Œéœ€è¦æ¢³ç†è¯¥æ–¹å‘ä»æ¨¡ä»¿å­¦ä¹ åˆ°ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå†åˆ°ç»“åˆé£æ ¼çº¦æŸçš„æ•´ä¸ªå‘å±•è„‰ç»œã€‚å…¶æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå¦‚ä½•ä»å¼‚æ„çš„ç¦»çº¿æ•°æ®ä¸­ï¼Œå­¦ä¹ å‡ºæ—¢èƒ½**ç²¾ç¡®å¯¹é½ç‰¹å®šè¡Œä¸ºé£æ ¼**ï¼Œåˆèƒ½**ä¿æŒé«˜ä»»åŠ¡æ€§èƒ½**çš„ç­–ç•¥ã€‚\n\nä»¥ä¸‹å°†æ²¿ç€å‡ ä¸ªå…³é”®çš„å‘å±•æ–¹å‘ï¼Œæ¢³ç†å…¶æ¼”è¿›è¿‡ç¨‹ï¼Œå¹¶é‡ç‚¹åˆ†ææ¯ä¸€æ­¥å·¥ä½œç›¸æ¯”ä¹‹å‰çš„æ”¹è¿›ä¹‹å¤„ã€‚\n\n### æ–¹å‘ä¸€ï¼šä»æ¨¡ä»¿ä¸“å®¶åˆ°å­¦ä¹ å¤šæ ·è¡Œä¸ºï¼ˆä»åŒè´¨åˆ°å¼‚è´¨æ•°æ®ï¼‰\n\næ—©æœŸçš„æ¨¡ä»¿å­¦ä¹ å‡è®¾æ¼”ç¤ºæ•°æ®æ¥è‡ªå•ä¸€ã€æœ€ä¼˜çš„ä¸“å®¶ã€‚\n\n1.  **è¡Œä¸ºå…‹éš†ä¸é€†å¼ºåŒ–å­¦ä¹ **ï¼š\n    *   **ä»£è¡¨æ€§å·¥ä½œ**ï¼šBC, GAIL, IRLã€‚\n    *   **æ ¸å¿ƒæ€æƒ³**ï¼šä»ï¼ˆå‡è®¾ï¼‰åŒè´¨çš„ä¸“å®¶è½¨è¿¹ä¸­å­¦ä¹ ï¼Œç›®æ ‡æ˜¯å¤ç°ä¸“å®¶çš„è¡Œä¸ºã€‚\n    *   **å±€é™æ€§**ï¼šæ— æ³•å¤„ç†æ•°æ®é›†ä¸­å­˜åœ¨çš„**è¡Œä¸ºå¼‚è´¨æ€§**ã€‚å¦‚æœæ•°æ®åŒ…å«å¤šç§å®Œæˆä»»åŠ¡çš„é£æ ¼ï¼ˆå¦‚è·‘æ­¥ã€çˆ¬è¡Œï¼‰ï¼Œæ ‡å‡†ILæ–¹æ³•ä¼šå­¦ä¹ ä¸€ä¸ªâ€œå¹³å‡â€ç­–ç•¥ï¼Œå¯¼è‡´é£æ ¼æ¨¡ç³Šæˆ–å†²çªã€‚\n\n2.  **å¤šæ ·åŒ–æ¨¡ä»¿å­¦ä¹ **ï¼š\n    *   **ä»£è¡¨æ€§å·¥ä½œ**ï¼šInfoGAIL, DiverseGAIL, CTVAEã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼š**æ‰¿è®¤å¹¶å»ºæ¨¡æ•°æ®çš„å¤šæ ·æ€§**ã€‚è¿™äº›æ–¹æ³•é€šè¿‡å¼•å…¥éšå˜é‡æˆ–è¾…åŠ©ä¿¡æ¯ï¼Œä»æ•°æ®ä¸­**æ— ç›‘ç£åœ°**å‘ç°æˆ–**æœ‰ç›‘ç£åœ°**åŒºåˆ†ä¸åŒçš„è¡Œä¸ºæ¨¡å¼ï¼ˆé£æ ¼ï¼‰ã€‚\n    *   **CTVAEçš„è´¡çŒ®**ï¼šåœ¨VAEæ¡†æ¶ä¸­å¼•å…¥è½¨è¿¹é£æ ¼æ ‡ç­¾ï¼Œè¿›è¡Œæœ‰ç›‘ç£çš„é£æ ¼æ¡ä»¶æ¨¡ä»¿å­¦ä¹ ã€‚\n    *   **é—ç•™é—®é¢˜**ï¼š\n        *   **å®šä¹‰æ¨¡ç³Š**ï¼šæ— ç›‘ç£æ–¹æ³•å­¦åˆ°çš„é£æ ¼éš¾ä»¥è§£é‡Šå’Œæ§åˆ¶ã€‚\n        *   **å¯¹é½å›°éš¾**ï¼šæœ‰ç›‘ç£æ–¹æ³•ä¾èµ–äºæ•´æ¡è½¨è¿¹çš„æ ‡ç­¾ï¼Œå­˜åœ¨**ä¿¡ç”¨åˆ†é…**é—®é¢˜ï¼ˆéš¾ä»¥ç¡®å®šè½¨è¿¹ä¸­æ¯ä¸ªåŠ¨ä½œå¯¹æœ€ç»ˆé£æ ¼çš„è´¡çŒ®ï¼‰ã€‚\n        *   **æ€§èƒ½å¦¥å**ï¼šä¸“æ³¨äºæ¨¡ä»¿é£æ ¼ï¼Œå¯èƒ½ç‰ºç‰²ä»»åŠ¡çš„æœ€ä¼˜æ€§èƒ½ã€‚\n\n### æ–¹å‘äºŒï¼šä»åœ¨çº¿å­¦ä¹ åˆ°å®Œå…¨ç¦»çº¿å­¦ä¹ ï¼ˆä»äº¤äº’åˆ°çº¯æ•°æ®é©±åŠ¨ï¼‰\n\nåœ¨çº¿æˆ–äº¤äº’å¼æ–¹æ³•ï¼ˆå¦‚åœ¨çº¿RLã€å¯¹æŠ—å¼ILï¼‰éœ€è¦ä¸ç¯å¢ƒäº¤äº’ï¼Œæˆæœ¬é«˜ä¸”ä¸å®‰å…¨ã€‚\n\n1.  **ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„å…´èµ·**ï¼š\n    *   **ä»£è¡¨æ€§å·¥ä½œ**ï¼šBCQ, CQL, IQLã€‚\n    *   **æ ¸å¿ƒæ€æƒ³**ï¼šç›´æ¥ä»é™æ€æ•°æ®é›†ä¸­å­¦ä¹ ç­–ç•¥ï¼Œæ— éœ€è¿›ä¸€æ­¥äº¤äº’ã€‚æ ¸å¿ƒæŒ‘æˆ˜æ˜¯å…‹æœ**åˆ†å¸ƒåç§»**ï¼Œé¿å…å¯¹OODåŠ¨ä½œçš„é«˜ä¼°ã€‚\n    *   **IQLçš„è´¡çŒ®**ï¼šé€šè¿‡æœŸæœ›åˆ†ä½æ•°å›å½’å­¦ä¹ ä»·å€¼å‡½æ•°ï¼Œæœ‰æ•ˆç¼“è§£ä»·å€¼é«˜ä¼°ï¼Œå†é€šè¿‡ä¼˜åŠ¿åŠ æƒå›å½’æå–ç­–ç•¥ï¼Œåœ¨ç¦»çº¿è®¾å®šä¸‹å®ç°äº†ç¨³å®šçš„ç­–ç•¥æå‡ã€‚\n    *   **å±€é™æ€§**ï¼šæ ‡å‡†ç¦»çº¿RLç›®æ ‡å•ä¸€ï¼ˆæœ€å¤§åŒ–å›æŠ¥ï¼‰ï¼Œä¸å…³å¿ƒè¡Œä¸ºçš„é£æ ¼å¤šæ ·æ€§ã€‚\n\n2.  **ç¦»çº¿å¤šæ ·åŒ–ç­–ç•¥å­¦ä¹ **ï¼š\n    *   **ä»£è¡¨æ€§å·¥ä½œ**ï¼šSORLã€‚\n    *   **æ ¸å¿ƒæ”¹è¿›**ï¼š**å°†å¤šæ ·åŒ–å­¦ä¹ å¼•å…¥ç¦»çº¿RLæ¡†æ¶**ã€‚SORLé¦–å…ˆé€šè¿‡EMç®—æ³•ä»æ•°æ®ä¸­æ— ç›‘ç£åœ°å‘ç°ä¸€ç»„ä»£è¡¨æ€§ç­–ç•¥ï¼ˆé£æ ¼ï¼‰ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªçº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œåœ¨ä¸è¿‡åº¦åç¦»æ¯ä¸ªåŸºç¡€é£æ ¼çš„å‰æä¸‹ï¼Œæå‡å…¶ä»»åŠ¡æ€§èƒ½ã€‚\n    *   **é—ç•™é—®é¢˜**ï¼š\n        *   **é£æ ¼å®šä¹‰ä¸å¯æ§**ï¼šæ— ç›‘ç£èšç±»å¾—åˆ°çš„é£æ ¼éš¾ä»¥è§£é‡Šå’ŒæŒ‡å®šã€‚\n        *   **å¯¹é½-æ€§èƒ½æƒè¡¡ç”Ÿç¡¬**ï¼šé€šè¿‡æ¸©åº¦å‚æ•° `Î²` è°ƒèŠ‚é£æ ¼çº¦æŸå¼ºåº¦ï¼Œä½†**ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½æ—¶å¾€å¾€ä¼šæŸå®³é£æ ¼å¯¹é½**ï¼ˆè§å›¾2ï¼‰ï¼Œä¸”æœ€ä¼˜ `Î²` éš¾ä»¥è°ƒèŠ‚ã€‚\n        *   **ç¼ºä¹é•¿æœŸè§„åˆ’**ï¼šå¯¹éœ€è¦è·¨æ—¶åºç»„åˆå­é£æ ¼ï¼ˆå¦‚ä»â€œæ…¢â€åŠ é€Ÿåˆ°â€œå¿«â€ï¼‰çš„ä»»åŠ¡æ”¯æŒä¸è¶³ã€‚\n\n### æ–¹å‘ä¸‰ï¼šé£æ ¼å®šä¹‰çš„ç»Ÿä¸€ä¸å®ç”¨åŒ–ï¼ˆä»éšå¼åˆ°æ˜¾å¼ï¼Œä»æ•´æ®µåˆ°å­æ®µï¼‰\n\nå¦‚ä½•å®šä¹‰ã€æ ‡æ³¨å’Œè¡¡é‡â€œé£æ ¼â€ä¸€ç›´æ˜¯å…³é”®ç“¶é¢ˆã€‚\n\n1.  **å‰åºå·¥ä½œ**ï¼šZhan et al. (2020), Yang et al. (2024)ã€‚\n    *   **è´¡çŒ®**ï¼šå¼•å…¥äº†**æ•°æ®ç¼–ç¨‹**å’Œ**æ ‡æ³¨å‡½æ•°**çš„æ¦‚å¿µï¼Œå…è®¸ç”¨æˆ·é€šè¿‡ç¼–ç¨‹æ–¹å¼å®šä¹‰æœ‰æ„ä¹‰çš„ã€å¯è§£é‡Šçš„é£æ ¼æ ‡ç­¾ï¼ˆå¦‚â€œé€Ÿåº¦å¿«â€ã€â€œèº¯å¹²ä½â€ï¼‰ï¼Œå¤§å¤§é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚\n    *   **å±€é™æ€§**ï¼š**åœ¨æ•´æ¡è½¨è¿¹çº§åˆ«è¿›è¡Œæ ‡æ³¨**ã€‚è¿™å¿½ç•¥äº†é£æ ¼å¯èƒ½åœ¨è½¨è¿¹å†…å˜åŒ–çš„äº‹å®ï¼ˆä¾‹å¦‚ï¼Œå…ˆèµ°å†è·‘ï¼‰ï¼Œå¯¼è‡´ä¸¥é‡çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚\n\n2.  **æœ¬æ–‡çš„æ”¹è¿›ï¼ˆæ ¸å¿ƒè´¡çŒ®ä¹‹ä¸€ï¼‰**ï¼š\n    *   **ç»Ÿä¸€çš„å½¢å¼åŒ–**ï¼šå°†é£æ ¼åŒ–ç­–ç•¥å­¦ä¹ é—®é¢˜**é‡æ–°å®šä¹‰ä¸ºç›®æ ‡æ¡ä»¶RLçš„æ³›åŒ–**ã€‚é£æ ¼è¢«è§†ä½œåœ¨ç»™å®šä»»åŠ¡ä¸‹ï¼Œå¯¹**å­è½¨è¿¹**çš„æŸç§æ ‡æ³¨ã€‚\n    *   **å­è½¨è¿¹æ ‡æ³¨**ï¼šå…³é”®åˆ›æ–°åœ¨äºå°†æ ‡æ³¨å‡½æ•°åº”ç”¨äº**æ»‘åŠ¨çª—å£å†…çš„å­è½¨è¿¹**ï¼Œè€Œéæ•´æ¡è½¨è¿¹ã€‚è¿™æ›´ç¬¦åˆé£æ ¼çš„å¤šæ—¶é—´å°ºåº¦ç‰¹æ€§ï¼Œ**ä»æ ¹æœ¬ä¸Šç¼“è§£äº†ä¿¡ç”¨åˆ†é…é—®é¢˜**ã€‚\n    *   **ä¼˜åŠ¿**ï¼š\n        *   **å¯è§£é‡Šæ€§**ï¼šç”¨æˆ·å®šä¹‰çš„æ ‡æ³¨å‡½æ•°ã€‚\n        *   **ä½æˆæœ¬**ï¼šè‡ªåŠ¨æ ‡æ³¨ã€‚\n        *   **æ˜“è¡¡é‡**ï¼šå¯¹é½åº¦å¯ä»¥ç›´è§‚è®¡ç®—ï¼ˆå­è½¨è¿¹æ ‡ç­¾åŒ¹é…æ¯”ä¾‹ï¼‰ã€‚\n        *   **çµæ´»å¯æ§**ï¼šé£æ ¼å¯ä»¥åœ¨ä¸€ä¸ªepisodeå†…å˜åŒ–ã€‚\n\n### æ–¹å‘å››ï¼šå®ç°é²æ£’çš„é£æ ¼å¯¹é½ä¸æ€§èƒ½æå‡ï¼ˆä»å‰²è£‚åˆ°ç»Ÿä¸€ä¼˜åŒ–ï¼‰\n\nå¦‚ä½•åŒæ—¶ä¿è¯å¼ºé£æ ¼å¯¹é½å’Œé«˜ä»»åŠ¡æ€§èƒ½ï¼Œå¹¶å¤„ç†åˆ†å¸ƒåç§»ä¸ç¨€ç–ç›‘ç£ï¼Œæ˜¯æœ€ç»ˆæŒ‘æˆ˜ã€‚\n\n1.  **å‰åºå·¥ä½œ**ï¼šSORL, WZBCã€‚\n    *   **SORL**ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µæ–¹å¼ï¼Œå…ˆå­¦é£æ ¼ï¼Œå†çº¦æŸä¼˜åŒ–ã€‚ä½†å¦‚å‰è¿°ï¼Œæ€§èƒ½æå‡ä¼šæŸå®³å¯¹é½ã€‚\n    *   **WZBC**ï¼šé€šè¿‡è½¨è¿¹ç›¸ä¼¼æ€§åŠ æƒå›å½’æé«˜æ¨¡ä»¿é²æ£’æ€§ï¼Œä½†ä»æ˜¯æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œæ— æ³•è¿›è¡Œè¶…è¶Šæ•°æ®çš„ç­–ç•¥æå‡ã€‚\n    *   **å…±åŒå±€é™**ï¼šç¼ºä¹æœ‰æ•ˆçš„æœºåˆ¶æ¥å¤„ç†**é£æ ¼æ¡ä»¶åˆ†å¸ƒä¸‹çš„OODé—®é¢˜**å’Œ**é•¿æœŸå†³ç­–/è½¨è¿¹ç¼åˆ**ã€‚\n\n2.  **æœ¬æ–‡çš„æ”¹è¿›ï¼ˆæ ¸å¿ƒè´¡çŒ®ä¹‹äºŒï¼‰**ï¼š\n    *   **SCIQLç®—æ³•**ï¼š\n        *   **åŸºç¡€**ï¼šå€Ÿé‰´IQLçš„ç¦»çº¿ä»·å€¼å­¦ä¹ æ¡†æ¶ï¼Œä½†å°†å…¶æ‰©å±•ä¸º**é£æ ¼æ¡ä»¶**çš„ä»·å€¼å‡½æ•°ã€‚\n        *   **å…³é”®æœºåˆ¶1ï¼šé£æ ¼é‡æ ‡æ³¨**ï¼šåœ¨è®­ç»ƒæ—¶ï¼Œä¸ä»…ä½¿ç”¨å½“å‰çŠ¶æ€-åŠ¨ä½œå¯¹çš„çœŸå®é£æ ¼æ ‡ç­¾ï¼Œè¿˜ä¼š**ä»æ•°æ®é›†ä¸­éšæœºé‡‡æ ·å…¶ä»–é£æ ¼æ ‡ç­¾**è¿›è¡Œå­¦ä¹ ã€‚è¿™ä½¿ç­–ç•¥èƒ½å¤Ÿæ³›åŒ–åˆ°æœªè§è¿‡çš„`(çŠ¶æ€, é£æ ¼)`ç»„åˆï¼Œå¢å¼ºäº†**åˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›**å’Œ**é²æ£’æ€§**ã€‚\n        *   **å…³é”®æœºåˆ¶2ï¼šè½¨è¿¹ç¼åˆ**ï¼šé€šè¿‡ä¼˜åŒ–é£æ ¼æ¡ä»¶çš„ä»·å€¼å‡½æ•°ï¼Œç­–ç•¥å­¦ä¼šäº†ä¸ºè¾¾åˆ°ç›®æ ‡é£æ ¼è€Œè¿›è¡Œé•¿æœŸè§„åˆ’ï¼Œèƒ½å¤Ÿå°†ä¸åŒè½¨è¿¹ç‰‡æ®µä¸­çš„è¡Œä¸ºâ€œç¼åˆâ€èµ·æ¥ã€‚\n    *   **Gated Advantage Weighted Regression**ï¼š\n        *   **ç›®æ ‡**ï¼šåœ¨**ä¸æŸå®³å·²æœ‰é£æ ¼å¯¹é½**çš„å‰æä¸‹æå‡ä»»åŠ¡æ€§èƒ½ã€‚\n        *   **åˆ›æ–°**ï¼šä¸æ˜¯ç®€å•åœ°å°†é£æ ¼ä¼˜åŠ¿ `A^Î»` å’Œä»»åŠ¡ä¼˜åŠ¿ `A^r` ç›¸åŠ ï¼Œè€Œæ˜¯ç”¨**é—¨æ§æœºåˆ¶**è¿›è¡Œè°ƒåˆ¶ï¼š`Î¾ = A^Î» + Ïƒ(A^Î») * A^r`ã€‚\n        *   **åŸç†**ï¼šSigmoidé—¨ `Ïƒ(A^Î»)` ä»…åœ¨å½“å‰åŠ¨ä½œå¯¹é£æ ¼å¯¹é½æœ‰ç›Šï¼ˆ`A^Î»` ä¸ºæ­£ï¼‰æ—¶ï¼Œæ‰å…è®¸ä»»åŠ¡ä¼˜åŠ¿ä¿¡å·é€šè¿‡ã€‚è¿™è¿‘ä¼¼å®ç°äº†**åœ¨é£æ ¼æœ€ä¼˜ç­–ç•¥é›†åˆå†…è¿›è¡Œä»»åŠ¡æ€§èƒ½ä¼˜åŒ–**çš„ç›®æ ‡ï¼Œä»è€Œåœ¨æå‡å›æŠ¥çš„åŒæ—¶ï¼Œæœ€å¤§ç¨‹åº¦åœ°ä¿æŒäº†é£æ ¼å¯¹é½ã€‚\n\n### æ€»ç»“ï¼šå‘å±•è„‰ç»œä¸æœ¬æ–‡å®šä½\n\n| å‘å±•é˜¶æ®µ | æ ¸å¿ƒç›®æ ‡ | å…³é”®æ–¹æ³•/ä»£è¡¨å·¥ä½œ | ç›¸æ¯”å‰ä½œçš„æ”¹è¿› | é—ç•™é—®é¢˜ |\n| :--- | :--- | :--- | :--- | :--- |\n| **1. åŒè´¨æ¨¡ä»¿** | å¤åˆ¶å•ä¸€ä¸“å®¶ | BC, GAIL | - | æ— æ³•å¤„ç†è¡Œä¸ºå¤šæ ·æ€§ |\n| **2. å¤šæ ·æ¨¡ä»¿** | ä»å¼‚è´¨æ•°æ®ä¸­å­¦ä¹ å¤šç§è¡Œä¸º | InfoGAIL, CTVAE | å»ºæ¨¡è¡Œä¸ºå¤šæ ·æ€§ | é£æ ¼å®šä¹‰æ¨¡ç³Š/ä¸å¯æ§ï¼›ä¿¡ç”¨åˆ†é…éš¾ï¼›ç‰ºç‰²æ€§èƒ½ |\n| **3. ç¦»çº¿RL** | ä»æ•°æ®ä¸­å­¦ä¹ é«˜æ€§èƒ½ç­–ç•¥ | CQL, IQL | æ— éœ€äº¤äº’ï¼Œå®‰å…¨é«˜æ•ˆ | ä¸å…³å¿ƒè¡Œä¸ºé£æ ¼ |\n| **4. ç¦»çº¿å¤šæ ·RL** | ç¦»çº¿å­¦ä¹ å¤šæ ·ä¸”é«˜æ€§èƒ½çš„ç­–ç•¥ | SORL | ç»“åˆå¤šæ ·åŒ–ä¸ç¦»çº¿RL | é£æ ¼ä¸å¯æ§ï¼›å¯¹é½-æ€§èƒ½æƒè¡¡ç”Ÿç¡¬ï¼›ç¼ºä¹é•¿æœŸè§„åˆ’ |\n| **5. é£æ ¼åŒ–ç¦»çº¿RL** | **å¯æ§çš„**é£æ ¼å¯¹é½ä¸æ€§èƒ½ä¼˜åŒ– | **æœ¬æ–‡ (SCIQL)** | **1. ç»Ÿä¸€ä¸”å®ç”¨çš„é£æ ¼å®šä¹‰**ï¼ˆå­è½¨è¿¹æ ‡æ³¨ï¼‰<br>**2. é²æ£’çš„é£æ ¼å¯¹é½**ï¼ˆé£æ ¼é‡æ ‡æ³¨+ä»·å€¼å­¦ä¹ ï¼‰<br>**3. è§£è€¦çš„å¯¹é½-æ€§èƒ½ä¼˜åŒ–**ï¼ˆé—¨æ§ä¼˜åŠ¿åŠ æƒå›å½’ï¼‰ | æœªæ¥æ–¹å‘ï¼šæ‰©å±•åˆ°å¤šå‡†åˆ™ã€é›¶æ ·æœ¬é£æ ¼ç”Ÿæˆç­‰ |\n\n**ç»“è®º**ï¼šæœ¬æ–‡ä»£è¡¨äº†é£æ ¼åŒ–ç¦»çº¿RLé¢†åŸŸçš„ä¸€ä¸ªé‡è¦è¿›å±•ã€‚å®ƒé€šè¿‡**å­è½¨è¿¹æ ‡æ³¨**è§£å†³äº†é£æ ¼å®šä¹‰ä¸ä¿¡ç”¨åˆ†é…çš„æ ¹æœ¬é—®é¢˜ï¼Œé€šè¿‡**SCIQLæ¡†æ¶**å®ç°äº†é²æ£’çš„é£æ ¼å¯¹é½å’Œé•¿æœŸè§„åˆ’ï¼Œå¹¶é€šè¿‡**GAWRæœºåˆ¶**ä¼˜é›…åœ°è§£å†³äº†é£æ ¼å¯¹é½ä¸ä»»åŠ¡æ€§èƒ½ä¹‹é—´çš„å†²çªã€‚å®éªŒè¡¨æ˜ï¼Œå…¶åœ¨é£æ ¼å¯¹é½",
            "timestamp": "2026-02-03T15:55:52.155565",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œä½œè€… Mathieu Petitbois åŠå…¶åˆä½œè€…åœ¨â€œé£æ ¼åŒ–ç­–ç•¥å­¦ä¹ â€é¢†åŸŸæœ‰ä¸€ç³»åˆ—ç›¸å…³çš„å‰æœŸå·¥ä½œã€‚ä»¥ä¸‹è¡¨æ ¼è¯¦ç»†æ¢³ç†äº†æ¯ç¯‡å‰ä½œçš„æ ¸å¿ƒå†…å®¹ï¼Œå¹¶é‡ç‚¹åˆ†æäº†æœ¬æ–‡ï¼ˆSCIQLï¼‰ä¸è¿™äº›å‰ä½œçš„ä¸»è¦åŒºåˆ«å’Œæ”¹è¿›ã€‚\n\n| å‰ä½œæ–‡çŒ® | æ ¸å¿ƒæ–¹æ³•ä¸ç›®æ ‡ | ä¸æœ¬æ–‡ï¼ˆSCIQLï¼‰çš„ä¸»è¦åŒºåˆ« | æœ¬æ–‡ï¼ˆSCIQLï¼‰çš„ä¸»è¦æ”¹è¿›ä¸åˆ›æ–° |\n| :--- | :--- | :--- | :--- |\n| **Zhan et al. (2020) - CTVAE** | **æ–¹æ³•**ï¼š åŸºäºè½¨è¿¹å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„ç›‘ç£å¼æ¨¡ä»¿å­¦ä¹ ã€‚ä½¿ç”¨**è½¨è¿¹çº§**é£æ ¼æ ‡ç­¾è¿›è¡Œæ ¡å‡†ã€‚<br>**ç›®æ ‡**ï¼š ä»å¸¦æœ‰é£æ ¼æ ‡ç­¾çš„æ•°æ®é›†ä¸­å­¦ä¹ èƒ½å¤Ÿæ¨¡ä»¿ç‰¹å®šé£æ ¼çš„æ”¿ç­–ã€‚ | 1. **é£æ ¼å®šä¹‰ç²’åº¦**ï¼š CTVAE ä½¿ç”¨**æ•´æ¡è½¨è¿¹**çš„å•ä¸€æ ‡ç­¾ï¼Œè€Œ SCIQL ä½¿ç”¨**å­è½¨è¿¹çª—å£**çš„æ ‡ç­¾ï¼Œèƒ½æ•æ‰é£æ ¼åœ¨æ—¶é—´ä¸Šçš„å˜åŒ–ã€‚<br>2. **å­¦ä¹ èŒƒå¼**ï¼š CTVAE æ˜¯çº¯ç²¹çš„æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰ï¼Œè€Œ SCIQL æ˜¯**ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**ï¼Œèƒ½åˆ©ç”¨å¥–åŠ±ä¿¡å·ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½ã€‚<br>3. **å¤„ç†åˆ†å¸ƒåç§»**ï¼š CTVAE ç¼ºä¹æ˜ç¡®çš„æœºåˆ¶æ¥å¤„ç†æ¨ç†æ—¶çš„çŠ¶æ€-é£æ ¼åˆ†å¸ƒä¸åŒ¹é…ï¼Œè€Œ SCIQL é€šè¿‡é£æ ¼é‡æ ‡æ³¨å’Œè½¨è¿¹ç¼åˆæ¥è§£å†³ã€‚ | 1. **ç»†ç²’åº¦ä¸å¤šæ—¶é—´å°ºåº¦é£æ ¼å»ºæ¨¡**ï¼š é€šè¿‡å­è½¨è¿¹æ ‡æ³¨ï¼Œè§£å†³äº†è½¨è¿¹å†…é£æ ¼å˜åŒ–çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œä½¿é£æ ¼å®šä¹‰æ›´çµæ´»ã€æ›´ç¬¦åˆå®é™…ã€‚<br>2. **å¼•å…¥RLä¿¡å·å®ç°æ€§èƒ½ä¼˜åŒ–**ï¼š ä¸ä»…æ¨¡ä»¿é£æ ¼ï¼Œè¿˜èƒ½åœ¨ä¿æŒé£æ ¼å¯¹é½çš„åŒæ—¶ï¼Œåˆ©ç”¨ä»»åŠ¡å¥–åŠ±ä¼˜åŒ–ç­–ç•¥çš„æ€§èƒ½ï¼Œè§£å†³äº†é£æ ¼ä¸ä»»åŠ¡å¯èƒ½å†²çªçš„éš¾é¢˜ã€‚<br>3. **å¢å¼ºé²æ£’æ€§**ï¼š é€šè¿‡ç»“åˆ hindsight é£æ ¼é‡æ ‡æ³¨å’ŒåŸºäºä»·å€¼çš„ç­–ç•¥æå–ï¼Œä½¿ç­–ç•¥å¯¹åˆ†å¸ƒåç§»å’Œæœªè§è¿‡çš„çŠ¶æ€-é£æ ¼ç»„åˆæ›´å…·é²æ£’æ€§ã€‚ |\n| **Yang et al. (2024) - BCPMI** | **æ–¹æ³•**ï¼š ç›‘ç£å¼è¡Œä¸ºå…‹éš†çš„æ‰©å±•ã€‚é€šè¿‡ç¥ç»äº’ä¿¡æ¯ä¼°è®¡ï¼ˆMINEï¼‰è®¡ç®—çŠ¶æ€-åŠ¨ä½œå¯¹ä¸é£æ ¼æ ‡ç­¾çš„äº’ä¿¡æ¯ï¼Œå¹¶ä»¥æ­¤åŠ æƒå…‹éš†æŸå¤±ã€‚<br>**ç›®æ ‡**ï¼š æ”¹å–„é£æ ¼ä¸å±€éƒ¨å†³ç­–ä¹‹é—´çš„ä¿¡ç”¨åˆ†é…ï¼Œä»¥æ›´å¥½åœ°å¯¹é½é£æ ¼ã€‚ | 1. **ä¿¡ç”¨åˆ†é…æœºåˆ¶**ï¼š BCPMI é€šè¿‡äº’ä¿¡æ¯åŠ æƒæ¥å…³è”å•æ­¥å†³ç­–ä¸é£æ ¼ï¼Œä½†ä»æ˜¯**å•æ­¥ã€é™æ€**çš„å…³è”ã€‚SCIQL é€šè¿‡**å­¦ä¹ é£æ ¼æ¡ä»¶ä»·å€¼å‡½æ•°**ï¼Œå®ç°äº†**é•¿æœŸã€åºåˆ—åŒ–**çš„ä¿¡ç”¨åˆ†é…ã€‚<br>2. **ç¼ºä¹è§„åˆ’èƒ½åŠ›**ï¼š BCPMI æ˜¯æ¨¡ä»¿å­¦ä¹ ï¼Œæ— æ³•â€œæ‹¼æ¥â€ä¸åŒè½¨è¿¹çš„ç‰‡æ®µæ¥åˆæˆæ–°çš„ã€æ•°æ®é›†ä¸­ä¸å®Œæ•´çš„é£æ ¼è¡Œä¸ºã€‚SCIQL çš„RLæ¡†æ¶å…·å¤‡è¿™ç§è½¨è¿¹ç¼åˆèƒ½åŠ›ã€‚<br>3. **æ— ä»»åŠ¡æ€§èƒ½ä¼˜åŒ–**ï¼š BCPMI ä»…å…³æ³¨é£æ ¼å¯¹é½ï¼Œä¸ä¼˜åŒ–åŸå§‹ä»»åŠ¡å¥–åŠ±ã€‚ | 1. **åŸºäºä»·å€¼çš„é•¿æœŸè§„åˆ’**ï¼š é€šè¿‡è®­ç»ƒé£æ ¼æ¡ä»¶Qå‡½æ•°ï¼Œç­–ç•¥èƒ½å¤Ÿä¸ºäº†åœ¨æœªæ¥è¾¾æˆç›®æ ‡é£æ ¼è€Œåšå‡ºå½“å‰å†³ç­–ï¼Œå®ç°äº†çœŸæ­£çš„åºåˆ—å†³ç­–å’Œè½¨è¿¹ç¼åˆã€‚<br>2. **ç»Ÿä¸€æ¡†æ¶è§£å†³åŒé‡ç›®æ ‡**ï¼š æå‡ºäº† **Gated Advantage Weighted Regression (GAWR)** æœºåˆ¶ï¼Œåœ¨ä¸€ä¸ªæ¡†æ¶å†…åŒæ—¶ä¼˜åŒ–é£æ ¼å¯¹é½å’Œä»»åŠ¡æ€§èƒ½ï¼Œè€ŒBCPMIä»…å¤„ç†å‰è€…ã€‚<br>3. **æ›´ä¼˜çš„åˆ†å¸ƒå¤–æ³›åŒ–**ï¼š åœ¨è®­ç»ƒæ—¶å¯¹é£æ ¼æ ‡ç­¾è¿›è¡Œéšæœºé‡é‡‡æ ·ï¼Œä½¿ç­–ç•¥èƒ½åº”å¯¹æ¨ç†æ—¶ä¸è®­ç»ƒæ•°æ®ä¸åŒçš„é£æ ¼æ¡ä»¶ï¼Œæå‡äº†å¯æ§æ€§å’Œé²æ£’æ€§ã€‚ |\n| **Mao et al. (2024) - SORL** | **æ–¹æ³•**ï¼š **æ— ç›‘ç£**çš„å¤šæ ·åŒ–ç­–ç•¥å­¦ä¹ ã€‚å…ˆç”¨EMç®—æ³•ä»æ•°æ®é›†ä¸­èšç±»å‡ºå¤šä¸ªä»£è¡¨æ€§ç­–ç•¥ï¼Œç„¶åé€šè¿‡çº¦æŸä¼˜åŒ–ï¼ˆStylized AWRï¼‰åœ¨ä¿æŒä¸å„èšç±»ç­–ç•¥ç›¸ä¼¼æ€§çš„å‰æä¸‹æå‡å…¶ä»»åŠ¡æ€§èƒ½ã€‚<br>**ç›®æ ‡**ï¼š ä»å¼‚æ„æ•°æ®é›†ä¸­æå–å¤šæ ·åŒ–çš„é«˜æ€§èƒ½ç­–ç•¥ã€‚ | 1. **é£æ ¼å®šä¹‰æ–¹å¼**ï¼š SORL æ˜¯**æ— ç›‘ç£**çš„ï¼Œé£æ ¼æ˜¯è‡ªåŠ¨å‘ç°çš„ã€éš¾ä»¥è§£é‡Šçš„èšç±»ã€‚SCIQL æ˜¯**æœ‰ç›‘ç£**çš„ï¼Œä½¿ç”¨ç”¨æˆ·å®šä¹‰çš„ã€å¯è§£é‡Šçš„æ ‡ç­¾å‡½æ•°ã€‚<br>2. **å¯¹é½ç›®æ ‡**ï¼š SORL çº¦æŸç­–ç•¥æ¥è¿‘æŸä¸ªèšç±»ç­–ç•¥ï¼Œè€ŒSCIQL æ˜ç¡®ä¼˜åŒ–ä¸€ä¸ªå¯é‡åŒ–çš„â€œé£æ ¼å¥–åŠ±â€ï¼ˆå³çŠ¶æ€-åŠ¨ä½œå¯¹å±äºç›®æ ‡é£æ ¼çš„æ¦‚ç‡ï¼‰ã€‚<br>3. **ä»»åŠ¡-é£æ ¼æƒè¡¡**ï¼š SORL é€šè¿‡ä¸€ä¸ªæ¸©åº¦å‚æ•° `Î²` éšå¼åœ°æƒè¡¡ä¸èšç±»ç­–ç•¥çš„ç›¸ä¼¼æ€§å’Œä»»åŠ¡æ€§èƒ½ï¼Œè¿™ä¸ªå‚æ•°éš¾ä»¥è°ƒèŠ‚ä¸”æ•ˆæœä¸ç¨³å®šã€‚SCIQL é€šè¿‡ **GAWR** çš„æ˜¾å¼é—¨æ§æœºåˆ¶ï¼Œæ›´ç¨³å®šã€æ›´å¯æ§åœ°æå‡ä»»åŠ¡æ€§èƒ½è€Œä¸æŸå®³é£æ ¼å¯¹é½ã€‚ | 1. **å¯è§£é‡Šæ€§ä¸å¯æ§æ€§**ï¼š ç”¨æˆ·å¯ä»¥é€šè¿‡å®šä¹‰æ ‡ç­¾å‡½æ•°æ¥ç²¾ç¡®æŒ‡å®šæƒ³è¦çš„é£æ ¼ï¼Œå¹¶å¯¹é½æ•ˆæœè¿›è¡Œç›´æ¥æµ‹é‡ï¼Œè€ŒSORLçš„èšç±»é£æ ¼æ˜¯é»‘ç›’ä¸”ä¸å¯æ§çš„ã€‚<br>2. **æ›´ä¼˜çš„å¸•ç´¯æ‰˜å‰æ²¿**ï¼š å®éªŒè¡¨æ˜ï¼ŒSCIQL åœ¨é£æ ¼å¯¹é½å’Œä»»åŠ¡æ€§èƒ½çš„æƒè¡¡ä¸Šè·å¾—äº†æ¯” SORL æ›´å¤§çš„**è¶…ä½“ç§¯**ï¼Œå³æ•´ä½“ä¸Šè¾¾åˆ°äº†æ›´ä¼˜çš„è”åˆæ€§èƒ½ã€‚<br>3. **é’ˆå¯¹æ€§çš„é£æ ¼æ¡ä»¶åŒ–**ï¼š SCIQL è®­ç»ƒçš„æ˜¯å•ä¸€çš„é£æ ¼æ¡ä»¶åŒ–ç­–ç•¥ç½‘ç»œ `Ï€(a|s, z)`ï¼Œå¯ä»¥é€šè¿‡è¾“å…¥ `z` ç›´æ¥æ§åˆ¶è¾“å‡ºé£æ ¼ã€‚SORL éœ€è¦ç»´æŠ¤ä¸€ç»„ç¦»æ•£çš„ç­–ç•¥ï¼Œçµæ´»æ€§è¾ƒå·®ã€‚ |\n| **Petitbois et al. (2025) - WZBC** | **æ–¹æ³•**ï¼š **æ— ç›‘ç£**å­¦ä¹ è½¨è¿¹çš„éšç©ºé—´è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨è½¨è¿¹ç›¸ä¼¼æ€§åŠ æƒå›å½’è¿›è¡Œè¡Œä¸ºå…‹éš†ï¼Œä»¥æé«˜å¯¹å¤åˆé”™è¯¯çš„é²æ£’æ€§ã€‚<br>**ç›®æ ‡**ï¼š ä»æ•°æ®ä¸­å­¦ä¹ å¯æ§çš„å¤šæ ·åŒ–è¡Œä¸ºï¼Œä¸»è¦å…³æ³¨**è½¨è¿¹é‡å»º**çš„é²æ£’æ€§ã€‚ | 1. **æ ¸å¿ƒé—®é¢˜**ï¼š WZBC ä¸»è¦è§£å†³æ¨¡ä»¿å­¦ä¹ ä¸­çš„**åˆ†å¸ƒåç§»å’Œå¤åˆé”™è¯¯**é—®é¢˜ã€‚SCIQL åˆ™åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥è§£å†³äº†**é£æ ¼å¯¹é½**ä¸**ä»»åŠ¡æ€§èƒ½ä¼˜åŒ–**è¿™ä¸¤ä¸ªç›®æ ‡çš„è”åˆä¼˜åŒ–é—®é¢˜ã€‚<br>2. **æ˜¯å¦ä½¿ç”¨å¥–åŠ±**ï¼š WZBC å±äºæ¨¡ä»¿å­¦ä¹ èŒƒç•´ï¼Œä¸åˆ©ç”¨ç¯å¢ƒå¥–åŠ±ã€‚SCIQL æ˜¯ç¦»çº¿RLï¼Œæ ¸å¿ƒåˆ›æ–°ä¹‹ä¸€å°±æ˜¯åˆ©ç”¨å¥–åŠ±ä¿¡å·åœ¨é£æ ¼çº¦æŸä¸‹æå‡æ€§èƒ½ã€‚<br>3. **é£æ ¼æ¡ä»¶åŒ–æ–¹å¼**ï¼š WZBC é€šè¿‡éšå˜é‡æ§åˆ¶è¡Œä¸ºï¼Œä½†è¯¥éšå˜é‡æ˜¯æ— ç›‘ç£å­¦ä¹ çš„ï¼Œå«ä¹‰ä¸æ˜ç¡®ã€‚SCIQL ä½¿ç”¨æœ‰ç›‘ç£çš„ã€æ˜ç¡®çš„é£æ ¼æ ‡ç­¾è¿›è¡Œæ¡ä»¶åŒ–ã€‚ | 1. **ä»é²æ£’æ¨¡ä»¿åˆ°é£æ ¼åŒ–RL**ï¼š æœ¬æ–‡å°†ç ”ç©¶é‡ç‚¹ä»â€œå¦‚ä½•æ›´å¥½åœ°é‡å»ºå¤šæ ·åŒ–è¡Œä¸ºâ€æ¨è¿›åˆ°äº†â€œå¦‚ä½•åœ¨æ˜ç¡®çš„è¡Œä¸ºé£æ ¼çº¦æŸä¸‹æ‰§è¡Œå¹¶ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ ä»»åŠ¡â€ã€‚<br>2. **ç³»ç»Ÿæ€§åœ°å¼•å…¥RLå·¥å…·**ï¼š å€Ÿé‰´äº†ç›®æ ‡æ¡ä»¶RLï¼ˆGCRLï¼‰çš„æ€æƒ³ï¼ˆå¦‚ hindsight relabelingï¼‰ï¼Œå¹¶å°†å…¶ä¸ IQL ç­‰å…ˆè¿›çš„ç¦»çº¿RLç®—æ³•ç»“åˆï¼Œå½¢æˆäº†ä¸“é—¨çš„é£æ ¼æ¡ä»¶åŒ–ç¦»çº¿RLæ¡†æ¶ï¼ˆSCIQLï¼‰ã€‚<br>3. **æå‡ºæ–°çš„ç­–ç•¥ä¼˜åŒ–æœºåˆ¶**ï¼š æœ¬æ–‡æœ€å¤§çš„ç†è®ºè´¡çŒ®ä¹‹ä¸€æ˜¯æå‡ºäº† **Gated Advantage Weighted Regression (GAWR)**ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç­–ç•¥æå‡è§„åˆ™ï¼Œç”¨äºè¿‘ä¼¼â€œåœ¨é£æ ¼æœ€ä¼˜ç­–ç•¥é›†åˆä¸­æå‡ä»»åŠ¡æ€§èƒ½â€è¿™ä¸€å¤æ‚ä¼˜åŒ–é—®é¢˜ã€‚ |\n\n## æ€»ç»“ï¼šæœ¬æ–‡ç›¸å¯¹äºå‰ä½œçš„æ ¸å¿ƒè¿›å±•\n\næœ¬æ–‡ **ã€ŠOffline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignmentã€‹** çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºï¼Œå®ƒ**ç³»ç»Ÿæ€§åœ°æ•´åˆå¹¶è¶…è¶Šäº†**å‰ä½œçš„å·¥ä½œï¼Œå½¢æˆäº†ä¸€ä¸ª**ç»Ÿä¸€ã€å¼ºå¤§ä¸”å®ç”¨**çš„æ¡†æ¶ï¼š\n\n1.  **ç»Ÿä¸€çš„å½¢å¼åŒ–å®šä¹‰**ï¼š å°†â€œè¡Œä¸ºé£æ ¼â€å¹¿ä¹‰åœ°å®šä¹‰ä¸ºåŸºäºä»»åŠ¡å’Œå‡†åˆ™çš„å­è½¨è¿¹æ ‡ç­¾ï¼Œèåˆäº†æœ‰ç›‘ç£ï¼ˆZhan, Yangï¼‰çš„å¯è§£é‡Šæ€§å’Œæ— ç›‘ç£ï¼ˆMao, Petitboisï¼‰å¯¹æ•°æ®åˆ†å¸ƒçš„åˆ©ç”¨ï¼Œå¹¶ç‰¹åˆ«å¼ºè°ƒäº†**å­è½¨è¿¹æ ‡æ³¨**ä»¥è§£å†³å¤šæ—¶é—´å°ºåº¦ä¸ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚\n\n2.  **ç®—æ³•å±‚é¢çš„ç»¼åˆåˆ›æ–°**ï¼š\n    *   **SCIQL ç®—æ³•**ï¼š å°†ç¦»çº¿ç›®æ ‡æ¡ä»¶RLï¼ˆIQL + é‡æ ‡æ³¨ï¼‰çš„æŠ€æœ¯é¦–æ¬¡åº”ç”¨äº**é£æ ¼å¯¹é½**è¿™ä¸€æ–°é¢†åŸŸï¼Œè§£å†³äº†é•¿æœŸå†³ç­–å’Œè½¨è¿¹ç¼åˆçš„æŒ‘æˆ˜ã€‚\n    *   **GAWR æœºåˆ¶**ï¼š è¿™æ˜¯æœ¬æ–‡æœ€å…³é”®çš„**æŠ€æœ¯åˆ›æ–°**ã€‚å®ƒåˆ›é€ æ€§åœ°ä½¿ç”¨é£æ ¼ä¼˜åŠ¿å‡½æ•°ä½œä¸ºâ€œé—¨æ§â€ï¼Œæ¥è°ƒèŠ‚ä»»åŠ¡ä¼˜åŠ¿ä¿¡å·å¯¹ç­–ç•¥æ›´æ–°çš„å½±å“ï¼Œä»è€Œå®ç°äº† **â€œåœ¨ä¸ç ´åé£æ ¼å¯¹é½çš„å‰æä¸‹ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½â€** è¿™ä¸€å®é™…ä»·å€¼æé«˜çš„ç›®æ ‡ã€‚è¿™ä¸SORLç®€å•çš„çº¿æ€§åŠ æƒæˆ–çº¦æŸä¼˜åŒ–æœ‰æœ¬è´¨åŒºåˆ«ï¼Œæ›´ç¨³å®šã€æ›´æœ‰æ•ˆã€‚\n\n3.  **å®éªŒéªŒè¯çš„å…¨é¢ä¼˜åŠ¿**ï¼š è®ºæ–‡é€šè¿‡å¤§é‡å®éªŒè¯æ˜ï¼ŒSCIQL åœ¨**é£æ ¼å¯¹é½ç²¾åº¦**ä¸Šæ˜¾è‘—ä¼˜äºæ‰€æœ‰æ¨¡ä»¿å­¦ä¹ ç±»åŸºçº¿ï¼ˆBC, CBC, BCPMIï¼‰ï¼Œåœ¨**é£æ ¼æ¡ä»¶ä¸‹çš„ä»»åŠ¡æ€§èƒ½ä¼˜åŒ–**ä¸Šåˆ™æ˜¾è‘—ä¼˜äºæœ€ç›¸å…³çš„ç¦»çº¿RLåŸºçº¿ï¼ˆSORLï¼‰ï¼Œå¹¶ä¸”å¯¹æ ‡ç­¾å™ªå£°è¡¨ç°å‡ºæ›´å¼ºçš„**é²æ£’æ€§**ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡çš„ä¸»è¦æ”¹å–„åœ¨äºï¼šä»ä¸€ä¸ªæ›´é€šç”¨ã€æ›´å®ç”¨çš„é£æ ¼å®šä¹‰å‡ºå‘ï¼Œæ„å»ºäº†ä¸€ä¸ªå…¼å…·æ¨¡ä»¿å­¦ä¹ ï¼ˆæ˜ç¡®ç›‘ç£ã€å¯¹é½æ€§å¥½ï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆèƒ½ä¼˜åŒ–æ€§èƒ½ã€èƒ½è§„åˆ’ç¼åˆï¼‰ä¼˜åŠ¿çš„ç¦»çº¿å­¦ä¹ ç®—æ³•ï¼Œå¹¶é€šè¿‡æ–°é¢–çš„GAWRæœºåˆ¶ä¼˜é›…åœ°è§£å†³äº†é£æ ¼ä¸ä»»åŠ¡ä¹‹é—´çš„å›ºæœ‰å†²çªï¼Œæœ€ç»ˆå­¦å¾—äº†æ—¢é«˜åº¦ç¬¦åˆæŒ‡å®šé£æ ¼åˆå…·æœ‰é«˜ä»»åŠ¡æ€§èƒ½çš„ç­–ç•¥ã€‚**",
            "timestamp": "2026-02-03T15:57:07.744521",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## è®ºæ–‡å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\næœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOffline RLï¼‰ä¸­å­¦ä¹ **é«˜è´¨é‡ä¸”é£æ ¼å¯¹é½**çš„ç­–ç•¥çš„æ–¹æ³•ã€‚æ ¸å¿ƒè´¡çŒ®æ˜¯**SCIQLï¼ˆStyle-Conditioned Implicit Q-Learningï¼‰ç®—æ³•**åŠå…¶å…³é”®ç»„ä»¶**GAWRï¼ˆGated Advantage Weighted Regressionï¼‰æœºåˆ¶**ã€‚ä»¥ä¸‹è¡¨æ ¼è¯¦ç»†è¯´æ˜äº†å…³é”®æŠ€æœ¯æ–¹æ³•åŠå…¶ç»†èŠ‚ã€‚\n\n| æŠ€æœ¯æ–¹æ³• | æ ¸å¿ƒç›®æ ‡ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…³é”®å‚æ•°ä¸å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. é£æ ¼çš„å½¢å¼åŒ–å®šä¹‰ä¸æ•°æ®ç¼–ç¨‹** | æä¾›ä¸€ä¸ª**é€šç”¨ã€å¯è§£é‡Šã€ä½æ ‡æ³¨æˆæœ¬**çš„é£æ ¼å®šä¹‰ï¼Œè§£å†³ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚ | **1.1 é£æ ¼å®šä¹‰ï¼š** å°†é£æ ¼å®šä¹‰ä¸ºåœ¨ç»™å®š**ä»»åŠ¡**å’Œ**å‡†åˆ™**ä¸‹ï¼Œå¯¹**å­è½¨è¿¹**çš„æ ‡æ³¨ã€‚å½¢å¼åŒ–ä¸ºï¼šä¸€ä¸ªå‡†åˆ™ Î» å°†å­è½¨è¿¹ Ï„_{t-w+1:t+w} æ˜ å°„åˆ°ä¸€ä¸ªæ ‡ç­¾ z âˆˆ â„’(Î»)ã€‚<br>**1.2 æ•°æ®ç¼–ç¨‹ï¼š** ä½¿ç”¨**æ ‡æ³¨å‡½æ•°**è‡ªåŠ¨ä¸ºç¦»çº¿æ•°æ®é›† ğ’Ÿ ä¸­çš„æ¯ä¸ªæ—¶é—´æ­¥ (s_t, a_t) ç”Ÿæˆé£æ ¼æ ‡ç­¾ z_t = Î»(Ï„_{t-w+1:t+w})ã€‚è¿™é¿å…äº†æ‰‹åŠ¨æ ‡æ³¨ï¼Œå¹¶å…è®¸é£æ ¼åœ¨ä¸€æ¡è½¨è¿¹å†…å˜åŒ–ã€‚<br>**1.3 ä»£ç†é©¬å°”å¯å¤«é£æ ¼ç›®æ ‡ï¼š** ç”±äºçœŸå®é£æ ¼ç›®æ ‡ï¼ˆå…¬å¼2ï¼‰æ˜¯éé©¬å°”å¯å¤«çš„ï¼Œè®ºæ–‡ä»æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ ä¸€ä¸ª**é£æ ¼å¥–åŠ±å‡½æ•°**ä½œä¸ºä»£ç†ç›®æ ‡ï¼š<br>r_ğ’Ÿ^Î»(s, a) = p_{Ï€_ğ’Ÿ}^Î»(z \\| s, a) â‰ˆ â„™(å­è½¨è¿¹ä¸­å¿ƒä¸ºz \\| æ•°æ®é›†ä¸­(s,a))ã€‚<br>è¿™é€šè¿‡ä¸€ä¸ªåˆ†ç±»å™¨ï¼ˆå¦‚ç®€å•çš„æŒ‡ç¤ºå‡½æ•°ï¼‰ä»æ•°æ®ä¸­ä¼°è®¡ï¼Œå°†é£æ ¼å¯¹é½é—®é¢˜è½¬åŒ–ä¸ºæ ‡å‡†çš„å¥–åŠ±æœ€å¤§åŒ–MDPé—®é¢˜ã€‚ | **å‚æ•°ï¼š** <br>- **çª—å£åŠå¾„ w(Î»):** å–å†³äºé£æ ¼å‡†åˆ™ Î»ï¼ˆä¾‹å¦‚ï¼Œé€Ÿåº¦ w=1ï¼Œè½¬å¼¯åŠå¾„ w=51ï¼‰ã€‚<br>- **æ ‡ç­¾é›†å¤§å° \\|â„’(Î»)\\|:** é£æ ¼ç±»åˆ«æ•°ã€‚<br>**å®ç°ï¼š** åœ¨è®­ç»ƒå‰å¯¹æ•°æ®é›†è¿›è¡Œä¸€æ¬¡æ€§é¢„å¤„ç†ï¼Œä¸ºæ¯ä¸ª(s,a)å¯¹å…³è”ä¸€ä¸ªæˆ–å¤šä¸ªé£æ ¼æ ‡ç­¾ã€‚ |\n| **2. é£æ ¼æ¡ä»¶éšå¼Qå­¦ä¹  (SCIQL)** | åœ¨ç¦»çº¿è®¾ç½®ä¸‹ï¼Œ**ä¼˜åŒ–é£æ ¼å¯¹é½ç›®æ ‡**ï¼Œè§£å†³ç¨€ç–ç›‘ç£å’Œåˆ†å¸ƒåç§»é—®é¢˜ï¼Œå®ç°é•¿æœŸå†³ç­–å’Œè½¨è¿¹æ‹¼æ¥ã€‚ | **2.1 ç®—æ³•åŸºç¡€ï¼š** åŸºäº**IQL**ï¼Œä½†æ‰©å±•ä¸º**é£æ ¼æ¡ä»¶**ã€‚ä¸ºæ¯ä¸ªé£æ ¼æ ‡ç­¾ z å­¦ä¹ é£æ ¼æ¡ä»¶ä»·å€¼å‡½æ•° V^Î»(s, z) å’Œ Q^Î»(s, a, z)ã€‚<br>**2.2 ä»·å€¼å‡½æ•°å­¦ä¹ ï¼ˆExpectile Regressionï¼‰ï¼š** <br>   - å­¦ä¹  V^Î»ï¼Œä½¿å…¶åå‘ Q^Î» çš„ä¸Šåˆ†ä½æ•°ï¼š<br>     â„’_V^Î»(Ï•^Î») = ğ”¼_{(s,a)âˆ¼ğ’Ÿ, zâˆ¼Î»(ğ’Ÿ)}[â„“â‚‚^Îº(Q_{Î¸Ì„^Î»}^Î»(s,a,z) - V_{Ï•^Î»}^Î»(s, z))]<br>     å…¶ä¸­ â„“â‚‚^Îº(u) = \\|Îº - ğŸ™{u<0}\\| uÂ², Îº âˆˆ [0.5, 1)ã€‚<br>   - å­¦ä¹  Q^Î» æ‹Ÿåˆè´å°”æ›¼ç›®æ ‡ï¼š<br>     â„’_Q^Î»(Î¸^Î») = ğ”¼_{(s,a,sâ€˜)âˆ¼ğ’Ÿ, zâˆ¼Î»(ğ’Ÿ)}[(Ï‡^Î»(s,a,z) + Î³V_{Ï•^Î»}^Î»(sâ€™, z) - Q_{Î¸^Î»}^Î»(s,a,z))Â²]<br>     å…¶ä¸­ Ï‡^Î»(s,a,z) æ˜¯é£æ ¼å¥–åŠ±ä¼°è®¡å™¨ï¼Œè®ºæ–‡å‘ç°ä½¿ç”¨**æŒ‡ç¤ºå‡½æ•°** Ï‡^Î»(s,a,z) = ğŸ™(z = z_c)ï¼ˆz_c æ˜¯æ•°æ®ä¸­(s,a)çš„çœŸå®æ ‡ç­¾ï¼‰æ•ˆæœå¾ˆå¥½ã€‚<br>**2.3 ç­–ç•¥æå–ï¼ˆAdvantage Weighted Regressionï¼‰ï¼š** <br>     åˆ©ç”¨å­¦åˆ°çš„ä»·å€¼å‡½æ•°ï¼Œé€šè¿‡AWRæå–é£æ ¼æ¡ä»¶ç­–ç•¥ Ï€^Î»ï¼š<br>     J_Ï€^Î»(Ïˆ^Î») = ğ”¼_{(s,a)âˆ¼ğ’Ÿ, zâˆ¼Î»(ğ’Ÿ)}[exp(Î²^Î» Â· A^Î»(s,a,z)) log Ï€_{Ïˆ^Î»}^Î»(a \\| s, z)]<br>     å…¶ä¸­ä¼˜åŠ¿å‡½æ•° A^Î»(s,a,z) = Q_{Î¸Ì„^Î»}^Î»(s,a,z) - V_{Ï•^Î»}^Î»(s, z)ã€‚è¯¥ç›®æ ‡é¼“åŠ±ç­–ç•¥å¤åˆ¶æ•°æ®é›†ä¸­èƒ½å¸¦æ¥é«˜é£æ ¼ä¼˜åŠ¿çš„åŠ¨ä½œã€‚ | **å‚æ•°ï¼š** <br>- **æœŸæœ›å›å½’å‚æ•° Îº:** æ§åˆ¶ä»·å€¼ä¼°è®¡çš„ä¿å®ˆæ€§ï¼ˆè®ºæ–‡ç”¨0.7ï¼‰ã€‚<br>- **é€†æ¸©åº¦ Î²^Î»:** æ§åˆ¶ç­–ç•¥å¯¹é«˜ä¼˜åŠ¿åŠ¨ä½œçš„åå‘ç¨‹åº¦ï¼ˆç»§æ‰¿è‡ªIQLï¼Œå¸¸ç”¨3.0ï¼‰ã€‚<br>- **æŠ˜æ‰£å› å­ Î³:** 0.99ã€‚<br>**å…³é”®åˆ›æ–°ï¼š** åœ¨Qå­¦ä¹ æ›´æ–°ä¸­ï¼Œå¯¹é£æ ¼æ ‡ç­¾ **z è¿›è¡Œé‡æ ‡æ³¨**ï¼ˆä» Î»(ğ’Ÿ) ä¸­å‡åŒ€é‡‡æ ·ï¼‰ï¼Œè€Œä¸ä»…é™äºå½“å‰è½¨è¿¹çš„æ ‡ç­¾ã€‚è¿™å¼•å…¥äº†** hindsight relabeling **ï¼Œä½¿ç­–ç•¥èƒ½å­¦ä¹ å¦‚ä½•è¾¾åˆ°ä»»æ„æŒ‡å®šé£æ ¼ï¼Œå¢å¼ºäº†é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ |\n| **3. é—¨æ§ä¼˜åŠ¿åŠ æƒå›å½’ (GAWR)** | åœ¨**ä¸æŸå®³é£æ ¼å¯¹é½**çš„å‰æä¸‹ï¼Œ**ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½**ï¼Œè¿‘ä¼¼æ±‚è§£å…¬å¼(7)çš„çº¦æŸä¼˜åŒ–é—®é¢˜ã€‚ | **3.1 æ ¸å¿ƒæ€æƒ³ï¼š** å°†é£æ ¼å¯¹é½è§†ä¸ºé¦–è¦çº¦æŸï¼Œä»»åŠ¡ä¼˜åŒ–ä½œä¸ºæ¬¡è¦ç›®æ ‡ã€‚é€šè¿‡ä¸€ä¸ª**é—¨æ§æœºåˆ¶**ï¼Œä»…å½“åŠ¨ä½œå¯¹é£æ ¼å¯¹é½æœ‰ç›Šï¼ˆæˆ–æ— å®³ï¼‰æ—¶ï¼Œæ‰å¼•å…¥ä»»åŠ¡ä¼˜åŠ¿ä¿¡å·ã€‚<br>**3.2 é—¨æ§ä¼˜åŠ¿å‡½æ•°ï¼š** <br>     Î¾(A^Î», A^r)(s,a,z) = A^Î»(s,a,z) + Ïƒ(A^Î»(s,a,z)) Â· A^r(s, a)<br>     å…¶ä¸­ Ïƒ(Â·) æ˜¯sigmoidå‡½æ•°ï¼ŒA^r(s,a) æ˜¯ä»»åŠ¡ä¼˜åŠ¿å‡½æ•°ï¼ˆé€šè¿‡æ ‡å‡†IQLå­¦ä¹ ï¼‰ã€‚<br>     - **ç¬¬ä¸€é¡¹ A^Î»:** ç¡®ä¿é£æ ¼å¯¹é½çš„åŸºæœ¬é©±åŠ¨ã€‚<br>     - **ç¬¬äºŒé¡¹ Ïƒ(A^Î»)Â·A^r:** é—¨æ§ä»»åŠ¡ä¼˜åŒ–é¡¹ã€‚å½“ A^Î» å¾ˆå¤§ï¼ˆæ­£ï¼Œå¯¹é£æ ¼æœ‰ç›Šï¼‰æ—¶ï¼ŒÏƒ(A^Î») â‰ˆ 1ï¼Œä»»åŠ¡ä¼˜åŠ¿è¢«å……åˆ†å¼•å…¥ï¼›å½“ A^Î» å¾ˆå°æˆ–ä¸ºè´Ÿæ—¶ï¼ŒÏƒ(A^Î») â‰ˆ 0ï¼Œä»»åŠ¡ä¼˜åŠ¿è¢«æŠ‘åˆ¶ã€‚<br>**3.3 ç­–ç•¥ä¼˜åŒ–ç›®æ ‡ï¼š** <br>     ä½¿ç”¨é—¨æ§ä¼˜åŠ¿ Î¾ è¿›è¡ŒåŠ æƒå›å½’ï¼Œå¾—åˆ°æœ€ç»ˆçš„é£æ ¼æ¡ä»¶é«˜æ€§èƒ½ç­–ç•¥ Ï€^{r\\|Î»}ï¼š<br>     J_Ï€^{r\\|Î»}(Ïˆ^{r\\|Î»}) = ğ”¼_{(s,a)âˆ¼ğ’Ÿ, zâˆ¼Î»(ğ’Ÿ)}[exp(Î²^{r\\|Î»} Â· Î¾(A^Î», A^r)(s,a,z)) log Ï€_{Ïˆ^{r\\|Î»}}^{r\\|Î»}(a \\| s, z)] | **å‚æ•°ï¼š** <br>- **ä»»åŠ¡é€†æ¸©åº¦ Î²^{r\\|Î»}:** æ§åˆ¶ä»»åŠ¡ä¼˜åŒ–å¼ºåº¦ï¼ˆè®ºæ–‡ä¸­ä¸Î²^Î»è®¾ç½®ç›¸åŒï¼‰ã€‚<br>- **ä¼˜åŠ¿å½’ä¸€åŒ–ï¼š** å®è·µä¸­å¯¹ A^Î» å’Œ A^r ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å…¶é‡çº§ç›¸è¿‘ã€‚<br>**æ•°å­¦åŸç†ï¼š** ä¸SORLç­‰ä½¿ç”¨çº¿æ€§åŠ æƒï¼ˆA^Î» + Î²A^rï¼‰çš„æ–¹æ³•ä¸åŒï¼ŒGAWRçš„**éçº¿æ€§é—¨æ§**ï¼ˆsigmoidï¼‰ç¡®ä¿äº†é£æ ¼å¯¹é½å…·æœ‰ä¼˜å…ˆçº§ã€‚å®ƒè¿‘ä¼¼åœ°å®ç°äº†åœ¨**é£æ ¼æœ€ä¼˜ç­–ç•¥é›† Î ^Î»(z)** å†…è¿›è¡Œä»»åŠ¡æ€§èƒ½ä¼˜åŒ–çš„æŠ•å½±æ€æƒ³ï¼ˆå…¬å¼7ï¼‰ï¼Œè€Œæ— éœ€æ˜¾å¼æ±‚è§£è¿™ä¸ªçº¦æŸé›†ã€‚ |\n| **4. æ•´ä½“è®­ç»ƒæµç¨‹ (ç®—æ³•1)** | æ•´åˆä»¥ä¸Šç»„ä»¶ï¼Œè¿›è¡Œé«˜æ•ˆçš„ç«¯åˆ°ç«¯ç¦»çº¿è®­ç»ƒã€‚ | è®­ç»ƒåˆ†ä¸ºä¸‰ä¸ªå¾ªç¯ï¼ˆå®é™…å¯å¹¶è¡Œï¼‰ï¼š<br>1.  **è®­ç»ƒä»»åŠ¡ä»·å€¼å‡½æ•°** (V^r, Q^r): ä½¿ç”¨æ ‡å‡†IQLï¼ˆå…¬å¼8,9ï¼‰ã€‚<br>2.  **è®­ç»ƒé£æ ¼ä»·å€¼å‡½æ•°** (V^Î», Q^Î»): ä½¿ç”¨SCIQLçš„æœŸæœ›å›å½’ï¼ˆå…¬å¼11,12ï¼‰ã€‚<br>3.  **è®­ç»ƒæœ€ç»ˆç­–ç•¥ Ï€^{r\\|Î»}:** ä½¿ç”¨GAWRç›®æ ‡ï¼ˆå…¬å¼15ï¼‰ï¼Œå…¶ä¾èµ–äºå‰ä¸¤æ­¥å­¦åˆ°çš„ A^Î» å’Œ A^rã€‚ | **å®ç°ç»†èŠ‚ï¼š** <br>- ä½¿ç”¨**ç›®æ ‡ç½‘ç»œ**ï¼ˆå‚æ•° Î¸Ì„ï¼‰ç¨³å®šQå­¦ä¹ ï¼Œé€šè¿‡Polyakå¹³å‡æ›´æ–°ï¼šÎ¸Ì„ â† (1-Ï…)Î¸Ì„ + Ï…Î¸ï¼Œ Ï…=0.005ã€‚<br>- ç½‘ç»œæ¶æ„ï¼šMLPï¼ˆCircle2Då’ŒHalfCheetahä¸º[256,256]ï¼ŒHumEnvä¸º[1024,1024,1024]ï¼‰ã€‚<br>- ä¼˜åŒ–å™¨ï¼šAdamï¼Œå­¦ä¹ ç‡3e-3ï¼Œç­–ç•¥ç½‘ç»œä½¿ç”¨ä½™å¼¦é€€ç«ã€‚ |\n\n### **æ€»ç»“ä¸åˆ›æ–°ç‚¹**\n1.  **ç†è®ºæ¡†æ¶åˆ›æ–°ï¼š** å°†é£æ ¼å¯¹é½é—®é¢˜ç»Ÿä¸€å¹¶å½¢å¼åŒ–ä¸º**ç›®æ ‡æ¡ä»¶RLçš„æ³›åŒ–**ï¼Œå¹¶é€šè¿‡æ•°æ®ç¼–ç¨‹å®ç°å¯",
            "timestamp": "2026-02-03T15:58:18.295916",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä½¿ç”¨äº†ä¸¤ç±»æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼Œåˆ†åˆ«è¡¡é‡**é£æ ¼å¯¹é½**å’Œ**ä»»åŠ¡æ€§èƒ½**ã€‚\n\n#### 1. é£æ ¼å¯¹é½æŒ‡æ ‡\n- **æŒ‡æ ‡åç§°**ï¼šç»éªŒå½’ä¸€åŒ–æ— æŠ˜æ‰£é£æ ¼å¯¹é½ (`S^ğŸ™(Ï€, Î», z)`)\n- **è®¡ç®—å…¬å¼**ï¼š\n  ```math\n  \\hat{S}^{\\mathbbm{1}}(\\lambda(\\tau), z) = \\frac{1}{|\\tau|} \\sum_{t=0}^{|\\tau|-1} \\mathbbm{1}\\{z_t = z\\}\n  ```\n- **æŒ‡æ ‡ç»†èŠ‚**ï¼š\n  - **ç›®çš„**ï¼šè¡¡é‡ç­–ç•¥åœ¨ç»™å®šç›®æ ‡é£æ ¼ `z` ä¸‹çš„å¯¹é½ç¨‹åº¦ã€‚\n  - **è®¡ç®—æ–¹å¼**ï¼šå¯¹äºç”Ÿæˆçš„è½¨è¿¹ `Ï„`ï¼Œä½¿ç”¨æ ‡æ³¨å‡½æ•° `Î»` ä¸ºæ¯ä¸ªæ—¶é—´æ­¥ `t` è®¡ç®—å…¶çª—å£ï¼ˆåŠå¾„ä¸º `w(Î»)`ï¼‰çš„æ ‡ç­¾ `z_t`ã€‚è¯¥æŒ‡æ ‡è®¡ç®—è½¨è¿¹ä¸­æ ‡ç­¾ç­‰äºç›®æ ‡é£æ ¼ `z` çš„æ—¶é—´æ­¥æ‰€å çš„æ¯”ä¾‹ã€‚\n  - **å–å€¼èŒƒå›´**ï¼š`[0, 1]`ï¼Œå€¼è¶Šé«˜è¡¨ç¤ºé£æ ¼å¯¹é½è¶Šå¥½ã€‚\n  - **ç±»æ¯”**ï¼šä½œè€…å°†å…¶ç±»æ¯”ä¸ºç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ä¸­çš„â€œæˆåŠŸç‡â€ã€‚\n  - **è¯„ä¼°æµç¨‹**ï¼šå¯¹æ¯ä¸ªé£æ ¼æ ‡ç­¾ `z`ï¼Œåœ¨5ä¸ªéšæœºç§å­ä¸‹å„è¿›è¡Œ10æ¬¡ç­–ç•¥ rolloutï¼Œè®¡ç®—å¹³å‡å¯¹é½åˆ†æ•°ã€‚\n\n#### 2. ä»»åŠ¡æ€§èƒ½æŒ‡æ ‡\n- **æŒ‡æ ‡åç§°**ï¼šå½’ä¸€åŒ–ä»»åŠ¡å›æŠ¥\n- **è®¡ç®—å…¬å¼**ï¼šåŸºäºç¯å¢ƒåŸå§‹å¥–åŠ±å‡½æ•° `r(s, a)` è®¡ç®—çš„æŠ˜æ‰£ç´¯ç§¯å›æŠ¥ï¼Œå¹¶è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ä»¥ä¾¿è·¨ç¯å¢ƒæ¯”è¾ƒã€‚\n- **æŒ‡æ ‡ç»†èŠ‚**ï¼š\n  - **ç›®çš„**ï¼šè¡¡é‡ç­–ç•¥åœ¨å®Œæˆåº•å±‚RLä»»åŠ¡ï¼ˆå¦‚ç§»åŠ¨é€Ÿåº¦ã€ç”»åœ†ç²¾åº¦ï¼‰æ–¹é¢çš„æ€§èƒ½ã€‚\n  - **è®¡ç®—æ–¹å¼**ï¼šåœ¨ç­–ç•¥è¯„ä¼°æ—¶ï¼Œè®°å½•å…¶äº§ç”Ÿçš„è½¨è¿¹çš„æŠ˜æ‰£ç´¯ç§¯å¥–åŠ±ã€‚è®ºæ–‡ä¸­æœªæ˜ç¡®ç»™å‡ºå½’ä¸€åŒ–å…¬å¼ï¼Œä½†é€šå¸¸åšæ³•æ˜¯ç›¸å¯¹äºæŸä¸ªåŸºçº¿ï¼ˆå¦‚éšæœºç­–ç•¥æˆ–ä¸“å®¶ç­–ç•¥ï¼‰çš„å›æŠ¥è¿›è¡Œç¼©æ”¾ã€‚\n  - **è”åˆè¯„ä¼°**ï¼šä¸ºäº†å…¨é¢è¯„ä¼°æ–¹æ³•ï¼Œè®ºæ–‡ç»˜åˆ¶äº†**å¸•ç´¯æ‰˜å‰æ²¿å›¾**ï¼Œå¹¶è®¡ç®—äº†**è¶…ä½“ç§¯**ï¼Œä»¥å±•ç¤ºé£æ ¼å¯¹é½ä¸ä»»åŠ¡æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚\n\n### äºŒã€ å®éªŒæ•°æ®é›†\n\nè®ºæ–‡åœ¨ä¸‰ä¸ªå…·æœ‰ä¸åŒå¤æ‚åº¦çš„ç¯å¢ƒä¸­è¿›è¡Œäº†å®éªŒï¼Œæ¯ä¸ªç¯å¢ƒéƒ½åŒ…å«å¤šä¸ªä»»åŠ¡ã€æ ‡æ³¨å‡†åˆ™å’Œç”Ÿæˆçš„æ•°æ®é›†å˜ä½“ã€‚\n\n| ç¯å¢ƒ | ä¸»è¦ä»»åŠ¡ | çŠ¶æ€/åŠ¨ä½œç»´åº¦ | æ•°æ®é›†å˜ä½“ï¼ˆåŠå…³é”®ç‰¹ç‚¹ï¼‰ | æ ‡æ³¨å‡†åˆ™ï¼ˆåŠæ ‡ç­¾æ•°ï¼‰ |\n| :--- | :--- | :--- | :--- | :--- |\n| **Circle2d** | åœ¨2Då¹³é¢ä¸Šç»˜åˆ¶æŒ‡å®šåœ†å½¢ | çŠ¶æ€: å†å²ä½ç½®/æ–¹å‘ (12ç»´) <br> åŠ¨ä½œ: è§’é€Ÿåº¦/é€Ÿåº¦ (2ç»´) | 1. **circle2d-inplace-v0**: ä»èµ·å§‹ç‚¹ç›´æ¥ç”»åœ†ã€‚<br>2. **circle2d-navigate-v0**: å…ˆç§»åŠ¨åˆ°ç›®æ ‡ç‚¹å†ç”»åœ†ã€‚ | 1. **position**: å¹³é¢ç½‘æ ¼ä½ç½® (8ç±») <br>2. **movement_direction**: ç§»åŠ¨æ–¹å‘ (8ç±») <br>3. **turn_direction**: è½¬å¼¯æ–¹å‘ (å·¦/å³/ç›´ï¼Œ3ç±») <br>4. **radius**: ç»˜åˆ¶åœ†çš„åŠå¾„ (3ç±»+â€œç›´è¡Œâ€ç±») <br>5. **speed**: ç§»åŠ¨é€Ÿåº¦ (3ç±») <br>6. **curvature_noise**: æ›²ç‡å˜åŒ–å™ªå£° (3ç±») |\n| **HalfCheetah** | æ§åˆ¶å¹³é¢æœºå™¨äººå‘å‰å¿«é€Ÿç§»åŠ¨ | çŠ¶æ€: 17ç»´ <br> åŠ¨ä½œ: 6ç»´ | 1. **halfcheetah-fixed-v0**: è½¨è¿¹å†…ç­–ç•¥å›ºå®šã€‚<br>2. **halfcheetah-stitch-v0**: è½¨è¿¹è¢«åˆ‡åˆ†ä¸ºçŸ­ç‰‡æ®µã€‚<br>3. **halfcheetah-vary-v0**: è½¨è¿¹å†…ç­–ç•¥å‘ç”Ÿå˜åŒ–ã€‚ | 1. **speed**: å‰è¿›é€Ÿåº¦ (æ…¢/ä¸­/å¿«ï¼Œ3ç±») <br>2. **angle**: èº¯å¹²ä¿¯ä»°è§’ (3ç±») <br>3. **torso_height**: èº¯å¹²é«˜åº¦ (3ç±») <br>4. **backfoot_height**: åè„šé«˜åº¦ (4ç±») <br>5. **frontfoot_height**: å‰è„šé«˜åº¦ (4ç±») |\n| **HumEnv** | æ§åˆ¶é«˜ç»´äººä½“æ¨¡å‹å¿«é€Ÿå¹³é¢ç§»åŠ¨ | çŠ¶æ€: 358ç»´ <br> åŠ¨ä½œ: 69ç»´ | 1. **humenv-simple-v0**: ä»ç«™ç«‹å§¿æ€å¼€å§‹ã€‚<br>2. **humenv-complex-v0**: ä»èººä¸‹å§¿æ€å¼€å§‹ï¼Œè½¨è¿¹å†…é£æ ¼æœ‰å˜åŒ–ã€‚ | 1. **simple - head_height**: å¤´éƒ¨é«˜åº¦ (ä½/é«˜ï¼Œ2ç±») <br>2. **complex - speed**: ç§»åŠ¨é€Ÿåº¦ (é™æ­¢/æ…¢/å¿«ï¼Œ3ç±») <br>3. **complex - head_height**: å¤´éƒ¨é«˜åº¦ (ä½/ä¸­/é«˜ï¼Œ3ç±») |\n\n### ä¸‰ã€ æ•°æ®é›†ä¸è¯„ä¼°çš„å…³é”®ç»†èŠ‚\n\n1.  **æ•°æ®ç”Ÿæˆ**ï¼š\n    - **Circle2d**ï¼šä½¿ç”¨ç¡¬ç¼–ç çš„æ™ºèƒ½ä½“ç”Ÿæˆå¤šæ ·åŒ–è½¨è¿¹ï¼ˆä¸åŒåœ†å¿ƒã€åŠå¾„ã€æ–¹å‘ã€é€Ÿåº¦ã€å™ªå£°ï¼‰ã€‚\n    - **HalfCheetah**ï¼šä½¿ç”¨SACè®­ç»ƒå…·æœ‰ä¸åŒé£æ ¼åå¥½ï¼ˆå¦‚ç‰¹å®šé€Ÿåº¦ã€èº¯å¹²é«˜åº¦ï¼‰çš„â€œåŸå‹ç­–ç•¥â€æ¥æ”¶é›†æ•°æ®ã€‚\n    - **HumEnv**ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„â€œMetamotivo-M1â€æ¨¡å‹ç”Ÿæˆå…·æœ‰äººç±»è¿åŠ¨é£æ ¼çš„æ•°æ®ã€‚\n\n2.  **æ ‡æ³¨æ–¹å¼**ï¼š\n    - é‡‡ç”¨**æ•°æ®ç¼–ç¨‹**èŒƒå¼ï¼Œä½¿ç”¨ç¡¬ç¼–ç çš„**æ ‡æ³¨å‡½æ•°**ã€‚\n    - å…³é”®åˆ›æ–°ï¼šåœ¨**å­è½¨è¿¹çª—å£**è€Œéæ•´æ¡è½¨è¿¹ä¸Šè¿›è¡Œæ ‡æ³¨ï¼Œä»¥æ•æ‰é£æ ¼çš„å¤šæ—¶é—´å°ºåº¦ç‰¹æ€§å¹¶ç¼“è§£ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚\n    - å¯¹äºæ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹ `(s_t, a_t)`ï¼Œå…¶æ ‡ç­¾ `z_t` ç”±å…¶å‰å `w(Î»)` æ­¥ç»„æˆçš„çª—å£å†³å®šã€‚\n\n3.  **è¯„ä¼°è§„æ¨¡**ï¼š\n    - å®éªŒæ¶µç›–äº† **14ä¸ªæ ‡æ³¨å‡†åˆ™**ã€**76ä¸ªå…·ä½“é£æ ¼æ ‡ç­¾**ã€**å¤šä¸ªæ•°æ®é›†å˜ä½“**ã€‚\n    - æ€»è®¡è¿›è¡Œäº† **820æ¬¡** è®­ç»ƒè¿è¡Œï¼ˆæ¯ä¸ªç®—æ³•å˜ä½“åœ¨æ¯ç§`(ç¯å¢ƒ, å‡†åˆ™, æ ‡ç­¾)`ç»„åˆä¸Šè¿è¡Œ5ä¸ªç§å­ï¼‰ã€‚\n    - æ€»è®¡è¿›è¡Œäº† **8200æ¬¡** è¯„ä¼°å›åˆï¼ˆæ¯æ¬¡è®­ç»ƒè¿è¡Œè¯„ä¼°10ä¸ªå›åˆï¼‰ã€‚\n\n4.  **åŸºçº¿æ–¹æ³•**ï¼š\n    - **BC**ï¼šæ ‡å‡†è¡Œä¸ºå…‹éš†ã€‚\n    - **CBC**ï¼šæ¡ä»¶è¡Œä¸ºå…‹éš†ï¼ˆä½¿ç”¨å½“å‰è½¨è¿¹é£æ ¼ï¼‰ã€‚\n    - **BCPMI**ï¼šä½¿ç”¨ç‚¹é—´äº’ä¿¡æ¯åŠ æƒçš„æ¡ä»¶è¡Œä¸ºå…‹éš†ã€‚\n    - **SORL**ï¼šé£æ ¼åŒ–ç¦»çº¿RLï¼ˆä¸»è¦å¯¹æ¯”æ–¹æ³•ï¼‰ã€‚\n    - **SCBC**ï¼šæœ¬æ–‡æ–¹æ³•çš„æ¨¡ä»¿å­¦ä¹ å˜ä½“ï¼ˆä»…ä½¿ç”¨ hindsight é£æ ¼é‡æ ‡æ³¨ï¼Œæ— ä»·å€¼å‡½æ•°ï¼‰ã€‚\n\n### æ€»ç»“\næœ¬æ–‡è®¾è®¡äº†ä¸€å¥—**ç³»ç»Ÿã€ä¸¥è°¨ä¸”å¯è§£é‡Šçš„è¯„ä¼°ä½“ç³»**ã€‚å…¶æ ¸å¿ƒåœ¨äºï¼š\n- **æŒ‡æ ‡**ï¼šæ¸…æ™°å®šä¹‰äº†å¯é‡åŒ–çš„é£æ ¼å¯¹é½åº¦å’Œä»»åŠ¡æ€§èƒ½æŒ‡æ ‡ã€‚\n- **æ•°æ®**ï¼šæ„å»ºäº†æ¶µç›–ä»ç®€å•åˆ°å¤æ‚ã€ä»åŒè´¨åˆ°å¼‚è´¨ã€ä»ä½ç»´åˆ°é«˜ç»´çš„å¤šæ ·åŒ–åŸºå‡†æ•°æ®é›†ã€‚\n- **æ ‡æ³¨**ï¼šæå‡ºäº†åŸºäºå­è½¨è¿¹çª—å£çš„æ ‡æ³¨å‡½æ•°ï¼Œå¹³è¡¡äº†**å¯è§£é‡Šæ€§ã€ä½æ ‡æ³¨æˆæœ¬ã€æ˜“è¯„ä¼°æ€§**å’Œ**ä¿¡ç”¨åˆ†é…**çš„æŒ‘æˆ˜ã€‚\n- **è§„æ¨¡**ï¼šè¿›è¡Œäº†å¤§è§„æ¨¡ã€å¯é‡å¤çš„å®éªŒï¼Œå……åˆ†éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚\n\nè¿™äº›å·¥ä½œä¸ºâ€œé£æ ¼åŒ–ç­–ç•¥å­¦ä¹ â€è¿™ä¸€é¢†åŸŸæä¾›äº†å®è´µçš„åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ã€‚",
            "timestamp": "2026-02-03T15:59:16.109304",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ä¸‰ä¸ªä¸åŒçš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­æ„å»ºäº†å¤šä¸ªæ•°æ®é›†è¿›è¡Œå®éªŒï¼Œæ¶µç›–äº†ä»ç®€å•åˆ°å¤æ‚çš„ä»»åŠ¡å’Œå¤šæ ·çš„è¡Œä¸ºé£æ ¼ã€‚\n\n| ç¯å¢ƒ | æ•°æ®é›†åç§° | æ ¸å¿ƒä»»åŠ¡æè¿° | é£æ ¼æ ‡æ³¨å‡½æ•°ï¼ˆCriteriaï¼‰ä¸æ ‡ç­¾ç¤ºä¾‹ |\n| :--- | :--- | :--- | :--- |\n| **Circle2d** (2Då¹³é¢å¯¼èˆª) | `circle2d-inplace-v0` | åœ¨å›ºå®šèµ·å§‹ç‚¹ç»˜åˆ¶ç›®æ ‡åœ†å½¢ | `position` (8ä¸ªåŒºåŸŸ), `movement_direction` (8ä¸ªæ–¹å‘), `turn_direction` (å·¦/å³/ç›´), `radius` (3ç§åŠå¾„), `speed` (3ç§é€Ÿåº¦), `curvature_noise` (3ç§å™ªå£°æ°´å¹³) |\n| | `circle2d-navigate-v0` | å…ˆå¯¼èˆªåˆ°ç›®æ ‡ç‚¹å†ç»˜åˆ¶åœ†å½¢ | åŒä¸Š |\n| **HalfCheetah** (æœºå™¨äººè¿åŠ¨æ§åˆ¶) | `halfcheetah-fixed-v0` | æ§åˆ¶çŒè±¹æœºå™¨äººå‘å‰å¥”è·‘ï¼Œç­–ç•¥å›ºå®š | `speed` (æ…¢/ä¸­/å¿«), `angle` (èº¯å¹²ä¿¯ä»°è§’), `torso_height` (èº¯å¹²é«˜åº¦), `backfoot_height`, `frontfoot_height` |\n| | `halfcheetah-stitch-v0` | è½¨è¿¹è¢«åˆ†å‰²æˆçŸ­ç‰‡æ®µ | åŒä¸Š |\n| | `halfcheetah-vary-v0` | è½¨è¿¹å†…ç­–ç•¥ï¼ˆé£æ ¼ï¼‰ä¼šå‘ç”Ÿå˜åŒ– | åŒä¸Š |\n| **HumEnv** (é«˜ç»´äººä½“è¿åŠ¨æ§åˆ¶) | `humenv-simple-v0` | ä»ç«™ç«‹çŠ¶æ€å¼€å§‹å¿«é€Ÿç§»åŠ¨ | `head_height` (ä½/é«˜) |\n| | `humenv-complex-v0` | ä»èººä¸‹çŠ¶æ€å¼€å§‹ç§»åŠ¨ï¼Œé£æ ¼åœ¨è½¨è¿¹å†…å˜åŒ– | `speed` (ä¸åŠ¨/æ…¢/å¿«), `head_height` (ä½/ä¸­/é«˜) |\n\n**æ•°æ®é›†ç‰¹ç‚¹**ï¼š\n- **å¤šæ ·æ€§**ï¼šæ•°æ®é€šè¿‡ç¡¬ç¼–ç ç­–ç•¥ã€SACè®­ç»ƒçš„ç­–ç•¥æˆ–è¡Œä¸ºåŸºç¡€æ¨¡å‹ï¼ˆMetamotivo-M1ï¼‰ç”Ÿæˆï¼Œè¦†ç›–äº†å¤šç§è¡Œä¸ºé£æ ¼ã€‚\n- **æŒ‘æˆ˜æ€§**ï¼šåŒ…å«å›ºå®šé£æ ¼ã€è½¨è¿¹æ‹¼æ¥å’Œè½¨è¿¹å†…é£æ ¼å˜åŒ–ç­‰ä¸åŒåœºæ™¯ï¼Œç”¨äºæµ‹è¯•ç®—æ³•çš„**é•¿æœŸå†³ç­–**å’Œ**è½¨è¿¹ç¼åˆ**èƒ½åŠ›ã€‚\n- **æ ‡æ³¨çµæ´»æ€§**ï¼šä½¿ç”¨**å­è½¨è¿¹æ ‡æ³¨å‡½æ•°**ï¼Œå°†é£æ ¼å®šä¹‰ä¸ºåœ¨ç‰¹å®šæ—¶é—´çª—å£ï¼ˆè€Œéæ•´æ¡è½¨è¿¹ï¼‰ä¸Šæ»¡è¶³çš„æ ‡ç­¾ï¼Œç¼“è§£äº†ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡ä½¿ç”¨ä¸¤ä¸ªæ ¸å¿ƒæŒ‡æ ‡æ¥è¯„ä¼°ç®—æ³•æ€§èƒ½ï¼š\n\n1.  **é£æ ¼å¯¹é½åº¦ (Style Alignment)**\n    - **å®šä¹‰**ï¼šè¡¡é‡ç­–ç•¥ç”Ÿæˆçš„è¡Œä¸ºä¸ç›®æ ‡é£æ ¼æ ‡ç­¾ `z` çš„ä¸€è‡´æ€§ã€‚\n    - **è®¡ç®—æ–¹å¼**ï¼šå¯¹äºç”Ÿæˆçš„è½¨è¿¹ `Ï„`ï¼Œè®¡ç®—å…¶æ—¶é—´æ­¥ä¸­è¢«æ ‡æ³¨ä¸ºç›®æ ‡é£æ ¼ `z` çš„æ¯”ä¾‹ï¼ˆå…¬å¼16ï¼‰ï¼š\n      ```math\n      \\hat{S}^{\\mathbbm{1}}(\\lambda(\\tau), z) = \\frac{1}{|\\tau|} \\sum_{t=0}^{|\\tau|-1} \\mathbbm{1}\\{z_t = z\\}\n      ```\n    - **èŒƒå›´**ï¼š`[0, 1]`ï¼Œå€¼è¶Šé«˜è¡¨ç¤ºå¯¹é½è¶Šå¥½ã€‚å¯ç±»æ¯”ä¸ºç›®æ ‡æ¡ä»¶RLä¸­çš„æˆåŠŸç‡ã€‚\n\n2.  **ä»»åŠ¡æ€§èƒ½ (Task Performance)**\n    - **å®šä¹‰**ï¼šè¡¡é‡ç­–ç•¥åœ¨å®ŒæˆåŸå§‹RLä»»åŠ¡ï¼ˆå¦‚å¥”è·‘é€Ÿåº¦ã€ç»˜åˆ¶åœ†å½¢ç²¾åº¦ï¼‰ä¸Šçš„è¡¨ç°ã€‚\n    - **è®¡ç®—æ–¹å¼**ï¼šä½¿ç”¨ç¯å¢ƒå®šä¹‰çš„å¥–åŠ±å‡½æ•° `r(s, a)`ï¼Œè®¡ç®—æŠ˜æ‰£ç´¯ç§¯å›æŠ¥ `J(Ï€)`ï¼ˆå…¬å¼1ï¼‰ï¼Œå¹¶è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ä»¥ä¾¿æ¯”è¾ƒã€‚\n    - **ç›®æ ‡**ï¼šåœ¨ä¿æŒé«˜é£æ ¼å¯¹é½åº¦çš„å‰æä¸‹ï¼Œå°½å¯èƒ½æå‡ä»»åŠ¡æ€§èƒ½ã€‚\n\n### ä¸‰ã€æ€§èƒ½æå‡ç»“æœ\nè®ºæ–‡å°†æå‡ºçš„ **SCIQL** ç®—æ³•ä¸å¤šä¸ªåŸºçº¿æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ï¼ŒåŒ…æ‹¬ BCã€CBCã€BCPMIã€SORL å’Œ SCBCã€‚\n\n#### 1. é£æ ¼å¯¹é½æ€§èƒ½ï¼ˆæ ¸å¿ƒç»“æœï¼Œè§è¡¨1ï¼‰\n**SCIQL åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—æœ€ä¼˜çš„é£æ ¼å¯¹é½æ€§èƒ½ã€‚**\n\n| æ•°æ®é›† | SCIQL å¯¹é½åº¦ | æœ€ä½³åŸºçº¿å¯¹é½åº¦ (SORL/BCPMI) | ç›¸å¯¹æå‡ |\n| :--- | :--- | :--- | :--- |\n| `circle2d-inplace-v0` | **74.6%** | 68.6% (SCBC) | **+8.7%** |\n| `circle2d-navigate-v0` | **75.5%** | 67.2% (SCBC) | **+12.3%** |\n| `halfcheetah-fixed-v0` | **78.0%** | 58.1% (BCPMI) | **+34.3%** |\n| `halfcheetah-stitch-v0` | **78.0%** | 58.9% (BCPMI) | **+32.4%** |\n| `halfcheetah-vary-v0` | **78.9%** | 52.6% (BCPMI) | **+50.0%** |\n| `humenv-simple-v0` | **99.6%** | 99.6% (SCBC) | æŒå¹³ |\n| `humenv-complex-v0` | **83.5%** | 47.7% (SORL) | **+75.1%** |\n\n**å…³é”®ç»“è®º**ï¼š\n- **SCIQL ä¼˜åŠ¿æ˜æ˜¾**ï¼šåœ¨éœ€è¦é•¿æœŸå†³ç­–å’Œè½¨è¿¹ç¼åˆçš„å¤æ‚æ•°æ®é›†ï¼ˆå¦‚HalfCheetahç³»åˆ—å’ŒHumEnv-complexï¼‰ä¸Šï¼ŒSCIQL çš„ä¼˜åŠ¿å°¤å…¶å·¨å¤§ï¼Œè¯æ˜äº†å…¶**ä»·å€¼å‡½æ•°å­¦ä¹ **å’Œ**é£æ ¼é‡æ ‡æ³¨**æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚\n- **åŸºçº¿å±€é™æ€§**ï¼šä»…è¿›è¡Œæ¨¡ä»¿å­¦ä¹ ï¼ˆBC, CBC, BCPMIï¼‰æˆ–ç®€å•é‡æ ‡æ³¨ï¼ˆSCBCï¼‰çš„æ–¹æ³•ï¼Œåœ¨é£æ ¼ç¨€ç–æˆ–å˜åŒ–çš„æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚SORL åœ¨é£æ ¼å¯¹é½ä¼˜åŒ–ä¸Šä¹Ÿä¸åŠ SCIQLã€‚\n\n#### 2. é£æ ¼æ¡ä»¶ä¸‹çš„ä»»åŠ¡æ€§èƒ½ä¼˜åŒ–ï¼ˆè§å›¾2ï¼‰\n**Gated Advantage Weighted Regression (GAWR) æœºåˆ¶ä½¿ SCIQL èƒ½åœ¨ä¿æŒé«˜é£æ ¼å¯¹é½çš„åŒæ—¶ï¼Œæœ‰æ•ˆæå‡ä»»åŠ¡æ€§èƒ½ã€‚**\n\n- **å¯¹æ¯”å®éªŒ**ï¼šå°† SCIQL ä¸ SORL åœ¨ä¸åŒæƒè¡¡å‚æ•°ï¼ˆ`Î²`ï¼‰ä¸‹çš„è¡¨ç°ç»˜åˆ¶ä¸ºå¸•ç´¯æ‰˜å‰æ²¿ã€‚\n- **ç»“æœ**ï¼š\n    - **SCIQL (`Î»>r`)**ï¼šåœ¨å‡ ä¹æ‰€æœ‰ç¯å¢ƒä¸­ï¼Œåœ¨**å‡ ä¹ä¸æŸå¤±é£æ ¼å¯¹é½åº¦**çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†ä»»åŠ¡æ€§èƒ½å¾—åˆ†ã€‚\n    - **æ•´ä½“ä¼˜åŠ¿**ï¼šSCIQL çš„å¸•ç´¯æ‰˜å‰æ²¿**æ”¯é…äº† SORL çš„å¸•ç´¯æ‰˜å‰æ²¿**ï¼Œä¸”å…¶è¦†ç›–çš„**è¶…ä½“ç§¯ (Hypervolume) æ¯” SORL é«˜å‡º 41.2% åˆ° 163.9%**ï¼Œè¡¨æ˜ SCIQL åœ¨é£æ ¼å¯¹é½ä¸ä»»åŠ¡æ€§èƒ½çš„è”åˆä¼˜åŒ–ä¸Šè¾¾åˆ°äº†æ›´ä¼˜çš„å¹³è¡¡ã€‚\n    - **é²æ£’æ€§**ï¼šSCIQL çš„æ€§èƒ½å¯¹æ¸©åº¦å‚æ•° `Î²` ä¸æ•æ„Ÿï¼Œè€Œ SORL çš„æ€§èƒ½ä¸¥é‡ä¾èµ– `Î²` çš„ç²¾ç»†è°ƒèŠ‚ã€‚\n\n### æ€»ç»“\n- **æŠ€æœ¯åˆ›æ–°**ï¼šSCIQL é€šè¿‡å°†é£æ ¼å¯¹é½é—®é¢˜**é‡æ„ä¸ºç›®æ ‡æ¡ä»¶RLé—®é¢˜**ï¼Œå¹¶å¼•å…¥**é£æ ¼æ¡ä»¶éšå¼Qå­¦ä¹ **å’Œ**é—¨æ§ä¼˜åŠ¿åŠ æƒå›å½’**ï¼ŒæˆåŠŸè§£å†³äº†ç¦»çº¿æ•°æ®ä¸­é£æ ¼ç¨€ç–ã€é•¿æœŸä¿¡ç”¨åˆ†é…å’Œé£æ ¼-ä»»åŠ¡ç›®æ ‡å†²çªä¸‰å¤§æŒ‘æˆ˜ã€‚\n- **å®é™…ä»·å€¼**ï¼šå®éªŒåœ¨å¤šæ ·åŒ–çš„ä»¿çœŸç¯å¢ƒä¸­éªŒè¯äº† SCIQL çš„ä¼˜è¶Šæ€§ã€‚å…¶**é«˜é£æ ¼å¯¹é½åº¦**å’Œ**é«˜æ•ˆçš„ä»»åŠ¡æ€§èƒ½ä¼˜åŒ–**èƒ½åŠ›ï¼Œä½¿å…¶åœ¨éœ€è¦**å¯æ§ã€å¤šæ ·ä¸”é«˜æ€§èƒ½è¡Œä¸º**çš„åº”ç”¨åœºæ™¯ï¼ˆå¦‚æ¸¸æˆNPCã€æœºå™¨äººäº¤äº’ã€è‡ªåŠ¨é©¾é©¶ï¼‰ä¸­å…·æœ‰é‡è¦åº”ç”¨æ½œåŠ›ã€‚ä»£ç å’Œæ•°æ®é›†å·²å¼€æºï¼Œä¿è¯äº†å¯å¤ç°æ€§ã€‚",
            "timestamp": "2026-02-03T16:00:14.428457",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ã€ŠOffline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignmentã€‹çš„ä¸»è¦å±€é™æ€§ä¸æœªæ¥æ”¹è¿›æ–¹å‘åˆ†æ\n\n### ä¸€ã€ä¸»è¦å±€é™æ€§\n\n1.  **é£æ ¼å®šä¹‰ä¾èµ–äºäººå·¥è®¾è®¡çš„æ ‡æ³¨å‡½æ•°**\n    - è®ºæ–‡é‡‡ç”¨**æ•°æ®ç¼–ç¨‹ï¼ˆData Programmingï¼‰**èŒƒå¼ï¼Œé€šè¿‡äººå·¥è®¾è®¡çš„**æ ‡æ³¨å‡½æ•°ï¼ˆLabeling Functionsï¼‰** åœ¨å­è½¨è¿¹çª—å£ä¸Šç”Ÿæˆé£æ ¼æ ‡ç­¾ã€‚è™½ç„¶è¿™æé«˜äº†å¯è§£é‡Šæ€§å¹¶é™ä½äº†æ ‡æ³¨æˆæœ¬ï¼Œä½†æ ‡æ³¨å‡½æ•°çš„è®¾è®¡æœ¬èº«éœ€è¦é¢†åŸŸçŸ¥è¯†ï¼Œä¸”å…¶è¡¨è¾¾èƒ½åŠ›å—é™äºè®¾è®¡è€…çš„å…ˆéªŒå‡è®¾ã€‚å¯¹äºå¤æ‚ã€æŠ½è±¡æˆ–éš¾ä»¥ç”¨ç¡¬ç¼–ç è§„åˆ™æè¿°çš„é£æ ¼ï¼ˆå¦‚â€œä¼˜é›…â€ã€â€œæ¿€è¿›â€ï¼‰ï¼Œè¯¥æ–¹æ³•å¯èƒ½éš¾ä»¥æœ‰æ•ˆæ•æ‰ã€‚\n\n2.  **é£æ ¼é‡‡æ ·åˆ†å¸ƒçš„é€‰æ‹©éœ€è¦å¯å‘å¼è°ƒæ•´**\n    - åœ¨SCIQLç®—æ³•ä¸­ï¼Œè®­ç»ƒæ—¶ä»æ•°æ®é›†`Î»(ğ’Ÿ)`ä¸­é‡‡æ ·é£æ ¼æ ‡ç­¾`z`çš„ç­–ç•¥ï¼ˆå¦‚ä½¿ç”¨å‡åŒ€åˆ†å¸ƒ`p_r`æˆ–å½“å‰æ ‡ç­¾åˆ†å¸ƒ`p_c`ï¼‰å¯¹æœ€ç»ˆæ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼ˆè§é™„å½•E.2ï¼‰ã€‚è®ºæ–‡å‘ç°ï¼Œå¯¹äºæŸäº›ç®€å•ã€æ— éœ€è½¨è¿¹æ‹¼æ¥çš„é£æ ¼ï¼ˆå¦‚Circle2Dä¸­çš„`turn_direction`ï¼‰ï¼Œä½¿ç”¨`p_c`æ•ˆæœæ›´å¥½ï¼›è€Œå¯¹äºéœ€è¦é•¿æœŸå†³ç­–å’Œæ‹¼æ¥çš„é£æ ¼ï¼Œ`p_r`æ›´æœ‰æ•ˆã€‚**ç¼ºä¹ä¸€ä¸ªæ™®é€‚çš„ã€è‡ªé€‚åº”çš„é‡‡æ ·ç­–ç•¥**ï¼Œéœ€è¦é’ˆå¯¹ä¸åŒä»»åŠ¡å’Œé£æ ¼æ ‡å‡†è¿›è¡Œæ‰‹åŠ¨è°ƒæ•´æˆ–å®éªŒé€‰æ‹©ã€‚\n\n3.  **é—¨æ§ä¼˜åŠ¿åŠ æƒå›å½’ï¼ˆGAWRï¼‰çš„å¯å‘å¼æ€§è´¨**\n    - GAWRæœºåˆ¶é€šè¿‡Sigmoidå‡½æ•°é—¨æ§ä»»åŠ¡ä¼˜åŠ¿ä¿¡å·ï¼Œä»¥åœ¨ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½æ—¶ä¿æŠ¤é£æ ¼å¯¹é½ã€‚è™½ç„¶å®éªŒè¯æ˜å…¶æœ‰æ•ˆï¼Œä½†è¿™ç§è®¾è®¡æ˜¯**å¯å‘å¼ï¼ˆheuristicï¼‰** çš„ï¼Œç¼ºä¹ä¸¥æ ¼çš„ç†è®ºä¿è¯ï¼ˆä¾‹å¦‚ï¼Œè¯æ˜å…¶è¿‘ä¼¼äºåŸæ–‡å…¬å¼(7)æ‰€å®šä¹‰çš„åœ¨é£æ ¼æœ€ä¼˜ç­–ç•¥é›†ä¸Šçš„æŠ•å½±ï¼‰ã€‚å…¶æ€§èƒ½å¯èƒ½ä¾èµ–äºä¼˜åŠ¿å‡½æ•°çš„é‡çº²å’ŒSigmoidå‡½æ•°çš„é¥±å’Œç‰¹æ€§ã€‚\n\n4.  **å¯¹å¤šé£æ ¼æ ‡å‡†ï¼ˆMulti-Criteriaï¼‰çš„æ‰©å±•æ€§æœªå……åˆ†æ¢ç´¢**\n    - è®ºæ–‡å®éªŒæ¶‰åŠå¤šä¸ªç‹¬ç«‹çš„é£æ ¼æ ‡å‡†ï¼ˆå¦‚é€Ÿåº¦ã€è§’åº¦ã€é«˜åº¦ï¼‰ï¼Œä½†**æœªç ”ç©¶å¦‚ä½•åŒæ—¶ä¼˜åŒ–æˆ–ç»„åˆå¤šä¸ªé£æ ¼æ ‡å‡†**ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›æ™ºèƒ½ä½“åŒæ—¶æ»¡è¶³â€œå¿«é€Ÿä¸”å§¿æ€ä¼˜é›…â€ç­‰å¤šä¸ªé£æ ¼çº¦æŸã€‚å½“å‰çš„æ¡†æ¶å’Œç®—æ³•å¹¶æœªæä¾›å¤„ç†å¤šæ ‡å‡†è”åˆä¼˜åŒ–æˆ–æƒè¡¡çš„æœºåˆ¶ã€‚\n\n5.  **è®¡ç®—å¤æ‚åº¦ä¸å¯æ‰©å±•æ€§**\n    - SCIQLéœ€è¦åŒæ—¶è®­ç»ƒ**é£æ ¼æ¡ä»¶ä»·å€¼å‡½æ•°**å’Œ**ä»»åŠ¡æ¡ä»¶ä»·å€¼å‡½æ•°**ï¼Œå¹¶è¿›è¡ŒGAWRç­–ç•¥æå–ã€‚è¿™ç›¸æ¯”ç®€å•çš„è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰æˆ–æ¡ä»¶è¡Œä¸ºå…‹éš†ï¼ˆCBCï¼‰éœ€è¦æ›´å¤šçš„ç½‘ç»œå‚æ•°å’Œè®¡ç®—èµ„æºï¼ˆè®ºæ–‡ä¸­æåˆ°è®­ç»ƒæ—¶é—´æ›´é•¿ï¼‰ã€‚å¯¹äºéå¸¸é«˜ç»´çš„çŠ¶æ€/åŠ¨ä½œç©ºé—´ï¼ˆå¦‚è§†è§‰è¾“å…¥ï¼‰ï¼Œå…¶å¯æ‰©å±•æ€§æœ‰å¾…è¿›ä¸€æ­¥éªŒè¯ã€‚\n\n### äºŒã€æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\n1.  **å¢å¼ºé£æ ¼è¡¨ç¤ºçš„è‡ªåŠ¨åŒ–ä¸è¡¨è¾¾èƒ½åŠ›**\n    - **ç ”ç©¶æ–¹å‘**ï¼šæ¢ç´¢å¦‚ä½•**è‡ªåŠ¨å­¦ä¹ æˆ–å¢å¼ºé£æ ¼è¡¨ç¤º**ï¼Œå‡å°‘å¯¹äººå·¥è®¾è®¡æ ‡æ³¨å‡½æ•°çš„ä¾èµ–ã€‚\n    - **å…·ä½“è·¯å¾„**ï¼š\n        - ç»“åˆ**è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰** æˆ–**å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰**ï¼Œåˆ©ç”¨å…¶ä¸°å¯Œçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä»è‡ªç„¶è¯­è¨€æè¿°ä¸­ç”Ÿæˆæˆ–è¾…åŠ©å®šä¹‰é£æ ¼å¥–åŠ±ã€‚\n        - ç ”ç©¶**åŠç›‘ç£æˆ–å¼±ç›‘ç£**çš„é£æ ¼å‘ç°æ–¹æ³•ï¼Œä»…éœ€å°‘é‡äººå·¥æ ‡æ³¨æˆ–åé¦ˆï¼Œå³å¯å­¦ä¹ å‡ºæœ‰æ„ä¹‰çš„é£æ ¼éšç©ºé—´ã€‚\n        - å°†æ ‡æ³¨å‡½æ•°ä»ç¡¬ç¼–ç è§„åˆ™æ‰©å±•ä¸º**å¯å­¦ä¹ çš„ç¥ç»ç½‘ç»œæ¨¡å—**ï¼Œä½¿å…¶èƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªé€‚åº”åœ°æå–æ›´å¤æ‚çš„é£æ ¼ç‰¹å¾ã€‚\n\n2.  **å‘å±•è‡ªé€‚åº”çš„é£æ ¼é‡‡æ ·ä¸è¯¾ç¨‹å­¦ä¹ æœºåˆ¶**\n    - **ç ”ç©¶æ–¹å‘**ï¼šè®¾è®¡**è‡ªé€‚åº”ç®—æ³•**ï¼ŒåŠ¨æ€è°ƒæ•´è®­ç»ƒæ—¶çš„é£æ ¼æ ‡ç­¾é‡‡æ ·ç­–ç•¥ã€‚\n    - **å…·ä½“è·¯å¾„**ï¼š\n        - æ ¹æ®å½“å‰ç­–ç•¥åœ¨ç‰¹å®šé£æ ¼ä¸Šçš„å¯¹é½éš¾åº¦æˆ–æ•°æ®é›†ä¸­è¯¥é£æ ¼çš„ç¨€ç–ç¨‹åº¦ï¼Œ**åŠ¨æ€è°ƒæ•´é‡‡æ ·æ¦‚ç‡**ã€‚\n        - å¼•å…¥**è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰**ï¼Œä»æ˜“åˆ°éš¾åœ°è®©ç­–ç•¥å­¦ä¹ ä¸åŒé£æ ¼ï¼Œæˆ–ä»ç®€å•çš„é£æ ¼é‡‡æ ·ï¼ˆå¦‚`p_c`ï¼‰é€æ­¥è¿‡æ¸¡åˆ°éœ€è¦æ›´å¤šæ‹¼æ¥çš„é‡‡æ ·ï¼ˆå¦‚`p_r`ï¼‰ã€‚\n\n3.  **ä¸ºGAWRæä¾›æ›´åšå®çš„ç†è®ºæ¡†æ¶æˆ–æ›¿ä»£æ–¹æ¡ˆ**\n    - **ç ”ç©¶æ–¹å‘**ï¼šå¯»æ±‚æ›´**åŸåˆ™æ€§ï¼ˆprincipledï¼‰** çš„æ–¹æ³•æ¥å®ç°â€œåœ¨é£æ ¼å¯¹é½çº¦æŸä¸‹ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½â€çš„ç›®æ ‡ã€‚\n    - **å…·ä½“è·¯å¾„**ï¼š\n        - å°†é—®é¢˜å½¢å¼åŒ–ä¸º**çº¦æŸå¼ºåŒ–å­¦ä¹ ï¼ˆConstrained RLï¼‰** æˆ–**å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ï¼ˆMulti-Objective RLï¼‰**ï¼Œå¹¶è®¾è®¡ç›¸åº”çš„ç¦»çº¿ç®—æ³•ã€‚è¿™å¯èƒ½éœ€è¦å¼•å…¥æ‹‰æ ¼æœ—æ—¥ä¹˜å­æˆ–å­¦ä¹ åå¥½å‘é‡ã€‚\n        - æ¢ç´¢åŸºäº**è¿­ä»£å¼ç­–ç•¥æå–**æˆ–**æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰** çš„æ–¹æ³•ï¼Œåœ¨æ¯ä¸€æ­¥å†³ç­–ä¸­æ˜¾å¼åœ°è€ƒè™‘é£æ ¼çº¦æŸã€‚\n\n4.  **æ”¯æŒå¤šé£æ ¼æ ‡å‡†çš„è”åˆä¼˜åŒ–ä¸ç»„åˆæ§åˆ¶**\n    - **ç ”ç©¶æ–¹å‘**ï¼šæ‰©å±•æ¡†æ¶ä»¥å¤„ç†**å¤šä¸ªã€å¯èƒ½å†²çªçš„é£æ ¼æ ‡å‡†**ã€‚\n    - **å…·ä½“è·¯å¾„**ï¼š\n        - å°†é£æ ¼æ¡ä»¶æ‰©å±•ä¸º**å¤šç»´åº¦æ¡ä»¶** `z = (zâ‚, zâ‚‚, ...)`ï¼Œå¹¶ç ”ç©¶å¦‚ä½•åœ¨é«˜ç»´ç¦»æ•£æˆ–è¿ç»­é£æ ¼ç©ºé—´ä¸­è¿›è¡Œæœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ å’Œæ¡ä»¶ç”Ÿæˆã€‚\n        - å¼€å‘å…è®¸ç”¨æˆ·åœ¨æ¨ç†æ—¶é€šè¿‡**æ»‘åŠ¨æ¡ã€æƒé‡æˆ–è‡ªç„¶è¯­è¨€**å®æ—¶è°ƒæ•´ä¸åŒé£æ ¼é‡è¦æ€§ï¼ˆå¦‚â€œå¤šä¸€ç‚¹é€Ÿåº¦ï¼Œå°‘ä¸€ç‚¹ä¼˜é›…â€ï¼‰çš„äº¤äº’å¼ç³»ç»Ÿã€‚\n\n5.  **æå‡ç®—æ³•æ•ˆç‡ä¸æ¢ç´¢é›¶æ ·æœ¬æ³›åŒ–**\n    - **ç ”ç©¶æ–¹å‘**ï¼šä¼˜åŒ–ç®—æ³•æ•ˆç‡ï¼Œå¹¶æ¢ç´¢å…¶å¯¹**æœªè§é£æ ¼**çš„æ³›åŒ–èƒ½åŠ›ã€‚\n    - **å…·ä½“è·¯å¾„**ï¼š\n        - ç ”ç©¶**æ¨¡å‹æ¶æ„å…±äº«**å’Œ**è¿ç§»å­¦ä¹ **æŠ€æœ¯ï¼Œå‡å°‘ä¸ºä¸åŒé£æ ¼è®­ç»ƒç‹¬ç«‹å­ç½‘ç»œçš„å¼€é”€ã€‚\n        - æ¢ç´¢**é›¶æ ·æœ¬ï¼ˆZero-shotï¼‰** æˆ–**å°‘æ ·æœ¬ï¼ˆFew-shotï¼‰** é£æ ¼æ¡ä»¶ç­–ç•¥ç”Ÿæˆï¼Œä¾‹å¦‚é€šè¿‡é£æ ¼æè¿°æˆ–ç¤ºä¾‹è½¨è¿¹çš„åµŒå…¥æ¥å¿«é€Ÿé€‚é…æ–°é£æ ¼ï¼Œè€Œæ— éœ€é‡æ–°è¿›è¡Œå®Œæ•´çš„ç¦»çº¿RLè®­ç»ƒã€‚\n\n**æ€»ç»“**ï¼šè¯¥è®ºæ–‡åœ¨ç¦»çº¿é£æ ¼åŒ–ç­–ç•¥å­¦ä¹ æ–¹é¢åšå‡ºäº†æ‰å®çš„è´¡çŒ®ï¼Œä½†å…¶æ ¸å¿ƒå±€é™åœ¨äºé£æ ¼å®šä¹‰çš„â€œäººå·¥è®¾è®¡â€ä¾èµ–æ€§ã€å…³é”®ç»„ä»¶çš„å¯å‘å¼ç‰¹æ€§ä»¥åŠå¯¹å¤æ‚ã€å¤šé£æ ¼åœºæ™¯æ‰©å±•æ€§çš„ä¸è¶³ã€‚æœªæ¥çš„å·¥ä½œæœ‰æœ›é€šè¿‡å¼•å…¥æ›´è‡ªåŠ¨åŒ–çš„é£æ ¼å­¦ä¹ ã€æ›´åŸåˆ™åŒ–çš„ä¼˜åŒ–æ¡†æ¶ä»¥åŠå¯¹å¤šæ ‡å‡†ã€é›¶æ ·æœ¬æ³›åŒ–çš„æ”¯æŒï¼Œæ¥æ¨åŠ¨è¿™ä¸€é¢†åŸŸå‘æ›´é€šç”¨ã€æ›´å®ç”¨çš„æ–¹å‘å‘å±•ã€‚",
            "timestamp": "2026-02-03T16:01:05.850897",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T10:49:22Z",
    "created_at": "2026-02-03T15:53:07.163529",
    "updated_at": "2026-02-03T15:53:07.163539"
}