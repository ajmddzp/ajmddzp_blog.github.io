{
    "id": "2601.13657v1",
    "title": "Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning",
    "authors": [
        "Myong-Yol Choi",
        "Hankyoul Ko",
        "Hanse Cho",
        "Changseung Kim",
        "Seunghwan Kim",
        "Jaemin Seo",
        "Hyondong Oh"
    ],
    "abstract": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ§åˆ¶å™¨ï¼Œç”¨äºåœ¨é€šä¿¡å—é™ç¯å¢ƒä¸‹å®ç°æ— äººæœºé›†ç¾¤çš„ååŒå¯¼èˆªï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤æ‚å¤šéšœç¢ç‰©ç¯å¢ƒä¸­ç¨³å®šè¿è¡Œã€‚å—ç”Ÿç‰©é›†ç¾¤ä¸­ä¿¡æ¯ä¸ªä½“æ— éœ€æ˜¾å¼é€šä¿¡å³å¯å¼•å¯¼ç¾¤ä½“çš„å¯å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§éšå¼é¢†å¯¼è€…-è·Ÿéšè€…æ¡†æ¶ã€‚åœ¨è¯¥èŒƒå¼ä¸­ï¼Œä»…é¢†å¯¼è€…æŒæ¡ç›®æ ‡ä¿¡æ¯ï¼Œè·Ÿéšæ— äººæœºåˆ™ä»…é€šè¿‡æœºè½½æ¿€å…‰é›·è¾¾æ„ŸçŸ¥å­¦ä¹ é²æ£’ç­–ç•¥ï¼Œæ— éœ€ä»»ä½•ä¸ªä½“é—´é€šä¿¡æˆ–é¢†å¯¼è€…è¯†åˆ«ã€‚ç³»ç»Ÿåˆ©ç”¨æ¿€å…‰é›·è¾¾ç‚¹äº‘èšç±»ä¸æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨å®ç°ç¨³å®šçš„é‚»å±…è¿½è¸ªï¼Œæä¾›ä¸ä¾èµ–å¤–éƒ¨å®šä½ç³»ç»Ÿçš„å¯é æ„ŸçŸ¥èƒ½åŠ›ã€‚æ–¹æ³•çš„æ ¸å¿ƒæ˜¯é€šè¿‡åœ¨GPUåŠ é€Ÿçš„è‹±ä¼Ÿè¾¾Isaac Simä¸­è®­ç»ƒçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ§åˆ¶å™¨ï¼Œä½¿è·Ÿéšè€…ä»…ä¾é å±€éƒ¨æ„ŸçŸ¥å³å¯å­¦ä¹ å¤æ‚æ¶Œç°è¡Œä¸ºâ€”â€”åœ¨é›†ç¾¤èšé›†ä¸é¿éšœä¹‹é—´å®ç°åŠ¨æ€å¹³è¡¡ã€‚è¿™ä½¿å¾—é›†ç¾¤èƒ½å¤Ÿéšå¼è·Ÿéšé¢†å¯¼è€…ï¼ŒåŒæ—¶é²æ£’åº”å¯¹é®æŒ¡å’Œè§†åœºå—é™ç­‰æ„ŸçŸ¥æŒ‘æˆ˜ã€‚é€šè¿‡å¤§é‡ä»¿çœŸå®éªŒåŠäº”æ¶æ— äººæœºé›†ç¾¤åœ¨å®¤å†…å¤–å¤šç§ç¯å¢ƒä¸‹çš„å®é™…æµ‹è¯•ï¼Œæœ¬æ–¹æ³•åœ¨æ— é€šä¿¡ã€æ— å¤–éƒ¨å®šä½çš„æ¡ä»¶ä¸‹æˆåŠŸå®ç°äº†ååŒå¯¼èˆªï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§ä¸ä»¿çœŸåˆ°ç°å®çš„è¿ç§»èƒ½åŠ›ã€‚",
    "url": "https://arxiv.org/abs/2601.13657v1",
    "html_url": "https://arxiv.org/html/2601.13657v1",
    "html_content": "Communication-Free Collective Navigation for a Swarm of UAVs\nvia LiDAR-Based Deep Reinforcement Learning\nMyong-Yol Choi, Hankyoul Ko, Hanse Cho, Changseung Kim, Seunghwan Kim, Jaemin Seo,\nand Hyondong Oh\nThis research was supported by Theater Defense Research Center funded by Defense Acquisition Program Administration under Grant UD200043CD, National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (2023R1A2C2003130), and Unmanned Vehicles Core Technology Research and Development Program through the National Research Foundation of Korea (NRF) and Unmanned Vehicle Advanced Research Center (UVARC) funded by the Ministry of Science and ICT, the Republic of Korea (2020M3C1C1A01082375).\n(Myong-Yol Choi led the project, and Myong-Yol Choi and Hankyoul Ko contributed equally to this work.)(Corresponding author: Hyondong Oh.)\nM.-Y. Choi, H. Ko, H. Cho, C. Kim, S. Kim, and J. Seo are with the Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Ulsan, Korea (e-mail: mychoi@unist.ac.kr; kyoul@unist.ac.kr; joahdzl@unist.ac.kr; pon02124@unist.ac.kr; kevin6960@unist.ac.kr; qkek1019@unist.ac.kr).H. Oh is with the Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea (e-mail: h.oh@kaist.ac.kr).\nAbstract\nThis paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UAV) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments. Inspired by biological swarms where informed individuals guide groups without explicit communication, we employ an implicit leader-follower framework. In this paradigm, only the leader possesses goal information, while follower UAVs learn robust policies using only onboard LiDAR sensing, without requiring any inter-agent communication or leader identification. Our system utilizes LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, providing reliable perception independent of external positioning systems. The core of our approach is a DRL controller, trained in GPU-accelerated Nvidia Isaac Sim, that enables followers to learn complex emergent behaviorsâ€”balancing flocking and obstacle avoidanceâ€”using only local perception. This allows the swarm to implicitly follow the leader while robustly addressing perceptual challenges such as occlusion and limited field-of-view. The robustness and sim-to-real transfer of our approach are confirmed through extensive simulations and challenging real-world experiments with a swarm of five UAVs, which successfully demonstrated collective navigation across diverse indoor and outdoor environments without any communication or external localization.\nI\nIntroduction\nI-A\nBackground and Motivation\nUnmanned aerial vehicle (UAV) swarms have demonstrated remarkable potential across diverse applications including search and rescue, surveillance, environmental monitoring, and precision agriculture due to their mobility, flexibility, and ability to access hard-to-reach areas\n[\n1\n]\n. By leveraging the complementary sensing capabilities and spatial coverage of multiple agents, UAV swarms can accomplish complex missions that are difficult or impossible for individual UAVs to achieve efficiently\n[\n2\n]\n. Collective navigation, where multiple UAVs move together toward common destinations while maintaining group cohesion, has emerged as a key enabling capability for these applications\n[\n3\n]\n.\nThe foundations of collective motion have been extensively studied across multiple disciplines. Reynolds\n[\n4\n]\npioneered distributed behavioral models using three fundamental rules: collision avoidance, velocity matching, and flock centering. Vicsek and Zafeiris\n[\n5\n]\nlater demonstrated that large-scale order can emerge from a much simpler rule: local velocity matching with noise. Olfati-Saber\n[\n6\n]\nprovided rigorous mathematical analysis introducing collective potential functions and\nÎ±\n\\alpha\n-lattice structures for multi-agent coordination.\nFigure 1:\nAn example of real-world validation of the proposed LiDAR-based collective navigation. (a) A swarm of five UAVs on standby for takeoff. (b) Trajectories of the swarm reconstructed from onboard LiDAR. (c) Sequential snapshots of the swarm avoiding obstacles to reach a goal without external localization or communication. More details can be found in the attached video at https://youtu.be/U4i3Spisugg.\nHowever, real-world deployment of UAV swarms faces critical challenges in communication-constrained environments. Inter-UAV communication can become unavailable or unreliable due to electronic jamming, signal interference, or natural disasters\n[\n7\n]\n. Moreover, even when communication is available, the bandwidth requirements and latency constraints for large-scale swarms can limit scalability and real-time coordination\n[\n8\n]\n. In such scenarios, communication-dependent methods become ineffective, necessitating robust communication-free collective navigation capabilities that rely solely on onboard perception and local decision-making.\nBiological studies have provided significant insights into communication-free collective navigation. Berdahl\net al.\n[\n9\n]\nidentified five key mechanisms in animal collective behavior: the many wrongs principle (averaging individual errors), leadership (guidance by knowledgeable individuals), emergent sensing (pooling sensory information), social learning, and collective learning. Particularly relevant to this work, Couzin\net al.\n[\n10\n]\ndemonstrated that effective group navigation can emerge from simple local interactions, where a small number of informed individuals can guide entire groups toward a destination without explicit communication or identification. This information asymmetry offers significant practical advantages for UAV swarms: it eliminates the need for mission broadcasting to all agents, reduces computational overhead, and enables scenarios where mission updates can only reach a subset of accessible UAVs. In communication-denied environments, where distributing goal information to the entire swarm may be infeasible, this approach becomes essential.\nThis biological phenomenon directly inspires the implicit leader-follower framework proposed in this paper. In this framework, follower UAVs navigate without goal information or explicit leader identification. Instead, followers learn to maintain cohesion with neighboring UAVs through local perception. As the informed leader navigates toward the goal, the entire swarm naturally moves in that direction through the followersâ€™ learned cohesion behavior, without requiring explicit coordination or mission broadcasting. This emergent coordination eliminates the need for leader identification or goal information while preserving robust collective navigation.\nI-B\nRelated Works\nI-B\n1\nCommunication-Dependent Swarm Navigation\nCurrent UAV swarm implementations for collective navigation are broadly categorized into centralized and distributed approaches based on coordination architectures. Centralized approaches rely on external infrastructure for global coordination and precise state estimation. Representative systems include motion capture-based control such as Crazyswarm\n[\n11\n]\nand nonlinear model predictive control\n[\n12\n]\n, which achieve high-precision indoor navigation in obstacle-known environments by leveraging ground control stations for agent tracking. While demonstrating excellent coordination accuracy, their infrastructure dependence limits applicability to controlled indoor settings and causes network collapse upon infrastructure failure.\nDistributed methods achieve coordination through decentralized decision-making without centralized control, typically requiring inter-agent communication for state or trajectory sharing. MADER\n[\n13\n]\nsegments trajectory planning into perception, polyhedral representation, and optimization for multi-agent coordination. EGO-Swarm\n[\n14\n]\nemploys optimization-based methods partitioned into mapping, planning, and control modules, enabling autonomous flight in cluttered forests through trajectory information sharing. Zhou\net al.\n[\n15\n]\nintroduce a spatial-temporal optimization framework for fully autonomous UAV swarms navigating dense, unknown environments without external facilities. VÃ¡sÃ¡rhelyi\net al.\n[\n16\n]\ndemonstrate evolutionary optimization for outdoor flocking using GNSS localization and wireless inter-agent communication. Recent learning-based approaches include deep reinforcement learning (DRL) for collision avoidance in fixed-wing UAVs\n[\n17\n]\nand end-to-end DRL for quadrotor swarms mapping local observations to motor thrusts, enabling zero-shot transfer to real robots in obstacle-dense environments\n[\n18\n]\n. However, these distributed methods fundamentally rely on continuous inter-agent communication for coordinationâ€”whether for trajectory sharing or state information exchange. Additionally, they typically assume either global mission knowledge across all agents or the ability to explicitly identify and communicate with informed leaders, limiting their applicability when communication is infeasible.\nI-B\n2\nPerception Strategies for Communication-Free Swarm Navigation\nTo enable operation when communication is unavailable, communication-free approaches employ onboard sensor-based perception for surrounding UAV detection and autonomous coordination, eliminating inter-agent communication dependency. Vision-based approaches have been extensively explored through diverse methodologies. Saska\net al.\n[\n19\n]\nachieved relative localization using monocular cameras with circular patterns in GNSS-denied environments. Schilling\net al.\n[\n20\n]\nutilized six omnidirectional cameras without position sharing or visual markers, with subsequent work\n[\n21\n]\nachieving markerless outdoor flocking with three UAVs. Ahmad\net al.\n[\n22\n]\nvalidated PACNav using ultraviolet-based relative sensing for navigation with four UAVs in natural forests. Wang\net al.\n[\n23\n]\nimplemented a bio-inspired visual projection field (VPF) approach with six UAVs in controlled indoor environments.\nHowever, vision-based systems face limitations. First, they suffer from illumination dependency, with performance degrading under varying lighting conditions including sunlight, shadows, or darkness. Second, monocular vision requires additional processing or motion to resolve scale ambiguity in depth estimation. Third, omnidirectional perception requires multiple cameras, critically increasing system complexity through increased onboard computational demands for processing multiple video streams and complex inter-camera calibration and synchronization. These factors become particularly prohibitive for small UAV platforms with limited computation budgets.\nLiDAR-based approaches provide promising alternatives that directly address these vision-based limitations. Unlike vision systems, LiDAR offers illumination-invariant perception, maintaining consistent performance across diverse lighting conditions. The direct time-of-flight measurements eliminate depth ambiguity inherent in monocular vision, while a single 360-degree scanning LiDAR achieves omnidirectional awareness without the computational overhead, multi-stream processing, and complex calibration requirements of multi-camera systems. These characteristics make LiDAR particularly suitable for resource-constrained UAV platforms. Recent work has begun exploring LiDAR for swarm coordination. Swarm-LIO2\n[\n24\n]\ndemonstrates a fully decentralized LiDAR-inertial state estimation system for aerial swarms, achieving robust neighbor detection through reflective tape that produces distinctive high-intensity returns in LiDAR reflectivity measurements. The system has been validated with five UAVs in real-world experiments, demonstrating the practicality of LiDAR-based perception for swarm coordination.\nI-B\n3\nControl Strategies for Communication-Free Swarm Navigation\nWhile communication-free perception enables neighbor detection without information exchange, achieving truly autonomous coordination requires control strategies that make navigation decisions based purely on local sensory observations, without communicated state information. This includes challenges of real-time occlusion handling, limited field-of-view (FOV), complex swarm interactions, and obstacle avoidance using only onboard sensor data. Traditional heuristic methods, such as Reynoldsâ€™ principles\n[\n4\n]\nand potential field-based obstacle avoidance\n[\n6\n]\n, provide stable flocking without communication but exhibit limitations. Manually designing fixed rules that generalize across diverse environmental conditionsâ€”including varying obstacle densities, swarm sizes, and spatial constraintsâ€”is extremely challenging. These rule-based approaches often suffer from local minima where conflicting objectives trap agents in suboptimal behaviors\n[\n6\n]\nand cannot adaptively adjust to dynamic swarm configurations\n[\n21\n,\n22\n,\n23\n]\n, often resulting in overly conservative maneuvers.\nLearning-based approaches emerged to address these limitations, enabling agents to learn adaptive control policies for balancing flocking and obstacle avoidance in communication-free settings. Imitation learning trains UAV policies using expert demonstrations, partially mitigating heuristic rule rigidity, but is constrained by the quality and coverage of expert data\n[\n20\n,\n25\n]\n. Acquiring high-quality demonstration data for communication-free coordination is inherently challenging, requiring experts to manually demonstrate complex behaviors under diverse conditions with partial observability.\nDRL offers a powerful alternative, overcoming both heuristic rule rigidity and imitation learningâ€™s data dependency by enabling agents to learn policies through direct environmental interaction. DRL can automatically discover strategies to balance multiple competing objectivesâ€”maintaining cohesion, avoiding collisions, and handling perception uncertaintiesâ€”through reward-driven optimization, without requiring explicit rule design or expert demonstrations. Huang\net al.\n[\n26\n]\nproposed vision-based decentralized collision avoidance using depth images and DRL for multi-UAV navigation without inter-UAV communication, but validated only in obstacle-free simulation environments. Bai\net al.\n[\n27\n]\nextended this for communication-denied environments with limited visual fields, but validated in simulations with simplified dynamics and pre-known obstacles, limiting real-world applicability assessment. As existing learning-based approaches are mostly validated in simulations, a critical gap in real-world validation remains, which this study addresses.\nI-C\nContributions\nTo overcome these limitations, we propose a novel LiDAR-based collective navigation system for robust operation in GNSS-denied and communication-denied environments. Building upon\n[\n24\n]\n, we eliminate all communication dependencies by enabling coordination through purely local sensing. Each UAV uses a single LiDAR sensor for 360-degree neighbor detection, removing the need for external localization or inter-agent communication. We employ an implicit leader-follower architecture where only the leader possesses goal information, while followers coordinate through learned reactive behaviors. Follower control policies are trained via DRL using proximal policy optimization (PPO)\n[\n28\n]\nto learn flocking and obstacle avoidance solely from local LiDAR observations. As an on-policy algorithm, PPO enables synchronous data collection from multiple UAVs and efficient batch updates in GPU-accelerated parallel environments. The DRL training enables followers to autonomously discover control strategies that balance cohesion with neighbors and collision avoidance with obstacles. Through this learned flocking behavior, the swarm collectively navigates to the destination as the leader moves toward the goal, without requiring explicit leader identification or goal information. The proposed system is validated through extensive simulations and real-world experiments involving five UAVs. A sample result of the real-world experiments is depicted in Fig.\n1\n. To the best of our knowledge, this is the first LiDAR-based collective navigation system for a swarm of UAVs using DRL that operates without any information exchange and has been validated in real-world deployments. The key contributions are:\ni)\nA fully communication-free, LiDAR-based perception framework for neighbor detection and tracking to achieve robust UAV swarm coordination;\nii)\nA DRL-based control policy that enables implicit leader-follower coordination by learning to balance flocking and obstacle avoidance under realistic perception constraints; and\niii)\nComprehensive validation via extensive simulations and real-world experiments with five UAVs in diverse indoor and outdoor environments.\nII\nProblem Formulation\nII-A\nProblem Definition\nThis study addresses the collective navigation of a swarm of\nN\nN\nUAVs toward a specific destination in a GNSS-denied and communication-constrained environment. As illustrated in Fig.\n2\n, the swarm consists of one informed leader UAV, which possesses destination information via pre-loaded waypoints, and\nN\nâˆ’\nN-\n1 uninformed follower UAVs without such information. We assume a complete absence of information exchange, meaning each UAV cannot explicitly distinguish whether other UAVs are leaders or followers, requiring an implicit leader-follower framework. Each follower must utilize its onboard LiDAR sensors to perceive its surroundings in real-time, making autonomous decisions based only on locally observed information. The core challenge is for the swarm to maintain cohesion and effectively avoid obstacles, moving collectively toward the destination without dispersing, while addressing perceptual challenges such as occlusion and limited FOV.\nII-B\nSystem Models\nThe state of each UAV\ni\ni\nis its local position\nğ©\nt\ni\nâˆˆ\nâ„\n3\n\\mathbf{p}^{i}_{t}\\in\\mathbb{R}^{3}\n, velocity\nğ¯\nt\ni\nâˆˆ\nâ„\n3\n\\mathbf{v}^{i}_{t}\\in\\mathbb{R}^{3}\n, and orientation (quaternion\nğª\nt\ni\nâˆˆ\nâ„\n4\n\\mathbf{q}^{i}_{t}\\in\\mathbb{R}^{4}\n) at time\nt\nt\n. Each UAV\ni\ni\nhas a LiDAR sensor providing a raw point cloud\nğ³\nt\ni\n\\mathbf{z}^{i}_{t}\nof its surroundings, which feeds into a perception module (Sec.\nIII-A\n). From this data, UAV\ni\ni\nâ€™s state\n(\nğ©\nt\ni\n,\nğ¯\nt\ni\n,\nğª\nt\ni\n)\n(\\mathbf{p}^{i}_{t},\\mathbf{v}^{i}_{t},\\mathbf{q}^{i}_{t})\nis estimated by LiDAR-inertial odometry (LIO). Concurrently, the perception module processes\nğ³\nt\ni\n\\mathbf{z}^{i}_{t}\nto detect and track neighbors\nğ’©\nt\ni\n\\mathcal{N}^{i}_{t}\nand obstacles\nğ’ª\nt\ni\n\\mathcal{O}^{i}_{t}\n.\nWhile all UAVs share the same kinematic model for position updates,\nğ©\nt\n+\n1\ni\n=\nğ©\nt\ni\n+\nğ¯\nt\ni\nâ€‹\nÎ”\nâ€‹\nt\n,\n\\mathbf{p}^{i}_{t+1}=\\mathbf{p}^{i}_{t}+\\mathbf{v}^{i}_{t}\\Delta t,\ntheir control policies for determining velocity commands differ by role.\nThe leaderâ€™s velocity is determined by a local planner\nL\nL\nusing its estimated state, perceived obstacles, and predefined waypoints\nW\nW\n:\nğ¯\nt\n+\n1\nl\n=\nL\nâ€‹\n(\nğ©\nt\nl\n,\nğª\nt\nl\n,\nğ’ª\nt\nl\n,\nW\n)\n.\n\\mathbf{v}^{l}_{t+1}=L(\\mathbf{p}^{l}_{t},\\mathbf{q}^{l}_{t},\\mathcal{O}^{l}_{t},W).\nEach follower learns a control policy\nÏ€\n\\pi\n, detailed in Sec.\nIII-B\n, that generates velocity commands based only on its estimated state and local perception of neighbors and obstacles:\nğ¯\nt\n+\n1\nf\n=\nÏ€\nâ€‹\n(\nğ¯\nt\nf\n,\nğª\nt\nf\n,\nğ’©\nt\nf\n,\nğ’ª\nt\nf\n)\n.\n\\mathbf{v}^{f}_{t+1}=\\pi(\\mathbf{v}^{f}_{t},\\mathbf{q}^{f}_{t},\\mathcal{N}^{f}_{t},\\mathcal{O}^{f}_{t}).\nThe followerâ€™s policy\nÏ€\n\\pi\nintentionally excludes position\nğ©\nt\nf\n\\mathbf{p}^{f}_{t}\nas an input, even though it is available from LIO. This design is critical for learning a generalizable policy. Including\nğ©\nt\nf\n\\mathbf{p}^{f}_{t}\ncould cause the policy to learn correlations between coordinates within the locally generated map and required control actions. Such map-dependent policies would fail in novel environments with different trajectories and layouts. By restricting input to the UAVâ€™s kinematic stateâ€”velocity\nğ¯\nt\nf\n\\mathbf{v}^{f}_{t}\nand orientation\nğª\nt\nf\n\\mathbf{q}^{f}_{t}\nâ€”along with local perception of neighbors\nğ’©\nt\nf\n\\mathcal{N}^{f}_{t}\nand obstacles\nğ’ª\nt\nf\n\\mathcal{O}^{f}_{t}\n, the policy learns a robust, egocentric strategy based on immediate kinematic state and surroundings rather than position relative to an arbitrary starting point, significantly enhancing transferability.\nFigure 2:\nScenario of the communication-free collective navigation problem. A leader visits a known waypoint sequence, while followers, unaware of the goal or leaderâ€™s identity, use only local perception for flocking and obstacle avoidance to indirectly follow the leader.\nII-C\nFlocking Considerations for Implicit Following\nFlocking refers to coordinated collective motion emerging when individual agents follow simple local interaction rules\n[\n4\n]\n. Traditional flocking behaviors consist of three fundamental rules: cohesion for staying close to neighboring UAVs, separation for maintaining safe distances to avoid collisions with neighbors, and alignment for matching velocity directions with neighbors.\nHowever, we deliberately exclude the alignment term from our flocking behavior. This design choice is crucial for our communication-free implicit leader-follower scenario, where followers cannot identify the leader and the leader may exhibit diverse motion patternsâ€”including hovering, directional changes, or backward movementâ€”depending on mission objectives and obstacle configurations. Incorporating velocity alignment would cause followers to align with the average velocity direction of all perceived neighbors rather than specifically following the leaderâ€™s directional changes. Since followers cannot distinguish the leader, when the leader suddenly changes direction to navigate toward a new location, followers maintaining alignment with neighborsâ€™ average velocity would dilute the leaderâ€™s influence, potentially causing loss of the leader and mission failure. Therefore, our flocking behavior incorporates only cohesion and separation, allowing followers to navigate collectively while being responsive to the leaderâ€™s positional changes rather than being constrained by the swarmâ€™s average velocity direction.\nII-D\nObjectives\nThe primary objective of this study is to develop a robust control policy,\nÏ€\n\\pi\n, trained via DRL. Using only onboard LiDAR data, this policy must enable followers to learn a balance between two types of local behaviors: flocking (cohesion and separation) and obstacle avoidance. The successful execution of these behaviors results in the emergent behavior of implicit leader-following, where the swarm naturally moves toward the destination as followers maintain positional cohesion with neighbors, which in turn are influenced by the leaderâ€™s goal-directed motion. Ultimately, the system aims to achieve collective movement in a fully communication-free manner.\nIII\nMethodology\nTo address the problem defined in Sec.\nII\n, the proposed system is built upon a decentralized control architecture where each UAV independently perceives its environment and makes decisions. This architecture is composed of two core modules: a LiDAR-based perception system (Fig.\n3\n) that detects neighboring UAVs and obstacles in real-time, and a DRL control policy (Fig.\n4\n) that learns to balance the competing demands of flocking and obstacle avoidance. The perception system generates local observations that are fed as an input to the control policy, which in turn outputs velocity commands. The integration of these modules enables the emergent behavior of stable collective navigation, allowing the entire swarm to reach a target destination known only to the leader, without explicit communication or reliance on an external positioning system.\nFigure 3:\nOverview of the onboard perception system. (a) The perception pipeline. (b) The object tracker filters raw LiDAR points, clusters them using DBSCAN, tracks them with an EKF, and validates them based on temporal consistency.\nIII-A\nLiDAR-Based Perception System\nThe proposed LiDAR-based perception pipeline consists of three main components: (1) a LIO module for ego-state estimation, (2) an object tracker module for neighbor detection and tracking, and (3) a point downsampling module that transforms the raw point cloud into low-dimensional features for computational efficiency. The resulting perception outputs are fed into the neural network. Each UAV is equipped with reflective tape to produce high-intensity LiDAR returns, crucial for detection. Based on the high-intensity returns, the object tracker operates in three stages with all geometry computed in the local map frame: filtering, clustering/tracking, and validation.\nIII-A\n1\nPoint Cloud Filtering\nThe first stage filters the raw point cloud of each UAV\ni\ni\n,\nğ³\nt\ni\n\\mathbf{z}^{i}_{t}\n, as shown in Fig.\n3\n(b)-1. To improve point density, we stack the most recent\nB\nB\npoint clouds. Each point\nğ©\nb\n{{}^{b}}\\mathbf{p}\nis transformed into the map frame using the transformation\nğ©\nm\n=\nT\nm\nâ†\nb\nâ€‹\nğ©\nb\n{{}^{m}}\\mathbf{p}=T_{m\\leftarrow b}\\,{{}^{b}}\\mathbf{p}\n, where\nT\nm\nâ†\nb\nT_{m\\leftarrow b}\nis obtained from the LIO module and subsequently gated by its Euclidean distance to the UAV position, retaining only points within range\n[\nd\nmin\n,\nd\nmax\n]\n[d_{\\min},d_{\\max}]\n.\nThe resulting distance-gated points,\nğ’«\nd\n\\mathcal{P}_{d}\n, are filtered based on two criteria. First, we identify high-intensity points\nğ’«\nhigh\n\\mathcal{P}_{\\text{high}}\n. Since reflective tape provides high-intensity returns, points with intensity\nI\nI\ngreater than or equal to threshold\nI\nhigh\nI_{\\text{high}}\nare utilized as key seeds to detect new objects.\nğ’«\nhigh\n=\n{\nğ©\nâˆˆ\nğ’«\nd\n|\nI\nâ‰¥\nI\nhigh\n}\n.\n\\mathcal{P}_{\\text{high}}=\\{\\,\\mathbf{p}\\in\\mathcal{P}_{d}\\;|\\;I\\geq I_{\\text{high}}\\,\\}.\nSecond, we retain region of interest (ROI) points\nğ’«\nroi\n\\mathcal{P}_{\\text{roi}}\nto maintain tracks during temporary occlusions. From remaining low-intensity points\nğ’«\nlow\n\\mathcal{P}_{\\text{low}}\n, we select those within radius\nr\nroi\nr_{\\text{roi}}\nof existing track centroids\nğœ\n^\nj\nm\n{{}^{m}}\\hat{\\mathbf{c}}^{j}\n.\nğ’«\nroi\n=\n{\nğ©\nâˆˆ\nğ’«\nlow\n|\nmin\nj\nâ¡\nâ€–\nğ©\nm\nâˆ’\nğœ\n^\nj\nm\nâ€–\n2\nâ‰¤\nr\nroi\n}\n.\n\\mathcal{P}_{\\text{roi}}=\\{\\,\\mathbf{p}\\in\\mathcal{P}_{\\text{low}}\\;|\\;\\min_{j}\\|{{}^{m}}\\mathbf{p}-{{}^{m}}\\hat{\\mathbf{c}}^{j}\\|_{2}\\leq r_{\\text{roi}}\\}.\nFinally, the union\nğ’«\nfiltered\n=\nğ’«\nhigh\nâˆª\nğ’«\nroi\n\\mathcal{P}_{\\text{filtered}}=\\mathcal{P}_{\\text{high}}\\cup\\mathcal{P}_{\\text{roi}}\nis passed to the next stage as shown in Fig.\n3\n(b)-2.\nIII-A\n2\nClustering and Tracking\nThe filtered point set\nğ’«\nfiltered\n\\mathcal{P}_{\\text{filtered}}\nis grouped into individual clusters\nC\nk\nC^{k}\nusing the density-based spatial clustering of applications with noise (DBSCAN) algorithm\n[\n29\n]\n, which forms clusters based on a distance threshold\nÎµ\n\\varepsilon\ndefining the maximum allowable distance between points and a minimum number of points\nn\nmin\nn_{\\text{min}}\n. Since point density decreases with distance, we apply distance-adaptive validation where the minimum point threshold scales with range. Each cluster is tracked using the extended Kalman filter (EKF)\n[\n30\n]\nwith a constant velocity model to estimate smooth motion trajectories of neighboring UAVs, as depicted in Fig.\n3\n(b)-3. Data association matches each valid cluster to the nearest existing track if their distance is below threshold\nd\nmatch\nd_{\\text{match}}\n. An associated track is updated via EKF, and a new track is created for a new cluster. A track is deactivated if unobserved for timeout\nt\ninactive\nt_{\\text{inactive}}\n.\nTABLE I:\nParameters for the LiDAR-Based Perception System\nParameter\nValue\nDescription\nPoint Cloud Filtering\nB\nB\n2\nNumber of stacked point clouds\nd\nmin\nd_{\\min}\n0.05m\nMinimum distance for point gating\nd\nmax\nd_{\\max}\n10.0m\nMaximum distance for point gating\nI\nhigh\nI_{\\text{high}}\n170\nThreshold for LiDAR intensity (0â€“255) for highly reflective points\nr\nroi\nr_{\\text{roi}}\n0.3m\nROI radius around existing tracks\nClustering and Tracking\nÎµ\n\\varepsilon\n0.1m\nDistance threshold for DBSCAN clustering\nn\nmin\nn_{\\text{min}}\n8\nâ€‹\nB\n8B\nMinimum points per cluster (scaled by stacked frames)\nd\nmatch\nd_{\\text{match}}\n0.2m\nAssociation threshold for matching clusters to tracks\nt\ninactive\nt_{\\text{inactive}}\n0.5s\nTimeout for deactivating unobserved tracks\nCluster Validation\nÏ\nhigh\n\\rho_{\\text{high}}\n0.05\nHigh-intensity ratio threshold for validation\nÏ„\non\n\\tau_{\\text{on}}\n0.01s\nContinuous duration for validation\nIII-A\n3\nCluster Validation\nThe final stage validates if a tracked object is a reliable neighbor UAV as shown in Fig.\n3\n(b)-4. A track is confirmed as a reliable neighbor if the ratio of high-intensity points within its cluster exceeds threshold\nÏ\nhigh\n\\rho_{\\text{high}}\nfor continuous duration\nÏ„\non\n\\tau_{\\text{on}}\n. The set of all validated neighbors at time\nt\nt\nconstitutes the perception output\nğ’©\nt\n\\mathcal{N}_{t}\n, used by the control policy.\nIII-B\nDeep Reinforcement Learning Framework\nThe follower UAVs learn a decentralized control policy using a DRL framework, as illustrated in Fig.\n4\n. We model the followerâ€™s control problem as a partially observable Markov decision process (POMDP), formally defined by the 7-tuple\n(\nğ’®\n,\nğ’œ\n,\nğ’¯\n,\nâ„›\n,\nÎ©\n,\nğ’ª\n,\nÎ³\n)\n(\\mathcal{S},\\mathcal{A},\\mathcal{T},\\mathcal{R},\\Omega,\\mathcal{O},\\gamma)\n, where\nğ’®\n\\mathcal{S}\nis the state space,\nğ’œ\n\\mathcal{A}\nis the action space,\nğ’¯\n:\nğ’®\nÃ—\nğ’œ\nÃ—\nğ’®\nâ†’\n[\n0\n,\n1\n]\n\\mathcal{T}:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow[0,1]\nis the state transition function,\nâ„›\n:\nğ’®\nÃ—\nğ’œ\nâ†’\nâ„\n\\mathcal{R}:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}\nis the reward function,\nÎ©\n\\Omega\nis the observation space,\nğ’ª\n:\nğ’®\nâ†’\nÎ©\n\\mathcal{O}:\\mathcal{S}\\rightarrow\\Omega\nis the observation function, and\nÎ³\nâˆˆ\n[\n0\n,\n1\n]\n\\gamma\\in[0,1]\nis the discount factor. In our model-free DRL approach, the agent does not explicitly know\nğ’®\n\\mathcal{S}\n,\nğ’¯\n\\mathcal{T}\n, or\nğ’ª\n\\mathcal{O}\n. Instead, it learns an optimal policy\nÏ€\n:\nÎ©\nâ†’\nğ’œ\n\\pi:\\Omega\\rightarrow\\mathcal{A}\nthrough direct interaction with the environment, relying on observations\no\nt\nâˆˆ\nÎ©\no_{t}\\in\\Omega\n, actions\na\nt\nâˆˆ\nğ’œ\na_{t}\\in\\mathcal{A}\n, and rewards\nr\nt\nâˆˆ\nâ„\nr_{t}\\in\\mathbb{R}\nsampled from\nâ„›\n\\mathcal{R}\n.\nFigure 4:\nThe proposed DRL architecture. An encoder processes observations into a latent vector, and actor and critic heads use it to determine the policy and estimate value.\nIII-B\n1\nObservation and Action Spaces\nThe agentâ€™s observation\no\nt\nâˆˆ\nÎ©\no_{t}\\in\\Omega\nat time step\nt\nt\nis a composite input vector designed for decentralized control, comprising: (1) a 7-D ego-state vector (3-D velocity and 4-D quaternion) for self-motion awareness, (2) a 42-D vector encoding the relative states of up to six neighbors, and (3) a two-channel\nH\nÃ—\nW\nH\\times W\noccupancy grid derived from LiDAR data for spatial understanding of obstacles, where\nH\n=\n72\nH=72\nand\nW\n=\n12\nW=12\ncorrespond to the horizontal and vertical resolution of the downsampled point cloud.\nFor neighbor representation, we select up to six nearest neighbors visible within the LiDARâ€™s FOV, accounting for occlusions that may temporarily hide agents. Each neighbor is encoded as a 7-D vector containing relative position, relative velocity, and a binary mask. The use of a fixed number of observable neighbors is a topological approach inspired by robust collective behaviors in animal swarms\n[\n31\n]\n. The selection of six neighbors balances performance and computational load, empirically verified in Sec.\nIV-E\n. When fewer than six neighbors are detected due to occlusions or limited vertical FOV, zero-padding and binary masks maintain a fixed input size while indicating the presence or absence of valid neighbor information.\nThe occupancy gridâ€™s first channel contains proximity information, representing distance to the nearest obstacle in each cell, while the second channel provides a binary mask indicating cell occupancy. Based on this observation, the agent generates an action\na\nt\nâˆˆ\nğ’œ\na_{t}\\in\\mathcal{A}\n, a continuous 3-D vector representing the desired velocity command\nğ¯\nâˆˆ\nâ„\n3\n\\mathbf{v}\\in\\mathbb{R}^{3}\n.\nIII-B\n2\nReward Function\nThe reward function\nâ„›\n\\mathcal{R}\nis a multi-objective sum designed to encourage complex emergent behaviors, with total reward:\nr\ntotal\n=\nr\nflock\n+\nr\nobstacle\n+\nr\nstable\n+\nr\nperception\n+\nr\ncollision\n.\nr_{\\text{total}}=r_{\\text{flock}}+r_{\\text{obstacle}}+r_{\\text{stable}}+r_{\\text{perception}}+r_{\\text{collision}}.\nThe flocking reward balances separation from nearby agents and cohesion with the flockâ€™s center of mass:\nr\nflock\n=\nw\nflock\nâ€‹\n(\nr\nseparation\n+\nr\ncohesion\n)\n,\nr_{\\text{flock}}=w_{\\text{flock}}(r_{\\text{separation}}+r_{\\text{cohesion}}),\nwhere\nr\nseparation\nr_{\\text{separation}}\npenalizes violating a safety distance, and\nr\ncohesion\nr_{\\text{cohesion}}\npenalizes straying from the flockâ€™s center.\nr\nseparation\n\\displaystyle r_{\\text{separation}}\n=\nâˆ’\nâˆ‘\nj\nâˆˆ\nğ’©\ni\n(\nd\nsep\nâˆ’\nâ€–\nğ©\ni\nâˆ’\nğ©\nj\nâ€–\n2\nd\nsep\nâˆ’\n2\nâ€‹\nr\nuav\n)\nâ€‹\nğ•€\nâ€–\nğ©\ni\nâˆ’\nğ©\nj\nâ€–\n2\n<\nd\nsep\n,\n\\displaystyle=-\\sum_{j\\in\\mathcal{N}^{i}}\\left(\\frac{d_{\\text{sep}}-\\|\\mathbf{p}^{i}-\\mathbf{p}^{j}\\|_{2}}{d_{\\text{sep}}-2r_{\\text{uav}}}\\right)\\mathbb{I}_{\\|\\mathbf{p}^{i}-\\mathbf{p}^{j}\\|_{2}<d_{\\text{sep}}},\nr\ncohesion\n\\displaystyle r_{\\text{cohesion}}\n=\nâˆ’\n(\nâ€–\nğ©\ni\nâˆ’\nğ©\ncom\nâ€–\n2\nâˆ’\nd\ncoh\n)\nâ€‹\nğ•€\nâ€–\nğ©\ni\nâˆ’\nğ©\ncom\nâ€–\n2\n>\nd\ncoh\n.\n\\displaystyle=-\\left(\\|\\mathbf{p}^{i}-\\mathbf{p}_{\\text{com}}\\|_{2}-d_{\\text{coh}}\\right)\\mathbb{I}_{\\|\\mathbf{p}^{i}-\\mathbf{p}_{\\text{com}}\\|_{2}>d_{\\text{coh}}}.\nHere,\nğ©\ni\n\\mathbf{p}^{i}\nis the UAVâ€™s position,\nğ©\nj\n\\mathbf{p}^{j}\nis the neighborâ€™s position,\nğ©\ncom\n\\mathbf{p}_{\\text{com}}\nis the center of mass,\nd\nsep\nd_{\\text{sep}}\nand\nd\ncoh\nd_{\\text{coh}}\nare distance thresholds,\nr\nuav\nr_{\\text{uav}}\nis the UAV radius, and\nğ•€\n\\mathbb{I}\nis the indicator function.\nThe obstacle avoidance reward penalizes obstacle proximity and approach:\nr\nobstacle\n=\nw\nobstacle\nâ€‹\n(\nr\nproximity\n+\nr\ndirection\n)\n,\nr_{\\text{obstacle}}=w_{\\text{obstacle}}(r_{\\text{proximity}}+r_{\\text{direction}}),\nwhere\nr\nproximity\nr_{\\text{proximity}}\npenalizes distance to the nearest obstacle, and\nr\ndirection\nr_{\\text{direction}}\npenalizes moving towards obstacles.\nr\nproximity\n\\displaystyle r_{\\text{proximity}}\n=\nâˆ’\n(\nd\nprox\nâˆ’\nmin\nk\nâ¡\n(\nd\nobs\nk\n)\nd\nprox\nâˆ’\nr\nuav\n)\n4\nâ€‹\nğ•€\nmin\nk\nâ¡\nd\nobs\nk\n<\nd\nprox\n,\n\\displaystyle=-\\left(\\frac{d_{\\text{prox}}-\\min_{k}(d^{k}_{\\text{obs}})}{d_{\\text{prox}}-r_{\\text{uav}}}\\right)^{4}\\mathbb{I}_{\\min_{k}d^{k}_{\\text{obs}}<d_{\\text{prox}}},\nr\ndirection\n\\displaystyle r_{\\text{direction}}\n=\nâˆ’\nâˆ‘\nk\nmax\nâ¡\n(\n0\n,\nd\nprox\nâˆ’\nd\nobs\nk\n)\nâ€‹\nâ€–\nğ¯\ni\nâ€–\n2\nâ€‹\nğ•€\n|\nÎ¸\nk\n|\n<\nÎ¸\nthreshold\n.\n\\displaystyle=-\\sum_{k}\\max(0,d_{\\text{prox}}-d^{k}_{\\text{obs}})\\|\\mathbf{v}^{i}\\|_{2}\\mathbb{I}_{|\\theta^{k}|<\\theta_{\\text{threshold}}}.\nHere,\nk\nk\nindexes the individual obstacle points observed by the LiDAR;\nd\nobs\nk\nd^{k}_{\\text{obs}}\nis the distance to the\nk\nk\n-th point, and\nÎ¸\nk\n\\theta^{k}\nis the angle between the UAVâ€™s velocity and the ray to the\nk\nk\n-th obstacle point.\nd\nprox\nd_{\\text{prox}}\nand\nÎ¸\nthreshold\n\\theta_{\\text{threshold}}\nare defined as the proximity and angle thresholds used for obstacle avoidance.\nThe stable flight reward is:\nr\nstable\n=\nw\nstable\nâ€‹\n(\nr\naltitude\n+\nr\nattitude\n)\n,\nr_{\\text{stable}}=w_{\\text{stable}}(r_{\\text{altitude}}+r_{\\text{attitude}}),\nincluding\nr\naltitude\nr_{\\text{altitude}}\nfor altitude maintenance and\nr\nattitude\nr_{\\text{attitude}}\nfor attitude stability.\nr\naltitude\n\\displaystyle r_{\\text{altitude}}\n=\nexp\nâ¡\n(\nâˆ’\n(\nh\ni\nâˆ’\nh\nl\nÎ±\n)\n2\n)\n,\n\\displaystyle=\\exp\\left(-\\left(\\frac{h^{i}-h^{l}}{\\alpha}\\right)^{2}\\right),\nr\nattitude\n\\displaystyle r_{\\text{attitude}}\n=\nexp\nâ¡\n(\nâˆ’\n(\nu\nz\nâˆ’\n1\nÎ²\n)\n2\n)\n.\n\\displaystyle=\\exp\\left(-\\left(\\frac{u_{z}-1}{\\beta}\\right)^{2}\\right).\nHere,\nh\ni\nh^{i}\nis the UAVâ€™s altitude,\nh\nl\nh^{l}\nis the leaderâ€™s altitude, and\nu\nz\nu_{z}\nis the z-component of the UAVâ€™s up-vector. Parameters\nÎ±\n\\alpha\nand\nÎ²\n\\beta\ncontrol sensitivity.\nFigure 5:\nIllustration of reward function components for learning communication-free collective navigation. (a) Flocking: cohesion encourages staying close to neighbors, while separation maintains safe distances. (b) Obstacle avoidance: proximity penalizes closeness to obstacles, and direction penalizes movement toward obstacles. (c) Stable flight: altitude maintenance encourages following the leaderâ€™s altitude, and attitude stability promotes upright orientation. (d) Neighbor perception: visibility rewards keeping neighbors within FOV, recovery triggers descent when all neighbors are lost.\nThe neighbor perception reward encourages maintaining awareness of neighboring UAVs:\nr\nperception\n=\nw\nperception\nâ€‹\n(\nr\nvisibility\n+\nr\nrecovery\n)\n,\nr_{\\text{perception}}=w_{\\text{perception}}(r_{\\text{visibility}}+r_{\\text{recovery}}),\ncomposed of\nr\nvisibility\nr_{\\text{visibility}}\nfor keeping neighbors within FOV by minimizing occlusion and\nr\nrecovery\nr_{\\text{recovery}}\nfor descent when no neighbors are detected. This descent acts as a recovery maneuver to re-acquire neighbors that may have fallen out of the limited vertical FOV.\nr\nvisibility\n\\displaystyle r_{\\text{visibility}}\n=\n|\nğ’©\nperceived\ni\n|\n|\nğ’©\ni\n|\n,\n\\displaystyle=\\frac{|\\mathcal{N}^{i}_{\\text{perceived}}|}{|\\mathcal{N}^{i}|},\nr\nrecovery\n\\displaystyle r_{\\text{recovery}}\n=\nâˆ’\n|\nh\ni\nâˆ’\nh\nrecovery\n|\nâ€‹\nğ•€\nğ’©\nperceived\ni\n=\n0\n.\n\\displaystyle=-|h^{i}-h_{\\text{recovery}}|\\mathbb{I}_{\\mathcal{N}^{i}_{\\text{perceived}}=0}.\nHere,\nğ’©\nperceived\ni\n\\mathcal{N}^{i}_{\\text{perceived}}\nis the set of neighbors detected by LiDAR accounting for occlusions and limited FOV, and\nh\nrecovery\nh_{\\text{recovery}}\nis the target descent altitude.\nFinally, the collision penalty is a sparse negative reward applied upon any collision event and triggers episode termination:\nr\ncollision\n=\nâˆ’\n10\nâ€‹\nğ•€\ncollision\n.\nr_{\\text{collision}}=-10\\mathbb{I}_{\\text{collision}}.\nA collision event is triggered if the distance between centers of any two UAVs becomes less than\n2\nâ€‹\nr\nuav\n2r_{\\text{uav}}\n, or the distance between a UAVâ€™s center and an obstacle point becomes less than\nr\nuav\nr_{\\text{uav}}\n.\nTABLE II:\nParameters for the Reward Function\nParameter\nValue\nDescription\nReward Weights\nw\nflock\nw_{\\text{flock}}\n1.5\nWeight for the flocking reward\nw\nobstacle\nw_{\\text{obstacle}}\n2.0\nWeight for the obstacle avoidance reward\nw\nstable\nw_{\\text{stable}}\n1.0\nWeight for the stable flight reward\nw\naware\nw_{\\text{aware}}\n1.0\nWeight for the situational awareness reward\nFlocking\nd\nsep\nd_{\\text{sep}}\n1.6m\nSafety distance for separation\nd\ncoh\nd_{\\text{coh}}\n2.0m\nCohesion distance threshold from center of mass\nr\nuav\nr_{\\text{uav}}\n0.2m\nRadius of the UAV\nObstacle Avoidance\nd\nprox\nd_{\\text{prox}}\n3.0m\nProximity threshold for obstacles\nÎ¸\nthreshold\n\\theta_{\\text{threshold}}\n20\nâˆ˜\n20^{\\circ}\nAngle threshold for direction penalty\nStable Flight\nÎ±\n\\alpha\n0.1\nScaling parameter for altitude reward\nÎ²\n\\beta\n0.1\nScaling parameter for attitude reward\nSituational Awareness\nh\nrecovery\nh_{\\text{recovery}}\n1.0 m\nTarget altitude for emergency descent\nIII-B\n3\nNetwork Architecture and Training\nOur control policy is implemented as an actor-critic network, illustrated in Fig.\n4\n. The network consists of a shared encoder processing observations, followed by actor and critic heads. The entire policy is trained end-to-end using the PPO algorithm\n[\n28\n]\n.\nThe two-channel\n72\nÃ—\n12\n72\\times 12\nLiDAR occupancy grid is processed by a convolutional neural network (CNN). The resulting feature map is flattened and passed through a fully-connected layer to produce a 256-D feature vector, which is concatenated with the 7-D ego-state vector and 42-D neighbor-state vector. The combined feature vector is passed through a multi-layer perceptron (MLP), outputting a final 512-D shared feature vector. The shared vector is consumed by two separate heads. The actor head is an MLP with a hidden layer of 256 units that outputs the mean and standard deviation for a continuous Gaussian policy. The critic head is a similar MLP outputting a single scalar representing the value function.\nThe training process consists of two core stages: advantage estimation, followed by policy and value function optimization. To evaluate how much better a specific action is compared with the average action in a given state, we use an advantage function,\nA\n^\nt\n\\hat{A}_{t}\n. We apply generalized advantage estimation (GAE)\n[\n32\n]\n, which computes advantages based on the temporal difference (TD) error,\nÎ´\nt\n\\delta_{t}\n:\nÎ´\nt\n=\nR\nt\n+\nÎ³\nâ€‹\nV\nÏ•\nâ€‹\n(\no\nt\n+\n1\n)\nâˆ’\nV\nÏ•\nâ€‹\n(\no\nt\n)\n,\n\\delta_{t}=R_{t}+\\gamma V_{\\phi}(o_{t+1})-V_{\\phi}(o_{t}),\n(1)\nwhere\nR\nt\nR_{t}\nis the reward,\nV\nÏ•\nV_{\\phi}\nis the value estimated by the critic network, and\nÎ³\n\\gamma\nis the discount factor. GAE calculates the advantage as an exponentially-weighted sum of these TD errors:\nA\n^\nt\nGAE\nâ€‹\n(\nÎ³\n,\nÎ»\n)\n=\nâˆ‘\nl\n=\n0\nT\nâˆ’\nt\nâˆ’\n1\n(\nÎ³\nâ€‹\nÎ»\n)\nl\nâ€‹\nÎ´\nt\n+\nl\n,\n\\hat{A}_{t}^{\\text{GAE}(\\gamma,\\lambda)}=\\sum_{l=0}^{T-t-1}(\\gamma\\lambda)^{l}\\delta_{t+l},\n(2)\nwhere\nÎ»\nâˆˆ\n[\n0\n,\n1\n]\n\\lambda\\in[0,1]\ncontrols the bias-variance trade-off. The resulting advantages\nA\n^\nt\n\\hat{A}_{t}\nare normalized to zero mean and unit variance before policy optimization.\nThe policy and value networks are optimized jointly by minimizing a total loss function:\nL\nâ€‹\n(\nÎ¸\n,\nÏ•\n)\n=\nğ”¼\n^\nt\nâ€‹\n[\nâˆ’\nL\nCLIP\nâ€‹\n(\nÎ¸\n)\n+\nc\n1\nâ€‹\nL\nVF\nâ€‹\n(\nÏ•\n)\nâˆ’\nc\n2\nâ€‹\nH\nâ€‹\n[\nÏ€\nÎ¸\n]\nâ€‹\n(\no\nt\n)\n]\n,\nL(\\theta,\\phi)=\\hat{\\mathbb{E}}_{t}[-L^{\\text{CLIP}}(\\theta)+c_{1}L_{\\text{VF}}(\\phi)-c_{2}H[\\pi_{\\theta}](o_{t})],\n(3)\nwhere\nc\n1\nc_{1}\nand\nc\n2\nc_{2}\nare hyperparameters weighting the different terms. The primary component is the clipped surrogate objective:\nL\nCLIP\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\n^\nt\nâ€‹\n[\nmin\nâ¡\n(\nr\nt\nâ€‹\n(\nÎ¸\n)\nâ€‹\nA\n^\nt\n,\nclip\nâ€‹\n(\nr\nt\nâ€‹\n(\nÎ¸\n)\n,\n1\nâˆ’\nÏµ\n,\n1\n+\nÏµ\n)\nâ€‹\nA\n^\nt\n)\n]\n.\nL^{\\text{CLIP}}(\\theta)=\\hat{\\mathbb{E}}_{t}\\left[\\min\\left(r_{t}(\\theta)\\hat{A}_{t},\\text{clip}(r_{t}(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_{t}\\right)\\right].\n(4)\nHere,\nr\nt\nâ€‹\n(\nÎ¸\n)\n=\nÏ€\nÎ¸\nâ€‹\n(\na\nt\n|\no\nt\n)\nÏ€\nÎ¸\nold\nâ€‹\n(\na\nt\n|\no\nt\n)\nr_{t}(\\theta)=\\frac{\\pi_{\\theta}(a_{t}|o_{t})}{\\pi_{\\theta_{\\text{old}}}(a_{t}|o_{t})}\nis the probability ratio between current and old policies. This clipping mechanism is the core of PPOâ€™s stability, discouraging overly large policy updates.\nFigure 6:\nSimulation environments. (a) The cluttered training environment with spawn area at the map center and goal point randomly sampled on a circle of 30m radius centered at the spawn region. (b) and (c) show unseen test environments for evaluating generalization.\nThe second component, the value function loss\nL\nVF\nL_{\\text{VF}}\n, reduces the variance of advantage estimates. The value network minimizes the error between its prediction\nV\nÏ•\nâ€‹\n(\no\nt\n)\nV_{\\phi}(o_{t})\nand the value target\nV\nt\ntarg\nV_{t}^{\\text{targ}}\n, computed using value estimates from the network used during data collection:\nV\nt\ntarg\n=\nA\n^\nt\n+\nV\nÏ•\nold\nâ€‹\n(\no\nt\n)\n.\nV_{t}^{\\text{targ}}=\\hat{A}_{t}+V_{\\phi_{\\text{old}}}(o_{t}).\n(5)\nThis loss is the squared error:\nL\nVF\nâ€‹\n(\nÏ•\n)\n=\n(\nV\nÏ•\nâ€‹\n(\no\nt\n)\nâˆ’\nV\nt\ntarg\n)\n2\n.\nL_{\\text{VF}}(\\phi)=(V_{\\phi}(o_{t})-V_{t}^{\\text{targ}})^{2}.\n(6)\nFinally, an entropy bonus\nH\nâ€‹\n[\nÏ€\nÎ¸\n]\nâ€‹\n(\no\nt\n)\nH[\\pi_{\\theta}](o_{t})\nencourages exploration, where\nH\nH\nis the entropy of the policyâ€™s action distribution:\nH\nâ€‹\n[\nÏ€\nÎ¸\n]\nâ€‹\n(\no\nt\n)\n=\nğ”¼\na\nâˆ¼\nÏ€\nÎ¸\nâ€‹\n[\nâˆ’\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\n|\no\nt\n)\n]\n.\nH[\\pi_{\\theta}](o_{t})=\\mathbb{E}_{a\\sim\\pi_{\\theta}}[-\\log\\pi_{\\theta}(a|o_{t})].\n(7)\nKey hyperparameters are set to\nÏµ\n=\n0.1\n\\epsilon=0.1\n,\nc\n1\n=\n1.0\nc_{1}=1.0\n,\nc\n2\n=\n0.001\nc_{2}=0.001\n,\nÎ³\n=\n0.99\n\\gamma=0.99\n,\nÎ»\n=\n0.95\n\\lambda=0.95\n, using the Adam optimizer with learning rates of\n1\nÃ—\n10\nâˆ’\n3\n1\\times 10^{-3}\nfor the encoder, actor, and critic.\nIV\nSimulations\nIV-A\nSimulation Setup\nWe trained and evaluated our DRL policy in Nvidia Isaac Sim using the OmniDrones\n[\n33\n]\nframework for GPU-accelerated parallel simulations. Each training simulation instance featured a swarm of five UAVs, comprising one leader and four followers, navigating an environment with randomly placed pillars as shown in Fig.\n6\n(a).\nIV-A\n1\nEnvironment Configuration\nAt the beginning of each training episode, the swarm was spawned in an obstacle-free region at the map center, with UAVs randomly positioned on a\n3\nÃ—\n3\n3\\times 3\ngrid with 1.6m spacing to ensure safe initial separation. The goal point was randomly sampled on a 30m radius circle centered at the spawn region. To enhance policy robustness and generalization, we randomized initial UAV orientations, the leaderâ€™s speed, and goal location for each episode.\nTraining was conducted across 512 parallel environments for 500 million timesteps, leveraging GPU acceleration (Nvidia RTX 4090 and Intel i9-14900K) for efficient data collection and policy updates. Each episode terminated under four conditions: (1) collision between UAVs, (2) collision between a UAV and obstacle, (3) flying below minimum allowable altitude, and (4) flying above maximum allowable altitude, ensuring the policy learns to maintain swarm cohesion, avoid collisions, and preserve stable flight within operational constraints.\nIV-A\n2\nLeader Navigation\nThe leader UAV utilized a planner combining the rapidly-exploring random tree (RRT)\n[\n34\n]\nwith the artificial potential field (APF)\n[\n35\n]\nfor real-time navigation. RRT generates a local path toward the waypoint to prevent APF from falling into local minima. Then, APF computes an attractive force toward the RRT-generated path direction and repulsive forces from nearby obstacles detected by the 360-degree LiDAR. The final velocity command is obtained by combining these APF forces, enabling the UAV to follow the RRT path while maintaining safe distances from obstacles. This integration allows smooth and safe navigation in unknown environments.\nIV-A\n3\nPerception Modeling\nWhile our physical system uses reflective tape for neighbor detection, in simulation we abstracted this process using the simulatorâ€™s ground-truth data to identify neighbors. This abstraction allowed us to focus training on the control policy while incorporating key perceptual challenges based on our hardwareâ€™s empirical characteristics: (1) estimation errors in relative neighbor states with Gaussian distributions (relative position:\nÏƒ\npos\n=\n0.02\n\\sigma_{\\text{pos}}=0.02\nm, relative velocity:\nÏƒ\nvel\n=\n0.05\n\\sigma_{\\text{vel}}=0.05\nm/s), (2) limited FOV identical to the physical sensor (\nâˆ’\n7\nâˆ˜\n-7^{\\circ}\nto\n52\nâˆ˜\n52^{\\circ}\nvertical), (3) occlusions calculated based on physical UAV dimensions, and (4) processing latency reflecting actual onboard computation. Latency was determined by measuring end-to-end delay on the onboard computer, from sensor data acquisition through our perception pipeline to policy input generation. To ensure temporal consistency, data synchronization introduces delays of 0.1s for ego state and LiDAR data for obstacles, and 0.2s for relative neighbor information.\nIV-B\nEvaluation Metrics\nTo quantitatively evaluate the proposed policy, six metrics were defined: success rate (\nSR\n), mission progress (\nMP\n), flock radius (\nFR\n), minimum separation (\nMS\n), alignment (\nAL\n), and minimum distance to obstacles (\nMDO\n). All results represent mean and standard deviation from 100 independent trials per scenario.\nIV-B\n1\nSuccess Rate (\nSR\n)\nThe percentage of trials where all UAVs successfully followed the leader to the destination without collisions, serving as the primary indicator of mission completion. Evaluation terminates when: (1) UAV-UAV collision, (2) UAV-obstacle collision, (3) flying below/above altitude limits, or (4) complete leader loss from all followersâ€™ perception.\nSR\nis defined as\nSR\n=\nN\nsuccessful trials\nN\ntrials\nÃ—\n100\n.\n\\text{{SR}}=\\frac{N_{\\text{successful trials}}}{N_{\\text{trials}}}\\times 100.\nIV-B\n2\nMission Progress (\nMP\n)\nThe ratio of distance advanced toward the destination relative to total distance from start to destination, averaged over all trials. This evaluates navigation effectiveness and if collective behavior enables consistent forward progress. Let the leaderâ€™s starting position be\nP\nstart\nP_{\\text{start}}\n, goal position be\nP\ngoal\nP_{\\text{goal}}\n, and position at trial\nk\nk\nend be\nP\nend\nk\nP_{\\text{end}}^{k}\n.\nMP\nis calculated as:\nMP\n=\n1\nN\ntrials\nâˆ‘\nk\n=\n1\nN\ntrials\n[\n\\displaystyle\\text{{MP}}=\\frac{1}{N_{\\text{trials}}}\\sum_{k=1}^{N_{\\text{trials}}}\\Bigg[\n(\n1\nâˆ’\nâ€–\nP\nend\nk\nâˆ’\nP\ngoal\nâ€–\n2\nâ€–\nP\nstart\nâˆ’\nP\ngoal\nâ€–\n2\n)\nÃ—\n100\n]\n.\n\\displaystyle\\left(1-\\frac{\\|P_{\\text{end}}^{k}-P_{\\text{goal}}\\|_{2}}{\\|P_{\\text{start}}-P_{\\text{goal}}\\|_{2}}\\right)\\times 100\\Bigg].\nIV-B\n3\nFlock Radius (\nFR\n)\nThe maximum flock extent, defined as distance from swarm center to farthest UAV, averaged over the episode and trials. Compact formation ensures agents remain within perceptual range. Larger\nFR\nindicates fragmentation risks. For\nN\nN\nUAVs, let\nğ©\nt\ni\n,\nk\n\\mathbf{p}^{i,k}_{t}\nbe position of UAV\ni\ni\nat time\nt\nt\nin trial\nk\nk\nand\nğ©\nt\nc\n,\nk\n=\n1\nN\nâ€‹\nâˆ‘\nj\n=\n1\nN\nğ©\nt\nj\n,\nk\n\\mathbf{p}^{c,k}_{t}=\\frac{1}{N}\\sum_{j=1}^{N}\\mathbf{p}^{j,k}_{t}\nbe the center. Let\nT\nk\nT_{k}\ndenote episode length of trial\nk\nk\n.\nFR\nis calculated by\nFR\n=\n1\nN\ntrials\nâˆ‘\nk\n=\n1\nN\ntrials\n[\n\\displaystyle\\text{{FR}}=\\frac{1}{N_{\\text{trials}}}\\sum_{k=1}^{N_{\\text{trials}}}\\Bigg[\n1\nT\nk\nâˆ‘\nt\n=\n1\nT\nk\nmax\ni\n=\n1\n,\nâ€¦\n,\nN\nâˆ¥\nğ©\nt\ni\n,\nk\nâˆ’\nğ©\nt\nc\n,\nk\nâˆ¥\n2\n]\n.\n\\displaystyle\\frac{1}{T_{k}}\\sum_{t=1}^{T_{k}}\\max_{i=1,\\dots,N}\\|\\mathbf{p}^{i,k}_{t}-\\mathbf{p}^{c,k}_{t}\\|_{2}\\Bigg].\nIV-B\n4\nMinimum Separation (\nMS\n)\nA safety metric evaluating collision avoidance between agents, defined as minimum distance between the closest UAV pair, averaged over episode and trials. Adequate separation is fundamental to safe collective navigation.\nMS\n=\n1\nN\ntrials\nâˆ‘\nk\n=\n1\nN\ntrials\n[\n\\displaystyle\\text{{MS}}=\\frac{1}{N_{\\text{trials}}}\\sum_{k=1}^{N_{\\text{trials}}}\\Bigg[\n1\nT\nk\nâˆ‘\nt\n=\n1\nT\nk\nmin\ni\nâ‰ \nj\nâˆ¥\nğ©\nt\ni\n,\nk\nâˆ’\nğ©\nt\nj\n,\nk\nâˆ¥\n2\n]\n.\n\\displaystyle\\frac{1}{T_{k}}\\sum_{t=1}^{T_{k}}\\min_{i\\neq j}\\|\\mathbf{p}^{i,k}_{t}-\\mathbf{p}^{j,k}_{t}\\|_{2}\\Bigg].\nIV-B\n5\nAlignment (\nAL\n)\nA metric measuring directional consistency, averaged over episode and trials. High\nAL\nindicates coherent swarm movement. It is the average cosine similarity between each UAVâ€™s velocity\nğ¯\nt\ni\n,\nk\n\\mathbf{v}^{i,k}_{t}\nand swarm average velocity\nğ¯\nt\navg\n,\nk\n=\n1\nN\nâ€‹\nâˆ‘\nj\n=\n1\nN\nğ¯\nt\nj\n,\nk\n\\mathbf{v}^{\\text{avg},k}_{t}=\\frac{1}{N}\\sum_{j=1}^{N}\\mathbf{v}^{j,k}_{t}\n. Values near\n1\n1\nindicate well-aligned formation.\nAL\n=\n1\nN\ntrials\nâˆ‘\nk\n=\n1\nN\ntrials\n[\n\\displaystyle\\text{{AL}}=\\frac{1}{N_{\\text{trials}}}\\sum_{k=1}^{N_{\\text{trials}}}\\Bigg[\n1\nT\nk\nâˆ‘\nt\n=\n1\nT\nk\n1\nN\nâˆ‘\ni\n=\n1\nN\nğ¯\nt\ni\n,\nk\nâ‹…\nğ¯\nt\navg\n,\nk\nâ€–\nğ¯\nt\ni\n,\nk\nâ€–\n2\nâ€‹\nâ€–\nğ¯\nt\navg\n,\nk\nâ€–\n2\n]\n.\n\\displaystyle\\frac{1}{T_{k}}\\sum_{t=1}^{T_{k}}\\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\mathbf{v}^{i,k}_{t}\\cdot\\mathbf{v}^{\\text{avg},k}_{t}}{\\|\\mathbf{v}^{i,k}_{t}\\|_{2}\\|\\mathbf{v}^{\\text{avg},k}_{t}\\|_{2}}\\Bigg].\nIV-B\n6\nMinimum Distance to Obstacles (\nMDO\n)\nA safety metric evaluating obstacle avoidance, defined as minimum distance between any UAV and any obstacle detection point, averaged over episode and trials. This measures safety margins maintained during navigation. Let\nğ¨\nt\nm\n,\nk\n\\mathbf{o}^{m,k}_{t}\ndenote position of the\nm\nm\n-th LiDAR detection point among\nM\nM\ntotal points:\nMDO\n=\n1\nN\ntrials\nâˆ‘\nk\n=\n1\nN\ntrials\n[\n\\displaystyle\\text{{MDO}}=\\frac{1}{N_{\\text{trials}}}\\sum_{k=1}^{N_{\\text{trials}}}\\Bigg[\n1\nT\nk\nâˆ‘\nt\n=\n1\nT\nk\nmin\ni\n=\n1\n,\nâ€¦\n,\nN\nm\n=\n1\n,\nâ€¦\n,\nM\nâˆ¥\nğ©\nt\ni\n,\nk\nâˆ’\nğ¨\nt\nm\n,\nk\nâˆ¥\n2\n]\n.\n\\displaystyle\\frac{1}{T_{k}}\\sum_{t=1}^{T_{k}}\\min_{\\begin{subarray}{c}i=1,\\dots,N\\\\\nm=1,\\dots,M\\end{subarray}}\\|\\mathbf{p}^{i,k}_{t}-\\mathbf{o}^{m,k}_{t}\\|_{2}\\Bigg].\nTABLE III:\nPerformance comparison with baseline methods\nMetric\nMethod\nNo obstacles\nTraining environment\nTest environment\nCluttered pillars\nSimplified forest\nForest\nMin. obstacle gap\n10m\n5m\n5m\n5m\nSR\n(%)\nPACNav\n[\n22\n]\n100.0\nÂ±\n\\pm\n0.0\n96.0\nÂ±\n\\pm\n0.0\n82.0\nÂ±\n\\pm\n0.0\n38.0\nÂ±\n\\pm\n0.0\n31.0\nÂ±\n\\pm\n0.0\nVPF\n[\n23\n]\n100.0\nÂ±\n\\pm\n0.0\n80.0\nÂ±\n\\pm\n0.0\n27.0\nÂ±\n\\pm\n0.0\n6.0\nÂ±\n\\pm\n0.0\n5.0\nÂ±\n\\pm\n0.0\nDAgger\n[\n25\n]\n100.0\nÂ±\n\\pm\n0.0\n93.0\nÂ±\n\\pm\n0.0\n69.0\nÂ±\n\\pm\n0.0\n34.0\nÂ±\n\\pm\n0.0\n22.0\nÂ±\n\\pm\n0.0\nProposed\n100.0\nÂ±\n\\pm\n0.0\n99.0\nÂ±\n\\pm\n0.0\n97.0\nÂ±\n\\pm\n0.0\n97.0\nÂ±\n\\pm\n0.0\n72.0\nÂ±\n\\pm\n0.0\nMP\n(%)\nPACNav\n[\n22\n]\n100.0\nÂ±\n\\pm\n0.0\n98.6\nÂ±\n\\pm\n8.8\n93.0\nÂ±\n\\pm\n18.6\n72.2\nÂ±\n\\pm\n26.6\n67.6\nÂ±\n\\pm\n28.2\nVPF\n[\n23\n]\n100.0\nÂ±\n\\pm\n0.0\n92.6\nÂ±\n\\pm\n17.5\n62.0\nÂ±\n\\pm\n32.3\n47.0\nÂ±\n\\pm\n24.4\n39.7\nÂ±\n\\pm\n22.7\nDAgger\n[\n25\n]\n100.0\nÂ±\n\\pm\n0.0\n97.1\nÂ±\n\\pm\n11.4\n85.6\nÂ±\n\\pm\n25.4\n74.5\nÂ±\n\\pm\n25.9\n69.1\nÂ±\n\\pm\n24.0\nProposed\n100.0\nÂ±\n\\pm\n0.0\n99.3\nÂ±\n\\pm\n7.0\n99.5\nÂ±\n\\pm\n3.3\n98.2\nÂ±\n\\pm\n10.4\n88.2\nÂ±\n\\pm\n23.6\nFR\n(m)\nPACNav\n[\n22\n]\n1.80\nÂ±\n\\pm\n0.14\n1.83\nÂ±\n\\pm\n0.12\n1.85\nÂ±\n\\pm\n0.10\n1.85\nÂ±\n\\pm\n0.10\n1.82\nÂ±\n\\pm\n0.10\nVPF\n[\n23\n]\n2.20\nÂ±\n\\pm\n0.44\n2.23\nÂ±\n\\pm\n0.55\n2.54\nÂ±\n\\pm\n1.57\n2.44\nÂ±\n\\pm\n1.44\n2.31\nÂ±\n\\pm\n0.79\nDAgger\n[\n25\n]\n2.96\nÂ±\n\\pm\n0.44\n2.85\nÂ±\n\\pm\n0.44\n2.65\nÂ±\n\\pm\n0.37\n2.59\nÂ±\n\\pm\n0.56\n2.59\nÂ±\n\\pm\n0.46\nProposed\n1.61\nÂ±\n\\pm\n0.13\n1.56\nÂ±\n\\pm\n0.12\n1.47\nÂ±\n\\pm\n0.09\n1.40\nÂ±\n\\pm\n0.06\n1.39\nÂ±\n\\pm\n0.15\nMS\n(m)\nPACNav\n[\n22\n]\n1.23\nÂ±\n\\pm\n0.07\n1.25\nÂ±\n\\pm\n0.06\n1.26\nÂ±\n\\pm\n0.06\n1.27\nÂ±\n\\pm\n0.04\n1.25\nÂ±\n\\pm\n0.05\nVPF\n[\n23\n]\n1.49\nÂ±\n\\pm\n0.03\n1.48\nÂ±\n\\pm\n0.04\n1.48\nÂ±\n\\pm\n0.05\n1.46\nÂ±\n\\pm\n0.05\n1.47\nÂ±\n\\pm\n0.06\nDAgger\n[\n25\n]\n1.54\nÂ±\n\\pm\n0.11\n1.53\nÂ±\n\\pm\n0.11\n1.51\nÂ±\n\\pm\n0.11\n1.49\nÂ±\n\\pm\n0.13\n1.49\nÂ±\n\\pm\n0.13\nProposed\n1.55\nÂ±\n\\pm\n0.02\n1.53\nÂ±\n\\pm\n0.02\n1.51\nÂ±\n\\pm\n0.02\n1.47\nÂ±\n\\pm\n0.03\n1.46\nÂ±\n\\pm\n0.04\nAL\nPACNav\n[\n22\n]\n0.87\nÂ±\n\\pm\n0.08\n0.83\nÂ±\n\\pm\n0.08\n0.79\nÂ±\n\\pm\n0.07\n0.73\nÂ±\n\\pm\n0.05\n0.75\nÂ±\n\\pm\n0.06\nVPF\n[\n23\n]\n0.92\nÂ±\n\\pm\n0.03\n0.92\nÂ±\n\\pm\n0.04\n0.89\nÂ±\n\\pm\n0.06\n0.89\nÂ±\n\\pm\n0.05\n0.87\nÂ±\n\\pm\n0.09\nDAgger\n[\n25\n]\n0.79\nÂ±\n\\pm\n0.02\n0.77\nÂ±\n\\pm\n0.03\n0.74\nÂ±\n\\pm\n0.04\n0.67\nÂ±\n\\pm\n0.04\n0.68\nÂ±\n\\pm\n0.05\nProposed\n0.94\nÂ±\n\\pm\n0.01\n0.94\nÂ±\n\\pm\n0.01\n0.93\nÂ±\n\\pm\n0.02\n0.90\nÂ±\n\\pm\n0.02\n0.88\nÂ±\n\\pm\n0.11\nMDO\n(m)\nPACNav\n[\n22\n]\n-\n2.18\nÂ±\n\\pm\n0.33\n1.89\nÂ±\n\\pm\n0.40\n0.82\nÂ±\n\\pm\n0.45\n0.82\nÂ±\n\\pm\n0.32\nVPF\n[\n23\n]\n-\n2.12\nÂ±\n\\pm\n0.41\n1.81\nÂ±\n\\pm\n0.57\n1.27\nÂ±\n\\pm\n0.45\n0.73\nÂ±\n\\pm\n0.45\nDAgger\n[\n25\n]\n-\n1.89\nÂ±\n\\pm\n0.40\n1.52\nÂ±\n\\pm\n0.54\n0.93\nÂ±\n\\pm\n0.58\n0.76\nÂ±\n\\pm\n0.41\nProposed\n-\n2.27\nÂ±\n\\pm\n0.15\n1.88\nÂ±\n\\pm\n0.16\n1.56\nÂ±\n\\pm\n0.09\n0.79\nÂ±\n\\pm\n0.32\nIV-C\nPerformance Comparison\nTo demonstrate the superiority and practical applicability of the proposed method, we conducted comparative evaluations against baseline methods satisfying two critical criteria: compatibility with real-world deployment on physical UAV platforms with actual sensor constraints and demonstrated effectiveness in communication-free collective navigation scenarios.\nIV-C\n1\nBaseline Methods\nOur baseline selection comprises three representative approaches meeting these criteria. The first two are heuristic-based methods validated in real-world experiments. PACNav\n[\n22\n]\nis a bio-inspired distributed control method enabling collective navigation without GNSS or inter-UAV communication by introducing two metrics: path persistence (measuring how consistently a UAV flies straight) and path similarity (measuring directional consistency among neighbors). Uninformed UAVs identify reliable leaders by selecting neighbors exhibiting high path persistence and similarity, enabling emergent collective navigation toward unknown destinations. The method incorporates reactive collision avoidance for environmental obstacles and inter-UAV conflicts, validated with four UAVs in natural forests.\nThe second baseline employs a bio-inspired VPF\n[\n23\n]\n, which mimics avian visual perception for communication-free flocking. Instead of exchanging state information, each UAV uses VPF to observe neighbors and obstacles, distinguishing between repulsion zones (objects appearing large) and attraction zones (objects appearing small) based on perceived size. This enables the three fundamental flocking behaviorsâ€”separation, attraction, and velocity alignmentâ€”without inter-UAV communication. The approach incorporates an implicit heterogeneous flocking framework, where a small minority of informed UAVs guide the uninformed majority through emergent collective behavior, with uninformed agents following the swarmâ€™s overall motion without explicitly identifying leaders. The method has been validated with six UAVs in indoor environments with obstacles, where a motion capture system was installed.\nFor the learning-based baseline, the DAgger approach\n[\n25\n]\ntrains an end-to-end visuomotor controller by imitating an expert policy combining modified Reynoldsâ€™ flocking rules with APF-based obstacle avoidance. The expert policy, which has access to ground-truth state information in simulation, generates optimal control commands that the student policy learns to replicate using only onboard camera images and IMU data. Through centralized training and decentralized execution, the learned policy achieves vision-only cooperative flight without GNSS or inter-UAV communication. While validated exclusively in high-fidelity Gazebo simulations with complex scenarios (pillar forests, narrow gaps), the method demonstrates superior learning efficiency and performance compared with other learning algorithms (MADDPG\n[\n17\n]\n, SAC\n[\n27\n]\n, and MAGAIL\n[\n36\n]\n). For our comparison, we directly implemented the expert policy itself rather than retraining the imitation learning policy. This design ensures fair comparison by eliminating approximation error from imitation learning, evaluating the fundamental control strategy rather than policy distillation quality, providing an upper bound on DAgger methodâ€™s performance.\nIV-C\n2\nFair Comparison Setup\nFor fair comparison, we unified all methods to use identical LiDAR-based perception (same FOV, range, occlusion handling) while preserving each methodâ€™s control strategy. Parameters were independently tuned to achieve similar flocking behavior in obstacle-free environments, ensuring equivalent baseline capability before testing in complex scenarios.\nTABLE IV:\nScalability analysis of the proposed DRL policy with varying numbers of follower UAVs\nMetric\nMin. obstacle gap\nNumber of followers\n2\n4\n6\n8\n10\nSR\n(%)\nNo obstacles\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n98.0\nÂ±\n\\pm\n0.0\n93.0\nÂ±\n\\pm\n0.0\n10m\n100.0\nÂ±\n\\pm\n0.0\n99.0\nÂ±\n\\pm\n0.0\n96.0\nÂ±\n\\pm\n0.0\n84.0\nÂ±\n\\pm\n0.0\n70.0\nÂ±\n\\pm\n0.0\n5m\n93.0\nÂ±\n\\pm\n0.0\n97.0\nÂ±\n\\pm\n0.0\n88.0\nÂ±\n\\pm\n0.0\n56.0\nÂ±\n\\pm\n0.0\n46.0\nÂ±\n\\pm\n0.0\nMP\n(%)\nNo obstacles\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n99.2\nÂ±\n\\pm\n7.5\n95.3\nÂ±\n\\pm\n18.2\n10m\n100.0\nÂ±\n\\pm\n0.0\n99.3\nÂ±\n\\pm\n7.0\n97.7\nÂ±\n\\pm\n12.8\n91.7\nÂ±\n\\pm\n21.9\n88.5\nÂ±\n\\pm\n23.4\n5m\n98.0\nÂ±\n\\pm\n9.7\n99.5\nÂ±\n\\pm\n3.3\n96.8\nÂ±\n\\pm\n10.2\n79.6\nÂ±\n\\pm\n22.9\n70.3\nÂ±\n\\pm\n27.6\nFR\n(m)\nNo obstacles\n1.13\nÂ±\n\\pm\n0.18\n1.61\nÂ±\n\\pm\n0.13\n1.93\nÂ±\n\\pm\n0.06\n2.15\nÂ±\n\\pm\n0.06\n2.32\nÂ±\n\\pm\n0.05\n10m\n1.08\nÂ±\n\\pm\n0.15\n1.56\nÂ±\n\\pm\n0.12\n1.88\nÂ±\n\\pm\n0.06\n2.14\nÂ±\n\\pm\n0.06\n2.33\nÂ±\n\\pm\n0.06\n5m\n1.04\nÂ±\n\\pm\n0.13\n1.47\nÂ±\n\\pm\n0.09\n1.85\nÂ±\n\\pm\n0.05\n2.12\nÂ±\n\\pm\n0.05\n2.32\nÂ±\n\\pm\n0.07\nMS\n(m)\nNo obstacles\n1.60\nÂ±\n\\pm\n0.01\n1.55\nÂ±\n\\pm\n0.02\n1.47\nÂ±\n\\pm\n0.03\n1.39\nÂ±\n\\pm\n0.02\n1.34\nÂ±\n\\pm\n0.02\n10m\n1.60\nÂ±\n\\pm\n0.01\n1.53\nÂ±\n\\pm\n0.02\n1.46\nÂ±\n\\pm\n0.03\n1.38\nÂ±\n\\pm\n0.02\n1.34\nÂ±\n\\pm\n0.02\n5m\n1.60\nÂ±\n\\pm\n0.01\n1.51\nÂ±\n\\pm\n0.02\n1.44\nÂ±\n\\pm\n0.03\n1.37\nÂ±\n\\pm\n0.02\n1.32\nÂ±\n\\pm\n0.03\nAL\nNo obstacles\n0.97\nÂ±\n\\pm\n0.02\n0.94\nÂ±\n\\pm\n0.01\n0.89\nÂ±\n\\pm\n0.02\n0.85\nÂ±\n\\pm\n0.02\n0.84\nÂ±\n\\pm\n0.04\n10m\n0.97\nÂ±\n\\pm\n0.02\n0.94\nÂ±\n\\pm\n0.01\n0.89\nÂ±\n\\pm\n0.02\n0.85\nÂ±\n\\pm\n0.02\n0.84\nÂ±\n\\pm\n0.03\n5m\n0.97\nÂ±\n\\pm\n0.02\n0.93\nÂ±\n\\pm\n0.02\n0.88\nÂ±\n\\pm\n0.02\n0.84\nÂ±\n\\pm\n0.03\n0.83\nÂ±\n\\pm\n0.04\nMDO\n(m)\nNo obstacles\n-\n-\n-\n-\n-\n10m\n2.45\nÂ±\n\\pm\n0.13\n2.27\nÂ±\n\\pm\n0.15\n2.17\nÂ±\n\\pm\n0.18\n2.00\nÂ±\n\\pm\n0.24\n1.68\nÂ±\n\\pm\n0.29\n5m\n2.12\nÂ±\n\\pm\n0.21\n1.88\nÂ±\n\\pm\n0.16\n1.65\nÂ±\n\\pm\n0.29\n1.49\nÂ±\n\\pm\n0.37\n1.35\nÂ±\n\\pm\n0.45\nIV-C\n3\nEvaluation Environments\nComparative evaluation was conducted with five UAVs (one leader and four followers) in various environments to verify generalization performance and robustness. First, evaluations were performed in the training environment with three obstacle density levels: none, sparse with minimum 10m gap, and dense with minimum 5m gap. Two new test environments were introduced: a simplified forest with irregularly distributed trees and rocks, and a more realistic complex forest featuring trees and rocks with detailed trunks and foliage as shown in Fig.\n6\n(b) and (c).\nFigure 7:\nExample trajectory of the proposed policy in the forest test environment with multiple waypoints. The swarm of five UAVs successfully navigates through dense obstacles by following the leader while maintaining cohesion. The leader sequentially visits four waypoints to reach the goal.\nIV-C\n4\nResults and Analysis\nAs shown in Table\nIII\n, the proposed DRL method consistently outperforms all baselines, with performance gaps widening in complex environments. All methods succeed without obstacles, but degrade sharply in cluttered scenarios (5â€“31% success in realistic forests), whereas our policy maintains 72% success and 88.2% mission progress. The proposed method achieves the most compact flock (smallest radius) and organized formation (highest alignment) while maintaining safe separation and obstacle distances, effectively balancing task completion, cohesion, and collision avoidance.\nBeyond quantitative evaluation with single goal points, Fig.\n7\ndemonstrates the proposed policyâ€™s versatility in handling sequential waypoint navigation. The swarm successfully follows the leader through four waypoints in the forest test environment, maintaining cohesion and obstacle avoidance. This capability naturally emerges from the learned flocking behavior and obstacle avoidance, as followers implicitly follow the leader regardless of waypoint number or configuration.\nIV-D\nScalability Analysis\nTo evaluate the scalability of the proposed DRL policy, a single model trained with five UAVs (one leader and four followers) was tested by varying the number of followers from two to ten. Table\nIV\nillustrates performance variation with respect to follower count and environmental complexity.\nSuccess rate and mission progress decreased with increasing swarm size and environmental complexity, with significant degradation when eight or more followers navigated narrow gaps. This decline stems from a geometric constraint: the policy maintains similar altitudes for reliable neighbor detection within LiDARâ€™s limited vertical FOV (\nâˆ’\n7\nâˆ˜\n-7^{\\circ}\nto\n52\nâˆ˜\n52^{\\circ}\n), causing horizontal rather than vertical formation expansion. With 10 followers, the flock radius reaches 2.32m (total diameter Â 4.6m), making navigation through 5m gaps physically challenging. This tight margin between swarm size and obstacle spacing causes high failure rates in large-scale swarms.\nFlock radius increased with swarm size regardless of environment, while minimum separation and alignment decreased. Reduced separation implies higher collision risk, and decreased alignment reveals a key limitation of decentralized control: difficulty achieving directional consensus in large swarms without communication. The policy demonstrates moderate scalability but degrades due to geometric constraints, suggesting that robust large-scale application requires policies that better utilize three-dimensional space.\nIV-E\nAblation Studies\nIV-E\n1\nEffect of Reward Components\nWe validated each reward term by systematically removing components and evaluating six configurations: (1) without flocking reward, (2) without obstacle avoidance, (3) without stable flight, (4) without neighbor perception, (5) uniform weights, and (6) proposed method with tuned weights. Each was trained for 500 million timesteps and tested across three obstacle densities.\nTABLE V:\nAblation study of reward components across environments with varying obstacle densities\nMetric\nMin. obstacle gap\nConfiguration\nw/o\nr\nflocking\nr_{\\text{flocking}}\nw/o\nr\nobstacle\nr_{\\text{obstacle}}\nw/o\nr\nstable\nr_{\\text{stable}}\nw/o\nr\nperception\nr_{\\text{perception}}\nUniform\nProposed\nSR\n(%)\nNo obstacles\n0.0\nÂ±\n\\pm\n0.0\n96.0\nÂ±\n\\pm\n0.0\n11.0\nÂ±\n\\pm\n0.0\n99.0\nÂ±\n\\pm\n0.0\n82.0\nÂ±\n\\pm\n0.0\n100.0\nÂ±\n\\pm\n0.0\n10m\n0.0\nÂ±\n\\pm\n0.0\n96.0\nÂ±\n\\pm\n0.0\n9.0\nÂ±\n\\pm\n0.0\n96.0\nÂ±\n\\pm\n0.0\n74.0\nÂ±\n\\pm\n0.0\n99.0\nÂ±\n\\pm\n0.0\n5m\n0.0\nÂ±\n\\pm\n0.0\n86.0\nÂ±\n\\pm\n0.0\n3.0\nÂ±\n\\pm\n0.0\n67.0\nÂ±\n\\pm\n0.0\n54.0\nÂ±\n\\pm\n0.0\n97.0\nÂ±\n\\pm\n0.0\nMS\n(%)\nNo obstacles\n-\n97.8\nÂ±\n\\pm\n11.8\n28.0\nÂ±\n\\pm\n39.8\n99.2\nÂ±\n\\pm\n8.5\n92.6\nÂ±\n\\pm\n20.9\n100.0\nÂ±\n\\pm\n0.0\n10m\n-\n99.2\nÂ±\n\\pm\n5.8\n32.5\nÂ±\n\\pm\n39.0\n98.9\nÂ±\n\\pm\n6.4\n92.0\nÂ±\n\\pm\n22.6\n99.3\nÂ±\n\\pm\n7.0\n5m\n-\n94.2\nÂ±\n\\pm\n16.3\n35.7\nÂ±\n\\pm\n29.3\n81.3\nÂ±\n\\pm\n30.3\n80.0\nÂ±\n\\pm\n32.5\n99.5\nÂ±\n\\pm\n3.3\nFR\n(m)\nNo obstacles\n-\n2.02\nÂ±\n\\pm\n0.11\n3.60\nÂ±\n\\pm\n0.87\n2.29\nÂ±\n\\pm\n0.15\n2.85\nÂ±\n\\pm\n0.26\n1.61\nÂ±\n\\pm\n0.13\n10m\n-\n2.03\nÂ±\n\\pm\n0.10\n3.35\nÂ±\n\\pm\n0.79\n2.26\nÂ±\n\\pm\n0.14\n2.86\nÂ±\n\\pm\n0.25\n1.56\nÂ±\n\\pm\n0.12\n5m\n-\n2.00\nÂ±\n\\pm\n0.09\n2.94\nÂ±\n\\pm\n0.78\n2.23\nÂ±\n\\pm\n0.14\n2.83\nÂ±\n\\pm\n0.29\n1.47\nÂ±\n\\pm\n0.09\nMS\n(m)\nNo obstacles\n-\n1.58\nÂ±\n\\pm\n0.02\n1.55\nÂ±\n\\pm\n0.05\n1.53\nÂ±\n\\pm\n0.04\n1.42\nÂ±\n\\pm\n0.07\n1.55\nÂ±\n\\pm\n0.02\n10m\n-\n1.58\nÂ±\n\\pm\n0.01\n1.55\nÂ±\n\\pm\n0.04\n1.52\nÂ±\n\\pm\n0.03\n1.40\nÂ±\n\\pm\n0.06\n1.53\nÂ±\n\\pm\n0.02\n5m\n-\n1.57\nÂ±\n\\pm\n0.02\n1.55\nÂ±\n\\pm\n0.04\n1.50\nÂ±\n\\pm\n0.04\n1.38\nÂ±\n\\pm\n0.05\n1.51\nÂ±\n\\pm\n0.02\nAL\nNo obstacles\n-\n0.94\nÂ±\n\\pm\n0.02\n0.90\nÂ±\n\\pm\n0.04\n0.94\nÂ±\n\\pm\n0.01\n0.95\nÂ±\n\\pm\n0.02\n0.94\nÂ±\n\\pm\n0.01\n10m\n-\n0.94\nÂ±\n\\pm\n0.01\n0.90\nÂ±\n\\pm\n0.04\n0.94\nÂ±\n\\pm\n0.01\n0.94\nÂ±\n\\pm\n0.02\n0.94\nÂ±\n\\pm\n0.01\n5m\n-\n0.94\nÂ±\n\\pm\n0.02\n0.90\nÂ±\n\\pm\n0.04\n0.92\nÂ±\n\\pm\n0.03\n0.93\nÂ±\n\\pm\n0.03\n0.93\nÂ±\n\\pm\n0.02\nMDO\n(m)\nNo obstacles\n-\n-\n-\n-\n-\n-\n10m\n-\n2.25\nÂ±\n\\pm\n0.18\n2.22\nÂ±\n\\pm\n0.17\n2.41\nÂ±\n\\pm\n0.15\n2.36\nÂ±\n\\pm\n0.14\n2.27\nÂ±\n\\pm\n0.15\n5m\n-\n1.66\nÂ±\n\\pm\n0.26\n1.86\nÂ±\n\\pm\n0.30\n1.82\nÂ±\n\\pm\n0.27\n1.74\nÂ±\n\\pm\n0.20\n1.88\nÂ±\n\\pm\n0.16\nAs shown in Table\nV\n, removing flocking reward completely destroys collective navigation (0% success), with followers hovering at initial positions while the leader moves away. Without cohesion incentives, followers cannot establish leader-following behavior, demonstrating this reward is fundamentally essential for implicit coordination in communication-free settings.\nRemoving obstacle avoidance shows minor degradation in simple environments (96% success rate) but drops to 86% in dense scenarios, indicating explicit guidance significantly improves robustness despite some learning through collision penalties. The stable flight reward proves criticalâ€”its removal causes catastrophic failure (11%, 9%, and 3% success rates) as followers drift in altitude, losing neighbors outside LiDARâ€™s limited FOV (\nâˆ’\n7\nâˆ˜\n-7^{\\circ}\nto\n52\nâˆ˜\n52^{\\circ}\n), with increased flock radius (2.94m to 3.60m) and erratic flight patterns. This highlights altitude consistency as prerequisite for effective coordination with limited FOV sensors.\nThe neighbor perception reward shows environment-dependent importance, with minimal impact in simple scenarios (99% and 96%) but significant drops to 67% in dense environments, suggesting visibility mechanisms become critical as occlusion increases. Uniform weights achieve moderate performance (82%, 74%, and 54%) but consistently underperform the proposed method (100%, 99%, and 97%). These results validate each componentâ€™s distinct role: flocking enables leader-following, obstacle avoidance ensures safety, stable flight maintains controllability, and perception enhances robustness, with tuned weights effectively balancing these competing objectives.\nIV-E\n2\nEffect of Observable Neighbors\nTo determine the appropriate number of observable neighbors, we performed an ablation study using eleven UAVs (one leader and ten followers) across environments with varying complexity. The results, shown in Fig.\n8\n, indicate a clear trend: while increasing the number of neighbors generally improves success rate, performance gains diminish after a certain point.\nIn the obstacle-free environment, success rate saturated at 94% with only four neighbors, showing no further improvement with additional information. In more challenging environments with obstacles, a larger number of neighbors proved beneficial, though a noticeable slowdown in performance gains was observed. For instance, in the dense obstacles environment (5m spacing), increasing observable neighbors from five to six yielded a substantial 20% increase in success rate (from 26% to 46%). In contrast, further increasing from six to ten provided only a marginal 4% gain (from 46% to 50%), after which performance showed no significant improvement.\nFigure 8:\nAn effect of observable neighbors on success rate in collective navigation.\nThis analysis reveals that\n|\nğ’©\n|\n=\n6\n|\\mathcal{N}|=6\nis a critical threshold capturing the majority of performance benefits from situational awareness, particularly in complex scenarios. Beyond this number, marginal gains in success rate are outweighed by increased computational cost. Specifically, each additional neighbor contributes new data points, and the computational load of the object tracker, which scales quadratically (\nO\nâ€‹\n(\nn\n2\n)\nO(n^{2})\n) with the number of points\nn\nn\n, becomes a significant bottleneck on onboard embedded systems. Therefore, setting observable neighbors to six provides balance, ensuring high performance across various environments without imposing unnecessary computational burdens on the agents.\nFigure 9:\nHardware and system overview. (a) A custom quadrotor equipped with a LiDAR, onboard computer, and reflective tape. (b) The onboard software architecture, showing the flow from sensor data to actuator commands.\nV\nExperiments\nFigure 10:\nReal-world validation in indoor environments. (a), (e) Experimental setups for cluttered blocks and wall. (b), (f) Corresponding swarm trajectories visualizing the flock radius. (c), (g) Minimum separation and (d), (h) Minimum distance to obstacles over time, respectively.\nFigure 11:\nReal-world validation in outdoor environments. (a), (e), (i) Experimental setups for pillars, display tower, and short forest. (b), (f), (j) Corresponding swarm trajectories visualizing the flock radius. (c), (g), (k) Minimum separation and (d), (h), (l) Minimum distance to obstacles over time, respectively.\nV-A\nSystem Overview\nWe used custom 250â€‰mm quadrotors equipped with a Livox Mid-360 LiDAR, an Nvidia Jetson Orin NX, and a Pixhawk 6C flight controller as shown in Fig.\n9\n. To ensure the UAV is detectable from any angle, reflective tape was affixed to all sides of its body. All computation, including FAST-LIO2\n[\n37\n]\nfor ego-pose estimation, an object tracker for neighbor detection and tracking, point downsampling, and neural network inference, was performed onboard.\nV-B\nPerformance of LiDAR-based detection\nWe evaluated the LiDAR-based detection performance under various conditions. The number of LiDAR returns decreases inversely with the square of the distance, and beyond approximately 10 m the low point density prevented reliable detection for our UAV size. In addition, the reflective tapeâ€™s intensity drops sharply at incident angles greater than about 45 degrees, leading to missed detections due to insufficient intensity. To address these limitations, reflective tape was affixed to all sides of the UAV.\nDetection rate is defined as the proportion of instances in which a UAV present in the scene is successfully detected by the LiDAR and precision represents the proportion of LiDAR detections that correspond to actual UAVs. Excluding cases where neighboring UAVs were outside the LiDARâ€™s FOV, our experiments demonstrated a detection rate of 100% and a precision of 99.2%, measured as the ratio of correctly detected instances across all time steps.\nV-C\nExperimental Results\nTo validate the sim-to-real transfer, we conducted experiments in controlled indoor and unstructured outdoor environments, with results in Fig.\n10\n, Fig.\n11\n, and Table\nVI\n. The results presented for each environment were obtained from a single trial, where the reported metrics represent the temporal mean and standard deviation over the entire episode length. The successful sim-to-real transfer of our policy was clearly demonstrated in diverse real-world environments. In indoor trials, the swarm successfully navigated cluttered obstacles while maintaining robust cohesion and separation. The low alignment scores observed were a direct result of confined space, which necessitated frequent maneuvering for collision avoidance. Both the minimum separation between agents and minimum distance to obstacles consistently remained above the collision distance threshold throughout all experiments, ensuring safe navigation without any collisions. Meanwhile, in outdoor scenarios with unpredictable variables such as wind and varying lighting conditions, the swarm demonstrated robust performance with these safety margins maintained across diverse obstacle configurations. The short forest scenario presented the most challenging obstacle avoidance conditions due to its dense vegetation, yet the policy successfully maintained collision-free navigation. The higher alignment scores in open areas suggest the policy adaptively transitions to a more organized and efficient formation when space allows. Collectively, these results validate that the single policy exhibits excellent generalization and robustness under complex and various real-world conditions.\nTABLE VI:\nReal-world experiment results with five UAVs\nMetric\nIndoor environments\nOutdoor environments\nCluttered blocks\nWall\nPillars\nDisplay tower\nShort forest\nFR\n(m)\n2.31\nÂ±\n\\pm\n0.28\n2.54\nÂ±\n\\pm\n0.28\n2.86\nÂ±\n\\pm\n0.53\n2.60\nÂ±\n\\pm\n0.27\n2.30\nÂ±\n\\pm\n0.30\nMS\n(m)\n1.50\nÂ±\n\\pm\n0.28\n1.39\nÂ±\n\\pm\n0.36\n1.34\nÂ±\n\\pm\n0.34\n1.55\nÂ±\n\\pm\n0.16\n1.36\nÂ±\n\\pm\n0.27\nAL\n0.48\nÂ±\n\\pm\n0.40\n0.52\nÂ±\n\\pm\n0.42\n0.72\nÂ±\n\\pm\n0.34\n0.81\nÂ±\n\\pm\n0.21\n0.74\nÂ±\n\\pm\n0.30\nMDO\n(m)\n1.88\nÂ±\n\\pm\n0.92\n4.25\nÂ±\n\\pm\n1.45\n2.82\nÂ±\n\\pm\n1.83\n5.82\nÂ±\n\\pm\n4.69\n1.57\nÂ±\n\\pm\n1.13\nVI\nConclusions\nThis paper introduced a fully communication-free system for collective UAV swarm navigation using a single LiDAR and a DRL policy. Our implicit leader-follower framework, enabled by a robust perception pipeline and a learned controller, allows a swarm to perform complex navigation in communication-denied environments. We rigorously validated our approach through extensive simulations and challenging real-world experiments. The DRL-based policy outperformed existing methods in simulation and demonstrated successful sim-to-real transfer, with a five-UAV swarm navigating diverse indoor and outdoor environments. This study validated the practicality and robustness of using DRL for communication-free collective navigation.\nFuture work will focus on enhancing scalability to larger swarms and exploring more complex collective behaviors like adaptive role-switching.\nAuthor Contributions\nMyong-Yol Choi: Conceptualization, project administration, investigation, methodology, software, validation, data curation, formal analysis, visualization, and writingâ€”original draft.\nHankyoul Ko: Investigation, software, validation, data curation, formal analysis, and visualization.\nHanse Cho: Investigation, data curation, and formal analysis.\nChangseung Kim, Seunghwan Kim, and Jaemin Seo: Investigation.\nHyondong Oh: Supervision, funding acquisition, resources, and writingâ€”review and editing.\nReferences\n[1]\nD.Â Floreano and R.Â J. Wood, â€œScience, technology and the future of small autonomous drones,â€\nNature\n, vol. 521, no. 7553, pp. 460â€“466, 2015.\n[2]\nY.Â Alqudsi and M.Â Makaraci, â€œUAV swarms: research, challenges, and future directions,â€\nJournal of Engineering and Applied Science\n, vol.Â 72, no.Â 1, p.Â 12, 2025.\n[3]\nM.Â Dorigo, G.Â Theraulaz, and V.Â Trianni, â€œReflections on the future of swarm robotics,â€\nScience robotics\n, vol.Â 5, no.Â 49, pp. 1â€“3, 2020.\n[4]\nC.Â W. Reynolds, â€œFlocks, herds and schools: A distributed behavioral model,â€ in\nProc. 14th Annual Conference on Computer Graphics and Interactive Techniques\n, 1987, pp. 25â€“34.\n[5]\nT.Â Vicsek and A.Â Zafeiris, â€œCollective motion,â€\nPhysics Reports\n, vol. 517, no. 3-4, pp. 71â€“140, 2012.\n[6]\nR.Â Olfati-Saber, â€œFlocking for multi-agent dynamic systems: Algorithms and theory,â€\nIEEE Transactions on Automatic Control\n, vol.Â 51, no.Â 3, pp. 401â€“420, 2006.\n[7]\nM.Â Coppola, K.Â N. McGuire, C.Â DeÂ Wagter, and G.Â C. DeÂ Croon, â€œA survey on swarming with micro air vehicles: Fundamental challenges and constraints,â€\nFrontiers in Robotics and AI\n, vol.Â 7, p.Â 18, 2020.\n[8]\nW.Â Chen, J.Â Liu, H.Â Guo, and N.Â Kato, â€œToward robust and intelligent drone swarm: Challenges and future directions,â€\nIEEE Network\n, vol.Â 34, no.Â 4, pp. 278â€“283, 2020.\n[9]\nA.Â M. Berdahl, A.Â B. Kao, A.Â Flack, P.Â A. Westley, E.Â A. Codling, I.Â D. Couzin, A.Â I. Dell, and D.Â Biro, â€œCollective animal navigation and migratory culture: From theoretical models to empirical evidence,â€\nPhilosophical Transactions of the Royal Society B: Biological Sciences\n, vol. 373, no. 1746, p. 20170009, 2018.\n[10]\nI.Â D. Couzin, J.Â Krause, N.Â R. Franks, and S.Â A. Levin, â€œEffective leadership and decision-making in animal groups on the move,â€\nNature\n, vol. 433, no. 7025, pp. 513â€“516, 2005.\n[11]\nJ.Â A. Preiss, W.Â Honig, G.Â S. Sukhatme, and N.Â Ayanian, â€œCrazyswarm: A large nano-quadcopter swarm,â€ in\nProc. IEEE International Conference on Robotics and Automation\n, 2017, pp. 3299â€“3304.\n[12]\nE.Â Soria, F.Â Schiano, and D.Â Floreano, â€œPredictive control of aerial swarms in cluttered environments,â€\nNature Machine Intelligence\n, vol.Â 3, no.Â 6, pp. 545â€“554, 2021.\n[13]\nJ.Â Tordesillas and J.Â P. How, â€œMader: Trajectory planner in multiagent and dynamic environments,â€\nIEEE Transactions on Robotics\n, vol.Â 38, no.Â 1, pp. 463â€“476, 2021.\n[14]\nX.Â Zhou, J.Â Zhu, H.Â Zhou, C.Â Xu, and F.Â Gao, â€œEgo-swarm: A fully autonomous and decentralized quadrotor swarm system in cluttered environments,â€ in\n2021 IEEE international conference on robotics and automation\n, 2021, pp. 4101â€“4107.\n[15]\nX.Â Zhou, X.Â Wen, Z.Â Wang, Y.Â Gao, H.Â Li, Q.Â Wang, T.Â Yang, H.Â Lu, Y.Â Cao, and C.Â Xu, â€œSwarm of micro flying robots in the wild,â€\nScience Robotics\n, vol.Â 7, no.Â 66, pp. 1â€“18, 2022.\n[16]\nG.Â VÃ¡sÃ¡rhelyi, C.Â VirÃ¡gh, G.Â Somorjai, T.Â Nepusz, A.Â E. Eiben, and T.Â Vicsek, â€œOptimized flocking of autonomous drones in confined environments,â€\nScience Robotics\n, vol.Â 3, no.Â 20, pp. 1â€“13, 2018.\n[17]\nC.Â Yan, C.Â Wang, X.Â Xiang, K.Â H. Low, X.Â Wang, X.Â Xu, and L.Â Shen, â€œCollision-avoiding flocking with multiple fixed-wing UAVs in obstacle-cluttered environments: A task-specific curriculum-based MADRL approach,â€\nIEEE Transactions on Neural Networks and Learning Systems\n, vol.Â 35, no.Â 8, pp. 10â€‰894â€“10â€‰908, 2023.\n[18]\nZ.Â Huang, Z.Â Yang, R.Â Krupani, B.Â ÅenbaÅŸlar, S.Â Batra, and G.Â S. Sukhatme, â€œCollision avoidance and navigation for a quadrotor swarm using end-to-end deep reinforcement learning,â€ in\n2024 IEEE International Conference on Robotics and Automation\n, 2024, pp. 300â€“306.\n[19]\nM.Â Saska, T.Â Baca, J.Â Thomas, J.Â Chudoba, L.Â Preucil, T.Â Krajnik, J.Â Faigl, G.Â Loianno, and V.Â Kumar, â€œSystem for deployment of groups of unmanned micro aerial vehicles in GPS-denied environments using onboard visual relative localization,â€\nAutonomous Robots\n, vol.Â 41, pp. 919â€“944, 2017.\n[20]\nF.Â Schilling, J.Â Lecoeur, F.Â Schiano, and D.Â Floreano, â€œLearning vision-based flight in drone swarms by imitation,â€\nIEEE Robotics and Automation Letters\n, vol.Â 4, no.Â 4, pp. 4523â€“4530, 2019.\n[21]\nF.Â Schilling, F.Â Schiano, and D.Â Floreano, â€œVision-based drone flocking in outdoor environments,â€\nIEEE Robotics and Automation Letters\n, vol.Â 6, no.Â 2, pp. 2954â€“2961, 2021.\n[22]\nA.Â Ahmad, D.Â B. Licea, G.Â Silano, T.Â BÃ¡Äa, and M.Â Saska, â€œPacnav: A collective navigation approach for UAV swarms deprived of communication and external localization,â€\nBioinspiration & Biomimetics\n, vol.Â 17, no.Â 6, p. 066019, 2022.\n[23]\nF.Â Wang, J.Â Huang, K.Â H. Low, and T.Â Hu, â€œCollective navigation of aerial vehicle swarms: A flocking inspired approach,â€\nIEEE Transactions on Intelligent Vehicles\n, vol.Â 9, no.Â 1, pp. 1040â€“1053, 2023.\n[24]\nF.Â Zhu, Y.Â Ren, L.Â Yin, F.Â Kong, Q.Â Liu, R.Â Xue, W.Â Liu, Y.Â Cai, G.Â Lu, and H.Â Li, â€œSwarm-LIO2: Decentralized, efficient LiDAR-inertial odometry for UAV swarms,â€\nIEEE Transactions on Robotics\n, vol.Â 41, pp. 960â€“981, 2024.\n[25]\nY.Â Wan, J.Â Tang, Z.Â Zhao, and X.Â Chen, â€œDistributed vision-only cooperative flight of multiple quadrotors in unknown cramped environments,â€\nIEEE Transactions on Intelligent Vehicles\n, vol.Â 10, no.Â 7, pp. 3902â€“3915, 2024.\n[26]\nH.Â Huang, G.Â Zhu, Z.Â Fan, H.Â Zhai, Y.Â Cai, Z.Â Shi, Z.Â Dong, and Z.Â Hao, â€œVision-based distributed multi-UAV collision avoidance via deep reinforcement learning for navigation,â€ in\n2022 IEEE/RSJ International Conference on Intelligent Robots and Systems\n, 2022, pp. 13â€‰745â€“13â€‰752.\n[27]\nC.Â Bai, P.Â Yan, H.Â Piao, W.Â Pan, and J.Â Guo, â€œLearning-based multi-UAV flocking control with limited visual field and instinctive repulsion,â€\nIEEE Transactions on Cybernetics\n, vol.Â 54, no.Â 1, pp. 462â€“475, 2023.\n[28]\nJ.Â Schulman, F.Â Wolski, P.Â Dhariwal, A.Â Radford, and O.Â Klimov, â€œProximal policy optimization algorithms,â€\narXiv preprint arXiv:1707.06347\n, 2017.\n[29]\nM.Â Ester, H.-P. Kriegel, J.Â Sander, and X.Â Xu, â€œA density-based algorithm for discovering clusters in large spatial databases with noise,â€ in\nProc. 2nd International Conference on Knowledge Discovery and Data Mining\n, 1996, pp. 226â€“231.\n[30]\nR.Â E. Kalman, â€œA new approach to linear filtering and prediction problems,â€\nJournal of Basic Engineering\n, vol.Â 82, no.Â 1, pp. 35â€“45, 1960.\n[31]\nM.Â Ballerini, N.Â Cabibbo, R.Â Candelier, A.Â Cavagna, E.Â Cisbani, I.Â Giardina, V.Â Lecomte, A.Â Orlandi, G.Â Parisi, A.Â Procaccini\netÂ al.\n, â€œInteraction ruling animal collective behavior depends on topological rather than metric distance: Evidence from a field study,â€\nProceedings of the National Academy of Sciences\n, vol. 105, no.Â 4, pp. 1232â€“1237, 2008.\n[32]\nJ.Â Schulman, P.Â Moritz, S.Â Levine, M.Â Jordan, and P.Â Abbeel, â€œHigh-dimensional continuous control using generalized advantage estimation,â€\narXiv preprint arXiv:1506.02438\n, 2015.\n[33]\nB.Â Xu, F.Â Gao, C.Â Yu, R.Â Zhang, Y.Â Wu, and Y.Â Wang, â€œOmnidrones: An efficient and flexible platform for reinforcement learning in drone control,â€\nIEEE Robotics and Automation Letters\n, vol.Â 9, no.Â 3, pp. 2838â€“2844, 2024.\n[34]\nS.Â LaValle, â€œRapidly-exploring random trees: A new tool for path planning,â€\nResearch Report 9811\n, 1998.\n[35]\nO.Â Khatib, â€œReal-time obstacle avoidance for manipulators and mobile robots,â€\nThe International Journal of Robotics Research\n, vol.Â 5, no.Â 1, pp. 90â€“98, 1986.\n[36]\nZ.Â Fang, D.Â Jiang, J.Â Huang, C.Â Cheng, Q.Â Sha, B.Â He, and G.Â Li, â€œAutonomous underwater vehicle formation control and obstacle avoidance using multi-agent generative adversarial imitation learning,â€\nOcean Engineering\n, vol. 262, p. 112182, 2022.\n[37]\nW.Â Xu, Y.Â Cai, D.Â He, J.Â Lin, and F.Â Zhang, â€œFAST-LIO2: Fast direct LiDAR-inertial odometry,â€\nIEEE Transactions on Robotics\n, vol.Â 38, no.Â 4, pp. 2053â€“2073, 2022.\nMyong-Yol Choi\nis currently a Ph.D. candidate at the Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea. He received a B.S. degree in Mechanical and Aerospace Engineering from Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea, in 2022. His research focuses on learning-based collective behaviors for autonomous swarms of unmanned vehicles.\nHankyoul Ko\nis currently an undergraduate student at the Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea. His research focuses on unmanned aerial vehicles, sensor fusion for localization, and robotic perception.\nHanse Cho\nis currently a M.S. student at the Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea. He received a B.S. degree in Mechanical and Control Engineering from Handong Global University, Pohang, Republic of Korea, in 2024. His research focuses on flocking and swarm control.\nChangseung Kim\nis currently a Ph.D. candidate at the Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea. He received a B.S. degree in Mechanical Engineering from Inha University, Incheon, Republic of Korea, in 2021. His research focuses on multi-sensor fusion simultaneous localization and mapping.\nSeunghwan Kim\nis currently a Ph.D. candidate at the Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea. He received a B.S. degree in Mechanical Engineering from Inha University, Incheon, Republic of Korea, in 2021. His research focuses on environmental monitoring, active perception, and autonomous exploration.\nJaemin Seo\nis currently a Ph.D. candidate at the Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea. He received a B.S. degree in Aerospace Engineering from Pusan National University, Busan, Republic of Korea, in 2019. His research focuses on environmental monitoring, informative path planning, and cooperative search strategy.\nHyondong Oh\nis currently an Associate Professor at the Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea. He received his B.S. and M.S. degrees in Aerospace Engineering from Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea, in 2004 and 2010, respectively, and a Ph.D. in Aerospace Engineering from Cranfield University, Cranfield, United Kingdom, in 2013. He was a Postdoctoral Researcher at the University of Surrey, Guildford, United Kingdom, from 2013 to 2014, an Assistant Professor at Loughborough University, Loughborough, United Kingdom, from 2014 to 2016, and an Associate Professor at the Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea, from 2016 to 2025. His research interests include decision making for unmanned vehicles, cooperative control, path planning, nonlinear guidance and control, and sensor/information fusion.",
    "preview_text": "This paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UAV) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments. Inspired by biological swarms where informed individuals guide groups without explicit communication, we employ an implicit leader-follower framework. In this paradigm, only the leader possesses goal information, while follower UAVs learn robust policies using only onboard LiDAR sensing, without requiring any inter-agent communication or leader identification. Our system utilizes LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, providing reliable perception independent of external positioning systems. The core of our approach is a DRL controller, trained in GPU-accelerated Nvidia Isaac Sim, that enables followers to learn complex emergent behaviors - balancing flocking and obstacle avoidance - using only local perception. This allows the swarm to implicitly follow the leader while robustly addressing perceptual challenges such as occlusion and limited field-of-view. The robustness and sim-to-real transfer of our approach are confirmed through extensive simulations and challenging real-world experiments with a swarm of five UAVs, which successfully demonstrated collective navigation across diverse indoor and outdoor environments without any communication or external localization.\n\nCommunication-Free Collective Navigation for a Swarm of UAVs\nvia LiDAR-Based Deep Reinforcement Learning\nMyong-Yol Choi, Hankyoul Ko, Hanse Cho, Changseung Kim, Seunghwan Kim, Jaemin Seo,\nand Hyondong Oh\nThis research was supported by Theater Defense Research Center funded by Defense Acquisition Program Administration under Grant UD200043CD, National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (2023R1A2C2003130), and Unmanned Vehicles Core Technology Research and Development Program t",
    "is_relevant": true,
    "relevance_score": 7.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ— é€šä¿¡æ— äººæœºç¾¤é›†ä½“å¯¼èˆªæ§åˆ¶å™¨ï¼Œåˆ©ç”¨LiDARæ„ŸçŸ¥å®ç°éšå¼é¢†å¯¼-è·Ÿéšæ¡†æ¶ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­éªŒè¯äº†é²æ£’æ€§å’Œä»¿çœŸåˆ°ç°å®çš„è¿ç§»èƒ½åŠ›ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\næ— äººæœºé›†ç¾¤åœ¨æœç´¢æ•‘æ´ã€ç¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸæ½œåŠ›å·¨å¤§ï¼Œä½†å…¶å®é™…éƒ¨ç½²å¸¸å—é™äºé€šä¿¡å—é™ç¯å¢ƒï¼ˆå¦‚å¹²æ‰°ã€æ‹’æ­¢ï¼‰ã€‚ç°æœ‰é›†ç¾¤å¯¼èˆªæ–¹æ³•å¤šä¾èµ–æŒç»­é€šä¿¡æˆ–å…¨å±€å®šä½ï¼Œåœ¨é€šä¿¡ä¸­æ–­æ—¶å¤±æ•ˆã€‚å—ç”Ÿç‰©é›†ç¾¤ä¸­å°‘æ•°â€œçŸ¥æƒ…ä¸ªä½“â€æ— éœ€æ˜¾å¼é€šä¿¡å³å¯å¼•å¯¼ç¾¤ä½“çš„ç°è±¡å¯å‘ï¼Œæœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ç§**å®Œå…¨æ— éœ€é€šä¿¡**ã€ä»…ä¾èµ–æœºè½½æ„ŸçŸ¥çš„æ— äººæœºé›†ç¾¤é›†ä½“å¯¼èˆªç³»ç»Ÿã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¿€å…‰é›·è¾¾ï¼ˆLiDARï¼‰å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„é€šä¿¡æ— å…³é›†ä½“å¯¼èˆªæ¡†æ¶ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š\n- **éšå¼é¢†å¯¼-è·Ÿéšæ¶æ„**ï¼šä»…é¢†å¯¼è€…çŸ¥æ™“ç›®æ ‡ç‚¹ï¼Œè·Ÿéšè€…å®Œå…¨æ— ç›®æ ‡ä¿¡æ¯ä¸”æ— éœ€è¯†åˆ«é¢†å¯¼è€…ï¼Œé€šè¿‡å±€éƒ¨äº¤äº’å®ç°éšå¼è·Ÿéšã€‚\n- **é²æ£’çš„LiDARæ„ŸçŸ¥ç®¡é“**ï¼šåˆ©ç”¨åå°„å¸¦å¢å¼ºæ£€æµ‹ï¼Œé€šè¿‡ç‚¹äº‘è¿‡æ»¤ã€DBSCANèšç±»å’Œæ‰©å±•å¡å°”æ›¼æ»¤æ³¢ï¼ˆEKFï¼‰è·Ÿè¸ªï¼Œå®ç°ç¨³å®šã€å…‰ç…§ä¸å˜çš„é‚»å±…ä¸éšœç¢ç‰©æ„ŸçŸ¥ï¼Œä¸ä¾èµ–å¤–éƒ¨å®šä½ã€‚\n- **DRLæ§åˆ¶ç­–ç•¥**ï¼šè·Ÿéšè€…ç­–ç•¥ä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰è®­ç»ƒï¼Œè¾“å…¥ä»…ä¸ºå±€éƒ¨LiDARè§‚æµ‹ï¼ˆè‡ªèº«çŠ¶æ€ã€æœ‰é™é‚»å±…ä¿¡æ¯ã€äºŒç»´å æ®æ …æ ¼ï¼‰ï¼Œè¾“å‡ºé€Ÿåº¦æŒ‡ä»¤ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¤šç›®æ ‡å¥–åŠ±å‡½æ•°ï¼ˆèšé›†ã€é¿éšœã€ç¨³å®šé£è¡Œã€é‚»å±…æ„ŸçŸ¥ï¼‰ï¼Œæ™ºèƒ½ä½“è‡ªä¸»å­¦ä¹ åœ¨å¤æ‚ç¯å¢ƒä¸­å¹³è¡¡é›†ç¾¤å‡èšåŠ›ä¸é¿éšœçš„æ¶Œç°è¡Œä¸ºã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\næ–¹æ³•åœ¨ä»¿çœŸå’Œå®ç‰©å®éªŒä¸­å‡å¾—åˆ°éªŒè¯ï¼š\n- **ä»¿çœŸå¯¹æ¯”**ï¼šåœ¨å¤šç§å¤æ‚éšœç¢ç¯å¢ƒä¸­ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨æˆåŠŸç‡ï¼ˆSRï¼‰ã€ä»»åŠ¡è¿›åº¦ï¼ˆMPï¼‰ã€é›†ç¾¤ç´§å‡‘åº¦ï¼ˆFRï¼‰ç­‰æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºPACNavã€VPFã€DAggerç­‰åŸºçº¿æ–¹æ³•ã€‚\n- **å®ç‰©éªŒè¯**ï¼šä½¿ç”¨äº”æ¶è‡ªå®šä¹‰æ— äººæœºï¼Œåœ¨å¤šæ ·åŒ–çš„å®¤å†…å¤–çœŸå®åœºæ™¯ï¼ˆå¦‚æ‚ä¹±åŒºå—ã€æŸ±å­ã€çŸ­æ£®æ—ï¼‰ä¸­æˆåŠŸå®ç°äº†æ— é€šä¿¡ã€æ— å¤–éƒ¨å®šä½çš„é›†ä½“å¯¼èˆªï¼Œè¯æ˜äº†ä¼˜å¼‚çš„**ä»¿çœŸåˆ°ç°å®è¿ç§»èƒ½åŠ›**ã€‚æ‰€æœ‰å®éªŒå‡ä¿æŒå®‰å…¨é—´è·ï¼Œæ— ç¢°æ’å‘ç”Ÿã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\næœ¬ç ”ç©¶é¦–æ¬¡å®ç°äº†åŸºäºLiDARå’ŒDRLã€ä¸”ç»è¿‡çœŸå®ç¯å¢ƒéªŒè¯çš„å®Œå…¨é€šä¿¡æ— å…³æ— äººæœºé›†ç¾¤å¯¼èˆªç³»ç»Ÿã€‚å…¶ä»·å€¼åœ¨äºï¼š\n- **æŠ€æœ¯çªç ´**ï¼šä¸ºåœ¨GPSæ‹’æ­¢ã€é€šä¿¡ä¸­æ–­çš„å¤æ‚ç¯å¢ƒä¸‹å®ç°å¯é ã€è‡ªä¸»çš„é›†ç¾¤ä½œä¸šæä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚\n- **ç†è®ºè´¡çŒ®**ï¼šéªŒè¯äº†DRLåœ¨è§£å†³é€šä¿¡æ— å…³ååŒæ§åˆ¶ä¸­æ„ŸçŸ¥-å†³ç­–è€¦åˆé—®é¢˜ä¸Šçš„å¼ºå¤§èƒ½åŠ›ï¼Œä»¥åŠéšå¼é¢†å¯¼æœºåˆ¶çš„å·¥ç¨‹å¯è¡Œæ€§ã€‚\n- **åº”ç”¨å‰æ™¯**ï¼šæå‡äº†æ— äººæœºé›†ç¾¤åœ¨å†›äº‹ä¾¦å¯Ÿã€ç¾éš¾å“åº”ç­‰å…³é”®ä»»åŠ¡ä¸­çš„ç”Ÿå­˜æ€§å’Œé€‚ç”¨æ€§ï¼Œä¸ºå¤§è§„æ¨¡ã€é«˜è‡ªä¸»æ€§é›†ç¾¤ç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **ä¸€ã€ ç ”ç©¶é—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**é€šä¿¡å—é™/æ‹’æ­¢ç¯å¢ƒ**ä¸‹æ— äººæœºé›†ç¾¤çš„**é›†ä½“å¯¼èˆª**é—®é¢˜ã€‚å…·ä½“æŒ‘æˆ˜åŒ…æ‹¬ï¼š\n- **æ— é€šä¿¡**ï¼šæ— äººæœºä¹‹é—´ä¸èƒ½äº¤æ¢ä»»ä½•çŠ¶æ€ã€ç›®æ ‡æˆ–èº«ä»½ä¿¡æ¯ã€‚\n- **æ— å¤–éƒ¨å®šä½**ï¼šä¸ä¾èµ–GNSSæˆ–è¿åŠ¨æ•æ‰ç³»ç»Ÿç­‰å¤–éƒ¨åŸºç¡€è®¾æ–½ã€‚\n- **æ„ŸçŸ¥å—é™**ï¼šä»…ä½¿ç”¨æœºè½½ä¼ æ„Ÿå™¨ï¼ˆæ–‡ä¸­ä¸ºLiDARï¼‰ï¼Œä¸”é¢ä¸´é®æŒ¡ã€æœ‰é™è§†åœºç­‰ç°å®é—®é¢˜ã€‚\n- **ä¿¡æ¯ä¸å¯¹ç§°**ï¼šåªæœ‰å°‘æ•°â€œé¢†å¯¼è€…â€æ— äººæœºçŸ¥æ™“ç›®æ ‡ç‚¹ï¼Œå¤§é‡â€œè·Ÿéšè€…â€æ—¢ä¸çŸ¥é“ç›®æ ‡ï¼Œä¹Ÿæ— æ³•è¯†åˆ«è°æ˜¯é¢†å¯¼è€…ã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ª**å®Œå…¨æ— éœ€é€šä¿¡çš„LiDAR-æ·±åº¦å¼ºåŒ–å­¦ä¹ é›†ç¾¤å¯¼èˆªç³»ç»Ÿ**ï¼Œå…¶åˆ›æ–°æ€§ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹ä¸‰ä¸ªç´§å¯†å…³è”çš„æ–¹é¢ï¼š\n\n1.  **å…¨é€šä¿¡è‡ªç”±çš„LiDARæ„ŸçŸ¥æ¡†æ¶**\n    - **æŠ€æœ¯**ï¼šé‡‡ç”¨å•ä¸€360åº¦æ‰«æLiDARï¼Œé€šè¿‡**åå°„å¸¦**äº§ç”Ÿé«˜å¼ºåº¦ç‚¹äº‘ï¼Œç»“åˆ**DBSCANèšç±»**å’Œ**æ‰©å±•å¡å°”æ›¼æ»¤æ³¢è·Ÿè¸ª**ï¼Œå®ç°ç¨³å®šã€å…‰ç…§ä¸å˜çš„é‚»å±…æ£€æµ‹ä¸è·Ÿè¸ªã€‚\n    - **åˆ›æ–°**ï¼šæ‘’å¼ƒäº†è§†è§‰æ–¹æ¡ˆå¯¹å…‰ç…§çš„ä¾èµ–å’Œå¤šç›¸æœºç³»ç»Ÿçš„å¤æ‚æ€§ï¼Œæä¾›äº†ä¸€ç§æ›´å¯é ã€è®¡ç®—æ•ˆç‡æ›´é«˜çš„æœºè½½æ„ŸçŸ¥æ–¹æ¡ˆï¼Œä¸”**å®Œå…¨ä¸ä¾èµ–ä»»ä½•æœºé—´é€šä¿¡**ã€‚\n\n2.  **åŸºäºDRLçš„éšå¼é¢†å¯¼è€…-è·Ÿéšè€…æ§åˆ¶ç­–ç•¥**\n    - **æŠ€æœ¯**ï¼šé‡‡ç”¨**è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•**è®­ç»ƒè·Ÿéšè€…æ— äººæœºçš„ç­–ç•¥ã€‚ç­–ç•¥è¾“å…¥**ä»…åŒ…å«**è‡ªèº«è¿åŠ¨çŠ¶æ€ï¼ˆé€Ÿåº¦ã€å§¿æ€ï¼‰ã€å±€éƒ¨æ„ŸçŸ¥çš„é‚»å±…çŠ¶æ€ï¼ˆç›¸å¯¹ä½ç½®ã€é€Ÿåº¦ï¼‰å’ŒLiDARç”Ÿæˆçš„éšœç¢ç‰©å æ®æ …æ ¼ï¼Œ**åˆ»æ„æ’é™¤äº†å…¨å±€ä½ç½®ä¿¡æ¯**ä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚\n    - **åˆ›æ–°**ï¼š\n        - **éšå¼è·Ÿéš**ï¼šè·Ÿéšè€…é€šè¿‡DRLå­¦ä¹ â€œèšé›†â€ä¸â€œé¿éšœâ€çš„å¹³è¡¡è¡Œä¸ºï¼Œä»è€Œ**è‡ªå‘åœ°**è·Ÿéšé‚»å±…ç¾¤ä½“ç§»åŠ¨ã€‚ç”±äºé¢†å¯¼è€…å‘ç›®æ ‡è¿åŠ¨ï¼Œæ•´ä¸ªé›†ç¾¤ä¾¿è¢«â€œç‰µå¼•â€è‡³ç›®çš„åœ°ã€‚**æ— éœ€è¯†åˆ«é¢†å¯¼è€…ï¼Œä¹Ÿæ— éœ€çŸ¥æ™“ç›®æ ‡**ã€‚\n        - **å¥–åŠ±å‡½æ•°è®¾è®¡**ï¼šç²¾å¿ƒè®¾è®¡çš„å¤šç›®æ ‡å¥–åŠ±å‡½æ•°ï¼ˆèšé›†ã€é¿éšœã€ç¨³å®šé£è¡Œã€é‚»å±…æ„ŸçŸ¥ï¼‰ï¼Œé©±åŠ¨æ™ºèƒ½ä½“å­¦ä¹ åœ¨å¤æ‚æ„ŸçŸ¥çº¦æŸä¸‹ï¼ˆå¦‚é®æŒ¡ã€å‚ç›´è§†åœºæœ‰é™ï¼‰çš„é²æ£’ååŒè¡Œä¸ºã€‚\n\n3.  **å…¨é¢çš„ä»¿çœŸåˆ°ç°å®éªŒè¯**\n    - **æŠ€æœ¯**ï¼šåœ¨GPUåŠ é€Ÿçš„Nvidia Isaac Simä¸­è¿›è¡Œå¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒï¼ˆ5äº¿æ­¥ï¼‰ï¼Œå¹¶åœ¨**å®¤å†…å¤–å¤šç§çœŸå®å¤æ‚ç¯å¢ƒ**ä¸­ï¼Œä½¿ç”¨**äº”æ¶æ— äººæœºé›†ç¾¤**è¿›è¡Œäº†æˆåŠŸéªŒè¯ã€‚\n    - **åˆ›æ–°**ï¼šæ®ä½œè€…æ‰€çŸ¥ï¼Œè¿™æ˜¯**é¦–ä¸ª**åœ¨çœŸå®ä¸–ç•Œä¸­éªŒè¯çš„ã€åŸºäºLiDARå’ŒDRLçš„ã€å®Œå…¨æ— éœ€ä¿¡æ¯äº¤æ¢çš„æ— äººæœºé›†ç¾¤é›†ä½“å¯¼èˆªç³»ç»Ÿã€‚è¯æ˜äº†æ–¹æ³•çš„**å®é™…å¯è¡Œæ€§ä¸å¼ºé²æ£’æ€§**ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆæ¶æ„**\nè®ºæ–‡é€šè¿‡ä¸€ä¸ª**å»ä¸­å¿ƒåŒ–æ¶æ„**æ•´åˆäº†ä¸Šè¿°åˆ›æ–°ç‚¹ï¼Œè§£å†³é—®é¢˜ï¼š\n\n1.  **æ„ŸçŸ¥å±‚**ï¼šæ¯ä¸ªæ— äººæœºç‹¬ç«‹è¿è¡ŒLiDARæƒ¯æ€§é‡Œç¨‹è®¡å’Œå¯¹è±¡è·Ÿè¸ªå™¨ï¼Œå®æ—¶è¾“å‡ºè‡ªèº«çŠ¶æ€ã€é‚»å±…åˆ—è¡¨å’Œéšœç¢ç‰©ä¿¡æ¯ã€‚\n2.  **å†³ç­–å±‚**ï¼š\n    - **é¢†å¯¼è€…**ï¼šä½¿ç”¨ä¼ ç»Ÿçš„å±€éƒ¨è§„åˆ’å™¨ï¼ˆRRT+äººå·¥åŠ¿åœºï¼‰ç»“åˆå·²çŸ¥èˆªç‚¹è¿›è¡Œå¯¼èˆªã€‚\n    - **è·Ÿéšè€…**ï¼šå°†æ„ŸçŸ¥å±‚è¾“å‡ºçš„**å±€éƒ¨è§‚å¯Ÿ**è¾“å…¥åˆ°**è®­ç»ƒå¥½çš„DRLç­–ç•¥ç½‘ç»œ**ä¸­ï¼Œç½‘ç»œç›´æ¥è¾“å‡ºé€Ÿåº¦æŒ‡ä»¤ã€‚\n3.  **æ¶Œç°çš„é›†ä½“è¡Œä¸º**ï¼šæ¯ä¸ªè·Ÿéšè€…ç‹¬ç«‹æ‰§è¡ŒåŸºäºå±€éƒ¨è§‚å¯Ÿçš„ç­–ç•¥ï¼Œä¸ºäº†è·å¾—â€œèšé›†â€å¥–åŠ±è€Œè¯•å›¾ä¿æŒåœ¨é‚»å±…é™„è¿‘ã€‚å½“é¢†å¯¼è€…å‘ç›®æ ‡ç§»åŠ¨æ—¶ï¼Œè¿™ç§å±€éƒ¨èšé›†è¡Œä¸ºåƒæ³¢çº¹ä¸€æ ·ä¼ é€’ï¼Œä½¿å¾—æ•´ä¸ªé›†ç¾¤**æ¶Œç°å‡º**é›†ä½“å‘ç›®æ ‡å¯¼èˆªçš„å…¨å±€æ™ºèƒ½ï¼Œè€Œæ— éœ€ä»»ä½•æ˜¾å¼çš„å…¨å±€åè°ƒæˆ–é€šä¿¡ã€‚\n\n### **å››ã€ å®é™…ä»·å€¼**\n- **å†›äº‹/å®‰é˜²**ï¼šåœ¨ç”µå­å¯¹æŠ—ã€é€šä¿¡å¹²æ‰°æˆ–ä¿å¯†è¦æ±‚é«˜çš„åœºæ™¯ä¸‹ï¼Œå®ç°å¯é çš„é›†ç¾¤ä¾¦å¯Ÿã€ç›‘è§†ä¸çªå‡»ã€‚\n- **ç¾éš¾å“åº”**ï¼šåœ¨é€šä¿¡åŸºç¡€è®¾æ–½æŸæ¯çš„ç¾åŒºï¼ˆå¦‚åœ°éœ‡ã€æ´ªæ°´ï¼‰ï¼Œé›†ç¾¤å¯è¿›å…¥æ‰§è¡Œæœæ•‘ã€è¯„ä¼°ä»»åŠ¡ã€‚\n- **å·¥ä¸šå·¡æ£€**ï¼šåœ¨GPSä¿¡å·å¼±çš„å¤§å‹è®¾æ–½ï¼ˆå¦‚çŸ¿å±±ã€æ—åœºã€å·¥å‚ï¼‰å†…éƒ¨ï¼Œå®ç°è‡ªåŠ¨åŒ–ååŒå·¡æ£€ã€‚\n- **å­¦æœ¯è´¡çŒ®**ï¼šä¸ºé€šä¿¡æ‹’æ­¢ç¯å¢ƒä¸‹çš„è‡ªä¸»é›†ç¾¤ç ”ç©¶æä¾›äº†ä¸€ä¸ªä»æ„ŸçŸ¥ã€æ§åˆ¶åˆ°éªŒè¯çš„å®Œæ•´æŠ€æœ¯èŒƒå¼ï¼Œè¯æ˜äº†DRLåœ¨è§£å†³å¤æ‚ã€ç°å®é›†ç¾¤ååŒé—®é¢˜ä¸Šçš„å·¨å¤§æ½œåŠ›ã€‚",
            "timestamp": "2026-01-27T15:54:32.850283",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## æ— äººæœºé›†ç¾¤é€šä¿¡æ— æ„ŸçŸ¥é›†ä½“å¯¼èˆªæŠ€æœ¯å‘å±•è„‰ç»œåˆ†æ\n\næœ¬æ–‡æå‡ºçš„â€œåŸºäºLiDARä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„é€šä¿¡æ— æ„ŸçŸ¥æ— äººæœºé›†ç¾¤é›†ä½“å¯¼èˆªâ€ç³»ç»Ÿï¼Œæ˜¯**é€šä¿¡æ— æ„ŸçŸ¥ã€ä»…ä¾èµ–å±€éƒ¨æ„ŸçŸ¥çš„é›†ç¾¤å¯¼èˆª**è¿™ä¸€ç‰¹å®šæ–¹å‘ä¸Šçš„æœ€æ–°è¿›å±•ã€‚å…¶å‘å±•è„‰ç»œå¯ä»¥æ¸…æ™°åœ°æ¢³ç†ä¸ºå‡ ä¸ªå…³é”®é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½é’ˆå¯¹å‰ä½œçš„å±€é™æ€§è¿›è¡Œäº†é‡è¦æ”¹è¿›ã€‚æ•´ä½“æ¥çœ‹ï¼Œè¯¥æ–¹å‘æ²¿ç€**æ„ŸçŸ¥æ–¹å¼**å’Œ**æ§åˆ¶ç­–ç•¥**ä¸¤æ¡ä¸»çº¿æ¼”è¿›ã€‚\n\n### å‘å±•è„‰ç»œæ€»è§ˆ\n\n```mermaid\ngraph TD\n    A[é€šä¿¡ä¾èµ–é˜¶æ®µ] --> B{é€šä¿¡æ— æ„ŸçŸ¥é˜¶æ®µå¼€å¯};\n    B --> C[æ„ŸçŸ¥æ–¹å¼æ¼”è¿›];\n    B --> D[æ§åˆ¶ç­–ç•¥æ¼”è¿›];\n    \n    C --> C1[è§†è§‰æ„ŸçŸ¥ä¸»å¯¼];\n    C1 --> C2[LiDARæ„ŸçŸ¥å…´èµ·];\n    C2 --> C3[æœ¬æ–‡å·¥ä½œ: <br>é²æ£’LiDARæ„ŸçŸ¥+åå°„å¸¦];\n    \n    D --> D1[å¯å‘å¼è§„åˆ™];\n    D1 --> D2[æ¨¡ä»¿å­¦ä¹ ];\n    D2 --> D3[æ·±åº¦å¼ºåŒ–å­¦ä¹ ];\n    D3 --> D4[æœ¬æ–‡å·¥ä½œ: <br>å¤šç›®æ ‡DRL+çœŸå®ä¸–ç•ŒéªŒè¯];\n    \n    C3 & D4 --> E[æœ¬æ–‡æ ¸å¿ƒè´¡çŒ®: <br>æ„ŸçŸ¥ä¸æ§åˆ¶èåˆçš„ç«¯åˆ°ç«¯ç³»ç»Ÿ];\n```\n\n---\n\n### é˜¶æ®µä¸€ï¼šé€šä¿¡ä¾èµ–é˜¶æ®µ (åŸºçº¿)\n\nåœ¨é€šä¿¡æ— æ„ŸçŸ¥æ€è·¯å‡ºç°ä¹‹å‰ï¼Œé›†ç¾¤å¯¼èˆªä¸¥é‡ä¾èµ–é€šä¿¡ã€‚\n\n- **ä»£è¡¨æ€§å·¥ä½œ**:\n    - **é›†ä¸­å¼æ–¹æ³• (å¦‚ Crazyswarm [11])**: ä¾èµ–å¤–éƒ¨è¿åŠ¨æ•æ‰ç³»ç»Ÿè¿›è¡Œå…¨å±€å®šä½å’Œåè°ƒã€‚**æ”¹è¿›ç‚¹**ï¼šå®ç°äº†é«˜ç²¾åº¦ç¼–é˜Ÿã€‚**å±€é™æ€§**ï¼šå®Œå…¨å—é™äºå—æ§çš„å®¤å†…ç¯å¢ƒï¼ŒåŸºç¡€è®¾æ–½å¤±æ•ˆåˆ™ç³»ç»Ÿå´©æºƒã€‚\n    - **åˆ†å¸ƒå¼æ–¹æ³• (å¦‚ EGO-Swarm [14], MADER [13])**: é€šè¿‡æœºé—´é€šä¿¡å…±äº«çŠ¶æ€æˆ–è½¨è¿¹ï¼Œè¿›è¡Œåˆ†æ•£å¼å†³ç­–ã€‚**æ”¹è¿›ç‚¹**ï¼šæ‘†è„±äº†é›†ä¸­å¼åŸºç¡€è®¾æ–½ï¼Œèƒ½åœ¨æœªçŸ¥ç¯å¢ƒä¸­è¿è¡Œã€‚**å±€é™æ€§**ï¼š**ä»ç„¶éœ€è¦æŒç»­ã€å¯é çš„æœºé—´é€šä¿¡**ã€‚æ­¤å¤–ï¼Œé€šå¸¸å‡è®¾æ‰€æœ‰æ™ºèƒ½ä½“éƒ½çŸ¥é“å…¨å±€ä»»åŠ¡ï¼Œæˆ–èƒ½æ˜ç¡®è¯†åˆ«å¹¶é€šä¿¡è·Ÿéšé¢†å¯¼è€…ã€‚\n\n**æ ¸å¿ƒé—®é¢˜**ï¼šåœ¨é€šä¿¡è¢«æ‹’æ­¢ï¼ˆå¹²æ‰°ã€é˜»å¡ï¼‰æˆ–ä¸å¯é çš„åœºæ™¯ä¸‹ï¼Œä¸Šè¿°æ–¹æ³•å‡å¤±æ•ˆã€‚\n\n---\n\n### é˜¶æ®µäºŒï¼šé€šä¿¡æ— æ„ŸçŸ¥é˜¶æ®µçš„å¼€å¯ä¸æ¼”è¿›\n\nä¸ºè§£å†³é€šä¿¡ä¾èµ–é—®é¢˜ï¼Œç ”ç©¶è€…è½¬å‘ç”Ÿç‰©å¯å‘çš„â€œéšå¼é¢†å¯¼-è·Ÿéšâ€æ¡†æ¶å’Œçº¯å±€éƒ¨æ„ŸçŸ¥ã€‚å‘å±•æ²¿ç€ä¸¤ä¸ªå­æ–¹å‘æ·±å…¥ï¼š\n\n#### å‘å±•æ–¹å‘ä¸€ï¼šæ„ŸçŸ¥æ–¹å¼çš„æ¼”è¿› â€”â€” ä»è§†è§‰åˆ°LiDAR\n\n**1. è§†è§‰æ„ŸçŸ¥ä¸»å¯¼æœŸ**\n- **ä»£è¡¨å·¥ä½œ**:\n    - **Saska et al. [19]**: ä½¿ç”¨å•ç›®ç›¸æœºå’Œåœ†å½¢å›¾æ¡ˆåœ¨æ— GPSç¯å¢ƒä¸‹è¿›è¡Œç›¸å¯¹å®šä½ã€‚**æ”¹è¿›**ï¼šé¦–æ¬¡å®ç°äº†æ— é€šä¿¡ã€æ— GPSçš„ç›¸å¯¹æ„ŸçŸ¥ã€‚\n    - **Schilling et al. [20, 21]**: ä½¿ç”¨å¤šä¸ªå…¨æ™¯ç›¸æœºï¼Œæ— éœ€æ ‡è®°å³å¯å®ç°å®¤å¤–ç¼–é˜Ÿã€‚**æ”¹è¿›**ï¼šæ‘†è„±äº†äººå·¥æ ‡è®°ï¼Œæé«˜äº†è‡ªç„¶åœºæ™¯é€‚åº”æ€§ã€‚\n    - **VPFæ–¹æ³• [23]**: æ¨¡ä»¿é¸Ÿç±»è§†è§‰æŠ•å½±åœºï¼Œé€šè¿‡æ„ŸçŸ¥ç‰©ä½“å¤§å°åˆ¤æ–­å¸å¼•/æ’æ–¥åŒºåŸŸã€‚**æ”¹è¿›**ï¼šæä¾›äº†æ›´ç”Ÿç‰©å¯å‘çš„æ„ŸçŸ¥-æ§åˆ¶æ¥å£ã€‚\n- **å±€é™æ€§**:\n    - **å…‰ç…§æ•æ„Ÿ**ï¼šåœ¨å¼ºå…‰ã€é˜´å½±æˆ–é»‘æš—ä¸‹æ€§èƒ½ä¸‹é™ã€‚\n    - **å°ºåº¦æ¨¡ç³Š**ï¼šå•ç›®è§†è§‰éœ€é¢å¤–è¿åŠ¨æˆ–å¤„ç†æ¥ä¼°è®¡æ·±åº¦ã€‚\n    - **ç³»ç»Ÿå¤æ‚**ï¼šå…¨æ™¯æ„ŸçŸ¥éœ€å¤šä¸ªç›¸æœºï¼Œå¸¦æ¥å·¨å¤§çš„è®¡ç®—è´Ÿè½½ã€æ ¡å‡†å’ŒåŒæ­¥éš¾é¢˜ã€‚\n\n**2. LiDARæ„ŸçŸ¥å…´èµ·æœŸ**\n- **ä»£è¡¨å·¥ä½œ**: **Swarm-LIO2 [24]**ã€‚\n- **æ”¹è¿›ç‚¹**:\n    - **å…‰ç…§ä¸å˜æ€§**ï¼šLiDARæ€§èƒ½ä¸å—å…‰ç…§æ¡ä»¶å½±å“ã€‚\n    - **ç›´æ¥æ·±åº¦æµ‹é‡**ï¼šè§£å†³äº†å•ç›®è§†è§‰çš„å°ºåº¦æ¨¡ç³Šé—®é¢˜ã€‚\n    - **å•ä¼ æ„Ÿå™¨å…¨æ™¯**ï¼šä¸€ä¸ª360Â°æ—‹è½¬LiDARå³å¯å®ç°å…¨æ™¯æ„ŸçŸ¥ï¼Œæå¤§é™ä½äº†ç³»ç»Ÿå¤æ‚åº¦å’Œè®¡ç®—è´Ÿæ‹…ã€‚\n    - **åå°„å¸¦æ ‡è®°**ï¼šé€šè¿‡åœ¨é«˜åå°„å¸¦ä¸Šç²˜è´´åå°„å¸¦ï¼Œåœ¨LiDARåå°„å¼ºåº¦æµ‹é‡ä¸­äº§ç”Ÿç‹¬ç‰¹çš„å¼ºå›æ³¢ï¼Œå®ç°äº†å¯é çš„é‚»å±…æ£€æµ‹ã€‚\n- **å±€é™æ€§**ï¼šSwarm-LIO2ä¸»è¦è´¡çŒ®åœ¨äº**å»ä¸­å¿ƒåŒ–çš„çŠ¶æ€ä¼°è®¡å’Œé‚»å±…æ£€æµ‹**ï¼Œå…¶**æ§åˆ¶éƒ¨åˆ†ä»ä¾èµ–äºä¼ ç»Ÿçš„åŠ¿åœºæˆ–è§„åˆ™æ–¹æ³•**ï¼Œåœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜æˆ–è¡¨ç°åƒµåŒ–ã€‚\n\n**3. æœ¬æ–‡çš„æ„ŸçŸ¥æ”¹è¿›**\n- **ç»§æ‰¿ä¸æ·±åŒ–**ï¼šå®Œå…¨ç»§æ‰¿äº†Swarm-LIO2çš„LiDAR+åå°„å¸¦æ„ŸçŸ¥æ€è·¯ã€‚\n- **ç³»ç»ŸåŒ–ä¸é²æ£’æ€§å¢å¼º**ï¼šæå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„ã€æ¨¡å—åŒ–çš„æœºè½½æ„ŸçŸ¥æµæ°´çº¿ï¼ˆå›¾3ï¼‰ï¼ŒåŒ…æ‹¬ï¼š\n    - **ç‚¹äº‘è¿‡æ»¤**ï¼šåŸºäºè·ç¦»å’Œåå°„å¼ºåº¦ï¼ˆ`I_high`é˜ˆå€¼ï¼‰ç­›é€‰ã€‚\n    - **èšç±»ä¸è·Ÿè¸ª**ï¼šä½¿ç”¨**DBSCAN**è¿›è¡Œè‡ªé€‚åº”å¯†åº¦èšç±»ï¼Œç»“åˆ**æ‰©å±•å¡å°”æ›¼æ»¤æ³¢**è¿›è¡Œå¹³æ»‘è·Ÿè¸ªã€‚\n    - **é›†ç¾¤éªŒè¯**ï¼šåŸºäºé«˜åå°„ç‚¹æ¯”ä¾‹å’ŒæŒç»­æ—¶é—´çš„åŒé‡éªŒè¯ï¼Œç¡®ä¿æ£€æµ‹å¯é æ€§ã€‚\n- **å®é™…ä»·å€¼**ï¼šè¯¥æ„ŸçŸ¥ç³»ç»Ÿåœ¨çœŸå®å®éªŒä¸­è¾¾åˆ°äº†**100%çš„æ£€æµ‹ç‡**å’Œ**99.2%çš„ç²¾ç¡®ç‡**ï¼Œä¸ºåç»­DRLæ§åˆ¶ç­–ç•¥æä¾›äº†**ç¨³å®šã€å¯é çš„å±€éƒ¨è§‚æµ‹è¾“å…¥**ï¼Œæ˜¯ä»¿çœŸåˆ°ç°å®è¿ç§»æˆåŠŸçš„å…³é”®åŸºç¡€ã€‚\n\n#### å‘å±•æ–¹å‘äºŒï¼šæ§åˆ¶ç­–ç•¥çš„æ¼”è¿› â€”â€” ä»è§„åˆ™åˆ°å­¦ä¹ \n\n**1. å¯å‘å¼è§„åˆ™æ§åˆ¶**\n- **ä»£è¡¨ç†è®º**ï¼šReynoldsä¸‰åŸåˆ™ [4] (åˆ†ç¦»ã€èšåˆã€å¯¹é½)ï¼ŒOlfati-Saberçš„åŠ¿åœºæ³• [6]ã€‚\n- **æ”¹è¿›ç‚¹**ï¼šæä¾›äº†æ— éœ€é€šä¿¡çš„åˆ†å¸ƒå¼åè°ƒç†è®ºåŸºç¡€ã€‚\n- **å±€é™æ€§**ï¼š\n    - **æ³›åŒ–èƒ½åŠ›å·®**ï¼šæ‰‹åŠ¨è®¾è®¡çš„è§„åˆ™éš¾ä»¥é€‚åº”å¤šå˜çš„ç¯å¢ƒï¼ˆéšœç¢ç‰©å¯†åº¦ã€é›†ç¾¤è§„æ¨¡ï¼‰ã€‚\n    - **æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜**ï¼šå¦‚åŠ¿åœºæ³•ä¸­çš„â€œæ­»é”â€é—®é¢˜ã€‚\n    - **è¡Œä¸ºåƒµåŒ–**ï¼šæ— æ³•è‡ªé€‚åº”è°ƒæ•´ä»¥åº”å¯¹åŠ¨æ€çš„é›†ç¾¤æ„å‹ã€‚\n\n**2. æ¨¡ä»¿å­¦ä¹ **\n- **ä»£è¡¨å·¥ä½œ**: **Schilling et al. [20], DAgger [25]**ã€‚\n- **æ”¹è¿›ç‚¹**ï¼šé€šè¿‡æ¨¡ä»¿ä¸“å®¶æ¼”ç¤ºï¼ˆé€šå¸¸æ˜¯è§„åˆ™ç®—æ³•çš„è¾“å‡ºï¼‰ï¼Œ**éƒ¨åˆ†ç¼“è§£äº†è§„åˆ™è®¾è®¡çš„åƒµåŒ–é—®é¢˜**ï¼Œèƒ½ä»æ•°æ®ä¸­å­¦ä¹ æ›´å¤æ‚çš„æ˜ å°„å…³ç³»ã€‚\n- **å±€é™æ€§**ï¼š\n    - **å—é™äºä¸“å®¶æ•°æ®**ï¼šä¸“å®¶æ¼”ç¤ºçš„è´¨é‡å’Œè¦†ç›–èŒƒå›´å†³å®šäº†ç­–ç•¥çš„ä¸Šé™ã€‚\n    - **è·å–æ•°æ®å›°éš¾**ï¼šåœ¨é€šä¿¡æ— æ„ŸçŸ¥ã€å±€éƒ¨å¯è§‚çš„å¤æ‚åœºæ™¯ä¸‹ï¼Œè·å–é«˜è´¨é‡çš„ä¸“å®¶æ¼”ç¤ºæœ¬èº«æå…·æŒ‘æˆ˜ã€‚\n\n**3. æ·±åº¦å¼ºåŒ–å­¦ä¹ **\n- **æ—©æœŸæ¢ç´¢**: **Huang et al. [26], Bai et al. [27]**ã€‚\n- **æ”¹è¿›ç‚¹**ï¼šDRLé€šè¿‡ä¸ç¯å¢ƒç›´æ¥äº¤äº’è¿›è¡Œä¼˜åŒ–ï¼Œ**æ— éœ€æ‰‹åŠ¨è®¾è®¡è§„åˆ™æˆ–ä¾èµ–ä¸“å®¶æ•°æ®**ï¼Œèƒ½è‡ªåŠ¨å­¦ä¹ å¹³è¡¡ç«äº‰æ€§ç›®æ ‡ï¼ˆèšåˆã€é¿éšœï¼‰çš„ç­–ç•¥ã€‚\n- **å±€é™æ€§**ï¼š\n    - **éªŒè¯åœºæ™¯ç®€å•**ï¼šå¤§å¤šåœ¨**æ— éšœç¢æˆ–å·²çŸ¥éšœç¢çš„ä»¿çœŸ**ä¸­éªŒè¯ï¼Œä¸ç°å®å·®è·å¤§ã€‚\n    - **æ„ŸçŸ¥å»ºæ¨¡ç®€åŒ–**ï¼šæœªå……åˆ†è€ƒè™‘çœŸå®ä¼ æ„Ÿå™¨çš„å±€é™ï¼ˆå¦‚è§†åœºè§’é™åˆ¶ã€é®æŒ¡ã€å™ªå£°ï¼‰ã€‚\n    - **ç¼ºä¹çœŸå®ä¸–ç•ŒéªŒè¯**ï¼šä»¿çœŸåˆ°ç°å®çš„é¸¿æ²Ÿæœªè¢«è·¨è¶Šã€‚\n\n**4. æœ¬æ–‡çš„æ§åˆ¶ç­–ç•¥æ”¹è¿›**\n- **åŸºäºDRLçš„å…¨é¢æ·±åŒ–**ï¼š\n    - **ç²¾å¿ƒè®¾è®¡çš„è§‚å¯Ÿä¸åŠ¨ä½œç©ºé—´**ï¼šè§‚å¯ŸåŒ…å«è‡ªæˆ‘çŠ¶æ€ã€æœ€å¤š6ä¸ªé‚»å±…çš„ç›¸å¯¹çŠ¶æ€ï¼ˆé‡‡ç”¨**æ‹“æ‰‘é‚»åŸŸ**è€Œéåº¦é‡é‚»åŸŸæ€æƒ³ï¼‰ä»¥åŠä¸€ä¸ªä¸¤é€šé“çš„LiDARå æ®æ …æ ¼ã€‚**å…³é”®è®¾è®¡**ï¼š**æ•…æ„æ’é™¤ç»å¯¹ä½ç½®**ä½œä¸ºè¾“å…¥ï¼Œè¿«ä½¿ç­–ç•¥å­¦ä¹ åŸºäºè‡ªèº«è¿åŠ¨å’Œå‘¨å›´ç¯å¢ƒçš„**è‡ªæˆ‘ä¸­å¿ƒç­–ç•¥**ï¼Œæå¤§å¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ã€‚\n    - **å¤šç›®æ ‡å¥–åŠ±å‡½æ•°**ï¼šè®¾è®¡äº†åŒ…å«**é›†ç¾¤å¥–åŠ±**ï¼ˆåˆ†ç¦»ä¸èšåˆï¼‰ã€**éšœç¢ç‰©é¿è®©å¥–åŠ±**ã€**ç¨³å®šé£è¡Œå¥–åŠ±**ã€**é‚»å±…æ„ŸçŸ¥å¥–åŠ±**å’Œ**ç¢°æ’æƒ©ç½š**çš„å¤åˆå¥–åŠ±å‡½æ•°ï¼ˆå›¾5ï¼‰ã€‚**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥äº†**é‚»å±…æ„ŸçŸ¥å¥–åŠ±**ï¼Œé¼“åŠ±æ™ºèƒ½ä½“å°†é‚»å±…ä¿æŒåœ¨æœ‰é™çš„LiDARè§†åœºè§’å†…ï¼Œå¹¶è®¾è®¡äº†åœ¨ä¸¢å¤±æ‰€æœ‰é‚»å±…æ—¶çš„**ç´§æ€¥ä¸‹é™æ¢å¤æœºåˆ¶**ï¼Œç›´æ¥é’ˆå¯¹çœŸå®ä¼ æ„Ÿå™¨çš„å±€é™æ€§è¿›è¡Œä¼˜åŒ–ã€‚\n    - **å¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒ**ï¼šåœ¨Nvidia Isaac Simä¸­ä½¿ç”¨512ä¸ªå¹¶è¡Œç¯å¢ƒè¿›è¡Œ5äº¿æ­¥è®­ç»ƒï¼Œåˆ©ç”¨GPUåŠ é€Ÿï¼Œä½¿ç­–ç•¥èƒ½å……åˆ†æ¢ç´¢å¤æ‚çŠ¶æ€ç©ºé—´ã€‚\n- **æ‘’å¼ƒé€Ÿåº¦å¯¹é½é¡¹**ï¼šä¸ºäº†é€‚åº”éšå¼é¢†å¯¼-è·Ÿéšåœºæ™¯ï¼ˆé¢†å¯¼è€…å¯èƒ½ä»»æ„æœºåŠ¨ï¼Œè·Ÿéšè€…æ— æ³•è¯†åˆ«é¢†å¯¼è€…ï¼‰ï¼Œ**ä¸»åŠ¨æ”¾å¼ƒäº†ä¼ ç»Ÿçš„é€Ÿåº¦å¯¹é½è§„åˆ™**ã€‚è¿™ä½¿å¾—è·Ÿéšè€…èƒ½æ›´çµæ´»åœ°å“åº”é¢†å¯¼è€…çš„ä½ç½®å˜åŒ–ï¼Œè€Œä¸æ˜¯è¢«é‚»å±…çš„å¹³å‡é€Ÿåº¦æ–¹å‘æŸç¼šã€‚\n- **é¦–æ¬¡å®ç°çœŸå®ä¸–ç•ŒéªŒè¯**ï¼šæœ¬æ–‡æœ€å¤§çš„çªç ´åœ¨äºå°†è®­ç»ƒå¥½çš„å•ä¸€ç­–ç•¥**é›¶æ ·æœ¬è¿ç§»**åˆ°çœŸå®äº”æ¶æ— äººæœºä¸Šï¼Œåœ¨å¤šæ ·åŒ–çš„å®¤å†…å¤–å¤æ‚ç¯å¢ƒä¸­æˆåŠŸå®Œæˆäº†é›†ä½“å¯¼èˆªä»»åŠ¡ï¼Œè¯æ˜äº†å…¶**å“è¶Šçš„ä»¿çœŸåˆ°ç°å®è¿ç§»èƒ½åŠ›å’Œé²æ£’æ€§**ã€‚\n\n---\n\n### æ€»ç»“ï¼šæœ¬æ–‡å·¥ä½œçš„å®šä½ä¸æ ¸å¿ƒè´¡çŒ®\n\næœ¬æ–‡ç«™åœ¨äº†å‰è¿°ä¸¤ä¸ªå‘å±•æ–¹å‘çš„äº¤æ±‡ç‚¹ä¸Šï¼Œå¹¶å®ç°äº†å…³é”®æ€§çªç ´ï¼š\n\n1.  **æ„ŸçŸ¥å±‚é¢**ï¼šé‡‡çº³å¹¶å¼ºåŒ–äº†**LiDAR+åå°„å¸¦**è¿™ä¸€æœ€é²æ£’ã€æœ€å®ç”¨çš„é€šä¿¡æ— æ„ŸçŸ¥æ–¹æ¡ˆï¼Œæ„å»ºäº†å·¥ç¨‹ä¸Šå¯é çš„æ„ŸçŸ¥æµæ°´çº¿ã€‚\n2.  **æ§åˆ¶å±‚é¢**ï¼šåˆ©ç”¨**æ·±åº¦å¼ºåŒ–å­¦ä¹ **ï¼Œè®¾è®¡å‡ºèƒ½è‡ªåŠ¨å¹³è¡¡å¤šç›®æ ‡ã€å¹¶ä¸“é—¨é’ˆå¯¹çœŸå®æ„ŸçŸ¥çº¦æŸï¼ˆé®æŒ¡ã€æœ‰é™FOVï¼‰è¿›è¡Œä¼˜åŒ–çš„é«˜çº§ç­–ç•¥ã€‚\n3.  **ç³»ç»Ÿæ•´åˆä¸éªŒè¯**ï¼šé¦–æ¬¡å°†é²æ£’çš„LiDARæ„ŸçŸ¥ç³»ç»Ÿä¸DRLæ§åˆ¶ç­–ç•¥æ·±åº¦èåˆï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„ã€ç«¯åˆ°ç«¯çš„é€šä¿¡æ— ",
            "timestamp": "2026-01-27T15:55:46.614729",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æä¸å¯¹æ¯”\n\næ ¹æ®æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œç‰¹åˆ«æ˜¯â€œI-B 2 Perception Strategies for Communication-Free Swarm Navigationâ€å’Œâ€œI-C Contributionsâ€éƒ¨åˆ†ï¼Œæœ¬æ–‡æ˜ç¡®å¼•ç”¨äº†å…¶ç›´æ¥å‰ä½œ **Swarm-LIO2 [24]**ã€‚è¯¥è®ºæ–‡æ˜¯æ„å»ºæœ¬ç ”ç©¶æ„ŸçŸ¥ç³»ç»Ÿçš„åŸºç¡€ã€‚ä»¥ä¸‹è¡¨æ ¼è¯¦ç»†åˆ†æäº†è¿™ç¯‡å‰ä½œï¼Œå¹¶é˜è¿°äº†æœ¬æ–‡ç›¸å¯¹äºå®ƒçš„ä¸»è¦æ”¹è¿›ä¸åŒºåˆ«ã€‚\n\n### å‰ä½œè¯¦è¿°\n\n| é¡¹ç›® | å†…å®¹ |\n| :--- | :--- |\n| **è®ºæ–‡æ ‡é¢˜** | Swarm-LIO2: Decentralized, efficient LiDAR-inertial odometry for UAV swarms |\n| **æ ¸å¿ƒè´¡çŒ®** | æå‡ºäº†ä¸€ç§**å®Œå…¨å»ä¸­å¿ƒåŒ–çš„æ¿€å…‰é›·è¾¾-æƒ¯æ€§é‡Œç¨‹è®¡ï¼ˆLIOï¼‰ç³»ç»Ÿ**ï¼Œç”¨äºæ— äººæœºé›†ç¾¤çš„çŠ¶æ€ä¼°è®¡ä¸é‚»å±…æ£€æµ‹ã€‚ |\n| **å…³é”®æŠ€æœ¯** | 1. **å»ä¸­å¿ƒåŒ–LIO**ï¼šæ¯ä¸ªæ— äººæœºç‹¬ç«‹è¿è¡ŒFAST-LIO2ç®—æ³•è¿›è¡Œè‡ªèº«ä½å§¿ä¼°è®¡ï¼Œä¸ä¾èµ–å¤–éƒ¨å®šä½ï¼ˆå¦‚GPSã€åŠ¨æ•ï¼‰ã€‚<br>2. **åŸºäºåå°„å¸¦çš„é‚»å±…æ£€æµ‹**ï¼šåœ¨æ— äººæœºæœºèº«ä¸Šç²˜è´´**é«˜åå°„ç‡èƒ¶å¸¦**ï¼Œä½¿å…¶åœ¨æ¿€å…‰é›·è¾¾çš„åå°„å¼ºåº¦ï¼ˆintensityï¼‰é€šé“ä¸­äº§ç”Ÿæ˜¾è‘—åŒºåˆ«äºç¯å¢ƒç‰©ä½“çš„é«˜äº®å›æ³¢ï¼Œä»è€Œå®ç°æ— æ ‡è®°ã€æ— é€šä¿¡çš„é‚»å±…è¯†åˆ«ã€‚<br>3. **é›†ç¾¤çŠ¶æ€ä¼°è®¡**ï¼šé€šè¿‡æ£€æµ‹åˆ°çš„é‚»å±…ç‚¹äº‘ï¼Œå¯ä»¥ä¼°è®¡ç›¸å¯¹ä½ç½®ï¼Œå®ç°äº†é›†ç¾¤åœ¨æ— å¤–éƒ¨åŸºç¡€è®¾æ–½ç¯å¢ƒä¸‹çš„ååŒå®šä½ã€‚ |\n| **éªŒè¯æƒ…å†µ** | åœ¨çœŸå®ä¸–ç•Œå®éªŒä¸­ï¼Œä½¿ç”¨**5æ¶æ— äººæœº**è¿›è¡Œäº†éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„å¯è¡Œæ€§ã€‚ |\n| **å±€é™æ€§** | 1. **ä¾§é‡äºæ„ŸçŸ¥ä¸çŠ¶æ€ä¼°è®¡**ï¼šSwarm-LIO2ä¸»è¦è§£å†³â€œå¦‚ä½•çœ‹åˆ°å¹¶è·Ÿè¸ªé‚»å±…â€çš„é—®é¢˜ï¼Œå…¶**æ§åˆ¶ç­–ç•¥å¹¶éæ ¸å¿ƒ**ã€‚æ–‡ä¸­å¯èƒ½ä½¿ç”¨äº†ç®€å•çš„è§„åˆ™ï¼ˆå¦‚äººå·¥åŠ¿åœºï¼‰è¿›è¡Œé¿éšœå’ŒåŸºç¡€ç¼–é˜Ÿã€‚<br>2. **æ§åˆ¶ç­–ç•¥ç®€å•**ï¼šåœ¨å¤æ‚ã€å¯†é›†éšœç¢ç‰©ç¯å¢ƒä¸­ï¼ŒåŸºäºå›ºå®šè§„åˆ™çš„æ§åˆ¶æ–¹æ³•å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œç¼ºä¹é€‚åº”æ€§ï¼Œéš¾ä»¥åœ¨é¿éšœã€ä¿æŒç¼–é˜Ÿå’Œè·Ÿéšé¢†å¯¼è€…ä¹‹é—´å–å¾—åŠ¨æ€å¹³è¡¡ã€‚ |\n\n### æœ¬æ–‡ä¸å‰ä½œï¼ˆSwarm-LIO2ï¼‰çš„ä¸»è¦åŒºåˆ«ä¸æ”¹è¿›\n\næœ¬æ–‡çš„å·¥ä½œæ˜¯**åœ¨å‰ä½œæä¾›çš„å¼ºå¤§æ„ŸçŸ¥åŸºç¡€ï¼ˆSwarm-LIO2ï¼‰ä¸Šï¼Œè§£å†³æ›´ä¸Šå±‚çš„â€œæ™ºèƒ½å†³ç­–ä¸æ§åˆ¶â€é—®é¢˜**ã€‚ä¸¤è€…çš„å…³ç³»æ˜¯é€’è¿›å’Œäº’è¡¥çš„ã€‚\n\n| å¯¹æ¯”ç»´åº¦ | å‰ä½œ (Swarm-LIO2 [24]) | æœ¬æ–‡å·¥ä½œ | **ä¸»è¦æ”¹å–„ä¸å…·ä½“æ”¹åŠ¨** |\n| :--- | :--- | :--- | :--- |\n| **æ ¸å¿ƒé—®é¢˜** | **æ„ŸçŸ¥ä¸çŠ¶æ€ä¼°è®¡**ï¼šå¦‚ä½•åœ¨æ— é€šä¿¡ã€æ— å¤–éƒ¨å®šä½ç¯å¢ƒä¸‹ï¼Œå®ç°é›†ç¾¤ä¸­ä¸ªä½“çš„è‡ªæˆ‘å®šä½å’Œé‚»å±…æ£€æµ‹ã€‚ | **å†³ç­–ä¸æ§åˆ¶**ï¼šåœ¨è·å¾—ä¸Šè¿°æ„ŸçŸ¥èƒ½åŠ›åï¼Œå¦‚ä½•å®ç°å¤æ‚ç¯å¢ƒä¸‹çš„**é›†ä½“å¯¼èˆª**ï¼Œç‰¹åˆ«æ˜¯**è·Ÿéšä¸€ä¸ªä¸çŸ¥æƒ…çš„é¢†å¯¼è€…**ã€‚ | **ä»â€œæ„ŸçŸ¥â€å±‚æå‡åˆ°â€œå†³ç­–ä¸æ§åˆ¶â€å±‚**ã€‚æœ¬æ–‡åˆ©ç”¨å‰ä½œçš„æ„ŸçŸ¥è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œä¸“æ³¨äºå­¦ä¹ é«˜çº§è¡Œä¸ºç­–ç•¥ã€‚ |\n| **æ§åˆ¶ç­–ç•¥** | æœªæ˜ç¡®å¼ºè°ƒæˆ–ä½¿ç”¨**ä¼ ç»Ÿè§„åˆ™å¼æ–¹æ³•**ï¼ˆå¦‚äººå·¥åŠ¿åœºã€é›·è¯ºè§„åˆ™ï¼‰ã€‚ | **åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„æ§åˆ¶å™¨**ï¼Œä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•è®­ç»ƒã€‚ | **1. ç­–ç•¥ç”Ÿæˆæ–¹å¼**ï¼šä»**äººå·¥è®¾è®¡è§„åˆ™**æ”¹ä¸º**æ•°æ®é©±åŠ¨å­¦ä¹ **ã€‚DRLèƒ½è‡ªåŠ¨å­¦ä¹ å¹³è¡¡å¤šç›®æ ‡ï¼ˆèšé›†ã€åˆ†ç¦»ã€é¿éšœï¼‰çš„å¤æ‚ç­–ç•¥ï¼Œé€‚åº”åŠ¨æ€å’ŒæœªçŸ¥ç¯å¢ƒã€‚<br>**2. åº”å¯¹æ„ŸçŸ¥æŒ‘æˆ˜**ï¼šç­–ç•¥è®¾è®¡æ—¶ä¸“é—¨è€ƒè™‘äº†å‰ä½œæ„ŸçŸ¥ç³»ç»Ÿçš„å±€é™æ€§ï¼ˆå¦‚é®æŒ¡ã€æœ‰é™è§†åœºï¼‰ï¼Œå¹¶é€šè¿‡å¥–åŠ±å‡½æ•°ï¼ˆå¦‚`r_perception`ï¼‰é¼“åŠ±æ™ºèƒ½ä½“ä¿æŒé‚»å±…åœ¨è§†é‡å†…ã€‚ |\n| **ç³»ç»Ÿæ¶æ„** | éšå¼é¢†å¯¼è€…-è·Ÿéšæ¡†æ¶**ä¸æ˜¯é‡ç‚¹**ï¼Œå¯èƒ½æ‰€æœ‰ä¸ªä½“è§’è‰²ç›¸åŒæˆ–éœ€æ˜ç¡®é€šä¿¡æ¥åˆ†é…è§’è‰²ã€‚ | æ˜ç¡®æå‡ºäº†**éšå¼é¢†å¯¼è€…-è·Ÿéšæ¡†æ¶**ä½œä¸ºæ ¸å¿ƒæ¶æ„ã€‚åªæœ‰é¢†å¯¼è€…çŸ¥é“ç›®æ ‡ï¼Œè·Ÿéšè€…ä»…é€šè¿‡å±€éƒ¨æ„ŸçŸ¥å­¦ä¹ è·Ÿéšè¡Œä¸ºã€‚ | **1. ä¿¡æ¯ä¸å¯¹ç§°**ï¼šå¼•å…¥äº†**è§’è‰²å·®å¼‚**ï¼Œæ›´ç¬¦åˆå®é™…å†›äº‹æˆ–å—é™åœºæ™¯ï¼ˆä»…èƒ½æ›´æ–°éƒ¨åˆ†ä¸ªä½“ä»»åŠ¡ï¼‰ã€‚<br>**2. æ— éœ€é¢†å¯¼è€…è¯†åˆ«**ï¼šè·Ÿéšè€…ç­–ç•¥**ä¸ä¾èµ–è¯†åˆ«å“ªä¸ªé‚»å±…æ˜¯é¢†å¯¼è€…**ï¼Œè€Œæ˜¯é€šè¿‡ç»´æŒä¸é‚»è¿‘é›†ç¾¤çš„å‡èšåŠ›ï¼Œè‡ªç„¶åœ°è·Ÿéšé¢†å¯¼è€…çš„è¿åŠ¨æ–¹å‘ï¼Œè¿™æ˜¯æœ¬æ–‡çš„å…³é”®åˆ›æ–°è¡Œä¸ºæ¶Œç°ã€‚ |\n| **ç¼–é˜Ÿè¡Œä¸ºè®¾è®¡** | å¯èƒ½åŒ…å«é€Ÿåº¦å¯¹é½ï¼ˆalignmentï¼‰ï¼Œä»¥å®ç°åè°ƒè¿åŠ¨ã€‚ | **åˆ»æ„æ’é™¤äº†é€Ÿåº¦å¯¹é½é¡¹**ã€‚ | **å…³é”®è®¾è®¡æ”¹åŠ¨**ï¼šåœ¨æ— é€šä¿¡ä¸”é¢†å¯¼è€…å¯èƒ½è¿›è¡Œå¤æ‚æœºåŠ¨ï¼ˆå¦‚è½¬å‘ã€åé€€ï¼‰çš„æƒ…å†µä¸‹ï¼Œé€Ÿåº¦å¯¹é½ä¼šå¯¼è‡´è·Ÿéšè€…ä¸é‚»å±…å¹³å‡é€Ÿåº¦å¯¹é½ï¼Œä»è€Œ**ç¨€é‡Šé¢†å¯¼è€…çš„å¯¼å‘ä½œç”¨**ï¼Œå¯èƒ½å¯¼è‡´è·Ÿä¸¢ã€‚æœ¬æ–‡åªé‡‡ç”¨**å‡èšï¼ˆcohesionï¼‰å’Œåˆ†ç¦»ï¼ˆseparationï¼‰**ï¼Œä½¿è·Ÿéšè€…å¯¹é¢†å¯¼è€…çš„**ä½ç½®å˜åŒ–**æ›´æ•æ„Ÿã€‚ |\n| **ç­–ç•¥è¾“å…¥è®¾è®¡** | æœªæ¶‰åŠï¼ˆéå…¶ç ”ç©¶é‡ç‚¹ï¼‰ã€‚ | è·Ÿéšè€…ç­–ç•¥çš„è¾“å…¥**æ•…æ„æ’é™¤äº†è‡ªèº«å…¨å±€ä½ç½®** `p_t^f`ï¼Œä»…åŒ…å«é€Ÿåº¦ã€å§¿æ€ã€é‚»å±…å’Œéšœç¢ç‰©ä¿¡æ¯ã€‚ | **æ—¨åœ¨æå‡æ³›åŒ–èƒ½åŠ›**ï¼šé¿å…ç­–ç•¥å­¦ä¹ åˆ°ä¸è®­ç»ƒåœ°å›¾ç‰¹å®šåæ ‡ç›¸å…³çš„è¡Œä¸ºï¼Œä»è€Œç¡®ä¿å­¦ä¹ åˆ°çš„æ˜¯åŸºäº**è‡ªèº«è¿åŠ¨çŠ¶æ€å’Œå‘¨å›´ç›¸å¯¹å…³ç³»**çš„ã€å¯è¿ç§»åˆ°ä»»ä½•æ–°ç¯å¢ƒçš„**ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ç­–ç•¥**ã€‚ |\n| **éªŒè¯æ·±åº¦** | éªŒè¯äº†æ„ŸçŸ¥ç³»ç»Ÿåœ¨çœŸå®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚ | **è¿›è¡Œäº†ä»ä»¿çœŸåˆ°ç°å®çš„å…¨é¢éªŒè¯**ï¼š<br>1. **å¤§è§„æ¨¡ä»¿çœŸå¯¹æ¯”**ï¼šä¸å¤šç§å…ˆè¿›åŸºçº¿ï¼ˆPACNav, VPF, DAggerï¼‰åœ¨å¤šç§å¤æ‚ç¯å¢ƒä¸­å¯¹æ¯”ï¼Œè¯æ˜å…¶ä¼˜è¶Šæ€§ã€‚<br>2. **çœŸå®ä¸–ç•Œå®éªŒ**ï¼šåœ¨**å¤šæ ·åŒ–çš„å®¤å†…å¤–ç¯å¢ƒ**ï¼ˆå¦‚æ‚ä¹±åŒºå—ã€å¢™å£ã€æŸ±å­ã€æ£®æ—ï¼‰ä¸­æˆåŠŸéƒ¨ç½²5æ¶æ— äººæœºé›†ç¾¤ï¼Œå®Œæˆé›†ä½“å¯¼èˆªã€‚ | **1. éªŒè¯èŒƒå›´æ‰©å±•**ï¼šä»å‰ä½œçš„â€œå¯è¡Œæ€§éªŒè¯â€æ‰©å±•åˆ°**æ€§èƒ½ä¼˜è¶Šæ€§éªŒè¯**å’Œ**æç«¯ç¯å¢ƒé²æ£’æ€§éªŒè¯**ã€‚<br>**2. å¡«è¡¥ç ”ç©¶ç©ºç™½**ï¼šè®ºæ–‡æŒ‡å‡ºï¼Œç°æœ‰çš„å­¦ä¹ å‹æ–¹æ³•å¤§å¤šä»…åœ¨ä»¿çœŸä¸­éªŒè¯ï¼Œæœ¬æ–‡çš„**çœŸå®ä¸–ç•Œå®éªŒ**æ˜¯ä¸€é¡¹é‡è¦è´¡çŒ®ï¼Œè¯æ˜äº†è¯¥DRLç­–ç•¥çš„**sim-to-realè½¬ç§»èƒ½åŠ›**ã€‚ |\n| **è´¡çŒ®æ€»ç»“** | æä¾›äº†ä¸€ä¸ª**é€šä¿¡æ— å…³ã€åŸºäºæ¿€å…‰é›·è¾¾çš„é›†ç¾¤æ„ŸçŸ¥è§£å†³æ–¹æ¡ˆ**ã€‚ | æå‡ºäº†ä¸€ä¸ª**å®Œæ•´çš„ã€é€šä¿¡æ— å…³çš„é›†ç¾¤å¯¼èˆªç³»ç»Ÿ**ï¼ŒåŒ…å«ï¼ši) åŸºäºå‰ä½œçš„æ„ŸçŸ¥æ¡†æ¶ï¼›ii) **å®ç°éšå¼é¢†å¯¼è€…è·Ÿéšçš„DRLæ§åˆ¶ç­–ç•¥**ï¼›iii) **å…¨é¢çš„ä»¿çœŸä¸å®ç‰©éªŒè¯**ã€‚ | **ç³»ç»Ÿæ€§æ•´åˆä¸æå‡**ï¼šæœ¬æ–‡å°†å‰ä½œçš„æ„ŸçŸ¥æ¨¡å—ä½œä¸ºå­ç³»ç»Ÿæ•´åˆè¿›æ¥ï¼Œå¹¶ä¸ºå…¶é…ä¸Šäº†â€œæ™ºèƒ½å¤§è„‘â€ï¼ˆDRLæ§åˆ¶å™¨ï¼‰ï¼Œä»è€Œæ„å»ºäº†ä¸€ä¸ªåŠŸèƒ½å®Œæ•´ã€æ€§èƒ½æ›´å¼ºã€æ›´é€‚åº”å¤æ‚ç°å®åœºæ™¯çš„**è‡ªä¸»é›†ç¾¤å¯¼èˆªç³»ç»Ÿ**ã€‚å‰ä½œæ˜¯â€œçœ¼ç›â€ï¼Œæœ¬æ–‡åœ¨æ­¤åŸºç¡€ä¸Šå¢åŠ äº†â€œå°è„‘â€ï¼ˆè¿åŠ¨åè°ƒï¼‰å’Œâ€œå¤§è„‘â€ï¼ˆå†³ç­–è§„åˆ’ï¼‰ã€‚ |\n\n**ç»“è®º**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°å¹¶éæ¨ç¿»å‰ä½œï¼Œè€Œæ˜¯**ç«™åœ¨å‰ä½œçš„è‚©è†€ä¸Šï¼Œè§£å†³äº†ä¸€ä¸ªæ›´å›°éš¾ã€æ›´ä¸Šå±‚çš„é—®é¢˜**ã€‚å‰ä½œ Swarm-LIO2 æä¾›äº†å¯é ã€æ— é€šä¿¡çš„â€œæ„Ÿå®˜â€ï¼Œè€Œæœ¬æ–‡åˆ™åˆ©ç”¨è¿™äº›æ„Ÿå®˜ï¼Œé€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ è®­ç»ƒå‡ºä¸€ä¸ªâ€œæ§åˆ¶å™¨â€ï¼Œä½¿å¾—é›†ç¾¤èƒ½å¤Ÿæ¶Œç°å‡ºå¤æ‚çš„ã€é€‚åº”æ€§çš„é›†ä½“å¯¼èˆªè¡Œä¸ºï¼Œç‰¹åˆ«æ˜¯åœ¨ä»…æœ‰å°‘æ•°çŸ¥æƒ…é¢†å¯¼è€…çš„éšå¼è·Ÿéšåœºæ™¯ä¸‹ã€‚ä¸¤è€…çš„ç»“åˆï¼Œæœ€ç»ˆå®ç°äº†ä¸€ä¸ªä»æ„ŸçŸ¥åˆ°å†³ç­–å®Œå…¨è‡ªä¸»ã€ä¸ä¾èµ–ä»»ä½•é€šä¿¡ä¸å¤–éƒ¨è®¾æ–½çš„æ— äººæœºé›†ç¾¤ç³»ç»Ÿã€‚",
            "timestamp": "2026-01-27T15:56:47.404822",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨è¯¦ç»†è§£æè¿™ç¯‡è®ºæ–‡æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚ä»¥ä¸‹åˆ†æå°†é‡ç‚¹å…³æ³¨å…¶æŠ€æœ¯åˆ›æ–°å’Œå®ç°ç»†èŠ‚ã€‚\n\n## è®ºæ–‡å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\næœ¬æ–‡çš„æ ¸å¿ƒæ˜¯æ„å»ºä¸€ä¸ª**å®Œå…¨æ— éœ€é€šä¿¡**çš„æ— äººæœºé›†ç¾¤å¯¼èˆªç³»ç»Ÿï¼Œå…¶æŠ€æœ¯æ¡†æ¶ä¸»è¦ç”±ä¸¤å¤§æ¨¡å—æ„æˆï¼š**åŸºäºæ¿€å…‰é›·è¾¾çš„æ„ŸçŸ¥ç³»ç»Ÿ**å’Œ**åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ§åˆ¶å™¨**ã€‚ä¸‹è¡¨è¯¦ç»†è¯´æ˜äº†å„é¡¹å…³é”®æŠ€æœ¯åŠå…¶ç»†èŠ‚ã€‚\n\n### å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è¡¨\n\n| æŠ€æœ¯æ¨¡å— | å…·ä½“æ–¹æ³• | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…³é”®å‚æ•° |\n| :--- | :--- | :--- | :--- |\n| **1. æ„ŸçŸ¥ç³»ç»Ÿï¼šåŸºäºæ¿€å…‰é›·è¾¾çš„é‚»å±…æ£€æµ‹ä¸è·Ÿè¸ª** | **ç‚¹äº‘è¿‡æ»¤ä¸å¢å¼º** | 1. **ç‚¹äº‘å †å **ï¼šä¸ºå¢åŠ ç‚¹å¯†åº¦ï¼Œå°†æœ€è¿‘ `B` å¸§ç‚¹äº‘é€šè¿‡LiDAR-æƒ¯æ€§é‡Œç¨‹è®¡ï¼ˆLIOï¼‰ä¼°è®¡çš„ä½å§¿ `T_{mâ†b}` å˜æ¢åˆ°åœ°å›¾åæ ‡ç³»ã€‚<br>2. **è·ç¦»é—¨æ§**ï¼šä¿ç•™åœ¨è·ç¦»èŒƒå›´ `[d_min, d_max]` å†…çš„ç‚¹äº‘ `P_d`ã€‚<br>3. **å¼ºåº¦è¿‡æ»¤**ï¼šåˆ©ç”¨**åå…‰è´´çº¸**äº§ç”Ÿçš„é«˜å¼ºåº¦å›æ³¢ä½œä¸ºæ£€æµ‹ç§å­ï¼š`P_high = {p âˆˆ P_d | I â‰¥ I_high}`ã€‚<br>4. **å…´è¶£åŒºåŸŸ(ROI)è¿‡æ»¤**ï¼šä¸ºåº”å¯¹çŸ­æš‚é®æŒ¡ï¼Œä¿ç•™ä½å¼ºåº¦ç‚¹ä¸­ä½äºç°æœ‰è·Ÿè¸ªç›®æ ‡è´¨å¿ƒ `Ä‰^j_m` åŠå¾„ `r_roi` å†…çš„ç‚¹ï¼š`P_roi = {p âˆˆ P_low | min_j â€–p_m - Ä‰^j_mâ€–_2 â‰¤ r_roi}`ã€‚ | `B = 2`<br>`d_min = 0.05m`<br>`d_max = 10.0m`<br>`I_high = 170`<br>`r_roi = 0.3m` |\n| | **èšç±»ä¸è·Ÿè¸ª** | 1. **DBSCANèšç±»**ï¼šå¯¹è¿‡æ»¤åçš„ç‚¹é›† `P_filtered` ä½¿ç”¨å¯†åº¦èšç±»ç®—æ³•ï¼Œå½¢æˆä¸ªä½“ç°‡ `C^k`ã€‚ç®—æ³•åŸºäºè·ç¦»é˜ˆå€¼ `Îµ` å’Œæœ€å°ç‚¹æ•° `n_min`ï¼ˆéšè·ç¦»è‡ªé€‚åº”è°ƒæ•´ï¼‰ã€‚<br>2. **æ‰©å±•å¡å°”æ›¼æ»¤æ³¢è·Ÿè¸ª**ï¼šå¯¹æ¯ä¸ªç°‡ä½¿ç”¨EKFè¿›è¡Œè·Ÿè¸ªã€‚**çŠ¶æ€æ¨¡å‹**ä¸ºæ’å®šé€Ÿåº¦æ¨¡å‹ï¼Œé¢„æµ‹é‚»å±…ä½ç½®å’Œé€Ÿåº¦ã€‚**è§‚æµ‹æ¨¡å‹**ä¸ºèšç±»åçš„è´¨å¿ƒä½ç½®ã€‚é€šè¿‡**æ•°æ®å…³è”**ï¼ˆæœ€è¿‘é‚»åŒ¹é…ï¼Œé˜ˆå€¼ `d_match`ï¼‰å°†æ–°èšç±»ä¸ç°æœ‰è½¨è¿¹å…³è”ï¼Œæ›´æ–°EKFçŠ¶æ€åæ–¹å·®ã€‚ | `Îµ = 0.1m`<br>`n_min = 8B` (åŸºç¡€å€¼)<br>`d_match = 0.2m`<br>`t_inactive = 0.5s` (è½¨è¿¹å¤±æ´»è¶…æ—¶) |\n| | **èšç±»éªŒè¯** | ä¸ºç¡®ä¿è·Ÿè¸ªç›®æ ‡æ˜¯å¯é çš„é‚»å±…æ— äººæœºï¼ˆè€Œéå™ªå£°æˆ–å…¶ä»–ç‰©ä½“ï¼‰ï¼Œå¯¹è½¨è¿¹è¿›è¡ŒéªŒè¯ï¼šä»…å½“ç°‡å†…**é«˜å¼ºåº¦ç‚¹æ¯”ä¾‹**è¶…è¿‡é˜ˆå€¼ `Ï_high`ï¼Œä¸”æŒç»­æ—¶é•¿è¶…è¿‡ `Ï„_on` æ—¶ï¼Œæ‰ç¡®è®¤ä¸ºæœ‰æ•ˆé‚»å±…ã€‚ | `Ï_high = 0.05`<br>`Ï„_on = 0.01s` |\n| **2. æ§åˆ¶ç­–ç•¥ï¼šåŸºäºDRLçš„éšå¼é¢†å¯¼-è·Ÿéš** | **é—®é¢˜å»ºæ¨¡ (POMDP)** | å°†è·Ÿéšè€…å†³ç­–è¿‡ç¨‹å»ºæ¨¡ä¸º**éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹**ï¼š`(S, A, T, R, Î©, O, Î³)`ã€‚æ™ºèƒ½ä½“ä»…èƒ½é€šè¿‡è§‚æµ‹ `o_t âˆˆ Î©` å­¦ä¹ ç­–ç•¥ `Ï€: Î© â†’ A`ï¼Œæ— éœ€çŸ¥é“çœŸå®çŠ¶æ€è½¬ç§» `T` å’Œè§‚æµ‹å‡½æ•° `O`ã€‚ | `Î³ = 0.99` (æŠ˜æ‰£å› å­) |\n| | **è§‚æµ‹ç©ºé—´è®¾è®¡** | è§‚æµ‹å‘é‡ `o_t` ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š<br>1. **è‡ªçŠ¶æ€**ï¼š7ç»´å‘é‡ï¼ˆ3ç»´é€Ÿåº¦ + 4ç»´å››å…ƒæ•°å§¿æ€ï¼‰ã€‚**åˆ»æ„æ’é™¤å…¨å±€ä½ç½®**ï¼Œä»¥å¢å¼ºç­–ç•¥åœ¨ä¸åŒç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚<br>2. **é‚»å±…çŠ¶æ€**ï¼šå›ºå®šç¼–ç æœ€å¤š6ä¸ªæœ€è¿‘é‚»å±…ã€‚æ¯ä¸ªé‚»å±…ä¸º7ç»´å‘é‡ï¼ˆç›¸å¯¹ä½ç½®ã€ç›¸å¯¹é€Ÿåº¦ + å­˜åœ¨æ€§æ©ç ï¼‰ã€‚ä¸è¶³æ—¶è¡¥é›¶ã€‚<br>3. **éšœç¢ç‰©å æ®æ …æ ¼**ï¼šä¸¤é€šé“çš„ `HÃ—W` æ …æ ¼ã€‚é€šé“1ï¼šåˆ°æœ€è¿‘éšœç¢ç‰©çš„è·ç¦»ï¼›é€šé“2ï¼šäºŒå€¼å æ®æ©ç ã€‚ | `H = 72` (æ°´å¹³åˆ†è¾¨ç‡)<br>`W = 12` (å‚ç›´åˆ†è¾¨ç‡)<br>é‚»å±…æ•° = 6 |\n| | **åŠ¨ä½œç©ºé—´** | è¿ç»­3ç»´å‘é‡ï¼Œç›´æ¥è¾“å‡ºæœŸæœ›é€Ÿåº¦æŒ‡ä»¤ `v âˆˆ â„^3`ã€‚ | é€Ÿåº¦æŒ‡ä»¤èŒƒå›´ç”±å®é™…æ— äººæœºåŠ¨åŠ›å­¦é™åˆ¶ |\n| | **å¤šç›®æ ‡å¥–åŠ±å‡½æ•°** | æ€»å¥–åŠ± `r_total = r_flock + r_obstacle + r_stable + r_perception + r_collision`ã€‚é€šè¿‡åŠ æƒå’Œå¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ å¤æ‚æ¶Œç°è¡Œä¸ºã€‚<br>**å…³é”®è®¾è®¡**ï¼š**æ’é™¤äº†é€Ÿåº¦å¯¹é½é¡¹**ï¼Œä»¥é˜²æ­¢è·Ÿéšè€…ä¸é‚»å±…å¹³å‡é€Ÿåº¦å¯¹é½è€Œç¨€é‡Šé¢†å¯¼è€…è½¬å‘çš„å½±å“ã€‚ | å„æƒé‡è§å³æ  |\n| | **- é›†ç¾¤å¥–åŠ± (r_flock)** | `r_flock = w_flock * (r_separation + r_cohesion)`<br>â€¢ **åˆ†ç¦»é¡¹**ï¼šæƒ©ç½šä¸é‚»å±…è·ç¦»å°äºå®‰å…¨è·ç¦» `d_sep` çš„æƒ…å†µï¼Œæƒ©ç½šéšè·ç¦»å‡å°è€Œå¢å¤§ã€‚<br>`r_separation = -âˆ‘_{jâˆˆN^i} ( (d_sep - â€–p^i - p^jâ€–_2) / (d_sep - 2r_uav) ) * I_{â€–p^i-p^jâ€–_2 < d_sep}`<br>â€¢ **èšåˆé¡¹**ï¼šæƒ©ç½šåç¦»é›†ç¾¤è´¨å¿ƒè·ç¦»è¶…è¿‡ `d_coh` çš„æƒ…å†µã€‚<br>`r_cohesion = -(â€–p^i - p_comâ€–_2 - d_coh) * I_{â€–p^i-p_comâ€–_2 > d_coh}` | `w_flock = 1.5`<br>`d_sep = 1.6m`<br>`d_coh = 2.0m`<br>`r_uav = 0.2m` |\n| | **- é¿éšœå¥–åŠ± (r_obstacle)** | `r_obstacle = w_obstacle * (r_proximity + r_direction)`<br>â€¢ **æ¥è¿‘é¡¹**ï¼šå››æ¬¡æ–¹æƒ©ç½šè·ç¦»æœ€è¿‘éšœç¢ç‰©å°äº `d_prox` çš„æƒ…å†µï¼Œä½¿æƒ©ç½šåœ¨é è¿‘æ—¶æ€¥å‰§ä¸Šå‡ã€‚<br>`r_proximity = -( (d_prox - min_k(d_obs^k)) / (d_prox - r_uav) )^4 * I_{min_k d_obs^k < d_prox}`<br>â€¢ **æ–¹å‘é¡¹**ï¼šæƒ©ç½šæœå‘éšœç¢ç‰©è¿åŠ¨ï¼ˆé€Ÿåº¦æ–¹å‘ä¸åˆ°éšœç¢ç‰©ç‚¹æ–¹å‘å¤¹è§’å°äº `Î¸_threshold`ï¼‰ã€‚ | `w_obstacle = 2.0`<br>`d_prox = 3.0m`<br>`Î¸_threshold = 20Â°` |\n| | **- ç¨³å®šé£è¡Œå¥–åŠ± (r_stable)** | `r_stable = w_stable * (r_altitude + r_attitude)`<br>â€¢ **é«˜åº¦é¡¹**ï¼šé¼“åŠ±ä¸é¢†å¯¼è€…ä¿æŒç›¸ä¼¼é«˜åº¦ï¼ˆé«˜æ–¯å‡½æ•°ï¼‰ã€‚<br>`r_altitude = exp( -((h^i - h^l)/Î±)^2 )`<br>â€¢ **å§¿æ€é¡¹**ï¼šé¼“åŠ±æ— äººæœºä¿æŒç›´ç«‹ï¼ˆä¸Šå‘é‡zåˆ†é‡æ¥è¿‘1ï¼‰ã€‚<br>`r_attitude = exp( -((u_z - 1)/Î²)^2 )` | `w_stable = 1.0`<br>`Î± = 0.1`<br>`Î² = 0.1` |\n| | **- é‚»å±…æ„ŸçŸ¥å¥–åŠ± (r_perception)** | `r_perception = w_perception * (r_visibility + r_recovery)`<br>â€¢ **å¯è§æ€§é¡¹**ï¼šå¥–åŠ±ä¿æŒæ›´å¤šé‚»å±…åœ¨æ„ŸçŸ¥èŒƒå›´å†…ï¼ˆæ¯”ä¾‹ï¼‰ã€‚<br>`r_visibility = |N_perceived^i| / |N^i|`<br>â€¢ **æ¢å¤é¡¹**ï¼šå½“å®Œå…¨ä¸¢å¤±é‚»å±…æ—¶ï¼Œè§¦å‘ä¸‹é™è‡³ `h_recovery` çš„æ¢å¤æœºåŠ¨ï¼Œä»¥é‡æ–°è·å–å› å‚ç›´è§†åœºæœ‰é™è€Œä¸¢å¤±çš„é‚»å±…ã€‚ | `w_perception = 1.0`<br>`h_recovery = 1.0m` |\n| | **- ç¢°æ’æƒ©ç½š (r_collision)** | ç¨€ç–è´Ÿå¥–åŠ±ï¼Œè§¦å‘å›åˆç»ˆæ­¢ï¼š`r_collision = -10 * I_collision` | ç¢°æ’åˆ¤å®šé˜ˆå€¼ï¼šæ— äººæœºé—´ < 0.4mï¼Œ æ— äººæœº-éšœç¢ç‰© < 0.2m |\n| | **ç½‘ç»œæ¶æ„ä¸è®­ç»ƒ** | **æ¶æ„**ï¼šå…±äº«ç¼–ç å™¨ï¼ˆCNNå¤„ç†æ …æ ¼ + MLPå¤„ç†å…¶ä»–è§‚æµ‹ï¼‰ â†’ 512ç»´å…±äº«ç‰¹å¾ â†’ **æ¼”å‘˜å¤´**ï¼ˆè¾“å‡ºé«˜æ–¯ç­–ç•¥çš„å‡å€¼å’Œæ–¹å·®ï¼‰å’Œ**è¯„è®ºå®¶å¤´**ï¼ˆè¾“å‡ºä»·å€¼å‡½æ•°ï¼‰ã€‚<br>**ç®—æ³•**ï¼šè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ã€‚<br>â€¢ **ä¼˜åŠ¿ä¼°è®¡**ï¼šä½¿ç”¨å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰ï¼š`Ã‚_t^{GAE(Î³,Î»)} = âˆ‘_{l=0",
            "timestamp": "2026-01-27T15:57:58.225877",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\nè¯¥è®ºæ–‡é€šè¿‡**ä»¿çœŸ**å’Œ**çœŸå®ä¸–ç•Œå®éªŒ**å¯¹æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†å…¨é¢éªŒè¯ã€‚å…¶è¯„ä»·ä½“ç³»ä¸ä¾èµ–äºä¼ ç»Ÿæ„ä¹‰ä¸Šçš„é™æ€â€œæ•°æ®é›†â€ï¼Œè€Œæ˜¯é€šè¿‡**åœ¨ç‰¹å®šç¯å¢ƒä¸­è¿è¡Œå¤§é‡ä»¿çœŸè¯•éªŒå’Œç‰©ç†é£è¡Œè¯•éªŒ**æ¥æ”¶é›†æ€§èƒ½æ•°æ®ã€‚ä»¥ä¸‹æ˜¯å…¶ä½¿ç”¨çš„è¯„ä»·æŒ‡æ ‡å’Œâ€œæ•°æ®â€æ¥æºçš„è¯¦ç»†åˆ†æã€‚\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡è¯¦æƒ…\nè®ºæ–‡å®šä¹‰äº†å…­ä¸ªæ ¸å¿ƒå®šé‡æŒ‡æ ‡ï¼Œç”¨äºä»ä¸åŒç»´åº¦è¯„ä¼°é›†ç¾¤å¯¼èˆªçš„æ€§èƒ½ã€‚æ‰€æœ‰æŒ‡æ ‡å‡åœ¨å¤šæ¬¡ç‹¬ç«‹è¯•éªŒï¼ˆé€šå¸¸ä¸º100æ¬¡ï¼‰ä¸­è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ã€‚\n\n| æŒ‡æ ‡åç§° | ç¼©å†™ | è®¡ç®—å…¬å¼ / å®šä¹‰ | è¯„ä¼°ç›®çš„ä¸ç»†èŠ‚ |\n| :--- | :--- | :--- | :--- |\n| **æˆåŠŸç‡** | SR | \\( SR = \\frac{N_{\\text{successful trials}}}{N_{\\text{trials}}} \\times 100 \\) | **æ ¸å¿ƒä»»åŠ¡å®ŒæˆæŒ‡æ ‡**ã€‚ä¸€æ¬¡è¯•éªŒæˆåŠŸçš„æ¡ä»¶æ˜¯ï¼šæ•´ä¸ªé›†ç¾¤ï¼ˆæ‰€æœ‰æ— äººæœºï¼‰åœ¨**æ— ç¢°æ’**ï¼ˆä¸éšœç¢ç‰©æˆ–å…¶ä»–æ— äººæœºï¼‰çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸè·Ÿéšé¢†èˆªè€…æŠµè¾¾ç›®æ ‡ç‚¹ã€‚è¯•éªŒç»ˆæ­¢æ¡ä»¶åŒ…æ‹¬ï¼šç¢°æ’ã€é£è¡Œé«˜åº¦è¶…é™ã€æˆ–æ‰€æœ‰è·Ÿéšè€…å®Œå…¨ä¸¢å¤±å¯¹é¢†èˆªè€…çš„æ„ŸçŸ¥ã€‚ |\n| **ä»»åŠ¡è¿›åº¦** | MP | \\( MP = \\frac{1}{N_{\\text{trials}}} \\sum_{k=1}^{N_{\\text{trials}}} \\left[ \\left(1 - \\frac{\\|P_{\\text{end}}^k - P_{\\text{goal}}\\|_2}{\\|P_{\\text{start}} - P_{\\text{goal}}\\|_2} \\right) \\times 100 \\right] \\) | **è¯„ä¼°å¯¼èˆªæ•ˆç‡**ã€‚è¡¡é‡é›†ç¾¤å‘ç›®æ ‡ç‚¹å‰è¿›çš„å¹³å‡è¿›åº¦æ¯”ä¾‹ã€‚å³ä½¿ä»»åŠ¡å¤±è´¥ï¼ˆå¦‚ä¸­é€”ç¢°æ’ï¼‰ï¼Œè¯¥æŒ‡æ ‡ä¹Ÿèƒ½åæ˜ å¤±è´¥å‰å·²å®Œæˆçš„è¿›åº¦ï¼Œæ¯”å•çº¯çš„äºŒå€¼æˆåŠŸç‡æä¾›æ›´ç»†ç²’åº¦çš„æ€§èƒ½ä¿¡æ¯ã€‚ |\n| **é›†ç¾¤åŠå¾„** | FR | \\( FR = \\frac{1}{N_{\\text{trials}}} \\sum_{k=1}^{N_{\\text{trials}}} \\left[ \\frac{1}{T_k} \\sum_{t=1}^{T_k} \\max_{i=1,\\dots,N} \\|\\mathbf{p}_t^{i,k} - \\mathbf{p}_t^{c,k}\\|_2 \\right] \\) | **è¯„ä¼°é›†ç¾¤ç´§å‡‘åº¦ä¸å‡èšåŠ›**ã€‚è®¡ç®—æ•´ä¸ªä»»åŠ¡è¿‡ç¨‹ä¸­ï¼Œè·ç¦»é›†ç¾¤ä¸­å¿ƒæœ€è¿œçš„æ— äººæœºçš„å¹³å‡è·ç¦»ã€‚è¾ƒå°çš„FRå€¼è¡¨æ˜é›†ç¾¤ä¿æŒç´§å‡‘ï¼Œè¿™å¯¹äºåœ¨é€šä¿¡ç¼ºå¤±ä¸‹ç»´æŒå±€éƒ¨æ„ŸçŸ¥å’ŒååŒè‡³å…³é‡è¦ã€‚å€¼è¿‡å¤§ä¼šå¢åŠ è·Ÿéšè€…ä¸¢å¤±é‚»å±…æ„ŸçŸ¥çš„é£é™©ã€‚ |\n| **æœ€å°é—´éš”** | MS | \\( MS = \\frac{1}{N_{\\text{trials}}} \\sum_{k=1}^{N_{\\text{trials}}} \\left[ \\frac{1}{T_k} \\sum_{t=1}^{T_k} \\min_{i \\neq j} \\|\\mathbf{p}_t^{i,k} - \\mathbf{p}_t^{j,k}\\|_2 \\right] \\) | **è¯„ä¼°é›†ç¾¤å†…éƒ¨å®‰å…¨æ€§**ã€‚è¡¡é‡æ•´ä¸ªä»»åŠ¡è¿‡ç¨‹ä¸­ï¼Œä»»æ„ä¸¤æ¶æ— äººæœºä¹‹é—´è·ç¦»çš„æœ€å°å€¼çš„å¹³å‡å€¼ã€‚è¿™æ˜¯**é˜²ç¢°æ’**çš„ç›´æ¥åº¦é‡ï¼Œå¿…é¡»å§‹ç»ˆå¤§äºæ— äººæœºçš„ç‰©ç†å°ºå¯¸ï¼ˆæ–‡ä¸­æ— äººæœºåŠå¾„ \\( r_{uav} = 0.2m \\)ï¼‰ã€‚ |\n| **å¯¹é½åº¦** | AL | \\( AL = \\frac{1}{N_{\\text{trials}}} \\sum_{k=1}^{N_{\\text{trials}}} \\left[ \\frac{1}{T_k} \\sum_{t=1}^{T_k} \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\mathbf{v}_t^{i,k} \\cdot \\mathbf{v}_t^{\\text{avg},k}}{\\|\\mathbf{v}_t^{i,k}\\|_2 \\|\\mathbf{v}_t^{\\text{avg},k}\\|_2} \\right] \\) | **è¯„ä¼°è¿åŠ¨æ–¹å‘ä¸€è‡´æ€§**ã€‚è®¡ç®—æ¯ä¸ªæ— äººæœºé€Ÿåº¦æ–¹å‘ä¸é›†ç¾¤å¹³å‡é€Ÿåº¦æ–¹å‘ä¹‹é—´ä½™å¼¦ç›¸ä¼¼åº¦çš„å¹³å‡å€¼ã€‚å€¼è¶Šæ¥è¿‘1ï¼Œè¡¨æ˜é›†ç¾¤è¿åŠ¨æ–¹å‘è¶Šä¸€è‡´ï¼Œé£è¡Œè¶Šæœ‰åºã€‚**è®ºæ–‡ç‰¹æ„åœ¨å¥–åŠ±å‡½æ•°ä¸­æœªåŠ å…¥é€Ÿåº¦å¯¹é½é¡¹**ï¼Œå› æ­¤è¿™ä¸ªæŒ‡æ ‡ç”¨äºè§‚å¯Ÿ emergent è¡Œä¸ºæ˜¯å¦è‡ªç„¶äº§ç”Ÿäº†å¯¹é½ã€‚ |\n| **æœ€å°éšœç¢ç‰©è·ç¦»** | MDO | \\( MDO = \\frac{1}{N_{\\text{trials}}} \\sum_{k=1}^{N_{\\text{trials}}} \\left[ \\frac{1}{T_k} \\sum_{t=1}^{T_k} \\min_{\\substack{i=1,\\dots,N \\\\ m=1,\\dots,M}} \\|\\mathbf{p}_t^{i,k} - \\mathbf{o}_t^{m,k}\\|_2 \\right] \\) | **è¯„ä¼°å¤–éƒ¨ç¯å¢ƒå®‰å…¨æ€§**ã€‚è¡¡é‡æ•´ä¸ªä»»åŠ¡è¿‡ç¨‹ä¸­ï¼Œä»»ä½•æ— äººæœºä¸ä»»ä½•è¢«æ„ŸçŸ¥åˆ°çš„éšœç¢ç‰©ç‚¹ä¹‹é—´è·ç¦»çš„æœ€å°å€¼çš„å¹³å‡å€¼ã€‚è¿™æ˜¯**é¿éšœ**æ€§èƒ½çš„ç›´æ¥åº¦é‡ï¼Œå€¼è¶Šå¤§è¡¨æ˜å®‰å…¨è£•åº¦è¶Šé«˜ã€‚ |\n\n### äºŒã€ â€œæ•°æ®é›†â€/å®éªŒç¯å¢ƒè¯¦æƒ…\nè®ºæ–‡çš„æ€§èƒ½æ•°æ®æ¥æºäºåœ¨**å¤šç§ä»¿çœŸç¯å¢ƒå’ŒçœŸå®ç‰©ç†ç¯å¢ƒ**ä¸­è¿›è¡Œçš„ç³»ç»Ÿæ€§è¯•éªŒï¼Œè€Œéä¸€ä¸ªé¢„å½•åˆ¶çš„é™æ€æ•°æ®é›†ã€‚\n\n#### 1. ä»¿çœŸè®­ç»ƒä¸æµ‹è¯•ç¯å¢ƒ\n- **è®­ç»ƒç¯å¢ƒ**ï¼š\n    - **åœºæ™¯**ï¼šä¸€ä¸ªä¸­å¿ƒæœ‰ç”ŸæˆåŒºåŸŸã€å‘¨å›´éšæœºåˆ†å¸ƒ**æŸ±çŠ¶éšœç¢ç‰©**çš„åœºåœ°ï¼ˆå¦‚å›¾6aï¼‰ã€‚\n    - **æ•°æ®ç”Ÿæˆæ–¹å¼**ï¼šåœ¨Nvidia Isaac Simä¸­ä½¿ç”¨**512ä¸ªå¹¶è¡Œç¯å¢ƒ**è¿›è¡ŒGPUåŠ é€Ÿè®­ç»ƒï¼Œå…±é‡‡é›†**5äº¿ä¸ªæ—¶é—´æ­¥**çš„æ•°æ®ã€‚\n    - **éšæœºåŒ–**ï¼šæ¯ä¸ªè®­ç»ƒå›åˆï¼ˆepisodeï¼‰éšæœºåŒ–ä»¥ä¸‹è¦ç´ ä»¥æå‡ç­–ç•¥é²æ£’æ€§ï¼š\n        - æ— äººæœºåˆå§‹ä½ç½®ï¼ˆåœ¨3x3ç½‘æ ¼å†…ï¼‰ã€åˆå§‹æœå‘ã€‚\n        - éšœç¢ç‰©ï¼ˆæŸ±å­ï¼‰çš„ä½ç½®ã€‚\n        - ç›®æ ‡ç‚¹ä½ç½®ï¼ˆåœ¨ä»¥ç”ŸæˆåŒºä¸ºä¸­å¿ƒã€åŠå¾„30ç±³çš„åœ†ä¸Šéšæœºé‡‡æ ·ï¼‰ã€‚\n        - é¢†èˆªè€…é€Ÿåº¦ã€‚\n\n- **æµ‹è¯•ç¯å¢ƒ**ï¼ˆç”¨äºè¯„ä¼°æ³›åŒ–èƒ½åŠ›ï¼‰ï¼š\n    | ç¯å¢ƒåç§° | éšœç¢ç‰©æè¿° | æœ€å°é—´éš™ | æµ‹è¯•ç›®çš„ |\n    | :--- | :--- | :--- | :--- |\n    | **æ— éšœç¢ç‰©** | ç©ºæ—·åœºåœ° | æ—  | åŸºçº¿æ€§èƒ½ï¼ŒéªŒè¯åŸºç¡€é›†ç¾¤è·Ÿéšèƒ½åŠ›ã€‚ |\n    | **è®­ç»ƒç¯å¢ƒï¼ˆç¨€ç–ï¼‰** | éšæœºæŸ±å­ | 10ç±³ | åœ¨ç†Ÿæ‚‰ä½†ç®€å•çš„éšœç¢ç¯å¢ƒä¸­æµ‹è¯•ã€‚ |\n    | **è®­ç»ƒç¯å¢ƒï¼ˆå¯†é›†ï¼‰** | éšæœºæŸ±å­ | 5ç±³ | åœ¨ç†Ÿæ‚‰ä½†å¤æ‚çš„éšœç¢ç¯å¢ƒä¸­æµ‹è¯•ã€‚ |\n    | **ç®€åŒ–æ£®æ—** | ä¸è§„åˆ™åˆ†å¸ƒçš„æ ‘æœ¨å’Œå²©çŸ³æ¨¡å‹ï¼ˆå›¾6bï¼‰ | 5ç±³ | åœ¨**æœªè§è¿‡çš„**ã€æ›´è‡ªç„¶çš„éšœç¢å¸ƒå±€ä¸­æµ‹è¯•æ³›åŒ–èƒ½åŠ›ã€‚ |\n    | **å¤æ‚æ£®æ—** | å¸¦æœ‰è¯¦ç»†æ ‘å¹²å’Œæ ‘å¶çš„æ ‘æœ¨ã€å²©çŸ³æ¨¡å‹ï¼ˆå›¾6cï¼‰ | 5ç±³ | åœ¨**æœªè§è¿‡çš„**ã€é«˜åº¦å¤æ‚å’ŒçœŸå®çš„éšœç¢ç¯å¢ƒä¸­æµ‹è¯•æé™æ³›åŒ–èƒ½åŠ›ã€‚ |\n\n#### 2. çœŸå®ä¸–ç•Œå®éªŒç¯å¢ƒ\nè®ºæ–‡åœ¨**5ä¸ªä¸åŒçš„ç‰©ç†åœºæ™¯**ä¸­è¿›è¡Œäº†éªŒè¯ï¼Œä½¿ç”¨**5æ¶**å®šåˆ¶æ— äººæœºï¼ˆ1é¢†èˆªï¼Œ4è·Ÿéšï¼‰ã€‚è¿™äº›å®éªŒæ„æˆäº†æœ€å…³é”®çš„â€œæµ‹è¯•é›†â€ï¼Œè¯æ˜äº†**ä»¿çœŸåˆ°ç°å®ï¼ˆSim-to-Realï¼‰** çš„æœ‰æ•ˆæ€§ã€‚\n\n| ç¯å¢ƒç±»å‹ | åœºæ™¯åç§° | ç¯å¢ƒç‰¹ç‚¹ä¸æŒ‘æˆ˜ | éªŒè¯ç›®çš„ |\n| :--- | :--- | :--- | :--- |\n| **å®¤å†…** | 1. æ‚ä¹±æ–¹å— | ç”±çº¸ç®±æ„æˆçš„å¯†é›†ã€ç»“æ„åŒ–éšœç¢ã€‚ | åœ¨å—æ§ä½†å¤æ‚çš„å‡ ä½•ç¯å¢ƒä¸­æµ‹è¯•é¿éšœå’Œé›†ç¾¤ä¿æŒã€‚ |\n| | 2. å¢™å£ | å…·æœ‰é•¿ç›´å¢™é¢çš„é€šé“ç¯å¢ƒã€‚ | æµ‹è¯•åœ¨å¤§å‹å¹³é¢éšœç¢å‰çš„ç¾¤ä½“è¡Œä¸ºã€‚ |\n| **å®¤å¤–** | 3. æŸ±å­ | ç”±çœŸå®æŸ±å­æ„æˆçš„ç±»ä¼¼ä»¿çœŸè®­ç»ƒçš„ç¯å¢ƒã€‚ | æµ‹è¯•ä»ä»¿çœŸåˆ°ç›¸ä¼¼ç°å®ç¯å¢ƒçš„ç›´æ¥è½¬ç§»ã€‚ |\n| | 4. å±•ç¤ºå¡” | å¼€é˜”åœºåœ°ä¸­æœ‰å¤§å‹ç‹¬ç«‹éšœç¢ç‰©ã€‚ | æµ‹è¯•åœ¨å¼€æ”¾ç©ºé—´å’Œå­¤ç«‹éšœç¢ä¸‹çš„è¡Œä¸ºã€‚ |\n| | 5. çŸ®æ ‘æ— | çœŸå®çš„ã€æ¤è¢«å¯†é›†çš„æ£®æ—åŒºåŸŸï¼ˆæœ€å…·æŒ‘æˆ˜æ€§ï¼‰ã€‚ | æµ‹è¯•åœ¨**å®Œå…¨éç»“æ„åŒ–ã€å¤æ‚**è‡ªç„¶ç¯å¢ƒä¸­çš„é²æ£’æ€§ã€é¿éšœå’Œæ„ŸçŸ¥èƒ½åŠ›ã€‚ |\n\n### æ€»ç»“\nè¯¥è®ºæ–‡é‡‡ç”¨äº†ä¸€å¥—**å¤šå±‚æ¬¡ã€å¤šç»´åº¦çš„è¯„ä¼°ä½“ç³»**ï¼š\n- **æŒ‡æ ‡**ï¼šæ¶µç›–äº†**ä»»åŠ¡å®Œæˆåº¦ï¼ˆSR, MPï¼‰ã€å®‰å…¨æ€§ï¼ˆMS, MDOï¼‰ã€é›†ç¾¤è´¨é‡ï¼ˆFR, ALï¼‰** ç­‰å…³é”®æ–¹é¢ã€‚\n- **æ•°æ®æ¥æº**ï¼šé€šè¿‡åœ¨**é«˜åº¦éšæœºåŒ–çš„ä»¿çœŸç¯å¢ƒ**ä¸­è¿›è¡Œå¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒå’Œç³»ç»Ÿæµ‹è¯•ï¼Œä»¥åŠåœ¨**å¤šæ ·åŒ–çš„çœŸå®å®¤å†…å¤–åœºæ™¯**ä¸­è¿›è¡Œç‰©ç†éªŒè¯æ¥è·å¾—æ€§èƒ½æ•°æ®ã€‚è¿™ç§è¯„ä¼°æ–¹æ³•ä¸ä»…è¯æ˜äº†ç®—æ³•çš„ç»å¯¹æ€§èƒ½ï¼ˆé€šè¿‡å¯¹æ¯”åŸºçº¿ï¼‰ï¼Œæ›´é‡è¦çš„æ˜¯è¯æ˜äº†å…¶**å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œä»¿çœŸåˆ°ç°å®çš„è¿ç§»èƒ½åŠ›**ï¼Œè¿™æ˜¯è¯¥ç ”ç©¶å®é™…ä»·å€¼çš„å…³é”®ä½“ç°ã€‚",
            "timestamp": "2026-01-27T15:59:07.811032",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†ä¸éªŒè¯ç¯å¢ƒ\nè®ºæ–‡çš„å®éªŒéªŒè¯åˆ†ä¸º**ä»¿çœŸ**å’Œ**çœŸå®ä¸–ç•Œ**ä¸¤éƒ¨åˆ†ï¼Œå‡å›´ç»•**é€šä¿¡æ‹’æ­¢ã€æ— å¤–éƒ¨å®šä½**çš„æ— äººæœºé›†ç¾¤é›†ä½“å¯¼èˆªåœºæ™¯å±•å¼€ã€‚\n\n#### 1. ä»¿çœŸå®éªŒç¯å¢ƒ\nåœ¨ **Nvidia Isaac Sim** ä¸­æ„å»ºäº†å¤šç§é«˜ä¿çœŸä»¿çœŸç¯å¢ƒï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ·±åº¦å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼š\n- **è®­ç»ƒç¯å¢ƒ**ï¼šä¸€ä¸ªåŒ…å«éšæœºåˆ†å¸ƒæŸ±çŠ¶éšœç¢ç‰©çš„æ‚ä¹±ç¯å¢ƒï¼ˆCluttered pillarsï¼‰ã€‚ç›®æ ‡ç‚¹éšæœºç”Ÿæˆåœ¨ä»¥åˆå§‹åŒºåŸŸä¸ºä¸­å¿ƒã€åŠå¾„ä¸º30ç±³çš„åœ†ä¸Šã€‚\n- **æµ‹è¯•/æ³›åŒ–ç¯å¢ƒ**ï¼š\n    - **ç®€åŒ–æ£®æ—**ï¼šåŒ…å«ä¸è§„åˆ™åˆ†å¸ƒçš„æ ‘æœ¨å’Œå²©çŸ³ã€‚\n    - **å¤æ‚æ£®æ—**ï¼šåŒ…å«å¸¦æœ‰è¯¦ç»†æ ‘å¹²å’Œæ ‘å¶çš„æ ‘æœ¨å’Œå²©çŸ³ï¼Œæ¨¡æ‹Ÿæ›´çœŸå®çš„æˆ·å¤–ç¯å¢ƒã€‚\n- **éšœç¢ç‰©å¯†åº¦**ï¼šå®éªŒè®¾ç½®äº†ä¸åŒçš„æœ€å°éšœç¢ç‰©é—´éš™ï¼ˆ`Min. obstacle gap`ï¼‰æ¥è¯„ä¼°ä¸åŒå¤æ‚åº¦ï¼š\n    - æ— éšœç¢ç‰©\n    - ç¨€ç–éšœç¢ï¼ˆ10ç±³é—´éš™ï¼‰\n    - å¯†é›†éšœç¢ï¼ˆ5ç±³é—´éš™ï¼‰\n\n#### 2. çœŸå®ä¸–ç•Œå®éªŒç¯å¢ƒ\nä½¿ç”¨**5æ¶å®šåˆ¶å››æ—‹ç¿¼æ— äººæœº**ï¼Œåœ¨å¤šæ ·åŒ–çš„å®¤å†…å¤–åœºæ™¯ä¸­éªŒè¯äº†**ä»¿çœŸåˆ°ç°å®**çš„è¿ç§»èƒ½åŠ›ï¼š\n- **å®¤å†…ç¯å¢ƒ**ï¼š\n    - æ‚ä¹±æ–¹å—ï¼ˆCluttered blocksï¼‰\n    - å¢™å£ï¼ˆWallï¼‰\n- **å®¤å¤–ç¯å¢ƒ**ï¼š\n    - æŸ±çŠ¶éšœç¢ï¼ˆPillarsï¼‰\n    - å±•ç¤ºå¡”ï¼ˆDisplay towerï¼‰\n    - çŸ®æ ‘æ—ï¼ˆShort forestï¼‰\n\n**å…³é”®ç‚¹**ï¼šæ‰€æœ‰å®éªŒå‡**ä¸ä½¿ç”¨ä»»ä½•æœºé—´é€šä¿¡ã€GPSæˆ–å¤–éƒ¨å®šä½ç³»ç»Ÿ**ï¼ˆå¦‚åŠ¨ä½œæ•æ‰ï¼‰ï¼Œå®Œå…¨ä¾èµ–æœºè½½LiDARæ„ŸçŸ¥å’ŒDRLç­–ç•¥ã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡å®šä¹‰äº†å…­ä¸ªæ ¸å¿ƒé‡åŒ–æŒ‡æ ‡ï¼Œç”¨äºå…¨é¢è¯„ä¼°é›†ç¾¤å¯¼èˆªæ€§èƒ½ï¼ˆç»“æœå‡ä¸º100æ¬¡ç‹¬ç«‹è¯•éªŒçš„å‡å€¼Â±æ ‡å‡†å·®ï¼‰ï¼š\n\n| æŒ‡æ ‡ | ç¼©å†™ | å®šä¹‰ä¸ç›®çš„ |\n| :--- | :--- | :--- |\n| **æˆåŠŸç‡** | `SR` | æ‰€æœ‰æ— äººæœºæ— ç¢°æ’è·Ÿéšé¢†èˆªè€…åˆ°è¾¾ç›®çš„åœ°çš„è¯•éªŒç™¾åˆ†æ¯”ã€‚**é¦–è¦ä»»åŠ¡å®Œæˆåº¦æŒ‡æ ‡**ã€‚ |\n| **ä»»åŠ¡è¿›åº¦** | `MP` | é›†ç¾¤å‘ç›®æ ‡å‰è¿›çš„è·ç¦»ä¸æ€»è·ç¦»çš„æ¯”å€¼ï¼ˆ%ï¼‰ï¼Œè¯„ä¼°å¯¼èˆªæœ‰æ•ˆæ€§ã€‚ |\n| **é›†ç¾¤åŠå¾„** | `FR` | é›†ç¾¤ä¸­å¿ƒåˆ°æœ€è¿œæ— äººæœºçš„å¹³å‡è·ç¦»ï¼ˆç±³ï¼‰ï¼Œè¡¡é‡é˜Ÿå½¢ç´§å‡‘æ€§ã€‚å€¼è¶Šå°ï¼Œå‡èšåŠ›è¶Šå¼ºã€‚ |\n| **æœ€å°é—´è·** | `MS` | ä»»æ„ä¸¤æ¶æ— äººæœºä¹‹é—´çš„æœ€å°å¹³å‡è·ç¦»ï¼ˆç±³ï¼‰ï¼Œ**è¯„ä¼°æœºé—´é˜²æ’å®‰å…¨æ€§**ã€‚ |\n| **å¯¹é½åº¦** | `AL` | å„æ— äººæœºé€Ÿåº¦æ–¹å‘ä¸é›†ç¾¤å¹³å‡é€Ÿåº¦æ–¹å‘ä½™å¼¦ç›¸ä¼¼åº¦çš„å¹³å‡å€¼ã€‚å€¼è¶Šæ¥è¿‘1ï¼Œè¿åŠ¨æ–¹å‘è¶Šä¸€è‡´ã€‚ |\n| **æœ€å°éšœç¢è·ç¦»** | `MDO` | ä»»ä½•æ— äººæœºä¸ä»»ä½•éšœç¢ç‰©æ£€æµ‹ç‚¹ä¹‹é—´çš„æœ€å°å¹³å‡è·ç¦»ï¼ˆç±³ï¼‰ï¼Œ**è¯„ä¼°é¿éšœå®‰å…¨æ€§**ã€‚ |\n\n### ä¸‰ã€ æ€§èƒ½æå‡ä¸å¯¹æ¯”åˆ†æ\nè®ºæ–‡å°†æå‡ºçš„DRLæ–¹æ³•ä¸ä¸‰ç§å…ˆè¿›çš„åŸºçº¿æ–¹æ³•è¿›è¡Œäº†å…¨é¢å¯¹æ¯”ï¼Œè¯æ˜äº†å…¶æ˜¾è‘—ä¼˜åŠ¿ã€‚\n\n#### 1. å¯¹æ¯”åŸºçº¿æ–¹æ³•\n- **PACNav**ï¼šåŸºäºç”Ÿç‰©å¯å‘çš„åˆ†å¸ƒå¼å¯å‘å¼æ–¹æ³•ï¼Œåœ¨çœŸå®æ£®æ—ä¸­éªŒè¯è¿‡ã€‚\n- **VPF**ï¼šåŸºäºè§†è§‰æŠ•å½±åœºçš„ç”Ÿç‰©å¯å‘æ–¹æ³•ï¼Œåœ¨å®¤å†…éªŒè¯è¿‡ã€‚\n- **DAgger**ï¼šæ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨ä¸“å®¶ç­–ç•¥ï¼ˆç»“åˆæ”¹è¿›çš„Reynoldsè§„åˆ™å’Œäººå·¥åŠ¿åœºï¼‰ã€‚\n\n#### 2. å…³é”®æ€§èƒ½ç»“æœï¼ˆåŸºäºè¡¨IIIï¼‰\nåœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„**å¯†é›†éšœç¢æµ‹è¯•ç¯å¢ƒ**ï¼ˆå¦‚å¤æ‚æ£®æ—ï¼Œ5ç±³é—´éš™ï¼‰ä¸­ï¼Œæ‰€ææ–¹æ³•å±•ç°å‡º**å‹å€’æ€§ä¼˜åŠ¿**ï¼š\n\n| ç¯å¢ƒ/æŒ‡æ ‡ | æ‰€ææ–¹æ³• | æœ€ä½³åŸºçº¿ (PACNav) | **æ€§èƒ½æå‡** |\n| :--- | :--- | :--- | :--- |\n| **æˆåŠŸç‡ (`SR`)** | **72.0%** | 31.0% | **â†‘ 132%** (ç»å¯¹æå‡41ä¸ªç™¾åˆ†ç‚¹) |\n| **ä»»åŠ¡è¿›åº¦ (`MP`)** | **88.2%** | 67.6% | **â†‘ 30%** (ç»å¯¹æå‡20.6ä¸ªç™¾åˆ†ç‚¹) |\n| **é›†ç¾¤åŠå¾„ (`FR`)** | **1.39 m** | 1.82 m | **é˜Ÿå½¢æ›´ç´§å‡‘ (â†“ 24%)** |\n| **å¯¹é½åº¦ (`AL`)** | **0.88** | 0.75 | **è¿åŠ¨æ–¹å‘æ›´ä¸€è‡´** |\n| **æœ€å°éšœç¢è·ç¦» (`MDO`)** | 0.79 m | 0.82 m | å®‰å…¨æ€§ç›¸å½“ï¼Œåœ¨æ›´ç´§å‡‘é˜Ÿå½¢ä¸‹ä¿æŒå®‰å…¨è·ç¦» |\n\n#### 3. æ ¸å¿ƒæ€§èƒ½æå‡æ€»ç»“\n- **å“è¶Šçš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨è®­ç»ƒæœªè§çš„å¤æ‚æ£®æ—ç¯å¢ƒä¸­ï¼Œæ‰€ææ–¹æ³•çš„æˆåŠŸç‡ï¼ˆ72%ï¼‰è¿œé«˜äºæ‰€æœ‰åŸºçº¿ï¼ˆ5%-31%ï¼‰ï¼Œè¯æ˜äº†DRLç­–ç•¥å­¦ä¹ åˆ°äº†å¯æ³›åŒ–çš„å¤æ‚è¡Œä¸ºï¼ˆå¹³è¡¡èšé›†ä¸é¿éšœï¼‰ã€‚\n- **é«˜æ•ˆçš„é›†ä½“å¯¼èˆª**ï¼šæ›´é«˜çš„ä»»åŠ¡è¿›åº¦ï¼ˆ`MP`ï¼‰è¡¨æ˜é›†ç¾¤èƒ½æ›´æœ‰æ•ˆåœ°æ•´ä½“å‘ç›®æ ‡ç§»åŠ¨ï¼Œé¿å…äº†å› å±€éƒ¨é¿éšœå¯¼è‡´çš„åœæ»æˆ–åˆ†æ•£ã€‚\n- **ç´§å‡‘ä¸”å®‰å…¨çš„é˜Ÿå½¢**ï¼šåœ¨ä¿æŒä¸åŸºçº¿ç›¸å½“ç”šè‡³æ›´ä¼˜çš„å®‰å…¨é—´è·ï¼ˆ`MS`ï¼‰å’Œéšœç¢è·ç¦»ï¼ˆ`MDO`ï¼‰çš„åŒæ—¶ï¼Œå®ç°äº†æ›´å°çš„é›†ç¾¤åŠå¾„ï¼ˆ`FR`ï¼‰ï¼Œè¿™å¯¹åœ¨æœ‰é™ç©ºé—´å†…å¯¼èˆªå’Œç»´æŒæ„ŸçŸ¥è‡³å…³é‡è¦ã€‚\n- **æˆåŠŸçš„ä»¿çœŸåˆ°ç°å®è¿ç§»**ï¼šçœŸå®ä¸–ç•Œå®éªŒåœ¨æ‰€æœ‰æµ‹è¯•åœºæ™¯ä¸­å‡æœªå‘ç”Ÿç¢°æ’ï¼ˆ`MS`å’Œ`MDO`å§‹ç»ˆé«˜äºç¢°æ’é˜ˆå€¼ï¼‰ï¼ŒéªŒè¯äº†åœ¨ä»¿çœŸä¸­è®­ç»ƒçš„DRLç­–ç•¥èƒ½ç›´æ¥å¤„ç†çœŸå®ä¸–ç•Œçš„å™ªå£°ã€é£æ‰°å’Œå¤æ‚å‡ ä½•ã€‚\n\n### ç»“è®º\nè®ºæ–‡é€šè¿‡åœ¨**é«˜åº¦éšæœºåŒ–çš„ä»¿çœŸç¯å¢ƒ**ä¸­è®­ç»ƒï¼Œå¹¶åœ¨**å¤šæ ·åŒ–çš„çœŸå®å®¤å†…å¤–åœºæ™¯**ä¸­éªŒè¯ï¼Œè¯æ˜äº†å…¶LiDAR+DRLæ¡†æ¶çš„ä¼˜è¶Šæ€§ã€‚ç›¸è¾ƒäºç°æœ‰çš„å¯å‘å¼æˆ–æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œæ‰€ææ–¹æ³•åœ¨**é€šä¿¡æ‹’æ­¢ã€æ— å¤–éƒ¨å®šä½çš„å¤æ‚éšœç¢ç¯å¢ƒ**ä¸­ï¼Œå®ç°äº†**æˆåŠŸç‡ç¿»å€ä»¥ä¸Šçš„æ˜¾è‘—æ€§èƒ½æå‡**ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å®‰å…¨æ€§å’Œé˜Ÿå½¢å‡èšåŠ›ï¼Œä¸ºè§£å†³æ— äººæœºé›†ç¾¤åœ¨å¯¹æŠ—æ€§æˆ–æ¶åŠ£ç¯å¢ƒä¸‹çš„è‡ªä¸»å¯¼èˆªé—®é¢˜æä¾›äº†åˆ‡å®å¯è¡Œçš„æ–¹æ¡ˆã€‚",
            "timestamp": "2026-01-27T15:59:54.933200",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„æ·±å…¥åˆ†æï¼Œæœ¬æ–‡æå‡ºçš„é€šä¿¡è‡ªç”±æ— äººæœºé›†ç¾¤å¯¼èˆªç³»ç»Ÿè™½ç„¶å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n### 1. **æ„ŸçŸ¥ç³»ç»Ÿçš„ç‰©ç†é™åˆ¶**\n    - **æ£€æµ‹è·ç¦»æœ‰é™**ï¼šLiDARç‚¹äº‘å¯†åº¦éšè·ç¦»å¹³æ–¹åæ¯”ä¸‹é™ï¼Œåœ¨çº¦10ç±³å¤–éš¾ä»¥å¯é æ£€æµ‹æ— äººæœºå¤§å°çš„ç›®æ ‡ï¼Œé™åˆ¶äº†é›†ç¾¤çš„ç‰©ç†æ‰©å±•èŒƒå›´ã€‚\n    - **è§†è§’ä¾èµ–æ€§**ï¼šåå°„èƒ¶å¸¦åœ¨å…¥å°„è§’å¤§äºçº¦45åº¦æ—¶åå°„å¼ºåº¦æ€¥å‰§ä¸‹é™ï¼Œå¯èƒ½å¯¼è‡´æ¼æ£€ã€‚å°½ç®¡é€šè¿‡å…¨å‘ç²˜è´´èƒ¶å¸¦ç¼“è§£ï¼Œä½†æç«¯è§’åº¦ä¸‹ä»å­˜åœ¨æŒ‘æˆ˜ã€‚\n    - **å‚ç›´è§†åœºï¼ˆFOVï¼‰ç‹­çª„**ï¼šLiDARå‚ç›´FOVä»…ä¸º-7Â°è‡³52Â°ï¼Œé™åˆ¶äº†æ— äººæœºåœ¨é«˜åº¦æ–¹å‘ä¸Šçš„æœºåŠ¨ç©ºé—´ï¼Œè¿«ä½¿é›†ç¾¤åœ¨ç›¸è¿‘é«˜åº¦é£è¡Œï¼Œå½±å“äº†ä¸‰ç»´é˜Ÿå½¢çš„çµæ´»æ€§ã€‚\n\n### 2. **ç­–ç•¥ä¸ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ç“¶é¢ˆ**\n    - **å‡ ä½•çº¦æŸå¯¼è‡´è§„æ¨¡ä¸Šé™**ï¼šå¦‚**è¡¨IV**æ‰€ç¤ºï¼Œå½“è·Ÿéšè€…æ•°é‡å¢è‡³8ä¸ªæˆ–ä»¥ä¸Šæ—¶ï¼Œåœ¨5ç±³çª„éš™ç¯å¢ƒä¸­çš„æˆåŠŸç‡æ˜¾è‘—ä¸‹é™ï¼ˆä¾‹å¦‚ï¼Œ10ä¸ªè·Ÿéšè€…æ—¶æˆåŠŸç‡ä»…46%ï¼‰ã€‚è¿™æ˜¯å› ä¸ºç­–ç•¥å€¾å‘äºä¿æŒç›¸è¿‘é«˜åº¦ä»¥ç»´æŒé‚»å±…æ£€æµ‹ï¼Œå¯¼è‡´é˜Ÿå½¢åœ¨æ°´å¹³æ–¹å‘æ‰©å¼ ã€‚é›†ç¾¤ç›´å¾„æ¥è¿‘éšœç¢é—´éš™æ—¶ï¼Œå¯¼èˆªå˜å¾—æå…¶å›°éš¾ã€‚\n    - **å»ä¸­å¿ƒåŒ–å…±è¯†å›°éš¾**ï¼šéšç€é›†ç¾¤è§„æ¨¡å¢å¤§ï¼Œå¯¹é½åº¦ï¼ˆ`AL`ï¼‰ä¸‹é™ï¼Œè¡¨æ˜åœ¨æ²¡æœ‰é€šä¿¡çš„æƒ…å†µä¸‹ï¼Œå¤§è§„æ¨¡é›†ç¾¤éš¾ä»¥è¾¾æˆè¿åŠ¨æ–¹å‘çš„ä¸€è‡´å…±è¯†ã€‚\n    - **è®¡ç®—è´Ÿè½½**ï¼šé‚»å±…è·Ÿè¸ªæ¨¡å—çš„è®¡ç®—å¤æ‚åº¦éšç‚¹æ•°å‘ˆ`O(nÂ²)`å¢é•¿ï¼Œå¯¹æœºè½½åµŒå…¥å¼ç³»ç»Ÿæ„æˆè´Ÿæ‹…ï¼Œé™åˆ¶äº†å¯å®æ—¶å¤„ç†çš„é‚»å±…æ•°é‡ä¸Šé™ï¼ˆè®ºæ–‡é€‰å®šä¸º6ä¸ªæ˜¯åŸºäºæ€§èƒ½ä¸è´Ÿè½½çš„æƒè¡¡ï¼‰ã€‚\n\n### 3. **å­¦ä¹ ç­–ç•¥çš„æ³›åŒ–è¾¹ç•Œ**\n    - **ç¯å¢ƒä¸é›†ç¾¤è§„æ¨¡æ³›åŒ–**ï¼šç­–ç•¥åœ¨è®­ç»ƒç¯å¢ƒï¼ˆ5æ¶æ— äººæœºã€æŸ±çŠ¶éšœç¢ï¼‰ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨æ›´å¤æ‚çš„â€œæ£®æ—â€æµ‹è¯•ç¯å¢ƒæˆ–æ›´å¤§è§„æ¨¡é›†ç¾¤ä¸­æ€§èƒ½æœ‰æ‰€è¡°å‡ï¼ˆå¦‚**è¡¨III**ä¸­æ£®æ—ç¯å¢ƒæˆåŠŸç‡72%ï¼‰ã€‚è¿™è¡¨æ˜ç­–ç•¥å­¦ä¹ åˆ°çš„è¡Œä¸ºå¯èƒ½ä»æœªå®Œå…¨è¦†ç›–æ‰€æœ‰ç°å®ä¸–ç•Œçš„å¤æ‚äº¤äº’ã€‚\n    - **åŠ¨æ€è§’è‰²ä¸ä»»åŠ¡**ï¼šå½“å‰æ¡†æ¶ä¸¥æ ¼åŒºåˆ†**å›ºå®š**çš„â€œé¢†å¯¼è€…â€ï¼ˆæœ‰ç›®æ ‡ä¿¡æ¯ï¼‰å’Œâ€œè·Ÿéšè€…â€ï¼ˆæ— ç›®æ ‡ä¿¡æ¯ï¼‰ã€‚ç³»ç»Ÿæ— æ³•å¤„ç†é¢†å¯¼è€…å¤±æ•ˆã€è§’è‰²åŠ¨æ€åˆ‡æ¢æˆ–ä»»åŠ¡åœ¨çº¿é‡åˆ†é…ç­‰åœºæ™¯ã€‚\n\n### 4. **ä¾èµ–ç‰¹å®šç¡¬ä»¶é…ç½®**\n    - **åå°„æ ‡è¯†ä¾èµ–**ï¼šç³»ç»Ÿä¾èµ–äºæ— äººæœºæœºèº«ä¸Šçš„**åå°„èƒ¶å¸¦**æ¥å®ç°å¯é çš„é‚»å±…æ£€æµ‹ã€‚è¿™å¢åŠ äº†éƒ¨ç½²å‰çš„å‡†å¤‡è¦æ±‚ï¼Œä¸”èƒ¶å¸¦å¯èƒ½ç£¨æŸã€æ±¡æŸï¼Œå½±å“æ€§èƒ½ã€‚\n    - **åŒæ„å‡è®¾**ï¼šæ‰€æœ‰è·Ÿéšè€…æ— äººæœºè¢«å‡å®šä¸ºå…·æœ‰ç›¸åŒçš„ä¼ æ„Ÿå™¨ï¼ˆLivox Mid-360 LiDARï¼‰å’Œè®¡ç®—å¹³å°ï¼ˆJetson Orin NXï¼‰ï¼Œåœ¨å¼‚æ„é›†ç¾¤ä¸­åº”ç”¨å¯èƒ½éœ€è¦é‡æ–°è°ƒæ•´æˆ–è®­ç»ƒã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡åœ¨ç»“è®ºéƒ¨åˆ†å·²ç®€è¦æåŠï¼Œå¹¶å¯å»¶ä¼¸å‡ºä»¥ä¸‹å…·ä½“æ”¹è¿›æ–¹å‘ï¼š\n\n### 1. **å¢å¼ºæ„ŸçŸ¥èƒ½åŠ›ä¸é²æ£’æ€§**\n    - **å¤šä¼ æ„Ÿå™¨èåˆ**ï¼šç»“åˆè½»é‡åŒ–çš„è§†è§‰æˆ–æ¯«ç±³æ³¢é›·è¾¾ï¼Œè¡¥å¿LiDARåœ¨ç‰¹å®šè§’åº¦ã€è·ç¦»æˆ–å¤©æ°”ï¼ˆå¦‚é›¾ã€å°˜ï¼‰ä¸‹çš„ä¸è¶³ï¼Œæå‡å…¨å¤©å€™ã€å…¨å‘æ£€æµ‹èƒ½åŠ›ã€‚\n    - **æ— æ ‡è¯†ï¼ˆMarker-Freeï¼‰æ£€æµ‹**ï¼šåˆ©ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ç›´æ¥ä»åŸå§‹ç‚¹äº‘ä¸­è¯†åˆ«å¹¶è·Ÿè¸ªæ— äººæœºå½¢çŠ¶ï¼Œæ¶ˆé™¤å¯¹åå°„èƒ¶å¸¦çš„ä¾èµ–ï¼Œæé«˜ç³»ç»Ÿçš„é€šç”¨æ€§å’Œéƒ¨ç½²ä¾¿æ·æ€§ã€‚\n    - **æ‰©å±•å‚ç›´æœºåŠ¨æ€§**ï¼šæ”¹è¿›ç­–ç•¥å¥–åŠ±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ï¼Œé¼“åŠ±é›†ç¾¤åœ¨å®‰å…¨å‰æä¸‹æ›´æœ‰æ•ˆåœ°åˆ©ç”¨ä¸‰ç»´ç©ºé—´ï¼Œä»¥ç¼“è§£ç‹­çª„å‚ç›´FOVå¸¦æ¥çš„çº¦æŸã€‚\n\n### 2. **æå‡ç³»ç»Ÿå¯æ‰©å±•æ€§ä¸é€‚åº”æ€§**\n    - **åˆ†å±‚æˆ–åˆ†ç°‡æ§åˆ¶**ï¼šå¯¹äºå¤§è§„æ¨¡é›†ç¾¤ï¼Œå¼•å…¥åˆ†å±‚ç»„ç»‡æ¶æ„ã€‚å±€éƒ¨å°ç»„å†…éƒ¨ç´§å¯†è·Ÿéšï¼Œå°ç»„ä¹‹é—´ç»´æŒæ¾æ•£è”ç³»ï¼Œä»è€Œé™ä½å…¨å±€åè°ƒçš„å¤æ‚æ€§ã€‚\n    - **é€šä¿¡è¾…åŠ©çš„æ··åˆæ¶æ„**ï¼šåœ¨é€šä¿¡å¯ç”¨ä½†ä¸å¯é çš„åœºæ™¯ä¸­ï¼Œç ”ç©¶**æœ€å°åŒ–é€šä¿¡**çš„ç­–ç•¥ï¼Œä¾‹å¦‚ä»…åœ¨æœ€å…³é”®æ—¶åˆ»ï¼ˆå¦‚å³å°†ä¸¢å¤±é‚»å±…ã€ç©¿è¶Šæç«¯ç‹­çª„é€šé“ï¼‰äº¤æ¢å°‘é‡çŠ¶æ€ä¿¡æ¯ï¼Œä»¥æå‡å¤§è§„æ¨¡é›†ç¾¤çš„é²æ£’æ€§è€Œä¸è¿èƒŒâ€œé€šä¿¡æ‹’æ­¢â€çš„æ ¸å¿ƒå‰æã€‚\n    - **è‡ªé€‚åº”é‚»å±…æ•°é‡**ï¼šä½¿ç­–ç•¥èƒ½åŠ¨æ€è°ƒæ•´å…¶å…³æ³¨çš„é‚»å±…æ•°é‡ï¼Œè€Œä¸æ˜¯å›ºå®šä¸º6ä¸ªï¼Œä»¥æ›´å¥½åœ°é€‚åº”ä¸åŒå¯†åº¦å’Œè§„æ¨¡çš„é›†ç¾¤çŠ¶æ€ã€‚\n\n### 3. **å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„è¿›é˜¶**\n    - **è¯¾ç¨‹å­¦ä¹ ä¸æ›´å¤æ‚çš„ç¯å¢ƒ**ï¼šåœ¨è®­ç»ƒä¸­å¼•å…¥æ¸è¿›å¼æ›´å¤æ‚çš„ç¯å¢ƒï¼ˆå¦‚åŠ¨æ€éšœç¢ã€æ›´å¤æ‚çš„æ£®æ—åœ°è²Œã€é£é›¨æ‰°åŠ¨ï¼‰å’Œæ›´å¤§çš„é›†ç¾¤è§„æ¨¡ï¼Œä»¥æå‡ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚\n    - **å…ƒå­¦ä¹ æˆ–ç»ˆèº«å­¦ä¹ **ï¼šä½¿æ— äººæœºèƒ½å¤Ÿåœ¨éƒ¨ç½²åï¼Œé€šè¿‡å°‘é‡åœ¨çº¿ç»éªŒå¿«é€Ÿé€‚åº”æ–°ç¯å¢ƒæˆ–æ–°çš„é›†ç¾¤é˜Ÿå‹ï¼Œå®ç°æŒç»­å­¦ä¹ ã€‚\n    - **åŠ¨æ€è§’è‰²åˆ†é…**ï¼šç ”ç©¶å¦‚ä½•ä½¿é›†ç¾¤ä¸­çš„ä¸ªä½“èƒ½å¤Ÿ**åŸºäºå±€éƒ¨æ„ŸçŸ¥**ï¼Œè‡ªä¸»æ¶Œç°å‡ºé¢†å¯¼è§’è‰²æˆ–åœ¨é¢†å¯¼è€…å¤±æ•ˆæ—¶æ¥æ›¿é¢†å¯¼ï¼Œå®ç°çœŸæ­£çš„**è‡ªç»„ç»‡**ä¸**å¼¹æ€§**ã€‚\n\n### 4. **æ¢ç´¢æ›´å¤æ‚çš„é›†ä½“è¡Œä¸º**\n    - **ååŒä»»åŠ¡æ‰§è¡Œ**ï¼šè¶…è¶Šå•çº¯çš„é›†ä½“å¯¼èˆªï¼Œç ”ç©¶åœ¨é€šä¿¡æ‹’æ­¢ç¯å¢ƒä¸‹ï¼Œå¦‚ä½•å®ç°**ååŒæœç´¢ã€åŒ…å›´ã€è½½è·æŠ•é€’**ç­‰å¤æ‚ä»»åŠ¡ã€‚è¿™éœ€è¦å°†ç›®æ ‡ä¿¡æ¯æˆ–ä»»åŠ¡çŠ¶æ€ä»¥æŸç§æ–¹å¼ç¼–ç åœ¨é¢†å¯¼è€…çš„è¿åŠ¨æ¨¡å¼ä¸­ï¼Œå¹¶è¢«è·Ÿéšè€…é€šè¿‡æ„ŸçŸ¥è§£ç ã€‚\n    - **å¼‚æ„é›†ç¾¤ååŒ**ï¼šå°†ç³»ç»Ÿæ‰©å±•è‡³åŒ…å«ä¸åŒä¼ æ„Ÿå™¨é…ç½®ã€æœºåŠ¨èƒ½åŠ›æˆ–åŠŸèƒ½çš„å¼‚æ„æ— äººæœºé›†ç¾¤ï¼Œç ”ç©¶å¦‚ä½•å‘æŒ¥å„è‡ªä¼˜åŠ¿å®Œæˆå…±åŒä»»åŠ¡ã€‚\n\n### æ€»ç»“\næœ¬æ–‡çš„å·¥ä½œä¸ºé€šä¿¡æ‹’æ­¢ç¯å¢ƒä¸‹çš„æ— äººæœºé›†ç¾¤å¯¼èˆªå¥ å®šäº†åšå®çš„æŠ€æœ¯åŸºç¡€ã€‚**æœªæ¥çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå¦‚ä½•çªç ´å½“å‰æ„ŸçŸ¥ç‰©ç†é™åˆ¶å’Œå›ºå®šè§’è‰²æ¶æ„ï¼Œå‘æ›´çµæ´»ã€æ›´å¯æ‰©å±•ã€æ›´æ™ºèƒ½çš„è‡ªç»„ç»‡é›†ç¾¤ç³»ç»Ÿæ¼”è¿›**ã€‚è¿™éœ€è¦æ„ŸçŸ¥ã€æ§åˆ¶ã€å­¦ä¹ ç­‰å¤šä¸ªé¢†åŸŸçš„äº¤å‰åˆ›æ–°ã€‚",
            "timestamp": "2026-01-27T16:00:42.058970",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T06:46:09Z",
    "created_at": "2026-01-27T15:53:09.223523",
    "updated_at": "2026-01-27T15:53:09.223530",
    "recommend": 0
}