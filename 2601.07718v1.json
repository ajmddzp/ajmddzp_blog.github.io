{
    "id": "2601.07718v1",
    "title": "Hiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids",
    "authors": [
        "Shaoting Zhu",
        "Ziwen Zhuang",
        "Mengjie Zhao",
        "Kun-Ying Lee",
        "Hang Zhao"
    ],
    "abstract": "åœ¨å¤æ‚éç»“æ„åŒ–ç¯å¢ƒä¸­å®ç°ç¨³å¥çš„äººå½¢æœºå™¨äººå¾’æ­¥è¡Œèµ°ï¼Œéœ€è¦ä»ååº”æ€§æœ¬ä½“æ„ŸçŸ¥è½¬å‘ä¸»åŠ¨æ€§ç¯å¢ƒæ„ŸçŸ¥ã€‚ç„¶è€Œï¼Œå¤–éƒ¨æ„ŸçŸ¥çš„é›†æˆä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼šåŸºäºåœ°å›¾æ„å»ºçš„æ–¹æ³•å­˜åœ¨çŠ¶æ€ä¼°è®¡æ¼‚ç§»é—®é¢˜ï¼Œä¾‹å¦‚åŸºäºæ¿€å…‰é›·è¾¾çš„æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†èº¯å¹²æŠ–åŠ¨ï¼›ç°æœ‰ç«¯åˆ°ç«¯æ–¹æ³•å¸¸å—é™äºå¯æ‰©å±•æ€§ä¸è®­ç»ƒå¤æ‚åº¦ï¼Œç‰¹åˆ«æ˜¯å…ˆå‰é‡‡ç”¨è™šæ‹Ÿéšœç¢ç‰©çš„ç ”ç©¶å¤šä¸ºä¸ªæ¡ˆå®ç°ã€‚æœ¬æ–‡æå‡ºã€Šé‡å¤–å¾’æ­¥ã€‹â€”â€”ä¸€ä¸ªå¯æ‰©å±•çš„ç«¯åˆ°ç«¯è·‘é…·æ„ŸçŸ¥æ¡†æ¶ï¼Œä¸“ä¸ºé²æ£’æ€§äººå½¢å¾’æ­¥è¡Œèµ°è®¾è®¡ã€‚ä¸ºç¡®ä¿å®‰å…¨æ€§ä¸è®­ç»ƒç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¼•å…¥ä¸¤é¡¹å…³é”®æœºåˆ¶ï¼šç»“åˆå¯æ‰©å±•çš„ã€Šåœ°å½¢è¾¹ç¼˜æ£€æµ‹ã€‹ä¸ã€Šè¶³éƒ¨ä½“ç§¯ç‚¹ã€‹çš„ç«‹è¶³ç‚¹å®‰å…¨æœºåˆ¶ï¼Œé˜²æ­¢è¾¹ç¼˜è‡´å‘½æ»‘ç§»ï¼›ä»¥åŠé€šè¿‡ç”Ÿæˆå¯è¡Œå¯¼èˆªç›®æ ‡æ¥ç¼“è§£å¥–åŠ±ç ´è§£çš„ã€Šå¹³å¦åŒºåŸŸé‡‡æ ·ã€‹ç­–ç•¥ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å•é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œç›´æ¥å°†åŸå§‹æ·±åº¦è¾“å…¥ä¸æœ¬ä½“æ„ŸçŸ¥æ˜ å°„ä¸ºå…³èŠ‚åŠ¨ä½œï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨çŠ¶æ€ä¼°è®¡ã€‚åœ¨å…¨å°ºå¯¸äººå½¢æœºå™¨äººä¸Šçš„å¤§é‡å®åœ°å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç­–ç•¥èƒ½ä»¥é«˜è¾¾2.5ç±³/ç§’çš„é€Ÿåº¦ç¨³å¥ç©¿è¶Šå¤æ‚åœ°å½¢ã€‚è®­ç»ƒä¸éƒ¨ç½²ä»£ç å·²å¼€æºï¼Œä»¥ä¿ƒè¿›å¯å¤ç°ç ”ç©¶ï¼Œå¹¶æ”¯æŒå¯¹å®ä½“æœºå™¨äººè¿›è¡Œæœ€å°ç¡¬ä»¶æ”¹è£…çš„éƒ¨ç½²ã€‚",
    "url": "https://arxiv.org/abs/2601.07718v1",
    "html_url": "https://arxiv.org/html/2601.07718v1",
    "html_content": "Hiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids\nShaoting Zhu\n12\n1, Ziwen Zhuang\n12\n1, Mengjie Zhao\n1\n, Kun-Ying Lee\n3\n, Hang Zhao\n12\n2\n1\nIIIS, Tsinghua University,\n2\nShanghai Qi Zhi Institute,\n3\nDepartment of Computer Science and Technology, Tsinghua University\n1Equal contributionâ€ƒ2Corresponding author\nAbstract\nAchieving robust humanoid hiking in complex, unstructured environments requires transitioning from reactive proprioception to proactive perception. However, integrating exteroception remains a significant challenge: mapping-based methods suffer from state estimation drift; for instance, LiDAR-based methods do not handle torso jitter well. Existing end-to-end approaches often struggle with scalability and training complexity; specifically, some previous works using virtual obstacles are implemented case-by-case. In this work, we present\nHiking in the Wild\n, a scalable, end-to-end parkour perceptive framework designed for robust humanoid hiking. To ensure safety and training stability, we introduce two key mechanisms: a foothold safety mechanism combining scalable\nTerrain Edge Detection\nwith\nFoot Volume Points\nto prevent catastrophic slippage on edges, and a\nFlat Patch Sampling\nstrategy that mitigates reward hacking by generating feasible navigation targets. Our approach utilizes a single-stage reinforcement learning scheme, mapping raw depth inputs and proprioception directly to joint actions, without relying on external state estimation. Extensive field experiments on a full-size humanoid demonstrate that our policy enables robust traversal of complex terrains at speeds up to 2.5 m/s. The training and deployment code is open-sourced to facilitate reproducible research and deployment on real robots with minimal hardware modifications.\nI\nIntroduction\nHumanoid robots hold immense promise for traversing complex real-world environments and executing difficult tasks. Unlike their wheeled counterparts, humanoids can step over obstacles and navigate discontinuous terrain. Recently, the field has witnessed significant strides in humanoid control, enabling robots to perform dynamic maneuvers such as dancing, backflips, and mimicking human motions\n[\n22\n,\n24\n,\n27\n]\n. However, there is a fundamental distinction between tracking a predefined motion and hiking in the wild. While tracking resembles memorizing a routine, hiking requires the robot to actively perceive the terrain, adapt to irregularities, and handle the unknown.\nOver the past few years, blind locomotion has established a robust baseline\n[\n13\n,\n34\n,\n33\n,\n38\n]\n. Relying solely on proprioception, these methods are remarkably robust, capable of handling grass or gravel by reacting to contact forces. Yet, the reactive nature of blind locomotion imposes intrinsic limits. Since the robot only responds after a collision, it is vulnerable to significant hazards. Failure to perceive a deep gap or a high step can lead to catastrophic falls. To hike safely, a robot must transition from reactive stability to proactive planning by â€looking ahead.â€\nIntegrating exteroceptive perception into locomotion remains a challenge. Existing approaches largely fall into two categories, each with critical bottlenecks. The first category relies on LiDAR to construct elevation maps (2.5D maps)\n[\n23\n,\n15\n]\nor voxel grids\n[\n3\n]\n, which depend heavily on precise state estimation. In the wild, however, position sensors are prone to drift. Additionally, LiDAR typically has low frequency and suffers from motion distortion, limiting performance in highly dynamic tasks and environments. Other works utilize depth images to reconstruct heightmap\n[\n7\n,\n40\n,\n39\n]\n, which are not scalable on unseen wild environments. They are limited to low-speed movements on simple structures like planes and stairs. Lastly, due to highly customized configurations (e.g. camera position), the code is often not open-sourced, making these methods difficult to scale and reproduce by the community.\nTo address these challenges, we present\nHiking in the Wild\n, a scalable, end-to-end perceptive parkour framework designed for robust humanoid locomotion in unstructured environments. Unlike complex modular pipelines, our approach leverages a single-stage reinforcement learning scheme that maps raw depth inputs and proprioception directly to joint actions. To handle the high dimensionality of visual data and the complexity of diverse terrain skills, we incorporate a Mixture-of-Experts (MoE) architecture\n[\n17\n,\n16\n]\n. Our design allows the policy to process depth inputs at high frequencies (up to 60 Hz). This high-bandwidth perception loop enables the robot to perform dynamic obstacle traversal and high-speed running (up to 2.5 m/s). Crucially, our system incorporates a realistic depth synthesis module that models sensor noise and artifacts during training, enabling zero-shot Sim-to-Real transfer without external localization systems.\nA critical yet often overlooked issue in humanoid RL is the precision of foothold placement. Unlike quadrupeds, humanoids are less stable, and stepping partially on an edge (e.g., the lip of a stair) often leads to catastrophic slippage. Previous model-based planners can generate precise footholds but are fragile to map errors\n[\n18\n]\n. We propose a robust soft-constraint mechanism: a geometric\nTerrain Edge Detector\ncoupled with\nVolume Points\nattached to the robotâ€™s feet. Some previous works using virtual obstacles are not scalable\n[\n50\n,\n4\n]\n. By penalizing the penetration of these volume points with terrain edges during training, the policy implicitly learns to center its feet on safe, flat surfaces, significantly enhancing safety on stairs and gaps without requiring explicit trajectory planning.\nFurthermore, training robust policies for the wild is complicated by the â€reward hackingâ€ phenomenon, where agents tasked with random velocity commands often learn to spin in place rather than traverse difficult terrain\n[\n4\n]\n. Some methods using a goal-based command alleviate this problem, but they lack control over speed of the robot\n[\n47\n,\n48\n]\n. To enforce meaningful exploration, we introduce a\nFlat Patch Sampling\nstrategy. Instead of arbitrary commands, we identify reachable flat regions in the terrain mesh to serve as feasible navigation targets. Velocity commands are then generated based on the relative position of these patches, with randomized speed limits. This curriculum ensures that the agent is always challenged with physically solvable tasks, accelerating convergence and improving directional compliance.\nWe validate our framework on a humanoid robot through extensive field experiments. The robot successfully completed hiking tasks in the wild, robustly traversing stairs, slopes, uneven grassy ground, and discrete gaps. Our system not only demonstrates the ability to navigate previously unseen environments but also achieves high-speed locomotion agility. The contributions of this work are summarized as follows:\n1.\nA scalable, single-stage end-to-end perceptive training and deployment framework, enabling zero-shot Sim-to-Real transfer without external state estimation.\n2.\nA novel safety mechanism using Terrain Edge Detection and Foot Volume Points, alleviating the precise foothold problem in learning-based control.\n3.\nA position-based velocity command generation method leveraging Flat Patch Sampling, eliminating reward hacking and ensuring robust navigation behaviors.\n4.\nDemonstration of robust wild hiking capabilities in a humanoid. The training and deployment code is open-sourced to the community, enabling deployment on real robots with minimal hardware modifications.\nII\nRelated Works\nFigure 1:\nSystem overview. Our framework trains an end-to-end policy using simulated depth and proprioception. To ensure safety and agility on complex terrains, we incorporate Scalable Edge Penalization to avoid risky footholds and Position-based Command generation for precise tracking. The trained policy is directly deployed to the real robot (Zero-shot) using only a 60Â Hz onboard depth camera as exteroception, achieving extremely high-dynamic locomotion without explicit localization or map reconstruction.\nII-A\nLearning-based Legged Locomotion\nLegged locomotion control has shifted from model-based approaches, such as Model Predictive Control (MPC)\n[\n10\n,\n37\n,\n20\n]\n, to data-driven Reinforcement Learning (RL) to better handle unmodeled dynamics. Early blind RL policies achieved impressive robustness on irregular terrains using only proprioception\n[\n34\n,\n38\n,\n13\n,\n46\n]\n. However, these methods lack the exteroception required to navigate large obstacles fast and safely.\nTo enable more agile ability, recent works have integrated exteroceptive perception into the control loop\n[\n4\n,\n35\n]\n. One category of methods relies on LidAR to build elevation maps (2.5D)\n[\n15\n,\n23\n,\n41\n]\nor voxel grids\n[\n3\n]\n. Nevertheless, these approaches depend heavily on precise localization, which limits their update frequency and overall robustness. Furthermore, LiDAR often suffers from motion distortion during fast movement, restricting these methods to low-speed scenarios. Alternatively, depth-image-based methods have been proposed. Some of these operate at low frequency\n[\n50\n]\n, while others employ intermediate modules to predict height maps\n[\n7\n,\n40\n,\n39\n]\n, potentially compromising performance in high-speed or unseen wild environments. In contrast, our method directly utilizes high-frequency depth images as input to train an end-to-end policy, achieving exceptionally high-dynamic behaviors across complex, unstructured terrains in the wild.\nII-B\nPerceptive Foothold Control\nExplicit foothold control typically decouples perception and planning, utilizing terrain representations like elevation maps to solve for optimal placements. Approaches range from heuristic search\n[\n44\n,\n8\n,\n19\n,\n21\n,\n6\n,\n30\n,\n1\n]\nand learned feasibility costs\n[\n26\n,\n45\n,\n28\n,\n43\n]\nto rigorous nonlinear optimization\n[\n12\n,\n29\n]\n. While hybrid frameworks utilizing probabilistic map uncertainty\n[\n19\n,\n9\n]\nor combining pre-planned references with online tracking\n[\n18\n,\n11\n]\nhave achieved impressive agility, they remain brittle to state estimation drift and reconstruction artifacts. Recently, end-to-end policies\n[\n4\n]\nuses edge penalization to implicitly learn safe foothold. However, itâ€™s not scalable to arbitary meshes. Our method takes a trimesh as input and automatically detects the edges, retains the robustness of the implicit paradigm while enforcing safety.\nIII\nMethod\nFigure 2:\nDepth processing: Top row shows\nâ„±\ns\nâ€‹\ni\nâ€‹\nm\n\\mathcal{F}_{sim}\non synthetic data, and bottom row shows\nâ„±\nr\nâ€‹\ne\nâ€‹\na\nâ€‹\nl\n\\mathcal{F}_{real}\non real-world data.\nIII-A\nProblem Formulation\nThe problem of perceptive humanoid locomotion is formulated as a Partially Observable Markov Decision Process (POMDP). We leverage Reinforcement Learning (RL) to optimize the locomotion policy. Specifically, the Proximal Policy Optimization (PPO)\n[\n36\n]\nalgorithmâ€”a robust actor-critic frameworkâ€”is employed for policy training. The key components of our MDP formulation, including the observation space, action space, termination criteria, and reward functions, are defined as follows:\nIII-A\n1\nObservation Space\nThe observation space is formulated to provide the policy with sufficient state information for stable locomotion. Specifically, the actorâ€™s observation comprises both proprioceptive and perceptive signals, including base angular velocity\nğ\nt\nâˆˆ\nâ„\n3\n\\boldsymbol{\\omega}_{t}\\in\\mathbb{R}^{3}\n, projected gravity vector\nğ \nt\nâˆˆ\nâ„\n3\n\\mathbf{g}_{t}\\in\\mathbb{R}^{3}\n, velocity commands\nğœ\nt\nâˆˆ\nâ„\n3\n\\mathbf{c}_{t}\\in\\mathbb{R}^{3}\n, joint positions\nğª\nt\nâˆˆ\nâ„\n29\n\\mathbf{q}_{t}\\in\\mathbb{R}^{29}\n, joint velocities\nğª\nË™\nt\nâˆˆ\nâ„\n29\n\\dot{\\mathbf{q}}_{t}\\in\\mathbb{R}^{29}\n, last action\nğš\nt\nâˆ’\n1\nâˆˆ\nâ„\n29\n\\mathbf{a}_{t-1}\\in\\mathbb{R}^{29}\n, and depth images\nğˆ\nt\nâˆˆ\nâ„\nW\nÃ—\nH\n\\mathbf{I}_{t}\\in\\mathbb{R}^{W\\times H}\n. To capture temporal dependencies, we employ a sliding window of history consisting of\nh\nh\nsteps. During training, the actorâ€™s input is subject to stochastic noise to improve robustness and bridge the sim-to-real gap. The comprehensive actor observation\nğ¨\nt\na\n\\mathbf{o}_{t}^{a}\nis defined as:\nğ¨\nt\na\n=\n{\n(\nğ\ni\n,\nğ \ni\n,\nğœ\ni\n,\nğª\ni\n,\nğª\nË™\ni\n,\nğš\ni\nâˆ’\n1\n)\n}\ni\n=\nt\nâˆ’\nh\n+\n1\nt\n+\nâ„‹\nt\n\\mathbf{o}_{t}^{a}=\\left\\{\\left(\\boldsymbol{\\omega}_{i},\\mathbf{g}_{i},\\mathbf{c}_{i},\\mathbf{q}_{i},\\dot{\\mathbf{q}}_{i},\\mathbf{a}_{i-1}\\right)\\right\\}_{i=t-h+1}^{t}+\\mathcal{H}_{t}\n(1)\nwhere\nâ„‹\nt\n\\mathcal{H}_{t}\nis the historical input sequence of depth images. We adopt an asymmetric actor-critic architecture to facilitate training in simulation. The criticâ€™s observation\nğ¨\nt\nc\n\\mathbf{o}_{t}^{c}\nincludes all noise-free actor observations, combined with base linear velocity\nğ¯\nt\nâˆˆ\nâ„\n3\n\\mathbf{v}_{t}\\in\\mathbb{R}^{3}\n.\nIII-A\n2\nAction Space\nThe policy outputs the target joint positions\nğš\nt\nâˆˆ\nâ„\n29\n\\mathbf{a}_{t}\\in\\mathbb{R}^{29}\n. Then, the joint torques\nğ‰\nt\n\\boldsymbol{\\tau}_{t}\nare computed via PD control:\nğ‰\nt\n=\nk\np\nâ€‹\n(\nğš\nt\nâˆ’\nğª\nt\n)\nâˆ’\nk\nd\nâ€‹\nğª\nË™\nt\n\\boldsymbol{\\tau}_{t}=k_{p}(\\mathbf{a}_{t}-\\mathbf{q}_{t})-k_{d}\\dot{\\mathbf{q}}_{t}\n(2)\nThe specific PD gain values are adopted from\nBeyondMimic\n[\n22\n]\n. These computed torques are then applied to the robotâ€™s actuators to execute the desired motion.\nIII-A\n3\nTermination Criteria\nAn episode is terminated upon (i) time-out, (ii) exceeding terrain boundaries, (iii) illegal torso contact, (iv) unstable orientation, or (v) insufficient root height. These constraints prevent the exploration of physically unfeasible states and accelerate training convergence.\nIII-A\n4\nReward Functions\nThe total reward\nR\nR\nis defined as the sum of four primary components: task, regularization, safety, and AMP-style rewards. Formally,\nR\n=\nr\ntask\n+\nr\nreg\n+\nr\nsafe\n+\nr\namp\nR=r_{\\text{task}}+r_{\\text{reg}}+r_{\\text{safe}}+r_{\\text{amp}}\n. These terms respectively facilitate command tracking, energy efficiency, constraint satisfaction, and natural locomotion styles.\nIII-B\nEgo-centric Depth Simulation\nIII-B\n1\nEfficient Depth Synthesis via Parallelized Ray-casting\nTo synthesize high-fidelity depth observations, we leverage the NVIDIA Warp framework\n[\n25\n]\nto implement a GPU-accelerated ray-caster. Given the cameraâ€™s extrinsic parametersâ€”comprising the optical center position\nğ©\nc\nâˆˆ\nâ„\n3\n\\mathbf{p}_{c}\\in\\mathbb{R}^{3}\nand orientation matrix\nğ‘\nc\nâˆˆ\nS\nâ€‹\nO\nâ€‹\n(\n3\n)\n\\mathbf{R}_{c}\\in SO(3)\nâ€”we cast a set of rays corresponding to the cameraâ€™s intrinsic manifold.\nFor each pixel\n(\ni\n,\nj\n)\n(i,j)\nin the image plane, a ray is emitted in the direction\nğ¯\ni\n,\nj\n\\mathbf{v}_{i,j}\n. The ray-caster computes the radial distance\nd\ni\n,\nj\nd_{i,j}\nby identifying the first intersection point between the ray and the scene geometry\nğ’¢\n\\mathcal{G}\n, which includes both the terrain environment and the robotâ€™s visual meshes:\nd\ni\n,\nj\n=\nmin\nâ¡\n{\nÏ„\nâˆ£\nğ©\nc\n+\nÏ„\nâ€‹\nğ¯\ni\n,\nj\nâˆ©\nğ’¢\nâ‰ \nâˆ…\n}\nd_{i,j}=\\min\\{\\tau\\mid\\mathbf{p}_{c}+\\tau\\mathbf{v}_{i,j}\\cap\\mathcal{G}\\neq\\emptyset\\}\nTo accurately emulate the output of physical RGB-D sensors, we transform the radial distance\nd\ni\n,\nj\nd_{i,j}\ninto the orthogonal depth\nz\ni\n,\nj\nz_{i,j}\n. This is achieved by projecting the distance onto the cameraâ€™s principal axis\nğ§\nc\n\\mathbf{n}_{c}\n:\nz\ni\n,\nj\n=\nd\ni\n,\nj\nâ‹…\n(\nğ¯\ni\n,\nj\nâ‹…\nğ§\nc\n)\nz_{i,j}=d_{i,j}\\cdot(\\mathbf{v}_{i,j}\\cdot\\mathbf{n}_{c})\nwhere\nğ¯\ni\n,\nj\n\\mathbf{v}_{i,j}\nis the unit ray direction and\nğ§\nc\n\\mathbf{n}_{c}\nis the cameraâ€™s forward-facing unit vector. This parallelized approach ensures real-time synthesis of dense depth maps within the simulation loop.\nIII-B\n2\nBidirectional Depth Alignment and Noise Modeling\nTo minimize the sim-to-real gap, we define two transformation pipelines,\nâ„±\ns\nâ€‹\ni\nâ€‹\nm\n\\mathcal{F}_{sim}\nand\nâ„±\nr\nâ€‹\ne\nâ€‹\na\nâ€‹\nl\n\\mathcal{F}_{real}\n, which map raw depth observations from their respective domains into a unified perception space\nğ’ª\n\\mathcal{O}\n. Our objective is to ensure that the processed distributions are harmonized, such that\nP\nâ€‹\n(\nâ„±\ns\nâ€‹\ni\nâ€‹\nm\nâ€‹\n(\nd\ns\nâ€‹\ni\nâ€‹\nm\n)\n)\nâ‰ˆ\nP\nâ€‹\n(\nâ„±\nr\nâ€‹\ne\nâ€‹\na\nâ€‹\nl\nâ€‹\n(\nd\nr\nâ€‹\ne\nâ€‹\na\nâ€‹\nl\n)\n)\nP(\\mathcal{F}_{sim}(d_{sim}))\\approx P(\\mathcal{F}_{real}(d_{real}))\n, where\nd\nâˆˆ\nâ„\nH\nÃ—\nW\nd\\in\\mathbb{R}^{H\\times W}\ndenotes the raw depth map.\nSimulation Pipeline\nâ„±\ns\nâ€‹\ni\nâ€‹\nm\n\\mathcal{F}_{sim}\nThe simulation pipeline degrades ideal depth to emulate physical sensor limitations through the following sequential stochastic operations:\n1.\nCrop and Resize\n: The raw depth map is cropped and rescaled to the target resolution to focus on the key features at the center of the image.\n2.\nRange-dependent Gaussian Noise\n: To account for precision decay, additive Gaussian noise\nÏµ\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nÏƒ\n2\n)\n\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2})\nis injected into pixels within a valid sensing range\n[\nd\nm\nâ€‹\ni\nâ€‹\nn\n,\nd\nm\nâ€‹\na\nâ€‹\nx\n]\n[d_{min},d_{max}]\n. The perturbed depth\nz\ni\n,\nj\nâ€²\nz^{\\prime}_{i,j}\nis defined as:\nz\ni\n,\nj\nâ€²\n=\n{\nz\ni\n,\nj\n+\nÏµ\n,\nif\nâ€‹\nz\ni\n,\nj\nâˆˆ\n[\nd\nm\nâ€‹\ni\nâ€‹\nn\n,\nd\nm\nâ€‹\na\nâ€‹\nx\n]\nz\ni\n,\nj\n,\notherwise\nz^{\\prime}_{i,j}=\\begin{cases}z_{i,j}+\\epsilon,&\\text{if }z_{i,j}\\in[d_{min},d_{max}]\\\\\nz_{i,j},&\\text{otherwise}\\end{cases}\n3.\nDisparity Artifact Synthesis\n: To simulate binocular matching failures (e.g., in over-exposed or textureless regions), we mask contiguous pixels as invalid â€white regionsâ€ using structural masks.\n4.\nGaussian Blur\n: A Gaussian blur kernel\nğŠ\n\\mathbf{K}\nis convolved with the image to simulate optical motion blur.\n5.\nClip and Normalization\nValues are clipped and normalized to\n[\n0\n,\n1\n]\n[0,1]\n.\n6.\nOut-of-Distribution (OOD) Perturbation\n: To enhance robustness against brief obstructions or temporary sensor glitches, we introduce a Bernoulli-distributed dropout. With a probability\nP\no\nâ€‹\no\nâ€‹\nd\nP_{ood}\n, the entire observation is replaced by random gaussian noise, forcing the policy to handle transient perceptual failures.\nReal-world Pipeline\nâ„±\nr\nâ€‹\ne\nâ€‹\na\nâ€‹\nl\n\\mathcal{F}_{real}\nDuring deployment, the physical depth stream is refined to match the characteristics of the trained policyâ€™s input space:\n1.\nCrop and Resize\n: The raw stream is cropped and resized to ensure the input manifold is consistent with the simulation geometry.\n2.\nDepth Inpainting\n: Physical sensors often exhibit â€black regionsâ€ (zero-valued pixels) due to occlusion or disparity shadows. We apply a spatial inpainting operator to recover these missing depth values.\n3.\nGaussian Blur\n: A Gaussian blur is applied to suppress high-frequency sensor jitter and electronic noise.\nIII-B\n3\nTemporal Depth Aggregation via Strided Sampling\nTo enhance policy robustness, particularly during high-speed locomotion where rapid terrain changes necessitate a broader temporal context, we incorporate a long-term history of depth observations. Unlike proprioceptive states that typically utilize a dense history of the last\nn\nn\nconsecutive steps, the depth input employs a strided temporal sampling strategy to balance the look-back window with computational efficiency.\nSpecifically, we define a history buffer consisting of\nm\nm\nframes sampled with a temporal stride\nâ„“\n\\ell\n. Furthermore, a single-frame delay is introduced during training to emulate physical sensor latency. Let\nI\nt\nI_{t}\ndenote the depth image at current time step\nt\nt\n. The historical input sequence\nâ„‹\nt\n\\mathcal{H}_{t}\nis formulated as:\nâ„‹\nt\n=\n{\nI\nt\nâˆ’\nk\nâ‹…\nâ„“\nâˆ£\nk\n=\n0\n,\n1\n,\nâ€¦\n,\nm\nâˆ’\n1\n}\n\\mathcal{H}_{t}=\\{I_{t-k\\cdot\\ell}\\mid k=0,1,\\dots,m-1\\}\nwhere\nk\nk\nis the frame index. This configuration allows the policy to perceive a total temporal horizon of\n(\nm\nâˆ’\n1\n)\nâ‹…\nâ„“\n(m-1)\\cdot\\ell\nsteps while only processing\nm\nm\ndiscrete frames.\nBy utilizing this sparse yet extended temporal representation, the agent can effectively capture the trend of the terrain profile and the robotâ€™s relative velocity without the redundant information overlap inherent in consecutive high-frequency frames. This strided history proves critical for anticipating obstacles and adjusting gait during high-speed maneuvers.\nIII-C\nTerrain Edge Contact Penalization\nDuring training, the robot tends to step close to the edge to minimize energy usage\n[\n4\n]\n. However, Stepping only partially on terrain edges, such as the edge of a stair, can make the robot unstable and lead to falls. To improve safety, we penalize the robotâ€™s feet contacts near terrain edges. First, we use an edge detector to find the boundaries of the terrain mesh. Second, we attach a set of â€volume pointsâ€ to the robotâ€™s feet. By penalizing these points when they penetrate the edge, the policy learns to select more stable, centered foot positions rather than risky ones.\nIII-C\n1\nTerrain Edge Detector\nFirst, we identify sharp terrain edges by comparing the dihedral angle between adjacent faces to a predefined threshold\nÏ„\n\\tau\n. To enhance computational efficiency, the resulting raw edges are further processed to filter noise and concatenate short segments. This procedure is summarized in\nalgorithmÂ 1\n.\nInput :\nTriangular Mesh\nâ„³\n=\n(\nğ’±\n,\nâ„±\n)\n\\mathcal{M}=(\\mathcal{V},\\mathcal{F})\n,\nSharpness Threshold\nÏ„\n\\tau\n,\nCylinder Radius\nr\nr\n,\nGrid Resolution\nN\ngrid\nN_{\\text{grid}}\nOutput :\nSpatial Collision Grid\nğ’®\n\\mathcal{S}\n1\n2\n1ex\n/*\n1. Sharp Edge Detection  */\n3\nLet\nğ’œ\n\\mathcal{A}\nbe the set of face adjacencies in\nâ„³\n\\mathcal{M}\n;\n4\nInitialize raw edge set\nE\nraw\nâ†\nâˆ…\nE_{\\text{raw}}\\leftarrow\\emptyset\n;\n5\nforeach\nadjacency\na\nâˆˆ\nğ’œ\na\\in\\mathcal{A}\ndo\n6\nLet\nÎ±\na\n\\alpha_{a}\nbe the dihedral angle of\na\na\n;\n7\nif\nÎ±\na\n>\nÏ„\n\\alpha_{a}>\\tau\nthen\n8\nLet\n(\nv\ni\n,\nv\nj\n)\n(v_{i},v_{j})\nbe the vertex indices shared by\na\na\n;\n9\nAdd segment coordinate\n(\nğ’±\nâ€‹\n[\nv\ni\n]\n,\nğ’±\nâ€‹\n[\nv\nj\n]\n)\n(\\mathcal{V}[v_{i}],\\mathcal{V}[v_{j}])\nto\nE\nraw\nE_{\\text{raw}}\n;\n10\n11\n12\n1ex\n/*\n2. Edge Processing  */\n13\nif\nE\nraw\nâ€‹\nis empty\nE_{\\text{raw}}\\text{ is empty}\nthen\n14\nreturn\nNone\n;\n15\n16\nelse\n17\nE\nfinal\nâ†\nProcessEdges\nâ€‹\n(\nE\nraw\n)\nE_{\\text{final}}\\leftarrow\\textnormal{{ProcessEdges}}(E_{\\text{raw}})\n;\n18\n19\n20\n1ex\n/*\n3. Spatial Structure Construction  */\n21\nInitialize cylinder set\nğ’\nâ†\nâˆ…\n\\mathcal{C}\\leftarrow\\emptyset\n;\n22\nforeach\nsegment\ne\nâˆˆ\nE\nfinal\ne\\in E_{\\text{final}}\ndo\n23\nConstruct cylinder\nc\nc\nfrom segment\ne\ne\nwith radius\nr\nr\n;\n24\nAdd\nc\nc\nto\nğ’\n\\mathcal{C}\n;\n25\n26\n27\nğ’®\nâ†\nCylinderSpatialGrid\nâ€‹\n(\nğ’\n,\nN\ngrid\n)\n\\mathcal{S}\\leftarrow\\textnormal{{CylinderSpatialGrid}}(\\mathcal{C},N_{\\text{grid}})\n;\n28\n29\nreturn\nğ’®\n\\mathcal{S}\n;\nAlgorithmÂ 1\nTerrain Edge Detection Algorithm\nThe edge processing step employs a greedy concatenation strategy to merge fragmented segments and reduce the total number of primitives.\nFigure 3:\nAutomatically detected edges across diverse terrains.\nIII-C\n2\nVolumetric Point Penetration Penalization\nFigure 4:\nVolume points distributed within the foot manifold.\nTo accurately monitor the contact state relative to the terrain edges, we distribute a set of volume points\nğ’«\n\\mathcal{P}\nwithin each footâ€™s collision manifold, as illustrated in\nFigureÂ 4\n. We leverage NVIDIA Warp\n[\n25\n]\nto perform massively parallel distance queries, computing the penetration depth of each point relative to the previously constructed spatial collision grid\nğ’®\n\\mathcal{S}\n.\nTo discourage unstable foot-ground interactions near terrain edges, we define a penalty term that considers both the geometric penetration and the dynamic state of the foot following\n[\n49\n]\n. For each point\ni\nâˆˆ\nğ’«\ni\\in\\mathcal{P}\n, let\nğ\ni\n\\mathbf{d}_{i}\nbe its penetration offset and\nğ¯\ni\n\\mathbf{v}_{i}\nbe its linear velocity in the world frame. The penalization reward\nr\nv\nâ€‹\no\nâ€‹\nl\nr_{vol}\nis formulated as:\nr\nv\nâ€‹\no\nâ€‹\nl\n=\nâˆ’\nâˆ‘\ni\n=\n1\n|\nğ’«\n|\nâ€–\nğ\ni\nâ€–\nâ‹…\n(\nâ€–\nğ¯\ni\nâ€–\n+\nÏµ\n)\nr_{vol}=-\\sum_{i=1}^{|\\mathcal{P}|}\\|\\mathbf{d}_{i}\\|\\cdot(\\|\\mathbf{v}_{i}\\|+\\epsilon)\n(3)\nwhere\nÏµ\n=\n10\nâˆ’\n3\n\\epsilon=10^{-3}\nis a small constant for numerical stability. This formulation ensures that the policy is penalized more heavily for high-velocity impacts or scraping motions near terrain edges, thereby encouraging the robot to seek stable footholds.\nIII-D\nPosition-based Velocity Command\nDuring training, conventional velocity commands that are sampled uniformly often lead to â€reward hacking,â€ where the robot turns in circles to collect rewards instead of actually crossing obstacles\n[\n4\n]\n. Previous research has attempted to solve this by modifying velocity tracking rewards\n[\n4\n,\n5\n]\nor using goal-based commands\n[\n47\n,\n48\n]\n. However, relying only on reward tuning with randomly sampled commands makes it difficult to reach maximum performance. Furthermore, pure goal commands often lack control over the robotâ€™s speed, and randomly sampled goals may not provide suitable targets. To address these issues, we introduce a new method that generates targets using â€flat patchesâ€ and creates specific velocity commands to improve training performance.\nIII-D\n1\nTarget Generation via Flat Patch\nFollowing the approach in IsaacLab\n[\n31\n]\n, we identify â€flat patchesâ€ on the terrain mesh to serve as reachable navigation targets. A location is considered a valid target if the terrain within a radius\nr\nr\nis sufficiently level. Specifically, we sample potential locations and use ray-casting to check the height difference of the surrounding terrain. A patch is accepted only if the maximum height difference is below a threshold\nÎ´\n\\delta\n. This ensures that targets are placed on stable ground rather than steep slopes or unreachable areas. An example can be found in\nFigureÂ 5\n.\nInput :\nMesh\nâ„³\n\\mathcal{M}\n, Patch radius\nr\nr\n, Max height difference\nÎ´\n\\delta\nOutput :\nSet of valid targets\nğ’«\n\\mathcal{P}\n1\n2\n1ex\nwhile\n|\nğ’«\n|\n<\nN\nt\nâ€‹\na\nâ€‹\nr\nâ€‹\ng\nâ€‹\ne\nâ€‹\nt\nâ€‹\ns\n|\\mathcal{P}|<N_{targets}\ndo\n3\n1. Sample a random 2D position\n(\nx\n,\ny\n)\n(x,y)\nin the environment;\n4\n2. Ray-cast to get a set of heights\nH\nH\nwithin radius\nr\nr\naround\n(\nx\n,\ny\n)\n(x,y)\n;\n5\n1ex\nif\nmax\nâ¡\n(\nH\n)\nâˆ’\nmin\nâ¡\n(\nH\n)\n<\nÎ´\n\\max(H)-\\min(H)<\\delta\nthen\n6\nAdd\n(\nx\n,\ny\n,\navg\nâ€‹\n(\nH\n)\n)\n(x,y,\\text{avg}(H))\nto\nğ’«\n\\mathcal{P}\n;\n7\nreturn\nğ’«\n\\mathcal{P}\n;\nAlgorithmÂ 2\nFlat Patch Sampling\nFigure 5:\nFlat patches on different terrains.\nIII-D\n2\nPosition-based Velocity Command Generation\nAfter generating the flat patches, we periodically select one as the navigation target. The robotâ€™s velocity commands are then generated based on the relative position of this target. Let\nğ©\nt\nâ€‹\na\nâ€‹\nr\nâ€‹\ng\nâ€‹\ne\nâ€‹\nt\nB\n=\n[\nx\ng\n,\ny\ng\n]\nT\n\\mathbf{p}_{target}^{B}=[x_{g},y_{g}]^{T}\nrepresent the target position in the robotâ€™s base frame. The desired linear velocity\nv\nx\nv_{x}\nand angular velocity\nÏ‰\nz\n\\omega_{z}\nare calculated as follows:\nv\nx\n\\displaystyle v_{x}\n=\nclip\nâ€‹\n(\nk\nv\nâ‹…\nx\ng\n,\n0\n,\nv\nm\nâ€‹\na\nâ€‹\nx\n)\n\\displaystyle=\\text{clip}(k_{v}\\cdot x_{g},0,v_{max})\n(4)\nÏ‰\nz\n\\displaystyle\\omega_{z}\n=\nclip\nâ€‹\n(\nk\nÏ‰\nâ‹…\natan2\nâ€‹\n(\ny\ng\n,\nx\ng\n)\n,\nâˆ’\nÏ‰\nm\nâ€‹\na\nâ€‹\nx\n,\nÏ‰\nm\nâ€‹\na\nâ€‹\nx\n)\n\\displaystyle=\\text{clip}(k_{\\omega}\\cdot\\text{atan2}(y_{g},x_{g}),-\\omega_{max},\\omega_{max})\n(5)\nwhere\nk\nv\nk_{v}\nand\nk\nÏ‰\nk_{\\omega}\nare the linear and angular stiffness (gain) coefficients. Because we use the forward camera on robotâ€™s head link, we only focus on forward locomotion and heading alignment; therefore, the lateral velocity command\nv\ny\nv_{y}\nis set to zero. The velocity limits\n(\nv\nm\nâ€‹\na\nâ€‹\nx\n,\nÏ‰\nm\nâ€‹\na\nâ€‹\nx\n)\n(v_{max},\\omega_{max})\nare adaptively adjusted based on the category of the terrain.\nHowever, relying solely on these position-based commands can limit the robotâ€™s ability to learn in-place turning, as the target is usually far away. To address this, we assign a small subset of agents on flat terrain to receive pure turning commands. For these agents, we set\nv\nx\n=\n0\nv_{x}=0\nand provide a random\nÏ‰\nz\n\\omega_{z}\n, forcing the policy to learn effective in-place rotation maneuvers. This combination improves the overall maneuverability of the robot across different environments.\nIII-E\nPolicy Training with Adversarial Motion Priors\nFollowing previous works\n[\n42\n,\n40\n]\n, we use the Adversarial Motion Priors (AMP) framework\n[\n32\n]\nto improve the robotâ€™s gait style and overall locomotion ability. Our reference dataset\nğ’Ÿ\n\\mathcal{D}\nis collected at a frequency of\nf\n=\n50\nâ€‹\nHz\nf=50\\,\\text{Hz}\nfrom three primary sources:\n1.\nSynthetic Data\n: Walking patterns generated by a Model Predictive Control (MPC) controller\n[\n10\n]\nto provide stable movement.\n2.\nHuman Motion\n: High-quality human data captured via the NOKOV motion capture system.\n3.\nRunning Motion\n: High-speed running motions selected from the LAFAN dataset\n[\n14\n]\n.\nThe human motions specifically include challenging tasks such as climbing onto/off high platforms and ascending/descending stairs. We use GMR\n[\n2\n]\nto retarget these human trajectories into robot motions.\nTo avoid the â€œmode collapseâ€ problem, we train the walking and running policies separately using different datasets. The walking dataset\nğ’Ÿ\nwalk\n\\mathcal{D}_{\\text{walk}}\ncombines sources 1 and 2, totaling\nT\n=\n379.62\nâ€‹\ns\nT=379.62\\,\\text{s}\nof motion data. The running dataset\nğ’Ÿ\nrun\n\\mathcal{D}_{\\text{run}}\nconsists of source 3, with a total duration of\nT\n=\n1.54\nâ€‹\ns\nT=1.54\\,\\text{s}\n. This multi-source approach allows the policy to learn both the stability of MPC and the natural agility of human-like movement.\nUnlike previous works that only use a single state pair\n(\nğ¬\nt\n,\nğ¬\nt\n+\n1\n)\n(\\mathbf{s}_{t},\\mathbf{s}_{t+1})\nas the transition for the discriminator, we use a short sequence of past states to better capture the motionâ€™s temporal features. The transition is defined as\n(\nğ’\nt\n,\nğ’\nt\n+\n1\n)\n(\\mathbf{S}_{t},\\mathbf{S}_{t+1})\n, where\nğ’\nt\n=\n[\nğ¬\nt\nâˆ’\nn\n,\nâ€¦\n,\nğ¬\nt\n]\n\\mathbf{S}_{t}=[\\mathbf{s}_{t-n},\\dots,\\mathbf{s}_{t}]\nrepresents a history of\nn\nn\nframes.\nğ¬\nt\n=\n{\n(\nğ¯\nt\n,\nğ\nt\n,\nğ \nt\n,\nğª\nt\n,\nğª\nË™\nt\n)\n}\n\\mathbf{s}_{t}=\\left\\{\\left(\\mathbf{v}_{t},\\boldsymbol{\\omega}_{t},\\mathbf{g}_{t},\\mathbf{q}_{t},\\dot{\\mathbf{q}}_{t}\\right)\\right\\}\n(6)\nTo increase the diversity of the motion data, we also apply symmetric augmentation to the dataset. Following the AMP framework, the discriminator\nD\nâ€‹\n(\nğ’\n)\nD(\\mathbf{S})\nis trained using a least-squares (MSE) loss:\nL\nD\n=\nğ”¼\nâ„³\nâ€‹\n[\n(\nD\nâ€‹\n(\nğ’\n)\nâˆ’\n1\n)\n2\n]\n+\nğ”¼\nğ’«\nâ€‹\n[\n(\nD\nâ€‹\n(\nğ’\n)\n+\n1\n)\n2\n]\nL_{D}=\\mathbb{E}_{\\mathcal{M}}[(D(\\mathbf{S})-1)^{2}]+\\mathbb{E}_{\\mathcal{P}}[(D(\\mathbf{S})+1)^{2}]\n(7)\nwhere\nâ„³\n\\mathcal{M}\nrepresents the reference motion dataset and\nğ’«\n\\mathcal{P}\nrepresents the motions produced by the current policy. To ensure training stability, we incorporate gradient penalty and weight decay. The style-reward\nr\nt\nr_{t}\nprovided by the discriminator to the policy is calculated as:\nr\nt\n=\nmax\nâ¡\n[\n0\n,\n1\nâˆ’\n0.25\nâ€‹\n(\nD\nâ€‹\n(\nğ’\nt\n)\nâˆ’\n1\n)\n2\n]\nr_{t}=\\max\\left[0,1-0.25(D(\\mathbf{S}_{t})-1)^{2}\\right]\n(8)\nCompared to the binary cross-entropy loss and log-based rewards, the combination of MSE loss and quadratic rewards provides smoother, non-saturating gradients that prevent the vanishing gradient problem and ensure more stable convergence toward the reference motion manifold.\nIV\nExperiments\nIn this section, we evaluate the robustness and performance of our framework through extensive experiments in both simulation and real-world environments. We test the policy across various challenging terrains, including stairs, high platforms, grassy ramps, and discrete gaps, using both walking and running gaits. Our evaluation aims to answer the following three questions:\nâ€¢\nQ1\n: Does our framework enable efficient training and reliable zero-shot deployment on physical hardware?\nâ€¢\nQ2\n: Does the edge-aware penalization mechanism improve foothold safety and stability when traversing discrete terrain features? Is it scalable to new terrains without any extra design?\nâ€¢\nQ3\n: What are the individual contributions of the key design components (e.g., perception, AMP, or command generation) to the robotâ€™s performance?\nIV-A\nExperiment Configurations\nTraining Environment:\nWe train our policies using NVIDIA Isaac Sim and Isaac Lab\n[\n31\n]\n. All training sessions are conducted on an NVIDIA RTX 4090 GPU. We utilize the 29-DoF Unitree G1 humanoid robot for both simulation training and physical deployment. The history length for prioception and depth image is 8 frames.\nHardware and Perception:\nFor real-world deployment, we utilize the factory-integrated Intel RealSense D435i depth camera that comes standard with the Unitree G1 without any hardware modification. Depth images are captured at 60Â Hz with a raw resolution of\n480\nÃ—\n270\n480\\times 270\n, which are then downsampled to\n64\nÃ—\n36\n64\\times 36\nand cropped to a final input size of\n36\nÃ—\n18\n36\\times 18\nfor the policy.\nOnboard Deployment:\nThe policy operates at a frequency of 50Â Hz on the robotâ€™s onboard NVIDIA Jetson Orin NX. We utilize\nonnxruntime\nfor efficient policy inference. To ensure low-latency performance, depth acquisition and image processing are handled in a dedicated asynchronous process at 60Â Hz. The base policy is post-trained for high-speed running and some specific terrains.\nIV-B\nReal-world Deployment\nFigure 6:\nSnapshots of the humanoid robot traversing diverse indoor and outdoor environments via zero-shot transfer: (a) running ascent onto a high platform; (b) running descent from a high platform; (c) running on a grassy slope; (d) stair ascent; (e) stair descent; (f) traversing a deep gap.\nWe highly recommend watching videos in the supplementary materials for better demonstration.\nWe evaluate our framework by deploying the trained policies directly onto the humanoid robot via zero-shot sim-to-real transfer. As illustrated in\nFigureÂ 6\n, the robot successfully performs various locomotion primitives across challenging terrains. Notably, the robot achieves a maximum running speed of\n2.5\nâ€‹\nm/s\n2.5~\\text{m/s}\nand successfully traversed difficult obstacles, including high platforms up to\n32\nâ€‹\ncm\n32~\\text{cm}\nand discrete gaps with a width of\n50\nâ€‹\ncm\n50~\\text{cm}\n.\nCrucially, the high-frequency depth perception (60Â Hz) is the key enabler for these high-dynamic tasks, such as running onto a high platform. The low-latency environmental feedback allows the policy to make rapid adjustments to the robotâ€™s posture and terrain changes in real-time.\nTo quantify the systemâ€™s reliability, we conduct 10 trials for each combination of terrain and gait. The resulting success rates are summarized in\nFigureÂ 7\n. Our policy maintains a high success rate across nearly all tested scenarios, demonstrating remarkable robustness to real-world sensory noise and physical discrepancies.\nFigure 7:\nSuccess rates across different terrains and modes.\nFurthermore, we conduct a long duration test to evaluate the stability of the system over a prolonged period. The robot is tasked with continuous locomotion across multiple staircases and flat surfaces. The robot successfully maintains its balance and walks for 4Â minutes without any falls or human intervention. This sustained performance highlights the effectiveness of our control framework and its potential for deployment in complex, long-range real-world tasks.\nIV-C\nEdge-aware Penalization Mechanism\nAs introduced in\nsubsection\nIII-C\n, we utilize the\nTerrain Edge Detector\nand\nVolumetric Point Penetration Penalization\nto ensure foothold safety. As illustrated in\nFigureÂ 8\n, with edge-aware penalization enabled, the robot tends to seek higher safety margins, effectively placing its feet away from terrain edges to maintain stable contact.\nFigure 8:\nVisualization of foothold placement.\nTo quantitatively evaluate this effect, we measure the\nSuccess Rate\nand\nMean Feet Landing Area Percentage\nin simulation across various challenging terrains. Landing Area quantifies the portion of the foot manifold that is on the terrain surface. We test each policy for 10,000 time-steps with 1,000 robots.\nTABLE I:\nQuantitative comparison Results.\nSuccess Rate (SR)\nâ†‘\n\\uparrow\nLanding Area %\nâ†‘\n\\uparrow\nTerrain Type\nOurs\nNo Edge\nOurs\nNo Edge\nStair Ascent\n100.00%\n100.00%\n0.99\n0.98\nStair Descent\n99.95%\n99.82%\n0.94\n0.87\nDeep Gap\n100.0%\n99.94%\n0.96\n0.94\nSmall Box\n99.09%\n93.17%\n0.96\n0.95\nAs shown in\nTableÂ I\n, our method outperforms\nNo Edge\nin both success rate and landing area. It should be noted that the mean landing area values are generally high because the robot spends a considerable portion of each episode on flat ground; thus, the observed gap indicates a substantial improvement specifically in edge-dense areas.\nFigure 9:\nThe robot slips and falls after stepping on a stair edge.\nIn the real world, stepping on edges sometimes causes slippage or unpredictable contact dynamics, driving the robot into Out-of-Distribution (OOD) states that lead to immediate falls, as shown in\nFigureÂ 9\n. By incentivizing the policy to maximize the landing area and avoid edges, our method significantly reduces these real-world risks, ensuring greater robustness during physical deployment. As shown in\nFigureÂ 6\n, our policy exhibits redundant safety margins and maintains stable footholds throughout the traverse.\nTo further demonstrate the scalability of our framework, we evaluate the edge detector on two new terrain types:\nStones\nand\nStakes\n. As illustrated in\nFigureÂ 10\n, the detector precisely identifies terrain edges without any manual feature engineering or parameter tuning. This robust zero-shot performance confirms that our edge-aware mechanism can scale to arbitrary terrain topologies.\nFigure 10:\nGeneralization of edge detector to new terrains.\nTABLE II:\nComprehensive ablation results across all terrain types and configurations.\nSuccess Rate (%)\nâ†‘\n\\uparrow\nMean Reaching Time (s)\nâ†“\n\\downarrow\nMethod\nLarge Box\nRamp\nSmall Box\nRough\nStair down\nPlatform down\nStair up\nPlatform up\nGap\nLarge Box\nRamp\nSmall Box\nRough\nStair down\nPlatform down\nStair up\nPlatform up\nGap\nOurs\n100.0\n100.0\n99.09\n100.0\n99.95\n100.0\n100.0\n100.0\n100.0\n5.50\n5.84\n8.03\n5.40\n4.45\n5.48\n5.35\n5.45\n5.43\nw/o depth history\n100.0\n100.0\n1.66\n100.0\n99.49\n99.70\n99.93\n100.0\n100.0\n5.90\n6.72\n11.68\n5.78\n5.54\n5.82\n6.98\n5.91\n5.79\nw/o pose-based\n100.0\n100.0\n90.68\n100.0\n100.0\n100.0\n99.74\n100.0\n100.0\n5.90\n5.74\n11.32\n5.72\n6.02\n5.82\n6.39\n6.34\n5.70\nw/o MoE\n100.0\n100.0\n84.51\n100.0\n99.68\n100.0\n100.0\n100.0\n100.0\n6.06\n6.22\n10.98\n5.49\n5.23\n5.65\n6.12\n5.51\n5.59\nw/o Amp\n99.56\n99.94\n0.00\n100.0\n99.94\n100.0\n100.0\n99.94\n99.94\n7.27\n5.68\nâ€“\n6.50\n5.80\n6.31\n6.17\n6.25\n6.07\nIV-D\nAblation Study on Training Recipe\nTo investigate the individual contributions of our proposed components, we conduct a series of ablation studies by comparing our full framework against the following versions:\nâ€¢\nSingle-frame Depth (w/o depth history)\n: The policy receives only the current depth image instead of the strided temporal history.\nâ€¢\nUniform Command (w/o pose-based)\n: The policy is trained using standard uniform velocity sampling with heading commands.\nâ€¢\nVanilla MLP (w/o MoE)\n: The policy network is replaced by a standard Multi-Layer Perceptron (MLP) with an equivalent number of parameters.\nâ€¢\nNo Motion Prior (w/o AMP)\n: The Adversarial Motion Prior is removed, training the policy solely through task-related rewards.\nWe test each policy for 10,000 time-steps with 1,000 robots in simulation. We evaluate the Success Rate and Mean Reaching Time across different terrain types.\nAs shown in\nTableÂ II\n, our full method achieves the highest success rate, demonstrating that the integration of all proposed components is essential for traversing complex geometries.\nV\nConclusion\nIn this paper, we present a scalable end-to-end perceptive locomotion framework for humanoid robots. By integrating a novel volumetric edge-aware penalization mechanism with position-based velocity command generation, our approach achieves high-dynamic behaviors on complex terrains. We conduct extensive experiments in both simulation and real-world deployment. It demonstrates that our framework enables humanoid robots to perform agile running and stable walking across a variety of challenging environments.\nDespite these advancements, our current system has two primary limitations. First, the perception system relies solely on a single onboard forward-facing depth camera, which leads to a lack of backward or lateral movement capability. Future work may attach multiple cameras to the robot to enable omnidirectional perception and agility. Second, we observe that training a variety of terrains and gait modes simultaneously can lead to mode collapse and performance degradation compared to specialized policies. Advanced multi-task reinforcement learning techniques can be investigated to enhance the capacity of a single unified policy.\nAcknowledgments\nThis work is accomplished with the help of Xiangting Meng, Baijun Ye, Siqiao Huang, and Yizhuo Gao.\nReferences\n[1]\nA. Agrawal, S. Chen, A. Rai, and K. Sreenath\n(2022)\nVision-aided dynamic quadrupedal locomotion on discrete terrain using motion libraries\n.\nIn\n2022 International Conference on Robotics and Automation (ICRA)\n,\npp.Â 4708â€“4714\n.\nCited by:\nÂ§\nII-B\n.\n[2]\nJ. P. Araujo, Y. Ze, P. Xu, J. Wu, and C. K. Liu\n(2025)\nRetargeting matters: general motion retargeting for humanoid motion tracking\n.\narXiv preprint arXiv:2510.02252\n.\nCited by:\nÂ§\nIII-E\n.\n[3]\nQ. Ben, B. Xu, K. Li, F. Jia, W. Zhang, J. Wang, J. Wang, D. Lin, and J. Pang\n(2025)\nGallant: voxel grid-based humanoid locomotion and local-navigation across 3d constrained terrains\n.\narXiv preprint arXiv:2511.14625\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[4]\nX. Cheng, K. Shi, A. Agarwal, and D. Pathak\n(2024)\nExtreme parkour with legged robots\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 11443â€“11450\n.\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-A\n,\nÂ§\nII-B\n,\nÂ§\nIII-C\n,\nÂ§\nIII-D\n.\n[5]\nY. Cheng, H. Liu, G. Pan, H. Liu, and L. Ye\n(2024)\nQuadruped robot traversing 3d complex environments with limited perception\n.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 9074â€“9081\n.\nCited by:\nÂ§\nIII-D\n.\n[6]\nA. Chilian and H. HirschmÃ¼ller\n(2009)\nStereo camera based navigation of mobile robots on rough terrain\n.\nIn\n2009 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp.Â 4571â€“4576\n.\nCited by:\nÂ§\nII-B\n.\n[7]\nH. Duan, B. Pandit, M. S. Gadde, B. Van Marum, J. Dao, C. Kim, and A. Fern\n(2024)\nLearning vision-based bipedal locomotion for challenging terrain\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 56â€“62\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[8]\nP. Fankhauser, M. Bjelonic, C. D. Bellicoso, T. Miki, and M. Hutter\n(2018)\nRobust rough-terrain locomotion with a quadrupedal robot\n.\nIn\n2018 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 5761â€“5768\n.\nCited by:\nÂ§\nII-B\n.\n[9]\nP. Fankhauser, M. Bloesch, and M. Hutter\n(2018)\nProbabilistic terrain mapping for mobile robots with uncertain localization\n.\nIEEE Robotics and Automation Letters\n3\n(\n4\n),\npp.Â 3019â€“3026\n.\nCited by:\nÂ§\nII-B\n.\n[10]\nM. Y. Galliker\n(2024)\nWhole-body humanoid mpc: realtime physics-based procedural loco-manipulation planning and control\n.\nNote:\nhttps://github.com/1x-technologies/wb_humanoid_mpc\nCited by:\nÂ§\nII-A\n,\nitemÂ 1\n.\n[11]\nS. Gangapurwala, M. Geisert, R. Orsolino, M. Fallon, and I. Havoutis\n(2021)\nReal-time trajectory adaptation for quadrupedal locomotion using deep reinforcement learning\n.\nIn\n2021 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 5973â€“5979\n.\nCited by:\nÂ§\nII-B\n.\n[12]\nR. Grandia, F. Jenelten, S. Yang, F. Farshidian, and M. Hutter\n(2023)\nPerceptive locomotion through nonlinear model-predictive control\n.\nIEEE Transactions on Robotics\n39\n(\n5\n),\npp.Â 3402â€“3421\n.\nCited by:\nÂ§\nII-B\n.\n[13]\nX. Gu, Y. Wang, and J. Chen\n(2024)\nHumanoid-gym: reinforcement learning for humanoid robot with zero-shot sim2real transfer\n.\narXiv preprint arXiv:2404.05695\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[14]\nF. G. Harvey, M. Yurick, D. Nowrouzezahrai, and C. Pal\n(2020)\nRobust motion in-betweening\n.\nACM Transactions on Graphics (TOG)\n39\n(\n4\n),\npp.Â 60â€“1\n.\nCited by:\nitemÂ 3\n.\n[15]\nJ. He, C. Zhang, F. Jenelten, R. Grandia, M. BÃ¤cher, and M. Hutter\n(2025)\nAttention-based map encoding for learning generalized legged locomotion\n.\nScience Robotics\n10\n(\n105\n),\npp.Â eadv3604\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[16]\nR. Huang, S. Zhu, Y. Du, and H. Zhao\n(2025)\nMoE-loco: mixture of experts for multitask locomotion\n.\narXiv preprint arXiv:2503.08564\n.\nCited by:\nÂ§I\n.\n[17]\nR. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton\n(1991)\nAdaptive mixtures of local experts\n.\nNeural computation\n3\n(\n1\n),\npp.Â 79â€“87\n.\nCited by:\nÂ§I\n.\n[18]\nF. Jenelten, J. He, F. Farshidian, and M. Hutter\n(2024)\nDtc: deep tracking control\n.\nScience Robotics\n9\n(\n86\n),\npp.Â eadh5401\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[19]\nF. Jenelten, T. Miki, A. E. Vijayan, M. Bjelonic, and M. Hutter\n(2020)\nPerceptive locomotion in rough terrainâ€“online foothold optimization\n.\nIEEE Robotics and Automation Letters\n5\n(\n4\n),\npp.Â 5370â€“5376\n.\nCited by:\nÂ§\nII-B\n.\n[20]\nS. Katayama, M. Murooka, and Y. Tazaki\n(2023)\nModel predictive control of legged and humanoid robots: models and algorithms\n.\nAdvanced Robotics\n37\n(\n5\n),\npp.Â 298â€“315\n.\nCited by:\nÂ§\nII-A\n.\n[21]\nD. Kim, D. Carballo, J. Di Carlo, B. Katz, G. Bledt, B. Lim, and S. Kim\n(2020)\nVision aided dynamic exploration of unstructured terrain with a small-scale quadruped robot\n.\nIn\n2020 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 2464â€“2470\n.\nCited by:\nÂ§\nII-B\n.\n[22]\nQ. Liao, T. E. Truong, X. Huang, G. Tevet, K. Sreenath, and C. K. Liu\n(2025)\nBeyondmimic: from motion tracking to versatile humanoid control via guided diffusion\n.\narXiv preprint arXiv:2508.08241\n.\nCited by:\nÂ§I\n,\nÂ§\nIII-A\n2\n.\n[23]\nJ. Long, J. Ren, M. Shi, Z. Wang, T. Huang, P. Luo, and J. Pang\n(2025)\nLearning humanoid locomotion with perceptive internal model\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 9997â€“10003\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[24]\nZ. Luo, Y. Yuan, T. Wang, C. Li, S. Chen, F. CastaÃ±eda, Z. Cao, J. Li, D. Minor, Q. Ben, X. Da, R. Ding, C. Hogg, L. Song, E. Lim, E. Jeong, T. He, H. Xue, W. Xiao, Z. Wang, S. Yuen, J. Kautz, Y. Chang, U. Iqbal, L. Fan, and Y. Zhu\n(2025)\nSONIC: supersizing motion tracking for natural humanoid whole-body control\n.\narXiv preprint arXiv:2511.07820\n.\nCited by:\nÂ§I\n.\n[25]\nM. Macklin\n(2022-03)\nWarp: a high-performance python framework for gpu simulation and graphics\n.\nNote:\nNVIDIA GPU Technology Conference (GTC)\nhttps://github.com/nvidia/warp\nCited by:\nÂ§\nIII-B\n1\n,\nÂ§\nIII-C\n2\n.\n[26]\nO. A. V. Magana, V. Barasuol, M. Camurri, L. Franceschi, M. Focchi, M. Pontil, D. G. Caldwell, and C. Semini\n(2019)\nFast and continuous foothold adaptation for dynamic locomotion through cnns\n.\nIEEE Robotics and Automation Letters\n4\n(\n2\n),\npp.Â 2140â€“2147\n.\nCited by:\nÂ§\nII-B\n.\n[27]\nHoloMotion: a foundation model for whole-body humanoid control\nExternal Links:\nLink\nCited by:\nÂ§I\n.\n[28]\nC. Mastalli, M. Focchi, I. Havoutis, A. Radulescu, S. Calinon, J. Buchli, D. G. Caldwell, and C. Semini\n(2017)\nTrajectory and foothold optimization using low-dimensional models for rough terrain locomotion\n.\nIn\n2017 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 1096â€“1103\n.\nCited by:\nÂ§\nII-B\n.\n[29]\nC. Mastalli, I. Havoutis, M. Focchi, D. G. Caldwell, and C. Semini\n(2020)\nMotion planning for quadrupedal locomotion: coupled planning, terrain mapping, and whole-body control\n.\nIEEE Transactions on Robotics\n36\n(\n6\n),\npp.Â 1635â€“1648\n.\nCited by:\nÂ§\nII-B\n.\n[30]\nC. Mastalli, I. Havoutis, A. W. Winkler, D. G. Caldwell, and C. Semini\n(2015)\nOn-line and on-board planning and perception for quadrupedal locomotion\n.\nIn\n2015 IEEE International Conference on Technologies for Practical Robot Applications (TePRA)\n,\npp.Â 1â€“7\n.\nCited by:\nÂ§\nII-B\n.\n[31]\nM. Mittal, P. Roth, J. Tigue, A. Richard, O. Zhang, P. Du, A. Serrano-MuÃ±oz, X. Yao, R. ZurbrÃ¼gg, N. Rudin, L. Wawrzyniak, M. Rakhsha, A. Denzler, E. Heiden, A. Borovicka, O. Ahmed, I. Akinola, A. Anwar, M. T. Carlson, J. Y. Feng, A. Garg, R. Gasoto, L. Gulich, Y. Guo, M. Gussert, A. Hansen, M. Kulkarni, C. Li, W. Liu, V. Makoviychuk, G. Malczyk, H. Mazhar, M. Moghani, A. Murali, M. Noseworthy, A. Poddubny, N. Ratliff, W. Rehberg, C. Schwarke, R. Singh, J. L. Smith, B. Tang, R. Thaker, M. Trepte, K. V. Wyk, F. Yu, A. Millane, V. Ramasamy, R. Steiner, S. Subramanian, C. Volk, C. Chen, N. Jawale, A. V. Kuruttukulam, M. A. Lin, A. Mandlekar, K. Patzwaldt, J. Welsh, H. Zhao, F. Anes, J. Lafleche, N. MoÃ«nne-Loccoz, S. Park, R. Stepinski, D. V. Gelder, C. Amevor, J. Carius, J. Chang, A. H. Chen, P. de Heras Ciechomski, G. Daviet, M. Mohajerani, J. von Muralt, V. Reutskyy, M. Sauter, S. Schirm, E. L. Shi, P. Terdiman, K. Vilella, T. Widmer, G. Yeoman, T. Chen, S. Grizan, C. Li, L. Li, C. Smith, R. Wiltz, K. Alexis, Y. Chang, D. Chu, L. â€. Fan, F. Farshidian, A. Handa, S. Huang, M. Hutter, Y. Narang, S. Pouya, S. Sheng, Y. Zhu, M. Macklin, A. Moravanszky, P. Reist, Y. Guo, D. Hoeller, and G. State\n(2025)\nIsaac lab: a gpu-accelerated simulation framework for multi-modal robot learning\n.\narXiv preprint arXiv:2511.04831\n.\nExternal Links:\nLink\nCited by:\nÂ§\nIII-D\n1\n,\nÂ§\nIV-A\n.\n[32]\nX. B. Peng, Z. Ma, P. Abbeel, S. Levine, and A. Kanazawa\n(2021)\nAmp: adversarial motion priors for stylized physics-based character control\n.\nACM Transactions on Graphics (ToG)\n40\n(\n4\n),\npp.Â 1â€“20\n.\nCited by:\nÂ§\nIII-E\n.\n[33]\nI. Radosavovic, S. Kamat, T. Darrell, and J. Malik\n(2024)\nLearning humanoid locomotion over challenging terrain\n.\narXiv preprint arXiv:2410.03654\n.\nCited by:\nÂ§I\n.\n[34]\nI. Radosavovic, T. Xiao, B. Zhang, T. Darrell, J. Malik, and K. Sreenath\n(2024)\nReal-world humanoid locomotion with reinforcement learning\n.\nScience Robotics\n9\n(\n89\n),\npp.Â eadi9579\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[35]\nN. Rudin, J. He, J. Aurand, and M. Hutter\n(2025)\nParkour in the wild: learning a general and extensible agile locomotion policy using multi-expert distillation and rl fine-tuning\n.\narXiv preprint arXiv:2505.11164\n.\nCited by:\nÂ§\nII-A\n.\n[36]\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov\n(2017)\nProximal policy optimization algorithms\n.\narXiv preprint arXiv:1707.06347\n.\nCited by:\nÂ§\nIII-A\n.\n[37]\nN. Scianca, D. De Simone, L. Lanari, and G. Oriolo\n(2020)\nMPC for humanoid gait generation: stability and feasibility\n.\nIEEE Transactions on Robotics\n36\n(\n4\n),\npp.Â 1171â€“1188\n.\nCited by:\nÂ§\nII-A\n.\n[38]\nJ. Siekmann, K. Green, J. Warila, A. Fern, and J. Hurst\n(2021)\nBlind bipedal stair traversal via sim-to-real reinforcement learning\n.\narXiv preprint arXiv:2105.08328\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[39]\nH. Song, H. Zhu, T. Yu, Y. Liu, M. Yuan, W. Zhou, H. Chen, and H. Li\n(2025)\nGait-adaptive perceptive humanoid locomotion with real-time under-base terrain reconstruction\n.\narXiv preprint arXiv:2512.07464\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[40]\nJ. Sun, G. Han, P. Sun, W. Zhao, J. Cao, J. Wang, Y. Guo, and Q. Zhang\n(2025)\nDpl: depth-only perceptive humanoid locomotion via realistic depth synthesis and cross-attention terrain reconstruction\n.\narXiv preprint arXiv:2510.07152\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n,\nÂ§\nIII-E\n.\n[41]\nH. Wang, Z. Wang, J. Ren, Q. Ben, T. Huang, W. Zhang, and J. Pang\n(2025)\nBeamdojo: learning agile humanoid locomotion on sparse footholds\n.\narXiv preprint arXiv:2502.10363\n.\nCited by:\nÂ§\nII-A\n.\n[42]\nY. Wang, C. Luo, P. Chen, J. Liu, W. Sun, T. Guo, K. Yang, B. Hu, Y. Zhang, and M. Zhao\n(2025)\nLearning vision-driven reactive soccer skills for humanoid robots\n.\narXiv preprint arXiv:2511.03996\n.\nCited by:\nÂ§\nIII-E\n.\n[43]\nL. Wellhausen and M. Hutter\n(2021)\nRough terrain navigation for legged robots using reachability planning and template learning\n.\nIn\n2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 6914â€“6921\n.\nCited by:\nÂ§\nII-B\n.\n[44]\nM. Wermelinger, P. Fankhauser, R. Diethelm, P. KrÃ¼si, R. Siegwart, and M. Hutter\n(2016)\nNavigation planning for legged robots in challenging terrain\n.\nIn\n2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 1184â€“1189\n.\nCited by:\nÂ§\nII-B\n.\n[45]\nB. Yang, L. Wellhausen, T. Miki, M. Liu, and M. Hutter\n(2021)\nReal-time optimal navigation planning using learned motion costs\n.\nIn\n2021 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 9283â€“9289\n.\nCited by:\nÂ§\nII-B\n.\n[46]\nM. Yuan, T. Yu, H. Song, B. Li, X. Jin, H. Chen, and W. Zeng\n(2025)\nPvP: data-efficient humanoid robot learning with proprioceptive-privileged contrastive representations\n.\narXiv preprint arXiv:2512.13093\n.\nCited by:\nÂ§\nII-A\n.\n[47]\nC. Zhang, N. Rudin, D. Hoeller, and M. Hutter\n(2024)\nLearning agile locomotion on risky terrains\n.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 11864â€“11871\n.\nCited by:\nÂ§I\n,\nÂ§\nIII-D\n.\n[48]\nS. Zhu, R. Huang, L. Mou, and H. Zhao\n(2025)\nRobust robot walker: learning agile locomotion over tiny traps\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 15987â€“15993\n.\nCited by:\nÂ§I\n,\nÂ§\nIII-D\n.\n[49]\nZ. Zhuang, Z. Fu, J. Wang, C. Atkeson, S. Schwertfeger, C. Finn, and H. Zhao\n(2023)\nRobot parkour learning\n.\narXiv preprint arXiv:2309.05665\n.\nCited by:\nÂ§\nIII-C\n2\n.\n[50]\nZ. Zhuang, S. Yao, and H. Zhao\n(2024)\nHumanoid parkour learning\n.\narXiv preprint arXiv:2406.10759\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.",
    "preview_text": "Achieving robust humanoid hiking in complex, unstructured environments requires transitioning from reactive proprioception to proactive perception. However, integrating exteroception remains a significant challenge: mapping-based methods suffer from state estimation drift; for instance, LiDAR-based methods do not handle torso jitter well. Existing end-to-end approaches often struggle with scalability and training complexity; specifically, some previous works using virtual obstacles are implemented case-by-case. In this work, we present \\textit{Hiking in the Wild}, a scalable, end-to-end parkour perceptive framework designed for robust humanoid hiking. To ensure safety and training stability, we introduce two key mechanisms: a foothold safety mechanism combining scalable \\textit{Terrain Edge Detection} with \\textit{Foot Volume Points} to prevent catastrophic slippage on edges, and a \\textit{Flat Patch Sampling} strategy that mitigates reward hacking by generating feasible navigation targets. Our approach utilizes a single-stage reinforcement learning scheme, mapping raw depth inputs and proprioception directly to joint actions, without relying on external state estimation. Extensive field experiments on a full-size humanoid demonstrate that our policy enables robust traversal of complex terrains at speeds up to 2.5 m/s. The training and deployment code is open-sourced to facilitate reproducible research and deployment on real robots with minimal hardware modifications.\n\nHiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids\nShaoting Zhu\n12\n1, Ziwen Zhuang\n12\n1, Mengjie Zhao\n1\n, Kun-Ying Lee\n3\n, Hang Zhao\n12\n2\n1\nIIIS, Tsinghua University,\n2\nShanghai Qi Zhi Institute,\n3\nDepartment of Computer Science and Technology, Tsinghua University\n1Equal contributionâ€ƒ2Corresponding author\nAbstract\nAchieving robust humanoid hiking in complex, unstructured environments requires transitioning from reactive proprioception to proactive perception. However, integratin",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯æ‰©å±•æ„ŸçŸ¥æ¡†æ¶ï¼Œç”¨äºäººå½¢æœºå™¨äººåœ¨å¤æ‚éç»“æ„åŒ–ç¯å¢ƒä¸­çš„ç¨³å¥å¾’æ­¥è¿åŠ¨æ§åˆ¶ã€‚",
    "detailed_summary": "## è®ºæ–‡ã€ŠHiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoidsã€‹è¯¦ç»†æ‘˜è¦\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\näººå½¢æœºå™¨äººåœ¨å¤æ‚éç»“æ„åŒ–ç¯å¢ƒä¸­è¿›è¡Œç¨³å¥çš„â€œé‡å¤–å¾’æ­¥â€éœ€è¦ä»**ååº”å¼æœ¬ä½“æ„ŸçŸ¥**è½¬å‘**ä¸»åŠ¨å¼å¤–éƒ¨æ„ŸçŸ¥**ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨æ˜¾è‘—ç“¶é¢ˆï¼šåŸºäºåœ°å›¾æ„å»ºçš„æ–¹æ³•ï¼ˆå¦‚LiDARï¼‰æ˜“å—çŠ¶æ€ä¼°è®¡æ¼‚ç§»å’Œè¿åŠ¨ç•¸å˜å½±å“ï¼›è€Œç«¯åˆ°ç«¯æ–¹æ³•åˆ™é¢ä¸´å¯æ‰©å±•æ€§å·®ã€è®­ç»ƒå¤æ‚ç­‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œç²¾ç¡®çš„è½è„šç‚¹æ§åˆ¶å¯¹äºåŒè¶³æœºå™¨äººçš„ç¨³å®šæ€§è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰å­¦ä¹ æ–¹æ¡ˆéš¾ä»¥å¤„ç†è¾¹ç¼˜è½è„šå¯¼è‡´çš„ç¾éš¾æ€§æ»‘å€’ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ä¸ªå¯æ‰©å±•çš„ç«¯åˆ°ç«¯æ„ŸçŸ¥æ¡†æ¶ï¼Œä»¥å®ç°äººå½¢æœºå™¨äººåœ¨å¤æ‚åœ°å½¢ä¸­çš„é«˜é€Ÿã€é²æ£’è¿åŠ¨ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸º **â€œHiking in the Wildâ€** çš„å®Œæ•´æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åŒ…æ‹¬ï¼š\n- **ç«¯åˆ°ç«¯å•é˜¶æ®µå¼ºåŒ–å­¦ä¹ ç­–ç•¥**ï¼šç›´æ¥ä»¥åŸå§‹æ·±åº¦å›¾åƒå’Œæœ¬ä½“æ„ŸçŸ¥ä¸ºè¾“å…¥ï¼Œè¾“å‡ºå…³èŠ‚åŠ¨ä½œï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨çŠ¶æ€ä¼°è®¡æˆ–åœ°å›¾é‡å»ºï¼Œå®ç°äº†é›¶æ ·æœ¬çš„ä»¿çœŸåˆ°ç°å®è¿ç§»ã€‚\n- **å¯æ‰©å±•çš„è½è„šç‚¹å®‰å…¨æœºåˆ¶**ï¼š\n    - **åœ°å½¢è¾¹ç¼˜æ£€æµ‹å™¨**ï¼šè‡ªåŠ¨ä»ä¸‰è§’ç½‘æ ¼åœ°å½¢ä¸­æ£€æµ‹å°–é”è¾¹ç¼˜ã€‚\n    - **è¶³éƒ¨ä½“ç§¯ç‚¹æƒ©ç½š**ï¼šé€šè¿‡åœ¨è®­ç»ƒä¸­æƒ©ç½šè¶³éƒ¨ä½“ç§¯ç‚¹ä¸åœ°å½¢è¾¹ç¼˜çš„ç©¿é€ï¼Œç­–ç•¥éšå¼å­¦ä¹ å°†è„šæ”¾ç½®åœ¨å¹³å¦ã€å®‰å…¨çš„åŒºåŸŸä¸­å¿ƒï¼Œæ˜¾è‘—æå‡äº†åœ¨æ¥¼æ¢¯ã€ç¼éš™ç­‰åœ°å½¢çš„å®‰å…¨æ€§ã€‚\n- **åŸºäºä½ç½®çš„æŒ‡ä»¤ç”Ÿæˆ**ï¼šé‡‡ç”¨**å¹³å¦åŒºåŸŸé‡‡æ ·**ç­–ç•¥ï¼Œåœ¨åœ°å½¢ä¸Šè¯†åˆ«å¯è¾¾çš„å¹³å¦åŒºåŸŸä½œä¸ºå¯¼èˆªç›®æ ‡ï¼Œå¹¶æ®æ­¤ç”Ÿæˆé€Ÿåº¦æŒ‡ä»¤ã€‚è¿™æœ‰æ•ˆé¿å…äº†æ™ºèƒ½ä½“â€œå¥–åŠ±æ¬ºéª—â€ï¼ˆå¦‚åŸåœ°è½¬åœˆï¼‰ï¼Œç¡®ä¿äº†æœ‰æ„ä¹‰çš„æ¢ç´¢å’Œæ–¹å‘éµä»æ€§ã€‚\n- **é«˜ä¿çœŸæ·±åº¦åˆæˆä¸å¤„ç†**ï¼šè®¾è®¡äº†åŒ…å«ä¼ æ„Ÿå™¨å™ªå£°å’Œä¼ªå½±çš„æ·±åº¦å›¾åƒä»¿çœŸæµç¨‹ï¼Œå¹¶ä¸çœŸå®æ•°æ®å¤„ç†æµç¨‹å¯¹é½ï¼Œä»¥å¼¥åˆä»¿çœŸä¸ç°å®å·®è·ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nåœ¨Unitree G1å…¨å°ºå¯¸äººå½¢æœºå™¨äººä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®åœ°éƒ¨ç½²æµ‹è¯•ï¼ŒæˆåŠŸå®ç°äº†**é›¶æ ·æœ¬è¿ç§»**ã€‚å…³é”®ç»“æœåŒ…æ‹¬ï¼š\n- **é«˜é€Ÿæ•æ·è¿åŠ¨**ï¼šåœ¨å¤æ‚åœ°å½¢ä¸Šå®ç°äº†é«˜è¾¾**2.5 m/s**çš„å¥”è·‘é€Ÿåº¦ã€‚\n- **å¤æ‚åœ°å½¢ç©¿è¶Š**ï¼šæœºå™¨äººç¨³å¥åœ°å®Œæˆäº†ä¸Šä¸‹æ¥¼æ¢¯ã€æ–œå¡ã€ä¸å¹³æ•´è‰åœ°ä»¥åŠå®½åº¦è¾¾**50å˜ç±³**çš„ç¦»æ•£ç¼éš™ç©¿è¶Šã€‚\n- **é«˜æˆåŠŸç‡ä¸ç¨³å®šæ€§**ï¼šåœ¨å„ç§åœ°å½¢å’Œæ­¥æ€ç»„åˆçš„æµ‹è¯•ä¸­ä¿æŒäº†é«˜æˆåŠŸç‡ï¼Œå¹¶å®Œæˆäº†æŒç»­4åˆ†é’Ÿçš„æ— è·Œå€’é•¿æ—¶è¡Œèµ°æµ‹è¯•ã€‚\n- **æ¶ˆèå®éªŒéªŒè¯**ï¼šå®éªŒè¯å®äº†æ·±åº¦å†å²ã€åŸºäºä½ç½®çš„æŒ‡ä»¤ã€æ··åˆä¸“å®¶ç½‘ç»œå’Œå¯¹æŠ—è¿åŠ¨å…ˆéªŒç­‰å…³é”®ç»„ä»¶å¯¹æ€§èƒ½å‡æœ‰é‡è¦è´¡çŒ®ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\næœ¬å·¥ä½œä¸ºæ„ŸçŸ¥å‹äººå½¢æœºå™¨äºº locomotion æä¾›äº†ä¸€ä¸ª**å¯æ‰©å±•ã€ç«¯åˆ°ç«¯**çš„è§£å†³æ–¹æ¡ˆã€‚å…¶ä»·å€¼ä½“ç°åœ¨ï¼š\n- **æŠ€æœ¯çªç ´**ï¼šé€šè¿‡åˆ›æ–°çš„å®‰å…¨æœºåˆ¶å’Œè®­ç»ƒç­–ç•¥ï¼Œè§£å†³äº†åŸºäºå­¦ä¹ çš„æ§åˆ¶ä¸­ç²¾ç¡®è½è„šç‚¹å’ŒæŒ‡ä»¤è·Ÿè¸ªçš„éš¾é¢˜ï¼Œå®ç°äº†åœ¨çœŸå®å¤æ‚ç¯å¢ƒä¸­çš„é«˜é€ŸåŠ¨æ€è¡Œä¸ºã€‚\n- **å®ç”¨æ€§ä¸å¯å¤ç°æ€§**ï¼šæ¡†æ¶ä¸ä¾èµ–å¤–éƒ¨å®šä½ï¼Œä»…éœ€å•ç›®æ·±åº¦ç›¸æœºï¼Œç¡¬ä»¶ä¿®æ”¹æå°ã€‚ä½œè€…**å¼€æºäº†è®­ç»ƒä¸éƒ¨ç½²ä»£ç **ï¼Œæå¤§åœ°ä¿ƒè¿›äº†ç¤¾åŒºçš„å¯å¤ç°ç ”ç©¶å’Œåœ¨å®é™…æœºå™¨äººä¸Šçš„éƒ¨ç½²ã€‚\n- **æ¨åŠ¨é¢†åŸŸå‘å±•**ï¼šä¸ºäººå½¢æœºå™¨äººèµ°å‘çœŸæ­£çš„é‡å¤–è‡ªä¸»å¯¼èˆªè¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨è§£å†³é«˜ç»´ã€åŠ¨æ€ç°å®é—®é¢˜ä¸Šçš„å·¨å¤§æ½œåŠ›ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **ä¸€ã€ è®ºæ–‡æ—¨åœ¨è§£å†³çš„æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**åŒè¶³äººå½¢æœºå™¨äººåœ¨å¤æ‚ã€éç»“æ„åŒ–é‡å¤–ç¯å¢ƒä¸­è¿›è¡Œé²æ£’ã€æ•æ·çš„â€œå¾’æ­¥â€å¼ç§»åŠ¨**è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒé’ˆå¯¹ç°æœ‰æ–¹æ³•çš„å‡ ä¸ªå…³é”®ç“¶é¢ˆï¼š\n1.  **ä»â€œååº”å¼â€åˆ°â€œå‰ç»å¼â€çš„è½¬å˜**ï¼šä¼ ç»Ÿçš„â€œç›²èµ°â€æ–¹æ³•ä»…ä¾èµ–æœ¬ä½“æ„ŸçŸ¥ï¼ˆå¦‚å…³èŠ‚åŠ›ï¼‰ï¼Œåœ¨ç¢°æ’åè¢«åŠ¨ååº”ï¼Œæ— æ³•ä¸»åŠ¨è§„é¿æ·±å‘ã€é«˜å°é˜¶ç­‰å±é™©åœ°å½¢ï¼Œæ˜“å¯¼è‡´ç¾éš¾æ€§å¤±è´¥ã€‚\n2.  **å¤–æ„ŸçŸ¥èåˆçš„éš¾é¢˜**ï¼š\n    - **åŸºäºåœ°å›¾çš„æ–¹æ³•**ï¼ˆå¦‚LiDARå»ºå›¾ï¼‰ä¸¥é‡ä¾èµ–ç²¾ç¡®çš„çŠ¶æ€ä¼°è®¡ï¼Œåœ¨é‡å¤–æ˜“äº§ç”Ÿæ¼‚ç§»ï¼Œä¸”å—é™äºä¼ æ„Ÿå™¨é¢‘ç‡å’Œè¿åŠ¨ç•¸å˜ã€‚\n    - **ç°æœ‰ç«¯åˆ°ç«¯æ–¹æ³•**ï¼šå­˜åœ¨**å¯æ‰©å±•æ€§å·®**ï¼ˆå¦‚é’ˆå¯¹è™šæ‹Ÿéšœç¢ç‰©éœ€é€ä¾‹è®¾è®¡ï¼‰ã€**è®­ç»ƒå¤æ‚**ã€**éš¾ä»¥å¤„ç†é«˜é€ŸåŠ¨æ€åœºæ™¯**ç­‰é—®é¢˜ã€‚\n3.  **äººå½¢æœºå™¨äººç‰¹æœ‰çš„å®‰å…¨æŒ‘æˆ˜**ï¼šä¸å››è¶³æœºå™¨äººç›¸æ¯”ï¼Œäººå½¢æœºå™¨äººç¨³å®šæ€§æ›´å·®ï¼Œ**è½è„šç‚¹ï¼ˆfootholdï¼‰çš„å¾®å°åå·®**ï¼ˆå¦‚è¸©åœ¨å°é˜¶è¾¹ç¼˜ï¼‰ææ˜“å¯¼è‡´æ»‘å€’å’Œæ‘”å€’ã€‚\n4.  **å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„â€œå¥–åŠ±é»‘å®¢â€é—®é¢˜**ï¼šæ™ºèƒ½ä½“ä¸ºæœ€å¤§åŒ–å¥–åŠ±ï¼Œå¯èƒ½å­¦ä¼šâ€œåŸåœ°è½¬åœˆâ€ç­‰ä½œå¼Šè¡Œä¸ºï¼Œè€ŒéçœŸæ­£ç©¿è¶Šå¤æ‚åœ°å½¢ã€‚\n\n### **äºŒã€ è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸º **â€œHiking in the Wildâ€** çš„å¯æ‰©å±•ç«¯åˆ°ç«¯æ„ŸçŸ¥è·‘é…·æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°ä½“ç°åœ¨ä»¥ä¸‹å››ä¸ªç´§å¯†å…³è”çš„æ–¹é¢ï¼š\n\n#### **1. å¯æ‰©å±•çš„ç«¯åˆ°ç«¯æ„ŸçŸ¥ä¸æ§åˆ¶æ¶æ„**\n- **æŠ€æœ¯æ–¹æ¡ˆ**ï¼šé‡‡ç”¨**å•é˜¶æ®µå¼ºåŒ–å­¦ä¹ **ï¼Œç›´æ¥å°†**åŸå§‹æ·±åº¦å›¾åƒ**å’Œ**æœ¬ä½“æ„ŸçŸ¥ä¿¡å·**æ˜ å°„åˆ°å…³èŠ‚åŠ¨ä½œã€‚æ— éœ€å¤–éƒ¨çŠ¶æ€ä¼°è®¡ï¼ˆå¦‚å®šä½ã€å»ºå›¾æ¨¡å—ï¼‰ã€‚\n- **æŠ€æœ¯åˆ›æ–°**ï¼š\n    - **é«˜å¸¦å®½æ„ŸçŸ¥**ï¼šåˆ©ç”¨**ä¸“å®¶æ··åˆæ¨¡å‹**å¤„ç†é«˜ç»´è§†è§‰æ•°æ®ï¼Œä½¿ç­–ç•¥èƒ½ä»¥é«˜è¾¾60Hzçš„é¢‘ç‡å¤„ç†æ·±åº¦è¾“å…¥ï¼Œæ”¯æ’‘é«˜é€Ÿï¼ˆ2.5 m/sï¼‰åŠ¨æ€è¡Œä¸ºã€‚\n    - **é€¼çœŸçš„æ·±åº¦åˆæˆä¸å¯¹é½**ï¼šè®¾è®¡äº†åŒå‘å¤„ç†æµæ°´çº¿ï¼ˆ`â„±_sim` å’Œ `â„±_real`ï¼‰ï¼Œåœ¨ä»¿çœŸä¸­æ¨¡æ‹Ÿä¼ æ„Ÿå™¨å™ªå£°ã€è¿åŠ¨æ¨¡ç³Šã€å¤±é…ä¼ªå½±ç­‰ï¼Œå®ç°äº†**é›¶æ ·æœ¬çš„ä»¿çœŸåˆ°ç°å®è¿ç§»**ã€‚\n    - **è·¨æ­¥é•¿æ—¶é—´èšåˆ**ï¼šå¯¹æ·±åº¦å›¾åƒé‡‡ç”¨è·¨æ­¥é‡‡æ ·çš„å†å²å¸§åºåˆ—ï¼Œä¸ºç­–ç•¥æä¾›æ›´é•¿çš„æ—¶åºä¸Šä¸‹æ–‡ï¼Œä½¿å…¶èƒ½é¢„åˆ¤åœ°å½¢è¶‹åŠ¿ï¼Œé€‚åº”é«˜é€Ÿè¿åŠ¨ã€‚\n\n#### **2. æ–°é¢–çš„è½è„šç‚¹å®‰å…¨æœºåˆ¶**\n- **è§£å†³çš„é—®é¢˜**ï¼šé˜²æ­¢æœºå™¨äººè„šéƒ¨è¸©åœ¨å±é™©åœ°å½¢è¾¹ç¼˜ï¼ˆå¦‚æ¥¼æ¢¯æ£±è§’ï¼‰å¯¼è‡´æ»‘å€’ã€‚\n- **æŠ€æœ¯æ–¹æ¡ˆ**ï¼šç»“åˆ **â€œåœ°å½¢è¾¹ç¼˜æ£€æµ‹å™¨â€** å’Œ **â€œè„šéƒ¨ä½“ç§¯ç‚¹â€** çš„è½¯çº¦æŸæƒ©ç½šæœºåˆ¶ã€‚\n    - **åœ°å½¢è¾¹ç¼˜æ£€æµ‹å™¨**ï¼šè‡ªåŠ¨ä»ä¸‰è§’ç½‘æ ¼åœ°å½¢ä¸­æ£€æµ‹å°–é”è¾¹ç¼˜ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ï¼Œ**å¯æ‰©å±•åˆ°ä»»æ„æ–°åœ°å½¢**ã€‚\n    - **è„šéƒ¨ä½“ç§¯ç‚¹**ï¼šåœ¨è„šéƒ¨ç¢°æ’ä½“å†…åˆ†å¸ƒä¸€ç»„ç‚¹ï¼Œå®æ—¶è®¡ç®—è¿™äº›ç‚¹ä¸åœ°å½¢è¾¹ç¼˜åœ†æŸ±ä½“çš„ç©¿é€æ·±åº¦å’Œé€Ÿåº¦ã€‚\n    - **æƒ©ç½šå¥–åŠ±**ï¼šæ ¹æ®ç©¿é€æ·±åº¦å’Œé€Ÿåº¦è®¡ç®—æƒ©ç½šé¡¹ï¼ˆå…¬å¼3ï¼‰ï¼Œå¼•å¯¼ç­–ç•¥åœ¨è®­ç»ƒä¸­**éšå¼å­¦ä¹ **å°†è„šæ”¾ç½®åœ¨å¹³å¦ã€å®‰å…¨çš„åŒºåŸŸä¸­å¿ƒï¼Œè€Œéç²¾ç¡®çš„è½¨è¿¹è§„åˆ’ã€‚\n\n#### **3. åŸºäºä½ç½®çš„æŒ‡ä»¤ç”Ÿæˆæ–¹æ³•**\n- **è§£å†³çš„é—®é¢˜**ï¼šæ ¹æ²»â€œå¥–åŠ±é»‘å®¢â€é—®é¢˜ï¼Œå¹¶ç¡®ä¿å¯¼èˆªè¡Œä¸ºçš„é²æ£’æ€§å’Œæ–¹å‘ä¾ä»æ€§ã€‚\n- **æŠ€æœ¯æ–¹æ¡ˆ**ï¼š**å¹³å¦åŒºåŸŸé‡‡æ · + ä½ç½®åé¦ˆæ§åˆ¶**ã€‚\n    1.  **å¹³å¦åŒºåŸŸé‡‡æ ·**ï¼šåœ¨è®­ç»ƒç¯å¢ƒä¸­ï¼Œå®æ—¶æ£€æµ‹åœ°å½¢ç½‘æ ¼ä¸Šå¯è¾¾çš„å¹³å¦åŒºåŸŸä½œä¸ºå¯¼èˆªç›®æ ‡ç‚¹ã€‚\n    2.  **æŒ‡ä»¤ç”Ÿæˆ**ï¼šåŸºäºæœºå™¨äººåŸºåº§åæ ‡ç³»ä¸ç›®æ ‡ç‚¹çš„ç›¸å¯¹ä½ç½®ï¼ˆ`x_g, y_g`ï¼‰ï¼Œé€šè¿‡æ¯”ä¾‹æ§åˆ¶ç”Ÿæˆçº¿é€Ÿåº¦å’Œè§’é€Ÿåº¦æŒ‡ä»¤ï¼ˆå…¬å¼4, 5ï¼‰ã€‚\n    - **å®é™…ä»·å€¼**ï¼šç¡®ä¿äº†è®­ç»ƒä¸­æ¯ä¸ªå›åˆçš„ç›®æ ‡éƒ½æ˜¯ç‰©ç†ä¸Šå¯å®ç°çš„ï¼Œè¿«ä½¿æ™ºèƒ½ä½“å­¦ä¹ çœŸæ­£çš„ç©¿è¶ŠæŠ€èƒ½ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶æå‡æ€§èƒ½ã€‚\n\n#### **4. æ•´ä½“æ¡†æ¶çš„éªŒè¯ä¸å¼€æº**\n- **å®é™…éªŒè¯**ï¼šåœ¨å…¨å°ºå¯¸äººå½¢æœºå™¨äººï¼ˆUnitree G1ï¼‰ä¸Šè¿›è¡Œäº†å¤§é‡é‡å¤–å®éªŒï¼ŒæˆåŠŸç©¿è¶Šæ¥¼æ¢¯ã€æ–œå¡ã€è‰åœ°ã€ç¦»æ•£æ²Ÿå£‘ç­‰å¤æ‚åœ°å½¢ï¼Œæœ€é«˜é€Ÿåº¦è¾¾2.5 m/sï¼Œå¹¶å±•ç¤ºäº†é•¿æ—¶é—´çš„ç¨³å®šæ€§ã€‚\n- **ç¤¾åŒºè´¡çŒ®**ï¼š**å®Œæ•´å¼€æºè®­ç»ƒä¸éƒ¨ç½²ä»£ç **ï¼Œå¼ºè°ƒåªéœ€æœ€å°ç¡¬ä»¶æ”¹åŠ¨ï¼ˆä½¿ç”¨æœºå™¨äººè‡ªå¸¦çš„æ·±åº¦ç›¸æœºï¼‰å³å¯åœ¨çœŸå®æœºå™¨äººä¸Šå¤ç°ï¼Œæå¤§åœ°ä¿ƒè¿›äº†å¯é‡å¤ç ”ç©¶å’Œå®é™…éƒ¨ç½²ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆçš„æ€»ä½“æ€è·¯**\nè®ºæ–‡çš„è§£å†³è·¯å¾„å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š**æ„å»ºä¸€ä¸ªä»¥é«˜é¢‘ç‡åŸå§‹æ·±åº¦æ„ŸçŸ¥ä¸ºè¾“å…¥ã€é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒæœºåˆ¶ï¼ˆå®‰å…¨çº¦æŸã€æ™ºèƒ½æŒ‡ä»¤ç”Ÿæˆã€è¿åŠ¨å…ˆéªŒï¼‰åœ¨ä»¿çœŸä¸­è®­ç»ƒå‡ºé²æ£’ç­–ç•¥ã€å¹¶å€ŸåŠ©é«˜åº¦é€¼çœŸçš„ä¼ æ„Ÿå™¨ä»¿çœŸå®ç°é›¶æ ·æœ¬ç°å®éƒ¨ç½²çš„ç«¯åˆ°ç«¯ç³»ç»Ÿã€‚**\n\n**æŠ€æœ¯è·¯çº¿å›¾**ï¼š\n```\nåŸå§‹æ·±åº¦è¾“å…¥ (60Hz) \n    â†’ [ä»¿çœŸï¼šå¸¦å™ªå£°å»ºæ¨¡çš„æ·±åº¦åˆæˆ + çœŸå®æ•°æ®å¯¹é½] \n    â†’ [ç­–ç•¥ç½‘ç»œï¼šMoEæ¶æ„å¤„ç†æ„ŸçŸ¥+æœ¬ä½“å†å²] \n    â†’ [è®­ç»ƒç¯å¢ƒï¼šå¹³å¦åŒºåŸŸé‡‡æ ·æŒ‡ä»¤ + è¾¹ç¼˜æ¥è§¦æƒ©ç½š + AMPè¿åŠ¨å…ˆéªŒ] \n    â†’ è®­ç»ƒå‡ºé²æ£’ç­–ç•¥ \n    â†’ [é›¶æ ·æœ¬éƒ¨ç½²ï¼šä»…éœ€å•ç›®æ·±åº¦ç›¸æœº] \n    â†’ ç°å®ä¸–ç•Œé«˜é€Ÿã€é²æ£’çš„é‡å¤–ç§»åŠ¨\n```\n\n### **æ€»ç»“**\nè¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒä»·å€¼åœ¨äºï¼Œå®ƒé€šè¿‡ä¸€ç³»åˆ—**ç³»ç»Ÿæ€§çš„ç®—æ³•åˆ›æ–°**ï¼Œå°†ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ åº”ç”¨äºäººå½¢æœºå™¨äººé‡å¤–ç§»åŠ¨è¿™ä¸€æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¹¶æˆåŠŸè§£å†³äº†**å®‰å…¨æ€§ã€å¯æ‰©å±•æ€§ã€è®­ç»ƒç¨³å®šæ€§**ç­‰å…³é”®éš¾é¢˜ã€‚å…¶æå‡ºçš„ **â€œè¾¹ç¼˜å®‰å…¨æœºåˆ¶â€** å’Œ **â€œå¹³å¦åŒºåŸŸæŒ‡ä»¤ç”Ÿæˆâ€** æ˜¯æå…·æ´å¯ŸåŠ›çš„è®¾è®¡ï¼Œä¸ä»…æå‡äº†æ€§èƒ½ï¼Œä¹Ÿä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦æ€è·¯ã€‚æœ€ç»ˆï¼Œé€šè¿‡**å®Œæ•´çš„å¼€æºå®ç°å’Œæ‰å®çš„ç°å®å®éªŒ**ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶çš„å®ç”¨æ€§å’Œå…ˆè¿›æ€§ã€‚",
            "timestamp": "2026-01-21T12:10:30.273016",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡ã€ŠHiking in the Wildã€‹åŠå…¶å‰ä½œå‘å±•è„‰ç»œåˆ†æ\n\næœ¬æ–‡æå‡ºçš„â€œHiking in the Wildâ€æ¡†æ¶æ˜¯**åŸºäºæ„ŸçŸ¥çš„äººå½¢æœºå™¨äººé‡å¤–è¿åŠ¨**è¿™ä¸€ç ”ç©¶æ–¹å‘çš„æœ€æ–°è¿›å±•ã€‚å…¶å‘å±•è„‰ç»œå¯ä»¥æ¸…æ™°åœ°æ¢³ç†ä¸ºå‡ ä¸ªå…³é”®é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½é’ˆå¯¹ç‰¹å®šç“¶é¢ˆè¿›è¡Œäº†æ”¹è¿›ï¼Œå¹¶åˆ†åŒ–å‡ºä¸åŒçš„æŠ€æœ¯å‘å±•æ–¹å‘ã€‚\n\n### å‘å±•è„‰ç»œä¸å…³é”®æ”¹è¿›\n\n#### é˜¶æ®µä¸€ï¼šç›²è¿åŠ¨ (Blind Locomotion)\n*   **ä»£è¡¨å·¥ä½œ**ï¼šRadosavovic et al. (2024) *Real-world humanoid locomotion with reinforcement learning*; Siekmann et al. (2021) *Blind bipedal stair traversal*; Gu et al. (2024) *Humanoid-gym*ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šä»…ä¾èµ–**æœ¬ä½“æ„ŸçŸ¥**ï¼ˆå…³èŠ‚ä½ç½®ã€é€Ÿåº¦ã€IMUç­‰ï¼‰ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒå‡ºé²æ£’çš„è¿åŠ¨ç­–ç•¥ã€‚\n*   **æ”¹è¿›ä¸ä»·å€¼**ï¼šè¯æ˜äº†æ•°æ®é©±åŠ¨æ–¹æ³•åœ¨å¤æ‚åœ°å½¢ï¼ˆè‰åœ°ã€ç¢çŸ³ã€æ¥¼æ¢¯ï¼‰ä¸Šçš„**ååº”å¼**é²æ£’æ€§ã€‚å®ƒå»ºç«‹äº†æ— éœ€å¤–éƒ¨æ„ŸçŸ¥çš„ç¨³å®šè¿åŠ¨åŸºçº¿ã€‚\n*   **æ ¹æœ¬å±€é™**ï¼š**è¢«åŠ¨ååº”**ã€‚æœºå™¨äººåªèƒ½åœ¨å‘ç”Ÿç¢°æ’æˆ–å¤±ç¨³åè¿›è¡Œè°ƒæ•´ï¼Œæ— æ³•é¢„è§å‰æ–¹çš„å¤§éšœç¢ï¼ˆå¦‚æ·±æ²Ÿã€é«˜å°ï¼‰ï¼Œå­˜åœ¨**å®‰å…¨æ€§å¤©èŠ±æ¿**ã€‚\n\n#### é˜¶æ®µäºŒï¼šå¼•å…¥å¤–æ„ŸçŸ¥ï¼Œæ¨¡å—åŒ–ä¸åœ°å›¾é‡å»º\n*   **å‘å±•æ–¹å‘Aï¼šåŸºäºLiDARä¸é«˜ç¨‹å›¾**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šHe et al. (2025) *Attention-based map encoding*; Long et al. (2025) *Learning humanoid locomotion with perceptive internal model*ã€‚\n    *   **æ ¸å¿ƒæ€æƒ³**ï¼šä½¿ç”¨LiDARæ‰«ææ„å»ºç¯å¢ƒçš„2.5Dé«˜ç¨‹å›¾æˆ–ä½“ç´ ç½‘æ ¼ï¼ŒåŸºäºæ­¤è¿›è¡Œè¿åŠ¨è§„åˆ’ã€‚\n    *   **æ”¹è¿›**ï¼šå¼•å…¥äº†**å‰ç»æ€§**ï¼Œèƒ½å¤Ÿè§„åˆ’ç»•è¿‡å¤§å‹éšœç¢ç‰©çš„è·¯å¾„ã€‚\n    *   **ç“¶é¢ˆ**ï¼š\n        1.  **ä¸¥é‡ä¾èµ–ç²¾ç¡®çš„çŠ¶æ€ä¼°è®¡ï¼ˆå®šä½ï¼‰**ï¼Œåœ¨é‡å¤–æ˜“äº§ç”Ÿæ¼‚ç§»ã€‚\n        2.  LiDARé¢‘ç‡ä½ï¼Œå­˜åœ¨è¿åŠ¨ç•¸å˜ï¼Œéš¾ä»¥æ”¯æŒ**é«˜é€ŸåŠ¨æ€è¿åŠ¨**ã€‚\n        3.  ç³»ç»Ÿå¤æ‚ï¼Œé€šå¸¸ä¸å¼€æºï¼Œ**å¯å¤ç°æ€§å’Œå¯æ‰©å±•æ€§å·®**ã€‚\n\n*   **å‘å±•æ–¹å‘Bï¼šåŸºäºæ·±åº¦å›¾åƒä¸é«˜åº¦å›¾é‡å»º**\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šDuan et al. (2024) *Learning vision-based bipedal locomotion*; Sun et al. (2025) *DPL*ã€‚\n    *   **æ ¸å¿ƒæ€æƒ³**ï¼šä½¿ç”¨æ·±åº¦ç›¸æœºï¼Œé€šè¿‡ä¸­é—´ç½‘ç»œæ¨¡å—ä»æ·±åº¦å›¾åƒä¸­**é‡å»ºå‡ºåœ°å½¢é«˜åº¦å›¾**ï¼Œå†åŸºäºé«˜åº¦å›¾è¿›è¡Œè§„åˆ’æˆ–æ§åˆ¶ã€‚\n    *   **æ”¹è¿›**ï¼šç›¸æ¯”LiDARï¼Œæ·±åº¦ç›¸æœºæ›´è½»ä¾¿ã€é«˜é¢‘ï¼Œæˆæœ¬æ›´ä½ã€‚\n    *   **ç“¶é¢ˆ**ï¼š\n        1.  é«˜åº¦å›¾é‡å»ºæ¨¡å—å¯èƒ½å¼•å…¥è¯¯å·®ï¼Œä¸”åœ¨**æœªè§è¿‡çš„é‡å¤–å¤æ‚åœ°å½¢ä¸Šæ³›åŒ–èƒ½åŠ›æœ‰é™**ã€‚\n        2.  æ•´ä¸ªæ„ŸçŸ¥-è§„åˆ’-æ§åˆ¶**æµæ°´çº¿é•¿**ï¼Œå»¶è¿Ÿé«˜ï¼Œé€šå¸¸ä»é™äº**ä½é€Ÿ**åœºæ™¯ã€‚\n        3.  æ–¹æ³•é«˜åº¦å®šåˆ¶åŒ–ï¼ˆå¦‚ç›¸æœºä½å§¿ï¼‰ï¼Œ**éš¾ä»¥è¿ç§»å’Œå¤ç°**ã€‚\n\n#### é˜¶æ®µä¸‰ï¼šç«¯åˆ°ç«¯æ„ŸçŸ¥è¿åŠ¨æ§åˆ¶çš„å…´èµ·\n*   **ä»£è¡¨å·¥ä½œ**ï¼šCheng et al. (2024) *Extreme parkour with legged robots* (ç”¨äºå››è¶³); Zhuang et al. (2024) *Humanoid parkour learning*ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šæ‘’å¼ƒå¤æ‚çš„ä¸­é—´è¡¨ç¤ºï¼ˆå¦‚åœ°å›¾é‡å»ºï¼‰å’Œæ¨¡å—åŒ–æµæ°´çº¿ï¼Œä½¿ç”¨**å¼ºåŒ–å­¦ä¹ ç›´æ¥è®­ç»ƒä¸€ä¸ªç­–ç•¥**ï¼Œå°†**åŸå§‹æ·±åº¦å›¾åƒå’Œæœ¬ä½“æ„ŸçŸ¥æ˜ å°„åˆ°å…³èŠ‚åŠ¨ä½œ**ã€‚\n*   **é‡å¤§æ”¹è¿›**ï¼š\n    1.  **ç®€åŒ–ç³»ç»Ÿ**ï¼šå»é™¤äº†å¯¹å¤–éƒ¨å®šä½å’Œåœ°å›¾é‡å»ºçš„ä¾èµ–ï¼Œé™ä½äº†ç³»ç»Ÿå¤æ‚åº¦å’Œæ•…éšœç‚¹ã€‚\n    2.  **é«˜å¸¦å®½æ§åˆ¶**ï¼šèƒ½å¤Ÿå¤„ç†é«˜é¢‘åŸå§‹æ„ŸçŸ¥æ•°æ®ï¼Œä¸ºå®ç°**é«˜é€ŸåŠ¨æ€è¡Œä¸º**ï¼ˆå¦‚è·‘é…·ï¼‰æä¾›äº†å¯èƒ½ã€‚\n    3.  **éšå¼å­¦ä¹ **ï¼šç­–ç•¥éšå¼åœ°å­¦ä¹ åœ°å½¢ç‰¹å¾ä¸åŠ¨ä½œçš„å…³è”ï¼Œè€Œéæ˜¾å¼è§„åˆ’ã€‚\n*   **é—ç•™é—®é¢˜**ï¼š\n    1.  **è®­ç»ƒç¨³å®šæ€§ä¸å®‰å…¨æ€§**ï¼šå¦‚ä½•ç¡®ä¿ç«¯åˆ°ç«¯è®­ç»ƒä¸­ï¼ŒåŒè¶³æœºå™¨äººèƒ½å­¦ä¼šå®‰å…¨ã€ç²¾ç¡®çš„è½è„šç‚¹ï¼Ÿ\n    2.  **å¥–åŠ±é»‘å®¢é—®é¢˜**ï¼šåœ¨å¤æ‚åœ°å½¢è®­ç»ƒä¸­ï¼Œæ™ºèƒ½ä½“å®¹æ˜“å­¦ä¼šâ€œæ¬ºéª—â€å¥–åŠ±å‡½æ•°ï¼ˆå¦‚åŸåœ°è½¬åœˆï¼‰ï¼Œè€ŒéçœŸæ­£ç©¿è¶Šéšœç¢ã€‚\n    3.  **å¯æ‰©å±•æ€§**ï¼šæ—©æœŸå·¥ä½œï¼ˆå¦‚Cheng et al.ï¼‰ä½¿ç”¨è™šæ‹Ÿéšœç¢è¿›è¡Œè¾¹ç¼˜æƒ©ç½šï¼Œéœ€è¦ä¸ºæ¯ç§åœ°å½¢æ‰‹å·¥è®¾è®¡ï¼Œ**æ— æ³•æ‰©å±•åˆ°ä»»æ„çš„ã€æœªçŸ¥çš„ç½‘æ ¼åœ°å½¢**ã€‚\n\n#### é˜¶æ®µå››ï¼ˆæœ¬æ–‡ï¼‰ï¼šå¯æ‰©å±•ã€å®‰å…¨ã€é«˜æ•ˆçš„ç«¯åˆ°ç«¯æ¡†æ¶\n*   **å·¥ä½œ**ï¼šZhu et al. *Hiking in the Wild*ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨ç«¯åˆ°ç«¯èŒƒå¼åŸºç¡€ä¸Šï¼Œç³»ç»Ÿæ€§åœ°è§£å†³äº†**å®‰å…¨æ€§**ã€**è®­ç»ƒæ•ˆç‡**å’Œ**å¯æ‰©å±•æ€§**ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ã€‚\n*   **å…³é”®æ”¹è¿›ä¸è´¡çŒ®**ï¼š\n\n| å‘å±•æ–¹å‘ | å‰ä½œçš„ç“¶é¢ˆ | æœ¬æ–‡çš„æ”¹è¿› |\n| :--- | :--- | :--- |\n| **1. å®‰å…¨æ€§ä¸ç²¾ç¡®è½è„šç‚¹** | æ¨¡å‹åŒ–æ–¹æ³•å¯¹åœ°å›¾è¯¯å·®è„†å¼±ï¼›æ—©æœŸç«¯åˆ°ç«¯æ–¹æ³•ï¼ˆå¦‚è¾¹ç¼˜æƒ©ç½šï¼‰éœ€è¦ä¸ºæ¯ç§åœ°å½¢æ‰‹å·¥è®¾è®¡ï¼Œä¸å¯æ‰©å±•ã€‚ | æå‡º **å¯æ‰©å±•åœ°å½¢è¾¹ç¼˜æ£€æµ‹ + è¶³éƒ¨ä½“ç§¯ç‚¹æƒ©ç½š** æœºåˆ¶ã€‚**è‡ªåŠ¨**ä»ä»»æ„ä¸‰è§’ç½‘æ ¼åœ°å½¢ä¸­æ£€æµ‹è¾¹ç¼˜ï¼Œå¹¶é€šè¿‡ä½“ç§¯ç‚¹ç©¿é€æƒ©ç½šï¼Œ**éšå¼åœ°**å¼•å¯¼ç­–ç•¥å°†è„šæ”¾ç½®åœ¨å¹³å¦åŒºåŸŸä¸­å¿ƒï¼Œæ— éœ€æ‰‹å·¥è®¾è®¡æˆ–æ˜¾å¼è½¨è¿¹è§„åˆ’ã€‚ |\n| **2. è®­ç»ƒæ•ˆç‡ä¸æŒ‡ä»¤è·Ÿè¸ª** | éšæœºé€Ÿåº¦æŒ‡ä»¤æ˜“å¯¼è‡´â€œå¥–åŠ±é»‘å®¢â€ï¼›çº¯ç›®æ ‡ç‚¹æŒ‡ä»¤æ— æ³•æ§åˆ¶é€Ÿåº¦ï¼Œä¸”éšæœºç›®æ ‡å¯èƒ½ä¸å¯è¾¾ã€‚ | æå‡º **åŸºäºå¹³å¦åŒºåŸŸé‡‡æ ·çš„ä½ç½®é€Ÿåº¦æŒ‡ä»¤ç”Ÿæˆ**ã€‚åœ¨å¯åˆ°è¾¾çš„**å¹³å¦åŒºåŸŸ**ä¸Šé‡‡æ ·ç›®æ ‡ç‚¹ï¼Œæ ¹æ®æœºå™¨äººç›¸å¯¹ç›®æ ‡çš„ä½ç½®å’Œé¢„è®¾é€Ÿåº¦é™å€¼ç”Ÿæˆé€Ÿåº¦æŒ‡ä»¤ã€‚è¿™ç¡®ä¿äº†è®­ç»ƒä»»åŠ¡å§‹ç»ˆæ˜¯ç‰©ç†ä¸Šå¯è§£çš„ï¼Œ**æ¶ˆé™¤äº†å¥–åŠ±é»‘å®¢ï¼ŒåŠ é€Ÿäº†æ”¶æ•›**ã€‚ |\n| **3. æ„ŸçŸ¥ä»¿çœŸä¸é›¶æ ·æœ¬è¿ç§»** | ä»¿çœŸä¸çœŸå®çš„æ·±åº¦æ•°æ®å­˜åœ¨å·®å¼‚ï¼Œå½±å“Sim-to-Realè¿ç§»ã€‚ | è®¾è®¡äº†**åŒå‘æ·±åº¦å¯¹é½ç®¡é“** (`â„±_sim`, `â„±_real`)ï¼Œåœ¨ä»¿çœŸä¸­æ³¨å…¥ä¼ æ„Ÿå™¨å™ªå£°ã€è¿åŠ¨æ¨¡ç³Šã€å¼‚å¸¸å€¼ç­‰ï¼Œåœ¨çœŸå®ä¸­è¿›è¡Œä¿®å¤ï¼Œ**æœ€å°åŒ–ä»¿çœŸä¸ç°å®å·®è·**ï¼Œå®ç°äº†**é›¶æ ·æœ¬**è¿ç§»ï¼Œæ— éœ€å¤–éƒ¨å®šä½ç³»ç»Ÿã€‚ |\n| **4. ç­–ç•¥å®¹é‡ä¸è¿åŠ¨é£æ ¼** | å•ä¸€ç­–ç•¥éš¾ä»¥åŒæ—¶æŒæ¡è¡Œèµ°ã€å¥”è·‘ç­‰å¤šç§æŠ€èƒ½å’Œåº”å¯¹å¤šæ ·åœ°å½¢ã€‚ | é‡‡ç”¨**æ··åˆä¸“å®¶æ¨¡å‹** å¤„ç†é«˜ç»´è§†è§‰æ•°æ®å’Œå¤æ‚æŠ€èƒ½ï¼›ä½¿ç”¨**å¯¹æŠ—è¿åŠ¨å…ˆéªŒ** èåˆMPCçš„ç¨³å®šæ€§ä¸äººç±»åŠ¨ä½œçš„è‡ªç„¶æ€§ï¼Œæå‡äº†è¿åŠ¨è´¨é‡å’Œå¤šæ ·æ€§ã€‚ |\n| **5. ç³»ç»Ÿæ€§èƒ½ä¸éªŒè¯** | å¤šæ•°æ–¹æ³•é™äºä½é€Ÿæˆ–ç®€å•ç»“æ„åŒ–ç¯å¢ƒã€‚ | å®ç°äº†**é«˜è¾¾2.5 m/sçš„å¥”è·‘é€Ÿåº¦**ï¼Œå¹¶èƒ½ç©¿è¶Šé«˜å°ï¼ˆ32cmï¼‰ã€å®½æ²Ÿï¼ˆ50cmï¼‰ç­‰å¤æ‚é‡å¤–åœ°å½¢ï¼Œé€šè¿‡äº†**é•¿æ—¶é—´çš„å®åœ°æµ‹è¯•**ï¼Œè¯æ˜äº†å…¶**é«˜åŠ¨æ€æ€§**å’Œ**é²æ£’æ€§**ã€‚ |\n\n### æ€»ç»“\nè¯¥æ–¹å‘çš„å‘å±•å‘ˆç°å‡ºæ¸…æ™°çš„æ¼”è¿›è·¯å¾„ï¼šä»**ç›²è¿åŠ¨**ï¼ˆååº”å¼é²æ£’ï¼‰åˆ°**æ„ŸçŸ¥è¿åŠ¨**ï¼ˆå‰ç»å¼è§„åˆ’ï¼‰ï¼Œå†ä»**å¤æ‚çš„æ¨¡å—åŒ–åœ°å›¾é‡å»º**èµ°å‘**ç®€æ´çš„ç«¯åˆ°ç«¯æ§åˆ¶**ã€‚æœ¬æ–‡ç«™åœ¨ç«¯åˆ°ç«¯èŒƒå¼çš„å‰æ²¿ï¼Œé€šè¿‡ä¸€ç³»åˆ—**å¯æ‰©å±•çš„ã€è‡ªåŠ¨åŒ–çš„æœºåˆ¶**ï¼ˆè¾¹ç¼˜æ£€æµ‹ã€å¹³å¦åŒºåŸŸé‡‡æ ·ã€æ·±åº¦å¯¹é½ï¼‰ï¼Œç³»ç»Ÿæ€§åœ°æ”»å…‹äº†è¯¥èŒƒå¼åœ¨è½åœ°åº”ç”¨ä¸­çš„æ ¸å¿ƒéš¾é¢˜â€”â€”**å®‰å…¨ã€é«˜æ•ˆè®­ç»ƒä¸é›¶æ ·æœ¬éƒ¨ç½²**ï¼Œå°†äººå½¢æœºå™¨äººçš„é‡å¤–è¿åŠ¨èƒ½åŠ›æ¨å‘äº†æ–°çš„é«˜åº¦ï¼ˆé«˜é€Ÿã€é«˜åŠ¨æ€ã€é«˜å¤æ‚åº¦åœ°å½¢ï¼‰ã€‚å…¶**å¼€æºä»£ç **ä¹Ÿæå¤§åœ°ä¿ƒè¿›äº†è¯¥æ–¹å‘çš„å¯å¤ç°ç ”ç©¶å’Œå®é™…éƒ¨ç½²ã€‚",
            "timestamp": "2026-01-21T12:11:26.325222",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡ã€ŠHiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoidsã€‹çš„å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œä½œè€…å›¢é˜Ÿï¼ˆç‰¹åˆ«æ˜¯é€šè®¯ä½œè€…Hang ZhaoåŠå…¶åˆä½œè€…ï¼‰åœ¨è¶³å¼æœºå™¨äººæ„ŸçŸ¥è¿åŠ¨é¢†åŸŸæœ‰ä¸€ç³»åˆ—ç›¸å…³çš„å‰æœŸå·¥ä½œã€‚è¿™äº›å·¥ä½œä¸»è¦èšç„¦äº**åŸºäºå­¦ä¹ çš„ã€å…·æœ‰æ„ŸçŸ¥èƒ½åŠ›çš„æœºå™¨äººè·‘é…·ï¼ˆParkourï¼‰æˆ–å¤æ‚åœ°å½¢ç©¿è¶Š**ã€‚\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†æ¢³ç†äº†å…¶æ ¸å¿ƒå‰ä½œï¼Œå¹¶é‡ç‚¹å¯¹æ¯”äº†æœ¬æ–‡ä¸æ¯ç¯‡å‰ä½œçš„ä¸»è¦åŒºåˆ«å’Œæ”¹è¿›ä¹‹å¤„ã€‚\n\n| å‰ä½œè®ºæ–‡æ ‡é¢˜ | å‘è¡¨å¹´ä»½/ä¼šè®® | æ ¸å¿ƒæ–¹æ³•ä¸è´¡çŒ® | æœ¬æ–‡ã€ŠHiking in the Wildã€‹ä¸ä¹‹ç›¸æ¯”çš„ä¸»è¦åŒºåˆ«ä¸æ”¹è¿› |\n| :--- | :--- | :--- | :--- |\n| **Extreme Parkour with Legged Robots** <br> (Cheng et al., ICRA 2024) [4] | 2024 (ICRA) | - **æ–¹æ³•**ï¼š ä¸ºå››è¶³æœºå™¨äººè®¾è®¡çš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå°†åŸå§‹æ·±åº¦å›¾åƒå’Œæœ¬ä½“æ„ŸçŸ¥æ˜ å°„åˆ°å…³èŠ‚åŠ¨ä½œã€‚<br>- **è´¡çŒ®**ï¼š å¼•å…¥äº†**è¾¹ç¼˜æƒ©ç½šï¼ˆedge penalizationï¼‰** æœºåˆ¶ï¼Œè®©æœºå™¨äººå­¦ä¹ é¿å¼€åœ°å½¢è¾¹ç¼˜ï¼Œå®ç°è·‘é…·åŠ¨ä½œï¼ˆå¦‚è·³ä¸Šé«˜å°ã€è·¨è¶Šé—´éš™ï¼‰ã€‚<br>- **å±€é™**ï¼š è®ºæ–‡æŒ‡å‡ºå…¶è¾¹ç¼˜æƒ©ç½šæœºåˆ¶**æ— æ³•æ‰©å±•åˆ°ä»»æ„ç½‘æ ¼åœ°å½¢ï¼ˆnot scalable to arbitrary meshesï¼‰**ï¼Œä¸”è®­ç»ƒä¸­å­˜åœ¨**å¥–åŠ±é»‘å®¢ï¼ˆreward hackingï¼‰**é—®é¢˜ï¼ˆæœºå™¨äººåŸåœ°è½¬åœˆä»¥è·å–å¥–åŠ±ï¼‰ã€‚ | **1. å¯æ‰©å±•çš„è¾¹ç¼˜æ„ŸçŸ¥æœºåˆ¶**ï¼š å‰ä½œçš„è¾¹ç¼˜æƒ©ç½šæ˜¯é’ˆå¯¹ç‰¹å®šè™šæ‹Ÿéšœç¢ç‰©æ‰‹å·¥è®¾è®¡çš„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ª**å¯æ‰©å±•çš„åœ°å½¢è¾¹ç¼˜æ£€æµ‹å™¨ï¼ˆScalable Terrain Edge Detectionï¼‰**ï¼Œèƒ½è‡ªåŠ¨ä»ä»»æ„ä¸‰è§’ç½‘æ ¼åœ°å½¢ä¸­æ£€æµ‹è¾¹ç¼˜ï¼Œæ— éœ€é’ˆå¯¹æ¯ç§åœ°å½¢è¿›è¡Œæ‰‹å·¥é…ç½®ï¼Œæå¤§åœ°æå‡äº†æ³›åŒ–èƒ½åŠ›ã€‚<br>**2. æ”¹è¿›çš„æŒ‡ä»¤ç”Ÿæˆç­–ç•¥**ï¼š å‰ä½œä½¿ç”¨å‡åŒ€é‡‡æ ·çš„é€Ÿåº¦æŒ‡ä»¤ï¼Œæ˜“å¯¼è‡´å¥–åŠ±é»‘å®¢ã€‚æœ¬æ–‡æå‡ºäº†**åŸºäºå¹³å¦åŒºåŸŸé‡‡æ ·çš„ä½ç½®é€Ÿåº¦æŒ‡ä»¤ç”Ÿæˆï¼ˆFlat Patch Sampling & Position-based Velocity Commandï¼‰**ã€‚é€šè¿‡åœ¨åœ°å½¢ä¸Šè¯†åˆ«å¯åˆ°è¾¾çš„å¹³å¦åŒºåŸŸä½œä¸ºç›®æ ‡ï¼Œå¹¶æ®æ­¤ç”Ÿæˆé€Ÿåº¦æŒ‡ä»¤ï¼Œä»æ ¹æœ¬ä¸Šé¿å…äº†å¥–åŠ±é»‘å®¢ï¼Œå¹¶ç¡®ä¿äº†è®­ç»ƒä»»åŠ¡çš„ç‰©ç†å¯å®ç°æ€§ã€‚<br>**3. åº”ç”¨å¹³å°ä¸æ€§èƒ½**ï¼š å‰ä½œé’ˆå¯¹å››è¶³æœºå™¨äººã€‚æœ¬æ–‡å°†å…¶æ¡†æ¶**æ‰©å±•å¹¶ä¸“é—¨ä¼˜åŒ–ç”¨äºåŒè¶³äººå½¢æœºå™¨äºº**ï¼Œè¿™æ˜¯ä¸€ä¸ªç¨³å®šæ€§è¦æ±‚æ›´é«˜ã€æ§åˆ¶æ›´å¤æ‚çš„å¹³å°ã€‚æœ¬æ–‡å®ç°äº†é«˜è¾¾**2.5 m/sçš„é«˜é€Ÿå¥”è·‘**ï¼Œå¹¶å±•ç¤ºäº†åœ¨å¤æ‚é‡å¤–ç¯å¢ƒï¼ˆè‰åœ°æ–œå¡ã€é«˜å°ã€æ·±æ²Ÿï¼‰ä¸­çš„é²æ£’æ€§ã€‚ |\n| **Robot Parkour Learning** <br> (Zhuang et al., arXiv 2023) [49] | 2023 (arXiv) | - **æ–¹æ³•**ï¼š è¾ƒæ—©æ¢ç´¢ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®©å››è¶³æœºå™¨äººå®Œæˆè·‘é…·ä»»åŠ¡ï¼ˆè·³ã€è·¨ã€é’»ï¼‰ã€‚<br>- **è´¡çŒ®**ï¼š æå‡ºäº†ä½¿ç”¨**ä½“ç§¯ç‚¹ï¼ˆVolume Pointsï¼‰** è¿›è¡Œæ¥è§¦æƒ©ç½šçš„æ¦‚å¿µï¼Œä¸ºåç»­å·¥ä½œæä¾›äº†åŸºç¡€ã€‚<br>- **å±€é™**ï¼š æ„ŸçŸ¥å’Œå®‰å…¨æ€§æœºåˆ¶å¯èƒ½è¾ƒä¸ºåˆæ­¥ï¼Œæœªé‡ç‚¹è§£å†³å¯æ‰©å±•æ€§å’ŒæŒ‡ä»¤ç”Ÿæˆçš„æ ¹æœ¬é—®é¢˜ã€‚ | **1. ç³»ç»ŸåŒ–ä¸å·¥ç¨‹åŒ–**ï¼š æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ª**å®Œæ•´çš„ã€å¯éƒ¨ç½²çš„ç«¯åˆ°ç«¯æ„ŸçŸ¥æ¡†æ¶**ï¼Œä»å‰ä½œçš„ç®—æ³•æ¦‚å¿µéªŒè¯ï¼Œå‘å±•ä¸ºåŒ…å«**é«˜ä¿çœŸæ·±åº¦åˆæˆã€æ—¶åºèšåˆã€æ··åˆä¸“å®¶ç½‘ç»œï¼ˆMoEï¼‰** ç­‰æ¨¡å—çš„æˆç†Ÿç³»ç»Ÿã€‚<br>**2. é›¶-shotä»¿çœŸåˆ°ç°å®è¿ç§»**ï¼š æœ¬æ–‡ç‰¹åˆ«å¼ºè°ƒäº†**åŒå‘æ·±åº¦å¯¹é½ä¸å™ªå£°å»ºæ¨¡**ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä»¿çœŸä¸çœŸå®æ•°æ®å¤„ç†æµæ°´çº¿ï¼ˆâ„±_sim, â„±_realï¼‰ï¼Œå®ç°äº†ä¸ä¾èµ–å¤–éƒ¨çŠ¶æ€ä¼°è®¡çš„**é›¶-shot Sim-to-Realè¿ç§»**ï¼Œå·¥ç¨‹è½åœ°æ€§æ›´å¼ºã€‚<br>**3. å®‰å…¨æ€§æœºåˆ¶é›†æˆ**ï¼š æœ¬æ–‡å°†å‰ä½œçš„ä½“ç§¯ç‚¹æ¦‚å¿µä¸**æ–°æå‡ºçš„å¯æ‰©å±•è¾¹ç¼˜æ£€æµ‹å™¨**ç›¸ç»“åˆï¼Œå½¢æˆäº†æ›´å¼ºå¤§çš„**ç«‹è¶³ç‚¹å®‰å…¨æœºåˆ¶**ï¼Œä¸“é—¨è§£å†³äººå½¢æœºå™¨äººè¸©è¸è¾¹ç¼˜æ˜“æ»‘å€’çš„è‡´å‘½é—®é¢˜ã€‚ |\n| **Humanoid Parkour Learning** <br> (Zhuang et al., arXiv 2024) [50] | 2024 (arXiv) | - **æ–¹æ³•**ï¼š å°†è·‘é…·å­¦ä¹ æ¡†æ¶åº”ç”¨åˆ°äººå½¢æœºå™¨äººã€‚<br>- **è´¡çŒ®**ï¼š æ¢ç´¢äº†äººå½¢æœºå™¨äººçš„è·³è·ƒå’Œè·¨è¶Šèƒ½åŠ›ã€‚<br>- **å±€é™**ï¼š è®ºæ–‡æŒ‡å‡ºå…¶æ–¹æ³•**ä¸å¯æ‰©å±•ï¼ˆnot scalableï¼‰**ï¼Œå¯èƒ½ä»ä¾èµ–äºç‰¹å®šåœºæ™¯çš„è™šæ‹Ÿéšœç¢ç‰©è®¾ç½®ï¼Œä¸”**ä»£ç æœªå¼€æº**ï¼Œç¤¾åŒºéš¾ä»¥å¤ç°ã€‚ | **1. æ¡†æ¶çš„å¯æ‰©å±•æ€§ä¸å¼€æº**ï¼š æœ¬æ–‡æœ€å¤§çš„æ”¹è¿›ä¹‹ä¸€æ˜¯æå‡ºäº†ä¸€ä¸ª**â€œå¯æ‰©å±•çš„ï¼ˆScalableï¼‰â€æ¡†æ¶**ï¼Œå…¶è¾¹ç¼˜æ£€æµ‹å’Œè®­ç»ƒç­–ç•¥é€‚ç”¨äºå„ç§æœªçŸ¥çš„é‡å¤–åœ°å½¢ç½‘æ ¼ã€‚**ä»£ç å®Œå…¨å¼€æº**ï¼Œæ—¨åœ¨æ¨åŠ¨å¯å¤ç°çš„ç ”ç©¶å’Œä¾¿æ·çš„çœŸå®æœºå™¨äººéƒ¨ç½²ã€‚<br>**2. æ„ŸçŸ¥é¢‘ç‡ä¸åŠ¨æ€æ€§èƒ½**ï¼š æœ¬æ–‡çš„æ¡†æ¶æ”¯æŒ**60 Hzçš„é«˜é¢‘æ·±åº¦æ„ŸçŸ¥è¾“å…¥**ï¼Œè¿™æ˜¯å®ç°é«˜è¾¾2.5 m/såŠ¨æ€å¥”è·‘çš„å…³é”®ã€‚å‰ä½œå¯èƒ½å—é™äºæ„ŸçŸ¥é¢‘ç‡æˆ–ç­–ç•¥æ¶æ„ï¼Œæœªèƒ½å®ç°å¦‚æ­¤é«˜é€Ÿçš„æ•æ·è¿åŠ¨ã€‚<br>**3. è®­ç»ƒç¨³å®šæ€§ä¸è¯¾ç¨‹è®¾è®¡**ï¼š æœ¬æ–‡é€šè¿‡**å¹³å¦åŒºåŸŸé‡‡æ ·**å’Œ**å¯¹æŠ—æ€§è¿åŠ¨å…ˆéªŒï¼ˆAMPï¼‰** çš„æ”¹è¿›åº”ç”¨ï¼ˆä½¿ç”¨å¤šæºæ•°æ®ã€åºåˆ—åˆ¤åˆ«ï¼‰ï¼Œæä¾›äº†æ›´ç¨³å®šã€é«˜æ•ˆçš„è®­ç»ƒè¯¾ç¨‹ï¼Œé¿å…äº†æ¨¡å¼å´©æºƒï¼Œå­¦ä¹ äº†æ›´è‡ªç„¶å’Œé²æ£’çš„è¿åŠ¨é£æ ¼ã€‚ |\n| **Robust Robot Walker: Learning Agile Locomotion over Tiny Traps** <br> (Zhu et al., ICRA 2025) [48] | 2025 (ICRA) | - **æ–¹æ³•**ï¼š ä¸“æ³¨äºåœ¨å……æ»¡â€œå°é™·é˜±â€ï¼ˆå¦‚ç¼éš™ã€å‡¸èµ·ï¼‰çš„æç«¯ä¸å¹³æ•´åœ°å½¢ä¸Šçš„é²æ£’è¡Œèµ°ã€‚<br>- **è´¡çŒ®**ï¼š å¯èƒ½é‡‡ç”¨äº†**åŸºäºç›®æ ‡çš„æŒ‡ä»¤ï¼ˆgoal-based commandï¼‰** æ¥å¼•å¯¼æœºå™¨äººã€‚<br>- **å±€é™**ï¼š è®ºæ–‡æŒ‡å‡ºï¼Œçº¯ç›®æ ‡æŒ‡ä»¤æ–¹æ³•**ç¼ºä¹å¯¹æœºå™¨äººé€Ÿåº¦çš„æ§åˆ¶ï¼ˆlack control over speedï¼‰**ï¼Œä¸”éšæœºé‡‡æ ·çš„ç›®æ ‡å¯èƒ½ä¸é€‚ç”¨ã€‚ | **1. æŒ‡ä»¤ç”Ÿæˆæœºåˆ¶çš„èåˆä¸æ”¹è¿›**ï¼š æœ¬æ–‡æ‰¿è®¤äº†çº¯ç›®æ ‡æŒ‡ä»¤çš„ç¼ºé™·ï¼Œå¹¶æå‡ºäº†ä¸€ç§**æ··åˆæ–¹æ³•**ï¼šåŸºäºå¹³å¦åŒºåŸŸé‡‡æ ·ç”Ÿæˆ**ä½ç½®ç›®æ ‡**ï¼Œä½†å°†å…¶è½¬åŒ–ä¸º**å¸¦æœ‰é€Ÿåº¦é™å¹…çš„é€Ÿç‡æŒ‡ä»¤**ï¼ˆå…¬å¼4ï¼Œ5ï¼‰ã€‚è¿™æ ·æ—¢æä¾›äº†æœ‰æ„ä¹‰çš„å¯¼èˆªæ–¹å‘ï¼Œåˆä¿ç•™äº†å¯¹å‰è¿›/è½¬å‘é€Ÿåº¦çš„ç²¾ç¡®æ§åˆ¶ã€‚<br>**2. ä»»åŠ¡èŒƒå›´ä¸åŠ¨æ€æ€§**ï¼š å‰ä½œèšç„¦äºä½é€Ÿã€é«˜ç²¾åº¦çš„â€œé™·é˜±â€åœ°å½¢ç©¿è¶Šã€‚æœ¬æ–‡åˆ™å±•ç¤ºäº†æ›´å¹¿æ³›çš„**å…¨åœ°å½¢ã€å…¨é€ŸåŸŸèƒ½åŠ›**ï¼ŒåŒ…æ‹¬å¹³ç¨³è¡Œèµ°ã€é«˜é€Ÿå¥”è·‘ã€ä¸Šä¸‹æ¥¼æ¢¯å’Œé«˜å°ç­‰å¤šç§æ¨¡å¼ï¼Œä½“ç°äº†æ¡†æ¶çš„é€šç”¨æ€§å’Œé«˜æ€§èƒ½ã€‚<br>**3. é’ˆå¯¹äººå½¢çš„ä¸“é—¨ä¼˜åŒ–**ï¼š æœ¬æ–‡çš„æ‰€æœ‰æœºåˆ¶ï¼ˆç‰¹åˆ«æ˜¯ç«‹è¶³ç‚¹å®‰å…¨ï¼‰éƒ½æ˜¯å›´ç»•**äººå½¢æœºå™¨äººç¨³å®šæ€§å·®ã€æ˜“æ‘”å€’**çš„æ ¸å¿ƒæŒ‘æˆ˜è¿›è¡Œè®¾è®¡å’Œä¼˜åŒ–çš„ï¼Œè€Œå‰ä½œå¯èƒ½ä¸»è¦é’ˆå¯¹å››è¶³å¹³å°ã€‚ |\n\n### **æ€»ç»“ï¼šæœ¬æ–‡ç›¸å¯¹äºå‰ä½œçš„æ ¸å¿ƒæ¼”è¿›**\n\n1.  **ä»â€œæ¡ˆä¾‹å¼â€åˆ°â€œå¯æ‰©å±•â€**ï¼š è§£å†³äº†å‰ä½œï¼ˆå¦‚[4],[50]ï¼‰ä¸­æ–¹æ³•ä¾èµ–äºç‰¹å®šè™šæ‹Ÿéšœç¢ç‰©ã€æ— æ³•æ³›åŒ–åˆ°æ–°åœ°å½¢çš„æ ¹æœ¬ç“¶é¢ˆï¼Œé€šè¿‡è‡ªåŠ¨åŒ–åœ°å½¢è¾¹ç¼˜æ£€æµ‹å’Œé€šç”¨åŒ–è®­ç»ƒç­–ç•¥ï¼Œå®ç°äº†çœŸæ­£çš„**å¯æ‰©å±•æ€§ï¼ˆScalabilityï¼‰**ã€‚\n2.  **ä»â€œå¥–åŠ±é»‘å®¢â€åˆ°â€œå®šå‘æ¢ç´¢â€**ï¼š ç”¨**åŸºäºå¹³å¦åŒºåŸŸé‡‡æ ·çš„ä½ç½®é€Ÿåº¦æŒ‡ä»¤ç”Ÿæˆ**ï¼Œå–ä»£äº†å®¹æ˜“å¯¼è‡´æ— æ•ˆæ¢ç´¢çš„éšæœºé€Ÿåº¦æˆ–çº¯ç›®æ ‡æŒ‡ä»¤ï¼Œä»æ ¹æœ¬ä¸Š**æ”¹å–„äº†è®­ç»ƒæ•ˆç‡ä¸ç­–ç•¥è´¨é‡**ã€‚\n3.  **ä»â€œç®—æ³•åŸå‹â€åˆ°â€œéƒ¨ç½²ç³»ç»Ÿâ€**ï¼š é›†æˆäº†**é«˜é¢‘æ„ŸçŸ¥æµæ°´çº¿ã€çœŸå®çš„æ·±åº¦ä¼ æ„Ÿå™¨å»ºæ¨¡ã€é›¶-shotè¿ç§»æŠ€æœ¯**ï¼Œå¹¶**å¼€æºä»£ç **ï¼Œä½¿ç ”ç©¶ä»æ¦‚å¿µéªŒè¯èµ°å‘äº†å¯åœ¨çœŸå®äººå½¢æœºå™¨äººä¸Šç¨³å®šè¿è¡Œçš„å®Œæ•´ç³»ç»Ÿã€‚\n4.  **ä»â€œå››è¶³/åŸºç¡€äººå½¢â€åˆ°â€œé«˜é€Ÿæ•æ·äººå½¢â€**ï¼š å°†æ¡†æ¶æˆåŠŸåº”ç”¨äºåŠ¨æ€æ€§èƒ½è¦æ±‚æé«˜çš„äººå½¢æœºå™¨äººï¼Œå¹¶å®ç°äº†**2.5 m/sçš„é‡å¤–å¥”è·‘**ï¼Œæ ‡å¿—ç€åœ¨**å¤æ‚åŠ¨æ€è¡Œä¸º**ä¸Šçš„é‡å¤§çªç ´ã€‚\n\nå› æ­¤ï¼Œæœ¬æ–‡å¯ä»¥çœ‹ä½œæ˜¯ä½œè€…å›¢é˜Ÿåœ¨å…¶ç³»åˆ—å·¥ä½œä¸Šçš„ä¸€æ¬¡**ç³»ç»Ÿæ€§æ€»ç»“ä¸é£è·ƒ**ï¼Œæ ¸å¿ƒè´¡çŒ®åœ¨äºè§£å†³äº†**å¯æ‰©å±•æ€§ã€è®­ç»ƒå¯é æ€§ã€ä»¥åŠé«˜æ€§èƒ½äººå½¢éƒ¨ç½²**è¿™ä¸‰ä¸ªå…³é”®é—®é¢˜ã€‚",
            "timestamp": "2026-01-21T12:12:25.713405",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨è¯¦ç»†è§£æè¿™ç¯‡é¢˜ä¸ºã€ŠHiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoidsã€‹çš„è®ºæ–‡æ‰€æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯æ–¹æ³•åˆ†æ\n\nè¯¥è®ºæ–‡çš„æ ¸å¿ƒç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ª**å¯æ‰©å±•ã€ç«¯åˆ°ç«¯ã€æ— éœ€å¤–éƒ¨çŠ¶æ€ä¼°è®¡**çš„äººå½¢æœºå™¨äººæ„ŸçŸ¥è¶Šé‡æ¡†æ¶ã€‚å…¶æŠ€æœ¯åˆ›æ–°ä¸»è¦ä½“ç°åœ¨**æ„ŸçŸ¥æ¨¡æ‹Ÿã€å®‰å…¨æœºåˆ¶å’Œè®­ç»ƒç­–ç•¥**ä¸‰ä¸ªæ–¹é¢ã€‚ä»¥ä¸‹æ˜¯å…·ä½“æŠ€æœ¯æ–¹æ³•çš„è¯¦ç»†è¯´æ˜ï¼š\n\n### å…³é”®æŠ€æœ¯æ–¹æ³•åˆ—è¡¨\n\n| æ–¹æ³•åç§° | æ ¸å¿ƒç›®æ ‡ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°/å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. åŒå‘æ·±åº¦æ¨¡æ‹Ÿä¸å¯¹é½** | å¼¥åˆä»¿çœŸä¸ç°å®ï¼ˆSim-to-Realï¼‰çš„æ„ŸçŸ¥å·®è·ï¼Œå®ç°é›¶æ ·æœ¬è¿ç§»ã€‚ | **æ•°å­¦åŸç†**ï¼šå®šä¹‰ä¸¤ä¸ªå˜æ¢ç®¡é“ â„±_sim å’Œ â„±_realï¼Œç›®æ ‡æ˜¯ä½¿å¤„ç†åçš„æ·±åº¦åˆ†å¸ƒå¯¹é½ï¼š**P(â„±_sim(d_sim)) â‰ˆ P(â„±_real(d_real))**ã€‚è¿™é€šè¿‡ä¸€ç³»åˆ—å¯å¾®/å¯é€†çš„é€€åŒ–ä¸å¢å¼ºæ“ä½œå®ç°ã€‚ | **ä»¿çœŸç®¡é“ (â„±_sim)**ï¼š<br>1. **è£å‰ªä¸ç¼©æ”¾**ï¼šèšç„¦å›¾åƒä¸­å¿ƒå…³é”®ç‰¹å¾ã€‚<br>2. **è·ç¦»ç›¸å…³é«˜æ–¯å™ªå£°**ï¼š`z'_i,j = z_i,j + Ïµ, Ïµ âˆ¼ N(0, ÏƒÂ²)`ï¼Œä»…ä½œç”¨äºæœ‰æ•ˆæ„ŸçŸ¥èŒƒå›´ [d_min, d_max] å†…ã€‚<br>3. **è§†å·®ä¼ªå½±åˆæˆ**ï¼šæ¨¡æ‹ŸåŒç›®åŒ¹é…å¤±è´¥äº§ç”Ÿçš„â€œç™½è‰²åŒºåŸŸâ€ã€‚<br>4. **é«˜æ–¯æ¨¡ç³Š**ï¼šå·ç§¯æ ¸ **K** æ¨¡æ‹Ÿè¿åŠ¨æ¨¡ç³Šã€‚<br>5. **OODæ‰°åŠ¨**ï¼šä»¥æ¦‚ç‡ `P_ood` ç”¨é«˜æ–¯å™ªå£°æ›¿æ¢æ•´ä¸ªè§‚æµ‹ï¼Œæ¨¡æ‹Ÿä¼ æ„Ÿå™¨çŸ­æš‚å¤±æ•ˆã€‚<br>**ç°å®ç®¡é“ (â„±_real)**ï¼š<br>1. **æ·±åº¦ä¿®å¤**ï¼šå¡«è¡¥ç‰©ç†ä¼ æ„Ÿå™¨å› é®æŒ¡äº§ç”Ÿçš„â€œé»‘è‰²åŒºåŸŸâ€ï¼ˆé›¶å€¼åƒç´ ï¼‰ã€‚ |\n| **2. è·¨æ­¥æ—¶åºæ·±åº¦èšåˆ** | ä¸ºç­–ç•¥æä¾›æ›´é•¿çš„æ—¶åºä¸Šä¸‹æ–‡ï¼Œä»¥åº”å¯¹é«˜é€Ÿè¿åŠ¨ä¸‹çš„åœ°å½¢å˜åŒ–ã€‚ | **æ•°å­¦åŸç†**ï¼šé‡‡ç”¨**è·¨æ­¥é‡‡æ ·**æ„å»ºå†å²ç¼“å†²åŒºï¼Œå¹³è¡¡å›çœ‹çª—å£ä¸è®¡ç®—æ•ˆç‡ã€‚ç»™å®šå½“å‰å¸§ `I_t`ï¼Œå†å²åºåˆ—å®šä¹‰ä¸ºï¼š**â„‹_t = { I_{t - kÂ·â„“} \\| k = 0, 1, â€¦, m-1 }**ã€‚å…¶ä¸­ï¼Œ`k` ä¸ºå¸§ç´¢å¼•ï¼Œ`â„“` ä¸ºæ—¶é—´æ­¥é•¿ï¼Œ`m` ä¸ºæ€»å¸§æ•°ã€‚æ€»æ—¶é—´è·¨åº¦ä¸º `(m-1)Â·â„“` æ­¥ã€‚ | **å‚æ•°**ï¼š<br>- å†å²é•¿åº¦ `m = 8` å¸§ï¼ˆä¸æœ¬ä½“æ„ŸçŸ¥å†å²ä¸€è‡´ï¼‰ã€‚<br>- æ­¥é•¿ `â„“` æœªåœ¨æ­£æ–‡æ˜ç¡®ç»™å‡ºï¼Œä½†å›¾ç¤ºè¡¨æ˜å…¶å¤§äº1ï¼Œä»¥å®ç°ç¨€ç–çš„é•¿æ—¶æ„ŸçŸ¥ã€‚<br>- è®­ç»ƒæ—¶å¼•å…¥å•å¸§å»¶è¿Ÿä»¥æ¨¡æ‹Ÿç‰©ç†ä¼ æ„Ÿå™¨å»¶è¿Ÿã€‚ |\n| **3. åœ°å½¢è¾¹ç¼˜æ¥è§¦æƒ©ç½šæœºåˆ¶** | è§£å†³äººå½¢æœºå™¨äººè½è„šç‚¹ä¸ç²¾ç¡®çš„é—®é¢˜ï¼Œé¿å…è¸©åœ¨è¾¹ç¼˜å¯¼è‡´æ»‘å€’ï¼Œæå‡å®‰å…¨æ€§ã€‚ | **a) åœ°å½¢è¾¹ç¼˜æ£€æµ‹ (ç®—æ³•1)**ï¼š<br>1. **é”è¾¹æ£€æµ‹**ï¼šè®¡ç®—ä¸‰è§’ç½‘æ ¼ç›¸é‚»é¢ä¹‹é—´çš„**äºŒé¢è§’ Î±**ï¼Œè‹¥ `Î± > é˜ˆå€¼ Ï„`ï¼Œåˆ™å…±äº«è¾¹ `(v_i, v_j)` è¢«æ ‡è®°ä¸ºè¾¹ç¼˜ã€‚<br>2. **è¾¹ç¼˜å¤„ç†**ï¼šä½¿ç”¨è´ªå¿ƒæ‹¼æ¥ç­–ç•¥åˆå¹¶ç¢ç‰‡åŒ–çº¿æ®µï¼Œå‡å°‘å›¾å…ƒæ•°é‡ã€‚<br>3. **ç©ºé—´ç»“æ„æ„å»º**ï¼šå°†æ¯æ¡è¾¹ç¼˜çº¿æ®µè½¬æ¢ä¸ºåŠå¾„ä¸º `r` çš„åœ†æŸ±ä½“ï¼Œå¹¶æ„å»ºç©ºé—´ç¢°æ’ç½‘æ ¼ **ğ’®** ä»¥åŠ é€ŸæŸ¥è¯¢ã€‚<br><br>**b) ä½“ç§¯ç‚¹ç©¿é€æƒ©ç½š**ï¼š<br>åœ¨è„šéƒ¨ç¢°æ’ä½“å†…åˆ†å¸ƒä¸€ç»„ä½“ç§¯ç‚¹ **ğ’«**ã€‚æƒ©ç½šé¡¹ç»¼åˆè€ƒè™‘ç©¿é€å‡ ä½•å’ŒåŠ¨æ€çŠ¶æ€ï¼š<br>**r_vol = - Î£_{i=1}^{ \\|ğ’«\\| } \\|d_i\\| Â· (\\|v_i\\| + Ïµ)** <br>å…¶ä¸­ï¼Œ`d_i` ä¸ºç©¿é€åç§»å‘é‡ï¼Œ`v_i` ä¸ºè¯¥ç‚¹åœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„çº¿é€Ÿåº¦ï¼Œ`Ïµ = 10^{-3}` ä¸ºæ•°å€¼ç¨³å®šæ€§å¸¸æ•°ã€‚ | **å‚æ•°**ï¼š<br>- é”åº¦é˜ˆå€¼ `Ï„`ï¼ˆæœªæ˜ç¡®æ•°å€¼ï¼‰ã€‚<br>- åœ†æŸ±ä½“åŠå¾„ `r`ï¼ˆæœªæ˜ç¡®æ•°å€¼ï¼‰ã€‚<br>- ç½‘æ ¼åˆ†è¾¨ç‡ `N_grid`ï¼ˆæœªæ˜ç¡®æ•°å€¼ï¼‰ã€‚<br>- ä½“ç§¯ç‚¹æ•°é‡ `\\|ğ’«\\|`ï¼šæ ¹æ®å›¾4ï¼Œæ¯åªè„šåˆ†å¸ƒçº¦æ•°åä¸ªç‚¹ã€‚ |\n| **4. åŸºäºä½ç½®çš„å¹³åœ°åŒºåŸŸé‡‡æ ·é€Ÿåº¦æŒ‡ä»¤ç”Ÿæˆ** | è§£å†³â€œå¥–åŠ±é»‘å®¢â€é—®é¢˜ï¼ˆæœºå™¨äººåŸåœ°è½¬åœˆè€Œéç©¿è¶Šåœ°å½¢ï¼‰ï¼Œå¹¶ç¡®ä¿ç”Ÿæˆç‰©ç†ä¸Šå¯è¾¾çš„å¯¼èˆªç›®æ ‡ã€‚ | **a) å¹³åœ°åŒºåŸŸé‡‡æ · (ç®—æ³•2)**ï¼š<br>åœ¨ç¯å¢ƒç½‘æ ¼ **â„³** ä¸­éšæœºé‡‡æ ·2Dä½ç½® `(x, y)`ã€‚åœ¨ä»¥è¯¥ç‚¹ä¸ºä¸­å¿ƒã€åŠå¾„ `r` çš„åœ†å†…è¿›è¡Œå°„çº¿æŠ•å°„ï¼Œå¾—åˆ°ä¸€ç»„é«˜åº¦å€¼ `H`ã€‚è‹¥ `max(H) - min(H) < é˜ˆå€¼ Î´`ï¼Œåˆ™è®¤ä¸ºè¯¥åŒºåŸŸè¶³å¤Ÿå¹³å¦ï¼Œå°†å…¶ä¸­å¿ƒç‚¹ `(x, y, avg(H))` åŠ å…¥æœ‰æ•ˆç›®æ ‡é›† **ğ’«**ã€‚<br><br>**b) æŒ‡ä»¤ç”Ÿæˆ**ï¼š<br>å°†é€‰ä¸­çš„ç›®æ ‡ç‚¹ `p_target^B = [x_g, y_g]^T` è½¬æ¢åˆ°æœºå™¨äººåŸºåº§åæ ‡ç³»ã€‚æœŸæœ›çš„çº¿é€Ÿåº¦å’Œè§’é€Ÿåº¦æŒ‡ä»¤è®¡ç®—å¦‚ä¸‹ï¼š<br>**v_x = clip( k_v Â· x_g, 0, v_max )** <br>**Ï‰_z = clip( k_Ï‰ Â· atan2(y_g, x_g), -Ï‰_max, Ï‰_max )** <br>ä¾§å‘é€Ÿåº¦ `v_y = 0`ã€‚é€Ÿåº¦é™å€¼ `(v_max, Ï‰_max)` æ ¹æ®åœ°å½¢çš„éš¾åº¦ç±»åˆ«è‡ªé€‚åº”è°ƒæ•´ã€‚ | **å‚æ•°**ï¼š<br>- å¹³å¦åŒºåŸŸåŠå¾„ `r`ã€‚<br>- æœ€å¤§é«˜åº¦å·®é˜ˆå€¼ `Î´`ã€‚<br>- çº¿æ€§/è§’å‘åˆšåº¦ç³»æ•° `k_v`, `k_Ï‰`ã€‚<br>- ä¸ºå­¦ä¹ åŸåœ°è½¬å‘ï¼Œå®‰æ’ä¸€å°éƒ¨åˆ†åœ¨å¹³å¦åœ°å½¢ä¸Šçš„æ™ºèƒ½ä½“æ¥æ”¶çº¯è½¬å‘æŒ‡ä»¤ï¼š`v_x = 0`, `Ï‰_z` ä¸ºéšæœºå€¼ã€‚ |\n| **5. åŸºäºå¯¹æŠ—è¿åŠ¨å…ˆéªŒçš„ç­–ç•¥è®­ç»ƒ** | æå‡æœºå™¨äººæ­¥æ€çš„**è‡ªç„¶æ€§**å’Œ**æ•´ä½“è¿åŠ¨èƒ½åŠ›**ï¼Œé¿å…ä¸è‡ªç„¶çš„æŠ–åŠ¨æˆ–æ­¥æ€ã€‚ | **æ•°å­¦åŸç†**ï¼š<br>1. **æ•°æ®æº**ï¼šç»“åˆMPCç”Ÿæˆçš„åˆæˆæ•°æ®ã€åŠ¨ä½œæ•æ‰çš„äººç±»æ•°æ®ï¼ˆå«çˆ¬æ¥¼æ¢¯ç­‰å¤æ‚åŠ¨ä½œï¼‰ã€LAFANæ•°æ®é›†ä¸­çš„è·‘æ­¥æ•°æ®ï¼Œä½¿ç”¨GMRæ–¹æ³•è¿›è¡Œè¿åŠ¨é‡å®šå‘ã€‚<br>2. **åˆ¤åˆ«å™¨è¾“å…¥**ï¼šä½¿ç”¨çŸ­æ—¶åºçŠ¶æ€å¯¹ `(S_t, S_{t+1})`ï¼Œå…¶ä¸­ `S_t = [s_{t-n}, â€¦, s_t]`ï¼Œ`s_t` åŒ…å«é€Ÿåº¦ã€è§’é€Ÿåº¦ã€é‡åŠ›å‘é‡ã€å…³èŠ‚ä½ç½®/é€Ÿåº¦ç­‰ä¿¡æ¯ï¼Œä»¥æ›´å¥½åœ°æ•æ‰è¿åŠ¨çš„æ—¶é—´ç‰¹å¾ã€‚<br>3. **åˆ¤åˆ«å™¨æŸå¤±**ï¼šé‡‡ç”¨æœ€å°äºŒä¹˜æŸå¤±ï¼Œ**L_D = E_â„³[(D(S)-1)Â²] + E_ğ’«[(D(S)+1)Â²]**ï¼Œå…¶ä¸­ `â„³` ä¸ºå‚è€ƒè¿åŠ¨æ•°æ®é›†ï¼Œ`ğ’«` ä¸ºç­–ç•¥ç”Ÿæˆçš„è¿åŠ¨ã€‚<br>4. **é£æ ¼å¥–åŠ±**ï¼š**r_t = max[ 0, 1 - 0.25(D(S_t)-1)Â² ]**ã€‚è¯¥äºŒæ¬¡å¥–åŠ±å‡½æ•°æä¾›æ›´å¹³æ»‘ã€éé¥±å’Œçš„æ¢¯åº¦ï¼Œæœ‰åˆ©äºç¨³å®šè®­ç»ƒã€‚ | **å‚æ•°**ï¼š<br>- å‚è€ƒæ•°æ®é¢‘ç‡ï¼š`f = 50 Hz`ã€‚<br>- æ•°æ®é›†ï¼šè¡Œèµ°æ•°æ®é›† `ğ’Ÿ_walk`ï¼ˆMPC+äººç±»æ•°æ®ï¼Œæ€»æ—¶é•¿ `T = 379.62 s`ï¼‰ï¼Œè·‘æ­¥æ•°æ®é›† `ğ’Ÿ_run`ï¼ˆLAFANæ•°æ®ï¼Œ`T = 1.54 s`ï¼‰ã€‚<br>- å†å²å¸§æ•° `n`ï¼ˆæœªæ˜ç¡®ç»™å‡ºï¼‰ã€‚<br>- ä¸ºé˜²æ¨¡å¼åå¡Œï¼Œè¡Œèµ°å’Œè·‘æ­¥ç­–ç•¥ä½¿ç”¨ä¸åŒæ•°æ®é›†åˆ†å¼€è®­ç»ƒã€‚ |\n\n### æ€»ç»“ä¸è¯„ä»·\n\n**æŠ€æœ¯åˆ›æ–°ç‚¹**ï¼š\n1.  **å…¨æµç¨‹æ„ŸçŸ¥é—­ç¯**ï¼šä»**åŸå§‹æ·±åº¦å›¾åƒ**åˆ°**å…³èŠ‚åŠ¨ä½œ**çš„ç«¯åˆ°ç«¯æ˜ å°„ï¼Œæ‘’å¼ƒäº†ä¾èµ–æ˜“æ¼‚ç§»çš„å¤–éƒ¨çŠ¶æ€ä¼°è®¡ï¼ˆå¦‚SLAMï¼‰æˆ–ä¸­é—´è¡¨ç¤ºï¼ˆå¦‚é«˜åº¦å›¾ï¼‰ï¼Œå®ç°äº†é«˜å¸¦å®½ï¼ˆ60Hzï¼‰çš„æ„ŸçŸ¥-æ§åˆ¶å¾ªç¯ã€‚\n2.  **å¯æ‰©å±•çš„å®‰å…¨çº¦æŸ**ï¼šæå‡ºçš„â€œè¾¹ç¼˜æ£€æµ‹+ä½“ç§¯ç‚¹æƒ©ç½šâ€æœºåˆ¶æ˜¯ä¸€ç§**è½¯çº¦æŸ**ï¼Œå®ƒå¹¶éé€šè¿‡ç²¾ç¡®çš„è½¨è¿¹è§„åˆ’æ¥ä¿è¯å®‰å…¨ï¼Œè€Œæ˜¯è®©å¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨è®­ç»ƒä¸­**éšå¼åœ°**å­¦ä¼šé¿å¼€å±é™©åŒºåŸŸã€‚å…¶å¯æ‰©å±•æ€§ä½“ç°åœ¨ç®—æ³•èƒ½è‡ªåŠ¨å¤„ç†ä»»æ„ä¸‰è§’ç½‘æ ¼åœ°å½¢ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šåœ°å½¢è¿›è¡Œæ‰‹å·¥è®¾è®¡ã€‚\n3.  **å¼•å¯¼å¼è¯¾ç¨‹å­¦ä¹ **ï¼šâ€œå¹³åœ°åŒºåŸŸé‡‡æ ·â€æŒ‡ä»¤ç”Ÿæˆæ–¹æ³•æœ¬è´¨ä¸Šæ˜¯ä¸€ç§**è¯¾ç¨‹å­¦ä¹ **ã€‚å®ƒé€šè¿‡ç¡®ä¿æ¯ä¸ªè®­ç»ƒå‘¨æœŸä¸­çš„å¯¼èˆªç›®æ ‡éƒ½æ˜¯ç‰©ç†ä¸Šå¯è¾¾çš„å¹³å¦åŒºåŸŸï¼Œä¸ºæ™ºèƒ½ä½“æä¾›äº†éš¾åº¦é€‚ä¸­ä¸”æŒç»­æœ‰æ„ä¹‰çš„æ¢ç´¢ä»»åŠ¡ï¼Œä»è€Œæœ‰æ•ˆé¿å…äº†å¥–åŠ±é»‘å®¢é—®é¢˜ï¼ŒåŠ é€Ÿäº†æ”¶æ•›ã€‚\n\n**å®é™…ä»·å€¼**ï¼š\n- **éƒ¨ç½²ç®€ä¾¿æ€§**ï¼šä»…éœ€å•ç›®æ·±åº¦ç›¸æœºï¼ˆå¦‚Intel RealSense D435iï¼‰å’Œæœºè½½è®¡ç®—å•å…ƒï¼ˆå¦‚Jets",
            "timestamp": "2026-01-21T12:13:30.487729",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡ã€ŠHiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoidsã€‹çš„è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\nè®ºæ–‡åœ¨å®éªŒéƒ¨åˆ†ä½¿ç”¨äº†å®šé‡å’Œå®šæ€§ç›¸ç»“åˆçš„è¯„ä»·æŒ‡æ ‡ï¼Œä¸»è¦åˆ†ä¸º**æˆåŠŸç‡**å’Œ**æ€§èƒ½æ•ˆç‡**ä¸¤å¤§ç±»ï¼Œå…·ä½“å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æŒ‡æ ‡ç±»åˆ« | å…·ä½“æŒ‡æ ‡ | å®šä¹‰ä¸è®¡ç®—æ–¹å¼ | è¯„ä¼°ç›®çš„ä¸ç»†èŠ‚ |\n| :--- | :--- | :--- | :--- |\n| **æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡** | **æˆåŠŸç‡ (Success Rate)** | åœ¨ç‰¹å®šåœ°å½¢å’Œæ­¥æ€ä¸‹ï¼Œæœºå™¨äººæˆåŠŸå®Œæˆç©¿è¶Šä»»åŠ¡çš„è¯•éªŒæ¬¡æ•°å æ€»è¯•éªŒæ¬¡æ•°çš„ç™¾åˆ†æ¯”ã€‚ | **ç›®çš„**ï¼šè¡¡é‡ç­–ç•¥åœ¨å¤æ‚åœ°å½¢ä¸Šçš„æ•´ä½“é²æ£’æ€§å’Œå¯é æ€§ã€‚<br>**ç»†èŠ‚**ï¼š<br>1. **ä»¿çœŸè¯„ä¼°**ï¼šåœ¨æ¯ç§åœ°å½¢ä¸Šä½¿ç”¨1000ä¸ªæœºå™¨äººå¹¶è¡Œæ¨¡æ‹Ÿ10000ä¸ªæ—¶é—´æ­¥é•¿ï¼Œè®¡ç®—æˆåŠŸå®Œæˆä»»åŠ¡çš„æœºå™¨äººæ¯”ä¾‹ã€‚<br>2. **å®ç‰©è¯„ä¼°**ï¼šå¯¹æ¯ç§â€œåœ°å½¢-æ­¥æ€â€ç»„åˆè¿›è¡Œ10æ¬¡è¯•éªŒï¼Œç»Ÿè®¡æˆåŠŸæ¬¡æ•°ã€‚<br>3. **æˆåŠŸåˆ¤å®š**ï¼šä»»åŠ¡å®Œæˆä¸”æœªè§¦å‘ç»ˆæ­¢æ¡ä»¶ï¼ˆå¦‚æ‘”å€’ã€è¶…å‡ºè¾¹ç•Œï¼‰ã€‚ |\n| | **å¹³å‡åˆ°è¾¾æ—¶é—´ (Mean Reaching Time)** | æœºå™¨äººä»èµ·ç‚¹æˆåŠŸåˆ°è¾¾ç›®æ ‡ç‚¹æ‰€èŠ±è´¹æ—¶é—´çš„å¹³å‡å€¼ï¼ˆå•ä½ï¼šç§’ï¼‰ã€‚ | **ç›®çš„**ï¼šè¡¡é‡ç­–ç•¥çš„**è¿åŠ¨æ•ˆç‡**å’Œ**æ•æ·æ€§**ã€‚æ—¶é—´è¶ŠçŸ­ï¼Œè¡¨æ˜è¿åŠ¨é€Ÿåº¦è¶Šå¿«ã€è·¯å¾„è¶Šä¼˜ã€‚<br>**ç»†èŠ‚**ï¼šä»…åœ¨ä»¿çœŸæ¶ˆèå®éªŒä¸­è¿›è¡Œè®¡ç®—ï¼Œç”¨äºå¯¹æ¯”ä¸åŒæ–¹æ³•ç»„ä»¶å¯¹è¿åŠ¨æ•ˆç‡çš„å½±å“ã€‚ |\n| **å®‰å…¨æ€§ä¸“é¡¹æŒ‡æ ‡** | **å¹³å‡è„šéƒ¨ç€é™†é¢ç§¯ç™¾åˆ†æ¯” (Mean Feet Landing Area Percentage)** | è„šéƒ¨ç¢°æ’æ¨¡å‹ä¸åœ°å½¢è¡¨é¢å®é™…æ¥è§¦é¢ç§¯å è„šéƒ¨æ€»åº•é¢ç§¯çš„ç™¾åˆ†æ¯”ã€‚ | **ç›®çš„**ï¼š**å®šé‡è¯„ä¼°â€œè¾¹ç¼˜æ„ŸçŸ¥æƒ©ç½šæœºåˆ¶â€çš„æœ‰æ•ˆæ€§**ï¼Œè¡¡é‡è½è„šç‚¹çš„å®‰å…¨æ€§ã€‚<br>**ç»†èŠ‚**ï¼š<br>1. **è®¡ç®—**ï¼šåœ¨ä»¿çœŸä¸­ï¼Œé€šè¿‡è„šéƒ¨ä½“ç§¯ç‚¹ä¸åœ°å½¢çš„ç©¿é€æ·±åº¦è®¡ç®—å®é™…æ¥è§¦é¢ç§¯ã€‚<br>2. **æ„ä¹‰**ï¼šç™¾åˆ†æ¯”è¶Šé«˜ï¼Œè¯´æ˜è„šæŒè¶Šå®Œæ•´åœ°è½åœ¨å¹³å¦åŒºåŸŸï¼Œé¿å…è¸©åœ¨è¾¹ç¼˜ï¼Œç¨³å®šæ€§è¶Šé«˜ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œç”±äºæœºå™¨äººå¤§éƒ¨åˆ†æ—¶é—´åœ¨å¹³åœ°ä¸Šï¼Œè¯¥å€¼çš„ç»å¯¹å·®è·è™½å°ï¼Œä½†åœ¨è¾¹ç¼˜å¯†é›†åŒºåŸŸï¼ˆå¦‚æ¥¼æ¢¯ï¼‰çš„æ”¹å–„æ˜¯æ˜¾è‘—çš„ã€‚ |\n| **ç³»ç»Ÿç¨³å®šæ€§æŒ‡æ ‡** | **é•¿æ—¶è¿è¡Œæµ‹è¯• (Long Duration Test)** | æœºå™¨äººåœ¨æ— äººå·¥å¹²é¢„æƒ…å†µä¸‹è¿ç»­è¡Œèµ°è€Œä¸æ‘”å€’çš„æŒç»­æ—¶é—´ã€‚ | **ç›®çš„**ï¼šè¯„ä¼°ç­–ç•¥å’Œç³»ç»Ÿçš„**é•¿æœŸç¨³å®šæ€§**ä¸**è€ä¹…æ€§**ã€‚<br>**ç»†èŠ‚**ï¼šåœ¨å®ç‰©æµ‹è¯•ä¸­ï¼Œè®©æœºå™¨äººåœ¨åŒ…å«å¤šä¸ªæ¥¼æ¢¯å’Œå¹³åœ°çš„å¤æ‚è·¯çº¿ä¸Šè¿ç»­è¡Œèµ°**4åˆ†é’Ÿ**æœªå‘ç”Ÿæ‘”å€’ï¼Œè¯æ˜äº†ç³»ç»Ÿåœ¨çœŸå®å™ªå£°å’Œå·®å¼‚ä¸‹çš„æŒç»­è¿è¡Œèƒ½åŠ›ã€‚ |\n| **å®šæ€§/æ¼”ç¤ºæŒ‡æ ‡** | **æœ€å¤§è¿åŠ¨é€Ÿåº¦ (Maximum Velocity)** | æœºå™¨äººåœ¨æ‰§è¡Œä»»åŠ¡è¿‡ç¨‹ä¸­è¾¾åˆ°çš„æœ€é«˜çº¿é€Ÿåº¦ã€‚ | **ç›®çš„**ï¼šå±•ç¤ºæ¡†æ¶æ”¯æŒçš„**é«˜åŠ¨æ€è¿åŠ¨èƒ½åŠ›**ã€‚<br>**ç»†èŠ‚**ï¼šå®ç‰©æµ‹è¯•ä¸­ï¼Œæœºå™¨äººåœ¨å¥”è·‘ä¸Šé«˜å°ç­‰ä»»åŠ¡ä¸­è¾¾åˆ°äº† **2.5 m/s** çš„é€Ÿåº¦ã€‚ |\n| | **éšœç¢ç‰©é€šè¿‡èƒ½åŠ›** | èƒ½å¤ŸæˆåŠŸè·¨è¶Šçš„éšœç¢ç‰©å°ºå¯¸ï¼Œå¦‚å¹³å°é«˜åº¦ã€æ²Ÿå£‘å®½åº¦ã€‚ | **ç›®çš„**ï¼šå±•ç¤ºç­–ç•¥å¤„ç†**æç«¯åœ°å½¢**çš„èƒ½åŠ›ã€‚<br>**ç»†èŠ‚**ï¼šå®ç‰©æµ‹è¯•ä¸­ï¼ŒæˆåŠŸè·¨è¶Šäº†**32å˜ç±³é«˜**çš„å¹³å°å’Œ**50å˜ç±³å®½**çš„æ²Ÿå£‘ã€‚ |\n\n### äºŒã€ æ•°æ®é›†\nè®ºæ–‡ä¸­ä½¿ç”¨çš„æ•°æ®é›†ä¸»è¦ç”¨äº**è®­ç»ƒé˜¶æ®µ**ï¼Œç‰¹åˆ«æ˜¯ä¸ºäº†åˆ©ç”¨å¯¹æŠ—è¿åŠ¨å…ˆéªŒï¼ˆAMPï¼‰æ¥æå‡æ­¥æ€çš„è‡ªç„¶æ€§å’Œå¤šæ ·æ€§ã€‚è¿™äº›æ•°æ®é›†å¹¶éç”¨äºæµ‹è¯•è¯„ä¼°ï¼Œè€Œæ˜¯ä½œä¸ºæ¨¡ä»¿å­¦ä¹ çš„å‚è€ƒã€‚\n\n| æ•°æ®é›†åç§° | æ•°æ®æ¥æºä¸å†…å®¹ | å¤„ç†ä¸ä½¿ç”¨ç»†èŠ‚ | ç›®çš„ä¸è´¡çŒ® |\n| :--- | :--- | :--- | :--- |\n| **è¡Œèµ°æ•°æ®é›† (ğ’Ÿ_walk)** | 1. **åˆæˆæ•°æ®**ï¼šç”±æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰æ§åˆ¶å™¨ç”Ÿæˆçš„ç¨³å®šè¡Œèµ°æ¨¡å¼ã€‚<br>2. **äººç±»è¿åŠ¨æ•°æ®**ï¼šä½¿ç”¨NOKOVå…‰å­¦åŠ¨æ•ç³»ç»Ÿé‡‡é›†çš„é«˜è´¨é‡äººä½“è¿åŠ¨ï¼ŒåŒ…å«**ä¸Šä¸‹é«˜å¹³å°ã€ä¸Šä¸‹æ¥¼æ¢¯**ç­‰æŒ‘æˆ˜æ€§åŠ¨ä½œã€‚ | 1. ä½¿ç”¨**é«˜æ–¯æ··åˆå›å½’ï¼ˆGMRï¼‰** å°†äººä½“è¿åŠ¨æ•°æ®é‡å®šå‘åˆ°æœºå™¨äººæ¨¡å‹ä¸Šã€‚<br>2. å°†ä¸¤éƒ¨åˆ†æ•°æ®åˆå¹¶ï¼Œæ€»æ—¶é•¿ **T = 379.62 ç§’**ã€‚<br>3. ç”¨äºè®­ç»ƒ**è¡Œèµ°ç­–ç•¥**ã€‚ | ä¸ºç­–ç•¥æä¾›**ç¨³å®šã€æ‹Ÿäºº**çš„æ­¥æ€å‚è€ƒã€‚ç»“åˆäº†MPCçš„ç¨³å®šæ€§å’Œäººç±»è¿åŠ¨çš„è‡ªç„¶æ€§ä¸å¤æ‚æ€§ï¼Œä½¿å­¦ä¹ åˆ°çš„è¡Œèµ°æ­¥æ€æ—¢é²æ£’åˆé€¼çœŸã€‚ |\n| **å¥”è·‘æ•°æ®é›† (ğ’Ÿ_run)** | **å¥”è·‘è¿åŠ¨æ•°æ®**ï¼šä»LAFANåŠ¨æ•æ•°æ®é›†ä¸­é€‰å–çš„é«˜é€Ÿå¥”è·‘åŠ¨ä½œç‰‡æ®µã€‚ | 1. æ€»æ—¶é•¿è¾ƒçŸ­ï¼Œ**T = 1.54 ç§’**ã€‚<br>2. ç”¨äºå•ç‹¬è®­ç»ƒ**å¥”è·‘ç­–ç•¥**ã€‚ | ä¸ºç­–ç•¥æä¾›**é«˜åŠ¨æ€ã€æ•æ·**çš„æ­¥æ€å‚è€ƒã€‚ä½¿æœºå™¨äººèƒ½å¤Ÿå­¦ä¹ å¹¶æ‰§è¡Œé«˜é€Ÿå¥”è·‘åŠ¨ä½œï¼Œæ˜¯å®ç°2.5 m/sé€Ÿåº¦çš„å…³é”®ã€‚ |\n| **AMPè®­ç»ƒæ•°æ®å½¢å¼** | ä¸Šè¿°æ•°æ®é›†è¢«å¤„ç†ä¸º**çŠ¶æ€åºåˆ—**ï¼Œè€Œéå•å¸§çŠ¶æ€ã€‚ | å®šä¹‰è¿‡æ¸¡å…ƒç»„ä¸º **(ğ’_t, ğ’_{t+1})**ï¼Œå…¶ä¸­ ğ’_t = [ğ¬_{t-n}, â€¦, ğ¬_t] æ˜¯åŒ…å«nå¸§å†å²çš„çŠ¶æ€åºåˆ—ã€‚çŠ¶æ€ğ¬_tåŒ…å«åŸºåº§çº¿é€Ÿåº¦ã€è§’é€Ÿåº¦ã€é‡åŠ›å‘é‡ã€å…³èŠ‚ä½ç½®å’Œé€Ÿåº¦ã€‚ | 1. **æ•è·è¿åŠ¨æ—¶åºç‰¹å¾**ï¼šç›¸æ¯”å•å¸§ï¼Œåºåˆ—èƒ½æ›´å¥½åœ°è¡¨å¾è¿åŠ¨çš„åŠ¨æ€ç‰¹æ€§ã€‚<br>2. **å¢åŠ æ•°æ®å¤šæ ·æ€§**ï¼šå¯¹æ•°æ®é›†è¿›è¡Œäº†å¯¹ç§°æ€§å¢å¼ºã€‚<br>3. **ç¨³å®šè®­ç»ƒ**ï¼šé‡‡ç”¨æœ€å°äºŒä¹˜ï¼ˆMSEï¼‰æŸå¤±å’ŒäºŒæ¬¡å‹é£æ ¼å¥–åŠ±ï¼Œæä¾›æ›´å¹³æ»‘çš„æ¢¯åº¦ï¼Œé¿å…æ¨¡å¼å´©æºƒã€‚ |\n\n### æ€»ç»“\n- **è¯„ä»·æŒ‡æ ‡**è®¾è®¡å…¨é¢ï¼Œå…¼é¡¾äº†**æˆåŠŸç‡ã€æ•ˆç‡ã€å®‰å…¨æ€§ã€ç¨³å®šæ€§**å››ä¸ªç»´åº¦ï¼Œæœ‰æ•ˆåœ°éªŒè¯äº†æ¡†æ¶çš„å„é¡¹åˆ›æ–°ç‚¹ï¼ˆå¦‚è¾¹ç¼˜å®‰å…¨æœºåˆ¶ã€å‘½ä»¤ç”Ÿæˆæ–¹æ³•ï¼‰çš„å®é™…æ•ˆæœã€‚\n- **æ•°æ®é›†**çš„æ„å»ºä½“ç°äº†â€œåšé‡‡ä¼—é•¿â€çš„æ€è·¯ï¼Œèåˆäº†**æ§åˆ¶å™¨ç”Ÿæˆã€äººç±»åŠ¨æ•ã€å…¬å¼€æ•°æ®é›†**ç­‰å¤šç§æ¥æºï¼Œå¹¶è¿›è¡Œäº†ç²¾ç»†çš„é‡å®šå‘å’Œåºåˆ—åŒ–å¤„ç†ï¼Œæ—¨åœ¨ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è¿åŠ¨å…ˆéªŒï¼Œæ˜¯ç”Ÿæˆè‡ªç„¶ä¸”é«˜æ€§èƒ½æ­¥æ€çš„åŸºç¡€ã€‚",
            "timestamp": "2026-01-21T12:14:16.533084",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†ä¸è®­ç»ƒç¯å¢ƒ\n\nè®ºæ–‡çš„å®éªŒä¸»è¦åˆ†ä¸º**ä»¿çœŸè®­ç»ƒ**å’Œ**çœŸå®ä¸–ç•Œéƒ¨ç½²**ä¸¤éƒ¨åˆ†ï¼Œå¹¶æœªä½¿ç”¨ä¼ ç»Ÿæ„ä¹‰ä¸Šçš„é™æ€â€œæ•°æ®é›†â€è¿›è¡Œè®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡**ç¨‹åºåŒ–ç”Ÿæˆ**å’Œ**ç‰©ç†æ¨¡æ‹Ÿ**æ¥æ„å»ºè®­ç»ƒç¯å¢ƒã€‚\n\n#### 1. è®­ç»ƒç¯å¢ƒä¸æ•°æ®ç”Ÿæˆ\n- **ä»¿çœŸå¹³å°**ï¼šä½¿ç”¨ **NVIDIA Isaac Sim** å’Œ **Isaac Lab** è¿›è¡Œè®­ç»ƒã€‚\n- **åœ°å½¢ç”Ÿæˆ**ï¼šåœ¨ä»¿çœŸä¸­ç¨‹åºåŒ–ç”Ÿæˆå¤šæ ·åŒ–çš„å¤æ‚åœ°å½¢ï¼Œç”¨äºè®­ç»ƒç­–ç•¥ã€‚è¿™äº›åœ°å½¢åŒ…æ‹¬ï¼š\n    - æ¥¼æ¢¯ï¼ˆä¸Š/ä¸‹ï¼‰\n    - é«˜å¹³å°ï¼ˆä¸Š/ä¸‹ï¼‰\n    - æ–œå¡\n    - ä¸å¹³æ•´çš„è‰åœ°\n    - ç¦»æ•£çš„æ²Ÿå£‘/é—´éš™\n    - ç²—ç³™åœ°é¢\n    - ç®±å­éšœç¢\n- **å‚è€ƒè¿åŠ¨æ•°æ®ï¼ˆç”¨äºAMPè®­ç»ƒï¼‰**ï¼šä¸ºäº†æå‡æ­¥æ€çš„è‡ªç„¶æ€§å’Œæ•æ·æ€§ï¼Œè®ºæ–‡ä½¿ç”¨äº†**å¯¹æŠ—æ€§è¿åŠ¨å…ˆéªŒï¼ˆAMPï¼‰**ï¼Œå…¶å‚è€ƒæ•°æ®é›† `ğ’Ÿ` æ¥æºäºä¸‰ä¸ªæ¸ é“ï¼š\n    1.  **åˆæˆæ•°æ®**ï¼šç”±æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰æ§åˆ¶å™¨ç”Ÿæˆçš„ç¨³å®šè¡Œèµ°æ¨¡å¼ã€‚\n    2.  **äººç±»åŠ¨ä½œæ•æ‰æ•°æ®**ï¼šä½¿ç”¨NOKOVåŠ¨æ•ç³»ç»Ÿé‡‡é›†çš„é«˜è´¨é‡äººç±»è¿åŠ¨æ•°æ®ï¼ŒåŒ…æ‹¬**æ”€çˆ¬é«˜å°ã€ä¸Šä¸‹æ¥¼æ¢¯**ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚\n    3.  **è·‘æ­¥è¿åŠ¨æ•°æ®**ï¼šä» **LAFAN æ•°æ®é›†** ä¸­é€‰å–çš„é«˜é€Ÿè·‘æ­¥åŠ¨ä½œã€‚\n    - **å¤„ç†**ï¼šä½¿ç”¨GMRï¼ˆé€šç”¨è¿åŠ¨é‡å®šå‘ï¼‰æŠ€æœ¯å°†è¿™äº›äººç±»åŠ¨ä½œé‡å®šå‘åˆ°æœºå™¨äººæ¨¡å‹ä¸Šã€‚\n    - **åˆ†ç¦»è®­ç»ƒ**ï¼šä¸ºé¿å…æ¨¡å¼åå¡Œï¼Œ**è¡Œèµ°ç­–ç•¥**å’Œ**è·‘æ­¥ç­–ç•¥**æ˜¯åˆ†å¼€è®­ç»ƒçš„ï¼Œåˆ†åˆ«ä½¿ç”¨ä¸åŒçš„æ•°æ®ç»„åˆã€‚\n\n#### 2. çœŸå®ä¸–ç•Œæµ‹è¯•ç¯å¢ƒ\n- **é›¶æ ·æœ¬ï¼ˆZero-shotï¼‰ç›´æ¥éƒ¨ç½²**ï¼šå°†ä»¿çœŸä¸­è®­ç»ƒå¥½çš„ç­–ç•¥ç›´æ¥éƒ¨ç½²åˆ°çœŸå®çš„Unitree G1äººå½¢æœºå™¨äººä¸Šï¼Œåœ¨**æœªç»è®­ç»ƒçš„çœŸå®å®¤å†…å¤–åœºæ™¯**ä¸­è¿›è¡Œæµ‹è¯•ã€‚\n- **æµ‹è¯•åœºæ™¯**ï¼ˆå¦‚å›¾6æ‰€ç¤ºï¼‰ï¼š\n    - å®¤å†…/å®¤å¤–ç¯å¢ƒ\n    - å¥”è·‘ä¸Š/ä¸‹é«˜å¹³å°\n    - åœ¨è‰å¡ä¸Šå¥”è·‘\n    - ä¸Š/ä¸‹æ¥¼æ¢¯\n    - è·¨è¶Šæ·±æ²Ÿ\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\n\nè®ºæ–‡é‡‡ç”¨äº†å®šé‡å’Œå®šæ€§ç›¸ç»“åˆçš„å¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡ï¼š\n\n1.  **æˆåŠŸç‡ï¼ˆSuccess Rate, SRï¼‰**ï¼šåœ¨ç‰¹å®šåœ°å½¢ä¸Šå®Œæˆç©¿è¶Šä»»åŠ¡è€Œä¸æ‘”å€’çš„æ¯”ç‡ã€‚è¿™æ˜¯è¡¡é‡**é²æ£’æ€§**çš„æ ¸å¿ƒæŒ‡æ ‡ã€‚\n2.  **å¹³å‡åˆ°è¾¾æ—¶é—´ï¼ˆMean Reaching Timeï¼‰**ï¼šæœºå™¨äººä»èµ·ç‚¹åˆ°è¾¾ç›®æ ‡ç‚¹æ‰€èŠ±è´¹çš„å¹³å‡æ—¶é—´ã€‚ç”¨äºè¡¡é‡**è¿åŠ¨æ•ˆç‡ä¸æ•æ·æ€§**ã€‚\n3.  **å¹³å‡è„šéƒ¨ç€é™†é¢ç§¯ç™¾åˆ†æ¯”ï¼ˆMean Feet Landing Area Percentageï¼‰**ï¼šè¡¡é‡è„šæŒä¸åœ°é¢æ¥è§¦é¢ç§¯çš„æ¯”ç‡ã€‚è¯¥æŒ‡æ ‡**ä¸“é—¨ç”¨äºè¯„ä¼°è¾¹ç¼˜æ„ŸçŸ¥æƒ©ç½šæœºåˆ¶çš„æœ‰æ•ˆæ€§**ï¼Œæ•°å€¼è¶Šé«˜ä»£è¡¨è½è„šç‚¹è¶Šå®‰å…¨ã€è¶Šç¨³å®šã€‚\n4.  **æœ€å¤§è¿åŠ¨é€Ÿåº¦**ï¼šè®°å½•æœºå™¨äººèƒ½è¾¾åˆ°çš„æœ€é«˜é€Ÿåº¦ï¼Œä½œä¸º**åŠ¨æ€æ€§èƒ½**çš„ä½“ç°ï¼ˆæ–‡ä¸­è¾¾åˆ° **2.5 m/s**ï¼‰ã€‚\n5.  **é•¿æœŸç¨³å®šæ€§**ï¼šè¿›è¡Œé•¿è¾¾**4åˆ†é’Ÿ**çš„è¿ç»­è¡Œèµ°æµ‹è¯•ï¼Œè¯„ä¼°ç³»ç»Ÿåœ¨é•¿æ—¶é—´è¿è¡Œä¸‹çš„å¯é æ€§ã€‚\n6.  **æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨ä»¿çœŸå’Œç°å®ä¸­æµ‹è¯•ç­–ç•¥åœ¨**æœªè§è¿‡çš„åœ°å½¢**ï¼ˆå¦‚â€œçŸ³å¤´â€ã€â€œæœ¨æ¡©â€ï¼‰ä¸Šçš„è¡¨ç°ï¼Œè¯„ä¼°å…¶é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚\n\n### ä¸‰ã€ å…³é”®æ€§èƒ½æå‡ä¸å®éªŒç»“æœ\n\n#### 1. æ•´ä½“æ€§èƒ½\n- **é«˜æˆåŠŸç‡**ï¼šåœ¨å¤§å¤šæ•°æµ‹è¯•åœ°å½¢ä¸Šï¼Œå®Œæ•´æ–¹æ³•çš„æˆåŠŸç‡æ¥è¿‘æˆ–è¾¾åˆ° **100%** ï¼ˆå‚è§è¡¨IIï¼‰ã€‚\n- **é«˜åŠ¨æ€æ€§**ï¼šå®ç°äº†äººå½¢æœºå™¨äººåœ¨å¤æ‚åœ°å½¢ä¸Šé«˜è¾¾ **2.5 m/s** çš„å¥”è·‘é€Ÿåº¦ã€‚\n- **å¼ºæ³›åŒ–æ€§**ï¼šé€šè¿‡**é›¶æ ·æœ¬Sim-to-Realè½¬ç§»**ï¼Œåœ¨çœŸå®ä¸–ç•Œä¸­æˆåŠŸåº”å¯¹äº†æ¥¼æ¢¯ã€é«˜å°ï¼ˆ32cmï¼‰ã€æ²Ÿå£‘ï¼ˆ50cmï¼‰ç­‰å¤šç§å¤æ‚éšœç¢ã€‚\n\n#### 2. æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°å¸¦æ¥çš„æ€§èƒ½æå‡ï¼ˆåŸºäºæ¶ˆèå®éªŒï¼‰\n\n| å¯¹æ¯”é¡¹ | å…³é”®æ€§èƒ½æå‡ | å…·ä½“æ•°æ®/è¡¨ç° |\n| :--- | :--- | :--- |\n| **è¾¹ç¼˜æ„ŸçŸ¥æƒ©ç½šæœºåˆ¶ (vs. No Edge)** | **æ˜¾è‘—æå‡è½è„šå®‰å…¨æ€§å’Œç¨³å®šæ€§** | åœ¨â€œå°ç®±å­â€åœ°å½¢ä¸Šï¼ŒæˆåŠŸç‡ä» **93.17%** æå‡è‡³ **99.09%**ï¼›åœ¨â€œä¸‹æ¥¼æ¢¯â€æ—¶ï¼Œè„šéƒ¨ç€é™†é¢ç§¯ä» **87%** æå‡è‡³ **94%** ï¼ˆè¡¨Iï¼‰ã€‚æœ‰æ•ˆé˜²æ­¢äº†å› è¸©è¸è¾¹ç¼˜å¯¼è‡´çš„æ»‘å€’ï¼ˆå¦‚å›¾9ï¼‰ã€‚ |\n| **æ—¶ç©ºæ·±åº¦å†å² (vs. å•å¸§æ·±åº¦)** | **æå‡åœ¨å¤æ‚åœ°å½¢ï¼ˆå¦‚å°ç®±å­ï¼‰ä¸Šçš„é€šè¿‡èƒ½åŠ›** | åœ¨â€œå°ç®±å­â€åœ°å½¢ä¸Šï¼ŒæˆåŠŸç‡ä» **1.66%** å¤§å¹…æå‡è‡³ **99.09%**ï¼›å¹³å‡åˆ°è¾¾æ—¶é—´ä¹Ÿä» **11.68ç§’** ç¼©çŸ­è‡³ **8.03ç§’** ï¼ˆè¡¨IIï¼‰ã€‚ |\n| **åŸºäºä½ç½®çš„æŒ‡ä»¤ç”Ÿæˆ (vs. å‡åŒ€éšæœºæŒ‡ä»¤)** | **æœ‰æ•ˆè§£å†³â€œå¥–åŠ±é»‘å®¢â€é—®é¢˜ï¼Œæå‡å¯¼èˆªæ•ˆç‡** | åœ¨â€œå°ç®±å­â€åœ°å½¢ä¸Šï¼ŒæˆåŠŸç‡ä» **90.68%** æå‡è‡³ **99.09%**ï¼›å¹³å‡åˆ°è¾¾æ—¶é—´ä» **11.32ç§’** ç¼©çŸ­è‡³ **8.03ç§’** ï¼ˆè¡¨IIï¼‰ã€‚ |\n| **æ··åˆä¸“å®¶ç½‘ç»œMoE (vs. æ ‡å‡†MLP)** | **æå‡æ¨¡å‹å®¹é‡ä¸å¤šä»»åŠ¡å­¦ä¹ èƒ½åŠ›** | åœ¨â€œå°ç®±å­â€åœ°å½¢ä¸Šï¼ŒæˆåŠŸç‡ä» **84.51%** æå‡è‡³ **99.09%** ï¼ˆè¡¨IIï¼‰ã€‚ |\n| **å¯¹æŠ—æ€§è¿åŠ¨å…ˆéªŒAMP (vs. æ— AMP)** | **å¯¹å­¦ä¹ å¤æ‚æŠ€èƒ½è‡³å…³é‡è¦** | åœ¨æ²¡æœ‰AMPçš„æƒ…å†µä¸‹ï¼Œåœ¨â€œå°ç®±å­â€åœ°å½¢ä¸Šçš„æˆåŠŸç‡ç›´æ¥é™ä¸º **0%**ï¼Œå®Œå…¨æ— æ³•é€šè¿‡ï¼ˆè¡¨IIï¼‰ã€‚AMPå¯¹äºè·å¾—è‡ªç„¶ã€é«˜æ•ˆçš„æ­¥æ€æ¨¡å¼å¿…ä¸å¯å°‘ã€‚ |\n\n#### 3. å®é™…éƒ¨ç½²ä»·å€¼ä½“ç°\n- **ç³»ç»Ÿé›†æˆåº¦é«˜**ï¼šä»…ä¾èµ–ä¸€ä¸ªå‰å‘æ·±åº¦ç›¸æœºï¼ˆ60Hzï¼‰å’Œæœ¬ä½“æ„ŸçŸ¥ï¼Œæ— éœ€å¤–éƒ¨å®šä½ï¼ˆå¦‚GPSã€æ¿€å…‰SLAMï¼‰ï¼Œé™ä½äº†ç³»ç»Ÿå¤æ‚åº¦å’Œæˆæœ¬ã€‚\n- **éƒ¨ç½²ç®€ä¾¿**ï¼šä»£ç å¼€æºï¼Œä¸”ä½¿ç”¨æœºå™¨äººåŸè£…ç¡¬ä»¶ï¼ˆUnitree G1 + Intel RealSense D435iï¼‰ï¼Œ**æ— éœ€ç¡¬ä»¶æ”¹è£…**ï¼Œä¾¿äºç¤¾åŒºå¤ç°å’Œåº”ç”¨ã€‚\n- **å®æ—¶æ€§ä¼˜**ï¼šç­–ç•¥åœ¨æœºè½½Jetson Orin NXä¸Šä»¥ **50Hz** è¿è¡Œï¼Œç»“åˆ **60Hz** çš„æ·±åº¦æ„ŸçŸ¥ï¼Œå®ç°äº†ä½å»¶è¿Ÿçš„é—­ç¯æ§åˆ¶ï¼Œæ”¯æ’‘äº†é«˜é€ŸåŠ¨æ€è¿åŠ¨ã€‚\n\n**æ€»ç»“**ï¼šè¯¥è®ºæ–‡é€šè¿‡åˆ›æ–°çš„ç®—æ³•æ¡†æ¶ï¼ˆè¾¹ç¼˜å®‰å…¨æœºåˆ¶ã€åŸºäºä½ç½®çš„æŒ‡ä»¤ã€MoEã€AMPç­‰ï¼‰ï¼Œåœ¨ç¨‹åºåŒ–ç”Ÿæˆçš„å¤šæ ·åŒ–ä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œæœ€ç»ˆåœ¨çœŸå®ä¸–ç•Œå®ç°äº†**é«˜æˆåŠŸç‡ã€é«˜é€Ÿåº¦ã€å¼ºé²æ£’æ€§**çš„é›¶æ ·æœ¬éƒ¨ç½²ã€‚å…¶æ€§èƒ½æå‡åœ¨æ¶ˆèå®éªŒä¸­å¾—åˆ°äº†é‡åŒ–éªŒè¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç¦»æ•£éšœç¢å’Œæå‡è®­ç»ƒæ•ˆç‡æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚",
            "timestamp": "2026-01-21T12:15:05.208178",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡ç»“è®ºéƒ¨åˆ†ï¼ˆV. Conclusionï¼‰çš„è®¨è®ºï¼Œå½“å‰æå‡ºçš„æ¡†æ¶å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼š\n\n- **æ„ŸçŸ¥ç³»ç»Ÿè§†é‡å—é™**ï¼šç³»ç»Ÿä»…ä¾èµ–**å•ä¸ªå‰å‘æ·±åº¦ç›¸æœº**ã€‚è¿™å¯¼è‡´æœºå™¨äººç¼ºä¹**å‘åæˆ–ä¾§å‘ç§»åŠ¨**çš„èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶åœ¨éœ€è¦å…¨æ–¹ä½æœºåŠ¨ï¼ˆå¦‚åé€€é¿éšœã€æ¨ªå‘è·¨è¶Šï¼‰åœºæ™¯ä¸­çš„åº”ç”¨ã€‚\n\n- **å¤šä»»åŠ¡è®­ç»ƒä¸­çš„æ€§èƒ½æƒè¡¡**ï¼šå½“å°è¯•åœ¨**å•ä¸€ç­–ç•¥**ä¸­åŒæ—¶è®­ç»ƒå¤šç§åœ°å½¢é€‚åº”å’Œæ­¥æ€æ¨¡å¼ï¼ˆå¦‚è¡Œèµ°ä¸å¥”è·‘ï¼‰æ—¶ï¼Œè§‚å¯Ÿåˆ°**æ¨¡å¼å´©æºƒ**å’Œ**æ€§èƒ½ä¸‹é™**çš„ç°è±¡ã€‚ä¸é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®­ç»ƒçš„ä¸“ç”¨ç­–ç•¥ç›¸æ¯”ï¼Œç»Ÿä¸€ç­–ç•¥çš„èƒ½åŠ›å¯èƒ½æœ‰æ‰€å¦¥åã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nåŸºäºä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡æå‡ºäº†ç›¸åº”çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼š\n\n- **æ‰©å±•æ„ŸçŸ¥ç³»ç»Ÿ**ï¼šåœ¨æœºå™¨äººä¸Š**å®‰è£…å¤šä¸ªæ‘„åƒå¤´**ï¼ˆå¦‚å‰ã€åã€ä¾§å‘ï¼‰ï¼Œæ„å»º**å…¨å‘æ„ŸçŸ¥ç³»ç»Ÿ**ã€‚è¿™å°†ä½¿æœºå™¨äººè·å¾—360åº¦çš„ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæ”¯æŒæ›´å¤æ‚çš„æ•æ·è¿åŠ¨ï¼Œå¦‚ä¾§ç§»ã€å€’è½¦å’ŒåŸåœ°æ—‹è½¬ã€‚\n\n- **æ¢ç´¢å…ˆè¿›çš„å¤šä»»åŠ¡å­¦ä¹ æŠ€æœ¯**ï¼šç ”ç©¶æ›´å…ˆè¿›çš„**å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æŠ€æœ¯**ï¼Œä»¥å¢å¼ºå•ä¸€ç»Ÿä¸€ç­–ç•¥çš„å®¹é‡å’Œé²æ£’æ€§ã€‚ç›®æ ‡æ˜¯è®©ä¸€ä¸ªç­–ç•¥èƒ½å¤Ÿé«˜æ•ˆåœ°æŒæ¡å¤šç§æŠ€èƒ½ï¼ˆå¦‚ä¸åŒé€Ÿåº¦çš„è¡Œèµ°ã€å¥”è·‘ã€è·¨è¶Šï¼‰ï¼ŒåŒæ—¶é¿å…æ¨¡å¼å´©æºƒï¼Œä¿æŒç”šè‡³è¶…è¶Šä¸“ç”¨ç­–ç•¥çš„æ€§èƒ½ã€‚\n\n## æ€»ç»“\nè¯¥è®ºæ–‡çš„æ ¸å¿ƒå±€é™æ€§åœ¨äº**ä¼ æ„Ÿå™¨çš„ç‰©ç†é…ç½®**å’Œ**å¤šæŠ€èƒ½å­¦ä¹ çš„ç®—æ³•æŒ‘æˆ˜**ã€‚å¯¹åº”çš„æ”¹è¿›æ–¹å‘éå¸¸æ˜ç¡®ï¼šé€šè¿‡**ç¡¬ä»¶å‡çº§**å®ç°å…¨å‘æ„ŸçŸ¥ï¼Œä»¥åŠé€šè¿‡**ç®—æ³•åˆ›æ–°**æ¥çªç ´å½“å‰å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ çš„ç“¶é¢ˆã€‚è¿™äº›æ”¹è¿›å°†ä½¿ä»¿äººæœºå™¨äººå…·å¤‡æ›´æ¥è¿‘äººç±»çš„ã€åœ¨å¤æ‚éç»“æ„åŒ–ç¯å¢ƒä¸­å…¨æ–¹ä½è‡ªä¸»ç§»åŠ¨çš„èƒ½åŠ›ã€‚",
            "timestamp": "2026-01-21T12:15:18.817906",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T16:50:50Z",
    "created_at": "2026-01-21T12:09:11.477540",
    "updated_at": "2026-01-21T12:09:11.477550"
}