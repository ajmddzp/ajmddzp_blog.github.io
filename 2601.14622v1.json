{
    "id": "2601.14622v1",
    "title": "Probing Prompt Design for Socially Compliant Robot Navigation with Vision Language Models",
    "authors": [
        "Ling Xiao",
        "Toshihiko Yamasaki"
    ],
    "abstract": "è¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°åº”ç”¨äºç¤¾äº¤æœºå™¨äººå¯¼èˆªé¢†åŸŸï¼Œç„¶è€Œç°æœ‰åŸºå‡†æµ‹è¯•å¤§å¤šå¿½è§†äº†é¢å‘ç¤¾ä¼šåˆè§„è¡Œä¸ºçš„ç³»ç»Ÿæ€§æç¤ºè®¾è®¡ã€‚è¿™ä¸€å±€é™åœ¨å®è·µä¸­å°¤ä¸ºçªå‡ºï¼Œå› ä¸ºè®¸å¤šç³»ç»Ÿä¸ºæå‡æ•ˆç‡ä¾èµ–å°å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚ç›¸è¾ƒäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å†³ç­–èƒ½åŠ›è¾ƒå¼±ï¼Œè¿™ä½¿å¾—æœ‰æ•ˆçš„æç¤ºè®¾è®¡å¯¹å®ç°ç²¾å‡†å¯¼èˆªè‡³å…³é‡è¦ã€‚å—äººç±»å­¦ä¹ ä¸åŠ¨æœºè®¤çŸ¥ç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬ä»ä¸¤ä¸ªç»´åº¦ç ”ç©¶æç¤ºè®¾è®¡ï¼šç³»ç»Ÿå¼•å¯¼ï¼ˆè¡ŒåŠ¨å¯¼å‘å‹ã€æ¨ç†å¯¼å‘å‹åŠæ„ŸçŸ¥-æ¨ç†å¤åˆå‹æç¤ºï¼‰ä¸åŠ¨æœºæ¡†æ¶è®¾è®¡ï¼ˆè®©æ¨¡å‹ä¸äººç±»ã€å…¶ä»–AIç³»ç»Ÿæˆ–å…¶è¿‡å¾€ç‰ˆæœ¬è¿›è¡Œç«äº‰ï¼‰ã€‚åœ¨ä¸¤ä¸ªç¤¾ä¼šåˆè§„å¯¼èˆªæ•°æ®é›†ä¸Šçš„å®éªŒæ­ç¤ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼šé¦–å…ˆï¼Œå¯¹äºæœªç»å¾®è°ƒçš„GPT-4oæ¨¡å‹ï¼Œä¸äººç±»ç«äº‰çš„è®¾å®šè¡¨ç°æœ€ä½³ï¼Œè€Œä¸å…¶ä»–AIç³»ç»Ÿç«äº‰çš„è®¾å®šè¡¨ç°æœ€å·®ã€‚å¯¹äºç»è¿‡å¾®è°ƒçš„æ¨¡å‹ï¼Œä¸è‡ªèº«è¿‡å¾€ç‰ˆæœ¬ç«äº‰çš„è®¾å®šæ•ˆæœæœ€å¼ºï¼Œå…¶æ¬¡æ˜¯ä¸äººç±»ç«äº‰ï¼Œä¸”æ€§èƒ½è¡¨ç°è¿›ä¸€æ­¥å—åˆ°æç¤ºè®¾è®¡ã€æ¨¡å‹é€‰æ‹©ä¸æ•°æ®é›†ç‰¹æ€§ä¸‰è€…è€¦åˆæ•ˆåº”çš„å½±å“ã€‚å…¶æ¬¡ï¼Œä¸å½“çš„ç³»ç»Ÿæç¤ºè®¾è®¡å¯èƒ½å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå…¶æ•ˆæœç”šè‡³å¯èƒ½é€Šäºç›´æ¥å¾®è°ƒã€‚ç¬¬ä¸‰ï¼Œç›´æ¥å¾®è°ƒè™½èƒ½å¤§å¹…æå‡æ„ŸçŸ¥ã€é¢„æµ‹å’Œæ¨ç†ç­‰è¯­ä¹‰å±‚é¢æŒ‡æ ‡ï¼Œä½†å¯¹è¡ŒåŠ¨å‡†ç¡®ç‡çš„æ”¹å–„æœ‰é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬æå‡ºçš„ç³»ç»Ÿæç¤ºåœ¨è¡ŒåŠ¨å‡†ç¡®ç‡ä¸Šäº§ç”Ÿäº†ä¸æˆæ¯”ä¾‹çš„æ›´å¤§æå‡ï¼Œè¿™è¡¨æ˜æ‰€è®¾è®¡çš„æç¤ºæ–¹æ¡ˆä¸»è¦å‘æŒ¥å†³ç­–å±‚é¢çš„çº¦æŸä½œç”¨ï¼Œè€Œéè¡¨å¾å¢å¼ºåŠŸèƒ½ã€‚",
    "url": "https://arxiv.org/abs/2601.14622v1",
    "html_url": "https://arxiv.org/html/2601.14622v1",
    "html_content": "Probing Prompt Design for Socially Compliant Robot Navigation with Vision Language Models\nLing Xiao\n1\nToshihiko Yamasaki\n2\n1\nHokkaido University, 9-9 Kita-14-jo Nishi, Kita-ku, Sapporo, Hokkaido 060-0814, Japan\n2\nThe University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan\nling@ist.hokudai.ac.jp,\nyamasaki@cvm.t.u-tokyo.ac.jp\nAbstract\nLanguage models are increasingly used for social robot navigation, yet existing benchmarks largely overlook principled prompt design for socially compliant behavior. This limitation is particularly relevant in practice, as many systems rely on small vision language models (VLMs) for efficiency. Compared to large language models, small VLMs exhibit weaker decision-making capabilities, making effective prompt design critical for accurate navigation.\nInspired by cognitive theories of human learning and motivation, we study prompt design along two dimensions: system guidance (action-focused, reasoning-oriented, and perceptionâ€“reasoning prompts) and motivational framing, where models compete against humans, other AI systems, or their past selves.\nExperiments on two socially compliant navigation datasets reveal three key findings. First, for non-finetuned GPT-4o, competition against humans achieves the best performance, while competition against other AI systems performs worst. For finetuned models, competition against the modelâ€™s past self yields the strongest results, followed by competition against humans, with performance further influenced by coupling effects among prompt design, model choice, and dataset characteristics. Second, inappropriate system prompt design can significantly degrade performance, even compared to direct finetuning. Third, while direct finetuning substantially improves semantic-level metrics such as perception, prediction, and reasoning, it yields limited gains in action accuracy. In contrast, our system prompts produce a disproportionately larger improvement in action accuracy, indicating that the proposed prompt design primarily acts as a decision-level constraint rather than a representational enhancement.\nFigure 1:\nNon-finetuned GPT-4o performs best with reasoning-focused prompts and human-competition framing, while finetuned VLMs benefit most from perceptionâ€“reasoning prompts combined with competition against the modelâ€™s past behavior.\n1\nIntroduction\nMobile robots are increasingly deployed in a wide range of real-world applications, including healthcare and eldercare assistance, delivery and logistics, as well as security and surveillance. Many of these applications require robots to operate in public spaces shared with pedestrians, where they must interact naturally with humans and navigate safely in dynamic, socially complex environments. In such settings, it is essential for robots to exhibit socially compliant behaviors in both interaction and navigation, ensuring not only physical safety but also efficiency, comfort, and user acceptance\nFrancis\net al.\n(\n2025\n); Payandeh\net al.\n(\n2024\n)\n. First, robots must accurately perceive their surroundings, understand and anticipate human intentions, and reason under uncertainty in dynamic and cluttered environments. Moreover, they must balance navigation efficiency with social norms, safety constraints, and human comfort. Addressing these challenges requires tightly integrated perception, prediction, and planning capabilities that can adapt robot behavior to diverse and evolving interaction scenarios.\nExisting approaches to socially compliant navigation can be broadly categorized into imitation learning (IL)-based methods\nCuan\net al.\n(\n2024\n)\n, reinforcement learning (RL)-based methods\nKathuria\net al.\n(\n2025\n)\n, and vision language model (VLM)-based approaches\nSong\net al.\n(\n2024\n)\n. Among these approaches, VLM-based methods have recently attracted increasing attention due to the strong contextual understanding and commonsense reasoning capabilities of modern language models, as well as their inherent explainability. For example, VLM-Social-Nav\nSong\net al.\n(\n2024\n)\nleverages GPT-4V to generate navigation instructions conditioned on visual observations. Kawabata et al.\nKawabata\net al.\n(\n2025\n)\npropose SocialNav-MoE, an efficient mixture-of-experts VLM for socially compliant navigation trained with reinforcement fine-tuning. Wang et al.\nWang\net al.\n(\n2025\n)\nintroduce MAction-SocialNav, which explicitly addresses action ambiguity by enabling the generation of multiple plausible actions within a single scenario. Kong et al.\nKong\net al.\n(\n2025\n)\npresent AutoSpatial, a model designed for efficient spatial reasoning in social robot navigation. Narasimhan et al.\nNarasimhan\net al.\n(\n2025\n)\npropose OLiVia-Nav, which incorporates both social and environmental context during robot trajectory planning and adapts to previously unseen social scenarios. Liu et al.\nLiu\net al.\n(\n2025\n)\npropose the MUSON dataset for this task and evaluate the performance of several small VLMs on it.\nDespite these advances, existing studies primarily focus on model architectures and training strategies, while prompt design remains largely underexplored. This limitation becomes particularly critical in efficiency-oriented deployments that rely on small VLMs\nWang\net al.\n(\n2025\n)\n. In such settings, well-designed prompts play a crucial role in stimulating the decision-making capability, thereby compensating for the limited capacity of smaller models.\nIn this paper, we address this gap by systematically investigating prompt design for socially compliant robot navigation (as shown in Fig.\n1\n). Inspired by theories of human cognitive learning, we study how different types of guidance, including action focused prompts, reasoning oriented prompts, and prompts that integrate perception and reasoning, as well as different motivational framing strategies (competing against humans, other AI models, or the modelâ€™s past self), influence navigation performance.\nWe conduct experiments on two benchmark datasets and show that prompt design plays a critical role in shaping model behavior. Our results demonstrate that well designed prompts can substantially improve decision quality and social compliance, particularly in resource constrained settings based on small VLMs. Overall, the main contributions of this work are summarized as follows:\nâ€¢\nWe conduct a systematic study of prompt design for socially compliant navigation, focusing on system guidance and motivational framing.\nâ€¢\nWe show that optimal prompt configurations differ across models: reasoning-oriented prompts with competition against humans perform best for non-finetuned GPT-4o, while perceptionâ€“reasoning prompts combined with competition against the modelâ€™s past behavior are most effective for finetuned small VLMs, with performance influenced by coupling effects among prompts, models, and datasets.\nâ€¢\nWe identify a semanticâ€“decision mismatch: finetuning mainly improves semantic-level metrics, whereas our proposed system prompts yield larger gains in action accuracy, indicating that prompt design primarily functions as a decision-level constraint.\n2\nRelated Work\n2.1\nSocial Robot Navigation\nFor social robot navigation, safety is paramount\nLiang\net al.\n(\n2021\n)\n. Classical methods enforce collision constraints or fuse multi-sensor data (2-D LiDAR, depth cameras) for smooth avoidance\nLiang\net al.\n(\n2021\n)\n.\nSafety alone, however, is insufficient in human-populated spaces. Robots must also respect social norms (such as personal space, group dynamics, cultural conventions) to be perceived as acceptable and trustworthy. Traditional methods often ignore such social norms, treating pedestrians merely as dynamic obstacles.\nLearning-based approaches seek to encode both safety and social awareness. Demonstration-driven motion learning\nSun\net al.\n(\n2021\n)\nand Reinforcement Learning (RL) in simulators\nLiang\net al.\n(\n2021\n)\nshow promise but depend on large datasets or highly realistic human simulations, which rarely capture nuanced interactions, yielding policies with poor generalization.\nRecently, VLMs have opened new directions. Several VLMs are developed to generate high-level actions\nPayandeh\net al.\n(\n2024\n); Liu\net al.\n(\n2025\n); Wang\net al.\n(\n2025\n); Kawabata\net al.\n(\n2025\n)\n, evaluate trajectories\nNarasimhan\net al.\n(\n2024\n)\n, and predict directions\nSong\net al.\n(\n2024\n)\n. Datasets such as SCAND\nKarnan\net al.\n(\n2022\n)\n, MuSoHu\nNguyen\net al.\n(\n2023\n)\n, SNEI\nPayandeh\net al.\n(\n2024\n)\n, and MUSON\nLiu\net al.\n(\n2025\n)\nhave further advanced research in this domain.\n2.2\nLanguage Prompt Tuning\nLanguage prompt tuning aims to improve the practical applicability of large-scale pre-trained language models by reformulating downstream tasks into natural language instructions that align with the modelsâ€™ pretraining objectives\nBrown\net al.\n(\n2020\n); Zhou\net al.\n(\n2022\n)\n. In this paradigm, task performance critically depends on how the task is expressed through a prompt template and a verbalizer, which together define the interface between the model and the target task.\nA variety of prompt-based learning strategies have been explored, including handcrafted prompts\nXiao and Yamasaki (\n2025\n); Schick and SchÃ¼tze (\n2021\n)\n, prompt mining and paraphrasing\nJiang\net al.\n(\n2020\n)\n, gradient-based prompt search\nShin\net al.\n(\n2020\n)\n, and automatic prompt generation\nGao\net al.\n(\n2021\n)\n. While these approaches demonstrate the effectiveness of prompt-based learning, prior studies have shown that discrete prompts are often highly sensitive to wording and formatting, which can lead to unstable performance across different tasks and models\nZhao\net al.\n(\n2021\n); Liu\net al.\n(\n2024\n)\n.\nThis sensitivity is particularly problematic in decision-critical and embodied scenarios such as socially compliant navigation, where many existing methods rely on small VLMs to achieve higher efficiency. However, small VLMs typically exhibit weaker decision-making capabilities than large models, making their behavior more vulnerable to suboptimal prompt formulations. As a result, how to retain computational efficiency while improving decision accuracy remains an open challenge. In this paper, we address this challenge through principled system prompt design.\n3\nProposed Methods\n3.1\nProposed Prompts\nInspired by theories of human cognitive learning and behavioral motivation\nDiMenichi and Tricomi (\n2015\n)\n, which suggest that human decision making is strongly influenced by both instructional structure and competitive incentives, we investigate prompt design for socially compliant robot navigation along two coupled dimensions: system guidance and motivational framing. Our goal is to understand how different combinations of these two dimensions affect the decision-making behavior of small VLMs, particularly in efficiency-oriented navigation settings.\nSpecifically, we design a set of textual system prompts that vary in the type of guidance provided to the model. These include action-focused prompts that directly constrain the modelâ€™s output to navigation actions, reasoning-oriented prompts that encourage explicit deliberation before action selection, and prompts that integrate perception and reasoning to jointly model environmental understanding and decision making. In parallel, we introduce different motivational framing strategies, where the model is instructed to perform competitively against humans, other AI systems, or its own past behavior. By systematically combining different guidance types with distinct motivational signals, we construct a prompt catalog that enables controlled analysis of their individual and joint effects on navigation performance. This design allows us to examine not only which prompt configurations yield higher action accuracy, but also how prompt structure influences the alignment between perception, reasoning, and final action decisions. Detailed prompt formulations used in our experiments are summarized below.\nTextual System Prompt Catalog\nShared Base Prompt.\nYou are an intelligent assistant specializing in socially compliant robot navigation. You must understand human behaviors, infer intentions, and plan safe, smooth, and socially appropriate paths.\nPrompt Variants.\nID\nAdditional Instruction\nA1\nPerform competitively against humans.\nA2\nPerform competitively against other AI systems.\nA3\nPerform competitively against your past self.\nR1\nExplain your reasoning clearly and perform competitively against humans.\nR2\nExplain your reasoning clearly and perform competitively against other AI systems.\nR3\nExplain your reasoning clearly and perform competitively against your past self.\nPR1\nExplain your perception and reasoning clearly while performing competitively against humans.\nPR2\nExplain your perception and reasoning clearly while performing competitively against other AI systems.\nPR3\nExplain your perception and reasoning clearly while performing competitively against your past self.\n3.2\nModel Structure\nWe adopt the TinyLLaVA framework as the VLM architecture in our study.\nAs illustrated in Fig.\n2\n, the model consists of a frozen vision encoder, a trainable multimodal projector, and a trainable small language model.\nThe vision encoder extracts visual features from the input RGB image and remains frozen during training.\nThese features are then mapped into the language embedding space through the projector, which is updated to enable effective cross modal alignment.\nThe projected visual tokens, together with the language instruction tokens, are fed into the small language model to generate task specific language responses.\n4\nExperimental Setup\n4.1\nImplementation Details\nAll experiments are conducted using the TinyLLaVA framework with distributed training enabled by DeepSpeed.\nWe fine-tune all VLMs on four NVIDIA GPUs using DeepSpeed ZeRO Stage-3 for memory-efficient optimization.\nThe language model is fully fine-tuned, while the vision tower remains frozen throughout training.\nThe multimodal connector is trained end-to-end to enable effective cross-modal alignment.\nSigLIP\nZhai\net al.\n(\n2023\n)\nis adopted as the vision encoder.\nFlashAttentionÂ 2 is used to accelerate attention computation, and all models are trained in FP16 precision.\nModels are trained for 5 epochs with a per-device batch size of 2 and gradient accumulation of 4 steps,\nresulting in an effective batch size of 32.\nWe use the AdamW optimizer with an initial learning rate of\n2\nÃ—\n10\nâˆ’\n5\n2\\times 10^{-5}\n,\na cosine learning rate schedule with a warmup ratio of 3%, and no weight decay.\nThe number of data loader workers is set to 8, without modality-length-based sample grouping.\nFigure 2:\nOverview of the TinyLLaVA framework used in our experiments.\nFigure 3:\nVisualizations of the samples from SNEI and MUSON datasets.\n4.2\nDatasets.\nWe evaluate our method on SNEI\nPayandeh\net al.\n(\n2024\n)\nand MUSON\nLiu\net al.\n(\n2025\n)\ndatasets. Fig.\n3\npresents representative samples from the datasets. Specifically, SNEI contains 325 egocentric images with five-turn conversational annotations provided by four independent annotators. In our experiments, 265 samples are used for training and 60 samples are reserved for testing. MUSON consists of 800 curated egocentric samples that explicitly model both dynamic and static constraints. The dataset is annotated using a five-step chain of thought pipeline with a human and VLM and human double-checking procedure. We use 640 samples for training and 160 samples for testing.\nBoth datasets cover diverse indoor and outdoor environments with varying crowd densities and two different robot platforms, which helps mitigate the risk of overfitting to narrow annotator bias.\nTable 1:\nPerformance in terms of Action Accuracy (AA). Higher values indicate better action-level decision accuracy. The best-performing result is highlighted in bold. AA\nSNEI\n{}_{\\text{SNEI}}\ndenotes AA on the SNEI dataset, and AA\nMUSON\n{}_{\\text{MUSON}}\ndenotes AA on the MUSON dataset.\nLLM\nSystem Prompt\nPrompt Type\nVariant\nAA\nSNEI\n{}_{\\text{SNEI}}\nâ†‘\n\\uparrow\nAA\nMUSON\n{}_{\\text{MUSON}}\nâ†‘\n\\uparrow\nGPT-4o\nNo System Prompt\nâ€“\nâ€“\n0.550\n0.531\nTextual System Prompts\nâ€“\nRaw\n0.500\n0.500\nA-Series\nA1\n0.600\n0.531\nA2\n0.500\n0.531\nA3\n0.517\n0.531\nR-Series\nR1\n0.633\n0.563\nR2\n0.550\n0.538\nR3\n0.550\n0.544\nPR-Series\nPR1\n0.550\n0.550\nPR2\n0.550\n0.544\nPR3\n0.583\n0.550\nTable 2:\nPerformance on the SNEI dataset. We evaluate action accuracy and semantic similarity for perception, prediction, and reasoning.\nLLM\nPrompt\nAA (\nâ†‘\n\\uparrow\n)\nSemantic Similarity (\nâ†‘\n\\uparrow\n)\nPrompt Type\nPrompt Family\nVariant\nPerception (Q1)\nPrediction (Q2)\nReasoning (Q3)\nBERT-F1\nSBERT\nBERT-F1\nSBERT\nBERT-F1\nSBERT\nPhi-2-2.7B\nJavaheripi\net al.\n(\n2023\n)\nNo System Prompt\nâ€“\nâ€“\n0.450\n0.349\n0.504\n0.386\n0.590\n0.324\n0.707\nTextual System Prompts\nâ€“\nRaw\n0.433\n0.354\n0.593\n0.346\n0.419\n0.298\n0.623\nA-Series\nA1\n0.267\n0.358\n0.615\n0.295\n0.335\n0.313\n0.652\nA2\n0.267\n0.349\n0.610\n0.289\n0.341\n0.315\n0.648\nA3\n0.317\n0.387\n0.654\n0.332\n0.362\n0.341\n0.679\nR-Series\nR1\n0.300\n0.364\n0.632\n0.309\n0.341\n0.339\n0.664\nR2\n0.267\n0.344\n0.611\n0.275\n0.352\n0.317\n0.651\nR3\n0.350\n0.393\n0.664\n0.341\n0.373\n0.355\n0.683\nPR-Series\nPR1\n0.383\n0.357\n0.620\n0.317\n0.374\n0.326\n0.620\nPR2\n0.317\n0.352\n0.628\n0.412\n0.471\n0.373\n0.669\nPR3\n0.500\n0.376\n0.668\n0.500\n0.630\n0.402\n0.734\nStablelm-2-zephyr-1_6b\nBellagente\net al.\n(\n2024\n)\nNo System Prompt\nâ€“\nâ€“\n0.483\n0.345\n0.633\n0.372\n0.522\n0.354\n0.660\nTextual System Prompts\nâ€“\nRaw\n0.467\n0.327\n0.632\n0.359\n0.510\n0.337\n0.658\nA-Series\nA1\n0.450\n0.375\n0.612\n0.419\n0.493\n0.297\n0.626\nA2\n0.367\n0.373\n0.644\n0.463\n0.558\n0.381\n0.680\nA3\n0.467\n0.380\n0.671\n0.453\n0.527\n0.357\n0.698\nR-Series\nR1\n0.333\n0.341\n0.559\n0.407\n0.392\n0.307\n0.583\nR2\n0.317\n0.380\n0.626\n0.497\n0.538\n0.369\n0.667\nR3\n0.417\n0.364\n0.609\n0.415\n0.532\n0.320\n0.628\nPR-Series\nPR1\n0.317\n0.317\n0.601\n0.311\n0.427\n0.291\n0.579\nPR2\n0.333\n0.338\n0.632\n0.475\n0.521\n0.340\n0.661\nPR3\n0.517\n0.423\n0.632\n0.443\n0.573\n0.374\n0.675\nTinyLlama-1.1B-Chat-v1.0\nZhang\net al.\n(\n2024\n)\nNo System Prompt\nâ€“\nâ€“\n0.417\n0.360\n0.660\n0.429\n0.555\n0.398\n0.717\nTextual System Prompts\nâ€“\nRaw\n0.433\n0.304\n0.611\n0.408\n0.456\n0.385\n0.692\nA-Series\nA1\n0.383\n0.338\n0.522\n0.466\n0.506\n0.382\n0.662\nA2\n0.383\n0.333\n0.583\n0.376\n0.465\n0.380\n0.666\nA3\n0.450\n0.336\n0.596\n0.400\n0.543\n0.355\n0.648\nR-Series\nR1\n0.467\n0.283\n0.625\n0.375\n0.462\n0.352\n0.673\nR2\n0.333\n0.367\n0.611\n0.404\n0.476\n0.381\n0.688\nR3\n0.483\n0.322\n0.656\n0.378\n0.495\n0.372\n0.691\nPR-Series\nPR1\n0.433\n0.389\n0.667\n0.447\n0.496\n0.395\n0.717\nPR2\n0.317\n0.298\n0.430\n0.361\n0.418\n0.265\n0.495\nPR3\n0.500\n0.386\n0.689\n0.474\n0.547\n0.402\n0.732\nTable 3:\nPerformance on the MUSON dataset. We evaluate action accuracy and semantic similarity for perception, prediction, and reasoning.\nLLM\nPrompt\nAA (\nâ†‘\n\\uparrow\n)\nSemantic Similarity (\nâ†‘\n\\uparrow\n)\nPrompt Type\nPrompt Family\nVariant\nPerception (Q1)\nPrediction (Q2)\nReasoning (Q3)\nBERT-F1\nSBERT\nBERT-F1\nSBERT\nBERT-F1\nSBERT\nPhi-2-2.7B\nJavaheripi\net al.\n(\n2023\n)\nNo System Prompt\nâ€“\nâ€“\n0.463\n0.365\n0.487\n0.366\n0.482\n0.451\n0.729\nTextual System Prompts\nâ€“\nRaw\n0.444\n0.341\n0.453\n0.340\n0.462\n0.433\n0.713\nA-Series\nA1\n0.400\n0.347\n0.564\n0.342\n0.573\n0.386\n0.731\nA2\n0.344\n0.345\n0.553\n0.367\n0.577\n0.373\n0.723\nA3\n0.481\n0.395\n0.576\n0.398\n0.592\n0.445\n0.720\nR-Series\nR1\n0.400\n0.394\n0.569\n0.393\n0.599\n0.420\n0.743\nR2\n0.388\n0.335\n0.537\n0.374\n0.556\n0.412\n0.733\nR3\n0.463\n0.338\n0.522\n0.356\n0.540\n0.365\n0.690\nPR-Series\nPR1\n0.381\n0.321\n0.536\n0.349\n0.545\n0.293\n0.642\nPR2\n0.475\n0.370\n0.548\n0.374\n0.545\n0.391\n0.659\nPR3\n0.513\n0.398\n0.592\n0.401\n0.598\n0.456\n0.745\nStablelm-2-zephyr-1_6b\nBellagente\net al.\n(\n2024\n)\nNo System Prompt\nâ€“\nâ€“\n0.475\n0.510\n0.683\n0.465\n0.670\n0.560\n0.732\nTextual System Prompts\nâ€“\nRaw\n0.344\n0.374\n0.596\n0.377\n0.606\n0.413\n0.728\nA-Series\nA1\n0.450\n0.390\n0.569\n0.370\n0.573\n0.442\n0.689\nA2\n0.438\n0.374\n0.591\n0.386\n0.600\n0.441\n0.741\nA3\n0.469\n0.396\n0.557\n0.376\n0.577\n0.450\n0.743\nR-Series\nR1\n0.450\n0.372\n0.502\n0.350\n0.521\n0.437\n0.711\nR2\n0.425\n0.403\n0.584\n0.414\n0.606\n0.450\n0.727\nR3\n0.488\n0.384\n0.571\n0.382\n0.588\n0.450\n0.743\nPR-Series\nPR1\n0.319\n0.308\n0.564\n0.290\n0.544\n0.403\n0.706\nPR2\n0.331\n0.345\n0.586\n0.354\n0.593\n0.406\n0.716\nPR3\n0.494\n0.513\n0.691\n0.486\n0.696\n0.570\n0.742\nTinyLlama-1.1B-Chat-v1.0\nZhang\net al.\n(\n2024\n)\nNo System Prompt\nâ€“\nâ€“\n0.375\n0.361\n0.609\n0.375\n0.600\n0.454\n0.766\nTextual System Prompts\nâ€“\nRaw\n0.344\n0.337\n0.583\n0.361\n0.585\n0.416\n0.725\nA-Series\nA1\n0.381\n0.298\n0.568\n0.360\n0.564\n0.416\n0.685\nA2\n0.419\n0.383\n0.611\n0.397\n0.610\n0.452\n0.748\nA3\n0.450\n0.428\n0.642\n0.440\n0.620\n0.485\n0.744\nR-Series\nR1\n0.538\n0.447\n0.651\n0.433\n0.634\n0.494\n0.764\nR2\n0.531\n0.457\n0.672\n0.448\n0.650\n0.513\n0.771\nR3\n0.544\n0.458\n0.691\n0.452\n0.668\n0.528\n0.797\nPR-Series\nPR1\n0.425\n0.467\n0.653\n0.442\n0.659\n0.488\n0.743\nPR2\n0.269\n0.447\n0.654\n0.448\n0.643\n0.490\n0.759\nPR3\n0.563\n0.497\n0.686\n0.475\n0.663\n0.490\n0.761\nFigure 4:\nVisualization of perception, prediction, reasoning, and decision making for the finetuned model using TinyLlama-1.1B-Chat-v1.0 as the language model. The results indicate that prompts integrating perception and reasoning with competition against the modelâ€™s past behavior achieve the best overall performance.\nFigure 5:\nVisualization of predicted actions using TinyLlama 1.1B Chat v1.0 as the language model. In relatively easy scenarios, for example the middle image in the top row, all prompt settings produce correct decisions. In more challenging cases, such as crowded scenes or blurry observations, the prompt that integrates perception and reasoning and frames competition against the modelâ€™s past self yields more accurate and safer action decisions.\n4.3\nEvaluation Metrics.\nSemantic Alignment (token-level)\n: BERT-F1.\nIt measures semantic similarity between a generated prediction and a reference text using contextualized token embeddings.\nLet the ground truth text be\ny\nt\ngt\n=\n{\nt\n1\n,\nâ€¦\n,\nt\nm\n}\ny_{t}^{\\text{gt}}=\\{t_{1},\\dots,t_{m}\\}\nand the predicted text be\ny\nt\npred\n=\n{\np\n1\n,\nâ€¦\n,\np\nn\n}\ny_{t}^{\\text{pred}}=\\{p_{1},\\dots,p_{n}\\}\n, where each token is represented by a contextual embedding.\nToken level similarity is computed using cosine similarity between embeddings.\nBERT-F1 is defined as\nBERT-F1\n=\n2\nâ‹…\nBERT-P\nâ‹…\nBERT-R\nBERT-P\n+\nBERT-R\n,\n\\displaystyle=\\frac{2\\cdot\\text{BERT-P}\\cdot\\text{BERT-R}}{\\text{BERT-P}+\\text{BERT-R}},\n(1)\nBERT-P\n=\n1\nn\nâ€‹\nâˆ‘\ni\n=\n1\nn\nmax\n1\nâ‰¤\nj\nâ‰¤\nm\nâ¡\ncos\nâ¡\n(\np\ni\n,\nt\nj\n)\n,\n\\displaystyle=\\frac{1}{n}\\sum_{i=1}^{n}\\max_{1\\leq j\\leq m}\\cos(p_{i},t_{j}),\nBERT-R\n=\n1\nm\nâ€‹\nâˆ‘\nj\n=\n1\nm\nmax\n1\nâ‰¤\ni\nâ‰¤\nn\nâ¡\ncos\nâ¡\n(\nt\nj\n,\np\ni\n)\n.\n\\displaystyle=\\frac{1}{m}\\sum_{j=1}^{m}\\max_{1\\leq i\\leq n}\\cos(t_{j},p_{i}).\nHere,\nBERT-P\nmeasures the extent to which the generated content is supported by the reference, penalizing irrelevant or hallucinated additions, while\nBERT-R\nmeasures how well the generation covers key information in the reference, penalizing omissions.\nThe\nBERT-F1\nscore provides a balanced assessment by jointly considering precision and recall.\nSemantic Alignment (sentence-level)\n: SBERT. It computes semantic closeness between a prediction and the ground truth using sentence level embeddings produced by Sentence BERT\nReimers and Gurevych (\n2019\n)\n.\nLet\nğ²\ns\npred\n\\mathbf{y}_{s}^{\\text{pred}}\nand\nğ²\ns\ngt\n\\mathbf{y}_{s}^{\\text{gt}}\ndenote the sentence embeddings of the prediction and ground truth, respectively.\nThe similarity is defined as\nSBERT\n=\ncos\nâ¡\n(\nğ²\ns\npred\n,\nğ²\ns\ngt\n)\n=\nğ²\ns\npred\nâ‹…\nğ²\ns\ngt\nâ€–\nğ²\ns\npred\nâ€–\nâ€‹\nâ€–\nğ²\ns\ngt\nâ€–\n.\n\\text{SBERT}=\\cos(\\mathbf{y}_{s}^{\\text{pred}},\\mathbf{y}_{s}^{\\text{gt}})=\\frac{\\mathbf{y}_{s}^{\\text{pred}}\\cdot\\mathbf{y}_{s}^{\\text{gt}}}{\\|\\mathbf{y}_{s}^{\\text{pred}}\\|\\,\\|\\mathbf{y}_{s}^{\\text{gt}}\\|}.\n(2)\nAction Safety\n: Action Accuracy (AA).\nThis score evaluates decision level correctness by assessing whether the robot action inferred from the model output matches the ground truth action.\nLet\ny\nc\npred\ny_{c}^{\\text{pred}}\nand\ny\nc\ngt\ny_{c}^{\\text{gt}}\ndenote the predicted and ground truth action labels for the\nc\nc\nth sample, and let\nN\nN\ndenote the total number of samples.\nAction Accuracy is defined as\nAA\n=\n1\nN\nâ€‹\nâˆ‘\nc\n=\n1\nN\nğ•€\nâ€‹\n(\ny\nc\npred\n=\ny\nc\ngt\n)\n,\n\\text{AA}=\\frac{1}{N}\\sum_{c=1}^{N}\\mathbb{I}\\!\\left(y_{c}^{\\text{pred}}=y_{c}^{\\text{gt}}\\right),\n(3)\nwhere\nğ•€\nâ€‹\n(\nâ‹…\n)\n\\mathbb{I}(\\cdot)\nis the indicator function.\nUnlike semantic similarity metrics,\nAA\ndirectly reflects whether the model selects the correct control decision.\n5\nExperimental Results\n5.1\nMain Results.\nNon-finetuned Model.\nWe first investigate the effectiveness of the proposed prompt engineering strategies in a zero shot VLM setting. Since GPT-4o currently represents one of the strongest general purpose VLMs, we adopt it as the evaluation model for this analysis.\nTo ensure a fair and consistent comparison across different prompt designs, we constrain the model action space to a fixed set of discrete navigation commands. This design choice enables direct and unambiguous evaluation of the generated actions while preserving the model internal reasoning process. Specifically, the allowable actions include â€œMove Forwardâ€, â€œForward Leftâ€, â€œForward Rightâ€, â€œStopâ€, â€œTurn Leftâ€, and â€œTurn Rightâ€. The results are summarized in Table\n1\n. The â€œNo System Promptâ€ setting indicates that no prefixed instruction is provided and the model receives only the input image and the user query without additional guidance or task framing. This configuration evaluates the inherent ability of GPT-4o to perform socially compliant navigation in the absence of explicit prompting.\nOverall, for non-finetuned GPT-4o, competition against humans yields the best performance, whereas competition against other AI models results in the worst performance. However, performance is also influenced by the interaction among prompt design, the language model, and the dataset.\nInterestingly, across both datasets, a naive system prompt performs worse than the noâ€“system-prompt baseline, indicating that the use of system prompts is not universally beneficial. This finding highlights that prompt engineering must be carefully designed, as inappropriate or weakly aligned prompts can degrade model performance rather than improve it.\nFinetuned Small VLMs.\nWe further evaluate finetuned small VLMs, with results summarized in Tables\n3\nand\n3\n. Across different language models, competition against the modelâ€™s past self consistently yields the best performance, followed by competition against humans, while competition against other AI systems remains the least effective. Nevertheless, performance is still influenced by residual coupling effects among datasets, model variants, and prompt designs. Notably, the proposed system prompts lead to substantially larger improvements in action accuracy than in the semantic quality of generated language responses. This suggests that the proposed prompts mainly constrains the final action selection process, improving the alignment between model reasoning and discrete action decisions, rather than broadly enhancing semantic expressiveness.\n5.2\nVisualizations.\nAs shown in Fig.\n5\n, we qualitatively compare perception, prediction, reasoning, and decision-making behaviors of a finetuned TinyLlama-1.1B-Chat-v1.0 model under different prompt strategies on the SNEI dataset. Without a system prompt, the model often exhibits weakly grounded reasoning, leading to unsafe navigation actions and potential human collisions. In contrast, prompts that integrate perception and reasoning and encourage competition against the modelâ€™s past behavior yield more accurate scene understanding, more coherent human motion predictions, and safer decisions. Fig.\n5\nfurther visualizes the resulting action predictions under different prompt settings.\n6\nLimitations and Future Work\nFirst, while the datasets used in this study enable controlled evaluation, their limited size may not fully capture the diversity of real-world social navigation scenarios, such as snowy weather, nighttime conditions, or situations involving traffic signals. Future work will consider larger and more diverse datasets to better assess generalization.\nThen, we focus on text-based prompt design and do not explore adaptive or learned prompting mechanisms. Incorporating online prompt adaptation and reinforcement learning to respond to environmental feedback is an important direction for future research.\nFinally, our experiments assume a constrained, discrete action space, which simplifies comparison but limits realism. Extending our analysis to continuous control and more complex embodied environments remains future work.\n7\nConclusions\nIn this paper, we study prompt design for socially compliant navigation in efficiency-oriented settings based on small VLMs. Inspired by theories of human cognitive learning and behavioral motivation, we investigate system guidance and motivational framing as two key dimensions of prompt design.\nExperiments on the SNEI and MUSON datasets show that optimal prompt configurations differ across models. For zero-shot GPT-4o, reasoning-oriented prompts with competition against humans achieve the best performance, whereas for finetuned small VLMs, prompts that integrate perception and reasoning combined with competition against the modelâ€™s past behavior perform best. Nevertheless, action accuracy is also influenced by coupling effects among prompt design, model choice, and dataset characteristics, leading to occasional exceptions.\nWe further show that system prompts are not universally beneficial: poorly designed prompts can underperform a noâ€“system-prompt baseline. Moreover, while finetuning primarily improves semantic-level understanding, our proposed prompts yield larger gains in action accuracy, indicating that prompt design mainly acts as a decision-level constraint.\nOverall, our results demonstrate that principled, motivation-aware prompt design can substantially improve decision accuracy and social compliance without increasing model size, offering practical guidance for deploying small VLMs in real-world robot navigation.\nAcknowledgments\nThis paper is financially supported by JSPS KAKENHI Grant Number 24K20787.\nReferences\nM. Bellagente, J. Tow, D. Mahan, D. Phung, M. Zhuravinskyi, R. Adithyan, J. Baicoianu, B. Brooks, N. Cooper, A. Datta,\net al.\n(2024)\nStable lm 2 1.6 b technical report\n.\narXiv preprint arXiv:2402.17834\n.\nCited by:\nTable 3\n,\nTable 3\n.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\net al.\n(2020)\nLanguage models are few-shot learners\n.\nIn\nProceedings of the Advances in Neural Information Processing Systems (NeurIPS)\n,\nVol.\n33\n,\npp.Â 1877â€“1901\n.\nCited by:\nÂ§2.2\n.\nC. Cuan, T. E. Lee, E. Fisher, A. Francis, L. Takayama, T. Zhang, A. Toshev, and S. Pirk (2024)\nGesture2path: imitation learning for gesture-aware navigation\n.\nIn\nProceedings of the International Conference on Social Robotics (ICSR)\n,\npp.Â 264â€“279\n.\nCited by:\nÂ§1\n.\nB. C. DiMenichi and E. Tricomi (2015)\nThe power of competition: effects of social motivation on attention, sustained physical effort, and learning\n.\nFrontiers in psychology\n6\n,\npp.Â 1282\n.\nCited by:\nÂ§3.1\n.\nA. Francis, C. PÃ©rez-dâ€™Arpino, C. Li, F. Xia, A. Alahi, R. Alami, A. Bera, A. Biswas, J. Biswas, R. Chandra,\net al.\n(2025)\nPrinciples and guidelines for evaluating social robot navigation algorithms\n.\nACM Transactions on Human-Robot Interaction\n14\n(\n2\n),\npp.Â 1â€“65\n.\nCited by:\nÂ§1\n.\nT. Gao, A. Fisch, and D. Chen (2021)\nMaking pre-trained language models better few-shot learners\n.\nIn\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP)\n,\npp.Â 3816â€“3830\n.\nCited by:\nÂ§2.2\n.\nM. Javaheripi, S. Bubeck, M. Abdin, J. Aneja, S. Bubeck, C. C. T. Mendes, W. Chen, A. Del Giorno, R. Eldan, S. Gopi,\net al.\n(2023)\nPhi-2: the surprising power of small language models\n.\nMicrosoft Research Blog\n1\n(\n3\n),\npp.Â 3\n.\nCited by:\nTable 3\n,\nTable 3\n.\nZ. Jiang, F. F. Xu, J. Araki, and G. Neubig (2020)\nHow can we know what language models know?\n.\nTransactions of the Association for Computational Linguistics\n8\n,\npp.Â 423â€“438\n.\nCited by:\nÂ§2.2\n.\nH. Karnan, A. Nair, X. Xiao, G. Warnell, S. Pirk, A. Toshev, J. Hart, J. Biswas, and P. Stone (2022)\nSocially compliant navigation dataset (scand): a large-scale dataset of demonstrations for social navigation\n.\nIEEE Robotics and Automation Letters\n7\n(\n4\n),\npp.Â 11807â€“11814\n.\nCited by:\nÂ§2.1\n.\nT. Kathuria, K. Liu, J. Jang, X. J. Yang, and M. Ghaffari (2025)\nLearning implicit social navigation behavior using deep inverse reinforcement learning\n.\nIEEE Robotics and Automation Letters\n10\n(\n5\n),\npp.Â 5146â€“5153\n.\nCited by:\nÂ§1\n.\nT. Kawabata, X. Zhang, and L. Xiao (2025)\nSocialNav-moe: a mixture-of-experts vision language model for socially compliant navigation with reinforcement fine-tuning\n.\narXiv preprint arXiv:2512.14757\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nY. Kong, D. Song, J. Liang, D. Manocha, Z. Yao, and X. Xiao (2025)\nAutospatial: visual-language reasoning for social robot navigation through efficient spatial reasoning learning\n.\narXiv preprint arXiv:2503.07557\n.\nCited by:\nÂ§1\n.\nJ. Liang, U. Patel, A. J. Sathyamoorthy, and D. Manocha (2021)\nCrowd-steer: realtime smooth and collision-free robot navigation in densely crowded scenarios trained using high-fidelity simulation\n.\nIn\nProceedings of the 29th International Conference on International Joint Conferences on Artificial Intelligence (IJCAI)\n,\npp.Â 4221â€“4228\n.\nCited by:\nÂ§2.1\n,\nÂ§2.1\n.\nX. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang (2024)\nGPT understands, too\n.\nAI Open\n5\n,\npp.Â 208â€“215\n.\nCited by:\nÂ§2.2\n.\nZ. Liu, X. Zhang, Z. Wang, T. Kawabata, X. Xiao, and L. Xiao (2025)\nMUSON: a reasoning-oriented multimodal dataset for socially compliant navigation in urban environments\n.\narXiv preprint arXiv:2512.22867\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nÂ§4.2\n.\nS. Narasimhan, A. H. Tan, D. Choi, and G. Nejat (2024)\nOlivia-nav: an online lifelong vision language approach for mobile robot social navigation\n.\narXiv preprint arXiv:2409.13675\n.\nCited by:\nÂ§2.1\n.\nS. Narasimhan, A. H. Tan, D. Choi, and G. Nejat (2025)\nOlivia-nav: an online lifelong vision language approach for mobile robot social navigation\n.\nIn\nProceedings of the IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 9130â€“9137\n.\nCited by:\nÂ§1\n.\nD. M. Nguyen, M. Nazeri, A. Payandeh, A. Datar, and X. Xiao (2023)\nToward human-like social robot navigation: a large-scale, multi-modal, social human navigation dataset\n.\nIn\nProceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 7442â€“7447\n.\nCited by:\nÂ§2.1\n.\nA. Payandeh, D. Song, M. Nazeri, J. Liang, P. Mukherjee, A. H. Raj, Y. Kong, D. Manocha, and X. Xiao (2024)\nSocial-llava: enhancing robot navigation through human-language reasoning in social spaces\n.\narXiv preprint arXiv:2501.09024\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nÂ§4.2\n.\nN. Reimers and I. Gurevych (2019)\nSentence-bert: sentence embeddings using siamese bert-networks\n.\nIn\nProceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\n,\npp.Â 3982\n.\nCited by:\nÂ§4.3\n.\nT. Schick and H. SchÃ¼tze (2021)\nItâ€™s not just size that matters: small language models are also few-shot learners\n.\nIn\nProceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)\n,\npp.Â 2339â€“2352\n.\nCited by:\nÂ§2.2\n.\nT. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh (2020)\nAutoPrompt: eliciting knowledge from language models with automatically generated prompts\n.\nIn\nProceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)\n,\npp.Â 4222â€“4235\n.\nCited by:\nÂ§2.2\n.\nD. Song, J. Liang, A. Payandeh, A. H. Raj, X. Xiao, and D. Manocha (2024)\nVlm-social-nav: socially aware robot navigation through scoring using vision-language models\n.\nIEEE Robotics and Automation Letters\n10\n(\n1\n),\npp.Â 508â€“515\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nH. Sun, W. Zhang, R. Yu, and Y. Zhang (2021)\nMotion planning for mobile robotsâ€”focusing on deep reinforcement learning: a systematic review\n.\nIEEE Access\n9\n,\npp.Â 69061â€“69081\n.\nCited by:\nÂ§2.1\n.\nZ. Wang, X. Zhang, Z. Liu, T. Kawabata, D. Song, X. Xiao, and L. Xiao (2025)\nMAction-socialnav: multi-action socially compliant navigation via reasoning-enhanced prompt tuning\n.\narXiv preprint arXiv:2512.21722\n.\nCited by:\nÂ§1\n,\nÂ§1\n,\nÂ§2.1\n.\nL. Xiao and T. Yamasaki (2025)\nLLM-advisor: an llm benchmark for cost-efficient path planning across multiple terrains\n.\narXiv preprint arXiv:2503.01236\n.\nCited by:\nÂ§2.2\n.\nX. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer (2023)\nSigmoid loss for language image pre-training\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)\n,\npp.Â 11975â€“11986\n.\nCited by:\nÂ§4.1\n.\nP. Zhang, G. Zeng, T. Wang, and W. Lu (2024)\nTinyllama: an open-source small language model\n.\narXiv preprint arXiv:2401.02385\n.\nCited by:\nTable 3\n,\nTable 3\n.\nZ. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh (2021)\nCalibrate before use: improving few-shot performance of language models\n.\nIn\nProceedings of the International Conference on Machine Learning (ICML)\n,\npp.Â 12697â€“12706\n.\nCited by:\nÂ§2.2\n.\nK. Zhou, J. Yang, C. C. Loy, and Z. Liu (2022)\nLearning to prompt for vision-language models\n.\nInternational Journal of Computer Vision\n130\n(\n9\n),\npp.Â 2337â€“2348\n.\nCited by:\nÂ§2.2\n.",
    "preview_text": "Language models are increasingly used for social robot navigation, yet existing benchmarks largely overlook principled prompt design for socially compliant behavior. This limitation is particularly relevant in practice, as many systems rely on small vision language models (VLMs) for efficiency. Compared to large language models, small VLMs exhibit weaker decision-making capabilities, making effective prompt design critical for accurate navigation. Inspired by cognitive theories of human learning and motivation, we study prompt design along two dimensions: system guidance (action-focused, reasoning-oriented, and perception-reasoning prompts) and motivational framing, where models compete against humans, other AI systems, or their past selves. Experiments on two socially compliant navigation datasets reveal three key findings. First, for non-finetuned GPT-4o, competition against humans achieves the best performance, while competition against other AI systems performs worst. For finetuned models, competition against the model's past self yields the strongest results, followed by competition against humans, with performance further influenced by coupling effects among prompt design, model choice, and dataset characteristics. Second, inappropriate system prompt design can significantly degrade performance, even compared to direct finetuning. Third, while direct finetuning substantially improves semantic-level metrics such as perception, prediction, and reasoning, it yields limited gains in action accuracy. In contrast, our system prompts produce a disproportionately larger improvement in action accuracy, indicating that the proposed prompt design primarily acts as a decision-level constraint rather than a representational enhancement.\n\nProbing Prompt Design for Socially Compliant Robot Navigation with Vision Language Models\nLing Xiao\n1\nToshihiko Yamasaki\n2\n1\nHokkaido University, 9-9 Kita-14-jo Nishi, Kita-ku, Sapporo, Hokkaido 060-0814, Japan\n2\nThe University of Tokyo, 7",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "VLM",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶å¦‚ä½•é€šè¿‡æç¤ºè®¾è®¡ä¼˜åŒ–è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç¤¾äº¤æœºå™¨äººå¯¼èˆªä¸­çš„å†³ç­–æ€§èƒ½ï¼Œä»¥æé«˜åŠ¨ä½œå‡†ç¡®æ€§ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦\n\n**è®ºæ–‡æ ‡é¢˜ï¼š** é¢å‘ç¤¾ä¼šåˆè§„æœºå™¨äººå¯¼èˆªçš„è§†è§‰è¯­è¨€æ¨¡å‹æç¤ºè®¾è®¡æ¢ç©¶\n\n**ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœºï¼š**\néšç€ç§»åŠ¨æœºå™¨äººåœ¨å…¬å…±ç©ºé—´ï¼ˆå¦‚åŒ»ç–—ã€ç‰©æµï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œå…¶å¯¼èˆªè¡Œä¸ºéœ€å…¼å…·å®‰å…¨æ€§ä¸ç¤¾ä¼šåˆè§„æ€§ï¼ˆå¦‚å°Šé‡ç¤¾äº¤è·ç¦»ã€é¢„æµ‹è¡Œäººæ„å›¾ï¼‰ã€‚åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–¹æ³•å› å…¶å¼ºå¤§çš„ä¸Šä¸‹æ–‡ç†è§£å’Œæ¨ç†èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶å¤šé›†ä¸­äºæ¨¡å‹æ¶æ„ä¸è®­ç»ƒç­–ç•¥ï¼Œ**æç¤ºè®¾è®¡**è¿™ä¸€å…³é”®ç¯èŠ‚å´è¢«å¿½è§†ã€‚å°¤å…¶åœ¨è¿½æ±‚éƒ¨ç½²æ•ˆç‡ã€ä¾èµ–**å°å‹VLM**çš„åœºæ™¯ä¸­ï¼Œç”±äºå…¶å†³ç­–èƒ½åŠ›è¾ƒå¼±ï¼Œç²¾å¿ƒè®¾è®¡çš„æç¤ºå¯¹äºæ¿€å‘æ¨¡å‹æ½œåŠ›ã€è¡¥å¿å…¶èƒ½åŠ›çŸ­æ¿è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ—¨åœ¨ç³»ç»Ÿæ€§åœ°ç ”ç©¶æç¤ºè®¾è®¡å¦‚ä½•å½±å“ç¤¾ä¼šåˆè§„å¯¼èˆªçš„æ€§èƒ½ã€‚\n\n**æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°ï¼š**\næœ¬æ–‡çš„åˆ›æ–°ç‚¹åœ¨äºä»**äººç±»è®¤çŸ¥å­¦ä¹ ä¸è¡Œä¸ºåŠ¨æœºç†è®º**ä¸­è·å¾—çµæ„Ÿï¼Œç³»ç»Ÿæ€§åœ°æ„å»ºå¹¶æ¢ç©¶äº†æç¤ºè®¾è®¡çš„ä¸¤ä¸ªç»´åº¦ï¼š\n1.  **ç³»ç»Ÿå¼•å¯¼**ï¼šè®¾è®¡äº†ä¸‰ç§å¼•å¯¼ç±»å‹çš„æç¤ºï¼š\n    *   **è¡ŒåŠ¨èšç„¦å‹**ï¼šç›´æ¥çº¦æŸæ¨¡å‹è¾“å‡ºå¯¼èˆªåŠ¨ä½œã€‚\n    *   **æ¨ç†å¯¼å‘å‹**ï¼šé¼“åŠ±æ¨¡å‹åœ¨è¡ŒåŠ¨å‰è¿›è¡Œæ˜ç¡®æ¨ç†ã€‚\n    *   **æ„ŸçŸ¥-æ¨ç†ç»“åˆå‹**ï¼šè¦æ±‚æ¨¡å‹åŒæ—¶è§£é‡Šç¯å¢ƒæ„ŸçŸ¥å’Œå†³ç­–æ¨ç†ã€‚\n2.  **åŠ¨æœºæ¡†æ¶**ï¼šå¼•å…¥äº†ä¸‰ç§ç«äº‰æ€§æ¿€åŠ±ç­–ç•¥ï¼ŒæŒ‡ç¤ºæ¨¡å‹ä¸**äººç±»**ã€**å…¶ä»–AIç³»ç»Ÿ**æˆ–**å…¶è¿‡å»çš„è‡ªèº«è¡¨ç°**è¿›è¡Œç«äº‰ã€‚\né€šè¿‡ç»„åˆè¿™ä¸¤ä¸ªç»´åº¦ï¼ˆå¦‚â€œæ¨ç†å¯¼å‘+ä¸äººç±»ç«äº‰â€ï¼‰ï¼Œæ„å»ºäº†ä¸€ä¸ªç³»ç»Ÿçš„æç¤ºç›®å½•ï¼Œä»¥åˆ†æå…¶å¯¹å°å‹VLMå¯¼èˆªå†³ç­–çš„ä¸ªä½“ä¸è”åˆå½±å“ã€‚\n\n**ä¸»è¦å®éªŒç»“æœï¼š**\nåœ¨SNEIå’ŒMUSONä¸¤ä¸ªç¤¾ä¼šåˆè§„å¯¼èˆªæ•°æ®é›†ä¸Šçš„å®éªŒæ­ç¤ºäº†å…³é”®å‘ç°ï¼š\n1.  **æ¨¡å‹ç‰¹å¼‚æ€§**ï¼šæœ€ä¼˜æç¤ºé…ç½®å› æ¨¡å‹è€Œå¼‚ã€‚å¯¹äº**æœªå¾®è°ƒçš„GPT-4o**ï¼Œâ€œæ¨ç†å¯¼å‘+ä¸äººç±»ç«äº‰â€æç¤ºæ•ˆæœæœ€ä½³ï¼›è€Œå¯¹äº**å¾®è°ƒåçš„å°å‹VLM**ï¼ˆå¦‚TinyLlamaï¼‰ï¼Œâ€œæ„ŸçŸ¥-æ¨ç†ç»“åˆ+ä¸è‡ªèº«è¿‡å»ç«äº‰â€æç¤ºæœ€ä¸ºæœ‰æ•ˆã€‚\n2.  **æç¤ºè®¾è®¡çš„åŒåˆƒå‰‘æ•ˆåº”**ï¼šä¸æ°å½“çš„æç¤ºè®¾è®¡ï¼ˆå¦‚ç®€å•çš„ç³»ç»Ÿæç¤ºï¼‰ç”šè‡³å¯èƒ½æ¯”ä¸ä½¿ç”¨ä»»ä½•ç³»ç»Ÿæç¤ºè¡¨ç°æ›´å·®ï¼Œå¼ºè°ƒäº†ç²¾å¿ƒè®¾è®¡çš„é‡è¦æ€§ã€‚\n3.  **è¯­ä¹‰-å†³ç­–é”™é…ç°è±¡**ï¼šå¾®è°ƒä¸»è¦æå‡çš„æ˜¯**è¯­ä¹‰å±‚é¢**çš„æŒ‡æ ‡ï¼ˆå¦‚æ„ŸçŸ¥ã€é¢„æµ‹ã€æ¨ç†çš„æ–‡æœ¬ç›¸ä¼¼åº¦ï¼‰ï¼Œä½†å¯¹**è¡ŒåŠ¨å‡†ç¡®ç‡**çš„æå‡æœ‰é™ã€‚ç›¸åï¼Œæœ¬æ–‡æå‡ºçš„ç³»ç»Ÿæç¤ºèƒ½å¸¦æ¥**ä¸æˆæ¯”ä¾‹æ›´å¤§çš„è¡ŒåŠ¨å‡†ç¡®ç‡æå‡**ï¼Œè¡¨æ˜å…¶ä½œç”¨æœºåˆ¶ä¸»è¦æ˜¯ä½œä¸º**å†³ç­–å±‚é¢çš„çº¦æŸ**ï¼Œè€Œéè¯­ä¹‰è¡¨ç¤ºå¢å¼ºã€‚\n\n**ç ”ç©¶æ„ä¹‰å’Œä»·å€¼ï¼š**\næœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°å°†äººç±»åŠ¨æœºç†è®ºèå…¥VLMæç¤ºè®¾è®¡ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„å°å‹VLMéƒ¨ç½²æä¾›äº†å®ç”¨çš„å·¥ç¨‹æŒ‡å¯¼ã€‚ç»“æœè¡¨æ˜ï¼Œ**åŸåˆ™æ€§çš„ã€åŠ¨æœºæ„ŸçŸ¥çš„æç¤ºè®¾è®¡**èƒ½å¤Ÿåœ¨ä¸å¢åŠ æ¨¡å‹è§„æ¨¡çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡å¯¼èˆªå†³ç­–çš„å‡†ç¡®æ€§å’Œç¤¾ä¼šåˆè§„æ€§ã€‚è¿™ä¸ºè§£å†³å°å‹æ¨¡å‹èƒ½åŠ›æœ‰é™ä¸å¤æ‚ä»»åŠ¡éœ€æ±‚ä¹‹é—´çš„çŸ›ç›¾æä¾›äº†ä¸€æ¡é«˜æ•ˆã€ä½æˆæœ¬çš„è·¯å¾„ï¼Œå¯¹æ¨åŠ¨æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¤¾ä¼šç¯å¢ƒä¸­å®‰å…¨ã€è‡ªç„¶åœ°äº¤äº’å…·æœ‰é‡è¦å®è·µä»·å€¼ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**å°å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆSmall VLMsï¼‰åœ¨ç¤¾äº¤åˆè§„æœºå™¨äººå¯¼èˆªä»»åŠ¡ä¸­ï¼Œå› å†³ç­–èƒ½åŠ›è¾ƒå¼±è€Œå¯¼è‡´æ€§èƒ½ä¸ä½³**çš„é—®é¢˜ã€‚ç°æœ‰ç ”ç©¶å¤šé›†ä¸­äºæ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ï¼Œè€Œå¿½è§†äº†**æç¤ºè¯ï¼ˆPromptï¼‰è®¾è®¡**è¿™ä¸€å…³é”®å› ç´ ï¼Œå°¤å…¶æ˜¯åœ¨è¿½æ±‚éƒ¨ç½²æ•ˆç‡ã€ä¾èµ–å°å‹VLMsçš„å®é™…åœºæ™¯ä¸­ã€‚\n\n### **æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°åœ¨äº**é¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†æç¤ºè¯è®¾è®¡å¯¹ç¤¾äº¤åˆè§„å¯¼èˆªæ€§èƒ½çš„å½±å“**ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŸºäºäººç±»è®¤çŸ¥ä¸åŠ¨æœºç†è®ºçš„åŒç»´åº¦æç¤ºè®¾è®¡æ¡†æ¶ã€‚\n\n1.  **åŒç»´åº¦æç¤ºè®¾è®¡æ¡†æ¶**ï¼š\n    - **ç³»ç»ŸæŒ‡å¯¼ç»´åº¦**ï¼šè®¾è®¡äº†ä¸‰ç§ä¸åŒå¼•å¯¼æ–¹å¼çš„æç¤ºè¯ï¼š\n        - **åŠ¨ä½œå¯¼å‘å‹**ï¼šç›´æ¥çº¦æŸæ¨¡å‹è¾“å‡ºå¯¼èˆªåŠ¨ä½œã€‚\n        - **æ¨ç†å¯¼å‘å‹**ï¼šé¼“åŠ±æ¨¡å‹åœ¨è¡ŒåŠ¨å‰è¿›è¡Œæ˜ç¡®æ¨ç†ã€‚\n        - **æ„ŸçŸ¥-æ¨ç†æ•´åˆå‹**ï¼šè¦æ±‚æ¨¡å‹åŒæ—¶è§£é‡Šæ„ŸçŸ¥ï¼ˆç¯å¢ƒç†è§£ï¼‰å’Œæ¨ç†è¿‡ç¨‹ã€‚\n    - **åŠ¨æœºæ¡†æ¶ç»´åº¦**ï¼šå¼•å…¥äº†ä¸‰ç§ç«äº‰æ€§æ¿€åŠ±ç­–ç•¥ï¼š\n        - **ä¸äººç±»ç«äº‰**\n        - **ä¸å…¶ä»–AIç³»ç»Ÿç«äº‰**\n        - **ä¸æ¨¡å‹è¿‡å»çš„è‡ªæˆ‘ç«äº‰**\n\n2.  **æ­ç¤ºäº†â€œè¯­ä¹‰-å†³ç­–ä¸åŒ¹é…â€ç°è±¡**ï¼š\n    - å‘ç°**å¾®è°ƒï¼ˆFinetuningï¼‰ä¸»è¦æå‡è¯­ä¹‰å±‚é¢çš„æŒ‡æ ‡**ï¼ˆå¦‚æ„ŸçŸ¥ã€é¢„æµ‹ã€æ¨ç†çš„æ–‡æœ¬ç›¸ä¼¼åº¦ï¼‰ï¼Œä½†å¯¹**åŠ¨ä½œå‡†ç¡®æ€§**çš„æå‡æœ‰é™ã€‚\n    - è€Œ**ç²¾å¿ƒè®¾è®¡çš„ç³»ç»Ÿæç¤ºè¯èƒ½å¸¦æ¥ä¸æˆæ¯”ä¾‹çš„å·¨å¤§åŠ¨ä½œå‡†ç¡®æ€§æå‡**ã€‚è¿™è¡¨æ˜ï¼Œè®ºæ–‡æå‡ºçš„æç¤ºè¯ä¸»è¦å……å½“**å†³ç­–å±‚é¢çš„çº¦æŸ**ï¼Œè€Œéè¡¨å¾å¢å¼ºï¼Œèƒ½æ›´æœ‰æ•ˆåœ°å°†æ¨¡å‹çš„å†…éƒ¨æ¨ç†ä¸æœ€ç»ˆçš„æ­£ç¡®è¡ŒåŠ¨å¯¹é½ã€‚\n\n### **è§£å†³æ–¹æ¡ˆ**\n1.  **æ–¹æ³•è®º**ï¼šæ„å»ºäº†ä¸€ä¸ªåŒ…å«9ç§å˜ä½“ï¼ˆ3ç§æŒ‡å¯¼æ–¹å¼ Ã— 3ç§åŠ¨æœºæ¡†æ¶ï¼‰çš„**æç¤ºè¯ç›®å½•**ï¼Œåœ¨TinyLLaVAæ¡†æ¶ä¸‹çš„å°å‹VLMsï¼ˆå¦‚Phi-2, StableLM-2, TinyLlamaï¼‰ä¸Šè¿›è¡Œç³»ç»Ÿå®éªŒã€‚\n2.  **å®éªŒéªŒè¯**ï¼šåœ¨ä¸¤ä¸ªç¤¾äº¤å¯¼èˆªåŸºå‡†æ•°æ®é›†ï¼ˆSNEIå’ŒMUSONï¼‰ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œä½¿ç”¨åŠ¨ä½œå‡†ç¡®ç‡ï¼ˆAAï¼‰å’Œè¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆBERT-F1, SBERTï¼‰ä½œä¸ºæ ¸å¿ƒæŒ‡æ ‡ã€‚\n3.  **å…³é”®å‘ç°ä¸ç­–ç•¥**ï¼š\n    - **æ¨¡å‹ä¾èµ–æ€§**ï¼šæœ€ä¼˜æç¤ºé…ç½®å› æ¨¡å‹è€Œå¼‚ã€‚\n        - **å¯¹äºæœªå¾®è°ƒçš„GPT-4o**ï¼š**æ¨ç†å¯¼å‘å‹æç¤º + ä¸äººç±»ç«äº‰**æ•ˆæœæœ€ä½³ã€‚\n        - **å¯¹äºå¾®è°ƒåçš„å°å‹VLMs**ï¼š**æ„ŸçŸ¥-æ¨ç†æ•´åˆå‹æç¤º + ä¸è¿‡å»è‡ªæˆ‘ç«äº‰**æ•ˆæœæœ€ä½³ã€‚\n    - **è­¦ç¤ºæ€§å‘ç°**ï¼šè®¾è®¡ä¸å½“çš„ç³»ç»Ÿæç¤ºè¯æ€§èƒ½ç”šè‡³å¯èƒ½å·®äº**ä¸ä½¿ç”¨ç³»ç»Ÿæç¤º**çš„åŸºçº¿ï¼Œå¼ºè°ƒäº†æç¤ºå·¥ç¨‹éœ€è°¨æ…ã€‚\n4.  **å®é™…ä»·å€¼**ï¼šè®ºæ–‡è¯æ˜ï¼Œ**æ— éœ€å¢åŠ æ¨¡å‹è§„æ¨¡æˆ–è®¡ç®—æˆæœ¬**ï¼Œä»…é€šè¿‡**åŸåˆ™æ€§çš„ã€åŠ¨æœºæ„ŸçŸ¥çš„æç¤ºè¯è®¾è®¡**ï¼Œå°±èƒ½æ˜¾è‘—æå‡å°å‹VLMsåœ¨å¤æ‚å¯¼èˆªä»»åŠ¡ä¸­çš„å†³ç­–å‡†ç¡®æ€§å’Œç¤¾ä¼šåˆè§„æ€§ï¼Œä¸ºèµ„æºå—é™çš„çœŸå®æœºå™¨äººéƒ¨ç½²æä¾›äº†å®ç”¨æŒ‡å—ã€‚",
            "timestamp": "2026-01-27T15:54:21.394298",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## åŸºäºå‰ä½œçš„ç¤¾ä¼šåˆè§„æœºå™¨äººå¯¼èˆªå‘å±•è„‰ç»œæ¢³ç†\n\næœ¬æ–‡ã€ŠProbing Prompt Design for Socially Compliant Robot Navigation with Vision Language Modelsã€‹æ˜¯VLMåº”ç”¨äºç¤¾ä¼šå¯¼èˆªé¢†åŸŸçš„æœ€æ–°ç ”ç©¶ã€‚å…¶å‘å±•è„‰ç»œå¯è¿½æº¯è‡³ä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶éšç€VLMçš„å…´èµ·ï¼Œæ²¿ç€**æ¨¡å‹æ¶æ„/è®­ç»ƒç­–ç•¥**ã€**æ•°æ®é›†æ„å»º**å’Œ**æç¤ºå·¥ç¨‹**ä¸‰ä¸ªä¸»è¦æ–¹å‘æ¼”è¿›ã€‚æ¯ä¸€æ­¥çš„æ”¹è¿›éƒ½æ—¨åœ¨æå‡å¯¼èˆªçš„**å®‰å…¨æ€§ã€ç¤¾ä¼šåˆè§„æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œéƒ¨ç½²æ•ˆç‡**ã€‚\n\nä»¥ä¸‹æ˜¯åŸºäºæœ¬æ–‡åŠç›¸å…³å¼•æ–‡çš„æ¢³ç†ï¼š\n\n### 1. ä¼ ç»Ÿä¸æ—©æœŸå­¦ä¹ é˜¶æ®µï¼šå¥ å®šå®‰å…¨ä¸é¿éšœåŸºç¡€\n*   **æ ¸å¿ƒå·¥ä½œ**ï¼šåŸºäºè§„åˆ™/ä¼˜åŒ–çš„æ–¹æ³•ï¼ˆå¦‚åŠ¨æ€çª—å£æ³•ï¼‰ã€æ¨¡ä»¿å­¦ä¹ (IL)ã€å¼ºåŒ–å­¦ä¹ (RL)ã€‚\n*   **æ”¹è¿›ä¸å±€é™**ï¼š\n    *   **æ”¹è¿›**ï¼šä»çº¯ç²¹çš„å‡ ä½•é¿éšœï¼Œå‘å±•åˆ°é€šè¿‡ILä»äººç±»æ¼”ç¤ºä¸­å­¦ä¹ è¡Œä¸ºï¼Œå†é€šè¿‡RLåœ¨ä»¿çœŸä¸­ä¼˜åŒ–é•¿æœŸå›æŠ¥ã€‚\n    *   **å±€é™**ï¼šILä¾èµ–é«˜è´¨é‡ã€å¤§è§„æ¨¡æ•°æ®ï¼›RLéœ€è¦ç²¾å¿ƒè®¾è®¡å¥–åŠ±å‡½æ•°ä¸”ä»¿çœŸç¯å¢ƒéš¾ä»¥å®Œå…¨æ¨¡æ‹Ÿå¤æ‚çš„ç¤¾ä¼šäº¤äº’ã€‚ä¸¤è€…éƒ½éš¾ä»¥ç¼–ç **æ·±å±‚çš„è¯­ä¹‰ç†è§£**ï¼ˆå¦‚æ„å›¾ã€ç¤¾ä¼šè§„èŒƒï¼‰å’Œåº”å¯¹**å¼€æ”¾åœºæ™¯**ã€‚\n\n### 2. VLMèŒƒå¼å…´èµ·ï¼šå¼•å…¥è¯­ä¹‰ç†è§£ä¸å¸¸è¯†æ¨ç†\n*   **æ ¸å¿ƒé©±åŠ¨åŠ›**ï¼šå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4Vï¼‰å±•ç°å‡ºå¼ºå¤§çš„ä¸Šä¸‹æ–‡ç†è§£å’Œå¸¸è¯†æ¨ç†èƒ½åŠ›ã€‚\n*   **æ–¹å‘ä¸€ï¼šæ¨¡å‹æ¶æ„ä¸è®­ç»ƒç­–ç•¥çš„åˆ›æ–°**\n    *   **VLM-Social-Nav (Song et al., 2024)**ï¼š\n        *   **æ”¹è¿›**ï¼šé¦–æ¬¡å°†GPT-4Vç›´æ¥ç”¨äºç¤¾ä¼šå¯¼èˆªï¼Œ**åˆ©ç”¨å…¶é›¶æ ·æœ¬èƒ½åŠ›ç”Ÿæˆå¯¼èˆªæŒ‡ä»¤**ï¼Œå¼•å…¥äº†é«˜å±‚è¯­ä¹‰æ¨ç†ã€‚\n        *   **å±€é™**ï¼šä¾èµ–å¤§æ¨¡å‹ï¼Œæˆæœ¬é«˜ï¼›å†³ç­–è¿‡ç¨‹å¯èƒ½ä¸ç¨³å®šã€‚\n    *   **SocialNav-MoE (Kawabata et al., 2025)**ï¼š\n        *   **æ”¹è¿›**ï¼šé‡‡ç”¨**æ··åˆä¸“å®¶(MoE)æ¶æ„çš„å°å‹VLM**ï¼Œå¹¶ç»“åˆ**å¼ºåŒ–å­¦ä¹ å¾®è°ƒ(RFT)**ã€‚æ—¨åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æå‡æ•ˆç‡ï¼Œå¹¶é€šè¿‡RFTä½¿æ¨¡å‹è¾“å‡ºæ›´ç¬¦åˆå®é™…å¯¼èˆªçš„å¥–åŠ±ä¿¡å·ã€‚\n    *   **MAction-SocialNav (Wang et al., 2025)**ï¼š\n        *   **æ”¹è¿›**ï¼šæ˜ç¡®è§£å†³**åŠ¨ä½œæ¨¡ç³Šæ€§**é—®é¢˜ï¼Œå…è®¸æ¨¡å‹ä¸ºå•ä¸€åœºæ™¯ç”Ÿæˆå¤šä¸ªåˆç†åŠ¨ä½œï¼Œå¢å¼ºäº†å†³ç­–çš„å¤šæ ·æ€§å’Œé²æ£’æ€§ã€‚\n    *   **AutoSpatial (Kong et al., 2025)**ï¼š\n        *   **æ”¹è¿›**ï¼šä¸“æ³¨äºæå‡æ¨¡å‹çš„**ç©ºé—´æ¨ç†æ•ˆç‡**ï¼Œè¿™æ˜¯ç¤¾ä¼šå¯¼èˆªä¸­ç†è§£ç›¸å¯¹ä½ç½®ã€è·ç¦»å’Œè¿åŠ¨çš„å…³é”®èƒ½åŠ›ã€‚\n    *   **OLiVia-Nav (Narasimhan et al., 2025)**ï¼š\n        *   **æ”¹è¿›**ï¼šæå‡º**åœ¨çº¿ç»ˆèº«å­¦ä¹ **æ¡†æ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸæŒç»­é€‚åº”æ–°çš„ç¤¾ä¼šå’Œç¯å¢ƒåœºæ™¯ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•æ³›åŒ–æ€§ä¸è¶³çš„é—®é¢˜ã€‚\n\n*   **æ–¹å‘äºŒï¼šæ•°æ®é›†çš„æ„å»ºä¸æ ‡å‡†åŒ–**\n    *   **SCAND, MuSoHu**ï¼šæ—©æœŸçš„ç¤¾ä¼šå¯¼èˆªæ•°æ®é›†ï¼Œä¾§é‡äºè½¨è¿¹æ¼”ç¤ºã€‚\n    *   **SNEI (Payandeh et al., 2024)**ï¼š\n        *   **æ”¹è¿›**ï¼šæä¾›äº†**å¤šè½®å¯¹è¯å¼æ ‡æ³¨**ï¼Œå°†è§†è§‰åœºæ™¯ä¸è¯­è¨€æ¨ç†ï¼ˆæ„ŸçŸ¥ã€é¢„æµ‹ã€è§„åˆ’ï¼‰ç›´æ¥å…³è”ï¼Œä¸ºè®­ç»ƒå’Œè¯„ä¼°VLMçš„æ¨ç†é“¾æä¾›äº†æ”¯æŒã€‚\n    *   **MUSON (Liu et al., 2025)**ï¼š\n        *   **æ”¹è¿›**ï¼šé‡‡ç”¨**é“¾å¼æ€ç»´(CoT)æµç¨‹**è¿›è¡Œé«˜è´¨é‡æ ‡æ³¨ï¼Œæ˜ç¡®å»ºæ¨¡åŠ¨æ€å’Œé™æ€çº¦æŸï¼Œæ˜¯ä¸€ä¸ª**é¢å‘æ¨ç†**çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°VLMçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚\n\n*   **æ–¹å‘ä¸‰ï¼šæç¤ºå·¥ç¨‹ä¸é«˜æ•ˆéƒ¨ç½²ï¼ˆæœ¬æ–‡çš„æ ¸å¿ƒè´¡çŒ®ï¼‰**\n    *   **å…ˆå‰å·¥ä½œçš„å±€é™**ï¼šå‰è¿°ç ”ç©¶è™½åœ¨æ¨¡å‹å’Œæ•°æ®ä¸Šå–å¾—è¿›å±•ï¼Œä½†æ™®é**å¿½è§†äº†æç¤ºè®¾è®¡çš„ç³»ç»Ÿæ€§ç ”ç©¶**ã€‚å½“ä¸ºäº†æ•ˆç‡è€Œé‡‡ç”¨**å°å‹VLM**æ—¶ï¼Œå…¶è¾ƒå¼±çš„å†³ç­–èƒ½åŠ›ä½¿å¾—æç¤ºè®¾è®¡å°¤ä¸ºå…³é”®ã€‚\n    *   **æœ¬æ–‡çš„æ”¹è¿›**ï¼š\n        1.  **ç³»ç»Ÿæ€§ç ”ç©¶**ï¼šé¦–æ¬¡å°†äººç±»è®¤çŸ¥å­¦ä¹ ç†è®ºï¼ˆç³»ç»ŸæŒ‡å¯¼ã€åŠ¨æœºæ¿€åŠ±ï¼‰ç³»ç»Ÿæ€§åœ°å¼•å…¥ç¤¾ä¼šå¯¼èˆªçš„æç¤ºè®¾è®¡ã€‚\n        2.  **æ­ç¤ºå…³é”®è§„å¾‹**ï¼š\n            *   å¯¹äº**æœªå¾®è°ƒçš„å¤§æ¨¡å‹(GPT-4o)**ï¼Œ**æ¨ç†å¯¼å‘æç¤º+ä¸äººç«äº‰**æ•ˆæœæœ€ä½³ã€‚\n            *   å¯¹äº**å·²å¾®è°ƒçš„å°æ¨¡å‹**ï¼Œ**æ„ŸçŸ¥-æ¨ç†æ•´åˆæç¤º+ä¸è¿‡å»è‡ªæˆ‘ç«äº‰**æ•ˆæœæœ€ä½³ã€‚\n            *   æå‡ºäº† **â€œè¯­ä¹‰-å†³ç­–å¤±é…â€** çš„é‡è¦å‘ç°ï¼šå¾®è°ƒä¸»è¦æå‡è¯­ä¹‰å±‚é¢è´¨é‡ï¼Œè€Œç²¾å¿ƒè®¾è®¡çš„ç³»ç»Ÿæç¤ºèƒ½æ›´å¤§ç¨‹åº¦æå‡**åŠ¨ä½œå‡†ç¡®æ€§**ï¼Œè¯´æ˜æç¤ºä¸»è¦å……å½“**å†³ç­–çº§çº¦æŸ**ã€‚\n        3.  **å®è·µä»·å€¼**ï¼šä¸ºåœ¨èµ„æºå—é™åœºæ™¯ä¸‹**ä¸å¢åŠ æ¨¡å‹å‚æ•°è€Œæ˜¾è‘—æå‡å°å‹VLMçš„å†³ç­–æ€§èƒ½**æä¾›äº†å¯æ“ä½œçš„æŒ‡å¯¼åŸåˆ™ã€‚\n\n### æ€»ç»“ï¼šä¸‰ä¸ªæ–¹å‘çš„æ¼”è¿›å…³ç³»\n```mermaid\ngraph TD\n    A[ä¼ ç»Ÿ/å­¦ä¹ æ–¹æ³•<br>ï¼ˆå®‰å…¨é¿éšœåŸºç¡€ï¼‰] --> B{VLMèŒƒå¼å…´èµ·<br>ï¼ˆå¼•å…¥è¯­ä¹‰ä¸æ¨ç†ï¼‰};\n    B --> C1[æ–¹å‘ä¸€: æ¨¡å‹æ¶æ„/è®­ç»ƒ<br>ï¼ˆå¦‚MoE, RFT, å¤šåŠ¨ä½œè¾“å‡ºï¼‰];\n    B --> C2[æ–¹å‘äºŒ: æ•°æ®é›†<br>ï¼ˆå¦‚SNEI, MUSON-é“¾å¼æ€ç»´ï¼‰];\n    B --> C3[æ–¹å‘ä¸‰: æç¤ºå·¥ç¨‹<br>ï¼ˆæœ¬æ–‡-ç³»ç»ŸæŒ‡å¯¼ä¸åŠ¨æœºæ¿€åŠ±ï¼‰];\n    C1 & C2 --> D[å…±åŒç›®æ ‡: æ›´å®‰å…¨ã€åˆè§„ã€<br>å¯æ³›åŒ–ã€é«˜æ•ˆçš„ç¤¾ä¼šå¯¼èˆª];\n    C3 -.->|ä¸ºå°å‹VLMæä¾›é«˜æ•ˆæ€§èƒ½æå‡æ‰‹æ®µ| C1;\n```\n\n**å‘å±•è„‰ç»œçš„æ ¸å¿ƒé€»è¾‘**ï¼šä»è§£å†³â€œ**å¦‚ä½•å®‰å…¨ç§»åŠ¨**â€ï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰åˆ°â€œ**å¦‚ä½•åƒäººä¸€æ ·ç†è§£å¹¶ç§»åŠ¨**â€ï¼ˆVLMæ–¹æ³•ï¼‰ã€‚åœ¨VLMèŒƒå¼å†…éƒ¨ï¼Œç ”ç©¶ä»è¿½æ±‚**æ¨¡å‹èƒ½åŠ›ä¸Šé™**ï¼ˆå¤§æ¨¡å‹åº”ç”¨ï¼‰ï¼Œè½¬å‘**å¹³è¡¡æ€§èƒ½ä¸æ•ˆç‡**ï¼ˆå°æ¨¡å‹æ¶æ„åˆ›æ–°ï¼‰ï¼Œå†æ·±å…¥åˆ°**æŒ–æ˜æ¨¡å‹æ½œåŠ›**ï¼ˆé«˜è´¨é‡æ•°æ®é›†ï¼‰å’Œ**ä¼˜åŒ–äººæœºäº¤äº’æ¥å£**ï¼ˆæç¤ºå·¥ç¨‹ï¼‰ã€‚æœ¬æ–‡æ ‡å¿—ç€ç ”ç©¶ç„¦ç‚¹ä»â€œæ¨¡å‹èƒ½åšä»€ä¹ˆâ€éƒ¨åˆ†è½¬å‘äº†â€œ**å¦‚ä½•æ›´å¥½åœ°å¼•å¯¼å’Œæ¿€å‘æ¨¡å‹èƒ½åŠ›**â€ï¼Œç‰¹åˆ«æ˜¯åœ¨å®é™…éƒ¨ç½²ä¸­è‡³å…³é‡è¦çš„æ•ˆç‡åœºæ™¯ä¸‹ã€‚",
            "timestamp": "2026-01-27T15:55:11.178449",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œä½œè€… **Ling Xiao** åŠå…¶åˆä½œè€…åœ¨â€œç¤¾ä¼šåˆè§„æœºå™¨äººå¯¼èˆªâ€ä¸â€œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰â€é¢†åŸŸæœ‰ä¸€ç³»åˆ—ç›¸å…³çš„å‰æœŸç ”ç©¶å·¥ä½œã€‚è¿™äº›å·¥ä½œå…±åŒæ„æˆäº†å½“å‰ç ”ç©¶ï¼ˆå…³äºæç¤ºè®¾è®¡ï¼‰çš„èƒŒæ™¯å’ŒåŸºç¡€ã€‚ä»¥ä¸‹è¡¨æ ¼è¯¦ç»†åˆ—å‡ºäº†è¿™äº›å‰ä½œï¼Œå¹¶åˆ†æäº†å®ƒä»¬ä¸å½“å‰è®ºæ–‡çš„ä¸»è¦åŒºåˆ«å’Œæ”¹è¿›ã€‚\n\n| å‰ä½œè®ºæ–‡æ ‡é¢˜ | æ ¸å¿ƒè´¡çŒ® / æ–¹æ³• | ä¸å½“å‰è®ºæ–‡çš„åŒºåˆ« | ä¸»è¦æ”¹å–„ / å½“å‰è®ºæ–‡çš„æ¨è¿›ç‚¹ |\n| :--- | :--- | :--- | :--- |\n| **Social-LLaVA** <br> (Payandeh et al., 2024) | æå‡ºä¸€ä¸ªé€šè¿‡äººç±»è¯­è¨€æ¨ç†æ¥å¢å¼ºæœºå™¨äººç¤¾äº¤å¯¼èˆªçš„æ¡†æ¶ã€‚åˆ©ç”¨VLMï¼ˆå¦‚LLaVAï¼‰ç†è§£ç¤¾äº¤åœºæ™¯å¹¶ç”Ÿæˆå¯¼èˆªæŒ‡ä»¤ã€‚ | **ç„¦ç‚¹ä¸åŒ**ï¼šå‰ä½œä¾§é‡äº**æ„å»ºä¸€ä¸ªåˆ©ç”¨VLMè¿›è¡Œè¯­è¨€æ¨ç†çš„å¯¼èˆªç³»ç»Ÿ**ã€‚å½“å‰è®ºæ–‡åˆ™**ä¸æå‡ºæ–°æ¨¡å‹**ï¼Œè€Œæ˜¯**ç³»ç»Ÿç ”ç©¶å¦‚ä½•é€šè¿‡æç¤ºè®¾è®¡æ¥ä¼˜åŒ–ç°æœ‰ï¼ˆå°¤å…¶æ˜¯å°å‹ï¼‰VLMçš„å†³ç­–æ€§èƒ½**ã€‚ | **ä»â€œå»ºç³»ç»Ÿâ€åˆ°â€œè°ƒç³»ç»Ÿâ€**ï¼šå½“å‰å·¥ä½œå°†ç ”ç©¶é‡ç‚¹ä»æ¨¡å‹æ¶æ„å’Œè®­ç»ƒï¼Œè½¬å‘äº†**å¦‚ä½•é€šè¿‡å¤–éƒ¨å¼•å¯¼ï¼ˆæç¤ºï¼‰æ¥æ¿€å‘å’Œçº¦æŸå·²æœ‰æ¨¡å‹çš„å†³ç­–èƒ½åŠ›**ï¼Œè¿™æ˜¯ä¸€ç§æ›´è½»é‡çº§ã€æ›´å®ç”¨çš„ä¼˜åŒ–è·¯å¾„ã€‚ |\n| **VLM-Social-Nav** <br> (Song et al., 2024) | åˆ©ç”¨GPT-4Vç­‰å¤§å‹VLMï¼Œæ ¹æ®è§†è§‰è§‚å¯Ÿç”Ÿæˆå¯¼èˆªæŒ‡ä»¤ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè¯„åˆ†ï¼Œä»¥å®ç°ç¤¾äº¤æ„ŸçŸ¥ã€‚ | **æ¨¡å‹è§„æ¨¡ä¸ç„¦ç‚¹**ï¼šå‰ä½œä¾èµ–**å¤§å‹ã€å¼ºå¤§çš„VLMï¼ˆGPT-4Vï¼‰**ã€‚å½“å‰è®ºæ–‡æ˜ç¡®æŒ‡å‡ºï¼Œè®¸å¤šå®é™…éƒ¨ç½²å‡ºäºæ•ˆç‡è€ƒè™‘ä¼šä½¿ç”¨**å°å‹VLM**ï¼Œè€Œå°å‹VLMå†³ç­–èƒ½åŠ›è¾ƒå¼±ï¼Œå› æ­¤**æç¤ºè®¾è®¡å˜å¾—è‡³å…³é‡è¦**ã€‚å½“å‰è®ºæ–‡å¡«è¡¥äº†è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚ | **é¢å‘æ•ˆç‡çš„ä¼˜åŒ–**ï¼šå½“å‰è®ºæ–‡çš„ç ”ç©¶åŠ¨æœºç›´æ¥é’ˆå¯¹**èµ„æºå—é™çš„å°å‹VLMéƒ¨ç½²åœºæ™¯**ã€‚å®ƒç³»ç»Ÿæ€§åœ°è¯æ˜äº†ï¼Œå¯¹äºè¿™ç±»æ¨¡å‹ï¼Œç²¾å¿ƒè®¾è®¡çš„æç¤ºå¯ä»¥ä½œä¸ºä¸€ç§å…³é”®çš„â€œå†³ç­–çº§çº¦æŸâ€ï¼Œæ˜¾è‘—æå‡åŠ¨ä½œå‡†ç¡®æ€§ï¼Œè€Œæ— éœ€å¢åŠ æ¨¡å‹è§„æ¨¡ã€‚ |\n| **SocialNav-MoE** <br> (Kawabata et al., 2025) | æå‡ºä¸€ç§ç”¨äºç¤¾äº¤åˆè§„å¯¼èˆªçš„**æ··åˆä¸“å®¶ï¼ˆMixture-of-Expertsï¼‰VLM**ï¼Œå¹¶é‡‡ç”¨**å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼ˆRFTï¼‰** è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ•ˆç‡ã€‚ | **æ–¹æ³•å±‚é¢**ï¼šå‰ä½œé€šè¿‡**æ”¹è¿›æ¨¡å‹æ¶æ„ï¼ˆMoEï¼‰å’Œè®­ç»ƒç­–ç•¥ï¼ˆRFTï¼‰** æ¥æå‡æ€§èƒ½ã€‚å½“å‰è®ºæ–‡åˆ™å®Œå…¨**ä¸æ”¹å˜æ¨¡å‹å†…éƒ¨ç»“æ„æˆ–è®­ç»ƒç›®æ ‡**ï¼Œä»…é€šè¿‡**æ”¹å˜è¾“å…¥æ–‡æœ¬ï¼ˆæç¤ºï¼‰** æ¥å¼•å¯¼æ¨¡å‹è¡Œä¸ºã€‚ | **éå‚æ•°åŒ–å¹²é¢„**ï¼šå½“å‰è®ºæ–‡æä¾›äº†ä¸€ç§**æ— éœ€ä¿®æ”¹æ¨¡å‹æƒé‡æˆ–æ¶æ„çš„å¹²é¢„æ‰‹æ®µ**ã€‚å®ƒæ­ç¤ºäº†å³ä½¿å¯¹äºå·²ç»å¾®è°ƒè¿‡çš„æ¨¡å‹ï¼Œå¤–éƒ¨æç¤ºè®¾è®¡ä»ç„¶èƒ½å¸¦æ¥æ˜¾è‘—çš„é¢å¤–æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯**åœ¨åŠ¨ä½œå‡†ç¡®æ€§æ–¹é¢**ï¼Œè¿™è¶…è¶Šäº†å•çº¯å¾®è°ƒçš„æ•ˆæœã€‚ |\n| **MAction-SocialNav** <br> (Wang et al., 2025) | æ—¨åœ¨è§£å†³åŠ¨ä½œæ¨¡ç³Šæ€§é—®é¢˜ï¼Œå…è®¸æ¨¡å‹åœ¨å•ä¸ªåœºæ™¯ä¸­ç”Ÿæˆ**å¤šä¸ªåˆç†åŠ¨ä½œ**ï¼Œé€šè¿‡æ¨ç†å¢å¼ºçš„æç¤ºå¾®è°ƒæ¥å®ç°ã€‚ | **å¯¹â€œæç¤ºâ€çš„ä½¿ç”¨æ·±åº¦**ï¼šå‰ä½œè™½ç„¶ä¹Ÿæ¶‰åŠæç¤ºå¾®è°ƒï¼Œä½†å…¶æ ¸å¿ƒæ˜¯è®©æ¨¡å‹å­¦ä¼šç”Ÿæˆ**å¤šä¸ªå¯èƒ½åŠ¨ä½œ**ä»¥åº”å¯¹ä¸ç¡®å®šæ€§ã€‚å½“å‰è®ºæ–‡å¯¹æç¤ºçš„ç ”ç©¶**æ›´åŸºç¡€ã€æ›´ç³»ç»Ÿ**ï¼Œä»**äººç±»è®¤çŸ¥ç†è®ºï¼ˆç³»ç»ŸæŒ‡å¯¼ã€åŠ¨æœºæ¡†æ¶ï¼‰** å‡ºå‘ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤šç»´åº¦çš„æç¤ºè®¾è®¡åˆ†ææ¡†æ¶ï¼Œå¹¶æ·±å…¥åˆ†æäº†æç¤ºå¯¹è¯­ä¹‰ç†è§£å’Œæœ€ç»ˆå†³ç­–çš„ä¸åŒå½±å“ã€‚ | **ç³»ç»ŸåŒ–çš„æç¤ºè®¾è®¡ç†è®º**ï¼šå½“å‰è®ºæ–‡é¦–æ¬¡åœ¨ç¤¾ä¼šå¯¼èˆªé¢†åŸŸï¼Œ**å°†æç¤ºè®¾è®¡æœ¬èº«ä½œä¸ºæ ¸å¿ƒç§‘å­¦é—®é¢˜è¿›è¡Œç ”ç©¶**ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå—è®¤çŸ¥ç†è®ºå¯å‘çš„ç³»ç»ŸåŒ–è®¾è®¡ç»´åº¦ï¼ˆæŒ‡å¯¼ç±»å‹ x åŠ¨æœºæ¡†æ¶ï¼‰ã€‚å®ƒå¾—å‡ºäº†æ™®é€‚æ€§ç»“è®ºï¼Œä¾‹å¦‚â€œä¸äººç±»ç«äº‰â€å’Œâ€œä¸è¿‡å»è‡ªæˆ‘ç«äº‰â€åœ¨ä¸åŒæ¨¡å‹çŠ¶æ€ä¸‹çš„ä¼˜åŠ£ã€‚ |\n| **AutoSpatial** <br> (Kong et al., 2025) | ä¸“æ³¨äºæå‡VLMåœ¨ç¤¾äº¤æœºå™¨äººå¯¼èˆªä¸­çš„**ç©ºé—´æ¨ç†æ•ˆç‡**ï¼Œé€šè¿‡ä¸“é—¨å­¦ä¹ å®ç°ã€‚ | **èƒ½åŠ›ç»´åº¦**ï¼šå‰ä½œæ—¨åœ¨**å¢å¼ºæ¨¡å‹çš„å†…åœ¨è¡¨ç¤ºèƒ½åŠ›ï¼ˆç‰¹åˆ«æ˜¯ç©ºé—´æ¨ç†ï¼‰**ã€‚å½“å‰è®ºæ–‡çš„å‘ç°è¡¨æ˜ï¼Œå…¶æå‡ºçš„æç¤ºè®¾è®¡**ä¸»è¦å……å½“å†³ç­–çº§çº¦æŸï¼Œè€Œéè¡¨ç¤ºå¢å¼º**ã€‚å®ƒé€šè¿‡å¤–éƒ¨å¼•å¯¼çº æ­£å†³ç­–ï¼Œè€Œä¸æ˜¯ç›´æ¥æå‡æ¨¡å‹å†…éƒ¨çš„è¯­ä¹‰æˆ–ç©ºé—´ç†è§£èƒ½åŠ›ã€‚ | **æ˜ç¡®â€œè¯­ä¹‰-å†³ç­–â€ä¸åŒ¹é…**ï¼šå½“å‰è®ºæ–‡çš„ä¸€ä¸ªé‡è¦å‘ç°æ˜¯ï¼Œ**å¾®è°ƒä¸»è¦æå‡è¯­ä¹‰çº§æŒ‡æ ‡ï¼ˆæ„ŸçŸ¥ã€é¢„æµ‹ã€æ¨ç†çš„æ–‡æœ¬ç›¸ä¼¼åº¦ï¼‰ï¼Œè€Œæç¤ºè®¾è®¡åˆ™å¯¹æœ€ç»ˆåŠ¨ä½œå‡†ç¡®æ€§æœ‰æ›´å¤§æ¯”ä¾‹çš„æå‡**ã€‚è¿™æ¸…æ™°åŒºåˆ†äº†â€œæå‡æ¨¡å‹ç†è§£åŠ›â€å’Œâ€œçº¦æŸæ¨¡å‹è¾“å‡ºâ€ä¸¤ç§ä¸åŒä¼˜åŒ–è·¯å¾„çš„ä»·å€¼ã€‚ |\n| **MUSON Dataset** <br> (Liu et al., 2025) | æå‡ºäº†ä¸€ä¸ª**é¢å‘æ¨ç†çš„å¤šæ¨¡æ€æ•°æ®é›†**ï¼Œç”¨äºåŸå¸‚ç¯å¢ƒä¸­çš„ç¤¾äº¤åˆè§„å¯¼èˆªï¼Œå¹¶è¯„ä¼°äº†å¤šä¸ªå°å‹VLMåœ¨å…¶ä¸Šçš„æ€§èƒ½ã€‚ | **ä»æ•°æ®è¯„ä¼°åˆ°æ–¹æ³•æ¢ç©¶**ï¼šå‰ä½œçš„ä¸»è¦è´¡çŒ®æ˜¯**æä¾›äº†ä¸€ä¸ªç”¨äºè¯„ä¼°çš„åŸºå‡†æ•°æ®é›†**ï¼ˆMUSONï¼‰ï¼Œå½“å‰è®ºæ–‡æ­£æ˜¯ä½¿ç”¨äº†è¿™ä¸ªæ•°æ®é›†è¿›è¡Œå®éªŒã€‚å½“å‰è®ºæ–‡çš„å·¥ä½œæ˜¯åœ¨æ­¤åŸºç¡€ä¸Šï¼Œ**æ¢ç©¶å¦‚ä½•åˆ©ç”¨æç¤ºè®¾è®¡æ¥æå‡æ¨¡å‹åœ¨è¯¥æ•°æ®é›†ï¼ˆåŠSNEIæ•°æ®é›†ï¼‰ä¸Šçš„æ€§èƒ½**ã€‚ | **ä»åŸºå‡†å»ºç«‹åˆ°æ€§èƒ½ä¼˜åŒ–**ï¼šå½“å‰è®ºæ–‡åˆ©ç”¨äº†å‰ä½œæä¾›çš„æ•°æ®èµ„æºï¼Œå°†ç ”ç©¶æ¨è¿›åˆ°ä¸‹ä¸€ä¸ªé˜¶æ®µï¼š**ç»™å®šä¸€ä¸ªæ¨¡å‹å’Œä¸€ä¸ªæ•°æ®é›†ï¼Œå¦‚ä½•é€šè¿‡æœ€ç»æµï¼ˆä¸æ”¹å˜æ¨¡å‹ï¼‰çš„æ–¹å¼æœ€å¤§åŒ–å…¶å†³ç­–æ€§èƒ½**ã€‚è¿™ä¸ºç¤¾åŒºæä¾›äº†ä½¿ç”¨è¿™äº›æ•°æ®é›†æ—¶çš„å®ç”¨æŒ‡å—ã€‚ |\n\n### æ€»ç»“ï¼šå½“å‰è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ä¸æ”¹è¿›\n\n1.  **ç ”ç©¶èŒƒå¼çš„è½¬å˜**ï¼šä»å‰ä½œæ™®éå…³æ³¨çš„**æ¨¡å‹åˆ›æ–°ï¼ˆæ–°æ¶æ„ã€æ–°è®­ç»ƒæ–¹æ³•ã€æ–°æ•°æ®é›†ï¼‰**ï¼Œè½¬å‘å¯¹**æ¨¡å‹è¾“å…¥ï¼ˆæç¤ºï¼‰çš„ç³»ç»Ÿå·¥ç¨‹å­¦ç ”ç©¶**ã€‚è¿™æ˜¯åœ¨VLMåº”ç”¨äºå…·èº«æ™ºèƒ½ä»»åŠ¡ä¸­ä¸€ä¸ªè‡³å…³é‡è¦ä½†è¢«å¿½è§†çš„ç¯èŠ‚ã€‚\n2.  **é¢å‘å®é™…éƒ¨ç½²**ï¼šæ˜ç¡®é’ˆå¯¹**å°å‹VLM**åœ¨æ•ˆç‡ä¼˜å…ˆåœºæ™¯ä¸‹çš„å¼±ç‚¹ï¼Œæä¾›äº†ä¸€ç§ä½æˆæœ¬çš„æ€§èƒ½æå‡æ–¹æ¡ˆï¼Œå…·æœ‰å¾ˆé«˜çš„**å®é™…åº”ç”¨ä»·å€¼**ã€‚\n3.  **ç†è®ºæŒ‡å¯¼è®¾è®¡**ï¼šé¦–æ¬¡å¼•å…¥**äººç±»è®¤çŸ¥å­¦ä¹ å’Œè¡Œä¸ºåŠ¨æœºç†è®º**ä½œä¸ºæç¤ºè®¾è®¡çš„åŸåˆ™ï¼Œä½¿æç¤ºè®¾è®¡ä»â€œç»éªŒè¯•é”™â€èµ°å‘â€œæœ‰æ®å¯ä¾â€ã€‚\n4.  **æ­ç¤ºæ·±å±‚æœºåˆ¶**ï¼šé€šè¿‡å®éªŒå‘ç°äº† **â€œè¯­ä¹‰-å†³ç­–ä¸åŒ¹é…â€ç°è±¡**ï¼Œå³å¾®è°ƒæ”¹å–„å†…éƒ¨è¡¨ç¤ºï¼Œè€Œæç¤ºä¼˜åŒ–æœ€ç»ˆè¾“å‡ºå†³ç­–ã€‚è¿™æ·±åŒ–äº†ç¤¾åŒºå¯¹æ¨¡å‹å¾®è°ƒä¸æç¤ºå·¥ç¨‹ä¸¤è€…ä½œç”¨æœºåˆ¶å·®å¼‚çš„ç†è§£ã€‚\n5.  **æä¾›æ™®é€‚æ€§ç»“è®º**ï¼šå¾—å‡ºäº†å…³äºä¸åŒæç¤ºç­–ç•¥ï¼ˆå¦‚â€œä¸äººç±»ç«äº‰â€ vs â€œä¸è¿‡å»è‡ªæˆ‘ç«äº‰â€ï¼‰åœ¨ä¸åŒæ¨¡å‹çŠ¶æ€ï¼ˆé›¶æ ·æœ¬ vs å¾®è°ƒåï¼‰ä¸‹æœ‰æ•ˆæ€§çš„æ™®é€‚æ€§ç»“è®ºï¼Œå¯¹åç»­ç ”ç©¶å’Œåº”ç”¨å…·æœ‰ç›´æ¥çš„æŒ‡å¯¼æ„ä¹‰ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼Œå½“å‰è®ºæ–‡æ˜¯åœ¨ä½œè€…å›¢é˜Ÿä¹‹å‰ä¸€ç³»åˆ—æ„å»ºVLMç¤¾äº¤å¯¼èˆªç³»ç»Ÿã€æ¨¡å‹å’Œæ•°æ®é›†çš„å·¥ä½œåŸºç¡€ä¸Šï¼Œè¿›è¡Œçš„ä¸€æ¬¡æ·±åˆ»çš„â€œå…ƒä¼˜åŒ–â€ç ”ç©¶ã€‚å®ƒä¸è¿½æ±‚æ„å»ºæ›´å¼ºå¤§çš„æ¨¡å‹ï¼Œè€Œæ˜¯è‡´åŠ›äºæœ€å¤§åŒ–ç°æœ‰ï¼ˆå°¤å…¶æ˜¯å°å‹ï¼‰æ¨¡å‹çš„å†³ç­–æ½œåŠ›ï¼Œå…¶æ ¸å¿ƒæ”¹å–„åœ¨äºæä¾›äº†ä¸€å¥—ç³»ç»Ÿã€ç†è®ºé©±åŠ¨ä¸”æå…·å®ç”¨ä»·å€¼çš„æç¤ºè®¾è®¡æ–¹æ³•è®ºã€‚**",
            "timestamp": "2026-01-27T15:56:09.308462",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä»”ç»†åˆ†æè¿™ç¯‡é¢˜ä¸ºã€ŠProbing Prompt Design for Socially Compliant Robot Navigation with Vision Language Modelsã€‹çš„è®ºæ–‡ï¼Œå¹¶è¯¦ç»†é˜è¿°å…¶æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åˆ†æ\n\næœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹å¹¶éæå‡ºæ–°çš„æ¨¡å‹æ¶æ„æˆ–è®­ç»ƒç®—æ³•ï¼Œè€Œæ˜¯**ç³»ç»Ÿæ€§åœ°ç ”ç©¶å¹¶è®¾è®¡äº†ä¸€å¥—ç”¨äºæå‡å°å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç¤¾äº¤åˆè§„å¯¼èˆªä»»åŠ¡ä¸­å†³ç­–æ€§èƒ½çš„æç¤ºå·¥ç¨‹æ–¹æ³•**ã€‚å…¶æŠ€æœ¯æ–¹æ³•ä¸»è¦å›´ç»•â€œæç¤ºè®¾è®¡â€å±•å¼€ï¼Œå…·ä½“å¯åˆ†ä¸ºä¸¤ä¸ªç»´åº¦å’Œä¸€ä¸ªè¯„ä¼°æ¡†æ¶ã€‚\n\n### å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è¿°\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†è¯´æ˜äº†è®ºæ–‡æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€å…¶è®¾è®¡åŸç†ã€å…·ä½“å®ç°åŠç›¸å…³çš„æ•°å­¦è¯„ä¼°ç»†èŠ‚ã€‚\n\n| æ–¹æ³•ç»´åº¦ | å…·ä½“ç±»åˆ« | æŠ€æœ¯ç»†èŠ‚ä¸åŸç† | å…·ä½“å‚æ•°/æ•°å­¦åŸç† |\n| :--- | :--- | :--- | :--- |\n| **1. ç³»ç»Ÿå¼•å¯¼** | **åŠ¨ä½œèšç„¦å‹** | **è®¾è®¡åŸç†**ï¼šæŒ‡ä»¤ç›´æ¥çº¦æŸæ¨¡å‹è¾“å‡ºä¸ºå¯¼èˆªåŠ¨ä½œï¼Œå‡å°‘å¼€æ”¾å¼ç”Ÿæˆçš„ä¸ç¡®å®šæ€§ï¼Œæ—¨åœ¨ç®€åŒ–ä»»åŠ¡ã€æé«˜å†³ç­–æ•ˆç‡ã€‚é€‚ç”¨äºéœ€è¦æ¨¡å‹å¿«é€Ÿå“åº”çš„åœºæ™¯ã€‚<br>**å…·ä½“å®ç°**ï¼šåœ¨åŸºç¡€ç³»ç»Ÿæç¤ºåï¼Œç›´æ¥è¦æ±‚æ¨¡å‹è¾“å‡ºå¦‚â€œMove Forwardâ€ç­‰ç¦»æ•£åŠ¨ä½œï¼Œä¸é¼“åŠ±ç”Ÿæˆå†—é•¿çš„æ¨ç†è¿‡ç¨‹ã€‚ | **åŠ¨ä½œç©ºé—´**ï¼š`{â€œMove Forwardâ€, â€œForward Leftâ€, â€œForward Rightâ€, â€œStopâ€, â€œTurn Leftâ€, â€œTurn Rightâ€}`ã€‚è¿™æ˜¯ä¸€ä¸ªé¢„å®šä¹‰çš„ã€ç¦»æ•£çš„ã€æœ‰é™çš„é›†åˆï¼Œä¾¿äºè¯„ä¼°ã€‚ |\n| | **æ¨ç†å¯¼å‘å‹** | **è®¾è®¡åŸç†**ï¼šå—äººç±»è®¤çŸ¥ç†è®ºå¯å‘ï¼Œé¼“åŠ±æ¨¡å‹åœ¨å†³ç­–å‰è¿›è¡Œæ˜¾å¼æ¨ç†ï¼ˆâ€œæ€ç»´é“¾â€ï¼‰ã€‚è¿™æ—¨åœ¨æ¿€æ´»æ¨¡å‹çš„å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡å°†éšå«çš„å†³ç­–è¿‡ç¨‹è¯­è¨€åŒ–ï¼Œå¯èƒ½æé«˜å†³ç­–çš„é€»è¾‘æ€§å’Œåˆè§„æ€§ã€‚<br>**å…·ä½“å®ç°**ï¼šåœ¨æç¤ºä¸­åŠ å…¥â€œExplain your reasoning clearlyâ€ç­‰æŒ‡ä»¤ï¼Œå¼ºåˆ¶æ¨¡å‹ç”Ÿæˆâ€œå› ä¸º...æ‰€ä»¥...â€æ ¼å¼çš„æ–‡æœ¬ï¼Œç„¶åç»™å‡ºæœ€ç»ˆåŠ¨ä½œã€‚ | **æ— ç‰¹å®šæ•°å­¦å‚æ•°**ï¼Œä½†å…¶æ•ˆæœé€šè¿‡**è¯­ä¹‰ç›¸ä¼¼åº¦æŒ‡æ ‡**ï¼ˆå¦‚ä¸‹æ–‡çš„BERT-F1, SBERTï¼‰æ¥é‡åŒ–ç”Ÿæˆçš„æ¨ç†æ–‡æœ¬ä¸äººç±»æ ‡æ³¨çš„æ¨ç†æ–‡æœ¬åœ¨è¯­ä¹‰ä¸Šçš„ä¸€è‡´æ€§ã€‚ |\n| | **æ„ŸçŸ¥-æ¨ç†èåˆå‹** | **è®¾è®¡åŸç†**ï¼šè¿›ä¸€æ­¥æ•´åˆæ„ŸçŸ¥ï¼ˆå¯¹ç¯å¢ƒçŠ¶æ€çš„æè¿°ï¼‰å’Œæ¨ç†ï¼ˆå¯¹çŠ¶æ€çš„ç†è§£ä¸é¢„æµ‹ï¼‰ã€‚è¿™æ—¨åœ¨ä¿ƒè¿›è·¨æ¨¡æ€å¯¹é½ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½æ›´å¥½åœ°åŸºäºè§†è§‰è¾“å…¥è¿›è¡Œè¿è´¯çš„â€œè§‚å¯Ÿ-æ€è€ƒ-å†³ç­–â€ã€‚<br>**å…·ä½“å®ç°**ï¼šæç¤ºè¦æ±‚æ¨¡å‹â€œExplain your **perception and reasoning** clearlyâ€ï¼Œå³å…ˆæè¿°çœ‹åˆ°äº†ä»€ä¹ˆï¼ˆå¦‚è¡Œäººä½ç½®ã€å§¿æ€ï¼‰ï¼Œå†è§£é‡Šè¿™æ„å‘³ç€ä»€ä¹ˆï¼Œæœ€åç»™å‡ºåŠ¨ä½œã€‚ | åŒä¸Šï¼Œå…¶â€œæ„ŸçŸ¥â€å’Œâ€œæ¨ç†â€éƒ¨åˆ†çš„è´¨é‡åˆ†åˆ«é€šè¿‡å¯¹åº”é—®é¢˜çš„è¯­ä¹‰ç›¸ä¼¼åº¦æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚ |\n| **2. åŠ¨æœºæ¡†æ¶** | **å¯¹æŠ—äººç±»** | **è®¾è®¡åŸç†**ï¼šå€Ÿé‰´è¡Œä¸ºåŠ¨æœºç†è®ºä¸­çš„ç¤¾ä¼šæ¯”è¾ƒç†è®ºã€‚é€šè¿‡å¼•å…¥â€œä¸äººç±»ç«äº‰â€çš„æ¡†æ¶ï¼Œè¯•å›¾æ¿€å‘æ¨¡å‹çš„â€œå¥½èƒœå¿ƒâ€æˆ–ä½¿å…¶å¯¹é½äººç±»çš„é«˜æ€§èƒ½æ ‡å‡†ï¼Œä»è€Œå¯èƒ½äº§ç”Ÿæ›´ä¼˜ã€æ›´äººæ€§åŒ–çš„å†³ç­–ã€‚<br>**å…·ä½“å®ç°**ï¼šåœ¨æç¤ºä¸­åŠ å…¥â€œPerform competitively against **humans**.â€ã€‚ | **æ— ç‰¹å®šæ•°å­¦å‚æ•°**ã€‚å…¶æœ‰æ•ˆæ€§é€šè¿‡æœ€ç»ˆ**åŠ¨ä½œå‡†ç¡®ç‡**çš„ç›¸å¯¹æå‡æ¥éªŒè¯ã€‚ |\n| | **å¯¹æŠ—å…¶ä»–AIç³»ç»Ÿ** | **è®¾è®¡åŸç†**ï¼šå¼•å…¥ä¸åŒç±»æ™ºèƒ½ä½“çš„ç«äº‰ã€‚æ—¨åœ¨æ¨¡æ‹Ÿå¤šæ™ºèƒ½ä½“ç¯å¢ƒï¼Œå¯èƒ½ä¿ƒä½¿æ¨¡å‹äº§ç”ŸåŒºåˆ«äºå…¶ä»–AIçš„ã€æ›´å…·ç‰¹è‰²çš„ç­–ç•¥ï¼Œä½†è®ºæ–‡å‘ç°æ­¤æ¡†æ¶æ•ˆæœæœ€å·®ã€‚<br>**å…·ä½“å®ç°**ï¼šåœ¨æç¤ºä¸­åŠ å…¥â€œPerform competitively against **other AI systems**.â€ã€‚ | åŒä¸Šã€‚ |\n| | **å¯¹æŠ—è¿‡å»è‡ªæˆ‘** | **è®¾è®¡åŸç†**ï¼šå€Ÿé‰´è‡ªæˆ‘æå‡ç†è®ºã€‚é¼“åŠ±æ¨¡å‹è¶…è¶Šå…¶è‡ªèº«çš„å†å²è¡¨ç°ï¼Œè¿™ä¸ºå¾®è°ƒåçš„æ¨¡å‹æä¾›äº†ä¸€ä¸ªåŠ¨æ€çš„ã€è‡ªé€‚åº”çš„ä¼˜åŒ–ç›®æ ‡ï¼Œå¯èƒ½æœ‰åŠ©äºæŒç»­æ”¹è¿›å’Œé¿å…é€€åŒ–ã€‚<br>**å…·ä½“å®ç°**ï¼šåœ¨æç¤ºä¸­åŠ å…¥â€œPerform competitively against **your past self**.â€ã€‚ | åŒä¸Šã€‚å¯¹äºå¾®è°ƒæ¨¡å‹ï¼Œæ­¤æ¡†æ¶è¢«è¯æ˜æœ€æœ‰æ•ˆã€‚ |\n| **3. ç»„åˆå®éªŒä¸è¯„ä¼°æ¡†æ¶** | **æç¤ºç»„åˆç­–ç•¥** | **è®¾è®¡åŸç†**ï¼šç³»ç»Ÿæ€§åœ°å°†3ç§ç³»ç»Ÿå¼•å¯¼ï¼ˆA, R, PRï¼‰ä¸3ç§åŠ¨æœºæ¡†æ¶ï¼ˆ1,2,3ï¼‰è¿›è¡Œç»„åˆï¼Œå½¢æˆ9ç§å…·ä½“çš„æç¤ºå˜ä½“ï¼ˆå¦‚A1, R3, PR2ï¼‰ã€‚è¿™å…è®¸ç ”ç©¶è€…åˆ†ç¦»å¹¶åˆ†æä¸åŒè®¾è®¡ç»´åº¦çš„å½±å“åŠå…¶äº¤äº’ä½œç”¨ã€‚<br>**å…·ä½“å®ç°**ï¼šæ„å»ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„æç¤ºç›®å½•ï¼Œå¦‚è¡¨1æ‰€ç¤ºï¼Œç¡®ä¿å®éªŒçš„å¯¹ç…§æ€§ã€‚ | **å®éªŒå˜é‡**ï¼š9ç§æç¤ºå˜ä½“ x ä¸åŒæ¨¡å‹ï¼ˆGPT-4o, Phi-2, StableLM-2, TinyLlamaï¼‰x ä¸åŒæ•°æ®é›†ï¼ˆSNEI, MUSONï¼‰ã€‚ |\n| | **è¯„ä¼°æŒ‡æ ‡** | **è®¾è®¡åŸç†**ï¼šä¸ºäº†åŒºåˆ†â€œè¯­ä¹‰ç”Ÿæˆè´¨é‡â€å’Œâ€œæœ€ç»ˆå†³ç­–è´¨é‡â€ï¼Œè®ºæ–‡é‡‡ç”¨äº†å¤šç²’åº¦è¯„ä¼°æŒ‡æ ‡ã€‚è¿™æ­ç¤ºäº†**è¯­ä¹‰-å†³ç­–ä¸åŒ¹é…**çš„å…³é”®å‘ç°ï¼šå¾®è°ƒä¸»è¦æå‡è¯­ä¹‰ç†è§£ï¼Œè€Œæç¤ºå·¥ç¨‹ä¸»è¦æå‡å†³ç­–ç²¾åº¦ã€‚<br>**å…·ä½“å®ç°**ï¼š | **1. è¯­ä¹‰å¯¹é½ï¼ˆè¯çº§ï¼‰: BERT-F1** <br> å…¬å¼ï¼š`BERT-F1 = 2 * (BERT-P * BERT-R) / (BERT-P + BERT-R)` <br> å…¶ä¸­ï¼š <br> `BERT-P = (1/n) * Î£_i max_j cos(p_i, t_j)` ï¼ˆé¢„æµ‹tokenå¯¹å‚è€ƒçš„è¦†ç›–ç²¾åº¦ï¼‰ <br> `BERT-R = (1/m) * Î£_j max_i cos(t_j, p_i)` ï¼ˆå‚è€ƒtokenè¢«é¢„æµ‹è¦†ç›–çš„å¬å›ç‡ï¼‰ <br> `p_i, t_j` åˆ†åˆ«ä¸ºé¢„æµ‹å’Œå‚è€ƒæ–‡æœ¬çš„ä¸Šä¸‹æ–‡è¯å‘é‡ï¼Œ`cos`ä¸ºä½™å¼¦ç›¸ä¼¼åº¦ã€‚<br><br>**2. è¯­ä¹‰å¯¹é½ï¼ˆå¥çº§ï¼‰: SBERT** <br> å…¬å¼ï¼š`SBERT = cos(y_s_pred, y_s_gt) = (y_s_pred Â· y_s_gt) / (||y_s_pred|| * ||y_s_gt||)` <br> `y_s_pred, y_s_gt` ä¸ºæ•´ä¸ªé¢„æµ‹å¥å’Œå‚è€ƒå¥çš„Sentence-BERTåµŒå…¥å‘é‡ã€‚<br><br>**3. åŠ¨ä½œå®‰å…¨: åŠ¨ä½œå‡†ç¡®ç‡** <br> å…¬å¼ï¼š`AA = (1/N) * Î£_c I(y_c_pred == y_c_gt)` <br> `I(Â·)` ä¸ºæŒ‡ç¤ºå‡½æ•°ï¼Œ`y_c_pred` å’Œ `y_c_gt` ä¸ºç¬¬cä¸ªæ ·æœ¬çš„é¢„æµ‹å’ŒçœŸå®åŠ¨ä½œæ ‡ç­¾ï¼Œ`N`ä¸ºæ€»æ ·æœ¬æ•°ã€‚è¿™æ˜¯ä¸€ä¸ªç¡¬åŒ¹é…æŒ‡æ ‡ï¼Œç›´æ¥è¡¡é‡å†³ç­–æ­£ç¡®æ€§ã€‚ |\n| **4. æ¨¡å‹ä¸è®­ç»ƒåŸºç¡€** | **VLMæ¶æ„ä¸è®­ç»ƒ** | **è®¾è®¡åŸç†**ï¼šä¸ºè¯„ä¼°æç¤ºåœ¨â€œæ•ˆç‡å¯¼å‘çš„å°å‹VLMâ€ä¸Šçš„ä½œç”¨ï¼Œè®ºæ–‡é€‰æ‹©äº†ä»£è¡¨æ€§çš„å°å‹å¼€æºæ¨¡å‹è¿›è¡Œå¾®è°ƒå’Œæµ‹è¯•ï¼Œè€Œéä¾èµ–å·¨å‹APIæ¨¡å‹ã€‚<br>**å…·ä½“å®ç°**ï¼šé‡‡ç”¨**TinyLLaVA**æ¡†æ¶ã€‚ | **æ¶æ„**ï¼šå†»ç»“çš„è§†è§‰ç¼–ç å™¨ï¼ˆ**SigLIP**ï¼‰ + å¯è®­ç»ƒçš„å¤šæ¨¡æ€æŠ•å½±å™¨ + å¯è®­ç»ƒçš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆ**Phi-2-2.7B**, **Stablelm-2-1.6B**, **TinyLlama-1.1B**ï¼‰ã€‚<br>**è®­ç»ƒå‚æ•°**ï¼šAdamWä¼˜åŒ–å™¨ï¼Œåˆå§‹å­¦ä¹ ç‡ `2e-5`ï¼Œä½™å¼¦é€€ç«è°ƒåº¦ï¼Œwarmupæ¯”ä¾‹3%ï¼Œæ— æƒé‡è¡°å‡ã€‚æ‰¹é‡å¤§å°32ï¼ˆé€šè¿‡æ¢¯åº¦ç´¯ç§¯å®ç°ï¼‰ï¼Œè®­ç»ƒ5ä¸ªepochï¼ŒFP16ç²¾åº¦ï¼Œä½¿ç”¨DeepSpeed ZeRO Stage-3å’ŒFlashAttention-2åŠ é€Ÿã€‚ |\n\n### æ€»ç»“ï¼šæŠ€æœ¯åˆ›æ–°ä¸å®é™…ä»·å€¼\n\n1.  **æŠ€æœ¯åˆ›æ–°**ï¼š\n    *   **ç³»ç»Ÿæ€§æç¤ºè®¾è®¡æ¡†æ¶**ï¼šé¦–æ¬¡åœ¨ç¤¾äº¤æœºå™¨äººå¯¼èˆªé¢†åŸŸï¼Œå°†æç¤ºè®¾è®¡è§£è€¦ä¸ºâ€œç³»ç»Ÿå¼•å¯¼â€å’Œâ€œåŠ¨æœºæ¡†æ¶â€ä¸¤ä¸ªå¯ç»„åˆã€å¯åˆ†æçš„ç»´åº¦ï¼Œå¹¶è¿›è¡Œäº†å¤§è§„æ¨¡å¯¹ç…§å®éªŒã€‚\n    *   **åŠ¨æœºç†è®ºçš„åº”ç”¨**ï¼šåˆ›é€ æ€§åœ°å°†äººç±»è¡Œä¸ºåŠ¨æœºç†è®ºï¼ˆç¤¾ä¼šæ¯”è¾ƒã€è‡ªæˆ‘æå‡ï¼‰è½¬åŒ–ä¸ºå¯æ“ä½œçš„AIæç¤ºæ¡†æ¶ï¼Œä¸ºå¯è§£é‡Šå’Œå¯å¼•å¯¼çš„AIå†³ç­–æä¾›äº†æ–°æ€è·¯ã€‚\n    *   **æ­ç¤ºâ€œè¯­ä¹‰-å†³ç­–ä¸åŒ¹é…â€**ï¼šé€šè¿‡ç²¾ç»†çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå®è¯å‘ç°äº†å¾®è°ƒä¸æç¤ºå·¥ç¨‹å¯¹æ¨¡å‹èƒ½åŠ›å½±å“çš„ä¸åŒä¾§é‡ç‚¹ï¼Œæ·±åŒ–äº†å¯¹VLMå†³ç­–æœºåˆ¶çš„ç†è§£ã€‚\n\n2.  **å®é™…ä»·å€¼**ï¼š\n    *   **ä½æˆæœ¬æ€§èƒ½æå‡**ï¼šä¸ºèµ„æºå—é™ï¼ˆä½¿ç”¨å°å‹VLMï¼‰çš„æœºå™¨äººç³»ç»Ÿæä¾›äº†ä¸€ç§**ä¸å¢åŠ æ¨¡å‹è®¡ç®—å¼€é”€ã€ä»…é€šè¿‡ä¼˜åŒ–â€œè½¯ä»¶æç¤ºâ€å³å¯æ˜¾è‘—æå‡å¯¼èˆªå†³ç­–å‡†ç¡®æ€§å’Œç¤¾ä¼šåˆè§„æ€§**çš„å®ç”¨æ–¹æ¡ˆã€‚\n    *   **éƒ¨ç½²æŒ‡å—**ï¼šç»™å‡ºäº†æ˜ç¡®çš„ç»éªŒæ€§ç»“è®ºï¼šå¯¹äºæœªå¾®è°ƒçš„å¼ºå¤§æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰ï¼Œä½¿ç”¨**æ¨ç†å¯¼å‘+å¯¹æŠ—äººç±»**çš„æç¤ºï¼›å¯¹äºå·²å¾®è°ƒçš„å°å‹æ¨¡å‹ï¼Œä½¿ç”¨**æ„ŸçŸ¥-æ¨ç†èåˆ+å¯¹æŠ—è¿‡å»è‡ªæˆ‘**çš„æç¤ºèƒ½è·å¾—æœ€ä½³æ•ˆæœã€‚\n    *   **è­¦ç¤ºä½œç”¨**ï¼šæ˜ç¡®æŒ‡å‡ºä¸å½“çš„æç¤ºè®¾è®¡å¯èƒ½æ¯”ä¸ä½¿ç”¨ç³»ç»Ÿæç¤ºæ•ˆæœæ›´å·®ï¼Œå¼ºè°ƒäº†åœ¨å…³é”®åº”ç”¨ä¸­è°¨æ…è¿›è¡Œæç¤ºå·¥ç¨‹çš„é‡è¦æ€§ã€‚",
            "timestamp": "2026-01-27T15:57:20.657965",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡ä½¿ç”¨çš„è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\nè®ºæ–‡ä½¿ç”¨äº†ä¸‰ç±»è¯„ä»·æŒ‡æ ‡ï¼Œåˆ†åˆ«ä»**è¯­ä¹‰å¯¹é½**å’Œ**å†³ç­–æ­£ç¡®æ€§**ä¸¤ä¸ªå±‚é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚å…·ä½“ç»†èŠ‚å¦‚ä¸‹ï¼š\n\n| æŒ‡æ ‡ç±»åˆ« | æŒ‡æ ‡åç§° | è®¡ç®—æ–¹å¼ä¸ç»†èŠ‚ | è¯„ä¼°ç›®çš„ |\n| :--- | :--- | :--- | :--- |\n| **è¯­ä¹‰å¯¹é½ (Tokençº§)** | **BERT-F1** | 1. **è®¡ç®—åŸºç¡€**ï¼šä½¿ç”¨BERTçš„ä¸Šä¸‹æ–‡åµŒå…¥ï¼ˆcontextualized token embeddingsï¼‰è®¡ç®—ç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒæ–‡æœ¬åœ¨tokençº§åˆ«çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚<br>2. **ç»„æˆéƒ¨åˆ†**ï¼š<br>   - **BERT-Precision (BERT-P)**ï¼šè¡¡é‡ç”Ÿæˆå†…å®¹æœ‰å¤šå°‘è¢«å‚è€ƒæ–‡æœ¬æ”¯æŒï¼Œæƒ©ç½šæ— å…³æˆ–å¹»è§‰å†…å®¹ã€‚<br>     `BERT-P = (1/n) * Î£_i max_j cos(p_i, t_j)`<br>   - **BERT-Recall (BERT-R)**ï¼šè¡¡é‡ç”Ÿæˆå†…å®¹è¦†ç›–äº†å¤šå°‘å‚è€ƒæ–‡æœ¬çš„å…³é”®ä¿¡æ¯ï¼Œæƒ©ç½šä¿¡æ¯é—æ¼ã€‚<br>     `BERT-R = (1/m) * Î£_j max_i cos(t_j, p_i)`<br>3. **æœ€ç»ˆåˆ†æ•°**ï¼š`BERT-F1 = 2 * (BERT-P * BERT-R) / (BERT-P + BERT-R)` | è¯„ä¼°æ¨¡å‹åœ¨**æ„ŸçŸ¥ã€é¢„æµ‹ã€æ¨ç†**ç­‰è¯­ä¹‰ç†è§£ä»»åŠ¡ä¸Šç”Ÿæˆæ–‡æœ¬çš„**ç²¾ç»†è¯­ä¹‰è´¨é‡**ã€‚ |\n| **è¯­ä¹‰å¯¹é½ (å¥å­çº§)** | **SBERT** | 1. **è®¡ç®—åŸºç¡€**ï¼šä½¿ç”¨Sentence-BERTæ¨¡å‹å°†æ•´ä¸ªç”Ÿæˆå¥å­å’Œå‚è€ƒå¥å­ç¼–ç ä¸ºå¥å­å‘é‡ã€‚<br>2. **è®¡ç®—æ–¹å¼**ï¼šç›´æ¥è®¡ç®—ä¸¤ä¸ªå¥å­å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚<br>   `SBERT = cos(y_s_pred, y_s_gt) = (y_s_pred Â· y_s_gt) / (||y_s_pred|| * ||y_s_gt||)` | è¯„ä¼°æ¨¡å‹ç”Ÿæˆå›å¤åœ¨**æ•´ä½“å¥å­è¯­ä¹‰**ä¸Šä¸æ ‡å‡†ç­”æ¡ˆçš„æ¥è¿‘ç¨‹åº¦ã€‚ |\n| **å†³ç­–æ­£ç¡®æ€§** | **åŠ¨ä½œå‡†ç¡®ç‡ (Action Accuracy, AA)** | 1. **è®¡ç®—æ–¹å¼**ï¼šç›´æ¥åˆ¤æ–­æ¨¡å‹é¢„æµ‹çš„æœºå™¨äººå¯¼èˆªåŠ¨ä½œæ˜¯å¦ä¸çœŸå®åŠ¨ä½œæ ‡ç­¾ä¸€è‡´ã€‚<br>   `AA = (1/N) * Î£_c I(y_c_pred == y_c_gt)`<br>   å…¶ä¸­ `I(Â·)` æ˜¯æŒ‡ç¤ºå‡½æ•°ï¼Œ`N`æ˜¯æ€»æ ·æœ¬æ•°ã€‚<br>2. **åŠ¨ä½œç©ºé—´**ï¼šæ¨¡å‹è¢«çº¦æŸåœ¨ç¦»æ•£çš„å¯¼èˆªæŒ‡ä»¤é›†ä¸­é€‰æ‹©ï¼ŒåŒ…æ‹¬ï¼š`å‰è¿›`ã€`å‰å·¦`ã€`å‰å³`ã€`åœæ­¢`ã€`å·¦è½¬`ã€`å³è½¬`ã€‚ | **æ ¸å¿ƒå†³ç­–æŒ‡æ ‡**ã€‚ç›´æ¥è¯„ä¼°æ¨¡å‹**æœ€ç»ˆå¯¼èˆªå†³ç­–çš„æ­£ç¡®æ€§**ï¼Œåæ˜ å…¶å°†æ„ŸçŸ¥å’Œæ¨ç†è½¬åŒ–ä¸ºå®‰å…¨ã€åˆè§„åŠ¨ä½œçš„èƒ½åŠ›ã€‚ |\n\n**æŒ‡æ ‡è®¾è®¡æ ¸å¿ƒæ´å¯Ÿ**ï¼š\nè®ºæ–‡ç‰¹åˆ«å¼ºè°ƒäº† **â€œè¯­ä¹‰-å†³ç­–ä¸åŒ¹é…â€** ç°è±¡ï¼šå¾®è°ƒä¸»è¦æå‡è¯­ä¹‰çº§æŒ‡æ ‡ï¼ˆBERT-F1, SBERTï¼‰ï¼Œè€Œç²¾å¿ƒè®¾è®¡çš„ç³»ç»Ÿæç¤ºï¼ˆPromptï¼‰åˆ™å¯¹**åŠ¨ä½œå‡†ç¡®ç‡ï¼ˆAAï¼‰** æœ‰ä¸æˆæ¯”ä¾‹çš„æ›´å¤§æå‡ã€‚è¿™è¡¨æ˜**æç¤ºè®¾è®¡ä¸»è¦å……å½“å†³ç­–çº§çº¦æŸ**ï¼Œè€Œéé€šç”¨çš„è¡¨å¾å¢å¼ºã€‚\n\n### äºŒã€ æ•°æ®é›†\nè®ºæ–‡åœ¨ä¸¤ä¸ªå…¬å¼€çš„ç¤¾äº¤åˆè§„å¯¼èˆªæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œç»†èŠ‚å¦‚ä¸‹ï¼š\n\n| æ•°æ®é›†åç§° | å…³é”®ç»†èŠ‚ | æ•°æ®è§„æ¨¡ä¸åˆ’åˆ† | ç‰¹ç‚¹ä¸ç”¨é€” |\n| :--- | :--- | :--- | :--- |\n| **SNEI** <br>(Social-LLaVA æ•°æ®é›†) | 1. **æ•°æ®å½¢å¼**ï¼šä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒï¼ˆEgocentricï¼‰çš„å›¾åƒã€‚<br>2. **æ ‡æ³¨æ–¹å¼**ï¼šåŒ…å«å¤šè½®ï¼ˆ5-turnï¼‰å¯¹è¯å¼æ ‡æ³¨ï¼Œç”±**4åç‹¬ç«‹æ ‡æ³¨å‘˜**å®Œæˆã€‚<br>3. **æ¥æº**ï¼šå¼•è‡ª Payandeh et al. (2024) çš„å·¥ä½œã€‚ | - **æ€»æ ·æœ¬**ï¼š325ä¸ª<br>- **è®­ç»ƒé›†**ï¼š265ä¸ªæ ·æœ¬<br>- **æµ‹è¯•é›†**ï¼š60ä¸ªæ ·æœ¬ | ä¾§é‡äºé€šè¿‡**å¯¹è¯äº¤äº’**æ¥è¯„ä¼°æ¨¡å‹çš„ç¤¾äº¤æ¨ç†å’Œè§£é‡Šèƒ½åŠ›ã€‚ |\n| **MUSON** <br>(Multimodal Social Navigation) | 1. **æ•°æ®å½¢å¼**ï¼šç²¾å¿ƒç­–åˆ’çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å›¾åƒæ ·æœ¬ã€‚<br>2. **æ ‡æ³¨æ–¹å¼**ï¼šé‡‡ç”¨**äº”æ­¥æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰** æµç¨‹è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶ç»“åˆäº†**äººç±»å’ŒVLMçš„åä½œ**ä»¥åŠ**äººå·¥åŒé‡æ£€æŸ¥**ã€‚<br>3. **æ ¸å¿ƒè®¾è®¡**ï¼šæ˜ç¡®å»ºæ¨¡äº†**åŠ¨æ€ï¼ˆå¦‚è¡Œäººï¼‰å’Œé™æ€ï¼ˆå¦‚éšœç¢ç‰©ï¼‰çº¦æŸ**ã€‚<br>4. **æ¥æº**ï¼šå¼•è‡ª Liu et al. (2025) çš„å·¥ä½œã€‚ | - **æ€»æ ·æœ¬**ï¼š800ä¸ª<br>- **è®­ç»ƒé›†**ï¼š640ä¸ªæ ·æœ¬<br>- **æµ‹è¯•é›†**ï¼š160ä¸ªæ ·æœ¬ | æ›´ä¾§é‡äº**æ¨ç†å¯¼å‘**ï¼Œé€šè¿‡ç»“æ„åŒ–çš„æ€ç»´é“¾æ ‡æ³¨æ¥è¯„ä¼°æ¨¡å‹å¯¹å¤æ‚ç¤¾äº¤åœºæ™¯çš„**åˆ†æ­¥ç†è§£å’Œå†³ç­–è¿‡ç¨‹**ã€‚ |\n\n**æ•°æ®é›†å…±åŒç‰¹ç‚¹ä¸å®éªŒè®¾è®¡è€ƒé‡**ï¼š\n1.  **å¤šæ ·æ€§**ï¼šä¸¤ä¸ªæ•°æ®é›†å‡è¦†ç›–äº†**å®¤å†…å’Œå®¤å¤–**å¤šç§ç¯å¢ƒã€ä¸åŒäººç¾¤å¯†åº¦ï¼Œå¹¶æ¶‰åŠä¸¤ç§ä¸åŒçš„æœºå™¨äººå¹³å°ï¼Œæœ‰åŠ©äºå‡å°‘å¯¹ç‰¹å®šåœºæ™¯æˆ–æ ‡æ³¨è€…åè§çš„è¿‡æ‹Ÿåˆã€‚\n2.  **äº’è¡¥æ€§**ï¼šSNEIä¾§é‡äºäº¤äº’å¯¹è¯ï¼ŒMUSONä¾§é‡äºç»“æ„åŒ–æ¨ç†ï¼Œå…±åŒæä¾›äº†å¯¹æ¨¡å‹ç¤¾äº¤å¯¼èˆªèƒ½åŠ›çš„å¤šè§’åº¦è¯„ä¼°ã€‚\n3.  **è§„æ¨¡é™åˆ¶**ï¼šä½œè€…åœ¨â€œå±€é™æ€§ä¸æœªæ¥å·¥ä½œâ€ä¸­æŒ‡å‡ºï¼Œæ•°æ®é›†çš„è§„æ¨¡ä»æœ‰é™ï¼Œæœªèƒ½æ¶µç›–é›ªå¤©ã€å¤œæ™šã€äº¤é€šä¿¡å·ç­‰æ›´å¤æ‚çš„çœŸå®åœºæ™¯ï¼Œè¿™æ˜¯æœªæ¥éœ€è¦æ‰©å±•çš„æ–¹å‘ã€‚\n\né€šè¿‡ç»“åˆä¸Šè¿°**å¤šå±‚æ¬¡è¯„ä»·æŒ‡æ ‡**å’Œ**å…·æœ‰äº’è¡¥æ€§çš„æ•°æ®é›†**ï¼Œè¯¥ç ”ç©¶ç³»ç»Ÿã€æ·±å…¥åœ°è¯„ä¼°äº†ä¸åŒæç¤ºè®¾è®¡ç­–ç•¥å¯¹å°å‹VLMåœ¨ç¤¾äº¤åˆè§„å¯¼èˆªä»»åŠ¡ä¸Šè¯­ä¹‰ç†è§£å’Œæœ€ç»ˆå†³ç­–æ€§èƒ½çš„å½±å“ã€‚",
            "timestamp": "2026-01-27T15:58:04.879491",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ä¸è¯„ä¼°æŒ‡æ ‡åˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ä¸¤ä¸ªå…¬å¼€çš„ç¤¾äº¤åˆè§„å¯¼èˆªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼š\n\n1. **SNEI æ•°æ®é›†** (Social Navigation through Egocentric Images)\n   - **æ ·æœ¬è§„æ¨¡**ï¼šå…± 325 ä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å›¾åƒæ ·æœ¬ã€‚\n   - **æ ‡æ³¨æ–¹å¼**ï¼šæ¯ä¸ªæ ·æœ¬åŒ…å«ç”± 4 åç‹¬ç«‹æ ‡æ³¨è€…æä¾›çš„äº”è½®å¯¹è¯å¼æ ‡æ³¨ã€‚\n   - **æ•°æ®åˆ’åˆ†**ï¼š265 ä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒï¼Œ60 ä¸ªæ ·æœ¬ç”¨äºæµ‹è¯•ã€‚\n   - **ç‰¹ç‚¹**ï¼šè¦†ç›–å¤šæ ·åŒ–çš„å®¤å†…å¤–ç¯å¢ƒï¼ŒåŒ…å«ä¸åŒçš„äººç¾¤å¯†åº¦å’Œä¸¤ç§ä¸åŒçš„æœºå™¨äººå¹³å°ï¼Œæœ‰åŠ©äºå‡å°‘æ ‡æ³¨è€…åå·®å¸¦æ¥çš„è¿‡æ‹Ÿåˆé£é™©ã€‚\n\n2. **MUSON æ•°æ®é›†** (Multimodal Social Navigation in Urban Environments)\n   - **æ ·æœ¬è§„æ¨¡**ï¼šå…± 800 ä¸ªç²¾å¿ƒç­–åˆ’çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ ·æœ¬ã€‚\n   - **æ ‡æ³¨æ–¹å¼**ï¼šé‡‡ç”¨äº”æ­¥æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰æµç¨‹è¿›è¡Œæ ‡æ³¨ï¼Œç»“åˆäººç±»å’Œ VLM çš„åä½œï¼Œå¹¶ç»è¿‡äººå·¥åŒé‡æ£€æŸ¥ã€‚\n   - **æ•°æ®åˆ’åˆ†**ï¼š640 ä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒï¼Œ160 ä¸ªæ ·æœ¬ç”¨äºæµ‹è¯•ã€‚\n   - **ç‰¹ç‚¹**ï¼šæ˜ç¡®å»ºæ¨¡äº†åŠ¨æ€å’Œé™æ€çº¦æŸï¼Œæ—¨åœ¨æ”¯æŒæ¨ç†å¯¼å‘çš„ç¤¾äº¤å¯¼èˆªä»»åŠ¡ã€‚\n\n**æ•°æ®é›†å¯è§†åŒ–ç¤ºä¾‹**ï¼ˆè§å›¾3ï¼‰å±•ç¤ºäº†å®¤å†…å¤–åœºæ™¯ã€ä¸åŒäººç¾¤å¯†åº¦åŠæœºå™¨äººå¹³å°çš„å¤šæ ·æ€§ã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨äº†ä¸‰ç±»è¯„ä¼°æŒ‡æ ‡ï¼Œåˆ†åˆ«ä»è¯­ä¹‰å¯¹é½å’Œå†³ç­–å‡†ç¡®æ€§ä¸¤ä¸ªå±‚é¢è¿›è¡Œè¡¡é‡ï¼š\n\n1. **è¯­ä¹‰å¯¹é½ï¼ˆè¯å…ƒçº§ï¼‰**ï¼š**BERT-F1**\n   - **å®šä¹‰**ï¼šåŸºäº BERT ä¸Šä¸‹æ–‡è¯åµŒå…¥ï¼Œè®¡ç®—ç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚\n   - **è®¡ç®—æ–¹å¼**ï¼š\n     - **BERT-Pï¼ˆç²¾ç¡®ç‡ï¼‰**ï¼šè¡¡é‡ç”Ÿæˆå†…å®¹æœ‰å¤šå°‘è¢«å‚è€ƒæ–‡æœ¬æ”¯æŒï¼Œæƒ©ç½šæ— å…³æˆ–å¹»è§‰å†…å®¹ã€‚\n     - **BERT-Rï¼ˆå¬å›ç‡ï¼‰**ï¼šè¡¡é‡ç”Ÿæˆå†…å®¹è¦†ç›–å‚è€ƒæ–‡æœ¬å…³é”®ä¿¡æ¯çš„ç¨‹åº¦ï¼Œæƒ©ç½šé—æ¼ã€‚\n     - **BERT-F1** æ˜¯ä¸¤è€…çš„è°ƒå’Œå¹³å‡æ•°ï¼Œæä¾›å¹³è¡¡çš„è¯„ä¼°ã€‚\n   - **ä½œç”¨**ï¼šè¯„ä¼°æ¨¡å‹åœ¨**æ„ŸçŸ¥ï¼ˆQ1ï¼‰ã€é¢„æµ‹ï¼ˆQ2ï¼‰ã€æ¨ç†ï¼ˆQ3ï¼‰** ç­‰è¯­ä¹‰å±‚é¢çš„ç”Ÿæˆè´¨é‡ã€‚\n\n2. **è¯­ä¹‰å¯¹é½ï¼ˆå¥å­çº§ï¼‰**ï¼š**SBERT**\n   - **å®šä¹‰**ï¼šä½¿ç”¨ Sentence-BERT è®¡ç®—ç”Ÿæˆå¥å­ä¸å‚è€ƒå¥å­åœ¨åµŒå…¥ç©ºé—´ä¸­çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚\n   - **å…¬å¼**ï¼š`SBERT = cos(ğ²_s_pred, ğ²_s_gt)`\n   - **ä½œç”¨**ï¼šä»æ•´ä½“å¥å­è¯­ä¹‰å±‚é¢è¯„ä¼°ç”Ÿæˆå†…å®¹ä¸çœŸå®å€¼çš„æ¥è¿‘ç¨‹åº¦ã€‚\n\n3. **åŠ¨ä½œå®‰å…¨æ€§**ï¼š**åŠ¨ä½œå‡†ç¡®ç‡ï¼ˆAction Accuracy, AAï¼‰**\n   - **å®šä¹‰**ï¼šç›´æ¥è¯„ä¼°æ¨¡å‹è¾“å‡ºçš„æœºå™¨äººåŠ¨ä½œæ˜¯å¦ä¸çœŸå®åŠ¨ä½œæ ‡ç­¾åŒ¹é…ã€‚\n   - **å…¬å¼**ï¼š`AA = (1/N) * Î£ ğ•€(y_c_pred = y_c_gt)`ï¼Œå…¶ä¸­ ğ•€ æ˜¯æŒ‡ç¤ºå‡½æ•°ã€‚\n   - **ä½œç”¨**ï¼šè¿™æ˜¯**æœ€å…³é”®çš„å†³ç­–çº§æŒ‡æ ‡**ï¼Œç›´æ¥åæ˜ æ¨¡å‹é€‰æ‹©æ­£ç¡®å¯¼èˆªæŒ‡ä»¤ï¼ˆå¦‚â€œå‰è¿›â€ã€â€œå·¦è½¬â€ã€â€œåœæ­¢â€ç­‰ï¼‰çš„èƒ½åŠ›ã€‚\n\n### ä¸‰ã€ä¸»è¦æ€§èƒ½æå‡ä¸å…³é”®å‘ç°\nè®ºæ–‡çš„æ ¸å¿ƒå‘ç°æ­ç¤ºäº†æç¤ºè®¾è®¡ä¸æ¨¡å‹ç±»å‹ã€æ•°æ®é›†ä¹‹é—´çš„å¤æ‚è€¦åˆå…³ç³»ï¼Œå…¶æ€§èƒ½æå‡ä¸»è¦ä½“ç°åœ¨**åŠ¨ä½œå‡†ç¡®ç‡ï¼ˆAAï¼‰** ä¸Šï¼š\n\n1. **å¯¹äºæœªå¾®è°ƒçš„ GPT-4oï¼ˆé›¶æ ·æœ¬è®¾ç½®ï¼‰**ï¼š\n   - **æœ€ä½³æç¤ºç»„åˆ**ï¼š**æ¨ç†å¯¼å‘æç¤ºï¼ˆRç³»åˆ—ï¼‰** + **ä¸äººç±»ç«äº‰ï¼ˆA1/R1/PR1ï¼‰**ã€‚\n   - **æ€§èƒ½è¡¨ç°**ï¼šåœ¨ SNEI æ•°æ®é›†ä¸Šï¼ŒAA ä»æ— ç³»ç»Ÿæç¤ºçš„ 0.550 æå‡è‡³ **0.633**ï¼ˆR1æç¤ºï¼‰ï¼›åœ¨ MUSON æ•°æ®é›†ä¸Šï¼Œä» 0.531 æå‡è‡³ **0.563**ã€‚\n   - **å…³é”®å‘ç°**ï¼šä¸å…¶å®ƒAIç³»ç»Ÿç«äº‰çš„æç¤ºï¼ˆA2/R2/PR2ï¼‰è¡¨ç°æœ€å·®ã€‚**ä¸æ°å½“çš„æç¤ºè®¾è®¡ï¼ˆå¦‚åŸå§‹ç³»ç»Ÿæç¤ºï¼‰ç”šè‡³å¯èƒ½ä½¿æ€§èƒ½ä½äºæ— æç¤ºåŸºçº¿**ã€‚\n\n2. **å¯¹äºå¾®è°ƒåçš„å°å‹ VLMs**ï¼ˆå¦‚ Phi-2, StableLM-2, TinyLlamaï¼‰ï¼š\n   - **æœ€ä½³æç¤ºç»„åˆ**ï¼š**æ„ŸçŸ¥-æ¨ç†é›†æˆæç¤ºï¼ˆPRç³»åˆ—ï¼‰** + **ä¸è¿‡å»è‡ªæˆ‘ç«äº‰ï¼ˆA3/R3/PR3ï¼‰**ã€‚\n   - **æ€§èƒ½æå‡ç¤ºä¾‹**ï¼ˆä»¥ TinyLlama-1.1B åœ¨ MUSON æ•°æ®é›†ä¸ºä¾‹ï¼‰ï¼š\n     - æ— ç³»ç»Ÿæç¤ºæ—¶ï¼ŒAA ä¸º 0.375ã€‚\n     - ä½¿ç”¨æœ€ä½³æç¤ºï¼ˆPR3ï¼‰åï¼ŒAA æå‡è‡³ **0.563**ï¼Œç›¸å¯¹æå‡æ˜¾è‘—ã€‚\n   - **å…³é”®å‘ç°**ï¼š\n     - **è¯­ä¹‰-å†³ç­–ä¸åŒ¹é…**ï¼šå¾®è°ƒä¸»è¦å¤§å¹…æå‡äº†è¯­ä¹‰çº§æŒ‡æ ‡ï¼ˆBERT-F1, SBERTï¼‰ï¼Œä½†å¯¹åŠ¨ä½œå‡†ç¡®ç‡ï¼ˆAAï¼‰çš„æå‡æœ‰é™ã€‚ä¾‹å¦‚ï¼Œå¾®è°ƒåè¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†å¯èƒ½å¾ˆé«˜ï¼Œä½†é€‰é”™åŠ¨ä½œçš„æƒ…å†µä¾ç„¶å­˜åœ¨ã€‚\n     - **æç¤ºçš„æ ¸å¿ƒä½œç”¨**ï¼šè®ºæ–‡æå‡ºçš„ç³»ç»Ÿæç¤ºèƒ½å¸¦æ¥**ä¸æˆæ¯”ä¾‹çš„å·¨å¤§åŠ¨ä½œå‡†ç¡®ç‡æå‡**ã€‚è¿™è¡¨æ˜ï¼Œç²¾å¿ƒè®¾è®¡çš„æç¤ºä¸»è¦å……å½“äº†**å†³ç­–çº§çº¦æŸ**ï¼Œå¼•å¯¼æ¨¡å‹å°†æ›´å¥½çš„å†…éƒ¨è¯­ä¹‰ç†è§£è½¬åŒ–ä¸ºæ­£ç¡®çš„æœ€ç»ˆåŠ¨ä½œï¼Œè€Œéæ™®éå¢å¼ºå…¶è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ã€‚\n\n**æ€»ç»“**ï¼šè®ºæ–‡é€šè¿‡ç³»ç»Ÿçš„æç¤ºè®¾è®¡ï¼Œåœ¨èµ„æºå—é™çš„å°å‹ VLM ä¸Šï¼Œå®ç°äº†**å†³ç­–å‡†ç¡®æ€§ï¼ˆAAï¼‰çš„æ˜¾è‘—æå‡**ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ‹¥æŒ¤æˆ–æ¨¡ç³Šåœºæ™¯ä¸­ã€‚è¿™ä¸ºåœ¨å®é™…æœºå™¨äººéƒ¨ç½²ä¸­ï¼Œä¸å¢åŠ æ¨¡å‹è®¡ç®—æˆæœ¬è€Œé€šè¿‡â€œæç¤ºå·¥ç¨‹â€æå‡ç¤¾äº¤åˆè§„å¯¼èˆªæ€§èƒ½æä¾›äº†é‡è¦ä¾æ®ã€‚",
            "timestamp": "2026-01-27T15:58:46.473707",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡ç¬¬6èŠ‚â€œLimitations and Future Workâ€ï¼Œä½œè€…æ˜ç¡®æŒ‡å‡ºäº†å½“å‰ç ”ç©¶çš„ä¸‰ä¸ªä¸»è¦å±€é™æ€§ï¼š\n\n- **æ•°æ®é›†è§„æ¨¡å’Œå¤šæ ·æ€§æœ‰é™**ï¼šå®éªŒä½¿ç”¨çš„SNEIå’ŒMUSONæ•°æ®é›†è™½ç„¶è¦†ç›–äº†å®¤å†…å¤–å¤šç§åœºæ™¯ï¼Œä½†å…¶è§„æ¨¡ï¼ˆSNEI: 325ä¸ªæ ·æœ¬ï¼›MUSON: 800ä¸ªæ ·æœ¬ï¼‰å’Œåœºæ™¯å¤šæ ·æ€§ä»ä¸è¶³ä»¥å®Œå…¨ä»£è¡¨çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚ä¾‹å¦‚ï¼ŒæœªåŒ…å«**æç«¯å¤©æ°”ï¼ˆå¦‚é›ªå¤©ï¼‰ã€å¤œé—´æ¡ä»¶ã€äº¤é€šä¿¡å·åœºæ™¯**ç­‰æ›´å…·æŒ‘æˆ˜æ€§çš„æƒ…å†µï¼Œè¿™å¯èƒ½å½±å“æ¨¡å‹åœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›è¯„ä¼°ã€‚\n\n- **æç¤ºè®¾è®¡æ–¹æ³•çš„é™æ€æ€§**ï¼šç ”ç©¶ä¸“æ³¨äº**é™æ€çš„ã€æ–‡æœ¬å¼çš„ç³»ç»Ÿæç¤ºè®¾è®¡**ï¼Œæœªæ¢ç´¢**è‡ªé€‚åº”æˆ–å¯å­¦ä¹ çš„æç¤ºæœºåˆ¶**ã€‚è¿™æ„å‘³ç€æç¤ºä¸€æ—¦è®¾å®šï¼Œåœ¨æœºå™¨äººå®é™…è¿è¡Œä¸­æ— æ³•æ ¹æ®ç¯å¢ƒåé¦ˆè¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œé™åˆ¶äº†å…¶åœ¨åŠ¨æ€ã€ä¸å¯é¢„æµ‹ç¯å¢ƒä¸­çš„å®æ—¶é€‚åº”èƒ½åŠ›ã€‚\n\n- **åŠ¨ä½œç©ºé—´çš„ç®€åŒ–å‡è®¾**ï¼šå®éªŒå‡è®¾äº†ä¸€ä¸ª**å—é™çš„ç¦»æ•£åŠ¨ä½œç©ºé—´**ï¼ˆå¦‚â€œå‰è¿›â€ã€â€œå·¦è½¬â€ç­‰6ä¸ªåŸºæœ¬æŒ‡ä»¤ï¼‰ã€‚è¿™ç®€åŒ–äº†è¯„ä¼°ï¼Œä½†**ä¸ç°å®ä¸–ç•Œä¸­æœºå™¨äººéœ€è¦çš„è¿ç»­ã€å¹³æ»‘æ§åˆ¶å­˜åœ¨å·®è·**ï¼Œå¯èƒ½æ— æ³•å……åˆ†éªŒè¯æç¤ºè®¾è®¡åœ¨æ›´å¤æ‚ã€é«˜ç»´æ§åˆ¶ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡æå‡ºäº†æ˜ç¡®çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼š\n\n- **æ‰©å±•æ•°æ®é›†**ï¼šæ„å»º**æ›´å¤§è§„æ¨¡ã€æ›´å¤šæ ·åŒ–**çš„ç¤¾ä¼šå¯¼èˆªæ•°æ®é›†ï¼Œæ¶µç›–æ›´å¹¿æ³›çš„çœŸå®åœºæ™¯ï¼ˆå¦‚ä¸åŒå¤©æ°”ã€å…‰ç…§ã€äººç¾¤å¯†åº¦ã€æ–‡åŒ–å·®å¼‚ï¼‰ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°å’Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n- **å¼€å‘è‡ªé€‚åº”æç¤ºæœºåˆ¶**ï¼šç ”ç©¶**åœ¨çº¿æç¤ºé€‚åº”æ–¹æ³•**ï¼Œä¾‹å¦‚ç»“åˆ**å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**ï¼Œä½¿ç³»ç»Ÿèƒ½æ ¹æ®å®æ—¶ç¯å¢ƒåé¦ˆï¼ˆå¦‚è¡Œäººååº”ã€å¯¼èˆªæˆåŠŸç‡ï¼‰åŠ¨æ€è°ƒæ•´æç¤ºç­–ç•¥ï¼Œå®ç°æ›´æ™ºèƒ½ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å†³ç­–ã€‚\n\n- **æ¢ç´¢è¿ç»­æ§åˆ¶ä¸å¤æ‚ç¯å¢ƒ**ï¼šå°†ç ”ç©¶æ‰©å±•åˆ°**è¿ç»­åŠ¨ä½œç©ºé—´**ï¼ˆå¦‚é€Ÿåº¦ã€è§’åº¦çš„ç²¾ç»†æ§åˆ¶ï¼‰å’Œæ›´å¤æ‚çš„**å…·èº«ç¯å¢ƒ**ï¼ˆå¦‚å¤šæ¥¼å±‚å»ºç­‘ã€åŠ¨æ€éšœç¢ç‰©ï¼‰ï¼Œä»¥éªŒè¯æç¤ºè®¾è®¡åœ¨æ›´æ¥è¿‘å®é™…æœºå™¨äººéƒ¨ç½²åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚\n\n- **æ·±åŒ–æ¨¡å‹ä¸æç¤ºçš„è€¦åˆç ”ç©¶**ï¼šè¿›ä¸€æ­¥åˆ†æ**æç¤ºè®¾è®¡ã€æ¨¡å‹æ¶æ„ä¸æ•°æ®é›†ç‰¹æ€§ä¹‹é—´çš„è€¦åˆæ•ˆåº”**ï¼Œå¯èƒ½å¼€å‘å‡ºé’ˆå¯¹ç‰¹å®šæ¨¡å‹æˆ–åœºæ™¯çš„è‡ªåŠ¨åŒ–æç¤ºä¼˜åŒ–æ¡†æ¶ã€‚\n\n**æ ¸å¿ƒæ€»ç»“**ï¼šå½“å‰å·¥ä½œä¸ºå°è§„æ¨¡VLMçš„é«˜æ•ˆæç¤ºè®¾è®¡æä¾›äº†é‡è¦æ´è§ï¼Œä½†å…¶**é™æ€ã€ç¦»æ•£ã€æ•°æ®å—é™**çš„ç‰¹æ€§é™åˆ¶äº†ç›´æ¥éƒ¨ç½²åˆ°å¼€æ”¾åŠ¨æ€ä¸–ç•Œã€‚æœªæ¥çš„æ ¸å¿ƒæ–¹å‘æ˜¯**å¢å¼ºé€‚åº”æ€§ã€æ‰©å±•æ³›åŒ–æ€§ã€é€¼è¿‘ç°å®æ§åˆ¶**ï¼Œä»¥æ¨åŠ¨ç¤¾ä¼šåˆè§„å¯¼èˆªä»å®éªŒå®¤èµ°å‘å®é™…åº”ç”¨ã€‚",
            "timestamp": "2026-01-27T15:59:08.563028",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T03:45:33Z",
    "created_at": "2026-01-27T15:53:16.489539",
    "updated_at": "2026-01-27T15:53:16.489546"
}