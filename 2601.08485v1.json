{
    "id": "2601.08485v1",
    "title": "AME-2: Agile and Generalized Legged Locomotion via Attention-Based Neural Map Encoding",
    "authors": [
        "Chong Zhang",
        "Victor Klemm",
        "Fan Yang",
        "Marco Hutter"
    ],
    "abstract": "Ë¶ÅÂú®ÂêÑÁßçÂú∞ÂΩ¢‰∏äÂÆûÁé∞ÊïèÊç∑‰∏îÈÄöÁî®ÁöÑËÖøÂºèËøêÂä®ÔºåÈúÄË¶ÅÊÑüÁü•‰∏éÊéßÂà∂ÁöÑÁ¥ßÂØÜÈõÜÊàêÔºåÂ∞§ÂÖ∂ÊòØÂú®Â≠òÂú®ÈÅÆÊå°ÂíåÁ´ãË∂≥ÁÇπÁ®ÄÁñèÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ∑≤Âú®Ë∑ëÈÖ∑ËØæÁ®ã‰∏≠Â±ïÁé∞Âá∫ÊïèÊç∑ÊÄßÔºå‰ΩÜÈÄöÂ∏∏‰æùËµñ‰∫éÁ´ØÂà∞Á´ØÁöÑÊÑüÁü•ËøêÂä®Ê®°ÂûãÔºåÂÖ∂Ê≥õÂåñËÉΩÂäõÂíåÂèØËß£ÈáäÊÄßÊúâÈôê„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÈíàÂØπÈÄöÁî®ËøêÂä®ÁöÑÊñπÊ≥ïÈÄöÂ∏∏Ë°®Áé∞Âá∫ÊúâÈôêÁöÑÊïèÊç∑ÊÄßÔºå‰∏îÂú®ËßÜËßâÈÅÆÊå°ÊñπÈù¢Â≠òÂú®Âõ∞Èöæ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜAME-2ÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éÊïèÊç∑ÈÄöÁî®ËøêÂä®ÁöÑÁªü‰∏ÄÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂÆÉÂú®ÊéßÂà∂Á≠ñÁï•‰∏≠ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÂú∞ÂõæÁºñÁ†ÅÂô®„ÄÇËØ•ÁºñÁ†ÅÂô®ÊèêÂèñÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÂú∞ÂõæÁâπÂæÅÔºåÂπ∂Âà©Áî®Ê≥®ÊÑèÂäõÊú∫Âà∂ËÅöÁÑ¶‰∫éÊòæËëóÂå∫ÂüüÔºå‰∏∫Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÊéßÂà∂ÁîüÊàêÂèØËß£Èáä‰∏îÊ≥õÂåñÊÄßÂº∫ÁöÑÂµåÂÖ•Ë°®Á§∫„ÄÇÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ≠¶‰π†ÁöÑÂú∞ÂõæÊûÑÂª∫ÊµÅÁ®ãÔºåËØ•ÊµÅÁ®ãËÉΩÂ§üÊèê‰æõÂø´ÈÄü„ÄÅ‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÁöÑÂú∞ÂΩ¢Ë°®Á§∫ÔºåÂØπÂô™Â£∞ÂíåÈÅÆÊå°ÂÖ∑ÊúâÈ≤ÅÊ£íÊÄßÔºåÂπ∂‰Ωú‰∏∫Á≠ñÁï•ËæìÂÖ•„ÄÇÂÆÉ‰ΩøÁî®Á•ûÁªèÁΩëÁªúÂ∞ÜÊ∑±Â∫¶ËßÇÊµãËΩ¨Êç¢‰∏∫Â∏¶Êúâ‰∏çÁ°ÆÂÆöÊÄßÁöÑÂ±ÄÈÉ®È´òÁ®ãÔºåÂπ∂‰∏éÈáåÁ®ãËÆ°‰ø°ÊÅØËûçÂêà„ÄÇËØ•ÊµÅÁ®ãËøò‰∏éÂπ∂Ë°åÊ®°ÊãüÈõÜÊàêÔºå‰ΩøÊàë‰ª¨ËÉΩÂ§üÈÄöËøáÂú®Á∫øÂú∞ÂõæÊûÑÂª∫ËÆ≠ÁªÉÊéßÂà∂Âô®ÔºåÊúâÂä©‰∫éÊ®°ÊãüÂà∞Áé∞ÂÆûÁöÑËøÅÁßª„ÄÇÊàë‰ª¨Âú®ÂõõË∂≥ÂíåÂèåË∂≥Êú∫Âô®‰∫∫‰∏ä‰ΩøÁî®ÊâÄÊèêÂá∫ÁöÑÂú∞ÂõæÊûÑÂª∫ÊµÅÁ®ãÈ™åËØÅ‰∫ÜAME-2ÔºåÊâÄÂæóÊéßÂà∂Âô®Âú®Ê®°ÊãüÂíåÁúüÂÆû‰∏ñÁïåÂÆûÈ™å‰∏≠ÂùáÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÊïèÊç∑ÊÄß‰ª•ÂèäÂØπÊú™ËßÅÂú∞ÂΩ¢ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ",
    "url": "https://arxiv.org/abs/2601.08485v1",
    "html_url": "https://arxiv.org/html/2601.08485v1",
    "html_content": "AME-2: Agile and Generalized Legged Locomotion via Attention-Based Neural Map Encoding\nChong Zhang\n123‚àó\n, Victor Klemm\n1\n, Fan Yang\n1\n, Marco Hutter\n1\nWebsite:\nsites.google.com/leggedrobotics.com/ame-2\n1\nRobotic Systems Lab, ETH Zurich, Switzerland\n2\nSecure, Reliable, and Intelligent Systems Lab, ETH Zurich, Switzerland\n3\nETH AI Center, Switzerland\n‚àó\nCorresponding: chong.zhang@ai.ethz.ch\nAbstract\nAchieving agile and generalized legged locomotion across terrains requires tight integration of perception and control, especially under occlusions and sparse footholds.\nExisting methods have demonstrated agility on parkour courses but often rely on end-to-end sensorimotor models with limited generalization and interpretability. By contrast, methods targeting generalized locomotion typically exhibit limited agility and struggle with visual occlusions.\nWe introduce AME-2, a unified reinforcement learning (RL) framework for agile and generalized locomotion that incorporates a novel attention-based map encoder in the control policy.\nThis encoder extracts local and global mapping features and uses attention mechanisms to focus on salient regions, producing an interpretable and generalized embedding for RL-based control.\nWe further propose a learning-based mapping pipeline that provides fast, uncertainty-aware terrain representations robust to noise and occlusions, serving as policy inputs. It uses neural networks to convert depth observations into local elevations with uncertainties, and fuses them with odometry. The pipeline also integrates with parallel simulation so that we can train controllers with online mapping, aiding sim-to-real transfer.\nWe validate AME-2 with the proposed mapping pipeline on a quadruped and a biped robot, and the resulting controllers demonstrate strong agility and generalization to unseen terrains in simulation and in real-world experiments.\nI\nIntroduction\nAgile and generalized locomotion is essential for legged robots to operate reliably in diverse real-world environments. It requires tightly coupled real-time perception and control, robustness to sensor noise and occlusions, agility through whole-body control, and precise behavior on terrains with sparse footholds. Designing such perceptive locomotion systems that are simultaneously agile and able to generalize across diverse terrains remains a critical challenge.\nClassical pipelines address perceptive locomotion by combining model-based control with explicit mapping and state estimation\n[\n1\n,\n2\n,\n3\n,\n4\n,\n5\n,\n6\n,\n7\n,\n8\n]\n. Typically, they maintain a state estimator\n[\n9\n]\n, build elevation maps from sensor data\n[\n10\n,\n11\n]\n, and use model-based planning and control methods to command the robot based on this mapping information. These approaches have proven effective for deliberate walking and carefully planned maneuvers on moderately structured terrain, where accurate state estimation and dense, slowly changing maps can be maintained. However, their reliance on precise models and often deterministic optimal control makes them sensitive to errors in estimation and mapping, especially under visual occlusions and violations of modeling assumptions\n[\n4\n]\n. Elevation maps are often updated at lower rates than the control loop and heuristically filtered\n[\n11\n,\n2\n,\n4\n]\n, which often requires per-terrain tuning to ensure safety and makes it difficult to obtain a general solution for diverse terrains that demand agile motions. Moreover, the optimization and planning components in these pipelines tend to be computationally heavy, which can limit agility on real robots. These limitations motivate learning-based approaches such as RL that can better exploit raw sensor data, operate under uncertainty, and produce more dynamic behaviors.\nLearning-based methods, in particular RL, have recently shown strong potential for legged locomotion\n[\n12\n]\n. One line of work combines RL with classical state estimation and mapping pipelines, using elevation maps or related representations as policy inputs\n[\n13\n,\n14\n,\n15\n,\n16\n,\n17\n,\n18\n,\n19\n,\n20\n,\n21\n,\n22\n,\n23\n,\n24\n,\n25\n,\n26\n]\n. These methods can improve robustness and generalization compared to purely model-based controllers, but they also inherit the computational cost and failure modes of the underlying estimation and mapping systems\n[\n27\n,\n28\n]\n. In simulation, noise is typically injected into both the state estimates and the mapping observations. A common strategy is to assume a fully observed egocentric map during training and then tune the real-world mapping stack to approximate this assumption\n[\n13\n,\n21\n,\n15\n]\n, which simplifies training but can make performance sensitive when occlusions occur or when agile whole-body contacts violate estimator assumptions. Another strategy is to heavily randomize state-estimation and mapping noise so that the controller becomes robust and conservative under mapping uncertainties in dense terrains\n[\n14\n]\n, yet such policies tend to struggle on terrains that require very high agility or precise foothold placement.\nMore recently, neural network‚Äìbased mapping has been explored, where networks are trained in simulation to reconstruct maps that can be used as policy inputs\n[\n29\n,\n30\n,\n31\n,\n32\n,\n33\n,\n34\n]\n. While this yields efficient mapping suitable for real-time control, the learned mappings are usually fitted to particular terrain distributions and sensor configurations, offer limited generalization to unseen terrains, and do not explicitly model uncertainties such as occlusions.\nAnother line of work directly maps raw exteroceptive perception and proprioception to actions\n[\n35\n,\n36\n,\n37\n,\n38\n,\n39\n,\n40\n,\n41\n,\n42\n,\n43\n,\n44\n,\n45\n,\n46\n,\n47\n]\n. These sensorimotor policies have demonstrated highly agile behaviors on challenging courses\n[\n36\n,\n38\n,\n42\n]\n, but typically exhibit limited generalization beyond the training environments and offer little interpretability, since terrain reasoning is implicitly encoded in the policy. More recent sensorimotor generalist models\n[\n48\n]\ndistill a single policy from multiple expert controllers to achieve strong performance, yet such methods still show limited generalization to unseen terrains and often require finetuning on new terrains, as evaluated in our benchmarks (Sec.\nVII\n). Overall, existing RL-based approaches tend to trade off agility, generalization, mapping efficiency, and interpretability rather than addressing these requirements within a unified framework.\nFigure 1:\nOur method enables agile and generalized legged locomotion across diverse terrains with onboard sensing and computation.\nIn this work, we present AME-2, a unified RL framework for perceptive legged locomotion with an\nA\nttention-based\nM\nap\nE\nncoding architecture trained jointly with the controller. The AME-2 encoder, built upon the design in\n[\n15\n]\n, first extracts local features (pixel-wise representation of terrain details) and global features (capturing global terrain context) from the elevation map, then uses the global features together with proprioception to assign attention weights to the local features. The resulting weighted local features are concatenated with the global features and proprioception to form a terrain-aware representation for policy learning. This design allows the policy to downweigh local regions that are less relevant for the current task, improving generalization to new terrains. Because the attention is conditioned on global terrain context, the policy can also learn distinct attention and motion patterns across different terrains. To enable learning agile controllers with this representation, we adopt a goal-reaching locomotion formulation from prior works\n[\n20\n,\n26\n,\n49\n,\n40\n,\n32\n,\n50\n]\nand adapt it so that the same reward functions and training settings can be used for different robots. Together, the AME-2 encoder and unified training formulation enable training a single generalist policy with terrain-aware skills, which in our experiments exhibits strong generalization to unseen terrains while maintaining high agility.\nTo enable real-world deployment of agile and generalized locomotion controllers, we develop a learning-based elevation mapping pipeline to remove the dependency on classical mapping stacks while maintaining generalization. The pipeline projects depth images into local grids and uses a lightweight neural network trained via Bayesian learning\n[\n51\n]\nto predict local elevations with per-cell uncertainty estimates. These local maps are fused into a consistent global frame with odometry, providing a fast and uncertainty-aware representation that accounts for occlusions and sensor noise. We then query egocentric elevations and associated uncertainties as the map inputs for the controller. This design is inspired by\n[\n52\n]\n, which fuses per-frame map predictions instead of training temporal networks for off-road navigation, thereby reducing data requirements and mitigating overfitting. Furthermore, we synthesize random terrain data, including many terrains that are not feasible for the robots, to broaden the training distribution for the local map predictor and improve generalization.\nThe same mapping pipeline runs both in parallel simulation and on the real robots, enabling online mapping during training rather than relying on idealized or hand-tuned maps. For training efficiency, we adopt a teacher‚Äìstudent scheme\n[\n48\n,\n53\n]\n: a teacher policy is first trained with ground-truth elevation maps, and a student policy with the same AME-2 architecture is then trained using the proposed mapping under teacher supervision alongside RL. The resulting student policy is directly deployable, operates with the lightweight, uncertainty-aware mapping in the loop, and retains the generalization and agility of the teacher.\nWe evaluate AME-2 on an ANYmal-D quadruped\n[\n54\n]\nand a LimX TRON1 biped\n[\n55\n]\nboth in simulation and on real hardware. As shown in Fig.\n1\n, the resulting controllers exhibit strong agility and robust generalization to a wide range of terrains. The proposed mapping pipeline produces high-quality elevation maps in real time, supporting these agile motions with onboard perception and computation.\nThe main contributions of this work are summarized as follows:\n1.\nWe propose\nAME-2\n, a unified RL framework with an attention-based map encoder to achieve agile and generalized legged locomotion.\n2.\nWe develop a\nlightweight, uncertainty-aware elevation mapping\npipeline that generalizes to diverse terrains, explicitly models occlusions and noise, and helps bridge the sim-to-real gap.\n3.\nWe use a\nteacher-student scheme\nthat yields deployable controllers using the learned mapping while retaining agility, generalization, and interpretability.\n4.\nWe demonstrate a\nstate-of-the-art combination of agility and generalization\nwith a quadruped and a biped robot across diverse challenging terrains under the same training setup.\nII\nRelated Works\nII-A\nPerceptive Locomotion with Explicit Mapping\nEarly explorations of perceptive locomotion use model-based methods with either prebuilt maps or classical online mapping. Most of them first perform foothold or trajectory planning and then apply a model-based tracking controller\n[\n1\n,\n2\n,\n3\n,\n5\n,\n6\n,\n7\n,\n8\n]\n, while some directly optimize a reactive controller over feasible footholds\n[\n4\n]\n. Although these methods offer strong guarantees and can generalize well under their modeling assumptions, their real-world performance is limited by model mismatch, state-estimation uncertainties, mapping errors, and computational burden.\nTo address these limitations, RL emerges as an alternative for perceptive locomotion\n[\n12\n]\n. One line of work keeps a strong model-based component and uses RL mainly as an add-on\n[\n21\n,\n22\n,\n56\n]\n. DTC\n[\n21\n]\naugments a model-based planner with an RL tracking controller to improve robustness while maintaining generalization, but the planner remains sensitive to mapping uncertainties and constrains agility through its modeling assumptions and computational cost. RLOC\n[\n22\n]\ninstead uses a model-based controller to track an RL planner, yet the overall performance is capped by the limitations of the controller.\nOther approaches remove the model-based modules entirely while still relying on explicit maps. When given prebuilt maps, RL controllers can exhibit highly agile motions\n[\n26\n]\n. With classical online mapping, they outperform model-based counterparts in terms of agility and robustness\n[\n13\n,\n14\n,\n15\n,\n16\n,\n17\n,\n18\n,\n19\n,\n23\n,\n24\n]\n, but their agility remains limited by the update rate and quality of the mapping. In addition, many such controllers suffer from generalization issues, as they tend to overfit to the training terrains\n[\n21\n]\n. A recent work\n[\n15\n]\nshows that RL can achieve generalized locomotion across diverse terrains, but its agility is still constrained by the online classical mapping stack and the learning framework.\nMore recently, RL-based methods have begun to use learned maps, where neural networks reconstruct egocentric terrains for the controller in real time\n[\n29\n,\n30\n,\n31\n,\n32\n,\n33\n,\n34\n]\n. These approaches demonstrate strong agility and successful real-world deployment, but the learned maps often fit the training distribution and show limited generalization in unseen, unstructured environments. In contrast, our work combines a fully RL-based controller with a learned mapping module designed for generalization. As a result, our policies achieve agility comparable to previous state of the art while enabling generalized locomotion across diverse terrains.\nII-B\nPerceptive Locomotion with Raw Sensor Data\nPerceptive locomotion with raw sensor data seeks to bypass the limitations of explicit mapping. Instead of constructing and maintaining a map, some works directly train policies from camera images\n[\n35\n,\n36\n,\n37\n,\n38\n,\n39\n,\n41\n,\n44\n,\n46\n,\n42\n,\n43\n,\n45\n,\n47\n]\nor lidar point clouds\n[\n40\n]\n. Early approaches in this direction still separate perception and control: a visual module outputs footholds or trajectories, which are then tracked by model-based controllers\n[\n57\n]\n. Between explicit mapping and fully end-to-end policies, other works learn intermediate representations from raw sensors (such as ray distances\n[\n49\n]\nand neural volumetric memory\n[\n58\n]\n) that remain cheap to compute while enabling fast control.\nMany recent methods adopt a monolithic design, where a single neural network maps raw sensor streams and proprioception directly to joint commands. Within this family, some distill deployable student policies from privileged teacher policies ‚Äî often using maps as privileged inputs ‚Äî to achieve efficient and stable learning\n[\n35\n,\n36\n,\n38\n,\n46\n,\n42\n,\n43\n,\n48\n]\n, while others learn from scratch with the help of representation learning techniques such as world models\n[\n44\n]\nor privileged-information reconstruction\n[\n41\n,\n45\n]\n.\nThe current state of the art in agile generalist locomotion\n[\n48\n]\ntrains multiple mapping-based teacher controllers and distills them into a single generalist student policy that operates directly on raw depth images. This student policy combines the skills of the teachers and can be finetuned efficiently on new terrains. However, its zero-shot generalization to previously unseen environments is limited.\nIn contrast, our work also trains neural networks from raw sensor data, but uses them exclusively within the mapping process rather than in a monolithic perception-to-action policy. Because our learned mapping is lightweight and well suited to massive parallelization in simulation, we can run the full mapping loop during training and then deploy exactly the same mapping at test time, preserving both speed and consistency. Without training a monolithic policy that directly maps raw sensor data to joint actions, we still obtain an agile controller while also achieving strong generalization across diverse terrains.\nII-C\nLearning-Based Mapping\nLearning-based mapping aims to retain the structure and interpretability of explicit maps while overcoming the computational and modeling limitations of classical mapping. Classical mapping pipelines combine geometric projection, hand-tuned filtering, and probabilistic fusion to produce high-quality maps, and can handle a wide range of complex terrains\n[\n10\n,\n11\n,\n23\n,\n24\n]\n. However, these methods are often too slow for agile locomotion and require heuristic, terrain-specific filtering to cope with occlusions and sensor noise.\nTo enable faster control, several works directly train terrain reconstruction modules in or after the policy training loop\n[\n29\n,\n30\n,\n31\n,\n32\n,\n33\n,\n34\n]\n. These learned mappings can infer local geometry quickly enough to support agile controllers, but are usually trained on a narrow distribution of environments, showing limited generalization beyond the training terrains.\nRecent off-road navigation approaches take a different route to obtain uncertainty-aware neural mapping with improved accuracy and data efficiency. In particular,\n[\n52\n]\nleverages neural processes\n[\n59\n]\nto model terrain elevation: a network predicts per-frame local elevation maps together with associated uncertainties, which are then fused over time using odometry. By predicting local maps with uncertainty instead of learning a fully end-to-end sensor-to-map model, this approach reduces the data requirements for accurate estimation and improves generalization in complex off-road environments. However, the computational costs of the models in\n[\n52\n]\nare too high (each environment takes\n>\n13\n>13\nGB GPU memory) and cannot be deployed with thousands of parallel simulation environments.\nInspired by these ideas, we design a learning-based elevation mapping module tailored for agile legged locomotion. We project depth-camera point clouds into local elevation grids and train lightweight, robust networks to predict local elevation and its uncertainty from these inputs, which are then fused over time using odometry. This design suppresses sensor noise and naturally models occlusions as regions of high uncertainty. We further improve generalization through extensive randomization in a custom data pipeline. The resulting module is lightweight enough to run in massively parallel simulation and fast enough to support highly agile motions in deployment, while providing an uncertainty-aware terrain representation for our controllers.\nIII\nSystem Overview\nFigure 2:\nAn overview of our system. We use RL to train a teacher policy with ground-truth mapping in simulation, and a student policy under the teacher‚Äôs supervision with our proposed neural mapping. The policies output joint-level actions that actuate the robots to reach position and heading goals, as illustrated by the yellow wireframes.\nIII-A\nFormulation\nAs illustrated in Fig.\n2\n, we use RL to train terrain-aware locomotion policies that reach position and heading goals. We formulate the problem as a partially observable Markov decision process (POMDP) and optimize the policies using PPO\n[\n60\n]\nin parallel simulation\n[\n61\n]\n. By maximizing goal-reaching rewards (detailed in Sec.\nIV\n), the policies learn agile locomotion skills that enable them to traverse diverse terrains. We first train a privileged teacher policy and then transfer the skills to a deployable student policy, which is also detailed in Sec.\nIV\n.\nIII-B\nController Inputs and Outputs\nOur controllers run at 50 Hz. The policy actions\na\na\nare joint PD targets\n[\n62\n]\ntracked at 400 Hz on real hardware.\nProprioception observations include base linear velocity\nv\nb\nv_{b}\n(only accessible to the teacher), base angular velocity\nœâ\nb\n\\omega_{b}\n, projected gravity\ng\nb\ng_{b}\n, joint positions\nq\nq\n, joint velocities\nq\nÀô\n\\dot{q}\n, previous actions, and goal commands\nc\nc\n.\nGround-truth mapping represents each point in the egocentric elevation grid by its 3D coordinates\n(\nx\n,\ny\n,\nz\n)\n(x,y,z)\n. Our neural mapping augments this with an uncertainty channel and produces a 4D representation\n(\nx\n,\ny\n,\nz\n,\nu\n)\n(x,y,z,u)\nfor each point, where\nu\nu\nis the uncertainty metric.\nIV\nLocomotion Control\nIV-A\nPolicy Architecture and AME-2 Encoder\nFigure 3:\nAn illustration of our AME-2 policy architecture.\nLeft top:\nThe general abstract of our policies. Proprioceptive observations are encoded into a proprioception embedding, which is used together with the mapping as the inputs of the AME-2 encoder to produce the map embedding. The proprioception embedding and the map embedding are then concatenated and fed into an MLP to generate actions.\nLeft bottom:\nThe AME-2 encoder design. We extract both local features and global features from the map, and then use the global features and the proprioception embedding to produce the attention-weighted local features.\nL\nL\nand\nW\nW\nare the length and width of the map,\nd\nm\n‚Äã\na\n‚Äã\np\nd_{map}\nis the dimension of map representation (3 for the teacher, 4 for the student),\nd\nP\n‚Äã\nE\nd_{PE}\nis the dimension of proprioception embedding.\nRight:\nThe proprioception encoder designs. We have different designs for teacher and student policies to facilitate sim-to-real transfer, while both designs can fit into our overall architecture.\nThe general policy architecture is illustrated in Fig.\n3\n, with the AME-2 encoder serving as the core feature extractor for the mapping input. We have a proprioception encoder to embed proprioceptive observations, and the AME-2 encoder to embed the map observations based on the proprioception embedding. We feed both the proprioception embedding and the map embedding through a multilayer perceptron (MLP) to output the actions.\nThe AME-2 encoder first extracts local map features with a convolutional neural network (CNN) and computes a positional embedding for each point with an MLP. These are then fused by another MLP to obtain pointwise local features. Next, an additional MLP followed by max pooling over the pointwise features produces global features that capture the overall terrain context. We combine these global features with a proprioceptive embedding through an MLP to obtain a query vector, which is used in a multi-head attention (MHA) module\n[\n63\n]\nwith the pointwise local features serving as keys and values. This yields a weighted local feature embedding that focuses on important terrain regions based on the current proprioceptive state and global context. Finally, the global features and weighted local features are concatenated to form the map embedding fed into the policy‚Äôs action decoder.\nThis design builds on the attention-based map encoder in previous work\n[\n15\n]\n, which we refer to as AME-1 for clarity. Unlike the AME-1 encoder, our AME-2 encoder additionally computes global features and uses them to weigh the local features. This enables more generalized locomotion over complex terrains, where motion patterns should vary with the terrain. We demonstrate the resulting performance gap between AME-1 and AME-2 encoders in Sec.\nVI\nand Sec.\nVII\n.\nThe choice of proprioception encoder in our architecture is flexible, as long as it produces an effective proprioception embedding. Hence, we use different designs for the teacher and the student. For the teacher, since ground-truth proprioceptive observations are available, we use a plain MLP over them to obtain the proprioception embedding. For the student, because of various environmental uncertainties and following lessons from previous works\n[\n64\n,\n65\n,\n66\n]\n, we stack the proprioceptive observations (except base linear velocities and commands) from the past 20 steps and use Long-Short I/O (LSIO)\n[\n66\n]\nto obtain a temporal embedding of both robot states and environment dynamics. This temporal embedding and the commands are then jointly fed into an MLP to produce the student‚Äôs proprioception embedding.\nIV-B\nAsymmetric Actor Critic\nWe adopt asymmetric actor‚Äìcritic training\n[\n67\n]\n. For the critic, we do not use the same attention-based design as in the actor, because the critic does not need to generalize beyond the training terrains and optimizing an MHA module with\n(\nL\n√ó\nW\n)\n(L\\times W)\nlocal feature inputs is costly. Instead, we use the mixture-of-experts (MoE) design from\n[\n68\n]\n, which is powerful for function fitting yet much more computationally efficient to optimize. Note that, although we can train a generalized teacher with our policy architecture and an MoE critic, an MoE actor does not yield a generalized teacher, as shown in Sec.\nVII\n.\nOn top of the noiseless proprioceptive observations used by the teacher actor and the ground-truth map, we additionally provide the contact state of each link to the critic. We do not provide such contact information to the teacher, since we can already train a robust, generalized teacher without it, and a larger information gap between teacher and student can hurt student training\n[\n69\n]\n.\nWe also apply left-right symmetry augmentation from\n[\n32\n]\nto the critic to improve sample efficiency and motion style, but not to the actor to avoid additional computational cost of actor optimization.\nThe critic design is shared for the teacher and the student.\nIV-C\nTeacher-Student RL\nWe use Teacher‚ÄìStudent RL\n[\n70\n,\n14\n,\n48\n,\n53\n]\nto facilitate sim-to-real transfer. Although directly training a student policy with our mapping pipeline is also possible, simulation can run at only about half the speed compared to using ground-truth mapping and requires significantly more GPU memory, making it computationally inefficient for us.\nOur student training objective linearly combines the RL losses from PPO, the action distillation losses from\n[\n53\n]\n, and a representation loss given by the mean squared error between the teacher and student map embeddings. In practice, we also disable the PPO surrogate loss during the first few iterations while using a large learning rate. These design choices enable stable, efficient, and well-aligned student training, which we will ablate in Sec.\nVII\n.\nIV-D\nEnvironments\nIV-D\n1\nRewards\nTABLE I:\nRewards for Controller Training.\nReward Term\nExpression\nWeight (\n√ó\nd\n‚Äã\nœÑ\n‚àó\n\\times d\\tau^{*}\n)\nNotes\nTask Rewards\nPosition Tracking\nEq.¬†(\n1\n)\n100\n100\nHeading Tracking\nEq.¬†(\n3\n)\n50\n50\nMoving to Goal\nEq.¬†(\n4\n)\n5\n5\nStanding at Goal\nEq.¬†(\n5\n)\n5\n5\nRegularization and Penalties\nEarly Termination\n1\n1\nif early termination triggered\n‚àí\n10\n/\nd\n‚Äã\nœÑ\n-10/d\\tau\nUndesired Events\n1\n1\nfor each undesired event\n‚àí\n1\n-1\nBase Roll Rate\n[\nœâ\nb\n]\nx\n2\n[\\omega_{b}]_{x}^{2}\n‚àí\n0.1\n-0.1\nJoint Regularization\n‚à•\nq\nÀô\n‚à•\n2\n+\n0.01\n‚Äã\n‚à•\nœÑ\n‚à•\n2\n+\n0.001\n‚Äã\n‚à•\nq\n¬®\n‚à•\n2\n\\lVert\\dot{q}\\rVert^{2}+0.01\\lVert\\tau\\rVert^{2}+0.001\\lVert\\ddot{q}\\rVert^{2}\n‚àí\n0.001\n-0.001\nœÑ\n\\tau\ndenotes the joint torques\nAction Smoothness\n‚à•\na\nt\n‚àí\na\nt\n‚àí\n1\n‚à•\n2\n\\lVert a_{t}-a_{t-1}\\rVert^{2}\n‚àí\n0.01\n-0.01\nLink Contact Forces\n‚à•\nmax\n‚Å°\n(\nF\ncon\n‚àí\nG\n,\nùüé\n)\n‚à•\n2\n\\lVert\\max{(F_{\\rm con}-G,\\mathbf{0})\\rVert^{2}}\n‚àí\n0.00001\n-0.00001\nF\ncon\nF_{\\rm con}\ndenotes the contact forces for each link and\nG\nG\nis the robot weight\nLink Acceleration\n‚àë\nl\n‚à•\nv\nÀô\nl\n‚à•\n\\sum_{l}\\lVert\\dot{v}_{l}\\rVert\n‚àí\n0.001\n-0.001\nv\nl\nv_{l}\nis the velocity of link\nl\nl\n; summing over all links\nSimulation Fidelity\nsumming over all joints\nJoint Position Limits\n‚àë\nj\nmax\n‚Å°\n(\n0\n,\nq\nj\n‚àí\n0.95\n‚Äã\nq\nj\nmax\n,\n0.95\n‚Äã\nq\nj\nmin\n‚àí\nq\nj\n)\n\\sum_{j}\\max(0,q_{j}-0.95q^{\\max}_{j},0.95q^{\\min}_{j}-q_{j})\n‚àí\n1000\n-1000\nq\nj\nmax\nq_{j}^{\\max}\nand\nq\nj\nmin\nq_{j}^{\\min}\nare position limits for joint\nj\nj\nJoint Velocity Limits\n‚àë\nj\nmax\n‚Å°\n(\n0\n,\n|\nq\nÀô\nj\n|\n‚àí\n0.9\n‚Äã\nq\nÀô\nj\nmax\n)\n\\sum_{j}\\max(0,|\\dot{q}_{j}|-0.9\\dot{q}^{\\max}_{j})\n‚àí\n1\n-1\nq\nÀô\nj\nmax\n\\dot{q}^{\\max}_{j}\nis the velocity limit for joint\nj\nj\nJoint Torque Limits\n‚àë\nj\nmax\n‚Å°\n(\n0\n,\n|\nœÑ\nj\n|\n‚àí\n0.8\n‚Äã\nœÑ\nj\nmax\n)\n\\sum_{j}\\max(0,|\\tau_{j}|-0.8\\tau^{\\max}_{j})\n‚àí\n1\n-1\nœÑ\nj\nmax\n\\tau^{\\max}_{j}\nis the torque limit for joint\nj\nj\nd\n‚àó\n‚Äã\nœÑ\n=\n0.02\n{}^{*}d\\tau=0.02\nsec is the policy interval.\nFigure 4:\nTraining and Test Terrains. We train our locomotion policies on primitive terrains of three categories: \\tiny1‚Éù dense, \\tiny2‚Éù climbing, and \\tiny3‚Éù sparse. To test generalization, we evaluate the policies on four terrains that are either entirely unseen or combinations of training and/or unseen terrains.\nWe use three types of rewards shared across both quadruped and biped training environments, as listed in Table\nI\n: task rewards which incentivize goal-reaching behaviors, regularization rewards which improve stability and safety, and simulation-fidelity rewards, which penalize near-limit joint states that can cause unrealistic simulation.\nThere are four task reward terms, respectively for goal position tracking, goal heading tracking, moving towards goal, and standing at goal.\nFollowing\n[\n49\n]\n, we define the position tracking reward as\nr\nposition\n‚Äã\n_\n‚Äã\ntracking\n=\n1\n1\n+\n0.25\n‚Äã\nd\nx\n‚Äã\ny\n2\n‚ãÖ\nt\nmask\n‚Äã\n(\n4\n)\n,\nr_{\\rm position\\_tracking}=\\frac{1}{1+0.25d_{xy}^{2}}\\cdot t_{\\rm mask}(4),\n(1)\nwhere\nd\nx\n‚Äã\ny\nd_{xy}\nis the horizontal distance from the robot to the goal position, and\nt\nmask\n‚Äã\n(\n‚ãÖ\n)\nt_{\\rm mask}(\\cdot)\nis a time-based mask function defined as\nt\nmask\n‚Äã\n(\nT\n)\n=\n1\nT\n‚ãÖ\nùüè\n‚Äã\n(\nt\nleft\n<\nT\n)\n,\nt_{\\rm mask}(T)=\\frac{1}{T}\\cdot\\mathbf{1}(t_{\\rm left}<T),\n(2)\nwith\nt\nleft\nt_{\\rm left}\ndenoting the remaining time of the current episode. Further, we define the heading tracking reward as\nr\nheading\n‚Äã\n_\n‚Äã\ntracking\n=\n1\n1\n+\nd\nyaw\n2\n‚ãÖ\nt\nmask\n‚Äã\n(\n2\n)\n‚ãÖ\nùüè\n‚Äã\n(\nd\nx\n‚Äã\ny\n<\n0.5\n)\n,\nr_{\\rm heading\\_tracking}=\\frac{1}{1+d_{\\rm yaw}^{2}}\\cdot t_{\\rm mask}(2)\\cdot\\mathbf{1}(d_{xy}<0.5),\n(3)\nIntuitively, these two tracking rewards encourage the robot to be at the goal position with the desired heading at the end of the episode, without constraining how it reaches the goal, thereby allowing complex locomotion skills to emerge for traversing terrains.\nTo further facilitate exploration, we introduce a moving-to-goal reward:\nr\nmove\n=\nùüè\n(\nd\nx\n‚Äã\ny\n<\n0.5\n‚à®\n(\ncos\nŒ∏\nv\nb\n,\ngoal\n>\n0.5\n‚àß\nv\nmin\n‚â§\n‚à•\n[\nv\nb\n]\nx\n‚Äã\ny\n‚à•\n‚â§\nv\nmax\n)\n)\n,\n\\begin{split}r_{\\text{move}}=\\mathbf{1}\\Big(&d_{xy}<0.5\\;\\lor\\\\\n&\\big(\\cos\\theta_{v_{b},\\text{goal}}>0.5\\land v_{\\min}\\leq\\|[{v}_{b}]_{xy}\\|\\leq v_{\\max}\\big)\\Big),\\end{split}\n(4)\nwhere\nŒ∏\nv\nb\n,\ngoal\n\\theta_{v_{b},\\text{goal}}\nis the angle between the base velocity and the vector from the base to the goal position,\nv\nmin\n=\n0.3\n‚Äã\nm/s\nv_{\\min}=0.3\\,\\text{m/s}\nis the lower bound of the horizontal base velocity\n[\nv\nb\n]\nx\n‚Äã\ny\n[{v}_{b}]_{xy}\nto be considered as moving, and\nv\nmax\n=\n2\n‚Äã\nm/s\nv_{\\max}=2\\,\\text{m/s}\nis an upper bound chosen according to hardware and SLAM constraints.\nIn other words,\nr\nmove\n=\n1\nr_{\\text{move}}=1\nif the robot is already close to the goal or if it is moving roughly towards the goal, and\nr\nmove\n=\n0\nr_{\\text{move}}=0\notherwise.\nTo enforce a stable standing posture after reaching the goal, we define the standing reward:\nr\nstand\n=\nùüè\n‚Äã\n(\nd\nx\n‚Äã\ny\n<\n0.5\n‚àß\nd\nyaw\n<\n0.5\n)\n‚ãÖ\nexp\n‚Å°\n(\n‚àí\nd\nfoot\n+\nd\ng\n+\nd\nq\n+\nd\nx\n‚Äã\ny\n4\n)\n,\n\\begin{split}r_{\\rm stand}=&\\mathbf{1}(d_{xy}<0.5\\land d_{\\rm yaw}<0.5)\\\\\n&\\cdot\\exp\\left(-\\frac{d_{\\rm foot}+d_{\\rm g}+d_{\\rm q}+d_{xy}}{4}\\right),\\end{split}\n(5)\nwhere\nd\nfoot\nd_{\\rm foot}\nis the number of feet not in contact divided by the total number of feet,\nd\ng\n=\n1\n‚àí\n[\ng\nb\n]\nz\n2\nd_{\\rm g}=1-[g_{b}]_{z}^{2}\nmeasures the base tilt relative to gravity, and\nd\nq\nd_{\\rm q}\nis the mean deviation of joint positions from the standing reference. This reward ensures the robot maintains a static, upright configuration at the goal.\nThe regularization and simulation fidelity rewards are presented in Table\nI\n. We set all weights to integer powers of 10, and make each term‚Äôs contribution\n1\n‚àº\n2\n1\\sim 2\norders of magnitude smaller than the task rewards. This enables successful learning without extensive weight tuning.\nIn the regularization rewards, we penalize each occurrence of these undesired events:\n‚Ä¢\nSpinning too fast: Yaw rate\n|\n[\nœâ\nb\n]\nz\n|\n>\n2.0\n\\lvert[\\omega_{b}]_{z}\\rvert>2.0\nrad/s, which can trigger drifts in the odometry.\n‚Ä¢\nLeaping on flat terrain: All feet are off the ground when the elevation difference is\n<\n30\n<30\ncm.\n‚Ä¢\nNon-foot contacts: We penalize both general non-foot contacts and non-foot contact switches (from no contact to contact). We separate these to allow stable interactions when necessary (e.g., climbing) while discouraging unnecessary non-foot contacts.\n‚Ä¢\nStumbling: Any link having a horizontal contact force larger than the vertical force.\n‚Ä¢\nSlippage: Any link moving while in contact.\n‚Ä¢\nSelf-collision: Collisions between robot links.\nUnlike previous works\n[\n19\n,\n23\n,\n33\n]\n, we do not explicitly reward or penalize foot contact positions in sparse terrains; instead, we formulate rewards for all robot links to enable emergent whole-body contacts. Those foothold position rewards are also difficult to define in a terrain-agnostic way: for example, quadruped climbing benefits from active knee contacts and near-edge foot placements, as shown in Fig.\n10\n, whereas on sparse terrains near-edge foot placements are undesirable.\nIV-D\n2\nTermination\nTo facilitate training, we set the following early termination conditions:\n‚Ä¢\nBad orientation: The projected gravity vector satisfies\n|\n[\ng\nb\n]\nx\n|\n>\n0.985\n|[g_{b}]_{x}|>0.985\n,\n|\n[\ng\nb\n]\ny\n|\n>\n0.7\n|[g_{b}]_{y}|>0.7\n, or\n[\ng\nb\n]\nz\n>\n0.0\n[g_{b}]_{z}>0.0\n(robot flipped).\n‚Ä¢\nBase collision: The base link experiences a contact force larger than the robot‚Äôs total weight.\n‚Ä¢\nHigh thigh acceleration: The acceleration of any thigh link exceeds a specific threshold. We design this to reduce impact during jumping, as stiff landings can damage hardware or reduce lifespan. Interestingly, we use typical data from dog (\n60\n‚Äã\nm\n/\ns\n2\n60\\rm\\ m/s^{2}\n[\n71\n]\n) and human (\n100\n‚Äã\nm\n/\ns\n2\n100\\rm\\ m/s^{2}\n[\n72\n]\n) biokinetics literature, and they directly work to emerge impact absorption or avoidance behaviors, which is shown in Sec.\nVI\n.\n‚Ä¢\nStagnation: Movement in the past 5‚Äâs is less than 0.5‚Äâm while the robot is still\n>\n1\n>1\nm away from the goal position.\nTriggering any of these conditions terminates the episode immediately and incurs early termination penalties.\nIV-D\n3\nTerrains and Curriculum\nWe train our controllers on primitive terrains and evaluate generalization on complex test terrains. Both are shown in Fig.\n4\n, while quantitative results are presented in Sec.\nVII\n. We use a terrain curriculum that scales difficulty from easy to hard (detailed in Appendix\nA\n), similar to\n[\n26\n]\n. To stabilize learning across different challenging terrains, we estimate the robot‚Äôs success rate using an exponential moving average. If the robot reaches the goal and its estimated success rate exceeds 0.5, we promote the environment to the next difficulty level. Conversely, the level is demoted if the robot remains\n>\n4\n>4\nm away from the goal (which is the average starting distance). Once the robot passes the highest level, the environment resets to a random difficulty.\nAdditionally, we use a perception noise curriculum and an initial heading curriculum. In the first 20% iterations of teacher training, we linearly increase mapping noise from zero to the maximum level, and simultaneously expand the initial heading from facing the goal to a random yaw in\n[\n‚àí\nœÄ\n,\nœÄ\n]\n[-\\pi,\\pi]\n.\nIV-D\n4\nDomain Randomization\nFigure 5:\nThe controller can reuse the built map for the same terrain, enabled by our mapping randomization. (a) When the robot moves in the direction of the dotted orange arrow, it observes a partial map of the staircase (visualized on the right). (b) After going down along the solid orange arrow, it perceives a complete map of the staircase (visualized on the right) when attempting to move back along the dotted orange arrow.\nWe employ domain randomization\n[\n73\n]\nduring training to enhance robustness and facilitate sim-to-real transfer. We apply the following randomization setup:\n‚Ä¢\nRobot Dynamics: We uniformly randomize the payload, friction coefficients, and actuation delays. For the TRON1 biped, we additionally randomize PD gains and motor armatures, following\n[\n15\n]\n.\n‚Ä¢\nObservation Noise: We apply uniform noises to the policy observations. We also degrade the student‚Äôs depth clouds by simulating missing points and sensor artifacts.\n‚Ä¢\nMapping: During student training, we randomly select a subset of environments to access complete maps, while others rely on partial online maps. This setup enables map reuse when the robot traverses the same terrain repeatedly, as illustrated in Fig.\n5\n. We also corrupt the student‚Äôs mapping by randomly removing points and assigning random height values with high uncertainties. Additionally, we simulate mapping drifts for both the teacher and the student.\nImplementation details are provided in Appendix\nB\n.\nIV-E\nTraining and Deployment\nIV-E\n1\nTraining Setup\nWe train our controllers in Isaac Gym\n[\n61\n]\nwith the PPO implementation in RSL-RL\n[\n74\n]\n. The both teacher policies are trained with 80000 iterations, and the both student policies are trained with 40000 iterations (surrogate loss disabled during first 5000 iterations). The hyperparameters are listed in Appendix\nC\n. The training cost for the ANYmal-D policies is\n‚àº\n60\n\\sim 60\nRTX-4090-days with\n8\n8\nGPUs in parallel. The training cost for the TRON1 policies is\n‚àº\n30\n\\sim 30\nRTX-4090-days with\n4\n4\nGPUs in parallel.\nThe difference between both robots‚Äô training costs are due to the mapping sizes. For ANYmal-D, we use a map of size\n36\n√ó\n14\n36\\times 14\n, with\n8\n8\n-cm resolution, centered at\nx\n=\n0.6\n‚Äã\nm\n,\ny\n=\n0\n‚Äã\nm\nx=0.6\\rm{\\ m},y=0\\rm{\\ m}\nin the base frame. For TRON1, we use a map of size\n18\n√ó\n13\n18\\times 13\n, with\n8\n8\n-cm resolution, centered at\nx\n=\n0.32\n‚Äã\nm\n,\ny\n=\n0\n‚Äã\nm\nx=0.32\\rm{\\ m},y=0\\rm{\\ m}\nin the base frame. These numbers are designed based on the robot dimensions and terrain sizes.\nIV-E\n2\nDeployment Setup\nWe deploy the controllers using ONNX Runtime\n[\n75\n]\n. On the onboard Intel Core i7-8850H CPUs, the policy inference time is approximately\n2\n2\nms.\nIV-E\n3\nOther Sim-to-Real Designs\nWe model the actuator dynamics for sim-to-real transfer. In simulation, we use the actuator network\n[\n76\n]\nfor ANYmal-D, and an identified DC-motor model for TRON1. We clip the applied torques using the joint torque-velocity constraints in\n[\n77\n]\n.\nTo enable continuous deployment (infinite-horizon execution), we define the command observations differently from prior goal-reaching works\n[\n48\n,\n49\n,\n50\n]\n. Following prior practice, the critic receives the full command: the goal‚Äôs relative position, the sine and cosine of the relative yaw, and the remaining episode time. The actor, however, receives a modified command representation: we clip the observed goal distance to a maximum of\n2\n‚Äã\nm\n2\\,\\text{m}\nand remove the remaining time. Additionally, if the actual goal distance exceeds\n2\n‚Äã\nm\n2\\,\\text{m}\n, we randomize the observed yaw command during training. This design decouples policy deployment from finite-horizon training and simplifies steering control.\nV\nNeural Mapping\nV-A\nMapping Pipeline\nFigure 6:\nOur proposed mapping pipeline. For each frame of depth clouds, we project it into local grids, and use a lightweight neural network to predict elevation estimations with uncertainties. The predictions are then fused into the global map via the odometry, and local maps can be queried from the global map to serve as the controller inputs. In visualization, the green points indicate the depth points or elevation estimations, and the blue lines indicate the uncertainties.\nOur proposed neural mapping pipeline is illustrated in Fig.\n6\n. It can not only achieve real-time computation on the hardware, but also run with thousands of parallel environments in simulation, thereby bridging the sim-to-real gap with identical mapping pipelines.\nFor each sensing frame, we project the point cloud onto a local 2D height grid. If multiple points fall into the same cell, we keep the maximum\nz\nz\n-value, which is most relevant for locomotion, and we assign a fixed minimum value to cells with no points. The resulting local elevations, however, are often noisy and incomplete due to occlusions. To mitigate this, we use a lightweight CNN trained with Bayesian learning (detailed in Sec.\nV-B\n) to jointly predict base-relative elevations and their uncertainties (in the form of log-variance). The predicted uncertainties capture both measurement noise and occlusions, while the elevation predictions themselves can also suppress noise.\nFigure 7:\nTerrain meshes used for mapping model training. We use four meshes: the locomotion training terrains, randomly stacked boxes, random heightfields, and random floating boxes. The colored points visualized are the elevation scans from random poses.\nWe then use the odometry poses to fuse these local predictions into a global grid map\n‚Ñ≥\n\\mathcal{M}\n. It has two layers: the elevation layer and the uncertainty layer (in the form of variance). The global map is initialized as flat ground at the robot‚Äôs standing height (ground height relative to the base) with large uncertainties. At each new frame, given the local elevation estimations with uncertainties and the current base pose, we project the local predictions to the global grid cells. For any global grid point\n(\nu\n,\nv\n)\n(u,v)\ncovered by the local grids, we denote the new estimation as\nh\nt\nh_{t}\nand its uncertainty as\nœÉ\nt\n2\n\\sigma_{t}^{2}\n, fusing them with existing values\nh\np\n‚Äã\nr\n‚Äã\ni\n‚Äã\no\n‚Äã\nr\nh_{prior}\nand\nœÉ\np\n‚Äã\nr\n‚Äã\ni\n‚Äã\no\n‚Äã\nr\n2\n\\sigma^{2}_{prior}\n.\nWe do not use standard Bayesian fusion because repeated observations of the same occluded or uncertain area should not have reduce uncertainties simply due to consistent predictions. Instead, we employ a\nProbabilistic Winner-Take-All\nstrategy. First, we calculate an effective measurement variance\nœÉ\n^\nt\n2\n\\hat{\\sigma}^{2}_{t}\nthat is lower-bounded by the prior to prevent over-confidence:\nœÉ\n^\nt\n2\n=\nmax\n‚Å°\n(\nœÉ\nt\n2\n,\n0.5\n‚ãÖ\nœÉ\np\n‚Äã\nr\n‚Äã\ni\n‚Äã\no\n‚Äã\nr\n2\n)\n.\n\\hat{\\sigma}^{2}_{t}=\\max(\\sigma^{2}_{t},0.5\\cdot\\sigma^{2}_{prior}).\n(6)\nAn update is considered valid only if the effective measurement variance is not significantly larger than the prior (\nœÉ\n^\nt\n2\n<\n1.5\n‚Äã\nœÉ\np\n‚Äã\nr\n‚Äã\ni\n‚Äã\no\n‚Äã\nr\n2\n\\hat{\\sigma}^{2}_{t}<1.5\\sigma^{2}_{prior}\n), or if the absolute uncertainty is low (\nœÉ\n^\nt\n2\n<\n0.2\n2\n\\hat{\\sigma}^{2}_{t}<0.2^{2}\n).\nFor valid updates, we determine the probability\np\nwin\np_{\\rm win}\nof overwriting the map based on the relative precision:\np\nwin\n=\n(\nœÉ\n^\nt\n2\n)\n‚àí\n1\n(\nœÉ\n^\nt\n2\n)\n‚àí\n1\n+\n(\nœÉ\np\n‚Äã\nr\n‚Äã\ni\n‚Äã\no\n‚Äã\nr\n2\n)\n‚àí\n1\n.\np_{\\rm win}=\\frac{(\\hat{\\sigma}^{2}_{t})^{-1}}{(\\hat{\\sigma}^{2}_{t})^{-1}+(\\sigma^{2}_{prior})^{-1}}.\n(7)\nFinally, the map is updated stochastically. We sample\nŒæ\n‚àº\nùí∞\n‚Äã\n[\n0\n,\n1\n]\n\\xi\\sim\\mathcal{U}[0,1]\nand let the new prediction take over the cell if the sample falls within the probability threshold:\n(\nh\nn\n‚Äã\ne\n‚Äã\nw\n,\nœÉ\nn\n‚Äã\ne\n‚Äã\nw\n2\n)\n‚Üê\n{\n(\nh\nt\n,\nœÉ\n^\nt\n2\n)\nif\n‚Äã\nŒæ\n<\np\nwin\n,\n(\nh\np\n‚Äã\nr\n‚Äã\ni\n‚Äã\no\n‚Äã\nr\n,\nœÉ\np\n‚Äã\nr\n‚Äã\ni\n‚Äã\no\n‚Äã\nr\n2\n)\notherwise.\n(h_{new},\\sigma^{2}_{new})\\leftarrow\\begin{cases}(h_{t},\\hat{\\sigma}^{2}_{t})&\\text{if }\\xi<p_{\\rm win},\\\\\n(h_{prior},\\sigma^{2}_{prior})&\\text{otherwise.}\\end{cases}\n(8)\nFor the controller inputs, we then just query the grids around the robot pose in the latest global map.\nThis\nProbabilistic Winner-Take-All\nstrategy offers the following benefits:\n‚Ä¢\nThe uncertainty of the same occluded point will not decrease through consistent predictions.\n‚Ä¢\nOver-confident predictions, if not consistent, cannot take over the cell.\n‚Ä¢\nThe system can rapidly update the map in response to dynamic terrain changes when high-confidence measurements are available.\n‚Ä¢\nThe pipeline is easy to integrate with parallel simulation, and fast enough to run on the hardware.\nV-B\nTraining\nFigure 8:\nMapping model architecture. We use a lightweight U-Net\n[\n78\n]\nmodel with a gated residual design for the estimation.\nWe train a CNN to predict per-frame elevation estimates and uncertainties with synthetic data and random terrains. The technical implementations are detailed below.\nV-B\n1\nTerrains and Data Sampling\nWe use both the locomotion training terrain meshes and additional procedurally generated terrains to train the mapping model, as shown in Fig.\n7\n. Instead of relying on physical simulation, we directly sample local elevation grids from random poses above these meshes using raytracing with Warp\n[\n79\n]\n, enabling sampling at hundreds of thousands of frames per second on a single RTX 4090 GPU.\nV-B\n2\nTraining Data Synthesis\nWe apply the following augmentations to the sampled local elevation grids:\n‚Ä¢\nadditive uniform noise with random magnitude on each cell;\n‚Ä¢\nrandom cropping of the map from the four borders;\n‚Ä¢\nsimulated occlusions with random sensor positions and field of view;\n‚Ä¢\nrandom ranges to clip the elevations;\n‚Ä¢\nrandom missing points and outliers at random ratios.\nBy doing so, we synthesize diverse, noisy, and partially observable local grids as the model inputs, and the original ground-truth elevations as the labels.\nV-B\n3\nModel and Optimization\nWe train the model to reconstruct the ground-truth elevations with\nŒ≤\n‚àí\n\\beta-\nNLL loss (\nŒ≤\n=\n0.5\n\\beta=0.5\n) from\n[\n80\n]\n:\nL\n0.5\n=\nùîº\nX\n,\nY\n‚Äã\n[\nsg\n‚Äã\n[\nœÉ\n^\n‚Äã\n(\nX\n)\n]\n‚Äã\n(\nlog\n‚Å°\nœÉ\n^\n2\n‚Äã\n(\nX\n)\n2\n+\n(\nY\n‚àí\nŒº\n^\n‚Äã\n(\nX\n)\n)\n2\n2\n‚Äã\nœÉ\n^\n2\n‚Äã\n(\nX\n)\n)\n]\n,\nL_{0.5}=\\mathbb{E}_{X,Y}\\left[\\mathrm{sg}\\left[\\hat{\\sigma}(X)\\right]\\left(\\frac{\\log\\hat{\\sigma}^{2}(X)}{2}+\\frac{(Y-\\hat{\\mu}(X))^{2}}{2\\hat{\\sigma}^{2}(X)}\\right)\\right],\n(9)\nwhere\nX\nX\ndenotes the inputs,\nY\nY\ndenotes the ground-truth elevations, and\nŒº\n^\n‚Äã\n(\nX\n)\n\\hat{\\mu}(X)\nand\nœÉ\n^\n2\n‚Äã\n(\nX\n)\n\\hat{\\sigma}^{2}(X)\ndenote the predicted estimation and variance, respectively. The operator\nsg\n‚Äã\n[\n‚ãÖ\n]\n\\mathrm{sg}[\\cdot]\ndenotes the stop-gradient operation. Compared to the standard negative log-likelihood (NLL) loss used in classical Bayesian learning\n[\n51\n]\n, this formulation reduces the tendency of the model to overestimate uncertainty on hard samples to trivially reduce the loss. It encourages the model to output high uncertainty when accurate predictions are not possible, and low uncertainty together with accurate predictions when they are, thereby capturing the noise and occlusions.\nFor batched optimization, terrain roughness can vary significantly across samples. As a result, flat terrains can dominate the batch loss and reduce the effective emphasis on challenging cases. To mitigate this, we re-weigh samples in each batch during training using their total variation (TV)\n[\n81\n]\n:\nTV\n‚Äã\n(\nY\nb\n)\n\\displaystyle\\mathrm{TV}(Y_{b})\n=\n1\nH\n‚Äã\nW\n‚Äã\n(\n‚Äñ\n‚àá\nx\nY\nb\n‚Äñ\n1\n+\n‚Äñ\n‚àá\ny\nY\nb\n‚Äñ\n1\n)\n,\n\\displaystyle=\\frac{1}{HW}\\Big(\\|\\nabla_{x}Y_{b}\\|_{1}+\\|\\nabla_{y}Y_{b}\\|_{1}\\Big),\n(10)\nw\nb\n\\displaystyle w_{b}\n=\nTV\n‚Äã\n(\nY\nb\n)\n‚àë\nb\n‚Ä≤\n=\n1\nB\nTV\n‚Äã\n(\nY\nb\n‚Ä≤\n)\n+\nŒµ\n.\n\\displaystyle=\\frac{\\mathrm{TV}(Y_{b})}{\\sum_{b^{\\prime}=1}^{B}\\mathrm{TV}(Y_{b^{\\prime}})+\\varepsilon}.\nHere,\nY\nb\nY_{b}\nis the ground-truth elevations of the\nb\nb\n-th sample in a batch, and\nH\nH\nand\nW\nW\nare the height and width. The weight\nw\nb\nw_{b}\nis normalized across the batch, and\nŒµ\n\\varepsilon\nis a small positive constant. By doing so, we assign higher weights to samples with larger elevation variations.\nWe use a shallow U-Net\n[\n78\n]\nmodel with a gated residual design, as illustrated in Fig.\n8\n. The CNNs output the uncertainty, a raw estimation, and a gating map. The final estimation is obtained by the gated combination of the raw estimation and the input, preserving accuracy in clearly observed areas while selectively overwriting noisy or occluded areas.\nWe train the models on 54 million frames for each robot, each model takes less than 1 hour to converge. For ANYmal-D, the local grids are of shape\n51\n√ó\n31\n51\\times 31\n, with\n4\n4\n-cm resolution, centered at\nx\n=\n1.0\n‚Äã\nm\n,\ny\n=\n0\n‚Äã\nm\nx=1.0\\rm{\\ m},y=0\\rm{\\ m}\nin the base frame. For TRON1, the local grids are of shape\n31\n√ó\n31\n31\\times 31\n, with\n4\n4\n-cm resolution, centered at\nx\n=\n0.6\n‚Äã\nm\n,\ny\n=\n0\n‚Äã\nm\nx=0.6\\rm{\\ m},y=0\\rm{\\ m}\nin the base frame.\nV-C\nSimulation Integration and Deployment\nWhen deployed in simulation with 1000 parallel ANYmal-D environments, the model‚Äôs inference time is below\n0.3\n0.3\nms, and the GPU memory consumption is about\n3\n3\nGB (for TRON1, these numbers are roughly 60% of those for ANYmal-D). Storing\n8\n,\nm\n√ó\n8\n,\nm\n8,\\mathrm{m}\\times 8,\\mathrm{m}\nglobal maps for 1000 environments requires about\n0.3\n0.3\nGB, and the global map size can be flexibly chosen since we recenter the map around the robot when it approaches the boundary. Obtaining depth clouds and handling other intermediate overheads takes an additional\n‚àº\n1.2\n\\sim 1.2\nGB of GPU memory per 1000 ANYmal-D environments.\nOn real hardware, the mapping pipeline takes approximately\n5\n5\nms per frame on the onboard CPU, with around\n2.5\n2.5\nms spent on model inference using ONNX Runtime. This enables fast, low-latency mapping that can keep up with the depth camera‚Äôs frame rate. For ANYmal-D, we merge the point clouds from two front facing cameras for processing. For TRON1, we use the only front facing camera.\nRegarding odometry, we use CompSLAM\n[\n82\n]\nwith Graph-MSF\n[\n83\n]\non ANYmal-D to obtain a high-frequency, accurate LiDAR‚Äìinertial odometry. On TRON1, we use DLIO\n[\n84\n]\ninstead, as it better handles the platform‚Äôs higher accelerations.\nNotably, although these odometry solutions can run reliably at high frequency, their velocity estimates are observed to be noisy and delayed. This is intuitive: for example, a 1¬†cm position drift over a 20¬†ms timestep can translate to a velocity error of 0.5¬†m/s, yet has little impact on map fusion. Therefore, we remove the linear velocity observations from the student policy.\nVI\nResults\nVI-A\nAgility Comparison with Prior Art\nFigure 9:\nOur ANYmal-D controller zero-shots the hardest parkour and rubble pile terrains reported in prior work\n[\n32\n,\n48\n]\n.\nWe achieve state-of-the-art agility on both ANYmal-D and TRON1, measured by the difficulty of the terrains they can climb up and down.\nOn ANYmal-D, despite being trained only on primitive terrains, our policy zero-shots the hardest parkour and rubble pile terrains reported in prior work\n[\n32\n,\n48\n]\n, as shown in Fig.\n9\n.\nOn TRON1, our policy climbs up platforms up to\n0.48\n0.48\nm and climbs down up to\n0.88\n0.88\nm, while prior work on bipeds reports\n0.5\n0.5\nm platforms\n[\n17\n]\nusing the Unitree H1 robot (with\n4\n√ó\n4\\times\npeak torque and\n1.5\n√ó\n1.5\\times\nbase height compared to TRON1).\nAcross all terrains, our controllers on both robots reach peak forward velocities above\n1.5\n1.5\nm/s, demonstrating a combination of agility and terrain-aware locomotion. On top of this agility, our controllers also exhibit stronger generalization than existing methods, as shown below and benchmarked in Sec.\nVII\n.\nVI-B\nReal World Results\nVI-B\n1\nQuadruped Parkour\nFigure 10:\nOur system enables ANYmal-D to move back and forth on a parkour course, a terrain that is unseen during training of both the controller and the mapping model.\nWe demonstrate the agility and generalization of our trained ANYmal-D controller on a parkour course, as shown in Fig.\n10\n. This course is not included in the training terrains of either the locomotion controller or the mapping model, yet our system can stably traverse it at speeds of up to\n2\n2\nm/s, composing climbing and jumping maneuvers.\nVI-B\n2\nSparse Terrain Locomotion\nFigure 11:\nOur systems enable ANYmal-D and TRON1 to traverse diverse sparse terrains. (a) ANYmal-D traverses a\n19\n19\n-cm wide balance beam, which is in the training terrains. (b) TRON1 traverses two unfixed floating\n19\n19\n-cm wide blocks, which make an unseen curved beam terrain. (c) ANYmal-D traverses two unfixed beams with gaps, which is unseen during training. (d) TRON1 traverses a beam followed by a gap, which is an unseen combination during training. (e) ANYmal-D traverses two rows of\n19\n19\n-cm wide stepping stones with\n10\n10\n-cm height differences, which is unseen during training. (f) TRON1 traverses diamond-layout stepping stones, which is unseen during training.\nAs shown in Fig.\n11\n, our controllers enable both ANYmal-D and TRON1 to traverse a variety of sparse terrains, many of which are unseen during training, exhibiting generalization capabilities.\nVI-B\n3\nBiped maneuvering\nFigure 12:\nTRON1 maneuvers over a 38-cm-high platform, a gap, a staircase, and rough terrain, showcasing its omnidirectional perceptive locomotion capabilities. Orange arrows indicate the robot‚Äôs approximate trajectories.\nTo demonstrate the omnidirectional perceptive locomotion capabilities of our controller, we command TRON1 to traverse a 38-cm-high platform, a gap, a staircase, and rough terrain, as shown in Fig.\n12\n. The robot executes smooth maneuvers in all directions.\nVI-B\n4\nRobustness\nFigure 13:\nOur controllers are robust to moving terrains. (a) TRON1 climbs over an unlocked tiltable platform cart. (b) TRON1 climbs onto and balances on an unlocked tiltable platform cart. (c) ANYmal-D recovers from unfixed tilted stepping stones using its knees.\nWe demonstrate the robustness of our controllers on moving terrains. As shown in Fig.\n13\n, TRON1 can traverse or balance on a platform cart with unlocked wheels and spring suspension that can easily tilt to one side with the robot weight. We also show that ANYmal-D can recover from unfixed tilted stepping stones using its knees, after an occasional trip causes a misstep that tilts the blocks.\nVI-B\n5\nMapping Results\nIn Fig.\n14\n, we show the mapping results after ANYmal-D traverses terrains that are unseen during training. These maps capture fine-grained terrain details, such as gaps and supports, that help the controller achieve agile and generalized locomotion. Among prior works, only\n[\n32\n]\nhas demonstrated maps of comparable quality during agile motions. However, that approach cannot reliably infer unseen parts of obstacles in unstructured terrains\n[\n48\n]\n, is not efficient enough to support concurrent locomotion training, and does not explicitly encode uncertainty under partial observations such as occlusions. For generalized locomotion, explicitly modeling uncertainty is important: occluded regions are not completed by learned priors but kept uncertain, and newly observed geometry can be integrated into the map based on the predicted uncertainty once it becomes visible.\nFigure 14:\nObtained maps after ANYmal-D‚Äôs traversal of terrains that are unseen during training. Our mapping supports agile and generalized locomotion. In the visualizations, meshes represent the estimated elevations, and thin lines indicate the uncertainties. The meshes are colored with a red‚Äìblue colormap to enhance height contrast, excluding regions that are never covered by the cameras.\nVI-C\nEmergent Behaviors\nVI-C\n1\nActive Perception\nFigure 15:\nEmergent active perception behavior. (a) In a climb-up attempt, ANYmal-D collides with the obstacle due to limited field of view and depth sensing quality. (b) Although unsuccessful, this trial reveals higher parts of the obstacle in the map, enabling the robot to succeed in the immediate retrial. (c) ANYmal-D then climbs onto the obstacle. The maps are visualized in the top-left corner of each subfigure, excluding regions that are never covered by the cameras.\nTrained with partial observations and uncertainty-aware mapping, our locomotion controllers exhibit emergent active perception behaviors. In Fig.\n15\n, we exemplify this with ANYmal-D attempting to climb onto a high obstacle. In one attempt, the robot makes contact with the obstacle and cannot immediately climb up, due to its limited field of view and imperfect depth sensing. This interaction, however, exposes higher parts of the obstacle to the map. In the subsequent attempt, the controller uses this updated map and successfully climbs onto the obstacle. The maps are visualized to show how information gained from previous interactions is reused to make subsequent movements more informed and successful.\nNotably, this addresses a stated limitation of prior state-of-the-art work\n[\n48\n]\n, where a recurrent controller fails to climb onto a high box when starting close to it due to a lack of longer-term memory. In contrast, our mapping module explicitly maintains long-term spatial information, providing the controller with an effective representation of the environment over time.\nMoreover, consistent with\n[\n48\n]\n, we also observe the robot looking upward while climbing onto platforms, gaining additional terrain information.\nVI-C\n2\nLoco-Navigation\nFigure 16:\nEmergent local navigation capabilities. (a) TRON1 turns around and aligns itself with the beam to traverse the terrain. (b) ANYmal-D avoids an obstacle when reaching the goal.\nSince our locomotion controllers are trained to reach goals, they also exhibit local navigation capabilities. As shown in Fig.\n16\n, the controllers can align the robot with the terrain before traversal and avoid collisions with obstacles.\nThese behaviors can simplify remote human operation by reducing the need for high-frequency, navigation-level commands (such as velocity commands\n[\n13\n]\n).\nVI-C\n3\nWhole-Body Contact\nAs previously shown in Fig.\n10\nand Fig.\n13\n, our ANYmal-D controller actively use knee contacts to help stabilize the robot and traverse challenging terrains. On one hand, such whole-body contact behaviors are beneficial for agility and robustness; on the other hand, because most existing legged robots are designed primarily for foot contacts, these behaviors may stress the hardware. That said, we believe that whole-body contact will become increasingly important for achieving whole-body dexterity in complex environments\n[\n64\n,\n85\n,\n86\n]\n.\nVI-C\n4\nImpact Reduction\nAs mentioned in Sec.\nIV-D\n2\n, we use human and dog motion data to set thigh-acceleration thresholds for early termination. This induces impact-reduction behaviors, as shown in Fig.\n17\n: when climbing down, ANYmal-D uses its knees to gently touch down, while TRON1 retracts the support leg to absorb the impact.\nFigure 17:\nEmergent impact reduction behaviors. (a) ANYmal-D uses its knee to support the body when climbing down, and gently touches the ground to reduce the impact. (b) TRON1 extends one leg while jumping down, and retracts it during landing before switching support to the other leg.\nVI-D\nInterpretable Feature Patterns\nWe visualize the feature patterns of our ANYmal-D student policy‚Äôs AME-2 encoder in Fig.\n18\n. For local features, we visualize their attention weights with a red‚Äìblue colormap. For global features, which are obtained via max pooling, we treat pooled points as selected (weight 1) or not (weight 0) and average this binary mask over the feature dimension.\nBy inspecting these feature maps, we observe that local attention weights often emphasize fine-grained terrain details, while global features tend to concentrate on a sparse set of distinctive points across terrain types (e.g., obstacle boundaries, high platform surfaces, and beam centers).\nWe also find that these patterns persist on test terrains, which helps explain our controller‚Äôs generalization to unseen terrains despite being trained only on primitive terrains.\nFigure 18:\nFeature patterns of our ANYmal-D controller. We visualize the neural elevation map together with the corresponding local-attention and global-context weights. Points indicate elevation estimates, lines indicate uncertainties, and higher-intensity red colors correspond to higher weights. Local attention weights provide fine-grained patterns for locomotion control, while global context features primarily focus on sparse points that characterize the terrain type.\nVII\nAblations and Benchmarking\nIn this section, we benchmark our design choices on test terrains (depicted in Fig.\n4\n) that are unseen during training. All benchmarking experiments are conducted on ANYmal-D in simulation, where richer prior works and baselines are available.\nVII-A\nTeacher Architecture\nTABLE II:\nSuccess Rates (%) of Different Teacher Architectures on Training and Test Terrains\nTeacher Architecture\nTraining Terrains\nTest Terrains\nDense\nClimbing\nSparse\nTest 1\nTest 2\nTest 3\nTest 4\navg. Test\nAME-2¬†(Teacher)\n97.9\n96.0\n95.2\n96.8\n93.8\n99.7\n90.6\n95.2\nAME-1\n[\n15\n]\n96.0\n95.7\n95.5\n99.2\n29.2\n32.2\n44.1\n51.2\nMoE\n[\n68\n]\n95.0\n96.7\n90.5\n52.9\n55.3\n41.0\n30.8\n45.0\nBest results (within 5% of the maximum) are shown in\nbold\n.\nWe compare our teacher policy architecture against two baselines: AME-1\n[\n15\n]\nand MoE\n[\n68\n]\n. AME-1 is the prior state of the art in generalized locomotion, outperforming policies based on MLPs, CNNs, multimodal transformers\n[\n37\n]\n, and Vision Transformers\n[\n87\n]\n. The MoE architecture from\n[\n68\n]\nhas demonstrated strong scalability by enabling a humanoid to track diverse human motions.\nThe results are summarized in Table\nII\n. All architecture designs scale well on the training terrains, highlighting the stability of our training framework. However, MoE shows very limited generalization to unseen test terrains. AME-1 generalizes well on sparse terrains (test 1) but struggles when different terrains are mixed (test 2-4), which we attribute to its encoder assigning attention to local features based only on proprioception, thus missing global context. In contrast, our AME-2 teacher generalizes reliably across all test terrains.\nVII-B\nStudent Designs\nTABLE III:\nSuccess Rates (%) of Different Student Designs on Training and Test Terrains\nStudent Design\nTraining Terrains\nTest Terrains\nDense\nClimbing\nSparse\nTest 1\nTest 2\nTest 3\nTest 4\navg. Test\nAME-2 (Student)\n96.4\n96.4\n91.8\n90.0\n77.7\n89.1\n72.9\n82.4\nVisual Recurrent Student\n[\n48\n]\n98.0\n97.9\n86.4\n34.5\n20.9\n99.8\n50.6\n51.5\nAME-2 (Student) w/o RL\n95.8\n87.9\n84.6\n46.9\n59.1\n90.6\n46.0\n60.7\nAME-2 (Student) w/o rep. loss\n95.8\n94.8\n86.5\n81.9\n82.6\n69.5\n60.4\n73.6\nAME-2 (Teacher)\n97.9\n96.0\n95.2\n96.8\n93.8\n99.7\n90.6\n95.2\nBest student results (within 5% of the maximum) are shown in\nbold\n.\nHaving a generalized teacher does not automatically yield a generalized deployable student. We therefore compare our AME-2 student against the following baselines:\n1.\nThe end-to-end student design from\n[\n48\n]\n, the previous state of the art in agile generalist locomotion, which uses recurrent networks with vision inputs.\n2.\nAn AME-2 student trained without RL losses, using only teacher action distillation and the representation loss.\n3.\nAn AME-2 student trained without the representation loss.\nThe results are summarized in Table\nIII\n. On the training terrains, all student policies perform well, with our proposed design achieving slightly higher success rates on sparse terrains. On unseen test terrains, our student achieves the best overall success rates.\nThe end-to-end visual recurrent student from\n[\n48\n]\nperforms slightly better than ours on\nTest 3\n, a parkour course with obstacles, but underperforms on all other test terrains that contain sparse regions. On\nTest 3\n, most of our student‚Äôs failures occur when it attempts to climb the obstacles, which can be misinterpreted as higher platforms due to occlusions and the observed surface elevations. In contrast, using the full depth image, as in the visual recurrent design, may provide richer cues and a longer reaction horizon for obstacle avoidance. Despite this modest drop in generalization around obstacles, our student controller achieves much higher success rates on sparse and mixed terrains.\nRegarding the learning setup, we find that direct teacher supervision (action distillation plus representation alignment) already yields good performance on the training terrains, but does not produce strong generalization to unseen terrains without RL. Incorporating RL is important for performance on the test terrains, and the representation loss gives a further improvement. Overall, our student policy architecture performs better on unseen terrains than the end-to-end visual recurrent policy, and our learning design, with mixed RL losses and teacher supervision losses (action distillation and representation alignment), further enhances the generalization.\nVII-C\nMapping Designs\nTABLE IV:\nL\n0.5\nL_{0.5}\nLoss of Different Neural Mapping Methods on Training and Test Terrains\nMapping Pipeline\nTraining Terrains\nTest Terrains\nDense\nClimbing\nSparse\nTest 1\nTest 2\nTest 3\nTest 4\navg. Test\nOurs\n-0.006\n-0.108\n0.020\n0.033\n0.227\n0.036\n-0.111\n0.046\nOurs (Loco-Only)\n0.020\n-0.090\n0.081\n0.104\n0.234\n0.087\n-0.075\n0.088\nTemporal Recurrent\n-0.162\n-0.114\n-0.101\n0.316\n0.135\n0.066\n-0.176\n0.085\nBest (lowest) results are shown in\nbold\n.\nTABLE V:\nSuccess Rates (%) of the Student Policy under Noise and Sensor Degradation\nSetup\nTraining Terrains\nTest Terrains\nDense\nClimbing\nSparse\nTest 1\nTest 2\nTest 3\nTest 4\navg. Test\nNominal\n96.4\n96.4\n91.8\n90.0\n77.7\n89.1\n72.9\n82.4\n20\n%\n20\\%\nMissing Points\n95.5\n94.9\n91.3\n91.1\n77.3\n90.3\n74.8\n83.4\n3\n%\n3\\%\nArtifacts\n96.4\n97.6\n93.2\n95.2\n72.8\n92.5\n80.3\n85.2\n1\n1\nCamera Disabled\n91.2\n82.0\n85.9\n95.1\n56.0\n1.1\n74.6\n56.7\nFigure 19:\nQualitative comparison between the temporal recurrent model and our mapping pipeline on training and test terrains. The visualized maps are egocentric, with the robot facing rightward. Our mapping pipeline produces accurate estimations when confident and assigns high uncertainty to occluded regions. In contrast, the temporal recurrent model generates less meaningful uncertainty maps, loses fine-grained details, and performs worse on unseen terrains.\nWe benchmark our mapping pipeline against two baselines:\n1.\nOur pipeline, with the mapping model trained only on locomotion training terrains.\n2.\nA temporal recurrent model that directly predicts egocentric elevation maps from local grid observations and delta transforms.\nThe first baseline tests whether additional diverse meshes (both traversable and untraversable) are necessary for better generalization. The second contrasts our per-frame fusion approach with an end-to-end recurrent neural memory model: the recurrent model tends to overfit terrain patterns seen during training and produces lower-quality maps on unseen terrains, as we will show in the qualitative comparison in Fig.\n19\n.\nAll pipelines are evaluated on data collected from the same rollout trajectories, with results reported in Table\nIV\n. We use the\nL\n0.5\nL_{0.5}\nloss defined in Eq.\n9\nas the metric, which balances estimation accuracy and uncertainty quantification. The temporal recurrent model (detailed in Appendix\nD\n) uses a CNN encoder for local grids, an MLP encoder for delta transforms, and Spatially-Enhanced Recurrent Units (SRU), an architecture that enables 100¬†m-scale mapless visual navigation\n[\n88\n]\n, as the memory module. To stabilize training and prevent the model from collapsing to large uncertainties and poor estimates, we have to train the recurrent model only on locomotion training terrains with the policy in the loop (as done in prior work\n[\n33\n,\n29\n]\n) and add an auxiliary L2 loss on the elevation estimates in addition to\nL\n0.5\nL_{0.5}\n. This setup requires roughly\n5\n√ó\n5\\times\nmore training time to converge than our mapping models.\nIt is worth noting that the recurrent model can optimize both the elevation estimates and the associated uncertainties in unseen regions based on the training distribution, which is advantageous under the\nL\n0.5\nL_{0.5}\nloss, whereas our pipeline uses heuristic defaults. However, as illustrated in Fig.\n19\n, our mapping pipeline produces higher-quality maps in practice, and can effectively model occlusions with high uncertainties.\nVII-D\nRobustness to Visual Noise\nWe validate the robustness of our system to noise and sensor degradation. We evaluate our student policy‚Äôs success rates under missing points (removed from depth clouds), artifacts (replaced by random points in depth clouds), and with the front upper camera disabled. The tested setups are all above the randomization ranges during training.\nThe results are presented in Table\nV\n. Under missing points and artifacts, we find the policy performance does not drop, and can sometimes be slightly better. This can be explained by that, fewer effective points in the depth clouds result in more uncertain regions and make the policy behave more conservatively.\nWhen the front upper camera is disabled, the system still performs well on most terrains without re-training, highlighting the flexibility of our method for different sensor configurations. However, on terrains that require active perception, such as the high obstacles to climb in\nTest 2\nand\nTest 3\n, the policy struggles, suggesting that these behaviors remain sensitive to the available sensor viewpoints.\nVIII\nDiscussion\nIn this paper, we propose AME-2, a unified RL framework for agile and generalizable legged locomotion with attention-based neural map encoding. We use goal-reaching rewards to incentivize agile behaviors, and design the AME-2 encoder to achieve generalization across diverse challenging terrains. To facilitate sim-to-real deployment, we combine teacher-student learning with RL and introduce a lightweight neural mapping pipeline that explicitly models partial observability while running identically in simulation and on real robots. Our framework applies to both a quadruped and a biped without changing the rewards or training settings.\nVIII-A\nModular v.s. End-to-End\nClassical locomotion systems\n[\n2\n,\n4\n]\nuse modular designs (perception, planning, and control) to make the problem tractable, and can generalize well when their underlying assumptions hold (e.g., low uncertainty, stable foot contacts, accurate mapping and dynamics modeling). Recent works instead reduce system complexity through end-to-end sensor-to-motor policy learning\n[\n48\n,\n38\n]\n, but have not yet demonstrated strong generalization to unseen terrains.\nOur work tries to bridge these two. We use a mapping module and a controller module, and train the neural controller end-to-end to generate joint actions from observations. In the controller, our AME-2 encoder also learns planning-level representations (global context and local contact-relevant features) for the subsequent decoder, playing a similar role to the planning module in classical solutions. This design preserves a modular structure at the system level while avoiding explicit online model-based planning. In the mapping module, we keep a simple heuristic structure, but also run a lightweight neural network in the loop to improve robustness and generalization under noise, partial observability, and unseen terrain patterns.\nVIII-B\nGoal Reaching v.s. Velocity Tracking\nWe use a goal-reaching formulation\n[\n20\n]\ninstead of velocity tracking\n[\n15\n]\n. This encourages more agile behaviors and reduces reliance on high-frequency navigation commands. A drawback is that it offers less direct control over the robot‚Äôs intermediate motion, which can be troublesome when semantic obstacles lie along the path but appear traversable from the geometry. We envision combining both command interfaces within a single controller in future work, e.g., via masked commands and distillation\n[\n89\n]\n.\nVIII-C\nWhole-Body Skills\nWe achieve whole-body locomotion skills in this work, but for a given terrain type the learned motions tend to follow similar contact patterns. For systems with higher DoFs, such as humanoids, the same terrain type at different scales may require different contact strategies. For example, a humanoid may step onto low obstacles with a single leg, use a two-leg jump for medium heights, and leverage both arms and legs for higher obstacles. It remains unclear whether our method, without additional references or priors such as\n[\n86\n]\n, can automatically discover such diverse contact patterns to handle a broader range of terrains in higher-DoF systems.\nVIII-D\nTowards Higher Success Rates on Unseen Terrains\nWe still observe more failures on unseen test terrains than on training terrains, as expected for learning-based systems. Notably, most failures occur during skill transitions. For example, on\nTest 2\nthe robot must first decelerate on a sparse region and then execute a climbing maneuver while maintaining precise hind-leg footholds. While these specific cases could likely be addressed by finetuning\n[\n48\n]\n, it remains unclear whether there is a scalable, principled approach for learning such challenging skill transitions with zero-shot generalization.\nVIII-E\nLimitations and Future Work\nThis work has several limitations. First, we use a 2.5-D elevation map and do not address fully 3-D locomotion. Future work could incorporate multi-layer elevation maps\n[\n24\n]\n, which are directly compatible with our policy architecture, or explore attention-based voxel representations. Second, our controllers are not designed for severely degraded perception, such as high grass or snow. A promising direction is to combine robust controllers\n[\n14\n]\nwith our agile and generalized controllers into a single scene-aware policy. Third, our mapping module can fail in highly dynamic environments with occlusions, and could be extended to explicitly reason about moving elements in the scene.\nAcknowledgments\nThis work is funded by ETH AI Center, NCCR automation, Apple Inc. Any views, opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and should not be interpreted as reflecting the views, policies or position, either expressed or implied, of Apple Inc.\nWe thank the help and insights from Yuni Fuchioka, Elena Krasnova, William Talbot, Junzhe He, Matthias Heyrman, Clemens Schwarke, Jonas Frey, Manthan Patel, Emilio Palma, Vladlen Koltun, Matthias M√ºller, Siyuan Hu. We thank the support from Martin Vechev. We thank the technical support from LimX Dynamics and ANYbotics.\nAppendix A\nTerrain Parameters\nWe use the following terrain proportions and parameters for training:\n‚Ä¢\nRough (Dense, 5%)\n: Heightfields generated from uniform noise, with terrain resolution smaller than the map resolution. Over the curriculum, the noise range increases from\n¬±\n0\n\\pm 0\nm to\n¬±\n0.2\n\\pm 0.2\nm for ANYmal-D, and from\n¬±\n0\n\\pm 0\nm to\n¬±\n0.15\n\\pm 0.15\nm for TRON1. Goals are sampled anywhere on the terrain.\n‚Ä¢\nStair Down (Dense, 5%)\n: Floating stairs descending with random widths and random walls on both sides. The slope is increased from\n5\n‚àò\n5^{\\circ}\nto\n45\n‚àò\n45^{\\circ}\nover the curriculum for both robots. Goals are sampled anywhere on the terrain.\n‚Ä¢\nStair Up (Dense, 5%)\n: Floating stairs ascending with random widths and random walls on both sides. The slope is increased from\n5\n‚àò\n5^{\\circ}\nto\n45\n‚àò\n45^{\\circ}\nover the curriculum for both robots. Goals are sampled anywhere on the terrain.\n‚Ä¢\nBoxes (Dense, 5%)\n: Heightfields generated by adding and removing random boxes on flat ground. Over the curriculum, the maximum box height increases from\n0.05\n0.05\nm to\n0.4\n0.4\nm for ANYmal-D, and from\n0.05\n0.05\nm to\n0.3\n0.3\nm for TRON1. Goals are sampled anywhere on the terrain.\n‚Ä¢\nObstacles (Dense, 5%)\n: Random positive and negative obstacles placed on random slopes. Over the curriculum, the obstacle density increases from\n0\n‚Äã\nm\n‚àí\n2\n0~\\mathrm{m}^{-2}\nto\n0.5\n‚Äã\nm\n‚àí\n2\n0.5~\\mathrm{m}^{-2}\nfor both robots. Goals are sampled anywhere on the terrain.\n‚Ä¢\nClimbing Up (Climbing, 20%)\n: A pit to climb out of, with vertices near the edges randomly perturbed by up to 20% of the height. Over the curriculum, the pit height increases from\n0.1\n0.1\nm to\n1.0\n1.0\nm for ANYmal-D, and from\n0.1\n0.1\nm to\n0.48\n0.48\nm for TRON1. Goals are sampled at the other end of the terrain.\n‚Ä¢\nClimbing Down (Climbing, 5%)\n: A platform to climb down from, with vertices near the edges randomly perturbed by up to 20% of the height. Over the curriculum, the platform height increases from\n0.2\n0.2\nm to\n1.0\n1.0\nm for ANYmal-D, and from\n0.2\n0.2\nm to\n0.88\n0.88\nm for TRON1. Goals are sampled at the other end of the terrain.\n‚Ä¢\nClimbing Consecutive (Climbing, 5%)\n: Two stacked layers of rings to climb out of. The robot needs to consecutively climb up twice and then climb down twice. Over the curriculum, the first ring height increases from\n0.05\n0.05\nm to\n0.5\n0.5\nm for ANYmal-D and from\n0.05\n0.05\nm to\n0.3\n0.3\nm for TRON1. The second ring height increases from\n0.05\n0.05\nm to\n0.4\n0.4\nm for ANYmal-D and from\n0.05\n0.05\nm to\n0.3\n0.3\nm for TRON1. Goals are sampled at the other end of the terrain.\n‚Ä¢\nGap (Sparse, 5%)\n: A gap to jump over, with vertices near the edges randomly perturbed by up to\n10\n%\n10\\%\nof the distance, and a height difference between the two sides randomly sampled within\n¬±\n30\n%\n\\pm 30\\%\nof the distance. Over the curriculum, the distance increases from\n0.1\n0.1\nm to\n1.1\n1.1\nm for ANYmal-D and from\n0.1\n0.1\nm to\n0.6\n0.6\nm for TRON1. Goals are sampled at the other end of the terrain.\n‚Ä¢\nPallets (Sparse, 5%)\n: Parallel beams with random orientations. Over the curriculum, the beam width decreases from\n0.4\n0.4\nm to\n0.16\n0.16\nm for both robots. The gap width increases from\n0.08\n0.08\nm to\n0.35\n0.35\nm for ANYmal-D and from\n0.08\n0.08\nm to\n0.2\n0.2\nm for TRON1. The inter-beam height difference increases from\n0\nm to\n0.3\n0.3\nm for ANYmal-D and from\n0\nm to\n0.2\n0.2\nm for TRON1. Goals are sampled at the other end of the terrain.\n‚Ä¢\nStones (Sparse, 30%)\n: We follow the ‚ÄúStones-Everywhere‚Äù design in\n[\n26\n]\nand\n[\n15\n]\n. Goals are sampled anywhere on the terrain.\n‚Ä¢\nBeam (Sparse, 5%)\n: A beam connecting two platforms, randomly inclined with roll, pitch, and yaw sampled within\n¬±\n0.1\n‚Äã\nrad\n\\pm 0.1~\\mathrm{rad}\n, and with a platform height difference sampled within\n¬±\n0.2\n\\pm 0.2\nm. Over the curriculum, the beam width decreases from\n0.9\n0.9\nm to\n0.18\n0.18\nm for both robots. Goals are sampled at the other end of the terrain.\nFor sparse terrains, we add physical floors to half of them and virtual floors (visible in the map without physical collision) to the other half. The floor height is randomized between\n‚àí\n1.5\n-1.5\nm and\n‚àí\n0.35\n-0.35\nm. This discourages the robot from walking on the floor when traversing shallow sparse terrains.\nAppendix B\nRandomization Parameters\nFor both robots, we randomize the payload within\n[\n‚àí\n5\n,\n5\n]\n‚Äã\nkg\n[-5,5]~\\mathrm{kg}\n, actuation delays within\n[\n0\n,\n0.02\n]\n‚Äã\ns\n[0,0.02]~\\mathrm{s}\n, and friction coefficients within\n[\n0.3\n,\n1.0\n]\n[0.3,1.0]\n. For TRON1, we additionally apply random biases to PD gains and armatures within\n¬±\n15\n%\n\\pm 15\\%\nof the raw values.\nFor policy observations, we add zero-mean uniform noise with the following maximum magnitudes:\n0.1\n0.1\nm/s for base linear velocity,\n0.2\n0.2\nrad/s for base angular velocity,\n0.05\n0.05\nfor projected gravity,\n0.01\n0.01\nrad for joint positions,\n1.5\n1.5\nrad/s for joint velocities, and\n0.05\n0.05\nm for map observations. We also add a random drift within\n[\n‚àí\n0.03\n,\n0.03\n]\n‚Äã\nm\n[-0.03,0.03]~\\mathrm{m}\nwhen querying map observations.\nDuring student policy training, we make\n15\n%\n15\\%\nof depth cloud points missing and\n2\n%\n2\\%\nartifacts. We give\n10\n%\n10\\%\nof the environments access to complete maps with variance\n0.0025\n‚Äã\nm\n2\n0.0025~\\mathrm{m}^{2}\n. We corrupt\n1\n%\n1\\%\npoints in the student map observations by assigning random values with a random variance larger than\n1\n‚Äã\nm\n2\n1~\\mathrm{m}^{2}\n.\nAppendix C\nPPO Parameters\nWe use the following PPO parameters in Table\nVI\n.\nTABLE VI:\nPPO Parameters\nParameter\nValue\nValue Loss Coefficient\n1.0\nValue Loss Clip\n0.2\nEntropy Coefficient\nDecay from 0.004 to 0.001\nLearning Epochs per Iteration\n4\nMini Batches per Iteration\n3\nSimulation Steps per Iteration\n24\nNumber of Environments\n4800\nLearning Rate\nAdaptive\nŒ≥\n\\gamma\n0.99\nŒª\n\\lambda\n0.95\nDesired KL Divergence\n0.01\nStudent only\nLearning Rate When Surrogate Loss Disabled\n0.001\nAction Distillation Loss Coefficient\n0.02\nRepresentation Loss Coefficient\n0.2\nAppendix D\nTemporal Recurrent Baseline Mapping Model\nThe architecture of the recurrent model used as the mapping baseline is shown in Fig.\n20\n.\nWe provide the model with ground-truth delta transforms, which are sufficient for map reconstruction and serve as a clean replacement for the proprioceptive observations used in prior work\n[\n29\n,\n33\n]\n, where transforms are not directly available.\nFigure 20:\nThe temporal recurrent model architecture to predict egocentric elevation mapping.\nReferences\n[1]\nP.¬†Fankhauser, M.¬†Bjelonic, C.¬†D. Bellicoso, T.¬†Miki, and M.¬†Hutter, ‚ÄúRobust rough-terrain locomotion with a quadrupedal robot,‚Äù in\n2018 IEEE International Conference on Robotics and Automation (ICRA)\n.‚ÄÉIEEE, 2018, pp. 5761‚Äì5768.\n[2]\nF.¬†Jenelten, R.¬†Grandia, F.¬†Farshidian, and M.¬†Hutter, ‚ÄúTamols: Terrain-aware motion optimization for legged systems,‚Äù\nIEEE Transactions on Robotics\n, vol.¬†38, no.¬†6, pp. 3395‚Äì3413, 2022.\n[3]\nS.¬†Fahmi, V.¬†Barasuol, D.¬†Esteban, O.¬†Villarreal, and C.¬†Semini, ‚ÄúVital: Vision-based terrain-aware locomotion for legged robots,‚Äù\nIEEE Transactions on Robotics\n, vol.¬†39, no.¬†2, pp. 885‚Äì904, 2022.\n[4]\nR.¬†Grandia, F.¬†Jenelten, S.¬†Yang, F.¬†Farshidian, and M.¬†Hutter, ‚ÄúPerceptive locomotion through nonlinear model-predictive control,‚Äù\nIEEE Transactions on Robotics\n, vol.¬†39, no.¬†5, pp. 3402‚Äì3421, 2023.\n[5]\nR.¬†Akizhanov, V.¬†Dh√©din, M.¬†Khadiv, and I.¬†Laptev, ‚ÄúLearning feasible transitions for efficient contact planning,‚Äù\narXiv preprint arXiv:2407.11788\n, 2024.\n[6]\nR.¬†J. Griffin, G.¬†Wiedebach, S.¬†McCrory, S.¬†Bertrand, I.¬†Lee, and J.¬†Pratt, ‚ÄúFootstep planning for autonomous walking over rough terrain,‚Äù in\n2019 IEEE-RAS 19th International Conference on Humanoid Robots (Humanoids)\n, 2019, pp. 9‚Äì16.\n[7]\nC.¬†Mastalli, W.¬†Merkt, G.¬†Xin, J.¬†Shim, M.¬†Mistry, I.¬†Havoutis, and S.¬†Vijayakumar, ‚ÄúAgile maneuvers in legged robots: a predictive control approach,‚Äù\narXiv preprint arXiv:2203.07554\n, 2022.\n[8]\nF.¬†Jenelten, T.¬†Miki, A.¬†E. Vijayan, M.¬†Bjelonic, and M.¬†Hutter, ‚ÄúPerceptive locomotion in rough terrain ‚Äì online foothold optimization,‚Äù\nIEEE Robotics and Automation Letters\n, vol.¬†5, no.¬†4, pp. 5370‚Äì5376, 2020.\n[9]\nF.¬†Jenelten, J.¬†Hwangbo, F.¬†Tresoldi, C.¬†D. Bellicoso, and M.¬†Hutter, ‚ÄúDynamic locomotion on slippery ground,‚Äù\nIEEE Robotics and Automation Letters\n, vol.¬†4, no.¬†4, pp. 4170‚Äì4176, 2019.\n[10]\nP.¬†Fankhauser, M.¬†Bloesch, and M.¬†Hutter, ‚ÄúProbabilistic terrain mapping for mobile robots with uncertain localization,‚Äù\nIEEE Robotics and Automation Letters\n, vol.¬†3, no.¬†4, pp. 3019‚Äì3026, 2018.\n[11]\nT.¬†Miki, L.¬†Wellhausen, R.¬†Grandia, F.¬†Jenelten, T.¬†Homberger, and M.¬†Hutter, ‚ÄúElevation mapping for locomotion and navigation using gpu,‚Äù in\n2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, 2022, pp. 2273‚Äì2280.\n[12]\nS.¬†Ha, J.¬†Lee, M.¬†van¬†de Panne, Z.¬†Xie, W.¬†Yu, and M.¬†Khadiv, ‚ÄúLearning-based legged locomotion: State of the art and future perspectives,‚Äù\nThe International Journal of Robotics Research\n, vol.¬†44, no.¬†8, pp. 1396‚Äì1427, 2025.\n[13]\nN.¬†Rudin, D.¬†Hoeller, P.¬†Reist, and M.¬†Hutter, ‚ÄúLearning to walk in minutes using massively parallel deep reinforcement learning,‚Äù in\nConference on robot learning\n.‚ÄÉPMLR, 2022, pp. 91‚Äì100.\n[14]\nT.¬†Miki, J.¬†Lee, J.¬†Hwangbo, L.¬†Wellhausen, V.¬†Koltun, and M.¬†Hutter, ‚ÄúLearning robust perceptive locomotion for quadrupedal robots in the wild,‚Äù\nScience robotics\n, vol.¬†7, no.¬†62, p. eabk2822, 2022.\n[15]\nJ.¬†He, C.¬†Zhang, F.¬†Jenelten, R.¬†Grandia, M.¬†B√§cher, and M.¬†Hutter, ‚ÄúAttention-based map encoding for learning generalized legged locomotion,‚Äù\nScience Robotics\n, vol.¬†10, no. 105, p. eadv3604, 2025.\n[16]\nA.¬†Allshire, H.¬†Choi, J.¬†Zhang, D.¬†McAllister, A.¬†Zhang, C.¬†M. Kim, T.¬†Darrell, P.¬†Abbeel, J.¬†Malik, and A.¬†Kanazawa, ‚ÄúVisual imitation enables contextual humanoid control,‚Äù\narXiv preprint arXiv:2505.03729\n, 2025.\n[17]\nJ.¬†Long, J.¬†Ren, M.¬†Shi, Z.¬†Wang, T.¬†Huang, P.¬†Luo, and J.¬†Pang, ‚ÄúLearning humanoid locomotion with perceptive internal model,‚Äù in\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n.‚ÄÉIEEE, 2025, pp. 9997‚Äì10‚Äâ003.\n[18]\nJ.¬†Ren, T.¬†Huang, H.¬†Wang, Z.¬†Wang, Q.¬†Ben, J.¬†Long, Y.¬†Yang, J.¬†Pang, and P.¬†Luo, ‚ÄúVb-com: Learning vision-blind composite humanoid locomotion against deficient perception,‚Äù\narXiv preprint arXiv:2502.14814\n, 2025.\n[19]\nH.¬†Wang, Z.¬†Wang, J.¬†Ren, Q.¬†Ben, T.¬†Huang, W.¬†Zhang, and J.¬†Pang, ‚ÄúBeamdojo: Learning agile humanoid locomotion on sparse footholds,‚Äù in\nRobotics: Science and Systems (RSS)\n, 2025.\n[20]\nN.¬†Rudin, D.¬†Hoeller, M.¬†Bjelonic, and M.¬†Hutter, ‚ÄúAdvanced skills by learning locomotion and local navigation end-to-end,‚Äù in\n2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, 2022, pp. 2497‚Äì2503.\n[21]\nF.¬†Jenelten, J.¬†He, F.¬†Farshidian, and M.¬†Hutter, ‚ÄúDtc: Deep tracking control,‚Äù\nScience Robotics\n, vol.¬†9, no.¬†86, p. eadh5401, 2024.\n[22]\nS.¬†Gangapurwala, M.¬†Geisert, R.¬†Orsolino, M.¬†Fallon, and I.¬†Havoutis, ‚ÄúRloc: Terrain-aware legged locomotion using reinforcement learning and optimal control,‚Äù\nIEEE Transactions on Robotics\n, vol.¬†38, no.¬†5, pp. 2908‚Äì2927, 2022.\n[23]\nY.¬†Dong, J.¬†Ma, L.¬†Zhao, W.¬†Li, and P.¬†Lu, ‚ÄúMarg: Mastering risky gap terrains for legged robots with elevation mapping,‚Äù\nIEEE Transactions on Robotics\n, 2025.\n[24]\nY.¬†Chen, J.¬†Ma, Z.¬†Luo, Y.¬†Han, Y.¬†Dong, B.¬†Xu, and P.¬†Lu, ‚ÄúLearning autonomous and safe quadruped traversal of complex terrains using multi-layer elevation maps,‚Äù\nIEEE Robotics and Automation Letters\n, 2025.\n[25]\nT.¬†Miki, J.¬†Lee, L.¬†Wellhausen, and M.¬†Hutter, ‚ÄúLearning to walk in confined spaces using 3d representation,‚Äù in\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, 2024, pp. 8649‚Äì8656.\n[26]\nC.¬†Zhang, N.¬†Rudin, D.¬†Hoeller, and M.¬†Hutter, ‚ÄúLearning agile locomotion on risky terrains,‚Äù in\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n.‚ÄÉIEEE, 2024, pp. 11‚Äâ864‚Äì11‚Äâ871.\n[27]\nF.¬†Shi, C.¬†Zhang, T.¬†Miki, J.¬†Lee, M.¬†Hutter, and S.¬†Coros, ‚ÄúRethinking robustness assessment: Adversarial attacks on learning-based quadrupedal locomotion controllers,‚Äù in\nRobotics: Science and Systems XX, Delft, The Netherlands, July 15-19, 2024\n, D.¬†Kulic, G.¬†Venture, K.¬†E. Bekris, and E.¬†Coronado, Eds., 2024.\n[28]\nJ.¬†Lee, M.¬†Bjelonic, A.¬†Reske, L.¬†Wellhausen, T.¬†Miki, and M.¬†Hutter, ‚ÄúLearning robust autonomous navigation and locomotion for wheeled-legged robots,‚Äù\nScience Robotics\n, vol.¬†9, no.¬†89, p. eadi9641, 2024.\n[29]\nJ.¬†Sun, G.¬†Han, P.¬†Sun, W.¬†Zhao, J.¬†Cao, J.¬†Wang, Y.¬†Guo, and Q.¬†Zhang, ‚ÄúDpl: Depth-only perceptive humanoid locomotion via realistic depth synthesis and cross-attention terrain reconstruction,‚Äù\narXiv preprint arXiv:2510.07152\n, 2025.\n[30]\nY.¬†Yang, G.¬†Shi, C.¬†Lin, X.¬†Meng, R.¬†Scalise, M.¬†G. Castro, W.¬†Yu, T.¬†Zhang, D.¬†Zhao, J.¬†Tan\net¬†al.\n, ‚ÄúAgile continuous jumping in discontinuous terrains,‚Äù in\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n.‚ÄÉIEEE, 2025, pp. 10‚Äâ245‚Äì10‚Äâ252.\n[31]\nD.¬†Hoeller, N.¬†Rudin, C.¬†Choy, A.¬†Anandkumar, and M.¬†Hutter, ‚ÄúNeural scene representation for locomotion on structured terrain,‚Äù\nIEEE Robotics and Automation Letters\n, vol.¬†7, no.¬†4, pp. 8667‚Äì8674, 2022.\n[32]\nD.¬†Hoeller, N.¬†Rudin, D.¬†Sako, and M.¬†Hutter, ‚ÄúAnymal parkour: Learning agile navigation for quadrupedal robots,‚Äù\nScience Robotics\n, vol.¬†9, no.¬†88, p. eadi7566, 2024.\n[33]\nR.¬†Yu, Q.¬†Wang, Y.¬†Wang, Z.¬†Wang, J.¬†Wu, and Q.¬†Zhu, ‚ÄúWalking with terrain reconstruction: Learning to traverse risky sparse footholds,‚Äù\narXiv preprint arXiv:2409.15692\n, 2024.\n[34]\nH.¬†Duan, B.¬†Pandit, M.¬†S. Gadde, B.¬†Van¬†Marum, J.¬†Dao, C.¬†Kim, and A.¬†Fern, ‚ÄúLearning vision-based bipedal locomotion for challenging terrain,‚Äù in\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n.‚ÄÉIEEE, 2024, pp. 56‚Äì62.\n[35]\nA.¬†Agarwal, A.¬†Kumar, J.¬†Malik, and D.¬†Pathak, ‚ÄúLegged locomotion in challenging terrains using egocentric vision,‚Äù in\nProceedings of The 6th Conference on Robot Learning\n.‚ÄÉPMLR, 2023.\n[36]\nX.¬†Cheng, K.¬†Shi, A.¬†Agarwal, and D.¬†Pathak, ‚ÄúExtreme parkour with legged robots,‚Äù in\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n.‚ÄÉIEEE, 2024, pp. 11‚Äâ443‚Äì11‚Äâ450.\n[37]\nR.¬†Yang, M.¬†Zhang, N.¬†Hansen, H.¬†Xu, and X.¬†Wang, ‚ÄúLearning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers,‚Äù in\nInternational Conference on Learning Representations\n, 2022.\n[38]\nZ.¬†Zhuang, S.¬†Yao, and H.¬†Zhao, ‚ÄúHumanoid parkour learning,‚Äù in\nConference on Robot Learning\n.‚ÄÉPMLR, 2025, pp. 1975‚Äì1991.\n[39]\nS.¬†Li, S.¬†Luo, J.¬†Wu, and Q.¬†Zhu, ‚ÄúMove: Multi-skill omnidirectional legged locomotion with limited view in 3d environments,‚Äù in\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n, 2025, pp. 7647‚Äì7653.\n[40]\nQ.¬†Ben, B.¬†Xu, K.¬†Li, F.¬†Jia, W.¬†Zhang, J.¬†Wang, J.¬†Wang, D.¬†Lin, and J.¬†Pang, ‚ÄúGallant: Voxel grid-based humanoid locomotion and local-navigation across 3d constrained terrains,‚Äù\narXiv preprint arXiv:2511.14625\n, 2025.\n[41]\nS.¬†Luo, S.¬†Li, R.¬†Yu, Z.¬†Wang, J.¬†Wu, and Q.¬†Zhu, ‚ÄúPie: Parkour with implicit-explicit learning framework for legged robots,‚Äù\nIEEE Robotics and Automation Letters\n, vol.¬†9, no.¬†11, pp. 9986‚Äì9993, 2024.\n[42]\nE.¬†Chane-Sane, J.¬†Amigo, T.¬†Flayols, L.¬†Righetti, and N.¬†Mansard, ‚ÄúSoloparkour: Constrained reinforcement learning for visual locomotion from privileged experience,‚Äù in\nProceedings of The 8th Conference on Robot Learning\n, ser. Proceedings of Machine Learning Research, vol. 270.‚ÄÉPMLR, 2025, pp. 4268‚Äì4285.\n[43]\nS.¬†Kareer, N.¬†Yokoyama, D.¬†Batra, S.¬†Ha, and J.¬†Truong, ‚ÄúVinl: Visual navigation and locomotion over obstacles,‚Äù in\n2023 IEEE International Conference on Robotics and Automation (ICRA)\n.‚ÄÉIEEE, 2023, pp. 2018‚Äì2024.\n[44]\nH.¬†Lai, J.¬†Cao, J.¬†Xu, H.¬†Wu, Y.¬†Lin, T.¬†Kong, Y.¬†Yu, and W.¬†Zhang, ‚ÄúWorld model-based perception for visual legged locomotion,‚Äù in\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n, 2025, pp. 11‚Äâ531‚Äì11‚Äâ537.\n[45]\nP.¬†Li, H.¬†Li, Y.¬†Ma, L.¬†Chang, X.¬†Yang, R.¬†Yu, Y.¬†Zhang, Y.¬†Cao, Q.¬†Zhu, and G.¬†Sartoretti, ‚ÄúKivi: Kinesthetic-visuospatial integration for dynamic and safe egocentric legged locomotion,‚Äù\narXiv preprint arXiv:2509.23650\n, 2025.\n[46]\nH.¬†Su, H.¬†Luo, S.¬†Yang, K.¬†Jiang, W.¬†Zhang, and H.¬†Chen, ‚ÄúLipm-guided reinforcement learning for stable and perceptive locomotion in bipedal robots,‚Äù in\n2025 IEEE-RAS 24th International Conference on Humanoid Robots (Humanoids)\n, 2025, pp. 1031‚Äì1038.\n[47]\nD.¬†Wang, X.¬†Wang, X.¬†Liu, J.¬†Shi, Y.¬†Zhao, C.¬†Bai, and X.¬†Li, ‚ÄúMore: Mixture of residual experts for humanoid lifelike gaits learning on complex terrains,‚Äù\narXiv preprint arXiv:2506.08840\n, 2025.\n[48]\nN.¬†Rudin, J.¬†He, J.¬†Aurand, and M.¬†Hutter, ‚ÄúParkour in the wild: Learning a general and extensible agile locomotion policy using multi-expert distillation and rl fine-tuning,‚Äù\narXiv preprint arXiv:2505.11164\n, 2025.\n[49]\nT.¬†He, C.¬†Zhang, W.¬†Xiao, G.¬†He, C.¬†Liu, and G.¬†Shi, ‚ÄúAgile but safe: Learning collision-free high-speed legged locomotion,‚Äù in\nProceedings of Robotics: Science and Systems\n, Delft, Netherlands, July 2024.\n[50]\nC.¬†Zhang, J.¬†Jin, J.¬†Frey, N.¬†Rudin, M.¬†Mattamala, C.¬†Cadena, and M.¬†Hutter, ‚ÄúResilient legged local navigation: Learning to traverse with compromised perception end-to-end,‚Äù in\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, 2024, pp. 34‚Äì41.\n[51]\nA.¬†Kendall and Y.¬†Gal, ‚ÄúWhat uncertainties do we need in bayesian deep learning for computer vision?‚Äù\nAdvances in neural information processing systems\n, vol.¬†30, 2017.\n[52]\nS.¬†Jung, D.¬†Gwak, B.¬†Boots, and J.¬†Hays, ‚ÄúUncertainty-aware accurate elevation modeling for off-road navigation via neural processes,‚Äù in\nConference on Robot Learning\n.‚ÄÉPMLR, 2025, pp. 2802‚Äì2822.\n[53]\nH.¬†Wang, Z.¬†Shi, C.¬†Zhu, Y.¬†Qiao, C.¬†Zhang, F.¬†Yang, P.¬†Ren, L.¬†Lu, and D.¬†Xuan, ‚ÄúIntegrating learning-based manipulation and physics-based locomotion for whole-body badminton robot control,‚Äù\narXiv preprint arXiv:2504.17771\n, 2025.\n[54]\nM.¬†Hutter, C.¬†Gehring, D.¬†Jud, A.¬†Lauber, C.¬†D. Bellicoso, V.¬†Tsounis, J.¬†Hwangbo, K.¬†Bodie, P.¬†Fankhauser, M.¬†Bloesch\net¬†al.\n, ‚ÄúAnymal-a highly mobile and dynamic quadrupedal robot,‚Äù in\n2016 IEEE/RSJ international conference on intelligent robots and systems (IROS)\n.‚ÄÉIEEE, 2016, pp. 38‚Äì44.\n[55]\nLimX Dynamics, ‚ÄúTRON 1: The first multi-modal biped robot,‚Äù\nhttps://www.limxdynamics.com/en/tron1\n, 2025, accessed: 2025-11-23.\n[56]\nH.¬†J. Lee, S.¬†Hong, and S.¬†Kim, ‚ÄúIntegrating model-based footstep planning with model-free reinforcement learning for dynamic legged locomotion,‚Äù in\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n.‚ÄÉIEEE, 2024, pp. 11‚Äâ248‚Äì11‚Äâ255.\n[57]\nW.¬†Yu, D.¬†Jain, A.¬†Escontrela, A.¬†Iscen, P.¬†Xu, E.¬†Coumans, S.¬†Ha, J.¬†Tan, and T.¬†Zhang, ‚ÄúVisual-locomotion: Learning to walk on complex terrains with vision,‚Äù in\n5th Annual Conference on Robot Learning\n, 2021.\n[58]\nR.¬†Yang, G.¬†Yang, and X.¬†Wang, ‚ÄúNeural volumetric memory for visual locomotion control,‚Äù in\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, 2023, pp. 1430‚Äì1440.\n[59]\nH.¬†Kim, A.¬†Mnih, J.¬†Schwarz, M.¬†Garnelo, A.¬†Eslami, D.¬†Rosenbaum, O.¬†Vinyals, and Y.¬†W. Teh, ‚ÄúAttentive neural processes,‚Äù\narXiv preprint arXiv:1901.05761\n, 2019.\n[60]\nJ.¬†Schulman, F.¬†Wolski, P.¬†Dhariwal, A.¬†Radford, and O.¬†Klimov, ‚ÄúProximal policy optimization algorithms,‚Äù\narXiv preprint arXiv:1707.06347\n, 2017.\n[61]\nV.¬†Makoviychuk, L.¬†Wawrzyniak, Y.¬†Guo, M.¬†Lu, K.¬†Storey, M.¬†Macklin, D.¬†Hoeller, N.¬†Rudin, A.¬†Allshire, A.¬†Handa\net¬†al.\n, ‚ÄúIsaac gym: High performance gpu-based physics simulation for robot learning,‚Äù\narXiv preprint arXiv:2108.10470\n, 2021.\n[62]\nX.¬†B. Peng and M.¬†van¬†de Panne, ‚ÄúLearning locomotion skills using deeprl: does the choice of action space matter?‚Äù in\nProceedings of the ACM SIGGRAPH / Eurographics Symposium on Computer Animation\n, ser. SCA ‚Äô17.‚ÄÉNew York, NY, USA: Association for Computing Machinery, 2017.\n[63]\nA.¬†Vaswani, N.¬†Shazeer, N.¬†Parmar, J.¬†Uszkoreit, L.¬†Jones, A.¬†N. Gomez, ≈Å.¬†Kaiser, and I.¬†Polosukhin, ‚ÄúAttention is all you need,‚Äù\nAdvances in neural information processing systems\n, vol.¬†30, 2017.\n[64]\nC.¬†Zhang, W.¬†Xiao, T.¬†He, and G.¬†Shi, ‚ÄúWococo: Learning whole-body humanoid control with sequential contacts,‚Äù in\nProceedings of The 8th Conference on Robot Learning\n, ser. Proceedings of Machine Learning Research, vol. 270.‚ÄÉPMLR, 2025, pp. 455‚Äì472.\n[65]\nT.¬†He, Z.¬†Luo, X.¬†He, W.¬†Xiao, C.¬†Zhang, W.¬†Zhang, K.¬†M. Kitani, C.¬†Liu, and G.¬†Shi, ‚ÄúOmnih2o: Universal and dexterous human-to-humanoid whole-body teleoperation and learning,‚Äù in\nProceedings of The 8th Conference on Robot Learning\n, ser. Proceedings of Machine Learning Research, vol. 270.‚ÄÉPMLR, November 2025, pp. 1516‚Äì1540. [Online]. Available:\nhttps://proceedings.mlr.press/v270/he25b.html\n[66]\nZ.¬†Li, X.¬†B. Peng, P.¬†Abbeel, S.¬†Levine, G.¬†Berseth, and K.¬†Sreenath, ‚ÄúReinforcement learning for versatile, dynamic, and robust bipedal locomotion control,‚Äù\nThe International Journal of Robotics Research\n, 2024.\n[67]\nL.¬†Pinto, M.¬†Andrychowicz, P.¬†Welinder, W.¬†Zaremba, and P.¬†Abbeel, ‚ÄúAsymmetric actor critic for image-based robot learning,‚Äù in\nRobotics: Science and Systems XIV, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA, June 26-30, 2018\n, H.¬†Kress-Gazit, S.¬†S. Srinivasa, T.¬†Howard, and N.¬†Atanasov, Eds., 2018. [Online]. Available:\nhttp://www.roboticsproceedings.org/rss14/p08.html\n[68]\nZ.¬†Chen, M.¬†Ji, X.¬†Cheng, X.¬†Peng, X.¬†B. Peng, and X.¬†Wang, ‚ÄúGmt: General motion tracking for humanoid whole-body control,‚Äù\narXiv preprint arXiv:2506.14770\n, 2025.\n[69]\nN.¬†Messikommer, J.¬†Xing, E.¬†Aljalbout, and D.¬†Scaramuzza, ‚ÄúStudent-informed teacher training,‚Äù in\n7th Robot Learning Workshop: Towards Robots with Human-Level Abilities\n, 2025. [Online]. Available:\nhttps://openreview.net/forum?id=q06EAFOwDI\n[70]\nJ.¬†Lee, J.¬†Hwangbo, L.¬†Wellhausen, V.¬†Koltun, and M.¬†Hutter, ‚ÄúLearning quadrupedal locomotion over challenging terrain,‚Äù\nScience robotics\n, vol.¬†5, no.¬†47, p. eabc5986, 2020.\n[71]\nT.¬†Pfau, A.¬†G. de¬†Rivaz, S.¬†Brighton, and R.¬†Weller, ‚ÄúKinetics of jump landing in agility dogs,‚Äù\nThe Veterinary Journal\n, vol. 190, no.¬†2, pp. 278‚Äì283, 2011.\n[72]\nS.¬†McErlain-Naylor, M.¬†King, and S.¬†Allen, ‚ÄúSurface acceleration transmission during drop landings in humans,‚Äù\nJournal of Biomechanics\n, vol. 118, p. 110269, 2021.\n[73]\nJ.¬†Tobin, R.¬†Fong, A.¬†Ray, J.¬†Schneider, W.¬†Zaremba, and P.¬†Abbeel, ‚ÄúDomain randomization for transferring deep neural networks from simulation to the real world,‚Äù in\n2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)\n.‚ÄÉIEEE, 2017, pp. 23‚Äì30.\n[74]\nC.¬†Schwarke, M.¬†Mittal, N.¬†Rudin, D.¬†Hoeller, and M.¬†Hutter, ‚ÄúRsl-rl: A learning library for robotics research,‚Äù\narXiv preprint arXiv:2509.10771\n, 2025.\n[75]\nO.¬†R. developers, ‚ÄúOnnx runtime,‚Äù\nhttps://onnxruntime.ai/\n, 2025.\n[76]\nJ.¬†Hwangbo, J.¬†Lee, A.¬†Dosovitskiy, D.¬†Bellicoso, V.¬†Tsounis, V.¬†Koltun, and M.¬†Hutter, ‚ÄúLearning agile and dynamic motor skills for legged robots,‚Äù\nScience Robotics\n, vol.¬†4, no.¬†26, p. eaau5872, 2019.\n[77]\nY.-H. Shin, T.-G. Song, G.¬†Ji, and H.-W. Park, ‚ÄúActuator-constrained reinforcement learning for high-speed quadrupedal locomotion,‚Äù\narXiv preprint arXiv:2312.17507\n, 2023.\n[78]\nO.¬†Ronneberger, P.¬†Fischer, and T.¬†Brox, ‚ÄúU-net: Convolutional networks for biomedical image segmentation,‚Äù in\nInternational Conference on Medical image computing and computer-assisted intervention\n.‚ÄÉSpringer, 2015, pp. 234‚Äì241.\n[79]\nM.¬†Macklin, ‚ÄúWarp: A high-performance python framework for gpu simulation and graphics,‚Äù\nhttps://github.com/nvidia/warp\n, March 2022, nVIDIA GPU Technology Conference (GTC).\n[80]\nM.¬†Seitzer, A.¬†Tavakoli, D.¬†Antic, and G.¬†Martius, ‚ÄúOn the pitfalls of heteroscedastic uncertainty estimation with probabilistic neural networks,‚Äù in\nInternational Conference on Learning Representations\n, 2022.\n[81]\nA.¬†Chambolle, ‚ÄúAn algorithm for total variation minimization and applications,‚Äù\nJournal of Mathematical imaging and vision\n, vol.¬†20, no.¬†1, pp. 89‚Äì97, 2004.\n[82]\nS.¬†Khattak, T.¬†Homberger, L.¬†Bernreiter, J.¬†Nubert, O.¬†Andersson, R.¬†Siegwart, K.¬†Alexis, and M.¬†Hutter, ‚ÄúCompslam: Complementary hierarchical multi-modal localization and mapping for robot autonomy in underground environments,‚Äù\narXiv preprint arXiv:2505.06483\n, 2025.\n[83]\nJ.¬†Nubert, S.¬†Khattak, and M.¬†Hutter, ‚ÄúGraph-based multi-sensor fusion for consistent localization of autonomous construction robots,‚Äù in\n2022 International Conference on Robotics and Automation (ICRA)\n.‚ÄÉIEEE, 2022, pp. 10‚Äâ048‚Äì10‚Äâ054.\n[84]\nK.¬†Chen, R.¬†Nemiroff, and B.¬†T. Lopez, ‚ÄúDirect lidar-inertial odometry: Lightweight lio with continuous-time motion correction,‚Äù in\n2023 IEEE International Conference on Robotics and Automation (ICRA)\n.‚ÄÉIEEE, 2023, pp. 3983‚Äì3989.\n[85]\nZ.¬†Zhuang and H.¬†Zhao, ‚ÄúEmbrace contacts: humanoid shadowing with full body ground contacts,‚Äù in\n9th Annual Conference on Robot Learning\n, 2025.\n[86]\nL.¬†Yang, X.¬†Huang, Z.¬†Wu, A.¬†Kanazawa, P.¬†Abbeel, C.¬†Sferrazza, C.¬†K. Liu, R.¬†Duan, and G.¬†Shi, ‚ÄúOmniretarget: Interaction-preserving data generation for humanoid whole-body loco-manipulation and scene interaction,‚Äù\narXiv preprint arXiv:2509.26633\n, 2025.\n[87]\nA.¬†Dosovitskiy, L.¬†Beyer, A.¬†Kolesnikov, D.¬†Weissenborn, X.¬†Zhai, T.¬†Unterthiner, M.¬†Dehghani, M.¬†Minderer, G.¬†Heigold, S.¬†Gelly, J.¬†Uszkoreit, and N.¬†Houlsby, ‚ÄúAn image is worth 16x16 words: Transformers for image recognition at scale,‚Äù\nICLR\n, 2021.\n[88]\nF.¬†Yang, P.¬†Frivik, D.¬†Hoeller, C.¬†Wang, C.¬†Cadena, and M.¬†Hutter, ‚ÄúSpatially-enhanced recurrent memory for long-range mapless navigation via end-to-end reinforcement learning,‚Äù\nThe International Journal of Robotics Research\n, p. 02783649251401926, 2025.\n[89]\nT.¬†He, W.¬†Xiao, T.¬†Lin, Z.¬†Luo, Z.¬†Xu, Z.¬†Jiang, J.¬†Kautz, C.¬†Liu, G.¬†Shi, X.¬†Wang, L.¬†J. Fan, and Y.¬†Zhu, ‚ÄúHOVER: versatile neural whole-body controller for humanoid robots,‚Äù in\nIEEE International Conference on Robotics and Automation, ICRA\n.‚ÄÉIEEE, 2025, pp. 9989‚Äì9996.",
    "preview_text": "Achieving agile and generalized legged locomotion across terrains requires tight integration of perception and control, especially under occlusions and sparse footholds. Existing methods have demonstrated agility on parkour courses but often rely on end-to-end sensorimotor models with limited generalization and interpretability. By contrast, methods targeting generalized locomotion typically exhibit limited agility and struggle with visual occlusions. We introduce AME-2, a unified reinforcement learning (RL) framework for agile and generalized locomotion that incorporates a novel attention-based map encoder in the control policy. This encoder extracts local and global mapping features and uses attention mechanisms to focus on salient regions, producing an interpretable and generalized embedding for RL-based control. We further propose a learning-based mapping pipeline that provides fast, uncertainty-aware terrain representations robust to noise and occlusions, serving as policy inputs. It uses neural networks to convert depth observations into local elevations with uncertainties, and fuses them with odometry. The pipeline also integrates with parallel simulation so that we can train controllers with online mapping, aiding sim-to-real transfer. We validate AME-2 with the proposed mapping pipeline on a quadruped and a biped robot, and the resulting controllers demonstrate strong agility and generalization to unseen terrains in simulation and in real-world experiments.\n\nAME-2: Agile and Generalized Legged Locomotion via Attention-Based Neural Map Encoding\nChong Zhang\n123‚àó\n, Victor Klemm\n1\n, Fan Yang\n1\n, Marco Hutter\n1\nWebsite:\nsites.google.com/leggedrobotics.com/ame-2\n1\nRobotic Systems Lab, ETH Zurich, Switzerland\n2\nSecure, Reliable, and Intelligent Systems Lab, ETH Zurich, Switzerland\n3\nETH AI Center, Switzerland\n‚àó\nCorresponding: chong.zhang@ai.ethz.ch\nAbstract\nAchieving agile and generalized legged locomotion across terrains requires tight integration of perception a",
    "is_relevant": true,
    "relevance_score": 9.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "AME-2ÊòØ‰∏Ä‰∏™Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂú∞ÂõæÁºñÁ†ÅÊ°ÜÊû∂ÔºåÁî®‰∫éÂÆûÁé∞ÊïèÊç∑ÂíåÈÄöÁî®ÁöÑËÖøÂºèÊú∫Âô®‰∫∫ËøêÂä®ÊéßÂà∂„ÄÇ",
    "detailed_summary": "## ËÆ∫Êñá„ÄäAME-2ÔºöÂü∫‰∫éÊ≥®ÊÑèÂäõÁ•ûÁªèÂú∞ÂõæÁºñÁ†ÅÁöÑÊïèÊç∑ÈÄöÁî®ËÖøÂºèËøêÂä®„ÄãËØ¶ÁªÜÊëòË¶Å\n\n### 1. Á†îÁ©∂ËÉåÊôØÂíåÂä®Êú∫\nÂÆûÁé∞ËÖøÂºèÊú∫Âô®‰∫∫Âú®Â§çÊùÇÂú∞ÂΩ¢‰∏äÁöÑ**ÊïèÊç∑‰∏îÈÄöÁî®**ÁöÑÁßªÂä®ÔºåÈúÄË¶ÅÁ¥ßÂØÜËÄ¶ÂêàÊÑüÁü•‰∏éÊéßÂà∂„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ≠òÂú®Â±ÄÈôêÔºöÂü∫‰∫éÊòæÂºèÂú∞ÂõæÁöÑÊ®°ÂûãÊñπÊ≥ïÈÄöÂ∏∏**ÊïèÊç∑ÊÄß‰∏çË∂≥**‰∏îÂØπÈÅÆÊå°ÊïèÊÑüÔºõËÄåÂü∫‰∫éÂéüÂßã‰º†ÊÑüÂô®ÁöÑÁ´ØÂà∞Á´ØÂ≠¶‰π†ÊñπÊ≥ïËôΩËÉΩÂÆûÁé∞È´òÊïèÊç∑ÊÄßÔºå‰ΩÜ**Ê≥õÂåñËÉΩÂäõÊúâÈôê**‰∏îÁº∫‰πèÂèØËß£ÈáäÊÄß„ÄÇÂõ†Ê≠§Ôºå‰∫üÈúÄ‰∏Ä‰∏™ËÉΩÂêåÊó∂ÂÖºÈ°æ**ÊïèÊç∑ÊÄß„ÄÅÊ≥õÂåñÊÄß„ÄÅÈ≤ÅÊ£íÊÄßÂíåÂèØËß£ÈáäÊÄß**ÁöÑÁªü‰∏ÄÊ°ÜÊû∂„ÄÇ\n\n### 2. Ê†∏ÂøÉÊñπÊ≥ïÂíåÊäÄÊúØÂàõÊñ∞\nÊú¨ÊñáÊèêÂá∫ **AME-2**Ôºå‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂÖ∂Ê†∏ÂøÉÂàõÊñ∞Âú®‰∫éÔºö\n- **Ê≥®ÊÑèÂäõÁ•ûÁªèÂú∞ÂõæÁºñÁ†ÅÂô® (AME-2 Encoder)**Ôºö‰ªéÈ´òÁ®ãÂú∞Âõæ‰∏≠ÊèêÂèñ**Â±ÄÈÉ®ÁâπÂæÅ**ÔºàÂú∞ÂΩ¢ÁªÜËäÇÔºâÂíå**ÂÖ®Â±ÄÁâπÂæÅ**ÔºàÂú∞ÂΩ¢‰∏ä‰∏ãÊñáÔºâÔºåÂπ∂Âà©Áî®ÂÖ®Â±ÄÁâπÂæÅ‰∏éÊú¨‰ΩìÊÑüÁü•ÈÄöËøáÊ≥®ÊÑèÂäõÊú∫Âà∂Âä®ÊÄÅÂä†ÊùÉÂ±ÄÈÉ®ÁâπÂæÅÔºåÁîüÊàê**Âú∞ÂΩ¢ÊÑüÁü•**ÁöÑË°®Á§∫Ôºå‰ªéËÄåÊèêÂçáÁ≠ñÁï•ÂØπ‰∏çÂêåÂú∞ÂΩ¢ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ\n- **ËΩªÈáèÁ∫ß„ÄÅ‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÁöÑÁ•ûÁªèÂª∫ÂõæÊµÅÊ∞¥Á∫ø**Ôºö‰ΩøÁî®ËΩªÈáèÁ∫ßCNNÔºåÈÄöËøáË¥ùÂè∂ÊñØÂ≠¶‰π†‰ªéÊ∑±Â∫¶ÂõæÂÉèÈ¢ÑÊµã**Â±ÄÈÉ®È´òÁ®ãÂèäÂÖ∂‰∏çÁ°ÆÂÆöÊÄß**ÔºåÂπ∂ÈÄöËøá‰∏ÄÁßç**Ê¶ÇÁéáÊÄßËµ¢ÂÆ∂ÈÄöÂêÉ**Á≠ñÁï•‰∏éÈáåÁ®ãËÆ°ËûçÂêàÔºåÊûÑÂª∫ÂÖ®Â±ÄÂú∞Âõæ„ÄÇËØ•ÊµÅÊ∞¥Á∫ø**ÊòæÂºèÂª∫Ê®°ÈÅÆÊå°ÂíåÂô™Â£∞**Ôºå‰∏îËÆ°ÁÆóÈ´òÊïàÔºåÂèØÂêåÊó∂Âú®‰ªøÁúüÂíåÁúüÊú∫‰∏äËøêË°å„ÄÇ\n- **Â∏àÁîüÂ≠¶‰π†ÊñπÊ°à**ÔºöÂÖà‰ΩøÁî®**ÁúüÂÆûÂú∞Âõæ**ËÆ≠ÁªÉ‰∏Ä‰∏™‚ÄúÊïôÂ∏à‚ÄùÁ≠ñÁï•ÔºåÂÜçÂú®‚ÄúÂ≠¶Áîü‚ÄùÁ≠ñÁï•ËÆ≠ÁªÉ‰∏≠Ôºå‰ΩøÁî®‰∏äËø∞Á•ûÁªèÂª∫ÂõæÊµÅÊ∞¥Á∫øÔºåÂπ∂ÂêåÊó∂ÁªìÂêà**Âº∫ÂåñÂ≠¶‰π†ÊçüÂ§±„ÄÅÂä®‰ΩúËí∏È¶èÊçüÂ§±ÂíåË°®ÂæÅÂØπÈΩêÊçüÂ§±**ËøõË°åÁõëÁù£ÔºåÊúÄÁªàÂæóÂà∞‰∏Ä‰∏™ÂèØÁõ¥Êé•ÈÉ®ÁΩ≤„ÄÅ‰øùÊåÅÊïôÂ∏àÁ≠ñÁï•ÊÄßËÉΩÁöÑ‚ÄúÂ≠¶Áîü‚ÄùÁ≠ñÁï•„ÄÇ\n\n### 3. ‰∏ªË¶ÅÂÆûÈ™åÁªìÊûú\nÂú®ANYmal-DÂõõË∂≥Êú∫Âô®‰∫∫ÂíåLimX TRON1ÂèåË∂≥Êú∫Âô®‰∫∫‰∏äËøõË°å‰∫Ü‰ªøÁúü‰∏éÂÆûÁâ©È™åËØÅÔºö\n- **ÊïèÊç∑ÊÄß**ÔºöÂú®Êú™ËÆ≠ÁªÉËøáÁöÑÂ§çÊùÇË∑ëÈÖ∑ÂíåÁ¢éÁü≥Âú∞ÂΩ¢‰∏äÂÆûÁé∞‰∫Ü**Èõ∂Ê†∑Êú¨**Á©øË∂äÔºåÊîÄÁà¨È´òÂ∫¶ËææÂà∞ÊàñË∂ÖËøáÁé∞ÊúâÂÖàËøõÊ∞¥Âπ≥„ÄÇ\n- **Ê≥õÂåñÊÄß**ÔºöÂú®Â§öÁßçÊú™ËßÅËøáÁöÑ**Á®ÄÁñèÂú∞ÂΩ¢**ÔºàÂ¶ÇÁ™ÑÊ¢Å„ÄÅÊµÆÁü≥„ÄÅÈ´òÂè∞Èò∂Ôºâ‰∏äË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈÄÇÂ∫îËÉΩÂäõ„ÄÇ\n- **È≤ÅÊ£íÊÄß**ÔºöÂØπ‰º†ÊÑüÂô®Âô™Â£∞„ÄÅÈÉ®ÂàÜÁÇπ‰∫ëÁº∫Â§±ÁîöËá≥ÂçïÁõ∏Êú∫Â§±ÊïàÂÖ∑Êúâ**Âº∫È≤ÅÊ£íÊÄß**ÔºõËÉΩÂú®ÁßªÂä®ÁöÑ‰∏çÁ®≥ÂÆöÂú∞ÂΩ¢‰∏äÊÅ¢Â§çÂπ≥Ë°°„ÄÇ\n- **Âª∫ÂõæË¥®Èáè**ÔºöÂª∫ÂõæÊµÅÊ∞¥Á∫øËÉΩÂÆûÊó∂ÁîüÊàêÂåÖÂê´**‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°**ÁöÑÈ´òË¥®ÈáèÂú∞ÂõæÔºåÊîØÊåÅÊïèÊç∑ËøêÂä®„ÄÇ\n- **ÂèØËß£ÈáäÊÄß**ÔºöAME-2ÁºñÁ†ÅÂô®ÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçÂèØËßÜÂåñÊòæÁ§∫ÔºåÂÖ∂ËÉΩÊ†πÊçÆÂú∞ÂΩ¢Á±ªÂûãÂÖ≥Ê≥®‰∏çÂêåÁöÑÂÖ≥ÈîÆÂå∫ÂüüÔºàÂ¶ÇËæπÁºò„ÄÅÂπ≥Âè∞Ë°®Èù¢Ôºâ„ÄÇ\n\n### 4. Á†îÁ©∂ÊÑè‰πâÂíå‰ª∑ÂÄº\nÊú¨Á†îÁ©∂ÁöÑÊÑè‰πâÂú®‰∫éÔºö\n- **ÊäÄÊúØÊï¥Âêà**ÔºöÊàêÂäüÂú∞Â∞Ü**Â≠¶‰π†ÂûãÂª∫Âõæ**‰∏é**Â≠¶‰π†ÂûãÊéßÂà∂**Âú®‰∏Ä‰∏™Ê°ÜÊû∂ÂÜÖÁªìÂêàÔºåÂú®‰øùÊåÅÊ®°ÂùóÂåñÁ≥ªÁªü‰ºòÁÇπÁöÑÂêåÊó∂ÔºåÈÄöËøáÁ´ØÂà∞Á´ØËÆ≠ÁªÉÂÖãÊúç‰∫Ü‰º†ÁªüÊñπÊ≥ïÂú®ÊïèÊç∑ÊÄßÂíå‰∏çÁ°ÆÂÆöÊÄßÂ§ÑÁêÜ‰∏äÁöÑÁì∂È¢à„ÄÇ\n- **ÊÄßËÉΩÁ™ÅÁ†¥**ÔºöÈ¶ñÊ¨°Âú®**Âêå‰∏ÄËÆ≠ÁªÉËÆæÁΩÆ‰∏ã**Ôºå‰∫éÂõõË∂≥ÂíåÂèåË∂≥‰∏§‰∏™Âπ≥Âè∞‰∏äÂêåÊó∂ÂÆûÁé∞‰∫Ü**ÂÖàËøõÊ∞¥Âπ≥ÁöÑÊïèÊç∑ÊÄß‰∏éÂº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨Âú∞ÂΩ¢Ê≥õÂåñËÉΩÂäõ**Ôºå‰∏∫ËÖøÂºèÊú∫Âô®‰∫∫Âú®Â§çÊùÇÈùûÁªìÊûÑÂåñÁéØÂ¢É‰∏≠ÁöÑÂÆûÈôÖÂ∫îÁî®Êèê‰æõ‰∫ÜÊõ¥ÂèØÈù†ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ\n- **Â∑•Á®ãÂÆûÁî®**ÔºöÊèêÂá∫ÁöÑ**ËΩªÈáèÁ∫ßÁ•ûÁªèÂª∫ÂõæÊµÅÊ∞¥Á∫ø**Âíå**Â∏àÁîüËÆ≠ÁªÉÊñπÊ°à**ÊúâÊïàÂº•Âêà‰∫Ü‰ªøÁúüÂà∞Áé∞ÂÆûÁöÑÈ∏øÊ≤üÔºåÊé®Âä®‰∫ÜÈ´òÊÄßËÉΩÂ≠¶‰π†ÊéßÂà∂Âô®Âú®ÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÁöÑÈÉ®ÁΩ≤„ÄÇ",
    "qa_pairs": [
        {
            "question": "ËøôÁØáËÆ∫ÊñáÁöÑÊ†∏ÂøÉÂàõÊñ∞ÁÇπÊòØ‰ªÄ‰πàÔºå‰ªñÊÉ≥Ëß£ÂÜ≥‰ªÄ‰πàÈóÆÈ¢òÔºåÊÄé‰πàËß£ÂÜ≥ÁöÑÔºü",
            "answer": "## ËÆ∫ÊñáÊ†∏ÂøÉÂàÜÊûêÔºöAME-2\n\n### **‰∏Ä„ÄÅ Ê†∏ÂøÉÈóÆÈ¢ò**\nËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥**Ë∂≥ÂºèÊú∫Âô®‰∫∫ÊÑüÁü•ËøêÂä®**È¢ÜÂüüÁöÑ‰∏Ä‰∏™ÂÖ≥ÈîÆÁüõÁõæÔºöÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âú®**ÊïèÊç∑ÊÄß**ÔºàÂ¶ÇË∑ëÈÖ∑ÔºâÂíå**Ê≥õÂåñÊÄß**ÔºàÈÄÇÂ∫îÊú™Áü•Â§çÊùÇÂú∞ÂΩ¢Ôºâ‰πãÈó¥Èöæ‰ª•ÂÖºÈ°æ„ÄÇ\n*   **‰º†ÁªüÊ®°ÂûãÊñπÊ≥ï**Ôºö‰æùËµñÊòæÂºèÂú∞ÂõæÂíåËßÑÂàíÔºåÊ≥õÂåñÊÄßÂ•Ω‰ΩÜËÆ°ÁÆóÈáç„ÄÅÂØπÊ®°ÂûãËØØÂ∑ÆÊïèÊÑüÔºåÈöæ‰ª•ÂÆûÁé∞È´òÈÄüÂä®ÊÄÅËøêÂä®„ÄÇ\n*   **Á´ØÂà∞Á´ØÂ≠¶‰π†ÊñπÊ≥ï**ÔºöÁõ¥Êé•‰ªéÂéüÂßã‰º†ÊÑüÂô®Êï∞ÊçÆÊò†Â∞ÑÂà∞Âä®‰ΩúÔºåËÉΩÂÆûÁé∞È´òÊïèÊç∑ÊÄßÔºå‰ΩÜÊ≥õÂåñËÉΩÂäõÊúâÈôê„ÄÅÂèØËß£ÈáäÊÄßÂ∑ÆÔºå‰∏îÈöæ‰ª•Â§ÑÁêÜÈÅÆÊå°ÂíåÁ®ÄÁñèÁ´ãË∂≥ÁÇπ„ÄÇ\n\n### **‰∫å„ÄÅ Ê†∏ÂøÉÂàõÊñ∞ÁÇπ**\nËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ **AME-2**ÔºåÂÖ∂ÂàõÊñ∞ÊòØ‰∏Ä‰∏™**Á≥ªÁªüÊÄßÁöÑÁªÑÂêà**ÔºåËÄåÈùûÂçï‰∏ÄÊäÄÊúØÔºåÊó®Âú®ÂêåÊó∂ÂÆûÁé∞**ÊïèÊç∑ÊÄß„ÄÅÊ≥õÂåñÊÄß„ÄÅÂèØËß£ÈáäÊÄßÂíåÂèØÈÉ®ÁΩ≤ÊÄß**„ÄÇ\n\n#### **1. ÂàõÊñ∞ÁöÑÊ≥®ÊÑèÂäõÂú∞ÂõæÁºñÁ†ÅÂô® (AME-2 Encoder)**\n*   **ÊòØ‰ªÄ‰πà**ÔºöÊéßÂà∂Á≠ñÁï•‰∏≠ÁöÑÊ†∏ÂøÉÊ®°ÂùóÔºåÁî®‰∫éÂ§ÑÁêÜÈ´òÁ®ãÂú∞ÂõæËæìÂÖ•„ÄÇ\n*   **ÊÄé‰πàÂàõÊñ∞**Ôºö\n    *   **ÂèåË∑ØÁâπÂæÅÊèêÂèñ**ÔºöÂêåÊó∂ÊèêÂèñ**Â±ÄÈÉ®ÁâπÂæÅ**ÔºàÂú∞ÂΩ¢ÁªÜËäÇÔºâÂíå**ÂÖ®Â±ÄÁâπÂæÅ**ÔºàÂú∞ÂΩ¢Êï¥‰Ωì‰∏ä‰∏ãÊñáÔºâ„ÄÇ\n    *   **‰∏ä‰∏ãÊñáÊÑüÁü•ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂**ÔºöÂà©Áî®**ÂÖ®Â±ÄÁâπÂæÅ**Âíå**Êú¨‰ΩìÊÑüÁü•Áä∂ÊÄÅ**ÁîüÊàêÊü•ËØ¢ÂêëÈáèÔºåÈÄöËøáÂ§öÂ§¥Ê≥®ÊÑèÂäõÔºàMHAÔºâÊ®°ÂùóÂØπÂ±ÄÈÉ®ÁâπÂæÅËøõË°åÂä†ÊùÉ„ÄÇËøô‰ΩøÂæóÁ≠ñÁï•ËÉΩÊ†πÊçÆÂΩìÂâçÂú∞ÂΩ¢ÂÖ®Â±Ä‰ø°ÊÅØÂíåÊú∫Âô®‰∫∫Áä∂ÊÄÅÔºåÂä®ÊÄÅÂÖ≥Ê≥®Âú∞Âõæ‰∏≠‰∏éÂΩìÂâç‰ªªÂä°ÊúÄÁõ∏ÂÖ≥ÁöÑÂå∫Âüü„ÄÇ\n    *   **‰∏éAME-1ÁöÑÂå∫Âà´**ÔºöÁõ∏ÊØîÂâç‰ΩúAME-1Ôºà‰ªÖÂü∫‰∫éÊú¨‰ΩìÊÑüÁü•Âä†ÊùÉÂ±ÄÈÉ®ÁâπÂæÅÔºâÔºåAME-2ÂºïÂÖ•‰∫ÜÂÖ®Â±Ä‰∏ä‰∏ãÊñáÔºå‰ΩøÂÖ∂ËÉΩÂ≠¶‰π†‰∏çÂêåÂú∞ÂΩ¢‰∏ãÁöÑÂ∑ÆÂºÇÂåñËøêÂä®Ê®°ÂºèÔºå‰ªéËÄåÊòæËëóÊèêÂçáÊ≥õÂåñËÉΩÂäõÔºàËÆ∫ÊñáÁ¨¨VII-AËäÇÈ™åËØÅÔºâ„ÄÇ\n\n#### **2. ËΩªÈáèÁ∫ß„ÄÅ‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÁöÑÁ•ûÁªèÈ´òÁ®ãÂª∫ÂõæÁÆ°ÈÅì**\n*   **Ëß£ÂÜ≥ÁöÑÈóÆÈ¢ò**ÔºöÊëÜËÑ±ÂØπÁªèÂÖ∏Âª∫ÂõæÊ†àÁöÑ‰æùËµñÔºåÊèê‰æõÂø´ÈÄü„ÄÅËÉΩÂ§ÑÁêÜÂô™Â£∞ÂíåÈÅÆÊå°„ÄÅ‰∏îÊòì‰∫é‰ªøÁúüÁöÑÂú∞ÂΩ¢Ë°®Á§∫„ÄÇ\n*   **ÊÄé‰πàÂàõÊñ∞**Ôºö\n    *   **Âü∫‰∫éÂ≠¶‰π†ÁöÑÂçïÂ∏ßÈ¢ÑÊµã**Ôºö‰ΩøÁî®ËΩªÈáèÁ∫ßCNNÔºàU-NetÔºâÁõ¥Êé•‰ªéÂçïÂ∏ßÊ∑±Â∫¶ÂõæÊäïÂΩ±ÁöÑÂ±ÄÈÉ®ÁΩëÊ†º‰∏≠Ôºå**ËÅîÂêàÈ¢ÑÊµãÈ´òÁ®ãÂÄºÂíåÊØè‰∏™ÂçïÂÖÉÊ†ºÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºàÊñπÂ∑ÆÔºâ**„ÄÇ‰∏çÁ°ÆÂÆöÊÄßÊòæÂºèÂª∫Ê®°‰∫ÜÂô™Â£∞ÂíåÈÅÆÊå°„ÄÇ\n    *   **Ê¶ÇÁéáÊÄß‚ÄúËµ¢ÂÆ∂ÈÄöÂêÉ‚ÄùËûçÂêàÁ≠ñÁï•**ÔºöÂ∞ÜÂ∏¶‰∏çÁ°ÆÂÆöÊÄßÁöÑÂ±ÄÈÉ®È¢ÑÊµã‰∏éÂü∫‰∫éÈáåÁ®ãËÆ°ÁöÑÂÖ®Â±ÄÂú∞ÂõæËûçÂêà„ÄÇËØ•Á≠ñÁï•ËÉΩÈò≤Ê≠¢ÂØπÈÅÆÊå°Âå∫ÂüüÁöÑËøáÂ∫¶Ëá™‰ø°Êõ¥Êñ∞ÔºåÂêåÊó∂ÂÖÅËÆ∏Âú®Ëé∑ÂæóÈ´òÁΩÆ‰ø°Â∫¶ÊµãÈáèÊó∂Âø´ÈÄüÊõ¥Êñ∞Âú∞Âõæ„ÄÇ\n    *   **Èù¢ÂêëÊ≥õÂåñÁöÑÊï∞ÊçÆÂêàÊàê**Ôºö‰ΩøÁî®Â§ßËßÑÊ®°ÈöèÊú∫ÁîüÊàêÁöÑÂú∞ÂΩ¢ÔºàÂåÖÊã¨Êú∫Âô®‰∫∫‰∏çÂèØÈÄöË°åÂú∞ÂΩ¢ÔºâËÆ≠ÁªÉÂª∫ÂõæÊ®°ÂûãÔºåÊèêÂçáÂÖ∂Èù¢ÂØπÊú™Áü•Âú∞ÂΩ¢ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ\n    *   **‰ªøÁúü-Áé∞ÂÆû‰∏ÄËá¥ÊÄß**ÔºöÂêå‰∏ÄÂ•óÂª∫ÂõæÁÆ°ÈÅìÂèØÊó†ÁºùËøêË°å‰∫éÂ§ßËßÑÊ®°Âπ∂Ë°å‰ªøÁúüÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰∏äÔºåÁõ¥Êé•Áº©Â∞è‰∫Ü‰ªøÁúü‰∏éÁé∞ÂÆûÂ∑ÆË∑ù„ÄÇ\n\n#### **3. ÊïôÂ∏à-Â≠¶ÁîüËÆ≠ÁªÉËåÉÂºè‰∏éÁªü‰∏ÄËÆ≠ÁªÉÊ°ÜÊû∂**\n*   **Ëß£ÂÜ≥ÁöÑÈóÆÈ¢ò**ÔºöÈ´òÊïàËÆ≠ÁªÉÂá∫ËÉΩÁõ¥Êé•ÈÉ®ÁΩ≤„ÄÅ‰∏î‰øùÊåÅÊïôÂ∏àÁ≠ñÁï•ÊÄßËÉΩÁöÑÊéßÂà∂Âô®„ÄÇ\n*   **ÊÄé‰πàÂàõÊñ∞**Ôºö\n    *   **‰∏§Èò∂ÊÆµËÆ≠ÁªÉ**Ôºö\n        1.  **ÊïôÂ∏àÁ≠ñÁï•**ÔºöÂú®‰ªøÁúü‰∏≠‰ΩøÁî®**ÁúüÂÆûÈ´òÁ®ãÂú∞Âõæ**ËÆ≠ÁªÉÔºåÂ≠¶‰π†ÊïèÊç∑ÂíåÊ≥õÂåñÁöÑËøêÂä®ÊäÄËÉΩ„ÄÇ\n        2.  **Â≠¶ÁîüÁ≠ñÁï•**Ôºö‰ΩøÁî®**‰∏äËø∞Á•ûÁªèÂª∫ÂõæÁÆ°ÈÅì**‰Ωú‰∏∫ËæìÂÖ•ÔºåÂú®ÊïôÂ∏àÁ≠ñÁï•ÁöÑÁõëÁù£‰∏ãÔºàÈÄöËøáÂä®‰ΩúËí∏È¶èÂíåË°®ÂæÅÂØπÈΩêÊçüÂ§±ÔºâÁªìÂêàRLËøõË°åËÆ≠ÁªÉ„ÄÇ\n    *   **Áªü‰∏ÄÁöÑÂ•ñÂä±‰∏éËØæÁ®ãËÆæËÆ°**Ôºö‰∏∫ÂõõË∂≥ÔºàANYmal-DÔºâÂíåÂèåË∂≥ÔºàTRON1ÔºâÊú∫Âô®‰∫∫ËÆæËÆ°‰∫Ü**Âêå‰∏ÄÂ•óÁõÆÊ†áÂà∞ËææÂ•ñÂä±ÂáΩÊï∞ÂíåËÆ≠ÁªÉËÆæÁΩÆ**ÔºåÊó†ÈúÄÈíàÂØπ‰∏çÂêåÊú∫Âô®‰∫∫Âπ≥Âè∞ËøõË°åÁπÅÁêêË∞ÉÊï¥ÔºåËØÅÊòé‰∫ÜÊ°ÜÊû∂ÁöÑÈÄöÁî®ÊÄß„ÄÇ\n\n### **‰∏â„ÄÅ Ëß£ÂÜ≥ÊñπÊ°àÊÄªÁªì**\nËÆ∫ÊñáÈÄöËøá‰∏Ä‰∏™**Á¥ßÂØÜËÄ¶ÂêàÁöÑËΩØÁ°¨‰ª∂ÂçèÂêåËÆæËÆ°**Êù•Ëß£ÂÜ≥Ê†∏ÂøÉÈóÆÈ¢òÔºö\n\n1.  **ÊÑüÁü•Â±ÇÔºàËæìÂÖ•Ôºâ**Ôºö‰ΩøÁî®**‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•Á•ûÁªèÂª∫ÂõæÁÆ°ÈÅì**ÔºåÊèê‰æõÈ≤ÅÊ£í„ÄÅÂø´ÈÄü„ÄÅÂèØÊ≥õÂåñÁöÑÂú∞ÂΩ¢Ë°®Á§∫„ÄÇ\n2.  **ÂÜ≥Á≠ñ‰∏éÊéßÂà∂Â±ÇÔºàÊ†∏ÂøÉÔºâ**ÔºöÈááÁî®**AME-2ÁºñÁ†ÅÂô®**ÁöÑÁ≠ñÁï•Êû∂ÊûÑÔºå‰ªéÂú∞ÂõæÂíåÊú¨‰ΩìÊÑüÁü•‰∏≠ÊèêÂèñÂÖ∑ÊúâÂèØËß£ÈáäÊÄßÁöÑ„ÄÅÂú∞ÂΩ¢ÊÑüÁü•ÁöÑÁâπÂæÅË°®Á§∫Ôºå‰ª•È©±Âä®Âº∫ÂåñÂ≠¶‰π†Á≠ñÁï•„ÄÇ\n3.  **ËÆ≠ÁªÉ‰∏éËøÅÁßªÂ±ÇÔºàÊ°•Ê¢ÅÔºâ**ÔºöÈÄöËøá**ÊïôÂ∏à-Â≠¶ÁîüËåÉÂºè**ÔºåÂ∞ÜÂú®ÁêÜÊÉ≥ÊÑüÁü•ÔºàÁúüÂÆûÂú∞ÂõæÔºâ‰∏ãÂ≠¶Âà∞ÁöÑÊäÄËÉΩÔºåÈ´òÊïàËøÅÁßªÂà∞‰ΩøÁî®ÂÆûÈôÖÂª∫ÂõæÁÆ°ÈÅìÁöÑÂèØÈÉ®ÁΩ≤Á≠ñÁï•‰∏ä„ÄÇ\n\n### **Âõõ„ÄÅ ÂÆûÈôÖ‰ª∑ÂÄº‰∏éÈ™åËØÅ**\n*   **ÊÄßËÉΩË°®Áé∞**ÔºöÂú®‰ªøÁúüÂíåÂÆûÁâ©ÂÆûÈ™å‰∏≠ÔºåANYmal-DÂíåTRON1Êú∫Âô®‰∫∫ÂùáÂ±ïÁ§∫‰∫Ü**ÂÖàËøõÁöÑÊïèÊç∑ÊÄß**ÔºàÊàêÂäüÈõ∂Ê†∑Êú¨ÈÄöËøáÈ´òÈöæÂ∫¶Ë∑ëÈÖ∑„ÄÅÊîÄÁà¨Âú∞ÂΩ¢ÔºâÂíå**Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ**ÔºàÂú®Â§ßÈáèÊú™ËßÅËøáÁöÑÁ®ÄÁñè„ÄÅÊ∑∑ÂêàÂú∞ÂΩ¢‰∏äÁ®≥ÂÆöËøêÂä®Ôºâ„ÄÇ\n*   **Ê∂åÁé∞Ë°å‰∏∫**ÔºöÊéßÂà∂Âô®Ë°®Áé∞Âá∫**‰∏ªÂä®ÊÑüÁü•**ÔºàÈÄöËøá‰∫§‰∫íÊõ¥Êñ∞Âú∞ÂõæÔºâ„ÄÅ**Â±ÄÈÉ®ÂØºËà™**ÔºàÈÅøÈöú„ÄÅÂØπÈΩêÂú∞ÂΩ¢Ôºâ„ÄÅ**ÂÖ®Ë∫´Êé•Ëß¶**Ôºà‰ΩøÁî®ËÜùÁõñËæÖÂä©ÔºâÂíå**ÂÜ≤ÂáªÂáèÁºì**Á≠âÈ´òÁ∫ßÊô∫ËÉΩË°å‰∏∫„ÄÇ\n*   **ÂèØÈÉ®ÁΩ≤ÊÄß**ÔºöÊï¥‰∏™Á≥ªÁªüÔºàÂª∫Âõæ+ÊéßÂà∂ÔºâÂèØÂú®Êú∫ËΩΩËÆ°ÁÆóÊú∫‰∏äÂÆûÊó∂ËøêË°åÔºàÁ≠ñÁï•Êé®ÁêÜÁ∫¶2msÔºåÂª∫ÂõæÁ∫¶5ms/Â∏ßÔºâÔºåÊª°Ë∂≥‰∫ÜÁé∞ÂÆûÂ∫îÁî®ÁöÑÈúÄÊ±Ç„ÄÇ\n\n**ÁªìËÆ∫**ÔºöAME-2ÁöÑÊ†∏ÂøÉË¥°ÁåÆÂú®‰∫é**Á≥ªÁªüÊÄßÂú∞Êï¥Âêà‰∫ÜÂàõÊñ∞ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂„ÄÅ‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•Âª∫ÂõæÂíåÈ´òÊïàÁöÑ‰ªøÁúüÂà∞Áé∞ÂÆûËøÅÁßªÊñπÊ≥ï**ÔºåÈ¶ñÊ¨°Âú®Ë∂≥ÂºèÊú∫Âô®‰∫∫‰∏äÂÆûÁé∞‰∫Ü**ÊïèÊç∑ÊÄß‰∏éÊ≥õÂåñÊÄßÁöÑÈ´òÊ∞¥Âπ≥Áªü‰∏Ä**Ôºå‰∏∫Êú∫Âô®‰∫∫Âú®Â§çÊùÇÈùûÁªìÊûÑÂåñÁéØÂ¢É‰∏≠ÁöÑÂèØÈù†Ëá™‰∏ªËøêË°åÊèê‰æõ‰∫ÜÂº∫ÊúâÂäõÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ",
            "timestamp": "2026-01-20T17:51:17.396468",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "Âü∫‰∫é‰ªñÁöÑÂâç‰ΩúÔºåÊ¢≥ÁêÜËøô‰∏™ÊñπÂêëÁöÑÊï¥‰∏™ÂèëÂ±ïËÑâÁªúÔºåÊØè‰∏ÄÊ≠•Áõ∏ÊØî‰∫é‰πãÂâçÁöÑÂ∑•‰ΩúÈÉΩÊîπËøõ‰∫Ü‰ªÄ‰πàÔºåÁùÄÈáç‰∫éÂá†‰∏™‰∏çÂêåÁöÑÂèëÂ±ïÊñπÂêë„ÄÇ",
            "answer": "## ËÆ∫ÊñáÂàÜÊûêÔºöAME-2 ÂèäÂÖ∂ÊäÄÊúØËÑâÁªúÊ¢≥ÁêÜ\n\nÊú¨ÊñáÊèêÂá∫ÁöÑ **AME-2** ÊòØË∂≥ÂºèÊú∫Âô®‰∫∫ÊÑüÁü•-ËøêÂä®‰∏Ä‰ΩìÂåñÈ¢ÜÂüüÁöÑ‰∏ÄÈ°πÊúÄÊñ∞Â∑•‰Ωú„ÄÇË¶ÅÁêÜËß£ÂÖ∂ÂàõÊñ∞‰∏é‰ª∑ÂÄºÔºåÂøÖÈ°ªÂ∞ÜÂÖ∂ÁΩÆ‰∫éËØ•È¢ÜÂüüÁöÑÂèëÂ±ïËÑâÁªú‰∏≠„ÄÇÂÖ∂Ê†∏ÂøÉÂèëÂ±ïË∑ØÂæÑÂèØ‰ª•Ê¶ÇÊã¨‰∏∫Ôºö‰ªé**Âü∫‰∫éÊ®°ÂûãÁöÑÊ®°ÂùóÂåñÊñπÊ≥ï**ÔºåÂà∞**ÁªìÂêàÁªèÂÖ∏Âú∞ÂõæÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï**ÔºåÂÜçÂà∞**Á´ØÂà∞Á´ØÊÑüÁü•-ËøêÂä®Á≠ñÁï•**ÔºåÊúÄÁªàËµ∞Âêë **AME-2 ÊâÄ‰ª£Ë°®ÁöÑ‚ÄúÁªìÊûÑÂåñÁ´ØÂà∞Á´Ø‚ÄùÊàñ‚ÄúÁ•ûÁªèÊ®°ÂùóÂåñ‚ÄùÊñπÊ≥ï**„ÄÇ\n\n‰∏ãÂõæÊ¢≥ÁêÜ‰∫ÜËØ•È¢ÜÂüüÂá†‰∏™ÂÖ≥ÈîÆÂèëÂ±ïÊñπÂêëÂèäÂÖ∂ÊºîËøõÂÖ≥Á≥ªÔºö\n\n```mermaid\nflowchart TD\n    A[‚ÄúÊó©ÊúüÂ∑•‰Ωú<br>Âü∫‰∫éÊ®°ÂûãÁöÑÊ®°ÂùóÂåñÊñπÊ≥ï‚Äù] --> B[‚ÄúÊñπÂêë‰∏ÄÔºöÁªìÂêàÊòæÂºèÂú∞ÂõæÁöÑRL‚Äù]\n    B --> B1[‚Äú‰ΩøÁî®ÁªèÂÖ∏Âú∞ÂõæÊ†à<br>ÔºàÂ¶ÇElevation MappingÔºâ‚Äù]\n    B1 --> B2[‚Äú‰ΩøÁî®Â≠¶‰π†ÂºèÂú∞Âõæ<br>ÔºàÁ•ûÁªèÈáçÂª∫Ôºâ‚Äù]\n    B2 --> E[‚ÄúAME-2<br>ÔºàÁ•ûÁªèÂú∞Âõæ+Ê≥®ÊÑèÂäõÁºñÁ†ÅÔºâ‚Äù]\n    \n    A --> C[‚ÄúÊñπÂêë‰∫åÔºöÁ´ØÂà∞Á´ØÊÑüÁü•-ËøêÂä®Á≠ñÁï•‚Äù]\n    C --> C1[‚Äú‰ªéÂéüÂßã‰º†ÊÑüÂô®Âà∞Âä®‰Ωú<br>ÔºàÂ¶ÇËßÜËßâ„ÄÅÊøÄÂÖâÔºâ‚Äù]\n    C1 --> C2[‚ÄúÁâπÊùÉ-Â≠¶ÁîüËí∏È¶èÊ°ÜÊû∂<br>ÔºàÂ¶ÇParkour in the WildÔºâ‚Äù]\n    C2 --> E\n    \n    B --> D[‚ÄúÊñπÂêë‰∏âÔºöÂ≠¶‰π†ÂºèÂú∞ÂõæÁîüÊàê‚Äù]\n    D --> D1[‚Äú‰∏∫ÊéßÂà∂ÂÆöÂà∂ÁöÑÁ•ûÁªèÈáçÂª∫‚Äù]\n    D1 --> D2[‚Äú‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÁöÑÁ•ûÁªèËøáÁ®ã‚Äù]\n    D2 --> E\n    \n    E --> F[‚ÄúÊ†∏ÂøÉÂàõÊñ∞‰∏éÊï¥Âêà<br>1. AME-2Ê≥®ÊÑèÂäõÁºñÁ†ÅÂô®<br>2. ËΩªÈáè‰∏çÁ°ÆÂÆöÊÄßÁ•ûÁªèÂú∞Âõæ<br>3. Áªü‰∏ÄËÆ≠ÁªÉÊ°ÜÊû∂‚Äù]\n```\n\n### ÂèëÂ±ïËÑâÁªú‰∏éÊ†∏ÂøÉÊîπËøõ\n\n#### ÊñπÂêë‰∏ÄÔºöÁªìÂêàÊòæÂºèÂú∞ÂõæÁöÑÂº∫ÂåñÂ≠¶‰π† (RL with Explicit Maps)\nÊ≠§ÊñπÂêëÊó®Âú®‰øùÁïôÂèØËß£ÈáäÁöÑÂú∞ÂõæË°®ÂæÅÔºåÂêåÊó∂Âà©Áî®RLÂÖãÊúçÊ®°Âûã‰∏çÂåπÈÖçÂíåËßÑÂàíËÆ°ÁÆóÁì∂È¢à„ÄÇ\n\n1.  **Êó©ÊúüÁªìÂêà (Â¶ÇRLOC, DTC)**Ôºö\n    *   **ÂÅöÊ≥ï**ÔºöÂ∞ÜRL‰Ωú‰∏∫Ê®°ÂùóÂµåÂÖ•ÁªèÂÖ∏ÁÆ°Á∫øÔºàÂ¶ÇÁî®‰∫éËΩ®ËøπË∑üË∏™ÊàñËßÑÂàíÔºâÔºåÊàñ‰ΩøÁî®ÁªèÂÖ∏ËßÑÂàíÂô®Êù•Ë∑üË∏™RLËæìÂá∫ÁöÑËΩ®Ëøπ„ÄÇ\n    *   **ÊîπËøõ**ÔºöÁõ∏ÊØîÁ∫ØÊ®°ÂûãÊñπÊ≥ïÔºåÊèêÂçá‰∫ÜÈ≤ÅÊ£íÊÄß„ÄÇ\n    *   **Â±ÄÈôê**ÔºöÊïèÊç∑ÊÄßÂèóÈôê‰∫éÂ∫ïÂ±ÇÊ®°ÂûãÂÅáËÆæÂíåËßÑÂàíÂô®ËÆ°ÁÆóÈÄüÂ∫¶ÔºõÊ≥õÂåñËÉΩÂäõÂèóÂú∞ÂõæË¥®ÈáèÂΩ±ÂìçÂ§ß„ÄÇ\n\n2.  **‰ΩøÁî®Âú®Á∫øÁªèÂÖ∏Âú∞ÂõæÊ†à (Â¶ÇAnymal Robust, Attention-based Map Encoding [AME-1])**Ôºö\n    *   **ÂÅöÊ≥ï**Ôºö‰ΩøÁî®ÂÆûÊó∂ÊûÑÂª∫ÁöÑÁªèÂÖ∏È´òÁ®ãÂõæÔºàElevation MapÔºâ‰Ωú‰∏∫RLÁ≠ñÁï•ÁöÑËæìÂÖ•„ÄÇ\n    *   **ÊîπËøõ**ÔºöÂÆûÁé∞‰∫ÜÊõ¥Âº∫ÁöÑÂú∞ÂΩ¢Ê≥õÂåñËÉΩÂäõÔºàAME-1ÔºâÔºåÂõ†‰∏∫Á≠ñÁï•Â≠¶‰π†Áõ¥Êé•Â§ÑÁêÜÂá†‰Ωï‰ø°ÊÅØ„ÄÇ\n    *   **Â±ÄÈôê**ÔºöÂú∞ÂõæÊõ¥Êñ∞È¢ëÁéáÂíåÂêØÂèëÂºèÊª§Ê≥¢ÂèØËÉΩÈôêÂà∂ÊïèÊç∑ÊÄßÔºõÂØπÈÅÆÊå°ÂíåÂô™Â£∞Â§ÑÁêÜËÉΩÂäõÊúâÈôêÔºõÂú∞ÂõæÁÆ°Á∫øËÆ°ÁÆóÊàêÊú¨È´ò„ÄÇ\n\n3.  **‰ΩøÁî®Â≠¶‰π†ÂºèÂú∞ÂõæÈáçÂª∫ (Â¶ÇAnymal Parkour, DPL)**Ôºö\n    *   **ÂÅöÊ≥ï**ÔºöÁî®Á•ûÁªèÁΩëÁªú‰ªé‰º†ÊÑüÂô®Êï∞ÊçÆÂÆûÊó∂ÈáçÂª∫Áî®‰∫éÊéßÂà∂ÁöÑËá™‰∏≠ÂøÉÂú∞Âõæ„ÄÇ\n    *   **ÊîπËøõ**ÔºöÂú∞ÂõæÁîüÊàêÊõ¥Âø´ÔºåÊîØÊåÅÈ´òÊïèÊç∑ÊÄßËøêÂä®ÔºàÂ¶ÇË∑ëÈÖ∑ÔºâÔºõÈÄöËøáÊ®°ÊãüÊï∞ÊçÆËÆ≠ÁªÉÔºåÁÆÄÂåñ‰∫Ü‰ªøÁúüÂà∞Áé∞ÂÆûÁöÑËΩ¨Áßª„ÄÇ\n    *   **Â±ÄÈôê**ÔºöÈáçÂª∫ÁΩëÁªúÂÆπÊòìËøáÊãüÂêàÂà∞ËÆ≠ÁªÉÂú∞ÂΩ¢ÂàÜÂ∏ÉÔºå**Âú®Êú™ËßÅËøáÁöÑ„ÄÅÈùûÁªìÊûÑÂåñÂú∞ÂΩ¢‰∏äÊ≥õÂåñËÉΩÂäõÊúâÈôê**ÔºõÈÄöÂ∏∏‰∏çÊòæÂºèÂª∫Ê®°‰∏çÁ°ÆÂÆöÊÄß„ÄÇ\n\n#### ÊñπÂêë‰∫åÔºöÁ´ØÂà∞Á´ØÊÑüÁü•-ËøêÂä®Á≠ñÁï• (End-to-End Sensorimotor Policies)\nÊ≠§ÊñπÂêëËøΩÊ±ÇÊûÅËá¥ÁöÑÊïèÊç∑ÊÄßÂíåÁ≥ªÁªüÁÆÄÊ¥ÅÊÄßÔºåÁªïËøáÊòæÂºèÂú∞ÂõæÊûÑÂª∫„ÄÇ\n\n1.  **ÂéüÂßã‰º†ÊÑüÂô®Âà∞Âä®‰Ωú (Â¶ÇVision-Locomotion, Extreme Parkour)**Ôºö\n    *   **ÂÅöÊ≥ï**ÔºöÂçï‰∏™Á•ûÁªèÁΩëÁªúÁõ¥Êé•‰ªéÂéüÂßãËßÜËßâÔºàRGB-DÔºâÊàñÊøÄÂÖâÈõ∑ËææÊï∞ÊçÆ‰∏éÊú¨‰ΩìÊÑüÁü•Êò†Â∞ÑÂà∞ÂÖ≥ËäÇÂä®‰Ωú„ÄÇ\n    *   **ÊîπËøõ**ÔºöÂú®ÁâπÂÆöÂ§çÊùÇÂú∞ÂΩ¢ÔºàÂ¶ÇË∑ëÈÖ∑ËµõÈÅìÔºâ‰∏äÂ±ïÁ§∫‰∫ÜÊûÅÈ´òÁöÑÊïèÊç∑ÊÄß„ÄÇ\n    *   **Â±ÄÈôê**ÔºöÁ≠ñÁï•ÂÉè‰∏Ä‰∏™ÈªëÁõíÔºå**ÂèØËß£ÈáäÊÄßÂ∑Æ**ÔºõÂ≠¶‰π†Âà∞ÁöÑÂú∞ÂΩ¢Êé®ÁêÜÈöêÂºè‰∏î‰∏çÊòéÁ°ÆÔºåÂØºËá¥**Ê≥õÂåñËÉΩÂäõ‰∏•ÈáçÂèóÈôê**ÔºåÂÆπÊòìËøáÊãüÂêàËÆ≠ÁªÉÁéØÂ¢É„ÄÇ\n\n2.  **ÁâπÊùÉ-Â≠¶ÁîüËí∏È¶èÊ°ÜÊû∂ (Â¶ÇParkour in the Wild)**Ôºö\n    *   **ÂÅöÊ≥ï**ÔºöÂÖàËÆ≠ÁªÉÂ§ö‰∏™‰ΩøÁî®ÁâπÊùÉ‰ø°ÊÅØÔºàÂ¶ÇÁúüÂÄºÂú∞ÂõæÔºâÁöÑ‚ÄúÊïôÂ∏à‚ÄùÁ≠ñÁï•ÔºåÂÜçÂ∞ÜÂÖ∂Ëí∏È¶èÂà∞‰∏Ä‰∏™‰ªéÂéüÂßã‰º†ÊÑüÂô®ËæìÂÖ•Êìç‰ΩúÁöÑ‚ÄúÂ≠¶Áîü‚ÄùÁ≠ñÁï•‰∏≠„ÄÇ\n    *   **ÊîπËøõ**ÔºöÊï¥Âêà‰∫ÜÂ§ö‰∏™ÊïôÂ∏àÁöÑÊäÄËÉΩÔºåÂæóÂà∞‰∏Ä‰∏™Áõ∏ÂØπÈÄöÁî®ÁöÑ‚ÄúÈÄöÊâç‚ÄùÁ≠ñÁï•ÔºõÂæÆË∞ÉÊïàÁéáÈ´ò„ÄÇ\n    *   **Â±ÄÈôê**ÔºöËÆ∫ÊñáÊåáÂá∫ÔºåÂÖ∂**Èõ∂Ê†∑Êú¨Ê≥õÂåñÂà∞ÂÖ®Êñ∞Âú∞ÂΩ¢ÁöÑËÉΩÂäõ‰ªçÁÑ∂ÊúâÈôê**ÔºõÂ≠¶ÁîüÁ≠ñÁï•‰ªçÂ±ûÁ´ØÂà∞Á´ØÈªëÁÆ±ÔºåÂú∞ÂΩ¢Ë°®ÂæÅËÉΩÂäõ‰∏çÂ¶ÇÊòæÂºèÂú∞Âõæ„ÄÇ\n\n#### ÊñπÂêë‰∏âÔºöÂ≠¶‰π†ÂºèÂú∞ÂõæÁîüÊàê (Learning-Based Mapping)\nÊ≠§ÊñπÂêëÁã¨Á´ã‰∫éÊéßÂà∂Á≠ñÁï•ÂèëÂ±ïÔºåÊó®Âú®‰∏∫Êú∫Âô®‰∫∫Êèê‰æõÊõ¥Â•ΩÁöÑÁéØÂ¢ÉË°®ÂæÅ„ÄÇ\n\n1.  **‰∏∫ÊéßÂà∂ÂÆöÂà∂ÁöÑÁ•ûÁªèÈáçÂª∫**Ôºö\n    *   **ÂÅöÊ≥ï**ÔºöËÆ≠ÁªÉËΩªÈáèÁΩëÁªúÂø´ÈÄüÈáçÂª∫Â±ÄÈÉ®Âú∞ÂΩ¢Âá†‰ΩïÔºå‰Ωú‰∏∫ÊéßÂà∂ËæìÂÖ•„ÄÇ\n    *   **ÊîπËøõ**ÔºöÈÄüÂ∫¶Âø´ÔºåÈÄÇÂêàÂÆûÊó∂ÊéßÂà∂„ÄÇ\n    *   **Â±ÄÈôê**ÔºöÊ≥õÂåñÊÄß‰∏çË∂≥Ôºå‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°‰∏çÊòéÁ°Æ„ÄÇ\n\n2.  **‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÁöÑÁ•ûÁªèËøáÁ®ã (Â¶ÇNeural Processes for Elevation Modeling)**Ôºö\n    *   **ÂÅöÊ≥ï**ÔºöÂà©Áî®Á•ûÁªèËøáÁ®ãÁ≠âÊ®°ÂûãÔºåÈ¢ÑÊµãÊØèÂ∏ßÂú∞ÂΩ¢È´òÁ®ãÂèäÂÖ∂‰∏çÁ°ÆÂÆöÊÄßÔºåÂÜçËøõË°åÊó∂Â∫èËûçÂêà„ÄÇ\n    *   **ÊîπËøõ**Ôºö**ÊòæÂºèËæìÂá∫‰∏çÁ°ÆÂÆöÊÄß**ÔºåËÉΩÊõ¥Â•ΩÂ§ÑÁêÜÂô™Â£∞ÂíåÈÅÆÊå°ÔºõÊï∞ÊçÆÊïàÁéáÊõ¥È´ò„ÄÇ\n    *   **Â±ÄÈôê**ÔºöÊ®°ÂûãËÆ°ÁÆóÂºÄÈîÄÂ§ßÔºåÈöæ‰ª•ÈõÜÊàêÂà∞ÈúÄË¶ÅÊï∞ÂçÉ‰∏™Âπ∂Ë°åÁéØÂ¢ÉÁöÑÂ§ßËßÑÊ®°RLËÆ≠ÁªÉ‰∏≠„ÄÇ\n\n---\n\n### **AME-2 ÁöÑÊ†∏ÂøÉÊï¥Âêà‰∏éÂàõÊñ∞**\n\nAME-2 Âπ∂ÈùûÁÆÄÂçïÂª∂Áª≠Êüê‰∏™ÊñπÂêëÔºåËÄåÊòØÂØπ‰∏äËø∞Â§ö‰∏™ÊñπÂêë‰ºòÁÇπÁöÑ**ÂàõÈÄ†ÊÄßÊï¥Âêà‰∏éÁ™ÅÁ†¥**ÔºåÂÖ∂ÊîπËøõ‰ΩìÁé∞Âú®Ôºö\n\n1.  **ÂàõÊñ∞ÁöÑÁ≠ñÁï•ÁºñÁ†ÅÂô® (AME-2 Encoder)**Ôºö\n    *   **Áõ∏ÊØî AME-1**ÔºöAME-1 ‰ªÖÂü∫‰∫éÊú¨‰ΩìÊÑüÁü•‰ø°ÊÅØÂØπÂ±ÄÈÉ®Âú∞ÂõæÁâπÂæÅËøõË°åÊ≥®ÊÑèÂäõÂä†ÊùÉ„ÄÇAME-2 **ÂºïÂÖ•‰∫ÜÂÖ®Â±ÄÂú∞ÂΩ¢ÁâπÂæÅ**ÔºåÂπ∂ËÅîÂêàÂÖ®Â±ÄÁâπÂæÅÂíåÊú¨‰ΩìÊÑüÁü•Êù•ÁîüÊàêÊ≥®ÊÑèÂäõÊùÉÈáç„ÄÇËøô‰ΩøÂæóÁ≠ñÁï•ËÉΩÊ†πÊçÆ**Âú∞ÂΩ¢ÂÖ®Â±Ä‰∏ä‰∏ãÊñá**Âä®ÊÄÅË∞ÉÊï¥ÂÖ≥Ê≥®ÁÇπÔºåÂ≠¶‰π†‰∏çÂêåÂú∞ÂΩ¢‰∏ãÁöÑÂ∑ÆÂºÇÂåñËøêÂä®Ê®°ÂºèÔºå‰ªéËÄåÂÆûÁé∞‰∫Ü**Ë¥®ÁöÑÊ≥õÂåñËÉΩÂäõÊèêÂçá**ÔºàËßÅË°®IIÔºåÂú®Ê∑∑ÂêàÂú∞ÂΩ¢‰∏äËøúË∂ÖAME-1Ôºâ„ÄÇ\n\n2.  **ËΩªÈáè‰∏î‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÁöÑÁ•ûÁªèÂú∞ÂõæÁÆ°Á∫ø**Ôºö\n    *   **Áõ∏ÊØîÁªèÂÖ∏Âú∞ÂõæÊ†à**ÔºöÈÄüÂ∫¶Êõ¥Âø´Ôºå‰∏ì‰∏∫ÊïèÊç∑ÊéßÂà∂‰ºòÂåñÔºõÈÄöËøáË¥ùÂè∂ÊñØÂ≠¶‰π†ÊòæÂºèËæìÂá∫ÊØèÂçïÂÖÉÊ†ºÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºåËá™ÁÑ∂Âª∫Ê®°‰∫ÜÈÅÆÊå°ÂíåÂô™Â£∞„ÄÇ\n    *   **Áõ∏ÊØî‰πãÂâçÁöÑÂ≠¶‰π†ÂºèÂú∞Âõæ**ÔºöÈÄöËøá**Ê¶ÇÁéáËµ¢ÂÆ∂ÈÄöÂêÉËûçÂêàÁ≠ñÁï•**Âíå**Âú®ÂπøÈòîÂú∞ÂΩ¢ÂàÜÂ∏ÉÔºàÂåÖÊã¨‰∏çÂèØÈÄöË°åÂú∞ÂΩ¢Ôºâ‰∏äÁöÑËÆ≠ÁªÉ**ÔºåËé∑Âæó‰∫ÜÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÂÆÉ‰∏çÂÉèÊüê‰∫õÈáçÂª∫ÁΩëÁªúÈÇ£Ê†∑‚ÄúÊÉ≥Ë±°‚ÄùÊàñË°•ÂÖ®ÈÅÆÊå°Âå∫ÂüüÔºåËÄåÊòØ‰øùÊåÅÂÖ∂È´ò‰∏çÁ°ÆÂÆöÊÄßÔºåËøôÊõ¥ÈÄÇÂêàÈúÄË¶ÅÁ≤æÁ°ÆÂá†‰ΩïÁöÑÊïèÊç∑ËøêÂä®„ÄÇ\n    *   **Áõ∏ÊØîÈáçÂûãÁ•ûÁªèËøáÁ®ãÊ®°Âûã**ÔºöÊûÅÂ∫¶ËΩªÈáèÂåñÔºåÂèØÂú®Â§ßËßÑÊ®°Âπ∂Ë°å‰ªøÁúü‰∏≠ËøêË°åÔºåÂÆûÁé∞‰∫ÜËÆ≠ÁªÉ‰∏éÈÉ®ÁΩ≤ÁÆ°Á∫øÁöÑÁªü‰∏Ä„ÄÇ\n\n3.  **Áªü‰∏ÄÁöÑËÆ≠ÁªÉ‰∏éËøÅÁßªÊ°ÜÊû∂**Ôºö\n    *   **ÈááÁî®‚ÄúÊïôÂ∏à-Â≠¶Áîü‚ÄùÊ°ÜÊû∂**Ôºå‰ΩÜ**Â≠¶ÁîüÁ≠ñÁï•Âπ∂ÈùûÁ´ØÂà∞Á´ØÈªëÁÆ±**ÔºåËÄåÊòØ‰ΩøÁî®‰∏éÊïôÂ∏àÁõ∏ÂêåÁöÑ„ÄÅÂèØËß£ÈáäÁöÑAME-2Êû∂ÊûÑÔºåËæìÂÖ•ÊòØÁ•ûÁªèÂú∞Âõæ‰∫ßÁîüÁöÑÂ∏¶‰∏çÁ°ÆÂÆöÊÄßÁöÑÈ´òÁ®ãÂõæ„ÄÇ\n    *   **ËÆ≠ÁªÉÊó∂**ÔºåÂú®‰ªøÁúü‰∏≠ËøêË°åÂÆåÊï¥ÁöÑÁ•ûÁªèÂú∞ÂõæÁÆ°Á∫øÔºåËÄåÈùû‰ΩøÁî®ÁêÜÊÉ≥Âú∞ÂõæÔºåÊûÅÂ§ßÁº©Â∞è‰∫Ü‰ªøÁúüÂà∞Áé∞ÂÆûÁöÑÂ∑ÆË∑ù„ÄÇ\n    *   **ÊçüÂ§±ÂáΩÊï∞**ÁªìÂêà‰∫ÜRLÊçüÂ§±„ÄÅÊïôÂ∏àÂä®‰ΩúËí∏È¶èÊçüÂ§±Âíå**Ë°®ÂæÅÂØπÈΩêÊçüÂ§±**ÔºàËÆ©Â≠¶ÁîüÂíåÊïôÂ∏àÁöÑÂú∞ÂõæÂµåÂÖ•Áõ∏‰ººÔºâÔºåËøôÁ®≥ÂÆö‰∫ÜËÆ≠ÁªÉÂπ∂‰øùÊåÅ‰∫ÜÊ≥õÂåñËÉΩÂäõÔºàËßÅË°®IIIÔºåÂÖ∂Â≠¶ÁîüÁ≠ñÁï•Âú®Êú™ËßÅÂú∞ÂΩ¢‰∏äÊòæËëó‰ºò‰∫éÁ´ØÂà∞Á´ØËßÜËßâÂæ™ÁéØÂ≠¶ÁîüÁ≠ñÁï•Ôºâ„ÄÇ\n\n### ÊÄªÁªì\n\nAME-2 ÁöÑÂèëÂ±ïËÑâÁªúË°®ÊòéÔºåË∂≥ÂºèÊú∫Âô®‰∫∫ÊÑüÁü•ËøêÂä®ÊéßÂà∂Ê≠£‰ªé **‚ÄúÊ®°ÂùóÂåñ‚Äù** ‰∏é **‚ÄúÁ´ØÂà∞Á´Ø‚Äù** ÁöÑ‰∫åÂÖÉÂØπÁ´ãÔºåËµ∞Âêë‰∏ÄÊù° **‚ÄúÁ•ûÁªèÊ®°ÂùóÂåñ‚Äù** Êàñ **‚ÄúÁªìÊûÑÂåñÁ´ØÂà∞Á´Ø‚Äù** ÁöÑ‰∏≠Èó¥ÈÅìË∑Ø„ÄÇÂÆÉ‰øùÁïô‰∫ÜÂú∞ÂõæËøô‰∏ÄÂèØËß£ÈáäÁöÑ‰∏≠Èó¥Ë°®ÂæÅÔºå‰ΩÜÁî®Â≠¶‰π†Âà∞ÁöÑ„ÄÅËΩªÈáèÁöÑ„ÄÅ‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÁöÑÊ®°ÂùóÊù•ÁîüÊàêÂÆÉÔºõÂÆÉ‰ΩøÁî®Á´ØÂà∞Á´ØRLËÆ≠ÁªÉÊéßÂà∂Âô®Ôºå‰ΩÜÈÄöËøáÂàõÊñ∞ÁöÑÊ≥®ÊÑèÂäõÊû∂ÊûÑ‰ΩøÁ≠ñÁï•ÂÖ∑Â§áÂú∞ÂΩ¢‰∏ä‰∏ãÊñáÁêÜËß£ËÉΩÂäõ„ÄÇÊúÄÁªàÔºåAME-2 Âú®**ÊïèÊç∑ÊÄß**ÔºàÂ™≤ÁæéÈ°∂Â∞ñÁ´ØÂà∞Á´ØÊñπÊ≥ïÔºâÂíå**Èõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõ**ÔºàËøúË∂Ö‰ª•ÂæÄÊñπÊ≥ïÔºâ‰πãÈó¥ÂèñÂæó‰∫ÜÁ™ÅÁ†¥ÊÄßÁöÑÂπ≥Ë°°Ôºå‰∏∫Ë∂≥ÂºèÊú∫Âô®‰∫∫Âú®Â§çÊùÇ„ÄÅÊú™Áü•Áé∞ÂÆûÁéØÂ¢É‰∏≠ÁöÑÂèØÈù†ÈÉ®ÁΩ≤Êèê‰æõ‰∫ÜÂº∫ÊúâÂäõÁöÑÊ°ÜÊû∂„ÄÇ",
            "timestamp": "2026-01-20T17:52:21.013507",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "‰ªñÁöÑÂâç‰ΩúÊúâÂì™‰∫õÔºü‰ΩøÁî®Ë°®Ê†º‰ªîÁªÜËÆ≤ËÆ≤‰ªñÁöÑÊØèÁØáÂâç‰ΩúÔºå‰ªñÂíåÂâç‰ΩúÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºå‰∏ªË¶ÅÊîπÂñÑÊòØ‰ªÄ‰πàÔºüÁùÄÈáç‰∫éÂÖ∑‰ΩìÁõ∏ÊØî‰∫é‰πãÂâçÊñáÁ´†ÁöÑÊîπÂä®",
            "answer": "## AME-2 ÁöÑÂâç‰ΩúÂàÜÊûê\n\n‰ΩúËÄÖÂõ¢ÈòüÔºàÁâπÂà´ÊòØ Chong Zhang Á≠â‰∫∫ÔºâÂú®Ë∂≥ÂºèÊú∫Âô®‰∫∫ÊÑüÁü•‰∏éÂº∫ÂåñÂ≠¶‰π†ÊéßÂà∂È¢ÜÂüüÊúâ‰∏ÄÁ≥ªÂàóÈáçË¶ÅÂ∑•‰Ωú„ÄÇ‰ª•‰∏ãÊòØÂÖ∂Ê†∏ÂøÉÂâç‰ΩúÁöÑËØ¶ÁªÜÊ¢≥ÁêÜÔºå‰ª•Âèä‰∏é AME-2 ÁöÑ‰∏ªË¶ÅÂå∫Âà´ÂíåÊîπËøõ„ÄÇ\n\n### Ââç‰ΩúÊ¶ÇËßàË°®\n\n| ËÆ∫ÊñáÁÆÄÁß∞/Ê†áÈ¢ò | ÂèëË°®Âπ¥‰ªΩ/‰ºöËÆÆ | Ê†∏ÂøÉË¥°ÁåÆ | ‰∏é AME-2 ÁöÑ‰∏ªË¶ÅÂå∫Âà´ | AME-2 ÁöÑ‰∏ªË¶ÅÊîπËøõ |\n| :--- | :--- | :--- | :--- | :--- |\n| **AME-1** (Attention-based Map Encoding) | 2025, Science Robotics | ÊèêÂá∫‰∫ÜÈ¶ñ‰∏™**Âü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÂú∞ÂõæÁºñÁ†ÅÂô®**ÔºåÁî®‰∫éÂ≠¶‰π†**Ê≥õÂåñÁöÑË∂≥ÂºèËøêÂä®**„ÄÇÊéßÂà∂Âô®‰ΩøÁî®È´òÁ®ãÂõæ‰Ωú‰∏∫ËæìÂÖ•ÔºåÈÄöËøáÊ≥®ÊÑèÂäõÊú∫Âà∂ËÅöÁÑ¶‰∫éÂ±ÄÈÉ®Âú∞ÂΩ¢ÁâπÂæÅ„ÄÇ | 1. **ÁºñÁ†ÅÂô®ËÆæËÆ°**ÔºöAME-1 ÁöÑÊ≥®ÊÑèÂäõÊùÉÈáç‰ªÖÂü∫‰∫é**Êú¨‰ΩìÊÑüÁü•**ÔºàproprioceptionÔºâËÆ°ÁÆóÔºåÁº∫‰πèÂÖ®Â±ÄÂú∞ÂΩ¢‰∏ä‰∏ãÊñá„ÄÇ<br>2. **Ê≥õÂåñËÉΩÂäõ**ÔºöÂú®Ê∑∑ÂêàÂ§çÊùÇÂú∞ÂΩ¢ÔºàÂ¶ÇÁ®ÄÁñè‰∏éÊîÄÁà¨Âú∞ÂΩ¢ÁªÑÂêàÔºâ‰∏äÊ≥õÂåñËÉΩÂäõÊúâÈôê„ÄÇ<br>3. **ÊïèÊç∑ÊÄß**Ôºö‰æùËµñ‰∫éÁªèÂÖ∏Âú®Á∫øÂª∫ÂõæÊ†àÔºåÊõ¥Êñ∞ÈÄüÁéáÂíåÂª∫ÂõæË¥®ÈáèÈôêÂà∂‰∫ÜÊïèÊç∑ÊÄß„ÄÇ<br>4. **Âª∫ÂõæÊ®°Âùó**ÔºöÊú™ÈõÜÊàêÂ≠¶‰π†ÂûãÂª∫ÂõæÔºå‰æùËµñ‰º†ÁªüÂá†‰ΩïÂª∫Âõæ„ÄÇ | 1. **ÁºñÁ†ÅÂô®ÂçáÁ∫ß**ÔºöÂºïÂÖ•**ÂÖ®Â±ÄÁâπÂæÅ**ÔºåÂπ∂‰∏éÊú¨‰ΩìÊÑüÁü•ÂÖ±ÂêåÁîüÊàêÊ≥®ÊÑèÂäõÊùÉÈáçÔºå‰ΩøÁ≠ñÁï•ËÉΩÊ†πÊçÆ**ÂÖ®Â±ÄÂú∞ÂΩ¢‰∏ä‰∏ãÊñá**Ë∞ÉÊï¥ËøêÂä®Ê®°ÂºèÔºåÊòæËëóÊèêÂçáÂú®**Ê∑∑Âêà/Êú™ËßÅÂú∞ÂΩ¢**ÁöÑÊ≥õÂåñËÉΩÂäõÔºàÊµãËØïÂú∞ÂΩ¢Âπ≥ÂùáÊàêÂäüÁéá‰ªé51.2%ÊèêÂçáËá≥95.2%Ôºâ„ÄÇ<br>2. **Áªü‰∏ÄÊ°ÜÊû∂**ÔºöÂ∞Ü**Â≠¶‰π†Âûã„ÄÅËΩªÈáèÂåñ„ÄÅ‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÁöÑÁ•ûÁªèÂª∫ÂõæÁÆ°ÈÅì**‰∏éÊéßÂà∂Âô®ËÆ≠ÁªÉÊ∑±Â∫¶ÈõÜÊàêÔºåÊëÜËÑ±ÂØπ‰º†ÁªüÂª∫ÂõæÊ†àÁöÑ‰æùËµñ„ÄÇ<br>3. **ÊïèÊç∑ÊÄßÊèêÂçá**ÔºöÂæóÁõä‰∫éÂø´ÈÄüÁ•ûÁªèÂª∫ÂõæÔºåÂÆûÁé∞‰∫Ü‰∏éÊúÄÂÖàËøõÁ´ØÂà∞Á´ØÁ≠ñÁï•Áõ∏ÂΩìÁöÑÊïèÊç∑ÊÄßÔºàÂ¶ÇÈõ∂Ê†∑Êú¨ÈÄöËøáÈ´òÈöæÂ∫¶Ë∑ëÈÖ∑Âú∞ÂΩ¢Ôºâ„ÄÇ<br>4. **Á≥ªÁªüÊï¥Âêà**ÔºöÈááÁî®**ÊïôÂ∏à-Â≠¶ÁîüËÆ≠ÁªÉÊñπÊ°à**Ôºå‰ΩøÊúÄÁªàÂèØÈÉ®ÁΩ≤ÁöÑÂ≠¶ÁîüÁ≠ñÁï•ËÉΩÁõ¥Êé•‰ΩøÁî®Âú®Á∫øÁ•ûÁªèÂª∫ÂõæÔºåÂÆûÁé∞‰∫Ü**ÊïèÊç∑ÊÄß„ÄÅÊ≥õÂåñÊÄß‰∏éÂèØÈÉ®ÁΩ≤ÊÄß**ÁöÑÁªü‰∏Ä„ÄÇ |\n| **Learning Agile Locomotion on Risky Terrains** | 2024, IROS | ‰∏ìÊ≥®‰∫éÂú®**È´òÈ£éÈô©Á®ÄÁñèËêΩËÑöÁÇπÂú∞ÂΩ¢**ÔºàÂ¶ÇÁ™ÑÊ¢Å„ÄÅË∏èËÑöÁü≥Ôºâ‰∏äÂ≠¶‰π†ÊïèÊç∑ËøêÂä®„ÄÇ‰ΩøÁî®**ÁõÆÊ†áÂà∞Ëææ**Ôºàgoal-reachingÔºâÂ•ñÂä±ÂÖ¨ÂºèÊù•ÊøÄÂä±Â§çÊùÇÊäÄËÉΩ„ÄÇ | 1. **ÊÑüÁü•ËæìÂÖ•**Ôºö‰æùËµñ‰∫é**ÁâπÊùÉ‰ø°ÊÅØ**Ôºàground-truthÂú∞ÂõæÔºâÊàñÁªèÂÖ∏Âª∫ÂõæÔºåÊú™Ëß£ÂÜ≥ÁúüÂÆû‰∏ñÁïåÊÑüÁü•ÁöÑ‰∏çÁ°ÆÂÆöÊÄß‰∏éÈÅÆÊå°ÈóÆÈ¢ò„ÄÇ<br>2. **Ê≥õÂåñËåÉÂõ¥**Ôºö‰∏ªË¶ÅÈíàÂØπÁ®ÄÁñèÂú∞ÂΩ¢ÔºåÊú™Á≥ªÁªüËß£ÂÜ≥Âú®ÂØÜÈõÜ„ÄÅÊîÄÁà¨Á≠âÂ§öÁßçÂú∞ÂΩ¢Á±ªÂà´Èó¥ÁöÑÊ≥õÂåñ„ÄÇ<br>3. **Êû∂ÊûÑ**ÔºöÊú™ÈááÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂ËøõË°åÂú∞ÂõæÁºñÁ†Å„ÄÇ | 1. **ÊÑüÁü•È≤ÅÊ£íÊÄß**ÔºöÊèêÂá∫‰∫ÜÂÆåÊï¥ÁöÑ**‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•Á•ûÁªèÂª∫ÂõæÁÆ°ÈÅì**ÔºåÊòæÂºèÂª∫Ê®°ÈÅÆÊå°ÂíåÂô™Â£∞ÔºåËß£ÂÜ≥‰∫Ü‰ªé‰ªøÁúüÂà∞ÁúüÂÆûÁöÑÂÖ≥ÈîÆÂ∑ÆË∑ù„ÄÇ<br>2. **Ê≥õÂåñÊâ©Â±ï**ÔºöÂ∞ÜËÆ≠ÁªÉÂíåËØÑ‰º∞Âú∞ÂΩ¢Êâ©Â±ïÂà∞‰∏âÂ§ßÁ±ªÔºàÂØÜÈõÜ„ÄÅÊîÄÁà¨„ÄÅÁ®ÄÁñèÔºâÂèäÂÖ∂ÁªÑÂêàÔºåËØÅÊòé‰∫Ü**Ë∑®Á±ªÂà´Ê≥õÂåñ**ËÉΩÂäõ„ÄÇ<br>3. **ÁºñÁ†ÅÂô®Â¢ûÂº∫**ÔºöÂú®AME-1ÁöÑÂü∫Á°Ä‰∏äÔºåÈÄöËøáÂºïÂÖ•ÂÖ®Â±Ä‰∏ä‰∏ãÊñáËøõ‰∏ÄÊ≠•‰ºòÂåñ‰∫ÜÂú∞ÂõæÁºñÁ†ÅÔºåÂΩ¢Êàê‰∫ÜÊõ¥Âº∫Â§ßÁöÑAME-2ÁºñÁ†ÅÂô®„ÄÇ |\n| **Anymal Parkour** | 2024, Science Robotics | ÂÆûÁé∞‰∫ÜÂõõË∂≥Êú∫Âô®‰∫∫ÁöÑ**Ë∑ëÈÖ∑**ËÉΩÂäõÔºåÂ±ïÁ§∫‰∫ÜÈ´òÂ∫¶ÁöÑÊïèÊç∑ÊÄß„ÄÇÈááÁî®‰∫Ü**Â≠¶‰π†ÂûãÂú∫ÊôØË°®Á§∫**ÔºàÁ•ûÁªèÂª∫ÂõæÔºâÊù•ÈáçÂª∫‰ª•Ëá™Êàë‰∏∫‰∏≠ÂøÉÁöÑÂú∞ÂΩ¢„ÄÇ | 1. **Âª∫ÂõæÊÄßË¥®**ÔºöÂÖ∂Â≠¶‰π†ÂûãÂª∫ÂõæÊòØ‰∏∫ÁâπÂÆöÂú∞ÂΩ¢ÂàÜÂ∏ÉÔºàÂ¶ÇË∑ëÈÖ∑ÈöúÁ¢çÔºâÊãüÂêàÁöÑÔºå**Ê≥õÂåñÂà∞Êú™ËßÅÈùûÁªìÊûÑÂåñÂú∞ÂΩ¢ËÉΩÂäõÊúâÈôê**„ÄÇ<br>2. **‰∏çÁ°ÆÂÆöÊÄß**ÔºöÂª∫ÂõæÊ®°ÂùóÊ≤°ÊúâÊòæÂºèËæìÂá∫‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°ÔºåÊó†Ê≥ïÂ¶•ÂñÑÂ§ÑÁêÜÈÅÆÊå°„ÄÇ<br>3. **ËÆ≠ÁªÉÊïàÁéá**ÔºöÂª∫ÂõæÁΩëÁªú‰∏éÁ≠ñÁï•ËÅîÂêàËÆ≠ÁªÉÔºåËÆ°ÁÆóÊàêÊú¨È´òÔºåÈöæ‰ª•ÊîØÊåÅÂ§ßËßÑÊ®°Âπ∂Ë°å‰ªøÁúüËÆ≠ÁªÉ„ÄÇ | 1. **Âª∫ÂõæÊ≥õÂåñ**ÔºöËÆæËÆ°‰∫ÜÂú®**Â§öÊ†∑ÂåñÂêàÊàêÂú∞ÂΩ¢**ÔºàÂåÖÊã¨‰∏çÂèØÈÄöË°åÂú∞ÂΩ¢Ôºâ‰∏äÈ¢ÑËÆ≠ÁªÉÁöÑËΩªÈáèÂª∫ÂõæÁΩëÁªúÔºåÂπ∂ÈááÁî®**Ê¶ÇÁéáËµ¢ÂÆ∂ÈÄöÂêÉËûçÂêàÁ≠ñÁï•**ÔºåÊòæËëóÊèêÂçá‰∫ÜÂª∫ÂõæÂØπÊú™ËßÅÂú∞ÂΩ¢ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ<br>2. **ÊòæÂºè‰∏çÁ°ÆÂÆöÊÄß**ÔºöÂª∫ÂõæÁΩëÁªúÁõ¥Êé•È¢ÑÊµãÊØè‰∏™ÂçïÂÖÉÊ†ºÁöÑÈ´òÁ®ã**‰∏çÁ°ÆÂÆöÊÄß**Ôºå‰ΩøÊéßÂà∂Âô®ËÉΩÊÑüÁü•‰ø°ÊÅØÁº∫Â§±Âå∫Âüü„ÄÇ<br>3. **ËÆ≠ÁªÉËß£ËÄ¶**ÔºöÂª∫ÂõæÊ®°ÂùóÁã¨Á´ãÈ¢ÑËÆ≠ÁªÉÔºåËÉΩ‰∏éÊéßÂà∂Âô®ËÆ≠ÁªÉÈ´òÊïàÂπ∂Ë°åÔºåÊîØÊåÅÊï∞ÂçÉ‰∏™‰ªøÁúüÁéØÂ¢ÉÂêåÊó∂ËøêË°å„ÄÇ |\n| **Parkour in the Wild** | 2025, arXiv | ÈÄöËøá**Â§ö‰∏ìÂÆ∂Ëí∏È¶è‰∏éRLÂæÆË∞É**ÔºåÂ≠¶‰π†‰∫Ü‰∏Ä‰∏™**ÈÄöÁî®‰∏îÂèØÊâ©Â±ïÁöÑÊïèÊç∑ËøêÂä®Á≠ñÁï•**„ÄÇÂ≠¶ÁîüÁ≠ñÁï•Áõ¥Êé•Â§ÑÁêÜÂéüÂßãÊ∑±Â∫¶ÂõæÂÉèÔºåÊòØÂΩìÂâçÁ´ØÂà∞Á´ØÊïèÊç∑Ê≥õÂåñËøêÂä®ÁöÑÂÖàËøõÊñπÊ≥ï„ÄÇ | 1. **Êû∂ÊûÑ**ÔºöÈááÁî®**Á´ØÂà∞Á´Ø**ÁöÑËßÜËßâÂæ™ÁéØÁΩëÁªúÔºåÁõ¥Êé•‰ªéÊ∑±Â∫¶ÂõæÂÉèÊò†Â∞ÑÂà∞Âä®‰Ωú„ÄÇ<br>2. **Ê≥õÂåñË°®Áé∞**ÔºöÂú®**Êú™ËßÅÁ®ÄÁñèÂú∞ÂΩ¢**‰∏äÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõËæÉÂº±ÔºàÂú®Ê∑∑ÂêàÊµãËØïÂú∞ÂΩ¢‰∏äÂπ≥ÂùáÊàêÂäüÁéá51.5%Ôºâ„ÄÇ<br>3. **ÂèØËß£ÈáäÊÄß**ÔºöÂú∞ÂΩ¢Êé®ÁêÜÈöêÂê´Âú®ÁΩëÁªú‰∏≠ÔºåÁº∫‰πèÂèØËß£ÈáäÁöÑ‰∏≠Èó¥Ë°®Á§∫„ÄÇ | 1. **Ê®°ÂùóÂåñ‰∏éÂèØËß£ÈáäÊÄß**Ôºö‰øùÊåÅ‰∫Ü**ÊòæÂºèÂª∫ÂõæÊ®°Âùó**ÔºåÊèê‰æõ‰∫ÜÂú∞ÂΩ¢ÁöÑÈ´òÂ∫¶Âíå‰∏çÁ°ÆÂÆöÊÄßÁΩëÊ†ºÔºåÁ≥ªÁªüÊõ¥ÂÖ∑ÂèØËß£ÈáäÊÄß„ÄÇAME-2ÁºñÁ†ÅÂô®ÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçÂèØËßÜÂåñÊè≠Á§∫‰∫ÜÁ≠ñÁï•ÂÖ≥Ê≥®ÁöÑÂú∞ÂΩ¢Âå∫Âüü„ÄÇ<br>2. **Ê≥õÂåñ‰ºòÂäø**ÔºöÂú®ÈúÄË¶Å**Á≤æÁ°ÆËêΩËÑöÁÇπ**ÁöÑÁ®ÄÁñèÂíåÊ∑∑ÂêàÂú∞ÂΩ¢‰∏äÔºåAME-2Â≠¶ÁîüÁ≠ñÁï•ÁöÑÊ≥õÂåñÊÄßËÉΩÂ§ßÂπÖË∂ÖË∂äËØ•Á´ØÂà∞Á´ØÂü∫Á∫øÔºàÂπ≥ÂùáÊàêÂäüÁéá82.4% vs 51.5%Ôºâ„ÄÇ<br>3. **ËÆ∞ÂøÜÊú∫Âà∂**ÔºöÈÄöËøáÂÖ®Â±ÄÂú∞ÂõæËøõË°å**ÊòæÂºèÁ©∫Èó¥ËÆ∞ÂøÜ**ÔºåËß£ÂÜ≥‰∫ÜËØ•Âü∫Á∫øÊñπÊ≥ïÂú®Èù†ËøëÈ´òÈöúÁ¢çÁâ©Êó∂Âõ†Áº∫‰πèÈïøÊúüËÆ∞ÂøÜËÄåÂ§±Ë¥•ÁöÑÈóÆÈ¢ò„ÄÇ |\n\n### Ê†∏ÂøÉÊîπËøõÊÄªÁªì\n\nAME-2 Áõ∏ÂØπ‰∫éÂÖ∂Ââç‰ΩúÔºåÁâπÂà´ÊòØÂú®‰∏é AME-1 Âíå *Parkour in the Wild* ÁöÑÂØπÊØî‰∏≠ÔºåÂÆûÁé∞‰∫Ü‰ª•‰∏ã**ÂÖ≥ÈîÆÊÄßÊï¥Âêà‰∏éÁ™ÅÁ†¥**Ôºö\n\n1.  **ÁºñÁ†ÅÂô®Êû∂ÊûÑÁöÑË¥®Âèò**ÔºöÂú® AME-1 ÁöÑÂü∫Á°Ä‰∏äÔºåÂàõÊñ∞ÊÄßÂú∞Â∞Ü**ÂÖ®Â±ÄÂú∞ÂΩ¢ÁâπÂæÅ**ÂºïÂÖ•Ê≥®ÊÑèÂäõÊùÉÈáçÁöÑËÆ°ÁÆó„ÄÇËøô‰ΩøÂæóÁ≠ñÁï•‰∏çÂÜçÊòØ‚ÄúÁõ≤ÁõÆÁöÑÂèçÂ∫îÂºè‚ÄùÔºåËÄåÊòØËÉΩÊ†πÊçÆÂú∞ÂΩ¢ÂÖ®Â±ÄÁªìÊûÑÔºàÂ¶Ç‚ÄúËøôÊòØ‰∏Ä‰∏™ÈúÄË¶ÅÊîÄÁà¨ÁöÑÂπ≥Âè∞‚ÄùËøòÊòØ‚Äú‰∏Ä‰∏™ÈúÄË¶ÅË∞®ÊÖéË°åËµ∞ÁöÑÁ™ÑÊ¢Å‚ÄùÔºâÊù•Âä®ÊÄÅË∞ÉÊï¥ÂÖ∂Â±ÄÈÉ®ÂÖ≥Ê≥®ÁÇπÂíåËøêÂä®Ê®°ÂºèÔºåËøôÊòØÂÆûÁé∞**Âº∫Ë∑®Âú∞ÂΩ¢Ê≥õÂåñ**ÁöÑÊ†∏ÂøÉ„ÄÇ\n\n2.  **ÊÑüÁü•-ÊéßÂà∂Èó≠ÁéØÁöÑÈáçÊûÑ**Ôºö‰∏çÂÜçÂ∞ÜÂª∫ÂõæËßÜ‰∏∫‰∏Ä‰∏™Áã¨Á´ãÁöÑ„ÄÅÊúÄÂ•ΩÊòØÂÆåÁæéÁöÑËæìÂÖ•Ê®°Âùó„ÄÇËÄåÊòØËÆæËÆ°‰∫Ü‰∏Ä‰∏™**‰∏∫ÊïèÊç∑ËøêÂä®ÈáèË∫´ÂÆöÂà∂**ÁöÑÁ•ûÁªèÂª∫ÂõæÁÆ°ÈÅìÔºåÂÖ∂ÁâπÁÇπÂåÖÊã¨Ôºö\n    *   **ËΩªÈáè‰∏éÈ´òÊïà**ÔºöÂèØËøêË°å‰∫éÂ§ßËßÑÊ®°Âπ∂Ë°å‰ªøÁúüÂíåÁúüÂÆûÊú∫ËΩΩCPU„ÄÇ\n    *   **‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•**ÔºöÊòæÂºèËæìÂá∫ÊñπÂ∑ÆÔºåÂå∫ÂàÜ‚ÄúÂ∑≤Áü•Âπ≥Âù¶‚Äù„ÄÅ‚ÄúÂ∑≤Áü•ÈöúÁ¢ç‚ÄùÂíå‚ÄúÊú™Áü•/ÈÅÆÊå°Âå∫Âüü‚Äù„ÄÇ\n    *   **Ê≥õÂåñÊÄßËÆ≠ÁªÉ**Ôºö‰ΩøÁî®ËøúË∂ÖÊú∫Âô®‰∫∫ÂèØË°åËåÉÂõ¥ÁöÑÂ§öÊ†∑ÂåñÂêàÊàêÂú∞ÂΩ¢ËøõË°åÈ¢ÑËÆ≠ÁªÉ„ÄÇ\n    *   **Âú®Á∫øËÆ≠ÁªÉÈõÜÊàê**ÔºöËØ•ÁÆ°ÈÅìÂèØÊó†ÁºùÊé•ÂÖ•RLËÆ≠ÁªÉÂæ™ÁéØÔºå‰ΩøÁ≠ñÁï•‰ªé‰∏ÄÂºÄÂßãÂ∞±Â≠¶‰π†Â§ÑÁêÜÊúâÂô™Â£∞„ÄÅ‰∏çÂÆåÊï¥ÁöÑÂª∫Âõæ„ÄÇ\n\n3.  **Áªü‰∏ÄÊïèÊç∑‰∏éÊ≥õÂåñÁöÑÁõÆÊ†á**ÔºöÊ≠§ÂâçÂ∑•‰ΩúÊàñÂú®ÁâπÂÆöÈ¢ÜÂüüÔºàÂ¶ÇË∑ëÈÖ∑ÔºâËøΩÊ±ÇÊûÅËá¥ÊïèÊç∑ÔºåÊàñÂú®ËæÉ‰øùÂÆàËÆæÂÆö‰∏ãËøΩÊ±ÇÂú∞ÂΩ¢Ê≥õÂåñ„ÄÇAME-2 ÈÄöËøá‰∏äËø∞ÊäÄÊúØÔºåÈ¶ñÊ¨°Âú®**Âêå‰∏ÄÊ°ÜÊû∂„ÄÅÂêå‰∏ÄÁ≠ñÁï•**‰∏≠ÂÆûÁé∞‰∫Ü‰∏éÊúÄÂÖàËøõÊïèÊç∑ÊéßÂà∂Âô®Â™≤ÁæéÁöÑÂä®ÊÄÅÊÄßËÉΩÔºåÂêåÊó∂ÂèàÂú®ÂπøÊ≥õ‰∏îÂ§çÊùÇÁöÑÊú™ËßÅÂú∞ÂΩ¢‰∏ä‰øùÊåÅ‰∫ÜÈ´òÊàêÂäüÁéá„ÄÇ\n\n4.  **Á≥ªÁªüÁ∫ßÂèØÈÉ®ÁΩ≤ÊÄß**ÔºöÈÄöËøá**ÊïôÂ∏à-Â≠¶ÁîüËÆ≠ÁªÉËåÉÂºè**ÔºåÂ∞ÜÊã•ÊúâÁâπÊùÉ‰ø°ÊÅØÔºàÁúüÂÆûÂú∞ÂõæÔºâÁöÑ„ÄÅÊ≥õÂåñËÉΩÂäõÂº∫ÁöÑ‚ÄúÊïôÂ∏à‚ÄùÁ≠ñÁï•ÁöÑÁü•ËØÜÔºåËí∏È¶èÂà∞‰ΩøÁî®ÂÆûÈôÖÁ•ûÁªèÂª∫ÂõæËæìÂÖ•ÁöÑ‚ÄúÂ≠¶Áîü‚ÄùÁ≠ñÁï•‰∏≠„ÄÇÊúÄÁªàÂæóÂà∞ÁöÑÁ≠ñÁï•ÊòØ**ÂèØÁõ¥Êé•ÈÉ®ÁΩ≤**ÁöÑÔºå‰∏îÁªßÊâø‰∫ÜÊïôÂ∏àÁöÑÊ≥õÂåñËÉΩÂäõÂíåÊïèÊç∑ÊÄß„ÄÇ\n\n**ÁªìËÆ∫**ÔºöAME-2 ‰∏çÊòØÂØπÂçï‰∏ÄÂâç‰ΩúÁöÑÁÆÄÂçïÊîπËøõÔºåËÄåÊòØ‰∏ÄÊ¨°**Á≥ªÁªüÊÄßÁöÑÊï¥Âêà‰∏éÂçáÁ∫ß**„ÄÇÂÆÉÁªßÊâø‰∫Ü AME-1 ÁöÑÊ≥®ÊÑèÂäõÂú∞ÂõæÁºñÁ†ÅÊÄùÊÉ≥Âπ∂Âä†‰ª•Â¢ûÂº∫ÔºåÂê∏Êî∂‰∫Ü *Anymal Parkour* ‰∏≠Â≠¶‰π†ÂûãÂª∫ÂõæÁöÑÊÄùË∑Ø‰ΩÜÂ§ßÂπÖÊèêÂçá‰∫ÜÂÖ∂Ê≥õÂåñÊÄßÂíåÂÆûÁî®ÊÄßÔºåÂêåÊó∂Âú®Ê≥õÂåñÊÄßËÉΩ‰∏äÊòæËëóË∂ÖË∂ä‰∫ÜÊúÄÊñ∞ÁöÑÁ´ØÂà∞Á´ØÊ≥õÂåñÊñπÊ≥ï *Parkour in the Wild*ÔºåÊúÄÁªàÂú®‰∏Ä‰∏™Ê°ÜÊû∂ÂÜÖÂêåÊó∂ÊîªÂÖã‰∫ÜË∂≥ÂºèÊú∫Âô®‰∫∫ÊÑüÁü• locomotion ‰∏≠**ÊïèÊç∑ÊÄß„ÄÅÊ≥õÂåñÊÄß„ÄÅÈ≤ÅÊ£íÊÄßÂíåÂèØÈÉ®ÁΩ≤ÊÄß**Èöæ‰ª•ÂÖºÂæóÁöÑÈïøÊúüÊåëÊàò„ÄÇ",
            "timestamp": "2026-01-20T17:53:26.142130",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ËÆ∫ÊñáÊèêÂá∫‰∫ÜÂì™‰∫õÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïÔºåËØ∑ÂàóË°®Ê†ºÂÖ∑‰ΩìËØ¶ÁªÜËØ¥ÊòéÊäÄÊúØÁªÜËäÇÔºåÈúÄË¶ÅÂåÖÂê´ÂÖ∑‰ΩìÁöÑÊï∞Â≠¶ÂéüÁêÜÊé®ÂØºÔºå‰ª•ÂèäÂÖ∑‰ΩìÂèÇÊï∞„ÄÇ",
            "answer": "## ËÆ∫ÊñáÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïÂàÜÊûê\n\nÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫AME-2ÁöÑÁªü‰∏ÄÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®ÂÆûÁé∞Ë∂≥ÂºèÊú∫Âô®‰∫∫Âú®Â§çÊùÇÂú∞ÂΩ¢‰∏äÁöÑÊïèÊç∑‰∏îÊ≥õÂåñÁöÑËøêÂä®„ÄÇÂÖ∂Ê†∏ÂøÉÂàõÊñ∞Âú®‰∫é**Ê≥®ÊÑèÂäõÁ•ûÁªèÂú∞ÂõæÁºñÁ†ÅÂô®**Âíå**ËΩªÈáèÁ∫ß„ÄÅ‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÁöÑÁ•ûÁªèÂª∫ÂõæÊµÅÊ∞¥Á∫ø**ÁöÑÁªìÂêà„ÄÇ‰ª•‰∏ãÊòØÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïÁöÑËØ¶ÁªÜËØ¥ÊòéÔºö\n\n### 1. AME-2ÔºöÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÁ•ûÁªèÂú∞ÂõæÁºñÁ†ÅÂô®\n\n| ÊäÄÊúØÁªÑ‰ª∂ | ÊäÄÊúØÁªÜËäÇ‰∏éÊï∞Â≠¶ÂéüÁêÜ | ÂÖ∑‰ΩìÂèÇÊï∞ |\n| :--- | :--- | :--- |\n| **Êï¥‰ΩìÊû∂ÊûÑ** | Á≠ñÁï•ÁΩëÁªúÁî±**Êú¨‰ΩìÊÑüËßâÁºñÁ†ÅÂô®**„ÄÅ**AME-2Âú∞ÂõæÁºñÁ†ÅÂô®**Âíå‰∏Ä‰∏™**Â§öÂ±ÇÊÑüÁü•Êú∫**Âä®‰ΩúËß£Á†ÅÂô®ÁªÑÊàê„ÄÇAME-2ÁºñÁ†ÅÂô®Ë¥üË¥£Â§ÑÁêÜÈ´òÁ®ãÂú∞ÂõæËæìÂÖ•ÔºåÁîüÊàêÂú∞ÂΩ¢ÊÑüÁü•ÁöÑÂµåÂÖ•Ë°®Á§∫„ÄÇ | Á≠ñÁï•ËøêË°åÈ¢ëÁéáÔºö50 Hz„ÄÇÂä®‰ΩúÔºàÂÖ≥ËäÇPDÁõÆÊ†áÔºâË∑üË∏™È¢ëÁéáÔºö400 Hz„ÄÇ |\n| **Âú∞ÂõæËæìÂÖ•Ë°®Á§∫** | **ÊïôÂ∏àÁ≠ñÁï•**Ôºö‰ΩøÁî®ÁúüÂÆûÈ´òÁ®ãÂõæÔºåÊØè‰∏™ÁΩëÊ†ºÁÇπË°®Á§∫‰∏∫3DÂùêÊ†á `(x, y, z)`„ÄÇ<br>**Â≠¶ÁîüÁ≠ñÁï•**Ôºö‰ΩøÁî®Á•ûÁªèÂª∫ÂõæÊµÅÊ∞¥Á∫øËæìÂá∫ÁöÑÂú∞ÂõæÔºåÊØè‰∏™ÁΩëÊ†ºÁÇπË°®Á§∫‰∏∫4D `(x, y, z, u)`ÔºåÂÖ∂‰∏≠ `u` ‰∏∫‰∏çÁ°ÆÂÆöÊÄßÂ∫¶ÈáèÔºàÊñπÂ∑ÆÔºâ„ÄÇ | **ANYmal-DÔºàÂõõË∂≥Ôºâ**ÔºöÂú∞ÂõæÂ∞∫ÂØ∏ 36 √ó 14ÔºåÂàÜËæ®Áéá 8 cmÔºå‰∏≠ÂøÉ‰Ωç‰∫éÂü∫ÂùêÊ†áÁ≥ª `(x=0.6 m, y=0 m)`„ÄÇ<br>**TRON1ÔºàÂèåË∂≥Ôºâ**ÔºöÂú∞ÂõæÂ∞∫ÂØ∏ 18 √ó 13ÔºåÂàÜËæ®Áéá 8 cmÔºå‰∏≠ÂøÉ‰Ωç‰∫é `(x=0.32 m, y=0 m)`„ÄÇ |\n| **AME-2ÁºñÁ†ÅÂô®Â∑•‰ΩúÊµÅÁ®ã** | 1. **Â±ÄÈÉ®ÁâπÂæÅÊèêÂèñ**Ôºö‰ΩøÁî®CNNÂ§ÑÁêÜÂú∞ÂõæÔºåÂæóÂà∞ÈÄêÁÇπÁöÑÂ±ÄÈÉ®ÁâπÂæÅ `f_local`„ÄÇ<br>2. **ÂÖ®Â±ÄÁâπÂæÅÊèêÂèñ**ÔºöÂØπÂ±ÄÈÉ®ÁâπÂæÅËøõË°åMLPÂ§ÑÁêÜÔºåÁÑ∂ÂêéÈÄöËøá**ÊúÄÂ§ßÊ±†Âåñ**ÂæóÂà∞ÂÖ®Â±ÄÁâπÂæÅ `f_global`ÔºåÊçïËé∑Âú∞ÂΩ¢Êï¥‰Ωì‰∏ä‰∏ãÊñá„ÄÇ<br>3. **Ê≥®ÊÑèÂäõÂä†ÊùÉ**ÔºöÂ∞ÜÂÖ®Â±ÄÁâπÂæÅ `f_global` ‰∏éÊú¨‰ΩìÊÑüËßâÂµåÂÖ• `e_proprio` ÊãºÊé•ÔºåÈÄöËøáMLPÁîüÊàê**Êü•ËØ¢ÂêëÈáè** `q`„ÄÇ‰ΩøÁî®**Â§öÂ§¥Ê≥®ÊÑèÂäõ**Êú∫Âà∂Ôºå‰ª• `f_local` ‰Ωú‰∏∫ÈîÆÂíåÂÄºÔºå`q` ‰Ωú‰∏∫Êü•ËØ¢ÔºåËÆ°ÁÆóÊ≥®ÊÑèÂäõÊùÉÈáçÂπ∂Âä†ÊùÉÂ±ÄÈÉ®ÁâπÂæÅÔºåÂæóÂà∞ `f_local_weighted`„ÄÇ<br>4. **ÁâπÂæÅËûçÂêà**ÔºöÂ∞ÜÂä†ÊùÉÂêéÁöÑÂ±ÄÈÉ®ÁâπÂæÅ `f_local_weighted` ‰∏éÂÖ®Â±ÄÁâπÂæÅ `f_global` ÊãºÊé•ÔºåÂΩ¢ÊàêÊúÄÁªàÁöÑÂú∞ÂõæÂµåÂÖ• `e_map`„ÄÇ | Ê≥®ÊÑèÂäõÊú∫Âà∂ÈááÁî®Ê†áÂáÜÁöÑÂ§öÂ§¥Ê≥®ÊÑèÂäõÔºàMHAÔºâ„ÄÇ‰∏éAME-1ÔºàÂâç‰ª£Â∑•‰ΩúÔºâÁöÑÂÖ≥ÈîÆÂå∫Âà´Âú®‰∫éÂºïÂÖ•‰∫Ü**ÂÖ®Â±ÄÁâπÂæÅ**Êù•ÊåáÂØºÊ≥®ÊÑèÂäõÔºå‰ΩøÁ≠ñÁï•ËÉΩÊ†πÊçÆÂú∞ÂΩ¢ÂÖ®Â±Ä‰∏ä‰∏ãÊñáË∞ÉÊï¥ÂÖ≥Ê≥®Âå∫Âüü„ÄÇ |\n| **Êú¨‰ΩìÊÑüËßâÁºñÁ†ÅÂô®** | **ÊïôÂ∏à**Ôºö‰ΩøÁî®ÁÆÄÂçïÁöÑMLPÂ§ÑÁêÜÊó†Âô™Â£∞ÁöÑÊú¨‰ΩìÊÑüËßâËßÇÊµãÂÄºÔºàÂåÖÊã¨Âü∫Â∫ßÁ∫øÈÄüÂ∫¶ `v_b`Ôºâ„ÄÇ<br>**Â≠¶Áîü**Ôºö‰∏∫Â∫îÂØπÁé∞ÂÆû‰∏çÁ°ÆÂÆöÊÄßÔºåÂ†ÜÂè†ËøáÂéª20Ê≠•ÁöÑÊú¨‰ΩìÊÑüËßâËßÇÊµãÔºà‰∏çÂåÖÊã¨ `v_b` ÂíåÂëΩ‰ª§ÔºâÔºå‰ΩøÁî®**ÈïøÁü≠Êó∂ËæìÂÖ•ËæìÂá∫ÁΩëÁªú** ÊèêÂèñÊó∂Â∫èÂµåÂÖ•ÔºåÂÜç‰∏éÂëΩ‰ª§‰∏ÄËµ∑ËæìÂÖ•MLP„ÄÇ | LSIOÁΩëÁªúÁî®‰∫éÊèêÂèñÊú∫Âô®‰∫∫Áä∂ÊÄÅÂíåÁéØÂ¢ÉÂä®ÊÄÅÁöÑÊó∂Â∫èË°®ÂæÅ„ÄÇ |\n| **ÈùûÂØπÁß∞ÊºîÂëò-ËØÑËÆ∫ÂÆ∂ËÆ≠ÁªÉ** | **ÊºîÂëò**Ôºö‰ΩøÁî®‰∏äËø∞AME-2Êû∂ÊûÑ„ÄÇ<br>**ËØÑËÆ∫ÂÆ∂**Ôºö‰∏çÈááÁî®ËÆ°ÁÆóÊòÇË¥µÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÄåÊòØ‰ΩøÁî®**Ê∑∑Âêà‰∏ìÂÆ∂** Êû∂ÊûÑÔºåËØ•Êû∂ÊûÑÂáΩÊï∞ÊãüÂêàËÉΩÂäõÂº∫‰∏î‰ºòÂåñÊïàÁéáÊõ¥È´ò„ÄÇËØÑËÆ∫ÂÆ∂È¢ùÂ§ñÊé•Êî∂ÊØè‰∏™ËøûÊùÜÁöÑÊé•Ëß¶Áä∂ÊÄÅ‰ø°ÊÅØ„ÄÇ | ‰ΩøÁî®PPOÁÆóÊ≥ïËøõË°å‰ºòÂåñ„ÄÇ‰∏∫ÊèêÂçáÊ†∑Êú¨ÊïàÁéáÂíåËøêÂä®È£éÊ†ºÔºåÂØπËØÑËÆ∫ÂÆ∂Â∫îÁî®‰∫ÜÂ∑¶Âè≥ÂØπÁß∞Â¢ûÂº∫„ÄÇ |\n\n### 2. ËΩªÈáèÁ∫ß„ÄÅ‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÁöÑÁ•ûÁªèÂª∫ÂõæÊµÅÊ∞¥Á∫ø\n\n| ÊäÄÊúØÁªÑ‰ª∂ | ÊäÄÊúØÁªÜËäÇ‰∏éÊï∞Â≠¶ÂéüÁêÜ | ÂÖ∑‰ΩìÂèÇÊï∞ |\n| :--- | :--- | :--- |\n| **Êï¥‰ΩìÊµÅÁ®ã** | 1. **Â±ÄÈÉ®ÁΩëÊ†ºÊäïÂΩ±**ÔºöÂ∞ÜÊ∑±Â∫¶ÂõæÂÉèÁÇπ‰∫ëÊäïÂΩ±Âà∞Êú∫Âô®‰∫∫Âü∫Â∫ßÁ≥ªÁöÑÂ±ÄÈÉ®2DÈ´òÂ∫¶ÁΩëÊ†º‰∏≠„ÄÇÊØè‰∏™ÂçïÂÖÉÊ†º‰øùÁïôÊúÄÈ´òÁöÑ `z` ÂÄºÔºàÂØπËøêÂä®ÊúÄÁõ∏ÂÖ≥ÔºâÔºåÊó†ÁÇπÂå∫ÂüüËµã‰∫àÂõ∫ÂÆöÊúÄÂ∞èÂÄº„ÄÇ<br>2. **Á•ûÁªèÈ´òÁ®ãÈ¢ÑÊµã**Ôºö‰ΩøÁî®ËΩªÈáèÁ∫ßCNNÔºàÂü∫‰∫éU-NetÁöÑÈó®ÊéßÊÆãÂ∑ÆËÆæËÆ°ÔºâÈ¢ÑÊµãÊØè‰∏™ÂçïÂÖÉÊ†ºÁöÑ**Áõ∏ÂØπÈ´òÁ®ã‰º∞ËÆ°ÂÄº `Œº`** Âíå**‰∏çÁ°ÆÂÆöÊÄßÔºàÂØπÊï∞ÊñπÂ∑Æ `log œÉ^2`Ôºâ**„ÄÇ<br>3. **Ê¶ÇÁéáËµ¢ÂÆ∂ÈÄöÂêÉËûçÂêà**ÔºöÂà©Áî®ÈáåÁ®ãËÆ°‰ΩçÂßøÔºåÂ∞ÜÂ∏¶‰∏çÁ°ÆÂÆöÊÄßÁöÑÂ±ÄÈÉ®È¢ÑÊµãËûçÂêàÂà∞ÂÖ®Â±ÄÂú∞Âõæ `‚Ñ≥` ‰∏≠„ÄÇ | **ANYmal-D**ÔºöÂ±ÄÈÉ®ÁΩëÊ†ºÂ∞∫ÂØ∏ 51 √ó 31ÔºåÂàÜËæ®Áéá 4 cmÔºå‰∏≠ÂøÉ‰Ωç‰∫é `(x=1.0 m, y=0 m)`„ÄÇ<br>**TRON1**ÔºöÂ±ÄÈÉ®ÁΩëÊ†ºÂ∞∫ÂØ∏ 31 √ó 31ÔºåÂàÜËæ®Áéá 4 cmÔºå‰∏≠ÂøÉ‰Ωç‰∫é `(x=0.6 m, y=0 m)`„ÄÇ<br>ÂÖ®Â±ÄÂú∞ÂõæÂ∞∫ÂØ∏Ôºö8 m √ó 8 mÔºàÂèØÂä®ÊÄÅÈáçÂÆö‰∏≠ÂøÉÔºâ„ÄÇ |\n| **‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°‰∏éÊçüÂ§±ÂáΩÊï∞** | ‰ΩøÁî® **Œ≤-NLLÊçüÂ§±** ËÆ≠ÁªÉÂª∫ÂõæÁΩëÁªúÔºåÈºìÂä±ÁΩëÁªúÂú®È¢ÑÊµã‰∏çÂáÜÊó∂ËæìÂá∫È´ò‰∏çÁ°ÆÂÆöÊÄßÔºåÈ¢ÑÊµãÂáÜÁ°ÆÊó∂ËæìÂá∫‰Ωé‰∏çÁ°ÆÂÆöÊÄß„ÄÇ<br>`L_{0.5} = E_{X,Y}[ sg[œÉÃÇ(X)] * ( (log œÉÃÇ¬≤(X))/2 + (Y - ŒºÃÇ(X))¬≤/(2œÉÃÇ¬≤(X)) ) ]`<br>ÂÖ∂‰∏≠ `X` ‰∏∫ËæìÂÖ•Â±ÄÈÉ®ÁΩëÊ†ºÔºå`Y` ‰∏∫ÁúüÂÆûÈ´òÁ®ãÔºå`ŒºÃÇ` Âíå `œÉÃÇ¬≤` ‰∏∫ÁΩëÁªúÈ¢ÑÊµãÁöÑÂùáÂÄºÂíåÊñπÂ∑ÆÔºå`sg[¬∑]` ‰∏∫ÂÅúÊ≠¢Ê¢ØÂ∫¶Êìç‰Ωú„ÄÇ`Œ≤=0.5` ÂèØÈò≤Ê≠¢ÁΩëÁªúÈÄöËøáÁÆÄÂçïÂ¢ûÂ§ß‰∏çÁ°ÆÂÆöÊÄßÊù•Èôç‰ΩéÊçüÂ§±„ÄÇ | ËÆ≠ÁªÉÊï∞ÊçÆÔºö5400‰∏áÂ∏ßÂêàÊàêÊï∞ÊçÆ„ÄÇÊâπÈáèËÆ≠ÁªÉÊó∂ÔºåÊ†πÊçÆÊ†∑Êú¨ÁöÑ**ÊÄªÂèòÂ∑Æ** ÂØπÊçüÂ§±ËøõË°åÈáçÂä†ÊùÉÔºåËÆ©Âú∞ÂΩ¢Êõ¥Â§çÊùÇÁöÑÊ†∑Êú¨Êã•ÊúâÊõ¥È´òÊùÉÈáçÔºö`TV(Y_b) = (‚Äñ‚àá_x Y_b‚Äñ_1 + ‚Äñ‚àá_y Y_b‚Äñ_1) / (H*W)`„ÄÇ |\n| **Ê¶ÇÁéáËµ¢ÂÆ∂ÈÄöÂêÉÂú∞ÂõæÊõ¥Êñ∞** | ÂØπ‰∫éÂÖ®Â±ÄÂú∞Âõæ‰∏≠ÊØè‰∏™Ë¢´Â±ÄÈÉ®È¢ÑÊµãË¶ÜÁõñÁöÑÂçïÂÖÉÊ†º `(u, v)`Ôºö<br>1. ËÆ°ÁÆóÊúâÊïàÊµãÈáèÊñπÂ∑ÆÔºö`œÉÃÇ_t¬≤ = max(œÉ_t¬≤, 0.5 * œÉ_prior¬≤)`ÔºåÈò≤Ê≠¢ËøáÂ∫¶Ëá™‰ø°„ÄÇ<br>2. Êõ¥Êñ∞ÊúâÊïàÊù°‰ª∂Ôºö`œÉÃÇ_t¬≤ < 1.5 * œÉ_prior¬≤` Êàñ `œÉÃÇ_t¬≤ < 0.2¬≤`„ÄÇ<br>3. ËÆ°ÁÆóËÉúÂá∫Ê¶ÇÁéáÔºö`p_win = (œÉÃÇ_t¬≤)^{-1} / ( (œÉÃÇ_t¬≤)^{-1} + (œÉ_prior¬≤)^{-1} )`„ÄÇ<br>4. ÈöèÊú∫Êõ¥Êñ∞ÔºöÈááÊ†∑ `Œæ ~ U[0,1]`ÔºåËã• `Œæ < p_win`ÔºåÂàôÁî®Êñ∞È¢ÑÊµã `(h_t, œÉÃÇ_t¬≤)` Ë¶ÜÁõñÊóßÂÄºÔºõÂê¶Âàô‰øùÁïôÊóßÂÄº `(h_prior, œÉ_prior¬≤)`„ÄÇ | ËøôÁßçÁ≠ñÁï•ËÉΩÔºöa) Èò≤Ê≠¢ÂØπÂêå‰∏ÄÈÅÆÊå°Âå∫ÂüüÂõ†ÊåÅÁª≠È¢ÑÊµãËÄåÈîôËØØÈôç‰Ωé‰∏çÁ°ÆÂÆöÊÄßÔºõb) ÈòªÊ≠¢Èùû‰∏ÄËá¥ÁöÑÈ´òÁΩÆ‰ø°Â∫¶È¢ÑÊµãË¶ÜÁõñÂú∞ÂõæÔºõc) Âú®Ëé∑ÂæóÈ´òÁΩÆ‰ø°Â∫¶ÊµãÈáèÊó∂Âø´ÈÄüÊõ¥Êñ∞Âú∞Âõæ„ÄÇ |\n| **‰ªøÁúüÈõÜÊàê‰∏éÈÉ®ÁΩ≤** | Âª∫ÂõæÊµÅÊ∞¥Á∫øÂú®‰ªøÁúüÔºàÊï∞ÂçÉ‰∏™Âπ∂Ë°åÁéØÂ¢ÉÔºâÂíåÁúüÂÆûÁ°¨‰ª∂‰∏äÂÆåÂÖ®‰∏ÄËá¥„ÄÇÂú®CPU‰∏äÂçïÂ∏ßÂ§ÑÁêÜÊó∂Èó¥Á∫¶5 msÔºàÂÖ∂‰∏≠Á•ûÁªèÁΩëÁªúÊé®ÁêÜÁ∫¶2.5 msÔºâÔºåÊª°Ë∂≥ÂÆûÊó∂ÊÄßË¶ÅÊ±Ç„ÄÇ | ‰ªøÁúü‰∏≠Ôºà1000‰∏™ANYmal-DÁéØÂ¢ÉÔºâÔºåÊ®°ÂûãÊé®ÁêÜÊó∂Èó¥ <0.3 msÔºåGPUÂÜÖÂ≠òÊ∂àËÄóÁ∫¶3 GB„ÄÇ |\n\n### 3. ÊïôÂ∏à-Â≠¶ÁîüÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂\n\n| ÊäÄÊúØÁªÑ‰ª∂ | ÊäÄÊúØÁªÜËäÇ‰∏éÊï∞Â≠¶ÂéüÁêÜ | ÂÖ∑‰ΩìÂèÇÊï∞ |\n| :--- | :--- | :--- |\n| **ËÆ≠ÁªÉÊµÅÁ®ã** | 1. **ÊïôÂ∏àÁ≠ñÁï•ËÆ≠ÁªÉ**ÔºöÂú®‰ªøÁúü‰∏≠‰ΩøÁî®**ÁúüÂÆûÈ´òÁ®ãÂú∞Âõæ**Âíå**Êó†Âô™Â£∞Êú¨‰ΩìÊÑüËßâ**ËøõË°åËÆ≠ÁªÉ„ÄÇ<br>2. **Â≠¶ÁîüÁ≠ñÁï•ËÆ≠ÁªÉ**Ôºö‰ΩøÁî®‰∏äËø∞**Á•ûÁªèÂª∫ÂõæÊµÅÊ∞¥Á∫ø**Êèê‰æõÁöÑÂú∞ÂõæÔºàÂê´Âô™Â£∞Âíå‰∏çÁ°ÆÂÆöÊÄßÔºâ‰Ωú‰∏∫ËæìÂÖ•ÔºåÂú®ÊïôÂ∏àÁ≠ñÁï•ÁöÑÁõëÁù£‰∏ãËøõË°åËÆ≠ÁªÉ„ÄÇ | **ÊïôÂ∏àËÆ≠ÁªÉ**Ôºö80,000 Ê¨°Ëø≠‰ª£„ÄÇ<br>**Â≠¶ÁîüËÆ≠ÁªÉ**Ôºö40,000 Ê¨°Ëø≠‰ª£ÔºàÂâç5,000Ê¨°Ëø≠‰ª£Á¶ÅÁî®PPOÊõø‰ª£ÊçüÂ§±Ôºâ„ÄÇ |\n| **Â≠¶ÁîüËÆ≠ÁªÉÁõÆÊ†áÂáΩÊï∞** | Â≠¶ÁîüÁ≠ñÁï•ÁöÑÊÄª‰ΩìÊçüÂ§±ÊòØ‰ª•‰∏ã‰∏âÈ°πÁöÑÁ∫øÊÄßÁªÑÂêàÔºö<br>1. **PPOÁöÑRLÊçüÂ§±**Ôºà‰ª∑ÂÄºÊçüÂ§±ÂíåÁ≠ñÁï•ÊçüÂ§±Ôºâ„ÄÇ<br>2. **Âä®‰ΩúËí∏È¶èÊçüÂ§±**Ôºö‰ΩøÂ≠¶ÁîüÁ≠ñÁï•ÁöÑËæìÂá∫Êé•ËøëÊïôÂ∏àÁ≠ñÁï•ÁöÑËæìÂá∫„ÄÇ<br>3. **Ë°®ÂæÅÊçüÂ§±**ÔºöÂ≠¶ÁîüÁ≠ñÁï•ÁöÑAME-2ÁºñÁ†ÅÂô®ËæìÂá∫ÁöÑÂú∞ÂõæÂµåÂÖ•‰∏éÊïôÂ∏àÁ≠ñÁï•ÁöÑÂØπÂ∫îÂµåÂÖ•‰πãÈó¥ÁöÑÂùáÊñπËØØÂ∑Æ„ÄÇ | ÊçüÂ§±Á≥ªÊï∞ÔºöÂä®‰ΩúËí∏È¶èÊçüÂ§± 0.02ÔºåË°®ÂæÅÊçüÂ§± 0.2„ÄÇÂ≠¶ÁîüËÆ≠ÁªÉÂàùÊúü‰ΩøÁî®ËæÉÈ´òÂ≠¶‰π†ÁéáÔºà0.001Ôºâ‰∏îÁ¶ÅÁî®PPOÊõø‰ª£ÊçüÂ§±Ôºå‰ª•Á®≥ÂÆöËÆ≠ÁªÉÂπ∂‰øÉËøõ‰∏éÊïôÂ∏àÁöÑÂØπÈΩê„ÄÇ |\n| **È¢ÜÂüüÈöèÊú∫Âåñ** | Âú®ËÆ≠ÁªÉ‰∏≠ÂπøÊ≥õÂ∫îÁî®ÈöèÊú∫Âåñ‰ª•ÊèêÂçáÈ≤ÅÊ£íÊÄßÂíå‰ªøÁúüÂà∞Áé∞ÂÆûÁöÑËΩ¨ÁßªÔºö<br>- **Êú∫Âô®‰∫∫Âä®ÂäõÂ≠¶**ÔºöÈöèÊú∫ÂåñË¥üËΩΩ„ÄÅÊë©Êì¶Á≥ªÊï∞„ÄÅÊâßË°åÂô®Âª∂ËøüÔºàÂèåË∂≥",
            "timestamp": "2026-01-20T17:54:31.913646",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "‰ªñ‰ΩøÁî®‰∫ÜÂì™‰∫õËØÑ‰ª∑ÊåáÊ†á‰∏éÊï∞ÊçÆÈõÜÔºåÂàóË°®Ê†ºÂÖ∑‰ΩìËÆ≤ËÆ≤‰ªñÁöÑËØÑ‰ª∑ÊåáÊ†áÁöÑÁªÜËäÇ‰∏éÊï∞ÊçÆÈõÜÁöÑÁªÜËäÇ",
            "answer": "## ËÆ∫ÊñáËØÑ‰ª∑ÊåáÊ†á‰∏éÊï∞ÊçÆÈõÜÂàÜÊûê\n\n### ‰∏Ä„ÄÅ ËØÑ‰ª∑ÊåáÊ†á\n\nËÆ∫Êñá‰∏ªË¶Å‰ΩøÁî®‰∏§Á±ªËØÑ‰ª∑ÊåáÊ†áÊù•ÈáèÂåñÁ≥ªÁªüÊÄßËÉΩÔºö**ÊéßÂà∂Âô®ÊàêÂäüÁéá** Âíå **Âú∞ÂõæÈáçÂª∫ÊçüÂ§±**„ÄÇÂÖ∑‰ΩìÁªÜËäÇÂ¶Ç‰∏ãË°®ÊâÄÁ§∫Ôºö\n\n| ÊåáÊ†áÁ±ªÂà´ | ÂÖ∑‰ΩìÊåáÊ†á | ËÆ°ÁÆóÊñπÂºè / ÂÆö‰πâ | ËØÑ‰º∞ÁõÆÁöÑ‰∏éÁªÜËäÇ |\n| :--- | :--- | :--- | :--- |\n| **ÊéßÂà∂Âô®ÊÄßËÉΩ** | **ÊàêÂäüÁéá** | Âú®ÁâπÂÆöÂú∞ÂΩ¢‰∏äÔºåÊú∫Âô®‰∫∫ÊàêÂäüÂà∞ËææÁõÆÊ†á‰ΩçÁΩÆÂπ∂‰øùÊåÅÁ®≥ÂÆöÂßøÊÄÅÁöÑÊµãËØïÂõûÂêàÊØî‰æã„ÄÇ | **Ê†∏ÂøÉËØÑ‰º∞ÊåáÊ†á**ÔºåÁî®‰∫éË°°ÈáèÊéßÂà∂Âô®ÁöÑ**ÊïèÊç∑ÊÄß**Âíå**Ê≥õÂåñËÉΩÂäõ**„ÄÇ<br>- **ËÆ≠ÁªÉÂú∞ÂΩ¢**ÔºöËØÑ‰º∞Âú®ËÆ≠ÁªÉËßÅËøáÁöÑÂéüÂßãÂú∞ÂΩ¢ÔºàÂØÜÈõÜ„ÄÅÊîÄÁà¨„ÄÅÁ®ÄÁñè‰∏âÁ±ªÔºâ‰∏äÁöÑÂü∫Á°ÄÊÄßËÉΩ„ÄÇ<br>- **ÊµãËØïÂú∞ÂΩ¢**ÔºöËØÑ‰º∞Âú®**Êú™ËßÅËøáÁöÑ**Êàñ**ËÆ≠ÁªÉÂú∞ÂΩ¢ÁªÑÂêà**ÁöÑÂ§çÊùÇÂú∞ÂΩ¢‰∏äÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõÔºàËßÅÂõæ4Ôºâ„ÄÇ<br>- **ÂØπÊØîÂü∫ÂáÜ**ÔºöÁî®‰∫é‰∏éAME-1„ÄÅMoE„ÄÅËßÜËßâÂæ™ÁéØÁΩëÁªúÁ≠âÂü∫Á∫øÊñπÊ≥ïËøõË°åÊ®™ÂêëÊØîËæÉÔºàË°®II, IIIÔºâ„ÄÇ |\n| **Âú∞ÂõæÈáçÂª∫Ë¥®Èáè** | **L‚ÇÄ.‚ÇÖ ÊçüÂ§±** | ÂÖ¨ÂºèÔºö`L‚ÇÄ.‚ÇÖ = E[ sg[œÉÃÇ(X)] * ( log œÉÃÇ¬≤(X)/2 + (Y - ŒºÃÇ(X))¬≤/(2œÉÃÇ¬≤(X)) ) ]` <br>ÂÖ∂‰∏≠ `sg[¬∑]` ‰∏∫ÂÅúÊ≠¢Ê¢ØÂ∫¶Êìç‰ΩúÔºå`ŒºÃÇ(X)` ‰∏∫È¢ÑÊµãÈ´òÁ®ãÔºå`œÉÃÇ¬≤(X)` ‰∏∫È¢ÑÊµãÊñπÂ∑ÆÔºå`Y` ‰∏∫ÁúüÂÆûÈ´òÁ®ã„ÄÇ | ËØÑ‰º∞**Á•ûÁªèÂú∞ÂõæÁÆ°ÈÅì**ÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÊÄßÂíå‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°Ë¥®Èáè„ÄÇ<br>- **ÁâπÁÇπ**ÔºöËØ•ÊçüÂ§±ÂáΩÊï∞Âπ≥Ë°°‰∫ÜÈ´òÁ®ã‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß (`(Y - ŒºÃÇ)¬≤`) Âíå‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°ÁöÑÂêàÁêÜÊÄß (`log œÉÃÇ¬≤`)„ÄÇ<br>- **‰ΩúÁî®**ÔºöÈáèÂåñÂú∞ÂõæÊ®°ÂùóÂú®‰∏çÂêåÂú∞ÂΩ¢ÔºàËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜÔºâ‰∏äÁöÑÈáçÂª∫ÊÄßËÉΩÔºåÂπ∂ÂØπÊØî‰∏çÂêåÂú∞ÂõæËÆæËÆ°ÊñπÊ°àÔºàÂ¶ÇÊú¨ÊñáÊñπÊ≥ï vs. Êó∂Â∫èÂæ™ÁéØÊ®°ÂûãÔºâÁöÑ‰ºòÂä£ÔºàË°®IVÔºâ„ÄÇ |\n| **È≤ÅÊ£íÊÄßËØÑ‰º∞** | **ÈÄÄÂåñÊÑüÁü•‰∏ãÁöÑÊàêÂäüÁéá** | Âú®ÊñΩÂä†‰∫ÜË∂ÖÂá∫ËÆ≠ÁªÉÈöèÊú∫ÂåñËåÉÂõ¥ÁöÑ‰º†ÊÑüÂô®ÈÄÄÂåñÔºàÂ¶ÇÁÇπ‰∫ëÁº∫Â§±„ÄÅÂô™Â£∞‰º™ÂΩ±„ÄÅÊëÑÂÉèÂ§¥Á¶ÅÁî®ÔºâÂêéÔºåÈáçÊñ∞ËØÑ‰º∞ÊéßÂà∂Âô®ÁöÑÊàêÂäüÁéá„ÄÇ | ËØÑ‰º∞Á≥ªÁªüÂØπ**ÁúüÂÆû‰∏ñÁïåÂô™Â£∞Âíå‰º†ÊÑüÂô®ÊïÖÈöú**ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ<br>- **ÊµãËØïÂú∫ÊôØ**Ôºö20%ÁÇπ‰∫ëÁº∫Â§±„ÄÅ3%Âô™Â£∞‰º™ÂΩ±„ÄÅÁ¶ÅÁî®Ââç‰∏äÊëÑÂÉèÂ§¥„ÄÇ<br>- **ÁªìÊûú**ÔºöÂ±ïÁ§∫‰∫ÜÁ≠ñÁï•Âú®‰∏•ÈáçÊÑüÁü•ÈÄÄÂåñ‰∏ã‰ªçËÉΩ‰øùÊåÅËæÉÈ´òÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÊò†Â∞ÑÂíåÊéßÂà∂ÁöÑÂçèÂêåÈ≤ÅÊ£íÊÄßÔºàË°®VÔºâ„ÄÇ |\n\n**ÂÖ∂‰ªñÈöêÂê´ÊåáÊ†á**Ôºö\n- **ÊïèÊç∑ÊÄßÂÆöÊÄßÂ±ïÁ§∫**ÔºöÈÄöËøáÊú∫Âô®‰∫∫ÂÆûÈôÖÊºîÁ§∫ÔºàÂ¶ÇÊîÄÁà¨È´òÂ∫¶„ÄÅË∑®Ë∂äÈó¥ÈöôÂÆΩÂ∫¶„ÄÅÂ≥∞ÂÄºÈÄüÂ∫¶ >1.5 m/sÔºâÂíå‰∏éSOTAÂ∑•‰ΩúÁöÑÂØπÊØîÔºàÂõæ9ÔºâÔºåËØÅÊòéÂÖ∂ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊïèÊç∑Ê∞¥Âπ≥„ÄÇ\n- **Âú∞ÂõæË¥®ÈáèÂÆöÊÄßÂàÜÊûê**ÔºöÈÄöËøáÂèØËßÜÂåñÈáçÂª∫ÁöÑÂú∞ÂõæÔºàÂõæ14, 19ÔºâÔºåÂ±ïÁ§∫ÂÖ∂ËÉΩÊçïÊçâÁ≤æÁªÜÂú∞ÂΩ¢ÁªìÊûÑÂπ∂ÂêàÁêÜÊ†áÊ≥®‰∏çÁ°ÆÂÆöÊÄßÂå∫Âüü„ÄÇ\n\n### ‰∫å„ÄÅ Êï∞ÊçÆÈõÜÔºàÂú∞ÂΩ¢ÈõÜÂêàÔºâ\n\nËÆ∫ÊñáÊ≤°Êúâ‰ΩøÁî®Â§ñÈÉ®Ê†áÊ≥®Êï∞ÊçÆÈõÜÔºåËÄåÊòØÈÄöËøá**Á®ãÂ∫èÂåñÁîüÊàê**Âíå**Áâ©ÁêÜ‰ªøÁúü**ÊûÑÂª∫‰∫ÜÁî®‰∫é**ËÆ≠ÁªÉ**Âíå**ÊµãËØï**ÁöÑÂú∞ÂΩ¢ÈõÜÂêà„ÄÇÁªÜËäÇÂ¶Ç‰∏ãÔºö\n\n| Êï∞ÊçÆÈõÜÁ±ªÂûã | Âú∞ÂΩ¢Á±ªÂà´‰∏éÊØî‰æã | ÁîüÊàêÊñπÂºè‰∏éÂÖ≥ÈîÆÂèÇÊï∞ | Áî®ÈÄî‰∏éÁªÜËäÇ |\n| :--- | :--- | :--- | :--- |\n| **ËÆ≠ÁªÉÂú∞ÂΩ¢** <br>(Áî®‰∫éÁ≠ñÁï•ÂíåÂú∞ÂõæÊ®°ÂûãËÆ≠ÁªÉ) | **1. ÂØÜÈõÜÂú∞ÂΩ¢ (Dense, 45%)** <br> - Á≤óÁ≥ôÂú∞Èù¢ (5%) <br> - ‰∏ãË°åÊ•ºÊ¢Ø (5%) <br> - ‰∏äË°åÊ•ºÊ¢Ø (5%) <br> - ÈöèÊú∫ÁÆ±Â≠ê (5%) <br> - ÊñúÂù°ÈöúÁ¢ç (5%) <br> - Âπ≥Âù¶Âú∞Èù¢ (20%) <br><br>**2. ÊîÄÁà¨Âú∞ÂΩ¢ (Climbing, 30%)** <br> - ÊîÄÁà¨‰∏äÂùë (20%) <br> - ÊîÄÁà¨‰∏ãÂè∞ (5%) <br> - ËøûÁª≠ÊîÄÁà¨ÁéØ (5%) <br><br>**3. Á®ÄÁñèÂú∞ÂΩ¢ (Sparse, 25%)** <br> - Èó¥Èöô (5%) <br> - Âπ≥Ë°åÊ®™Ê¢Å (5%) <br> - ÈöèÊú∫Ë∏èËÑöÁü≥ (15%) | - **Á®ãÂ∫èÂåñÁîüÊàê**Ôºö‰ΩøÁî®È´òÂ∫¶Âú∫„ÄÅÂá†‰ΩïÂü∫ÂÖÉÔºàÊ•ºÊ¢Ø„ÄÅÁÆ±Â≠ê„ÄÅÊ®™Ê¢Å„ÄÅÂùëÊ¥ûÔºâÁªÑÂêàËÄåÊàê„ÄÇ<br>- **ËØæÁ®ãÂ≠¶‰π†**ÔºöÊØè‰∏™Âú∞ÂΩ¢Á±ªÂà´ÁöÑÈöæÂ∫¶ÂèÇÊï∞ÔºàÂ¶ÇÈ´òÂ∫¶„ÄÅÂÆΩÂ∫¶„ÄÅÂù°Â∫¶ÔºâÈöèËÆ≠ÁªÉËø≠‰ª£‰ªéÊòìÂà∞Èöæ**Âä®ÊÄÅÁº©Êîæ**ÔºàËØ¶ËßÅÈôÑÂΩïAÔºâ„ÄÇ<br>- **ÈöèÊú∫Âåñ**ÔºöÂú∞ÂΩ¢È°∂ÁÇπ‰ΩçÁΩÆ„ÄÅÊúùÂêë„ÄÅÂπ≥Âè∞È´òÂ∫¶Â∑ÆÁ≠âÂä†ÂÖ•ÈöèÊú∫Êâ∞Âä®ÔºåÂ¢ûÂä†Â§öÊ†∑ÊÄß„ÄÇ<br>- **Áâ©ÁêÜ/ËôöÊãüÂú∞Èù¢**ÔºöÂØπÁ®ÄÁñèÂú∞ÂΩ¢Ôºå‰∏ÄÂçäÊ∑ªÂä†Áâ©ÁêÜÁ¢∞ÊíûÂú∞Èù¢ÔºåÂè¶‰∏ÄÂçäÊ∑ªÂä†‰ªÖÂú∞ÂõæÂèØËßÅÁöÑËôöÊãüÂú∞Èù¢ÔºåÈò≤Ê≠¢Êú∫Âô®‰∫∫‚ÄúÂÅ∑Êáí‚ÄùËµ∞Âú∞Èù¢„ÄÇ | **Á≠ñÁï•ËÆ≠ÁªÉ**ÔºöÁî®‰∫éËÆ≠ÁªÉÊïôÂ∏àÂíåÂ≠¶ÁîüÁ≠ñÁï•ÔºåÂ≠¶‰π†Âü∫Á°ÄÁöÑÁßªÂä®„ÄÅÊîÄÁà¨„ÄÅÂπ≥Ë°°ÊäÄËÉΩ„ÄÇ<br>**Âú∞ÂõæÊ®°ÂûãËÆ≠ÁªÉ**Ôºö‰Ωú‰∏∫ÂêàÊàêËÆ≠ÁªÉÊï∞ÊçÆÁöÑ‰∏ªË¶ÅÊù•Ê∫ê‰πã‰∏ÄÔºà‰∏éÈ¢ùÂ§ñÁîüÊàêÁöÑÂú∞ÂΩ¢ÁªìÂêàÔºâ„ÄÇ |\n| **ÊµãËØïÂú∞ÂΩ¢** <br>(Áî®‰∫éËØÑ‰º∞Ê≥õÂåñËÉΩÂäõ) | **Test 1**: Á®ÄÁñèÂú∞ÂΩ¢ÔºàÊú™ËßÅËøáÁöÑË∏èËÑöÁü≥Â∏ÉÂ±ÄÔºâ<br>**Test 2**: ÊîÄÁà¨‰∏éÁ®ÄÁñèÊ∑∑ÂêàÂú∞ÂΩ¢<br>**Test 3**: Ë∑ëÈÖ∑Âú∞ÂΩ¢ÔºàÈ´òÈöúÁ¢çÁâ©Ôºâ<br>**Test 4**: ÂØÜÈõÜ„ÄÅÊîÄÁà¨„ÄÅÁ®ÄÁñè‰∏âÂêà‰∏ÄÂ§çÊùÇÂú∞ÂΩ¢ | Áî±ËÆ≠ÁªÉ‰∏≠Êú™Âá∫Áé∞ËøáÁöÑÂú∞ÂΩ¢Â∏ÉÂ±ÄÊàñËÆ≠ÁªÉÂú∞ÂΩ¢ÁöÑ**Êñ∞È¢ñÁªÑÂêà**ÊûÑÊàê„ÄÇ‰æãÂ¶ÇÔºåÂ∞Ü‰∏çÂêåËÆ≠ÁªÉÂú∞ÂΩ¢ÁöÑÁâπÂæÅÔºàÂ¶ÇÈ´òÂè∞„ÄÅÁ™ÑÊ¢Å„ÄÅÂ§ßÈó¥ÈöôÔºâ‰ª•Êñ∞ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ªÁªÑÂêà„ÄÇ | **Èõ∂Ê†∑Êú¨Ê≥õÂåñËØÑ‰º∞**ÔºöÊ†∏ÂøÉÊµãËØïÈõÜÔºåÁî®‰∫éÈ™åËØÅÊéßÂà∂Âô®ÂØπ**ÂÖ®Êñ∞„ÄÅÂ§çÊùÇÂú∞ÂΩ¢**ÁöÑÈÄÇÂ∫îËÉΩÂäõÔºåÊòØË°°Èáè‚ÄúÂπø‰πâ locomotion‚ÄùÁöÑÂÖ≥ÈîÆ„ÄÇ |\n| **Âú∞ÂõæÊ®°Âûã‰∏ìÁî®ËÆ≠ÁªÉÂú∞ÂΩ¢** <br>(Áî®‰∫éËÆ≠ÁªÉÁ•ûÁªèÂú∞ÂõæÁΩëÁªú) | 1. ** locomotionËÆ≠ÁªÉÂú∞ÂΩ¢ÁΩëÊ†º**<br>2. **ÈöèÊú∫Â†ÜÂè†ÁöÑÁÆ±Â≠ê**<br>3. **ÈöèÊú∫È´òÂ∫¶Âú∫**<br>4. **ÈöèÊú∫ÊºÇÊµÆÁöÑÁÆ±Â≠ê** | - **Êâ©Â±ïÊÄß**ÔºöÂåÖÂê´Â§ßÈáèÂØπÊú∫Âô®‰∫∫ËÄåË®Ä**‰∏çÂèØÈÄöË°å**ÁöÑÂú∞ÂΩ¢Ôºå‰ª•ÊãìÂÆΩÂú∞ÂõæÊ®°ÂûãÁöÑÊÑüÁü•ÂàÜÂ∏É„ÄÇ<br>- **È´òÊïàÈááÊ†∑**Ôºö‰ΩøÁî®WarpÂ∫ìËøõË°åÂÖâÁ∫øËøΩË∏™ÔºåÁõ¥Êé•‰ªéÁΩëÊ†ºÈááÊ†∑Â±ÄÈÉ®È´òÁ®ãÁΩëÊ†ºÔºåÊó†ÈúÄÁâ©ÁêÜ‰ªøÁúüÔºåÈááÊ†∑ÈÄüÂ∫¶ÊûÅÂø´ÔºàÊØèÁßíÊï∞ÂçÅ‰∏áÂ∏ßÔºâ„ÄÇ<br>- **Êï∞ÊçÆÂ¢ûÂº∫**ÔºöÂØπÈááÊ†∑ÁöÑÂ±ÄÈÉ®ÁΩëÊ†ºÊñΩÂä†ÂùáÂåÄÂô™Â£∞„ÄÅÈöèÊú∫Ë£ÅÂâ™„ÄÅÊ®°ÊãüÈÅÆÊå°„ÄÅÈöèÊú∫ÁÇπÁº∫Â§±ÂíåÂºÇÂ∏∏ÂÄºÁ≠âÔºåÊ®°ÊãüÁúüÂÆû‰º†ÊÑüÂô®Áº∫Èô∑„ÄÇ | **Âú∞ÂõæÁΩëÁªúËÆ≠ÁªÉ**Ôºö‰∏∫Á•ûÁªèÂú∞ÂõæÈ¢ÑÊµãÁΩëÁªúÊèê‰æõÂ§ßËßÑÊ®°„ÄÅÂ§öÊ†∑ÂåñÁöÑÁõëÁù£Êï∞ÊçÆÔºåÁ°Æ‰øùÂÖ∂ËÉΩÂ§üÊ≥õÂåñÂà∞ÂêÑÁßçÂú∞ÂΩ¢ÁªìÊûÑÔºåÂπ∂Ê≠£Á°ÆÂ§ÑÁêÜÂô™Â£∞ÂíåÈÅÆÊå°„ÄÇ |\n\n### ÊÄªÁªì\nAME-2 Â∑•‰ΩúÁöÑËØÑ‰º∞‰ΩìÁ≥ªÁ¥ßÂØÜÁªìÂêàÂÖ∂Á†îÁ©∂ÁõÆÊ†áÔºö\n1.  **ÊåáÊ†áËÆæËÆ°**Ôºö‰ª•**ÊàêÂäüÁéá**‰∏∫Ê†∏ÂøÉÈáèÂåñÊïèÊç∑‰∏éÊ≥õÂåñÔºå‰ª•**L‚ÇÄ.‚ÇÖÊçüÂ§±**‰øùÈöúÂú∞ÂõæË¥®ÈáèÔºå‰ª•**È≤ÅÊ£íÊÄßÊµãËØï**È™åËØÅÁ≥ªÁªüÂèØÈù†ÊÄß„ÄÇ\n2.  **Êï∞ÊçÆÈõÜÊûÑÂª∫**ÔºöÈááÁî®ÂàÜÂ±ÇÁöÑÁ®ãÂ∫èÂåñÁîüÊàêÊñπÊ≥ïÔºå‰∏•Ê†ºÂå∫ÂàÜ**ËÆ≠ÁªÉÈõÜ**ÔºàÁî®‰∫éÂ≠¶‰π†Âü∫Á°ÄÊäÄËÉΩÔºâÂíå**ÊµãËØïÈõÜ**ÔºàÁî®‰∫éËØÑ‰º∞Èõ∂Ê†∑Êú¨Ê≥õÂåñÔºâÔºåÂπ∂‰∏ìÈó®‰∏∫Âú∞ÂõæÊ®°ÂùóÊûÑÂª∫‰∫ÜÊõ¥ÂπøÊ≥õÁöÑ**Âú∞ÂΩ¢Êï∞ÊçÆÈõÜ**„ÄÇËøôÁßçÊï∞ÊçÆÁ≠ñÁï•ÊòØÂÖ∂ÂÆûÁé∞‚ÄúÂπø‰πâ‚Äù locomotion ÁöÑÂÖ≥ÈîÆÂü∫Á°Ä„ÄÇ",
            "timestamp": "2026-01-20T17:55:24.894201",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ËÆ∫ÊñáÂú®Âì™‰∫õÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºü‰∏ªË¶ÅÁöÑËØÑ‰º∞ÊåáÊ†áÂíåÊÄßËÉΩÊèêÂçáÊòØÂ§öÂ∞ëÔºü",
            "answer": "## ÂÆûÈ™åÊï∞ÊçÆÈõÜ‰∏éËØÑ‰º∞ÊåáÊ†áÂàÜÊûê\n\n### ‰∏Ä„ÄÅ ÂÆûÈ™åÊï∞ÊçÆÈõÜÔºàËÆ≠ÁªÉ‰∏éÊµãËØïÂú∞ÂΩ¢Ôºâ\n\nËÆ∫ÊñáÂú®**‰ªøÁúü**Âíå**ÁúüÂÆû‰∏ñÁïå**‰∏§‰∏™Â±ÇÈù¢ËøõË°å‰∫ÜÂ§ßÈáèÂÆûÈ™åÔºå‰ΩøÁî®ÁöÑÂú∞ÂΩ¢Êï∞ÊçÆÈõÜ‰∏ªË¶ÅÂàÜ‰∏∫**ËÆ≠ÁªÉÂú∞ÂΩ¢**Âíå**ÊµãËØïÂú∞ÂΩ¢**‰∏§Â§ßÁ±ªÔºåÊó®Âú®È™åËØÅÊñπÊ≥ïÁöÑ**ÊïèÊç∑ÊÄß**Âíå**Ê≥õÂåñËÉΩÂäõ**„ÄÇ\n\n#### 1. ËÆ≠ÁªÉÂú∞ÂΩ¢ÔºàPrimitive TerrainsÔºâ\nÂú®‰ªøÁúüËÆ≠ÁªÉ‰∏≠ÔºåÊéßÂà∂Âô®Âú®**‰∏âÁ±ªÂü∫Á°ÄÂú∞ÂΩ¢**‰∏äËøõË°åËÆ≠ÁªÉÔºåÊØèÁ±ªÂåÖÂê´Â§öÁßçÂèò‰ΩìÔºåÂπ∂ÈÄöËøáËØæÁ®ãÂ≠¶‰π†ÈÄêÊ≠•Â¢ûÂä†ÈöæÂ∫¶Ôºö\n- **ÂØÜÈõÜÂú∞ÂΩ¢**ÔºöÂåÖÊã¨Á≤óÁ≥ôÈ´òÂ∫¶Âú∫„ÄÅ‰∏ä‰∏ãÊ•ºÊ¢Ø„ÄÅÈöèÊú∫ÁÆ±‰Ωì„ÄÅÊñúÂù°‰∏äÁöÑÈöúÁ¢çÁâ©Á≠â„ÄÇ\n- **ÊîÄÁà¨Âú∞ÂΩ¢**ÔºöÂåÖÊã¨Áà¨Âá∫ÂáπÂùë„ÄÅ‰ªéÂπ≥Âè∞Áà¨‰∏ã„ÄÅËøûÁª≠ÊîÄÁà¨ÁéØÂΩ¢ÁªìÊûÑÁ≠â„ÄÇ\n- **Á®ÄÁñèÂú∞ÂΩ¢**ÔºöÂåÖÊã¨Ë∑®Ë∂äÈó¥Èöô„ÄÅÂπ≥Ë°åÊ¢Å„ÄÅÈöèÊú∫ÂàÜÂ∏ÉÁöÑË∏èËÑöÁü≥„ÄÅÁã¨Êú®Ê°•Á≠â„ÄÇ\n\n**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ≠ÁªÉ‰ªÖ‰ΩøÁî®Ëøô‰∫õ‚ÄúÂü∫Á°ÄÊ®°Âùó‚ÄùÔºå**‰∏çÂåÖÂê´**ÂêéÁª≠ÊµãËØï‰∏≠Âá∫Áé∞ÁöÑÂ§çÊùÇÁªÑÂêàÂú∞ÂΩ¢Ôºå‰ª•Ê£ÄÈ™åÊ≥õÂåñËÉΩÂäõ„ÄÇ\n\n#### 2. ÊµãËØïÂú∞ÂΩ¢ÔºàGeneralization TerrainsÔºâ\n‰∏∫‰∫ÜËØÑ‰º∞Ê≥õÂåñÊÄßËÉΩÔºåËÆ∫ÊñáËÆæËÆ°‰∫Ü**4ÁßçÊú™ËßÅËøáÁöÑÂ§çÊùÇÊµãËØïÂú∞ÂΩ¢**ÔºàËßÅÂõæ4ÔºâÔºö\n- **Test 1**: Á®ÄÁñèÂú∞ÂΩ¢ÔºàÂ¶ÇË∏èËÑöÁü≥Ôºâ„ÄÇ\n- **Test 2**: ÂØÜÈõÜ‰∏éÁ®ÄÁñèÂú∞ÂΩ¢ÁöÑÊ∑∑ÂêàÔºàÂ¶ÇÂÖ¨Âõ≠Ë∑ëÈÖ∑‰∏éÊ¢ÅÁöÑÁªÑÂêàÔºâ„ÄÇ\n- **Test 3**: È´òÊïèÊç∑ÊÄßË¶ÅÊ±ÇÂú∞ÂΩ¢ÔºàÂ¶ÇÂ§çÊùÇÁöÑÂÖ¨Âõ≠Ë∑ëÈÖ∑ÈöúÁ¢çÂú∫Ôºâ„ÄÇ\n- **Test 4**: ÊîÄÁà¨‰∏éÁ®ÄÁñèÂú∞ÂΩ¢ÁöÑÊ∑∑Âêà„ÄÇ\n\n**Ê≠§Â§ñ**ÔºåÂú®**ÁúüÂÆû‰∏ñÁïå**‰∏≠ÔºåËÆ∫ÊñáÂú®Â§öÁßçÊûÅÂÖ∑ÊåëÊàòÊÄßÁöÑ**ÁúüÂÆû„ÄÅÊú™ËßÅÂú∞ÂΩ¢**‰∏äËøõË°å‰∫ÜÊµãËØïÔºå‰æãÂ¶ÇÔºö\n- Â§çÊùÇÁöÑÂÖ¨Âõ≠Ë∑ëÈÖ∑ËµõÈÅìÔºàÂåÖÂê´È´òÂè∞„ÄÅÈó¥ÈöôÔºâ„ÄÇ\n- Áã≠Á™ÑÂπ≥Ë°°Êú®„ÄÅÊµÆÂä®ÊñπÂùóÊûÑÊàêÁöÑÂºØÊõ≤Ê¢Å„ÄÅÂ∏¶Èó¥ÈöôÁöÑÊ¢Å„ÄÅÈíªÁü≥Â∏ÉÂ±ÄÁöÑË∏èËÑöÁü≥Á≠â„ÄÇ\n- È´òÂπ≥Âè∞„ÄÅÈó¥Èöô„ÄÅÊ•ºÊ¢Ø„ÄÅÁ≤óÁ≥ôÂú∞ÂΩ¢ÁöÑÁªÑÂêà„ÄÇ\n\n### ‰∫å„ÄÅ ‰∏ªË¶ÅËØÑ‰º∞ÊåáÊ†á\n\nËÆ∫ÊñáÈááÁî®**ÂÆöÈáè**‰∏é**ÂÆöÊÄß**Áõ∏ÁªìÂêàÁöÑËØÑ‰º∞ÊñπÂºèÔºö\n\n1.  **‰∏ªË¶ÅÂÆöÈáèÊåáÊ†áÔºöÊàêÂäüÁéá**\n    - Âú®‰ªøÁúü‰∏≠ÔºåÂØπÊØè‰∏™ÊµãËØïÂú∞ÂΩ¢ËøõË°åÂ§öÊ¨°rolloutÔºåËÆ°ÁÆó**ÊàêÂäüÂà∞ËææÁõÆÊ†á‰ΩçÁΩÆÂπ∂‰øùÊåÅÁ®≥ÂÆö**ÁöÑÊØîÁéá„ÄÇ\n    - ËøôÊòØË°°Èáè**Ê≥õÂåñËÉΩÂäõ**Âíå**È≤ÅÊ£íÊÄß**ÁöÑÊ†∏ÂøÉÊåáÊ†á„ÄÇ\n\n2.  **ÊïèÊç∑ÊÄßËØÑ‰º∞ÊåáÊ†áÔºàÂÆöÊÄß‰∏∫‰∏ªÔºåÂÆöÈáè‰∏∫ËæÖÔºâ**\n    - **Âú∞ÂΩ¢ÈöæÂ∫¶ÊûÅÈôê**ÔºöÊú∫Âô®‰∫∫ËÉΩÂ§üÊîÄÁà¨/Áà¨‰∏ãÁöÑÊúÄÂ§ßÂπ≥Âè∞È´òÂ∫¶„ÄÅËÉΩÂ§üË∑®Ë∂äÁöÑÊúÄÂ§ßÈó¥ÈöôÁ≠âÔºå‰∏éÂÖàÂâçÂ∑•‰ΩúÂØπÊØî„ÄÇ\n    - **ËøêÂä®ÈÄüÂ∫¶**ÔºöËÆ∞ÂΩïÊú∫Âô®‰∫∫Âú®Â§çÊùÇÂú∞ÂΩ¢‰∏äËÉΩËææÂà∞ÁöÑÂ≥∞ÂÄºÂâçËøõÈÄüÂ∫¶Ôºà>1.5 m/sÔºâ„ÄÇ\n    - **‰ªªÂä°ÂÆåÊàêÂ∫¶**ÔºöËÉΩÂê¶Èõ∂Ê†∑Êú¨Ôºàzero-shotÔºâÂÆåÊàêÂÖàÂâçÂ∑•‰Ωú‰∏≠ÊúÄÂÖ∑ÊåëÊàòÊÄßÁöÑÂú∞ÂΩ¢ÔºàÂ¶ÇÊúÄÈöæÁöÑÂÖ¨Âõ≠Ë∑ëÈÖ∑ÂíåÁ¢éÁü≥Â†ÜÔºâ„ÄÇ\n\n3.  **Êò†Â∞ÑË¥®ÈáèËØÑ‰º∞ÊåáÊ†á**\n    - ‰ΩøÁî® **`L‚ÇÄ.‚ÇÖ`ÊçüÂ§±**ÔºàÂÖ¨Âºè9ÔºâËØÑ‰º∞Á•ûÁªèÊò†Â∞ÑÁÆ°ÈÅìÈ¢ÑÊµãÈ´òÁ®ãÁöÑÂáÜÁ°ÆÊÄßÂíå‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°ÁöÑÂêàÁêÜÊÄß„ÄÇËØ•ÊçüÂ§±Âπ≥Ë°°‰∫Ü‰º∞ËÆ°ËØØÂ∑ÆÂíå‰∏çÁ°ÆÂÆöÊÄßÊ†°ÂáÜ„ÄÇ\n\n4.  **È≤ÅÊ£íÊÄßËØÑ‰º∞ÊåáÊ†á**\n    - Âú®‰ªøÁúü‰∏≠ÊµãËØïÊéßÂà∂Âô®Âú®**‰º†ÊÑüÂô®ÈÄÄÂåñ**ÔºàÂ¶ÇÁÇπ‰∫ëÁº∫Â§±„ÄÅÂô™Â£∞‰º™ÂΩ±„ÄÅÁõ∏Êú∫Á¶ÅÁî®ÔºâÊÉÖÂÜµ‰∏ãÁöÑÊàêÂäüÁéá„ÄÇ\n\n### ‰∏â„ÄÅ ÊÄßËÉΩÊèêÂçá‰∏éÂÖ≥ÈîÆÁªìÊûú\n\n#### 1. Ê≥õÂåñÊÄßËÉΩÊòæËëóÊèêÂçáÔºàÊ†∏ÂøÉË¥°ÁåÆÔºâ\n- **AME-2 vs. Âü∫Á∫ø**ÔºöÂú®Êú™ËßÅËøáÁöÑÊ∑∑ÂêàÊµãËØïÂú∞ÂΩ¢‰∏äÔºåAME-2ÊïôÂ∏àÁ≠ñÁï•ÁöÑÂπ≥ÂùáÊàêÂäüÁéáÈ´òËææ**95.2%**ÔºåÊòæËëó‰ºò‰∫é‰πãÂâçÁöÑAME-1Êû∂ÊûÑÔºà**51.2%**ÔºâÂíåMoEÊû∂ÊûÑÔºà**45.0%**Ôºâ„ÄÇËøôËØÅÊòé‰∫Ü**ÂºïÂÖ•ÂÖ®Â±Ä‰∏ä‰∏ãÊñáÁâπÂæÅÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂**ÂØπÊ≥õÂåñËá≥ÂÖ≥ÈáçË¶Å„ÄÇ\n- **AME-2Â≠¶Áîü vs. Á´ØÂà∞Á´ØËßÜËßâÂ≠¶Áîü**Ôºö‰∏éÂΩìÂâçÂÖàËøõÁöÑÁ´ØÂà∞Á´ØÊïèÊç∑ÈÄöÁî®Á≠ñÁï•Áõ∏ÊØîÔºåAME-2Â≠¶ÁîüÁ≠ñÁï•Âú®Âπ≥ÂùáÊµãËØïÊàêÂäüÁéá‰∏äÂèñÂæó**82.4%** vs. **51.5%** ÁöÑÊòæËëó‰ºòÂäøÔºåÂ∞§ÂÖ∂Âú®Á®ÄÁñèÂíåÊ∑∑ÂêàÂú∞ÂΩ¢‰∏äË°®Áé∞Êõ¥‰ºò„ÄÇ\n\n#### 2. ÂÆûÁé∞ÊïèÊç∑‰∏éÊ≥õÂåñÁöÑÁªü‰∏Ä\n- **Èõ∂Ê†∑Êú¨Ê≥õÂåñ**Ôºö‰ªÖÂú®Âü∫Á°ÄÂú∞ÂΩ¢‰∏äËÆ≠ÁªÉÁöÑÊéßÂà∂Âô®ÔºåËÉΩÂ§ü**Èõ∂Ê†∑Êú¨**ÂÆåÊàêÁúüÂÆû‰∏ñÁïå‰∏≠Â§çÊùÇÁöÑÂÖ¨Âõ≠Ë∑ëÈÖ∑ËµõÈÅìÔºàÂõæ10ÔºâÂíåÂ§öÁßçÁ®ÄÁñèÂú∞ÂΩ¢ÔºàÂõæ11ÔºâÔºåËØÅÊòé‰∫ÜÂÖ∂Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ\n- **Â™≤ÁæéÊúÄÂÖàËøõÊïèÊç∑ÊñπÊ≥ïÁöÑÊÄßËÉΩ**ÔºöÂú®ÊïèÊç∑ÊÄßÊñπÈù¢ÔºåANYmal-DÊéßÂà∂Âô®ÂèØ‰ª•Èõ∂Ê†∑Êú¨ÂÆåÊàêÂÖàÂâçÂ∑•‰ΩúÔºàÂ¶ÇANYmal ParkourÔºâ‰∏≠ÊúÄÂÖ∑ÊåëÊàòÊÄßÁöÑÂú∞ÂΩ¢„ÄÇTRON1ÂèåË∂≥Êú∫Âô®‰∫∫ÂÆûÁé∞‰∫ÜÈ´òËææ0.48mÁöÑÊîÄÁà¨Âíå0.88mÁöÑ‰∏ãÁà¨È´òÂ∫¶ÔºåÊÄßËÉΩ‰∏éÁ°¨‰ª∂ËÉΩÂäõÊõ¥Âº∫ÁöÑÂèåË∂≥Êú∫Âô®‰∫∫Â∑•‰ΩúÁõ∏ÂΩì„ÄÇ\n\n#### 3. Êò†Â∞ÑÁÆ°ÈÅìÈ´òÊïà‰∏îÈ≤ÅÊ£í\n- **ÂÆûÊó∂ÊÄß**ÔºöÊï¥‰∏™Êò†Â∞ÑÁÆ°ÈÅìÂú®ÊùøËΩΩCPU‰∏ä‰ªÖÈúÄÁ∫¶**5ms/Â∏ß**ÔºåÊîØÊåÅ50HzÁöÑÊéßÂà∂È¢ëÁéá„ÄÇ\n- **‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•**ÔºöËÉΩÂ§üÊòæÂºèÂª∫Ê®°ÈÅÆÊå°ÂíåÂô™Â£∞Âå∫ÂüüÔºàÈ´ò‰∏çÁ°ÆÂÆöÊÄßÔºâÔºå‰∏∫ÊéßÂà∂Âô®Êèê‰æõÊõ¥ÂèØÈù†ÁöÑÁéØÂ¢ÉË°®Á§∫„ÄÇ\n- **ËÆ≠ÁªÉÊïàÁéá**ÔºöËΩªÈáèÁ∫ßËÆæËÆ°ÂÖÅËÆ∏Âú®Êï∞ÂçÉ‰∏™Âπ∂Ë°å‰ªøÁúüÁéØÂ¢É‰∏≠ËøêË°åÔºå‰∏éÊéßÂà∂Âô®ËÆ≠ÁªÉÂæ™ÁéØÈõÜÊàêÔºåÂä©Âäõ‰ªøÁúüÂà∞ÁúüÂÆûÁöÑËøÅÁßª„ÄÇ\n\n#### 4. Ê∂åÁé∞Âá∫È´òÁ∫ßË°å‰∏∫ËÉΩÂäõ\n- **‰∏ªÂä®ÊÑüÁü•**ÔºöÊú∫Âô®‰∫∫ÈÄöËøá‰∏éÁéØÂ¢É‰∫§‰∫íÊù•Êõ¥Êñ∞Âú∞ÂõæÔºå‰ªéËÄåÂú®ÂêéÁª≠Â∞ùËØï‰∏≠Êõ¥ÊàêÂäüÂú∞ÂÆåÊàê‰ªªÂä°ÔºàÂõæ15Ôºâ„ÄÇ\n- **Â±ÄÈÉ®ÂØºËà™**ÔºöÊéßÂà∂Âô®Â±ïÁé∞Âá∫ÈÅøÈöúÂíåÂØπÈΩêÂú∞ÂΩ¢Á≠âÂ±ÄÈÉ®ÂØºËà™ËÉΩÂäõ„ÄÇ\n- **ÂÖ®Ë∫´Êé•Ëß¶‰∏éÂÜ≤ÂáªÂáèÁºì**ÔºöÊú∫Âô®‰∫∫Ëá™Âèë‰ΩøÁî®ËÜùÁõñÁ≠âÈùûË∂≥ÈÉ®Êé•Ëß¶ÁÇπËøõË°åÁ®≥ÂÆöÔºåÂπ∂Ë°®Áé∞Âá∫ËΩªÊüîËêΩÂú∞ÁöÑË°å‰∏∫„ÄÇ\n\n**ÊÄªÁªì**ÔºöAME-2Ê°ÜÊû∂ÈÄöËøáÂú®**Áªü‰∏ÄÊû∂ÊûÑ**‰∏ãÁªìÂêà**Ê≥®ÊÑèÂäõÁ•ûÁªèÂú∞ÂõæÁºñÁ†Å**Âíå**ËΩªÈáèÁ∫ß‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•Á•ûÁªèÊò†Â∞Ñ**ÔºåÊàêÂäüÂÆûÁé∞‰∫ÜÂÖàÂâçÂ∑•‰Ωú‰∏≠Èöæ‰ª•ÂÖºÈ°æÁöÑ**È´òÊ∞¥Âπ≥ÊïèÊç∑ÊÄß**‰∏é**Âº∫Â§ßÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõ**ÔºåÂπ∂Âú®ÂõõË∂≥ÂíåÂèåË∂≥‰∏§ÁßçÊú∫Âô®‰∫∫Âπ≥Âè∞‰∏äÂæóÂà∞‰∫ÜÈ™åËØÅ„ÄÇÂÖ∂ÊÄßËÉΩÂú®‰ªøÁúüÂíåÁúüÂÆû‰∏ñÁïåÂÆûÈ™å‰∏≠ÈÉΩË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑ‰∏ìÊ≥®‰∫éÊïèÊç∑ÊàñÊ≥õÂåñÁöÑÊñπÊ≥ï„ÄÇ",
            "timestamp": "2026-01-20T17:56:07.817828",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ËÆ∫ÊñáÁöÑ‰∏ªË¶ÅÂ±ÄÈôêÊÄßÊúâÂì™‰∫õÔºüÊú™Êù•ÂèØËÉΩÁöÑÊîπËøõÊñπÂêëÊòØ‰ªÄ‰πàÔºü",
            "answer": "## ËÆ∫Êñá‰∏ªË¶ÅÂ±ÄÈôêÊÄßÂàÜÊûê\n\nÂü∫‰∫éÂØπËÆ∫ÊñáÂÜÖÂÆπÁöÑÂàÜÊûêÔºåAME-2Ê°ÜÊû∂ÂèäÂÖ∂Áõ∏ÂÖ≥ÊäÄÊúØÂ≠òÂú®‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÂ±ÄÈôêÊÄßÔºö\n\n**1. ÁéØÂ¢ÉÊÑüÁü•‰∏éË°®Á§∫ÁöÑÂ±ÄÈôêÊÄß**\n- **‰æùËµñ2.5Áª¥È´òÁ®ãÂõæ**ÔºöÁ≥ªÁªü‰ΩøÁî®2.5Áª¥È´òÁ®ãÂõæÔºàÂç≥ÊØè‰∏™`(x, y)`ÂùêÊ†áÂØπÂ∫î‰∏Ä‰∏™È´òÂ∫¶`z`ÔºâÔºåÊó†Ê≥ïÂ§ÑÁêÜÁúüÊ≠£ÁöÑ‰∏âÁª¥ÁªìÊûÑÔºàÂ¶ÇÊÇ¨ÂûÇÁâ©„ÄÅÊ¥ûÁ©¥„ÄÅÂ§öÂ±ÇÁªìÊûÑÔºâ„ÄÇËøôÈôêÂà∂‰∫ÜÊú∫Âô®‰∫∫Âú®Â§çÊùÇ‰∏âÁª¥ÁéØÂ¢ÉÔºàÂ¶ÇËÑöÊâãÊû∂„ÄÅÊúâÈ°∂Ê£öÁöÑÈÄöÈÅìÔºâ‰∏≠ÁöÑÊìç‰ΩúËÉΩÂäõ„ÄÇ\n- **ÂØπ‰∏•ÈáçÈÄÄÂåñÁöÑÊÑüÁü•Âú∫ÊôØÈ≤ÅÊ£íÊÄß‰∏çË∂≥**ÔºöËÆ∫ÊñáÊåáÂá∫ÔºåÊéßÂà∂Âô®Âπ∂Èùû‰∏∫È´òËçâ‰∏õ„ÄÅÁßØÈõ™Á≠â‰∏•ÈáçÈÅÆÊå°ÊàñÂπ≤Êâ∞Ê∑±Â∫¶‰º†ÊÑüÂô®ÁöÑÁéØÂ¢ÉËÆæËÆ°„ÄÇÂú®Ëøô‰∫õÂú∫ÊôØ‰∏ãÔºåÊò†Â∞ÑÂíåÊéßÂà∂ÁöÑÊÄßËÉΩÂèØËÉΩ‰ºöÊòæËëó‰∏ãÈôç„ÄÇ\n\n**2. ÊéßÂà∂Á≠ñÁï•‰∏éÊ≥õÂåñËÉΩÂäõÁöÑÂ±ÄÈôêÊÄß**\n- **ÊäÄËÉΩËøáÊ∏°ÁöÑÂ§±Ë¥•**ÔºöÂú®Êú™ËßÅËøáÁöÑÂ§çÊùÇÊµãËØïÂú∞ÂΩ¢ÔºàÂ∞§ÂÖ∂ÊòØÈúÄË¶Å‰∏çÂêåËøêÂä®ÊäÄËÉΩÂø´ÈÄüÂàáÊç¢ÁöÑÂú∞ÂΩ¢ÔºåÂ¶ÇÁ®ÄÁñèËêΩËÑöÁÇπÂêéÊé•ÊîÄÁà¨Ôºâ‰∏äÔºåÂ§±Ë¥•ÁéáÈ´ò‰∫éËÆ≠ÁªÉÂú∞ÂΩ¢„ÄÇËøôË°®ÊòéÁ≠ñÁï•Âú®**Èõ∂Ê†∑Êú¨Ê≥õÂåñ**Â§ÑÁêÜÂ§çÊùÇÊäÄËÉΩÂ∫èÂàóÊñπÈù¢‰ªçÊúâÊèêÂçáÁ©∫Èó¥„ÄÇ\n- **ËøêÂä®Ê®°ÂºèÂ§öÊ†∑ÊÄßÊúâÈôê**ÔºöÂØπ‰∫éÁªôÂÆöÁöÑÂú∞ÂΩ¢Á±ªÂûãÔºåÂ≠¶‰π†Âà∞ÁöÑÊé•Ëß¶Ê®°ÂºèË∂ã‰∫éÁõ∏‰ºº„ÄÇËÆ∫ÊñáË¥®ÁñëÔºåÂØπ‰∫éËá™Áî±Â∫¶Êõ¥È´òÁöÑÁ≥ªÁªüÔºàÂ¶Ç‰∫∫ÂΩ¢Êú∫Âô®‰∫∫ÔºâÔºåÈù¢ÂØπÂêå‰∏ÄÂú∞ÂΩ¢Á±ªÂûãÁöÑ‰∏çÂêåÂ∞∫Â∫¶Êó∂ÔºåAME-2ÊòØÂê¶ËÉΩÂú®Ê≤°ÊúâÈ¢ùÂ§ñÂÖàÈ™åÁü•ËØÜÁöÑÊÉÖÂÜµ‰∏ãÔºåËá™Âä®ÂèëÁé∞Â§öÊ†∑ÂåñÁöÑÂÖ®Ë∫´Êé•Ëß¶Á≠ñÁï•ÔºàÂ¶ÇÂçïËÑöË∏©„ÄÅÂèåËÑöË∑≥„ÄÅÊâãËÑöÂπ∂Áî®ÊîÄÁà¨Ôºâ„ÄÇ\n\n**3. Á≥ªÁªüËÆæËÆ°‰∏é‰∫§‰∫íÁöÑÂ±ÄÈôêÊÄß**\n- **ÁõÆÊ†áÂà∞Ëææ‰∏éÁõ¥Êé•ÊéßÂà∂ÁöÑÊùÉË°°**ÔºöÈááÁî®**ÁõÆÊ†áÂà∞Ëææ**ËÄåÈùû**ÈÄüÂ∫¶Ë∑üË∏™**ÁöÑÊåá‰ª§ÂΩ¢ÂºèÔºåËôΩÁÑ∂ÈºìÂä±‰∫ÜÊõ¥ÊïèÊç∑ÁöÑË°å‰∏∫Âπ∂Èôç‰Ωé‰∫ÜÂØπÈ´òÈ¢ëÂØºËà™Êåá‰ª§ÁöÑ‰æùËµñÔºå‰ΩÜ‰πüÂâäÂº±‰∫ÜÂØπÊú∫Âô®‰∫∫‰∏≠Èó¥ËøêÂä®Ë∑ØÂæÑÁöÑÁõ¥Êé•ÊéßÂà∂„ÄÇÂΩìÂ≠òÂú®ËØ≠‰πâ‰∏ä‰∏çÂèØÈÄöË°å‰ΩÜÂá†‰Ωï‰∏äÁúã‰ººÂèØÈÄöË°åÁöÑÈöúÁ¢çÁâ©Êó∂ÔºåËøôÂèØËÉΩÂ∏¶Êù•ÈóÆÈ¢ò„ÄÇ\n- **Âä®ÊÄÅÁéØÂ¢ÉÂ§ÑÁêÜËÉΩÂäõÊúâÈôê**ÔºöÊò†Â∞ÑÊ®°ÂùóÂú®Â≠òÂú®‰∏•ÈáçÈÅÆÊå°ÁöÑ**È´òÂ∫¶Âä®ÊÄÅÁéØÂ¢É**‰∏≠ÂèØËÉΩÂ§±Êïà„ÄÇÂΩìÂâçÁ≥ªÁªüÊú™ÊòéÁ°ÆÂª∫Ê®°Âú∫ÊôØ‰∏≠ÁöÑÁßªÂä®Áâ©‰ΩìÔºàÂ¶ÇÁßªÂä®ÁöÑÁÆ±Â≠ê„ÄÅÂºÄÂÖ≥ÁöÑÈó®Ôºâ„ÄÇ\n\n**4. ËÆ°ÁÆó‰∏éÈÉ®ÁΩ≤ÁöÑÁõ∏ÂÖ≥ËÄÉÈáè**\n- **ËÆ≠ÁªÉÊàêÊú¨ËæÉÈ´ò**ÔºöANYmal-DÁ≠ñÁï•ÁöÑËÆ≠ÁªÉÈúÄË¶ÅÁ∫¶60‰∏™RTX-4090ÊòæÂç°Êó•„ÄÇËôΩÁÑ∂ÂèñÂæó‰∫ÜÂº∫Â§ßÊÄßËÉΩÔºå‰ΩÜËæÉÈ´òÁöÑËÆ°ÁÆóËµÑÊ∫êÈúÄÊ±ÇÂèØËÉΩÂΩ±ÂìçÊñπÊ≥ïÁöÑÂèØÂèäÊÄßÂíåÂø´ÈÄüËø≠‰ª£„ÄÇ\n\n---\n\n## Êú™Êù•ÂèØËÉΩÁöÑÊîπËøõÊñπÂêë\n\nÈíàÂØπ‰∏äËø∞Â±ÄÈôêÊÄßÔºåËÆ∫ÊñáÊèêÂá∫‰∫ÜÊòéÁ°ÆÁöÑÊú™Êù•Â∑•‰ΩúÊñπÂêëÔºåÂπ∂ÊöóÁ§∫‰∫ÜÂÖ∂‰ªñÊΩúÂú®ÁöÑÁ†îÁ©∂Ë∑ØÂæÑÔºö\n\n**1. Â¢ûÂº∫ÁéØÂ¢ÉÊÑüÁü•‰∏éË°®ÂæÅ**\n- **Âêë3DË°®ÂæÅÊâ©Â±ï**ÔºöÊé¢Á¥¢‰∏éÁ≠ñÁï•Êû∂ÊûÑÂÖºÂÆπÁöÑ**Â§öÂ±ÇÈ´òÁ®ãÂõæ**Êàñ**Âü∫‰∫éÊ≥®ÊÑèÂäõÁöÑ‰ΩìÁ¥†Ë°®ÂæÅ**Ôºå‰ª•ÊîØÊåÅÂÆåÂÖ®‰∏âÁª¥ÁéØÂ¢É‰∏ãÁöÑËøêÂä®„ÄÇ\n- **ÊèêÂçáÂØπÈÄÄÂåñÊÑüÁü•ÁöÑÈ≤ÅÊ£íÊÄß**ÔºöÂ∞Ü‰∏ìÈó®ÈíàÂØπÂô™Â£∞ÂíåÈÅÆÊå°ËÆæËÆ°ÁöÑÈ≤ÅÊ£íÊéßÂà∂Âô®ÔºàÂ¶ÇÊñáÁåÆ‰∏≠ÁöÑ‰øùÂÆàÁ≠ñÁï•Ôºâ‰∏éAME-2ÁöÑÊïèÊç∑ÈÄöÁî®ÊéßÂà∂Âô®ÁªìÂêàÔºåÂΩ¢Êàê‰∏Ä‰∏™**Âú∫ÊôØÊÑüÁü•ÁöÑÂçï‰∏ÄÁ≠ñÁï•**„ÄÇ\n\n**2. ÊèêÂçáÊéßÂà∂Á≠ñÁï•ÁöÑÊ≥õÂåñ‰∏éÂ§öÊ†∑ÊÄß**\n- **Á†îÁ©∂ÂèØÊâ©Â±ïÁöÑÈõ∂Ê†∑Êú¨ÊäÄËÉΩËøáÊ∏°**ÔºöÊé¢Á¥¢ËÉΩÂê¶ÈÄöËøáÊõ¥ÂéüÂàôÊÄßÁöÑÊñπÊ≥ïÔºàÂ¶ÇÂàÜÂ±ÇÂº∫ÂåñÂ≠¶‰π†„ÄÅËØæÁ®ãËÆæËÆ°ÁöÑÊîπËøõ„ÄÅÂºïÂÖ•ÊäÄËÉΩÊ®°ÂùóÔºâÊù•Â≠¶‰π†ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊäÄËÉΩËøáÊ∏°ÔºåÂÆûÁé∞Êõ¥Â•ΩÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñ„ÄÇ\n- **ÁªìÂêàÂ§öÁßçÊåá‰ª§Êé•Âè£**ÔºöÂú®Âçï‰∏ÄÊéßÂà∂Âô®ÂÜÖÊï¥Âêà**ÁõÆÊ†áÂà∞Ëææ**Âíå**ÈÄüÂ∫¶Ë∑üË∏™**‰∏§ÁßçÊåá‰ª§Ê®°ÂºèÔºå‰æãÂ¶ÇÈÄöËøáÊé©Á†ÅÂëΩ‰ª§ÂíåËí∏È¶èÊäÄÊúØÔºå‰ª•ÂÖºÈ°æÊïèÊç∑ÊÄß‰∏éÁ≤æÁ°ÆË∑ØÂæÑÊéßÂà∂„ÄÇ\n- **ÂºïÂÖ•ËøêÂä®ÂÖàÈ™å**ÔºöÂØπ‰∫éÈ´òËá™Áî±Â∫¶Á≥ªÁªüÔºàÂ¶Ç‰∫∫ÂΩ¢Êú∫Âô®‰∫∫ÔºâÔºåÂèØ‰ª•ÂºïÂÖ•**ËøêÂä®ÂèÇËÄÉÊàñÂÖàÈ™åÁü•ËØÜ**ÔºàÂ¶Ç‰∫∫Á±ªËøêÂä®Êï∞ÊçÆ„ÄÅÊºîÁ§∫ÔºâÔºå‰ª•ÈºìÂä±ÂèëÁé∞Êõ¥Â§öÊ†∑Âåñ„ÄÅÁ¨¶ÂêàÁâ©ÁêÜÁõ¥ËßâÁöÑÂÖ®Ë∫´Êé•Ëß¶Ê®°Âºè„ÄÇ\n\n**3. ÊîπËøõÁ≥ªÁªüÂØπÂä®ÊÄÅ‰∏éËØ≠‰πâÁöÑÁêÜËß£**\n- **ÊòæÂºèÊé®ÁêÜÂä®ÊÄÅÁâ©‰Ωì**ÔºöÊâ©Â±ïÊò†Â∞ÑÊ®°ÂùóÔºå‰ΩøÂÖ∂ËÉΩÂ§üËØÜÂà´„ÄÅÈ¢ÑÊµãÂπ∂ËûçÂêàÂú∫ÊôØ‰∏≠**ËøêÂä®ÂÖÉÁ¥†**ÁöÑÁä∂ÊÄÅÔºå‰ªéËÄåÂú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ÂÅöÂá∫Êõ¥ÂÆâÂÖ®ÁöÑÂÜ≥Á≠ñ„ÄÇ\n- **ËûçÂêàËØ≠‰πâ‰ø°ÊÅØ**ÔºöÂ∞ÜÂá†‰ΩïÂú∞Âõæ‰∏é**ËØ≠‰πâÊÑüÁü•**Áõ∏ÁªìÂêàÔºåÂ∏ÆÂä©Êú∫Âô®‰∫∫ÁêÜËß£‚ÄúÂá†‰Ωï‰∏äÂèØÈÄöËøá‰ΩÜËØ≠‰πâ‰∏äÂç±Èô©‚ÄùÁöÑÂå∫ÂüüÔºàÂ¶ÇÊùæÂä®ÁöÑÁü≥Êùø„ÄÅÁéªÁíÉË°®Èù¢ÔºâÔºåÂº•Ë°•Á∫ØÂá†‰ΩïÊéßÂà∂ÁöÑ‰∏çË∂≥„ÄÇ\n\n**4. ‰ºòÂåñËÆ≠ÁªÉÊïàÁéá‰∏éÊ°ÜÊû∂ÈÄöÁî®ÊÄß**\n- **Èôç‰ΩéËÆ≠ÁªÉÊàêÊú¨**ÔºöÁ†îÁ©∂Êõ¥È´òÊïàÁöÑËÆ≠ÁªÉÁÆóÊ≥ï„ÄÅÁΩëÁªúÊû∂ÊûÑÊàñËØæÁ®ãÂ≠¶‰π†Á≠ñÁï•Ôºå‰ª•Âú®‰øùÊåÅÊÄßËÉΩÁöÑÂêåÊó∂ÂáèÂ∞ëËÆ°ÁÆóÈúÄÊ±Ç„ÄÇ\n- **Êé¢Á¥¢Êõ¥Áªü‰∏ÄÁöÑÊ°ÜÊû∂**ÔºöËÆ∫ÊñáÂ∞ùËØïÂú®Ê®°ÂùóÂåñÔºàÊÑüÁü•-ËßÑÂàí-ÊéßÂà∂ÔºâÂíåÁ´ØÂà∞Á´ØÂ≠¶‰π†‰πãÈó¥Êû∂Ëµ∑Ê°•Ê¢Å„ÄÇÊú™Êù•ÂèØ‰ª•ÁªßÁª≠Ê∑±ÂåñËøô‰∏ÄÊñπÂêëÔºåÊé¢Á¥¢Â¶Ç‰ΩïÊõ¥‰ºòÈõÖÂú∞Â∞ÜÁªèÂÖ∏ÊñπÊ≥ïÁöÑÂèØËß£ÈáäÊÄß„ÄÅÂÆâÂÖ®ÊÄß‰ºòÂäø‰∏éÂ≠¶‰π†ÊñπÊ≥ïÁöÑÁÅµÊ¥ªÊÄß„ÄÅÈÄÇÂ∫îÊÄßÁõ∏ÁªìÂêà„ÄÇ\n\n**ÊÄªÁªìËÄåË®Ä**ÔºåAME-2Âú®ÊïèÊç∑‰∏éÈÄöÁî®ÁöÑË∂≥ÂºèËøêÂä®ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÖ∂Â±ÄÈôêÊÄß‰∏ªË¶ÅÈõÜ‰∏≠Âú®**‰∏âÁª¥ÁéØÂ¢ÉÂ§ÑÁêÜ„ÄÅÊûÅÁ´ØÊÑüÁü•ÈÄÄÂåñ„ÄÅÂ§çÊùÇÊäÄËÉΩÂ∫èÂàóÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñ„ÄÅ‰ª•ÂèäÂØπÂä®ÊÄÅÂíåËØ≠‰πâ‰ø°ÊÅØÁöÑÁêÜËß£**‰∏ä„ÄÇÊú™Êù•ÁöÑÊîπËøõÂ∞ÜÊ≤øÁùÄ**Â¢ûÂº∫ÁéØÂ¢ÉË°®ÂæÅ„ÄÅÊèêÂçáÁ≠ñÁï•Ê≥õÂåñËÉΩÂäõ„ÄÅËûçÂêàÂ§öÊ®°ÊÄÅ‰ø°ÊÅØ‰ª•Âèä‰ºòÂåñÁ≥ªÁªüÊïàÁéá**Á≠âÊñπÂêëÂ±ïÂºÄ„ÄÇ",
            "timestamp": "2026-01-20T17:56:44.460077",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-13T12:18:50Z",
    "created_at": "2026-01-20T17:49:43.727148",
    "updated_at": "2026-01-20T17:49:43.727155"
}