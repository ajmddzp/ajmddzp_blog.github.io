{
    "id": "2601.07813v1",
    "title": "Data-driven control of hydraulic impact hammers under strict operational and control constraints",
    "authors": [
        "Francisco Leiva",
        "Claudio Canales",
        "Michelle Valenzuela",
        "Javier Ruiz-del-Solar"
    ],
    "abstract": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹é™æ€æ¶²å‹å†²å‡»é”¤ï¼ˆäº¦ç§°å²©çŸ³ç ´ç¢æœºï¼Œå¸¸ç”¨äºé‡‡çŸ¿è¡Œä¸šï¼‰çš„æ•°æ®é©±åŠ¨æ§åˆ¶æ–¹æ³•ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ§åˆ¶å²©çŸ³ç ´ç¢æœºï¼Œä½¿å…¶æœ«ç«¯æ‰§è¡Œå™¨èƒ½å¤Ÿåˆ°è¾¾ä»»æ„ç›®æ ‡ä½å§¿ï¼Œè¿™åœ¨å¸¸è§„æ“ä½œä¸­æ˜¯å°†ç ´ç¢é”¤å‡†ç¡®å®šä½è‡³å¾…ç ´ç¢å²©çŸ³ä¸Šæ–¹çš„å¿…è¦æ­¥éª¤ã€‚æ‰€æå‡ºçš„æ–¹æ³•è€ƒè™‘äº†å¤šç§çº¦æŸæ¡ä»¶ï¼ŒåŒ…æ‹¬å› ä¼ æ„Ÿèƒ½åŠ›æœ‰é™å¯¼è‡´çš„æœªè§‚æµ‹çŠ¶æ€å˜é‡ï¼Œä»¥åŠåœ¨å…³èŠ‚å±‚é¢å¿…é¡»ä½¿ç”¨ç¦»æ•£æ§åˆ¶æ¥å£çš„ä¸¥æ ¼è¦æ±‚ã€‚\n\né¦–å…ˆï¼Œè¯¥æ–¹æ³•é€šè¿‡ç³»ç»Ÿè¾¨è¯†è·å–æ¶²å‹è‡‚çš„è¿‘ä¼¼åŠ¨åŠ›å­¦æ¨¡å‹ã€‚è¿™ä¸€è¿‡ç¨‹ä»…åˆ©ç”¨é¥æ“ä½œæ•°æ®ï¼Œé‡‡ç”¨ç›‘ç£å­¦ä¹ æ–¹å¼å®Œæˆã€‚éšåï¼Œåˆ©ç”¨å­¦ä¹ å¾—åˆ°çš„åŠ¨åŠ›å­¦æ¨¡å‹è®¾è®¡æ§åˆ¶å™¨ï¼Œä»¥å®ç°æœ«ç«¯æ‰§è¡Œå™¨å¯¹ç›®æ ‡ä½å§¿çš„ç²¾å‡†åˆ°è¾¾ã€‚åœ¨ç­–ç•¥åˆæˆé˜¶æ®µï¼Œæœ¬æ–‡åŒæ—¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰ç®—æ³•è¿›è¡Œå¯¹æ¯”ç ”ç©¶ã€‚\n\nä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬ä»¥é…å¤‡æ¶²å‹å†²å‡»é”¤ä½œä¸ºæœ«ç«¯æ‰§è¡Œå™¨çš„Bobcat E10å°å‹æŒ–æ˜æœºè‡‚çš„è‡ªåŠ¨åŒ–æ§åˆ¶ä¸ºå¯¹è±¡ã€‚åŸºäºè¯¥è®¾å¤‡ï¼Œåˆ†åˆ«åœ¨ä»¿çœŸç¯å¢ƒå’ŒçœŸå®åœºæ™¯ä¸­å¯¹ç³»ç»Ÿè¾¨è¯†å’Œç­–ç•¥åˆæˆé˜¶æ®µè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„æœ€ä¼˜ç­–ç•¥åœ¨çœŸå®ç¯å¢ƒä¸­èƒ½å¤Ÿç¨³å®šå®ç°æœ«ç«¯æ‰§è¡Œå™¨çš„ç›®æ ‡ä½å§¿å®šä½ï¼Œå…¶ä½ç½®è¯¯å·®å°äº12å˜ç±³ï¼Œä¿¯ä»°è§’è¯¯å·®å°äº0.08å¼§åº¦ã€‚è€ƒè™‘åˆ°å†²å‡»é”¤å‡¿å…·ç›´å¾„ä¸º4å˜ç±³ï¼Œè¯¥ç²¾åº¦æ°´å¹³å·²å®Œå…¨æ»¡è¶³å²©çŸ³ç ´ç¢ä½œä¸šéœ€æ±‚ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸€æˆæœä»…éœ€çº¦68åˆ†é’Ÿçš„é¥æ“ä½œæ•°æ®è¿›è¡Œè®­ç»ƒå’Œ8åˆ†é’Ÿæ•°æ®è¿›è¡ŒåŠ¨åŠ›å­¦æ¨¡å‹è¯„ä¼°ï¼Œä¸”åœ¨ç­–ç•¥ä»ä»¿çœŸåˆ°ç°å®çš„è¿ç§»è¿‡ç¨‹ä¸­æœªè¿›è¡Œä»»ä½•è°ƒæ•´å³å¯å®ç°ã€‚çœŸå®ç¯å¢ƒä¸­çš„ç­–ç•¥æ‰§è¡Œæ¼”ç¤ºå¯è§äºï¼šhttps://youtu.be/e-7tDhZ4ZgAã€‚",
    "url": "https://arxiv.org/abs/2601.07813v1",
    "html_url": "https://arxiv.org/html/2601.07813v1",
    "html_content": "Data-driven control of hydraulic impact hammers under strict operational and control constraints\nFrancisco Leiva\n1\n, Claudio Canales\n1\n, Michelle Valenzuela\n1\nand Javier Ruiz-del-Solar\n1\nThis work was supported by FONDECYT project 1251823, ANID-PIA project CIA250010, and ANID/Doctorado Nacional/2023-21232021.\n1\nAdvanced Mining Technology Center (AMTC) and Department of Electrical Engineering, Universidad de Chile, Tupper 2007, Santiago, Chile.\nfrancisco.leiva@ing.uchile.cl, claudio.canales@amtc.uchile.cl, michelle.valenzuela@amtc.uchile.cl, jruizd@ing.uchile.cl\nAbstract\nThis paper presents a data-driven methodology for the control of static hydraulic impact hammers, also known as rock breakers, which are commonly used in the mining industry. The task addressed in this work is that of controlling the rock-breaker so its end-effector reaches arbitrary target poses from any given initial configuration, which is required in normal operation to place the hammer on top of rocks that need to be fractured. The proposed approach considers several constraints, such as unobserved state variables due to limited sensing and the strict requirement of using a discrete control interface at the joint level. First, the proposed methodology addresses the problem of system identification in order to obtain an approximate dynamic model of the hydraulic arm. This is done via supervised learning, using only teleoperation data. The learned dynamic model is then exploited to obtain a controller capable of reaching target end-effector poses. For policy synthesis, both reinforcement learning (RL) and model predictive control (MPC) algorithms are utilized and contrasted. As a case study, we consider the automation of a BobcatÂ E10 mini-excavator arm with a hydraulic impact hammer attached as end-effector. Using this machine, both the system identification and policy synthesis stages are extensively studied in simulation and in the real world. The best RL-based policy consistently reaches target end-effector poses with position errors below 12 [cm] and pitch angle errors below 0.08 [rad] when deployed in the real world. Considering that the impact hammer has a 4 [cm] diameter chisel, this level of precision is sufficient for breaking rocks. Notably, this is accomplished by relying only on approximately 68Â min of teleoperation data to train and 8Â min to evaluate the dynamic model, and without performing any adjustments or fine-tuning for a successful policy Sim2Real transfer. A demonstration of policy execution in the real world can be found in\nhttps://youtu.be/e-7tDhZ4ZgA\n.\nI\nIntroduction\nThe increasing demand for automation comes with many challenges. This is particularly true when automating heavy-duty hydraulic machinery operating in unstructured environments. The automation of such systems is difficult due to their highly nonlinear dynamics, delayed responses, and the constraints associated with intervening manually operated machines that are already deployed in the field.\nThis work addresses the automation of hydraulic impact hammers (also known as rock-breakers) used in mining. These machines can be described as static hydraulic arms with an hydraulic impact hammer attached as an end-effector. Operators remotely control these machines to break rocks that are too large to fall through steel grates installed on top of ore passes, allowing material to be transported to lower production levels\n(Correa\net al.\n,\n2022\n)\n. Particularly, we address the problem of synthesizing a controller capable of reaching end-effector target poses (for instance, rock breaking poses) for these machines. This task, commonly known as\nâ€œreachingâ€\n, is constantly performed by human operators to place the impact hammer on top of rocks that need to be fractured, or to place it in safe areas whenever material is deposited on the steel grates by a load-haul-dump (LHD) machine. Fig.\n1\nillustrates the problem of reaching arbitrary target end-effector poses using a mini-excavator whose arm has the the same kinematic structure that full-sized rock breakers, and that has an hydraulic impact hammer attached as end-effector.\nThe main challenges associated with the automation of hydraulic impact hammers used in mining arise from the prohibition of any physical modification on the machines (which restricts sensing), the inability to accurately capture the system dynamics (as proprietary components often conceal their internal responses), and the requirement that control must be executed through discrete commands at the joint level (since this allows standardizing a common control interface for a fleet of impact hammers). These constraints originate from industrial requirements aiming to minimize intervention on machines that are already actively operating in production.\nFigure 1:\nDepiction of the reaching task performed by impact hammers in mining, using a Bobcat E10 mini-excavator. To perform this task, the hydraulic arm of the machine has to be controlled so its end-effector reaches target poses from arbitrary initial configurations, using minimal sensing and a discrete control interface at the joint level.\nTo achieve the automation goal given the above constraints, a two-stage data-driven methodology is adopted. In the first stage, the systemâ€™s dynamics is approximated by training a parameterized function using operational data, and then, in the second stage, the obtained dynamic model is exploited to synthesize reaching controllers. The synthesis of policies is conducted by leveraging an efficient computational pipeline, in which the learned model is used to predict the real systemâ€™s response to control commands. This allows generating experiences that are used to learn a controller using reinforcement learning (RL). In addition, a model predictive control (MPC) scheme is also implemented as a baseline. The obtained policies are then deployed and tested in the real world without performing any fine-tuning.\nThis methodology is implemented and validated to synthesize a reaching controller for a hydraulic mini-excavator. This mini-excavator is electro-hydraulically intervened to allow its automation, and in a manner that emulates the control challenges described for static rock-breakers used in mining. This results in unobserved state variables that makes classical dynamic modeling non-viable, and in a discrete control interface that makes accurately reaching target poses challenging.\nWith the above, the main contributions of this work are the following:\nâ€¢\nA practical methodology for automating hydraulic arms with unobserved variables and a discrete control interface, covering the full pipeline from system identification to controller deployment. By relying on a minimal sensing setup, the approach can be transferred to other machines with minimal adaptation. The results demonstrate that effective hydraulic control can be achieved even in conditions of low observability and with limited ground truth data, thereby streamlining the deployment of industrial automation.\nâ€¢\nA policy (obtained by exploiting a learned data-driven model) that perform the reaching task in a three-dimensional space. Unlike most existing controllers, the policy operates in a discrete action space, dictated by the hardware and operational requirements of the target application (secondary rock reduction in mining using hydraulic breakers). This design choice introduces additional complexity in both training and implementation. Despite these challenges, the controller achieves high accuracy, consistently reaching target poses with a position error below\n12\n12\n[cm] and an absolute orientation error below\n0.08\n0.08\n[rad]. Remarkably, the policy is transferred directly from simulation to the real machine without any parameter tuning.\nII\nRelated work\nThe automation of industrial machinery has been extensively explored in recent years, driven by the demand for increased efficiency, safety, and productivity in complex work environments. In this domain, hydraulic machines are unique because of their power and versatility. These machines are widely used in construction, mining, forestry, and demolition, where they perform physically demanding tasks such as excavation, rock breaking, material handling, and infrastructure maintenance. Their broad range of applications has made them a focal point in the study of robotic automation. Consequently, hydraulic system automation has been an active area of research for several years, with numerous works that address the unique challenges posed by their nonlinear dynamics, delayed responses, and the need for precise control under uncertain conditions\n(Mattila\net al.\n,\n2017\n)\n.\nTraditional approaches to hydraulic machine automation are predominantly model-based. Early works such as\nPlummer and Vaughan (\n1990\n); Sohl and Bobrow (\n1997\n); Habibi and Richards (\n1991\n); Sirouspour and Salcudean (\n2001\n)\naimed to achieve control by deriving dynamic models of the hydraulic system and applying different control techniques. For instance, in\nSohl and Bobrow (\n1997\n)\n, a non-linear tracking controller based on Lyapunov theory and backstepping is presented for a hydraulic servosystem, guaranteeing exponential stability for force tracking. However, the approach relies heavily on accurate parameter identification, such as valve gains and fluid bulk modulus, and requires access to signal derivatives, which are often noisy in practice. This class of requirements are difficult to fulfill in real-world industrial settings, often leading to complicated or limited implementations, compromising effectiveness, scalability, and robustness. This challenge is common across many classical model-based control strategies.\nEven today, model-based approaches remain an active area of research in hydraulic control. Modern studies continue to explore analytical control strategies, often combining model-based formulations with advanced techniques such as adaptive control, sliding mode control (SMC) and model predictive control (MPC). These methods frequently incorporate online parameter estimation or disturbance observers to improve performance under uncertainty. For example, in\nBender\net al.\n(\n2017\n)\n, an offsetâ€‘free MPC controller is implemented in a hydraulic mini-excavator, allowing precise control of the arm movement while handling disturbances. However, these methods still inherently rely on the assumed model structure, which remains a critical weakness.\nIn parallel, model-free control strategies remain widely used in practice, particularly proportional-integral-derivative (PID) control (e.g.,\nEgli\net al.\n(\n2024\n)\n), due to their simplicity and ease of implementation. Nevertheless, PID controllers require extensive tuning for each machine, and their performance tends to degrade as the system moves away from the operating point around which the gains were calibrated.\nGiven the difficulty of deriving accurate analytical models for hydraulic machines, data-driven approaches have long been considered a promising alternative. In\nSong and Koivo (\n1995\n)\n, it was proposed to learn the inverse dynamics of the plant (an hydraulic excavator) using operational data. More recently, this data-driven strategy has gained renewed interest, driven by advances in machine learning techniques. Works such as\nPark\net al.\n(\n2017\n)\n,\nLee\net al.\n(\n2022\n)\n,\nMa and Zhou (\n2024\n)\n,\nMa\net al.\n(\n2024\n)\nand\nGreiser\net al.\n(\n2024\n)\nuse modern machine learning techniques to model complex system behaviors from sensor and control data. While these approaches offer clear advantages, such as adaptability, reduced modeling effort, and the ability to capture non-linearities, they are not without challenges. Issues such as data quality, generalization to unseen conditions, or limited interpretability remain active areas of concern.\nAn alternative strategy to control hydraulic machines and learn complex tasks is reinforcement learning (RL). One of the main advantages of RL is its ability to learn control policies directly from interaction with the environment, without necessarily requiring an accurate model of the system dynamics. This is particularly appealing for hydraulic systems, which are notoriously difficult to model due to strong nonlinearities and complex internal dynamics, making classical control design a highly challenging task. Moreover, RL methods have shown promising results in simulation and even on real machines, successfully learning behaviors that are difficult to encode explicitly using conventional control techniques.\nHowever, it is important to note that RL is not without drawbacks. The design of a reward function that effectively captures the objective of a task often requires extensive domain knowledge and entails a substantial trial-and-error process. Additionally, policies trained in simulation may fail to transfer to real-world machines due to discrepancies between simulated and real dynamics, a well known issue referred to as the sim-to-real gap. To mitigate this, it is crucial to ensure that the simulated dynamics closely approximate those of the real system. As seen in\nEgli and Hutter (\n2020\n)\n,\nEgli and Hutter (\n2022\n)\n,\nSpinelli\net al.\n(\n2024\n)\nand\nSpinelli\net al.\n(\n2025\n)\n, a promising way to tackle this issue is the use of a data-driven approach to approximate the plant dynamics and then to rely on this learned model to synthesize an RL-based policy.\nIt is also worth mentioning that, even if many of these approaches offer promising results, they rely strongly on the observability of the system. For example, in works such as those of\nJud\net al.\n(\n2021\n)\n,\nEgli and Hutter (\n2020\n)\n,\nEgli and Hutter (\n2022\n)\n, and\nSpinelli\net al.\n(\n2024\n)\n, precise measurements for the joint positions and velocities, often alongside pressures, torques, engine RPM measurements, among others, are utilized to characterize the instantaneous state of the machines being controlled.\nRegarding the automation of hydraulic impact hammers (rock breakers) used in mining, prior work has reported the development of teleoperation, assisted teleoperation\n(Correa\net al.\n,\n2022\n)\n, controllers that solve a particular task of the full operating cycle of the rock breaker (e.g. rock breaking in\nSamtani\net al.\n(\n2023\n)\n) and complete modular automation systems\n(Lampinen\net al.\n,\n2021\n)\n.\nIn\nLampinen\net al.\n(\n2021\n)\n, for example, motion control is achieved by learning a mapping between input control signals and the resulting velocity for each valve-actuator pair, following the approach proposed by\nNurmi and Mattila (\n2017\n)\n. These learned models are then used to control the machine to track trajectories generated by a planner that takes into account the flow restrictions of the hammerâ€™s hydraulic unit, using the algorithm proposed by\nLampinen\net al.\n(\n2020\n)\n. In\nSamtani\net al.\n(\n2023\n)\nonly the rock-breaking stage of the machineâ€™s operation cycle (which starts once the end-effector is positioned over a rock) is automated via model-free RL.\nGiven the state of the art, this work follows the data-driven approach adopted by many recent works (e.g.,\nEgli and Hutter (\n2020\n,\n2022\n)\n): learning a model of the system dynamics, and then obtaining a policy by exploiting the learned model. However, in contrast to the existing literature, this work addresses the challenge of learning a dynamic model by relying on a minimal set of measured variables, namely, only joint positions, and performing the policy synthesis stage considering a discrete action space. Note that both restrictions arise from operational constraints on the target application and, while\nLampinen\net al.\n(\n2021\n)\nachieve good results regarding the motion control of a full-sized impact hammer, their method is not directly applicable in these settings, since their control law is designed for a continuous action space. Moreover, in contrast to works like those of\nLampinen\net al.\n(\n2020\n)\nand\nEgli and Hutter (\n2022\n)\n, we address the problem of reaching an arbitrary target pose from a given initial configuration, without the need for a full plan or waypoints.\nIII\nProblem description\nIII-A\nAutomation of impact hammers in underground mining\nImpact hammers, also known as rock breakers, are hydraulic arms, typically with four degrees of freedom and a hydraulic impact hammer as end-effector. These machines are widely used in underground mining, where they perform the task of fracturing rocks that cannot pass through transfer steel grates installed on top of ore passes, which allows the material to be transported to a lower production level due to gravity\n(Correa\net al.\n,\n2022\n)\n.\nImpact hammers used in these settings are installed adjacent to steel grates and are remotely controlled by expert human operators, who work in a safe control room, usually many kilometers away from the mine. The operators controlling the impact hammers do so by remotely actuating its electro-valves using a joystick, and by relying mainly on visual information of the environment, which is provided by CCTV cameras installed near the steel grates.\nSince there are several ore passes in an underground mine, a human operator is responsible for the teleoperation of multiple impact hammers at once; however, a given operator can only control one machine at a time. The loaders that deposit material on the steel grates (e.g., load-haul-dump machines) cannot do so if the grates are completely obstructed due to large rocks, implying that the rock-breaking task is a potential bottleneck in the production chain.\nThe above poses the automation of impact hammers in underground mining as a relevant problem to solve due to the potential enhancements in terms of production that a fleet of efficient and autonomous impact hammers could provide.\nIII-A\n1\nChallenges and constraints\nAutomating a fleet of impact hammers comes with many challenges. Firstly, many different models of impact hammers may be present in a mine, which means that developing a solution that accounts for different dynamics and control interfaces is required. Secondly, most impact hammers in a mine do not come with proprioceptive sensors other than those required to provide diagnostics on its hydraulic unit (e.g., sensors to measure the temperature and pressure of the hydraulic fluid of its tank), which is insufficient to implement classical closed-loop control systems. Finally, these machines are subjected to deterioration due to the task they perform, making replacement of their end-effectors, actuators, and other mechanisms common during maintenance sessions.\nThe aforementioned challenges impose requirements on any control system that is to be deployed on a fleet of impact hammers. The variability across impact hammersâ€™ models and the lack of sensor information on them motivate the selection of a minimal set of variables to characterize their state. This avoids relying on information that is not available or may not be obtained for all hammers in the fleet,\n1\n1\n1\nExamples on this matter include pressure measurements for a proper characterization of the hydraulic actuatorsâ€™ dynamics.\nwhilst standardizing and reducing implementation costs.\nIn addition, the variability across impact hammersâ€™ control interfaces has already resulted in the adoption of discrete control for their teleoperation in some underground mines. Although this design decision may prevent operators from having fine-grained control over the impact hammers, it comes with the benefit of only requiring characterizing the dead zones of the actuators of each impact hammer for its implementation at scale. This information is enough for the full standardization of the program that maps joystick commands to electro-valves setpoints for a whole fleet.\n2\n2\n2\nNote that this is only true if the machines composing said fleet share the same kinematic chain structure, which is generally the case for impact hammers with four degrees of freedom.\nIII-B\nThe Bobcat E10 mini-excavator\nAs a testbed, we consider a Bobcat E10 mini-excavator equipped with an hydraulic impact hammer as end-effector. The hydraulic arm of this mini-excavator has the same kinematic chain structure as most impact hammers used in mining operations; in fact, this machine has previously been used to test an RL-based policy to control the impact hammerâ€™s boom and end-effector whenever the machine is in a suitable configuration to attempt breaking a rock\n(Samtani\net al.\n,\n2023\n)\n.\nThe Bobcat E10 used in this work has been modified by installing electrovalves to control its actuators. To control its hydraulic arm, four pairs of electrovalves send pilot signals to the original machineâ€™s valves, so as to extend or retract the hydraulic cylinders of the arm. Similarly, to control the impact hammer used as end-effector, an on-off electrovalve is used. All of these electrovalves are actuated using a programmable I/O module connected to a programmable logic controller (PLC), which, in turn, is connected to an on-board PC and to a set of rotary encoders, using a CAN bus interface. The above is illustrated in Figure\n2\n.\nFigure 2:\nSimplified electro-hydraulic diagram for the modified Bobcat E10â€™s mini-excavator.\nAn important feature of the Bobcat E10 mini-excavator is that its armâ€™s valves are integrated into a single hydraulic valve block, which makes it difficult to get accurate pressure measurements to monitor the state of the valves. This imposes restrictions on the overall control design for the arm, as there is missing information to fully characterize its dynamics. A simplified hydraulic circuit diagram of the intervened mini-excavator is described in Appendix\nA\n.\nTo get the Bobcat E10â€™s hydraulic arm configuration, four absolute rotary encoders measure the angular position of each joint. Custom mounts were fabricated to accommodate each encoder so that their shafts are aligned to their corresponding joint axis. Moreover, these encoders are connected to the same PLC used for actuation, using the CAN protocol.\nAlthough it may seem unusual to use rotary encoders to get the angular positions of the armâ€™s joints instead of using draw-wire encoders to measure the displacement of each of its hydraulic cylinders, this design decision is due to two main factors related to impact hammers in underground mining. First, directly measuring joint positions using rotary encoders does not require a detailed modeling of the machineâ€™s kinematic chain (whereas draw-wire encoders generally do); moreover, for a given impact hammer in the fleet, the said kinematic chain may be unknown (for instance, because of structural modifications that occurred in maintenance sessions) and it will vary across different rock breaker models. Second, the use of properly protected draw-wire encoders, for instance those installed inside hydraulic cylinders, cannot be easily implemented in underground mines at scale, as it would require the disassembly of each intervened impact hammer and possibly the replacement of some actuators due to incompatibilities.\nIV\nProposed approach\nTo provide a solution to the problem of controlling a hydraulic impact hammer under limited observability and the requirement of using discrete control for its actuation, we follow an approach that can be described as a two-stage process. The first stage addresses the problem of modeling the impact hammersâ€™ dynamics (i.e., performing system identification), and the second stage addresses the problem of synthesizing a reaching controller for the machine, by leveraging the model obtained in the first stage.\nFor the first stage, the parameters\nğœ½\n\\bm{\\theta}\nof a function\nf\nğœ½\nf_{\\bm{\\theta}}\nthat approximate the impact hammerâ€™s dynamics are learned through supervised learning given teleoperation data. For a history of previous (discrete) actions\n(\nğ’‚\nt\nâˆ’\nk\n,\nâ€¦\nâ€‹\nğ’‚\nt\n)\n(\\bm{a}_{t-k},...\\bm{a}_{t})\n, and arm configurations\n(\nğ’’\nt\nâˆ’\nk\n,\nâ€¦\n,\nğ’’\nt\n)\n(\\bm{q}_{t-k},...,\\bm{q}_{t})\n, the model\nf\nğœ½\nf_{\\bm{\\theta}}\nhas to predict the next joint configuration,\nğ’’\nt\n+\n1\n\\bm{q}_{t+1}\n.\nFor the second stage, the model\nf\nğœ½\nf_{\\bm{\\theta}}\nis used to generate experiences given some initial (approximate) state and a stream of actions, where the predicted configurations are fed back to the model itself to obtain rollouts over some temporal horizon. The generated data are used to obtain a reaching policy\nÏ€\nÏ•\n\\pi_{\\bm{\\phi}}\nvia RL or MPC. Note that this policy is conditioned on observations\nğ’\nt\n\\bm{o}_{t}\nconstructed using only the rollout data obtained using\nf\nğœ½\nf_{\\bm{\\theta}}\nto govern the underlying system dynamics. These observations contain both the current and target end-effector poses,\nğ’³\nt\neef\n\\mathcal{X}^{\\text{eef}}_{t}\nand\nğ’³\ntarget\n\\mathcal{X}^{\\text{target}}\n, respectively, where the current end-effector pose can be computed using forward kinematics. The synthesized policy can then be directly deployed in the real-world.\nThe whole described procedure is illustrated in Fig.\n3\n, where optionally, besides configurations and actions, joint velocity estimates,\nğ’’\nË™\nt\n\\dot{\\bm{q}}_{t}\n, can be used to condition both the dynamic model\nf\nğœ½\nf_{\\bm{\\theta}}\nand the policy\nÏ€\nÏ•\n\\pi_{\\bm{\\phi}}\n. In what follows, each stage of the proposed approach is described in detail.\nFigure 3:\nOverview of the methodology used to obtain reaching controllers for hydraulic impact hammers, given a learned dynamic model\nf\nğœ½\nf_{\\bm{\\theta}}\n. Note that estimating\nğ’’\nË™\nt\n\\dot{\\bm{q}}_{t}\nis only necessary when\nf\nğœ½\nf_{\\bm{\\theta}}\nis trained to predict velocity residuals (see Section\nIV-A\n1\n).\nIV-A\nData-driven system identification\nWe consider the problem of modeling the dynamics of a four-DoF hydraulic arm with an impact hammer attached as end-effector. To get a model of the machine kinematics, we must note that detailed 3D models of rock breakers, such as those used in mining, are rarely publicly available, and if they are, the real machines may not reflect the geometry documented by the manufacturer due to modifications performed during maintenance sessions. Therefore, the parameters of their kinematic chains, in general, must be obtained by taking measurements of the machines themselves (e.g, as in\nLampinen\net al.\n(\n2021\n)\n), and then using CAD software to obtain coarse models of their links. The outcome of this process can then be utilized to specify an URDF file that describes the impact hammerâ€™s kinematics in a format that is ROS compatible\n(Quigley\net al.\n,\n2009\n)\n. However, in this work the armâ€™s hydraulic cylinders are omitted given the limitations of the standard URDF when attempting to represent kinematic loops, and also because we use rotational encoders to directly measure the jointsâ€™ angular positions.\nBy applying the above on the Bobcat E10 mini-excavator, we get an URDF file that describes its kinematics. A rendering of the model obtained, alongside joint names, relevant frames, and a scaled steel grill similar to those used in real mining operations, is shown in Fig.\n4\n.\nFigure 4:\nKinematic layout of the Bobcat E10 mini-excavator, alongside a scaled steel grill. The fixed frames\n{\nB\n}\n\\{B\\}\nand\n{\nG\n}\n\\{G\\}\nare attached to the mini-excavator base link and to the center of the grill, respectively. The cylindrical sector defines the boundaries for the position of end-effector poses that may be elements of the restricted workspace\nğ’²\n+\n\\mathcal{W}^{+}\n.\nTo model the response of the impact hammerâ€™s arm actuators given control commands, we follow a data-driven approach. However, many variables that would be required for an accurate dynamic model are often unknown, and we abstain from relying on measurements that would not be available straightforwardly for a fleet of impact hammers used in an underground mine. For the Bobcat E10 mini-excavator, examples for variables in the first category include pressure differentials for the valves (see Section\nIII-B\n), whilst measurements that, although available, are purposefully omitted, include the RPMs of its motor and the temperature of its hydraulic fluid.\nGiven the above, to learn a forward dynamics model for an impact hammer, and in particular, for the Bobcat E10â€™s arm, we only consider the measurements provided by rotary encoders, and the control commands sent to electrovalves. Since not all rotary encoders provide speed estimations, we rely only on angular position measurements. These measurements are used to obtain the hydraulic arm configuration at time step\nt\nt\n,\nğ’’\nt\nâˆˆ\nâ„\n4\n\\bm{q}_{t}\\in\\mathbb{R}^{4}\n. The electro-valve setpoints, on the other hand, are fixed due to the control restrictions for impact hammer fleets discussed in Section\nIII-A\n1\n. Thus, each hydraulic actuator contracts or expands given a fixed electrical signal, or may simply not act for a zero set point. Since these setpoints are fixed once tuned, a given (normalized) control command for time step\nt\nt\n,\nğ’‚\nt\n\\bm{a}_{t}\n, can be defined as an element of the set\n{\nâˆ’\n1\n,\n0\n,\n1\n}\n4\n\\{-1,0,1\\}^{4}\n.\nIV-A\n1\nLearning a model for the hydraulic arm actuators\nWe aim to learn a model such that, given available measurements to characterize the state of the impact hammer for a discrete time step\nt\nt\n, predicts what its configuration will be for the next time step\nt\n+\n1\nt+1\n. We try to accomplish this by learning the parameters\nğœ½\n\\bm{\\theta}\nof a model\nf\nğœ½\nf_{\\bm{\\theta}}\nthat satisfies Eq.Â (\n1\n), where\nğ’™\n~\nt\n\\bm{\\tilde{x}}_{t}\nis a function of previous configurations and control commands.\nğ’’\nt\n+\n1\n=\nğ’’\nt\n+\nf\nğœ½\nâ€‹\n(\nğ’™\n~\nt\n)\n\\bm{q}_{t+1}=\\bm{q}_{t}+f_{\\bm{\\theta}}(\\bm{\\tilde{x}}_{t})\n(1)\nTo properly characterize the state of the arm and the delays of the hydraulic actuators,\nğ’™\n~\nt\n\\bm{\\tilde{x}}_{t}\nis defined according to Eq.Â (\n2\n), where\nğ’’\nt\nâˆ’\nk\n:\nt\n=\n(\nğ’’\nt\nâˆ’\nk\n,\nğ’’\nt\nâˆ’\nk\n+\n1\n,\nâ€¦\n,\nğ’’\nt\n)\nâˆˆ\nâ„\n4\nÃ—\n(\nk\n+\n1\n)\n\\bm{q}_{t-k:t}=(\\bm{q}_{t-k},\\bm{q}_{t-k+1},...,\\bm{q}_{t})\\in\\mathbb{R}^{4\\times(k+1)}\n,\nğ’‚\nt\nâˆ’\nk\n:\nt\n=\n(\nğ’‚\nt\nâˆ’\nk\n,\nğ’‚\nt\nâˆ’\nk\n+\n1\n,\nâ€¦\n,\nğ’‚\nt\n)\nâˆˆ\nâ„\n4\nÃ—\n(\nk\n+\n1\n)\n\\bm{a}_{t-k:t}=(\\bm{a}_{t-k},\\bm{a}_{t-k+1},...,\\bm{a}_{t})\\in\\mathbb{R}^{4\\times(k+1)}\n, and\ng\ng\nis the composition of functions that will be defined later.\nğ’™\n~\nt\n=\ng\nâ€‹\n(\nğ’’\nt\nâˆ’\nk\n:\nt\n,\nğ’‚\nt\nâˆ’\nk\n:\nt\n)\n\\bm{\\tilde{x}}_{t}=g(\\bm{q}_{t-k:t},\\bm{a}_{t-k:t})\n(2)\nWe consider two different approaches for a practical instantiation of parameterized models that satisfy Eq.Â (\n1\n). For the first approach, the goal is to learn how to predict the residuals of the angular position (\nÎ”\nâ€‹\nğ’’\nt\n\\Delta\\bm{q}_{t}\n) to estimate\nğ’’\nt\n+\n1\n\\bm{q}_{t+1}\n. For the second approach, the goal is to learn how to predict angular velocity residuals (\nÎ”\nâ€‹\nğ’’\nË™\nt\n\\Delta\\dot{\\bm{q}}_{t}\n) to obtain future velocity estimates and then compute\nğ’’\nt\n+\n1\n\\bm{q}_{t+1}\nby integrating the predicted velocities. In both cases, multilayer perceptrons (MLPs) or Kolmogorov-Arnold networks (KANs)\n(Liu\net al.\n,\n2025\n)\nare utilized as function approximators. In what follows, each approach is described in further detail.\nâ€¢\nLearning\nÎ”\nâ€‹\nğ’’\nt\n\\Delta\\bm{q}_{t}\n: This approach follows Eq.Â (\n1\n) by representing\nf\nğœ½\nf_{\\bm{\\theta}}\nas an MLP or a KAN, and defining\ng\ng\nas the composition of a min-max normalization for the arm configurations, a flattening operation over both configurations and actions, and the concatenation of the min-max normalized current configuration to the result. The min-max normalization for the angular positions of the arm is defined by Eq.Â (\n3\n), where the\nj\nj\n-th components of\nğ’’\nmin\n\\bm{q}_{\\text{min}}\nand\nğ’’\nmax\n\\bm{q}_{\\text{max}}\nare set according to the joint limits of the arm, and\nğŸ\n\\bm{1}\nis a column vector in\nâ„\n4\n\\mathbb{R}^{4}\nwith all its components equal to\n1\n1\n.\nminmax\nâ€‹\n(\nğ’’\nt\n,\nğ’’\nmin\n,\nğ’’\nmax\n)\n=\n2\nâ€‹\n(\nğ’’\nt\nâˆ’\nğ’’\nmin\nğ’’\nmax\nâˆ’\nğ’’\nmin\n)\nâˆ’\nğŸ\n=\nğ’’\nÂ¯\nt\n\\texttt{minmax}(\\bm{q}_{t},\\bm{q}_{\\text{min}},\\bm{q}_{\\text{max}})=2\\left(\\frac{\\bm{q}_{t}-\\bm{q}_{\\text{min}}}{\\bm{q}_{\\text{max}}-\\bm{q}_{\\text{min}}}\\right)-\\bm{1}=\\bar{\\bm{q}}_{t}\n(3)\nThe above results in\nğ’™\n~\nt\n\\bm{\\tilde{x}}_{t}\nbeing defined by Eq.Â (\n4\n)\n3\n3\n3\nNote that, although\nğ’’\nÂ¯\nt\nT\n\\bm{\\bar{q}}^{T}_{t}\nis already in\nğ’’\nÂ¯\nt\nâˆ’\nk\n:\nt\nT\n\\bm{\\bar{q}}^{T}_{t-k:t}\n, we include it as the last four components of\nğ’™\n~\nt\n\\bm{\\tilde{x}}_{t}\nin Eq.Â (\n4\n) to get vectors of the same dimensions and overall structure than those defined by Eq.Â (\n7\n). In practice, this redundancy is introduced to simplify implementation.\n, where\nğ’’\nÂ¯\nt\nâˆ’\nk\n:\nt\nT\n=\n(\nğ’’\nÂ¯\nt\nâˆ’\nk\nT\n,\nğ’’\nÂ¯\nt\nâˆ’\nk\n+\n1\nT\n,\nâ€¦\n,\nğ’’\nÂ¯\nt\nT\n)\nâˆˆ\nâ„\n4\nâ€‹\n(\nk\n+\n1\n)\n\\bm{\\bar{q}}^{T}_{t-k:t}=(\\bm{\\bar{q}}_{t-k}^{T},\\bm{\\bar{q}}_{t-k+1}^{T},...,\\bm{\\bar{q}}_{t}^{T})\\in\\mathbb{R}^{4(k+1)}\nand\nğ’‚\nt\nâˆ’\nk\n:\nt\nT\n=\n(\nğ’‚\nt\nâˆ’\nk\nT\n,\nğ’‚\nt\nâˆ’\nk\n+\n1\nT\n,\nâ€¦\n,\nğ’‚\nt\nT\n)\nâˆˆ\nâ„\n4\nâ€‹\n(\nk\n+\n1\n)\n\\bm{a}^{T}_{t-k:t}=(\\bm{a}_{t-k}^{T},\\bm{a}_{t-k+1}^{T},...,\\bm{a}_{t}^{T})\\in\\mathbb{R}^{4(k+1)}\nare row vectors.\nğ’™\n~\nt\n=\n(\nğ’’\nÂ¯\nt\nâˆ’\nk\n:\nt\nT\n,\nğ’‚\nt\nâˆ’\nk\n:\nt\nT\n,\nğ’’\nÂ¯\nt\nT\n)\nâˆˆ\nâ„\n8\nâ€‹\n(\nk\n+\n1\n)\n+\n4\n\\displaystyle\\bm{\\tilde{x}}_{t}=(\\bm{\\bar{q}}^{T}_{t-k:t},\\bm{a}^{T}_{t-k:t},\\bm{\\bar{q}}^{T}_{t})\\in\\mathbb{R}^{8(k+1)+4}\n(4)\nâ€¢\nLearning\nÎ”\nâ€‹\nğ’’\nË™\nt\n\\Delta\\dot{\\bm{q}}_{t}\n: This approach follows Eq.Â (\n1\n) by defining\nf\nğœ½\nâ€‹\n(\nğ’™\n~\nt\n)\n:=\nÎ”\nâ€‹\nt\nâ‹…\n(\nğ’’\nË™\nt\nâˆ’\n1\n+\nf\nğœ½\nâ€²\nâ€‹\n(\nğ’™\n~\nt\n)\n)\nf_{\\bm{\\theta}}(\\bm{\\tilde{x}}_{t}):=\\Delta t\\cdot(\\dot{\\bm{q}}_{t-1}+f^{\\prime}_{\\bm{\\theta}}(\\bm{\\tilde{x}}_{t}))\n, such that\nf\nğœ½\nâ€²\nf^{\\prime}_{\\bm{\\theta}}\n(represented by an MLP or a KAN), predicts velocity residuals to get velocity estimations that are then integrated to get future angular positions, as in Eqs. (\n5\n) and (\n6\n).\nğ’’\nË™\nt\n\\displaystyle\\dot{\\bm{q}}_{t}\n=\nğ’’\nË™\nt\nâˆ’\n1\n+\nf\nğœ½\nâ€²\nâ€‹\n(\nğ’™\n~\nt\n)\n\\displaystyle=\\dot{\\bm{q}}_{t-1}+f^{\\prime}_{\\bm{\\theta}}(\\bm{\\tilde{x}}_{t})\n(5)\nğ’’\nt\n+\n1\n\\displaystyle\\bm{q}_{t+1}\n=\nğ’’\nt\n+\nÎ”\nâ€‹\nt\nâ‹…\nğ’’\nË™\nt\n\\displaystyle=\\bm{q}_{t}+\\Delta t\\cdot\\dot{\\bm{q}}_{t}\n(6)\nHere,\nf\nğœ½\nâ€²\nâ€‹\n(\nğ’™\n~\nt\n)\nf^{\\prime}_{\\bm{\\theta}}(\\bm{\\tilde{x}}_{t})\nis conditioned on velocities which are estimated given measured angular positions. To get these estimates, a loop tracker for the measured joint positions is implemented, such that a filtered angular velocity estimation can be obtained as an intermediate variable. For parameters\nk\ni\nk_{i}\nand\nk\np\nk_{p}\n, the implemented loop tracker for a given joint is illustrated in Fig.\n5\n. Note that to filter high frequencies, we do not use\nq\nË™\n^\n\\hat{\\dot{q}}\nas the velocity estimate; instead, we use the integral term of the proportional-integral (PI) controller that tracks the joint position.\nUsing the loop tracker, we can get an estimate for\nğ’’\nË™\nt\n\\dot{\\bm{q}}_{t}\ngiven a history of joint positions up to time step\nt\nt\n. Similarly to the case where we learn angular position residuals, here we define\ng\ng\nas the composition of a velocity estimator (implemented as previously described), a min-max normalization of angular velocity estimations (as\nminmax\nâ€‹\n(\nğ’’\nË™\nt\n,\nğ’’\nË™\nmin\n,\nğ’’\nË™\nmax\n)\n\\texttt{minmax}(\\dot{\\bm{q}}_{t},\\dot{\\bm{q}}_{\\text{min}},\\dot{\\bm{q}}_{\\text{max}})\n, see. Eq.Â (\n3\n)), a flattening operation over velocity estimations and actions, and the concatenation of the min-max normalized current configuration. This results in\nğ’™\n~\nt\n\\bm{\\tilde{x}}_{t}\nbeing defined by Eq.Â (\n7\n).\nğ’™\n~\nt\n=\n(\nğ’’\nË™\nÂ¯\nt\nâˆ’\nk\n:\nt\nT\n,\nğ’‚\nt\nâˆ’\nk\n:\nt\nT\n,\nğ’’\nÂ¯\nt\nT\n)\nâˆˆ\nâ„\n8\nâ€‹\n(\nk\n+\n1\n)\n+\n4\n\\displaystyle\\bm{\\tilde{x}}_{t}=(\\bar{\\dot{\\bm{q}}}^{T}_{t-k:t},\\bm{a}^{T}_{t-k:t},\\bar{\\bm{q}}^{T}_{t})\\in\\mathbb{R}^{8(k+1)+4}\n(7)\nFigure 5:\nBlock diagram of the loop tracker implemented to get\nğ’’\nË™\nt\n\\dot{\\bm{q}}_{t}\nestimates.\nThe models that predict residuals are trained using backpropagation through time\n(Werbos,\n1988\n)\n. To do so, we rely on a dataset\nğ’Ÿ\n\\mathcal{D}\nof temporally ordered tuples\n(\nğ’’\nt\n,\nğ’’\nË™\nt\n,\nğ’‚\nt\n)\n(\\bm{q}_{t},\\dot{\\bm{q}}_{t},\\bm{a}_{t})\nthat allows the construction of short trajectories of the form\nÏ„\ni\n=\n(\nğ’’\nt\ni\n,\nğ’’\nË™\nt\ni\n,\nğ’‚\nt\ni\n,\nâ€¦\n,\nğ’’\nt\ni\n+\nH\n,\nğ’’\nË™\nt\ni\n+\nH\n,\nğ’‚\nt\ni\n+\nH\n)\n{\\tau_{i}=(\\bm{q}_{t_{i}},\\dot{\\bm{q}}_{t_{i}},\\bm{a}_{t_{i}},...,\\bm{q}_{t_{i}+H},\\dot{\\bm{q}}_{t_{i}+H},\\bm{a}_{t_{i}+H})}\nfor a given initial time step\nt\ni\nt_{i}\n, along with a corresponding state approximation\nğ’™\n~\nt\ni\n\\tilde{\\bm{x}}_{t_{i}}\nassociated with the first configuration action tuple of the trajectory. These trajectories are used to calculate a loss function that measures the approximation error of the models over a short time horizon\nH\nH\n:\nâ„’\nâ€‹\n(\nğœ½\n)\n=\n1\nH\nâ€‹\nâˆ‘\nj\n=\n1\nH\nâ€–\nğ’’\n^\nt\ni\n+\nj\nâˆ’\nğ’’\nt\ni\n+\nj\nâ€–\n2\n2\n,\n\\mathcal{L}(\\bm{\\theta})=\\frac{1}{H}\\sum_{j=1}^{H}\\|\\hat{\\bm{q}}_{t_{i}+j}-\\bm{q}_{t_{i}+j}\\|^{2}_{2},\n(8)\nwhere the configurations\nğ’’\n^\nt\ni\n+\nj\n\\hat{\\bm{q}}_{t_{i}+j}\nare predicted in an â€œopen-loopâ€ manner by the model, that is, starting from\nğ’™\n~\nt\ni\n\\tilde{\\bm{x}}_{t_{i}}\n:\nğ’’\n^\nt\ni\n+\n1\n\\displaystyle\\hat{\\bm{q}}_{t_{i}+1}\n=\nğ’’\nt\ni\n+\nf\nğœ½\nâ€‹\n(\nğ’™\n~\nt\ni\n)\n,\n\\displaystyle=\\bm{q}_{t_{i}}+f_{\\bm{\\theta}}(\\tilde{\\bm{x}}_{t_{i}}),\nğ’’\n^\nt\ni\n+\n2\n\\displaystyle\\hat{\\bm{q}}_{t_{i}+2}\n=\nğ’’\n^\nt\ni\n+\n1\n+\nf\nğœ½\nâ€‹\n(\nğ’™\n~\n^\nt\ni\n+\n1\n)\n,\n\\displaystyle=\\hat{\\bm{q}}_{t_{i}+1}+f_{\\bm{\\theta}}(\\hat{\\tilde{\\bm{x}}}_{t_{i}+1}),\nâ‹®\n\\displaystyle\\mathmakebox[\\widthof{{}={}}][c]{\\vdots}\nğ’’\n^\nt\ni\n+\nH\n\\displaystyle\\hat{\\bm{q}}_{t_{i}+H}\n=\nğ’’\n^\nt\ni\n+\nH\nâˆ’\n1\n+\nf\nğœ½\nâ€‹\n(\nğ’™\n~\n^\nt\ni\n+\nH\nâˆ’\n1\n)\n.\n\\displaystyle=\\hat{\\bm{q}}_{t_{i}+H-1}+f_{\\bm{\\theta}}(\\hat{\\tilde{\\bm{x}}}_{t_{i}+H-1}).\nNote that\nğ’™\n~\nt\ni\n+\nj\n\\tilde{\\bm{x}}_{t_{i}+j}\n,\nj\nâˆˆ\n{\n1\n,\nâ€¦\n,\nH\nâˆ’\n1\n}\nj\\in\\{1,...,H-1\\}\nis constructed by progressively updating the components of\nğ’™\n~\nt\ni\n\\tilde{\\bm{x}}_{t_{i}}\nwith the ground-truth actions in\nÏ„\ni\n\\tau_{i}\nand the predictions of the model (which may be angular positions or velocities, depending on the chosen model).\nIV-B\nController synthesis for reaching\nIV-B\n1\nProblem formulation\nIn this work, we address the reaching task for hydraulic rock breakers, that is, we aim at synthesizing a controller capable of reaching end-effector target poses. In practice, performing this task would allow these machines to position their hydraulic hammer on top of rocks that need to be fractured.\nThe interaction between the agent and the environment is modeled as a Partially Observable Markov Decision Process (POMDP), which is defined by a set of states\nğ’®\n\\mathcal{S}\n, a set of actions\nğ’œ\n\\mathcal{A}\n, a scalar reward function\nâ„›\nâ€‹\n(\nğ’”\n,\nğ’‚\n)\n\\mathcal{R}(\\bm{s},\\bm{a})\n, a stochastic transition function\nT\nâ€‹\n(\nğ’”\n,\nğ’‚\n,\nğ’”\nâ€²\n)\n=\np\nâ€‹\n(\nğ’”\nâ€²\n|\nğ’”\n,\nğ’‚\n)\nT(\\bm{s},\\bm{a},\\bm{s}^{\\prime})=p(\\bm{s}^{\\prime}|\\bm{s},\\bm{a})\n, an observation function\nğ’ª\nâ€‹\n(\nğ’”\nâ€²\n,\nğ’‚\n,\nğ’\n)\n=\np\nâ€‹\n(\nğ’\n|\nğ’”\nâ€²\n,\nğ’‚\n)\n\\mathcal{O}(\\bm{s}^{\\prime},\\bm{a},\\bm{o})=p(\\bm{o}|\\bm{s}^{\\prime},\\bm{a})\n, a set of observations\nÎ©\n\\Omega\n, and a discount factor\nÎ³\nâˆˆ\n[\n0\n,\n1\n)\n\\gamma\\in[0,1)\n. At each discrete time step\nt\nt\nthe agent observes\nğ’\nt\n\\bm{o}_{t}\n, executes an action\nğ’‚\nt\n\\bm{a}_{t}\naccording to its policy\nÏ€\nâ€‹\n(\nğ’‚\nt\n|\nğ’\nt\n)\n\\pi(\\bm{a}_{t}|\\bm{o}_{t})\n, receives a scalar reward\nr\nt\nr_{t}\n, and transitions to a new state\nğ’”\nt\n+\n1\n\\bm{s}_{t+1}\n.\nWe will assume that the rock breakers operate in a restricted region of their workspace and that this region is always free of obstacles (see Section\nIV-B\n3\n). This allows solving the reaching task without relying on exteroceptive information.\nIV-B\n2\nModeling\nâ€¢\nDynamics:\nThe impact hammer dynamics is forward simulated using a parameterized function\nf\nğœ½\nf_{\\bm{\\theta}}\n, which is trained as described in Section\nIV-A\n1\n. Depending on whether the learned model predicts the angular position residuals or the angular velocity residuals, this is accomplished by iteratively evaluating Eq.Â (\n1\n), or Eqs.Â (\n5\n) andÂ (\n6\n).\nâ€¢\nObservations:\nTo provide the agent with enough information to make decisions (given the underlying learned dynamics), the policy is conditioned on\nğ’™\n~\nt\n\\tilde{\\bm{x}}_{t}\n, on a representation of the current impact hammerâ€™s end-effector pose,\nğ’³\nt\neef\nâˆˆ\nSE(3)\n\\mathcal{X}^{\\text{eef}}_{t}\\in\\text{SE(3)}\n, and on a representation of the target pose,\nğ’³\ntarget\nâˆˆ\nSE(3)\n\\mathcal{X}^{\\text{target}}\\in\\text{SE(3)}\n. Note that\nğ’™\n~\nt\n\\tilde{\\bm{x}}_{t}\nis given by Eq.Â (\n4\n) or by Eq.Â (\n7\n), depending on the chosen dynamics model.\nWe use two complementary representations for the end effector poses (instantaneous and target):\nâ€“\nAs tuples\n(\nğ’‘\n,\nğ’“\n)\n(\\bm{p},\\bm{r})\n, where\nğ’‘\nâˆˆ\nâ„\n3\n\\bm{p}\\in\\mathbb{R}^{3}\ndenotes position and\nğ’“\nâˆˆ\nğ’®\n3\n\\bm{r}\\in\\mathcal{S}^{3}\ndenotes orientation.\nâ€“\nAs configurations\nğ’’\nâˆˆ\nâ„\n4\n\\bm{q}\\in\\mathbb{R}^{4}\nthat would be mapped to the poses via forward kinematics.\nThus, the observations\nğ’\nt\n\\bm{o}_{t}\nare given by the concatenation of\nğ’™\n~\nt\n\\tilde{\\bm{x}}_{t}\nwith representations for the instantaneous and target end-effector poses.\nâ€¢\nActions:\nGiven the operational constraints described in Section\nIII-A\n1\n, the control of the impact hammer is done using discrete control commands. Thus, using the same definition as in Section\nIV-A\n, a normalized action at time\nt\nt\n,\nğ’‚\nt\n\\bm{a}_{t}\n, is an element of the set\n{\nâˆ’\n1\n,\n0\n,\n1\n}\n4\n\\{-1,0,1\\}^{4}\n.\nâ€¢\nReward function:\nThe reward function is designed to encourage the completion of the reaching task, while also regularizing the policyâ€™s actions to prevent the exploitation of the underlying learned dynamics inaccuracies.\nWe define a penalty function to measure the deviation of the current end-effector pose,\nğ’³\nt\neef\n\\mathcal{X}^{\\text{eef}}_{t}\n, from the target pose,\nğ’³\ntarget\n\\mathcal{X}^{\\text{target}}\n, by weighting their positional and rotational differences. Representing the poses as tuples\n(\nğ’“\n,\nğ’‘\n)\n(\\bm{r},\\bm{p})\n, the position error is quantified as the Euclidean distance between their positions:\nd\nâ€‹\n(\nğ’‘\ntarget\n,\nğ’‘\nt\neef\n)\n=\nâ€–\nğ’‘\ntarget\nâˆ’\nğ’‘\nt\neef\nâ€–\n2\n,\nd(\\bm{p}^{\\text{target}},\\bm{p}_{t}^{\\text{eef}})=\\|\\bm{p}^{\\text{target}}-\\bm{p}_{t}^{\\text{eef}}\\|_{2},\n(9)\nwhereas the rotational error is computed as the normalized geodesic distance between their unit quaternions:\nd\ng\nâ€‹\n(\nğ’“\ntarget\n,\nğ’“\nt\neef\n)\n=\n1\nÏ€\nâ€‹\ncos\nâˆ’\n1\nâ¡\n(\n2\nâ€‹\n(\nğ’“\ntarget\nâ‹…\nğ’“\nt\neef\n)\n2\nâˆ’\n1\n)\n.\nd_{g}(\\bm{r}^{\\text{target}},\\bm{r}_{t}^{\\text{eef}})=\\frac{1}{\\pi}\\cos^{-1}\\left(2(\\bm{r}^{\\text{target}}\\cdot\\bm{r}_{t}^{\\text{eef}})^{2}-1\\right).\n(10)\nThus, the described penalization function is given by:\nr\nt\nğ’³\n=\nâˆ’\nÎ»\nğ’‘\nâ€‹\nd\nâ€‹\n(\nğ’‘\ntarget\n,\nğ’‘\nt\neef\n)\nâˆ’\nÎ»\nğ’“\nâ€‹\nd\ng\nâ€‹\n(\nğ’“\ntarget\n,\nğ’“\nt\neef\n)\n,\nr^{\\mathcal{X}}_{t}=-\\lambda_{\\bm{p}}d(\\bm{p}^{\\text{target}},\\bm{p}_{t}^{\\text{eef}})-\\lambda_{\\bm{r}}d_{g}(\\bm{r}^{\\text{target}},\\bm{r}_{t}^{\\text{eef}}),\n(11)\nwhere\nÎ»\nğ’‘\n,\nÎ»\nğ’“\n>\n0\n\\lambda_{\\bm{p}},\\lambda_{\\bm{r}}>0\nare fixed scalars.\nIn addition, we include a penalty for deviations in joint-space from the desired target configuration,\nğ’’\ntarget\n\\bm{q}^{\\text{target}}\n(which can be mapped to\nğ’³\ntarget\n\\mathcal{X}^{\\text{target}}\nvia forward kinematics):\nr\nt\nğ’’\n=\nâˆ’\nÎ»\nğ’’\nâ€‹\nâ€–\nğ’’\nt\nâˆ’\nğ’’\ntarget\nâ€–\n2\n.\nr_{t}^{\\bm{q}}=-\\lambda_{\\bm{q}}\\|\\bm{q}_{t}-\\bm{q}^{\\text{target}}\\|_{2}.\n(12)\nTo encourage fine-grained control, we consider binary rewards that the agent receives only if it manages to meet certain accuracy criteria. These binary rewards are computed in terms of positional and rotational errors (Eqs.Â (\n13\n)\n4\n4\n4\nHere\nâŸ¦\nâ‹…\nâŸ§\n\\llbracket\\cdot\\rrbracket\nis the Iverson bracket, meaning\nâŸ¦\nP\nâŸ§\n=\n1\n\\llbracket P\\rrbracket=1\nif\nP\nP\nis True, and\nâŸ¦\nP\nâŸ§\n=\n0\n\\llbracket P\\rrbracket=0\notherwise.\nandÂ (\n14\n)\n5\n5\n5\nWe only consider the yaw and pitch Euler angles, since given the impact hammer kinematics, the roll-angle is fixed. Although a similar argument could be used to also omit the yaw-angle, we use it because its value is fully determined by the â€œswingâ€ joint, i.e, by\nğ’’\nt\nâ€‹\n[\n0\n]\n\\bm{q}_{t}[0]\n(see Fig.\n4\n).\n), and deviations in joint space (Eq.Â (\n15\n)), where\nÏµ\nğ’‘\n,\nÏµ\nğ’“\n,\nÏµ\nÎ±\n,\nÏµ\nq\n>\n0\n\\epsilon_{\\bm{p}},\\epsilon_{\\bm{r}},\\epsilon_{\\alpha},\\epsilon_{q}>0\nare fixed scalar thresholds.\nb\nğ’³\n\\displaystyle b_{\\mathcal{X}}\n=\nâŸ¦\nd\n(\nğ’‘\ntarget\n,\nğ’‘\nt\neef\n)\n<\nÏµ\nğ’‘\n)\nâŸ§\nâ‹…\nâŸ¦\n(\nd\ng\n(\nğ’“\ntarget\n,\nğ’“\nt\neef\n)\n<\nÏµ\nğ’“\n)\nâŸ§\n\\displaystyle=\\bigl\\llbracket d(\\bm{p}^{\\text{target}},\\bm{p}_{t}^{\\text{eef}})<\\epsilon_{\\bm{p}})\\bigr\\rrbracket\\cdot\\bigl\\llbracket(d_{g}(\\bm{r}^{\\text{target}},\\bm{r}_{t}^{\\text{eef}})<\\epsilon_{\\bm{r}})\\bigr\\rrbracket\n(13)\nb\nÎ±\n\\displaystyle b_{\\alpha}\n=\nâŸ¦\n(\nÎ±\nyaw\nâˆ’\nÎ±\nyaw\ntarget\n)\n2\n<\nÏµ\nÎ±\nâŸ§\nâ‹…\nâŸ¦\n(\nÎ±\npitch\nâˆ’\nÎ±\npitch\ntarget\n)\n2\n<\nÏµ\nÎ±\nâŸ§\n\\displaystyle=\\llbracket(\\alpha_{\\text{yaw}}-\\alpha_{\\text{yaw}}^{\\text{target}})^{2}<\\epsilon_{\\alpha}\\rrbracket\\cdot\\llbracket(\\alpha_{\\text{pitch}}-\\alpha_{\\text{pitch}}^{\\text{target}})^{2}<\\epsilon_{\\alpha}\\rrbracket\n(14)\nb\nğ’’\n\\displaystyle b_{\\bm{q}}\n=\nÎ \nj\n=\n0\n3\nâŸ¦\n(\nğ’’\nt\n[\nj\n]\nâˆ’\nğ’’\ntarget\n[\nj\n]\n)\n2\n<\nÏµ\nq\nâŸ§\n\\displaystyle=\\Pi_{j=0}^{3}\\llbracket(\\bm{q}_{t}[j]-\\bm{q}^{\\text{target}}[j])^{2}<\\epsilon_{q}\\rrbracket\n(15)\nThe reward the agent receives depending on the fulfillment of the above conditions is given by Eq.Â (\n16\n), where\nw\nğ’³\n,\nw\nÎ±\n,\nw\nğ’’\n>\n0\nw_{\\mathcal{X}},w_{\\alpha},w_{\\bm{q}}>0\nare fixed scalars.\nr\nt\nÏµ\n=\nw\nğ’³\nâ€‹\n(\nb\nğ’³\n|\n(\nÏµ\nğ’‘\n,\nÏµ\nğ’“\n)\n+\nb\nğ’³\n|\n(\nÏµ\nğ’‘\nâ€²\n,\nÏµ\nğ’“\nâ€²\n)\n)\n+\nw\nÎ±\nâ€‹\nb\nÎ±\n+\nw\nğ’’\nâ€‹\nb\nğ’’\nr^{\\epsilon}_{t}=w_{\\mathcal{X}}\\left(b_{\\mathcal{X}}|_{(\\epsilon_{\\bm{p}},\\epsilon_{\\bm{r}})}+b_{\\mathcal{X}}|_{(\\epsilon^{\\prime}_{\\bm{p}},\\epsilon^{\\prime}_{\\bm{r}})}\\right)+w_{\\alpha}b_{\\alpha}+w_{\\bm{q}}b_{\\bm{q}}\n(16)\nTo encourage temporal consistency in the selection of actions, we penalize changes in action components between successive time steps:\nr\nt\nğ’‚\n=\nâˆ’\nÎ»\nğ’‚\nâ‹…\n1\n4\nâ€‹\nâˆ‘\ni\n=\n0\n3\n|\nğ’‚\nt\nâ€‹\n[\ni\n]\nâˆ’\nğ’‚\nt\nâˆ’\n1\nâ€‹\n[\ni\n]\n|\n,\nr_{t}^{\\bm{a}}=-\\lambda_{\\bm{a}}\\cdot\\frac{1}{4}\\sum_{i=0}^{3}|\\bm{a}_{t}[i]-\\bm{a}_{t-1}[i]|,\n(17)\nwhere\nÎ»\nğ’‚\n>\n0\n\\lambda_{\\bm{a}}>0\nis the action regularization weight.\nFinally, we also penalize the agent if the impact hammerâ€™s end effector goes outside a restricted region of its workspace,\nğ’²\n+\nâŠ‚\nğ’²\nâŠ‚\nSE\nâ€‹\n(\n3\n)\n\\mathcal{W}^{+}\\subset\\mathcal{W}\\subset\\text{SE}(3)\n(see Fig.\n4\n), where\nğ’²\n\\mathcal{W}\ndenotes the full reachable workspace, and\nÎ»\nğ’²\n+\n>\n0\n\\lambda_{\\mathcal{W}^{+}}>0\nis a fixed scalar:\nr\nt\nğ’²\n+\n=\nâˆ’\nÎ»\nğ’²\n+\nâŸ¦\nÂ¬\n(\nğ’³\nt\neef\nâˆˆ\nğ’²\n+\n)\nâŸ§\n.\nr_{t}^{\\mathcal{W}^{+}}=-\\lambda_{\\mathcal{W}^{+}}\\llbracket\\neg(\\mathcal{X}_{t}^{\\text{eef}}\\in\\mathcal{W}^{+})\\rrbracket.\n(18)\nThus, the full reward is a weighted combination of all of the above terms:\nr\nt\n=\nr\nt\nğ’³\n+\nr\nt\nğ’’\n+\nr\nt\nÏµ\n+\nr\nt\nğ’‚\n+\nr\nt\nğ’²\n.\nr_{t}=r_{t}^{\\mathcal{X}}+r_{t}^{\\bm{q}}+r_{t}^{\\epsilon}+r_{t}^{\\bm{a}}+r_{t}^{\\mathcal{W}}.\n(19)\nIV-B\n3\nEpisodic conditions\nThe reaching task, as formulated in this work, is episodic. In each episode, a random initial configuration for the impact hammer is selected. This initial configuration is such that the position of the end-effector of the hammer remains inside a restricted region of its workspace, which is denoted by\nğ’²\n+\n\\mathcal{W^{+}}\n(see Fig.\n4\n), and its orientation has a bounded pitch angle,\n|\nÎ±\npitch\ntarget\n|\nâ‰¤\nÎ±\npitch\nmax\n|\\alpha_{\\text{pitch}}^{\\text{target}}|\\leq\\alpha_{\\text{pitch}}^{\\text{max}}\n. Afterwards, a target end effector pose,\nğ’³\ntarget\nâˆˆ\nSE\nâ€‹\n(\n3\n)\n\\mathcal{X}_{\\text{target}}\\in\\text{SE}(3)\n, also within the restricted workspace and satisfying the pitch angle condition, is randomly selected.\nFor a given episode, if the end effector of the impact hammer reaches\nğ’³\ntarget\n\\mathcal{X}_{\\text{target}}\n, then the episode is deemed successful. On the contrary, if the hammerâ€™s end effector does not reach\nğ’³\ntarget\n\\mathcal{X}_{\\text{target}}\nafter\nT\nreset\n=\n500\nT_{\\text{reset}}=500\ndiscrete time steps, then the episode is considered unsuccessful, and episodic conditions are reset. Note that these conditions are checked only to measure the performance of the reaching controller but do not imply getting to a terminal state.\nFig.\n6a\nshows sampled end-effector poses that fulfill the position and pitch conditions described above for the BobcatÂ E10 mini-excavator, by setting\nÎ±\npitch\nmax\n=\n1.05\n\\alpha_{\\text{pitch}}^{\\text{max}}=1.05\n[rad]. Fig.\n6b\n, on the other hand, shows the sampled end effector poses that, besides the above conditions, fulfill\n|\nÎ±\npitch\ntarget\n|\nâ‰¤\nÎ±\npitch\nts\n|\\alpha_{\\text{pitch}}^{\\text{target}}|\\leq\\alpha_{\\text{pitch}}^{\\text{ts}}\n, with\nÎ±\npitch\nmax\n>\nÎ±\npitch\nts\n=\n0.69\n\\alpha_{\\text{pitch}}^{\\text{max}}>\\alpha_{\\text{pitch}}^{\\text{ts}}=0.69\n[rad], and the\nz\nz\n-axis component of their position being in\n[\nz\nmin\nts\n,\nz\nmax\nts\n]\n[z^{\\text{ts}}_{\\text{min}},z^{\\text{ts}}_{\\text{max}}]\n, with\nz\nmin\nts\n=\n0.155\nz^{\\text{ts}}_{\\text{min}}=0.155\n[m] (which matches the grill height) and\nz\nmax\nts\n=\n2.355\nz^{\\text{ts}}_{\\text{max}}=2.355\n[m], respectively.\nDuring policy learning, initial and target poses (and their respective configurations) are sampled considering the conditions applied to generate Fig.\n6a\n. For evaluation purposes the same applies for initial poses, however, the target poses are obtained by enforcing the conditions used to generate Fig.\n6b\n. Note that the latter poses, besides being a subset of those used during learning, fulfill restrictions that are\nad-hoc\nto the task space associated to rock-breaking, in which it is expected for the end-effector to reach poses that are quasi-orthogonal to the surfaces of the rocks that need to be fractured.\n(a)\n(b)\nFigure 6:\nSampled end-effector poses used to set episodic conditions for the reaching task during (a) policy learning, and (b) policy evaluation.\nIV-B\n4\nRL-based controller\nGiven the formulation described in Section\nIV-B\n1\n, the goal is to learn a policy that maximizes the expected discounted return that the agent receives, i.e., to maximize\nJ\nRL\nâ€‹\n(\nÏ€\n)\n=\nğ”¼\nğ’‚\nt\nâˆ¼\nÏ€\nâ€‹\n(\nğ’‚\nt\n|\nğ’\nt\n)\nâ€‹\n[\nâˆ‘\nt\n=\n1\nT\nÎ³\nt\nâˆ’\n1\nâ€‹\nr\nt\n]\nJ_{\\text{RL}}(\\pi)=\\mathbb{E}_{\\bm{a}_{t}\\sim\\pi(\\bm{a}_{t}|\\bm{o}_{t})}\\left[\\sum_{t=1}^{T}\\gamma^{t-1}r_{t}\\right]\n.\nTo accomplish the above, we train a policy using the Proximal Policy Optimization (PPO) algorithm\n(Schulman\net al.\n,\n2017\n)\n. To adapt PPO to discrete actions, we follow an approach close to that proposed by\nDulac-Arnold\net al.\n(\n2015\n)\nfor their â€œWolpertinger architectureâ€: Given a continuous proto-action\nğ’‚\nt\np\nâˆˆ\n[\nâˆ’\n1\n,\n1\n]\n4\n\\bm{a}^{p}_{t}\\in[-1,1]^{4}\n(output by the PPO policy), we construct a valid action\nğ’‚\nt\n\\bm{a}_{t}\nby discretizing each component of the proto-action,\nğ’‚\nt\np\nâ€‹\n[\nj\n]\n,\nj\nâˆˆ\n{\n0\n,\n1\n,\n2\n,\n3\n}\n\\bm{a}_{t}^{p}[j],j\\in\\{0,1,2,3\\}\n, according to Eq.Â (\n20\n).\nğ’‚\nt\nâ€‹\n[\nj\n]\n=\n{\nâˆ’\n1\nif\nâ€‹\nğ’‚\nt\np\nâ€‹\n[\nj\n]\n<\nâˆ’\n0.5\n,\n0\nif\nâ€‹\n|\nğ’‚\nt\np\nâ€‹\n[\nj\n]\n|\nâ‰¤\n0.5\n,\n1\nif\nâ€‹\nğ’‚\nt\np\nâ€‹\n[\nj\n]\n>\n0.5\n\\bm{a}_{t}[j]=\\begin{cases}-1&\\text{if }\\bm{a}^{p}_{t}[j]<-0.5,\\\\\n0&\\text{if }\\left|\\bm{a}^{p}_{t}[j]\\right|\\leq 0.5,\\\\\n1&\\text{if }\\bm{a}^{p}_{t}[j]>0.5\\end{cases}\n(20)\nWhile the discretized action\nğ’‚\nt\n\\bm{a}_{t}\nis used for control, the proto-action is used to update the policy given the learning objectives of PPO.\nIV-B\n5\nMPC-based controller\nAs an alternative to RL, we also synthesize a controller following an MPC approach. MPC formulates control as a receding horizon optimization problem that leverages the learned dynamic model to plan action sequences over a finite time horizon. At each time step\nt\nt\n, the MPC controller solves the following optimization problem:\nğ’‚\nt\n:\nt\n+\nH\nMPC\nâˆ’\n1\nâˆ—\n\\displaystyle\\bm{a}^{*}_{t:t+H_{\\text{MPC}}-1}\n=\narg\nâ€‹\nmax\nğ’‚\nt\n:\nt\n+\nH\nMPC\nâˆ’\n1\nâ€‹\nâˆ‘\nk\n=\n0\nH\nMPC\nâˆ’\n1\nÎ³\nk\nâ€‹\nr\nt\n+\nk\n\\displaystyle=\\operatorname*{arg\\,max}_{\\bm{a}_{t:t+H_{\\text{MPC}}-1}}\\sum_{k=0}^{H_{\\text{MPC}}-1}\\gamma^{k}r_{t+k}\n(21)\nsubject to:\nğ’’\nt\n+\nk\n+\n1\n\\displaystyle\\bm{q}_{t+k+1}\n=\nğ’’\nt\n+\nk\n+\nf\nğœ½\nâ€‹\n(\nğ’™\n~\nt\n+\nk\n)\n,\nk\nâˆˆ\n{\n0\n,\nâ€¦\n,\nH\nMPC\nâˆ’\n1\n}\n\\displaystyle=\\bm{q}_{t+k}+f_{\\bm{\\theta}}(\\bm{\\tilde{x}}_{t+k}),\\quad k\\in\\{0,...,H_{\\text{MPC}}-1\\}\n(22)\nğ’‚\nt\n+\nk\n\\displaystyle\\bm{a}_{t+k}\nâˆˆ\n{\nâˆ’\n1\n,\n0\n,\n1\n}\n4\n,\nk\nâˆˆ\n{\n0\n,\nâ€¦\n,\nH\nMPC\nâˆ’\n1\n}\n\\displaystyle\\in\\{-1,0,1\\}^{4},\\quad k\\in\\{0,...,H_{\\text{MPC}}-1\\}\n(23)\nwhere\nğ’‚\nt\n:\nt\n+\nH\nMPC\nâˆ’\n1\nâˆ—\n\\bm{a}^{*}_{t:t+H_{\\text{MPC}}-1}\ndenotes the optimal action sequence over the planning horizon\nH\nMPC\nH_{\\text{MPC}}\n,\nÎ³\n\\gamma\nis the discount factor, and\nr\nt\n+\nk\nr_{t+k}\nare the rewards computed using the function defined in Section\nIV-B\n2\n. ConstraintÂ (\n22\n) enforces that the predicted configurations evolve according to the learned dynamic model\nf\nğœ½\nf_{\\bm{\\theta}}\n, and constraintÂ (\n23\n) restricts the actions to be discrete. Note that only the first action\nğ’‚\nt\nâˆ—\n\\bm{a}^{*}_{t}\nof the optimal sequence is executed, and the optimization is repeated at the next time step with updated state information.\nTo solve the problem defined by Eqs.Â (\n21\n)â€“(\n23\n), we employ a variant of the Cross-Entropy Method (CEM)\n(De Boer\net al.\n,\n2005\n)\n, a sampling-based optimization algorithm that iteratively refines a distribution over candidate action sequences. At each iteration, CEM samples a population of\nN\npop\nN_{\\text{pop}}\ncontinuous proto-action sequences\n{\nğ’‚\nt\n:\nt\n+\nH\nMPC\nâˆ’\n1\np\n,\n(\ni\n)\n}\ni\n=\n1\nN\npop\n\\{\\bm{a}^{p,(i)}_{t:t+H_{\\text{MPC}}-1}\\}_{i=1}^{N_{\\text{pop}}}\nfrom a parameterized Gaussian distribution\nğ’©\nâ€‹\n(\nğ\n,\nğšº\n)\n\\mathcal{N}(\\bm{\\mu},\\bm{\\Sigma})\nwith diagonal covariance matrix over the continuous action space\n[\nâˆ’\n1\n,\n1\n]\n4\nÃ—\nH\nMPC\n[-1,1]^{4\\times H_{\\text{MPC}}}\n, discretizes each proto-action to obtain valid discrete action sequences\n{\nğ’‚\nt\n:\nt\n+\nH\nMPC\nâˆ’\n1\n(\ni\n)\n}\ni\n=\n1\nN\npop\n\\{\\bm{a}^{(i)}_{t:t+H_{\\text{MPC}}-1}\\}_{i=1}^{N_{\\text{pop}}}\naccording to Eq.Â (\n20\n), evaluates their cumulative rewards using the learned model\nf\nğœ½\nf_{\\bm{\\theta}}\nvia forward simulation, selects the top\nN\nelite\nN_{\\text{elite}}\nsequences with highest returns, and updates the distribution parameters\n(\nğ\n,\nğšº\n)\n(\\bm{\\mu},\\bm{\\Sigma})\nto concentrate probability mass around these elite samples. This process repeats for a fixed number of iterations or until convergence, yielding an approximately optimal action sequence.\nThe variant we use is referred to as iCEM (improved CEM), which incorporates the enhancements proposed by\nPinneri\net al.\n(\n2021\n)\ninto the standard CEM formulation. These improvements include colored action noise to enforce temporal smoothness in the planned trajectories, importance mixing to leverage information from previous optimization rounds, and momentum-based updates to stabilize the distribution refinement process. These modifications have been shown to improve sample efficiency and solution quality, particularly in contact-rich manipulation tasks with complex dynamics\n(Sancaktar\net al.\n,\n2022\n; Li\net al.\n,\n2024\n; Jiang\net al.\n,\n2024\n)\n.\nV\nExperimental results\nV-A\nComputational pipelines\nThe models used for the data-driven identification of the system (see Section\nIV-A\n) are implemented and trained using Flax\n(Heek\net al.\n,\n2024\n)\n. The simulation environment is entirely built on top of JAX\n(Bradbury\net al.\n,\n2018\n)\n, taking advantage of XLA (accelerated linear algebra), just-in-time (JIT) compilation, and parallelization. Moreover, the simulation pipeline integrates Brax\n(Freeman\net al.\n,\n2021\n)\nto compute forward kinematics. For visualization purposes, we leverage the data structures provided by Brax to render the simulation using the MuJoCo\n(Todorov\net al.\n,\n2012\n)\nvisualizer. For RL-based policy learning, we employ the PPO implementation provided by Brax, whereas for MPC-based control, iCEM is implemented in JAX.\nImplementing simulation, modeling, and learning in JAX (and JAX-based libraries) allows for a highly efficient computational workflow. As a result, for instance, we can train RL-based policies using PPO for hundreds of millions of steps in just a few minutes on mid-range consumer desktops (e.g., equipped with an Intel i7-12700 CPU, an NVIDIA RTX 4060 GPU, and 32 GB of RAM).\nTo deploy the controllers on the real machine, a ROS-based pipeline\n(Quigley\net al.\n,\n2009\n)\nwas developed. Moreover, a pre-deployment stage using the Gazebo simulator\n(Koenig and Howard,\n2004\n)\nwas incorporated to allow testing in a safe environment, thereby minimizing the risk of accidents that could harm either staff or the machine. The implementation details for the ROS-based and ROS-Gazebo computational pipelines are provided in the Appendix\nB\n.\nV-B\nData collection for system identification\nWe collect teleoperation data to construct a dataset\nğ’Ÿ\n\\mathcal{D}\nof temporally ordered tuples\n(\nğ’’\nt\n,\nğ’’\nË™\nt\n,\nğ’‚\nt\n)\n(\\bm{q}_{t},\\dot{\\bm{q}}_{t},\\bm{a}_{t})\n. This dataset is then used to learn the parameters of the dynamic model\nf\nğœ½\nf_{\\bm{\\theta}}\nthrough the minimization of the loss function in Eq.Â (\n8\n) (see Sec.\nIV-A\n1\nfor a detailed explanation).\nThe teleoperation of the Bobcat E10 mini-excavator is performed by sending discrete commands\nğ’‚\nt\nâˆˆ\n{\nâˆ’\n1\n,\n0\n,\n1\n}\n4\n\\bm{a}_{t}\\in\\{-1,0,1\\}^{4}\nto its actuators. These control commands are constructed by discretizing the analog signals from the left and right sticks of a wireless Xbox 360 controller. The joystick interpreter is implemented in ROS, and the control commands are sent via CAN bus to the PLC installed in the mini-excavator. The PLC program performs a direct mapping of the discrete control commands to current set points for each electro-valve, so each stick axis of the Xbox 360 controller (four in total) can actuate one of the four hydraulic cylinders of the mini-excavator arm. It is important to note that this direct joint-level teleoperation resembles that of impact hammers in underground mining (see Section\nIII-A\n1\n).\nThe instantaneous angular positions of the hydraulic arm joints,\nğ’’\nt\n\\bm{q}_{t}\n, are obtained directly through rotational encoder measurements (as explained in Section\nIII-B\n), and their angular velocity,\nğ’’\nË™\nt\n\\dot{\\bm{q}}_{t}\n, is estimated using the observer described in Section\nIV-A\n1\n.\nUsing the ROS-based teleoperation module, the mini-excavator arm is controlled at 20 Hz, and the tuples\n(\nğ’’\nt\n,\nğ’’\nË™\nt\n,\nğ’‚\nt\n)\n(\\bm{q}_{t},\\dot{\\bm{q}}_{t},\\bm{a}_{t})\nare stored at the same rate. To construct a dataset with representative samples, the mini-excavator arm is operated with the objective of covering the subset of its configuration space that would result in its end-effector position to lie within the restricted workspace,\nğ’²\n+\n\\mathcal{W}^{+}\n, described in Section\nIV-B\n3\n. With this aim, while controlling the machine, the human operator can see a 3D representation of the restricted workspace limits, and has real-time visual feedback of the history of visited end-effector positions, thus, is instructed to â€œpaintâ€ the interior of the restricted workspace, with the end effector position acting as a 3D pencil.\nFig.\n7\nshows the visual feedback described, constructed from data acquired by teleoperating the mini-excavator arm for approximately 21, 23 and 32 minutes, respectively.\nFigure 7:\nEnd effector trajectories resulting from the teleoperation of the Bobcat E10 mini-excavator, for three data collection sessions (left, center, and right). The trajectoriesâ€™ color represent the instantaneous end effector pitch angle.\nFigure 8:\nOpen-loop predictions performed by the best learned dynamic models over an evaluation subset. Given an initial ground-truth construction of\nğ’™\n~\nt\n\\tilde{\\bm{x}}_{t}\nand a sequence of\n10\n3\n10^{3}\nground-truth actions captured at\n20\n20\nHz, the dashed lines show the predicted angular positions for the Bobcat E10 armâ€™s joints. Note that the terms\nğ’™\n~\nt\n+\nj\n\\tilde{\\bm{x}}_{t+j}\n,\nj\nâˆˆ\n{\n1\n,\nâ€¦\n,\nH\nâˆ’\n1\n}\n,\nH\n=\n10\n3\nj\\in\\{1,...,H-1\\},H=10^{3}\n, are sequentially constructed by updating the components of\nğ’™\n~\nt\n\\tilde{\\bm{x}}_{t}\nwith ground-truth actions and model predictions.\nThis data, which amounts to a total of approx. 76 min of teleoperation, is used to learn a model for the hydraulic armâ€™s actuators response, as will be discussed in the following section.\nV-C\nTraining and evaluation of models for the arm dynamics\nThe teleoperation dataset\nğ’Ÿ\n\\mathcal{D}\ncontains temporally ordered tuples\n(\nğ’’\nt\n,\nğ’’\nË™\nt\n,\nğ’‚\nt\n)\n(\\bm{q}_{t},\\dot{\\bm{q}}_{t},\\bm{a}_{t})\n. In principle, this dataset can be constituted by the aggregation of several trajectories (with a length of at least\nH\n+\n1\nH+1\nelements each, plus the data to construct\nğ’™\n~\nt\n\\tilde{\\bm{x}}_{t}\n), as long as these are managed separately when sampling short sequences\nÏ„\ni\n=\n(\nğ’’\nt\ni\n,\nğ’’\nË™\nt\ni\n,\nğ’‚\nt\ni\n,\nâ€¦\n,\nğ’’\nt\ni\n+\nH\n,\nğ’’\nË™\nt\ni\n+\nH\n,\nğ’‚\nt\ni\n+\nH\n)\n{\\tau_{i}=(\\bm{q}_{t_{i}},\\dot{\\bm{q}}_{t_{i}},\\bm{a}_{t_{i}},...,\\bm{q}_{t_{i}+H},\\dot{\\bm{q}}_{t_{i}+H},\\bm{a}_{t_{i}+H})}\n, for a given initial time step\nt\ni\nt_{i}\n. In practice, the dataset we use for training is composed by the three trajectories depicted in Fig.\n7\n, and is divided into training and validation subsets. The training subset is constructed by using\n90\n%\n90\\%\nof each trajectory, starting from the first sample, whereas the evaluation subset is constructed using the remaining portion of the trajectories.\nFollowing the method described in Section\nIV-A\n1\n, to learn the parameters\nğœ½\n\\bm{\\theta}\nof a given dynamic model\nf\nğœ½\nf_{\\bm{\\theta}}\n, we minimize the loss function defined in Eq.Â (\n8\n), which is computed by sampling the trajectories\nÏ„\ni\n\\tau_{i}\nfrom\nğ’Ÿ\n\\mathcal{D}\n. Note that evaluating\nâ„’\nâ€‹\n(\nğœ½\n)\n\\mathcal{L}(\\bm{\\theta})\nrequires setting a temporal horizon\nH\nH\n, the number of temporal lags on angular positions/velocities and actions used to construct\nğ’™\n~\nt\n\\tilde{\\bm{x}}_{t}\n, which is given by\nk\nk\n, and an architecture for an MLP or KAN to represent\nf\nğœ½\nf_{\\bm{\\theta}}\n. Moreover, as discussed in Section\nIV-A\n1\n, we can choose to predict angular position residuals,\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\n, or angular velocity residuals,\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n.\nTo manage the aforementioned possibilities, for each prediction type and for each type of neural network (MLP/KAN), we search for suitable hyperparameters through the optimization of the metric\nâ„’\nâ€‹\n(\nğœ½\n)\n|\nH\n=\n80\neval\n\\mathcal{L}(\\bm{\\theta})|^{\\text{eval}}_{H=80}\n, which has the same definition as the loss function used for training (see Eq.Â (\n8\n)), but is evaluated using\n100\n100\nshort trajectories sampled from the evaluation subset over a fixed time horizon,\nH\n=\n80\nH=80\n. This process is done using the Optuna library\n(Akiba\net al.\n,\n2019\n)\n. More details on this optimization process are documented in Appendix\nC\n.\nThe above produces a ranking of models with respect to the evaluation metric\nâ„’\nâ€‹\n(\nğœ½\n)\n|\nH\n=\n80\neval\n\\mathcal{L}(\\bm{\\theta})|^{\\text{eval}}_{H=80}\n. Since the top ranked models only differ by a slight margin with respect to\nâ„’\nâ€‹\n(\nğœ½\n)\n|\nH\n=\n80\neval\n\\mathcal{L}(\\bm{\\theta})|^{\\text{eval}}_{H=80}\n, we filter those with higher lags\nk\nk\nand prefer smaller models due to their faster inference time (which is specially relevant for the policy obtained using iCEM). Under these criteria, the selected models are characterized in Table\nI\n.\nTABLE I:\nSelected parameterizations and training hyperparameters for the dynamic models.\nPred.\nParameterization\nğ’Œ\n\\bm{k}\nğ‘¯\n\\bm{H}\nğ’\nâ€‹\nğ’“\n\\bm{lr}\nğ’\nğ‘©\n\\bm{n_{B}}\nğ“›\nâ€‹\n(\nğœ½\n)\n|\nğ‘¯\n=\nğŸ–ğŸ\neval\n\\bm{\\mathcal{L}(\\bm{\\theta})|^{\\text{{eval}}}_{H=80}}\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\nMLP\nâ€‹\n(\n128\n,\n128\n)\n\\text{MLP}(128,128)\n18\n18\n8\n8\n10\nâˆ’\n3\n10^{-3}\n512\n512\n5.586\nâ‹…\n10\nâˆ’\n3\n5.586\\cdot 10^{-3}\nKAN\nâ€‹\n(\n32\n,\n16\n,\n32\n)\n\\text{KAN}(32,16,32)\n18\n18\n8\n8\n10\nâˆ’\n5\n10^{-5}\n1024\n1024\n5.563\nâ‹…\n10\nâˆ’\n3\n5.563\\cdot 10^{-3}\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\nMLP\nâ€‹\n(\n512\n,\n128\n)\n\\text{MLP}(512,128)\n18\n18\n16\n16\n10\nâˆ’\n3\n10^{-3}\n256\n256\n5.675\nâ‹…\n10\nâˆ’\n3\n5.675\\cdot 10^{-3}\nKAN\nâ€‹\n(\n32\n,\n32\n,\n16\n)\n\\text{KAN}(32,32,16)\n18\n18\n16\n16\n10\nâˆ’\n4\n10^{-4}\n512\n512\n5.555\nâ‹…\n10\nâˆ’\n3\n5.555\\cdot 10^{-3}\nk\nk\n: temporal lags;\nH\nH\n: training loss horizon;\nl\nâ€‹\nr\nlr\n: learning rate;\nn\nB\nn_{B}\n: batch size.\nSince these learned models are meant to serve as a surrogate for the real Bobcat E10 arm dynamics, their performance is further assessed in an open-loop prediction setting. Fig.\n8\nshows a ground-truth trajectory of configurations and normalized discrete commands sampled from the evaluation subset, alongside the configurations predicted by the learned dynamic models. The ground-truth trajectory consists of\n10\n3\n10^{3}\nconfiguration-action pairs, which (as described in Section\nV-B\n) are captured at\n20\n20\nHz, so the open-loop predictions result in a rollout of\n50\n50\nseconds. Note that the models only have access to a first ground-truth construction of\nğ’™\n~\nt\n\\tilde{\\bm{x}}_{t}\nand to the sequence of ground truth actions, therefore, prediction errors compound.\nThe results obtained show that all the dynamic models can approximately capture the response of the actuators with low error (for short time horizons) in the evaluation subset. The dynamic models not being perfect is an expected result, given that their inputs lack relevant variables that influence the machineâ€™s responses, such as differential hydraulic pressures, the hydraulic fluidâ€™s temperature, and the motorâ€™s RPM. Moreover, the models are trained under a low data regime that does not contain samples from excitation signals purposefully designed for system identification, but only from the arm teleoperation.\nNote that since the purpose of these models is to enable the synthesis of reaching controllers via RL or MPC, we want them not only to achieve low error in the teleoperation evaluation subset, but also to behave properly outside the training distribution. The above is crucial because we want the reaching controllers to perform appropriately for arbitrary initial and target poses sampled from the restricted workspace. Moreover, out-of-distribution (OOD) action sequences will likely be fed to the dynamic models during the RL training and the MPC optimization, so inconsistent dynamic model transitions for OOD inputs will result in policies with a high reality gap, which may render them useless.\nGiven the above, it is not clear whether there is an overall best dynamic model across those selected through hyperparameter optimization. Therefore, using each of them to synthesize policies seems a necessary experimental step, as only through the comparison of their respective policies it would be possible to know which dynamic model serves its purpose the best. This is done in the next section.\nV-D\nSynthesis and evaluation of controllers for reaching\nUsing the best learned dynamic models, we can synthesize policies for reaching. To do so, the learned dynamic models\nf\nğœ½\nf_{\\bm{\\theta}}\nare used as surrogates of the real dynamics of the mini-excavator arm. Given an action\nğ’‚\nt\n\\bm{a}_{t}\nand an initial history of angular positions/velocities and previous actions to construct\nğ’™\n~\nt\n\\tilde{\\bm{x}}_{t}\n, we can get a next configuration for the hydraulic arm. Given a new control signal\nğ’‚\nt\n+\n1\n\\bm{a}_{t+1}\nand the predicted configuration to construct\nğ’™\n~\nt\n+\n1\n\\tilde{\\bm{x}}_{t+1}\n, and repeating the above procedure, we can obtain a sequence\n(\nğ’™\n~\nt\n,\nğ’‚\nt\n,\nâ€¦\n,\nğ’™\n~\nT\n,\nğ’‚\nT\n)\n(\\tilde{\\bm{x}}_{t},\\bm{a}_{t},...,\\tilde{\\bm{x}}_{T},\\bm{a}_{T})\nfor a given time horizon\nT\nT\n. If we refer to Section\nIV-B\n2\n, we can see that this sequence can then be used to compute a sequence of observations, actions, and rewards,\n(\nğ’\nt\n,\nğ’‚\nt\n,\nr\nt\n,\nâ€¦\nâ€‹\nğ’\nT\n,\nğ’‚\nT\n,\nr\nT\n)\n(\\bm{o}_{t},\\bm{a}_{t},r_{t},...\\bm{o}_{T},\\bm{a}_{T},r_{T})\n, which we can leverage to obtain policies via RL or MPC (see Sections\nIV-B\n4\nand\nIV-B\n5\n).\nIt is important to note that the selection of a given dynamic model to train an RL-based policy conditions the policyâ€™s observations, because observations are constructed using\n(\nğ’™\n~\nt\n,\nğ’‚\nt\n)\n(\\tilde{\\bm{x}}_{t},\\bm{a}_{t})\ntuples, and the definition of\nğ’™\n~\nt\n\\tilde{\\bm{x}}_{t}\nvaries depending on the dynamic modelâ€™s temporal lags and whether it predicts angular position or angular velocity residuals. However, since all selected dynamic models share the same number of temporal lags\nk\nk\n(see Table\nI\n), the main difference between the observations used as input for the RL-based policies is simply whether they are constructed using normalized angular positions or angular velocity estimations (see Eqs.Â (\n4\n) andÂ (\n7\n)).\nTo implement the above, we use the JAX-based computational pipeline described in Section\nV-A\n. Specifically, we implement a Brax-compliant environment that specifies the observations, actions, reward function, and episodic settings of the problem modeling, as described in Section\nIV-B\n2\n, and use the Bobcat E10 robot description illustrated in Fig.\n4\nto manage aspects such as the forward kinematics computation. This allows using the PPO implementation provided by Brax, and to easily parallelize the execution of rollouts based on the learned dynamic model predictions, which is crucial to achieve real-time control using MPC. The resulting environment can be rendered using the MuJoCo visualizer, as shown in Fig.\n9\n.\nUsing this environment, we can train RL-based policies using PPO for\n400\nâ€‹\nM\n400\\text{M}\nsteps, which only takes about\n6\n6\nmin. using a mid-range computer. Similarly, we can instantiate iCEM-based controllers that will provide action sequences that aim at maximizing cumulative rewards over short horizons (said rewards being obtained using the same reward function used for RL).\nFigure 9:\nTraining environment renderization in the MuJoCo visualizer, with markers to represent the restricted workspace\nğ’²\n+\n\\mathcal{W^{+}}\nand a randomly chosen target end-effector pose.\nTo evaluate the performance of the policies, we consider the following metrics:\nTABLE II:\nEvaluation results obtained in the Brax-based training environment. The performance metrics are computed across\n500\n500\nindependent reaching trials (episodes). The reported average and standard deviation per metric is computed across five independently synthesized controller per policy type.\nAlgo.\nDyn. model\nAvg. return\nSR\n(\n0.02\n,\n0.02\n)\n\\bm{(0.02,0.02)}\nSR\n(\n0.04\n,\n0.04\n)\n\\bm{(0.04,0.04)}\nOO\nğ’²\n\\bm{\\mathcal{W}}\nOO\nZ\n\\bm{Z}\nMin.\nğ’›\n\\bm{z}\n[m]\nPPO\nMLP\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\n644.45\n644.45\np  m 10.31\n0.97\nÂ±\n0.03\n0.97\\pm 0.03\n1.00\nÂ±\n0.00\n1.00\\pm 0.00\n0.13\nÂ±\n0.01\n0.13\\pm 0.01\n0.08\nÂ±\n0.01\n0.08\\pm 0.01\nâˆ’\n0.22\n-0.22\np  m 0.28\nMLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n683.36\n683.36\np  m 16.94\n0.99\nÂ±\n0.01\n0.99\\pm 0.01\n1.00\nÂ±\n0.00\n1.00\\pm 0.00\n0.15\nÂ±\n0.01\n0.15\\pm 0.01\n0.11\nÂ±\n0.01\n0.11\\pm 0.01\nâˆ’\n0.10\n-0.10\np  m 0.08\nKAN\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\n458.85\n458.85\np  m 17.13\n0.91\nÂ±\n0.02\n0.91\\pm 0.02\n0.98\nÂ±\n0.01\n0.98\\pm 0.01\n0.15\nÂ±\n0.02\n0.15\\pm 0.02\n0.10\nÂ±\n0.02\n0.10\\pm 0.02\nâˆ’\n1.17\n-1.17\np  m 0.04\nKAN\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\nâˆ—\nâˆ’\n220.78\n-220.78\np  m 604.06\n0.26\nÂ±\n0.21\n0.26\\pm 0.21\n0.42\nÂ±\n0.35\n0.42\\pm 0.35\n0.36\nÂ±\n0.25\n0.36\\pm 0.25\n0.24\nÂ±\n0.25\n0.24\\pm 0.25\nâˆ’\n0.71\n-0.71\np  m 0.34\niCEM\nMLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n15.96\n15.96\np  m 5.95\n0.20\nÂ±\n0.01\n0.20\\pm 0.01\n0.28\nÂ±\n0.01\n0.28\\pm 0.01\n0.03\nÂ±\n0.01\n0.03\\pm 0.01\n0.02\nÂ±\n0.01\n0.02\\pm 0.01\n0.14\n0.14\np  m 0.01\niCEM\nâ€ \nâˆ’\n348.59\n-348.59\np  m 1.10\n0.66\nÂ±\n0.01\n0.66\\pm 0.01\n0.78\nÂ±\n0.00\n0.78\\pm 0.00\n0.02\nÂ±\n0.00\n0.02\\pm 0.00\n0.01\nÂ±\n0.00\n0.01\\pm 0.00\n0.13\n0.13\np  m 0.02\niCEM\nâ€¡\nâˆ’\n148.48\n-148.48\np  m 0.31\n0.81\nÂ±\n0.01\n0.81\\pm 0.01\n0.91\nÂ±\n0.01\n0.91\\pm 0.01\n0.01\nÂ±\n0.00\n0.01\\pm 0.00\n0.00\nÂ±\n0.00\n0.00\\pm 0.00\n0.14\n0.14\np  m 0.01\nâˆ—\nLearning rate set to\n0.8\nâ‹…\n10\nâˆ’\n4\n0.8\\cdot 10^{-4}\nand evaluated at\n800\n800\nM training steps.\nâ€ \nr\nt\nÏµ\nr^{\\epsilon}_{t}\nset to zero.\nâ€¡\nr\nt\nÏµ\nr^{\\epsilon}_{t}\nand\nr\nt\nğ’³\nr_{t}^{\\mathcal{X}}\nset to zero.\nâ€¢\nAvg. return: Undiscounted sum of instantaneous rewards obtained up until\nT\nmax\n=\n500\nT_{\\text{max}}=500\nsteps pass, averaged across evaluation episodes.\nâ€¢\nSuccess rate (conditioned on precision requirements): Given a threshold for the Euclidean distance between the current and target end-effector poses,\nÎµ\nğ’‘\n\\varepsilon_{\\bm{p}}\n, and a threshold for the absolute difference between their pitch angles,\nÎµ\nÎ±\n\\varepsilon_{\\alpha}\n, we consider the reaching task as successful if\nb\nSR\nb_{\\text{SR}}\n, defined by Eq.Â (\n24\n), equals one at least once during a given episode.\nb\nSR\nt\n=\nâŸ¦\nd\n(\nğ’‘\ntarget\n,\nğ’‘\nt\neef\n)\n<\nÎµ\nğ’‘\n)\nâŸ§\nâ‹…\nâŸ¦\n|\nÎ±\npitch\nâˆ’\nÎ±\npitch\ntarget\n|\n<\nÎµ\nÎ±\nâŸ§\nb^{t}_{\\text{SR}}=\\bigl\\llbracket d(\\bm{p}^{\\text{target}},\\bm{p}_{t}^{\\text{eef}})<\\varepsilon_{\\bm{p}})\\bigr\\rrbracket\\cdot\\llbracket|\\alpha_{\\text{pitch}}-\\alpha_{\\text{pitch}}^{\\text{target}}|<\\varepsilon_{\\alpha}\\rrbracket\n(24)\nThus, the success rate (SR) of a reaching policy is computed as\nSR\n(\nÎµ\nğ’‘\n,\nÎµ\nÎ±\n)\n:=\n1\nN\nâˆ‘\ni\n=\n1\nN\nâŸ¦\nâˆ‘\nt\n=\n1\nT\nmax\nb\nSR\nt\n,\n(\ni\n)\nâ‰¥\n1\nâŸ§\n\\text{SR}(\\varepsilon_{\\bm{p}},\\varepsilon_{\\alpha}):=\\frac{1}{N}\\sum_{i=1}^{N}\\llbracket\\sum_{t=1}^{T_{\\text{max}}}b_{\\text{SR}}^{t,(i)}\\geq 1\\rrbracket\n, where the dependence on threshold selection is highlighted, and the metric is computed for\nN\nN\nepisodes, each of them with a time horizon\nT\nmax\nT_{\\text{max}}\n.\nâ€¢\nRate at which the end-effector leaves the restricted workspace\nğ’²\n+\n\\mathcal{W}^{+}\n(OO\nğ’²\n\\mathcal{W}\n): Consider the variable\nb\nğ’²\nb_{\\mathcal{W}}\ndefined in Eq.Â (\n25\n), which equals one if the end effector is within the restricted workspace.\nb\nğ’²\nt\n=\nâŸ¦\n(\nğ’³\nt\neef\nâˆˆ\nğ’²\n+\n)\nâŸ§\nb^{t}_{\\mathcal{W}}=\\llbracket(\\mathcal{X}_{t}^{\\text{eef}}\\in\\mathcal{W}^{+})\\rrbracket\n(25)\nSimilarly to the success rate, the out-of-workspace rate is defined as\nOO\nğ’²\n:=\n1\nN\nâˆ‘\ni\n=\n1\nN\nâŸ¦\nâˆ‘\nt\n=\n1\nT\nmax\n(\n1\nâˆ’\nb\nğ’²\nt\n,\n(\ni\n)\n)\nâ‰¥\n1\nâŸ§\n\\text{OO}\\mathcal{W}:=\\frac{1}{N}\\sum_{i=1}^{N}\\llbracket\\sum_{t=1}^{T_{\\text{max}}}(1-b^{t,(i)}_{\\mathcal{W}})\\geq 1\\rrbracket\nfor\nN\nN\nevaluation episodes, where each of them has a time horizon\nT\nmax\nT_{\\text{max}}\n.\nâ€¢\nRate at which the end-effector goes below the lower\nz\nz\n-axis boundary of\nğ’²\n+\n\\mathcal{W}^{+}\n(OO\nZ\nZ\n): Defined analogous to OO\nğ’²\n\\mathcal{W}\n. Note that OO\nZ\nZ\nâ‰¤\n\\leq\nOO\nğ’²\n\\mathcal{W}\n.\nâ€¢\nMinimum\nz\nz\n-axis component of the end-effector position across all the evaluated trajectories: This metrics measures the worst possible end-effector breach through the lower\nz\nz\n-axis plane that defines the restricted workspace.\nWith the above, we train RL-based policies for each best learned dynamic model (which are characterized in Table\nI\n). We consider five independent training trials per policy and evaluate them over a set of\nN\n=\n500\nN=500\nepisodes, each of them with fixed but different episodic conditions. For MPC-based policies, we do the same, but only using the MLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\nmodel, since it allows synthesizing the best RL-based controllers, as it will be explained later. Moreover, for all the policies we set a control frequency of\n20\n20\nHz (matching the time discretization of the dynamic models) and\n4\n4\naction repeats. In addition, all the RL-based policies are evaluated after\n270\n270\nM training steps, except for the PPO KAN\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\nwhose learning rate and training steps were further tuned due to worse performance being obtained when training under the same conditions than the rest of the controllers. The results obtained are presented in Table\nII\nand all the training details and hyperparameters used to obtain policies are documented in Appendix\nD\n.\nFrom training RL-based policies using PPO, we verified that using different dynamic models has an impact on the final controllerâ€™s performance. The first indicator of this difference lies in the cumulative reward obtained at the end of the training process, where the policies trained using the best KAN\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\nmodel achieve far lower avg. returns than the rest, which motivates filtering them from further analysis. We can also note a difference in average return (albeit less pronounced) between the policies trained using the KAN\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\nmodel and those trained using MLPs as surrogate dynamic models, the latter achieving better overall performances. It is also observed that the breaches through the lower plane of the restricted workspace are much less pronounced for the PPO-MLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\nmodel, as the â€œmin.\nz\nz\nâ€ metric is lower for this controller when compared to the rest of the RL-based policies.\nFrom training MPC-based policies, we note that optimizing slight variations of the reward function (by setting\nr\nt\nÏµ\n=\n0\nr_{t}^{\\epsilon}=0\nand\nr\nt\nÏµ\n=\nr\nt\nğ’³\n=\n0\nr_{t}^{\\epsilon}=r_{t}^{\\mathcal{X}}=0\n) improves performance. This result can be explained considering that\nr\nt\nÏµ\nr_{t}^{\\epsilon}\nis a positive scalar only given to the agent when certain conditions are fulfilled, which makes the reward function highly non linear and may make its optimization over short horizons more susceptible to convergence to local optima. Moreover, also setting\nr\nt\nğ’³\n=\n0\nr_{t}^{\\mathcal{X}}=0\nfurther improves results for iCEM, which can be explained considering the reward ablation documented in Appendix\nD\nfor an RL based policy (using the same dynamic model used by iCEM), which suggests that\nr\nt\nğ’’\nr_{t}^{\\bm{q}}\nand\nr\nt\nğ’³\nr_{t}^{\\mathcal{X}}\nmay be conflicting terms when optimized simultaneously. When comparing the overall performance of controllers based on RL and MPC, we can see that although RL policies in general achieve a higher success rate, iCEM controllers are better at avoiding breaching the restricted workspace boundaries. This result may be due to iCEM discarding the action sequences that would result in such breaches during optimization, and PPO policies, on the other hand, affording the cost associated with leaving\nğ’²\n+\n\\mathcal{W}^{+}\nwhenever a higher return (in the long run) can be achieved.\nThe results motivate studying the behavior of the best policies obtained in the real world. To do so, in the next section we conduct experiments to measure the performance of all the RL-based policies (except the PPO-KAN\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\npolicies) and the best iCEM controller when undergoing a Sim2Sim and a Sim2Real transfer.\nV-E\nSim2Sim and Sim2Real transfer of the learned controllers\nWe deploy the synthesized policies in the real-world by leveraging a ROS-based computational pipeline. Nevertheless, a pre-deployment stage using the Gazebo simulator was utilized to test the different system components prior to using them to control the real mini-excavator. In both the pre-deployment (Sim2Sim) and real-world deployment (Sim2Real) stages, the reaching controllers are encapsulated in action servers that, once queried with a target pose, will execute actions (according to an RL or MPC-based policy) until successfully reaching the target or preemptively stopping when an undesired behavior is detected. We must note, however, that to isolate the performance of the policies, all safety measures are disabled during the Sim2Sim and Sim2Real transfer experiments. The computational pipeline that allows implementing the pre-deployment and real-world deployment stages is described in Appendix\nB\n. In what follows, the performed experiments and their results are documented.\nV-E\n1\nSim2Sim transfer\nA first pre-deployment experiment is conducted using Gazebo with the reaching controllers encapsulated in ROS-based action servers. When evaluating a given policy, the learned dynamic model used to synthesize it is encapsulated in a module that allows bypassing the Gazebo simulator physics. This is done by using the inferences of these dynamic models, conditioned on previous joint positions/velocities and actions, to set the hydraulic arm configuration directly, similar to what is done in the Brax-based environment (see Appendix\nB\n).\nThe Sim2Sim transfer experiments use the exact same initial and target end-effector poses set during the experiments conducted in the Brax-based environment, for the same\n500\n500\ndistinct episodes (see Section\nV-D\n). Moreover, as in the Brax-based environment evaluation, the execution of the action servers for a given target pose is only terminated by timeout (after\n500\n500\ntime steps). In this case, however, only a single policy is evaluated (which corresponds to the first of the five synthesized controllers for each policy type). Under these conditions, this Sim2Sim transfer study allows measuring the effects of time delays due to the communication of messages to construct observations and set control commands in an isolated manner (as the learned dynamic models dictate the response of the impact hammer to actions). It is worth mentioning that such delays are not present in the Brax-based training environment, but exist during real world deployment.\nTo get a fine-grained performance measure of the controllers when evaluated under this new setting, we construct success rate matrices by computing the SR metric with thresholds\n(\nÎµ\nğ’‘\n,\nÎµ\nÎ±\n)\nâˆˆ\n{\n0.02\n,\n0.04\n,\nâ€¦\n,\n0.34\n}\nÃ—\n{\n0.02\n,\n0.04\n,\nâ€¦\n,\n0.16\n}\n(\\varepsilon_{\\bm{p}},\\varepsilon_{\\alpha})\\in\\{0.02,0.04,...,0.34\\}\\times\\{0.02,0.04,...,0.16\\}\n, which can be visualized in Appendix\nE\n. Table\nIII\nshows the performance metrics obtained, highlighting the success rates obtained at\n(\nÏµ\nğ’‘\n,\nÏµ\nÎ±\n)\n=\n(\n0.02\n,\n0.02\n)\n(\\epsilon_{\\bm{p}},\\epsilon_{\\alpha})=(0.02,0.02)\n, and\n(\nÏµ\nğ’‘\n,\nÏµ\nÎ±\n)\n=\n(\n0.12\n,\n0.08\n)\n(\\epsilon_{\\bm{p}},\\epsilon_{\\alpha})=(0.12,0.08)\n.\nTABLE III:\nSim2Sim transfer evaluation results obtained across\n500\n500\nindependent reaching trials.\nAlgo.\nDyn. model\nSR\na\nSR\nb\nOO\nğ’²\n\\bm{\\mathcal{W}}\nOO\nZ\n\\bm{Z}\nMin.\nğ’›\n\\bm{z}\n[m]\nBrax\nPPO\nMLP\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\n0.99\n0.99\n1.00\n1.00\n0.124\n0.124\n0.068\n0.068\nâˆ’\n0.066\n-0.066\nMLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n0.98\n0.98\n1.00\n1.00\n0.130\n0.130\n0.092\n0.092\nâˆ’\n0.103\n-0.103\nKAN\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\n0.92\n0.92\n1.00\n1.00\n0.136\n0.136\n0.088\n0.088\nâˆ’\n1.184\n-1.184\niCEM\nâ€¡\nMLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n0.81\n0.81\n0.98\n0.98\n0.012\n0.012\n0.004\n0.004\n0.124\n0.124\nGazebo\nPPO\nMLP\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\n0.02\n0.02\n0.93\n0.93\n0.124\n0.124\n0.066\n0.066\nâˆ’\n0.004\n-0.004\nMLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n0.11\n0.11\n1.00\n1.00\n0.118\n0.118\n0.068\n0.068\n0.000\n0.000\nKAN\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\n0.12\n0.12\n0.90\n0.90\n0.142\n0.142\n0.086\n0.086\nâˆ’\n0.574\n-0.574\niCEM\nâ€¡\nMLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n0.74\n0.74\n0.98\n0.98\n0.122\n0.122\n0.068\n0.068\n0.107\n0.107\na\nSR\n(\n0.02\n,\n0.02\n)\n(0.02,0.02)\n.\nb\nSR\n(\n0.12\n,\n0.08\n)\n(0.12,0.08)\n.\nâ€¡\nr\nt\nÏµ\nr^{\\epsilon}_{t}\nand\nr\nt\nğ’³\nr_{t}^{\\mathcal{X}}\nset to zero.\nThe results show a noticeable performance drop for the success rates at high precision requirements for the RL-based policies, while this is not the case for the iCEM controller. We attribute this to two complementary causes: the differences in the reward function optimized by RL-based policies and the iCEM controller, and the asynchronous nature of the Gazebo-ROS computational pipeline, which introduces delays that result in perturbations on the environment state transitions. Firstly, PPO policies are trained using the full definition of\nr\nt\nr_{t}\n(see Eq.Â (\n19\n)), while iCEM policies consider a slight variation of this function, where the terms\nr\nt\nğ’³\nr_{t}^{\\mathcal{X}}\nand\nr\nt\nÏµ\nr_{t}^{\\epsilon}\nare set to zero. One can think of\nr\nt\nÏµ\nr_{t}^{\\epsilon}\nas a bonus term encouraging the end-effector to stabilize on the target pose, since it is only non-zero if precision requirements over the end-effector pose and the arm configuration are fulfilled (see Eqs.Â (\n13\n)â€“(\n15\n)). This makes oscillations around the target pose less pronounced once it has been reached at a certain level of precision, however, when deploying the controllers in the Gazebo simulator, the dynamic models, the reaching controller, and the simulator itself, all run asynchronously, and that induces time delays that are not present when training and evaluating the policies in Brax. These perturbations harm the performance of RL-based policies at low success rate thresholds. In contrast, the iCEM controller tends to oscillate around the target (since it does not optimize for\nr\nt\nÏµ\nr_{t}^{\\epsilon}\n), which makes it more likely to eventually approach the target pose (during the whole execution of a given episode), regardless of the aforementioned perturbations. We must note, however, that iCEM is also affected by the asynchronous nature of Gazebo, as the OO\nğ’²\n\\mathcal{W}\nand OO\nZ\nZ\nmetrics are higher in this environment compared to Brax, however, the Min.\nz\nz\ncoordinate of its end-effector during all the experiments is still higher than those of RL-based policies. Despite these observations, when deployed in Gazebo, one can see that the PPO MLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\ncontroller nearly matches the SR of the iCEM controller at thresholds\n(\nÏµ\nğ’‘\n,\nÏµ\nÎ±\n)\n=\n(\n0.06\n,\n0.06\n)\n(\\epsilon_{\\bm{p}},\\epsilon_{\\alpha})=(0.06,0.06)\n, and surpasses it at\n(\nÏµ\nğ’‘\n,\nÏµ\nÎ±\n)\n=\n(\n0.08\n,\n0.04\n)\n(\\epsilon_{\\bm{p}},\\epsilon_{\\alpha})=(0.08,0.04)\nand higher (see Fig.\n14\nin Appendix\nE\n). Moreover, it matches or surpasses the precision levels of the rest of RL-based policies.\nV-E\n2\nSim2Real transfer\nThe policies are deployed in the real Bobcat E10 mini-excavator using the same ROS-based action servers used for the Sim2Sim transfer experiments. Again, the execution of the action servers is only stopped by a timeout after\n500\n500\ntime steps pass. Since in this case it is not possible to easily set random initial configurations for each reaching episode, given an initial configuration for the hydraulic arm, the evaluation experiments consist of attempting to reach\n100\n100\nposes, which are sequentially fed to the action servers. Given that episodes only finish by time-out, this translates in the target end effector pose changing every\n500\n500\ntime-steps, making the initial condition for episodes (except for the first one) dependent on the target pose of the previous episode and the performance of the policy itself. Moreover, the\n100\n100\ntarget end-effector poses coincide with the first\n100\n100\ntargets that are used for the evaluation of policies in simulation, both for the Brax-based environment and when using the Gazebo-ROS pipeline.\n(a)\n(b)\nFigure 10:\nSuccess rate matrices obtained when evaluating the reaching policies in (a) the Gazebo simulator and (b) the real-world, in both cases, by encapsulating the policies in action servers.\nTABLE IV:\nSim2Real transfer evaluation results obtained across\n100\n100\nsuccessive reaching trials.\nAlgo.\nDyn. model\nSR\na\nSR\nb\nOO\nğ’²\n\\bm{\\mathcal{W}}\nOO\nZ\n\\bm{Z}\nMin.\nğ’›\n\\bm{z}\n[m]\nGazebo\nPPO\nMLP\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\n0.04\n0.04\n0.94\n0.94\n0.040\n0.040\n0.010\n0.010\n0.091\n0.091\nMLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n0.08\n0.08\n1.00\n1.00\n0.090\n0.090\n0.020\n0.020\n0.005\n0.005\nKAN\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\n0.13\n0.13\n0.87\n0.87\n0.060\n0.060\n0.030\n0.030\n0.000\n0.000\niCEM\nâ€¡\nMLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n0.77\n0.77\n0.98\n0.98\n0.070\n0.070\n0.020\n0.020\n0.151\n0.151\nR. World\nPPO\nMLP\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\n0.01\n0.01\n0.43\n0.43\n0.010\n0.010\n0.010\n0.010\n0.154\n0.154\nMLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n0.02\n0.02\n0.99\n0.99\n0.070\n0.070\n0.040\n0.040\n0.050\n0.050\nKAN\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\n0.00\n0.00\n0.69\n0.69\n0.020\n0.020\n0.020\n0.020\n0.099\n0.099\niCEM\nâ€¡\nMLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n0.08\n0.08\n0.71\n0.71\n0.200\n0.200\n0.050\n0.050\n0.120\n0.120\na\nSR\n(\n0.02\n,\n0.02\n)\n(0.02,0.02)\n.\nb\nSR\n(\n0.12\n,\n0.08\n)\n(0.12,0.08)\n.\nâ€¡\nr\nt\nÏµ\nr^{\\epsilon}_{t}\nand\nr\nt\nğ’³\nr_{t}^{\\mathcal{X}}\nset to zero.\nFigure 11:\nReal-world PPO-MLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\npolicy execution for two sequentially queried target poses.\nThe execution of these experiments for policies trained using PPO with dynamic models MLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n, MLP\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\nand KAN\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\n, and for the iCEM controller alongside the MLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\nmodel results in the success rate matrices shown in Fig.\n10\nand in the performance metrics summarized in Table\nIV\n. Moreover, these experiments translate into approximately\n50\n50\nminutes of autonomous operation per evaluated controller, whose footage, for a qualitative evaluation of performance, can be seen in the following link:\nhttps://youtu.be/lshZwPQMlFg\n.\nThe results obtained show that RL-based controllers display a detriment on their success rate at low Euclidean distances and pitch angle error thresholds when comparing their performance in the real world and in the Gazebo-ROS pipeline. The reaching controller that is the least affected by the Sim2Real transfer and also achieves better overall performance remains the PPO-MLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\npolicy, achieving a near perfect success rate for the sequential queried target poses\n100\n100\nat thresholds\n(\nÏµ\nğ’‘\n,\nÏµ\nÎ±\n)\n=\n(\n0.12\n,\n0.08\n)\n(\\epsilon_{\\bm{p}},\\epsilon_{\\alpha})=(0.12,0.08)\n. Moreover, contrary to what was observed in the Sim2Sim experiments for the iCEM controller, in the real world it drastically dropped its success rate for all thresholds while also achieving higher OO\nğ’²\n\\mathcal{W}\nand OO\nZ\nZ\nvalues.\nWe note that a common failure mode observed for the PPO KAN\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\nand the iCEM controller during real world deployment was that sometimes they were unable to generate commands that resulted in an actual displacement of the hydraulic arm actuators. This happened whenever the mini-excavatorâ€™s arm reached certain configurations and specific target poses were being queried. In such situations, the policies would often output commands set to zero for all joints or would rapidly alternate the values of certain action components, which, given the hydraulic coupling and high actuation delays of the armâ€™s hydraulic cylinders, resulted in no movement, regardless of the enforced action regularization via the reward function definition, and the action repeats used to synthesize and deploy the controllers. This behavior may be explained due to the learned dynamic models being imperfect and producing misleading transitions in certain regions of the state space, which translates into inadequate control strategies for certain situations and, therefore, poor performance when facing those situations in the real world.\nThe fact that the PPO MLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\npolicy achieves good performance and does not display the failure modes that iCEM shows (although both depend on the same dynamic model), may be due to the PPO MLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\npolicy, since parameterized as an MLP itself, has the capacity to â€œaverageâ€ its behavior across poorly represented regions of the state space; moreover, given that its observations are mostly constructed using velocity estimations, it is likely that this policy puts less importance on the instantaneous configuration of the arm, which may help generalization.\nFinally, in addition to the Sim2Real transfer experiment documented above, we further tested the PPO MLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\npolicy, setting fixed euclidean distance and absolute pitch error thresholds to\nÏµ\nğ’‘\n=\n0.12\n\\epsilon_{\\bm{p}}=0.12\n[m] and\nÏµ\nÎ±\n=\n0.08\n\\epsilon_{\\alpha}=0.08\n[rad] to decide whether a target pose was reached, so as to assess its performance in a setting more close to the practical application of the controller. The result of these experiments is qualitatively presented in Fig.\n11\nfor two sequentially queried target poses, and the corresponding video with further trials can be found in\nhttps://youtu.be/e-7tDhZ4ZgA\n.\nVI\nConclusion\nIn this work, we presented a practical, data-driven methodology to automate hydraulic impact hammers such as those used for secondary reduction in mining. This challenge is addressed considering operational constraints that may be present in real mining operations, such as unobserved state variables, and the requirement of relying on a discrete control interface. In particular, a methodology for learning dynamic models using teleoperation data and then exploiting said models to obtain policies for the task of reaching target end-effector poses is presented and validated. Said validation is performed both in simulation and the real world, using a Bobcat E10 mini-excavator with an impact hammer attached as end-effector.\nThe results obtained are promising given that the best obtained reaching policy is able to consistently reach target poses with relatively high precision in the real world, regardless of the mini-excavator arm being inherently challenging to control due to partially observed states, actuation delays, and hydraulic coupling, and using discrete commands at the joint level to perform the reaching task.\nHowever, the developed reaching controller has some drawbacks. One of its limitations is that it does not fully comply with the restrictions over the end-effector position given by the restricted workspace, and enforced by the reward term\nr\nt\nğ’²\nr_{t}^{\\mathcal{W}}\n. This limitation could be alleviated by activating the safety measures included in the action server and using the control supervisor module included in the deployment computational pipeline (see Appendix\nB\n), since if the action server is stopped due to the end-effector leaving the workspace or due to the detection of oscillatory behaviors, a feedback signal reporting the unsuccessful completion of the reaching task could be sent to an hypothetical higher level system (e.g., a state machine) that in turn could trigger a recovery behavior.\nHowever, the most notable limitation of the reaching policies is that they are not informed by exteroceptive information, so it would not be able for them to avoid collisions if the constrained workspace in which the impact hammer operates ceases to be obstacle free. This would be troublesome for the deployment of these controller in a real mining operation, since frontal loaders and LHDs depositing material above ore passesâ€™ grills should be regarded as potential obstacles, regardless of additional safety measures. Moreover, the grill itself and the rocks remaining above it should also be regarded as obstacles, because grill geometries vary across ore passes and moving from one target pose to another may induce an unintended collision with a rock. Therefore, developing a collision-aware reaching controller is left for future work.\nFinally, the performance gap between iCEM and PPO-based policies in certain regions of the state space motivates investigating whether constraining the action search space to regions closer to the training distribution could improve MPC real-world performance.\nFigure 12:\nSimplified hydraulic circuit of the intervened Bobcat E10 mini-excavator (based on the original Bobcat E10 documentation and the modifications implemented on the machine).\nAppendix A\nSimplified Bobcat E10 hydraulic circuit\nFig.\n12\nshows a simplified hydraulic circuit for the Bobcat E10 mini-excavator. From right to left, this diagram can be read as follows:\nâ€¢\nThe supply line provided by the pumps goes through a pressure control sub-circuit that generates a pilot signal, and also directly feeds the (simplified) hydraulic block displayed in the diagram.\nâ€¢\nThe pilot signal feeds eight proportional electro-valves that are controlled via discrete current signals sent to them using an I/O module (see Fig.\n2\n). The electro-valvesâ€™ output goes to the valves within the hydraulic block shown in the diagram.\nâ€¢\nThe valves within the hydraulic block control the four actuators of the mini-excavator arm, that is, the hydraulic cylinders associated to the â€œboomâ€, â€œswingâ€, â€œarmâ€ and â€œbucketâ€ joints (see Fig.\n4\n).\nâ€¢\nThe hydraulic block also feeds the supply line to an on-off electro-valve that actuates the impact hammer end-effector. Note that this on-off valve is also controlled by using the I/O module mentioned previously.\nFurthermore, this diagram shows some aspects of the Bobcat E10 mini-excavator that make controlling its arm challenging. Note, for instance, that controlling the armâ€™s hydraulic cylinders depends on a pilot stage and a main hydraulic stage, which contributes to actuation delays. Also note that the valves within the hydraulic block share supply, which contributes to coupling across actuators.\nAppendix B\nROS-based computational pipelines\nTo deploy reaching controllers in the real-world, a ROS-based pipeline was developed. This computational pipeline, along with its components and their interactions, is illustrated in Fig.\n13\n. To minimize discrepancies between components, the pipeline only bifurcates from the drivers (which are distinct for the simulation in Gazebo and the real world) until the end, resulting in two distinct execution branches.\nFor the pre-deployment stage, the learned dynamic models described in Section\nV-C\nwere used to replicate the behavior of the real machine in the Gazebo simulator. To do so, these models are instantiated in the\nbobcat_e10_sim_driver\ncomponent. This module allows bypassing Gazeboâ€™s built-in physics, enforcing the dynamics learned from the data obtained by operating the real machine, allowing the replication of hard-to-model effects, such as command-response delays.\nThe reaching controller is encapsulated within the\nautonomous_server\nmodule, which allows selecting among MPC and RL-based policies and provides them with the necessary data to populate the buffers they use to construct\nğ’™\n~\nt\n\\tilde{\\bm{x}}_{t}\n. This data is obtained from the\nJointStates\nmessages, which specify the joint positions and velocities of the impact hammer. Through this server, the user can specify a target end-effector pose. The same server generates the corresponding commands, sends them to the subsequent components, maintains a fixed control frequency, and monitors terminal conditions to determine when to stop the hammerâ€™s operation, for instance, upon reaching the goal within a predefined tolerance, leaving the workspace, or exhibiting oscillatory behaviors. The two latter safety measures, however, are fully turned off during the experiments reported in this work, so as to measure the controllersâ€™ performance in an isolated manner.\nAt this stage, it can be observed that the commands generated by both the\nteleoperation_server\nand the\nautonomous_server\nfollow the same pipeline. Whenever a command is sent, it is received by the\ncontrol_supervisor\nand the\ncontrol_interface\nmodules. The\ncontrol_supervisor\nmodule is responsible for enforcing several safety constraints. It monitors factors such as control frequency, command delays, and potential self-collisions. If an abnormal condition is detected, it triggers an emergency stop signal to stop the hammer. This signal is received by the\ncontrol_interface\n, which converts the commands into a different message type that can be consumed by the simulated or real machine drivers. When the\ncontrol_supervisor\nissues an emergency signal, the\ncontrol_interface\nimmediately mutes all outgoing commands. However, we must note that to isolate the performance of the real policiesâ€™ during evaluations in Gazebo and in the real world, the\ncontrol_supervisor\nmodule remains disabled for all experiments conducted in this work.\nFinally, in the case of the real machine, the\nbobcat_e10_driver\ntranslates the incoming ROS commands into CAN frames, which are received by the\nemcb_200u_driver\nand then written to the machineâ€™s PLC. This component also reads information from the PLC and shares it with the Bobcat driver through CAN frames, which are parsed, pre-processed, and published to the rest of the system; in particular, this allows the population of data to construct the\nJointState\nmessages of the real machine.\nFigure 13:\nROS-based pipeline for the reaching controller when deployed in simulation (using Gazebo) and in the real world.\nAppendix C\nTraining details for dynamic models\nTable\nV\nshows the search space of all the hyperparameters that are jointly optimized when attempting to learn dynamic models via different parameterizations (MLPs or KANs). Note that for a given parameterization and hyperparameter, the search space does not make a distinction between the type of prediction of the dynamic model (\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\nor\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n). Also note that all hyperpamater search spaces are discrete and, except for\nk\nk\n(the number of lags used to construct the approximate state\nğ’™\n~\nt\n\\tilde{\\bm{x}}_{t}\n), are defined with values that are of common usage in standard machine learning practice. For\nk\nk\n, the search space is defined considering the maximum measured actuation delay (around one second) when controlling each actuator of the mini-excavator arm with a constant joint command (\nâˆ’\n1\n-1\nor\n1\n1\n, prior to denormalization) at\n20\n20\nHz.\nWhen training MLPs, we use sigmoid linear unit (SiLU) activation functions for each hidden layer and a linear activation function for the output layer. When training KANs, we use Legendre basis functions for each layer. The instantiation of KANs in JAX is done using the jaxKAN package\n(Rigas and Papachristou,\n2025\n)\n.\nTABLE V:\nSearch spaces for the dynamic models hyperparameters.\nParameterization\nHyperparameter\nSearch space\nMLP & KAN\nNb. of lags\nk\nk\n{\n18\n,\nâ€¦\n,\n25\n}\n\\{18,...,25\\}\nLearning rate\n{\n10\nâˆ’\n3\n,\n10\nâˆ’\n3\n,\n10\nâˆ’\n5\n}\n\\{10^{-3},10^{-3},10^{-5}\\}\nBatch size\n{\n256\n,\n512\n,\n1024\n,\n2048\n}\n\\{256,512,1024,2048\\}\nMLP\nLoss time horizon\nH\nH\n{\n4\n,\n8\n,\n16\n,\n32\n}\n\\{4,8,16,32\\}\nNb. of layers\n{\n2\n,\n3\n,\n4\n,\n5\n}\n\\{2,3,4,5\\}\nNb. of units per layer\n{\n64\n,\n128\n,\n256\n,\n512\n}\n\\{64,128,256,512\\}\nKAN\nLoss time horizon\nH\nH\n{\n8\n,\n16\n,\n32\n}\n\\{8,16,32\\}\nNb. of layers\n{\n3\n,\n4\n,\n5\n}\n\\{3,4,5\\}\nNb. of unites per layer\n{\n4\n,\n8\n,\n16\n,\n32\n}\n\\{4,8,16,32\\}\nTo adjust the hyperparameters shown in Table\nV\n, we use the Optuna package. To cover the range of possibilities for the type of model parameterization (MLP or KAN) and prediction (\nÎ”\nâ€‹\nğ’’\n\\Delta\\bm{q}\nor\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\n), four independent optimization processes are conducted, each of them for models parameterized using an MLP or a KAN, and predicting angular position or angular velocity residuals. The optimization, in all cases, is performed with respect to\nâ„’\nâ€‹\n(\nğœ½\n)\n|\nH\n=\n80\n\\mathcal{L}(\\bm{\\theta})|_{H=80}\n, which, as stated in Section\nV-C\n, corresponds to the loss function used for training with a fixed time horizon,\nH\n=\n80\nH=80\n.\nIn all cases, we train each model for a maximum of\n1000\n1000\nand\n1200\n1200\nepochs for MLP and KAN parameterizations, respectively, and use the validation subset to end a given trial if\nâ„’\nâ€‹\n(\nğœ½\n)\n|\nH\n=\n80\n\\mathcal{L}(\\bm{\\theta})|_{H=80}\ndoes not decrease for\n50\n50\nconsecutive training steps (early stopping). The optimization process allows us to obtain the four top dynamic models that are studied in Section\nV-C\n.\nAppendix D\nFurther details regarding policy synthesis\nTable\nVI\nshows the hyperparameters and the values they take for the control and modeling aspects associated with the reaching task. This table also presents the parameters set for policy synthesis using PPO and iCEM.\nTABLE VI:\nModeling and algorithms hyperparameters.\nParameter\nValue\nControl\nControl frequency [Hz]\n20\n20\nAction repeats\n4\n4\nLoop tracker\nk\np\n,\nk\ni\nk_{p},k_{i}\n10.0\n,\n60.0\n10.0,60.0\nEpisodic\nconditions\nÎ±\npitch\nmax\n,\nÎ±\npitch\nts\n\\alpha^{\\text{max}}_{\\text{pitch}},\\alpha^{\\text{ts}}_{\\text{pitch}}\n[rad]\n1.047\n,\n0.698\n1.047,0.698\nz\nmin\nts\n,\nz\nmax\nts\nz^{\\text{ts}}_{\\text{min}},z^{\\text{ts}}_{\\text{max}}\n[m]\n0.155\n,\n2.355\n0.155,2.355\nMax number of steps\nT\nreset\nT_{\\text{reset}}\n500\nReward\nfunction\nÎ»\nğ’‘\n,\nÎ»\nğ’“\n,\nÎ»\nğ’’\n,\nÎ»\nğ’‚\n,\nÎ»\nğ’²\n+\n\\lambda_{\\bm{p}},\\lambda_{\\bm{r}},\\lambda_{\\bm{q}},\\lambda_{\\bm{a}},\\lambda_{\\mathcal{W}^{+}}\n1.0\n,\n1.0\n,\n1.0\n,\n0.5\n,\n2.0\n1.0,1.0,1.0,0.5,2.0\nÏµ\nğ’‘\n,\nÏµ\nğ’“\n,\nÏµ\nğ’‘\nâ€²\n,\nÏµ\nğ’“\nâ€²\n\\epsilon_{\\bm{p}},\\epsilon_{\\bm{r}},\\epsilon^{\\prime}_{\\bm{p}},\\epsilon^{\\prime}_{\\bm{r}}\n0.1\n,\n0.1\n,\n0.05\n,\n0.02\n0.1,0.1,0.05,0.02\nÏµ\nÎ±\n,\nÏµ\nq\n,\nw\nğ’³\n,\nw\nÎ±\n,\nw\nğ’’\n\\epsilon_{\\alpha},\\epsilon_{q},w_{\\mathcal{X}},w_{\\alpha},w_{\\bm{q}}\n0.0001\n,\n0.0025\n,\n0.5\n,\n1.0\n,\n1.0\n0.0001,0.0025,0.5,1.0,1.0\nPPO\nTraining steps\n400\n400\nM\nLearning rate\n3\nâ‹…\n10\nâˆ’\n4\n3\\cdot 10^{-4}\nEntropy cost\n10\nâˆ’\n2\n10^{-2}\nDiscount factor\nÎ³\n\\gamma\n0.97\n0.97\nUnroll length\n40\n40\nClipping\nÏµ\n\\epsilon\n0.2\nBatch size\n256\n256\nNumber of mini-batches\n32\n32\nNumber of environments\n128\n128\nUpdates per batch\n4\n4\nPolicy parameterization\nMLP\n(\n256\n,\n)\nÃ—\n4\n\\text{MLP}(256,)_{\\times 4}\nCritic parameterization\nMLP\n(\n256\n,\n)\nÃ—\n5\n\\text{MLP}(256,)_{\\times 5}\niCEM\nHorizon\nH\nMPC\nH_{\\text{MPC}}\n20\n20\nNumber of samples\nN\npop\nN_{\\text{pop}}\n500\n500\nNumber of elites\nN\ne\nN_{e}\n50\n50\nInitial std\nÏƒ\n0\n\\sigma_{0}\n0.5\n0.5\nSmoothing coefficient\nÎ±\n\\alpha\n0.05\n0.05\nOptimization steps\nK\nK\n12\n12\nExponent\n2.0\n2.0\nElite set fraction\n0.5\n0.5\nAction repeats\n4\n4\nThe policy learning process using PPO is conducted using a computer equipped with an Intel i7-12700 CP CPU, a NVIDIA RTX 4060 GPU, and 32 GB of RAM. Training the reaching policies for\n400\nâ€‹\nM\n400\\text{M}\nof steps takes approximately\n6\n6\nmin. on average. The iCEM controller instantiated with the parameters reported in Table\nVI\n(running on the same computer) averages an optimization computation time of approx.\n90\n90\nms. This result makes this method suitable for controlling the machine at\n20\n20\nHz when using\n4\n4\naction repeats, considering that the experiments conducted in Gazebo (Sim2Sim) show that the lag in the generation of control commands that the above induces does not affect performance severely (see Section\nV-E\n1\n).\nTABLE VII:\nPerformance of the PPO MLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\npolicy when trained using different reward functions.\nReward function\nAvg. Return\nâˆ—\nSR\n(\n0.02\n,\n0.02\n)\n\\bm{(0.02,0.02)}\nSR\n(\n0.04\n,\n0.04\n)\n\\bm{(0.04,0.04)}\nOO\nğ’²\n\\bm{\\mathcal{W}}\nOO\nZ\n\\bm{Z}\nMin.\nğ’›\n\\bm{z}\n[m]\nEEF-PL [m]\nr\nt\nğ’³\nr_{t}^{\\mathcal{X}}\nâˆ’\n201.65\n-201.65\np  m 98.81\n0.30\nÂ±\n0.33\n0.30\\pm 0.33\n0.44\nÂ±\n0.38\n0.44\\pm 0.38\n0.22\nÂ±\n0.04\n0.22\\pm 0.04\n0.12\nÂ±\n0.03\n0.12\\pm 0.03\nâˆ’\n0.20\nÂ±\n0.12\n-0.20\\pm 0.12\n4.50\nÂ±\n1.04\n4.50\\pm 1.04\nr\nt\nğ’’\nr_{t}^{\\bm{q}}\nâˆ’\n91.17\n-91.17\np  m 0.49\n0.97\nÂ±\n0.02\n0.97\\pm 0.02\n1.00\nÂ±\n0.00\n1.00\\pm 0.00\n0.21\nÂ±\n0.01\n0.21\\pm 0.01\n0.16\nÂ±\n0.01\n0.16\\pm 0.01\nâˆ’\n0.16\nÂ±\n0.01\n-0.16\\pm 0.01\n2.41\nÂ±\n0.04\n2.41\\pm 0.04\nr\nt\nğ’³\n+\nr\nt\nÏµ\nr_{t}^{\\mathcal{X}}+r_{t}^{\\epsilon}\n842.83\n842.83\np  m 20.26\n1.00\nÂ±\n0.01\n1.00\\pm 0.01\n1.00\nÂ±\n0.00\n1.00\\pm 0.00\n0.16\nÂ±\n0.01\n0.16\\pm 0.01\n0.12\nÂ±\n0.01\n0.12\\pm 0.01\nâˆ’\n0.24\nÂ±\n0.06\n-0.24\\pm 0.06\n2.12\nÂ±\n0.04\n2.12\\pm 0.04\nr\nt\nğ’’\n+\nr\nt\nÏµ\nr_{t}^{\\bm{q}}+r_{t}^{\\epsilon}\n870.74\n870.74\np  m 10.72\n1.00\nÂ±\n0.00\n1.00\\pm 0.00\n1.00\nÂ±\n0.00\n1.00\\pm 0.00\n0.22\nÂ±\n0.01\n0.22\\pm 0.01\n0.17\nÂ±\n0.01\n0.17\\pm 0.01\nâˆ’\n0.19\nÂ±\n0.09\n-0.19\\pm 0.09\n2.10\nÂ±\n0.02\n2.10\\pm 0.02\nr\nt\nğ’³\n+\nr\nt\nğ’’\nr_{t}^{\\mathcal{X}}+r_{t}^{\\bm{q}}\nâˆ’\n290.18\n-290.18\np  m 68.68\n0.59\nÂ±\n0.35\n0.59\\pm 0.35\n0.77\nÂ±\n0.21\n0.77\\pm 0.21\n0.27\nÂ±\n0.10\n0.27\\pm 0.10\n0.20\nÂ±\n0.09\n0.20\\pm 0.09\nâˆ’\n0.30\nÂ±\n0.21\n-0.30\\pm 0.21\n3.83\nÂ±\n1.37\n3.83\\pm 1.37\nr\nt\nğ’³\n+\nr\nt\nğ’’\n+\nr\nt\nÏµ\nr_{t}^{\\mathcal{X}}+r_{t}^{\\bm{q}}+r_{t}^{\\epsilon}\n753.17\n753.17\np  m 12.60\n1.00\nÂ±\n0.00\n1.00\\pm 0.00\n1.00\nÂ±\n0.00\n1.00\\pm 0.00\n0.18\nÂ±\n0.01\n0.18\\pm 0.01\n0.13\nÂ±\n0.01\n0.13\\pm 0.01\nâˆ’\n0.24\nÂ±\n0.06\n-0.24\\pm 0.06\n2.10\nÂ±\n0.03\n2.10\\pm 0.03\nr\nt\nğ’³\n+\nr\nt\nğ’’\n+\nr\nt\nÏµ\n+\nr\nt\nğ’‚\nr_{t}^{\\mathcal{X}}+r_{t}^{\\bm{q}}+r_{t}^{\\epsilon}+r_{t}^{\\bm{a}}\n719.32\n719.32\np  m 14.33\n0.99\nÂ±\n0.01\n0.99\\pm 0.01\n1.00\nÂ±\n0.00\n1.00\\pm 0.00\n0.19\nÂ±\n0.02\n0.19\\pm 0.02\n0.14\nÂ±\n0.01\n0.14\\pm 0.01\nâˆ’\n0.24\nÂ±\n0.06\n-0.24\\pm 0.06\n2.14\nÂ±\n0.03\n2.14\\pm 0.03\nr\nt\nğ’³\n+\nr\nt\nğ’’\n+\nr\nt\nğ’‚\n+\nr\nt\nğ’²\nr_{t}^{\\mathcal{X}}+r_{t}^{\\bm{q}}+r_{t}^{\\bm{a}}+r_{t}^{\\mathcal{W}}\nâˆ’\n356.61\n-356.61\np  m 203.17\n0.59\nÂ±\n0.30\n0.59\\pm 0.30\n0.78\nÂ±\n0.37\n0.78\\pm 0.37\n0.16\nÂ±\n0.04\n0.16\\pm 0.04\n0.10\nÂ±\n0.03\n0.10\\pm 0.03\nâˆ’\n0.12\nÂ±\n0.11\n-0.12\\pm 0.11\n3.97\nÂ±\n1.77\n3.97\\pm 1.77\nr\nt\nğ’³\n+\nr\nt\nğ’’\n+\nr\nt\nÏµ\n+\nr\nt\nğ’‚\n+\nr\nt\nğ’²\nr_{t}^{\\mathcal{X}}+r_{t}^{\\bm{q}}+r_{t}^{\\epsilon}+r_{t}^{\\bm{a}}+r_{t}^{\\mathcal{W}}\n683.36\n683.36\np  m 16.94\n0.99\nÂ±\n0.01\n0.99\\pm 0.01\n1.00\nÂ±\n0.00\n1.00\\pm 0.00\n0.15\nÂ±\n0.01\n0.15\\pm 0.01\n0.11\nÂ±\n0.01\n0.11\\pm 0.01\nâˆ’\n0.10\nÂ±\n0.08\n-0.10\\pm 0.08\n2.15\nÂ±\n0.05\n2.15\\pm 0.05\nâˆ—\nFor each row, the computation of returns depends on the reward function used for training.\nFor the PPO policy trained using the learned MLP\nÎ”\nâ€‹\nğ’’\nË™\n\\Delta\\dot{\\bm{q}}\nmodel, an ablation study on the components of the reward function is provided in Table\nVII\n. Besides the evaluation metrics used in the results presented in Sections\nV-D\nand\nV-E\n, we include the avg. traversed end-effector path length (EEF-PL), which is computed by summing the Euclidean distance between successive end-effector positions for a given episode up until the target is reached with a certain precision (\n0.02\n0.02\n[cm] and\n0.02\n0.02\n[rad]), or until\nT\nmax\nT_{\\text{max}}\nsteps pass. The distances obtained are then averaged across the evaluation episodes.\nThe results suggest that including the\nr\nt\nÏµ\nr_{t}^{\\epsilon}\nterm in the reward function definition is crucial for achieving high success rates at high precision for controllers trained using PPO.\nThe results also show that good performance in terms of success rate can be achieved by relying on\nr\nt\nÏµ\nr_{t}^{\\epsilon}\nalongside either\nr\nt\nğ’’\nr_{t}^{\\bm{q}}\n,\nr\nt\nğ’³\nr_{t}^{\\mathcal{X}}\nor both terms. We must note that this is expected, since both\nr\nt\nğ’’\nr_{t}^{\\bm{q}}\nand\nr\nt\nğ’³\nr_{t}^{\\mathcal{X}}\nserve the purpose of guiding the robot towards the target pose; however, combining\nr\nt\nğ’’\nr_{t}^{\\bm{q}}\nand\nr\nt\nğ’³\nr_{t}^{\\mathcal{X}}\nwithout the bonus term results in high variance across training trials. We attribute this behavior to\nr\nt\nğ’’\nr_{t}^{\\bm{q}}\nand\nr\nt\nğ’³\nr_{t}^{\\mathcal{X}}\nbeing potentially conflicting, and\nr\nt\nÏµ\nr_{t}^{\\epsilon}\nserving as a term that dominates the optimization landscape due to the high reward the agent receives if it reaches the target pose. Finally, we must also note that, regardless of the potential tension between terms, we choose to use both\nr\nt\nğ’’\nr_{t}^{\\bm{q}}\nand\nr\nt\nğ’³\nr_{t}^{\\mathcal{X}}\nin the final reward function definition because the agentâ€™s observations are constructed using both position-quaternion tuples and the associated configurations to represent current and target end-effector poses.\nAppendix E\nFurther results regarding the Sim2Sim transfer experiments\nFig.\n14\nshows the success rate matrices obtained by evaluating the RL- and MPC-based reaching policies in the Brax-based environment in which they were synthesized (Fig.\n14a\n) and the matrices obtained when evaluating the same policies in Gazebo (Fig.\n14b\n). For a detailed analysis on these results, refer to Section\nV-E\n1\n.\n(a)\n(b)\nFigure 14:\nSuccess rate matrices obtained when evaluating the reaching policies in (a) the Brax-based environment used for training, and (b) the Gazebo simulator, by encapsulating the policies in action servers.\nAcknowledgments\nThe authors thank Claudio Palacios for designing and implementing the electro-hydraulic interventions that were required to automate the BobcatÂ E10. The authors also acknowledge Pablo Alfessi for creating a CAD model of the BobcatÂ E10, and for designing, manufacturing, and installing the mountings of the rotaryâ€™s encoders on the robotâ€™s arm. Finally, the authors also thank Cristian Rivera for implementing most of the additional instrumentation required for automating the machine, and for designing and implementing the program that runs on the PLC.\nReferences\nT. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama (2019)\nOptuna: a next-generation hyperparameter optimization framework\n.\nIn\nThe 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining\n,\npp.Â 2623â€“2631\n.\nCited by:\nÂ§\nV-C\n.\nF. A. Bender, S. GÃ¶ltz, T. BrÃ¤unl, and O. Sawodny (2017)\nModeling and offset-free model predictive control of a hydraulic mini excavator\n.\nIEEE Transactions on Automation Science and Engineering\n14\n,\npp.Â 1682â€“1694\n.\nExternal Links:\nLink\nCited by:\nÂ§II\n.\nJ. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang (2018)\nJAX: composable transformations of Python+NumPy programs\nExternal Links:\nLink\nCited by:\nÂ§\nV-A\n.\nM. Correa, D. CÃ¡rdenas, D. Carvajal, and J. Ruiz-del-Solar (2022)\nHaptic teleoperation of impact hammers in underground mining\n.\nApplied Sciences\n12\n(\n3\n),\npp.Â 1428\n.\nCited by:\nÂ§I\n,\nÂ§II\n,\nÂ§\nIII-A\n.\nP. De Boer, D. P. Kroese, S. Mannor, and R. Y. Rubinstein (2005)\nA tutorial on the cross-entropy method\n.\nAnnals of operations research\n134\n(\n1\n),\npp.Â 19â€“67\n.\nCited by:\nÂ§\nIV-B\n5\n.\nG. Dulac-Arnold, R. Evans, H. van Hasselt, P. Sunehag, T. Lillicrap, J. Hunt, T. Mann, T. Weber, T. Degris, and B. Coppin (2015)\nDeep reinforcement learning in large discrete action spaces\n.\narXiv preprint arXiv:1512.07679\n.\nCited by:\nÂ§\nIV-B\n4\n.\nP. Egli and M. Hutter (2020)\nTowards rl-based hydraulic excavator automation\n.\nIn\n2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 2692â€“2697\n.\nCited by:\nÂ§II\n,\nÂ§II\n,\nÂ§II\n.\nP. Egli and M. Hutter (2022)\nA general approach for the automation of hydraulic excavator arms using reinforcement learning\n.\nIEEE Robotics and Automation Letters\n7\n,\npp.Â 5679â€“5686\n.\nExternal Links:\nLink\nCited by:\nÂ§II\n,\nÂ§II\n,\nÂ§II\n.\nP. Egli, L. Terenzi, and M. Hutter (2024)\nReinforcement learning-based bucket-filling for autonomous excavation\n.\nIEEE Transactions on Field Robotics\n.\nCited by:\nÂ§II\n.\nC. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem (2021)\nBrax - a differentiable physics engine for large scale rigid body simulation\nExternal Links:\nLink\nCited by:\nÂ§\nV-A\n.\nL. Greiser, O. Demir, B. Hartmann, H. Hose, and S. Trimpe (2024)\nFeedforward controllers from learned dynamic local model networks with application to excavator assistance functions\n.\n2024 IEEE 63rd Conference on Decision and Control (CDC)\n,\npp.Â 4105â€“4112\n.\nExternal Links:\nLink\nCited by:\nÂ§II\n.\nS. R. Habibi and R. J. Richards (1991)\nComputed-torque and variable-structure multi-variable control of a hydraulic industrial robot\n.\nProceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering\n205\n,\npp.Â 123 â€“ 140\n.\nExternal Links:\nLink\nCited by:\nÂ§II\n.\nJ. Heek, A. Levskaya, A. Oliver, M. Ritter, B. Rondepierre, A. Steiner, and M. van Zee (2024)\nFlax: a neural network library and ecosystem for JAX\nExternal Links:\nLink\nCited by:\nÂ§\nV-A\n.\nT. Jiang, Y. Guan, L. Ma, J. Xu, J. Meng, W. Chen, Z. Zeng, L. Li, D. Wu, and R. Chen (2024)\nDexSim2RealÂ²: building explicit world model for precise articulated object dexterous manipulation\n.\nCoRR\nabs/2409.08750\n.\nNote:\narXiv preprint arXiv:2409.08750\nCited by:\nÂ§\nIV-B\n5\n.\nD. Jud, S. Kerscher, M. Wermelinger, E. Jelavic, P. Egli, P. Leemann, G. Hottiger, and M. Hutter (2021)\nHEAP - the autonomous walking excavator\n.\nArXiv\nabs/2106.05059\n.\nExternal Links:\nLink\nCited by:\nÂ§II\n.\nN. Koenig and A. Howard (2004)\nDesign and use paradigms for gazebo, an open-source multi-robot simulator\n.\nIn\n2004 IEEE/RSJ international conference on intelligent robots and systems (IROS)(IEEE Cat. No. 04CH37566)\n,\nVol.\n3\n,\npp.Â 2149â€“2154\n.\nCited by:\nÂ§\nV-A\n.\nS. Lampinen, J. Niemi, and J. Mattila (2020)\nFlow-bounded trajectory-scaling algorithm for hydraulic robotic manipulators\n.\nIn\n2020 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)\n,\npp.Â 619â€“624\n.\nCited by:\nÂ§II\n,\nÂ§II\n.\nS. Lampinen, L. Niu, L. Hulttinen, J. Niemi, and J. Mattila (2021)\nAutonomous robotic rock breaking using a real-time 3d visual perception system\n.\nJournal of Field Robotics\n38\n(\n7\n),\npp.Â 980â€“1006\n.\nCited by:\nÂ§II\n,\nÂ§II\n,\nÂ§II\n,\nÂ§\nIV-A\n.\nM. Lee, H. Choi, C. Kim, J. Moon, D. Kim, and D. Lee (2022)\nPrecision motion control of robotized industrial hydraulic excavators via data-driven model inversion\n.\nIEEE Robotics and Automation Letters\n7\n(\n2\n),\npp.Â 1912â€“1919\n.\nCited by:\nÂ§II\n.\nC. Li, Z. Ai, T. Wu, X. Li, W. Ding, and H. Xu (2024)\nDeformNet: latent space modeling and dynamics prediction for deformable object manipulation\n.\nCoRR\nabs/2402.07648\n.\nNote:\narXiv preprint arXiv:2402.07648\nCited by:\nÂ§\nIV-B\n5\n.\nZ. Liu, Y. Wang, S. Vaidya, F. Ruehle, J. Halverson, M. Soljacic, T. Y. Hou, and M. Tegmark (2025)\nKAN: kolmogorovâ€“arnold networks\n.\nIn\nThe Thirteenth International Conference on Learning Representations\n,\nExternal Links:\nLink\nCited by:\nÂ§\nIV-A\n1\n.\nD. Ma, Y. Liu, W. Liu, and B. Zhou (2024)\nA data-driven modeling and motion control of heavy-load hydraulic manipulators via reversible transformation\n.\narXiv preprint arXiv:2411.13856\n.\nCited by:\nÂ§II\n.\nD. Ma and B. Zhou (2024)\nData-driven multi-step nonlinear model predictive control for industrial heavy load hydraulic robot\n.\nArXiv\nabs/2411.13859\n.\nExternal Links:\nLink\nCited by:\nÂ§II\n.\nJ. Mattila, J. E. M. KoivumÃ¤ki, D. G. Caldwell, and C. Semini (2017)\nA survey on control of hydraulic robotic manipulators with projection to future trends\n.\nIEEE/ASME Transactions on Mechatronics\n22\n,\npp.Â 669â€“680\n.\nExternal Links:\nLink\nCited by:\nÂ§II\n.\nJ. Nurmi and J. Mattila (2017)\nAutomated feed-forward learning for pressure-compensated mobile hydraulic valves with significant dead-zone\n.\nIn\nFluid Power Systems Technology\n,\nVol.\n58332\n,\npp.Â V001T01A027\n.\nCited by:\nÂ§II\n.\nJ. Park, B. Lee, S. Kang, P. Y. Kim, and H. J. Kim (2017)\nOnline learning control of hydraulic excavators based on echo-state networks\n.\nIEEE Transactions on Automation Science and Engineering\n14\n,\npp.Â 249â€“259\n.\nExternal Links:\nLink\nCited by:\nÂ§II\n.\nC. Pinneri, S. Sawant, S. Blaes, J. Achterhold, J. Stueckler, M. Rolinek, and G. Martius (2021)\nSample-efficient cross-entropy method for real-time planning\n.\nIn\nConference on Robot Learning\n,\npp.Â 1049â€“1065\n.\nCited by:\nÂ§\nIV-B\n5\n.\nA. R. Plummer and N. D. Vaughan (1990)\nRobust adaptive control for hydraulic servosystems\n.\nJournal of Dynamic Systems Measurement and Control-transactions of The Asme\n118\n,\npp.Â 237â€“244\n.\nExternal Links:\nLink\nCited by:\nÂ§II\n.\nM. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, A. Y. Ng,\net al.\n(2009)\nROS: an open-source robot operating system\n.\nIn\nICRA workshop on open source software\n,\nVol.\n3\n,\npp.Â 5\n.\nCited by:\nÂ§\nIV-A\n,\nÂ§\nV-A\n.\nS. Rigas and M. Papachristou (2025)\nJaxKAN: a unified jax framework for kolmogorov-arnold networks\n.\nJournal of Open Source Software\n10\n(\n108\n),\npp.Â 7830\n.\nCited by:\nAppendix C\n.\nP. Samtani, F. Leiva, and J. Ruiz-del-Solar (2023)\nLearning to break rocks with deep reinforcement learning\n.\nIEEE Robotics and Automation Letters\n8\n(\n2\n),\npp.Â 1077â€“1084\n.\nCited by:\nÂ§II\n,\nÂ§II\n,\nÂ§\nIII-B\n.\nC. Sancaktar, S. Blaes, and G. Martius (2022)\nCurious exploration via structured world models yields zero-shot object manipulation\n.\nIn\nAdvances in Neural Information Processing Systems 35 (NeurIPS 2022)\n,\npp.Â 24170â€“24183\n.\nExternal Links:\nLink\nCited by:\nÂ§\nIV-B\n5\n.\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017)\nProximal policy optimization algorithms\n.\narXiv preprint arXiv:1707.06347\n.\nCited by:\nÂ§\nIV-B\n4\n.\nM. R. Sirouspour and S. E. Salcudean (2001)\nNonlinear control of hydraulic robots\n.\nIEEE Trans. Robotics Autom.\n17\n,\npp.Â 173â€“182\n.\nExternal Links:\nLink\nCited by:\nÂ§II\n.\nG. A. Sohl and J. E. Bobrow (1997)\nExperiments and simulations on the nonlinear control of a hydraulic servosystem\n.\nProceedings of the 1997 American Control Conference (Cat. No.97CH36041)\n1\n,\npp.Â 631â€“635 vol.1\n.\nExternal Links:\nLink\nCited by:\nÂ§II\n.\nB. Song and A. J. Koivo (1995)\nNeural adaptive control of excavators\n.\nProceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots\n1\n,\npp.Â 162â€“167 vol.1\n.\nExternal Links:\nLink\nCited by:\nÂ§II\n.\nF. A. Spinelli, P. Egli, J. Nubert, F. Nan, T. Bleumer, P. Goegler, S. Brockes, F. Hofmann, and M. Hutter (2024)\nReinforcement learning control for autonomous hydraulic material handling machines with underactuated tools\n.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 12694â€“12701\n.\nCited by:\nÂ§II\n,\nÂ§II\n.\nF. A. Spinelli, Y. Zhai, F. Nan, P. Egli, J. Nubert, T. Bleumer, L. Miller, F. Hofmann, and M. Hutter (2025)\nLarge scale robotic material handling: learning, planning, and control\n.\narXiv preprint arXiv:2508.09003\n.\nCited by:\nÂ§II\n.\nE. Todorov, T. Erez, and Y. Tassa (2012)\nMuJoCo: a physics engine for model-based control\n.\nIn\n2012 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp.Â 5026â€“5033\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nV-A\n.\nP. J. Werbos (1988)\nGeneralization of backpropagation with application to a recurrent gas market model\n.\nNeural networks\n1\n(\n4\n),\npp.Â 339â€“356\n.\nCited by:\nÂ§\nIV-A\n1\n.",
    "preview_text": "This paper presents a data-driven methodology for the control of static hydraulic impact hammers, also known as rock breakers, which are commonly used in the mining industry. The task addressed in this work is that of controlling the rock-breaker so its end-effector reaches arbitrary target poses, which is required in normal operation to place the hammer on top of rocks that need to be fractured. The proposed approach considers several constraints, such as unobserved state variables due to limited sensing and the strict requirement of using a discrete control interface at the joint level. First, the proposed methodology addresses the problem of system identification to obtain an approximate dynamic model of the hydraulic arm. This is done via supervised learning, using only teleoperation data. The learned dynamic model is then exploited to obtain a controller capable of reaching target end-effector poses. For policy synthesis, both reinforcement learning (RL) and model predictive control (MPC) algorithms are utilized and contrasted. As a case study, we consider the automation of a Bobcat E10 mini-excavator arm with a hydraulic impact hammer attached as end-effector. Using this machine, both the system identification and policy synthesis stages are studied in simulation and in the real world. The best RL-based policy consistently reaches target end-effector poses with position errors below 12 cm and pitch angle errors below 0.08 rad in the real world. Considering that the impact hammer has a 4 cm diameter chisel, this level of precision is sufficient for breaking rocks. Notably, this is accomplished by relying only on approximately 68 min of teleoperation data to train and 8 min to evaluate the dynamic model, and without performing any adjustments for a successful policy Sim2Real transfer. A demonstration of policy execution in the real world can be found in https://youtu.be/e-7tDhZ4ZgA.\n\nData-driven control of hydraulic impact hammers under strict operational and co",
    "is_relevant": true,
    "relevance_score": 3.0,
    "extracted_keywords": [
        "Reinforcement Learning"
    ],
    "one_line_summary": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ å’Œæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼Œç”¨äºåœ¨ä¸¥æ ¼çº¦æŸä¸‹æ§åˆ¶æ¶²å‹å†²å‡»é”¤çš„æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T18:45:41Z",
    "created_at": "2026-01-21T12:09:12.676804",
    "updated_at": "2026-01-21T12:09:12.676810"
}