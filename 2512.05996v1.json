{
    "id": "2512.05996v1",
    "title": "FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting",
    "authors": [
        "Yi Liu",
        "Jingyu Song",
        "Vedanth Kallakuri",
        "Katherine A. Skinner"
    ],
    "abstract": "åˆ†ææ°´ä¸‹é±¼ç±»å›¾åƒå¯¹ç”Ÿæ€ç›‘æµ‹è‡³å…³é‡è¦ï¼Œä½†ç”±äºè§†è§‰è´¨é‡é€€åŒ–ä¸æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œè¿™é¡¹ä»»åŠ¡ä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºFishDetector-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¼±ç›‘ç£æ¡ä»¶ä¸‹å®ç°é±¼ç±»æ£€æµ‹ã€åˆ†å‰²ä¸è®¡æ•°ã€‚åœ¨DeepFishæ•°æ®é›†ä¸Šï¼Œè¯¥æ¡†æ¶ç›¸æ¯”åŸºçº¿æ–¹æ³•å–å¾—æ˜¾è‘—æå‡ï¼šå¹³å‡ç²¾åº¦æé«˜20%ï¼Œå¹³å‡äº¤å¹¶æ¯”æå‡10%ï¼ŒåŒæ—¶å¹³å‡ç»å¯¹è¯¯å·®é™ä½30%ï¼Œç½‘æ ¼å¹³å‡è¯¯å·®å‡å°‘35%ã€‚è¿™äº›æ”¹è¿›æºäºä¸¤å¤§æ ¸å¿ƒè®¾è®¡ï¼šä¸€æ˜¯æ–°é¢–çš„â€œæ£€æµ‹åˆ°è®¡æ•°â€æç¤ºæœºåˆ¶ï¼Œç¡®ä¿ç©ºé—´ä¸€è‡´çš„æ£€æµ‹ä¸è®¡æ•°ç»“æœï¼›äºŒæ˜¯åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆåˆ©ç”¨ç¨€ç–ç‚¹æ ‡æ³¨çš„å¯æ‰©å±•èŒƒå¼ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†è¯¥å¥–åŠ±è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å…¶ä»–æ°´ä¸‹æ•°æ®é›†ä¸Šä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯å®äº†å…¶å¼ºå¤§çš„è·¨åŸŸé²æ£’æ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒFishDetector-R1é€šè¿‡å¼±ç›‘ç£ä¸ºç²¾å‡†æµ·æ´‹è§†è§‰ç†è§£æä¾›äº†å¯é ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚é¡¹ç›®é¡µé¢è¯¦è§ï¼šhttps://umfieldrobotics.github.io/FishDetector-R1ã€‚",
    "url": "https://arxiv.org/abs/2512.05996v1",
    "html_url": "https://arxiv.org/html/2512.05996v1",
    "html_content": "FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting\nYi Liu\n*\n, Jingyu Song\n*\n, Vedanth Kallakuri, Katherine A. Skinner\nUniversity of Michigan, Ann Arbor, MI USA\nAbstract\nAnalyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce\nFishDetector-R1\n, a unified MLLM-based framework for fish detection, segmentation, and counting under\nweak supervision\n. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel\ndetect-to-count\nprompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (\nRLVR\n) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is\nhttps://umfieldrobotics.github.io/FishDetector-R1\n.\nâ€ \nâ€ \nfootnotetext:\n*\nEqual contribution. Corresponding at:\njingyuso@umich.edu\nâ€ \nâ€ \nfootnotetext:\nThis work was supported by a Propelling Original Data Science (PODS) Grant from the Michigan Institute for Data and AI in Society (MIDAS) at the University of Michigan.\n1\nIntroduction\nRecent advances in underwater perception systems have underscored the growing need for robust visual understanding of marine environments, where visual data play a central role in ecological monitoring, fisheries management, and underwater exploration\n[\n33\n,\n31\n,\n32\n]\n. Effective analysis requires not only detecting fish but also performing instance-level segmentation and estimating their counts, which supports downstream tasks such as species identification, behavioral analysis, and habitat mapping. However, these tasks remain particularly challenging in underwater imagery, where low visibility, color distortion, and light scattering severely degrade the performance of conventional vision models.\nFigure 1\n:\nOur proposed FishDetector-R1 aims to achieve AI-enabled fish image analysis with the guidance of sparse point labels and text prompts.\nOver the past decade, a range of learning-based approaches have been proposed to address these challenges. Fully supervised instance segmentation methods achieve strong performance but rely heavily on large-scale, densely annotated datasets\n[\n39\n,\n25\n,\n2\n,\n5\n]\n. The high cost and labor intensity of generating pixel-wise annotations make such approaches difficult to scale in underwater environments\n[\n18\n,\n28\n,\n17\n]\n. As a more annotation-efficient alternative, point-level weak supervision offers significant advantages in terms of speed and scalability\n[\n4\n,\n16\n]\n. However, existing weakly supervised methods based on point annotations often suffer from a substantial performance gap relative to fully supervised models because sparse points provide limited pixel-level guidance\n[\n15\n,\n30\n]\n.\nThis leaves a key question: how can we close the weak-to-dense performance gap in challenging underwater settings while still relying only on sparse, scalable point-level labels?\nWe address this gap with two complementary ingredients. First, we find that foundation models are well positioned to fill this gap due to their transferable visual understanding capabilities from large-scale pretraining. Multimodal large language models (MLLMs) like GPT-4 series\n[\n12\n,\n1\n,\n24\n]\n, Qwen2.5-VL series\n[\n3\n]\nand Gemini series\n[\n34\n,\n6\n]\ncombine rich semantic knowledge with strong reasoning capability, while segmentation foundation models such as SAM\n[\n14\n,\n27\n]\ncan complement them by providing robust semantic priors for accurate mask generation from sparse prompts, enabling effective deployment in visually challenging domains.\nSecond, we develop an effective framework that tailors the MLLM to the specific challenges of underwater fish visual analysis. We propose a novel joint detectâ€“to-count task formulation that turns sparse point labels into consistent, verifiable reward signals to enforce spatial alignment between the predicted detection and counting number. Building on recent successes of Reinforcement Learning from Verifiable Rewards (RLVR) for adapting foundation models\n[\n20\n,\n21\n,\n37\n,\n22\n,\n38\n]\n, we fine-tune the MLLM under this detectâ€“to-count objective, yielding mutually reinforcing gains in detection and counting while supplying precise spatial priors that guide segmentation mask generation effectively. To the best of our knowledge, we are the first to integrate an MLLM with a segmentation foundation model to tackle scalable marine fish visual analysisâ€”covering detection, instance segmentation, and countingâ€”using only weak point-level supervision.\nTogether, these two ingredients constitute\nFishDetector-R1\n(\nFig.\n1\n), a unified framework for detection, segmentation, and counting from weak point-level supervision. FishDetector-R1 moves beyond prior approaches that treat these tasks in isolation and yields concurrent improvements across all three tasks. To summarize, our contributions are as follows:\n1.\nWe propose FishDetector-R1, the first unified framework to integrate an MLLM with a segmentation foundation model for comprehensive marine fish analysis (detection, segmentation, and counting) using only weak, point-level supervision.\n2.\nWe design a novel joint detectâ€“to-count learning paradigm to adapt foundation models to the challenging underwater domain in a complementary manner. By formulating sparse point labels as verifiable rewards within an RLVR framework, our method enforces spatial and numerical consistency, enabling the generation of high-quality masks from minimal annotation.\n3.\nWe conduct extensive quantitative and qualitative experiments on the DeepFish dataset\n[\n25\n]\nto demonstrate the effectiveness of the proposed FishDetector-R1 pipeline, achieving performance competitive with and even surpassing fully supervised methods. We further validate its strong generalization through zero-shot transfer on another underwater dataset\n[\n13\n]\n.\n2\nRelated Work\n2.1\nFish Detection in Underwater Scenes\nFully supervised segmentation methods\n[\n39\n,\n17\n,\n7\n]\nachieve high accuracy in underwater scenes by training on dense pixel-wise annotations. However, such labels are time-consuming and costly to obtain, especially in underwater imagery where object boundaries are often ambiguous\n[\n28\n,\n17\n]\n. To reduce annotation cost, weakly supervised approaches\n[\n15\n,\n16\n,\n30\n]\nuse point-level labels that are faster to collect\n[\n4\n]\n, but typically yield lower segmentation performance due to the lack of dense spatial supervision, creating a clear gap between fully and weakly supervised models. While prior methods improve annotation efficiency, none have closed this gap on challenging underwater segmentation tasks. In contrast,\nFishDetector-R1\nis the first framework to effectively leverage only point-level supervision to achieve high-quality instance segmentation, matching or surpassing fully supervised baselines on the DeepFish benchmark\n[\n25\n]\n.\n2.2\nVisual Foundation Models\nVisual foundation models such as SAM\n[\n14\n]\nand SAM 2\n[\n27\n]\nprovide flexible segmentation from simple prompts like points or bounding boxes and demonstrate strong generalization across diverse visual domains. Their ability to operate in a zero-shot setting makes them attractive for domains with limited labels. Recent adaptations to underwater imagery, such as AquaSAM\n[\n36\n]\nand WaterSAM\n[\n11\n]\n, attempt to specialize these models by either freezing encoders or introducing lightweight adapters to improve segmentation under challenging visual conditions like turbidity and color distortion. While effective, their reliance on dense supervision limits scalability and practicality in annotation-scarce scenarios. In contrast, our method effectively leverages sparse point-level labels to train a unified framework for joint detection, segmentation, and counting with reinforcement fine-tuning, supporting scalable deployment.\nFigure 2\n:\nOverview of the proposed FishDetector-R1 framework\n. A two-stage detect-to-count pipeline integrates an MLLM with SAM 2 to jointly perform detection, segmentation, and counting. Reinforcement fine-tuning with GRPO and weak point-level supervision adapts the MLLM, ensuring consistency between detection and counting while enabling pixel-wise segmentation with only sparse labels.\n2.3\nMultimodal Large Language Models\nMLLMs such as GPT-4.1\n[\n24\n]\n, Llama\n[\n35\n]\n, and Qwen2.5-VL\n[\n3\n]\ncombine visual perception with natural language reasoning, enabling object grounding, spatial reasoning, and prompt-based visual interaction. These models show strong potential in general domains for zero-shot grounding and segmentation by aligning semantic priors from language instructions with visual content. However, their application to underwater imagery remains largely unexplored, even though underwater monitoring demands high-level reasoning to discern subtle visual cues. In this work, we adapt an open-source MLLM with weak point-level supervision to generate reliable bounding boxes and keypoints under noisy underwater conditions. These semantic priors then guide SAM 2 for instance-level segmentation, bridging the gap between high-level reasoning and fine-grained perception in an annotation-efficient manner.\n2.4\nReinforcement Learning for MLLMs\nReinforcement Learning from Verifiable Reward (RLVR) has emerged as an effective strategy to improve the reasoning and alignment capabilities of both language models and multimodal models\n[\n40\n]\n.While traditional methods like PPO\n[\n29\n]\nand DPO\n[\n26\n]\nare widely adopted, more recent approaches such as GRPO\n[\n9\n]\noffer improved stability and efficiency via group preference optimization. Building on this, frameworks like Perception-R1\n[\n38\n]\n, Seg-R1\n[\n37\n]\n, and VisionReasoner\n[\n21\n]\nextend RL fine-tuning to multimodal perception, showing strong results in detection, segmentation, and counting. However, these works typically treat each task in isolation with separate reward functions, and focus on general-domain benchmarks, leaving domain-specific settings like underwater imagery underexplored. Our work addresses this gap by applying GRPO with point-level supervision and a unified reward design that jointly couples detection and counting. This enables all three tasksâ€”detection, segmentation, and countingâ€”to reinforce one another, improving adaptation under weak supervision.\n3\nMethodology\n3.1\nOverview\nWe propose a two-stage framework,\nFishDetector-R1\n, that integrates an MLLM (Qwen2.5-VL\n[\n3\n]\n) with a segmentation foundation model (SAM 2\n[\n27\n]\n) for underwater fish detection, segmentation, and counting as illustrated in\nFig.\n2\n. In the first stage, guided by a detect-to-count prompt, Qwen2.5-VL takes an input image, localizes each fish with a bounding box and keypoint, and then derives the total count from its detections, promoting consistency between localization and counting. In the second stage, these spatial priors are passed to SAM 2 to generate high-resolution pixel-wise instance masks. To further adapt the framework to underwater imagery, we apply RL fine-tuning to Qwen2.5-VL with weak point labels. This training step precedes SAM 2, ensuring that the MLLM learns to generate spatially consistent detections and counts, which then serve as strong priors for segmentation. Motivated by recent findings (e.g., Perception-R1\n[\n38\n]\n), we directly apply RL fine-tuning without a preliminary supervised fine-tuning (SFT) stage. Our empirical results confirm that this strategy improves task performance while preserving annotation efficiency.\n3.2\nPrompt Design\nTo support joint detection, segmentation, and counting, we design a structured prompt tailored for\nQwen2.5-VL\n, an MLLM with strong grounding and reasoning capabilities. As shown in\nFig.\n3\n, given an underwater RGB image, the model is prompted to first localize each fish instance with the total count directly derived from detections, following a detect-to-count strategy. This formulation encourages the model to understand that reliable counting depends on accurate localization, i.e., it must â€œknow where the fish areâ€ before reporting how many fish there are (example in\nFig.\n4\n). The resulting detection outputs â€“ bounding boxes and keypoints â€“ are also passed as spatial priors to\nSAM 2\n, enabling high-quality instance segmentation. In this way, the prompt design unifies all three tasks within a single pipeline.\nWe adopt a structured output format composed of three distinct components:\n<think>\n,\n<detection>\n, and\n<fish_count>\n.\nâ€¢\nThe\n<think>\nfield records the modelâ€™s internal reasoning and visual understanding process.\nâ€¢\nThe\n<detection>\nfield contains structured outputs for each fish instance, including a bounding box and a central keypoint, which both support counting and serve as effective prompts for SAM 2.\nâ€¢\nThe\n<fish_count>\nfield provides the total number of fish, derived from the detections to ensure consistency between localization and counting.\nThis design enforces a detect-to-count reasoning process, provides explicit spatial cues to guide segmentation, and ensures response completeness. Furthermore, during RL fine-tuning, the predicted count is compared against weak point-level annotations to construct reward signals, aligning detection and counting objectives without requiring dense labels.\nFigure 3\n:\nExample Q&A pairs from FishDetector-R1 using our designed detect-to-count prompt.\n3.3\nGroup Relative Policy Optimization\nFollowing recent RL fine-tuning work on MLLMs\n[\n20\n,\n37\n,\n21\n]\n, we adopt Group Relative Policy Optimization (GRPO)\n[\n9\n]\nas our post-training strategy. GRPO is an efficient reinforcement learning framework that removes the need for a separate critic by directly comparing the relative quality of responses within a group. Given a task input\nt\nt\n, the current policy\nÏ€\nÎ¸\nold\n\\pi_{\\theta_{\\text{old}}}\ngenerates a set of\nG\nG\ncandidate responses\n{\no\n1\n,\no\n2\n,\nâ€¦\n,\no\nG\n}\n\\{o_{1},o_{2},\\ldots,o_{G}\\}\nwith corresponding rewards\n{\nr\n1\n,\nr\n2\n,\nâ€¦\n,\nr\nG\n}\n\\{r_{1},r_{2},\\ldots,r_{G}\\}\n. These rewards are normalized within the group to compute relative advantages, which are then used to update the policy. This group-wise formulation provides more stable optimization while reducing training costs compared to traditional actorâ€“critic methods.\nThe GRPO objective function is defined as:\nğ’¥\nGRPO\nâ€‹\n(\nÎ¸\n)\n=\n\\displaystyle\\mathcal{J}_{\\text{GRPO}}(\\theta)=\nğ”¼\no\ni\nâˆ¼\nÏ€\nÎ¸\nold\n[\n1\nG\nâˆ‘\ni\n=\n1\nG\nmin\n(\nÏ€\nÎ¸\nâ€‹\n(\no\ni\n|\nt\n)\nÏ€\nÎ¸\nold\nâ€‹\n(\no\ni\n|\nt\n)\nA\n^\ni\n,\n\\displaystyle\\;\\mathbb{E}_{o_{i}\\sim\\pi_{\\theta_{\\text{old}}}}\\Bigg[\\frac{1}{G}\\sum_{i=1}^{G}\\min\\Bigg(\\frac{\\pi_{\\theta}(o_{i}|t)}{\\pi_{\\theta_{\\text{old}}}(o_{i}|t)}\\hat{A}_{i},\nclip\n(\nÏ€\nÎ¸\nâ€‹\n(\no\ni\n|\nt\n)\nÏ€\nÎ¸\nold\nâ€‹\n(\no\ni\n|\nt\n)\n,\n1\nâˆ’\nÏµ\n,\n1\n+\nÏµ\n)\nA\n^\ni\n)\n]\n\\displaystyle\\quad\\text{clip}\\left(\\frac{\\pi_{\\theta}(o_{i}|t)}{\\pi_{\\theta_{\\text{old}}}(o_{i}|t)},1-\\epsilon,1+\\epsilon\\right)\\hat{A}_{i}\\Bigg)\\Bigg]\nâˆ’\nÎ²\nâ€‹\nD\nKL\nâ€‹\n(\nÏ€\nÎ¸\nâˆ¥\nÏ€\nref\n)\n\\displaystyle-\\beta D_{\\text{KL}}(\\pi_{\\theta}\\|\\pi_{\\text{ref}})\n(1)\nwhere\nÏµ\n\\epsilon\nis the clipping threshold,\nÎ²\n\\beta\nis the coefficient of the KL penalty, and\nA\n^\ni\n\\hat{A}_{i}\ndenotes the normalized advantage for response\no\ni\no_{i}\n, computed as:\nA\n^\ni\n=\nr\ni\nâˆ’\nmean\nâ€‹\n(\n{\nr\n1\n,\nâ€¦\n,\nr\nG\n}\n)\nstd\nâ€‹\n(\n{\nr\n1\n,\nâ€¦\n,\nr\nG\n}\n)\n\\hat{A}_{i}=\\frac{r_{i}-\\text{mean}(\\{r_{1},\\ldots,r_{G}\\})}{\\text{std}(\\{r_{1},\\ldots,r_{G}\\})}\n(2)\nBy leveraging group-wise comparisons and reward normalization, GRPO enables stable and sample-efficient policy optimization purely based on relative preferences.\n3.4\nReward Design\nThe overall reward function used for RL fine-tuning consists of four components: (1) a format reward, (2) an accuracy reward, (3) a count reward, and (4) a non-repetition reward. Each component is designed to encourage the model to produce syntactically valid, semantically accurate, and non-redundant outputs that align with weak point-level supervision.\nFormat Reward.\nThe format reward\nR\nformat\nR_{\\text{format}}\nhas two sub-parts: response structure formatting and detection content formatting.\nâ€¢\nThe response is required to contain three structured tags:\n<think>\n,\n<detection>\n, and\n<fish_count>\n.\nA correct structural response yields 1.0 reward.\nâ€¢\nThe content within the\n<detection>\ntag must follow the format:\n{\"bbox_2d\": [x1, y1, x2, y2], \"point_2d\": [x, y], \"label\": \"fish\"}\n.\nIf all predicted instances match this structure, the model receives up to 3.0 additional reward points.\nDetection Accuracy Reward.\nTo encourage correct detection and precise localization, we design an accuracy reward\nR\ndetect\nR_{\\text{detect}}\n. Predicted keypoints are matched to ground-truth points using the Hungarian algorithm within a Euclidean distance threshold. A prediction is considered valid if its distance to a ground-truth point is within a predefined threshold. The reward is defined as:\naccuracy_reward\n=\nÎ»\ndetect\nâ‹…\n(\nN\nvalid\nN\ngt\n)\n,\n\\text{accuracy\\_reward}=\\lambda_{\\text{detect}}\\cdot\\left(\\frac{N_{\\text{valid}}}{N_{\\text{gt}}}\\right),\n(3)\nwhere\nN\nvalid\nN_{\\text{valid}}\ndenotes the number of matched predictions and\nN\ngt\nN_{\\text{gt}}\nthe total number of ground-truth fish.\nÎ»\ndetect\n\\lambda_{\\text{detect}}\nspecifies the maximum reward assigned to the accuracy performance, which is set to 4.0 empirically. The accuracy reward scales proportionally with the fraction of correctly matched instances.\nIn addition, we enforce consistency between the number of detected instances\nN\npred\nN_{\\text{pred}}\nand the reported count\nN\ncount\nN_{\\text{count}}\nin the\n<fish_count>\ntag by introducing a match reward:\nmatch_reward\n=\n{\n0\n,\nif\nâ€‹\nN\npred\n=\nN\ncount\n,\nâˆ’\n1\n,\notherwise\n.\n\\text{match\\_reward}=\\begin{cases}0,&\\text{if }N_{\\text{pred}}=N_{\\text{count}},\\\\\n-1,&\\text{otherwise}.\\end{cases}\n(4)\nThe overall detection-related reward is then computed as:\nR\ndetect\n=\naccuracy_reward\n+\nmatch_reward\n.\nR_{\\text{detect}}=\\text{accuracy\\_reward}+\\text{match\\_reward}.\n(5)\nThis formulation jointly optimizes detection and counting: accurate localization improves counting reliability, while consistent counting further encourages complete detection.\nCount Reward.\nTo further enforce correct enumeration, a count reward\nR\ncount\nR_{\\text{count}}\nis assigned based on whether the predicted count matches the number of ground-truth instances:\nR\ncount\n=\n{\n1\n,\nif\nâ€‹\nN\ncount\n=\nN\ngt\nâˆ’\n1\n,\notherwise\nR_{\\text{count}}=\\begin{cases}1,&\\text{if }N_{\\text{count}}=N_{\\text{gt}}\\\\\n-1,&\\text{otherwise}\\end{cases}\n(6)\nwhere\nN\ncount\nN_{\\text{count}}\nis the number of fish reported by the model and\nN\ngt\nN_{\\text{gt}}\nthe total number of ground-truth fish.\nNon-Repetition Reward.\nTo mitigate repetitive responses and promote output diversity, we adopt a non-repetition reward\nR\nnon-repeat\nR_{\\text{non-repeat}}\ninspired by Seg-Zero\n[\n20\n]\n.\nTotal Reward.\nThe total reward used for RL optimization is defined as:\nR\ntotal\n=\nw\n1\nâ‹…\nR\nformat\n+\nw\n2\nâ‹…\nR\ndetect\n+\nw\n3\nâ‹…\nR\ncount\n+\nw\n4\nâ‹…\nR\nnon-repeat\nR_{\\text{total}}=w_{1}\\cdot R_{\\text{format}}+w_{2}\\cdot R_{\\text{detect}}+w_{3}\\cdot R_{\\text{count}}+w_{4}\\cdot R_{\\text{non-repeat}}\n(7)\nwhere\nÎ±\n\\alpha\nand\nÎ²\n\\beta\ncontrol the relative weight of detection and counting rewards.\nThis formulation jointly accounts for syntactic correctness, localization accuracy, count fidelity, and output diversity, thereby providing rich supervision signals at minimal annotation cost.\nWe examine the effect of different rewardâ€“weight configurations and find that the absolute reward scale matters little, whereas the relative balance between components is critical for final performanceâ€”consistent with GRPOâ€™s reliance on group-wise relative advantages rather than absolute magnitudes. Based on this observation, we set\nw\n1\nw_{1}\n,\nw\n2\nw_{2}\n,\nw\n3\nw_{3}\n, and\nw\n4\nw_{4}\nto 1, as this configuration consistently yielded the strongest performance across our initial sweeps and offered a stable trade-off between detection and counting quality.\nTable 1\n:\nUnified comparison of detection (\nAP\n0.5\n:\n0.95\n\\text{AP}_{0.5:0.95}\n,\nAR\n0.5\n:\n0.95\n\\text{AR}_{0.5:0.95}\n), segmentation (Foreground, Background, mIoU), and counting (MAE, Match Rate, GAME) performance across baseline and proposed MLLM variants on the testset of DeepFish\nFishSeg\nand\nFishLoc\nsubset. The\nbest\nresult in each column is shown in bold, and the\nsecond best\nis underlined.\nModel\nDetection\nSegmentation\nCounting\nAP\n0.5\n:\n0.95\n\\text{AP}_{0.5:0.95}\nâ†‘\n\\uparrow\nAR\n0.5\n:\n0.95\n\\text{AR}_{0.5:0.95}\nâ†‘\n\\uparrow\nForeground\nâ†‘\n\\uparrow\nBackground\nâ†‘\n\\uparrow\nmIoU\nâ†‘\n\\uparrow\nMAE\nâ†“\n\\downarrow\nMatch Rate\nâ†‘\n\\uparrow\nGAME\nâ†“\n\\downarrow\nBaseline\nGPT-4.1\n5.47\n17.43\n43.77\n98.69\n71.23\n0.387\n0.796\n1.394\nGemini-2.0-flash\n54.60\n61.24\n84.01\n99.64\n91.83\n1.879\n0.582\n3.434\nGemini-2.5-flash\n24.10\n46.46\n57.14\n98.85\n78.00\n2.228\n0.568\n3.081\nQwen2.5-VL 3B\n44.63\n62.12\n45.10\n97.64\n71.67\n0.604\n0.616\n1.136\nQwen2.5-VL 7B\n48.05\n55.84\n81.25\n99.58\n90.42\n0.579\n0.706\n0.915\nLangSAM\n35.37\n68.41\n81.33\n99.54\n90.43\n1.347\n0.323\n1.782\nOurs\nFishDetector-Base 3B\n53.58\n62.21\n67.88\n99.10\n83.49\n0.647\n0.691\n0.901\nFishDetector-R1 3B\n61.71\n66.64\n86.47\n99.69\n93.08\n0.386\n0.760\n0.613\n\\arrayrulecolor\ngray!50FishDetector-Base 7B\n47.52\n58.67\n80.86\n99.56\n90.21\n0.497\n0.715\n0.924\nFishDetector-R1 7B\n60.71\n63.63\n87.90\n99.78\n93.84\n0.398\n0.765\n0.587\n\\arrayrulecolor\nblack\n4\nExperiments\n4.1\nEvaluation Setting\nAs discussed in\nSec.\n1\n, FishDetector-R1 is a unified framework for comprehensive marine fish analysis including detection, segmentation, and counting using weak supervision signals. We evaluate FishDetector-R1 on all these tasks following standard protocals. Specifically, for segmentation, we measure mean Intersection-over-Union (mIoU) between predicted masks and ground-truth annotations. For detection, we follow the COCO evaluation protocol\n[\n18\n]\nand report Average Precision (AP) and Average Recall (AR) across multiple IoU thresholds. Here we report the value of\nAP\n0.5\n:\n0.95\n\\text{AP}_{0.5:0.95}\nand\nAR\n0.5\n:\n0.95\n\\text{AR}_{0.5:0.95}\n, representing the mean AP and AR computed at IoU thresholds from 0.5 to 0.95.\nFurthermore, to measure counting accuracy, we employ several complementary metrics. Mean Absolute Error (MAE) quantifies counting error, while the Match Rate evaluates the consistency between the predicted and ground-truth counts. Given predicted counts\ny\n^\ni\n\\hat{y}_{i}\nand ground-truth counts\ny\ni\ny_{i}\nfor\nN\nN\nimages, the MAE and Match Rate are calculated as:\nMAE\n=\n1\nN\nâ€‹\nâˆ‘\ni\n=\n1\nN\n|\ny\n^\ni\nâˆ’\ny\ni\n|\n,\n\\text{MAE}=\\frac{1}{N}\\sum_{i=1}^{N}\\left|\\hat{y}_{i}-y_{i}\\right|,\n(8)\nMatch Rate\n=\n1\nN\nâ€‹\nâˆ‘\ni\n=\n1\nN\nğŸ™\nâ€‹\n(\ny\n^\ni\n=\ny\ni\n)\n,\n\\text{Match Rate}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{1}\\!\\left(\\hat{y}_{i}=y_{i}\\right),\n(9)\nwhere\nğŸ™\nâ€‹\n(\nâ‹…\n)\n\\mathbb{1}(\\cdot)\nis the indicator function that equals 1 if the predicted count exactly matches the ground truth and 0 otherwise.\nIn addition, we report the Grid Average Mean Absolute Error (GAME)\n[\n8\n]\nto compute counting errors at different spatial scales, where each image is divided into\n4\nL\n4^{L}\nnon-overlapping grids at level\nL\nL\n. The error is then computed over all sub-regions:\nGAME\nâ€‹\n(\nL\n)\n=\n1\nN\nâ€‹\nâˆ‘\ni\n=\n1\nN\nâˆ‘\nr\n=\n1\n4\nL\n|\ny\n^\ni\nr\nâˆ’\ny\ni\nr\n|\n,\n\\text{GAME}(L)=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{r=1}^{4^{L}}\\left|\\hat{y}_{i}^{r}-y_{i}^{r}\\right|,\n(10)\nGAME\n=\n1\n4\nâ€‹\nâˆ‘\nL\n=\n1\n4\nGAME\nâ€‹\n(\nL\n)\n,\n\\text{GAME}=\\frac{1}{4}\\sum_{L=1}^{4}\\text{GAME}(L),\n(11)\nwhere\ny\n^\ni\nr\n\\hat{y}_{i}^{r}\nand\ny\ni\nr\ny_{i}^{r}\ndenote the predicted keypoints and ground-truth point labels in the\nr\nr\n-th region of the\ni\ni\n-th image. Lower values of GAME indicate better spatial consistency in counting predictions, reducing cases where totals are correct but fish are mislocalized.\nWe conduct experiments using the open-source DeepFish benchmark\n[\n25\n]\n. We evaluate grounding (i.e., detection and segmentation) performance on the test split of the DeepFish\nFishSeg\nsubset. For counting, we use the test split of the DeepFish\nFishLoc\nsubset. We also test FishDetector-R1 on the SUIM dataset\n[\n13\n]\nto demonstrate the generalization capability of FishDetector-R1 across different environments and fish species in\nSec.\n4.4\n.\n4.2\nImplementation Details\nWe build our framework based on the 3B and 7B variants of Qwen2.5-VL\n[\n3\n]\n. For FishDetector-Base, we use the official pretrained Qwen2.5-VL weight with our detect-to-count prompt design. For FishDetector-R1, we apply GRPO fine-tuning on the training split of the DeepFish localization subset\n[\n25\n]\n, which contains 1,600 images with point-level fish annotations. Full parameter fine-tuning on the Qwen2.5-VL is performed on 4\nÃ—\n\\times\nNVIDIA A100 GPUs, with a batch size of 16 per device and 8 rollouts per input. Training is conducted only on the MLLM for 4 epochs with a learning rate of\n1\nÃ—\n10\nâˆ’\n6\n1\\times 10^{-6}\n. The SAM 2 is frozen and only used for segmentation evaluation. We include more implementation details in the supplementary material.\nFigure 4\n:\nQualitative Comparison between Qwen2.5-VL and FishDetector-R1.\nOn a challenging scene from DeepFish\nFishLoc\n, our detect-to-count strategy enables more accurate localization and structured outputs.\n4.3\nExperimental Results\n4.3.1\nOverall Comparison\nBaselines.\nWe benchmark our framework against state-of-the-art MLLMs, including GPT-4.1\n[\n24\n]\n, Gemini-2.0/2.5\n[\n34\n,\n6\n]\n, and Qwen2.5-VL\n[\n3\n]\n, across detection, segmentation, and counting. Specifically, we prompt the MLLMs to generate coordinates of bounding boxes, which are then passed to SAM 2\n[\n27\n]\nto further generate segmentation masks. The MLLMs are also instructed to output the total number of fish, serving as their predictions for the counting task. The prompts are designed to be compact and general. We use the same prompts for these MLLMs to ensure fair comparison. Detailed prompts can be found in the supplementary material.\nAdditionally, we compare it against LangSAM\n[\n23\n]\n, a representative open-source model capable of open-set detection and segmentation by leveraging SAM 2\n[\n27\n]\nand Grounding DINO\n[\n19\n]\n.\nQuantitative Results.\nThe overall comparison is shown in\nSec.\n3.4\n. Our model achieves consistent and substantial improvements over all baselines across the three tasks. Notably, FishDetector-Base, which applies the detect-to-count prompt without any gradient-based fine-tuning, already yields clear gains over Qwen2.5-VL, confirming the effectiveness of our prompt design. Building on this, FishDetector-R1 further surpasses FishDetector-Base in all tasks, demonstrating the benefits of our proposed reward formulation and RLVR pipeline. We also observe that the 3B variant of FishDetector-Base gains more from the detect-to-count prompting than the 7B variant, suggesting that larger models benefit less from prompt-only adaptation and thus require additional alignment (e.g., RL fine-tuning) to fully realize performance gains. While LangSAM achieves the highest AR and the fourth highest mIoU, the poor detection precision and counting performance highlight the significant limitation of misdetection. The model struggles to distinguish fish-like objects in the background, undermining its reliability.\nQualitative Comparison.\nFigure\n4\npresents a comparison with the original Qwen2.5-VL in a challenging crowded scene, illustrating that FishDetector-R1 maintains stronger performance even under difficult conditions. We present more qualitative results in the supplementary material.\nTable 2\n:\nComparison of segmentation accuracy (mIoU) on DeepFish\nFishSeg\ndataset across different supervision methods. The\nbest\nresult in each column is shown in bold, and the\nsecond best\nis underlined.\nMethod\nSupervision Type\nmIoU\nDeepFish\n[\n28\n]\nDense Annotations\n93.0\nA-LCFCN\n[\n15\n]\nPoint Labels\n86.2\nFishDetector-R1 3B\nPoint Labels\n93.1\nFishDetector-R1 7B\nPoint Labels\n93.8\n4.3.2\nComparison with Traditional Fully & Weakly Supervised Methods\nTo complete our evaluation, we compare FishDetector-R1 with both fully and weakly supervised baselines. For the fully supervised setting, we report the DeepFish benchmark\n[\n28\n]\n, which trains a segmentation model using dense pixel-level masks with a ResNet-50 backbone\n[\n10\n]\n. For the weakly supervised setting, we include A-LCFCN\n[\n15\n]\n, a state-of-the-art point-supervised method trained on the same DeepFish\nFishLoc\nsubset.\nAs shown in\nTab.\n2\n, there is a clear performance gap between weak and dense supervision (86.2 vs. 93.0 mIoU). Remarkably, FishDetector-R1 closes this gap entirely. With only sparse point annotations, the 3B variant matches the fully supervised baseline, while the 7B variant surpasses it. This demonstrates that our approach effectively combines weak supervision with the spatial reasoning and semantic priors inherent in foundation models. By leveraging multimodal prompting and reinforcement alignment, our framework not only exceeds traditional weakly supervised methods but also rivals dense annotation-based models, offering a scalable and annotation-efficient solution for underwater segmentation.\nTable 3\n:\nComparison of segmentation accuracy (mIoU) on the\nFish (and Vertebrates)\n(FV) category of SUIM dataset\n[\n13\n]\n. The\nbest\nresult in each column is shown in bold, and the\nsecond best\nis underlined.\nMethod\nForeground\nBackground\nmIoU\nLangSAM\n66.20\n96.51\n81.35\nQwen2.5-VL 3B\n30.74\n91.96\n61.35\nQwen2.5-VL 7B\n30.33\n93.05\n61.69\nFishDetector-Base 3B\n37.85\n95.46\n66.66\nFishDetector-Base 7B\n46.76\n94.86\n70.81\nFishDetector-R1 3B\n70.18\n97.66\n83.92\nFishDetector-R1 7B\n69.67\n97.72\n83.70\nTable 4\n:\nAblation study of FishDetector-R1 (7B) with different reward configurations. Detection (\nAP\n0.5\n:\n0.95\n\\text{AP}_{0.5:0.95}\n,\nAR\n0.5\n:\n0.95\n\\text{AR}_{0.5:0.95}\n), segmentation (Foreground, Background, mIoU), and counting (MAE, Match Rate, GAME) metrics are reported. The\nbest\nresult in each column is shown in bold, and the\nsecond best\nis underlined.\nReward Setting\nDetection\nSegmentation\nCounting\nAP\n0.5\n:\n0.95\n\\text{AP}_{0.5:0.95}\nAR\n0.5\n:\n0.95\n\\text{AR}_{0.5:0.95}\nForeground\nBackground\nmIoU\nMAE\nMatch Rate\nGAME\nBase\n47.52\n58.67\n80.86\n99.56\n90.21\n0.497\n0.715\n0.924\n+\nR\ncount\nR_{\\text{count}}\n26.46\n39.91\n51.71\n98.59\n87.82\n0.414\n0.770\n1.346\n+\nR\ndetect\nR_{\\text{detect}}\n57.10\n63.00\n87.10\n99.68\n93.41\n0.442\n0.757\n0.693\n+\nR\ncount\nR_{\\text{count}}\n+\nR\ndetect\nR_{\\text{detect}}\n60.71\n63.63\n87.94\n99.78\n93.84\n0.398\n0.765\n0.587\n4.4\nGeneralization Experiment\nIn addition to experiments in\nSec.\n4.3\n, we further evaluate FishDetector-R1 on an additional underwater fish dataset (i.e., SUIM\n[\n13\n]\n) to assess the generalizability to new environments and new fish species. Specifically, we run zero-shot inference using FishDetector-Base and FishDetector-R1 on the SUIM dataset and report segmentation performance in\nTab.\n3\n. As shown in\nTab.\n3\n, FishDetector-R1 substantially outperforms both the base models and prior multimodal baselines. The performance gain is consistent across both 3B and 7B variants, highlighting the robustness of our reinforcement alignment and detect-to-count prompt design. These results indicate that the improvements observed on DeepFish generalize well to new underwater environments, confirming the scalability and adaptability of our framework beyond the training distribution.\n4.5\nAblation Study\nWe further conduct ablation studies to examine the role of each proposed reward component. In this section, we report results of the 7B model, with additional ablations provided in the supplementary material.\nReward Design.\nAs shown in\nTab.\n4\n, using only the count reward\nR\ncount\nR_{\\text{count}}\nimproves overall count accuracy (lower MAE and higher Match Rate), but detection and segmentation scores drop noticeably. This indicates that the model learns to match the total number of objects but does not reliably localize them in the image.\nUsing only the detection reward\nR\ndetect\nR_{\\text{detect}}\nleads to the opposite trend. Detection and segmentation performance improve (higher AP/AR and mIoU), and spatial counting consistency also improves (lower GAME). However, the total count is still not as accurate as in the\nR\ncount\nR_{\\text{count}}\nsetting.\nWhen both rewards are used together, the model achieves the best overall results. Detection and segmentation reach their highest values, and counting is both more accurate (lowest MAE) and more spatially consistent (lowest GAME). This shows that these two rewards are complementary:\nR\ncount\nR_{\\text{count}}\nprovides high-level supervision on object cardinality, while\nR\ndetect\nR_{\\text{detect}}\noffers fine-grained guidance for precise localization. Combining them leads to stronger performance across detection, segmentation, and counting.\nTable 5\n:\nAlignment rate between detected instances and predicted fish counts from model output response. Higher is better.\nModel\nAlignment Rate (%)\nâ†‘\n\\uparrow\n3B\n7B\nFishDetector-Base\n97.2\n98.6\nFishDetector-R1\n99.6\n100\nInternal Detect-to-Count Consistency\nWe evaluate how consistently the modelâ€™s predicted fish count matches the number of detected instances. As shown in\nTab.\n5\n, FishDetector-Base already produces high agreement, but small mismatches remain. After RL fine-tuning, FishDetector-R1 achieves near-perfect alignment for both 3B and 7B variants. This indicates that the reward-based training not only improves accuracy, but also strengthens the internal link between detection and counting, leading to more stable and reliable outputs for downstream ecological analysis.\n5\nLimitations and Future Work\nWhile FishDetector-R1 achieves notable gains, several limitations remain. First, the computational overhead of large MLLMs makes real-time deployment on resource-constrained edge devices challenging. Future work will explore quantization and edge-optimization to improve efficiency. Second, the framework can still hallucinate spurious detections or counts in cluttered scenes, highlighting the need for uncertainty modeling or verification mechanisms to enhance reliability.\nThird, the current framework operates on single-frame inputs, limiting its ability to leverage temporal cues for tracking or resolving occlusions that are common in underwater environments. Future extensions will incorporate temporal modeling to improve robustness in continuous or video-based monitoring.\nFinally, although FishDetector-R1 is class-agnostic by design, in this study we restrict supervision and evaluation to\nfish-only\ndetection and segmentation, which covers a relatively narrow range of habitats and species. Extending to multi-class and species-level settings (including non-fish marine life and anthropogenic objects) is an important step toward broader ecological impact.\n6\nConclusion\nWe propose\nFishDetector-R1\n, a unified framework for underwater fish detection, segmentation, and counting that combines an MLLM with SAM 2. Through detect-to-count prompting and reinforcement fine-tuning with sparse point labels, our method achieves strong performance across tasks on the DeepFish dataset. Notably, FishDetector-R1 bridges the gap between traditional weak and fully supervised models, delivering high-quality pixel-wise segmentation with minimal annotation cost. The improvement further generalizes to other underwater datasets, demonstrating robust cross-domain scalability. This enables scalable, annotation-efficient fish analysis, supporting real-world applications in ecological monitoring and marine habitat assessment.\nReferences\nAchiam etÂ al. [2023]\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, FlorenciaÂ Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, etÂ al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774\n, 2023.\nAlÂ Muksit etÂ al. [2022]\nAbdullah AlÂ Muksit, Fakhrul Hasan, MdÂ Fahad HasanÂ Bhuiyan Emon, MdÂ Rakibul Haque, ArifÂ Reza Anwary, and Swakkhar Shatabda.\nYolo-fish: A robust fish detection model to detect fish in realistic underwater environment.\nEcological Informatics\n, 72:101847, 2022.\nBai etÂ al. [2025]\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, etÂ al.\nQwen2. 5-vl technical report.\narXiv preprint arXiv:2502.13923\n, 2025.\nBearman etÂ al. [2016]\nAmy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei.\nWhatâ€™s the point: Semantic segmentation with point supervision.\nIn\nEuropean conference on computer vision\n, pages 549â€“565. Springer, 2016.\nCai and Vasconcelos [2018]\nZhaowei Cai and Nuno Vasconcelos.\nCascade r-cnn: Delving into high quality object detection.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n, pages 6154â€“6162, 2018.\nComanici etÂ al. [2025]\nGheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, etÂ al.\nGemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.\narXiv preprint arXiv:2507.06261\n, 2025.\nGarcia etÂ al. [2020]\nRafael Garcia, Ricard Prados, Josep Quintana, Alexander Tempelaar, Nuno Gracias, Shale Rosen, HÃ¥vard VÃ¥gstÃ¸l, and Kristoffer LÃ¸vall.\nAutomatic segmentation of fish using deep learning with application to fish size measurement.\nICES Journal of Marine Science\n, 77(4):1354â€“1366, 2020.\nGuerrero-GÃ³mez-Olmedo etÂ al. [2015]\nRicardo Guerrero-GÃ³mez-Olmedo, Beatriz Torre-JimÃ©nez, Roberto LÃ³pez-Sastre, Saturnino Maldonado-BascÃ³n, and Daniel Onoro-Rubio.\nExtremely overlapping vehicle counting.\nIn\nIberian conference on pattern recognition and image analysis\n, pages 423â€“431. Springer, 2015.\nGuo etÂ al. [2025]\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, etÂ al.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\narXiv preprint arXiv:2501.12948\n, 2025.\nHe etÂ al. [2016]\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n, pages 770â€“778, 2016.\nHong etÂ al. [2024]\nYang Hong, Xiaowei Zhou, Ruzhuang Hua, Qingxuan Lv, and Junyu Dong.\nWatersam: Adapting sam for underwater object segmentation.\nJournal of Marine Science and Engineering\n, 12(9):1616, 2024.\nHurst etÂ al. [2024]\nAaron Hurst, Adam Lerer, AdamÂ P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, etÂ al.\nGpt-4o system card.\narXiv preprint arXiv:2410.21276\n, 2024.\nIslam etÂ al. [2020]\nMdÂ Jahidul Islam, Chelsey Edge, Yuyang Xiao, Peigen Luo, Muntaqim Mehtaz, Christopher Morse, SadmanÂ Sakib Enan, and Junaed Sattar.\nSemantic segmentation of underwater imagery: Dataset and benchmark.\nIn\n2020 IEEE/RSJ international conference on intelligent robots and systems (IROS)\n, pages 1769â€“1776. IEEE, 2020.\nKirillov etÂ al. [2023]\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, AlexanderÂ C Berg, Wan-Yen Lo, etÂ al.\nSegment anything.\nIn\nICCV\n, pages 4015â€“4026, 2023.\nLaradji etÂ al. [2020]\nIssam Laradji, Alzayat Saleh, Pau Rodriguez, Derek Nowrouzezahrai, MostafaÂ Rahimi Azghadi, and David Vazquez.\nAffinity lcfcn: Learning to segment fish with weak supervision.\narXiv preprint arXiv:2011.03149\n, 2020.\nLaradji etÂ al. [2018]\nIssamÂ H Laradji, Negar Rostamzadeh, PedroÂ O Pinheiro, David Vazquez, and Mark Schmidt.\nWhere are the blobs: Counting by localization with point supervision.\nIn\nProceedings of the european conference on computer vision (ECCV)\n, pages 547â€“562, 2018.\nLian etÂ al. [2023]\nShijie Lian, Hua Li, Runmin Cong, Suqi Li, Wei Zhang, and Sam Kwong.\nWatermask: Instance segmentation for underwater imagery.\nIn\nICCV\n, pages 1305â€“1315, 2023.\nLin etÂ al. [2014]\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and CÂ Lawrence Zitnick.\nMicrosoft coco: Common objects in context.\nIn\nEuropean conference on computer vision\n, pages 740â€“755. Springer, 2014.\nLiu etÂ al. [2024]\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, etÂ al.\nGrounding dino: Marrying dino with grounded pre-training for open-set object detection.\nIn\nEuropean conference on computer vision\n, pages 38â€“55. Springer, 2024.\nLiu etÂ al. [2025a]\nYuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia.\nSeg-zero: Reasoning-chain guided segmentation via cognitive reinforcement.\narXiv preprint arXiv:2503.06520\n, 2025a.\nLiu etÂ al. [2025b]\nYuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, and Jiaya Jia.\nVisionreasoner: Unified visual perception and reasoning via reinforcement learning.\narXiv preprint arXiv:2505.12081\n, 2025b.\nLiu etÂ al. [2025c]\nZiyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang.\nVisual-rft: Visual reinforcement fine-tuning.\narXiv preprint arXiv:2503.01785\n, 2025c.\nMedeiros [2025]\nLuca Medeiros.\nlang-segment-anything: Sam with text prompt.\nhttps://github.com/luca-medeiros/lang-segment-anything\n, 2025.\nGitHub repository, accessed 2025-11.\nOpenAI [2025]\nOpenAI.\nIntroducing GPT-4.1 in the api.\nhttps://openai.com/index/gpt-4-1/\n, 2025.\nAccessed: 2025-09-16.\nQin etÂ al. [2016]\nHongwei Qin, Xiu Li, Jian Liang, Yigang Peng, and Changshui Zhang.\nDeepfish: Accurate underwater live fish recognition with a deep architecture.\nNeurocomputing\n, 187:49â€“58, 2016.\nRafailov etÂ al. [2023]\nRafael Rafailov, Archit Sharma, Eric Mitchell, ChristopherÂ D Manning, Stefano Ermon, and Chelsea Finn.\nDirect preference optimization: Your language model is secretly a reward model.\nAdvances in neural information processing systems\n, 36:53728â€“53741, 2023.\nRavi etÂ al. [2024]\nNikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, etÂ al.\nSam 2: Segment anything in images and videos.\narXiv preprint arXiv:2408.00714\n, 2024.\nSaleh etÂ al. [2020]\nAlzayat Saleh, IssamÂ H Laradji, DmitryÂ A Konovalov, Michael Bradley, David Vazquez, and Marcus Sheaves.\nA realistic fish-habitat dataset to evaluate algorithms for underwater visual analysis.\nScientific reports\n, 10(1):14671, 2020.\nSchulman etÂ al. [2017]\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\nProximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347\n, 2017.\nShao etÂ al. [2022]\nFeifei Shao, Long Chen, Jian Shao, Wei Ji, Shaoning Xiao, Lu Ye, Yueting Zhuang, and Jun Xiao.\nDeep learning for weakly-supervised object detection and localization: A survey.\nNeurocomputing\n, 496:192â€“207, 2022.\nSong etÂ al. [2024]\nJingyu Song, Onur Bagoren, Razan Andigani, Advaith Sethuraman, and KatherineÂ A Skinner.\nTurtlmap: Real-time localization and dense mapping of low-texture underwater environments with a low-cost unmanned underwater vehicle.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, pages 1191â€“1198. IEEE, 2024.\nSong etÂ al. [2025]\nJingyu Song, Haoyu Ma, Onur Bagoren, AdvaithÂ V. Sethuraman, Yiting Zhang, and KatherineÂ A. Skinner.\nOceansim: A gpu-accelerated underwater robot perception simulation framework.\nIn\n2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n. IEEE, 2025.\nStankus [2021]\nAustin Stankus.\nState of world aquaculture 2020 and regional reviews: Fao webinar series.\nFAO aquaculture newsletter\n, (63):17â€“18, 2021.\nTeam etÂ al. [2023]\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, AndrewÂ M Dai, Anja Hauth, Katie Millican, etÂ al.\nGemini: a family of highly capable multimodal models.\narXiv preprint arXiv:2312.11805\n, 2023.\nTouvron etÂ al. [2023]\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, etÂ al.\nLlama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971\n, 2023.\nXu etÂ al. [2023]\nMuduo Xu, Jianhao Su, and Yutao Liu.\nAquasam: Underwater image foreground segmentation.\nIn\nInternational Forum on Digital TV and Wireless Multimedia Communications\n, pages 3â€“14, 2023.\nYou and Wu [2025]\nZuyao You and Zuxuan Wu.\nSeg-r1: Segmentation can be surprisingly simple with reinforcement learning.\narXiv preprint arXiv:2506.22624\n, 2025.\nYu etÂ al. [2025]\nEn Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, etÂ al.\nPerception-r1: Pioneering perception policy with reinforcement learning.\narXiv preprint arXiv:2504.07954\n, 2025.\nZhang etÂ al. [2022]\nWenbo Zhang, Chaoyi Wu, and Zhenshan Bao.\nDpanet: dual pooling-aggregated attention network for fish segmentation.\nIET computer vision\n, 16(1):67â€“82, 2022.\nZhang etÂ al. [2025]\nZhihao Zhang, Qiaole Dong, Qi Zhang, Jun Zhao, Enyu Zhou, Zhiheng Xi, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, etÂ al.\nReinforcement fine-tuning enables mllms learning novel tasks stably.\narXiv preprint arXiv:2506.23508\n, 2025.\nZheng etÂ al. [2025]\nYaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong.\nEasyr1: An efficient, scalable, multi-modality rl training framework.\nhttps://github.com/hiyouga/EasyR1\n, 2025.\n\\thetitle\nSupplementary Material\nIn this material, we provide additional results and analyses to further substantiate the effectiveness and practicality of FishDetector-R1. We first describe additional implementation details, including the RL training setup, prompt design, and open-source configuration used in our experiments. We then present a series of supplementary ablations: (i) reward configuration studies on the 3B base model, (ii) sensitivity analysis over reward weight choices, and (iii) the impact of different SAMÂ 2 backbone sizes. Next, we include extended qualitative results on DeepFish\n[\n28\n]\nfor both segmentation and point-level localization, as well as zero-shot transfer visualizations on SUIM\n[\n13\n]\n, highlighting robustness across habitats and domains. Finally, we discuss potential negative societal impacts of our framework in large-scale ecological and fisheries applications.\n7\nAdditional Implementation Details\nWe provide additional implementation details to supplement the main paper. We implement and train FishDetector-R1 based on Easy-R1\n[\n41\n]\n, an open-source RL training framework. For all the instances of FishDetector-R1, we set the temperature value as the default value\n1.0\n1.0\n.\nFurthermore,\nTab.\n6\nsummarizes the prompts used during evaluation for both the MLLM baseline\n[\n3\n]\nand our FishDetector variants. For Qwen2.5-VL, we follow the official documentation: the grounding task uses a detection-style prompt to generate bounding boxes that are then passed to SAMÂ 2\n[\n27\n]\nfor segmentation, while the counting task adopts a point-based strategy that outputs both the total number of fish and their 2D keypoint locations for evaluating localization accuracy. In contrast, our\ndetect-to-count\nprompt in FishDetector-Base/R1 unifies grounding and counting within a single instruction, aiming to enforce consistency between localized detections and global count predictions.\nFor other MLLM baselines (e.g., GPT-4.1\n[\n24\n]\n, Gemini-2.0/2.5\n[\n34\n,\n6\n]\n), we found that their outputs were somewhat sensitive to the exact same prompt. To ensure a fair comparison and allow each baseline to perform optimally, we applied minor prompt adjustments tailored to each baseline. All baseline prompts will be released with our open-source package. In addition, we will release FishDetector-R1 as fully open-source to benefit the broader community.\nTable 6\n:\nPrompts used for MLLM baseline\n[\n3\n]\nand our FishDetector.\nBaseline separates grounding and counting, while our detect-to-count prompt unifies them for consistency.\nMethod\nTask\nPrompt\nExample\nQwen2.5-VL\nGrounding\nDetect all fish in the image and return their locations in the form of bounding box coordinates.\n[{\"bbox_2d\": [x1, y1, x2, y2], \"label\": \"fish\"}]\nCounting\nCount the number of fish in the image, including those that are only partially visible.\nFirst detect their keypoints, then output the total count in\n<fish_count></fish_count>\n.\n[{\"point_2d\": [x, y], \"label\": \"fish\"}], <fish_count>1</fish_count>\nFishDetector-\nBase / R1\nGrounding\nand\nCounting\nGiven an underwater image, identify all fish. Step 1: Draw a bounding box tightly around each fish.\nStep 2: Mark the keypoint at the fish center. Step 3: Return the number of fish consistent with detections.\n<think>...</think>\n<detection>[{\"bbox_2d\": [x1,y1,x2,y2], \"point_2d\": [x,y], \"label\": \"fish\"}]</detection>\n<fish_count>1</fish_count>\nTable 7\n:\nAblation study of FishDetector-R1 (3B) with different reward configurations. Detection (\nAP\n0.5\n:\n0.95\n\\text{AP}_{0.5:0.95}\n,\nAR\n0.5\n:\n0.95\n\\text{AR}_{0.5:0.95}\n), segmentation (Foreground, Background, mIoU), and counting (MAE, Match Rate, GAME) metrics are reported. The\nbest\nresult in each column is shown in bold, and the\nsecond best\nis underlined.\nReward Setting\nDetection\nSegmentation\nCounting\nAP\n0.5\n:\n0.95\n\\text{AP}_{0.5:0.95}\nâ†‘\n\\uparrow\nAR\n0.5\n:\n0.95\n\\text{AR}_{0.5:0.95}\nâ†‘\n\\uparrow\nForeground\nâ†‘\n\\uparrow\nBackground\nâ†‘\n\\uparrow\nmIoU\nâ†‘\n\\uparrow\nMAE\nâ†“\n\\downarrow\nMatch Rate\nâ†‘\n\\uparrow\nGAME\nâ†“\n\\downarrow\nBase\n53.6\n62.2\n67.9\n99.1\n83.5\n0.647\n0.691\n0.901\n+\nR\ncount\nR_{\\text{count}}\n5.0\n18.3\n24.4\n97.6\n61.0\n0.415\n0.768\n1.646\n+\nR\ndetect\nR_{\\text{detect}}\n59.8\n65.2\n86.7\n99.7\n93.2\n0.442\n0.757\n0.693\n+\nR\ncount\nR_{\\text{count}}\n+\nR\ndetect\nR_{\\text{detect}}\n61.7\n66.6\n86.5\n99.7\n93.1\n0.398\n0.765\n0.587\n8\nSupplementary Experiments\n8.1\nAblation on Reward Configuration with 3B Base Model\nTo further validate the robustness of our approach, we report ablation experiments on the 3B model, demonstrating that FishDetector-R1 yields consistent improvements independent of the underlying model variant. As shown in\nTab.\n7\n, reward design plays a decisive role in shaping overall performance. While adding the counting reward alone (\nR\ncount\nR_{\\text{count}}\n) improves overall count accuracy with less MAE and higher Match Rate, it substantially degrades both detection and segmentation accuracy and leads to worst localization precision, indicating that this signal is insufficient on its own to guide precise grounding. In contrast, introducing the detection reward (\nR\ndetect\nR_{\\text{detect}}\n) produces substantial gains across all metrics, increasing AP from 53.6 to 59.8 and mIoU from 83.5 to 93.2, underscoring the importance of explicit localization feedback. Combining\nR\ndetect\nR_{\\text{detect}}\nand\nR\ncount\nR_{\\text{count}}\nyields the strongest detection performance and competitive segmentation quality, and achieves more accurate (lowest MAE) and spatial consistent counting results(lowest GAME), suggesting that the two rewards offer complementary benefitsâ€”where\nR\ndetect\nR_{\\text{detect}}\nprovides the primary grounding signal and\nR\ncount\nR_{\\text{count}}\ncontributes additional structural guidance. These findings confirm the effectiveness of our reward formulation and the stability of FishDetector-R1 across configurations.\n8.2\nAblation on Reward Weight Configuration\nTo understand how different reward components contribute to RL fine-tuning, we analyze the effect of varying the reward weights in the total objective. As described in the main paper, the reward function integrates four complementary signalsâ€”format correctness, detection accuracy, counting consistency, and non-repetitionâ€”to encourage the model to produce structured, accurate, and non-redundant outputs under weak point-level supervision. The overall formulation is:\nR\ntotal\n=\nw\n1\nâ‹…\nR\nformat\n+\nw\n2\nâ‹…\nR\ndetect\n+\nw\n3\nâ‹…\nR\ncount\n+\nw\n4\nâ‹…\nR\nnon-repeat\n.\nR_{\\text{total}}=w_{1}\\cdot R_{\\text{format}}+w_{2}\\cdot R_{\\text{detect}}+w_{3}\\cdot R_{\\text{count}}+w_{4}\\cdot R_{\\text{non-repeat}}.\n(12)\nIn this ablation, we focus on the weights directly related to task performance,\nw\n2\nw_{2}\nand\nw\n3\nw_{3}\n, while keeping\nw\n1\nw_{1}\nand\nw\n4\nw_{4}\nfixed because they regulate structural formatting and repetition behavior that are irrelevant to our detection or counting tasks. Full definitions and implementation details for each reward component are provided in the main paper; here, we present an extended analysis of how the choice of\nw\n2\nw_{2}\nand\nw\n3\nw_{3}\ninfluences detection and segmentation performance based on 3B model. As shown in\nSec.\n8.2\n, increasing the relative weight\nw\n2\nw_{2}\nenhance segmentation quality but leads to a decline in counting accuracy and localization robustness. In contrast, emphasizing\nw\n3\nw_{3}\nyields gains on the overall counting accuracy but this comes at the cost of compromising detection precision, localization precision, and segmentation performance. The proposed configuration represents an optimal compromise, offering the strongest localization capability and highly competitive detection performance, with minimal performance loss across other metrics.\nTable 8\n:\nAblation study of FishDetector-R1 (3B) with different different weight configuration (\nw\n2\nw_{2}\n:\nw\n3\nw_{3}\n). Detection (\nAP\n0.5\n:\n0.95\n\\text{AP}_{0.5:0.95}\n,\nAR\n0.5\n:\n0.95\n\\text{AR}_{0.5:0.95}\n), segmentation (Foreground, Background, mIoU), and counting (MAE, Match Rate, GAME) metrics are reported.\nWeight Configuration\nDetection\nSegmentation\nCounting\nw\n2\n:\nw\n3\nw_{2}:w_{3}\nAP\n0.5\n:\n0.95\n\\text{AP}_{0.5:0.95}\nâ†‘\n\\uparrow\nAR\n0.5\n:\n0.95\n\\text{AR}_{0.5:0.95}\nâ†‘\n\\uparrow\nForeground\nâ†‘\n\\uparrow\nBackground\nâ†‘\n\\uparrow\nmIoU\nâ†‘\n\\uparrow\nMAE\nâ†“\n\\downarrow\nMatch Rate\nâ†‘\n\\uparrow\nGAME\nâ†“\n\\downarrow\nFishDetector-R1 3B\n\\arrayrulecolor\ngray!504 : 1\n62.8\n65.3\n87.0\n99.7\n93.4\n0.534\n0.747\n0.717\n2 : 1\n59.7\n66.1\n87.4\n99.7\n93.6\n0.395\n0.774\n0.626\n1 : 1 (Ours)\n61.7\n66.6\n86.5\n99.7\n93.1\n0.398\n0.765\n0.587\n1 : 2\n53.6\n62.2\n86.4\n99.7\n93.0\n0.375\n0.774\n0.650\n1 : 4\n56.9\n63.9\n85.5\n99.6\n92.6\n0.373\n0.776\n0.631\n\\arrayrulecolor\nblack\n8.3\nEffects on Different Sizes of SAM 2 Model\nTo understand how the choice of segmentation backbone influences overall performance, we evaluate FishDetector-R1 using four different sizes of the SAMÂ 2 model, as summarized in\nLABEL:tab:sam2size\n. The results show that SAMÂ 2-Large is consistently the strongest choice across both the 3B and 7B variants of FishDetector-R1, achieving the highest foreground accuracy, background accuracy, and overall mIoU in every setting. This validates our decision to use SAMÂ 2-Large as the default mask generation model in the main experiments. At the same time, the performance degradation when moving to smaller SAMÂ 2 models is modestâ€”particularly for the 7B variantâ€”indicating that lighter backbones still deliver satisfactory segmentation quality. This suggests that FishDetector-R1 can flexibly support resource-constrained or edge deployments while retaining strong performance.\nTable 9\n:\nComparison of segmentation quality with different size of SAM 2 model.\nSAM 2 Model Size\nForeground\nBackground\nmIoU\nFishDetector-R1 3B\n\\arrayrulecolor\ngray!50SAM 2-Large\n86.47\n99.69\n93.08\nSAM 2-Base\n84.39\n99.64\n92.02\nSAM 2-Small\n84.52\n99.65\n92.09\nSAM 2-Tiny\n83.03\n99.61\n91.32\nFishDetector-R1 7B\nSAM 2-Large\n87.90\n99.78\n93.84\nSAM 2-Base\n86.95\n99.71\n93.33\nSAM 2-Small\n86.26\n99.70\n92.98\nSAM 2-Tiny\n86.39\n99.70\n93.05",
    "preview_text": "Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.\n\nFishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting\nYi Liu\n*\n, Jingyu Song\n*\n, Vedanth Kallakuri, Katherine A. Skinner\nUniversity of Michigan, Ann Arbor, MI USA\nAbstract\nAnalyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce\nFishDetector-R1\n, a unified MLLM-based framework for fish detection, segmentation, and counting under\nweak supervision\n. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel\ndetect-to-count\nprompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Re",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "unified MLLM-based framework",
        "weak supervision",
        "detection",
        "segmentation",
        "counting",
        "reinforcement learning"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºå¼±ç›‘ç£ä¸‹çš„é±¼ç±»æ£€æµ‹ã€åˆ†å‰²å’Œè®¡æ•°ï¼Œä¸æ¶‰åŠè§†é¢‘æ‰©æ•£æˆ–å¤šæ¨¡æ€ç”Ÿæˆç­‰å…³é”®è¯ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T06:23:56Z",
    "created_at": "2026-01-10T10:40:28.366385",
    "updated_at": "2026-01-10T10:40:28.366395",
    "flag": true
}