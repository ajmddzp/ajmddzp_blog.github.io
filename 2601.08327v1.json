{
    "id": "2601.08327v1",
    "title": "Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition",
    "authors": [
        "Gabriele Calzolari",
        "Vidya Sumathy",
        "Christoforos Kanellakis",
        "George Nikolakopoulos"
    ],
    "abstract": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å»ä¸­å¿ƒåŒ–çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿ç»“æ„å¼‚æ„çš„æ™ºèƒ½ä½“å›¢é˜Ÿèƒ½å¤Ÿåœ¨å…·æœ‰éƒ¨åˆ†å¯è§‚æµ‹æ€§ã€é€šä¿¡çº¦æŸå’ŒåŠ¨æ€äº¤äº’ç‰¹æ€§çš„ç¯å¢ƒä¸­ååŒå‘ç°å¹¶è·å–éšæœºåˆ†å¸ƒçš„ç›®æ ‡ã€‚æ¯ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥å‡é€šè¿‡å¤šæ™ºèƒ½ä½“è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å°†æ¨¡æ‹Ÿçš„è·ç¦»ä¼ æ„Ÿæ•°æ®ä¸ç›¸é‚»æ™ºèƒ½ä½“é—´äº¤æ¢çš„é€šä¿¡åµŒå…¥å‘é‡ç›¸ç»“åˆï¼Œä»è€Œæ”¯æŒåŸºäºå±€éƒ¨æ„ŸçŸ¥å’Œå…³ç³»ä¿¡æ¯çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å†³ç­–ã€‚ç‰¹åˆ«åœ°ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡å®‰å…¨æ»¤æ³¢å™¨æ•´åˆäº†åŸºäºå›¾çš„é€šä¿¡æœºåˆ¶å’Œè½¨è¿¹æ„ŸçŸ¥å®‰å…¨ç­–ç•¥ã€‚è¯¥æ¶æ„é‡‡ç”¨ç»“æ„åŒ–å¥–åŠ±è®¾è®¡ï¼Œæ—¨åœ¨ä¿ƒè¿›æœ‰æ•ˆçš„ç›®æ ‡å‘ç°ä¸è·å–ã€ç¢°æ’é¿å…ä»¥åŠé€šè¿‡æ¨åŠ¨ä¿¡æ¯æ­£äº¤åŒ–å®ç°æ™ºèƒ½ä½“é€šä¿¡å‘é‡é—´çš„è§£è€¦ã€‚é€šè¿‡ç³»ç»Ÿçš„æ¶ˆèå®éªŒéªŒè¯äº†æ‰€æå‡ºå¥–åŠ±å‡½æ•°çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œä»¿çœŸç»“æœè¡¨æ˜ä»»åŠ¡èƒ½å¤Ÿå®‰å…¨ç¨³å®šåœ°æ‰§è¡Œï¼Œè¿›ä¸€æ­¥è¯å®äº†è¯¥æ¡†æ¶çš„ä¼˜è¶Šæ€§èƒ½ã€‚",
    "url": "https://arxiv.org/abs/2601.08327v1",
    "html_url": "https://arxiv.org/html/2601.08327v1",
    "html_content": "Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition\nGabriele Calzolari\nVidya Sumathy\nChristoforos Kanellakis\nGeorge Nikolakopoulos\nDepartment of Computer Science, Electrical and Space Engineering, LuleÃ¥ University of Technology, LuleÃ¥, Sweden.\nAbstract\nThis paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments characterized by partial observability, communication constraints, and dynamic interactions. Each agentâ€™s policy is trained with the Multi-Agent Proximal Policy Optimization algorithm and employs a Graph Attention Network encoder that integrates simulated range-sensing data with communication embeddings exchanged among neighboring agents, enabling context-aware decision-making from both local sensing and relational information. In particular, this work introduces a unified framework that integrates graph-based communication and trajectory-aware safety through safety filters. The architecture is supported by a structured reward formulation designed to encourage effective target discovery and acquisition, collision avoidance, and de-correlation between the agentsâ€™ communication vectors by promoting informational orthogonality. The effectiveness of the proposed reward function is demonstrated through a comprehensive ablation study. Moreover, simulation results demonstrate safe, and stable task execution confirming the frameworkâ€™s effectiveness.\nkeywords:\nCooperative target acquisition, Safe autonomous coordination, Decentralized multi-agent reinforcement learning, Heterogeneous robotic systems, Learning-based control\nâ€ \nâ€ \nthanks:\nThis work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The authors are within the Robotics and AI Group, Department of Computer Science, Electrical and Space Engineering, LuleÃ¥ University of Technology, Sweden. Corresponding authorâ€™s e-mail: gabcal@ltu.se\n1\nIntroduction\nHeterogeneous multi-robot systems composed of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) offer complementary capabilities for complex missions such as surveillance, exploration, and search and rescue as highlighted by\nShahar\net al.\n(\n2025\n)\n. UAVs provide wide-area perception and rapid mobility, while UGVs contribute endurance, payload capacity, and ground-level sensing. Coordinating such diverse platforms in complex and communication-constrained environments requires the autonomous agents to make decentralized decisions under partial observability and distinct motion dynamics. These challenges have motivated the growing use of distributed Multi-Agent Reinforcement Learning (MARL) to enable multi-robot cooperation in uncertain environments as analyzed by\nWang\net al.\n(\n2022\n)\n. Recent advances have demonstrated the feasibility of learning decentralized control policies that scale to large teams and complex tasks as per\nYu\net al.\n(\n2022\n); Kuba\net al.\n(\n2022\n); Jing\net al.\n(\n2025\n)\n. On-policy actorâ€“critic methods such as the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm have become strong baselines for cooperative tasks, while trust-region extensions, such as HAPPO and HATRPO derived from Heterogeneous-Agent Trust Region Learning (HATRL) proposed by\nZhong\net al.\n(\n2024\n)\n, improve training stability for heterogeneous agents. However, most of these frameworks ignore strong safety guarantees and inter-agent communication limitations, which restricts their deployment in physical robotic systems.\nIn parallel, graph-based neural architectures have emerged as an innate representation for relational reasoning and communication in multi-agent systems as presented by\nNiu\net al.\n(\n2021\n); Hu\net al.\n(\n2024\n); Liu\net al.\n(\n2023\n)\n. By modeling agents as nodes and their interactions as edges, such methods learn both who to communicate with and what to share. Despite their success, these approaches typically neglect the safety-critical aspects of real-world robotics. Recent advances in safe reinforcement learning have expanded both the theoretical and algorithmic foundations of constrained policy optimization and safe exploration\nGu\net al.\n(\n2024\n); Wachi\net al.\n(\n2024\n)\n. These works formalize diverse safety specifications, ranging from cost-based and probabilistic constraints to state-wise safety requirements, and analyze their implications for policy learning and constraint satisfaction. Even-then, integration with heterogeneous MARL and communication learning remains limited.\n1.1\nRelated works\nMulti-Agent Reinforcement Learning (MARL) has advanced cooperative control under uncertainty. In particular, the MAPPO framework investigated by\nYu\net al.\n(\n2022\n)\nshows that a simple actorâ€“critic structure can yield robust policies with Centralized Training and Decentralized Execution (CTDE). Scalable MARL approaches aim to maintain learning efficiency and coordination as the number of agents grows. The work in\nCui\net al.\n(\n2025\n)\npresents an MADDPG-based UAVâ€“USV collaborative decision-making method that enhances visual relative-position estimation via CNNs, introduces tailored multi-constraint reward functions for heterogeneous cooperation, and achieves faster convergence and superior performance compared to traditional strategies. In this context,\nZhang\net al.\n(\n2024\n)\nintroduces a scalable constrained policy optimization framework called Scalable MAPPO-Lagrangian (Scal-MAPPO-L), that enables safe and decentralized learning in large multi-agent systems by employing local\nÎº\n\\kappa\n-hop policy updates, and thereby mitigating the exponential growth of the joint stateâ€“action space while preserving cooperative performance. The work in\nOkawa\net al.\n(\n2023\n)\nintroduces Information-sharing Constrained Policy Optimization (IsCPO), a MARL framework that sequentially updates agentsâ€™ policies with shared surrogate-cost and divergence information to guarantee constraint satisfaction during learning and achieve suboptimal yet safe multi-agent policies in large-scale control tasks.\nGuo\net al.\n(\n2024\n)\nfurther improves cooperative decision-making by using MAPPO-PIS, an intent-sharingâ€“driven cooperative decision-making framework for Connected and Autonomous Vehicles (CAVs) that integrates Intention Generator Module (IGM) and the Safety Enhanced Module (SEM) correction to improve multi-agent policy learning, achieving superior safety, efficiency, and traffic performance in humanâ€“machine mixed merging scenarios. Although these frameworks are effective, many do not explicitly guarantee physical safety or enforce dynamic feasibility constraints. Our approach extends this foundation by incorporating safety filters for real-time, model-consistent safety within a MAPPO structure. Communication learning has emerged as a critical component of decentralized coordination. The Multi-Agent Graph-attentIon Communication (MAGIC) framework proposed by\nNiu\net al.\n(\n2021\n)\nemploys a graph-attention communication protocol by jointly learning a Scheduler that determines when agents communicate and whom they address through a differentiable, attention-based dynamic graph encoder, and a Message Processor that applies Graph Attention Networks (GATs) over these dynamic graphs, enabling fully end-to-end training of communication structure and content. The Deep Hierarchical Communication Graph (DHCG) analyzed by\nLiu\net al.\n(\n2023\n)\nlearns directed acyclic communication structures by optimizing graph topology end-to-end, enabling convergent and efficient information flow among agents.\nHu\net al.\n(\n2024\n)\nproposes CommFormer that reframes communication as a graph learning problem using transformer-based attention to infer sparse and adaptive communication topologies. Similarly, Agent Transformer Memory (ATM)\nYang\net al.\n(\n2022\n)\naddresses partial observability in multi-agent RL by using a transformer-based working-memory mechanism to integrate factored entity observations with temporal information and to encode entity-specific action semantics.\nSun\net al.\n(\n2025\n)\nformalized communication redundancy through a dimensional analysis of message embeddings and demonstrated that enforcing decorrelation within inter-agent representations significantly improves communication efficiency and cooperative performance in large-scale MARL systems. Comprehensive surveys\nGronauer and Diepold (\n2022\n); Huh and Mohapatra (\n2024\n)\nemphasize the importance of adaptive, efficient communication. Our framework introduces a communication dissimilarity regularization term in the reward function that explicitly penalizes correlated message vectors.\nThe work in\nFan\net al.\n(\n2024\n)\nintroduce an offline-compatible safe experience reshaping mechanism that learns underlying constraint geometry, projects unsafe actions onto its tangent safe space, and trains policies exclusively on these safety-corrected experiences. A pixel-observation safe RL algorithm that jointly learns a low-dimensional latent dynamics model and a latent barrier-like safety function to encode state-wise constraints with unknown hazard regions, enabling simultaneous safe policy optimization that significantly reduces safety violations and accelerates safety convergence while maintaining competitive reward performance is proposed in\nZhan\net al.\n(\n2024\n)\n. To address the lack of a unified, practical methodology in SafeRL, where agents must optimize performance while minimizing risks of unintended harm,\nJi\net al.\n(\n2024\n)\nintroduce a foundational, safety-centric framework that consolidates diverse RL algorithms into a cohesive, efficient research platform designed to streamline and accelerate progress in AI safety. Safe MARL frameworks\nGarg\net al.\n(\n2024\n)\nand constrained optimization approaches\nZhang\net al.\n(\n2024\n)\nhave begun exploring similar directions, but most remain centralized or homogeneous. Our work bridges this gap by embedding trajectory-aware safety filter for both holonomic (UAV) and differential-drive (UGV) agents directly into the decentralized MARL loop.\nFigure 1\n:\nIllustration of the VMAS environment in which three agents are trained to locate and acquire randomly placed targets (orange). The diagram highlights the key components of each agentâ€™s policy, including the GNN encoder, MLP heads, and safety filters.\n1.2\nOur contribution\nTo address these challenges, this paper proposes a decentralized MARL framework for structurally heterogeneous agents, such as UAVâ€“UGV teams, that combines graph-based communication, safety filters that evaluate the feasibility of the trajectory induced by each proposed action and rescale it in the event of a predicted collision, and stable on-policy learning. Moreover, a communication orthogonality regularization enforces representational dissimilarity among agentsâ€™ message embeddings, mitigating interference and redundancy in shared communication channels. In particular, the main contributions are:\nâ€¢\nA decentralized MARL architecture enabling coordination between structurally heterogeneous agents for collaborative target acquisition along with a communication orthogonality regularizer that promotes diverse messaging and robust multi-agent coordination.\nâ€¢\nA trajectory-based safety filter that enforces collision avoidance by predicting the state evolution induced by each proposed action and rescaling unsafe actions when a future violation is detected, ensuring that the executed control inputs remain within the admissible safe set.\nâ€¢\nComprehensive ablation studies that quantify the contributions of goal-reaching, safety enforcement, and communication diversity to overall task performance.\n2\nMethodology\nThis section introduces the reinforcement learning framework in\n2.1\n, the structure of the policy and critic networks in\n2.2\n, the proposed safety filters in\n2.3\nand the reward function formulation in\n2.4\n.\n2.1\nReinforcement learning framework\nThe partially-observable environment consists of a continuous 2D squared workspace of side length\nd\nd\nshown in Fig.\n1\nthat includes\nn\nt\nn_{t}\nrandomly distributed targets representing landmarks that\nn\nh\nn_{h}\nholonomic agents (UAVs) with radius\nr\nh\nr_{h}\nand\nn\nd\nn_{d}\ndifferential-drive agents (UGVs) with radius\nr\nd\nr_{d}\nhave to explore and reach to.\nWe denote by\nâ„‹\n=\n{\n1\n,\nâ€¦\n,\nn\nh\n}\n\\mathcal{H}=\\{1,\\ldots,n_{h}\\}\nand\nğ’Ÿ\n=\n{\nn\nh\n+\n1\n,\nâ€¦\n,\nn\nh\n+\nn\nd\n}\n\\mathcal{D}=\\{n_{h}+1,\\ldots,n_{h}+n_{d}\\}\nthe sets of holonomic and differential-drive agents, respectively.\nThe state of each agent\ni\ni\nis\nğ±\ni\nt\n=\n{\n[\nğ©\ni\nt\n,\nğ¯\ni\nt\n]\nâŠ¤\nâˆˆ\nâ„\n4\n,\ni\nâˆˆ\nâ„‹\n,\n[\nğ©\ni\nt\n,\nÎ¸\ni\nt\n,\nv\ni\nt\n,\nÏ‰\ni\nt\n]\nâŠ¤\nâˆˆ\nâ„\n5\n,\ni\nâˆˆ\nğ’Ÿ\n,\n\\mathbf{x}_{i}^{t}=\\begin{cases}\\big[\\mathbf{p}_{i}^{t},\\ \\mathbf{v}_{i}^{t}\\big]^{\\!\\top}\\in\\mathbb{R}^{4},&i\\in\\mathcal{H},\\\\\n\\big[\\mathbf{p}_{i}^{t},\\ \\theta_{i}^{t},\\ v_{i}^{t},\\ \\omega_{i}^{t}\\big]^{\\!\\top}\\in\\mathbb{R}^{5},&i\\in\\mathcal{D},\\end{cases}\n(1)\nwhere\nğ©\ni\nt\n=\n[\nx\ni\nt\n,\ny\ni\nt\n]\nâŠ¤\n\\mathbf{p}_{i}^{t}=[x_{i}^{t},y_{i}^{t}]^{\\!\\top}\nis the position,\nğ¯\ni\nt\n=\n[\nv\nx\n,\ni\nt\n,\nv\ny\n,\ni\nt\n]\nâŠ¤\n\\mathbf{v}_{i}^{t}=[v_{x,i}^{t},v_{y,i}^{t}]^{\\!\\top}\nis the velocity (holonomic),\nÎ¸\ni\nt\n\\theta_{i}^{t}\nis the heading,\nv\ni\nt\nv_{i}^{t}\nthe linear speed, and\nÏ‰\ni\nt\n\\omega_{i}^{t}\nthe angular speed of the agent\ni\ni\n.\nMoreover, all agents are equipped with a simulated range sensor based on ray casting, which provides\nn\nl\nn_{l}\nrange measurements indicating detected targets or the maximum sensing range, denoted\nr\nh\nl\nr_{h}^{l}\nfor holonomic agents and\nr\nd\nl\nr_{d}^{l}\nfor differential-drive agents.\nAt the beginning of each episode, the agents have no prior information about the environment or target locations. A target\ng\nâˆˆ\nğ’¢\ng\\in\\mathcal{G}\nis deemed considered covered when at least one agent lies within the distance threshold\nÏ\ncov\n\\rho_{\\text{cov}}\n, as defined in Eq.Â (\n2\n).\nmin\ni\nâˆˆ\nâ„‹\nâˆª\nğ’Ÿ\nâ¡\nâ€–\nğ©\ni\nt\nâˆ’\nğ©\ng\nt\nâ€–\n2\nâ‰¤\nÏ\ncov\n\\min_{i\\in\\mathcal{H}\\cup\\mathcal{D}}\\big\\|\\mathbf{p}^{t}_{i}-\\mathbf{p}^{t}_{g}\\big\\|_{2}\\;\\leq\\;\\rho_{\\text{cov}}\n(2)\nTo explore the environment and find the targets, at each step\nt\nt\n, each agent\ni\ni\noutputs an action\na\ni\nt\na_{i}^{t}\ncomposed by a continuous movement action\nğ’–\ni\nt\nâˆˆ\nâ„\n2\n\\bm{u}_{i}^{t}\\in\\mathbb{R}^{2}\nsubject to the agentâ€™s dynamics and a communication vector\nğ’„\ni\nt\nâˆˆ\nâ„\nd\nc\n\\bm{c}^{t}_{i}\\in\\mathbb{R}^{d_{c}}\n. Agents are allowed to exchange information with agents within a finite communication radius\nr\nc\nr_{c}\n. These actions are computed by the policies based on some observations received from the environment at the beginning of the step. In particular, each agent\ni\ni\nreceives an observation vector\nğ¨\ni\nt\n\\mathbf{o}_{i}^{t}\nthat combines its sensor measurements, the communication vector, and selected attributes of its own state as per Eq. (\n3\n).\nğ¨\ni\nt\n=\n{\n(\nl\ni\nt\n,\nğœ\ni\nt\n,\nğ¯\ni\nt\n)\n,\ni\nâˆˆ\nâ„‹\n,\n(\nl\ni\nt\n,\nğœ\ni\nt\n,\nğ¯\ni\nt\n,\nÎ¸\ni\nt\n,\nÏ‰\ni\nt\n)\n,\ni\nâˆˆ\nğ’Ÿ\n,\n\\mathbf{o}_{i}^{t}=\\begin{cases}\\big(l_{i}^{t},\\ \\mathbf{c}_{i}^{t},\\ \\mathbf{v}_{i}^{t}\\big),&i\\in\\mathcal{H},\\\\\n\\big(l_{i}^{t},\\ \\mathbf{c}_{i}^{t},\\ \\mathbf{v}_{i}^{t},\\ \\theta_{i}^{t},\\ \\omega_{i}^{t}\\big),&i\\in\\mathcal{D},\\end{cases}\n(3)\nwhere\nl\ni\nt\nl_{i}^{t}\ndenotes the range-sensor readings,\nğœ\ni\nt\n\\mathbf{c}_{i}^{t}\nthe communication vector,\nğ¯\ni\nt\n\\mathbf{v}_{i}^{t}\nthe agentâ€™s velocity,\nÎ¸\ni\nt\n\\theta_{i}^{t}\nthe heading, and\nÏ‰\ni\nt\n\\omega_{i}^{t}\nthe angular velocity. Upon selecting an action\nğ’–\ni\nt\n\\bm{u}_{i}^{t}\n, a safety filter\nÎ“\n\\Gamma\nis applied, with its formulation defined in\n2.3\n. The filter enforces safety constraints by projecting the nominal control input\nğ®\ni\nt\n\\mathbf{u}_{i}^{t}\ngenerated by the policy onto the admissible action set, resulting in the safe action\nğ®\n~\ni\nt\n\\tilde{\\mathbf{u}}_{i}^{t}\n. Thus ensuring that the executed command does not lead to collisions with other agents.\nHolonomic agents are fully actuated in the plane and use a two-dimensional safe control input\nğ®\n~\ni\nt\n=\n[\nf\n~\nx\n,\nf\n~\ny\n]\nâŠ¤\n\\tilde{\\mathbf{u}}_{i}^{t}=[\\,\\tilde{f}_{x},\\tilde{f}_{y}\\,]^{\\top}\n, where each force component satisfies\nf\n~\nx\n,\nf\n~\ny\nâˆˆ\n[\nâˆ’\nu\nmax\n,\nu\nmax\n]\n\\tilde{f}_{x},\\tilde{f}_{y}\\in[-u_{\\max},u_{\\max}]\n. The safe control input directly affects the discrete-time dynamics, which can be written in state-update form as per Eq. (\n4\n).\nğ±\ni\nt\n+\n1\n=\n[\nğ©\ni\nt\n+\n1\nğ¯\ni\nt\n+\n1\n]\n=\n[\nğ©\ni\nt\n+\nğ¯\ni\nt\nâ€‹\nÎ”\nâ€‹\nt\nğ¯\ni\nt\n+\n(\n1\nm\ni\nâ€‹\nğ®\n~\ni\nt\nâˆ’\nc\nd\nâ€‹\nğ¯\ni\nt\n)\nâ€‹\nÎ”\nâ€‹\nt\n]\n\\mathbf{x}_{i}^{t+1}=\\begin{bmatrix}\\mathbf{p}_{i}^{t+1}\\\\\n\\mathbf{v}_{i}^{t+1}\\end{bmatrix}=\\begin{bmatrix}\\mathbf{p}_{i}^{t}+\\mathbf{v}_{i}^{t}\\,\\Delta t\\\\\n\\mathbf{v}_{i}^{t}+\\Big(\\tfrac{1}{m_{i}}\\tilde{\\mathbf{u}}_{i}^{t}-c_{d}\\,\\mathbf{v}_{i}^{t}\\Big)\\Delta t\\end{bmatrix}\n(4)\nwhere\nm\nm\ndenotes the agentâ€™s mass,\nÎ”\nâ€‹\nt\n\\Delta t\nis the environmentâ€™s time-step, and\nc\nd\nâ‰¥\n0\nc_{d}\\!\\geq\\!0\nrepresents the linear drag coefficient. Differential-drive agents evolve according to unicycle kinematics integrated via a fourth-order Runge-Kutta (RK4) scheme. Their safe control action is given by\nğ®\n~\ni\nt\n=\n[\nv\n~\ni\nt\n,\nÏ‰\n~\ni\nt\n]\nâŠ¤\n\\tilde{\\mathbf{u}}_{i}^{t}=[\\,\\tilde{v}_{i}^{t},\\ \\tilde{\\omega}_{i}^{t}\\,]^{\\top}\n, where the linear and angular velocities satisfy\nv\n~\ni\nt\n,\nÏ‰\n~\ni\nt\nâˆˆ\n[\nâˆ’\nu\nmax\n,\nu\nmax\n]\n\\tilde{v}_{i}^{t},\\,\\tilde{\\omega}_{i}^{t}\\in[-u_{\\max},\\,u_{\\max}]\n.\n2.2\nPolicy and critic networks\nEach agentâ€™s policy is structured as a Graph Neural Network (GNN) encoder followed by a Multi-Layer Perceptron (MLP) output head.\nThe first stage of each agentâ€™s policy is represented by a graph neural network (GNN) defined over a dynamic, position-dependent graph\nğ’¢\nt\n=\n(\nğ’±\n,\nâ„°\nt\n)\n\\mathcal{G}_{t}=(\\mathcal{V},\\mathcal{E}_{t})\n,\nwhere each node\nv\ni\nâˆˆ\nğ’±\nv_{i}\\in\\mathcal{V}\ncorresponds to an agent at time\nt\nt\n.\nThe edge set\nâ„°\nt\n\\mathcal{E}_{t}\nis constructed from the agentsâ€™ spatial configuration, such that\nan edge exists between two agents if their Euclidean distance is within the predefined\ncommunication range\nr\nc\nr_{c}\naccording to Eq. (\n5\n).\nâ„°\nt\n=\n{\n(\nv\ni\n,\nv\nj\n)\nâˆ£\nâ€–\nğ©\ni\nt\nâˆ’\nğ©\nj\nt\nâ€–\n2\nâ‰¤\nr\nc\n,\ni\nâ‰ \nj\n}\n\\mathcal{E}_{t}=\\{(v_{i},v_{j})\\mid\\|\\mathbf{p}^{t}_{i}-\\mathbf{p}^{t}_{j}\\|_{2}\\leq r_{c},\\,i\\neq j\\}\n(5)\nEach node feature vector contains the local observation of the corresponding agent. The GNN then constructs edge features by computing pairwise relational quantities, specifically the relative position\nğ©\ni\nt\nâˆ’\nğ©\nj\nt\n\\mathbf{p}^{t}_{i}-\\mathbf{p}^{t}_{j}\n, the inter-agent distance\nâ€–\nğ©\ni\nt\nâˆ’\nğ©\nj\nt\nâ€–\n2\n\\|\\mathbf{p}^{t}_{i}-\\mathbf{p}^{t}_{j}\\|_{2}\n, and the relative velocity\nğ¯\ni\nt\nâˆ’\nğ¯\nj\nt\n\\mathbf{v}^{t}_{i}-\\mathbf{v}^{t}_{j}\n. Message passing within the GNN is implemented using the GATv2Conv operator, proposed by\nBrody\net al.\n(\n2022\n)\n, which performs attention-weighted aggregation over neighboring nodes.\nAfter graph aggregation, each agent passes its embedding through a policy head conditioned on its dynamic model where each MLP consists of two hidden layers with 256 units and ELU activations followed by a linear output layer matching the action dimension.\nFurthermore, the critic is implemented using a DeepSets architecture proposed by\nZaheer\net al.\n(\n2018\n)\n. The joint observation\nğ¨\nt\n=\n(\n{\nğ¨\ni\nt\n}\ni\nâˆˆ\nâ„‹\n,\n{\nğ¨\ni\nt\n}\ni\nâˆˆ\nğ’Ÿ\n)\n\\mathbf{o}^{t}=\\big(\\{\\mathbf{o}_{i}^{t}\\}_{i\\in\\mathcal{H}},\\{\\mathbf{o}_{i}^{t}\\}_{i\\in\\mathcal{D}}\\big)\nis first embedded through a local encoder\nÏ•\nâ€‹\n(\nğ’\nt\n)\n\\phi(\\bm{o}^{t})\n, and the global value is obtained by symmetric aggregation according to Eq. (\n6\n).\nV\nâ€‹\n(\nğ’”\n)\n=\nÏ\nâ€‹\n(\n1\nN\na\nâ€‹\nâˆ‘\ni\n=\n1\nN\na\nÏ•\nâ€‹\n(\nğ’\nt\n)\n)\n,\nV(\\bm{s})=\\rho\\!\\left(\\frac{1}{N_{a}}\\sum_{i=1}^{N_{a}}\\phi(\\bm{o}^{t})\\right),\n(6)\nwhere\nÏ•\nâ€‹\n(\nâ‹…\n)\n\\phi(\\cdot)\nand\nÏ\nâ€‹\n(\nâ‹…\n)\n\\rho(\\cdot)\nare mappings realized as MLPs with ELU activations. In our setup,\nÏ•\n\\phi\nconsists of two hidden layers of 128 neurons and\nÏ\n\\rho\nof two layers with 256 neurons.\n2.3\nSafety filters\nTo guarantee real-time safety, each agentâ€™s control input is projected onto the closest admissible control that satisfies a prescribed set of safety constraints, ensuring that the predicted trajectory remains collision-free. In particular,\nd\nsafe\nd_{\\text{safe}}\nis the minimum required inter-agent separation to avoid collision.\n2.3.1\nHolonomic agents\nTo guarantee collision avoidance under uncertain neighbor motion, it is proposed a safety filter that selects the closest feasible control input\nto the nominal force while ensuring that the agentâ€™s trajectory\nremains collision-free over the entire prediction horizon.\nFor any candidate scaled control input\nğ®\n~\ni\nâ€‹\n(\nÎ±\n)\n=\nÎ±\nâ€‹\nğ®\ni\nt\n\\tilde{\\mathbf{u}}_{i}(\\alpha)=\\alpha\\,\\mathbf{u}_{i}^{t}\nwhere\nÎ±\nâˆˆ\nğ’œ\n=\n{\n1.0\n,\n0.8\n,\n0.6\n,\n0.4\n,\n0.25\n,\n0.1\n,\n0.0\n}\n,\n\\alpha\\in\\mathcal{A}=\\{1.0,\\,0.8,\\,0.6,\\,0.4,\\,0.25,\\,0.1,\\,0.0\\},\nthe safety filter evaluates the feasibility of the resulting trajectory by propagating the agentâ€™s motion under\nğ®\n~\ni\nâ€‹\n(\nÎ±\n)\n\\tilde{\\mathbf{u}}_{i}(\\alpha)\nand verifying that it remains outside the maximum possible occupancy regions of all nearby agents.\nAmong all feasible candidates, the control input corresponding to the largest admissible scaling factor is selected. In particular, the ego agentâ€™s trajectory is approximated over\nÏ„\nâˆˆ\n[\n0\n,\nÎ”\nâ€‹\nt\n]\n\\tau\\in[0,\\Delta t]\nas\nğ©\ni\nt\n+\nÏ„\nâ€‹\n(\nÎ±\n)\n\\displaystyle\\mathbf{p}_{i}^{\\,t+\\tau}(\\alpha)\n=\nğ©\ni\nt\n+\n[\nğ¯\ni\nt\n+\nÎ”\nâ€‹\nt\n2\nâ€‹\nm\ni\nâ€‹\nğ®\ni\nâ€‹\n(\nÎ±\n)\n]\nâ€‹\nÏ„\n\\displaystyle=\\mathbf{p}_{i}^{\\,t}+\\Bigl[\\mathbf{v}_{i}^{\\,t}+\\tfrac{\\Delta t}{2m_{i}}\\,\\mathbf{u}_{i}(\\alpha)\\Bigr]\\tau\n(7)\ns.t.\nâ€–\nğ©\ni\nt\n+\nÏ„\nâ€‹\n(\nÎ±\n)\nâˆ’\nğ©\nj\nt\n+\nÏ„\nâ€–\n2\nâ‰¥\nd\nsafe\n.\n\\displaystyle\\bigl\\lVert\\mathbf{p}_{i}^{\\,t+\\tau}(\\alpha)-\\mathbf{p}_{j}^{\\,t+\\tau}\\bigr\\rVert_{2}\\;\\geq\\;d_{\\mathrm{safe}}.\nEach neighboring holonomic agent\nj\nj\npossesses bounded per-axis\nvelocities\nğ¯\nj\nt\nâˆˆ\n[\nâˆ’\nv\nmax\n,\nv\nmax\n]\n\\mathbf{v}^{t}_{j}\\in[-v_{\\max},v_{\\max}]\n.\nThis induces a worst-case reachable set at time\nt\n+\nÎ”\nâ€‹\nt\nt+\\Delta t\ndescribed by the\naxis-aligned rectangle\nâ„›\nj\nt\n=\n{\nğ©\nâˆˆ\nâ„\n2\n|\n|\nğ©\nâˆ’\nğ‚\nj\nt\n|\nâˆ\nâ‰¤\nğ‡\nj\n}\n,\n\\mathcal{R}^{t}_{j}=\\big\\{\\,\\mathbf{p}\\in\\mathbb{R}^{2}\\;\\big|\\;\\lvert\\mathbf{p}-\\mathbf{C}^{t}_{j}\\rvert_{\\infty}\\leq\\mathbf{H}_{j}\\big\\},\n(8)\nwhere the center and half-size are given by\nğ‚\nj\nt\n=\nğ©\nj\nt\n+\n1\n2\nâ€‹\n(\nğ¯\nmin\n+\nğ¯\nmax\n)\nâ€‹\nÎ”\nâ€‹\nt\n,\nğ‡\nj\n=\n1\n2\nâ€‹\n(\nğ¯\nmax\nâˆ’\nğ¯\nmin\n)\nâ€‹\nÎ”\nâ€‹\nt\n.\n\\mathbf{C}^{t}_{j}=\\mathbf{p}^{t}_{j}+\\tfrac{1}{2}(\\mathbf{v}_{\\min}+\\mathbf{v}_{\\max})\\Delta t,\\qquad\\mathbf{H}_{j}=\\tfrac{1}{2}(\\mathbf{v}_{\\max}-\\mathbf{v}_{\\min})\\Delta t.\n(9)\nTo account for the circular bodies of the ego and neighbor agents, this set is\ninflated by the sum of agentsâ€™ radii\nr\ni\n+\nr\nj\nr_{i}+r_{j}\nin the\nL\nâˆ\nL_{\\infty}\nmetric,\nâ„›\nj\n+\n=\n{\nğ±\nâˆˆ\nâ„\n2\n|\n|\nğ±\nâˆ’\nğ‚\nj\nt\n|\nâˆ\nâ‰¤\nğ‡\nj\n+\n(\nr\ni\n+\nr\nj\n+\n+\nd\nsafe\n)\nğŸ\n}\n.\n\\mathcal{R}_{j}^{+}=\\big\\{\\,\\mathbf{x}\\in\\mathbb{R}^{2}\\;\\big|\\;\\lvert\\mathbf{x}-\\mathbf{C}^{t}_{j}\\rvert_{\\infty}\\leq\\mathbf{H}_{j}+(r_{i}+r_{j}++d_{\\mathrm{safe}})\\mathbf{1}\\big\\}.\n(10)\nIn addition to holonomic neighbors, the environment contains differential-drive agents with maximum linear speed\nv\nmax\nv_{\\max}\n.\nOver a short horizon\nÎ”\nâ€‹\nt\n\\Delta t\n, the exact unicycle reachable set is a\nnonlinear subset of a circular sector whose angular width is bounded by\nÏ‰\nmax\nâ€‹\nÎ”\nâ€‹\nt\n\\omega^{\\max}\\Delta t\n.\nTo enable efficient continuous-time collision checking, a conservative over-approximation is adopted by assigning to each diff-drive agent an isotropic velocity bound\nâ€–\nğ©\nË™\nj\nt\nâ€–\n2\nâ‰¤\nv\nmax\n\\big\\|\\dot{\\mathbf{p}}^{t}_{j}\\big\\|_{2}\\leq v^{\\max}\nwhich implies that its position at time\nt\n+\nÎ”\nâ€‹\nt\nt+\\Delta t\nbelongs to the disc\nğ’Ÿ\nj\nt\n=\n{\nğ±\nâˆˆ\nâ„\n2\n|\nâ€–\nğ±\nâˆ’\nğ©\nj\nt\nâ€–\n2\nâ‰¤\nv\nmax\nâ€‹\nÎ”\nâ€‹\nt\n}\n.\n\\mathcal{D}^{t}_{j}=\\big\\{\\,\\mathbf{x}\\in\\mathbb{R}^{2}\\;\\big|\\;\\big\\|\\mathbf{x}-\\mathbf{p}^{t}_{j}\\big\\|_{2}\\leq v^{\\max}\\Delta t\\big\\}.\n(11)\nThis disc is inflated by the sum of agentsâ€™ radii and an additional safety\ndistance\nd\nsafe\nd_{\\mathrm{safe}}\n, yielding the conservative collision set\nğ’Ÿ\nj\n+\n=\n{\nğ±\nâˆˆ\nâ„\n2\n|\nâ€–\nğ±\nâˆ’\nğ©\nj\nt\nâ€–\n2\nâ‰¤\nv\nmax\nâ€‹\nÎ”\nâ€‹\nt\n+\n(\nr\ni\n+\nr\nj\n+\nd\nsafe\n)\n}\n.\n\\mathcal{D}_{j}^{+}=\\big\\{\\,\\mathbf{x}\\in\\mathbb{R}^{2}\\;\\big|\\;\\big\\|\\mathbf{x}-\\mathbf{p}^{t}_{j}\\big\\|_{2}\\leq v^{\\max}\\Delta t+(r_{i}+r_{j}+d_{\\mathrm{safe}})\\big\\}.\n(12)\nFinally, it is selected the\nÎ±\n\\alpha\nthat satisfies Eq. (\n13\n).\nmax\nÎ±\nâˆˆ\nğ’œ\n\\displaystyle\\max_{\\alpha\\in\\mathcal{A}}\nÎ±\n\\displaystyle\\alpha\n(13)\ns.t.\nğ©\ni\nâ€‹\n(\nt\n+\nÏ„\n;\nÎ±\n)\nâˆ‰\nğ’\n,\nâˆ€\nÏ„\nâˆˆ\n[\n0\n,\nÎ”\nâ€‹\nt\n]\n.\n\\displaystyle\\mathbf{p}_{i}(t+\\tau;\\alpha)\\;\\notin\\;\\mathcal{C},\\qquad\\forall\\,\\tau\\in[0,\\Delta t].\nwhere\nğ’\n=\n(\nâ‹ƒ\nj\nâˆˆ\nH\nâ„›\nj\n+\n)\nâˆª\n(\nâ‹ƒ\nj\nâˆˆ\nD\nğ’Ÿ\nj\n+\n)\n.\n\\mathcal{C}\\;=\\;\\Bigl(\\,\\bigcup_{j\\in H}\\mathcal{R}_{j}^{+}\\Bigr)\\;\\cup\\;\\Bigl(\\,\\bigcup_{j\\in D}\\mathcal{D}_{j}^{+}\\Bigr).\n(14)\n2.3.2\nDifferential drive agent\nSimilar considerations are done for the definition of the safety filter used by the differential drive agent but in this case the forward evolution under\nu\nâ€‹\n(\nÎ±\n)\nu(\\alpha)\nis approximated using a fourth-order\nRungeâ€“Kutta (RK4) integrator,\nand the interval\n[\n0\n,\nÎ”\nâ€‹\nt\n]\n[0,\\Delta t]\nis subdivided into 5 uniform substeps to produce intermediate states\n{\nx\nk\nâ€‹\n(\nÎ±\n)\n}\nk\n=\n1\n5\n\\{x_{k}(\\alpha)\\}_{k=1}^{5}\n. Each intermediate position\np\nk\nâ€‹\n(\nÎ±\n)\nâˆˆ\nâ„\n2\np_{k}(\\alpha)\\in\\mathbb{R}^{2}\nis subjected to collision checks in a way analogous to the one discussed before for the holonomic agent.\n(a)\n(b)\n(c)\nFigure 2\n:\nComparison of the mean episode reward trajectories for the full multi-agent system (a), holonomic agents (b), and diff-drive agents (c) under different reward shaping configurations. Reward schemes are color-coded as follows: R1 (blue), R2 (orange), R3 (green), and R4 (grey).\n2.4\nReward function formulation\nAt each step\nt\nt\n, agent\ni\ni\nreceives a scalar reward\nr\ni\nt\nr_{i}^{t}\nexpressed by Eq. (\n15\n).\nr\ni\nt\n=\nğ’˜\nâŠ¤\nâ€‹\nğ’“\ni\nt\n=\n[\nw\ndist\nw\ngoal\nw\ncoll\nw\ncomm\n]\nâ€‹\n[\nr\ni\ndist\nâ€‹\n(\nt\n)\nr\ni\ngoal\nâ€‹\n(\nt\n)\nr\ni\ncoll\nâ€‹\n(\nt\n)\nr\ni\ncomm\nâ€‹\n(\nt\n)\n]\n,\nr_{i}^{t}\\;=\\;\\bm{w}^{\\top}\\bm{r}_{i}^{t}\\;=\\;\\begin{bmatrix}w_{\\text{dist}}&w_{\\text{goal}}&w_{\\text{coll}}&w_{\\text{comm}}\\end{bmatrix}\\begin{bmatrix}r_{i}^{\\text{dist}}(t)\\\\[3.0pt]\nr_{i}^{\\text{goal}}(t)\\\\[3.0pt]\nr_{i}^{\\text{coll}}(t)\\\\[3.0pt]\nr_{i}^{\\text{comm}}(t)\\end{bmatrix},\n(15)\nwhere\nğ’˜\nâˆˆ\nâ„\n4\n\\bm{w}\\!\\in\\!\\mathbb{R}^{4}\ncontains the reward weights and\nğ’“\ni\nt\nâˆˆ\nâ„\n4\n\\bm{r}_{i}^{t}\\!\\in\\!\\mathbb{R}^{4}\nthe corresponding reward terms. These last terms capture progress toward targets, goal completion, collision avoidance, and communication diversity. The distance-based reward\nr\ni\ndist\nâ€‹\n(\nt\n)\nr_{i}^{\\text{dist}}(t)\nencourages agents to approach their nearest target and is defined as\nr\ni\ndist\nâ€‹\n(\nt\n)\n=\nmin\ng\nâˆˆ\nğ’¢\nâ¡\nâ€–\nğ©\ni\nt\nâˆ’\nğ©\ng\nt\nâ€–\nâˆ’\nmin\ng\nâˆˆ\nğ’¢\nâ¡\nâ€–\nğ©\ni\nt\nâˆ’\n1\nâˆ’\nğ©\ng\nt\nâˆ’\n1\nâ€–\n.\nr_{i}^{\\text{dist}}(t)=\\min_{g\\in\\mathcal{G}}\\|\\mathbf{p}_{i}^{t}-\\mathbf{p}_{g}^{t}\\|-\\min_{g\\in\\mathcal{G}}\\|\\mathbf{p}_{i}^{\\,t-1}-\\mathbf{p}^{t-1}_{g}\\|.\n(16)\nMoreover, when the agent covers a target, a fixed positive reward\nr\ngoal\nr_{\\text{goal}}\nis assigned to\nr\ni\ngoal\nâ€‹\n(\nt\n)\nr_{i}^{\\text{goal}}(t)\n. To discourage unsafe proximity to other agents, we define\nr\ni\ncoll\nâ€‹\n(\nt\n)\n=\nr\ncoll\nr_{i}^{\\text{coll}}(t)=r_{\\text{coll}}\nwhenever a collision happens. Furthermore, each agent produces a message vector\nc\ni\nt\nâˆˆ\nâ„\nd\nc\nc_{i}^{t}\\in\\mathbb{R}^{d_{c}}\n, and message diversity is encouraged via Eq. (\n17\n).\nr\ni\ncomm\nâ€‹\n(\nt\n)\n=\nâˆ‘\nj\nâ‰ \ni\n(\n1\nâˆ’\nÎ“\n2\nâ€‹\n(\nc\ni\nt\n,\nc\nj\nt\n)\n)\n,\nÎ“\nâ€‹\n(\nc\ni\nt\n,\nc\nj\nt\n)\n=\nâŸ¨\nc\ni\nt\n,\nc\nj\nt\nâŸ©\nâ€–\nc\ni\nt\nâ€–\nâ€‹\nâ€–\nc\nj\nt\nâ€–\n,\nr_{i}^{\\text{comm}}(t)\\;=\\;\\sum_{j\\neq i}\\!\\left(1-\\Gamma^{2}\\!\\big(c_{i}^{t},c_{j}^{t}\\big)\\right),\\quad\\Gamma(c_{i}^{t},c_{j}^{t})=\\frac{\\langle c_{i}^{t},c_{j}^{t}\\rangle}{\\|c_{i}^{t}\\|\\,\\|c^{t}_{j}\\|},\n(17)\nTo assess the contribution of the individual reward components, it is performed an ablation study using the progressively structured reward functions given by the reward terms vectors defined in Eq.\n18\n. The analysis encompasses both the performance metrics recorded during policy training and the outcomes obtained in simulation over randomly generated environments, as reported in\n3.2\nand\n3.3\n.\nR1:\nğ’“\ni\n(\n1\n)\nâ€‹\n(\nt\n)\n\\displaystyle\\textbf{R1:}\\quad\\bm{r}_{i}^{(1)}(t)\n=\n[\nr\ni\ndist\nâ€‹\n(\nt\n)\n,\n0\n,\n0\n,\n0\n]\nâŠ¤\n,\n\\displaystyle=[\\,r_{i}^{\\text{dist}}(t),0,0,0\\,]^{\\top},\n(18)\nR2:\nğ’“\ni\n(\n2\n)\nâ€‹\n(\nt\n)\n\\displaystyle\\textbf{R2:}\\quad\\bm{r}_{i}^{(2)}(t)\n=\n[\nr\ni\ndist\nâ€‹\n(\nt\n)\n,\nr\ni\ngoal\nâ€‹\n(\nt\n)\n,\n0\n,\n0\n]\nâŠ¤\n,\n\\displaystyle=[\\,r_{i}^{\\text{dist}}(t),\\ r_{i}^{\\text{goal}}(t),0,0\\,]^{\\top},\nR3:\nğ’“\ni\n(\n3\n)\nâ€‹\n(\nt\n)\n\\displaystyle\\textbf{R3:}\\quad\\bm{r}_{i}^{(3)}(t)\n=\n[\nr\ni\ndist\nâ€‹\n(\nt\n)\n,\nr\ni\ngoal\nâ€‹\n(\nt\n)\n,\nr\ni\ncoll\nâ€‹\n(\nt\n)\n,\n0\n]\nâŠ¤\n,\n\\displaystyle=[\\,r_{i}^{\\text{dist}}(t),\\ r_{i}^{\\text{goal}}(t),\\ r_{i}^{\\text{coll}}(t),0\\,]^{\\top},\nR4:\nğ’“\ni\n(\n4\n)\nâ€‹\n(\nt\n)\n\\displaystyle\\textbf{R4:}\\quad\\bm{r}_{i}^{(4)}(t)\n=\n[\nr\ni\ndist\nâ€‹\n(\nt\n)\n,\nr\ni\ngoal\nâ€‹\n(\nt\n)\n,\nr\ni\ncoll\nâ€‹\n(\nt\n)\n,\nr\ni\ncomm\nâ€‹\n(\nt\n)\n]\nâŠ¤\n.\n\\displaystyle=[\\,r_{i}^{\\text{dist}}(t),\\ r_{i}^{\\text{goal}}(t),\\ r_{i}^{\\text{coll}}(t),\\ r_{i}^{\\text{comm}}(t)\\,]^{\\top}.\n3\nResults\nThis section presents the training setup in\n2.1\nand the training and simulation results in\n3.2\nand\n3.3\n, respectively.\n3.1\nTraining\nThe agents are trained within a custom environment built on the VMAS simulator proposed by\nBettini\net al.\n(\n2022\n)\n, configured according to the parameters reported in Table\n1\n. Moreover, the training is performed using the BenchMARL framework proposed by\nBettini\net al.\n(\n2024\n)\n, leveraging its implementation of the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm. In particular, MAPPO is an on-policy actor-critic method adhering to the centralized training and decentralized execution (CTDE) paradigm, whereby each agent learns an individual policy, while a shared centralized critic is optimized using joint observations to stabilize multi-agent learning.\nTable 1\n:\nKey environment and reward function parameters\nSymbol\nValue\nSymbol\nValue\nn\nd\n,\nn\nh\n,\nn\nt\n,\nd\nn_{d},n_{h},n_{t},d\n1\n,\n2\n,\n3\n,\n10\nâ€‹\nm\n1,2,3,10m\nw\ndist\nw_{\\text{dist}}\n1.0\n1.0\nr\nd\n,\nr\nh\n,\nv\nmax\nr_{d},r_{h},v_{\\max}\n0.5\nâ€‹\nm\n,\n0.5\nâ€‹\nm\n,\n10\nâ€‹\nm\n/\ns\n0.5m,0.5m,10m/s\nw\ngoal\nw_{\\text{goal}}\n1.0\n1.0\nn\nl\n,\nr\nd\nl\n,\nr\nh\nl\nn_{l},r^{l}_{d},r^{l}_{h}\n16\n,\n1.5\nâ€‹\nm\n,\n3.0\nâ€‹\nm\n16,1.5m,3.0m\nw\ncoll\nw_{\\text{coll}}\n1.0\n1.0\nd\nsafe\n,\nÏ\ncov\nd_{\\text{safe}},\\rho_{\\text{cov}}\n0.05\nâ€‹\nm\n,\n1.5\nâ€‹\nm\n0.05m,1.5m\nw\ncomm\nw_{\\text{comm}}\n0.1\n0.1\nm\n,\nu\nmax\n,\nÎ”\nâ€‹\nt\n,\nc\nd\nm,u_{\\max},\\Delta t,c_{d}\n1\nâ€‹\nk\nâ€‹\ng\n,\n1\n,\n0.1\nâ€‹\ns\n,\n0.25\n1kg,1,0.1s,0.25\nr\ngoal\nr_{\\text{goal}}\n10.0\n10.0\nd\nc\n,\nr\nc\nd_{c},r_{c}\n16\n,\n4.5\nâ€‹\nm\n16,4.5m\nr\ncoll\nr_{\\text{coll}}\nâˆ’\n8.0\n-8.0\n(a)\n(b)\n(c)\nFigure 3\n:\n(a) Entropy evolution for holonomic and diff-drive agents under the four reward\nconfigurations. (b) Mean number of acquired targets over time with standard deviation\nbands. (c) Probability mass function of the number of targets acquired at the final\ntimestep. Colors denote reward schemes: R1 (blue), R2 (orange), R3 (green), and R4 (grey).\nFigure 4\n:\nPer-agent target discovery over time for the four reward configurations (R1â€“R4). Each subplot shows the mean discovery rate across simulations, with colored curves indicating the diff-drive agent (green), holonomic agent 1 (orange), and holonomic agent 2 (purple).\n3.2\nTraining results\nThe agent policies have been trained for 834 optimization steps in 600 parallel environments using MAPPO. Figure\n2(a)\nshows the\nevolution of the mean episode reward collected during training across the four reward configurations. Policies trained under R2 and R3 exhibit clear and\nstable convergence, following highly similar learning trajectories. In contrast, R1 remains close to zero throughout training, indicating that this reward\nconfiguration fails to provide a sufficiently informative learning signal. R4 achieves the highest episode rewards and shows evidence of convergence,\nalthough with noticeably higher variance. A similar behaviour is observed when analysing the mean episode reward per agent type, as shown in Figures\n2(b)\nand\n2(c)\n. For both R2 and R3, the differential-drive agent consistently achieves slightly higher rewards than the holonomic agents,\nwhereas under R4 the holonomic agents marginally outperform the differential-drive one. The close correspondence between the per-agent reward curves across all reward configurations suggests that the learned policies support effective collaborative task execution, without inducing a pronounced asymmetric\nworkload among the heterogeneous agents. Figure\n3(a)\nreports the entropy evolution of the learned policies, which provides insight into the explorationâ€“exploitation dynamics during training. Under R2, R3, and R4, entropy decays rapidly, indicating that the agents progressively acquire stable and consistent behaviours. Notably, for\nR2 and R3 the holonomic agents retain higher entropy for longer than the differential-drive agent, suggesting a degree of behavioural specialization.\nConversely, R4 yields nearly identical entropy profiles across agent types, indicating more symmetric role emergence. In contrast, R1 maintains high\nentropy throughout training, reflecting persistent policy randomness and the absence of meaningful behavioural convergence.\n3.3\nSimulation results\nThe trained policies have been further evaluated on 200 simulated environments,\neach executed for 100 timesteps, using scenarios consistent with those employed\nduring training. Figure\n3(b)\nreports the mean number of\ntargets acquired over time and provides an overall indication of task\nexecution efficiency. Policies trained with R1 exhibit minimal progress,\nconfirming their inability to explore the environment effectively and to\nlocate targets. In contrast, the policies trained with R2, R3, and R4\nsuccessfully acquire targets within the available time horizon. The steep\ninitial increase observed for the latter configurations indicates that the\nagents have learned meaningful acquisition behaviours. However, the\nconsiderable variance associated with R2 and R3 suggests reduced robustness\nand a stronger dependence on the specific simulation instance. These observations are corroborated in\nFigure\n3(c)\n, which shows the probability mass function of\nthe number of targets acquired at the final timestep. Policies obtained with R2,\nR3, and R4 typically manage to acquire one or two targets, whereas R1 fails to\nachieve comparable performance in most evaluation runs. To assess the distribution of workload across the heterogeneous agents, Figure\n4\npresents the per-agent target discovery dynamics.\nR4 yields policies in which the discovery trajectories of the diff-drive and\nholonomic agents are closely aligned, indicating a more balanced workload and\nthus more symmetric collaboration. Conversely, R2 and R3 lead to more\npronounced discrepancies between agents, revealing a degree of specialization\nand a less uniform allocation of the acquisition effort. Among these,\nR3 exhibits more stable behaviour than R2, as reflected by the lower variance\nin entropy during training and by the clearer role differentiation observed for\nthe two holonomic agents relative to the diff-drive agent. Across all evaluation metrics, policies trained under R1 demonstrate markedly\ninferior performance. They fail to develop reliable exploration or acquisition\nstrategies and consistently underperform relative to the other reward\nconfigurations.\n4\nConclusion\nThis work presents a decentralized multi-agent reinforcement learning framework for heterogeneous robotic teams for target acquisition in unknown environments, combining graph-based policies with safety filters to ensure collision-free coordination. The simulation results demonstrated stable learning, effective cooperation, and balanced workload distribution across agents under suitable reward shaping. Moreover, ablation studies confirmed the importance of goal-oriented, collision-avoidance, and communication-related reward terms. Future work will improve the performances in the target acquisition, investigate the efficacy of the framework with larger teams, and real-world deployment of the learned policies.\n{ack}\nThe computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) and Chalmers e-Science Centre at C3SE, partially funded by the Swedish Research Council through grant agreement no. 2022-06725. Computations were performed on the Alvis cluster.\nReferences\nM. Bettini, R. Kortvelesy, J. Blumenkamp, and A. Prorok (2022)\nVMAS: a vectorized multi-agent simulator for collective robot learning\n.\nExternal Links:\n2207.03530\n,\nLink\nCited by:\nÂ§3.1\n.\nM. Bettini, A. Prorok, and V. Moens (2024)\nBenchMARL: benchmarking multi-agent reinforcement learning\n.\nJournal of Machine Learning Research\n25\n(\n217\n),\npp.Â 1â€“10\n.\nCited by:\nÂ§3.1\n.\nS. Brody, U. Alon, and E. Yahav (2022)\nHow attentive are graph attention networks?\n.\nExternal Links:\n2105.14491\n,\nLink\nCited by:\nÂ§2.2\n.\nZ. Cui, W. Guan, S. Qu, T. Hu, S. Hao, and X. Zhang (2025)\nUAV-usv collaborative decision-making method based on multi-agent reinforcement learning\n.\nIFAC-PapersOnLine\n59\n(\n22\n),\npp.Â 639â€“644\n.\nNote:\n16th IFAC Conference on Control Applications in Marine Systems, Robotics and Vehicles CAMS 2025\nExternal Links:\nISSN 2405-8963\nCited by:\nÂ§1.1\n.\nK. Fan, Z. Chen, G. Ferrigno, and E. D. Momi (2024)\nLearn from safe experience: safe reinforcement learning for task automation of surgical robot\n.\nIEEE Transactions on Artificial Intelligence\n5\n(\n7\n),\npp.Â 3374â€“3383\n.\nCited by:\nÂ§1.1\n.\nK. Garg, J. Usevitch, J. Breeden, M. Black, D. Agrawal, H. Parwana, and D. Panagou (2024)\nAdvances in the theory of control barrier functions: addressing practical challenges in safe control synthesis for autonomous and robotic systems\n.\nAnnual Reviews in Control\n57\n,\npp.Â 100945\n.\nExternal Links:\nISSN 1367-5788\nCited by:\nÂ§1.1\n.\nS. Gronauer and K. Diepold (2022)\nMulti-agent deep reinforcement learning: a survey\n.\nArtificial Intelligence Review\n55\n(\n2\n),\npp.Â 895â€“943\n.\nExternal Links:\nISSN 1573-7462\nCited by:\nÂ§1.1\n.\nS. Gu, L. Yang, Y. Du, G. Chen, F. Walter, J. Wang, and A. Knoll (2024)\nA review of safe reinforcement learning: methods, theories, and applications\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n46\n(\n12\n),\npp.Â 11216â€“11235\n.\nCited by:\nÂ§1\n.\nY. Guo, J. Liu, R. Yu, P. Hang, and J. Sun (2024)\nMAPPO-pis: a multi-agent proximal policy optimization method with prior intent sharing for cavsâ€™ cooperative decision-making\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 244â€“263\n.\nCited by:\nÂ§1.1\n.\nS. Hu, L. Shen, Y. Zhang, and D. Tao (2024)\nLearning multi-agent communication from graph modeling perspective\n.\nExternal Links:\n2405.08550\n,\nLink\nCited by:\nÂ§1.1\n,\nÂ§1\n.\nD. Huh and P. Mohapatra (2024)\nMulti-agent reinforcement learning: a comprehensive survey\n.\nExternal Links:\n2312.10256\n,\nLink\nCited by:\nÂ§1.1\n.\nJ. Ji, J. Zhou, B. Zhang, J. Dai, X. Pan, R. Sun, W. Huang, Y. Geng, M. Liu, and Y. Yang (2024)\nOmnisafe: an infrastructure for accelerating safe reinforcement learning research\n.\nJournal of Machine Learning Research\n25\n(\n285\n),\npp.Â 1â€“6\n.\nCited by:\nÂ§1.1\n.\nY. Jing, B. Guo, N. Li, R. Xu, and Z. Yu (2025)\nFederated multi-agent reinforcement learning: a comprehensive survey of methods, applications and challenges\n.\nExpert Systems with Applications\n293\n,\npp.Â 128729\n.\nExternal Links:\nISSN 0957-4174\nCited by:\nÂ§1\n.\nJ. G. Kuba, R. Chen, M. Wen, Y. Wen, F. Sun, J. Wang, and Y. Yang (2022)\nTrust region policy optimisation in multi-agent reinforcement learning\n.\nExternal Links:\n2109.11251\n,\nLink\nCited by:\nÂ§1\n.\nZ. Liu, L. Wan, X. Sui, Z. Chen, K. Sun, and X. Lan (2023)\nDeep hierarchical communication graph in multi-agent reinforcement learning\n.\nIn\nProceedings of the Thirty-Second International Joint Conference on\nArtificial Intelligence, IJCAI-23\n,\nE. Elkind (Ed.)\n,\npp.Â 208â€“216\n.\nNote:\nMain Track\nCited by:\nÂ§1.1\n,\nÂ§1\n.\nY. Niu, R. R. Paleja, and M. C. Gombolay (2021)\nMulti-agent graph-attention communication and teaming.\n.\nIn\nAAMAS\n,\nVol.\n21\n,\npp.Â 20th\n.\nCited by:\nÂ§1.1\n,\nÂ§1\n.\nY. Okawa, H. Dan, N. Morita, and M. Ogawa (2023)\nMulti-agent reinforcement learning with information-sharing constrained policy optimization for global cost environment\n.\nIFAC-PapersOnLine\n56\n(\n2\n),\npp.Â 1558â€“1565\n.\nNote:\n22nd IFAC World Congress\nExternal Links:\nISSN 2405-8963\nCited by:\nÂ§1.1\n.\nF. S. Shahar, M. T. H. Sultan, M. Nowakowski, and A. Åukaszewicz (2025)\nUGV-uav integration advancements for coordinated missions: a review\n.\nJournal of Intelligent & Robotic Systems\n111\n(\n2\n),\npp.Â 69\n.\nExternal Links:\nISSN 1573-0409\nCited by:\nÂ§1\n.\nC. Sun, P. He, R. Wang, and C. Zheng (2025)\nRevisiting communication efficiency in multi-agent reinforcement learning from the dimensional analysis perspective\n.\nExternal Links:\n2501.02888\n,\nLink\nCited by:\nÂ§1.1\n.\nA. Wachi, X. Shen, and Y. Sui (2024)\nA survey of constraint formulations in safe reinforcement learning\n.\nExternal Links:\n2402.02025\n,\nLink\nCited by:\nÂ§1\n.\nY. Wang, M. Damani, P. Wang, Y. Cao, and G. Sartoretti (2022)\nDistributed reinforcement learning for robot teams: a review\n.\nCurrent Robotics Reports\n3\n(\n4\n),\npp.Â 239â€“257\n.\nExternal Links:\nISSN 2662-4087\nCited by:\nÂ§1\n.\nY. Yang, G. Chen, W. Wang, X. Hao, J. Hao, and P. Heng (2022)\nTransformer-based working memory for multiagent reinforcement learning with action parsing\n.\nIn\nAdvances in Neural Information Processing Systems\n,\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.)\n,\nVol.\n35\n,\npp.Â 34874â€“34886\n.\nCited by:\nÂ§1.1\n.\nC. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. WU (2022)\nThe surprising effectiveness of ppo in cooperative multi-agent games\n.\nIn\nAdvances in Neural Information Processing Systems\n,\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.)\n,\nVol.\n35\n,\npp.Â 24611â€“24624\n.\nCited by:\nÂ§1.1\n,\nÂ§1\n.\nM. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. Salakhutdinov, and A. Smola (2018)\nDeep sets\n.\nExternal Links:\n1703.06114\n,\nLink\nCited by:\nÂ§2.2\n.\nS. Zhan, Y. Wang, Q. Wu, R. Jiao, C. Huang, and Q. Zhu (2024)\nState-wise safe reinforcement learning with pixel observations\n.\nIn\n6th Annual Learning for Dynamics & Control Conference\n,\npp.Â 1187â€“1201\n.\nCited by:\nÂ§1.1\n.\nL. Zhang, L. Li, W. Wei, H. Song, Y. Yang, and J. Liang (2024)\nScalable constrained policy optimization for safe multi-agent reinforcement learning\n.\nIn\nAdvances in Neural Information Processing Systems\n,\nA. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (Eds.)\n,\nVol.\n37\n,\npp.Â 138698â€“138730\n.\nExternal Links:\nDocument\nCited by:\nÂ§1.1\n,\nÂ§1.1\n.\nY. Zhong, J. G. Kuba, X. Feng, S. Hu, J. Ji, and Y. Yang (2024)\nHeterogeneous-agent reinforcement learning\n.\nJournal of Machine Learning Research\n25\n(\n32\n),\npp.Â 1â€“67\n.\nCited by:\nÂ§1\n.",
    "preview_text": "This paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments characterized by partial observability, communication constraints, and dynamic interactions. Each agent's policy is trained with the Multi-Agent Proximal Policy Optimization algorithm and employs a Graph Attention Network encoder that integrates simulated range-sensing data with communication embeddings exchanged among neighboring agents, enabling context-aware decision-making from both local sensing and relational information. In particular, this work introduces a unified framework that integrates graph-based communication and trajectory-aware safety through safety filters. The architecture is supported by a structured reward formulation designed to encourage effective target discovery and acquisition, collision avoidance, and de-correlation between the agents' communication vectors by promoting informational orthogonality. The effectiveness of the proposed reward function is demonstrated through a comprehensive ablation study. Moreover, simulation results demonstrate safe and stable task execution, confirming the framework's effectiveness.\n\nSafe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition\nGabriele Calzolari\nVidya Sumathy\nChristoforos Kanellakis\nGeorge Nikolakopoulos\nDepartment of Computer Science, Electrical and Space Engineering, LuleÃ¥ University of Technology, LuleÃ¥, Sweden.\nAbstract\nThis paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments characterized by partial observability, communication constraints, and dynamic interactions. Each agentâ€™s policy is trained with the Multi-Agent Proximal Policy Optimization algorithm and employs a Graph Attention Network",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¼‚æ„å›¢é˜Ÿåœ¨éƒ¨åˆ†å¯è§‚æµ‹ã€é€šä¿¡å—é™ç¯å¢ƒä¸­ååŒå‘ç°å’Œè·å–ç›®æ ‡ï¼Œå¼ºè°ƒå®‰å…¨æ€§å’Œé€šä¿¡æ­£åˆ™åŒ–ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nå¼‚æ„å¤šæœºå™¨äººç³»ç»Ÿï¼ˆå¦‚æ— äººæœºä¸æ— äººåœ°é¢è½¦è¾†ååŒï¼‰åœ¨æœç´¢æ•‘æ´ç­‰ä»»åŠ¡ä¸­ä¼˜åŠ¿äº’è¡¥ï¼Œä½†å…¶åœ¨éƒ¨åˆ†å¯è§‚æµ‹ã€é€šä¿¡å—é™ç¯å¢ƒä¸‹çš„å®‰å…¨åè°ƒé¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¸¸å¿½è§†ä¸¥æ ¼çš„å®‰å…¨ä¿è¯ä¸é€šä¿¡æ•ˆç‡ï¼Œé™åˆ¶äº†å…¶åœ¨ç‰©ç†ç³»ç»Ÿä¸­çš„å®é™…éƒ¨ç½²ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡ºä¸€ä¸ª**å»ä¸­å¿ƒåŒ–å¼‚æ„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶**ï¼Œç”¨äºåè°ƒç›®æ ‡æœç´¢ä¸è·å–ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š\n- **ç®—æ³•ä¸æ¶æ„**ï¼šé‡‡ç”¨åŸºäº**å¤šæ™ºèƒ½ä½“è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–**çš„é›†ä¸­è®­ç»ƒåˆ†æ•£æ‰§è¡ŒèŒƒå¼ã€‚æ¯ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥ç½‘ç»œä½¿ç”¨**å›¾æ³¨æ„åŠ›ç½‘ç»œ**ç¼–ç å™¨ï¼Œæ•´åˆå±€éƒ¨æ„ŸçŸ¥ä¸é‚»åŸŸé€šä¿¡ä¿¡æ¯ã€‚\n- **å®‰å…¨ä¿è¯**ï¼šä¸ºæ¯ç§è¿åŠ¨æ¨¡å‹ï¼ˆå®Œæ•´çº¦æŸä¸å·®é€Ÿé©±åŠ¨ï¼‰è®¾è®¡äº†**è½¨è¿¹æ„ŸçŸ¥å®‰å…¨è¿‡æ»¤å™¨**ã€‚å®ƒé€šè¿‡é¢„æµ‹åŠ¨ä½œå¼•å‘çš„æœªæ¥çŠ¶æ€ï¼Œå¹¶åœ¨æ£€æµ‹åˆ°ç¢°æ’é£é™©æ—¶ç¼©æ”¾æ§åˆ¶è¾“å…¥ï¼Œç¡®ä¿å®æ—¶é¿éšœã€‚\n- **é€šä¿¡ä¼˜åŒ–**ï¼šåœ¨å¥–åŠ±å‡½æ•°ä¸­å¼•å…¥**é€šä¿¡æ­£äº¤æ€§æ­£åˆ™åŒ–é¡¹**ï¼Œæƒ©ç½šæ™ºèƒ½ä½“é—´é€šä¿¡å‘é‡çš„ç›¸å…³æ€§ï¼Œä»¥ä¿ƒè¿›ä¿¡æ¯å¤šæ ·æ€§ï¼Œå‡å°‘å†—ä½™ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\nåœ¨VMASä»¿çœŸç¯å¢ƒä¸­ï¼Œå¯¹åŒ…å«ä¸åŒå¥–åŠ±ç»„åˆ†çš„å››ç§é…ç½®è¿›è¡Œäº†æ¶ˆèç ”ç©¶ã€‚ç»“æœè¡¨æ˜ï¼š\n- åŒ…å«ç›®æ ‡åˆ°è¾¾ã€é¿ç¢°åŠé€šä¿¡å¤šæ ·æ€§å¥–åŠ±çš„å®Œæ•´é…ç½®ï¼ˆR4ï¼‰å–å¾—äº†**æœ€é«˜çš„å¹³å‡å›åˆå¥–åŠ±**å’Œ**æœ€ç¨³å®šçš„æ”¶æ•›æ€§**ã€‚\n- å®‰å…¨è¿‡æ»¤å™¨æœ‰æ•ˆé¿å…äº†ç¢°æ’ã€‚ä¸åŸºçº¿é…ç½®ç›¸æ¯”ï¼Œå®Œæ•´å¥–åŠ±æ–¹æ¡ˆå¼•å¯¼çš„ç­–ç•¥å®ç°äº†**æ›´å‡è¡¡çš„æ™ºèƒ½ä½“é—´å·¥ä½œé‡åˆ†é…**å’Œ**æ›´é«˜çš„ä»»åŠ¡å®Œæˆæ•ˆç‡**ã€‚\n- ä»…ä½¿ç”¨è·ç¦»å¥–åŠ±çš„ç®€å•é…ç½®ï¼ˆR1ï¼‰æ— æ³•å¼•å¯¼å‡ºæœ‰æ•ˆçš„åä½œç­–ç•¥ï¼ŒéªŒè¯äº†ç»“æ„åŒ–å¥–åŠ±è®¾è®¡çš„å¿…è¦æ€§ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\næœ¬å·¥ä½œå°†**å›¾ç¥ç»ç½‘ç»œé€šä¿¡å­¦ä¹ ã€å®æ—¶å®‰å…¨è¿‡æ»¤ä¸å¼‚æ„MARL**ç›¸ç»“åˆï¼Œä¸ºåœ¨å¤æ‚ã€å®‰å…¨å…³é”®ç¯å¢ƒä¸­éƒ¨ç½²å¼‚æ„æœºå™¨äººå›¢é˜Ÿæä¾›äº†ä¸€ç§**ç»Ÿä¸€ä¸”å®ç”¨çš„æ–¹æ³•**ã€‚å…¶å¥–åŠ±å‡½æ•°è®¾è®¡ä¸æ¶ˆèåˆ†æä¸ºå¤šæ™ºèƒ½ä½“åè°ƒä¸­çš„å®‰å…¨ã€æ•ˆç‡ä¸é€šä¿¡æƒè¡¡æä¾›äº†é‡è¦è§è§£ï¼Œæ¨åŠ¨äº†å®‰å…¨MARLå‘å®é™…åº”ç”¨è¿ˆè¿›ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **ç ”ç©¶é—®é¢˜**\næœ¬æ–‡æ—¨åœ¨è§£å†³**å¼‚æ„å¤šæœºå™¨äººç³»ç»Ÿï¼ˆå¦‚æ— äººæœºUAVä¸æ— äººè½¦UGVæ··åˆç¼–é˜Ÿï¼‰åœ¨éƒ¨åˆ†å¯è§‚æµ‹ã€é€šä¿¡å—é™çš„åŠ¨æ€ç¯å¢ƒä¸­ï¼Œè¿›è¡ŒååŒç›®æ ‡æœç´¢ä¸è·å–ä»»åŠ¡æ—¶ï¼Œæ‰€é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜**ï¼š\n1.  **å¼‚æ„æ€§åè°ƒ**ï¼šä¸åŒè¿åŠ¨å­¦æ¨¡å‹ï¼ˆå…¨å‘ç§»åŠ¨UAV vs. å·®é€Ÿé©±åŠ¨UGVï¼‰çš„æ™ºèƒ½ä½“å¦‚ä½•æœ‰æ•ˆååŒã€‚\n2.  **å®‰å…¨ä¿è¯**ï¼šåœ¨å»ä¸­å¿ƒåŒ–å†³ç­–è¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•ç¡®ä¿å®æ—¶ã€æ— ç¢°æ’çš„è½¨è¿¹å®‰å…¨ã€‚\n3.  **é€šä¿¡æ•ˆç‡**ï¼šåœ¨æœ‰é™é€šä¿¡å¸¦å®½ä¸‹ï¼Œå¦‚ä½•é¿å…é€šä¿¡å†—ä½™ï¼Œæå‡ä¿¡æ¯å…±äº«çš„æœ‰æ•ˆæ€§ã€‚\n4.  **éƒ¨åˆ†å¯è§‚æµ‹æ€§**ï¼šå•ä¸ªæ™ºèƒ½ä½“ä»…èƒ½é€šè¿‡å±€éƒ¨ä¼ æ„Ÿå™¨æ„ŸçŸ¥ç¯å¢ƒï¼Œéœ€é€šè¿‡é€šä¿¡å¼¥è¡¥ä¿¡æ¯ç¼ºå£ã€‚\n\n### **æ ¸å¿ƒåˆ›æ–°ç‚¹**\næœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ã€å®‰å…¨çš„ã€å»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå…¶åˆ›æ–°æ€§ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹ä¸‰ä¸ªæ–¹é¢çš„**é›†æˆä¸ä¼˜åŒ–**ï¼š\n\n1.  **é›†æˆå®‰å…¨è¿‡æ»¤å™¨çš„å»ä¸­å¿ƒåŒ–å¼‚æ„MARLæ¶æ„**\n    *   **æŠ€æœ¯åˆ›æ–°**ï¼šå°†**åŸºäºè½¨è¿¹é¢„æµ‹çš„å®‰å…¨è¿‡æ»¤å™¨**ç›´æ¥åµŒå…¥åˆ°MAPPOè®­ç»ƒä¸æ‰§è¡Œå¾ªç¯ä¸­ã€‚è¯¥è¿‡æ»¤å™¨å¹¶éç®€å•æƒ©ç½šç¢°æ’ï¼Œè€Œæ˜¯åœ¨æ¯ä¸ªæ—¶é—´æ­¥å¯¹ç­–ç•¥è¾“å‡ºçš„åŠ¨ä½œè¿›è¡Œ**å‰å‘æ¨¡æ‹Ÿå’Œå¯è¡Œæ€§éªŒè¯**ï¼Œå¹¶å°†ä¸å®‰å…¨åŠ¨ä½œå®æ—¶ç¼©æ”¾è‡³æœ€è¿‘çš„å®‰å…¨åŠ¨ä½œã€‚\n    *   **å®é™…ä»·å€¼**ï¼šä¸ºç‰©ç†æœºå™¨äººç³»ç»Ÿæä¾›äº†**å®æ—¶ã€å¯è¯æ˜çš„å®‰å…¨ä¿è¯**ï¼Œä½¿å­¦å¾—çš„ç­–ç•¥åœ¨éƒ¨ç½²æ—¶èƒ½ä¸¥æ ¼éµå®ˆé˜²ç¢°æ’çº¦æŸï¼Œå¼¥åˆäº†ä»¿çœŸå­¦ä¹ ä¸ç°å®éƒ¨ç½²ä¹‹é—´çš„å®‰å…¨é¸¿æ²Ÿã€‚\n\n2.  **åŸºäºå›¾æ³¨æ„åŠ›ç½‘ç»œä¸é€šä¿¡æ­£äº¤æ€§æ­£åˆ™åŒ–çš„ååŒæ„ŸçŸ¥**\n    *   **æŠ€æœ¯åˆ›æ–°**ï¼š\n        *   é‡‡ç”¨**GATv2Conv**ä½œä¸ºç¼–ç å™¨ï¼ŒåŠ¨æ€æ„å»ºåŸºäºè·ç¦»çš„é€šä¿¡å›¾ï¼Œå®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é‚»åŸŸä¿¡æ¯èšåˆã€‚\n        *   åœ¨å¥–åŠ±å‡½æ•°ä¸­å¼•å…¥**é€šä¿¡æ­£äº¤æ€§æ­£åˆ™åŒ–é¡¹**ï¼Œæ˜¾å¼æƒ©ç½šæ™ºèƒ½ä½“é—´é€šä¿¡å‘é‡çš„ç›¸å…³æ€§ï¼ˆè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çš„å¹³æ–¹ï¼‰ã€‚\n    *   **å®é™…ä»·å€¼**ï¼š\n        *   GATä½¿æ™ºèƒ½ä½“èƒ½è‡ªé€‚åº”åœ°å…³æ³¨é‡è¦é‚»å±…çš„ä¿¡æ¯ã€‚\n        *   é€šä¿¡æ­£åˆ™åŒ–é¼“åŠ±ä¿¡æ¯å¤šæ ·æ€§ï¼Œå‡å°‘äº†é€šä¿¡ä¿¡é“ä¸­çš„å†—ä½™å’Œå¹²æ‰°ï¼Œæå‡äº†æœ‰é™å¸¦å®½ä¸‹çš„ååŒæ•ˆç‡ï¼Œå¹¶é€šè¿‡æ¶ˆèå®éªŒè¯å®äº†å…¶æœ‰æ•ˆæ€§ã€‚\n\n3.  **é’ˆå¯¹å¼‚æ„ååŒç›®æ ‡è·å–çš„ç»“æ„åŒ–å¥–åŠ±å‡½æ•°è®¾è®¡**\n    *   **æŠ€æœ¯åˆ›æ–°**ï¼šè®¾è®¡äº†ä¸€ä¸ªåŒ…å«å››ä¸ªå¯æ¶ˆèç»„ä»¶çš„å¤åˆå¥–åŠ±å‡½æ•°ï¼š\n        ```python\n        å¥–åŠ± = w1 * è·ç¦»å¥–åŠ± + w2 * ç›®æ ‡è¾¾æˆå¥–åŠ± + w3 * ç¢°æ’æƒ©ç½š + w4 * é€šä¿¡å¤šæ ·æ€§å¥–åŠ±\n        ```\n        å…¶ä¸­â€œè·ç¦»å¥–åŠ±â€é¼“åŠ±æ¥è¿‘ç›®æ ‡ï¼Œâ€œé€šä¿¡å¤šæ ·æ€§å¥–åŠ±â€å³ä¸Šè¿°æ­£äº¤æ€§å¥–åŠ±ã€‚\n    *   **å®é™…ä»·å€¼**ï¼šé€šè¿‡ç³»ç»Ÿçš„æ¶ˆèç ”ç©¶ï¼Œé‡åŒ–äº†æ¯ä¸ªå¥–åŠ±ç»„ä»¶ï¼ˆç›®æ ‡å¯¼å‘ã€å®‰å…¨ã€é€šä¿¡å¤šæ ·æ€§ï¼‰å¯¹æœ€ç»ˆä»»åŠ¡æ€§èƒ½ï¼ˆç›®æ ‡è·å–ç‡ã€æ¢ç´¢æ•ˆç‡ã€å·¥ä½œé‡å¹³è¡¡ï¼‰çš„è´¡çŒ®ï¼Œä¸ºå¤æ‚å¼‚æ„MARLä»»åŠ¡çš„å¥–åŠ±å·¥ç¨‹æä¾›äº†å¯å€Ÿé‰´çš„èŒƒå¼ã€‚\n\n### **è§£å†³æ–¹æ¡ˆæ€»è§ˆ**\nè®ºæ–‡é€šè¿‡ä»¥ä¸‹**æ–¹æ³•é“¾**è§£å†³ä¸Šè¿°é—®é¢˜ï¼š\n1.  **ç®—æ³•åŸºç¡€**ï¼šé‡‡ç”¨ **MAPPO** æ¡†æ¶ï¼Œéµå¾ªâ€œé›†ä¸­è®­ç»ƒï¼Œåˆ†æ•£æ‰§è¡Œâ€èŒƒå¼ï¼Œä¿è¯è®­ç»ƒç¨³å®šæ€§ä¸æ‰§è¡Œç‹¬ç«‹æ€§ã€‚\n2.  **æ„ŸçŸ¥ä¸é€šä¿¡**ï¼šæ™ºèƒ½ä½“å°†**å±€éƒ¨æ¿€å…‰é›·è¾¾è§‚æµ‹**ä¸**æ¥æ”¶åˆ°çš„é‚»å±…é€šä¿¡å‘é‡**ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡**GNNç¼–ç å™¨**è¿›è¡Œèåˆï¼Œæå–åŒ…å«ç©ºé—´å…³ç³»çš„ç‰¹å¾ã€‚\n3.  **å†³ç­–ä¸å®‰å…¨**ï¼šç‰¹å¾é€å…¥MLPç­–ç•¥ç½‘ç»œè¾“å‡ºåŸå§‹åŠ¨ä½œï¼ˆæ§åˆ¶æŒ‡ä»¤+é€šä¿¡å‘é‡ï¼‰ã€‚**å®‰å…¨è¿‡æ»¤å™¨**å¯¹æ­¤åŠ¨ä½œè¿›è¡Œå‰å‘è½¨è¿¹æ¨¡æ‹Ÿï¼Œæ£€æŸ¥ä¸æ‰€æœ‰é‚»å±…ä¿å®ˆå¯è¾¾é›†çš„å†²çªï¼Œå¹¶æŒ‰è¦æ±‚ç¼©æ”¾åŠ¨ä½œï¼Œç¡®ä¿å®‰å…¨åæ‰æ‰§è¡Œã€‚\n4.  **ä¼˜åŒ–ç›®æ ‡**ï¼šä½¿ç”¨é›†æˆäº†**é€šä¿¡æ­£äº¤æ€§æ­£åˆ™åŒ–**çš„ç»“æ„åŒ–å¥–åŠ±å‡½æ•°æ¥è®­ç»ƒç­–ç•¥ï¼Œé¼“åŠ±é«˜æ•ˆã€å®‰å…¨ã€ååŒçš„è¡Œä¸ºã€‚\n\n### **æ€»ç»“**\næœ¬æ–‡çš„æ ¸å¿ƒè´¡çŒ®åœ¨äº**å°†è½¨è¿¹çº§å®‰å…¨ä¿è¯ä¸ä¿ƒè¿›é€šä¿¡æ•ˆç‡çš„å­¦ä¹ æœºåˆ¶ï¼Œæ·±åº¦èåˆåˆ°ä¸€ä¸ªå»ä¸­å¿ƒåŒ–çš„å¼‚æ„MARLæ¡†æ¶ä¸­**ã€‚å®ƒä¸ä»…æå‡ºäº†æ–°çš„æŠ€æœ¯ç»„ä»¶ï¼ˆå®‰å…¨è¿‡æ»¤å™¨ã€é€šä¿¡æ­£åˆ™åŒ–ï¼‰ï¼Œæ›´é‡è¦çš„æ˜¯é€šè¿‡**ç³»ç»Ÿçš„æ¶æ„é›†æˆå’Œå®éªŒåˆ†æ**ï¼Œå±•ç¤ºäº†å¦‚ä½•åœ¨ä¿è¯å®‰å…¨è¿™ä¸€ç¡¬çº¦æŸçš„å‰æä¸‹ï¼Œå®ç°å¼‚æ„å¤šæ™ºèƒ½ä½“åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„é«˜æ•ˆååŒå­¦ä¹ ï¼Œå‘ç°å®ä¸–ç•Œéƒ¨ç½²è¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ã€‚",
            "timestamp": "2026-01-20T17:50:57.616808",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡å‘å±•è„‰ç»œæ¢³ç†\n\næœ¬æ–‡ã€ŠSafe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisitionã€‹æ˜¯**å¼‚æ„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰**é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯é¢å‘**å®‰å…¨ååŒç›®æ ‡è·å–**ä»»åŠ¡çš„æœ€æ–°ç ”ç©¶æˆæœã€‚å…¶å‘å±•è„‰ç»œå¯ä»¥æ¢³ç†ä¸ºå‡ ä¸ªå…³é”®æ–¹å‘ï¼Œæ¯ä¸€æ­¥éƒ½åœ¨è§£å†³å‰ä½œçš„å±€é™æ€§ã€‚\n\n### 1. æ ¸å¿ƒé—®é¢˜æ¼”è¿›ï¼šä»åŒè´¨åè°ƒåˆ°å¼‚æ„å®‰å…¨ååŒ\næ—©æœŸçš„MARLç ”ç©¶ï¼ˆå¦‚Yu et al., 2022çš„MAPPOï¼‰ä¸»è¦å…³æ³¨**åŒè´¨æ™ºèƒ½ä½“**åœ¨å®Œå…¨å¯è§‚æµ‹æˆ–ç®€å•é€šä¿¡ä¸‹çš„åˆä½œï¼Œ**å¿½è§†äº†æ™ºèƒ½ä½“ç»“æ„å·®å¼‚ï¼ˆå¦‚UAV/UGVåŠ¨åŠ›å­¦ä¸åŒï¼‰å’Œç¡¬æ€§å®‰å…¨çº¦æŸ**ã€‚æœ¬æ–‡çš„â€œå‰ä½œâ€è„‰ç»œæ­£æ˜¯å›´ç»•è§£å†³è¿™ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜å±•å¼€ã€‚\n\n### 2. ä¸»è¦å‘å±•æ–¹å‘åŠæ”¹è¿›\n\n#### å‘å±•æ–¹å‘ä¸€ï¼š**å¤„ç†å¼‚æ„æ€§ä¸å¯æ‰©å±•æ€§**\n*   **æ—©æœŸå·¥ä½œ (å¦‚ Yu et al., 2022; Kuba et al., 2022)**ï¼šæå‡ºäº†MAPPOã€HAPPOç­‰ç®—æ³•ï¼Œå¥ å®šäº†CTDEï¼ˆé›†ä¸­è®­ç»ƒåˆ†æ•£æ‰§è¡Œï¼‰èŒƒå¼ï¼Œè¯æ˜äº†åœ¨å¤æ‚ä»»åŠ¡ä¸­å­¦ä¹ åˆ†æ•£ç­–ç•¥çš„å¯è¡Œæ€§ï¼Œä½†ä¸»è¦é’ˆå¯¹åŒè´¨æˆ–ç®€å•å¼‚æ„æ™ºèƒ½ä½“ã€‚\n*   **æ”¹è¿›å·¥ä½œ (å¦‚ Zhong et al., 2024)**ï¼šæå‡ºäº†**HATRLï¼ˆå¼‚æ„æ™ºèƒ½ä½“ä¿¡ä»»åŸŸå­¦ä¹ ï¼‰**ï¼Œä¸“é—¨é’ˆå¯¹å¼‚æ„æ™ºèƒ½ä½“ä¼˜åŒ–ç­–ç•¥æ›´æ–°ï¼Œæé«˜äº†è®­ç»ƒç¨³å®šæ€§ã€‚è¿™æ˜¯å¯¹åŒè´¨ç®—æ³•æ¡†æ¶çš„é‡è¦æ³›åŒ–ã€‚\n*   **æœ¬æ–‡çš„æ”¹è¿›**ï¼š\n    *   **ç»Ÿä¸€æ¡†æ¶**ï¼šæ˜ç¡®å°†**è¿åŠ¨å­¦æˆªç„¶ä¸åŒçš„æ™ºèƒ½ä½“ï¼ˆå…¨å‘ç§»åŠ¨çš„UAVå’Œå·®é€Ÿé©±åŠ¨çš„UGVï¼‰** çº³å…¥åŒä¸€å­¦ä¹ æ¡†æ¶ï¼Œå…¶ç­–ç•¥ç½‘ç»œå…±äº«ç¼–ç å™¨ä½†æ‹¥æœ‰**ç‹¬ç«‹çš„æ¡ä»¶ç­–ç•¥å¤´**ï¼Œä»¥é€‚é…ä¸åŒçš„åŠ¨ä½œç©ºé—´ã€‚\n    *   **å¯æ‰©å±•é€šä¿¡**ï¼šé‡‡ç”¨åŸºäº**å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰** çš„ç¼–ç å™¨ï¼Œæ ¹æ®æ™ºèƒ½ä½“é—´è·ç¦»åŠ¨æ€æ„å»ºé€šä¿¡å›¾ï¼Œä½¿æ¡†æ¶èƒ½é€‚åº”å›¢é˜Ÿè§„æ¨¡çš„å˜åŒ–ï¼Œè¿™æ˜¯å¯¹å›ºå®šé€šä¿¡æ‹“æ‰‘æˆ–å®Œå…¨è¿æ¥é€šä¿¡çš„æ”¹è¿›ã€‚\n\n#### å‘å±•æ–¹å‘äºŒï¼š**é›†æˆå®‰å…¨ä¿è¯**\n*   **æ—©æœŸMARLå·¥ä½œ**ï¼šå¤§å¤šä¸“æ³¨äºä»»åŠ¡æ€§èƒ½ï¼ˆå¦‚å¥–åŠ±æœ€å¤§åŒ–ï¼‰ï¼Œ**ç¼ºä¹å¯¹ç¢°æ’ç­‰å®‰å…¨çº¦æŸçš„æ˜¾å¼ã€å¯è¯æ˜çš„ä¿è¯**ï¼Œé™åˆ¶äº†åœ¨ç‰©ç†æœºå™¨äººä¸­çš„éƒ¨ç½²ã€‚\n*   **å¹¶è¡Œå®‰å…¨RLè¿›å±• (å¦‚ Gu et al., 2024; Wachi et al., 2024)**ï¼šåœ¨å•æ™ºèƒ½ä½“é¢†åŸŸå»ºç«‹äº†çº¦æŸç­–ç•¥ä¼˜åŒ–çš„ç†è®ºåŸºç¡€ï¼Œä½†æœªä¸å¤šæ™ºèƒ½ä½“ã€ç‰¹åˆ«æ˜¯å¼‚æ„å¤šæ™ºèƒ½ä½“åœºæ™¯æ·±åº¦ç»“åˆã€‚\n*   **è¿‘æœŸå®‰å…¨MARLæ¢ç´¢ (å¦‚ Zhang et al., 2024çš„Scal-MAPPO-L; Okawa et al., 2023çš„IsCPO)**ï¼šå¼€å§‹å°†æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ç­‰çº¦æŸä¼˜åŒ–æ–¹æ³•å¼•å…¥MARLï¼Œæä¾›äº†ç­–ç•¥å±‚é¢çš„å®‰å…¨çº¦æŸã€‚ä½†è¿™ç±»æ–¹æ³•é€šå¸¸åœ¨**å­¦ä¹ è¿‡ç¨‹ä¸­**é€šè¿‡æƒ©ç½šæ¥æ»¡è¶³çº¦æŸï¼Œä¸ä¿è¯**æ‰§è¡Œæ—¶**æ¯ä¸€æ—¶åˆ»çš„ç»å¯¹å®‰å…¨ã€‚\n*   **æœ¬æ–‡çš„æ”¹è¿›**ï¼š\n    *   **å¼•å…¥è½¨è¿¹æ„ŸçŸ¥å®‰å…¨æ»¤æ³¢å™¨**ï¼šè¿™æ˜¯å…³é”®åˆ›æ–°ã€‚å®ƒåœ¨**ç­–ç•¥ç½‘ç»œè¾“å‡ºåŠ¨ä½œåã€ç¯å¢ƒæ‰§è¡Œå‰**ä»‹å…¥ï¼Œå¯¹æè®®çš„åŠ¨ä½œè¿›è¡Œå‰å‘æ¨¡æ‹Ÿï¼Œé¢„æµ‹çŸ­æœŸè½¨è¿¹ï¼Œå¹¶**å®æ—¶ç¼©æ”¾ä¸å®‰å…¨åŠ¨ä½œ**ã€‚è¿™æä¾›äº†**ä½å±‚çº§ã€æ¨¡å‹è¾…åŠ©çš„å³æ—¶å®‰å…¨ä¿è¯**ï¼Œä¸åŸºäºå­¦ä¹ /æƒ©ç½šçš„é«˜å±‚çº¦æŸå½¢æˆäº’è¡¥ã€‚\n    *   **å¼‚æ„å®‰å…¨å»ºæ¨¡**ï¼šæ»¤æ³¢å™¨é’ˆå¯¹UAVï¼ˆä½¿ç”¨çŸ©å½¢å¯è¾¾é›†ï¼‰å’ŒUGVï¼ˆä½¿ç”¨åœ†å½¢ä¿å®ˆè¿‘ä¼¼ï¼‰çš„ä¸åŒåŠ¨åŠ›å­¦æ¨¡å‹åˆ†åˆ«è®¾è®¡ï¼Œå®ç°äº†**å¯¹å¼‚æ„å¹³å°çš„å®‰å…¨ç»Ÿä¸€ç®¡ç†**ã€‚\n\n#### å‘å±•æ–¹å‘ä¸‰ï¼š**ä¼˜åŒ–é€šä¿¡æ•ˆç‡ä¸æœ‰æ•ˆæ€§**\n*   **æ—©æœŸé€šä¿¡å­¦ä¹ **ï¼šæ™ºèƒ½ä½“é€šå¸¸å¹¿æ’­åŸå§‹æˆ–ç®€å•ç¼–ç çš„ä¿¡æ¯ï¼Œå¯¼è‡´ä¿¡é“å†—ä½™å’Œå¹²æ‰°ã€‚\n*   **å›¾é€šä¿¡å­¦ä¹  (å¦‚ Niu et al., 2021çš„MAGIC; Liu et al., 2023çš„DHCG)**ï¼šåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå­¦ä¹ â€œä¸è°é€šä¿¡â€å’Œâ€œé€šä¿¡ä»€ä¹ˆâ€ï¼Œå®ç°äº†è‡ªé€‚åº”ã€é«˜æ•ˆçš„é€šä¿¡æ‹“æ‰‘ã€‚\n*   **é€šä¿¡å»å†—ä½™ç ”ç©¶ (å¦‚ Sun et al., 2025)**ï¼šä»ç†è®ºä¸Šåˆ†æäº†é€šä¿¡åµŒå…¥çš„å†—ä½™é—®é¢˜ï¼Œå¹¶æå‡ºé€šè¿‡å»ç›¸å…³æå‡æ•ˆç‡ã€‚\n*   **æœ¬æ–‡çš„æ”¹è¿›**ï¼š\n    *   **æå‡ºé€šä¿¡æ­£äº¤æ€§æ­£åˆ™åŒ–**ï¼šå°†å…¶ä½œä¸º**å¥–åŠ±å‡½æ•°çš„ä¸€éƒ¨åˆ†**ï¼ˆ`r_i^comm`ï¼‰ï¼Œç›´æ¥æƒ©ç½šæ™ºèƒ½ä½“é—´é€šä¿¡å‘é‡çš„é«˜ç›¸å…³æ€§ï¼ˆé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦å¹³æ–¹ï¼‰ã€‚è¿™é¼“åŠ±æ™ºèƒ½ä½“äº§ç”Ÿ**ä¿¡æ¯äº’è¡¥**çš„æ¶ˆæ¯ï¼Œå‡å°‘äº†ä¿¡é“ä¸­çš„å†—ä½™ä¿¡æ¯ï¼Œä»**å­¦ä¹ ç›®æ ‡å±‚é¢**ä¿ƒè¿›äº†é«˜æ•ˆåä½œï¼Œæ˜¯å¯¹å•çº¯æ”¹è¿›é€šä¿¡æ¶æ„çš„è¡¥å……ã€‚\n\n#### å‘å±•æ–¹å‘å››ï¼š**è®¾è®¡ç»“æ„åŒ–å¥–åŠ±å‡½æ•°**\n*   **å¸¸è§åšæ³•**ï¼šä½¿ç”¨ç¨€ç–çš„æœ€ç»ˆä»»åŠ¡å¥–åŠ±æˆ–ç®€å•çš„ç¨ å¯†å¥–åŠ±ï¼ˆå¦‚è·ç¦»å‡å°‘ï¼‰ï¼Œå­¦ä¹ ä¿¡å·å¼±ï¼Œæ”¶æ•›æ…¢ã€‚\n*   **è¿‘æœŸè¿›å±• (å¦‚ Cui et al., 2025)**ï¼šä¸ºç›®æ ‡é©±åŠ¨çš„å¼‚æ„ååŒä»»åŠ¡è®¾è®¡å¤šçº¦æŸå¥–åŠ±å‡½æ•°ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚\n*   **æœ¬æ–‡çš„æ”¹è¿›**ï¼š\n    *   **ç³»ç»ŸåŒ–å¥–åŠ±æ¶ˆèç ”ç©¶**ï¼šæ˜ç¡®å°†å¥–åŠ±åˆ†è§£ä¸º**è·ç¦»å¥–åŠ±ã€ç›®æ ‡è¾¾æˆå¥–åŠ±ã€ç¢°æ’æƒ©ç½šã€é€šä¿¡å¤šæ ·æ€§å¥–åŠ±**å››ä¸ªç»„ä»¶ï¼Œå¹¶é€šè¿‡æ¸è¿›å¼å®éªŒï¼ˆR1->R4ï¼‰é‡åŒ–äº†æ¯ä¸ªç»„ä»¶å¯¹æœ€ç»ˆæ€§èƒ½ï¼ˆå­¦ä¹ ç¨³å®šæ€§ã€ç›®æ ‡è·å–æ•ˆç‡ã€å·¥ä½œé‡å¹³è¡¡ï¼‰çš„è´¡çŒ®ã€‚è¿™ä¸ºå¤æ‚MARLä»»åŠ¡çš„å¥–åŠ±å·¥ç¨‹æä¾›äº†å¯è§£é‡Šçš„èŒƒä¾‹ã€‚\n\n### æ€»ç»“è„‰ç»œå›¾\n```\nåŒè´¨MARL (å¦‚MAPPO)\n        â†“ (è§£å†³å¼‚æ„æ€§)\nå¼‚æ„MARLç®—æ³• (å¦‚HATRL)\n        â†“ (é›†æˆå®‰å…¨ä¿è¯)\nå®‰å…¨MARL/çº¦æŸä¼˜åŒ– (å¦‚Scal-MAPPO-L)\n        â†“ (ä¼˜åŒ–é€šä¿¡)\nå›¾é€šä¿¡ä¸å»å†—ä½™å­¦ä¹  (å¦‚MAGIC, Sun et al.)\n        |\n        | (æœ¬æ–‡å·¥ä½œï¼šç»Ÿä¸€ä¸é›†æˆ)\n        â†“\n**Safe Heterogeneous MARL with Communication Regularization**\n**æ ¸å¿ƒæ•´åˆä¸åˆ›æ–°**ï¼š\n1. **å¼‚æ„ç»Ÿä¸€**ï¼šåœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹å¤„ç†UAV/UGVã€‚\n2. **å®‰å…¨å‡çº§**ï¼šå¼•å…¥**å®æ—¶è½¨è¿¹å®‰å…¨æ»¤æ³¢å™¨**ï¼Œæä¾›æ‰§è¡Œæ—¶ä¿è¯ã€‚\n3. **é€šä¿¡ä¼˜åŒ–**ï¼šåœ¨å¥–åŠ±ä¸­å¢åŠ **æ­£äº¤æ€§æ­£åˆ™åŒ–**ï¼Œä¿ƒè¿›ä¿¡æ¯é«˜æ•ˆã€‚\n4. **å¥–åŠ±å¯è§£é‡Š**ï¼šé€šè¿‡**ç³»ç»Ÿæ¶ˆè**éªŒè¯å„å¥–åŠ±ç»„ä»¶ä»·å€¼ã€‚\n```\n\n**å®é™…ä»·å€¼**ï¼šæœ¬æ–‡æ¨åŠ¨MARLå‘**æ›´è´´è¿‘å®é™…æœºå™¨äººåº”ç”¨**çš„æ–¹å‘è¿ˆè¿›äº†ä¸€å¤§æ­¥ï¼Œé€šè¿‡æ•´åˆ**å¼‚æ„æ¨¡å‹ã€å®æ—¶å®‰å…¨ä¿è¯ã€é«˜æ•ˆé€šä¿¡**ä¸‰å¤§å…³é”®è¦ç´ ï¼Œä¸ºæœªæ¥åœ¨æœç´¢æ•‘æ´ã€ååŒå‹˜æ¢ç­‰çœŸå®åœºæ™¯ä¸­éƒ¨ç½²å¯é çš„å¼‚æ„æœºå™¨äººå›¢é˜Ÿæä¾›äº†é‡è¦çš„ç®—æ³•æ¡†æ¶å’Œè®¾è®¡æ€è·¯ã€‚",
            "timestamp": "2026-01-20T17:51:45.095092",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œä½œè€…å›¢é˜Ÿï¼ˆGabriele Calzolari, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulosï¼‰åœ¨æœ¬æ–‡ä¸­å¼•ç”¨äº†å¤šç¯‡ç›¸å…³ç ”ç©¶ï¼Œä½†**æœ¬æ–‡æœ¬èº«å¹¶æœªæ˜ç¡®åˆ—å‡ºæˆ–è®¨è®ºä½œè€…å›¢é˜Ÿè‡ªå·±çš„â€œå‰ä½œâ€**ã€‚å¼•ç”¨çš„æ–‡çŒ®å‡ä¸ºå…¶ä»–ç ”ç©¶å›¢é˜Ÿçš„å·¥ä½œï¼Œç”¨äºé˜è¿°ç ”ç©¶èƒŒæ™¯å’Œå¯¹æ¯”ã€‚\n\nå› æ­¤ï¼Œæˆ‘å°†åŸºäºè®ºæ–‡â€œå¼•è¨€â€å’Œâ€œç›¸å…³å·¥ä½œâ€éƒ¨åˆ†ï¼Œæ¢³ç†æœ¬æ–‡æ‰€**å¯¹æ¯”å’Œå€Ÿé‰´çš„å…³é”®å‰ä½œ**ï¼Œå¹¶åˆ†ææœ¬æ–‡ç›¸å¯¹äºè¿™äº›å·¥ä½œçš„ä¸»è¦æ”¹è¿›ä¸åˆ›æ–°ã€‚è¿™äº›â€œå‰ä½œâ€ä»£è¡¨äº†è¯¥é¢†åŸŸçš„ç ”ç©¶åŸºçº¿ã€‚\n\n### å…³é”®ç›¸å…³å‰ä½œå¯¹æ¯”åˆ†æ\n\n| å‰ä½œï¼ˆä½œè€…/å¹´ä»½ï¼‰ | æ ¸å¿ƒæ–¹æ³•/æ¡†æ¶ | ä¸»è¦ç‰¹ç‚¹ä¸å±€é™æ€§ | æœ¬æ–‡çš„æ”¹è¿›ä¸åŒºåˆ« |\n| :--- | :--- | :--- | :--- |\n| **Yu et al. (2022)** | **MAPPO** | è¯æ˜äº†ç®€å•çš„æ¼”å‘˜-è¯„è®ºå®¶ç»“æ„åœ¨CTDEèŒƒå¼ä¸‹èƒ½äº§ç”Ÿé²æ£’çš„å¤šæ™ºèƒ½ä½“ç­–ç•¥ï¼Œæˆä¸ºåˆä½œä»»åŠ¡çš„å¼ºåŸºçº¿ã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼šæœ¬æ–‡ä»¥MAPPOä¸ºç®—æ³•åŸºç¡€ï¼Œä½†**å…³é”®æ”¹è¿›**åœ¨äºé›†æˆäº†**å®‰å…¨è¿‡æ»¤å™¨**å’Œ**é€šä¿¡æ­£åˆ™åŒ–**ã€‚åŸMAPPOæ¡†æ¶å¿½ç•¥äº†å¼ºå®‰å…¨ä¿è¯å’Œé€šä¿¡é™åˆ¶ï¼Œæœ¬æ–‡åˆ™ç›´æ¥è§£å†³äº†è¿™ä¸¤ä¸ªéƒ¨ç½²ç‰©ç†æœºå™¨äººçš„å…³é”®ç“¶é¢ˆã€‚ |\n| **Niu et al. (2021) - MAGIC** | **åŸºäºå›¾æ³¨æ„åŠ›çš„é€šä¿¡** | ä½¿ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰å­¦ä¹ ä½•æ—¶é€šä¿¡ã€ä¸è°é€šä¿¡ä»¥åŠå¤„ç†æ¶ˆæ¯ï¼Œå®ç°é€šä¿¡ç»“æ„å’Œå†…å®¹çš„ç«¯åˆ°ç«¯è®­ç»ƒã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼š1. **å®‰å…¨æ€§é›†æˆ**ï¼šMAGICä¸“æ³¨äºé€šä¿¡å­¦ä¹ ï¼Œæœªè€ƒè™‘å®‰å…¨ã€‚æœ¬æ–‡åœ¨GATç¼–ç å™¨çš„åŸºç¡€ä¸Šï¼Œ**å¢åŠ äº†è½¨è¿¹æ„ŸçŸ¥çš„å®‰å…¨è¿‡æ»¤å™¨**ï¼Œç¡®ä¿åŠ¨ä½œçš„ç‰©ç†å®‰å…¨æ€§ã€‚2. **é€šä¿¡æ•ˆç‡**ï¼šæœ¬æ–‡é¢å¤–å¼•å…¥äº†**é€šä¿¡æ­£äº¤æ€§æ­£åˆ™åŒ–**åˆ°å¥–åŠ±å‡½æ•°ä¸­ï¼Œæ˜ç¡®æƒ©ç½šæ¶ˆæ¯å‘é‡çš„ç›¸å…³æ€§ï¼Œä»¥å‡å°‘å†—ä½™ï¼Œè€ŒMAGICæœªæ˜¾å¼ä¼˜åŒ–æ­¤ç‚¹ã€‚ |\n| **Cui et al. (2025)** | **MADDPG-based å¼‚æ„ååŒ** | é’ˆå¯¹UAV-USVååŒï¼Œä½¿ç”¨CNNå¢å¼ºè§†è§‰å®šä½ï¼Œè®¾è®¡å¤šçº¦æŸå¥–åŠ±å‡½æ•°ï¼Œå®ç°äº†æ¯”ä¼ ç»Ÿç­–ç•¥æ›´å¿«çš„æ”¶æ•›å’Œæ›´ä¼˜æ€§èƒ½ã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼š1. **ç®—æ³•ä¸å®‰å…¨æ¡†æ¶**ï¼šæœ¬æ–‡é‡‡ç”¨**MAPPO**è€ŒéMADDPGï¼Œå¹¶é›†æˆäº†**å®æ—¶å®‰å…¨è¿‡æ»¤å™¨**ï¼Œæä¾›äº†æ›´å¼ºçš„å³æ—¶å®‰å…¨ä¿è¯ã€‚Cuiç­‰äººçš„å·¥ä½œæœªå¼ºè°ƒå®æ—¶å®‰å…¨è¿‡æ»¤æœºåˆ¶ã€‚2. **é€šä¿¡å­¦ä¹ **ï¼šæœ¬æ–‡æ˜¾å¼å»ºæ¨¡äº†åŸºäºå›¾çš„é€šä¿¡å­¦ä¹ å¹¶ä¼˜åŒ–å…¶æ•ˆç‡ï¼Œè€Œè¯¥å·¥ä½œæœªæ·±å…¥æ¢è®¨è‡ªé€‚åº”é€šä¿¡å­¦ä¹ ã€‚ |\n| **Zhang et al. (2024) - Scal-MAPPO-L** | **å¯æ‰©å±•çš„çº¦æŸç­–ç•¥ä¼˜åŒ–** | é€šè¿‡å±€éƒ¨Îº-hopç­–ç•¥æ›´æ–°å®ç°å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å®‰å…¨åˆ†æ•£å­¦ä¹ ï¼Œç¼“è§£è”åˆçŠ¶æ€-åŠ¨ä½œç©ºé—´çš„æŒ‡æ•°å¢é•¿ã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼š1. **å®‰å…¨å®ç°æ–¹å¼**ï¼šScal-MAPPO-Låœ¨**ç­–ç•¥ä¼˜åŒ–å±‚é¢**é€šè¿‡æ‹‰æ ¼æœ—æ—¥æ–¹æ³•å¤„ç†çº¦æŸã€‚æœ¬æ–‡åˆ™é‡‡ç”¨**åˆ†å±‚æ¶æ„**ï¼šç­–ç•¥ç½‘ç»œè¾“å‡ºåä¹‰åŠ¨ä½œï¼Œç”±**ç‹¬ç«‹çš„å®‰å…¨è¿‡æ»¤å™¨**åœ¨æ‰§è¡Œå‰è¿›è¡ŒæŠ•å½±å’Œä¿®æ­£ã€‚2. **å¼‚æ„æ€§**ï¼šæœ¬æ–‡æ˜ç¡®å¤„ç†**ç»“æ„å¼‚æ„**ï¼ˆUAV/UGVåŠ¨åŠ›å­¦ä¸åŒï¼‰çš„æ™ºèƒ½ä½“ï¼Œå¹¶ä¸ºå…¶è®¾è®¡äº†**å„è‡ªå¯¹åº”çš„å®‰å…¨è¿‡æ»¤å™¨**ï¼ˆåˆ†åˆ«å¤„ç†å®Œæ•´é©±åŠ¨å’Œå·®é€Ÿé©±åŠ¨æ¨¡å‹ï¼‰ã€‚ |\n| **Guo et al. (2024) - MAPPO-PIS** | **æ„å›¾å…±äº«é©±åŠ¨çš„ååŒå†³ç­–** | ä¸ºç½‘è”è‡ªåŠ¨é©¾é©¶è½¦è¾†(CAV)å¼•å…¥æ„å›¾ç”Ÿæˆæ¨¡å—å’Œå®‰å…¨å¢å¼ºæ¨¡å—ï¼Œæå‡æ··åˆäº¤é€šåœºæ™¯ä¸‹çš„å®‰å…¨æ€§ã€æ•ˆç‡å’Œäº¤é€šæ€§èƒ½ã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼š1. **åº”ç”¨é¢†åŸŸä¸æ™ºèƒ½ä½“ç±»å‹**ï¼šMAPPO-PISé’ˆå¯¹**CAV**ï¼ˆåŒæ„æˆ–è¿‘ä¼¼åŒæ„ï¼‰ã€‚æœ¬æ–‡é’ˆå¯¹**å¼‚æ„æœºå™¨äººå›¢é˜Ÿ**ï¼ˆUAV/UGVï¼‰ï¼Œå…¶åŠ¨åŠ›å­¦å·®å¼‚æ›´å¤§ã€‚2. **å®‰å…¨æ¨¡å—é›†æˆåº¦**ï¼šä¸¤è€…éƒ½åŒ…å«å®‰å…¨æ¨¡å—ï¼Œä½†æœ¬æ–‡çš„**å®‰å…¨è¿‡æ»¤å™¨æ˜¯â€œè½¨è¿¹æ„ŸçŸ¥â€**çš„ï¼Œå®ƒé¢„æµ‹åŠ¨ä½œå¼•å‘çš„æœªæ¥çŠ¶æ€å¹¶ç›´æ¥ä¿®æ­£æ§åˆ¶è¾“å…¥ï¼Œä¸åŠ¨åŠ›å­¦æ¨¡å‹ç´§å¯†ç»“åˆã€‚ |\n| **Sun et al. (2025)** | **é€šä¿¡å†—ä½™çš„ç»´åº¦åˆ†æ** | é€šè¿‡å¯¹æ¶ˆæ¯åµŒå…¥çš„ç»´åº¦åˆ†æå½¢å¼åŒ–é€šä¿¡å†—ä½™ï¼Œè¯æ˜å¼ºåˆ¶æ™ºèƒ½ä½“é—´è¡¨ç¤ºå»ç›¸å…³èƒ½æ˜¾è‘—æé«˜å¤§è§„æ¨¡MARLä¸­çš„é€šä¿¡æ•ˆç‡å’ŒååŒæ€§èƒ½ã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼šæœ¬æ–‡å¸æ”¶äº†å…¶æ ¸å¿ƒæ€æƒ³ï¼Œä½†**å®ç°æ–¹å¼ä¸åŒ**ã€‚Sunç­‰äººå¯èƒ½ä»ç†è®ºæˆ–æ¶æ„å±‚é¢åˆ†æã€‚æœ¬æ–‡åˆ™æå‡ºä¸€ç§**å®ç”¨çš„ã€åŸºäºå¥–åŠ±çš„å·¥ç¨‹åŒ–æ–¹æ³•**ï¼šå°†é€šä¿¡å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦æƒ©ç½šé¡¹ï¼ˆ`r_i^comm`ï¼‰ç›´æ¥çº³å…¥å¥–åŠ±å‡½æ•°ï¼Œåœ¨è®­ç»ƒä¸­é¼“åŠ±ä¿¡æ¯æ­£äº¤æ€§ã€‚ |\n\n### æ€»ç»“ï¼šæœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ä¸æ”¹è¿›\n\nç›¸è¾ƒäºä¸Šè¿°â€œå‰ä½œâ€ï¼ˆå³é¢†åŸŸç°æœ‰å·¥ä½œï¼‰ï¼Œæœ¬æ–‡çš„ä¸»è¦æ”¹å–„ä½“ç°åœ¨ **â€œç»Ÿä¸€â€** å’Œ **â€œæ·±åŒ–â€** ï¼š\n\n1.  **ç»Ÿä¸€å®‰å…¨ä¸å­¦ä¹ **ï¼šæ„å»ºäº†ä¸€ä¸ª**å°†åˆ†æ•£å¼MARLï¼ˆMAPPOï¼‰ã€å›¾æ³¨æ„åŠ›é€šä¿¡ï¼ˆGATï¼‰å’Œå®æ—¶å®‰å…¨æ§åˆ¶ï¼ˆSafety Filterï¼‰** ä¸‰è€…ç´§å¯†ç»“åˆçš„æ¡†æ¶ã€‚å¤šæ•°å‰ä½œåªä¾§é‡å…¶ä¸­ä¸€æˆ–ä¸¤ä¸ªæ–¹é¢ã€‚\n2.  **æ·±åŒ–å¼‚æ„æ€§å¤„ç†**ï¼šä¸ä»…æ™ºèƒ½ä½“ç±»å‹å¼‚æ„ï¼ˆUAV/UGVï¼‰ï¼Œè¿˜ä¸ºå…¶**åˆ†åˆ«å®šåˆ¶äº†ç¬¦åˆå„è‡ªåŠ¨åŠ›å­¦æ¨¡å‹ï¼ˆå®Œæ•´é©±åŠ¨ vs å·®é€Ÿé©±åŠ¨ï¼‰çš„å®‰å…¨è¿‡æ»¤å™¨**ï¼Œè¿™æ˜¯å¯¹é€šç”¨å®‰å…¨MARLæ–¹æ³•çš„é‡è¦ç»†åŒ–ã€‚\n3.  **åˆ›æ–°é€šä¿¡ä¼˜åŒ–æ‰‹æ®µ**ï¼šæå‡ºäº†**é€šä¿¡æ­£äº¤æ€§æ­£åˆ™åŒ–å¥–åŠ±é¡¹**ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆã€æ˜“äºåœ¨ç°æœ‰MARLè®­ç»ƒæµç¨‹ä¸­å®æ–½çš„é€šä¿¡å»å†—ä½™æ–¹æ³•ï¼Œç›´æ¥ä¼˜åŒ–äº†é€šä¿¡å†…å®¹çš„æœ‰æ•ˆæ€§ã€‚\n4.  **å…¨é¢çš„å®è¯åˆ†æ**ï¼šé€šè¿‡ç³»ç»Ÿçš„**æ¶ˆèå®éªŒï¼ˆR1-R4ï¼‰**ï¼Œå®šé‡åˆ†æäº†ç›®æ ‡æ¢ç´¢ã€å®‰å…¨çº¦æŸã€é€šä¿¡å¤šæ ·æ€§ç­‰å¥–åŠ±æˆåˆ†å¯¹æœ€ç»ˆä»»åŠ¡æ€§èƒ½ï¼ˆå¦‚ç›®æ ‡è·å–æ•ˆç‡ã€å·¥ä½œé‡å¹³è¡¡ã€å­¦ä¹ ç¨³å®šæ€§ï¼‰çš„å…·ä½“è´¡çŒ®ï¼Œä¸ºå¥–åŠ±å‡½æ•°è®¾è®¡æä¾›äº†å®è¯ä¾æ®ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡å¹¶éå¯¹ä½œè€…è‡ªèº«å‰ä½œçš„è¿­ä»£ï¼Œè€Œæ˜¯åœ¨ç»¼åˆç°æœ‰é¢†åŸŸæœ€ä½³å®è·µï¼ˆMAPPO, GATé€šä¿¡ï¼‰çš„åŸºç¡€ä¸Šï¼Œé’ˆå¯¹å¼‚æ„å¤šæœºå™¨äººç³»ç»Ÿåœ¨ç‰©ç†éƒ¨ç½²ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼ˆå®‰å…¨æ€§ã€é€šä¿¡æ•ˆç‡ï¼‰ï¼Œæå‡ºäº†ä¸€ä¸ªåˆ›æ–°ä¸”å®ç”¨çš„é›†æˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶é€šè¿‡ä¸¥è°¨çš„å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚**",
            "timestamp": "2026-01-20T17:52:30.878822",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## è®ºæ–‡å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\næœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¼‚æ„å¤šæ™ºèƒ½ä½“ååŒç›®æ ‡è·å–çš„**å»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åœ¨äº**èåˆäº†å›¾æ³¨æ„åŠ›é€šä¿¡ã€è½¨è¿¹æ„ŸçŸ¥å®‰å…¨è¿‡æ»¤å™¨ä»¥åŠé€šä¿¡æ­£äº¤æ€§æ­£åˆ™åŒ–**ã€‚ä»¥ä¸‹æ˜¯å…³é”®æŠ€æœ¯çš„è¯¦ç»†è¯´æ˜ï¼š\n\n### 1. åŸºäºå›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰çš„é€šä¿¡ä¸ç­–ç•¥ç¼–ç å™¨\n**æŠ€æœ¯ç»†èŠ‚**ï¼š\n- **ç½‘ç»œæ¶æ„**ï¼šæ¯ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥ç½‘ç»œç”±ä¸€ä¸ª**å›¾ç¥ç»ç½‘ç»œç¼–ç å™¨**å’Œä¸€ä¸ª**å¤šå±‚æ„ŸçŸ¥æœºè¾“å‡ºå¤´**ç»„æˆã€‚\n- **å›¾æ„å»º**ï¼šåœ¨æ¯ä¸€æ­¥ `t`ï¼Œæ ¹æ®æ™ºèƒ½ä½“çš„ç©ºé—´ä½ç½®åŠ¨æ€æ„å»ºä¸€ä¸ªæ— å‘å›¾ `ğ’¢_t = (ğ’±, â„°_t)`ã€‚\n    - **èŠ‚ç‚¹** `v_i âˆˆ ğ’±`ï¼šä»£è¡¨æ¯ä¸ªæ™ºèƒ½ä½“ã€‚\n    - **è¾¹** `â„°_t`ï¼šå½“ä¸¤ä¸ªæ™ºèƒ½ä½“ `i` å’Œ `j` ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»å°äºé€šä¿¡èŒƒå›´ `r_c` æ—¶ï¼Œå³ `â€–ğ©_i^t âˆ’ ğ©_j^tâ€–_2 â‰¤ r_c`ï¼Œåˆ™å­˜åœ¨ä¸€æ¡è¾¹ã€‚\n- **èŠ‚ç‚¹ç‰¹å¾**ï¼šæ¯ä¸ªèŠ‚ç‚¹çš„ç‰¹å¾å‘é‡æ˜¯æ™ºèƒ½ä½“çš„å±€éƒ¨è§‚æµ‹ `ğ¨_i^t`ï¼ˆåŒ…å«ä¼ æ„Ÿå™¨è¯»æ•°ã€é€šä¿¡å‘é‡å’Œè‡ªèº«çŠ¶æ€ï¼‰ã€‚\n- **è¾¹ç‰¹å¾**ï¼šè®¡ç®—ç›¸å¯¹ä½ç½® `ğ©_i^t âˆ’ ğ©_j^t`ã€ç›¸å¯¹è·ç¦» `â€–ğ©_i^t âˆ’ ğ©_j^tâ€–_2` å’Œç›¸å¯¹é€Ÿåº¦ `ğ¯_i^t âˆ’ ğ¯_j^t`ã€‚\n- **æ¶ˆæ¯ä¼ é€’**ï¼šä½¿ç”¨ **GATv2Conv** ç®—å­è¿›è¡Œæ³¨æ„åŠ›åŠ æƒèšåˆã€‚å¯¹äºèŠ‚ç‚¹ `i`ï¼Œå…¶é‚»å±… `j âˆˆ N(i)` çš„æ¶ˆæ¯èšåˆå…¬å¼ä¸ºï¼š\n    ```\n    ğ¡_i' = Î±_{i,i}Î˜ğ¡_i + Î£_{jâˆˆN(i)} Î±_{i,j}Î˜ğ¡_j\n    ```\n    å…¶ä¸­ `Î˜` æ˜¯å¯å­¦ä¹ çš„æƒé‡çŸ©é˜µï¼Œ`Î±_{i,j}` æ˜¯ç”±GATv2è®¡ç®—çš„æ³¨æ„åŠ›ç³»æ•°ï¼Œå®ƒä¾èµ–äºèŠ‚ç‚¹ `i` å’Œ `j` çš„ç‰¹å¾ã€‚\n- **è¾“å‡º**ï¼šç»è¿‡GNNç¼–ç åï¼Œæ¯ä¸ªæ™ºèƒ½ä½“çš„åµŒå…¥å‘é‡é€šè¿‡ä¸€ä¸ªMLPç­–ç•¥å¤´ï¼ˆ2ä¸ªéšè—å±‚ï¼Œ256ä¸ªå•å…ƒï¼ŒELUæ¿€æ´»ï¼‰è¾“å‡ºè¿ç»­åŠ¨ä½œ `ğ’–_i^t` å’Œé€šä¿¡å‘é‡ `ğ’„_i^t âˆˆ â„^{d_c}`ã€‚\n- **å…·ä½“å‚æ•°**ï¼š\n    - é€šä¿¡å‘é‡ç»´åº¦ `d_c = 16`\n    - é€šä¿¡åŠå¾„ `r_c = 4.5 m`\n    - MLPéšè—å±‚ï¼š2å±‚ï¼Œæ¯å±‚256ä¸ªç¥ç»å…ƒ\n\n### 2. è½¨è¿¹æ„ŸçŸ¥å®‰å…¨è¿‡æ»¤å™¨\n**æŠ€æœ¯ç›®æ ‡**ï¼šåœ¨ç­–ç•¥ç½‘ç»œè¾“å‡ºçš„åä¹‰æ§åˆ¶åŠ¨ä½œ `ğ®_i^t` è¢«æ‰§è¡Œå‰ï¼Œå¯¹å…¶è¿›è¡Œå®æ—¶æŠ•å½±ï¼Œç¡®ä¿åœ¨é¢„æµ‹æ—¶åŸŸå†…ä¸ä¼šå‘ç”Ÿç¢°æ’ã€‚\n\n#### 2.1 å…¨å‘æ™ºèƒ½ä½“å®‰å…¨è¿‡æ»¤å™¨\n**æ•°å­¦åŸç†ä¸æ¨å¯¼**ï¼š\n1. **åŠ¨ä½œç¼©æ”¾**ï¼šå°†åä¹‰æ§åˆ¶ `ğ®_i^t` æŒ‰ä¸€ç»„ç¦»æ•£çš„ç¼©æ”¾å› å­ `Î± âˆˆ ğ’œ = {1.0, 0.8, 0.6, 0.4, 0.25, 0.1, 0.0}` è¿›è¡Œç¼©æ”¾ï¼Œå¾—åˆ°å€™é€‰æ§åˆ¶ `ğ®Ìƒ_i(Î±) = Î±ğ®_i^t`ã€‚\n2. **è½¨è¿¹é¢„æµ‹**ï¼šå¯¹äºæ¯ä¸ª `Î±`ï¼Œåœ¨æ—¶é—´åŒºé—´ `Ï„ âˆˆ [0, Î”t]` å†…é¢„æµ‹æ™ºèƒ½ä½“ `i` çš„è½¨è¿¹ã€‚è®ºæ–‡é‡‡ç”¨ç®€åŒ–åŠ¨åŠ›å­¦æ¨¡å‹è¿›è¡Œè¿‘ä¼¼ï¼š\n    ```\n    ğ©_i^{t+Ï„}(Î±) = ğ©_i^t + [ğ¯_i^t + (Î”t / (2m_i)) ğ®Ìƒ_i(Î±)] * Ï„\n    ```\n    å…¶ä¸­ `m_i` æ˜¯è´¨é‡ï¼Œ`Î”t = 0.1 s` æ˜¯ç¯å¢ƒæ­¥é•¿ã€‚\n3. **ç¢°æ’é›†æ„å»º**ï¼š\n    - **å¯¹äºå…¨å‘é‚»å±… `j âˆˆ â„‹`**ï¼šè€ƒè™‘å…¶æœ€å¤§é€Ÿåº¦èŒƒå›´ `ğ¯_j^t âˆˆ [-v_max, v_max]`ï¼Œå…¶æœ€åæƒ…å†µå¯è¾¾é›†æ˜¯ä¸€ä¸ªè½´å¯¹é½çš„çŸ©å½¢ `â„›_j^t`ï¼Œä¸­å¿ƒä¸º `ğ‚_j^t`ï¼ŒåŠé•¿ä¸º `ğ‡_j`ã€‚\n        ```\n        ğ‚_j^t = ğ©_j^t + 0.5(ğ¯_min + ğ¯_max)Î”t\n        ğ‡_j = 0.5(ğ¯_max - ğ¯_min)Î”t\n        ```\n    - **è†¨èƒ€ç¢°æ’é›†**ï¼šä¸ºäº†è€ƒè™‘æ™ºèƒ½ä½“çš„åœ†å½¢æœ¬ä½“å’Œå®‰å…¨è·ç¦» `d_safe`ï¼Œå°†çŸ©å½¢è†¨èƒ€å¾—åˆ° `â„›_j^+`ï¼š\n        ```\n        â„›_j^+ = { ğ± âˆˆ â„Â² | |ğ± âˆ’ ğ‚_j^t|_âˆ â‰¤ ğ‡_j + (r_i + r_j + d_safe)ğŸ }\n        ```\n        å…¶ä¸­ `r_i` å’Œ `r_j` æ˜¯åŠå¾„ï¼Œ`d_safe = 0.05 m`ã€‚\n    - **å¯¹äºå·®é€Ÿé©±åŠ¨é‚»å±… `j âˆˆ ğ’Ÿ`**ï¼šç”±äºå…¶è¿åŠ¨å­¦å¤æ‚ï¼Œé‡‡ç”¨ä¿å®ˆçš„åœ†å½¢è¿‡è¿‘ä¼¼ã€‚å…¶ä½ç½®åœ¨ `Î”t` å†…ä½äºä»¥ `ğ©_j^t` ä¸ºä¸­å¿ƒã€åŠå¾„ä¸º `v_max * Î”t` çš„åœ†ç›˜ `ğ’Ÿ_j^t` å†…ã€‚è†¨èƒ€åçš„ç¢°æ’é›†ä¸ºï¼š\n        ```\n        ğ’Ÿ_j^+ = { ğ± âˆˆ â„Â² | â€–ğ± âˆ’ ğ©_j^tâ€–_2 â‰¤ v_max * Î”t + (r_i + r_j + d_safe) }\n        ```\n4. **ä¼˜åŒ–é—®é¢˜**ï¼šå®‰å…¨è¿‡æ»¤å™¨é€‰æ‹©æ»¡è¶³çº¦æŸçš„æœ€å¤§ç¼©æ”¾å› å­ `Î±`ï¼š\n    ```\n    max Î±\n    s.t. ğ©_i(t+Ï„; Î±) âˆ‰ ğ’, âˆ€ Ï„ âˆˆ [0, Î”t]\n    where ğ’ = (â‹ƒ_{jâˆˆâ„‹} â„›_j^+) âˆª (â‹ƒ_{jâˆˆğ’Ÿ} ğ’Ÿ_j^+)\n    ```\n\n#### 2.2 å·®é€Ÿé©±åŠ¨æ™ºèƒ½ä½“å®‰å…¨è¿‡æ»¤å™¨\n**æŠ€æœ¯ç»†èŠ‚**ï¼š\n- **è½¨è¿¹é¢„æµ‹**ï¼šç”±äºå·®é€Ÿé©±åŠ¨æ¨¡å‹ï¼ˆç‹¬è½®è½¦è¿åŠ¨å­¦ï¼‰éçº¿æ€§ï¼Œä½¿ç”¨**å››é˜¶é¾™æ ¼-åº“å¡”æ³•**åœ¨ `[0, Î”t]` åŒºé—´å†…ç§¯åˆ†ï¼Œç”Ÿæˆ5ä¸ªä¸­é—´çŠ¶æ€ `{ğ±_k(Î±)}`ã€‚\n- **ç¢°æ’æ£€æŸ¥**ï¼šå¯¹æ¯ä¸ªä¸­é—´ä½ç½® `ğ©_k(Î±)`ï¼Œä½¿ç”¨ä¸å…¨å‘æ™ºèƒ½ä½“ç±»ä¼¼çš„ä¿å®ˆç¢°æ’é›†ï¼ˆ`â„›_j^+` å’Œ `ğ’Ÿ_j^+`ï¼‰è¿›è¡Œæ£€æŸ¥ã€‚\n- **å®‰å…¨åŠ¨ä½œé€‰æ‹©**ï¼šåŒæ ·ä»ç¦»æ•£é›† `ğ’œ` ä¸­é€‰æ‹©æ»¡è¶³æ‰€æœ‰ä¸­é—´çŠ¶æ€å‡ä¸è¿›å…¥ç¢°æ’é›† `ğ’` çš„æœ€å¤§ `Î±`ã€‚\n\n### 3. ç»“æ„åŒ–å¥–åŠ±å‡½æ•°ä¸é€šä¿¡æ­£äº¤æ€§æ­£åˆ™åŒ–\n**å¥–åŠ±å‡½æ•°æ€»å½¢å¼**ï¼š\n```\nr_i^t = ğ’˜^âŠ¤ ğ’“_i^t = w_dist * r_i^{dist}(t) + w_goal * r_i^{goal}(t) + w_coll * r_i^{coll}(t) + w_comm * r_i^{comm}(t)\n```\n**å„åˆ†é¡¹è¯¦ç»†è¯´æ˜**ï¼š\n\n| å¥–åŠ±é¡¹ | æ•°å­¦å®šä¹‰ | å‚æ•°ä¸è¯´æ˜ |\n| :--- | :--- | :--- |\n| **è·ç¦»å¥–åŠ±**<br>`r_i^{dist}` | `min_{gâˆˆğ’¢} â€–ğ©_i^t âˆ’ ğ©_g^tâ€– âˆ’ min_{gâˆˆğ’¢} â€–ğ©_i^{t-1} âˆ’ ğ©_g^{t-1}â€–` | é¼“åŠ±æ™ºèƒ½ä½“å‘æœ€è¿‘ç›®æ ‡ç§»åŠ¨ã€‚å¥–åŠ±ä¸ºè´Ÿæ—¶è¡¨ç¤ºè·ç¦»å‡å°ã€‚æƒé‡ `w_dist = 1.0`ã€‚ |\n| **ç›®æ ‡è¾¾æˆå¥–åŠ±**<br>`r_i^{goal}` | å½“æ™ºèƒ½ä½“ `i` è¦†ç›–ä¸€ä¸ªç›®æ ‡æ—¶ï¼ˆå³ `â€–ğ©_i^t âˆ’ ğ©_g^tâ€–_2 â‰¤ Ï_cov`ï¼‰ï¼Œè·å¾—å›ºå®šå¥–åŠ±ã€‚ | `Ï_cov = 1.5 m` ä¸ºè¦†ç›–é˜ˆå€¼ã€‚`r_goal = 10.0`ï¼Œæƒé‡ `w_goal = 1.0`ã€‚ |\n| **ç¢°æ’æƒ©ç½š**<br>`r_i^{coll}` | å½“å‘ç”Ÿç¢°æ’æ—¶ï¼Œç»™äºˆå›ºå®šæƒ©ç½šã€‚ | `r_coll = -8.0`ï¼Œæƒé‡ `w_coll = 1.0`ã€‚ |\n| **é€šä¿¡å¤šæ ·æ€§å¥–åŠ±**<br>`r_i^{comm}` | `Î£_{jâ‰ i} (1 - Î“Â²(ğ’„_i^t, ğ’„_j^t))`<br>å…¶ä¸­ `Î“(ğ’„_i, ğ’„_j) = (âŸ¨ğ’„_i, ğ’„_jâŸ©) / (â€–ğ’„_iâ€– â€–ğ’„_jâ€–)` | **æ ¸å¿ƒåˆ›æ–°**ï¼š`Î“` æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ã€‚è¯¥å¥–åŠ±æƒ©ç½šæ™ºèƒ½ä½“é—´é€šä¿¡å‘é‡çš„ç›¸å…³æ€§ï¼Œé¼“åŠ±**ä¿¡æ¯æ­£äº¤æ€§**ï¼Œä»¥å‡å°‘é€šä¿¡å†—ä½™å’Œå¹²æ‰°ã€‚æƒé‡ `w_comm = 0.1`ã€‚ |\n\n### 4. è®­ç»ƒæ¡†æ¶ä¸ç®—æ³•\n- **åŸºç¡€ç®—æ³•**ï¼šé‡‡ç”¨ **å¤šæ™ºèƒ½ä½“è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆMAPPOï¼‰**ï¼Œéµå¾ªé›†ä¸­è®­ç»ƒåˆ†æ•£æ‰§è¡ŒèŒƒå¼ã€‚\n- **è¯„è®ºå®¶ç½‘ç»œ**ï¼šä½¿ç”¨ **DeepSets** æ¶æ„çš„é›†ä¸­å¼è¯„è®º",
            "timestamp": "2026-01-20T17:53:34.839795",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\n\nè¯¥è®ºæ–‡é€šè¿‡ä¸€ç³»åˆ—å®šé‡å’Œå®šæ€§æŒ‡æ ‡æ¥è¯„ä¼°å…¶æå‡ºçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶çš„æ€§èƒ½ã€‚è¿™äº›æŒ‡æ ‡ä¸»è¦åˆ†ä¸ºä¸‰ç±»ï¼š**è®­ç»ƒè¿‡ç¨‹æŒ‡æ ‡**ã€**ä»»åŠ¡æ€§èƒ½æŒ‡æ ‡**å’Œ**è¡Œä¸ºåˆ†ææŒ‡æ ‡**ã€‚å…·ä½“ç»†èŠ‚å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æŒ‡æ ‡ç±»åˆ« | å…·ä½“æŒ‡æ ‡ | è®¡ç®—æ–¹å¼ / å®šä¹‰ | è¯„ä¼°ç›®çš„ä¸ç»†èŠ‚ |\n| :--- | :--- | :--- | :--- |\n| **è®­ç»ƒè¿‡ç¨‹æŒ‡æ ‡** | **å¹³å‡å›åˆå¥–åŠ±** | æ¯ä¸ªè®­ç»ƒå›åˆï¼ˆepisodeï¼‰ä¸­æ‰€æœ‰æ™ºèƒ½ä½“è·å¾—å¥–åŠ±çš„æ€»å’Œçš„å¹³å‡å€¼ã€‚å¥–åŠ±å‡½æ•°ä¸ºå…¬å¼(15)çš„åŠ æƒå’Œã€‚ | **ç›®çš„**ï¼šè¡¡é‡ç­–ç•¥å­¦ä¹ çš„æ•´ä½“è¿›å±•å’Œç¨³å®šæ€§ã€‚<br>**ç»†èŠ‚**ï¼šè®ºæ–‡ç»˜åˆ¶äº†ä¸åŒå¥–åŠ±é…ç½®ï¼ˆR1-R4ï¼‰ä¸‹ï¼Œå…¨ç³»ç»Ÿã€å…¨å‘ï¼ˆUAVï¼‰å’Œå·®é€Ÿï¼ˆUGVï¼‰æ™ºèƒ½ä½“çš„å¹³å‡å¥–åŠ±å­¦ä¹ æ›²çº¿ï¼ˆå›¾2ï¼‰ã€‚é«˜ä¸”ç¨³å®šçš„å¥–åŠ±è¡¨æ˜ç­–ç•¥æœ‰æ•ˆæ”¶æ•›ã€‚ |\n| | **ç­–ç•¥ç†µ** | ç­–ç•¥ç½‘ç»œè¾“å‡ºåŠ¨ä½œåˆ†å¸ƒçš„ä¿¡æ¯ç†µã€‚ | **ç›®çš„**ï¼šåæ˜ æ™ºèƒ½ä½“çš„æ¢ç´¢-åˆ©ç”¨å¹³è¡¡åŠç­–ç•¥æ”¶æ•›çš„ç¡®å®šæ€§ã€‚<br>**ç»†èŠ‚**ï¼šè®ºæ–‡ç»˜åˆ¶äº†ä¸åŒæ™ºèƒ½ä½“ç±»å‹åœ¨ä¸åŒå¥–åŠ±é…ç½®ä¸‹çš„ç†µæ¼”åŒ–æ›²çº¿ï¼ˆå›¾3aï¼‰ã€‚ç†µå€¼å¿«é€Ÿä¸‹é™è¡¨æ˜ç­–ç•¥ä»æ¢ç´¢è½¬å‘åˆ©ç”¨ï¼Œå¹¶å½¢æˆç¨³å®šè¡Œä¸ºï¼›æŒç»­é«˜ç†µåˆ™è¡¨æ˜å­¦ä¹ å¤±è´¥ã€‚ |\n| **ä»»åŠ¡æ€§èƒ½æŒ‡æ ‡** | **å·²è·å–ç›®æ ‡å¹³å‡æ•°é‡** | åœ¨è¯„ä¼°çš„æ¯ä¸ªæ—¶é—´æ­¥ï¼Œè¢«æ™ºèƒ½ä½“è¦†ç›–ï¼ˆæ»¡è¶³å…¬å¼(2)ï¼‰çš„ç›®æ ‡æ•°é‡çš„å¹³å‡å€¼ã€‚ | **ç›®çš„**ï¼šç›´æ¥é‡åŒ–ä»»åŠ¡å®Œæˆæ•ˆç‡ã€‚<br>**ç»†èŠ‚**ï¼šè®ºæ–‡å±•ç¤ºäº†åœ¨200æ¬¡æ¨¡æ‹Ÿä¸­ï¼Œéšæ—¶é—´æ­¥å¢åŠ çš„å¹³å‡è·å–ç›®æ ‡æ•°æ›²çº¿ï¼ˆå›¾3bï¼‰ï¼Œå¹¶é™„æœ‰æ ‡å‡†å·®å¸¦ï¼Œä»¥è¯„ä¼°ç­–ç•¥çš„**æœ‰æ•ˆæ€§å’Œé²æ£’æ€§**ã€‚ |\n| | **æœ€ç»ˆç›®æ ‡è·å–æ•°æ¦‚ç‡è´¨é‡å‡½æ•°** | åœ¨æ¨¡æ‹Ÿç»“æŸæ—¶ï¼ˆç¬¬100æ­¥ï¼‰ï¼Œå·²è·å–ç›®æ ‡æ•°é‡çš„æ¦‚ç‡åˆ†å¸ƒã€‚ | **ç›®çš„**ï¼šè¯„ä¼°ç­–ç•¥å®Œæˆä»»åŠ¡çš„ä¸€è‡´æ€§å’Œå¯é æ€§ã€‚<br>**ç»†èŠ‚**ï¼šé€šè¿‡ç›´æ–¹å›¾ï¼ˆå›¾3cï¼‰å±•ç¤ºï¼Œç›´è§‚æ¯”è¾ƒä¸åŒå¥–åŠ±é…ç½®ä¸‹ç­–ç•¥èƒ½ç¨³å®šè·å–0ã€1ã€2æˆ–3ä¸ªç›®æ ‡çš„å¯èƒ½æ€§ã€‚ |\n| **è¡Œä¸ºåˆ†ææŒ‡æ ‡** | **å•æ™ºèƒ½ä½“ç›®æ ‡å‘ç°ç‡** | æ¯ä¸ªæ™ºèƒ½ä½“ä¸ªä½“éšæ—¶é—´æ­¥å‘ç°ï¼ˆé¦–æ¬¡è¦†ç›–ï¼‰ç›®æ ‡çš„æ•°é‡æˆ–æ¯”ä¾‹ã€‚ | **ç›®çš„**ï¼šåˆ†æå¼‚æ„å›¢é˜Ÿä¸­çš„**å·¥ä½œé‡åˆ†é…**å’Œ**è§’è‰²æ¶Œç°**ã€‚<br>**ç»†èŠ‚**ï¼šè®ºæ–‡ç»˜åˆ¶äº†æ¯ä¸ªæ™ºèƒ½ä½“ï¼ˆ1ä¸ªUGVï¼Œ2ä¸ªUAVï¼‰åœ¨ä¸åŒå¥–åŠ±é…ç½®ä¸‹çš„å¹³å‡å‘ç°æ›²çº¿ï¼ˆå›¾4ï¼‰ã€‚æ›²çº¿é‡åˆåº¦åæ˜ åä½œçš„å¯¹ç§°æ€§ï¼Œåˆ†ç¦»åº¦åæ˜ ä¸“ä¸šåŒ–åˆ†å·¥ã€‚ |\n| | **ç¢°æ’æ¬¡æ•°/æƒ©ç½š** | åœ¨æ¨¡æ‹Ÿè¿‡ç¨‹ä¸­è§¦å‘ç¢°æ’æƒ©ç½šï¼ˆ`r_coll`ï¼‰çš„äº‹ä»¶æ¬¡æ•°ã€‚ | **ç›®çš„**ï¼šè¯„ä¼°**å®‰å…¨æ€§**ã€‚è™½ç„¶æœªå•ç‹¬ç»˜åˆ¶æ›²çº¿ï¼Œä½†ç¢°æ’æƒ©ç½šæ˜¯å¥–åŠ±å‡½æ•°ï¼ˆR3, R4ï¼‰å’Œå®‰å…¨æ»¤æ³¢å™¨çš„ç›´æ¥ä¼˜åŒ–ç›®æ ‡ï¼Œå…¶å‘ç”Ÿé¢‘ç‡ä½æ˜¯å®‰å…¨æ€§çš„ä½“ç°ã€‚ |\n| | **é€šä¿¡å¤šæ ·æ€§** | é€šè¿‡å¥–åŠ±å‡½æ•°ä¸­çš„ `r_i^comm(t)`ï¼ˆå…¬å¼17ï¼‰é—´æ¥è¡¡é‡ï¼Œè¯¥å¥–åŠ±é¡¹æƒ©ç½šæ™ºèƒ½ä½“é—´é€šä¿¡å‘é‡çš„ç›¸å…³æ€§ã€‚ | **ç›®çš„**ï¼šä¿ƒè¿›é€šä¿¡æ•ˆç‡ï¼Œå‡å°‘å†—ä½™ã€‚é«˜`r_i^comm`å¥–åŠ±æ„å‘³ç€é€šä¿¡å†…å®¹æ›´å…·ä¿¡æ¯æ­£äº¤æ€§ã€‚ |\n\n### äºŒã€ æ•°æ®é›†/å®éªŒç¯å¢ƒ\n\nè¯¥ç ”ç©¶**æœªä½¿ç”¨**çœŸå®ä¸–ç•Œæˆ–æ ‡å‡†åŒ–çš„ç¦»çº¿æ•°æ®é›†ï¼Œè€Œæ˜¯é‡‡ç”¨**è‡ªå®šä¹‰çš„ä»¿çœŸç¯å¢ƒ**è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚ç¯å¢ƒç»†èŠ‚å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| ç¯å¢ƒå±æ€§ | å…·ä½“é…ç½®ä¸ç»†èŠ‚ |\n| :--- | :--- |\n| **ä»¿çœŸå¹³å°** | **VMAS**ï¼šä¸€ä¸ªç”¨äºé›†ä½“æœºå™¨äººå­¦ä¹ çš„çŸ¢é‡åŒ–å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿå™¨ã€‚è®ºæ–‡åŸºäºæ­¤å¹³å°æ„å»ºäº†è‡ªå®šä¹‰ç¯å¢ƒã€‚ |\n| **ä»»åŠ¡åœºæ™¯** | **ååŒç›®æ ‡è·å–**ï¼šåœ¨ä¸€ä¸ªè¿ç»­çš„2Dæ­£æ–¹å½¢å·¥ä½œåŒºï¼ˆè¾¹é•¿ `d = 10m`ï¼‰ä¸­ï¼Œæ™ºèƒ½ä½“å›¢é˜Ÿéœ€æ¢ç´¢å¹¶æŠµè¾¾éšæœºåˆ†å¸ƒçš„ç›®æ ‡ã€‚ |\n| **æ™ºèƒ½ä½“é…ç½®** | - **å¼‚æ„å›¢é˜Ÿ**ï¼š`n_h = 2` ä¸ª**å…¨å‘æ™ºèƒ½ä½“**ï¼ˆæ¨¡æ‹ŸUAVï¼‰ï¼Œ`n_d = 1` ä¸ª**å·®é€Ÿé©±åŠ¨æ™ºèƒ½ä½“**ï¼ˆæ¨¡æ‹ŸUGVï¼‰ã€‚<br>- **åŠ¨åŠ›å­¦**ï¼šå…¨å‘æ™ºèƒ½ä½“éµå¾ªå…¬å¼(4)çš„åŠ¨åŠ›å­¦ï¼›å·®é€Ÿæ™ºèƒ½ä½“éµå¾ªé€šè¿‡RK4ç§¯åˆ†çš„ç‹¬è½®è½¦æ¨¡å‹ã€‚<br>- **æ„ŸçŸ¥**ï¼šæ¯ä¸ªæ™ºèƒ½ä½“é…å¤‡ä¸€ä¸ªåŸºäºå°„çº¿æŠ•å°„çš„æ¨¡æ‹Ÿè·ç¦»ä¼ æ„Ÿå™¨ï¼ˆ`n_l = 16`æ¡å°„çº¿ï¼‰ï¼ŒUAVæ¢æµ‹èŒƒå›´`r_h^l=3.0m`ï¼ŒUGVæ¢æµ‹èŒƒå›´`r_d^l=1.5m`ã€‚<br>- **é€šä¿¡**ï¼šé€šä¿¡èŒƒå›´ `r_c = 4.5m`ï¼Œé€šä¿¡å‘é‡ç»´åº¦ `d_c = 16`ã€‚ |\n| **ç›®æ ‡è®¾ç½®** | éšæœºæ”¾ç½® `n_t = 3` ä¸ªç›®æ ‡ã€‚ç›®æ ‡è¢«è§†ä¸ºâ€œå·²è¦†ç›–â€çš„æ¡ä»¶æ˜¯è‡³å°‘æœ‰ä¸€ä¸ªæ™ºèƒ½ä½“è¿›å…¥å…¶ `Ï_cov = 1.5m` èŒƒå›´å†…ï¼ˆå…¬å¼2ï¼‰ã€‚ |\n| **ç¯å¢ƒå‚æ•°** | å…³é”®å‚æ•°è§è¡¨1ï¼ŒåŒ…æ‹¬ï¼šæ™ºèƒ½ä½“åŠå¾„(`r_d=r_h=0.5m`)ã€å®‰å…¨è·ç¦»(`d_safe=0.05m`)ã€æœ€å¤§æ§åˆ¶è¾“å…¥(`u_max=1`)ã€è´¨é‡(`m=1kg`)ã€æ—¶é—´æ­¥é•¿(`Î”t=0.1s`)ã€çº¿æ€§é˜»å°¼(`c_d=0.25`)ç­‰ã€‚ |\n| **è®­ç»ƒè®¾ç½®** | - **ç®—æ³•**ï¼šä½¿ç”¨BenchMARLæ¡†æ¶ä¸­çš„**MAPPO**ç®—æ³•ã€‚<br>- **è§„æ¨¡**ï¼šåœ¨**600ä¸ªå¹¶è¡Œç¯å¢ƒ**ä¸­è®­ç»ƒã€‚<br>- **æ­¥æ•°**ï¼šè¿›è¡Œ834æ¬¡ä¼˜åŒ–è¿­ä»£ã€‚<br>- **èŒƒå¼**ï¼šé›†ä¸­è®­ç»ƒä¸åˆ†æ•£æ‰§è¡Œã€‚ |\n| **è¯„ä¼°è®¾ç½®** | - **è§„æ¨¡**ï¼šåœ¨**200ä¸ª**éšæœºç”Ÿæˆçš„ä»¿çœŸåœºæ™¯ä¸­è¿›è¡Œè¯„ä¼°ã€‚<br>- **æ—¶é•¿**ï¼šæ¯ä¸ªåœºæ™¯è¿è¡Œ**100ä¸ªæ—¶é—´æ­¥**ã€‚<br>- **å¯¹æ¯”å®éªŒ**ï¼šé€šè¿‡**æ¶ˆèç ”ç©¶**ï¼Œå¯¹æ¯”å››ç§ä¸åŒå¥–åŠ±å‡½æ•°é…ç½®ï¼ˆR1-R4ï¼Œå…¬å¼18ï¼‰ä¸‹çš„æ€§èƒ½ã€‚ |\n\n### æ€»ç»“\nè¯¥è®ºæ–‡çš„è¯„ä»·ä½“ç³»**ç´§å¯†ç»“åˆå…¶ç ”ç©¶ç›®æ ‡**ï¼š\n1.  **æœ‰æ•ˆæ€§**ï¼šé€šè¿‡ç›®æ ‡è·å–æ•°é‡ã€å›åˆå¥–åŠ±ç›´æ¥è¡¡é‡ã€‚\n2.  **å®‰å…¨æ€§ä¸é²æ£’æ€§**ï¼šé€šè¿‡ç¢°æ’é¿å…ï¼ˆå®‰å…¨æ»¤æ³¢å™¨+ç¢°æ’å¥–åŠ±ï¼‰å’Œæ€§èƒ½æŒ‡æ ‡çš„æ–¹å·®é—´æ¥è¡¡é‡ã€‚\n3.  **åä½œè´¨é‡**ï¼šé€šè¿‡å·¥ä½œé‡åˆ†é…åˆ†æï¼ˆå•æ™ºèƒ½ä½“å‘ç°ç‡ï¼‰å’Œé€šä¿¡å¤šæ ·æ€§å¥–åŠ±è¿›è¡Œè¡¡é‡ã€‚\n4.  **å­¦ä¹ ç‰¹æ€§**ï¼šé€šè¿‡å¥–åŠ±æ›²çº¿å’Œç­–ç•¥ç†µåˆ†æå­¦ä¹ è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚\n\næ‰€æœ‰å®éªŒå‡åœ¨**é«˜åº¦å¯æ§çš„è‡ªå®šä¹‰ä»¿çœŸç¯å¢ƒ**ä¸­å®Œæˆï¼Œè¿™ä½¿å…¶èƒ½å¤Ÿç²¾ç¡®éš”ç¦»å’Œè¯„ä¼°æ¡†æ¶ä¸­å„ä¸ªç»„ä»¶ï¼ˆå¦‚å®‰å…¨æ»¤æ³¢å™¨ã€é€šä¿¡æ­£åˆ™åŒ–ï¼‰çš„è´¡çŒ®ï¼Œä½†ç»“è®ºå‘ç°å®ä¸–ç•Œè¿ç§»çš„æœ‰æ•ˆæ€§ä»éœ€è¿›ä¸€æ­¥éªŒè¯ã€‚",
            "timestamp": "2026-01-20T17:54:24.307662",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒè®¾ç½®ä¸è¯„ä¼°åˆ†æ\n\n### å®éªŒç¯å¢ƒä¸æ•°æ®é›†\n- **å®éªŒå¹³å°**ï¼šè®ºæ–‡åœ¨**è‡ªå®šä¹‰çš„ä»¿çœŸç¯å¢ƒ**ä¸­è¿›è¡Œå®éªŒï¼Œè¯¥ç¯å¢ƒåŸºäº **VMASï¼ˆVectorized Multi-Agent Simulatorï¼‰** å¹³å°æ„å»ºã€‚\n- **ç¯å¢ƒé…ç½®**ï¼š\n  - è¿ç»­äºŒç»´æ­£æ–¹å½¢å·¥ä½œç©ºé—´ï¼ˆè¾¹é•¿ 10 ç±³ï¼‰ã€‚\n  - åŒ…å« **3 ä¸ªéšæœºåˆ†å¸ƒçš„ç›®æ ‡ç‚¹**ï¼ˆæ©™è‰²æ ‡è®°ï¼‰ã€‚\n  - å›¢é˜Ÿç”± **2 ä¸ªå…¨å‘ç§»åŠ¨æ™ºèƒ½ä½“ï¼ˆUAVsï¼‰** å’Œ **1 ä¸ªå·®é€Ÿé©±åŠ¨æ™ºèƒ½ä½“ï¼ˆUGVï¼‰** ç»„æˆã€‚\n  - æ™ºèƒ½ä½“é…å¤‡æ¨¡æ‹Ÿè·ç¦»ä¼ æ„Ÿå™¨ï¼ˆ16 æ¡å°„çº¿ï¼‰ï¼Œé€šä¿¡èŒƒå›´æœ‰é™ï¼ˆ4.5 ç±³ï¼‰ã€‚\n- **è®­ç»ƒä¸è¯„ä¼°**ï¼š\n  - **è®­ç»ƒé˜¶æ®µ**ï¼šåœ¨ 600 ä¸ªå¹¶è¡Œç¯å¢ƒä¸­è¿›è¡Œ 834 æ¬¡ä¼˜åŒ–æ­¥éª¤ã€‚\n  - **è¯„ä¼°é˜¶æ®µ**ï¼šåœ¨ **200 ä¸ªéšæœºç”Ÿæˆçš„ä»¿çœŸåœºæ™¯** ä¸­æµ‹è¯•ï¼Œæ¯ä¸ªåœºæ™¯è¿è¡Œ 100 ä¸ªæ—¶é—´æ­¥é•¿ã€‚\n\n### ä¸»è¦è¯„ä¼°æŒ‡æ ‡\n1. **å¹³å‡å›åˆå¥–åŠ±ï¼ˆMean Episode Rewardï¼‰**ï¼šè¡¡é‡æ•´ä½“ä»»åŠ¡æ€§èƒ½ã€‚\n2. **ç›®æ ‡è·å–æ•°é‡ï¼ˆNumber of Acquired Targetsï¼‰**ï¼šç»Ÿè®¡æ™ºèƒ½ä½“æˆåŠŸè¦†ç›–çš„ç›®æ ‡æ•°é‡ï¼ˆè¦†ç›–æ¡ä»¶ï¼šæ™ºèƒ½ä½“ä¸ç›®æ ‡è·ç¦» â‰¤ 1.5 ç±³ï¼‰ã€‚\n3. **ç­–ç•¥ç†µï¼ˆPolicy Entropyï¼‰**ï¼šåæ˜ ç­–ç•¥çš„æ¢ç´¢-åˆ©ç”¨å¹³è¡¡ç¨‹åº¦ã€‚\n4. **æ™ºèƒ½ä½“é—´å·¥ä½œé‡åˆ†å¸ƒï¼ˆWorkload Distributionï¼‰**ï¼šé€šè¿‡å„æ™ºèƒ½ä½“ç‹¬ç«‹å‘ç°ç›®æ ‡çš„æ•°é‡è¯„ä¼°åä½œå‡è¡¡æ€§ã€‚\n5. **å®‰å…¨æ€§èƒ½**ï¼šé€šè¿‡ç¢°æ’æƒ©ç½šé¡¹ï¼ˆ`r_coll = -8.0`ï¼‰å’Œ**å®‰å…¨è¿‡æ»¤å™¨**ç¡®ä¿é›¶ç¢°æ’ã€‚\n\n### æ€§èƒ½æå‡ä¸å…³é”®ç»“æœ\nè®ºæ–‡é€šè¿‡**æ¶ˆèå®éªŒ**æ¯”è¾ƒäº†å››ç§å¥–åŠ±é…ç½®ï¼ˆR1â€“R4ï¼‰ï¼ŒéªŒè¯äº†å„ç»„ä»¶è´¡çŒ®ï¼š\n\n| å¥–åŠ±é…ç½® | å¥–åŠ±ç»„æˆ | å¹³å‡å›åˆå¥–åŠ± | ç›®æ ‡è·å–æ•ˆç‡ | åä½œå‡è¡¡æ€§ | å®‰å…¨æ€§ |\n|:---|:---|:---|:---|:---|:---|\n| **R1** | ä»…è·ç¦»å¥–åŠ± | â‰ˆ 0ï¼ˆæ— æ”¶æ•›ï¼‰ | æä½ï¼ˆå‡ ä¹æ— ç›®æ ‡è·å–ï¼‰ | ä¸é€‚ç”¨ | æ— æ˜¾å¼ä¿éšœ |\n| **R2** | + ç›®æ ‡å®Œæˆå¥–åŠ± | ç¨³å®šæ”¶æ•› | ä¸­ç­‰ï¼ˆ1â€“2 ä¸ªç›®æ ‡ï¼‰ | è§’è‰²åˆ†åŒ–æ˜æ˜¾ | æ— æ˜¾å¼ä¿éšœ |\n| **R3** | + ç¢°æ’æƒ©ç½š | ç¨³å®šæ”¶æ•›ï¼ˆæ–¹å·®è¾ƒä½ï¼‰ | ä¸­ç­‰ï¼ˆ1â€“2 ä¸ªç›®æ ‡ï¼‰ | è§’è‰²åˆ†åŒ–æ›´æ¸…æ™° | é€šè¿‡å®‰å…¨è¿‡æ»¤å™¨å®ç°é›¶ç¢°æ’ |\n| **R4** | + é€šä¿¡æ­£äº¤æ€§å¥–åŠ± | **æœ€é«˜å¥–åŠ±**ï¼ˆæ”¶æ•›ä½†æ–¹å·®è¾ƒå¤§ï¼‰ | **æœ€ä¼˜**ï¼ˆç¨³å®šè·å– 1â€“2 ä¸ªç›®æ ‡ï¼‰ | **æœ€å‡è¡¡**ï¼ˆå·¥ä½œé‡åˆ†å¸ƒå¯¹ç§°ï¼‰ | é€šè¿‡å®‰å…¨è¿‡æ»¤å™¨å®ç°é›¶ç¢°æ’ |\n\n#### æ ¸å¿ƒæ€§èƒ½æå‡ï¼š\n1. **ä»»åŠ¡æ•ˆç‡**ï¼š\n   - R4 é…ç½®åœ¨ç›®æ ‡è·å–æ•°é‡å’Œé€Ÿåº¦ä¸Šè¡¨ç°æœ€ä½³ï¼Œè¡¨æ˜**é€šä¿¡æ­£äº¤æ€§å¥–åŠ±**ä¿ƒè¿›äº†æ›´é«˜æ•ˆçš„æ¢ç´¢ä¸åä½œã€‚\n   - ç›¸æ¯” R1ï¼ˆåŸºçº¿ï¼‰ï¼ŒR4 å®ç°äº†ä»å‡ ä¹é›¶æ•ˆç‡åˆ°ç¨³å®šè·å–ç›®æ ‡çš„æ˜¾è‘—æå‡ã€‚\n\n2. **å®‰å…¨ä¿è¯**ï¼š\n   - é›†æˆ**è½¨è¿¹æ„ŸçŸ¥å®‰å…¨è¿‡æ»¤å™¨**åï¼ˆR3ã€R4ï¼‰ï¼Œåœ¨æµ‹è¯•ä¸­å®ç°äº†**é›¶ç¢°æ’**ï¼ŒéªŒè¯äº†æ¡†æ¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å®‰å…¨æ€§ã€‚\n\n3. **åä½œå‡è¡¡æ€§**ï¼š\n   - R4 é…ç½®ä¸‹ï¼Œä¸åŒæ™ºèƒ½ä½“ï¼ˆUAVs ä¸ UGVï¼‰çš„ç›®æ ‡å‘ç°è½¨è¿¹é«˜åº¦å¯¹é½ï¼Œè¡¨æ˜**é€šä¿¡å¤šæ ·æ€§å¥–åŠ±**ä¿ƒè¿›äº†æ›´å¯¹ç§°çš„åä½œï¼Œé¿å…äº†æ™ºèƒ½ä½“å†—ä½™æˆ–å¹²æ‰°ã€‚\n\n4. **å­¦ä¹ ç¨³å®šæ€§**ï¼š\n   - R2 å’Œ R3 å¥–åŠ±é…ç½®ç†µå€¼ä¸‹é™å¿«ï¼Œç­–ç•¥æ”¶æ•›ç¨³å®šï¼›R4 è™½å¥–åŠ±æœ€é«˜ï¼Œä½†è®­ç»ƒæ–¹å·®è¾ƒå¤§ï¼Œåæ˜ äº†é€šä¿¡æ­£åˆ™åŒ–å¼•å…¥çš„æ¢ç´¢å¤æ‚æ€§ã€‚\n\n### ç»“è®º\nè®ºæ–‡é€šè¿‡ç³»ç»Ÿæ¶ˆèå®éªŒè¯æ˜ï¼š\n- **å®Œæ•´å¥–åŠ±å‡½æ•°ï¼ˆR4ï¼‰** åœ¨ç›®æ ‡è·å–ã€å®‰å…¨æ€§å’Œåä½œå‡è¡¡æ€§ä¸Šç»¼åˆè¡¨ç°æœ€ä¼˜ã€‚\n- **å®‰å…¨è¿‡æ»¤å™¨**æ˜¯ç¡®ä¿é›¶ç¢°æ’çš„å…³é”®ç»„ä»¶ï¼Œå¼¥è¡¥äº†ä¼ ç»Ÿ MARL æ–¹æ³•åœ¨å®‰å…¨ä¿è¯ä¸Šçš„ä¸è¶³ã€‚\n- **é€šä¿¡æ­£äº¤æ€§å¥–åŠ±**æœ‰æ•ˆæå‡äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ä¿¡æ¯æ•ˆç‡ä¸åä½œæ€§èƒ½ã€‚\n\n> **å®é™…ä»·å€¼**ï¼šè¯¥æ¡†æ¶ä¸ºå¼‚æ„æœºå™¨äººå›¢é˜Ÿï¼ˆå¦‚ UAV-UGV è”åˆæœæ•‘ï¼‰åœ¨é€šä¿¡å—é™ã€éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­çš„**å®‰å…¨ååŒå†³ç­–**æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶é€šè¿‡ä»¿çœŸéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ä¸é²æ£’æ€§ã€‚",
            "timestamp": "2026-01-20T17:54:56.398201",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œè¯¥ç ”ç©¶åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼š\n\n- **ä»¿çœŸç¯å¢ƒçš„å±€é™æ€§**ï¼šæ‰€æœ‰å®éªŒå‡åœ¨**VMASæ¨¡æ‹Ÿå™¨**ä¸­è¿›è¡Œï¼Œæœªæ¶‰åŠçœŸå®ç‰©ç†æœºå™¨äººå¹³å°çš„éƒ¨ç½²ä¸éªŒè¯ã€‚æ¨¡æ‹Ÿç¯å¢ƒä¸çœŸå®ä¸–ç•Œåœ¨ä¼ æ„Ÿå™¨å™ªå£°ã€é€šä¿¡å»¶è¿Ÿã€åŠ¨åŠ›å­¦æ¨¡å‹ä¸ç¡®å®šæ€§ç­‰æ–¹é¢å­˜åœ¨å·®è·ã€‚\n- **å›¢é˜Ÿè§„æ¨¡ä¸åœºæ™¯å¤æ‚åº¦æœ‰é™**ï¼šå®éªŒé…ç½®ä¸º**å°è§„æ¨¡å¼‚æ„å›¢é˜Ÿ**ï¼ˆ1ä¸ªUGV + 2ä¸ªUAVï¼Œ3ä¸ªç›®æ ‡ï¼‰ã€‚æ¡†æ¶åœ¨**å¤§è§„æ¨¡å›¢é˜Ÿ**ï¼ˆæ•°åæˆ–ä¸Šç™¾ä¸ªæ™ºèƒ½ä½“ï¼‰ã€æ›´å¤æ‚åŠ¨æ€ç¯å¢ƒæˆ–å¯†é›†éšœç¢ç‰©åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ä¸å¯æ‰©å±•æ€§å°šæœªå¾—åˆ°éªŒè¯ã€‚\n- **å®‰å…¨æ»¤æ³¢å™¨çš„ä¿å®ˆæ€§ä¸è®¡ç®—è´Ÿæ‹…**ï¼š\n    - é‡‡ç”¨**ç¦»æ•£ç¼©æ”¾åŠ¨ä½œé›†**ï¼ˆÎ± âˆˆ {1.0, 0.8, ... 0.0}ï¼‰è¿›è¡Œè½¨è¿¹å®‰å…¨æ£€æŸ¥ï¼Œå¯èƒ½å¯¼è‡´æ§åˆ¶åŠ¨ä½œ**ä¸å¤Ÿå¹³æ»‘æˆ–æœ€ä¼˜**ã€‚\n    - å¯¹å·®åˆ†é©±åŠ¨æ™ºèƒ½ä½“ä½¿ç”¨**ä¿å®ˆçš„åœ†å½¢è¿‡è¿‘ä¼¼**è¿›è¡Œç¢°æ’æ£€æŸ¥ï¼Œå¯èƒ½**è¿‡åº¦é™åˆ¶**äº†å…¶å¯è¡ŒåŠ¨ä½œç©ºé—´ã€‚\n    - å®æ—¶è½¨è¿¹é¢„æµ‹ä¸æ£€æŸ¥åœ¨**æ›´çŸ­æ§åˆ¶å‘¨æœŸæˆ–æ›´å¤æ‚åŠ¨åŠ›å­¦æ¨¡å‹**ä¸‹å¯èƒ½å¸¦æ¥è®¡ç®—å¼€é”€æŒ‘æˆ˜ã€‚\n- **é€šä¿¡æ­£åˆ™åŒ–çš„ç®€å•æ€§**ï¼šé€šä¿¡å¤šæ ·æ€§å¥–åŠ±é¡¹ï¼ˆ`r_i^comm`ï¼‰ä»…åŸºäº**æ¶ˆæ¯å‘é‡é—´çš„ç¬æ—¶ä½™å¼¦ç›¸ä¼¼åº¦**è¿›è¡Œæƒ©ç½šã€‚è¿™ç§æ–¹æ³•å¯èƒ½æ— æ³•æœ‰æ•ˆå­¦ä¹ **ç»“æ„åŒ–ã€è¯­ä¹‰åŒ–**çš„é€šä¿¡åè®®ï¼Œæˆ–å¤„ç†æ›´å¤æ‚çš„é€šä¿¡çº¦æŸï¼ˆå¦‚å¸¦å®½é™åˆ¶ã€å¼‚æ­¥é€šä¿¡ï¼‰ã€‚\n- **å¥–åŠ±å‡½æ•°è®¾è®¡çš„æ•æ„Ÿæ€§**ï¼šå°½ç®¡è¿›è¡Œäº†æ¶ˆèå®éªŒï¼Œä½†**å¥–åŠ±æƒé‡ï¼ˆ`w_dist`, `w_goal`, `w_coll`, `w_comm`ï¼‰éœ€è¦æ‰‹åŠ¨è°ƒä¼˜**ã€‚åœ¨ä¸åŒä»»åŠ¡æˆ–å›¢é˜Ÿé…ç½®ä¸‹ï¼Œå¯èƒ½éœ€è¦é‡æ–°è°ƒæ•´ï¼Œç¼ºä¹è‡ªé€‚åº”æˆ–è‡ªåŠ¨åŒ–çš„å¥–åŠ±å¡‘å½¢æœºåˆ¶ã€‚\n- **ç­–ç•¥æ³›åŒ–èƒ½åŠ›æœªå……åˆ†æ¢ç´¢**ï¼šè®­ç»ƒå’Œè¯„ä¼°åœ¨**ç›¸ä¼¼åˆ†å¸ƒçš„ç¯å¢ƒ**ï¼ˆéšæœºç›®æ ‡ä½ç½®ï¼‰ä¸­è¿›è¡Œã€‚æ¡†æ¶å¯¹äº**æœªè§è¿‡çš„ç¯å¢ƒå¸ƒå±€ã€åŠ¨æ€éšœç¢æˆ–ç›®æ ‡ç±»å‹**çš„æ³›åŒ–èƒ½åŠ›æœªè¿›è¡Œæµ‹è¯•ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡åœ¨ç»“è®ºéƒ¨åˆ†åŠåˆ†æä¸­æš—ç¤ºäº†ä»¥ä¸‹æœªæ¥å·¥ä½œæ–¹å‘ï¼š\n\n- **å‘çœŸå®ä¸–ç•Œéƒ¨ç½²è¿ç§»**ï¼š\n    ```python\n    # æœªæ¥å·¥ä½œå¯èƒ½åŒ…æ‹¬ï¼šåœ¨çœŸå®UAV/UGVå¹³å°ä¸Šé›†æˆå­¦ä¹ åˆ°çš„ç­–ç•¥ï¼Œ\n    # å¤„ç†å®é™…ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆå¦‚ç›¸æœºã€æ¿€å…‰é›·è¾¾ï¼‰ã€é€šä¿¡å»¶è¿Ÿå’Œä¸ç¡®å®šçš„åŠ¨åŠ›å­¦ã€‚\n    ```\n- **æ‰©å±•è§„æ¨¡ä¸å¤æ‚åº¦**ï¼š\n    - ç ”ç©¶æ¡†æ¶åœ¨**å¤§è§„æ¨¡å¼‚æ„å›¢é˜Ÿ**ä¸­çš„å¯æ‰©å±•æ€§ã€‚\n    - åœ¨æ›´å¤æ‚çš„**3Dç¯å¢ƒã€åŠ¨æ€éšœç¢ç‰©ã€éƒ¨åˆ†å¯é€šä¿¡åŒºåŸŸæˆ–å¯¹æŠ—æ€§åœºæ™¯**ä¸­éªŒè¯æ€§èƒ½ã€‚\n- **å¢å¼ºå®‰å…¨æ»¤æ³¢å™¨**ï¼š\n    - å¼€å‘**è¿ç»­ã€å¯å¾®çš„å®‰å…¨æŠ•å½±æ–¹æ³•**ï¼Œæ›¿ä»£ç¦»æ•£ç¼©æ”¾ï¼Œä»¥æä¾›æ›´å¹³æ»‘ã€æ›´ä¼˜çš„æ§åˆ¶ã€‚\n    - ä¸ºå·®åˆ†é©±åŠ¨æ™ºèƒ½ä½“è®¾è®¡**æ›´ç²¾ç¡®çš„è½¨è¿¹å¯è¾¾é›†è¡¨ç¤º**ï¼Œå‡å°‘ä¿å®ˆæ€§ã€‚\n    - æ¢ç´¢**åˆ†å¸ƒå¼æˆ–åˆ†å±‚å®‰å…¨éªŒè¯**ï¼Œä»¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚\n- **æ·±åŒ–é€šä¿¡å­¦ä¹ æœºåˆ¶**ï¼š\n    - å¼•å…¥**ç»“æ„åŒ–é€šä¿¡åè®®å­¦ä¹ **ï¼Œå¦‚åŸºäºæ³¨æ„åŠ›çš„å…³é”®ä¿¡æ¯ç­›é€‰ã€å®šæ—¶é€šä¿¡æˆ–åˆ†å±‚é€šä¿¡æ‹“æ‰‘ã€‚\n    - è€ƒè™‘**å¸¦å®½é™åˆ¶ã€ä¿¡æ¯å‹ç¼©æˆ–å¼‚æ­¥é€šä¿¡**ç­‰å®é™…çº¦æŸã€‚\n- **æ”¹è¿›å­¦ä¹ ç®—æ³•ä¸å¥–åŠ±è®¾è®¡**ï¼š\n    - é›†æˆ**è‡ªåŠ¨å¥–åŠ±å¡‘å½¢æˆ–è¯¾ç¨‹å­¦ä¹ **ï¼Œå‡å°‘å¯¹æ‰‹åŠ¨è°ƒä¼˜çš„ä¾èµ–ã€‚\n    - æ¢ç´¢**ç¦»çº¿å¼ºåŒ–å­¦ä¹ æˆ–æ¨¡ä»¿å­¦ä¹ **ï¼Œåˆ©ç”¨å…ˆéªŒæ•°æ®åŠ é€Ÿå®‰å…¨ç­–ç•¥å­¦ä¹ ã€‚\n    - ç»“åˆ**ç†è®ºå®‰å…¨ä¿è¯**ï¼ˆå¦‚æ§åˆ¶å±éšœå‡½æ•°ï¼‰ä¸å­¦ä¹ ç­–ç•¥ï¼Œæä¾›æ›´ä¸¥æ ¼çš„å®‰å…¨è¯ä¹¦ã€‚\n- **æå‡æ³›åŒ–ä¸é²æ£’æ€§**ï¼š\n    - åœ¨è®­ç»ƒä¸­å¼•å…¥**é¢†åŸŸéšæœºåŒ–**ï¼Œå¢å¼ºç­–ç•¥å¯¹æ¨¡å‹ä¸ç¡®å®šæ€§å’Œç¯å¢ƒå˜åŒ–çš„é²æ£’æ€§ã€‚\n    - æµ‹è¯•æ¡†æ¶åœ¨**åˆ†å¸ƒå¤–ä»»åŠ¡**ï¼ˆå¦‚ä¸åŒæ•°é‡ç›®æ ‡ã€æ–°å‹éšœç¢ç‰©ï¼‰ä¸Šçš„é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬é€‚åº”èƒ½åŠ›ã€‚\n\n**æ€»ç»“**ï¼šè¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„**å®‰å…¨å¼‚æ„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶**ï¼Œä½†å…¶ä¸»è¦å±€é™æ€§åœ¨äº**ä»¿çœŸéªŒè¯ã€è§„æ¨¡ä¸å¤æ‚æ€§ã€å®‰å…¨æ–¹æ³•çš„ä¿å®ˆæ€§ä»¥åŠé€šä¿¡å­¦ä¹ çš„æ·±åº¦**ã€‚æœªæ¥çš„æ”¹è¿›å°†èšç„¦äº**çœŸå®ä¸–ç•Œéƒ¨ç½²ã€å¯æ‰©å±•æ€§ã€ç®—æ³•æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›**çš„æå‡ï¼Œä»¥æ¨åŠ¨å…¶åœ¨å®é™…æœºå™¨äººååŒä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚",
            "timestamp": "2026-01-20T17:55:27.887381",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-13T08:30:43Z",
    "created_at": "2026-01-20T17:49:42.306914",
    "updated_at": "2026-01-20T17:49:42.306927",
    "recommend": 0
}