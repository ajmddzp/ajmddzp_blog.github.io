{
  "id": "2512.01946v2",
  "title": "Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models",
  "authors": [
    "Paul Pacaud",
    "Ricardo Garcia",
    "Shizhe Chen",
    "Cordelia Schmid"
  ],
  "abstract": "Robust robotic manipulation requires reliable failure detection and recovery. Although current Vision-Language Models (VLMs) show promise, their accuracy and generalization are limited by the scarcity of failure data. To address this data gap, we propose an automatic robot failure synthesis approach that procedurally perturbs successful trajectories to generate diverse planning and execution failures. This method produces not only binary classification labels but also fine-grained failure categories and step-by-step reasoning traces in both simulation and the real world. With it, we construct three new failure detection benchmarks: RLBench-Fail, BridgeDataV2-Fail, and UR5-Fail, substantially expanding the diversity and scale of existing failure datasets. We then train Guardian, a VLM with multi-view images for detailed failure reasoning and detection. Guardian achieves state-of-the-art performance on both existing and newly introduced benchmarks. It also effectively improves task success rates when integrated into a state-of-the-art manipulation system in simulation and real robots, demonstrating the impact of our generated failure data. Code, Data, and Models available at https://www.di.ens.fr/willow/research/guardian/.",
  "url": "https://arxiv.org/abs/2512.01946v2",
  "html_url": "https://arxiv.org/html/2512.01946v2",
  "html_content": "Guardian: Detecting Robotic Planning and Execution Errors\nwith Vision-Language Models\nPaul Pacaud\n∗\n, Ricardo Garcia\n∗\n, Shizhe Chen\n∗\n, Cordelia Schmid\n∗\n∗\nInria, École normale supérieure, CNRS, PSL Research University\nfirstname.lastname@inria.fr\n.\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\nAbstract\nRobust robotic manipulation requires reliable failure detection and recovery.\nAlthough current Vision-Language Models (VLMs) show promise, their accuracy and generalization are limited by the scarcity of failure data.\nTo address this data gap, we propose an automatic robot failure synthesis approach that procedurally perturbs successful trajectories to generate diverse planning and execution failures.\nThis method produces not only binary classification labels but also fine-grained failure categories and step-by-step reasoning traces in both simulation and the real world.\nWith it, we construct three new failure detection benchmarks: RLBench-Fail, BridgeDataV2-Fail, and UR5-Fail, substantially expanding the diversity and scale of existing failure datasets.\nWe then train Guardian, a VLM with multi-view images for detailed failure reasoning and detection.\nGuardian achieves state-of-the-art performance on both existing and newly introduced benchmarks. It also effectively improves task success rates when integrated into a state-of-the-art manipulation system in simulation and real robots, demonstrating the impact of our generated failure data. Code, Data, and Models available at\nhttps://www.di.ens.fr/willow/research/guardian/\n.\nI\nIntroduction\nDespite recent advances in task planning and action execution enabled by Large Language Models (LLMs)\n[\nmistralsmall\n,\ngrattafiori2024llama3herdmodels\n]\nand Vision-Language Models (VLMs)\n[\nzhu2025internvl3exploringadvancedtraining\n]\n, current robotic manipulation systems remain vulnerable to a wide range of failures\n[\nsinha2023systemlevelviewoutofdistributiondata\n,\nKawaharazuka_2024\n,\nkroemer2020reviewrobotlearningmanipulation\n]\n.\nThey can suffer from incorrect task plans\n[\nhuang2022language\n]\ndue to LLM hallucinations\n[\nHuang_2025\n]\n, perception errors such as confusing similar objects\n[\ngarcia2025generalizablevisionlanguageroboticmanipulation\n,\ngoyal2023rvtroboticviewtransformer\n]\n, and low-level control issues such as unstable grasps or slippage\n[\nwu2025robomind\n]\n.\nThese failures often compound during task execution, severely hindering the reliability of robots in real-world scenarios.\nTo improve robustness, there is a growing interest in models that can leverage vision and language to reason about task execution and detect robot failures\n[\nliu2023reflect\n,\nchen2024automatingrobotfailurerecovery\n,\nduan2025aha\n]\n.\nA key challenge in robotic failure detection is the lack of comprehensive failure datasets.\nMost real-world robot datasets consist of successful demonstrations\n[\nkhazatsky2025droidlargescaleinthewildrobot\n,\nembodimentcollaboration2024openxembodimentroboticlearning\n,\npumacay2024colosseumbenchmarkevaluatinggeneralization\n]\n, offering limited insight into failure modes.\nAutomatically collecting failures by running trained policies is slow and unsafe, while manually collecting failure data\n[\nchen2024automatingrobotfailurerecovery\n,\nbu2025agibot\n]\nis labor-intensive and often lacks diversity and realism.\nAs a result, prior work\n[\nduan2025aha\n]\nlargely relies on simulation to construct failure cases, which however suffers from the notorious sim-to-real gap\n[\nsimtorealgapzhao2020\n]\n.\nFurthermore, they predominantly focus on execution errors, with limited coverage of planning failures\n[\nduan2025aha\n]\nand lack fine-grained failure reasons\n[\nliu2023reflect\n,\nagia2024unpackingfailuremodesgenerative\n,\nduan2025aha\n,\nchen2024automatingrobotfailurerecovery\n,\nbu2025agibot\n]\n.\nThis scarcity of diverse, realistic, and richly annotated failure data significantly hampers the development and evaluation of failure detection methods.\nAnother challenge is effectively leveraging and reasoning over visual context for failure detection.\nREFLECT\n[\nliu2023reflect\n]\nconverts images into textual descriptions and then leverages LLM reasoning to detect failures, but this multi-stage pipeline is vulnerable to error accumulation from incomplete or inaccurate image captions.\nOther recent approaches\n[\nagia2024unpackingfailuremodesgenerative\n,\netukuru2024robotutilitymodelsgeneral\n]\nuse VLMs to process images directly, yet typically rely on single-view observations, making them susceptible to occlusions and incomplete scene understanding. AHA\n[\nduan2025aha\n]\naddresses this by concatenating multi-view images across timesteps into a single grid-based representation; although this expands visual coverage, the compressed format can hinder fine-grained spatial-temporal understanding.\nMoreover, most existing methods output categorical labels without providing explicit reasoning traces, limiting interpretability and the ability to diagnose complex failure modes.\nFigure 1\n:\nIllustration of our Guardian model - a VLM fine-tuned on our constructed failure datasets. It detects planning failures (top) and execution failures (bottom) in robotic manipulation.\nTo address these challenges, we introduce an automatic failure generation approach that produces diverse planning and execution failures, and we train a reasoning VLM\nGuardian\non the resulting data for robust failure detection.\nOur method procedurally perturbs successful robot trajectories in both simulation and real-world settings to generate failures annotated with fine-grained categories and step-by-step reasoning traces.\nUsing this pipeline, we construct three new datasets:\nRLBench-Fail\n(14K execution and 7K planning samples in the RLBench simulator\n[\njames2019rlbenchrobotlearningbenchmark\n]\n),\nBridgeDataV2-Fail\n(10K execution and 6K planning samples using the real-robot BridgeDataV2 data\n[\nwalke2024bridgedatav2datasetrobot\n]\n), and\nUR5-Fail\n(570 execution and 370 planning samples), our collected real-robot failure dataset with policy-driven rollouts.\nWe then develop\nGuardian\n, a reasoning VLM fine-tuned for failure detection. As illustrated in\nFig.\n1\n, Guardian formulates failure detection as a visual question-answering task, reasoning over task instructions, subtask descriptions, and the visual scene. It leverages high-resolution, multi-view images to provide fine-grained and interpretable failure categorization.\nGuardian achieves state-of-the-art performance on both\nRLBench-Fail\nand\nBridgeDataV2-Fail\n, and generalizes in a zero-shot manner to\nRoboFail\n[\nliu2023reflect\n]\nand our\nUR5-Fail\ndataset.\nWe further demonstrate Guardian’s plug-and-play capability as a feedback module by integrating it into a robotic manipulation system 3D-LOTUS++\n[\ngarcia2025generalizablevisionlanguageroboticmanipulation\n]\n.\nGuardian significantly improves task success through planning and execution monitoring in both simulated and real-world tasks.\nIn summary, our contributions are three-fold:\n•\nWe propose a novel robot failure generation approach that perturbs successful robot demonstrations in both simulation and real-world settings, and autonomously annotates failures with fine-grained categories and reasoning traces. This yields three new datasets for robot failure detection:\nRLBench-Fail\n,\nBridgeDataV2-Fail\n, and\nUR5-Fail\n.\n•\nWe introduce\nGuardian\n, a fine-tuned VLM for planning and execution failure detection.\nGuardian\nintegrates high-resolution, multi-view visual inputs with explicit reasoning to predict fine-grained categorization.\n•\nGuardian\nachieves state-of-the-art performance across four benchmarks, covering both in- and out-of-domain datasets. It further improves task completion of a vision-language manipulation framework in simulated and real robot tasks.\nWe will release datasets, code, and models.\nII\nRelated Work\nRobot failure datasets\n.\nExisting robot failure datasets cover only limited settings.\nRoboFail\n[\nliu2023reflect\n]\nprovides a small collection of hand-crafted simulation and real-world failures.\nSentinel\n[\nagia2024unpackingfailuremodesgenerative\n]\ngenerates failures by rolling out a trained policy on out-of-distribution scenarios created through randomizing object scales and poses, but it includes only four tasks.\nAHA\n[\nduan2025aha\n]\nperturbs trajectories in RLBench simulator\n[\njames2019rlbenchrobotlearningbenchmark\n]\nto create over 49K image–query pairs, yet the data is restricted to simulation and does not include high-level planning failures. Moreover, none of these datasets provides step-by-step reasoning annotations, limiting the development of models capable of fine-grained failure reasoning.\nIn this work, we propose an automated approach that generates large-scale datasets with diverse planning and execution failures, enriched with fine-grained categories and reasoning traces in both simulation and real-robot environments.\nFailure detection in robotic manipulation.\nTraditional monitoring methods rely on explicit models of tasks, identifying model deviations\n[\nde1998execution\n,\ngianni2011unified\n]\n. Unlike LLM-based approaches, they depend on rigid model-based predictions. Recent works formulate failure detection as a Question Answering (QA) task, by zero-shot prompting LLMs and VLMs\n[\nagia2024unpackingfailuremodesgenerative\n,\netukuru2024robotutilitymodelsgeneral\n,\nchen2024automatingrobotfailurerecovery\n,\nma2024generative\n,\nshirai2024visionlanguageinterpreterrobottask\n,\nskreta2024replanroboticreplanningperception\n,\nmei2024replanvlmreplanningrobotictasks\n]\n. For instance, REFLECT\n[\nliu2023reflect\n]\nsummarizes multi-sensory inputs as texts, including audio, and feeds the text to an LLM, but its reliance on consecutive off-the-shelf models leads to accumulated errors. Consequently, efforts have been made to fine-tune VLMs specifically for robotic failure detection. SuccessVQA\n[\npmlr-v232-du23b\n]\nfine-tunes Flamingo\n[\nflamingo\n]\nusing both simulated and real-world data, but evaluates only the final task success, while\nGuardian\nis designed for more granular monitoring, checking the execution of each subtask. Closest to our work, AHA\n[\nduan2025aha\n]\nfine-tunes LLaVA-1.5\n[\nliu2023visualinstructiontuning\n]\non multi-class failures generated in RLBench.\nAHA concatenates multiple camera viewpoints and keysteps into a single image, leading to compressed visual inputs. Unlike AHA and SuccessVQA, which treat failure detection as direct classification, Sentinel\n[\nagia2024unpackingfailuremodesgenerative\n]\nand Cosmos-Reason1\n[\nnvidia2025cosmosreason1physicalcommonsense\n]\nframe it as a reason-first process with explicit chain-of-thought (CoT)\n[\n10.5555/3600270.3602070\n]\nreasoning but focus only on single-view binary failure detection. Sentinel proposes a zero-shot CoT prompting framework to leverage off-the-shelf VLMs, while Cosmos-Reason1 is fine-tuned for physical reasoning. Extending prior approaches, Guardian unifies planning and execution verification via chain-of-thought reasoning over separated multi-view inputs, for multi-class failure detection.\nLearning-based robotic manipulation policies.\nRecent robotic policies have achieved remarkable performance on manipulation tasks\n[\ngarcia2025generalizablevisionlanguageroboticmanipulation\n,\nembodimentcollaboration2024openxembodimentroboticlearning\n,\nze20243ddiffusionpolicygeneralizable\n,\nblack2025pi\n]\n. However, their real-world applicability is still limited by planning and execution errors leading to task failures\n[\nwu2025robomind\n]\n. Inspired by Manipulate-Anything\n[\nduan2024manipulateanythingautomatingrealworldrobots\n]\nand Robot Utility Models\n[\netukuru2024robotutilitymodelsgeneral\n]\n, which demonstrated significant benefits from integrating VLM verifiers into their pipelines,\nGuardian\nis designed as a plug-and-play verification module trained on specific failure data. It can seamlessly integrate into existing policies, enhancing robustness by verifying planning and execution stages.\nIII\nRobot Failure Datasets Construction\nFigure 2\n:\nFailure Data Generation Pipeline. We introduce a novel generation pipeline generating failure cases both online in simulation (RLBench), and offline on the real-world dataset (BridgeDataV2). For each positive example, given its correct plan and successful trajectory, we generate a corresponding incorrect plan and unsuccessful trajectory.\nIII-A\nData Sources\nSimulated data enables controlled failure generation through procedural perturbations\n[\nduan2025aha\n]\n, while real robot data reduces the sim-to-real gap but requires substantial human supervision\n[\nliu2023reflect\n]\n.\nTo balance precise control and real-world fidelity, we use both simulated and real-robot datasets to construct robot failure datasets.\nWe propose an automated method that derives planning and execution failures directly from successful demonstrations, avoiding manual failure collection.\nIn both domains, tasks are decomposed into subtasks with corresponding video segments, which form the basis for generating failures.\nFig.\n2\n(middle) illustrates successful episodes from the simulated and real robot datasets.\nSimulated Data\n.\nWe use the RLBench\n[\njames2019rlbenchrobotlearningbenchmark\n]\nsimulator, selecting 52 tasks from RLBench-18Task\n[\nshridhar2022perceiveractormultitasktransformerrobotic\n]\nand GemBench\n[\ngarcia2025generalizablevisionlanguageroboticmanipulation\n]\nbenchmarks in our training data.\nFor each task, we generate successful scripted trajectories with varied object placements and segment them into subtasks following 3D-LOTUS++\n[\ngarcia2025generalizablevisionlanguageroboticmanipulation\n]\n.\nReal Robot Data.\nWe use BridgeDataV2\n[\nwalke2024bridgedatav2datasetrobot\n]\nwith\nECoT annotations\n[\nzawalski2025roboticcontrolembodiedchainofthought\n]\n, which provide fine-grained subtasks and object labels using large VLMs.\nWe further clean these annotations automatically using heuristics and Mistral-Small-3.1-24B\n[\nmistralsmall\n]\nto filter episodes with missing targets or unreliable bounding boxes.\nTo increase the number of successful trajectories, we augment data by reversing successful executions when applicable, by swapping their start and end images, and updating the associated instructions accordingly (e.g., “open drawer” becomes “close drawer”, “flip pot upright” becomes “flip pot upside down”). This yields approximately 20% additional successful demonstrations.\nIII-B\nAutomated Failure Data Generation\nWe design failure modes based on established failure taxonomies\n[\nliu2023reflect\n,\nduan2025aha\n]\nand analysis of robot policy failures\n[\nwu2025robomind\n]\n.\nThe failures are categorized into two types: planning and execution.\nA planning error denotes an incorrect decomposition of a task into subplans, whereas an execution error reflects unsuccessful completion of a subplan.\nPlanning Failures.\nAs shown in\nFig.\n2\n(top), we construct five types of planning failures:\n(1)\nWrong object manipulated\n– some subtasks manipulate the wrong object.\n(2)\nWrong object state or placement\n– some subtasks select the wrong target location, or state for the correct object.\n(3)\nWrong order\n– one or several subtasks are not in the correct order, violating causal dependencies.\n(4)\nMissing subtask\n– required subtasks are missing from the plan, breaking task completeness.\n(5)\nContradictory subtasks\n– some subtasks conflict with each other.\nTypes 1-3 are generated using an LLM (Mistral-Small-24B) to subtly alter the plan, while types 4-5 are created through rule-based perturbations. Each planning example comprises the task instruction, plan, and the initial front-view image.\nExecution Failures.\nIn simulation, we directly perturb subtask-level actions (\nFig.\n2\n, bottom left), leveraging the simulator’s precise control. A randomly selected subtask on the trajectory is modified using four failure modes:\n(1)\nNo gripper close\n– the gripper is correctly positioned to grasp the object, but it fails to close its jaws.\n(2)\nWrong object state or placement\n– the correct object is manipulated but ends in an incorrect state or placement.\n(3)\nWrong object manipulated\n– the wrong object is used.\n(4)\nImprecise grasping/pushing\n– the gripper attempts to grasp or push the correct object by moving toward it and closing its jaws, but misses due to inaccurate positioning.\nFigure 3\n:\nTraining examples. Guardian is trained on reasoning traces produced by our automatic data generation pipeline.\nFor real robot data, modifying actions directly is impractical due to current limitations of image editing and generation models.\nTherefore, we perturb the subtask text instruction paired with the pre-recorded trajectory segment (\nFig.\n2\n, bottom right) without direct robot control:\n(1)\nTask-execution semantic mismatch\n— an LLM (prompted with the original instruction and visible objects), or a rule-based preposition swap, generates a semantically altered instruction while preserving the start/end images.\n(2)\nRevert action\n— keep the instruction unchanged; replace the end image with the start one to show no progress.\nEach execution example contains the task and subtask descriptions, plus pre-/post-action multi-view images.\nIII-C\nChain-of-Thought (CoT) Generation\nCoT reasoning has shown promise in improving the interpretability and performance of VLMs\n[\nzhang2024improve\n]\n.\nTherefore, we further explore whether reasoning can help failure detection.\nWe introduce an automatic method to generate step-by-step CoTs for training reasoning models.\nFor each sample, we first collect the object category, spatial location, and robot state from the RLBench simulator or from ECoT\n[\nzawalski2025roboticcontrolembodiedchainofthought\n]\nannotations, together with the corresponding failure reason.\nWe then prompt a large reasoning-capable VLM (InternVL3-38B)\n[\nzhu2025internvl3exploringadvancedtraining\n]\nto generate step-by-step reasoning traces based on the initial text–image inputs and the aforementioned information.\nFor planning samples, the model is instructed to sequentially verify each subtask and subsequently analyze the overall plan.\nFor execution samples, the model is guided to describe the pre- and post-action images before assessing subtask completion.\nThe reasoning trace contains 118 tokens on average.\nFig.\n3\nillustrates training examples with chain-of-thoughts verifying plan correctness and subtask completion.\nTABLE I\n:\nRobot failure datasets sizes across splits.\nTraining\nValidation\nTest\nDataset\nExec\nPlan\nExec\nPlan\nExec\nPlan\nRoboFail\n[\nliu2023reflect\n]\n-\n-\n-\n-\n153\n30\nRLBench-Fail\n12358\n5808\n1000\n500\n1000\n500\nBridgeDataV2-Fail\n7830\n4880\n1000\n500\n1000\n500\nUR5-Fail\n400\n200\n30\n30\n140\n140\nIII-D\nReal-Robot, Policy-Driven Data Collection\nWe curate\nUR5-Fail\n, a real-robot dataset, collected using a UR5 arm with three cameras.\nWe run the 3D-LOTUS++ policy\n[\ngarcia2025generalizablevisionlanguageroboticmanipulation\n]\non 34 tasks, recording initial and final multi-view images for each subtask. Subtasks are manually labeled as success or failure to obtain execution failure data. For planning failures, we annotate ground-truth plans and generate failures using the method described in\nSec.\nIII-B\n. Unlike\nRoboFail\n[\nliu2023reflect\n]\n, which is single-view and relies solely on teleoperation,\nUR5-Fail\nis three-view and features autonomous policy rollouts yielding more realistic failures.\nFigure 4\n:\nFailure mode distributions in real executions and our constructed data.\nIII-E\nDataset Statistics and Evaluation\nThe resulting datasets,\nRLBench-Fail\n,\nBridgeDataV2-Fail\n, and\nUR5-Fail\n, contain balanced success/failure examples across both planning and execution, with fine-grained category labels and reasoning traces.\nEach dataset is split into training, validation, and test sets, with the validation and test sets featuring unseen tasks/environments to evaluate generalization.\nDataset statistics are shown in\nTable\nI\n.\nTo measure the quality and diversity of our synthetic datasets, i.e., whether the generated failures reflect real policy execution, we run the 3D-LOTUS++ policy\n[\ngarcia2025generalizablevisionlanguageroboticmanipulation\n]\non 92 RLBench tasks and manually annotate failure modes for 3 failure episodes per task.\nAs shown in\nFig.\n4\n, our designed failure modes reflect real failures, and the overall distribution of our synthetic and real failures remains similar.\nIV\nMethod: The Guardian Model\nFigure 5\n:\nLeft: Overview of the Guardian model architecture. Right: Integration of Guardian model into a robot manipulation pipeline for planning and execution verification.\nIV-A\nProblem Formulation\nWe formulate robot failure detection as a visual question answering problem. For planning verification, given a high-level task instruction\nT\nT\n, a proposed plan\nP\n=\n(\nP\n1\n,\n⋯\n,\nP\nN\n)\nP=(P_{1},\\cdots,P_{N})\n, and the initial visual context\nI\nstart\nI_{\\text{start}}\n, the model\nVLM\nplan\n\\text{VLM}_{\\text{plan}}\nmust not only decide whether the plan is correct but also categorize the type of failure when it occurs:\nVLM\nplan\n​\n(\nI\nstart\n,\nT\n,\nP\n)\n→\n(\nB\nplan\n,\nC\nplan\n)\n,\n\\text{VLM}_{\\text{plan}}(I_{\\text{start}},T,P)\\rightarrow(B_{\\text{plan}},C_{\\text{plan}}),\n(1)\nwhere\nB\nplan\n∈\n{\n0\n,\n1\n}\nB_{\\text{plan}}\\in\\{0,1\\}\nindicates whether the plan is valid, and\nC\nplan\nC_{\\text{plan}}\nspecifies the failure category among the five planning failures constructed in\nSec.\nIII-B\n, shown in\nFig.\n4\n.\nFor execution verification, given the task goal\nT\nT\n, a subtask description\nP\ni\nP_{i}\n, and the visual observations before and after execution,\nI\nstart\nI_{\\text{start}}\nand\nI\nend\nI_{\\text{end}}\n, the model\nVLM\nexec\n\\text{VLM}_{\\text{exec}}\nsimilarly outputs\nVLM\nexec\n​\n(\nI\nstart\n,\nI\nend\n,\nT\n,\nP\ni\n)\n→\n(\nB\nexec\n,\nC\nexec\n)\n,\n\\text{VLM}_{\\text{exec}}(I_{\\text{start}},I_{\\text{end}},T,P_{i})\\rightarrow(B_{\\text{exec}},C_{\\text{exec}}),\n(2)\nwhere\nB\nexec\n∈\n{\n0\n,\n1\n}\nB_{\\text{exec}}\\in\\{0,1\\}\nindicates execution success, and\nC\nexec\nC_{\\text{exec}}\ndenotes the execution failure category among the five execution failures constructed in\nSec.\nIII-B\n, shown in\nFig.\n4\n.\nIV-B\nModel Architecture\nThe\nGuardian\nmodel is built upon the state-of-the-art open-source VLM InternVL3-8B\n[\nzhu2025internvl3exploringadvancedtraining\n]\n.\nAs illustrated in\nFig.\n5\n(left), it consists of three key components: a text tokenizer that converts text prompts into discrete token embeddings, a visual encoder (InternViT-300M) that transforms individual images into visual embeddings, and a transformer-based LLM (Qwen2.5-7B) that processes the concatenated multimodal token sequence to predict the answer.\nRather than concatenating multiple images into a single grid-based image as in AHA\n[\nduan2025aha\n]\n,\nGuardian\nprocesses each image independently through the visual encoder. This design preserves fine-grained spatial details within each image and allows the model to explicitly reason about spatial and temporal changes for more accurate failure detection. Furthermore, unlike SuccessVQA\n[\nnvidia2025cosmosreason1physicalcommonsense\n]\nand AHA\n[\nduan2025aha\n]\nthat output direct classifications,\nGuardian\nleverages an explicit reasoning trace before concluding success or failure.\nIV-C\nModel Training\nWe fine-tune\nGuardian\non robot failure datasets using parameter-efficient Low-Rank Adaptation (LoRA)\n[\nhu2022lora\n]\n, while freezing the visual encoder. Training minimizes cross-entropy loss for next-token prediction.\nAlthough CoT has shown promise to improve performance, it brings additional computation overhead.\nInspired by prior work\n[\nchen2025trainingstrategiesefficientembodied\n]\n, we explore three strategies for incorporating CoT into failure detection:\n(1)\nVanilla:\na baseline model trained and evaluated to directly predict final answers (\nA\n) without\nCoT\n;\n(2)\nThinking:\nthe model is trained and inferred with explicit reasoning, always generating\nCoT\nbefore\nA\n;\n(3)\nDropout:\nin training, the model alternates between generating\nCoT\n+\nA\nand directly predicting\nA\n, while at test time, only\nA\nis produced.\nIV-D\nIntegration into Robotic Manipulation Framework\nGuardian\ncan be seamlessly plugged into existing robotic manipulation pipelines as a verification layer without requiring any architectural modification.\nWithout loss of generality, consider a modular robotic manipulation framework. As shown in\nFig.\n5\n(right),\nGuardian\ncan be inserted at each planning and subtask execution step to detect potential failures.\nUpon detection, it can trigger replanning or re-execute the corresponding motion policy to facilitate recovery.\nV\nExperiments\nV-A\nExperimental Setup\nEvaluation datasets.\nWe evaluate models on our newly constructed benchmarks\nRLBench-Fail\n,\nBridgeDataV2-Fail\n, and\nUR5-Fail\n, as well as the existing benchmark\nRoboFail\n[\nliu2023reflect\n]\n, which is a manually created real-world failure dataset using a UR5 arm with a single camera.\nEvaluation metric.\nWe report the mean binary detection accuracy for each dataset (success/failure) in\nSec.\nV-B\nand discuss multi-class categorization in\nSec.\nV-C\n.\nImplementation details.\nWe trained our models using LoRA (rank 16, effective batch size 16) with the AdamW optimizer (weight decay 0.05), bf16 precision, and a cosine learning rate schedule peaking at\n4\n×\n10\n−\n5\n4\\times 10^{-5}\n. Training was conducted on the\nRLBench-Fail\nand\nBridgeDataV2-Fail\nsets (unless noted), completing in 5.5 hours on 4×H100 GPUs. During training, we randomly selected one or four views on\nRLBench-Fail\ndata to mitigate overfitting to the multiple views. The best checkpoint is selected using the validation sets.\nTABLE II:\nComparison of SOTA failure detection models. Execution and Planning binary accuracies are reported.\n∗\ndenotes numbers from the AHA paper. Guardian is fine-tuned from InternVL3-8B on our constructed datasets, boosting the performance of the base model by 23%.\n(a)\nIn-Domain datasets.\nModel\nRLBench-Fail\nBDV2-Fail\nExec\nPlan\nExec\nPlan\nLarge-scale generalist models\nQwen3-VL-235B-A22B\n0.59\n0.83\n0.75\n0.86\nGPT4.1\n0.63\n0.87\n0.72\n0.86\nSpecialized / small models\nCLIP+MLP\n[\npmlr-v139-radford21a\n]\n0.65\n0.53\n0.58\n0.54\nSentinel\n[\nagia2024unpackingfailuremodesgenerative\n]\n0.57\n-\n0.57\n-\nCosmos-Reason1-7B\n[\nnvidia2025cosmosreason1physicalcommonsense\n]\n0.54\n0.60\n0.53\n0.61\nInternVL3-8B\n0.59\n0.70\n0.66\n0.71\nGuardian-8B-Thinking\n0.83\n0.87\n0.85\n0.91\n(b)\nOut-Of-Domain datasets.\nModel\nRoboFail\n[\nliu2023reflect\n]\nUR5-Fail\nExec\nPlan\nExec\nPlan\nLarge-scale generalist models\nQwen3-VL-235B-A22B\n0.82\n0.70\n0.79\n0.84\nGPT4.1\n0.82\n0.67\n0.79\n0.88\nSpecialized / small models\nCLIP+MLP\n[\npmlr-v139-radford21a\n]\n0.42\n0.43\n0.51\n0.51\nAHA-13B\n[\nduan2025aha\n]\n0.64\n∗\n-\n-\n-\nSentinel\n[\nagia2024unpackingfailuremodesgenerative\n]\n0.79\n-\n0.74\n-\nCosmos-Reason1-7B\n[\nnvidia2025cosmosreason1physicalcommonsense\n]\n0.67\n0.70\n0.58\n0.63\nInternVL3-8B\n0.77\n0.53\n0.71\n0.75\nGuardian-8B-Thinking\n0.86\n0.70\n0.77\n0.89\nV-B\nComparison with state of the art\nCompared methods.\nWe compare our approach against SOTA general and specialized vision–language baselines. General baselines include small-scale (InternVL3-8B\n[\nzhu2025internvl3exploringadvancedtraining\n]\n), and large-scale models (Qwen3-VL-235B-A22B-Thinking\n[\nqwen3vl2025\n]\n, GPT4.1\n[\nopenai2025gpt41\n]\n). Specialized baselines comprise Cosmos-Reason1-7B\n[\nnvidia2025cosmosreason1physicalcommonsense\n]\n, AHA-13B\n[\nduan2025aha\n]\n, Sentinel\n[\nagia2024unpackingfailuremodesgenerative\n]\n, and an MLP baseline using CLIP text-image features\n[\npmlr-v139-radford21a\n]\n. We apply test-time CoT to the two large-scale models and Cosmos-Reason, as they are trained to produce reasoning traces and benefit from explicit reasoning at inference.\nSince AHA-13B is not publicly available, we report its binary accuracy on\nRoboFail\nfrom the original paper\n[\nduan2025aha\n]\n. Sentinel is limited to execution monitoring and is not evaluated for planning.\nResults.\nTable\nII\nreports in-domain (ID) and out-of-domain (OOD) binary accuracies.\nTable\nII(a)\nshows that our Guardian model, fine-tuned from InternVL3-8B on failure datasets, surpasses/matches the larger Qwen3VL and GPT4.1 on in-domain execution and planning sets. In\nTable\nII(b)\nwe evaluate models zero-shot on OOD datasets: Guardian attains 0.86/0.70 (Exec/Plan) binary accuracy on\nRoboFail\nand 0.77/0.89 on\nUR5-Fail\n, outperforming/matching larger general-purpose and specialized VLMs. Sentinel, applied atop Qwen3-VL, underperforms its base VLM, likely due to Sentinel’s rigid CoT prompt. Cosmos-Reason, trained on single-view binary failures, often overlooks fine-grained manipulator/object state changes. AHA, solely tuned to simulation failure data, does not generalize well to the real robot failures of\nRoboFail\n.\nTABLE III:\nImpact of the Guardian fine-tuning data mix on the binary accuracy averaged over planning and execution.\nTraining Data\nRLBench\nBDV2\nRobo\nUR5\nRLBench\nBDV2\nUR5\n-Fail\n-Fail\n-Fail\n-Fail\n✗\n✗\n✗\n0.65\n0.69\n0.65\n0.73\n✓\n✗\n✗\n0.80\n0.72\n0.69\n0.74\n✗\n✓\n✗\n0.66\n0.87\n0.72\n0.69\n✓\n✓\n✗\n0.85\n0.88\n0.78\n0.83\n✓\n✓\n✓\n0.85\n0.88\n0.74\n0.85\nFigure 6\n:\nTraining data size impact on the Execution binary accuracy across benchmarks.\nFigure 7\n:\nConfusion matrices for Guardian’s failure categorization for plans and executions. Values are percentages; rows denote ground-truth classes and columns the predicted classes.\nV-C\nFailure Data Ablations\nIn this section, we examine the composition and scale of our data, and discuss the multi-class categorization.\nTraining data mixture.\nTable\nIII\nshows the impact of fine-tuning datasets.\nRow 1 is the baseline InternVL 3-8B without fine-tuning.\nFine-tuning on simulated\nRLBench-Fail\ndata (row 2) significantly improves the performance on the same dataset, but the gains do not transfer well to the other real-world datasets due to the domain gap.\nA similar trend is observed when training solely on\nBridgeDataV2-Fail\nin row 3.\nIn contrast, combining both datasets during fine-tuning in the fourth row leads to significant gains in domain and improves generalization to the OOD benchmarks. This indicates the importance of generating diverse planning and execution data.\nFurther fine-tuning with the small real-robot\nUR5-Fail\ndataset improves performance on\nUR5-Fail\nwhile maintaining comparable accuracy on the other datasets.\nTraining data size.\nFig.\n6\npresents the scaling behaviors of the training data size on the execution binary accuracies across\nRLBench-Fail\n,\nBridgeDataV2-Fail\n,\nRoboFail\n, and\nUR5-Fail\n. We observe a scaling behavior on in-domain and out-of-domain sets when fine-tuning on our procedurally generated data. This suggests that scaling the generated data size further could improve model performance.\nMulti-class Categorization.\nSince our datasets contain fine-grained failure categorization, Guardian supports multi-class failure prediction.\nThe confusion matrices in\nFig.\n7\nshow strong overall discrimination across categories.\nAs shown in the\nFig.\n7\n(left), most execution categories are identified with high accuracy.\nThe main weakness lies in the VLM’s visual grounding of subtle cues:\nno gripper close\nis confused with\nimprecise grasping/pushing\n, and\nwrong object state or placement\nis often mistaken for\nsuccess\n.\nFor planning shown in\nFig.\n7\n(right),\nwrong object state or placement\n,\nwrong order\n,\nmissing subtask\n, and\nwrong object manipulated\nshow higher confusion, particularly in long-horizon plans.\nImproving the model’s commonsense and temporal reasoning would further enhance its ability to capture these nuanced dependencies.\nTABLE IV\n:\nImage representation impact on the Execution binary accuracy (number of views and image format). The AHA method\n[\nduan2025aha\n]\nequals row 3 (4-view concatenation).\nViews\nImage\nFormat\nRLBench-Fail\nExecution\n1\nconcat\n0.61\nseparated\n0.63\n4\nconcat (AHA\n[\nduan2025aha\n]\n)\n0.72\nseparated\n0.81\nFigure 8\n:\nVerification with Guardian during online task execution on RLBench. Left: Successful correction of a generated plan. Right: Successful correction of a subtask execution.\nV-D\nFailure Detection Method Ablations\nIn this section, we compare visual representation choices and reasoning strategies for VLM-based failure detection.\nMulti-view images.\nTable\nIV\ninvestigates the image representation choice on failure detection.\nWe fine-tune and evaluate InternVL3-8B solely on\nRLBench-Fail\nexecution data.\nWe vary the number of viewpoints (one or four) and the multi-image format, either separated as in\nGuardian\n, or concatenated into a single image as in AHA\n[\nduan2025aha\n]\n.\nFor concatenated inputs, we represent the views as rows, and start/end images as columns. Using multi-view images consistently outperforms their single-view counterpart. In the single-view setting, separating or concatenating two images (start and end) yields comparable performance, as the resolution loss from concatenation is minimal.\nHowever, in the four-view setting, separating the images significantly boosts the performance by 9 points over concatenation. Concatenating images not only shifts from natural image distributions but also compresses visual information in limited image resolution.\nTABLE V\n:\nComparison of training and test-time strategies with and without CoT (\nSec.\nIV-C\n). “A” denotes the final answer. We report the average binary accuracy and inference time [seconds/sample] on one H100.",
  "preview_text": "Robust robotic manipulation requires reliable failure detection and recovery. Although current Vision-Language Models (VLMs) show promise, their accuracy and generalization are limited by the scarcity of failure data. To address this data gap, we propose an automatic robot failure synthesis approach that procedurally perturbs successful trajectories to generate diverse planning and execution failures. This method produces not only binary classification labels but also fine-grained failure categories and step-by-step reasoning traces in both simulation and the real world. With it, we construct three new failure detection benchmarks: RLBench-Fail, BridgeDataV2-Fail, and UR5-Fail, substantially expanding the diversity and scale of existing failure datasets. We then train Guardian, a VLM with multi-view images for detailed failure reasoning and detection. Guardian achieves state-of-the-art performance on both existing and newly introduced benchmarks. It also effectively improves task success rates when integrated into a state-of-the-art manipulation system in simulation and real robots, demonstrating the impact of our generated failure data. Code, Data, and Models available at https://www.di.ens.fr/willow/research/guardian/.\n\nGuardian: Detecting Robotic Planning and Execution Errors\nwith Vision-Language Models\nPaul Pacaud\n∗\n, Ricardo Garcia\n∗\n, Shizhe Chen\n∗\n, Cordelia Schmid\n∗\n∗\nInria, École normale supérieure, CNRS, PSL Research University\nfirstname.lastname@inria.fr\n.\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\nAbstract\nRobust robotic manipulation requires reliable failure detection and recovery.\nAlthough current Vision-Language Models (VLMs) show promise, their accuracy and generalization are limited by the scarcity of failure data.\nTo address this data gap, we propose an automatic robot failure synthesis approach that procedurally perturbs succe",
  "is_relevant": null,
  "relevance_score": 0.0,
  "extracted_keywords": [],
  "one_line_summary": "",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "2025-12-01T17:57:27Z",
  "created_at": "2026-01-08T10:08:12.563788",
  "updated_at": "2026-01-08T10:08:12.563801"
}