{
    "id": "2601.10796v1",
    "title": "Bidirectional Human-Robot Communication for Physical Human-Robot Interaction",
    "authors": [
        "Junxiang Wang",
        "Cindy Wang",
        "Rana Soltani Zarrin",
        "Zackory Erickson"
    ],
    "abstract": "ÊúâÊïàÁöÑ‰∫∫Êú∫Áâ©ÁêÜ‰∫§‰∫í‰∏ç‰ªÖÈúÄË¶ÅÁ≥ªÁªüËÉΩÈÄÇÂ∫îÁî®Êà∑ÂÅèÂ•ΩÔºåËøòÈúÄÁ°Æ‰øùÂÖ∂Ë°å‰∏∫ÂÖ∑ÊúâÈÄèÊòéÂ∫¶„ÄÇÊú¨ÊñáÊèêÂá∫BRIDGEÁ≥ªÁªüÔºå‰∏ÄÁßçÁî®‰∫éÁâ©ÁêÜËæÖÂä©‰ªªÂä°ÁöÑÂèåÂêë‰∫∫Êú∫ÈÄö‰ø°Ê°ÜÊû∂„ÄÇËØ•ÊñπÊ≥ïÂÖÅËÆ∏Áî®Êà∑ÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÂÆûÊó∂Ë∞ÉÊï¥Êú∫Âô®‰∫∫ÁöÑËßÑÂàíËΩ®Ëøπ‚Äî‚ÄîÂåÖÊã¨‰ΩçÁΩÆ„ÄÅÈÄüÂ∫¶‰∏é‰ΩúÁî®Âäõ„ÄÇÊàë‰ª¨ÈááÁî®Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLLÔºâÁªìÂêàËßÑÂàíËøêÂä®Áä∂ÊÄÅ‰∏éÂØπËØùÂéÜÂè≤ÔºåËß£ÊûêÁî®Êà∑Êåá‰ª§‰∏≠ÈöêÂê´ÁöÑËΩ®ËøπË∞ÉÊï¥ÊÑèÂõæ„ÄÇÂÄºÂæóÂº∫Ë∞ÉÁöÑÊòØÔºåÊú¨Á≥ªÁªüËÉΩÂØπÁî®Êà∑Êåá‰ª§ÁîüÊàêËØ≠Ë®ÄÂèçÈ¶àÔºöÊó¢ÂèØÁ°ÆËÆ§ÊâßË°åË∞ÉÊï¥ÁªìÊûúÔºå‰πüÂèØÊèêÂá∫ÊæÑÊ∏ÖÊÄßÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÈÄöËøá‰∏ÄÈ°πÂåÖÂê´18‰ΩçËÄÅÂπ¥ÂèÇ‰∏éËÄÖÁöÑÁî®Êà∑Á†îÁ©∂ÔºåÂú®‰∏âÁßçËæÖÂä©‰ªªÂä°Âú∫ÊôØ‰∏≠ËØÑ‰º∞Êú¨ÊñπÊ≥ïÔºåÂπ∂Â∞ÜBRIDGE‰∏éÊó†ËØ≠Ë®ÄÂèçÈ¶àÁöÑÁÆÄÂåñÁâàÊú¨ÂèäÂü∫ÂáÜÁ≥ªÁªüËøõË°åÂØπÊØî„ÄÇÁªìÊûúË°®ÊòéÔºåÂèÇ‰∏éËÄÖËÉΩÊàêÂäüËøêÁî®ËØ•Á≥ªÁªüÂÆûÊó∂Ë∞ÉÊï¥ËΩ®Ëøπ„ÄÇÊ≠§Â§ñÔºåÂèåÂêëÂèçÈ¶àÊú∫Âà∂ÊòæËëóÊèêÂçá‰∫Ü‰∫§‰∫íÊÄß‰∏éÈÄèÊòéÂ∫¶ÁöÑËØÑÂàÜÔºåËØÅÊòéÊú∫Âô®‰∫∫ÁöÑËØ≠Ë®ÄÂõûÂ∫îÂØπÊûÑÂª∫Êõ¥Áõ¥ËßÇÁöÑÁî®Êà∑‰ΩìÈ™åÂÖ∑ÊúâÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇÊºîÁ§∫ËßÜÈ¢ë‰∏é‰ª£Á†ÅÂ∑≤ÂèëÂ∏É‰∫éÈ°πÁõÆÁΩëÁ´ôÔºöhttps://bidir-comm.github.io/",
    "url": "https://arxiv.org/abs/2601.10796v1",
    "html_url": "https://arxiv.org/html/2601.10796v1",
    "html_content": "\\setcctype\nby-nc-nd\nBidirectional Human-Robot Communication for\nPhysical Human-Robot Interaction\nJunxiang Wang\n‚àó\nCarnegie Mellon University\n,\nCindy Wang\nCarnegie Mellon University\n,\nRana Soltani Zarrin\nHonda Research Institute USA\nand\nZackory Erickson\nCarnegie Mellon University\n(2025-12-01)\nAbstract.\nEffective physical human-robot interaction requires systems that are not only adaptable to user preferences but also transparent about their actions. This paper introduces BRIDGE, a system for bidirectional human-robot communication in physical assistance. Our method allows users to modify a robot‚Äôs planned trajectory‚Äîposition, velocity, and force‚Äîin real time using natural language. We utilize a large language model (LLM) to interpret any trajectory modifications implied by user commands in the context of the planned motion and conversation history. Importantly, our system provides verbal feedback in response to the user, either assuring any resulting changes or posing a clarifying question. We evaluated our method in a user study with 18 older adults across three assistive tasks, comparing BRIDGE to an ablation without verbal feedback and a baseline. Results show that participants successfully used the system to modify trajectories in real time. Moreover, the bidirectional feedback led to significantly higher ratings of interactivity and transparency, demonstrating that the robot‚Äôs verbal response is critical for a more intuitive user experience. Videos and code can be found on our project website:\nhttps://bidir-comm.github.io/\nHuman-robot communication, assistive robotics\n‚àó\njunxiang@cmu.edu\n‚Ä†\n‚Ä†\ncopyright:\ncc\n‚Ä†\n‚Ä†\ndoi:\n10.1145/3757279.3785634\n‚Ä†\n‚Ä†\njournalyear:\n2026\n‚Ä†\n‚Ä†\nisbn:\n979-8-4007-2128-1/2026/03\n‚Ä†\n‚Ä†\nconference:\nProceedings of the 21st ACM/IEEE International Conference on Human-Robot Interaction; March 16‚Äì19, 2026; Edinburgh, Scotland, UK\n‚Ä†\n‚Ä†\nbooktitle:\nProceedings of the 21st ACM/IEEE International Conference on Human-Robot Interaction (HRI ‚Äô26), March 16‚Äì19, 2026, Edinburgh, Scotland, UK\n‚Ä†\n‚Ä†\nccs:\nComputing methodologies¬†Discourse, dialog and pragmatics\n‚Ä†\n‚Ä†\nccs:\nComputer systems organization¬†External interfaces for robotics\nFigure 1.\nExample interaction with BRIDGE. User is in a scratching scenario, where they command the robot to move to the target scratching position (marked with star) through a bidirectional verbal interaction.\nUser issues two consecutive verbal commands with the second one needing interpretation only within the previous context. Each verbal command is responded with a verbal feedback from the robot. After these commands, the user is able to move the robot to the goal position.\n1.\nIntroduction\nA growing body of research has shown that robots can address a range of caregiving activities in autonomous physical assistance\n(Nanavati et¬†al\n.\n,\n2023\n; Erickson et¬†al\n.\n,\n2018\n; Gordon et¬†al\n.\n,\n2024\n)\n. However, many systems lack two capabilities essential for interactive autonomy: real-time user-guided adaptation and transparent communication. Real-time adaptation allows users to adjust a robot‚Äôs ongoing autonomous motion‚Äîsuch as tuning speed or pressure‚Äîto fit their own personal preferences and levels of comfort\n(Jenamani et¬†al\n.\n,\n2025\n; Gupte et¬†al\n.\n,\n2023\n; Gopinathan et¬†al\n.\n,\n2017\n)\n. In parallel, clear communication of a robot‚Äôs intent and state‚Äîsuch as any upcoming changes to its motions‚Äîsupports higher transparency and user trust towards the robot\n(Mehrotra et¬†al\n.\n,\n2024\n; Alhaji et¬†al\n.\n,\n2021\n; Fischer et¬†al\n.\n,\n2018\n; Wang et¬†al\n.\n,\n2025\n)\n.\nIn this paper, we propose BRIDGE, a system for\nB\nidirectional human-\nR\nobot assistive\nI\nnteraction with\nD\nialog\nG\nuidanc\nE\n, addressing both real-time adaptation and transparency. BRIDGE allows users to issue verbal commands to change a robot‚Äôs position, velocity, and force\nin real time\nas the robot autonomously executes a planned interaction for physical assistance. To establish\nbidirectional communication\n, our system responds to every user utterance with verbal feedback‚Äîeither an\nassurance\nof the desired change or a\nclarifying question\n‚Äîthereby closing the interaction loop (an example shown in Figure\n1\n).\nWe evaluate our system via a within-subjects user study with older adults (\nn\n=\n18\nn=18\n) in three physically assistive tasks. Participants successfully modified the robot‚Äôs position, velocity, and force through speech in real time. Beyond this capability, BRIDGE‚Äôs verbal feedback to user adjustments yields higher perceived interactivity and transparency than a unidirectional ablation that allows trajectory modifications but offers no feedback, underscoring the importance of bidirectional communication.\nIn summary, our contributions in this paper are as follows:\n‚Ä¢\nWe propose a bidirectional human-robot communication framework in physically assistive scenarios. This framework couples a user‚Äôs trajectory commands with real-time, transparent verbal feedback from the robot, fostering a more intuitive interaction.\n‚Ä¢\nWe present a novel LLM-based pipeline that efficiently interprets user utterances in trajectory and conversation context. The pipeline simultaneously generates modifications to the trajectory‚Äôs position, velocity, or force and verbal feedback through concise assurances or clarifying questions.\n‚Ä¢\nWe conduct a user study with 18 older adults and three physically assistive tasks, which demonstrates that bidirectional communication leads to higher perceived interactivity and transparency than a unidirectional ablation, while speech-based modifications remain fast enough for real-time applications.\n2.\nRelated Works\n2.1.\nLanguage-guided robot motions in HRI\nThe concept of influencing a robot‚Äôs action through language inputs has been widely explored in different contexts, predominantly in the field of robotic manipulation. Language is often integrated in learning reward functions\n(Liang et¬†al\n.\n,\n2024\n; Yang et¬†al\n.\n,\n2024\n; Sharma et¬†al\n.\n,\n2022\n)\n, selecting motion primitives on a high level\n(Shi et¬†al\n.\n,\n2024\n; Zha et¬†al\n.\n,\n2024\n)\n, computing latent actions\n(Cui et¬†al\n.\n,\n2023\n; Karamcheti et¬†al\n.\n,\n2022\n)\n, or training general language-guided policies\n(Lynch et¬†al\n.\n,\n2023\n)\nand vision-language-action models (VLA)\n(Kim et¬†al\n.\n,\n2024\n; Intelligence et¬†al\n.\n,\n2025\n)\n. In comparison, our method leverages the zero-shot reasoning capabilities of a general-purpose LLM. We focus on the assistive domain and utilize an LLM for real-time interpretation of utterances into trajectory modifications, without task-specific training or fine-tuning.\nIn physically assistive robotics, language is commonly used for issuing task-level commands, such as initiating motions in feeding assistance\n(Padmanabha et¬†al\n.\n,\n2024\n; Bhattacharjee et¬†al\n.\n,\n2020\n)\nor selection of predefined motion primitives\n(Alonso et¬†al\n.\n,\n2021\n)\n. Other use cases mostly revolve around commanding a robot arm for assistive object retrieval\n(Poirier et¬†al\n.\n,\n2019\n; Pulikottil et¬†al\n.\n,\n2018\n)\n. These systems are often confined to one particular assistive task, whereas our developed system can be applied to a range of physically assistive trajectories. We also make modifications to trajectories on a parameter level, directly changing all motion aspects of position, velocity, and force, without being limited to configured actions.\n2.2.\nHuman-robot dialog systems\nWhile the previous section mainly focuses on voice interfaces being used only in the human-to-robot direction, many prior works also explore dialog systems between humans and robots, similar to our system. Speech serves as one of the most intuitive interfaces that can grant both personalization and transparency, especially for older adults and assistive scenarios\n(Pradhan et¬†al\n.\n,\n2020\n; Song et¬†al\n.\n,\n2022\n; Mahmood et¬†al\n.\n,\n2024\n)\n.\nSocially assistive robots often make use of conversations in the contexts of therapy or affective support\n(Rudzicz et¬†al\n.\n,\n2015\n; Lima et¬†al\n.\n,\n2021\n; Spitale et¬†al\n.\n,\n2023\n)\n, while these applications generally do not involve physical interactions, which is the emphasis of our work.\nDialog can also play an important role in the domain of human-robot collaboration\n(Wang et¬†al\n.\n,\n2024\n; Mandi et¬†al\n.\n,\n2024\n; Yu et¬†al\n.\n,\n2025\n; Allgeuer et¬†al\n.\n,\n2024\n)\n, and the information exchanged often revolves around task assignment, hence which party (human or robot) should be assigned with which step in a task. In contrast, we address the domain of assistive robotics, where humans can provide their preferences verbally when robots autonomously provide physical assistance. Additionally, we focus on assistive tasks that can be completed even with only human commands and no robot verbal feedback, and in this work, we look into the effect of providing such verbal feedback to humans‚Äîhence how bidirectional communication influences an interaction that may also be completed with unidirectional human-to-robot communication.\n3.\nMethods\nFigure 2.\nFlowchart of BRIDGE: our bidirectional communication system, including two cases of verbal feedback depending on whether a user utterance is clear: (1) assuring and executing any modifications to the trajectory, or (2) posing a clarification question to request for further user input.\nThe bidirectional communication system takes as inputs a trajectory and a user utterance, and either (1) infers and generates a trajectory modification from the utterance to update the trajectory, paired with verbal assurance, or (2) poses a clarifying question when the utterance is unclear, seeking additional user response.\nOur framework takes as input (1) a planned physically assistive trajectory and (2) a user utterance, and produces real-time modifications to that trajectory based on the utterance. As outlined in Figure\n2\n, the system either applies and communicates trajectory changes when the utterance directly implies so, or poses a clarifying question seeking for more user input. In this section, we first introduce compact representations for trajectories (¬ß\n3.1\n) and modifications to trajectories (¬ß\n3.2\n), then discuss the LLM-based interpreter that maps an utterance to the correct modification along with a concise communication as feedback (¬ß\n3.3\n).\n3.1.\nTrajectory representation\n3.1.1.\nAssumptions\nWe assume we are provided with a planned end-effector trajectory, as a sequence of 3D waypoints:\n(1)\nœÑ\n=\n{\nw\ni\n:\nw\ni\n=\n(\nt\ni\n,\nùê±\ni\n,\nv\ni\n,\nf\ni\n)\n}\ni\n=\n1\nN\n,\n\\tau=\\{w_{i}:w_{i}=(t_{i},\\mathbf{x}_{i},v_{i},f_{i})\\}_{i=1}^{N},\nwhere each waypoint\nw\ni\nw_{i}\nconsists of a timestamp\nt\ni\n‚àà\n‚Ñù\n‚â•\n0\nt_{i}\\in\\mathbb{R}_{\\geq 0}\n, end-effector position\nùê±\ni\n‚àà\n‚Ñù\n3\n\\mathbf{x}_{i}\\in\\mathbb{R}^{3}\n, velocity magnitude\nv\ni\n‚àà\n‚Ñù\n‚â•\n0\nv_{i}\\in\\mathbb{R}_{\\geq 0}\n, and desired force magnitude\nf\ni\n‚àà\n‚Ñù\n‚â•\n0\nf_{i}\\in\\mathbb{R}_{\\geq 0}\n. Between consecutive waypoints, the robot end-effector is assumed to follow a straight-line Cartesian path, with velocity and force linearly interpolated in time. Force magnitudes\nf\ni\nf_{i}\nare intended for assistive contact with the user and are tracked by a low-level controller implementing either impedance or admittance control. We assume the planned trajectory interacts with a person, hence approaching certain relevant human body landmarks (major body joints such as wrists, elbows, and shoulders). We further assume access to estimated 3D positions of these landmarks, which could be obtained from body pose estimation.\nFigure 3.\nExample YAML trajectory and user utterances as inputs to BRIDGE, along with the generated communications and trajectory modifications in YAML format.\nSeveral example user utterances with the same input trajectory, as well as our system‚Äôs outputs to these examples.\n3.1.2.\nTrajectory representation\nWe build a minimal representation of the input trajectory\nœÑ\n\\tau\nthat gives the LLM a high-level sketch of the planned interaction. Since the trajectory interacts with a person, we compute the nearest human-body landmark for each waypoint and use only this label to form a symbolic representation, without involving kinematic details. This representation is serialized in YAML format for structure (see top-left of Figure\n3\nfor a snippet example). If no landmark falls within a proximity threshold, the label is left unassigned (\nNone\nin the YAML), which has no geometric implication. Such a representation supports high-level identification of where interactions occur and also provides coarse physical grounding for utterance interpretation, as we discuss in ¬ß\n3.3.2\n.\n3.2.\nModification representation\nWe support trajectory modifications in position, velocity, and force, and we organize them at three scopes:\nglobal\nchanges that apply to the entire trajectory (¬ß\n3.2.1\n),\nlandmark\nchanges whose influence is defined with respect to specific body landmarks (¬ß\n3.2.2\n), and\nwaypoint\nchanges that affect certain waypoints (¬ß\n3.2.3\n). These scopes do not imply priority, so if a single utterance specifies changes in multiple scopes, we apply them concurrently to the trajectory. Across utterances, changes accumulate, with velocity clamped to a fixed cap (see implementation details in ¬ß\n4.1\n). We serialize these trajectory modifications also in YAML schema.\n3.2.1.\nGlobal Changes\nWe support changes to the velocity and force of the entire trajectory, applied uniformly to\nall\nwaypoints. These changes correspond to broad user utterances such as ‚ÄúMove faster.‚Äù We apply these changes in a multiplicative manner, with the scaling factor represented in the corresponding YAML fields (see examples 1 and 2 in Figure\n3\n). Thus, a value greater than 1 indicates an increase, and a value between 0 and 1 indicates a decrease.\n3.2.2.\nLandmark Changes\nBRIDGE also allows users to modify kinematic parameters relative to body landmarks. These changes fall into two main categories: (1) velocity and force changes around body landmarks, expressed in utterances such as ‚ÄúGo slower near my arm‚Äù (see example 3 in Figure\n3\n), and (2) position changes based on attraction to/repulsion from body landmarks, expressed in utterances such as ‚ÄúMove up a lot higher on my arm‚Äù (see example 4 in Figure\n3\n). We next elaborate on each of these categories.\nLocal velocity and force changes.\nThese changes are represented in YAML similarly to global changes, but their effect on each waypoint is subject to Gaussian decay based on the waypoint‚Äôs distance from the local landmark. For example, suppose a landmark located at\nùê©\nlandmark\n‚àà\n‚Ñù\n3\n\\mathbf{p}_{\\text{landmark}}\\in\\mathbb{R}^{3}\nhas a\nvelocity\nYAML field of\nk\n>\n1\nk>1\n, then waypoint\nw\nw\n‚Äôs velocity\nv\nv\nshould be increased by a factor of:\n(2)\n1\n+\n(\nk\n‚àí\n1\n)\n‚Äã\nexp\n‚Å°\n(\n‚àí\n‚Äñ\nùê©\nlandmark\n‚àí\nùê±\n‚Äñ\n2\n2\n‚Äã\nœÉ\n2\n)\n1+(k-1)\\exp\\left(\\frac{-\\|\\mathbf{p}_{\\text{landmark}}-\\mathbf{x}\\|^{2}}{2\\sigma^{2}}\\right)\nwhere\nœÉ\n\\sigma\ncontrols the spread‚Äîa larger\nœÉ\n\\sigma\nresults in wider influence around the landmark, while a smaller\nœÉ\n\\sigma\nleads to a more localized effect. We adopted the Gaussian decay model since this creates a decaying influence naturally implied by phrases like ‚Äúaround my wrist.‚Äù We set\nœÉ\n=\n0.07\n\\sigma=0.07\nm, as empirical pilot testing shows this value creates a localized and smoothly-decaying effect. The Gaussian decay for a decrease in velocity or force is expressed similarly.\nPosition changes.\nIn order to modify the position of individual waypoints in the input trajectory, we apply the notion of artificial potential fields\n(Khatib,\n1986\n)\n, commonly used in robotics for manipulation and navigation with obstacle avoidance. Attractive and repulsive potentials can therefore be placed at body landmarks, and we compute how much to displace a waypoint\nw\nw\nfrom the gradients of all potential functions, evaluated at the waypoint‚Äôs position\nùê±\n\\mathbf{x}\n. We specify both attractive and repulsive potentials, as well as the intensity of each, via the\nattract\nfield in the YAML entry for each landmark (see example 4 in Figure\n3\n). Following the convention of multiplicative factors for velocity and force, a value greater than 1 indicates an attractive potential, and a value between 0 and 1 indicates a repulsive potential. For attractive potentials, we use the standard quadratic formulation. Suppose we have an\nattract\nfield of\nk\n>\n1\nk>1\nfor a landmark located at\nùê©\nlandmark\n\\mathbf{p}_{\\text{landmark}}\n:\n(3)\nU\natt\n‚Äã\n(\nùê±\n)\n=\nk\n2\n‚Äã\nk\np\n‚Äã\n‚Äñ\nùê©\nlandmark\n‚àí\nùê±\n‚Äñ\n2\nU_{\\text{att}}(\\mathbf{x})=\\frac{k}{2}k_{p}\\|\\mathbf{p}_{\\text{landmark}}-\\mathbf{x}\\|^{2}\nThe gain\nk\np\nk_{p}\nis empirically determined to be 0.01‚Äâm\n-2\n. Our application differs from most manipulation and navigation scenarios in that there could be multiple attractive potentials (e.g. attraction to the forearm overall is represented as attractions to the elbow and the wrist) as opposed to a single goal. In order to ensure convergence and avoid waypoints already close to an attractive potential being pulled towards another goal, we weight each attractive potential by the inverse of its distance to the point of interest. The total attractive displacement for a waypoint located at\nùê±\n\\mathbf{x}\nis therefore:\n(4)\nŒî\natt\n‚Äã\n(\nùê±\n)\n=\n‚àí\n‚àë\nj\nw\nj\n‚àë\nk\nw\nk\n‚Äã\n‚àá\nU\natt\n,\nj\n‚Äã\n(\nùê±\n)\n,\nw\nj\n=\n1\n‚Äñ\nùê©\nlandmark\n,\nj\n‚àí\nùê±\n‚Äñ\n\\Delta_{\\text{att}}(\\mathbf{x})=-\\sum_{j}\\frac{w_{j}}{\\sum_{k}w_{k}}\\nabla U_{\\text{att},j}(\\mathbf{x}),\\quad w_{j}=\\frac{1}{\\|\\mathbf{p}_{\\text{landmark},j}-\\mathbf{x}\\|}\nFor repulsive potentials, we use the formulation with a limit distance of effect\nœÅ\n0\n\\rho_{0}\n. Suppose a landmark at\nùê©\nlandmark\n\\mathbf{p}_{\\text{landmark}}\nhas\nk\n‚àà\n(\n0\n,\n1\n)\nk\\in(0,1)\n:\n(5)\nU\nrep\n‚Äã\n(\nùê±\n)\n=\n{\nŒ∑\n2\n‚Äã\nk\n‚Äã\n(\n1\n‚Äñ\nùê©\nlandmark\n‚àí\nùê±\n‚Äñ\n‚àí\n1\nœÅ\n0\n)\n2\nif\n‚Äã\n‚Äñ\nùê©\nlandmark\n‚àí\nùê±\n‚Äñ\n‚â§\nœÅ\n0\n0\nif\n‚Äã\n‚Äñ\nùê©\nlandmark\n‚àí\nùê±\n‚Äñ\n>\nœÅ\n0\nU_{\\text{rep}}(\\mathbf{x})=\\begin{cases}\\frac{\\eta}{2k}\\left(\\frac{1}{\\|\\mathbf{p}_{\\text{landmark}}-\\mathbf{x}\\|}-\\frac{1}{\\rho_{0}}\\right)^{2}\\,&\\text{if }\\|\\mathbf{p}_{\\text{landmark}}-\\mathbf{x}\\|\\leq\\rho_{0}\\\\\n0\\,&\\text{if }\\|\\mathbf{p}_{\\text{landmark}}-\\mathbf{x}\\|>\\rho_{0}\\end{cases}\nThe gain\nŒ∑\n\\eta\nand the distance of effect\nœÅ\n0\n\\rho_{0}\nare empirically determined to be 0.5‚Äâm\n2\nand 0.1‚Äâm, respectively. Therefore, assuming presence of multiple attractive and repulsive potentials, the net displacement of the position\nùê±\n\\mathbf{x}\nof a waypoint\nw\nw\nis computed as a single step in the opposite direction of the potential field gradients:\n(6)\nŒî\n‚Äã\n(\nùê±\n)\n=\nŒî\natt\n‚Äã\n(\nùê±\n)\n‚àí\n‚àë\n‚àá\nU\nrep\n‚Äã\n(\nùê±\n)\n\\Delta(\\mathbf{x})=\\Delta_{\\text{att}}(\\mathbf{x})-\\sum\\nabla U_{\\text{rep}}(\\mathbf{x})\nNote that this displacement is only applied once to each waypoint, unlike the classical, iterative application of potential fields.\n3.2.3.\nWaypoint Changes\nLastly, our system also supports changes to the velocity and force of individual waypoints. This capability is useful for handling utterances such as ‚ÄúGo faster when you move away from me,‚Äù which may target sections of the trajectory far away from any specific body landmark. To execute this change, the system identifies the waypoints implied by the utterance through referencing the trajectory YAML and applies the change uniformly to each one. Changes to any one waypoint does not directly affect its neighboring waypoints. Although the uniform application is similar to global changes, waypoint changes require generating a separate YAML entry for every modified waypoint.\n3.3.\nInterpretation of User Utterances\nWe design a structured LLM prompt that translates user utterances and YAML trajectories into trajectory modifications in YAML, accompanied by a brief sentence communicating back to the user. In this section, we first discuss some features that allow BRIDGE to adapt to various levels of desired change (¬ß\n3.3.1\n), as well as interpreting utterances in context of both the trajectory and the conversation history (¬ß\n3.3.2\n). Next, we focus on the generation of robot verbal feedback, which forms the concept of bidirectional communication (¬ß\n3.3.3\n). Lastly, we provide some rationale and design choices regarding the compactness of LLM outputs (¬ß\n3.3.4\n). The complete prompt used in BRIDGE can be found in the Appendix.\n3.3.1.\nGranularity\nThe prompt is designed to handle both generic and fine-grained adjustments to the motion aspects of position, velocity, and force. For a generic adjustment without specifying further granularity (e.g. ‚ÄúGo slower‚Äù), we configure the change magnitude\nk\nk\nto a default factor of 2 (\nk\n=\n2\nk=2\nfor increases and\nk\n=\n0.5\nk=0.5\nfor decreases), as we find this to be an empirically distinguishable magnitude for typical assistive trajectories. When the user desires more fine-grained control, (e.g. ‚ÄúGo slightly slower‚Äù and ‚ÄúPress much harder‚Äù), the prompt provides examples that allow the LLM to reason about the implied magnitude from the utterance; see examples 2 and 4 in Figure\n3\nfor sample utterances and their corresponding YAML modifications. Finally, we bound the maximum change magnitude to a factor of 3 to avoid drastic changes.\n3.3.2.\nContext-aware Utterance Interpretation\nTo create a fluid and intuitive interaction, the prompt contains two sources of context‚Äîplanned trajectory and conversation history‚Äîenabling the LLM to interpret ambiguous commands without requiring excessive specificity from the user.\nTrajectory context.\nThe prompt‚Äôs trajectory context is provided via the YAML representation introduced in ¬ß\n3.1\n, which specifies the nearest body landmark for each waypoint. This context allows the LLM to resolve spatial ambiguities. For instance, a user does not need to specify whether they mean their ‚Äúleft‚Äù or ‚Äúright‚Äù elbow, as this information can be inferred from the planned motion. Trajectory context also enables correct understanding of high-level references, like a command relating to an entire ‚Äúarm.‚Äù Even though ‚Äúarm‚Äù is not a specific body joint, the LLM can use trajectory information to deduce which landmarks (e.g., shoulder, elbow, wrist) are relevant to the user‚Äôs command (see examples 3 and 4 in Figure\n3\n).\nConversation context.\nThe prompt also incorporates context from conversation history, specifically the most recent verbal exchange (user utterance, YAML trajectory changes, and robot verbal feedback). This context allows the LLM to correctly interpret follow-up commands. For example, the user may say ‚ÄúGo faster‚Äù and a subsequent command of ‚ÄúA little bit more.‚Äù While vague in isolation, this second utterance can be correctly interpreted by the LLM as another, smaller velocity increase. Retaining conversation history also enables reversing changes with utterances such as ‚ÄúUndo that‚Äù or ‚ÄúForget what I just said.‚Äù This conversational memory allows users to make iterative refinements naturally, as shown in Figure\n1\n.\n3.3.3.\nBidirectional Communication\nBidirectional communication, a key feature of BRIDGE, is achieved by providing verbal feedback for each user utterance. We expect that the format of dialog in general, regardless of the exact content, will enhance the perception of interactivity, which is important for physically assistive scenarios. Specifically, BRIDGE provides two different types of feedback to handle different utterances: offering assurance for any utterances that imply changes to the robot‚Äôs trajectory, or proactively asking a clarifying question when the meaning of an utterance is unclear. The aim of both types of feedback is to fully communicate the robot‚Äôs internal state to the user to support mutual understanding.\nAssurance for change-making utterances.\nWhen a desired modification can be extracted from a user utterance, the LLM also generates one concise sentence to assure the user of the upcoming motion modification (see examples 1‚Äì4 in Figure\n3\n). These assurances are generated without involving technical details, only communicating the magnitude of change when it is easy to interpret from a user perspective (see examples 1 and 3 in Figure\n3\n).\nFigure 4.\nSnapshots from all three tasks implemented for the user study ((1) scratching, (2) feeding, and (3) bathing), with example user utterances (orange) and verbal responses from the robot (yellow) generated by BRIDGE.\nOne image per task showing task details and example verbal exchanges between the user and the robot. In the scratching task, the robot‚Äôs scratching tool is in contact with the user‚Äôs left arm. In the feeding task, the robot is holding a spoonful of applesauce and going towards the user. In the bathing task, the robot is holding a piece of washcloth, in contact with the user‚Äôs left wrist.\nClarifying questions for unclear utterances.\nWhen an utterance is ambiguous or does not map to an adjustable parameter, even considering trajectory and conversational context, BRIDGE seeks clarification from the user. Such utterances could be a general expression of feeling (e.g., ‚ÄúThis doesn‚Äôt feel good‚Äù), or it could come from incorrect speech detections, which are common in real-world environments. In these cases, the system makes no modification to the trajectory and instead produces a clarifying question to ask for more information.\nThese questions are deliberately kept open, rather than suggesting specific options, to ensure that users retain control over expressing adjustments. Upon the user‚Äôs response, a second-stage prompt is then constructed to query the LLM for a new YAML block based on the user‚Äôs clarification. This prompt is designed to be much more concise than the main prompt and contains only the immediate context of the question and answer, to facilitate minimal response latency from the LLM. This process of posing clarifying questions is iterative, until the user‚Äôs intent is no longer ambiguous (as shown in Figure\n2\n). We use a flag in the YAML representation of trajectory modification to indicate whether further clarification is required (see example 5 in Figure\n3\n). These clarifying questions are phrased in plain conversational language, hence easy for users to interpret and act on.\n3.3.4.\nCompactness of Response\nBecause LLMs generate responses autoregressively‚Äîone token at a time‚Äîthe total response latency is directly influenced by the number of output tokens. To minimize latency and maximize response speed, we design the prompt to require the shortest possible response. Specifically, the output is restricted to only the YAML fields that contain modifications (change magnitude\nk\n‚â†\n1\nk\\neq 1\n). Utterances unrelated to trajectory edits (e.g., ‚ÄúI‚Äôm happy today‚Äù) are also ignored entirely.\nMost importantly, we prioritize\nlandmark\nchanges over\nwaypoint\nchanges. A command like ‚ÄúMove faster near my wrist‚Äù could be represented by modifying a list of individual waypoints, but this would result in a verbose output with separate entries for each waypoint. Instead, we prioritize using a landmark change, which not only provides a more compact representation but also offers the smoother exponential decay described in ¬ß\n3.2.2\n. Collectively, these measures ensure the LLM‚Äôs output is concise, minimizing latency and creating a more interactive experience.\n4.\nUser Study\nWe conducted a\nwithin-subjects\nuser study to evaluate BRIDGE‚Äôs efficacy for real-time trajectory modification and to measure the specific contribution of bidirectional verbal feedback. Specifically, we test the following two hypotheses:\nH1 (Efficacy)\n‚Äì Users will be able to use BRIDGE to effectively modify the position, velocity, and force of planned trajectories for different physically assistive tasks.\nH2 (Contribution of Bidirectional Feedback)\n‚Äì Bidirectional verbal feedback from the robot will facilitate a more interactive and transparent experience for users, compared to a no-feedback method with the same ability to make trajectory changes.\n4.1.\nTasks and Implementation\nWe designed three different assistive tasks, where each task focuses on a different aspect of the motion (position, velocity, or force) for the user to make modifications to. All tasks were performed autonomously with a Stretch 3 robot, a mobile manipulator with a 5-DoF arm and a gripper.\n(1)\nScratching (position)\n: The robot held a 3D-printed scratching tool and began scratching near the participant‚Äôs left wrist. Participants were given the goal of modifying the position of scratching to an area on the upper forearm, indicated by stickers placed on the participant‚Äôs arm.\n(2)\nFeeding (velocity)\n: The robot held a spoon and scooped from a bowl of applesauce to feed the participant a total of three times. Participants were given the goal of modifying the velocity of feeding to a level they were comfortable with. To encourage participants to actively adjust speed, the initial motion was intentionally designed with a slow velocity.\n(3)\nBathing (force)\n: The robot held a piece of dry washcloth and wiped the participant‚Äôs left forearm from the wrist to the elbow, a total of four times. Participants were given the goal of modifying the force of bathing to their liking. Given the subjective nature of force preference, the primary purpose of this task was to ensure participants felt empowered to make adjustments, rather than to converge on a specific target force.\nFigure\n4\nshows one snapshot from each task during the user study, along with sample user utterances and the corresponding response from the robot. Despite each task focusing on one motion aspect, participants were welcomed to change more than one aspect in each task, or even in the same utterance. We used Microsoft Azure‚Äôs speech-to-text service to transcribe user speech. Once the user finished speaking, the service sent the complete utterance to the robot, which then paused its motion and queried the LLM (GPT-4.1) to generate desired modifications. The query would take approximately 1-2 seconds, an interval kept short by our compact YAML representation. When the query finished, the robot updated its trajectory and then restarted its motion along the new trajectory.\nMultiple utterances in the same interaction were treated as cumulative, so e.g. the second utterance would act on the modified trajectory from the first utterance. The maximum velocity of the robot end-effector was bound to 0.1‚Äâm/s for safety.\nFigure 5.\nBox plots showing the distribution of survey responses across all participants and tasks. After fitting ordinal mixed-effects models to each question, we conduct Wald tests to assess pairwise differences between BRIDGE and both the no communication baseline and the unidirectional communication ablation. ‚Äúns‚Äù denote lack of significant difference, and asterisks denote significance levels (\np\n<\n0.05\np<0.05\n,\np\n<\n0.01\np<0.01\n,\np\n<\n0.001\np<0.001\n,\np\n<\n0.0001\np<0.0001\n).\nOur method is rated highly and outperforms the no-modification baseline with significant difference on all six Likert items. Compared with the unidirectional ablation, our method is rated significantly higher in perceived interactivity.\n4.2.\nParticipants and Setting\nWe recruited\nn\n=\n18\nn=18\nolder adults from a local independent living community (7 male/11 female; age range 74‚Äì90 with\nM\n=\n82.1\nM=82.1\nand\nS\n‚Äã\nD\n=\n4.4\nSD=4.4\n). Only two participants reported any experience with autonomous robots in general (levels 2 and 3 on a 5-point scale; all other participants reported no experience).\nThe study was conducted in an empty apartment within the community, with all tasks completed in a single session. The study design, the experiment protocol, and the consent forms received approval from our Institutional Review Board.\n4.3.\nCommunication Strategies and Procedure\nTo test our hypotheses, we designed three communication strategies that participants experienced in a within-subjects manner:\n‚Ä¢\nBRIDGE\n: This is our full proposed bidirectional communication system. The robot would modify its trajectory according to the user‚Äôs verbal command and provide verbal feedback in the form of either an assurance or a clarifying question.\n‚Ä¢\nUnidirectional communication\n(Ablation): This strategy is designed to isolate the effect of the robot‚Äôs verbal feedback. The robot can still modify its trajectory based on user commands, but it provides no verbal assurance or clarifying questions. If an utterance is unclear, the robot would simply pause and then resume its previous motion without change.\n‚Ä¢\nNo-modification strategy\n(Baseline): In this strategy, the robot would still listen for user speech and pause its motion, but it would not make any modifications to its trajectory. After the standard pause, it would always resume its original, unmodified path. This baseline is designed to measure the efficacy of trajectory changes in our method and the ablation.\nEach participant completed nine interactions with the robot. Each interaction (trial) consisted of performing one of the three assistive tasks (scratching, feeding, or bathing) with one of the three communication strategies above. A participant would complete all three trials for a single task‚Äîhence experiencing all communication strategies for the task‚Äîbefore proceeding to the next task. To mitigate ordering effects, we counterbalanced the sequence in which the three tasks were presented to each participant, as well as the order of the three communication strategies within each task.\n4.4.\nMeasures\nAfter each trial, the participants were asked to answer a survey with the following Likert items on a 7-point scale (7 for strongly agree, 1 for strongly disagree):\nL1.\n(Enjoyment)\nI enjoyed the interaction.\nL2.\n(Perceived Interactivity)\nThe robot felt interactive and responsive.\nL3.\n(Grounding)\nI was confident the robot understood what I meant.\nL4.\n(Transparency)\nI could tell exactly what changed in the robot‚Äôs motion after my input.\nL5.\n(Perceived Control)\nI felt in control of the robot‚Äôs motions.\nL6.\n(Task success)\nAt the end of the trial, I was able to achieve the overall task objective.\nThe italicized terms inside parentheses denote the high-level concepts each item was designed to evaluate and were not shown to the participants.\nWe also logged the following data for quantitative analysis: the content of each user utterance, the LLM‚Äôs full response (YAML and verbal feedback) and processing latency (i.e., the motion pause duration), and the interaction timestamp for each utterance.\n5.\nResults and Discussion\nFigure\n5\nshows the distribution of all participants‚Äô responses to the Likert-item questions. We fit ordinal mixed-effects (proportional-odds) models with random intercepts for participant, task, and their interaction effects. Communication strategy was held as a fixed effect. Omnibus effects were assessed with a likelihood-ratio test. Pairwise differences between our method and the baseline or the ablation were evaluated with Wald tests from the fitted model, with Holm adjustment for multiple comparisons.\nFigure 6.\nEfficacy of trajectory modifications, averaged over all participants. The plots show changes in position for the scratching task (top) and velocity for the feeding task (bottom) over normalized task progression. They compare the communication strategies that allow modifications (BRIDGE and the unidirectional ablation, solid lines) against the no-modification baseline (dotted line). The snapshots on the right visualize the difference in robot state, comparing a trial with modifications to one without at a representative point in the interaction.\nCurves for strategies with modification and the no-modification baseline clearly diverge for both parameters plotted. Pictures on the right visualize closer proximity to target in the scratching task and higher velocity in the feeding task.\nFigure\n6\nprovides objective evidence for our efficacy hypothesis (H1), illustrating how users successfully modified the robot‚Äôs motion to achieve the task goals. As per our task design, we visualize the two tasks that were designed with clear, objective targets. The plots show the robot‚Äôs state over the normalized task progression: the top plot shows the robot‚Äôs proximity to the target position for the scratching task, while the bottom plot shows the robot‚Äôs velocity for the feeding task, both averaged across all participants. We do not visualize the results for the bathing task since controlling the force is guided by subjective preference rather than a desired target. In the visualized tasks, the successful modifications are evident in the clear divergence of the trajectories for the modifiable strategies (BRIDGE and the unidirectional ablation, solid lines) compared to the static, preplanned path of the no-modification baseline (dotted line). Variations in the baseline reflect the designed dynamics of the original trajectory. To complement the plotted data, the snapshots on the right of Figure\n6\nvisualize the effect of user commands, where colored stars and arrows (red for without modification, green for with modification) visually depict the difference in robot state at a representative point in the interaction.\n5.1.\nModification Efficacy\nAs shown in the top plot for scratching in Figure\n6\n, users were able to command the robot to scratch at the target location under the bidirectional and unidirectional strategies (as proximity goes towards zero). In contrast, under the no-modification baseline, the robot‚Äôs motion was unaffected by user commands and simply followed its original, pre-planned trajectory. The accompanying snapshots on the top-right of Figure\n6\nprovide a visual confirmation, showing the robot successfully reaching the target area (marked with star) in trial with modifications. Similarly, the bottom plot for feeding in Figure\n6\nshows that users were able to command substantial increases in the robot‚Äôs speed under the bidirectional and unidirectional strategies, increasing it by a factor of three to four compared to the cautious initial trajectory. In contrast, the velocity profile in the baseline method remained unchanged and followed the preplanned trajectory. This demonstrates the effectiveness of our underlying speech-to-modification system for changing different aspects of a robot‚Äôs trajectory.\nTo complement the objective data, we analyzed participants‚Äô ratings of efficacy to the Likert items as shown in Figure\n5\n. Participants reported a high degree of task success (L6) and perceived control (L5) when using BRIDGE or the unidirectional ablation. Statistical tests revealed that both strategies were rated significantly higher than the baseline for both L6 (\np\n<\n0.0001\np<0.0001\n) and L5 (\np\n<\n0.0001\np<0.0001\n). Importantly, the high task success ratings hold across all three tasks, confirming the effectiveness of our method for modifying the intended aspect of each task: position for scratching, velocity for feeding, and force for bathing (see Figure\n7\n).\nMoreover, there was no significant difference in task success or perceived control between our bidirectional method and the unidirectional ablation. This finding confirms that both strategies were equally effective at achieving task goals of modifying trajectories, which carries the crucial implication that any differences observed in other subjective ratings (discussed in ¬ß\n5.2\n) can be attributed directly to the presence of the robot‚Äôs verbal feedback.\nFinally, the system‚Äôs efficiency was confirmed by its low latency. The average time from a user finishing an utterance to the robot executing the modification was measured to be 1.7‚Äâs across all trials with modifications, which consists of 1.3‚Äâs for the LLM query and 0.4‚Äâs for the motion planner. This rapid response time directly results from our emphasis on compact representations in LLM outputs and confirms the viability of our system for real-time interaction.\nIn summary, these three sources of evidence‚Äîthe objective success in modifying trajectories, the high ratings of task success and user perceived control, and the low system latency‚Äîcollectively support our\nefficacy hypothesis (H1)\n. The results confirm that our system is effective at performing real-time trajectory modifications.\n5.2.\nUser Perception and Feedback\nTo test H2, we analyzed the participants‚Äô perception of their interactions as reported in survey ratings (shown in Figure\n5\n). BRIDGE was rated significantly higher than the no-modification baseline across all measures (\np\n<\n0.0001\np<0.0001\n).\nMore importantly, when compared to the unidirectional ablation, BRIDGE was perceived as significantly more interactive (L2,\np\n<\n0.001\np<0.001\n). Participants also expressed higher confidence that the robot understood them (L3,\np\n<\n0.05\np<0.05\n) and found it easier to discern changes in the robot‚Äôs motion (L4,\np\n<\n0.05\np<0.05\n).\nThis quantitative preference is supported by qualitative feedback gathered in post-study debriefings. When asked which communication strategy they preferred, all participants selected the bidirectional communication by BRIDGE and provided insight into the two types of verbal responses: assurances and clarifying questions.\nVerbal assurances elicited varied preferences among the participants. Many appreciated its value: P15 noting that with assurances, ‚Äúyou know the robot has interpreted what you want.‚Äù P18 felt that assurances made them ‚Äúprepared for what‚Äôs going to happen.‚Äù However, some participants took a more indifferent stance: P5 said ‚ÄúIt doesn‚Äôt matter‚Äù whether the assurances were communicated, and P8 stated that ‚Äú[the robot] didn‚Äôt need to repeat to [them] what it‚Äôs going to do.‚Äù P13 offered the nuance that while ‚Äúreassuring‚Äù initially, assurances could become ‚Äúannoying‚Äù in long-term use.\nIn contrast, participants were universally positive about the merit of clarifying questions. P5 said ‚ÄúIf [the robot] doesn‚Äôt understand, then [they] want to know,‚Äù and P15 noted that without clarifying questions, they ‚Äúdon‚Äôt know if [they] said something that the robot wasn‚Äôt understanding.‚Äù P13 stated that clarifying questions provide a method for them to ‚Äúlearn from [the robot].‚Äù Throughout the user study, BRIDGE generated a clarifying question in response to 17% of all user utterances, demonstrating its role as a key mechanism for resolving ambiguity.\nOverall, this varied reception to different types of feedback suggests an opportunity for personalization, such as tuning the frequency or verbosity of verbal feedback to user preference and adapting over time with familiarity.\nIn summary, through both the survey analysis and qualitative feedback, results show that BRIDGE was strongly preferred over the unidirectional ablation, fostering a more interactive and transparent experience, hence supporting\nH2\n.\n5.3.\nPerceptual Bias and the Need for Transparency\nFigure 7.\nBox plot showing the distribution of survey responses for perceived task success (L6), separate for each task. ‚Äúns‚Äù denote lack of significant difference, and asterisks denote significance levels (\n‚àó\n*\n‚àó\n*\n‚àó\n*\n‚àó\n*\nmeans\np\n<\n0.0001\np<0.0001\n).\nOur bidirectional method and the unidirectional ablation both receive high task success ratings for all tasks, but for the no-modification baseline, it is rated the highest and comparable to the other strategies in bathing, rated less in feeding, and least in scratching. Significant differences are observed for both scratching and feeding, but not for bathing.\nAn interesting finding emerged when we analyzed the perceived task success (L6) in a per-task manner, as shown in Figure\n7\n. The data reveals a stark contrast between tasks with objective versus subjective goals.\nThe scratching task had a clear, objective target, and participants correctly identified the baseline‚Äôs failure to react to their inputs, rating its success very low (upper quartile of 2). This was significantly different from our method, which was rated very successful.\nHowever, for the bathing task, where the ‚Äúcorrect‚Äù force is subjective, participants reported a high degree of success (lower quartile of 5) even for the baseline method, with no statistically significant difference from our method. Such ratings suggest a powerful perceptual bias: when a change is difficult to perceive and driven by user command, participants tend to believe their command was successful, even when the robot‚Äôs behavior remained the same. This interpretation is supported by qualitative feedback, where participants claimed they perceived commanded force changes, and sometimes velocity changes in the feeding task too, even when experiencing the baseline method. Our observation finds psychological grounding in research on causal attribution\n(Kelley and Michela,\n1980\n)\nand the illusion of control\n(Langer,\n1975\n)\n, where people may infer causal relationship between their own action and a temporally subsequent event.\nThis finding underscores the importance of transparency in physical human-robot interactions. When a user‚Äôs perception can diverge from the robot‚Äôs actual behavior, the liability is on the robot to provide clear and transparent feedback. Without such grounding, the user may develop an inaccurate mental model of the system‚Äôs capabilities, leading to frustration and mistrust in the long run.\nThis result further justifies the need for clarifying questions as well. Between BRIDGE and the unidirectional ablation, a major functional difference is that utterances that trigger a clarifying question in BRIDGE will not change the robot‚Äôs motion in the ablation. Despite the two both having high perceived success, the ablation places the burden on users to notice and correct errors‚Äîespecially difficult given the perceptual bias we observe. With clarifying questions, such uncertainty surfaces immediately, and users enjoy a more intuitive experience, as reflected by qualitative comments.\n6.\nConclusion\nWe present BRIDGE, a framework for bidirectional human-robot communication during physically assistive scenarios, where users can verbally modify a robot‚Äôs trajectory in real time across the motion aspects of position, velocity, and force. Our method leverages an LLM to efficiently translate any user utterance into compactly represented trajectory modifications while simultaneously generating appropriate verbal feedback‚Äîeither as an assurance of a desired change or as a clarifying question. A user study with 18 older adults demonstrates the efficacy of our method for trajectory modifications and proves the need for the robot‚Äôs bidirectional verbal feedback, which significantly enhances user experience by improving perceived interactivity and transparency.\nFuture work.\nOur work has a few limitations that could open avenues for future research. First, our user study only involved a single session per participant, so how to appropriately structure bidirectional verbal response in a long term deployment scenario remains unexplored. Additionally, our system can modify a range of physically assistive planned trajectories assuming the user stays stationary, but how to effectively adapt BRIDGE to real-time policies or general manipulation settings remain open questions. Moreover, testing BRIDGE on more dynamic tasks and scenarios may offer additional insight into the value of each component of our system. Lastly, spoken language may not be the most intuitive form of communication at all times, and other methods of communication (e.g. tactile, gestures) could be considered as well.\nAcknowledgements.\nThis work is supported by Honda Research Institute USA.\nReferences\n(1)\nAlhaji et¬†al\n.\n(2021)\nBasel Alhaji, Michael Prilla, and Andreas Rausch. 2021.\nTrust dynamics and verbal assurances in human robot physical collaboration.\nFrontiers in Artificial Intelligence\n4 (2021), 703504.\nAllgeuer et¬†al\n.\n(2024)\nPhilipp Allgeuer, Hassan Ali, and Stefan Wermter. 2024.\nWhen robots get chatty: Grounding multimodal human-robot conversation and collaboration. In\nInternational Conference on Artificial Neural Networks\n. Springer, 306‚Äì321.\nAlonso et¬†al\n.\n(2021)\nRuben Alonso, Emanuele Concas, and Diego Reforgiato¬†Recupero. 2021.\nAn abstraction layer exploiting voice assistant technologies for effective human‚ÄîRobot interaction.\nApplied Sciences\n11, 19 (2021), 9165.\nBhattacharjee et¬†al\n.\n(2020)\nTapomayukh Bhattacharjee, Ethan¬†K Gordon, Rosario Scalise, Maria¬†E Cabrera, Anat Caspi, Maya Cakmak, and Siddhartha¬†S Srinivasa. 2020.\nIs more autonomy always better? exploring preferences of users with mobility impairments in robot-assisted feeding. In\nProceedings of the 2020 ACM/IEEE international conference on human-robot interaction\n. 181‚Äì190.\nCui et¬†al\n.\n(2023)\nYuchen Cui, Siddharth Karamcheti, Raj Palleti, Nidhya Shivakumar, Percy Liang, and Dorsa Sadigh. 2023.\nNo, to the right: Online language corrections for robotic manipulation via shared autonomy. In\nProceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction\n. 93‚Äì101.\nErickson et¬†al\n.\n(2018)\nZackory Erickson, Henry¬†M Clever, Greg Turk, C¬†Karen Liu, and Charles¬†C Kemp. 2018.\nDeep haptic model predictive control for robot-assisted dressing. In\n2018 IEEE international conference on robotics and automation (ICRA)\n. IEEE, 4437‚Äì4444.\nFischer et¬†al\n.\n(2018)\nKerstin Fischer, Hanna¬†Mareike Weigelin, and Leon Bodenhagen. 2018.\nIncreasing trust in human‚Äìrobot medical interactions: effects of transparency and adaptability.\nPaladyn, Journal of Behavioral Robotics\n9, 1 (2018), 95‚Äì109.\nGopinathan et¬†al\n.\n(2017)\nSugeeth Gopinathan, Sonja¬†K √ñtting, and Jochen¬†J Steil. 2017.\nA user study on personalized stiffness control and task specificity in physical human‚Äìrobot interaction.\nFrontiers in Robotics and AI\n4 (2017), 58.\nGordon et¬†al\n.\n(2024)\nEthan¬†K Gordon, Rajat¬†Kumar Jenamani, Amal Nanavati, Ziang Liu, Daniel Stabile, Xilai Dai, Tapomayukh Bhattacharjee, Tyler Schrenk, Jonathan Ko, Haya Bolotski, et¬†al\n.\n2024.\nAn adaptable, safe, and portable robot-assisted feeding system. In\nCompanion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction\n. 74‚Äì76.\nGupte et¬†al\n.\n(2023)\nVivek Gupte, Dan¬†R Suissa, and Yael Edan. 2023.\nOptometrist‚Äôs Algorithm for Personalizing Robot-Human Handovers. In\n2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)\n. IEEE, 2366‚Äì2372.\nIntelligence et¬†al\n.\n(2025)\nPhysical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et¬†al\n.\n2025.\nœÄ\n0.5\n\\pi_{0.5}\n: a Vision-Language-Action Model with Open-World Generalization.\narXiv preprint arXiv:2504.16054\n(2025).\nJenamani et¬†al\n.\n(2025)\nRajat¬†Kumar Jenamani, Tom Silver, Ben Dodson, Shiqin Tong, Anthony Song, Yuting Yang, Ziang Liu, Benjamin Howe, Aimee Whitneck, and Tapomayukh Bhattacharjee. 2025.\nFEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization.\narXiv preprint arXiv:2506.14968\n(2025).\nKaramcheti et¬†al\n.\n(2022)\nSiddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. 2022.\nLILA: Language-informed latent actions. In\nConference on robot learning\n. PMLR, 1379‚Äì1390.\nKelley and Michela (1980)\nHarold¬†H Kelley and John¬†L Michela. 1980.\nAttribution theory and research.\nAnnual review of psychology\n31, 1 (1980), 457‚Äì501.\nKhatib (1986)\nOussama Khatib. 1986.\nReal-time obstacle avoidance for manipulators and mobile robots.\nThe International Journal of Robotics Research (IJRR)\n5, 1 (1986), 90‚Äì98.\nKim et¬†al\n.\n(2024)\nMoo¬†Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et¬†al\n.\n2024.\nOpenVLA: An open-source vision-language-action model.\narXiv preprint arXiv:2406.09246\n(2024).\nLanger (1975)\nEllen¬†J Langer. 1975.\nThe illusion of control.\nJournal of personality and social psychology\n32, 2 (1975), 311.\nLiang et¬†al\n.\n(2024)\nJacky Liang, Fei Xia, Wenhao Yu, Andy Zeng, Montserrat¬†Gonzalez Arenas, Maria Attarian, Maria Bauza, Matthew Bennice, Alex Bewley, Adil Dostmohamed, et¬†al\n.\n2024.\nLearning to learn faster from human feedback with language model predictive control.\narXiv preprint arXiv:2402.11450\n(2024).\nLima et¬†al\n.\n(2021)\nMaria¬†R Lima, Maitreyee Wairagkar, Manish Gupta, Ferdinando¬†Rodriguez y Baena, Payam Barnaghi, David¬†J Sharp, and Ravi Vaidyanathan. 2021.\nConversational affective social robots for ageing and dementia support.\nIEEE Transactions on Cognitive and Developmental Systems\n14, 4 (2021), 1378‚Äì1397.\nLynch et¬†al\n.\n(2023)\nCorey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. 2023.\nInteractive language: Talking to robots in real time.\nIEEE Robotics and Automation Letters\n(2023).\nMahmood et¬†al\n.\n(2024)\nAmama Mahmood, Junxiang Wang, and Chien-Ming Huang. 2024.\nSituated Understanding of Errors in Older Adults‚Äô Interactions with Voice Assistants: A Month-Long, In-Home Study.\narXiv preprint arXiv:2403.02421\n(2024).\nMandi et¬†al\n.\n(2024)\nZhao Mandi, Shreeya Jain, and Shuran Song. 2024.\nRoco: Dialectic multi-robot collaboration with large language models. In\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n. IEEE, 286‚Äì299.\nMehrotra et¬†al\n.\n(2024)\nSiddharth Mehrotra, Chadha Degachi, Oleksandra Vereschak, Catholijn¬†M Jonker, and Myrthe¬†L Tielman. 2024.\nA systematic review on fostering appropriate trust in Human-AI interaction: Trends, opportunities and challenges.\nACM Journal on Responsible Computing\n1, 4 (2024), 1‚Äì45.\nNanavati et¬†al\n.\n(2023)\nAmal Nanavati, Vinitha Ranganeni, and Maya Cakmak. 2023.\nPhysically assistive robots: A systematic review of mobile and manipulator robots that physically assist people with disabilities.\nAnnual Review of Control, Robotics, and Autonomous Systems\n7 (2023).\nPadmanabha et¬†al\n.\n(2024)\nAkhil Padmanabha, Jessie Yuan, Janavi Gupta, Zulekha Karachiwalla, Carmel Majidi, Henny Admoni, and Zackory Erickson. 2024.\nVoicePilot: Harnessing LLMs as speech interfaces for physically assistive robots. In\nProceedings of the 37th Annual ACM Symposium on User Interface Software and Technology\n. 1‚Äì18.\nPoirier et¬†al\n.\n(2019)\nSamuel Poirier, Fran√ßois Routhier, and Alexandre Campeau-Lecours. 2019.\nVoice control interface prototype for assistive robots for people living with upper limb disabilities. In\n2019 IEEE 16th international conference on rehabilitation Robotics (ICORR)\n. IEEE, 46‚Äì52.\nPradhan et¬†al\n.\n(2020)\nAlisha Pradhan, Amanda Lazar, and Leah Findlater. 2020.\nUse of intelligent voice assistants by older adults with low technology use.\nACM Transactions on Computer-Human Interaction (TOCHI)\n27, 4 (2020), 1‚Äì27.\nPulikottil et¬†al\n.\n(2018)\nTerrin¬†Babu Pulikottil, Marco Caimmi, Maria¬†Grazia D‚ÄôAngelo, Emilia Biffi, Stefania Pellegrinelli, and Lorenzo¬†Molinari Tosatti. 2018.\nA voice control system for assistive robotic arms: preliminary usability tests on patients. In\n2018 7th IEEE International Conference on Biomedical Robotics and Biomechatronics (Biorob)\n. IEEE, 167‚Äì172.\nRudzicz et¬†al\n.\n(2015)\nFrank Rudzicz, Rosalie Wang, Momotaz Begum, and Alex Mihailidis. 2015.\nSpeech interaction with personal assistive robots supporting aging at home for individuals with Alzheimer‚Äôs disease.\nACM Transactions on Accessible Computing (TACCESS)\n7, 2 (2015), 1‚Äì22.\nSharma et¬†al\n.\n(2022)\nPratyusha Sharma, Balakumar Sundaralingam, Valts Blukis, Chris Paxton, Tucker Hermans, Antonio Torralba, Jacob Andreas, and Dieter Fox. 2022.\nCorrecting robot plans with natural language feedback.\narXiv preprint arXiv:2204.05186\n(2022).\nShi et¬†al\n.\n(2024)\nLucy¬†Xiaoyang Shi, Zheyuan Hu, Tony¬†Z Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, and Chelsea Finn. 2024.\nYell at your robot: Improving on-the-fly from language corrections.\narXiv preprint arXiv:2403.12910\n(2024).\nSong et¬†al\n.\n(2022)\nYao Song, Yanpu Yang, and Peiyao Cheng. 2022.\nThe investigation of adoption of voice-user interface (VUI) in smart home systems among Chinese older adults.\nSensors\n22, 4 (2022), 1614.\nSpitale et¬†al\n.\n(2023)\nMicol Spitale, Silvia Silleresi, Franca Garzotto, and Maja¬†J Matariƒá. 2023.\nUsing socially assistive robots in speech-language therapy for children with language impairments.\nInternational Journal of Social Robotics\n15, 9 (2023), 1525‚Äì1542.\nWang et¬†al\n.\n(2024)\nHuaxiaoyue Wang, Kushal Kedia, Juntao Ren, Rahma Abdullah, Atiksh Bhardwaj, Angela Chao, Kelly¬†Y Chen, Nathaniel Chin, Prithwish Dan, Xinyi Fan, et¬†al\n.\n2024.\nMosaic: A modular system for assistive and interactive cooking.\narXiv preprint arXiv:2402.18796\n(2024).\nWang et¬†al\n.\n(2025)\nJunxiang Wang, Emek¬†Barƒ±≈ü K√º√ß√ºktabak, Rana¬†Soltani Zarrin, and Zackory Erickson. 2025.\nCoRI: Communication of Robot Intent for Physical Human-Robot Interaction. In\n9th Annual Conference on Robot Learning\n.\nYang et¬†al\n.\n(2024)\nZhaojing Yang, Miru Jun, Jeremy Tien, Stuart¬†J. Russell, Anca Dragan, and Erdem Biyik. 2024.\nTrajectory Improvement and Reward Learning from Comparative Language Feedback. In\n8th Annual Conference on Robot Learning\n.\nYu et¬†al\n.\n(2025)\nAlbert Yu, Chengshu Li, Luca Macesanu, Arnav Balaji, Ruchira Ray, Raymond Mooney, and Roberto Mart√≠n-Mart√≠n. 2025.\nMixed-Initiative Dialog for Human-Robot Collaborative Manipulation.\narXiv preprint arXiv:2508.05535\n(2025).\nZha et¬†al\n.\n(2024)\nLihan Zha, Yuchen Cui, Li-Heng Lin, Minae Kwon, Montserrat¬†Gonzalez Arenas, Andy Zeng, Fei Xia, and Dorsa Sadigh. 2024.\nDistilling and retrieving generalizable knowledge for robot manipulation via language corrections. In\n2024 IEEE international conference on robotics and automation (ICRA)\n. IEEE, 15172‚Äì15179.\nAppendix A\nFull LLM Prompt\nMost of the LLM prompt we used is fixed, but there are sections towards the end that we dynamically populate based on the trajectory, user utterance, as well as conversation history. We first present the fixed content in the prompt, where we mark the areas to be dynamically populated with bold italicized font, and we give some examples for those dynamic contents afterwards.\nA.1.\nFixed content\nYou are a robot with a planned trajectory defined by a sequence of waypoints to move to for interacting with what you see around you. Each waypoint specifies the position, velocity, and force (pressure) of your gripper that interacts with a person. You will be given:\n- A YAML dictionary of local waypoints with their nearest body landmark.\n- A list of all body landmarks that you detected.\n- The most recent sequence of user utterances and your own prior YAML responses, if they exist.\n- The user‚Äôs current utterance made while you are moving.\nBased on the user‚Äôs current utterance, you should output a YAML block in the below format\nyaml\nwaypoint [x]:\n    force: [multiplier]\n    velocity: [multiplier]\n...\nglobal:\n    clarification: true | false\n    force:[multiplier]\n    stop: true | false\n    velocity: [multiplier]\n[body landmark]:\n    attract: [multiplier]\n    force: [multiplier]\n    velocity: [multiplier]\n...\nWhere:\n- Each numeric field represents a multiplier relative to the current value:\n- All values are always ¬ø 0\n- Values greater than 1.0 mean the person wants that quantity to increase (e.g., ‚Äúfaster‚Äù, ‚Äúfirmer‚Äù, ‚Äúpush harder‚Äù) or attract the robot.\n- Values between 0.0 and 1.0, always represented as a fraction, mean the person wants that quantity to decrease (e.g., ‚Äúslower‚Äù, ‚Äúgentler‚Äù, ‚Äúless force‚Äù) or repel the robot. These values should be represented as a fraction with a numerator of 1 (e.g., 1/2.0).\n- 1.0 means no change.\n- ‚Äò\nstop: true\n‚Äô means the user has requested that the robot immediately stop moving.\n- ‚Äò\nclarification: true\n‚Äô means the robot requires a follow-up clarification.\nFor the attract, velocity, and force fields, if the request‚Äôs language implies gradation (e.g., ‚Äújust a bit faster‚Äù or ‚Äúway too rough‚Äù), adjust the magnitude accordingly:\n- Default change in intensity is a multiplier of 2.0 (double) for increasing change or 1/2.0 (half) for decreasing change.\n- The max increase should be around 3.0, and max decrease around 1/3.0.\n- Infer how strong or subtle the change is from modifiers like ‚Äúa little‚Äù, ‚Äúslightly‚Äù, ‚Äúa lot‚Äù, ‚Äúway too‚Äù, ‚Äúmuch more‚Äù, etc. and choose an appropriate multiplier based on this reasoning knowing that the default is to double of half. Use your own judgment to map qualitative descriptions into meaningful quantitative adjustments.\nThe following is a list of all detected body landmarks: left wrist, right wrist, left elbow, right elbow, left shoulder, right shoulder, mouth\nAdjustments should be categorized into one of three types:\n1. Body Landmarks: When a request references a specific body landmark, update only its corresponding entry or entires included within that body landmark (e.g. ‚Äúfoot‚Äù and ‚Äúknee‚Äù for ‚Äúleg‚Äù). Body landmark entries also include an ‚Äú\nattract\n‚Äù field to reflect movement preferences, where:\n- attract ¬ø 1.0: move closer to the body landmark (e.g., ‚Äústay closer to my left side‚Äù)\n- attract ¬° 1.0: move farther away (repel) (e.g., ‚Äústay away from my knee‚Äù)\n- attract = 1.0: no change.\n2. Global: When a request affects the overall trajectory (e.g. ‚Äúgo faster‚Äù, ‚Äúuse less pressure‚Äù, ‚Äúfinish this task slower‚Äù).\n3. Local Waypoints: When a request targets a specific section of the trajectory without mentioning a landmark (e.g., ‚Äúgo slower on the way toward me‚Äù).\nRules for applying changes:\n- Body landmark references: Only modify the landmark entry. Assume that the body landmarks listed in the given YAML are the only relevant entries.\n- Local vs. Global: Default to global unless the request clearly refers to a specific part of the trajectory.\n- Multiple changes: Multiple values can be modified by one request, but do not apply both local and global changes for the same quantity unless the request clearly calls for both.\n- Stop parameter: only set ‚Äôstop‚Äô to true if the user explicitly says ‚Äústop,‚Äù and treat phrases like ‚Äústop here‚Äù as positional adjustments rather than a global stop.\n- Recognition errors: The utterance was transcribed by a speech-to-text service, so if the utterance seems incomplete, vague, or misrecognized within this context, make a best-effort guess based on nearby words and the current trajectory to resolve any recognition errors.\n- Irrelevant utterances: If the utterance is still irrelevant after resolving recognition errors, then ignore it if it is unlikely to be directed at you or that do not clearly include a directive about how the robot‚Äôs position, velocity, or force (e.g., ‚ÄúI‚Äôm sore‚Äù, ‚ÄúI‚Äôm happy today‚Äù, and ‚ÄúI feel fast right now‚Äù).\n- Consider the trajectory stage based on direction:\n- Waypoints that are not near a body landmark but come before waypoints that are represent the robot moving toward the person for interaction.\n- Waypoints in contact with or near a body landmark indicate interaction.\n- Waypoints that are no longer near a body landmark but come after waypoints that were represent the robot moving away from the person following interaction.\n- Unclear utterances: If the utterance is unclear (‚ÄúThis doesn‚Äôt feel good‚Äù) and does not specify a target parameter (force, velocity, attract, stop) or magnitude/scope of change, do not apply a change and instead output a short and open-ended clarifying question. Do not hint at possible parameters (speed, position, force, stop) or suggest specific adjustments in your question unless the utterance clearly specifies it. If a landmark is mentioned in the utterance, then include it in your question.\n- Always set the field ‚Äò\nclarification: true\n‚Äô if the user‚Äôs utterance was unclear and requires the follow-up clarification prompt. Otherwise set ‚Äò\nclarification: false\n‚Äô.\n- References to previous utterances: The user may make requests that refer to previous changes (e.g., ‚ÄúActually, a little more‚Äù, ‚ÄúForget what I just said‚Äù, etc.). In these cases, user the prior utterance(s) to interpret the intent:\n- If the utterance lacks a clear target but refers to the last modified quantity/location (‚Äúa little more‚Äù, ‚Äúreduce it slightly‚Äù), apply the change to that same parameter. This should essentially amplify or deamplify the previous change. Infer how strong or subtle the multiplier should be with respect to the previous value.\n- Undo: If the utterance implies an undo of the previous change, revert to the value before the last change by applying the reciprocal multiplier.\n- ‚ÄúUp‚Äù and ‚Äúdown‚Äù position reasoning: The user may describe adjustments using directional terms such as ‚Äúup‚Äù, ‚Äúdown‚Äù, ‚Äúhigher‚Äù, ‚Äúlower‚Äù, etc. In these cases:\n1. Anchor point: Identify the current body landmark from the YAML block listing the nearest landmark to each waypoint.\n2. Target search: Reference the list of detected landmarks to select the target landmark:\n- Relative ordering rule:\n- For limbs: move stepwise along the natural distal-to-proximal order for ‚Äúup‚Äù and the reverse for ‚Äúdown‚Äù movement (e.g., hands to wrist to elbow to shoulder, foot to ankle to knee).\n- For torso/head: move from lower to upper for ‚Äúup‚Äù and the reverse for ‚Äúdown‚Äù movement.\n- The target should be the immediate anatomical landmark above or below the anchor, depending on the utterance.\n3. Output the target landmark and update the changes accordingly. Unless the user specifies otherwise, treat relative position changes as attraction changes.\nExample 1:\n- History:\nPrevious Utterance: ‚ÄúGo further from my mouth.‚Äù\nPrevious Response:\nmouth:\n            attract: 1/2.0\n- Current utterance: ‚ÄúUndo that.‚Äù\n- Output:\nmouth:\n        attract: 2.0\n    I‚Äôm coming closer to your mouth to undo the\n      previous change.\nExample 2:\n- History:\nPrevious Utterance: ‚ÄúApply less force around my knee.‚Äù\nPrevious Response:\n[left/right] knee:\n            force: 1/2.0\n- Current utterance: ‚ÄúLess.‚Äù\n- Output:\n[left/right] knee:\n        force: 1/2.0\n    I‚Äôm applying even less pressure to your knee.\nThe following is the given YAML block:\n[Trajectory represented in YAML]\nThe following is the user‚Äôs utterance:\n[Detected user utterance]\nThe following is the history of previous utterances and responses, if any:\n[Conversation history]\nOutput your response in the form of the given YAML block with any necessary adjustment changes based on the person‚Äôs utterance. Only include waypoints and fields that changed in the response YAML (so nothing with a value of 1.0). If nothing changes, output nothing. Make sure the output is wrapped in yaml, with\nyaml <your yaml block>\n. After outputting the YAML, also output a very concise, natural-sounding, single sentence that confirms the most significant change being made to the robot‚Äôs trajectory (e.g., ‚ÄúI‚Äôm decreasing the pressure by half.‚Äù). Only include this sentence if a change is made. Do not explain or justify it.\nA.2.\nDynamic Content\nThere are three sections in the prompt to be filled in dynamically based on the inputs. First of all, ‚ÄúTrajectory represented in YAML‚Äù refers to the YAML representation of the input trajectory, and below we give one example of a full trajectory for the bathing task:\nwaypoint 1:\n    nearest landmark: none\nwaypoint 2:\n    nearest landmark: none\nwaypoint 3:\n    nearest landmark: left wrist\nwaypoint 4:\n    nearest landmark: left elbow\nwaypoint 5:\n    nearest landmark: none\nwaypoint 6:\n    nearest landmark: none\nwaypoint 7:\n    nearest landmark: left wrist\nwaypoint 8:\n    nearest landmark: left elbow\nwaypoint 9:\n    nearest landmark: none\nwaypoint 10:\n    nearest landmark: none\nwaypoint 11:\n    nearest landmark: left wrist\nwaypoint 12:\n    nearest landmark: left elbow\nwaypoint 13:\n    nearest landmark: none\nwaypoint 14:\n    nearest landmark: none\nwaypoint 15:\n    nearest landmark: left wrist\nwaypoint 16:\n    nearest landmark: left elbow\nwaypoint 17:\n    nearest landmark: none\nwaypoint 18:\n    nearest landmark: none\nwaypoint 19:\n    nearest landmark: none\nNext, ‚ÄúDetected user utterance‚Äù refers to the user utterance picked up from the speech-to-text service, in plain string without any formatting. This can be any of the examples shown in Figure\n3\nin the main text.\nFinally, ‚ÄúConversation history‚Äù refers to the most recent user utterance and YAML response from the LLM, formatted in the same style as the two examples included in the prompt itself. If there is no previous user response, the content here is simply left empty.\nAppendix B\nAdditional Efficacy Results\nFigure 8.\nEmpirical complementary cumulative distribution function (CCDF) showing the ratio of remaining modifications as a function of the progress of each task, from all trials with our method or unidirectional communication. Task progress is based on time, normalized to a 0‚Äì1 scale.\nPlot showing the curves for all three tasks starts initially above the diagonal, then crosses the diagonal around 0.15‚Äì0.3, and then subsequently remaining below the diagonal.\nFigure\n8\nshows the empirical complementary cumulative distribution function (CCDF), which models the ratio of modifications that has\nnot\nbeen made as a function of task progress. This is computed from the timestamp of utterances for each trial involving either our method or the unidirectional communication ablation, only counting utterances directly corresponding to the goal motion aspect of each task (position for scratching, velocity for feeding, and force for bathing). The dashed diagonal line represents making modifications at a constant rate throughout task progression.\nWe see a common trend in all tasks where the CCDF starts above the diagonal at the beginning, crosses the diagonal, and then remain below the diagonal. This indicates that the users tend to observe the robot‚Äôs behavior at the start, then make frequent modifications to reach the task objective around a task progress of 15%‚Äì30%, and lastly finish with less frequent modifications for small adjustments. Specifically for scratching and bathing tasks, participants typically make over 80% of their modifications before the task progression reaches 65%. For the feeding task, since the participants tend to increase the velocity throughout the task, trials would finish quickly after the velocity reaches a degree that the participants are satisfied with; hence we see a sharp drop in the CCDF only close to the end of the task progress (around 80%). This tendency to make more adjustment earlier in the task progression supports the effectiveness of our trajectory modifications.",
    "preview_text": "Effective physical human-robot interaction requires systems that are not only adaptable to user preferences but also transparent about their actions. This paper introduces BRIDGE, a system for bidirectional human-robot communication in physical assistance. Our method allows users to modify a robot's planned trajectory -- position, velocity, and force -- in real time using natural language. We utilize a large language model (LLM) to interpret any trajectory modifications implied by user commands in the context of the planned motion and conversation history. Importantly, our system provides verbal feedback in response to the user, either assuring any resulting changes or posing a clarifying question. We evaluated our method in a user study with 18 older adults across three assistive tasks, comparing BRIDGE to an ablation without verbal feedback and a baseline. Results show that participants successfully used the system to modify trajectories in real time. Moreover, the bidirectional feedback led to significantly higher ratings of interactivity and transparency, demonstrating that the robot's verbal response is critical for a more intuitive user experience. Videos and code can be found on our project website: https://bidir-comm.github.io/\n\n\\setcctype\nby-nc-nd\nBidirectional Human-Robot Communication for\nPhysical Human-Robot Interaction\nJunxiang Wang\n‚àó\nCarnegie Mellon University\n,\nCindy Wang\nCarnegie Mellon University\n,\nRana Soltani Zarrin\nHonda Research Institute USA\nand\nZackory Erickson\nCarnegie Mellon University\n(2025-12-01)\nAbstract.\nEffective physical human-robot interaction requires systems that are not only adaptable to user preferences but also transparent about their actions. This paper introduces BRIDGE, a system for bidirectional human-robot communication in physical assistance. Our method allows users to modify a robot‚Äôs planned trajectory‚Äîposition, velocity, and force‚Äîin real time using natural language. We utilize a large language model (LLM) to interpret a",
    "is_relevant": true,
    "relevance_score": 3.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "VLA",
        "diffusion",
        "Flow Matching",
        "locomotion",
        "VLM",
        "whole body control"
    ],
    "one_line_summary": "ËØ•ËÆ∫ÊñáÊèêÂá∫BRIDGEÁ≥ªÁªüÔºåÈÄöËøáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂÆûÁé∞Ëá™ÁÑ∂ËØ≠Ë®ÄÂèåÂêë‰∫∫Êú∫‰∫§‰∫íÔºåÁî®‰∫éÂÆûÊó∂‰øÆÊîπÊú∫Âô®‰∫∫ËΩ®ËøπÔºå‰ΩÜÊú™Áõ¥Êé•Ê∂âÂèäÂº∫ÂåñÂ≠¶‰π†„ÄÅÊâ©Êï£Ê®°ÂûãÊàñÂÖ®Ë∫´ÊéßÂà∂Á≠âÂÖ≥ÈîÆËØçÁöÑÊ†∏ÂøÉÊäÄÊúØ„ÄÇ",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-15T19:01:02Z",
    "created_at": "2026-01-20T17:49:56.564131",
    "updated_at": "2026-01-20T17:49:56.564140"
}