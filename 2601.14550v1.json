{
    "id": "2601.14550v1",
    "title": "TacUMI: A Multi-Modal Universal Manipulation Interface for Contact-Rich Tasks",
    "authors": [
        "Tailai Cheng",
        "Kejia Chen",
        "Lingyun Chen",
        "Liding Zhang",
        "Yue Zhang",
        "Yao Ling",
        "Mahdi Hamad",
        "Zhenshan Bing",
        "Fan Wu",
        "Karan Sharma",
        "Alois Knoll"
    ],
    "abstract": "ä»»åŠ¡åˆ†è§£å¯¹äºç†è§£å’Œå­¦ä¹ å¤æ‚çš„é•¿æœŸæ“ä½œä»»åŠ¡è‡³å…³é‡è¦ã€‚å°¤å…¶åœ¨æ¶‰åŠä¸°å¯Œç‰©ç†äº¤äº’çš„ä»»åŠ¡ä¸­ï¼Œä»…ä¾èµ–è§†è§‰è§‚æµ‹å’Œæœºå™¨äººæœ¬ä½“æ„ŸçŸ¥ä¿¡æ¯å¾€å¾€éš¾ä»¥æ­ç¤ºæ½œåœ¨çš„äº‹ä»¶è½¬æ¢ã€‚è¿™è¦æ±‚é«˜æ•ˆé‡‡é›†é«˜è´¨é‡å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶é‡‡ç”¨é²æ£’çš„åˆ†å‰²æ–¹æ³•å°†æ¼”ç¤ºåˆ†è§£ä¸ºæœ‰æ„ä¹‰çš„æ¨¡å—ã€‚åŸºäºæ‰‹æŒå¼æ¼”ç¤ºè®¾å¤‡é€šç”¨æ“ä½œç•Œé¢ï¼ˆUMIï¼‰çš„è®¾è®¡ç†å¿µï¼Œæˆ‘ä»¬æå‡ºTacUMIâ€”â€”ä¸€ä¸ªé›†æˆè§†è§‰è§¦è§‰ä¼ æ„Ÿå™¨ã€åŠ›æ‰­çŸ©ä¼ æ„Ÿå™¨ä¸ä½å§¿è·Ÿè¸ªå™¨çš„å¤šæ¨¡æ€æ•°æ®é‡‡é›†ç³»ç»Ÿï¼Œé€šè¿‡ç´§å‡‘çš„æœºå™¨äººå…¼å®¹å¤¹æŒå™¨è®¾è®¡ï¼Œå®ç°åœ¨äººç±»æ¼”ç¤ºè¿‡ç¨‹ä¸­åŒæ­¥è·å–å…¨éƒ¨æ¨¡æ€æ•°æ®ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºä¸€ç§å¤šæ¨¡æ€åˆ†å‰²æ¡†æ¶ï¼Œåˆ©ç”¨æ—¶åºæ¨¡å‹æ£€æµ‹åºåˆ—åŒ–æ“ä½œä¸­å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„äº‹ä»¶è¾¹ç•Œã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çº¿ç¼†è£…é…ä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿè¾¾åˆ°è¶…è¿‡90%çš„åˆ†å‰²å‡†ç¡®ç‡ï¼Œä¸”å¤šæ¨¡æ€èåˆå¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†TacUMIä¸ºæ¥è§¦å¯†é›†å‹ä»»åŠ¡ä¸­å¤šæ¨¡æ€æ¼”ç¤ºçš„å¯æ‰©å±•é‡‡é›†ä¸åˆ†å‰²å¥ å®šäº†å®ç”¨åŸºç¡€ã€‚",
    "url": "https://arxiv.org/abs/2601.14550v1",
    "html_url": "https://arxiv.org/html/2601.14550v1",
    "html_content": "TacUMI: A Multi-Modal Universal Manipulation Interface for Contact-Rich Tasks\nTailai Cheng\n1,2\n, Kejia Chen\n1\n, Lingyun Chen\n1\n, Liding Zhang\n1\n, Yue Zhang\n1\n, Yao Ling\n1\n,\nMahdi Hamad\n2\n, Zhenshan Bing\n3,1\n, Fan Wu\n4,1\n, Karan Sharma\n2\n, Alois Knoll\n1\n1\nSchool of Computation, Information and Technology, Technical University of Munich, Germany.\n2\nAgile Robots SE, Munich, Germany\n3\nState Key Laboratory for Novel Software Technology and the School of Science and Technology, Nanjing University (Suzhou Campus), China.\n4\nShanghai University, China.\nAbstract\nTask decomposition is critical for understanding and learning complex long-horizon manipulation tasks.\nEspecially for tasks involving rich physical interactions, relying solely on visual observations and robot proprioceptive information often fails to reveal the underlying event transitions.\nThis raises the requirement for efficient collection of high-quality multi-modal data as well as robust segmentation method to decompose demonstrations into meaningful modules.\nBuilding on the idea of the handheld demonstration device Universal Manipulation Interface (UMI), we introduce TacUMI, a multi-modal data collection system that integrates additionally ViTac sensors, forceâ€“torque sensor, and pose tracker into a compact, robot-compatible gripper design, which enables synchronized acquisition of all these modalities during human demonstrations.\nWe then propose a multi-modal segmentation framework that leverages temporal models to detect semantically meaningful event boundaries in sequential manipulations.\nEvaluation on a challenging cable mounting task shows more than 90% segmentation accuracy and highlights a remarkable improvement with more modalities, which validates that TacUMI establishes a practical foundation for both scalable collection and segmentation of multi-modal demonstrations in contact-rich tasks.\nThe design and experiment results are available at our project website:\nhttps://tac-umi.github.io/TacUMI/\n.\nI\nINTRODUCTION\nIn real-world applications such as industrial assembly and household services, robots are often required to perform long-horizon tasks consisting of a sequence of modular skills.\nHowever, learning such long-horizon tasks, especially when there are intensive physical interactions, remains a fundamental challenge.\nWhile recent imitation learning frameworks, such as ACT\n[\n15\n]\nand Diffusion Policy\n[\n3\n]\n, have achieved strong performance on short-horizon tasks, they treat the whole demonstration as a monolithic sequence without explicit segmentation, which makes them struggle to learn long-horizon tasks at once\n[\n12\n]\n.\nIn contrast, learning separately modular motor skills and high-level coordinator to compose them have proven to be an efficient and promising paradigm\n[\n18\n,\n7\n]\n.\nFigure 1:\nOverall setup for the cable mounting task.\nThe left hand holds a modified UMI gripper equipped with a ViTac sensor and a Vive Tracker, while the right hand holds the proposed TacUMI gripper for multimodal data collection.\nA camera positioned in the center records third-person visual information during the demonstrations.\nThis hierarchical learning paradigm requires firstly decomposing a long demonstration into semantically meaningful, modular skills.\nIn this segmentation, tactile information can play an important role, as it can capture contact dynamics that may be missed by visual observation\n[\n2\n]\n.\nFor example, during cable mounting, operators often stretch the cable to create tension.\nThis step is difficult to discern visually but can be clearly captured through tactile signals.\nHowever, tactile sensing has not been sufficiently explored in existing research due to limitation in both data acquisition and learning methods.\nIn terms of data collection, teleoperation-based systems\n[\n10\n,\n17\n,\n1\n]\nprovide precise demonstrations, but their high cost, lack of intuitive control, and limited force feedback severely hinder practical usability.\nRecent handheld data collection devices, such as the Universal Manipulation Interface (UMI)\n[\n4\n]\nand FastUMI\n[\n16\n]\n, have lowered the barrier for human-in-the-loop demonstration.\nNevertheless, they rely solely on vision and pose, and the critical tactile information is missing from their design.\nWhen it comes to learning methods, visual and proprioceptive data have been widely explored for task decomposition\n[\n18\n,\n14\n,\n11\n]\n.\nHowever, less attention were paid to the utilization and integration of tactile sensing.\nTo address these limitations, we present a new solution that tightly integrates multi-modal sensing hardware with a task segmentation learning framework.\nOn the hardware side, we develop an handheld data collection device inspired by UMI, but enhanced with ViTac sensors on fingertips, a 6D force-torque (F/T) sensor on the wrist, and a high-precision 6D pose tracker.\nEspecially for F/T signals, we introduce a continuously lockable jaw mechanism that eliminates the userâ€™s interference when grasping.\nThis enables synchronize collecting clean measurement of contact tactile, forces/torque, and gripper pose information during demonstrations.\nOn the algorithmic side, we propose a BiLSTM-based task segmentation framework that learns to partition long-horizon demonstrations into a sequence of skills.\nUsing the rich temporal structure and complementary signals from multiple modalities, our approach identifies transition boundaries between each stage of long-horizon manipulation tasks.\nThis segmentation lays a foundation for learning modular skills and composing them, which makes learning from long-horizon tasks more scalable and interpretable.\nTo validate our proposed data collection and segmentation approach, we evaluate our system on a challenging cable mounting task.\nIn comparison to other UMI-based designs as well as teleoperation, TacUMI obtains uninterferred F/T data in the most efficient way.\nBy integrating multiple, especially the tactile, modalities collected by TacUMI, the learned segmentation model reveals clear and semantically meaningful transition across different phases of the manipulation process.\nMoreover, we deploy the model trained from TacUMI-collected dataset on another dataset collected by teleoperating robots, and achieves comparable segmentation accuracy.\nThese evaluations demonstrate the effectiveness of our system for interpreting and segmenting long-horizon manipulation tasks.\nII\nRELATED WORK\nII-A\nData Collection Interfaces for Robot\nHigh-quality multimodal data is crucial for understanding complex, contact-rich manipulation tasks.\nConventional teleoperation systems\n[\n13\n,\n5\n,\n3\n,\n2\n]\noffer precise control and direct mapping to robot embodiments, but they are often costly, cumbersome to set up, and lack intuitive force feedback for the operator.\nThese limitations have motivated the development of\nhandheld robotic data collection devices\nthat allow operators to physically manipulate objects in a natural manner while capturing sensor data.\nThe Universal Manipulation Interface (UMI)\n[\n4\n]\nand FastUMI\n[\n16\n]\nprovide low-cost handheld tools that enable rapid collection of visual and pose data from human demonstrations.\nWhile effective for efficient data acquisition, both systems rely solely on vision-based sensing and thus cannot capture tactile feedback or the interaction forces between the gripper and the environment.\nMoreover, their pose estimation, built on SLAM-based methods, inevitably suffers from cumulative drift during long-horizon demonstrations.\nForceMimic\n[\n9\n]\nextends data collection by incorporating a 6D force-torque (F/T) sensor and a ratchet-based locking mechanism, allowing the recording of wrench data during manipulation. However, its soft gripper structure and discrete locking mechanism limit the achievable clamping force and stability.\nAs a result, the gripper can fail to provide sufficient normal force and friction at the contact interface, leading to slippage or even loss of the manipulated object under external loads.\nIn contrast, our device integrates synchronized\ntactile sensing\n,\nforce-torque measurement\n, and\nprecise 6-DoF pose tracking\nvia a pose tracker, eliminating cumulative drift issues in long tasks.\nThe continuous locking mechanism allows the operator to maintain a stable grasp without sustaining trigger pressure, removing internal actuation forces from FT recordings and ensuring clean interaction data.\nFurthermore, our gripperâ€™s high clamping force capability prevents slippage during high-tension interactions (e.g., DLO manipulation), extending applicability to a wider range of contact-rich scenarios.\nII-B\nEvent Segmentation\nEvent segmentation aims to decompose continuous demonstrations into semantically meaningful skills, which is essential for learning and executing long-horizon tasks.\nUnsupervised methods such as Bottom-Up Skill Discovery\n[\n18\n]\nemploy agglomerative clustering to build hierarchical task structures from raw demonstrations, identifying recurring motion patterns as candidate skills.\nXSkill\n[\n14\n]\nlearns a shared skill representation space and a set of learnable skill prototypes from unlabelled human and robot videos, aligning them into a unified latent space for cross-embodiment skill discovery.\nSkIDRAW\n[\n11\n]\njointly performs unsupervised skill discovery and trajectory segmentation directly from raw, unlabelled trajectories.\nWhile these approaches are effective for discovering latent skills without annotations, they are unsuitable for our cable mounting task, where each skill is explicitly defined and unsupervised clustering may lead to ambiguous or inconsistent segmentation.\nOther segmentation works, such as\n[\n6\n,\n8\n]\n, operate by detecting motion boundaries from human or robot demonstrations, relying primarily on camera video and proprioceptive data. However, these modalities alone are insufficient for our scenario: in cable mounting, certain skills, such as\ntensioning the cable\n, involve minimal observable motion and thus cannot be reliably detected using only visual and proprioceptive cues.\nOur segmentation approach follows the\nobject-centric\nperspective from LEMMo-Plan\n[\n2\n]\n, where segmentation is driven by changes in the state of the manipulated object rather than solely by end-effector kinematics.\nAlthough LEMMo-Plan showed that ViTac images can directly capture subtle skill transitions during object manipulation, we found that using tactile data alone was insufficient to achieve robust performance.\nTo address this, we integrate ViTac data, force-torque measurements, and third-person visual observations, and process them using a BiLSTM-based temporal model.\nThis multi-modal integration enables reliable detection of subtle contact-state changes and accurate segmentation of complex, long-horizon cable mounting demonstrations.\nIII\nMETHODOLOGY\nIII-A\nMechanical Design for Robot-Executable Demonstrations\nWe propose a new handheld demonstration device that supports multi-modal, synchronized, and robot-compatible data collection for complex manipulation tasks.\nInspired by previous designs\n[\n4\n,\n16\n,\n9\n]\n, our system significantly extends hardware capabilities for demonstrating long-horizon and contact-rich manipulation tasks.\nThe mechanism preserves a robot wristâ€“end effector layout and integrates tactile sensing, 6D forceâ€“torque (F/T) sensing, and 6D pose tracking in a compact, one-handed form factor (Fig.\n2\n)\nFigure 2:\nTacUMI structure.\nExploded view of the proposed multimodal data collection gripper, showing:\n(1) ViTac sensor (GelSight Mini);\n(2) Vive tracker for 6-DoF pose tracking;\n(3) Trigger connected to the rack-and-pinion mechanism for grasp actuation;\n(4) Continuous locking part that enables the trigger to be locked at arbitrary jaw opening widths;\n(5) 6-axis Force/Torque (F/T) sensor;\n(6) Handle ergonomically designed for single-handed operation.\nInsets illustrate the locking mechanism in the locked and released states.\nIII-A\n1\nRobot-Compatible Forceâ€“Torque Stack\nA 6-axis F/T sensor is mounted between the gripper and a rear handle shaped as a\nstandard robot flange\n(Fig.\n2\nâ‘¤).\nThis preserves the physical stack-up used on robots, so the measured wrench during human operation matches robotâ€“environment interactions at execution.\nBecause the F/T sensor sits between the wrist-equivalent interface and the tool, the recorded wrench is directly transferable to downstream control without frame re-ordering or ad-hoc calibration.\nIII-A\n2\nRack-and-Pinion Trigger with Continuous Self-Locking\nWe retain the UMI-style rack-and-pinion trigger for intuitive force scaling, where finger closure is proportional to trigger effort\n[\n4\n]\n.\nIn prior designs, continuous trigger pressing was required to maintain a grasp, and this actuation leaked into F/T measurements.\nTo avoid this, we introduce a\ncontinuous self-locking mechanism\n(Fig.\n2\nâ‘£) that rigidly fixes the jaw aperture at any opening.\nOnce locked, the fingers and grasped object form a stiff extension of the tool; the F/T sensor then measures only\nexternal\ncontact wrenches.\nUnlike ratcheted locks and soft fingertips in previous work\n[\n9\n]\n, the continuous lock avoids quantization and enables high gripping forces across object sizes, maintaining hold even under significant tensile loads.\nOur software optionally filters any residual trigger-coupled components when only external contacts are desired for segmentation.\nIII-A\n3\nViTac Sensing for Contact-Rich Transitions\nCustom fingertip slots accept ViTac modules (Fig.\n2\nâ‘ ), which capture fine surface deformations (Fig.\n5\n).\nThese tactile cues disambiguate visually similar states and thus\nimprove phase understanding and segmentation in contact-centric tasks.\nIII-A\n4\nDrift-Free 6-DoF Pose Tracking\nA rigidly mounted Vive Tracker on the top cover provides accurate 6-DoF trajectories with negligible long-horizon drift, enabling a fixed transform to the tool center point (TCP).\nCompared with visualâ€“inertial SLAM pipelines\n[\n4\n]\n, this eliminates cumulative drift during extended demonstrations and yields stable trajectories for retargeting.\nIn summary, with a robot-compatible F/T stack with a\ncontinuous\nmechanical lock, the device isolates\nexternal\ninteraction wrenches during demonstrations, which prior UMI-family designs lack.\nIntegrated further with tactile and pose sensing, it is a compact, use-friendly and robot-compatible data collection tool which supports synchronized multi-modal\nsensing and is especially suitable for capturing contact-rich interactions in long-horizon demonstrations.\nErgonomically, it supports single-handed operation: the center of mass sits near the palm, the trigger is indexed to the forefingers, and the lock is thumb-engaged with index release.\nMeanwhile, it preserves wristâ€“tool configuration which allows seamless transfer to real robots.\nThe overall structure is lightweight and largely composed of 3D-printed components, making it easy to manufacture, customize, and reproduce.\nFigure 3:\nPipeline of the proposed event segmentation algorithm.\nThe framework consists of four main stages:\n(a)\nData Extraction:\ntactile images are processed by a ResNet50 to obtain 256-dimensional embeddings; third-person view images are processed by a ResNet18 with GroupNorm to obtain another 256-dimensional visual embedding; raw 6D F/T signals are preprocessed and 6D tracker pose are transformed to TCP.\n(b)\nSliding Window:\nfused feature sequences of size\n[\nT\n,\n532\n]\n[T,532]\nare segmented into overlapping windows of length 50 with stride 10.\n(c)\nTraining:\neach window is fed into one of three sequence models (BiLSTM, TCN, or Transformer) to capture temporal dependencies and produce per-frame skill predictions.\n(d)\nsoft voting inference:\noverlapping predictions for the same frame are aggregated by averaging class probabilities and assigning the highest-probability label, restoring the original sequence length for final output.\nIII-B\nEvent Segmentation\nWe aim to segment long-horizon manipulation demonstrations temporally into meaningful short-skill segments using rich multi-modal sensory feedback.\nOur approach leverages synchronized\ntactile images, force-torque data, third-person view RGB images, and pose data\nto detect skill transitions with high granularity.\nIII-B\n1\nNetwork Architecture\nAt each timestep, we fuse representation from tactile, RGB image, F/T and pose (Fig.\n3\n(a)).\nFor tactile, we feed raw ViTac frames to a ResNet-50 backbone pretrained on ImageNet, truncate the classification head, and add a linear layer to yield a 256-D embedding.\nFor the third-person views, inspired by Diffusion Policy\n[\n3\n]\n, we use a ResNet-18 with all BatchNorm replaced by GroupNorm, remove global average pooling and fully connected layers, apply a Spatial softmax to extract spatial attention features, and project to a 256-D embedding.\nThe raw 6-D F/T stream is preprocessed to remove operator trigger artifacts (\npull\n,\nlock\n,\nrelease\n) using a secondary segmentation model (see Section\nIII-C\n), yielding an interaction-only signal (Fig.\n4\nb).\nThe pose data obtained from trackers are transformed into the grippersâ€™ tool center point (TCP). For each hand, we represent the TCP pose as a 7-D vector. Concatenating both left and right hand TCP poses yields a 14-D pose feature.\nThe resulting vectors are normalized and concatenated with the tactile and visual embeddings to form\nğ’™\nt\n=\n[\nğ‘°\ntactile\n(\nt\n)\nâ€‹\nâ€–\nğ‘°\n3rd-view\n(\nt\n)\nâ€–\nâ€‹\nğ’‡\n(\nt\n)\nâˆ¥\nğ‘·\n(\nt\n)\n]\nâˆˆ\nâ„\n532\nâ€‹\n,\n\\boldsymbol{x}_{t}=\\big[\\boldsymbol{I}_{\\text{tactile}}^{(t)}\\,\\|\\,\\boldsymbol{I}_{\\text{3rd-view}}^{(t)}\\,\\|\\,\\boldsymbol{f}^{(t)}\\,\\|\\,\\boldsymbol{P}^{(t)}\\big]\\in\\mathbb{R}^{532}\\text{,}\n(1)\nwhere\nğ‘°\n(\nâ‹…\n)\n(\nt\n)\nâˆˆ\nâ„\n256\n\\boldsymbol{I}_{(\\cdot)}^{(t)}\\in\\mathbb{R}^{256}\n,\nğ’‡\n(\nt\n)\nâˆˆ\nâ„\n6\n\\boldsymbol{f}^{(t)}\\in\\mathbb{R}^{6}\n, and\nğ‘·\n(\nt\n)\nâˆˆ\nâ„\n14\n\\boldsymbol{P}^{(t)}\\in\\mathbb{R}^{14}\n.\nTo capture temporal dependencies across multimodal sequences, we evaluate three different backbone architectures:\nâ€¢\na three-layer Bidirectional LSTM (BiLSTM) with hidden size 128 per direction.\nTaking the fused multi-modal representation\nğ’™\nt\n\\boldsymbol{x}_{t}\nat each timestep as input, the BiLSTM produces a sequence of hidden states\n{\nğ’‰\nt\n}\nt\n=\n1\nT\n\\{\\boldsymbol{h}_{t}\\}_{t=1}^{T}\n, which are subsequently passed through a dropout layer and a linear classifier to produce\nğ’›\nt\nâˆˆ\nâ„\nC\n\\boldsymbol{z}_{t}\\in\\mathbb{R}^{C}\n,\nwhere\nC\nC\ndenotes the number of skills.\nâ€¢\na Temporal Convolutional Network (TCN) composed of stacked residual blocks with dilated 1D convolutions, GELU activations, and Layer Normalization.\nThis architecture enlarges the receptive field and captures multiscale temporal patterns while maintaining efficient computation.\nâ€¢\na Transformer Encoder with learned linear input projection, sinusoidal positional encoding, and\nL\nL\nstacked encoder layers, each consisting of multi-head self-attention and feedforward sublayers.\nAll three architectures output frame-wise logits from sequence-to-sequence mapping\nf\n:\nâ„\nT\nÃ—\n532\nâ†’\nâ„\nT\nÃ—\nC\nf:\\mathbb{R}^{T\\times 532}\\;\\rightarrow\\;\\mathbb{R}^{T\\times C}\n.\nIII-B\n2\nSliding Window and Training Strategy\nIn sequential tasks, there is usually a label imbalance in terms of number of frames.\nParticularly, the\nidle\nclass where nothing happens are often over-represented.\nTo handle this imbalance and also to increase training efficiency, we adopt a sliding window to extract fixed-length overlapping subsequences from each demonstration(Fig.\n3\n(b)).\nThis strategy both augments the dataset and improves the modelâ€™s focus on meaningful skill transitions.\nWe use a window size of 50 and a stride of 10.\nTo ensure that the model learns from active manipulation segments, we discard any window in which more than 80% of the frames are labeled as\nidle\n, enforcing a minimum action ratio of 20%.\nEach retained window is paired with its corresponding frame-wise label sequence and used to supervise the temporal model. Training is conducted on the long-horizon task using the Adam optimizer with standard learning rate scheduling. To prevent overfitting, we apply dropout after the BiLSTM layers and employ early stopping based on validation segmentation accuracy.\nIII-B\n3\nSoft Voting for Inference\nAfter the BiLSTM model predicts frame-wise skill labels for each input window, we obtain multiple overlapping predictions for the same frame due to the use of a sliding window during both training and inference. Each frame in the original demonstration may appear in several windows, each providing an independent classification output.\nTo reconcile these overlapping predictions and restore the temporal alignment with the original sequence, we apply a soft voting strategy. Specifically, for each frame\nt\nt\nin the original sequence, we collect all predicted class probability vectors from the windows that include frame\nt\nt\n.\nWe aggregate probabilities from all overlapping windows that include\nt\nt\n.\nSpecifically, let\nK\nt\nK_{t}\ndenote the number of such windows and\np\nt\n,\nc\n(\nk\n)\np^{(k)}_{t,c}\nthe predicted probability of class\nc\nc\nfrom the\nk\nk\n-th window.\nWe compute the averaged class probability and assign the final label by\ny\n^\nt\n=\narg\nâ¡\nmax\nc\nâˆˆ\n{\n1\n,\nâ€¦\n,\nC\n}\nâ¡\n1\nK\nt\nâ€‹\nâˆ‘\nk\n=\n1\nK\nt\np\nt\n,\nc\n(\nk\n)\n,\n\\hat{y}_{t}=\\arg\\max_{c\\in\\{1,\\dots,C\\}}\\frac{1}{K_{t}}\\sum_{k=1}^{K_{t}}p^{(k)}_{t,c},\n(2)\nso that\ny\n^\nt\n\\hat{y}_{t}\ncorresponds to the class with the highest mean probability across windows. As shown in (\n2\n).\nThis soft aggregation method ensures temporal consistency, reduces prediction noise, and produces a smooth, frame-aligned label sequence for the entire demonstration. The final output preserves the original sequence length and provides a single semantic label per frame, suitable for downstream evaluation and visualization.\nIII-C\nF/T Data Preprocessing\nFigure 4:\nComparison of original and processed Force/Torque (F/T) data.\n(a)\nOriginal F/T data:\nThe recorded signals reveal distinct operator-induced events, namely pulling and locking the trigger (blue-shaded region) followed by releasing it (green-shaded region), both of which cause significant disturbances in the measured force and torque signals.\n(b)\nProcessed F/T data:\nThe influence of these three actions has been removed, yielding clean signals that can be directly utilized by the robot without the undesired human-induced artifacts.\nIn this section, we describe two preprocessing steps applied to raw force-torque (F/T) signals before they are used for segmentation.\nIII-C\n1\nTrigger Action Filtering\nA unique downstream use case of our segmentation model is to\nfilter out trigger-related transitions\nin force-torque signals.\nAs shown in Fig.\n4\n(a), with help of the continuous locking mechanism, the collected force and torque signals drop sharply to near zero right after locking, which indicates that internal actuation effects have been eliminated and resembles the measurement of wrist-mounted F/T sensors on robots.\nNevertheless, there are still temporary interferences when the operator pulls the trigger, locks it or releases it.\nTo filter these interferences out, we train a second instance of the same segmentation architecture to explicitly classify these intervals.\nWe then replace the identified interference intervals with Gaussian noise whose mean and standard deviation estimated from the first 20 frames of each sequence, where no trigger actions occur.\nThis noise injection preserves the natural statistical characteristics of the signal, as shown in Fig.\n4\n(b), while effectively removing internal actuation effects.\nThe resulting interaction-only force-torque stream maintains temporal continuity and is better suited for downstream robotic learning and control.\nIII-C\n2\nGripper-to-Robot F/T Data Mapping\nTo adapt the gripper-collected F/T data for use across different robot platforms, we map the measured wrench from the gripper F/T sensor frame\n{\nG\n}\n\\{\\mathrm{G}\\}\nto the robot end-effector frame\n{\nR\n}\n\\{\\mathrm{R}\\}\n. The mapping is given by\nğ…\nR\n=\nR\nG\nâ€‹\nR\nâ€‹\nğ…\nG\n,\n\\mathbf{F}_{R}=R_{GR}\\mathbf{F}_{G},\n(3)\nğ‰\nR\n=\nR\nG\nâ€‹\nR\nâ€‹\nğ‰\nG\n+\nğ«\nG\nâ€‹\nR\nÃ—\n(\nR\nG\nâ€‹\nR\nâ€‹\nğ…\nG\n)\n,\n\\boldsymbol{\\tau}_{R}=R_{GR}\\boldsymbol{\\tau}_{G}+\\mathbf{r}_{GR}\\times\\left(R_{GR}\\mathbf{F}_{G}\\right),\n(4)\nwhere\nğ…\nG\nâˆˆ\nâ„\n3\n\\mathbf{F}_{G}\\in\\mathbb{R}^{3}\nand\nğ‰\nG\nâˆˆ\nâ„\n3\n\\boldsymbol{\\tau}_{G}\\in\\mathbb{R}^{3}\ndenote the force and torque measured in\n{\nG\n}\n\\{\\mathrm{G}\\}\n,\nğ…\nR\nâˆˆ\nâ„\n3\n\\mathbf{F}_{R}\\in\\mathbb{R}^{3}\nand\nğ‰\nR\nâˆˆ\nâ„\n3\n\\boldsymbol{\\tau}_{R}\\in\\mathbb{R}^{3}\nare the corresponding quantities expressed in\n{\nR\n}\n\\{\\mathrm{R}\\}\n,\nR\nG\nâ€‹\nR\nâˆˆ\nS\nâ€‹\nO\nâ€‹\n(\n3\n)\nR_{GR}\\in SO(3)\nis the rotation matrix (expressed in the TCP coordinate system) that transforms vectors from the gripper-mounted F/T sensor frame to the robot-mounted F/T sensor frame, and\nğ«\nG\nâ€‹\nR\nâˆˆ\nâ„\n3\n\\mathbf{r}_{GR}\\in\\mathbb{R}^{3}\nis the displacement vector (expressed in the TCP coordinate system) from the measurement center\nO\nG\nO_{G}\nof the gripper-mounted F/T sensor to the measurement center\nO\nR\nO_{R}\nof the robot-mounted F/T sensor.\nEquationsÂ (\n3\n) and (\n4\n) follow the standard wrench transformation under a change of reference frame, and ensure that the interaction forces measured at the gripper are consistently expressed in the robot end-effector frame. This unified representation enables seamless transfer of the gripper-collected F/T data across robot platforms with varying wrist sensor and TCP configurations.\nIV\nEXPERIMENTS\nFigure 5:\nOverview of the cable mounting task process.\nUsing the proposed\nTacUMI\n, the cable is sequentially inserted into three U-type clips with different orientations.\nIn this section, we perform a cable mounting experiment to validate both the correctness and efficiency of multimodal data collection using our proposed handheld gripper, and to demonstrate the effectiveness of the event segmentation algorithm.\nIV-A\nExperimental Setup\nIV-A\n1\nHardware and Sensors\nWe conduct experiments using our custom-designed handheld tools, whose multi-modal data collection system consists of four sensors: a Bota Systems SensONE 6D force-torque sensor, a GelSight Mini ViTac sensor, an HTC Vive Tracker for 6-DoF pose tracking, and a fixed-position RealSense D435i camera for third-person visual observation.\nThese sensors operate at different frequencies: the force-torque sensor at 1000â€‰Hz, the ViTac sensor (with online processing) at 16.67â€‰Hz, the Vive Tracker at 60â€‰Hz, and the RGB-D camera also at 60â€‰Hz.\nTo ensure temporal alignment across modalities, we down-sample all sensor streams to a common frequency of 16.67â€‰Hz, matching that of the tactile sensor, the lowest among all devices.\nFor dual-arm manipulation tasks, we distribute different sensing modalities to different hands.\nFor example, in the cable mounting task(shown in Fig.\n5\n), the right-hand TacUMI is equipped with a forceâ€“torque sensor and a Vive Tracker to maintain stable grasping and guide the cable along its path, while the left-hand gripper carries a ViTac sensor and a Vive Tracker to perform fine manipulations, including tensioning, positioning, and inserting the cable into clips.\nDuring each trial, the operator progresses along the clip sequence, coordinating both grippers to maintain cable stability while executing precise insertions.\nVariations in clip position and orientation introduce non-repetitive transitions between subtasks, providing a challenging benchmark for multimodal task segmentation and validating the robustness of our proposed approach.\nTo validate that the multimodal demonstrations collected with our handheld gripper can be effectively transferred to a robot platform, we additionally collect a test dataset from teleopration, where we employ\nsigma.7\nhaptic devices to control two\nFranka Emika\nrobots in a masterâ€“slave configuration.\nThe teleoperation setup mirrors the handheld gripper setup:\nthe guiding arm is equipped with a wrist-mounted F/T sensor,\nand the manipulating arm is equipped with a fingertip-mounted tactile sensor, so it captures the the same modalities (tactile, force-torque, and pose from proprioceptive data) as the handheld device.\nWe then evaluate on the teleopration dataset whether the segmentation models trained exclusively on handheld-gripper data could generalize well to robot-collected data.\nFigure 6:\nComparison of four gripper designs.\n(a) Design without a locking mechanism: the fingertips are closed by pulling the trigger backward.\n(b) Design with a ratchet-based locking mechanism: once locked, the trigger can be released by pushing forward the two levers showed in the green circle.\n(c) Design with tension spring-driven fingertips: the fingertips are normally closed by a tension spring (showed in the green circle) and open when the trigger is pulled backward.\n(d) Design with a continuous locking mechanism: the proposed design introduced in detail in previous figure, enabling stable and adjustable locking at arbitrary grasping width.\nIV-B\nData Collection Correctness and Efficiency\nTo evaluate the correctness of F/T data collected by different gripper designs, we compared four variants as illustrated in Fig.\n6\n.\nThe first design (Fig.\n6\n(a)) places the F/T sensor between the handle and the gripper body, but does not include any locking mechanism, so the jaws are closed only by continuously pulling the trigger.\nThe second design (Fig.\n6\n(b)) adds a ratchet-based locking mechanism, similar in principle to ForceMimic\n[\n9\n]\n, allowing the trigger to be locked and released via mechanical levers.\nThe third design (Fig.\n6\n(c)) employs a reverse actuation structure with a tension spring mounted under the fingertips, keeping them normally closed, pulling the trigger opens the fingertips against the spring force.\nThe fourth design (Fig.\n6\n(d)) corresponds to our proposed continuous locking mechanism described in Section\nIII-A\n, which ensures stable grasping during demonstrations.\nWe used each design to collect multimodal data during the cable mounting task and mapped the measured F/T signals into the robot end-effector frame using EquationsÂ (\n3\n) and (\n4\n).\nFigure 7:\nComparison of force measurements in the cable mounting task across different gripper designs and teleoperated robot.\n(a) Gripper without locking mechanism.\n(b) Gripper with ratchet-based locking mechanism.\n(c) Gripper with tension spring-based mechanism.\n(d) Proposed\nTacUMI\ngripper integrating tactile and force sensing.\n(e) Teleoperated robot with wrist-mounted FT sensor.\nAs shown in Fig.\n7\n(a), the first design successfully detected F/T data, however, since the operator had to continuously hold the trigger, an additional non-constant actuation force was introduced, which could not be removed through preprocessing.\nIn Fig.\n7\n(b), the second design with a ratchet-based locking mechanism exhibited discontinuous locking of the grasping width. As a result, the gripper failed to securely hold the cable, causing it to slip out during tightening and leading to unsuccessful task execution.\nIn contrast, both Fig.\n7\n(c) and Fig.\n7\n(d) show F/T data that are consistent with those collected from the teleoperation in Fig.\n7\n(e). Nevertheless, the spring-based design in Fig.\n6\n(c) lacks generalizability: due to the stiffness of the tension spring, it cannot grasp cables with larger diameters or objects requiring wider fingertip openings.\nIn contrast, our final design TacUMI shown in Fig.\n6\n(d) not only enables grasping across a wide range of grasping widths but also produces F/T data, that (after preprocessing) closely match those obtained from teleoperated robot with a wrist-mounted F/T sensor. The minor deviations are attributable to execution differences between handheld and teleoperated demonstrations. This confirms that the TacUMI design achieves correct and robot-consumable F/T data collection.\nBeyond correctness, we also compare the efficiency of teleoperation with our handheld gripper design. In each trial of the cable mounting task, the cable had to be sequentially inserted into three clips arranged in different orientations.\nWe measured the total task completion time across multiple trials for both methods. Results show that the teleoperation approach required an average of\n4 minutes\nper trial, whereas our handheld gripper completed the same task in only\n1 minute 10 seconds\non average. This demonstrates a substantial improvement in data collection speed, enabling faster acquisition of high-quality multimodal datasets for long-horizon task segmentation.\nIV-C\nAblation on Model Architectures and Input Modalities\nIV-C\n1\nEvaluation Metrics\nWe evaluate our event segmentation models using the following metrics:\nâ€¢\nFrame-wise Accuracy:\nThe proportion of correctly predicted labels over all frames in the restored sequence after applying the soft voting strategy.\nâ€¢\nF1 Score per Class:\nHarmonic mean of precision and recall for each skill class.\nWe conduct an extensive ablation study to analyze the impact of model architectures and input modalities on segmentation performance.\nFor architectures, we compare BiLSTM, TCN, and Transformer encoder introduced in Sec.\nIII-B\n.\nFor input modalities, we evaluate single-modality settings,\ndual-modality combinations,\nthree-modality combinations,\nand the full multimodal configuration(Table\nI\n).\nTABLE I:\nFrame-wise accuracy (%) on TacUMI data.\nInput Modality\nBiLSTM\nTCN\nTransformer\nCamera only\n0.7608\n0.7217\n0.3180\nCamera + Tactile\n0.9076\n0.8880\n0.6765\nCamera + F/T\n0.8632\n0.8325\n0.6645\nCamera + Pose\n0.8165\n0.7675\n0.4242\nCamera + Tactile + F/T\n0.9359\n0.9051\n0.7459\nCamera + Tactile + F/T + Pose\n0.9402\n0.8945\n0.7596\nWe first analyze the frame-wise accuracy results.\nBiLSTM consistently outperforms TCN and Transformer, as it can exploit both past and future context to accurately detect event boundaries and exhibits greater robustness to limited data scale (30k frames) and sensor noise, whereas the Transformer encoder requires larger datasets and is more sensitive to noise, and the TCN lacks the ability to access bidirectional temporal context.\nIn terms of input modalities, models relying solely on third-person visual input perform worst, often failing to identify subtle skill transitions.\nAdding tactile or F/T signals substantially boosts accuracy, while TCP pose alone provides only marginal gains.\nThe best performance is obtained when combining all four modalities, with BiLSTM achieving the highest overall accuracy.\nFigure 8:\nPer-class segmentation performance across input modalities and model architectures.\nHeatmap of average F1-scores for each action class (\nidle, grasped, under a linear force, under torque, released\n) across all modalityâ€“architecture combinations.\nHigher values (towards red) indicate better class-wise segmentation accuracy.\nFinally, we analyze class-wise segmentation performance (Fig.\n8\n).\nFor\ngrasped\nand\nidle\nphases, most models achieve high F1 scores, as these skills involve clear and distinguishable signals.\nHowever, performance drops notably for the\nreleased\nand\nunder torque\nclasses, as the transitions into these states are subtle and less distinguishable from neighboring phases.\nThe F1 score trends are consistent with the frame-wise accuracy results: performance improves as more modalities are incorporated, with BiLSTM consistently outperforming TCN and Transformer.\nNevertheless, the improvement from three modalities (\nthird-person view + tactile + F/T\n) to the full multimodal configuration (adding TCP pose) is negligible.\nThis outcome is expected for our cable mounting task, since TCP pose remains nearly unchanged during critical phases such as grasping, stretching the cable (\nunder linear force\n), and inserting it into the clip (\nunder torque\n), and thus provides little additional information for segmentation.\nAmong all events, the\nreleased\nclass shows the lowest F1 score.\nThis is primarily due to its extremely short duration, typically only 2â€“5 frames, so misclassifying even one or two frames leads to a sharp drop in F1 score.\nIn particular, BiLSTM with the full multimodal input yields balanced performance across all classes, demonstrating that complementary sensing modalities are critical for capturing fine-grained manipulation events.\nIV-D\nCross-Platform Validation on Robot Data\nTo verify that multimodal demonstrations collected with the handheld gripper can be reliably transferred to robot platforms, we additionally evaluate the trained segmentation models on teleoperation-collected datasets. Specifically, all models are trained exclusively on\nTacUMI\ncollected data but tested on robot-side demonstrations recorded via teleoperation.\nAs shown in Table\nII\n, when using only third-person camera input, the segmentation performance is very poor.\nThis is expected, since the test set consists of robot end-effector manipulations, whereas the training and validation sets contain demonstrations with the handheld TacUMI, leading to a substantial domain gap between the two embodiments.\nSimilar to the trends observed in Table\nI\n, adding tactile or F/T signals significantly improves accuracy; however, the results remain 10â€“20% lower than those achieved with gripper-collected test sets. This gap can be explained by the higher difficulty of controlling two robots via\nsigma.7\nhaptic devices, which introduces additional noise and less stable demonstrations compared to handheld execution.\nImportantly, when tactile and FT modalities are combined, the accuracy nearly matches that of the gripper test set, further confirming the critical role of multimodal fusion in achieving robust event segmentation across platforms.\nTABLE II:\nFrame-wise accuracy (%) on robot data\nInput Modality\nBiLSTM\nTCN\nTransformer\nCamera only\n0.2288\n0.1820\n0.2227\nCamera + Tactile\n0.7474\n0.6002\n0.5351\nCamera + F/T\n0.6611\n0.6366\n0.4126\nCamera + Pose\n0.4694\n0.4636\n0.2256\nCamera + Tactile + F/T\n0.9155\n0.8793\n0.8092\nCamera + Tactile + F/T + Pose\n0.9104\n0.7262\n0.7796\nIn conclusion, these results confirm that data collected with TacUMI not only supports accurate event segmentation in handheld demonstrations, but also transfers effectively to robot-executed tasks. This cross-platform generalization highlights the correctness of our data collection design and its suitability for downstream robot learning.\nV\nCONCLUSION\nWe presented TacUMI, a next-generation handheld data collection interface that extends the Universal Manipulation Interface family with multimodal sensing and task segmentation capabilities.\nBy integrating fingertip tactile sensing, a wrist-mounted forceâ€“torque sensor, and drift-free 6-DoF pose tracking into a robot-compatible design, our system enables the acquisition of high-quality multimodal demonstrations for contact-rich, long-horizon tasks.\nThe proposed continuous locking mechanism ensures clean force measurements and stable grasps.\nOn the algorithmic side, we introduced a sequence modeling framework that learns from collected multimodal-data to segment long-horizon tasks into short reusable skills. Experiments on cable mounting show that our approach achieves accurate segmentation, and transfers reliably from handheld demonstrations to robot-executed tasks.\nAlthough the results validate both the hardware design and the segmentation framework, the potential of TacUMI for other tasks has not yet been revealed. First, we aim to broaden the evaluation to a wider range of contact-rich, long-horizon manipulation tasks in order to further establish the generalizability of TacUMI. Second, beyond task segmentation, TacUMI can serve as a platform for multimodal imitation learning, enabling robots to acquire executable skills directly from the segmented demonstrations. Together, these extensions would advance the utility of TacUMI from reliable data collection and segmentation toward end-to-end robot learning in complex manipulation domains.\nReferences\n[1]\nJ. Campbell and K. Yamane\n(2020)\nLearning whole-body human-robot haptic interaction in social contexts\n.\nIn\n2020 IEEE international conference on robotics and automation (ICRA)\n,\npp.Â 10177â€“10183\n.\nCited by:\nÂ§I\n.\n[2]\nK. Chen, Z. Shen, Y. Zhang, L. Chen, F. Wu, Z. Bing, S. Haddadin, and A. Knoll\n(2025)\nLEMMo-plan: llm-enhanced learning from multi-modal demonstration for planning sequential contact-rich manipulation tasks\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 11972â€“11978\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n,\nÂ§\nII-B\n.\n[3]\nC. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song\n(2023)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nThe International Journal of Robotics Research\n,\npp.Â 02783649241273668\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n,\nÂ§\nIII-B\n1\n.\n[4]\nC. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake, and S. Song\n(2024)\nUniversal manipulation interface: in-the-wild robot teaching without in-the-wild robots\n.\nIn\nProceedings of Robotics: Science and Systems (RSS)\n,\nCited by:\nÂ§I\n,\nÂ§\nII-A\n,\nÂ§\nIII-A\n2\n,\nÂ§\nIII-A\n4\n,\nÂ§\nIII-A\n.\n[5]\nF. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, K. Daniilidis, C. Finn, and S. Levine\n(2021)\nBridge data: boosting generalization of robotic skills with cross-domain datasets\n.\narXiv preprint arXiv:2109.13396\n.\nCited by:\nÂ§\nII-A\n.\n[6]\nT. Eiband, J. Liebl, C. Willibald, and D. Lee\n(2023)\nOnline task segmentation by merging symbolic and data-driven skill recognition during kinesthetic teaching\n.\nRobotics and Autonomous Systems\n162\n,\npp.Â 104367\n.\nExternal Links:\nISSN 0921-8890\nCited by:\nÂ§\nII-B\n.\n[7]\nM. Guo and M. BÃ¼rger\n(2021)\nGeometric task networks: learning efficient and explainable skill coordination for object manipulation\n.\nIEEE Transactions on Robotics\n38\n(\n3\n),\npp.Â 1723â€“1734\n.\nCited by:\nÂ§I\n.\n[8]\nS. B. Kang and K. Ikeuchi\n(1995)\nToward automatic robot instruction from perception-temporal segmentation of tasks from human hand motion\n.\nIEEE Transactions on Robotics and Automation\n11\n(\n5\n),\npp.Â 670â€“681\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-B\n.\n[9]\nW. Liu, J. Wang, Y. Wang, W. Wang, and C. Lu\n(2025)\nForcemimic: force-centric imitation learning with force-motion capture system for contact-rich manipulation\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 1105â€“1112\n.\nCited by:\nÂ§\nII-A\n,\nÂ§\nIII-A\n2\n,\nÂ§\nIII-A\n,\nÂ§\nIV-B\n.\n[10]\nK. Shaw, S. Bahl, and D. Pathak\n(2023)\nVideodex: learning dexterity from internet videos\n.\nIn\nConference on Robot Learning\n,\npp.Â 654â€“665\n.\nCited by:\nÂ§I\n.\n[11]\nD. Tanneberg, K. Ploeger, E. Rueckert, and J. Peters\n(2021)\nSkid raw: skill discovery from raw trajectories\n.\nIEEE robotics and automation letters\n6\n(\n3\n),\npp.Â 4696â€“4703\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[12]\nR. Wolf, Y. Shi, S. Liu, and R. Rayyes\n(2025)\nDiffusion models for robotic manipulation: a survey\n.\narXiv preprint arXiv:2504.08438\n.\nCited by:\nÂ§I\n.\n[13]\nP. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel\n(2024)\nGello: a general, low-cost, and intuitive teleoperation framework for robot manipulators\n.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 12156â€“12163\n.\nCited by:\nÂ§\nII-A\n.\n[14]\nM. Xu, Z. Xu, C. Chi, M. Veloso, and S. Song\n(2023)\nXskill: cross embodiment skill discovery\n.\nIn\nConference on robot learning\n,\npp.Â 3536â€“3555\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[15]\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn\n(2023)\nLearning fine-grained bimanual manipulation with low-cost hardware\n.\nExternal Links:\n2304.13705\n,\nLink\nCited by:\nÂ§I\n.\n[16]\nZhaxizhuoma, K. Liu, C. Guan, Z. Jia, Z. Wu, X. Liu, T. Wang, S. Liang, P. Chen, P. Zhang, H. Song, D. Qu, D. Wang, Z. Wang, N. Cao, Y. Ding, B. Zhao, and X. Li\n(2025)\nFastUMI: a scalable and hardware-independent universal manipulation interface with dataset\n.\nExternal Links:\n2409.19499\n,\nLink\nCited by:\nÂ§I\n,\nÂ§\nII-A\n,\nÂ§\nIII-A\n.\n[17]\nY. Zhu, A. Joshi, P. Stone, and Y. Zhu\n(2023)\nViola: imitation learning for vision-based manipulation with object proposal priors\n.\nIn\nConference on Robot Learning\n,\npp.Â 1199â€“1210\n.\nCited by:\nÂ§I\n.\n[18]\nY. Zhu, P. Stone, and Y. Zhu\n(2022)\nBottom-up skill discovery from unsegmented demonstrations for long-horizon robot manipulation\n.\nIEEE Robotics and Automation Letters\n7\n(\n2\n),\npp.Â 4126â€“4133\n.\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-B\n.",
    "preview_text": "Task decomposition is critical for understanding and learning complex long-horizon manipulation tasks. Especially for tasks involving rich physical interactions, relying solely on visual observations and robot proprioceptive information often fails to reveal the underlying event transitions. This raises the requirement for efficient collection of high-quality multi-modal data as well as robust segmentation method to decompose demonstrations into meaningful modules. Building on the idea of the handheld demonstration device Universal Manipulation Interface (UMI), we introduce TacUMI, a multi-modal data collection system that integrates additionally ViTac sensors, force-torque sensor, and pose tracker into a compact, robot-compatible gripper design, which enables synchronized acquisition of all these modalities during human demonstrations. We then propose a multi-modal segmentation framework that leverages temporal models to detect semantically meaningful event boundaries in sequential manipulations. Evaluation on a challenging cable mounting task shows more than 90 percent segmentation accuracy and highlights a remarkable improvement with more modalities, which validates that TacUMI establishes a practical foundation for both scalable collection and segmentation of multi-modal demonstrations in contact-rich tasks.\n\nTacUMI: A Multi-Modal Universal Manipulation Interface for Contact-Rich Tasks\nTailai Cheng\n1,2\n, Kejia Chen\n1\n, Lingyun Chen\n1\n, Liding Zhang\n1\n, Yue Zhang\n1\n, Yao Ling\n1\n,\nMahdi Hamad\n2\n, Zhenshan Bing\n3,1\n, Fan Wu\n4,1\n, Karan Sharma\n2\n, Alois Knoll\n1\n1\nSchool of Computation, Information and Technology, Technical University of Munich, Germany.\n2\nAgile Robots SE, Munich, Germany\n3\nState Key Laboratory for Novel Software Technology and the School of Science and Technology, Nanjing University (Suzhou Campus), China.\n4\nShanghai University, China.\nAbstract\nTask decomposition is critical for understanding and learning complex long-horizon manipulation tasks.\nEsp",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "manipulation",
        "multi-modal",
        "contact-rich tasks",
        "segmentation",
        "demonstrations"
    ],
    "one_line_summary": "TacUMIæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ•°æ®æ”¶é›†ç³»ç»Ÿï¼Œç”¨äºæ¥è§¦ä¸°å¯Œçš„æ“ä½œä»»åŠ¡ï¼Œé€šè¿‡é›†æˆä¼ æ„Ÿå™¨å®ç°é«˜è´¨é‡æ¼”ç¤ºæ•°æ®çš„åŒæ­¥é‡‡é›†å’Œåˆ†å‰²ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T00:14:28Z",
    "created_at": "2026-01-27T15:53:15.630950",
    "updated_at": "2026-01-27T15:53:15.630956"
}