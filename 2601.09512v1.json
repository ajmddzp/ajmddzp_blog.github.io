{
    "id": "2601.09512v1",
    "title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion",
    "authors": [
        "Ralf RÃ¶mer",
        "Yi Zhang",
        "Angela P. Schoellig"
    ],
    "abstract": "ä¸ºæ•™ä¼šæœºå™¨äººæ‰§è¡Œå¤æ‚çš„æ“ä½œä»»åŠ¡ï¼Œå½“å‰æ™®éé‡‡ç”¨çš„æ–¹æ³•æ˜¯åœ¨ä»»åŠ¡ç‰¹å®šæ•°æ®ä¸Šå¯¹é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œç”±äºè¿™ç§æ–¹æ³•ä¼šæ›´æ–°ç°æœ‰è¡¨å¾ï¼Œæ— æ³•é€‚åº”æœºå™¨äººåœ¨ç°å®ä¸–ç•Œä¸­éœ€è¦é•¿æœŸè¿è¡Œçš„éœ€æ±‚â€”â€”æœºå™¨äººå¿…é¡»æŒç»­é€‚åº”æ–°ä»»åŠ¡ä¸ç¯å¢ƒï¼ŒåŒæ—¶ä¿æŒå·²æŒæ¡çš„çŸ¥è¯†ã€‚ç°æœ‰çš„æœºå™¨äººæŒç»­å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å­˜å‚¨å†å²æ•°æ®ï¼ˆæ ·æœ¬ï¼‰ï¼Œéš¾ä»¥åº”å¯¹é•¿ä»»åŠ¡åºåˆ—ï¼Œæˆ–ä¾èµ–ä»»åŠ¡æ ‡è¯†ç¬¦è¿›è¡Œéƒ¨ç½²ã€‚ä¸ºçªç ´è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºCLAREæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€æ ·æœ¬ã€å‚æ•°é«˜æ•ˆçš„é€šç”¨æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºVLAæ¨¡å‹ã€‚CLAREé€šè¿‡åœ¨é€‰å®šçš„å‰é¦ˆå±‚ä¸­å¼•å…¥è½»é‡çº§æ¨¡å—åŒ–é€‚é…å™¨ï¼Œå¹¶ä¾æ®å±‚çº§ç‰¹å¾ç›¸ä¼¼æ€§æŒ‡å¯¼ï¼Œåœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ä»…åœ¨å¿…è¦å¤„è‡ªä¸»æ‰©å±•æ¨¡å‹ã€‚åœ¨éƒ¨ç½²é˜¶æ®µï¼ŒåŸºäºè‡ªç¼–ç å™¨çš„è·¯ç”±æœºåˆ¶æ— éœ€ä»»åŠ¡æ ‡ç­¾å³å¯åŠ¨æ€æ¿€æ´»æœ€ç›¸å…³çš„é€‚é…å™¨ã€‚é€šè¿‡åœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜CLAREèƒ½åœ¨æ–°ä»»åŠ¡ä¸Šå®ç°é«˜æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶æœ‰æ•ˆé¿å…å¯¹å…ˆå‰ä»»åŠ¡çš„ç¾éš¾æ€§é—å¿˜ï¼Œå…¶æ€§èƒ½ç”šè‡³æ˜¾è‘—è¶…è¶ŠåŸºäºæ ·æœ¬çš„æ–¹æ³•ã€‚ä»£ç ä¸æ•°æ®å·²å‘å¸ƒäºhttps://tum-lsy.github.io/clareã€‚",
    "url": "https://arxiv.org/abs/2601.09512v1",
    "html_url": "https://arxiv.org/html/2601.09512v1",
    "html_content": "CLARE:\nC\nontinual\nL\nearning for Vision-Language-Action Models\nvia Autonomous\nA\ndapter\nR\nouting and\nE\nxpansion\nRalf RÃ¶mer\nâˆ—,1\nYi Zhang\nâˆ—,1\nAngela P. Schoellig\n1,2\nâˆ—\nEqual contribution.\n1\nTechnical University of Munich, Germany; TUM School of Computation, Information and Technology, Learning Systems and Robotics Lab; Munich Institute of Robotics and Machine Intelligence (MIRMI).\n2\nRobotics Institute Germany.Corresponding email:\n{ralf.roemer@tum.de}\nAbstract\nTo teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data.\nHowever, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired.\nExisting continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment.\nTo address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs.\nCLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity.\nDuring deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels.\nThrough extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods.\nCode and data are available at the project website:\ntum-lsy.github.io/clare\n.\nI\nIntroduction\nRobots deployed in homes, hospitals, or warehouses must operate for long periods while facing ever-changing conditions and task demands.\nA household robot may encounter a newly purchased appliance, or an assistive robot may meet patients with unfamiliar mobility profiles.\nIn such settings, robots must continually acquire new skills without sacrificing previously acquired capabilities.\nThis long-term adaptability, known as continual or lifelong learning\n[\nparisi2019continual\n]\n, remains an open challenge in robotics despite decades of research\n[\nthrun1995lifelong\n,\nbillard2025roadmap\n,\ndohare2024loss\n]\n.\nFigure 1:\nStarting from a pretrained vision-language-action modelÂ (VLA), CLARE autonomously and continually expands selected feedforward networkÂ (FFN) layers with new lightweight adapters.\nDuring inference, the most relevant adapters are selected based on feature similarity, captured by learned autoencoder discriminators.\nBy freezing existing parameters and fine-tuning only the new ones at each stage, we can acquire new task-specific knowledge without catastrophic forgetting of previously learned skills.\nRecent advances in vision-language-action models (VLAs) have demonstrated impressive performance on complex, long-horizon tasks by integrating perception, language understanding, and action generation within a unified model\n[\nopenvla\n,\npi0_5\n,\nflower\n,\nsmolvla\n]\n.\nPre-training on diverse data sources, including internet-scale vision-language data and robot demonstrations\n[\nopenX\n]\n, provides VLAs with broad priors that enable a certain degree of generalization\n[\npi0_5\n]\n.\nHowever, state-of-the-art VLAs still cannot adapt reliably to unseen tasks without fine-tuning on task-specific data\n[\npi0_5\n,\nflower\n,\nsmolvla\n]\n.\nIn a continual learning setting, where new tasks and environments emerge over time, a naive approach would be to iteratively fine-tune a VLA on new datasets.\nHowever, updating the parameters shared across modalities without regard for previously learned representations leads to significant degradation of both semantic grounding and policy performance on old tasks, a common issue known as catastrophic forgetting\n[\nforgetting\n]\n.\nTherefore, current VLAs are not inherently capable of continual learning, where new tasks and environments emerge over time.\nExperience replay\n[\nlopez2017gradient\n,\nexperience_replay\n,\nxie2022lifelong\n]\ncan mitigate forgetting; however, it has major practical drawbacks in robotics. Past data may be unavailable due to storage or privacy constraints in a lifelong learning setting, selecting representative samples for replay is challenging, and maintaining and accessing a large replay buffer increases both computational and memory overhead.\nHence, there is a strong need for exemplar-free continual learning methods for VLAs that can preserve old knowledge while acquiring new skills.\nModular and expandable architectures\n[\ntail\n,\nsemoa\n]\nrepresent a promising direction for scalable continual learning as they allocate new capacity for each task instead of overwriting shared representations.\nHowever, existing approaches in this direction typically require oracle task identifiers, which are generally not available when robots operate autonomously in open-world settings.\nMoreover, the application of these methods to robotics has so far been mostly limited to multitask learning, where all tasks and data are available in advance\n[\ntail\n,\nsdp\n]\n.\nTo close this gap, we introduce Continual Learning via Adapter Routing and ExpansionÂ (CLARE), a general framework that enables VLAs to continually incorporate new task-specific knowledge without exemplars, task labels, or pre-defined expansion rules.\nCLARE injects lightweight adapters into a subset of feedforward layers and expands only when the feature statistics indicate substantial novelty, as visualized in\nFigure\n1\n.\nAt deployment, an autoencoder-based routing mechanism dynamically selects among the adapters, enabling autonomous task-agnostic inference.\nThis design maintains a balance between stability and plasticity by preserving pre-trained representations and adding capacity as needed, enabling VLAs to learn new tasks with minimal parameter growth.\nIn summary, our main contributions are:\nâ€¢\nA lightweight, modular adapter framework enabling VLAs to acquire new skills without overwriting prior representations.\nâ€¢\nAn autonomous routing mechanism that activates the most suitable adapters during inference using feature similarity without task identifiers.\nâ€¢\nA dynamic expansion strategy that increases parameter count by only about 2% per task in our experiments.\nâ€¢\nExtensive simulation experiments on the LIBERO benchmark demonstrating that CLARE significantly outperforms continual learning baselines.\nWe focus on relatively small models in this work to evaluate the validity of CLARE with limited computational resources.\nHowever, our ideas can be straightforwardly extended to large-scale VLAs in the future.\nII\nRelated Work\nII-\n1\nVision-Language-Action Models\nBuilding upon the success of scaling laws in vision and language models\n[\nscaling_law\n,\nachiam2023gpt\n]\n, the robotics community has recently begun to collect massive multimodal datasets\n[\nopenX\n]\nand exploit generative modeling architectures\n[\nho2020denoising\n,\nflow_matching\n]\nto equip robots with broad semantic priors and task generalization abilities\n[\nopenvla\n,\nflower\n,\npi0_5\n,\nsmolvla\n]\n.\nThese VLAs are usually based on vision-language models\n[\npali3\n]\nand pre-trained on robot demonstrations\n[\nopenX\n]\nvia imitation learning, enabling end-to-end mapping from high-dimensional multimodal observations to robot actions.\nDespite the growing scale of training data and model capacity, the ability of VLAs to generalize zero-shot to unseen tasks and environments remains very limited\n[\npi0_5\n,\nlin2024data\n]\n.\nVLAs often overfit to their pre-training domains, as real-world robot data is much more expensive and scarce than web-scale vision and text datasets\n[\nbillard2025roadmap\n]\n.\nTherefore, to achieve high performance on a specific task, it has become the standard approach to fine-tune a pre-trained VLA on curated, high-quality demonstrations\n[\nopenvla\n,\nflower\n,\npi0_5\n,\nsmolvla\n]\n.\nHowever, in settings where tasks arrive sequentially and old data may be unavailable, this naive fine-tuning recipe is inadequate since it overwrites previously learned task knowledge, leading to catastrophic forgetting\n[\nzhou2025learning\n]\n.\nII-\n2\nContinual Learning\nAcquiring new skills from a stream of data without catastrophic forgetting of previously learned capabilities or losing plasticity is a hard problem in deep learning\n[\ndohare2024loss\n]\n.\nA widely used approach is experience replay (ER)\n[\nlopez2017gradient\n,\nexperience_replay\n,\nxie2022lifelong\n]\n, which retains a subset of past examples and mixes them together with new data to preserve existing representations during training.\nSince storing exemplars may be infeasible, regularization-based methods\n[\newc\n,\nzenke2017continual\n]\nconstrain parameter updates for weights deemed important to a past task.\nFor example, Elastic Weight ConsolidationÂ (EWC)\n[\newc\n]\nuses a Fisher-information penalty to protect critical parameters during subsequent training.\nRelated to this idea, PackNet\n[\npacknet\n]\nprunes less relevant parameters from the previously learned task and re-trains them for the new incoming data.\nHowever, methods like EWC or PackNet struggle with long task sequences as they are restricted by a fixed set of initial parameters.\nTo overcome capacity bottlenecks and avoid catastrophic forgetting, architectural methods\n[\nrusu2016progressive\n,\ntail\n,\nsemoa\n]\ninject new parameters or modules for novel tasks.\nBy keeping the original model frozen and leveraging techniques such as low-rank adaptationÂ (LoRA)\n[\nlora\n]\n, these methods can store new task-specific knowledge in a memory-efficient way.\nII-\n3\nContinual Imitation Learning in Robotics\nData scarcity and safety concerns make continual learning for robotics particularly challenging\n[\nthrun1995lifelong\n,\nbillard2025roadmap\n]\n.\nLOTUS\n[\nlotus\n]\n, a hierarchical method, constructs an ever-growing library of skill policies\n[\nbuds\n]\nand uses a meta-policy to select the correct skill during deployment.\nHowever, training the meta-policy requires ER of previous data, leading to high storage requirements.\nSparse Diffusion PolicyÂ (SDP)\n[\nsdp\n]\nintroduces task-specific expert modules to diffusion policies but requires oracle task identifiers during deployment to manually dispatch the input to the correct expert.\nHence, SDP cannot operate fully autonomously in continual learning scenarios.\nAnother recent work\n[\nats\n]\nfine-tunes a pre-trained base VLA checkpoint on each new task,\nand employs a task scheduler to select from a model library for deployment.\nHowever, this method is not memory-efficient and does not enable knowledge sharing between tasks, limiting its practicality for long-term deployment in the real world.\nIII\nProblem Setup\nWe consider a robotic system with state\nğ’”\nt\n\\bm{s}_{t}\nand action\nğ’‚\nt\n{\\bm{a}}_{t}\nat timestep\nt\nt\n.\nWe focus on the common problem of task-incremental learning, in which the robot must sequentially learn new tasks\n{\nğ’¯\nn\n}\nn\n=\n1\nN\n{\\{\\mathcal{T}_{n}\\}_{n=1}^{N}}\nduring its operational lifetime, with the total number of tasks,\nN\nN\n, unknown.\nEach task\nğ’¯\nn\n=\n(\nÏ\n0\nn\n,\nğ’\nn\n)\n{\\mathcal{T}_{n}=(\\rho_{0}^{n},\\bm{l}_{n})}\nis characterized by an initial distribution of the state of the robot and the environment\nÏ\n0\nn\n\\rho_{0}^{n}\nand a natural language instruction\nğ’\nn\n\\bm{l}_{n}\ndescribing the desired outcome.\nWe assume the availability of a base policy\nÏ€\n0\n=\nÏ€\nğœ½\n0\n{\\pi_{0}=\\pi_{\\bm{\\theta}_{0}}}\nwith model parameters\nğœ½\n0\n\\bm{\\theta}_{0}\nthat has been pre-trained on large-scale internet and robot data.\nThe policy takes as input an observation\nğ’\nt\n=\n(\nğ‘°\nt\n1\n,\nâ€¦\n,\nğ‘°\nt\nN\nc\n,\nğ’’\nt\n,\nğ’\n)\n{{\\bm{o}}_{t}=(\\bm{I}_{t}^{1},\\dots,\\bm{I}_{t}^{N_{\\text{c}}},\\bm{q}_{t},\\bm{l})}\nconsisting of camera images\nğ‘°\nt\nn\nc\n\\bm{I}_{t}^{n_{\\text{c}}}\n,\nn\nc\n=\n1\n,\nâ€¦\n,\nN\nc\n{n_{\\text{c}}=1,\\dots,N_{\\text{c}}}\n, proprioceptive state\nğ’’\nt\n\\bm{q}_{t}\nand language instruction\nğ’\n\\bm{l}\n, and generates an action chunkÂ (sequence)\nğ‘¨\nt\n=\n(\nğ’‚\nt\n,\nâ€¦\n,\nğ’‚\nt\n+\nH\nâˆ’\n1\n)\nâˆ¼\nÏ€\n0\n(\nâ‹…\n|\nğ’\nt\n)\n\\bm{A}_{t}=({\\bm{a}}_{t},\\dots,{\\bm{a}}_{t+H-1})\\sim\\pi_{0}(\\cdot\\,|\\,{\\bm{o}}_{t})\n.\nThe first\nh\nâ‰¤\nH\n{h\\leq H}\nactions in\nğ‘¨\nt\n\\bm{A}_{t}\nare applied to the robot, and the policy generates a new action chunk at timestep\nt\n+\nh\nt+h\nin a receding horizon manner\n[\nchi2023diffusion\n,\nzhao2023learning\n]\n.\nPre-training has provided the base policy with general visual, language, and action representations, but it cannot solve new tasks zero-shot.\nThe robot should be able to learn a new task\nğ’¯\nn\n\\mathcal{T}_{n}\nat stage\nn\nn\nwhile retaining the general knowledge from pre-training and without forgetting how to solve previous tasks\nğ’¯\n1\n,\nâ€¦\n,\nğ’¯\nn\nâˆ’\n1\n\\mathcal{T}_{1},\\dots,\\mathcal{T}_{n-1}\n.\nSpecifically, given an expert demonstration dataset\nğ’Ÿ\nn\n=\n{\n(\nğ’\nt\nn\n,\nğ’‚\nt\nn\n)\n,\nğ’\nn\n}\nt\n=\n1\nT\n{\\mathcal{D}_{n}=\\{({\\bm{o}}_{t}^{n},\\bm{a}_{t}^{n}),\\bm{l}_{n}\\}_{t=1}^{T}}\nof observation-action pairs for task\nğ’¯\nn\n\\mathcal{T}_{n}\n, we aim to train a new policy\nÏ€\nn\n=\nÏ€\nğœ½\nn\n{\\pi_{n}=\\pi_{\\bm{\\theta}_{n}}}\nwith parameters\nğœ½\nn\n{\\bm{\\theta}_{n}}\n.\nDue to the aforementioned reasons, we consider exemplar-free continual learning, i.e., data from earlier stages\nğ’Ÿ\n1\n,\nâ€¦\n,\nğ’Ÿ\nn\nâˆ’\n1\n\\mathcal{D}_{1},\\dots,\\mathcal{D}_{n-1}\nis\nnot\navailable.\nHence, only the previous model parameters\nğœ½\nn\nâˆ’\n1\n\\bm{\\theta}_{n-1}\nand the new data\nğ’Ÿ\nn\n\\mathcal{D}_{n}\ncan be used to adapt the policy to the new task.\nIV\nMethodology\nAlgorithm 1\nContinual learning for VLAs with CLARE.\n1:\nPretrained base VLA policy with parameters\nğœ½\n0\n\\bm{\\theta}_{0}\n, set of expandable layers\nâ„°\n\\mathcal{E}\n, expansion threshold\nÎ³\n\\gamma\n.\n2:\nfor all\nlayers\nâ„“\nâˆˆ\nâ„°\n\\ell\\in\\mathcal{E}\ndo\n3:\nSet\nğ’œ\nâ„“\n=\nâˆ…\n\\mathcal{A}_{\\ell}=\\emptyset\n,\nk\nâ„“\n=\n0\nk_{\\ell}=0\n.\nâŠ³\n\\triangleright\nInitialize adapter modules\n4:\nSet\nğ’Ÿ\nâ„“\n=\nâˆ…\n\\mathcal{D}_{\\ell}=\\emptyset\n.\nâŠ³\n\\triangleright\nInitialize discriminator modules\n5:\nfor all\ntasks\nğ’¯\nn\nâˆˆ\n{\nğ’¯\n1\n,\nğ’¯\n2\n,\nâ€¦\n}\n\\mathcal{T}_{n}\\in\\{\\mathcal{T}_{1},\\mathcal{T}_{2},\\dots\\}\ndo\nâŠ³\n\\triangleright\nContinual learn.\n6:\nSet\nğœ½\nn\nâ†\nğœ½\nn\nâˆ’\n1\n\\bm{\\theta}_{n}\\leftarrow\\bm{\\theta}_{n-1}\n.\n7:\nCollect demonstration data\nğ’Ÿ\nn\n\\mathcal{D}_{n}\n.\n8:\nfor all\nlayers\nâ„“\nâˆˆ\nâ„°\n\\ell\\in\\mathcal{E}\ndo\nâŠ³\n\\triangleright\nDynamic Expansion\n9:\nfor all\ndiscriminators\nD\nâ„“\nj\nâˆˆ\nğ’Ÿ\nâ„“\nD_{\\ell}^{j}\\in\\mathcal{D}_{\\ell}\ndo\n10:\nCompute\nz\nz\n-score\nz\nâ„“\nj\nz_{\\ell}^{j}\nviaÂ (\n7\n).\n11:\nExpand\nğ’Ÿ\nâ„“\nâ†\nğ’Ÿ\nâ„“\nâˆª\n{\nD\nâ„“\nn\n}\n{\\mathcal{D}_{\\ell}\\leftarrow\\mathcal{D}_{\\ell}\\cup\\big\\{D_{\\ell}^{n}\\big\\}}\n.\nâŠ³\n\\triangleright\nNew discriminator\n12:\nUpdate model parameters:\nğœ½\nn\nâ†\n(\nğœ½\nn\n,\nD\nâ„“\nn\n)\n\\bm{\\theta}_{n}\\leftarrow(\\bm{\\theta}_{n},D_{\\ell}^{n})\n.\n13:\nif\nn\n=\n1\nn=1\nor\nz\nâ„“\nj\n>\nÎ³\nz_{\\ell}^{j}>\\gamma\nfor all\nj\n=\n1\n,\nâ€¦\n,\nn\nâˆ’\n1\nj=1,\\dots,{n-1}\nthen\n14:\nSet\nk\nâ„“\nâ†\nk\nâ„“\n+\n1\nk_{\\ell}\\leftarrow k_{\\ell}+1\n.\n15:\nExpand\nğ’œ\nâ„“\nâ†\nğ’œ\nâ„“\nâˆª\n{\nA\nâ„“\nk\nâ„“\n}\n{\\mathcal{A}_{\\ell}\\leftarrow\\mathcal{A}_{\\ell}\\cup\\big\\{A_{\\ell}^{k_{\\ell}}\\big\\}}\n.\nâŠ³\n\\triangleright\nNew adapter\n16:\nUpdate model parameters:\nğœ½\nn\nâ†\n(\nğœ½\nn\n,\nA\nâ„“\nk\nâ„“\n)\n\\bm{\\theta}_{n}\\leftarrow(\\bm{\\theta}_{n},A_{\\ell}^{k_{\\ell}})\n.\n17:\nLink new discriminator\nB\nâ„“\nâ€‹\n(\nD\nâ„“\nn\n)\n=\nA\nâ„“\nk\nâ„“\nB_{\\ell}(D_{\\ell}^{n})=A_{\\ell}^{k_{\\ell}}\n.\n18:\nelse\n19:\nLink\nD\nâ„“\nn\nD_{\\ell}^{n}\nto an existing adapter viaÂ (\n8\n).\n20:\nif\nn\n>\n1\nn>1\nand no layers\nâ„“\nâˆˆ\nâ„°\n\\ell\\in\\mathcal{E}\nwere expanded\nthen\n21:\nExpand the shallowest layer\nâ„“\n1\nâˆˆ\nâ„°\n\\ell_{1}\\in\\mathcal{E}\n.\n22:\nTrain\nA\nâ„“\nk\nâ„“\nA_{\\ell}^{k_{\\ell}}\nof all layers\nâ„“\nâˆˆ\nâ„°\n\\ell\\in\\mathcal{E}\nfrom\nğ’Ÿ\nn\n\\mathcal{D}_{n}\nviaÂ (\n1\n).\n23:\nTrain\nD\nâ„“\nn\nD_{\\ell}^{n}\nof all layers\nâ„“\nâˆˆ\nâ„°\n\\ell\\in\\mathcal{E}\nfrom\nğ’Ÿ\nn\n\\mathcal{D}_{n}\nviaÂ (\n5\n).\nIn this section, we describe our proposed method, CLARE, for continual learning without forgetting for pre-trained generative policies.\nThe training and inference strategies are summarized in Algorithms\n1\nand\n2\n, respectively.\nIV-A\nBase Policy\nDemonstration datasets for imitation learning typically involve high-dimensional, multimodal distributions\n[\nurain2024deep\n]\n.\nTo capture these distributions, we train the policy using flow matching\n[\nflow_matching\n]\nduring both pre-training and continual learning.\nHereby, the policy at stage\nn\nn\nlearns a vector field\nğ’—\nğœ½\nn\n\\bm{v}_{\\bm{\\theta}_{n}}\nthat transports action chunk samples from a simple base distribution (e.g., a Gaussian) to the target distribution.\nWe adopt the standard conditional flow matching loss\nâ„’\nâ€‹\n(\nğœ½\nn\n)\n=\nğ”¼\ns\n,\n(\nğ‘¨\n1\n,\nğ’\n)\n,\nğ‘¨\n0\nâ€‹\n[\nâ€–\nğ’—\nğœ½\nn\nâ€‹\n(\nğ‘¨\ns\n,\nğ’\n,\ns\n)\nâˆ’\n(\nğ‘¨\n1\nâˆ’\nğ‘¨\n0\n)\nâ€–\n2\n]\n,\n\\displaystyle\\mathcal{L}(\\bm{\\theta}_{n})\\!=\\mathbb{E}_{s,(\\bm{A}^{1},{\\bm{o}}),\\bm{A}^{0}}\\!\\left[\\left\\|\\bm{v}_{\\bm{\\theta}_{n}}(\\bm{A}^{s},{\\bm{o}},s)-(\\bm{A}^{1}\\!-\\!\\bm{A}^{0})\\right\\|_{2}\\right]\\!,\\!\n(1)\nwhere\ns\nâˆ¼\nğ’°\nâ€‹\n(\n[\n0\n,\n1\n]\n)\ns\\sim\\mathcal{U}([0,1])\n,\n(\nğ‘¨\n1\n,\nğ’\n)\nâˆ¼\nğ’Ÿ\nn\n(\\bm{A}^{1},{\\bm{o}})\\sim\\mathcal{D}_{n}\n,\nğ‘¨\n0\nâˆ¼\nğ’©\nâ€‹\n(\nğŸ\n,\nğ‘°\n)\n\\bm{A}^{0}\\sim\\mathcal{N}(\\bm{0},\\bm{I})\n, and\nğ‘¨\ns\n=\n(\n1\nâˆ’\ns\n)\nâ€‹\nğ‘¨\n0\n+\ns\nâ€‹\nğ‘¨\n1\n\\bm{A}^{s}=(1-s)\\bm{A}^{0}+s\\bm{A}^{1}\n.\nAfter training, we can generate new action chunks\nğ‘¨\nt\n=\nğ‘¨\n1\nâˆ¼\nÏ€\nn\n(\nâ‹…\n|\nğ’\nt\n)\n\\bm{A}_{t}=\\bm{A}^{1}\\sim\\pi_{n}(\\cdot\\,|\\,{\\bm{o}}_{t})\nby Euler integration of the learned vector field, starting from Gaussian noise\nğ‘¨\n0\nâˆ¼\nğ’©\nâ€‹\n(\nğŸ\n,\nğ‘°\n)\n{\\bm{A}^{0}\\sim\\mathcal{N}(\\bm{0},\\bm{I})}\n, via\nğ‘¨\ns\n+\nÎ´\nâ€‹\ns\n=\nğ‘¨\ns\n+\nÎ´\nâ€‹\ns\nâ€‹\nğ’—\nğœ½\nn\nâ€‹\n(\nğ‘¨\ns\n,\nğ’\nt\n,\ns\n)\n\\bm{A}^{s+\\delta s}=\\bm{A}^{s}+\\delta s\\bm{v}_{\\bm{\\theta}_{n}}\\left(\\bm{A}^{s},{\\bm{o}}_{t},s\\right)\n, with\nK\n=\nâŒˆ\n1\n/\nÎ´\nâ€‹\ns\nâŒ‰\nK=\\lceil 1/{\\delta s}\\rceil\nintegration steps.\nNearly all state-of-the-art VLAs are transformer-based\n[\nopenvla\n,\nflower\n,\npi0_5\n,\nsmolvla\n]\n.\nHowever, since our method is agnostic to the specific model architecture, we keep the following presentation of our methodology general.\nAlgorithm 2\nAutonomous routing during deployment.\n1:\nAdapters\nğ’œ\nâ„“\n\\mathcal{A}_{\\ell}\n, discriminators\nğ’Ÿ\nâ„“\n\\mathcal{D}_{\\ell}\n, learned linking\nB\nâ„“\n:\nğ’Ÿ\nâ„“\nâ†’\nğ’œ\nâ„“\nB_{\\ell}:\\mathcal{D}_{\\ell}\\rightarrow\\mathcal{A}_{\\ell}\n, input feature\nğ’™\nâ„“\n{\\bm{x}}_{\\ell}\n.\n2:\nfor all\ndiscriminators\nD\nâ„“\nj\nâˆˆ\nğ’Ÿ\nâ„“\nD_{\\ell}^{j}\\in\\mathcal{D}_{\\ell}\ndo\n3:\nCompute the reconstruction error\nğ’†\nâ„“\nj\nâ€‹\n(\nğ’™\nâ„“\n)\n\\bm{e}_{\\ell}^{j}({\\bm{x}}_{\\ell})\nviaÂ (\n4\n).\n4:\nSelect the most relevant adapter\nA\nâ„“\nâˆ—\nâˆˆ\nğ’œ\nâ„“\nA_{\\ell}^{*}\\in\\mathcal{A}_{\\ell}\nviaÂ (\n6\n).\n5:\nSum the outputs of original FFN and adapter viaÂ (\n3\n).\nIV-B\nModularized Adapters\nThe policy must continually acquire new task-specific knowledge, but it should leverage the general representations from pre-training to adapt to new tasks in a parameter-efficient way.\nTo achieve this, we draw inspiration from the mixture-of-expertsÂ (MoE) approach in large language modelsÂ (LLMs)\n[\nshazeer2017outrageously\n,\ndai2024deepseekmoe\n]\n, which combines the outputs of specialized sub-networks during inference.\nHowever, while their number is fixed in MoE, our setup requires continually injecting parameters into the model to learn new tasks, and we aim to do this in a memory-efficient way.\nMultiple studies have shown that a large fraction of factual associations and high-level knowledge in transformer-based LLMs is stored inside mid-layer feedforward (FFN) modules\n[\nmeng2022locating\n,\ngeva2021transformer\n]\n.\nMotivated by this insight, we define a set of\nn\ne\nn_{\\text{e}}\nexpandable FFN layers\nâ„°\n=\n{\nâ„“\n1\n,\nâ€¦\n,\nâ„“\nn\ne\n}\n{\\mathcal{E}=\\{\\ell_{1},\\dots,\\ell_{n_{\\text{e}}}\\}}\nfor continual learning.\nAt each stage\nn\nn\n, a maximum of one adapter module is added as a side branch per expandable layer according to the dynamic expansion strategy detailed in\nSection\nIV-D\n.\nWe employ a lightweight encoder-decoder structure with ReLU activation functions for the adapters.\nDenoting the input feature of an expandable layer\nâ„“\nâˆˆ\nâ„°\n{\\ell\\in\\mathcal{E}}\nby\nğ’™\nâ„“\nâˆˆ\nâ„\nd\nâ„“\n\\bm{x}_{\\ell}\\in\\mathbb{R}^{d_{\\ell}}\n, the output of the\ni\ni\n-th adapter in that layer is given by\nA\nâ„“\ni\nâ€‹\n(\nğ’™\nâ„“\n)\n=\nğ‘¾\nâ„“\n,\ni\nup\nâ€‹\nReLU\nâ€‹\n(\nğ‘¾\nâ„“\n,\ni\ndown\nâ€‹\nğ’™\nâ„“\n)\n,\n\\displaystyle A_{\\ell}^{i}(\\bm{x}_{\\ell})=\\bm{W}_{\\ell,i}^{\\text{up}}\\mathrm{ReLU}\\big(\\bm{W}_{\\ell,i}^{\\text{down}}\\bm{x}_{\\ell}\\big),\n(2)\nwhere\nğ‘¾\nâ„“\n,\ni\nup\nâˆˆ\nâ„\nd\nâ„“\nÃ—\nr\n{\\bm{W}_{\\ell,i}^{\\text{up}}\\in\\mathbb{R}^{d_{\\ell}\\times r}}\n,\nğ‘¾\nâ„“\n,\ni\ndown\nâˆˆ\nâ„\nr\nÃ—\nd\nâ„“\n{\\bm{W}_{\\ell,i}^{\\text{down}}\\in\\mathbb{R}^{r\\times d_{\\ell}}}\n, and\nr\nâ‰ª\nd\nâ„“\nr\\ll d_{\\ell}\n.\nWe denote the set of adapters in layer\nâ„“\n\\ell\nat stage\nn\nn\nby\nğ’œ\nâ„“\nn\n=\n{\nA\nâ„“\n1\n,\nâ€¦\n,\nA\nâ„“\nk\nâ„“\n}\n{\\mathcal{A}_{\\ell}^{n}=\\big\\{A_{\\ell}^{1},\\dots,A_{\\ell}^{k_{\\ell}}\\big\\}}\nwith\nk\nâ„“\nâ‰¤\nn\nk_{\\ell}\\leq n\n.\nTo maintain distinct representations for each task, we train only the newly added adapters on the data\nğ’Ÿ\nn\n\\mathcal{D}_{n}\nand freeze the rest of the model.\nDuring inference, a routing mechanism described in\nSection\nIV-C\nactivates one adapter\nA\nâ„“\nâˆ—\nâˆˆ\nğ’œ\nâ„“\n{A_{\\ell}^{*}\\in\\mathcal{A}_{\\ell}}\nper layer, and its output is added to that of the original pre-trained layer\nFFN\nâ„“\npre\nâ€‹\n(\nâ‹…\n)\n\\mathrm{FFN}_{\\ell}^{\\text{pre}}(\\cdot)\nas\nFFN\nâ„“\nâ€‹\n(\nğ’™\nâ„“\n)\n=\nFFN\nâ„“\npre\nâ€‹\n(\nğ’™\nâ„“\n)\n+\nA\nâ„“\nâˆ—\nâ€‹\n(\nğ’™\nâ„“\n)\n.\n\\displaystyle\\mathrm{FFN}_{\\ell}(\\bm{x}_{\\ell})=\\mathrm{FFN}_{\\ell}^{\\text{pre}}(\\bm{x}_{\\ell})+A_{\\ell}^{*}(\\bm{x}_{\\ell}).\n(3)\nAdding new adapters as parallel side branches to the model is beneficial as it preserves the original network structure and does not change the input and output of existing layers and adapters.\nIV-C\nAutonomous Routing\nDuring deployment, a routing mechanism needs to determine which adapter\nA\nâ„“\nâˆ—\nâˆˆ\nğ’œ\nâ„“\nA_{\\ell}^{*}\\in\\mathcal{A}_{\\ell}\nto activate in each layer\nâ„“\nâˆˆ\nâ„°\n\\ell\\in\\mathcal{E}\n.\nThis selection should be autonomous and based solely on the current observation, i.e., without requiring task labels, since these are typically not provided in open, real-world scenarios.\nUnlike fixed-size routing in MoE, our setup requires selecting from a continually increasing set of adapters.\nWe achieve this by designing an expandable and lightweight routing mechanism that selects, for each expandable layer\nâ„“\nâˆˆ\nâ„°\n{\\ell\\in\\mathcal{E}}\n, the adapter most relevant to the current situation, as shown in\nFigure\n2\n.\nWe pair each layer with an expanding set of autoencoder discriminators\nD\nâ„“\n=\n{\nD\nâ„“\n1\n,\nD\nâ„“\n2\n,\nâ€¦\n}\n{D_{\\ell}=\\{D_{\\ell}^{1},D_{\\ell}^{2},\\dots\\}}\n, all of which receive the same features\nğ’™\nâ„“\n{\\bm{x}}_{\\ell}\nas input, and attach a new discriminator\nD\nâ„“\nn\nD_{\\ell}^{n}\nat each stage\nn\nn\n.\nEvery discriminator\nD\nâ„“\nj\nD_{\\ell}^{j}\n,\nj\n=\n1\n,\nâ€¦\n,\nn\nj=1,\\dots,n\n, is linked to one corresponding adapter\nA\nâ„“\ni\n=\nB\nâ„“\nâ€‹\n(\nD\nâ„“\nj\n)\n{A_{\\ell}^{i}=B_{\\ell}(D_{\\ell}^{j})}\nthrough a surjective mapping\nB\nâ„“\n:\nD\nâ„“\nâ†’\nğ’œ\nâ„“\n{B_{\\ell}:D_{\\ell}\\rightarrow\\mathcal{A}_{\\ell}}\n, as explained in\nSection\nIV-D\n.\nWe use the reconstruction errors of the discriminators\ne\nâ„“\nj\nâ€‹\n(\nğ’™\nâ„“\n)\n=\nâ€–\nğ’™\nâ„“\nâˆ’\nD\nâ„“\nj\nâ€‹\n(\nğ’™\nâ„“\n)\nâ€–\n2\n,\nj\n=\n1\n,\nâ€¦\n,\nn\n,\n\\displaystyle e_{\\ell}^{j}({\\bm{x}}_{\\ell})=\\left\\|{\\bm{x}}_{\\ell}-D_{\\ell}^{j}({\\bm{x}}_{\\ell})\\right\\|_{2},\\qquad j=1,\\dots,n,\n(4)\nto determine the most relevant adapter.\nBy training the discriminators added at stage\nn\nn\nwith the loss\nâ„’\nrecon\nâ€‹\n(\nD\nâ„“\nn\n)\n=\nğ”¼\nğ’™\nâ„“\nâˆ¼\nğ’Ÿ\nn\nâ€‹\n[\ne\nâ„“\nj\nâ€‹\n(\nğ’™\nâ„“\n)\n]\n,\n\\displaystyle\\mathcal{L}_{\\text{recon}}(D_{\\ell}^{n})=\\mathbb{E}_{\\bm{x}_{\\ell}\\sim\\mathcal{D}_{n}}\\left[e_{\\ell}^{j}({\\bm{x}}_{\\ell})\\right],\n(5)\nwe ensure that the discriminators have a lower reconstruction error when the input features belong to the training distribution of their corresponding adapter.\nDuring inference, we activate the most relevant adapter that is linked to the discriminator with the smallest reconstruction errorÂ (\n4\n) via the routing mechanism\nA\nâ„“\nâˆ—\nâ€‹\n(\nğ’™\nâ„“\n)\n\\displaystyle A_{\\ell}^{*}({\\bm{x}}_{\\ell})\n=\nB\nâ„“\nâ€‹\n(\nD\nâ„“\nj\nâˆ—\n)\n,\n\\displaystyle=B_{\\ell}\\big(D_{\\ell}^{j^{*}}\\big),\n(6a)\nwhere\nj\nâˆ—\n\\displaystyle\\text{where}\\quad\\qquad j^{*}\n=\narg\nâ€‹\nmin\nj\nâˆˆ\n{\n1\n,\nâ€¦\n,\nn\n}\nâ¡\ne\nâ„“\nj\nâ€‹\n(\nğ’™\nâ„“\n)\n.\n\\displaystyle=\\operatorname*{arg\\,min}_{j\\in\\{1,\\dots,n\\}}e_{\\ell}^{j}({\\bm{x}}_{\\ell}).\n(6b)\nThe routing strategy for a layer\nâ„“\nâˆˆ\nâ„°\n\\ell\\in\\mathcal{E}\nis summarized in\nAlgorithm\n2\n.\nThe distribution of features\nğ’™\nâ„“\n\\bm{x}_{\\ell}\ndepends on the adapters in the shallower layers and, therefore, changes when training those adapters.\nTo ensure stable training of the discriminators, we adopt a two-stage training strategy. First, we train the new adapters jointly using the flow matching lossÂ (\n1\n). Then, we freeze all parameters except for the newly added discriminators and train them using the reconstruction lossÂ (\n5\n).\nFigure 2:\nCLARE sequentially adds adapters and discriminators as side branches to selected feedforward network layers of a pretrained VLA. Top: During inference, our routing mechanism activates only the most relevant adapter that is linked to the discriminator with the lowest reconstruction error for the input feature. Bottom: During the dynamic expansion phase, if all\nz\nz\n-scores exceed a threshold\nÎ³\n\\gamma\n, a new adapter and discriminator are added to the corresponding layer. If at least one\nz\nz\n-score value is smaller than\nÎ³\n\\gamma\n, we only add a discriminator and link it to the most relevant adapter.\nIV-D\nDynamic Expansion\nTo effectively capture task-specific knowledge without catastrophic forgetting in the context of exemplar-free continual learning, a certain expansion of the model is necessary for each new task.\nA straightforward approach would be to add new adapters to all expandable layers.\nHowever, this limits knowledge sharing between the tasks and leads to an excessive linear increase in the number of adapter parameters.\nTherefore, we only expand a layer\nâ„“\n\\ell\nat stage\nn\nn\nif the features of the new task\nğ’¯\nn\n\\mathcal{T}_{n}\ndeviate substantially from all previous tasks.\nSince the discriminators are trained on different data, comparing their reconstruction losses requires normalization.\nTo this end, we maintain the running mean\nÎ¼\nâ„“\nj\n\\mu_{\\ell}^{j}\nand standard deviation\nÏƒ\nâ„“\nj\n\\sigma_{\\ell}^{j}\nof the reconstruction loss for each discriminator\nD\nâ„“\nj\nD_{\\ell}^{j}\nand calculate the normalized\nz\nz\n-scores\nz\nâ„“\nj\nâ€‹\n(\nğ’™\nâ„“\n)\n=\n1\n|\nğ’Ÿ\nn\n|\nâ€‹\nâˆ‘\nğ’™\nâ„“\nâˆˆ\nğ’Ÿ\nn\ne\nâ„“\nj\nâ€‹\n(\nğ’™\nâ„“\n)\nâˆ’\nÎ¼\nâ„“\nj\nÏƒ\nâ„“\nj\n.\n\\displaystyle z_{\\ell}^{j}(\\bm{x}_{\\ell})=\\frac{1}{|\\mathcal{D}_{n}|}\\sum_{{\\bm{x}}_{\\ell}\\in\\mathcal{D}_{n}}\\frac{e_{\\ell}^{j}({\\bm{x}}_{\\ell})-\\mu_{\\ell}^{j}}{\\sigma_{\\ell}^{j}}.\n(7)\nIf all discriminators\nD\nâ„“\n1\n,\nâ€¦\n,\nD\nâ„“\nn\nâˆ’\n1\nD_{\\ell}^{1},\\dots,D_{\\ell}^{n-1}\nhave a\nz\nz\n-scoreÂ (\n7\n) larger than a threshold\nÎ³\n\\gamma\n, the features\nğ’™\nâ„“\n{\\bm{x}}_{\\ell}\nof the new task\nğ’¯\nn\n\\mathcal{T}_{n}\nin layer\nâ„“\n\\ell\nare out-of-distribution with respect to all previously learned tasks.\nIn this case, we expand the layer by a new adapter\nA\nâ„“\nk\nâ„“\nA_{\\ell}^{k_{\\ell}}\nand link the new discriminator\nD\nâ„“\nn\nD_{\\ell}^{n}\nto it, i.e.,\nB\nâ„“\nâ€‹\n(\nD\nâ„“\nn\n)\n=\nA\nâ„“\nk\nâ„“\nB_{\\ell}(D_{\\ell}^{n})=A_{\\ell}^{k_{\\ell}}\n.\nOn the contrary, if at least one discriminator\nD\nâ„“\nj\nD_{\\ell}^{j}\nyields a\nz\nz\n-score smaller than the specified threshold\nÎ³\n\\gamma\n, the layer\nâ„“\n\\ell\nis deemed not to require expansion.\nThis dynamic expansion strategy, illustrated in\nFigure\n2\n, results in a memory-efficient, sub-linear increase in the number of adapter parameters.\nEven if a layer is not expanded, we still need to attach a new discriminator to it.\nTo explain why this is necessary, we consider the following scenario:\nAssume that at stage\nn\nn\n, a new adapter\nA\nâ„“\n2\ni\nA_{\\ell_{2}}^{i}\nis added to layer\nâ„“\n2\n\\ell_{2}\n, but the shallower layer\nâ„“\n1\n\\ell_{1}\nis not expanded.\nThen, the routing mechanism activates only adapters from earlier stages in layer\nâ„“\n1\n\\ell_{1}\nduring training of\nA\nâ„“\n2\ni\nA_{\\ell_{2}}^{i}\n.\nHowever, a new adapter\nA\nâ„“\n1\nj\nA_{\\ell_{1}}^{j}\ncould be added to layer\nâ„“\n1\n\\ell_{1}\nat the next stage\nn\n+\n1\nn+1\n.\nIn this case, when revisiting task\nn\nn\n, the router might activate the new adapter\nA\nâ„“\n1\nj\nA_{\\ell_{1}}^{j}\ninstead of an earlier one.\nAs a consequence, the input features\nğ’™\nâ„“\n2\n\\bm{x}_{\\ell_{2}}\nto layer\nâ„“\n2\n\\ell_{2}\nwhen performing task\nğ’¯\nn\n\\mathcal{T}_{n}\nare\ndifferent\nfrom those seen during training of\nA\nâ„“\n2\ni\nA_{\\ell_{2}}^{i}\n.\nThis distribution shift in feature space can lead to unpredictable behavior and task failure\n[\ngu2025safe\n,\nromer2025failure\n]\n.\nTo avoid this problem and ensure consistent routing behavior, we attach an auxiliary discriminator if a layer is not expanded, as visualized in\nFigure\n2\n.\nThe auxiliary discriminator is linked to the same adapter as the existing discriminator with the smallest reconstruction lossÂ (\n5\n),\ni.e.,\nB\nâ„“\nâ€‹\n(\nD\nâ„“\nn\n)\n\\displaystyle B_{\\ell}(D_{\\ell}^{n})\n=\nA\nâ„“\ni\n=\nB\nâ„“\nâ€‹\n(\nD\nâ„“\nj\nâˆ—\n)\n,\n\\displaystyle=A_{\\ell}^{i}=B_{\\ell}(D_{\\ell}^{j^{*}}),\n(8a)\nwhere\nj\nâˆ—\n\\displaystyle\\text{where}\\quad\\qquad j^{*}\n=\narg\nâ€‹\nmin\nj\nâˆˆ\n{\n1\n,\nâ€¦\n,\nn\nâˆ’\n1\n}\nâ¡\nğ”¼\nğ’™\nâ„“\nâˆ¼\nğ’Ÿ\nn\nâ€‹\n[\ne\nâ„“\nj\nâ€‹\n(\nğ’™\nâ„“\n)\n]\n.\n\\displaystyle=\\operatorname*{arg\\,min}_{j\\in\\{1,\\dots,n-1\\}}\\mathbb{E}_{{\\bm{x}}_{\\ell}\\sim\\mathcal{D}_{n}}\\left[e_{\\ell}^{j}({\\bm{x}}_{\\ell})\\right].\n(8b)\nIntuitively, since adapterÂ (\n8a\n) was trained on features most similar to task\nğ’¯\nn\n\\mathcal{T}_{n}\n, we consider its learned representations to be transferable to\nğ’¯\nn\n\\mathcal{T}_{n}\n.\nWe note that with our dynamic expansion strategy, an adapter can potentially be activated by more than one discriminator, as shown inÂ Figures\n1\nand\n2\n.\nHyperparameter\nAdapters\nDiscriminators\n# Parameters (FFN)\n0.26M\n0.33M\n# Parameters (Proj.)\n3.2M\n1.4M\nLearning rate\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\n5\nÃ—\n10\nâˆ’\n4\n5\\times 10^{-4}\nLearning rate schedule\ncosine\nconstant\nBatch size\n32\n32\nTraining steps\n20,000\n2,000\nExpansion threshold\nÎ³\n\\gamma\n2.5\n-\nTABLE I:\nModel and training hyperparameters. The injected modules are much smaller than the base models, which have about 200M parameters.\nWe found that introducing at least some new parameters for each new task is essential for the policy to acquire and retain novel skills.\nIn addition, we observed that shallower layers typically exhibit a stronger distribution shift between tasks than deeper layers.\nThus, if no layer is deemed to require expansion, we still add an adapter to the shallowest layer\nâ„“\n1\nâˆˆ\nâ„°\n{\\ell_{1}\\in\\mathcal{E}}\nto capture the peculiarities of a new task.\nAt the first stage, we expand all layers\nâ„“\nâˆˆ\nâ„°\n{\\ell\\in\\mathcal{E}}\nby default.\nIn summary, our dynamic expansion mechanism ensures that CLARE adds only a small, task-dependent number of parameters without compromising performance when revisiting previous tasks.\nV\nEvaluation\nWe conduct extensive simulation experiments\nwith the primary goal of answering the following research questions:\nâ€¢\nQ1:\nHow well can CLARE learn new tasks, and is the performance on previous tasks affected?\nâ€¢\nQ2:\nCan our autonomous dynamic expansion strategy reuse relevant skills from previous tasks?\nâ€¢\nQ3:\nWhich layers are best suited for expansion?\nFigure 3:\nArchitecture of our pretrained diffusion transformerÂ (DiT) base policy. We investigate two variants of observation encoding and generative modeling: DiT-EncDec employs a self-attention transformer encoder and a denoising diffusion objective, while DiT-Dec performs a linear projection of the concatenated input tokens and uses a flow matching objective. The potential locations for inserting CLARE adapters are shown as dashed blocks. Our experiments indicate that adding adapters in the encoder module yields the best performance.\nV-A\nExperimental Setup\nV-A\n1\nTasks\nWe conduct our experiments using the LIBERO benchmark\n[\nlibero\n]\n, which is designed specifically for continual learning.\nHereby, a Franka robotic manipulator with a parallel yaw gripper needs to perform tasks in a kitchen environment, and 50 human expert demonstrations are available per task.\nWe pre-train the policy on 90 short-horizon tasks from LIBERO-90 and evaluate continual learning performance on 10 sequentially arriving long-horizon tasks from LIBERO-10, which require the robot to understand language instructions and execute different types of motion, such as pick-and-place, opening a drawer, or turning a knob.\nV-A\n2\nPolicy\nThe observations contain RGB images from a wrist-mounted and a third-person camera, the robotâ€™s end-effector pose and gripper state, and a language command.\nThe policy generates action chunks of length\nH\n=\n16\nH=16\nand applies\nh\n=\n8\nh=8\nactions to the robot at a control frequency of\n20\nâ€‹\nHz\n20\\,$\\mathrm{Hz}$\nbefore replanning.\nWe adopt a diffusion transformerÂ (DiT)\n[\ndit_policy\n]\narchitecture with adaptive layer normalizationÂ (adaLN)\n[\ndit\n]\nconditioning for our pre-trained base policy, as visualized in\nFigure\n3\n.\nWe leverage pre-trained DINOv2\n[\ndinov2\n]\nand CLIP\n[\nclip\n]\nmodels as vision and text encoders, respectively, and keep their parameters frozen during continual learning.\nThe visual and language features, as well as the proprioceptive state, are first projected into tokens of the same dimension through linear layers before they are fed into the transformer backbone.\nWe consider two different DiT variants, which are illustrated in\nFigure\n3\n.\nDiT-EncDec has an encoder-decoder backbone\n[\ndit_policy\n]\n, and adapters can be added to all transformer layers in the encoder and the decoder.\nDiT-Dec uses a decoder-only backbone\n[\ndit_policy\n]\n, and adapters can be added to the linear projection layer of the encoder and the transformer layers in the decoder.\nTo evaluate CLARE for different generative modeling techniques, we train DiT-Dec with the flow matching lossÂ (\n1\n) and DiT-EncDec with a denoising diffusion objective\n[\nho2020denoising\n,\nchi2023diffusion\n]\n.\nOur training hyperparameters are provided in\nTable\nI\n.\nBoth VLA variants have approximatelyÂ 200M base parameters.\nV-A\n3\nMetrics\nWe use three metrics to measure performance\n[\ndiaz2018don\n,\nlibero\n]\n: Area under the success rate curve (AUC), forward transferÂ (FWT), and negative backward transfer (NBT).\nDenoting the success rate on task\nn\nn\nafter learning the first\nm\nâ‰¥\nn\nm\\geq n\ntasks as\nr\nn\n|\nm\nr_{n|m}\n, the metrics are defined as\nAUC\n\\displaystyle\\mathrm{AUC}\n=\n1\nN\nâ€‹\nâˆ‘\nn\n=\n1\nN\n(\n1\nN\nâˆ’\nn\n+\n1\nâ€‹\nâˆ‘\nm\n=\nn\nN\nr\nn\n|\nm\n)\n,\n\\displaystyle=\\frac{1}{N}\\sum_{n=1}^{N}\\left(\\frac{1}{N-n+1}\\sum_{m=n}^{N}r_{n|m}\\right),\nFWT\n\\displaystyle\\mathrm{FWT}\n=\n1\nN\nâ€‹\nâˆ‘\nn\n=\n1\nN\nr\nn\n|\nn\n,\n\\displaystyle=\\frac{1}{N}\\sum_{n=1}^{N}{r_{n|n}},\nNBT\n\\displaystyle\\mathrm{NBT}\n=\n1\nN\nâˆ’\n1\nâ€‹\nâˆ‘\nn\n=\n1\nN\nâˆ’\n1\n(\n1\nN\nâˆ’\nn\nâ€‹\nâˆ‘\nm\n=\nn\n+\n1\nN\n(\nr\nn\n|\nn\nâˆ’\nr\nn\n|\nm\n)\n)\n.\n\\displaystyle=\\frac{1}{N-1}\\sum_{n=1}^{N-1}\\left(\\frac{1}{N-n}\\sum_{m=n+1}^{N}\\left(r_{n|n}-r_{n|m}\\right)\\right).\nIntuitively, AUC measures overall performance on new and old tasks, FWT quantifies the ability to learn new tasks, and NBT measures the degree of forgetting (lower being better).\nAll numerical results are given in percentage points.\nV-A\n4\nEvaluation Protocol\nAfter each continual learning stage\nn\nn\n, we evaluate the policy on all previously learned tasks\nğ’¯\n1\n,\nâ€¦\n,\nğ’¯\nn\n\\mathcal{T}_{1},\\dots,\\mathcal{T}_{n}\nusingÂ 100 rollouts per task.\nThe evaluations are conducted over 50 distinct initial configurations of relevant objects, with each configuration used twice.\nWe average all results across three random seeds.\nV-A\n5\nBaselines\nWe compare our method with five baselines for continual learning without oracle task identifiers:\nâ€¢\nSequential Fully Fine-tuning (SeqFFT)\n[\nlibero\n,\nllm_rlhf\n]\ntreats all model parameters as trainable and sequentially fine-tunes the whole policy for each new task.\nâ€¢\nSequential Low-Rank Adaptation (SeqLoRA)\n[\nlora\n]\nadds task-specific LoRA adapters to selected layers.\nAfter training the adapters on the new data, they are merged back into the base model weights.\nWe add adapters to all linear layers within the attention, feedforward, and adaLN modules, and, for DiT-Dec, additionally into the linear projection layer.\nâ€¢\nPackNet\n[\npacknet\n]\nfreezes the most important 25% of model weights, and the remaining free weights are used to learn the next task. This process is repeated iteratively, with each new task having fewer free weights available.\nâ€¢\nExperience ReplayÂ (ER)\n[\nexperience_replay\n]\nis a popular method that stores previous data. For each new task, the model is trained on a mix of 50% previous and 50% new data.\nâ€¢\nLOTUS\n[\nlotus\n]\nis a hierarchical method that constructs a skill library, which is updated and expanded when learning new tasks. A learned meta-policy flexibly composes these skills during deployment.\nNote that while CLARE is designed not to rely on previous data, as it may be unavailable, we allow ER and LOTUS to access previous data in our experiments to match the setup in prior works.\nThis privileged information gives these two baselines an inherent advantage.\nV-B\nResults\nBackbone\nExpand. layers\nAUC\nâ†‘\n\\uparrow\nFWT\nâ†‘\n\\uparrow\nNBT\nâ†“\n\\downarrow\nDiT-EncDec\nEncoder\n65.38\nÂ±\n2.68\n{}^{\\pm\\text{2.68}}\n66.53\nÂ±\n2.18\n{}^{\\pm\\text{2.18}}\n1.70\nÂ±\n1.20\n{}^{\\pm\\text{1.20}}\nDecoder\n28.99\nÂ±\n2.20\n{}^{\\pm\\text{2.20}}\n30.87\nÂ±\n4.25\n{}^{\\pm\\text{4.25}}\n2.95\nÂ±\n3.41\n{}^{\\pm\\text{3.41}}\nEnc. & Dec.\n66.60\nÂ±\n0.29\n{}^{\\pm\\text{0.29}}\n65.77\nÂ±\n0.41\n{}^{\\pm\\text{0.41}}\n1.50\nÂ±\n0.65\n{}^{\\pm\\text{0.65}}\nDiT-Dec\nLin. projection\n75.11\nÂ±\n1.31\n{}^{\\pm\\text{1.31}}\n75.03\nÂ±\n1.42\n{}^{\\pm\\text{1.42}}\n1.85\nÂ±\n0.38\n{}^{\\pm\\text{0.38}}\nDecoder\n41.75\nÂ±\n2.42\n{}^{\\pm\\text{2.42}}\n45.47\nÂ±\n3.77\n{}^{\\pm\\text{3.77}}\n7.02\nÂ±\n1.65\n{}^{\\pm\\text{1.65}}\nTABLE II:\nAblation study for the choice of expandable layers. Adding adapters to the observation encoder module yields the best results.\nWe first examine a key design choice ofÂ CLARE; the set of layers to expand\nâ„°\n\\mathcal{E}\n.\nTo this end, we set\nÎ³\n=\n0\n\\gamma=0\n, such that a new adapter is added to each expandable layer to learn a new task.\nThe results provided in\nTable\nII\ndemonstrate that adding adapters only to the encoder part of the model vastly outperforms expanding the decoder, achievingÂ 30 toÂ 40% higher absolute AUC and FWT for both backbones.\nNotably, expanding all transformer layers for DiT-EncDec does not improve performance compared to expanding only the encoder.\nThese results demonstrate that the policyâ€™s encoder module, whether implemented as a transformer or a linear projection, is well-suited to store task-specific knowledge in continual learning with VLAs.\nHence, we adopt this approach in all subsequent experiments.\nBackbone\nMethod\nAUC\nâ†‘\n\\uparrow\nFWT\nâ†‘\n\\uparrow\nNBT\nâ†“\n\\downarrow\nDiT-EncDec\nSeqFFT\n21.00\nÂ±\n0.49\n{}^{\\pm\\text{0.49}}\n71.13\nÂ±\n1.97\n{}^{\\pm\\text{1.97}}\n70.33\nÂ±\n2.21\n{}^{\\pm\\text{2.21}}\nSeqLoRA\n16.26\nÂ±\n0.72\n{}^{\\pm\\text{0.72}}\n55.00\nÂ±\n0.54\n{}^{\\pm\\text{0.54}}\n53.08\nÂ±\n0.21\n{}^{\\pm\\text{0.21}}\nPackNet\n[\npacknet\n]\n20.91\nÂ±\n0.39\n{}^{\\pm\\text{0.39}}\n73.77\nÂ±\n1.31\n{}^{\\pm\\text{1.31}}\n73.74\nÂ±\n1.42\n{}^{\\pm\\text{1.42}}\nER\n[\nexperience_replay\n]\n55.87\nÂ±\n1.47\n{}^{\\pm\\text{1.47}}\n67.67\nÂ±\n1.65\n{}^{\\pm\\text{1.65}}\n15.79\nÂ±\n0.48\n{}^{\\pm\\text{0.48}}\nCLARE (ours)\n66.71\nÂ±\n1.29\n{}^{\\pm\\text{1.29}}\n66.07\nÂ±\n1.32\n{}^{\\pm\\text{1.32}}\n-0.80\nÂ±\n0.76\n{}^{\\pm\\text{0.76}}\nDiT-Dec\nSeqFFT\n22.37\nÂ±\n0.27\n{}^{\\pm\\text{0.27}}\n76.13\nÂ±\n0.97\n{}^{\\pm\\text{0.97}}\n74.70\nÂ±\n1.05\n{}^{\\pm\\text{1.05}}\nSeqLoRA\n21.37\nÂ±\n1.03\n{}^{\\pm\\text{1.03}}\n73.10\nÂ±\n1.77\n{}^{\\pm\\text{1.77}}\n71.64\nÂ±\n1.60\n{}^{\\pm\\text{1.60}}\nPackNet\n[\npacknet\n]\n4.84\nÂ±\n0.24\n{}^{\\pm\\text{0.24}}\n37.20\nÂ±\n1.04\n{}^{\\pm\\text{1.04}}\n41.34\nÂ±\n1.16\n{}^{\\pm\\text{1.16}}\nER\n[\nexperience_replay\n]\n60.54\nÂ±\n0.21\n{}^{\\pm\\text{0.21}}\n76.60\nÂ±\n0.94\n{}^{\\pm\\text{0.94}}\n22.74\nÂ±\n1.82\n{}^{\\pm\\text{1.82}}\nCLARE (ours)\n75.11\nÂ±\n1.31\n{}^{\\pm\\text{1.31}}\n75.03\nÂ±\n1.42\n{}^{\\pm\\text{1.42}}\n1.85\nÂ±\n0.38\n{}^{\\pm\\text{0.38}}\nLOTUS\n[\nlotus\n]\n52.93\nÂ±\n1.57\n{}^{\\pm\\text{1.57}}\n58.12\nÂ±\n0.24\n{}^{\\pm\\text{0.24}}\n-7.16\nÂ±\n2.99\n{}^{\\pm\\text{2.99}}\nTABLE III:\nBaseline comparison. CLARE achieves the highest overall performance, as measured by AUC, and demonstrates strong capabilities to acquire new skills without forgetting.\nFigure 4:\nSuccess rate curves of CLARE and five baselines on the LIBERO-Long benchmark. The solid lines represent the average success rates across three random seeds, and the shaded regions indicate the standard deviations. The results demonstrate that our method achieves a higher overall success rate and more effectively mitigates catastrophic forgetting during continual learning compared to the baselines, despite ER and LOTUS using previous data.\nA summary of our baseline comparison is provided in\nTable\nIII\n.\nOur method achieves the highest overall performance, as measured by AUC, with both backbones, outperforming the best baseline, ER, by approximately 11% and 15% in absolute terms, respectively.\nCompared to SeqFFT and ER, which fine-tune the full model, CLARE achieves comparable FWT, indicating that it can store new task-specific knowledge in a much smaller number of learnable parameters.\nMoreover, our method achieves a very low NBT close to zero, demonstrating that it can avoid forgetting without relying on exemplar data or oracle task identifiers.\nWhile this requires adding new parameters to the model, their relative increase with respect to the base policy is only 1.7% and 2.3%, respectively, on average per task.\nTo better understand the continual learning behavior, we provide the task success rates of DiT-Dec for all stages in\nFigure\n4\n.\nThese results further demonstrate that CLARE does not exhibit catastrophic forgetting in contrast to most baselines, while also achieving a higher overall success rate across most tasks.\nInterestingly, for LOTUS, performance on some tasks improves during later stages, which is also indicated by the negative NBT value reported in\nTable\nIII\n.\nWe attribute this behavior to the iterative use of ER.\nFigure 5:\nAblation study for the dynamic expansion threshold\nÎ³\n\\gamma\n. Increasing\nÎ³\n\\gamma\nsignificantly reduces the number of adapters added to the model but slightly reduces the capability to learn new tasks, as shown by the small decrease in AUC and FWT.\nIn contrast, NBT remains at around zero, indicating that the model does not exhibit catastrophic forgetting.\nWe also evaluate the impact of the expansion threshold\nÎ³\n\\gamma\n, using DiT-EncDec with expandable encoder layers, and provide the results in\nFigure\n5\n.\nIncreasing\nÎ³\n\\gamma\nfrom 0 to 20 reduces the number of new adapters added to the model from 60 to 16, i.e., almost by a factor of 4.\nAt the same time, we observe a moderate decrease in AUC (from 65% to 57%) and FWT (from 67% to 57%).\nIntuitively, compressing new knowledge into fewer model parameters reduces the robotâ€™s ability to learn novel tasks.\nHowever, we also note that AUC remains higher than for ER, the best-performing baseline, and NBT stays close to zero, indicating that the policy does not exhibit forgetting even when only a small number of adapters is added per task.\nComparing the results in\nTable\nII\nand\nFigure\n5\n, we find that the choice of expandable layers has a significantly larger impact than the expansion threshold value.\nIn summary, adapters should be added to the encoder part, and\nÎ³\n\\gamma\ncan be chosen according to the importance of high task performance compared to low memory requirements.\nVI\nConclusions\nOur proposed framework, CLARE, enables pre-trained VLAs to continually learn new tasks without catastrophic forgetting, requiring neither stored exemplars nor task identifiers.\nBy combining lightweight adapters, a feature-similarity-driven expansion strategy, and an autoencoder-based routing module, CLARE increases model capacity only when needed while maintaining previously learned representations.\nOur experiments demonstrate that our method is parameter-efficient and able to achieve and retain high task performance, significantly outperforming even strong baselines that have access to previous data.\nThe modular design of CLARE is also compatible with emerging large-scale VLAs\n[\npi0_5\n,\nflower\n]\n, and we consider evaluation on such models and on hardware to be natural steps for further work.\nAcknowledgements\nRalf RÃ¶mer gratefully acknowledges the support of the research group ConVeY funded by the German\nResearch Foundation under grant GRK 2428.\nThis work has been partially supported by the German Federal Ministry of Research, Technology and Space (BMFTR) under the Robotics Institute Germany (RIG).\nReferences",
    "preview_text": "To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.\n\nCLARE:\nC\nontinual\nL\nearning for Vision-Language-Action Models\nvia Autonomous\nA\ndapter\nR\nouting and\nE\nxpansion\nRalf RÃ¶mer\nâˆ—,1\nYi Zhang\nâˆ—,1\nAngela P. Schoellig\n1,2\nâˆ—\nEqual contribution.\n1\nTechnical University of Munich, Germany; TUM School of Computation, Information and Technology, Learning Systems and Robotics Lab; Munich Institute of Robotics and Machine Intelligence (MIRMI).\n2\nRobotics Institute Germany.Corresponding email:\n{ralf.roemer@tum.de}\nAbstract\nTo teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data.\nHowever, since this recipe updates exis",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "VLA",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "CLAREæå‡ºäº†ä¸€ç§åŸºäºè‡ªä¸»é€‚é…å™¨è·¯ç”±å’Œæ‰©å±•çš„æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œä»¥è§£å†³æœºå™¨äººé•¿æœŸæ“ä½œä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šCLARE: é¢å‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æŒç»­å­¦ä¹ æ¡†æ¶\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nåœ¨æœºå™¨äººé¢†åŸŸï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šæ•°æ®å¾®è°ƒé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å·²æˆä¸ºæ•™æˆå¤æ‚æ“ä½œä»»åŠ¡çš„å¸¸è§åšæ³•ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¼šè¦†ç›–ç°æœ‰è¡¨å¾ï¼Œä¸é€‚åˆæœºå™¨äººåœ¨ç°å®ä¸–ç•Œä¸­çš„é•¿æœŸè¿è¡Œï¼Œå› ä¸ºæœºå™¨äººå¿…é¡»æŒç»­é€‚åº”æ–°ä»»åŠ¡å’Œç¯å¢ƒï¼ŒåŒæ—¶ä¿ç•™å·²ä¹ å¾—çš„çŸ¥è¯†ã€‚ç°æœ‰çš„æœºå™¨äººæŒç»­å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å­˜å‚¨å†å²æ•°æ®ï¼ˆæ ·æœ¬ï¼‰ï¼Œéš¾ä»¥å¤„ç†é•¿ä»»åŠ¡åºåˆ—ï¼Œæˆ–ä¾èµ–ä»»åŠ¡æ ‡è¯†ç¬¦è¿›è¡Œéƒ¨ç½²ã€‚ä¸ºè§£å†³è¿™äº›å±€é™ï¼Œæœ¬æ–‡æå‡ºäº†**CLARE**ï¼Œä¸€ä¸ªé€šç”¨çš„ã€å‚æ•°é«˜æ•ˆçš„ã€æ— éœ€æ ·æœ¬çš„VLAæŒç»­å­¦ä¹ æ¡†æ¶ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\nCLAREçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶**è½»é‡çº§æ¨¡å—åŒ–é€‚é…å™¨**ä¸**è‡ªä¸»è·¯ç”±æ‰©å±•æœºåˆ¶**ã€‚\n*   **æ¨¡å—åŒ–é€‚é…å™¨**ï¼šåœ¨é€‰å®šçš„å‰é¦ˆç½‘ç»œå±‚ä¸­æ³¨å…¥è½»é‡çº§çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„é€‚é…å™¨ä½œä¸ºæ—æ”¯ã€‚å­¦ä¹ æ–°ä»»åŠ¡æ—¶ï¼Œä»…è®­ç»ƒæ–°æ·»åŠ çš„é€‚é…å™¨ï¼Œå†»ç»“åŸæœ‰æ¨¡å‹å‚æ•°ï¼Œä»è€Œé¿å…ç¾éš¾æ€§é—å¿˜ã€‚\n*   **è‡ªä¸»åŠ¨æ€æ‰©å±•**ï¼šåŸºäºå±‚é—´ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œä»…å½“æ–°ä»»åŠ¡çš„ç‰¹å¾ä¸æ‰€æœ‰å·²å­¦ä»»åŠ¡ç‰¹å¾å·®å¼‚æ˜¾è‘—ï¼ˆé€šè¿‡Zåˆ†æ•°é˜ˆå€¼Î³åˆ¤æ–­ï¼‰æ—¶ï¼Œæ‰åœ¨å¯¹åº”å±‚æ‰©å±•æ–°çš„é€‚é…å™¨ï¼Œå®ç°**äºšçº¿æ€§**çš„å‚æ•°å¢é•¿ï¼ˆå®éªŒä¸­æ¯ä»»åŠ¡çº¦å¢åŠ 2%å‚æ•°ï¼‰ã€‚\n*   **è‡ªåŠ¨ç¼–ç å™¨è·¯ç”±**ï¼šæ¯ä¸ªå¯æ‰©å±•å±‚é…å¤‡ä¸€ç»„è‡ªåŠ¨ç¼–ç å™¨åˆ¤åˆ«å™¨ã€‚éƒ¨ç½²æ—¶ï¼Œé€šè¿‡è®¡ç®—è¾“å…¥ç‰¹å¾ä¸å„åˆ¤åˆ«å™¨çš„é‡æ„è¯¯å·®ï¼ŒåŠ¨æ€æ¿€æ´»**æœ€ç›¸å…³**çš„é€‚é…å™¨ï¼Œæ— éœ€ä»»åŠ¡æ ‡ç­¾å³å¯å®ç°è‡ªä¸»ã€ä»»åŠ¡æ— å…³çš„æ¨ç†ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\nåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼š\n*   **é«˜æ€§èƒ½ä¸æŠ—é—å¿˜**ï¼šCLAREåœ¨æ•´ä½“æ€§èƒ½ï¼ˆAUCï¼‰ä¸Šæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬éœ€è¦å†å²æ•°æ®çš„ç»éªŒå›æ”¾æ–¹æ³•ã€‚å…¶è´Ÿå‘åå‘è¿ç§»å€¼æ¥è¿‘é›¶ï¼Œè¡¨æ˜å‡ ä¹**æ²¡æœ‰ç¾éš¾æ€§é—å¿˜**ã€‚\n*   **å‚æ•°é«˜æ•ˆ**ï¼šåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ¨¡å‹å‚æ•°æ€»é‡ä»…éšä»»åŠ¡æ•°äºšçº¿æ€§å¢é•¿ã€‚\n*   **å…³é”®è®¾è®¡éªŒè¯**ï¼šæ¶ˆèå®éªŒè¯æ˜ï¼Œå°†é€‚é…å™¨æ·»åŠ åˆ°æ¨¡å‹çš„**ç¼–ç å™¨éƒ¨åˆ†**ï¼ˆè€Œéè§£ç å™¨ï¼‰èƒ½è·å¾—æœ€ä½³æ€§èƒ½ï¼›åŠ¨æ€æ‰©å±•é˜ˆå€¼Î³å…è®¸åœ¨æ€§èƒ½ä¸å†…å­˜å ç”¨ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\nCLAREä¸ºæœºå™¨äººåœ¨å¼€æ”¾ä¸–ç•Œä¸­çš„é•¿æœŸéƒ¨ç½²æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚å®ƒé¦–æ¬¡å®ç°äº†VLAæ¨¡å‹åœ¨**æ— éœ€å­˜å‚¨å†å²æ•°æ®ã€æ— éœ€ä»»åŠ¡æ ‡è¯†ç¬¦**æ¡ä»¶ä¸‹çš„é«˜æ•ˆæŒç»­å­¦ä¹ ã€‚å…¶æ¨¡å—åŒ–ã€å‚æ•°é«˜æ•ˆçš„è®¾è®¡æ˜“äºæ‰©å±•åˆ°å¤§è§„æ¨¡VLAæ¨¡å‹ï¼Œå¯¹æ¨åŠ¨ç»ˆèº«å­¦ä¹ æœºå™¨äººçš„å®é™…åº”ç”¨å…·æœ‰é‡è¦ä»·å€¼ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šCLARE\n\n### **ç ”ç©¶èƒŒæ™¯ä¸æ ¸å¿ƒé—®é¢˜**\nè¿™ç¯‡è®ºæ–‡æ—¨åœ¨è§£å†³**æœºå™¨äººè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æŒç»­å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜**ã€‚å…·ä½“è€Œè¨€ï¼š\n- **é—®é¢˜**ï¼šç°æœ‰çš„é¢„è®­ç»ƒVLAæ¨¡å‹é€šå¸¸é€šè¿‡åœ¨æ–°ä»»åŠ¡æ•°æ®ä¸Šå¾®è°ƒæ¥é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œçš„é•¿æœŸéƒ¨ç½²ä¸­ï¼Œæœºå™¨äººéœ€è¦**æŒç»­å­¦ä¹ ä¸€ç³»åˆ—æ–°ä»»åŠ¡å’Œç¯å¢ƒ**ï¼ŒåŒæ—¶**ä¿ç•™å·²å­¦æŠ€èƒ½**ã€‚ç®€å•çš„é¡ºåºå¾®è°ƒä¼šè¦†ç›–åŸæœ‰å‚æ•°ï¼Œå¯¼è‡´å¯¹æ—§ä»»åŠ¡çš„æ€§èƒ½ä¸¥é‡é€€åŒ–ï¼ˆå³ç¾éš¾æ€§é—å¿˜ï¼‰ã€‚\n- **ç°æœ‰æ–¹æ³•çš„å±€é™**ï¼š\n    - **ç»éªŒå›æ”¾**ï¼šéœ€è¦å­˜å‚¨æ—§æ•°æ®ï¼Œå­˜åœ¨å­˜å‚¨ã€éšç§å’Œè®¡ç®—å¼€é”€é—®é¢˜ã€‚\n    - **æ­£åˆ™åŒ–æ–¹æ³•**ï¼šéš¾ä»¥å¤„ç†é•¿ä»»åŠ¡åºåˆ—ï¼Œå—é™äºå›ºå®šå‚æ•°å®¹é‡ã€‚\n    - **æ¨¡å—åŒ–æ¶æ„**ï¼šé€šå¸¸éœ€è¦ä»»åŠ¡æ ‡è¯†ç¬¦è¿›è¡Œéƒ¨ç½²ï¼Œè¿™åœ¨è‡ªä¸»å¼€æ”¾ä¸–ç•Œä¸­ä¸å¯ç”¨ã€‚\n\n### **æ ¸å¿ƒåˆ›æ–°ç‚¹**\nCLARE æå‡ºäº†ä¸€ä¸ª**æ— éœ€ç¤ºä¾‹ã€æ— éœ€ä»»åŠ¡æ ‡è¯†ç¬¦ã€å‚æ•°é«˜æ•ˆçš„æŒç»­å­¦ä¹ æ¡†æ¶**ï¼Œå…¶åˆ›æ–°ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹ä¸‰ä¸ªç´§å¯†å…³è”çš„æœºåˆ¶ä¸Šï¼š\n\n1.  **è½»é‡çº§æ¨¡å—åŒ–é€‚é…å™¨**\n    - **æŠ€æœ¯**ï¼šåœ¨é¢„è®­ç»ƒVLAæ¨¡å‹é€‰å®šçš„å‰é¦ˆç½‘ç»œå±‚ä¸­ï¼Œä»¥**å¹¶è¡Œä¾§æ”¯**çš„å½¢å¼æ³¨å…¥è½»é‡çº§çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„é€‚é…å™¨ã€‚\n    - **ä»·å€¼**ï¼šå†»ç»“åŸæœ‰æ¨¡å‹å‚æ•°ï¼Œä»…è®­ç»ƒæ–°æ·»åŠ çš„é€‚é…å™¨æ¥å­¦ä¹ æ–°ä»»åŠ¡çŸ¥è¯†ï¼Œä»è€Œ**ä»æ ¹æœ¬ä¸Šé¿å…äº†è¦†ç›–æ—§è¡¨å¾**ï¼Œå®ç°äº†ç¨³å®šæ€§ã€‚\n\n2.  **åŸºäºè‡ªç¼–ç å™¨çš„è‡ªä¸»è·¯ç”±æœºåˆ¶**\n    - **æŠ€æœ¯**ï¼šä¸ºæ¯ä¸ªå¯æ‰©å±•å±‚é…å¤‡ä¸€ç»„**è‡ªç¼–ç å™¨åˆ¤åˆ«å™¨**ã€‚æ¯ä¸ªåˆ¤åˆ«å™¨ä¸ä¸€ä¸ªé€‚é…å™¨å…³è”ï¼Œå¹¶é€šè¿‡è®¡ç®—è¾“å…¥ç‰¹å¾çš„**é‡æ„è¯¯å·®**æ¥è¡¡é‡ä»»åŠ¡ç›¸å…³æ€§ã€‚\n    - **ä»·å€¼**ï¼šåœ¨æ¨ç†æ—¶ï¼Œ**åŠ¨æ€é€‰æ‹©é‡æ„è¯¯å·®æœ€å°çš„åˆ¤åˆ«å™¨æ‰€å…³è”çš„é€‚é…å™¨**è¿›è¡Œæ¿€æ´»ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿ**è‡ªä¸»ã€æ— éœ€ä»»åŠ¡æ ‡ç­¾åœ°**é€‰æ‹©æœ€ç›¸å…³çš„æŠ€èƒ½æ¨¡å—ï¼Œå®ç°äº†å®Œå…¨è‡ªä¸»çš„éƒ¨ç½²ã€‚\n\n3.  **åŸºäºç‰¹å¾ç›¸ä¼¼æ€§çš„åŠ¨æ€æ‰©å±•ç­–ç•¥**\n    - **æŠ€æœ¯**ï¼šå¼•å…¥ä¸€ä¸ª**å¯æ‰©å±•æ€§é˜ˆå€¼Î³**ã€‚å­¦ä¹ æ–°ä»»åŠ¡æ—¶ï¼Œé€šè¿‡è®¡ç®—æ–°ä»»åŠ¡ç‰¹å¾ä¸æ‰€æœ‰ç°æœ‰åˆ¤åˆ«å™¨çš„å½’ä¸€åŒ–é‡æ„è¯¯å·®ï¼ˆz-scoreï¼‰ï¼Œæ¥åˆ¤æ–­ç‰¹å¾æ˜¯å¦â€œæ–°é¢–â€ã€‚\n    - **å†³ç­–é€»è¾‘**ï¼š\n        - è‹¥æŸå±‚æ‰€æœ‰ç°æœ‰åˆ¤åˆ«å™¨çš„z-scoreå‡è¶…è¿‡Î³ï¼Œåˆ™è®¤ä¸ºæ–°ä»»åŠ¡ç‰¹å¾ä¸è¯¥å±‚æ‰€æœ‰æ—§ä»»åŠ¡éƒ½æ˜¾è‘—ä¸åŒï¼Œ**æ‰©å±•ä¸€ä¸ªæ–°é€‚é…å™¨**ã€‚\n        - å¦åˆ™ï¼Œè®¤ä¸ºæ–°ä»»åŠ¡ä¸æŸä¸ªæ—§ä»»åŠ¡ç›¸ä¼¼ï¼Œ**ä»…æ·»åŠ ä¸€ä¸ªè¾…åŠ©åˆ¤åˆ«å™¨**å¹¶å°†å…¶é“¾æ¥åˆ°æœ€ç›¸å…³çš„ç°æœ‰é€‚é…å™¨ä¸Šï¼Œå®ç°**æŠ€èƒ½å¤ç”¨**ã€‚\n    - **ä»·å€¼**ï¼šå®ç°äº†**æŒ‰éœ€ã€äºšçº¿æ€§**çš„å‚æ•°å¢é•¿ï¼ˆè®ºæ–‡ä¸­çº¦2%/ä»»åŠ¡ï¼‰ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æå¤§æå‡äº†å†…å­˜æ•ˆç‡ã€‚å®ƒå¹³è¡¡äº†**å¯å¡‘æ€§**ï¼ˆå­¦ä¹ æ–°ä»»åŠ¡ï¼‰å’Œ**ç¨³å®šæ€§**ï¼ˆä¿ç•™æ—§ä»»åŠ¡ï¼‰ã€‚\n\n### **è§£å†³æ–¹æ¡ˆæ€»ç»“**\nCLARE é€šè¿‡ **â€œå†»ç»“ä¸»å¹² + åŠ¨æ€æ‰©å±•é€‚é…å™¨ + è‡ªä¸»è·¯ç”±â€** ä¸‰ä½ä¸€ä½“çš„æ–¹å¼ï¼Œç³»ç»Ÿæ€§åœ°è§£å†³äº†VLAæŒç»­å­¦ä¹ çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼š\n- **é˜²é—å¿˜**ï¼šå†»ç»“é¢„è®­ç»ƒå‚æ•°ï¼Œæ–°çŸ¥è¯†å­˜å‚¨åœ¨ç‹¬ç«‹çš„é€‚é…å™¨ä¸­ã€‚\n- **å‚æ•°é«˜æ•ˆ**ï¼šåŠ¨æ€æ‰©å±•é¿å…äº†ä¸ºæ¯ä¸ªä»»åŠ¡ç›²ç›®æ·»åŠ æ¨¡å—ã€‚\n- **éƒ¨ç½²è‡ªä¸»**ï¼šåŸºäºç‰¹å¾çš„è·¯ç”±æœºåˆ¶æ— éœ€äººå·¥æä¾›ä»»åŠ¡IDã€‚\n- **çŸ¥è¯†å…±äº«**ï¼šé€šè¿‡åŠ¨æ€æ‰©å±•å†³ç­–ï¼Œç›¸ä¼¼ä»»åŠ¡å¯ä»¥å…±äº«åŒä¸€ä¸ªé€‚é…å™¨ï¼Œä¿ƒè¿›äº†æ­£å‘è¿ç§»ã€‚\n\n### **å®é™…ä»·å€¼**\n- **æœºå™¨äººé•¿æœŸè‡ªæ²»**ï¼šä¸ºåœ¨å®¶åº­ã€åŒ»é™¢ç­‰å¼€æ”¾ç¯å¢ƒä¸­é•¿æœŸè¿è¡Œçš„æœºå™¨äººæä¾›äº†å¯æŒç»­å­¦ä¹ æ–°æŠ€èƒ½è€Œä¸é—å¿˜æ—§æŠ€èƒ½çš„å¯è¡Œæ–¹æ¡ˆã€‚\n- **å®ç”¨æ€§å¼º**ï¼šæ— éœ€å­˜å‚¨æ—§æ•°æ®ï¼ˆè§£å†³éšç§å’Œå­˜å‚¨é—®é¢˜ï¼‰ï¼Œæ— éœ€ä»»åŠ¡æ ‡è¯†ç¬¦ï¼ˆé€‚åº”çœŸå®ä¸–ç•Œï¼‰ï¼Œå‚æ•°å¢é•¿å°ï¼ˆæ˜“äºéƒ¨ç½²ï¼‰ã€‚\n- **æ¶æ„é€šç”¨**ï¼šæ–¹æ³•ä¸ä¾èµ–äºç‰¹å®šVLAæ¶æ„ï¼Œå¯è¿ç§»åˆ°æœªæ¥æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ä¸Šã€‚\n\n**ç®€è€Œè¨€ä¹‹ï¼ŒCLARE çš„æ ¸å¿ƒæ˜¯è®©VLAæ¨¡å‹åƒâ€œç§¯æœ¨åº“â€ä¸€æ ·å·¥ä½œï¼šé‡åˆ°æ–°ä»»åŠ¡æ—¶ï¼Œæ™ºèƒ½åˆ¤æ–­æ˜¯å¢åŠ ä¸€å—æ–°ç§¯æœ¨ï¼ˆæ–°é€‚é…å™¨ï¼‰è¿˜æ˜¯å¤ç”¨æ—§ç§¯æœ¨ï¼ˆç°æœ‰é€‚é…å™¨ï¼‰ï¼›æ‰§è¡Œä»»åŠ¡æ—¶ï¼Œè‡ªåŠ¨æŒ‘é€‰æœ€åˆé€‚çš„ç§¯æœ¨ç»„åˆï¼ˆè‡ªä¸»è·¯ç”±ï¼‰ï¼Œä»è€Œå®ç°äº†é«˜æ•ˆã€å¥å£®ä¸”è‡ªä¸»çš„ç»ˆèº«å­¦ä¹ ã€‚**",
            "timestamp": "2026-01-20T17:57:36.156344",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## CLAREè®ºæ–‡æŠ€æœ¯è„‰ç»œä¸å‘å±•æ–¹å‘æ¢³ç†\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„åˆ†æï¼ŒCLAREï¼ˆContinual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansionï¼‰æ˜¯**è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æŒç»­å­¦ä¹ **é¢†åŸŸçš„ä¸€é¡¹æœ€æ–°å·¥ä½œã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯è§£å†³æœºå™¨äººç­‰æ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œä¸­**é•¿æœŸã€è¿ç»­å­¦ä¹ æ–°ä»»åŠ¡è€Œä¸é—å¿˜æ—§æŠ€èƒ½**çš„éš¾é¢˜ã€‚ä»¥ä¸‹å°†æ¢³ç†è¯¥æ–¹å‘çš„æ•´ä½“å‘å±•è„‰ç»œï¼Œå¹¶ç€é‡åˆ†æå‡ ä¸ªå…³é”®çš„å‘å±•æ–¹å‘åŠå…¶æ”¹è¿›ã€‚\n\n### å‘å±•è„‰ç»œæ€»è§ˆ\n\nè¯¥é¢†åŸŸçš„å‘å±•å¯ä»¥æ¦‚æ‹¬ä¸ºä»**ç®€å•çš„å¾®è°ƒ**åˆ°**å¤æ‚çš„ã€æ¨¡å—åŒ–çš„ã€æ— éœ€èŒƒä¾‹çš„æŒç»­å­¦ä¹ æ¶æ„**çš„æ¼”è¿›ã€‚æ¯ä¸€æ­¥çš„æ”¹è¿›éƒ½æ—¨åœ¨æ›´å¥½åœ°å¹³è¡¡**ç¨³å®šæ€§**ï¼ˆä¿ç•™æ—§çŸ¥è¯†ï¼‰ä¸**å¯å¡‘æ€§**ï¼ˆå­¦ä¹ æ–°çŸ¥è¯†ï¼‰ä¹‹é—´çš„çŸ›ç›¾ã€‚\n\n### å…³é”®å‘å±•é˜¶æ®µä¸æ”¹è¿›\n\n#### 1. åŸºçº¿æ–¹æ³•ï¼šæœ´ç´ å¾®è°ƒä¸æ­£åˆ™åŒ–\n*   **ä»£è¡¨å·¥ä½œ**ï¼š**Sequential Fine-Tuning (SeqFFT)**ï¼Œ **Elastic Weight Consolidation (EWC)**ï¼Œ **PackNet**ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œç›´æ¥åœ¨æ–°ä»»åŠ¡æ•°æ®ä¸Šæ›´æ–°å…¨éƒ¨æˆ–éƒ¨åˆ†å‚æ•°ã€‚\n*   **æ”¹è¿›ä¸å±€é™**ï¼š\n    *   **SeqFFT**ï¼šæ–¹æ³•æœ€ç®€å•ï¼Œä½†ç›´æ¥æ›´æ–°å…±äº«å‚æ•°ä¼šå¯¼è‡´ä¸¥é‡çš„**ç¾éš¾æ€§é—å¿˜**ã€‚\n    *   **EWC/PackNet**ï¼šé€šè¿‡**è®¡ç®—å‚æ•°é‡è¦æ€§å¹¶æ–½åŠ çº¦æŸ**ï¼ˆEWCï¼‰æˆ–**å†»ç»“/é‡ç”¨éƒ¨åˆ†å‚æ•°**ï¼ˆPackNetï¼‰æ¥ä¿æŠ¤æ—§çŸ¥è¯†ã€‚\n    *   **ä¸»è¦å±€é™**ï¼šæ¨¡å‹å®¹é‡å›ºå®šï¼Œåœ¨é•¿ä»»åŠ¡åºåˆ—ä¸­ä¼šé­é‡**å®¹é‡ç“¶é¢ˆ**ï¼›éš¾ä»¥åœ¨å®Œå…¨å¼€æ”¾ã€æ— ä»»åŠ¡æ ‡è¯†çš„ç¯å¢ƒä¸­éƒ¨ç½²ã€‚\n\n#### 2. åŸºäºå›æ”¾çš„æ–¹æ³•\n*   **ä»£è¡¨å·¥ä½œ**ï¼š**Experience Replay (ER)**ï¼Œ **LOTUS**ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šå­˜å‚¨ä¸€éƒ¨åˆ†æ—§ä»»åŠ¡çš„æ•°æ®ï¼ˆèŒƒä¾‹ï¼‰ï¼Œåœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ä¸æ–°æ•°æ®æ··åˆè®­ç»ƒï¼Œä»¥â€œæé†’â€æ¨¡å‹æ—§çŸ¥è¯†ã€‚\n*   **æ”¹è¿›ä¸å±€é™**ï¼š\n    *   **ER**ï¼šæœ‰æ•ˆç¼“è§£é—å¿˜ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚\n    *   **LOTUS**ï¼šæ„å»ºäº†**åˆ†å±‚æŠ€èƒ½åº“**ï¼Œé€šè¿‡å…ƒç­–ç•¥ç»„åˆæŠ€èƒ½ï¼Œæ›´å…·ç»“æ„æ€§ã€‚\n    *   **ä¸»è¦å±€é™**ï¼š**ä¸¥é‡ä¾èµ–å­˜å‚¨æ—§æ•°æ®**ï¼Œè¿™åœ¨æœºå™¨äººé•¿æœŸéƒ¨ç½²ä¸­å¯èƒ½å› å­˜å‚¨ã€éšç§æˆ–æ•°æ®ä¸å¯ç”¨è€Œå˜å¾—ä¸åˆ‡å®é™…ã€‚**èŒƒä¾‹é€‰æ‹©**æœ¬èº«ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚\n\n#### 3. æ¨¡å—åŒ–ä¸å¯æ‰©å±•æ¶æ„\n*   **ä»£è¡¨å·¥ä½œ**ï¼š**Sparse Diffusion Policy (SDP)**ï¼Œ **TAIL**ï¼Œ **SEMOA**ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šä¸å†è¦†ç›–å…±äº«å‚æ•°ï¼Œè€Œæ˜¯ä¸º**æ–°ä»»åŠ¡åˆ†é…ç‹¬ç«‹çš„å‚æ•°æ¨¡å—**ï¼ˆå¦‚é€‚é…å™¨ã€ä¸“å®¶ç½‘ç»œï¼‰ã€‚å†»ç»“ä¸»å¹²ç½‘ç»œï¼Œä»…è®­ç»ƒæ–°å¢æ¨¡å—ã€‚\n*   **æ”¹è¿›**ï¼š\n    *   ä»æ ¹æœ¬ä¸Šé¿å…äº†å‚æ•°è¦†ç›–ï¼Œ**ç†è®ºä¸Šå¯ä»¥å®Œå…¨æ¶ˆé™¤ç¾éš¾æ€§é—å¿˜**ã€‚\n    *   å‚æ•°é«˜æ•ˆï¼Œé€šå¸¸ä½¿ç”¨åƒ**LoRA**è¿™æ ·çš„ä½ç§©é€‚é…æŠ€æœ¯ã€‚\n*   **ä¸»è¦å±€é™ï¼ˆCLAREè®ºæ–‡é‡ç‚¹é’ˆå¯¹çš„é—®é¢˜ï¼‰**ï¼š\n    *   **éœ€è¦ä»»åŠ¡æ ‡è¯†ç¬¦**ï¼šåœ¨æ¨ç†æ—¶ï¼Œéœ€è¦å¤–éƒ¨æä¾›â€œå½“å‰æ˜¯å“ªä¸ªä»»åŠ¡â€çš„ä¿¡æ¯æ¥æ¿€æ´»å¯¹åº”çš„æ¨¡å—ï¼ˆå¦‚SDPï¼‰ã€‚è¿™åœ¨å¼€æ”¾ä¸–ç•Œä¸­**æ— æ³•å®ç°è‡ªä¸»è¿è¡Œ**ã€‚\n    *   **é™æ€æˆ–å¯å‘å¼æ‰©å±•**ï¼šæ¨¡å—çš„æ·»åŠ å¾€å¾€æ˜¯é¢„å®šä¹‰æˆ–å¯å‘å¼çš„ï¼ˆä¾‹å¦‚æ¯ä»»åŠ¡å›ºå®šæ·»åŠ ï¼‰ï¼Œ**ä¸å¤Ÿæ™ºèƒ½å’Œé«˜æ•ˆ**ï¼Œå¯èƒ½å¯¼è‡´å‚æ•°å†—ä½™æˆ–çŸ¥è¯†æ— æ³•å…±äº«ã€‚\n\n#### 4. CLAREï¼šè‡ªä¸»è·¯ç”±ä¸åŠ¨æ€æ‰©å±•çš„æ¨¡å—åŒ–æŒç»­å­¦ä¹ \n*   **æ ¸å¿ƒåˆ›æ–°**ï¼šCLAREåœ¨æ¨¡å—åŒ–æ¶æ„çš„åŸºç¡€ä¸Šï¼Œåšå‡ºäº†**ä¸‰é¡¹å…³é”®æ”¹è¿›**ï¼Œæ—¨åœ¨å®ç°**å®Œå…¨è‡ªä¸»ã€æ— èŒƒä¾‹ã€é«˜æ•ˆ**çš„æŒç»­å­¦ä¹ ã€‚\n*   **å…·ä½“æ”¹è¿›ç‚¹**ï¼š\n    1.  **è‡ªä¸»è·¯ç”±æœºåˆ¶**ï¼š\n        *   **ä¹‹å‰çš„é—®é¢˜**ï¼šSDPç­‰æ–¹æ³•éœ€è¦**Oracleä»»åŠ¡æ ‡è¯†**ã€‚\n        *   **CLAREçš„æ”¹è¿›**ï¼šå¼•å…¥**åŸºäºè‡ªç¼–ç å™¨çš„åˆ¤åˆ«å™¨**ã€‚æ¯ä¸ªåˆ¤åˆ«å™¨å­¦ä¹ é‡å»ºç‰¹å®šä»»åŠ¡çš„ç‰¹å¾ã€‚åœ¨æ¨ç†æ—¶ï¼Œé€šè¿‡æ¯”è¾ƒå½“å‰è¾“å…¥ç‰¹å¾ä¸å„åˆ¤åˆ«å™¨çš„**é‡å»ºè¯¯å·®**ï¼Œ**åŠ¨æ€ã€è‡ªåŠ¨åœ°**é€‰æ‹©æœ€ç›¸å…³çš„é€‚é…å™¨ï¼Œæ— éœ€ä»»ä½•ä»»åŠ¡æ ‡ç­¾ã€‚\n    2.  **åŠ¨æ€æ‰©å±•ç­–ç•¥**ï¼š\n        *   **ä¹‹å‰çš„é—®é¢˜**ï¼šå›ºå®šæ¯ä»»åŠ¡æ·»åŠ æ¨¡å—ï¼Œä¸é«˜æ•ˆã€‚\n        *   **CLAREçš„æ”¹è¿›**ï¼šæå‡ºåŸºäº**å±‚é—´ç‰¹å¾ç›¸ä¼¼æ€§**çš„å†³ç­–æœºåˆ¶ã€‚ä»…å½“æ–°ä»»åŠ¡çš„ç‰¹å¾ä¸æ‰€æœ‰æ—§ä»»åŠ¡ç‰¹å¾å·®å¼‚æ˜¾è‘—ï¼ˆé€šè¿‡Z-scoreé˜ˆå€¼Î³åˆ¤æ–­ï¼‰æ—¶ï¼Œæ‰åœ¨å¯¹åº”å±‚**æ‰©å±•æ–°çš„é€‚é…å™¨**ã€‚å¦åˆ™ï¼Œæ–°ä»»åŠ¡å°†**å…±äº«å¤ç”¨**å·²æœ‰çš„ã€æœ€ç›¸å…³çš„é€‚é…å™¨ã€‚è¿™å®ç°äº†**æ¬¡çº¿æ€§çš„å‚æ•°å¢é•¿**ï¼ˆè®ºæ–‡ä¸­çº¦2%/ä»»åŠ¡ï¼‰ã€‚\n    3.  **é’ˆå¯¹VLAsçš„ä¼˜åŒ–è®¾è®¡**ï¼š\n        *   **ä¹‹å‰çš„é—®é¢˜**ï¼šè®¸å¤šæŒç»­å­¦ä¹ æ–¹æ³•åœ¨é€šç”¨CV/NLPä»»åŠ¡ä¸ŠéªŒè¯ï¼Œæœªå……åˆ†è€ƒè™‘VLAæ¨¡å‹ï¼ˆèåˆè§†è§‰ã€è¯­è¨€ã€åŠ¨ä½œï¼‰çš„ç‰¹æ€§ã€‚\n        *   **CLAREçš„æ”¹è¿›**ï¼šé€šè¿‡å®éªŒå‘ç°ï¼Œåœ¨VLAä¸­ï¼Œ**ç¼–ç å™¨éƒ¨åˆ†**ï¼ˆå°¤å…¶æ˜¯å¤„ç†è§‚æµ‹è¾“å…¥çš„æ¨¡å—ï¼‰æ¯”è§£ç å™¨éƒ¨åˆ†æ›´é€‚åˆå­˜å‚¨ä»»åŠ¡ç‰¹å®šçŸ¥è¯†ã€‚CLAREæ®æ­¤æœ‰é€‰æ‹©åœ°åœ¨ç¼–ç å™¨å±‚æ’å…¥é€‚é…å™¨ï¼Œè·å¾—äº†æœ€ä½³æ€§èƒ½ã€‚\n\n### ç€é‡å‘å±•çš„å‡ ä¸ªæ–¹å‘\n\n1.  **ä»â€œéœ€è¦æ ‡è¯†â€åˆ°â€œå®Œå…¨è‡ªä¸»â€**ï¼šè¿™æ˜¯éƒ¨ç½²åˆ°çœŸå®å¼€æ”¾ä¸–ç•Œæœºå™¨äººçš„å…³é”®ä¸€æ­¥ã€‚CLAREçš„**è‡ªç¼–ç å™¨è·¯ç”±**æ˜¯è¿™ä¸€æ–¹å‘çš„ä»£è¡¨æ€§è§£å†³æ–¹æ¡ˆï¼Œä½¿æœºå™¨äººèƒ½æ ¹æ®å½“å‰æ„ŸçŸ¥è‡ªä¸»å†³å®šè¡Œä¸ºæ¨¡å¼ã€‚\n2.  **ä»â€œå­˜å‚¨æ•°æ®â€åˆ°â€œæ— èŒƒä¾‹å­¦ä¹ â€**ï¼šä¸ºäº†è§£å†³æ•°æ®å­˜å‚¨å’Œéšç§çš„ç¡¬çº¦æŸï¼Œç ”ç©¶é‡ç‚¹è½¬å‘äº†**ä¸ä¾èµ–æ—§æ•°æ®**çš„æ–¹æ³•ã€‚CLAREé€šè¿‡**å†»ç»“ä¸»å¹²ã€åŠ¨æ€æ‰©å±•æ¨¡å—**çš„ç­–ç•¥ï¼Œåœ¨ä¸æ¥è§¦æ—§æ•°æ®çš„æƒ…å†µä¸‹ä¿æŠ¤äº†æ—§çŸ¥è¯†ã€‚\n3.  **ä»â€œå›ºå®šæ‰©å±•â€åˆ°â€œæ™ºèƒ½ã€é«˜æ•ˆæ‰©å±•â€**ï¼šä¸ºäº†åº”å¯¹ç†è®ºä¸Šæ— é™çš„ä»»åŠ¡åºåˆ—ï¼Œå¿…é¡»æ§åˆ¶æ¨¡å‹å®¹é‡çš„å¢é•¿ã€‚CLAREçš„**åŠ¨æ€æ‰©å±•**ä»£è¡¨äº†è¿™ä¸€æ–¹å‘ï¼Œå®ƒæ ¹æ®ä»»åŠ¡**å®é™…éœ€æ±‚**å¢åŠ å‚æ•°ï¼Œä¿ƒè¿›äº†ä»»åŠ¡é—´çš„**çŸ¥è¯†å…±äº«ä¸è¿ç§»**ï¼Œæ¯”ç®€å•åœ°ä¸ºæ¯ä¸ªä»»åŠ¡åˆ†é…ç‹¬ç«‹æ¨¡å—æ›´åŠ é«˜æ•ˆã€‚\n4.  **ä»â€œé€šç”¨æ¶æ„â€åˆ°â€œé¢†åŸŸå®šåˆ¶åŒ–â€**ï¼šCLAREçš„å·¥ä½œè¡¨æ˜ï¼Œé’ˆå¯¹**VLAè¿™ç§ç‰¹å®šæ¨¡å‹æ¶æ„**è¿›è¡ŒæŒç»­å­¦ä¹ è®¾è®¡ï¼ˆå¦‚é€‰æ‹©ç¼–ç å™¨å±‚è¿›è¡Œæ‰©å±•ï¼‰èƒ½å¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ã€‚è¿™æç¤ºæœªæ¥å·¥ä½œéœ€æ›´ç´§å¯†ç»“åˆå…·ä½“æ¨¡å‹ï¼ˆå¦‚æ‰©æ•£Transformerã€Flow Matchingï¼‰çš„ç‰¹æ€§è¿›è¡Œä¼˜åŒ–ã€‚\n\n### æ€»ç»“\n\nCLAREç«™åœ¨äº†**æ¨¡å—åŒ–æŒç»­å­¦ä¹ **æ–¹å‘çš„å‰æ²¿ï¼Œå®ƒé€šè¿‡**è‡ªä¸»è·¯ç”±**å’Œ**åŠ¨æ€æ‰©å±•**ä¸¤å¤§æ ¸å¿ƒæŠ€æœ¯ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å…ˆå‰å·¥ä½œä¸­â€œä¾èµ–ä»»åŠ¡æ ‡è¯†â€å’Œâ€œæ‰©å±•ä¸é«˜æ•ˆâ€çš„æ ¸å¿ƒç—›ç‚¹ã€‚å…¶å‘å±•è„‰ç»œæ¸…æ™°åœ°å±•ç¤ºäº†æŒç»­å­¦ä¹ ç ”ç©¶å‘**æ›´å®ç”¨ã€æ›´è‡ªä¸»ã€æ›´é«˜æ•ˆ**ç›®æ ‡è¿ˆè¿›çš„è¶‹åŠ¿ï¼šå³è®©å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿåƒç”Ÿç‰©ç³»ç»Ÿä¸€æ ·ï¼Œåœ¨èµ„æºå—é™çš„æ¡ä»¶ä¸‹ï¼ŒæŒç»­ã€ç¨³å®šåœ°ç§¯ç´¯å’Œæ•´åˆæ–°çŸ¥è¯†ï¼Œæœ€ç»ˆå®ç°é•¿æœŸã€å¯é çš„è‡ªä¸»æ™ºèƒ½ã€‚",
            "timestamp": "2026-01-20T17:58:26.921060",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## CLAREè®ºæ–‡çš„å‰ä½œåˆ†æ\n\nç»è¿‡ä»”ç»†é˜…è¯»ï¼Œæœ¬æ–‡ï¼ˆCLAREï¼‰å¹¶æœªåœ¨æ­£æ–‡æˆ–å‚è€ƒæ–‡çŒ®ä¸­æ˜ç¡®åˆ—å‡ºä½œè€…å›¢é˜Ÿï¼ˆRalf RÃ¶mer, Yi Zhang, Angela P. Schoelligï¼‰æ­¤å‰å‘è¡¨çš„ã€ä¸CLAREç›´æ¥ç›¸å…³çš„â€œå‰ä½œâ€ã€‚è®ºæ–‡åœ¨â€œRelated Workâ€éƒ¨åˆ†å¼•ç”¨äº†å¤§é‡å…¶ä»–ç ”ç©¶è€…çš„å·¥ä½œï¼Œä½†å¹¶æœªå°†å…¶æ ‡æ³¨ä¸ºæœ¬æ–‡ä½œè€…çš„å‰æœŸç ”ç©¶ã€‚\n\nå› æ­¤ï¼Œ**æ— æ³•æä¾›æ‚¨æ‰€è¦æ±‚çš„ã€å…³äºæœ¬æ–‡ä½œè€…â€œå‰ä½œâ€çš„è¯¦ç»†å¯¹æ¯”è¡¨æ ¼**ã€‚\n\nä¸è¿‡ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨è¯¦ç»†åˆ†æCLAREè®ºæ–‡**æ‰€å¼•ç”¨çš„å…³é”®ç›¸å…³ç ”ç©¶**ï¼Œå¹¶é‡ç‚¹é˜è¿°CLAREä¸è¿™äº›**æ ¸å¿ƒåŸºçº¿æˆ–ç›¸å…³æ–¹æ³•**çš„åŒºåˆ«ä¸æ”¹è¿›ã€‚è¿™æœ‰åŠ©äºç†è§£CLAREåœ¨æŠ€æœ¯è„‰ç»œä¸­çš„åˆ›æ–°å®šä½ã€‚\n\n### CLAREä¸æ ¸å¿ƒç›¸å…³å·¥ä½œçš„å¯¹æ¯”åˆ†æ\n\nä¸‹è¡¨æ¢³ç†äº†CLAREè®ºæ–‡ä¸­é‡ç‚¹å¯¹æ¯”æˆ–å€Ÿé‰´çš„å…³é”®æ–¹æ³•ï¼Œå¹¶åˆ†æäº†CLAREç›¸å¯¹äºå®ƒä»¬çš„ä¸»è¦åŒºåˆ«ä¸æ”¹è¿›ï¼š\n\n| æ–¹æ³•/å·¥ä½œ | æ ¸å¿ƒæ€æƒ³ | ä¸CLAREçš„ä¸»è¦åŒºåˆ« | CLAREçš„ä¸»è¦æ”¹è¿›ä¸ä¼˜åŠ¿ |\n| :--- | :--- | :--- | :--- |\n| **Sequential Fine-Tuning (SeqFFT)** | å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°è¿›è¡Œå…¨é‡å¾®è°ƒæ¥å­¦ä¹ æ¯ä¸ªæ–°ä»»åŠ¡ã€‚ | **ç¾éš¾æ€§é—å¿˜ä¸¥é‡**ï¼šç›´æ¥æ›´æ–°å…±äº«å‚æ•°ï¼Œä¸¥é‡è¦†ç›–æ—§ä»»åŠ¡çŸ¥è¯†ã€‚**ç¼ºä¹æ¨¡å—åŒ–**ï¼šæ‰€æœ‰ä»»åŠ¡å…±äº«åŒä¸€å¥—å‚æ•°ï¼Œæ— æ³•éš”ç¦»çŸ¥è¯†ã€‚ | **é¿å…é—å¿˜**ï¼šå†»ç»“ä¸»å¹²ç½‘ç»œï¼Œä»…é€šè¿‡æ–°å¢çš„è½»é‡çº§é€‚é…å™¨ï¼ˆAdapterï¼‰å­¦ä¹ æ–°ä»»åŠ¡ï¼Œä»æ ¹æœ¬ä¸Šéš”ç¦»äº†ä¸åŒä»»åŠ¡çš„çŸ¥è¯†å­˜å‚¨ã€‚ |\n| **Low-Rank Adaptation (LoRA)** | åœ¨Transformerå±‚çš„æƒé‡çŸ©é˜µæ—æ·»åŠ ä½ç§©åˆ†è§£çš„æ—è·¯çŸ©é˜µè¿›è¡Œå¾®è°ƒï¼Œè®­ç»ƒåå°†é€‚é…å™¨æƒé‡åˆå¹¶å›ä¸»å¹²ã€‚ | **é™æ€ä¸åˆå¹¶**ï¼šLoRAé€šå¸¸ä¸ºæ¯ä¸ªä»»åŠ¡è®­ç»ƒç‹¬ç«‹çš„é€‚é…å™¨ï¼Œä½†åœ¨æ¨ç†æ—¶å¸¸éœ€åˆå¹¶æˆ–éœ€è¦ä»»åŠ¡æ ‡è¯†æ¥åˆ‡æ¢ã€‚**éæŒç»­æ‰©å±•**ï¼šæ ‡å‡†LoRAä¸æ¶‰åŠæ¨¡å‹ç»“æ„çš„åŠ¨æ€ã€è‡ªä¸»æ‰©å±•ã€‚ | **åŠ¨æ€æ‰©å±•ä¸è·¯ç”±**ï¼šCLAREçš„é€‚é…å™¨æ˜¯**åŠ¨æ€ã€æŒ‰éœ€æ·»åŠ **çš„ï¼Œå¹¶ä¸”é€šè¿‡**è‡ªä¸»è·¯ç”±æœºåˆ¶**åœ¨æ¨ç†æ—¶åŠ¨æ€æ¿€æ´»ï¼Œæ— éœ€ä»»åŠ¡æ ‡è¯†ï¼Œä¹Ÿæ— éœ€åˆå¹¶æƒé‡ï¼Œå®ç°äº†çœŸæ­£çš„æŒç»­å­¦ä¹ æ¶æ„ã€‚ |\n| **Experience Replay (ER)** | å­˜å‚¨ä¸€éƒ¨åˆ†æ—§ä»»åŠ¡çš„æ•°æ®ï¼ˆæ ·æœ¬å›æ”¾ï¼‰ï¼Œä¸æ–°ä»»åŠ¡æ•°æ®æ··åˆè®­ç»ƒï¼Œä»¥ç¼“è§£é—å¿˜ã€‚ | **ä¾èµ–æ—§æ•°æ®**ï¼šéœ€è¦å­˜å‚¨å’Œè®¿é—®å†å²æ•°æ®ï¼Œå­˜åœ¨å­˜å‚¨å¼€é”€ã€éšç§å’Œå¯è¡Œæ€§é—®é¢˜ã€‚**éå‚æ•°é«˜æ•ˆ**ï¼šä»éœ€æ›´æ–°å¤§é‡å…±äº«å‚æ•°ã€‚ | **æ— éœ€æ ·æœ¬**ï¼šCLAREæ˜¯**æ— ç¤ºä¾‹**çš„æŒç»­å­¦ä¹ æ–¹æ³•ï¼Œå®Œå…¨ä¸ä¾èµ–å†å²æ•°æ®ï¼Œè§£å†³äº†å®é™…éƒ¨ç½²ä¸­æ•°æ®ä¸å¯ç”¨çš„æ ¸å¿ƒéš¾é¢˜ã€‚ |\n| **PackNet** | é€šè¿‡è¿­ä»£å¼å‰ªæï¼Œä¸ºæ¯ä¸ªæ–°ä»»åŠ¡é‡Šæ”¾å¹¶åˆ†é…ä¸€éƒ¨åˆ†ç½‘ç»œå‚æ•°ï¼Œå†»ç»“å…¶ä½™é‡è¦å‚æ•°ã€‚ | **å›ºå®šå®¹é‡ç“¶é¢ˆ**ï¼šç½‘ç»œæ€»å‚æ•°å›ºå®šï¼Œéšç€ä»»åŠ¡åºåˆ—å¢é•¿ï¼Œå¯åˆ†é…å‚æ•°è¶Šæ¥è¶Šå°‘ï¼Œé™åˆ¶å­¦ä¹ èƒ½åŠ›ã€‚**éœ€è¦ä»»åŠ¡ID**ï¼šæ¨ç†æ—¶éœ€è¦çŸ¥é“å½“å‰ä»»åŠ¡ä»¥æ¿€æ´»å¯¹åº”çš„å‚æ•°æ©ç ã€‚ | **å¯æ‰©å±•å®¹é‡**ï¼šCLAREèƒ½å¤Ÿ**åŠ¨æ€æ‰©å±•**æ¨¡å‹å®¹é‡ï¼ˆæ·»åŠ æ–°é€‚é…å™¨ï¼‰ï¼Œé¿å…äº†å›ºå®šå‚æ•°é¢„ç®—çš„ç“¶é¢ˆã€‚**è‡ªä¸»è·¯ç”±**ï¼šæ— éœ€ä»»åŠ¡æ ‡è¯†å³å¯è‡ªåŠ¨é€‰æ‹©æ­£ç¡®å‚æ•°ã€‚ |\n| **Sparse Diffusion Policy (SDP)** | ä¸ºæ‰©æ•£ç­–ç•¥å¼•å…¥ä»»åŠ¡ç‰¹å®šçš„ä¸“å®¶æ¨¡å—ã€‚ | **éœ€è¦ä»»åŠ¡æ ‡è¯†**ï¼šåœ¨éƒ¨ç½²æ—¶**éœ€è¦å¤–éƒ¨æä¾›ä»»åŠ¡æ ‡è¯†ç¬¦**æ¥æ‰‹åŠ¨å°†è¾“å…¥åˆ†æ´¾ç»™å¯¹åº”çš„ä¸“å®¶ï¼Œæ— æ³•è‡ªä¸»å†³ç­–ã€‚ | **å®Œå…¨è‡ªä¸»**ï¼šCLAREé€šè¿‡åŸºäºè‡ªç¼–ç å™¨çš„è·¯ç”±æœºåˆ¶ï¼Œ**ä»…æ ¹æ®å½“å‰è§‚æµ‹çš„ç‰¹å¾ç›¸ä¼¼åº¦è‡ªåŠ¨é€‰æ‹©**æœ€ç›¸å…³çš„é€‚é…å™¨ï¼Œå®ç°äº†å®Œå…¨è‡ªä¸»çš„ä»»åŠ¡æ— å…³æ¨ç†ã€‚ |\n| **Mixture of Experts (MoE)** | åœ¨å¤§å‹æ¨¡å‹ä¸­é›†æˆå¤šä¸ªä¸“å®¶å­ç½‘ç»œï¼Œé€šè¿‡è·¯ç”±ç½‘ç»œåŠ¨æ€é€‰æ‹©æ¿€æ´»ã€‚ | **å›ºå®šè§„æ¨¡**ï¼šMoEä¸­ä¸“å®¶æ•°é‡é€šå¸¸æ˜¯é¢„å…ˆè®¾å®šä¸”å›ºå®šçš„ï¼Œä¸æ”¯æŒåœ¨æŒç»­å­¦ä¹ è¿‡ç¨‹ä¸­åŠ¨æ€å¢åŠ æ–°ä¸“å®¶ã€‚**é™æ€è·¯ç”±**ï¼šè·¯ç”±ç½‘ç»œæœ¬èº«æ˜¯å›ºå®šçš„ï¼Œä¸éšæ–°ä»»åŠ¡è€Œæ¼”è¿›ã€‚ | **æŒç»­æ‰©å±•**ï¼šCLAREçš„é€‚é…å™¨ï¼ˆç›¸å½“äºâ€œä¸“å®¶â€ï¼‰é›†åˆå’Œå¯¹åº”çš„åˆ¤åˆ«å™¨ï¼ˆè·¯ç”±ä¾æ®ï¼‰ä¼š**éšç€æ–°ä»»åŠ¡è€ŒåŠ¨æ€ã€è‡ªä¸»åœ°å¢é•¿**ï¼Œä¸“ä¸ºæŒç»­å­¦ä¹ åœºæ™¯è®¾è®¡ã€‚ |\n| **LOTUS** | æ„å»ºä¸€ä¸ªä¸æ–­å¢é•¿çš„æŠ€èƒ½ç­–ç•¥åº“ï¼Œå¹¶é€šè¿‡å…ƒç­–ç•¥è¿›è¡Œç»„åˆã€‚ | **å±‚çº§ç»“æ„ä¸æ•°æ®ä¾èµ–**ï¼šéœ€è¦è®­ç»ƒä¸€ä¸ªé«˜çº§åˆ«çš„å…ƒç­–ç•¥æ¥é€‰æ‹©æŠ€èƒ½ï¼Œè€Œå…ƒç­–ç•¥çš„è®­ç»ƒé€šå¸¸**ä¾èµ–ç»éªŒå›æ”¾ï¼ˆERï¼‰** ä½¿ç”¨æ—§æ•°æ®ã€‚ | **ç«¯åˆ°ç«¯ä¸æ— ç¤ºä¾‹**ï¼šCLAREæ˜¯**ç«¯åˆ°ç«¯**çš„å•ä¸€ç­–ç•¥æ¨¡å‹ï¼Œæ— éœ€ç»´æŠ¤å¤æ‚çš„æŠ€èƒ½åº“å’Œå…ƒç­–ç•¥ã€‚å…¶è·¯ç”±æœºåˆ¶å†…ç”Ÿäºæ¨¡å‹å‰å‘ä¼ æ’­ä¸­ï¼Œä¸”**å®Œå…¨æ— éœ€å†å²æ•°æ®**ã€‚ |\n\n### æ€»ç»“ï¼šCLAREçš„æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°\n\nç›¸è¾ƒäºä¸Šè¿°ç›¸å…³å·¥ä½œï¼ŒCLAREçš„**ä¸»è¦æ”¹å–„**å¯å½’çº³ä¸ºä»¥ä¸‹å‡ ç‚¹ï¼š\n\n1.  **æ— ç¤ºä¾‹çš„æŒç»­å­¦ä¹ **ï¼šå½»åº•æ‘†è„±äº†å¯¹å­˜å‚¨å†å²æ•°æ®çš„ä¾èµ–ï¼Œè§£å†³äº†æœºå™¨äººé•¿æœŸéƒ¨ç½²ä¸­çš„å®é™…çº¦æŸã€‚\n2.  **è‡ªä¸»åŠ¨æ€æ‰©å±•**ï¼šå¼•å…¥åŸºäºç‰¹å¾ç›¸ä¼¼åº¦ï¼ˆz-scoreï¼‰çš„é˜ˆå€¼åˆ¤æ–­ï¼Œ**ä»…åœ¨å¿…è¦æ—¶**ä¸ºæ¨¡å‹æ·»åŠ æ–°çš„é€‚é…å™¨æ¨¡å—ï¼Œå®ç°äº†å‚æ•°æ•ˆç‡é«˜ï¼ˆ~2%/ä»»åŠ¡ï¼‰ä¸”æ€§èƒ½ä¼˜å¼‚çš„å®¹é‡å¢é•¿ã€‚\n3.  **ä»»åŠ¡æ ‡è¯†æ— å…³çš„è‡ªä¸»è·¯ç”±**ï¼šåˆ›æ–°åœ°ä½¿ç”¨è½»é‡çº§è‡ªç¼–ç å™¨ä½œä¸ºâ€œåˆ¤åˆ«å™¨â€ï¼Œé€šè¿‡è®¡ç®—è¾“å…¥ç‰¹å¾çš„é‡æ„è¯¯å·®æ¥åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„é€‚é…å™¨ï¼Œå®ç°äº†åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­**æ— éœ€äººå·¥æä¾›ä»»åŠ¡æ ‡ç­¾çš„å®Œå…¨è‡ªä¸»æ¨ç†**ã€‚\n4.  **é’ˆå¯¹VLAæ¨¡å‹çš„å®šåˆ¶åŒ–è®¾è®¡**ï¼šé€šè¿‡å®éªŒå‘ç°å¹¶ç¡®å®šäº†åœ¨VLAçš„**ç¼–ç å™¨éƒ¨åˆ†**ï¼ˆè€Œéè§£ç å™¨ï¼‰æ’å…¥é€‚é…å™¨æ•ˆæœæœ€ä½³ï¼Œè¿™ä¸ºåŸºäºTransformerçš„VLAsçš„æŒç»­å­¦ä¹ æä¾›äº†é‡è¦çš„æ¶æ„è®¾è®¡æ´å¯Ÿã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼ŒCLAREçš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå°†â€œåŠ¨æ€ç»“æ„æ‰©å±•â€ã€â€œæ— ç¤ºä¾‹å­¦ä¹ â€å’Œâ€œè‡ªä¸»è·¯ç”±â€è¿™ä¸‰ä¸ªæŒç»­å­¦ä¹ ä¸­çš„ç†æƒ³ç‰¹æ€§ï¼Œé¦–æ¬¡åœ¨ä¸€ä¸ªé¢å‘æœºå™¨äººVLAæ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ä¸­å®ç°ï¼Œå¹¶åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å³ä½¿æ˜¯æœ‰æ•°æ®ä¼˜åŠ¿çš„åŸºçº¿æ–¹æ³•ã€‚**",
            "timestamp": "2026-01-20T17:59:11.489536",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## CLARE å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\nCLARE çš„æ ¸å¿ƒæ˜¯é€šè¿‡**è½»é‡çº§æ¨¡å—åŒ–é€‚é…å™¨**ã€**åŸºäºç‰¹å¾ç›¸ä¼¼æ€§çš„åŠ¨æ€æ‰©å±•ç­–ç•¥**å’Œ**è‡ªç¼–ç å™¨è·¯ç”±æœºåˆ¶**ï¼Œå®ç°è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰çš„æ— ç¤ºä¾‹ã€æ— ä»»åŠ¡æ ‡è¯†çš„æŒç»­å­¦ä¹ ã€‚å…¶å…³é”®æŠ€æœ¯ç»†èŠ‚å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æŠ€æœ¯æ¨¡å— | æ ¸å¿ƒæ€æƒ³ | å…·ä½“æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…³é”®å‚æ•°ä¸è®¾è®¡ |\n| :--- | :--- | :--- | :--- |\n| **1. æ¨¡å—åŒ–é€‚é…å™¨** | åœ¨é¢„è®­ç»ƒVLAçš„é€‰å®šå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰å±‚æ—å¹¶è¡Œæ’å…¥è½»é‡çº§ä¾§æ”¯é€‚é…å™¨ï¼Œä»¥å­˜å‚¨æ–°ä»»åŠ¡çŸ¥è¯†ï¼Œé¿å…è¦†ç›–åŸæœ‰å‚æ•°ã€‚ | - **ç»“æ„**ï¼šé‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼ŒåŒ…å«ä¸€ä¸ªä¸‹æŠ•å½±çŸ©é˜µå’Œä¸€ä¸ªä¸ŠæŠ•å½±çŸ©é˜µï¼Œä¸­é—´ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ã€‚ <br> - **å…¬å¼**ï¼šå¯¹äºå¯æ‰©å±•å±‚ â„“ï¼Œå…¶ç¬¬ i ä¸ªé€‚é…å™¨çš„è¾“å‡ºä¸ºï¼š <br> `A_â„“^i(x_â„“) = W_â„“,i^up Â· ReLU(W_â„“,i^down Â· x_â„“)` <br> å…¶ä¸­ `x_â„“ âˆˆ â„^(d_â„“)` æ˜¯å±‚è¾“å…¥ç‰¹å¾ï¼Œ`W_â„“,i^down âˆˆ â„^(rÃ—d_â„“)`, `W_â„“,i^up âˆˆ â„^(d_â„“Ã—r)`ï¼Œä¸” `r â‰ª d_â„“`ã€‚ <br> - **é›†æˆæ–¹å¼**ï¼šæ¨ç†æ—¶ï¼Œé€‰å®šé€‚é…å™¨çš„è¾“å‡ºä¸åŸå§‹FFNå±‚è¾“å‡ºç›¸åŠ ï¼š <br> `FFN_â„“(x_â„“) = FFN_â„“^pre(x_â„“) + A_â„“^*(x_â„“)` | - **ç“¶é¢ˆç»´åº¦ r**ï¼šè¿œå°äºåŸå§‹ç‰¹å¾ç»´åº¦ `d_â„“`ï¼Œç¡®ä¿å‚æ•°é«˜æ•ˆã€‚ <br> - **æ‰©å±•å±‚é›†åˆ â„°**ï¼šå®éªŒè¡¨æ˜ï¼Œåœ¨æ¨¡å‹çš„**ç¼–ç å™¨éƒ¨åˆ†**ï¼ˆå¦‚è§†è§‰/è¯­è¨€ç‰¹å¾æŠ•å½±å±‚æˆ–Transformerç¼–ç å™¨å±‚ï¼‰æ·»åŠ é€‚é…å™¨æ•ˆæœæœ€ä½³ã€‚ <br> - **è®­ç»ƒç­–ç•¥**ï¼šå­¦ä¹ æ–°ä»»åŠ¡æ—¶ï¼Œ**ä»…è®­ç»ƒæ–°æ·»åŠ çš„é€‚é…å™¨**ï¼Œå†»ç»“åŸºç¡€æ¨¡å‹å’Œå·²æœ‰é€‚é…å™¨å‚æ•°ã€‚ |\n| **2. è‡ªç¼–ç å™¨è·¯ç”±æœºåˆ¶** | ä¸ºæ¯ä¸ªå¯æ‰©å±•å±‚é…å¤‡ä¸€ç»„è‡ªç¼–ç å™¨åˆ¤åˆ«å™¨ï¼Œé€šè¿‡è®¡ç®—è¾“å…¥ç‰¹å¾çš„é‡æ„è¯¯å·®ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„é€‚é…å™¨ï¼Œæ— éœ€ä»»åŠ¡æ ‡è¯†ã€‚ | - **åˆ¤åˆ«å™¨ç»“æ„**ï¼šè½»é‡çº§è‡ªç¼–ç å™¨ï¼Œæ¥æ”¶ä¸é€‚é…å™¨ç›¸åŒçš„è¾“å…¥ç‰¹å¾ `x_â„“`ã€‚ <br> - **é‡æ„è¯¯å·®**ï¼šç¬¬ j ä¸ªåˆ¤åˆ«å™¨å¯¹ç‰¹å¾ `x_â„“` çš„é‡æ„è¯¯å·®ä¸ºï¼š <br> `e_â„“^j(x_â„“) = â€– x_â„“ - D_â„“^j(x_â„“) â€–_2` <br> - **è·¯ç”±å†³ç­–**ï¼šé€‰æ‹©ä¸å…·æœ‰æœ€å°é‡æ„è¯¯å·®çš„åˆ¤åˆ«å™¨ç›¸å…³è”çš„é€‚é…å™¨ï¼š <br> `j* = argmin_(jâˆˆ{1,...,n}) e_â„“^j(x_â„“)` <br> `A_â„“^*(x_â„“) = B_â„“(D_â„“^(j*))` <br> å…¶ä¸­ `B_â„“: D_â„“ â†’ A_â„“` æ˜¯åˆ¤åˆ«å™¨åˆ°é€‚é…å™¨çš„æ˜ å°„å‡½æ•°ã€‚ <br> - **è®­ç»ƒç›®æ ‡**ï¼šæ–°ä»»åŠ¡ n çš„åˆ¤åˆ«å™¨ `D_â„“^n` ä½¿ç”¨é‡æ„æŸå¤±è¿›è¡Œè®­ç»ƒï¼š <br> `â„’_recon(D_â„“^n) = ğ”¼_(x_â„“âˆ¼D_n)[ e_â„“^j(x_â„“) ]` | - **åˆ¤åˆ«å™¨æ•°é‡**ï¼šæ¯ä¸ªä»»åŠ¡åœ¨æ¯ä¸ªå¯æ‰©å±•å±‚éƒ½ä¼šæ–°å¢ä¸€ä¸ªåˆ¤åˆ«å™¨ã€‚ <br> - **è®­ç»ƒé¡ºåº**ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼šå…ˆè®­ç»ƒæ–°é€‚é…å™¨ï¼ˆä½¿ç”¨æµåŒ¹é…æŸå¤±ï¼‰ï¼Œå†å†»ç»“æ‰€æœ‰å‚æ•°ï¼Œå•ç‹¬è®­ç»ƒæ–°åˆ¤åˆ«å™¨ï¼ˆä½¿ç”¨é‡æ„æŸå¤±ï¼‰ï¼Œä»¥ç¡®ä¿ç¨³å®šæ€§ã€‚ |\n| **3. åŠ¨æ€æ‰©å±•ç­–ç•¥** | å¹¶éæ¯å±‚æ¯ä»»åŠ¡éƒ½æ·»åŠ æ–°é€‚é…å™¨ï¼Œè€Œæ˜¯ä»…å½“æ–°ä»»åŠ¡ç‰¹å¾ä¸æ‰€æœ‰æ—§ä»»åŠ¡ç‰¹å¾æ˜¾è‘—ä¸åŒæ—¶ï¼Œæ‰åœ¨å¯¹åº”å±‚è¿›è¡Œæ‰©å±•ï¼Œå®ç°äºšçº¿æ€§å‚æ•°å¢é•¿ã€‚ | - **æ–°é¢–æ€§åº¦é‡**ï¼šä½¿ç”¨**Z-score**æ ‡å‡†åŒ–æ‰€æœ‰å·²æœ‰åˆ¤åˆ«å™¨åœ¨æ–°ä»»åŠ¡æ•°æ®ä¸Šçš„å¹³å‡é‡æ„è¯¯å·®ï¼Œä»¥è¿›è¡Œå…¬å¹³æ¯”è¾ƒï¼š <br> `z_â„“^j(x_â„“) = (1/|D_n|) Î£_(x_â„“âˆˆD_n) (e_â„“^j(x_â„“) - Î¼_â„“^j) / Ïƒ_â„“^j` <br> å…¶ä¸­ `Î¼_â„“^j`, `Ïƒ_â„“^j` æ˜¯åˆ¤åˆ«å™¨ `D_â„“^j` å†å²é‡æ„è¯¯å·®çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚ <br> - **æ‰©å±•æ¡ä»¶**ï¼šå¯¹äºå±‚ â„“ï¼Œå¦‚æœ**æ‰€æœ‰**å·²æœ‰åˆ¤åˆ«å™¨ (`j=1,...,n-1`) çš„Z-scoreå‡å¤§äºé˜ˆå€¼ Î³ï¼Œåˆ™åˆ¤å®šä¸ºæ–°ç‰¹å¾ï¼Œæ‰©å±•ä¸€ä¸ªæ–°é€‚é…å™¨ `A_â„“^(k_â„“)` å¹¶é“¾æ¥æ–°åˆ¤åˆ«å™¨ `D_â„“^n`ã€‚ <br> - **é“¾æ¥ç­–ç•¥**ï¼šè‹¥ä¸æ‰©å±•ï¼Œåˆ™ä»æ·»åŠ ä¸€ä¸ª**è¾…åŠ©åˆ¤åˆ«å™¨** `D_â„“^n`ï¼Œå¹¶å°†å…¶é“¾æ¥åˆ°ä¸ç°æœ‰åˆ¤åˆ«å™¨ä¸­é‡æ„è¯¯å·®æœ€å°çš„é‚£ä¸ªæ‰€å…³è”çš„é€‚é…å™¨ï¼š <br> `j* = argmin_(jâˆˆ{1,...,n-1}) ğ”¼_(x_â„“âˆ¼D_n)[ e_â„“^j(x_â„“) ]` <br> `B_â„“(D_â„“^n) = B_â„“(D_â„“^(j*))` <br> - **ä¿åº•æ‰©å±•**ï¼šå¦‚æœä¸€è½®å­¦ä¹ ä¸­æ²¡æœ‰ä»»ä½•å±‚è¢«æ‰©å±•ï¼Œåˆ™åœ¨æœ€æµ…å±‚ `â„“_1 âˆˆ â„°` å¼ºåˆ¶æ·»åŠ ä¸€ä¸ªé€‚é…å™¨ï¼Œä»¥ç¡®ä¿æ•è·æ–°ä»»åŠ¡ç‰¹æ€§ã€‚ | - **æ‰©å±•é˜ˆå€¼ Î³**ï¼šæ§åˆ¶æ‰©å±•çš„æ¿€è¿›ç¨‹åº¦ã€‚Î³ è¶Šå¤§ï¼Œæ‰©å±•è¶Šä¿å®ˆï¼Œæ·»åŠ çš„é€‚é…å™¨è¶Šå°‘ã€‚è®ºæ–‡ä¸­ä¸»è¦ä½¿ç”¨ `Î³ = 2.5`ã€‚ <br> - **å‚æ•°å¢é•¿**ï¼šå¹³å‡æ¯ä¸ªä»»åŠ¡ä»…å¢åŠ çº¦ **1.7% - 2.3%** çš„å‚æ•°ï¼ˆç›¸å¯¹äºçº¦2äº¿çš„åŸºç¡€æ¨¡å‹ï¼‰ã€‚ |\n| **4. åŸºç¡€ç­–ç•¥ä¸æŸå¤±å‡½æ•°** | é‡‡ç”¨åŸºäºæµåŒ¹é…ï¼ˆFlow Matchingï¼‰çš„ç”Ÿæˆå¼ç­–ç•¥æ¨¡å‹ï¼Œå­¦ä¹ ä»å™ªå£°åˆ°åŠ¨ä½œåºåˆ—çš„å‘é‡åœºã€‚ | - **æµåŒ¹é…æŸå¤±**ï¼šç”¨äºè®­ç»ƒç­–ç•¥ï¼ˆåŒ…æ‹¬é€‚é…å™¨ï¼‰ã€‚ <br> `â„’(Î¸_n) = ğ”¼_(s, (A^1, o), A^0)[ â€– v_(Î¸_n)(A^s, o, s) - (A^1 - A^0) â€–_2 ]` <br> å…¶ä¸­ï¼š <br> - `s ~ U([0,1])`ï¼Œ `(A^1, o) ~ D_n`ï¼ˆç›®æ ‡åŠ¨ä½œä¸è§‚æµ‹ï¼‰ï¼Œ `A^0 ~ N(0, I)`ï¼ˆé«˜æ–¯å™ªå£°ï¼‰ã€‚ <br> - `A^s = (1-s)A^0 + sA^1` æ˜¯æ’å€¼ç‚¹ã€‚ <br> - `v_(Î¸_n)` æ˜¯å¾…å­¦ä¹ çš„å‘é‡åœºã€‚ <br> - **æ¨ç†**ï¼šé€šè¿‡ä» `A^0` å¼€å§‹å¯¹ `v_(Î¸_n)` è¿›è¡Œæ¬§æ‹‰ç§¯åˆ†ï¼Œç”ŸæˆåŠ¨ä½œåºåˆ— `A^1`ã€‚ | - **åŠ¨ä½œå—é•¿åº¦ H**ï¼š`H = 16`ã€‚ <br> - **æ‰§è¡Œæ­¥é•¿ h**ï¼š`h = 8`ï¼Œå³æ¯ç”Ÿæˆä¸€ä¸ªåŠ¨ä½œå—ï¼Œæ‰§è¡Œå…¶ä¸­å‰8æ­¥åé‡æ–°è§„åˆ’ã€‚ <br> - **æ§åˆ¶é¢‘ç‡**ï¼š`20 Hz`ã€‚ |\n\n### **æŠ€æœ¯åˆ›æ–°ä¸ä»·å€¼æ€»ç»“**\n\n1.  **æ— ç¤ºä¾‹ä¸æ— æ ‡è¯†çš„æŒç»­å­¦ä¹ **ï¼šé€šè¿‡**è‡ªç¼–ç å™¨è·¯ç”±**å®ç°äº†å®Œå…¨è‡ªä¸»çš„ä»»åŠ¡è¯†åˆ«ä¸é€‚é…å™¨é€‰æ‹©ï¼Œè§£å†³äº†å®é™…æœºå™¨äººé•¿æœŸéƒ¨ç½²ä¸­æ•°æ®æ— æ³•å­˜å‚¨ã€ä»»åŠ¡è¾¹ç•Œæ¨¡ç³Šçš„æ ¸å¿ƒéš¾é¢˜ã€‚\n2.  **å‚æ•°é«˜æ•ˆçš„åŠ¨æ€æ¶æ„**ï¼š**åŸºäºZ-scoreçš„åŠ¨æ€æ‰©å±•**æœºåˆ¶ï¼Œä½¿æ¨¡å‹å®¹é‡æ ¹æ®ä»»åŠ¡æ–°é¢–æ€§**æŒ‰éœ€å¢é•¿**ï¼Œé¿å…äº†å›ºå®šæ¶æ„çš„å®¹é‡ç“¶é¢ˆæˆ–å‚æ•°æµªè´¹ï¼Œå®ç°äº†äºšçº¿æ€§çš„å‚æ•°å¢é•¿ã€‚\n3.  **æ¨¡å—åŒ–ä¸å…¼å®¹æ€§**ï¼šå°†é€‚é…å™¨ä½œä¸º**å¹¶è¡Œä¾§æ”¯**æ’å…¥ï¼Œä¸æ”¹å˜åŸç½‘ç»œè¾“å…¥è¾“å‡ºï¼Œæ˜“äºä¸ç°æœ‰å¤§å‹VLAï¼ˆå¦‚OpenVLA, Pi0.5ï¼‰é›†æˆã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨**ç¼–ç å™¨æ¨¡å—**æ·»åŠ é€‚é…å™¨æ•ˆæœæœ€ä¼˜ï¼Œè¿™ä¸ºç†è§£VLAä¸­ä»»åŠ¡çŸ¥è¯†çš„å­˜å‚¨ä½ç½®æä¾›äº†æ´è§ã€‚\n4.  **å¼ºå¤§çš„æ€§èƒ½è¡¨ç°**ï¼šåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCLAREåœ¨**æ•´ä½“æ€§èƒ½ï¼ˆAUCï¼‰** ä¸Šæ˜¾è‘—è¶…è¶Šäº†åŒ…æ‹¬éœ€è¦å­˜å‚¨æ—§æ•°æ®çš„ç»éªŒå›æ”¾ï¼ˆERï¼‰åœ¨å†…çš„æ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶**åå‘è´Ÿè¿ç§»ï¼ˆNBTï¼‰æ¥è¿‘é›¶**ï¼Œå‡ ä¹å®Œå…¨é¿å…äº†ç¾éš¾æ€§é—å¿˜ã€‚",
            "timestamp": "2026-01-20T18:00:10.783471",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ ä½¿ç”¨çš„æ•°æ®é›†\n\n**æ•°æ®é›†åç§°ï¼šLIBERO åŸºå‡†æµ‹è¯•å¥—ä»¶**\n\n| å±æ€§ | ç»†èŠ‚æè¿° |\n| :--- | :--- |\n| **æ ¸å¿ƒç”¨é€”** | ä¸“é—¨ä¸ºæœºå™¨äºº**æŒç»­å­¦ä¹ ï¼ˆContinual Learningï¼‰** åœºæ™¯è®¾è®¡ã€‚ |\n| **ç¯å¢ƒä¸æœºå™¨äºº** | æ¨¡æ‹Ÿå¨æˆ¿ç¯å¢ƒï¼Œä½¿ç”¨ Franka æœºæ¢°è‡‚ï¼ˆé…å¤‡å¹³è¡Œå¤¹çˆªï¼‰ã€‚ |\n| **ä»»åŠ¡ç±»å‹** | åŒ…å«éœ€è¦ç†è§£è¯­è¨€æŒ‡ä»¤å¹¶æ‰§è¡Œå¤šç§æ“ä½œçš„é•¿è§†é‡ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šæ‹¾å–-æ”¾ç½®ã€æ‰“å¼€æŠ½å±‰ã€è½¬åŠ¨æ—‹é’®ç­‰ã€‚ |\n| **æ•°æ®æ„æˆ** | - **é¢„è®­ç»ƒæ•°æ®**ï¼šæ¥è‡ª **LIBERO-90** çš„ 90 ä¸ªçŸ­è§†é‡ä»»åŠ¡æ¼”ç¤ºã€‚<br>- **æŒç»­å­¦ä¹ è¯„ä¼°æ•°æ®**ï¼šæ¥è‡ª **LIBERO-10** çš„ 10 ä¸ª**é¡ºåºåˆ°è¾¾**çš„é•¿è§†é‡ä»»åŠ¡ã€‚æ¯ä¸ªä»»åŠ¡æä¾› **50 æ¡**äººç±»ä¸“å®¶æ¼”ç¤ºè½¨è¿¹ã€‚ |\n| **è§‚æµ‹è¾“å…¥** | å¤šæ¨¡æ€è§‚æµ‹ï¼š<br>1. **è§†è§‰**ï¼šè…•éƒ¨æ‘„åƒå¤´å’Œç¬¬ä¸‰äººç§°æ‘„åƒå¤´çš„ RGB å›¾åƒã€‚<br>2. **æœ¬ä½“æ„ŸçŸ¥**ï¼šæœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨ä½å§¿å’Œå¤¹çˆªçŠ¶æ€ã€‚<br>3. **è¯­è¨€**ï¼šè‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚ |\n| **åŠ¨ä½œè¾“å‡º** | ç­–ç•¥ç”Ÿæˆé•¿åº¦ä¸º `H=16` çš„åŠ¨ä½œå—ï¼Œä»¥ `20 Hz` çš„æ§åˆ¶é¢‘ç‡æ‰§è¡Œå…¶ä¸­ `h=8` ä¸ªåŠ¨ä½œåè¿›è¡Œé‡è§„åˆ’ã€‚ |\n\n### äºŒã€ ä½¿ç”¨çš„è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä½¿ç”¨äº†ä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡æ¥å…¨é¢è¯„ä¼°æŒç»­å­¦ä¹ æ€§èƒ½ï¼Œæ—¨åœ¨è¡¡é‡**æ–°ä»»åŠ¡å­¦ä¹ èƒ½åŠ›**å’Œ**æ—§ä»»åŠ¡é—å¿˜ç¨‹åº¦**ã€‚\n\n| æŒ‡æ ‡ | å…¬å¼ | è§£é‡Šä¸ç»†èŠ‚ |\n| :--- | :--- | :--- |\n| **æ›²çº¿ä¸‹é¢ç§¯ (AUC)** | `AUC = (1/N) * Î£_{n=1}^N [ (1/(N-n+1)) * Î£_{m=n}^N r_{n\\|m} ]` | **ç»¼åˆè¯„ä»·æŒ‡æ ‡**ã€‚è®¡ç®—æ‰€æœ‰ä»»åŠ¡åœ¨æ‰€æœ‰å­¦ä¹ é˜¶æ®µåçš„å¹³å‡æˆåŠŸç‡ã€‚`r_{n\\|m}` è¡¨ç¤ºåœ¨å­¦å®Œå‰ `m` ä¸ªä»»åŠ¡åï¼Œå¯¹ç¬¬ `n` ä¸ªä»»åŠ¡çš„æµ‹è¯•æˆåŠŸç‡ã€‚è¯¥å€¼è¶Šé«˜ï¼Œè¡¨ç¤ºæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ï¼ˆå…¼é¡¾æ–°ä»»åŠ¡å­¦ä¹ å’Œæ—§ä»»åŠ¡ä¿ç•™ï¼‰è¶Šå¥½ã€‚ |\n| **å‰å‘è¿ç§» (FWT)** | `FWT = (1/N) * Î£_{n=1}^N r_{n\\|n}` | **è¡¡é‡æ–°ä»»åŠ¡å­¦ä¹ èƒ½åŠ›**ã€‚è®¡ç®—æ¯ä¸ªä»»åŠ¡åœ¨**åˆšå­¦å®Œå®ƒä¹‹å**çš„å³æ—¶æˆåŠŸç‡ï¼Œç„¶åå–å¹³å‡ã€‚è¯¥å€¼è¶Šé«˜ï¼Œè¡¨ç¤ºæ¨¡å‹å¿«é€Ÿé€‚åº”å¹¶æŒæ¡æ–°ä»»åŠ¡çš„èƒ½åŠ›è¶Šå¼ºã€‚ |\n| **è´Ÿå‘åå‘è¿ç§» (NBT)** | `NBT = (1/(N-1)) * Î£_{n=1}^{N-1} [ (1/(N-n)) * Î£_{m=n+1}^N (r_{n\\|n} - r_{n\\|m}) ]` | **è¡¡é‡ç¾éš¾æ€§é—å¿˜ç¨‹åº¦**ã€‚è®¡ç®—æ¯ä¸ªæ—§ä»»åŠ¡åœ¨åç»­å­¦ä¹ æ–°ä»»åŠ¡åï¼Œå…¶æˆåŠŸç‡çš„**å¹³å‡ä¸‹é™å€¼**ã€‚`(r_{n\\|n} - r_{n\\|m})` è¡¨ç¤ºé—å¿˜é‡ã€‚**NBT å€¼è¶Šä½è¶Šå¥½**ï¼Œç†æƒ³æƒ…å†µä¸‹æ¥è¿‘ 0 æˆ–ä¸ºè´Ÿå€¼ï¼ˆè¡¨ç¤ºæ²¡æœ‰é—å¿˜ç”šè‡³æ€§èƒ½æå‡ï¼‰ã€‚ |\n\n**è¯„ä¼°åè®®ç»†èŠ‚ï¼š**\n- åœ¨æŒç»­å­¦ä¹ çš„æ¯ä¸ªé˜¶æ®µ `n` ç»“æŸåï¼Œæ¨¡å‹ä¼šåœ¨æ‰€æœ‰å·²å­¦ä»»åŠ¡ `ğ’¯â‚, â€¦, ğ’¯â‚™` ä¸Šè¿›è¡Œè¯„ä¼°ã€‚\n- æ¯ä¸ªä»»åŠ¡è¿›è¡Œ **100 æ¬¡** rollout æµ‹è¯•ã€‚\n- æµ‹è¯•åœ¨ **50 ç§**ä¸åŒçš„ç›¸å…³ç‰©ä½“åˆå§‹é…ç½®ä¸‹è¿›è¡Œï¼Œæ¯ç§é…ç½®æµ‹è¯•ä¸¤æ¬¡ã€‚\n- æ‰€æœ‰ç»“æœåœ¨ **3 ä¸ªéšæœºç§å­**ä¸Šå–å¹³å‡ï¼Œä»¥å¢åŠ ç»Ÿè®¡å¯é æ€§ã€‚\n\n**æ€»ç»“ï¼š**\né€šè¿‡ç»“åˆ **LIBERO** è¿™ä¸€ä¸“ä¸šçš„æœºå™¨äººæŒç»­å­¦ä¹ æ•°æ®é›†ï¼Œä»¥åŠ **AUCã€FWTã€NBT** è¿™å¥—äº’è¡¥çš„è¯„ä¼°æŒ‡æ ‡ï¼Œè®ºæ–‡å¯¹ CLARE æ–¹æ³•è¿›è¡Œäº†ä¸¥è°¨ã€å…¨é¢çš„é‡åŒ–åˆ†æã€‚ç»“æœè¡¨æ˜ï¼ŒCLARE åœ¨ä¿æŒé«˜å‰å‘è¿ç§»èƒ½åŠ›ï¼ˆå­¦ä¹ æ–°ä»»åŠ¡ï¼‰çš„åŒæ—¶ï¼Œå®ç°äº†æä½çš„è´Ÿå‘åå‘è¿ç§»ï¼ˆå‡ ä¹æ— é—å¿˜ï¼‰ï¼Œä»è€Œè·å¾—äº†æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•çš„ç»¼åˆæ€§èƒ½ï¼ˆAUCï¼‰ã€‚",
            "timestamp": "2026-01-20T18:00:40.448734",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## å®éªŒæ•°æ®é›†ä¸è¯„ä¼°æŒ‡æ ‡åˆ†æ\n\n### æ•°æ®é›†\nè®ºæ–‡åœ¨ **LIBERO åŸºå‡†æµ‹è¯•** ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå…·ä½“åŒ…æ‹¬ï¼š\n- **é¢„è®­ç»ƒæ•°æ®é›†**ï¼šLIBERO-90ï¼ˆåŒ…å« 90 ä¸ªçŸ­è§†è·ä»»åŠ¡ï¼‰ã€‚\n- **æŒç»­å­¦ä¹ è¯„ä¼°æ•°æ®é›†**ï¼šLIBERO-10ï¼ˆåŒ…å« 10 ä¸ªæŒ‰é¡ºåºåˆ°è¾¾çš„é•¿è§†è·ä»»åŠ¡ï¼‰ã€‚è¿™äº›ä»»åŠ¡æ¶‰åŠå¤æ‚çš„å¨æˆ¿ç¯å¢ƒæ“ä½œï¼Œå¦‚æŠ“å–æ”¾ç½®ã€æ‰“å¼€æŠ½å±‰ã€æ—‹è½¬æ—‹é’®ç­‰ï¼Œéœ€è¦æœºå™¨äººç†è§£è¯­è¨€æŒ‡ä»¤å¹¶æ‰§è¡Œå¤šç§åŠ¨ä½œã€‚\n\n### è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨ä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡æ¥é‡åŒ–æŒç»­å­¦ä¹ æ€§èƒ½ï¼š\n- **AUCï¼ˆæˆåŠŸç‡æ›²çº¿ä¸‹é¢ç§¯ï¼‰**ï¼šè¡¡é‡æ¨¡å‹åœ¨æ‰€æœ‰å·²å­¦ä»»åŠ¡ä¸Šçš„æ•´ä½“æ€§èƒ½ã€‚\n- **FWTï¼ˆå‰å‘è¿ç§»ï¼‰**ï¼šè¡¡é‡æ¨¡å‹å­¦ä¹ æ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚\n- **NBTï¼ˆè´Ÿå‘åå‘è¿ç§»ï¼‰**ï¼šè¡¡é‡æ¨¡å‹å¯¹æ—§ä»»åŠ¡çš„é—å¿˜ç¨‹åº¦ï¼ˆå€¼è¶Šä½è¶Šå¥½ï¼‰ã€‚\n\n### æ€§èƒ½æå‡\nCLARE åœ¨å®éªŒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°å¦‚ä¸‹ï¼š\n\n| éª¨å¹²ç½‘ç»œ | å¯¹æ¯”æ–¹æ³• | AUCï¼ˆCLARE vs. æœ€ä½³åŸºçº¿ï¼‰ | FWTï¼ˆCLAREï¼‰ | NBTï¼ˆCLAREï¼‰ |\n|:---|:---|:---|:---|:---|\n| **DiT-EncDec** | æœ€ä½³åŸºçº¿ï¼šERï¼ˆä½¿ç”¨å†å²æ•°æ®ï¼‰ | **66.71%** vs. 55.87%ï¼ˆ**+10.84%**ï¼‰ | 66.07% | **-0.80%**ï¼ˆå‡ ä¹æ— é—å¿˜ï¼‰ |\n| **DiT-Dec** | æœ€ä½³åŸºçº¿ï¼šERï¼ˆä½¿ç”¨å†å²æ•°æ®ï¼‰ | **75.11%** vs. 60.54%ï¼ˆ**+14.57%**ï¼‰ | 75.03% | 1.85% |\n\n**å…³é”®ç»“è®º**ï¼š\n1.  **å“è¶Šçš„æŠ—é—å¿˜èƒ½åŠ›**ï¼šCLARE çš„ NBT æ¥è¿‘é›¶ï¼Œè¡¨æ˜åœ¨æ— éœ€å­˜å‚¨å†å²æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå‡ ä¹å®Œå…¨é¿å…äº†ç¾éš¾æ€§é—å¿˜ã€‚\n2.  **é«˜æ•ˆå­¦ä¹ æ–°ä»»åŠ¡**ï¼šFWT ä¸å¾®è°ƒæ•´ä¸ªæ¨¡å‹çš„æ–¹æ³•ï¼ˆå¦‚ SeqFFTï¼‰ç›¸å½“ï¼Œè¯æ˜å…¶èƒ½ä»¥æå°‘çš„å‚æ•°å¢é•¿ï¼ˆå¹³å‡æ¯ä»»åŠ¡ä»…å¢åŠ  **~2%** çš„å‚æ•°ï¼‰æœ‰æ•ˆå­¦ä¹ æ–°æŠ€èƒ½ã€‚\n3.  **æ•´ä½“æ€§èƒ½é¢†å…ˆ**ï¼šåœ¨æœ€é‡è¦çš„ AUC æŒ‡æ ‡ä¸Šï¼ŒCLARE å¤§å¹…è¶…è¶Šæ‰€æœ‰åŸºçº¿ï¼ŒåŒ…æ‹¬ä¾èµ–å†å²æ•°æ®çš„ç»éªŒå›æ”¾ï¼ˆERï¼‰æ–¹æ³•ï¼Œå‡¸æ˜¾äº†å…¶æ–¹æ³•åœ¨**æ— ç¤ºä¾‹æŒç»­å­¦ä¹ **è®¾å®šä¸‹çš„ä¼˜è¶Šæ€§ã€‚\n\n**å®é™…ä»·å€¼ä½“ç°**ï¼šCLARE çš„æ€§èƒ½è¯æ˜ï¼Œæœºå™¨äººå¯ä»¥åœ¨ä¸å­˜å‚¨æ•æ„Ÿå†å²æ•°æ®ã€ä¸ä¾èµ–äººå·¥æä¾›ä»»åŠ¡æ ‡ç­¾çš„å®Œå…¨è‡ªä¸»åœºæ™¯ä¸­ï¼ŒæŒç»­ã€ç¨³å®šåœ°å­¦ä¹ æ–°æŠ€èƒ½ï¼Œè¿™å¯¹äºå®¶åº­ã€åŒ»ç–—ç­‰é•¿æœŸéƒ¨ç½²çš„æœºå™¨äººåº”ç”¨å…·æœ‰é‡è¦å®ç”¨æ„ä¹‰ã€‚",
            "timestamp": "2026-01-20T18:00:59.862131",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„åˆ†æï¼ŒCLARE æ¡†æ¶å­˜åœ¨ä»¥ä¸‹å‡ ä¸ªä¸»è¦å±€é™æ€§ï¼š\n\n- **å®éªŒè§„æ¨¡ä¸åœºæ™¯é™åˆ¶**ï¼š\n    - è®ºæ–‡å®éªŒä¸»è¦åŸºäºç›¸å¯¹è¾ƒå°çš„æ¨¡å‹ï¼ˆçº¦ 2 äº¿å‚æ•°ï¼‰å’Œæ¨¡æ‹Ÿç¯å¢ƒï¼ˆLIBERO åŸºå‡†æµ‹è¯•ï¼‰ã€‚è¿™é™åˆ¶äº†ç»“è®ºåœ¨æ›´å¤§è§„æ¨¡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰å’Œå¤æ‚çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„æ™®é€‚æ€§ã€‚\n    - å®éªŒä»»åŠ¡åºåˆ—ç›¸å¯¹è¾ƒçŸ­ï¼ˆ10ä¸ªä»»åŠ¡ï¼‰ï¼Œåœ¨æ›´é•¿çš„ä»»åŠ¡åºåˆ—æˆ–ä»»åŠ¡é—´å­˜åœ¨æ›´å¤æ‚å¹²æ‰°çš„æƒ…å†µä¸‹ï¼Œå…¶åŠ¨æ€æ‰©å±•å’Œè·¯ç”±æœºåˆ¶çš„æœ‰æ•ˆæ€§æœ‰å¾…éªŒè¯ã€‚\n\n- **æ–¹æ³•æœ¬èº«çš„æ½œåœ¨æŒ‘æˆ˜**ï¼š\n    - **åŠ¨æ€æ‰©å±•çš„é˜ˆå€¼æ•æ„Ÿæ€§**ï¼šæ‰©å±•é˜ˆå€¼ `Î³` æ˜¯ä¸€ä¸ªéœ€è¦æ‰‹åŠ¨è®¾å®šçš„è¶…å‚æ•°ã€‚å¦‚å›¾5æ‰€ç¤ºï¼Œ`Î³` å€¼çš„é€‰æ‹©éœ€è¦åœ¨æ€§èƒ½ï¼ˆAUC/FWTï¼‰å’Œå‚æ•°æ•ˆç‡ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œç¼ºä¹è‡ªé€‚åº”çš„ç¡®å®šæœºåˆ¶ã€‚\n    - **è·¯ç”±æœºåˆ¶çš„å¯é æ€§**ï¼šåŸºäºè‡ªåŠ¨ç¼–ç å™¨é‡æ„è¯¯å·®çš„è·¯ç”±æœºåˆ¶åœ¨ç‰¹å¾åˆ†å¸ƒå‘ç”Ÿå‰§çƒˆæˆ–æ„å¤–å˜åŒ–æ—¶å¯èƒ½å¤±æ•ˆã€‚è®ºæ–‡ä¸­æåˆ°ï¼Œå¦‚æœæµ…å±‚é€‚é…å™¨åœ¨åç»­ä»»åŠ¡ä¸­è¢«æ·»åŠ ï¼Œå¯èƒ½å¯¼è‡´æ·±å±‚ç‰¹å¾åˆ†å¸ƒåç§»ï¼Œä»è€Œå½±å“è·¯ç”±å’Œç­–ç•¥æ€§èƒ½ã€‚è™½ç„¶é€šè¿‡æ·»åŠ è¾…åŠ©åˆ¤åˆ«å™¨æ¥ç¼“è§£ï¼Œä½†æ­¤é—®é¢˜åœ¨å¼€æ”¾ä¸–ç•Œä¸­å¯èƒ½ä¾ç„¶å­˜åœ¨ã€‚\n    - **çŸ¥è¯†å…±äº«çš„å±€é™æ€§**ï¼šæ¡†æ¶ä¸»è¦é€šè¿‡â€œé“¾æ¥â€æœºåˆ¶ï¼ˆå°†æ–°åˆ¤åˆ«å™¨å…³è”åˆ°ç°æœ‰é€‚é…å™¨ï¼‰å®ç°çŸ¥è¯†è¿ç§»ã€‚è¿™ç§åŸºäºç‰¹å¾ç›¸ä¼¼æ€§çš„è½¯æ€§å…±äº«ï¼Œå¯èƒ½ä¸å¦‚æ˜¾å¼çš„ã€ç»“æ„åŒ–çš„çŸ¥è¯†è’¸é¦æˆ–å‚æ•°å…±äº«æœºåˆ¶é‚£æ ·é«˜æ•ˆï¼Œå°¤å…¶æ˜¯åœ¨ä»»åŠ¡é«˜åº¦ç›¸å…³æ—¶ã€‚\n\n- **è®¡ç®—ä¸éƒ¨ç½²è€ƒé‡**ï¼š\n    - **æ¨ç†å¼€é”€**ï¼šè™½ç„¶è®­ç»ƒæ˜¯å‚æ•°é«˜æ•ˆçš„ï¼Œä½†æ¨ç†æ—¶éœ€è¦åœ¨æ¯ä¸ªå¯æ‰©å±•å±‚è¿è¡Œå¤šä¸ªåˆ¤åˆ«å™¨æ¥è®¡ç®—é‡æ„è¯¯å·®ä»¥è¿›è¡Œè·¯ç”±ï¼Œè¿™å¼•å…¥äº†é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚å¯¹äºéœ€è¦ä½å»¶è¿Ÿå“åº”çš„å®æ—¶æœºå™¨äººåº”ç”¨ï¼Œè¿™å¯èƒ½æˆä¸ºä¸€ä¸ªç“¶é¢ˆã€‚\n    - **æ¨¡å—å¢é•¿çš„é•¿æœŸç®¡ç†**ï¼šéšç€ä»»åŠ¡æ•°é‡çº¿æ€§å¢é•¿ï¼Œåˆ¤åˆ«å™¨æ•°é‡ä¹Ÿä¼šå¢åŠ ã€‚è®ºæ–‡æœªè®¨è®ºå¯¹åˆ¤åˆ«å™¨é›†åˆè¿›è¡Œä¿®å‰ªæˆ–åˆå¹¶çš„æœºåˆ¶ï¼Œé•¿æœŸéƒ¨ç½²å¯èƒ½å¯¼è‡´è·¯ç”±æ¨¡å—è‡ƒè‚¿ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œæœªæ¥çš„ç ”ç©¶å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹å‘å±•å¼€ï¼š\n\n- **æ‰©å±•åˆ°æ›´å¤§è§„æ¨¡æ¨¡å‹ä¸çœŸå®ä¸–ç•Œ**ï¼š\n    - åœ¨å‚æ•°è§„æ¨¡æ›´å¤§ï¼ˆå¦‚æ•°åäº¿å‚æ•°ï¼‰çš„å…ˆè¿› VLAï¼ˆå¦‚ RT-2ã€OpenVLAï¼‰ä¸ŠéªŒè¯ CLARE çš„æœ‰æ•ˆæ€§ã€‚\n    - åœ¨çœŸå®ç‰©ç†æœºå™¨äººå¹³å°å’Œæ›´å¼€æ”¾ã€åŠ¨æ€çš„ç¯å¢ƒä¸­ï¼ˆå¦‚å®¶åº­ã€ä»“åº“ï¼‰è¿›è¡ŒæŒç»­å­¦ä¹ æµ‹è¯•ï¼Œè¯„ä¼°å…¶å¯¹è§†è§‰å˜åŒ–ã€å¹²æ‰°å’Œé•¿å°¾ä»»åŠ¡çš„é²æ£’æ€§ã€‚\n\n- **å¢å¼ºæ–¹æ³•çš„è‡ªé€‚åº”æ€§ä¸é²æ£’æ€§**ï¼š\n    - **è‡ªé€‚åº”æ‰©å±•é˜ˆå€¼**ï¼šç ”ç©¶æ ¹æ®ä»»åŠ¡å¤æ‚åº¦æˆ–ç‰¹å¾æ–°é¢–æ€§è‡ªåŠ¨è°ƒæ•´æ‰©å±•é˜ˆå€¼ `Î³` çš„æœºåˆ¶ï¼Œå‡å°‘å¯¹è¶…å‚æ•°çš„ä¾èµ–ã€‚\n    - **æ›´ç¨³å¥çš„è·¯ç”±æœºåˆ¶**ï¼šæ¢ç´¢é™¤é‡æ„è¯¯å·®ä¹‹å¤–çš„å…¶ä»–è·¯ç”±æŒ‡æ ‡ï¼ˆå¦‚åŸºäºæ³¨æ„åŠ›çš„é‡è¦æ€§è¯„åˆ†ï¼‰ï¼Œæˆ–è®¾è®¡å¯¹ç‰¹å¾åˆ†å¸ƒå˜åŒ–æ›´ä¸æ•æ„Ÿçš„åˆ¤åˆ«å™¨æ¶æ„ã€‚\n    - **æ”¹è¿›çŸ¥è¯†è¿ç§»**ï¼šåœ¨é€‚é…å™¨ä¹‹é—´å¼•å…¥è½»é‡çº§çš„æ­£åˆ™åŒ–æˆ–çŸ¥è¯†è’¸é¦æŸå¤±ï¼Œä»¥æ›´ä¸»åŠ¨åœ°ä¿ƒè¿›ç›¸ä¼¼ä»»åŠ¡é—´çš„æ­£å‘è¿ç§»ï¼ŒåŒæ—¶é˜²æ­¢è´Ÿè¿ç§»ã€‚\n\n- **ä¼˜åŒ–é•¿æœŸéƒ¨ç½²æ•ˆç‡**ï¼š\n    - **è·¯ç”±æ¨¡å—å‹ç¼©**ï¼šç ”ç©¶å¯¹åˆ¤åˆ«å™¨é›†åˆè¿›è¡Œå‘¨æœŸæ€§ä¿®å‰ªã€èšç±»æˆ–åˆå¹¶çš„æ–¹æ³•ï¼Œä»¥æ§åˆ¶è·¯ç”±æœºåˆ¶çš„å¢é•¿ï¼Œç»´æŒæ¨ç†æ•ˆç‡ã€‚\n    - **ç¡¬ä»¶æ„ŸçŸ¥éƒ¨ç½²**ï¼šä¼˜åŒ–é€‚é…å™¨å’Œåˆ¤åˆ«å™¨çš„è®¡ç®—å›¾ï¼Œæ¢ç´¢é€‚ç”¨äºè¾¹ç¼˜æˆ–æœºå™¨äººè®¾å¤‡çš„éƒ¨ç½²ç­–ç•¥ï¼Œä¾‹å¦‚æ¨¡å‹ç¼–è¯‘æˆ–ä¸“ç”¨ç¡¬ä»¶åŠ é€Ÿã€‚\n\n- **æ¢ç´¢æ›´ä¸°å¯Œçš„æŒç»­å­¦ä¹ åœºæ™¯**ï¼š\n    - å°†æ¡†æ¶åº”ç”¨äº**ç±»å¢é‡**æˆ–**åŸŸå¢é‡**å­¦ä¹ åœºæ™¯ï¼Œå…¶ä¸­ä»»åŠ¡è¾¹ç•Œæ›´æ¨¡ç³Šï¼Œå¯¹è·¯ç”±æœºåˆ¶çš„è¦æ±‚æ›´é«˜ã€‚\n    - ç»“åˆ**åœ¨çº¿å­¦ä¹ **æˆ–**å°‘é‡æ¼”ç¤º**è®¾ç½®ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨å°‘é‡äº¤äº’ä¸­å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼ŒåŒæ—¶ä»èƒ½é¿å…é—å¿˜ã€‚\n\n**æ€»ç»“**ï¼šCLARE ä¸ºæ— ç¤ºä¾‹ã€æ— ä»»åŠ¡æ ‡è¯†çš„ VLA æŒç»­å­¦ä¹ æä¾›äº†ä¸€ä¸ªåˆ›æ–°ä¸”æœ‰æ•ˆçš„æ¡†æ¶ï¼Œä½†å…¶åœ¨**å¤§è§„æ¨¡æ¨¡å‹éªŒè¯ã€çœŸå®ä¸–ç•Œé²æ£’æ€§ã€é•¿æœŸéƒ¨ç½²æ•ˆç‡ä»¥åŠè‡ªé€‚åº”è¶…å‚æ•°è°ƒæ•´**æ–¹é¢å­˜åœ¨å±€é™ã€‚æœªæ¥çš„å·¥ä½œåº”è‡´åŠ›äºå°†è¿™äº›æœ‰å‰æ™¯çš„æ¦‚å¿µæ‰©å±•åˆ°æ›´å®é™…ã€æ›´å¤æ‚çš„æœºå™¨äººå­¦ä¹ åœºæ™¯ä¸­ã€‚",
            "timestamp": "2026-01-20T18:01:31.188376",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-14T14:23:42Z",
    "created_at": "2026-01-20T17:49:50.238166",
    "updated_at": "2026-01-20T17:49:50.238176"
}