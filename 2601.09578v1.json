{
    "id": "2601.09578v1",
    "title": "Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping",
    "authors": [
        "Jiajun Sun",
        "Yangyi Ou",
        "Haoyuan Zheng",
        "Chao yang",
        "Yue Ma"
    ],
    "abstract": "在复杂环境中，自主机器人导航与环境感知对SLAM技术提出了更高要求。本文提出一种融合热信息增强三维点云地图语义的新方法。通过先对可见光与红外图像进行像素级融合，将实时激光雷达点云投影至该融合图像流，随后在热通道中分割热源特征以即时识别高温目标，并将温度信息作为语义层应用于最终的三维地图。该方法生成的地图不仅具有精确的几何结构，更具备对环境的关键语义理解，在灾害快速评估、工业预防性维护等特定应用场景中具有重要价值。",
    "url": "https://arxiv.org/abs/2601.09578v1",
    "html_url": "https://arxiv.org/html/2601.09578v1",
    "html_content": "MULTIMODAL SIGNAL PROCESSING FOR THERMAL-VISIBLE-LIDAR FUSION IN\nREAL-TIME 3D SEMANTIC MAPPING\nJiajun Sun\n1\n*, Yangyi Ou\n1\n*, Haoyuan Zheng\n2\n, Chao Yang\n2\n, Yue Ma\n2\n†\n\\dagger\n1\nCollege of Mechatronics and Control Engineering, Shenzhen University, China, 518060\n2\nSchool of Robotics, Xi’an-Jiaotong Liverpool University, China, 215123\nAbstract\nIn complex environments, autonomous robot navigation\nand environmental perception pose higher requirements for\nSLAM technology. This paper presents a novel method for\nsemantically enhancing 3D point cloud maps with thermal\ninformation. By first performing pixel-level fusion of visible\nand infrared images, the system projects real-time LiDAR\npoint clouds onto this fused image stream. It then segments\nheat source features in the thermal channel to instantly identify high temperature targets and applies this temperature information as a semantic layer on the final 3D map. This approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment, making it highly valuable for specific applications like rapid disaster assessment and industrial preventive maintenance.\n1\n1\nfootnotetext:\nThese authors contributed equally to this work.\n2\n2\nfootnotetext:\nCorresponding author:Yue.Ma02@xjtlu.edu.cn\nIndex Terms\n—Multimodal SLAM, Thermal infrared imaging, Environmental perception, 3D semantic mapping, Multi-sensor fusion, High-temperature target detection\n1\nIntroduction\nTraditional infrared thermal imaging performs defect identification based on surface temperature distribution, but its detection capability for deep structural defects is severely insufficient\n[\n11\n]\n. When internal building defects (such as concrete internal voids, rebar corrosion) have not yet caused significant surface temperature differences, 2D thermal imaging is difficult to detect these hidden dangers. Even advanced active thermal imaging technology (AT), when identifying shallow flat-bottom hole defects inside carbon fiber reinforced polymer (CFRP), performs significantly worse than digital shear\nspeckle technology (DS), which can clearly present subtle\ndamage boundaries that traditional thermal imaging cannot\ndistinguish\n[\n5\n]\n.\nThe detection effectiveness of 2D thermal imaging is\nhighly dependent on environmental conditions, with temperature gradient changes, sunlight intensity, wind speed and other factors significantly affecting detection results\n[\n4\n]\n.\nIn the Miaozhansì Jingang Pagoda stone carving hollow detection project in Yunnan, research found that only during specific time periods (11:00–12:00) could obvious temperature differences exceeding\n7.65\n∘\n7.65^{\\circ}\nC be observed, while temperature differences in other periods were too low, causing blurred hollow boundaries.\nVisible light cameras, although capable of providing high-resolution texture information, have limited performance and can cause image quality degradation in harsh environments such as low light and smoke\n[\n12\n]\n.\nAdditionally, traditional methods struggle to provide precise geometric parameters and mechanical state quantification data for defects.\nFor example, in stone artifact hollow detection, thermal imaging alone cannot obtain key quantification indicators such as hollow area volume (\n6.938\n×\n10\n5\n​\nmm\n3\n6.938\\times 10^{5}\\text{ mm}^{3}\nin the study) and deformation height (maximum\n13.61\n​\nmm\n13.61\\text{ mm}\n)\n[\n13\n]\n.\nTo address the limitations of traditional 2D imaging, this paper proposes a tri-modal fusion framework for 3D thermal entity reconstruction that integrates visual, spatial, and temporal information.\nThe main contributions are:\n(1) a large-scale applicable system combining thermal imaging, localization, and visible light to achieve temperature-texture-geometry joint modeling and 3D semantic point cloud generation;\n(2) a targetless extrinsic calibration method validated against benchmark approaches; and\n(3) field experiments on large structures demonstrating multimodal redundancy for robust performance under sensor failures as shown in Fig.1.\n2\nMethods\nFigure 1:\nSystem Framework Overview\nThe proposed framework uses LiDAR–inertial odometry as the core for high-precision geometry and pose estimation, and introduces a visual–thermal fusion module for real-time semantic enhancement of 3D point clouds.\nLiDAR data are preprocessed with precise timestamps and synchronized with IMU and dual-light cameras;\nIMU pre-integration removes motion-induced distortions, and tightly coupled LiDAR–IMU odometry with scan-to-map matching and octree-based feature extraction achieves accurate state estimation and mapping.\nThermal and visible images are geometrically aligned, fused into composite textures, and projected onto point clouds, producing RGB maps with both geometric accuracy and physical semantics.\n2.1\nSystem Overall Architecture\nThis system integrates three sensors: LiDAR, visible light camera, and thermal infrared camera, constructing a high-precision, multi-information fusion SLAM system. The system architecture is shown in Figure.2.\nFigure 2:\nBuilding 1 (left) and Building 2 (right)\nTable 1:\nSensor Models\nSensor Type\nModel\nRGB Camera\nHikvision MV-CU013A0UC\nThermal IR Camera\nHikvision MV-CI003-GL-N6\nIMU\nNot specified\nLiDAR\nNot specified\nSynchronization\nUnified clock source\n2.2\nMulti-sensor Calibration\n2.2.1. Camera Intrinsic Calibration\nZhang’s calibration method\n[\n14\n]\nis used for intrinsic calibration. The camera projection model is:\n[\nu\nv\n1\n]\n=\n[\nf\nx\n0\nc\nx\n0\nf\ny\nc\ny\n0\n0\n1\n]\n​\n[\nX\nc\nY\nc\nZ\nc\n]\n\\begin{bmatrix}u\\\\\nv\\\\\n1\\end{bmatrix}=\\begin{bmatrix}f_{x}&0&c_{x}\\\\\n0&f_{y}&c_{y}\\\\\n0&0&1\\end{bmatrix}\\begin{bmatrix}X_{c}\\\\\nY_{c}\\\\\nZ_{c}\\end{bmatrix}\n(1)\n2.2.2. LiDAR-Camera Extrinsic Calibration\nExtrinsic calibration is performed by matching LiDAR edges with corresponding image edges. For\nk\nk\n-nearest neighbors\n{\ne\nj\n}\nj\n=\n1\nk\n\\{e_{j}\\}_{j=1}^{k}\n, the covariance matrix is calculated as:\nC\n=\n1\nk\n​\n∑\nj\n=\n1\nk\ne\nj\n​\ne\nj\nT\nC=\\frac{1}{k}\\sum_{j=1}^{k}e_{j}e_{j}^{T}\n(2)\nConsidering measurement noise, the relationship between true point position and measured value is:\np\ni\n=\nd\ni\n⋅\nr\ni\n=\n(\nd\n^\ni\n+\nξ\nd\n)\n⋅\nr\n^\ni\n​\nexp\n⁡\n(\n[\nB\ni\n​\nξ\nr\n]\n×\n)\np_{i}=d_{i}\\cdot r_{i}=(\\hat{d}_{i}+\\xi_{d})\\cdot\\hat{r}_{i}\\exp([B_{i}\\xi_{r}]_{\\times})\n(3)\n2.3\nInfrared-Visible Light Image Fusion\n2.3.1. Adaptive Thermal Region Detection\nAn adaptive threshold strategy is adopted, with the dynamic threshold calculated as:\nT\na\n​\nd\n​\na\n​\np\n​\nt\n​\ni\n​\nv\n​\ne\n=\nclip\n​\n(\nμ\n+\nk\n​\nσ\n−\nT\nm\n​\ni\n​\nn\nT\nm\n​\na\n​\nx\n−\nT\nm\n​\ni\n​\nn\n,\n0\n,\n1\n)\nT_{adaptive}=\\text{clip}\\left(\\frac{\\mu+k\\sigma-T_{min}}{T_{max}-T_{min}},0,1\\right)\n(4)\nA temporal smoothing strategy is introduced to suppress inter-frame flickering:\nI\ns\n​\nm\n​\no\n​\no\n​\nt\n​\nh\n=\nα\n⋅\nI\nc\n​\nu\n​\nr\n​\nr\n​\ne\n​\nn\n​\nt\n+\n(\n1\n−\nα\n)\n⋅\nI\np\n​\nr\n​\ne\n​\nv\n​\ni\n​\no\n​\nu\n​\ns\nI_{smooth}=\\alpha\\cdot I_{current}+(1-\\alpha)\\cdot I_{previous}\n(5)\nThe final fused image is obtained through weighted superposition:\nI\nf\n​\nu\n​\ns\n​\ne\n​\nd\n=\nw\n⋅\nI\nt\n​\nh\n​\ne\n​\nr\n​\nm\n​\na\n​\nl\n+\n(\n1\n−\nw\n)\n⋅\nI\nv\n​\ni\n​\ns\n​\ni\n​\nb\n​\nl\n​\ne\nI_{fused}=w\\cdot I_{thermal}+(1-w)\\cdot I_{visible}\n(6)\n2.4\nState Estimation and Mapping\n2.4.1. State Vector Definition\nThe system state vector is defined as:\nx\n=\n[\np\n,\nv\n,\nq\n,\nb\na\n,\nb\ng\n,\ng\n]\nT\nx=[p,v,q,b_{a},b_{g},g]^{T}\n(7)\nwhere\np\np\n,\nv\nv\n, and\nq\nq\nrepresent IMU position, velocity, and attitude in the world coordinate system. The IMU kinematics model is:\na\nw\n=\nR\n​\n(\na\nb\n−\nb\na\n−\nn\na\n)\n−\ng\na^{w}=R(a^{b}-b_{a}-n_{a})-g\n(8)\n2.4.2. Voxelized Mapping\nAdaptive voxel grids are used to construct 3D environment maps\n[\n8\n]\n. Measurement residuals are constructed by matching current frame LiDAR points with local planes in the map, and Jacobian matrices with respect to the state vector are calculated for nonlinear optimization.\n3\nExperiments and Discussion\n3.1\nExperimental Setup\nAs shown in Figure.3, the first object is a university sports plaza with a steel-concrete roof that heats and cools rapidly; emissivity differences between roof and curtain walls cause thermal imaging errors, while high thermal mass slows temperature changes. The second object is a campus hall with cracks and stains, where varying solar angles lead to uneven thermal distribution, limiting single-frame infrared imaging for capturing dynamic evolution.\nFigure 3:\n3D spatial optical-temperature multimodal model of sports plaza. 9:26 (top left), 11:46 (top right), 14:12 (down left), 18:34 (down right).\nTable 2 lists detection parameters for two outdoor structures. The sports plaza and cafeteria walls were scanned at 11:44 and 18:26 under cloudy conditions (\n25\n−\n30\n∘\n25-30^{\\circ}\nC) using handheld devices for global modeling. Both mainly consist of concrete, cement, and ceramic tiles with emissivity near 1, so environmental radiation effects were negligible\n[\n9\n]\n.\nItem\nStructure 1\nStructure 2\nStructure Type\nSports Plaza\nCafeteria\nLocation\nUniv. Campus\nUniv. Campus\nDetection Time\n9:00-22:10\n9:00-22:10\nDetection Area\nFront Face\nEntire Building\nWeather\nClear\nClear\nTemp. Range\n25\n−\n33\n∘\n25-33^{\\circ}\nC\n26\n−\n30\n∘\n26-30^{\\circ}\nC\nTable 2:\nDetection Parameters\n3.2\nSports Plaza Construction Results\nWe collected model data at different times of day, including 11:46 and 18:34, showing that the system can capture dynamic thermal evolution and optical textures of outdoor structures in real time. Clear spatial temperature differences were observed—for example, the sports plaza’s shaded right side remained cooler than its sunlit center—demonstrating the strong influence of environmental factors like vegetation, which traditional fixed-angle 2D thermal imaging cannot effectively contextualize\n[\n3\n]\n.\nFigure 4:\n3D spatial optical-temperature multimodal model of Cafeteria. 9:26 (top left), 11:46 (top right), 14:12 (down left), 18:34 (down right).\nThe sequence reveals daily thermal cycles, with rising temperatures peaking around noon and cooling after sunset. At 11:46, strong gradients appeared between sunlit and shaded areas, while by 22:10 differences had decreased but residual heat remained, reflecting material response. By fusing thermal and optical data with LiDAR-based 3D models, these patterns can be spatially localized to structural features (e.g., concrete pillars), enabling precise assessment of thermal anomalies and overcoming the limits of traditional 2D imaging for complex or inaccessible areas\n[\n6\n]\n.\nFigure 5:\nSports Plaza Model Cross-Sectional View time:11:46\n3.3\nBuilding Model Construction Results\nBeyond fixed-orientation monitoring results of the sports plaza, Figure\n4\nshows the system-generated university building 3D optical-thermal multimodal model under the same daily cycle (11:46 and 18:34) as the overpass in Figure\n3\n. Results reveal complex spatiotemporal thermal patterns on large building facades under different solar radiation conditions.These models reveal significant temperature difference characteristics between different structural components and materials, such as stark thermal contrast between brown glass doors and surrounding concrete walls.\nFigure 6:\n3D spatial optical-temperature multimodal model of\nCafeteria. 9;26(top left) 11:46 (top right) 14:12(down left)\n18:34 (down right)\nLeveraging fine optical textures, the system accurately locates defects such as cracks, seepage, and stains, while long-term humid areas consistently show lower temperatures that can be mapped onto 3D models. Temporal results confirm dynamic thermal responses under solar loads, with noon heating, lagged peaks due to thermal inertia, and evening cooling\n[\n10\n]\n.\nCompared to traditional 3D thermal imaging\n[\n7\n]\n, integrating thermal data with optical edges addresses dynamic shadows and non-uniform fields, reducing misjudgment in time-varying environments. The Thermal-LIO system further enables precise spatiotemporal localization of anomalies (e.g., persistent low-temperature zones), supporting quantitative assessment and targeted maintenance.\n3.4\nComparative Analysis\nThis study validated a multi-perspective thermal diagnostic method that overcomes limitations of single-view imaging by fusing LiDAR geometry with multi-angle visible and thermal data into a unified ”Photo-Thermal” 3D model. In a case study, a persistent low-temperature anomaly showed consistent 3D morphology across all views, confirming it as a real defect caused by seepage mud deposition rather than a geometric artifact.\nFigure 7:\nCafeteria Model Cross-Sectional and Multi\nPerspective View time:11:46\n4\nDiscussion\nThis paper presents a multimodal fusion SLAM framework that integrates LiDAR, IMU, and visible-infrared cameras for high-precision mapping and semantic enhancement. Through pixel-level fusion of visible and thermal images and LiDAR projection, the system identifies heat sources and generates 3D maps enriched with geometry, texture, and temperature semantics.\nExperiments show stable real-time performance, enabling precise localization of thermal anomalies in 3D space and significantly improving defect diagnosis compared to existing methods. With multimodal redundancy ensuring robustness, the framework offers strong potential for disaster assessment and industrial maintenance. Future work will extend to more complex environments, machine learning-based defect classification, and large-scale infrastructure monitoring.\nTable 3:\nComparative Analysis of Key Metrics with Existing Methods\nMetric\nOur Method\nShin & Kim\n[\n7\n]\nChen et al.\n[\n1\n]\nDe Pazzi et al.\n[\n2\n]\nFrame Rate (FPS)\n>\n20\n>20\n<\n5\n<5\n8\n​\n-\n​\n12\n8\\text{-}12\n<\n3\n<3\nGeometric Accuracy\nmm-level\ncm-level\nmm-level\ncm-level\nDetection Range\n>\n50\n>50\nm\n<\n10\n<10\nm\n20\n​\n-\n​\n30\n20\\text{-}30\nm\n<\n15\n<15\nm\nDefect Detection\nCapability\nCracks, seepage,\npeeling, biological\nattachment\nStructure\ncombination only\nStructure\n+ edge\nThermal\nappearance only\n3D Semantic Mapping\nFull support\nLimited\nPartial\nNot supported\nEnvironmental Adaptability\nAll-weather\nLimited\nModerate\nLimited\nData Fusion Accuracy (%)\n94.2\n78.5\n85.7\n76.3\n5\nConclusion\nThis paper introduces a multimodal SLAM framework that combines LiDAR, IMU, and visible-infrared cameras for high-precision 3D mapping with thermal semantics. By fusing visible and thermal imagery and integrating LiDAR data, the system generates detailed maps containing geometric, textural, and temperature information. Experiments confirm robust real-time operation and accurate thermal anomaly detection, greatly improving defect identification.\nReferences\n[1]\nW. Chen, Y. Wang, H. Chen, and Y. Liu\n(2021)\nEIL-SLAM: depth-enhanced edge-based infrared-LiDAR SLAM\n.\nJournal of Field Robotics\n39\n(\n2\n),\npp. 117–130\n.\nCited by:\nTable 3\n.\n[2]\nD. De Pazzi, M. Pertile, and S. Chiodini\n(2022)\n3D radiometric mapping by means of LiDAR SLAM and thermal camera data fusion\n.\nSensors\n22\n(\n21\n),\npp. 8486\n.\nCited by:\nTable 3\n.\n[3]\nA. H. Khalid and K. Kontis\n(2009)\n2D surface thermal imaging using rise-time analysis from laser-induced luminescence phosphor thermometry\n.\nMeasurement Science and Technology\n20\n(\n2\n),\npp. 025305\n.\nCited by:\n§3.2\n.\n[4]\nF. J. Pallarés, M. Betti, G. Bartoli, and L. Pallarés\n(2021)\nStructural health monitoring (shm) and nondestructive testing (ndt) of slender masonry structures: a practical review\n.\nConstruction and Building Materials\n297\n,\npp. 123768\n.\nCited by:\n§1\n.\n[5]\nJ. Queirós, H. Luís, L. Mourão, and V. dos Santos\n(2021)\nInspection of damaged composite structures with active thermography and digital shearography\n.\nJournal of Composite Materials\n.\nCited by:\n§1\n.\n[6]\nV. V. Rondinella and T. Wiss\n(1999)\nMaterials research on inert matrices: a screening study\n.\nJournal of Nuclear Materials\n274\n(\n1–2\n),\npp. 47–53\n.\nCited by:\n§3.2\n.\n[7]\nY.-S. Shin and A. Kim\n(2019)\nSparse depth enhanced direct thermal-infrared slam beyond the visible spectrum\n.\nIEEE Robotics and Automation Letters\n4\n(\n3\n),\npp. 2918–2925\n.\nCited by:\n§3.3\n,\nTable 3\n.\n[8]\nR. Usamentiaga, P. Venegas, J. Guerediaga,\net al.\n(2017)\nInfrared thermography for temperature measurement and non-destructive testing\n.\nApplied Thermal Engineering\n110\n,\npp. 1532–1546\n.\nCited by:\n§2.4.2\n.\n[9]\nB. Vermeersch and G. De Mey\n(2007)\nA fixed-angle heat spreading model for dynamic thermal characterization of rear-cooled substrates\n.\nIn\nProceedings of the 23rd IEEE Semiconductor Thermal Measurement and Management Symposium\n,\npp. 95–101\n.\nCited by:\n§3.1\n.\n[10]\nS. Vidas, P. Moghadam, and S. Sridharan\n(2014)\nReal-time mobile 3d temperature mapping\n.\nIEEE Sensors Journal\n15\n(\n2\n),\npp. 1145–1152\n.\nCited by:\n§3.3\n.\n[11]\nG. Yao, Y. He, and X. Maldague\n(2019)\nInfrared polarization imaging for material characterization: a review\n.\nInfrared Physics & Technology\n100\n,\npp. 1–12\n.\nCited by:\n§1\n.\n[12]\nX. Zhang, Y. Liu, H. Wu, and H. Wang\n(2023)\nNon-invasive infrared thermography technology for thermal comfort: a review\n.\nBuilding and Environment\n,\npp. 111079\n.\nCited by:\n§1\n.\n[13]\nY. Zhang, B. Liu, Z. Cai, H. Guo, and X. He\n(2023)\nQuantifying blistering on vajrasana pagoda using complementary in-situ non-destructive techniques\n.\nHeritage Science\n.\nCited by:\n§1\n.\n[14]\nZ. Zhang\n(2000)\nA flexible new technique for camera calibration\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n22\n(\n11\n),\npp. 1330–1334\n.\nCited by:\n§2.2.1\n.",
    "preview_text": "In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology. This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information. By first performing pixel-level fusion of visible and infrared images, the system projects real-time LiDAR point clouds onto this fused image stream. It then segments heat source features in the thermal channel to instantly identify high temperature targets and applies this temperature information as a semantic layer on the final 3D map. This approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment, making it highly valuable for specific applications like rapid disaster assessment and industrial preventive maintenance.\n\nMULTIMODAL SIGNAL PROCESSING FOR THERMAL-VISIBLE-LIDAR FUSION IN\nREAL-TIME 3D SEMANTIC MAPPING\nJiajun Sun\n1\n*, Yangyi Ou\n1\n*, Haoyuan Zheng\n2\n, Chao Yang\n2\n, Yue Ma\n2\n†\n\\dagger\n1\nCollege of Mechatronics and Control Engineering, Shenzhen University, China, 518060\n2\nSchool of Robotics, Xi’an-Jiaotong Liverpool University, China, 215123\nAbstract\nIn complex environments, autonomous robot navigation\nand environmental perception pose higher requirements for\nSLAM technology. This paper presents a novel method for\nsemantically enhancing 3D point cloud maps with thermal\ninformation. By first performing pixel-level fusion of visible\nand infrared images, the system projects real-time LiDAR\npoint clouds onto this fused image stream. It then segments\nheat source features in the thermal channel to instantly identify high temperature targets and applies this temperature information as a semantic layer on the final 3D map. This approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment, making it highly valuable for specific applications like rapid disaster assessment and industrial preventi",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "multimodal fusion",
        "thermal imaging",
        "LiDAR",
        "3D semantic mapping",
        "SLAM",
        "real-time processing"
    ],
    "one_line_summary": "该论文提出了一种融合热成像、可见光和LiDAR的多模态信号处理方法，用于实时3D语义建图，旨在增强环境感知能力，适用于灾害评估和工业维护等应用。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-14T15:46:57Z",
    "created_at": "2026-01-20T17:49:50.501601",
    "updated_at": "2026-01-20T17:49:50.501611",
    "recommend": 0
}