{
  "id": "2512.01550v1",
  "title": "NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction",
  "authors": [
    "Fei Liu",
    "Shichao Xie",
    "Minghua Luo",
    "Zedong Chu",
    "Junjun Hu",
    "Xiaolong Wu",
    "Mu Xu"
  ],
  "abstract": "Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.",
  "url": "https://arxiv.org/abs/2512.01550v1",
  "html_url": "https://arxiv.org/html/2512.01550v1",
  "html_content": "NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction\nFei Liu\n∗\nShichao Xie\n∗\nMinghua Luo Zedong Chu\n†\nJunjun Hu Xiaolong Wu\n†\nMu Xu\nAmap, Alibaba Group\n*Joint first authors\n†\n\\dagger\nCorresponding authors\nAbstract\nEmbodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework.\nOur approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM’s structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.\nI\nIntroduction\nEmbodied navigation, a cornerstone challenge in artificial intelligence, has recently witnessed remarkable progress driven by the advent of Vision-Language Models (VLMs)\n[\n1\n,\n2\n,\n3\n,\n4\n,\n5\n]\n. These models endow agents with the ability to perceive, interpret instructions, and operate in complex environments. Despite these advances, a significant performance gap persists in long-horizon tasks, where agents frequently fail to maintain course, comprehend observations, or make consistently correct decisions. This gap stems from two primary limitations: (1) a planning and memory deficit, as deployable VLMs often have limited context windows and planning capabilities, causing them to get ”lost” in the navigation environment\n[\n6\n,\n7\n,\n8\n]\n; and (2) a lack of predictive foresight, as current models are fundamentally reactive and cannot anticipate future environmental states to guide their actions proactively\n[\n9\n,\n10\n,\n11\n]\n.\nExisting research has pursued these challenges on separate fronts. One trajectory enhances VLM reasoning through curated datasets and Chain-of-Thought (CoT) prompting\n[\n12\n,\n13\n]\n. The other develops world models to predict future states, informing action planning\n[\n14\n,\n15\n]\n. However, a critical oversight is the disconnection between these paradigms. A VLM-centric agent can suffer from semantic hallucinations, where its plan disconnects from visual reality, while a world model without language guidance can experience semantic drift, its predictions becoming untethered from the instructional goal.\nWe posit that VLM planning and predictive foresight should not be separate but unified and mutually reinforcing within a single VLM\n[\n16\n]\n. To this end, we introduce NavForesee as in Figure\nLABEL:fig:navforesee\n, a unified model that integrates multi-modal understanding with world model generation. Our approach is inspired by human navigation, which is not a continuous, low-level process but a hierarchical one centered on milestones. Humans typically navigate by heading towards a sequence of meaningful landmarks, largely ignoring the minutiae of the path between them. We argue that an artificial agent should do the same. NavForesee adopts this strategy by operating through two synergistic functions: (1) Hierarchical Language planning. As a planner, NavForesee generates a high-level plan by summarizing the navigation task into completed sub-instructions, identifying the current sub-instruction, and formulating the next steps as semantic action ”trunks.” This grounds the agent’s planning in the overall instruction. (2) Dual-Horizon Predictive Foresight. As a world model, NavForesee ”imagines” the future on two timescales. For long-term guidance, it predicts the key visual features of the environment at the completion of the current sub-instruction—effectively envisioning the next milestone. For short-term execution, it forecasts immediate future features to enhance local awareness, enabling robust obstacle avoidance and better understanding of environmental dynamics.\nInspired by latent-space world models\n[\n17\n,\n18\n,\n19\n,\n15\n]\n, this prediction deliberately avoids computationally expensive pixel-level generation. Instead, NavForesee forecasts a compact set of high-level features—depth, DINOv2, and SAM features—that capture essential geometric and semantic information as in DreamVLA. The predicted features are fed to an action policy module which is simply an MLP to generate continuous waypoints and flags for arriving or not. By tightly coupling hierarchical planning with dual-horizon predictive foresight, NavForesee generates coherent, goal-oriented actions, guided by both a long-term vision of its milestones and an immediate awareness of its surroundings.\nWe conducted extensive experiments on the R2R-CE\n[\n20\n]\nand RxR-CE\n[\n21\n]\nbenchmarks. Training exclusively on the publicly available R2R-CE and RxR-CE datasets, NavForesee demonstrates highly competitive performance, achieving a Success Rate (SR) of 66.2% and an Oracle Success Rate (OSR) of 78.4% on the R2R-CE benchmark—comparable to state-of-the-art methods.\nIn summary, our key contributions are threefold:\n•\nWe propose NavForesee, a VLN framework that unifies vision–language model (VLM) planning with world model prediction for navigation tasks.\n•\nWe introduce a hierarchical language planning paradigm that addresses long-instruction, goal-oriented missions by explicitly tracking mission progress and generating concise textual sub-plans.\n•\nWe design a dual-horizon world model prediction mechanism for both short-term execution and long-term milestone navigation, implicitly forming a perception–planning and prediction–action loop that guides agent behavior.\nII\nRelated Works\nII-A\nVisual Language Navigation\nVision-and-Language Navigation (VLN) requires an embodied agent to interpret natural language instructions, perceive visual surroundings, and generate a sequence of actions to reach a specified goal. The advent of large-scale pre-trained VLMs has catalyzed significant progress, largely superseding earlier methods based on topological graphs\n[\n22\n,\n23\n,\n24\n]\n, top-down semantic maps\n[\n25\n,\n26\n,\n27\n]\n, or instruction augmentation\n[\n28\n]\n. Recent works leveraging VLMs can be broadly categorized into two main paradigms.\nThe first uses the VLM as a high-level planner, auto-regressively generating action plans\n[\n29\n,\n30\n,\n31\n]\nor textual trajectories\n[\n32\n]\n. While strong in reasoning, this step-by-step generation is prone to error accumulation and slow inference. The second employs the VLM as an end-to-end policy, directly mapping inputs to actions. However, this often leads to overfitting on training scenes and underutilizes the VLM’s high-level reasoning capabilities.\nTo bridge the gap between these two approaches, dual-system architectures have been proposed\n[\n6\n,\n33\n]\n. These models often adopt a ”Fast-and-Slow” reasoning paradigm, combining a deliberative ”slow” system for high-level reasoning with a lightweight ”fast” reactive controller for low-level execution. Reinforcement learning is frequently employed to align the outputs of both systems and bootstrap the learning of coherent reasoning-action patterns. Despite this progress, a fundamental challenge remains: long, complex reasoning chains (e.g., long CoTs) do not always align with the spatial and dynamic realities of the environment. Furthermore, frequent or periodic elaborate reasoning processes may be unnecessary, as human navigation often relies on simpler, high-level semantic plans rather than continuous, detailed deliberation.\nII-B\nNavigation World Model\nA world model is designed to learn a predictive model of an environment, forecasting future states from historical observations and optional conditioning information, such as actions or instructions. Predictions can be generated in either raw pixel space or a more compact latent space\n[\n18\n]\n. The concept has gained significant traction recently, propelled by large-scale video generation models like Sora(\n[\n34\n]\n, which can produce long-term, consistent, even interactive video sequences from text prompts. A key application of world models in robotics is to serve as a simulation engine, allowing an agent to ”imagine” the outcomes of different action sequences and evaluate control policies before execution\n[\n16\n,\n19\n]\n.\nIn the context of visual navigation, recent works have begun to leverage world models to provide agents with environmental foresight. For instance, NavMorph utilizes a Recurrent State-Space Model (RSSM) to model environmental dynamics in a compact latent space, refining the agent’s policy with imagined future states\n[\n11\n]\n. Similarly, HNR\n[\n9\n]\nadvocates for predicting multi-level semantic features instead of raw pixels, enabling faster and higher-quality imagination to evaluate multiple next-step actions in parallel. Other approaches, like NWM\n[\n35\n]\n, use a controlled video generation model to plan entire trajectories through simulation.\nDespite their promise, existing world models for navigation face two primary limitations. First, action-conditioned models that rely on extensive trajectory sampling and evaluation are often computationally prohibitive, rendering them infeasible for deployment on resource-constrained agents. Second, and more critically for our work, prior research has focused almost exclusively on learning environmental dynamics, largely neglecting to integrate this predictive capability with the high-level language reasoning abilities of modern VLMs. This separation leaves a critical gap, which our work aims to address by unifying these two powerful paradigms.\nFigure 1\n:\nVLM-driven hierarchical navigation plan dataset generation. Episodes from R2R-CE and RxR-CE are processed by\nGemini 2.5 Pro\n, which decomposes long instructions into sub-instructions and identifies keyframe milestones. To generation of waypoint-level reasoning labels, waypoints are sampled between milestones annotated with a navigation summary, future plan, and action (\nforward\n,\nleft\n,\nright\n, or\nstop\n).\nIII\nMethods\nFigure 2\n:\nOverall architecture of NavForesee. The model is built on the Qwen2.5-VL-3B-Instruct backbone, integrating two complementary functionalities: (1) VLM-based hierarchical planning and (2) world model-based dual-horizon visual prediction. For hierarchical planning, textual instruction and visual observations are encoded via Qwen’s original multimodal encoders to produce auto-regressive sub-goal plans. For prediction, a position encoder encodes the agent’s relative pose, and short- and long-horizon dream queries (depth and semantic subqueries) are appended to multimodal embeddings. These queries, processed through structured attention, feed lightweight convolutional decoders for environmental predictions and an MLP head for navigation actions.\nIII-A\nProblem Formulation\nWe target instruction-guided navigation missions in which an embodied agent must interpret a natural language instruction\nl\nl\nand navigate from a given start position to an intended goal location, strictly following the described route. The challenge lies in robustly understanding the instruction, maintaining situational awareness over long horizons, and deciding actions that lead to successful navigation in unseen environments.\nAt time step\nt\nt\n, the agent perceives the environment and obtains a panoramic RGB observation\no\nt\no_{t}\n. It maintains a memory of the past\nH\nH\nobservations,\nO\nt\n−\nH\n:\nt\n−\n1\n=\n[\no\nt\n−\nH\n,\n…\n,\no\nt\n−\n1\n]\nO_{t-H:t-1}=[o_{t-H},\\dots,o_{t-1}]\n, to support temporal reasoning. The navigation policy produces a sequence of\nK\nK\nfuture waypoints\nw\nt\n:\nt\n+\nK\n∈\nℝ\nK\n×\n5\nw_{t:t+K}\\in\\mathbb{R}^{K\\times 5}\n, where each waypoint is defined as\nw\nt\n=\n[\nx\nt\n,\ny\nt\n,\nsin\n⁡\nθ\nt\n,\ncos\n⁡\nθ\nt\n,\nc\nt\n]\n,\nw_{t}=\\left[x_{t},y_{t},\\sin\\theta_{t},\\cos\\theta_{t},c_{t}\\right],\nwith\n(\nx\nt\n,\ny\nt\n)\n(x_{t},y_{t})\ndenoting planar positions,\nθ\nt\n\\theta_{t}\nthe heading angle, and the binary flag\nc\nt\nc_{t}\nindicating whether a\nstop\naction should be triggered. Unless all predicted actions are marked as\nstop\n, the agent continuously moves following the generated waypoints.\nTo solve this problem, we adopt Qwen2.5-VL as our backbone and extend it with two complementary modules. First, we enable\nhierarchical planning\nby decomposing the full instruction into sequential sub-instructions, identifying completed ones and predicting the next step under the current context—leveraging the model’s language understanding capabilities and pretraining on our constructed dataset. Second, we integrate\nworld model foresight\nfor predicting short- and long-term environmental changes, enhancing vision–language coherence and yielding more reliable action policies. Together, these capabilities allow the agent to imitate human navigation behaviors, combining explicit language planning with implicit spatiotemporal prediction.\nIII-B\nVLM-driven Hierarchical Planning Dataset\nWe construct a hierarchical language planning dataset specifically for instruction-guided navigation missions, leveraging advanced Vision–Language Models (VLMs) for multi-modal understanding and sequence analysis. Our goal is to provide training data that captures both short-term execution steps and long-term navigation milestones.\nAs illustrated in Figure\n1\n, we start from public Vision-and-Language Navigation (VLN) benchmarks—R2R-CE (10k episodes) and RxR-CE (20k episodes)—which provide paired natural language instructions and full image observation sequences. Each episode is processed with\nGemini 2.5 Pro\n, guided by a custom prompt template that specifies the model’s role, defines the mission, outlines analytical steps, and enforces an explicit output format. The VLM systematically decomposes each long instruction into a series of sequential sub-instructions, while identifying a dense visual chain of keyframes representing navigation milestones. For paths involving extended travel or sharp turns, we require the inclusion of intermediate milestones to maintain visual continuity in the generated plan. This hierarchical structure enables downstream world models to better learn both short-term and long-term prediction.\nFor every annotated episode, the output is standardized to include: the milestone frame index, the textual description of the completed sub-instruction, and the upcoming planned instruction. Post-processing involves filtering incomplete annotations, correcting logical inconsistencies in the VLM outputs, and converting each episode into multiple navigation segments. We sample waypoints along each trajectory, with each waypoint forming the endpoint of a segment between milestones. Each sampled waypoint is assigned a\nplanning label\ncomprising: (1) a navigation summary (completed sub-instruction), (2) a future plan (next instruction), and (3) a language action (\nforward\n,\nleft\n,\nright\n,\nstop\n).\nThis pipeline produces approximately\n1.3\n1.3\nM training samples from RxR-CE and\n0.2\n0.2\nM from R2R-CE. To ensure balanced training data, we down-sample over-represented straight-motion cases and augment examples involving stopping actions. The final dataset provides richly annotated, balanced samples for training the hierarchical language planning and predictive modules in NavForesee.\nIII-C\nModel Architecture\nOverall Architecture\nThe overall architecture of NavForesee is illustrated in Figure\n2\n. We adopt Qwen2.5-VL-3B-Instruct\n[\n36\n]\n, a large-scale vision–language model with strong multi-modal understanding capabilities, as the backbone. NavForesee is designed to integrate two complementary functionalities: VLM-based language planning and World model-based visual prediction.\nCorrespondingly, we define two primary training objectives: VLM planning training and world model training. Training data from both tasks are jointly mixed to ensure that the model preserves its multi-modal planning ability, while simultaneously extending its capability to generate visual features.\nFor the VLM planning training, textual planning data are directly fed into Qwen for auto-regressive training, leveraging its original text encoder and image encoder components without modification.\nFor the world model training, we introduce an additional position encoder (pos-encoder) to encode the agent’s relative position and orientation from image observations. Two sets of dream queries—corresponding to short- and long-horizon predictions—are appended to the multi-modal embeddings. Each set of dream queries includes depth and semantics subqueries, enabling dual-horizon prediction. Furthermore, an action query, alongside the dream queries, is integrated into the multi-modal inputs and processed by Qwen2.5-VL via a structured attention mechanism. Lightweight convolutional layers serve as decoders to transform dream embeddings into environmental predictions (depth and semantics), while a simple MLP predicts action outputs (waypoints, orientation estimates, and arrival flags).\nStructured Attention Mask\nTo maintain a clear separation between short- and long-horizon predictions and to avoid cross-type contamination, each dream query type (depth and semantics) is explicitly decomposed into short-horizon and long-horizon components. As illustrated in Figure\n2\n, we design a structured attention mask tailored for dual-horizon prediction. Long-horizon predictions naturally depend on short-horizon predictions, using them as guidance to ensure temporal coherence. Mutual attention between depth and semantics queries is masked to prevent cross-modal leakage or unintended feature mixing. In contrast, the action query attends to all available information—including past context and both horizons of dream queries—enabling it to make globally consistent navigation predictions.\nIII-D\nDual-horizon World Model Prediction\nSpecifically, to enable accurate dual-horizon environmental feature prediction, we employ the world model architecture that serves as guidance for learning the inverse dynamics of a navigation agent. Here, short-term prediction refers to generating forecasts for\nk\nk\nsteps ahead, while long-term prediction targets navigation milestones, corresponding to an adaptive horizon determined by progress toward the next milestone.\nFor visual feature prediction within Qwen2.5-VL, we introduce two sets of learnable dream queries, namely the short-term\nQ\nS\n∈\nℝ\nL\n×\nd\nQ_{S}\\in\\mathbb{R}^{L\\times d}\nand and long-term\nQ\nL\n∈\nℝ\nL\n×\nd\nQ_{L}\\in\\mathbb{R}^{L\\times d}\nto, which extract temporally aligned feature embeddings specialized for prediction at distinct horizons. To enhance the model’s capability in capturing spatial-temporal correlations and learning environmental dynamics, we further integrate position-orientation state embeddings\ns\nt\n−\nH\n:\nt\ns_{t-H:t}\nfor each input frame through an encoder\nh\n(\n.\n)\nh(.)\n.\nThese dream queries are concatenated with textual instruction embeddings\nl\nl\nand visual observation sequences\nO\nt\n−\nH\n:\nt\nO_{t-H:t}\nand processed by the Qwen2.5-VL backbone\nf\n(\n.\n)\nf(.)\n. Specially,\nE\nS\n=\nf\n​\n(\nl\n,\nO\nt\n−\nH\n:\nt\n,\nh\n​\n(\ns\nt\n−\nH\n:\nt\n)\n|\nQ\nS\n)\n,\n\\displaystyle E_{S}=f(l,O_{t-H:t},h(s_{t-H:t})|Q_{S}),\nE\nL\n=\nf\n​\n(\nl\n,\nO\nt\n−\nH\n:\nt\n,\nh\n​\n(\ns\nt\n−\nH\n:\nt\n)\n,\nQ\nS\n|\nQ\nL\n)\n\\displaystyle E_{L}=f(l,O_{t-H:t},h(s_{t-H:t}),Q_{S}|Q_{L})\nwhere causal attention masking ensures auto-regressive generation: short-term embeddings are produced first, and long-term embeddings are conditioned on short-term predictions.\nWe design lightweight decoders to interpret\nE\nL\nE_{L}\nand\nE\nS\nE_{S}\ninto predicted depth\nd\np\nd_{p}\n, and high-level semantics\nc\np\nc_{p}\n(e.g. derived from DINOV2, SAM). Short-term predictions correspond to a fixed horizon\nk\nk\nwhereas long-term predictions adaptively extrapolate over\nM\nt\nM_{t}\nsteps, dependent on the agent’s progress toward the next milestone:\np\nt\n+\nk\n=\nD\n​\n(\nE\nS\n)\n=\n[\nd\np\n​\n(\nt\n)\n,\nc\np\n​\n(\nt\n)\n]\n,\n\\displaystyle p_{t+k}=D(E_{S})=[d_{p}(t),c_{p}(t)],\np\nt\n+\nM\nt\n=\nD\n​\n(\nE\nL\n)\n=\n[\nd\np\n​\n(\nt\n+\nM\nt\n)\n,\nc\np\n​\n(\nt\n+\nM\nt\n)\n]\n\\displaystyle p_{t+M_{t}}=D(E_{L})=[d_{p}(t+M_{t}),c_{p}(t+M_{t})]\nIII-E\nPredictive Action Policy Learning\nGiven two temporally order states\no\nt\no_{t}\nand\no\nt\n+\n1\no_{t+1}\n, the intermediate action\na\n^\n​\n(\nt\n)\n\\hat{a}(t)\ncan be inferred via inverse dynamics. We leverage this principle to learn an action policy conditioned on the instruction\nl\nl\n, historical visual observations\nO\nt\n−\nH\n:\nt\nO_{t-H:t}\nand the dual-horizon predictive latent features\nE\nS\nE_{S}\nand\nE\nL\nE_{L}\ngenerated by the world model.\nTo enhance the encoding of task-relevant context for action prediction, we introduce a learnable action query\nQ\na\nQ_{a}\n. This query is concatenated with the dream queries and multi-modal input embeddings to form a unified action embedding. The Qwen2.5-VL backbone processes these embeddings to produce the contextual representation for action inference, which is subsequently projected into the action space:\nE\na\n=\nf\n​\n(\nl\n,\nO\nt\n−\nH\n:\nt\n,\nh\n​\n(\ns\nt\n−\nH\n:\nt\n)\n,\nQ\nS\n,\nQ\nL\n|\nQ\na\n)\n\\displaystyle E_{a}=f(l,O_{t-H:t},h(s_{t-H:t}),Q_{S},Q_{L}|Q_{a})\na\n^\nt\n:\nt\n+\nk\n=\nM\ni\n​\nn\n​\nv\n​\n(\nE\nS\n,\nE\nL\n|\nE\na\n)\n\\displaystyle\\hat{a}_{t:t+k}=M_{inv}(E_{S},E_{L}|E_{a})\nwhere\nE\na\nE_{a}\nis the action embedding and\nM\ni\n​\nn\n​\nv\nM_{i}nv\ndenotes the inverse dynamics model.\nNotably, in our action policy learning pipeline, the action embedding\nE\na\nE_{a}\nis extracted through the Qwen2.5-VL backbone, while action predictions are primarily conditioned on the dual-horizon predictive features, ensuring that decisions are informed by both past observations and forecasted environmental dynamics\nIII-F\nClose the Planning/Prediction and Action Loop\nFor VLM planning training, we finetune Qwen2.5-VL model based on the constructed dataset in an auto-regressive manner independently to build a powerful model capable of conducting hierarchical navigation.\nFor world model prediction and action policy learning, the training tasks are classified depth prediction, semantic feature prediction and action prediction. Depth prediction error\nL\nd\nL_{d}\nis measured using the Scale-invariant Logarithmic Loss (SiLogLoss) at the pix-level level. The semantics feature prediction error\nL\nc\nL_{c}\nand action error\nL\na\nL_{a}\nare computed using mean squared error (MSE). The overall training loss\nL\nL\ncomprise\nL\nd\nL_{d}\n,\nL\nc\nL_{c}\nand\nL\na\nL_{a}\nL\n=\nα\n​\nL\nd\n+\nβ\n​\nL\nc\n+\nL\na\nL=\\alpha L_{d}+\\beta L_{c}+L_{a}\nwhere\nα\n\\alpha\nand\nβ\n\\beta\nare weighting hyperparameters balancing the tasks.\nIV\nExperimental Evaluation\nTABLE I:\nComparison with other methods on the Val-Unseen split of R2R-CE and RxR-CE\nMethod\nObservation\nR2R-CE Val-Unseen\nRxR-CE Val-Unseen\nS.RGB\nPano.\nDepth\nOdo\nNE\n↓\n\\downarrow\nOS\n↑\n\\uparrow\nSR\n↑\n\\uparrow\nSPL\n↑\n\\uparrow\nNE\n↓\n\\downarrow\nSR\n↑\n\\uparrow\nSPL\n↑\n\\uparrow\nHPN+DN*\n[\n37\n]\n✓\n✓\n✓\n6.31\n40.0\n36.0\n34.0\n-\n-\n-\nCMA*\n[\n38\n]\n✓\n✓\n✓\n6.20\n52.0\n41.0\n36.0\n8.76\n26.5\n22.1\nSim2Sim\n[\n39\n]\n✓\n✓\n✓\n6.07\n52.0\n43.0\n36.0\n8.76\n26.5\n22.1\nGridMM*\n[\n8\n]\n✓\n✓\n✓\n5.11\n61.0\n49.0\n41.0\n-\n-\n-\nDreamWalker*\n[\n40\n]\n✓\n✓\n✓\n5.53\n59.0\n49.0\n44.0\n-\n-\n-\nReborn*\n[\n41\n]\n✓\n✓\n✓\n5.40\n57.0\n50.0\n46.0\n5.98\n48.6\n42.0\nETPNav*\n[\n42\n]\n✓\n✓\n✓\n4.71\n65.0\n57.0\n49.0\n5.64\n54.7\n44.8\nHNR*\n[\n9\n]\n✓\n✓\n✓\n4.42\n67.0\n61.0\n51.0\n5.50\n56.3\n46.7\nAG-CMTP\n[\n43\n]\n✓\n✓\n✓\n7.90\n39.0\n23.0\n19.0\n-\n-\n-\nR2R-CMTP\n[\n43\n]\n✓\n✓\n✓\n7.90\n38.0\n26.0\n22.0\n-\n-\n-\nInstruc-Nav\n[\n31\n]\n✓\n✓\n✓\n6.89\n-\n31.0\n24.0\n-\n-\n-\nLAW\n[\n44\n]\n✓\n✓\n✓\n6.83\n44.0\n35.0\n31.0\n10.90\n8.0\n8.0\nCM2\n[\n45\n]\n✓\n✓\n✓\n7.02\n41.0\n34.0\n27.0\n-\n-\n-\nWS-MGMap\n[\n46\n]\n✓\n✓\n✓\n6.28\n47.0\n38.0\n34.0\n-\n-\n-\nAO-Planner\n[\n47\n]\n✓\n✓\n5.55\n59.0\n47.0\n33.0\n-\n-\n-\nSeq2Seq\n[\n48\n]\n✓\n✓\n7.77\n37.0\n25.0\n22.0\n12.10\n13.9\n11.9\nCMA\n[\n48\n]\n✓\n✓\n7.37\n40.0\n32.0\n30.0\n-\n-\n-\nNA Vid\n[\n49\n]\n✓\n5.47\n49.0\n37.0\n35.0\n-\n-\n-\nUni-NA Vid\n[\n50\n]\n✓\n5.58\n53.5\n47.0\n42.7\n6.24\n48.7\n40.9\nNaVILA\n[\n51\n]\n✓\n5.22\n62.5\n54.0\n49.0\n6.77\n49.3\n44.0\nStream VLN\n[\n52\n]\n✓\n4.98\n64.2\n56.9\n51.9\n6.22\n52.9\n46.0\nCorrectNav\n[\n53\n]\n✓\n4.24\n67.5\n65.1\n62.3\n4.09\n69.3\n63.3\nNavForesee(Ours)\n✓\n3.94\n78.4\n66.2\n59.7\n4.20\n66.3\n53.2\nTABLE II:\nPerformance comparison between VLM planning and dual-horizon world model prediction\nIndex\nVLM planning\nLong-term prediction\nShort-term prediction\nSR\n↑\n\\uparrow\nOSR\n↑\n\\uparrow\nNE\n↓\n\\downarrow\nSPL\n↑\n\\uparrow\n1\n✓\n✓\n✓\n66.2%\n78.4%\n3.94\n59.7%\n2\n✗\n✓\n✓\n48.8%\n75.5%\n5.61\n39.4%\n3\n✓\n✗\n✓\n58.6%\n76.4%\n4.47\n50.1%\n4\n✗\n✗\n✗\n52.6%\n67.4%\n5.53\n46.7%\nWe evaluate our model in continuous environment of the Habitant simulator on the R2R-CE and RxR-CE datasets.\nR2R-CE\ndataset is derived from the Matterport3D indoor environments, discretized for path planning but operationalized in the Habitat simulator under a continuous navigation setting. It provides fine-grained, step-by-step natural language instructions, allowing for detailed guidance at each navigation step. In the simulator, the embodied agent\ncan execute turns as small as\n15\n∘\n15^{\\circ}\nand perceives the scene through a\n90\n∘\n90^{\\circ}\nhorizontal field-of-view.\nRxR-CE\nis a large-scale, multilingual VLN dataset comprising about 126K human-annotated instructions. Compared to R2R-CE, RxR-CE covers more diverse and complex trajectories, increasing the difficulty of the navigation tasks. The agent in this setting uses a coarser minimum turn\nincrement of\n30\n∘\n30^{\\circ}\nand a narrower\n79\n∘\n79^{\\circ}\nhorizontal field-ofview, which demands more deliberate movement planning for effective scene coverage.\nWe evaluate navigation performance using standard metrics including success rate (SR), oracle success rate (OS), success weighted by path length (SPL), and navigation error (NE).\nIV-A\nComparison with State-of-the-Art Methods\nTable 1 reports the performance of the proposed method compared with other approaches on the R2R‑CE and RxR‑CE datasets. Overall, NavForesee delivers competitive results against state‑of‑the‑art (SOTA) methods.\nSpecifically, on the val unseen split of the R2R‑CE dataset, NavForesee achieves SOTA performance by improving SR by 1.1%, OSR by 10.9%, and reducing NE by 0.3 m. This improvement can be attributed to the integration of the world model prediction module, which enables the agent to better capture environmental dynamics, avoid obstacles, and explore the surroundings more effectively.\nIn contrast, NavForesee performs slightly worse than SOTA methods on RxR‑CE, indicating limited generalization to more complex environments. It is worth noting that we train soly on NavForesee on R2R‑CE and RxR‑CE datasets, whereas other methods exploit diverse and large‑scale datasets to enhance generalization.\nAlthough NavForesee does not consistently outperform all baselines, it achieves the highest OSR across both datasets. This demonstrates the value of incorporating world model prediction into VLN agents and implies its promising potential for future vision‑and‑language navigation tasks.\nIV-B\nAblation Study\nAs shown in Table\nII\n, removing any of the three key modules—hierarchical VLM planning, long-term prediction, or short-term prediction—results in clear performance degradation. The full NavForesee model, which combines all modules, achieves the highest SR (66.2%), OSR (78.4%), lowest NE (3.94), and best SPL (59.7%), validating the benefit of their integration. Without VLM planning, the success rate drops sharply to 48.8% and the SPL decreases by more than 16 points, reflecting the importance of explicit instruction decomposition and progress tracking for efficient navigation. Disabling long-term prediction also leads to a noticeable reduction in SR (58.6%) and higher NE, highlighting the role of milestone foresight in providing strategic guidance over extended trajectories. When all three modules are removed, navigation quality deteriorates the most, confirming that planning and both prediction horizons together are crucial for accurate, efficient long-horizon navigation.\nFigure 3\n:\nShort-term depth and semantics predictions. From top to bottom: frames with timestamps, future ground truth frames with timestamps, future depth prediction for future frames, semantics predictions for future frames. Semantic features are DinoV2 features and visualized by a pretrained segmentation head. Instructions: UP the stairs. Turn to the left and enter into the second open door on the left. Walk towards the foot of the bed. Turn right and enter the open door to the bathroom\nFigure 4\n:\nNavForesee’s geometric-semantic feature imagination across different motion modes. The model accurately predicts environmental dynamics in straight motion, generalizes effectively to turning scenarios, and infers detailed object geometry and depth distribution from minimal visual input, such as a brief glimpse into a room\nIV-C\nQualitative Analysis\nFigure\n3\nillustrates the short-term depth and semantic feature predictions generated by our world model over the course of a complete navigation episode, forecasting up to four future steps. Although the predicted depth maps appear somewhat coarse—owing to the constraints of pixel-level supervised training on R2R-CE and RxR-CE—they nonetheless preserve the scene’s global geometry and spatial layout, faithfully capturing agent movements such as ascending or descending staircases, entering and exiting rooms, and making sharp or gradual turns. This ability to retain high-level spatial coherence despite reduced pixel detail ensures that the model’s predictions remain informative for downstream navigation decisions. The semantics predictions, obtained via a pretrained segmentation head, exhibit strong alignment with ground truth labels, successfully reflecting dynamic environmental changes in synchrony with the agent’s actions.\nFigure\n4\nfurther provides detailed examples that showcase NavForesee’s ability to imaginatively anticipate semantic features across diverse motion patterns. In addition to delivering accurate environment dynamics predictions when following a straightforward trajectory, NavForesee demonstrates remarkable generalization by reliably extrapolating future geometric and semantic structures when performing more complex navigational behaviors such as turns. In the final scenario, the agent receives only a brief partial observation—a quick glance into a room—yet the model is able to produce a vivid and coherent internal imagination of the room’s layout. This includes accurately inferring the relative shape and position of the bed, as well as estimating the depth distribution across the room, thus indicating its capacity to reason about unseen spatial regions.\nV\nConclusion\nWe proposed NavForesee, a vision–language navigation framework that unifies hierarchical language planning with dual-horizon predictive world modeling. By decomposing long instructions into sub-goals and anticipating both short-term dynamics and long-term milestones, NavForesee forms an implicit perception–planning and prediction–action loop.\nExperiments on R2R-CE and RxR-CE show strong performance—66.2% SR and 78.4% OSR on R2R-CE—comparable to state-of-the-art despite training only on public data. Qualitative results further reveal solid depth and semantics predictions that guide agent decisions in complex scenarios.\nThese findings highlight the benefit of equipping embodied agents with foresight: by “foreseeing” future states, NavForesee effectively fuses language planning with spatiotemporal imagination to improve visual-language navigation.\nReferences\n[1]\nW.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang,\nY. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing, “Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality,” March\n2023. [Online]. Available:\nhttps://lmsys.org/blog/2023-03-30-vicuna/\n[2]\nS. Chen, X. Chen, C. Zhang, M. Li, G. Yu, H. Fei, H. Zhu, J. Fan, and T. Chen,\n“Ll3da: Visual interactive instruction tuning for omni-3d understanding,\nreasoning, and planning,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2311.18651\n[3]\nA.-M. Halacheva, J.-N. Zaech, X. Wang, D. P. Paudel, and L. V. Gool,\n“Gaussianvlm: Scene-centric 3d vision-language models using language-aligned\ngaussian splats for embodied reasoning and beyond,” 2025. [Online].\nAvailable:\nhttps://arxiv.org/abs/2507.00886\n[4]\nH. Huang, Y. Chen, Z. Wang, R. Huang, R. Xu, T. Wang, L. Liu, X. Cheng,\nY. Zhao, J. Pang, and Z. Zhao, “Chat-scene: Bridging 3d scene and large\nlanguage models with object identifiers,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2312.08168\n[5]\nH. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” 2023.\n[Online]. Available:\nhttps://arxiv.org/abs/2304.08485\n[6]\nQ. Liu, T. Huang, Z. Zhang, and H. Tang, “Nav-r1: Reasoning and navigation in\nembodied scenes,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2509.10884\n[7]\nH. Zhou, J. Yu, and W. Yang, “Dual memory units with uncertainty regulation\nfor weakly supervised video anomaly detection,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2302.05160\n[8]\nZ. Wang, X. Li, J. Yang, Y. Liu, and S. Jiang, “Gridmm: Grid memory map for\nvision-and-language navigation,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2307.12907\n[9]\nZ. Wang, X. Li, J. Yang, Y. Liu, J. Hu, M. Jiang, and S. Jiang, “Lookahead\nexploration with neural radiance representation for continuous\nvision-language navigation,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2404.01943\n[10]\nX. Zhao, W. Cai, L. Tang, and T. Wang, “Imaginenav: Prompting vision-language\nmodels as embodied navigator through scene imagination,” 2024. [Online].\nAvailable:\nhttps://arxiv.org/abs/2410.09874\n[11]\nX. Yao, J. Gao, and C. Xu, “Navmorph: A self-evolving world model for\nvision-and-language navigation in continuous environments,” 2025. [Online].\nAvailable:\nhttps://arxiv.org/abs/2506.23468\n[12]\nB. Lin, Y. Nie, Z. Wei, J. Chen, S. Ma, J. Han, H. Xu, X. Chang, and X. Liang,\n“Navcot: Boosting llm-based vision-and-language navigation via learning\ndisentangled reasoning,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2403.07376\n[13]\nS. Wang, Y. Wang, W. Li, X. Cai, Y. Wang, M. Chen, K. Wang, Z. Su, D. Li, and\nZ. Fan, “Aux-think: Exploring reasoning strategies for data-efficient\nvision-language navigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2505.11886\n[14]\nQ. Zhao, Y. Lu, M. J. Kim, Z. Fu, Z. Zhang, Y. Wu, Z. Li, Q. Ma, S. Han,\nC. Finn, A. Handa, M.-Y. Liu, D. Xiang, G. Wetzstein, and T.-Y. Lin,\n“Cot-vla: Visual chain-of-thought reasoning for vision-language-action\nmodels,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2503.22020\n[15]\nY. Huang, J. Zhang, S. Zou, X. Liu, R. Hu, and K. Xu, “Ladi-wm: A latent\ndiffusion-based world model for predictive manipulation,” 2025. [Online].\nAvailable:\nhttps://arxiv.org/abs/2505.11528\n[16]\nJ. Cen, C. Yu, H. Yuan, Y. Jiang, S. Huang, J. Guo, X. Li, Y. Song, H. Luo,\nF. Wang, D. Zhao, and H. Chen, “Worldvla: Towards autoregressive action\nworld model,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2506.21539\n[17]\nE. Karypidis, I. Kakogeorgiou, S. Gidaris, and N. Komodakis, “Dino-foresight:\nLooking into the future with dino,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2412.11673\n[18]\nF. Baldassarre, M. Szafraniec, B. Terver, V. Khalidov, F. Massa, Y. LeCun,\nP. Labatut, M. Seitzer, and P. Bojanowski, “Back to the features: Dino as a\nfoundation for video world models,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2507.19468\n[19]\nW. Zhang, H. Liu, Z. Qi, Y. Wang, X. Yu, J. Zhang, R. Dong, J. He, F. Lu,\nH. Wang, Z. Zhang, L. Yi, W. Zeng, and X. Jin, “Dreamvla: A\nvision-language-action model dreamed with comprehensive world knowledge,”\n2025. [Online]. Available:\nhttps://arxiv.org/abs/2507.04447\n[20]\nJ. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, “Beyond the\nnav-graph: Vision-and-language navigation in continuous environments,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2004.02857\n[21]\nA. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge, “Room-across-room:\nMultilingual vision-and-language navigation with dense spatiotemporal\ngrounding,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/2010.07954\n[22]\nY. Hong, C. Rodriguez-Opazo, Y. Qi, Q. Wu, and S. Gould, “Language and visual\nentity relationship graph for agent navigation,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/2010.09304\n[23]\nZ. Deng, K. Narasimhan, and O. Russakovsky, “Evolving graphical planner:\nContextual global planning for vision-and-language navigation,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2007.05655\n[24]\nS. Chen, P.-L. Guhur, M. Tapaswi, C. Schmid, and I. Laptev, “Think global, act\nlocal: Dual-scale graph transformer for vision-and-language navigation,”\n2022. [Online]. Available:\nhttps://arxiv.org/abs/2202.11742\n[25]\nM. Z. Irshad, N. C. Mithun, Z. Seymour, H.-P. Chiu, S. Samarasekera, and\nR. Kumar, “Sasra: Semantically-aware spatio-temporal reasoning agent for\nvision-and-language navigation in continuous environments,” 2021. [Online].\nAvailable:\nhttps://arxiv.org/abs/2108.11945\n[26]\nG. Georgakis, K. Schmeckpeper, K. Wanchoo, S. Dan, E. Miltsakaki, D. Roth, and\nK. Daniilidis, “Cross-modal map learning for vision and language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.05137\n[27]\nP. Chen, D. Ji, K. Lin, R. Zeng, T. H. Li, M. Tan, and C. Gan,\n“Weakly-supervised multi-granularity map learning for vision-and-language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2210.07506\n[28]\nS. Zhang, Y. Qiao, Q. Wang, L. Guo, Z. Wei, and J. Liu, “Flexvln: Flexible\nadaptation for diverse vision-and-language navigation tasks,” 2025.\n[Online]. Available:\nhttps://arxiv.org/abs/2503.13966\n[29]\nY. Long, X. Li, W. Cai, and H. Dong, “Discuss before moving: Visual language\nnavigation via multi-expert discussions,” in\n2024 IEEE International\nConference on Robotics and Automation (ICRA)\n, 2024, pp. 17 380–17 387.\n[30]\nP. Chen, X. Sun, H. Zhi, R. Zeng, T. H. Li, G. Liu, M. Tan, and C. Gan,\n“\na\n2\na^{2}\nnav: Action-aware zero-shot robot navigation by exploiting\nvision-and-language ability of foundation models,” 2023. [Online].\nAvailable:\nhttps://arxiv.org/abs/2308.07997\n[31]\nY. Long, W. Cai, H. Wang, G. Zhan, and H. Dong, “Instructnav: Zero-shot system\nfor generic instruction navigation in unexplored environment,” 2024.\n[Online]. Available:\nhttps://arxiv.org/abs/2406.04882\n[32]\nY. Wang, Y. Fang, T. Wang, Y. Feng, Y. Tan, S. Zhang, P. Liu, Y. Ji, and R. Xu,\n“Dreamnav: A trajectory-based imaginative framework for zero-shot\nvision-and-language navigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2509.11197\n[33]\nX. Xue, J. Hu, M. Luo, X. Shichao, J. Chen, Z. Xie, Q. Kuichen, G. Wei, M. Xu,\nand Z. Chu, “Omninav: A unified framework for prospective exploration and\nvisual-language navigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2509.25687\n[34]\nY. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun,\nJ. Gao, L. He, and L. Sun, “Sora: A review on background, technology,\nlimitations, and opportunities of large vision models,” 2024. [Online].\nAvailable:\nhttps://arxiv.org/abs/2402.17177\n[35]\nA. Bar, G. Zhou, D. Tran, T. Darrell, and Y. LeCun, “Navigation world\nmodels,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2412.03572\n[36]\nS. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang,\nJ. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu,\nY. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and\nJ. Lin, “Qwen2.5-vl technical report,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2502.13923\n[37]\nJ. Krantz, A. Gokaslan, D. Batra, S. Lee, and O. Maksymets, “Waypoint models\nfor instruction-guided navigation in continuous environments,” 2021.\n[Online]. Available:\nhttps://arxiv.org/abs/2110.02207\n[38]\nY. Hong, Z. Wang, Q. Wu, and S. Gould, “Bridging the gap between learning in\ndiscrete and continuous environments for vision-and-language navigation,”\n2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.02764\n[39]\nJ. Krantz and S. Lee, “Sim-2-sim transfer for vision-and-language navigation\nin continuous environments,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2204.09667\n[40]\nH. Wang, W. Liang, L. V. Gool, and W. Wang, “Dreamwalker: Mental planning for\ncontinuous vision-language navigation,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2308.07498\n[41]\nD. An, Z. Wang, Y. Li, Y. Wang, Y. Hong, Y. Huang, L. Wang, and J. Shao, “1st\nplace solutions for rxr-habitat vision-and-language navigation competition\n(cvpr 2022),” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2206.11610\n[42]\nD. An, H. Wang, W. Wang, Z. Wang, Y. Huang, K. He, and L. Wang, “Etpnav:\nEvolving topological planning for vision-language navigation in continuous\nenvironments,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2304.03047\n[43]\nK. Chen, J. K. Chen, J. Chuang, M. Vázquez, and S. Savarese, “Topological\nplanning with transformers for vision-and-language navigation,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2012.05292\n[44]\nS. Raychaudhuri, S. Wani, S. Patel, U. Jain, and A. X. Chang,\n“Language-aligned waypoint (law) supervision for vision-and-language\nnavigation in continuous environments,” 2021. [Online]. Available:\nhttps://arxiv.org/abs/2109.15207\n[45]\nG. Georgakis, K. Schmeckpeper, K. Wanchoo, S. Dan, E. Miltsakaki, D. Roth, and\nK. Daniilidis, “Cross-modal map learning for vision and language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.05137\n[46]\nP. Chen, D. Ji, K. Lin, R. Zeng, T. H. Li, M. Tan, and C. Gan,\n“Weakly-supervised multi-granularity map learning for vision-and-language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2210.07506\n[47]\nJ. Chen, B. Lin, X. Liu, L. Ma, X. Liang, and K.-Y. K. Wong,\n“Affordances-oriented planning using foundation models for continuous\nvision-language navigation,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2407.05890\n[48]\nJ. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, “Beyond the\nnav-graph: Vision-and-language navigation in continuous environments,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2004.02857\n[49]\nJ. Zhang, K. Wang, R. Xu, G. Zhou, Y. Hong, X. Fang, Q. Wu, Z. Zhang, and\nH. Wang, “Navid: Video-based vlm plans the next step for vision-and-language\nnavigation,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2402.15852\n[50]\nJ. Zhang, K. Wang, S. Wang, M. Li, H. Liu, S. Wei, Z. Wang, Z. Zhang, and\nH. Wang, “Uni-navid: A video-based vision-language-action model for unifying\nembodied navigation tasks,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2412.06224\n[51]\nA.-C. Cheng, Y. Ji, Z. Yang, Z. Gongye, X. Zou, J. Kautz, E. Bıyık, H. Yin,\nS. Liu, and X. Wang, “Navila: Legged robot vision-language-action model for\nnavigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2412.04453\n[52]\nM. Wei, C. Wan, X. Yu, T. Wang, Y. Yang, X. Mao, C. Zhu, W. Cai, H. Wang,\nY. Chen, X. Liu, and J. Pang, “Streamvln: Streaming vision-and-language\nnavigation via slowfast context modeling,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2507.05240\n[53]\nZ. Yu, Y. Long, Z. Yang, C. Zeng, H. Fan, J. Zhang, and H. Dong, “Correctnav:\nSelf-correction flywheel empowers vision-language-action navigation model,”\n2025. [Online]. Available:\nhttps://arxiv.org/abs/2508.10416\nSupplementary Material\nI\nImplementation Details\nI-A\nModel Architecture\nBase Model\nWe employ Qwen2.5-VL-3B-Instruct\n[\n36\n]\nas the backbone of NavForesee. It adopts the Qwen2.5 LLM as its text decoder and integrates a vision encoder. The vision encoder utilizes a Vision Transformer (ViT) architecture to encode visual observations, while the text decoder is responsible for generating the hierarchical planning outputs and action trunk predictions. Detailed descriptions of Qwen2.5-VL can be found in\n[\n36\n]\n. For hierarchical planning, we directly use the original multimodal encoders and text decoder of Qwen2.5-VL without any modifications. For world model prediction and action policy learning, we introduce a position encoder to represent the agent’s relative position and orientation derived from image observations.\nLightweight decoders transform the dream query embeddings into environmental predictions (depth and semantics), while a simple MLP predicts action outputs (waypoints, orientation estimates, and arrival flags).\nDream Query Design\nTwo sets of dream queries (short-term and long-term), along with an action query, are appended to the multimodal embeddings. Each set of dream queries contains depth and semantics subqueries, enabling dual-horizon prediction. We use DINOv2 and SAM features as semantic representations. Thus, there are six query subsets in total—depth, DINOv2, and SAM for both short-term and long-term horizons—with each subset consisting of 64 tokens. The action query consists of a single token dedicated to action prediction.\nWorld Model Decoders\nWe design task-specific lightweight world model decoders to transform dream embeddings into depth maps, semantic features, and actions. For depth and semantics predictions, we employ decoder architectures with identical design: dream embeddings and a set of learnable masks are processed by a 2-layer ViT-based decoder to produce predicted features. Additionally, we apply the decoder from VQ-VAE to render depth features into depth maps.\nAction Prediction\nThe action prediction module takes the action embedding produced by Qwen2.5-VL as input and generates predicted waypoints, orientation estimates, and arrival flags. First, a 2-layer transformer processes the action embedding to capture dependencies on the world model’s dream embeddings. Then, the processed action embedding is passed to the action prediction head, which outputs the final navigation predictions, including waypoints, orientation estimates, and arrival flags. The action prediction head consists of a simple MLP with two linear layers and a ReLU activation in between.\nI-B\nTraining Details\nWe interleave the VLM planning training data and world model training data to jointly train NavForesee. The training batch size is set to 4, and the number of image observations is flexible, up to a maximum length of 20. Depth and semantic features are precomputed and loaded during training. We use the AdamW optimizer with an initial learning rate of\n1\n×\n10\n−\n5\n1\\times 10^{-5}\n. Depth and semantics predictions are weighted with\nα\n=\n0.25\n\\alpha=0.25\nand\nβ\n=\n0.3\n\\beta=0.3\n. The model is trained for a total of 3 epochs on 64 NVIDIA H20 GPUs, with ViT parameters frozen. The fixed short-term prediction horizon is set to 4, and the number of predicted waypoints is set to 5.\nII\nExperimental Evaluations\nII-A\nHierarchical Planning Evaluation\nTo evaluate the hierarchical planning capabilities of NavForesee, we conduct experiments on the Val-Unseen split of the R2R-CE and RxR-CE datasets. An example is illustrated in Figure\n5\n. We perform hierarchical planning for each step of an episode. NavForesee generates a navigation summary, plan, and actions strictly following the output format specified in the prompt template. Apart from the initial position, NavForesee consistently identifies milestones along the route, summarizes completed sub-instructions, and formulates the next sub-instruction in alignment with the overall instruction context. This demonstrates that NavForesee effectively leverages its multimodal understanding capabilities to decompose complex navigation tasks into manageable sub-goals, thereby enabling more structured and efficient navigation. Notably, the hierarchical planning module is jointly trained with the world model prediction and action policy learning modules, indicating that NavForesee maintains strong language planning capabilities even when extended with additional functionalities. Furthermore, the hierarchical plans are precise and concise, which greatly benefits subsequent navigation decisions.\nFigure 5\n:\nHierarchical planning examples generated by NavForesee for the instruction ”Go up the stairs and straight forward the doorway. Turn right, move forward, and enter the doorway on the right. Move forward into the bedroom and stop in front of the toilet”. From top to bottom: frames with timestamps, global navigation map, and navigation planning outputs. NavForesee accurately identifies milestones along the route, summarizes completed sub-instructions, and generates the next sub-instruction in accordance with the instruction context.\nFigure 6\n:\nShort-term and long-term depth predictions. From top to bottom: frames with timestamps, future ground truth frames with timestamps, short-term depth predictions for future frames, and long-term depth predictions for milestones. Instruction: ”Up the stairs. Turn to the left and enter the second open door on the left. Walk towards the foot of the bed. Turn right and enter the open door to the bathroom.”\nII-B\nShort-term and Long-term Prediction Evaluation\nFigure\n6\nillustrates the short-term and long-term depth predictions produced by our world model over a complete navigation episode. Short-term predictions forecast up to four future steps, whereas long-term predictions extrapolate over an adaptive horizon determined by progress towards the next milestone. Compared to short-term predictions, long-term depth predictions may be less accurate in capturing detailed depth at milestone locations, since milestone positions are unknown during inference. At the beginning of the episode, the long-term predictions effectively capture the scene when the agent ascends the stairs. As the agent approaches the first milestone (the doorway), the long-term predictions degrade slightly, likely due to the increased uncertainty of longer horizons and the absence of explicit milestone information. In such cases, long-term predictions tend to track short-term outputs, because long-term queries can attend to short-term queries. Nevertheless, the long-term predictions maintain the overall scene layout and depth distribution, providing valuable guidance for strategic navigation. This demonstrates that NavForesee’s world model effectively anticipates environmental changes over both short and long horizons, enhancing the agent’s planning and action capabilities in complex scenarios.\nTABLE III\n:\nPerformance comparison between depth prediction and semantics prediction\nIndex\nDepth\nSemantics\nSR\n↑\n\\uparrow\nOSR\n↑\n\\uparrow\nNE\n↓\n\\downarrow\nSPL\n↑\n\\uparrow\n1\n✓\n✓\n66.2%\n78.4%\n3.94\n59.7%\n2\n✗\n✓\n61.8%\n76.7%\n4.37\n54.9%\n3\n✓\n✗\n60.0%\n76.2%\n4.59\n52.9%\nII-C\nAblation Study on Depth and Semantics Predictions\nWe conduct ablation studies to evaluate the individual contributions of depth and semantics predictions in the world model. As shown in Table\nIII\n, removing either depth or semantics predictions results in a clear performance drop. The full NavForesee model, which integrates both depth and semantics predictions, achieves the highest SR (66.2%), OSR (78.4%), lowest NE (3.94), and best SPL (59.7%), validating the benefit of their combination. Without depth prediction, the SR drops to 61.8% and SPL decreases by 4.8 points, highlighting the importance of depth information for spatial reasoning and obstacle avoidance. Disabling semantics predictions leads to an even larger SR reduction (60.0%) and higher NE, underscoring the critical role of semantic features in recognizing landmarks and guiding navigation. These findings confirm that both depth and semantics predictions are essential for accurate and efficient navigation.",
  "preview_text": "Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.\n\nNavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction\nFei Liu\n∗\nShichao Xie\n∗\nMinghua Luo Zedong Chu\n†\nJunjun Hu Xiaolong Wu\n†\nMu Xu\nAmap, Alibaba Group\n*Joint first authors\n†\n\\dagger\nCorresponding authors\nAbstract\nEmbodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robus",
  "is_relevant": true,
  "relevance_score": 6.0,
  "extracted_keywords": [
    "unified generation understanding",
    "multimodal generation"
  ],
  "one_line_summary": "NavForesee 是一个统一的视觉-语言世界模型，结合语言规划与时空预测，用于层次化导航决策。",
  "detailed_summary": "## 论文详细摘要：NavForesee\n\n- **研究背景和动机**  \n  长距离视觉语言导航（VLN）任务中，智能体常因缺乏长期规划能力与环境预见性而在复杂场景中“迷失”。现有方法或依赖易产生语义幻觉的纯语言推理，或使用脱离指令目标的世界模型，导致决策与实际环境脱节。为此，本文提出 **NavForesee**，旨在统一视觉语言模型（VLM）的高层语义规划与世界模型的时空预测能力，构建具备“前瞻性”思维的具身智能体。\n\n- **核心方法和技术创新**  \n  NavForesee 创新性地融合两大功能于单一 VLM 框架：\n  - **分层语言规划**：将长指令分解为子目标，动态跟踪进度并生成语义动作“主干”，实现目标对齐的高层导航策略。\n  - **双时域世界模型预测**：在隐空间内同步预测短期环境动态（如障碍物变化）与长期里程碑视觉特征（如关键帧布局），避免耗时的像素级生成。通过结构化注意力机制耦合二者，形成“感知—规划/预测—行动”的闭环反馈。\n\n- **主要实验结果**  \n  在 R2R-CE 和 RxR-CE 基准上，仅使用公开数据训练的 NavForesee 取得极具竞争力的表现：\n  - R2R-CE 上成功率达 **66.2%**，Oracle 成功率达 **78.4%**，接近当前最优水平；\n  - 消融实验证明，移除任一模块均显著降低性能，验证了双系统协同的有效性。\n\n- **研究意义和价值**  \n  本工作首次将显式语言规划与隐式时空预测深度融合，展示了“心智模拟”对提升导航鲁棒性的关键作用。其统一架构为构建更智能、可解释、具备前瞻能力的具身 AI 提供了新范式，推动 VLN 向人类层次的层次化导航迈进。",
  "qa_pairs": [
    {
      "question": "这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？",
      "answer": "## 论文核心创新点与问题解决分析\n\n### **核心问题：长距离视觉语言导航（VLN）中的规划与预测脱节**\n\n在复杂自然语言指令引导下的**长距离具身导航任务**中，现有方法面临两大关键挑战：\n- **规划与记忆缺陷**：主流视觉语言模型（VLM）受限于上下文窗口和推理能力，在长时间导航中容易“迷失”，无法有效分解任务、跟踪进度。\n- **缺乏前瞻性预测能力**：当前模型多为反应式（reactive），无法主动预判未来环境状态，导致决策缺乏远见。\n\n更根本的问题是：**语言规划模块与世界模型预测模块相互割裂**。这导致：\n- VLM 规划可能产生“语义幻觉”（脱离视觉现实）；\n- 世界模型预测可能出现“语义漂移”（偏离任务目标）。\n\n---\n\n### **核心创新：NavForesee —— 统一的层级规划与双时域预测框架**\n\nNavForesee 提出一个**统一的 Vision-Language World Model**，首次将**显式语言规划**与**隐式时空预测**深度融合于单一 VLM 架构中，形成闭环的“感知–规划/预测–行动”循环。\n\n#### **1. 层级语言规划（Hierarchical Language Planning）**\n- **目标**：模仿人类导航行为，以里程碑为中心进行高层抽象。\n- **实现方式**：\n  - 将长指令自动分解为子目标（sub-instructions）；\n  - 实时输出三元组：已完成的子任务总结 + 当前子任务 + 下一步动作主干（如 `forward`, `left`）；\n  - 基于 Gemini 2.5 Pro 构建高质量训练数据集，包含 1.3M 样本。\n- **价值**：提供语义锚点，避免长链推理错误累积，增强任务一致性。\n\n#### **2. 双时域预测世界模型（Dual-Horizon Predictive Foresight）**\n- **目标**：赋予模型“想象未来”的能力，兼顾短期执行与长期战略。\n- **实现方式**：\n  ```python\n  # 预测两种时域特征（非像素级，高效）\n  short_horizon = predict(depth, DINOv2, SAM)  # 短期动态（k步内）\n  long_horizon  = predict(depth, DINOv2, SAM)  # 长期里程碑（自适应步数 M_t）\n  ```\n  - 使用**紧凑的高维特征**（深度 + DINOv2/SAM 语义）替代耗时的像素生成；\n  - 引入**结构化注意力机制**：长时预测依赖短时预测，确保时间连贯性；\n  - 避免跨模态泄露，保持预测清晰分离。\n\n#### **3. 统一架构与闭环反馈机制**\n- **统一 backbone**：基于 Qwen2.5-VL-3B-Instruct，共享参数同时支持规划与预测；\n- **联合训练目标**：\n  $$\n  \\mathcal{L} = \\alpha \\mathcal{L}_{\\text{depth}} + \\beta \\mathcal{L}_{\\text{semantics}} + \\mathcal{L}_{\\text{action}}\n  $$\n- **闭环控制流**：\n  > **语言计划 → 指导预测方向 → 预测结果 → 反哺动作策略 → 执行动作 → 更新状态 → 新计划…**\n\n---\n\n### **技术创新总结**\n\n| 创新维度 | 具体贡献 | 实际价值 |\n|--------|--------|--------|\n| **范式创新** | 首次统一 VLM 规划与世界模型预测于单一体系 | 解决“规划-预测”割裂问题，提升整体智能性 |\n| **架构设计** | 双查询机制（short & long dream queries）+ 结构化注意力 | 实现高效、有序的双时域预测，降低计算开销 |\n| **表示学习** | 预测 latent-space 几何-语义特征（depth + DINOv2/SAM） | 平衡表达力与效率，适合部署在资源受限平台 |\n| **数据构建** | 利用强 VLM 自动生成带里程碑标注的层级规划数据 | 无需人工标注即可获得高质量监督信号 |\n\n---\n\n### **实验验证与实际价值**\n\n- 在 **R2R-CE** 上达到 **66.2% SR / 78.4% OSR**，接近 SOTA；\n- 消融实验证明：**三者缺一不可**（层级规划 + 短期预测 + 长期预测）；\n- 定性结果显示模型能从极简观测中推断房间布局（如仅瞥一眼即想象完整卧室结构），展现强大空间推理能力。\n\n> ✅ **结论**：NavForesee 成功融合了“思考”与“想象”两种认知能力，为构建真正具备前瞻性和鲁棒性的具身智能体提供了新范式。",
      "timestamp": "2026-01-09T10:00:16.631133",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "基于他的前作，梳理这个方向的整个发展脉络，每一步相比于之前的工作都改进了什么，着重于几个不同的发展方向。",
      "answer": "## NavForesee 的研究脉络与发展演进分析\n\nNavForesee 并非孤立创新，而是建立在视觉-语言导航（VLN）与具身智能体预测建模的长期发展基础之上。其核心贡献在于**首次将基于大模型的语言规划能力与世界模型的时空预测能力在统一框架下深度融合**，实现了从“被动反应”到“主动预见”的范式跃迁。\n\n以下梳理该方向的关键发展阶段，并重点剖析每一步相对于前作的技术改进与发展方向：\n\n---\n\n### 1. **早期 VLN：图结构与语义地图主导**\n> **代表工作**：[22][23][24][25][26]  \n> **技术路线**：依赖手工构建的拓扑图、语义地图或注意力机制进行路径推理。\n\n- **相比之前**：\n  - 相比纯端到端策略网络，引入了**显式的空间结构表示**（如节点-边图），提升了长距离导航的稳定性。\n- **局限性**：\n  - 图结构需预定义或离线构建，难以泛化至新环境；\n  - 缺乏对自然语言指令的深层语义理解能力。\n\n> ✅ **NavForesee 改进**：完全抛弃固定图结构，采用**动态、在线的多模态理解+生成式预测**，实现真正开放世界的连续导航。\n\n---\n\n### 2. **VLM 驱动的高级规划时代**\n> **代表工作**：[29][30][31][32][12][13]  \n> **技术路线**：利用大型视觉语言模型（VLM）作为“大脑”，通过 Chain-of-Thought（CoT）自回归生成文本计划或动作序列。\n\n- **关键进步**：\n  - 引入**语言层面的推理链**（reasoning chain），使 agent 能够分解复杂指令、跟踪进度；\n  - 实现零样本迁移和更强的指令遵循能力（如 InstructNav [31]）。\n- **典型问题**：\n  - 推理过程是“开环”的——计划可能脱离视觉现实（**语义幻觉**）；\n  - 计划过于细粒度，导致推理延迟高、错误累积严重。\n\n> ✅ **NavForesee 改进**：\n  - 提出**层次化语言规划**（Hierarchical Language Planning）：将长指令分解为“已完成子任务 + 当前目标 + 下一语义动作主干”，避免冗长 CoT；\n  - 规划结果直接用于指导后续的世界模型预测，形成闭环反馈。\n\n---\n\n### 3. **双系统架构：快慢思维结合**\n> **代表工作**：[6][33][8]（如 GridMM, Omninav）  \n> **技术路线**：“慢系统”负责高层语义规划，“快系统”执行低层动作控制，两者协同决策。\n\n- **关键突破**：\n  - 明确区分**战略级规划**与**战术级执行**，提升效率与鲁棒性；\n  - 利用强化学习对齐两个系统的输出。\n- **遗留挑战**：\n  - “慢系统”仍主要依赖文本推理，缺乏对环境动态的物理感知；\n  - 快慢系统之间信息交互有限，未实现真正的感知-预测-行动融合。\n\n> ✅ **NavForesee 改进**：\n  - 将“慢系统”（VLM 规划）与“快系统”（世界模型预测）**统一于同一 VLM 架构内**，共享上下文表征；\n  - 通过**结构化注意力机制**实现双向耦合：规划引导预测，预测反哺规划。\n\n---\n\n### 4. **世界模型引入：从像素到特征空间的想象**\n> **代表工作**：[9][11][35][17][18]（如 HNR, NavMorph, DreamVLA）  \n> **技术路线**：训练一个生成模型来预测未来状态，提供“前瞻性视野”。\n\n- **重大转变**：\n  - 从**反应式决策**转向**基于模拟的规划**（imagine before act）；\n  - 使用**潜空间表示**（如 DINOv2、SAM 特征）替代像素级生成，显著降低计算成本并保留关键语义信息。\n- **主要缺陷**：\n  - 多数世界模型**独立于语言系统训练**，导致预测偏离任务目标（**语义漂移**）；\n  - 缺少高层语义指导，无法聚焦关键里程碑。\n\n> ✅ **NavForesee 改进**：\n  - 设计**双时间尺度预测机制**（Dual-Horizon Prediction）：\n    - **短期预测**：预估未来几步的深度与语义特征，增强局部避障与动态感知；\n    - **长期预测**：想象到达下一个**语义里程碑**时的场景特征，提供战略指引；\n  - 所有预测均以完整语言指令和历史观测为条件，确保语义一致性。\n\n---\n\n### 5. **NavForesee 的集成创新：三位一体的统一框架**\n\n| 维度 | 前序方法 | NavForesee 的突破 |\n|------|--------|------------------|\n| **架构设计** | 分离式模块（规划器 + 控制器 + 预测器） | **单一 VLM 统一建模**：规划、预测、动作联合训练 |\n| **规划方式** | 自回归 CoT 或图搜索 | **层次化语言计划**：简洁、可解释、抗误差累积 |\n| **预测机制** | 单一时间尺度、无语言引导 | **双时间尺度 + 指令条件化**：短程保细节，远程看目标 |\n| **信息流动** | 单向或弱耦合 | **闭环反馈循环**：<br>**Perception → Planning/Prediction → Action → Perception** |\n| **表示形式** | 像素 / 离散动作 | **几何-语义联合特征空间**：<br>Depth + DINOv2 + SAM → 更高效、更具物理意义 |\n\n```python\n# 伪代码示意：NavForesee 的内部循环\ndef navforesee_step(instruction, history_obs):\n    # Step 1: 层次化语言规划\n    plan = vlm.generate(\n        f\"已走完: {summary}; 当前目标: {current_goal}; 下一步: {next_subgoal}\"\n    )\n    \n    # Step 2: 双时间尺度世界模型预测\n    short_term_feat = world_model.predict_next_k_steps(history_obs, plan)\n    long_term_feat   = world_model.predict_milestone_scene(plan.next_subgoal)\n    \n    # Step 3: 动作策略生成（基于预测特征）\n    action = mlp_policy(short_term_feat, long_term_feat, plan.action_trunk)\n    \n    return action, plan, short_term_feat, long_term_feat\n```\n\n---\n\n### 总结：NavForesee 的三大发展方向革新\n\n1. **从“分治”到“融合”**  \n   > 不再将规划、记忆、预测视为独立模块，而是通过一个统一的 VLM 实现**多功能协同演化**，极大提升系统内聚性与泛化能力。\n\n2. **从“推理即行动”到“想象即导航”**  \n   > 将世界模型从辅助工具升级为核心组件，使其不仅能预测未来，还能**主动塑造决策过程**，实现真正的“心智模拟”。\n\n3. **从“逐帧响应”到“里程碑驱动”**  \n   > 模仿人类导航行为，关注**关键语义节点**而非每一步微调，大幅提升长程任务的鲁棒性与效率。\n\n---\n\n### 结论\n\nNavForesee 标志着 VLN 领域的一个重要转折点：它不再追求更复杂的图结构或更深的推理链，而是回归智能本质——**通过统一的感知-认知-预测架构，让机器学会“预见未来”**。这一思想不仅适用于导航，也为具身 AI 在复杂任务中的自主决策提供了全新范式。",
      "timestamp": "2026-01-09T10:01:11.724417",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "他的前作有哪些？使用表格仔细讲讲他的每篇前作，他和前作的区别是什么，主要改善是什么？着重于具体相比于之前文章的改动",
      "answer": "## NavForesee 与作者前作的对比分析\n\n根据论文内容，**NavForesee** 的主要作者（如 Fei Liu、Shichao Xie、Minghua Luo、Junjun Hu、Xiaolong Wu、Mu Xu）均来自 **Amap, Alibaba Group**。虽然论文未明确列出所有作者的完整发表历史，但通过引用文献和团队背景可识别出其关键前作，尤其是 **Omninav [33]** 和 **Reborn* [41]** 等工作，这些构成了 NavForesee 的直接技术基础。\n\n下表系统梳理了该团队在导航领域的主要前作，并重点分析 **NavForesee 相比于前作的核心技术创新与改进**：\n\n| 论文 | 发表年份 | 核心方法 | 主要局限性 | NavForesee 的关键改进 |\n|------|----------|--------|-----------|------------------------|\n| **Reborn* [41]** (CVPR 2022 冠军方案) | 2022 | 基于强化学习（RL）的端到端导航模型，结合多模态融合与路径规划策略。使用 R2R/RxR 数据进行训练，强调动作决策的鲁棒性。 | - 缺乏显式高层语言规划能力<br>- 预测机制薄弱，依赖当前观测<br>- 规划与执行耦合紧密，难以处理长指令 | ✅ **引入分层语言规划**：将长指令分解为子目标，显式跟踪任务进度，提升对复杂指令的理解与执行一致性。<br>✅ **加入双视野世界模型预测**：不再是纯反应式，而是能“预见”未来环境状态。 |\n| **Omninav [33]** | 2025 | 提出统一框架，结合前瞻性探索（prospective exploration）与视觉-语言导航（VLN），利用 VLM 进行推理辅助导航。 | - 推理与动作生成仍相对分离<br>- 缺少结构化的内部反馈循环<br>- 未整合生成式世界模型进行环境动态预测 | ✅ **深度融合规划与预测**：VLM 不仅用于推理，还作为生成式世界模型，统一实现“语言规划 + 视觉想象”。<br>✅ **构建感知–规划/预测–行动闭环**：预测结果反哺规划与动作决策，形成自洽的内部循环。 |\n| **HNR* [9]** | 2024 | 利用神经辐射场（NeRF）表示进行前瞻探索，预测多级语义特征以评估下一步动作，提升连续环境下的导航性能。 | - 预测聚焦于短期动作评估<br>- 缺少高层语义指导的长期里程碑预测<br>- 语言规划能力弱 | ✅ **提出双视野预测机制**：<br> 🔹 **短时预测**：增强局部感知（障碍规避）<br> 🔹 **长时预测**：生成**下一个里程碑的视觉特征**（如 DINOv2、SAM），实现真正的战略级引导。<br>✅ **语言驱动的世界模型**：预测过程受完整指令和子计划条件控制，避免“语义漂移”。 |\n\n---\n\n### 🔍 NavForesee 相比前作的具体技术改动与创新点\n\n#### 1. **从“反应式决策”到“预见式导航”的范式转变**\n- **前作（如 Reborn*, HNR*）**：主要基于当前观测和历史记忆做出反应式动作选择，缺乏对未来状态的主动建模。\n- **NavForesee 改进**：\n  - 引入 **Vision-Language World Model**，使单一 VLM 同时具备：\n    - **语言规划能力**（分解任务、生成子目标）\n    - **环境生成能力**（预测未来深度与语义特征）\n  - 实现了“**先想象，再行动**”的类人导航模式。\n\n```python\n# 伪代码示意：NavForesee 的双功能 VLM\ndef navforesee_model(instruction, history_obs):\n    # 功能1: 层次化语言规划\n    sub_goal = vlm_plan(instruction, history_obs)  # 如 \"进入第二个左转的门\"\n    \n    # 功能2: 双视野世界模型预测\n    short_term_feat = predict_k_steps_ahead(history_obs, action_hint)\n    long_term_milestone = predict_next_landmark(sub_goal)  # 预测目标区域外观\n    \n    # 融合预测与规划生成动作\n    action = policy_head(short_term_feat, long_term_milestone, sub_goal)\n    \n    return action, sub_goal, long_term_milestone\n```\n\n#### 2. **双视野（Dual-Horizon）预测机制 —— 战略与战术结合**\n- **前作（如 HNR*）**：仅预测下一步或几步内的环境变化，服务于即时动作评估。\n- **NavForesee 改进**：\n  - **短时预测（Short-term）**：预测未来 $k$ 步（实验中设为4步）的深度与语义特征，用于**局部避障与动态理解**。\n  - **长时预测（Long-term）**：预测到达**下一个语义里程碑**（如“床尾”、“浴室门口”）时的环境特征，提供**全局战略指引**。\n  - 两者通过**结构化注意力掩码**隔离但又允许长时依赖短时输出，保证时空一致性。\n\n> 📌 公式体现：\n> \n> $$\n> E_S = f(l, O_{t-H:t}, h(s_{t-H:t}) \\mid Q_S) \\quad \\text{(短时嵌入)}\n> $$\n> $$\n> E_L = f(l, O_{t-H:t}, h(s_{t-H:t}), Q_S \\mid Q_L) \\quad \\text{(长时嵌入，依赖短时)}\n> $$\n\n#### 3. **语言驱动的层次化规划数据集构建**\n- **前作**：大多依赖原始 R2R/RxR 指令，缺乏显式的子任务划分与进度标记。\n- **NavForesee 改进**：\n  - 使用 **Gemini 2.5 Pro** 对 R2R-CE/RxR-CE 数据进行自动标注：\n    - 将长指令分解为**子指令序列**\n    - 标注关键帧作为**导航里程碑**\n    - 为每个路径段生成：已完成摘要、下一步计划、语言动作标签\n  - 构建了约 **1.5M 样本**的高质量分层规划数据集，支持监督训练。\n\n#### 4. **轻量级特征预测代替像素生成，兼顾效率与实用性**\n- **前作（如 DreamWalker* [40]）**：尝试生成未来图像帧，计算开销大，且易失真。\n- **NavForesee 改进**：\n  - 预测**紧凑的高维特征空间**而非原始像素：\n    - **Depth**：几何结构\n    - **DINOv2 特征**：语义信息\n    - **SAM 特征**：对象边界与分割\n  - 使用轻量卷积解码器还原特征，大幅降低计算成本，更适合部署。\n\n#### 5. **统一架构下的联合训练与闭环设计**\n- **前作**：常采用两阶段或模块化设计（如先规划后执行），存在误差传播问题。\n- **NavForesee 改进**：\n  - 在 **Qwen2.5-VL-3B-Instruct** 单一 backbone 上联合训练：\n    - VLM 规划任务（自回归生成文本）\n    - 世界模型预测任务（深度 + 语义 MSE/SiLogLoss）\n    - 动作策略任务（MLP 输出 waypoints）\n  - 总损失函数：\n    $$\n    \\mathcal{L} = \\alpha \\mathcal{L}_d + \\beta \\mathcal{L}_c + \\mathcal{L}_a\n    $$\n  - 形成 **perception → planning/prediction → action → new perception** 的隐式闭环。\n\n---\n\n### ✅ 总结：NavForesee 的核心突破\n\n| 维度 | 前作做法 | NavForesee 创新 |\n|------|---------|----------------|\n| **规划方式** | 端到端动作预测 或 分离式 CoT 推理 | ✅ **VLM 驱动的层次化语言规划**，显式分解任务 |\n| **预测能力** | 无 或 仅短期动作评估 | ✅ **双视野世界模型**：短时（执行）+ 长时（里程碑） |\n| **模型架构** | 多模块拼接 或 纯策略网络 | ✅ **统一 VLM 架构**，兼具语言理解与世界生成能力 |\n| **训练数据** | 原始轨迹-指令对 | ✅ **自动生成分层规划标签**，支持监督学习 |\n| **实际价值** | 高延迟、难泛化 | ✅ 更强的**长程一致性**与**环境适应性**，适合真实场景部署 |\n\n>",
      "timestamp": "2026-01-09T10:02:18.180018",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "论文提出了哪些关键技术方法，请列表格具体详细说明技术细节，需要包含具体的数学原理推导，以及具体参数。",
      "answer": "## 论文关键技术方法详解：NavForesee 的核心技术创新\n\n本文提出 **NavForesee**，一种将视觉-语言模型（VLM）的高层语义规划能力与世界模型的时空预测能力统一于单一框架中的导航系统。其核心贡献在于构建了一个“感知–规划/预测–行动”闭环，显著提升了长视野、复杂指令下的具身导航性能。\n\n以下从技术模块出发，详细列出 NavForesee 的关键方法，并包含数学原理推导和具体参数配置。\n\n---\n\n### 📋 技术方法总览表\n\n| 模块 | 技术名称 | 核心思想 | 数学表达 | 关键参数 |\n|------|--------|--------|--------|--------|\n| 1 | 层次化语言规划（Hierarchical Language Planning） | 将长指令分解为子目标，跟踪进度并生成下一步动作主干 | 文本自回归建模 | VLM: Qwen2.5-VL-3B-Instruct |\n| 2 | 双时间尺度世界模型预测（Dual-Horizon World Model Prediction） | 预测短期环境动态与长期里程碑状态，提供前瞻性感知 | $E_S = f(l, O_{t-H:t}, h(s_{t-H:t}) \\mid Q_S)$<br>$E_L = f(\\cdot, Q_S \\mid Q_L)$ | 短期 $k=4$ 步<br>长期自适应 $M_t$ |\n| 3 | 多模态特征预测目标 | 预测深度图 + DINOv2/SAM 语义特征，避免像素级生成开销 | $\\mathcal{L}_d$: SiLogLoss<br>$\\mathcal{L}_c$: MSE | 特征维度: 64 tokens × 6 query sets |\n| 4 | 基于逆动力学的动作策略学习 | 利用预测特征反推未来动作序列 | $\\hat{a}_{t:t+k} = M_{inv}(E_S, E_L \\mid E_a)$ | 预测 $K=5$ 个连续航点 |\n| 5 | 结构化注意力机制 | 控制不同查询之间的信息流动，防止跨模态泄漏 | 注意力掩码设计 | Dream Queries: 6 类共 385 tokens |\n\n---\n\n## 一、层次化语言规划（Hierarchical Language Planning）\n\n### 🔍 核心思想\n受人类导航启发，不进行逐帧决策，而是以“里程碑”为中心进行高层语义规划。将原始长指令分解为：\n- 已完成的子任务（navigation summary）\n- 当前所在阶段（current sub-instruction）\n- 下一步计划（future plan）\n- 对应的语言动作（forward/left/right/stop）\n\n该过程通过大模型（Gemini 2.5 Pro）对 R2R-CE 和 RxR-CE 数据集进行自动标注生成训练数据。\n\n### 🧮 数学建模\n这是一个典型的**条件文本生成任务**，采用自回归方式建模：\n\n$$\nP(y_{1:T} \\mid l, O_{1:t}) = \\prod_{i=1}^{T} P(y_i \\mid y_{<i}, l, O_{1:t})\n$$\n\n其中：\n- $l$: 完整自然语言指令\n- $O_{1:t}$: 历史图像观测序列\n- $y_i$: 第 $i$ 个输出 token（属于规划标签空间）\n\n输出格式固定为 JSON-like 结构：\n```json\n{\n  \"summary\": \"已走过的路径描述\",\n  \"plan\": \"接下来要执行的子指令\",\n  \"action\": \"forward\"\n}\n```\n\n### ⚙️ 参数设置\n| 参数 | 值 | 说明 |\n|------|----|------|\n| 主干模型 | Qwen2.5-VL-3B-Instruct | 支持图文输入与文本输出 |\n| 输入长度 | 最多 20 张历史图像 | 支持长上下文记忆 |\n| 输出长度 | ~128 tokens | 足够表达结构化规划 |\n| 训练方式 | Auto-regressive fine-tuning | 使用交叉熵损失函数 |\n\n> ✅ 数据规模：约 **1.3M (RxR-CE) + 0.2M (R2R-CE)** 样本，经采样平衡处理。\n\n---\n\n## 二、双时间尺度世界模型预测（Dual-Horizon Visual Prediction）\n\n### 🔍 核心思想\n在隐空间中同时预测两个时间尺度的环境状态：\n- **短期预测（Short-term Horizon）**：预测未来 $k$ 步内的环境变化，用于局部避障与精细控制。\n- **长期预测（Long-term Horizon）**：预测到达下一个“里程碑”时的关键视觉特征，提供战略引导。\n\n使用“dream queries”作为可学习的提示向量，在 VLM 中触发对未来状态的想象。\n\n### 🧮 数学建模\n\n#### 1. 输入编码\n定义如下输入：\n- $l$: 指令文本嵌入\n- $O_{t-H:t} = \\{o_{t-H}, ..., o_t\\}$: 历史全景图像\n- $s_{t-H:t}$: 相对位姿（位置+朝向），由 pose encoder 编码为 $h(s_{t-H:t})$\n\n引入三类可学习查询：\n- $Q_S \\in \\mathbb{R}^{L \\times d}$: 短期预测查询（含 depth & semantics 子查询）\n- $Q_L \\in \\mathbb{R}^{L \\times d}$: 长期预测查询\n- $Q_a \\in \\mathbb{R}^{1 \\times d}$: 动作预测查询\n\n#### 2. 双阶段特征提取（因果注意力控制）\n\n利用结构化注意力掩码实现层级依赖：\n\n$$\nE_S = f(l, O_{t-H:t}, h(s_{t-H:t}) \\mid Q_S)\n$$\n$$\nE_L = f(l, O_{t-H:t}, h(s_{t-H:t}), Q_S \\mid Q_L)\n$$\n\n> ✅ 解读：长期预测 $E_L$ 显式依赖短期预测 $E_S$，形成“短促动长”的认知逻辑。\n\n#### 3. 解码器输出预测\n\n轻量化解码器 $D(\\cdot)$ 将 dream embedding 映射为实际特征：\n\n- 短期预测（固定步数 $k=4$）：\n  $$\n  p_{t+k} = D(E_S) = [d_p(t), c_p(t)]\n  $$\n- 长期预测（自适应步数 $M_t$，取决于距里程碑距离）：\n  $$\n  p_{t+M_t} = D(E_L) = [d_p(t+M_t), c_p(t+M_t)]\n  $$\n\n其中：\n- $d_p$: 预测深度图（pixel-level）\n- $c_p$: 高层语义特征（DINOv2 + SAM）\n\n---\n\n### ⚙️ 具体参数配置\n\n| 组件 | 参数 | 说明 |\n|------|------|------|\n| Dream Queries | 共 6 组：<br>- Short-term: depth, DINOv2, SAM<br>- Long-term: depth, DINOv2, SAM<br>每组 64 tokens → 总计 384 tokens | 每个子查询独立作用于特定任务 |\n| Action Query | 1 token | 用于动作策略推理 |\n| 特征类型 | - Depth: 像素级深度图<br>- Semantics: DINOv2 (self-supervised vision feature), SAM (segment anything features) | 不做像素生成，仅预测紧凑特征 |\n| 短期预测步数 $k$ | 4 | 固定向前看 4 步 |\n| 长期预测步数 $M_t$ | 自适应 | 依据当前进度动态调整 |\n| 解码器结构 | ViT-based decoder (2-layer) + VQ-VAE head for depth | 轻量化设计，降低计算负担 |\n\n---\n\n## 三、基于逆动力学的动作策略学习（Predictive Action Policy）\n\n### 🔍 核心思想\n不是直接从图像或指令映射到动作，而是利用“预测的未来状态”来反推应采取的动作——即模仿**逆动力学学习**（Inverse Dynamics Modeling）。\n\n### 🧮 数学建模\n\n给定当前与下一时刻的状态 $(o_t, o_{t+1})$，估计中间动作 $a_t$。扩展至多步预测：\n\n$$\nE_a = f(l, O_{t-H:t}, h(s_{t-H:t}), Q_S, Q_L \\mid Q_a)\n$$\n$$\n\\hat{a}_{t:t+k} = M_{inv}(E_S, E_L \\mid E_a)\n$$\n\n其中：\n- $E_a$: 来自 VLM 的动作上下文嵌入\n- $M_{inv}$: 一个简单的 MLP 网络，将 dream embeddings 映射为动作空间\n\n动作表示为连续航点序列：\n$$",
      "timestamp": "2026-01-09T10:03:18.778038",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "他使用了哪些评价指标与数据集，列表格具体讲讲他的评价指标的细节与数据集的细节",
      "answer": "## 评价指标与数据集详解：NavForesee 的实验设计\n\n该论文在 **R2R-CE** 和 **RxR-CE** 两个主流连续环境视觉语言导航（VLN）基准上进行了全面评估，采用了一系列标准且具有挑战性的评价指标。以下是对所用**数据集**和**评价指标**的详细解析。\n\n---\n\n### 一、使用的核心数据集\n\n| 数据集 | R2R-CE | RxR-CE |\n|--------|--------|--------|\n| **全称** | Room-to-Room Continuous Environment | Room-across-Room Continuous Environment |\n| **来源** | 基于 Matterport3D 室内场景构建 | 基于多语言人类指令扩展 |\n| **特点** | - 提供细粒度、逐句自然语言指令<br>- 在 Habitat 模拟器中以**连续动作空间**运行<br>- 支持最小 $15^\\circ$ 转向和 $90^\\circ$ 视野 | - 大规模、**多语言**（英语、印地语、泰卢固语）<br>- 包含约 126K 条人工标注指令<br>- 路径更复杂、更具挑战性<br>- 最小转向 $30^\\circ$，视野 $79^\\circ$，要求更高规划精度 |\n| **训练/验证划分** | 使用 `val-unseen` 分割进行测试（即模型未见过这些环境） | 同样使用 `val-unseen` 分割，确保泛化能力评估公平 |\n| **任务目标** | 根据自然语言指令，在连续 3D 环境中从起点导航至目标位置，并严格遵循路径 |\n\n> ✅ **关键点**：这两个数据集代表了当前 VLN 领域最先进的**连续环境设定**（Continuous Environment），相比早期离散图结构任务更具现实意义。NavForesee **仅使用公开的 R2R-CE 和 RxR-CE 数据进行训练**，未引入额外大规模预训练数据，凸显其方法的有效性。\n\n---\n\n### 二、核心评价指标详解\n\n论文采用了 VLN 领域广泛接受的标准指标，综合衡量导航的**成功率、效率与鲁棒性**：\n\n| 指标缩写 | 全称 | 计算方式 | 物理含义 |\n|---------|------|----------|----------|\n| **SR**<br>(Success Rate) | 成功率 | $ \\text{SR} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}(\\text{final position}_i \\in \\text{goal region}) $ | 导航最终是否到达目标区域的成功比例。是**最基础也是最重要的指标**。 |\n| **OSR**<br>(Oracle Success Rate) | 最优成功率 | 在整个轨迹中，**最近一次距离目标最近的位置是否在目标范围内**？若是，则记为成功。 | 衡量即使最终失败，过程中是否“曾接近”正确位置。反映模型是否有正确的方向感，对长程任务尤为重要。 |\n| **SPL**<br>(Success weighted by Path Length) | 成功率加权路径长度 | $ \\text{SPL} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}_i \\cdot \\frac{l_i}{\\max(l_i, d_i)} $<br>其中 $l_i$: 最短到目标路径长度；$d_i$: 实际行走路径长度 | 综合考虑**成功与否**与**路径效率**。只有成功时 SPL > 0，且路径越短得分越高。防止“绕远路成功”的低效行为。 |\n| **NE**<br>(Navigation Error) | 导航误差 | $ \\text{NE} = \\frac{1}{N} \\sum_{i=1}^{N} \\min_t \\| p_t^{(i)} - g^{(i)} \\| $<br>即所有轨迹中，**离目标最近的距离的平均值** | 衡量失败案例中的平均偏差程度。数值越小，说明即使失败也“差得不远”，体现模型的方向一致性。 |\n\n> 📊 **总结表格：各指标关注重点**\n>\n> | 指标 | 关注维度 | 数值越高越好？ |\n> |------|--------|----------------|\n> | SR   | 最终是否成功到达 | ✅ 是 |\n> | OSR  | 是否曾经靠近目标 | ✅ 是 |\n> | SPL  | 成功 + 路径高效 | ✅ 是 |\n> | NE   | 平均偏离目标多远 | ❌ 否（越低越好） |\n\n---\n\n### 三、实验结果表现（来自 Table I）\n\n在 **R2R-CE val-unseen** 上的表现尤为突出：\n\n```markdown\n| 方法           | NE ↓  | OSR ↑ | SR ↑  | SPL ↑ |\n|----------------|-------|-------|-------|-------|\n| HNR* [9]       | 4.42  | 67.0  | 61.0  | 51.0  |\n| CorrectNav [53]| 4.24  | 67.5  | 65.1  | 62.3  |\n| **NavForesee (Ours)** | **3.94** | **78.4** | **66.2** | **59.7** |\n```\n\n- **SR 达到 66.2%**：略高于 SOTA（如 CorrectNav 的 65.1%），表明整体导航能力强劲。\n- **OSR 高达 78.4%**：显著领先，说明 NavForesee 在整个旅程中始终保持更强的目标指向性，“走偏”的概率更低。\n- **NE 仅为 3.94m**：最低误差，证明其预测与规划极为精准，即使失败也离目标很近。\n- **SPL 为 59.7%**：虽非最高，但结合高 SR 和低 NE，说明其路径合理且高效。\n\n> 🔍 **亮点发现**：NavForesee 在 **OSR 上取得最大优势**，这直接归功于其提出的 **“双时间尺度预测”机制**——通过长期里程碑预测保持战略方向，从而极大提升了“是否曾接近目标”的能力。\n\n---\n\n### 四、消融实验进一步验证指标敏感性（Table II & III）\n\n#### A. 模块消融（Table II）\n| 设置 | VLM 规划 | 长期预测 | 短期预测 | SR | OSR | NE | SPL |\n|------|----------|----------|----------|-----|------|-----|------|\n| 1 (Full) | ✓ | ✓ | ✓ | **66.2%** | **78.4%** | **3.94** | **59.7%** |\n| 2 | ✗ | ✓ | ✓ | 48.8% | 75.5% | 5.61 | 39.4% |\n| 3 | ✓ | ✗ | ✓ | 58.6% | 76.4% | 4.47 | 50.1% |\n| 4 | ✗ | ✗ | ✗ | 52.6% | 67.4% | 5.53 | 46.7% |\n\n- 移除 **VLM 层次化规划** → SR 下降 **17.4%**，SPL 下降超 16 点 → 说明显式语言分解对任务理解至关重要。\n- 移除 **长期预测** → SR 下降 7.6%，NE 明显上升 → 说明缺乏里程碑想象会导致战略迷失。\n- 三项全关 → 性能最差 → 验证了三大模块协同工作的必要性。\n\n#### B. 特征消融（Table III）\n| 设置 | 深度预测 | 语义预测 | SR | OSR | NE | SPL |\n|------|----------|----------|-----|------|-----|------|\n| 1 (Full) | ✓ | ✓ | **66.2%** | **78.4%** | **3.94** | **59.7%** |\n| 2 | ✗ | ✓ | 61.8% | 76.7% | 4.37 | 54.9% |\n| 3 | ✓ | ✗ | 60.0% | 76.2% | 4.59 | 52.9% |\n\n- 移除 **语义预测** 影响更大（SR↓6.2 vs ↓5.4）→ 说明识别关键物体/房间等语义信息对导航决策更为关键。\n- 移除 **深度预测** 仍导致明显下降 → 几何结构与避障同样不可或缺。\n\n---\n\n### 结论：技术创新与实际价值\n\nNavForesee 的实验设计充分体现了其**技术先进性与实用价值**：\n\n- **创新融合**：",
      "timestamp": "2026-01-09T10:04:21.778757",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "论文在哪些数据集上进行了实验？主要的评估指标和性能提升是多少？",
      "answer": "## 论文实验数据集、评估指标与性能提升分析\n\n### 实验数据集\n论文在两个主流的视觉-语言导航（VLN）连续环境数据集上进行了实验：\n- **R2R-CE**：基于 Matterport3D 室内环境构建，提供细粒度的自然语言指令，适用于路径规划并在 Habitat 模拟器中以连续方式运行。\n- **RxR-CE**：大规模多语言 VLN 数据集，包含约 12.6 万条人工标注指令，轨迹更复杂多样，挑战性更高。\n\n> 所有训练仅使用公开的 R2R-CE 和 RxR-CE 数据集，未引入额外外部数据。\n\n---\n\n### 主要评估指标\n采用视觉-语言导航任务的标准评估指标：\n- **SR (Success Rate)**：成功到达目标的比例。\n- **OSR (Oracle Success Rate)**：若存在一条可达路径，则视为成功，衡量路径合理性。\n- **SPL (Success weighted by Path Length)**：考虑路径长度的成功率，反映效率。\n- **NE (Navigation Error)**：预测路径与真实路径之间的平均距离误差（越低越好）。\n\n---\n\n### 性能表现与提升\n\n#### 在 **R2R-CE Val-Unseen** 上的表现（核心成果）\n| 指标 | NavForesee (Ours) | 最优基线（HNR*） | 提升 |\n|------|-------------------|------------------|------|\n| **SR**   | **66.2%** | 61.0% | **+5.2%** |\n| **OSR**  | **78.4%** | 67.0% | **+11.4%** |\n| **SPL**  | **59.7%** | 51.0% | **+8.7%** |\n| **NE↓**  | **3.94m** | 4.42m | **↓0.48m** |\n\n> ✅ **达到并超越 SOTA 水平**，尤其在 OSR 上显著领先，表明其具备更强的全局路径理解与容错能力。\n\n#### 在 **RxR-CE Val-Unseen** 上的表现\n| 指标 | NavForesee (Ours) | 最优基线（CorrectNav） |\n|------|-------------------|------------------------|\n| **SR**   | 66.3% | **69.3%** |\n| **SPL**  | 53.2% | **63.3%** |\n| **NE↓**  | 4.20m | **4.09m** |\n\n> ⚠️ 表现略逊于当前最优方法，说明在更复杂的多语言、长程任务中泛化能力仍有提升空间。但 **OSR 达到 78.4%**（表 IV-A 注），仍为最高之一，体现其强大的语义对齐与目标识别能力。\n\n---\n\n### 关键结论\n- **技术优势**：通过统一 **VLM 层次化语言规划** 与 **双时间尺度世界模型预测**，实现了感知–规划–预测–行动的闭环反馈机制。\n- **实际价值**：\n  - 首次将语言驱动的高层计划与隐式时空预测深度融合，避免“语义幻觉”与“语义漂移”。\n  - 使用 **DINOv2 + SAM + Depth 特征预测** 替代像素级生成，高效且富含几何与语义信息。\n  - 在仅用公开数据训练的情况下，在 R2R-CE 上实现全面领先，验证了方法的有效性与可扩展性。\n\n```markdown\n> 💡 核心创新点总结：\n> - **Hierarchical Language Planning**：分解长指令 → 跟踪进度 → 生成子目标。\n> - **Dual-Horizon World Model**：短期预测（局部避障）+ 长期预测（里程碑想象）。\n> - **Unified Architecture**：单一 VLM 同时完成规划与预测，形成闭环。\n```",
      "timestamp": "2026-01-09T10:04:42.978087",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "论文的主要局限性有哪些？未来可能的改进方向是什么？",
      "answer": "## 论文的主要局限性与未来改进方向\n\n尽管 **NavForesee** 在长视野导航任务中表现出色，但其设计和实验仍存在若干**关键局限性**。这些限制也为后续研究提供了清晰的**改进方向**。\n\n---\n\n### - 主要局限性分析\n\n- **训练数据依赖人工标注的分层计划**\n  - 分层语言规划的数据由 Gemini 2.5 Pro 自动生成（如图1），虽经后处理，但仍可能引入**语义不一致或逻辑错误**。\n  - 模型性能受限于外部 VLM 的规划质量，形成“**预处理瓶颈**”，缺乏端到端优化能力。\n\n- **预测模块未完全解耦动作与状态**\n  - 短期与长期预测均以固定/自适应步数为单位，但**未显式建模动作序列**（如连续前进 vs. 转向）。\n  - 预测基于历史观测和指令，但**未明确条件化于未来动作假设**，削弱了“想象-评估”策略的能力。\n\n- **泛化能力在复杂多语言场景中受限**\n  - 在 RxR-CE 上表现略逊于 SOTA（SR 66.3% vs. CorrectNav 69.3%），表明对**更复杂路径、多语言指令或稀有场景**的泛化仍有不足。\n  - 仅使用 R2R-CE 和 RxR-CE 训练，未引入大规模合成或跨域数据增强。\n\n- **预测输出为高维特征而非可解释语义**\n  - 预测目标为 DINOv2/SAM 特征和深度图，虽高效，但**缺乏高层语义标签**（如“门”、“楼梯”）。\n  - 导致“里程碑”概念隐式编码，难以进行**可解释性分析或人类对齐验证**。\n\n- **计算与部署成本较高**\n  - 基于 Qwen2.5-VL-3B-Instruct，参数量大，且需同时运行规划、双时域预测与动作解码。\n  - 尽管避免像素生成，但**多查询机制与结构化注意力仍增加推理延迟**，不利于实时机器人部署。\n\n---\n\n### - 未来可能的改进方向\n\n```markdown\n- **构建端到端可微的分层规划器**\n  - 引入强化学习或对比学习，使模型能自主优化子目标划分，摆脱对外部 VLM 的依赖。\n  - 示例：通过轨迹成功率作为奖励信号，联合优化“分解-执行-反馈”闭环。\n\n- **引入动作条件化世界模型**\n  - 扩展预测模块，支持“若执行某动作序列，则环境将如何变化”的反事实推理。\n  - 可借鉴 Dreamer 或 World Model 的 RSSM 架构，实现动作-状态联合建模。\n\n- **增强跨环境与多模态泛化**\n  - 结合合成数据生成（如 Habitat-Matterport 扩展）、跨域自监督预训练，提升鲁棒性。\n  - 探索语音指令、草图输入等多模态引导方式，拓展应用场景。\n\n- **融合显式语义地图与隐式特征预测**\n  - 在隐式特征预测基础上，叠加轻量级语义分割头，输出可读地标标签。\n  - 实现“**隐式想象 + 显式符号**”双通路导航，兼顾效率与可解释性。\n\n- **轻量化与边缘部署优化**\n  - 设计蒸馏方案，将 NavForesee 知识迁移到小型模型（如 700M 参数级 VLM）。\n  - 探索动态推理机制：仅在关键决策点激活长时预测，降低平均计算开销。\n```\n\n---\n\n### - 总结：技术价值与演进路径\n\n| 维度 | 当前贡献 | 未来突破点 |\n|------|--------|-----------|\n| **架构创新** | ✅ 统一 VLM 规划与世界模型 | ⏩ 实现动作条件化想象与反事实推理 |\n| **表示学习** | ✅ 高效特征空间预测 | ⏩ 融合符号语义与神经表征 |\n| **训练范式** | ⚠️ 依赖外部生成数据 | ⏩ 端到端联合优化分层计划 |\n| **应用潜力** | ✅ 复杂室内导航 | ⏩ 户外开放场景、具身操作任务 |\n\n> **核心洞见**：NavForesee 的真正价值在于提出“**语言规划 ↔ 时空想象**”的闭环范式。未来工作应聚焦于**打破预处理依赖、增强因果推理能力、并推动向真实机器人系统的落地迁移**，从而迈向真正具备“心智模拟”能力的具身智能体。",
      "timestamp": "2026-01-09T10:05:18.509088",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    }
  ],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "2025-12-01T11:24:16Z",
  "created_at": "2026-01-09T09:59:20.394952",
  "updated_at": "2026-01-09T09:59:20.394962"
}