{
    "id": "2512.01550v1",
    "title": "NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction",
    "authors": [
        "Fei Liu",
        "Shichao Xie",
        "Minghua Luo",
        "Zedong Chu",
        "Junjun Hu",
        "Xiaolong Wu",
        "Mu Xu"
    ],
    "abstract": "在人工智能领域，基于复杂自然语言指令的具身导航任务，尤其是面向长时程目标的导航，仍然是一项艰巨挑战。现有智能体在面对未知环境时往往难以进行稳健的长期规划，导致任务失败率居高不下。为突破这些局限，我们提出了NavForesee——一种新颖的视觉语言模型，它将高层语言规划与预测性世界模型想象统一在单一框架内。我们的方法使单个视觉语言模型能够同时执行规划与前瞻预测：在完整指令和历史观测的条件下，模型通过学习任务分解、进度追踪及子目标生成来理解导航指令；同时，它作为生成式世界模型，通过预测短期环境动态与长期导航关键节点提供重要前瞻信息。该模型的结构化规划指导其针对性预测，而想象出的未来场景则为导航行动提供丰富上下文，从而形成感知-规划/预测-行动的强大内部反馈循环。通过在R2R-CE和RxR-CE基准上的大量实验，我们证明NavForesee在复杂场景中取得了极具竞争力的性能。本研究彰显了显式语言规划与隐式时空预测相融合的巨大潜力，为开发更智能、更强大的具身智能体开辟了新路径。",
    "url": "https://arxiv.org/abs/2512.01550v1",
    "html_url": "https://arxiv.org/html/2512.01550v1",
    "html_content": "NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction\nFei Liu\n∗\nShichao Xie\n∗\nMinghua Luo Zedong Chu\n†\nJunjun Hu Xiaolong Wu\n†\nMu Xu\nAmap, Alibaba Group\n*Joint first authors\n†\n\\dagger\nCorresponding authors\nAbstract\nEmbodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework.\nOur approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM’s structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.\nI\nIntroduction\nEmbodied navigation, a cornerstone challenge in artificial intelligence, has recently witnessed remarkable progress driven by the advent of Vision-Language Models (VLMs)\n[\n1\n,\n2\n,\n3\n,\n4\n,\n5\n]\n. These models endow agents with the ability to perceive, interpret instructions, and operate in complex environments. Despite these advances, a significant performance gap persists in long-horizon tasks, where agents frequently fail to maintain course, comprehend observations, or make consistently correct decisions. This gap stems from two primary limitations: (1) a planning and memory deficit, as deployable VLMs often have limited context windows and planning capabilities, causing them to get ”lost” in the navigation environment\n[\n6\n,\n7\n,\n8\n]\n; and (2) a lack of predictive foresight, as current models are fundamentally reactive and cannot anticipate future environmental states to guide their actions proactively\n[\n9\n,\n10\n,\n11\n]\n.\nExisting research has pursued these challenges on separate fronts. One trajectory enhances VLM reasoning through curated datasets and Chain-of-Thought (CoT) prompting\n[\n12\n,\n13\n]\n. The other develops world models to predict future states, informing action planning\n[\n14\n,\n15\n]\n. However, a critical oversight is the disconnection between these paradigms. A VLM-centric agent can suffer from semantic hallucinations, where its plan disconnects from visual reality, while a world model without language guidance can experience semantic drift, its predictions becoming untethered from the instructional goal.\nWe posit that VLM planning and predictive foresight should not be separate but unified and mutually reinforcing within a single VLM\n[\n16\n]\n. To this end, we introduce NavForesee as in Figure\nLABEL:fig:navforesee\n, a unified model that integrates multi-modal understanding with world model generation. Our approach is inspired by human navigation, which is not a continuous, low-level process but a hierarchical one centered on milestones. Humans typically navigate by heading towards a sequence of meaningful landmarks, largely ignoring the minutiae of the path between them. We argue that an artificial agent should do the same. NavForesee adopts this strategy by operating through two synergistic functions: (1) Hierarchical Language planning. As a planner, NavForesee generates a high-level plan by summarizing the navigation task into completed sub-instructions, identifying the current sub-instruction, and formulating the next steps as semantic action ”trunks.” This grounds the agent’s planning in the overall instruction. (2) Dual-Horizon Predictive Foresight. As a world model, NavForesee ”imagines” the future on two timescales. For long-term guidance, it predicts the key visual features of the environment at the completion of the current sub-instruction—effectively envisioning the next milestone. For short-term execution, it forecasts immediate future features to enhance local awareness, enabling robust obstacle avoidance and better understanding of environmental dynamics.\nInspired by latent-space world models\n[\n17\n,\n18\n,\n19\n,\n15\n]\n, this prediction deliberately avoids computationally expensive pixel-level generation. Instead, NavForesee forecasts a compact set of high-level features—depth, DINOv2, and SAM features—that capture essential geometric and semantic information as in DreamVLA. The predicted features are fed to an action policy module which is simply an MLP to generate continuous waypoints and flags for arriving or not. By tightly coupling hierarchical planning with dual-horizon predictive foresight, NavForesee generates coherent, goal-oriented actions, guided by both a long-term vision of its milestones and an immediate awareness of its surroundings.\nWe conducted extensive experiments on the R2R-CE\n[\n20\n]\nand RxR-CE\n[\n21\n]\nbenchmarks. Training exclusively on the publicly available R2R-CE and RxR-CE datasets, NavForesee demonstrates highly competitive performance, achieving a Success Rate (SR) of 66.2% and an Oracle Success Rate (OSR) of 78.4% on the R2R-CE benchmark—comparable to state-of-the-art methods.\nIn summary, our key contributions are threefold:\n•\nWe propose NavForesee, a VLN framework that unifies vision–language model (VLM) planning with world model prediction for navigation tasks.\n•\nWe introduce a hierarchical language planning paradigm that addresses long-instruction, goal-oriented missions by explicitly tracking mission progress and generating concise textual sub-plans.\n•\nWe design a dual-horizon world model prediction mechanism for both short-term execution and long-term milestone navigation, implicitly forming a perception–planning and prediction–action loop that guides agent behavior.\nII\nRelated Works\nII-A\nVisual Language Navigation\nVision-and-Language Navigation (VLN) requires an embodied agent to interpret natural language instructions, perceive visual surroundings, and generate a sequence of actions to reach a specified goal. The advent of large-scale pre-trained VLMs has catalyzed significant progress, largely superseding earlier methods based on topological graphs\n[\n22\n,\n23\n,\n24\n]\n, top-down semantic maps\n[\n25\n,\n26\n,\n27\n]\n, or instruction augmentation\n[\n28\n]\n. Recent works leveraging VLMs can be broadly categorized into two main paradigms.\nThe first uses the VLM as a high-level planner, auto-regressively generating action plans\n[\n29\n,\n30\n,\n31\n]\nor textual trajectories\n[\n32\n]\n. While strong in reasoning, this step-by-step generation is prone to error accumulation and slow inference. The second employs the VLM as an end-to-end policy, directly mapping inputs to actions. However, this often leads to overfitting on training scenes and underutilizes the VLM’s high-level reasoning capabilities.\nTo bridge the gap between these two approaches, dual-system architectures have been proposed\n[\n6\n,\n33\n]\n. These models often adopt a ”Fast-and-Slow” reasoning paradigm, combining a deliberative ”slow” system for high-level reasoning with a lightweight ”fast” reactive controller for low-level execution. Reinforcement learning is frequently employed to align the outputs of both systems and bootstrap the learning of coherent reasoning-action patterns. Despite this progress, a fundamental challenge remains: long, complex reasoning chains (e.g., long CoTs) do not always align with the spatial and dynamic realities of the environment. Furthermore, frequent or periodic elaborate reasoning processes may be unnecessary, as human navigation often relies on simpler, high-level semantic plans rather than continuous, detailed deliberation.\nII-B\nNavigation World Model\nA world model is designed to learn a predictive model of an environment, forecasting future states from historical observations and optional conditioning information, such as actions or instructions. Predictions can be generated in either raw pixel space or a more compact latent space\n[\n18\n]\n. The concept has gained significant traction recently, propelled by large-scale video generation models like Sora(\n[\n34\n]\n, which can produce long-term, consistent, even interactive video sequences from text prompts. A key application of world models in robotics is to serve as a simulation engine, allowing an agent to ”imagine” the outcomes of different action sequences and evaluate control policies before execution\n[\n16\n,\n19\n]\n.\nIn the context of visual navigation, recent works have begun to leverage world models to provide agents with environmental foresight. For instance, NavMorph utilizes a Recurrent State-Space Model (RSSM) to model environmental dynamics in a compact latent space, refining the agent’s policy with imagined future states\n[\n11\n]\n. Similarly, HNR\n[\n9\n]\nadvocates for predicting multi-level semantic features instead of raw pixels, enabling faster and higher-quality imagination to evaluate multiple next-step actions in parallel. Other approaches, like NWM\n[\n35\n]\n, use a controlled video generation model to plan entire trajectories through simulation.\nDespite their promise, existing world models for navigation face two primary limitations. First, action-conditioned models that rely on extensive trajectory sampling and evaluation are often computationally prohibitive, rendering them infeasible for deployment on resource-constrained agents. Second, and more critically for our work, prior research has focused almost exclusively on learning environmental dynamics, largely neglecting to integrate this predictive capability with the high-level language reasoning abilities of modern VLMs. This separation leaves a critical gap, which our work aims to address by unifying these two powerful paradigms.\nFigure 1\n:\nVLM-driven hierarchical navigation plan dataset generation. Episodes from R2R-CE and RxR-CE are processed by\nGemini 2.5 Pro\n, which decomposes long instructions into sub-instructions and identifies keyframe milestones. To generation of waypoint-level reasoning labels, waypoints are sampled between milestones annotated with a navigation summary, future plan, and action (\nforward\n,\nleft\n,\nright\n, or\nstop\n).\nIII\nMethods\nFigure 2\n:\nOverall architecture of NavForesee. The model is built on the Qwen2.5-VL-3B-Instruct backbone, integrating two complementary functionalities: (1) VLM-based hierarchical planning and (2) world model-based dual-horizon visual prediction. For hierarchical planning, textual instruction and visual observations are encoded via Qwen’s original multimodal encoders to produce auto-regressive sub-goal plans. For prediction, a position encoder encodes the agent’s relative pose, and short- and long-horizon dream queries (depth and semantic subqueries) are appended to multimodal embeddings. These queries, processed through structured attention, feed lightweight convolutional decoders for environmental predictions and an MLP head for navigation actions.\nIII-A\nProblem Formulation\nWe target instruction-guided navigation missions in which an embodied agent must interpret a natural language instruction\nl\nl\nand navigate from a given start position to an intended goal location, strictly following the described route. The challenge lies in robustly understanding the instruction, maintaining situational awareness over long horizons, and deciding actions that lead to successful navigation in unseen environments.\nAt time step\nt\nt\n, the agent perceives the environment and obtains a panoramic RGB observation\no\nt\no_{t}\n. It maintains a memory of the past\nH\nH\nobservations,\nO\nt\n−\nH\n:\nt\n−\n1\n=\n[\no\nt\n−\nH\n,\n…\n,\no\nt\n−\n1\n]\nO_{t-H:t-1}=[o_{t-H},\\dots,o_{t-1}]\n, to support temporal reasoning. The navigation policy produces a sequence of\nK\nK\nfuture waypoints\nw\nt\n:\nt\n+\nK\n∈\nℝ\nK\n×\n5\nw_{t:t+K}\\in\\mathbb{R}^{K\\times 5}\n, where each waypoint is defined as\nw\nt\n=\n[\nx\nt\n,\ny\nt\n,\nsin\n⁡\nθ\nt\n,\ncos\n⁡\nθ\nt\n,\nc\nt\n]\n,\nw_{t}=\\left[x_{t},y_{t},\\sin\\theta_{t},\\cos\\theta_{t},c_{t}\\right],\nwith\n(\nx\nt\n,\ny\nt\n)\n(x_{t},y_{t})\ndenoting planar positions,\nθ\nt\n\\theta_{t}\nthe heading angle, and the binary flag\nc\nt\nc_{t}\nindicating whether a\nstop\naction should be triggered. Unless all predicted actions are marked as\nstop\n, the agent continuously moves following the generated waypoints.\nTo solve this problem, we adopt Qwen2.5-VL as our backbone and extend it with two complementary modules. First, we enable\nhierarchical planning\nby decomposing the full instruction into sequential sub-instructions, identifying completed ones and predicting the next step under the current context—leveraging the model’s language understanding capabilities and pretraining on our constructed dataset. Second, we integrate\nworld model foresight\nfor predicting short- and long-term environmental changes, enhancing vision–language coherence and yielding more reliable action policies. Together, these capabilities allow the agent to imitate human navigation behaviors, combining explicit language planning with implicit spatiotemporal prediction.\nIII-B\nVLM-driven Hierarchical Planning Dataset\nWe construct a hierarchical language planning dataset specifically for instruction-guided navigation missions, leveraging advanced Vision–Language Models (VLMs) for multi-modal understanding and sequence analysis. Our goal is to provide training data that captures both short-term execution steps and long-term navigation milestones.\nAs illustrated in Figure\n1\n, we start from public Vision-and-Language Navigation (VLN) benchmarks—R2R-CE (10k episodes) and RxR-CE (20k episodes)—which provide paired natural language instructions and full image observation sequences. Each episode is processed with\nGemini 2.5 Pro\n, guided by a custom prompt template that specifies the model’s role, defines the mission, outlines analytical steps, and enforces an explicit output format. The VLM systematically decomposes each long instruction into a series of sequential sub-instructions, while identifying a dense visual chain of keyframes representing navigation milestones. For paths involving extended travel or sharp turns, we require the inclusion of intermediate milestones to maintain visual continuity in the generated plan. This hierarchical structure enables downstream world models to better learn both short-term and long-term prediction.\nFor every annotated episode, the output is standardized to include: the milestone frame index, the textual description of the completed sub-instruction, and the upcoming planned instruction. Post-processing involves filtering incomplete annotations, correcting logical inconsistencies in the VLM outputs, and converting each episode into multiple navigation segments. We sample waypoints along each trajectory, with each waypoint forming the endpoint of a segment between milestones. Each sampled waypoint is assigned a\nplanning label\ncomprising: (1) a navigation summary (completed sub-instruction), (2) a future plan (next instruction), and (3) a language action (\nforward\n,\nleft\n,\nright\n,\nstop\n).\nThis pipeline produces approximately\n1.3\n1.3\nM training samples from RxR-CE and\n0.2\n0.2\nM from R2R-CE. To ensure balanced training data, we down-sample over-represented straight-motion cases and augment examples involving stopping actions. The final dataset provides richly annotated, balanced samples for training the hierarchical language planning and predictive modules in NavForesee.\nIII-C\nModel Architecture\nOverall Architecture\nThe overall architecture of NavForesee is illustrated in Figure\n2\n. We adopt Qwen2.5-VL-3B-Instruct\n[\n36\n]\n, a large-scale vision–language model with strong multi-modal understanding capabilities, as the backbone. NavForesee is designed to integrate two complementary functionalities: VLM-based language planning and World model-based visual prediction.\nCorrespondingly, we define two primary training objectives: VLM planning training and world model training. Training data from both tasks are jointly mixed to ensure that the model preserves its multi-modal planning ability, while simultaneously extending its capability to generate visual features.\nFor the VLM planning training, textual planning data are directly fed into Qwen for auto-regressive training, leveraging its original text encoder and image encoder components without modification.\nFor the world model training, we introduce an additional position encoder (pos-encoder) to encode the agent’s relative position and orientation from image observations. Two sets of dream queries—corresponding to short- and long-horizon predictions—are appended to the multi-modal embeddings. Each set of dream queries includes depth and semantics subqueries, enabling dual-horizon prediction. Furthermore, an action query, alongside the dream queries, is integrated into the multi-modal inputs and processed by Qwen2.5-VL via a structured attention mechanism. Lightweight convolutional layers serve as decoders to transform dream embeddings into environmental predictions (depth and semantics), while a simple MLP predicts action outputs (waypoints, orientation estimates, and arrival flags).\nStructured Attention Mask\nTo maintain a clear separation between short- and long-horizon predictions and to avoid cross-type contamination, each dream query type (depth and semantics) is explicitly decomposed into short-horizon and long-horizon components. As illustrated in Figure\n2\n, we design a structured attention mask tailored for dual-horizon prediction. Long-horizon predictions naturally depend on short-horizon predictions, using them as guidance to ensure temporal coherence. Mutual attention between depth and semantics queries is masked to prevent cross-modal leakage or unintended feature mixing. In contrast, the action query attends to all available information—including past context and both horizons of dream queries—enabling it to make globally consistent navigation predictions.\nIII-D\nDual-horizon World Model Prediction\nSpecifically, to enable accurate dual-horizon environmental feature prediction, we employ the world model architecture that serves as guidance for learning the inverse dynamics of a navigation agent. Here, short-term prediction refers to generating forecasts for\nk\nk\nsteps ahead, while long-term prediction targets navigation milestones, corresponding to an adaptive horizon determined by progress toward the next milestone.\nFor visual feature prediction within Qwen2.5-VL, we introduce two sets of learnable dream queries, namely the short-term\nQ\nS\n∈\nℝ\nL\n×\nd\nQ_{S}\\in\\mathbb{R}^{L\\times d}\nand and long-term\nQ\nL\n∈\nℝ\nL\n×\nd\nQ_{L}\\in\\mathbb{R}^{L\\times d}\nto, which extract temporally aligned feature embeddings specialized for prediction at distinct horizons. To enhance the model’s capability in capturing spatial-temporal correlations and learning environmental dynamics, we further integrate position-orientation state embeddings\ns\nt\n−\nH\n:\nt\ns_{t-H:t}\nfor each input frame through an encoder\nh\n(\n.\n)\nh(.)\n.\nThese dream queries are concatenated with textual instruction embeddings\nl\nl\nand visual observation sequences\nO\nt\n−\nH\n:\nt\nO_{t-H:t}\nand processed by the Qwen2.5-VL backbone\nf\n(\n.\n)\nf(.)\n. Specially,\nE\nS\n=\nf\n​\n(\nl\n,\nO\nt\n−\nH\n:\nt\n,\nh\n​\n(\ns\nt\n−\nH\n:\nt\n)\n|\nQ\nS\n)\n,\n\\displaystyle E_{S}=f(l,O_{t-H:t},h(s_{t-H:t})|Q_{S}),\nE\nL\n=\nf\n​\n(\nl\n,\nO\nt\n−\nH\n:\nt\n,\nh\n​\n(\ns\nt\n−\nH\n:\nt\n)\n,\nQ\nS\n|\nQ\nL\n)\n\\displaystyle E_{L}=f(l,O_{t-H:t},h(s_{t-H:t}),Q_{S}|Q_{L})\nwhere causal attention masking ensures auto-regressive generation: short-term embeddings are produced first, and long-term embeddings are conditioned on short-term predictions.\nWe design lightweight decoders to interpret\nE\nL\nE_{L}\nand\nE\nS\nE_{S}\ninto predicted depth\nd\np\nd_{p}\n, and high-level semantics\nc\np\nc_{p}\n(e.g. derived from DINOV2, SAM). Short-term predictions correspond to a fixed horizon\nk\nk\nwhereas long-term predictions adaptively extrapolate over\nM\nt\nM_{t}\nsteps, dependent on the agent’s progress toward the next milestone:\np\nt\n+\nk\n=\nD\n​\n(\nE\nS\n)\n=\n[\nd\np\n​\n(\nt\n)\n,\nc\np\n​\n(\nt\n)\n]\n,\n\\displaystyle p_{t+k}=D(E_{S})=[d_{p}(t),c_{p}(t)],\np\nt\n+\nM\nt\n=\nD\n​\n(\nE\nL\n)\n=\n[\nd\np\n​\n(\nt\n+\nM\nt\n)\n,\nc\np\n​\n(\nt\n+\nM\nt\n)\n]\n\\displaystyle p_{t+M_{t}}=D(E_{L})=[d_{p}(t+M_{t}),c_{p}(t+M_{t})]\nIII-E\nPredictive Action Policy Learning\nGiven two temporally order states\no\nt\no_{t}\nand\no\nt\n+\n1\no_{t+1}\n, the intermediate action\na\n^\n​\n(\nt\n)\n\\hat{a}(t)\ncan be inferred via inverse dynamics. We leverage this principle to learn an action policy conditioned on the instruction\nl\nl\n, historical visual observations\nO\nt\n−\nH\n:\nt\nO_{t-H:t}\nand the dual-horizon predictive latent features\nE\nS\nE_{S}\nand\nE\nL\nE_{L}\ngenerated by the world model.\nTo enhance the encoding of task-relevant context for action prediction, we introduce a learnable action query\nQ\na\nQ_{a}\n. This query is concatenated with the dream queries and multi-modal input embeddings to form a unified action embedding. The Qwen2.5-VL backbone processes these embeddings to produce the contextual representation for action inference, which is subsequently projected into the action space:\nE\na\n=\nf\n​\n(\nl\n,\nO\nt\n−\nH\n:\nt\n,\nh\n​\n(\ns\nt\n−\nH\n:\nt\n)\n,\nQ\nS\n,\nQ\nL\n|\nQ\na\n)\n\\displaystyle E_{a}=f(l,O_{t-H:t},h(s_{t-H:t}),Q_{S},Q_{L}|Q_{a})\na\n^\nt\n:\nt\n+\nk\n=\nM\ni\n​\nn\n​\nv\n​\n(\nE\nS\n,\nE\nL\n|\nE\na\n)\n\\displaystyle\\hat{a}_{t:t+k}=M_{inv}(E_{S},E_{L}|E_{a})\nwhere\nE\na\nE_{a}\nis the action embedding and\nM\ni\n​\nn\n​\nv\nM_{i}nv\ndenotes the inverse dynamics model.\nNotably, in our action policy learning pipeline, the action embedding\nE\na\nE_{a}\nis extracted through the Qwen2.5-VL backbone, while action predictions are primarily conditioned on the dual-horizon predictive features, ensuring that decisions are informed by both past observations and forecasted environmental dynamics\nIII-F\nClose the Planning/Prediction and Action Loop\nFor VLM planning training, we finetune Qwen2.5-VL model based on the constructed dataset in an auto-regressive manner independently to build a powerful model capable of conducting hierarchical navigation.\nFor world model prediction and action policy learning, the training tasks are classified depth prediction, semantic feature prediction and action prediction. Depth prediction error\nL\nd\nL_{d}\nis measured using the Scale-invariant Logarithmic Loss (SiLogLoss) at the pix-level level. The semantics feature prediction error\nL\nc\nL_{c}\nand action error\nL\na\nL_{a}\nare computed using mean squared error (MSE). The overall training loss\nL\nL\ncomprise\nL\nd\nL_{d}\n,\nL\nc\nL_{c}\nand\nL\na\nL_{a}\nL\n=\nα\n​\nL\nd\n+\nβ\n​\nL\nc\n+\nL\na\nL=\\alpha L_{d}+\\beta L_{c}+L_{a}\nwhere\nα\n\\alpha\nand\nβ\n\\beta\nare weighting hyperparameters balancing the tasks.\nIV\nExperimental Evaluation\nTABLE I:\nComparison with other methods on the Val-Unseen split of R2R-CE and RxR-CE\nMethod\nObservation\nR2R-CE Val-Unseen\nRxR-CE Val-Unseen\nS.RGB\nPano.\nDepth\nOdo\nNE\n↓\n\\downarrow\nOS\n↑\n\\uparrow\nSR\n↑\n\\uparrow\nSPL\n↑\n\\uparrow\nNE\n↓\n\\downarrow\nSR\n↑\n\\uparrow\nSPL\n↑\n\\uparrow\nHPN+DN*\n[\n37\n]\n✓\n✓\n✓\n6.31\n40.0\n36.0\n34.0\n-\n-\n-\nCMA*\n[\n38\n]\n✓\n✓\n✓\n6.20\n52.0\n41.0\n36.0\n8.76\n26.5\n22.1\nSim2Sim\n[\n39\n]\n✓\n✓\n✓\n6.07\n52.0\n43.0\n36.0\n8.76\n26.5\n22.1\nGridMM*\n[\n8\n]\n✓\n✓\n✓\n5.11\n61.0\n49.0\n41.0\n-\n-\n-\nDreamWalker*\n[\n40\n]\n✓\n✓\n✓\n5.53\n59.0\n49.0\n44.0\n-\n-\n-\nReborn*\n[\n41\n]\n✓\n✓\n✓\n5.40\n57.0\n50.0\n46.0\n5.98\n48.6\n42.0\nETPNav*\n[\n42\n]\n✓\n✓\n✓\n4.71\n65.0\n57.0\n49.0\n5.64\n54.7\n44.8\nHNR*\n[\n9\n]\n✓\n✓\n✓\n4.42\n67.0\n61.0\n51.0\n5.50\n56.3\n46.7\nAG-CMTP\n[\n43\n]\n✓\n✓\n✓\n7.90\n39.0\n23.0\n19.0\n-\n-\n-\nR2R-CMTP\n[\n43\n]\n✓\n✓\n✓\n7.90\n38.0\n26.0\n22.0\n-\n-\n-\nInstruc-Nav\n[\n31\n]\n✓\n✓\n✓\n6.89\n-\n31.0\n24.0\n-\n-\n-\nLAW\n[\n44\n]\n✓\n✓\n✓\n6.83\n44.0\n35.0\n31.0\n10.90\n8.0\n8.0\nCM2\n[\n45\n]\n✓\n✓\n✓\n7.02\n41.0\n34.0\n27.0\n-\n-\n-\nWS-MGMap\n[\n46\n]\n✓\n✓\n✓\n6.28\n47.0\n38.0\n34.0\n-\n-\n-\nAO-Planner\n[\n47\n]\n✓\n✓\n5.55\n59.0\n47.0\n33.0\n-\n-\n-\nSeq2Seq\n[\n48\n]\n✓\n✓\n7.77\n37.0\n25.0\n22.0\n12.10\n13.9\n11.9\nCMA\n[\n48\n]\n✓\n✓\n7.37\n40.0\n32.0\n30.0\n-\n-\n-\nNA Vid\n[\n49\n]\n✓\n5.47\n49.0\n37.0\n35.0\n-\n-\n-\nUni-NA Vid\n[\n50\n]\n✓\n5.58\n53.5\n47.0\n42.7\n6.24\n48.7\n40.9\nNaVILA\n[\n51\n]\n✓\n5.22\n62.5\n54.0\n49.0\n6.77\n49.3\n44.0\nStream VLN\n[\n52\n]\n✓\n4.98\n64.2\n56.9\n51.9\n6.22\n52.9\n46.0\nCorrectNav\n[\n53\n]\n✓\n4.24\n67.5\n65.1\n62.3\n4.09\n69.3\n63.3\nNavForesee(Ours)\n✓\n3.94\n78.4\n66.2\n59.7\n4.20\n66.3\n53.2\nTABLE II:\nPerformance comparison between VLM planning and dual-horizon world model prediction\nIndex\nVLM planning\nLong-term prediction\nShort-term prediction\nSR\n↑\n\\uparrow\nOSR\n↑\n\\uparrow\nNE\n↓\n\\downarrow\nSPL\n↑\n\\uparrow\n1\n✓\n✓\n✓\n66.2%\n78.4%\n3.94\n59.7%\n2\n✗\n✓\n✓\n48.8%\n75.5%\n5.61\n39.4%\n3\n✓\n✗\n✓\n58.6%\n76.4%\n4.47\n50.1%\n4\n✗\n✗\n✗\n52.6%\n67.4%\n5.53\n46.7%\nWe evaluate our model in continuous environment of the Habitant simulator on the R2R-CE and RxR-CE datasets.\nR2R-CE\ndataset is derived from the Matterport3D indoor environments, discretized for path planning but operationalized in the Habitat simulator under a continuous navigation setting. It provides fine-grained, step-by-step natural language instructions, allowing for detailed guidance at each navigation step. In the simulator, the embodied agent\ncan execute turns as small as\n15\n∘\n15^{\\circ}\nand perceives the scene through a\n90\n∘\n90^{\\circ}\nhorizontal field-of-view.\nRxR-CE\nis a large-scale, multilingual VLN dataset comprising about 126K human-annotated instructions. Compared to R2R-CE, RxR-CE covers more diverse and complex trajectories, increasing the difficulty of the navigation tasks. The agent in this setting uses a coarser minimum turn\nincrement of\n30\n∘\n30^{\\circ}\nand a narrower\n79\n∘\n79^{\\circ}\nhorizontal field-ofview, which demands more deliberate movement planning for effective scene coverage.\nWe evaluate navigation performance using standard metrics including success rate (SR), oracle success rate (OS), success weighted by path length (SPL), and navigation error (NE).\nIV-A\nComparison with State-of-the-Art Methods\nTable 1 reports the performance of the proposed method compared with other approaches on the R2R‑CE and RxR‑CE datasets. Overall, NavForesee delivers competitive results against state‑of‑the‑art (SOTA) methods.\nSpecifically, on the val unseen split of the R2R‑CE dataset, NavForesee achieves SOTA performance by improving SR by 1.1%, OSR by 10.9%, and reducing NE by 0.3 m. This improvement can be attributed to the integration of the world model prediction module, which enables the agent to better capture environmental dynamics, avoid obstacles, and explore the surroundings more effectively.\nIn contrast, NavForesee performs slightly worse than SOTA methods on RxR‑CE, indicating limited generalization to more complex environments. It is worth noting that we train soly on NavForesee on R2R‑CE and RxR‑CE datasets, whereas other methods exploit diverse and large‑scale datasets to enhance generalization.\nAlthough NavForesee does not consistently outperform all baselines, it achieves the highest OSR across both datasets. This demonstrates the value of incorporating world model prediction into VLN agents and implies its promising potential for future vision‑and‑language navigation tasks.\nIV-B\nAblation Study\nAs shown in Table\nII\n, removing any of the three key modules—hierarchical VLM planning, long-term prediction, or short-term prediction—results in clear performance degradation. The full NavForesee model, which combines all modules, achieves the highest SR (66.2%), OSR (78.4%), lowest NE (3.94), and best SPL (59.7%), validating the benefit of their integration. Without VLM planning, the success rate drops sharply to 48.8% and the SPL decreases by more than 16 points, reflecting the importance of explicit instruction decomposition and progress tracking for efficient navigation. Disabling long-term prediction also leads to a noticeable reduction in SR (58.6%) and higher NE, highlighting the role of milestone foresight in providing strategic guidance over extended trajectories. When all three modules are removed, navigation quality deteriorates the most, confirming that planning and both prediction horizons together are crucial for accurate, efficient long-horizon navigation.\nFigure 3\n:\nShort-term depth and semantics predictions. From top to bottom: frames with timestamps, future ground truth frames with timestamps, future depth prediction for future frames, semantics predictions for future frames. Semantic features are DinoV2 features and visualized by a pretrained segmentation head. Instructions: UP the stairs. Turn to the left and enter into the second open door on the left. Walk towards the foot of the bed. Turn right and enter the open door to the bathroom\nFigure 4\n:\nNavForesee’s geometric-semantic feature imagination across different motion modes. The model accurately predicts environmental dynamics in straight motion, generalizes effectively to turning scenarios, and infers detailed object geometry and depth distribution from minimal visual input, such as a brief glimpse into a room\nIV-C\nQualitative Analysis\nFigure\n3\nillustrates the short-term depth and semantic feature predictions generated by our world model over the course of a complete navigation episode, forecasting up to four future steps. Although the predicted depth maps appear somewhat coarse—owing to the constraints of pixel-level supervised training on R2R-CE and RxR-CE—they nonetheless preserve the scene’s global geometry and spatial layout, faithfully capturing agent movements such as ascending or descending staircases, entering and exiting rooms, and making sharp or gradual turns. This ability to retain high-level spatial coherence despite reduced pixel detail ensures that the model’s predictions remain informative for downstream navigation decisions. The semantics predictions, obtained via a pretrained segmentation head, exhibit strong alignment with ground truth labels, successfully reflecting dynamic environmental changes in synchrony with the agent’s actions.\nFigure\n4\nfurther provides detailed examples that showcase NavForesee’s ability to imaginatively anticipate semantic features across diverse motion patterns. In addition to delivering accurate environment dynamics predictions when following a straightforward trajectory, NavForesee demonstrates remarkable generalization by reliably extrapolating future geometric and semantic structures when performing more complex navigational behaviors such as turns. In the final scenario, the agent receives only a brief partial observation—a quick glance into a room—yet the model is able to produce a vivid and coherent internal imagination of the room’s layout. This includes accurately inferring the relative shape and position of the bed, as well as estimating the depth distribution across the room, thus indicating its capacity to reason about unseen spatial regions.\nV\nConclusion\nWe proposed NavForesee, a vision–language navigation framework that unifies hierarchical language planning with dual-horizon predictive world modeling. By decomposing long instructions into sub-goals and anticipating both short-term dynamics and long-term milestones, NavForesee forms an implicit perception–planning and prediction–action loop.\nExperiments on R2R-CE and RxR-CE show strong performance—66.2% SR and 78.4% OSR on R2R-CE—comparable to state-of-the-art despite training only on public data. Qualitative results further reveal solid depth and semantics predictions that guide agent decisions in complex scenarios.\nThese findings highlight the benefit of equipping embodied agents with foresight: by “foreseeing” future states, NavForesee effectively fuses language planning with spatiotemporal imagination to improve visual-language navigation.\nReferences\n[1]\nW.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang,\nY. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing, “Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality,” March\n2023. [Online]. Available:\nhttps://lmsys.org/blog/2023-03-30-vicuna/\n[2]\nS. Chen, X. Chen, C. Zhang, M. Li, G. Yu, H. Fei, H. Zhu, J. Fan, and T. Chen,\n“Ll3da: Visual interactive instruction tuning for omni-3d understanding,\nreasoning, and planning,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2311.18651\n[3]\nA.-M. Halacheva, J.-N. Zaech, X. Wang, D. P. Paudel, and L. V. Gool,\n“Gaussianvlm: Scene-centric 3d vision-language models using language-aligned\ngaussian splats for embodied reasoning and beyond,” 2025. [Online].\nAvailable:\nhttps://arxiv.org/abs/2507.00886\n[4]\nH. Huang, Y. Chen, Z. Wang, R. Huang, R. Xu, T. Wang, L. Liu, X. Cheng,\nY. Zhao, J. Pang, and Z. Zhao, “Chat-scene: Bridging 3d scene and large\nlanguage models with object identifiers,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2312.08168\n[5]\nH. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” 2023.\n[Online]. Available:\nhttps://arxiv.org/abs/2304.08485\n[6]\nQ. Liu, T. Huang, Z. Zhang, and H. Tang, “Nav-r1: Reasoning and navigation in\nembodied scenes,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2509.10884\n[7]\nH. Zhou, J. Yu, and W. Yang, “Dual memory units with uncertainty regulation\nfor weakly supervised video anomaly detection,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2302.05160\n[8]\nZ. Wang, X. Li, J. Yang, Y. Liu, and S. Jiang, “Gridmm: Grid memory map for\nvision-and-language navigation,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2307.12907\n[9]\nZ. Wang, X. Li, J. Yang, Y. Liu, J. Hu, M. Jiang, and S. Jiang, “Lookahead\nexploration with neural radiance representation for continuous\nvision-language navigation,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2404.01943\n[10]\nX. Zhao, W. Cai, L. Tang, and T. Wang, “Imaginenav: Prompting vision-language\nmodels as embodied navigator through scene imagination,” 2024. [Online].\nAvailable:\nhttps://arxiv.org/abs/2410.09874\n[11]\nX. Yao, J. Gao, and C. Xu, “Navmorph: A self-evolving world model for\nvision-and-language navigation in continuous environments,” 2025. [Online].\nAvailable:\nhttps://arxiv.org/abs/2506.23468\n[12]\nB. Lin, Y. Nie, Z. Wei, J. Chen, S. Ma, J. Han, H. Xu, X. Chang, and X. Liang,\n“Navcot: Boosting llm-based vision-and-language navigation via learning\ndisentangled reasoning,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2403.07376\n[13]\nS. Wang, Y. Wang, W. Li, X. Cai, Y. Wang, M. Chen, K. Wang, Z. Su, D. Li, and\nZ. Fan, “Aux-think: Exploring reasoning strategies for data-efficient\nvision-language navigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2505.11886\n[14]\nQ. Zhao, Y. Lu, M. J. Kim, Z. Fu, Z. Zhang, Y. Wu, Z. Li, Q. Ma, S. Han,\nC. Finn, A. Handa, M.-Y. Liu, D. Xiang, G. Wetzstein, and T.-Y. Lin,\n“Cot-vla: Visual chain-of-thought reasoning for vision-language-action\nmodels,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2503.22020\n[15]\nY. Huang, J. Zhang, S. Zou, X. Liu, R. Hu, and K. Xu, “Ladi-wm: A latent\ndiffusion-based world model for predictive manipulation,” 2025. [Online].\nAvailable:\nhttps://arxiv.org/abs/2505.11528\n[16]\nJ. Cen, C. Yu, H. Yuan, Y. Jiang, S. Huang, J. Guo, X. Li, Y. Song, H. Luo,\nF. Wang, D. Zhao, and H. Chen, “Worldvla: Towards autoregressive action\nworld model,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2506.21539\n[17]\nE. Karypidis, I. Kakogeorgiou, S. Gidaris, and N. Komodakis, “Dino-foresight:\nLooking into the future with dino,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2412.11673\n[18]\nF. Baldassarre, M. Szafraniec, B. Terver, V. Khalidov, F. Massa, Y. LeCun,\nP. Labatut, M. Seitzer, and P. Bojanowski, “Back to the features: Dino as a\nfoundation for video world models,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2507.19468\n[19]\nW. Zhang, H. Liu, Z. Qi, Y. Wang, X. Yu, J. Zhang, R. Dong, J. He, F. Lu,\nH. Wang, Z. Zhang, L. Yi, W. Zeng, and X. Jin, “Dreamvla: A\nvision-language-action model dreamed with comprehensive world knowledge,”\n2025. [Online]. Available:\nhttps://arxiv.org/abs/2507.04447\n[20]\nJ. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, “Beyond the\nnav-graph: Vision-and-language navigation in continuous environments,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2004.02857\n[21]\nA. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge, “Room-across-room:\nMultilingual vision-and-language navigation with dense spatiotemporal\ngrounding,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/2010.07954\n[22]\nY. Hong, C. Rodriguez-Opazo, Y. Qi, Q. Wu, and S. Gould, “Language and visual\nentity relationship graph for agent navigation,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/2010.09304\n[23]\nZ. Deng, K. Narasimhan, and O. Russakovsky, “Evolving graphical planner:\nContextual global planning for vision-and-language navigation,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2007.05655\n[24]\nS. Chen, P.-L. Guhur, M. Tapaswi, C. Schmid, and I. Laptev, “Think global, act\nlocal: Dual-scale graph transformer for vision-and-language navigation,”\n2022. [Online]. Available:\nhttps://arxiv.org/abs/2202.11742\n[25]\nM. Z. Irshad, N. C. Mithun, Z. Seymour, H.-P. Chiu, S. Samarasekera, and\nR. Kumar, “Sasra: Semantically-aware spatio-temporal reasoning agent for\nvision-and-language navigation in continuous environments,” 2021. [Online].\nAvailable:\nhttps://arxiv.org/abs/2108.11945\n[26]\nG. Georgakis, K. Schmeckpeper, K. Wanchoo, S. Dan, E. Miltsakaki, D. Roth, and\nK. Daniilidis, “Cross-modal map learning for vision and language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.05137\n[27]\nP. Chen, D. Ji, K. Lin, R. Zeng, T. H. Li, M. Tan, and C. Gan,\n“Weakly-supervised multi-granularity map learning for vision-and-language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2210.07506\n[28]\nS. Zhang, Y. Qiao, Q. Wang, L. Guo, Z. Wei, and J. Liu, “Flexvln: Flexible\nadaptation for diverse vision-and-language navigation tasks,” 2025.\n[Online]. Available:\nhttps://arxiv.org/abs/2503.13966\n[29]\nY. Long, X. Li, W. Cai, and H. Dong, “Discuss before moving: Visual language\nnavigation via multi-expert discussions,” in\n2024 IEEE International\nConference on Robotics and Automation (ICRA)\n, 2024, pp. 17 380–17 387.\n[30]\nP. Chen, X. Sun, H. Zhi, R. Zeng, T. H. Li, G. Liu, M. Tan, and C. Gan,\n“\na\n2\na^{2}\nnav: Action-aware zero-shot robot navigation by exploiting\nvision-and-language ability of foundation models,” 2023. [Online].\nAvailable:\nhttps://arxiv.org/abs/2308.07997\n[31]\nY. Long, W. Cai, H. Wang, G. Zhan, and H. Dong, “Instructnav: Zero-shot system\nfor generic instruction navigation in unexplored environment,” 2024.\n[Online]. Available:\nhttps://arxiv.org/abs/2406.04882\n[32]\nY. Wang, Y. Fang, T. Wang, Y. Feng, Y. Tan, S. Zhang, P. Liu, Y. Ji, and R. Xu,\n“Dreamnav: A trajectory-based imaginative framework for zero-shot\nvision-and-language navigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2509.11197\n[33]\nX. Xue, J. Hu, M. Luo, X. Shichao, J. Chen, Z. Xie, Q. Kuichen, G. Wei, M. Xu,\nand Z. Chu, “Omninav: A unified framework for prospective exploration and\nvisual-language navigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2509.25687\n[34]\nY. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun,\nJ. Gao, L. He, and L. Sun, “Sora: A review on background, technology,\nlimitations, and opportunities of large vision models,” 2024. [Online].\nAvailable:\nhttps://arxiv.org/abs/2402.17177\n[35]\nA. Bar, G. Zhou, D. Tran, T. Darrell, and Y. LeCun, “Navigation world\nmodels,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2412.03572\n[36]\nS. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang,\nJ. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu,\nY. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and\nJ. Lin, “Qwen2.5-vl technical report,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2502.13923\n[37]\nJ. Krantz, A. Gokaslan, D. Batra, S. Lee, and O. Maksymets, “Waypoint models\nfor instruction-guided navigation in continuous environments,” 2021.\n[Online]. Available:\nhttps://arxiv.org/abs/2110.02207\n[38]\nY. Hong, Z. Wang, Q. Wu, and S. Gould, “Bridging the gap between learning in\ndiscrete and continuous environments for vision-and-language navigation,”\n2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.02764\n[39]\nJ. Krantz and S. Lee, “Sim-2-sim transfer for vision-and-language navigation\nin continuous environments,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2204.09667\n[40]\nH. Wang, W. Liang, L. V. Gool, and W. Wang, “Dreamwalker: Mental planning for\ncontinuous vision-language navigation,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2308.07498\n[41]\nD. An, Z. Wang, Y. Li, Y. Wang, Y. Hong, Y. Huang, L. Wang, and J. Shao, “1st\nplace solutions for rxr-habitat vision-and-language navigation competition\n(cvpr 2022),” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2206.11610\n[42]\nD. An, H. Wang, W. Wang, Z. Wang, Y. Huang, K. He, and L. Wang, “Etpnav:\nEvolving topological planning for vision-language navigation in continuous\nenvironments,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2304.03047\n[43]\nK. Chen, J. K. Chen, J. Chuang, M. Vázquez, and S. Savarese, “Topological\nplanning with transformers for vision-and-language navigation,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2012.05292\n[44]\nS. Raychaudhuri, S. Wani, S. Patel, U. Jain, and A. X. Chang,\n“Language-aligned waypoint (law) supervision for vision-and-language\nnavigation in continuous environments,” 2021. [Online]. Available:\nhttps://arxiv.org/abs/2109.15207\n[45]\nG. Georgakis, K. Schmeckpeper, K. Wanchoo, S. Dan, E. Miltsakaki, D. Roth, and\nK. Daniilidis, “Cross-modal map learning for vision and language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.05137\n[46]\nP. Chen, D. Ji, K. Lin, R. Zeng, T. H. Li, M. Tan, and C. Gan,\n“Weakly-supervised multi-granularity map learning for vision-and-language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2210.07506\n[47]\nJ. Chen, B. Lin, X. Liu, L. Ma, X. Liang, and K.-Y. K. Wong,\n“Affordances-oriented planning using foundation models for continuous\nvision-language navigation,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2407.05890\n[48]\nJ. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, “Beyond the\nnav-graph: Vision-and-language navigation in continuous environments,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2004.02857\n[49]\nJ. Zhang, K. Wang, R. Xu, G. Zhou, Y. Hong, X. Fang, Q. Wu, Z. Zhang, and\nH. Wang, “Navid: Video-based vlm plans the next step for vision-and-language\nnavigation,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2402.15852\n[50]\nJ. Zhang, K. Wang, S. Wang, M. Li, H. Liu, S. Wei, Z. Wang, Z. Zhang, and\nH. Wang, “Uni-navid: A video-based vision-language-action model for unifying\nembodied navigation tasks,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2412.06224\n[51]\nA.-C. Cheng, Y. Ji, Z. Yang, Z. Gongye, X. Zou, J. Kautz, E. Bıyık, H. Yin,\nS. Liu, and X. Wang, “Navila: Legged robot vision-language-action model for\nnavigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2412.04453\n[52]\nM. Wei, C. Wan, X. Yu, T. Wang, Y. Yang, X. Mao, C. Zhu, W. Cai, H. Wang,\nY. Chen, X. Liu, and J. Pang, “Streamvln: Streaming vision-and-language\nnavigation via slowfast context modeling,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2507.05240\n[53]\nZ. Yu, Y. Long, Z. Yang, C. Zeng, H. Fan, J. Zhang, and H. Dong, “Correctnav:\nSelf-correction flywheel empowers vision-language-action navigation model,”\n2025. [Online]. Available:\nhttps://arxiv.org/abs/2508.10416\nSupplementary Material\nI\nImplementation Details\nI-A\nModel Architecture\nBase Model\nWe employ Qwen2.5-VL-3B-Instruct\n[\n36\n]\nas the backbone of NavForesee. It adopts the Qwen2.5 LLM as its text decoder and integrates a vision encoder. The vision encoder utilizes a Vision Transformer (ViT) architecture to encode visual observations, while the text decoder is responsible for generating the hierarchical planning outputs and action trunk predictions. Detailed descriptions of Qwen2.5-VL can be found in\n[\n36\n]\n. For hierarchical planning, we directly use the original multimodal encoders and text decoder of Qwen2.5-VL without any modifications. For world model prediction and action policy learning, we introduce a position encoder to represent the agent’s relative position and orientation derived from image observations.\nLightweight decoders transform the dream query embeddings into environmental predictions (depth and semantics), while a simple MLP predicts action outputs (waypoints, orientation estimates, and arrival flags).\nDream Query Design\nTwo sets of dream queries (short-term and long-term), along with an action query, are appended to the multimodal embeddings. Each set of dream queries contains depth and semantics subqueries, enabling dual-horizon prediction. We use DINOv2 and SAM features as semantic representations. Thus, there are six query subsets in total—depth, DINOv2, and SAM for both short-term and long-term horizons—with each subset consisting of 64 tokens. The action query consists of a single token dedicated to action prediction.\nWorld Model Decoders\nWe design task-specific lightweight world model decoders to transform dream embeddings into depth maps, semantic features, and actions. For depth and semantics predictions, we employ decoder architectures with identical design: dream embeddings and a set of learnable masks are processed by a 2-layer ViT-based decoder to produce predicted features. Additionally, we apply the decoder from VQ-VAE to render depth features into depth maps.\nAction Prediction\nThe action prediction module takes the action embedding produced by Qwen2.5-VL as input and generates predicted waypoints, orientation estimates, and arrival flags. First, a 2-layer transformer processes the action embedding to capture dependencies on the world model’s dream embeddings. Then, the processed action embedding is passed to the action prediction head, which outputs the final navigation predictions, including waypoints, orientation estimates, and arrival flags. The action prediction head consists of a simple MLP with two linear layers and a ReLU activation in between.\nI-B\nTraining Details\nWe interleave the VLM planning training data and world model training data to jointly train NavForesee. The training batch size is set to 4, and the number of image observations is flexible, up to a maximum length of 20. Depth and semantic features are precomputed and loaded during training. We use the AdamW optimizer with an initial learning rate of\n1\n×\n10\n−\n5\n1\\times 10^{-5}\n. Depth and semantics predictions are weighted with\nα\n=\n0.25\n\\alpha=0.25\nand\nβ\n=\n0.3\n\\beta=0.3\n. The model is trained for a total of 3 epochs on 64 NVIDIA H20 GPUs, with ViT parameters frozen. The fixed short-term prediction horizon is set to 4, and the number of predicted waypoints is set to 5.\nII\nExperimental Evaluations\nII-A\nHierarchical Planning Evaluation\nTo evaluate the hierarchical planning capabilities of NavForesee, we conduct experiments on the Val-Unseen split of the R2R-CE and RxR-CE datasets. An example is illustrated in Figure\n5\n. We perform hierarchical planning for each step of an episode. NavForesee generates a navigation summary, plan, and actions strictly following the output format specified in the prompt template. Apart from the initial position, NavForesee consistently identifies milestones along the route, summarizes completed sub-instructions, and formulates the next sub-instruction in alignment with the overall instruction context. This demonstrates that NavForesee effectively leverages its multimodal understanding capabilities to decompose complex navigation tasks into manageable sub-goals, thereby enabling more structured and efficient navigation. Notably, the hierarchical planning module is jointly trained with the world model prediction and action policy learning modules, indicating that NavForesee maintains strong language planning capabilities even when extended with additional functionalities. Furthermore, the hierarchical plans are precise and concise, which greatly benefits subsequent navigation decisions.\nFigure 5\n:\nHierarchical planning examples generated by NavForesee for the instruction ”Go up the stairs and straight forward the doorway. Turn right, move forward, and enter the doorway on the right. Move forward into the bedroom and stop in front of the toilet”. From top to bottom: frames with timestamps, global navigation map, and navigation planning outputs. NavForesee accurately identifies milestones along the route, summarizes completed sub-instructions, and generates the next sub-instruction in accordance with the instruction context.\nFigure 6\n:\nShort-term and long-term depth predictions. From top to bottom: frames with timestamps, future ground truth frames with timestamps, short-term depth predictions for future frames, and long-term depth predictions for milestones. Instruction: ”Up the stairs. Turn to the left and enter the second open door on the left. Walk towards the foot of the bed. Turn right and enter the open door to the bathroom.”\nII-B\nShort-term and Long-term Prediction Evaluation\nFigure\n6\nillustrates the short-term and long-term depth predictions produced by our world model over a complete navigation episode. Short-term predictions forecast up to four future steps, whereas long-term predictions extrapolate over an adaptive horizon determined by progress towards the next milestone. Compared to short-term predictions, long-term depth predictions may be less accurate in capturing detailed depth at milestone locations, since milestone positions are unknown during inference. At the beginning of the episode, the long-term predictions effectively capture the scene when the agent ascends the stairs. As the agent approaches the first milestone (the doorway), the long-term predictions degrade slightly, likely due to the increased uncertainty of longer horizons and the absence of explicit milestone information. In such cases, long-term predictions tend to track short-term outputs, because long-term queries can attend to short-term queries. Nevertheless, the long-term predictions maintain the overall scene layout and depth distribution, providing valuable guidance for strategic navigation. This demonstrates that NavForesee’s world model effectively anticipates environmental changes over both short and long horizons, enhancing the agent’s planning and action capabilities in complex scenarios.\nTABLE III\n:\nPerformance comparison between depth prediction and semantics prediction\nIndex\nDepth\nSemantics\nSR\n↑\n\\uparrow\nOSR\n↑\n\\uparrow\nNE\n↓\n\\downarrow\nSPL\n↑\n\\uparrow\n1\n✓\n✓\n66.2%\n78.4%\n3.94\n59.7%\n2\n✗\n✓\n61.8%\n76.7%\n4.37\n54.9%\n3\n✓\n✗\n60.0%\n76.2%\n4.59\n52.9%\nII-C\nAblation Study on Depth and Semantics Predictions\nWe conduct ablation studies to evaluate the individual contributions of depth and semantics predictions in the world model. As shown in Table\nIII\n, removing either depth or semantics predictions results in a clear performance drop. The full NavForesee model, which integrates both depth and semantics predictions, achieves the highest SR (66.2%), OSR (78.4%), lowest NE (3.94), and best SPL (59.7%), validating the benefit of their combination. Without depth prediction, the SR drops to 61.8% and SPL decreases by 4.8 points, highlighting the importance of depth information for spatial reasoning and obstacle avoidance. Disabling semantics predictions leads to an even larger SR reduction (60.0%) and higher NE, underscoring the critical role of semantic features in recognizing landmarks and guiding navigation. These findings confirm that both depth and semantics predictions are essential for accurate and efficient navigation.",
    "preview_text": "Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.\n\nNavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction\nFei Liu\n∗\nShichao Xie\n∗\nMinghua Luo Zedong Chu\n†\nJunjun Hu Xiaolong Wu\n†\nMu Xu\nAmap, Alibaba Group\n*Joint first authors\n†\n\\dagger\nCorresponding authors\nAbstract\nEmbodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robus",
    "is_relevant": null,
    "relevance_score": 0.0,
    "extracted_keywords": [],
    "one_line_summary": "",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T11:24:16Z",
    "created_at": "2026-01-10T10:30:59.401717",
    "updated_at": "2026-01-10T10:30:59.401725",
    "flag": true
}