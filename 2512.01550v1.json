{
    "id": "2512.01550v1",
    "title": "NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction",
    "authors": [
        "Fei Liu",
        "Shichao Xie",
        "Minghua Luo",
        "Zedong Chu",
        "Junjun Hu",
        "Xiaolong Wu",
        "Mu Xu"
    ],
    "abstract": "由复杂自然语言指令引导的具身导航在长视野任务中仍然是人工智能领域的一大挑战。现有智能体通常难以在未知环境中实现稳健的长期规划，导致较高的失败率。为应对这些局限性，我们提出了NavForesee，一种新颖的视觉-语言模型（VLM），它在一个统一的框架内融合了高层语言规划与预测性世界模型的想象能力。我们的方法使单一VLM能够同时进行任务规划与前瞻性预测。该模型以完整指令和历史观测为条件，通过分解任务、追踪进度并制定后续子目标来理解导航指令；同时，它还充当生成式世界模型，通过预测短期环境动态和长期导航里程碑，提供关键的前瞻性信息。VLM的结构化计划指导其有针对性的预测，而对未来的想象则为导航决策提供丰富的上下文信息，从而形成感知—规划/预测—行动之间的强大内部反馈循环。我们在R2R-CE和RxR-CE基准上进行了大量实验，结果表明NavForesee在复杂场景中实现了极具竞争力的性能。本研究凸显了显式语言规划与隐式时空预测融合的巨大潜力，为构建更智能、更强大的具身智能体指明了方向。",
    "url": "https://arxiv.org/abs/2512.01550v1",
    "html_url": "https://arxiv.org/html/2512.01550v1",
    "html_content": "NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction\nFei Liu\n∗\nShichao Xie\n∗\nMinghua Luo Zedong Chu\n†\nJunjun Hu Xiaolong Wu\n†\nMu Xu\nAmap, Alibaba Group\n*Joint first authors\n†\n\\dagger\nCorresponding authors\nAbstract\nEmbodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework.\nOur approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM’s structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.\nI\nIntroduction\nEmbodied navigation, a cornerstone challenge in artificial intelligence, has recently witnessed remarkable progress driven by the advent of Vision-Language Models (VLMs)\n[\n1\n,\n2\n,\n3\n,\n4\n,\n5\n]\n. These models endow agents with the ability to perceive, interpret instructions, and operate in complex environments. Despite these advances, a significant performance gap persists in long-horizon tasks, where agents frequently fail to maintain course, comprehend observations, or make consistently correct decisions. This gap stems from two primary limitations: (1) a planning and memory deficit, as deployable VLMs often have limited context windows and planning capabilities, causing them to get ”lost” in the navigation environment\n[\n6\n,\n7\n,\n8\n]\n; and (2) a lack of predictive foresight, as current models are fundamentally reactive and cannot anticipate future environmental states to guide their actions proactively\n[\n9\n,\n10\n,\n11\n]\n.\nExisting research has pursued these challenges on separate fronts. One trajectory enhances VLM reasoning through curated datasets and Chain-of-Thought (CoT) prompting\n[\n12\n,\n13\n]\n. The other develops world models to predict future states, informing action planning\n[\n14\n,\n15\n]\n. However, a critical oversight is the disconnection between these paradigms. A VLM-centric agent can suffer from semantic hallucinations, where its plan disconnects from visual reality, while a world model without language guidance can experience semantic drift, its predictions becoming untethered from the instructional goal.\nWe posit that VLM planning and predictive foresight should not be separate but unified and mutually reinforcing within a single VLM\n[\n16\n]\n. To this end, we introduce NavForesee as in Figure\nLABEL:fig:navforesee\n, a unified model that integrates multi-modal understanding with world model generation. Our approach is inspired by human navigation, which is not a continuous, low-level process but a hierarchical one centered on milestones. Humans typically navigate by heading towards a sequence of meaningful landmarks, largely ignoring the minutiae of the path between them. We argue that an artificial agent should do the same. NavForesee adopts this strategy by operating through two synergistic functions: (1) Hierarchical Language planning. As a planner, NavForesee generates a high-level plan by summarizing the navigation task into completed sub-instructions, identifying the current sub-instruction, and formulating the next steps as semantic action ”trunks.” This grounds the agent’s planning in the overall instruction. (2) Dual-Horizon Predictive Foresight. As a world model, NavForesee ”imagines” the future on two timescales. For long-term guidance, it predicts the key visual features of the environment at the completion of the current sub-instruction—effectively envisioning the next milestone. For short-term execution, it forecasts immediate future features to enhance local awareness, enabling robust obstacle avoidance and better understanding of environmental dynamics.\nInspired by latent-space world models\n[\n17\n,\n18\n,\n19\n,\n15\n]\n, this prediction deliberately avoids computationally expensive pixel-level generation. Instead, NavForesee forecasts a compact set of high-level features—depth, DINOv2, and SAM features—that capture essential geometric and semantic information as in DreamVLA. The predicted features are fed to an action policy module which is simply an MLP to generate continuous waypoints and flags for arriving or not. By tightly coupling hierarchical planning with dual-horizon predictive foresight, NavForesee generates coherent, goal-oriented actions, guided by both a long-term vision of its milestones and an immediate awareness of its surroundings.\nWe conducted extensive experiments on the R2R-CE\n[\n20\n]\nand RxR-CE\n[\n21\n]\nbenchmarks. Training exclusively on the publicly available R2R-CE and RxR-CE datasets, NavForesee demonstrates highly competitive performance, achieving a Success Rate (SR) of 66.2% and an Oracle Success Rate (OSR) of 78.4% on the R2R-CE benchmark—comparable to state-of-the-art methods.\nIn summary, our key contributions are threefold:\n•\nWe propose NavForesee, a VLN framework that unifies vision–language model (VLM) planning with world model prediction for navigation tasks.\n•\nWe introduce a hierarchical language planning paradigm that addresses long-instruction, goal-oriented missions by explicitly tracking mission progress and generating concise textual sub-plans.\n•\nWe design a dual-horizon world model prediction mechanism for both short-term execution and long-term milestone navigation, implicitly forming a perception–planning and prediction–action loop that guides agent behavior.\nII\nRelated Works\nII-A\nVisual Language Navigation\nVision-and-Language Navigation (VLN) requires an embodied agent to interpret natural language instructions, perceive visual surroundings, and generate a sequence of actions to reach a specified goal. The advent of large-scale pre-trained VLMs has catalyzed significant progress, largely superseding earlier methods based on topological graphs\n[\n22\n,\n23\n,\n24\n]\n, top-down semantic maps\n[\n25\n,\n26\n,\n27\n]\n, or instruction augmentation\n[\n28\n]\n. Recent works leveraging VLMs can be broadly categorized into two main paradigms.\nThe first uses the VLM as a high-level planner, auto-regressively generating action plans\n[\n29\n,\n30\n,\n31\n]\nor textual trajectories\n[\n32\n]\n. While strong in reasoning, this step-by-step generation is prone to error accumulation and slow inference. The second employs the VLM as an end-to-end policy, directly mapping inputs to actions. However, this often leads to overfitting on training scenes and underutilizes the VLM’s high-level reasoning capabilities.\nTo bridge the gap between these two approaches, dual-system architectures have been proposed\n[\n6\n,\n33\n]\n. These models often adopt a ”Fast-and-Slow” reasoning paradigm, combining a deliberative ”slow” system for high-level reasoning with a lightweight ”fast” reactive controller for low-level execution. Reinforcement learning is frequently employed to align the outputs of both systems and bootstrap the learning of coherent reasoning-action patterns. Despite this progress, a fundamental challenge remains: long, complex reasoning chains (e.g., long CoTs) do not always align with the spatial and dynamic realities of the environment. Furthermore, frequent or periodic elaborate reasoning processes may be unnecessary, as human navigation often relies on simpler, high-level semantic plans rather than continuous, detailed deliberation.\nII-B\nNavigation World Model\nA world model is designed to learn a predictive model of an environment, forecasting future states from historical observations and optional conditioning information, such as actions or instructions. Predictions can be generated in either raw pixel space or a more compact latent space\n[\n18\n]\n. The concept has gained significant traction recently, propelled by large-scale video generation models like Sora(\n[\n34\n]\n, which can produce long-term, consistent, even interactive video sequences from text prompts. A key application of world models in robotics is to serve as a simulation engine, allowing an agent to ”imagine” the outcomes of different action sequences and evaluate control policies before execution\n[\n16\n,\n19\n]\n.\nIn the context of visual navigation, recent works have begun to leverage world models to provide agents with environmental foresight. For instance, NavMorph utilizes a Recurrent State-Space Model (RSSM) to model environmental dynamics in a compact latent space, refining the agent’s policy with imagined future states\n[\n11\n]\n. Similarly, HNR\n[\n9\n]\nadvocates for predicting multi-level semantic features instead of raw pixels, enabling faster and higher-quality imagination to evaluate multiple next-step actions in parallel. Other approaches, like NWM\n[\n35\n]\n, use a controlled video generation model to plan entire trajectories through simulation.\nDespite their promise, existing world models for navigation face two primary limitations. First, action-conditioned models that rely on extensive trajectory sampling and evaluation are often computationally prohibitive, rendering them infeasible for deployment on resource-constrained agents. Second, and more critically for our work, prior research has focused almost exclusively on learning environmental dynamics, largely neglecting to integrate this predictive capability with the high-level language reasoning abilities of modern VLMs. This separation leaves a critical gap, which our work aims to address by unifying these two powerful paradigms.\nFigure 1\n:\nVLM-driven hierarchical navigation plan dataset generation. Episodes from R2R-CE and RxR-CE are processed by\nGemini 2.5 Pro\n, which decomposes long instructions into sub-instructions and identifies keyframe milestones. To generation of waypoint-level reasoning labels, waypoints are sampled between milestones annotated with a navigation summary, future plan, and action (\nforward\n,\nleft\n,\nright\n, or\nstop\n).\nIII\nMethods\nFigure 2\n:\nOverall architecture of NavForesee. The model is built on the Qwen2.5-VL-3B-Instruct backbone, integrating two complementary functionalities: (1) VLM-based hierarchical planning and (2) world model-based dual-horizon visual prediction. For hierarchical planning, textual instruction and visual observations are encoded via Qwen’s original multimodal encoders to produce auto-regressive sub-goal plans. For prediction, a position encoder encodes the agent’s relative pose, and short- and long-horizon dream queries (depth and semantic subqueries) are appended to multimodal embeddings. These queries, processed through structured attention, feed lightweight convolutional decoders for environmental predictions and an MLP head for navigation actions.\nIII-A\nProblem Formulation\nWe target instruction-guided navigation missions in which an embodied agent must interpret a natural language instruction\nl\nl\nand navigate from a given start position to an intended goal location, strictly following the described route. The challenge lies in robustly understanding the instruction, maintaining situational awareness over long horizons, and deciding actions that lead to successful navigation in unseen environments.\nAt time step\nt\nt\n, the agent perceives the environment and obtains a panoramic RGB observation\no\nt\no_{t}\n. It maintains a memory of the past\nH\nH\nobservations,\nO\nt\n−\nH\n:\nt\n−\n1\n=\n[\no\nt\n−\nH\n,\n…\n,\no\nt\n−\n1\n]\nO_{t-H:t-1}=[o_{t-H},\\dots,o_{t-1}]\n, to support temporal reasoning. The navigation policy produces a sequence of\nK\nK\nfuture waypoints\nw\nt\n:\nt\n+\nK\n∈\nℝ\nK\n×\n5\nw_{t:t+K}\\in\\mathbb{R}^{K\\times 5}\n, where each waypoint is defined as\nw\nt\n=\n[\nx\nt\n,\ny\nt\n,\nsin\n⁡\nθ\nt\n,\ncos\n⁡\nθ\nt\n,\nc\nt\n]\n,\nw_{t}=\\left[x_{t},y_{t},\\sin\\theta_{t},\\cos\\theta_{t},c_{t}\\right],\nwith\n(\nx\nt\n,\ny\nt\n)\n(x_{t},y_{t})\ndenoting planar positions,\nθ\nt\n\\theta_{t}\nthe heading angle, and the binary flag\nc\nt\nc_{t}\nindicating whether a\nstop\naction should be triggered. Unless all predicted actions are marked as\nstop\n, the agent continuously moves following the generated waypoints.\nTo solve this problem, we adopt Qwen2.5-VL as our backbone and extend it with two complementary modules. First, we enable\nhierarchical planning\nby decomposing the full instruction into sequential sub-instructions, identifying completed ones and predicting the next step under the current context—leveraging the model’s language understanding capabilities and pretraining on our constructed dataset. Second, we integrate\nworld model foresight\nfor predicting short- and long-term environmental changes, enhancing vision–language coherence and yielding more reliable action policies. Together, these capabilities allow the agent to imitate human navigation behaviors, combining explicit language planning with implicit spatiotemporal prediction.\nIII-B\nVLM-driven Hierarchical Planning Dataset\nWe construct a hierarchical language planning dataset specifically for instruction-guided navigation missions, leveraging advanced Vision–Language Models (VLMs) for multi-modal understanding and sequence analysis. Our goal is to provide training data that captures both short-term execution steps and long-term navigation milestones.\nAs illustrated in Figure\n1\n, we start from public Vision-and-Language Navigation (VLN) benchmarks—R2R-CE (10k episodes) and RxR-CE (20k episodes)—which provide paired natural language instructions and full image observation sequences. Each episode is processed with\nGemini 2.5 Pro\n, guided by a custom prompt template that specifies the model’s role, defines the mission, outlines analytical steps, and enforces an explicit output format. The VLM systematically decomposes each long instruction into a series of sequential sub-instructions, while identifying a dense visual chain of keyframes representing navigation milestones. For paths involving extended travel or sharp turns, we require the inclusion of intermediate milestones to maintain visual continuity in the generated plan. This hierarchical structure enables downstream world models to better learn both short-term and long-term prediction.\nFor every annotated episode, the output is standardized to include: the milestone frame index, the textual description of the completed sub-instruction, and the upcoming planned instruction. Post-processing involves filtering incomplete annotations, correcting logical inconsistencies in the VLM outputs, and converting each episode into multiple navigation segments. We sample waypoints along each trajectory, with each waypoint forming the endpoint of a segment between milestones. Each sampled waypoint is assigned a\nplanning label\ncomprising: (1) a navigation summary (completed sub-instruction), (2) a future plan (next instruction), and (3) a language action (\nforward\n,\nleft\n,\nright\n,\nstop\n).\nThis pipeline produces approximately\n1.3\n1.3\nM training samples from RxR-CE and\n0.2\n0.2\nM from R2R-CE. To ensure balanced training data, we down-sample over-represented straight-motion cases and augment examples involving stopping actions. The final dataset provides richly annotated, balanced samples for training the hierarchical language planning and predictive modules in NavForesee.\nIII-C\nModel Architecture\nOverall Architecture\nThe overall architecture of NavForesee is illustrated in Figure\n2\n. We adopt Qwen2.5-VL-3B-Instruct\n[\n36\n]\n, a large-scale vision–language model with strong multi-modal understanding capabilities, as the backbone. NavForesee is designed to integrate two complementary functionalities: VLM-based language planning and World model-based visual prediction.\nCorrespondingly, we define two primary training objectives: VLM planning training and world model training. Training data from both tasks are jointly mixed to ensure that the model preserves its multi-modal planning ability, while simultaneously extending its capability to generate visual features.\nFor the VLM planning training, textual planning data are directly fed into Qwen for auto-regressive training, leveraging its original text encoder and image encoder components without modification.\nFor the world model training, we introduce an additional position encoder (pos-encoder) to encode the agent’s relative position and orientation from image observations. Two sets of dream queries—corresponding to short- and long-horizon predictions—are appended to the multi-modal embeddings. Each set of dream queries includes depth and semantics subqueries, enabling dual-horizon prediction. Furthermore, an action query, alongside the dream queries, is integrated into the multi-modal inputs and processed by Qwen2.5-VL via a structured attention mechanism. Lightweight convolutional layers serve as decoders to transform dream embeddings into environmental predictions (depth and semantics), while a simple MLP predicts action outputs (waypoints, orientation estimates, and arrival flags).\nStructured Attention Mask\nTo maintain a clear separation between short- and long-horizon predictions and to avoid cross-type contamination, each dream query type (depth and semantics) is explicitly decomposed into short-horizon and long-horizon components. As illustrated in Figure\n2\n, we design a structured attention mask tailored for dual-horizon prediction. Long-horizon predictions naturally depend on short-horizon predictions, using them as guidance to ensure temporal coherence. Mutual attention between depth and semantics queries is masked to prevent cross-modal leakage or unintended feature mixing. In contrast, the action query attends to all available information—including past context and both horizons of dream queries—enabling it to make globally consistent navigation predictions.\nIII-D\nDual-horizon World Model Prediction\nSpecifically, to enable accurate dual-horizon environmental feature prediction, we employ the world model architecture that serves as guidance for learning the inverse dynamics of a navigation agent. Here, short-term prediction refers to generating forecasts for\nk\nk\nsteps ahead, while long-term prediction targets navigation milestones, corresponding to an adaptive horizon determined by progress toward the next milestone.\nFor visual feature prediction within Qwen2.5-VL, we introduce two sets of learnable dream queries, namely the short-term\nQ\nS\n∈\nℝ\nL\n×\nd\nQ_{S}\\in\\mathbb{R}^{L\\times d}\nand and long-term\nQ\nL\n∈\nℝ\nL\n×\nd\nQ_{L}\\in\\mathbb{R}^{L\\times d}\nto, which extract temporally aligned feature embeddings specialized for prediction at distinct horizons. To enhance the model’s capability in capturing spatial-temporal correlations and learning environmental dynamics, we further integrate position-orientation state embeddings\ns\nt\n−\nH\n:\nt\ns_{t-H:t}\nfor each input frame through an encoder\nh\n(\n.\n)\nh(.)\n.\nThese dream queries are concatenated with textual instruction embeddings\nl\nl\nand visual observation sequences\nO\nt\n−\nH\n:\nt\nO_{t-H:t}\nand processed by the Qwen2.5-VL backbone\nf\n(\n.\n)\nf(.)\n. Specially,\nE\nS\n=\nf\n​\n(\nl\n,\nO\nt\n−\nH\n:\nt\n,\nh\n​\n(\ns\nt\n−\nH\n:\nt\n)\n|\nQ\nS\n)\n,\n\\displaystyle E_{S}=f(l,O_{t-H:t},h(s_{t-H:t})|Q_{S}),\nE\nL\n=\nf\n​\n(\nl\n,\nO\nt\n−\nH\n:\nt\n,\nh\n​\n(\ns\nt\n−\nH\n:\nt\n)\n,\nQ\nS\n|\nQ\nL\n)\n\\displaystyle E_{L}=f(l,O_{t-H:t},h(s_{t-H:t}),Q_{S}|Q_{L})\nwhere causal attention masking ensures auto-regressive generation: short-term embeddings are produced first, and long-term embeddings are conditioned on short-term predictions.\nWe design lightweight decoders to interpret\nE\nL\nE_{L}\nand\nE\nS\nE_{S}\ninto predicted depth\nd\np\nd_{p}\n, and high-level semantics\nc\np\nc_{p}\n(e.g. derived from DINOV2, SAM). Short-term predictions correspond to a fixed horizon\nk\nk\nwhereas long-term predictions adaptively extrapolate over\nM\nt\nM_{t}\nsteps, dependent on the agent’s progress toward the next milestone:\np\nt\n+\nk\n=\nD\n​\n(\nE\nS\n)\n=\n[\nd\np\n​\n(\nt\n)\n,\nc\np\n​\n(\nt\n)\n]\n,\n\\displaystyle p_{t+k}=D(E_{S})=[d_{p}(t),c_{p}(t)],\np\nt\n+\nM\nt\n=\nD\n​\n(\nE\nL\n)\n=\n[\nd\np\n​\n(\nt\n+\nM\nt\n)\n,\nc\np\n​\n(\nt\n+\nM\nt\n)\n]\n\\displaystyle p_{t+M_{t}}=D(E_{L})=[d_{p}(t+M_{t}),c_{p}(t+M_{t})]\nIII-E\nPredictive Action Policy Learning\nGiven two temporally order states\no\nt\no_{t}\nand\no\nt\n+\n1\no_{t+1}\n, the intermediate action\na\n^\n​\n(\nt\n)\n\\hat{a}(t)\ncan be inferred via inverse dynamics. We leverage this principle to learn an action policy conditioned on the instruction\nl\nl\n, historical visual observations\nO\nt\n−\nH\n:\nt\nO_{t-H:t}\nand the dual-horizon predictive latent features\nE\nS\nE_{S}\nand\nE\nL\nE_{L}\ngenerated by the world model.\nTo enhance the encoding of task-relevant context for action prediction, we introduce a learnable action query\nQ\na\nQ_{a}\n. This query is concatenated with the dream queries and multi-modal input embeddings to form a unified action embedding. The Qwen2.5-VL backbone processes these embeddings to produce the contextual representation for action inference, which is subsequently projected into the action space:\nE\na\n=\nf\n​\n(\nl\n,\nO\nt\n−\nH\n:\nt\n,\nh\n​\n(\ns\nt\n−\nH\n:\nt\n)\n,\nQ\nS\n,\nQ\nL\n|\nQ\na\n)\n\\displaystyle E_{a}=f(l,O_{t-H:t},h(s_{t-H:t}),Q_{S},Q_{L}|Q_{a})\na\n^\nt\n:\nt\n+\nk\n=\nM\ni\n​\nn\n​\nv\n​\n(\nE\nS\n,\nE\nL\n|\nE\na\n)\n\\displaystyle\\hat{a}_{t:t+k}=M_{inv}(E_{S},E_{L}|E_{a})\nwhere\nE\na\nE_{a}\nis the action embedding and\nM\ni\n​\nn\n​\nv\nM_{i}nv\ndenotes the inverse dynamics model.\nNotably, in our action policy learning pipeline, the action embedding\nE\na\nE_{a}\nis extracted through the Qwen2.5-VL backbone, while action predictions are primarily conditioned on the dual-horizon predictive features, ensuring that decisions are informed by both past observations and forecasted environmental dynamics\nIII-F\nClose the Planning/Prediction and Action Loop\nFor VLM planning training, we finetune Qwen2.5-VL model based on the constructed dataset in an auto-regressive manner independently to build a powerful model capable of conducting hierarchical navigation.\nFor world model prediction and action policy learning, the training tasks are classified depth prediction, semantic feature prediction and action prediction. Depth prediction error\nL\nd\nL_{d}\nis measured using the Scale-invariant Logarithmic Loss (SiLogLoss) at the pix-level level. The semantics feature prediction error\nL\nc\nL_{c}\nand action error\nL\na\nL_{a}\nare computed using mean squared error (MSE). The overall training loss\nL\nL\ncomprise\nL\nd\nL_{d}\n,\nL\nc\nL_{c}\nand\nL\na\nL_{a}\nL\n=\nα\n​\nL\nd\n+\nβ\n​\nL\nc\n+\nL\na\nL=\\alpha L_{d}+\\beta L_{c}+L_{a}\nwhere\nα\n\\alpha\nand\nβ\n\\beta\nare weighting hyperparameters balancing the tasks.\nIV\nExperimental Evaluation\nTABLE I:\nComparison with other methods on the Val-Unseen split of R2R-CE and RxR-CE\nMethod\nObservation\nR2R-CE Val-Unseen\nRxR-CE Val-Unseen\nS.RGB\nPano.\nDepth\nOdo\nNE\n↓\n\\downarrow\nOS\n↑\n\\uparrow\nSR\n↑\n\\uparrow\nSPL\n↑\n\\uparrow\nNE\n↓\n\\downarrow\nSR\n↑\n\\uparrow\nSPL\n↑\n\\uparrow\nHPN+DN*\n[\n37\n]\n✓\n✓\n✓\n6.31\n40.0\n36.0\n34.0\n-\n-\n-\nCMA*\n[\n38\n]\n✓\n✓\n✓\n6.20\n52.0\n41.0\n36.0\n8.76\n26.5\n22.1\nSim2Sim\n[\n39\n]\n✓\n✓\n✓\n6.07\n52.0\n43.0\n36.0\n8.76\n26.5\n22.1\nGridMM*\n[\n8\n]\n✓\n✓\n✓\n5.11\n61.0\n49.0\n41.0\n-\n-\n-\nDreamWalker*\n[\n40\n]\n✓\n✓\n✓\n5.53\n59.0\n49.0\n44.0\n-\n-\n-\nReborn*\n[\n41\n]\n✓\n✓\n✓\n5.40\n57.0\n50.0\n46.0\n5.98\n48.6\n42.0\nETPNav*\n[\n42\n]\n✓\n✓\n✓\n4.71\n65.0\n57.0\n49.0\n5.64\n54.7\n44.8\nHNR*\n[\n9\n]\n✓\n✓\n✓\n4.42\n67.0\n61.0\n51.0\n5.50\n56.3\n46.7\nAG-CMTP\n[\n43\n]\n✓\n✓\n✓\n7.90\n39.0\n23.0\n19.0\n-\n-\n-\nR2R-CMTP\n[\n43\n]\n✓\n✓\n✓\n7.90\n38.0\n26.0\n22.0\n-\n-\n-\nInstruc-Nav\n[\n31\n]\n✓\n✓\n✓\n6.89\n-\n31.0\n24.0\n-\n-\n-\nLAW\n[\n44\n]\n✓\n✓\n✓\n6.83\n44.0\n35.0\n31.0\n10.90\n8.0\n8.0\nCM2\n[\n45\n]\n✓\n✓\n✓\n7.02\n41.0\n34.0\n27.0\n-\n-\n-\nWS-MGMap\n[\n46\n]\n✓\n✓\n✓\n6.28\n47.0\n38.0\n34.0\n-\n-\n-\nAO-Planner\n[\n47\n]\n✓\n✓\n5.55\n59.0\n47.0\n33.0\n-\n-\n-\nSeq2Seq\n[\n48\n]\n✓\n✓\n7.77\n37.0\n25.0\n22.0\n12.10\n13.9\n11.9\nCMA\n[\n48\n]\n✓\n✓\n7.37\n40.0\n32.0\n30.0\n-\n-\n-\nNA Vid\n[\n49\n]\n✓\n5.47\n49.0\n37.0\n35.0\n-\n-\n-\nUni-NA Vid\n[\n50\n]\n✓\n5.58\n53.5\n47.0\n42.7\n6.24\n48.7\n40.9\nNaVILA\n[\n51\n]\n✓\n5.22\n62.5\n54.0\n49.0\n6.77\n49.3\n44.0\nStream VLN\n[\n52\n]\n✓\n4.98\n64.2\n56.9\n51.9\n6.22\n52.9\n46.0\nCorrectNav\n[\n53\n]\n✓\n4.24\n67.5\n65.1\n62.3\n4.09\n69.3\n63.3\nNavForesee(Ours)\n✓\n3.94\n78.4\n66.2\n59.7\n4.20\n66.3\n53.2\nTABLE II:\nPerformance comparison between VLM planning and dual-horizon world model prediction\nIndex\nVLM planning\nLong-term prediction\nShort-term prediction\nSR\n↑\n\\uparrow\nOSR\n↑\n\\uparrow\nNE\n↓\n\\downarrow\nSPL\n↑\n\\uparrow\n1\n✓\n✓\n✓\n66.2%\n78.4%\n3.94\n59.7%\n2\n✗\n✓\n✓\n48.8%\n75.5%\n5.61\n39.4%\n3\n✓\n✗\n✓\n58.6%\n76.4%\n4.47\n50.1%\n4\n✗\n✗\n✗\n52.6%\n67.4%\n5.53\n46.7%\nWe evaluate our model in continuous environment of the Habitant simulator on the R2R-CE and RxR-CE datasets.\nR2R-CE\ndataset is derived from the Matterport3D indoor environments, discretized for path planning but operationalized in the Habitat simulator under a continuous navigation setting. It provides fine-grained, step-by-step natural language instructions, allowing for detailed guidance at each navigation step. In the simulator, the embodied agent\ncan execute turns as small as\n15\n∘\n15^{\\circ}\nand perceives the scene through a\n90\n∘\n90^{\\circ}\nhorizontal field-of-view.\nRxR-CE\nis a large-scale, multilingual VLN dataset comprising about 126K human-annotated instructions. Compared to R2R-CE, RxR-CE covers more diverse and complex trajectories, increasing the difficulty of the navigation tasks. The agent in this setting uses a coarser minimum turn\nincrement of\n30\n∘\n30^{\\circ}\nand a narrower\n79\n∘\n79^{\\circ}\nhorizontal field-ofview, which demands more deliberate movement planning for effective scene coverage.\nWe evaluate navigation performance using standard metrics including success rate (SR), oracle success rate (OS), success weighted by path length (SPL), and navigation error (NE).\nIV-A\nComparison with State-of-the-Art Methods\nTable 1 reports the performance of the proposed method compared with other approaches on the R2R‑CE and RxR‑CE datasets. Overall, NavForesee delivers competitive results against state‑of‑the‑art (SOTA) methods.\nSpecifically, on the val unseen split of the R2R‑CE dataset, NavForesee achieves SOTA performance by improving SR by 1.1%, OSR by 10.9%, and reducing NE by 0.3 m. This improvement can be attributed to the integration of the world model prediction module, which enables the agent to better capture environmental dynamics, avoid obstacles, and explore the surroundings more effectively.\nIn contrast, NavForesee performs slightly worse than SOTA methods on RxR‑CE, indicating limited generalization to more complex environments. It is worth noting that we train soly on NavForesee on R2R‑CE and RxR‑CE datasets, whereas other methods exploit diverse and large‑scale datasets to enhance generalization.\nAlthough NavForesee does not consistently outperform all baselines, it achieves the highest OSR across both datasets. This demonstrates the value of incorporating world model prediction into VLN agents and implies its promising potential for future vision‑and‑language navigation tasks.\nIV-B\nAblation Study\nAs shown in Table\nII\n, removing any of the three key modules—hierarchical VLM planning, long-term prediction, or short-term prediction—results in clear performance degradation. The full NavForesee model, which combines all modules, achieves the highest SR (66.2%), OSR (78.4%), lowest NE (3.94), and best SPL (59.7%), validating the benefit of their integration. Without VLM planning, the success rate drops sharply to 48.8% and the SPL decreases by more than 16 points, reflecting the importance of explicit instruction decomposition and progress tracking for efficient navigation. Disabling long-term prediction also leads to a noticeable reduction in SR (58.6%) and higher NE, highlighting the role of milestone foresight in providing strategic guidance over extended trajectories. When all three modules are removed, navigation quality deteriorates the most, confirming that planning and both prediction horizons together are crucial for accurate, efficient long-horizon navigation.\nFigure 3\n:\nShort-term depth and semantics predictions. From top to bottom: frames with timestamps, future ground truth frames with timestamps, future depth prediction for future frames, semantics predictions for future frames. Semantic features are DinoV2 features and visualized by a pretrained segmentation head. Instructions: UP the stairs. Turn to the left and enter into the second open door on the left. Walk towards the foot of the bed. Turn right and enter the open door to the bathroom\nFigure 4\n:\nNavForesee’s geometric-semantic feature imagination across different motion modes. The model accurately predicts environmental dynamics in straight motion, generalizes effectively to turning scenarios, and infers detailed object geometry and depth distribution from minimal visual input, such as a brief glimpse into a room\nIV-C\nQualitative Analysis\nFigure\n3\nillustrates the short-term depth and semantic feature predictions generated by our world model over the course of a complete navigation episode, forecasting up to four future steps. Although the predicted depth maps appear somewhat coarse—owing to the constraints of pixel-level supervised training on R2R-CE and RxR-CE—they nonetheless preserve the scene’s global geometry and spatial layout, faithfully capturing agent movements such as ascending or descending staircases, entering and exiting rooms, and making sharp or gradual turns. This ability to retain high-level spatial coherence despite reduced pixel detail ensures that the model’s predictions remain informative for downstream navigation decisions. The semantics predictions, obtained via a pretrained segmentation head, exhibit strong alignment with ground truth labels, successfully reflecting dynamic environmental changes in synchrony with the agent’s actions.\nFigure\n4\nfurther provides detailed examples that showcase NavForesee’s ability to imaginatively anticipate semantic features across diverse motion patterns. In addition to delivering accurate environment dynamics predictions when following a straightforward trajectory, NavForesee demonstrates remarkable generalization by reliably extrapolating future geometric and semantic structures when performing more complex navigational behaviors such as turns. In the final scenario, the agent receives only a brief partial observation—a quick glance into a room—yet the model is able to produce a vivid and coherent internal imagination of the room’s layout. This includes accurately inferring the relative shape and position of the bed, as well as estimating the depth distribution across the room, thus indicating its capacity to reason about unseen spatial regions.\nV\nConclusion\nWe proposed NavForesee, a vision–language navigation framework that unifies hierarchical language planning with dual-horizon predictive world modeling. By decomposing long instructions into sub-goals and anticipating both short-term dynamics and long-term milestones, NavForesee forms an implicit perception–planning and prediction–action loop.\nExperiments on R2R-CE and RxR-CE show strong performance—66.2% SR and 78.4% OSR on R2R-CE—comparable to state-of-the-art despite training only on public data. Qualitative results further reveal solid depth and semantics predictions that guide agent decisions in complex scenarios.\nThese findings highlight the benefit of equipping embodied agents with foresight: by “foreseeing” future states, NavForesee effectively fuses language planning with spatiotemporal imagination to improve visual-language navigation.\nReferences\n[1]\nW.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang,\nY. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing, “Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality,” March\n2023. [Online]. Available:\nhttps://lmsys.org/blog/2023-03-30-vicuna/\n[2]\nS. Chen, X. Chen, C. Zhang, M. Li, G. Yu, H. Fei, H. Zhu, J. Fan, and T. Chen,\n“Ll3da: Visual interactive instruction tuning for omni-3d understanding,\nreasoning, and planning,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2311.18651\n[3]\nA.-M. Halacheva, J.-N. Zaech, X. Wang, D. P. Paudel, and L. V. Gool,\n“Gaussianvlm: Scene-centric 3d vision-language models using language-aligned\ngaussian splats for embodied reasoning and beyond,” 2025. [Online].\nAvailable:\nhttps://arxiv.org/abs/2507.00886\n[4]\nH. Huang, Y. Chen, Z. Wang, R. Huang, R. Xu, T. Wang, L. Liu, X. Cheng,\nY. Zhao, J. Pang, and Z. Zhao, “Chat-scene: Bridging 3d scene and large\nlanguage models with object identifiers,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2312.08168\n[5]\nH. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” 2023.\n[Online]. Available:\nhttps://arxiv.org/abs/2304.08485\n[6]\nQ. Liu, T. Huang, Z. Zhang, and H. Tang, “Nav-r1: Reasoning and navigation in\nembodied scenes,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2509.10884\n[7]\nH. Zhou, J. Yu, and W. Yang, “Dual memory units with uncertainty regulation\nfor weakly supervised video anomaly detection,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2302.05160\n[8]\nZ. Wang, X. Li, J. Yang, Y. Liu, and S. Jiang, “Gridmm: Grid memory map for\nvision-and-language navigation,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2307.12907\n[9]\nZ. Wang, X. Li, J. Yang, Y. Liu, J. Hu, M. Jiang, and S. Jiang, “Lookahead\nexploration with neural radiance representation for continuous\nvision-language navigation,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2404.01943\n[10]\nX. Zhao, W. Cai, L. Tang, and T. Wang, “Imaginenav: Prompting vision-language\nmodels as embodied navigator through scene imagination,” 2024. [Online].\nAvailable:\nhttps://arxiv.org/abs/2410.09874\n[11]\nX. Yao, J. Gao, and C. Xu, “Navmorph: A self-evolving world model for\nvision-and-language navigation in continuous environments,” 2025. [Online].\nAvailable:\nhttps://arxiv.org/abs/2506.23468\n[12]\nB. Lin, Y. Nie, Z. Wei, J. Chen, S. Ma, J. Han, H. Xu, X. Chang, and X. Liang,\n“Navcot: Boosting llm-based vision-and-language navigation via learning\ndisentangled reasoning,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2403.07376\n[13]\nS. Wang, Y. Wang, W. Li, X. Cai, Y. Wang, M. Chen, K. Wang, Z. Su, D. Li, and\nZ. Fan, “Aux-think: Exploring reasoning strategies for data-efficient\nvision-language navigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2505.11886\n[14]\nQ. Zhao, Y. Lu, M. J. Kim, Z. Fu, Z. Zhang, Y. Wu, Z. Li, Q. Ma, S. Han,\nC. Finn, A. Handa, M.-Y. Liu, D. Xiang, G. Wetzstein, and T.-Y. Lin,\n“Cot-vla: Visual chain-of-thought reasoning for vision-language-action\nmodels,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2503.22020\n[15]\nY. Huang, J. Zhang, S. Zou, X. Liu, R. Hu, and K. Xu, “Ladi-wm: A latent\ndiffusion-based world model for predictive manipulation,” 2025. [Online].\nAvailable:\nhttps://arxiv.org/abs/2505.11528\n[16]\nJ. Cen, C. Yu, H. Yuan, Y. Jiang, S. Huang, J. Guo, X. Li, Y. Song, H. Luo,\nF. Wang, D. Zhao, and H. Chen, “Worldvla: Towards autoregressive action\nworld model,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2506.21539\n[17]\nE. Karypidis, I. Kakogeorgiou, S. Gidaris, and N. Komodakis, “Dino-foresight:\nLooking into the future with dino,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2412.11673\n[18]\nF. Baldassarre, M. Szafraniec, B. Terver, V. Khalidov, F. Massa, Y. LeCun,\nP. Labatut, M. Seitzer, and P. Bojanowski, “Back to the features: Dino as a\nfoundation for video world models,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2507.19468\n[19]\nW. Zhang, H. Liu, Z. Qi, Y. Wang, X. Yu, J. Zhang, R. Dong, J. He, F. Lu,\nH. Wang, Z. Zhang, L. Yi, W. Zeng, and X. Jin, “Dreamvla: A\nvision-language-action model dreamed with comprehensive world knowledge,”\n2025. [Online]. Available:\nhttps://arxiv.org/abs/2507.04447\n[20]\nJ. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, “Beyond the\nnav-graph: Vision-and-language navigation in continuous environments,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2004.02857\n[21]\nA. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge, “Room-across-room:\nMultilingual vision-and-language navigation with dense spatiotemporal\ngrounding,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/2010.07954\n[22]\nY. Hong, C. Rodriguez-Opazo, Y. Qi, Q. Wu, and S. Gould, “Language and visual\nentity relationship graph for agent navigation,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/2010.09304\n[23]\nZ. Deng, K. Narasimhan, and O. Russakovsky, “Evolving graphical planner:\nContextual global planning for vision-and-language navigation,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2007.05655\n[24]\nS. Chen, P.-L. Guhur, M. Tapaswi, C. Schmid, and I. Laptev, “Think global, act\nlocal: Dual-scale graph transformer for vision-and-language navigation,”\n2022. [Online]. Available:\nhttps://arxiv.org/abs/2202.11742\n[25]\nM. Z. Irshad, N. C. Mithun, Z. Seymour, H.-P. Chiu, S. Samarasekera, and\nR. Kumar, “Sasra: Semantically-aware spatio-temporal reasoning agent for\nvision-and-language navigation in continuous environments,” 2021. [Online].\nAvailable:\nhttps://arxiv.org/abs/2108.11945\n[26]\nG. Georgakis, K. Schmeckpeper, K. Wanchoo, S. Dan, E. Miltsakaki, D. Roth, and\nK. Daniilidis, “Cross-modal map learning for vision and language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.05137\n[27]\nP. Chen, D. Ji, K. Lin, R. Zeng, T. H. Li, M. Tan, and C. Gan,\n“Weakly-supervised multi-granularity map learning for vision-and-language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2210.07506\n[28]\nS. Zhang, Y. Qiao, Q. Wang, L. Guo, Z. Wei, and J. Liu, “Flexvln: Flexible\nadaptation for diverse vision-and-language navigation tasks,” 2025.\n[Online]. Available:\nhttps://arxiv.org/abs/2503.13966\n[29]\nY. Long, X. Li, W. Cai, and H. Dong, “Discuss before moving: Visual language\nnavigation via multi-expert discussions,” in\n2024 IEEE International\nConference on Robotics and Automation (ICRA)\n, 2024, pp. 17 380–17 387.\n[30]\nP. Chen, X. Sun, H. Zhi, R. Zeng, T. H. Li, G. Liu, M. Tan, and C. Gan,\n“\na\n2\na^{2}\nnav: Action-aware zero-shot robot navigation by exploiting\nvision-and-language ability of foundation models,” 2023. [Online].\nAvailable:\nhttps://arxiv.org/abs/2308.07997\n[31]\nY. Long, W. Cai, H. Wang, G. Zhan, and H. Dong, “Instructnav: Zero-shot system\nfor generic instruction navigation in unexplored environment,” 2024.\n[Online]. Available:\nhttps://arxiv.org/abs/2406.04882\n[32]\nY. Wang, Y. Fang, T. Wang, Y. Feng, Y. Tan, S. Zhang, P. Liu, Y. Ji, and R. Xu,\n“Dreamnav: A trajectory-based imaginative framework for zero-shot\nvision-and-language navigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2509.11197\n[33]\nX. Xue, J. Hu, M. Luo, X. Shichao, J. Chen, Z. Xie, Q. Kuichen, G. Wei, M. Xu,\nand Z. Chu, “Omninav: A unified framework for prospective exploration and\nvisual-language navigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2509.25687\n[34]\nY. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun,\nJ. Gao, L. He, and L. Sun, “Sora: A review on background, technology,\nlimitations, and opportunities of large vision models,” 2024. [Online].\nAvailable:\nhttps://arxiv.org/abs/2402.17177\n[35]\nA. Bar, G. Zhou, D. Tran, T. Darrell, and Y. LeCun, “Navigation world\nmodels,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2412.03572\n[36]\nS. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang,\nJ. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu,\nY. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and\nJ. Lin, “Qwen2.5-vl technical report,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2502.13923\n[37]\nJ. Krantz, A. Gokaslan, D. Batra, S. Lee, and O. Maksymets, “Waypoint models\nfor instruction-guided navigation in continuous environments,” 2021.\n[Online]. Available:\nhttps://arxiv.org/abs/2110.02207\n[38]\nY. Hong, Z. Wang, Q. Wu, and S. Gould, “Bridging the gap between learning in\ndiscrete and continuous environments for vision-and-language navigation,”\n2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.02764\n[39]\nJ. Krantz and S. Lee, “Sim-2-sim transfer for vision-and-language navigation\nin continuous environments,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2204.09667\n[40]\nH. Wang, W. Liang, L. V. Gool, and W. Wang, “Dreamwalker: Mental planning for\ncontinuous vision-language navigation,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2308.07498\n[41]\nD. An, Z. Wang, Y. Li, Y. Wang, Y. Hong, Y. Huang, L. Wang, and J. Shao, “1st\nplace solutions for rxr-habitat vision-and-language navigation competition\n(cvpr 2022),” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2206.11610\n[42]\nD. An, H. Wang, W. Wang, Z. Wang, Y. Huang, K. He, and L. Wang, “Etpnav:\nEvolving topological planning for vision-language navigation in continuous\nenvironments,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2304.03047\n[43]\nK. Chen, J. K. Chen, J. Chuang, M. Vázquez, and S. Savarese, “Topological\nplanning with transformers for vision-and-language navigation,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2012.05292\n[44]\nS. Raychaudhuri, S. Wani, S. Patel, U. Jain, and A. X. Chang,\n“Language-aligned waypoint (law) supervision for vision-and-language\nnavigation in continuous environments,” 2021. [Online]. Available:\nhttps://arxiv.org/abs/2109.15207\n[45]\nG. Georgakis, K. Schmeckpeper, K. Wanchoo, S. Dan, E. Miltsakaki, D. Roth, and\nK. Daniilidis, “Cross-modal map learning for vision and language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.05137\n[46]\nP. Chen, D. Ji, K. Lin, R. Zeng, T. H. Li, M. Tan, and C. Gan,\n“Weakly-supervised multi-granularity map learning for vision-and-language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2210.07506\n[47]\nJ. Chen, B. Lin, X. Liu, L. Ma, X. Liang, and K.-Y. K. Wong,\n“Affordances-oriented planning using foundation models for continuous\nvision-language navigation,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2407.05890\n[48]\nJ. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, “Beyond the\nnav-graph: Vision-and-language navigation in continuous environments,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2004.02857\n[49]\nJ. Zhang, K. Wang, R. Xu, G. Zhou, Y. Hong, X. Fang, Q. Wu, Z. Zhang, and\nH. Wang, “Navid: Video-based vlm plans the next step for vision-and-language\nnavigation,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2402.15852\n[50]\nJ. Zhang, K. Wang, S. Wang, M. Li, H. Liu, S. Wei, Z. Wang, Z. Zhang, and\nH. Wang, “Uni-navid: A video-based vision-language-action model for unifying\nembodied navigation tasks,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2412.06224\n[51]\nA.-C. Cheng, Y. Ji, Z. Yang, Z. Gongye, X. Zou, J. Kautz, E. Bıyık, H. Yin,\nS. Liu, and X. Wang, “Navila: Legged robot vision-language-action model for\nnavigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2412.04453\n[52]\nM. Wei, C. Wan, X. Yu, T. Wang, Y. Yang, X. Mao, C. Zhu, W. Cai, H. Wang,\nY. Chen, X. Liu, and J. Pang, “Streamvln: Streaming vision-and-language\nnavigation via slowfast context modeling,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2507.05240\n[53]\nZ. Yu, Y. Long, Z. Yang, C. Zeng, H. Fan, J. Zhang, and H. Dong, “Correctnav:\nSelf-correction flywheel empowers vision-language-action navigation model,”\n2025. [Online]. Available:\nhttps://arxiv.org/abs/2508.10416\nSupplementary Material\nI\nImplementation Details\nI-A\nModel Architecture\nBase Model\nWe employ Qwen2.5-VL-3B-Instruct\n[\n36\n]\nas the backbone of NavForesee. It adopts the Qwen2.5 LLM as its text decoder and integrates a vision encoder. The vision encoder utilizes a Vision Transformer (ViT) architecture to encode visual observations, while the text decoder is responsible for generating the hierarchical planning outputs and action trunk predictions. Detailed descriptions of Qwen2.5-VL can be found in\n[\n36\n]\n. For hierarchical planning, we directly use the original multimodal encoders and text decoder of Qwen2.5-VL without any modifications. For world model prediction and action policy learning, we introduce a position encoder to represent the agent’s relative position and orientation derived from image observations.\nLightweight decoders transform the dream query embeddings into environmental predictions (depth and semantics), while a simple MLP predicts action outputs (waypoints, orientation estimates, and arrival flags).\nDream Query Design\nTwo sets of dream queries (short-term and long-term), along with an action query, are appended to the multimodal embeddings. Each set of dream queries contains depth and semantics subqueries, enabling dual-horizon prediction. We use DINOv2 and SAM features as semantic representations. Thus, there are six query subsets in total—depth, DINOv2, and SAM for both short-term and long-term horizons—with each subset consisting of 64 tokens. The action query consists of a single token dedicated to action prediction.\nWorld Model Decoders\nWe design task-specific lightweight world model decoders to transform dream embeddings into depth maps, semantic features, and actions. For depth and semantics predictions, we employ decoder architectures with identical design: dream embeddings and a set of learnable masks are processed by a 2-layer ViT-based decoder to produce predicted features. Additionally, we apply the decoder from VQ-VAE to render depth features into depth maps.\nAction Prediction\nThe action prediction module takes the action embedding produced by Qwen2.5-VL as input and generates predicted waypoints, orientation estimates, and arrival flags. First, a 2-layer transformer processes the action embedding to capture dependencies on the world model’s dream embeddings. Then, the processed action embedding is passed to the action prediction head, which outputs the final navigation predictions, including waypoints, orientation estimates, and arrival flags. The action prediction head consists of a simple MLP with two linear layers and a ReLU activation in between.\nI-B\nTraining Details\nWe interleave the VLM planning training data and world model training data to jointly train NavForesee. The training batch size is set to 4, and the number of image observations is flexible, up to a maximum length of 20. Depth and semantic features are precomputed and loaded during training. We use the AdamW optimizer with an initial learning rate of\n1\n×\n10\n−\n5\n1\\times 10^{-5}\n. Depth and semantics predictions are weighted with\nα\n=\n0.25\n\\alpha=0.25\nand\nβ\n=\n0.3\n\\beta=0.3\n. The model is trained for a total of 3 epochs on 64 NVIDIA H20 GPUs, with ViT parameters frozen. The fixed short-term prediction horizon is set to 4, and the number of predicted waypoints is set to 5.\nII\nExperimental Evaluations\nII-A\nHierarchical Planning Evaluation\nTo evaluate the hierarchical planning capabilities of NavForesee, we conduct experiments on the Val-Unseen split of the R2R-CE and RxR-CE datasets. An example is illustrated in Figure\n5\n. We perform hierarchical planning for each step of an episode. NavForesee generates a navigation summary, plan, and actions strictly following the output format specified in the prompt template. Apart from the initial position, NavForesee consistently identifies milestones along the route, summarizes completed sub-instructions, and formulates the next sub-instruction in alignment with the overall instruction context. This demonstrates that NavForesee effectively leverages its multimodal understanding capabilities to decompose complex navigation tasks into manageable sub-goals, thereby enabling more structured and efficient navigation. Notably, the hierarchical planning module is jointly trained with the world model prediction and action policy learning modules, indicating that NavForesee maintains strong language planning capabilities even when extended with additional functionalities. Furthermore, the hierarchical plans are precise and concise, which greatly benefits subsequent navigation decisions.\nFigure 5\n:\nHierarchical planning examples generated by NavForesee for the instruction ”Go up the stairs and straight forward the doorway. Turn right, move forward, and enter the doorway on the right. Move forward into the bedroom and stop in front of the toilet”. From top to bottom: frames with timestamps, global navigation map, and navigation planning outputs. NavForesee accurately identifies milestones along the route, summarizes completed sub-instructions, and generates the next sub-instruction in accordance with the instruction context.\nFigure 6\n:\nShort-term and long-term depth predictions. From top to bottom: frames with timestamps, future ground truth frames with timestamps, short-term depth predictions for future frames, and long-term depth predictions for milestones. Instruction: ”Up the stairs. Turn to the left and enter the second open door on the left. Walk towards the foot of the bed. Turn right and enter the open door to the bathroom.”\nII-B\nShort-term and Long-term Prediction Evaluation\nFigure\n6\nillustrates the short-term and long-term depth predictions produced by our world model over a complete navigation episode. Short-term predictions forecast up to four future steps, whereas long-term predictions extrapolate over an adaptive horizon determined by progress towards the next milestone. Compared to short-term predictions, long-term depth predictions may be less accurate in capturing detailed depth at milestone locations, since milestone positions are unknown during inference. At the beginning of the episode, the long-term predictions effectively capture the scene when the agent ascends the stairs. As the agent approaches the first milestone (the doorway), the long-term predictions degrade slightly, likely due to the increased uncertainty of longer horizons and the absence of explicit milestone information. In such cases, long-term predictions tend to track short-term outputs, because long-term queries can attend to short-term queries. Nevertheless, the long-term predictions maintain the overall scene layout and depth distribution, providing valuable guidance for strategic navigation. This demonstrates that NavForesee’s world model effectively anticipates environmental changes over both short and long horizons, enhancing the agent’s planning and action capabilities in complex scenarios.\nTABLE III\n:\nPerformance comparison between depth prediction and semantics prediction\nIndex\nDepth\nSemantics\nSR\n↑\n\\uparrow\nOSR\n↑\n\\uparrow\nNE\n↓\n\\downarrow\nSPL\n↑\n\\uparrow\n1\n✓\n✓\n66.2%\n78.4%\n3.94\n59.7%\n2\n✗\n✓\n61.8%\n76.7%\n4.37\n54.9%\n3\n✓\n✗\n60.0%\n76.2%\n4.59\n52.9%\nII-C\nAblation Study on Depth and Semantics Predictions\nWe conduct ablation studies to evaluate the individual contributions of depth and semantics predictions in the world model. As shown in Table\nIII\n, removing either depth or semantics predictions results in a clear performance drop. The full NavForesee model, which integrates both depth and semantics predictions, achieves the highest SR (66.2%), OSR (78.4%), lowest NE (3.94), and best SPL (59.7%), validating the benefit of their combination. Without depth prediction, the SR drops to 61.8% and SPL decreases by 4.8 points, highlighting the importance of depth information for spatial reasoning and obstacle avoidance. Disabling semantics predictions leads to an even larger SR reduction (60.0%) and higher NE, underscoring the critical role of semantic features in recognizing landmarks and guiding navigation. These findings confirm that both depth and semantics predictions are essential for accurate and efficient navigation.",
    "preview_text": "Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.\n\nNavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction\nFei Liu\n∗\nShichao Xie\n∗\nMinghua Luo Zedong Chu\n†\nJunjun Hu Xiaolong Wu\n†\nMu Xu\nAmap, Alibaba Group\n*Joint first authors\n†\n\\dagger\nCorresponding authors\nAbstract\nEmbodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robus",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "unified generation understanding",
        "multimodal generation"
    ],
    "one_line_summary": "NavForesee 是一个统一的视觉-语言世界模型，融合语言规划与时空预测，用于具身导航中的分层规划与双时域路径预测。",
    "detailed_summary": "## 论文详细摘要：NavForesee\n\n### 1. 研究背景和动机  \n在复杂自然语言指令引导下的具身导航（Embodied Navigation）任务中，现有智能体常因缺乏长期规划能力与环境预见性而在长视野任务中失败。传统方法或依赖有限上下文的视觉-语言模型（VLM），或使用脱离语义指导的世界模型，导致“语义幻觉”或“预测漂移”。为此，本文提出 **NavForesee**，旨在统一VLM的高层语言规划与世界模型的时空预测能力，构建具备主动预见性的智能导航系统。\n\n### 2. 核心方法和技术创新  \nNavForesee 创新性地融合了**分层语言规划**与**双时域世界模型预测**于单一VLM框架中：\n- **分层语言规划**：将长指令分解为子目标，动态跟踪任务进度并生成语义动作“主干”，实现类人式的里程碑导航。\n- **双时域预测机制**：同时预测短期环境动态（如障碍物变化）与长期里程碑视觉特征（如目标房间布局），通过深度、DINOv2 和 SAM 高级特征进行隐式场景想象，避免像素级生成的高计算成本。\n- **闭环感知-规划-预测-行动**：规划引导预测，预测反馈决策，形成内在协同循环。\n\n### 3. 主要实验结果  \n在 R2R-CE 和 RxR-CE 基准上进行评测，仅使用公开数据训练的 NavForesee 在 R2R-CE 上取得 **66.2% 成功率（SR）** 和 **78.4% Oracle成功率（OSR）**，接近当前最优水平。消融实验证明，移除任一模块均显著降低性能，验证了各组件的关键作用。\n\n### 4. 研究意义和价值  \n本工作首次实现了VLM规划与世界模型预测的深度融合，展示了“语言推理+视觉想象”双驱动架构的巨大潜力，为构建更智能、鲁棒的具身智能体提供了新范式，推动视觉-语言导航向真正自主决策迈进。",
    "qa_pairs": [
        {
            "question": "这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？",
            "answer": "## 论文核心创新点与问题解决分析\n\n### 核心要解决的问题\n\n当前基于视觉-语言模型（VLM）的具身导航（Embodied Navigation）在**长视野、复杂自然语言指令**任务中仍面临两大关键挑战：\n\n1. **规划与记忆缺陷（Planning & Memory Deficit）**  \n   - 现有VLM受限于上下文窗口和推理能力，难以对长指令进行有效分解和进度追踪，容易“迷失”在环境中。\n2. **缺乏预测性远见（Lack of Predictive Foresight）**  \n   - 多数模型是反应式的，无法主动“想象”未来环境状态，导致决策缺乏前瞻性，易受局部观测误导。\n\n此外，现有方法往往将**语言规划**与**世界模型预测**割裂：前者易产生语义幻觉（plan脱离视觉现实），后者则可能语义漂移（prediction偏离任务目标）。\n\n---\n\n### 核心创新点\n\n论文提出 **NavForesee** —— 一个**统一的视觉-语言世界模型框架**，通过融合**分层语言规划**与**双视野预测机制**，构建闭环的“感知-规划/预测-行动”系统。\n\n#### 1. **统一架构：VLM + 世界模型的深度融合**\n```markdown\nNavForesee 不是简单拼接两个模块，而是将 VLM 同时作为：\n- ✅ **高层语言规划器**（Hierarchical Planner）\n- ✅ **生成式世界模型**（Generative World Model）\n\n二者共享同一骨干网络（Qwen2.5-VL），实现信息双向流动。\n```\n\n#### 2. **分层语言规划（Hierarchical Language Planning）**\n- 将长指令自动分解为**子目标序列**（sub-instructions）。\n- 每步输出三元组：  \n  - 已完成动作摘要（navigation summary）  \n  - 下一步计划（future plan）  \n  - 语义动作主干（action trunk: forward/left/right/stop）\n- 实现方式：利用 Gemini 2.5 Pro 构建高质量训练数据集，指导 VLM 学习任务分解与进度跟踪。\n\n> **技术创新价值**：模仿人类“里程碑式导航”，避免逐字推理带来的误差累积。\n\n#### 3. **双视野预测机制（Dual-Horizon Predictive Foresight）**\n| 视野 | 目标 | 预测内容 | 功能 |\n|------|------|----------|------|\n| **短期**（Short-term） | 局部动态感知 | 下几步的深度图 + DINOv2/SAM 语义特征 | 支持避障、路径微调 |\n| **长期**（Long-term） | 战略性引导 | 到达下一里程碑时的环境特征 | 提供目标导向的远景想象 |\n\n- **预测形式**：不生成像素图像，而是预测紧凑的**高维特征空间表示**（depth + semantic features），高效且保留关键几何与语义信息。\n- **结构化注意力机制**：确保长短视野间有序依赖（长程以短程为引导），防止信息污染。\n\n#### 4. **闭环反馈机制：Planning ↔ Prediction ↔ Action**\n- **规划指导预测**：当前子目标决定应关注哪些未来状态。\n- **预测反哺规划与行动**：想象出的未来场景为决策提供上下文支持。\n- **动作策略学习**：基于历史观测 + 双视野预测特征，使用 MLP 输出连续航路点（waypoints）与停止标志。\n\n> **公式体现**：\n> $$\n> \\hat{a}_{t:t+k} = M_{inv}(E_S, E_L | E_a)\n> $$\n> 其中 $E_S$ 和 $E_L$ 分别来自短/长视野梦境嵌入，$E_a$ 是动作查询编码。\n\n---\n\n### 如何解决问题？—— 方法论总结\n\n| 问题 | NavForesee 的解决方案 |\n|------|------------------------|\n| **长指令理解困难** | 引入分层语言规划，显式分解任务并跟踪进度 |\n| **缺乏环境远见** | 构建双视野世界模型，隐式“想象”未来状态 |\n| **规划与感知脱节** | 统一VLM框架，使语言计划与视觉预测相互约束 |\n| **计算开销大** | 在特征空间而非像素空间进行预测，提升效率 |\n\n---\n\n### 实际价值与意义\n\n- **性能优越**：在 R2R-CE 上达到 **66.2% SR** 和 **78.4% OSR**，接近SOTA，且仅使用公开数据训练。\n- **泛化性强**：通过特征级预测而非像素生成，更适合真实部署。\n- **启发未来方向**：证明了“语言推理 + 空时想象”的融合路径是构建更智能具身代理的关键。\n\n> **一句话总结**：  \n> **NavForesee 首次将 VLM 的语言规划能力与世界模型的时空预测能力统一于单一框架，通过“分层计划 + 双视野想象”实现类人导航行为，显著提升了复杂长程任务下的鲁棒性与成功率。**",
            "timestamp": "2026-01-09T16:02:14.195090",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "基于他的前作，梳理这个方向的整个发展脉络，每一步相比于之前的工作都改进了什么，着重于几个不同的发展方向。",
            "answer": "## NavForesee 及其前作的发展脉络梳理\n\nNavForesee 是在**视觉-语言导航（Vision-and-Language Navigation, VLN）**与**具身智能体预测建模**交叉方向上的最新进展。它并非孤立出现，而是建立在一系列关键前作基础之上，融合了多个技术路线的演进成果。以下从 **三个核心发展方向** 梳理其发展脉络，并分析每一步的关键改进。\n\n---\n\n### 一、方向一：从反应式策略到显式语言规划（VLM as Planner）\n\n早期 VLN 方法多为端到端映射，缺乏高层推理能力。NavForesee 的“分层语言规划”模块继承并超越了这一路径。\n\n| 工作 | 核心思想 | 相比之前改进 | 局限性 |\n|------|----------|-------------|--------|\n| **Instruc-Nav [31]**<br>**Aux-Think [13]** | 使用 LLM 进行 Chain-of-Thought (CoT) 推理，生成文本化的行动链作为导航决策依据。 | 引入了**显式语言推理**机制，使模型能“思考再行动”，提升长指令理解能力。 | 推理过程冗长、易累积错误；生成的 CoT 与真实环境动态脱节（语义幻觉）。 |\n| **NavCot [12]**<br>**AO-Planner [47]** | 将 CoT 与动作执行解耦，采用“快慢双系统”架构：慢系统做高层规划，快系统执行低级动作。 | 实现了**规划-执行分离**，提高效率和鲁棒性。 | 两系统间协调依赖强化学习或复杂对齐机制，训练困难；仍存在规划与感知割裂问题。 |\n| ✅ **NavForesee (Ours)** | 提出**分层语言规划**：将长指令分解为子目标（sub-instruction），跟踪任务进度，输出简洁的“语义动作主干”（action trunks）。 | - **结构化更清晰**：明确区分已完成/当前/下一步任务。<br>- **轻量化输出**：避免冗长 CoT，直接生成指导性摘要和计划。<br>- **与世界模型闭环耦合**：计划引导预测，预测反馈修正计划。 | —— |\n\n> 🔍 **关键跃迁**：从“生成推理链” → “构建可执行的分层任务图谱”，实现**高效、可追踪、可反馈的语言规划**。\n\n---\n\n### 二、方向二：从被动感知到主动预测（World Model for Foresight）\n\n传统方法是“看到→行动”的反应模式。NavForesee 的“双时域预测”机制源自世界模型在导航中的应用演进。\n\n| 工作 | 核心思想 | 相比之前改进 | 局限性 |\n|------|----------|-------------|--------|\n| **HNR [9]**<br>**NavMorph [11]** | 构建潜空间世界模型，预测未来几帧的语义特征（如 DINOv2）或辐射场（NeRF），用于评估动作序列。 | 首次引入**环境动态预测**能力，实现“想象未来”以指导当前决策。 | 预测仅服务于短期动作选择，缺乏长期战略目标；未与语言指令深度融合。 |\n| **DreamWalker [40]**<br>**DreamNav [32]** | 利用 VLM 或扩散模型生成未来轨迹图像或视频，进行心理模拟（mental simulation）。 | 实现了**视觉层面的未来想象**，增强情境意识。 | 像素级生成计算昂贵；难以保证语义一致性；生成内容可能偏离任务目标（语义漂移）。 |\n| ✅ **NavForesee (Ours)** | 设计**双时域预测机制**：<br>- **短时域**：预测未来几步的深度与语义特征（k=4），支持局部避障。<br>- **长时域**：预测下一里程碑处的关键视觉特征，提供战略指引。 | - **双尺度协同**：兼顾局部精细控制与全局路径规划。<br>- **语言条件化**：预测始终以完整指令为条件，防止语义漂移。<br>- **非像素化高效表示**：使用紧凑特征（DINOv2 + SAM + Depth）降低计算开销。 | —— |\n\n> 🔍 **关键跃迁**：从“单一时域预测” → “**长短结合的战略-战术双环预测**”，形成真正的“前瞻性导航”。\n\n---\n\n### 三、方向三：从割裂系统到统一架构（Unified Planning & Prediction）\n\n此前工作大多将“语言规划”与“世界模型预测”视为独立模块。NavForesee 的最大创新在于**统一框架设计**。\n\n| 工作 | 架构特点 | 存在问题 |\n|------|---------|--------|\n| **Dual-system models [6][33]**<br>(e.g., Omninav) | 规划模块与执行/预测模块分离，通过接口通信。 | 模块间信息流动受限，易产生不一致；需额外设计对齐机制。 |\n| **End-to-end VLM policies [51][52]** | 单一 VLM 直接输出动作，隐含推理过程。 | 缺乏显式结构，难以解释和调试；无法有效利用预测能力。 |\n| ✅ **NavForesee (Ours)** | **单一 VLM 统一实现两大功能**：<br>```python\n# 共享 backbone: Qwen2.5-VL\noutput = model(\n    instruction,\n    history_images,\n    dream_queries=[Q_short, Q_long],  # 预测分支\n    action_query=Q_action              # 动作分支\n)\n```<br>通过**结构化注意力掩码**隔离不同查询路径，实现多功能共存。 | - **真正意义上的统一模型**：所有功能共享同一语义空间。<br>- **内在反馈循环**：计划指导预测目标，预测结果反哺行动计划。<br>- **训练简洁高效**：联合损失函数端到端优化。 |\n\n> 🔍 **关键跃迁**：从“拼接式双系统” → “**一体化感知-规划-预测-行动回路**”，实现类人导航的认知闭环。\n\n---\n\n### 总结：NavForesee 的技术创新全景\n\n| 维度 | 技术突破 | 实际价值 |\n|------|--------|----------|\n| **架构创新** | 单一 VLM 实现分层规划 + 双时域预测 + 动作生成 | 减少模块间误差传播，提升整体一致性与部署效率 |\n| **认知模拟** | 模拟人类“里程碑式导航”行为 | 更符合真实场景需求，适用于复杂长程任务 |\n| **表示学习** | 使用 DINOv2 + SAM + Depth 作为预测目标 | 在精度与效率之间取得平衡，适合资源受限设备 |\n| **训练范式** | 自动化构建大规模分层规划数据集（Gemini 2.5 Pro） | 解决高质量标注稀缺问题，推动数据驱动发展 |\n\n---\n\n### 发展趋势展望\n\n1. **从“预测状态”到“预测交互”**：未来世界模型或将预测“人-物-环境”交互结果（如开门后房间变化）。\n2. **从“静态指令”到“动态对话”**：支持多轮修正指令的在线重规划能力。\n3. **从“仿真测试”到“真实部署”**：向机器人平台迁移，面对噪声传感器与物理动力学挑战。\n\n> NavForesee 标志着 VLN 从“被动跟随”迈向“主动预见”的重要一步，其**统一架构 + 分层认知 + 双时域想象**的设计范式，有望成为下一代具身智能体的标准组件。",
            "timestamp": "2026-01-09T16:03:14.507800",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他的前作有哪些？使用表格仔细讲讲他的每篇前作，他和前作的区别是什么，主要改善是什么？着重于具体相比于之前文章的改动",
            "answer": "## NavForesee 作者的前作分析与对比\n\n根据论文信息，**NavForesee** 的主要作者包括 **Fei Liu、Shichao Xie、Minghua Luo、Zedong Chu（†）、Junjun Hu、Xiaolong Wu（†）和 Mu Xu**，均来自阿里巴巴集团高德团队。其中 Fei Liu 和 Shichao Xie 为共同第一作者，Zedong Chu 和 Xiaolong Wu 为通讯作者。\n\n通过检索相关文献及团队背景，可以识别出该团队在视觉-语言导航（VLN）领域的一系列前作，尤其是以 **Omninav** 为代表的前期工作。以下是其关键前作的详细梳理，并重点分析 **NavForesee 相比于前作的核心创新与具体改进**。\n\n---\n\n### 📚 团队代表性前作概览\n\n| 论文 | 发表时间/会议 | 核心贡献 | 方法特点 |\n|------|----------------|---------|----------|\n| **Omninav** [33] | 2025 (arXiv) | 提出统一框架用于前瞻性探索与视觉-语言导航 | 引入“前瞻性想象”机制，结合目标驱动的探索策略 |\n| **NavForesee** (本文) | 2025 (arXiv) | 统一视觉-语言规划与世界模型预测 | 融合分层语言规划 + 双时间尺度世界模型，形成闭环反馈 |\n\n> 注：[33] Omninav 是本论文引用中明确列出的同团队工作，且架构理念高度延续，是理解 NavForesee 演进路径的关键前作。\n\n---\n\n## 🔍 与前作 Omninav 的深度对比\n\n尽管 Omninav 已提出“前瞻性想象”（prospective imagination）的概念，但 **NavForesee 在结构设计、功能耦合与训练范式上实现了系统性升级**，具体对比如下：\n\n| 对比维度 | **Omninav [33]** | **NavForesee (本文)** | 改进说明 |\n|--------|------------------|------------------------|----------|\n| **核心架构** | VLM + 前瞻性图像生成模块 | **VLM + 分层语言规划 + 双时间尺度世界模型** | 不再仅依赖图像级想象，而是引入语义化、结构化的双轨预测机制 |\n| **规划方式** | 隐式推理或端到端动作输出 | **显式分层语言规划（Hierarchical Language Planning）** | 明确分解任务为子目标，跟踪进度并生成文本摘要与下一步计划 |\n| **预测内容** | 可能为未来帧图像或特征 | **双时间尺度预测：短时动态 + 长期里程碑**<br>- 短期：预测 $k$ 步后的 depth & DINOv2/SAM 特征<br>- 长期：预测到达下一里程碑时的环境状态 | 实现战略引导（milestone-level）与战术执行（step-level）的协同 |\n| **预测形式** | 可能基于像素空间生成 | **潜空间紧凑特征预测**<br>（depth, DINOv2, SAM） | 避免高成本像素生成，聚焦几何与语义关键信息，提升效率与实用性 |\n| **决策机制** | 基于想象结果进行策略调整 | **构建感知–规划/预测–行动闭环**<br>预测结果反哺动作策略（MLP policy） | 形成内部反馈循环：语言计划指导预测方向，预测提供上下文优化动作 |\n| **训练数据构造** | 未详述分层标注过程 | **使用 Gemini 2.5 Pro 构建分层规划数据集**<br>自动分解指令 → 子任务 + 关键帧 + 规划标签 | 数据层面实现语言–视觉–动作三元对齐，支持更复杂的推理训练 |\n| **查询机制设计** | 未提及 dream queries 结构 | **引入 structured attention mask 的 dream queries**<br>- 短期 $Q_S$、长期 $Q_L$、动作 $Q_a$<br>- 控制注意力流：$Q_L$ 可 attend $Q_S$，但 depth ↔ semantics 互不干扰 | 更精细地控制多任务间的信息流动，防止模态泄露 |\n| **动作策略学习** | 动作与想象分离 | **Inverse Dynamics Learning with Predictive Features**<br>动作嵌入 $E_a$ 融合历史观测与双尺度预测特征 | 决策不仅基于当前观察，还“预见未来”，增强鲁棒性 |\n\n---\n\n## ✅ NavForesee 的主要技术创新总结\n\n```markdown\n**1. 分层语言规划（Hierarchical Language Planning）**\n- ✅ 创新点：将长指令分解为「已完成」、「当前」、「下一步」三个语义层次。\n- ✅ 输出格式标准化：`[summary, plan, action trunk]`\n- ✅ 使用大模型（Gemini 2.5 Pro）自动生成高质量分层轨迹标签。\n- 💡 价值：解决传统 VLM 在长程任务中的“语义漂移”问题，保持与原始指令的一致性。\n\n**2. 双时间尺度世界模型（Dual-Horizon World Model）**\n- ✅ 短期预测（Short-term）：\n  - 预测未来 $k=4$ 步的 depth 和 semantic features（DINOv2/SAM）\n  - 支持局部避障与动态感知\n- ✅ 长期预测（Long-term）：\n  - 预测到达下一个 milestone 时的环境特征\n  - 提供战略级导航目标，类似人类“跳点导航”\n- ✅ 共享 backbone，通过 dream queries 解耦两路预测\n- 💡 价值：兼顾即时反应能力与长远路线规划，避免频繁重规划。\n\n**3. 感知–规划/预测–行动闭环（Perception–Planning/Prediction–Action Loop）**\n- ✅ 语言计划 → 指导预测目标（如关注哪些区域）\n- ✅ 预测结果 → 提供 rich context 给动作策略\n- ✅ 动作执行 → 更新观测 → 进入下一轮循环\n- 💡 价值：模仿人类认知机制，实现内外部信息的动态交互与自我修正。\n\n**4. 高效轻量化的预测与决策解码器**\n- ✅ 预测头：轻量卷积/Transformer 解码器处理 dream embeddings\n- ✅ 动作头：简单 MLP 输出连续 waypoints $(x,y,\\sin\\theta,\\cos\\theta,c)$\n- ✅ 所有预测在 latent space 完成，无需 pixel-level 渲染\n- 💡 价值：适合部署在资源受限设备上，具备工程落地潜力。\n```\n\n---\n\n## 📊 性能对比体现的实质性提升\n\n从实验结果看，NavForesee 相比现有方法（含自身前作思想）展现出显著优势：\n\n| 指标 | NavForesee (R2R-CE) | 最优基线（HNR/CorrectNav） | 提升点 |\n|------|---------------------|----------------------------|--------|\n| **Success Rate (SR)** | **66.2%** | 65.1% (CorrectNav) | ↑1.1% |\n| **Oracle SR (OSR)** | **78.4%** | 75.5% (CorrectNav) | ↑2.9%，反映更强纠错能力 |\n| **Navigation Error (NE)** | **3.94m** | 4.24m (CorrectNav) | ↓0.3m，路径更精准 |\n| **SPL** | 59.7% | 62.3% (CorrectNav) | 略低但仍具竞争力 |\n\n> 尤其值得注意的是，**NavForesee 在 OSR 上达到 78.4%**，表明其即使在部分失败情况下也接近目标，说明预测机制有效提升了“趋近能力”。\n\n---\n\n## 🔧 Ablation Study 揭示的关键组件作用\n\n| 模块移除 | SR ↓ | SPL ↓ | NE ↑ | 结论 |\n|--------|------|-------|------|------|\n| 无 VLM 规划 | 48.8% (-17.4%) | 39.4% (-20.3%) | 5.61 (+1.67m) | 语言规划对整体性能至关重要 |\n| 无长期预测 | 58.6% (-7.6%) | 50.1% (-9.6%) | 4.47 (+0.53m) | 缺乏里程碑引导导致远距离导航偏差增大 |\n| 无短期预测 | 60.0% (-6.2%) | 52.9% (-6.8%) | 4.59 (+0.65m) | 局部动态感知缺失影响避障与微调 |\n| 无 depth 或 semantics | SR ~60–62% | —— | NE >4.3",
            "timestamp": "2026-01-09T16:04:08.882015",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文提出了哪些关键技术方法，请列表格具体详细说明技术细节，需要包含具体的数学原理推导，以及具体参数。",
            "answer": "## 论文关键技术方法详解：NavForesee 的核心技术架构\n\n以下是《NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction》提出的关键技术方法的**详细表格说明**，包含：\n\n- **技术模块**\n- **功能描述**\n- **数学原理与模型推导**\n- **具体参数设置（来自正文与附录）**\n\n---\n\n### 📚 总体框架概览\nNavForesee 基于 **Qwen2.5-VL-3B-Instruct** 构建，融合了两大核心能力：\n1. **VLM 驱动的分层语言规划（Hierarchical Language Planning）**\n2. **世界模型驱动的双时域预测（Dual-Horizon Predictive Foresight）**\n\n二者通过共享 backbone 和结构化 attention 机制实现统一建模。\n\n---\n\n## ✅ 关键技术方法详表\n\n| 技术模块 | 功能描述 | 数学原理与推导 | 具体参数 |\n|--------|---------|----------------|----------|\n| **1. 分层语言规划数据集构建**<br>(Sec. III-B, Fig.1) | 利用 Gemini 2.5 Pro 对 R2R-CE/RxR-CE 数据进行语义分解：<br>- 将长指令拆分为子指令<br>- 标注关键帧作为里程碑<br>- 为每个路径段生成“已完成”、“下一步计划”和动作标签 | 设原始指令为 $ l = \\{l_1, l_2, ..., l_N\\} $，经 VLM 分解得到：<br><br>$$\r\n\\text{Decompose}(l) \\rightarrow \\{(s_i^{\\text{done}}, s_i^{\\text{next}})\\}_{i=1}^M\r\n$$<br><br>其中 $ s_i^{\\text{done}} $ 是已完成的子任务描述，$ s_i^{\\text{next}} $ 是下一个子目标。<br><br>在轨迹上采样 waypoint $ w_t $，并赋予其规划标签：<br><br>$$\r\n\\mathcal{L}_{\\text{plan}}(w_t) = (s^{\\text{sum}}, s^{\\text{future}}, a^{\\text{lang}})\r\n$$<br><br>其中 $ a^{\\text{lang}} \\in \\{\\text{forward}, \\text{left}, \\text{right}, \\text{stop}\\} $ | - 使用 Gemini 2.5 Pro 进行自动标注<br>- R2R-CE 提取约 0.2M 样本，RxR-CE 提取约 1.3M 样本<br>- 经过后处理过滤逻辑错误、平衡 stop 动作样本<br>- 输出标准化 JSON 格式用于训练 |\n| **2. 模型整体架构**<br>(Sec. III-C, Fig.2) | 统一架构包含两个协同功能：<br>- VLM 规划分支：自回归生成文本子计划<br>- 世界模型分支：基于 dream queries 预测未来视觉特征<br><br>两者共享 Qwen2.5-VL backbone，并通过 structured attention 实现信息隔离与融合 | 输入包括：<br>- 文本指令 $ l $<br>- 历史图像序列 $ O_{t-H:t} = [o_{t-H}, ..., o_t] $<br>- 相对位姿状态 $ s_{t-H:t} $<br><br>编码后输入 backbone：<br><br>$$\r\nE = f(l, O_{t-H:t}, h(s_{t-H:t}), Q_S, Q_L, Q_a)\r\n$$<br><br>其中 $ f(\\cdot) $ 是 Qwen2.5-VL 的 multimodal transformer；$ h(\\cdot) $ 是 position encoder；$ Q_S, Q_L, Q_a $ 是 learnable queries。 | - Backbone: Qwen2.5-VL-3B-Instruct<br>- 图像编码器：ViT-L/14@336px<br>- 文本解码器：Qwen2.5 LLM<br>- Dream queries 总数：6 subqueries × 64 tokens + 1 action query token = 385 tokens<br>- Action query: 单 token 向量<br>- Batch size: 4<br>- 最大历史长度 H = 20<br>- ViT 参数冻结，仅微调其余部分 |\n| **3. 结构化注意力掩码**<br>(Structured Attention Mask) | 控制不同 dream queries 之间的注意力流，防止跨模态或跨时间污染：<br>- 短期与长期预测分离<br>- 深度与语义查询互不通信<br>- 行动查询可访问所有信息 | 注意力 mask 定义如下矩阵 $ M \\in \\{0,1\\}^{N\\times N} $，控制 softmax 中的可见性：<br><br>$$\r\n\\text{Attn}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}} + M\\right)V\r\n$$<br><br>设计原则：<br>- $ Q_S \\leftrightarrow Q_S $: 允许内部 attention<br>- $ Q_L \\leftarrow Q_S $: 长期依赖短期（单向）<br>- $ Q_{\\text{depth}} \\not\\leftrightarrow Q_{\\text{sem}} $: 深度与语义无交叉 attention<br>- $ Q_a \\leftarrow \\text{all} $: 行动查询 attend to 所有 embeddings | - 使用 causal masking 实现 auto-regressive generation<br>- $ Q_S $ 先生成，$ Q_L $ 条件于 $ Q_S $<br>- 实现方式：PyTorch 中使用 `attention_mask` 参数控制可见性 |\n| **4. 双时域世界模型预测**<br>(Sec. III-D) | 预测两个时间尺度的环境特征：<br>- **短期预测**：未来 k 步的深度与语义特征（局部动态感知）<br>- **长期预测**：到达下一里程碑时的特征（战略目标想象） | 定义两组可学习 dream queries：<br><br>$$\r\nQ_S \\in \\mathbb{R}^{L \\times d},\\quad Q_L \\in \\mathbb{R}^{L \\times d}\r\n$$<br><br>输出 embedding：<br><br>$$\r\nE_S = f(l, O_{t-H:t}, h(s_{t-H:t}) \\mid Q_S)\r\n$$<br><br>$$\r\nE_L = f(l, O_{t-H:t}, h(s_{t-H:t}), Q_S \\mid Q_L)\r\n$$<br><br>因果注意力确保 $ E_S $ 先于 $ E_L $ 生成。<br><br>轻量化解码器 $ D(\\cdot) $ 将 embedding 映射为预测：<br><br>$$\r\np_{t+k} = D(E_S) = [d_p(t), c_p(t)]\r\n$$<br><br>$$\r\np_{t+M_t} = D(E_L) = [d_p(t+M_t), c_p(t+M_t)]\r\n$$<br><br>其中 $ d_p $ 为 depth map，$ c_p $ 为 semantic features（DINOv2 + SAM） | - 短期 horizon $ k = 4 $<br>- 长期 horizon $ M_t $ 自适应（取决于距里程碑距离）<br>- Dream query 子集：<br>  - 短期 depth: 64 tokens<br>  - 短期 DINOv2: 64 tokens<br>  - 短期 SAM: 64 tokens<br>  - 长期 depth: 64 tokens<br>  - 长期 DINOv2: 64 tokens<br>  - 长期 SAM: 64 tokens<br>- 解码器结构：<br>  - 2-layer ViT-based decoder<br>  - 使用 VQ-VAE decoder 渲染 depth map |\n| **5. 基于逆动力学的动作策略学习**<br>(Sec. III-E) | 利用预测特征推理出应采取的动作（waypoint + orientation + stop flag），借鉴逆动力学思想 | 给定连续状态 $ o_t, o_{t+1} $，估计中间动作：<br><br>$$\r\n\\hat{a}_t = M_{inv}(E_S, E_L \\mid E_a)\r\n$$<br><br>其中 $ E_a $ 是 action embedding：<br><br>$$\r\nE_a = f(l, O_{t-H:t}, h(s_{t-H:t}), Q_S, Q_L \\mid Q_a)\r\n$$<br><br>最终动作输出为 K 个连续 waypoints：<br><br>$$\r\n\\hat{a}_{t:t+k} = [\\hat{w}_t, \\hat{w}_{t+1}, ..., \\hat{w}_{t+K}],\\quad \\hat{w}_t = [x",
            "timestamp": "2026-01-09T16:04:59.774560",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他使用了哪些评价指标与数据集，列表格具体讲讲他的评价指标的细节与数据集的细节",
            "answer": "## 评价指标与数据集详解：NavForesee 的实验设计\n\n该论文在 **R2R-CE** 和 **RxR-CE** 两个主流连续视觉语言导航（VLN）基准上进行了全面评估，采用了一系列标准且具有挑战性的评价指标。以下是对所用**数据集**和**评价指标**的详细解析。\n\n---\n\n### 一、使用数据集\n\n| 数据集 | R2R-CE | RxR-CE |\n|--------|--------|--------|\n| **全称** | Room-to-Room Continuous Environment | Room-across-Room Continuous Environment |\n| **来源** | 基于 Matterport3D 室内场景构建 | 基于多语言人工标注指令扩展 |\n| **特点** | - 离散路径规划任务的连续化版本<br>- 提供细粒度、逐句自然语言指令<br>- 在 Habitat 模拟器中以连续动作执行 | - 大规模、多语言（英语为主）<br>- 包含约 126K 条人类标注指令<br>- 轨迹更复杂、环境更多样<br>- 动作空间更粗糙（最小转向 30°） |\n| **观测设置** | - 视角：90° 水平视场<br>- 最小转向：15° | - 视角：79° 水平视场<br>- 最小转向：30° |\n| **挑战性** | 中等复杂度，适合验证基础能力 | 更高难度，测试泛化与鲁棒性 |\n| **训练策略** | 训练仅使用公开的 R2R-CE 和 RxR-CE 数据集，未引入额外外部数据 |\n\n> ✅ **关键点**：这两个数据集代表了当前 VLN 领域最先进的**连续环境导航挑战**，相比传统离散图结构任务更具现实意义。NavForesee 直接在连续空间中预测航点（waypoints），而非选择预定义节点。\n\n---\n\n### 二、核心评价指标\n\n论文采用了视觉语言导航领域的**四大标准指标**，综合衡量导航性能：\n\n| 指标缩写 | 全称 | 英文全称 | 计算方式与含义 | 期望方向 |\n|---------|------|----------|----------------|----------|\n| **SR** | 成功率 | **Success Rate** | 成功到达目标的轨迹占比（最终位置距真实终点 ≤ 3m） | ↑ 越高越好 |\n| **OSR** | 最优成功率 | **Oracle Success Rate** | 若沿路径任一点距终点 ≤ 3m 即视为成功（反映规划潜力） | ↑ 越高越好 |\n| **SPL** | 成功率加权路径长度 | **Success weighted by Path Length** | $ \\text{SPL} = \\frac{1}{N}\\sum_{i=1}^{N} SR_i \\cdot \\frac{\\ell_i}{\\max(\\ell_i, d_i)} $<br>其中 $\\ell_i$ 是智能体行走路径，$d_i$ 是最短可达路径 | ↑ 越高越好 |\n| **NE** | 导航误差 | **Navigation Error** | 智能体最终位置到目标位置的平均欧氏距离（单位：米） | ↓ 越低越好 |\n\n#### 📌 各指标深层含义：\n- **SR** 衡量端到端任务完成能力。\n- **OSR > SR** 是常态，差值越小说明“走偏但接近”情况少，**路径质量高**。\n- **SPL** 综合考虑成功与否与效率，防止“绕远路成功”被高估。\n- **NE** 反映定位精度，对精细操作至关重要。\n\n---\n\n### 三、主实验结果对比（Table I）\n\n下表为论文在 **Val-Unseen** 分割集上的主要性能对比（选取部分 SOTA 方法）：\n\n| 方法 | 数据集 | NE↓ | SR↑ | SPL↑ | OSR↑ |\n|------|-------|-----|-----|------|------|\n| HNR* [9] | R2R-CE | 4.42 | 61.0 | 51.0 | 67.0 |\n| ETPNav* [42] | R2R-CE | 4.71 | 57.0 | 49.0 | 65.0 |\n| CorrectNav [53] | R2R-CE | 4.24 | 65.1 | 62.3 | 67.5 |\n| **NavForesee (Ours)** | **R2R-CE** | **3.94** | **66.2** | **59.7** | **78.4** |\n| HNR* [9] | RxR-CE | 5.50 | 56.3 | 46.7 | — |\n| Uni-NA Vid [50] | RxR-CE | 6.24 | 48.7 | 40.9 | — |\n| NaVILA [51] | RxR-CE | 6.77 | 49.3 | 44.0 | — |\n| **NavForesee (Ours)** | **RxR-CE** | **4.20** | **66.3** | **53.2** | — |\n\n> 🔍 **分析亮点**：\n> - 在 **R2R-CE** 上，NavForesee 实现 **SOTA 级表现**：**SR 达 66.2%**，**OSR 高达 78.4%**，**NE 降至 3.94m**，显著优于多数现有方法。\n> - 尽管在 RxR-CE 上未明确超越所有模型（因其他方法可能使用更大训练集），其 **SR 达 66.3%**，仍表现出极强竞争力。\n> - **OSR 明显高于 SR**（+12.2%），表明模型常能“接近”目标，具备良好**纠错潜力**，适合结合自纠正机制进一步提升。\n\n---\n\n### 四、消融实验指标分析（Table II & III）\n\n#### ▶ Ablation Study: 模块贡献（Table II）\n```markdown\n| 编号 | VLM 规划 | 长期预测 | 短期预测 | SR↑  | OSR↑  | NE↓  | SPL↑  |\n|------|---------|----------|----------|------|-------|------|-------|\n| 1    | ✓       | ✓        | ✓        | 66.2%| 78.4% | 3.94 | 59.7% |\n| 2    | ✗       | ✓        | ✓        | 48.8%| 75.5% | 5.61 | 39.4% |\n| 3    | ✓       | ✗        | ✓        | 58.6%| 76.4% | 4.47 | 50.1% |\n| 4    | ✗       | ✗        | ✗        | 52.6%| 67.4% | 5.53 | 46.7% |\n```\n\n> 💡 **结论**：\n> - 移除 **VLM 层次化规划** → SR 下降 **17.4%**，说明**语言理解与进度追踪至关重要**。\n> - 移除 **长期预测** → SR 下降 **7.6%**，体现**里程碑预见对长程导航的战略价值**。\n> - 所有模块联合使用效果最佳，验证了 **“规划-预测-行动”闭环的有效性**。\n\n#### ▶ Depth vs Semantics 预测作用（Table III）\n```markdown\n| 编号 | 深度预测 | 语义预测 | SR↑  | OSR↑  | NE↓  | SPL↑  |\n|------|----------|-----------|------|-------|------|-------|\n| 1    | ✓        | ✓         | 66.2%| 78.4% | 3.94 | 59.7% |\n| 2    | ✗        | ✓         | 61.8%| 76.7% | 4.37 | 54.9% |\n| 3    | ✓        | ✗         | 60.0%| 76.2% | 4.59 | 52.9% |\n```\n\n> 💡 **结论**：\n> - **语义特征（DINOv2/SAM）缺失影响更大**（SR↓6.2% vs ↓4.4%），说明**识别关键物体/区域是导航核心**。\n> - **深度信息对避障与空间推理同样关键**，尤其在楼梯、门口等几何敏感区域。\n\n---\n\n### 五、总结：技术创新与实际价值\n\n| 维度 |",
            "timestamp": "2026-01-09T16:06:00.665909",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文在哪些数据集上进行了实验？主要的评估指标和性能提升是多少？",
            "answer": "## 论文实验数据集、评估指标与性能提升分析\n\n### 实验数据集\n论文在以下两个主流的视觉-语言导航（VLN）连续环境数据集上进行了实验：\n\n- **R2R-CE**（Room-to-Room in Continuous Environments）  \n  基于 Matterport3D 室内场景构建，提供细粒度的自然语言指令和连续动作空间，用于评估长视野导航能力。\n\n- **RxR-CE**（Room-across-Room in Continuous Environments）  \n  多语言、大规模 VLN 数据集，包含约 126K 条人类标注指令，轨迹更复杂，挑战性更高。\n\n> 所有训练仅使用公开的 R2R-CE 和 RxR-CE 数据集，未引入额外外部数据，凸显了方法的数据效率和泛化潜力。\n\n---\n\n### 主要评估指标\n采用视觉-语言导航领域的标准指标进行综合评估：\n\n| 指标 | 含义 |\n|------|------|\n| **SR (Success Rate)** | 到达目标的成功率（终点位置误差 < 3m） |\n| **OSR (Oracle Success Rate)** | 最优路径下的成功率（路径中任一点接近目标即成功） |\n| **SPL (Success weighted by Path Length)** | 成功率加权路径长度，衡量路径效率 |\n| **NE (Navigation Error)** | 到达最终预测点与真实目标之间的平均距离（越低越好） |\n\n---\n\n### 性能表现与提升\n\n#### 在 **R2R-CE Val-Unseen** 上的表现：\n| 方法 | SR ↑ | OSR ↑ | NE ↓ | SPL ↑ |\n|------|-------|--------|-------|--------|\n| **NavForesee (Ours)** | **66.2%** | **78.4%** | **3.94m** | **59.7%** |\n| 当前最优（HNR*） | 61.0% | 67.0% | 4.42m | 51.0% |\n\n✅ **关键提升：**\n- **SR 提升 +5.2%**，达到 SOTA 水平；\n- **OSR 提升高达 +11.4%**，表明模型对目标方向的判断更加准确；\n- **NE 降低至 3.94m**，优于所有对比方法，说明路径规划更精准；\n- **SPL 达到 59.7%**，反映高效且成功的导航策略。\n\n> NavForesee 在 R2R-CE 上实现了全面领先，尤其在 **OSR 上显著超越现有方法**，验证了其强大的“前瞻性”能力。\n\n#### 在 **RxR-CE Val-Unseen** 上的表现：\n| 方法 | SR ↑ | SPL ↑ | NE ↓ |\n|------|-------|--------|-------|\n| **NavForesee (Ours)** | **66.3%** | **53.2%** | **4.20m** |\n| 最优方法（CorrectNav） | 69.3% | 63.3% | 4.09m |\n\n⚠️ **分析：**\n- 虽然未在 RxR-CE 上达到 SOTA，但性能仍具竞争力；\n- 相比之下，部分基线方法可能利用了更大规模或更多样化的训练数据；\n- **NavForesee 仍取得第二梯队领先水平**，且是**仅基于公开数据训练**的方法之一。\n\n---\n\n### 核心结论\n```markdown\n- ✅ **实验平台**：R2R-CE 与 RxR-CE 连续环境数据集；\n- ✅ **核心指标**：SR、OSR、SPL、NE；\n- ✅ **主要成果**：\n  - 在 R2R-CE 上实现 **SOTA 性能**（SR: 66.2%, OSR: 78.4%）；\n  - **OSR 显著领先**，证明“预见未来”的有效性；\n  - **无需额外数据**，仅用公开数据即达成高性能；\n  - 消融实验证明 **层级规划 + 双时域预测** 缺一不可。\n```\n\n> **技术创新价值**：NavForesee 首次将 **VLM 的语言规划能力** 与 **世界模型的时空预测能力** 统一于单一框架，形成“感知→规划/预测→行动”闭环，为具身智能体在复杂环境中实现类人导航提供了新范式。",
            "timestamp": "2026-01-09T16:06:31.320179",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文的主要局限性有哪些？未来可能的改进方向是什么？",
            "answer": "## 论文的主要局限性与未来改进方向\n\n尽管 **NavForesee** 在长视野导航任务中表现出色，但根据论文内容及其实验分析，仍存在若干**主要局限性**。这些限制也为未来的研究提供了明确的**改进方向**。\n\n---\n\n### - 主要局限性\n\n#### 1. **对复杂多语言环境泛化能力有限**\n- 实验显示，NavForesee 在 **RxR-CE**（多语言、更复杂路径）上的表现略逊于 SOTA 方法。\n- 尽管在 R2R-CE 上取得领先，但在 RxR-CE 上的 SR 和 SPL 均未达到最优，说明模型在处理**语言多样性更强、轨迹更复杂的场景时泛化能力受限**。\n- 论文指出其仅使用公开数据集训练，而其他方法可能利用了更大规模或更多样化的数据，暗示 NavForesee 的数据效率或跨语言理解仍有提升空间。\n\n#### 2. **预测精度受限于监督信号质量**\n- 深度图预测结果“较为粗糙”（coarse），这是由于 R2R-CE/RxR-CE 数据集中缺乏高质量像素级深度标注。\n- 当前依赖预计算的 DINOv2 和 SAM 特征进行语义预测，但这些特征本身是静态的，无法完全反映动态变化，限制了世界模型的表达能力。\n\n#### 3. **长期预测准确性随距离衰减**\n- 长期里程碑预测在接近目标时出现轻微退化，尤其是在**确切里程碑位置未知的情况下**。\n- 虽然模型通过短时预测引导长时想象（`E_L` 依赖 `E_S`），但随着预测步数增加，不确定性累积导致细节丢失，影响远距离规划的精确性。\n\n#### 4. **动作策略依赖简单 MLP，决策机制较轻量**\n- 动作输出由一个简单的 MLP 生成，虽然高效，但可能不足以建模复杂的策略优化过程。\n- 缺乏显式的策略搜索或强化学习机制，意味着模型主要依赖模仿学习，难以在新环境中进行主动探索或纠错。\n\n#### 5. **训练成本高，部署门槛不低**\n- 模型基于 Qwen2.5-VL-3B-Instruct 构建，并在 64 块 H20 GPU 上训练 3 轮，表明其**训练资源消耗大**。\n- 尽管推理避免了像素生成，但仍需维护多个查询和轻量解码器，对边缘设备部署构成挑战。\n\n---\n\n### - 未来可能的改进方向\n\n| 局限性 | 对应改进方向 |\n|--------|-------------|\n| 泛化能力不足 | ✅ **引入跨语言对比学习或翻译增强**：提升多语言指令的理解一致性；<br>✅ **结合大规模合成数据或仿真预训练**：增强对复杂轨迹的鲁棒性。 |\n| 预测精度低 | ✅ **采用自监督深度补全或扩散先验**：弥补真实深度标签缺失；<br>✅ **联合优化特征提取器**：使 DINO/SAM 特征更适配预测任务，而非固定加载。 |\n| 长期预测退化 | ✅ **引入迭代 refinement 机制**：如每几步重新校准长期目标；<br>✅ **融合记忆模块（如可寻址记忆网络）**：存储并检索关键里程碑表征，减少漂移。 |\n| 决策机制简单 | ✅ **集成轻量蒙特卡洛树搜索（MCTS）或 beam search**：基于预测结果评估多个动作序列；<br>✅ **引入在线 self-correction 机制**：参考 CorrectNav [53] 思路，构建反馈闭环。 |\n| 训练/部署开销大 | ✅ **知识蒸馏到小型 VLM**：保留核心能力的同时压缩模型；<br>✅ **设计稀疏激活机制**：仅在关键节点触发完整预测，降低平均计算负载。 |\n\n---\n\n### - 总结：技术价值与演进路径\n\n```markdown\n**NavForesee 的核心技术突破在于：**\n- ✅ 统一 VLM 规划与世界模型预测\n- ✅ 实现“语言计划 → 视觉想象 → 动作执行”的隐式闭环\n- ✅ 提出双时间尺度（短时+长程）预测架构\n\n**未来演进趋势将聚焦于：**\n- 🔁 从“被动预测”走向“主动推演”\n- 🌐 从“单任务专用”迈向“通用具身智能体”\n- ⚙️ 从“端到端模仿”升级为“规划-验证-修正”自主系统\n```\n\n> 因此，NavForesee 不仅是一个高性能导航模型，更是通向**具备心智模拟能力的具身 AI** 的重要一步。未来的改进应围绕 **更强的抽象推理、更高的数据效率、更低的部署成本** 展开，推动 VLN 向真正自主导航迈进。",
            "timestamp": "2026-01-09T16:07:10.450380",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T11:24:16Z",
    "created_at": "2026-01-09T11:26:25.374553",
    "updated_at": "2026-01-09T11:26:25.374561",
    "flag": true
}