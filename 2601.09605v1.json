{
    "id": "2601.09605v1",
    "title": "Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets",
    "authors": [
        "Jeremiah Coholich",
        "Justin Wit",
        "Robert Azarcon",
        "Zsolt Kira"
    ],
    "abstract": "åŸºäºè§†è§‰çš„æœºå™¨äººæ“ä½œç­–ç•¥è¿‘æœŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨é¢å¯¹å¦‚ç›¸æœºè§†è§’å˜åŒ–ç­‰åˆ†å¸ƒåç§»æ—¶ä»æ˜¾è„†å¼±ã€‚æœºå™¨äººæ¼”ç¤ºæ•°æ®ç¨€ç¼ºä¸”å¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„ç›¸æœºè§†è§’å˜åŒ–ã€‚ä»¿çœŸæŠ€æœ¯ä¸ºå¤§è§„æ¨¡æ”¶é›†æœºå™¨äººæ¼”ç¤ºæ•°æ®æä¾›äº†å¯èƒ½ï¼Œèƒ½å¤Ÿå…¨é¢è¦†ç›–ä¸åŒè§†è§’ï¼Œä½†ä¹Ÿå¸¦æ¥äº†è§†è§‰ä»¿çœŸåˆ°ç°å®çš„æŒ‘æˆ˜ã€‚ä¸ºå¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºMANGOâ€”â€”ä¸€ç§éé…å¯¹å›¾åƒè½¬æ¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨æ–°é¢–çš„åˆ†å‰²æ¡ä»¶InfoNCEæŸå¤±å‡½æ•°ã€é«˜åº¦è§„èŒƒåŒ–çš„åˆ¤åˆ«å™¨è®¾è®¡ä»¥åŠæ”¹è¿›çš„PatchNCEæŸå¤±ã€‚æˆ‘ä»¬å‘ç°è¿™äº›è¦ç´ å¯¹äºä¿æŒä»¿çœŸåˆ°ç°å®è½¬æ¢è¿‡ç¨‹ä¸­çš„è§†è§’ä¸€è‡´æ€§è‡³å…³é‡è¦ã€‚è®­ç»ƒMANGOæ—¶ï¼Œæˆ‘ä»¬ä»…éœ€å°‘é‡æ¥è‡ªç°å®ä¸–ç•Œçš„å›ºå®šç›¸æœºæ•°æ®ï¼Œä½†å®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½é€šè¿‡è½¬æ¢ä»¿çœŸè§‚æµ‹ç”Ÿæˆå¤šæ ·åŒ–çš„æœªè§è§†è§’ã€‚åœ¨è¯¥é¢†åŸŸä¸­ï¼ŒMANGOçš„è¡¨ç°ä¼˜äºæˆ‘ä»¬æµ‹è¯•çš„æ‰€æœ‰å…¶ä»–å›¾åƒè½¬æ¢æ–¹æ³•ã€‚ä½¿ç”¨ç»MANGOå¢å¼ºæ•°æ®è®­ç»ƒçš„æ¨¡ä»¿å­¦ä¹ ç­–ç•¥ï¼Œåœ¨æœªç»å¢å¼ºç­–ç•¥å®Œå…¨å¤±è´¥çš„è§†è§’ä¸Šå¯å®ç°é«˜è¾¾60%çš„æˆåŠŸç‡ã€‚",
    "url": "https://arxiv.org/abs/2601.09605v1",
    "html_url": "https://arxiv.org/html/2601.09605v1",
    "html_content": "Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets\nJeremiah Coholich\n1\n,\nJustin Wit\n1\n,\nRobert Azarcon\n1\n,\nZsolt Kira\n1\n1\nInstitute of Robotics and Intelligent Machine, Georgia Institute of Technology, Atlanta, GA, USA. Emails: {jcoholich, jwit3, razarcon3, zkira}@gatech.edu.\nAbstract\nVision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge. To bridge this gap, we propose MANGO â€“ an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss. We find that these elements are crucial for maintaining viewpoint consistency during sim2real translation. When training MANGO, we only require a small amount of fixed-camera data from the real world, but show that our method can generate diverse unseen viewpoints by translating simulated observations. In this domain, MANGO outperforms all other image translation methods we tested. Imitation-learning policies trained on data augmented by MANGO are able to achieve success rates as high as 60% on views that the non-augmented policy fails completely on. For more results, visit:\nhttps://sites.google.com/view/sim2real-viewpoints\n.\nI\nIntroduction\nSignificant progress has been made in developing vision-based imitation-learning policies for robot manipulation. Performant single-task architectures\n[\n50\n,\n40\n,\n21\n,\n6\n]\nand intuitive teleoperation methods\n[\n18\n,\n50\n,\n42\n]\nhave given way to large robot datasets and multi-task Vision-Language-Action models (VLAs)\n[\n22\n,\n36\n,\n4\n,\n16\n,\n3\n]\n. However, the robot demonstrations used to train these models are scarce and labor-intensive to generate. In comparison to web-scraped vision-language datasets, robot learning datasets often lack diversity which results in models with poor zero-shot performance to new setups.\nFigure 1:\n(a) Standard image translation methods fail to generalize to new viewpoints when trained on fixed-viewpoint target domain dataset. (b) Our method, MANGO, enables realistic generation of unseen viewpoints, which are used to improve the robustness of downstream robot polices.\nFor example, many tabletop manipulation datasets employ fixed, third person cameras for observations\n[\n12\n,\n39\n,\n23\n,\n41\n,\n21\n,\n6\n,\n46\n]\n, making downstream policies brittle to camera viewpoint shifts. Indeed, we have observed that when robot polices are trained on fixed-camera datasets, changes in camera viewpoint during deployment cause success rates to crater to zero (Table\nIV\n). Cameras are often fixed during demo collection to ensure consistent visual observations, avoid repeated calibration with depth or motion capture sensors, or simply for convenience. Generalizing to truly unseen viewpoints is difficult because a change in viewpoint affects the entire scene, and the model must implicitly estimate the robotâ€™s position relative to the new camera position.\nTo augment fixed-camera robot data, we propose to collect simulated demonstrations from a simple digital twin via task and motion planning whose visual observations are taken from diverse camera viewpoints. Crucially, we train a novel image translation model for bridging the visual sim2real gap. We name our proposed approach Multiview Augmentation with Novel Generated Observations, or MANGO. With MANGO, we simultaneously bypass manual data collection and solve the viewpoint diversity issue. Our MANGO image translation model is trained on a small real-world dataset collected from a single fixed camera plus a larger simulation dataset of images with segmentations. After training, MANGO is able to translate diverse simulation viewpoints to unseen real-world viewpoints. While our method relies on a digital twin, we use a deliberately simple simulation rendered with low-fidelity OpenGL settings, without extensive visual engineering. The entire pipeline yields synthetic demonstrations with realistic and varied camera viewpoints.\nFigure 2:\nMANGO is trained on unpaired real and sim images, specifically a real dataset obtained with a fixed camera and a simulation dataset with diverse camera viewpoints. To ensure the simulation viewpoint is preserved during translation, we employ a novel segmentation-based InfoNCE loss, a modified PatchNCE loss from\n[\n29\n]\n, and a random patch sampling and rotation process to regularize the discriminator\nD\nD\n.\nWe argue for GAN-based models over diffusion models for robot data augmentation. Robot demonstration datasets contain many image observations which are downsampled to small image sizes. A robot dataset for a small, single-task policy may contain upwards of 180,000 images, assuming 150 demonstrations collected at 20 Hz. Using ZeroNVS\n[\n32\n]\n, a state-of-the-art diffusion model for novel viewpoint generation, augmenting this dataset would take 435 GPU-hours. Training an ACT policy on such a dataset would only require\nâˆ¼\n5\n\\sim 5\nhours on a single GPU, meaning that diffusion model-based augmentation would severely bottleneck compute requirements. Our lightweight GAN-based approach is approximately 2,700x times faster than ZeroNVS, making it practical for visual dataset augmentation.\nOur core contributions are as follows:\n1.\nMANGO: A sim2real image translation method incorporating a novel, segmentation-informed contrastive InfoNCE loss capable of preserving unseen simulation viewpoints during translation\n2.\nExperimental proof that demonstrations generated with MANGO improves downstream robot policy robustness to shifts in camera position, even in comparison to a diffusion-based method\n3.\nAnalysis of why our method is successful on the domain of robot demonstration datasets in comparison to the many other approaches developed for the generic problem of unpaired image translation\nII\nRelated Work\nII-A\nVisual Sim2Real Translation for Robotics\nSimulation is a popular tool among roboticists, but to actually leverage simulated data for real robot policies requires bridging the sim2real gap. For visual observations, one option is to improve simulator realism\n[\n24\n]\n, however this is a labor-intensive engineering effort which must be done for every scene. Domain-randomization of lighting, colors, and textures is another option\n[\n30\n,\n2\n,\n10\n]\n, but determining the degree and types of randomizations to apply is a challenge, and policies trained with domain randomization often sacrifice performance for robustness.\nImage-to-image translation has been proposed to cross this sim2real gap by learning from data. A wide variety of unpaired image-to-image translation architectures exist\n[\n51\n,\n29\n,\n49\n,\n8\n,\n44\n,\n35\n,\n7\n]\n. To translate from sim2real, roboticists can directly train these models on datasets of image observations collected from simulation and the real world. For example, in\n[\n47\n]\nand\n[\n38\n]\n, the authors train an unmodified CycleGAN to translate visual observations for grasping and navigation.\nOthers have tailored these methods to incorporate specific knowledge about the robot and downstream application. DigitalTwin-CycleGAN adds an action cycle-consistency loss to CycleGAN for a sim2real visual grasping task\n[\n25\n]\n. This loss makes the image translation model dependent on learning a successful grasping policy concurrently. RL-CycleGAN incorporates Q-function consistency on translated images\n[\n31\n]\n, where the Q-function is obtained while learning a task-specific RL policy. RetinaGAN enforces cycle consistency with an object detector which requires thousands of labeled images to train beforehand\n[\n13\n]\n. GraspGAN trains an image translation model without cycle-consistency and instead enforces accurate image content translation through a grasp success predictor\n[\n5\n]\n. Additionally, they include an auxiliary generator objective of reproducing the ground-truth sim image segmentation. CyCADA unifies these methods under a general â€œtask lossâ€ framework\n[\n15\n]\n. In contrast, MANGO is agnostic to the downstream learning algorithm, enabling us to train one image translation model for many tasks.\nDiffusion models\n[\n14\n]\nhave emerged as the primary architecture for image generation over generative adversarial networks (GANs)\n[\n11\n]\n, with some exceptions\n[\n45\n,\n20\n,\n33\n]\n. However, we find that for the specific domain of unpaired image-to-image translation, GANs obtain results competitive with the best diffusion approaches\n[\n49\n]\n. We hypothesize that that the output multimodality of diffusion models is a disadvantage when the style and content of the generated image are tightly-specified by the input image and target domain dataset, respectively. MANGO uses the GAN loss; however in theory our novel segmentation-based InfoNCE loss could be applied to any image translation architecture containing a generator network with spatially-indexed latent feature maps.\nII-B\nRobot Policy Viewpoint Invariance\nRoboNet offered early proof that training a robot policy on multiple views enables generalization to viewpoint shifts\n[\n9\n]\n. Multi-view Masked World Models (MV-MWM)\n[\n34\n]\ndemonstrates impressive robustness to camera viewpoints by training a viewpoint-invariant visual encoder and task-specific world model. MoVie\n[\n43\n]\nachieves view generalization by adapting an image encoder to the novel views during test-time. In contrast, MANGO does not require any test-time adaptation or real-world images from viewpoints outside of the fixed-camera images used for training.\n[\n1\n]\ntrains an RL policy that is robust to single-camera viewpoint changes after learning from a teacher policy trained with a multi-view observation. VISTA leverages pretrained models with 3D priors to generate novel viewpoints given a single real-world image observation\n[\n37\n]\n. However, since they do not use simulation-generated demonstrations they are unable to generate new robot trajectories and must rely on human demonstration collection. Additionally, VISTA finetunes the ZeroNVS\n[\n32\n]\ndiffusion model, which suffers from high resource requirements as discussed in Section\nI\n.\nLearned 3D representations are inherently viewpoint invariant in theory, but still overfit to the specific 2D sensor locations in practice. Additionally, building strong 3D implicit or explicit representations typically requires more data than a single RGB sensor. For example, GROOT\n[\n52\n]\nachieves impressive viewpoint invariance but requires task-specific object annotations. 3D Diffusion Policy and 3D Diffusor-Actor both build 3D scene representations from calibrated RGBD cameras, but are shown to be brittle to the viewpoints used for this synthesis\n[\n46\n,\n21\n]\n. Adapt3R achieves greater viewpoint robustness through only mapping embedding vectors to 3D instead creating entire scene pointclouds, but still requires multiple calibrated RGBD external sensors\n[\n41\n]\n. MANGO demonstrations viewpoint robustness with only a single external RGB camera.\nIII\nMethod\nIII-A\nImage-to-image Translation\nWe propose MANGO, a novel unpaired image-to-image translation method to translate visual observations from sim to real (Figure\n2\n). MANGO is trained on on a small, fixed-viewpoint dataset of real images yet is capable of accurately translating viewpoint-diverse observations from a simple digital twin to realistic unseen viewpoints.\nThe objective of unpaired image-to-image translation is to translate images from domain\nA\nA\nto domain\nB\nB\nwithout access to a paired dataset of images\nğ’Ÿ\np\nâ€‹\na\nâ€‹\ni\nâ€‹\nr\nâ€‹\ne\nâ€‹\nd\n=\n{\nd\nA\n,\nd\nB\n|\nd\nA\nâˆˆ\nA\n,\nd\nb\nâˆˆ\nB\n}\ni\n=\n0\nN\n\\mathcal{D}_{paired}=\\{d_{A},\\;d_{B}|d_{A}\\in A,\\;d_{b}\\in B\\}_{i=0}^{N}\n. Instead, we learn from two disjoint datasets\nğ’Ÿ\nA\n\\mathcal{D}_{A}\nand\nğ’Ÿ\nB\n\\mathcal{D}_{B}\n, where our domain\nA\nA\nis simulation, our domain\nB\nB\nis the real-world, and\n|\nğ’Ÿ\nA\n|\n>\n|\nğ’Ÿ\nB\n|\n|\\mathcal{D}_{A}|>|\\mathcal{D}_{B}|\n. This problem is considered unsupervised because there is no label, or ground-truth image, in\nğ’Ÿ\nB\n\\mathcal{D}_{B}\nthat images in\nğ’Ÿ\nA\n\\mathcal{D}_{A}\nshould map to.\nImage translators like MANGO must change the style of the input image while preserving its content. We employ the GAN architecture with a highly-regularized discriminator to learn the target domain style. For accurate content preservation, we use the InfoNCE\n[\n28\n]\nloss between input and output image features in a similar style as CUT\n[\n29\n]\n, but with a modified scoring function. Additionally, we propose a novel segmentation-based InfoNCE loss on generator features.\nIII-B\nStyle Loss\nWe use the standard GAN loss\n[\n11\n]\nto enforce target domain style on the generated images, given by Equation\n1\n.\nG\nG\nis the generator network, and\nD\nD\nis the discriminator network.\nâ„’\nGAN\n(\nG\n,\nD\n,\nğ’Ÿ\nA\n,\nğ’Ÿ\nB\n)\n=\nğ”¼\nx\nâˆ¼\nğ’Ÿ\nB\nlog\n(\nD\n(\nx\n)\n)\n+\nğ”¼\ny\nâˆ¼\nğ’Ÿ\nA\nâ€‹\nlog\nâ¡\n(\n1\nâˆ’\nD\nâ€‹\n(\nG\nâ€‹\n(\ny\n)\n)\n)\n\\begin{split}\\mathcal{L}_{\\text{GAN}}(&G,D,\\mathcal{D}_{A},\\mathcal{D}_{B})=\\mathbb{E}_{x\\sim\\mathcal{D}_{B}}\\log(D(x))\\\\\n&+\\ \\mathbb{E}_{y\\sim\\mathcal{D}_{A}}\\log(1-D(G(y)))\\end{split}\n(1)\nOne assumption underlying image-translation GANs, such as CycleGAN\n[\n51\n]\nand CUT\n[\n29\n]\n, is that the shared attributes among all images in\nD\nB\nD_{B}\nconstitute the target domain â€styleâ€. However, our real-world robot image observations in\nğ’Ÿ\nB\n\\mathcal{D}_{B}\nonly differ from one another in robot and object poses. Much of the image content, such as the background and tabletop, is nearly identical in all images in\nğ’Ÿ\nB\n\\mathcal{D}_{B}\n. A naÃ¯ve discriminator will memorize the repetitive details and force the generator to recreate them. To mitigate this problem, Pix2pix\n[\n17\n]\nproposed a â€PatchGANâ€, where the discriminator only receives local image patches and cannot therefore enforce global image elements. We take this a step further and randomly sample patch locations and apply per-patch random rotations. This process is shown in Figure\n2\n. The result is a highly-regularized discriminator capable of enforcing the style of\nğ’Ÿ\nB\n\\mathcal{D}_{B}\non images with viewpoints not seen in\nğ’Ÿ\nB\n\\mathcal{D}_{B}\n.\nFigure 3:\nOur real-world robot setup, including a Franka Emika Panda arm, a wrist camera, and an external camera that is only repositioned for evaluations.\nIII-C\nContent Loss\nMANGO content translation losses consists of a modified version of the PatchNCE loss\n[\n29\n]\nand a novel segmentation-based NCE loss.\nIII-C\n1\nModified PatchNCE Loss\nThe PatchNCE loss consists of an InfoNCE loss across encoder features generated by a source domain image\nd\nA\nâˆˆ\nD\nA\nd_{A}\\in D_{A}\nand its corresponding translated output image\nG\nâ€‹\n(\nd\nA\n)\nG(d_{A})\n. For an input image\nd\nd\n, we randomly sample\nN\nN\nlatent features from the encoderâ€™s feature map at\nL\nL\ndifferent layers. We call the set of features at layer\nl\nl\nğ’µ\nl\n\\mathcal{Z}_{l}\nand\n|\nğ’µ\nl\n|\n=\nN\nâ€‹\nâˆ€\nl\nâˆˆ\n{\nl\n0\n,\nâ€¦\n,\nl\nL\n}\n|\\mathcal{Z}_{l}|=N\\ \\forall\\ l\\in\\{l_{0},...,l_{L}\\}\n. The translated image\nd\n^\n\\hat{d}\nis passed through the encoder again to obtain\n|\nğ’µ\n^\nl\n|\nâ€‹\nâˆ€\nl\nâˆˆ\n{\nl\n0\n,\nâ€¦\n,\nl\nL\n}\n|\\hat{\\mathcal{Z}}_{l}|\\ \\forall\\ l\\in\\{l_{0},...,l_{L}\\}\n. All\nğ’µ\n^\nl\n\\hat{\\mathcal{Z}}_{l}\nare obtained from the same feature map indices as in\nğ’µ\nl\n\\mathcal{Z}_{l}\n.\nThe InfoNCE loss for feature\ni\ni\nin encoder layer\nl\nl\nis given by Equation\n2\n. This is the categorical cross-entropy loss on the probability that a feature\nğ³\nâˆˆ\nğ’µ\n\\mathbf{z}\\in\\mathcal{Z}\nwill be correctly classified as the corresponding feature in\nğ’µ\n^\n\\hat{\\mathcal{Z}}\n, based on a scoring function\nÏ\nl\nâ€‹\n(\nâ‹…\n)\n\\rho_{l}(\\cdot)\n. See\n[\n29\n]\nfor further details.\nÏ„\n\\tau\nis a temperature hyperparameter.\nâ„“\nNCE\nâ€‹\n(\nl\n,\nğ³\n,\nğ’µ\n^\n,\ni\n)\n=\nâˆ’\nlog\nâ¡\n[\nexp\nâ¡\n(\nÏ\nl\nâ€‹\n(\nğ³\n,\nğ³\n^\ni\n)\n/\nÏ„\n)\nâˆ‘\nğ³\n^\nâˆˆ\nğ’µ\n^\nexp\nâ¡\n(\nÏ\nl\nâ€‹\n(\nğ³\n,\nğ³\n^\n)\n/\nÏ„\n)\n]\n\\ell_{\\text{NCE}}(l,\\mathbf{z},\\hat{\\mathcal{Z}},i)=-\\log\\left[\\frac{\\exp\\left(\\rho_{l}(\\mathbf{z},\\hat{\\mathbf{z}}_{i})/\\tau\\right)}{\\displaystyle{\\sum\\limits_{\\hat{\\mathbf{z}}\\in\\hat{\\mathcal{Z}}}\\exp\\left(\\rho_{l}(\\mathbf{z},\\hat{\\mathbf{z}})/\\tau\\right)}}\\right]\n(2)\nÏ\nl\nâ€‹\n(\nâ‹…\n)\n\\rho_{l}(\\cdot)\nis defined in Equation\n3\n. Features\nğ³\ni\n\\mathbf{z}_{i}\nand\nğ³\nj\n\\mathbf{z}_{j}\nare passed through a function\nH\nl\nH_{l}\nthen scored with cosine similarity.\nÏ\nl\nâ€‹\n(\nğ³\ni\n,\nğ³\nj\n)\n=\nH\nl\nâ€‹\n(\nğ³\ni\n)\nâ‹…\nH\nl\nâ€‹\n(\nğ³\nj\n)\nâ€–\nH\nl\nâ€‹\n(\nğ³\ni\n)\nâ€–\nâ€‹\nâ€–\nH\nl\nâ€‹\n(\nğ³\nj\n)\nâ€–\n\\rho_{l}(\\mathbf{z}_{i},\\mathbf{z}_{j})=\\frac{H_{l}(\\mathbf{z}_{i})\\cdot H_{l}(\\mathbf{z}_{j})}{\\|H_{l}(\\mathbf{z}_{i})\\|\\|H_{l}(\\mathbf{z}_{j})\\|}\n(3)\nThe full loss is given in Equation\n4\nâ„’\nPatchNCE\nâ€‹\n(\nG\n,\nH\n,\nğ’Ÿ\n)\n=\nğ”¼\nd\nâˆ¼\nğ’Ÿ\nâ€‹\nâˆ‘\nl\n=\n1\nL\nâˆ‘\ni\n=\n1\n|\nğ’µ\nl\n|\nâ„“\nNCE\nâ€‹\n(\nl\n,\nğ³\nl\n,\ni\n,\nğ’µ\n^\nl\n,\ni\n)\n\\mathcal{L}_{\\text{PatchNCE}}(G,H,\\mathcal{D})=\\mathbb{E}_{d\\sim\\mathcal{D}}\\sum_{l=1}^{L}\\sum_{i=1}^{|\\mathcal{Z}_{l}|}\\ell_{\\text{NCE}}(\\ l,\\ \\mathbf{z}_{l,i},\\ \\hat{\\mathcal{Z}}_{l},\\ i\\ )\n(4)\nThe assumption behind Equation\n2\nis that input and output features from the same feature map locations are â€œpositiveâ€ samples and should have high similarity scores. All other features are â€œnegativeâ€ samples and should be repelled. However, we observe that many different input image patches are highly similar due to repeated patterns or textures in robot image datasets, which include background elements, the tabletop, etc. Furthermore, in the simulated image dataset\nğ’Ÿ\nA\n\\mathcal{D}_{A}\n, these regions all have\nidentical\npixel values. Therefore, Equation\n2\nwill repel many false negative features.\nTo mitigate this, we train MANGO with a modified scoring function. If the cosine similarity of a negative sample exceeds a threshold\nÎ¸\n\\theta\n, we multiply the score by a factor\n0\nâ‰¤\nÎ±\n<\n1\n0\\leq\\alpha<1\n. The modified scoring function is given by Equation\n5\n.\nÏ\n~\nl\nâ€‹\n(\nğ³\ni\n,\nğ³\nj\n)\n=\n{\nÎ±\nâ€‹\nÏ\nl\nâ€‹\n(\nğ³\ni\n,\nğ³\nj\n)\nif\nâ€‹\nÏ\nl\nâ€‹\n(\nğ³\ni\n,\nğ³\nj\n)\n>\nÎ¸\nâ€‹\nand\nâ€‹\ni\nâ‰ \nj\n,\nÏ\nl\nâ€‹\n(\nğ³\ni\n,\nğ³\nj\n)\notherwise.\n\\tilde{\\rho}_{l}(\\mathbf{z}_{i},\\mathbf{z}_{j})=\\begin{cases}\\alpha\\,\\rho_{l}(\\mathbf{z}_{i},\\mathbf{z}_{j})&\\text{if }\\rho_{l}(\\mathbf{z}_{i},\\mathbf{z}_{j})>\\theta\\ \\text{and}\\ i\\neq j,\\\\\n\\rho_{l}(\\mathbf{z}_{i},\\mathbf{z}_{j})&\\text{otherwise.}\\end{cases}\n(5)\nWe have empirically found this to be more effective than increasing\nÏ„\n\\tau\n. We denote the modified NCE loss which uses the scoring function in Equation\n5\nas\nâ„’\n~\nPatchNCE\n\\tilde{\\mathcal{L}}_{\\text{PatchNCE}}\n. In practice, we set\nÎ±\n=\n0.5\n\\alpha=0.5\nand\nÎ¸\n=\n0.9\n\\theta=0.9\n.\nIII-C\n2\nSegmentation NCE Loss\nWe leverage the simulator used to generate\nğ’Ÿ\nA\n\\mathcal{D}_{A}\nto obtain ground-truth segmentation maps for each image. We propose an InfoNCE loss which clusters generator features by segmentation category in order to ensure that object boundaries are preserved during translation. We note that for object-centric manipulation, this is a crucial aspect to preserve when training policies.\nEach image in\nğ’Ÿ\nA\n\\mathcal{D}_{A}\ncontains\nC\nC\nsegmentation classes and each feature\nğ³\ni\nâˆˆ\nğ’µ\n\\mathbf{z}_{i}\\in\\mathcal{Z}\ngenerated from\nd\nâˆ¼\nD\nA\nd\\sim D_{A}\nhas an associated class label\ny\ni\nâˆˆ\nğ’´\ny_{i}\\in\\mathcal{Y}\n. In the case when a layerâ€™s feature map is of a lower resolution than the input image, we scale the image segmentation with nearest-neighbors downsampling to obtain\nğ’´\n\\mathcal{Y}\n.\nThe Segmentation NCE (SegNCE) loss is defined in Equation\n6\n. In contrast to\nâ„“\nNCE\n\\ell_{\\text{NCE}}\nas shown in Equation\n2\n, there are multiple positive samples for the query feature\nğ³\ni\n\\mathbf{z}_{i}\n. All features that are in the same segmentation class as the query feature are classified as positive samples and indexed by\nj\nj\n.\nIn Equation\n2\n, the target distribution for the cross-entropy loss is a one-hot vector. In Equation\n6\nthe target distribution is a uniform distribution over features from the same segmentation class and zero elsewhere.\nHere, we use the original scoring function\nÏ\nl\nâ€‹\n(\nâ‹…\n)\n\\rho_{l}(\\cdot)\ndefined in Equation\n3\n; since we are operating with ground-truth image segmentations, there are no false negatives.\nâ„“\nSegNCE\nâ€‹\n(\nl\n,\nğ’µ\n,\ni\n,\nğ’´\n)\n=\n1\n{\nj\n|\nj\nâˆˆ\n1\n.\n.\n|\nğ’µ\n|\ny\nj\n=\ny\ni\ni\nâ‰ \nj\n}\nâ€‹\nâˆ‘\n{\nj\n|\nj\nâˆˆ\n1\n.\n.\n|\nğ’µ\n|\ny\nj\n=\ny\ni\ni\nâ‰ \nj\n}\nâ„“\nNCE\nâ€‹\n(\nl\n,\nğ³\ni\n,\nğ’µ\n,\nj\n)\n\\begin{split}\\ell_{\\text{SegNCE}}(l,\\mathcal{Z},i,\\mathcal{Y})=\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\\\n\\frac{1}{\\left\\{j\\middle|\\begin{subarray}{c}j\\in 1..|\\mathcal{Z}|\\\\\ny_{j}=y_{i}\\\\\ni\\neq j\\end{subarray}\\right\\}}\\sum_{\\left\\{j\\middle|\\begin{subarray}{c}j\\in 1..|\\mathcal{Z}|\\\\\ny_{j}=y_{i}\\\\\ni\\neq j\\end{subarray}\\right\\}}\\ell_{\\text{NCE}}(l,\\mathbf{z}_{i},\\mathcal{Z},j)\\end{split}\n(6)\nThe full loss term is given in Equation\n7\n.\nâ„’\nSegNCE\nâ€‹\n(\nG\n,\nH\n,\nğ’Ÿ\nA\n)\n=\nğ”¼\nd\nâˆ¼\nğ’Ÿ\nA\nâ€‹\nâˆ‘\nl\n=\n1\nL\nâˆ‘\ni\n=\n1\nS\nâ„“\nSegNCE\nâ€‹\n(\nl\n,\nğ’µ\nl\n,\ni\n,\nğ’´\nl\n)\n\\mathcal{L}_{\\text{SegNCE}}(G,H,\\mathcal{D}_{A})=\\mathbb{E}_{d\\sim\\mathcal{D}_{A}}\\sum_{l=1}^{L}\\sum_{i=1}^{S}\\ell_{\\text{SegNCE}}(l,\\mathcal{Z}_{l},i,\\mathcal{Y}_{l})\n(7)\nThe SegNCE loss is computed from input image generator features only.\nIII-D\nModel training\nThe total loss function for\nG\nG\nis given in equation\n8\n. We include an identity PatchNCE loss following\n[\n29\n]\n. The full discriminator loss is given in Equation\n9\nand is simply the GAN objective.\nâ„’\nG\n=\nâ„’\n~\nPatchNCE\nâ€‹\n(\nG\n,\nH\n,\nğ’Ÿ\nA\n)\n+\nâ„’\n~\nPatchNCE\nâ€‹\n(\nG\n,\nH\n,\nğ’Ÿ\nB\n)\n+\nâ„’\nSegNCE\nâ€‹\n(\nG\n,\nH\n,\nğ’Ÿ\nA\n)\n+\nâ„’\nGAN\nâ€‹\n(\nG\n,\nD\n,\nğ’Ÿ\nA\n,\nğ’Ÿ\nB\n)\n\\begin{split}\\mathcal{L}_{G}=\\tilde{\\mathcal{L}}_{\\text{PatchNCE}}(G,H,\\mathcal{D}_{A})\\\\\n+\\tilde{\\mathcal{L}}_{\\text{PatchNCE}}(G,H,\\mathcal{D}_{B})\\\\\n+\\mathcal{L}_{\\text{SegNCE}}(G,H,\\mathcal{D}_{A})\\\\\n+\\mathcal{L}_{\\text{GAN}}(G,D,\\mathcal{D}_{A},\\mathcal{D}_{B})\\end{split}\n(8)\nâ„’\nD\n=\nâˆ’\nâ„’\nGAN\nâ€‹\n(\nG\n,\nD\n,\nğ’Ÿ\nA\n,\nğ’Ÿ\nB\n)\n\\mathcal{L}_{D}=-\\mathcal{L}_{\\text{GAN}}(G,D,\\mathcal{D}_{A},\\mathcal{D}_{B})\n(9)\nIV\nExperiments\nOur experiments are designed to answer the following questions:\n1.\nHow well can MANGO translate sim images to unseen unseen real-world viewpoints?\n2.\nAre imitation-learning policies trained with synthetic data from MANGO more robust to shifts in camera position?\n3.\nHow does MANGO compare to baselines such as domain randomization and diffusion-based image augmentation methods?\nIV-A\nImage Translation with MANGO\nIV-A\n1\nTraining Details\nOur generator\nG\nG\nis a 12M parameter ResNet-based network. The discriminator\nD\nD\nis a wider three-layer CNN with 11M parameters. Additionally, we parameterize the\nH\nl\nH_{l}\nin Equations\n3\nand\n5\nas a 2-layer MLP with 700k parameters.\nWe first benchmark just the image translation method on observations from a â€œ\npick up coke\nâ€ task without robot policy training.\nğ’Ÿ\nA\n\\mathcal{D}_{A}\nis a dataset of 8,098 image observations from simulation with camera viewpoints randomized within a box of dimensions (100, 100, 84) cm (L\nÃ—\n\\times\nW\nÃ—\n\\times\nH).\nğ’Ÿ\nB\n\\mathcal{D}_{B}\ncontains 3,094 images obtained from roughly 10 minutes of real-world teleoperated play data. All images are cropped and scaled to 256x256. Training a single model takes approximately 20 hours on an RTX 2080 Ti GPU.\nWe curate three test datasets: Fixed View, Randomized View, and Wrist View. Each test set contains 128 sim/real image pairs. The Fixed View test set contains sim and real images from the same fixed view as\nğ’Ÿ\nB\n\\mathcal{D}_{B}\n. The randomized view testset contains both sim and real viewpoints randomized within the box of size (100, 100, 84) cm. In order to create paired images for the Randomized View testset, we use the robot state in conjunction with AprilTags for coke can and camera pose estimation. Note that these are not needed for the deployment of MANGO, only for test set creation.\nTABLE I:\nSim2real Unpaired Image Translation FID Scores.\nEach score is the average score of two models trained with different random seeds.\nD\nD\nis the discriminator network and\nğ’Ÿ\nA\n\\mathcal{D}_{A}\nis the sim training image dataset.\nMethod\nFixed View\nFID\n(\nâ†“\n\\downarrow\n)\nRandomized\nView FID\n(\nâ†“\n\\downarrow\n)\nWrist View\nFID\n(\nâ†“\n\\downarrow\n)\nNo Translation\n340.3\n297.4\n268.8\nCUT\n[\n29\n]\n412.3\n373.9\n266.3\nCycleGAN\n[\n51\n]\n393.9\n359.9\n265.8\nBasic\nD\nD\n371.3\n318.5\n293.5\nWithout SegNCE loss\n267.0\n207.5\n199.9\nWithout\nÏ\n~\nl\nâ€‹\n(\nâ‹…\n)\n\\tilde{\\rho}_{l}(\\cdot)\n192.9\n184.1\n195.3\nFixed-cam\nğ’Ÿ\nA\n\\mathcal{D}_{A}\n108.0\n198.7\n202.5\nMANGO\n182.3\n160.9\n191.3\nIV-A\n2\nResults\nTABLE II:\nSim2sim Experiment Results.\nSuccess rates are reported for 50 rollout of three random seeds. FID scores are calculated by comparing training data with the Oracle dataset.\nUnseen Object\nShared Object\nCross-Embodiment\nData Augmentation\nThreading\nâ†‘\n\\uparrow\nFID\nâ†“\n\\downarrow\nHammer\nâ†‘\n\\uparrow\nFID\nâ†“\n\\downarrow\nCoffee\nâ†‘\n\\uparrow\nFID\nâ†“\n\\downarrow\nStack\nâ†‘\n\\uparrow\nFID\nâ†“\n\\downarrow\nPickPlace\nâ†‘\n\\uparrow\nFID\nâ†“\n\\downarrow\nNut Asm.\nâ†‘\n\\uparrow\nFID\nâ†“\n\\downarrow\nNone (Fixed Camera Only)\n10.67\n54.46\n18.00\n59.45\n14.00\n43.83\n47.33\n25.86\n30.67\n86.28\n13.33\n45.13\nDepth est.+Repoj.\n2.67\n80.47\n20.67\n61.44\n9.33\n68.77\n42.67\n61.29\n31.33\n93.92\n10.00\n61.23\nVISTA\n28.00\n48.19\n56.00\n44.95\n40.67\n43.99\n66.67\n38.60\n45.33\n64.57\n28.67\n25.96\nUntranslated (Domain B)\n1.33\n74.56\n0.00\n58.50\n0.00\n48.61\n2.67\n76.5\n44.67\n80.27\n0.00\n99.59\nMANGO (Domain B\nâ†’\n\\rightarrow\nA)\n30.00\n50.03\n86.00\n32.11\n64.67\n42.45\n71.33\n37.47\n13.33\n100.12\n45.33\n53.88\nSimulator (Oracle)\n57.33\nâ€“\n100.00\nâ€“\n80.67\nâ€“\n86.67\nâ€“\n86.00\nâ€“\n64.00\nâ€“\nTable\nI\ngives the scores of our proposed method against baselines and ablations. MANGO obtains the lowest FID score by 23 points on the Randomized Camera testset. The patch discriminator\nD\nD\nhas the largest impact on the FID score for all testsets. Translated image examples are given in Figure\n5\n.\nTABLE III:\nAverage pairwise LPIPS on natural image datasets.\nA core challenge in robot learning is lack of dataset diversity as compared to web data.\nâ€ \n{\\dagger}\nAverage pairwise LPIPS computed on 1000 randomly sampled images.\nDataset\nAverage Pariwise LPIPS (\nâ†‘\n\\uparrow\n)\nLaion-5B\nâ€ \n0.725\nImageNet\nâ€ \n0.819\nCifar-10\nâ€ \n0.221\nCifar-100\nâ€ \n0.250\nHorse2zebra\nğ’Ÿ\nA\n\\mathcal{D}_{A}\n(Horses)\n0.747\nHorse2zebra\nğ’Ÿ\nB\n\\mathcal{D}_{B}\n(Zebras)\n0.765\nSeg2Cityscapes\nğ’Ÿ\nB\n\\mathcal{D}_{B}\n(Real)\n0.548\npick up coke\nğ’Ÿ\nB\n\\mathcal{D}_{B}\n(Real)\n0.155\nNote that while the relative FID scores correlate well with relative image quality, the scores in Table\nI\nare high compared to the numbers reported in other literature. We posit that this is due to the small size of our test sets and that our robotics lab scene may be out-of-distribution for the Inception network used for FID Score.\nWe hypothesize that off-the-shelf methods like CUT and CycleGAN struggle on robot dataset due to their lack of diversity. Typically, these unpaired image-to-image translation methods are tested on computer vision benchmark datasets containing diverse images scraped from the internet. In comparison, robotics datasets contain limited diversity. To support this claim, we computed average pairwise LPIPS (a learned perceptual similarity metric)\n[\n48\n]\non various image datasets and on our own dataset, shown in Table\nIII\n. Our\nğ’Ÿ\nB\n\\mathcal{D}_{B}\nshows the lowest score for diversity.\nIV-B\nSimulation Experiments\nWe evaluate MANGO on simulated tasks from Robomimic\n[\n27\n]\nand Mimicgen\n[\n26\n]\nand give the results in Table\nII\n. Specifically, we measure the FID scores of data generated with MANGO, as well as the success rates of behavioral-cloning policies trained with the data. Instead of sim2real, we create two visually disparate simulation environments and run sim2sim experiments. Example observations from each domain and MANGO translations are given in Figure\n4\n. We benchmark MANGO against policies with single-camera observations, untranslated Domain A data, and VISTA\n[\n37\n]\n. To provide a fair comparison, we only train MANGO on image observations from the same camera viewpoints and tasks used by VISTA, and report metrics on the same six evaluation tasks. â€Unseen objectâ€ tasks contain objects not seen in training, â€shared objectâ€ contains objects seen during training but in different contexts, and â€cross-embodimentâ€ tasks are seen performed by the Rethink Sawyer robot instead of Franka Panda. We train MANGO with fixed camera data from the six tasks used for testing in domain B, and varied camera data of the eight training tasks in domain A. This ensures our model does not see views from the test viewpoint distribution, aside from the single fixed view. MANGO-trained BC policies obtain the highest success rate on 5 out of 6 tasks.\nFigure 4:\nSim2Sim training data with translations by MANGO for three of the tasks included in Table\nII\nFigure 5:\nLeft:\nSample image observations for each task and data-augmentation method from Table\nIV\n.\nRight:\nThe three shifted viewpoints that comprise our â€œShifted Camsâ€ evaluations in Table\nIV\nIV-C\nReal Robot Experiments\nTABLE IV:\nSuccess rates for imitation learning policies across tasks, viewpoint shifts, and data augmentation methods.\nWe note that the only method comparable to MANGO is VISTA, which leverages a 4.5B parameter pretrained model in contrast MANGOâ€™s 35M parameters.\nData\nAugmentation\nPick up coke\nStack cups\nClose laptop\nStack blocks\nFixed Cam\nShifted Cams\nFixed Cam\nShifted Cams\nFixed Cam\nShifted Cams\nFixed Cam\nShifted Cams\nNone\n8/10\n5/30\n8/10\n1/30\n10/10\n19/30\n8/10\n1/30\nSim\n6/10\n14/30\n9/10\n5/30\n10/10\n20/30\n9/10\n9/30\nSim DR\n8/10\n19/30\n8/10\n5/30\n10/10\n25/30\n7/10\n3/30\nVISTA\n7/10\n23/30\n7/10\n8/30\n10/10\n29/30\n8/10\n18/30\nOurs\n6/10\n17/30\n8/10\n13/30\n10/10\n22/30\n9/10\n11/30\nWe benchmark MANGO on four real-world robotic manipulation tasks: â€œ\npick up coke\nâ€, â€œ\nstack cups\nâ€, â€œ\nclose laptop\nâ€, and â€œ\nstack cups\nâ€. We train a single image translation model for all tasks, where\nğ’Ÿ\nA\n\\mathcal{D}_{A}\nconsists of 59,520 simulated observations and 35,294 fixed-camera real observations from tasks. Our real robot setup is shown in Figure\n3\n.\nIV-C\n1\nPolicy Training and Evaluation Details\nWe train action chunking transformer (ACT)\n[\n50\n]\npolicies on synthetic data generated by MANGO. The generated data is translated from our digital twin demonstrations which leverage the task and motion planner from RLBench\n[\n19\n]\n. All policies are cotrained with 150 human-teleoperated demos with fixed-camera and wrist camera observations. ACT was chosen to isolate the effects of our image data as it does not incorporate any pretraining or language conditioning. We train each ACT policy for 10k epochs with a chunk size of 20. Rollouts are done without temporal aggregation.\nWe compare ACT models trained on MANGO data to strong sim2real and viewpoint augmentation baselines, depicted in Figure\n5\n. For domain randomization we follow\n[\n30\n]\nand randomize color, texture, and lighting for all objects in the scene. VISTA is a viewpoint augmentation method that leverages a fine-tuned ZeroNVS model\n[\n37\n]\n. Unlike MANGO, VISTA has built-in rejection sampling for generated images based on LPIPS distance to the original images.\nWe evaluate each trained policy on 10 variations per task on four different camera viewpoints. The first camera viewpoint is the fixed-camera which the real demonstrations and MANGO training data are collected from. The three shifted viewpoints, depicted in Figure\n5\n, are aggregated into the â€œShifted Camâ€ column in Table\nIV\nfor brevity.\nIV-C\n2\nResults\nResults for MANGO-trained policies and baselines are given in Table\nIV\n. We observe that our method is necessary for viewpoint robustness, as compared to ACT models trained on fixed-cam human demonstrations only, which obtain the lowest success rates on shifted cameras. We also observe that our method is necessary to bridge the sim2real gap, since the policies trained on sim demos without translation perform consistently worse than MANGO.\nV\nDiscussion and Conclusion\nWe train a novel image generation method on simulated and real robot data. We observe that with only fixed-camera real data, our novel SegNCE loss, discriminator design, and modified PatchNCE loss enable translation of simulated demos to highly-realistic novel views. MANGO-generated data improves the robustness of downstream imitation learning policies to camera shift, as demonstrated by greatly increased success rates six simulated and four real-world manipulation tasks. We observe that our method is superior for sim2real translation in this setting, beating all other image translation methods we tested.\nThere are several limitations to this work. MANGO still requires a small amount of real-world data from the evaluation domain for training. Additionally, we struggle to beat VISTA in 3 out of 4 real world tasks when evaluated on the shifted camera viewpoints. The primary benefit of MANGO is its ability to preserve image quality during translations with a lightweight model that is practical for translating robot demonstration datasets with hundreds of thousands of image observations. MANGO, including data generation and image translation, requires less than 0.2% of the GPU-hours required by VISTA. However, larger pretrained models inherit a more general understanding of 3D geometry and scenes. Incorporating MANGOâ€™s novel loss formulations, specifically the segmentation-based InfoNCE loss, into heavier pretrained models for sim2real visual observation translation or real2real augmentation is a promising direction for future work.\nReferences\n[1]\nC. Acar, K. Binici, A. TekirdaÄŸ, and Y. Wu\n(2023)\nVisual-policy learning through multi-camera view to single-camera view knowledge distillation for robot manipulation tasks\n.\nIEEE Robotics and Automation Letters\n9\n(\n1\n),\npp.Â 691â€“698\n.\nCited by:\nÂ§\nII-B\n.\n[2]\nO. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray,\net al.\n(2020)\nLearning dexterous in-hand manipulation\n.\nThe International Journal of Robotics Research\n39\n(\n1\n),\npp.Â 3â€“20\n.\nCited by:\nÂ§\nII-A\n.\n[3]\nJ. Bjorck, F. CastaÃ±eda, N. Cherniadev, X. Da, R. Ding, L. Fan, Y. Fang, D. Fox, F. Hu, S. Huang,\net al.\n(2025)\nGr00t n1: an open foundation model for generalist humanoid robots\n.\narXiv preprint arXiv:2503.14734\n.\nCited by:\nÂ§I\n.\n[4]\nK. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter,\net al.\n(2024)\nPi_0: a vision-language-action flow model for general robot control\n.\narXiv preprint arXiv:2410.24164\n.\nCited by:\nÂ§I\n.\n[5]\nK. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige,\net al.\n(2018)\nUsing simulation and domain adaptation to improve efficiency of deep robotic grasping\n.\nIn\n2018 IEEE international conference on robotics and automation (ICRA)\n,\npp.Â 4243â€“4250\n.\nCited by:\nÂ§\nII-A\n.\n[6]\nC. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song\n(2023)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nThe International Journal of Robotics Research\n,\npp.Â 02783649241273668\n.\nCited by:\nÂ§I\n,\nÂ§I\n.\n[7]\nJ. Choi, S. Kim, Y. Jeong, Y. Gwon, and S. Yoon\n(2021)\nIlvr: conditioning method for denoising diffusion probabilistic models\n.\narXiv preprint arXiv:2108.02938\n.\nCited by:\nÂ§\nII-A\n.\n[8]\nY. Choi, M. Choi, M. Kim, J. Ha, S. Kim, and J. Choo\n(2018)\nStargan: unified generative adversarial networks for multi-domain image-to-image translation\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 8789â€“8797\n.\nCited by:\nÂ§\nII-A\n.\n[9]\nS. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and C. Finn\n(2019)\nRobonet: large-scale multi-robot learning\n.\narXiv preprint arXiv:1910.11215\n.\nCited by:\nÂ§\nII-B\n.\n[10]\nR. Garcia, R. Strudel, S. Chen, E. Arlaud, I. Laptev, and C. Schmid\n(2023)\nRobust visual sim-to-real transfer for robotic manipulation\n.\nIn\n2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 992â€“999\n.\nCited by:\nÂ§\nII-A\n.\n[11]\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio\n(2014)\nGenerative adversarial nets\n.\nAdvances in neural information processing systems\n27\n.\nCited by:\nÂ§\nII-A\n,\nÂ§\nIII-B\n.\n[12]\nS. Haldar, V. Mathur, D. Yarats, and L. Pinto\n(2023)\nWatch and match: supercharging imitation with regularized optimal transport\n.\nIn\nConference on Robot Learning\n,\npp.Â 32â€“43\n.\nCited by:\nÂ§I\n.\n[13]\nD. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y. Bai\n(2021)\nRetinagan: an object-aware approach to sim-to-real transfer\n.\nIn\n2021 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 10920â€“10926\n.\nCited by:\nÂ§\nII-A\n.\n[14]\nJ. Ho, A. Jain, and P. Abbeel\n(2020)\nDenoising diffusion probabilistic models\n.\nAdvances in neural information processing systems\n33\n,\npp.Â 6840â€“6851\n.\nCited by:\nÂ§\nII-A\n.\n[15]\nJ. Hoffman, E. Tzeng, T. Park, J. Zhu, P. Isola, K. Saenko, A. Efros, and T. Darrell\n(2018)\nCycada: cycle-consistent adversarial domain adaptation\n.\nIn\nInternational conference on machine learning\n,\npp.Â 1989â€“1998\n.\nCited by:\nÂ§\nII-A\n.\n[16]\nP. Intelligence, K. Black, N. Brown, J. Darpinian, K. Dhabalia, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai,\net al.\n(2025)\nPi_0.5: a vision-language-action model with open-world generalization\n.\narXiv preprint arXiv:2504.16054\n.\nCited by:\nÂ§I\n.\n[17]\nP. Isola, J. Zhu, T. Zhou, and A. A. Efros\n(2017)\nImage-to-image translation with conditional adversarial networks\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 1125â€“1134\n.\nCited by:\nÂ§\nIII-B\n.\n[18]\nA. Iyer, Z. Peng, Y. Dai, I. Guzey, S. Haldar, S. Chintala, and L. Pinto\n(2024)\nOpen teach: a versatile teleoperation system for robotic manipulation\n.\narXiv preprint arXiv:2403.07870\n.\nCited by:\nÂ§I\n.\n[19]\nS. James, Z. Ma, D. Rovick Arrojo, and A. J. Davison\n(2020)\nRLBench: the robot learning benchmark & learning environment\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\nÂ§\nIV-C\n1\n.\n[20]\nM. Kang, J. Zhu, R. Zhang, J. Park, E. Shechtman, S. Paris, and T. Park\n(2023)\nScaling up gans for text-to-image synthesis\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 10124â€“10134\n.\nCited by:\nÂ§\nII-A\n.\n[21]\nT. Ke, N. Gkanatsios, and K. Fragkiadaki\n(2024)\n3d diffuser actor: policy diffusion with 3d scene representations\n.\narXiv preprint arXiv:2402.10885\n.\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-B\n.\n[22]\nM. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi,\net al.\n(2024)\nOpenvla: an open-source vision-language-action model\n.\narXiv preprint arXiv:2406.09246\n.\nCited by:\nÂ§I\n.\n[23]\nM. A. Lee, Y. Zhu, K. Srinivasan, P. Shah, S. Savarese, L. Fei-Fei, A. Garg, and J. Bohg\n(2019)\nMaking sense of vision and touch: self-supervised learning of multimodal representations for contact-rich tasks\n.\nIn\n2019 International conference on robotics and automation (ICRA)\n,\npp.Â 8943â€“8950\n.\nCited by:\nÂ§I\n.\n[24]\nX. Li, K. Hsu, J. Gu, K. Pertsch, O. Mees, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani,\net al.\n(2024)\nEvaluating real-world robot manipulation policies in simulation\n.\narXiv preprint arXiv:2405.05941\n.\nCited by:\nÂ§\nII-A\n.\n[25]\nD. Liu, Y. Chen, and Z. Wu\n(2023)\nDigital twin (dt)-cyclegan: enabling zero-shot sim-to-real transfer of visual grasping models\n.\nIEEE Robotics and Automation Letters\n8\n(\n5\n),\npp.Â 2421â€“2428\n.\nCited by:\nÂ§\nII-A\n.\n[26]\nA. Mandlekar, S. Nasiriany, B. Wen, I. Akinola, Y. Narang, L. Fan, Y. Zhu, and D. Fox\n(2023)\nMimicgen: a data generation system for scalable robot learning using human demonstrations\n.\narXiv preprint arXiv:2310.17596\n.\nCited by:\nÂ§\nIV-B\n.\n[27]\nA. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei, S. Savarese, Y. Zhu, and R. MartÃ­n-MartÃ­n\n(2021)\nWhat matters in learning from offline human demonstrations for robot manipulation\n.\nIn\narXiv preprint arXiv:2108.03298\n,\nCited by:\nÂ§\nIV-B\n.\n[28]\nA. v. d. Oord, Y. Li, and O. Vinyals\n(2018)\nRepresentation learning with contrastive predictive coding\n.\narXiv preprint arXiv:1807.03748\n.\nCited by:\nÂ§\nIII-A\n.\n[29]\nT. Park, A. A. Efros, R. Zhang, and J. Zhu\n(2020)\nContrastive learning for unpaired image-to-image translation\n.\nIn\nComputer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part IX 16\n,\npp.Â 319â€“345\n.\nCited by:\nFigure 2\n,\nÂ§\nII-A\n,\nÂ§\nIII-A\n,\nÂ§\nIII-B\n,\nÂ§\nIII-C\n1\n,\nÂ§\nIII-C\n,\nÂ§\nIII-D\n,\nTABLE I\n.\n[30]\nL. Pinto, M. Andrychowicz, P. Welinder, W. Zaremba, and P. Abbeel\n(2017)\nAsymmetric actor critic for image-based robot learning\n.\narXiv preprint arXiv:1710.06542\n.\nCited by:\nÂ§\nII-A\n,\nÂ§\nIV-C\n1\n.\n[31]\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari\n(2020)\nRl-cyclegan: reinforcement learning aware simulation-to-real\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 11157â€“11166\n.\nCited by:\nÂ§\nII-A\n.\n[32]\nK. Sargent, Z. Li, T. Shah, C. Herrmann, H. Yu, Y. Zhang, E. R. Chan, D. Lagun, L. Fei-Fei, D. Sun,\net al.\n(2024)\nZeronvs: zero-shot 360-degree view synthesis from a single image\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 9420â€“9429\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[33]\nA. Sauer, T. Karras, S. Laine, A. Geiger, and T. Aila\n(2023)\nStylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis\n.\nIn\nInternational conference on machine learning\n,\npp.Â 30105â€“30118\n.\nCited by:\nÂ§\nII-A\n.\n[34]\nY. Seo, J. Kim, S. James, K. Lee, J. Shin, and P. Abbeel\n(2023)\nMulti-view masked world models for visual robotic manipulation\n.\nIn\nInternational Conference on Machine Learning\n,\npp.Â 30613â€“30632\n.\nCited by:\nÂ§\nII-B\n.\n[35]\nV. Sushko, E. SchÃ¶nfeld, D. Zhang, J. Gall, B. Schiele, and A. Khoreva\n(2020)\nYou only need adversarial supervision for semantic image synthesis\n.\narXiv preprint arXiv:2012.04781\n.\nCited by:\nÂ§\nII-A\n.\n[36]\nA. Szot, B. Mazoure, H. Agrawal, R. D. Hjelm, Z. Kira, and A. Toshev\n(2025)\nGrounding multimodal large language models in actions\n.\nAdvances in Neural Information Processing Systems\n37\n,\npp.Â 20198â€“20224\n.\nCited by:\nÂ§I\n.\n[37]\nS. Tian, B. Wulfe, K. Sargent, K. Liu, S. Zakharov, V. Guizilini, and J. Wu\n(2024)\nView-invariant policy learning via zero-shot novel view synthesis\n.\narXiv preprint arXiv:2409.03685\n.\nCited by:\nÂ§\nII-B\n,\nÂ§\nIV-B\n,\nÂ§\nIV-C\n1\n.\n[38]\nJ. Truong, S. Chernova, and D. Batra\n(2021)\nBi-directional domain adaptation for sim2real transfer of embodied navigation agents\n.\nIEEE Robotics and Automation Letters\n6\n(\n2\n),\npp.Â 2634â€“2641\n.\nCited by:\nÂ§\nII-A\n.\n[39]\nH. Walke, K. Black, A. Lee, M. J. Kim, M. Du, C. Zheng, T. Zhao, P. Hansen-Estruch, Q. Vuong, A. He, V. Myers, K. Fang, C. Finn, and S. Levine\n(2023)\nBridgeData v2: a dataset for robot learning at scale\n.\nIn\nConference on Robot Learning (CoRL)\n,\nCited by:\nÂ§I\n.\n[40]\nL. Wang, X. Chen, J. Zhao, and K. He\n(2024)\nScaling proprioceptive-visual learning with heterogeneous pre-trained transformers\n.\nAdvances in neural information processing systems\n37\n,\npp.Â 124420â€“124450\n.\nCited by:\nÂ§I\n.\n[41]\nA. Wilcox, M. Ghanem, M. Moghani, P. Barroso, B. Joffe, and A. Garg\n(2025)\nAdapt3R: adaptive 3d scene representation for domain transfer in imitation learning\n.\narXiv preprint arXiv:2503.04877\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[42]\nP. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel\n(2024)\nGello: a general, low-cost, and intuitive teleoperation framework for robot manipulators\n.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 12156â€“12163\n.\nCited by:\nÂ§I\n.\n[43]\nS. Yang, Y. Ze, and H. Xu\n(2023)\nMovie: visual model-based policy adaptation for view generalization\n.\nAdvances in Neural Information Processing Systems\n36\n,\npp.Â 21507â€“21523\n.\nCited by:\nÂ§\nII-B\n.\n[44]\nZ. Yi, H. Zhang, P. Tan, and M. Gong\n(2017)\nDualgan: unsupervised dual learning for image-to-image translation\n.\nIn\nProceedings of the IEEE international conference on computer vision\n,\npp.Â 2849â€“2857\n.\nCited by:\nÂ§\nII-A\n.\n[45]\nJ. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan,\net al.\n(2022)\nScaling autoregressive models for content-rich text-to-image generation\n.\narXiv preprint arXiv:2206.10789\n2\n(\n3\n),\npp.Â 5\n.\nCited by:\nÂ§\nII-A\n.\n[46]\nY. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, and H. Xu\n(2024)\n3d diffusion policy: generalizable visuomotor policy learning via simple 3d representations\n.\narXiv preprint arXiv:2403.03954\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[47]\nH. Zhang, H. Liang, L. Cong, J. Lyu, L. Zeng, P. Feng, and J. Zhang\n(2023)\nReinforcement learning based pushing and grasping objects from ungraspable poses\n.\narXiv preprint arXiv:2302.13328\n.\nCited by:\nÂ§\nII-A\n.\n[48]\nR. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang\n(2018)\nThe unreasonable effectiveness of deep features as a perceptual metric\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 586â€“595\n.\nCited by:\nÂ§\nIV-A\n2\n.\n[49]\nM. Zhao, F. Bao, C. Li, and J. Zhu\n(2022)\nEgsde: unpaired image-to-image translation via energy-guided stochastic differential equations\n.\nAdvances in Neural Information Processing Systems\n35\n,\npp.Â 3609â€“3623\n.\nCited by:\nÂ§\nII-A\n,\nÂ§\nII-A\n.\n[50]\nT. Z. Zhao, V. Kumar, S. Levine, and C. Finn\n(2023)\nLearning fine-grained bimanual manipulation with low-cost hardware\n.\narXiv preprint arXiv:2304.13705\n.\nCited by:\nÂ§I\n,\nÂ§\nIV-C\n1\n.\n[51]\nJ. Zhu, T. Park, P. Isola, and A. A. Efros\n(2017)\nUnpaired image-to-image translation using cycle-consistent adversarial networks\n.\nIn\nProceedings of the IEEE international conference on computer vision\n,\npp.Â 2223â€“2232\n.\nCited by:\nÂ§\nII-A\n,\nÂ§\nIII-B\n,\nTABLE I\n.\n[52]\nY. Zhu, Z. Jiang, P. Stone, and Y. Zhu\n(2023)\nLearning generalizable manipulation policies with object-centric 3d representations\n.\narXiv preprint arXiv:2310.14386\n.\nCited by:\nÂ§\nII-B\n.",
    "preview_text": "Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge. To bridge this gap, we propose MANGO -- an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss. We find that these elements are crucial for maintaining viewpoint consistency during sim2real translation. When training MANGO, we only require a small amount of fixed-camera data from the real world, but show that our method can generate diverse unseen viewpoints by translating simulated observations. In this domain, MANGO outperforms all other image translation methods we tested. Imitation-learning policies trained on data augmented by MANGO are able to achieve success rates as high as 60\\% on views that the non-augmented policy fails completely on.\n\nSim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets\nJeremiah Coholich\n1\n,\nJustin Wit\n1\n,\nRobert Azarcon\n1\n,\nZsolt Kira\n1\n1\nInstitute of Robotics and Intelligent Machine, Georgia Institute of Technology, Atlanta, GA, USA. Emails: {jcoholich, jwit3, razarcon3, zkira}@gatech.edu.\nAbstract\nVision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge. To bridge this gap, we propose MANGO â€“ an unpaired image translation",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMANGOçš„sim2realå›¾åƒç¿»è¯‘æ–¹æ³•ï¼Œç”¨äºå¢å¼ºæœºå™¨äººæ“ä½œç­–ç•¥çš„è§†è§’é²æ£’æ€§ï¼Œä½†ä¸VLAã€diffusionã€Flow Matchingã€VLMç­‰æŠ€æœ¯ç›´æ¥ç›¸å…³æ€§è¾ƒä½ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-14T16:25:13Z",
    "created_at": "2026-01-20T17:49:50.729847",
    "updated_at": "2026-01-20T17:49:50.729860",
    "recommend": 0
}